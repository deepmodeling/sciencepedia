{"hands_on_practices": [{"introduction": "The first step in studying chromosomal abnormalities is learning to read and interpret the standard nomenclature used in clinical and research settings. This exercise focuses on deconstructing the International System for Human Cytogenomic Nomenclature (ISCN) for the classic $t(9;22)$ translocation, which gives rise to the Philadelphia chromosome. Mastering this skill is essential for translating a cytogenetic finding into its molecular and clinical consequences, such as the formation of the oncogenic $BCR-ABL1$ fusion gene [@problem_id:2798391].", "problem": "A constitutional karyotype is written as $46,XY,t(9;22)(q34;q11.2)$. Using the rules of the International System for Human Cytogenomic Nomenclature (ISCN) and the definition that a derivative chromosome $der(n)$ is the structurally rearranged chromosome that retains the centromere of chromosome $n$, interpret this notation to decide which derivative chromosomes are $der(9)$ and $der(22)$ and determine which gene loci relocate to which derivative. Assume the breakpoints occur within the $ABL1$ locus on $9q34$ and the $BCR$ (Breakpoint Cluster Region) locus on $22q11.2$, and that a reciprocal exchange of segments distal to the breakpoints has occurred with no additional rearrangements.\n\nWhich option correctly specifies the identities of $der(9)$ and $der(22)$ and the relocation of $ABL1$ and $BCR$?\n\nA. $der(22)$ (the Philadelphia chromosome) carries the distal $9q$ segment from $q34$ including $ABL1$, whereas $der(9)$ carries the distal $22q$ segment from $q11.2$ including $BCR$; thus $ABL1$ relocates to $der(22)$ and $BCR$ relocates to $der(9)$.\n\nB. $der(9)$ (the Philadelphia chromosome) carries the distal $9q$ segment from $q34$ including $ABL1$, whereas $der(22)$ carries the distal $22q$ segment from $q11.2$ including $BCR$; thus $ABL1$ relocates to $der(9)$ and $BCR$ relocates to $der(22)$.\n\nC. Both $ABL1$ and $BCR$ relocate to $der(22)$, leaving $der(9)$ without either breakpoint-associated locus.\n\nD. The breakpoints at $(q34;q11.2)$ do not affect $ABL1$ or $BCR$ because they lie on the $p$ arms; both genes remain on their original chromosomes with no relocation.\n\nE. $der(22)$ retains $BCR$ at $22q11.2$ and does not acquire $ABL1$ sequences, whereas $ABL1$ relocates to $der(9)$; thus $BCR$ does not move while $ABL1$ moves to chromosome $9$.", "solution": "The problem statement must first be rigorously validated for scientific soundness and consistency before any attempt at a solution is made.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n-   Constitutional Karyotype: $46,XY,t(9;22)(q34;q11.2)$\n-   Nomenclature System: International System for Human Cytogenomic Nomenclature (ISCN)\n-   Definition of Derivative Chromosome: $der(n)$ is the structurally rearranged chromosome that retains the centromere of chromosome $n$.\n-   Breakpoint Locations: The breakpoints occur within the $ABL1$ locus on chromosome $9q34$ and the $BCR$ (Breakpoint Cluster Region) locus on chromosome $22q11.2$.\n-   Translocation Mechanism: A reciprocal exchange of segments distal to the breakpoints has occurred.\n-   Constraint: No additional rearrangements have occurred.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated based on established criteria:\n-   **Scientifically Grounded:** The problem describes the $t(9;22)(q34;q11.2)$ reciprocal translocation, which is the well-documented cytogenetic aberration responsible for the formation of the Philadelphia chromosome. The genes $ABL1$ and $BCR$ are correctly mapped to these chromosomal bands. The ISCN nomenclature and the definition of a derivative chromosome are standard in human genetics. The entire premise is factually correct and based on fundamental principles of cytogenetics.\n-   **Well-Posed:** The problem provides all necessary information to deduce the structure of the resulting derivative chromosomes. The karyotype is specified, the breakpoints are identified, the mechanism of exchange is defined as reciprocal, and the rule for naming derivative chromosomes is given. This allows for the derivation of a unique and unambiguous solution.\n-   **Objective:** The problem uses precise, standard, and unbiased scientific language. There are no subjective or ambiguous terms.\n\nThe problem statement displays no flaws. It is not scientifically unsound, incomplete, contradictory, or ill-posed. It is a standard, formal problem in medical genetics.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will now be derived.\n\n### Derivation\nThe given karyotype is $46,XY,t(9;22)(q34;q11.2)$. This notation must be deconstructed.\n1.  `$46,XY$`: Indicates a total of $46$ chromosomes in a male individual. This is the constitutional background.\n2.  `$t(9;22)$`: Denotes a translocation ($t$) between chromosome $9$ and chromosome $22$.\n3.  `(q34;q11.2)`: Specifies the breakpoints. The break on chromosome $9$ occurs on the long arm ($q$) at band $34$. The break on chromosome $22$ occurs on the long arm ($q$) at band $11.2$.\n\nThe translocation is described as a \"reciprocal exchange of segments distal to the breakpoints.\" This means:\n-   Chromosome $9$ breaks at $9q34$. The segment from this breakpoint to the telomere of the long arm (the distal $9q$ segment) is exchanged.\n-   Chromosome $22$ breaks at $22q11.2$. The segment from this breakpoint to the telomere of the long arm (the distal $22q$ segment) is exchanged.\n\nThe resulting rearranged chromosomes are termed derivative chromosomes ($der$). According to the provided definition, $der(n)$ retains the centromere of the original chromosome $n$.\n-   **$der(9)$**: This chromosome is composed of the intact short arm ($p$) and centromere of chromosome $9$, plus the proximal portion of its long arm up to the breakpoint at $9q34$. The segment of chromosome $22$ distal to $22q11.2$ is attached at this breakpoint. Therefore, $der(9)$ consists of ($9$pter $\\rightarrow$ $9q34$)::($22q11.2$ $\\rightarrow$ $22$qter).\n-   **$der(22)$**: This chromosome is composed of the intact short arm ($p$) and centromere of chromosome $22$, plus the proximal portion of its long arm up to the breakpoint at $22q11.2$. The segment of chromosome $9$ distal to $9q34$ is attached at this breakpoint. Therefore, $der(22)$ consists of ($22$pter $\\rightarrow$ $22q11.2$)::($9q34$ $\\rightarrow$ $9$qter). The $der(22)$ is the significantly smaller of the two derivative chromosomes and is historically known as the Philadelphia chromosome.\n\nNow, we must consider the location of the genes.\n-   The $ABL1$ gene is located at $9q34$. The breakpoint is within this gene. The segment distal to $9q34$ contains the majority of the $ABL1$ coding sequence. This segment is translocated to chromosome $22$. Thus, $ABL1$ sequences relocate to the $der(22)$.\n-   The $BCR$ gene is located at $22q11.2$. The breakpoint is within this gene. The segment distal to $22q11.2$ contains the distal portion of the $BCR$ gene. This segment is translocated to chromosome $9$. Thus, sequences from the $BCR$ locus relocate to the $der(9)$.\n\nThe medically significant consequence of this translocation is the formation of a novel fusion gene, $BCR-ABL1$, on the $der(22)$ (the Philadelphia chromosome). This occurs when the proximal part of the $BCR$ gene (which remains on the $der(22)$ backbone) fuses with the translocated distal part of the $ABL1$ gene.\n\n### Option-by-Option Analysis\n\n**A. $der(22)$ (the Philadelphia chromosome) carries the distal $9q$ segment from $q34$ including $ABL1$, whereas $der(9)$ carries the distal $22q$ segment from $q11.2$ including $BCR$; thus $ABL1$ relocates to $der(22)$ and $BCR$ relocates to $der(9)$.**\n-   The $der(22)$ is correctly identified as the Philadelphia chromosome.\n-   It correctly states that $der(22)$ carries the distal $9q$ segment containing $ABL1$ sequences.\n-   It correctly states that $der(9)$ carries the distal $22q$ segment, which contains part of the $BCR$ locus.\n-   The conclusion that $ABL1$ relocates to $der(22)$ and $BCR$ (or part of it) relocates to $der(9)$ is a direct and accurate consequence of the reciprocal translocation.\n-   **Verdict: Correct.**\n\n**B. $der(9)$ (the Philadelphia chromosome) carries the distal $9q$ segment from $q34$ including $ABL1$, whereas $der(22)$ carries the distal $22q$ segment from $q11.2$ including $BCR$; thus $ABL1$ relocates to $der(9)$ and $BCR$ relocates to $der(22)$.**\n-   This option incorrectly identifies $der(9)$ as the Philadelphia chromosome. The Philadelphia chromosome is $der(22)$. This initial error invalidates the entire statement.\n-   **Verdict: Incorrect.**\n\n**C. Both $ABL1$ and $BCR$ relocate to $der(22)$, leaving $der(9)$ without either breakpoint-associated locus.**\n-   This describes a complex or non-reciprocal event, which contradicts the problem statement specifying a \"reciprocal exchange\". In a reciprocal translocation, $der(9)$ must receive a segment from chromosome $22$, which contains a portion of the $BCR$ gene.\n-   **Verdict: Incorrect.**\n\n**D. The breakpoints at $(q34;q11.2)$ do not affect $ABL1$ or $BCR$ because they lie on the $p$ arms; both genes remain on their original chromosomes with no relocation.**\n-   The notation $q34$ and $q11.2$ explicitly refers to the long arms ($q$), not the short arms ($p$).\n-   The problem states the breakpoints are *within* the $ABL1$ and $BCR$ loci, directly contradicting the claim that they are not affected.\n-   **Verdict: Incorrect.**\n\n**E. $der(22)$ retains $BCR$ at $22q11.2$ and does not acquire $ABL1$ sequences, whereas $ABL1$ relocates to $der(9)$; thus $BCR$ does not move while $ABL1$ moves to $chromosome 9$.**\n-   This statement is a sequence of errors. The $der(22)$ *does* acquire $ABL1$ sequences; This is the central event. $ABL1$ relocates to $der(22)$, not $der(9)$. The $BCR$ gene is broken, so it is false to say it \"does not move\"; its distal part moves to $der(9)$. The final clause \"$ABL1$ moves to chromosome $9$\" is meaningless, as it originates on chromosome $9$.\n-   **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2798391"}, {"introduction": "Beyond identifying a translocation, a key task in genetic counseling is to quantify the risk of unbalanced offspring for carriers. This practice problem moves from a qualitative description of chromosomal error to a quantitative prediction of its heritable impact, applying principles of meiotic segregation and probability to a carrier of a Robertsonian translocation. By integrating hypothetical but realistic empirical data on segregation patterns and fetal viability, you will calculate the live-birth risk for trisomy 21, a core skill that bridges chromosomal mechanics and clinical outcomes [@problem_id:2798339].", "problem": "A female carrier of a Robertsonian translocation between chromosomes $14$ and $21$ (denoted rob(14;21)) produces meiotic products via trivalent pairing and segregation. Assume the partner is karyotypically normal, fertilization occurs randomly with respect to gamete class, and postzygotic survival to livebirth depends only on the zygotic karyotype. Use the following empirically supported assumptions:\n\n- In oogenesis for rob(14;21), trivalent orientation results in two segregation modes: alternate and adjacent. The probability of alternate segregation is $0.72$, and the probability of adjacent segregation is $0.28$.\n- Upon fertilization by a normal sperm, alternate segregation yields either a cytogenetically normal or a balanced translocation zygote (both euploid for chromosome $21$), whereas adjacent segregation yields unbalanced conceptions that are either trisomy $21$ or monosomy $21$.\n- Within the adjacent class, the two unbalanced outcomes (trisomy $21$ and monosomy $21$) are equally frequent.\n- The probability that a trisomy $21$ conception survives to livebirth is $0.30$; the probability that a monosomy $21$ conception survives to livebirth is $0$.\n\nUsing only the principles of meiotic segregation in Robertsonian translocations and basic probability, compute the unconditional risk per conception of a liveborn infant with trisomy $21$ for this female rob(14;21) carrier. Express your answer as a decimal and round to four significant figures.", "solution": "The problem statement has been evaluated and is determined to be valid. It is scientifically grounded in the principles of human genetics, well-posed, objective, and contains all necessary information for a unique solution. We will proceed with the calculation.\n\nThe objective is to compute the unconditional risk per conception of a liveborn infant with trisomy $21$. This risk is the result of a sequence of probabilistic events. Let us define the relevant events and their probabilities based on the provided data.\n\nLet $A$ be the event of alternate segregation during oogenesis. The problem states its probability is:\n$$ P(A) = 0.72 $$\nLet $B$ be the event of adjacent segregation. Its probability is:\n$$ P(B) = 0.28 $$\nThe sum of these probabilities is $P(A) + P(B) = 0.72 + 0.28 = 1$, which is consistent.\n\nA conception with trisomy $21$ arises only from adjacent segregation. Within the class of adjacent segregation products, there are two possible unbalanced outcomes that lead to aneuploid zygotes after fertilization with a normal gamete: one that results in trisomy $21$ and another that results in monosomy $21$. The problem states these are equally frequent.\nLet $T_{21}$ be the event that a conception has trisomy $21$. Let $M_{21}$ be the event that a conception has monosomy $21$. The conditional probabilities given adjacent segregation ($B$) are:\n$$ P(T_{21} | B) = 0.5 $$\n$$ P(M_{21} | B) = 0.5 $$\nAlternate segregation ($A$) leads to euploid conceptions (either normal karyotype or a balanced translocation carrier), so the probability of a trisomy $21$ conception given alternate segregation is zero:\n$$ P(T_{21} | A) = 0 $$\n\nThe overall probability of a conception having trisomy $21$, $P(T_{21})$, can be calculated using the law of total probability:\n$$ P(T_{21}) = P(T_{21} | A) P(A) + P(T_{21} | B) P(B) $$\nSubstituting the given values:\n$$ P(T_{21}) = (0 \\times 0.72) + (0.5 \\times 0.28) $$\n$$ P(T_{21}) = 0 + 0.14 $$\n$$ P(T_{21}) = 0.14 $$\nThis is the probability that any given conception will be a trisomy $21$ zygote.\n\nThe problem requires the risk of a *liveborn* infant. This incorporates the probability of postzygotic survival. Let $S_{T21}$ be the event that a trisomy $21$ conception survives to livebirth. The problem provides this probability as:\n$$ P(S_{T21} | T_{21}) = 0.30 $$\nWe are asked to find the unconditional probability of a liveborn infant with trisomy $21$. This corresponds to the joint probability of a conception being trisomy $21$ *and* that conception surviving to livebirth. This is denoted as $P(T_{21} \\cap S_{T21})$.\nUsing the definition of conditional probability, $P(T_{21} \\cap S_{T21}) = P(S_{T21} | T_{21}) \\times P(T_{21})$.\n\nWe have all necessary components for the final calculation:\n$$ P(\\text{Liveborn Trisomy 21}) = P(S_{T21} | T_{21}) \\times P(T_{21}) $$\nSubstituting the calculated and given values:\n$$ P(\\text{Liveborn Trisomy 21}) = 0.30 \\times 0.14 $$\n$$ P(\\text{Liveborn Trisomy 21}) = 0.042 $$\n\nThe problem requires the answer to be expressed as a decimal rounded to four significant figures. The calculated value $0.042$ has two significant figures. To express this with four significant figures, we append two zeros.\n$$ 0.04200 $$\nThis is the final answer. The unconditional risk per conception for this female carrier to have a liveborn child with trisomy $21$ is $4.2\\%$.", "answer": "$$\n\\boxed{0.04200}\n$$", "id": "2798339"}, {"introduction": "Modern genetics seeks to understand not only the consequences of translocations but also the molecular mechanisms that increase the propensity for chromosome breakage at specific genomic locations. This advanced exercise introduces a computational approach, using a penalized logistic regression model to predict the likelihood of translocation breakpoints based on local genomic features like $GC$ content and replication timing. Engaging with this problem [@problem_id:2798351] develops skills in statistical modeling, demonstrating how mechanistic hypotheses about DNA fragility can be formally tested with genomic data.", "problem": "Chromosomal translocations arise when double-strand breaks in deoxyribonucleic acid (DNA) are misrepaired, joining nonhomologous chromosomes. The spatial distribution of breakpoints is non-uniform and is empirically associated with molecular features such as guanine-cytosine (GC) content, replication timing, gene density, and RNA–DNA hybrid loop (R-loop) propensity. Consider modeling whether a genomic bin contains a translocation breakpoint as a binary outcome driven by these features.\n\nAssume the genome is partitioned into bins indexed by $i \\in \\{1,\\dots,n\\}$. For each bin $i$, define a feature vector $\\mathbf{x}_i \\in \\mathbb{R}^5$ with an explicit intercept term, $\\mathbf{x}_i = (1, x_{i1}, x_{i2}, x_{i3}, x_{i4})$, where:\n- $x_{i1}$ is GC content as a fraction in $[0,1]$,\n- $x_{i2}$ is replication timing as a fraction of S-phase in $[0,1]$ (larger means earlier replication),\n- $x_{i3}$ is gene density as the fraction of coding bases in the bin in $[0,1]$,\n- $x_{i4}$ is R-loop propensity as a fraction in $[0,1]$.\n\nLet $y_i \\in \\{0,1\\}$ indicate whether bin $i$ contains a confirmed translocation breakpoint. Assume that, conditional on features, the $y_i$ are independent draws from a Bernoulli distribution with success probability $p_i$, and use a Generalized Linear Model (GLM) with the logit link so that the log-odds are linear in features. Starting from these assumptions and the independence of the Bernoulli trials, derive the penalized maximum likelihood estimator for the coefficients $\\boldsymbol{\\beta} \\in \\mathbb{R}^5$ under an $\\ell_2$ (ridge) penalty of strength $\\lambda \\ge 0$ that does not penalize the intercept. Your derivation must begin from the Bernoulli likelihood and the definition of the logit link, and proceed to a computable objective and its gradient suitable for numerical optimization.\n\nThen implement a program that:\n- Fits the coefficients $\\boldsymbol{\\beta}$ by minimizing the negative penalized log-likelihood for each independent training set below,\n- Computes the predicted breakpoint probability $\\hat{p}_\\star$ for a specified single-bin feature vector $\\mathbf{x}_\\star$ in each case,\n- Uses only general-purpose numerical optimization or your own gradient-based solver (no black-box machine learning model calls),\n- Treats the intercept as unpenalized in the $\\ell_2$ term.\n\nYour implementation should be deterministic and numerically stable.\n\nTest suite:\nFor each case, the training design matrix is given as rows of $(x_{i1}, x_{i2}, x_{i3}, x_{i4})$ with an implicit intercept. All features are unitless fractions. The outcome labels are $y_i \\in \\{0,1\\}$. The regularization strength $\\lambda$ is given per case. For each case, after training, compute the scalar predicted probability $\\hat{p}_\\star$ for the provided $\\mathbf{x}_\\star$.\n\n- Case A (general, mixed signals):\n  - Training features $(x_{i1}, x_{i2}, x_{i3}, x_{i4})$ and labels:\n    1. $(0.40, 0.70, 0.10, 0.30) \\rightarrow y_1 = 1$\n    2. $(0.35, 0.20, 0.05, 0.10) \\rightarrow y_2 = 0$\n    3. $(0.55, 0.80, 0.20, 0.40) \\rightarrow y_3 = 1$\n    4. $(0.60, 0.65, 0.25, 0.50) \\rightarrow y_4 = 1$\n    5. $(0.30, 0.30, 0.08, 0.12) \\rightarrow y_5 = 0$\n    6. $(0.45, 0.50, 0.12, 0.20) \\rightarrow y_6 = 0$\n    7. $(0.70, 0.85, 0.30, 0.60) \\rightarrow y_7 = 1$\n    8. $(0.38, 0.40, 0.09, 0.18) \\rightarrow y_8 = 0$\n    9. $(0.52, 0.75, 0.22, 0.45) \\rightarrow y_9 = 1$\n    10. $(0.48, 0.55, 0.14, 0.22) \\rightarrow y_{10} = 0$\n    11. $(0.62, 0.78, 0.26, 0.55) \\rightarrow y_{11} = 1$\n    12. $(0.33, 0.35, 0.07, 0.15) \\rightarrow y_{12} = 0$\n  - Regularization: $\\lambda = 1.0$\n  - Predict for $\\mathbf{x}_\\star = (0.50, 0.60, 0.15, 0.25)$.\n\n- Case B (near-separable edge case; strong regularization):\n  - Training features and labels:\n    1. $(0.80, 0.90, 0.35, 0.70) \\rightarrow y_1 = 1$\n    2. $(0.78, 0.88, 0.34, 0.65) \\rightarrow y_2 = 1$\n    3. $(0.20, 0.15, 0.03, 0.05) \\rightarrow y_3 = 0$\n    4. $(0.22, 0.18, 0.04, 0.06) \\rightarrow y_4 = 0$\n    5. $(0.82, 0.92, 0.36, 0.72) \\rightarrow y_5 = 1$\n    6. $(0.18, 0.12, 0.02, 0.04) \\rightarrow y_6 = 0$\n    7. $(0.25, 0.20, 0.05, 0.08) \\rightarrow y_7 = 0$\n    8. $(0.77, 0.86, 0.33, 0.60) \\rightarrow y_8 = 1$\n  - Regularization: $\\lambda = 10.0$\n  - Predict for $\\mathbf{x}_\\star = (0.75, 0.85, 0.32, 0.58)$.\n\n- Case C (balanced, moderate signals; light regularization):\n  - Training features and labels:\n    1. $(0.50, 0.50, 0.12, 0.30) \\rightarrow y_1 = 1$\n    2. $(0.47, 0.48, 0.11, 0.28) \\rightarrow y_2 = 0$\n    3. $(0.53, 0.52, 0.13, 0.31) \\rightarrow y_3 = 1$\n    4. $(0.40, 0.45, 0.09, 0.20) \\rightarrow y_4 = 0$\n    5. $(0.60, 0.60, 0.15, 0.35) \\rightarrow y_5 = 1$\n    6. $(0.42, 0.46, 0.10, 0.22) \\rightarrow y_6 = 0$\n    7. $(0.58, 0.58, 0.14, 0.34) \\rightarrow y_7 = 1$\n    8. $(0.45, 0.47, 0.10, 0.24) \\rightarrow y_8 = 0$\n  - Regularization: $\\lambda = 0.1$\n  - Predict for $\\mathbf{x}_\\star = (0.55, 0.56, 0.13, 0.33)$.\n\nFinal output specification:\nYour program should produce a single line of output containing the three predicted probabilities as a comma-separated list enclosed in square brackets, in the order Case A, Case B, Case C. Each probability must be rounded to exactly $6$ decimal places. For example, the output format must be exactly like $[\\hat{p}_A,\\hat{p}_B,\\hat{p}_C]$ with no spaces, where each $\\hat{p}$ is a decimal string with $6$ digits after the decimal point.", "solution": "The problem requires the derivation of the penalized maximum likelihood estimator for a logistic regression model with an $\\ell_2$ penalty (ridge regression) that exempts the intercept term. Following this derivation, an implementation must be provided to fit this model to given datasets and compute predictions.\n\nLet's begin with the formal derivation.\n\n**1. Model Specification**\n\nThe outcome variable $y_i$ for the $i$-th genomic bin is a binary indicator, $y_i \\in \\{0, 1\\}$. We assume it follows a Bernoulli distribution, conditional on a feature vector $\\mathbf{x}_i$.\n$$\ny_i | \\mathbf{x}_i \\sim \\text{Bernoulli}(p_i)\n$$\nThe probability of a success, $p_i = P(y_i=1 | \\mathbf{x}_i)$, is modeled using a Generalized Linear Model (GLM) with a logit link function. The feature vector for bin $i$ is $\\mathbf{x}_i = (1, x_{i1}, x_{i2}, x_{i3}, x_{i4})^T \\in \\mathbb{R}^5$, and the coefficient vector is $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4)^T \\in \\mathbb{R}^5$.\n\nThe logit link function relates the probability $p_i$ to a linear combination of the features:\n$$\n\\text{logit}(p_i) = \\ln\\left(\\frac{p_i}{1-p_i}\\right) = \\mathbf{x}_i^T \\boldsymbol{\\beta}\n$$\nThe inverse of the link function gives the probability $p_i$ as the sigmoid function, denoted by $\\sigma(z) = \\frac{1}{1+e^{-z}}$:\n$$\np_i = \\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}_i^T \\boldsymbol{\\beta}}}\n$$\n\n**2. Likelihood Function**\n\nThe probability mass function for a single Bernoulli observation $y_i$ is:\n$$\nP(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}) = p_i^{y_i} (1 - p_i)^{1-y_i}\n$$\nGiven a dataset of $n$ independent observations $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, the total likelihood function $L(\\boldsymbol{\\beta})$ is the product of individual probabilities:\n$$\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^n P(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}) = \\prod_{i=1}^n p_i^{y_i} (1 - p_i)^{1-y_i}\n$$\nIt is analytically and computationally more convenient to work with the log-likelihood function, $\\ell(\\boldsymbol{\\beta}) = \\ln L(\\boldsymbol{\\beta})$:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\ln \\left( p_i^{y_i} (1 - p_i)^{1-y_i} \\right) = \\sum_{i=1}^n \\left[ y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i) \\right]\n$$\nWe can express the log-likelihood in terms of $\\boldsymbol{\\beta}$ by using the logit link properties. A common and stable formulation is derived by noting that $\\ln(p_i) = \\ln(\\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta}))$ and $\\ln(1-p_i) = \\ln(1-\\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta}))$. This leads to:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i (\\mathbf{x}_i^T \\boldsymbol{\\beta}) - \\ln(1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}) \\right]\n$$\n\n**3. Penalized Objective Function**\n\nThe objective is to find the penalized maximum likelihood estimate of $\\boldsymbol{\\beta}$. We introduce an $\\ell_2$ penalty (ridge) of strength $\\lambda \\ge 0$. The problem specifies that the intercept term, $\\beta_0$, is not penalized. Let $p=4$ be the number of features excluding the intercept. The penalty term $R(\\boldsymbol{\\beta})$ is:\n$$\nR(\\boldsymbol{\\beta}) = \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2\n$$\nThe conventional factor of $\\frac{1}{2}$ simplifies the gradient.\n\nThe penalized log-likelihood is $\\ell_p(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) - R(\\boldsymbol{\\beta})$. Maximizing $\\ell_p(\\boldsymbol{\\beta})$ is equivalent to minimizing its negative, which we define as the objective function $J(\\boldsymbol{\\beta})$:\n$$\nJ(\\boldsymbol{\\beta}) = -\\ell_p(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta}) + R(\\boldsymbol{\\beta})\n$$\nSubstituting the expressions for $\\ell(\\boldsymbol{\\beta})$ and $R(\\boldsymbol{\\beta})$, we obtain the final objective function to be minimized:\n$$\nJ(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ \\ln(1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^T \\boldsymbol{\\beta}) \\right] + \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2\n$$\nThis function is convex, and for $\\lambda > 0$, it is strictly convex, guaranteeing a unique minimum.\n\n**4. Gradient for Numerical Optimization**\n\nTo minimize $J(\\boldsymbol{\\beta})$ using a gradient-based optimization algorithm, we must compute its gradient, $\\nabla_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta})$. We find the partial derivative with respect to each coefficient $\\beta_j$.\n\nFirst, consider the unpenalized part of the objective. For any $\\beta_j$ ($j \\in \\{0, \\dots, p\\}$):\n$$\n\\frac{\\partial}{\\partial \\beta_j} \\left( \\sum_{i=1}^n \\left[ \\ln(1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^T \\boldsymbol{\\beta}) \\right] \\right) = \\sum_{i=1}^n \\left[ \\frac{\\partial}{\\partial \\beta_j} \\ln(1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}) - \\frac{\\partial}{\\partial \\beta_j} y_i (\\mathbf{x}_i^T \\boldsymbol{\\beta}) \\right]\n$$\nUsing the chain rule:\n$$\n\\frac{\\partial}{\\partial \\beta_j} \\ln(1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}) = \\frac{1}{1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}} \\cdot e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}} \\cdot \\frac{\\partial (\\mathbf{x}_i^T \\boldsymbol{\\beta})}{\\partial \\beta_j} = \\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta}) \\cdot x_{ij} = p_i x_{ij}\n$$\nAnd:\n$$\n\\frac{\\partial}{\\partial \\beta_j} y_i (\\mathbf{x}_i^T \\boldsymbol{\\beta}) = y_i x_{ij}\n$$\nSo, the gradient of the negative log-likelihood is:\n$$\n\\sum_{i=1}^n (p_i - y_i) x_{ij}\n$$\nNext, the gradient of the penalty term:\n$$\n\\frac{\\partial R(\\boldsymbol{\\beta})}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j} \\left( \\frac{\\lambda}{2} \\sum_{k=1}^{p} \\beta_k^2 \\right) = \\begin{cases} \\lambda \\beta_j & \\text{if } j \\in \\{1,\\dots,p\\} \\\\ 0 & \\text{if } j=0 \\end{cases}\n$$\nCombining these, the partial derivative of the total objective function $J(\\boldsymbol{\\beta})$ is:\n$$\n\\frac{\\partial J}{\\partial \\beta_j} = \\sum_{i=1}^n (p_i - y_i) x_{ij} + \\begin{cases} \\lambda \\beta_j & \\text{if } j \\in \\{1,\\dots,p\\} \\\\ 0 & \\text{if } j=0 \\end{cases}\n$$\nIn matrix notation, let $\\mathbf{X}$ be the $n \\times (p+1)$ design matrix whose rows are $\\mathbf{x}_i^T$, $\\mathbf{y}$ be the $n \\times 1$ vector of outcomes, and $\\mathbf{p}$ be the $n \\times 1$ vector of predicted probabilities $p_i = \\sigma(\\mathbf{X}\\boldsymbol{\\beta})_i$. Let $\\boldsymbol{\\beta}^* = (0, \\beta_1, \\dots, \\beta_p)^T$. The gradient vector is:\n$$\n\\nabla_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta}) = \\mathbf{X}^T (\\mathbf{p} - \\mathbf{y}) + \\lambda \\boldsymbol{\\beta}^*\n$$\nThis objective function $J(\\boldsymbol{\\beta})$ and its gradient $\\nabla_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta})$ are suitable for any standard numerical optimization routine, such as L-BFGS-B. The calculation of the terms $\\ln(1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}})$ must be handled with care to maintain numerical stability, for instance by using the log-sum-exp trick: $\\ln(1+e^z) = \\max(0,z) + \\ln(1+e^{-|z|})$.\n\n**5. Prediction**\n\nOnce the optimal coefficients $\\hat{\\boldsymbol{\\beta}}$ are found by minimizing $J(\\boldsymbol{\\beta})$, the predicted probability $\\hat{p}_\\star$ for a new feature vector $\\mathbf{x}_\\star$ is calculated using the inverse link function:\n$$\n\\hat{p}_\\star = \\sigma(\\mathbf{x}_\\star^T \\hat{\\boldsymbol{\\beta}}) = \\frac{1}{1 + e^{-\\mathbf{x}_\\star^T \\hat{\\boldsymbol{\\beta}}}}\n$$\n\nThis completes the required derivation. The implementation will follow these derived equations.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the penalized logistic regression problem for three test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {\n            \"X\": np.array([\n                [0.40, 0.70, 0.10, 0.30], [0.35, 0.20, 0.05, 0.10],\n                [0.55, 0.80, 0.20, 0.40], [0.60, 0.65, 0.25, 0.50],\n                [0.30, 0.30, 0.08, 0.12], [0.45, 0.50, 0.12, 0.20],\n                [0.70, 0.85, 0.30, 0.60], [0.38, 0.40, 0.09, 0.18],\n                [0.52, 0.75, 0.22, 0.45], [0.48, 0.55, 0.14, 0.22],\n                [0.62, 0.78, 0.26, 0.55], [0.33, 0.35, 0.07, 0.15]\n            ]),\n            \"y\": np.array([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0]),\n            \"lambda_reg\": 1.0,\n            \"x_star\": np.array([0.50, 0.60, 0.15, 0.25])\n        },\n        # Case B\n        {\n            \"X\": np.array([\n                [0.80, 0.90, 0.35, 0.70], [0.78, 0.88, 0.34, 0.65],\n                [0.20, 0.15, 0.03, 0.05], [0.22, 0.18, 0.04, 0.06],\n                [0.82, 0.92, 0.36, 0.72], [0.18, 0.12, 0.02, 0.04],\n                [0.25, 0.20, 0.05, 0.08], [0.77, 0.86, 0.33, 0.60]\n            ]),\n            \"y\": np.array([1, 1, 0, 0, 1, 0, 0, 1]),\n            \"lambda_reg\": 10.0,\n            \"x_star\": np.array([0.75, 0.85, 0.32, 0.58])\n        },\n        # Case C\n        {\n            \"X\": np.array([\n                [0.50, 0.50, 0.12, 0.30], [0.47, 0.48, 0.11, 0.28],\n                [0.53, 0.52, 0.13, 0.31], [0.40, 0.45, 0.09, 0.20],\n                [0.60, 0.60, 0.15, 0.35], [0.42, 0.46, 0.10, 0.22],\n                [0.58, 0.58, 0.14, 0.34], [0.45, 0.47, 0.10, 0.24]\n            ]),\n            \"y\": np.array([1, 0, 1, 0, 1, 0, 1, 0]),\n            \"lambda_reg\": 0.1,\n            \"x_star\": np.array([0.55, 0.56, 0.13, 0.33])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X_train = case[\"X\"]\n        y_train = case[\"y\"]\n        lambda_reg = case[\"lambda_reg\"]\n        x_star = case[\"x_star\"]\n\n        # 1. Add intercept term to training data\n        intercept_train = np.ones((X_train.shape[0], 1))\n        X_b_train = np.c_[intercept_train, X_train]\n\n        # 2. Define objective function and gradient for the optimizer\n        def objective_and_grad(beta):\n            # linear combination\n            z = X_b_train @ beta\n\n            # Numerically stable calculation of log(1 + exp(z))\n            # log(1+exp(z)) = log(exp(0)+exp(z)) = logsumexp(0,z)\n            # Use max(0,z) + log(1+exp(-|z|)) to avoid overflow\n            lse = np.maximum(0, z) + np.log(1 + np.exp(-np.abs(z)))\n            \n            # Negative log-likelihood (cross-entropy loss)\n            log_loss = np.sum(lse - y_train * z)\n\n            # L2 penalty term (excluding intercept)\n            beta_no_intercept = beta[1:]\n            l2_penalty = 0.5 * lambda_reg * np.dot(beta_no_intercept, beta_no_intercept)\n\n            # Total cost\n            cost = log_loss + l2_penalty\n\n            # Gradient calculation\n            # Numerically stable sigmoid\n            p = 1 / (1 + np.exp(-z))\n            \n            # Gradient of log-loss part\n            grad_log_loss = X_b_train.T @ (p - y_train)\n\n            # Gradient of L2 penalty part\n            grad_penalty = lambda_reg * beta\n            grad_penalty[0] = 0.0  # Do not penalize intercept\n\n            gradient = grad_log_loss + grad_penalty\n            \n            return cost, gradient\n\n        # 3. Fit the model by minimizing the objective function\n        initial_beta = np.zeros(X_b_train.shape[1])\n        res = minimize(\n            fun=objective_and_grad,\n            x0=initial_beta,\n            method='L-BFGS-B',\n            jac=True,  # fun returns both objective and jacobian\n        )\n        beta_hat = res.x\n\n        # 4. Predict probability for the test vector x_star\n        x_star_b = np.insert(x_star, 0, 1) # Add intercept term\n        z_star = x_star_b @ beta_hat\n        p_star = 1 / (1 + np.exp(-z_star))\n\n        results.append(p_star)\n\n    # 5. Format and print the final output\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2798351"}]}