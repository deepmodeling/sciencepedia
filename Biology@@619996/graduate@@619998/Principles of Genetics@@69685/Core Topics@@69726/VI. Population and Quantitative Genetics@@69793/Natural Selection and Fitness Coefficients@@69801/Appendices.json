{"hands_on_practices": [{"introduction": "Understanding how natural selection alters the genetic makeup of a population begins with tracking allele frequencies across a single generation. This practice problem [@problem_id:2832269] grounds you in the fundamentals of viability selection in a diploid population, starting from Hardy–Weinberg proportions. By calculating the next-generation allele frequency, $p'$, you will directly apply the concepts of genotype-specific fitness values ($w_{AA}$, $w_{Aa}$, $w_{aa}$), mean population fitness ($\\bar{w}$), and the effect of dominance ($h$).", "problem": "A large, randomly mating diploid population harbors two alleles, $A$ and $a$, at a single autosomal locus. Zygotes form by random union of gametes, so before selection genotype frequencies follow Hardy–Weinberg Equilibrium (HWE): $p^{2}$ for $AA$, $2pq$ for $Aa$, and $q^{2}$ for $aa$, where $p$ is the allele $A$ frequency and $q=1-p$. Viability selection then acts with relative genotype fitnesses $w_{AA}=1+s$, $w_{Aa}=1+hs$, and $w_{aa}=1$, where $s$ is the selection coefficient and $h$ is the dominance parameter. Assume no other evolutionary forces and that adults contribute gametes proportionally to their post-selection frequencies.\n\nStarting from these definitions only, derive the expression for the next-generation allele-$A$ frequency $p'$ after viability selection and compute its value for $p=0.3$, $s=0.02$, and $h=0.25$. Express your final numerical answer as a decimal rounded to four significant figures.", "solution": "The problem requires the derivation of the allele frequency in the next generation, $p'$, under viability selection, starting from first principles. Subsequently, this expression must be evaluated for a specific set of parameters.\n\nThe initial state of the population is described by Hardy-Weinberg Equilibrium (HWE) frequencies for a single locus with two alleles, $A$ and $a$. Let $p$ be the frequency of allele $A$ and $q=1-p$ be the frequency of allele $a$. The genotype frequencies at the zygote stage (before selection) are:\n- Frequency of $AA$: $f_{zygote}(AA) = p^2$\n- Frequency of $Aa$: $f_{zygote}(Aa) = 2pq$\n- Frequency of $aa$: $f_{zygote}(aa) = q^2$\n\nViability selection acts on these genotypes. The relative fitnesses are given as $w_{AA} = 1+s$, $w_{Aa} = 1+hs$, and $w_{aa} = 1$. The frequency of each genotype among the surviving adults is proportional to its initial frequency multiplied by its relative fitness. To obtain the true frequencies of the adult genotypes, we must normalize these products by their sum. This sum is the mean fitness of the population, $\\bar{w}$.\n\nThe mean fitness $\\bar{w}$ is calculated as the weighted average of the genotype fitnesses, where the weights are the HWE genotype frequencies:\n$$ \\bar{w} = f_{zygote}(AA) \\cdot w_{AA} + f_{zygote}(Aa) \\cdot w_{Aa} + f_{zygote}(aa) \\cdot w_{aa} $$\nSubstituting the given expressions, we have:\n$$ \\bar{w} = p^2(1+s) + 2pq(1+hs) + q^2(1) $$\n\nThe frequencies of the genotypes among the adult population that survives to reproduce are then:\n- Frequency of adult $AA$: $f_{adult}(AA) = \\frac{p^2 w_{AA}}{\\bar{w}} = \\frac{p^2(1+s)}{\\bar{w}}$\n- Frequency of adult $Aa$: $f_{adult}(Aa) = \\frac{2pq w_{Aa}}{\\bar{w}} = \\frac{2pq(1+hs)}{\\bar{w}}$\n- Frequency of adult $aa$: $f_{adult}(aa) = \\frac{q^2 w_{aa}}{\\bar{w}} = \\frac{q^2}{\\bar{w}}$\n\nThe allele frequency of $A$ in the next generation, $p'$, is the frequency of $A$ alleles in the gamete pool produced by these surviving adults. Assuming Mendelian genetics, homozygous $AA$ individuals produce only $A$ gametes, while heterozygous $Aa$ individuals produce $A$ and $a$ gametes in equal proportion ($1/2$ each). Therefore, $p'$ is the sum of the frequency of $AA$ adults and half the frequency of $Aa$ adults:\n$$ p' = f_{adult}(AA) + \\frac{1}{2} f_{adult}(Aa) $$\nSubstituting the expressions for the adult frequencies:\n$$ p' = \\frac{p^2 w_{AA}}{\\bar{w}} + \\frac{1}{2} \\left( \\frac{2pq w_{Aa}}{\\bar{w}} \\right) = \\frac{p^2 w_{AA} + pq w_{Aa}}{\\bar{w}} $$\nFactoring out $p$ from the numerator gives an alternative form:\n$$ p' = \\frac{p(p w_{AA} + q w_{Aa})}{\\bar{w}} $$\nSubstituting the full expression for $\\bar{w}$ and the specific fitness values, we arrive at the general formula for the allele frequency in the next generation:\n$$ p' = \\frac{p^2(1+s) + pq(1+hs)}{p^2(1+s) + 2pq(1+hs) + q^2} $$\nThis concludes the derivation.\n\nNext, we must compute the numerical value of $p'$ given $p=0.3$, $s=0.02$, and $h=0.25$.\nFirst, we determine the frequency of the $a$ allele: $q = 1-p = 1-0.3 = 0.7$.\n\nWe calculate the numerator of the expression for $p'$:\n$$ \\text{Numerator} = p^2(1+s) + pq(1+hs) $$\n$$ \\text{Numerator} = (0.3)^2(1+0.02) + (0.3)(0.7)(1+(0.25)(0.02)) $$\n$$ \\text{Numerator} = (0.09)(1.02) + (0.21)(1+0.005) $$\n$$ \\text{Numerator} = (0.09)(1.02) + (0.21)(1.005) $$\n$$ \\text{Numerator} = 0.0918 + 0.21105 = 0.30285 $$\n\nNext, we calculate the denominator, which is the mean fitness $\\bar{w}$:\n$$ \\bar{w} = p^2(1+s) + 2pq(1+hs) + q^2 $$\n$$ \\bar{w} = (0.3)^2(1+0.02) + 2(0.3)(0.7)(1.005) + (0.7)^2 $$\n$$ \\bar{w} = 0.0918 + (0.42)(1.005) + 0.49 $$\n$$ \\bar{w} = 0.0918 + 0.4221 + 0.49 = 1.0039 $$\n\nFinally, we compute $p'$ by dividing the numerator by the denominator:\n$$ p' = \\frac{0.30285}{1.0039} \\approx 0.3016635123... $$\nThe problem requires the result to be rounded to four significant figures. The fifth significant figure is $6$, so we round up the fourth.\n$$ p' \\approx 0.3017 $$", "answer": "$$ \\boxed{0.3017} $$", "id": "2832269"}, {"introduction": "While tracking allele frequency change generation by generation is informative, population genetics is also concerned with long-term evolutionary outcomes. This exercise [@problem_id:2832267] challenges you to move beyond single-generation dynamics and determine the equilibrium state of a population under a specific mode of selection: overdominance, or heterozygote advantage. You will derive the condition for a stable internal equilibrium allele frequency, $p^*$, and calculate the mean population fitness at this point, demonstrating a powerful mechanism by which natural selection actively maintains genetic variation.", "problem": "Consider a very large, randomly mating diploid population with discrete non-overlapping generations under viability selection at a single locus with two alleles $A$ and $a$. Assume Hardy–Weinberg Equilibrium (HWE) before selection in each generation. The genotype relative viabilities are $w_{AA}=0.9$, $w_{Aa}=1$, and $w_{aa}=0.8$. Let $p$ denote the frequency of allele $A$ among zygotes before selection and $q=1-p$.\n\nStarting from the foundational definitions of relative fitness, mean fitness, and the allele-frequency recursion under viability selection (obtained by weighting genotype frequencies by their relative fitnesses and normalizing by the mean fitness), do the following:\n\n1. Derive the condition that characterizes any internal equilibrium allele frequency $p^{*}\\in(0,1)$ under overdominance (that is, when the heterozygote has the highest viability), expressed in terms of the fitness parameters.\n2. Using that condition, determine the internal equilibrium allele frequency $p^{*}$ for the given fitness values.\n3. Evaluate the mean fitness $\\bar{w}$ at this equilibrium frequency.\n\nReport only the mean fitness at equilibrium as your final answer, and express it as an exact rational number. Do not include any units.", "solution": "The problem posed is a standard exercise in theoretical population genetics and is well-defined. We shall proceed with its solution.\n\nThe foundational principle of viability selection in a diploid population is that different genotypes survive from the zygote stage to the adult stage at different rates. These differential survival rates are quantified by relative fitnesses, or viabilities, which we denote as $w_{AA}$, $w_{Aa}$, and $w_{aa}$.\n\nWe begin by considering a very large population where mating is random. Under these conditions, the genotype frequencies among zygotes in any generation are given by the Hardy-Weinberg proportions:\n- Frequency of $AA$ zygotes: $f(AA)_{zygotes} = p^2$\n- Frequency of $Aa$ zygotes: $f(Aa)_{zygotes} = 2pq$\n- Frequency of $aa$ zygotes: $f(aa)_{zygotes} = q^2$\nwhere $p$ is the frequency of allele $A$ and $q = 1-p$ is the frequency of allele $a$ in the gene pool of the previous generation.\n\nAfter selection, the frequencies of the three genotypes among the surviving adults are proportional to their initial frequencies multiplied by their respective relative viabilities.\n- Proportional frequency of $AA$ adults $\\propto p^2 w_{AA}$\n- Proportional frequency of $Aa$ adults $\\propto 2pq w_{Aa}$\n- Proportional frequency of $aa$ adults $\\propto q^2 w_{aa}$\n\nThe sum of these proportional frequencies gives the mean fitness of the population, $\\bar{w}$, which serves as the normalization constant to bring the frequencies of surviving adults to sum to $1$.\n$$ \\bar{w} = p^2 w_{AA} + 2pq w_{Aa} + q^2 w_{aa} $$\n\nThe frequency of allele $A$ in the next generation, denoted $p'$, is the frequency of allele $A$ among the gametes produced by the surviving adults. Since $AA$ individuals produce only $A$ gametes and $Aa$ individuals produce $A$ gametes with a frequency of $1/2$, the frequency $p'$ is given by the sum of the frequencies of $AA$ adults and half the frequency of $Aa$ adults.\n$$ p' = \\frac{p^2 w_{AA} + \\frac{1}{2}(2pq w_{Aa})}{\\bar{w}} = \\frac{p^2 w_{AA} + pq w_{Aa}}{\\bar{w}} = \\frac{p(p w_{AA} + q w_{Aa})}{\\bar{w}} $$\nThis equation is the allele-frequency recursion under viability selection.\n\nAn equilibrium is a state where the allele frequency does not change between generations, i.e., $p' = p$. The change in allele frequency per generation is $\\Delta p = p' - p$.\n$$ \\Delta p = \\frac{p(p w_{AA} + q w_{Aa})}{\\bar{w}} - p = \\frac{p(p w_{AA} + q w_{Aa}) - p\\bar{w}}{\\bar{w}} $$\nWe can express $\\bar{w}$ in terms of the average fitnesses of the alleles, $\\bar{w}_A = p w_{AA} + q w_{Aa}$ and $\\bar{w}_a = p w_{Aa} + q w_{aa}$, such that $\\bar{w} = p \\bar{w}_A + q \\bar{w}_a$.\nSubstituting this into the expression for $\\Delta p$:\n$$ \\Delta p = \\frac{p \\bar{w}_A - p(p\\bar{w}_A + q\\bar{w}_a)}{\\bar{w}} = \\frac{p \\bar{w}_A (1 - p) - pq\\bar{w}_a}{\\bar{w}} = \\frac{pq(\\bar{w}_A - \\bar{w}_a)}{\\bar{w}} $$\nEquilibrium ($\\Delta p = 0$) occurs if $p=0$, $q=0$ (i.e., $p=1$), or $\\bar{w}_A - \\bar{w}_a = 0$. The first two are trivial equilibria where one allele is fixed. An internal equilibrium, where both alleles are maintained in the population ($p^* \\in (0,1)$), requires the third condition:\n$$ \\bar{w}_A = \\bar{w}_a $$\nThis is the general condition for an internal equilibrium.\n\n1. To derive the equilibrium allele frequency $p^*$ in terms of fitness parameters, we solve this condition:\n$$ p^* w_{AA} + (1-p^*) w_{Aa} = p^* w_{Aa} + (1-p^*) w_{aa} $$\nRearranging the terms to solve for $p^*$:\n$$ p^*(w_{AA} - w_{Aa}) + w_{Aa} = p^*(w_{Aa} - w_{aa}) + w_{aa} $$\n$$ w_{Aa} - w_{aa} = p^*(w_{Aa} - w_{aa} - (w_{AA} - w_{Aa})) $$\n$$ w_{Aa} - w_{aa} = p^*(2w_{Aa} - w_{AA} - w_{aa}) $$\nThis yields the expression for the internal equilibrium allele frequency:\n$$ p^* = \\frac{w_{Aa} - w_{aa}}{2w_{Aa} - w_{AA} - w_{aa}} $$\nThis equilibrium is stable under overdominance, where $w_{Aa} > w_{AA}$ and $w_{Aa} > w_{aa}$. In this case, both the numerator ($w_{Aa} - w_{aa}$) and the denominator ($(w_{Aa} - w_{AA}) + (w_{Aa} - w_{aa})$) are positive, ensuring $p^* > 0$. It can also be shown that $p^* < 1$.\n\n2. We now use the given fitness values: $w_{AA}=0.9$, $w_{Aa}=1$, and $w_{aa}=0.8$. These values correspond to overdominance. We substitute them into the derived formula for $p^*$:\n$$ p^* = \\frac{1 - 0.8}{2(1) - 0.9 - 0.8} = \\frac{0.2}{2 - 1.7} = \\frac{0.2}{0.3} = \\frac{2}{3} $$\nThe internal equilibrium frequency for allele $A$ is $p^* = 2/3$. Consequently, the equilibrium frequency for allele $a$ is $q^* = 1 - p^* = 1/3$.\n\n3. Finally, we evaluate the mean fitness of the population at this equilibrium, $\\bar{w}^*$. At equilibrium, we have the identity $\\bar{w}^* = \\bar{w}_A = \\bar{w}_a$. We can use either expression. Let us use $\\bar{w}_A = p w_{AA} + q w_{Aa}$:\n$$ \\bar{w}^* = p^* w_{AA} + q^* w_{Aa} = \\left(\\frac{2}{3}\\right)(0.9) + \\left(\\frac{1}{3}\\right)(1) $$\n$$ \\bar{w}^* = \\left(\\frac{2}{3}\\right)\\left(\\frac{9}{10}\\right) + \\frac{1}{3} = \\frac{18}{30} + \\frac{1}{3} = \\frac{3}{5} + \\frac{1}{3} $$\nTo sum these fractions, we find a common denominator of $15$:\n$$ \\bar{w}^* = \\frac{9}{15} + \\frac{5}{15} = \\frac{14}{15} $$\nThis is the mean fitness at the stable internal equilibrium, expressed as an exact rational number.\nAs a check, we can compute $\\bar{w}^*$ using the full expression:\n$$ \\bar{w}^* = (p^*)^2 w_{AA} + 2p^*q^* w_{Aa} + (q^*)^2 w_{aa} = \\left(\\frac{2}{3}\\right)^2(0.9) + 2\\left(\\frac{2}{3}\\right)\\left(\\frac{1}{3}\\right)(1) + \\left(\\frac{1}{3}\\right)^2(0.8) $$\n$$ \\bar{w}^* = \\left(\\frac{4}{9}\\right)\\left(\\frac{9}{10}\\right) + \\frac{4}{9} + \\left(\\frac{1}{9}\\right)\\left(\\frac{8}{10}\\right) = \\frac{36}{90} + \\frac{40}{90} + \\frac{8}{90} = \\frac{84}{90} = \\frac{14}{15} $$\nThe result is consistent.", "answer": "$$\\boxed{\\frac{14}{15}}$$", "id": "2832267"}, {"introduction": "The previous exercises involved calculating evolutionary dynamics from known parameters. This advanced practice problem [@problem_id:2832272] reverses the task: here, you will infer the underlying selection coefficient, $s$, from a time series of allele frequency data. By implementing a Hidden Markov Model (HMM) based on a diffusion approximation of the Wright-Fisher model, you will gain hands-on experience with modern statistical methods in population genetics, including both maximum likelihood and Bayesian inference. This exercise bridges the gap between abstract theoretical models and the practical analysis of empirical or simulated evolutionary data.", "problem": "You are given a time series of allele counts sampled at discrete times from a diploid population evolving under a Wright–Fisher diffusion with genic selection. Let the unobserved population allele frequency process be a continuous-time diffusion on the unit interval with infinitesimal drift and infinitesimal variance defined by the Wright–Fisher diffusion with genic selection. The selection coefficient is a constant denoted by $s$. Assume an effective population size $N_e$ that is constant over time. Observations are independent binomial draws from the latent allele frequency at each specified sampling time. Your task is to formulate the likelihood of the observed time series under this model and infer the selection coefficient $s$ by both maximum likelihood and Bayesian methods.\n\nFundamental base and data-generating assumptions:\n1. Latent process: The allele frequency $X_t \\in (0,1)$ evolves as a diffusion with drift $m(x) = s \\, x (1-x)$ and infinitesimal variance $v(x) = \\dfrac{x(1-x)}{2 N_e}$, where $s$ is constant and $N_e$ is the effective population size.\n2. Observations: At time $t_i$, conditional on $X_{t_i} = x_i$, the observed count $K_i$ of the allele in a sample size $n_i$ is distributed as $K_i \\sim \\mathrm{Binomial}(n_i, x_i)$, independently across sampling times given the latent states.\n3. Prior on the initial frequency: Use a uniform Beta prior $X_{t_0} \\sim \\mathrm{Beta}(\\alpha_0,\\beta_0)$ with $\\alpha_0 = 1$ and $\\beta_0 = 1$.\n4. Prior on selection coefficient for the Bayesian analysis: Use a zero-mean normal prior $s \\sim \\mathcal{N}(0,\\sigma_s^2)$ with $\\sigma_s = 0.05$.\n\nComputational requirements to construct the likelihood:\n1. Discretize the allele frequency state space on an equidistant grid of $M$ points over $(0,1)$ with $M = 401$. Treat the prior over the grid as a probability mass function by multiplying the Beta density by the grid spacing and renormalizing to sum to $1$.\n2. Approximate the diffusion transition over a time increment $\\Delta t$ by an Euler–Maruyama step: a state-dependent Gaussian with mean $x + s \\, x (1-x)\\, \\Delta t$ and variance $\\dfrac{x(1-x)}{2N_e}\\, \\Delta t$, truncated to $(0,1)$ by renormalization over the grid. Implement this as a row-stochastic transition matrix over the grid by evaluating the Gaussian density at grid points, multiplying by the grid spacing, and normalizing each row to sum to $1$.\n3. Use a Hidden Markov Model (HMM) forward algorithm on the grid to compute the marginal likelihood $L(s)$ of the observed time series for a given $s$, integrating over the latent frequency path. Incorporate the binomial observation model exactly and use logarithmic stabilization where necessary. The forward recursion should multiply by the transition matrix between observation times and update with the binomial likelihood at each observation.\n4. For maximum likelihood (ML), evaluate $L(s)$ over a uniform grid of candidate values $s \\in \\{-0.10, -0.095, \\ldots, 0.095, 0.10\\}$ and report the maximizer $\\hat{s}_{\\mathrm{ML}}$.\n5. For the Bayesian summary, compute the discrete posterior over the same $s$ grid proportional to $L(s)\\, \\phi(s; 0, \\sigma_s^2)$, where $\\phi$ is the normal density with variance $\\sigma_s^2 = 0.05^2$, and report the posterior mean $\\mathbb{E}[s \\mid \\text{data}]$ and posterior standard deviation $\\sqrt{\\mathbb{E}[s^2 \\mid \\text{data}] - \\mathbb{E}[s \\mid \\text{data}]^2}$.\n\nTest suite:\nImplement your program to process the following three independent test cases. Each case specifies $(N_e, \\{t_i\\}_{i=0}^{T}, \\{n_i\\}_{i=0}^{T}, \\{k_i\\}_{i=0}^{T})$:\n\n- Case A (general increasing under positive selection):\n  - $N_e = 500$\n  - Times (in generations): $t = \\{\\,0, 5, 10, 15, 20\\,\\}$\n  - Sample sizes: $n = \\{\\,100, 100, 100, 100, 100\\,\\}$\n  - Observed counts: $k = \\{\\,10, 13, 16, 20, 24\\,\\}$\n\n- Case B (approximately neutral drift):\n  - $N_e = 300$\n  - Times (in generations): $t = \\{\\,0, 10, 20, 30\\,\\}$\n  - Sample sizes: $n = \\{\\,80, 80, 80, 80\\,\\}$\n  - Observed counts: $k = \\{\\,32, 29, 33, 30\\,\\}$\n\n- Case C (decreasing under negative selection):\n  - $N_e = 600$\n  - Times (in generations): $t = \\{\\,0, 8, 16, 24, 32\\,\\}$\n  - Sample sizes: $n = \\{\\,120, 120, 120, 120, 120\\,\\}$\n  - Observed counts: $k = \\{\\,72, 68, 55, 44, 30\\,\\}$\n\nAngle units are not applicable. There are no physical units in this problem.\n\nReporting:\n- For each test case, compute and return a list of three floats: $[\\hat{s}_{\\mathrm{ML}}, \\mathbb{E}[s \\mid \\text{data}], \\mathrm{SD}(s \\mid \\text{data})]$.\n- All three floats must be rounded to exactly four decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of these three-element lists, enclosed in square brackets. For example, an output with three cases must look like: $[[a_1,a_2,a_3],[b_1,b_2,b_3],[c_1,c_2,c_3]]$, where each $a_j$, $b_j$, and $c_j$ are floats rounded to four decimal places.", "solution": "The problem statement is scrutinized for validity before any attempt at a solution.\n\n### Step 1: Extract Givens\n\n- **Latent Process Model**: The unobserved population allele frequency, $X_t \\in (0,1)$, evolves according to a Wright-Fisher diffusion process with genic selection. The infinitesimal drift is $m(x) = s \\, x (1-x)$ and the infinitesimal variance is $v(x) = \\dfrac{x(1-x)}{2 N_e}$. The selection coefficient $s$ and effective population size $N_e$ are constant.\n- **Observation Model**: At sampling times $t_i$, the observed allele count $K_i$ from a sample of size $n_i$ is a random variable following a binomial distribution, $K_i \\sim \\mathrm{Binomial}(n_i, x_i)$, conditional on the latent frequency $X_{t_i} = x_i$. Observations are independent given the latent frequencies.\n- **Priors**:\n    - The prior on the initial allele frequency is uniform: $X_{t_0} \\sim \\mathrm{Beta}(\\alpha_0=1, \\beta_0=1)$.\n    - The prior on the selection coefficient for Bayesian analysis is a normal distribution: $s \\sim \\mathcal{N}(0, \\sigma_s^2)$ with $\\sigma_s = 0.05$.\n- **Computational Specifications**:\n    - **State Space Discretization**: The allele frequency space $(0,1)$ is discretized into an equidistant grid of $M=401$ points. The prior distribution is discretized by evaluating the Beta density, multiplying by the grid spacing, and renormalizing.\n    - **Parameter Space Discretization**: The selection coefficient $s$ is evaluated on a grid: $s \\in \\{-0.10, -0.095, \\ldots, 0.095, 0.10\\}$.\n    - **Transition Approximation**: The diffusion process over a time interval $\\Delta t$ is approximated by an Euler-Maruyama step, resulting in a Gaussian transition density with mean $x + s \\, x (1-x)\\, \\Delta t$ and variance $\\dfrac{x(1-x)}{2N_e}\\, \\Delta t$. This is used to construct a row-stochastic transition matrix on the frequency grid.\n    - **Likelihood Calculation**: A Hidden Markov Model (HMM) forward algorithm is required to compute the marginal likelihood $L(s)$ by integrating over the latent frequency path.\n- **Inference Tasks**:\n    1.  Find the Maximum Likelihood Estimate (MLE) $\\hat{s}_{\\mathrm{ML}}$ by maximizing $L(s)$ over the specified grid for $s$.\n    2.  Compute the Bayesian posterior mean $\\mathbb{E}[s \\mid \\text{data}]$ and posterior standard deviation $\\mathrm{SD}(s \\mid \\text{data})$ using the specified prior for $s$.\n- **Test Data**: Three distinct data sets (Case A, B, C) are provided, each with specified $N_e$, sampling times $\\{t_i\\}$, sample sizes $\\{n_i\\}$, and observed counts $\\{k_i\\}$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is fundamentally sound. The Wright-Fisher model is a canonical model in population genetics. The use of a diffusion approximation, binomial sampling, and HMM-based inference are standard, well-established methods for analyzing genetic time series data.\n- **Well-Posed**: The problem is well-posed. It clearly defines the model, all necessary parameters, data, and the computational procedure to be followed. This structured approach ensures that a unique and meaningful solution exists within the defined framework.\n- **Objective**: The problem statement is expressed in precise, objective mathematical and statistical language, free of any subjectivity or ambiguity.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, formally specified, complete, and internally consistent.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. We proceed to formulate and implement the solution.\n\n### Methodological Formulation\n\nThe problem requires inferring the selection coefficient $s$ from a time series of allele counts. This is a classic state-space model problem, for which we employ a Hidden Markov Model (HMM) framework.\n\n1.  **Model Components**:\n    -   **Latent States**: The continuous latent allele frequency $X_t$ is discretized onto an equidistant grid of $M=401$ points, $\\{x_j\\}_{j=0}^{M-1}$, where $x_j \\in [0,1]$. The state of the HMM at time $t_i$ is the grid point corresponding to the allele frequency.\n    -   **Observations**: The observed data at time $t_i$ are the allele counts $k_i$ from a sample of size $n_i$.\n    -   **Initial State Distribution**: The prior on the initial frequency $X_{t_0}$ is $\\mathrm{Beta}(1,1)$, which is a uniform distribution on $[0,1]$. When discretized on our grid, this yields a uniform probability mass function (PMF), $P(X_{t_0}=x_j) = 1/M$ for all $j=0, \\dots, M-1$.\n    -   **Emission Probabilities**: The probability of observing $k_i$ counts given a latent frequency $x_j$ is given by the Binomial PMF: $P(\\text{data}_i | X_{t_i}=x_j) = \\binom{n_i}{k_i} x_j^{k_i} (1-x_j)^{n_i-k_i}$.\n    -   **Transition Probabilities**: The transition probability between latent states from time $t_{i-1}$ to $t_i$ is governed by the Wright-Fisher diffusion. We approximate this using an Euler-Maruyama scheme over the time interval $\\Delta t = t_i - t_{i-1}$. For a state $x_j$, the frequency at the next time step is approximately normal with mean $\\mu_j = x_j + s x_j(1-x_j)\\Delta t$ and variance $\\sigma_j^2 = \\frac{x_j(1-x_j)}{2N_e}\\Delta t$. The transition probability from state $x_j$ to $x_k$, denoted $T_{jk}$, is found by evaluating the PDF of $\\mathcal{N}(\\mu_j, \\sigma_j^2)$ at $x_k$ and normalizing the resulting vector for each $j$ so that $\\sum_k T_{jk} = 1$. The endpoints $x=0$ and $x=1$ are absorbing boundaries, so $T_{00}=1$ and $T_{M-1,M-1}=1$.\n\n2.  **Likelihood Calculation via Forward Algorithm**:\n    The marginal likelihood $L(s) = P(\\text{data}_0, \\dots, \\text{data}_T | s)$ is computed using the forward algorithm. We define the forward variable $\\alpha_i(j) = P(\\text{data}_0, \\dots, \\text{data}_i, X_{t_i}=x_j | s)$. To maintain numerical stability, all computations are performed in logarithmic space. Let $\\ell_i(j) = \\log \\alpha_i(j)$.\n\n    -   **Initialization ($i=0$)**: The initial log-forward variable is the sum of the log-prior and the log-emission probability for the first observation:\n        $$ \\log P(X_{t_0}=x_j) + \\log P(\\text{data}_0 | X_{t_0}=x_j) $$\n        The initial contribution to the total log-likelihood is the log-sum-exp over all states $j$. The forward variable is then normalized by subtracting this value. Let $\\ell'_0(j)$ be this normalized log-forward variable.\n\n    -   **Recursion ($i > 0$)**: The recursion consists of two steps:\n        1.  **Prediction**: The log-forward variable is propagated from time $t_{i-1}$ to $t_i$ using the log-transition matrix $\\log T_{jk}$:\n            $$ \\hat{\\ell}_i(k) = \\log\\left(\\sum_{j=0}^{M-1} \\exp(\\ell'_{i-1}(j)) T_{jk}\\right) = \\underset{j}{\\mathrm{logsumexp}}(\\ell'_{i-1}(j) + \\log T_{jk}) $$\n        2.  **Update**: The propagated variable is updated with the log-emission probability for the observation at time $t_i$:\n            $$ \\ell_i(j) = \\hat{\\ell}_i(j) + \\log P(\\text{data}_i | X_{t_i}=x_j) $$\n        The log-likelihood contribution at step $i$ is $\\underset{j}{\\mathrm{logsumexp}}(\\ell_i(j))$. The total log-likelihood $\\log L(s)$ is the sum of these contributions over all time points. The forward variable is again normalized for the next step.\n\n3.  **Parameter Inference**:\n    This entire procedure is repeated for each candidate value of $s$ in the specified grid, yielding a vector of log-likelihoods, $\\{\\log L(s_k)\\}$.\n\n    -   **Maximum Likelihood**: The MLE is the value of $s$ that maximizes the likelihood:\n        $$ \\hat{s}_{\\mathrm{ML}} = \\arg\\max_{s} \\log L(s) $$\n    -   **Bayesian Inference**: The posterior distribution for $s$ on the discrete grid is calculated using Bayes' theorem:\n        $$ P(s_k | \\text{data}) \\propto L(s_k) P(s_k) $$\n        where $P(s_k)$ is the prior probability density of $s_k \\sim \\mathcal{N}(0, \\sigma_s^2)$. In log space: $\\log P(s_k | \\text{data}) \\propto \\log L(s_k) + \\log P(s_k)$. After computing the unnormalized log-posterior, we normalize it to a valid PMF, $\\{p_k\\}$, by exponentiating and dividing by the sum. The posterior mean and standard deviation are then computed:\n        $$ \\mathbb{E}[s | \\text{data}] = \\sum_k s_k p_k $$\n        $$ \\mathrm{SD}(s | \\text{data}) = \\sqrt{\\left(\\sum_k s_k^2 p_k\\right) - \\left(\\mathbb{E}[s | \\text{data}]\\right)^2} $$\n\nThe implementation will follow this logic precisely, creating a function to perform this analysis for each test case and formatting the final output as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, binom\nfrom scipy.special import logsumexp\n\ndef compute_inference(case):\n    \"\"\"\n    Computes ML and Bayesian estimates for the selection coefficient s for a single case.\n    \"\"\"\n    Ne, times, n_samples, k_counts = case\n    \n    # Define computational parameters\n    M = 401\n    x_grid = np.linspace(0.0, 1.0, M)\n    s_grid = np.linspace(-0.1, 0.1, 41)\n    sigma_s = 0.05\n    \n    log_likelihoods = np.full_like(s_grid, -np.inf)\n\n    # Prior for initial state X_t0 ~ Beta(1,1) -> uniform on [0,1].\n    # Discretized on grid. The problem specifies \"multiplying the Beta density \n    # by the grid spacing and renormalizing\". For a uniform density, this\n    # results in a uniform PMF.\n    log_prior_x0 = np.log(np.full(M, 1.0/M))\n\n    for s_idx, s in enumerate(s_grid):\n        \n        # HMM Forward Algorithm for a given s\n        current_log_L = 0.0\n        \n        # 1. Initialization at t=0\n        log_emission_0 = binom.logpmf(k_counts[0], n_samples[0], x_grid)\n        log_alpha_unnorm_0 = log_prior_x0 + log_emission_0\n        \n        log_L_0 = logsumexp(log_alpha_unnorm_0)\n        current_log_L = log_L_0\n        log_fwd = log_alpha_unnorm_0 - log_L_0\n        \n        # 2. Recursion for subsequent time points\n        for i in range(1, len(times)):\n            dt = float(times[i] - times[i-1])\n            if dt <= 0:\n                continue\n\n            # a. Construct transition matrix T for dt\n            # Using Euler-Maruyama approximation for the Wright-Fisher diffusion\n            T = np.zeros((M, M))\n            mu = x_grid + s * x_grid * (1.0 - x_grid) * dt\n            var = x_grid * (1.0 - x_grid) / (2.0 * Ne) * dt\n            \n            # Boundary conditions: absorbing states at x=0 and x=1\n            T[0, 0] = 1.0\n            T[M-1, M-1] = 1.0\n            \n            # Interior points\n            # Use a small positive constant to prevent sqrt(0)\n            std_dev = np.sqrt(np.maximum(var, 1e-300))\n            \n            for j in range(1, M - 1):\n                if std_dev[j] < 1e-100: # Variance is effectively zero, delta-function transition\n                    closest_idx = np.argmin(np.abs(x_grid - mu[j]))\n                    T[j, closest_idx] = 1.0\n                else:\n                    # Evaluate Gaussian PDF on the grid and normalize the row\n                    row_pdf = norm.pdf(x_grid, loc=mu[j], scale=std_dev[j])\n                    row_sum = np.sum(row_pdf)\n                    if row_sum > 0:\n                        T[j, :] = row_pdf / row_sum\n                    else: # Can happen if mu is far from grid and var is tiny\n                        closest_idx = np.argmin(np.abs(x_grid - mu[j]))\n                        T[j, closest_idx] = 1.0\n            \n            # b. Propagate forward variable in log space\n            with np.errstate(divide='ignore'):\n                log_T = np.log(T) # log(T_jk)\n            \n            # log_pred_k = logsumexp_j (log_fwd_j + log_T_jk)\n            # Vectorized: logsumexp along axis=0 of (M,1) + (M,M)\n            log_pred = logsumexp(log_fwd[:, np.newaxis] + log_T, axis=0)\n\n            # If all paths have zero probability, the likelihood is zero.\n            if np.all(np.isneginf(log_pred)):\n                current_log_L = -np.inf\n                break\n\n            # c. Update with new observation\n            log_emission_i = binom.logpmf(k_counts[i], n_samples[i], x_grid)\n            log_alpha_unnorm_i = log_pred + log_emission_i\n            \n            # d. Update total likelihood and normalize forward variable\n            log_L_i = logsumexp(log_alpha_unnorm_i)\n            current_log_L += log_L_i\n            log_fwd = log_alpha_unnorm_i - log_L_i\n        \n        # Store computed log-likelihood for this s value\n        log_likelihoods[s_idx] = current_log_L\n\n    # 3. Perform Inference from the computed grid of log-likelihoods\n    \n    # Maximum Likelihood Estimate\n    s_ml = s_grid[np.argmax(log_likelihoods)]\n    \n    # Bayesian Inference\n    log_prior_s = norm.logpdf(s_grid, loc=0.0, scale=sigma_s)\n    log_posterior = log_likelihoods + log_prior_s\n    \n    # Normalize posterior\n    log_posterior_norm = log_posterior - logsumexp(log_posterior)\n    posterior = np.exp(log_posterior_norm)\n    \n    # Compute posterior mean and standard deviation\n    post_mean = np.sum(s_grid * posterior)\n    post_mean_sq = np.sum(s_grid**2 * posterior)\n    post_var = post_mean_sq - post_mean**2\n    post_std = np.sqrt(max(0, post_var)) # Ensure variance is non-negative\n\n    return [round(s_ml, 4), round(post_mean, 4), round(post_std, 4)]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run inference, and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (general increasing under positive selection)\n        (500, [0, 5, 10, 15, 20], [100, 100, 100, 100, 100], [10, 13, 16, 20, 24]),\n        # Case B (approximately neutral drift)\n        (300, [0, 10, 20, 30], [80, 80, 80, 80], [32, 29, 33, 30]),\n        # Case C (decreasing under negative selection)\n        (600, [0, 8, 16, 24, 32], [120, 120, 120, 120, 120], [72, 68, 55, 44, 30])\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_inference(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2832272"}]}