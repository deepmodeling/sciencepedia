{"hands_on_practices": [{"introduction": "Mitochondrial DNA ($mtDNA$) copy number is a critical indicator of cellular metabolic health and is implicated in a wide range of human diseases. This practice moves beyond relative quantification to the rigorous estimation of absolute copy number per cell, a non-trivial task in a heterogeneous cell population [@problem_id:2834509]. By integrating quantitative PCR data with cell-cycle analysis, you will develop a foundational skill in quantitative molecular biology: the ability to normalize complex datasets to derive meaningful, absolute biological metrics.", "problem": "A single DNA extract from a mixed population of mammalian cells is quantified by quantitative Polymerase Chain Reaction (qPCR) using two assays: one targets a mitochondrial DNA (mtDNA) locus and the other targets a single-copy nuclear locus present once per haploid genome. The same extract and the same input volume are used for both qPCR assays so that mtDNA and nuclear DNA measurements correspond to the same number of cells. Absolute quantification for each assay is performed using a standard curve that relates the quantification cycle number, $C_t$, to the base-$10$ logarithm of the initial template copy number, $\\log_{10}(N_0)$, through a linear model $C_t = m \\log_{10}(N_0) + b$. The standard curves (from independent calibrations) for the two assays are:\n- For the mtDNA assay: slope $m_{\\mathrm{mt}} = -3.35$, intercept $b_{\\mathrm{mt}} = 39.10$.\n- For the nuclear assay: slope $m_{\\mathrm{nuc}} = -3.40$, intercept $b_{\\mathrm{nuc}} = 37.50$.\n\nIn the sample of interest, the measured $C_t$ values are:\n- mtDNA assay: $C_{t,\\mathrm{mt}} = 15.20$.\n- nuclear assay: $C_{t,\\mathrm{nuc}} = 22.80$.\n\nFlow cytometry of the same cell population indicates a mixture of ploidies and cell-cycle stages with the following fractions:\n- Fraction of diploid cells $= 0.80$, fraction of tetraploid cells $= 0.20$.\n- Within diploid cells: fraction in G1 $= 0.55$, fraction in S $= 0.30$, fraction in G2/M $= 0.15$.\n- Within tetraploid cells: fraction in G1 $= 0.45$, fraction in S $= 0.35$, fraction in G2/M $= 0.20$.\n\nAssume the following well-tested facts: (i) the qPCR amplification model underlying the standard curve applies, (ii) the nuclear target is present once per haploid genome, so nuclear qPCR copies measure haploid genome equivalents, and (iii) nuclear DNA content is $2C$ in G1 and $4C$ in G2/M for diploid cells, with S-phase cells having, on average, the midpoint DNA content $3C$ due to approximately uniform replication progression across S-phase; correspondingly, tetraploid cells have $4C$ in G1, $8C$ in G2/M, and $6C$ on average in S-phase.\n\nUsing only these principles, compute the absolute mtDNA copy number per cell in this population by appropriately adjusting for the mixture of ploidy and cell-cycle stages. Round your final answer to four significant figures and express it as copies per cell (dimensionless).", "solution": "The problem requires the determination of the average mitochondrial DNA (mtDNA) copy number per cell from a mixed population of cells, using data from quantitative Polymerase Chain Reaction (qPCR) and flow cytometry.\n\nFirst, we must validate the problem statement.\nThe givens are:\n1.  The qPCR quantification model: $C_t = m \\log_{10}(N_0) + b$, where $C_t$ is the quantification cycle, $N_0$ is the initial template copy number, $m$ is the slope, and $b$ is the intercept.\n2.  Parameters for the mtDNA assay: $m_{\\mathrm{mt}} = -3.35$, $b_{\\mathrm{mt}} = 39.10$.\n3.  Parameters for the nuclear DNA (nuc) assay: $m_{\\mathrm{nuc}} = -3.40$, $b_{\\mathrm{nuc}} = 37.50$.\n4.  Measured $C_t$ values for the sample: $C_{t,\\mathrm{mt}} = 15.20$, $C_{t,\\mathrm{nuc}} = 22.80$.\n5.  Cell population structure:\n    -   Fraction of diploid cells, $f_{d} = 0.80$.\n    -   Fraction of tetraploid cells, $f_{t} = 0.20$.\n6.  Cell cycle distribution within subpopulations:\n    -   Diploid fractions: G1 ($f_{d,G1}=0.55$), S ($f_{d,S}=0.30$), G2/M ($f_{d,G2/M}=0.15$).\n    -   Tetraploid fractions: G1 ($f_{t,G1}=0.45$), S ($f_{t,S}=0.35$), G2/M ($f_{t,G2/M}=0.20$).\n7.  Assumptions on DNA content, where $C$ is the DNA content of a haploid genome:\n    -   Diploid cells: G1=$2C$, S (average)=$3C$, G2/M=$4C$. These correspond to $2$, $3$, and $4$ haploid genome equivalents.\n    -   Tetraploid cells: G1=$4C$, S (average)=$6C$, G2/M=$8C$. These correspond to $4$, $6$, and $8$ haploid genome equivalents.\n\nThe problem is scientifically sound, as it is based on established principles of molecular biology (qPCR) and cell biology (cell cycle, ploidy). The provided data are internally consistent and numerically plausible for a real biological experiment. The problem is well-posed, objective, and contains all necessary information for a unique solution. Therefore, the problem is deemed valid and a solution will be constructed.\n\nThe objective is to find the average mtDNA copy number per cell, which we denote $\\langle N_{\\mathrm{mt/cell}} \\rangle$. This is the total number of mtDNA copies, $N_{0,\\mathrm{mt}}$, divided by the total number of cells, $N_{\\mathrm{cells}}$.\n$$\n\\langle N_{\\mathrm{mt/cell}} \\rangle = \\frac{N_{0,\\mathrm{mt}}}{N_{\\mathrm{cells}}}\n$$\nThe qPCR measurements provide $N_{0,\\mathrm{mt}}$ (total mtDNA copies) and $N_{0,\\mathrm{nuc}}$ (total haploid genome equivalents). The number of cells, $N_{\\mathrm{cells}}$, is not directly measured. However, the total number of haploid genome equivalents is the product of the average number of haploid genomes per cell, $\\langle G \\rangle$, and the total number of cells.\n$$\nN_{0,\\mathrm{nuc}} = \\langle G \\rangle \\cdot N_{\\mathrm{cells}}\n$$\nWe can rearrange this to find $N_{\\mathrm{cells}} = N_{0,\\mathrm{nuc}} / \\langle G \\rangle$. Substituting this into the first equation:\n$$\n\\langle N_{\\mathrm{mt/cell}} \\rangle = \\frac{N_{0,\\mathrm{mt}}}{N_{0,\\mathrm{nuc}} / \\langle G \\rangle} = \\left(\\frac{N_{0,\\mathrm{mt}}}{N_{0,\\mathrm{nuc}}}\\right) \\cdot \\langle G \\rangle\n$$\nThe solution path is therefore: 1) calculate the ratio of initial copy numbers from qPCR data, 2) calculate the average number of haploid genomes per cell from cell population data, and 3) multiply these two results.\n\nStep 1: Calculate the ratio of initial template copy numbers.\nFrom the qPCR model, we first invert the equation to solve for $N_0$:\n$$\nN_0 = 10^{\\frac{C_t - b}{m}}\n$$\nApplying this to the mtDNA and nuclear assays:\n$$\nN_{0,\\mathrm{mt}} = 10^{\\frac{C_{t,\\mathrm{mt}} - b_{\\mathrm{mt}}}{m_{\\mathrm{mt}}}} = 10^{\\frac{15.20 - 39.10}{-3.35}} = 10^{\\frac{-23.90}{-3.35}}\n$$\n$$\nN_{0,\\mathrm{nuc}} = 10^{\\frac{C_{t,\\mathrm{nuc}} - b_{\\mathrm{nuc}}}{m_{\\mathrm{nuc}}}} = 10^{\\frac{22.80 - 37.50}{-3.40}} = 10^{\\frac{-14.70}{-3.40}}\n$$\nThe ratio of mtDNA copies to nuclear genome equivalents is:\n$$\n\\frac{N_{0,\\mathrm{mt}}}{N_{0,\\mathrm{nuc}}} = \\frac{10^{\\frac{23.90}{3.35}}}{10^{\\frac{14.70}{3.40}}} = 10^{\\left(\\frac{23.90}{3.35} - \\frac{14.70}{3.40}\\right)}\n$$\n\nStep 2: Calculate the average number of haploid genomes per cell, $\\langle G \\rangle$.\nThis is a weighted average based on the population composition.\nFirst, we find the average genome content for each subpopulation.\nFor diploid cells (genome counts $g_d = \\{2, 3, 4\\}$):\n$$\n\\langle G_d \\rangle = f_{d,G1} \\cdot 2 + f_{d,S} \\cdot 3 + f_{d,G2/M} \\cdot 4 = (0.55)(2) + (0.30)(3) + (0.15)(4) = 1.10 + 0.90 + 0.60 = 2.60\n$$\nFor tetraploid cells (genome counts $g_t = \\{4, 6, 8\\}$):\n$$\n\\langle G_t \\rangle = f_{t,G1} \\cdot 4 + f_{t,S} \\cdot 6 + f_{t,G2/M} \\cdot 8 = (0.45)(4) + (0.35)(6) + (0.20)(8) = 1.80 + 2.10 + 1.60 = 5.50\n$$\nThe overall average number of haploid genomes per cell is the weighted average of these two values:\n$$\n\\langle G \\rangle = f_d \\cdot \\langle G_d \\rangle + f_t \\cdot \\langle G_t \\rangle = (0.80)(2.60) + (0.20)(5.50) = 2.08 + 1.10 = 3.18\n$$\n\nStep 3: Combine the results to find the final answer.\n$$\n\\langle N_{\\mathrm{mt/cell}} \\rangle = 10^{\\left(\\frac{23.90}{3.35} - \\frac{14.70}{3.40}\\right)} \\cdot \\langle G \\rangle\n$$\nWe compute the value of the exponent:\n$$\n\\frac{23.90}{3.35} - \\frac{14.70}{3.40} \\approx 7.134328 - 4.323529 = 2.810799\n$$\nNow we compute the final value:\n$$\n\\langle N_{\\mathrm{mt/cell}} \\rangle \\approx 10^{2.810799} \\cdot 3.18 \\approx 646.853 \\cdot 3.18 \\approx 2057.00\n$$\nRounding to four significant figures as required by the problem statement, the final answer is $2057$.", "answer": "$$\\boxed{2057}$$", "id": "2834509"}, {"introduction": "Many chloroplast genomes are not static molecules but exist as a dynamic equilibrium of structural isomers due to recombination across large inverted repeats. This exercise challenges you to model this process using the fundamental principles of reversible reaction kinetics to predict the steady-state ratio of these isomers [@problem_id:2834500]. Crucially, you will then connect this molecular stoichiometry to a quantitative prediction of read coverage in a next-generation sequencing experiment, demonstrating how simple physical models can yield powerful insights into genomic data.", "problem": "A land plant chloroplast genome has the canonical quadripartite structure with a Large Single Copy (LSC) region of length $L_{\\mathrm{LSC}} = 86\\,\\text{kb}$, a Small Single Copy (SSC) region of length $L_{\\mathrm{SSC}} = 18\\,\\text{kb}$, and two identical Inverted Repeats (IRs) of length $L_{\\mathrm{IR}} = 26\\,\\text{kb}$ each. Homologous recombination between the IRs produces two alternative whole-genome isomers, denoted isomer $A$ and isomer $B$, that differ by the orientation of the SSC relative to the LSC. Assume the isomerization dynamics can be modeled as a reversible two-state process $A \\rightleftharpoons B$ with first-order rate constants $k_{f}$ for the $A \\to B$ direction and $k_{r}$ for the $B \\to A$ direction. In this sample, the measured values are $k_{f} = 2.0 \\times 10^{-4}\\,\\text{s}^{-1}$ and $k_{r} = 3.0 \\times 10^{-4}\\,\\text{s}^{-1}$.\n\nA whole-chloroplast shotgun sequencing library is prepared with $N = 3.0 \\times 10^{7}$ reads of length $r = 150\\,\\text{bp}$ from randomly fragmented molecules, with no sequence bias and uniform sampling across molecules. Reads are mapped to a reference that contains both isomer $A$ and isomer $B$ configurations. Consider two isomer-specific junctions at the LSC–IR boundary: junction $J_{A}$ exists only in isomer $A$, and the alternative junction $J_{B}$ exists only in isomer $B$. Mapping counts for $J_{A}$ and $J_{B}$ are computed using only reads that span the unique LSC–IR boundary by at least one base on each side, and that do not fully reside within an IR.\n\nStarting solely from (i) reversible two-state kinetics at steady state and (ii) uniform random sampling of bases in shotgun sequencing, derive the expected steady-state stoichiometry (fraction) $p$ of isomer $A$ and show how this stoichiometry propagates to the expected coverage depth at the isomer-specific junctions. Then, compute the expected ratio $R$ of coverage depth at $J_{A}$ to that at $J_{B}$, and report $R$ rounded to four significant figures. The final answer should be the single number $R$ with no units.", "solution": "The problem requires the calculation of the expected ratio of sequencing coverage depths for two alternative genome isomers at steady state. The problem is well-posed and scientifically grounded. We will proceed by first determining the steady-state stoichiometry of the isomers from the given kinetics, and then establishing the relationship between this stoichiometry and the expected sequencing coverage.\n\nFirst, we analyze the kinetics of the isomerization process, which is modeled as a first-order reversible reaction:\n$$ A \\rightleftharpoons B $$\nThe rate of change in the concentration of isomer $A$, denoted $[A]$, is given by the differential equation:\n$$ \\frac{d[A]}{dt} = -k_{f}[A] + k_{r}[B] $$\nwhere $[B]$ is the concentration of isomer $B$, $k_{f}$ is the forward rate constant for the $A \\to B$ conversion, and $k_{r}$ is the reverse rate constant for the $B \\to A$ conversion.\n\nAt steady state, the net rate of change is zero, so $\\frac{d[A]}{dt} = 0$. This leads to the steady-state condition:\n$$ k_{f}[A]_{ss} = k_{r}[B]_{ss} $$\nwhere $[A]_{ss}$ and $[B]_{ss}$ are the steady-state concentrations.\n\nThe problem asks for the stoichiometry, which is the fraction of each isomer. Let $p$ be the fraction of isomer $A$ and $1-p$ be the fraction of isomer $B$. These fractions are defined as:\n$$ p = \\frac{[A]_{ss}}{[A]_{ss} + [B]_{ss}} $$\n$$ 1-p = \\frac{[B]_{ss}}{[A]_{ss} + [B]_{ss}} $$\nSubstituting these into the steady-state condition yields:\n$$ k_{f}p = k_{r}(1-p) $$\nWe solve this algebraic equation for $p$:\n$$ k_{f}p = k_{r} - k_{r}p $$\n$$ p(k_{f} + k_{r}) = k_{r} $$\n$$ p = \\frac{k_{r}}{k_{f} + k_{r}} $$\nThis expression gives the expected steady-state fraction of isomer $A$.\n\nNext, we must relate this molecular stoichiometry to the expected coverage depth from shotgun sequencing. The problem states that sequencing involves uniform random sampling of bases with no sequence bias. This implies that the number of sequencing reads derived from a particular type of molecule is directly proportional to the abundance of that molecule in the initial sample.\n\nLet $N$ be the total number of reads. The expected number of reads originating from isomer $A$ molecules, $N_A$, and isomer $B$ molecules, $N_B$, are:\n$$ N_A = N \\times p $$\n$$ N_B = N \\times (1-p) $$\n\nThe coverage at a junction is defined by the number of reads that span it. Let's consider the properties of a read spanning a junction. For a read of length $r$ to span a specific point-like junction on a circular genome of length $L_{total}$, its starting position must fall within a specific window of size $r-1$. The total number of possible start positions is $L_{total}$. The chloroplast genome has a quadripartite structure with two identical Inverted Repeats (IRs). This results in two LSC-IR junctions. We assume the isomer-specific junctions $J_A$ and $J_B$ correspond to the two LSC-IR boundaries in each respective isomer.\n\nThe probability that a randomly chosen read from an isomer $A$ molecule spans one of its two $J_A$ junctions is $\\frac{2(r-1)}{L_{total}}$, where $L_{total}$ is the total length of the genome. The expected total number of reads spanning any $J_A$ junction, which we interpret as the coverage depth $C_{J_A}$, is:\n$$ C_{J_A} = N_A \\times \\frac{2(r-1)}{L_{total}} = (N \\times p) \\left( \\frac{2(r-1)}{L_{total}} \\right) $$\nSimilarly, the genome of isomer $B$ has the same total length $L_{total}$ and also possesses two isomer-specific junctions of type $J_B$. The structure and length parameters are identical for both isomers. Thus, the expected coverage depth at $J_B$ junctions is:\n$$ C_{J_B} = N_B \\times \\frac{2(r-1)}{L_{total}} = (N \\times (1-p)) \\left( \\frac{2(r-1)}{L_{total}} \\right) $$\nThe problem asks for the ratio $R$ of the coverage depth at $J_A$ to that at $J_B$.\n$$ R = \\frac{C_{J_A}}{C_{J_B}} = \\frac{(N \\times p) \\left( \\frac{2(r-1)}{L_{total}} \\right)}{(N \\times (1-p)) \\left( \\frac{2(r-1)}{L_{total}} \\right)} $$\nThe terms related to sequencing parameters ($N$, $r$) and genome length ($L_{total}$) cancel out, demonstrating that this ratio is independent of them, provided the uniform sampling assumption holds. The ratio simplifies to the ratio of the isomer fractions:\n$$ R = \\frac{p}{1-p} $$\nThis shows how the molecular stoichiometry propagates directly to the ratio of junction-spanning read counts.\n\nNow we can substitute the expression for $p$ into the equation for $R$:\n$$ R = \\frac{\\frac{k_{r}}{k_{f} + k_{r}}}{1 - \\frac{k_{r}}{k_{f} + k_{r}}} = \\frac{\\frac{k_{r}}{k_{f} + k_{r}}}{\\frac{k_{f} + k_{r} - k_{r}}{k_{f} + k_{r}}} = \\frac{\\frac{k_{r}}{k_{f} + k_{r}}}{\\frac{k_{f}}{k_{f} + k_{r}}} = \\frac{k_{r}}{k_{f}} $$\nThe ratio of coverages is simply the ratio of the reverse to the forward rate constants, which is the equilibrium constant for the reaction written as $B \\rightleftharpoons A$.\n\nFinally, we substitute the given numerical values for the rate constants:\n$k_{f} = 2.0 \\times 10^{-4}\\,\\text{s}^{-1}$\n$k_{r} = 3.0 \\times 10^{-4}\\,\\text{s}^{-1}$\n\n$$ R = \\frac{3.0 \\times 10^{-4}}{2.0 \\times 10^{-4}} = \\frac{3.0}{2.0} = 1.5 $$\nThe problem requires the answer to be rounded to four significant figures.\n$$ R = 1.500 $$\nThe extraneous information ($L_{\\mathrm{LSC}}$, $L_{\\mathrm{SSC}}$, $L_{\\mathrm{IR}}$, $N$, $r$) is not required for the calculation of the ratio $R$, but is necessary for ensuring the consistency of the problem setup. For example, the read length $r=150\\,\\text{bp}$ is much smaller than any of the genomic regions, validating the local nature of the spanning read analysis.", "answer": "$$\\boxed{1.500}$$", "id": "2834500"}, {"introduction": "Disentangling the effects of random genetic drift from deterministic natural selection is a central challenge in evolutionary biology. This advanced practice invites you to build a computational tool to do just that for mitochondrial heteroplasmy, using a framework built on the classic Wright–Fisher model [@problem_id:2834548]. By implementing a maximum likelihood estimation procedure and performing a likelihood ratio test, you will gain hands-on experience with the sophisticated statistical methods that population geneticists use to infer evolutionary forces directly from sequence data.", "problem": "You are given cross-tissue mitochondrial heteroplasmy observations from independent cell lineages that diverged from a common progenitor. Assume the following modeling framework grounded in the Wright–Fisher model and diffusion approximation:\n\n- The frequency of a focal mitochondrial haplotype (heteroplasmy) in a lineage is denoted by $p$. The initial heteroplasmy at the shared progenitor is $p_0 \\in (0,1)$.\n- Over $t$ discrete generations in a lineage with effective mitochondrial genome number (effective cell lineage size) $N_e$, Wright–Fisher drift occurs. Under the diffusion approximation, the variance of the allele frequency due to drift after $t$ generations is\n  $$ \\mathrm{Var}(p_t \\mid p_0, N_e, t) \\approx p_t (1 - p_t) \\left(1 - e^{-t / (2 N_e)}\\right). $$\n- Weak directional selection with selection coefficient $s$ acts multiplicatively per generation. In continuous-time approximation, the deterministic mean trajectory follows the logistic differential equation\n  $$ \\frac{dp}{dt} = s\\, p (1 - p), $$\n  whose solution implies the expected mean at time $t$ is\n  $$ \\mu_t = \\mathrm{logistic}\\!\\left(\\mathrm{logit}(p_0) + s t\\right), $$\n  where $\\mathrm{logistic}(x) = \\frac{1}{1 + e^{-x}}$ and $\\mathrm{logit}(p) = \\ln\\!\\left(\\frac{p}{1-p}\\right)$.\n- To marginalize over the unobserved true heteroplasmy at sampling, approximate $p_t$ by a Beta distribution with mean $\\mu_t$ and variance\n  $$ v_t = \\mu_t (1 - \\mu_t) \\left(1 - e^{-t / (2 N_e)}\\right). $$\n  This implies a Beta distribution $\\mathrm{Beta}(\\alpha_t, \\beta_t)$ with\n  $$ \\alpha_t = \\mu_t \\left( \\frac{\\mu_t (1 - \\mu_t)}{v_t} - 1 \\right), \\quad \\beta_t = (1 - \\mu_t) \\left( \\frac{\\mu_t (1 - \\mu_t)}{v_t} - 1 \\right), $$\n  provided $0 < v_t < \\mu_t(1-\\mu_t)$. In the limit $v_t \\to 0$, use the Binomial likelihood with success probability $\\mu_t$.\n- Sequencing measurements observe $k_i$ mutant reads out of $n_i$ total reads in tissue $i$. Conditional on $p_t \\sim \\mathrm{Beta}(\\alpha_t, \\beta_t)$ and read sampling, the marginal count distribution is Beta–Binomial:\n  $$ \\Pr(K = k) = \\binom{n}{k} \\frac{B(k + \\alpha, n - k + \\beta)}{B(\\alpha, \\beta)}, $$\n  where $B(\\cdot,\\cdot)$ is the Beta function. If $v_t = 0$, use the Binomial $\\Pr(K=k) = \\binom{n}{k} \\mu_t^k (1 - \\mu_t)^{n - k}$.\n\nYour task is to write a complete, runnable program that, for each dataset below, performs the following:\n\n1. Fit the Wright–Fisher drift model by maximum likelihood to estimate the effective cell lineage size $N_e$ and the selection coefficient $s$ using the likelihood defined by the product over tissues of Beta–Binomial (or Binomial when $v_t \\to 0$) probabilities. Use $p_0$ as given and treat lineages as independent conditional on $(N_e, s)$.\n2. Compute the log-likelihood under the null model $H_0: s = 0$ by maximizing over $N_e$ only.\n3. Compute the log-likelihood under the alternative model $H_1: s$ free (with $N_e$ free).\n4. Perform a likelihood ratio test (LRT) comparing $H_0$ and $H_1$ with test statistic\n   $$ \\Lambda = 2 \\left(\\ell_1^{\\star} - \\ell_0^{\\star}\\right), $$\n   where $\\ell_0^{\\star}$ and $\\ell_1^{\\star}$ are the maximized log-likelihoods under $H_0$ and $H_1$, respectively. Under standard regularity, report the $p$-value using a chi-square distribution with $1$ degree of freedom.\n5. For numerical stability, enforce $\\mu_t \\in [10^{-9}, 1 - 10^{-9}]$. When computing Beta parameters, if $v_t \\le 10^{-12}$, fall back to Binomial likelihood at $\\mu_t$.\n\nAlgorithmic constraints for estimation:\n\n- Use a grid search over $N_e$ in the set $\\{30, 50, 80, 120, 200, 300, 500, 800, 1200, 2000, 3000, 4000, 5000\\}$.\n- Use a grid search over $s$ in the closed interval $[-0.05, 0.05]$ with step size $0.001$.\n- Ensure that the alternative model’s $s$ grid includes $0$ so that $H_0$ is nested within $H_1$.\n- Report the maximum likelihood estimates as the grid points achieving the maximum log-likelihood.\n\nTest suite (each dataset is a tuple with all required quantities):\n\n- Dataset A (neutral drift, increasing time): $p_0 = 0.5$, times $t = [10, 20, 30, 40]$, depths $n = [300, 300, 300, 300]$, mutant counts $k = [150, 152, 148, 151]$.\n- Dataset B (positive selection signal): $p_0 = 0.6$, times $t = [20, 30, 40]$, depths $n = [400, 400, 400]$, mutant counts $k = [288, 318, 332]$.\n- Dataset C (boundary case with very small drift time, negative selection signal): $p_0 = 0.4$, times $t = [1, 1, 2]$, depths $n = [200, 200, 200]$, mutant counts $k = [80, 79, 77]$.\n- Dataset D (near-boundary frequency, neutral): $p_0 = 0.1$, times $t = [10, 30, 50]$, depths $n = [500, 500, 500]$, mutant counts $k = [55, 44, 79]$.\n\nRequired final output format:\n\n- Your program should produce a single line of output containing the results for all datasets as a comma-separated list enclosed in square brackets, where each dataset’s result is a list of four floats\n  $$[\\hat{N}_e, \\hat{s}, \\Lambda, p\\text{-value}],$$\n  rounded to three decimal places. The overall output should be\n  $$\\big[ [\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot] \\big].$$\n- There are no physical units involved; all quantities are dimensionless. Angles are not used. Express any fractional quantities as decimal floats.\n\nYour program must be complete and runnable as-is, with no user input. It must compute the requested estimates and test statistics using the specified model and grids, and print the results exactly in the specified format.", "solution": "The problem statement has been subjected to rigorous validation and is found to be valid. It constitutes a well-posed problem in statistical population genetics, grounded in the established theoretical framework of the Wright–Fisher model, diffusion approximation, and maximum likelihood inference. The problem is self-contained, scientifically sound, and provides all necessary data and algorithmic constraints to derive a unique solution based on the specified methods.\n\nThe objective is to estimate the effective cell lineage size, $N_e$, and the selection coefficient, $s$, from mitochondrial heteroplasmy data using a maximum likelihood approach. This involves a comparison between a null model of neutral genetic drift ($H_0: s=0$) and an alternative model that includes both drift and selection ($H_1: s \\neq 0$).\n\nThe analytical procedure is structured as follows. For each dataset, we are given an initial heteroplasmy frequency $p_0$, and a set of observations, where each observation $i$ consists of the time in generations $t_i$, the total sequencing depth $n_i$, and the number of mutant reads $k_i$.\n\nFirst, we define the log-likelihood of a single observation $(k_i, n_i)$ at time $t_i$ for a given set of parameters $(N_e, s)$. The total log-likelihood for a dataset is the sum of the log-likelihoods over all its independent observations: $\\mathcal{L}(N_e, s) = \\sum_i \\ell_i(k_i, n_i, t_i \\mid N_e, s, p_0)$.\n\nThe calculation of each term $\\ell_i$ proceeds in these steps:\n\n1.  **Deterministic Mean Frequency**: The expected frequency under selection, $\\mu_{t_i}$, is calculated using the logistic growth model solution:\n    $$ \\mu_{t_i} = \\mathrm{logistic}\\!\\left(\\mathrm{logit}(p_0) + s t_i\\right) = \\frac{1}{1 + e^{-(\\ln(p_0/(1-p_0)) + s t_i)}} $$\n    To ensure numerical stability, $\\mu_{t_i}$ is constrained to the interval $[10^{-9}, 1 - 10^{-9}]$.\n\n2.  **Stochastic Variance**: The variance due to genetic drift, $v_{t_i}$, is approximated by:\n    $$ v_{t_i} = \\mu_{t_i} (1 - \\mu_{t_i}) \\left(1 - e^{-t_i / (2 N_e)}\\right) $$\n\n3.  **Likelihood Model Selection**: The choice between the Beta-Binomial and Binomial models for the read counts depends on the magnitude of the drift-induced variance $v_{t_i}$.\n    - If $v_{t_i} \\le 10^{-12}$, the variance is negligible. This signifies a no-drift limit, and the distribution of reads is modeled by a Binomial distribution, $\\mathrm{Bin}(n_i, \\mu_{t_i})$. The log-likelihood contribution from observation $i$ is:\n      $$ \\ell_i = \\log \\binom{n_i}{k_i} + k_i \\log(\\mu_{t_i}) + (n_i - k_i) \\log(1 - \\mu_{t_i}) $$\n    - If $v_{t_i} > 10^{-12}$, drift is significant. The underlying heteroplasmy $p_{t_i}$ is modeled as a Beta-distributed random variable, $p_{t_i} \\sim \\mathrm{Beta}(\\alpha_{t_i}, \\beta_{t_i})$. The parameters $\\alpha_{t_i}$ and $\\beta_{t_i}$ are derived by matching the mean and variance to $\\mu_{t_i}$ and $v_{t_i}$:\n      $$ \\alpha_{t_i} = \\mu_{t_i} \\left( \\frac{\\mu_{t_i} (1 - \\mu_{t_i})}{v_{t_i}} - 1 \\right), \\quad \\beta_{t_i} = (1 - \\mu_{t_i}) \\left( \\frac{\\mu_{t_i} (1 - \\mu_{t_i})}{v_{t_i}} - 1 \\right) $$\n      A more stable form for computation is derived by substituting the expression for $v_{t_i}$:\n      $$ \\phi_{t_i} = \\left(e^{t_i / (2 N_e)} - 1\\right)^{-1} $$\n      $$ \\alpha_{t_i} = \\mu_{t_i} \\phi_{t_i}, \\quad \\beta_{t_i} = (1-\\mu_{t_i}) \\phi_{t_i} $$\n      The marginal distribution of read counts is then a Beta-Binomial distribution. The log-likelihood contribution is:\n      $$ \\ell_i = \\log \\binom{n_i}{k_i} + \\log B(k_i + \\alpha_{t_i}, n_i - k_i + \\beta_{t_i}) - \\log B(\\alpha_{t_i}, \\beta_{t_i}) $$\n      where $B(x, y)$ is the Beta function. In practice, calculations are performed using the log-gamma function, $\\log \\Gamma(\\cdot)$, since $\\log B(x,y) = \\log\\Gamma(x) + \\log\\Gamma(y) - \\log\\Gamma(x+y)$.\n\n4.  **Maximum Likelihood Estimation**: We perform a grid search to find the parameters that maximize the total log-likelihood $\\mathcal{L}(N_e, s)$.\n    - For the alternative model ($H_1$), we search over the specified grids for $N_e \\in \\{30, 50, \\dots, 5000\\}$ and $s \\in [-0.05, 0.05]$ to find $(\\hat{N}_e, \\hat{s})$ that yield the maximum log-likelihood, $\\ell_1^\\star$.\n    - For the null model ($H_0$), we fix $s=0$ and search only over the grid for $N_e$ to find the $\\hat{N}_{e,0}$ that yields the maximum log-likelihood under this constraint, $\\ell_0^\\star$.\n\n5.  **Likelihood Ratio Test (LRT)**: The evidence for selection is quantified using the LRT. The test statistic $\\Lambda$ is calculated as:\n    $$ \\Lambda = 2 (\\ell_1^\\star - \\ell_0^\\star) $$\n    Under the null hypothesis, $\\Lambda$ is asymptotically distributed as a chi-square distribution with $1$ degree of freedom, corresponding to the one additional free parameter ($s$) in the alternative model. The $p$-value is then computed from this $\\chi^2_1$ distribution.\n\nThis complete procedure is applied to each dataset to obtain the estimates $(\\hat{N}_e, \\hat{s})$ from the alternative model, the LRT statistic $\\Lambda$, and the associated $p$-value.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to process all datasets and print the final results.\n    \"\"\"\n    \n    # Define the specified parameter grids for the search.\n    NE_GRID = np.array([30, 50, 80, 120, 200, 300, 500, 800, 1200, 2000, 3000, 4000, 5000])\n    S_GRID = np.linspace(-0.05, 0.05, 101)  # -0.05 to 0.05 with step 0.001\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A (neutral drift, increasing time)\n        {'p0': 0.5, 't': [10, 20, 30, 40], 'n': [300, 300, 300, 300], 'k': [150, 152, 148, 151]},\n        # Dataset B (positive selection signal)\n        {'p0': 0.6, 't': [20, 30, 40], 'n': [400, 400, 400], 'k': [288, 318, 332]},\n        # Dataset C (boundary case with very small drift time, negative selection signal)\n        {'p0': 0.4, 't': [1, 1, 2], 'n': [200, 200, 200], 'k': [80, 79, 77]},\n        # Dataset D (near-boundary frequency, neutral)\n        {'p0': 0.1, 't': [10, 30, 50], 'n': [500, 500, 500], 'k': [55, 44, 79]},\n    ]\n    \n    # Store results for all datasets\n    all_results = []\n    \n    for case in test_cases:\n        result = process_dataset(case, NE_GRID, S_GRID)\n        all_results.append([f\"{x:.3f}\" for x in result])\n        \n    # Format the final output as specified\n    print(f\"[{','.join(map(str, all_results))}]\".replace(\"'\", \"\"))\n\ndef calculate_log_likelihood(p0, ts, ns, ks, Ne, s):\n    \"\"\"\n    Calculates the total log-likelihood for a dataset given parameters.\n    \"\"\"\n    logit_p0 = np.log(p0 / (1 - p0))\n    total_log_likelihood = 0.0\n    \n    # Numerically stable calculation of log(binom(n,k))\n    log_binom_coeffs = gammaln(np.array(ns) + 1) - gammaln(np.array(ks) + 1) - gammaln(np.array(ns) - np.array(ks) + 1)\n\n    for i in range(len(ts)):\n        t, n, k = ts[i], ns[i], ks[i]\n        \n        # 1. Calculate deterministic mean frequency mu_t\n        mu_t = 1.0 / (1.0 + np.exp(-(logit_p0 + s * t)))\n        mu_t = np.clip(mu_t, 1e-9, 1.0 - 1e-9)\n        \n        # 2. Calculate stochastic variance v_t\n        drift_term = 1.0 - np.exp(-t / (2.0 * Ne))\n        v_t = mu_t * (1.0 - mu_t) * drift_term\n        \n        # 3. Select likelihood model based on variance\n        if v_t = 1e-12:\n            # Binomial likelihood\n            log_likelihood_term = log_binom_coeffs[i] + k * np.log(mu_t) + (n - k) * np.log(1.0 - mu_t)\n        else:\n            # Beta-Binomial likelihood\n            phi = 1.0 / np.expm1(t / (2.0 * Ne))\n            alpha = mu_t * phi\n            beta = (1.0 - mu_t) * phi\n            \n            log_beta_num = gammaln(k + alpha) + gammaln(n - k + beta) - gammaln(n + alpha + beta)\n            log_beta_den = gammaln(alpha) + gammaln(beta) - gammaln(alpha + beta)\n            \n            log_likelihood_term = log_binom_coeffs[i] + log_beta_num - log_beta_den\n            \n        total_log_likelihood += log_likelihood_term\n        \n    return total_log_likelihood\n\ndef process_dataset(case, Ne_grid, s_grid):\n    \"\"\"\n    Performs MLE and LRT for a single dataset.\n    \"\"\"\n    p0, ts, ns, ks = case['p0'], case['t'], case['n'], case['k']\n    \n    # --- Alternative Model (H1) ---\n    max_ll_h1 = -np.inf\n    mle_Ne_h1 = None\n    mle_s_h1 = None\n    \n    for Ne in Ne_grid:\n        for s in s_grid:\n            ll = calculate_log_likelihood(p0, ts, ns, ks, Ne, s)\n            if ll > max_ll_h1:\n                max_ll_h1 = ll\n                mle_Ne_h1 = Ne\n                mle_s_h1 = s\n                \n    # --- Null Model (H0: s=0) ---\n    max_ll_h0 = -np.inf\n    mle_Ne_h0 = None\n    \n    for Ne in Ne_grid:\n        ll = calculate_log_likelihood(p0, ts, ns, ks, Ne, s=0.0)\n        if ll > max_ll_h0:\n            max_ll_h0 = ll\n            mle_Ne_h0 = Ne\n            \n    # --- Likelihood Ratio Test ---\n    # Ensure non-negative LRT statistic due to grid search and precision\n    lrt_statistic = max(0.0, 2 * (max_ll_h1 - max_ll_h0))\n    p_value = chi2.sf(lrt_statistic, df=1)\n    \n    return [float(mle_Ne_h1), float(mle_s_h1), lrt_statistic, p_value]\n\nsolve()\n```", "id": "2834548"}]}