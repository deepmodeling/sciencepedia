{"hands_on_practices": [{"introduction": "The success of any genomics project hinges on achieving sufficient sequencing coverage across the target genome. This foundational exercise allows you to derive the relationship between sequencing parameters and expected coverage from first principles. By modeling read placement as a random sampling process, you will calculate the expected sequencing depth and the probability of leaving a portion of the genome entirely unsequenced, building an intuition for the celebrated Lander-Waterman theory. [@problem_id:2479969]", "problem": "In a whole-genome shotgun experiment using Next-Generation Sequencing (NGS), consider a circular bacterial genome of length $G = 4.6 \\times 10^{6}$ base pairs (bp). Single-end reads of fixed length $L = 150$ bp are sequenced, and $N = 153{,}334$ read start positions are generated independently and uniformly at random over the genome. A base is said to be covered by a read if it lies within the readâ€™s span downstream of its start coordinate, with wrap-around understood due to circularity.\n\nStarting only from the core sampling model (independent and identically distributed uniform read starts on a circular genome), the Bernoulli indicator for whether a single read covers a fixed base, the Binomial distribution for the count of covering reads across $N$ trials, and the well-known Binomial-to-Poisson limit for rare events, derive from first principles:\n1) the expected sequencing depth (coverage) at a randomly chosen base, and\n2) under the Poisson model, the probability that a randomly chosen base is uncovered.\n\nReport your final results as two numbers in this order: the expected coverage followed by the zero-coverage probability. Round both numbers to four significant figures. Express the coverage as a dimensionless fold value and the probability as a decimal.", "solution": "The problem statement is scientifically grounded, well-posed, and self-contained. It presents a standard model in bioinformatics for genome sequencing coverage. All necessary parameters are provided, and the required derivations are based on fundamental principles of probability theory. Thus, I will proceed with the analysis.\n\nThe problem requires the derivation of two quantities from first principles: the expected sequencing depth (coverage) and the probability of a base being uncovered.\n\nFirst, let us define the parameters provided:\n- The genome length is $G = 4.6 \\times 10^{6}$ base pairs.\n- The read length is $L = 150$ base pairs.\n- The number of sequenced reads is $N = 153{,}334$.\n\nThe model assumes that read start positions are independent and uniformly distributed across the circular genome.\n\n**1. Derivation of Expected Sequencing Depth ($\\lambda$)**\n\nLet us consider an arbitrary, fixed base on the genome. For this base to be covered by a single read of length $L$, the read's starting position must fall within a specific window of length $L$ \"upstream\" of the base. Due to the circular nature of the genome, the total number of possible start positions is $G$. The length of the interval of start positions that result in coverage of our chosen base is $L$.\n\nTherefore, the probability, $p$, that a single, randomly placed read covers our chosen base is the ratio of the length of the favorable region to the total genome length:\n$$p = \\frac{L}{G}$$\n\nNow, let us define a Bernoulli indicator random variable, $I_i$, for each read $i \\in \\{1, 2, \\ldots, N\\}$. This variable takes the value $1$ if read $i$ covers the fixed base, and $0$ otherwise.\n$$I_i = \\begin{cases} 1 & \\text{if read } i \\text{ covers the base} \\\\ 0 & \\text{otherwise} \\end{cases}$$\nThe probability of success for this Bernoulli trial is $P(I_i=1) = p = \\frac{L}{G}$. The expected value of this indicator variable is $E[I_i] = 1 \\cdot P(I_i=1) + 0 \\cdot P(I_i=0) = p$.\n\nThe total coverage depth, $C$, at the specific base is the total number of reads that cover it. This is the sum of the indicator variables for all $N$ reads:\n$$C = \\sum_{i=1}^{N} I_i$$\nThe expected sequencing depth, which we shall denote by $\\lambda$, is the expected value of $C$. By the linearity of expectation, we have:\n$$\\lambda = E[C] = E\\left[\\sum_{i=1}^{N} I_i\\right] = \\sum_{i=1}^{N} E[I_i]$$\nSince the reads are independent and identically distributed (i.i.d.), the expectation $E[I_i]$ is the same for all $i$. Thus:\n$$\\lambda = N \\cdot E[I_1] = N \\cdot p = \\frac{NL}{G}$$\nThis expression gives the expected coverage at any randomly selected base.\n\nSubstituting the given numerical values:\n$$\\lambda = \\frac{153{,}334 \\times 150}{4.6 \\times 10^{6}} = \\frac{23{,}000{,}100}{4{,}600{,}000} \\approx 5.000021739$$\nRounding to four significant figures, the expected coverage is $5.000$. This is a dimensionless quantity, representing the average number of times a base is sequenced, often referred to as \"fold coverage\".\n\n**2. Derivation of the Probability of a Base Being Uncovered**\n\nThe problem specifies that we must use the Binomial-to-Poisson limit. The coverage count $C = \\sum_{i=1}^{N} I_i$ is a sum of $N$ i.i.d. Bernoulli trials. Therefore, $C$ follows a Binomial distribution, $C \\sim \\text{Binomial}(N, p)$.\nThe probability mass function (PMF) is given by:\n$$P(C=k) = \\binom{N}{k} p^k (1-p)^{N-k}$$\nThe Poisson approximation to the Binomial distribution is valid when $N$ is large, $p$ is small, and their product $\\lambda = Np$ is a finite, moderate value. Let us verify these conditions:\n- $N = 153{,}334$ is large.\n- $p = \\frac{L}{G} = \\frac{150}{4.6 \\times 10^6} \\approx 3.26 \\times 10^{-5}$ is small.\n- $\\lambda = Np \\approx 5.000$ is a moderate constant.\n\nThe conditions are satisfied. Thus, we can approximate the Binomial distribution of the coverage count $C$ with a Poisson distribution with parameter $\\lambda = \\frac{NL}{G}$.\n$$P(C=k) \\approx \\frac{e^{-\\lambda} \\lambda^k}{k!}$$\nThe probability that a randomly chosen base is uncovered is the probability that its coverage count is zero, i.e., $P(C=0)$.\nUsing the Poisson PMF with $k=0$:\n$$P(C=0) = \\frac{e^{-\\lambda} \\lambda^0}{0!}$$\nSince $\\lambda^0 = 1$ and $0! = 1$, this simplifies to:\n$$P(C=0) = e^{-\\lambda}$$\nUsing the previously calculated value for $\\lambda$:\n$$P(C=0) = \\exp\\left(-\\frac{NL}{G}\\right) \\approx \\exp(-5.000021739)$$\n$$P(C=0) \\approx 0.00673780$$\nRounding this probability to four significant figures, we get $0.006738$.\n\nIn summary:\nThe expected coverage is $\\lambda = \\frac{NL}{G} \\approx 5.000021739$, which rounds to $5.000$.\nThe probability of zero coverage is $e^{-\\lambda} \\approx 0.00673780$, which rounds to $0.006738$.", "answer": "$$\\boxed{\\begin{pmatrix} 5.000 & 0.006738 \\end{pmatrix}}$$", "id": "2479969"}, {"introduction": "Assembling millions of short sequence reads into a contiguous and accurate genome is a cornerstone of modern microbiology, often involving sophisticated machine learning models. This practice will ground you in the essential metrics used to evaluate assembly quality, such as the $N50$ and $NG50$ statistics. You will compute these values from a hypothetical assembly and analyze the fundamental trade-off between achieving long, continuous contigs and maintaining high accuracy. [@problem_id:2479883]", "problem": "A bacterial isolate with a moderately complex genome is assembled from short and long reads and then scaffolded using a Graph Neural Network (GNN) scaffolder. You are given a draft assembly consisting of contigs with lengths in base pairs (bp) and, for each contig, the number of bases flagged as misassembled by an alignment-based evaluator. The estimated haploid genome size is $G = 6{,}400{,}000$ bp. The assembly consists of the following contigs (each item lists contig length followed by misassembled bases for that contig):\n- $980{,}000$ bp with $2{,}100$ misassembled bp\n- $760{,}000$ bp with $1{,}900$ misassembled bp\n- $610{,}000$ bp with $1{,}600$ misassembled bp\n- $540{,}000$ bp with $1{,}200$ misassembled bp\n- $480{,}000$ bp with $1{,}000$ misassembled bp\n- $420{,}000$ bp with $800$ misassembled bp\n- $360{,}000$ bp with $700$ misassembled bp\n- $300{,}000$ bp with $500$ misassembled bp\n- $240{,}000$ bp with $400$ misassembled bp\n- $180{,}000$ bp with $300$ misassembled bp\n- $120{,}000$ bp with $200$ misassembled bp\n- $80{,}000$ bp with $150$ misassembled bp\n\nFollow the standard, widely used definitions:\n- The assembly $N50$ is the contig length $L$ such that the sum of contig lengths that are at least $L$ is at least half the total assembled length.\n- The genome-based $NG50$ is defined similarly but uses half the reference genome size $G/2$ in place of half the assembled length.\n- The misassembly rate $r$ in this problem is defined as the fraction of assembled bases flagged as misassembled, i.e., $r = \\left(\\sum m_i\\right) \\big/ \\left(\\sum \\ell_i\\right)$, where $\\ell_i$ and $m_i$ are the length and misassembled bases of contig $i$.\n\nTasks:\n1. Compute the total assembly length and then determine $N50$ and $NG50$ according to the definitions above.\n2. Compute the misassembly rate $r$ as defined above.\n3. From first principles, briefly analyze how $N50$, $NG50$, and $r$ jointly reflect the tradeoff between contiguity and accuracy in a machine learning scaffolding context, and why $NG50$ can differ from $N50$ when the assembly is incomplete relative to $G$.\n4. Define the contiguity score $S_c = \\dfrac{N50 + NG50}{2G}$ and the accuracy score $S_a = 1 - r$. Then define the composite quality index as the harmonic mean $Q = \\dfrac{2}{\\dfrac{1}{S_c} + \\dfrac{1}{S_a}}$. Compute $Q$.\n\nExpress your final answer for $Q$ as a decimal fraction rounded to four significant figures. Do not include any units in your final answer.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It provides a complete set of data and definitions from the field of bioinformatics for a solvable quantitative analysis. All given values are realistic for a bacterial genome assembly evaluation. We shall proceed with the solution.\n\nThe problem requires the calculation of several assembly quality metrics and a composite quality index. Let the set of contigs be indexed by $i$, where $\\ell_i$ is the length of contig $i$ and $m_i$ is the number of misassembled bases in it. The estimated genome size is given as $G = 6{,}400{,}000$ base pairs (bp).\n\nFirst, we must compute the total assembly length, which is the sum of all contig lengths $\\sum_{i} \\ell_i$. The given contig lengths are: $980{,}000$, $760{,}000$, $610{,}000$, $540{,}000$, $480{,}000$, $420{,}000$, $360{,}000$, $300{,}000$, $240{,}000$, $180{,}000$, $120{,}000$, and $80{,}000$.\n$$\n\\sum_{i} \\ell_i = 980{,}000 + 760{,}000 + \\dots + 80{,}000 = 5{,}070{,}000 \\text{ bp}\n$$\nThe total length of the assembled contigs is $5{,}070{,}000$ bp.\n\nTask 1: Compute $N50$ and $NG50$.\nTo compute the $N50$ value, we first determine the threshold, which is half of the total assembly length:\n$$\n\\frac{1}{2} \\sum_{i} \\ell_i = \\frac{5{,}070{,}000}{2} = 2{,}535{,}000 \\text{ bp}\n$$\nNext, we sort the contigs by length in descending order (they are already provided in this order) and calculate the cumulative sum of their lengths.\n1. $\\ell_1 = 980{,}000$\n2. $\\sum_{i=1}^{2} \\ell_i = 980{,}000 + 760{,}000 = 1{,}740{,}000$\n3. $\\sum_{i=1}^{3} \\ell_i = 1{,}740{,}000 + 610{,}000 = 2{,}350{,}000$\n4. $\\sum_{i=1}^{4} \\ell_i = 2{,}350{,}000 + 540{,}000 = 2{,}890{,}000$\nThe cumulative sum first exceeds the threshold of $2{,}535{,}000$ bp upon the inclusion of the fourth contig. The $N50$ is the length of this contig, $\\ell_4$.\n$$\nN50 = 540{,}000 \\text{ bp}\n$$\nTo compute the $NG50$ value, the threshold is half of the estimated genome size $G$:\n$$\n\\frac{G}{2} = \\frac{6{,}400{,}000}{2} = 3{,}200{,}000 \\text{ bp}\n$$\nUsing the same cumulative sums:\n4. $\\sum_{i=1}^{4} \\ell_i = 2{,}890{,}000$\n5. $\\sum_{i=1}^{5} \\ell_i = 2{,}890{,}000 + 480{,}000 = 3{,}370{,}000$\nThe cumulative sum first exceeds the threshold of $3{,}200{,}000$ bp upon the inclusion of the fifth contig. The $NG50$ is the length of this contig, $\\ell_5$.\n$$\nNG50 = 480{,}000 \\text{ bp}\n$$\n\nTask 2: Compute the misassembly rate $r$.\nFirst, we sum the number of misassembled bases, $m_i$:\n$$\n\\sum_{i} m_i = 2{,}100 + 1{,}900 + 1{,}600 + 1{,}200 + 1{,}000 + 800 + 700 + 500 + 400 + 300 + 200 + 150 = 10{,}850 \\text{ bp}\n$$\nThe misassembly rate $r$ is the ratio of the total misassembled bases to the total assembled length:\n$$\nr = \\frac{\\sum_{i} m_i}{\\sum_{i} \\ell_i} = \\frac{10{,}850}{5{,}070{,}000}\n$$\n\nTask 3: Analyze the tradeoff between contiguity and accuracy.\nIn genome assembly, and particularly in scaffolding using machine learning models like GNNs, there is a fundamental tradeoff. Contiguity, measured by metrics like $N50$ and $NG50$, reflects how well short contigs have been joined into longer sequences. An aggressive scaffolding algorithm may propose many links between contigs, increasing the length of the resulting scaffolds and thus improving $N50/NG50$. However, this aggression increases the risk of making incorrect joins, or misassemblies. An increase in misassemblies elevates the misassembly rate $r$, which signifies a decrease in accuracy. Therefore, striving for maximum contiguity can compromise accuracy, and conversely, a conservative approach to avoid misassemblies may result in a more fragmented assembly with lower contiguity. The pair of metrics $(N50, r)$ or $(NG50, r)$ quantitatively captures this duality.\nThe difference between $N50$ and $NG50$ arises from their normalization base. $N50$ is an internal quality measure, relative to the size of the assembly itself ($\\sum \\ell_i$). $NG50$ is an external measure, relative to an independent estimate of the genome size ($G$). When an assembly is incomplete, as in this case where $\\sum \\ell_i = 5{,}070{,}000 < G = 6{,}400{,}000$, the threshold for $N50$ ($\\frac{1}{2}\\sum \\ell_i$) is lower than for $NG50$ ($\\frac{G}{2}$). Consequently, a smaller cumulative sum of contig lengths is required to reach the $N50$ threshold, which can lead to $N50 \\ge NG50$, as observed here ($540{,}000 > 480{,}000$).\n\nTask 4: Compute the composite quality index $Q$.\nFirst, we calculate the contiguity score $S_c$ and the accuracy score $S_a$.\nThe contiguity score $S_c$ is defined as:\n$$\nS_c = \\frac{N50 + NG50}{2G} = \\frac{540{,}000 + 480{,}000}{2 \\times 6{,}400{,}000} = \\frac{1{,}020{,}000}{12{,}800{,}000} = \\frac{102}{1280} = \\frac{51}{640}\n$$\nNumerically, $S_c = 0.0796875$.\n\nThe accuracy score $S_a$ is defined as $1 - r$:\n$$\nS_a = 1 - r = 1 - \\frac{10{,}850}{5{,}070{,}000} = \\frac{5{,}070{,}000 - 10{,}850}{5{,}070{,}000} = \\frac{5{,}059{,}150}{5{,}070{,}000} = \\frac{101183}{101400}\n$$\nNumerically, $S_a \\approx 0.9978609467$.\n\nFinally, the composite quality index $Q$ is the harmonic mean of $S_c$ and $S_a$:\n$$\nQ = \\frac{2}{\\frac{1}{S_c} + \\frac{1}{S_a}} = \\frac{2S_c S_a}{S_c + S_a}\n$$\nSubstituting the fractional values:\n$$\nQ = \\frac{2}{\\frac{640}{51} + \\frac{101400}{101183}}\n$$\nWe proceed with a numerical calculation:\n$$\nQ = \\frac{2}{12.5490196\\dots + 1.0021446\\dots} = \\frac{2}{13.5511642\\dots} \\approx 0.1475880\n$$\nRounding this result to four significant figures, as required, we get:\n$$\nQ \\approx 0.1476\n$$", "answer": "$$\n\\boxed{0.1476}\n$$", "id": "2479883"}, {"introduction": "Once genomes are assembled, we can compare them to uncover the evolutionary forces that shape microbial life. This practice guides you through implementing one of the most powerful methods in molecular evolution: calculating the ratio of nonsynonymous to synonymous substitution rates ($\\omega = d_N/d_S$). By applying a classic codon model with appropriate statistical corrections, you will learn to infer whether genes are under purifying, neutral, or positive (Darwinian) selection. [@problem_id:2479914]", "problem": "You are given a gene family present in multiple bacterial strains. For each gene, an alignment across strains has been analyzed to count the observed nonsynonymous substitutions and synonymous substitutions, and to estimate the number of nonsynonymous sites and synonymous sites (opportunities) based on codon structure. Assume the Central Dogma of Molecular Biology, where codons in Deoxyribonucleic Acid (DNA) encode amino acids, and that nonsynonymous substitutions change the encoded amino acid whereas synonymous substitutions do not. Under a homogeneous, time-reversible nucleotide substitution process with equal base frequencies and equal substitution rates between all nucleotides, the Jukesâ€“Cantor model from year $1969$ provides a correction for multiple hits at the same site. In the approximate codon-counting framework of Neiâ€“Gojobori from year $1986$, distances for nonsynonymous and synonymous categories are derived by applying the Jukesâ€“Cantor correction separately to the observed proportions of differences per site in each category.\n\nYou must implement a program that, for each test case, computes the nonsynonymous distance $d_N$, the synonymous distance $d_S$, and their ratio $\\omega = d_N/d_S$. Let $x_N$ be the observed count of nonsynonymous substitutions, $x_S$ the observed count of synonymous substitutions, $n_N$ the total nonsynonymous sites, and $n_S$ the total synonymous sites. When counts are small or zero, apply a Haldaneâ€“Anscombe continuity correction to estimate proportions robustly by using\n$$\n\\hat{p}_N = \\frac{x_N + 0.5}{n_N + 1}, \\qquad \\hat{p}_S = \\frac{x_S + 0.5}{n_S + 1}.\n$$\nThen compute Jukesâ€“Cantor corrected distances for each category:\n$$\nd_N = -\\frac{3}{4}\\ln\\left(1 - \\frac{4}{3}\\hat{p}_N\\right), \\qquad d_S = -\\frac{3}{4}\\ln\\left(1 - \\frac{4}{3}\\hat{p}_S\\right).\n$$\nDefine the selection regime using a symmetric tolerance parameter $\\tau$ around neutrality:\n- purifying selection if $\\omega \\leq 1 - \\tau$,\n- neutral if $1 - \\tau < \\omega < 1 + \\tau$,\n- positive selection if $\\omega \\geq 1 + \\tau$.\n\nFor numerical stability, if either $\\hat{p}_N$ or $\\hat{p}_S$ makes the Jukesâ€“Cantor argument nonpositive, cap the corresponding $\\hat{p}$ to a value strictly less than $\\frac{3}{4}$ by a negligible margin so that the logarithm is defined.\n\nYour program must implement the above logic and apply it to the following test suite. Each test case is a quadruple $(x_N, x_S, n_N, n_S)$, and the tolerance parameter is $\\tau = 0.1$:\n- Test case $1$: $(x_N, x_S, n_N, n_S) = (12, 25, 5000, 1500)$.\n- Test case $2$: $(x_N, x_S, n_N, n_S) = (80, 5, 4000, 2000)$.\n- Test case $3$: $(x_N, x_S, n_N, n_S) = (30, 10, 3000, 1000)$.\n- Test case $4$: $(x_N, x_S, n_N, n_S) = (0, 0, 500, 500)$.\n- Test case $5$: $(x_N, x_S, n_N, n_S) = (14, 10, 20, 300)$.\n\nOutput specification:\n- For each test case, produce a list of the form $[\\omega, d_N, d_S, r]$, where $\\omega$, $d_N$, and $d_S$ are floats rounded to $6$ decimal places, and $r$ is an integer regime code with values $-1$ for purifying, $0$ for neutral, and $+1$ for positive selection.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each element is the per-test-case list defined above (for example, a list of lists without any intervening text). Angles are not involved, and there are no physical units to report. All numerical answers are pure numbers without percentage signs.", "solution": "The problem has been scrutinized and found to be valid. It is a well-posed exercise in quantitative molecular evolution, grounded in the established, albeit simplified, theoretical framework of Neiâ€“Gojobori and Jukesâ€“Cantor. The problem statement is self-contained, logically consistent, and free of scientific or factual unsoundness. We shall proceed with its systematic solution.\n\nThe objective is to compute the ratio of nonsynonymous to synonymous substitution rates, $\\omega$, and to classify the mode of natural selection acting on a gene. This is achieved by following a precise algorithmic procedure for each test case, which is a tuple of counts and sites $(x_N, x_S, n_N, n_S)$.\n\nThe calculation proceeds in four distinct steps:\n\nFirst, we estimate the proportion of nonsynonymous differences, $\\hat{p}_N$, and the proportion of synonymous differences, $\\hat{p}_S$, from the raw counts. The provided counts are the number of observed nonsynonymous substitutions, $x_N$, the number of observed synonymous substitutions, $x_S$, the total number of available nonsynonymous sites, $n_N$, and the total number of available synonymous sites, $n_S$. To avoid issues with proportions of $0$ or $1$, especially when counts are small, a Haldaneâ€“Anscombe continuity correction is applied. This is a standard procedure for robustly estimating binomial proportions. The corrected proportions are:\n$$\n\\hat{p}_N = \\frac{x_N + 0.5}{n_N + 1}\n$$\n$$\n\\hat{p}_S = \\frac{x_S + 0.5}{n_S + 1}\n$$\n\nSecond, we correct these observed proportions for multiple substitutions that may have occurred at the same site over evolutionary time but are unobservable in a pairwise sequence comparison. The Jukesâ€“Cantor model of $1969$, assuming equal nucleotide frequencies and substitution rates, provides a logarithmic correction. The evolutionary distance, or the estimated number of substitutions per site, is calculated separately for the nonsynonymous ($d_N$) and synonymous ($d_S$) categories:\n$$\nd_N = -\\frac{3}{4}\\ln\\left(1 - \\frac{4}{3}\\hat{p}_N\\right)\n$$\n$$\nd_S = -\\frac{3}{4}\\ln\\left(1 - \\frac{4}{3}\\hat{p}_S\\right)\n$$\nA critical constraint of the Jukesâ€“Cantor model is that the argument of the natural logarithm, $\\ln$, must be positive. This implies that $1 - \\frac{4}{3}\\hat{p} > 0$, which simplifies to $\\hat{p} < \\frac{3}{4}$. If for any category, the estimated proportion $\\hat{p}$ is greater than or equal to this saturation threshold of $\\frac{3}{4}$, the distance is formally undefined. The problem dictates a numerical stabilization procedure: cap such a proportion to a value negligibly smaller than $\\frac{3}{4}$ to ensure the computation can proceed. This is a pragmatic, though artificial, solution to the model's limitation.\n\nThird, we compute the ratio $\\omega = d_N/d_S$. This ratio is a fundamental measure in molecular evolution, representing the relative rate of nonsynonymous to synonymous substitutions. It serves as an indicator of the selective pressure acting on the protein-coding gene. An $\\omega$ ratio of $1$ suggests that nonsynonymous mutations are fixed at the same rate as synonymous (neutral) mutations. A ratio less than $1$ implies that nonsynonymous mutations are deleterious and are removed by purifying selection. A ratio greater than $1$ indicates that nonsynonymous mutations are advantageous and are being fixed by positive (Darwinian) selection.\n\nFourth, we classify the selection regime based on the value of $\\omega$. Using the specified symmetric tolerance parameter $\\tau = 0.1$, we define three distinct regimes:\n- Purifying selection: if $\\omega \\leq 1 - \\tau$, which is $\\omega \\leq 0.9$. A regime code of $-1$ is assigned.\n- Neutral evolution: if $1 - \\tau < \\omega < 1 + \\tau$, which is $0.9 < \\omega < 1.1$. A regime code of $0$ is assigned.\n- Positive selection: if $\\omega \\geq 1 + \\tau$, which is $\\omega \\geq 1.1$. A regime code of $+1$ is assigned.\n\nThis complete procedure will be implemented and applied to each of the $5$ test cases provided. The final result for each case will be a list containing the rounded values of $\\omega$, $d_N$, $d_S$, and the integer regime code $r$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and print results.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (x_N, x_S, n_N, n_S).\n    test_cases = [\n        (12, 25, 5000, 1500),\n        (80, 5, 4000, 2000),\n        (30, 10, 3000, 1000),\n        (0, 0, 500, 500),\n        (14, 10, 20, 300),\n    ]\n\n    # The tolerance parameter for defining selection regimes.\n    tau = 0.1\n\n    results = []\n    for case in test_cases:\n        result = calculate_selection_regime(case, tau)\n        results.append(result)\n\n    # Final print statement must produce a single line: a list of lists.\n    # The str() representation of a list of lists matches the required format.\n    print(str(results).replace(\" \", \"\"))\n\n\ndef calculate_selection_regime(case, tau):\n    \"\"\"\n    Calculates dN, dS, omega, and the selection regime for a single case.\n\n    Args:\n        case (tuple): A tuple containing (x_N, x_S, n_N, n_S).\n        tau (float): The tolerance parameter for regime classification.\n\n    Returns:\n        list: A list of the form [omega, d_N, d_S, r], where floats are\n              rounded to 6 decimal places.\n    \"\"\"\n    x_N, x_S, n_N, n_S = case\n\n    # Step 1: Calculate proportions with Haldane-Anscombe continuity correction.\n    p_N_hat = (x_N + 0.5) / (n_N + 1)\n    p_S_hat = (x_S + 0.5) / (n_S + 1)\n\n    # Step 2: Calculate Jukes-Cantor corrected distances.\n    # First, handle numerical stability by capping proportions if necessary.\n    # The Jukes-Cantor correction is undefined for p >= 3/4.\n    p_cap_limit = 3.0 / 4.0\n    epsilon = 1e-9 # A negligible margin for capping\n\n    if p_N_hat >= p_cap_limit:\n        p_N_hat = p_cap_limit - epsilon\n    \n    if p_S_hat >= p_cap_limit:\n        p_S_hat = p_cap_limit - epsilon\n\n    # Calculate dN and dS. The continuity correction ensures p_hat > 0,\n    # so dN and dS will be positive, and division by zero for omega is avoided.\n    d_N = -0.75 * np.log(1.0 - (4.0 / 3.0) * p_N_hat)\n    d_S = -0.75 * np.log(1.0 - (4.0 / 3.0) * p_S_hat)\n\n    # Step 3: Calculate the omega ratio.\n    omega = d_N / d_S\n\n    # Step 4: Determine the selection regime.\n    regime = 0 # Default to neutral\n    if omega <= 1 - tau:\n        regime = -1 # Purifying selection\n    elif omega >= 1 + tau:\n        regime = 1 # Positive selection\n\n    # Format the output as specified: round floats to 6 decimal places.\n    return [\n        round(omega, 6),\n        round(d_N, 6),\n        round(d_S, 6),\n        regime\n    ]\n\n# Execute the main function.\nsolve()\n```", "id": "2479914"}]}