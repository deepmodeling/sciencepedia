{"hands_on_practices": [{"introduction": "Reliable quantitative data is the bedrock of functional genomics. This first exercise guides you through a fundamental process in quantitative metabolomics: building a calibration curve to convert instrument signals into absolute concentrations [@problem_id:2494820]. You will learn to assess the linear range of your assay and, critically, model how measurement error changes with concentration—a key step towards robust statistical analysis using weighted regression.", "problem": "A microbiology laboratory is developing an absolute quantification assay for the short-chain fatty acid butyrate in anaerobic culture supernatants using Liquid Chromatography–Tandem Mass Spectrometry (LC–MS/MS) with a Stable Isotope–Labeled Internal Standard (SIL-IS). The instrument response is recorded as the analyte-to-internal-standard peak area ratio (dimensionless). External calibration standards were prepared in a synthetic matrix spanning low to high micromolar concentrations. For each standard, the instrument was injected in triplicate under constant-source conditions to assess both calibration linearity and the mean–variance relationship of the response.\n\nYou are given the following calibration levels (butyrate concentration, in $\\mu\\text{M}$) and triplicate responses (area ratio, dimensionless). Each line lists the concentration $c$ followed by the three replicate responses $y_{1},y_{2},y_{3}$:\n\n- $c = 1$: $0.0147$, $0.0150$, $0.0153$\n- $c = 2$: $0.0294$, $0.0300$, $0.0306$\n- $c = 4$: $0.0588$, $0.0600$, $0.0612$\n- $c = 8$: $0.1176$, $0.1200$, $0.1224$\n- $c = 16$: $0.2352$, $0.2400$, $0.2448$\n- $c = 32$: $0.4704$, $0.4800$, $0.4896$\n- $c = 64$: $0.9408$, $0.9600$, $0.9792$\n- $c = 128$: $1.666$, $1.700$, $1.734$\n\nAssume the following foundations that are standard in functional metabolomics quantification:\n- The mean instrument response is proportional to analyte concentration within the linear dynamic range: $y = \\beta c + \\varepsilon$, with $E[\\varepsilon \\mid c] = 0$.\n- Weighted least squares is appropriate under heteroscedasticity when weights are proportional to the inverse of the conditional variance: $w(\\mu) \\propto 1/\\operatorname{Var}(Y \\mid \\mu)$, where $\\mu = E[Y \\mid c]$.\n- The unbiased sample variance at each level is $s^{2} = \\frac{1}{n-1}\\sum_{j=1}^{n}(y_{j}-\\bar{y})^{2}$ with $n = 3$.\n- A variance–mean power model is plausible for ion-current–based detectors with internal standardization: $\\operatorname{Var}(Y \\mid \\mu) = a \\mu^{p}$ with $a > 0$, $p > 0$.\n\nTasks you must perform:\n1) Construct the calibration curve by fitting the proportional model $y = \\beta c$ through the origin using ordinary least squares (OLS) on the per-level means $\\bar{y}$ of the first seven standards ($c \\leq 64\\ \\mu\\text{M}$). Use this to define the predicted response $\\hat{y} = \\hat{\\beta} c$.\n2) Define the linear dynamic range as the maximal interval $\\left[c_{\\min}, c_{\\max}\\right]$ over the provided calibrators such that, for every calibrator in that interval, the absolute relative residual satisfies $\\left|\\bar{y} - \\hat{y}\\right|/\\hat{y} < \\delta$ with $\\delta = 0.05$. Determine $c_{\\max}$ under this criterion.\n3) Using all levels, estimate the exponent $p$ in the variance–mean power model by performing an ordinary least squares regression of $\\ln(s^{2})$ on $\\ln(\\bar{y})$ across the levels. Use natural logarithms and the unbiased sample variances computed from the triplicates at each level.\n4) From the fitted variance model, specify the weight function $w(\\bar{y}) \\propto 1/\\widehat{\\operatorname{Var}}(Y \\mid \\bar{y})$ for use in weighted least squares.\n\nAnswer specification:\n- Report only the estimated exponent $p$ from Task $3$.\n- Express $p$ as a dimensionless real number.\n- Round your answer to four significant figures.", "solution": "The problem is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Data**: Eight calibration levels with concentrations $c$ (in $\\mu\\text{M}$) and triplicate responses $(y_1, y_2, y_3)$ (dimensionless area ratio):\n    - $c = 1$: $\\{0.0147, 0.0150, 0.0153\\}$\n    - $c = 2$: $\\{0.0294, 0.0300, 0.0306\\}$\n    - $c = 4$: $\\{0.0588, 0.0600, 0.0612\\}$\n    - $c = 8$: $\\{0.1176, 0.1200, 0.1224\\}$\n    - $c = 16$: $\\{0.2352, 0.2400, 0.2448\\}$\n    - $c = 32$: $\\{0.4704, 0.4800, 0.4896\\}$\n    - $c = 64$: $\\{0.9408, 0.9600, 0.9792\\}$\n    - $c = 128$: $\\{1.666, 1.700, 1.734\\}$\n- **Models and Definitions**:\n    - Mean response model: $y = \\beta c + \\varepsilon$, with $E[\\varepsilon \\mid c] = 0$.\n    - Weighting for weighted least squares: $w(\\mu) \\propto 1/\\operatorname{Var}(Y \\mid \\mu)$, where $\\mu = E[Y \\mid c]$.\n    - Unbiased sample variance formula: $s^{2} = \\frac{1}{n-1}\\sum_{j=1}^{n}(y_{j}-\\bar{y})^{2}$ with $n=3$.\n    - Variance-mean power model: $\\operatorname{Var}(Y \\mid \\mu) = a \\mu^{p}$ with $a > 0$, $p > 0$.\n- **Tasks**:\n    1. OLS fit of $y = \\beta c$ on means for $c \\leq 64\\ \\mu\\text{M}$.\n    2. Determine linear dynamic range $c_{\\max}$ using absolute relative residual criterion $|\\bar{y} - \\hat{y}|/\\hat{y} < 0.05$.\n    3. Estimate exponent $p$ from OLS regression of $\\ln(s^{2})$ on $\\ln(\\bar{y})$ using all data.\n    4. Specify weight function $w(\\bar{y})$.\n- **Required Output**: Report the value of $p$ from Task $3$, rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, describing a standard calibration and variance modeling procedure in quantitative mass spectrometry. It is well-posed, providing all necessary data and specifying the analytical methods required to perform the tasks. The language is objective and free of ambiguity. The problem is self-contained, consistent, and computationally verifiable. No violations of scientific principles, logical inconsistencies, or factual errors are present. The data, while idealized, are physically plausible.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be provided.\n\nThe objective is to determine the exponent $p$ of the variance-mean power model, $\\operatorname{Var}(Y \\mid \\mu) = a \\mu^{p}$. This model describes the heteroscedasticity of the instrument response. We will use the sample statistics, the mean response $\\bar{y}$ and the sample variance $s^2$, as estimates for the population parameters $\\mu$ and $\\operatorname{Var}(Y \\mid \\mu)$, respectively. The model is thus approximated as $s^2 \\approx a \\bar{y}^p$.\n\nTo estimate $p$, we linearize this power-law relationship by taking the natural logarithm of both sides:\n$$ \\ln(s^2) = \\ln(a) + p \\ln(\\bar{y}) $$\nThis equation is of the form $Y' = \\beta_0 + \\beta_1 X'$, where $Y' = \\ln(s^2)$, $X' = \\ln(\\bar{y})$, the intercept is $\\beta_0 = \\ln(a)$, and the slope is $\\beta_1 = p$. The parameter $p$ can thus be estimated by performing an ordinary least squares (OLS) regression of $\\ln(s^2)$ on $\\ln(\\bar{y})$ across all eight concentration levels.\n\nFirst, we compute the sample mean $\\bar{y}_i$ and the unbiased sample variance $s_i^2$ for each concentration level $c_i$, where $i = 1, \\dots, 8$. The number of replicates is $n=3$.\n\nThe calculated values are:\n- For $c_1=1$: $\\bar{y}_1 = (0.0147+0.0150+0.0153)/3 = 0.0150$.\n$s_1^2 = \\frac{1}{2}\\left((0.0147-0.0150)^2 + (0.0150-0.0150)^2 + (0.0153-0.0150)^2\\right) = 9 \\times 10^{-8}$.\n\n- For $c_2=2$: $\\bar{y}_2 = (0.0294+0.0300+0.0306)/3 = 0.0300$.\n$s_2^2 = \\frac{1}{2}\\left((0.0294-0.0300)^2 + (0.0300-0.0300)^2 + (0.0306-0.0300)^2\\right) = 3.6 \\times 10^{-7}$.\n\n- For $c_3=4$: $\\bar{y}_3 = (0.0588+0.0600+0.0612)/3 = 0.0600$.\n$s_3^2 = 1.44 \\times 10^{-6}$.\n\n- For $c_4=8$: $\\bar{y}_4 = (0.1176+0.1200+0.1224)/3 = 0.1200$.\n$s_4^2 = 5.76 \\times 10^{-6}$.\n\n- For $c_5=16$: $\\bar{y}_5 = (0.2352+0.2400+0.2448)/3 = 0.2400$.\n$s_5^2 = 2.304 \\times 10^{-5}$.\n\n- For $c_6=32$: $\\bar{y}_6 = (0.4704+0.4800+0.4896)/3 = 0.4800$.\n$s_6^2 = 9.216 \\times 10^{-5}$.\n\n- For $c_7=64$: $\\bar{y}_7 = (0.9408+0.9600+0.9792)/3 = 0.9600$.\n$s_7^2 = 3.6864 \\times 10^{-4}$.\n\n- For $c_8=128$: $\\bar{y}_8 = (1.666+1.700+1.734)/3 = 1.700$.\n$s_8^2 = \\frac{1}{2}\\left((1.666-1.700)^2 + (1.700-1.700)^2 + (1.734-1.700)^2\\right) = 1.156 \\times 10^{-3}$.\n\nThe data provided are artificially perfect. We can demonstrate that the relationship $s_i^2 = k \\cdot \\bar{y}_i^2$ holds for a constant $k$ across all data points, which implies $p=2$. Let us test this hypothesis by calculating the ratio $k = s_i^2 / \\bar{y}_i^2$ for all points.\n\nFor the first point ($i=1$):\n$$ k = \\frac{s_1^2}{\\bar{y}_1^2} = \\frac{9 \\times 10^{-8}}{(0.0150)^2} = \\frac{9 \\times 10^{-8}}{2.25 \\times 10^{-4}} = 4 \\times 10^{-4} $$\nFor the second point ($i=2$):\n$$ k = \\frac{s_2^2}{\\bar{y}_2^2} = \\frac{3.6 \\times 10^{-7}}{(0.0300)^2} = \\frac{3.6 \\times 10^{-7}}{9 \\times 10^{-4}} = 4 \\times 10^{-4} $$\nThis pattern continues perfectly for all points. Let us verify the last point ($i=8$), which appears to deviate from the simple doubling sequence of the other calibrators:\n$$ k = \\frac{s_8^2}{\\bar{y}_8^2} = \\frac{1.156 \\times 10^{-3}}{(1.700)^2} = \\frac{1.156 \\times 10^{-3}}{2.89} = 4 \\times 10^{-4} $$\nSince the ratio $s_i^2 / \\bar{y}_i^2$ is constant and equal to $4 \\times 10^{-4}$ for all $i=1, \\dots, 8$, we can state that the sample data perfectly fit the model:\n$$ s^2 = (4 \\times 10^{-4}) \\bar{y}^2 $$\nComparing this to the general model $s^2 = a \\bar{y}^p$, we can directly identify the parameters by inspection: $a = 4 \\times 10^{-4}$ and $p=2$.\n\nIf one were to proceed with the formal OLS regression of $\\ln(s^2)$ on $\\ln(\\bar{y})$, the data points would lie on a perfect straight line described by $\\ln(s^2) = \\ln(4 \\times 10^{-4}) + 2 \\ln(\\bar{y})$. A linear regression on such data will yield a slope of exactly $2$ with a coefficient of determination $R^2=1$.\n\nTherefore, the estimated exponent is $p=2$. The problem requires this to be reported to four significant figures.", "answer": "$$\\boxed{2.000}$$", "id": "2494820"}, {"introduction": "Transcriptomics data can be deceptively complex, with hidden biases that can lead to erroneous conclusions if not properly addressed. This simulation [@problem_id:2494870] tackles one of the most important systematic errors in RNA-seq: length bias caused by changing isoform ratios between samples. By comparing a naive gene-level summarization to an isoform-aware approach, you will gain a concrete understanding of why isoform-level quantification is crucial for accurate differential expression analysis.", "problem": "You are tasked with implementing a deterministic simulation of gene-level expression quantification under RNA sequencing (RNA-seq) using a principled sampling model and comparing two estimators: an isoform-aware estimator and a naïve gene-level length correction estimator. The goal is to formalize and quantify how changes in isoform ratios induce length bias when aggregating to gene-level, and to compute the multiplicative bias and relative error introduced by the naïve approach.\n\nFoundations and assumptions:\n- Central Dogma and expression measurement: RNA sequencing (RNA-seq) measures fragments derived from transcripts; under uniform fragmentation and unbiased sampling, the expected number of fragments mapping to an isoform is proportional to its molar concentration and its effective length.\n- Deterministic expectation model: For a gene with isoforms indexed by $j \\in \\{1,\\dots,I\\}$, in sample $s$, let $\\theta_{j,s}$ denote the isoform molar concentration, $L_j$ the effective length in nucleotides (nt), and $k_s$ a sequencing depth scaling constant. The expected fragment count for isoform $j$ is\n$$\\mathbb{E}[C_{j,s}] = k_s \\, \\theta_{j,s} \\, L_j.$$\n- Let the gene-level molar abundance be $G_s = \\sum_{j=1}^{I} \\theta_{j,s}$. Define isoform fractions $p_{j,s} = \\theta_{j,s}/G_s$ so that $\\sum_{j} p_{j,s} = 1$ with $p_{j,s} \\ge 0$, and therefore $\\theta_{j,s} = G_s \\, p_{j,s}$.\n\nEstimators to compare:\n- Isoform-aware estimator (idealized): Given expected counts and effective lengths, define\n$$\\widehat{G}_{\\mathrm{aware},s} = \\sum_{j=1}^{I} \\frac{\\mathbb{E}[C_{j,s}]}{k_s \\, L_j}.$$\n- Naïve gene-level length correction: First aggregate expected counts to gene-level $\\mathbb{E}[C_{\\mathrm{gene},s}] = \\sum_{j=1}^{I} \\mathbb{E}[C_{j,s}]$. Then divide by a fixed gene-level length computed from a reference sample $r$,\n$$L_{\\mathrm{ref}} = \\sum_{j=1}^{I} p_{j,r} \\, L_j,$$\nto obtain\n$$\\widehat{G}_{\\mathrm{naive},s} = \\frac{\\mathbb{E}[C_{\\mathrm{gene},s}]}{k_s \\, L_{\\mathrm{ref}}}.$$\n\nOutput quantities to compute for each test case:\n- Multiplicative bias factor\n$$B_s = \\frac{\\widehat{G}_{\\mathrm{naive},s}}{\\widehat{G}_{\\mathrm{aware},s}}.$$\n- Relative error (expressed as a decimal, not a percentage)\n$$\\delta_s = B_s - 1.$$\n\nImplementation instructions:\n- Treat the above as a deterministic simulation using expected counts; do not introduce randomness.\n- Use the provided test suite of parameter sets. For each test case, you are given:\n  - Isoform effective lengths in nucleotides: a list $[L_1,\\dots,L_I]$ (units: nt).\n  - Reference isoform fractions $[p_{1,r},\\dots,p_{I,r}]$ satisfying $\\sum_j p_{j,r} = 1$.\n  - Sample isoform fractions $[p_{1,s},\\dots,p_{I,s}]$ satisfying $\\sum_j p_{j,s} = 1$.\n  - A sequencing depth scaling constant $k_s$ and a gene-level molar abundance $G_s$ to construct the expected counts. For this simulation, set $k_s = 1$ and $G_s = 1$ in every test case to keep units implicit and to isolate length-mix effects.\n- Compute $\\mathbb{E}[C_{j,s}] = k_s \\, G_s \\, p_{j,s} \\, L_j$ for each isoform $j$, then compute $\\widehat{G}_{\\mathrm{aware},s}$ and $\\widehat{G}_{\\mathrm{naive},s}$ as above, followed by $B_s$ and $\\delta_s$.\n\nTest suite:\n- Case $1$ (two isoforms, shift toward shorter isoform in the sample):\n  - $[L_1,L_2] = [1000,2000]$ nt\n  - $[p_{1,r},p_{2,r}] = [0.5,0.5]$\n  - $[p_{1,s},p_{2,s}] = [0.8,0.2]$\n  - $k_s = 1$, $G_s = 1$\n- Case $2$ (two isoforms, shift toward longer isoform in the sample):\n  - $[L_1,L_2] = [1000,2000]$ nt\n  - $[p_{1,r},p_{2,r}] = [0.5,0.5]$\n  - $[p_{1,s},p_{2,s}] = [0.2,0.8]$\n  - $k_s = 1$, $G_s = 1$\n- Case $3$ (three isoforms, pronounced swap from medium to short isoform):\n  - $[L_1,L_2,L_3] = [500,1500,3000]$ nt\n  - $[p_{1,r},p_{2,r},p_{3,r}] = [0.2,0.7,0.1]$\n  - $[p_{1,s},p_{2,s},p_{3,s}] = [0.7,0.2,0.1]$\n  - $k_s = 1$, $G_s = 1$\n- Case $4$ (three isoforms, equal lengths, any ratio change should not bias):\n  - $[L_1,L_2,L_3] = [1000,1000,1000]$ nt\n  - $[p_{1,r},p_{2,r},p_{3,r}] = [0.1,0.8,0.1]$\n  - $[p_{1,s},p_{2,s},p_{3,s}] = [0.7,0.2,0.1]$\n  - $k_s = 1$, $G_s = 1$\n- Case $5$ (three isoforms, identical ratios in reference and sample, no bias expected):\n  - $[L_1,L_2,L_3] = [750,1800,2400]$ nt\n  - $[p_{1,r},p_{2,r},p_{3,r}] = [0.3,0.4,0.3]$\n  - $[p_{1,s},p_{2,s},p_{3,s}] = [0.3,0.4,0.3]$\n  - $k_s = 1$, $G_s = 1$\n- Case $6$ (two isoforms, extreme swap toward much longer isoform):\n  - $[L_1,L_2] = [500,5000]$ nt\n  - $[p_{1,r},p_{2,r}] = [0.9,0.1]$\n  - $[p_{1,s},p_{2,s}] = [0.1,0.9]$\n  - $k_s = 1$, $G_s = 1$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for the test cases, ordered from Case $1$ through Case $6$, as a JSON-like list of pairs. Each pair corresponds to one test case and must be of the form $[B_s,\\delta_s]$, with both entries rounded to six decimal places. The entire output should be printed as one line with no extra spaces, for example:\n- Example format: $[[0.800000,-0.200000],[1.200000,0.200000],\\dots]$.\n\nAngles are not involved. All outputs are unitless decimals. Do not use percentage signs; express all errors as decimals. Your program must be self-contained, require no input, and use only the specified libraries.", "solution": "The problem has been examined and is determined to be valid. It is a well-posed, scientifically grounded exercise in modeling a known source of systematic error in transcriptomics. It is based on a simplified but conceptually sound deterministic model of RNA sequencing, providing a clear basis for calculation. All necessary parameters and definitions are provided, and there are no internal contradictions or ambiguities.\n\nThe task is to quantify the bias of a naïve gene-level expression estimator relative to an idealized isoform-aware estimator. We begin by formalizing the provided models to derive a general expression for this bias.\n\nThe expected fragment count for isoform $j$ in sample $s$ is given as:\n$$ \\mathbb{E}[C_{j,s}] = k_s \\, \\theta_{j,s} \\, L_j $$\nwhere $k_s$ is a sample-specific sequencing depth constant, $\\theta_{j,s}$ is the molar concentration of the isoform, and $L_j$ is its effective length. The gene-level molar abundance is $G_s = \\sum_{j=1}^{I} \\theta_{j,s}$, and isoform fractions are $p_{j,s} = \\theta_{j,s} / G_s$. By substitution, we have $\\theta_{j,s} = G_s \\, p_{j,s}$, which allows us to re-express the expected count as:\n$$ \\mathbb{E}[C_{j,s}] = k_s \\, G_s \\, p_{j,s} \\, L_j $$\nFor the simulation, we are instructed to use $k_s=1$ and $G_s=1$, but we will proceed with the general derivation to maintain rigor.\n\nFirst, consider the idealized isoform-aware estimator, $\\widehat{G}_{\\mathrm{aware},s}$. This estimator operates at the isoform level before aggregation.\n$$ \\widehat{G}_{\\mathrm{aware},s} = \\sum_{j=1}^{I} \\frac{\\mathbb{E}[C_{j,s}]}{k_s \\, L_j} $$\nSubstituting the expression for $\\mathbb{E}[C_{j,s}]$:\n$$ \\widehat{G}_{\\mathrm{aware},s} = \\sum_{j=1}^{I} \\frac{k_s \\, G_s \\, p_{j,s} \\, L_j}{k_s \\, L_j} = \\sum_{j=1}^{I} G_s \\, p_{j,s} = G_s \\sum_{j=1}^{I} p_{j,s} $$\nBy definition, the isoform fractions sum to one: $\\sum_{j=1}^{I} p_{j,s} = 1$. Therefore:\n$$ \\widehat{G}_{\\mathrm{aware},s} = G_s $$\nAs expected, the isoform-aware estimator is unbiased in this deterministic model and perfectly recovers the true gene-level abundance $G_s$. This serves as our ground truth for comparison.\n\nNext, we analyze the naïve estimator, $\\widehat{G}_{\\mathrm{naive},s}$. This method first aggregates counts at the gene level and then applies a single length correction factor derived from a reference sample $r$.\nThe aggregated expected count for the gene is:\n$$ \\mathbb{E}[C_{\\mathrm{gene},s}] = \\sum_{j=1}^{I} \\mathbb{E}[C_{j,s}] = \\sum_{j=1}^{I} k_s \\, G_s \\, p_{j,s} \\, L_j = k_s \\, G_s \\left(\\sum_{j=1}^{I} p_{j,s} \\, L_j\\right) $$\nLet us define the abundance-weighted average isoform length for a given sample $s$ as $L_s^{\\mathrm{avg}} = \\sum_{j=1}^{I} p_{j,s} \\, L_j$. The aggregated count is then $\\mathbb{E}[C_{\\mathrm{gene},s}] = k_s \\, G_s \\, L_s^{\\mathrm{avg}}$.\n\nThe naïve estimator normalizes this count using a fixed reference length, $L_{\\mathrm{ref}}$, which is the average isoform length computed using the isoform fractions from a reference sample $r$:\n$$ L_{\\mathrm{ref}} = \\sum_{j=1}^{I} p_{j,r} \\, L_j $$\nThe estimate is then:\n$$ \\widehat{G}_{\\mathrm{naive},s} = \\frac{\\mathbb{E}[C_{\\mathrm{gene},s}]}{k_s \\, L_{\\mathrm{ref}}} = \\frac{k_s \\, G_s \\, L_s^{\\mathrm{avg}}}{k_s \\, L_{\\mathrm{ref}}} = G_s \\frac{L_s^{\\mathrm{avg}}}{L_{\\mathrm{ref}}} $$\n\nThe multiplicative bias factor, $B_s$, is the ratio of the naïve estimate to the true abundance (given by the aware estimate):\n$$ B_s = \\frac{\\widehat{G}_{\\mathrm{naive},s}}{\\widehat{G}_{\\mathrm{aware},s}} = \\frac{G_s \\frac{L_s^{\\mathrm{avg}}}{L_{\\mathrm{ref}}}}{G_s} = \\frac{L_s^{\\mathrm{avg}}}{L_{\\mathrm{ref}}} $$\nSubstituting the full definitions for the average lengths, we arrive at the final analytical expression for the bias:\n$$ B_s = \\frac{\\sum_{j=1}^{I} p_{j,s} \\, L_j}{\\sum_{j=1}^{I} p_{j,r} \\, L_j} $$\nThis result elegantly demonstrates that the multiplicative bias is precisely the ratio of the abundance-weighted average isoform length in the sample of interest to that of the reference sample. Bias is introduced if, and only if, these two average lengths differ. This occurs when the relative proportions of isoforms ($p_{j,s}$ versus $p_{j,r}$) change between samples for a gene whose isoforms have different lengths.\n\nThe relative error, $\\delta_s$, is defined as $B_s - 1$:\n$$ \\delta_s = \\frac{\\sum_{j=1}^{I} p_{j,s} \\, L_j}{\\sum_{j=1}^{I} p_{j,r} \\, L_j} - 1 $$\n\nThe problem is now reduced to the mechanical application of these derived formulae to the $6$ provided test cases. The implementation will compute these quantities for each parameter set.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the RNA-seq bias simulation problem for a suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (isoform_lengths, reference_fractions, sample_fractions).\n    # k_s and G_s are 1 for all cases as per the problem and cancel out.\n    test_cases = [\n        # Case 1: shift toward shorter isoform\n        {\n            \"L\": np.array([1000.0, 2000.0]),\n            \"p_r\": np.array([0.5, 0.5]),\n            \"p_s\": np.array([0.8, 0.2]),\n        },\n        # Case 2: shift toward longer isoform\n        {\n            \"L\": np.array([1000.0, 2000.0]),\n            \"p_r\": np.array([0.5, 0.5]),\n            \"p_s\": np.array([0.2, 0.8]),\n        },\n        # Case 3: three isoforms, pronounced swap from medium to short\n        {\n            \"L\": np.array([500.0, 1500.0, 3000.0]),\n            \"p_r\": np.array([0.2, 0.7, 0.1]),\n            \"p_s\": np.array([0.7, 0.2, 0.1]),\n        },\n        # Case 4: equal lengths, ratio change should not induce bias\n        {\n            \"L\": np.array([1000.0, 1000.0, 1000.0]),\n            \"p_r\": np.array([0.1, 0.8, 0.1]),\n            \"p_s\": np.array([0.7, 0.2, 0.1]),\n        },\n        # Case 5: identical ratios, no bias expected\n        {\n            \"L\": np.array([750.0, 1800.0, 2400.0]),\n            \"p_r\": np.array([0.3, 0.4, 0.3]),\n            \"p_s\": np.array([0.3, 0.4, 0.3]),\n        },\n        # Case 6: extreme swap toward much longer isoform\n        {\n            \"L\": np.array([500.0, 5000.0]),\n            \"p_r\": np.array([0.9, 0.1]),\n            \"p_s\": np.array([0.1, 0.9]),\n        }\n    ]\n\n    results_as_strings = []\n    for case in test_cases:\n        L = case[\"L\"]\n        p_r = case[\"p_r\"]\n        p_s = case[\"p_s\"]\n\n        # Calculate abundance-weighted average isoform length for reference (L_ref)\n        # and sample (L_s). Using dot product for weighted sum: sum(p_i * L_i)\n        L_ref = np.dot(p_r, L)\n        L_s = np.dot(p_s, L)\n\n        # The multiplicative bias factor B_s is the ratio of average lengths.\n        # This is derived in the solution text.\n        # Handle the case where L_ref is zero to avoid division by zero,\n        # although this is not expected with the given test cases.\n        if L_ref == 0:\n            # If L_ref is 0, this implies all isoforms have 0 length or 0 expression.\n            # If L_s is also 0, bias is undefined but can be treated as 1 (no change).\n            # If L_s is not 0, bias is infinite. This scenario is ill-defined.\n            # For this problem, we assume L_ref > 0.\n            B_s = 1.0 if L_s == 0 else np.inf\n        else:\n            B_s = L_s / L_ref\n\n        # The relative error delta_s is B_s - 1.\n        delta_s = B_s - 1.0\n\n        # Format the result pair [B_s, delta_s] as a string with each number\n        # rounded to six decimal places, per the output specification.\n        results_as_strings.append(f\"[{B_s:.6f},{delta_s:.6f}]\")\n\n    # Join the formatted string pairs with commas and enclose in brackets\n    # to create the final JSON-like output string.\n    final_output = f\"[{','.join(results_as_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2494870"}, {"introduction": "Functional genomics experiments are rarely about a single variable; often, we want to understand how multiple factors interact to shape a biological system. This practice problem places you in the role of analyzing a proteomic dataset from a classic $2 \\times 2$ factorial experiment, a powerful design for uncovering such interactions [@problem_id:2494904]. You will specify the appropriate linear model and derive the contrast needed to estimate the interaction effect, demonstrating how a well-structured experiment allows for the clear separation of biological signals from technical variation like batch effects.", "problem": "A microbiology laboratory uses Liquid Chromatography–Tandem Mass Spectrometry (LC-MS/MS) to quantify the $\\log_2$-intensity of a specific protein in a bacterium grown under a $2 \\times 2$ factorial design with two factors: carbon source (glucose versus acetate) and oxygen availability (aerobic versus anaerobic). Because LC-MS/MS runs are subject to batch effects, the experiment is executed in two batches, each containing all four treatment combinations with the same number of biological replicates per cell. Let the carbon source factor be encoded by $x_{C} \\in \\{-1,+1\\}$ with $+1$ for glucose and $-1$ for acetate, and oxygen availability by $x_{O} \\in \\{-1,+1\\}$ with $+1$ for aerobic and $-1$ for anaerobic. Assume that within each batch, there are $n$ biological replicates per treatment combination, and that measurements are on the $\\log_2$ scale.\n\nStarting from the core definition of a fixed-effects linear model for factorial designs and the principle that blocking accounts for systematic but non-interesting sources of variation, perform the following:\n\n1) Specify a linear model for the observed $\\log_2$-intensity $Y$ that includes an intercept, main effects for carbon source and oxygen availability, their interaction, and a fixed batch effect. Use the above effect coding for $x_{C}$ and $x_{O}$, and let the batch indicator be represented by fixed-effect parameters for the two batches. State the necessary identifiability constraints for the model parameters.\n\n2) Using the model in part $1)$ and the above effect coding, derive the contrast (as a linear combination of the four cell means) that estimates the carbon-by-oxygen interaction coefficient. Express this contrast symbolically in terms of the four cell means $\\bar{Y}_{++}$, $\\bar{Y}_{+-}$, $\\bar{Y}_{-+}$, and $\\bar{Y}_{--}$, where the first and second subscripts denote the signs of $x_{C}$ and $x_{O}$, respectively.\n\n3) The experiment was conducted in two LC-MS/MS batches with balanced replication ($n$ identical across all cells and batches). The within-batch sample means ($\\log_2$-intensities) for the four treatment combinations were:\n\nBatch $1$: glucose–aerobic ($\\bar{Y}^{(1)}_{++}$) $= 13.20$, glucose–anaerobic ($\\bar{Y}^{(1)}_{+-}$) $= 12.05$, acetate–aerobic ($\\bar{Y}^{(1)}_{-+}$) $= 12.65$, acetate–anaerobic ($\\bar{Y}^{(1)}_{--}$) $= 11.95$.\n\nBatch $2$: glucose–aerobic ($\\bar{Y}^{(2)}_{++}$) $= 13.55$, glucose–anaerobic ($\\bar{Y}^{(2)}_{+-}$) $= 12.10$, acetate–aerobic ($\\bar{Y}^{(2)}_{-+}$) $= 12.45$, acetate–anaerobic ($\\bar{Y}^{(2)}_{--}$) $= 12.05$.\n\nAssuming equal replication within each batch and cell, compute the estimated interaction coefficient $\\hat{\\gamma}$ from the model in part $1)$, adjusted for batch by virtue of the balanced blocking. Report only the numerical value of $\\hat{\\gamma}$ in $\\log_2$-scale units, and round your answer to four significant figures.", "solution": "The problem as stated is scientifically grounded, well-posed, and contains all necessary information to derive a unique solution. The experimental design described is a standard balanced $2 \\times 2$ factorial with blocking, and the analysis using a fixed-effects linear model is appropriate. I will proceed with the solution.\n\nFirst, the linear model must be specified. Let $Y_{ijkl}$ denote the observed log$_2$-intensity for the $i$-th biological replicate ($i=1, \\dots, n$), for the $j$-th level of carbon source ($x_{Cj} \\in \\{-1, +1\\}$), the $k$-th level of oxygen availability ($x_{Ok} \\in \\{-1, +1\\}$), in the $l$-th batch ($l \\in \\{1, 2\\}$). The model includes an intercept, main effects, a two-way interaction, and a fixed batch effect. The equation for the model is:\n$$Y_{ijkl} = \\beta_0 + \\beta_C x_{Cj} + \\beta_O x_{Ok} + \\gamma (x_{Cj} x_{Ok}) + B_l + \\epsilon_{ijkl}$$\nIn this model:\n- $\\beta_0$ is the grand mean, or overall intercept.\n- $\\beta_C$ is the main effect of the carbon source. For a change from acetate (coded as $-1$) to glucose (coded as $+1$), the expected change in $Y$ is $2\\beta_C$.\n- $\\beta_O$ is the main effect of oxygen availability. For a change from anaerobic (coded as $-1$) to aerobic (coded as $+1$), the expected change in $Y$ is $2\\beta_O$.\n- $\\gamma$ is the interaction coefficient for the carbon-by-oxygen interaction. It quantifies how the effect of one factor depends on the level of the other. The problem denotes this as $\\hat{\\gamma}$ for the estimate.\n- $B_l$ is the fixed effect of batch $l$. This term accounts for the systematic difference in average intensity between the two LC-MS/MS batches.\n- $\\epsilon_{ijkl}$ represents the independent and identically distributed random error for each measurement, assumed to have a mean of $0$ and a constant variance $\\sigma^2$.\n\nFor this model to be identifiable, the parameters must be uniquely determined. The presence of both a grand mean $\\beta_0$ and separate batch effects $B_1$ and $B_2$ makes the model overparameterized. A constraint must be imposed. A standard constraint for fixed effects in the presence of an intercept is the sum-to-zero constraint:\n$$B_1 + B_2 = 0$$\nThis constraint ensures that $\\beta_0$ represents the true grand mean across all conditions and batches, and the individual batch effects $B_l$ represent deviations from this grand mean.\n\nSecond, we derive the contrast to estimate the interaction coefficient $\\gamma$. The expected value of an observation for a given treatment combination $(j, k)$, averaged over the balanced batches, is required. The expected value for the mean of observations in cell $(j,k)$, denoted $\\bar{Y}_{jk}$, is:\n$$E[\\bar{Y}_{jk}] = E\\left[\\frac{1}{2n} \\sum_{l=1}^{2} \\sum_{i=1}^{n} Y_{ijkl}\\right] = \\frac{1}{2} \\sum_{l=1}^{2} (\\beta_0 + \\beta_C x_{Cj} + \\beta_O x_{Ok} + \\gamma (x_{Cj} x_{Ok}) + B_l)$$\n$$E[\\bar{Y}_{jk}] = \\beta_0 + \\beta_C x_{Cj} + \\beta_O x_{Ok} + \\gamma (x_{Cj} x_{Ok}) + \\frac{1}{2}(B_1+B_2)$$\nApplying the constraint $B_1+B_2=0$, the batch effects cancel out. The expected cell means are:\n$E[\\bar{Y}_{++}] = \\beta_0 + \\beta_C + \\beta_O + \\gamma$ (for $x_C=+1, x_O=+1$)\n$E[\\bar{Y}_{+-}] = \\beta_0 + \\beta_C - \\beta_O - \\gamma$ (for $x_C=+1, x_O=-1$)\n$E[\\bar{Y}_{-+}] = \\beta_0 - \\beta_C + \\beta_O - \\gamma$ (for $x_C=-1, x_O=+1$)\n$E[\\bar{Y}_{--}] = \\beta_0 - \\beta_C - \\beta_O + \\gamma$ (for $x_C=-1, x_O=-1$)\n\nTo isolate $\\gamma$, we form a specific linear combination of these expected means:\n$$(E[\\bar{Y}_{++}] + E[\\bar{Y}_{--}]) - (E[\\bar{Y}_{+-}] + E[\\bar{Y}_{-+}])$$\n$$= (\\beta_0 + \\beta_C + \\beta_O + \\gamma + \\beta_0 - \\beta_C - \\beta_O + \\gamma) - (\\beta_0 + \\beta_C - \\beta_O - \\gamma + \\beta_0 - \\beta_C + \\beta_O - \\gamma)$$\n$$= (2\\beta_0 + 2\\gamma) - (2\\beta_0 - 2\\gamma) = 4\\gamma$$\nFrom this, we find that $\\gamma = \\frac{1}{4} (E[\\bar{Y}_{++}] + E[\\bar{Y}_{--}] - E[\\bar{Y}_{+-}] - E[\\bar{Y}_{-+}])$. The estimator for $\\gamma$, denoted $\\hat{\\gamma}$, is found by substituting the observed sample means for the expected values:\n$$\\hat{\\gamma} = \\frac{1}{4} (\\bar{Y}_{++} + \\bar{Y}_{--} - \\bar{Y}_{+-} - \\bar{Y}_{-+})$$\nThis expression is the required contrast in terms of the four cell means.\n\nThird, we compute the numerical value for $\\hat{\\gamma}$. The design is balanced, so the overall mean for each treatment combination is the simple arithmetic mean of the means from the two batches.\nGiven data:\nBatch $1$: $\\bar{Y}^{(1)}_{++} = 13.20$, $\\bar{Y}^{(1)}_{+-} = 12.05$, $\\bar{Y}^{(1)}_{-+} = 12.65$, $\\bar{Y}^{(1)}_{--} = 11.95$.\nBatch $2$: $\\bar{Y}^{(2)}_{++} = 13.55$, $\\bar{Y}^{(2)}_{+-} = 12.10$, $\\bar{Y}^{(2)}_{-+} = 12.45$, $\\bar{Y}^{(2)}_{--} = 12.05$.\n\nThe overall cell means are calculated as:\n$\\bar{Y}_{++} = \\frac{13.20 + 13.55}{2} = \\frac{26.75}{2} = 13.375$\n$\\bar{Y}_{+-} = \\frac{12.05 + 12.10}{2} = \\frac{24.15}{2} = 12.075$\n$\\bar{Y}_{-+} = \\frac{12.65 + 12.45}{2} = \\frac{25.10}{2} = 12.55$\n$\\bar{Y}_{--} = \\frac{11.95 + 12.05}{2} = \\frac{24.00}{2} = 12.00$\n\nSubstituting these values into the formula for $\\hat{\\gamma}$:\n$$\\hat{\\gamma} = \\frac{1}{4} (13.375 + 12.00 - 12.075 - 12.55)$$\n$$\\hat{\\gamma} = \\frac{1}{4} (25.375 - 24.625)$$\n$$\\hat{\\gamma} = \\frac{1}{4} (0.75)$$\n$$\\hat{\\gamma} = 0.1875$$\nThe result, $0.1875$, is reported to four significant figures as requested.", "answer": "$$\\boxed{0.1875}$$", "id": "2494904"}]}