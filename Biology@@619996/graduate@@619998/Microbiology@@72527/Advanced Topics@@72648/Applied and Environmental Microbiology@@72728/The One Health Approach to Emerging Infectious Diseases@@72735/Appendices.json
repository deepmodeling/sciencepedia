{"hands_on_practices": [{"introduction": "The basic reproduction number, $R_0$, is a fundamental metric for assessing the epidemic potential of an infectious disease. For zoonotic pathogens that circulate in both human and animal populations, calculating $R_0$ requires methods that can handle cross-species transmission dynamics. This exercise [@problem_id:2539214] guides you through the construction of the Next-Generation Matrix (NGM), a standard and powerful technique to derive $R_0$ for multi-host systems, providing a quantitative foundation for understanding and controlling emerging infectious diseases.", "problem": "A novel zoonotic virus emerges in a region where humans and a sympatric animal reservoir species interact through environmental contact and occasional direct exposure. Following the One Health (OH) framework, consider a minimal susceptible–infectious–recovered model with two host populations (humans and animals). Assume frequency-dependent transmission between and within species, exponentially distributed infectious periods, and no vital dynamics over the short epidemic time horizon. Let $S_h$, $I_h$, $R_h$ denote the human susceptible, infectious, and recovered populations, respectively, and $S_a$, $I_a$, $R_a$ the analogous animal compartments. The total host population sizes are constant at $N_h$ and $N_a$, with disease-free equilibrium values $S_h^{\\ast} = N_h$ and $S_a^{\\ast} = N_a$.\n\nThe force of infection in humans is given by\n$$\n\\lambda_h = \\beta_{hh}\\frac{I_h}{N_h} + \\beta_{ha}\\frac{I_a}{N_a},\n$$\nand in animals by\n$$\n\\lambda_a = \\beta_{aa}\\frac{I_a}{N_a} + \\beta_{ah}\\frac{I_h}{N_h},\n$$\nwhere $\\beta_{ij}$ is the effective transmission coefficient from infectious host type $j$ to susceptible host type $i$. Infectious individuals recover at per-capita rates $\\gamma_h$ (humans) and $\\gamma_a$ (animals).\n\nStarting from the fundamental definition of the next-generation operator in a compartmental epidemic model, and using the disease-free equilibrium, construct the Next-Generation Matrix (NGM) $K$ for the infectious subsystem $(I_h, I_a)$ and compute the basic reproduction number ($R_0$) as the spectral radius (dominant eigenvalue) of $K$.\n\nUse the following scientifically plausible parameter values that reflect asymmetric cross-species transmission consistent with an emerging zoonosis:\n- $N_h = 1.2 \\times 10^{6}$,\n- $N_a = 1.0 \\times 10^{6}$,\n- $\\beta_{hh} = 0.08 \\text{ day}^{-1}$,\n- $\\beta_{aa} = 0.20 \\text{ day}^{-1}$,\n- $\\beta_{ha} = 0.10 \\text{ day}^{-1}$ (animal-to-human),\n- $\\beta_{ah} = 0.02 \\text{ day}^{-1}$ (human-to-animal),\n- mean infectious period in humans $= 7 \\text{ days}$ (so $\\gamma_h = 1/7 \\text{ day}^{-1}$),\n- mean infectious period in animals $= 5 \\text{ days}$ (so $\\gamma_a = 1/5 \\text{ day}^{-1}$).\n\nRound your final numerical value of $R_0$ to four significant figures. Express the final answer as a pure number without units.", "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n- A two-host susceptible–infectious–recovered (SIR) model for humans (h) and animals (a).\n- Total population sizes: $N_h$, $N_a$.\n- Disease-free equilibrium (DFE): $S_h^{\\ast} = N_h$, $S_a^{\\ast} = N_a$.\n- Force of infection in humans: $\\lambda_h = \\beta_{hh}\\frac{I_h}{N_h} + \\beta_{ha}\\frac{I_a}{N_a}$.\n- Force of infection in animals: $\\lambda_a = \\beta_{aa}\\frac{I_a}{N_a} + \\beta_{ah}\\frac{I_h}{N_h}$.\n- Transmission coefficients: $\\beta_{hh}$, $\\beta_{aa}$, $\\beta_{ha}$, $\\beta_{ah}$.\n- Per-capita recovery rates: $\\gamma_h$, $\\gamma_a$.\n- Parameter values:\n  - $N_h = 1.2 \\times 10^{6}$\n  - $N_a = 1.0 \\times 10^{6}$\n  - $\\beta_{hh} = 0.08 \\text{ day}^{-1}$\n  - $\\beta_{aa} = 0.20 \\text{ day}^{-1}$\n  - $\\beta_{ha} = 0.10 \\text{ day}^{-1}$\n  - $\\beta_{ah} = 0.02 \\text{ day}^{-1}$\n  - $\\gamma_h = 1/7 \\text{ day}^{-1}$\n  - $\\gamma_a = 1/5 \\text{ day}^{-1}$\n- The task is to construct the Next-Generation Matrix (NGM) $K$ for the infectious subsystem $(I_h, I_a)$ and compute the basic reproduction number $R_0 = \\rho(K)$ (spectral radius of $K$), rounded to four significant figures.\n\nValidation verdict:\nThe problem is scientifically grounded, employing the standard SIR framework and the established Next-Generation Matrix method for calculating the basic reproduction number. It is well-posed, providing all necessary parameters and a clear objective. The language is objective and precise. The model is self-contained and internally consistent. The parameter values are stated as plausible and do not create scientific or logical contradictions. The problem is a standard, non-trivial exercise in mathematical epidemiology. Therefore, the problem is deemed valid and a solution will be provided.\n\nThe dynamics of the infectious compartments for the two-host model are described by the following system of differential equations:\n$$\n\\frac{dI_h}{dt} = \\lambda_h S_h - \\gamma_h I_h = \\left(\\beta_{hh}\\frac{I_h}{N_h} + \\beta_{ha}\\frac{I_a}{N_a}\\right)S_h - \\gamma_h I_h\n$$\n$$\n\\frac{dI_a}{dt} = \\lambda_a S_a - \\gamma_a I_a = \\left(\\beta_{aa}\\frac{I_a}{N_a} + \\beta_{ah}\\frac{I_h}{N_h}\\right)S_a - \\gamma_a I_a\n$$\nTo compute the basic reproduction number, $R_0$, we use the next-generation method. This requires linearizing the system around the disease-free equilibrium (DFE), which is $(S_h, I_h, R_h, S_a, I_a, R_a) = (N_h, 0, 0, N_a, 0, 0)$.\n\nWe decompose the linearized system for the infectious compartments, $\\mathbf{I} = (I_h, I_a)^T$, into two parts: $\\frac{d\\mathbf{I}}{dt} = \\mathcal{F} - \\mathcal{V}$, where $\\mathcal{F}$ represents the rate of new infections and $\\mathcal{V}$ represents the rate of transitions between compartments (in this case, recovery).\n\nThe vector of new infection rates is:\n$$\n\\mathcal{F} = \\begin{pmatrix} \\left(\\beta_{hh}\\frac{I_h}{N_h} + \\beta_{ha}\\frac{I_a}{N_a}\\right)S_h \\\\ \\left(\\beta_{aa}\\frac{I_a}{N_a} + \\beta_{ah}\\frac{I_h}{N_h}\\right)S_a \\end{pmatrix}\n$$\nThe vector of transition rates is:\n$$\n\\mathcal{V} = \\begin{pmatrix} \\gamma_h I_h \\\\ \\gamma_a I_a \\end{pmatrix}\n$$\nThe next-generation method defines matrices $F$ and $V$ as the Jacobians of $\\mathcal{F}$ and $\\mathcal{V}$ with respect to the infectious variables, evaluated at the DFE.\n$$\nF = \\frac{\\partial \\mathcal{F}}{\\partial \\mathbf{I}}\\bigg|_{\\text{DFE}} \\quad \\text{and} \\quad V = \\frac{\\partial \\mathcal{V}}{\\partial \\mathbf{I}}\\bigg|_{\\text{DFE}}\n$$\nAt the DFE, $S_h = N_h$ and $S_a = N_a$.\nCalculating the partial derivatives for $F$:\n$$\nF_{11} = \\frac{\\partial \\mathcal{F}_h}{\\partial I_h}\\bigg|_{\\text{DFE}} = \\frac{\\beta_{hh}}{N_h}S_h\\bigg|_{\\text{DFE}} = \\beta_{hh}\n$$\n$$\nF_{12} = \\frac{\\partial \\mathcal{F}_h}{\\partial I_a}\\bigg|_{\\text{DFE}} = \\frac{\\beta_{ha}}{N_a}S_h\\bigg|_{\\text{DFE}} = \\beta_{ha}\\frac{N_h}{N_a}\n$$\n$$\nF_{21} = \\frac{\\partial \\mathcal{F}_a}{\\partial I_h}\\bigg|_{\\text{DFE}} = \\frac{\\beta_{ah}}{N_h}S_a\\bigg|_{\\text{DFE}} = \\beta_{ah}\\frac{N_a}{N_h}\n$$\n$$\nF_{22} = \\frac{\\partial \\mathcal{F}_a}{\\partial I_a}\\bigg|_{\\text{DFE}} = \\frac{\\beta_{aa}}{N_a}S_a\\bigg|_{\\text{DFE}} = \\beta_{aa}\n$$\nThus, the matrix $F$ is:\n$$\nF = \\begin{pmatrix} \\beta_{hh} & \\beta_{ha}\\frac{N_h}{N_a} \\\\ \\beta_{ah}\\frac{N_a}{N_h} & \\beta_{aa} \\end{pmatrix}\n$$\nThe matrix $V$ is a diagonal matrix of recovery rates:\n$$\nV = \\begin{pmatrix} \\gamma_h & 0 \\\\ 0 & \\gamma_a \\end{pmatrix}\n$$\nThe Next-Generation Matrix (NGM) is defined as $K = FV^{-1}$.\n$$\nV^{-1} = \\begin{pmatrix} \\frac{1}{\\gamma_h} & 0 \\\\ 0 & \\frac{1}{\\gamma_a} \\end{pmatrix}\n$$\n$$\nK = \\begin{pmatrix} \\beta_{hh} & \\beta_{ha}\\frac{N_h}{N_a} \\\\ \\beta_{ah}\\frac{N_a}{N_h} & \\beta_{aa} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\gamma_h} & 0 \\\\ 0 & \\frac{1}{\\gamma_a} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\beta_{hh}}{\\gamma_h} & \\frac{\\beta_{ha}}{\\gamma_a}\\frac{N_h}{N_a} \\\\ \\frac{\\beta_{ah}}{\\gamma_h}\\frac{N_a}{N_h} & \\frac{\\beta_{aa}}{\\gamma_a} \\end{pmatrix}\n$$\nThe basic reproduction number, $R_0$, is the spectral radius of $K$, denoted $\\rho(K)$. This is the dominant eigenvalue of $K$. The eigenvalues $\\lambda$ are found by solving the characteristic equation $\\det(K - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix} \\frac{\\beta_{hh}}{\\gamma_h} - \\lambda & \\frac{\\beta_{ha}}{\\gamma_a}\\frac{N_h}{N_a} \\\\ \\frac{\\beta_{ah}}{\\gamma_h}\\frac{N_a}{N_h} & \\frac{\\beta_{aa}}{\\gamma_a} - \\lambda \\end{pmatrix} = 0\n$$\n$$\n\\left(\\frac{\\beta_{hh}}{\\gamma_h} - \\lambda\\right)\\left(\\frac{\\beta_{aa}}{\\gamma_a} - \\lambda\\right) - \\left(\\frac{\\beta_{ha}}{\\gamma_a}\\frac{N_h}{N_a}\\right)\\left(\\frac{\\beta_{ah}}{\\gamma_h}\\frac{N_a}{N_h}\\right) = 0\n$$\nThis simplifies to:\n$$\n\\lambda^2 - \\left(\\frac{\\beta_{hh}}{\\gamma_h} + \\frac{\\beta_{aa}}{\\gamma_a}\\right)\\lambda + \\frac{\\beta_{hh}\\beta_{aa}}{\\gamma_h\\gamma_a} - \\frac{\\beta_{ha}\\beta_{ah}}{\\gamma_h\\gamma_a} = 0\n$$\nThe solutions for $\\lambda$ from this quadratic equation are given by the quadratic formula. $R_0$ is the larger of the two roots:\n$$\nR_0 = \\frac{1}{2} \\left[ \\left(\\frac{\\beta_{hh}}{\\gamma_h} + \\frac{\\beta_{aa}}{\\gamma_a}\\right) + \\sqrt{\\left(\\frac{\\beta_{hh}}{\\gamma_h} + \\frac{\\beta_{aa}}{\\gamma_a}\\right)^2 - 4\\left(\\frac{\\beta_{hh}\\beta_{aa}}{\\gamma_h\\gamma_a} - \\frac{\\beta_{ha}\\beta_{ah}}{\\gamma_h\\gamma_a}\\right)} \\right]\n$$\nSimplifying the term under the square root:\n$$\n\\sqrt{\\left(\\frac{\\beta_{hh}}{\\gamma_h} - \\frac{\\beta_{aa}}{\\gamma_a}\\right)^2 + 4\\frac{\\beta_{ha}\\beta_{ah}}{\\gamma_h\\gamma_a}}\n$$\nSo, the symbolic expression for $R_0$ is:\n$$\nR_0 = \\frac{1}{2} \\left( \\frac{\\beta_{hh}}{\\gamma_h} + \\frac{\\beta_{aa}}{\\gamma_a} + \\sqrt{\\left(\\frac{\\beta_{hh}}{\\gamma_h} - \\frac{\\beta_{aa}}{\\gamma_a}\\right)^2 + 4\\frac{\\beta_{ha}\\beta_{ah}}{\\gamma_h\\gamma_a}} \\right)\n$$\nNow we substitute the given numerical values:\n- $\\gamma_h = \\frac{1}{7} \\text{ day}^{-1}$, $\\gamma_a = \\frac{1}{5} \\text{ day}^{-1}$.\n- $\\frac{N_h}{N_a} = \\frac{1.2 \\times 10^6}{1.0 \\times 10^6} = 1.2$.\n\nFirst, we compute the components:\n- $R_{hh} = \\frac{\\beta_{hh}}{\\gamma_h} = \\frac{0.08}{1/7} = 0.56$.\n- $R_{aa} = \\frac{\\beta_{aa}}{\\gamma_a} = \\frac{0.20}{1/5} = 1.0$.\n- The cross-infection term: $4\\frac{\\beta_{ha}\\beta_{ah}}{\\gamma_h\\gamma_a} = 4\\frac{(0.10)(0.02)}{(1/7)(1/5)} = 4 \\times 0.002 \\times 35 = 4 \\times 0.07 = 0.28$.\n\nSubstitute these into the formula for $R_0$:\n$$\nR_0 = \\frac{1}{2} \\left( 0.56 + 1.0 + \\sqrt{(0.56 - 1.0)^2 + 0.28} \\right)\n$$\n$$\nR_0 = \\frac{1}{2} \\left( 1.56 + \\sqrt{(-0.44)^2 + 0.28} \\right)\n$$\n$$\nR_0 = \\frac{1}{2} \\left( 1.56 + \\sqrt{0.1936 + 0.28} \\right)\n$$\n$$\nR_0 = \\frac{1}{2} \\left( 1.56 + \\sqrt{0.4736} \\right)\n$$\n$$\nR_0 \\approx \\frac{1}{2} \\left( 1.56 + 0.688186 \\right)\n$$\n$$\nR_0 \\approx \\frac{1}{2} (2.248186)\n$$\n$$\nR_0 \\approx 1.124093\n$$\nRounding to four significant figures, the result is $1.124$.", "answer": "$$\n\\boxed{1.124}\n$$", "id": "2539214"}, {"introduction": "A central tenet of the One Health framework is that environmental conditions can drive the risk of zoonotic disease outbreaks. This hands-on practice [@problem_id:2539159] challenges you to develop a statistical workflow to test this hypothesis, using time series analysis to link environmental anomalies, such as changes in vegetation and rainfall, to disease case counts. By fitting and comparing nested Generalized Linear Models, you will learn a rigorous methodology for identifying environmental predictors of disease incidence while controlling for confounding factors like seasonality.", "problem": "You are given a task grounded in the One Health paradigm that integrates environmental signals with human zoonotic incidence. Let $t \\in \\{0,1,2,\\dots,N-1\\}$ index monthly time steps. Let $Y_t$ denote the monthly count of reported zoonotic cases, and let $V_t$ and $R_t$ denote monthly environmental signals respectively derived from remote-sensing vegetation indices and rainfall measurements. Assume that $Y_t$ are non-negative integers and that $V_t$ and $R_t$ are real-valued.\n\nYou must design and implement a workflow that tests the hypothesis that anomalies in the environmental signals precede increases in zoonotic case incidence after accounting for seasonality. Your program must operationalize the following fundamental ideas without relying on any shortcut formulas:\n\n- Count data generating mechanisms can be represented by a stochastic counting process with conditional mean $\\mu_t$.\n- Seasonality can be accounted for using a finite set of deterministic periodic regressors (for example, a Fourier basis with annual period), and anomalies can be defined by removing calendar-month means and scaling by calendar-month standard deviations.\n- A principled statistical test of association compares a model containing lagged environmental anomalies to a reduced model that omits them, while keeping seasonality controls identical, and evaluates whether the inclusion of lagged anomalies yields a statistically significant improvement in fit and at least one directionally positive effect at a strictly positive lag.\n\nYour program must implement the following steps in purely mathematical and algorithmic terms:\n\n1. Construct environmental anomalies. For each month-of-year $m \\in \\{0,1,\\dots,11\\}$, compute the calendar-month mean $\\overline{V}_m$ and standard deviation $s_{V,m}$ for $\\{V_t : t \\bmod 12 = m\\}$, and analogously $\\overline{R}_m$ and $s_{R,m}$ for $\\{R_t\\}$. Define standardized anomalies $A^V_t := (V_t - \\overline{V}_{t \\bmod 12})/\\max(s_{V,t \\bmod 12},\\varepsilon)$ and $A^R_t := (R_t - \\overline{R}_{t \\bmod 12})/\\max(s_{R,t \\bmod 12},\\varepsilon)$, for a small stabilizer $\\varepsilon > 0$ specified by you.\n2. Build a regression design. Include an intercept, annual Fourier terms $\\sin(2\\pi t/12)$, $\\cos(2\\pi t/12)$, $\\sin(4\\pi t/12)$, $\\cos(4\\pi t/12)$, and an optional linear trend term $t/N$. Add columns for a chosen set of strictly positive lags $\\mathcal{L} \\subset \\{1,2,3\\}$ applied to both $A^V_t$ and $A^R_t$. To ensure strict precedence, do not include non-lagged ($0$-lag) environmental terms. Discard the first $\\max(\\mathcal{L})$ rows so that all lagged values are defined.\n3. Fit two nested models for the conditional mean $\\mu_t$ of $Y_t$ at each remaining time index $t$:\n   - A reduced model that depends only on the intercept and the seasonal (and trend) regressors.\n   - A full model that augments the reduced model with the lagged anomaly regressors for all lags in $\\mathcal{L}$ and for both environmental series.\n   Treat $Y_t$ as arising from a Poisson Generalized Linear Model (GLM) with a logarithmic link, so that $\\log \\mu_t$ is linear in the chosen regressors. Estimate all coefficients by maximum likelihood.\n4. Perform a likelihood-ratio test comparing the full and reduced models. Let $D_{\\mathrm{red}}$ and $D_{\\mathrm{full}}$ be the twice-negative-log-likelihood-based deviances of the reduced and full models, respectively. Compute the test statistic $\\Delta D := D_{\\mathrm{red}} - D_{\\mathrm{full}}$ and the degrees of freedom $\\Delta p$ equal to the number of environmental lag coefficients added in the full model. Under the null hypothesis that the lagged environmental anomalies have no effect, $\\Delta D$ is approximately $\\chi^2_{\\Delta p}$-distributed. Compute the corresponding tail probability as a decimal (not a percentage).\n5. In addition, for each individual environmental lag coefficient, compute a one-sided Wald test for the hypothesis that the coefficient is strictly positive. Declare “directionally positive” if at least one lagged environmental coefficient has a positive estimate and a one-sided $p$-value below your specified significance level $\\alpha$.\n6. Decision rule: Return a boolean for each dataset equal to true if and only if both criteria are satisfied simultaneously: (i) the likelihood-ratio test tail probability is strictly less than $\\alpha$, and (ii) at least one environmental lag has a significantly positive one-sided Wald test at the same $\\alpha$.\n\nData generation for the test suite. To allow automated evaluation, generate synthetic datasets using the following mechanism that reflects One Health reasoning that environmental changes can precede zoonotic cases via ecological pathways:\n\n- Let $N$ denote the number of months.\n- Generate $V_t$ and $R_t$ as smooth seasonal signals with superimposed noise, then transform them into anomalies as in step $1$.\n- Let the latent linear predictor for the case process be\n  $\\eta_t = \\log(\\lambda_0) + A_{\\mathrm{cases}}\\sin(2\\pi t/12) + \\beta_V A^V_{t - L_V} + \\beta_R A^R_{t - L_R}$,\n  with the convention that terms with undefined lags are zero. Set the conditional mean to $\\mu_t := \\exp(\\eta_t)$ and sample $Y_t$ from a Poisson distribution with mean $\\mu_t$.\n- Here, $\\lambda_0 > 0$ is a baseline rate, $A_{\\mathrm{cases}}$ is a seasonal amplitude on the log-scale for cases, $\\beta_V$ and $\\beta_R$ are true effect sizes for vegetation and rainfall anomalies, and $L_V, L_R \\in \\{1,2,3\\}$ are strictly positive lags in months.\n\nSignificance level. Use $\\alpha = 0.05$ as a decimal.\n\nAngle units. All trigonometric arguments are in radians.\n\nTest suite. Your program must apply the above workflow to the following four synthetic scenarios, each specified as a tuple of parameters $(N,\\ \\beta_V,\\ \\beta_R,\\ L_V,\\ L_R,\\ \\lambda_0,\\ A_{\\mathrm{cases}},\\ \\sigma_{\\mathrm{env}},\\ \\text{seed})$:\n\n- Case $1$ (happy path): $(120,\\ 0.18,\\ 0.0,\\ 2,\\ 1,\\ 2.0,\\ 0.6,\\ 0.4,\\ 12345)$.\n- Case $2$ (null): $(120,\\ 0.0,\\ 0.0,\\ 2,\\ 1,\\ 2.0,\\ 0.8,\\ 0.4,\\ 2021)$.\n- Case $3$ (seasonality-only confounding controlled by anomalies): $(120,\\ 0.0,\\ 0.0,\\ 2,\\ 1,\\ 2.0,\\ 1.0,\\ 0.6,\\ 777)$.\n- Case $4$ (short series with true effect): $(48,\\ 0.35,\\ 0.0,\\ 1,\\ 1,\\ 1.5,\\ 0.5,\\ 0.4,\\ 314159)$.\n\nImplementation requirements:\n\n- Use $\\mathcal{L}=\\{1,2,3\\}$ as the set of tested lags in the full model for all cases.\n- Use an $\\varepsilon$ stabilizer of your choice for anomaly standardization and any numerically reasonable regularization for optimization, but you must justify these in your solution.\n- Your program must produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets (e.g., $[true,false,true,false]$), one boolean per case in the order listed above. Use Python boolean literals $True$ and $False$.\n\nYour final output must be exactly one line with the list format described above. No other text may be printed.", "solution": "The problem presented is a well-defined and scientifically grounded task in quantitative environmental epidemiology. It requires the implementation of a statistical workflow to test for a predictive relationship between environmental factors and zoonotic disease incidence, a central theme in the One Health paradigm. All provided information is self-contained and logically consistent. The problem is therefore deemed valid and a full solution shall be provided.\n\nOur objective is to test the hypothesis that anomalies in environmental signals, specifically vegetation ($V_t$) and rainfall ($R_t$), are predictive of subsequent increases in the incidence of a zoonotic disease ($Y_t$). The analysis must control for underlying seasonality and long-term trends. The methodology is structured as a comparison between nested statistical models using a Poisson Generalized Linear Model (GLM) framework, appropriate for count data such as disease case reports.\n\nThe workflow proceeds in several articulated steps: data synthesis, anomaly construction, regression modeling, and hypothesis testing.\n\n### 1. Synthetic Data Generation\n\nTo test the analytical pipeline, we first generate synthetic time series data according to the parameters specified for each test case. Each dataset is defined by the tuple $(N, \\beta_V, \\beta_R, L_V, L_R, \\lambda_0, A_{\\text{cases}}, \\sigma_{\\text{env}}, \\text{seed})$.\n\nThe environmental signals $V_t$ and $R_t$ are generated as seasonal signals with additive Gaussian noise. We define them as:\n$$ V_t = 5 \\sin\\left(\\frac{2\\pi t}{12}\\right) + \\epsilon_{V,t} $$\n$$ R_t = 3 \\cos\\left(\\frac{2\\pi t}{12}\\right) + \\epsilon_{R,t} $$\nwhere $\\epsilon_{V,t}, \\epsilon_{R,t}$ are independent noise terms drawn from a normal distribution $\\mathcal{N}(0, \\sigma_{\\text{env}}^2)$. The specific amplitudes ($5$ and $3$) and phases ($0$ and $\\pi/2$) are chosen as fixed, reasonable values; their absolute magnitude is rendered irrelevant by the subsequent standardization step.\n\nNext, we compute the standardized environmental anomalies, $A^V_t$ and $A^R_t$, as detailed in the following section.\n\nFinally, the zoonotic case counts $Y_t$ are generated from a Poisson process. The conditional mean $\\mu_t$ of the process is determined by a log-linear model incorporating seasonality and the true lagged effects of the environmental anomalies:\n$$ \\eta_t = \\log(\\lambda_0) + A_{\\text{cases}}\\sin\\left(\\frac{2\\pi t}{12}\\right) + \\beta_V A^V_{t - L_V} + \\beta_R A^R_{t - L_R} $$\n$$ \\mu_t = \\exp(\\eta_t) $$\n$$ Y_t \\sim \\text{Poisson}(\\mu_t) $$\nBy convention, any term with an undefined lag (i.e., where $t - L < 0$) is set to zero. This generative model provides a ground truth against which our analytical method can be validated.\n\n### 2. Anomaly Construction\n\nAnomalies are defined as deviations from the expected value for a given time of year. For each month-of-year $m \\in \\{0, 1, \\dots, 11\\}$, we compute the empirical mean and standard deviation of the environmental signals over all years in the dataset. For the vegetation index $V_t$, these are:\n$$ \\overline{V}_m = \\text{mean}\\{V_t : t \\bmod 12 = m\\} $$\n$$ s_{V,m} = \\text{std}\\{V_t : t \\bmod 12 = m\\} $$\nThe standardized anomaly $A^V_t$ is then calculated as:\n$$ A^V_t = \\frac{V_t - \\overline{V}_{t \\bmod 12}}{\\max(s_{V,t \\bmod 12}, \\varepsilon)} $$\nAn analogous procedure yields the rainfall anomalies $A^R_t$. We use a small stabilizing constant $\\varepsilon = 10^{-8}$ to prevent division by zero in the unlikely event that an environmental signal exhibits no variance for a specific calendar month. This value is small enough to be inconsequential otherwise.\n\n### 3. Regression Model Specification\n\nWe test the hypothesis by comparing two nested models. The analysis is restricted to time points $t \\in \\{\\max(\\mathcal{L}), \\dots, N-1\\}$, where $\\mathcal{L} = \\{1, 2, 3\\}$ is the set of lags being tested, ensuring all lagged predictors are defined.\n\nThe outcome variable $Y_t$ is modeled as $Y_t \\sim \\text{Poisson}(\\mu_t)$ with a logarithmic link function, $\\log(\\mu_t) = \\mathbf{x}_t^\\top \\boldsymbol{\\beta}$.\n\n**Reduced Model ($M_{\\text{red}}$):** This is the null model, which includes only control variables for seasonality and trend. The predictor vector $\\mathbf{x}_{t, \\text{red}}$ for time $t$ is:\n$$ \\mathbf{x}_{t, \\text{red}}^\\top = \\left[ 1, \\sin\\left(\\frac{2\\pi t}{12}\\right), \\cos\\left(\\frac{2\\pi t}{12}\\right), \\sin\\left(\\frac{4\\pi t}{12}\\right), \\cos\\left(\\frac{4\\pi t}{12}\\right), \\frac{t}{N} \\right] $$\nThis includes an intercept, two pairs of annual and semi-annual Fourier terms to capture seasonality, and a linear trend term to account for secular changes.\n\n**Full Model ($M_{\\text{full}}$):** This is the alternative model, which augments the reduced model with lagged environmental anomaly terms. The predictor vector $\\mathbf{x}_{t, \\text{full}}$ is:\n$$ \\mathbf{x}_{t, \\text{full}}^\\top = \\left[ \\mathbf{x}_{t, \\text{red}}^\\top, A^V_{t-1}, A^V_{t-2}, A^V_{t-3}, A^R_{t-1}, A^R_{t-2}, A^R_{t-3} \\right] $$\nThe full model has $p_{\\text{full}} = 12$ parameters, while the reduced model has $p_{\\text{red}} = 6$. The number of additional parameters is $\\Delta p = p_{\\text{full}} - p_{\\text{red}} = 6$.\n\n### 4. Maximum Likelihood Estimation\n\nThe vectors of regression coefficients, $\\boldsymbol{\\beta}_{\\text{red}}$ and $\\boldsymbol{\\beta}_{\\text{full}}$, are estimated by maximizing the log-likelihood function of the Poisson model. For a generic model with design matrix $\\mathbf{X}$ and coefficient vector $\\boldsymbol{\\beta}$, the log-likelihood is:\n$$ \\ell(\\boldsymbol{\\beta}; \\mathbf{Y}, \\mathbf{X}) = \\sum_{t} \\left( Y_t (\\mathbf{x}_t^\\top \\boldsymbol{\\beta}) - \\exp(\\mathbf{x}_t^\\top \\boldsymbol{\\beta}) - \\log(Y_t!) \\right) $$\nMaximizing this is equivalent to minimizing the negative log-likelihood (NLL), ignoring the constant term $\\sum_t \\log(Y_t!)$:\n$$ \\text{NLL}(\\boldsymbol{\\beta}) = \\sum_{t} \\left( \\exp(\\mathbf{x}_t^\\top \\boldsymbol{\\beta}) - Y_t (\\mathbf{x}_t^\\top \\boldsymbol{\\beta}) \\right) $$\nWe find the maximum likelihood estimate $\\hat{\\boldsymbol{\\beta}}$ by numerically minimizing this function using the L-BFGS-B algorithm, a quasi-Newton method well-suited for this purpose. The algorithm requires the gradient of the NLL, which is:\n$$ \\nabla_{\\boldsymbol{\\beta}} \\text{NLL}(\\boldsymbol{\\beta}) = \\sum_{t} \\left( \\exp(\\mathbf{x}_t^\\top \\boldsymbol{\\beta})\\mathbf{x}_t - Y_t \\mathbf{x}_t \\right) = \\mathbf{X}^\\top (\\boldsymbol{\\mu} - \\mathbf{Y}) $$\nwhere $\\boldsymbol{\\mu} = \\exp(\\mathbf{X}\\boldsymbol{\\beta})$. We perform this optimization separately for the reduced and full models to obtain $\\hat{\\boldsymbol{\\beta}}_{\\text{red}}$ and $\\hat{\\boldsymbol{\\beta}}_{\\text{full}}$, along with their corresponding minimized NLL values, $\\text{NLL}_{\\text{red}}$ and $\\text{NLL}_{\\text{full}}$.\n\n### 5. Hypothesis Testing\n\nThe significance of the environmental anomalies is assessed via two statistical tests.\n\n**Likelihood-Ratio Test (LRT):** This test assesses the overall significance of adding the block of environmental predictors. The test statistic $\\Delta D$ is the difference in the deviances of the models, which is equivalent to twice the difference in the log-likelihoods:\n$$ \\Delta D = 2 \\left( \\text{NLL}(\\hat{\\boldsymbol{\\beta}}_{\\text{red}}) - \\text{NLL}(\\hat{\\boldsymbol{\\beta}}_{\\text{full}}) \\right) $$\nUnder the null hypothesis that all additional coefficients in the full model are zero, $\\Delta D$ follows a chi-squared distribution with $\\Delta p = 6$ degrees of freedom. We compute the p-value as the tail probability $P(\\chi^2_{\\Delta p} \\ge \\Delta D)$.\n\n**One-Sided Wald Test:** This test assesses the significance and direction of individual environmental lag coefficients. For each environmental coefficient $\\hat{\\beta}_j$ in the full model, we test the one-sided hypothesis $H_0: \\beta_j \\le 0$ versus $H_1: \\beta_j > 0$. The Wald test statistic is:\n$$ z_j = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)} $$\nwhere $\\text{SE}(\\hat{\\beta}_j)$ is the standard error of the coefficient estimate. The standard errors are derived from the diagonal of the inverse of the observed Fisher information matrix, which is the negative of the Hessian of the log-likelihood evaluated at the MLE, $\\hat{\\boldsymbol{\\beta}}_{\\text{full}}$. The Hessian of the NLL is given by $\\mathbf{H} = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$, where $\\mathbf{W}$ is a diagonal matrix with entries $W_{tt} = \\hat{\\mu}_t = \\exp(\\mathbf{x}_t^\\top \\hat{\\boldsymbol{\\beta}}_{\\text{full}})$. The covariance matrix of the coefficients is thus $\\mathbf{Cov}(\\hat{\\boldsymbol{\\beta}}_{\\text{full}}) = \\mathbf{H}^{-1}$. The one-sided p-value is calculated as $P(Z \\ge z_j)$ where $Z \\sim \\mathcal{N}(0,1)$.\n\n### 6. Decision Criterion\n\nA positive association is declared (returning `True`) if and only if both of the following conditions are met, using a significance level of $\\alpha = 0.05$:\n1. The likelihood-ratio test is significant: LRT p-value $< 0.05$.\n2. At least one of the six environmental lag coefficients exhibits a directionally positive effect: $\\hat{\\beta}_j > 0$ and its one-sided Wald p-value is $< 0.05$.\n\nThis composite criterion ensures that the environmental variables as a group provide a statistically significant improvement in model fit, and that this improvement is driven by at least one physically interpretable positive association, where an increase in an environmental anomaly precedes an increase in disease cases.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2, norm\n\ndef solve():\n    \"\"\"\n    Implements the full workflow for testing the association between environmental\n    anomalies and zoonotic case incidence for a suite of synthetic datasets.\n    \"\"\"\n    \n    test_cases = [\n        # (N, beta_V, beta_R, L_V, L_R, lambda_0, A_cases, sigma_env, seed)\n        (120, 0.18, 0.0, 2, 1, 2.0, 0.6, 0.4, 12345),\n        (120, 0.0, 0.0, 2, 1, 2.0, 0.8, 0.4, 2021),\n        (120, 0.0, 0.0, 2, 1, 2.0, 1.0, 0.6, 777),\n        (48, 0.35, 0.0, 1, 1, 1.5, 0.5, 0.4, 314159),\n    ]\n\n    results = []\n    \n    # Parameters for analysis\n    ALPHA = 0.05\n    EPSILON = 1e-8\n    LAGS_TO_TEST = [1, 2, 3]\n    MAX_LAG = max(LAGS_TO_TEST)\n\n    for case in test_cases:\n        N, beta_V, beta_R, L_V, L_R, lambda_0, A_cases, sigma_env, seed = case\n        \n        # Set seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Generate synthetic data\n        t = np.arange(N)\n        \n        # Generate environmental signals\n        V_t = 5 * np.sin(2 * np.pi * t / 12) + np.random.normal(0, sigma_env, N)\n        R_t = 3 * np.cos(2 * np.pi * t / 12) + np.random.normal(0, sigma_env, N)\n        \n        # --- Anomaly Construction ---\n        A_V = np.zeros(N)\n        A_R = np.zeros(N)\n        months = t % 12\n        for m in range(12):\n            mask = (months == m)\n            if np.sum(mask) > 1:\n                v_mean, v_std = np.mean(V_t[mask]), np.std(V_t[mask])\n                r_mean, r_std = np.mean(R_t[mask]), np.std(R_t[mask])\n                A_V[mask] = (V_t[mask] - v_mean) / np.maximum(v_std, EPSILON)\n                A_R[mask] = (R_t[mask] - r_mean) / np.maximum(r_std, EPSILON)\n\n        # Generate zoonotic cases Y_t\n        eta_t = np.log(lambda_0) + A_cases * np.sin(2 * np.pi * t / 12)\n        if beta_V != 0 and L_V > 0:\n            eta_t[L_V:] += beta_V * A_V[:-L_V]\n        if beta_R != 0 and L_R > 0:\n            eta_t[L_R:] += beta_R * A_R[:-L_R]\n        \n        mu_t = np.exp(eta_t)\n        Y_t = np.random.poisson(mu_t)\n\n        # --- Analysis on Generated Data ---\n        \n        # 2. Build regression design\n        effective_t = np.arange(MAX_LAG, N)\n        Y_eff = Y_t[MAX_LAG:]\n        \n        # Reduced model design matrix\n        X_red = np.vstack([\n            np.ones(N - MAX_LAG),\n            np.sin(2 * np.pi * effective_t / 12),\n            np.cos(2 * np.pi * effective_t / 12),\n            np.sin(4 * np.pi * effective_t / 12),\n            np.cos(4 * np.pi * effective_t / 12),\n            effective_t / N\n        ]).T\n\n        # Full model design matrix\n        lag_features = []\n        for lag in LAGS_TO_TEST:\n            lag_features.append(A_V[MAX_LAG - lag : N - lag])\n        for lag in LAGS_TO_TEST:\n            lag_features.append(A_R[MAX_LAG - lag : N - lag])\n        \n        X_full = np.hstack([X_red] + [f.reshape(-1, 1) for f in lag_features])\n\n        # 3. Fit models via MLE\n        \n        def nll(beta, X, y):\n            mu = np.exp(X @ beta)\n            return np.sum(mu - y * (X @ beta))\n\n        def grad_nll(beta, X, y):\n            mu = np.exp(X @ beta)\n            return X.T @ (mu - y)\n\n        beta_init_red = np.zeros(X_red.shape[1])\n        res_red = minimize(nll, beta_init_red, args=(X_red, Y_eff), method='L-BFGS-B', jac=grad_nll)\n        nll_red = res_red.fun\n\n        beta_init_full = np.zeros(X_full.shape[1])\n        res_full = minimize(nll, beta_init_full, args=(X_full, Y_eff), method='L-BFGS-B', jac=grad_nll)\n        nll_full = res_full.fun\n        beta_hat_full = res_full.x\n        \n        # 4. Perform likelihood-ratio test\n        delta_D = 2 * (nll_red - nll_full)\n        df = X_full.shape[1] - X_red.shape[1]\n        lrt_pval = chi2.sf(delta_D, df)\n        \n        lrt_significant = lrt_pval < ALPHA\n        \n        # 5. Perform Wald tests for environmental lags\n        any_wald_positive_significant = False\n        if lrt_significant: # Optimization: only compute if LRT is significant\n            try:\n                mu_hat_full = np.exp(X_full @ beta_hat_full)\n                W = np.diag(mu_hat_full)\n                hessian = X_full.T @ W @ X_full\n                cov_matrix = np.linalg.inv(hessian)\n                std_errors = np.sqrt(np.diag(cov_matrix))\n                \n                env_coeffs = beta_hat_full[-df:]\n                env_ses = std_errors[-df:]\n                \n                for i in range(df):\n                    if env_coeffs[i] > 0:\n                        z_score = env_coeffs[i] / env_ses[i]\n                        wald_pval_one_sided = norm.sf(z_score)\n                        if wald_pval_one_sided < ALPHA:\n                            any_wald_positive_significant = True\n                            break\n            except np.linalg.LinAlgError:\n                # If Hessian is singular, cannot perform Wald tests.\n                any_wald_positive_significant = False\n        \n        # 6. Decision rule\n        final_decision = lrt_significant and any_wald_positive_significant\n        results.append(final_decision)\n\n    # Format the final output\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "2539159"}, {"introduction": "Integrating pathogen genomics into outbreak response provides unprecedented power to reconstruct transmission pathways, a cornerstone of modern One Health surveillance. This practice [@problem_id:2539206] simulates this process, tasking you with building a simplified Bayesian model to infer transmission links from a dated phylogeny and host metadata. By combining temporal, phylogenetic, and prior information, you will compute posterior probabilities for transmission routes and estimate the expected number of cross-species events, a key metric for understanding zoonotic dynamics.", "problem": "You are given small, stylized outbreak datasets that encode a dated phylogeny via a symmetric matrix of Most Recent Common Ancestor (MRCA) times and host meta-data for each sampled case. For each dataset, you must reconstruct the likely cross-species transmission structure by computing, for each recipient case, a posterior probability distribution over its possible donors among earlier cases. You must then aggregate these per-recipient posteriors to obtain the expected number of cross-species transmission events. This task formalizes a One Health inference problem by integrating multi-host serial interval information with phylogenetic timing constraints using Bayesian principles.\n\nFundamental base:\n- Bayes rule: for hypotheses $H_i$ and data $D$, $P(H_i \\mid D) \\propto P(H_i) \\, P(D \\mid H_i)$.\n- The serial interval (SI) is the time from a donor’s symptom onset (or sampling) to a recipient’s symptom onset (or sampling). We model SI with a Lognormal distribution whose parameters are derived from specified mean and standard deviation values per host-to-host pair.\n- A dated phylogeny constrains transmission via MRCA times: the more recent the MRCA relative to the donor’s sampling time, the more compatible the pair is with direct transmission.\n\nModel assumptions and definitions:\n- Let $i$ and $j$ index cases with sampling times $t_i$ and $t_j$ (in days). Let $s_i$ and $s_j$ denote their host species, and let $m_{ij}$ be the MRCA time (in days) for the pair $\\{i,j\\}$.\n- Candidate donors for recipient $j$ are restricted to cases $i$ such that $t_i &lt; t_j$.\n- For each ordered host pair $(s_i \\rightarrow s_j)$, the serial interval is modeled as Lognormal with mean $\\mu_{s_i \\rightarrow s_j}$ and standard deviation $\\sigma_{s_i \\rightarrow s_j}$ (both in days). For a realized inter-sampling time $\\tau_{ij} = t_j - t_i$, the temporal likelihood is $L_{\\text{time}}(i \\rightarrow j) = f_{\\text{LN}}(\\tau_{ij} \\,;\\, \\mu_{s_i \\rightarrow s_j}, \\sigma_{s_i \\rightarrow s_j})$, where $f_{\\text{LN}}$ denotes the Lognormal probability density function parameterized by its mean and standard deviation (see below).\n- A phylogenetic compatibility weight penalizes donors whose lineages coalesce too far before the donor’s sampling time. Define a tolerance window $\\Delta$ (days) and a decay rate $\\alpha$ (per day). The phylogenetic weight is\n$$\nL_{\\text{phylo}}(i \\rightarrow j) \\;=\\; \\exp\\!\\Big(-\\alpha \\, \\max\\!\\big(0,\\; (t_i - m_{ij}) - \\Delta\\big)\\Big).\n$$\n- The prior over candidate donors incorporates a cross-species barrier factor $b \\in (0,1]$: for a candidate $i$ for recipient $j$,\n$$\nP_0(i \\rightarrow j) \\;\\propto\\; \\begin{cases}\n1, & \\text{if } s_i = s_j,\\\\\nb, & \\text{if } s_i \\neq s_j.\n\\end{cases}\n$$\n- The unnormalized posterior weight for $i \\rightarrow j$ is\n$$\nw_{ij} \\;=\\; P_0(i \\rightarrow j)\\, L_{\\text{time}}(i \\rightarrow j)\\, L_{\\text{phylo}}(i \\rightarrow j).\n$$\nNormalize $\\{w_{ij}\\}_{i: t_i &lt; t_j}$ over all candidates $i$ to obtain posterior probabilities $P(i \\rightarrow j \\mid \\text{data})$ that sum to $1$ for each $j$ with at least one candidate donor. If a $j$ has no candidates, it contributes $0$ to the cross-species expectation below.\n\nObjective:\n- For each dataset, compute the expected number of cross-species transmission events by summing, over all recipients $j$, the posterior probability that their donor is of a different species:\n$$\n\\mathbb{E}[\\text{cross-species events}] \\;=\\; \\sum_{j} \\sum_{i: t_i &lt; t_j,\\, s_i \\neq s_j} P(i \\rightarrow j \\mid \\text{data}).\n$$\n\nLognormal parameterization:\n- Given a Lognormal with mean $\\mu &gt; 0$ and standard deviation $\\sigma &gt; 0$ in real time, the corresponding Normal parameters for $\\ln X$ are\n$$\n\\tilde{\\mu} \\;=\\; \\ln\\!\\Big(\\frac{\\mu^2}{\\sqrt{\\sigma^2 + \\mu^2}}\\Big),\\quad\n\\tilde{\\sigma} \\;=\\; \\sqrt{\\ln\\!\\Big(1 + \\frac{\\sigma^2}{\\mu^2}\\Big)}.\n$$\nThe Lognormal density at $x &gt; 0$ is\n$$\nf_{\\text{LN}}(x;\\mu,\\sigma) \\;=\\; \\frac{1}{x \\, \\tilde{\\sigma}\\,\\sqrt{2\\pi}} \\exp\\!\\Big(-\\frac{(\\ln x - \\tilde{\\mu})^2}{2\\,\\tilde{\\sigma}^2}\\Big).\n$$\nSet $f_{\\text{LN}}(x;\\mu,\\sigma)=0$ for $x \\le 0$.\n\nData and test suite (all times in days; species are Human (H) and Swine (S)):\n\n- Test case $1$:\n  - Cases $n = 5$ with sampling times $t = [\\,2.0,\\,6.0,\\,9.5,\\,12.0,\\,15.0\\,]$ and species $[\\,\\mathrm{H},\\,\\mathrm{H},\\,\\mathrm{S},\\,\\mathrm{S},\\,\\mathrm{H}\\,]$ corresponding to indices $1$ through $5$.\n  - Symmetric MRCA time matrix $M^{(1)}$ (entries $m^{(1)}_{ij}$ for $i &lt; j$):\n    - $m^{(1)}_{12}=1.5$, $m^{(1)}_{13}=1.0$, $m^{(1)}_{14}=0.8$, $m^{(1)}_{15}=1.2$,\n    - $m^{(1)}_{23}=3.0$, $m^{(1)}_{24}=2.5$, $m^{(1)}_{25}=3.5$,\n    - $m^{(1)}_{34}=8.0$, $m^{(1)}_{35}=6.0$,\n    - $m^{(1)}_{45}=10.0$.\n    Diagonal entries $m^{(1)}_{ii}$ are unused.\n  - Serial interval means and standard deviations $(\\mu,\\sigma)$ in days:\n    - $\\mathrm{H}\\rightarrow \\mathrm{H}$: $(4.0,\\,2.0)$,\n    - $\\mathrm{H}\\rightarrow \\mathrm{S}$: $(6.0,\\,2.5)$,\n    - $\\mathrm{S}\\rightarrow \\mathrm{H}$: $(7.0,\\,3.0)$,\n    - $\\mathrm{S}\\rightarrow \\mathrm{S}$: $(5.0,\\,2.0)$.\n  - Cross-species barrier $b = 0.5$, phylogenetic tolerance $\\Delta = 3.0$, decay rate $\\alpha = 0.25$.\n\n- Test case $2$:\n  - Cases $n = 5$ with sampling times $t = [\\,1.0,\\,4.0,\\,7.0,\\,10.0,\\,11.0\\,]$ and species $[\\,\\mathrm{H},\\,\\mathrm{S},\\,\\mathrm{H},\\,\\mathrm{S},\\,\\mathrm{S}\\,]$.\n  - Symmetric MRCA time matrix $M^{(2)}$:\n    - $m^{(2)}_{12}=0.9$, $m^{(2)}_{13}=0.8$, $m^{(2)}_{14}=0.7$, $m^{(2)}_{15}=0.7$,\n    - $m^{(2)}_{23}=3.8$, $m^{(2)}_{24}=3.2$, $m^{(2)}_{25}=3.5$,\n    - $m^{(2)}_{34}=6.0$, $m^{(2)}_{35}=6.2$,\n    - $m^{(2)}_{45}=9.0$.\n  - Serial intervals as in test case $1$.\n  - Cross-species barrier $b = 0.7$, phylogenetic tolerance $\\Delta = 2.0$, decay rate $\\alpha = 0.4$.\n\n- Test case $3$ (edge conditions):\n  - Cases $n = 4$ with sampling times $t = [\\,3.0,\\,3.0,\\,9.0,\\,20.0\\,]$ and species $[\\,\\mathrm{H},\\,\\mathrm{S},\\,\\mathrm{H},\\,\\mathrm{S}\\,]$.\n  - Symmetric MRCA time matrix $M^{(3)}$:\n    - $m^{(3)}_{12}=2.5$, $m^{(3)}_{13}=2.0$, $m^{(3)}_{14}=2.0$,\n    - $m^{(3)}_{23}=2.2$, $m^{(3)}_{24}=2.1$,\n    - $m^{(3)}_{34}=8.0$.\n  - Serial intervals as in test case $1$.\n  - Cross-species barrier $b = 0.3$, phylogenetic tolerance $\\Delta = 4.0$, decay rate $\\alpha = 0.2$.\n\nTasks:\n- Implement the computation of posterior probabilities $P(i \\rightarrow j \\mid \\text{data})$ for all valid donor-recipient pairs $(i,j)$ with $t_i &lt; t_j$ as defined above.\n- For each test case, compute the expectation of cross-species events as the sum over recipients $j$ of the posterior probability that $s_i \\neq s_j$ for their donor.\n\nUnits and outputs:\n- All times are in days. There are no angles. The final outputs are unitless real numbers.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[x_1,x_2,x_3]$), where $x_k$ is the expected number of cross-species transmission events for test case $k$, each rounded to exactly six decimal places.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\nStep 1: Extract Givens\n- **Cases**: Indexed by $i, j, \\dots$. Each case has a sampling time $t_i$ and a host species $s_i$.\n- **Phylogeny**: A symmetric matrix of Most Recent Common Ancestor (MRCA) times, $M$, where $m_{ij}$ is the MRCA time for cases $i$ and $j$.\n- **Candidate Donors**: For a recipient case $j$, the set of candidate donors is $\\{i \\mid t_i < t_j\\}$.\n- **Serial Interval (SI) Model**: For each ordered host pair $(s_i \\rightarrow s_j)$, the SI is modeled by a Lognormal distribution with mean $\\mu_{s_i \\rightarrow s_j}$ and standard deviation $\\sigma_{s_i \\rightarrow s_j}$.\n- **Temporal Likelihood**: $L_{\\text{time}}(i \\rightarrow j) = f_{\\text{LN}}(\\tau_{ij} \\,;\\, \\mu_{s_i \\rightarrow s_j}, \\sigma_{s_i \\rightarrow s_j})$, where $\\tau_{ij} = t_j - t_i$. For $\\tau_{ij} \\le 0$, $f_{\\text{LN}} = 0$.\n- **Lognormal PDF**: For a variable $X$ with mean $\\mu > 0$ and standard deviation $\\sigma > 0$, the PDF is given by $f_{\\text{LN}}(x;\\mu,\\sigma) = \\frac{1}{x \\, \\tilde{\\sigma}\\,\\sqrt{2\\pi}} \\exp\\!\\Big(-\\frac{(\\ln x - \\tilde{\\mu})^2}{2\\,\\tilde{\\sigma}^2}\\Big)$ for $x > 0$. The parameters $\\tilde{\\mu}$ and $\\tilde{\\sigma}$ for the underlying Normal distribution of $\\ln X$ are $\\tilde{\\mu} = \\ln\\!\\Big(\\frac{\\mu^2}{\\sqrt{\\sigma^2 + \\mu^2}}\\Big)$ and $\\tilde{\\sigma} = \\sqrt{\\ln\\!\\Big(1 + \\frac{\\sigma^2}{\\mu^2}\\Big)}$.\n- **Phylogenetic Weight**: $L_{\\text{phylo}}(i \\rightarrow j) = \\exp\\!\\Big(-\\alpha \\, \\max\\!\\big(0,\\; (t_i - m_{ij}) - \\Delta\\big)\\Big)$, with phylogenetic tolerance $\\Delta$ and decay rate $\\alpha$.\n- **Prior Probability**: $P_0(i \\rightarrow j) \\propto 1$ if $s_i = s_j$ and $P_0(i \\rightarrow j) \\propto b$ if $s_i \\neq s_j$, where $b \\in (0,1]$ is a cross-species barrier factor.\n- **Posterior Weight**: The unnormalized posterior weight is $w_{ij} = P_0(i \\rightarrow j)\\, L_{\\text{time}}(i \\rightarrow j)\\, L_{\\text{phylo}}(i \\rightarrow j)$.\n- **Posterior Probability**: $P(i \\rightarrow j \\mid \\text{data}) = w_{ij} / \\sum_{k: t_k < t_j} w_{kj}$.\n- **Objective**: Compute the expected number of cross-species transmission events, $\\mathbb{E}[\\text{cross-species events}] = \\sum_{j} \\sum_{i: t_i < t_j,\\, s_i \\neq s_j} P(i \\rightarrow j \\mid \\text{data})$.\n\n- **Test Case Data**: Three test cases are provided with specific values for $n$ (number of cases), $t$ (sampling times), $s$ (species), $M$ (MRCA matrix), SI parameters $(\\mu, \\sigma)$ for host pairs (Human H, Swine S), and model parameters $b, \\Delta, \\alpha$.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes a simplified Bayesian model for infectious disease source attribution, integrating temporal (serial interval) and genetic (phylogenetic) data. This is a standard and well-established methodology in the field of molecular epidemiology and phylodynamics. The use of a Lognormal distribution for incubation or generation periods is common. The phylogenetic constraint is a logical heuristic. The model constitutes a valid, albeit simplified, scientific inference problem.\n- **Well-Posed**: The problem is well-posed. All required data, parameters, and functional forms are explicitly provided. The objective is a clearly defined, computable quantity. A unique solution exists for each test case.\n- **Objective**: The problem is stated in precise, objective, mathematical, and scientific language. It is free from subjective or opinion-based claims.\n- **Completeness and Consistency**: The problem is self-contained. All necessary formulas and data are included. The data for the three test cases are complete. There are no internal contradictions. For example, the condition $t_i < t_j$ for donors ensures the serial interval $\\tau_{ij} = t_j - t_i$ is positive, which is the domain of the Lognormal distribution. The handling of cases with no donors is specified.\n- **No other flaws detected**: The problem is not trivial, unrealistic, or ill-structured. It requires a careful and systematic implementation of the provided model.\n\nStep 3: Verdict and Action\nThe problem is valid. A reasoned solution will be provided.\n\nThe objective is to compute the expected number of cross-species transmission events for each given dataset. This expectation is the sum, over all recipients, of the posterior probability that their donor was of a different species. The solution requires a systematic implementation of the specified Bayesian model.\n\nThe overall algorithm proceeds as follows for each test case:\n1. Initialize a running total for the expected number of cross-species events, $\\mathbb{E}_{\\text{total}}$, to $0$.\n2. For each case $j$ in the dataset, consider it as a potential recipient.\n3. Identify the set of candidate donors for recipient $j$, which includes all cases $i$ with sampling time $t_i < t_j$.\n4. If there are no candidate donors for $j$, this recipient contributes $0$ to $\\mathbb{E}_{\\text{total}}$, and we proceed to the next recipient.\n5. If there are candidate donors, for each candidate donor $i$, we calculate an unnormalized posterior weight $w_{ij}$. This weight is the product of three components:\n    a. **Prior weight $P_0(i \\rightarrow j)$**: This term reflects a preference for within-species transmission. It is set to $1$ if the donor $i$ and recipient $j$ are of the same species ($s_i = s_j$) and to the cross-species barrier factor $b$ if their species are different ($s_i \\neq s_j$).\n    b. **Temporal likelihood $L_{\\text{time}}(i \\rightarrow j)$**: This term measures the compatibility of the observed time difference $\\tau_{ij} = t_j - t_i$ with the expected serial interval for the species pair $(s_i, s_j)$. It is calculated as the probability density function (PDF) of a Lognormal distribution, $f_{\\text{LN}}(\\tau_{ij} \\,;\\, \\mu_{s_i \\rightarrow s_j}, \\sigma_{s_i \\rightarrow s_j})$. The parameters of the Lognormal distribution, its mean $\\mu$ and standard deviation $\\sigma$, must first be converted to the parameters of the underlying Normal distribution, $\\tilde{\\mu}$ and $\\tilde{\\sigma}$, using the provided formulas:\n    $$ \\tilde{\\mu} = \\ln\\left(\\frac{\\mu^2}{\\sqrt{\\sigma^2 + \\mu^2}}\\right), \\quad \\tilde{\\sigma} = \\sqrt{\\ln\\left(1 + \\frac{\\sigma^2}{\\mu^2}\\right)} $$\n    The PDF of the Lognormal distribution for a value $x > 0$ is then calculated.\n    c. **Phylogenetic weight $L_{\\text{phylo}}(i \\rightarrow j)$**: This term penalizes donor-recipient pairs where the genetic lineage of the donor is too evolutionarily distant from their MRCA. It is calculated using the formula:\n    $$ L_{\\text{phylo}}(i \\rightarrow j) = \\exp\\!\\Big(-\\alpha \\, \\max\\!\\big(0,\\; (t_i - m_{ij}) - \\Delta\\big)\\Big) $$\n    Here, $m_{ij}$ is the MRCA time of cases $i$ and $j$, $\\Delta$ is a tolerance window, and $\\alpha$ is a decay rate. A penalty is applied only if the time from the MRCA to the donor's sampling, $t_i - m_{ij}$, exceeds the tolerance $\\Delta$.\n\n6. After computing the weight $w_{ij}$ for all candidate donors $i$ for a given recipient $j$, the weights are normalized to obtain posterior probabilities:\n$$ P(i \\rightarrow j \\mid \\text{data}) = \\frac{w_{ij}}{\\sum_{k: t_k < t_j} w_{kj}} $$\nThe denominator is the sum of weights over all candidate donors for recipient $j$. If this sum is $0$, all posterior probabilities are $0$.\n7. The contribution of recipient $j$ to the total expected number of cross-species events is the sum of the posterior probabilities of all its cross-species donors:\n$$ \\mathbb{E}_{j} = \\sum_{i: t_i < t_j,\\, s_i \\neq s_j} P(i \\rightarrow j \\mid \\text{data}) $$\n8. This contribution $\\mathbb{E}_j$ is added to the running total $\\mathbb{E}_{\\text{total}}$.\n9. After iterating through all possible recipients $j$, the final value of $\\mathbb{E}_{\\text{total}}$ is the result for the test case.\n\nThis procedure will be applied to each of the three test cases using their specific parameters and data. The implementation will use numerical libraries for mathematical computations, respecting the specified execution environment.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import lognorm\n\ndef get_lognorm_pdf_func(mu, sigma):\n    \"\"\"\n    Creates a lognormal probability density function for a given mean and std dev.\n\n    Args:\n        mu (float): The mean of the lognormal distribution.\n        sigma (float): The standard deviation of the lognormal distribution.\n\n    Returns:\n        function: A function that computes the PDF value for a given x.\n    \"\"\"\n    if mu <= 0 or sigma <= 0:\n        return lambda x: 0.0\n\n    mu_sq = mu**2\n    sigma_sq = sigma**2\n    \n    # Parameters for the underlying normal distribution\n    tilde_sigma_sq = np.log(1.0 + sigma_sq / mu_sq)\n    tilde_sigma = np.sqrt(tilde_sigma_sq)\n    tilde_mu = np.log(mu_sq / np.sqrt(sigma_sq + mu_sq))\n    \n    # Scipy's lognorm uses shape `s` (our tilde_sigma) and `scale` (exp(tilde_mu))\n    scale = np.exp(tilde_mu)\n    \n    # Return a function that computes the PDF\n    def pdf(x):\n        if x <= 0:\n            return 0.0\n        return lognorm.pdf(x, s=tilde_sigma, scale=scale)\n    \n    return pdf\n\ndef run_inference(n, t, s, m_raw, si_params_raw, b, Delta, alpha):\n    \"\"\"\n    Computes the expected number of cross-species transmission events for a single dataset.\n    \"\"\"\n    species_map = {'H': 0, 'S': 1}\n    s_numeric = [species_map[sp] for sp in s]\n    \n    # Build full symmetric MRCA matrix M from upper-triangular entries\n    M = np.zeros((n, n))\n    for (i_one, j_one), val in m_raw.items():\n        i, j = i_one - 1, j_one - 1\n        M[i, j] = M[j, i] = val\n\n    # Create a dictionary of PDF functions for each species-pair transition\n    si_pdfs = {\n        (species_map[s_from], species_map[s_to]): get_lognorm_pdf_func(mu, sigma)\n        for (s_from, s_to), (mu, sigma) in si_params_raw.items()\n    }\n    \n    total_expected_cross_species = 0.0\n    \n    # Iterate through all cases, considering each as a potential recipient j\n    for j in range(n):\n        # Identify candidate donors i (must be sampled before j)\n        candidate_donors = [i for i in range(n) if t[i] < t[j]]\n        \n        if not candidate_donors:\n            continue\n            \n        weights = []\n        is_cross_species = []\n        \n        # For each candidate donor, calculate the unnormalized posterior weight\n        for i in candidate_donors:\n            s_i, s_j = s_numeric[i], s_numeric[j]\n            t_i, t_j = t[i], t[j]\n            m_ij = M[i, j]\n            \n            # 1. Prior probability\n            prior = b if s_i != s_j else 1.0\n            \n            # 2. Temporal likelihood\n            tau_ij = t_j - t_i\n            pdf_func = si_pdfs[(s_i, s_j)]\n            l_time = pdf_func(tau_ij)\n            \n            # 3. Phylogenetic likelihood\n            phylo_penalty_term = max(0, (t_i - m_ij) - Delta)\n            l_phylo = np.exp(-alpha * phylo_penalty_term)\n            \n            # Unnormalized posterior weight\n            w_ij = prior * l_time * l_phylo\n            weights.append(w_ij)\n            is_cross_species.append(1 if s_i != s_j else 0)\n\n        # Normalize weights to get posterior probabilities\n        total_weight = sum(weights)\n        \n        if total_weight == 0:\n            continue\n\n        posteriors = [w / total_weight for w in weights]\n        \n        # Sum posterior probabilities for cross-species donors for recipient j\n        recipient_j_exp_cross_species = sum(\n            p * c for p, c in zip(posteriors, is_cross_species)\n        )\n        \n        # Add to total expectation\n        total_expected_cross_species += recipient_j_exp_cross_species\n        \n    return total_expected_cross_species\n\ndef solve():\n    # Common SI parameters for all test cases\n    si_params_common = {\n        ('H', 'H'): (4.0, 2.0), ('H', 'S'): (6.0, 2.5),\n        ('S', 'H'): (7.0, 3.0), ('S', 'S'): (5.0, 2.0)\n    }\n\n    test_cases = [\n        {\n            'n': 5, 't': np.array([2.0, 6.0, 9.5, 12.0, 15.0]), 's': ['H', 'H', 'S', 'S', 'H'],\n            'm_raw': {\n                (1, 2): 1.5, (1, 3): 1.0, (1, 4): 0.8, (1, 5): 1.2,\n                (2, 3): 3.0, (2, 4): 2.5, (2, 5): 3.5,\n                (3, 4): 8.0, (3, 5): 6.0,\n                (4, 5): 10.0\n            },\n            'si_params_raw': si_params_common,\n            'b': 0.5, 'Delta': 3.0, 'alpha': 0.25\n        },\n        {\n            'n': 5, 't': np.array([1.0, 4.0, 7.0, 10.0, 11.0]), 's': ['H', 'S', 'H', 'S', 'S'],\n            'm_raw': {\n                (1, 2): 0.9, (1, 3): 0.8, (1, 4): 0.7, (1, 5): 0.7,\n                (2, 3): 3.8, (2, 4): 3.2, (2, 5): 3.5,\n                (3, 4): 6.0, (3, 5): 6.2,\n                (4, 5): 9.0\n            },\n            'si_params_raw': si_params_common,\n            'b': 0.7, 'Delta': 2.0, 'alpha': 0.4\n        },\n        {\n            'n': 4, 't': np.array([3.0, 3.0, 9.0, 20.0]), 's': ['H', 'S', 'H', 'S'],\n            'm_raw': {\n                (1, 2): 2.5, (1, 3): 2.0, (1, 4): 2.0,\n                (2, 3): 2.2, (2, 4): 2.1,\n                (3, 4): 8.0\n            },\n            'si_params_raw': si_params_common,\n            'b': 0.3, 'Delta': 4.0, 'alpha': 0.2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_inference(**case)\n        results.append(f\"{result:.6f}\")\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "2539206"}]}