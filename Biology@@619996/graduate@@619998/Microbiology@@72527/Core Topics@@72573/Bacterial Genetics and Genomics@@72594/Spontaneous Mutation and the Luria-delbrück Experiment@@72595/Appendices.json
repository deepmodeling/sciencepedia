{"hands_on_practices": [{"introduction": "Before we can analyze data from a fluctuation test, we must first build a simple mathematical model of the mutation process. This practice guides you through a foundational derivation, showing how the expected number of mutation *events*, denoted by $m$, is related to the per-division mutation rate $\\mu$ and the final population size $N$. By applying first principles like the conservation of cell number and linearity of expectation, you will derive the key relationship that underpins most methods for estimating mutation rates [@problem_id:2533616].", "problem": "Consider a clonal bacterial population initiated by a single wild-type cell that grows by binary fission without death until it reaches exactly $N$ cells. Focus on a single forward mutation at a specified locus. Assume that in each cell division the number of new mutation-initiation events at this locus is an independent $\\mathrm{Poisson}(\\mu)$ random variable with parameter $\\mu$, where $\\mu$ is constant across divisions and satisfies $\\mu \\ll 1$. Let $M$ be the total count of mutation-initiation events accumulated over the entire growth from $1$ to $N$ cells, and let $m = \\mathbb{E}[M]$ denote its expectation. Using only conservation of cell number under binary fission and linearity of expectation, derive the leading-order asymptotic expression for $m$ as $N \\to \\infty$. Express your final answer as a single closed-form analytic expression in terms of $\\mu$ and $N$. No rounding is required. In your derivation, justify the approximation you use and identify the biological and probabilistic conditions under which it is valid in the Luria–Delbrück framework.", "solution": "The objective is to find the leading-order asymptotic expression for $m = \\mathbb{E}[M]$, the expected total number of mutation-initiation events in a bacterial population growing from $1$ to $N$ cells. The derivation will adhere strictly to the stipulated principles of conservation of cell number and linearity of expectation.\n\nFirst, we apply the principle of conservation of cell number under binary fission. The population starts with $1$ cell and grows to a final size of $N$ cells. Since growth occurs via binary fission and there is no cell death, each division event increases the total cell count by exactly one. To achieve a net increase of $N-1$ cells, a total of exactly $N-1$ division events must occur.\n\nLet $M$ be the random variable representing the total count of new mutation-initiation events over the entire growth process. Let the individual division events be indexed from $i=1$ to $i=N-1$. Let $X_i$ be the random variable for the number of new mutations occurring during the $i$-th division. The total number of mutations $M$ is the sum of the mutations from each individual division:\n$$M = \\sum_{i=1}^{N-1} X_i$$\nWe are asked to compute the expectation of $M$, which is denoted by $m$. We apply the property of linearity of expectation. This property states that the expectation of a sum of random variables is the sum of their individual expectations. It is a fundamental property that holds regardless of whether the random variables are independent.\n$$m = \\mathbb{E}[M] = \\mathbb{E}\\left[\\sum_{i=1}^{N-1} X_i\\right] = \\sum_{i=1}^{N-1} \\mathbb{E}[X_i]$$\nThe problem states that for each cell division, the number of new mutation-initiation events, $X_i$, is an independent random variable drawn from a Poisson distribution with a constant parameter $\\mu$. That is, $X_i \\sim \\mathrm{Poisson}(\\mu)$ for all $i \\in \\{1, 2, \\dots, N-1\\}$.\n\nThe expectation of a Poisson-distributed random variable with parameter $\\lambda$ is equal to $\\lambda$. Therefore, for each $X_i$, its expectation is:\n$$\\mathbb{E}[X_i] = \\mu$$\nSubstituting this constant expectation back into the summation for $m$:\n$$m = \\sum_{i=1}^{N-1} \\mu$$\nSince $\\mu$ is a constant, the sum is simply the product of $\\mu$ and the number of terms, which is $N-1$. This gives the exact expression for the expected number of mutation events:\n$$m = \\mu(N-1)$$\nThe problem requires the leading-order asymptotic expression for $m$ as the final population size $N$ approaches infinity ($N \\to \\infty$). We can expand the exact expression for $m$:\n$$m = \\mu N - \\mu$$\nIn the limit as $N \\to \\infty$, the term $\\mu N$ grows without bound, whereas the term $\\mu$ remains constant. The term $\\mu N$ is therefore the dominant, or leading-order, term. Thus, the leading-order asymptotic expression for $m$ is:\n$$m \\sim \\mu N \\quad \\text{as } N \\to \\infty$$\nThe approximation made is that for large $N$, $N-1 \\approx N$. The validity of this approximation is readily justified. The relative error introduced by this approximation is given by:\n$$ \\text{Relative Error} = \\frac{|\\text{exact} - \\text{approximate}|}{|\\text{exact}|} = \\frac{|\\mu(N-1) - \\mu N|}{|\\mu(N-1)|} = \\frac{|-\\mu|}{|\\mu(N-1)|} = \\frac{1}{N-1} $$\nAs $N \\to \\infty$, the relative error $\\frac{1}{N-1}$ approaches $0$, confirming that $\\mu N$ is the correct leading-order asymptotic expression. In the context of microbiology, final population sizes $N$ are typically of the order $10^8$ or greater, for which this approximation is exceptionally accurate.\n\nFinally, we discuss the conditions under which this model is valid within the Luria–Delbrück framework. The derivation relies upon a set of idealizations that formalize the hypothesis of spontaneous mutation.\n1.  **Spontaneous Random Mutation**: The core of the Luria-Delbrück model is that mutations are not directed by the environment but arise randomly. The model captures this by treating mutation as a stochastic event (a Poisson-distributed random variable) occurring during cell replication. The calculation of the expected number of *events*, distinct from the number of final *mutant cells*, highlights this aspect directly.\n2.  **No Cell Death**: The assumption of growth without death is a simplification. Real bacterial cultures have a death rate. However, during the exponential (log) phase of growth, the division rate far exceeds the death rate, so this assumption provides a reasonable approximation of the population dynamics.\n3.  **Poisson Process for Mutations**: Modeling the number of new mutations per division as a Poisson($\\mu$) random variable implies that these events are independent and rare. The given condition $\\mu \\ll 1$ ensures this rarity. For small $\\mu$, the Poisson($\\mu$) distribution is closely approximated by a Bernoulli($\\mu$) process, meaning that in any given division, there is a small probability $\\mu$ of one mutation occurring and a probability $1-\\mu$ of no mutation occurring. This is a biologically plausible and mathematically convenient model for mutations at a single locus.\n4.  **Constant Mutation Parameter $\\mu$**: The assumption that $\\mu$ is constant over all $N-1$ divisions implies that the mutation rate is independent of population density, nutrient availability, or cell age. This is another simplification, as mutation rates can be stress-induced or vary under different physiological conditions. However, for a foundational model, it is a standard and necessary assumption.\n\nIn summary, the derivation directly uses the principles of cell conservation and linearity of expectation to arrive at an exact expression for the mean number of mutation events, from which the leading asymptotic term is trivially identified. The underlying model is a cornerstone of the Luria-Delbrück theory, formalizing the hypothesis of spontaneous mutation under idealized growth conditions.", "answer": "$$\\boxed{\\mu N}$$", "id": "2533616"}, {"introduction": "With a theoretical model connecting the mutation rate to the expected number of mutational events, our next challenge is to estimate this rate from experimental data. This exercise introduces the classic \"method of zeros\" or P-zero method, a clever and widely used technique that relies only on the fraction of cultures with no mutants [@problem_id:2533544]. You will not only calculate the mutation rate but also learn to propagate experimental uncertainty using the delta method, an essential skill for rigorously reporting scientific findings.", "problem": "A fluctuation assay is performed to estimate the per-division mutation rate in a bacterial population using the zero-class (P-zero) approach of the Luria–Delbrück fluctuation test. A total of $C=100$ independent replicate cultures are inoculated from a common small ancestral population and grown under identical conditions to the same final population size of $N=4.0 \\times 10^{8}$ cells per culture. After selection, $Z=20$ cultures yield no mutant colonies (the zero-mutant class). Assume that mutations arise as a Poisson process with rate $\\mu$ per cell division, that cell divisions are independent and identical, that there is negligible cell death, and that the number of divisions is well-approximated by the final population size $N$ (valid when the initial inoculum is negligible relative to $N$). Under these conditions, the probability that a culture has zero mutants is $p_{0}=\\exp(-m)$, where $m$ is the expected number of mutations per culture, and $m \\approx \\mu N$.\n\nUsing the P-zero method, define the estimator $\\hat{p}_{0}=Z/C$ and $\\hat{m}=-\\ln(\\hat{p}_{0})$, and then $\\hat{\\mu}=\\hat{m}/N$. Treat $Z \\sim \\mathrm{Binomial}(C,p_{0})$ and use the delta method to derive a large-sample approximation to the standard error of $\\hat{\\mu}$ that accounts for sampling variability in $Z$ but ignores uncertainty in $N$. Compute the numerical value of this standard error for the data above. Round your reported standard error to three significant figures. Express the standard error in units of per division.", "solution": "The objective is to find the standard error of the mutation rate estimator, $\\hat{\\mu}$. The estimator is defined as a function of the observed fraction of zero-mutant cultures, $\\hat{p}_0 = Z/C$:\n$$ \\hat{\\mu} = \\frac{\\hat{m}}{N} = \\frac{-\\ln(\\hat{p}_{0})}{N} $$\nHere, $Z$ is the number of zero-mutant cultures, $C$ is the total number of cultures, and $N$ is the final population size per culture.\n\nWe must apply the delta method to find the approximate variance of $\\hat{\\mu}$. The delta method provides an approximation for the variance of a function of a random variable. For a statistic $X$ with $E[X] = \\theta$ and a differentiable function $g(X)$, the variance of $g(X)$ is approximated by:\n$$ \\mathrm{Var}(g(X)) \\approx \\left[g'(\\theta)\\right]^{2} \\mathrm{Var}(X) $$\nIn our case, the random statistic is $\\hat{p}_{0}$. The problem states that $Z$ follows a binomial distribution, $Z \\sim \\mathrm{Binomial}(C, p_{0})$, where $p_{0}$ is the true probability of a culture having zero mutants. The estimator for $p_{0}$ is $\\hat{p}_{0} = Z/C$.\n\nFirst, we determine the expected value and variance of our statistic, $\\hat{p}_{0}$.\nThe expected value of $\\hat{p}_{0}$ is:\n$$ E[\\hat{p}_{0}] = E\\left[\\frac{Z}{C}\\right] = \\frac{1}{C}E[Z] = \\frac{1}{C}(Cp_{0}) = p_{0} $$\nThe variance of $\\hat{p}_{0}$ is:\n$$ \\mathrm{Var}(\\hat{p}_{0}) = \\mathrm{Var}\\left(\\frac{Z}{C}\\right) = \\frac{1}{C^2}\\mathrm{Var}(Z) = \\frac{1}{C^2}(Cp_{0}(1-p_{0})) = \\frac{p_{0}(1-p_{0})}{C} $$\n\nNext, we define our function $g$ such that $\\hat{\\mu} = g(\\hat{p}_{0})$. The function is:\n$$ g(x) = -\\frac{\\ln(x)}{N} $$\nWe must find the derivative of this function with respect to its argument $x$, evaluated at the expected value of the statistic, which is $p_{0}$.\n$$ g'(x) = \\frac{d}{dx}\\left(-\\frac{\\ln(x)}{N}\\right) = -\\frac{1}{Nx} $$\nEvaluating at $x=p_{0}$, we have:\n$$ g'(p_{0}) = -\\frac{1}{Np_{0}} $$\n\nNow, we apply the delta method formula to find the variance of $\\hat{\\mu}$:\n$$ \\mathrm{Var}(\\hat{\\mu}) \\approx [g'(p_{0})]^{2} \\mathrm{Var}(\\hat{p}_{0}) $$\n$$ \\mathrm{Var}(\\hat{\\mu}) \\approx \\left(-\\frac{1}{Np_{0}}\\right)^{2} \\left(\\frac{p_{0}(1-p_{0})}{C}\\right) $$\n$$ \\mathrm{Var}(\\hat{\\mu}) \\approx \\frac{1}{N^{2}p_{0}^{2}} \\frac{p_{0}(1-p_{0})}{C} = \\frac{1-p_{0}}{CN^{2}p_{0}} $$\n\nThe standard error, $\\mathrm{SE}(\\hat{\\mu})$, is the square root of the variance:\n$$ \\mathrm{SE}(\\hat{\\mu}) \\approx \\sqrt{\\frac{1-p_{0}}{CN^{2}p_{0}}} $$\nTo compute a numerical value, we must substitute the unknown true parameter $p_{0}$ with its estimate from the data, $\\hat{p}_{0} = Z/C$. This yields the estimated standard error, $\\widehat{\\mathrm{SE}}(\\hat{\\mu})$:\n$$ \\widehat{\\mathrm{SE}}(\\hat{\\mu}) = \\sqrt{\\frac{1-\\hat{p}_{0}}{CN^{2}\\hat{p}_{0}}} $$\n\nWe are given the following values:\nTotal cultures, $C = 100$.\nZero-mutant cultures, $Z = 20$.\nFinal population size, $N = 4.0 \\times 10^{8}$.\n\nFirst, we compute the estimate $\\hat{p}_{0}$:\n$$ \\hat{p}_{0} = \\frac{Z}{C} = \\frac{20}{100} = 0.2 $$\n\nNow, we substitute the numerical values into the expression for the standard error:\n$$ \\widehat{\\mathrm{SE}}(\\hat{\\mu}) = \\sqrt{\\frac{1-0.2}{100 \\times (4.0 \\times 10^{8})^{2} \\times 0.2}} $$\n$$ \\widehat{\\mathrm{SE}}(\\hat{\\mu}) = \\sqrt{\\frac{0.8}{100 \\times (1.6 \\times 10^{17}) \\times 0.2}} $$\n$$ \\widehat{\\mathrm{SE}}(\\hat{\\mu}) = \\sqrt{\\frac{0.8}{20 \\times (1.6 \\times 10^{17})}} $$\n$$ \\widehat{\\mathrm{SE}}(\\hat{\\mu}) = \\sqrt{\\frac{0.8}{3.2 \\times 10^{18}}} $$\nSimplifying the fraction inside the square root:\n$$ \\frac{0.8}{3.2 \\times 10^{18}} = \\frac{8 \\times 10^{-1}}{3.2 \\times 10^{18}} = 2.5 \\times 10^{-19} $$\nAlternatively, this can be written as $0.25 \\times 10^{-18}$.\nTaking the square root:\n$$ \\widehat{\\mathrm{SE}}(\\hat{\\mu}) = \\sqrt{0.25 \\times 10^{-18}} = \\sqrt{25 \\times 10^{-20}} = 5 \\times 10^{-10} $$\nThe problem requires the result to be rounded to three significant figures. Therefore, the standard error is $5.00 \\times 10^{-10}$ per division.", "answer": "$$\\boxed{5.00 \\times 10^{-10}}$$", "id": "2533544"}, {"introduction": "The central insight of the Luria-Delbrück experiment was its ability to distinguish between random, spontaneous mutations and environmentally-induced, adaptive mutations. This final practice provides you with the statistical tools to formally test this hypothesis [@problem_id:2533630]. You will use the Fano factor—the ratio of the variance to the mean of mutant counts—to test for the \"overdispersion\" that is the tell-tale signature of clonal jackpots arising from early, spontaneous mutation events.", "problem": "A classic approach to detecting spontaneous mutations in the Luria-Delbrück (LD) setting is to compare the dispersion of mutant counts across replicate cultures against the Poisson benchmark. In a Luria-Delbrück (LD) model, early-arising mutations lead to clonal expansion and a heavy-tailed distribution of mutant counts across independent cultures, producing overdispersion relative to a Poisson model. Consider an experiment with $n$ independent replicate cultures, each yielding a mutant count $X_i$. The Poisson benchmark asserts that $X_i \\sim \\text{Poisson}(\\lambda)$ independently with common mean $ \\lambda $, hence $\\operatorname{Var}(X_i) = \\operatorname{E}(X_i) = \\lambda$. In contrast, the LD model implies $\\operatorname{Var}(X_i) \\gg \\operatorname{E}(X_i)$. Define the sample mean $\\overline{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$, the unbiased sample variance $S^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline{X})^{2}$, and the empirical Fano factor $F = \\frac{S^{2}}{\\overline{X}}$.\n\n- Propose a one-sided test based on $F$ to distinguish the Poisson model from the LD model, using the fundamental properties of the Poisson and multinomial distributions as your starting point.\n- Derive the null distribution of a suitable test statistic under the Poisson model without appealing to any pre-packaged results about its distribution; start from the fact that if $X_i \\sim \\text{Poisson}(\\lambda)$ are independent, then the total $T = \\sum_{i=1}^{n} X_i \\sim \\text{Poisson}(n\\lambda)$, and conditional on $T$, $(X_1,\\dots,X_n) \\mid T \\sim \\text{Multinomial}(T; \\frac{1}{n},\\dots,\\frac{1}{n})$.\n- Then apply your test to the following summary data from $n = 11$ replicate cultures: $\\overline{X} = 5$ and $S^{2} = 10$. Compute the one-sided $p$-value for testing the Poisson benchmark against the LD alternative, using the null distribution you derived and the observed data. Round your answer to four significant figures. Express the final answer as a pure decimal (no units).", "solution": "This problem requires constructing and applying a statistical test to distinguish between two hypotheses for the distribution of mutant counts $X_i$.\n\nThe null hypothesis, $H_0$, corresponds to the Poisson benchmark, where mutations are induced. Under $H_0$, the counts $X_i$ are independent and identically distributed draws from a Poisson distribution with some mean $\\lambda$. A key property of the Poisson distribution is that its variance equals its mean: $\\operatorname{Var}(X_i) = \\operatorname{E}(X_i) = \\lambda$. The Fano factor, $\\frac{\\operatorname{Var}(X_i)}{\\operatorname{E}(X_i)}$, is therefore equal to $1$.\n\nThe alternative hypothesis, $H_A$, represents the Luria-Delbrück model of spontaneous mutation. This process leads to clonal jackpots and results in overdispersion, where the variance of mutant counts is significantly larger than the mean: $\\operatorname{Var}(X_i) \\gg \\operatorname{E}(X_i)$. The Fano factor is thus greater than $1$.\n\nA logical test would compare a sample-based estimate of the Fano factor to the value $1$ expected under $H_0$. The empirical Fano factor is given as $F = \\frac{S^2}{\\overline{X}}$. Since $H_A$ predicts overdispersion, we will reject $H_0$ if $F$ is sufficiently large. This defines a one-sided test. For convenience, we define a test statistic, often called the index of dispersion, which is proportional to $F$:\n$$ D = (n-1)F = \\frac{(n-1)S^2}{\\overline{X}} = \\frac{\\sum_{i=1}^{n} (X_i - \\overline{X})^{2}}{\\overline{X}} $$\nWe reject $H_0$ for large values of $D$. To determine what constitutes a \"large\" value, we must derive the distribution of $D$ under the null hypothesis $H_0$.\n\nUnder $H_0$, the parameter $\\lambda$ is unknown. A standard method to eliminate such a nuisance parameter is to condition on a sufficient statistic. For a Poisson distribution, the sufficient statistic for $\\lambda$ is the total count, $T = \\sum_{i=1}^{n} X_i$. We are given that if $X_i \\sim \\text{Poisson}(\\lambda)$ independently, then, conditional on the total count $T=t$, the vector of counts $(X_1, \\dots, X_n)$ follows a Multinomial distribution with $t$ trials and $n$ categories, each with probability $p_i = \\frac{1}{n}$.\n\nLet us analyze our test statistic $D$ conditional on $T=t$. Note that $\\overline{X} = \\frac{T}{n} = \\frac{t}{n}$.\n$$ D \\mid (T=t) = \\frac{\\sum_{i=1}^{n} (X_i - t/n)^2}{t/n} $$\nThis expression is the Pearson's chi-squared statistic for testing the goodness-of-fit of the observed counts $X_i$ to the expected counts $E_i = t \\cdot p_i = t/n$ under a uniform multinomial model. For a sufficiently large number of trials $t$, the distribution of this statistic converges to a chi-squared distribution, $\\chi^2$. The number of degrees of freedom is the number of categories minus $1$ (due to the constraint $\\sum X_i = t$), which is $n-1$.\n\nWe can confirm this by calculating the expected value of $D$ under the conditional multinomial distribution.\n$$ \\operatorname{E}[D \\mid T=t] = \\operatorname{E}\\left[\\frac{\\sum_{i=1}^{n} (X_i - t/n)^2}{t/n} \\mid T=t\\right] = \\frac{n}{t} \\sum_{i=1}^{n} \\operatorname{E}[(X_i - t/n)^2 \\mid T=t] $$\nThe term $\\operatorname{E}[(X_i - t/n)^2 \\mid T=t]$ is the variance of $X_i$ under the conditional distribution. For a multinomial distribution $\\text{Multinomial}(t; p_1, \\dots, p_n)$, the variance of the count in the $i$-th category is $\\operatorname{Var}(X_i) = t p_i (1-p_i)$. Here, $p_i = 1/n$.\n$$ \\operatorname{Var}(X_i \\mid T=t) = t \\frac{1}{n} \\left(1 - \\frac{1}{n}\\right) = \\frac{t(n-1)}{n^2} $$\nSubstituting this into the expression for the expected value of $D$:\n$$ \\operatorname{E}[D \\mid T=t] = \\frac{n}{t} \\sum_{i=1}^{n} \\frac{t(n-1)}{n^2} = \\frac{n}{t} \\cdot n \\cdot \\frac{t(n-1)}{n^2} = n-1 $$\nThe expected value of our test statistic is $n-1$, which matches the expectation of a $\\chi^2_{n-1}$ random variable. Thus, the null distribution of the test statistic $D$ is well-approximated by a chi-squared distribution with $n-1$ degrees of freedom:\n$$ D = \\frac{(n-1)S^2}{\\overline{X}} \\sim \\chi^2_{n-1} $$\n\nNow, we apply this test to the given data: $n = 11$, $\\overline{X} = 5$, and $S^2 = 10$.\nFirst, we calculate the observed value of the test statistic, $D_{obs}$:\n$$ D_{obs} = \\frac{(n-1)S^2}{\\overline{X}} = \\frac{(11-1) \\cdot 10}{5} = \\frac{10 \\cdot 10}{5} = \\frac{100}{5} = 20 $$\nThe degrees of freedom for the null distribution are $df = n-1 = 11-1 = 10$.\nThe $p$-value for our one-sided test is the probability of observing a value of $D$ as extreme as or more extreme than $D_{obs}$, assuming $H_0$ is true. This corresponds to the upper tail of the $\\chi^2_{10}$ distribution.\n$$ p\\text{-value} = P(\\chi^2_{10} \\ge 20) $$\nUsing a statistical calculator or software to find the value of this cumulative probability:\n$$ P(\\chi^2_{10} \\ge 20) \\approx 0.0292526 $$\nRounding to four significant figures as required, we get $0.02925$. This small $p$-value (e.g., less than a typical significance level of $\\alpha = 0.05$) indicates that the observed data provides significant evidence against the Poisson model and in favor of the Luria-Delbrück model of spontaneous mutation.", "answer": "$$\n\\boxed{0.02925}\n$$", "id": "2533630"}]}