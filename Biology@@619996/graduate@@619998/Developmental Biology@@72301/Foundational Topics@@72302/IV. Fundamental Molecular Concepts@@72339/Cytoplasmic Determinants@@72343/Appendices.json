{"hands_on_practices": [{"introduction": "To function correctly, cytoplasmic determinants must be localized to specific regions of the oocyte or embryo. This exercise explores the fundamental physical principles governing this transport. By performing a scaling analysis on the advection-diffusion equation, you will derive the Péclet number, a critical dimensionless parameter that quantifies whether directed transport or random diffusion dominates the localization process over cellular length scales [@problem_id:2626718].", "problem": "Cytoplasmic determinants in large embryonic cells are transported by a combination of diffusion, driven by concentration gradients consistent with Fick’s law, and advection, driven by cytoplasmic or cortical flows. Consider a one-dimensional embryo of length $L$ with a uniform flow speed $u$ carrying a determinant with molecular diffusion coefficient $D$. Starting from the advection–diffusion balance embodied in the standard transport equation for concentration $c(x,t)$, use first-principles scaling to identify the single dimensionless number that quantifies the ratio of advective to diffusive transport over the length scale $L$. Then evaluate this number for $D=0.5\\,\\mu\\text{m}^2/\\text{s}$, $u=0.05\\,\\mu\\text{m}/\\text{s}$, and $L=100\\,\\mu\\text{m}$. Based on well-established criteria from transport theory, interpret whether advection or diffusion dominates spatial localization under these conditions. Report only the dimensionless number as your final answer, rounded to three significant figures, with no units.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of transport phenomena, is well-posed, and provides all necessary data for a unique solution. We will proceed with the derivation.\n\nThe transport of a substance with concentration $c(x, t)$ in a one-dimensional medium, subject to both advection with a uniform velocity $u$ and diffusion with a coefficient $D$, is governed by the advection-diffusion equation. This equation represents a statement of mass conservation. The total flux $J$ is the sum of the advective flux, $J_{adv} = uc$, and the diffusive flux, given by Fick's first law as $J_{diff} = -D \\frac{\\partial c}{\\partial x}$. The continuity equation in one dimension is:\n$$ \\frac{\\partial c}{\\partial t} + \\frac{\\partial J}{\\partial x} = 0 $$\nSubstituting the expression for the total flux $J = uc - D \\frac{\\partial c}{\\partial x}$, and assuming $u$ and $D$ are constants, we obtain the governing partial differential equation:\n$$ \\frac{\\partial c}{\\partial t} + u \\frac{\\partial c}{\\partial x} = D \\frac{\\partial^2 c}{\\partial x^2} $$\nThe term $u \\frac{\\partial c}{\\partial x}$ represents the change in concentration due to advection (bulk flow), and the term $D \\frac{\\partial^2 c}{\\partial x^2}$ represents the change due to diffusion (random molecular motion).\n\nTo find the dimensionless number that compares these two processes, we perform a scaling analysis, also known as non-dimensionalization. We introduce characteristic scales for the variables: $L$ for length (the size of the embryo), and $C_0$ for concentration (a typical or reference concentration value). We can then define dimensionless variables, denoted with a prime:\n$$ x' = \\frac{x}{L}, \\quad c' = \\frac{c}{C_0} $$\nThe derivatives with respect to the original variables can be expressed in terms of the new dimensionless variables using the chain rule:\n$$ \\frac{\\partial c}{\\partial x} = \\frac{\\partial (C_0 c')}{\\partial x} = C_0 \\frac{\\partial c'}{\\partial x'} \\frac{\\partial x'}{\\partial x} = \\frac{C_0}{L} \\frac{\\partial c'}{\\partial x'} $$\n$$ \\frac{\\partial^2 c}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left( \\frac{C_0}{L} \\frac{\\partial c'}{\\partial x'} \\right) = \\frac{C_0}{L} \\frac{\\partial^2 c'}{\\partial x'^2} \\frac{\\partial x'}{\\partial x} = \\frac{C_0}{L^2} \\frac{\\partial^2 c'}{\\partial x'^2} $$\nThe relative importance of the advection and diffusion terms can be compared by examining their magnitudes. Let us consider the spatial part of the equation, which represents the balance between the two transport mechanisms at steady state, or simply the instantaneous spatial balance.\nThe magnitude of the advection term is scaled as:\n$$ | \\text{Advection Term} | = \\left| u \\frac{\\partial c}{\\partial x} \\right| \\sim u \\frac{C_0}{L} $$\nThe magnitude of the diffusion term is scaled as:\n$$ | \\text{Diffusion Term} | = \\left| D \\frac{\\partial^2 c}{\\partial x^2} \\right| \\sim D \\frac{C_0}{L^2} $$\nThe ratio of advective to diffusive transport is therefore the ratio of the magnitudes of these two terms. This ratio defines the required dimensionless number:\n$$ \\text{Ratio} = \\frac{| \\text{Advection Term} |}{| \\text{Diffusion Term} |} \\sim \\frac{u \\frac{C_0}{L}}{D \\frac{C_0}{L^2}} = \\frac{uL}{D} $$\nThis dimensionless group is known as the Péclet number, denoted as $Pe$.\n$$ Pe = \\frac{uL}{D} $$\nThe Péclet number provides a direct measure of the relative strength of advection versus diffusion over a characteristic length scale $L$.\n\nNow, we evaluate this number using the provided data:\n- Diffusion coefficient, $D = 0.5\\,\\mu\\text{m}^2/\\text{s}$\n- Flow speed, $u = 0.05\\,\\mu\\text{m}/\\text{s}$\n- Embryo length, $L = 100\\,\\mu\\text{m}$\n\nThe units are consistent, so no conversion is necessary.\n$$ Pe = \\frac{(0.05\\,\\mu\\text{m}/\\text{s}) (100\\,\\mu\\text{m})}{0.5\\,\\mu\\text{m}^2/\\text{s}} = \\frac{5}{0.5} = 10 $$\nThe problem requires the answer to be rounded to three significant figures. The calculated value is exactly $10$. Expressed to three significant figures, this is $10.0$.\n\nBased on well-established criteria from transport theory, if $Pe \\gg 1$, transport is dominated by advection. If $Pe \\ll 1$, transport is dominated by diffusion. If $Pe \\approx 1$, both mechanisms are of comparable importance. In this case, $Pe = 10.0$, which is substantially greater than $1$. Therefore, we conclude that advection is the dominant mechanism for the spatial localization of the cytoplasmic determinant under these conditions. The cytoplasmic flow is more effective at transporting the determinant across the embryo than random diffusion.", "answer": "$$\n\\boxed{10.0}\n$$", "id": "2626718"}, {"introduction": "The asymmetric localization of determinants is a key mechanism for generating diversity among daughter cells, but how does this process play out at the molecular level? This problem tackles the inherent stochasticity of segregating a finite number of molecules during cell division. You will use fundamental probability principles to model this process and derive the coefficient of variation, providing a quantitative measure of the reliability and noise in the inheritance of cell fate determinants [@problem_id:2626786].", "problem": "A developing embryo contains $N$ identical cytoplasmic determinant molecules that are homogeneously mixed prior to cytokinesis. At division, each molecule independently enters a specified daughter cell (call it Daughter A) with probability $p$ (with $0  p  1$), reflecting the geometric and mechanical biases of the cleavage plane; the remaining molecules enter the other daughter cell. Let $X$ denote the random variable for the number of determinant molecules inherited by Daughter A.\n\nUsing only fundamental probability principles and the definition of the coefficient of variation (CV), derive the coefficient of variation of $X$ as a function of $N$ and $p$. Provide your answer as a single closed-form algebraic expression in terms of $N$ and $p$. No rounding is required. Report the result as a dimensionless quantity without a percentage sign. For reference, in a perfectly symmetric division one would have $p = \\frac{1}{2}$, but you must leave your final result in terms of general $p$.", "solution": "The problem as stated is scientifically grounded and mathematically well-posed. We proceed with the derivation.\n\nThe problem asks for the coefficient of variation, $\\text{CV}_X$, of the random variable $X$, which represents the number of cytoplasmic determinant molecules inherited by a specific daughter cell, Daughter A. The total number of molecules is $N$. Each of the $N$ molecules enters Daughter A independently with a probability $p$.\n\nLet us first model the random variable $X$. We can define $N$ independent indicator random variables, $X_i$ for $i \\in \\{1, 2, \\dots, N\\}$, where each $X_i$ corresponds to the fate of the $i$-th molecule.\nLet $X_i = 1$ if the $i$-th molecule enters Daughter A, and $X_i = 0$ otherwise.\nAccording to the problem statement, the probability of a \"success\" (the molecule entering Daughter A) is $P(X_i = 1) = p$. Consequently, the probability of \"failure\" is $P(X_i = 0) = 1-p$.\nThese are Bernoulli trials.\n\nThe total number of molecules in Daughter A, $X$, is the sum of these individual indicator variables:\n$$X = \\sum_{i=1}^{N} X_i$$\nThis formulation correctly identifies $X$ as a binomially distributed random variable, $X \\sim \\text{Binomial}(N, p)$. We will derive its properties from fundamental principles as requested.\n\nThe coefficient of variation is defined as the ratio of the standard deviation to the mean:\n$$\\text{CV}_X = \\frac{\\sigma_X}{\\mu_X}$$\nwhere $\\mu_X = E[X]$ is the expected value (mean) of $X$, and $\\sigma_X = \\sqrt{\\text{Var}(X)}$ is the standard deviation of $X$. We require $\\mu_X \\neq 0$, which is guaranteed since $N$ is a positive integer and $p > 0$.\n\nFirst, we calculate the mean, $\\mu_X$.\nThe expected value of each Bernoulli trial $X_i$ is:\n$$E[X_i] = 1 \\cdot P(X_i = 1) + 0 \\cdot P(X_i = 0) = 1 \\cdot p + 0 \\cdot (1-p) = p$$\nBy the linearity of expectation, the mean of the sum is the sum of the means:\n$$\\mu_X = E[X] = E\\left[\\sum_{i=1}^{N} X_i\\right] = \\sum_{i=1}^{N} E[X_i] = \\sum_{i=1}^{N} p = Np$$\n\nNext, we calculate the variance, $\\text{Var}(X) = \\sigma_X^2$.\nFirst, we find the variance of a single indicator variable, $X_i$:\n$$\\text{Var}(X_i) = E[X_i^2] - (E[X_i])^2$$\nFor a Bernoulli variable, $X_i$ takes values $0$ or $1$, so $X_i^2 = X_i$. Thus, $E[X_i^2] = E[X_i] = p$.\nSubstituting this into the variance formula:\n$$\\text{Var}(X_i) = p - p^2 = p(1-p)$$\nThe problem states that the molecules segregate independently. Therefore, the random variables $X_i$ are mutually independent. For a sum of independent random variables, the variance of the sum is the sum of the variances:\n$$\\sigma_X^2 = \\text{Var}(X) = \\text{Var}\\left(\\sum_{i=1}^{N} X_i\\right) = \\sum_{i=1}^{N} \\text{Var}(X_i) = \\sum_{i=1}^{N} p(1-p) = Np(1-p)$$\n\nNow we have the necessary components to calculate the coefficient of variation. The standard deviation is the square root of the variance:\n$$\\sigma_X = \\sqrt{Np(1-p)}$$\nWe substitute the expressions for the mean $\\mu_X$ and the standard deviation $\\sigma_X$ into the definition of the coefficient of variation:\n$$\\text{CV}_X = \\frac{\\sigma_X}{\\mu_X} = \\frac{\\sqrt{Np(1-p)}}{Np}$$\nTo obtain the final closed-form expression, we simplify this result:\n$$\\text{CV}_X = \\frac{\\sqrt{N} \\sqrt{p(1-p)}}{N p} = \\frac{\\sqrt{p(1-p)}}{\\sqrt{N} p}$$\nFurther simplification by separating the terms involving $p$:\n$$\\text{CV}_X = \\frac{1}{\\sqrt{N}} \\cdot \\frac{\\sqrt{p}\\sqrt{1-p}}{p} = \\frac{1}{\\sqrt{N}} \\cdot \\frac{\\sqrt{1-p}}{\\sqrt{p}}$$\nCombining the terms under a single square root gives the final expression:\n$$\\text{CV}_X = \\sqrt{\\frac{1-p}{Np}}$$\nThis result is a dimensionless quantity expressed as a function of $N$ and $p$, as required.", "answer": "$$\\boxed{\\sqrt{\\frac{1-p}{Np}}}$$", "id": "2626786"}, {"introduction": "Modern developmental biology relies on quantitative analysis of live-imaging data to understand the dynamics of patterning molecules. This advanced practice bridges the gap between theoretical models and experimental observation by tasking you with building a computational inference pipeline [@problem_id:2626753]. By applying a Bayesian linear model to synthetic fluorescence data, you will learn how to estimate key kinetic parameters like synthesis and degradation rates, a core skill for any quantitative biologist.", "problem": "You are given live-imaging fluorescence intensity profiles along a one-dimensional embryonic axis for a cytoplasmic determinant protein whose abundance evolves according to first-order kinetics. The aim is to infer the spatially varying synthesis rate and the global decay rate using a Bayesian linear model derived from first principles. Work in arbitrary fluorescence units (a.u.) for intensity, seconds for time, synthesis in a.u./s, and decay in $s^{-1}$. All outputs must be reported in these units. Angles are not used.\n\nBase assumptions grounded in well-tested biological principles:\n- From the Central Dogma of Molecular Biology (CDMB), translation produces protein at a rate proportional to local messenger ribonucleic acid (mRNA), which we abstract here as a spatially varying synthesis field. The loss of protein via degradation and dilution is well described by first-order kinetics.\n- Fluorescence intensity is proportional to protein abundance. Set the proportionality constant to $1$ for convenience.\n- Imaging noise is dominated by photon shot noise that is well approximated by a Gaussian distribution when signal counts are not vanishingly small.\n\nModel and parameterization:\n- Let $I(x,t)$ denote fluorescence intensity at position $x$ and time $t$. Assume a first-order kinetic model at each $x$:\n$$\\frac{dI(x,t)}{dt} = s(x) - k\\,I(x,t),$$\nwhere $s(x)$ is the local synthesis rate (a.u./s) and $k$ is a global decay rate ($s^{-1}$) that applies uniformly across $x$.\n- Approximate the dynamics with an explicit Euler step of size $\\Delta t$:\n$$I(x,t+\\Delta t) \\approx I(x,t) + \\Delta t \\left(s(x) - k\\,I(x,t)\\right) + \\eta(x,t),$$\nwhere $\\eta(x,t) \\sim \\mathcal{N}(0,\\sigma^2)$ independently across $x$ and $t$.\n- Discretize space into $M$ uniform bins and assume $s(x)$ is piecewise-constant: $s(x)=s_j$ for positions $x$ in bin $j \\in \\{1,\\ldots,M\\}$. Define\n$$y_{t,x} \\equiv I(x,t+\\Delta t)-I(x,t) = \\Delta t \\sum_{j=1}^{M} \\mathbf{1}\\{x \\in \\text{bin } j\\}\\,s_j - \\Delta t\\,k\\,I(x,t) + \\eta(x,t).$$\nCollecting all observations into a vector $\\mathbf{y}$ yields a linear model $\\mathbf{y} = \\mathbf{X}\\,\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$ with parameter vector $\\boldsymbol{\\theta} = [s_1,\\ldots,s_M,k]^\\top$.\n\nBayesian specification:\n- Prior: $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0,\\boldsymbol{\\Sigma}_0)$ with $\\boldsymbol{\\mu}_0=\\mathbf{0}$ and $\\boldsymbol{\\Sigma}_0=\\mathrm{diag}(\\sigma_s^2,\\ldots,\\sigma_s^2,\\sigma_k^2)$.\n- Likelihood: $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2 \\mathbf{I})$ with known $\\sigma^2$.\n\nTasks:\n- Implement a program that:\n  $1.$ Generates synthetic spatiotemporal data $(I(x,t))$ under the model for each test case described below (using the specified random seeds for reproducibility), starting from $I(x,0)=0$ for all $x$.\n  $2.$ Constructs the design matrix $\\mathbf{X}$ and response vector $\\mathbf{y}$ from the generated data using the Euler relation above.\n  $3.$ Computes the Gaussian posterior mean $\\boldsymbol{\\mu}_N$ for $\\boldsymbol{\\theta}$ using Bayes’ theorem for linear-Gaussian models, and reports the posterior means for the decay rate and the synthesis rates.\n\nInference output specification:\n- For each test case, output a list whose first entry is the posterior mean of $k$ (units $s^{-1}$), followed by the posterior means of $s_1,\\ldots,s_M$ (units a.u./s) in bin order from anterior to posterior (left to right in the discretization).\n- Your program should produce a single line of output containing the results as a comma-separated list of these per-test-case lists, enclosed in square brackets. For example: $[[k_1,s_{1,1},\\ldots,s_{1,M_1}],[k_2,s_{2,1},\\ldots],[k_3,\\ldots]]$ with no additional text.\n\nHyperparameters (shared unless stated otherwise):\n- Prior variances: $\\sigma_s^2 = 100.0$ and $\\sigma_k^2 = 100.0$.\n- Initial condition: $I(x,0)=0$ for all $x$.\n- All units are as specified above.\n\nTest suite:\n- Test case $1$ (general case):\n  - $M=4$, number of spatial points $N_x=40$, number of time frames $N_t=30$, $\\Delta t=2.0$.\n  - True parameters: $k=0.05$, $(s_1,s_2,s_3,s_4)=(1.0,0.6,0.2,0.0)$.\n  - Noise standard deviation: $\\sigma=0.05$.\n  - Random seed: $123$.\n- Test case $2$ (boundary case with near-zero decay and uniform synthesis):\n  - $M=3$, $N_x=30$, $N_t=60$, $\\Delta t=1.0$.\n  - True parameters: $k=0.001$, $(s_1,s_2,s_3)=(0.2,0.2,0.2)$.\n  - Noise standard deviation: $\\sigma=0.02$.\n  - Random seed: $456$.\n- Test case $3$ (edge case with fast decay and short time step):\n  - $M=2$, $N_x=20$, $N_t=50$, $\\Delta t=0.2$.\n  - True parameters: $k=0.5$, $(s_1,s_2)=(1.5,0.5)$.\n  - Noise standard deviation: $\\sigma=0.05$.\n  - Random seed: $789$.\n\nRequired final output format:\n- A single line containing a list of lists, one per test case, where each inner list is ordered as $[k_{\\text{post-mean}}, s_{1,\\text{post-mean}}, \\ldots, s_{M,\\text{post-mean}}]$. Do not print any other text. Units: $k$ in $s^{-1}$ and $s_j$ in a.u./s. No percentage symbols and no angles are involved.", "solution": "The problem requires the implementation of a Bayesian inference procedure to estimate parameters of a first-order kinetic model for protein concentration dynamics. The problem is scientifically sound, mathematically well-posed, and provides all necessary information for a unique solution. I will proceed with the solution.\n\nThe physical process is described by the ordinary differential equation for the fluorescence intensity $I(x,t)$ at position $x$ and time $t$:\n$$ \\frac{dI(x,t)}{dt} = s(x) - k\\,I(x,t) $$\nHere, $s(x)$ is the spatially varying synthesis rate and $k$ is the global first-order decay rate. The problem directs us to use an explicit Euler method for numerical approximation. For a time step $\\Delta t$, the intensity at time $t+\\Delta t$ is approximated as:\n$$ I(x,t+\\Delta t) \\approx I(x,t) + \\Delta t \\left(s(x) - k\\,I(x,t)\\right) + \\eta(x,t) $$\nwhere $\\eta(x,t)$ is a Gaussian noise term with mean $0$ and variance $\\sigma^2$, i.e., $\\eta(x,t) \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nTo frame this as a linear regression problem, we rearrange the equation to isolate the observables and parameters:\n$$ I(x,t+\\Delta t) - I(x,t) = \\Delta t \\cdot s(x) - \\Delta t \\cdot k \\cdot I(x,t) + \\eta(x,t) $$\nThe synthesis rate $s(x)$ is assumed to be piecewise constant over $M$ spatial bins. Let $s_j$ be the synthesis rate in bin $j$. We can write $s(x) = \\sum_{j=1}^{M} s_j \\mathbf{1}\\{x \\in \\text{bin } j\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nLet's define the response variable for an observation at $(x,t)$ as $y_{t,x} = I(x,t+\\Delta t) - I(x,t)$. The model equation for a single observation becomes:\n$$ y_{t,x} = \\Delta t \\sum_{j=1}^{M} \\mathbf{1}\\{x \\in \\text{bin } j\\} s_j - (\\Delta t \\cdot I(x,t)) k + \\eta(x,t) $$\nThis is a linear model. We must estimate the parameter vector $\\boldsymbol{\\theta} = [s_1, s_2, \\ldots, s_M, k]^\\top$.\n\nWe collect all observations over space and time. Let there be $N_x$ spatial points and $N_t$ time points. The measurements for the change in intensity are taken from $t=0$ to $t=(N_t-2)\\Delta t$. This gives $N_{obs} = (N_t-1) \\times N_x$ total observations. Let $\\mathbf{y}$ be the $N_{obs} \\times 1$ vector of all $y_{t,x}$ values stacked. Let $\\mathbf{X}$ be the corresponding $N_{obs} \\times (M+1)$ design matrix. Each row of $\\mathbf{X}$ corresponds to one observation $(t,x)$. If point $x$ belongs to bin $j^*$, the row of $\\mathbf{X}$ is:\n$$ \\mathbf{x}_{(t,x)}^\\top = [ 0, \\ldots, \\Delta t, \\ldots, 0, -\\Delta t \\cdot I(x,t) ] $$\nwhere the term $\\Delta t$ is at the $j^*$-th position (from $1$ to $M$) and the final element is the coefficient of $k$. The full system is expressed as the linear model:\n$$ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon} $$\nwhere $\\boldsymbol{\\varepsilon}$ is the vector of noise terms, with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$.\n\nThe problem specifies a Bayesian framework. The prior on the parameters $\\boldsymbol{\\theta}$ is a Gaussian distribution: $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)$, with $\\boldsymbol{\\mu}_0 = \\mathbf{0}$ and $\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}(\\sigma_s^2, \\ldots, \\sigma_s^2, \\sigma_k^2)$. The likelihood is $p(\\mathbf{y}|\\boldsymbol{\\theta}) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})\\right)$.\n\nAccording to Bayes' theorem for a linear-Gaussian model, the posterior distribution $p(\\boldsymbol{\\theta}|\\mathbf{y})$ is also Gaussian, $\\boldsymbol{\\theta}|\\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_N, \\boldsymbol{\\Sigma}_N)$, with posterior covariance $\\boldsymbol{\\Sigma}_N$ and posterior mean $\\boldsymbol{\\mu}_N$ given by:\n$$ \\boldsymbol{\\Sigma}_N = \\left(\\boldsymbol{\\Sigma}_0^{-1} + \\frac{1}{\\sigma^2}\\mathbf{X}^\\top\\mathbf{X}\\right)^{-1} $$\n$$ \\boldsymbol{\\mu}_N = \\boldsymbol{\\Sigma}_N \\left(\\boldsymbol{\\Sigma}_0^{-1}\\boldsymbol{\\mu}_0 + \\frac{1}{\\sigma^2}\\mathbf{X}^\\top\\mathbf{y}\\right) $$\nGiven that the prior mean $\\boldsymbol{\\mu}_0$ is the zero vector, the expression for the posterior mean simplifies to:\n$$ \\boldsymbol{\\mu}_N = \\boldsymbol{\\Sigma}_N \\left(\\frac{1}{\\sigma^2}\\mathbf{X}^\\top\\mathbf{y}\\right) $$\nThe task is to compute $\\boldsymbol{\\mu}_N$ for each test case. This involves three steps:\n1.  **Synthetic Data Generation**: For each test case, we generate the spatiotemporal data $I(x,t)$ for $t \\in \\{0, \\Delta t, \\ldots, (N_t-1)\\Delta t\\}$ and $x \\in \\{1, \\ldots, N_x\\}$, starting from $I(x,0)=0$ and using the provided true parameters and the discrete-time evolution equation with added noise at each step.\n2.  **Assembly of $\\mathbf{X}$ and $\\mathbf{y}$**: From the generated data $I(x,t)$, we construct the vector $\\mathbf{y}$ of intensity differences and the design matrix $\\mathbf{X}$ as described above.\n3.  **Posterior Mean Calculation**: We then use the formulas for $\\boldsymbol{\\Sigma}_N$ and $\\boldsymbol{\\mu}_N$ to compute the posterior mean of the parameters $\\boldsymbol{\\theta}$. The prior covariance matrix $\\boldsymbol{\\Sigma}_0$ is diagonal, so its inverse $\\boldsymbol{\\Sigma}_0^{-1}$ is also diagonal with elements $1/\\sigma_s^2$ and $1/\\sigma_k^2$. The final vector $\\boldsymbol{\\mu}_N$ will contain the estimates for $[s_1, \\ldots, s_M, k]^\\top$. The results must be rearranged to the specified output format $[k, s_1, \\ldots, s_M]$.\n\nThe implementation will proceed as follows for each test case:\n- Set up the simulation parameters ($M, N_x, N_t, \\Delta t$) and true model parameters ($k, \\{s_j\\}$).\n- Initialize a random number generator with the specified seed.\n- Generate the $N_t \\times N_x$ intensity matrix $I$ by iterating the Euler step.\n- Construct the $((N_t-1)N_x) \\times 1$ response vector $\\mathbf{y}$ and the $((N_t-1)N_x) \\times (M+1)$ design matrix $\\mathbf{X}$.\n- Construct the prior inverse covariance matrix $\\boldsymbol{\\Sigma}_0^{-1}$.\n- Calculate the posterior precision matrix $\\boldsymbol{\\Sigma}_N^{-1} = \\boldsymbol{\\Sigma}_0^{-1} + (1/\\sigma^2)\\mathbf{X}^\\top\\mathbf{X}$.\n- Invert it to find the posterior covariance $\\boldsymbol{\\Sigma}_N$.\n- Calculate the posterior mean $\\boldsymbol{\\mu}_N = \\boldsymbol{\\Sigma}_N( (1/\\sigma^2) \\mathbf{X}^\\top \\mathbf{y})$.\n- Extract and reorder the elements of $\\boldsymbol{\\mu}_N$ to match the required output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_inference_for_case(case_params):\n    \"\"\"\n    Generates synthetic data and performs Bayesian inference for one test case.\n    \"\"\"\n    # 1. Unpack parameters and set up simulation\n    M = case_params['M']\n    N_x = case_params['Nx']\n    N_t = case_params['Nt']\n    delta_t = case_params['dt']\n    k_true = case_params['k_true']\n    s_true = np.array(case_params['s_true'])\n    sigma = case_params['sigma']\n    seed = case_params['seed']\n\n    rng = np.random.default_rng(seed)\n\n    # 2. Generate synthetic data\n    pts_per_bin = N_x // M\n    s_true_map = np.repeat(s_true, pts_per_bin)\n    \n    intensities = np.zeros((N_t, N_x))  # I(x,0) = 0 is satisfied\n\n    for t in range(N_t - 1):\n        dI_dt = s_true_map - k_true * intensities[t, :]\n        noise = rng.normal(loc=0.0, scale=sigma, size=N_x)\n        intensities[t+1, :] = intensities[t, :] + delta_t * dI_dt + noise\n\n    # 3. Construct the design matrix X and response vector y\n    n_obs = (N_t - 1) * N_x\n    n_params = M + 1 # s_1, ..., s_M, k\n\n    # Response vector y (flattened over space, then time)\n    y = (intensities[1:, :] - intensities[:-1, :]).flatten()\n\n    # Design matrix X\n    X = np.zeros((n_obs, n_params))\n    \n    # Fill columns for s_j\n    # Create the block for one time point\n    X_t_block = np.zeros((N_x, M))\n    for j in range(M):\n        start_idx = j * pts_per_bin\n        end_idx = (j + 1) * pts_per_bin\n        X_t_block[start_idx:end_idx, j] = delta_t\n    \n    # Tile this block for all time points\n    X[:, :M] = np.tile(X_t_block, (N_t - 1, 1))\n\n    # Fill the last column for k\n    I_flat = intensities[:-1, :].flatten()\n    X[:, M] = -delta_t * I_flat\n    \n    # 4. Compute the posterior mean using Bayes' theorem for linear-Gaussian models\n    sigma_sq = sigma**2\n    sigma_s_sq = 100.0\n    sigma_k_sq = 100.0\n\n    # Prior inverse covariance matrix (Sigma_0^-1)\n    Sigma_0_inv_diag = np.array([1.0/sigma_s_sq] * M + [1.0/sigma_k_sq])\n    Sigma_0_inv = np.diag(Sigma_0_inv_diag)\n    \n    # Posterior precision matrix (Sigma_N^-1)\n    # Sigma_N^-1 = Sigma_0^-1 + (1/sigma^2) * X.T @ X\n    XTX = X.T @ X\n    Sigma_N_inv = Sigma_0_inv + (1.0/sigma_sq) * XTX\n    \n    # Posterior covariance matrix (Sigma_N)\n    Sigma_N = np.linalg.inv(Sigma_N_inv)\n    \n    # Posterior mean (mu_N)\n    # mu_N = Sigma_N @ (Sigma_0^-1 @ mu_0 + (1/sigma^2) * X.T @ y)\n    # With mu_0 = 0, this simplifies to:\n    XTy = X.T @ y\n    mu_N = Sigma_N @ ((1.0/sigma_sq) * XTy)\n\n    # 5. Format the output\n    # mu_N is ordered as [s_1, ..., s_M, k]\n    k_posterior_mean = mu_N[M]\n    s_posterior_means = mu_N[:M].tolist()\n    \n    # Required output format is [k, s_1, ..., s_M]\n    result = [k_posterior_mean] + s_posterior_means\n    \n    return result\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"M\": 4, \"Nx\": 40, \"Nt\": 30, \"dt\": 2.0,\n            \"k_true\": 0.05, \"s_true\": (1.0, 0.6, 0.2, 0.0),\n            \"sigma\": 0.05, \"seed\": 123\n        },\n        # Test case 2\n        {\n            \"M\": 3, \"Nx\": 30, \"Nt\": 60, \"dt\": 1.0,\n            \"k_true\": 0.001, \"s_true\": (0.2, 0.2, 0.2),\n            \"sigma\": 0.02, \"seed\": 456\n        },\n        # Test case 3\n        {\n            \"M\": 2, \"Nx\": 20, \"Nt\": 50, \"dt\": 0.2,\n            \"k_true\": 0.5, \"s_true\": (1.5, 0.5),\n            \"sigma\": 0.05, \"seed\": 789\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_inference_for_case(case)\n        all_results.append(result)\n\n    # Format the final output string as a list of lists.\n    # e.g., [[k1,s11,s12,...],[k2,s21,...]]\n    def format_list(lst):\n        return f\"[{','.join(f'{x:.10f}' for x in lst)}]\"\n        \n    output_str = f\"[{','.join(format_list(res) for res in all_results)}]\"\n    \n    print(output_str)\n\n# Run the solver\nsolve()\n```", "id": "2626753"}]}