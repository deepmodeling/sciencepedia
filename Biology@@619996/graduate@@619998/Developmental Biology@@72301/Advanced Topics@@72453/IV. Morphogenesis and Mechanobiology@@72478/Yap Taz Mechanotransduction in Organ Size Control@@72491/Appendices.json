{"hands_on_practices": [{"introduction": "A central observation in mechanobiology is that the nuclear fraction of the transcriptional co-activator YAP often increases with extracellular matrix stiffness in a switch-like manner. This first practice helps you build a quantitative intuition for this phenomenon by applying the Hill equation, a classic model for describing cooperative biological processes. By deriving this relationship from first principles and calculating a specific outcome, you will gain a concrete understanding of how parameters like the half-activation stiffness ($K$) and the Hill coefficient ($n$) define the sensitivity and steepness of the cell's response to mechanical cues [@problem_id:2688216].", "problem": "Yes-associated protein (YAP) is a mechanosensitive transcriptional co-regulator whose nuclear localization increases with substrate stiffness due to cytoskeletal tension and focal adhesion maturation. Consider a coarse-grained mechanotransduction module in which a stiffness-sensing complex can exist in an inactive state or an active state that promotes nuclear import of YAP. Assume the following well-tested facts and definitions as the fundamental base: (i) equilibrium occupancy of a receptor by a ligand follows mass-action balance; (ii) cooperative activation of a complex by an input can be captured by an effective Hill-Langmuir form arising from $n$-th order binding or concerted transition; (iii) the half-activation parameter $K$ is the input value at which the active-state probability is $1/2$; and (iv) at steady state, the nuclear fraction of YAP is proportional to the active-state probability of the mechanosensor, with the proportionality set to unity by normalization of total YAP. Treat substrate Young’s modulus $E$ (in $\\mathrm{kPa}$) as the effective input to the sensor.\nStarting from these assumptions and without assuming any particular target formula, derive an expression for the steady-state nuclear fraction $N(E)$ as a function of $E$, the half-activation stiffness $K$, and the effective Hill coefficient $n$. Then, using your derived expression, compute $N$ for $E = 10$ $\\mathrm{kPa}$, $K = 8$ $\\mathrm{kPa}$, and $n = 3$. Express the final value of $N$ as a unitless decimal rounded to four significant figures.", "solution": "The problem requires the derivation of an expression for the steady-state nuclear fraction of YAP, denoted as $N(E)$, as a function of substrate Young's modulus $E$, a half-activation stiffness $K$, and a Hill coefficient $n$. The derivation must proceed from the provided assumptions.\n\nAssumption (ii) states that the cooperative activation of the stiffness-sensing complex by the input, $E$, can be described by an effective Hill-Langmuir form. The Hill-Langmuir equation models the fractional saturation or activation of a system exhibiting cooperative binding. Let $P_{\\text{active}}(E)$ be the probability that the mechanosensor is in the active state. This probability is the fractional response of the system to the input stiffness $E$. The general form of the Hill equation is:\n$$ P_{\\text{active}}(E) = \\frac{E^n}{K_{A}^n + E^n} $$\nwhere $n$ is the Hill coefficient quantifying the degree of cooperativity, and $K_{A}$ is the apparent activation constant, which corresponds to the value of the input $E$ that yields half-maximal activation.\n\nAssumption (iii) defines the parameter $K$ as the half-activation stiffness, meaning the input value $E$ at which the active-state probability is $1/2$. We must verify that our chosen form of the Hill equation is consistent with this definition. Let us set $E = K$. Then, the expression for the probability becomes:\n$$ P_{\\text{active}}(K) = \\frac{K^n}{K^n + K^n} = \\frac{K^n}{2 K^n} = \\frac{1}{2} $$\nThis is consistent with the definition of $K$ provided in the problem. Therefore, the parameter $K_{A}$ in the general Hill equation corresponds to the parameter $K$ given in the problem statement. The correct expression for the active-state probability is:\n$$ P_{\\text{active}}(E) = \\frac{E^n}{K^n + E^n} $$\n\nAssumption (iv) provides the link between the mechanosensor's state and the nuclear fraction of YAP, $N(E)$. It states that at steady state, $N(E)$ is proportional to $P_{\\text{active}}(E)$, and the proportionality constant is unity ($1$) due to normalization. Thus, we have the direct relationship:\n$$ N(E) = 1 \\cdot P_{\\text{active}}(E) $$\nSubstituting the expression for $P_{\\text{active}}(E)$, we arrive at the derived expression for the steady-state nuclear fraction of YAP:\n$$ N(E) = \\frac{E^n}{K^n + E^n} $$\n\nThe second part of the problem is to compute the value of $N$ for the specific parameters $E = 10 \\ \\mathrm{kPa}$, $K = 8 \\ \\mathrm{kPa}$, and $n = 3$. We substitute these values into the derived formula. The units of $E$ and $K$ are identical and will cancel, yielding a dimensionless result for $N$, as expected for a fraction.\n$$ N = \\frac{10^3}{8^3 + 10^3} $$\nWe proceed with the calculation of the powers:\n$$ 10^3 = 1000 $$\n$$ 8^3 = 8 \\times 8 \\times 8 = 64 \\times 8 = 512 $$\nSubstituting these values back into the expression for $N$:\n$$ N = \\frac{1000}{512 + 1000} = \\frac{1000}{1512} $$\nTo obtain the final numerical answer, we perform the division:\n$$ N \\approx 0.66137566... $$\nThe problem requires the result to be rounded to four significant figures. The fifth significant digit is $7$, which is greater than or equal to $5$, so we round up the fourth significant digit.\n$$ N \\approx 0.6614 $$\nThis is the final numerical answer.", "answer": "$$\n\\boxed{0.6614}\n$$", "id": "2688216"}, {"introduction": "Building on the understanding of how YAP enters the nucleus, we now explore its function as a transcriptional co-activator. This exercise models the next step in the signaling cascade: how the concentration of nuclear YAP drives the expression of target genes controlling cell growth and proliferation. Starting from the law of mass action for protein-DNA binding, you will derive the relationship between nuclear YAP concentration and the fractional occupancy of its binding sites on a gene promoter, and then calculate the resulting transcription rate. This practice [@problem_id:2688132] is essential for connecting an upstream mechanosensing event to its ultimate functional output.", "problem": "In the context of organ size control by the Hippo signaling pathway, Yes-associated protein (YAP) and its paralog, transcriptional coactivator with PDZ-binding motif (TAZ), accumulate in the nucleus under high mechanical input and co-activate transcription by binding to TEA domain transcription factors (TEADs) on target gene promoters. Consider a promoter with multiple TEAD recognition sites, each of which can recruit a YAP–TEAD complex to activate transcription. Assume that the total number of sites is small enough that promoter-bound complex does not appreciably deplete the nuclear pool of active YAP, and that the sites behave identically and independently.\n\nUnder a thermodynamic equilibrium model of ligand–receptor binding, the dissociation constant is defined by $K_{d} = \\frac{[\\text{T}][\\text{Y}]}{[\\text{TY}]}$, where $[\\text{T}]$ is the concentration of free TEAD site on the promoter, $[\\text{Y}]$ is the nuclear concentration of active YAP, and $[\\text{TY}]$ is the concentration of the occupied site (the TEAD–YAP complex at the site). The total site concentration per promoter satisfies $[\\text{T}]_{\\mathrm{tot}} = [\\text{T}] + [\\text{TY}]$. Let $f$ denote the fraction of TEAD sites occupied by YAP, defined by $f = \\frac{[\\text{TY}]}{[\\text{T}]_{\\mathrm{tot}}}$.\n\nAssume the following biologically plausible parameters measured in a stiff microenvironment relevant to growth control in epithelial tissues:\n- Nuclear active YAP concentration $N = 175\\,\\mathrm{nM}$.\n- Dissociation constant for YAP recruitment to a TEAD site on the promoter $K_{d} = 83\\,\\mathrm{nM}$.\n- Number of identical TEAD sites on the promoter $m = 4$.\n- Basal transcription initiation rate per promoter in the absence of YAP–TEAD occupancy $R_{b} = 0.8\\,\\mathrm{mRNA \\cdot min^{-1}}$.\n- Additive activation increment per occupied site $\\alpha = 1.5\\,\\mathrm{mRNA \\cdot min^{-1} \\cdot site^{-1}}$.\n\nStarting from the law of mass action and the definition of $K_{d}$ above, derive an expression for $f$ in terms of $N$ and $K_{d}$, and then use it to compute the expected transcription initiation rate per promoter,\n$$\nR = R_{b} + \\alpha \\, m \\, f \\, .\n$$\nExpress your final transcription rate in $\\mathrm{mRNA \\cdot min^{-1} \\cdot promoter^{-1}}$, and round your answer to four significant figures.", "solution": "The objective is to derive an expression for the fractional occupancy $f$ and then calculate the transcription rate $R$.\n\nWe begin with the definition of the dissociation constant, $K_{d}$, and the conservation of total TEAD sites, $[\\text{T}]_{\\mathrm{tot}}$.\n$$\nK_{d} = \\frac{[\\text{T}][\\text{Y}]}{[\\text{TY}]}\n$$\n$$\n[\\text{T}]_{\\mathrm{tot}} = [\\text{T}] + [\\text{TY}]\n$$\nFrom the second equation, we express the concentration of free sites, $[\\text{T}]$, as:\n$$\n[\\text{T}] = [\\text{T}]_{\\mathrm{tot}} - [\\text{TY}]\n$$\nSubstitute this expression for $[\\text{T}]$ into the equation for $K_{d}$:\n$$\nK_{d} = \\frac{([\\text{T}]_{\\mathrm{tot}} - [\\text{TY}])[\\text{Y}]}{[\\text{TY}]}\n$$\nOur goal is to find an expression for the fractional occupancy, $f = \\frac{[\\text{TY}]}{[\\text{T}]_{\\mathrm{tot}}}$. To reformulate the equation in terms of $f$, we can divide the right-hand side by $[\\text{T}]_{\\mathrm{tot}}$ in both the numerator's parenthesis and the denominator:\n$$\nK_{d} = \\frac{\\left(\\frac{[\\text{T}]_{\\mathrm{tot}}}{[\\text{T}]_{\\mathrm{tot}}} - \\frac{[\\text{TY}]}{[\\text{T}]_{\\mathrm{tot}}}\\right)[\\text{Y}]}{\\frac{[\\text{TY}]}{[\\text{T}]_{\\mathrm{tot}}}}\n$$\nBy substituting the definition of $f$, this simplifies to:\n$$\nK_{d} = \\frac{(1 - f)[\\text{Y}]}{f}\n$$\nWe now solve this algebraic equation for $f$.\n$$\nK_{d} f = (1 - f)[\\text{Y}]\n$$\n$$\nK_{d} f = [\\text{Y}] - f [\\text{Y}]\n$$\n$$\nK_{d} f + f [\\text{Y}] = [\\text{Y}]\n$$\n$$\nf (K_{d} + [\\text{Y}]) = [\\text{Y}]\n$$\nThis gives the expression for the fractional occupancy:\n$$\nf = \\frac{[\\text{Y}]}{K_{d} + [\\text{Y}]}\n$$\nThe problem provides the nuclear concentration of active YAP as $N = 175\\,\\mathrm{nM}$ and the dissociation constant as $K_{d} = 83\\,\\mathrm{nM}$. We identify $[\\text{Y}]$ with $N$.\n$$\nf = \\frac{N}{K_{d} + N}\n$$\nSubstituting the numerical values:\n$$\nf = \\frac{175}{83 + 175} = \\frac{175}{258}\n$$\nNow, we compute the expected transcription initiation rate $R$ using the provided model:\n$$\nR = R_{b} + \\alpha \\, m \\, f\n$$\nThe given parameters are $R_{b} = 0.8\\,\\mathrm{mRNA \\cdot min^{-1}}$, $\\alpha = 1.5\\,\\mathrm{mRNA \\cdot min^{-1} \\cdot site^{-1}}$, and $m = 4$ sites. Substituting these values along with the expression for $f$:\n$$\nR = 0.8 + (1.5)(4)\\left(\\frac{175}{258}\\right)\n$$\n$$\nR = 0.8 + 6 \\left(\\frac{175}{258}\\right)\n$$\n$$\nR = 0.8 + \\frac{1050}{258}\n$$\nPerforming the calculation:\n$$\nR \\approx 0.8 + 4.069767...\n$$\n$$\nR \\approx 4.869767...\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\nR \\approx 4.870\\,\\mathrm{mRNA \\cdot min^{-1} \\cdot promoter^{-1}}\n$$\nThe trailing zero is significant and must be included.", "answer": "$$\n\\boxed{4.870}\n$$", "id": "2688132"}, {"introduction": "Our final practice integrates the concepts from the previous exercises into a realistic research scenario, bridging the gap between theoretical models and experimental data. While the Hill equation provides an excellent model for the YAP response, its parameters must be determined from noisy experimental measurements. This advanced practice [@problem_id:2688230] guides you through the process of using maximum a posteriori (MAP) estimation, a powerful Bayesian method, to infer the most likely values for the Hill parameters $K$ and $n$ from a given dataset. Completing this exercise will equip you with a key computational skill for quantitative biology: fitting models to data in a statistically robust and numerically stable way.", "problem": "In organ size control, Yes-associated protein (YAP) and transcriptional coactivator with PDZ-binding motif (TAZ) transduce mechanical cues from the extracellular matrix to regulate gene expression. A widely replicated observation is that the nuclear fraction of YAP/TAZ increases with substrate stiffness in a monotonic, saturating manner consistent with a Hill-type input-output relation. Consider the following generative model for independent measurements of normalized nuclear fraction.\n\nYou are given a set of independent measurements $\\{(E_i, N_i)\\}_{i=1}^m$, where $E_i$ is the substrate Young’s modulus in kilopascals ($\\mathrm{kPa}$) and $N_i$ is the measured nuclear fraction of YAP/TAZ (dimensionless, between $0$ and $1$ in the noise-free model). Assume the following model structure:\n- The deterministic input-output relation is given by a Hill form with known saturation bounds $N_{\\min} = 0$ and $N_{\\max} = 1$:\n$$\nf(E; K, n) \\equiv \\frac{E^n}{K^n + E^n},\n$$\nwhere $K$ (in $\\mathrm{kPa}$) is the stiffness at half-maximal response and $n$ (dimensionless) is the Hill coefficient.\n- The observation model is additive, independent, zero-mean Gaussian noise:\n$$\nN_i = f(E_i; K, n) + \\varepsilon_i,\\quad \\varepsilon_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma^2).\n$$\n- The prior on parameters encodes positivity and scale via independent Normal priors on their logarithms:\n$$\n\\log K \\sim \\mathcal{N}(m_K, s_K^2), \\quad \\log n \\sim \\mathcal{N}(m_n, s_n^2),\n$$\nwith all logarithms being natural logarithms.\n\nTasks:\n1) Using the assumptions above and the independence of measurements, write the likelihood $p(\\{N_i\\}\\mid \\{E_i\\}, K, n, \\sigma)$ and the corresponding log-likelihood. Then combine with the priors to write the posterior density $p(K, n \\mid \\{E_i, N_i\\}, \\sigma, m_K, s_K, m_n, s_n)$ up to a proportionality constant. Finally, provide the negative log-posterior expression to be minimized for maximum a posteriori estimation.\n2) Propose an algorithm that computes the maximum a posteriori estimates $(\\hat{K}, \\hat{n})$ by minimizing the negative log-posterior with respect to $K$ and $n$. Your algorithm must enforce $K &gt; 0$ and $n &gt; 0$, and it should be numerically stable when $E$ spans orders of magnitude. State any reparameterizations you use and justify them based on the model.\n3) Implement your algorithm as a complete, runnable program that takes no input and applies the procedure to the following test suite. For each test case, produce MAP estimates for $K$ (in $\\mathrm{kPa}$) and $n$ (dimensionless), rounding each to $3$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, flattened across test cases in the order given, namely $\\big[\\hat{K}_1,\\hat{n}_1,\\hat{K}_2,\\hat{n}_2,\\dots\\big]$.\n\nUse the following test suite, where each test case specifies $\\{E_i\\}$ in $\\mathrm{kPa}$, $\\{N_i\\}$ (dimensionless), $\\sigma$ (dimensionless), and prior hyperparameters $(m_K, s_K, m_n, s_n)$:\n\n- Test case $1$ (typical stiffness range, moderate noise):\n  - $E = [\\,0.5,\\,1.0,\\,2.0,\\,4.0,\\,8.0,\\,16.0\\,]$\n  - $N = [\\,0.015495,\\,0.010303,\\,0.180200,\\,0.490000,\\,0.869540,\\,0.939700\\,]$\n  - $\\sigma = 0.05$\n  - $(m_K, s_K, m_n, s_n) = (1.3862943611,\\,0.8,\\,0.6931471806,\\,0.5)$\n- Test case $2$ (broader stiffness range, higher noise):\n  - $E = [\\,0.5,\\,1.0,\\,2.0,\\,5.0,\\,10.0,\\,20.0,\\,40.0\\,]$\n  - $N = [\\,0.006700,\\,0.109400,\\,0.106600,\\,0.363300,\\,0.460000,\\,0.706900,\\,0.811100\\,]$\n  - $\\sigma = 0.10$\n  - $(m_K, s_K, m_n, s_n) = (2.0794415417,\\,0.8,\\,0.4054651081,\\,0.5)$\n- Test case $3$ (limited sub-saturating regime, low noise; potential identifiability challenge):\n  - $E = [\\,0.2,\\,0.4,\\,0.8,\\,1.6\\,]$\n  - $N = [\\,0.005100,\\,0.000597,\\,0.027970,\\,0.280700\\,]$\n  - $\\sigma = 0.03$\n  - $(m_K, s_K, m_n, s_n) = (1.0986122887,\\,0.7,\\,1.0986122887,\\,0.7)$\n\nConstraints and requirements:\n- All computations must be performed assuming $E$ and $K$ are in $\\mathrm{kPa}$ and $n$ is dimensionless.\n- Angles are not used in this problem; no angle unit is required.\n- Your final output must be a single line in the exact format described: a single Python print producing one bracketed, comma-separated list of floats rounded to $3$ decimal places, flattened across test cases as $[\\,\\hat{K}_1,\\hat{n}_1,\\hat{K}_2,\\hat{n}_2,\\hat{K}_3,\\hat{n}_3\\,]$.", "solution": "The objective is to find the maximum a posteriori (MAP) estimates for the parameters $K$ and $n$ of a Hill-type model describing YAP/TAZ nuclear localization as a function of substrate stiffness $E$. This requires the formulation and subsequent minimization of the negative log-posterior probability distribution of the parameters.\n\nThe problem is addressed in two parts as requested: first, the derivation of the objective function (the negative log-posterior), and second, the specification of a numerical algorithm to find its minimum.\n\n**1. Derivation of the Negative Log-Posterior**\n\nLet the parameters of interest be $K$ and $n$. We are given a set of $m$ independent measurements $\\{ (E_i, N_i) \\}_{i=1}^m$.\n\nThe deterministic relationship is the Hill function:\n$$\nf(E_i; K, n) = \\frac{E_i^n}{K^n + E_i^n}\n$$\n\nThe observation model assumes additive, independent, and identically distributed (i.i.d.) Gaussian noise, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. Thus, the likelihood of observing a single data point $N_i$ given $E_i$, $K$, $n$, and $\\sigma$ is given by the probability density function of the Normal distribution:\n$$\np(N_i \\mid E_i, K, n, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(N_i - f(E_i; K, n))^2}{2\\sigma^2} \\right)\n$$\nDue to the independence of measurements, the total likelihood of the dataset $\\{N_i\\}$ is the product of the individual likelihoods:\n$$\np(\\{N_i\\} \\mid \\{E_i\\}, K, n, \\sigma) = \\prod_{i=1}^m p(N_i \\mid E_i, K, n, \\sigma) = \\left(2\\pi\\sigma^2\\right)^{-m/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^m (N_i - f(E_i; K, n))^2 \\right)\n$$\nThe corresponding log-likelihood, denoted $\\mathcal{L}_{\\text{data}}$, is:\n$$\n\\mathcal{L}_{\\text{data}} = \\log p(\\{N_i\\} \\mid \\{E_i\\}, K, n, \\sigma) = -\\frac{m}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^m (N_i - f(E_i; K, n))^2\n$$\nThe priors for the parameters are specified for their logarithms: $\\log K \\sim \\mathcal{N}(m_K, s_K^2)$ and $\\log n \\sim \\mathcal{N}(m_n, s_n^2)$. The joint prior density is thus:\n$$\np(\\log K, \\log n) = p(\\log K) p(\\log n) = \\frac{1}{\\sqrt{2\\pi s_K^2}} \\exp\\left( -\\frac{(\\log K - m_K)^2}{2s_K^2} \\right) \\cdot \\frac{1}{\\sqrt{2\\pi s_n^2}} \\exp\\left( -\\frac{(\\log n - m_n)^2}{2s_n^2} \\right)\n$$\nFrom Bayes' theorem, the posterior probability of the parameters is proportional to the product of the likelihood and the prior:\n$$\np(K, n \\mid \\{E_i, N_i\\}, \\sigma, \\dots) \\propto p(\\{N_i\\} \\mid \\{E_i\\}, K, n, \\sigma) \\cdot p(K,n)\n$$\nIt is more convenient to work with the log-posterior and to reparameterize the problem in terms of $\\theta_K = \\log K$ and $\\theta_n = \\log n$. In this parameter space, the prior is a simple Gaussian. The posterior for $\\theta_K$ and $\\theta_n$ is:\n$$\np(\\theta_K, \\theta_n \\mid \\text{data}) \\propto p(\\text{data} \\mid \\theta_K, \\theta_n) \\cdot p(\\theta_K, \\theta_n)\n$$\nThe log-posterior is therefore:\n$$\n\\log p(\\theta_K, \\theta_n \\mid \\text{data}) = \\log p(\\text{data} \\mid \\theta_K, \\theta_n) + \\log p(\\theta_K) + \\log p(\\theta_n) + C\n$$\nwhere $C$ is a constant of normalization. Substituting the expressions for the log-likelihood and log-priors:\n$$\n\\log p(\\theta_K, \\theta_n \\mid \\dots) = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^m (N_i - f(E_i; e^{\\theta_K}, e^{\\theta_n}))^2 - \\frac{(\\theta_K - m_K)^2}{2s_K^2} - \\frac{(\\theta_n - m_n)^2}{2s_n^2} + C'\n$$\nThe MAP estimate is found by maximizing this log-posterior, which is equivalent to minimizing its negative. The negative log-posterior, which we will denote as $\\mathcal{L}_{\\text{MAP}}$, is our objective function to be minimized. Ignoring constants that do not affect the location of the minimum:\n$$\n\\mathcal{L}_{\\text{MAP}}(\\theta_K, \\theta_n) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^m \\left(N_i - f(E_i; e^{\\theta_K}, e^{\\theta_n})\\right)^2 + \\frac{(\\theta_K - m_K)^2}{2s_K^2} + \\frac{(\\theta_n - m_n)^2}{2s_n^2}\n$$\n\n**2. Algorithm for MAP Estimation**\n\nTo compute the MAP estimates $(\\hat{K}, \\hat{n})$, we minimize the objective function $\\mathcal{L}_{\\text{MAP}}(\\theta_K, \\theta_n)$ with respect to $\\theta_K$ and $\\theta_n$.\n\nFirst, we perform a reparameterization as suggested by the prior structure:\n$$\n\\theta_K = \\log K, \\quad \\theta_n = \\log n\n$$\nThis reparameterization is advantageous for two primary reasons:\n1.  **Constraint Enforcement**: The parameters $K$ and $n$ must be positive. By optimizing over $\\theta_K, \\theta_n \\in (-\\infty, \\infty)$, the original parameters $K = e^{\\theta_K}$ and $n = e^{\\theta_n}$ are inherently constrained to the positive domain $\\mathbb{R}^+$. This transforms a constrained optimization problem into an unconstrained one.\n2.  **Numerical Stability**: The stiffness values $E$ and the parameter $K$ can span several orders of magnitude. Operations like $E^n$ are prone to numerical overflow or underflow. The Hill function can be rewritten in a more stable form using logarithms:\n    $$\n    f(E; K, n) = \\frac{E^n}{K^n + E^n} = \\frac{1}{1 + (K/E)^n} = \\frac{1}{1 + \\exp(n \\log(K/E))} = \\frac{1}{1 + \\exp(n(\\log K - \\log E))}\n    $$\n    In terms of our optimization variables $\\theta_K$ and $\\theta_n$:\n    $$\n    f(E; \\theta_K, \\theta_n) = \\frac{1}{1 + \\exp(e^{\\theta_n}(\\theta_K - \\log E))}\n    $$\n    This form is numerically robust and avoids large intermediate values.\n\nThe optimization problem is now to find $(\\hat{\\theta}_K, \\hat{\\theta}_n) = \\arg\\min_{\\theta_K, \\theta_n} \\mathcal{L}_{\\text{MAP}}(\\theta_K, \\theta_n)$. Since $\\mathcal{L}_{\\text{MAP}}$ is a continuous and differentiable (though non-linear) function, we can use a quasi-Newton method for unconstrained optimization. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is a suitable choice, as it is efficient and does not require an analytical Hessian matrix.\n\nThe algorithm is as follows:\n1.  For a given test case (data $\\{E_i, N_i\\}$, noise $\\sigma$, and prior hyperparameters $m_K, s_K, m_n, s_n$), define the objective function $\\mathcal{L}_{\\text{MAP}}(\\theta_K, \\theta_n)$ as formulated above.\n2.  Select an initial guess for the optimization, $(\\theta_{K,0}, \\theta_{n,0})$. A logical choice is the mean of the prior distribution, i.e., $(\\theta_{K,0}, \\theta_{n,0}) = (m_K, m_n)$.\n3.  Use a numerical optimization routine (e.g., `scipy.optimize.minimize` with the `BFGS` method) to find the parameters $(\\hat{\\theta}_K, \\hat{\\theta}_n)$ that minimize $\\mathcal{L}_{\\text{MAP}}$.\n4.  Transform the estimated log-parameters back to the original parameter space to obtain the final MAP estimates:\n    $$\n    \\hat{K} = \\exp(\\hat{\\theta}_K), \\quad \\hat{n} = \\exp(\\hat{\\theta}_n)\n    $$\n5.  Repeat this procedure for all test cases provided.\nThe implementation of this algorithm is provided in the final answer section.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the MAP estimation problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"E\": np.array([0.5, 1.0, 2.0, 4.0, 8.0, 16.0]),\n            \"N\": np.array([0.015495, 0.010303, 0.180200, 0.490000, 0.869540, 0.939700]),\n            \"sigma\": 0.05,\n            \"priors\": (1.3862943611, 0.8, 0.6931471806, 0.5)  # (m_k, s_k, m_n, s_n)\n        },\n        {\n            \"E\": np.array([0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 40.0]),\n            \"N\": np.array([0.006700, 0.109400, 0.106600, 0.363300, 0.460000, 0.706900, 0.811100]),\n            \"sigma\": 0.10,\n            \"priors\": (2.0794415417, 0.8, 0.4054651081, 0.5)\n        },\n        {\n            \"E\": np.array([0.2, 0.4, 0.8, 1.6]),\n            \"N\": np.array([0.005100, 0.000597, 0.027970, 0.280700]),\n            \"sigma\": 0.03,\n            \"priors\": (1.0986122887, 0.7, 1.0986122887, 0.7)\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        E_data = case[\"E\"]\n        N_data = case[\"N\"]\n        sigma = case[\"sigma\"]\n        m_k, s_k, m_n, s_n = case[\"priors\"]\n\n        # Pre-compute log(E) for efficiency\n        log_E_data = np.log(E_data)\n\n        def neg_log_posterior(params, E_log, N_obs, sig, mk, sk, mn, sn):\n            \"\"\"\n            Calculates the negative log-posterior for the given parameters.\n            `params` is a list or array [theta_k, theta_n].\n            \"\"\"\n            theta_k, theta_n = params\n            n = np.exp(theta_n)\n\n            # 1. Numerically stable Hill function calculation (vectorized)\n            # f(E) = 1 / (1 + exp(n * (log(K) - log(E))))\n            exponent_term = n * (theta_k - E_log)\n            # Clip exponent to avoid overflow in exp, which can happen for\n            # very large n or large (theta_k - E_log).\n            exponent_term = np.clip(exponent_term, -700, 700)\n            f_model = 1.0 / (1.0 + np.exp(exponent_term))\n\n            # 2. Likelihood term (sum of squared errors)\n            likelihood_term = np.sum((N_obs - f_model)**2) / (2.0 * sig**2)\n\n            # 3. Prior terms for log K and log n\n            prior_k_term = (theta_k - mk)**2 / (2.0 * sk**2)\n            prior_n_term = (theta_n - mn)**2 / (2.0 * sn**2)\n            \n            return likelihood_term + prior_k_term + prior_n_term\n\n        # Initial guess for optimization: the prior means for (log K, log n)\n        initial_guess = [m_k, m_n]\n        \n        # Perform optimization using BFGS\n        opt_result = minimize(\n            fun=neg_log_posterior,\n            x0=initial_guess,\n            args=(log_E_data, N_data, sigma, m_k, s_k, m_n, s_n),\n            method='BFGS'\n        )\n\n        # Extract optimal log-parameters\n        theta_k_hat, theta_n_hat = opt_result.x\n        \n        # Convert back to K and n\n        k_hat = np.exp(theta_k_hat)\n        n_hat = np.exp(theta_n_hat)\n        \n        # Append rounded results to the list\n        results.append(round(k_hat, 3))\n        results.append(round(n_hat, 3))\n\n    # Format the final output as a single comma-separated string in brackets\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2688230"}]}