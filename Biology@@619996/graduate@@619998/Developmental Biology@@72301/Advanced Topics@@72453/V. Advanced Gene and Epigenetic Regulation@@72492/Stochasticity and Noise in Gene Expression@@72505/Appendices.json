{"hands_on_practices": [{"introduction": "The simple birth-death process is a cornerstone model for gene expression, predicting a Poisson distribution of messenger ribonucleic acid (mRNA) counts at steady state. This exercise provides a hands-on opportunity to bridge theory and experiment by analyzing single-molecule fluorescence in situ hybridization (smFISH) data. You will derive and implement a maximum likelihood estimator to infer the transcription rate and, crucially, use a statistical test to evaluate whether the simple Poisson model adequately captures the observed variability or if there is evidence of overdispersion—a common feature in biological systems [@problem_id:2676041].", "problem": "You are given independent single-cell mRNA count data acquired by single-molecule Fluorescence In Situ Hybridization (smFISH) from fixed embryos under a simple birth–death gene expression model at steady state. The biological assumption is that transcription occurs at a constant rate $k$ (in molecules per minute) and mRNA degrades by first-order kinetics with a known per-molecule degradation rate $\\gamma$ (in per minute). Under these assumptions, the stationary mRNA count in a cell, $X$, is distributed as a Poisson random variable with mean $\\lambda$, where $\\lambda = k / \\gamma$. You may assume that cells are identically distributed and independent.\n\nStarting from the definitions of the birth–death process and the Poisson model, you must derive a statistically principled estimator for $k$ using maximum likelihood, given $\\gamma$ and observed counts. Then, using a distributional property of the Poisson model, you must construct a goodness-of-fit test for overdispersion, meaning that the variance of the counts exceeds the mean. The test should be performed at significance level $\\alpha = 0.05$.\n\nYour program must:\n- For each provided dataset, compute the maximum likelihood estimate of $k$ (in molecules per minute).\n- For each dataset, perform a one-sided equidispersion test for overdispersion based on a statistic whose null distribution under the Poisson model does not depend on unknown parameters. Report the corresponding $p$-value for the overdispersion alternative.\n- For each dataset, return a boolean decision indicating whether overdispersion is statistically significant at $\\alpha = 0.05$.\n- Round all reported floating-point outputs to $6$ decimal places. The decision must be a boolean reported as either True or False (case-sensitive).\n- The output must be a single line containing a list of results for all datasets in the order they are listed below. Each result must be a list of the form $[ \\hat{k}, p_{\\text{over}}, \\text{decision} ]$, where $\\hat{k}$ is the estimated transcription rate in molecules per minute, $p_{\\text{over}}$ is the overdispersion $p$-value, and $\\text{decision}$ is True if overdispersion is detected at $\\alpha = 0.05$ and False otherwise.\n- Units: Express $\\hat{k}$ in molecules per minute.\n\nUse the following test suite of datasets. For each case, the count vector is given, along with the known $\\gamma$:\n\n- Case $1$:\n  - $\\gamma = 0.1$ per minute.\n  - Counts: $[18, 22, 17, 19, 21, 24, 20, 19, 23, 18, 20, 21, 19, 22, 20]$.\n- Case $2$:\n  - $\\gamma = 0.2$ per minute.\n  - Counts: $[2, 5, 10, 15, 40, 35, 22, 18, 25, 30, 3, 28, 12, 50, 7]$.\n- Case $3$:\n  - $\\gamma = 0.05$ per minute.\n  - Counts: $[0, 0, 0, 1, 2, 0, 3, 1, 0, 0, 2, 1]$.\n- Case $4$:\n  - $\\gamma = 0.3$ per minute.\n  - Counts: $[4, 6, 3, 5, 4, 7, 5, 6, 4, 5]$.\n\nPrincipled starting points you may use in your derivation:\n- The stationary distribution of the linear birth–death process with constant birth rate $k$ and linear death rate $\\gamma X$ is Poisson with mean $\\lambda = k / \\gamma$.\n- The Poisson probability mass function is $P(X=x \\mid \\lambda) = \\exp(-\\lambda)\\lambda^{x} / x!$ for $x \\in \\{0,1,2,\\dots\\}$.\n- For $n$ independent Poisson observations with common mean $\\lambda$, the likelihood factorizes across observations.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[[\\dots],[\\dots],[\\dots],[\\dots]]$). Each inner list must be ordered as $[\\hat{k}, p_{\\text{over}}, \\text{decision}]$ with all floating-point values rounded to $6$ decimal places and the boolean printed as True or False (case-sensitive).", "solution": "The problem as stated is scientifically grounded, well-posed, and contains sufficient information for a unique solution. It represents a standard and fundamental exercise in statistical inference applied to a canonical model of stochastic gene expression. We shall therefore proceed with a complete derivation and solution.\n\nThe problem is divided into two parts: first, the derivation of the maximum likelihood estimator (MLE) for the transcription rate $k$, and second, the construction of a goodness-of-fit test for overdispersion.\n\n**Part 1: Maximum Likelihood Estimation of Transcription Rate $k$**\n\nWe are given that the mRNA count $X$ in a single cell follows a Poisson distribution with mean $\\lambda = k / \\gamma$. The probability mass function (PMF) is:\n$$\nP(X = x \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!}\n$$\nWe have a set of $n$ independent and identically distributed (i.i.d.) observations, denoted by $\\{x_1, x_2, \\dots, x_n\\}$. The likelihood function $L(\\lambda)$ is the product of the individual probabilities:\n$$\nL(\\lambda \\mid x_1, \\dots, x_n) = \\prod_{i=1}^{n} P(X_i = x_i \\mid \\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{x_i}}{x_i!}\n$$\nThis can be simplified to:\n$$\nL(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} x_i}}{\\prod_{i=1}^{n} x_i!}\n$$\nTo find the value of $\\lambda$ that maximizes this function, it is computationally more convenient to maximize the log-likelihood function, $\\ell(\\lambda) = \\ln L(\\lambda)$:\n$$\n\\ell(\\lambda) = \\ln\\left( \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} x_i}}{\\prod_{i=1}^{n} x_i!} \\right) = -n\\lambda + \\left(\\sum_{i=1}^{n} x_i\\right) \\ln \\lambda - \\sum_{i=1}^{n} \\ln(x_i!)\n$$\nWe find the maximum by taking the derivative of $\\ell(\\lambda)$ with respect to $\\lambda$ and setting it to zero:\n$$\n\\frac{d\\ell}{d\\lambda} = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} x_i = 0\n$$\nSolving for $\\lambda$ yields the maximum likelihood estimator for $\\lambda$:\n$$\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\bar{x}\n$$\nwhere $\\bar{x}$ is the sample mean of the observed counts.\n\nThe problem asks for the estimator of $k$, not $\\lambda$. We use the invariance property of maximum likelihood estimators, which states that if $\\hat{\\theta}$ is the MLE of $\\theta$, then for any function $g(\\theta)$, the MLE of $g(\\theta)$ is $g(\\hat{\\theta})$. Here, $k$ is a function of $\\lambda$, specifically $k = g(\\lambda) = \\lambda\\gamma$. Therefore, the MLE for the transcription rate $k$ is:\n$$\n\\hat{k}_{\\text{MLE}} = \\hat{\\lambda}_{\\text{MLE}} \\cdot \\gamma = \\bar{x} \\cdot \\gamma\n$$\n\n**Part 2: Goodness-of-Fit Test for Overdispersion**\n\nA defining property of a Poisson distribution with mean $\\lambda$ is that its variance is also $\\lambda$. This property is known as equidispersion, i.e., $\\text{Var}(X) = \\mathbb{E}[X]$. Overdispersion is a condition where the observed variance is significantly greater than the mean.\n\nWe formulate the hypothesis test as follows:\n- **Null Hypothesis ($H_0$)**: The data are drawn from a Poisson distribution, so $\\text{Var}(X) = \\mathbb{E}[X]$.\n- **Alternative Hypothesis ($H_1$)**: The data are overdispersed, so $\\text{Var}(X)  \\mathbb{E}[X]$.\n\nThis is a one-sided test. To perform this test, we require a test statistic whose distribution under the null hypothesis is known and does not depend on the unknown parameter $\\lambda$. A suitable statistic is the chi-squared index of dispersion:\n$$\nI = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{\\bar{x}} = \\frac{(n-1)S^2}{\\bar{x}}\n$$\nwhere $\\bar{x}$ is the sample mean and $S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2$ is the unbiased sample variance.\n\nUnder the null hypothesis that the data are i.i.d. Poisson($\\lambda$) observations, and for a reasonably large mean $\\lambda$, this statistic $I$ is approximately distributed as a chi-squared ($\\chi^2$) random variable with $n-1$ degrees of freedom. The single degree of freedom is lost because we use the sample mean $\\bar{x}$ to estimate the true mean $\\lambda$.\n\nThe testing procedure is as follows:\n1.  Calculate the sample mean $\\bar{x}$ and sample variance $S^2$ from the data $\\{x_1, \\dots, x_n\\}$.\n2.  Compute the observed value of the test statistic, $I_{obs} = \\frac{(n-1)S^2}{\\bar{x}}$.\n3.  Since the alternative hypothesis is overdispersion ($S^2  \\bar{x}$), large values of $I_{obs}$ support $H_1$. We therefore perform an upper-tailed test.\n4.  The $p$-value is the probability of observing a test statistic at least as extreme as $I_{obs}$ under the null distribution:\n    $$\n    p_{\\text{over}} = P(\\chi^2_{n-1} \\ge I_{obs})\n    $$\n    This value is computed using the survival function (SF), also known as the complementary cumulative distribution function (1-CDF), of the $\\chi^2$ distribution with $n-1$ degrees of freedom.\n5.  A decision is made by comparing the $p$-value to the significance level $\\alpha = 0.05$. If $p_{\\text{over}}  0.05$, we reject the null hypothesis and conclude that there is statistically significant evidence of overdispersion.\n\nThese derived formulae and procedures will be implemented to analyze the provided datasets.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Derives the MLE for the transcription rate k, performs an overdispersion test,\n    and formats the results for the given datasets.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (0.1, [18, 22, 17, 19, 21, 24, 20, 19, 23, 18, 20, 21, 19, 22, 20]),\n        # Case 2\n        (0.2, [2, 5, 10, 15, 40, 35, 22, 18, 25, 30, 3, 28, 12, 50, 7]),\n        # Case 3\n        (0.05, [0, 0, 0, 1, 2, 0, 3, 1, 0, 0, 2, 1]),\n        # Case 4\n        (0.3, [4, 6, 3, 5, 4, 7, 5, 6, 4, 5]),\n    ]\n\n    results = []\n    alpha = 0.05\n\n    for gamma, counts in test_cases:\n        # Convert counts to a numpy array for vectorized operations\n        counts_np = np.array(counts, dtype=np.float64)\n        n = len(counts_np)\n\n        # Part 1: Maximum Likelihood Estimation of k\n        # The MLE for the Poisson mean lambda is the sample mean.\n        sample_mean = np.mean(counts_np)\n        # Using the invariance property of MLEs, k_hat = gamma * lambda_hat.\n        k_hat = sample_mean * gamma\n\n        # Part 2: Overdispersion Test\n        # The test statistic is the chi-squared index of dispersion.\n        # It is undefined if the mean is 0.\n        if sample_mean == 0:\n            # If all counts are 0, there is no variance. This is not overdispersion.\n            # p-value is 1, as the observed statistic (0) is the smallest possible.\n            p_over = 1.0\n        else:\n            # Unbiased sample variance (ddof=1)\n            sample_variance = np.var(counts_np, ddof=1)\n            # Degrees of freedom for the chi-squared distribution is n-1.\n            df = n - 1\n            # Chi-squared index of dispersion statistic\n            dispersion_index = df * sample_variance / sample_mean\n            # p-value is the upper-tail probability of the chi-squared distribution.\n            p_over = chi2.sf(dispersion_index, df)\n\n        # Decision: significant overdispersion if p-value  alpha\n        decision = p_over  alpha\n\n        results.append([k_hat, p_over, decision])\n\n    # Final print statement in the exact required format.\n    # Each sublist is formatted to a string with no spaces, and then joined.\n    formatted_results = []\n    for k_val, p_val, d_val in results:\n        # Round floats to 6 decimal places, ensuring trailing zeros.\n        k_str = f\"{k_val:.6f}\"\n        p_str = f\"{p_val:.6f}\"\n        # Convert boolean to required string \"True\" or \"False\".\n        d_str = str(d_val)\n        \n        formatted_results.append(f\"[{k_str},{p_str},{d_str}]\")\n    \n    final_output_str = f\"[{','.join(formatted_results)}]\"\n    print(final_output_str)\n\nsolve()\n\n```", "id": "2676041"}, {"introduction": "When gene expression data show more variance than predicted by simple models, a key question arises: what is the source of this extra \"noise\"? This practice introduces a powerful experimental and analytical framework, the dual-reporter assay, to decompose total expression variability into its intrinsic and extrinsic components. By analyzing the covariance between two identically-regulated reporters, you will implement a method to estimate the contributions of cell-wide (extrinsic) fluctuations and gene-specific (intrinsic) stochasticity, providing deeper insight into the cellular environment [@problem_id:2676064].", "problem": "A two-reporter assay measures two fluorescent reporters, denoted by the random variables $X$ and $Y$, across single cells in a homogeneous population. Assume a multiplicative-noise model in which a shared extrinsic factor and independent intrinsic factors act on the reporters in log-space. Specifically, suppose there exist independent normal random variables $E$, $I_{X}$, and $I_{Y}$ with $E \\sim \\mathcal{N}(0,\\sigma_{E}^{2})$, $I_{X} \\sim \\mathcal{N}(0,\\sigma_{I}^{2})$, and $I_{Y} \\sim \\mathcal{N}(0,\\sigma_{I}^{2})$, such that\n$$\nX \\;=\\; \\mu_{X} \\,\\exp\\!\\left(E + I_{X} - \\tfrac{1}{2}(\\sigma_{E}^{2}+\\sigma_{I}^{2})\\right),\\qquad\nY \\;=\\; \\mu_{Y} \\,\\exp\\!\\left(E + I_{Y} - \\tfrac{1}{2}(\\sigma_{E}^{2}+\\sigma_{I}^{2})\\right),\n$$\nwhere $\\mu_{X} \\gt 0$ and $\\mu_{Y} \\gt 0$ are unknown deterministic means. Under this model, the extrinsic factor $E$ is shared by both reporters, modeling cell-to-cell variability (extrinsic noise) as lognormal, while the intrinsic factors $I_{X}$ and $I_{Y}$ are independent between reporters and across cells, modeling reporter-specific fluctuations (intrinsic noise) as lognormal. Define the intrinsic coefficient of variation (CV) as $\\mathrm{CV}_{\\mathrm{int}} \\equiv \\sqrt{\\exp(\\sigma_{I}^{2}) - 1}$ and the extrinsic coefficient of variation as $\\mathrm{CV}_{\\mathrm{ext}} \\equiv \\sqrt{\\exp(\\sigma_{E}^{2}) - 1}$. For each reporter $R \\in \\{X,Y\\}$, let $\\mu_{R} \\equiv \\mathbb{E}[R]$, $\\mathrm{Var}(R)$, and let $\\mathrm{Cov}(X,Y)$.\n\nIn the limit of a large number of cells (so that sample moments converge to population moments), the following identities hold:\n- $\\mathbb{E}[X] = \\mu_{X}$ and $\\mathbb{E}[Y] = \\mu_{Y}$,\n- $\\mathrm{Var}(X) = \\mu_{X}^{2}\\left(\\exp(\\sigma_{E}^{2}+\\sigma_{I}^{2}) - 1\\right)$ and $\\mathrm{Var}(Y) = \\mu_{Y}^{2}\\left(\\exp(\\sigma_{E}^{2}+\\sigma_{I}^{2}) - 1\\right)$,\n- $\\mathrm{Cov}(X,Y) = \\mu_{X}\\mu_{Y}\\left(\\exp(\\sigma_{E}^{2}) - 1\\right)$.\n\nConsequently, if one is given measured moment summaries computed from many cells—namely a measured mean $\\hat{\\mu}_{X}$, a measured mean $\\hat{\\mu}_{Y}$, measured variances $\\widehat{\\mathrm{Var}}(X)$ and $\\widehat{\\mathrm{Var}}(Y)$, and a measured covariance $\\widehat{\\mathrm{Cov}}(X,Y)$—then, under the above model and in the asymptotic regime, the maximum likelihood estimates for the squared coefficients of variation satisfy\n$$\n\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2} \\;=\\; \\max\\!\\left\\{0,\\;\\frac{\\widehat{\\mathrm{Cov}}(X,Y)}{\\hat{\\mu}_{X}\\hat{\\mu}_{Y}}\\right\\},\n$$\nand, writing $s_{X} \\equiv \\widehat{\\mathrm{Var}}(X)/\\hat{\\mu}_{X}^{2}$, $s_{Y} \\equiv \\widehat{\\mathrm{Var}}(Y)/\\hat{\\mu}_{Y}^{2}$, and $c \\equiv \\widehat{\\mathrm{Cov}}(X,Y)/(\\hat{\\mu}_{X}\\hat{\\mu}_{Y})$, the intrinsic component satisfies\n$$\n\\widehat{\\mathrm{CV}}_{\\mathrm{int}}^{2} \\;=\\; \\max\\!\\left\\{0,\\;\\frac{1}{2}\\left(\\frac{s_{X} - \\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}}{1+\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}} + \\frac{s_{Y} - \\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}}{1+\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}}\\right)\\right\\}.\n$$\nFinally, $\\widehat{\\mathrm{CV}}_{\\mathrm{ext}} \\equiv \\sqrt{\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}}$ and $\\widehat{\\mathrm{CV}}_{\\mathrm{int}} \\equiv \\sqrt{\\widehat{\\mathrm{CV}}_{\\mathrm{int}}^{2}}$. The non-negativity operators $\\max\\{\\cdot,0\\}$ enforce the parameter constraints implied by the model.\n\nYour task is to write a program that, given a small test suite of measured moment summaries $\\left(\\hat{\\mu}_{X},\\hat{\\mu}_{Y},\\widehat{\\mathrm{Var}}(X),\\widehat{\\mathrm{Var}}(Y),\\widehat{\\mathrm{Cov}}(X,Y)\\right)$, computes the maximum likelihood estimates $\\widehat{\\mathrm{CV}}_{\\mathrm{int}}$ and $\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}$ for each case.\n\nInput is hard-coded as the following test suite of five-tuples, each corresponding to $\\left(\\hat{\\mu}_{X},\\hat{\\mu}_{Y},\\widehat{\\mathrm{Var}}(X),\\widehat{\\mathrm{Var}}(Y),\\widehat{\\mathrm{Cov}}(X,Y)\\right)$:\n- Test case $1$: $\\left(1000,\\,1500,\\,362500,\\,815625,\\,135000\\right)$.\n- Test case $2$: $\\left(500,\\,700,\\,40000,\\,78400,\\,0\\right)$.\n- Test case $3$: $\\left(2000,\\,2000,\\,160000,\\,160000,\\,160000\\right)$.\n- Test case $4$: $\\left(800,\\,1200,\\,64000,\\,144000,\\,-500\\right)$.\n\nFor each test case, output the pair $\\left[\\widehat{\\mathrm{CV}}_{\\mathrm{int}},\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}\\right]$ as decimal floats. Report all numbers rounded to $6$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is an inner list in the same bracketed, comma-separated format. For example, the output should have the form $\\left[\\left[a_{1},b_{1}\\right],\\left[a_{2},b_{2}\\right],\\ldots\\right]$ with all $a_{k}$ and $b_{k}$ rounded to $6$ decimals. Because coefficients of variation are dimensionless, no physical units are required in the output.", "solution": "The problem statement presented is valid. It is scientifically grounded, well-posed, and objective. It describes a standard model for analyzing noise in gene expression from dual-reporter assays, a common technique in quantitative and systems biology. The model is based on established principles of stochastic processes, specifically the use of log-normal distributions to represent multiplicative noise. The provided equations for the moments ($\\mathbb{E}[R]$, $\\mathrm{Var}(R)$, $\\mathrm{Cov}(X,Y)$) are correct derivations from the specified statistical model for the reporters $X$ and $Y$. Let us verify this foundation. The reporter levels are given by $X = \\mu_{X} \\exp(E + I_{X} - \\frac{1}{2}(\\sigma_{E}^{2}+\\sigma_{I}^{2}))$ and $Y = \\mu_{Y} \\exp(E + I_{Y} - \\frac{1}{2}(\\sigma_{E}^{2}+\\sigma_{I}^{2}))$, where $E \\sim \\mathcal{N}(0,\\sigma_{E}^{2})$, $I_{X} \\sim \\mathcal{N}(0,\\sigma_{I}^{2})$, and $I_{Y} \\sim \\mathcal{N}(0,\\sigma_{I}^{2})$ are independent. The exponent for $X$ is the random variable $W_X = E + I_{X} - \\frac{1}{2}(\\sigma_{E}^{2}+\\sigma_{I}^{2})$, which follows a normal distribution $\\mathcal{N}(-\\frac{1}{2}(\\sigma_{E}^{2}+\\sigma_{I}^{2}), \\sigma_{E}^{2}+\\sigma_{I}^{2})$. The expectation of $X$ is $\\mathbb{E}[X] = \\mu_X \\mathbb{E}[\\exp(W_X)] = \\mu_X \\exp(\\mathbb{E}[W_X] + \\frac{1}{2}\\mathrm{Var}(W_X)) = \\mu_X \\exp(-\\frac{1}{2}(\\sigma_{E}^{2}+\\sigma_{I}^{2}) + \\frac{1}{2}(\\sigma_{E}^{2}+\\sigma_{I}^{2})) = \\mu_X$. This confirms the mean. The variance is $\\mathrm{Var}(X) = \\mu_{X}^{2} \\left((\\exp(\\mathrm{Var}(W_X))-1)\\exp(2\\mathbb{E}[W_X]+\\mathrm{Var}(W_X))\\right) = \\mu_X^2 (\\exp(\\sigma_E^2+\\sigma_I^2)-1)$. The covariance $\\mathrm{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$. The product is $XY = \\mu_X\\mu_Y \\exp(2E+I_X+I_Y - (\\sigma_E^2+\\sigma_I^2))$, whose expectation is $\\mu_X\\mu_Y \\exp(\\sigma_E^2)$. Thus, $\\mathrm{Cov}(X,Y) = \\mu_X\\mu_Y(\\exp(\\sigma_E^2)-1)$. All moment equations are correct. The definitions of intrinsic and extrinsic coefficients of variation, $\\mathrm{CV}_{\\mathrm{int}} \\equiv \\sqrt{\\exp(\\sigma_{I}^{2}) - 1}$ and $\\mathrm{CV}_{\\mathrm{ext}} \\equiv \\sqrt{\\exp(\\sigma_{E}^{2}) - 1}$, are also standard. The problem then provides formulas for the maximum likelihood estimates ($\\widehat{\\mathrm{CV}}_{\\mathrm{int}}$, $\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}$) based on sample moments. These formulas logically follow from the moment equations by applying the method of moments and enforcing non-negativity constraints, which is necessary as squared quantities cannot be negative. The problem is thus a well-defined computational exercise in applying these soundly-derived formulas to given data.\n\nThe computational procedure is as follows. For each test case, given as a tuple of measured moments $(\\hat{\\mu}_{X}, \\hat{\\mu}_{Y}, \\widehat{\\mathrm{Var}}(X), \\widehat{\\mathrm{Var}}(Y), \\widehat{\\mathrm{Cov}}(X,Y))$, we must compute $\\widehat{\\mathrm{CV}}_{\\mathrm{int}}$ and $\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}$.\n\nFirst, we calculate the estimated squared extrinsic coefficient of variation, $\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}$. This is derived from the normalized covariance:\n$$\n\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2} \\;=\\; \\max\\!\\left\\{0,\\;\\frac{\\widehat{\\mathrm{Cov}}(X,Y)}{\\hat{\\mu}_{X}\\hat{\\mu}_{Y}}\\right\\}\n$$\nThe $\\max$ operator ensures that the estimate is non-negative, consistent with its definition as $\\exp(\\sigma_E^2)-1 \\ge 0$. A negative sample covariance, which can arise from sampling error, correctly yields an estimate of zero extrinsic noise.\n\nSecond, we calculate the estimated squared intrinsic coefficient of variation, $\\widehat{\\mathrm{CV}}_{\\mathrm{int}}^{2}$. The total squared coefficient of variation for each reporter, denoted $s_X$ and $s_Y$, is given by $s_{X} = \\widehat{\\mathrm{Var}}(X)/\\hat{\\mu}_{X}^{2}$ and $s_{Y} = \\widehat{\\mathrm{Var}}(Y)/\\hat{\\mu}_{Y}^{2}$. From the model, the population total CV squared is $(\\exp(\\sigma_E^2+\\sigma_I^2)-1) = (1+\\mathrm{CV}_{\\mathrm{ext}}^2)(1+\\mathrm{CV}_{\\mathrm{int}}^2) - 1$. Solving for $\\mathrm{CV}_{\\mathrm{int}}^2$ gives $\\mathrm{CV}_{\\mathrm{int}}^2 = (s_R - \\mathrm{CV}_{\\mathrm{ext}}^2)/(1+\\mathrm{CV}_{\\mathrm{ext}}^2)$ for a reporter $R \\in \\{X,Y\\}$. The provided estimator averages the estimates from both reporters to produce a single, more robust value:\n$$\n\\widehat{\\mathrm{CV}}_{\\mathrm{int}}^{2} \\;=\\; \\max\\!\\left\\{0,\\;\\frac{1}{2}\\left(\\frac{s_{X} - \\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}}{1+\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}} + \\frac{s_{Y} - \\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}}{1+\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}}\\right)\\right\\}\n$$\nThe denominator $1+\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}$ is always greater than or equal to $1$, so there is no risk of division by zero. The $\\max$ operator again enforces the non-negativity of $\\widehat{\\mathrm{CV}}_{\\mathrm{int}}^{2}$.\n\nFinally, the coefficients of variation are obtained by taking the square root of these squared quantities:\n$$\n\\widehat{\\mathrm{CV}}_{\\mathrm{ext}} = \\sqrt{\\widehat{\\mathrm{CV}}_{\\mathrm{ext}}^{2}}\n$$\n$$\n\\widehat{\\mathrm{CV}}_{\\mathrm{int}} = \\sqrt{\\widehat{\\mathrm{CV}}_{\\mathrm{int}}^{2}}\n$$\n\nThis sequence of calculations will be performed for each provided test case. The results will be formatted as specified, with each number rounded to $6$ decimal places.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes intrinsic and extrinsic coefficients of variation from measured moments\n    of a dual-reporter assay, based on a multiplicative noise model.\n    \"\"\"\n    \n    # Test suite of measured moment summaries:\n    # Each tuple is (mu_X, mu_Y, Var(X), Var(Y), Cov(X,Y))\n    test_cases = [\n        (1000, 1500, 362500, 815625, 135000),\n        (500, 700, 40000, 78400, 0),\n        (2000, 2000, 160000, 160000, 160000),\n        (800, 1200, 64000, 144000, -500),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        mu_x, mu_y, var_x, var_y, cov_xy = case\n        \n        # Step 1: Calculate the squared extrinsic CV\n        # This is based on the normalized covariance.\n        mu_x_mu_y_product = mu_x * mu_y\n        if mu_x_mu_y_product == 0:\n            # Handle division by zero, though not expected with problem data\n            # since mu_X  0 and mu_Y  0.\n            c = 0 \n        else:\n            c = cov_xy / mu_x_mu_y_product\n        \n        cv_ext_sq = max(0, c)\n\n        # Step 2: Calculate the squared intrinsic CV\n        # This uses the total normalized variances s_x and s_y.\n        if mu_x == 0 or mu_y == 0:\n            s_x = 0\n            s_y = 0\n        else:\n            s_x = var_x / (mu_x**2)\n            s_y = var_y / (mu_y**2)\n\n        denominator = 1 + cv_ext_sq\n        \n        term_x = (s_x - cv_ext_sq) / denominator\n        term_y = (s_y - cv_ext_sq) / denominator\n        \n        cv_int_sq_est = 0.5 * (term_x + term_y)\n        cv_int_sq = max(0, cv_int_sq_est)\n        \n        # Step 3: Calculate the final CVs by taking the square root\n        cv_ext = np.sqrt(cv_ext_sq)\n        cv_int = np.sqrt(cv_int_sq)\n        \n        results.append([cv_int, cv_ext])\n\n    # Format the output string as per the problem specification:\n    # A list of lists, with each number rounded to 6 decimal places.\n    result_strings = []\n    for res_pair in results:\n        cv_int_val, cv_ext_val = res_pair\n        result_strings.append(f\"[{cv_int_val:.6f},{cv_ext_val:.6f}]\")\n        \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2676064"}, {"introduction": "While steady-state models provide a useful baseline, developmental systems are inherently dynamic, with cell division representing a fundamental event that perturbs cellular homeostasis. This exercise moves from static analysis to dynamic consequences, asking you to model how messenger ribonucleic acid (mRNA) molecules are partitioned between daughter cells during cytokinesis. By applying first principles of probability, you will derive the resulting mRNA distribution and quantify the significant, albeit transient, increase in noise immediately following cell division, highlighting how key biological processes can actively shape cellular variability [@problem_id:2676065].", "problem": "A single gene is expressed constitutively in a cell as a simple birth–death process with constant transcription rate and first-order degradation rate. It is well established that, at steady state prior to cell division, the messenger ribonucleic acid (mRNA) copy number in the mother cell is Poisson-distributed with some mean $\\mu$. During cytokinesis, each of the $N$ mRNA molecules present just before division is independently assigned to daughter A with probability $p \\in (0,1)$ (and to daughter B with probability $1-p$), a mechanistic model commonly called binomial partitioning. Let $K$ denote the mRNA copy number in daughter A immediately after division.\n\nUsing only the definitions of the Poisson distribution, the binomial distribution, and the law of total probability, derive the unconditional distribution of $K$ for daughter A. Then, to quantify the perturbation to steady state at the moment after division, compare the fluctuation level before and after division using the coefficient of variation squared, defined for a nonnegative integer-valued random variable $X$ by $\\mathrm{CV}^{2}(X) \\equiv \\mathrm{Var}(X)/(\\mathbb{E}[X])^{2}$. Compute the ratio $r \\equiv \\mathrm{CV}^{2}(K)/\\mathrm{CV}^{2}(N)$ at the instant after division in terms of $p$ and $\\mu$.\n\nYour final answer must be a single row matrix containing, in order: the closed-form expression for the probability mass function $\\mathbb{P}[K=k]$ as a function of $k \\in \\{0,1,2,\\dots\\}$, $p$, and $\\mu$, and the closed-form expression for $r$. No numerical evaluation or rounding is required, and no units should be included in the final answer.", "solution": "The problem statement has been evaluated and is found to be valid. It is scientifically sound, well-posed, and based on standard models in stochastic biology. All necessary information is provided, and the problem is free of contradictions or ambiguity. We proceed with the derivation.\n\nThe problem requires the derivation of the probability distribution for the messenger ribonucleic acid (mRNA) copy number, $K$, in a daughter cell, and the calculation of a ratio of squared coefficients of variation. This will be accomplished in two parts.\n\nFirst, we derive the unconditional probability mass function (PMF) of $K$.\nThe number of mRNA molecules in the mother cell, $N$, is given to follow a Poisson distribution with mean $\\mu$. The PMF of $N$ is therefore:\n$$\n\\mathbb{P}[N=n] = \\frac{\\mu^{n} \\exp(-\\mu)}{n!} \\quad \\text{for } n \\in \\{0, 1, 2, \\dots\\}\n$$\nDuring cell division, each of the $N$ molecules is partitioned to daughter cell A with probability $p$. Given a specific number of molecules $n$ in the mother cell, the number of molecules $K$ inherited by daughter A follows a binomial distribution with $n$ trials and a success probability of $p$. The conditional PMF of $K$ given $N=n$ is:\n$$\n\\mathbb{P}[K=k | N=n] = \\binom{n}{k} p^{k} (1-p)^{n-k} \\quad \\text{for } k \\in \\{0, 1, \\dots, n\\}\n$$\nNote that $\\mathbb{P}[K=k | N=n] = 0$ if $k  n$.\n\nTo find the unconditional PMF of $K$, $\\mathbb{P}[K=k]$, we use the law of total probability, summing over all possible values of $n$:\n$$\n\\mathbb{P}[K=k] = \\sum_{n=0}^{\\infty} \\mathbb{P}[K=k | N=n] \\mathbb{P}[N=n]\n$$\nSince $\\mathbb{P}[K=k | N=n]$ is zero for $n  k$, the sum can start from $n=k$:\n$$\n\\mathbb{P}[K=k] = \\sum_{n=k}^{\\infty} \\left[ \\binom{n}{k} p^{k} (1-p)^{n-k} \\right] \\left[ \\frac{\\mu^{n} \\exp(-\\mu)}{n!} \\right]\n$$\nSubstitute the definition of the binomial coefficient, $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$:\n$$\n\\mathbb{P}[K=k] = \\sum_{n=k}^{\\infty} \\left[ \\frac{n!}{k!(n-k)!} p^{k} (1-p)^{n-k} \\right] \\left[ \\frac{\\mu^{n} \\exp(-\\mu)}{n!} \\right]\n$$\nWe can cancel the $n!$ terms and rearrange the expression by grouping terms that do not depend on the summation index $n$:\n$$\n\\mathbb{P}[K=k] = \\frac{p^{k} \\exp(-\\mu)}{k!} \\sum_{n=k}^{\\infty} \\frac{1}{(n-k)!} (1-p)^{n-k} \\mu^{n}\n$$\nTo simplify the sum, we rewrite $\\mu^{n}$ as $\\mu^{k}\\mu^{n-k}$ and factor out $\\mu^{k}$:\n$$\n\\mathbb{P}[K=k] = \\frac{(p\\mu)^{k} \\exp(-\\mu)}{k!} \\sum_{n=k}^{\\infty} \\frac{(\\mu(1-p))^{n-k}}{(n-k)!}\n$$\nLet us introduce a new summation index $j = n-k$. As $n$ goes from $k$ to $\\infty$, $j$ goes from $0$ to $\\infty$. The sum becomes:\n$$\n\\sum_{j=0}^{\\infty} \\frac{(\\mu(1-p))^{j}}{j!}\n$$\nThis is the Taylor series expansion for the exponential function, $\\exp(x) = \\sum_{j=0}^{\\infty} \\frac{x^{j}}{j!}$, with $x = \\mu(1-p)$. Therefore, the sum is equal to $\\exp(\\mu(1-p))$.\n\nSubstituting this result back into the expression for $\\mathbb{P}[K=k]$:\n$$\n\\mathbb{P}[K=k] = \\frac{(p\\mu)^{k} \\exp(-\\mu)}{k!} \\exp(\\mu(1-p))\n$$\nSimplifying the exponential terms, $\\exp(-\\mu) \\exp(\\mu(1-p)) = \\exp(-\\mu + \\mu - p\\mu) = \\exp(-p\\mu)$.\nThe final PMF for $K$ is therefore:\n$$\n\\mathbb{P}[K=k] = \\frac{(p\\mu)^{k} \\exp(-p\\mu)}{k!} \\quad \\text{for } k \\in \\{0, 1, 2, \\dots\\}\n$$\nThis is the PMF of a Poisson distribution with mean $p\\mu$.\n\nSecond, we compute the ratio $r \\equiv \\mathrm{CV}^{2}(K)/\\mathrm{CV}^{2}(N)$. The squared coefficient of variation for a random variable $X$ is defined as $\\mathrm{CV}^{2}(X) \\equiv \\mathrm{Var}(X)/(\\mathbb{E}[X])^{2}$.\n\nFor the mother cell's mRNA count $N$, which follows a Poisson distribution with parameter $\\mu$:\n$$\n\\mathbb{E}[N] = \\mu \\quad \\text{and} \\quad \\mathrm{Var}(N) = \\mu\n$$\nThus, the squared coefficient of variation for $N$ is:\n$$\n\\mathrm{CV}^{2}(N) = \\frac{\\mathrm{Var}(N)}{(\\mathbb{E}[N])^{2}} = \\frac{\\mu}{\\mu^{2}} = \\frac{1}{\\mu}\n$$\n\nFor the daughter cell's mRNA count $K$, which we have shown follows a Poisson distribution with parameter $p\\mu$:\n$$\n\\mathbb{E}[K] = p\\mu \\quad \\text{and} \\quad \\mathrm{Var}(K) = p\\mu\n$$\nThus, the squared coefficient of variation for $K$ is:\n$$\n\\mathrm{CV}^{2}(K) = \\frac{\\mathrm{Var}(K)}{(\\mathbb{E}[K])^{2}} = \\frac{p\\mu}{(p\\mu)^{2}} = \\frac{1}{p\\mu}\n$$\n\nFinally, we compute the ratio $r$:\n$$\nr = \\frac{\\mathrm{CV}^{2}(K)}{\\mathrm{CV}^{2}(N)} = \\frac{1/(p\\mu)}{1/\\mu} = \\frac{1}{p\\mu} \\cdot \\mu = \\frac{1}{p}\n$$\nThe result indicates that the noise, as measured by the squared coefficient of variation, is amplified by a factor of $1/p$ immediately after partitioning. Since $p \\in (0,1)$, this factor is always greater than $1$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{(p\\mu)^{k} \\exp(-p\\mu)}{k!}  \\frac{1}{p} \\end{pmatrix}}\n$$", "id": "2676065"}]}