{"hands_on_practices": [{"introduction": "A cornerstone of the histone code hypothesis is that post-translational modifications physically alter the properties of chromatin. This first practice focuses on one of the most direct consequences: the change in electrostatic charge. By calculating the expected net charge of histone tails following lysine acetylation, you will quantitatively connect a specific biochemical reaction to a key biophysical parameter that governs chromatin compaction and DNA accessibility [@problem_id:2821692].", "problem": "The histone code concept emphasizes that patterns of post-translational modifications (PTMs) on histone tails, such as lysine acetylation, modulate chromatin function in part by altering electrostatic interactions. Consider a nucleosome whose histone tails collectively contain $N = 44$ lysine residues that are potential targets for acetylation. At physiological pH, each unmodified lysine contributes approximately $+1$ to the tail charge due to its protonated $\\varepsilon$-amino group. Acetylation neutralizes this positive charge, contributing $0$ at that site. You are told that, prior to any acetylation, the measured net charge of the tails is $Q_{0} = +36$ (this is the algebraic sum over all tail residues and does not necessarily equal $N$). In a steady-state cell population, suppose a fraction $f$ of tail lysines are acetylated, with acetylation occurring uniformly and independently across the $N$ sites.\n\nStarting only from (i) the definition that the net charge is the algebraic sum over site charges and (ii) that acetylation at a lysine site changes its contribution by $-1$ relative to the unacetylated state, derive an expression for the expected net tail charge $\\mathbb{E}[Q]$ as a function of $f$, $N$, and $Q_{0}$. Then evaluate this expression at $f = 0.30$, $N = 44$, and $Q_{0} = +36$.\n\nExpress your final answer as a single real number with no units, and do not round.", "solution": "The problem statement is first subjected to rigorous validation.\n\nStep 1: Extracted Givens.\n- The number of potential lysine acetylation sites on histone tails is $N = 44$.\n- The charge contribution of an unmodified lysine is $+1$.\n- The charge contribution of an acetylated lysine is $0$.\n- The initial net charge of the tails (before acetylation) is $Q_{0} = +36$.\n- A fraction $f$ of the $N$ lysine sites are acetylated.\n- Acetylation occurs uniformly and independently across the $N$ sites.\n- The derivation must start from two principles: (i) net charge is the algebraic sum of site charges, and (ii) acetylation at a lysine site changes its charge contribution by $-1$.\n- The task is to derive an expression for the expected net tail charge $\\mathbb{E}[Q]$ as a function of $f$, $N$, and $Q_{0}$, and then evaluate it for $f = 0.30$, $N = 44$, and $Q_{0} = +36$.\n\nStep 2: Validation Using Extracted Givens.\nThe problem is assessed against the criteria for validity.\n- **Scientifically Grounded**: The problem is based on a central tenet of the histone code hypothesis in molecular biologyâ€”that post-translational modifications like acetylation alter the electrostatic properties of histone tails. The charge values assigned to unmodified ($+1$) and acetylated ($0$) lysine are biochemically correct. The fact that $Q_{0} \\neq N$ is also realistic, as it correctly implies that residues other than the $N$ specified lysines contribute to the overall tail charge. The problem is scientifically sound.\n- **Well-Posed**: The problem is well-posed. It specifies a probabilistic model (uniform and independent acetylation) and asks for the expected value of a system property (net charge). This is a standard problem in statistical mechanics and probability theory, which guarantees a unique and meaningful solution.\n- **Objective**: The problem is stated in precise, quantitative, and unbiased language. All terms are clearly defined.\n\nThe problem exhibits none of the flaws listed in the validation checklist. It is not scientifically unsound, non-formalizable, incomplete, contradictory, unrealistic, or ill-posed. It is a valid scientific problem.\n\nStep 3: Verdict and Action.\nThe problem is valid. A solution will be derived.\n\nThe net charge of the histone tails, $Q$, can be expressed as the sum of the initial charge $Q_{0}$ and the total change in charge, $\\Delta Q$, resulting from acetylation.\n$$Q = Q_{0} + \\Delta Q$$\nAccording to the problem's explicit instruction, the change in charge contribution from a single lysine site upon acetylation is from $+1$ to $0$, which is a net change of $-1$. Let $K$ be the random variable representing the total number of lysine residues that are acetylated out of the $N$ available sites. Since each acetylation event reduces the total charge by $1$, the total change in charge is:\n$$\\Delta Q = K \\times (-1) = -K$$\nSubstituting this into the expression for $Q$ gives:\n$$Q = Q_{0} - K$$\nWe are asked to find the expected net tail charge, $\\mathbb{E}[Q]$. By the linearity of the expectation operator:\n$$\\mathbb{E}[Q] = \\mathbb{E}[Q_{0} - K] = \\mathbb{E}[Q_{0}] - \\mathbb{E}[K]$$\nSince $Q_{0}$ is a given constant value, its expected value is the value itself: $\\mathbb{E}[Q_{0}] = Q_{0}$.\n$$\\mathbb{E}[Q] = Q_{0} - \\mathbb{E}[K]$$\nThe next step is to determine $\\mathbb{E}[K]$, the expected number of acetylated lysines. The problem states that acetylation occurs uniformly and independently at each of the $N$ sites, with a fraction $f$ of them being acetylated in a steady-state population. This is equivalent to stating that the probability of any given lysine site being acetylated is $p = f$.\n\nLet us define $N$ independent Bernoulli random variables, $X_{i}$ for $i = 1, 2, \\dots, N$, where $X_{i} = 1$ if the $i$-th lysine is acetylated and $X_{i} = 0$ if it is not. The probability of success (acetylation) is $P(X_{i} = 1) = f$.\nThe total number of acetylated lysines is the sum of these random variables:\n$$K = \\sum_{i=1}^{N} X_{i}$$\nThe expected value of $K$ is, by linearity of expectation:\n$$\\mathbb{E}[K] = \\mathbb{E}\\left[\\sum_{i=1}^{N} X_{i}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[X_{i}]$$\nThe expected value of a single Bernoulli variable $X_{i}$ is:\n$$\\mathbb{E}[X_{i}] = 1 \\cdot P(X_{i} = 1) + 0 \\cdot P(X_{i} = 0) = 1 \\cdot f + 0 \\cdot (1-f) = f$$\nTherefore, the expected total number of acetylated lysines is:\n$$\\mathbb{E}[K] = \\sum_{i=1}^{N} f = N \\cdot f$$\nSubstituting this result back into the expression for $\\mathbb{E}[Q]$ yields the desired general formula:\n$$\\mathbb{E}[Q] = Q_{0} - Nf$$\nThis expression gives the expected net charge as a function of the initial charge $Q_{0}$, the number of sites $N$, and the fraction acetylated $f$.\n\nFinally, we must evaluate this expression for the given values: $Q_{0} = 36$, $N = 44$, and $f = 0.30$.\n$$\\mathbb{E}[Q] = 36 - (44)(0.30)$$\nThe calculation is straightforward:\n$$(44)(0.30) = 13.2$$\n$$\\mathbb{E}[Q] = 36 - 13.2 = 22.8$$\nThe expected net charge of the histone tails under the specified conditions is $22.8$.", "answer": "$$\\boxed{22.8}$$", "id": "2821692"}, {"introduction": "Building upon the concept of charge alteration, this next exercise explores its functional impact on chromatin dynamics. Using principles from statistical mechanics, you will model how modifications that change histone tail charge can alter the thermodynamic equilibrium between wrapped and unwrapped DNA states. This practice demonstrates how the histone code can be translated into a quantitative language of free energy and probabilities, providing a powerful framework for understanding gene accessibility [@problem_id:2642837].", "problem": "Chromatin accessibility in development often depends on the thermodynamics of local nucleosomal DNA unwrapping. Consider a two-state model in which the DNA segment of interest is either wrapped ($W$) or unwrapped ($U$). Let the baseline standard free energy difference for unwrapping at a reference histone tail charge be $\\Delta G_{b} \\equiv G_{U} - G_{W}$, where $G_{U}$ and $G_{W}$ are the standard Gibbs free energies of the unwrapped and wrapped states, respectively. A histone code modification changes the effective positive charge on the proximal histone tail segment by an amount $\\Delta q$ (in units of the elementary charge), with negative $\\Delta q$ corresponding to a loss of positive charge (for example, lysine acetylation). Assume that, to first order in $\\Delta q$, the change in the unwrapping free energy due to electrostatics is linear, so that the modified free energy difference is $\\Delta G(\\Delta q) = \\Delta G_{b} + \\alpha \\,\\Delta q$, where $\\alpha$ is a positive constant with units of energy per unit charge that encapsulates the effective electrostatic coupling between the histone tail and DNA in the unwrapping reaction coordinate. The system is at absolute temperature $T$, and the molar gas constant is $R$.\n\nStarting from equilibrium statistical mechanics for a two-state system in the canonical ensemble and without introducing any additional phenomenological formulas, derive an exact expression for the fold-change in the unwrapped-state probability,\n$$F(\\Delta q) \\equiv \\frac{P_{U}(\\Delta q)}{P_{U}(0)},$$\nin terms of $\\Delta G_{b}$, $\\alpha$, $\\Delta q$, $R$, and $T$. Then, under the biologically relevant rare-unwrapping regime where the unwrapped state is much less populated than the wrapped state at baseline (so that $P_{U}(0) \\ll 1$), simplify $F(\\Delta q)$ to a closed-form expression that depends only on $\\alpha$, $\\Delta q$, $R$, and $T$. Provide as your final answer this simplified analytic expression for $F(\\Delta q)$. The final expression is dimensionless. Do not include any units in your final answer.", "solution": "The problem requires the derivation of the fold-change in the unwrapped-state probability for a two-state model of nucleosomal DNA, first in an exact form and then under a simplifying approximation. We shall begin from the fundamental principles of statistical mechanics for a system in the canonical ensemble.\n\nThe system can exist in two states: wrapped ($W$) and unwrapped ($U$), with standard Gibbs free energies $G_{W}$ and $G_{U}$, respectively. The Gibbs free energy difference for the unwrapping transition, $\\Delta G$, is a function of the change in histone tail charge, $\\Delta q$. The provided functional form is $\\Delta G(\\Delta q) = G_{U}(\\Delta q) - G_{W}(\\Delta q) = \\Delta G_{b} + \\alpha \\,\\Delta q$, where $\\Delta G_{b}$ is the baseline free energy difference at $\\Delta q = 0$.\n\nAccording to the Boltzmann distribution for a system at thermal equilibrium at absolute temperature $T$, the probability of occupying a state $i$ with free energy $G_{i}$ is given by:\n$$ P_{i} = \\frac{\\exp\\left(-\\frac{G_{i}}{RT}\\right)}{Z} $$\nwhere $Z$ is the partition function and $R$ is the molar gas constant. For this two-state system, the partition function is the sum of the Boltzmann factors for all accessible states:\n$$ Z = \\exp\\left(-\\frac{G_{W}}{RT}\\right) + \\exp\\left(-\\frac{G_{U}}{RT}\\right) $$\nThe probability of the system being in the unwrapped state, $P_{U}$, is therefore:\n$$ P_{U} = \\frac{\\exp\\left(-\\frac{G_{U}}{RT}\\right)}{\\exp\\left(-\\frac{G_{W}}{RT}\\right) + \\exp\\left(-\\frac{G_{U}}{RT}\\right)} $$\nTo express this in terms of the free energy difference $\\Delta G = G_{U} - G_{W}$, we divide both the numerator and the denominator by $\\exp\\left(-\\frac{G_{W}}{RT}\\right)$:\n$$ P_{U} = \\frac{\\exp\\left(-\\frac{G_{U}-G_{W}}{RT}\\right)}{1 + \\exp\\left(-\\frac{G_{U}-G_{W}}{RT}\\right)} = \\frac{\\exp\\left(-\\frac{\\Delta G}{RT}\\right)}{1 + \\exp\\left(-\\frac{\\Delta G}{RT}\\right)} $$\nThis is the general expression for the probability of the higher-energy state in any two-state system. Now, we apply this to the specific cases defined in the problem.\n\nThe probability of the unwrapped state as a function of the charge modification $\\Delta q$ is found by substituting $\\Delta G(\\Delta q) = \\Delta G_{b} + \\alpha \\,\\Delta q$:\n$$ P_{U}(\\Delta q) = \\frac{\\exp\\left(-\\frac{\\Delta G_{b} + \\alpha \\,\\Delta q}{RT}\\right)}{1 + \\exp\\left(-\\frac{\\Delta G_{b} + \\alpha \\,\\Delta q}{RT}\\right)} $$\nThe baseline probability, $P_{U}(0)$, corresponds to the case where $\\Delta q = 0$, so $\\Delta G(0) = \\Delta G_{b}$:\n$$ P_{U}(0) = \\frac{\\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)}{1 + \\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)} $$\nThe problem asks for the fold-change, $F(\\Delta q)$, defined as the ratio $\\frac{P_{U}(\\Delta q)}{P_{U}(0)}$. Constructing this ratio gives the exact expression:\n$$ F(\\Delta q) = \\frac{P_{U}(\\Delta q)}{P_{U}(0)} = \\frac{\\frac{\\exp\\left(-\\frac{\\Delta G_{b} + \\alpha \\,\\Delta q}{RT}\\right)}{1 + \\exp\\left(-\\frac{\\Delta G_{b} + \\alpha \\,\\Delta q}{RT}\\right)}}{\\frac{\\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)}{1 + \\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)}} $$\nWe can simplify this expression by separating the terms:\n$$ F(\\Delta q) = \\frac{\\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)\\exp\\left(-\\frac{\\alpha \\,\\Delta q}{RT}\\right)}{\\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)} \\times \\frac{1 + \\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)}{1 + \\exp\\left(-\\frac{\\Delta G_{b} + \\alpha \\,\\Delta q}{RT}\\right)} $$\n$$ F(\\Delta q) = \\exp\\left(-\\frac{\\alpha \\,\\Delta q}{RT}\\right) \\left( \\frac{1 + \\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)}{1 + \\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)\\exp\\left(-\\frac{\\alpha \\,\\Delta q}{RT}\\right)} \\right) $$\nThis is the exact expression for the fold-change.\n\nNext, we must simplify this expression under the \"rare-unwrapping regime\" condition, specified as $P_{U}(0) \\ll 1$. Let us analyze this condition:\n$$ P_{U}(0) = \\frac{\\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)}{1 + \\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)} \\ll 1 $$\nFor this fraction to be much smaller than $1$, the numerator must be much smaller than the denominator. This implies that $\\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right) \\ll 1$. This is physically equivalent to the statement that the free energy cost of unwrapping, $\\Delta G_{b}$, is much larger than the available thermal energy, $RT$, making the unwrapped state energetically very unfavorable and thus rare.\n\nWe now apply this approximation, $\\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right) \\ll 1$, to the exact expression for $F(\\Delta q)$.\nConsider the term in parentheses:\n$$ \\frac{1 + \\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)}{1 + \\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)\\exp\\left(-\\frac{\\alpha \\,\\Delta q}{RT}\\right)} $$\nIn the numerator, since $\\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right) \\ll 1$, we have $1 + \\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right) \\approx 1$.\nIn the denominator, the term $\\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)\\exp\\left(-\\frac{\\alpha \\,\\Delta q}{RT}\\right)$ is a product of a very small number, $\\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)$, and another number, $\\exp\\left(-\\frac{\\alpha \\,\\Delta q}{RT}\\right)$. Unless the second term is extraordinarily large (which would correspond to a histone modification that almost entirely eliminates the unwrapping energy barrier), the product will also be much smaller than $1$. Therefore, it is a valid approximation to treat this product as negligible compared to $1$. The denominator becomes $1 + \\exp\\left(-\\frac{\\Delta G_{b}}{RT}\\right)\\exp\\left(-\\frac{\\alpha \\,\\Delta q}{RT}\\right) \\approx 1$.\n\nWith both numerator and denominator of the fractional term approximating to $1$, the entire term in parentheses simplifies to $1$.\nSubstituting this simplification back into the expression for $F(\\Delta q)$:\n$$ F(\\Delta q) \\approx \\exp\\left(-\\frac{\\alpha \\,\\Delta q}{RT}\\right) \\times 1 = \\exp\\left(-\\frac{\\alpha \\,\\Delta q}{RT}\\right) $$\nThis is the simplified closed-form expression for the fold-change in the rare-unwrapping regime. It depends only on $\\alpha$, $\\Delta q$, $R$, and $T$, as required. The physical interpretation is that when the baseline unwrapped state is rare, its probability is directly proportional to its Boltzmann factor, $P_U(0) \\propto \\exp(-\\Delta G_b/RT)$. The fold-change then becomes simply the ratio of the Boltzmann factors, which is determined by the change in energy, $\\alpha \\Delta q$.", "answer": "$$ \\boxed{\\exp\\left(-\\frac{\\alpha \\Delta q}{RT}\\right)} $$", "id": "2642837"}, {"introduction": "This final practice brings the concepts of the histone code into the realm of modern data science. You will implement a Bayesian classifier to annotate genomic regions based on their histone modification signatures, a common task in computational biology. This hands-on coding exercise illustrates how characteristic combinations of marks at functional elements, as measured by techniques like ChIP-seq, can be used to systematically map the chromatin landscape on a genome-wide scale [@problem_id:2821688].", "problem": "You are given normalized signal intensities from Chromatin Immunoprecipitation sequencing (ChIP-seq) for three histone modifications: H3K4me3, H3K27ac, and H3K27me3, measured across genomic regions. Each region is represented as a real-valued vector in $\\mathbb{R}^3$ with components corresponding to H3K4me3, H3K27ac, and H3K27me3, respectively. The biological premise (commonly observed and well-tested) is that promoter regions are characterized by high H3K4me3 and moderate-to-high H3K27ac with low H3K27me3, enhancer regions by low H3K4me3 and high H3K27ac with low H3K27me3, and polycomb-repressed regions by high H3K27me3 with low H3K4me3 and H3K27ac. Your task is to formalize classification of regions into promoter, enhancer, or polycomb-repressed states using a principled Bayesian approach grounded in Bayesâ€™ theorem.\n\nStarting from Bayesâ€™ theorem and the independence assumption of features given class (that is, conditional independence of each histone mark given the chromatin state), implement a classifier that treats each histone mark as a continuous random variable with a class-conditional Gaussian distribution. Use the following fundamental base:\n- Bayesâ€™ theorem: $P(C \\mid \\mathbf{x}) \\propto P(C)\\,P(\\mathbf{x}\\mid C)$.\n- Naive conditional independence: $P(\\mathbf{x}\\mid C) = \\prod_{j=1}^{3} P(x_j \\mid C)$.\n- Gaussian likelihood per feature: $P(x_j \\mid C) = \\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j})$ where $\\mu_{C,j}$ and $\\sigma^2_{C,j}$ are the class-conditional mean and variance of feature $j$ for class $C$.\n- Maximum likelihood estimation for each class $C$ with $n_C$ training examples $\\{\\mathbf{x}^{(i)}\\}_{i=1}^{n_C}$: $\\mu_{C,j} = \\frac{1}{n_C}\\sum_{i=1}^{n_C} x^{(i)}_j$ and $\\sigma^2_{C,j} = \\frac{1}{n_C}\\sum_{i=1}^{n_C} \\left(x^{(i)}_j - \\mu_{C,j}\\right)^2$.\n- Empirical class prior: $P(C) = \\frac{n_C}{N}$ where $N$ is the total number of training examples across all classes.\n- Numerical stability requirements: compute in the log domain using $\\log P(\\mathbf{x}\\mid C) = \\sum_{j=1}^{3} \\left[-\\frac{1}{2}\\log\\left(2\\pi\\sigma^2_{C,j}\\right) - \\frac{(x_j - \\mu_{C,j})^2}{2\\sigma^2_{C,j}}\\right]$, and regularize variances as $\\sigma^2_{C,j} \\leftarrow \\sigma^2_{C,j} + \\epsilon$ with $\\epsilon = 10^{-6}$ to avoid zero variance. In the event of exact ties in posterior, select the class with the smallest index.\n\nClasses are encoded as integers: promoter $\\rightarrow 0$, enhancer $\\rightarrow 1$, polycomb-repressed $\\rightarrow 2$.\n\nImplement this Gaussian Naive Bayes (GNB) classifier and assess classification accuracy, defined as $\\text{accuracy} = \\frac{\\text{number of correctly classified test samples}}{\\text{total number of test samples}}$, expressed as a decimal. Your program must compute accuracies for the following three test cases, each with a specified training and test set. Do not introduce any randomness.\n\nTest Case $1$ (well-separated class-conditional structure):\n- Training set $\\mathbf{X}_{\\text{train}}$ (rows are samples, columns are [H3K4me3, H3K27ac, H3K27me3]):\n  - Promoter ($y=0$): $[10.0, 4.0, 0.5]$, $[9.5, 3.8, 0.4]$, $[10.2, 4.1, 0.6]$, $[9.8, 4.2, 0.5]$, $[10.1, 3.9, 0.5]$\n  - Enhancer ($y=1$): $[1.0, 8.5, 0.5]$, $[0.9, 8.8, 0.6]$, $[1.2, 8.9, 0.4]$, $[1.1, 8.6, 0.5]$, $[0.8, 8.7, 0.5]$\n  - Polycomb-repressed ($y=2$): $[0.5, 0.6, 9.5]$, $[0.4, 0.5, 9.8]$, $[0.6, 0.5, 9.7]$, $[0.5, 0.4, 9.6]$, $[0.6, 0.6, 9.9]$\n- Test set $\\mathbf{X}_{\\text{test}}$ and labels $\\mathbf{y}_{\\text{test}}$:\n  - $[9.9, 4.0, 0.5]\\rightarrow 0$, $[10.3, 4.1, 0.7]\\rightarrow 0$, $[1.0, 8.7, 0.5]\\rightarrow 1$, $[1.3, 8.4, 0.6]\\rightarrow 1$, $[0.5, 0.5, 9.6]\\rightarrow 2$, $[0.7, 0.7, 9.8]\\rightarrow 2$\n\nTest Case $2$ (overlapping promoter and enhancer in H3K27ac; separation depends on H3K4me3):\n- Training set:\n  - Promoter ($y=0$): $[5.5, 7.5, 1.0]$, $[5.8, 7.8, 1.2]$, $[5.2, 7.2, 0.8]$, $[5.6, 7.6, 1.1]$\n  - Enhancer ($y=1$): $[1.2, 7.7, 1.0]$, $[1.0, 7.5, 1.1]$, $[1.5, 7.9, 0.9]$, $[1.3, 7.6, 1.2]$\n  - Polycomb-repressed ($y=2$): $[1.0, 1.5, 7.8]$, $[0.8, 1.2, 8.0]$, $[1.2, 1.3, 7.6]$, $[1.1, 1.4, 8.2]$\n- Test set and labels:\n  - $[5.4, 7.4, 1.0]\\rightarrow 0$, $[1.4, 7.4, 1.1]\\rightarrow 1$, $[1.0, 1.3, 8.1]\\rightarrow 2$, $[3.2, 7.5, 1.0]\\rightarrow 1$\n\nTest Case $3$ (degenerate variances requiring regularization):\n- Training set:\n  - Promoter ($y=0$): $[2.0, 4.0, 0.5]$, $[2.0, 4.0, 0.5]$, $[2.0, 4.0, 0.5]$\n  - Enhancer ($y=1$): $[1.8, 4.0, 0.4]$, $[1.8, 4.0, 0.4]$, $[1.8, 4.0, 0.4]$\n  - Polycomb-repressed ($y=2$): $[0.2, 0.2, 6.0]$, $[0.2, 0.2, 6.0]$, $[0.2, 0.2, 6.0]$\n- Test set and labels:\n  - $[1.9, 4.0, 0.45]\\rightarrow 0$, $[1.8, 4.0, 0.4]\\rightarrow 1$, $[0.2, 0.2, 6.0]\\rightarrow 2$\n\nYour program must:\n- Implement training to estimate $\\mu_{C,j}$, $\\sigma^2_{C,j}$, and $P(C)$.\n- Implement prediction by maximizing $\\log P(C) + \\sum_{j=1}^{3} \\log \\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j} + \\epsilon)$ with $\\epsilon = 10^{-6}$.\n- Compute accuracy for each test case as a decimal fraction.\n\nFinal Output Format:\n- Produce a single line of output containing the accuracies for Test Case $1$, Test Case $2$, and Test Case $3$ in this order, as a comma-separated list enclosed in square brackets, with each value rounded to exactly $4$ decimal places, for example the string consisting of a left bracket, the three rounded values $a$, $b$, $c$ separated by commas, and a right bracket: $[a,b,c]$.", "solution": "The problem requires the formulation and implementation of a Gaussian Naive Bayes (GNB) classifier to categorize genomic regions into one of three chromatin states: promoter (class $0$), enhancer (class $1$), or polycomb-repressed (class $2$). The classification is based on a feature vector $\\mathbf{x} \\in \\mathbb{R}^3$ representing the signal intensities of three histone modifications: H3K4me3 ($x_1$), H3K27ac ($x_2$), and H3K27me3 ($x_3$).\n\nThe problem is scientifically well-grounded, mathematically well-posed, and provides all necessary information for a unique solution. The biological premise is sound, and the specified statistical model is a standard approach for such classification tasks. The provisions for numerical stability are critical for a robust implementation. Therefore, the problem is valid, and we shall proceed with the derivation and implementation of the solution.\n\nThe core of the solution is Bayes' theorem, which provides a rule for updating our belief about the class $C$ given the observed data $\\mathbf{x}$:\n$$\nP(C \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid C) P(C)}{P(\\mathbf{x})}\n$$\nFor classification, we seek the class $\\hat{C}$ that maximizes the posterior probability $P(C \\mid \\mathbf{x})$. Since $P(\\mathbf{x})$ is constant for all classes for a given data point $\\mathbf{x}$, this is equivalent to maximizing the product of the class-conditional likelihood $P(\\mathbf{x} \\mid C)$ and the class prior $P(C)$:\n$$\n\\hat{C} = \\arg\\max_{C} P(\\mathbf{x} \\mid C) P(C)\n$$\nThe \"naive\" assumption of the Naive Bayes classifier is that the features $x_j$ are conditionally independent given the class $C$. This simplifies the likelihood term:\n$$\nP(\\mathbf{x} \\mid C) = \\prod_{j=1}^{3} P(x_j \\mid C)\n$$\nEach feature's class-conditional distribution $P(x_j \\mid C)$ is modeled as a Gaussian (Normal) distribution, $\\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j})$, where $\\mu_{C,j}$ and $\\sigma^2_{C,j}$ are the mean and variance of feature $j$ for class $C$.\n\nTo avoid numerical underflow with small probabilities and for computational convenience, we work with the logarithm of the posterior. The decision rule becomes:\n$$\n\\hat{C} = \\arg\\max_{C} \\left[ \\log P(C) + \\sum_{j=1}^{3} \\log P(x_j \\mid C) \\right]\n$$\nThis expression is often called the discriminant function, $g_C(\\mathbf{x})$.\n\nThe training phase consists of estimating the model parameters from a labeled training set.\nLet the training set for class $C$ be $\\{\\mathbf{x}^{(i)}\\}_{i=1}^{n_C}$, where $n_C$ is the number of samples in class $C$. Let $N$ be the total number of training samples.\nThe parameters are estimated using Maximum Likelihood Estimation (MLE):\n\n1.  **Class Priors**, $P(C)$: The empirical frequency of each class in the training data.\n    $$\n    P(C) = \\frac{n_C}{N}\n    $$\n\n2.  **Class-Conditional Means**, $\\mu_{C,j}$: The sample mean of feature $j$ for all training samples belonging to class $C$.\n    $$\n    \\mu_{C,j} = \\frac{1}{n_C} \\sum_{i=1}^{n_C} x^{(i)}_j\n    $$\n\n3.  **Class-Conditional Variances**, $\\sigma^2_{C,j}$: The sample variance of feature $j$ for all training samples belonging to class $C$. This is the biased estimator for variance, consistent with the MLE formulation.\n    $$\n    \\sigma^2_{C,j} = \\frac{1}{n_C} \\sum_{i=1}^{n_C} (x^{(i)}_j - \\mu_{C,j})^2\n    $$\n\nFor prediction, we use these estimated parameters to compute the discriminant function $g_C(\\mathbf{x})$ for a new data point $\\mathbf{x}$. The formula for the logarithm of the Gaussian probability density function is:\n$$\n\\log P(x_j \\mid C) = \\log \\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j}) = -\\frac{1}{2} \\log(2\\pi\\sigma^2_{C,j}) - \\frac{(x_j - \\mu_{C,j})^2}{2\\sigma^2_{C,j}}\n$$\nA critical implementation detail is the handling of zero variances, which can occur if all training samples in a class have the same value for a feature. This would lead to division by zero or the logarithm of zero. To prevent this, a small regularization constant $\\epsilon = 10^{-6}$ is added to each estimated variance:\n$$\n\\sigma^2_{C,j, \\text{reg}} = \\sigma^2_{C,j} + \\epsilon\n$$\nThe final discriminant function to be maximized is:\n$$\ng_C(\\mathbf{x}) = \\log P(C) + \\sum_{j=1}^{3} \\left[ -\\frac{1}{2} \\log(2\\pi \\sigma^2_{C,j, \\text{reg}}) - \\frac{(x_j - \\mu_{C,j})^2}{2 \\sigma^2_{C,j, \\text{reg}}} \\right]\n$$\nThe predicted class $\\hat{C}$ for a given $\\mathbf{x}$ is the class that yields the highest value for $g_C(\\mathbf{x})$. In case of a tie, the class with the smallest integer index is chosen.\n\nFinally, the performance of the classifier is evaluated using accuracy, defined as the fraction of correctly classified samples in the test set:\n$$\n\\text{accuracy} = \\frac{\\sum_{i=1}^{m} I(\\hat{y}_i = y_i)}{m}\n$$\nwhere $m$ is the number of test samples, $y_i$ is the true label, $\\hat{y}_i$ is the predicted label for the $i$-th test sample, and $I(\\cdot)$ is the indicator function.\n\nThe implementation will consist of a class that encapsulates this logic. A `fit` method will estimate the parameters from training data, and a `predict` method will use these parameters to classify new data points. We will process each test case by training the model on its corresponding training set and evaluating accuracy on its test set.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Gaussian Naive Bayes classifier and evaluates its accuracy\n    on three provided test cases related to chromatin state classification.\n    \"\"\"\n\n    class GaussianNaiveBayes:\n        \"\"\"\n        A Gaussian Naive Bayes classifier.\n\n        Parameters are estimated using Maximum Likelihood Estimation.\n        \"\"\"\n        def __init__(self, var_smoothing=1e-6):\n            self.var_smoothing = var_smoothing\n            self.class_priors_ = None\n            self.means_ = None\n            self.variances_ = None\n            self.classes_ = None\n\n        def fit(self, X, y):\n            \"\"\"\n            Train the classifier by estimating parameters from the data.\n\n            Args:\n                X (np.ndarray): Training data of shape (n_samples, n_features).\n                y (np.ndarray): Target values of shape (n_samples,).\n            \"\"\"\n            self.classes_ = np.unique(y)\n            n_samples, n_features = X.shape\n            n_classes = len(self.classes_)\n\n            self.means_ = np.zeros((n_classes, n_features))\n            self.variances_ = np.zeros((n_classes, n_features))\n            self.class_priors_ = np.zeros(n_classes)\n\n            for idx, c in enumerate(self.classes_):\n                X_c = X[y == c]\n                self.means_[idx, :] = X_c.mean(axis=0)\n                self.variances_[idx, :] = X_c.var(axis=0)\n                self.class_priors_[idx] = X_c.shape[0] / float(n_samples)\n\n        def predict(self, X):\n            \"\"\"\n            Perform classification on an array of test vectors X.\n\n            Args:\n                X (np.ndarray): Test data of shape (n_samples, n_features).\n\n            Returns:\n                np.ndarray: Predicted class labels for each sample in X.\n            \"\"\"\n            # Add variance smoothing for numerical stability\n            vars_reg = self.variances_ + self.var_smoothing\n\n            log_priors = np.log(self.class_priors_)\n\n            # The log posterior is proportional to log_prior + log_likelihood\n            # log_likelihood for a Gaussian is sum over features of:\n            # -0.5 * log(2*pi*var) - 0.5 * ((x-mu)^2 / var)\n            \n            log_posteriors = []\n            for x_sample in X:\n                joint_log_likelihood = []\n                for i in range(len(self.classes_)):\n                    log_likelihood_class = -0.5 * np.sum(np.log(2. * np.pi * vars_reg[i, :]))\n                    log_likelihood_class -= 0.5 * np.sum(((x_sample - self.means_[i, :]) ** 2) / vars_reg[i, :])\n                    joint_log_likelihood.append(log_priors[i] + log_likelihood_class)\n                log_posteriors.append(joint_log_likelihood)\n\n            # The class with the highest log posterior is the prediction.\n            # np.argmax handles ties by returning the first index, which matches\n            # the problem's tie-breaking rule (smallest class index).\n            predictions = np.argmax(log_posteriors, axis=1)\n            return self.classes_[predictions]\n            \n    # Define test cases from the problem statement\n    test_cases = [\n        {\n            \"train_set\": {\n                0: np.array([[10.0, 4.0, 0.5], [9.5, 3.8, 0.4], [10.2, 4.1, 0.6], [9.8, 4.2, 0.5], [10.1, 3.9, 0.5]]),\n                1: np.array([[1.0, 8.5, 0.5], [0.9, 8.8, 0.6], [1.2, 8.9, 0.4], [1.1, 8.6, 0.5], [0.8, 8.7, 0.5]]),\n                2: np.array([[0.5, 0.6, 9.5], [0.4, 0.5, 9.8], [0.6, 0.5, 9.7], [0.5, 0.4, 9.6], [0.6, 0.6, 9.9]]),\n            },\n            \"test_set\": np.array([\n                [9.9, 4.0, 0.5], [10.3, 4.1, 0.7], \n                [1.0, 8.7, 0.5], [1.3, 8.4, 0.6], \n                [0.5, 0.5, 9.6], [0.7, 0.7, 9.8]\n            ]),\n            \"test_labels\": np.array([0, 0, 1, 1, 2, 2]),\n        },\n        {\n            \"train_set\": {\n                0: np.array([[5.5, 7.5, 1.0], [5.8, 7.8, 1.2], [5.2, 7.2, 0.8], [5.6, 7.6, 1.1]]),\n                1: np.array([[1.2, 7.7, 1.0], [1.0, 7.5, 1.1], [1.5, 7.9, 0.9], [1.3, 7.6, 1.2]]),\n                2: np.array([[1.0, 1.5, 7.8], [0.8, 1.2, 8.0], [1.2, 1.3, 7.6], [1.1, 1.4, 8.2]]),\n            },\n            \"test_set\": np.array([\n                [5.4, 7.4, 1.0], [1.4, 7.4, 1.1], \n                [1.0, 1.3, 8.1], [3.2, 7.5, 1.0]\n            ]),\n            \"test_labels\": np.array([0, 1, 2, 1]),\n        },\n        {\n            \"train_set\": {\n                0: np.array([[2.0, 4.0, 0.5], [2.0, 4.0, 0.5], [2.0, 4.0, 0.5]]),\n                1: np.array([[1.8, 4.0, 0.4], [1.8, 4.0, 0.4], [1.8, 4.0, 0.4]]),\n                2: np.array([[0.2, 0.2, 6.0], [0.2, 0.2, 6.0], [0.2, 0.2, 6.0]]),\n            },\n            \"test_set\": np.array([\n                [1.9, 4.0, 0.45], [1.8, 4.0, 0.4], [0.2, 0.2, 6.0]\n            ]),\n            \"test_labels\": np.array([0, 1, 2]),\n        },\n    ]\n\n    accuracies = []\n    \n    for case in test_cases:\n        # Prepare training data\n        X_train_list, y_train_list = [], []\n        for label, data in case[\"train_set\"].items():\n            X_train_list.append(data)\n            y_train_list.extend([label] * data.shape[0])\n        \n        X_train = np.vstack(X_train_list)\n        y_train = np.array(y_train_list)\n        \n        # Prepare test data\n        X_test = case[\"test_set\"]\n        y_test = case[\"test_labels\"]\n        \n        # Initialize and train the classifier\n        gnb = GaussianNaiveBayes(var_smoothing=1e-6)\n        gnb.fit(X_train, y_train)\n        \n        # Make predictions\n        y_pred = gnb.predict(X_test)\n        \n        # Calculate accuracy\n        accuracy = np.mean(y_pred == y_test)\n        accuracies.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{acc:.4f}' for acc in accuracies])}]\")\n\nsolve()\n```", "id": "2821688"}]}