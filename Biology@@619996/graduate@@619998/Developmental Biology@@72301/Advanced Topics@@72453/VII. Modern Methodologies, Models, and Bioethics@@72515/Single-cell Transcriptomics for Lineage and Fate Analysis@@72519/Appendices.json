{"hands_on_practices": [{"introduction": "Before any meaningful biological insights can be drawn from single-cell transcriptomics data, a rigorous quality control (QC) step is essential to filter out low-quality cells and technical artifacts. This foundational practice will guide you through implementing a standard QC pipeline, translating biological heuristics about cell health and data quality into precise computational metrics. Mastering this step [@problem_id:2672354] is crucial for ensuring the robustness and reliability of all subsequent lineage and fate analyses.", "problem": "You are given the task of implementing quality control for single-cell ribonucleic acid sequencing (scRNA-seq) counts at the single-cell level to support lineage and fate analysis. Base your reasoning on two foundational principles: (i) the Central Dogma of molecular biology, where messenger ribonucleic acid (mRNA) abundance reflects gene expression, and (ii) the design of unique molecular identifiers (UMIs) in scRNA-seq, which count captured transcript molecules and yield nonnegative integer counts per gene per cell. For each cell, you will be given a vector of nonnegative integer counts for a fixed set of genes and the indices of mitochondrial genes. Quality control relies on the following definitions grounded in well-tested practice: total UMI counts, number of detected genes, mitochondrial fraction, and zero fraction. These metrics are proxies for cell viability, complexity, and potential stress that would bias lineage and fate reconstruction.\n\nGiven a vector of counts $\\mathbf{x} = (x_1, x_2, \\dots, x_N)$ with $x_i \\in \\mathbb{Z}_{\\ge 0}$, a set of mitochondrial gene indices $\\mathcal{M} \\subseteq \\{1,2,\\dots,N\\}$, and thresholds $c_{\\min}$, $c_{\\max}$, $g_{\\min}$, $f_{\\mathrm{mt},\\max}$, $f_{0,\\max}$, and $k_{\\min}$, define:\n- Total UMI count $T = \\sum_{i=1}^{N} x_i$.\n- Number of detected genes $G = \\left|\\{ i \\in \\{1,\\dots,N\\} : x_i \\ge k_{\\min} \\}\\right|$.\n- Mitochondrial fraction $f_{\\mathrm{mt}} = \\begin{cases} \\dfrac{\\sum_{i \\in \\mathcal{M}} x_i}{T}, & \\text{if } T > 0 \\\\ 0, & \\text{if } T = 0 \\end{cases}$.\n- Zero fraction $f_0 = \\dfrac{\\left|\\{ i \\in \\{1,\\dots,N\\} : x_i = 0 \\}\\right|}{N}$.\n\nA cell passes quality control if and only if all of the following hold simultaneously:\n- $c_{\\min} \\le T \\le c_{\\max}$,\n- $G \\ge g_{\\min}$,\n- $f_{\\mathrm{mt}} \\le f_{\\mathrm{mt},\\max}$,\n- $f_0 \\le f_{0,\\max}$.\n\nAll fractional quantities must be expressed as decimals. For reporting, round $f_{\\mathrm{mt}}$ and $f_0$ to four decimal places.\n\nYour program must compute, for each test case, the tuple $[T, G, f_{\\mathrm{mt}}, f_0, P]$ where $P$ is a boolean indicating whether the cell passes quality control as defined above. Use inclusive inequalities exactly as stated. If $T = 0$, use $f_{\\mathrm{mt}} = 0$ by definition above, and evaluate pass/fail by the same rules.\n\nTest Suite:\nUse the following five test cases. Each case provides $(\\mathbf{x}, \\mathcal{M}, c_{\\min}, c_{\\max}, g_{\\min}, f_{\\mathrm{mt},\\max}, f_{0,\\max}, k_{\\min})$. The gene index convention for $\\mathcal{M}$ is $0$-based in the input vectors, but interpret it consistently as the positions within $\\mathbf{x}$.\n\n1. Case A (happy path):\n   - $\\mathbf{x} = [2, 3, 4, 5, 6, 0, 0, 1, 2, 3]$\n   - $\\mathcal{M} = \\{0, 1\\}$\n   - $c_{\\min} = 20$, $c_{\\max} = 1000$, $g_{\\min} = 5$, $f_{\\mathrm{mt},\\max} = 0.2$, $f_{0,\\max} = 0.6$, $k_{\\min} = 1$\n\n2. Case B (boundary inclusivity at $c_{\\min}$ and $f_{\\mathrm{mt},\\max}$):\n   - $\\mathbf{x} = [2, 2, 8, 4, 4, 0, 0, 0, 0, 0]$\n   - $\\mathcal{M} = \\{0, 1\\}$\n   - $c_{\\min} = 20$, $c_{\\max} = 1000$, $g_{\\min} = 5$, $f_{\\mathrm{mt},\\max} = 0.2$, $f_{0,\\max} = 0.6$, $k_{\\min} = 1$\n\n3. Case C (failure due to elevated mitochondrial fraction):\n   - $\\mathbf{x} = [10, 0, 4, 4, 4, 4, 0, 2, 1, 1]$\n   - $\\mathcal{M} = \\{0, 1\\}$\n   - $c_{\\min} = 20$, $c_{\\max} = 1000$, $g_{\\min} = 5$, $f_{\\mathrm{mt},\\max} = 0.2$, $f_{0,\\max} = 0.6$, $k_{\\min} = 1$\n\n4. Case D (edge case with no detected molecules):\n   - $\\mathbf{x} = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]$\n   - $\\mathcal{M} = \\{0, 1\\}$\n   - $c_{\\min} = 20$, $c_{\\max} = 1000$, $g_{\\min} = 5$, $f_{\\mathrm{mt},\\max} = 0.2$, $f_{0,\\max} = 0.6$, $k_{\\min} = 1$\n\n5. Case E (boundary inclusivity at $c_{\\max}$ and $f_{\\mathrm{mt},\\max}$):\n   - $\\mathbf{x} = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]$\n   - $\\mathcal{M} = \\{0, 1\\}$\n   - $c_{\\min} = 20$, $c_{\\max} = 1000$, $g_{\\min} = 5$, $f_{\\mathrm{mt},\\max} = 0.2$, $f_{0,\\max} = 0.6$, $k_{\\min} = 1$\n\nFinal Output Format:\nYour program should produce a single line of output containing a list of five elements, one per test case in the order A through E. Each element must be the list $[T, G, f_{\\mathrm{mt}}, f_0, P]$ with $f_{\\mathrm{mt}}$ and $f_0$ rounded to four decimal places as decimals, and $P$ as a boolean. The entire output must be printed as a single Python-style list on one line, for example:\n[[T_A,G_A,fmt_A,f0_A,P_A],[T_B,G_B,fmt_B,f0_B,P_B],...]", "solution": "The problem presented is a well-defined and computationally tractable task that is fundamental to the analysis of single-cell transcriptomics data. It is scientifically grounded in the established principles of molecular biology and bioinformatics quality control. The provided definitions are precise and the constraints are unambiguous. Therefore, the problem is valid and we proceed to its rigorous solution.\n\nThe objective is to implement an algorithm for quality control of individual cells based on their messenger ribonucleic acid (mRNA) expression profiles, which are quantified using unique molecular identifiers (UMIs). We are given, for each cell, a count vector $\\mathbf{x} = (x_1, x_2, \\dots, x_N)$ where $x_i \\in \\mathbb{Z}_{\\ge 0}$ is the UMI count for gene $i$. We must compute four quality metrics and use them to determine if a cell passes a set of predefined thresholds.\n\nThe metrics are defined as follows:\n1.  Total UMI count, $T$: This metric reflects the total number of mRNA molecules captured from a cell. It is a proxy for cell size and sequencing library quality. A very low $T$ may indicate a dead cell or failed capture, while a very high $T$ may indicate a multiplet (two or more cells captured as one). It is computed as the sum of all gene counts:\n    $$T = \\sum_{i=1}^{N} x_i$$\n2.  Number of detected genes, $G$: This metric represents the transcriptional complexity of a cell. A higher $G$ suggests a more transcriptionally active and complex cell. We define a gene as \"detected\" if its count $x_i$ is at least $k_{\\min}$. For this problem, $k_{\\min} = 1$. The metric is the cardinality of the set of detected genes:\n    $$G = \\left|\\{ i \\in \\{1,\\dots,N\\} : x_i \\ge k_{\\min} \\}\\right|$$\n3.  Mitochondrial fraction, $f_{\\mathrm{mt}}$: This is the proportion of total UMIs that map to mitochondrial genes. An elevated $f_{\\mathrm{mt}}$ is a common indicator of cellular stress or apoptosis, as stressed cells tend to have compromised cytoplasmic mRNA and a relative abundance of mitochondrial transcripts. Given a set of mitochondrial gene indices $\\mathcal{M}$, it is computed as:\n    $$f_{\\mathrm{mt}} = \\begin{cases} \\dfrac{\\sum_{i \\in \\mathcal{M}} x_i}{T}, & \\text{if } T > 0 \\\\ 0, & \\text{if } T = 0 \\end{cases}$$\n4.  Zero fraction, $f_0$: This metric is the proportion of genes with zero observed counts. While scRNA-seq data is inherently sparse, an extremely high zero fraction, especially in conjunction with low $T$ and $G$, might indicate a poor-quality cell. It is calculated as:\n    $$f_0 = \\dfrac{\\left|\\{ i \\in \\{1,\\dots,N\\} : x_i = 0 \\}\\right|}{N}$$\n    where $N$ is the total number of genes in the vector $\\mathbf{x}$.\n\nA cell is deemed to pass quality control, denoted by a boolean value $P=\\text{True}$, if and only if all four of the following conditions are simultaneously satisfied:\n-   $c_{\\min} \\le T \\le c_{\\max}$\n-   $G \\ge g_{\\min}$\n-   $f_{\\mathrm{mt}} \\le f_{\\mathrm{mt},\\max}$\n-   $f_0 \\le f_{0,\\max}$\n\nWe will now apply this procedure to each of the five specified test cases. For all cases, the total number of genes is $N=10$, and the threshold for gene detection is $k_{\\min}=1$. The indices in $\\mathcal{M}$ are $0$-based.\n\nCase A:\n-   $\\mathbf{x} = [2, 3, 4, 5, 6, 0, 0, 1, 2, 3]$\n-   $\\mathcal{M} = \\{0, 1\\}$\n-   Thresholds: $c_{\\min} = 20$, $c_{\\max} = 1000$, $g_{\\min} = 5$, $f_{\\mathrm{mt},\\max} = 0.2$, $f_{0,\\max} = 0.6$.\n-   $T = 2+3+4+5+6+0+0+1+2+3 = 26$.\n-   $G = |\\{i | x_i \\ge 1\\}| = 8$.\n-   $\\sum_{i \\in \\mathcal{M}} x_i = x_0 + x_1 = 2+3 = 5$.\n-   $f_{\\mathrm{mt}} = 5 / 26 \\approx 0.192307...$.\n-   $f_0 = 2 / 10 = 0.2$.\n-   Conditions check:\n    1. $20 \\le 26 \\le 1000 \\implies \\text{True}$.\n    2. $8 \\ge 5 \\implies \\text{True}$.\n    3. $0.1923... \\le 0.2 \\implies \\text{True}$.\n    4. $0.2 \\le 0.6 \\implies \\text{True}$.\n-   All conditions pass. $P = \\text{True}$.\n-   Result: $[26, 8, 0.1923, 0.2000, \\text{True}]$.\n\nCase B:\n-   $\\mathbf{x} = [2, 2, 8, 4, 4, 0, 0, 0, 0, 0]$\n-   $\\mathcal{M} = \\{0, 1\\}$\n-   Thresholds: $c_{\\min} = 20$, $c_{\\max} = 1000$, $g_{\\min} = 5$, $f_{\\mathrm{mt},\\max} = 0.2$, $f_{0,\\max} = 0.6$.\n-   $T = 2+2+8+4+4 = 20$.\n-   $G = |\\{i | x_i \\ge 1\\}| = 5$.\n-   $\\sum_{i \\in \\mathcal{M}} x_i = x_0 + x_1 = 2+2 = 4$.\n-   $f_{\\mathrm{mt}} = 4 / 20 = 0.2$.\n-   $f_0 = 5 / 10 = 0.5$.\n-   Conditions check:\n    1. $20 \\le 20 \\le 1000 \\implies \\text{True}$.\n    2. $5 \\ge 5 \\implies \\text{True}$.\n    3. $0.2 \\le 0.2 \\implies \\text{True}$.\n    4. $0.5 \\le 0.6 \\implies \\text{True}$.\n-   All conditions pass. $P = \\text{True}$.\n-   Result: $[20, 5, 0.2000, 0.5000, \\text{True}]$.\n\nCase C:\n-   $\\mathbf{x} = [10, 0, 4, 4, 4, 4, 0, 2, 1, 1]$\n-   $\\mathcal{M} = \\{0, 1\\}$\n-   Thresholds: $c_{\\min} = 20$, $c_{\\max} = 1000$, $g_{\\min} = 5$, $f_{\\mathrm{mt},\\max} = 0.2$, $f_{0,\\max} = 0.6$.\n-   $T = 10+0+4+4+4+4+0+2+1+1 = 30$.\n-   $G = |\\{i | x_i \\ge 1\\}| = 8$.\n-   $\\sum_{i \\in \\mathcal{M}} x_i = x_0 + x_1 = 10+0 = 10$.\n-   $f_{\\mathrm{mt}} = 10 / 30 \\approx 0.333333...$.\n-   $f_0 = 2 / 10 = 0.2$.\n-   Conditions check:\n    1. $20 \\le 30 \\le 1000 \\implies \\text{True}$.\n    2. $8 \\ge 5 \\implies \\text{True}$.\n    3. $0.3333... \\le 0.2 \\implies \\text{False}$.\n    4. $0.2 \\le 0.6 \\implies \\text{True}$.\n-   One condition fails. $P = \\text{False}$.\n-   Result: $[30, 8, 0.3333, 0.2000, \\text{False}]$.\n\nCase D:\n-   $\\mathbf{x} = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]$\n-   $\\mathcal{M} = \\{0, 1\\}$\n-   Thresholds: $c_{\\min} = 20$, $c_{\\max} = 1000$, $g_{\\min} = 5$, $f_{\\mathrm{mt},\\max} = 0.2$, $f_{0,\\max} = 0.6$.\n-   $T = 0$.\n-   $G = |\\{i | x_i \\ge 1\\}| = 0$.\n-   $f_{\\mathrm{mt}} = 0$ (by definition as $T=0$).\n-   $f_0 = 10 / 10 = 1.0$.\n-   Conditions check:\n    1. $20 \\le 0 \\le 1000 \\implies \\text{False}$.\n    2. $0 \\ge 5 \\implies \\text{False}$.\n    3. $0 \\le 0.2 \\implies \\text{True}$.\n    4. $1.0 \\le 0.6 \\implies \\text{False}$.\n-   Multiple conditions fail. $P = \\text{False}$.\n-   Result: $[0, 0, 0.0000, 1.0000, \\text{False}]$.\n\nCase E:\n-   $\\mathbf{x} = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]$\n-   $\\mathcal{M} = \\{0, 1\\}$\n-   Thresholds: $c_{\\min} = 20$, $c_{\\max} = 1000$, $g_{\\min} = 5$, $f_{\\mathrm{mt},\\max} = 0.2$, $f_{0,\\max} = 0.6$.\n-   $T = 10 \\times 100 = 1000$.\n-   $G = |\\{i | x_i \\ge 1\\}| = 10$.\n-   $\\sum_{i \\in \\mathcal{M}} x_i = x_0 + x_1 = 100+100 = 200$.\n-   $f_{\\mathrm{mt}} = 200 / 1000 = 0.2$.\n-   $f_0 = 0 / 10 = 0.0$.\n-   Conditions check:\n    1. $20 \\le 1000 \\le 1000 \\implies \\text{True}$.\n    2. $10 \\ge 5 \\implies \\text{True}$.\n    3. $0.2 \\le 0.2 \\implies \\text{True}$.\n    4. $0.0 \\le 0.6 \\implies \\text{True}$.\n-   All conditions pass. $P = \\text{True}$.\n-   Result: $[1000, 10, 0.2000, 0.0000, \\text{True}]$.\n\nThe implementation will follow this logic precisely, ensuring correct handling of data types and floating point comparisons.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the scRNA-seq quality control problem for a given suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (happy path)\n        ([2, 3, 4, 5, 6, 0, 0, 1, 2, 3], {0, 1}, 20, 1000, 5, 0.2, 0.6, 1),\n        # Case B (boundary inclusivity at c_min and fmt_max)\n        ([2, 2, 8, 4, 4, 0, 0, 0, 0, 0], {0, 1}, 20, 1000, 5, 0.2, 0.6, 1),\n        # Case C (failure due to elevated mitochondrial fraction)\n        ([10, 0, 4, 4, 4, 4, 0, 2, 1, 1], {0, 1}, 20, 1000, 5, 0.2, 0.6, 1),\n        # Case D (edge case with no detected molecules)\n        ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], {0, 1}, 20, 1000, 5, 0.2, 0.6, 1),\n        # Case E (boundary inclusivity at c_max and fmt_max)\n        ([100, 100, 100, 100, 100, 100, 100, 100, 100, 100], {0, 1}, 20, 1000, 5, 0.2, 0.6, 1)\n    ]\n\n    def process_cell(x_list, M_set, c_min, c_max, g_min, fmt_max, f0_max, k_min):\n        \"\"\"\n        Processes a single cell's count vector to determine its quality metrics and pass/fail status.\n        \"\"\"\n        # Convert inputs to NumPy arrays for efficient computation.\n        x = np.array(x_list, dtype=np.int64)\n        N = len(x)\n\n        # 1. Total UMI count (T)\n        T = np.sum(x)\n\n        # 2. Number of detected genes (G)\n        G = np.sum(x >= k_min)\n\n        # 3. Mitochondrial fraction (f_mt)\n        # Handle the T=0 case as per problem definition.\n        if T > 0:\n            mt_counts = np.sum(x[list(M_set)])\n            f_mt = mt_counts / T\n        else:\n            f_mt = 0.0\n\n        # 4. Zero fraction (f_0)\n        # N will be > 0 based on problem constraints (non-empty x vector)\n        f_0 = np.sum(x == 0) / N\n\n        # 5. Quality control decision (P)\n        pass_T = (c_min <= T <= c_max)\n        pass_G = (G >= g_min)\n        pass_fmt = (f_mt <= fmt_max)\n        pass_f0 = (f_0 <= f0_max)\n        P = all([pass_T, pass_G, pass_fmt, pass_f0])\n\n        # For reporting, round fractional quantities to four decimal places.\n        rounded_f_mt = round(f_mt, 4)\n        rounded_f_0 = round(f_0, 4)\n        \n        # In Python, round(0.2, 4) is 0.2, not 0.2000. The desired output\n        # format is Python's default representation, so `round` is sufficient.\n\n        return [int(T), int(G), rounded_f_mt, rounded_f_0, P]\n\n    results = []\n    for case in test_cases:\n        # Unpack the parameters for the current test case\n        x_val, M_val, c_min_val, c_max_val, g_min_val, fmt_max_val, f0_max_val, k_min_val = case\n        \n        # Process the cell and store the result\n        result = process_cell(x_val, M_val, c_min_val, c_max_val, g_min_val, fmt_max_val, f0_max_val, k_min_val)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The str() of a list produces the required [item1, item2, ...] format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2672354"}, {"introduction": "A key challenge in single-cell analysis is to distinguish meaningful biological heterogeneity from technical noise. This practice focuses on the crucial step of identifying highly variable genes (HVGs), which serve as the most informative features for downstream tasks like clustering and trajectory inference. By implementing a model that normalizes for the inherent mean-variance relationship in count data [@problem_id:2672352], you will develop a deep, practical understanding of how to select genes that truly reflect diverse cellular states and lineages.", "problem": "You are tasked with formalizing and implementing a principled, model-based selection of highly variable genes (HVGs) for single-cell ribonucleic acid sequencing (scRNA-seq) data, suitable for lineage and fate analysis in developmental biology. Base your reasoning on foundational principles: the Central Dogma of molecular biology (deoxyribonucleic acid to ribonucleic acid to protein), the stochastic nature of transcriptional bursting leading to overdispersed count data, and well-tested count models for sequencing data such as the Poisson and Negative Binomial distributions. Your solution must derive an algorithm from these principles without relying on prepackaged formulas.\n\nStart from these widely accepted bases:\n- In droplet-based scRNA-seq, the observed unique molecular identifier counts for gene $g$ in cell $i$, denoted $Y_{gi}$, arise from a sampling process whose mean scales with cell-specific library size $L_i$ and gene-specific expression $\\theta_g$. Under a Poisson sampling model, $\\operatorname{Var}(Y_{gi}) \\approx \\mathbb{E}[Y_{gi}]$, and under a Negative Binomial model, $\\operatorname{Var}(Y_{gi}) \\approx \\mathbb{E}[Y_{gi}] + \\phi_g \\mathbb{E}[Y_{gi}]^2$ for dispersion $\\phi_g \\ge 0$.\n- To compare genes across cells with heterogeneous library sizes, counts should be normalized by a per-cell factor.\n- The variance of expression increases with the mean due to both sampling noise and biological variability; thus, a fair HVG definition requires removing the mean–variance dependence to highlight genes whose variability exceeds expectation.\n\nDefine and implement the following HVG statistic from first principles:\n1. For a count matrix with $G$ genes and $C$ cells, compute the per-cell library size $L_i = \\sum_{g=1}^{G} Y_{gi}$.\n2. Perform counts-per-ten-thousand normalization: $U_{gi} = \\dfrac{10000 \\cdot Y_{gi}}{L_i}$ for each cell $i$ where $L_i > 0$. If any $L_i = 0$, define $L_i \\leftarrow 1$ for the purpose of this normalization to avoid division by zero.\n3. Apply a variance-stabilizing log-transform: $Z_{gi} = \\log\\!\\big(1 + U_{gi}\\big)$, where $\\log$ denotes the natural logarithm with base $e$.\n4. For each gene $g$, compute the empirical mean $m_g = \\dfrac{1}{C} \\sum_{i=1}^{C} Z_{gi}$ and empirical unbiased variance $v_g = \\dfrac{1}{C-1} \\sum_{i=1}^{C} \\big(Z_{gi} - m_g\\big)^2$.\n5. Fit a global ordinary least squares linear trend of variance on mean across genes: $v_g \\approx a + b \\, m_g$. Estimate $a$ and $b$ by minimizing $\\sum_{g=1}^{G} \\big(v_g - (a + b m_g)\\big)^2$. Derive the closed-form solution $b = \\dfrac{\\operatorname{Cov}(m,v)}{\\operatorname{Var}(m)}$ and $a = \\bar{v} - b \\, \\bar{m}$, where $\\bar{m}$ and $\\bar{v}$ are the across-gene means and $\\operatorname{Cov}$ and $\\operatorname{Var}$ are the across-gene covariance and variance. If $\\operatorname{Var}(m) = 0$, define $b \\leftarrow 0$ and $a \\leftarrow \\bar{v}$.\n6. For each gene $g$, compute the residual $e_g = v_g - (a + b m_g)$ and the mean squared error $\\mathrm{MSE} = \\dfrac{1}{G-2} \\sum_{g=1}^{G} e_g^2$ (valid for $G \\ge 3$). If $\\mathrm{MSE} = 0$, define the standardized residual for all genes as $0$.\n7. Define the standardized residual $z_g = \\dfrac{e_g}{\\sqrt{\\mathrm{MSE}}}$ for $\\mathrm{MSE} > 0$. The HVG ranking score is $z_g$. Higher $z_g$ indicates variability exceeding the fitted expectation.\n8. To select the top $K$ HVGs, sort genes by descending $z_g$. In the event of ties in $z_g$, break ties by ascending gene index (zero-based). If $K > G$, return all $G$ genes in that order.\n\nYour task is to implement a program that applies this pipeline to the following test suite. Each test case provides a count matrix and a selection parameter $K$. Gene indices are zero-based. All numeric entries below are raw counts $Y_{gi}$.\n\nTest case $1$ (happy path, heterogeneous library sizes):\n- Matrix with $G = 5$ genes and $C = 6$ cells:\n  - Gene $0$: $[10, 10, 10, 10, 10, 10]$\n  - Gene $1$: $[1, 2, 3, 4, 5, 6]$\n  - Gene $2$: $[0, 10, 0, 20, 0, 30]$\n  - Gene $3$: $[0, 0, 1, 0, 0, 0]$\n  - Gene $4$: $[5, 5, 8, 6, 5, 4]$\n- Select top $K = 2$ HVGs.\n\nTest case $2$ (boundary: zero-variance and all-zero gene present):\n- Matrix with $G = 3$ and $C = 4$:\n  - Gene $0$: $[0, 0, 0, 0]$\n  - Gene $1$: $[0, 5, 0, 10]$\n  - Gene $2$: $[3, 3, 3, 3]$\n- Select top $K = 1$ HVG.\n\nTest case $3$ (edge: identical genes to exercise tie-break rule):\n- Matrix with $G = 4$ and $C = 3$:\n  - Gene $0$: $[1, 0, 1]$\n  - Gene $1$: $[1, 0, 1]$\n  - Gene $2$: $[0, 2, 4]$\n  - Gene $3$: $[0, 0, 0]$\n- Select top $K = 2$ HVGs.\n\nTest case $4$ (edge: request more HVGs than available):\n- Matrix with $G = 4$ and $C = 4$:\n  - Gene $0$: $[1, 2, 1, 2]$\n  - Gene $1$: $[5, 5, 5, 5]$\n  - Gene $2$: $[0, 0, 10, 0]$\n  - Gene $3$: $[3, 0, 0, 0]$\n- Select top $K = 5$ HVGs.\n\nYour program must implement the pipeline exactly as defined above and produce a single line of output containing a list of the selected gene index lists for the test cases in order. The output format must be a single line string of the form \"[[i0,i1,...],[j0,j1,...],...]\" with no spaces, where each inner list contains the selected zero-based gene indices for that test case in sorted order by the prescribed HVG ranking rule. No physical units are involved. Angles are not involved. Percentages must not be used anywhere. Ensure all computations follow the definitions above exactly, including using the natural logarithm and unbiased variance with denominator $C-1$.", "solution": "The problem requires the formalization and implementation of a statistical method for identifying highly variable genes (HVGs) from single-cell ribonucleic acid sequencing (scRNA-seq) count data. The approach must be derived from foundational principles of molecular biology and statistics.\n\nThe central challenge in identifying biologically significant gene expression heterogeneity is to distinguish it from technical noise and other confounding factors. The expression of a gene is a stochastic process. As stated, the Central Dogma describes the flow of genetic information from deoxyribonucleic acid (DNA) to ribonucleic acid (RNA) to protein. Transcription, the synthesis of RNA from a DNA template, occurs in stochastic bursts. This intrinsic randomness, combined with the technical noise from reverse transcription, amplification, and sequencing, leads to observed count data ($Y_{gi}$) that are highly variable.\n\nA key observation in sequencing data is that a gene's expression variance is strongly dependent on its mean expression level. This mean-variance relationship is a fundamental property of count distributions like the Poisson, where $\\operatorname{Var}(Y) = \\mathbb{E}[Y]$, and the Negative Binomial, where $\\operatorname{Var}(Y) = \\mathbb{E}[Y] + \\phi \\mathbb{E}[Y]^2$. Furthermore, droplet-based scRNA-seq experiments produce cells with varying library sizes ($L_i$), which is the total number of molecules detected in a cell. A gene's observed count $Y_{gi}$ in cell $i$ is proportional to both its intrinsic expression level $\\theta_g$ and the cell's library size $L_i$. To meaningfully compare gene expression across cells, this library size effect must be neutralized.\n\nThe prescribed algorithm addresses these challenges systematically, as detailed below.\n\n1.  **Library Size Calculation**:\n    The first step is to quantify the total sequencing depth for each cell. The library size for cell $i$, denoted $L_i$, is the sum of counts for all genes within that cell:\n    $$ L_i = \\sum_{g=1}^{G} Y_{gi} $$\n    where $Y_{gi}$ is the raw count for gene $g$ in cell $i$, $G$ is the total number of genes, and $C$ is the total number of cells.\n\n2.  **Counts-Per-Million Style Normalization**:\n    To make gene expression values comparable across cells with different library sizes, we normalize the raw counts. The problem specifies a \"counts-per-ten-thousand\" normalization. The normalized expression value $U_{gi}$ is calculated for each gene $g$ in each cell $i$:\n    $$ U_{gi} = \\frac{10000 \\cdot Y_{gi}}{L_i} $$\n    This scaling transforms the counts to a common scale, conceptually representing the number of molecules that would have been observed for that gene if the library size were exactly $10000$. The problem correctly specifies that if a library size $L_i$ is $0$, it should be set to $1$ to prevent division by zero, resulting in $U_{gi} = 0$ for all genes in that cell.\n\n3.  **Variance-Stabilizing Transformation**:\n    Even after normalization, the variance of $U_{gi}$ still depends on its mean. To mitigate this, a variance-stabilizing transformation is applied. The problem specifies the logarithm transform, which is common for count-like data:\n    $$ Z_{gi} = \\log(1 + U_{gi}) $$\n    The addition of $1$ prevents taking the logarithm of zero. This transformation compresses the data range and makes the variance more independent of the mean, although often a residual trend remains.\n\n4.  **Empirical Mean and Variance Estimation**:\n    After transformation, we quantify the expression level and variability for each gene across all $C$ cells. For each gene $g$, we compute its empirical mean, $m_g$, and its empirical unbiased sample variance, $v_g$:\n    $$ m_g = \\frac{1}{C} \\sum_{i=1}^{C} Z_{gi} $$\n    $$ v_g = \\frac{1}{C-1} \\sum_{i=1}^{C} (Z_{gi} - m_g)^2 $$\n    The use of the denominator $C-1$ for the variance provides an unbiased estimate of the population variance.\n\n5.  **Modeling the Mean-Variance Trend**:\n    The core of the method is to model the remaining relationship between the variance $v_g$ and the mean $m_g$. This trend represents the expected level of variance for a gene given its average expression level. By fitting a model to this trend, we can identify genes whose variance significantly exceeds this expectation. The problem specifies an ordinary least squares (OLS) linear model:\n    $$ v_g \\approx a + b \\, m_g $$\n    The parameters $a$ and $b$ are estimated by minimizing the sum of squared residuals across all genes, $\\sum_{g=1}^{G} (v_g - (a + b m_g))^2$. The closed-form solutions for the OLS estimators are:\n    $$ b = \\frac{\\operatorname{Cov}(m,v)}{\\operatorname{Var}(m)} \\quad \\text{and} \\quad a = \\bar{v} - b \\, \\bar{m} $$\n    Here, $\\bar{m}$ and $\\bar{v}$ are the means of $m_g$ and $v_g$ across all genes, and $\\operatorname{Var}(m)$ and $\\operatorname{Cov}(m,v)$ are the variance and covariance calculated across the set of $(m_g, v_g)$ pairs for all genes. Specifically, $\\operatorname{Var}(m) = \\frac{1}{G}\\sum_{g=1}^{G}(m_g - \\bar{m})^2$ and $\\operatorname{Cov}(m,v) = \\frac{1}{G}\\sum_{g=1}^{G}(m_g - \\bar{m})(v_g - \\bar{v})$. If $\\operatorname{Var}(m) = 0$, which occurs if all genes have the same mean expression $m_g$, the slope $b$ is undefined; the problem specifies setting $b \\leftarrow 0$ and $a \\leftarrow \\bar{v}$ in this situation.\n\n6.  **Calculating Standardized Residuals**:\n    A gene's deviation from the expected trend is captured by its residual, $e_g$:\n    $$ e_g = v_g - (a + b m_g) $$\n    A large positive residual indicates that a gene is more variable than expected for its expression level. To make these residuals comparable across different datasets, they are standardized. This requires an estimate of the error variance from the regression. The Mean Squared Error (MSE) provides this:\n    $$ \\mathrm{MSE} = \\frac{1}{G-2} \\sum_{g=1}^{G} e_g^2 $$\n    The denominator $G-2$ is used because two parameters ($a$ and $b$) were estimated from the data, which is a standard adjustment for obtaining an unbiased estimate of the error variance (assuming $G \\ge 3$). The standardized residual, $z_g$, which serves as the final HVG score, is then:\n    $$ z_g = \\frac{e_g}{\\sqrt{\\mathrm{MSE}}} $$\n    If $\\mathrm{MSE} = 0$, all residuals are $0$, so all $z_g$ are defined as $0$.\n\n7.  **Ranking and Selection**:\n    The final step is to select the top $K$ HVGs. Genes are ranked in descending order of their standardized residual score $z_g$. A higher $z_g$ signifies greater evidence for biologically interesting variability. The problem specifies that ties in $z_g$ must be resolved by choosing the gene with the lower (ascending) zero-based index. If the requested number of genes $K$ is greater than the total number of genes $G$, all genes are returned in their ranked order.\n\nThis principled pipeline provides a robust and interpretable method for feature selection in scRNA-seq analysis, grounded in statistical models of count data and designed to isolate biological signal from technical noise.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the HVG selection pipeline on all test cases.\n    \"\"\"\n    test_cases = [\n        (\n            np.array([\n                [10, 10, 10, 10, 10, 10],\n                [1, 2, 3, 4, 5, 6],\n                [0, 10, 0, 20, 0, 30],\n                [0, 0, 1, 0, 0, 0],\n                [5, 5, 8, 6, 5, 4]\n            ]),\n            2\n        ),\n        (\n            np.array([\n                [0, 0, 0, 0],\n                [0, 5, 0, 10],\n                [3, 3, 3, 3]\n            ]),\n            1\n        ),\n        (\n            np.array([\n                [1, 0, 1],\n                [1, 0, 1],\n                [0, 2, 4],\n                [0, 0, 0]\n            ]),\n            2\n        ),\n        (\n            np.array([\n                [1, 2, 1, 2],\n                [5, 5, 5, 5],\n                [0, 0, 10, 0],\n                [3, 0, 0, 0]\n            ]),\n            5\n        )\n    ]\n\n    results = []\n    for Y, K in test_cases:\n        selected_indices = find_hvgs(Y, K)\n        results.append(selected_indices)\n\n    # Format the final output string exactly as specified.\n    print(f\"[{','.join([str(res) for res in results])}]\".replace(\" \", \"\"))\n\ndef find_hvgs(Y, K):\n    \"\"\"\n    Implements the highly variable gene (HVG) selection pipeline.\n\n    Args:\n        Y (np.ndarray): A GxC count matrix (G genes, C cells).\n        K (int): The number of top HVGs to select.\n\n    Returns:\n        list: A list of the top K gene indices, sorted by HVG score.\n    \"\"\"\n    if Y.shape[0] == 0 or Y.shape[1] == 0:\n        return []\n\n    G, C = Y.shape\n\n    # Step 1: Compute per-cell library size\n    L = Y.sum(axis=0, dtype=np.float64)\n\n    # Step 2: Perform counts-per-ten-thousand normalization\n    # Handle L_i = 0 case\n    L[L == 0] = 1\n    # Use broadcasting for normalization\n    U = (10000.0 * Y) / L[np.newaxis, :]\n\n    # Step 3: Apply a variance-stabilizing log-transform\n    Z = np.log1p(U)\n\n    # Step 4: Compute empirical mean and unbiased variance for each gene\n    # The problem implies C > 1, so ddof=1 is safe.\n    if C <= 1:\n        # Cannot compute variance, return empty list or handle as per a more\n        # complete specification. For this problem, C > 1 is guaranteed by tests.\n        return list(range(G))[:K]\n        \n    m = Z.mean(axis=1)\n    v = Z.var(axis=1, ddof=1)\n\n    # Step 5: Fit a global OLS linear trend\n    if G == 0:\n        return []\n\n    m_bar = m.mean()\n    v_bar = v.mean()\n\n    # Use population variance (ddof=0) for the 'm' vector as per standard OLS derivation.\n    m_var = m.var(ddof=0)\n\n    if m_var == 0:\n        b = 0.0\n        a = v_bar\n    else:\n        # Use population covariance (ddof=0). np.cov returns a 2x2 matrix.\n        cov_mv = np.cov(m, v, ddof=0)[0, 1]\n        b = cov_mv / m_var\n        a = v_bar - b * m_bar\n    \n    # Step 6 & 7: Compute standardized residuals\n    # The problem implies G >= 3 for MSE calculation.\n    if G < 3:\n        # The z-score calculation is not well-defined.\n        # However, the test cases all have G >= 3.\n        # Fallback to sorting by raw residual if needed. For this problem, we follow spec.\n        e = v - (a + b * m)\n        z = e # Use raw residual if G < 3, no standardization possible\n    else:\n        e = v - (a + b * m)\n        mse_sum_sq = np.sum(e**2)\n        mse = mse_sum_sq / (G - 2)\n        \n        if mse == 0:\n            z = np.zeros(G)\n        else:\n            z = e / np.sqrt(mse)\n\n    # Step 8: Select top K HVGs\n    gene_indices = np.arange(G)\n\n    # Sort by descending z-score, breaking ties with ascending gene index.\n    # We can create a list of tuples (z_score, gene_index) and sort.\n    # The key (-z, idx) implements the desired sorting order.\n    sorted_genes = sorted(zip(z, gene_indices), key=lambda x: (-x[0], x[1]))\n    \n    # Extract the sorted indices\n    sorted_indices = [idx for score, idx in sorted_genes]\n\n    # Return the top K, or all G if K > G.\n    return sorted_indices[:K]\n\nsolve()\n```", "id": "2672352"}, {"introduction": "Trajectory inference algorithms can sometimes misinterpret complex developmental topologies, such as mistaking two distinct lineages converging on a similar fate for a single lineage bifurcating into two. This advanced practice challenges you to implement a statistical framework to adjudicate between these two scenarios based on their underlying gene expression programs. By using linear models to residualize confounding effects and then testing for correlation [@problem_id:2672336], you will learn how to move beyond visual interpretation to a quantitative, hypothesis-driven analysis of cell fate trajectories.", "problem": "You are tasked with building a program that adjudicates whether two independent progenitor-to-fate transcriptional transitions are consistent with convergent differentiation using single-cell transcriptomics (single-cell ribonucleic acid sequencing (scRNA-seq)) summary statistics. The fundamental base you must rely on includes: (i) the Central Dogma linking messenger ribonucleic acid to protein and phenotype (gene expression as a proxy for cell state), (ii) well-established additive linear models for gene programs (confounder programs can be projected out from observed changes using least-squares residualization), and (iii) the null model that, in the absence of a common fate-directed program, gene-wise changes across independent transitions are uncorrelated. You must express all computations in purely mathematical and logical terms; there are no physical units in this task.\n\nDefinitions:\n- Let $d$ be the number of genes shared between two progenitor-to-fate transitions.\n- Let $\\mathbf{g} \\in \\mathbb{R}^{d}$ denote the gene-wise log-fold-change vector for transition $A \\rightarrow \\text{child}$.\n- Let $\\mathbf{h} \\in \\mathbb{R}^{d}$ denote the gene-wise log-fold-change vector for transition $B \\rightarrow \\text{child}$.\n- Let $\\mathbf{C} \\in \\mathbb{R}^{d \\times k}$ be a matrix of $k$ confounder program vectors (columns correspond to programs), which must be linearly residualized from $\\mathbf{g}$ and $\\mathbf{h}$.\n- After removing confounders, compute the sample Pearson correlation $r$ across the $d$ genes between the residualized vectors.\n- Under the null hypothesis of no convergent program, gene-wise residuals from the two transitions are independent and identically distributed with zero correlation. Use a two-sided test for zero correlation and report the corresponding $p$-value (angles, when used in internal computations, must be in radians; however, this task does not require you to report any angle).\n- Decision rule: declare \"convergent differentiation\" if and only if $r \\ge r_{\\min}$ and $p \\le \\alpha$, with thresholds $r_{\\min}$ and $\\alpha$ provided below.\n\nResidualization requirement:\n- Given a vector $\\mathbf{v} \\in \\mathbb{R}^{d}$ and confounder matrix $\\mathbf{C} \\in \\mathbb{R}^{d \\times k}$, compute the least-squares residual\n$$\n\\mathbf{v}_{\\text{res}} \\;=\\; \\mathbf{v} \\;-\\; \\mathbf{C}\\,\\hat{\\boldsymbol{\\beta}}, \\quad \\text{where } \\hat{\\boldsymbol{\\beta}} \\text{ minimizes } \\|\\mathbf{v} - \\mathbf{C}\\boldsymbol{\\beta}\\|_2^2.\n$$\nIf $k = 0$ (no confounders), then $\\mathbf{v}_{\\text{res}} = \\mathbf{v}$. If either residualized vector has zero empirical variance, define $r=0$ and $p=1$ for that case.\n\nCorrelation test:\n- Let $n = d$ and let $r$ be the sample Pearson correlation computed across the $n$ genes between $\\mathbf{g}_{\\text{res}}$ and $\\mathbf{h}_{\\text{res}}$. Under the null, use the well-tested fact that the Student $t$-statistic derived from $r$ has a $t$-distribution with $\\nu = n-2$ degrees of freedom. Compute the two-sided $p$-value accordingly. If $n \\le 2$, set $p=1$ by definition.\n\nParameters for all test cases:\n- Use $\\alpha = 0.05$ and $r_{\\min} = 0.5$.\n\nTest suite:\nImplement your program to process the following five cases. For each case, you are given $\\mathbf{g}$, $\\mathbf{h}$, and the confounder matrix $\\mathbf{C}$ as an explicit list of column vectors. All numbers are real. Every vector has length $d$ appropriate to that case.\n\n- Case $1$:\n  - $\\mathbf{g} = [\\, 3.55,\\; -1.02,\\; 4.51,\\; 0.03,\\; -0.51,\\; 1.00,\\; 3.52,\\; -1.03 \\,]$\n  - $\\mathbf{h} = [\\, 1.47,\\; -1.06,\\; 2.58,\\; -0.01,\\; -2.88,\\; 1.08,\\; 1.51,\\; -1.08 \\,]$\n  - $\\mathbf{C}$ has one column: $\\mathbf{c}_1 = [\\, 1,\\; 0,\\; 1,\\; 0,\\; 1,\\; 0,\\; 1,\\; 0 \\,]^\\top$\n- Case $2$:\n  - $\\mathbf{g} = [\\, 4.00,\\; 2.00,\\; 0.00,\\; 3.00,\\; 2.00 \\,]$\n  - $\\mathbf{h} = [\\, 1.50,\\; 3.50,\\; 1.50,\\; -0.50,\\; 2.50 \\,]$\n  - $\\mathbf{C}$ has one column: $\\mathbf{c}_1 = [\\, 1,\\; 1,\\; 1,\\; 1,\\; 1 \\,]^\\top$\n- Case $3$:\n  - $\\mathbf{g} = [\\, 0.50,\\; -0.50,\\; 0.00 \\,]$\n  - $\\mathbf{h} = [\\, 0.60,\\; 0.10,\\; -0.60 \\,]$\n  - $\\mathbf{C}$ has zero columns (no confounders).\n- Case $4$:\n  - $\\mathbf{g} = [\\, 51,\\; 48,\\; 50,\\; 53,\\; 49,\\; 52 \\,]$\n  - $\\mathbf{h} = [\\, 38,\\; 41,\\; 43,\\; 39,\\; 42,\\; 40 \\,]$\n  - $\\mathbf{C}$ has one column: $\\mathbf{c}_1 = [\\, 1,\\; 1,\\; 1,\\; 1,\\; 1,\\; 1 \\,]^\\top$\n- Case $5$:\n  - $\\mathbf{g} = [\\, 3,\\; 6,\\; 3,\\; 6,\\; 3,\\; 6 \\,]$\n  - $\\mathbf{h} = [\\, 0,\\; 1,\\; 0,\\; -1,\\; 0,\\; 1 \\,]$\n  - $\\mathbf{C}$ has one column: $\\mathbf{c}_1 = [\\, 1,\\; 2,\\; 1,\\; 2,\\; 1,\\; 2 \\,]^\\top$\n\nYour program must process the cases in the order $1$ through $5$, and for each case, output a boolean indicating whether convergent differentiation is supported by the decision rule described above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order of the five cases, where each entry is a boolean (for example, $[ \\text{True}, \\text{False}, \\ldots ]$). No extra text should be printed.", "solution": "The problem requires the formulation and implementation of a statistical procedure to determine if two progenitor-to-fate transcriptional transitions exhibit convergent differentiation. This adjudication will be based on gene-wise log-fold-change vectors and a set of predefined decision rules. The process is based on principles of linear modeling and statistical hypothesis testing.\n\nThe procedure is executed in four sequential steps for each test case.\n\nStep 1: Confounder Residualization\nThe observed gene expression changes, represented by vectors $\\mathbf{g} \\in \\mathbb{R}^{d}$ and $\\mathbf{h} \\in \\mathbb{R}^{d}$, may be influenced by biological or technical processes unrelated to the differentiation fate of interest. These are termed confounders and are modeled as a set of $k$ gene program vectors, forming the columns of a matrix $\\mathbf{C} \\in \\mathbb{R}^{d \\times k}$. To isolate the signal of interest, the linear effects of these confounders must be removed. This is achieved via least-squares residualization.\n\nFor a given vector $\\mathbf{v}$ (which will be $\\mathbf{g}$ or $\\mathbf{h}$), we find the vector of coefficients $\\hat{\\boldsymbol{\\beta}} \\in \\mathbb{R}^{k}$ that minimizes the squared Euclidean norm of the residual, $\\|\\mathbf{v} - \\mathbf{C}\\boldsymbol{\\beta}\\|_2^2$. This is a standard linear regression problem, and the solution $\\hat{\\boldsymbol{\\beta}}$ is given by the normal equations:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{C}^\\top\\mathbf{C})^{-1}\\mathbf{C}^\\top\\mathbf{v}\n$$\nThe residual vector $\\mathbf{v}_{\\text{res}}$ is the component of $\\mathbf{v}$ that is orthogonal to the column space of $\\mathbf{C}$:\n$$\n\\mathbf{v}_{\\text{res}} = \\mathbf{v} - \\mathbf{C}\\hat{\\boldsymbol{\\beta}} = \\mathbf{v} - \\mathbf{C}(\\mathbf{C}^\\top\\mathbf{C})^{-1}\\mathbf{C}^\\top\\mathbf{v}\n$$\nThis procedure is applied independently to both $\\mathbf{g}$ and $\\mathbf{h}$ to obtain the residualized vectors $\\mathbf{g}_{\\text{res}}$ and $\\mathbf{h}_{\\text{res}}$. If there are no confounders, i.e., $k=0$, the confounder matrix $\\mathbf{C}$ is empty, and no projection is performed, so $\\mathbf{v}_{\\text{res}} = \\mathbf{v}$. Computationally, this projection is best performed using numerically stable algorithms such as those provided by `numpy.linalg.lstsq`.\n\nStep 2: Pearson Correlation\nAfter removing confounding effects, we quantify the similarity between the two residualized transitions by computing the sample Pearson correlation coefficient, $r$, between $\\mathbf{g}_{\\text{res}}$ and $\\mathbf{h}_{\\text{res}}$. The vectors are treated as samples of size $d$, where $d$ is the number of genes. The formula for $r$ is:\n$$\nr = \\frac{\\sum_{i=1}^{d} (g_{\\text{res},i} - \\bar{g}_{\\text{res}})(h_{\\text{res},i} - \\bar{h}_{\\text{res}})}{\\sqrt{\\sum_{i=1}^{d} (g_{\\text{res},i} - \\bar{g}_{\\text{res}})^2 \\sum_{i=1}^{d} (h_{\\text{res},i} - \\bar{h}_{\\text{res}})^2}}\n$$\nwhere $\\bar{g}_{\\text{res}}$ and $\\bar{h}_{\\text{res}}$ are the sample means of the elements of $\\mathbf{g}_{\\text{res}}$ and $\\mathbf{h}_{\\text{res}}$, respectively. A positive correlation suggests that the genes that are up-regulated in transition $A$ are also up-regulated in transition $B$, and similarly for down-regulated genes, which is the signature of convergent differentiation.\n\nA special case must be handled: if either $\\mathbf{g}_{\\text{res}}$ or $\\mathbf{h}_{\\text{res}}$ has zero empirical variance (i.e., all its elements are identical), the denominator of the correlation formula becomes zero. This can occur if a vector lies entirely within the column space of the confounder matrix $\\mathbf{C}$. In this situation, as per the problem definition, we set $r=0$ and the corresponding p-value $p=1$.\n\nStep 3: Significance Testing\nTo assess whether the observed correlation $r$ is statistically significant or merely due to random chance, we perform a hypothesis test. The null hypothesis, $H_0$, is that there is no underlying correlation between the residual gene expression changes of the two transitions ($\\rho=0$). Under this null hypothesis, for a sample size of $n=d$ genes, the test statistic $t$ defined as:\n$$\nt = r \\sqrt{\\frac{n-2}{1-r^2}}\n$$\nfollows a Student's t-distribution with $\\nu = n-2$ degrees of freedom. We are interested in a two-sided test because a significant negative correlation would also be non-random, even if it does not support convergence. The p-value is the probability of observing a correlation at least as extreme as $r$ if the null hypothesis were true:\n$$\np = P(|T| \\ge |t|), \\quad \\text{where } T \\sim t_{n-2}\n$$\nThis is computed as $2 \\times \\text{CDF}_t(-|t|; \\nu)$, where $\\text{CDF}_t$ is the cumulative distribution function of the Student's t-distribution. Per the problem, if the number of genes $d$ is less than or equal to $2$, the degrees of freedom $\\nu = n-2$ would be non-positive, making the test undefined. In such cases, we follow the rule to set $p=1$.\n\nStep 4: Decision Rule\nThe final step is to apply the decision rule to declare whether the data support convergent differentiation. This determination is made based on two criteria: the magnitude of the correlation and its statistical significance. The problem specifies the thresholds $r_{\\min}=0.5$ and $\\alpha=0.05$. Convergent differentiation is declared if and only if both conditions are met:\n1. $r \\ge r_{\\min}$ (i.e., $r \\ge 0.5$)\n2. $p \\le \\alpha$ (i.e., $p \\le 0.05$)\n\nThis composite rule ensures that we only flag cases where the effect is both reasonably large (a correlation of at least $0.5$) and statistically unlikely to have arisen by chance. This entire four-step procedure is then systematically applied to each of the five provided test cases to produce the final boolean outputs.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and print results.\n    \"\"\"\n    \n    # Parameters for the decision rule\n    alpha = 0.05\n    r_min = 0.5\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"g\": np.array([3.55, -1.02, 4.51, 0.03, -0.51, 1.00, 3.52, -1.03]),\n            \"h\": np.array([1.47, -1.06, 2.58, -0.01, -2.88, 1.08, 1.51, -1.08]),\n            \"C\": np.array([[1, 0, 1, 0, 1, 0, 1, 0]]).T\n        },\n        {\n            \"g\": np.array([4.00, 2.00, 0.00, 3.00, 2.00]),\n            \"h\": np.array([1.50, 3.50, 1.50, -0.50, 2.50]),\n            \"C\": np.array([[1, 1, 1, 1, 1]]).T\n        },\n        {\n            \"g\": np.array([0.50, -0.50, 0.00]),\n            \"h\": np.array([0.60, 0.10, -0.60]),\n            \"C\": np.empty((3, 0))\n        },\n        {\n            \"g\": np.array([51, 48, 50, 53, 49, 52]),\n            \"h\": np.array([38, 41, 43, 39, 42, 40]),\n            \"C\": np.array([[1, 1, 1, 1, 1, 1]]).T\n        },\n        {\n            \"g\": np.array([3, 6, 3, 6, 3, 6]),\n            \"h\": np.array([0, 1, 0, -1, 0, 1]),\n            \"C\": np.array([[1, 2, 1, 2, 1, 2]]).T\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        g, h, C = case[\"g\"], case[\"h\"], case[\"C\"]\n        d = len(g)\n        k = C.shape[1]\n\n        # Step 1: Residualization\n        if k == 0:\n            g_res = g\n            h_res = h\n        else:\n            # Residualize g\n            beta_g = np.linalg.lstsq(C, g, rcond=None)[0]\n            g_res = g - C @ beta_g\n            \n            # Residualize h\n            beta_h = np.linalg.lstsq(C, h, rcond=None)[0]\n            h_res = h - C @ beta_h\n\n        # Check for zero variance in residualized vectors\n        # A small tolerance is used for floating point comparisons\n        if np.var(g_res) < 1e-12 or np.var(h_res) < 1e-12:\n            r = 0.0\n            p_value = 1.0\n        else:\n            # Step 2: Pearson Correlation\n            mean_g_res = np.mean(g_res)\n            mean_h_res = np.mean(h_res)\n            \n            g_res_centered = g_res - mean_g_res\n            h_res_centered = h_res - mean_h_res\n            \n            numerator = np.sum(g_res_centered * h_res_centered)\n            denominator = np.sqrt(np.sum(g_res_centered**2) * np.sum(h_res_centered**2))\n            \n            # Handle potential division by zero just in case, though variance check should prevent this\n            if denominator < 1e-12:\n                r = 0.0\n            else:\n                r = numerator / denominator\n                # Clamp r to [-1, 1] to handle potential floating point inaccuracies\n                r = np.clip(r, -1.0, 1.0)\n            \n            # Step 3: Significance Test\n            if d <= 2:\n                p_value = 1.0\n            # If r is exactly 1 or -1, the t-statistic is infinite and p-value is 0\n            elif np.abs(r) == 1.0:\n                p_value = 0.0\n            else:\n                df = d - 2\n                t_stat = r * np.sqrt(df / (1 - r**2))\n                # Two-sided p-value from Student's t-distribution\n                p_value = 2 * t.sf(np.abs(t_stat), df)\n\n        # Step 4: Decision Rule\n        decision = (r >= r_min) and (p_value <= alpha)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2672336"}]}