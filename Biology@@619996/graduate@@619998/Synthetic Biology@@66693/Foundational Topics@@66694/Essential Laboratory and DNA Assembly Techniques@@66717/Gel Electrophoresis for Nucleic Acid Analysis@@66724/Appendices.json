{"hands_on_practices": [{"introduction": "The most fundamental application of gel electrophoresis is determining the size of an unknown nucleic acid fragment. This is achieved by comparing its migration distance to a \"ladder\" of known sizes, leveraging the empirical relationship where distance, $d$, is proportional to the logarithm of fragment size, $s$. This exercise [@problem_id:2740371] guides you through the rigorous statistical process of fitting this model, predicting an unknown size via inverse regression, and quantifying the confidence in your prediction by propagating experimental uncertainties.", "problem": "A synthetic biology lab uses slab gel electrophoresis to size Deoxyribonucleic Acid (DNA) fragments. Over a practical range, the migration distance in millimeters (mm) of a fragment is empirically well-modeled as an approximately linear function of the base-$10$ logarithm of its length in base pairs (bp), due to size-dependent sieving in the polymer gel matrix. Assume the following empirical model: the measured migration distance $d$ (in mm) is related to the unknown fragment size $s$ (in bp) via a linear relation\n$$\nd = a + b \\,\\log_{10}(s) + \\varepsilon,\n$$\nwhere $a$ and $b$ are unknown real parameters and $\\varepsilon$ is an additive noise term. Assume that for the ladder bands, $\\varepsilon$ are independent and identically distributed Gaussian random variables with mean $0$ and unknown variance $\\sigma^2$, and that the known ladder sizes are exact (i.e., without uncertainty). A target band’s measured migration distance $d^\\star$ has an independent Gaussian measurement error with known standard deviation $\\sigma_{d^\\star}$.\n\nYour task is to write a program that, for each provided test case, performs the following steps from first principles:\n- Fit the linear model $d = a + b \\,\\log_{10}(s)$ to the ladder data by ordinary least squares, justified as the maximum-likelihood estimator under the independent Gaussian noise model on $d$. Estimate the parameter vector $\\theta = (a,b)^\\top$ and the residual variance $\\sigma^2$ from the data.\n- Given a target band with measured migration distance $d^\\star$, perform inverse prediction to estimate $x^\\star = \\log_{10}(s^\\star)$ via the model inversion $x^\\star = (d^\\star - a)/b$. Treat $a$ and $b$ as estimated (random) and propagate their uncertainty to $x^\\star$ using a first-order Taylor expansion (Delta method), and include the target’s measurement error $\\sigma_{d^\\star}$ as an additional independent contribution to the variance of $x^\\star$.\n- Construct a two-sided $(1-\\alpha)$ confidence interval for $x^\\star$ using the Student’s $t$ distribution with the appropriate degrees of freedom from the regression fit. Transform this interval back to base pairs via $s^\\star = 10^{x^\\star}$, so the interval endpoints are $10^{x^\\star \\pm t_{\\alpha/2,\\nu}\\,\\mathrm{SE}(x^\\star)}$. Report the point estimate and confidence limits as base pairs.\n- Express the final results for each test case as integer base pairs by rounding to the nearest integer.\n\nFundamental base and constraints to use:\n- Use the empirical and widely accepted observation that, for slab gels and a fixed buffer and gel composition over a practical size range, the migration distance $d$ is approximately linear in $\\log_{10}(s)$.\n- Use the definitions and properties of ordinary least squares under Gaussian noise, and first-order error propagation (Taylor expansion) for uncertainty propagation.\n- Use Student’s $t$ distribution to account for parameter uncertainty with $\\nu = n-2$ degrees of freedom, where $n$ is the number of ladder bands.\n\nInput specification for each test case (provided below; there is no external input):\n- A list of ladder sizes in base pairs $[s_1,\\dots,s_n]$ with exact values.\n- A list of measured ladder migration distances in mm $[d_1,\\dots,d_n]$ with independent Gaussian noise of identical, unknown variance.\n- A target band’s measured migration distance $d^\\star$ in mm and its measurement standard deviation $\\sigma_{d^\\star}$ in mm.\n- A confidence level specified by $\\alpha = 0.05$ (i.e., $95\\%$ confidence).\n\nComputation requirements:\n- Perform regression on $d$ as the response and $x=\\log_{10}(s)$ as the predictor.\n- Estimate parameters by minimizing the sum of squared residuals.\n- Estimate the covariance of $(a,b)$ from the regression and use a first-order Delta method to approximate the variance of $x^\\star = (d^\\star - a)/b$, including the additive contribution from the independent measurement error of $d^\\star$ with variance $\\sigma_{d^\\star}^2$.\n- Construct the $(1-\\alpha)$ confidence interval for $x^\\star$ using the Student’s $t$ quantile with $\\nu = n-2$ degrees of freedom, then transform to base pairs as described above.\n- Round $s^\\star$ and its two confidence limits to the nearest integer number of base pairs.\n\nUnits:\n- All migration distances must be treated in mm.\n- All fragment sizes must be reported in base pairs (bp).\n- All confidence intervals must be reported as integer base pairs after rounding.\n\nTest suite:\nProvide results for each of the following three test cases. In each case, $\\alpha$ is fixed at $0.05$.\n\n- Case A:\n  - Ladder sizes (bp): $[100,200,400,800,1500,3000]$.\n  - Ladder measured distances (mm): $[60.4,53.3,48.2,41.6,37.1,30.3]$.\n  - Target measured distance (mm): $d^\\star = 45.0$.\n  - Target measurement standard deviation (mm): $\\sigma_{d^\\star} = 0.5$.\n\n- Case B (boundary on sample size with only three ladder bands and higher noise on the target):\n  - Ladder sizes (bp): $[500,1500,4500]$.\n  - Ladder measured distances (mm): $[46.72,37.33,29.94]$.\n  - Target measured distance (mm): $d^\\star = 35.0$.\n  - Target measurement standard deviation (mm): $\\sigma_{d^\\star} = 1.0$.\n\n- Case C (mild extrapolation beyond the largest ladder size):\n  - Ladder sizes (bp): $[50,100,200,400,800]$.\n  - Ladder measured distances (mm): $[77.625,69.8,63.0,54.7,47.5]$.\n  - Target measured distance (mm): $d^\\star = 45.0$.\n  - Target measurement standard deviation (mm): $\\sigma_{d^\\star} = 0.3$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of results, one per test case, where each result is the triplet $[s^\\star,\\mathrm{low},\\mathrm{high}]$ of integers (all in base pairs), with no additional whitespace. For example: $[[123,100,150],[\\dots],[\\dots]]$.", "solution": "The problem requires the estimation of a Deoxyribonucleic Acid (DNA) fragment's size and its corresponding confidence interval from gel electrophoresis data. This is a classic inverse prediction or calibration problem in statistics, which will be solved from first principles.\n\nThe foundation of the analysis is the empirical linear model relating the migration distance $d$ (in mm) to the base-10 logarithm of the fragment size $s$ (in bp):\n$$\nd = a + b \\,x + \\varepsilon, \\quad \\text{where} \\quad x = \\log_{10}(s)\n$$\nHere, $a$ and $b$ are model parameters, and $\\varepsilon$ represents measurement noise, assumed to be an independent and identically distributed (i.i.d.) Gaussian random variable with mean $0$ and unknown variance $\\sigma^2$.\n\nThe procedure is structured as follows:\n1.  Estimate the parameters $a$ and $b$ using Ordinary Least Squares (OLS) on the provided ladder data.\n2.  Estimate the residual variance $\\sigma^2$.\n3.  Perform inverse prediction to estimate the log-size $x^\\star = \\log_{10}(s^\\star)$ for a target band with measured distance $d^\\star$.\n4.  Propagate the uncertainty from the estimated parameters $(\\hat{a}, \\hat{b})$ and the target measurement $d^\\star$ to calculate the standard error of the estimated log-size, $\\mathrm{SE}(\\hat{x}^\\star)$.\n5.  Construct a $(1-\\alpha)$ confidence interval for $x^\\star$ using the Student's $t$-distribution.\n6.  Transform the point estimate and confidence limits from the logarithmic scale back to the original base pair scale and round to the nearest integer.\n\n**Step 1: Ordinary Least Squares (OLS) Parameter Estimation**\nGiven a set of $n$ ladder bands with known sizes $s_i$ and measured distances $d_i$, we first transform the sizes to the logarithmic scale: $x_i = \\log_{10}(s_i)$. The system of linear equations for the ladder data is:\n$$\nd_i = a + b x_i + \\varepsilon_i, \\quad \\text{for} \\quad i = 1, \\dots, n\n$$\nIn matrix notation, this is $\\mathbf{d} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$, where:\n$$\n\\mathbf{d} = \\begin{pmatrix} d_1 \\\\ \\vdots \\\\ d_n \\end{pmatrix}, \\quad\n\\mathbf{X} = \\begin{pmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{pmatrix}, \\quad\n\\boldsymbol{\\theta} = \\begin{pmatrix} a \\\\ b \\end{pmatrix}, \\quad\n\\boldsymbol{\\varepsilon} = \\begin{pmatrix} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix}\n$$\nThe OLS estimator $\\hat{\\boldsymbol{\\theta}}$ minimizes the sum of squared residuals, $SSR = \\sum_{i=1}^n (d_i - (a + b x_i))^2$. Under the Gaussian noise assumption, this corresponds to the maximum likelihood estimator. The solution is given by the normal equations:\n$$\n\\hat{\\boldsymbol{\\theta}} = \\begin{pmatrix} \\hat{a} \\\\ \\hat{b} \\end{pmatrix} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{d}\n$$\nThe covariance matrix of the parameter estimates $\\hat{\\boldsymbol{\\theta}}$ is $\\mathrm{Cov}(\\hat{\\boldsymbol{\\theta}}) = \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}$.\n\n**Step 2: Estimation of Residual Variance**\nThe unknown variance $\\sigma^2$ is estimated using the residual sum of squares. The residuals are given by $\\mathbf{e} = \\mathbf{d} - \\mathbf{X}\\hat{\\boldsymbol{\\theta}}$. The unbiased estimator for $\\sigma^2$ is the mean squared error, $s_e^2$:\n$$\ns_e^2 = \\frac{\\mathbf{e}^\\top \\mathbf{e}}{n-p} = \\frac{1}{n-2} \\sum_{i=1}^n (d_i - (\\hat{a} + \\hat{b} x_i))^2\n$$\nwhere $p=2$ is the number of parameters in the model ($a$ and $b$). The denominator $\\nu = n-2$ represents the degrees of freedom for the residuals. The estimated covariance matrix of the parameters is then $\\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol{\\theta}}) = s_e^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}$.\n\n**Step 3: Inverse Prediction for the Target Fragment**\nFor a target fragment with a measured migration distance $d^\\star$, we invert the fitted model to obtain a point estimate for its log-size, $\\hat{x}^\\star$:\n$$\n\\hat{x}^\\star = \\frac{d^\\star - \\hat{a}}{\\hat{b}}\n$$\nThe point estimate for the size in base pairs is $s^\\star = 10^{\\hat{x}^\\star}$.\n\n**Step 4: Uncertainty Propagation using the Delta Method**\nThe variance of $\\hat{x}^\\star$ arises from two independent sources: the uncertainty in the estimated parameters $(\\hat{a}, \\hat{b})$ and the measurement uncertainty of the target distance $d^\\star$, which has a given variance $\\sigma_{d^\\star}^2$. We use a first-order Taylor expansion (Delta method) to approximate this variance.\nLet the function for inverse prediction be $g(a, b, d^\\star) = \\frac{d^\\star - a}{b}$. The total variance is the sum of variances from each source:\n$$\n\\mathrm{Var}(\\hat{x}^\\star) \\approx \\mathrm{Var}_{(\\hat{a},\\hat{b})}(\\hat{x}^\\star) + \\mathrm{Var}_{d^\\star}(\\hat{x}^\\star)\n$$\nThe variance from the parameter uncertainty is approximated by:\n$$\n\\mathrm{Var}_{(\\hat{a},\\hat{b})}(\\hat{x}^\\star) \\approx \\begin{pmatrix} \\frac{\\partial g}{\\partial a} \\\\ \\frac{\\partial g}{\\partial b} \\end{pmatrix}^\\top \\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol{\\theta}}) \\begin{pmatrix} \\frac{\\partial g}{\\partial a} \\\\ \\frac{\\partial g}{\\partial b} \\end{pmatrix}\n$$\nThe partial derivatives, evaluated at the estimated values, are:\n$$\n\\frac{\\partial g}{\\partial a} = -\\frac{1}{\\hat{b}}, \\quad \\frac{\\partial g}{\\partial b} = -\\frac{d^\\star - \\hat{a}}{\\hat{b}^2} = -\\frac{\\hat{x}^\\star}{\\hat{b}}\n$$\nThe variance contribution from the model parameters is thus:\n$$\n\\frac{1}{\\hat{b}^2} \\begin{pmatrix} 1 & \\hat{x}^\\star \\end{pmatrix} \\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol{\\theta}}) \\begin{pmatrix} 1 \\\\ \\hat{x}^\\star \\end{pmatrix} = \\frac{s_e^2}{\\hat{b}^2} \\begin{pmatrix} 1 & \\hat{x}^\\star \\end{pmatrix} (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\begin{pmatrix} 1 \\\\ \\hat{x}^\\star \\end{pmatrix}\n$$\nIt is a standard result in regression that $\\mathbf{v}^\\top (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{v} = \\frac{1}{n} + \\frac{(v_2 - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}$ for a vector $\\mathbf{v} = (1, v_2)^\\top$. Here, $v_2 = \\hat{x}^\\star$. So, this term becomes:\n$$\n\\mathrm{Var}_{(\\hat{a},\\hat{b})}(\\hat{x}^\\star) \\approx \\frac{s_e^2}{\\hat{b}^2} \\left[ \\frac{1}{n} + \\frac{(\\hat{x}^\\star - \\bar{x})^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\right]\n$$\nThe variance from the target measurement $d^\\star$ is:\n$$\n\\mathrm{Var}_{d^\\star}(\\hat{x}^\\star) \\approx \\left(\\frac{\\partial g}{\\partial d^\\star}\\right)^2 \\mathrm{Var}(d^\\star) = \\left(\\frac{1}{\\hat{b}}\\right)^2 \\sigma_{d^\\star}^2\n$$\nCombining both independent sources, the total estimated variance of $\\hat{x}^\\star$ is:\n$$\n\\widehat{\\mathrm{Var}}(\\hat{x}^\\star) = \\frac{s_e^2}{\\hat{b}^2} \\left[ \\frac{1}{n} + \\frac{(\\hat{x}^\\star - \\bar{x})^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\right] + \\frac{\\sigma_{d^\\star}^2}{\\hat{b}^2}\n$$\nThe standard error of the estimate is $\\mathrm{SE}(\\hat{x}^\\star) = \\sqrt{\\widehat{\\mathrm{Var}}(\\hat{x}^\\star)}$.\n\n**Step 5: Confidence Interval Construction**\nBecause the variance $\\sigma^2$ was estimated from the data, we use the Student's $t$-distribution with $\\nu = n-2$ degrees of freedom to construct the confidence interval. A two-sided $(1-\\alpha)$ confidence interval for the true log-size $x^\\star$ is:\n$$\n\\hat{x}^\\star \\pm t_{\\alpha/2, \\nu} \\cdot \\mathrm{SE}(\\hat{x}^\\star)\n$$\nwhere $t_{\\alpha/2, \\nu}$ is the upper $(\\alpha/2)$-quantile of the $t$-distribution with $\\nu$ degrees of freedom.\n\n**Step 6: Transformation and Final Result**\nThe point estimate and confidence interval for $x^\\star$ are transformed back to the base pair scale:\n- Point estimate: $s^\\star = 10^{\\hat{x}^\\star}$\n- Confidence Limits: $[s_{low}, s_{high}] = [10^{\\hat{x}^\\star - t_{\\alpha/2, \\nu} \\cdot \\mathrm{SE}(\\hat{x}^\\star)}, 10^{\\hat{x}^\\star + t_{\\alpha/2, \\nu} \\cdot \\mathrm{SE}(\\hat{x}^\\star)}]$\nFinally, these three values ($s^\\star, s_{low}, s_{high}$) are rounded to the nearest integer as required. This completes the formal procedure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef calculate_size_and_ci(ladder_sizes, ladder_dists, d_star, sigma_d_star, alpha):\n    \"\"\"\n    Calculates DNA fragment size and confidence interval from gel electrophoresis data.\n\n    Args:\n        ladder_sizes (list or np.ndarray): Sizes of ladder fragments in base pairs (bp).\n        ladder_dists (list or np.ndarray): Measured migration distances of ladder fragments in mm.\n        d_star (float): Measured migration distance of the target fragment in mm.\n        sigma_d_star (float): Standard deviation of the target fragment's distance measurement in mm.\n        alpha (float): Significance level for the confidence interval (e.g., 0.05 for 95% CI).\n\n    Returns:\n        list: A list containing [s_star, s_low, s_high], all rounded to the nearest integer.\n    \"\"\"\n    s_ladder = np.array(ladder_sizes, dtype=np.float64)\n    d_ladder = np.array(ladder_dists, dtype=np.float64)\n    n = len(s_ladder)\n\n    # Step 1: OLS Parameter Estimation\n    # The predictor variable is x = log10(s)\n    x_ladder = np.log10(s_ladder)\n    \n    # Construct the design matrix X\n    X = np.c_[np.ones(n), x_ladder]\n    \n    # Calculate OLS estimator for theta = [a, b]^T\n    # theta_hat = inv(X^T * X) * X^T * d\n    try:\n        XTX_inv = np.linalg.inv(X.T @ X)\n        theta_hat = XTX_inv @ X.T @ d_ladder\n    except np.linalg.LinAlgError:\n        # This case should not happen with the given test data\n        return [np.nan, np.nan, np.nan]\n        \n    a_hat, b_hat = theta_hat\n\n    # Step 2: Estimate Residual Variance\n    # Degrees of freedom for residuals\n    nu = n - 2\n    if nu <= 0:\n        # Not enough data points for regression\n        return [np.nan, np.nan, np.nan]\n        \n    # Calculate residuals\n    d_predicted = X @ theta_hat\n    residuals = d_ladder - d_predicted\n    \n    # Estimate residual variance (mean squared error)\n    s_e_sq = np.sum(residuals**2) / nu\n\n    # Step 3: Inverse Prediction for the Target Fragment\n    # Point estimate for the log-size x_star\n    x_star_hat = (d_star - a_hat) / b_hat\n\n    # Step 4: Uncertainty Propagation\n    # Calculate terms needed for the variance of x_star_hat\n    x_bar = np.mean(x_ladder)\n    S_xx = np.sum((x_ladder - x_bar)**2)\n\n    # Variance from parameter uncertainty\n    var_from_params = (s_e_sq / b_hat**2) * (1/n + (x_star_hat - x_bar)**2 / S_xx)\n    \n    # Variance from target measurement uncertainty\n    var_from_d_star = (sigma_d_star**2) / b_hat**2\n\n    # Total variance of x_star_hat\n    var_x_star = var_from_params + var_from_d_star\n    \n    # Standard error of x_star_hat\n    se_x_star = np.sqrt(var_x_star)\n\n    # Step 5: Confidence Interval Construction\n    # Find the critical t-value for a (1-alpha) confidence interval\n    t_critical = t.ppf(1 - alpha / 2, df=nu)\n\n    # Margin of error for x_star\n    margin_of_error = t_critical * se_x_star\n\n    # Confidence interval for x_star\n    x_low = x_star_hat - margin_of_error\n    x_high = x_star_hat + margin_of_error\n\n    # Step 6: Transform back to base pairs and round\n    # Point estimate for size s_star\n    s_star_est = 10**x_star_hat\n    \n    # Confidence limits for s_star\n    s_low_est = 10**x_low\n    s_high_est = 10**x_high\n\n    # Round to nearest integer\n    s_star_rounded = int(np.round(s_star_est))\n    s_low_rounded = int(np.round(s_low_est))\n    s_high_rounded = int(np.round(s_high_est))\n\n    return [s_star_rounded, s_low_rounded, s_high_rounded]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"ladder_sizes\": [100, 200, 400, 800, 1500, 3000],\n            \"ladder_dists\": [60.4, 53.3, 48.2, 41.6, 37.1, 30.3],\n            \"d_star\": 45.0,\n            \"sigma_d_star\": 0.5,\n            \"alpha\": 0.05\n        },\n        {\n            \"ladder_sizes\": [500, 1500, 4500],\n            \"ladder_dists\": [46.72, 37.33, 29.94],\n            \"d_star\": 35.0,\n            \"sigma_d_star\": 1.0,\n            \"alpha\": 0.05\n        },\n        {\n            \"ladder_sizes\": [50, 100, 200, 400, 800],\n            \"ladder_dists\": [77.625, 69.8, 63.0, 54.7, 47.5],\n            \"d_star\": 45.0,\n            \"sigma_d_star\": 0.3,\n            \"alpha\": 0.05\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_size_and_ci(\n            case[\"ladder_sizes\"],\n            case[\"ladder_dists\"],\n            case[\"d_star\"],\n            case[\"sigma_d_star\"],\n            case[\"alpha\"]\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, so we need to convert each inner list to its string representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2740371"}, {"introduction": "Reproducibility is a cornerstone of scientific research, making it vital to understand the sources of variation in your measurements. This practice [@problem_id:2740424] introduces a powerful statistical tool, Analysis of Variance (ANOVA), to partition the total observed variability in band size and intensity into contributions from the samples, the gels, and random error. By completing this exercise, you will learn to quantitatively assess the reliability of your workflow and identify the dominant sources of noise, a critical skill for experimental optimization.", "problem": "You are given replicate agarose gel electrophoresis measurements for nucleic acid bands from the same set of samples run across multiple gels. In synthetic biology workflows, apparent band size (in base pairs) and band intensity (in arbitrary units) are affected by both sample-intrinsic factors and gel-to-gel variation. Model the data using a balanced two-way random-effects framework on a log-transformed scale to quantify variance components attributable to gels, samples, and residual effects.\n\nBase assumptions and definitions:\n- The electrophoretic mobility of nucleic acids scales approximately with the logarithm of molecular size. To stabilize variance and linearize effects, apply the natural logarithm to both apparent size and intensity measurements. Let $Y_{g,s}$ denote the log-transformed measurement for gel index $g \\in \\{1,\\dots,G\\}$ and sample index $s \\in \\{1,\\dots,S\\}$.\n- Adopt the additive two-way random-effects model for each response (log-size and log-intensity separately):\n$Y_{g,s} = \\mu + A_g + B_s + \\varepsilon_{g,s},$\nwhere $\\mu$ is a fixed intercept, $A_g \\sim \\mathcal{N}(0,\\sigma_A^2)$ is the random gel effect, $B_s \\sim \\mathcal{N}(0,\\sigma_B^2)$ is the random sample effect, and $\\varepsilon_{g,s} \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$ is the residual (including lane-to-lane and unmodeled interaction).\n- For a balanced design with one observation per cell and $G$ gels by $S$ samples, the two-way analysis-of-variance decomposition yields sums of squares:\n  - Let $\\bar{Y}_{g\\cdot}$ be the mean over samples on gel $g$, $\\bar{Y}_{\\cdot s}$ be the mean over gels for sample $s$, and $\\bar{Y}_{\\cdot\\cdot}$ be the grand mean.\n  - Define\n    $$SS_A = S \\sum_{g=1}^{G} \\left(\\bar{Y}_{g\\cdot} - \\bar{Y}_{\\cdot\\cdot}\\right)^2,\\quad SS_B = G \\sum_{s=1}^{S} \\left(\\bar{Y}_{\\cdot s} - \\bar{Y}_{\\cdot\\cdot}\\right)^2,$$\n    $$SS_E = \\sum_{g=1}^{G}\\sum_{s=1}^{S} \\left(Y_{g,s} - \\bar{Y}_{g\\cdot} - \\bar{Y}_{\\cdot s} + \\bar{Y}_{\\cdot\\cdot}\\right)^2.$$\n  - Degrees of freedom are $df_A = G-1$, $df_B = S-1$, $df_E = (G-1)(S-1)$, with mean squares $MS_A = SS_A/df_A$, $MS_B = SS_B/df_B$, $MS_E = SS_E/df_E$.\n- The expected mean squares under the model are\n  $$\\mathbb{E}[MS_A] = \\sigma_\\varepsilon^2 + S \\sigma_A^2,\\quad \\mathbb{E}[MS_B] = \\sigma_\\varepsilon^2 + G \\sigma_B^2,\\quad \\mathbb{E}[MS_E] = \\sigma_\\varepsilon^2.$$\n- Use method-of-moments by equating observed mean squares to their expectations to obtain variance component estimates. Enforce non-negativity by truncation at zero: if any estimator is negative, set it to zero.\n\nTasks:\n- For each response (log-size and log-intensity) and each test case below, compute the variance components $\\hat{\\sigma}_A^2$ (inter-gel variability), $\\hat{\\sigma}_B^2$ (inter-sample variability), and $\\hat{\\sigma}_\\varepsilon^2$ (residual).\n- Identify the dominant source of variance as the index of the largest among $\\{\\hat{\\sigma}_A^2,\\hat{\\sigma}_B^2,\\hat{\\sigma}_\\varepsilon^2\\}$, encoded as an integer $0$ for gel, $1$ for sample, and $2$ for residual.\n\nInput data and units:\n- Apparent sizes are in base pairs (bp) and intensities are in arbitrary units (a.u.). Apply the natural logarithm to both before analysis. No other unit conversions are needed, and final numeric answers are variances on the log scale (dimensionless).\n\nTest suite:\n- Case $1$ ($G=3$, $S=4$). Sizes (bp) by gels (rows) and samples (columns):\n  $$\\begin{bmatrix}\n  505 & 748 & 1007 & 1490\\\\\n  495 & 760 & 990 & 1510\\\\\n  500 & 752 & 1003 & 1498\n  \\end{bmatrix}$$\n  Intensities (a.u.) by gels (rows) and samples (columns):\n  $$\\begin{bmatrix}\n  1.20 & 0.85 & 1.50 & 1.10\\\\\n  1.10 & 0.80 & 1.55 & 1.05\\\\\n  1.25 & 0.90 & 1.45 & 1.00\n  \\end{bmatrix}$$\n- Case $2$ ($G=3$, $S=4$). Sizes (bp):\n  $$\\begin{bmatrix}\n  1001 & 1002 & 1003 & 1004\\\\\n  1000 & 1001 & 1002 & 1003\\\\\n  1002 & 1003 & 1004 & 1005\n  \\end{bmatrix}$$\n  Intensities (a.u.):\n  $$\\begin{bmatrix}\n  0.50 & 0.60 & 0.70 & 0.80\\\\\n  0.51 & 0.61 & 0.71 & 0.81\\\\\n  0.49 & 0.59 & 0.69 & 0.79\n  \\end{bmatrix}$$\n- Case $3$ ($G=3$, $S=4$). Sizes (bp):\n  $$\\begin{bmatrix}\n  800 & 805 & 795 & 798\\\\\n  820 & 825 & 815 & 818\\\\\n  780 & 785 & 775 & 778\n  \\end{bmatrix}$$\n  Intensities (a.u.):\n  $$\\begin{bmatrix}\n  0.80 & 0.82 & 0.79 & 0.81\\\\\n  1.60 & 1.62 & 1.58 & 1.61\\\\\n  0.40 & 0.42 & 0.39 & 0.41\n  \\end{bmatrix}$$\n\nOutput specification:\n- For each test case, produce a list containing, in order for log-size then log-intensity: $[\\hat{\\sigma}_A^2,\\hat{\\sigma}_B^2,\\hat{\\sigma}_\\varepsilon^2,\\text{dominant\\_index}]$. Concatenate the two responses so that each case yields a list of length $8$ as $[\\hat{\\sigma}_{A,\\text{size}}^2,\\hat{\\sigma}_{B,\\text{size}}^2,\\hat{\\sigma}_{\\varepsilon,\\text{size}}^2,\\text{dom}_{\\text{size}},\\hat{\\sigma}_{A,\\text{int}}^2,\\hat{\\sigma}_{B,\\text{int}}^2,\\hat{\\sigma}_{\\varepsilon,\\text{int}}^2,\\text{dom}_{\\text{int}}]$.\n- Round all variance components to $6$ decimal places. Dominant indices must be integers as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$), where each $resultk$ is the Python-style list for case $k$.\n\nNo user input is required; hard-code the test suite as provided and compute the outputs accordingly.", "solution": "The problem statement is critically validated. All givens are extracted and examined for scientific soundness, completeness, and clarity.\n\n### Step 1: Extract Givens\n- **Model**: A balanced two-way random-effects model, $Y_{g,s} = \\mu + A_g + B_s + \\varepsilon_{g,s}$, where $g \\in \\{1,\\dots,G\\}$ and $s \\in \\{1,\\dots,S\\}$.\n- **Response Variable**: $Y_{g,s}$ is the natural logarithm of the measured data (apparent size or intensity).\n- **Random Effects**: Gel effect $A_g \\sim \\mathcal{N}(0,\\sigma_A^2)$, sample effect $B_s \\sim \\mathcal{N}(0,\\sigma_B^2)$, and residual effect $\\varepsilon_{g,s} \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$.\n- **Analysis of Variance (ANOVA) Formulae**:\n  - Sum of Squares for Gels ($A$): $SS_A = S \\sum_{g=1}^{G} (\\bar{Y}_{g\\cdot} - \\bar{Y}_{\\cdot\\cdot})^2$.\n  - Sum of Squares for Samples ($B$): $SS_B = G \\sum_{s=1}^{S} (\\bar{Y}_{\\cdot s} - \\bar{Y}_{\\cdot\\cdot})^2$.\n  - Sum of Squares for Residuals ($E$): $SS_E = \\sum_{g=1}^{G}\\sum_{s=1}^{S} (Y_{g,s} - \\bar{Y}_{g\\cdot} - \\bar{Y}_{\\cdot s} + \\bar{Y}_{\\cdot\\cdot})^2$.\n  - Degrees of Freedom: $df_A = G-1$, $df_B = S-1$, $df_E = (G-1)(S-1)$.\n  - Mean Squares: $MS_A = SS_A/df_A$, $MS_B = SS_B/df_B$, $MS_E = SS_E/df_E$.\n- **Estimation Method**: Method-of-moments estimators for variance components derived from expected mean squares:\n  - $\\mathbb{E}[MS_A] = \\sigma_\\varepsilon^2 + S \\sigma_A^2 \\implies \\hat{\\sigma}_A^2 = (MS_A - MS_E)/S$.\n  - $\\mathbb{E}[MS_B] = \\sigma_\\varepsilon^2 + G \\sigma_B^2 \\implies \\hat{\\sigma}_B^2 = (MS_B - MS_E)/G$.\n  - $\\mathbb{E}[MS_E] = \\sigma_\\varepsilon^2 \\implies \\hat{\\sigma}_\\varepsilon^2 = MS_E$.\n- **Constraint**: Non-negativity is enforced by setting any negative variance estimate to $0$.\n- **Task**: For each test case and each response (size and intensity), compute $\\{\\hat{\\sigma}_A^2, \\hat{\\sigma}_B^2, \\hat{\\sigma}_\\varepsilon^2\\}$ and identify the dominant variance component (index $0$ for gel, $1$ for sample, $2$ for residual).\n- **Data**: Three test cases with $G=3$, $S=4$, providing matrices for size (bp) and intensity (a.u.).\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem applies a standard statistical model (two-way random-effects ANOVA) to a common experimental scenario in molecular biology (analysis of gel electrophoresis data). The use of log-transformation for size and intensity is a valid and frequent practice for variance stabilization and linearization. The model, ANOVA decomposition, and method-of-moments estimation are all fundamental concepts in statistics. It is scientifically sound.\n- **Well-Posed**: The problem provides a complete set of data and a deterministic algorithm for computing the desired results. For each test case, a unique solution exists.\n- **Objective**: The language is precise and devoid of subjectivity. All terms are clearly defined.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined computational task based on established principles of statistics and its application in biology. The solution is executed as follows.\n\n### Solution Methodology\nThe task is to perform a variance components analysis for a two-way random-effects model. The procedure is identical for both log-transformed apparent size and log-transformed intensity data. For each data matrix, the following steps are executed:\n\n1.  **Data Transformation**: Given a data matrix $D$ of size $G \\times S$, the analysis is performed on the log-transformed matrix $Y$, where each element is $Y_{g,s} = \\ln(D_{g,s})$.\n\n2.  **Calculation of Means**: We compute the mean of $Y$ over different dimensions:\n    - The grand mean, $\\bar{Y}_{\\cdot\\cdot} = \\frac{1}{GS} \\sum_{g=1}^{G} \\sum_{s=1}^{S} Y_{g,s}$.\n    - The means for each gel (row means), $\\bar{Y}_{g\\cdot} = \\frac{1}{S} \\sum_{s=1}^{S} Y_{g,s}$ for $g=1, \\dots, G$.\n    - The means for each sample (column means), $\\bar{Y}_{\\cdot s} = \\frac{1}{G} \\sum_{g=1}^{G} Y_{g,s}$ for $s=1, \\dots, S$.\n\n3.  **Calculation of Sums of Squares (SS)**: Using the calculated means, we obtain the sums of squares for the two factors (gels and samples) and the residual error, according to the provided formulae:\n    - $SS_A = S \\sum_{g=1}^{G} (\\bar{Y}_{g\\cdot} - \\bar{Y}_{\\cdot\\cdot})^2$\n    - $SS_B = G \\sum_{s=1}^{S} (\\bar{Y}_{\\cdot s} - \\bar{Y}_{\\cdot\\cdot})^2$\n    - $SS_E = \\sum_{g=1}^{G}\\sum_{s=1}^{S} (Y_{g,s} - \\bar{Y}_{g\\cdot} - \\bar{Y}_{\\cdot s} + \\bar{Y}_{\\cdot\\cdot})^2$\n\n4.  **Calculation of Mean Squares (MS)**: The sums of squares are converted to mean squares by dividing by their respective degrees of freedom:\n    - $df_A = G-1$, $df_B = S-1$, $df_E = (G-1)(S-1)$.\n    - $MS_A = SS_A / df_A$.\n    - $MS_B = SS_B / df_B$.\n    - $MS_E = SS_E / df_E$.\n\n5.  **Estimation of Variance Components**: The variance components are estimated by solving the system of equations derived from the expected mean squares. The non-negativity constraint is applied.\n    - $\\hat{\\sigma}_\\varepsilon^2 = MS_E$.\n    - $\\hat{\\sigma}_A^2 = \\max(0, (MS_A - MS_E)/S)$.\n    - $\\hat{\\sigma}_B^2 = \\max(0, (MS_B - MS_E)/G)$.\n\n6.  **Identification of Dominant Variance**: The dominant source of variance is determined by finding the maximum value among the estimated components $\\{\\hat{\\sigma}_A^2, \\hat{\\sigma}_B^2, \\hat{\\sigma}_\\varepsilon^2\\}$. The result is encoded as an integer index: $0$ for a dominant gel effect ($\\hat{\\sigma}_A^2$), $1$ for a dominant sample effect ($\\hat{\\sigma}_B^2$), and $2$ for a dominant residual effect ($\\hat{\\sigma}_\\varepsilon^2$).\n\nThis procedure is implemented and applied to all test cases specified in the problem statement.", "answer": "```python\nimport numpy as np\n\ndef analyze_variance(data_matrix):\n    \"\"\"\n    Performs a two-way random-effects ANOVA to estimate variance components.\n\n    Args:\n        data_matrix (np.ndarray): A GxS matrix of measurements, where G is the\n                                  number of gels and S is the number of samples.\n\n    Returns:\n        tuple: A tuple containing (sigma_A_sq, sigma_B_sq, sigma_eps_sq, dominant_idx),\n               representing gel variance, sample variance, residual variance, and the\n               index of the dominant variance component.\n    \"\"\"\n    # 1. Data Transformation\n    Y = np.log(data_matrix)\n    G, S = Y.shape\n\n    # 2. Calculation of Means\n    grand_mean = np.mean(Y)\n    gel_means = np.mean(Y, axis=1)\n    sample_means = np.mean(Y, axis=0)\n\n    # 3. Calculation of Sums of Squares (SS)\n    ss_a = S * np.sum((gel_means - grand_mean)**2)\n    ss_b = G * np.sum((sample_means - grand_mean)**2)\n    \n    # Calculate residuals for SS_E\n    residuals = Y - gel_means[:, np.newaxis] - sample_means[np.newaxis, :] + grand_mean\n    ss_e = np.sum(residuals**2)\n\n    # 4. Calculation of Mean Squares (MS)\n    df_a = G - 1\n    df_b = S - 1\n    df_e = (G - 1) * (S - 1)\n\n    ms_a = ss_a / df_a if df_a > 0 else 0\n    ms_b = ss_b / df_b if df_b > 0 else 0\n    ms_e = ss_e / df_e if df_e > 0 else 0\n\n    # 5. Estimation of Variance Components\n    sigma_eps_sq = ms_e\n    sigma_a_sq = max(0, (ms_a - ms_e) / S)\n    sigma_b_sq = max(0, (ms_b - ms_e) / G)\n\n    # 6. Identification of Dominant Variance\n    variances = [sigma_a_sq, sigma_b_sq, sigma_eps_sq]\n    dominant_idx = np.argmax(variances)\n\n    return sigma_a_sq, sigma_b_sq, sigma_eps_sq, dominant_idx\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"sizes\": np.array([\n                [505, 748, 1007, 1490],\n                [495, 760, 990, 1510],\n                [500, 752, 1003, 1498]\n            ]),\n            \"intensities\": np.array([\n                [1.20, 0.85, 1.50, 1.10],\n                [1.10, 0.80, 1.55, 1.05],\n                [1.25, 0.90, 1.45, 1.00]\n            ])\n        },\n        # Case 2\n        {\n            \"sizes\": np.array([\n                [1001, 1002, 1003, 1004],\n                [1000, 1001, 1002, 1003],\n                [1002, 1003, 1004, 1005]\n            ]),\n            \"intensities\": np.array([\n                [0.50, 0.60, 0.70, 0.80],\n                [0.51, 0.61, 0.71, 0.81],\n                [0.49, 0.59, 0.69, 0.79]\n            ])\n        },\n        # Case 3\n        {\n            \"sizes\": np.array([\n                [800, 805, 795, 798],\n                [820, 825, 815, 818],\n                [780, 785, 775, 778]\n            ]),\n            \"intensities\": np.array([\n                [0.80, 0.82, 0.79, 0.81],\n                [1.60, 1.62, 1.58, 1.61],\n                [0.40, 0.42, 0.39, 0.41]\n            ])\n        }\n    ]\n\n    all_case_results = []\n    for case in test_cases:\n        size_results = analyze_variance(case[\"sizes\"])\n        intensity_results = analyze_variance(case[\"intensities\"])\n        \n        # Combine results for the case\n        combined_results = size_results + intensity_results\n        \n        # Format the list of 8 results into the required string format\n        formatted_list = [\n            f\"{combined_results[0]:.6f}\", f\"{combined_results[1]:.6f}\", f\"{combined_results[2]:.6f}\", str(combined_results[3]),\n            f\"{combined_results[4]:.6f}\", f\"{combined_results[5]:.6f}\", f\"{combined_results[6]:.6f}\", str(combined_results[7])\n        ]\n        \n        case_result_str = \"[\" + \",\".join(formatted_list) + \"]\"\n        all_case_results.append(case_result_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_case_results)}]\")\n\nsolve()\n```", "id": "2740424"}, {"introduction": "Moving from analyzing data to designing optimal experiments is a hallmark of an expert researcher, especially when trying to resolve multiple DNA fragments of similar sizes. This exercise [@problem_id:2740396] challenges you to build a computational tool that uses biophysical models of DNA mobility and diffusion to find the best experimental parameters. You will perform an *in silico* optimization across gel concentration ($C$), electric field ($E$), and run time ($t$) to maximize band resolution, learning how to rationally design experiments before stepping into the lab.", "problem": "Design a program that, given sets of Polymerase Chain Reaction (PCR) amplicon lengths with overlapping size ranges, chooses electrophoretic separation parameters to maximize resolvability using a principled physical model of nucleic acid migration and band broadening in a sieving matrix. Your program must search over discrete grids of gel concentration, electric field, and run time to identify the parameter combination that maximizes a well-defined objective that reflects the worst-case pairwise resolvability among adjacent bands on the lane. No operational procedures are to be considered; the task is purely computational and model-based.\n\nStart from the following foundational base, which is widely supported by experiment and theory for double-stranded deoxyribonucleic acid (DNA) under typical gel electrophoresis conditions:\n\n- In free solution, the electrophoretic mobility of long double-stranded DNA is approximately size-independent due to a near-constant charge-to-friction ratio; denote the free-solution mobility by $\\mu_{\\mathrm{fs}}$ with units $\\mathrm{cm^2 \\, V^{-1} \\, s^{-1}}$.\n- In a polymeric sieving matrix such as agarose, the Ferguson relation states that for a fixed fragment size, $\\log \\mu$ decreases approximately linearly with gel concentration. A size-dependent retardation coefficient captures the increased sieving of larger fragments.\n- The migration distance after time $t$ under a uniform electric field $E$ is $x = v \\, t$, where $v = \\mu E$ and $\\mu$ is the size- and gel-dependent mobility. The unit of $x$ is chosen to be $\\mathrm{mm}$.\n- Band broadening arises from at least two sources: initial injection width and diffusion. Model the band variance as the sum of the variance associated with the initial full width at half maximum, plus the diffusive variance accumulated over time, plus a small instrument variance floor. The diffusion coefficient $D$ is a decreasing function of fragment size and gel concentration. The diffusive variance contribution over time $t$ is $2 D t$ in units of $\\mathrm{length^2}$.\n\nImpose the following concrete model choices to make the problem fully specified and testable:\n\n- Use the mobility model\n$$\n\\mu(L,C) \\;=\\; \\mu_{\\mathrm{fs}} \\, \\exp\\!\\left(-\\kappa \\, C \\,\\left(\\frac{L}{L_0}\\right)^{\\beta}\\right)\n$$,\nwhere $L$ is the fragment length in base pairs, $C$ is the agarose gel concentration expressed as a percent weight per volume, $\\mu_{\\mathrm{fs}} = 3.0 \\times 10^{-4} \\;\\mathrm{cm^2 \\, V^{-1} \\, s^{-1}}$, $\\kappa = 1.2 \\;\\mathrm{percent^{-1}}$, $L_0 = 1000 \\;\\mathrm{bp}$, and $\\beta = 0.45$.\n- Use the diffusion model\n$$\nD(L,C) \\;=\\; \\frac{D_0}{1 + \\left(\\frac{L}{L_d}\\right)^{\\delta}} \\,\\exp(-d_c \\, C)\n$$,\nwith $D_0 = 1.5 \\times 10^{-7} \\;\\mathrm{cm^2 \\, s^{-1}}$, $L_d = 100 \\;\\mathrm{bp}$, $\\delta = 0.6$, and $d_c = 0.3 \\;\\mathrm{percent^{-1}}$.\n- Convert positions from $\\mathrm{cm}$ to $\\mathrm{mm}$ by multiplying by $10$. Convert variances from $\\mathrm{cm^2}$ to $\\mathrm{mm^2}$ by multiplying by $100$.\n- Let the initial full width at half maximum be $w_0$ in $\\mathrm{mm}$, and use a Gaussian-equivalent standard deviation $\\sigma_0 = \\dfrac{w_0}{2 \\sqrt{2 \\ln 2}}$. Let the instrument variance floor be $\\sigma_{\\mathrm{inst}}^2$ with $\\sigma_{\\mathrm{inst}}$ in $\\mathrm{mm}$.\n- For a given parameter triple $(C,E,t)$, compute the position of band $i$ as\n$$\nx_i \\;=\\; 10 \\,\\mu(L_i,C)\\, E \\, t\n$$,\nin $\\mathrm{mm}$, and the variance as\n$$\n\\sigma_i^2 \\;=\\; \\sigma_0^2 \\;+\\; 2 D(L_i,C) \\, t \\times 100 \\;+\\; \\sigma_{\\mathrm{inst}}^2\n$$,\nin $\\mathrm{mm^2}$.\n- Define the pairwise resolvability between adjacent bands $i$ and $j$ (adjacent in the sorted order by $x$) as\n$$\nR_{ij} \\;=\\; \\frac{|x_i - x_j|}{2 \\sqrt{\\sigma_i^2 + \\sigma_j^2}}.\n$$\n- For each parameter triple $(C,E,t)$, define the objective to maximize lexicographically as follows. Let $A$ be the minimum of $R_{ij}$ over all adjacent pairs on the lane. Let $B$ be the arithmetic mean of these $R_{ij}$. Maximize $A$; if two parameter triples have equal $A$ within a tolerance of $10^{-9}$, select the one with larger $B$; if still tied, select the one with smaller $C$, then smaller $E$, then smaller $t$.\n- Enforce geometric validity constraints. Let $x_{\\min}$ and $x_{\\max}$ be the smallest and largest band positions. The separation is valid only if $x_{\\min} \\ge m_{\\mathrm{start}}$ and $x_{\\max} \\le L_{\\mathrm{gel}} - m_{\\mathrm{end}}$, where $m_{\\mathrm{start}}$ and $m_{\\mathrm{end}}$ are margins in $\\mathrm{mm}$ and $L_{\\mathrm{gel}}$ is the lane length in $\\mathrm{mm}$. If invalid, discard the parameter triple from consideration.\n\nYour program must evaluate a provided test suite of cases. For each case, the inputs are the list of fragment lengths, the discrete grids for $C$ in percent, $E$ in $\\mathrm{V/cm}$, and $t$ in $\\mathrm{s}$, the lane length $L_{\\mathrm{gel}}$ in $\\mathrm{mm}$, the start and end margins $m_{\\mathrm{start}}$ and $m_{\\mathrm{end}}$ in $\\mathrm{mm}$, and the injection parameters $w_0$ and $\\sigma_{\\mathrm{inst}}$ in $\\mathrm{mm}$. The temperature is fixed at $T = 298 \\;\\mathrm{K}$ and does not explicitly appear in the model. For each case, your program must output the selected optimal tuple $(C^\\star, E^\\star, t^\\star, A^\\star)$, where $A^\\star$ is the optimal minimum adjacent resolvability, as a list formatted as $[C^\\star, E^\\star, t^\\star, A^\\star]$. Then aggregate the lists for all cases into a single line.\n\nExpress $C^\\star$ as a decimal number in percent without the percent sign, $E^\\star$ in $\\mathrm{V/cm}$, $t^\\star$ in $\\mathrm{s}$, and $A^\\star$ as a dimensionless number. Round $C^\\star$ and $E^\\star$ to three decimal places, output $t^\\star$ as an integer, and round $A^\\star$ to three decimal places.\n\nTest suite to implement:\n\n- Case $1$:\n    - Fragment lengths (in base pairs): $[320, 345, 410, 500, 620, 710]$.\n    - Gel concentration grid $C$ (percent): $[0.8, 1.2, 1.6, 2.0]$.\n    - Field grid $E$ (in $\\mathrm{V/cm}$): $[6.0, 8.0, 10.0]$.\n    - Time grid $t$ (in $\\mathrm{s}$): $[2400, 3600, 4800]$.\n    - Lane length $L_{\\mathrm{gel}}$ (in $\\mathrm{mm}$): $60$.\n    - Margins $m_{\\mathrm{start}} = 5$ and $m_{\\mathrm{end}} = 5$ (in $\\mathrm{mm}$).\n    - Injection full width at half maximum $w_0 = 1.0$ and instrument standard deviation $\\sigma_{\\mathrm{inst}} = 0.05$ (in $\\mathrm{mm}$).\n- Case $2$:\n    - Fragment lengths (in base pairs): $[100, 180, 250, 400, 700, 1200, 1800]$.\n    - Gel concentration grid $C$ (percent): $[0.5, 1.0, 1.5, 2.0, 2.5]$.\n    - Field grid $E$ (in $\\mathrm{V/cm}$): $[5.0, 7.0, 9.0]$.\n    - Time grid $t$ (in $\\mathrm{s}$): $[3000, 4200, 5400]$.\n    - Lane length $L_{\\mathrm{gel}}$ (in $\\mathrm{mm}$): $70$.\n    - Margins $m_{\\mathrm{start}} = 5$ and $m_{\\mathrm{end}} = 5$ (in $\\mathrm{mm}$).\n    - Injection full width at half maximum $w_0 = 0.8$ and instrument standard deviation $\\sigma_{\\mathrm{inst}} = 0.05$ (in $\\mathrm{mm}$).\n- Case $3$:\n    - Fragment lengths (in base pairs): $[500, 510, 520, 600]$.\n    - Gel concentration grid $C$ (percent): $[0.6, 0.9, 1.2, 1.5, 1.8, 2.1]$.\n    - Field grid $E$ (in $\\mathrm{V/cm}$): $[6.0, 7.0, 8.0, 9.0, 10.0]$.\n    - Time grid $t$ (in $\\mathrm{s}$): $[2700, 3300, 3900, 4500]$.\n    - Lane length $L_{\\mathrm{gel}}$ (in $\\mathrm{mm}$): $60$.\n    - Margins $m_{\\mathrm{start}} = 5$ and $m_{\\mathrm{end}} = 5$ (in $\\mathrm{mm}$).\n    - Injection full width at half maximum $w_0 = 0.6$ and instrument standard deviation $\\sigma_{\\mathrm{inst}} = 0.05$ (in $\\mathrm{mm}$).\n- Case $4$:\n    - Fragment lengths (in base pairs): $[80, 90, 100, 3000]$.\n    - Gel concentration grid $C$ (percent): $[0.8, 1.3, 1.8, 2.3]$.\n    - Field grid $E$ (in $\\mathrm{V/cm}$): $[4.0, 6.0, 8.0]$.\n    - Time grid $t$ (in $\\mathrm{s}$): $[3600, 5400, 7200]$.\n    - Lane length $L_{\\mathrm{gel}}$ (in $\\mathrm{mm}$): $80$.\n    - Margins $m_{\\mathrm{start}} = 5$ and $m_{\\mathrm{end}} = 5$ (in $\\mathrm{mm}$).\n    - Injection full width at half maximum $w_0 = 0.9$ and instrument standard deviation $\\sigma_{\\mathrm{inst}} = 0.05$ (in $\\mathrm{mm}$).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each case’s result formatted as a list $[C^\\star,E^\\star,t^\\star,A^\\star]$. For example, a valid output with two hypothetical cases would be formatted as $[[0.800,6.000,3600,1.234],[1.600,8.000,4800,0.987]]$.", "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically grounded, well-posed, objective, and internally consistent, presenting a solvable computational optimization task based on established physical models of DNA gel electrophoresis. We shall therefore proceed with a complete, principled solution.\n\nThe problem requires designing an algorithm to determine the optimal electrophoretic separation parameters $(C, E, t)$—gel concentration, electric field strength, and run time, respectively—that maximize the resolvability of a given set of deoxyribonucleic acid (DNA) fragments. The optimization is performed via a grid search over discrete sets of these parameters. The solution is based on a quantitative physical model of DNA migration and band broadening.\n\nThe algorithmic procedure for each test case is as follows:\n\n1.  **System Initialization**:\n    For each test case, we are given a set of DNA fragment lengths $\\{L_i\\}$, discrete grids for $C$, $E$, and $t$, and experimental constants: lane length $L_{\\mathrm{gel}}$, margins $m_{\\mathrm{start}}$ and $m_{\\mathrm{end}}$, initial band width $w_0$, and instrument noise standard deviation $\\sigma_{\\mathrm{inst}}$.\n    The initial variance contribution from the injection width $w_0$ (in $\\mathrm{mm}$) is constant for all bands in a given experiment. Assuming a Gaussian band profile, the initial band standard deviation $\\sigma_0$ is given by $\\sigma_0 = \\frac{w_0}{2 \\sqrt{2 \\ln 2}}$, and its variance is $\\sigma_0^2$. The instrument variance floor is given as $\\sigma_{\\mathrm{inst}}^2$.\n\n2.  **Parameter Grid Search**:\n    The core of the algorithm is a systematic search over every possible combination of parameters $(C, E, t)$ from the provided discrete grids. For each parameter triple, we evaluate its effectiveness.\n\n3.  **Physical Model Evaluation**:\n    For a given parameter triple $(C, E, t)$ and for each DNA fragment of length $L_i$ (in base pairs), we compute its final position $x_i$ and band variance $\\sigma_i^2$.\n\n    a.  **Electrophoretic Mobility**: The mobility $\\mu(L_i, C)$ in units of $\\mathrm{cm^2 \\, V^{-1} \\, s^{-1}}$ is calculated using the specified relation:\n        $$\n        \\mu(L_i,C) = \\mu_{\\mathrm{fs}} \\, \\exp\\!\\left(-\\kappa \\, C \\,\\left(\\frac{L_i}{L_0}\\right)^{\\beta}\\right)\n        $$\n        where $\\mu_{\\mathrm{fs}} = 3.0 \\times 10^{-4} \\;\\mathrm{cm^2 \\, V^{-1} \\, s^{-1}}$, $\\kappa = 1.2 \\;\\mathrm{percent^{-1}}$, $L_0 = 1000 \\;\\mathrm{bp}$, and $\\beta = 0.45$.\n\n    b.  **Migration Distance**: The migration distance $x_i$ in $\\mathrm{mm}$ is calculated from the mobility $\\mu(L_i, C)$, electric field $E$ (in $\\mathrm{V/cm}$), and time $t$ (in $\\mathrm{s}$):\n        $$\n        x_i = 10 \\cdot \\mu(L_i,C) \\cdot E \\cdot t\n        $$\n        The factor of $10$ converts the result from $\\mathrm{cm}$ to $\\mathrm{mm}$.\n\n    c.  **Diffusion Coefficient**: The diffusion coefficient $D(L_i, C)$ in units of $\\mathrm{cm^2 \\, s^{-1}}$ is calculated as:\n        $$\n        D(L_i,C) = \\frac{D_0}{1 + \\left(\\frac{L_i}{L_d}\\right)^{\\delta}} \\exp(-d_c \\, C)\n        $$\n        with $D_0 = 1.5 \\times 10^{-7} \\;\\mathrm{cm^2 \\, s^{-1}}$, $L_d = 100 \\;\\mathrm{bp}$, $\\delta = 0.6$, and $d_c = 0.3 \\;\\mathrm{percent^{-1}}$.\n\n    d.  **Band Variance**: The total variance $\\sigma_i^2$ of the band in $\\mathrm{mm^2}$ is the sum of the initial variance, the diffusive variance, and the instrument variance:\n        $$\n        \\sigma_i^2 = \\sigma_0^2 + 200 \\cdot D(L_i,C) \\cdot t + \\sigma_{\\mathrm{inst}}^2\n        $$\n        The factor of $200$ arises from the diffusive variance term $2 D t$ (in $\\mathrm{cm^2}$) being converted to $\\mathrm{mm^2}$ by multiplying by $100$.\n\n4.  **Constraint Validation and Objective Calculation**:\n    a.  **Geometric Constraints**: After computing positions $\\{x_i\\}$ for all fragments, we determine the minimum and maximum positions, $x_{\\min} = \\min(\\{x_i\\})$ and $x_{\\max} = \\max(\\{x_i\\})$. The parameter triple $(C, E, t)$ is considered valid only if the geometric constraints are met:\n        $$\n        x_{\\min} \\ge m_{\\mathrm{start}} \\quad \\text{and} \\quad x_{\\max} \\le L_{\\mathrm{gel}} - m_{\\mathrm{end}}\n        $$\n        If a triple is invalid, it is discarded, and the algorithm proceeds to the next triple.\n\n    b.  **Resolvability Calculation**: For a valid triple, we evaluate the separation quality. The fragments are first sorted according to their migration distance $x_i$. The pairwise resolvability $R_{ij}$ between each pair of adjacent bands $(i, j)$ in the sorted list is calculated:\n        $$\n        R_{ij} = \\frac{|x_i - x_j|}{2 \\sqrt{\\sigma_i^2 + \\sigma_j^2}}\n        $$\n\n    c.  **Objective Function**: From the set of all adjacent-pair resolvabilities $\\{R_{ij}\\}$, we compute the primary and secondary objective metrics:\n        -   Primary Objective $A$: The minimum resolvability, $A = \\min(\\{R_{ij}\\})$. This represents the worst-case separation on the lane.\n        -   Secondary Objective $B$: The arithmetic mean of the resolvabilities, $B = \\mathrm{mean}(\\{R_{ij}\\})$.\n\n5.  **Lexicographical Optimization**:\n    The algorithm maintains the best-found parameter set $(C^\\star, E^\\star, t^\\star)$ and its corresponding objective values $(A^\\star, B^\\star)$. For each new valid parameter set $(C, E, t)$ with objectives $(A, B)$, a lexicographical comparison is performed to update the optimal solution:\n    1.  The new set is preferred if $A > A^\\star + 10^{-9}$.\n    2.  If $|A - A^\\star| \\le 10^{-9}$, the new set is preferred if $B > B^\\star$.\n    3.  If they are also tied on $B$, the new set is preferred if $C < C^\\star$.\n    4.  If they are also tied on $C$, the new set is preferred if $E < E^\\star$.\n    5.  If they are also tied on $E$, the new set is preferred if $t < t^\\star$.\n\n    This process guarantees the selection of a unique optimal parameter set from the grid.\n\n6.  **Output Generation**:\n    After the entire grid has been searched, the optimal parameters $(C^\\star, E^\\star, t^\\star)$ and the corresponding best minimum resolvability $A^\\star$ are stored. This procedure is repeated for all test cases. The final results are formatted to the specified precision ($3$ decimal places for $C^\\star$, $E^\\star$, and $A^\\star$; integer for $t^\\star$) and compiled into a single output string as required.", "answer": "```python\nimport numpy as np\n\n# Define physical and model constants as per the problem statement.\nMU_FS = 3.0e-4  # cm^2 V^-1 s^-1\nKAPPA = 1.2     # percent^-1\nL0 = 1000.0     # bp\nBETA = 0.45\n\nD0 = 1.5e-7     # cm^2 s^-1\nLD = 100.0      # bp\nDELTA = 0.6\nDC = 0.3        # percent^-1\nTOLERANCE = 1e-9\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the validation and solving process for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"lengths\": [320, 345, 410, 500, 620, 710],\n            \"C_grid\": [0.8, 1.2, 1.6, 2.0],\n            \"E_grid\": [6.0, 8.0, 10.0],\n            \"t_grid\": [2400, 3600, 4800],\n            \"L_gel\": 60.0,\n            \"m_start\": 5.0,\n            \"m_end\": 5.0,\n            \"w0\": 1.0,\n            \"sigma_inst\": 0.05\n        },\n        {\n            \"lengths\": [100, 180, 250, 400, 700, 1200, 1800],\n            \"C_grid\": [0.5, 1.0, 1.5, 2.0, 2.5],\n            \"E_grid\": [5.0, 7.0, 9.0],\n            \"t_grid\": [3000, 4200, 5400],\n            \"L_gel\": 70.0,\n            \"m_start\": 5.0,\n            \"m_end\": 5.0,\n            \"w0\": 0.8,\n            \"sigma_inst\": 0.05\n        },\n        {\n            \"lengths\": [500, 510, 520, 600],\n            \"C_grid\": [0.6, 0.9, 1.2, 1.5, 1.8, 2.1],\n            \"E_grid\": [6.0, 7.0, 8.0, 9.0, 10.0],\n            \"t_grid\": [2700, 3300, 3900, 4500],\n            \"L_gel\": 60.0,\n            \"m_start\": 5.0,\n            \"m_end\": 5.0,\n            \"w0\": 0.6,\n            \"sigma_inst\": 0.05\n        },\n        {\n            \"lengths\": [80, 90, 100, 3000],\n            \"C_grid\": [0.8, 1.3, 1.8, 2.3],\n            \"E_grid\": [4.0, 6.0, 8.0],\n            \"t_grid\": [3600, 5400, 7200],\n            \"L_gel\": 80.0,\n            \"m_start\": 5.0,\n            \"m_end\": 5.0,\n            \"w0\": 0.9,\n            \"sigma_inst\": 0.05\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        best_solution = {\n            \"params\": None,\n            \"A\": -1.0,\n            \"B\": -1.0\n        }\n\n        # Pre-calculate constant variance terms\n        sigma0_sq = (case[\"w0\"] / (2 * np.sqrt(2 * np.log(2))))**2\n        sigma_inst_sq = case[\"sigma_inst\"]**2\n        \n        # Grid search over all parameter combinations\n        for C in case[\"C_grid\"]:\n            for E in case[\"E_grid\"]:\n                for t in case[\"t_grid\"]:\n                    \n                    bands_data = []\n                    for L in case[\"lengths\"]:\n                        # Calculate mobility\n                        mu = MU_FS * np.exp(-KAPPA * C * (L / L0)**BETA)\n                        \n                        # Calculate diffusion coefficient\n                        D = (D0 / (1 + (L / LD)**DELTA)) * np.exp(-DC * C)\n                        \n                        # Calculate position in mm\n                        x = 10.0 * mu * E * t\n                        \n                        # Calculate variance in mm^2\n                        sigma_sq = sigma0_sq + 200.0 * D * t + sigma_inst_sq\n\n                        bands_data.append({\"x\": x, \"var\": sigma_sq})\n                    \n                    positions = [b[\"x\"] for b in bands_data]\n                    x_min, x_max = min(positions), max(positions)\n\n                    # Check geometric validity constraint\n                    if x_min < case[\"m_start\"] or x_max > (case[\"L_gel\"] - case[\"m_end\"]):\n                        continue\n\n                    # Sort bands by position to find adjacent pairs\n                    bands_data.sort(key=lambda b: b[\"x\"])\n                    \n                    # Calculate pairwise resolvabilities\n                    resolvabilities = []\n                    for i in range(len(bands_data) - 1):\n                        b1 = bands_data[i]\n                        b2 = bands_data[i+1]\n                        \n                        delta_x = b2[\"x\"] - b1[\"x\"]\n                        sum_var = b1[\"var\"] + b2[\"var\"]\n                        \n                        R_ij = delta_x / (2 * np.sqrt(sum_var))\n                        resolvabilities.append(R_ij)\n                        \n                    # Calculate objective functions A and B\n                    A = min(resolvabilities)\n                    B = np.mean(resolvabilities)\n\n                    # Lexicographical comparison to find the optimal solution\n                    if best_solution[\"params\"] is None:\n                         best_solution[\"params\"] = (C, E, t)\n                         best_solution[\"A\"] = A\n                         best_solution[\"B\"] = B\n                    else:\n                        best_A, best_B = best_solution[\"A\"], best_solution[\"B\"]\n                        best_C, best_E, best_t = best_solution[\"params\"]\n                        \n                        # Maximize A\n                        if A > best_A + TOLERANCE:\n                            is_better = True\n                        elif abs(A - best_A) <= TOLERANCE:\n                            # Maximize B\n                            if B > best_B:\n                                is_better = True\n                            elif B == best_B: # Using exact float compare since no tolerance is given for B\n                                # Minimize C\n                                if C < best_C:\n                                    is_better = True\n                                elif C == best_C:\n                                    # Minimize E\n                                    if E < best_E:\n                                        is_better = True\n                                    elif E == best_E:\n                                        # Minimize t\n                                        if t < best_t:\n                                            is_better = True\n                                        else:\n                                            is_better = False\n                                    else:\n                                        is_better = False\n                                else:\n                                    is_better = False\n                            else:\n                                is_better = False\n                        else:\n                            is_better = False\n                            \n                        if is_better:\n                            best_solution[\"params\"] = (C, E, t)\n                            best_solution[\"A\"] = A\n                            best_solution[\"B\"] = B\n        \n        C_star, E_star, t_star = best_solution[\"params\"]\n        A_star = best_solution[\"A\"]\n        \n        all_results.append([C_star, E_star, t_star, A_star])\n\n    # Format the final output string exactly as specified\n    inner_list_strs = []\n    for r in all_results:\n        C_val, E_val, t_val, A_val = r\n        s = f\"[{C_val:.3f},{E_val:.3f},{int(t_val)},{A_val:.3f}]\"\n        inner_list_strs.append(s)\n    \n    final_output = f\"[{','.join(inner_list_strs)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2740396"}]}