{"hands_on_practices": [{"introduction": "The Hill function is a cornerstone for modeling cooperative processes, yet it is often used phenomenologically. This first practice delves into the mechanistic origins of cooperativity by tasking you with deriving the binding curve for a two-site receptor directly from the Law of Mass Action. By comparing this exact, microscopic model to an effective Hill function, you will gain a deeper intuition for how positive cooperativity ($K_{d1} > K_{d2}$) gives rise to the characteristic sigmoidal response and what the fitted Hill parameters $n$ and $K$ truly represent [@problem_id:2744319].", "problem": "You are to derive and implement from first principles the equilibrium fractional site occupancy for a single receptor possessing two ligand-binding sites that bind one ligand species sequentially under the Law of Mass Action. Work in the grand canonical ensemble with a binding polynomial for the receptor states, and assume stepwise macroscopic dissociation constants. Then, using the exact equilibrium expression you derived, calibrate an effective Hill model that approximates the exact occupancy across a broad range of ligand concentrations.\n\nAssumptions and definitions:\n- The receptor has two equivalent physical sites that bind sequentially with macroscopic stepwise dissociation constants $K_{d1}$ and $K_{d2}$ defined by mass-action equilibria between the unliganded receptor, singly liganded receptor, and doubly liganded receptor at equilibrium.\n- Let the free ligand concentration be $L$ in nanomolar (nM).\n- The fractional site occupancy $Y(L)$ is the expected number of bound ligands per receptor divided by $2$, and is dimensionless.\n- The effective Hill model to be fitted is the two-parameter family $Y_{\\mathrm{Hill}}(L) = \\dfrac{L^{n}}{K^{n} + L^{n}}$, where $n$ (dimensionless) is the Hill exponent and $K$ (in nM) is the half-saturation constant of the Hill model.\n\nTask A (derivation and exact computation):\n- Starting from the Law of Mass Action and the definition of macroscopic stepwise dissociation constants for sequential binding, derive the exact expression for $Y(L)$ as a function of $L$, $K_{d1}$, and $K_{d2}$ without assuming non-cooperativity. Your derivation must proceed from the binding polynomial constructed from the admissible receptor states, and $Y(L)$ must be obtained as the expected occupancy normalized by the number of sites.\n- Implement the exact $Y(L)$ you derived.\n\nTask B (effective Hill fit):\n- Using nonlinear least squares, fit the parameters $n$ and $K$ of $Y_{\\mathrm{Hill}}(L)$ to the exact $Y(L)$ computed over a log-spaced grid of $L$ values spanning six decades. Use a fitting grid of $L$ values from $10^{-3}$ nM to $10^{3}$ nM, sampled at $400$ logarithmically spaced points. The objective is to minimize the unweighted sum of squared residuals between the exact $Y(L)$ and $Y_{\\mathrm{Hill}}(L)$ across this grid.\n- Constrain the fit to $n \\in [0.1, 4.0]$ and $K \\in [10^{-6}, 10^{6}]$ nM. Choose reasonable initial guesses.\n\nParameter values to use:\n- $K_{d1} = 10$ nM.\n- $K_{d2} = 2$ nM.\n\nTest suite:\n- Compute the exact $Y(L)$ for the following five ligand concentrations (all in nM): $L \\in \\{0, 0.01, 2, 10, 1000\\}$.\n- Fit the effective Hill model as specified above to obtain estimates $\\hat{n}$ and $\\hat{K}$.\n- The program must return a single list aggregating these results as floating-point numbers: $[Y(0), Y(0.01), Y(2), Y(10), Y(1000), \\hat{n}, \\hat{K}]$.\n\nUnits and formatting:\n- Use nanomolar (nM) for all concentrations $L$, $K_{d1}$, $K_{d2}$, and the fitted $\\hat{K}$. The fractional occupancy $Y(L)$ and $\\hat{n}$ are dimensionless.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places, for example: $[x_1,x_2,x_3,x_4,x_5,x_6,x_7]$ where each $x_i$ is a decimal representation with six digits after the decimal point and no units appended.\n\nScientific realism and constraints:\n- Base your derivation strictly on the Law of Mass Action and the definition of macroscopic stepwise dissociation constants. Do not assume independent binding unless it follows from your derivation with these parameters.\n- Ensure numerical stability in the implementation and fitting across the specified range of $L$ values.", "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded in the principles of chemical equilibrium and statistical mechanics, well-posed with a clear objective, and provides a complete and consistent set of data and definitions. The problem is a standard exercise in biophysical chemistry and is solvable as stated.\n\nThe solution proceeds in two parts as requested: first, the derivation of the exact fractional site occupancy, and second, the numerical procedure to fit an effective Hill model.\n\n**Part A: Derivation of Exact Fractional Site Occupancy, $Y(L)$**\n\nThe system consists of a receptor, $R$, with two identical binding sites for a ligand, $L$. The binding is sequential, meaning the ligands bind one after another. The states of the receptor are unliganded ($R$), singly-liganded ($RL$), and doubly-liganded ($RL_2$). The equilibria between these states are governed by the Law of Mass Action, with macroscopic stepwise dissociation constants $K_{d1}$ and $K_{d2}$.\n\nThe first equilibrium reaction is the binding of the first ligand:\n$$R + L \\rightleftharpoons RL$$\nThe macroscopic dissociation constant for this step is defined as:\n$$K_{d1} = \\frac{[R][L]}{[RL]} \\quad (1)$$\nwhere $[R]$, $[L]$, and $[RL]$ are the equilibrium concentrations of the unliganded receptor, free ligand, and singly-liganded receptor, respectively.\n\nThe second equilibrium reaction is the binding of the second ligand:\n$$RL + L \\rightleftharpoons RL_2$$\nThe macroscopic dissociation constant for this second step is:\n$$K_{d2} = \\frac{[RL][L]}{[RL_2]} \\quad (2)$$\nwhere $[RL_2]$ is the equilibrium concentration of the doubly-liganded receptor.\n\nTo determine the fractional occupancy, we first describe the relative populations of the receptor states. This is conveniently done using a binding polynomial, which is central to the grand canonical ensemble treatment of this system. We express the concentrations of the liganded states, $[RL]$ and $[RL_2]$, in terms of the unliganded receptor concentration $[R]$ and the free ligand concentration $L$.\n\nFrom equation $(1)$:\n$$[RL] = \\frac{[R]L}{K_{d1}}$$\nFrom equation $(2)$:\n$$[RL_2] = \\frac{[RL]L}{K_{d2}}$$\nSubstituting the expression for $[RL]$ into the equation for $[RL_2]$:\n$$[RL_2] = \\left(\\frac{[R]L}{K_{d1}}\\right) \\frac{L}{K_{d2}} = \\frac{[R]L^2}{K_{d1}K_{d2}}$$\n\nThe total concentration of all receptor species, $[R]_{\\text{total}}$, is the sum of the concentrations of the individual states:\n$$[R]_{\\text{total}} = [R] + [RL] + [RL_2] = [R] + \\frac{[R]L}{K_{d1}} + \\frac{[R]L^2}{K_{d1}K_{d2}}$$\nFactoring out $[R]$ yields:\n$$[R]_{\\text{total}} = [R] \\left( 1 + \\frac{L}{K_{d1}} + \\frac{L^2}{K_{d1}K_{d2}} \\right)$$\nThe term in the parenthesis is the binding polynomial, $P(L)$:\n$$P(L) = 1 + \\frac{L}{K_{d1}} + \\frac{L^2}{K_{d1}K_{d2}}$$\n\nThe average number of ligands bound per receptor, $\\langle \\nu \\rangle$, is the statistical expectation of the number of bound ligands. It is calculated by summing the number of ligands bound in each state ($0$, $1$, or $2$) multiplied by the probability of observing that state. The probability of a state is its concentration divided by the total receptor concentration.\n$$\\langle \\nu \\rangle = \\frac{(0 \\cdot [R]) + (1 \\cdot [RL]) + (2 \\cdot [RL_2])}{[R]_{\\text{total}}}$$\nSubstituting the expressions for $[RL]$, $[RL_2]$, and $[R]_{\\text{total}}$:\n$$\\langle \\nu \\rangle = \\frac{ \\frac{[R]L}{K_{d1}} + 2 \\frac{[R]L^2}{K_{d1}K_{d2}} }{ [R] \\left( 1 + \\frac{L}{K_{d1}} + \\frac{L^2}{K_{d1}K_{d2}} \\right) }$$\nThe $[R]$ term cancels, leaving an expression dependent only on $L$ and the dissociation constants:\n$$\\langle \\nu \\rangle = \\frac{ \\frac{L}{K_{d1}} + \\frac{2L^2}{K_{d1}K_{d2}} }{ 1 + \\frac{L}{K_{d1}} + \\frac{L^2}{K_{d1}K_{d2}} }$$\nThis expression can also be derived from the binding polynomial via the relation $\\langle \\nu \\rangle = L \\frac{d}{dL} \\ln(P(L))$.\n\nThe problem defines the fractional site occupancy, $Y(L)$, as the average number of bound ligands per receptor divided by the total number of sites, which is $N=2$.\n$$Y(L) = \\frac{\\langle \\nu \\rangle}{N} = \\frac{\\langle \\nu \\rangle}{2}$$\nSubstituting the expression for $\\langle \\nu \\rangle$:\n$$Y(L) = \\frac{1}{2} \\left( \\frac{ \\frac{L}{K_{d1}} + \\frac{2L^2}{K_{d1}K_{d2}} }{ 1 + \\frac{L}{K_{d1}} + \\frac{L^2}{K_{d1}K_{d2}} } \\right) = \\frac{ \\frac{L}{2K_{d1}} + \\frac{L^2}{K_{d1}K_{d2}} }{ 1 + \\frac{L}{K_{d1}} + \\frac{L^2}{K_{d1}K_{d2}} }$$\nThis is the final, exact expression for the fractional site occupancy. For implementation, it is numerically advantageous to clear the denominators by multiplying the numerator and denominator by $K_{d1}K_{d2}$:\n$$Y(L) = \\frac{ \\frac{L K_{d2}}{2} + L^2 }{ K_{d1}K_{d2} + L K_{d2} + L^2 }$$\nGiven the parameter values $K_{d1} = 10\\,\\text{nM}$ and $K_{d2} = 2\\,\\text{nM}$:\n$$Y(L) = \\frac{ L + L^2 }{ 20 + 2L + L^2 }$$\n\n**Part B: Fitting the Effective Hill Model**\n\nThe task is to fit the two-parameter Hill equation, $Y_{\\text{Hill}}(L)$, to the exact occupancy function $Y(L)$ derived above.\n$$Y_{\\text{Hill}}(L) = \\frac{L^n}{K^n + L^n}$$\nHere, $n$ is the Hill exponent and $K$ is the effective half-saturation constant. The fit is performed using nonlinear least squares over a specified grid of ligand concentrations.\n\n1.  **Data Generation**: A grid of $400$ ligand concentrations, $L_i$, is generated, logarithmically spaced from $10^{-3}\\,\\text{nM}$ to $10^{3}\\,\\text{nM}$. For each $L_i$, the exact fractional occupancy $Y_i = Y(L_i)$ is calculated using the derived formula with $K_{d1} = 10$ and $K_{d2} = 2$.\n2.  **Nonlinear Least Squares**: The goal is to find the parameters $\\hat{n}$ and $\\hat{K}$ that minimize the sum of squared residuals (SSR) between the exact model and the Hill approximation:\n    $$\\text{SSR}(n, K) = \\sum_{i=1}^{400} \\left( Y_i - Y_{\\text{Hill}}(L_i; n, K) \\right)^2$$\n    This optimization is subject to the constraints $n \\in [0.1, 4.0]$ and $K \\in [10^{-6}, 10^{6}]$.\n3.  **Initial Guesses**: Reasonable initial guesses ($p_0$) for the parameters $(n, K)$ are required for the iterative optimization algorithm.\n    -   The cooperativity is positive since $K_{d1} > K_{d2}$, so we expect $n > 1$. An initial guess of $n_0=1.5$ is suitable.\n    -   The parameter $K$ is the ligand concentration at which $Y_{\\text{Hill}}(L)$ is $0.5$. The exact concentration for half-saturation ($Y(L) = 0.5$) is $L_{0.5} = \\sqrt{K_{d1}K_{d2}} = \\sqrt{10 \\cdot 2} = \\sqrt{20} \\approx 4.47\\,\\text{nM}$. This serves as an excellent initial guess, $K_0=4.5$.\n\nThe implementation will use `scipy.optimize.curve_fit` to perform this constrained nonlinear regression.\n\nFinally, the program will calculate the exact occupancy $Y(L)$ at the specific ligand concentrations $L \\in \\{0, 0.01, 2, 10, 1000\\}\\,\\text{nM}$ and append the fitted parameters $\\hat{n}$ and $\\hat{K}$ to produce the final output list.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef solve():\n    \"\"\"\n    Derives and implements the exact fractional occupancy for a two-site receptor,\n    then fits an effective Hill model to the exact curve.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    # Macroscopic stepwise dissociation constants in nM.\n    Kd1 = 10.0\n    Kd2 = 2.0\n\n    # Ligand concentrations for test suite evaluation in nM.\n    L_test = [0.0, 0.01, 2.0, 10.0, 1000.0]\n\n    # --- Task A: Exact Fractional Occupancy ---\n\n    def Y_exact(L, k1, k2):\n        \"\"\"\n        Calculates the exact fractional site occupancy for a two-site receptor.\n\n        Args:\n            L (float or np.ndarray): Free ligand concentration(s) in nM.\n            k1 (float): First macroscopic dissociation constant in nM.\n            k2 (float): Second macroscopic dissociation constant in nM.\n\n        Returns:\n            float or np.ndarray: The fractional site occupancy (dimensionless).\n        \"\"\"\n        # Derived from first principles: Y(L) = (L/(2*k1) + L^2/(k1*k2)) / (1 + L/k1 + L^2/(k1*k2))\n        # Numerically stable form: Y(L) = (L*k2/2 + L^2) / (k1*k2 + L*k2 + L^2)\n        \n        # Handle the L=0 case to avoid 0/0 or precision issues, although the formula is robust.\n        L = np.asarray(L)\n        # The numerator of the rational function\n        numerator = L * k2 / 2.0 + L**2\n        # The denominator of the rational function\n        denominator = (k1 * k2) + (k2 * L) + L**2\n        \n        # Create an output array of zeros\n        occupancy = np.zeros_like(L, dtype=float)\n        # Avoid division by zero, though denominator is always positive for L>=0\n        non_zero_denom = denominator != 0\n        occupancy[non_zero_denom] = numerator[non_zero_denom] / denominator[non_zero_denom]\n        \n        # If L was a scalar, return a scalar\n        if occupancy.ndim == 0:\n            return float(occupancy)\n        return occupancy\n\n    # Calculate exact occupancies for the test suite\n    Y_test_values = [Y_exact(l, Kd1, Kd2) for l in L_test]\n\n    # --- Task B: Effective Hill Model Fit ---\n\n    def Y_hill(L, n, K):\n        \"\"\"\n        The two-parameter Hill equation for fractional occupancy.\n\n        Args:\n            L (float or np.ndarray): Free ligand concentration(s) in nM.\n            n (float): Hill coefficient (dimensionless).\n            K (float): Half-saturation constant in nM.\n\n        Returns:\n            float or np.ndarray: The approximate fractional occupancy.\n        \"\"\"\n        # To avoid overflow with large L and n, reformulate as 1 / (1 + (K/L)^n)\n        # However, for L=0, (K/L) is inf. So, we use the standard form and handle L=0.\n        L = np.asarray(L)\n        kn = K**n\n        ln = L**n\n        # Pre-allocate result array\n        occupancy = np.zeros_like(L, dtype=float)\n        # Calculate where denominator is non-zero\n        denom = kn + ln\n        non_zero_denom = denom != 0\n        occupancy[non_zero_denom] = ln[non_zero_denom] / denom[non_zero_denom]\n        \n        return occupancy\n        \n\n    # 1. Generate data for fitting\n    # Grid of L values: 400 points, log-spaced from 10^-3 to 10^3 nM.\n    L_grid = np.logspace(-3, 3, 400)\n    # Corresponding exact Y values\n    Y_data = Y_exact(L_grid, Kd1, Kd2)\n\n    # 2. Perform nonlinear least squares fitting\n    # Initial guesses: n_0=1.5 (positive cooperativity), K_0=sqrt(Kd1*Kd2)\n    p0 = [1.5, np.sqrt(Kd1 * Kd2)]\n    # Bounds for parameters: n in [0.1, 4.0], K in [1e-6, 1e6]\n    bounds = ([0.1, 1e-6], [4.0, 1e6])\n\n    # Use scipy.optimize.curve_fit\n    try:\n        popt, _ = curve_fit(Y_hill, L_grid, Y_data, p0=p0, bounds=bounds)\n        n_fit, K_fit = popt\n    except RuntimeError:\n        # Fallback in case the optimization fails, though it is not expected here\n        n_fit, K_fit = np.nan, np.nan\n\n    # --- Assemble and Format Final Output ---\n    # Aggregate results: [Y(0), Y(0.01), Y(2), Y(10), Y(1000), n_hat, K_hat]\n    results = Y_test_values + [n_fit, K_fit]\n\n    # Format the output string with 6 decimal places for each value\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2744319"}, {"introduction": "Building upon a mechanistic understanding of the Hill function, we now explore how its parameters are governed by physical reality. This exercise integrates cooperative binding kinetics with fundamental chemical thermodynamics, demonstrating how the dissociation constant $K_d$ changes with temperature according to the van 't Hoff relation. By simulating dose-response curves at different temperatures and then fitting the model to recover the underlying thermodynamic parameters, you will practice building and validating multi-layered models that have predictive power across varying environmental conditions [@problem_id:2744343].", "problem": "You are tasked to write a complete, self-contained program that simulates dose–response curves for a temperature-sensitive cooperative protein and then fits the temperature dependence of the effective Hill parameters from first principles. The simulation and fitting must be grounded in the following base:\n\n1. Chemical thermodynamics at equilibrium: For a reversible binding process under a standard state of $1\\,\\mathrm{M}$, the standard Gibbs free energy satisfies $\\Delta G(T) = \\Delta H - T \\Delta S$ and the equilibrium association constant satisfies $K_{a}(T) = \\exp\\!\\left(-\\dfrac{\\Delta G(T)}{R T}\\right)$. The dissociation constant is $K_{d}(T) = \\dfrac{1}{K_{a}(T)}$ and must be expressed in $\\mathrm{M}$ when the standard state is $1\\,\\mathrm{M}$. Use the universal gas constant $R = 8.314462618\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$.\n\n2. Mass-action cooperative activation model: Consider a promoter or protein that becomes active only when exactly $n$ ligands are bound. Assume rapid equilibrium, strong positive cooperativity such that only the unbound and the fully bound $n$-liganded states are appreciably populated, and that the observable activity $y$ is an affine function of the probability of the fully bound state. You must derive from these definitions how the steady-state activity $y$ depends on ligand concentration $c$ (in $\\mathrm{M}$), the dissociation constant $K_{d}(T)$ (in $\\mathrm{M}$), and the cooperativity parameter $n$.\n\nYour program must do the following, strictly using the above bases without invoking any unmotivated shortcut formulas in your implementation:\n\n- For each test case and each specified absolute temperature $T$ (in $\\mathrm{K}$), compute $K_{d}(T)$ from $\\Delta H$ (in $\\mathrm{J\\,mol^{-1}}$) and $\\Delta S$ (in $\\mathrm{J\\,mol^{-1}\\,K^{-1}}$).\n- For each temperature, simulate a noise-free dose–response curve $y(c;T)$ over a specified set of ligand concentrations $c$ (in $\\mathrm{M}$). The simulated readout $y$ must be dimensionless and bounded. The baseline and dynamic range parameters are given in the test cases and must be respected.\n- For each temperature, fit the simulated $y(c;T)$ to a cooperative binding model parameterized by four effective parameters: $y_{\\min}$, $y_{\\max}$, an effective half-response concentration $K_{1/2}(T)$ (in $\\mathrm{M}$), and an effective Hill cooperativity $n_{\\mathrm{eff}}(T)$ (dimensionless). You must infer these by non-linear regression from the simulated curves for each $T$.\n- Having obtained $K_{1/2}(T)$ across temperatures for a given test case, fit its temperature dependence using the van ’t Hoff relation written in the form $\\ln K_{1/2}(T) = \\dfrac{\\Delta H}{R}\\dfrac{1}{T} - \\dfrac{\\Delta S}{R}$. From a linear fit of $\\ln K_{1/2}$ versus $1/T$, extract estimates of $\\Delta H$ (in $\\mathrm{J\\,mol^{-1}}$) and $\\Delta S$ (in $\\mathrm{J\\,mol^{-1}\\,K^{-1}}$). Convert $\\Delta H$ to $\\mathrm{kJ\\,mol^{-1}}$ for reporting. Also compute the mean of $n_{\\mathrm{eff}}(T)$ across the temperatures of that test case.\n- No random noise is to be added; the simulation must be deterministic and reproducible.\n\nPhysical and numerical units and output requirements:\n\n- All temperatures must be treated in $\\mathrm{K}$.\n- All concentrations and dissociation constants must be treated in $\\mathrm{M}$.\n- Report estimated $\\Delta H$ in $\\mathrm{kJ\\,mol^{-1}}$ and estimated $\\Delta S$ in $\\mathrm{J\\,mol^{-1}\\,K^{-1}}$.\n- For each test case, report as three floats in this order: estimated $\\Delta H$ in $\\mathrm{kJ\\,mol^{-1}}$, estimated $\\Delta S$ in $\\mathrm{J\\,mol^{-1}\\,K^{-1}}$, and the mean effective Hill coefficient (dimensionless). Round the reported $\\Delta H$ to $1$ decimal place, $\\Delta S$ to $1$ decimal place, and the mean effective Hill coefficient to $2$ decimal places.\n\nTest suite:\n\nImplement exactly the following three test cases. For each case, simulate $y(c;T)$ at the specified temperatures and concentration grids.\n\n- Test case A (moderate cooperativity):\n  - True cooperativity $n = 2.0$ (dimensionless).\n  - Thermodynamics: $\\Delta H = -40000\\,\\mathrm{J\\,mol^{-1}}$, $\\Delta S = -50\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$.\n  - Readout bounds: $y_{\\min} = 0.05$ and $y_{\\max} = 1.15$ (dimensionless).\n  - Temperatures: $T \\in \\{290, 300, 310, 320\\}\\,\\mathrm{K}$.\n  - Concentration grid: $c$ logarithmically spaced from $10^{-8}\\,\\mathrm{M}$ to $10^{-2}\\,\\mathrm{M}$ with $41$ points.\n\n- Test case B (non-cooperative boundary):\n  - True cooperativity $n = 1.0$ (dimensionless).\n  - Thermodynamics: $\\Delta H = -20000\\,\\mathrm{J\\,mol^{-1}}$, $\\Delta S = -20\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$.\n  - Readout bounds: $y_{\\min} = 0.00$ and $y_{\\max} = 1.00$ (dimensionless).\n  - Temperatures: $T \\in \\{285, 300, 330\\}\\,\\mathrm{K}$.\n  - Concentration grid: $c$ logarithmically spaced from $10^{-7}\\,\\mathrm{M}$ to $10^{-1}\\,\\mathrm{M}$ with $41$ points.\n\n- Test case C (high cooperativity):\n  - True cooperativity $n = 4.0$ (dimensionless).\n  - Thermodynamics: $\\Delta H = -60000\\,\\mathrm{J\\,mol^{-1}}$, $\\Delta S = -120\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$.\n  - Readout bounds: $y_{\\min} = 0.10$ and $y_{\\max} = 0.90$ (dimensionless).\n  - Temperatures: $T \\in \\{295, 305, 315\\}\\,\\mathrm{K}$.\n  - Concentration grid: $c$ logarithmically spaced from $10^{-8}\\,\\mathrm{M}$ to $10^{-2}\\,\\mathrm{M}$ with $41$ points.\n\nAnswer specification and final output format:\n\n- Your program must compute the requested estimates for each of the three test cases in the order A, then B, then C.\n- The final printed output must be a single line containing a comma-separated list enclosed in square brackets of length $9$, structured as $[\\widehat{\\Delta H}_{A}\\,\\mathrm{(kJ/mol)}, \\widehat{\\Delta S}_{A}\\,\\mathrm{(J/mol/K)}, \\overline{n}_{A}, \\widehat{\\Delta H}_{B}\\,\\mathrm{(kJ/mol)}, \\widehat{\\Delta S}_{B}\\,\\mathrm{(J/mol/K)}, \\overline{n}_{B}, \\widehat{\\Delta H}_{C}\\,\\mathrm{(kJ/mol)}, \\widehat{\\Delta S}_{C}\\,\\mathrm{(J/mol/K)}, \\overline{n}_{C}]$, rounded as specified above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3,\\dots]$). No other text should be printed.", "solution": "The problem statement has been evaluated and is deemed valid. It is a well-posed, scientifically grounded computational problem in the domain of synthetic biology and chemical kinetics. All necessary constants, parameters, and models are provided, and the task is a logical and consistent implementation of a standard simulation and parameter estimation pipeline. We shall proceed with the solution.\n\n### 1. Derivation of the Dose-Response Model\n\nThe problem posits a protein, $P$, that is activated upon binding $n$ ligand molecules, $L$, in a single cooperative step. This can be represented by the chemical equilibrium:\n$$\nP + nL \\rightleftharpoons PL_n\n$$\nUnder the assumption of strong positive cooperativity, the intermediate states with fewer than $n$ ligands bound are considered negligibly populated. The total concentration of the protein is $[P]_{\\text{total}} = [P] + [PL_n]$, where $[P]$ is the concentration of unbound protein and $[PL_n]$ is the concentration of the fully-bound, active complex.\n\nThe macroscopic dissociation constant for this process, which we denote as $K_{D,n}$, is given by the law of mass action:\n$$\nK_{D,n} = \\frac{[P][L]^n}{[PL_n]}\n$$\nThe units of $K_{D,n}$ are $\\mathrm{M}^n$. The problem, however, specifies a dissociation constant $K_d$ with units of $\\mathrm{M}$. This is a common convention in biochemistry, where $K_d$ is defined as the ligand concentration at which half of the maximum response is achieved. This corresponds to the half-saturation point, where $[P] = [PL_n]$. Setting $[P] = [PL_n]$ and $[L] = K_d$ in the equilibrium expression yields:\n$$\nK_{D,n} = \\frac{[P](K_d)^n}{[P]} = (K_d)^n\n$$\nThus, the macroscopic constant $K_{D,n}$ is related to the half-saturation concentration $K_d$ by $K_{D,n} = (K_d)^n$.\n\nThe fraction of active protein, $\\theta$, is the ratio of the concentration of the active state to the total protein concentration:\n$$\n\\theta = \\frac{[PL_n]}{[P]_{\\text{total}}} = \\frac{[PL_n]}{[P] + [PL_n]}\n$$\nBy rearranging the expression for $K_{D,n}$, we can write $[P] = \\frac{[PL_n] K_{D,n}}{[L]^n} = \\frac{[PL_n] (K_d)^n}{c^n}$, where we use $c$ for the ligand concentration $[L]$. Substituting this into the equation for $\\theta$:\n$$\n\\theta = \\frac{[PL_n]}{\\frac{[PL_n] (K_d)^n}{c^n} + [PL_n]} = \\frac{1}{\\frac{(K_d)^n}{c^n} + 1} = \\frac{c^n}{(K_d)^n + c^n}\n$$\nThis is the well-known Hill equation. The problem states that the observable output, $y$, is an affine function of this fraction. Therefore, the simulation model is:\n$$\ny(c;T) = y_{\\min} + (y_{\\max} - y_{\\min}) \\cdot \\theta = y_{\\min} + (y_{\\max} - y_{\\min}) \\frac{c^n}{(K_d(T))^n + c^n}\n$$\n\n### 2. Thermodynamic Basis of the Dissociation Constant\n\nThe temperature dependence of $K_d(T)$ is governed by chemical thermodynamics. The standard Gibbs free energy of the binding reaction is $\\Delta G(T) = \\Delta H - T\\Delta S$. This is related to the thermodynamic equilibrium association constant $K_a(T)$ by $\\Delta G(T) = -RT \\ln K_a(T)$, where $R$ is the universal gas constant. The dissociation constant is the reciprocal, $K_d(T) = 1/K_a(T)$.\n$$\nK_d(T) = \\frac{1}{\\exp\\left(-\\frac{\\Delta G(T)}{RT}\\right)} = \\exp\\left(\\frac{\\Delta G(T)}{RT}\\right) = \\exp\\left(\\frac{\\Delta H - T\\Delta S}{RT}\\right)\n$$\nThe problem states that even though the thermodynamic constant is dimensionless, we are to interpret its numerical value as a concentration in $\\mathrm{M}$, which is a standard convention when the standard state is $1\\,\\mathrm{M}$.\n\n### 3. Parameter Estimation Procedure\n\nThe task requires a two-stage parameter estimation process.\n\n**Stage I: Non-linear Fitting of Dose-Response Curves**\n\nFor each temperature $T$ in a given test case, we first simulate a dose-response curve $y(c;T)$ using the model derived above, with the true parameters ($n, \\Delta H, \\Delta S, y_{\\min}, y_{\\max}$). The problem then requires fitting this simulated data to a four-parameter Hill model:\n$$\ny_{\\text{fit}}(c; p) = p_0 + (p_1 - p_0) \\frac{c^{p_3}}{p_2^{p_3} + c^{p_3}}\n$$\nwhere the parameter vector is $p = \\{y_{\\min,eff}, y_{\\max,eff}, K_{1/2}(T), n_{\\mathrm{eff}}(T)\\}$. This is a non-linear least squares regression problem. Since the simulated data is noise-free and follows the exact form of the fitting model, the regression is expected to perfectly recover the parameters used for simulation. Specifically, we expect to find that the fitted effective half-response concentration $K_{1/2}(T)$ is equal to the $K_d(T)$ used in the simulation, and the fitted effective Hill cooperativity $n_{\\mathrm{eff}}(T)$ is equal to the true cooperativity $n$. We will perform this fitting for each temperature specified in a test case.\n\n**Stage II: Linear Fitting via the van 't Hoff Relation**\n\nAfter Stage I, we will have a set of estimated half-response concentrations, $\\{K_{1/2}(T_1), K_{1/2}(T_2), \\dots\\}$, for the set of temperatures $\\{T_1, T_2, \\dots\\}$. The temperature dependence of $K_{1/2}(T)$ (which equals $K_d(T)$) is described by the van 't Hoff equation, obtained by taking the natural logarithm of a rearranged $K_d(T)$ expression:\n$$\n\\ln K_{1/2}(T) = \\ln K_d(T) = \\frac{\\Delta H}{RT} - \\frac{\\Delta S}{R} = \\left(\\frac{\\Delta H}{R}\\right) \\frac{1}{T} - \\frac{\\Delta S}{R}\n$$\nThis equation predicts a linear relationship between $\\ln K_{1/2}(T)$ and $1/T$. The slope of this line is $m = \\frac{\\Delta H}{R}$ and the y-intercept is $b = -\\frac{\\Delta S}{R}$.\nBy performing a linear regression on the data points $(\\frac{1}{T}, \\ln K_{12}(T))$, we can estimate the slope $m$ and intercept $b$. From these, we can recover the thermodynamic parameters:\n$$\n\\widehat{\\Delta H} = m \\cdot R\n$$\n$$\n\\widehat{\\Delta S} = -b \\cdot R\n$$\nSince the entire procedure is deterministic and noise-free, the recovered $\\widehat{\\Delta H}$ and $\\widehat{\\Delta S}$ should be identical to the true values, within numerical precision.\n\nFinally, the mean of the fitted effective Hill coefficients, $\\overline{n}_{\\mathrm{eff}}$, is calculated across all temperatures for each test case. This value should also be identical to the true $n$.\n\n### 4. Computational Strategy\n\nThe implementation will be in Python, using the `numpy` library for numerical operations and `scipy.optimize.curve_fit` for the non-linear regression in Stage I. For the linear regression in Stage II, `numpy.polyfit` is sufficient and robust. The program will loop through each of the three specified test cases, perform the simulation-and-fitting pipeline, and store the final estimated parameters ($\\widehat{\\Delta H}$, $\\widehat{\\Delta S}$, $\\overline{n}_{\\mathrm{eff}}$), which are then formatted and printed as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef solve():\n    \"\"\"\n    Simulates dose-response curves for a temperature-sensitive cooperative protein\n    and fits the temperature dependence of the effective Hill parameters.\n    \"\"\"\n    R = 8.314462618  # Universal gas constant in J mol^-1 K^-1\n\n    test_cases = [\n        # Test case A (moderate cooperativity)\n        {\n            \"n\": 2.0, \"delta_h\": -40000.0, \"delta_s\": -50.0,\n            \"y_min\": 0.05, \"y_max\": 1.15,\n            \"temps\": [290.0, 300.0, 310.0, 320.0],\n            \"c_range\": (1e-8, 1e-2), \"c_points\": 41\n        },\n        # Test case B (non-cooperative boundary)\n        {\n            \"n\": 1.0, \"delta_h\": -20000.0, \"delta_s\": -20.0,\n            \"y_min\": 0.00, \"y_max\": 1.00,\n            \"temps\": [285.0, 300.0, 330.0],\n            \"c_range\": (1e-7, 1e-1), \"c_points\": 41\n        },\n        # Test case C (high cooperativity)\n        {\n            \"n\": 4.0, \"delta_h\": -60000.0, \"delta_s\": -120.0,\n            \"y_min\": 0.10, \"y_max\": 0.90,\n            \"temps\": [295.0, 305.0, 315.0],\n            \"c_range\": (1e-8, 1e-2), \"c_points\": 41\n        }\n    ]\n\n    # Model for non-linear regression\n    def hill_model(c, y_min_eff, y_max_eff, k_half, n_eff):\n        return y_min_eff + (y_max_eff - y_min_eff) * (c**n_eff) / (k_half**n_eff + c**n_eff)\n\n    final_results = []\n    for case in test_cases:\n        # Unpack problem parameters\n        n_true = case[\"n\"]\n        delta_h_true = case[\"delta_h\"]\n        delta_s_true = case[\"delta_s\"]\n        y_min_true = case[\"y_min\"]\n        y_max_true = case[\"y_max\"]\n        temps = np.array(case[\"temps\"])\n        c_grid = np.logspace(np.log10(case[\"c_range\"][0]), np.log10(case[\"c_range\"][1]), case[\"c_points\"])\n\n        k_half_fits = []\n        n_eff_fits = []\n        \n        # Stage 1: Simulate and fit for each temperature\n        for T in temps:\n            # Calculate Kd from thermodynamics\n            delta_g = delta_h_true - T * delta_s_true\n            k_d = np.exp(delta_g / (R * T))\n\n            # Simulate dose-response curve (noise-free)\n            y_sim = y_min_true + (y_max_true - y_min_true) * (c_grid**n_true) / (k_d**n_true + c_grid**n_true)\n            \n            # Fit the simulated curve to the Hill model\n            # Initial guess is the set of true parameters, ensuring robust convergence\n            p0 = [y_min_true, y_max_true, k_d, n_true]\n            # bounds can help in more complex cases, but not strictly needed here\n            popt, _ = curve_fit(hill_model, c_grid, y_sim, p0=p0, maxfev=10000)\n\n            k_half_fits.append(popt[2])\n            n_eff_fits.append(popt[3])\n\n        k_half_fits = np.array(k_half_fits)\n        n_eff_fits = np.array(n_eff_fits)\n\n        # Stage 2: Van 't Hoff analysis\n        inv_T = 1.0 / temps\n        ln_k_half = np.log(k_half_fits)\n        \n        # Linear regression of ln(K_1/2) vs 1/T\n        slope, intercept = np.polyfit(inv_T, ln_k_half, 1)\n\n        # Recover thermodynamic parameters from slope and intercept\n        delta_h_est = slope * R      # J/mol\n        delta_s_est = -intercept * R # J/mol/K\n\n        # Calculate mean effective Hill coefficient\n        n_eff_mean = np.mean(n_eff_fits)\n\n        # Format results as specified\n        delta_h_est_kj = round(delta_h_est / 1000.0, 1)\n        delta_s_est_rounded = round(delta_s_est, 1)\n        n_eff_mean_rounded = round(n_eff_mean, 2)\n\n        final_results.extend([delta_h_est_kj, delta_s_est_rounded, n_eff_mean_rounded])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```", "id": "2744343"}, {"introduction": "Theoretical models meet the challenges of experimental reality in this final practice. Real-world data is inherently noisy, and the nature of this noise critically affects the accuracy and reliability of parameter estimates. This exercise guides you through a comparative analysis of three essential statistical frameworks—Ordinary Least Squares (OLS), Weighted Least Squares (WLS), and Bayesian inference—for fitting the Hill equation to synthetic data with realistic, non-uniform (heteroscedastic) noise [@problem_id:2744316]. You will learn to identify the correct likelihood-based approach and appreciate the power of Bayesian methods for robustly quantifying parameter uncertainty.", "problem": "You are tasked with implementing and comparing three parameter estimation methods for a cooperative binding model in synthetic biology using synthetic data with heteroscedastic errors. The model is the Hill-function input–output relation derived from rapid equilibrium binding and the law of mass action. The observable response at ligand concentration $x$ is\n$$\ny(x;\\theta) \\equiv \\frac{V_{\\max} \\, x^{n}}{K^{n} + x^{n}},\n$$\nwhere the parameter vector is $\\theta = (V_{\\max},K,n)$ with $V_{\\max} > 0$, $K > 0$, and $n > 0$. You will generate synthetic observations $\\{(x_i, y_i)\\}_{i=1}^{N}$ according to\n$$\ny_i = y(x_i;\\theta^\\star) + \\varepsilon_i,\n$$\nwhere $\\theta^\\star$ are the true parameters for a given test case, and $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2(x_i))$ are independent Gaussian errors with heteroscedastic variance that depends on $x$ but not on $\\theta$. The heteroscedastic standard deviation is defined as\n$$\n\\sigma(x) = \\sigma_0 \\left(1 + \\beta \\frac{x}{x + s}\\right),\n$$\nwith known positive constants $\\sigma_0$, $\\beta$, and $s$.\n\nFrom fundamental principles:\n- Under rapid equilibrium, the fraction of occupied sites in a cooperative system with Hill coefficient $n$ is the ratio of the binding polynomial terms, which leads to the Hill function form above for the response $y(x;\\theta)$ after scaling by $V_{\\max}$.\n- Ordinary Least Squares (OLS) corresponds to maximum likelihood estimation under independent, identically distributed Gaussian errors with constant variance. It minimizes the unweighted sum of squared residuals.\n- Weighted Least Squares (WLS) corresponds to maximum likelihood estimation under independent Gaussian errors with known, non-constant variance. It minimizes a variance-weighted sum of squared residuals, using known weights constructed from $\\sigma^2(x)$.\n- Bayesian inference combines the Gaussian likelihood (with the heteroscedastic but $\\theta$-independent variance) and prior beliefs to obtain a posterior over $\\theta$. To enforce positivity, place independent normal priors on the logarithms of parameters, i.e., on $z = (\\log V_{\\max}, \\log K, \\log n)$.\n\nYour program must:\n1. Generate synthetic datasets for each test case below by sampling $\\varepsilon_i$ as independent draws from $\\mathcal{N}(0,\\sigma^2(x_i))$ using a fixed random seed of $123456$ for reproducibility (use this same seed for all cases; do not re-seed between cases). Use $x_i$ values on a uniform grid of $N$ points spanning the specified interval $[x_{\\min}, x_{\\max}]$, inclusive.\n2. Implement three estimators of $\\theta$:\n   - Ordinary Least Squares (OLS): minimize $\\sum_{i=1}^{N} \\left(y_i - y(x_i;\\theta)\\right)^2$ over $\\theta$.\n   - Weighted Least Squares (WLS): minimize $\\sum_{i=1}^{N} \\left(\\frac{y_i - y(x_i;\\theta)}{\\sigma(x_i)}\\right)^2$ over $\\theta$, using the known $\\sigma(x)$ defined above.\n   - Bayesian (Laplace-approximated posterior): put independent normal priors on $z = (\\log V_{\\max}, \\log K, \\log n)$ with means and standard deviations\n     $$\n     \\mu_V = \\log(1.0), \\quad \\tau_V = 1.5,\\quad\n     \\mu_K = \\log(50.0), \\quad \\tau_K = 1.5,\\quad\n     \\mu_n = \\log(2.0), \\quad \\tau_n = 1.0.\n     $$\n     Use the Gaussian likelihood with known heteroscedastic variance and compute the Maximum A Posteriori (MAP) estimate $z_{\\text{MAP}}$ by minimizing the negative log-posterior. Then approximate the posterior around $z_{\\text{MAP}}$ by a multivariate normal distribution via the Laplace approximation using the inverse of the Hessian of the negative log-posterior evaluated at $z_{\\text{MAP}}$. Under this approximation, the posterior distribution of $z$ is $\\mathcal{N}(\\mu_z, \\Sigma_z)$ with $\\mu_z \\approx z_{\\text{MAP}}$. Transforming back to parameter space component-wise via $\\theta_i = \\exp(z_i)$ yields approximately log-normal marginal posteriors. Use the log-normal moments and quantiles to compute:\n     - An estimate of $\\theta$ given by the posterior mean for each component: for component $i$, $\\mathbb{E}[\\theta_i] \\approx \\exp(\\mu_{z,i} + \\tfrac{1}{2}\\Sigma_{z,ii})$.\n     - A $95$-level marginal credible interval for each component $i$ given by $[\\exp(\\mu_{z,i} - 1.96\\sqrt{\\Sigma_{z,ii}}), \\exp(\\mu_{z,i} + 1.96\\sqrt{\\Sigma_{z,ii}})]$.\n   In all optimizations, enforce positivity by optimizing in $z$-space and mapping back via exponentials. Use numerically stable methods. When needed, restrict to a broad but finite box in $z$-space equivalent to $V_{\\max} \\in [0.1, 5.0]$, $K \\in [0.1, 300.0]$, $n \\in [0.2, 6.0]$.\n\n3. For each test case, compute the following quantitative outputs:\n   - The index of the method with the smallest root-mean-square relative error across parameters with respect to the true $\\theta^\\star$, defined as\n     $$\n     \\mathrm{RRMSE}(\\widehat{\\theta}) \\equiv \\sqrt{\\frac{1}{3}\\sum_{j=1}^{3} \\left(\\frac{\\widehat{\\theta}_j - \\theta^\\star_j}{\\theta^\\star_j}\\right)^2 }.\n     $$\n     Report the index corresponding to the ordering $[ \\text{OLS}, \\text{WLS}, \\text{Bayes} ]$ mapped to $[0,1,2]$.\n   - A boolean indicating whether WLS strictly improves the variance-weighted residual sum of squares relative to OLS, i.e., whether\n     $$\n     \\sum_{i=1}^{N} \\left(\\frac{y_i - y(x_i;\\widehat{\\theta}_{\\mathrm{WLS}})}{\\sigma(x_i)}\\right)^2 \\;<\\; \\sum_{i=1}^{N} \\left(\\frac{y_i - y(x_i;\\widehat{\\theta}_{\\mathrm{OLS}})}{\\sigma(x_i)}\\right)^2.\n     $$\n   - A list of three booleans indicating whether the $95$-level Bayesian credible interval for each component of $\\theta$ (i.e., $V_{\\max}$, $K$, $n$ in this order) contains the true value.\n\nTest suite (use exactly these values; all numbers are to be interpreted as real scalars):\n- Case $1$:\n  - True parameters $\\theta^\\star = (V_{\\max},K,n) = (1.0, 30.0, 2.5)$.\n  - Grid: $N = 40$, $x_{\\min} = 0.0$, $x_{\\max} = 200.0$, uniformly spaced including endpoints.\n  - Noise parameters: $\\sigma_0 = 0.03$, $\\beta = 2.0$, $s = 50.0$.\n- Case $2$:\n  - True parameters $\\theta^\\star = (V_{\\max},K,n) = (0.8, 10.0, 4.0)$.\n  - Grid: $N = 30$, $x_{\\min} = 0.0$, $x_{\\max} = 60.0$, uniformly spaced including endpoints.\n  - Noise parameters: $\\sigma_0 = 0.02$, $\\beta = 3.0$, $s = 20.0$.\n- Case $3$:\n  - True parameters $\\theta^\\star = (V_{\\max},K,n) = (1.5, 100.0, 1.0)$.\n  - Grid: $N = 35$, $x_{\\min} = 0.0$, $x_{\\max} = 300.0$, uniformly spaced including endpoints.\n  - Noise parameters: $\\sigma_0 = 0.05$, $\\beta = 1.5$, $s = 80.0$.\n\nImplementation requirements:\n- Use the fixed random seed $123456$ before generating any noise so that all noise draws are reproducible. Do not change the seed between cases.\n- Optimize in $z$-space where $z = (\\log V_{\\max}, \\log K, \\log n)$ and map to $\\theta$ via exponentials. Use standard numerical optimization for smooth functions. Ensure numerical stability near $x=0$.\n- For the Bayesian Laplace approximation, compute the Hessian by numerically approximating second derivatives of the negative log-posterior at $z_{\\text{MAP}}$ using finite differences and invert it to obtain $\\Sigma_z$. If the Hessian is not positive definite due to numerical error, add a small diagonal jitter for stabilization.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of the form $[m, b, [c_V, c_K, c_n]]$, where $m$ is the method index as defined above, $b$ is the boolean for WLS improvement, and $[c_V, c_K, c_n]$ are the coverage booleans for the three parameters. The overall output must therefore be a list of three such lists, in the order of Cases $1$, $2$, and $3$, for example: $[[m_1,b_1,[c_{V,1},c_{K,1},c_{n,1}]],[m_2,b_2,[c_{V,2},c_{K,2},c_{n,2}]],[m_3,b_3,[c_{V,3},c_{K,3},c_{n,3}]]]$.", "solution": "The problem presents a valid and well-posed computational task in the domain of systems biology and biostatistics. It requires the implementation and comparison of three parameter estimation methods—Ordinary Least Squares (OLS), Weighted Least Squares (WLS), and Bayesian inference via Laplace approximation—for a cooperative binding model described by the Hill function. The problem is scientifically grounded, with all models, assumptions, and procedures being standard in the field. All necessary parameters and conditions are provided, ensuring a unique and verifiable result.\n\nThe core of the problem is to estimate the parameter vector $\\theta = (V_{\\max}, K, n)$ of the Hill function,\n$$\ny(x;\\theta) = \\frac{V_{\\max} \\, x^{n}}{K^{n} + x^{n}},\n$$\ngiven a set of synthetic data points $\\{(x_i, y_i)\\}_{i=1}^{N}$. The data are generated from the model with true parameters $\\theta^\\star$ and contaminated with independent, heteroscedastic Gaussian noise:\n$$\ny_i = y(x_i;\\theta^\\star) + \\varepsilon_i, \\quad \\text{where} \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2(x_i)).\n$$\nThe standard deviation of the noise $\\sigma(x)$ is a known function of the ligand concentration $x$:\n$$\n\\sigma(x) = \\sigma_0 \\left(1 + \\beta \\frac{x}{x + s}\\right),\n$$\nwith given constants $\\sigma_0, \\beta, s$.\n\nTo enforce the positivity constraints on the parameters ($V_{\\max} > 0, K > 0, n > 0$), the estimation is performed in the space of the logarithms of the parameters, $z = (\\log V_{\\max}, \\log K, \\log n)$. The model is re-parameterized as $y(x;\\theta(z))$. For numerical stability, the Hill function is implemented as $y(x;z) = \\exp(z_1) \\frac{r^{\\exp(z_3)}}{1 + r^{\\exp(z_3)}}$, where $r = x / \\exp(z_2)$. All optimizations are performed with respect to $z$ within specified bounds.\n\nThe three estimation procedures are defined by their objective functions, which are minimized to find the parameter estimates.\n\n1.  **Ordinary Least Squares (OLS)**: This method assumes constant variance (homoscedastic) errors and corresponds to finding the parameters that minimize the sum of squared residuals. The OLS objective function is:\n    $$\n    S_{\\text{OLS}}(z) = \\sum_{i=1}^{N} \\left(y_i - y(x_i; \\theta(z))\\right)^2.\n    $$\n    The OLS estimate is $\\widehat{\\theta}_{\\text{OLS}} = \\exp(\\arg\\min_{z} S_{\\text{OLS}}(z))$.\n\n2.  **Weighted Least Squares (WLS)**: This method correctly accounts for the known heteroscedasticity of the noise. It corresponds to the maximum likelihood estimator (MLE) under the specified Gaussian noise model. The objective is to minimize the sum of squared residuals, where each residual is weighted by the inverse of the corresponding standard deviation. The WLS objective function is:\n    $$\n    S_{\\text{WLS}}(z) = \\sum_{i=1}^{N} \\left(\\frac{y_i - y(x_i; \\theta(z))}{\\sigma(x_i)}\\right)^2 = \\sum_{i=1}^{N} w_i^2 \\left(y_i - y(x_i; \\theta(z))\\right)^2,\n    $$\n    where the weights are $w_i = 1/\\sigma(x_i)$. The WLS estimate is $\\widehat{\\theta}_{\\text{WLS}} = \\exp(\\arg\\min_{z} S_{\\text{WLS}}(z))$.\n\n3.  **Bayesian Inference with Laplace Approximation**: This method combines the likelihood (as in WLS) with prior beliefs about the parameters. The problem specifies independent normal priors on the transformed parameters $z_j \\sim \\mathcal{N}(\\mu_j, \\tau_j^2)$. The posterior distribution of $z$ given the data $y$ is given by Bayes' theorem: $P(z|y) \\propto P(y|z) P(z)$. The objective is to find the Maximum A Posteriori (MAP) estimate, $z_{\\text{MAP}}$, by minimizing the negative log-posterior:\n    $$\n    L(z) = -\\log P(z|y) \\propto -\\log P(y|\\theta(z)) - \\log P(z).\n    $$\n    Ignoring constant terms, this is equivalent to minimizing:\n    $$\n    L(z) \\propto \\frac{1}{2} \\sum_{i=1}^{N} \\left(\\frac{y_i - y(x_i; \\theta(z))}{\\sigma(x_i)}\\right)^2 + \\frac{1}{2} \\sum_{j=1}^{3} \\left(\\frac{z_j - \\mu_j}{\\tau_j}\\right)^2.\n    $$\n    After finding $z_{\\text{MAP}} = \\arg\\min_z L(z)$, the posterior is approximated by a multivariate normal distribution $\\mathcal{N}(\\mu_z, \\Sigma_z)$, where $\\mu_z = z_{\\text{MAP}}$ and the covariance matrix $\\Sigma_z$ is the inverse of the Hessian of the negative log-posterior evaluated at the MAP:\n    $$\n    \\Sigma_z = \\left( \\nabla_z^2 L(z) \\big|_{z=z_{\\text{MAP}}} \\right)^{-1}.\n    $$\n    The Hessian is computed numerically using finite differences. The Bayesian point estimate is the posterior mean of $\\theta$, which for the resulting log-normal marginals is $\\mathbb{E}[\\theta_j] \\approx \\exp(\\mu_{z,j} + \\frac{1}{2}\\Sigma_{z,jj})$. The $95\\%$ marginal credible intervals are given by $[\\exp(\\mu_{z,j} \\pm 1.96\\sqrt{\\Sigma_{z,jj}})]$.\n\nFor each test case, the program first generates the synthetic data using a fixed random seed for reproducibility. It then applies the three estimation procedures using a numerical optimizer (`scipy.optimize.minimize` with the `L-BFGS-B` method). Finally, it computes the required metrics:\n-   The Root-Mean-Square Relative Error (RRMSE) for each method to determine the best point estimate.\n-   A boolean check to confirm that WLS provides a better fit than OLS in terms of the variance-weighted residual sum of squares.\n-   A list of booleans indicating whether the true parameters are captured by the $95\\%$ Bayesian credible intervals.\n\nThe following Python code implements this complete procedure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements and compares OLS, WLS, and Bayesian parameter estimation\n    for the Hill function model with heteroscedastic noise.\n    \"\"\"\n    RNG = np.random.default_rng(123456)\n\n    def hill_function_z(x, z):\n        Vmax, K, n = np.exp(z)\n        result = np.zeros_like(x, dtype=float)\n        mask = x > 0\n        if np.any(mask):\n            ratio = x[mask] / K\n            ratio_n = np.power(ratio, n)\n            result[mask] = Vmax * ratio_n / (1.0 + ratio_n)\n        return result\n\n    def noise_std_dev(x, sigma0, beta, s):\n        return sigma0 * (1.0 + beta * x / (x + s))\n\n    def compute_hessian(f, x, h=1e-5):\n        n = len(x)\n        hessian = np.zeros((n, n))\n        f_x = f(x)\n        for i in range(n):\n            x_plus_h_i = x.copy(); x_plus_h_i[i] += h\n            x_minus_h_i = x.copy(); x_minus_h_i[i] -= h\n            hessian[i, i] = (f(x_plus_h_i) - 2 * f_x + f(x_minus_h_i)) / (h**2)\n            for j in range(i + 1, n):\n                x_pp = x.copy(); x_pp[i] += h; x_pp[j] += h\n                x_pm = x.copy(); x_pm[i] += h; x_pm[j] -= h\n                x_mp = x.copy(); x_mp[i] -= h; x_mp[j] += h\n                x_mm = x.copy(); x_mm[i] -= h; x_mm[j] -= h\n                val = (f(x_pp) - f(x_pm) - f(x_mp) + f(x_mm)) / (4 * h * h)\n                hessian[i, j] = val\n                hessian[j, i] = val\n        return hessian\n\n    test_cases = [\n        {'id': 1, 'theta_star': (1.0, 30.0, 2.5), 'grid': (40, 0.0, 200.0), 'noise': (0.03, 2.0, 50.0)},\n        {'id': 2, 'theta_star': (0.8, 10.0, 4.0), 'grid': (30, 0.0, 60.0), 'noise': (0.02, 3.0, 20.0)},\n        {'id': 3, 'theta_star': (1.5, 100.0, 1.0), 'grid': (35, 0.0, 300.0), 'noise': (0.05, 1.5, 80.0)}\n    ]\n\n    prior_params = {\n        'means': np.log(np.array([1.0, 50.0, 2.0])),\n        'stds': np.array([1.5, 1.5, 1.0])\n    }\n\n    bounds_theta = np.array([[0.1, 5.0], [0.1, 300.0], [0.2, 6.0]])\n    bounds_z = np.log(bounds_theta)\n    opt_bounds = list(zip(bounds_z[:, 0], bounds_z[:, 1]))\n    \n    z0 = prior_params['means']\n    all_results = []\n\n    for case in test_cases:\n        theta_star = np.array(case['theta_star'])\n        N, x_min, x_max = case['grid']\n        sigma0, beta, s_noise = case['noise']\n\n        x_obs = np.linspace(x_min, x_max, N)\n        y_true = hill_function_z(x_obs, np.log(theta_star))\n        sigma_vals = noise_std_dev(x_obs, sigma0, beta, s_noise)\n        noise = RNG.normal(0, sigma_vals)\n        y_obs = y_true + noise\n\n        def obj_ols(z):\n            y_pred = hill_function_z(x_obs, z)\n            return np.sum((y_obs - y_pred)**2)\n\n        weights = 1.0 / sigma_vals\n        def obj_wls(z):\n            y_pred = hill_function_z(x_obs, z)\n            return np.sum((weights * (y_obs - y_pred))**2)\n        \n        prior_ivars = 1.0 / (prior_params['stds']**2)\n        def obj_bayes_neg_log_post(z):\n            log_likelihood_term = obj_wls(z)\n            log_prior_term = np.sum(prior_ivars * ((z - prior_params['means'])**2))\n            return log_likelihood_term + log_prior_term\n\n        res_ols = minimize(obj_ols, z0, method='L-BFGS-B', bounds=opt_bounds)\n        theta_ols = np.exp(res_ols.x)\n\n        res_wls = minimize(obj_wls, z0, method='L-BFGS-B', bounds=opt_bounds)\n        theta_wls = np.exp(res_wls.x)\n\n        res_bayes = minimize(obj_bayes_neg_log_post, z0, method='L-BFGS-B', bounds=opt_bounds)\n        z_map = res_bayes.x\n\n        hessian_obj = compute_hessian(obj_bayes_neg_log_post, z_map)\n        hessian_neg_log_post = hessian_obj / 2.0\n\n        eigvals = np.linalg.eigvalsh(hessian_neg_log_post)\n        if np.any(eigvals <= 1e-9):\n            hessian_neg_log_post += np.eye(len(z_map)) * 1e-6\n        \n        try:\n            cov_z = np.linalg.inv(hessian_neg_log_post)\n        except np.linalg.LinAlgError:\n            cov_z = np.linalg.pinv(hessian_neg_log_post, rcond=1e-15)\n\n        theta_bayes = np.exp(z_map + 0.5 * np.diag(cov_z))\n\n        estimates = {'OLS': theta_ols, 'WLS': theta_wls, 'Bayes': theta_bayes}\n        rrmse = {name: np.sqrt(np.mean(((est - theta_star) / theta_star)**2)) for name, est in estimates.items()}\n        method_order = ['OLS', 'WLS', 'Bayes']\n        rrmse_values = [rrmse[name] for name in method_order]\n        best_method_idx = np.argmin(rrmse_values)\n\n        wls_rss_at_wls_est = res_wls.fun\n        wls_rss_at_ols_est = obj_wls(res_ols.x)\n        wls_improves = wls_rss_at_wls_est < wls_rss_at_ols_est\n\n        mu_z, sigma_z_diag_sqrt = z_map, np.sqrt(np.abs(np.diag(cov_z)))\n        coverage = []\n        for i in range(3):\n            lower_bound = np.exp(mu_z[i] - 1.96 * sigma_z_diag_sqrt[i])\n            upper_bound = np.exp(mu_z[i] + 1.96 * sigma_z_diag_sqrt[i])\n            is_covered = (lower_bound <= theta_star[i] <= upper_bound)\n            coverage.append(is_covered)\n        \n        all_results.append([best_method_idx, bool(wls_improves), [bool(c) for c in coverage]])\n    \n    def format_case_result(res):\n        m, b, c_list = res\n        c_list_str = f\"[{','.join(str(c).lower() for c in c_list)}]\"\n        return f\"[{m},{str(b).lower()},{c_list_str}]\"\n    \n    final_output_str = f\"[{','.join(format_case_result(r) for r in all_results)}]\"\n    # Python's default json-like string representation uses True/False. The example uses true/false.\n    # The final print must match this format.\n    final_output_str = final_output_str.replace(\"True\", \"true\").replace(\"False\", \"false\")\n    print(final_output_str)\n\nsolve()\n```", "id": "2744316"}]}