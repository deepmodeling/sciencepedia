{"hands_on_practices": [{"introduction": "A cornerstone of synthetic biology is the ability to predictably tune gene expression. This exercise [@problem_id:2727500] delves into the rational design of constitutive promoter libraries using a foundational biophysical model. By approximating transcription initiation rate as a function of RNA polymerase binding energy, where the binding probability follows a Boltzmann distribution, we can use a position-specific energy matrix to score any given promoter sequence. This practice will guide you through calculating the distribution of promoter strengths in a combinatorial library, providing a quantitative link between DNA sequence and transcriptional output.", "problem": "You are designing a library of bacterial sigma factor promoters composed of two hexamers: the minus thirty-five region and the minus ten region. The library randomizes exactly two positions in each hexamer while leaving all other positions fixed to a specified sequence. Assume transcription initiation rate is proportional to the probability of RNA polymerase binding, and approximate the binding by an additive position-specific energy model. Concretely, use the following foundational bases:\n\n- The Central Dogma asserts that gene expression begins with transcription initiation, where RNA polymerase binds promoter DNA. In a coarse-grained thermodynamic model, the equilibrium probability of occupancy at the promoter is proportional to the Boltzmann factor.\n- Let the total binding energy be $E$ measured in units of $k_B T$ (Boltzmann constant times absolute temperature), so that the relative promoter strength $S$ is proportional to $\\exp(-E)$.\n- Assume additivity (an energy matrix model): the total energy is the sum of per-position contributions across the minus thirty-five and minus ten hexamers. Each position contributes an energy that depends only on the nucleotide at that position.\n\nDefinitions and data:\n\n- The minus thirty-five consensus sequence is $-$35: $\\text{TTGACA}$, and the minus ten consensus sequence is $-$10: $\\text{TATAAT}$. Positions are indexed within each hexamer from $1$ to $6$.\n- An energy matrix for each hexamer is provided as $\\varepsilon_{\\text{-35}}$ and $\\varepsilon_{\\text{-10}}$, each of shape $6 \\times 4$ in units of $k_B T$. Each row corresponds to positions $1$ to $6$ and each column corresponds to bases in the order $[\\text{A}, \\text{C}, \\text{G}, \\text{T}]$. The energy matrices are:\n  - For $-$35:\n    - Row $1$: $[2.0, 2.5, 2.0, 0.0]$\n    - Row $2$: $[2.0, 2.0, 2.5, 0.0]$\n    - Row $3$: $[2.0, 2.5, 0.0, 2.0]$\n    - Row $4$: $[0.0, 2.0, 2.0, 2.5]$\n    - Row $5$: $[2.0, 0.0, 2.0, 2.5]$\n    - Row $6$: $[0.0, 2.0, 2.0, 2.5]$\n  - For $-$10:\n    - Row $1$: $[2.0, 2.5, 2.0, 0.0]$\n    - Row $2$: $[0.0, 2.5, 2.0, 2.0]$\n    - Row $3$: $[2.0, 2.0, 2.5, 0.0]$\n    - Row $4$: $[0.0, 2.0, 2.5, 2.0]$\n    - Row $5$: $[0.0, 2.5, 2.0, 2.0]$\n    - Row $6$: $[2.0, 2.0, 2.5, 0.0]$\n- Use the nucleotide-to-index mapping $A \\mapsto 0$, $C \\mapsto 1$, $G \\mapsto 2$, $T \\mapsto 3$ when indexing the columns.\n- The fixed background sequences for non-randomized positions are exactly the consensus sequences $-$35: $\\text{TTGACA}$ and $-$10: $\\text{TATAAT}$. For the consensus base at any position, the per-position energy in the matrices above is $0.0$.\n\nLibrary definition:\n\n- In each test case, exactly two positions in the $-$35 hexamer and exactly two positions in the $-$10 hexamer are randomized. All randomized positions are sampled uniformly over $\\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$. The total library therefore contains $4^4$ variants.\n- For a given variant, the total energy $E$ is the sum of the $-$35 and $-$10 per-position energies at the chosen bases. The predicted promoter strength for a variant is $S = \\exp(-E)$ (dimensionless, since $E$ is in units of $k_B T$).\n\nYour tasks for each test case:\n\n- Enumerate all $4^4$ variants implied by the specified randomized positions.\n- Compute the complete distribution of energies $\\{E_i\\}$ and strengths $\\{S_i\\}$ across the library.\n- For the specified energy threshold $E_{\\mathrm{thr}}$ (in units of $k_B T$), compute the fraction of variants with $E_i \\le E_{\\mathrm{thr}}$ expressed as a decimal.\n- Compute two summary statistics of the strength distribution: the mean strength $\\overline{S}$ and the median strength $\\tilde{S}$ across the full library.\n\nReporting requirements:\n\n- All energies are in units of $k_B T$; strengths are dimensionless. The requested outputs are decimals.\n- For each test case, output a list of three values $[f, \\overline{S}, \\tilde{S}]$, where $f$ is the fraction with $E \\le E_{\\mathrm{thr}}$, rounded to $6$ decimal places, and $\\overline{S}$ and $\\tilde{S}$ are the mean and median of $S$ rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each element is the bracketed triple for a test case. For example, a valid output structure has the form $[[f_1, \\overline{S}_1, \\tilde{S}_1],[f_2, \\overline{S}_2, \\tilde{S}_2],\\dots]$.\n\nTest suite:\n\n- Case $1$: Randomized positions in $-$35 are $\\{2, 5\\}$, and in $-$10 are $\\{3, 6\\}$; threshold $E_{\\mathrm{thr}} = 5.0$.\n- Case $2$: Randomized positions in $-$35 are $\\{1, 3\\}$, and in $-$10 are $\\{2, 5\\}$; threshold $E_{\\mathrm{thr}} = 0.0$.\n- Case $3$: Randomized positions in $-$35 are $\\{4, 6\\}$, and in $-$10 are $\\{1, 4\\}$; threshold $E_{\\mathrm{thr}} = 2.0$.\n- Case $4$: Randomized positions in $-$35 are $\\{4, 6\\}$, and in $-$10 are $\\{1, 4\\}$; threshold $E_{\\mathrm{thr}} = 100.0$.\n\nConventions:\n\n- Positions within each hexamer are indexed from $1$ to $6$. When combining energies, sum the contributions from the two hexamers. Non-randomized positions use the fixed consensus base.\n- Angle units are not applicable. No physical units other than $k_B T$ are involved.", "solution": "The problem has been validated and is deemed scientifically sound, well-posed, and objective. It presents a standard application of a position-specific energy matrix model, a cornerstone of biophysical modeling for protein-DNA interactions, here applied to the rational design of a synthetic promoter library. The premises are consistent with established principles in molecular biology and statistical mechanics. We will proceed with a formal solution.\n\nThe fundamental principle governing the system is derived from statistical mechanics. The relative promoter strength, $S$, which is proportional to the probability of RNA polymerase binding to the promoter DNA, is given by the Boltzmann factor:\n$$S \\propto \\exp\\left(-\\frac{E}{k_B T}\\right)$$\nThe problem states that the binding energy $E$ is measured in units of $k_B T$, which simplifies the expression for promoter strength to:\n$$S = \\exp(-E)$$\nThe total binding energy $E$ is determined by an additive, position-specific energy model. The promoter sequence consists of two hexamers, the $-\\text{35}$ region and the $-\\text{10}$ region. The total energy is the sum of contributions from all $12$ positions:\n$$E = E_{\\text{-35}} + E_{\\text{-10}} = \\sum_{p=1}^{6} \\varepsilon_{\\text{-35}}(p, b_p) + \\sum_{p=1}^{6} \\varepsilon_{\\text{-10}}(p, b_p)$$\nwhere $\\varepsilon(p, b_p)$ is the energy contribution of the base $b_p$ at position $p$ within its respective hexamer. The energy values are provided by the matrices $\\varepsilon_{\\text{-35}}$ and $\\varepsilon_{\\text{-10}}$.\n\nA critical simplification arises from the library design. The non-randomized positions are fixed to the consensus sequences ($\\text{TTGACA}$ for $-\\text{35}$ and $\\text{TATAAT}$ for $-\\text{10}$). By inspecting the provided energy matrices, the energy contribution for any consensus base at its correct position is specified as $0.0 k_B T$. Consequently, the eight non-randomized positions contribute a total of $0.0$ to the binding energy. The total energy $E$ for any given variant is therefore determined solely by the sum of energy contributions from the four specified randomized positions.\n\nFor each test case, we are tasked with analyzing a library of $4^4 = 256$ unique promoter variants. This is computationally tractable through direct enumeration. The algorithm to obtain the required statistics for each case is as follows:\n\n1.  **Identify Randomized Positions**: For a given test case, we identify the two positions in the $-\\text{35}$ hexamer and the two positions in the $-\\text{10}$ hexamer that are being randomized. Let these be $p_{\\text{35},1}, p_{\\text{35},2}$ and $p_{\\text{10},1}, p_{\\text{10},2}$.\n\n2.  **Enumerate Variants and Calculate Energies**: We generate all $256$ possible variants by considering every combination of the four bases $\\{A, C, G, T\\}$ at each of the four randomized positions. For each variant $i$, the total energy $E_i$ is calculated:\n    $$E_i = \\varepsilon_{\\text{-35}}(p_{\\text{35},1}, b_1) + \\varepsilon_{\\text{-35}}(p_{\\text{35},2}, b_2) + \\varepsilon_{\\text{-10}}(p_{\\text{10},1}, b_3) + \\varepsilon_{\\text{-10}}(p_{\\text{10},2}, b_4)$$\n    where $b_1, b_2, b_3, b_4 \\in \\{A, C, G, T\\}$ are the bases for that specific variant. This process populates a list of $256$ energy values, $\\{E_i\\}$.\n\n3.  **Calculate Strengths**: For each energy $E_i$, the corresponding promoter strength is calculated as $S_i = \\exp(-E_i)$. This generates a list of $256$ strength values, $\\{S_i\\}$.\n\n4.  **Compute Required Statistics**: With the complete distributions $\\{E_i\\}$ and $\\{S_i\\}$, we compute the three required metrics:\n    *   **Fraction of variants below energy threshold ($f$)**: We count the number of variants whose energy $E_i$ is less than or equal to the specified threshold $E_{\\mathrm{thr}}$. This count is divided by the total library size ($256$) to obtain the fraction $f$.\n      $$f = \\frac{1}{256} \\sum_{i=1}^{256} \\mathbb{I}(E_i \\le E_{\\mathrm{thr}})$$\n      where $\\mathbb{I}$ is the indicator function, which is $1$ if the condition is true and $0$ otherwise.\n    *   **Mean strength ($\\overline{S}$)**: The arithmetic mean of the strength distribution is calculated.\n      $$\\overline{S} = \\frac{1}{256} \\sum_{i=1}^{256} S_i$$\n    *   **Median strength ($\\tilde{S}$)**: The list of strengths $\\{S_i\\}$ is sorted. Since the number of variants ($256$) is even, the median is the arithmetic mean of the two central elements (the $128$-th and $129$-th elements of the sorted list, assuming $1$-based indexing).\n\nThis structured enumeration and calculation procedure is implemented for each test case provided in the problem statement. The positions specified in the problem are $1$-indexed and are converted to $0$-based indices for accessing the energy matrices in the program. The final numerical results are rounded to $6$ decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import product\n\ndef solve():\n    \"\"\"\n    Solves the promoter library design problem for all test cases.\n    \"\"\"\n\n    # Define energy matrices as per the problem statement.\n    # Rows correspond to positions 1-6.\n    # Columns correspond to bases [A, C, G, T].\n    e_35 = np.array([\n        [2.0, 2.5, 2.0, 0.0],  # Pos 1, Consensus T\n        [2.0, 2.0, 2.5, 0.0],  # Pos 2, Consensus T\n        [2.0, 2.5, 0.0, 2.0],  # Pos 3, Consensus G\n        [0.0, 2.0, 2.0, 2.5],  # Pos 4, Consensus A\n        [2.0, 0.0, 2.0, 2.5],  # Pos 5, Consensus C\n        [0.0, 2.0, 2.0, 2.5],  # Pos 6, Consensus A\n    ])\n\n    e_10 = np.array([\n        [2.0, 2.5, 2.0, 0.0],  # Pos 1, Consensus T\n        [0.0, 2.5, 2.0, 2.0],  # Pos 2, Consensus A\n        [2.0, 2.0, 2.5, 0.0],  # Pos 3, Consensus T\n        [0.0, 2.0, 2.5, 2.0],  # Pos 4, Consensus A\n        [0.0, 2.5, 2.0, 2.0],  # Pos 5, Consensus A\n        [2.0, 2.0, 2.5, 0.0],  # Pos 6, Consensus T\n    ])\n\n    # Each test case: ((pos35_1, pos35_2), (pos10_1, pos10_2), e_threshold)\n    # Positions are 1-indexed.\n    test_cases = [\n        ((2, 5), (3, 6), 5.0),\n        ((1, 3), (2, 5), 0.0),\n        ((4, 6), (1, 4), 2.0),\n        ((4, 6), (1, 4), 100.0),\n    ]\n\n    all_results = []\n\n    for pos_35_rand, pos_10_rand, e_thr in test_cases:\n        # Extract the energy profiles for the four randomized positions.\n        # Positions are 1-indexed, so subtract 1 for 0-indexed array access.\n        energy_profiles = [\n            e_35[pos_35_rand[0] - 1],\n            e_35[pos_35_rand[1] - 1],\n            e_10[pos_10_rand[0] - 1],\n            e_10[pos_10_rand[1] - 1],\n        ]\n\n        # Generate all 4^4 = 256 combinations of energy contributions.\n        # 'product' generates the Cartesian product of the input iterables.\n        energy_combinations = product(*energy_profiles)\n\n        energies = []\n        # Sum the energy contributions for each variant in the library.\n        for e_combo in energy_combinations:\n            total_energy = sum(e_combo)\n            energies.append(total_energy)\n        \n        energies_arr = np.array(energies)\n        \n        # Calculate strengths: S = exp(-E)\n        strengths_arr = np.exp(-energies_arr)\n        \n        # 1. Compute fraction of variants with E <= E_thr\n        count_below_thresh = np.sum(energies_arr <= e_thr)\n        fraction_f = count_below_thresh / len(energies_arr)\n        \n        # 2. Compute mean strength\n        mean_s = np.mean(strengths_arr)\n        \n        # 3. Compute median strength\n        median_s = np.median(strengths_arr)\n        \n        # Round results to 6 decimal places and store.\n        case_result = [\n            round(fraction_f, 6),\n            round(mean_s, 6),\n            round(median_s, 6)\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string as specified: [[f1,S_bar1,S_tilde1],[f2,S_bar2,S_tilde2],...]\n    # Using a list comprehension and join to avoid spaces inserted by str(list).\n    result_strings = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "2727500"}, {"introduction": "While predictive models are powerful, synthetic parts must ultimately be characterized experimentally. This practice [@problem_id:2727532] shifts our focus from in silico design to the analysis of experimental data for inducible promoters. You will learn to process raw sequencing counts from a high-throughput promoter assay, normalize them into meaningful activity units, and fit the resulting dose-response data to the Hill function. This process enables the extraction of critical parameters such as the half-maximal effective concentration ($EC50$) and the Hill coefficient ($n$), which together quantitatively define a promoter's characteristic transfer function.", "problem": "You are given count data from a pooled promoter-barcode library assayed across multiple inducer concentrations. Each promoter is associated with multiple unique DNA barcodes that report transcriptional output as sequencing read counts. For each concentration, you also have the total number of reads collected in that sample. From first principles, follow the Central Dogma of Molecular Biology and the Hill-Langmuir equilibrium model: transcriptional output is proportional to the probability that the transcriptional machinery is in an active state, which for a single ligand with possible cooperativity can be modeled as a Hill function. Use this to compute normalized dose-response curves for each promoter and fit a Hill function to estimate the half-maximal effective concentration and the Hill coefficient.\n\nFundamental base you must use:\n- Central Dogma of Molecular Biology: DNA $\\to$ RNA $\\to$ protein, with promoter activity modulating transcription initiation.\n- Hill-Langmuir occupancy model for ligand-regulated activation with cooperativity: the probability that an activator-regulated promoter is active is given by a Hill function in the ligand concentration $c$.\n- Sequencing counts are sampling from an underlying abundance; for sufficiently large counts, Poisson sampling noise is well-approximated by a normal distribution with variance equal to the mean, which implies that weighting by the inverse of the standard deviation is appropriate in least-squares fitting.\n- Normalization by library depth converts raw counts to comparable activity units such as counts per million.\n\nDefinitions and tasks:\n- For each concentration $c_i$ (in $\\mu\\mathrm{M}$), let $T_i$ be the total reads sequenced in that sample (unit: counts). For a promoter $p$ with barcodes indexed by $b$, let $X_{p,b,i}$ be the count for barcode $b$ at concentration index $i$. Define the promoter-level summed count $S_{p,i} = \\sum_b X_{p,b,i}$.\n- Define promoter activity as counts-per-million $Y_{p,i} = 10^6 \\cdot S_{p,i} / T_i$ (unit: counts per million reads).\n- Fit the Hill function\n$$\nf(c) \\;=\\; f_{\\min} \\;+\\; \\bigl(f_{\\max} - f_{\\min}\\bigr)\\,\\frac{c^{n}}{EC50^{n}+c^{n}},\n$$\nwith parameters $f_{\\min} \\ge 0$ (lower asymptote, unit: counts per million), $f_{\\max} \\ge f_{\\min}$ (upper asymptote, unit: counts per million), $EC50 > 0$ (half-maximal effective concentration, unit: $\\mu\\mathrm{M}$), and $n > 0$ (Hill coefficient, unitless). Use weighted nonlinear least squares with per-point standard deviations\n$$\n\\sigma_{p,i} \\;=\\; \\max\\!\\left(1, \\sqrt{S_{p,i}}\\right)\\cdot \\frac{10^6}{T_i},\n$$\nmotivated by a Poisson model for counts and propagation of uncertainty through the linear normalization to $Y_{p,i}$.\n\nAngle units are not applicable. Concentration must be treated in $\\mu\\mathrm{M}$ and reported in $\\mu\\mathrm{M}$. Return $EC50$ in $\\mu\\mathrm{M}$ rounded to three decimals, and $n$ as a unitless value rounded to three decimals. All numeric outputs must be in decimal form (not percentages).\n\nYour program must:\n- For each test case, compute $Y_{p,i}$ across concentrations and fit $f(c)$ to obtain estimates of $EC50$ and $n$ for each promoter.\n- Use reasonable initial guesses derived solely from the provided data (e.g., $f_{\\min}$ from the minimum of $Y_{p,i}$, $f_{\\max}$ from its maximum, $EC50$ from the concentration closest to the half-maximum, and $n$ initialized to a moderate value).\n- Constrain parameters to physically meaningful ranges: $f_{\\min} \\ge 0$, $f_{\\max} \\ge 0$, $EC50 > 0$, $n > 0$.\n\nTest suite (two cases) to ensure coverage:\n- Case 1 (happy path; moderate cooperativity and dynamic ranges):\n  - Concentrations $c$ in $\\mu\\mathrm{M}$: $[0.0, 0.1, 0.3, 1.0, 3.0, 10.0, 30.0]$.\n  - Total reads $T$ per concentration (counts): $[2000000, 2100000, 1900000, 2050000, 2200000, 1950000, 2100000]$.\n  - Promoters with barcode counts $X_{p,b,i}$ (integers), listed as arrays across the seven concentrations in the order above:\n    - Promoter P1 (3 barcodes):\n      - Barcode 1: $[36, 71, 296, 2297, 6344, 6862, 7541]$\n      - Barcode 2: $[33, 65, 271, 2105, 5815, 6290, 6913]$\n      - Barcode 3: $[31, 61, 255, 1978, 5463, 5910, 6494]$\n    - Promoter P2 (3 barcodes):\n      - Barcode 1: $[24, 86, 223, 852, 2490, 4359, 6020]$\n      - Barcode 2: $[18, 65, 167, 639, 1867, 3269, 4516]$\n      - Barcode 3: $[18, 64, 167, 639, 1867, 3269, 4515]$\n- Case 2 (edge coverage; nearly constitutive weakly inducible promoter and an ultrasensitive promoter):\n  - Concentrations $c$ in $\\mu\\mathrm{M}$: $[0.0, 0.1, 0.3, 1.0, 3.0, 10.0, 30.0]$.\n  - Total reads $T$ per concentration (counts): $[1500000, 1700000, 1600000, 1800000, 1650000, 1900000, 2000000]$.\n  - Promoters with barcode counts:\n    - Promoter P3 (2 barcodes):\n      - Barcode 1: $[3120, 3538, 3333, 3763, 3482, 4132, 4640]$\n      - Barcode 2: $[2880, 3265, 3077, 3473, 3214, 3814, 4284]$\n    - Promoter P4 (4 barcodes):\n      - Barcode 1: $[4, 1404, 18707, 25160, 23095, 26600, 28000]$\n      - Barcode 2: $[4, 1204, 16035, 21565, 19796, 22800, 24000]$\n      - Barcode 3: $[4, 1304, 17371, 23363, 21446, 24700, 26000]$\n      - Barcode 4: $[3, 1104, 14699, 19768, 18147, 20900, 22000]$\n\nComputational requirements:\n- For each promoter, compute $Y_{p,i}$ in counts per million (CPM) at each $c_i$.\n- Perform a weighted nonlinear least-squares fit of $f(c)$ to the $(c_i, Y_{p,i})$ pairs to estimate parameters. Use bounds $f_{\\min} \\in [0, 10\\cdot \\max_i Y_{p,i}]$, $f_{\\max} \\in [0, 10\\cdot \\max_i Y_{p,i}]$, $EC50 \\in [10^{-9}, 10^{6}]$ (in $\\mu\\mathrm{M}$), $n \\in [0.1, 6.0]$.\n- Return, for each promoter, only $EC50$ (in $\\mu\\mathrm{M}$) and $n$ (unitless), each rounded to three decimals.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result is a list with one element per test case. Each test case element is a list of promoters in the order provided, where each promoter is represented by a two-element list $[EC50, n]$ with both numbers rounded to three decimals. For example: $[[[1.234,2.000],[5.678,1.500]],[[100.000,1.000],[0.200,4.000]]]$.", "solution": "The problem is subjected to rigorous validation and is determined to be sound. It is scientifically grounded in the Central Dogma of Molecular Biology and the Hill-Langmuir model, which are standard frameworks for describing gene regulation and dose-response pharmacology. The problem is mathematically well-posed, providing all necessary data, equations, and constraints for a standard nonlinear regression task. The terminology is precise and objective, with no evident contradictions or ambiguities that would preclude a unique and meaningful solution. The provided test data are realistic for a high-throughput sequencing-based promoter assay. Therefore, I will proceed with a full solution.\n\nThe solution methodology involves the following steps for each promoter provided in the test cases:\n$1$. Data Aggregation and Normalization: First, the raw barcode counts for a given promoter $p$ at each concentration $c_i$ are summed to obtain the total promoter-level count, $S_{p,i} = \\sum_b X_{p,b,i}$. These summed counts are then normalized by the total sequencing depth of each sample, $T_i$, and scaled to obtain the activity in units of counts per million (CPM), according to the formula $Y_{p,i} = 10^6 \\cdot S_{p,i} / T_i$. This yields the dose-response data points $(c_i, Y_{p,i})$.\n\n$2$. Error Model and Weighting: The problem states that sequencing counts follow a Poisson distribution. For a sufficiently large count $S$, the variance is approximately equal to the mean, $\\mathrm{Var}(S) \\approx S$, so the standard deviation is $\\mathrm{StdDev}(S) \\approx \\sqrt{S}$. This uncertainty propagates through the normalization step. The standard deviation of the normalized activity $Y_{p,i}$ is thus $\\sigma_{p,i} = \\mathrm{StdDev}(10^6 \\cdot S_{p,i} / T_i) = (10^6/T_i) \\cdot \\mathrm{StdDev}(S_{p,i}) \\approx (10^6/T_i) \\cdot \\sqrt{S_{p,i}}$. To ensure numerical stability for zero or very low counts, the standard deviation for the counts is taken as $\\max(1, \\sqrt{S_{p,i}})$. Weighted least-squares fitting minimizes the sum of squared residuals, weighted by the inverse of the variance, which is equivalent to dividing each residual by the standard deviation. Thus, the weights for the fitting algorithm are $w_i = 1/\\sigma_{p,i}$.\n\n$3$. Nonlinear Regression: The Hill function, $f(c) = f_{\\min} + (f_{\\max} - f_{\\min}) \\frac{c^{n}}{EC50^{n} + c^{n}}$, is fitted to the normalized data points $(c_i, Y_{p,i})$ using a weighted nonlinear least-squares algorithm. The parameters to be estimated are the basal activity $f_{\\min}$, the maximal activity $f_{\\max}$, the half-maximal effective concentration $EC50$, and the Hill coefficient $n$.\n\n$4$. Parameter Initialization and Bounding: The fitting algorithm requires initial parameter guesses and bounds to ensure convergence to a physically meaningful solution.\n- Initial guesses ($p_0$):\n  - $f_{\\min, 0}$: The minimum observed activity, $\\min(Y_p)$.\n  - $f_{\\max, 0}$: The maximum observed activity, $\\max(Y_p)$.\n  - $EC50_0$: The concentration $c_k$ at which the observed activity $Y_{p,k}$ is closest to the midpoint of the dynamic range, $(f_{\\min, 0} + f_{\\max, 0})/2$.\n  - $n_0$: A moderate value, set to $1.5$.\n- Parameter bounds: The fit is constrained according to the problem specification:\n  - $f_{\\min} \\in [0, 10 \\cdot \\max_i Y_{p,i}]$\n  - $f_{\\max} \\in [0, 10 \\cdot \\max_i Y_{p,i}]$\n  - $EC50 \\in [10^{-9}, 10^6]$ $\\mu\\mathrm{M}$\n  - $n \\in [0.1, 6.0]$\n\nThe implementation will utilize `numpy` for efficient numerical computation and `scipy.optimize.curve_fit` for the nonlinear regression. The procedure will be applied to each promoter in the provided test suite, and the estimated $EC50$ and $n$ values, rounded to three decimal places, will be formatted into the specified output structure.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef solve():\n    \"\"\"\n    Solves the promoter dose-response fitting problem for the given test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"c\": [0.0, 0.1, 0.3, 1.0, 3.0, 10.0, 30.0],\n            \"T\": [2000000, 2100000, 1900000, 2050000, 2200000, 1950000, 2100000],\n            \"promoters\": {\n                \"P1\": [\n                    [36, 71, 296, 2297, 6344, 6862, 7541],\n                    [33, 65, 271, 2105, 5815, 6290, 6913],\n                    [31, 61, 255, 1978, 5463, 5910, 6494]\n                ],\n                \"P2\": [\n                    [24, 86, 223, 852, 2490, 4359, 6020],\n                    [18, 65, 167, 639, 1867, 3269, 4516],\n                    [18, 64, 167, 639, 1867, 3269, 4515]\n                ]\n            }\n        },\n        {\n            \"c\": [0.0, 0.1, 0.3, 1.0, 3.0, 10.0, 30.0],\n            \"T\": [1500000, 1700000, 1600000, 1800000, 1650000, 1900000, 2000000],\n            \"promoters\": {\n                \"P3\": [\n                    [3120, 3538, 3333, 3763, 3482, 4132, 4640],\n                    [2880, 3265, 3077, 3473, 3214, 3814, 4284]\n                ],\n                \"P4\": [\n                    [4, 1404, 18707, 25160, 23095, 26600, 28000],\n                    [4, 1204, 16035, 21565, 19796, 22800, 24000],\n                    [4, 1304, 17371, 23363, 21446, 24700, 26000],\n                    [3, 1104, 14699, 19768, 18147, 20900, 22000]\n                ]\n            }\n        }\n    ]\n\n    def hill_function(c, f_min, f_max, ec50, n):\n        \"\"\"Standard Hill-Langmuir function.\"\"\"\n        # Add a small epsilon to ec50 to prevent division by zero inside the fitting algorithm if ec50 is near zero\n        # and c is zero, though the mathematical form is safe.\n        ec50_safe = ec50 + 1e-12\n        return f_min + (f_max - f_min) * (c**n) / (ec50_safe**n + c**n)\n\n    all_cases_results = []\n    for case in test_cases:\n        concentrations = np.array(case[\"c\"], dtype=float)\n        total_reads = np.array(case[\"T\"], dtype=float)\n        \n        case_promoter_results = []\n        # Ensure order of promoters is maintained\n        promoter_names = sorted(case[\"promoters\"].keys())\n        \n        for promoter_name in promoter_names:\n            barcode_counts = np.array(case[\"promoters\"][promoter_name], dtype=float)\n            \n            # Step 1: Aggregate counts\n            summed_counts = np.sum(barcode_counts, axis=0)\n            \n            # Step 2: Normalize to CPM\n            y_data = 1e6 * summed_counts / total_reads\n            \n            # Step 3: Calculate weights for fitting\n            sigma = np.maximum(1.0, np.sqrt(summed_counts)) * 1e6 / total_reads\n            \n            # Step 4: Initial guesses for parameters\n            y_min_obs = np.min(y_data)\n            y_max_obs = np.max(y_data)\n            \n            # Handle flat data case\n            if np.isclose(y_min_obs, y_max_obs):\n                y_max_obs = y_min_obs + 1.0\n\n            p0_f_min = y_min_obs\n            p0_f_max = y_max_obs\n            \n            half_max_y = (p0_f_min + p0_f_max) / 2\n            # Find concentration closest to where response is half-maximal\n            try:\n                # Add a small epsilon to concentrations before taking logs for interpolation\n                # This could be an alternative for EC50 guess, but problem states to use closest point.\n                idx = np.argmin(np.abs(y_data - half_max_y))\n                p0_ec50 = concentrations[idx] if concentrations[idx] > 0 else 1e-6\n            except (ValueError, IndexError):\n                p0_ec50 = np.median(concentrations[concentrations > 0]) # Fallback\n            \n            p0_n = 1.5\n            \n            p0 = [p0_f_min, p0_f_max, p0_ec50, p0_n]\n\n            # Step 5: Define parameter bounds\n            max_y = np.max(y_data)\n            # Add a floor to the upper bound to avoid a zero bound if max_y is zero.\n            upper_f_bound = max(1.0, 10.0 * max_y)\n            \n            bounds_low = [0, 0, 1e-9, 0.1]\n            bounds_high = [upper_f_bound, upper_f_bound, 1e6, 6.0]\n            \n            # Step 6: Perform weighted nonlinear least-squares fitting\n            try:\n                popt, _ = curve_fit(\n                    hill_function,\n                    concentrations,\n                    y_data,\n                    p0=p0,\n                    sigma=sigma,\n                    absolute_sigma=True,\n                    bounds=(bounds_low, bounds_high),\n                    maxfev=5000,\n                    method='trf'\n                )\n                \n                ec50 = popt[2]\n                n_h = popt[3]\n                \n            except RuntimeError:\n                # If fit fails, use NaN as placeholders. Real-world code would need robust error handling.\n                ec50, n_h = np.nan, np.nan\n\n            case_promoter_results.append(f\"[{ec50:.3f},{n_h:.3f}]\")\n            \n        all_cases_results.append(f\"[{','.join(case_promoter_results)}]\")\n        \n    print(f\"[{','.join(all_cases_results)}]\")\n\nsolve()\n```", "id": "2727532"}, {"introduction": "Building reliable genetic circuits requires not only well-characterized parts but also parts that are orthogonal—that is, they do not interfere with one another. This advanced problem [@problem_id:2727571] tackles the critical issue of cross-reactivity between transcription factors (TFs) and their DNA binding sites. Using the same energy matrix framework from our first exercise, you will develop a quantitative method to predict the likelihood of one TF binding to the intended site of another. This practice provides a formal approach to computing metrics like false binding rate, which is key for evaluating and ensuring the specificity of regulatory components in a complex system.", "problem": "You are tasked with formalizing and computing cross-reactivity predictions between transcription factors (TFs) using additive energy matrices, a common abstraction in thermodynamic models of gene regulation. Assume the following fundamental bases: (i) the Central Dogma of Molecular Biology, ensuring that sequence-level determinants can modulate transcription via TF-DNA binding; (ii) a standard thermodynamic additive model in which the binding energy of a sequence is the sum of per-position contributions from a TF-specific energy matrix; and (iii) an independent and identically distributed background model for bases, with per-base probabilities specified. All energies are measured in units of Boltzmann’s constant times temperature, $k_{\\mathrm{B}}T$ (dimensionless). Angles do not appear. No physical units are required for the outputs.\n\nA transcription factor is defined by an energy matrix of length $L$, with rows corresponding to positions $1,\\dots,L$ and columns in the fixed base order $[\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}]$. For a DNA sequence $s = (b_1,\\dots,b_L)$ with $b_i \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$, and for a TF $X \\in \\{A,B\\}$ with energy matrix $E^{(X)} \\in \\mathbb{R}^{L \\times 4}$, the binding energy is defined as\n$$\n\\mathcal{E}_{X}(s) \\;=\\; \\sum_{i=1}^{L} E^{(X)}_{i,\\,b_i},\n$$\nwhere $E^{(X)}_{i,\\,b_i}$ denotes the energy contribution at position $i$ for base $b_i$ under TF $X$, and lower energy implies stronger binding. Sequences are drawn independently according to a background distribution $p \\in [0,1]^4$ over $\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$, in that order, so the probability of a particular sequence $s$ is\n$$\n\\mathbb{P}(s) \\;=\\; \\prod_{i=1}^{L} p_{b_i}.\n$$\nFor a specified threshold $T_X$ for TF $X$, define the high-affinity set\n$$\n\\mathcal{H}_X \\;=\\; \\{s \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}^{L} \\,:\\, \\mathcal{E}_X(s) \\le T_X\\}.\n$$\nThe cross-reactive overlap mass is\n$$\nP_{\\mathrm{overlap}} \\;=\\; \\sum_{s} \\mathbf{1}\\{s \\in \\mathcal{H}_A \\cap \\mathcal{H}_B\\} \\,\\mathbb{P}(s).\n$$\nThe false binding rate of TF $B$ under a promoter library selected for TF $A$ by the high-affinity criterion is defined as the conditional probability\n$$\n\\mathrm{FBR}_{B \\mid A} \\;=\\; \n\\begin{cases}\n\\dfrac{\\sum_{s} \\mathbf{1}\\{s \\in \\mathcal{H}_A \\cap \\mathcal{H}_B\\}\\,\\mathbb{P}(s)}{\\sum_{s} \\mathbf{1}\\{s \\in \\mathcal{H}_{A}\\}\\,\\mathbb{P}(s)} & \\text{if } \\sum_{s} \\mathbf{1}\\{s \\in \\mathcal{H}_A\\}\\,\\mathbb{P}(s) > 0,\\\n$$1em]\n0 & \\text{otherwise.}\n\\end{cases}\n$$\nAll probabilities must be reported as decimals (not percentages). Threshold comparisons are inclusive. For each test case below, compute and return $P_{\\mathrm{overlap}}$ and $\\mathrm{FBR}_{B \\mid A}$.\n\nUse the fixed base order $[\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}]$ in all matrices and probability vectors. All numeric values are in $\\mathbb{R}$, with energies in units of $k_{\\mathrm{B}}T$.\n\nTest suite:\n- Test case $1$:\n  - Length $L = 7$.\n  - Background $p^{(1)} = (0.25, 0.25, 0.25, 0.25)$.\n  - Energy matrices $E^{(A)}$ and $E^{(B)}$ are\n    $$\n    E^{(A)} = \n    \\begin{bmatrix}\n    -1.0 & 0.5 & 0.5 & -0.8\\\\\n    0.2 & -0.9 & -0.2 & 0.4\\\\\n    -1.2 & 0.6 & 0.3 & -0.5\\\\\n    0.0 & -0.7 & -0.7 & 0.2\\\\\n    -0.8 & 0.4 & 0.6 & -0.6\\\\\n    0.3 & -1.0 & -0.4 & 0.2\\\\\n    -0.5 & 0.5 & -0.3 & 0.1\n    \\end{bmatrix},\\quad\n    E^{(B)} =\n    \\begin{bmatrix}\n    -0.6 & 0.2 & -0.7 & 0.3\\\\\n    0.5 & -0.8 & -0.4 & 0.1\\\\\n    -0.4 & 0.0 & -0.9 & 0.2\\\\\n    -0.2 & -0.6 & 0.1 & 0.3\\\\\n    0.2 & -0.5 & -0.6 & 0.1\\\\\n    -0.7 & 0.3 & -0.8 & 0.4\\\\\n    0.0 & -0.3 & -0.6 & 0.2\n    \\end{bmatrix}.\n    $$\n  - Thresholds $T_A^{(1)} = -3.5$, $T_B^{(1)} = -3.0$.\n\n- Test case $2$:\n  - Same $L$, $E^{(A)}$, and $E^{(B)}$ as in test case $1$.\n  - Background $p^{(2)} = (0.25, 0.25, 0.25, 0.25)$.\n  - Thresholds $T_A^{(2)} = -4.2$, $T_B^{(2)} = -3.8$.\n\n- Test case $3$:\n  - Same $L$, $E^{(A)}$, and $E^{(B)}$ as in test case $1$.\n  - Background $p^{(3)} = (0.4, 0.1, 0.1, 0.4)$.\n  - Thresholds $T_A^{(3)} = -3.2$, $T_B^{(3)} = -2.8$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in order, and each element is itself a two-element list of the form $[P_{\\mathrm{overlap}}, \\mathrm{FBR}_{B \\mid A}]$. Both values for each test case must be rounded to six decimal places. For example, the final output format must be\n$$\n[\\,[x_1,y_1],[x_2,y_2],[x_3,y_3]\\,],\n$$\nbut with the actual computed numbers substituted for $x_i$ and $y_i$.", "solution": "We derive the requested quantities by starting from the additive energy model and an independent background over nucleotides. Let positions be indexed by $i \\in \\{1,\\dots,L\\}$ and bases by $b \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ in the fixed order. For a transcription factor $X \\in \\{A,B\\}$, the energy matrix $E^{(X)} \\in \\mathbb{R}^{L \\times 4}$ specifies contributions $E^{(X)}_{i,b}$, such that the total binding energy for sequence $s=(b_1,\\dots,b_L)$ is\n$$\n\\mathcal{E}_X(s) \\;=\\; \\sum_{i=1}^{L} E^{(X)}_{i,\\,b_i}.\n$$\nUnder an independent background $p \\in [0,1]^4$ with $p_{\\mathrm{A}}+p_{\\mathrm{C}}+p_{\\mathrm{G}}+p_{\\mathrm{T}}=1$, the probability of sequence $s$ is\n$$\n\\mathbb{P}(s) \\;=\\; \\prod_{i=1}^{L} p_{b_i}.\n$$\nA high-affinity set for TF $X$ at threshold $T_X$ is defined by\n$$\n\\mathcal{H}_X \\;=\\; \\{ s \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}^{L} \\,:\\, \\mathcal{E}_X(s) \\le T_X \\}.\n$$\nThe overlap probability mass (a measure of predicted cross-reactivity in sequence space) is\n$$\nP_{\\mathrm{overlap}} \\;=\\; \\sum_{s} \\mathbf{1}\\{ s \\in \\mathcal{H}_A \\cap \\mathcal{H}_B \\} \\,\\mathbb{P}(s).\n$$\nThe false binding rate of $B$ given selection for $A$ by the high-affinity criterion is the conditional probability\n$$\n\\mathrm{FBR}_{B \\mid A} \\;=\\; \\mathbb{P}\\big( s \\in \\mathcal{H}_B \\,\\big|\\, s \\in \\mathcal{H}_A \\big)\n\\;=\\;\n\\frac{\\sum_{s} \\mathbf{1}\\{ s \\in \\mathcal{H}_A \\cap \\mathcal{H}_B \\}\\,\\mathbb{P}(s)}{\\sum_{s} \\mathbf{1}\\{ s \\in \\mathcal{H}_A \\}\\,\\mathbb{P}(s)},\n$$\nwith the convention that if the denominator is zero the value is defined to be $0$.\n\nThese definitions derive from thermodynamic reasoning: in equilibrium, lower energies denote stronger binding. A strict thermodynamic occupancy model would weight contributions by a Boltzmann factor $\\exp(-\\mathcal{E}_X(s))$ and a chemical potential; however, when a high-affinity cutoff is used to define a promoter library for $A$, the relevant false binding rate of $B$ given $A$ is a conditional probability over the design distribution of selected sequences. If sequences in the library are drawn from the background conditioned on $\\mathcal{E}_A(s) \\le T_A$, then the above ratio computes the expected fraction of those that also satisfy $\\mathcal{E}_B(s) \\le T_B$. The overlap mass $P_{\\mathrm{overlap}}$ quantifies the intrinsic cross-reactive overlap of high-affinity regions under the background.\n\nAlgorithmic design follows directly from these sums. For small motif lengths $L$, an exact computation is feasible by enumerating all $4^{L}$ sequences. For each sequence $s$:\n- Compute $\\mathcal{E}_A(s)$ and $\\mathcal{E}_B(s)$ via a sum of per-position contributions, which is $\\mathcal{O}(L)$.\n- Compute the background probability $\\mathbb{P}(s) = \\prod_{i=1}^{L} p_{b_i}$, which is $\\mathcal{O}(L)$.\n- Accumulate three weighted sums:\n  - $S_{A} = \\sum_{s} \\mathbf{1}\\{\\mathcal{E}_A(s) \\le T_A\\}\\,\\mathbb{P}(s)$,\n  - $S_{B} = \\sum_{s} \\mathbf{1}\\{\\mathcal{E}_B(s) \\le T_B\\}\\,\\mathbb{P}(s)$ (not strictly needed for the requested outputs but conceptually useful),\n  - $S_{AB} = \\sum_{s} \\mathbf{1}\\{\\mathcal{E}_A(s) \\le T_A \\wedge \\mathcal{E}_B(s) \\le T_B\\}\\,\\mathbb{P}(s)$.\nThen $P_{\\mathrm{overlap}} = S_{AB}$ and $\\mathrm{FBR}_{B \\mid A} = S_{AB}/S_{A}$ if $S_{A} > 0$, else $0$.\n\nThis exact enumeration has complexity $\\mathcal{O}(L \\cdot 4^{L})$, which is tractable for $L = 7$. Alternative scalable methods for larger $L$ include dynamic programming over energy bins to approximate the joint energy distribution $(\\mathcal{E}_A,\\mathcal{E}_B)$, or Fourier-space convolution (via characteristic functions) followed by saddlepoint approximations, but these are unnecessary here.\n\nImplementation details:\n- Use the base index order $[\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}]$ consistently to access matrix columns and background probabilities.\n- Apply inclusive thresholds ($\\le$).\n- Round the final values $P_{\\mathrm{overlap}}$ and $\\mathrm{FBR}_{B \\mid A}$ to six decimal places.\n- Aggregate results per the specified output format as a single line: $[\\,[x_1,y_1],[x_2,y_2],[x_3,y_3]\\,]$.\n\nThe provided program encodes the matrices and backgrounds exactly as specified and performs the enumerations to produce the requested outputs deterministically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import product\n\ndef enumerate_overlap_and_fbr(EA, EB, TA, TB, p_bg):\n    \"\"\"\n    Compute:\n      - P_overlap = sum_s 1[E_A(s) <= TA and E_B(s) <= TB] * P_bg(s)\n      - FBR_{B|A} = P(E_B(s) <= TB | E_A(s) <= TA), with 0.0 if denominator is zero.\n    EA, EB: numpy arrays of shape (L, 4)\n    TA, TB: floats (thresholds)\n    p_bg: numpy array of shape (4,) with base order [A, C, G, T], sum to 1\n    \"\"\"\n    L = EA.shape[0]\n    # Pre-validate shapes\n    assert EA.shape == EB.shape and EA.shape[1] == 4\n    assert p_bg.shape == (4,)\n    # Enumerate all sequences\n    S_A = 0.0\n    S_AB = 0.0\n    # Precompute to speed multiplication\n    bases = range(4)  # 0:A, 1:C, 2:G, 3:T\n    for seq in product(bases, repeat=L):\n        # Energy sums\n        eA = 0.0\n        eB = 0.0\n        prob = 1.0\n        for i, b in enumerate(seq):\n            eA += EA[i, b]\n            eB += EB[i, b]\n            prob *= p_bg[b]\n            # Early exit not beneficial due to need of prob anyway\n        condA = (eA <= TA)\n        if condA:\n            S_A += prob\n            if eB <= TB:\n                S_AB += prob\n        else:\n            # If fails A, it cannot contribute to intersection\n            pass\n    P_overlap = S_AB\n    if S_A > 0.0:\n        FBR = S_AB / S_A\n    else:\n        FBR = 0.0\n    return P_overlap, FBR\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Common energy matrices for test cases 1-3 (order [A,C,G,T])\n    EA = np.array([\n        [-1.0,  0.5,  0.5, -0.8],\n        [ 0.2, -0.9, -0.2,  0.4],\n        [-1.2,  0.6,  0.3, -0.5],\n        [ 0.0, -0.7, -0.7,  0.2],\n        [-0.8,  0.4,  0.6, -0.6],\n        [ 0.3, -1.0, -0.4,  0.2],\n        [-0.5,  0.5, -0.3,  0.1],\n    ], dtype=float)\n\n    EB = np.array([\n        [-0.6,  0.2, -0.7,  0.3],\n        [ 0.5, -0.8, -0.4,  0.1],\n        [-0.4,  0.0, -0.9,  0.2],\n        [-0.2, -0.6,  0.1,  0.3],\n        [ 0.2, -0.5, -0.6,  0.1],\n        [-0.7,  0.3, -0.8,  0.4],\n        [ 0.0, -0.3, -0.6,  0.2],\n    ], dtype=float)\n\n    test_cases = [\n        # Each tuple: (EA, EB, TA, TB, p_bg)\n        (EA, EB, -3.5, -3.0, np.array([0.25, 0.25, 0.25, 0.25], dtype=float)),\n        (EA, EB, -4.2, -3.8, np.array([0.25, 0.25, 0.25, 0.25], dtype=float)),\n        (EA, EB, -3.2, -2.8, np.array([0.4, 0.1, 0.1, 0.4], dtype=float)),\n    ]\n\n    results = []\n    for EA_i, EB_i, TA_i, TB_i, p_bg_i in test_cases:\n        p_overlap, fbr = enumerate_overlap_and_fbr(EA_i, EB_i, TA_i, TB_i, p_bg_i)\n        results.append((p_overlap, fbr))\n\n    # Format: [[x1,y1],[x2,y2],[x3,y3]] with six decimal places, no spaces\n    formatted = \"[\" + \",\".join(f\"[{x:.6f},{y:.6f}]\" for x, y in results) + \"]\"\n    print(formatted)\n\nsolve()\n```", "id": "2727571"}]}