{"hands_on_practices": [{"introduction": "A cornerstone of modeling synthetic gene circuits is fitting mechanistic models, like the Hill function, to experimental data. However, the non-convex nature of the resulting optimization problem often leads to convergence at suboptimal local minima. This practice introduces a powerful technique, homotopy continuation, which transforms a difficult optimization problem into a sequence of simpler ones, effectively guiding the parameter search toward a more robust and accurate solution. By gradually deforming a simplified version of the problem back to the original, you will learn a practical strategy to improve the reliability of your parameter estimates. [@problem_id:2757696]", "problem": "You are tasked with implementing a homotopy continuation strategy to estimate parameters of a transcriptional activation module in synthetic biology from steady-state data, avoiding poor local minima during nonlinear least-squares fitting. Start from the following fundamental base appropriate for gene expression modules: the Central Dogma of molecular biology states that Deoxyribonucleic Acid (DNA) is transcribed to Ribonucleic Acid (RNA), which is translated to protein. Under constant inducer input, a single gene product concentration, denoted $P(t)$, can be modeled by a mass balance where the synthesis rate (which depends on inducer input) and the first-order loss rate (degradation and dilution) determine $dP/dt$. The steady-state $P_{\\mathrm{ss}}$ is defined by the balance of these opposing fluxes. The interaction between the inducer and the promoter can be modeled by a Hill activation function. All symbols must be interpreted in the units stated below.\n\nYour model assumptions and definitions are as follows:\n- Dynamic mass balance: $dP/dt = \\text{synthesis} - \\text{loss}$. At steady-state, $dP/dt = 0$.\n- First-order loss: $\\text{loss} = k_{\\mathrm{deg}} P$, where $k_{\\mathrm{deg}}$ is a positive constant (in $\\mathrm{min}^{-1}$).\n- Synthesis depends on inducer concentration $u$ via a Hill activation: $\\text{synthesis} = k_{\\mathrm{syn}} \\cdot f(u)$, with $f(u) = \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}}$.\n- Reparameterize using $S = k_{\\mathrm{syn}}/k_{\\mathrm{deg}}$ (in arbitrary fluorescence units), so that the steady-state protein level satisfies $P_{\\mathrm{ss}}(u; S, K_{\\mathrm{d}}, n) = S \\cdot \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}}$.\n- All inputs $u$ are inducer concentrations in micromolar ($\\mu\\mathrm{M}$). The output $P_{\\mathrm{ss}}$ is in arbitrary fluorescence units (a.u.). The Hill coefficient $n$ is dimensionless.\n\nGoal: For each test case listed below, estimate the parameter vector $\\theta = [S, K_{\\mathrm{d}}, n]$ by minimizing the sum of squared residuals between the model prediction and provided “experimental” steady-state readouts. To avoid poor local minima, use a homotopy continuation on input amplitude: define a scale parameter $s \\in (0,1]$, scale all inputs by $s$ as $u \\mapsto s\\,u$, and at each successive $s$ solve the least-squares problem initialized from the previous solution. Start from a deliberately poor initial guess and progress through the scale sequence until $s = 1$.\n\nHomotopy procedure requirements:\n- Let the scale sequence be $s \\in \\{\\,0.1, 0.3, 0.6, 1.0\\,\\}$ (unitless).\n- At each $s$, fit $\\theta$ to the steady-state data measured at the scaled inputs $s\\,u$, using the previous step’s estimate as the initial guess. Use bound constraints $S \\in [10^{-6}, 5 \\cdot 10^{3}]$, $K_{\\mathrm{d}} \\in [10^{-6}, 10^{4}]$, and $n \\in [0.5, 6.0]$.\n- Use the initial guess $\\theta_{0} = [500.0, 500.0, 3.0]$ at $s = 0.1$.\n- At each $s$, minimize the unweighted sum of squared residuals $\\sum_{i} \\left(P_{\\mathrm{ss}}(s\\,u_{i}; \\theta) - y_{i}(s)\\right)^{2}$, where $y_{i}(s)$ are the provided “experimental” steady-state readings at scale $s$.\n\nData and units:\n- Use the same set of base input concentrations for all cases: $u \\in \\{\\,0, 10, 50, 200\\,\\}$ (in $\\mu\\mathrm{M}$). The homotopy scales $s$ transform these to $s\\,u$.\n- For this exercise, the “experimental” steady-state readouts $y_{i}(s)$ at each $s$ are generated without noise by the same model with the true parameters given below. This yields a fully deterministic test, while the fitting algorithm must not use the true parameters in any way other than to generate the synthetic data.\n\nTest suite (three cases with distinct regimes):\n1. Happy-path case: true parameters $\\theta^{\\ast}_{1} = [1200.0, 30.0, 2.0]$ with $S$ in a.u., $K_{\\mathrm{d}}$ in $\\mu\\mathrm{M}$.\n2. High-cooperativity case: true parameters $\\theta^{\\ast}_{2} = [800.0, 5.0, 4.0]$.\n3. Low-dynamic-range case: true parameters $\\theta^{\\ast}_{3} = [300.0, 100.0, 1.5]$.\n\nTasks to implement:\n- For each case $k \\in \\{1,2,3\\}$, construct the scale list $[0.1, 0.3, 0.6, 1.0]$, the base input list $[0, 10, 50, 200]$, and, for each scale $s$, compute the synthetic steady-state readings $y_{i}^{(k)}(s) = P_{\\mathrm{ss}}(s\\,u_{i}; \\theta^{\\ast}_{k})$.\n- Implement the homotopy continuation fitting across the scales in ascending order, starting at $s=0.1$ from the initial guess $\\theta_{0} = [500.0, 500.0, 3.0]$, and bounded as specified. At each scale, fit only to that scale’s data.\n- Return the final estimates at $s=1.0$ for each case.\n\nNumerical and unit requirements:\n- Report the final estimates $\\widehat{\\theta}_{k} = [\\widehat{S}_{k}, \\widehat{K}_{\\mathrm{d},k}, \\widehat{n}_{k}]$ for $k \\in \\{1,2,3\\}$ with $\\widehat{S}_{k}$ in a.u., $\\widehat{K}_{\\mathrm{d},k}$ in $\\mu\\mathrm{M}$, and $\\widehat{n}_{k}$ dimensionless.\n- Round each reported number to three decimal places. No other units should be printed.\n\nFinal output specification:\n- Your program should produce a single line of output containing a list of three inner lists, one per test case, each inner list being $[\\widehat{S}, \\widehat{K}_{\\mathrm{d}}, \\widehat{n}]$ with the three numbers rounded to three decimals. The output must contain no spaces, and the entire line must be enclosed in square brackets. For example, an output format with three cases would look like $[[a,b,c],[d,e,f],[g,h,i]]$ where each symbol is replaced with a decimal number rounded to three decimals.", "solution": "The problem is subjected to validation before any attempt at a solution.\n\n### Step 1: Extract Givens\n- **Model**: Steady-state protein level $P_{\\mathrm{ss}}(u; S, K_{\\mathrm{d}}, n) = S \\cdot \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}}$.\n- **Parameters**: $\\theta = [S, K_{\\mathrm{d}}, n]$.\n  - $S$: maximum synthesis/degradation rate ratio (a.u.).\n  - $K_{\\mathrm{d}}$: activation constant ($\\mu\\mathrm{M}$).\n  - $n$: Hill coefficient (dimensionless).\n- **Goal**: Estimate $\\theta$ for three test cases using a homotopy continuation strategy to minimize sum of squared residuals.\n- **Homotopy Procedure**:\n  - Scale sequence: $s \\in \\{0.1, 0.3, 0.6, 1.0\\}$.\n  - At each $s$, fit $\\theta$ to data at scaled inputs $s \\cdot u$ using the previous step's estimate as the initial guess.\n  - Initial guess at $s=0.1$: $\\theta_{0} = [500.0, 500.0, 3.0]$.\n  - Parameter bounds: $S \\in [10^{-6}, 5 \\cdot 10^{3}]$, $K_{\\mathrm{d}} \\in [10^{-6}, 10^{4}]$, $n \\in [0.5, 6.0]$.\n  - Objective function: a non-linear least-squares minimization of $\\sum_{i} \\left(P_{\\mathrm{ss}}(s\\,u_{i}; \\theta) - y_{i}(s)\\right)^{2}$.\n- **Data**:\n  - Base input concentrations: $u \\in \\{0, 10, 50, 200\\}$ ($\\mu\\mathrm{M}$).\n  - \"Experimental\" data $y_{i}(s)$ are generated without noise from the model using true parameters: $y_{i}(s) = P_{\\mathrm{ss}}(s\\,u_{i}; \\theta^{\\ast})$.\n- **Test Cases (True Parameters)**:\n  1. $\\theta^{\\ast}_{1} = [1200.0, 30.0, 2.0]$.\n  2. $\\theta^{\\ast}_{2} = [800.0, 5.0, 4.0]$.\n  3. $\\theta^{\\ast}_{3} = [300.0, 100.0, 1.5]$.\n- **Output Specification**:\n  - Report final estimates $\\widehat{\\theta}_{k} = [\\widehat{S}_{k}, \\widehat{K}_{\\mathrm{d},k}, \\widehat{n}_{k}]$ for $k \\in \\{1,2,3\\}$.\n  - Round each number to three decimal places.\n  - Final output is a single line, a list of lists, with no spaces: `[[a,b,c],[d,e,f],[g,h,i]]`.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded (Critical)**: The problem is based on the mass-action kinetics and Hill functions, which are canonical models in molecular and systems biology for describing gene regulatory networks. This is a standard, scientifically sound approach.\n- **Well-Posed**: The problem is well-posed. It details a specific numerical procedure (homotopy continuation for nonlinear least-squares) with all necessary components: a model, an objective function, initial conditions, parameter bounds, and a clear sequence of steps. The use of noise-free synthetic data ensures that a \"true\" solution exists (the parameters used to generate the data), against which the algorithm's performance can be measured.\n- **Objective (Critical)**: The problem is stated in precise, quantitative, and objective language, free from ambiguity or subjective interpretation.\n- **Incomplete or Contradictory Setup**: The problem is self-contained and provides all necessary information. There are no contradictions in the provided data or constraints.\n- **Unrealistic or Infeasible**: The parameter values, concentrations, and model behavior are plausible for synthetic gene circuits. The task is a standard in-silico (computational) experiment.\n- **Non-Formalizable or Irrelevant**: The problem is an archetypal example of *parameter estimation and model fitting to experimental data* within the field of *synthetic biology*. It is directly relevant and mathematically formalizable.\n\n### Step 3: Verdict and Action\nThe problem is determined to be valid. It constitutes a well-defined computational task in systems biology, grounded in established scientific principles. We proceed with the solution.\n\n---\n\nThe problem requires the estimation of parameters for a nonlinear biological model. This model describes the steady-state concentration of a protein, $P_{\\mathrm{ss}}$, as a function of an inducer concentration, $u$. The governing equation is a reparameterized Hill activation function:\n$$ P_{\\mathrm{ss}}(u; S, K_{\\mathrm{d}}, n) = S \\cdot \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}} $$\nThe parameter vector to be determined is $\\theta = [S, K_{\\mathrm{d}}, n]$. $S$ represents the maximal expression level, $K_{\\mathrm{d}}$ is the activation coefficient (the concentration of $u$ yielding a half-maximal response), and $n$ is the Hill coefficient, which quantifies the cooperativity or steepness of the transcriptional response.\n\nParameter estimation is formulated as a nonlinear least-squares optimization problem. The goal is to find the parameter vector $\\theta$ that minimizes the sum of squared residuals (SSR) between the model's predictions and the provided 'experimental' data, $y_i$. For a set of $N$ input-output pairs $(u_i, y_i)$, the objective function is:\n$$ \\mathcal{L}(\\theta) = \\sum_{i=1}^{N} \\left( P_{\\mathrm{ss}}(u_{i}; \\theta) - y_{i} \\right)^{2} $$\nDirectly minimizing such a non-convex function can be difficult, as optimization algorithms may converge to suboptimal local minima, especially when starting from an initial guess far from the global minimum. To circumvent this, the problem prescribes a homotopy continuation method. This technique deforms the original difficult problem into a sequence of simpler, related problems that guide the optimizer toward a better solution.\n\nThe homotopy is constructed by introducing a continuation parameter, $s$, which scales the input concentrations: $u \\mapsto s \\cdot u$ for $s \\in (0, 1]$. The optimization is first solved for a small $s$ (e.g., $s=0.1$). At small $s$, the scaled inputs $s \\cdot u_i$ are all in the low-concentration regime, where the Hill function is approximately a power law ($P_{\\mathrm{ss}} \\approx S (u/K_{\\mathrm{d}})^n$), making the optimization landscape smoother. The solution from this simplified problem then serves as the initial guess for the next problem in the sequence, which uses a slightly larger value of $s$. This iterative process continues until $s=1$, at which point the original, full-range problem is solved, but with the benefit of starting from a highly-informed initial guess derived from the continuation path.\n\nThe specified algorithm is executed as follows for each of the three test cases, which are defined by their true parameter vectors $\\theta^{\\ast}_{k}$:\n1.  Define the discrete sequence of scale parameters $s_j \\in \\{0.1, 0.3, 0.6, 1.0\\}$.\n2.  Initialize the parameter estimate with a deliberately poor guess: $\\hat{\\theta}^{(0)} = [500.0, 500.0, 3.0]$.\n3.  Iterate through the scales $s_j$ for $j=1, 2, 3, 4$:\n    a. For the current test case $k$ and scale $s_j$, generate the synthetic data points $y_{i,k}^{(j)} = P_{\\mathrm{ss}}(s_j u_i; \\theta^{\\ast}_{k})$ using the base input concentrations $u_i \\in \\{0, 10, 50, 200\\}$.\n    b. The residual vector for this optimization step is defined as $\\mathbf{r}(\\theta)$, where its $i$-th component is $r_i(\\theta) = P_{\\mathrm{ss}}(s_j u_i; \\theta) - y_{i,k}^{(j)}$.\n    c. Solve the bounded nonlinear least-squares problem:\n    $$ \\hat{\\theta}^{(j)} = \\arg\\min_{\\theta \\in \\Theta} \\| \\mathbf{r}(\\theta) \\|_2^2 $$\n    The optimization is initialized with the estimate from the previous step, $\\hat{\\theta}^{(j-1)}$. The parameter search space $\\Theta$ is constrained by the specified bounds: $S \\in [10^{-6}, 5 \\cdot 10^{3}]$, $K_{\\mathrm{d}} \\in [10^{-6}, 10^{4}]$, and $n \\in [0.5, 6.0]$.\n4.  The final estimated parameter vector for test case $k$ is the result from the last step of the continuation, $\\widehat{\\theta}_{k} = \\hat{\\theta}^{(4)}$, corresponding to the solution at $s=1.0$.\n\nThis procedure is implemented using the `scipy.optimize.least_squares` function, which is well-suited for bounded nonlinear least-squares problems. Since the data are generated synthetically from the same model being fitted and are free of noise, a successful optimization procedure should recover the true parameters $\\theta^{\\ast}_{k}$ up to the limits of numerical precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef solve():\n    \"\"\"\n    Implements a homotopy continuation strategy to estimate parameters of a\n    transcriptional activation module from synthetic steady-state data.\n    \"\"\"\n\n    # Define the base input concentrations (in micromolar)\n    base_u = np.array([0., 10., 50., 200.])\n\n    # Define the test cases with their true parameters [S, Kd, n]\n    test_cases = [\n        [1200.0, 30.0, 2.0],  # Happy-path case\n        [800.0, 5.0, 4.0],   # High-cooperativity case\n        [300.0, 100.0, 1.5], # Low-dynamic-range case\n    ]\n\n    # Homotopy continuation parameters\n    scales = [0.1, 0.3, 0.6, 1.0]\n    initial_guess = np.array([500.0, 500.0, 3.0])\n    bounds = ([1e-6, 1e-6, 0.5], [5000.0, 10000.0, 6.0])\n\n    def p_ss_model(theta, u):\n        \"\"\"\n        Calculates the steady-state protein level using the Hill activation model.\n        P_ss(u; S, Kd, n) = S * (u^n) / (Kd^n + u^n)\n        \n        Args:\n            theta (array-like): Parameter vector [S, Kd, n].\n            u (array-like): Inducer concentrations.\n        \n        Returns:\n            numpy.ndarray: Predicted steady-state protein levels.\n        \"\"\"\n        S, Kd, n = theta\n        u_n = np.power(u, n)\n        Kd_n = np.power(Kd, n)\n        \n        # Denominator is guaranteed to be non-zero due to bounds on Kd and u >= 0.\n        denominator = Kd_n + u_n\n        \n        # Handle u=0 case to avoid 0/0 if Kd was 0 (not possible here, but good practice)\n        # np.divide is robust and allows specifying output for division by zero.\n        # Here, numerator is S * u_n.\n        # safe division: if denominator is zero, result is 0 (as u would be 0).\n        # We can also handle this explicitly for clarity.\n        # If u=0, u_n=0, numerator is 0, so fraction is 0.\n        # If u > 0, denominator > 0.\n        # The direct calculation is safe given the constraints.\n        return S * u_n / denominator\n\n    def residuals(theta, u, y_data):\n        \"\"\"\n        Calculates the residuals between the model prediction and experimental data.\n        \n        Args:\n            theta (array-like): Parameter vector [S, Kd, n].\n            u (array-like): Inducer concentrations.\n            y_data (array-like): Experimental data points.\n        \n        Returns:\n            numpy.ndarray: Vector of residuals.\n        \"\"\"\n        return p_ss_model(theta, u) - y_data\n\n    final_results = []\n\n    for true_params in test_cases:\n        # Reset the parameter estimate to the initial guess for each new test case\n        theta_estimate = np.copy(initial_guess)\n\n        # Homotopy continuation loop through the scales\n        for s in scales:\n            # 1. Scale inputs for the current continuation step\n            scaled_u = s * base_u\n            \n            # 2. Generate synthetic \"experimental\" data for this scale using true parameters\n            y_data = p_ss_model(true_params, scaled_u)\n            \n            # 3. Perform nonlinear least-squares fitting\n            # The initial guess (x0) is the result from the previous, smaller scale\n            result = least_squares(\n                fun=residuals,\n                x0=theta_estimate,\n                bounds=bounds,\n                args=(scaled_u, y_data)\n            )\n            \n            # 4. Update the parameter estimate for the next iteration\n            theta_estimate = result.x\n        \n        # Store the final estimate for s=1.0\n        final_results.append(theta_estimate.tolist())\n\n    # Format the results for final output\n    # Round numbers to three decimal places\n    rounded_results = [[round(val, 3) for val in res] for res in final_results]\n    \n    # Convert to string and remove spaces to match the required format\n    output_str = str(rounded_results).replace(\" \", \"\")\n\n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```", "id": "2757696"}, {"introduction": "While steady-state models are useful, capturing the dynamics of biological systems requires Ordinary Differential Equations (ODEs). As these models grow in complexity, calculating the gradient of an objective function for optimization becomes a major computational bottleneck if done naively. This exercise introduces the continuous-time adjoint method, a highly efficient and scalable algorithm for computing parameter sensitivities. Mastering this technique is essential for tackling large-scale parameter estimation problems in systems biology, enabling the calibration of complex dynamic models against time-series data. [@problem_id:2757781]", "problem": "Consider a standard two-stage gene expression model that captures transcription and translation under an inducer, a scenario commonly encountered in synthetic biology when estimating parameters from time-series data. Let the messenger RNA (mRNA) concentration be $m(t)$ and the protein concentration be $p(t)$. The inducer concentration $I$ is constant and known. The dynamics are given by the system of ordinary differential equations\n$$\n\\frac{d}{dt}\\begin{bmatrix} m \\\\ p \\end{bmatrix}\n=\n\\begin{bmatrix}\nk_{\\mathrm{tx}}\\,H(I;K,n) - \\delta_m\\, m \\\\\nk_{\\mathrm{tl}}\\, m - \\delta_p\\, p\n\\end{bmatrix},\n\\quad\nH(I;K,n) \\equiv \\frac{1}{1 + \\left(\\frac{I}{K}\\right)^n},\n$$\nwhere the unknown parameter vector is\n$$\n\\theta \\equiv \\begin{bmatrix} k_{\\mathrm{tx}} & K & n & \\delta_m & k_{\\mathrm{tl}} & \\delta_p \\end{bmatrix}^{\\top}.\n$$\nAssume the initial condition $$x(0)=\\begin{bmatrix} m(0) & p(0) \\end{bmatrix}^{\\top} = \\begin{bmatrix} 0 & 0 \\end{bmatrix}^{\\top}$$. Measurements are collected only for the protein $p(t)$ at discrete times $0 < t_1 < \\cdots < t_M$, producing data $y_i$ with known positive weights $w_i$. Define the least-squares objective\n$$\nJ(\\theta) \\equiv \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left(p(t_i;\\theta) - y_i\\right)^2.\n$$\nAll variables and parameters are dimensionless.\n\nTask. Implement a program that computes the gradient $\\nabla_{\\theta}J$ using the continuous-time adjoint method for an objective with measurements at discrete times, starting from the adjoint equations derived from the Lagrangian construction. Between measurement times, the adjoint $\\lambda(t) \\in \\mathbb{R}^2$ satisfies the homogeneous linear adjoint dynamics implied by the forward system, with terminal condition $\\lambda(t_M^+) = \\mathbf{0}$. At each measurement time $t_i$, apply the jump condition induced by the discrete loss term. Use the resulting adjoint trajectory to evaluate the gradient as a time integral of $\\left(\\partial f/\\partial \\theta\\right)^{\\top}\\lambda$.\n\nYour implementation must:\n- Compute the forward state trajectory with a dense output so that $x(t)$ is available at arbitrary $t$.\n- Integrate the adjoint backward in time from $t_M$ to $0$, applying the jump at each $t_i$:\n$$\n\\lambda(t_i^-) = \\lambda(t_i^+) + \\frac{\\partial}{\\partial x}\\left(\\frac{1}{2}w_i\\left(p(t_i;\\theta)-y_i\\right)^2\\right),\n$$\nwhere the gradient with respect to $x$ is taken treating $p$ as the second component of $x$.\n- Accumulate the gradient using\n$$\n\\nabla_{\\theta}J(\\theta) = \\int_{0}^{t_M} \\left(\\frac{\\partial f}{\\partial \\theta}(t, x(t), \\theta)\\right)^{\\top}\\lambda(t)\\, dt,\n$$\nnoting that the discrete measurement terms do not depend explicitly on $\\theta$ in this setup.\n\nVerification requirement. For each test case below, also compute a central finite-difference approximation of $\\nabla_{\\theta}J$ with a per-parameter step size $h_j = \\max\\{10^{-6}\\cdot|\\theta_j|,\\,10^{-8}\\}$, and report the maximum relative discrepancy over components:\n$$\n\\varepsilon \\equiv \\max_{j\\in\\{1,\\dots,6\\}} \\frac{\\left|\\, \\left[\\nabla_{\\theta}J(\\theta)\\right]_j - \\left[\\nabla_{\\theta}J(\\theta)\\right]_j^{\\mathrm{FD}} \\,\\right|}{\\max\\left\\{1,\\left| \\left[\\nabla_{\\theta}J(\\theta)\\right]_j^{\\mathrm{FD}} \\right|\\right\\}}.\n$$\n\nTest suite. Use the following three independent test cases. In every case, generate synthetic data by simulating the forward model at the specified true parameter $\\theta^{\\mathrm{true}}$, then adding independent Gaussian noise of stated standard deviation to the protein values at the measurement times. Use the given random seed to initialize a pseudorandom number generator so that results are deterministic. Then evaluate the gradient at the specified $\\theta^{\\mathrm{eval}}$ and compute $\\varepsilon$ for that case.\n\n- Case A (happy path):\n  - Inducer: $I = 100$.\n  - Measurement times: $\\{0.25,\\,0.5,\\,1.0,\\,2.0,\\,3.0,\\,4.0\\}$.\n  - Weights: all equal to $1$.\n  - Initial condition: $m(0)=0$, $p(0)=0$.\n  - True parameters: $\\theta^{\\mathrm{true}} = [15.0,\\,50.0,\\,2.0,\\,0.3,\\,5.0,\\,0.2]^{\\top}$.\n  - Evaluation parameters: $\\theta^{\\mathrm{eval}} = [12.0,\\,60.0,\\,2.5,\\,0.25,\\,4.5,\\,0.25]^{\\top}$.\n  - Noise standard deviation: $0.05$.\n  - Random seed: $0$.\n\n- Case B (single terminal measurement, heavier weight):\n  - Inducer: $I = 80$.\n  - Measurement times: $\\{6.0\\}$.\n  - Weights: $\\{10.0\\}$.\n  - Initial condition: $m(0)=0$, $p(0)=0$.\n  - True parameters: $\\theta^{\\mathrm{true}} = [10.0,\\,40.0,\\,1.5,\\,0.4,\\,3.5,\\,0.1]^{\\top}$.\n  - Evaluation parameters: $\\theta^{\\mathrm{eval}} = [9.0,\\,50.0,\\,1.2,\\,0.35,\\,3.2,\\,0.12]^{\\top}$.\n  - Noise standard deviation: $0.0$.\n  - Random seed: $1$.\n\n- Case C (slow dynamics, long horizon):\n  - Inducer: $I = 120$.\n  - Measurement times: $\\{0.5,\\,1.0,\\,2.0,\\,4.0,\\,8.0,\\,12.0\\}$.\n  - Weights: all equal to $1$.\n  - Initial condition: $m(0)=0$, $p(0)=0$.\n  - True parameters: $\\theta^{\\mathrm{true}} = [8.0,\\,30.0,\\,3.0,\\,0.1,\\,2.0,\\,0.05]^{\\top}$.\n  - Evaluation parameters: $\\theta^{\\mathrm{eval}} = [7.5,\\,35.0,\\,2.8,\\,0.08,\\,1.8,\\,0.06]^{\\top}$.\n  - Noise standard deviation: $0.02$.\n  - Random seed: $2$.\n\nNumerical guidance:\n- Integrate the forward and adjoint systems with absolute tolerance $10^{-12}$ and relative tolerance $10^{-9}$.\n- Use a dense output for the forward integration to evaluate $x(t)$ during the backward adjoint integration.\n- All variables and parameters are dimensionless.\n\nFinal output. Your program should produce a single line of output containing the three values of $\\varepsilon$ for Cases A, B, and C, respectively, as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$). Each $r_i$ must be a floating-point number.", "solution": "The problem presented is a well-posed and scientifically grounded task in the field of computational systems biology, specifically addressing parameter sensitivity analysis for a common gene expression model. It is a canonical application of the continuous-time adjoint method for ordinary differential equations (ODEs) with a discrete-time objective function. The problem is complete, unambiguous, and computationally feasible. Therefore, I will proceed with a full solution.\n\nThe core of the problem is to compute the gradient $\\nabla_{\\theta}J$ of a least-squares objective function $J(\\theta)$ with respect to a parameter vector $\\theta$. The model describes the dynamics of messenger RNA (mRNA), $m(t)$, and protein, $p(t)$, concentrations.\n\n**1. Forward Model and Objective Function**\n\nThe state of the system is given by the vector $x(t) = [m(t), p(t)]^{\\top}$. The dynamics are governed by the following system of ODEs:\n$$\n\\frac{d x}{dt} = f(x, \\theta) =\n\\begin{bmatrix}\nk_{\\mathrm{tx}}\\,H(I;K,n) - \\delta_m\\, m(t) \\\\\nk_{\\mathrm{tl}}\\, m(t) - \\delta_p\\, p(t)\n\\end{bmatrix}\n$$\nwith the initial condition $x(0) = [0, 0]^{\\top}$. The regulatory term is a repressive Hill function $H(I;K,n) = \\frac{1}{1 + (I/K)^n}$, where $I$ is the constant inducer concentration. The parameter vector is $\\theta = [k_{\\mathrm{tx}}, K, n, \\delta_m, k_{\\mathrm{tl}}, \\delta_p]^{\\top}$.\n\nThe objective function measures the squared differences between the model's predicted protein concentration $p(t_i; \\theta)$ and experimental data $y_i$ at discrete measurement times $0 < t_1 < \\cdots < t_M$:\n$$\nJ(\\theta) \\equiv \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left(p(t_i;\\theta) - y_i\\right)^2\n$$\nwhere $w_i$ are positive weights.\n\n**2. The Continuous-Time Adjoint Method**\n\nThe gradient $\\nabla_{\\theta}J$ is found using the adjoint method, which avoids the computationally expensive approach of solving sensitivity equations for each parameter. This method involves a forward integration of the state equations and a backward integration of the adjoint equations.\n\nThe gradient is expressed as an integral over the time horizon $[0, t_M]$:\n$$\n\\nabla_{\\theta}J(\\theta) = \\int_{0}^{t_M} \\left(\\frac{\\partial f}{\\partial \\theta}(t, x(t), \\theta)\\right)^{\\top}\\lambda(t)\\, dt\n$$\nHere, $\\lambda(t) \\in \\mathbb{R}^2$ is the adjoint state vector, which is the solution to a linear ODE system integrated backward in time.\n\nThe dynamics of the adjoint state $\\lambda(t) = [\\lambda_m(t), \\lambda_p(t)]^\\top$ are given by:\n$$\n\\frac{d\\lambda}{dt} = -A(t)^{\\top} \\lambda(t),\n$$\nwhere $A(t) = \\frac{\\partial f}{\\partial x}$ is the Jacobian matrix of the forward system evaluated along the forward trajectory $x(t)$. For this problem, the Jacobian is constant:\n$$\nA = \\frac{\\partial f}{\\partial x} =\n\\begin{bmatrix}\n\\frac{\\partial f_m}{\\partial m} & \\frac{\\partial f_m}{\\partial p} \\\\\n\\frac{\\partial f_p}{\\partial m} & \\frac{\\partial f_p}{\\partial p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-\\delta_m & 0 \\\\\nk_{\\mathrm{tl}} & -\\delta_p\n\\end{bmatrix}\n$$\nThus, the adjoint system is:\n$$\n\\frac{d\\lambda}{dt} = -A^{\\top} \\lambda = -\\begin{bmatrix} -\\delta_m & k_{\\mathrm{tl}} \\\\ 0 & -\\delta_p \\end{bmatrix} \\lambda = \\begin{bmatrix} \\delta_m & -k_{\\mathrm{tl}} \\\\ 0 & \\delta_p \\end{bmatrix} \\lambda\n$$\n\nThe boundary conditions for this backward-in-time problem are composed of a terminal condition at $t_M$ and discrete `jumps` at each measurement time $t_i$. The terminal condition is $\\lambda(t_M^+) = \\mathbf{0}$. At each $t_i$, the adjoint state experiences a discontinuous jump:\n$$\n\\lambda(t_i^-) = \\lambda(t_i^+) + \\nabla_x \\left(\\frac{1}{2}w_i\\left(p(t_i) - y_i\\right)^2\\right)\n$$\nwhere $\\lambda(t_i^-)$ and $\\lambda(t_i^+)$ are the states just before and after the jump time $t_i$. The gradient of the cost term with respect to the state $x=[m, p]^\\top$ is:\n$$\n\\nabla_x \\left(\\frac{1}{2}w_i(p(t_i) - y_i)^2\\right) =\n\\begin{bmatrix} 0 \\\\ w_i (p(t_i) - y_i) \\end{bmatrix}\n$$\n\n**3. Augmented System for Gradient Computation**\n\nTo compute the gradient integral simultaneously with the backward integration of $\\lambda(t)$, we define an augmented state vector $Z(t) = [\\lambda(t)^\\top, G(t)^\\top]^\\top$, where $G(t) = \\int_{t}^{t_M} (\\frac{\\partial f}{\\partial \\theta})^\\top\\lambda(\\tau)d\\tau$. The gradient we seek is $G(0)$. The differential equation for $G(t)$ is $\\frac{dG}{dt} = -(\\frac{\\partial f}{\\partial \\theta})^\\top\\lambda(t)$, and the terminal condition is $G(t_M)=\\mathbf{0}$.\n\nThe partial derivatives of $f$ with respect to the parameters are required:\nLet $H = \\frac{1}{1+(I/K)^n}$.\n$$\n\\frac{\\partial f}{\\partial k_{\\mathrm{tx}}} = \\begin{bmatrix} H \\\\ 0 \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial K} = \\begin{bmatrix} k_{\\mathrm{tx}} \\frac{n}{K}H(1-H) \\\\ 0 \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial n} = \\begin{bmatrix} -k_{\\mathrm{tx}} \\ln(I/K) H(1-H) \\\\ 0 \\end{bmatrix}\n$$\n$$\n\\frac{\\partial f}{\\partial \\delta_m} = \\begin{bmatrix} -m \\\\ 0 \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial k_{\\mathrm{tl}}} = \\begin{bmatrix} 0 \\\\ m \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial \\delta_p} = \\begin{bmatrix} 0 \\\\ -p \\end{bmatrix}\n$$\nThe full augmented ODE system for backward integration is:\n$$\n\\frac{dZ}{dt} =\n\\begin{bmatrix}\n\\dot{\\lambda} \\\\\n\\dot{G}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\begin{bmatrix} \\delta_m & -k_{\\mathrm{tl}} \\\\ 0 & \\delta_p \\end{bmatrix} \\lambda \\\\\n- \\left(\\frac{\\partial f(t, x(t), \\theta)}{\\partial \\theta}\\right)^\\top \\lambda\n\\end{bmatrix}\n$$\n\n**4. Numerical Implementation Strategy**\n\nThe overall algorithm proceeds as follows:\n1.  **Generate Synthetic Data**: For each test case, simulate the forward model using the true parameters $\\theta^{\\mathrm{true}}$ to obtain $p(t_i;\\theta^{\\mathrm{true}})$. Add Gaussian noise as specified to generate the data points $y_i$.\n2.  **Forward Pass**: Solve the forward ODE system $\\dot{x} = f(x, \\theta^{\\mathrm{eval}})$ from $t=0$ to $t_M$ using the evaluation parameters $\\theta^{\\mathrm{eval}}$. This must be done with a dense output option to provide an interpolant for $x(t)$ at any time $t \\in [0, t_M]$.\n3.  **Backward Pass**:\n    a. Initialize the augmented state $Z(t_M) = \\mathbf{0} \\in \\mathbb{R}^8$.\n    b. Process the measurement times in reverse chronological order, from $t_M$ down to $t_1$.\n    c. For each measurement time $t_i$ ($i=M, \\dots, 1$):\n        i. Apply the jump to the adjoint components of $Z$: $\\lambda_p(t_i^-) = \\lambda_p(t_i^+) + w_i(p(t_i;\\theta^{\\mathrm{eval}}) - y_i)$.\n        ii. Integrate the augmented system $\\dot{Z}$ backward in time from $t_i$ to $t_{i-1}$ (using $t_0=0$).\n    d. The final state of the gradient components $G$ at $t=0$ is the desired gradient $\\nabla_{\\theta}J$.\n4.  **Finite Difference Verification**:\n    a. For each parameter $\\theta_j$, compute the central finite difference approximation of the gradient component $(\\nabla_{\\theta}J)_j^{\\mathrm{FD}}$ by perturbing $\\theta_j$ by $\\pm h_j$ and re-evaluating the objective function $J$. This requires two forward ODE solutions for each of the $6$ parameters.\n    b. Calculate the maximum relative discrepancy $\\varepsilon$ between the adjoint gradient and the finite difference approximation as specified in the problem statement.\n5.  **Report Results**: Collect the discrepancy $\\varepsilon$ for each test case.\n\nThis procedure, while complex, is a direct implementation of the adjoint sensitivity analysis theory and provides a powerful and efficient method for gradient computation in dynamic models. The use of high-precision numerical integrators is critical for accuracy, especially for the verification step.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A\n        {\n            \"I\": 100.0,\n            \"times\": np.array([0.25, 0.5, 1.0, 2.0, 3.0, 4.0]),\n            \"weights\": np.ones(6),\n            \"theta_true\": np.array([15.0, 50.0, 2.0, 0.3, 5.0, 0.2]),\n            \"theta_eval\": np.array([12.0, 60.0, 2.5, 0.25, 4.5, 0.25]),\n            \"noise_std\": 0.05,\n            \"seed\": 0,\n        },\n        # Case B\n        {\n            \"I\": 80.0,\n            \"times\": np.array([6.0]),\n            \"weights\": np.array([10.0]),\n            \"theta_true\": np.array([10.0, 40.0, 1.5, 0.4, 3.5, 0.1]),\n            \"theta_eval\": np.array([9.0, 50.0, 1.2, 0.35, 3.2, 0.12]),\n            \"noise_std\": 0.0,\n            \"seed\": 1,\n        },\n        # Case C\n        {\n            \"I\": 120.0,\n            \"times\": np.array([0.5, 1.0, 2.0, 4.0, 8.0, 12.0]),\n            \"weights\": np.ones(6),\n            \"theta_true\": np.array([8.0, 30.0, 3.0, 0.1, 2.0, 0.05]),\n            \"theta_eval\": np.array([7.5, 35.0, 2.8, 0.08, 1.8, 0.06]),\n            \"noise_std\": 0.02,\n            \"seed\": 2,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        discrepancy = process_case(case)\n        results.append(discrepancy)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef process_case(case_params):\n    \"\"\"\n    Processes a single test case: generates data, computes adjoint and finite\n    difference gradients, and returns the discrepancy.\n    \"\"\"\n    # Unpack case parameters\n    I = case_params[\"I\"]\n    t_meas = case_params[\"times\"]\n    weights = case_params[\"weights\"]\n    theta_true = case_params[\"theta_true\"]\n    theta_eval = case_params[\"theta_eval\"]\n    noise_std = case_params[\"noise_std\"]\n    seed = case_params[\"seed\"]\n\n    # Numerical settings\n    atol, rtol = 1e-12, 1e-9\n    x0 = np.array([0.0, 0.0])\n    t_max = t_meas[-1] if len(t_meas) > 0 else 0.0\n\n    # 1. Generate synthetic data\n    rng = np.random.default_rng(seed)\n\n    def forward_rhs(t, x, theta, I_val):\n        k_tx, K, n, delta_m, k_tl, delta_p = theta\n        m, p = x\n        hill = 1.0 / (1.0 + (I_val / K)**n)\n        dm_dt = k_tx * hill - delta_m * m\n        dp_dt = k_tl * m - delta_p * p\n        return np.array([dm_dt, dp_dt])\n\n    sol_true = solve_ivp(\n        lambda t, x: forward_rhs(t, x, theta_true, I),\n        (0, t_max), x0, atol=atol, rtol=rtol, dense_output=True\n    )\n    p_true_at_t = sol_true.sol(t_meas)[1]\n    noise = rng.normal(0, noise_std, size=len(t_meas))\n    y_data = p_true_at_t + noise\n    \n    data_map = dict(zip(t_meas, y_data))\n    weights_map = dict(zip(t_meas, weights))\n\n    # 2. Compute gradient with adjoint method\n    adjoint_grad = compute_adjoint_gradient(theta_eval, I, t_meas, data_map, weights_map, atol, rtol)\n\n    # 3. Compute gradient with finite differences\n    fd_grad = compute_fd_gradient(theta_eval, I, t_meas, y_data, weights, atol, rtol)\n\n    # 4. Compare gradients\n    numerator = np.abs(adjoint_grad - fd_grad)\n    denominator = np.maximum(1.0, np.abs(fd_grad))\n    relative_discrepancy = numerator / denominator\n    \n    return np.max(relative_discrepancy)\n\ndef compute_adjoint_gradient(theta, I, t_meas, data_map, weights_map, atol, rtol):\n    \"\"\"Computes the gradient using the continuous-time adjoint method.\"\"\"\n    k_tx, K, n, delta_m, k_tl, delta_p = theta\n    x0 = np.array([0.0, 0.0])\n    t_max = t_meas[-1] if len(t_meas) > 0 else 0.0\n\n    def forward_rhs(t, x, theta_loc, I_loc):\n        k_tx_loc, K_loc, n_loc, delta_m_loc, k_tl_loc, delta_p_loc = theta_loc\n        m, p = x\n        hill = 1.0 / (1.0 + (I_loc / K_loc)**n_loc)\n        dm_dt = k_tx_loc * hill - delta_m_loc * m\n        dp_dt = k_tl_loc * m - delta_p_loc * p\n        return np.array([dm_dt, dp_dt])\n    \n    # Forward pass\n    sol_fwd = solve_ivp(\n        lambda t, x: forward_rhs(t, x, theta, I), \n        (0, t_max), x0, atol=atol, rtol=rtol, dense_output=True\n    )\n\n    memoized_sol_fwd = sol_fwd.sol\n\n    def backward_rhs(t, z, theta_loc, I_loc):\n        k_tx_loc, K_loc, n_loc, delta_m_loc, k_tl_loc, delta_p_loc = theta_loc\n        lambda_m, lambda_p = z[0], z[1]\n        \n        m_t, p_t = memoized_sol_fwd(t)\n\n        # Adjoint dynamics\n        dlambda_m_dt = delta_m_loc * lambda_m - k_tl_loc * lambda_p\n        dlambda_p_dt = delta_p_loc * lambda_p\n        \n        # Gradient dynamics (dG/dt = - (df/dtheta)^T * lambda)\n        s = I_loc / K_loc\n        s_n = s**n_loc\n        h = 1.0 / (1.0 + s_n)\n        \n        # Note: some derivatives (df/dtheta) are time-dependent via m, p\n        dG_ktx_dt = -h * lambda_m\n        if K_loc > 0:\n            dG_K_dt = -k_tx_loc * (n_loc / K_loc) * h * (1 - h) * lambda_m\n        else:\n            dG_K_dt = 0.0\n        if I_loc > 0 and K_loc > 0:\n            dG_n_dt = k_tx_loc * np.log(s) * h * (1 - h) * lambda_m\n        else:\n            dG_n_dt = 0.0\n        dG_dm_dt = m_t * lambda_m\n        dG_ktl_dt = -m_t * lambda_p\n        dG_dp_dt = p_t * lambda_p\n\n        return np.array([\n            dlambda_m_dt, dlambda_p_dt,\n            dG_ktx_dt, dG_K_dt, dG_n_dt, dG_dm_dt, dG_ktl_dt, dG_dp_dt\n        ])\n\n    # Backward pass\n    z_aug = np.zeros(8)\n    sorted_times = sorted(t_meas, reverse=True)\n    integration_points = sorted_times + ([0.0] if t_max > 0 else [])\n    \n    unique_points = sorted(list(set(integration_points)), reverse=True)\n\n    for i in range(len(unique_points) - 1):\n        t_start = unique_points[i]\n        t_end = unique_points[i+1]\n\n        # Apply jump if t_start is a measurement time\n        if t_start in data_map:\n            p_at_t_start = memoized_sol_fwd(t_start)[1]\n            residual = p_at_t_start - data_map[t_start]\n            weight = weights_map[t_start]\n            z_aug[1] += weight * residual\n        \n        # Integrate backward\n        sol_bwd = solve_ivp(\n            lambda t, z: backward_rhs(t, z, theta, I),\n            (t_start, t_end), z_aug, atol=atol, rtol=rtol, method='LSODA'\n        )\n        z_aug = sol_bwd.y[:, -1]\n    \n    # Sign flip for gradient because of G(t) definition\n    return -z_aug[2:]\n\ndef compute_fd_gradient(theta, I, t_meas, y_data, weights, atol, rtol):\n    \"\"\"Computes the gradient using central finite differences.\"\"\"\n    grad_fd = np.zeros_like(theta)\n\n    def forward_rhs(t, x, theta_loc, I_loc):\n        k_tx_loc, K_loc, n_loc, delta_m_loc, k_tl_loc, delta_p_loc = theta_loc\n        m, p = x\n        hill = 1.0 / (1.0 + (I_loc / K_loc)**n_loc)\n        dm_dt = k_tx_loc * hill - delta_m_loc * m\n        dp_dt = k_tl_loc * m - delta_p_loc * p\n        return np.array([dm_dt, dp_dt])\n\n    def objective_J(theta_loc):\n        x0 = np.array([0.0, 0.0])\n        t_max = t_meas[-1] if len(t_meas) > 0 else 0.0\n        if t_max == 0:\n            return 0.0\n        \n        sol = solve_ivp(\n            lambda t, x: forward_rhs(t, x, theta_loc, I),\n            (0, t_max), x0, atol=atol, rtol=rtol, t_eval=t_meas\n        )\n        p_model = sol.y[1]\n        \n        if p_model.shape != y_data.shape: # Handle single measurement case with t_eval\n             return 0.5 * np.sum(weights * (np.array([p_model[0]]) - y_data)**2)\n\n        return 0.5 * np.sum(weights * (p_model - y_data)**2)\n\n    for j in range(len(theta)):\n        h_j = max(1e-6 * abs(theta[j]), 1e-8)\n        \n        theta_plus = np.copy(theta)\n        theta_plus[j] += h_j\n        \n        theta_minus = np.copy(theta)\n        theta_minus[j] -= h_j\n        \n        J_plus = objective_J(theta_plus)\n        J_minus = objective_J(theta_minus)\n        \n        grad_fd[j] = (J_plus - J_minus) / (2 * h_j)\n        \n    return grad_fd\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2757781"}, {"introduction": "Biological systems are inherently noisy, with significant cell-to-cell variability in parameters like protein expression rates. Instead of seeking a single \"best-fit\" parameter set, it is often more insightful to infer the *distribution* of parameters across a population. This advanced practice guides you through the construction of a hierarchical Bayesian model to do just that. You will learn to integrate out unobserved single-cell variability to connect a probabilistic model of heterogeneity to population-averaged data, allowing you to estimate the governing hyperparameters using maximum a posteriori (MAP) estimation. [@problem_id:2757765]", "problem": "A synthetic gene circuit expresses a fluorescent protein under an inducible promoter. Individual cells differ in their kinetic parameter for protein production due to extrinsic variability. Consider a hierarchical model in which the per-cell kinetic parameter is denoted by $k_i$ for cell $i$, and is assumed to be drawn independently from a lognormal distribution with parameters $\\mu$ and $\\sigma$, meaning that $\\log k_i$ is Gaussian with mean $\\mu$ and standard deviation $\\sigma$. Under a fixed induction condition $c$, the expected per-cell fluorescence is proportional to $k_i$ with a known, condition-specific proportionality constant $s_c$ determined by the inducer level and reporter calibration. The experiment measures, for each condition $c$, the sample mean of fluorescence across $n_c$ cells, producing a single observed value $y_c$. In addition, there is independent measurement noise on the reported sample mean with standard deviation $\\tau_c$.\n\nAssume the following model components:\n- The per-cell kinetic parameter $k_i$ is independent across cells with $\\log k_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$.\n- Conditional on $k_i$ and condition $c$, the expected fluorescence for cell $i$ is $s_c k_i$.\n- The reported sample mean $y_c$ is modeled as a single draw from a Gaussian distribution centered at the true population mean under condition $c$, with a variance equal to the sum of two contributions: the sampling variance of the cell-level fluorescence averaged over $n_c$ cells and a condition-specific measurement variance $\\tau_c^2$.\n- Priors: $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$ and $\\sigma \\sim \\text{Half-Normal}(b_0)$, with density for $\\sigma>0$ proportional to $\\exp\\{-\\sigma^2/(2 b_0^2)\\}$.\n\nYour task is to perform Bayesian parameter estimation by integrating out the cell-level variability and computing the maximum a posteriori (MAP) estimate of the hyperparameters $\\mu$ and $\\sigma$ given population-level observations. The integration should be carried out exactly using properties of the lognormal distribution to obtain the marginal likelihood for the observed sample means $y_c$ across conditions. Then combine with the stated priors to construct the posterior density $\\pi(\\mu,\\sigma \\mid \\{(y_c,s_c,n_c,\\tau_c)\\}_c)$ and compute the MAP estimate $(\\mu_{\\text{MAP}},\\sigma_{\\text{MAP}})$.\n\nFundamental bases you may assume without proof:\n- If $X$ is a random variable and $Y = a X + b$ with constants $a$ and $b$, then $\\mathbb{E}[Y] = a \\mathbb{E}[X] + b$ and $\\operatorname{Var}(Y) = a^2 \\operatorname{Var}(X)$.\n- The Central Limit Theorem implies that the sample mean of independent, identically distributed random variables has approximately Gaussian fluctuations around the true mean, with variance equal to the population variance divided by the sample size.\n- The definitions of the normal and lognormal distributions.\n\nConstruct a program that, for each test case below, computes the MAP estimate $(\\mu_{\\text{MAP}},\\sigma_{\\text{MAP}})$ by maximizing the log posterior over $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$. You may use any correct and numerically stable method. For numerical stability, enforce a lower bound of $\\sigma \\geq 10^{-6}$.\n\nUse the same prior hyperparameters for all test cases: $m_0 = 0$, $s_0 = 1$, $b_0 = 1$.\n\nTest suite (each test case provides arrays of observed means $y$, proportionality constants $s$, sample sizes $n$, and measurement noise standard deviations $\\tau$):\n- Test case 1:\n  - $y = {0.791, 1.442, 2.934}$\n  - $s = {0.5, 1.0, 2.0}$\n  - $n = {1000, 1000, 1000}$\n  - $\\tau = {0.05, 0.05, 0.05}$\n- Test case 2:\n  - $y = {0.9381, 2.7593}$\n  - $s = {1.0, 3.0}$\n  - $n = {200, 200}$\n  - $\\tau = {0.02, 0.02}$\n- Test case 3:\n  - $y = {0.23, 1.15, 5.2}$\n  - $s = {0.2, 1.2, 5.0}$\n  - $n = {50, 50, 50}$\n  - $\\tau = {0.1, 0.1, 0.1}$\n- Test case 4:\n  - $y = {1.842}$\n  - $s = {1.7}$\n  - $n = {1000}$\n  - $\\tau = {0.02}$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The results must be the concatenation, in order, of the rounded MAP estimates for each test case: for test case $j$, append $\\mu_{\\text{MAP}}$ and $\\sigma_{\\text{MAP}}$ rounded to six decimal places. For example, a valid output format with two test cases would be $[\\mu_{\\text{MAP},1},\\sigma_{\\text{MAP},1},\\mu_{\\text{MAP},2},\\sigma_{\\text{MAP},2}]$ with each value rounded to six decimals.", "solution": "The problem statement is subjected to rigorous validation.\n\n### Step 1: Extract Givens\n\n- **Model of per-cell kinetic parameter**: The kinetic parameter for cell $i$, denoted $k_i$, is drawn independently from a lognormal distribution. Specifically, $\\log k_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n- **Model of per-cell fluorescence**: The expected fluorescence for cell $i$ under condition $c$ is $s_c k_i$, where $s_c$ is a known, condition-specific proportionality constant.\n- **Model of observed data**: The reported sample mean of fluorescence across $n_c$ cells, $y_c$, is modeled as a single draw from a Gaussian distribution. The center of this distribution is the true population mean of cell-level fluorescence under condition $c$. The variance is the sum of two components: the sampling variance of the cell-level fluorescence averaged over $n_c$ cells, and a condition-specific measurement variance $\\tau_c^2$.\n- **Priors**: The prior for $\\mu$ is $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$. The prior for $\\sigma$ is $\\sigma \\sim \\text{Half-Normal}(b_0)$ for $\\sigma > 0$, with density proportional to $\\exp\\{-\\sigma^2/(2b_0^2)\\}$.\n- **Task**: Perform Bayesian parameter estimation to compute the maximum a posteriori (MAP) estimate of the hyperparameters $(\\mu_{\\text{MAP}}, \\sigma_{\\text{MAP}})$ given population-level observations $\\{(y_c, s_c, n_c, \\tau_c)\\}_c$.\n- **Numerical Constraint**: For numerical stability, enforce a lower bound of $\\sigma \\geq 10^{-6}$.\n- **Prior Hyperparameters**: For all test cases, $m_0 = 0$, $s_0 = 1$, $b_0 = 1$.\n- **Test Data**: Four test cases are provided, each consisting of arrays for $\\{y_c\\}$, $\\{s_c\\}$, $\\{n_c\\}$, and $\\{\\tau_c\\}$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the specified criteria.\n\n- **Scientifically Grounded**: The problem describes a hierarchical Bayesian model for gene expression, a standard and well-accepted approach in quantitative and synthetic biology. The use of a lognormal distribution for kinetic parameters reflects the common observation of right-skewed distributions for biological quantities. The model for observed data, which incorporates both biological (sampling) and technical (measurement) noise, is a statistically sound construction. The problem is firmly based on established principles of statistical modeling in biology.\n- **Well-Posed**: The task is to find the maximum of a posterior probability distribution. The priors for $\\mu$ (Normal) and $\\sigma$ (Half-Normal) are proper, and the likelihood function is derived from Gaussian assumptions. This structure leads to a well-defined objective function for optimization. The problem is structured to yield a unique MAP estimate.\n- **Objective**: The problem is formulated in precise, unambiguous mathematical language. All terms are defined, and the data are quantitative. There are no subjective or opinion-based statements.\n- **Completeness and Consistency**: The problem is self-contained. It provides all necessary components: the data-generating model, the prior distributions, all required parameters and hyperparameters, and the specific data for each test case. There are no apparent contradictions.\n- **Realism**: The model is a standard, albeit simplified, representation of a real biological experiment. The data values provided are plausible for fluorescence measurements.\n- **Structure**: The problem requires a multi-step derivation of the posterior followed by numerical optimization. This is a substantive and non-trivial task that correctly assesses understanding of Bayesian inference in a hierarchical context.\n\n### Step 3: Verdict and Action\n\nThe problem is deemed **valid**. It is scientifically sound, well-posed, and complete. A solution will be constructed.\n\n### Solution Derivation\n\nThe objective is to find the MAP estimate of the parameters $\\mu$ and $\\sigma$, which are the values that maximize the posterior probability density $\\pi(\\mu, \\sigma \\mid \\text{data})$. By Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\n\\pi(\\mu, \\sigma \\mid \\{y_c\\}_c) \\propto p(\\{y_c\\}_c \\mid \\mu, \\sigma) \\, p(\\mu) \\, p(\\sigma)\n$$\nMaximizing the posterior is equivalent to maximizing its logarithm, the log-posterior.\n\n**1. Derivation of the Marginal Likelihood**\n\nFirst, we derive the likelihood of the observed data $\\{y_c\\}_c$ by marginalizing out the unobserved cell-level parameters $k_i$.\n\nThe per-cell kinetic parameter $k_i$ is drawn from a lognormal distribution, $k_i \\sim \\text{LogNormal}(\\mu, \\sigma^2)$, meaning $\\log k_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The mean and variance of $k_i$ are:\n$$\n\\mathbb{E}[k_i] = \\exp(\\mu + \\sigma^2/2)\n$$\n$$\n\\operatorname{Var}(k_i) = (\\exp(\\sigma^2) - 1) \\exp(2\\mu + \\sigma^2)\n$$\nThe expected fluorescence for a single cell $i$ under condition $c$ is $F_{i,c} = s_c k_i$. Its mean and variance are:\n$$\n\\mathbb{E}[F_{i,c}] = s_c \\mathbb{E}[k_i] = s_c \\exp(\\mu + \\sigma^2/2)\n$$\n$$\n\\operatorname{Var}(F_{i,c}) = s_c^2 \\operatorname{Var}(k_i) = s_c^2 (\\exp(\\sigma^2) - 1) \\exp(2\\mu + \\sigma^2)\n$$\nThe problem states that the observed sample mean $y_c$ is drawn from a Gaussian distribution. The mean of this Gaussian is the true population mean of fluorescence, $\\mu_c(\\mu, \\sigma) = \\mathbb{E}[F_{i,c}]$.\n$$\n\\mu_c(\\mu, \\sigma) = s_c \\exp(\\mu + \\sigma^2/2)\n$$\nThe variance of this Gaussian, $\\sigma_c^2(\\mu, \\sigma)$, is the sum of the sampling variance of the mean over $n_c$ cells and the measurement variance $\\tau_c^2$.\n$$\n\\sigma_c^2(\\mu, \\sigma) = \\frac{\\operatorname{Var}(F_{i,c})}{n_c} + \\tau_c^2 = \\frac{s_c^2 (\\exp(\\sigma^2) - 1) \\exp(2\\mu + \\sigma^2)}{n_c} + \\tau_c^2\n$$\nThus, the model for a single observation $y_c$ is $y_c \\sim \\mathcal{N}(\\mu_c(\\mu, \\sigma), \\sigma_c^2(\\mu, \\sigma))$. The likelihood for observing $y_c$ is given by the Gaussian probability density function. Assuming observations for different conditions $c$ are independent, the total log-likelihood is:\n$$\n\\log L(\\mu, \\sigma) = \\log p(\\{y_c\\}_c \\mid \\mu, \\sigma) = \\sum_c \\log p(y_c \\mid \\mu, \\sigma)\n$$\n$$\n\\log L(\\mu, \\sigma) = -\\frac{1}{2} \\sum_c \\left[ \\log(2\\pi \\sigma_c^2(\\mu, \\sigma)) + \\frac{(y_c - \\mu_c(\\mu, \\sigma))^2}{\\sigma_c^2(\\mu, \\sigma)} \\right]\n$$\n\n**2. Assembling the Log-Posterior**\n\nThe log-priors for $\\mu$ and $\\sigma$ (ignoring normalization constants) are:\n$$\n\\log p(\\mu) = -\\frac{(\\mu - m_0)^2}{2s_0^2} + \\text{const}\n$$\n$$\n\\log p(\\sigma) = -\\frac{\\sigma^2}{2b_0^2} + \\text{const}, \\quad \\text{for } \\sigma > 0\n$$\nThe full log-posterior to be maximized is $\\log \\pi(\\mu, \\sigma) = \\log L(\\mu, \\sigma) + \\log p(\\mu) + \\log p(\\sigma)$.\n\n**3. Optimization Strategy**\n\nWe seek $(\\mu_{\\text{MAP}}, \\sigma_{\\text{MAP}}) = \\arg\\max_{\\mu, \\sigma>0} \\log \\pi(\\mu, \\sigma)$. This is equivalent to minimizing the negative log-posterior. For improved numerical stability, we re-parameterize the model. Let $\\phi = \\mu + \\sigma^2/2$. Then $\\mu = \\phi - \\sigma^2/2$. The expressions for the mean and variance of $y_c$ become more stable:\n$$\n\\mu_c(\\phi) = s_c \\exp(\\phi)\n$$\n$$\n\\exp(2\\mu + \\sigma^2) = \\exp(2(\\phi - \\sigma^2/2) + \\sigma^2) = \\exp(2\\phi)\n$$\n$$\n\\sigma_c^2(\\phi, \\sigma) = \\frac{s_c^2 (\\exp(\\sigma^2) - 1) \\exp(2\\phi)}{n_c} + \\tau_c^2 = \\frac{(\\exp(\\sigma^2) - 1) \\mu_c(\\phi)^2}{n_c} + \\tau_c^2\n$$\nThe objective function to be minimized is the negative log-posterior expressed in terms of $(\\phi, \\sigma)$, ignoring constants:\n$$\nJ(\\phi, \\sigma) = \\frac{1}{2} \\sum_c \\left[ \\log(\\sigma_c^2(\\phi, \\sigma)) + \\frac{(y_c - \\mu_c(\\phi))^2}{\\sigma_c^2(\\phi, \\sigma)} \\right] + \\frac{(\\phi - \\sigma^2/2 - m_0)^2}{2s_0^2} + \\frac{\\sigma^2}{2b_0^2}\n$$\nThis function is minimized with respect to $\\phi \\in \\mathbb{R}$ and $\\sigma > 0$. The constraint $\\sigma \\geq 10^{-6}$ will be imposed. The optimization is performed numerically using `scipy.optimize.minimize`. Once the optimal $(\\phi_{\\text{MAP}}, \\sigma_{\\text{MAP}})$ are found, we recover $\\mu_{\\text{MAP}}$ using the transformation:\n$$\n\\mu_{\\text{MAP}} = \\phi_{\\text{MAP}} - \\frac{\\sigma_{\\text{MAP}}^2}{2}\n$$\nThe following program implements this procedure for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the MAP estimation problem for all test cases.\n    \"\"\"\n    # Prior hyperparameters (fixed for all test cases)\n    m0 = 0.0\n    s0 = 1.0\n    b0 = 1.0\n    priors = (m0, s0, b0)\n\n    # Test suite as defined in the problem statement\n    test_cases = [\n        {\n            \"y\": np.array([0.791, 1.442, 2.934]),\n            \"s\": np.array([0.5, 1.0, 2.0]),\n            \"n\": np.array([1000, 1000, 1000]),\n            \"tau\": np.array([0.05, 0.05, 0.05]),\n        },\n        {\n            \"y\": np.array([0.9381, 2.7593]),\n            \"s\": np.array([1.0, 3.0]),\n            \"n\": np.array([200, 200]),\n            \"tau\": np.array([0.02, 0.02]),\n        },\n        {\n            \"y\": np.array([0.23, 1.15, 5.2]),\n            \"s\": np.array([0.2, 1.2, 5.0]),\n            \"n\": np.array([50, 50, 50]),\n            \"tau\": np.array([0.1, 0.1, 0.1]),\n        },\n        {\n            \"y\": np.array([1.842]),\n            \"s\": np.array([1.7]),\n            \"n\": np.array([1000]),\n            \"tau\": np.array([0.02]),\n        },\n    ]\n\n    results = []\n    for case_data in test_cases:\n        # Initial guess for optimization parameters [phi, sigma]\n        # A reasonable guess for phi = mu + sigma^2/2 is the log of the scaled observation\n        phi_guess = np.mean(np.log(case_data['y'] / case_data['s']))\n        sigma_guess = 0.5\n        x0 = [phi_guess, sigma_guess]\n\n        # Define bounds for the optimization: phi is unbounded, sigma >= 1e-6\n        bounds = [(None, None), (1e-6, None)]\n\n        # Run the optimization to minimize the negative log-posterior\n        result = minimize(\n            negative_log_posterior,\n            x0,\n            args=(case_data, priors),\n            bounds=bounds,\n            method='L-BFGS-B'\n        )\n\n        phi_map, sigma_map = result.x\n        \n        # Transform back from phi to mu\n        mu_map = phi_map - 0.5 * sigma_map**2\n\n        results.extend([round(mu_map, 6), round(sigma_map, 6)])\n    \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef negative_log_posterior(params, data, priors):\n    \"\"\"\n    Calculates the negative of the log-posterior probability density.\n    This is the objective function to be minimized.\n    Uses the numerically stable reparameterization (phi, sigma).\n    \n    Args:\n        params (list-like): A list or array [phi, sigma].\n        data (dict): A dictionary containing numpy arrays for y, s, n, tau.\n        priors (tuple): A tuple of prior hyperparameters (m0, s0, b0).\n        \n    Returns:\n        float: The value of the negative log-posterior.\n    \"\"\"\n    phi, sigma = params\n    y, s, n, tau = data[\"y\"], data[\"s\"], data[\"n\"], data[\"tau\"]\n    m0, s0, b0 = priors\n\n    # --- Log-prior term ---\n    # The original parameter mu is expressed in terms of phi and sigma\n    mu = phi - 0.5 * sigma**2\n    \n    # Calculate log-prior contribution (proportional to negative log-prior)\n    log_prior_cost = ( (mu - m0)**2 / (2 * s0**2) + \n                       sigma**2 / (2 * b0**2) )\n\n    # --- Log-likelihood term ---\n    try:\n        # Calculate mu_c = s_c * exp(phi)\n        exp_phi = np.exp(phi)\n        mu_c = s * exp_phi\n\n        # Use np.expm1 for precision: exp(x) - 1\n        exp_sigma2_minus_1 = np.expm1(sigma**2)\n\n        # Calculate sigma_c^2 using the reparameterized formula\n        sampling_variance = (exp_sigma2_minus_1 * mu_c**2) / n\n        sigma_c_sq = sampling_variance + tau**2\n        \n        # Guard against non-positive variance which can occur during optimization search\n        if np.any(sigma_c_sq <= 0):\n            return np.inf\n\n        # Calculate log-likelihood contribution (proportional to negative log-likelihood)\n        log_likelihood_cost = 0.5 * np.sum(np.log(sigma_c_sq) + (y - mu_c)**2 / sigma_c_sq)\n\n    except (OverflowError, RuntimeWarning):\n        # Return a large value if numerical issues arise (e.g., overflow in exp)\n        return np.inf\n\n    # Total cost is the sum of prior and likelihood costs\n    total_cost = log_prior_cost + log_likelihood_cost\n    \n    return total_cost\n\n# Execute the main function\nsolve()\n\n```", "id": "2757765"}]}