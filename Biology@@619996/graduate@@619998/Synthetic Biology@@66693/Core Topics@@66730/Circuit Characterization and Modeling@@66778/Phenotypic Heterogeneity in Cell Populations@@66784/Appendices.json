{"hands_on_practices": [{"introduction": "A fundamental task in quantitative biology is to connect observable statistics from a cell population to the underlying molecular mechanisms. This exercise provides practice in using the most basic summary statistics of a distribution—the mean and variance—to make a powerful inference about the nature of gene expression noise. By calculating the squared coefficient of variation and the Fano factor from hypothetical experimental data, you will learn to distinguish the signature of transcriptional bursting, a key source of heterogeneity, from simpler noise profiles [@problem_id:2759682].", "problem": "In a clonal population of engineered cells expressing a fluorescent reporter from a constitutive promoter, single-cell protein copy numbers are quantified at steady state from calibrated microscopy. The measured population mean and variance are $\\langle X \\rangle = 100$ and $\\mathrm{Var}(X) = 5000$, respectively. Using the definition of the coefficient of variation (CV), where the coefficient of variation is $\\mathrm{CV} \\equiv \\sqrt{\\mathrm{Var}(X)}/\\langle X \\rangle$, compute the squared coefficient of variation, $\\mathrm{CV}^{2}$, from these measurements.\n\nThen, reason from first principles of stochastic gene expression in the Central Dogma of molecular biology (transcription and translation as stochastic birth–death processes) to decide which of the following is the more plausible dominant contributor to the observed heterogeneity in protein copy number: transcriptional bursting or translation shot noise. You may use the following widely accepted facts as foundational starting points: for a Poisson birth–death process, the variance equals the mean, and overdispersed counts with variance substantially exceeding the mean are a hallmark of bursty production.\n\nProvide the exact value of $\\mathrm{CV}^{2}$ as a dimensionless number with no units. No rounding is required. After giving the numerical value, justify your choice qualitatively in words (your justification will be evaluated separately from the numerical answer).", "solution": "The problem requires the calculation of the squared coefficient of variation, $\\mathrm{CV}^{2}$, from provided experimental data, and a subsequent reasoned argument to identify the dominant source of noise in protein expression.\n\nFirst, the problem must be validated.\nThe givens are:\n- A clonal cell population with a reporter gene.\n- Steady-state single-cell protein copy number is a random variable, $X$.\n- The measured population mean of protein copy number is $\\langle X \\rangle = 100$.\n- The measured population variance of protein copy number is $\\mathrm{Var}(X) = 5000$.\n- The definition of the coefficient of variation is $\\mathrm{CV} \\equiv \\frac{\\sqrt{\\mathrm{Var}(X)}}{\\langle X \\rangle}$.\n- Foundational principles: For a Poisson birth-death process, the variance equals the mean. Overdispersion, where variance substantially exceeds the mean, is a signature of bursty production.\n\nThe problem is scientifically grounded, well-posed, objective, and internally consistent. It describes a standard problem in quantitative biology with realistic parameters. All information required for a solution is provided. The problem is valid.\n\nWe proceed to the solution.\n\nPart 1: Calculation of $\\mathrm{CV}^{2}$.\nThe coefficient of variation is defined as $\\mathrm{CV} = \\frac{\\sqrt{\\mathrm{Var}(X)}}{\\langle X \\rangle}$.\nSquaring this expression yields the formula for the squared coefficient of variation:\n$$\n\\mathrm{CV}^{2} = \\left( \\frac{\\sqrt{\\mathrm{Var}(X)}}{\\langle X \\rangle} \\right)^2 = \\frac{\\mathrm{Var}(X)}{\\langle X \\rangle^2}\n$$\nWe are given the experimental values for the mean and variance:\n$$\n\\langle X \\rangle = 100\n$$\n$$\n\\mathrm{Var}(X) = 5000\n$$\nSubstituting these values into the expression for $\\mathrm{CV}^{2}$:\n$$\n\\mathrm{CV}^{2} = \\frac{5000}{(100)^2} = \\frac{5000}{10000} = \\frac{1}{2} = 0.5\n$$\nThe squared coefficient of variation is a dimensionless quantity.\n\nPart 2: Identification of the dominant noise source.\nThe Central Dogma describes gene expression as a two-stage process: transcription (DNA to mRNA) and translation (mRNA to protein). Both steps are inherently stochastic, contributing to cell-to-cell variability in protein numbers, $X$. We must determine whether noise from transcription or translation is dominant.\n\nLet us consider the two possibilities presented:\nHypothesis A: The dominant noise source is \"translation shot noise\". This term describes the stochasticity inherent in the synthesis and degradation of individual protein molecules. If the mRNA template concentration were stable and non-fluctuating, protein synthesis would be a simple birth-death process. According to the foundational principles given, such a process, if Poissonian, results in a protein distribution where the variance is equal to the mean.\n$$\n\\mathrm{Var}(X) = \\langle X \\rangle\n$$\nThis corresponds to a Fano factor, defined as $F \\equiv \\frac{\\mathrm{Var}(X)}{\\langle X \\rangle}$, equal to $1$.\n\nHypothesis B: The dominant noise source is \"transcriptional bursting\". This implies that the transcription process itself is highly intermittent. The gene promoter stochastically switches between an active state, producing mRNA molecules in bursts, and an inactive state. This leads to large, sporadic fluctuations in mRNA copy number. These upstream fluctuations in the mRNA template are then transmitted and amplified during translation, resulting in a protein distribution that is \"overdispersed\"—meaning its variance is substantially larger than its mean.\n$$\n\\mathrm{Var}(X) \\gg \\langle X \\rangle\n$$\nThis corresponds to a Fano factor $F \\gg 1$.\n\nNow, we evaluate these hypotheses against the experimental data. The measured mean is $\\langle X \\rangle = 100$ and the variance is $\\mathrm{Var}(X) = 5000$.\nLet us calculate the Fano factor for the observed protein distribution:\n$$\nF = \\frac{\\mathrm{Var}(X)}{\\langle X \\rangle} = \\frac{5000}{100} = 50\n$$\nOur result is $F = 50$. This value is substantially greater than $1$. The variance is $50$ times larger than the mean.\n\nComparing this result to our hypotheses:\n- Hypothesis A predicts $F=1$. The data show $F=50$. This is a severe discrepancy. Therefore, the data strongly contradict the hypothesis that translation shot noise from a stable mRNA pool is the dominant contributor to heterogeneity.\n- Hypothesis B predicts $F \\gg 1$. The data show $F=50$, which is consistent with this prediction. The observation that $\\mathrm{Var}(X)$ dramatically exceeds $\\langle X \\rangle$ is the classic hallmark of bursty gene expression, where the primary source of noise is the stochastic production of mRNA.\n\nConclusion: The large observed variance relative to the mean is incompatible with a simple Poisson model of protein production. It is, however, the direct signature of a burst-like production mechanism. The large fluctuations must originate upstream of translation. Therefore, transcriptional bursting is the more plausible dominant contributor to the observed phenotypic heterogeneity.", "answer": "$$\n\\boxed{0.5}\n$$", "id": "2759682"}, {"introduction": "While interpreting summary statistics is insightful, deriving them from first principles provides a deeper understanding of how different processes contribute to phenotypic heterogeneity. This problem challenges you to build a stochastic model for a stable protein in a growing and dividing bacterial population, accounting for both gene expression and partitioning at division. By deriving the exact steady-state variance from the model's master equation, you will see precisely how production \"shot noise\" and division-related \"partitioning noise\" combine to shape the overall cell-to-cell variability [@problem_id:2759671].", "problem": "A constitutive gene in a bacterial synthetic circuit expresses a stable protein whose only loss mechanism is dilution by cell division in an exponentially growing, asynchronous population. Assume the following biophysically grounded model.\n\n1. Production occurs as a memoryless birth process at constant rate $k$ per cell, so that in an infinitesimal interval $\\mathrm{d}t$ the probability of one new protein is $k\\,\\mathrm{d}t$ and multiple births are negligible to first order in $\\mathrm{d}t$.\n2. Cell divisions occur as a memoryless process at rate $\\lambda$ per cell (exponentially distributed interdivision times with mean $1/\\lambda$). At each division, protein copies are partitioned independently to the two daughters so that, for a mother cell with $x$ proteins, the copy number $Y$ in a tracked random daughter is distributed as $\\mathrm{Binomial}(x, 1/2)$.\n3. You follow one random lineage by choosing one daughter uniformly at each division event. The protein does not actively degrade.\n\nStarting only from the above stochastic rules, and the standard definitions of the Chemical Master Equation (CME) and conditional expectation, derive the exact steady-state variance $\\mathrm{Var}[X]$ of the protein copy number $X$ across the asynchronous population. Your derivation must explicitly account for contributions from both production noise and partitioning noise and should proceed from first principles by writing and solving the evolution equations for the first and second moments.\n\nExpress your final answer as a single closed-form analytical expression in terms of $k$ and $\\lambda$ only. Do not include units in your final answer. No numerical rounding is required.", "solution": "We model the single-cell protein count $X(t)$ as a continuous-time jump process with two reaction channels:\n\n1. Production (birth): $X \\to X+1$ at rate $k$.\n2. Division with binomial partitioning: $X \\to Y$, where $Y \\mid X=x \\sim \\mathrm{Binomial}(x, 1/2)$, at rate $\\lambda$.\n\nLet $m(t) \\equiv \\mathbb{E}[X(t)]$ and $s(t) \\equiv \\mathbb{E}[X(t)^{2}]$. For a general jump process, Dynkin’s formula (equivalently, the moment equations derived from the Chemical Master Equation (CME)) gives, for any function $f$ of the state,\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\,\\mathbb{E}[f(X)] \\;=\\; \\mathbb{E}\\Big[ \\sum_{\\text{events}} \\text{rate} \\times \\big(f(\\text{post-state}) - f(\\text{pre-state})\\big) \\Big].\n$$\n\nFirst moment. Choose $f(X)=X$.\n\n- Production contributes $k \\times \\big((X+1)-X\\big)=k \\times 1$.\n- Division contributes $\\lambda \\times \\mathbb{E}[Y - X \\mid X]$. Since $Y \\mid X=x \\sim \\mathrm{Binomial}(x, 1/2)$, we have $\\mathbb{E}[Y \\mid X=x]=x/2$, so $\\mathbb{E}[Y - X \\mid X=x] = -x/2$.\n\nTaking expectations gives\n$$\n\\frac{\\mathrm{d}m}{\\mathrm{d}t} \\;=\\; k \\;+\\; \\lambda \\,\\mathbb{E}\\!\\left[-\\frac{X}{2}\\right] \\;=\\; k \\;-\\; \\frac{\\lambda}{2}\\,m.\n$$\nAt steady state, $\\mathrm{d}m/\\mathrm{d}t=0$, so\n$$\nm_{\\ast} \\;=\\; \\frac{2k}{\\lambda}.\n$$\n\nSecond moment. Choose $f(X)=X^{2}$.\n\n- Production: $(X+1)^{2}-X^{2} = 2X+1$, so the contribution is $k\\,\\mathbb{E}[2X+1]=2k\\,m + k$.\n- Division: We need $\\mathbb{E}[Y^{2} - X^{2} \\mid X]$. For $Y \\mid X=x \\sim \\mathrm{Binomial}(x, 1/2)$, we have $\\mathbb{E}[Y \\mid x]=x/2$ and $\\mathrm{Var}(Y \\mid x)=x\\cdot \\tfrac{1}{2}\\cdot \\tfrac{1}{2} = x/4$. Hence\n$$\n\\mathbb{E}[Y^{2} \\mid x] \\;=\\; \\mathrm{Var}(Y \\mid x) + \\big(\\mathbb{E}[Y \\mid x]\\big)^{2} \\;=\\; \\frac{x}{4} + \\frac{x^{2}}{4}.\n$$\nTherefore,\n$$\n\\mathbb{E}[Y^{2} - X^{2} \\mid X=x] \\;=\\; \\left(\\frac{x}{4} + \\frac{x^{2}}{4}\\right) - x^{2} \\;=\\; \\frac{x}{4} - \\frac{3x^{2}}{4}.\n$$\nTaking expectations,\n$$\n\\frac{\\mathrm{d}s}{\\mathrm{d}t} \\;=\\; 2k\\,m + k \\;+\\; \\lambda\\left(\\frac{1}{4}m - \\frac{3}{4}s\\right).\n$$\nAt steady state, $\\mathrm{d}s/\\mathrm{d}t=0$, so\n$$\n0 \\;=\\; 2k\\,m + k \\;+\\; \\frac{\\lambda}{4}m \\;-\\; \\frac{3\\lambda}{4}s.\n$$\nSubstitute $m_{\\ast} = 2k/\\lambda$ to solve for $s_{\\ast}$:\n$$\n0 \\;=\\; 2k\\left(\\frac{2k}{\\lambda}\\right) + k + \\frac{\\lambda}{4}\\left(\\frac{2k}{\\lambda}\\right) - \\frac{3\\lambda}{4}s_{\\ast}\n\\;=\\; \\frac{4k^{2}}{\\lambda} + k + \\frac{k}{2} - \\frac{3\\lambda}{4}s_{\\ast}.\n$$\nThus\n$$\n\\frac{3\\lambda}{4}s_{\\ast} \\;=\\; \\frac{4k^{2}}{\\lambda} + \\frac{3k}{2}\n\\quad\\Longrightarrow\\quad\ns_{\\ast} \\;=\\; \\frac{4}{3\\lambda}\\left(\\frac{4k^{2}}{\\lambda} + \\frac{3k}{2}\\right)\n\\;=\\; \\frac{16k^{2}}{3\\lambda^{2}} + \\frac{2k}{\\lambda}.\n$$\n\nVariance. The steady-state variance is $v_{\\ast} \\equiv \\mathrm{Var}[X]_{\\ast} = s_{\\ast} - m_{\\ast}^{2}$. Using $m_{\\ast}=2k/\\lambda$,\n$$\nv_{\\ast}\n\\;=\\; \\left(\\frac{16k^{2}}{3\\lambda^{2}} + \\frac{2k}{\\lambda}\\right) - \\left(\\frac{2k}{\\lambda}\\right)^{2}\n\\;=\\; \\frac{16k^{2}}{3\\lambda^{2}} + \\frac{2k}{\\lambda} - \\frac{4k^{2}}{\\lambda^{2}}\n\\;=\\; \\frac{4k^{2}}{3\\lambda^{2}} + \\frac{2k}{\\lambda}.\n$$\n\nInterpretation as contributions from production and partitioning noise. Write $m_{\\ast} = 2k/\\lambda$. Then\n$$\nv_{\\ast} \\;=\\; m_{\\ast} \\;+\\; \\frac{1}{3}m_{\\ast}^{2}.\n$$\nThe term linear in $m_{\\ast}$ comes from the shot noise of Poisson production, while the quadratic term $(1/3)m_{\\ast}^{2}$ arises from stochastic binomial partitioning at division (its coefficient depends on the exponential interdivision-time statistics encoded by $\\lambda$).\n\nExpressed solely in terms of $k$ and $\\lambda$, the steady-state variance is\n$$\nv_{\\ast} \\;=\\; \\frac{2k}{\\lambda} \\;+\\; \\frac{4}{3}\\left(\\frac{k}{\\lambda}\\right)^{2}.\n$$", "answer": "$$\\boxed{\\frac{2k}{\\lambda}+\\frac{4}{3}\\left(\\frac{k}{\\lambda}\\right)^{2}}$$", "id": "2759671"}, {"introduction": "Comparing models to data using only a few moments like the mean and variance can be limiting, as different distribution shapes can share the same moments. This computational exercise introduces a more rigorous method for model validation based on information theory: the Kullback-Leibler (KL) divergence. You will implement the calculation of $D_{\\mathrm{KL}}(P \\Vert Q)$ to quantify the \"distance\" between an empirical data distribution ($P$) and a theoretical model's prediction ($Q$), a powerful technique for hypothesis testing and selecting the most plausible mechanistic model [@problem_id:2759675].", "problem": "In a synthetic biology experiment studying phenotypic heterogeneity in isogenic cell populations, single-cell reporter measurements yield empirical count histograms of gene expression levels across a finite set of discrete expression categories (bins). Mechanistic hypotheses about the sources of noise (intrinsic, extrinsic, or mixed) are represented by candidate models that induce discrete distributions over the same bins. To compare hypotheses, implement a program that, using only foundational definitions from information theory and probability, computes the Kullback–Leibler (KL) divergence between the empirical distribution and each candidate mechanistic distribution for a set of test cases.\n\nAssumptions and definitions:\n- The central dogma of molecular biology and well-established stochastic gene expression models imply that cell-to-cell variability arises from probabilistic processes; here, variability is summarized by discrete distributions over expression bins.\n- Let the empirical histogram be a vector of nonnegative counts $\\{C_i\\}_{i=1}^n$ over $n$ bins, and let the candidate model be a vector of nonnegative raw intensities $\\{R_i\\}_{i=1}^n$ over the same bins. The raw intensities need not be normalized.\n- To avoid undefined behavior when any model assigns zero probability to a bin with nonzero empirical probability, enforce absolute continuity via additive smoothing with a small positive pseudocount $\\alpha$.\n- Use a single, global pseudocount $\\alpha = 10^{-12}$ added to every bin of both the empirical counts and candidate model intensities before normalization.\n- Convert smoothed counts and intensities to probability mass functions via the law of total probability: for each bin $i$, define $P_i = \\dfrac{C_i + \\alpha}{\\sum_{j=1}^n (C_j + \\alpha)}$ and $Q_i = \\dfrac{R_i + \\alpha}{\\sum_{j=1}^n (R_j + \\alpha)}$.\n- Using the definition of Kullback–Leibler divergence from information theory, compute the divergence $D_{\\mathrm{KL}}(P \\Vert Q)$ between the empirical distribution $P$ and the candidate model distribution $Q$ for each test case, using the natural logarithm so the result is in nats.\n\nImplementation constraints:\n- Treat each test case independently.\n- For each test case, compute a single real number equal to $D_{\\mathrm{KL}}(P \\Vert Q)$.\n- Express each result in nats as a float rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$).\n\nTest suite (each case is a pair: empirical counts, candidate model raw intensities):\n- Case $\\#1$: empirical counts $[50, 100, 150, 125, 75]$, candidate model raw intensities $[0.5, 1.0, 1.5, 1.25, 0.75]$.\n- Case $\\#2$: empirical counts $[80, 0, 0, 0, 20]$, candidate model raw intensities $[0.0, 1.0, 0.0, 1.0, 0.0]$.\n- Case $\\#3$: empirical counts $[5, 40, 10, 40, 5]$, candidate model raw intensities $[0.05, 0.2, 0.5, 0.2, 0.05]$.\n- Case $\\#4$: empirical counts $[0, 0, 100, 0, 0]$, candidate model raw intensities $[1, 1, 1, 1, 1]$.\n- Case $\\#5$: empirical counts $[1, 2, 3, 0, 0]$, candidate model raw intensities $[1000, 2000, 3000, 0, 0]$.\n\nAngle units are not applicable. No physical units are involved. The final output must be a single line in the exact format $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$, with each value rounded to six decimal places and expressed in nats as a decimal number without a percentage sign.", "solution": "The problem statement is coherent, scientifically grounded, and well-posed. All necessary definitions, constants, and data are provided to execute the task. It is a direct application of foundational principles from information theory to a standard problem in quantitative biology. Therefore, the problem is valid, and a solution will be presented.\n\nThe objective is to compute the Kullback-Leibler ($D_{\\mathrm{KL}}$) divergence between an empirical probability distribution $P$ and a model-derived probability distribution $Q$. Both distributions are defined over a discrete set of $n$ bins.\n\nLet the empirical data be given by a vector of counts $C = \\{C_i\\}_{i=1}^n$, and the candidate model by a vector of raw, unnormalized intensities $R = \\{R_i\\}_{i=1}^n$. The problem specifies a procedure of additive smoothing to ensure absolute continuity, which is necessary to prevent division by zero or the logarithm of zero in the $D_{\\mathrm{KL}}$ formula. A single pseudocount, $\\alpha = 10^{-12}$, is added to each bin for both $C$ and $R$.\n\nFirst, we define the smoothed counts, $C'_i$, and smoothed intensities, $R'_i$:\n$$ C'_i = C_i + \\alpha $$\n$$ R'_i = R_i + \\alpha $$\nfor each bin $i$ from $1$ to $n$.\n\nNext, these smoothed values are normalized to form valid probability mass functions, $P = \\{P_i\\}_{i=1}^n$ and $Q = \\{Q_i\\}_{i=1}^n$. Normalization is performed by dividing by the sum of all smoothed values, in accordance with the law of total probability.\n\nThe empirical probability $P_i$ for bin $i$ is:\n$$ P_i = \\frac{C'_i}{\\sum_{j=1}^n C'_j} = \\frac{C_i + \\alpha}{\\sum_{j=1}^n (C_j + \\alpha)} $$\n\nThe model probability $Q_i$ for bin $i$ is:\n$$ Q_i = \\frac{R'_i}{\\sum_{j=1}^n R'_j} = \\frac{R_i + \\alpha}{\\sum_{j=1}^n (R_j + \\alpha)} $$\n\nWith both distributions $P$ and $Q$ defined, the Kullback-Leibler divergence from $Q$ to $P$, denoted $D_{\\mathrm{KL}}(P \\Vert Q)$, is calculated. It measures the information lost when $Q$ is used to approximate $P$. The problem specifies the use of the natural logarithm, so the units of divergence will be nats. The definition is:\n$$ D_{\\mathrm{KL}}(P \\Vert Q) = \\sum_{i=1}^n P_i \\ln\\left(\\frac{P_i}{Q_i}\\right) $$\nThe use of the smoothing constant $\\alpha > 0$ ensures that $P_i > 0$ and $Q_i > 0$ for all $i$ (as long as not all input counts or intensities are zero), so the ratio $P_i/Q_i$ is always well-defined and positive, and the logarithm is always defined.\n\nThe computational procedure for each test case is as follows:\n1. Given the vectors of empirical counts $C$ and model intensities $R$.\n2. Define the pseudocount $\\alpha = 10^{-12}$.\n3. Create the smoothed vectors $C'$ and $R'$ by adding $\\alpha$ to every element.\n4. Calculate the total sum of the smoothed vectors: $S_P = \\sum C'_i$ and $S_Q = \\sum R'_i$.\n5. Normalize to obtain the probability distributions: $P_i = C'_i / S_P$ and $Q_i = R'_i / S_Q$.\n6. Compute the $D_{\\mathrm{KL}}(P \\Vert Q)$ by summing the terms $P_i \\times (\\ln(P_i) - \\ln(Q_i))$ over all bins $i$.\n7. Round the final result to six decimal places.\n\nFor example, let us consider Case $4$: $C = [0, 0, 100, 0, 0]$ and $R = [1, 1, 1, 1, 1]$. Here, $n=5$.\n- Smoothed counts $C' = [10^{-12}, 10^{-12}, 100+10^{-12}, 10^{-12}, 10^{-12}]$.\n- Sum of smoothed counts $S_P = 100 + 5 \\times 10^{-12}$.\n- Smoothed intensities $R' = [1+10^{-12}, 1+10^{-12}, 1+10^{-12}, 1+10^{-12}, 1+10^{-12}]$.\n- Sum of smoothed intensities $S_Q = 5 + 5 \\times 10^{-12}$.\n- The distributions $P$ and $Q$ are calculated by normalizing $C'$ and $R'$.\n- $P$ will be a distribution sharply peaked at the third bin, with $P_3 \\approx 1$.\n- $Q$ will be an almost uniform distribution, with $Q_i \\approx 0.2$ for all $i$.\n- The divergence will be dominated by the third term: $D_{\\mathrm{KL}}(P \\Vert Q) \\approx P_3 \\ln(P_3/Q_3) \\approx 1 \\times \\ln(1/0.2) = \\ln(5) \\approx 1.609438$. The contribution from other bins where $P_i$ is very small will be negligible.\n\nThis procedure will be applied to all provided test cases to generate the final list of results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Kullback-Leibler divergence for a set of test cases comparing\n    empirical and model distributions in a synthetic biology context.\n    \"\"\"\n    # Define the pseudocount for additive smoothing.\n    alpha = 1e-12\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (empirical_counts, model_raw_intensities)\n    test_cases = [\n        # Case #1\n        ([50, 100, 150, 125, 75], [0.5, 1.0, 1.5, 1.25, 0.75]),\n        # Case #2\n        ([80, 0, 0, 0, 20], [0.0, 1.0, 0.0, 1.0, 0.0]),\n        # Case #3\n        ([5, 40, 10, 40, 5], [0.05, 0.2, 0.5, 0.2, 0.05]),\n        # Case #4\n        ([0, 0, 100, 0, 0], [1, 1, 1, 1, 1]),\n        # Case #5\n        ([1, 2, 3, 0, 0], [1000, 2000, 3000, 0, 0]),\n    ]\n\n    results = []\n    for counts, intensities in test_cases:\n        # Convert lists to NumPy arrays for vectorized operations.\n        # Ensure float type for precision.\n        C = np.array(counts, dtype=np.float64)\n        R = np.array(intensities, dtype=np.float64)\n\n        # 1. Apply additive smoothing with the pseudocount alpha.\n        C_smoothed = C + alpha\n        R_smoothed = R + alpha\n\n        # 2. Normalize to get the probability mass functions P and Q.\n        # The law of total probability is applied.\n        P = C_smoothed / np.sum(C_smoothed)\n        Q = R_smoothed / np.sum(R_smoothed)\n        \n        # 3. Compute the Kullback-Leibler divergence D_KL(P || Q).\n        # The formula is sum(P_i * log(P_i / Q_i)).\n        # We must handle the case where P_i is near zero due to smoothing.\n        # The term P_i * log(P_i) approaches 0 as P_i -> 0.\n        # Modern numerical libraries handle this gracefully.\n        # The use of np.log implies the natural logarithm, giving a result in \"nats\".\n        \n        # A stable way to compute this is Sum(P * (log(P) - log(Q))).\n        # The problem statement ensures P and Q are strictly positive due to alpha.\n        kl_divergence = np.sum(P * (np.log(P) - np.log(Q)))\n\n        # 4. Round the result to six decimal places as required.\n        rounded_result = round(kl_divergence, 6)\n        results.append(rounded_result)\n\n    # Format the final output as a comma-separated list in brackets.\n    output_string = f\"[{','.join(map(str, results))}]\"\n    print(output_string)\n\nsolve()\n```", "id": "2759675"}]}