{"hands_on_practices": [{"introduction": "The \"Test\" phase of the DBTL cycle is where designs are empirically validated. A cornerstone of this phase is the quantitative characterization of a synthetic circuit, often by measuring its output across a range of input signals. This practice problem [@problem_id:1428071] focuses on the precise preparation of an inducer concentration gradient using serial dilutions, a fundamental laboratory technique crucial for generating the high-quality dose-response data needed to \"Learn\" about a system's behavior.", "problem": "A student in a systems biology laboratory is executing the \"Test\" phase of a design-build-test-learn cycle. They are characterizing a synthetic genetic circuit engineered into *E. coli*. This circuit is designed to express a fluorescent protein, with the level of fluorescence being dependent on the concentration of an external inducer molecule, L-arabinose. To create a dose-response curve, the student must expose the bacterial culture to a precisely controlled gradient of inducer concentrations on a 96-well microplate.\n\nThe experiment is prepared in Row A of the plate. The following parameters are used:\n- A stock solution of L-arabinose is available at a concentration $C_{stock} = 500$ mM.\n- The final volume in each well of Row A containing bacterial culture and inducer is $V_{final} = 200$ µL.\n- The first well, A1, is prepared by adding $V_{prep} = 4.0$ µL of the L-arabinose stock solution directly into a volume of bacterial culture sufficient to reach the final volume.\n- To create the subsequent concentrations, a two-fold serial dilution is performed. This procedure involves transferring $V_{transfer} = 100$ µL from the previous well to the next, starting with a transfer from A1 to A2. Each subsequent well (A2, A3, etc.) already contains $100$ µL of bacterial culture before this transfer is made. After mixing, the process is repeated for the next well in the series (e.g., from A2 to A3).\n\nCalculate the final concentration of L-arabinose in well A8. Express your answer in micromolar (µM), and round your final answer to three significant figures.", "solution": "The concentration in a well after mixing a volume of stock solution with a volume of culture to a final volume is given by conservation of mass:\n$$\nC_{\\text{final}}=C_{\\text{stock}}\\frac{V_{\\text{stock}}}{V_{\\text{final}}}.\n$$\nFor well A1, $C_{\\text{stock}}=500\\ \\text{mM}$, $V_{\\text{stock}}=4.0$ µL, and $V_{\\text{final}}=200$ µL, so\n$$\nC_{\\text{A1}}=500\\,\\text{mM}\\times\\frac{4.0}{200}=500\\,\\text{mM}\\times\\frac{1}{50}=10\\,\\text{mM}.\n$$\nFor each subsequent well, $V_{\\text{transfer}}=100$ µL is moved into a well already containing $100$ µL of culture, yielding a two-fold dilution. Thus each step multiplies the concentration by $\\frac{1}{2}$:\n$$\nC_{\\text{A}(n+1)}=\\frac{1}{2}C_{\\text{A}n}.\n$$\nFrom A1 to A8 there are $7$ such dilutions, so\n$$\nC_{\\text{A8}}=C_{\\text{A1}}\\left(\\frac{1}{2}\\right)^{7}=10\\,\\text{mM}\\times\\frac{1}{128}=\\frac{10}{128}\\,\\text{mM}=0.078125\\,\\text{mM}.\n$$\nConverting millimolar to micromolar using $1\\,\\text{mM}=1000\\,\\text{µM}$:\n$$\nC_{\\text{A8}}=0.078125\\times 1000=78.125\\ \\text{µM}.\n$$\nRounding to three significant figures gives $78.1$ µM.", "answer": "$$\\boxed{78.1}$$", "id": "1428071"}, {"introduction": "A key challenge in synthetic biology is the context-dependency of genetic parts; a circuit that functions in one host may fail in another. This exercise [@problem_id:1428108] places you in the \"Learn\" phase, confronting a common scenario where a functional plasmid design ceases to work when moved from a cloning strain to a protein expression strain. Successfully diagnosing the issue requires moving beyond the circuit diagram to reason about the host-circuit interactions, a critical step in refining designs for robustness.", "problem": "A synthetic biology team is following the Design-Build-Test-Learn (DBTL) cycle to create a simple genetic switch. Their \"Design\" is a plasmid intended to function as an inducible expression system. The plasmid contains two key components:\n1. A gene expressing the LacI repressor protein, driven by a weak, constitutive promoter.\n2. A gene for a Green Fluorescent Protein (GFP), driven by a strong, LacI-repressible promoter ($P_{\\text{lac}}$).\n\nThe team \"Builds\" this plasmid and \"Tests\" it in two different strains of *E. coli*.\n- **Test 1:** In the cloning strain *E. coli* DH5α, the circuit works perfectly. The cells show very low fluorescence in the absence of an inducer and high fluorescence when the inducer is added.\n- **Test 2:** The exact same plasmid is moved to the protein expression strain *E. coli* BL21(DE3). In this strain, the circuit fails. The cells exhibit high GFP fluorescence even in the complete absence of any inducer.\n\nNow in the \"Learn\" phase, the team must form a hypothesis to explain this context-dependent failure. Based on the known general characteristics of cloning strains versus protein expression strains, which one of the following hypotheses is the most plausible explanation for the high basal (leaky) expression observed in BL21(DE3)?\n\nA. The GFP protein is folded into its active, fluorescent conformation much more efficiently in the cytoplasm of BL21(DE3) than in DH5α, leading to a higher fluorescent signal for the same amount of protein.\n\nB. The plasmid's origin of replication leads to a significantly higher plasmid copy number per cell in the BL21(DE3) host environment compared to the DH5α host.\n\nC. The BL21(DE3) strain has a much higher concentration of endogenous proteases that specifically degrade the LacI repressor, reducing its ability to bind to the $P_{\\text{lac}}$ promoter.\n\nD. The ribosomes in BL21(DE3) translate the `gfp` mRNA sequence at a significantly faster rate than the ribosomes in DH5α, leading to a rapid accumulation of GFP.", "solution": "We begin by identifying the regulatory logic of the circuit. The LacI repressor is produced from a weak constitutive promoter, and the GFP reporter is driven by a strong $P_{\\text{lac}}$ promoter repressed by LacI. In the absence of inducer, repression succeeds if the cellular concentration of active LacI tetramers is sufficiently high relative to the number of $P_{\\text{lac}}$ operator sites and the promoter’s intrinsic strength. When moved across hosts, any factor that decreases the effective LacI repression or increases the effective transcriptional drive of $P_{\\text{lac}}$ can manifest as high basal fluorescence.\n\nWe now evaluate each hypothesis against known, general properties of cloning strains (e.g., DH5α) and protein expression strains (e.g., BL21(DE3)) and against the mechanistic requirements for LacI repression.\n\n- Hypothesis A (improved folding in BL21(DE3)): Enhanced folding efficiency would scale the fluorescence signal per GFP molecule by a multiplicative factor $f$ without altering the transcriptional state of $P_{\\text{lac}}$. This affects readout sensitivity but does not convert a tightly repressed promoter into a transcriptionally derepressed state. High basal fluorescence attributable purely to folding would imply that the same low basal expression in DH5α is simply less detectable, yet the observation is a qualitative failure of repression in BL21(DE3). This is therefore not the most plausible cause of leaky transcription.\n\n- Hypothesis B (higher plasmid copy number in BL21(DE3)): Let $N$ denote the plasmid copy number. The number of operator sites scales as $N$. The total LacI produced from a weak promoter also scales with $N$, but because the LacI promoter is weak and $P_{\\text{lac}}$ is strong, effective repression requires a substantial excess of active repressor over the number of available operator sites, factoring in binding stoichiometry, nonspecific binding, and stochastic fluctuations. In practice, high copy numbers can titrate repressors, and tight control of lac-based promoters often necessitates elevated LacI expression (e.g., lacI^{q}) to overcome operator titration. Host-dependent differences in the control of ColE1-like origins can alter $N$, and expression hosts can yield different $N$ than cloning strains. Thus, an increased $N$ in BL21(DE3) is a plausible general mechanism to increase basal expression by outstripping the weak LacI supply relative to the strong $P_{\\text{lac}}$, thereby causing leak.\n\n- Hypothesis C (higher protease levels degrading LacI in BL21(DE3)): A defining feature of BL21(DE3) is that it is protease-deficient (notably `lon-`, `ompT-`), which reduces proteolysis of recombinant proteins. The claim of “much higher concentration of endogenous proteases that specifically degrade LacI” contradicts this general property. This hypothesis is not plausible.\n\n- Hypothesis D (faster translation in BL21(DE3)): There is no general property that BL21(DE3) ribosomes translate mRNA “significantly faster” than DH5α ribosomes across arbitrary genes. Moreover, any global increase in translation rate would affect both LacI and GFP, and would not selectively abolish repression. This is not a compelling or general explanation for leaky transcription.\n\nAmong the options, the mechanism that aligns with a known and commonly encountered failure mode in lac-repressed circuits—operator titration due to increased gene dosage—is Hypothesis B. It explains why a weakly expressed LacI (from a weak constitutive promoter) can be sufficient in DH5α yet fail in BL21(DE3) if the host context increases plasmid copy number and thus the number of strong $P_{\\text{lac}}$ promoters to repress. It also fits the broader principle that host-dependent replication control can modulate copy number and thereby repressor demand.", "answer": "$$\\boxed{B}$$", "id": "1428108"}, {"introduction": "The most sophisticated applications of the DBTL cycle close the loop by using knowledge from the \"Learn\" phase to computationally optimize the next \"Design\". This advanced problem [@problem_id:2782988] delves into model-based design, tasking you with determining an optimal set of experimental conditions to most efficiently characterize a system's parameters. By applying the principles of D-optimality and the Fisher Information Matrix, you will practice how to design experiments that maximize learning, making the entire engineering cycle more efficient and powerful.", "problem": "In the Design-Build-Test-Learn (DBTL) cycle of synthetic biology, the Design step selects experimental inputs to maximize the quality of learning in the Learn step. Consider an inducible gene expression system where the steady-state reporter level is modeled by a Hill response to an inducer concentration. Let the deterministic mean response be given by the Hill function $f(x;\\boldsymbol{\\theta})$, where the inducer concentration $x$ is strictly positive and the parameter vector is $\\boldsymbol{\\theta} = (\\alpha,\\beta,K,n)$ with $\\alpha \\in \\mathbb{R}$, $\\beta \\in \\mathbb{R}$, $K \\in \\mathbb{R}_{>0}$, and $n \\in \\mathbb{R}_{>0}$. Assume homoscedastic independent Gaussian measurement noise with variance $\\sigma^2$, so that each observation $y_i$ at input $x_i$ satisfies $y_i = f(x_i;\\boldsymbol{\\theta}) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ and the $\\varepsilon_i$ are independent. The goal is to choose a multiset of inducer concentrations that is D-optimal for estimating $\\boldsymbol{\\theta}$ at a specified nominal parameter value, under bounded inputs.\n\nUse the following fundamental base:\n- The Hill mean response is $f(x;\\boldsymbol{\\theta}) = \\alpha + \\beta \\dfrac{x^n}{K^n + x^n}$.\n- For independent Gaussian observations with known variance $\\sigma^2$, the Fisher Information Matrix (FIM) for a design $\\{x_1,\\ldots,x_m\\}$ is $I(\\boldsymbol{\\theta}) = \\dfrac{1}{\\sigma^2} \\sum_{i=1}^{m} \\left(\\nabla_{\\boldsymbol{\\theta}} f(x_i;\\boldsymbol{\\theta})\\right)\\left(\\nabla_{\\boldsymbol{\\theta}} f(x_i;\\boldsymbol{\\theta})\\right)^\\top$.\n- The D-optimality criterion maximizes $\\det\\!\\left(I(\\boldsymbol{\\theta})\\right)$, equivalently maximizes $\\log \\det\\!\\left(I(\\boldsymbol{\\theta})\\right)$ when $I(\\boldsymbol{\\theta})$ is positive definite.\n\nTask: Write a complete program that, for each test case below, computes a D-optimal exact design under the following constraints and conventions:\n- You must choose exactly $m$ trials as a multiset from a finite candidate set $\\Gamma \\subset (0,\\infty)$ of allowable inducer concentrations. Repeated selections correspond to technical replicates at the same concentration. The input bounds are enforced by $\\Gamma$.\n- For a given test case, treat $\\boldsymbol{\\theta}$ and $\\sigma$ as known nominal constants for local D-optimal design.\n- If $I(\\boldsymbol{\\theta})$ is singular for a design, define $\\log \\det\\!\\left(I(\\boldsymbol{\\theta})\\right) = -\\infty$. Among designs achieving the maximum value of $\\log \\det$ up to a tolerance $\\tau = 10^{-12}$, choose the lexicographically smallest sorted multiset of concentrations (ascending order) as the tie-breaker.\n- All concentrations must be reported in micromolar, in non-decreasing order, as floating-point numbers. Duplicates indicate replicates.\n- Angles do not appear. There are no percentages in the output.\n\nYour program must compute $\\nabla_{\\boldsymbol{\\theta}} f(x;\\boldsymbol{\\theta})$ from first principles and evaluate the D-optimal design by exhaustive search over all multisets of size $m$ drawn from $\\Gamma$.\n\nTest suite:\n- Case $1$ (happy path):\n  - $\\boldsymbol{\\theta} = (\\alpha,\\beta,K,n) = (10.0, 90.0, 50.0, 2.0)$\n  - $\\sigma = 5.0$\n  - $\\Gamma = [1.0, 5.0, 10.0, 20.0, 40.0, 60.0, 80.0, 100.0]$ micromolar\n  - $m = 4$\n- Case $2$ (bounded candidates near the dissociation constant and higher noise):\n  - $\\boldsymbol{\\theta} = (\\alpha,\\beta,K,n) = (5.0, 95.0, 10.0, 2.0)$\n  - $\\sigma = 10.0$\n  - $\\Gamma = [2.0, 5.0, 10.0, 20.0]$ micromolar\n  - $m = 4$\n- Case $3$ (under-determined edge case where $m < \\dim(\\boldsymbol{\\theta})$):\n  - $\\boldsymbol{\\theta} = (\\alpha,\\beta,K,n) = (2.0, 18.0, 2.0, 3.0)$\n  - $\\sigma = 1.0$\n  - $\\Gamma = [0.5, 1.0, 2.0, 4.0]$ micromolar\n  - $m = 2$\n\nRequired final output format: Your program should produce a single line of output containing the three designs, each as a list of micromolar concentrations rounded to six digits after the decimal point, aggregated as a comma-separated list enclosed in square brackets, for example, `[[x_{1,1},x_{1,2},\\ldots],[x_{2,1},\\ldots],[x_{3,1},\\ldots]]`. There must be no spaces in the printed line.\n\nScientific realism notes:\n- All candidate concentrations are strictly positive to avoid non-differentiability in logarithms.\n- The Learn step uses Maximum Likelihood Estimation (MLE), whose asymptotic covariance is the inverse of the FIM at the true parameter, motivating D-optimality for design in DBTL.", "solution": "The problem requires the determination of a D-optimal experimental design for estimating the $4$ parameters $\\boldsymbol{\\theta} = (\\alpha, \\beta, K, n)$ of a Hill function model. The design consists of a multiset of $m$ inducer concentrations $\\{x_1, \\ldots, x_m\\}$ chosen from a finite candidate set $\\Gamma$. The D-optimality criterion seeks to maximize the determinant of the Fisher Information Matrix (FIM), $I(\\boldsymbol{\\theta})$, or equivalently, its logarithm, $\\log \\det(I(\\boldsymbol{\\theta}))$.\n\nThe mean response is given by the Hill function:\n$$f(x;\\boldsymbol{\\theta}) = \\alpha + \\beta \\dfrac{x^n}{K^n + x^n}$$\nFor independent Gaussian observations with variance $\\sigma^2$, the FIM for a design $D = \\{x_1, \\ldots, x_m\\}$ is:\n$$I(\\boldsymbol{\\theta}) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{m} \\left(\\nabla_{\\boldsymbol{\\theta}} f(x_i;\\boldsymbol{\\theta})\\right) \\left(\\nabla_{\\boldsymbol{\\theta}} f(x_i;\\boldsymbol{\\theta})\\right)^\\top$$\nMaximizing $\\det(I(\\boldsymbol{\\theta}))$ is equivalent to maximizing $\\det\\left(\\sum_{i=1}^{m} \\mathbf{g}_i \\mathbf{g}_i^\\top\\right)$, where $\\mathbf{g}_i = \\nabla_{\\boldsymbol{\\theta}} f(x_i;\\boldsymbol{\\theta})$, because the constant pre-factor $1/\\sigma^2$ does not alter the location of the optimum. We define the unscaled information matrix $M(D) = \\sum_{x_i \\in D} \\mathbf{g}_i \\mathbf{g}_i^\\top$ and seek to maximize its log-determinant, $\\log \\det(M(D))$.\n\nThe first-principles derivation begins with computing the gradient vector $\\nabla_{\\boldsymbol{\\theta}} f(x;\\boldsymbol{\\theta})$, which consists of the partial derivatives with respect to each parameter in $\\boldsymbol{\\theta}$.\n\n1.  Derivative with respect to the basal level, $\\alpha$:\n    $$\\frac{\\partial f}{\\partial \\alpha} = 1$$\n\n2.  Derivative with respect to the dynamic range, $\\beta$:\n    $$\\frac{\\partial f}{\\partial \\beta} = \\frac{x^n}{K^n + x^n}$$\n\n3.  Derivative with respect to the dissociation constant, $K$:\n    $$\\frac{\\partial f}{\\partial K} = \\beta \\frac{\\partial}{\\partial K} \\left( \\frac{x^n}{K^n + x^n} \\right) = \\beta x^n \\frac{\\partial}{\\partial K} \\left( (K^n + x^n)^{-1} \\right)$$\n    Using the chain rule:\n    $$\\frac{\\partial f}{\\partial K} = \\beta x^n \\left( -1 \\cdot (K^n + x^n)^{-2} \\cdot \\frac{\\partial}{\\partial K}(K^n) \\right) = -\\beta x^n (K^n + x^n)^{-2} (n K^{n-1})$$\n    $$\\frac{\\partial f}{\\partial K} = - \\frac{\\beta n K^{n-1} x^n}{(K^n + x^n)^2}$$\n\n4.  Derivative with respect to the Hill coefficient, $n$:\n    $$\\frac{\\partial f}{\\partial n} = \\beta \\frac{\\partial}{\\partial n} \\left( \\frac{x^n}{K^n + x^n} \\right)$$\n    Using the identity $a^b = e^{b \\ln a}$ and applying the quotient rule to $g(n) = \\frac{u(n)}{v(n)}$ where $u(n) = x^n$ and $v(n) = K^n + x^n$:\n    $$\\frac{\\partial u}{\\partial n} = x^n \\ln x \\quad \\text{and} \\quad \\frac{\\partial v}{\\partial n} = K^n \\ln K + x^n \\ln x$$\n    $$\\frac{\\partial g}{\\partial n} = \\frac{v \\frac{\\partial u}{\\partial n} - u \\frac{\\partial v}{\\partial n}}{v^2} = \\frac{(K^n + x^n)(x^n \\ln x) - x^n(K^n \\ln K + x^n \\ln x)}{(K^n + x^n)^2}$$\n    $$= \\frac{K^n x^n \\ln x + (x^n)^2 \\ln x - K^n x^n \\ln K - (x^n)^2 \\ln x}{(K^n + x^n)^2} = \\frac{K^n x^n (\\ln x - \\ln K)}{(K^n + x^n)^2}$$\n    Thus,\n    $$\\frac{\\partial f}{\\partial n} = \\frac{\\beta K^n x^n \\ln(x/K)}{(K^n + x^n)^2}$$\n\nThe full gradient vector for a given concentration $x$ at nominal parameters $\\boldsymbol{\\theta}$ is:\n$$\\nabla_{\\boldsymbol{\\theta}} f(x;\\boldsymbol{\\theta}) = \\begin{pmatrix} 1 \\\\ \\frac{x^n}{K^n + x^n} \\\\ - \\frac{\\beta n K^{n-1} x^n}{(K^n + x^n)^2} \\\\ \\frac{\\beta K^n x^n \\ln(x/K)}{(K^n + x^n)^2} \\end{pmatrix}$$\n\nThe solution is found via an exhaustive search over all possible multisets of size $m$ drawn from the candidate set $\\Gamma$. This corresponds to combinations with replacement. If $|\\Gamma|=k$, the number of such multisets is $\\binom{k+m-1}{m}$. For each candidate design $D = \\{x_1, \\ldots, x_m\\}$, the computational procedure is as follows:\n1.  Initialize a $4 \\times 4$ zero matrix $M(D)$.\n2.  For each concentration $x_i \\in D$, compute the gradient vector $\\mathbf{g}_i = \\nabla_{\\boldsymbol{\\theta}} f(x_i;\\boldsymbol{\\theta})$ using the nominal parameter values.\n3.  Compute the outer product $\\mathbf{g}_i \\mathbf{g}_i^\\top$, which is a $4 \\times 4$ rank-$1$ matrix, and add it to $M(D)$.\n4.  After summing over all $x_i \\in D$, compute the objective function value, which is the log-determinant of the resulting matrix $M(D)$. If $M(D)$ is singular, its determinant is $0$, and its log-determinant is defined as $-\\infty$. This is handled numerically by checking the sign returned by `numpy.linalg.slogdet`.\n\nThe optimal design is the one that maximizes this objective function. A list of all candidate designs and their corresponding log-determinant values is generated. The maximum log-determinant, $\\mathcal{L}_{max}$, is identified. All designs whose log-determinant $\\mathcal{L}$ satisfies $|\\mathcal{L} - \\mathcal{L}_{max}| \\le \\tau$, where the tolerance is $\\tau = 10^{-12}$, are considered optimal. Among this set of optimal designs, the problem specifies selecting the one which is lexicographically smallest, with its a sorted non-decreasing sequence of concentrations.\n\nFor Case $3$, where the number of trials $m=2$ is less than the number of parameters $d=4$, the FIM is necessarily singular. The matrix $M(D)$ is a sum of $m=2$ rank-$1$ matrices, so its rank is at most $2$. A $4 \\times 4$ matrix with rank less than $4$ has a determinant of $0$. Consequently, all possible designs yield an objective value of $-\\infty$. The tie-breaking rule becomes the sole criterion for selection. The optimal design is therefore the lexicographically smallest multiset of size $2$ from $\\Gamma$, which is $\\{0.5, 0.5\\}$. This demonstrates a correct interpretation of the problem's criteria for under-determined cases.", "answer": "```python\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Computes D-optimal designs for three test cases by exhaustive search.\n    \"\"\"\n    test_cases = [\n        {\n            \"theta\": (10.0, 90.0, 50.0, 2.0),  # (alpha, beta, K, n)\n            \"sigma\": 5.0,\n            \"gamma\": [1.0, 5.0, 10.0, 20.0, 40.0, 60.0, 80.0, 100.0],\n            \"m\": 4,\n        },\n        {\n            \"theta\": (5.0, 95.0, 10.0, 2.0),\n            \"sigma\": 10.0,\n            \"gamma\": [2.0, 5.0, 10.0, 20.0],\n            \"m\": 4,\n        },\n        {\n            \"theta\": (2.0, 18.0, 2.0, 3.0),\n            \"sigma\": 1.0,\n            \"gamma\": [0.5, 1.0, 2.0, 4.0],\n            \"m\": 2,\n        },\n    ]\n\n    all_results = []\n    tau = 1e-12\n\n    def get_gradient(x, theta):\n        \"\"\"\n        Computes the gradient of the Hill function f with respect to theta.\n        theta = (alpha, beta, K, n)\n        \"\"\"\n        alpha, beta, K, n = theta\n        \n        # To prevent log(0) or division by zero, though problem constraints make it unlikely\n        if x <= 0 or K <= 0:\n            return np.zeros(4)\n\n        x_n = x**n\n        K_n = K**n\n        denominator = K_n + x_n\n        \n        if denominator == 0:\n            return np.zeros(4)\n        \n        common_term_beta = x_n / denominator\n        \n        # Partial derivatives\n        df_d_alpha = 1.0\n        df_d_beta = common_term_beta\n        df_d_K = -beta * n * (K**(n - 1)) * x_n / (denominator**2)\n        df_d_n = beta * K_n * x_n * np.log(x / K) / (denominator**2)\n        \n        return np.array([df_d_alpha, df_d_beta, df_d_K, df_d_n])\n\n    for case in test_cases:\n        theta = case[\"theta\"]\n        gamma = case[\"gamma\"]\n        m = case[\"m\"]\n        \n        num_params = len(theta)\n        \n        design_candidates = itertools.combinations_with_replacement(gamma, m)\n        \n        evaluated_designs = []\n        \n        for design in design_candidates:\n            fim_unscaled = np.zeros((num_params, num_params))\n            for x_i in design:\n                g = get_gradient(x_i, theta)\n                fim_unscaled += np.outer(g, g)\n            \n            sign, logdet = np.linalg.slogdet(fim_unscaled)\n            \n            # If matrix is singular or not positive definite, logdet is -inf\n            if sign <= 0:\n                objective_value = -np.inf\n            else:\n                objective_value = logdet\n            \n            evaluated_designs.append((objective_value, list(design)))\n\n        # Find the maximum log-determinant value\n        if not evaluated_designs:\n            # Should not happen with the given constraints\n            all_results.append([])\n            continue\n            \n        max_log_det = -np.inf\n        for log_det_val, _ in evaluated_designs:\n            if log_det_val > max_log_det:\n                max_log_det = log_det_val\n        \n        # Filter for designs that are optimal within the tolerance\n        best_designs = []\n        for log_det_val, design in evaluated_designs:\n            if abs(log_det_val - max_log_det) <= tau:\n                best_designs.append(design)\n        \n        # The first item in best_designs is the lexicographically smallest\n        # because itertools.combinations_with_replacement generates them in order.\n        optimal_design = best_designs[0]\n        all_results.append(optimal_design)\n\n    # Format the final output string exactly as specified\n    def format_list(lst):\n        return f\"[{','.join(f'{x:.6f}' for x in lst)}]\"\n\n    results_as_strings = [format_list(res) for res in all_results]\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "2782988"}]}