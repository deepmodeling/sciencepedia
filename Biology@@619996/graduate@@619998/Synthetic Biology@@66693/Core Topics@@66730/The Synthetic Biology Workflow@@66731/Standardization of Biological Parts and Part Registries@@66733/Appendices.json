{"hands_on_practices": [{"introduction": "A cornerstone of part standardization is quantitative characterization. This practice guides you through the process of analyzing time-course experimental data to model an inducible promoter's behavior. By fitting a Hill function to Relative Promoter Unit (RPU) data, you will extract key parameters—maximal activity ($\\alpha$), half-maximal concentration ($K$), and cooperativity ($n$)—and quantify their uncertainty, skills that are essential for creating a high-quality, reusable part entry in a registry. [@problem_id:2775671]", "problem": "You are building a standardized characterization pipeline for promoter parts to register in a biological parts registry. Following common practice in synthetic biology, you will estimate Relative Promoter Units (RPU) by normalizing target promoter activity against a reference promoter measured under identical conditions, and then fit a Hill function to induction-response data, reporting parameter estimates and their confidence intervals.\n\nBase principles and definitions you must assume:\n- The Central Dogma: transcription and translation produce reporter protein whose abundance drives fluorescence intensity.\n- Fluorescence is proportional to reporter protein amount per culture volume; optical density (OD) is proportional to biomass concentration.\n- In balanced exponential growth, the culture-averaged reporter protein per biomass is approximately constant over short windows where biomass increases smoothly. In this regime, after subtracting measurement backgrounds, corrected fluorescence $F_{\\mathrm{corr}}$ scales linearly with corrected OD $\\mathrm{OD}_{\\mathrm{corr}}$, and the slope $s$ of a linear regression $F_{\\mathrm{corr}} \\approx s \\,\\mathrm{OD}_{\\mathrm{corr}} + b$ is a proxy for promoter activity.\n- Relative Promoter Units (RPU) are defined as the ratio of the target promoter slope to the reference promoter slope computed from matched time-course data collected under the same inducer condition.\n- The induction-response of many promoters is well described by a Hill function $f(I) = \\dfrac{\\alpha I^n}{K^n + I^n}$, where $I$ is the inducer concentration, $\\alpha$ is the maximal activity in RPU, $K$ is the half-maximal inducer concentration (in $\\mu\\mathrm{M}$), and $n$ is a dimensionless Hill coefficient.\n\nYour task is to:\n1. For each test case, generate time-course optical density and fluorescence data for both a reference and a target promoter over multiple inducer concentrations, using the deterministic data-generating model specified below. Subtract time-dependent measurement backgrounds to obtain corrected signals.\n2. From the corrected time-course for each inducer concentration, estimate a promoter activity slope $s$ by performing an ordinary least squares linear regression of corrected fluorescence versus corrected OD over a specified OD window. Compute RPU per inducer as $s_{\\mathrm{target}}/s_{\\mathrm{ref}}$.\n3. Fit the Hill function $f(I)$ to the RPU versus $I$ data using nonlinear least squares with positivity bounds $\\alpha > 0$, $K > 0$, and $0.5 \\le n \\le 5.0$. Estimate the asymptotic $95\\%$ confidence intervals for $\\alpha$, $K$, and $n$ from the parameter covariance matrix returned by the regression, using the Student’s $t$ critical value with degrees of freedom equal to the number of data points minus the number of fitted parameters.\n4. Report, for each test case, a flat list of $9$ floating-point numbers in the order: $\\alpha$, $K$, $n$, $\\alpha_{\\mathrm{low}}$, $\\alpha_{\\mathrm{high}}$, $K_{\\mathrm{low}}$, $K_{\\mathrm{high}}$, $n_{\\mathrm{low}}$, $n_{\\mathrm{high}}$, where “low” and “high” denote the lower and upper endpoints of the $95\\%$ confidence interval. Express $K$ in $\\mu\\mathrm{M}$ and all other parameters as dimensionless numbers. Round all output numbers to six decimal places.\n\nDeterministic data-generating model (no randomness):\n- For each test case, you are given a set of parameters that define the time grid, inducer concentrations, growth, background signals, instrumental perturbations, and the true target promoter response. From these, generate the following signals for each inducer concentration $I_i$ and time point $t_j$:\n    - True biomass trajectory: $\\mathrm{OD}_{\\mathrm{true}}(t_j) = \\mathrm{OD}_0 \\exp(r \\, t_j)$.\n    - Background signals: $\\mathrm{OD}_{\\mathrm{blank}}(t_j) = \\beta_0 + \\beta_1 t_j$, $F_{\\mathrm{bg}}(t_j) = \\phi_0 + \\phi_1 t_j$.\n    - Reference promoter fluorescence: $F_{\\mathrm{ref}}(i,j) = F_{\\mathrm{bg}}(t_j) + s_{\\mathrm{ref}} \\, \\mathrm{OD}_{\\mathrm{true}}(t_j) + A_{\\mathrm{ref}} \\sin(\\omega_{\\mathrm{ref}} t_j + \\psi_{\\mathrm{ref}} i)$, with a small OD perturbation $\\delta \\mathrm{OD}_{\\mathrm{ref}}(i,j) = A_{\\mathrm{OD}} \\cos(\\omega_{\\mathrm{OD}} t_j + \\psi_{\\mathrm{OD}} i)$ added to the measured OD: $\\mathrm{OD}_{\\mathrm{ref}}(i,j) = \\mathrm{OD}_{\\mathrm{true}}(t_j) + \\delta \\mathrm{OD}_{\\mathrm{ref}}(i,j)$.\n    - Target promoter fluorescence: $F_{\\mathrm{tar}}(i,j) = F_{\\mathrm{bg}}(t_j) + s_{\\mathrm{tar}}(I_i) \\, \\mathrm{OD}_{\\mathrm{true}}(t_j) + A_{\\mathrm{tar}} \\sin(\\omega_{\\mathrm{tar}} t_j + \\psi_{\\mathrm{tar}} i)$, with $\\mathrm{OD}_{\\mathrm{tar}}(i,j) = \\mathrm{OD}_{\\mathrm{true}}(t_j) + \\delta \\mathrm{OD}_{\\mathrm{tar}}(i,j)$, where $\\delta \\mathrm{OD}_{\\mathrm{tar}}(i,j)$ uses the same form and amplitude as the reference OD perturbation. The target per-biomass fluorescence slope is $s_{\\mathrm{tar}}(I_i) = s_{\\mathrm{ref}} \\cdot \\dfrac{\\alpha_{\\mathrm{true}} I_i^{n_{\\mathrm{true}}}}{K_{\\mathrm{true}}^{n_{\\mathrm{true}}} + I_i^{n_{\\mathrm{true}}}}$.\n    - Corrected signals: $\\mathrm{OD}_{\\mathrm{corr}} = \\mathrm{OD}_{\\mathrm{meas}} - \\mathrm{OD}_{\\mathrm{blank}}$, $F_{\\mathrm{corr}} = F_{\\mathrm{meas}} - F_{\\mathrm{bg}}$.\n- For slope estimation, use only time points where $\\mathrm{OD}_{\\mathrm{corr}} \\in [0.06, 0.30]$. If fewer than three points satisfy this, fall back to all available points. Fit $F_{\\mathrm{corr}}$ versus $\\mathrm{OD}_{\\mathrm{corr}}$ by ordinary least squares to obtain the slope. If a fitted slope is non-positive due to perturbations at very low induction, replace it with a small positive floor $10^{-9}$ to maintain physical plausibility.\n- Compute RPU per inducer concentration $I_i$ as $\\mathrm{RPU}(I_i) = s_{\\mathrm{tar}}(I_i)/s_{\\mathrm{ref}}$.\n\nNonlinear regression and confidence intervals:\n- Fit $f(I) = \\dfrac{\\alpha I^n}{K^n + I^n}$ to the $(I_i, \\mathrm{RPU}(I_i))$ pairs using nonlinear least squares with bounds $\\alpha \\in (10^{-12}, 10^{6})$, $K \\in (10^{-9}, 10^{6})$, and $n \\in [0.5, 5.0]$. Use reasonable initial guesses based on the data (for example, $\\alpha$ initialized to the maximum observed RPU, $K$ to a positive inducer concentration near the half-maximum, and $n$ to $2.0$).\n- Let $\\hat{\\theta} = (\\hat{\\alpha}, \\hat{K}, \\hat{n})$ be the fitted parameters, and let $\\Sigma$ be the parameter covariance matrix returned by the optimizer. The standard errors are $\\mathrm{SE}(\\hat{\\theta}_k) = \\sqrt{\\Sigma_{kk}}$. With degrees of freedom $d = N - 3$ where $N$ is the number of $(I_i, \\mathrm{RPU}(I_i))$ pairs, compute the two-sided $95\\%$ confidence interval for each parameter as $\\hat{\\theta}_k \\pm t_{0.975, d} \\cdot \\mathrm{SE}(\\hat{\\theta}_k)$, where $t_{0.975, d}$ is the $0.975$ quantile of the Student’s $t$ distribution with $d$ degrees of freedom. If $d \\le 0$, approximate with the standard normal quantile $1.96$.\n\nTest suite (three cases):\n- Case 1 (happy path, broad dynamic range):\n    - Time grid: $t_j = j \\Delta t$, $j = 0,\\dots,5$, $\\Delta t = 0.5$ hours.\n    - Inducer concentrations (in $\\mu\\mathrm{M}$): $I = [0, 1, 3, 10, 30, 100]$.\n    - Growth: $\\mathrm{OD}_0 = 0.05$, $r = 0.9 \\,\\mathrm{h}^{-1}$.\n    - Backgrounds: $\\mathrm{OD}_{\\mathrm{blank}}(t) = 0.005 + 0.001 t$, $F_{\\mathrm{bg}}(t) = 40 + 3 t$.\n    - Reference slope: $s_{\\mathrm{ref}} = 9000$.\n    - True target Hill parameters: $\\alpha_{\\mathrm{true}} = 2.0$, $K_{\\mathrm{true}} = 12.0 \\,\\mu\\mathrm{M}$, $n_{\\mathrm{true}} = 2.2$.\n    - Perturbations: $A_{\\mathrm{OD}} = 0.002$, $\\omega_{\\mathrm{OD}} = 0.3$, $\\psi_{\\mathrm{OD}} = 0.2$; $A_{\\mathrm{ref}} = 50$, $\\omega_{\\mathrm{ref}} = 0.7$, $\\psi_{\\mathrm{ref}} = 0.15$; $A_{\\mathrm{tar}} = 60$, $\\omega_{\\mathrm{tar}} = 0.6$, $\\psi_{\\mathrm{tar}} = 0.17$.\n- Case 2 (low dynamic range, small $K$):\n    - Time grid: $t_j = j \\Delta t$, $j = 0,\\dots,5$, $\\Delta t = 0.6$ hours.\n    - Inducer concentrations (in $\\mu\\mathrm{M}$): $I = [0, 0.5, 1.0, 2.0, 4.0]$.\n    - Growth: $\\mathrm{OD}_0 = 0.06$, $r = 0.6 \\,\\mathrm{h}^{-1}$.\n    - Backgrounds: $\\mathrm{OD}_{\\mathrm{blank}}(t) = 0.004 + 0.0008 t$, $F_{\\mathrm{bg}}(t) = 30 + 2 t$.\n    - Reference slope: $s_{\\mathrm{ref}} = 12000$.\n    - True target Hill parameters: $\\alpha_{\\mathrm{true}} = 0.6$, $K_{\\mathrm{true}} = 1.5 \\,\\mu\\mathrm{M}$, $n_{\\mathrm{true}} = 1.2$.\n    - Perturbations: $A_{\\mathrm{OD}} = 0.0015$, $\\omega_{\\mathrm{OD}} = 0.3$, $\\psi_{\\mathrm{OD}} = 0.2$; $A_{\\mathrm{ref}} = 30$, $\\omega_{\\mathrm{ref}} = 0.7$, $\\psi_{\\mathrm{ref}} = 0.15$; $A_{\\mathrm{tar}} = 35$, $\\omega_{\\mathrm{tar}} = 0.6$, $\\psi_{\\mathrm{tar}} = 0.17$.\n- Case 3 (steep cooperativity):\n    - Time grid: $t_j = j \\Delta t$, $j = 0,\\dots,5$, $\\Delta t = 0.4$ hours.\n    - Inducer concentrations (in $\\mu\\mathrm{M}$): $I = [0, 5, 8, 10, 12, 20, 50]$.\n    - Growth: $\\mathrm{OD}_0 = 0.04$, $r = 1.1 \\,\\mathrm{h}^{-1}$.\n    - Backgrounds: $\\mathrm{OD}_{\\mathrm{blank}}(t) = 0.003 + 0.0009 t$, $F_{\\mathrm{bg}}(t) = 35 + 2.5 t$.\n    - Reference slope: $s_{\\mathrm{ref}} = 8000$.\n    - True target Hill parameters: $\\alpha_{\\mathrm{true}} = 1.5$, $K_{\\mathrm{true}} = 10.0 \\,\\mu\\mathrm{M}$, $n_{\\mathrm{true}} = 3.0$.\n    - Perturbations: $A_{\\mathrm{OD}} = 0.0025$, $\\omega_{\\mathrm{OD}} = 0.3$, $\\psi_{\\mathrm{OD}} = 0.2$; $A_{\\mathrm{ref}} = 40$, $\\omega_{\\mathrm{ref}} = 0.7$, $\\psi_{\\mathrm{ref}} = 0.15$; $A_{\\mathrm{tar}} = 45$, $\\omega_{\\mathrm{tar}} = 0.6$, $\\psi_{\\mathrm{tar}} = 0.17$.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated flat list of $27$ numbers enclosed in square brackets. The numbers should be ordered by concatenating the nine-number result of Case 1, followed by Case 2, then Case 3. Each floating-point number must be rounded to six decimal places, and $K$ values must be in $\\mu\\mathrm{M}$ while all others are dimensionless. For example: \"[a1,K1,n1,low_a1,high_a1,low_K1,high_K1,low_n1,high_n1,a2,K2,n2,...]\".", "solution": "We design a computational pipeline aligned with principles of standardized promoter characterization used in parts registries. The pipeline has three stages: signal correction and activity extraction, RPU computation, and Hill function fitting with uncertainty quantification.\n\nFirst, we base our approach on foundational biophysical and measurement principles:\n- According to the Central Dogma, promoter activity modulates transcription and translation of a reporter; the accumulated reporter protein concentration influences the measured fluorescence intensity. Over modest time windows during balanced exponential growth, the production and dilution of reporter per biomass reach a quasi-steady behavior.\n- Optical density at a given wavelength is proportional to biomass concentration. Therefore, over a short interval where growth is exponential, corrected fluorescence $F_{\\mathrm{corr}}$ (with background subtracted) is approximately proportional to corrected OD $\\mathrm{OD}_{\\mathrm{corr}}$ (with blank subtracted).\n- Fitting a linear model $F_{\\mathrm{corr}} \\approx s \\,\\mathrm{OD}_{\\mathrm{corr}} + b$ yields a slope $s$ that represents the reporter fluorescence per unit biomass within that regime. This slope serves as a proxy for promoter activity, under the assumption that other factors (such as maturation and degradation) are adequately captured or approximately constant across matched conditions.\n\nSecond, to achieve standardization across experiments and enable sharing in a parts registry, we compute Relative Promoter Units (RPU) defined as the ratio of the activity of a target promoter to that of a well-characterized reference promoter, measured under identical conditions. If $s_{\\mathrm{tar}}(I_i)$ and $s_{\\mathrm{ref}}(I_i)$ denote the slopes for the target and reference promoters at inducer concentration $I_i$, then $\\mathrm{RPU}(I_i) = s_{\\mathrm{tar}}(I_i)/s_{\\mathrm{ref}}(I_i)$. This normalizes away instrument-dependent scales and culture conditions, producing a dimensionless response curve.\n\nThird, we fit the classical Hill function\n$$\nf(I) = \\frac{\\alpha I^n}{K^n + I^n}\n$$\nto the $(I_i, \\mathrm{RPU}(I_i))$ data using nonlinear least squares. We constrain the parameters to physically meaningful domains: $\\alpha > 0$, $K > 0$, and $0.5 \\le n \\le 5.0$. Let the fitted parameters be $\\hat{\\alpha}$, $\\hat{K}$, and $\\hat{n}$. The nonlinear regression returns an approximate parameter covariance matrix $\\Sigma$, often given by $\\sigma^2 (J^{\\top} J)^{-1}$, where $J$ is the Jacobian at the optimum and $\\sigma^2$ is the residual variance estimated from the fit. The marginal standard error for each parameter is $\\mathrm{SE}(\\hat{\\theta}_k) = \\sqrt{\\Sigma_{kk}}$. With $N$ data points and $P = 3$ fitted parameters, the degrees of freedom are $d = N - P$. A two-sided $95\\%$ confidence interval for each parameter is\n$$\n\\hat{\\theta}_k \\pm t_{0.975, d} \\cdot \\mathrm{SE}(\\hat{\\theta}_k),\n$$\nwhere $t_{0.975, d}$ is the quantile of the Student’s $t$ distribution. If $d \\le 0$, we revert to the standard normal quantile $1.96$.\n\nAlgorithmic steps mapped to the test suite:\n\n1. Data generation:\n   - For each test case, construct the time grid $t_j = j \\Delta t$ and compute $\\mathrm{OD}_{\\mathrm{true}}(t_j) = \\mathrm{OD}_0 e^{r t_j}$. Compute the backgrounds $F_{\\mathrm{bg}}(t_j)$ and $\\mathrm{OD}_{\\mathrm{blank}}(t_j)$.\n   - For each inducer concentration $I_i$, compute the true target slope $s_{\\mathrm{tar}}(I_i) = s_{\\mathrm{ref}} \\cdot \\dfrac{\\alpha_{\\mathrm{true}} I_i^{n_{\\mathrm{true}}}}{K_{\\mathrm{true}}^{n_{\\mathrm{true}}} + I_i^{n_{\\mathrm{true}}}}$.\n   - Construct the measured signals by adding deterministic sinusoidal perturbations to emulate instrument and biological variability:\n     - $\\mathrm{OD}_{\\mathrm{ref}}(i,j) = \\mathrm{OD}_{\\mathrm{true}}(t_j) + A_{\\mathrm{OD}} \\cos(\\omega_{\\mathrm{OD}} t_j + \\psi_{\\mathrm{OD}} i)$,\n     - $F_{\\mathrm{ref}}(i,j) = F_{\\mathrm{bg}}(t_j) + s_{\\mathrm{ref}} \\mathrm{OD}_{\\mathrm{true}}(t_j) + A_{\\mathrm{ref}} \\sin(\\omega_{\\mathrm{ref}} t_j + \\psi_{\\mathrm{ref}} i)$,\n     - $\\mathrm{OD}_{\\mathrm{tar}}(i,j) = \\mathrm{OD}_{\\mathrm{true}}(t_j) + A_{\\mathrm{OD}} \\cos(\\omega_{\\mathrm{OD}} t_j + \\psi_{\\mathrm{OD}} i)$,\n     - $F_{\\mathrm{tar}}(i,j) = F_{\\mathrm{bg}}(t_j) + s_{\\mathrm{tar}}(I_i) \\mathrm{OD}_{\\mathrm{true}}(t_j) + A_{\\mathrm{tar}} \\sin(\\omega_{\\mathrm{tar}} t_j + \\psi_{\\mathrm{tar}} i)$.\n\n2. Signal correction and slope extraction:\n   - For each $(i,j)$, compute $\\mathrm{OD}_{\\mathrm{corr,ref}}(i,j) = \\mathrm{OD}_{\\mathrm{ref}}(i,j) - \\mathrm{OD}_{\\mathrm{blank}}(t_j)$ and $F_{\\mathrm{corr,ref}}(i,j) = F_{\\mathrm{ref}}(i,j) - F_{\\mathrm{bg}}(t_j)$. Do the analogous corrections for the target promoter.\n   - Select time points with $\\mathrm{OD}_{\\mathrm{corr}} \\in [0.06, 0.30]$. If fewer than three points satisfy this, use all available points.\n   - Fit $F_{\\mathrm{corr}}$ versus $\\mathrm{OD}_{\\mathrm{corr}}$ using ordinary least squares to obtain the slope for each promoter and inducer concentration. If a slope is non-positive due to low-signal perturbations, replace it with $10^{-9}$.\n\n3. RPU computation:\n   - For each $I_i$, compute $\\mathrm{RPU}(I_i) = s_{\\mathrm{tar}}(I_i)/s_{\\mathrm{ref}}$ from the extracted slopes.\n\n4. Hill fit and uncertainty:\n   - Fit $f(I)$ to $(I_i, \\mathrm{RPU}(I_i))$ with bounds: $\\alpha \\in (10^{-12}, 10^{6}), K \\in (10^{-9}, 10^{6}), n \\in [0.5, 5.0]$. Use initial guesses $\\alpha_0 = \\max_i \\mathrm{RPU}(I_i)$, $K_0$ as a positive inducer concentration near the half-maximum (fallback to the median positive $I$ if needed), $n_0 = 2.0$.\n   - From the fit, obtain $\\hat{\\alpha}$, $\\hat{K}$, $\\hat{n}$ and covariance $\\Sigma$. Compute standard errors and $95\\%$ confidence intervals using the Student’s $t$ quantile $t_{0.975, d}$ with $d = N - 3$.\n\n5. Output:\n   - For each test case, output the nine numbers $\\hat{\\alpha}$, $\\hat{K}$, $\\hat{n}$, $\\alpha_{\\mathrm{low}}$, $\\alpha_{\\mathrm{high}}$, $K_{\\mathrm{low}}$, $K_{\\mathrm{high}}$, $n_{\\mathrm{low}}$, $n_{\\mathrm{high}}$, in that order, rounding each to six decimals. Concatenate the three cases into a single flat list and print it as a single line in the specified format.\n\nThis approach connects measurement physics (proportionality of fluorescence and OD), standardized activity definition (RPU), and a mechanistic response model (Hill function), providing parameter estimates and confidence intervals suitable for part registry metadata.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import curve_fit\nfrom scipy.stats import t as student_t\n\ndef hill_function(I, alpha, K, n):\n    # Hill function with broadcasting support\n    I = np.asarray(I, dtype=float)\n    return alpha * (I ** n) / (K ** n + I ** n)\n\ndef generate_case(time_steps, dt, I_array, OD0, r, od_blank_params, f_bg_params,\n                  s_ref, true_params, perturb_params):\n    \"\"\"\n    Generate deterministic time-course data for reference and target promoters.\n    Returns dictionary with keys:\n    - t: time vector (T,)\n    - I: inducer vector (N_I,)\n    - OD_blank: (T,)\n    - F_bg: (T,)\n    - OD_ref: (N_I, T)\n    - F_ref: (N_I, T)\n    - OD_tar: (N_I, T)\n    - F_tar: (N_I, T)\n    \"\"\"\n    t = np.arange(time_steps, dtype=float) * dt\n    I = np.array(I_array, dtype=float)\n    T = t.size\n    N = I.size\n\n    # Growth\n    OD_true = OD0 * np.exp(r * t)\n\n    # Backgrounds\n    beta0, beta1 = od_blank_params\n    phi0, phi1 = f_bg_params\n    OD_blank = beta0 + beta1 * t\n    F_bg = phi0 + phi1 * t\n\n    # True target slope as function of I\n    alpha_true, K_true, n_true = true_params\n    s_tar_I = s_ref * hill_function(I, alpha_true, K_true, n_true)\n\n    # Perturbations\n    A_OD, w_OD, psi_OD = perturb_params['OD']\n    A_ref, w_ref, psi_ref = perturb_params['ref']\n    A_tar, w_tar, psi_tar = perturb_params['tar']\n\n    # Allocate arrays\n    OD_ref = np.zeros((N, T), dtype=float)\n    F_ref = np.zeros((N, T), dtype=float)\n    OD_tar = np.zeros((N, T), dtype=float)\n    F_tar = np.zeros((N, T), dtype=float)\n\n    for idx in range(N):\n        # OD perturbations (same for ref and tar for simplicity)\n        delta_OD = A_OD * np.cos(w_OD * t + psi_OD * idx)\n        od_meas = OD_true + delta_OD\n\n        # Reference signals\n        OD_ref[idx, :] = od_meas\n        F_ref[idx, :] = F_bg + s_ref * OD_true + A_ref * np.sin(w_ref * t + psi_ref * idx)\n\n        # Target signals\n        OD_tar[idx, :] = od_meas\n        F_tar[idx, :] = F_bg + s_tar_I[idx] * OD_true + A_tar * np.sin(w_tar * t + psi_tar * idx)\n\n    return {\n        't': t,\n        'I': I,\n        'OD_blank': OD_blank,\n        'F_bg': F_bg,\n        'OD_ref': OD_ref,\n        'F_ref': F_ref,\n        'OD_tar': OD_tar,\n        'F_tar': F_tar\n    }\n\ndef select_window_and_slope(OD_corr, F_corr, od_min=0.06, od_max=0.30, min_points=3):\n    \"\"\"\n    Select points in OD window and compute slope of F vs OD using OLS.\n    If fewer than min_points in window, use all points.\n    Returns slope (float).\n    \"\"\"\n    OD_corr = np.asarray(OD_corr, dtype=float)\n    F_corr = np.asarray(F_corr, dtype=float)\n\n    mask = (OD_corr >= od_min) & (OD_corr <= od_max)\n    if np.count_nonzero(mask) < min_points:\n        x = OD_corr\n        y = F_corr\n    else:\n        x = OD_corr[mask]\n        y = F_corr[mask]\n\n    # Fit linear model y = m x + b\n    if x.size < 2:\n        # Fallback if degenerate: return tiny positive slope\n        return 1e-9\n    # Use numpy polyfit for slope\n    m, b = np.polyfit(x, y, 1)\n    if not np.isfinite(m) or m <= 0:\n        # Enforce physical plausibility: non-negative slope\n        m = 1e-9\n    return float(m)\n\ndef fit_hill_with_ci(I, y):\n    \"\"\"\n    Fit Hill function to (I, y) with bounds and compute 95% CI.\n    Returns (alpha, K, n, alpha_low, alpha_high, K_low, K_high, n_low, n_high)\n    \"\"\"\n    I = np.asarray(I, dtype=float)\n    y = np.asarray(y, dtype=float)\n\n    # Initial guesses\n    alpha0 = float(np.max(y))\n    # Estimate K0: choose I closest to half-max among positive I\n    half = alpha0 / 2.0 if alpha0 > 0 else 1.0\n    pos_mask = I > 0\n    if np.any(pos_mask):\n        I_pos = I[pos_mask]\n        y_pos = y[pos_mask]\n        # Find index with y closest to half\n        idx = int(np.argmin(np.abs(y_pos - half)))\n        K0 = float(I_pos[idx])\n    else:\n        K0 = 1.0\n    if K0 <= 0:\n        # Fallback to median positive I or 1.0\n        K0 = float(np.median(I[I > 0])) if np.any(I > 0) else 1.0\n    n0 = 2.0\n\n    # Bounds\n    lower_bounds = (1e-12, 1e-9, 0.5)\n    upper_bounds = (1e6, 1e6, 5.0)\n\n    # Fit\n    try:\n        popt, pcov = curve_fit(\n            hill_function, I, y,\n            p0=(alpha0, K0, n0),\n            bounds=(lower_bounds, upper_bounds),\n            maxfev=100000\n        )\n    except Exception:\n        # In rare failure, return NaNs\n        return [np.nan]*9\n\n    alpha_hat, K_hat, n_hat = popt\n\n    # Compute standard errors\n    if pcov is None or not np.all(np.isfinite(pcov)):\n        se = np.array([np.nan, np.nan, np.nan], dtype=float)\n    else:\n        # Ensure non-negative diagonal\n        diag = np.diag(pcov)\n        diag = np.where(diag < 0, np.nan, diag)\n        se = np.sqrt(diag)\n\n    # Degrees of freedom\n    dof = max(0, len(I) - 3)\n    if dof > 0 and np.isfinite(se).all():\n        tcrit = float(student_t.ppf(0.975, dof))\n    else:\n        tcrit = 1.96\n\n    # Confidence intervals\n    def ci_centered(est, se_val):\n        if not np.isfinite(se_val):\n            return (np.nan, np.nan)\n        low = est - tcrit * se_val\n        high = est + tcrit * se_val\n        return (float(low), float(high))\n\n    alpha_low, alpha_high = ci_centered(alpha_hat, se[0])\n    K_low, K_high = ci_centered(K_hat, se[1])\n    n_low, n_high = ci_centered(n_hat, se[2])\n\n    return [float(alpha_hat), float(K_hat), float(n_hat),\n            alpha_low, alpha_high, K_low, K_high, n_low, n_high]\n\ndef process_case(case):\n    \"\"\"\n    Given a generated case dict, compute RPUs across inducer concentrations,\n    fit Hill, and return the 9-number result list.\n    \"\"\"\n    I = case['I']\n    t = case['t']\n    OD_blank = case['OD_blank']\n    F_bg = case['F_bg']\n    OD_ref_meas = case['OD_ref']\n    F_ref_meas = case['F_ref']\n    OD_tar_meas = case['OD_tar']\n    F_tar_meas = case['F_tar']\n\n    N = I.size\n    T = t.size\n\n    # Correct signals\n    # Broadcast backgrounds across inducer index\n    OD_blank_b = OD_blank.reshape(1, T)\n    F_bg_b = F_bg.reshape(1, T)\n\n    OD_ref_corr = OD_ref_meas - OD_blank_b\n    F_ref_corr = F_ref_meas - F_bg_b\n    OD_tar_corr = OD_tar_meas - OD_blank_b\n    F_tar_corr = F_tar_meas - F_bg_b\n\n    # Compute slopes per inducer\n    slopes_ref = np.zeros(N, dtype=float)\n    slopes_tar = np.zeros(N, dtype=float)\n    for i in range(N):\n        slopes_ref[i] = select_window_and_slope(OD_ref_corr[i, :], F_ref_corr[i, :])\n        slopes_tar[i] = select_window_and_slope(OD_tar_corr[i, :], F_tar_corr[i, :])\n\n    # Compute RPU; avoid division by zero\n    with np.errstate(divide='ignore', invalid='ignore'):\n        rpu = slopes_tar / slopes_ref\n    # Replace non-finite or negative with small positive floor\n    rpu = np.where(~np.isfinite(rpu) | (rpu <= 0), 1e-9, rpu)\n\n    # Fit Hill and compute CI\n    fit_results = fit_hill_with_ci(I, rpu)\n    return fit_results\n\ndef solve():\n    # Define test cases as per the problem statement.\n\n    test_cases = []\n\n    # Case 1\n    params1 = {\n        'time_steps': 6,\n        'dt': 0.5,\n        'I': [0, 1, 3, 10, 30, 100],\n        'OD0': 0.05,\n        'r': 0.9,\n        'od_blank_params': (0.005, 0.001),\n        'f_bg_params': (40.0, 3.0),\n        's_ref': 9000.0,\n        'true_params': (2.0, 12.0, 2.2),\n        'perturb_params': {\n            'OD': (0.002, 0.3, 0.2),\n            'ref': (50.0, 0.7, 0.15),\n            'tar': (60.0, 0.6, 0.17),\n        }\n    }\n    test_cases.append(params1)\n\n    # Case 2\n    params2 = {\n        'time_steps': 6,\n        'dt': 0.6,\n        'I': [0, 0.5, 1.0, 2.0, 4.0],\n        'OD0': 0.06,\n        'r': 0.6,\n        'od_blank_params': (0.004, 0.0008),\n        'f_bg_params': (30.0, 2.0),\n        's_ref': 12000.0,\n        'true_params': (0.6, 1.5, 1.2),\n        'perturb_params': {\n            'OD': (0.0015, 0.3, 0.2),\n            'ref': (30.0, 0.7, 0.15),\n            'tar': (35.0, 0.6, 0.17),\n        }\n    }\n    test_cases.append(params2)\n\n    # Case 3\n    params3 = {\n        'time_steps': 6,\n        'dt': 0.4,\n        'I': [0, 5, 8, 10, 12, 20, 50],\n        'OD0': 0.04,\n        'r': 1.1,\n        'od_blank_params': (0.003, 0.0009),\n        'f_bg_params': (35.0, 2.5),\n        's_ref': 8000.0,\n        'true_params': (1.5, 10.0, 3.0),\n        'perturb_params': {\n            'OD': (0.0025, 0.3, 0.2),\n            'ref': (40.0, 0.7, 0.15),\n            'tar': (45.0, 0.6, 0.17),\n        }\n    }\n    test_cases.append(params3)\n\n    results_flat = []\n\n    for p in test_cases:\n        # Generate data for the case\n        case_data = generate_case(\n            time_steps=p['time_steps'],\n            dt=p['dt'],\n            I_array=p['I'],\n            OD0=p['OD0'],\n            r=p['r'],\n            od_blank_params=p['od_blank_params'],\n            f_bg_params=p['f_bg_params'],\n            s_ref=p['s_ref'],\n            true_params=p['true_params'],\n            perturb_params=p['perturb_params']\n        )\n        # Process case: compute slopes, RPUs, fit Hill, obtain CI\n        res = process_case(case_data)\n        results_flat.extend(res)\n\n    # Round to six decimals and print in required format.\n    results_str = [\"{:.6f}\".format(x) if np.isfinite(x) else \"nan\" for x in results_flat]\n    print(\"[\" + \",\".join(results_str) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2775671"}, {"introduction": "Once a part is designed, its physical DNA sequence must be validated against the intended specification. This practice demonstrates a critical quality control step by having you build a verification pipeline based on fundamental bioinformatics algorithms. You will use dynamic programming to align an observed sequencing read against expected part sequences and apply Bayesian inference to calculate the posterior probability of a correct match, thereby making a quantitative decision about the part's integrity. [@problem_id:2775656]", "problem": "You are tasked with implementing a self-contained sequencing verification pipeline in the context of standardized biological parts and part registries. A registry provides a finite set of expected part sequences, each represented as a string over the nucleotide alphabet $\\{ \\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T} \\}$. Given an observed read sequence, you must align it against each expected sequence, flag base-level mismatches along the maximum-likelihood alignment under a specified sequencer error model, and compute a posterior probability that the observed read was generated from the expected part rather than from a background model. All computations must be based on first principles of probability and sequence alignment, as described below.\n\nBase assumptions and definitions:\n- Let the expected sequence be denoted by $S$ and the observed read be denoted by $R$.\n- Assume an independent per-event error model for the sequencer with parameters $p_{\\mathrm{sub}}$, $p_{\\mathrm{ins}}$, and $p_{\\mathrm{del}}$, each in the open interval $(0,1)$, representing substitution, insertion, and deletion probabilities, respectively.\n- For substitutions under the correct-part hypothesis, when aligning a base $x \\in \\{ \\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T} \\}$ from $S$ to a base $y \\in \\{ \\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T} \\}$ in $R$:\n  - If $x = y$, the emission probability is $(1 - p_{\\mathrm{sub}})$.\n  - If $x \\neq y$, the emission probability for any of the three alternatives is $p_{\\mathrm{sub}} / 3$.\n- For insertions under the correct-part hypothesis, treat each inserted base as an independent event with probability $p_{\\mathrm{ins}}$, and assume the inserted base identity is uniformly distributed over $\\{ \\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T} \\}$, contributing a factor of $p_{\\mathrm{ins}} / 4$ per inserted base.\n- For deletions under the correct-part hypothesis, each deleted base contributes a factor of $p_{\\mathrm{del}}$ per deleted base.\n- Let the background model be an independent and identically distributed nucleotide process with base probabilities $q_{\\mathrm{A}}, q_{\\mathrm{C}}, q_{\\mathrm{G}}, q_{\\mathrm{T}}$ that sum to $1$. Under the background model, the likelihood of the read $R$ is the product over its bases of the corresponding background probabilities.\n\nAlignment requirement:\n- For each expected sequence $S$, compute the global alignment to $R$ that maximizes the log-likelihood under the correct-part hypothesis using dynamic programming. Allowed moves are:\n  - Diagonal: align $S[i]$ to $R[j]$ and add $\\log(1 - p_{\\mathrm{sub}})$ if $S[i] = R[j]$, otherwise add $\\log(p_{\\mathrm{sub}} / 3)$.\n  - Up: delete $S[i]$ and add $\\log(p_{\\mathrm{del}})$.\n  - Left: insert $R[j]$ relative to $S$ and add $\\log(p_{\\mathrm{ins}} / 4)$.\n- Use global alignment with linear gap penalties implied by the insertion and deletion probabilities above. Ties in the dynamic program can be resolved arbitrarily for scoring, but when reporting the best part by posterior probability you must break ties by choosing the smallest index.\n\nPosterior probability requirement:\n- For each expected part $S_k$ with prior probability $\\pi_k \\in (0,1)$, define two competing hypotheses for the observed read $R$:\n  - $H_1$: $R$ was generated from $S_k$ via the sequencer error model.\n  - $H_0$: $R$ was generated by the background model with base probabilities $q_{\\mathrm{A}}, q_{\\mathrm{C}}, q_{\\mathrm{G}}, q_{\\mathrm{T}}$.\n- Let $\\mathcal{L}_1$ denote the maximum-likelihood value under $H_1$ obtained by the optimal alignment, and let $\\mathcal{L}_0$ denote the background likelihood under $H_0$. Using Bayes’ theorem, the posterior probability of correctness is\n  $$ P(H_1 \\mid R, S_k) \\;=\\; \\frac{\\mathcal{L}_1 \\, \\pi_k}{\\mathcal{L}_1 \\, \\pi_k + \\mathcal{L}_0 \\, (1 - \\pi_k)}. $$\n- For numerical stability, your computation may be performed in the log domain.\n\nMismatch flagging:\n- Along the maximum-likelihood alignment for the selected best part, compute the count of base-level mismatches, defined as the number of aligned diagonal positions where $S[i] \\neq R[j]$. Do not count insertions or deletions toward this mismatch count.\n\nDecision rule and output per test case:\n- For each test case with a set of expected parts $\\{ S_k \\}$ and corresponding priors $\\{ \\pi_k \\}$, compute $P(H_1 \\mid R, S_k)$ for each $k$ and select the index $k^\\ast$ that maximizes the posterior probability. If two or more parts achieve the same posterior probability up to exact equality in floating-point arithmetic, choose the smallest index.\n- Report a result triple for the test case as $[k^\\ast, M^\\ast, P^\\ast]$, where $k^\\ast$ is the selected part index using zero-based indexing, $M^\\ast$ is the mismatch count along its maximum-likelihood alignment, and $P^\\ast$ is the posterior probability for that part rounded to six decimal places as a decimal number (not as a fraction and not with a percent sign).\n\nAngle units and physical units do not apply in this problem.\n\nTest suite and parameters:\nImplement your program to run the following set of test cases internally, with no external input. Each case specifies the expected part sequences, the observed read, the sequencer error parameters, the background base probabilities, and the priors over parts.\n\n- Case A:\n  - Expected parts: $[\\mathrm{ATGACCAT}, \\mathrm{ATGACCAG}]$\n  - Observed read: $\\mathrm{ATGACCAT}$\n  - Error model: $p_{\\mathrm{sub}} = 0.001$, $p_{\\mathrm{ins}} = 0.000001$, $p_{\\mathrm{del}} = 0.000001$\n  - Background: $q_{\\mathrm{A}} = 0.25$, $q_{\\mathrm{C}} = 0.25$, $q_{\\mathrm{G}} = 0.25$, $q_{\\mathrm{T}} = 0.25$\n  - Priors: $[\\pi_0 = 0.5, \\pi_1 = 0.5]$\n- Case B:\n  - Expected parts: $[\\mathrm{ATGACCAT}, \\mathrm{ATGACCAG}]$\n  - Observed read: $\\mathrm{ATGACCAG}$\n  - Error model: $p_{\\mathrm{sub}} = 0.02$, $p_{\\mathrm{ins}} = 0.001$, $p_{\\mathrm{del}} = 0.001$\n  - Background: $q_{\\mathrm{A}} = 0.25$, $q_{\\mathrm{C}} = 0.25$, $q_{\\mathrm{G}} = 0.25$, $q_{\\mathrm{T}} = 0.25$\n  - Priors: $[\\pi_0 = 0.5, \\pi_1 = 0.5]$\n- Case C:\n  - Expected parts: $[\\mathrm{TTGACA}]$\n  - Observed read: $\\mathrm{TTGACAA}$\n  - Error model: $p_{\\mathrm{sub}} = 0.005$, $p_{\\mathrm{ins}} = 0.01$, $p_{\\mathrm{del}} = 0.005$\n  - Background: $q_{\\mathrm{A}} = 0.25$, $q_{\\mathrm{C}} = 0.25$, $q_{\\mathrm{G}} = 0.25$, $q_{\\mathrm{T}} = 0.25$\n  - Priors: $[\\pi_0 = 0.8]$\n- Case D:\n  - Expected parts: $[\\mathrm{GCTAGC}]$\n  - Observed read: $\\mathrm{GCTAGC}$\n  - Error model: $p_{\\mathrm{sub}} = 0.3$, $p_{\\mathrm{ins}} = 0.05$, $p_{\\mathrm{del}} = 0.05$\n  - Background: $q_{\\mathrm{A}} = 0.25$, $q_{\\mathrm{C}} = 0.25$, $q_{\\mathrm{G}} = 0.25$, $q_{\\mathrm{T}} = 0.25$\n  - Priors: $[\\pi_0 = 0.1]$\n- Case E:\n  - Expected parts: $[\\mathrm{AACCGGTT}, \\mathrm{AACCGGTA}]$\n  - Observed read: $\\mathrm{AACCGGTC}$\n  - Error model: $p_{\\mathrm{sub}} = 0.01$, $p_{\\mathrm{ins}} = 0.001$, $p_{\\mathrm{del}} = 0.001$\n  - Background: $q_{\\mathrm{A}} = 0.25$, $q_{\\mathrm{C}} = 0.25$, $q_{\\mathrm{G}} = 0.25$, $q_{\\mathrm{T}} = 0.25$\n  - Priors: $[\\pi_0 = 0.5, \\pi_1 = 0.5]$\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-style list of results, where each result is a list $[k^\\ast, M^\\ast, P^\\ast]$ corresponding to the cases listed above in the same order. The line must contain only this list, with items separated by commas and enclosed in square brackets.\n\nNo user input should be read. All computations must be self-contained within the program. All probabilities must be treated as decimals as given above, and any requested rounding must be applied to the final posterior probabilities as specified.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extracted Givens\n- **Task**: Implement a sequencing verification pipeline for standardized biological parts.\n- **Inputs for each test case**: A set of expected part sequences $\\{S_k\\}$, an observed read sequence $R$, sequencer error parameters $p_{\\mathrm{sub}}, p_{\\mathrm{ins}}, p_{\\mathrm{del}} \\in (0, 1)$, background nucleotide probabilities $\\{q_{\\mathrm{A}}, q_{\\mathrm{C}}, q_{\\mathrm{G}}, q_{\\mathrm{T}}\\}$, and prior probabilities $\\{\\pi_k\\} \\in (0, 1)$ for each part.\n- **Hypothesis $H_1$ (Correct Part)**: The read $R$ is generated from an expected sequence $S$ with specified error probabilities.\n  - Emission probability for an aligned pair $(x, y)$ where $x \\in S, y \\in R$: $(1 - p_{\\mathrm{sub}})$ if $x=y$, and $p_{\\mathrm{sub}}/3$ if $x \\neq y$.\n  - Probability for an inserted base in $R$: $p_{\\mathrm{ins}}/4$.\n  - Probability for a deleted base from $S$: $p_{\\mathrm{del}}$.\n- **Hypothesis $H_0$ (Background)**: The read $R$ is generated from an i.i.d. background model with probabilities $q_{\\mathrm{base}}$. The likelihood $\\mathcal{L}_0$ is the product of these probabilities over the bases in $R$.\n- **Alignment**: Global alignment (Needleman-Wunsch) must be used to find the maximum log-likelihood of $R$ given $S$ under $H_1$. The scores are the logarithms of the probabilities: $\\log(1 - p_{\\mathrm{sub}})$, $\\log(p_{\\mathrm{sub}}/3)$, $\\log(p_{\\mathrm{ins}}/4)$, and $\\log(p_{\\mathrm{del}})$.\n- **Posterior Probability**: For each part $S_k$, compute $P(H_1 \\mid R, S_k) = \\frac{\\mathcal{L}_1 \\, \\pi_k}{\\mathcal{L}_1 \\, \\pi_k + \\mathcal{L}_0 \\, (1 - \\pi_k)}$, where $\\mathcal{L}_1$ is the maximum likelihood from the alignment.\n- **Mismatch Count**: Along the maximum-likelihood alignment path for the best part, count the number of substitutions ($S[i]$ aligned to $R[j]$ with $S[i] \\neq R[j]$).\n- **Decision Rule**: Select the part index $k^*$ that maximizes the posterior probability. Ties are broken by choosing the smallest index.\n- **Output**: For each test case, a triple $[k^*, M^*, P^*]$, where $k^*$ is the chosen index, $M^*$ is its mismatch count, and $P^*$ is its posterior probability rounded to six decimal places.\n\nStep 2: Validation\n- **Scientifically Grounded**: The problem is an application of fundamental principles from bioinformatics and probability theory. Sequence alignment using dynamic programming, probabilistic modeling of sequencer errors, and Bayesian inference are standard, well-established methods in computational biology. The model is a simplification but is scientifically valid.\n- **Well-Posed**: The problem is specified with mathematical precision. All inputs, parameters, and objective functions are clearly defined. The dynamic programming framework guarantees that a maximum-likelihood alignment score can be found. The formula for the posterior and the tie-breaking rule ensure a unique solution exists.\n- **Objective**: The problem is stated in precise, unbiased, and formal language, free of subjective elements.\n\nStep 3: Verdict\nThe problem is valid. It is a well-defined computational exercise in bioinformatics that requires the correct application of established algorithms and probabilistic principles. I will proceed with the solution.\n\nThe solution requires a systematic, principle-based approach. For each test case, a series of calculations must be performed for every expected part sequence $S_k$ against the observed read $R$.\n\nFirst, we address the background hypothesis, $H_0$. Under this model, the read $R$ is assumed to be generated by an independent and identically distributed process. The likelihood of observing $R = r_1 r_2 \\dots r_n$ is $\\mathcal{L}_0$. For numerical stability, we operate in the logarithmic domain. The log-likelihood is:\n$$ \\log \\mathcal{L}_0 = \\sum_{j=1}^{|R|} \\log q_{r_j} $$\nwhere $q_{r_j}$ is the background probability of the nucleotide $r_j$ at position $j$ in the read $R$.\n\nNext, we evaluate the correct-part hypothesis, $H_1$. The likelihood $\\mathcal{L}_{1,k}$ that $R$ was generated from a specific part $S_k$ is determined by finding the most probable alignment between them. This is equivalent to maximizing the sum of log-probabilities of all events (matches, substitutions, insertions, deletions) in the alignment. This is a classic global alignment problem, solvable with a dynamic programming algorithm analogous to Needleman-Wunsch.\n\nLet $S_k$ have length $m$ and $R$ have length $n$. We construct a dynamic programming table $D$ of size $(m+1) \\times (n+1)$, where $D_{i,j}$ stores the maximum log-likelihood of aligning the prefix $S_k[1..i]$ with $R[1..j]$. The scores for the alignment operations are defined as:\n- Match score: $s_{\\mathrm{match}} = \\log(1 - p_{\\mathrm{sub}})$\n- Substitution score: $s_{\\mathrm{sub}} = \\log(p_{\\mathrm{sub}} / 3)$\n- Insertion score: $s_{\\mathrm{ins}} = \\log(p_{\\mathrm{ins}} / 4)$\n- Deletion score: $s_{\\mathrm{del}} = \\log(p_{\\mathrm{del}})$\n\nThe DP table is initialized for global alignment with linear gap penalties:\n$$ D_{0,0} = 0 $$\n$$ D_{i,0} = i \\cdot s_{\\mathrm{del}} \\quad \\text{for } i \\in [1, m] $$\n$$ D_{0,j} = j \\cdot s_{\\mathrm{ins}} \\quad \\text{for } j \\in [1, n] $$\n\nThe recurrence relation for filling the rest of the table for $i \\in [1, m]$ and $j \\in [1, n]$ is:\n$$ D_{i,j} = \\max \\begin{cases} D_{i-1,j-1} + s(S_k[i], R[j]) & \\text{(Match/Mismatch)} \\\\ D_{i-1,j} + s_{\\mathrm{del}} & \\text{(Deletion)} \\\\ D_{i,j-1} + s_{\\mathrm{ins}} & \\text{(Insertion)} \\end{cases} $$\nwhere $s(S_k[i], R[j])$ is $s_{\\mathrm{match}}$ if $S_k[i] = R[j]$ and $s_{\\mathrm{sub}}$ otherwise. The maximum log-likelihood for the alignment of $S_k$ and $R$ is the value in the final cell, $\\log \\mathcal{L}_{1,k} = D_{m,n}$.\n\nTo determine the mismatch count $M^*$, we must reconstruct the optimal alignment path. This is achieved by a traceback procedure, starting from cell $(m,n)$ and moving back to $(0,0)$ by following the choices that led to the maximum score at each step. During the traceback, we increment a counter for each diagonal move from $(i,j)$ to $(i-1,j-1)$ where $S_k[i] \\neq R[j]$.\n\nWith $\\log \\mathcal{L}_0$ and $\\log \\mathcal{L}_{1,k}$ computed, we can find the posterior probability $P(H_1 \\mid R, S_k)$. The problem provides the formula:\n$$ P(H_1 \\mid R, S_k) = \\frac{\\mathcal{L}_1 \\, \\pi_k}{\\mathcal{L}_1 \\, \\pi_k + \\mathcal{L}_0 \\, (1 - \\pi_k)} $$\nTo prevent numerical underflow with small likelihood values, this is reformulated for log-space computation:\n$$ P(H_1 \\mid R, S_k) = \\frac{1}{1 + \\frac{\\mathcal{L}_0(1-\\pi_k)}{\\mathcal{L}_1\\pi_k}} = \\frac{1}{1 + \\exp\\left(\\log\\left(\\frac{\\mathcal{L}_0(1-\\pi_k)}{\\mathcal{L}_1\\pi_k}\\right)\\right)} $$\n$$ P(H_1 \\mid R, S_k) = \\left(1 + \\exp\\left( (\\log\\mathcal{L}_0 + \\log(1-\\pi_k)) - (\\log\\mathcal{L}_{1,k} + \\log\\pi_k) \\right)\\right)^{-1} $$\nThis expression is numerically stable and can be computed using standard floating-point arithmetic.\n\nFinally, for each test case, we compute this posterior probability for all candidate parts $\\{S_k\\}$. The index $k^*$ is selected corresponding to the maximum posterior probability, breaking ties by choosing the smallest index. The mismatch count $M^*$ and the rounded posterior probability $P^*$ are reported for this winning part. This entire procedure is encapsulated in the provided code.", "answer": "```python\nimport numpy as np\n\ndef align_and_count_mismatches(S, R, log_p_match, log_p_sub, log_p_ins, log_p_del):\n    \"\"\"\n    Performs global alignment to find max log-likelihood and counts mismatches.\n    \"\"\"\n    m, n = len(S), len(R)\n    # DP table for scores\n    dp = np.full((m + 1, n + 1), -np.inf)\n    # Traceback table: 0 for diagonal, 1 for up (deletion), 2 for left (insertion)\n    trace = np.zeros((m + 1, n + 1), dtype=int)\n\n    # Initialization\n    dp[0, 0] = 0\n    for i in range(1, m + 1):\n        dp[i, 0] = i * log_p_del\n        trace[i, 0] = 1\n    for j in range(1, n + 1):\n        dp[0, j] = j * log_p_ins\n        trace[0, j] = 2\n\n    # Fill DP table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            match_mismatch_score = log_p_match if S[i - 1] == R[j - 1] else log_p_sub\n            \n            scores = [\n                dp[i - 1, j - 1] + match_mismatch_score,  # Diagonal\n                dp[i - 1, j] + log_p_del,              # Up (Deletion)\n                dp[i, j - 1] + log_p_ins               # Left (Insertion)\n            ]\n            \n            best_score_idx = np.argmax(scores)\n            dp[i, j] = scores[best_score_idx]\n            trace[i, j] = best_score_idx\n\n    log_L1 = dp[m, n]\n\n    # Traceback for mismatch count\n    mismatches = 0\n    i, j = m, n\n    while i > 0 or j > 0:\n        move = trace[i, j]\n        if move == 0:  # Diagonal move\n            if S[i - 1] != R[j - 1]:\n                mismatches += 1\n            i -= 1\n            j -= 1\n        elif move == 1:  # Up move (deletion)\n            i -= 1\n        else:  # Left move (insertion)\n            j -= 1\n            \n    return log_L1, mismatches\n\ndef solve():\n    test_cases = [\n        {\n            \"parts\": [\"ATGACCAT\", \"ATGACCAG\"],\n            \"read\": \"ATGACCAT\",\n            \"params\": {\"p_sub\": 0.001, \"p_ins\": 1e-6, \"p_del\": 1e-6},\n            \"background\": {\"A\": 0.25, \"C\": 0.25, \"G\": 0.25, \"T\": 0.25},\n            \"priors\": [0.5, 0.5]\n        },\n        {\n            \"parts\": [\"ATGACCAT\", \"ATGACCAG\"],\n            \"read\": \"ATGACCAG\",\n            \"params\": {\"p_sub\": 0.02, \"p_ins\": 0.001, \"p_del\": 0.001},\n            \"background\": {\"A\": 0.25, \"C\": 0.25, \"G\": 0.25, \"T\": 0.25},\n            \"priors\": [0.5, 0.5]\n        },\n        {\n            \"parts\": [\"TTGACA\"],\n            \"read\": \"TTGACAA\",\n            \"params\": {\"p_sub\": 0.005, \"p_ins\": 0.01, \"p_del\": 0.005},\n            \"background\": {\"A\": 0.25, \"C\": 0.25, \"G\": 0.25, \"T\": 0.25},\n            \"priors\": [0.8]\n        },\n        {\n            \"parts\": [\"GCTAGC\"],\n            \"read\": \"GCTAGC\",\n            \"params\": {\"p_sub\": 0.3, \"p_ins\": 0.05, \"p_del\": 0.05},\n            \"background\": {\"A\": 0.25, \"C\": 0.25, \"G\": 0.25, \"T\": 0.25},\n            \"priors\": [0.1]\n        },\n        {\n            \"parts\": [\"AACCGGTT\", \"AACCGGTA\"],\n            \"read\": \"AACCGGTC\",\n            \"params\": {\"p_sub\": 0.01, \"p_ins\": 0.001, \"p_del\": 0.001},\n            \"background\": {\"A\": 0.25, \"C\": 0.25, \"G\": 0.25, \"T\": 0.25},\n            \"priors\": [0.5, 0.5]\n        }\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        R = case[\"read\"]\n        params = case[\"params\"]\n        p_sub, p_ins, p_del = params[\"p_sub\"], params[\"p_ins\"], params[\"p_del\"]\n        log_p_match = np.log(1 - p_sub)\n        log_p_sub = np.log(p_sub / 3)\n        log_p_ins = np.log(p_ins / 4)\n        log_p_del = np.log(p_del)\n\n        log_q_map = {base: np.log(p) for base, p in case[\"background\"].items()}\n        log_L0 = sum(log_q_map[base] for base in R)\n\n        part_results = []\n        for S_k, pi_k in zip(case[\"parts\"], case[\"priors\"]):\n            log_L1_k, mismatches = align_and_count_mismatches(S_k, R, log_p_match, log_p_sub, log_p_ins, log_p_del)\n\n            # Compute posterior probability in log space for stability\n            log_pi_k = np.log(pi_k)\n            log_1_minus_pi_k = np.log(1 - pi_k)\n            \n            term_H1 = log_L1_k + log_pi_k\n            term_H0 = log_L0 + log_1_minus_pi_k\n            \n            # Posterior = 1 / (1 + exp(term_H0 - term_H1))\n            diff_log_terms = term_H0 - term_H1\n            posterior = 1.0 / (1.0 + np.exp(diff_log_terms))\n            \n            part_results.append({\n                \"posterior\": posterior,\n                \"mismatches\": mismatches\n            })\n        \n        # Determine the best part based on max posterior, with tie-breaking\n        best_part_idx, max_posterior = -1, -1.0\n        for i, res in enumerate(part_results):\n            if res[\"posterior\"] > max_posterior:\n                max_posterior = res[\"posterior\"]\n                best_part_idx = i\n\n        best_mismatches = part_results[best_part_idx][\"mismatches\"]\n        \n        final_results.append([\n            best_part_idx,\n            best_mismatches,\n            round(max_posterior, 6)\n        ])\n\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```", "id": "2775656"}, {"introduction": "A registry's value lies in its ability to be searched and accessed programmatically. This exercise introduces you to querying modern part registries that use the Resource Description Framework (RDF) data model and the SPARQL query language. You will learn to construct sophisticated queries to filter, sort, and retrieve parts based on a combination of qualitative annotations, such as their functional role, and quantitative characterization data, such as their measured strength. [@problem_id:2775693]", "problem": "You are asked to formalize a query-construction problem grounded in the standardization of biological parts and their registration in knowledge bases that expose Resource Description Framework (RDF) interfaces, such as SynBioHub, which encodes Synthetic Biology Open Language (SBOL) data in RDF. You will generate Structured Query Language for RDF (SPARQL) queries to retrieve standardized parts based on Sequence Ontology roles and a numeric characterization threshold, and you will demonstrate correct pagination and sorting. Your program will not query a live endpoint; instead, it will algorithmically construct query strings and verify their compliance with formally specified constraints.\n\nFundamental base and core definitions:\n- Resource Description Framework (RDF) models knowledge as triples of the form subject-predicate-object. A triple can be written as $(s, p, o)$.\n- Uniform Resource Identifier (URI) and Internationalized Resource Identifier (IRI) are standardized identifiers for RDF resources.\n- Synthetic Biology Open Language (SBOL) represents biological parts, such as components and their roles; in RDF form, a part can be a node of type $sbol:ComponentDefinition$ with a role from the Sequence Ontology (SO).\n- Sequence Ontology (SO) assigns IRIs to sequence feature types; a transcriptional terminator is represented by $so:0000141$ under the prefix $so: \\langle http://purl.obolibrary.org/obo/SO_ \\rangle$, which expands to the IRI $\\langle http://purl.obolibrary.org/obo/SO_0000141 \\rangle$.\n- SPARQL is a declarative query language for RDF. Clauses such as SELECT, WHERE, FILTER, ORDER BY, LIMIT, and OFFSET are used to retrieve, constrain, sort, and paginate results. The clause ORDER BY supports $ASC(\\cdot)$ or $DESC(\\cdot)$ to specify ascending or descending order.\n\nProblem requirements:\n- Your task is to generate, for each parameter set, a SPARQL SELECT query that retrieves all SBOL parts that are of type $sbol:ComponentDefinition$ and have the Sequence Ontology role $so:0000141$ (terminator). Each such part must have a numeric characterization we will call Relative Promoter Units (RPU) for the purpose of this exercise, modeled as a measurement resource linked to the part. The query must return the variables $?part$, $?label$, and $?rpu$ at minimum; when sorting by display identifier, it must additionally return $?displayId$.\n- The RDF vocabulary to assume is:\n  - Prefixes:\n    - $rdf:$ $\\langle http://www.w3.org/1999/02/22-rdf-syntax-ns\\# \\rangle$\n    - $rdfs:$ $\\langle http://www.w3.org/2000/01/rdf-schema\\# \\rangle$\n    - $xsd:$ $\\langle http://www.w3.org/2001/XMLSchema\\# \\rangle$\n    - $sbol:$ $\\langle http://sbols.org/v2\\# \\rangle$\n    - $so:$ $\\langle http://purl.obolibrary.org/obo/SO_ \\rangle$\n    - $meas:$ $\\langle http://example.org/measurement\\# \\rangle$\n  - Required triple patterns in the WHERE clause:\n    - $?part \\ rdf:type \\ sbol:ComponentDefinition.$\n    - $?part \\ sbol:role \\ so:0000141.$\n    - $?part \\ rdfs:label \\ ?label.$\n    - $?part \\ meas:hasMeasurement \\ ?m.$\n    - $?m \\ rdf:type \\ meas:Measurement.$\n    - $?m \\ meas:type \\ meas:RPU.$\n    - $?m \\ meas:value \\ ?rpu.$\n  - Numeric threshold filtering must be enforced using a SPARQL FILTER of the form $FILTER(xsd:decimal(?rpu) > T)$ where $T$ is the numeric threshold parameter.\n  - Sorting and pagination:\n    - Let the sort key be $K \\in \\{\\text{\"label\"}, \\text{\"rpu\"}, \\text{\"displayId\"}\\}$ and the sort direction be $D \\in \\{\\text{\"ASC\"}, \\text{\"DESC\"}\\}$.\n    - If $K = \\text{\"label\"}$, include $ORDER \\ BY \\ D(?label)$.\n    - If $K = \\text{\"rpu\"}$, include $ORDER \\ BY \\ D(xsd:decimal(?rpu))$.\n    - If $K = \\text{\"displayId\"}$, you must also include the triple pattern $?part \\ sbol:displayId \\ ?displayId.$ in the WHERE clause, include $?displayId$ in the projection (SELECT clause), and include $ORDER \\ BY \\ D(?displayId)$.\n    - Use $LIMIT \\ L$ and $OFFSET \\ O$ for pagination, where $L$ is the maximum number of rows and $O$ is the row offset.\n- Your program must, for each parameter set, construct the SPARQL query string and then verify that the constructed string contains the correct structural components implied by the parameters. For each parameter set, output a boolean indicating whether the constructed query satisfies all required constraints for that case.\n\nInput specification:\n- There is no runtime input; instead, you must hard-code the following test suite of parameter tuples $(T, L, O, K, D)$:\n  1. $(T{=}1.0, \\ L{=}25, \\ O{=}0, \\ K{=}\\text{\"label\"}, \\ D{=}\\text{\"ASC\"})$\n  2. $(T{=}0.0, \\ L{=}10, \\ O{=}20, \\ K{=}\\text{\"rpu\"}, \\ D{=}\\text{\"DESC\"})$\n  3. $(T{=}3.5, \\ L{=}50, \\ O{=}100, \\ K{=}\\text{\"displayId\"}, \\ D{=}\\text{\"ASC\"})$\n  4. $(T{=}2.0, \\ L{=}0, \\ O{=}0, \\ K{=}\\text{\"rpu\"}, \\ D{=}\\text{\"ASC\"})$  (boundary case with zero limit)\n  5. $(T{=}-1.0, \\ L{=}5, \\ O{=}5, \\ K{=}\\text{\"label\"}, \\ D{=}\\text{\"DESC\"})$  (edge case with negative threshold)\n\nVerification criteria your program must check for each constructed query:\n- The presence of all required PREFIX declarations listed above.\n- The presence of all required WHERE triple patterns, including the $sbol:displayId$ triple only when $K = \\text{\"displayId\"}$.\n- The FILTER must be of the exact form $FILTER(xsd:decimal(?rpu) > T)$ with $T$ instantiated to the numeric threshold of the test case.\n- The SELECT clause must include $?part$, $?label$, and $?rpu$, and must include $?displayId$ if and only if $K = \\text{\"displayId\"}$.\n- The ORDER BY clause must match the parameterized sort key and direction as specified above, including the required $xsd:decimal$ cast when $K = \\text{\"rpu\"}$.\n- The LIMIT and OFFSET values must match $L$ and $O$ exactly.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, if all five test cases pass their respective verifications, the output would be $[\\text{True},\\text{True},\\text{True},\\text{True},\\text{True}]$. The actual output should be $[\\text{b}_1,\\text{b}_2,\\text{b}_3,\\text{b}_4,\\text{b}_5]$ where each $\\text{b}_i$ is a boolean.\n\nAngle units, physical units, and percentages:\n- No angle units or physical units are involved. All numeric thresholds are pure numbers. Do not output any percentages.\n\nYour solution must be a complete, runnable Python program that constructs and verifies the SPARQL queries for the provided test suite and prints the required single-line boolean list as specified above.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the domain of synthetic biology data management, using established standards such as RDF, SBOL, and SPARQL. The requirements are objective, well-posed, and self-contained, providing a clear basis for algorithmic implementation. The task is to construct and verify SPARQL queries, which is a formal and verifiable procedure.\n\nThe solution is architected as a Python program that performs two sequential operations for each provided parameter set $(T, L, O, K, D)$: query construction and query verification. This approach ensures that the program not only generates the required output but also internally validates its own logic against the problem's strict specifications.\n\nFirst, a query generation function is designed. This function takes a set of parameters as input: a numerical threshold $T$, a limit $L$, an offset $O$, a sort key $K \\in \\{\\text{\"label\"}, \\text{\"rpu\"}, \\text{\"displayId\"}\\}$, and a sort direction $D \\in \\{\\text{\"ASC\"}, \\text{\"DESC\"}\\}$. It systematically assembles a SPARQL query string from its constituent clauses.\n\nThe `PREFIX` clause is static and includes all six required namespace prefixes: $rdf:$, $rdfs:$, $xsd:$, $sbol:$, $so:$, and $meas:$.\n\nThe `SELECT` clause is constructed dynamically. It always projects the variables $?part$, $?label$, and $?rpu$. Critically, if the sort key $K$ is specified as $\\text{\"displayId\"}$, the variable $?displayId$ is also included in the projection.\n\nThe `WHERE` clause contains a fixed set of seven triple patterns that define the core graph structure to be matched. These patterns establish that the resource identified by $?part$ must be an $sbol:ComponentDefinition$, possess the Sequence Ontology role for a terminator ($so:0000141$), and be associated with a numerical measurement of type $meas:RPU$ via an intermediate measurement resource $?m$. If the sort key $K$ is $\\text{\"displayId\"}$, an additional triple pattern, $?part \\ sbol:displayId \\ ?displayId.$, is appended to the `WHERE` clause. Finally, a `FILTER` expression of the form $FILTER(xsd:decimal(?rpu) > T)$ is included to enforce the numerical threshold constraint on the measurement value.\n\nThe query modifiers are appended last. The `ORDER BY` clause's content is determined by the parameters $K$ and $D$. For $K=\\text{\"label\"}$, it becomes $ORDER \\ BY \\ D(?label)$. For $K=\\text{\"rpu\"}$, it is $ORDER \\ BY \\ D(xsd:decimal(?rpu))$, which correctly sorts the values numerically. For $K=\\text{\"displayId\"}$, it is $ORDER \\ BY \\ D(?displayId)$. The `LIMIT` and `OFFSET` clauses are directly populated from the parameters $L$ and $O$ to handle pagination.\n\nSecond, a verification function is implemented to check the generated query string for correctness. This step is crucial for confirming that the construction logic adheres to all constraints. The verification is a series of checks on the generated string:\n1.  **Prefixes**: It confirms the presence of all required `PREFIX` declarations.\n2.  **SELECT Clause**: It verifies that the `SELECT` clause contains $?part$, $?label$, and $?rpu$. It also enforces the \"if and only if\" condition for $?displayId$: its presence is mandatory if $K = \\text{\"displayId\"}$ and forbidden otherwise.\n3.  **WHERE Clause**: It checks for the presence of all mandatory triple patterns. Similarly, it enforces the \"if and only if\" condition for the $?part \\ sbol:displayId \\ ?displayId.$ pattern.\n4.  **FILTER Clause**: It validates that the `FILTER` clause is present in the exact form $FILTER(xsd:decimal(?rpu) > T)$, with $T$ instantiated to the specific value for the test case.\n5.  **ORDER BY Clause**: It confirms the `ORDER BY` clause matches the structure dictated by parameters $K$ and $D$.\n6.  **LIMIT and OFFSET**: It ensures the `LIMIT` and `OFFSET` clauses are present with the correct integer values $L$ and $O$.\n\nThe program iterates through the five hard-coded test cases. For each case, it generates the query and runs the verification logic. The boolean result of the verification is stored. The final output is a list of these boolean values, formatted as a single string as required. This design provides a robust and verifiable solution to the specified problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_and_verify_query(T, L, O, K, D):\n    \"\"\"\n    Generates a SPARQL query based on the given parameters and verifies its structure.\n\n    Args:\n        T (float): The numeric threshold for RPU.\n        L (int): The LIMIT for pagination.\n        O (int): The OFFSET for pagination.\n        K (str): The sort key (\"label\", \"rpu\", or \"displayId\").\n        D (str): The sort direction (\"ASC\" or \"DESC\").\n\n    Returns:\n        bool: True if the generated query passes all verification checks, False otherwise.\n    \"\"\"\n    \n    # 1. Query Construction\n    \n    prefixes = [\n        \"PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\",\n        \"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\",\n        \"PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\",\n        \"PREFIX sbol: <http://sbols.org/v2#>\",\n        \"PREFIX so: <http://purl.obolibrary.org/obo/SO_>\",\n        \"PREFIX meas: <http://example.org/measurement#>\"\n    ]\n    prefix_str = \"\\n\".join(prefixes)\n\n    select_vars = \"?part ?label ?rpu\"\n    if K == \"displayId\":\n        select_vars += \" ?displayId\"\n    select_clause = f\"SELECT {select_vars}\"\n\n    where_patterns = [\n        \"?part rdf:type sbol:ComponentDefinition.\",\n        \"?part sbol:role so:0000141.\",\n        \"?part rdfs:label ?label.\",\n        \"?part meas:hasMeasurement ?m.\",\n        \"?m rdf:type meas:Measurement.\",\n        \"?m meas:type meas:RPU.\",\n        \"?m meas:value ?rpu.\"\n    ]\n    if K == \"displayId\":\n        where_patterns.append(\"?part sbol:displayId ?displayId.\")\n    \n    filter_clause_str = f\"FILTER(xsd:decimal(?rpu) > {str(T)})\"\n    \n    where_patterns_str = \"  \" + \"\\n  \".join(where_patterns)\n    where_clause = f\"WHERE {{\\n{where_patterns_str}\\n  {filter_clause_str}\\n}}\"\n\n    order_by_clause = \"\"\n    if K == \"label\":\n        order_by_clause = f\"ORDER BY {D}(?label)\"\n    elif K == \"rpu\":\n        order_by_clause = f\"ORDER BY {D}(xsd:decimal(?rpu))\"\n    elif K == \"displayId\":\n        order_by_clause = f\"ORDER BY {D}(?displayId)\"\n\n    limit_clause = f\"LIMIT {L}\"\n    offset_clause = f\"OFFSET {O}\"\n    \n    modifiers_str = \"\\n\".join([order_by_clause, limit_clause, offset_clause])\n\n    query = \"\\n\".join([prefix_str, select_clause, where_clause, modifiers_str])\n\n    # 2. Query Verification\n\n    # Check Prefixes\n    if not all(p in query for p in prefixes):\n        return False\n\n    # Check SELECT clause\n    try:\n        select_part = query.split(\"SELECT\")[1].split(\"WHERE\")[0]\n        if not (\"?part\" in select_part and \"?label\" in select_part and \"?rpu\" in select_part):\n            return False\n        if K == \"displayId\":\n            if \"?displayId\" not in select_part:\n                return False\n        else: # \"if and only if\" check\n            if \"?displayId\" in select_part:\n                return False\n    except IndexError:\n        return False\n\n    # Check WHERE triple patterns\n    base_where_patterns = [\n        \"?part rdf:type sbol:ComponentDefinition.\",\n        \"?part sbol:role so:0000141.\",\n        \"?part rdfs:label ?label.\",\n        \"?part meas:hasMeasurement ?m.\",\n        \"?m rdf:type meas:Measurement.\",\n        \"?m meas:type meas:RPU.\",\n        \"?m meas:value ?rpu.\"\n    ]\n    if not all(p in query for p in base_where_patterns):\n        return False\n    \n    displayid_pattern = \"?part sbol:displayId ?displayId.\"\n    if K == \"displayId\":\n        if displayid_pattern not in query:\n            return False\n    else: # \"if and only if\" check\n        if displayid_pattern in query:\n            return False\n\n    # Check FILTER clause\n    expected_filter = f\"FILTER(xsd:decimal(?rpu) > {str(T)})\"\n    if expected_filter not in query:\n        return False\n\n    # Check ORDER BY clause\n    if order_by_clause not in query:\n        return False\n\n    # Check LIMIT and OFFSET clauses\n    if f\"LIMIT {L}\" not in query or f\"OFFSET {O}\" not in query:\n        return False\n        \n    return True\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 25, 0, \"label\", \"ASC\"),\n        (0.0, 10, 20, \"rpu\", \"DESC\"),\n        (3.5, 50, 100, \"displayId\", \"ASC\"),\n        (2.0, 0, 0, \"rpu\", \"ASC\"),\n        (-1.0, 5, 5, \"label\", \"DESC\"),\n    ]\n\n    results = []\n    for case in test_cases:\n        T, L, O, K, D = case\n        is_valid = generate_and_verify_query(T, L, O, K, D)\n        results.append(is_valid)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2775693"}]}