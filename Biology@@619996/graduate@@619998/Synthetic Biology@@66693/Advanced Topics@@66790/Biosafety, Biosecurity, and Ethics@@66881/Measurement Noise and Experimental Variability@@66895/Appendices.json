{"hands_on_practices": [{"introduction": "Every experimental measurement carries some degree of uncertainty, and a crucial skill for any quantitative scientist is understanding how these individual uncertainties propagate into a final calculated result. This practice guides you through deriving and applying the 'delta method,' a first-order approximation for error propagation, to a common calculation in fluorescence assays [@problem_id:2749348]. By constructing an uncertainty budget, you will learn to dissect the total variance and identify which input measurement is the dominant source of noise, a key insight for optimizing experimental protocols.", "problem": "Consider a high-throughput fluorescence assay in synthetic biology where a promoterâ€™s activity is reported as a calibrated fold-change relative to a reference. The reported quantity is the function $Q$ of four jointly distributed random variables $X = (I_s, I_r, B, k)$ defined by\n$$\nQ = \\frac{k\\,(I_s - B)}{I_r - B},\n$$\nwhere $I_s$ is the instrument-reported mean intensity for the sample, $I_r$ is the instrument-reported mean intensity for a reference, $B$ is a background offset measured in the same run, and $k$ is a calibration factor mapping arbitrary units to a normalized scale. Assume that $(I_s, I_r, B, k)$ is well approximated as jointly Gaussian near its mean $\\mu = (\\mu_s, \\mu_r, \\mu_b, k_0)$ with standard deviations $\\sigma = (\\sigma_s, \\sigma_r, \\sigma_b, \\sigma_k)$. Assume that $I_s$ and $I_r$ have correlation coefficient $\\rho$ and that $B$ and $k$ are independent of $(I_s, I_r)$ and of each other. You may assume $\\mu_r > \\mu_b$ and $\\mu_s > \\mu_b$ for all test cases so that the denominator is positive at the linearization point.\n\nTask 1 (derivation): Starting only from first principles of variance and covariance definitions, linearization by first-order Taylor expansion, and the definition of covariance propagation for linear transformations, derive a first-order approximation to $\\mathrm{Var}(Q)$ in terms of the gradient of $Q$ evaluated at the mean $\\mu$ and the covariance matrix of $X$. Explicitly express the gradient components as partial derivatives of $Q$ with respect to $I_s$, $I_r$, $B$, and $k$, evaluated at $(\\mu_s, \\mu_r, \\mu_b, k_0)$. Do not invoke any pre-packaged \"shortcut\" formulas; instead, build the result from the definitions of expectation, variance, covariance, and the first-order Taylor approximation.\n\nTask 2 (uncertainty budget): Using the linearized variance from Task 1, express the total approximate variance as a sum of contributions attributable to the marginal variances of $I_s$, $I_r$, $B$, and $k$, together with the contribution from the covariance between $I_s$ and $I_r$. Define the fractional contribution of each term as that term divided by the total approximate variance. Note that the covariance contribution can be negative; report it as a signed decimal. Define the standard uncertainty $u_Q$ as the positive square root of the total approximate variance.\n\nTask 3 (computation and reporting): For each parameter set in the test suite below, compute $u_Q$ and the fractional contributions of the terms corresponding to $I_s$, $I_r$, $B$, $k$, and the covariance between $I_s$ and $I_r$. For each case, output a list of six floating-point numbers in the order $[u_Q, f_{I_s}, f_{I_r}, f_B, f_k, f_{\\mathrm{cov}}]$, where each $f_{\\cdot}$ is the fractional contribution defined in Task 2. Round each value to six decimal places.\n\nTest suite (each case is given as $(\\mu_s, \\sigma_s, \\mu_r, \\sigma_r, \\mu_b, \\sigma_b, k_0, \\sigma_k, \\rho)$):\n- Case A: $(\\mu_s, \\sigma_s, \\mu_r, \\sigma_r, \\mu_b, \\sigma_b, k_0, \\sigma_k, \\rho) = (10000, 300, 8000, 250, 500, 50, 1.0, 0.02, 0.6)$.\n- Case B (small denominator difference): $(\\mu_s, \\sigma_s, \\mu_r, \\sigma_r, \\mu_b, \\sigma_b, k_0, \\sigma_k, \\rho) = (700, 30, 600, 30, 500, 20, 1.0, 0.05, 0.6)$.\n- Case C (no correlation, exact background): $(\\mu_s, \\sigma_s, \\mu_r, \\sigma_r, \\mu_b, \\sigma_b, k_0, \\sigma_k, \\rho) = (9500, 400, 9000, 400, 0, 0, 0.8, 0.01, 0.0)$.\n- Case D (strong positive correlation): $(\\mu_s, \\sigma_s, \\mu_r, \\sigma_r, \\mu_b, \\sigma_b, k_0, \\sigma_k, \\rho) = (12000, 600, 11000, 600, 400, 80, 1.2, 0.03, 0.95)$.\n\nFinal output format: Your program should produce a single line of output containing a Python-like list of four sublists, one per case, in the order A, B, C, D. Each sublist must be of the form $[u_Q, f_{I_s}, f_{I_r}, f_B, f_k, f_{\\mathrm{cov}}]$ with each entry rounded to six decimal places. For example, an output with two hypothetical cases could look like $[[x_{11}, x_{12}, x_{13}, x_{14}, x_{15}, x_{16}],[x_{21}, x_{22}, x_{23}, x_{24}, x_{25}, x_{26}]]$ where each $x_{ij}$ is a decimal rounded to six places. No physical units are required; report pure numbers. Angles are not involved. Percentages must not be used; fractional contributions must be decimals.", "solution": "We begin from first principles. Let $X = (I_s, I_r, B, k)^{\\top}$ be a random vector with mean $\\mu = (\\mu_s, \\mu_r, \\mu_b, k_0)^{\\top}$ and covariance matrix $\\Sigma$. Consider the scalar function\n$$\nQ(X) = \\frac{k\\,(I_s - B)}{I_r - B}.\n$$\nBy the first-order Taylor expansion (linearization) of $Q$ around the mean $\\mu$, we have\n$$\nQ(X) \\approx Q(\\mu) + \\nabla Q(\\mu)^{\\top} (X - \\mu),\n$$\nwhere $\\nabla Q(\\mu)$ is the gradient of $Q$ evaluated at $\\mu$. Taking variance on both sides and using the definition of variance and covariance for linear transformations, namely that for a constant vector $a$ and random vector $X$ with covariance $\\Sigma$, $\\mathrm{Var}(a^{\\top}X) = a^{\\top}\\Sigma a$, we obtain the first-order (delta method) approximation\n$$\n\\mathrm{Var}(Q) \\approx \\nabla Q(\\mu)^{\\top} \\,\\Sigma\\, \\nabla Q(\\mu).\n$$\n\nIt remains to compute the gradient components. Define $N = I_s - B$ and $D = I_r - B$, so that $Q = k\\,N/D$. Using the quotient rule and the chain rule, with partial derivatives taken holding other variables fixed, we obtain\n$$\n\\frac{\\partial Q}{\\partial k} = \\frac{N}{D},\n\\qquad\n\\frac{\\partial Q}{\\partial I_s} = \\frac{k}{D},\n\\qquad\n\\frac{\\partial Q}{\\partial I_r} = -\\,\\frac{k\\,N}{D^2},\n\\qquad\n\\frac{\\partial Q}{\\partial B} = k\\,\\frac{N - D}{D^2}.\n$$\nEvaluating at the mean point $(\\mu_s, \\mu_r, \\mu_b, k_0)$ gives $N_0 = \\mu_s - \\mu_b$, $D_0 = \\mu_r - \\mu_b$, and the gradient vector\n$$\ng = \\nabla Q(\\mu) =\n\\begin{bmatrix}\n\\frac{\\partial Q}{\\partial I_s} \\\\\n\\frac{\\partial Q}{\\partial I_r} \\\\\n\\frac{\\partial Q}{\\partial B} \\\\\n\\frac{\\partial Q}{\\partial k}\n\\end{bmatrix}_{\\mu}\n=\n\\begin{bmatrix}\n\\frac{k_0}{D_0} \\\\\n-\\,\\frac{k_0\\,N_0}{D_0^2} \\\\\nk_0\\,\\frac{N_0 - D_0}{D_0^2} \\\\\n\\frac{N_0}{D_0}\n\\end{bmatrix}.\n$$\n\nUnder the stated assumptions, the covariance matrix $\\Sigma$ has the following nonzero entries: $\\mathrm{Var}(I_s) = \\sigma_s^2$, $\\mathrm{Var}(I_r) = \\sigma_r^2$, $\\mathrm{Var}(B) = \\sigma_b^2$, $\\mathrm{Var}(k) = \\sigma_k^2$, and $\\mathrm{Cov}(I_s, I_r) = \\rho\\,\\sigma_s\\,\\sigma_r$. All covariances involving $B$ or $k$ with other variables are zero, and $\\mathrm{Cov}(I_s, I_s) = \\sigma_s^2$, $\\mathrm{Cov}(I_r, I_r) = \\sigma_r^2$, $\\mathrm{Cov}(B, B) = \\sigma_b^2$, $\\mathrm{Cov}(k, k) = \\sigma_k^2$ by definition.\n\nTherefore, substituting into the quadratic form yields\n$$\n\\mathrm{Var}(Q) \\approx\n\\left(\\frac{\\partial Q}{\\partial I_s}\\right)^{2}\\sigma_s^{2}\n+\n\\left(\\frac{\\partial Q}{\\partial I_r}\\right)^{2}\\sigma_r^{2}\n+\n\\left(\\frac{\\partial Q}{\\partial B}\\right)^{2}\\sigma_b^{2}\n+\n\\left(\\frac{\\partial Q}{\\partial k}\\right)^{2}\\sigma_k^{2}\n+\n2\\left(\\frac{\\partial Q}{\\partial I_s}\\right)\\left(\\frac{\\partial Q}{\\partial I_r}\\right)\\mathrm{Cov}(I_s, I_r).\n$$\nThis expression is a direct application of the bilinear form $g^{\\top}\\Sigma g$ and the specified sparsity of $\\Sigma$. The standard uncertainty is then defined as\n$$\nu_Q = \\sqrt{\\mathrm{Var}(Q)}.\n$$\n\nFor the uncertainty budget, define the following additive contributions:\n$$\nT_{I_s} = \\left(\\frac{\\partial Q}{\\partial I_s}\\right)^{2}\\sigma_s^{2},\\quad\nT_{I_r} = \\left(\\frac{\\partial Q}{\\partial I_r}\\right)^{2}\\sigma_r^{2},\\quad\nT_{B} = \\left(\\frac{\\partial Q}{\\partial B}\\right)^{2}\\sigma_b^{2},\\quad\nT_{k} = \\left(\\frac{\\partial Q}{\\partial k}\\right)^{2}\\sigma_k^{2},\\quad\nT_{\\mathrm{cov}} = 2\\left(\\frac{\\partial Q}{\\partial I_s}\\right)\\left(\\frac{\\partial Q}{\\partial I_r}\\right)\\rho\\,\\sigma_s\\,\\sigma_r.\n$$\nThe total is\n$$\nT_{\\mathrm{tot}} = T_{I_s} + T_{I_r} + T_{B} + T_{k} + T_{\\mathrm{cov}},\n$$\nand $u_Q = \\sqrt{T_{\\mathrm{tot}}}$ provided $T_{\\mathrm{tot}} > 0$. The fractional contributions are then defined as\n$$\nf_{I_s} = \\frac{T_{I_s}}{T_{\\mathrm{tot}}},\\quad\nf_{I_r} = \\frac{T_{I_r}}{T_{\\mathrm{tot}}},\\quad\nf_{B} = \\frac{T_{B}}{T_{\\mathrm{tot}}},\\quad\nf_{k} = \\frac{T_{k}}{T_{\\mathrm{tot}}},\\quad\nf_{\\mathrm{cov}} = \\frac{T_{\\mathrm{cov}}}{T_{\\mathrm{tot}}}.\n$$\nNote that $f_{\\mathrm{cov}}$ can be negative, especially when $\\rho > 0$ because $\\frac{\\partial Q}{\\partial I_s} > 0$ and $\\frac{\\partial Q}{\\partial I_r}  0$ for $N_0, D_0, k_0 > 0$, making $T_{\\mathrm{cov}}$ negative and potentially reducing the total variance.\n\nAlgorithmic steps for each case:\n1. Compute $N_0 = \\mu_s - \\mu_b$ and $D_0 = \\mu_r - \\mu_b$.\n2. Compute the gradient components at $\\mu$: $\\frac{\\partial Q}{\\partial I_s} = k_0/D_0$, $\\frac{\\partial Q}{\\partial I_r} = -k_0 N_0/D_0^2$, $\\frac{\\partial Q}{\\partial B} = k_0 (N_0 - D_0)/D_0^2$, $\\frac{\\partial Q}{\\partial k} = N_0/D_0$.\n3. Compute the five contributions $T_{I_s}$, $T_{I_r}$, $T_{B}$, $T_{k}$, $T_{\\mathrm{cov}}$ using the given standard deviations and $\\rho$.\n4. Sum to get $T_{\\mathrm{tot}}$ and then $u_Q = \\sqrt{T_{\\mathrm{tot}}}$, and compute the fractional contributions $f_{\\cdot}$ by dividing each $T_{\\cdot}$ by $T_{\\mathrm{tot}}$.\n5. Round $u_Q$ and all $f_{\\cdot}$ to six decimal places and report them in the specified order.\n\nApplying this procedure to the provided test suite yields the required output. The calculation is numerically stable provided $D_0$ is not too small and the parameter sets satisfy $\\mu_r > \\mu_b$ and $\\mu_s > \\mu_b$, which is guaranteed by the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_uncertainty_and_budget(mu_s, sigma_s, mu_r, sigma_r, mu_b, sigma_b, k0, sigma_k, rho):\n    # Compute N0 and D0\n    N0 = mu_s - mu_b\n    D0 = mu_r - mu_b\n\n    # Gradient components at the mean\n    dQ_dIs = k0 / D0\n    dQ_dIr = -k0 * N0 / (D0 ** 2)\n    dQ_dB  = k0 * (N0 - D0) / (D0 ** 2)\n    dQ_dk  = N0 / D0\n\n    # Variances and covariance\n    var_Is = sigma_s ** 2\n    var_Ir = sigma_r ** 2\n    var_B  = sigma_b ** 2\n    var_k  = sigma_k ** 2\n    cov_IsIr = rho * sigma_s * sigma_r\n\n    # Contributions\n    T_Is  = (dQ_dIs ** 2) * var_Is\n    T_Ir  = (dQ_dIr ** 2) * var_Ir\n    T_B   = (dQ_dB  ** 2) * var_B\n    T_k   = (dQ_dk  ** 2) * var_k\n    T_cov = 2.0 * dQ_dIs * dQ_dIr * cov_IsIr\n\n    T_tot = T_Is + T_Ir + T_B + T_k + T_cov\n\n    # Numerical guard: total variance should be non-negative; clamp tiny negatives to zero due to rounding\n    if T_tot  0 and abs(T_tot)  1e-15:\n        T_tot = 0.0\n    if T_tot = 0:\n        # In pathological cases (not expected in the test suite), return zeros\n        u_Q = 0.0\n        f_Is = f_Ir = f_B = f_k = f_cov = 0.0\n    else:\n        u_Q = np.sqrt(T_tot)\n        f_Is  = T_Is  / T_tot\n        f_Ir  = T_Ir  / T_tot\n        f_B   = T_B   / T_tot\n        f_k   = T_k   / T_tot\n        f_cov = T_cov / T_tot\n\n    return u_Q, f_Is, f_Ir, f_B, f_k, f_cov\n\ndef fmt6(x: float) -> str:\n    s = f\"{x:.6f}\"\n    # Normalize negative zero to zero\n    if s == \"-0.000000\":\n        s = \"0.000000\"\n    return s\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (mu_s, sigma_s, mu_r, sigma_r, mu_b, sigma_b, k0, sigma_k, rho)\n    test_cases = [\n        (10000.0, 300.0, 8000.0, 250.0, 500.0, 50.0, 1.0, 0.02, 0.6),   # Case A\n        (700.0,   30.0,  600.0,  30.0,  500.0, 20.0, 1.0, 0.05, 0.6),   # Case B\n        (9500.0,  400.0, 9000.0, 400.0, 0.0,   0.0,  0.8, 0.01, 0.0),   # Case C\n        (12000.0, 600.0, 11000.0,600.0, 400.0, 80.0, 1.2, 0.03, 0.95),  # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_s, sigma_s, mu_r, sigma_r, mu_b, sigma_b, k0, sigma_k, rho = case\n        u_Q, f_Is, f_Ir, f_B, f_k, f_cov = compute_uncertainty_and_budget(\n            mu_s, sigma_s, mu_r, sigma_r, mu_b, sigma_b, k0, sigma_k, rho\n        )\n        # Round and format to six decimals as strings\n        formatted = [\n            fmt6(u_Q),\n            fmt6(f_Is),\n            fmt6(f_Ir),\n            fmt6(f_B),\n            fmt6(f_k),\n            fmt6(f_cov),\n        ]\n        results.append(f\"[{','.join(formatted)}]\")\n\n    # Final print statement in the exact required format: single line with list of sublists.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2749348"}, {"introduction": "While foundational methods like error propagation handle statistical fluctuations, they are notoriously sensitive to gross errors or outliers that are common in high-throughput experiments. This exercise introduces the principles of robust statistics, which provide reliable estimates even when data is contaminated with aberrant measurements [@problem_id:2749352]. You will implement a powerful median-of-means estimator to handle outliers in promoter activity data, learning a practical workflow for generating trustworthy conclusions from messy, real-world datasets.", "problem": "A synthetic genetic construct drives a fluorescent reporter in a microbial chassis. Each replicate measurement consists of a fluorescence reading and a concurrent optical density at 600 nanometers (OD600), which together approximate the promoter activity as fluorescence per unit biomass. In practice, experimental variability includes multiplicative noise (for example, gain fluctuations) and rare gross errors (for example, pipetting errors, clogs, or bubbles), yielding a small fraction of outliers. To robustly estimate the promoter activity while handling outliers, consider the log-ratio model in which each replicate $\\left(i = 1,\\dots,n\\right)$ produces a pair $\\left(F_i, D_i\\right)$ of positive reals: a fluorescence reading $F_i$ in Molecules of Equivalent Fluorescein (MEFL) and an optical density $D_i$ (dimensionless). Define the log-ratio $y_i = \\log F_i - \\log D_i$. Assume an additive contamination model $y_i = \\mu + \\varepsilon_i$, where $\\mu$ is the location parameter of interest (the log promoter activity) and the noise $\\varepsilon_i$ is mostly drawn from a light-tailed distribution but may include a small, unknown fraction of arbitrarily large outliers.\n\nTask: From first principles of robust statistics, implement a robust location estimator for $\\mu$ using the median-of-means. Proceed as follows.\n\n- Partition the $n$ log-ratios $y_i$ into $B$ disjoint blocks that preserve the original ordering and are as equal in size as possible. For example, if $n = q B + r$ with $0 \\le r  B$, then form $r$ blocks of size $q+1$ followed by $B-r$ blocks of size $q$.\n- Compute each block mean and take the median across the $B$ block means. This median-of-means is the estimator $\\widehat{\\mu}$.\n- Estimate the scale using the Median Absolute Deviation (MAD) from $\\widehat{\\mu}$: $\\widehat{\\sigma} = c_{\\mathrm{MAD}} \\cdot \\operatorname{median}_i \\left| y_i - \\widehat{\\mu} \\right|$, with $c_{\\mathrm{MAD}} = 1.4826$ to be consistent with the standard deviation of a Gaussian distribution. For numerical stability, if $\\widehat{\\sigma} = 0$, replace it by a small positive value $\\sigma_{\\min} = 10^{-6}$.\n- Flag outliers by thresholding robust residuals: a replicate $i$ is an outlier if $\\left| y_i - \\widehat{\\mu} \\right|  c \\cdot \\widehat{\\sigma}$ with $c = 3.0$.\n- Report the final promoter activity on the original scale as $\\widehat{A} = \\exp\\left(\\widehat{\\mu}\\right)$ in molecules of equivalent fluorescein per unit optical density (MEFL per OD). Also report the fraction of outliers as a decimal $f = \\left(\\text{number of flagged replicates}\\right)/n$.\n\nYou must implement the algorithm exactly as described above. All computations must use the natural logarithm and the exponential. The final answers for each test case must be floats rounded to six decimal places. Express $\\widehat{A}$ implicitly in MEFL per OD (do not append units in the output) and express $f$ as a decimal (not a percentage).\n\nTest suite. For each case, you are given arrays of fluorescence $F$ (in MEFL), arrays of optical density $D$ (dimensionless), and the number of blocks $B$.\n\n- Case $1$ (clean replicates, moderate variability), $n = 12$, $B = 3$:\n  - $F_1 = [4900, 5100, 5050, 4950, 5150, 4850, 5000, 5250, 4800, 5200, 4750, 5050]$\n  - $D_1 = [0.98, 1.02, 1.01, 0.99, 1.03, 0.97, 1.00, 1.05, 0.96, 1.04, 0.95, 1.00]$\n- Case $2$ (replicates with gross outliers), $n = 20$, $B = 4$:\n  - $F_2 = [2950, 3050, 3000, 3100, 2900, 3150, 2850, 3200, 3180, 2820, 3050, 20000, 8000, 2000, 100, 2900, 2800, 3300, 3100, 2950]$\n  - $D_2 = [0.98, 1.01, 1.00, 1.02, 0.99, 1.03, 0.97, 1.05, 1.04, 0.96, 1.00, 1.01, 0.20, 1.60, 1.00, 1.02, 0.95, 1.08, 1.10, 0.99]$\n- Case $3$ (heavy-tailed variability), $n = 25$, $B = 5$:\n  - $F_3 = [12000, 11000, 13000, 12500, 11500, 13500, 11200, 12800, 13200, 11300, 12700, 11800, 13100, 10900, 12900, 12100, 11600, 13300, 11900, 12200, 16000, 8000, 20000, 7000, 12600]$\n  - $D_3 = [1.00, 0.92, 1.08, 1.05, 0.97, 1.10, 0.94, 1.02, 1.06, 0.95, 1.03, 0.99, 1.07, 0.93, 1.04, 1.01, 0.96, 1.09, 0.98, 1.00, 1.12, 0.90, 1.15, 0.88, 1.05]$\n- Case $4$ (small $n$, two extreme outliers), $n = 7$, $B = 2$:\n  - $F_4 = [4000, 4100, 3900, 4050, 3950, 10000, 100]$\n  - $D_4 = [1.00, 1.02, 0.98, 1.01, 1.00, 0.50, 1.20]$\n\nRequired program output. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order:\n$[\\widehat{A}_1, f_1, \\widehat{A}_2, f_2, \\widehat{A}_3, f_3, \\widehat{A}_4, f_4]$,\nwith each float rounded to six decimal places. No other text should be printed.", "solution": "The problem requires the implementation of a robust statistical procedure to estimate promoter activity from experimental data subject to noise and gross errors. Before proceeding to a solution, a rigorous validation of the problem statement is mandatory.\n\nThe problem is scientifically grounded. The use of a fluorescent reporter and optical density to measure promoter activity is a standard technique in synthetic biology. The log-ratio transformation, $y_i = \\log F_i - \\log D_i$, is a sound statistical method for converting a model with multiplicative errors, common in biological measurements, into an additive error model, $y_i = \\mu + \\varepsilon_i$. This transformation linearizes the measurement scale and stabilizes variance. The core challenge is the presence of outliers, for which robust statistical estimators are the correct theoretical tool. The proposed median-of-means estimator is a well-established method known for its high breakdown point, meaning it can tolerate a substantial fraction of outliers without failing. The use of the Median Absolute Deviation (MAD) for scale estimation is also a standard robust technique, complementary to the median-of-means location estimator.\n\nThe problem is well-posed. It provides an explicit, step-by-step algorithm, including all necessary parameters and constants ($c_{\\mathrm{MAD}} = 1.4826$, $\\sigma_{\\min} = 10^{-6}$, $c = 3.0$). The data for all test cases are provided, and the required output format is precisely specified. The algorithm is deterministic, ensuring a unique solution. The partitioning rule for creating blocks is unambiguous. The problem statement is objective, free of speculation, and contains no scientific or logical contradictions.\n\nTherefore, the problem is deemed valid. A solution will be constructed based on the specified first principles.\n\nThe goal is to find a robust estimate of the location parameter $\\mu$ from the log-ratios $y_i$. Standard estimators like the sample mean are not robust because a single outlier can arbitrarily corrupt the estimate. The median-of-means estimator provides this required robustness.\n\nLet us proceed with the specified algorithm.\n\n**Step 1: Data Transformation**\nFor each replicate $i$ with a measured fluorescence $F_i$ and optical density $D_i$, we compute the log-ratio. The ratio $A_i = F_i / D_i$ represents the promoter activity for that replicate. The noise is assumed to be multiplicative. By taking the natural logarithm, we transform the problem into an additive-noise domain:\n$$\ny_i = \\log(A_i) = \\log(F_i / D_i) = \\log F_i - \\log D_i\n$$\nThe statistical model is $y_i = \\mu + \\varepsilon_i$, where $\\mu = \\mathbb{E}[y_i]$ is the true log-promoter activity, and $\\varepsilon_i$ is the noise term containing both typical measurement fluctuations and rare, large outliers.\n\n**Step 2: Partitioning**\nThe set of $n$ log-ratios $\\{y_1, y_2, \\dots, y_n\\}$ is partitioned into $B$ disjoint blocks while maintaining the original sequence order. Let $n = qB + r$, where $q$ is the quotient and $r$ is the remainder ($0 \\le r  B$). The partitioning rule dictates that we form $r$ blocks of size $q+1$ followed by $B-r$ blocks of size $q$. This ensures the blocks are as close to equal size as possible.\n\n**Step 3: Median-of-Means Estimation of Location ($\\widehat{\\mu}$)**\nFor each block $b \\in \\{1, \\dots, B\\}$, we compute the arithmetic mean of its constituent $y_i$ values. Let this be $M_b$. The collection of block means is $\\{M_1, M_2, \\dots, M_B\\}$. The median-of-means estimator for $\\mu$ is then the sample median of these block means:\n$$\n\\widehat{\\mu} = \\operatorname{median}\\{M_1, M_2, \\dots, M_B\\}\n$$\nThis estimator is robust. If an outlier corrupts one block, its mean $M_b$ may be arbitrarily large or small. However, as long as fewer than $B/2$ blocks are contaminated by outliers, the median of the block means will not be affected, providing a reliable estimate of $\\mu$.\n\n**Step 4: Robust Estimation of Scale ($\\widehat{\\sigma}$)**\nTo identify outliers, we need a robust measure of the data's scale or dispersion. The standard deviation is not robust. We use the Median Absolute Deviation (MAD) from the estimated location $\\widehat{\\mu}$. First, we compute the absolute residuals $|y_i - \\widehat{\\mu}|$ for all $i=1, \\dots, n$. The MAD is the median of these values.\nThe scale estimator $\\widehat{\\sigma}$ is a scaled version of the MAD:\n$$\n\\widehat{\\sigma} = c_{\\mathrm{MAD}} \\cdot \\operatorname{median}_i \\{|y_i - \\widehat{\\mu}|\\}\n$$\nThe constant $c_{\\mathrm{MAD}} = 1.4826$ is chosen to ensure that for data drawn from a Gaussian distribution, $\\widehat{\\sigma}$ is a consistent estimator of the standard deviation. It is derived from the standard normal cumulative distribution function $\\Phi$ as $1/\\Phi^{-1}(0.75)$.\nFor numerical stability, if $\\widehat{\\sigma}$ computes to $0$ (which occurs if more than $50\\%$ of the $y_i$ values are equal to $\\widehat{\\mu}$), it is set to a small positive floor value, $\\sigma_{\\min} = 10^{-6}$.\n\n**Step 5: Outlier Identification**\nWith robust estimates for location ($\\widehat{\\mu}$) and scale ($\\widehat{\\sigma}$), we can identify outliers using a robust version of the \"Z-score\". A replicate $i$ is flagged as an outlier if its log-ratio $y_i$ is too far from the estimated center $\\widehat{\\mu}$, as measured in units of the robust scale $\\widehat{\\sigma}$. The criterion is:\n$$\n|y_i - \\widehat{\\mu}|  c \\cdot \\widehat{\\sigma}\n$$\nwhere the problem specifies a threshold multiplier of $c = 3.0$. This corresponds to a \"3-sigma\" rule, adapted for robust estimators.\nThe fraction of outliers, $f$, is the number of flagged replicates divided by the total number of replicates, $n$.\n\n**Step 6: Final Reporting**\nThe final estimate for the promoter activity, $\\widehat{A}$, is obtained by transforming the log-scale estimate $\\widehat{\\mu}$ back to the original linear scale:\n$$\n\\widehat{A} = \\exp(\\widehat{\\mu})\n$$\nThis value, $\\widehat{A}$, represents a robust estimate of the geometric mean of the promoter activity across replicates. The results to be reported for each test case are the pair $(\\widehat{A}, f)$, with numerical values rounded to six decimal places.\n\nThe implementation will now follow these exact steps for each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the median-of-means robust estimator for promoter activity\n    and applies it to a suite of test cases.\n    \"\"\"\n\n    C_MAD = 1.4826\n    SIGMA_MIN = 1e-6\n    OUTLIER_THRESHOLD_C = 3.0\n\n    test_cases = [\n        {\n            \"F\": np.array([4900, 5100, 5050, 4950, 5150, 4850, 5000, 5250, 4800, 5200, 4750, 5050]),\n            \"D\": np.array([0.98, 1.02, 1.01, 0.99, 1.03, 0.97, 1.00, 1.05, 0.96, 1.04, 0.95, 1.00]),\n            \"B\": 3\n        },\n        {\n            \"F\": np.array([2950, 3050, 3000, 3100, 2900, 3150, 2850, 3200, 3180, 2820, 3050, 20000, 8000, 2000, 100, 2900, 2800, 3300, 3100, 2950]),\n            \"D\": np.array([0.98, 1.01, 1.00, 1.02, 0.99, 1.03, 0.97, 1.05, 1.04, 0.96, 1.00, 1.01, 0.20, 1.60, 1.00, 1.02, 0.95, 1.08, 1.10, 0.99]),\n            \"B\": 4\n        },\n        {\n            \"F\": np.array([12000, 11000, 13000, 12500, 11500, 13500, 11200, 12800, 13200, 11300, 12700, 11800, 13100, 10900, 12900, 12100, 11600, 13300, 11900, 12200, 16000, 8000, 20000, 7000, 12600]),\n            \"D\": np.array([1.00, 0.92, 1.08, 1.05, 0.97, 1.10, 0.94, 1.02, 1.06, 0.95, 1.03, 0.99, 1.07, 0.93, 1.04, 1.01, 0.96, 1.09, 0.98, 1.00, 1.12, 0.90, 1.15, 0.88, 1.05]),\n            \"B\": 5\n        },\n        {\n            \"F\": np.array([4000, 4100, 3900, 4050, 3950, 10000, 100]),\n            \"D\": np.array([1.00, 1.02, 0.98, 1.01, 1.00, 0.50, 1.20]),\n            \"B\": 2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        F_arr, D_arr, B = case[\"F\"], case[\"D\"], case[\"B\"]\n        n = len(F_arr)\n\n        # Step 1: Compute log-ratios\n        y = np.log(F_arr) - np.log(D_arr)\n\n        # Step 2: Partition data into B blocks\n        q = n // B\n        r = n % B\n        \n        block_sizes = [q + 1] * r + [q] * (B - r)\n        blocks = []\n        current_pos = 0\n        for size in block_sizes:\n            blocks.append(y[current_pos : current_pos + size])\n            current_pos += size\n\n        # Step 3: Compute median-of-means for mu_hat\n        block_means = np.array([np.mean(block) for block in blocks])\n        mu_hat = np.median(block_means)\n\n        # Step 4: Estimate scale using MAD\n        abs_residuals = np.abs(y - mu_hat)\n        mad = np.median(abs_residuals)\n        sigma_hat = C_MAD * mad\n        if sigma_hat == 0.0:\n            sigma_hat = SIGMA_MIN\n\n        # Step 5: Flag outliers\n        is_outlier = np.abs(y - mu_hat) > OUTLIER_THRESHOLD_C * sigma_hat\n        num_outliers = np.sum(is_outlier)\n        f_outliers = num_outliers / n\n\n        # Step 6: Report final promoter activity\n        A_hat = np.exp(mu_hat)\n\n        results.append(f\"{A_hat:.6f}\")\n        results.append(f\"{f_outliers:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2749352"}, {"introduction": "In single-cell biology, measurement variability is not just noise; it is a rich source of information about underlying biological processes. This practice moves beyond merely quantifying noise to actively modeling its source in single-cell mRNA count data [@problem_id:2749367]. You will implement maximum likelihood estimation and use the Akaike Information Criterion to distinguish between different biological scenariosâ€”such as purely intrinsic noise (Poisson), cell-to-cell heterogeneity (Negative Binomial), or gene inactivation (Zero-Inflated)â€”demonstrating how statistical distributions can reveal cellular mechanisms.", "problem": "You are analyzing single-cell messenger ribonucleic acid (mRNA) count measurements from a synthetic gene circuit. Each dataset consists of independent and identically distributed nonnegative integer counts from cells measured under a fixed condition. Measurement noise and biological variability can be modeled using established discrete distributions: a Poisson model for intrinsic birthâ€“death noise, a Negative Binomial (NB) model for overdispersion arising from cell-to-cell variability, and a Zero-Inflated Poisson (ZIP) model for structural zeros arising from gene-inactive subpopulations or measurement dropout. Starting only from core probabilistic definitions (probability mass functions, independence of replicates, and mixture modeling), and well-tested facts about model selection using likelihood-based criteria, you must construct maximum likelihood estimators, compare models, and report a scalar summary of noise.\n\nFundamental base to use:\n- Treat the count for each cell as an independent realization of a nonnegative integer-valued random variable.\n- The Poisson model assumes a single rate parameter $\\lambda$.\n- The Negative Binomial (NB) model assumes a meanâ€“overdispersion representation with mean $\\mu$ and a positive dispersion (size) parameter $r$ such that the variance exceeds the mean.\n- The Zero-Inflated Poisson (ZIP) model is a two-component mixture with a structural-zero probability $\\pi$ and a Poisson component with rate $\\lambda$, producing zeros either structurally or from the Poisson component.\n- Use Maximum Likelihood Estimation (MLE) to estimate parameters for each model from the observed counts.\n- Use Akaike Information Criterion (AIC), defined in terms of the maximized log-likelihood and the number of free parameters $k$, to select the model with the smallest AIC for each dataset.\n- After selecting a model, summarize noise by the coefficient of variation squared $\\mathrm{CV}^2$, defined as $\\mathrm{Var}(X)/[\\mathbb{E}(X)]^2$, computed under the selected model at its MLE.\n\nYour program must implement the following tasks for each dataset:\n- Construct the likelihood under each of the three models (Poisson, NB, ZIP) directly from the model definitions and the independence assumption for replicates. Do not assume any result that is not derivable from these definitions.\n- Compute the MLE for each model, either in closed form where available or by numerical maximization subject to parameter constraints implied by the model definitions.\n- Compute the maximized log-likelihood and the Akaike Information Criterion (AIC) using the number of free parameters $k$, where $k$ is $1$ for Poisson, $2$ for Negative Binomial, and $2$ for Zero-Inflated Poisson.\n- Select the model with the smallest AIC.\n- Using the fitted parameters of the selected model, compute the model-implied coefficient of variation squared $\\mathrm{CV}^2$.\n\nConstraints and implementation requirements:\n- Treat all datasets as unitless counts. No physical units are involved.\n- Angles are not used.\n- All final numeric outputs must be rounded to $6$ decimal places.\n- For each dataset, produce a pair $[m, c]$ where $m$ is an integer code for the selected model and $c$ is the rounded $\\mathrm{CV}^2$:\n  - $m = 0$ for Poisson,\n  - $m = 1$ for Negative Binomial,\n  - $m = 2$ for Zero-Inflated Poisson.\n- Your program should produce a single line of output containing a comma-separated list of these per-dataset results, enclosed in square brackets, for example: $[[m_1, c_1], [m_2, c_2], \\ldots]$.\n\nTest suite (datasets):\n- Dataset $1$ (typical intrinsic noise): $[8,12,9,11,10,7,13,10,9,11,8,12,10,9,11]$.\n- Dataset $2$ (overdispersion without strong zero inflation): $[0,3,2,25,18,7,0,15,12,30,4,6,21,9,11,17,5]$.\n- Dataset $3$ (pronounced zero inflation): $[0,0,0,0,0,0,0,0,0,0,0,0,6,4,7,3,5,8,2,6]$.\n- Dataset $4$ (boundary case with rare expression): $[0,0,0,0,0,1,0,0,0,1]$.\n\nFinal output format:\n- Your program must print a single line containing a list of $4$ lists, one per dataset, in the exact order above. Each inner list must be $[m,c]$ with $m$ as defined above and $c$ equal to the selected modelâ€™s $\\mathrm{CV}^2$ rounded to $6$ decimals. For example, a valid output would look like $[[0,0.123456],[1,3.141593],[2,0.271828],[2,42.000000]]$.", "solution": "The problem requires a rigorous statistical analysis of single-cell mRNA count data. The objective is to identify the most appropriate statistical model for each of four datasets from a candidate set of threeâ€”Poisson, Negative Binomial (NB), and Zero-Inflated Poisson (ZIP)â€”and to quantify the noise of the underlying process using the selected model. This task is a standard application of maximum likelihood estimation (MLE) and information-theoretic model selection. The entire procedure will be derived from fundamental principles of probability theory and statistics.\n\nLet a dataset of counts be denoted by $D = \\{x_1, x_2, \\ldots, x_n\\}$, representing $n$ independent and identically distributed (i.i.d.) observations of a random variable $X$.\n\n**1. Model Definitions and Likelihood Functions**\n\nThe analysis is predicated on the construction of the likelihood function $L(\\theta | D)$, which is the joint probability of observing the data $D$ given the model parameters $\\theta$. Due to the i.i.d. assumption, the likelihood is the product of the individual probability mass functions (PMFs): $L(\\theta | D) = \\prod_{i=1}^{n} P(X=x_i | \\theta)$. For computational stability and mathematical convenience, we work with the log-likelihood, $\\ln L(\\theta | D) = \\sum_{i=1}^{n} \\ln P(X=x_i | \\theta)$.\n\n*   **Poisson Model**: This model is defined by a single rate parameter $\\lambda > 0$, representing both the mean and variance of the distribution. The PMF is:\n    $$ P(X=k | \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} $$\n    The log-likelihood function for the dataset $D$ under the Poisson model is:\n    $$ \\ln L(\\lambda | D) = \\sum_{i=1}^{n} (x_i \\ln \\lambda - \\lambda - \\ln(x_i!)) = (\\ln \\lambda) \\sum_{i=1}^{n} x_i - n\\lambda - \\sum_{i=1}^{n} \\ln(x_i!) $$\n\n*   **Negative Binomial (NB) Model**: The NB model accommodates overdispersion (variance greater than the mean). It is parameterized here by its mean $\\mu > 0$ and a positive dispersion parameter $r > 0$. The variance is given by $\\mathrm{Var}(X) = \\mu + \\mu^2/r$. The PMF is derived from the standard $(r, p)$ parameterization where $p = r/(r+\\mu)$:\n    $$ P(X=k | \\mu, r) = \\frac{\\Gamma(k+r)}{k!\\Gamma(r)} \\left(\\frac{r}{r+\\mu}\\right)^r \\left(\\frac{\\mu}{r+\\mu}\\right)^k $$\n    where $\\Gamma(\\cdot)$ is the gamma function. The log-likelihood is:\n    $$ \\ln L(\\mu, r | D) = \\sum_{i=1}^{n} \\left[ \\ln\\Gamma(x_i+r) - \\ln\\Gamma(r) - \\ln(x_i!) + r\\ln\\left(\\frac{r}{r+\\mu}\\right) + x_i\\ln\\left(\\frac{\\mu}{r+\\mu}\\right) \\right] $$\n\n*   **Zero-Inflated Poisson (ZIP) Model**: This is a two-component mixture model used for datasets with an excess of zero counts. It is parameterized by a Poisson rate $\\lambda > 0$ and a zero-inflation probability $0 \\le \\pi  1$. A zero is observed with probability $\\pi$ from a \"structural zero\" process, or with probability $(1-\\pi)e^{-\\lambda}$ from the Poisson process.\n    The PMF is:\n    $$\n    P(X=k | \\pi, \\lambda) = \n    \\begin{cases} \n    \\pi + (1-\\pi)e^{-\\lambda}  \\text{if } k=0 \\\\\n    (1-\\pi)\\frac{\\lambda^k e^{-\\lambda}}{k!}  \\text{if } k > 0\n    \\end{cases}\n    $$\n    Let $n_0$ be the number of zero counts and $D_{>0}$ be the non-zero counts in the dataset. The log-likelihood is:\n    $$ \\ln L(\\pi, \\lambda | D) = n_0 \\ln(\\pi + (1-\\pi)e^{-\\lambda}) + \\sum_{x_i \\in D_{>0}} \\ln\\left((1-\\pi)\\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\\right) $$\n\n**2. Maximum Likelihood Estimation (MLE)**\n\nThe principle of MLE directs us to find the parameter values $\\hat{\\theta}_{MLE}$ that maximize the log-likelihood function $\\ln L(\\theta | D)$.\n\n*   For the **Poisson** model, the MLE for $\\lambda$ is found by setting the derivative of the log-likelihood to zero, which yields a closed-form solution:\n    $$ \\frac{\\partial \\ln L}{\\partial \\lambda} = \\frac{1}{\\lambda}\\sum_{i=1}^{n}x_i - n = 0 \\implies \\hat{\\lambda}_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\bar{x} $$\n    The MLE is the sample mean.\n\n*   For the **NB** and **ZIP** models, no general closed-form solutions for the MLEs exist. Therefore, we must resort to numerical methods to maximize the log-likelihood function subject to parameter constraints ($\\mu, r, \\lambda > 0$ and $0 \\le \\pi  1$). We will minimize the negative log-likelihood using a quasi-Newton algorithm, specifically L-BFGS-B, which efficiently handles box constraints.\n\n**3. Model Selection**\n\nTo compare models with different numbers of parameters, we use the Akaike Information Criterion (AIC). AIC provides a relative measure of model quality, balancing the goodness of fit (maximized log-likelihood) against model complexity (number of free parameters, $k$). The model with the lowest AIC is preferred.\n$$ \\mathrm{AIC} = 2k - 2\\ln L(\\hat{\\theta}_{MLE} | D) $$\nThe number of free parameters for each model is specified as:\n*   Poisson: $k=1$ (parameter $\\lambda$)\n*   Negative Binomial: $k=2$ (parameters $\\mu, r$)\n*   Zero-Inflated Poisson: $k=2$ (parameters $\\pi, \\lambda$)\n\n**4. Noise Quantification: Coefficient of Variation Squared ($\\mathrm{CV}^2$)**\n\nAfter selecting the best-fitting model, the biological noise is summarized by the squared coefficient of variation, defined as $\\mathrm{CV}^2 = \\mathrm{Var}(X) / [\\mathbb{E}(X)]^2$. The expectation $\\mathbb{E}(X)$ and variance $\\mathrm{Var}(X)$ are computed using the MLE parameters of the chosen model.\n\n*   **Poisson**: $\\mathbb{E}(X) = \\lambda$, $\\mathrm{Var}(X) = \\lambda$.\n    $$ \\mathrm{CV}^2 = \\frac{\\lambda}{\\lambda^2} = \\frac{1}{\\lambda} $$\n*   **Negative Binomial**: $\\mathbb{E}(X) = \\mu$, $\\mathrm{Var}(X) = \\mu + \\mu^2/r$.\n    $$ \\mathrm{CV}^2 = \\frac{\\mu + \\mu^2/r}{\\mu^2} = \\frac{1}{\\mu} + \\frac{1}{r} $$\n*   **Zero-Inflated Poisson**: $\\mathbb{E}(X) = (1-\\pi)\\lambda$, $\\mathrm{Var}(X) = (1-\\pi)\\lambda(1+\\pi\\lambda)$.\n    $$ \\mathrm{CV}^2 = \\frac{(1-\\pi)\\lambda(1+\\pi\\lambda)}{((1-\\pi)\\lambda)^2} = \\frac{1+\\pi\\lambda}{(1-\\pi)\\lambda} $$\n\n**5. Computational Implementation**\n\nThe described procedure is implemented in a Python program. For each dataset, functions are defined to compute the negative log-likelihood for each of the three models. For the NB and ZIP models, `scipy.optimize.minimize` is used to find the parameter values that minimize this function. After obtaining the MLE parameters for all models, their corresponding AIC values are calculated and compared to select the optimal model. Finally, the $\\mathrm{CV}^2$ is computed using the parameters of the winning model. The final results are formatted as specified.", "answer": "```python\nimport numpy as np\nfrom scipy import optimize\nfrom scipy import special\n\ndef solve():\n    \"\"\"\n    Main solver function to process all datasets and print the final results.\n    \"\"\"\n    test_cases = [\n        [8, 12, 9, 11, 10, 7, 13, 10, 9, 11, 8, 12, 10, 9, 11],\n        [0, 3, 2, 25, 18, 7, 0, 15, 12, 30, 4, 6, 21, 9, 11, 17, 5],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 4, 7, 3, 5, 8, 2, 6],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n    ]\n\n    results = []\n    for data_list in test_cases:\n        data = np.array(data_list)\n        result = analyze_dataset(data)\n        results.append(result)\n\n    # Format output as a string representation of a list of lists.\n    # Ex: [[0,0.1],[1,0.56789]]\n    output_str = \"[\" + \",\".join([f\"[{m},{c:.6f}]\" for m, c in results]) + \"]\"\n    print(output_str)\n\ndef analyze_dataset(data):\n    \"\"\"\n    Analyzes a single dataset: fits all three models, selects the best one via AIC,\n    and computes its CV^2.\n    \"\"\"\n    # Small epsilon to avoid log(0) issues in optimization bounds.\n    epsilon = 1e-9\n\n    # --- Poisson Model ---\n    # Closed-form MLE\n    lambda_mle = np.mean(data)\n    if lambda_mle == 0: # All-zero data case\n        logL_pois = 0\n        cv2_pois = np.inf # Or undefined, conventionally large\n    else:\n        logL_pois = np.sum(\n            data * np.log(lambda_mle) - lambda_mle - special.gammaln(data + 1)\n        )\n        cv2_pois = 1.0 / lambda_mle\n    k_pois = 1\n    aic_pois = 2 * k_pois - 2 * logL_pois\n    \n    # --- Negative Binomial Model ---\n    def neg_logL_nb(params, data):\n        mu, r = params\n        if r = 0 or mu = 0: return np.inf\n        \n        term1 = special.gammaln(data + r) - special.gammaln(data + 1) - special.gammaln(r)\n        term2 = r * np.log(r) - r * np.log(r + mu)\n        term3 = data * np.log(mu) - data * np.log(r + mu)\n        \n        logL = np.sum(term1 + term2 + term3)\n        return -logL\n\n    # Initial guess using method of moments\n    mean_data = np.mean(data)\n    var_data = np.var(data, ddof=0) # MLE for variance uses n, not n-1\n    mu_init = mean_data if mean_data > 0 else epsilon\n    if var_data > mean_data and mean_data > 0:\n        r_init = mean_data**2 / (var_data - mean_data)\n    else:\n        r_init = 1000.0 # Large r for near-Poisson data\n    \n    res_nb = optimize.minimize(\n        neg_logL_nb,\n        x0=[mu_init, r_init],\n        args=(data,),\n        method='L-BFGS-B',\n        bounds=[(epsilon, None), (epsilon, None)]\n    )\n    \n    if res_nb.success:\n        mu_mle_nb, r_mle_nb = res_nb.x\n        logL_nb = -res_nb.fun\n        k_nb = 2\n        aic_nb = 2 * k_nb - 2 * logL_nb\n        cv2_nb = (1.0 / mu_mle_nb) + (1.0 / r_mle_nb)\n    else:\n        aic_nb = np.inf\n        cv2_nb = np.inf\n\n\n    # --- Zero-Inflated Poisson Model ---\n    def neg_logL_zip(params, data, n0, non_zeros):\n        pi, lam = params\n        if not (0 = pi  1 and lam > 0): return np.inf\n        \n        logL_zeros = n0 * np.log(pi + (1 - pi) * np.exp(-lam))\n        \n        n_pos = len(non_zeros)\n        if n_pos > 0:\n            logL_non_zeros = n_pos * np.log(1 - pi) + \\\n                             np.sum(non_zeros) * np.log(lam) - \\\n                             n_pos * lam - \\\n                             np.sum(special.gammaln(non_zeros + 1))\n        else: # Case of all zeros\n            logL_non_zeros = 0\n            \n        logL = logL_zeros + logL_non_zeros\n        if np.isneginf(logL):\n            return np.inf\n        return -logL\n\n    n0 = np.sum(data == 0)\n    non_zeros = data[data > 0]\n\n    pi_init = n0 / len(data)\n    if len(non_zeros) > 0:\n        lam_init = np.mean(non_zeros)\n    else:\n        lam_init = epsilon\n\n    if pi_init >= 1.0:\n        pi_init = 1.0 - epsilon\n\n    res_zip = optimize.minimize(\n        neg_logL_zip,\n        x0=[pi_init, lam_init],\n        args=(data, n0, non_zeros),\n        method='L-BFGS-B',\n        bounds=[(0, 1.0 - epsilon), (epsilon, None)]\n    )\n\n    if res_zip.success:\n        pi_mle_zip, lam_mle_zip = res_zip.x\n        logL_zip = -res_zip.fun\n        k_zip = 2\n        aic_zip = 2 * k_zip - 2 * logL_zip\n        \n        mean_zip = (1 - pi_mle_zip) * lam_mle_zip\n        if mean_zip > 0 :\n            cv2_zip = ((1 - pi_mle_zip) * lam_mle_zip * (1 + pi_mle_zip * lam_mle_zip)) / mean_zip**2\n        else:\n            cv2_zip = np.inf\n    else:\n        aic_zip = np.inf\n        cv2_zip = np.inf\n\n\n    # Model selection\n    models = [\n        (0, aic_pois, cv2_pois),\n        (1, aic_nb, cv2_nb),\n        (2, aic_zip, cv2_zip)\n    ]\n    \n    # Select model with minimum AIC\n    best_model = min(models, key=lambda x: x[1])\n    \n    return best_model[0], best_model[2]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2749367"}]}