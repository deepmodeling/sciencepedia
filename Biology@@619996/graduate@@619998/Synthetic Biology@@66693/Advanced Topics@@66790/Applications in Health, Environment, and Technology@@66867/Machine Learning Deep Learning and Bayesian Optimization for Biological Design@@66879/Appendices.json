{"hands_on_practices": [{"introduction": "Before training any machine learning model, raw feature data must often be preprocessed to ensure robust and efficient learning. This exercise focuses on Z-scoring, a fundamental standardization technique, and its mathematical consequences for model fitting [@problem_id:2749099]. By working through this problem, you will gain a first-principles understanding of how standardization alters the geometric relationships between features and why it is crucial for improving the numerical stability and performance of many machine learning algorithms.", "problem": "In a Bayesian design-of-experiments loop for optimizing ribosome binding site designs, you maintain a design matrix with two engineered feature channels per construct: a promoter activity score and a codon-usage motif density score, both dimensionless outputs from validated computational pipelines. You wish to understand, from first principles, how column-wise feature standardization by Z-scoring changes the inner-product geometry of the feature space and the conditioning of the linear-model normal equations that are used inside Gaussian-process surrogate fitting and gradient-based acquisition function approximations.\n\nLet the data matrix be denoted by $X \\in \\mathbb{R}^{n \\times d}$ with $n = 4$ constructs and $d = 2$ features. The rows are constructs, the columns are feature channels. You are given\n$$\nX \\;=\\;\n\\begin{pmatrix}\n2 & 1 \\\\\n4 & 5 \\\\\n6 & 7 \\\\\n8 & 13\n\\end{pmatrix}.\n$$\n\nStarting only from the Euclidean inner product on $\\mathbb{R}^{n}$, the definitions of column-wise mean, variance, and standard deviation under the population convention (denominator $n$), and the definition of the $2$-norm condition number of a symmetric positive definite matrix (largest eigenvalue divided by smallest eigenvalue), carry out the following steps:\n\n1) Derive the linear operators that implement column centering and Z-scoring for the columns of $X$, and express the standardized feature Gram matrix $G_{z}$ (the inner-product matrix of standardized feature columns) in terms of your derived operators and $X$. Use only the population-variance convention for each feature column.\n\n2) Explain how this transformation changes the geometry of the feature space in terms of column norms and pairwise inner products, explicitly connecting the inner products after Z-scoring to the Pearson correlation coefficients.\n\n3) For the specific $X$ above, compute symbolically:\n- the centered feature Gram matrix $G_{c}$,\n- its $2$-norm condition number $\\kappa_{c}$,\n- the standardized feature Gram matrix $G_{z}$,\n- and its $2$-norm condition number $\\kappa_{z}$.\n\n4) Define the improvement factor as the ratio by which Z-scoring improves the $2$-norm conditioning of the feature Gram matrix, namely $I := \\kappa_{c} / \\kappa_{z}$. Provide your final answer for $I$ as a single exact analytic expression (do not approximate; no units).", "solution": "This problem must be validated before a solution is attempted.\n\nFirst, I extract the givens of the problem statement.\n- The data matrix is $X \\in \\mathbb{R}^{n \\times d}$ with $n = 4$ and $d = 2$.\n- The specific values for $X$ are given as $X \\;=\\; \\begin{pmatrix} 2 & 1 \\\\ 4 & 5 \\\\ 6 & 7 \\\\ 8 & 13 \\end{pmatrix}$.\n- The inner product is the standard Euclidean inner product on $\\mathbb{R}^{n}$.\n- Column-wise statistics (mean, variance, standard deviation) are to be computed using the population convention, with a denominator of $n$.\n- The $2$-norm condition number of a symmetric positive definite matrix $A$ is defined as $\\kappa_{2}(A) = \\frac{\\lambda_{\\text{max}}(A)}{\\lambda_{\\text{min}}(A)}$.\n- The task is to perform four steps: 1) derive operators for centering and Z-scoring and use them to express the standardized Gram matrix $G_z$; 2) explain the geometric effect of this transformation; 3) compute the centered Gram matrix $G_c$, its condition number $\\kappa_c$, the standardized Gram matrix $G_z$, and its condition number $\\kappa_z$; 4) compute the improvement factor $I = \\kappa_c / \\kappa_z$.\n\nSecond, I validate the problem. The problem is a formal exercise in linear algebra and statistics, directly applicable to the pre-processing steps in machine learning. It is scientifically grounded, well-posed, and objective. It contains all necessary information and definitions, with no internal contradictions. The column vectors of $X$ are linearly independent, which ensures that the Gram matrices $G_c$ and $G_z$ will be positive definite and their condition numbers will be well-defined and greater than zero. The problem is not trivial, as it requires a sequence of rigorous calculations and conceptual understanding. The problem is therefore deemed valid.\n\nI shall now proceed with the solution in four parts as requested.\n\nPart 1: Derivation of Operators and Gram Matrix Expression\nLet the data matrix be $X = [\\mathbf{x}_1, \\dots, \\mathbf{x}_d]$, where each column vector $\\mathbf{x}_j \\in \\mathbb{R}^n$. Let $\\mathbf{1}_n$ be the $n \\times 1$ vector of ones. The vector of column means is $\\boldsymbol{\\mu} = (\\mu_1, \\dots, \\mu_d)^T$, where $\\mu_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij}$. In matrix form, this is $\\boldsymbol{\\mu}^T = \\frac{1}{n}\\mathbf{1}_n^T X$.\nColumn centering subtracts the corresponding mean from each element of a column. The centered matrix $X_c$ is given by:\n$$X_c = X - \\mathbf{1}_n \\boldsymbol{\\mu}^T = X - \\mathbf{1}_n (\\frac{1}{n}\\mathbf{1}_n^T X) = (I_n - \\frac{1}{n}\\mathbf{1}_n\\mathbf{1}_n^T)X$$\nwhere $I_n$ is the $n \\times n$ identity matrix. We define the centering operator as the matrix $P_c = I_n - \\frac{1}{n}\\mathbf{1}_n\\mathbf{1}_n^T$. $P_c$ is a symmetric and idempotent projection matrix. Thus, the centered matrix is $X_c = P_c X$.\n\nThe population variance of the $j$-th column is $\\sigma_j^2 = \\frac{1}{n}\\sum_{i=1}^n (X_{ij} - \\mu_j)^2$. This is the squared Euclidean norm of the $j$-th centered column, divided by $n$. That is, $\\sigma_j^2 = \\frac{1}{n} \\|\\mathbf{x}_{c,j}\\|_2^2$, where $\\mathbf{x}_{c,j}$ is the $j$-th column of $X_c$. The standard deviation is $\\sigma_j = \\sqrt{\\sigma_j^2}$.\n\nZ-scoring, or standardization, involves centering the data and then dividing each column by its standard deviation. Let $S$ be the $d \\times d$ diagonal matrix of standard deviations, $S = \\text{diag}(\\sigma_1, \\dots, \\sigma_d)$. The standardized matrix $X_z$ is obtained by post-multiplying $X_c$ by $S^{-1}$:\n$$X_z = X_c S^{-1} = (P_c X) S^{-1}$$\nThe standardized feature Gram matrix, $G_z$, is the inner product matrix of the columns of $X_z$. This is $G_z = X_z^T X_z$. Substituting the expression for $X_z$:\n$$G_z = ((P_c X) S^{-1})^T ((P_c X) S^{-1}) = (S^{-1})^T (P_c X)^T (P_c X) S^{-1}$$\nSince $S$ is diagonal, $S^{-1}$ is diagonal and thus symmetric, so $(S^{-1})^T = S^{-1}$. Since $P_c$ is symmetric, $(P_c X)^T = X^T P_c^T = X^T P_c$. The expression becomes:\n$$G_z = S^{-1} X^T P_c P_c X S^{-1}$$\nUsing the idempotency of $P_c$ ($P_c^2=P_c$), we have:\n$$G_z = S^{-1} (X^T P_c X) S^{-1}$$\nThe term $X^T P_c X = X^T P_c^T P_c X = (P_c X)^T (P_c X) = X_c^T X_c$ is precisely the centered Gram matrix, $G_c$. Therefore, the standardized Gram matrix is given by:\n$$G_z = S^{-1} G_c S^{-1}$$\n\nPart 2: Geometric Interpretation\nThe transformation from $X$ to $X_z$ involves two geometric operations on the column vectors $\\{\\mathbf{x}_j\\}_{j=1}^d$ in $\\mathbb{R}^n$.\nFirst, centering via the operator $P_c$ projects each feature vector $\\mathbf{x}_j$ onto the subspace orthogonal to the vector $\\mathbf{1}_n$. This is equivalent to shifting the origin of the feature space to the data's centroid $(\\mu_1, \\dots, \\mu_d)$. The resulting vectors $\\mathbf{x}_{c,j}$ have a mean of zero.\n\nSecond, scaling by $S^{-1}$ adjusts the length of each centered vector. The squared norm of a standardized column $\\mathbf{x}_{z,j}$ is:\n$$\\|\\mathbf{x}_{z,j}\\|_2^2 = \\left\\| \\frac{\\mathbf{x}_{c,j}}{\\sigma_j} \\right\\|_2^2 = \\frac{1}{\\sigma_j^2} \\|\\mathbf{x}_{c,j}\\|_2^2$$\nBy definition, $\\sigma_j^2 = \\frac{1}{n}\\|\\mathbf{x}_{c,j}\\|_2^2$, so $\\|\\mathbf{x}_{c,j}\\|_2^2 = n\\sigma_j^2$. Substituting this in, we find:\n$$\\|\\mathbf{x}_{z,j}\\|_2^2 = \\frac{1}{\\sigma_j^2} (n\\sigma_j^2) = n$$\nThis means that Z-scoring makes all feature vectors have the same Euclidean norm of $\\sqrt{n}$.\n\nThe entries of the standardized Gram matrix $G_z$ are the inner products of these standardized vectors. An off-diagonal element $(G_z)_{ij}$ is:\n$$(G_z)_{ij} = \\mathbf{x}_{z,i}^T \\mathbf{x}_{z,j} = \\left(\\frac{\\mathbf{x}_{c,i}}{\\sigma_i}\\right)^T \\left(\\frac{\\mathbf{x}_{c,j}}{\\sigma_j}\\right) = \\frac{\\mathbf{x}_{c,i}^T \\mathbf{x}_{c,j}}{\\sigma_i \\sigma_j}$$\nThe Pearson correlation coefficient $\\rho_{ij}$ between features $\\mathbf{x}_i$ and $\\mathbf{x}_j$ is defined as the cosine of the angle between their centered versions:\n$$\\rho_{ij} = \\frac{\\mathbf{x}_{c,i}^T \\mathbf{x}_{c,j}}{\\|\\mathbf{x}_{c,i}\\|_2 \\|\\mathbf{x}_{c,j}\\|_2} = \\frac{\\mathbf{x}_{c,i}^T \\mathbf{x}_{c,j}}{(\\sqrt{n}\\sigma_i)(\\sqrt{n}\\sigma_j)} = \\frac{\\mathbf{x}_{c,i}^T \\mathbf{x}_{c,j}}{n\\sigma_i \\sigma_j}$$\nBy comparing the two expressions, we see that $(G_z)_{ij} = n \\rho_{ij}$. The diagonal elements are $(G_z)_{ii} = n \\rho_{ii} = n(1) = n$, consistent with $\\|\\mathbf{x}_{z,i}\\|_2^2 = n$.\nIn summary, Z-scoring transforms the feature vectors to all have length $\\sqrt{n}$, and the Gram matrix of these vectors becomes $n$ times the Pearson correlation matrix of the original features.\n\nPart 3: Computation for the given matrix $X$\nThe data matrix is $X \\;=\\; \\begin{pmatrix} 2 & 1 \\\\ 4 & 5 \\\\ 6 & 7 \\\\ 8 & 13 \\end{pmatrix}$. The column vectors are $\\mathbf{x}_1 = (2, 4, 6, 8)^T$ and $\\mathbf{x}_2 = (1, 5, 7, 13)^T$. Here $n=4$.\n\nFirst, we compute the centered matrix $X_c$.\nThe means are $\\mu_1 = \\frac{2+4+6+8}{4} = 5$ and $\\mu_2 = \\frac{1+5+7+13}{4} = \\frac{26}{4} = 6.5$.\nThe centered columns are:\n$\\mathbf{x}_{c,1} = (2-5, 4-5, 6-5, 8-5)^T = (-3, -1, 1, 3)^T$.\n$\\mathbf{x}_{c,2} = (1-6.5, 5-6.5, 7-6.5, 13-6.5)^T = (-5.5, -1.5, 0.5, 6.5)^T = \\frac{1}{2}(-11, -3, 1, 13)^T$.\n\nNext, we compute the centered Gram matrix $G_c = X_c^T X_c$:\n$(G_c)_{11} = \\mathbf{x}_{c,1}^T \\mathbf{x}_{c,1} = (-3)^2 + (-1)^2 + 1^2 + 3^2 = 9+1+1+9 = 20$.\n$(G_c)_{22} = \\mathbf{x}_{c,2}^T \\mathbf{x}_{c,2} = (-5.5)^2 + (-1.5)^2 + 0.5^2 + 6.5^2 = 30.25+2.25+0.25+42.25 = 75$.\n$(G_c)_{12} = \\mathbf{x}_{c,1}^T \\mathbf{x}_{c,2} = (-3)(-5.5) + (-1)(-1.5) + (1)(0.5) + (3)(6.5) = 16.5 + 1.5 + 0.5 + 19.5 = 38$.\nSo, $G_c = \\begin{pmatrix} 20 & 38 \\\\ 38 & 75 \\end{pmatrix}$.\n\nTo find the condition number $\\kappa_c$, we find the eigenvalues of $G_c$. The characteristic equation is $\\det(G_c - \\lambda I) = 0$:\n$(20-\\lambda)(75-\\lambda) - 38^2 = 0 \\implies \\lambda^2 - 95\\lambda + 1500 - 1444 = 0 \\implies \\lambda^2 - 95\\lambda + 56 = 0$.\nThe eigenvalues are $\\lambda = \\frac{95 \\pm \\sqrt{95^2 - 4(1)(56)}}{2} = \\frac{95 \\pm \\sqrt{9025 - 224}}{2} = \\frac{95 \\pm \\sqrt{8801}}{2}$.\nThe condition number is $\\kappa_c = \\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}} = \\frac{95 + \\sqrt{8801}}{95 - \\sqrt{8801}}$.\n\nNow, we compute the standardized Gram matrix $G_z$.\nThe population variances are $\\sigma_1^2 = \\frac{(G_c)_{11}}{n} = \\frac{20}{4} = 5$ and $\\sigma_2^2 = \\frac{(G_c)_{22}}{n} = \\frac{75}{4}$.\nThe standard deviations are $\\sigma_1 = \\sqrt{5}$ and $\\sigma_2 = \\sqrt{\\frac{75}{4}} = \\frac{5\\sqrt{3}}{2}$.\nThe entries of $G_z$ are:\n$(G_z)_{11} = \\frac{(G_c)_{11}}{\\sigma_1^2} = \\frac{20}{5} = 4$.\n$(G_z)_{22} = \\frac{(G_c)_{22}}{\\sigma_2^2} = \\frac{75}{75/4} = 4$.\n$(G_z)_{12} = (G_z)_{21} = \\frac{(G_c)_{12}}{\\sigma_1 \\sigma_2} = \\frac{38}{\\sqrt{5} \\cdot \\frac{5\\sqrt{3}}{2}} = \\frac{76}{5\\sqrt{15}}$.\nSo, $G_z = \\begin{pmatrix} 4 & \\frac{76}{5\\sqrt{15}} \\\\ \\frac{76}{5\\sqrt{15}} & 4 \\end{pmatrix}$.\n\nTo find the condition number $\\kappa_z$, we find the eigenvalues of $G_z$. $G_z$ has the form $k \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ with $k=4$ and $\\rho = \\frac{76/ (5\\sqrt{15})}{4} = \\frac{19}{5\\sqrt{15}}$.\nThe eigenvalues of $\\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ are $1 \\pm \\rho$. Thus the eigenvalues of $G_z$ are $4(1 \\pm \\rho)$.\nThe condition number is $\\kappa_z = \\frac{4(1+\\rho)}{4(1-\\rho)} = \\frac{1+\\rho}{1-\\rho}$.\n$\\kappa_z = \\frac{1 + \\frac{19}{5\\sqrt{15}}}{1 - \\frac{19}{5\\sqrt{15}}} = \\frac{5\\sqrt{15} + 19}{5\\sqrt{15} - 19}$.\n\nPart 4: Computation of the Improvement Factor $I$\nThe improvement factor is defined as $I = \\frac{\\kappa_c}{\\kappa_z}$. Using the expressions derived above:\n$$I = \\frac{\\frac{95 + \\sqrt{8801}}{95 - \\sqrt{8801}}}{\\frac{5\\sqrt{15} + 19}{5\\sqrt{15} - 19}}$$\nThis can be rewritten as:\n$$I = \\left(\\frac{95 + \\sqrt{8801}}{95 - \\sqrt{8801}}\\right) \\left(\\frac{5\\sqrt{15} - 19}{5\\sqrt{15} + 19}\\right)$$\nThis is the final exact analytic expression.", "answer": "$$\\boxed{\\left(\\frac{95 + \\sqrt{8801}}{95 - \\sqrt{8801}}\\right) \\left(\\frac{5\\sqrt{15} - 19}{5\\sqrt{15} + 19}\\right)}$$", "id": "2749099"}, {"introduction": "For sequence-based biological design, deep learning models like Convolutional Neural Networks (CNNs) are powerful tools for identifying predictive local patterns or motifs. This practice guides you through the core learning mechanism of a neural network: backpropagation [@problem_id:2749093]. You will manually compute the gradient of a simple loss function with respect to the model's filter weights, tracing the error signal from the output back to the parameters that need to be updated.", "problem": "In de novo regulatory sequence design for synthetic biology, convolutional neural networks are often used to detect short sequence motifs across deoxyribonucleic acid (DNA) sequences encoded as one-hot vectors. Consider a single-filter one-dimensional convolutional model that maps a single input DNA sequence to a scalar prediction for enhancer activity. The input sequence has length $L=5$ over the alphabet $\\{A,C,G,T\\}$, and is one-hot encoded with column order $(A,C,G,T)$. The filter has width $k=3$ and weight matrix $w \\in \\mathbb{R}^{3 \\times 4}$ with rows indexed by offset $j \\in \\{0,1,2\\}$ and columns by nucleotides $(A,C,G,T)$ in that order. The model uses cross-correlation (the deep learning convention of one-dimensional convolution), stride $1$, no padding, and no bias. For position $t \\in \\{0,1,2\\}$, the preactivation is\n$$\nz_t \\;=\\; \\sum_{j=0}^{2} \\sum_{c \\in \\{A,C,G,T\\}} w_{j,c} \\, x_{t+j,c},\n$$\nwhere $x_{i,c} \\in \\{0,1\\}$ is the one-hot encoding of the nucleotide at position $i$. The activation is the Rectified Linear Unit (ReLU), defined as $\\phi(u) = \\max(0,u)$. The model pools by summation to produce $\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t)$. The loss for a single training example with target $y$ is the mean squared error (MSE) $L = \\tfrac{1}{2}\\,(\\hat{y} - y)^2$.\n\nFor the single training example specified below, compute the gradient $\\nabla_w L$, that is, all partial derivatives $\\frac{\\partial L}{\\partial w_{j,c}}$. Express your final answer as a single row matrix listing the entries in the order $(j=0 \\text{ row } [A,C,G,T],\\, j=1 \\text{ row } [A,C,G,T],\\, j=2 \\text{ row } [A,C,G,T])$. No rounding is required.\n\nInput specification:\n- Sequence (length $L=5$): positions $0$ through $4$ are $\\big(A,\\,C,\\,G,\\,T,\\,A\\big)$.\n- Column order for one-hot encoding: $(A,C,G,T)$.\n- Filter width $k=3$, stride $1$, valid cross-correlation, no bias.\n- Weights\n$$\nw \\;=\\;\n\\begin{pmatrix}\n2 & -1 & 0 & 1\\\\\n-1 & 2 & 1 & -2\\\\\n0 & 1 & -1 & 2\n\\end{pmatrix},\n$$\nwith rows $j=0,1,2$ and columns $(A,C,G,T)$.\n- Activation: Rectified Linear Unit (ReLU), $\\phi(u) = \\max(0,u)$.\n- Pooling: sum, $\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t)$.\n- Loss: $L = \\tfrac{1}{2}\\,(\\hat{y} - y)^2$ with target $y=4$.\n\nProvide the gradient as specified. Your answer must be a single row matrix of real numbers.", "solution": "The problem requires the computation of the gradient of the loss function $L$ with respect to the filter weights $w$, denoted as $\\nabla_w L$. This requires applying the chain rule for multivariable calculus, a procedure known in this context as backpropagation. We must compute each partial derivative $\\frac{\\partial L}{\\partial w_{j,c}}$ for $j \\in \\{0,1,2\\}$ and $c \\in \\{A,C,G,T\\}$.\n\nThe model's structure is as follows:\n$x \\rightarrow z \\rightarrow \\phi(z) \\rightarrow \\hat{y} \\rightarrow L$.\nThe loss $L = \\frac{1}{2}(\\hat{y} - y)^2$, where $\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t)$ and $z_t = \\sum_{j=0}^{2} \\sum_{c} w_{j,c} x_{t+j,c}$.\n\nWe begin with the forward pass to compute all necessary intermediate values.\n\nFirst, we represent the input sequence $(A,C,G,T,A)$ as a one-hot encoded matrix $x \\in \\mathbb{R}^{5 \\times 4}$, where rows correspond to positions $0$ through $4$ and columns to nucleotides $(A,C,G,T)$.\n$$\nx =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0\n\\end{pmatrix}\n$$\nThe rows are $x_0, x_1, x_2, x_3, x_4$.\n\nNext, we compute the preactivations $z_t$ for $t \\in \\{0,1,2\\}$ using the cross-correlation operation.\nFor $t=0$, the input window is $(x_0, x_1, x_2)$, corresponding to nucleotides $(A,C,G)$.\n$$\nz_0 = \\sum_{j=0}^2 w_{j, \\text{nuc}(j)} = w_{0,A} + w_{1,C} + w_{2,G} = 2 + 2 + (-1) = 3\n$$\nFor $t=1$, the input window is $(x_1, x_2, x_3)$, corresponding to nucleotides $(C,G,T)$.\n$$\nz_1 = \\sum_{j=0}^2 w_{j, \\text{nuc}(j+1)} = w_{0,C} + w_{1,G} + w_{2,T} = (-1) + 1 + 2 = 2\n$$\nFor $t=2$, the input window is $(x_2, x_3, x_4)$, corresponding to nucleotides $(G,T,A)$.\n$$\nz_2 = \\sum_{j=0}^2 w_{j, \\text{nuc}(j+2)} = w_{0,G} + w_{1,T} + w_{2,A} = 0 + (-2) + 0 = -2\n$$\nThe vector of preactivations is $z = (3, 2, -2)$.\n\nWe apply the ReLU activation function $\\phi(u) = \\max(0,u)$ to each $z_t$.\n$$\n\\phi(z_0) = \\max(0,3) = 3\n$$\n$$\n\\phi(z_1) = \\max(0,2) = 2\n$$\n$$\n\\phi(z_2) = \\max(0,-2) = 0\n$$\nThe vector of activations is $(3, 2, 0)$.\n\nThe final prediction $\\hat{y}$ is obtained by summing the activations.\n$$\n\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t) = 3 + 2 + 0 = 5\n$$\n\nNow, we proceed with the backward pass to compute the gradient $\\nabla_w L$. The partial derivative of the loss $L$ with respect to a weight $w_{j,c}$ is given by the chain rule:\n$$\n\\frac{\\partial L}{\\partial w_{j,c}} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_{j,c}}\n$$\nWe analyze the term $\\frac{\\partial \\hat{y}}{\\partial w_{j,c}}$ further. As $\\hat{y}$ is a sum over $\\phi(z_t)$, and each $z_t$ depends on the weights $w$, we have:\n$$\n\\frac{\\partial \\hat{y}}{\\partial w_{j,c}} = \\sum_{t=0}^{2} \\frac{\\partial \\phi(z_t)}{\\partial w_{j,c}} = \\sum_{t=0}^{2} \\frac{\\partial \\phi(z_t)}{\\partial z_t} \\frac{\\partial z_t}{\\partial w_{j,c}}\n$$\nCombining these, the full expression for the gradient element is:\n$$\n\\frac{\\partial L}{\\partial w_{j,c}} = \\frac{\\partial L}{\\partial \\hat{y}} \\sum_{t=0}^{2} \\frac{\\partial \\phi(z_t)}{\\partial z_t} \\frac{\\partial z_t}{\\partial w_{j,c}}\n$$\nWe compute each component of this expression.\nThe derivative of the loss $L$ with respect to the prediction $\\hat{y}$ is:\n$$\n\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y = 5 - 4 = 1\n$$\nThe derivative of the ReLU function is a step function: $\\phi'(u) = 1$ if $u > 0$ and $0$ if $u \\le 0$. We denote this as $\\mathbb{I}(u > 0)$.\n$$\n\\frac{\\partial \\phi(z_0)}{\\partial z_0} = \\mathbb{I}(3 > 0) = 1\n$$\n$$\n\\frac{\\partial \\phi(z_1)}{\\partial z_1} = \\mathbb{I}(2 > 0) = 1\n$$\n$$\n\\frac{\\partial \\phi(z_2)}{\\partial z_2} = \\mathbb{I}(-2 > 0) = 0\n$$\nThe derivative of the preactivation $z_t$ with respect to a weight $w_{j,c}$ is:\n$$\n\\frac{\\partial z_t}{\\partial w_{j,c}} = \\frac{\\partial}{\\partial w_{j,c}} \\left( \\sum_{j'=0}^{2} \\sum_{c'} w_{j',c'} \\, x_{t+j',c'} \\right) = x_{t+j,c}\n$$\nsince $x_{t+j',c'}$ are constants with respect to $w_{j,c}$ and the derivative is non-zero only for matching indices.\n\nSubstituting these components back into the gradient expression:\n$$\n\\frac{\\partial L}{\\partial w_{j,c}} = (1) \\sum_{t=0}^{2} \\frac{\\partial \\phi(z_t)}{\\partial z_t} x_{t+j,c} = (1) \\cdot x_{0+j,c} + (1) \\cdot x_{1+j,c} + (0) \\cdot x_{2+j,c} = x_{j,c} + x_{1+j,c}\n$$\nThis formula shows that the gradient with respect to a weight $w_{j,c}$ is the sum of the one-hot encoded values of nucleotide $c$ at the relevant input positions covered by the filter, where the corresponding convolutional windows ($t=0,1$) yielded a positive preactivation.\n\nWe now compute the gradient matrix $\\nabla_w L \\in \\mathbb{R}^{3 \\times 4}$.\n\nFor $j=0$:\n$(\\nabla_w L)_{0,c} = x_{0,c} + x_{1,c}$.\n$x_0$ corresponds to $A \\implies (1,0,0,0)$.\n$x_1$ corresponds to $C \\implies (0,1,0,0)$.\nThe gradient for the first row of weights is $(1,0,0,0) + (0,1,0,0) = (1,1,0,0)$.\n\nFor $j=1$:\n$(\\nabla_w L)_{1,c} = x_{1,c} + x_{2,c}$.\n$x_1$ corresponds to $C \\implies (0,1,0,0)$.\n$x_2$ corresponds to $G \\implies (0,0,1,0)$.\nThe gradient for the second row of weights is $(0,1,0,0) + (0,0,1,0) = (0,1,1,0)$.\n\nFor $j=2$:\n$(\\nabla_w L)_{2,c} = x_{2,c} + x_{3,c}$.\n$x_2$ corresponds to $G \\implies (0,0,1,0)$.\n$x_3$ corresponds to $T \\implies (0,0,0,1)$.\nThe gradient for the third row of weights is $(0,0,1,0) + (0,0,0,1) = (0,0,1,1)$.\n\nAssembling the complete gradient matrix:\n$$\n\\nabla_w L =\n\\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{pmatrix}\n$$\nThe problem requires the answer as a single row matrix, which is formed by concatenating the rows of $\\nabla_w L$.\nThe resulting vector is $(1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 1\n\\end{pmatrix}\n}\n$$", "id": "2749093"}, {"introduction": "A trained predictive model is only useful if it can guide us toward better designs. Bayesian Optimization (BO) provides a principled framework for this, using a surrogate model's predictions and uncertainties to intelligently select the next experiment. This exercise asks you to derive the closed-form expression for Expected Improvement (EI), one of the most classic and effective acquisition functions used in BO [@problem_id:2749128]. Mastering this derivation illuminates how BO formalizes the trade-off between exploiting known high-performing regions and exploring uncertain ones, a key to its sample efficiency in costly design-build-test-learn cycles.", "problem": "In a closed-loop design campaign for optimizing a synthetic promoter to maximize transcriptional output, you use Bayesian optimization with a Gaussian Process (GP) surrogate. At a candidate design $x$, the GP posterior predictive distribution for the unknown true objective $f(x)$ is Gaussian with mean $\\mu(x)$ and variance $\\sigma^{2}(x)$, written as $f(x) \\sim \\mathcal{N}\\!\\left(\\mu(x), \\sigma^{2}(x)\\right)$. Let the current best observed objective value be $f_{\\text{best}}$, and suppose the aim is to maximize $f(x)$.\n\nDefine the improvement random variable as $I(x) = \\max\\!\\big(0, f(x) - f_{\\text{best}}\\big)$, and the Expected Improvement (EI) acquisition function as $\\text{EI}(x) = \\mathbb{E}\\!\\left[I(x)\\right]$ with the expectation taken under the GP posterior predictive distribution. Starting from these definitions and the normality of $f(x)$, derive a closed-form expression for $\\text{EI}(x)$ in terms of $\\mu(x)$, $\\sigma(x)$, $f_{\\text{best}}$, the standard normal cumulative distribution function $\\Phi(\\cdot)$, and the standard normal probability density function $\\phi(\\cdot)$. Assume $\\sigma(x) > 0$. Your final answer must be a single closed-form analytic expression. Do not provide intermediate steps in the final answer box.", "solution": "The problem statement is a standard derivation in the field of Bayesian optimization and is scientifically grounded, well-posed, and objective. It contains no logical or factual flaws. Thus, we proceed with the solution.\n\nThe objective is to derive the closed-form expression for the Expected Improvement (EI) acquisition function. We are given the following definitions:\nThe posterior predictive distribution for the objective function $f(x)$ at a point $x$ is Gaussian:\n$$f(x) \\sim \\mathcal{N}(\\mu(x), \\sigma^{2}(x))$$\nThe improvement random variable $I(x)$ is defined relative to the current best observed value, $f_{\\text{best}}$:\n$$I(x) = \\max(0, f(x) - f_{\\text{best}})$$\nThe Expected Improvement is the expectation of this random variable, taken with respect to the posterior predictive distribution of $f(x)$:\n$$\\text{EI}(x) = \\mathbb{E}[I(x)] = \\mathbb{E}[\\max(0, f(x) - f_{\\text{best}})]$$\nTo compute this expectation, we must integrate the improvement over the probability density of $f(x)$. Let $p(y | x)$ be the probability density function (PDF) of the Gaussian distribution $\\mathcal{N}(\\mu(x), \\sigma^{2}(x))$ evaluated at $y$. The expectation is then given by the integral:\n$$\\text{EI}(x) = \\int_{-\\infty}^{\\infty} \\max(0, y - f_{\\text{best}}) \\, p(y | x) \\, dy$$\nThe term $\\max(0, y - f_{\\text{best}})$ is non-zero only for $y > f_{\\text{best}}$. Therefore, we can change the lower limit of integration to $f_{\\text{best}}$ and drop the $\\max$ operator:\n$$\\text{EI}(x) = \\int_{f_{\\text{best}}}^{\\infty} (y - f_{\\text{best}}) \\, p(y | x) \\, dy$$\nThe PDF $p(y|x)$ is given by:\n$$p(y|x) = \\frac{1}{\\sqrt{2\\pi}\\sigma(x)} \\exp\\left(-\\frac{(y - \\mu(x))^2}{2\\sigma^{2}(x)}\\right)$$\nTo simplify the integral, we perform a change of variables to standardize the distribution. Let a new random variable $Z$ be defined as:\n$$Z = \\frac{y - \\mu(x)}{\\sigma(x)}$$\nThis variable $Z$ follows the standard normal distribution, $Z \\sim \\mathcal{N}(0, 1)$. From this definition, we have $y = \\mu(x) + \\sigma(x)Z$, and the differential is $dy = \\sigma(x) dZ$. The PDF of $Z$ is the standard normal PDF, denoted by $\\phi(z)$. The lower limit of integration $y = f_{\\text{best}}$ becomes:\n$$z_{lower} = \\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}$$\nThe upper limit $y \\to \\infty$ corresponds to $z \\to \\infty$. Substituting these into the integral gives:\n$$\\text{EI}(x) = \\int_{\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}}^{\\infty} ((\\mu(x) + \\sigma(x)z) - f_{\\text{best}}) \\, \\phi(z) \\, dz$$\nWe can rearrange the terms inside the integral and split it into two parts:\n$$\\text{EI}(x) = \\int_{\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}}^{\\infty} (\\mu(x) - f_{\\text{best}}) \\phi(z) dz + \\int_{\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}}^{\\infty} \\sigma(x)z \\phi(z) dz$$\nLet us define the scaled improvement term for notational clarity:\n$$Z_{imp} = \\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}$$\nNote that $\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)} = -Z_{imp}$. The integral becomes:\n$$\\text{EI}(x) = (\\mu(x) - f_{\\text{best}}) \\int_{-Z_{imp}}^{\\infty} \\phi(z) dz + \\sigma(x) \\int_{-Z_{imp}}^{\\infty} z \\phi(z) dz$$\nWe evaluate each integral separately.\n\nFor the first integral, we recognize that it is related to the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt$.\n$$\\int_{-Z_{imp}}^{\\infty} \\phi(z) dz = 1 - \\int_{-\\infty}^{-Z_{imp}} \\phi(z) dz = 1 - \\Phi(-Z_{imp})$$\nUsing the property of the standard normal CDF that $1 - \\Phi(-z) = \\Phi(z)$, this simplifies to:\n$$\\int_{-Z_{imp}}^{\\infty} \\phi(z) dz = \\Phi(Z_{imp})$$\nSo the first term is $(\\mu(x) - f_{\\text{best}}) \\Phi(Z_{imp})$.\n\nFor the second integral, we have $\\int z \\phi(z) dz$. The integrand is $z \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$.\n$$\\int_{-Z_{imp}}^{\\infty} z \\phi(z) dz = \\int_{-Z_{imp}}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz$$\nThis integral can be solved directly, as $-\\phi'(z) = z \\phi(z)$. Alternatively, using substitution, let $u = -z^2/2$, so $du = -z dz$.\n$$\\int z \\exp\\left(-\\frac{z^2}{2}\\right) dz = -\\int \\exp(u) du = -\\exp(u) = -\\exp\\left(-\\frac{z^2}{2}\\right)$$\nEvaluating the definite integral:\n$$\\sigma(x) \\int_{-Z_{imp}}^{\\infty} z \\phi(z) dz = \\frac{\\sigma(x)}{\\sqrt{2\\pi}} \\left[ -\\exp\\left(-\\frac{z^2}{2}\\right) \\right]_{-Z_{imp}}^{\\infty}$$\n$$= \\frac{\\sigma(x)}{\\sqrt{2\\pi}} \\left( 0 - \\left(-\\exp\\left(-\\frac{(-Z_{imp})^2}{2}\\right)\\right) \\right)$$\n$$= \\frac{\\sigma(x)}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{Z_{imp}^2}{2}\\right)$$\nThis is precisely $\\sigma(x)\\phi(Z_{imp})$, because the standard normal PDF is an even function, $\\phi(z) = \\phi(-z)$, and is defined as $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{z^2}{2})$.\n\nCombining the two terms, we get:\n$$\\text{EI}(x) = (\\mu(x) - f_{\\text{best}}) \\Phi(Z_{imp}) + \\sigma(x) \\phi(Z_{imp})$$\nSubstituting back the definition of $Z_{imp}$:\n$$\\text{EI}(x) = (\\mu(x) - f_{\\text{best}}) \\Phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right) + \\sigma(x) \\phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right)$$\nThis is the final closed-form expression for the Expected Improvement. It correctly depends on $\\mu(x)$, $\\sigma(x)$, $f_{\\text{best}}$, and the standard normal CDF $\\Phi$ and PDF $\\phi$. The assumption $\\sigma(x) > 0$ ensures the denominator is non-zero, validating the derivation.", "answer": "$$\\boxed{(\\mu(x) - f_{\\text{best}}) \\Phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right) + \\sigma(x) \\phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right)}$$", "id": "2749128"}]}