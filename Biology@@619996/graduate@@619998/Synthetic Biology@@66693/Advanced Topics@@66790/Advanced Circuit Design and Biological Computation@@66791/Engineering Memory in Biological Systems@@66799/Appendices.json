{"hands_on_practices": [{"introduction": "The foundation of engineered cellular memory often lies in creating bistable systems, where a genetic circuit can stably exist in one of two states, such as \"on\" or \"off\". A powerful and common motif to achieve this is the positive feedback loop. This exercise [@problem_id:2037230] provides a hands-on calculation to explore the dynamics of such a system, challenging you to determine the critical concentration of an inducer molecule required to \"flip\" the memory switch from its low to high state by analyzing its underlying mathematical model.", "problem": "A synthetic biologist is engineering a genetic \"memory\" circuit in a bacterial strain. The state of the memory is determined by the concentration of a fluorescent protein, P. The circuit is designed such that the gene encoding P is expressed at a constant low basal rate. In addition, the protein P can act as a transcriptional activator for its own gene, creating a positive feedback loop. This self-activation is cooperative, requiring two molecules of P to bind the promoter, and is dependent on the presence of a chemical inducer, S.\n\nThe dynamics of the protein concentration, $[P]$, can be described by the following differential equation:\n$$\n\\frac{d[P]}{dt} = k_{basal} + V_{max}([S]) \\frac{[P]^2}{K^2 + [P]^2} - \\gamma [P]\n$$\nwhere the maximum rate of activated production, $V_{max}$, depends on the inducer concentration $[S]$ according to the Michaelis-Menten-like function:\n$$\nV_{max}([S]) = V_{peak} \\frac{[S]}{K_S + [S]}\n$$\nThis system is designed to exhibit memory (hysteresis). When the inducer concentration $[S]$ is slowly increased from zero, the system remains in a low-expression state until $[S]$ reaches a critical value, $[S]_{up}$, at which point the protein concentration abruptly switches to a high-expression state. It has been determined experimentally that this upward switch occurs precisely when the protein concentration reaches a critical value of $[P]_{up} = 2.00 \\text{ }\\mu\\text{M}$.\n\nUsing the parameters provided below, calculate the critical inducer concentration $[S]_{up}$ required to trigger this switch.\n\n- Basal production rate, $k_{basal} = 0.075 \\text{ }\\mu\\text{M} \\cdot \\text{min}^{-1}$\n- Peak activated production rate, $V_{peak} = 2.00 \\text{ }\\mu\\text{M} \\cdot \\text{min}^{-1}$\n- Protein degradation rate constant, $\\gamma = 0.100 \\text{ min}^{-1}$\n- Activation constant for protein P, $K = 4.00 \\text{ }\\mu\\text{M}$\n- Activation constant for inducer S, $K_S = 10.0 \\text{ }\\mu\\text{M}$\n\nExpress your answer for $[S]_{up}$ in units of $\\mu\\text{M}$, rounded to three significant figures.", "solution": "The protein dynamics are given by\n$$\n\\frac{d[P]}{dt} = k_{basal} + V_{max}([S]) \\frac{[P]^{2}}{K^{2} + [P]^{2}} - \\gamma [P].\n$$\nDefine $f(P;S) = k_{basal} + V_{max}([S]) \\frac{P^{2}}{K^{2} + P^{2}} - \\gamma P$. At the upward switching point (a saddle-node bifurcation), the steady-state and tangency conditions hold simultaneously:\n$$\nf(P_{up}; S_{up}) = 0, \\quad \\frac{\\partial f}{\\partial P}(P_{up}; S_{up}) = 0.\n$$\nCompute the derivative:\n$$\n\\frac{\\partial f}{\\partial P} = V_{max}([S]) \\cdot \\frac{2 K^{2} P}{(K^{2} + P^{2})^{2}} - \\gamma.\n$$\nThus, the tangency condition gives\n$$\nV_{max}([S_{up}]) = \\gamma \\frac{(K^{2} + P_{up}^{2})^{2}}{2 K^{2} P_{up}}.\n$$\nSubstitute this into the steady-state condition:\n$$\n0 = k_{basal} + \\left[\\gamma \\frac{(K^{2} + P_{up}^{2})^{2}}{2 K^{2} P_{up}}\\right] \\frac{P_{up}^{2}}{K^{2} + P_{up}^{2}} - \\gamma P_{up}\n= k_{basal} + \\gamma \\frac{(K^{2} + P_{up}^{2}) P_{up}}{2 K^{2}} - \\gamma P_{up}.\n$$\nThis simplifies to the consistency condition\n$$\nk_{basal} + \\gamma P_{up} \\frac{P_{up}^{2} - K^{2}}{2 K^{2}} = 0,\n$$\nwhich is satisfied by the given $P_{up}$ and parameters (verified below).\n\nNow relate $V_{max}$ to the inducer:\n$$\nV_{max}([S]) = V_{peak} \\frac{[S]}{K_{S} + [S]} \\quad \\Rightarrow \\quad [S] = \\frac{K_{S} V_{max}}{V_{peak} - V_{max}}.\n$$\nEvaluate $V_{max}$ at the switch using the given $P_{up}$:\n$$\nV_{max}([S_{up}]) = \\gamma \\frac{(K^{2} + P_{up}^{2})^{2}}{2 K^{2} P_{up}}.\n$$\nWith $P_{up} = 2.00$, $K = 4.00$, $\\gamma = 0.100$ (in the stated units),\n$$\nK^{2} = 16,\\quad P_{up}^{2} = 4,\\quad K^{2} + P_{up}^{2} = 20,\\quad (K^{2} + P_{up}^{2})^{2} = 400,\n$$\n$$\nV_{max}([S_{up}]) = 0.100 \\cdot \\frac{400}{2 \\cdot 16 \\cdot 2} = 0.100 \\cdot \\frac{400}{64} = 0.625.\n$$\nCheck the steady-state consistency:\n$$\nk_{basal} + V_{max}\\frac{P_{up}^{2}}{K^{2} + P_{up}^{2}} - \\gamma P_{up} = 0.075 + 0.625 \\cdot \\frac{4}{20} - 0.100 \\cdot 2.00 = 0.075 + 0.125 - 0.200 = 0,\n$$\nconfirming the conditions are met.\n\nFinally, solve for $[S]_{up}$ using $V_{peak} = 2.00$ and $K_{S} = 10.0$:\n$$\n[S]_{up} = \\frac{K_{S} V_{max}([S_{up}])}{V_{peak} - V_{max}([S_{up}])} = \\frac{10.0 \\cdot 0.625}{2.00 - 0.625} = \\frac{6.25}{1.375} = \\frac{50}{11} \\approx 4.54545.\n$$\nRounded to three significant figures, $[S]_{up} = 4.55$ in the required units.", "answer": "$$\\boxed{4.55}$$", "id": "2037230"}, {"introduction": "A synthetic circuit's performance is not solely determined by its design, but also by its interaction with the complex host environment, a phenomenon known as context-dependence. This advanced practice [@problem_id:2734336] delves into this challenge by modeling how a memory device's genomic integration site can affect its reliability through biophysical mechanisms like DNA supercoiling and transcriptional interference. By implementing a quantitative model, you will predict how the local genetic landscape alters a memory switch's stability, a crucial step in engineering robust biological systems.", "problem": "Design a program that models how genomic integration context modulates transcriptional initiation via local DNA supercoiling and transcriptional interference, and then predicts the reliability of a bistable memory over a finite observation time. The system consists of two promoters, denoted promoter A and promoter B, forming a symmetric two-state memory (for instance, mutual repression or a recombinase-based latch). The model must be derived from fundamental biological and physical principles and expressed in a way that a developer can implement without domain-specific heuristics beyond the definitions given here.\n\nUse the following fundamental base:\n- Central Dogma of Molecular Biology (transcription produces RNA at a rate determined by promoter accessibility).\n- Statistical thermodynamics of initiation: the transcription initiation rate increases exponentially as the effective free-energy barrier decreases.\n- Twin-supercoiled-domain model: transcribing RNA polymerases generate positive supercoils ahead and negative supercoils behind; at steady state, local supercoiling contributions from multiple sources sum and decay with genomic distance due to relaxation processes.\n- Poisson process for promoter occlusion: interfering polymerase passages are treated as a Poisson process with a mean arrival flux; the promoter is free a fraction of time equal to the zero-event probability within an occlusion window.\n- Two-state reliability as survival under a constant hazard: the probability of no flip by time $T$ is the survival function of a Poisson process with rate equal to the spontaneous flip rate.\n\nModel specification:\n1) Local supercoiling density at promoter $X \\in \\{\\mathrm{A}, \\mathrm{B}\\}$ is\n$$\\sigma_X = \\sigma_0 + \\sum_{j=1}^{n_X} \\left( \\xi \\, r_{j,X} \\, e^{-d_{j,X}/\\ell} \\, o_{j,X} \\right),$$\nwhere $\\sigma_0$ is the background supercoiling density (dimensionless), $\\xi$ is a proportionality constant (in units of supercoiling density per $\\mathrm{s}^{-1}$), $r_{j,X}$ is the transcription initiation rate of neighbor $j$ (in $\\mathrm{s}^{-1}$), $d_{j,X}$ is the genomic distance from neighbor $j$ to promoter $X$ (in base pairs), $\\ell$ is the characteristic decay length (in base pairs), and $o_{j,X} \\in \\{-1,+1\\}$ is an orientation/sign factor encoding whether the neighbor deposits net negative ($-1$) or positive ($+1$) supercoiling at the promoter.\n\n2) Intrinsic initiation rate (absent occlusion) at promoter $X$ is\n$$r_{i,X} = r_{\\mathrm{base},X} \\, \\exp\\!\\left(-\\kappa \\, \\sigma_X\\right),$$\nwhere $r_{\\mathrm{base},X}$ is the baseline initiation rate at zero supercoiling (in $\\mathrm{s}^{-1}$), and $\\kappa$ is a dimensionless sensitivity constant. Negative supercoiling ($\\sigma_X < 0$) lowers the barrier and increases $r_{i,X}$.\n\n3) Transcriptional interference is modeled as promoter occlusion by an interfering flux $J_X$ (in $\\mathrm{s}^{-1}$) of RNA polymerases that traverse promoter $X$. The effective initiation rate becomes\n$$r_{\\mathrm{eff},X} = r_{i,X} \\, \\exp\\!\\left(-\\tau \\, J_X\\right),$$\nwhere $\\tau$ is the occlusion time per passage (in $\\mathrm{s}$). The interfering flux is\n$$J_X = r_{\\mathrm{int},X} \\, f_X \\, \\exp\\!\\left(-d_{\\mathrm{int},X}/\\lambda_{\\mathrm{occ}}\\right),$$\nwith $r_{\\mathrm{int},X}$ the initiation rate of the interfering promoter (in $\\mathrm{s}^{-1}$), $f_X \\in [0,1]$ the readthrough fraction reaching promoter $X$ (dimensionless), $d_{\\mathrm{int},X}$ the distance from the interfering promoter to $X$ (in base pairs), and $\\lambda_{\\mathrm{occ}}$ the characteristic decay length for traversal probability (in base pairs).\n\n4) The spontaneous flip rate out of state $X$ is modeled as\n$$k_{\\mathrm{flip},X} = k_0 \\, \\exp\\!\\left(-\\eta \\, r_{\\mathrm{eff},X}\\right),$$\nwhere $k_0$ is a baseline flip rate (in $\\mathrm{s}^{-1}$) and $\\eta$ is a sensitivity constant (in $\\mathrm{s}$) that captures how increased stabilizing expression reduces the flip hazard.\n\n5) If the memory stores either state with equal prior probability and the hazards are time-homogeneous, the reliability over a time horizon $T$ (in $\\mathrm{s}$) is\n$$R = \\tfrac{1}{2} \\left[\\exp\\!\\left(-k_{\\mathrm{flip},\\mathrm{A}} \\, T\\right) + \\exp\\!\\left(-k_{\\mathrm{flip},\\mathrm{B}} \\, T\\right)\\right],$$\nwhich is the average survival probability of no flip across the two possible stored states.\n\nUnits and constants:\n- All rates $r$ and $k$ are in $\\mathrm{s}^{-1}$.\n- Distances $d$ and $d_{\\mathrm{int}}$ are in base pairs.\n- Time constants $\\tau$, $\\eta$, and horizon $T$ are in $\\mathrm{s}$.\n- Supercoiling density $\\sigma$ is dimensionless.\nUse the following fixed constants for all test cases:\n- $\\sigma_0 = -0.055$,\n- $\\kappa = 25.0$,\n- $\\xi = 0.01$,\n- $\\ell = 1500.0$,\n- $\\tau = 1.5$,\n- $\\lambda_{\\mathrm{occ}} = 800.0$,\n- $k_0 = 1.0 \\times 10^{-4}$,\n- $\\eta = 10.0$.\n\nInput specification for each test case:\n- Promoter A parameters: $r_{\\mathrm{base},\\mathrm{A}}$, neighbor list $\\{(r_{j,\\mathrm{A}}, d_{j,\\mathrm{A}}, o_{j,\\mathrm{A}})\\}_{j=1}^{n_\\mathrm{A}}$, and interfering set $(r_{\\mathrm{int},\\mathrm{A}}, d_{\\mathrm{int},\\mathrm{A}}, f_\\mathrm{A})$.\n- Promoter B parameters: $r_{\\mathrm{base},\\mathrm{B}}$, neighbor list $\\{(r_{j,\\mathrm{B}}, d_{j,\\mathrm{B}}, o_{j,\\mathrm{B}})\\}_{j=1}^{n_\\mathrm{B}}$, and interfering set $(r_{\\mathrm{int},\\mathrm{B}}, d_{\\mathrm{int},\\mathrm{B}}, f_\\mathrm{B})$.\n- Time horizon $T$.\n\nYour program must compute $R$ for each test case and output the results.\n\nTest suite:\nProvide exactly $5$ test cases using the parameters below.\n\n- Test case $1$ (moderate negative supercoiling and mild interference, $T = 3600.0$):\n  - Promoter A: $r_{\\mathrm{base},\\mathrm{A}} = 0.05$, neighbors $\\{(0.3, 600, -1)\\}$, interfering $(0.4, 500, 0.2)$.\n  - Promoter B: $r_{\\mathrm{base},\\mathrm{B}} = 0.05$, neighbors $\\{(0.2, 800, -1)\\}$, interfering $(0.3, 700, 0.15)$.\n  - $T = 3600.0$.\n\n- Test case $2$ (strong positive supercoiling and occlusion for A, favorable for B, $T = 3600.0$):\n  - Promoter A: $r_{\\mathrm{base},\\mathrm{A}} = 0.05$, neighbors $\\{(0.6, 300, +1), (0.5, 500, +1)\\}$, interfering $(1.0, 200, 0.6)$.\n  - Promoter B: $r_{\\mathrm{base},\\mathrm{B}} = 0.05$, neighbors $\\{(0.4, 1000, -1)\\}$, interfering $(0.2, 900, 0.1)$.\n  - $T = 3600.0$.\n\n- Test case $3$ (no neighbors, no interference, $T = 3600.0$):\n  - Promoter A: $r_{\\mathrm{base},\\mathrm{A}} = 0.05$, neighbors $\\{\\}$, interfering $(0.0, 1000, 0.0)$.\n  - Promoter B: $r_{\\mathrm{base},\\mathrm{B}} = 0.05$, neighbors $\\{\\}$, interfering $(0.0, 1000, 0.0)$.\n  - $T = 3600.0$.\n\n- Test case $4$ (very long distances causing negligible coupling, $T = 7200.0$):\n  - Promoter A: $r_{\\mathrm{base},\\mathrm{A}} = 0.05$, neighbors $\\{(0.8, 5000, +1)\\}$, interfering $(0.5, 4000, 0.1)$.\n  - Promoter B: $r_{\\mathrm{base},\\mathrm{B}} = 0.05$, neighbors $\\{(1.0, 6000, -1)\\}$, interfering $(0.5, 6000, 0.05)$.\n  - $T = 7200.0$.\n\n- Test case $5$ (strong beneficial negative supercoiling, no interference, $T = 3600.0$):\n  - Promoter A: $r_{\\mathrm{base},\\mathrm{A}} = 0.05$, neighbors $\\{(1.2, 200, -1), (1.0, 400, -1)\\}$, interfering $(0.0, 500, 0.0)$.\n  - Promoter B: $r_{\\mathrm{base},\\mathrm{B}} = 0.05$, neighbors $\\{(1.2, 300, -1)\\}$, interfering $(0.0, 500, 0.0)$.\n  - $T = 3600.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the reliability results as a comma-separated list of decimal numbers enclosed in square brackets, with each reliability rounded to exactly six decimal places, in the same order as the test cases. For example, an output with three hypothetical results must look like\n\"[0.912345,0.876543,0.999000]\".", "solution": "The problem statement is subjected to validation and is determined to be valid. It is self-contained, providing a complete set of equations, parameters, and constants. The model is computationally well-posed, leading to a unique solution for each set of inputs. The underlying principles are scientifically grounded in molecular biology and biophysics, specifically the twin-supercoiled-domain model, statistical mechanics of transcription, and Poisson process modeling of stochastic molecular events. There are no logical contradictions, factual inaccuracies, or ambiguities. I will therefore proceed to construct the solution.\n\nThe objective is to compute the reliability, $R$, of a bistable genetic memory system over a time horizon $T$. The reliability is determined by the stability of its two states, represented by promoters $A$ and $B$. The stability of each state is quantified by a spontaneous flip rate, $k_{\\mathrm{flip},X}$, where $X \\in \\{A, B\\}$. The calculation follows a clear causal chain, from the genomic context to the final system reliability, which I will now detail.\n\nFirst, for each promoter $X$, we must calculate the local DNA supercoiling density, $\\sigma_X$. This is the sum of a background supercoiling $\\sigma_0$ and contributions from $n_X$ neighboring transcribing genes. The formula is:\n$$\n\\sigma_X = \\sigma_0 + \\sum_{j=1}^{n_X} \\left( \\xi \\, r_{j,X} \\, e^{-d_{j,X}/\\ell} \\, o_{j,X} \\right)\n$$\nHere, $\\xi$ is a proportionality constant, $r_{j,X}$ is the transcription rate of neighbor $j$, $d_{j,X}$ is the genomic distance to it, $\\ell$ is the characteristic decay length for supercoiling, and $o_{j,X}$ is an orientation factor ($+1$ for positive supercoiling, $-1$ for negative).\n\nSecond, the local supercoiling density $\\sigma_X$ directly influences the promoter's intrinsic transcription initiation rate, $r_{i,X}$. DNA unwinding is part of transcription initiation, so negative supercoiling ($\\sigma_X < 0$) typically enhances this process. The model captures this with an exponential dependence:\n$$\nr_{i,X} = r_{\\mathrm{base},X} \\, \\exp(-\\kappa \\, \\sigma_X)\n$$\nwhere $r_{\\mathrm{base},X}$ is the baseline rate at zero supercoiling and $\\kappa$ is the sensitivity of the promoter to supercoiling.\n\nThird, we account for transcriptional interference. A flux of RNA polymerases, $J_X$, traversing promoter $X$ can occlude it, preventing its own machinery from binding. This interfering flux is modeled as:\n$$\nJ_X = r_{\\mathrm{int},X} \\, f_X \\, \\exp(-d_{\\mathrm{int},X}/\\lambda_{\\mathrm{occ}})\n$$\nwhere $r_{\\mathrm{int},X}$ is the rate of the interfering promoter, $f_X$ is the fraction of transcripts that read through to promoter $X$, $d_{\\mathrm{int},X}$ is the distance, and $\\lambda_{\\mathrm{occ}}$ is a decay length for the probability of a polymerase to reach promoter $X$.\n\nFourth, the combined effects of supercoiling and interference yield the effective initiation rate, $r_{\\mathrm{eff},X}$. The occlusion effect reduces the intrinsic rate $r_{i,X}$ by a factor corresponding to the probability that the promoter is not occluded. This is modeled as a Poisson process:\n$$\nr_{\\mathrm{eff},X} = r_{i,X} \\, \\exp(-\\tau \\, J_X)\n$$\nwhere $\\tau$ is the characteristic time the promoter is occluded per interfering polymerase passage.\n\nFifth, the stability of the memory state maintained by promoter $X$ is inversely related to its effective expression rate $r_{\\mathrm{eff},X}$. Higher expression reinforces the state, reducing its probability of flipping. This is modeled as a constant hazard, the spontaneous flip rate $k_{\\mathrm{flip},X}$:\n$$\nk_{\\mathrm{flip},X} = k_0 \\, \\exp(-\\eta \\, r_{\\mathrm{eff},X})\n$$\nwhere $k_0$ is a baseline flip rate and $\\eta$ is a sensitivity constant quantifying the stabilizing effect of expression.\n\nFinally, with the flip rates for both states, $k_{\\mathrm{flip},A}$ and $k_{\\mathrm{flip},B}$, we can compute the overall system reliability $R$ over a time horizon $T$. Assuming each state is equally likely initially, the reliability is the average survival probability:\n$$\nR = \\frac{1}{2} \\left[\\exp(-k_{\\mathrm{flip},A} \\, T) + \\exp(-k_{\\mathrm{flip},B} \\, T)\\right]\n$$\nThis completes the sequence of calculations. The provided program implements these steps for each given test case, using the specified constants, to generate the final list of reliability values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating memory reliability in a biological system.\n    \"\"\"\n    \n    # Define fixed constants from the problem statement\n    SIGMA_0 = -0.055\n    KAPPA = 25.0\n    XI = 0.01\n    L_DECAY = 1500.0\n    TAU = 1.5\n    LAMBDA_OCC = 800.0\n    K0 = 1.0e-4\n    ETA = 10.0\n\n    def calculate_k_flip(r_base, neighbors, interference_params):\n        \"\"\"\n        Calculates the spontaneous flip rate for a single promoter.\n\n        Args:\n            r_base (float): Baseline initiation rate.\n            neighbors (list): List of tuples for supercoiling neighbors.\n                              Each tuple is (r_j, d_j, o_j).\n            interference_params (tuple): Parameters for transcriptional interference.\n                                         Tuple is (r_int, d_int, f).\n\n        Returns:\n            float: The calculated spontaneous flip rate k_flip.\n        \"\"\"\n        # Step 1: Calculate local supercoiling density sigma_X\n        supercoiling_sum = 0.0\n        for r_j, d_j, o_j in neighbors:\n            supercoiling_sum += XI * r_j * np.exp(-d_j / L_DECAY) * o_j\n        sigma_x = SIGMA_0 + supercoiling_sum\n\n        # Step 2: Calculate intrinsic initiation rate r_i,X\n        r_i_x = r_base * np.exp(-KAPPA * sigma_x)\n\n        # Step 3: Calculate interfering flux J_X\n        r_int_x, d_int_x, f_x = interference_params\n        j_x = r_int_x * f_x * np.exp(-d_int_x / LAMBDA_OCC)\n\n        # Step 4: Calculate effective initiation rate r_eff,X\n        r_eff_x = r_i_x * np.exp(-TAU * j_x)\n\n        # Step 5: Calculate spontaneous flip rate k_flip,X\n        k_flip_x = K0 * np.exp(-ETA * r_eff_x)\n        \n        return k_flip_x\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"A\": {\"r_base\": 0.05, \"neighbors\": [(0.3, 600, -1)], \"interfering\": (0.4, 500, 0.2)},\n            \"B\": {\"r_base\": 0.05, \"neighbors\": [(0.2, 800, -1)], \"interfering\": (0.3, 700, 0.15)},\n            \"T\": 3600.0\n        },\n        # Test case 2\n        {\n            \"A\": {\"r_base\": 0.05, \"neighbors\": [(0.6, 300, 1), (0.5, 500, 1)], \"interfering\": (1.0, 200, 0.6)},\n            \"B\": {\"r_base\": 0.05, \"neighbors\": [(0.4, 1000, -1)], \"interfering\": (0.2, 900, 0.1)},\n            \"T\": 3600.0\n        },\n        # Test case 3\n        {\n            \"A\": {\"r_base\": 0.05, \"neighbors\": [], \"interfering\": (0.0, 1000, 0.0)},\n            \"B\": {\"r_base\": 0.05, \"neighbors\": [], \"interfering\": (0.0, 1000, 0.0)},\n            \"T\": 3600.0\n        },\n        # Test case 4\n        {\n            \"A\": {\"r_base\": 0.05, \"neighbors\": [(0.8, 5000, 1)], \"interfering\": (0.5, 4000, 0.1)},\n            \"B\": {\"r_base\": 0.05, \"neighbors\": [(1.0, 6000, -1)], \"interfering\": (0.5, 6000, 0.05)},\n            \"T\": 7200.0\n        },\n        # Test case 5\n        {\n            \"A\": {\"r_base\": 0.05, \"neighbors\": [(1.2, 200, -1), (1.0, 400, -1)], \"interfering\": (0.0, 500, 0.0)},\n            \"B\": {\"r_base\": 0.05, \"neighbors\": [(1.2, 300, -1)], \"interfering\": (0.0, 500, 0.0)},\n            \"T\": 3600.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Calculate k_flip for Promoter A\n        params_A = case[\"A\"]\n        k_flip_A = calculate_k_flip(\n            params_A[\"r_base\"], params_A[\"neighbors\"], params_A[\"interfering\"]\n        )\n\n        # Calculate k_flip for Promoter B\n        params_B = case[\"B\"]\n        k_flip_B = calculate_k_flip(\n            params_B[\"r_base\"], params_B[\"neighbors\"], params_B[\"interfering\"]\n        )\n        \n        # Get time horizon T\n        T = case[\"T\"]\n\n        # Step 6: Calculate reliability R\n        reliability = 0.5 * (np.exp(-k_flip_A * T) + np.exp(-k_flip_B * T))\n        results.append(reliability)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2734336"}, {"introduction": "Building a biological memory device is only half the battle; the other half is accurately reading the information it has recorded. This final practice [@problem_id:2734309] frames this task as a classic decoding problem, where the history of an environmental input is a hidden signal that must be inferred from a sequence of noisy observations of the memory's state. You will apply the principles of Hidden Markov Models (HMMs) to develop a computational pipeline that can reconstruct the most probable input history, demonstrating a key skill in the analysis of data from molecular recorders.", "problem": "You are modeling a genome-editing biological memory device in which a cell’s molecular input at each discrete time step is hidden, but leaves a record via edits accumulated in a genomic locus. The system is described by a discrete-time probabilistic model with two hidden processes: a binary input process and a binary memory state process. Observations are the readouts of whether an edit is detected at each time. You are asked to derive, from first principles, the forward and backward recursions needed to compute the posterior marginal of the input process given observations, and then implement a program that applies these recursions to a specified test suite. The final program output must be a single line containing a list of decoded input sequences for each test case.\n\nModel specification. Time runs from $t=1$ to $t=T$. The hidden input process is $I_t \\in \\{0,1\\}$, where $I_t=1$ denotes that the external input is present and $I_t=0$ denotes that it is absent. The hidden memory state is $S_t \\in \\{0,1\\}$, where $S_t=0$ denotes an unedited locus and $S_t=1$ denotes an edited locus. The observation is $O_t \\in \\{0,1\\}$, where $O_t=1$ denotes that an edit is detected in sequencing and $O_t=0$ denotes that no edit is detected. The initial memory state before the first time step is fixed to $S_0=0$ almost surely.\n\nThe model adheres to the following conditional independencies and factorizations, which are grounded in the standard assumptions of hidden Markov models and dynamic Bayesian networks: the input process is a first-order Markov chain with transition matrix $A_I$ and initial distribution $\\pi_I$, the memory state transition depends on the current input and the previous memory state, and the observation depends only on the current memory state. Formally,\n- $P(I_1=i) = \\pi_I[i]$ and $P(I_t=i \\mid I_{t-1}=j) = A_I[j,i]$ for $t \\ge 2$.\n- $P(S_t \\mid S_{t-1}, I_t)$ is specified by an input-modulated, biologically realistic edit process that is irreversible: once edited, the locus remains edited. Specifically, for $S_{t-1}=0$, $P(S_t=1 \\mid S_{t-1}=0, I_t=0) = p_{\\mathrm{off}}$ and $P(S_t=1 \\mid S_{t-1}=0, I_t=1) = p_{\\mathrm{on}}$, with $P(S_t=0 \\mid S_{t-1}=0, I_t=i) = 1 - P(S_t=1 \\mid S_{t-1}=0, I_t=i)$ for $i \\in \\{0,1\\}$. For $S_{t-1}=1$, $P(S_t=1 \\mid S_{t-1}=1, I_t=i) = 1$ for both $i \\in \\{0,1\\}$.\n- The observation is generated with sequencing noise independent across time given the memory state: $P(O_t=1 \\mid S_t=1) = q$ and $P(O_t=1 \\mid S_t=0) = r$, with $P(O_t=0 \\mid S_t=s) = 1 - P(O_t=1 \\mid S_t=s)$ for $s \\in \\{0,1\\}$.\n\nTasks.\n1. Using only the product rule of probability, the law of total probability, Bayes’ rule, and the Markov property stipulated above, derive forward and backward recursions to compute the smoothed posterior marginal $P(I_t=i \\mid O_{1:T})$ for each $t \\in \\{1,\\dots,T\\}$ and $i \\in \\{0,1\\}$, where $O_{1:T}$ denotes the observed sequence. Your derivation must not assume any pre-stated hidden Markov model formulas; it must start from these fundamentals. The derivation should be expressed in terms of the joint latent process $(S_t, I_t)$ and the given conditional distributions.\n2. Design an algorithm that implements these recursions with numerical stability for arbitrary $T$, using scaling or log-transforms to avoid underflow. Your algorithm should produce the decoded input history defined by $\\hat{I}_t = \\arg\\max_{i \\in \\{0,1\\}} P(I_t=i \\mid O_{1:T})$ for each time $t$.\n3. Implement the algorithm as a program that runs on the provided test suite below and outputs the decoded input histories as a list of lists in a single line. No physical units are involved. All probabilities must be represented as decimals in $[0,1]$.\n\nTest suite. For each test case, you are given $T$, the input transition matrix $A_I$, the input prior $\\pi_I$, the memory transition edit probabilities $p_{\\mathrm{off}}$ and $p_{\\mathrm{on}}$, the observation parameters $q$ and $r$, and the observation sequence $O_{1:T}$.\n\n- Test case $1$:\n  - $T=6$\n  - $A_I = \\begin{bmatrix} 0.95 & 0.05 \\\\ 0.10 & 0.90 \\end{bmatrix}$\n  - $\\pi_I = \\begin{bmatrix} 0.90 & 0.10 \\end{bmatrix}$\n  - $p_{\\mathrm{off}} = 0.01$, $p_{\\mathrm{on}} = 0.20$\n  - $q = 0.95$, $r = 0.05$\n  - $O_{1:6} = [0,0,1,1,1,1]$\n- Test case $2$:\n  - $T=4$\n  - $A_I = \\begin{bmatrix} 0.98 & 0.02 \\\\ 0.02 & 0.98 \\end{bmatrix}$\n  - $\\pi_I = \\begin{bmatrix} 0.95 & 0.05 \\end{bmatrix}$\n  - $p_{\\mathrm{off}} = 0.005$, $p_{\\mathrm{on}} = 0.15$\n  - $q = 0.98$, $r = 0.02$\n  - $O_{1:4} = [0,0,0,0]$\n- Test case $3$:\n  - $T=5$\n  - $A_I = \\begin{bmatrix} 0.90 & 0.10 \\\\ 0.10 & 0.90 \\end{bmatrix}$\n  - $\\pi_I = \\begin{bmatrix} 0.50 & 0.50 \\end{bmatrix}$\n  - $p_{\\mathrm{off}} = 0.001$, $p_{\\mathrm{on}} = 0.30$\n  - $q = 0.95$, $r = 0.05$\n  - $O_{1:5} = [1,1,1,1,1]$\n- Test case $4$:\n  - $T=5$\n  - $A_I = \\begin{bmatrix} 0.90 & 0.10 \\\\ 0.10 & 0.90 \\end{bmatrix}$\n  - $\\pi_I = \\begin{bmatrix} 0.80 & 0.20 \\end{bmatrix}$\n  - $p_{\\mathrm{off}} = 0.05$, $p_{\\mathrm{on}} = 0.10$\n  - $q = 0.90$, $r = 0.10$\n  - $O_{1:5} = [0,1,0,1,0]$\n\nFinal output format. Your program should produce a single line of output containing the decoded input sequences for the four test cases as a comma-separated list enclosed in square brackets, where each decoded sequence is itself a list of integers $0$ or $1$ in temporal order. For example, the required format is $[\\,[0,1,1],\\,[0,0],\\,[1,1,1],\\,[0,1]\\,]$ for four cases. The program must not read any input and must not print any other text. The answers for each case are the decoded lists of integers, which are directly testable.", "solution": "The problem requires the derivation and implementation of an algorithm to infer a hidden input sequence, $I_{1:T}$, from a sequence of noisy observations, $O_{1:T}$. The system is a discrete-time probabilistic model involving two coupled hidden processes: the input $I_t \\in \\{0, 1\\}$ and a memory state $S_t \\in \\{0, 1\\}$. This structure is a specific instance of a dynamic Bayesian network, and the inference task is a classic smoothing problem. We will proceed methodically, beginning with a formal derivation from first principles.\n\nLet the joint latent state at time $t$ be $X_t = (S_t, I_t)$. The state space of $X_t$ consists of $4$ states: $\\{(0,0), (0,1), (1,0), (1,1)\\}$. The problem is now to infer the $I_t$ component of the hidden state sequence $X_{1:T}$ given the observation sequence $O_{1:T}$. This is achieved using the forward-backward algorithm, which we derive from the fundamental axioms of probability theory.\n\n**Model Specification in Terms of the Joint State $X_t = (S_t, I_t)$**\n\n1.  **Initial State Probability**: $P(X_1 = (s,i))$.\n    Given the fixed initial condition $S_0=0$ and the independence of $I_1$ from $S_0$, we have:\n    $$P(X_1=(s,i)) = P(S_1=s, I_1=i) = P(S_1=s \\mid I_1=i, S_0=0) P(I_1=i) = P(S_1=s \\mid S_0=0, I_1=i) \\pi_I[i]$$\n\n2.  **State Transition Probability**: $P(X_t=(s,i) \\mid X_{t-1}=(s',j))$.\n    Using the chain rule and the specified conditional independencies:\n    $$P(X_t=(s,i) \\mid X_{t-1}=(s',j)) = P(S_t=s, I_t=i \\mid S_{t-1}=s', I_{t-1}=j)$$\n    $$= P(S_t=s \\mid I_t=i, S_{t-1}=s') P(I_t=i \\mid I_{t-1}=j)$$\n    The first term, $P(S_t=s \\mid S_{t-1}=s', I_t=i)$, is defined by the irreversible editing process with probabilities $p_{\\mathrm{off}}$ and $p_{\\mathrm{on}}$. The second term is the input Markov chain transition, $A_I[j,i]$.\n\n3.  **Observation (Emission) Probability**: $P(O_t=o \\mid X_t=(s,i))$.\n    The observation $O_t$ depends only on the memory state $S_t$:\n    $$P(O_t=o \\mid X_t=(s,i)) = P(O_t=o \\mid S_t=s)$$\n    This is defined by the sequencing noise parameters $q$ and $r$.\n\n**Derivation of the Forward-Backward Algorithm**\n\nOur goal is to compute $P(I_t=i \\mid O_{1:T})$. We first find the joint smoothed posterior $P(S_t=s, I_t=i \\mid O_{1:T})$ and then marginalize over $S_t$.\n\n**Forward Recursion**\nLet $\\alpha_t(s,i) \\equiv P(S_t=s, I_t=i, O_{1:t})$ be the joint probability of the latent state at time $t$ and the observation sequence up to time $t$.\n\n-   **Initialization ($t=1$):**\n    By the product rule, and using the model definitions:\n    $$\\alpha_1(s,i) = P(O_1 \\mid S_1=s, I_1=i) P(S_1=s, I_1=i) = P(O_1 \\mid S_1=s) P(S_1=s \\mid S_0=0, I_1=i) \\pi_I[i]$$\n\n-   **Recursion ($t > 1$):**\n    We express $\\alpha_t(s,i)$ in terms of $\\alpha_{t-1}$.\n    $$\\alpha_t(s,i) = P(S_t=s, I_t=i, O_{1:t}) = P(O_t \\mid S_t=s, I_t=i, O_{1:t-1}) P(S_t=s, I_t=i, O_{1:t-1})$$\n    By conditional independence, $P(O_t \\mid S_t=s, I_t=i, O_{1:t-1}) = P(O_t \\mid S_t=s)$. For the second term, we marginalize over the state at $t-1$:\n    $$P(S_t=s, I_t=i, O_{1:t-1}) = \\sum_{s' \\in \\{0,1\\}} \\sum_{j \\in \\{0,1\\}} P(S_t=s, I_t=i, S_{t-1}=s', I_{t-1}=j, O_{1:t-1})$$\n    $$= \\sum_{s'} \\sum_{j} P(S_t=s, I_t=i \\mid S_{t-1}=s', I_{t-1}=j, O_{1:t-1}) P(S_{t-1}=s', I_{t-1}=j, O_{1:t-1})$$\n    The state transition is independent of past observations, so $P(\\dots \\mid S_{t-1}, I_{t-1}, O_{1:t-1}) = P(S_t=s, I_t=i \\mid S_{t-1}=s', I_{t-1}=j)$. The last term is precisely $\\alpha_{t-1}(s',j)$.\n    Combining these gives the recursion:\n    $$\\alpha_t(s,i) = P(O_t \\mid S_t=s) \\sum_{s'} \\sum_{j} P(S_t=s \\mid S_{t-1}=s', I_t=i) P(I_t=i \\mid I_{t-1}=j) \\alpha_{t-1}(s',j)$$\n\n**Backward Recursion**\nLet $\\beta_t(s,i) \\equiv P(O_{t+1:T} \\mid S_t=s, I_t=i)$ be the probability of the future observation sequence given the state at time $t$.\n\n-   **Initialization ($t=T$):**\n    The future observation sequence is empty, an event of probability $1$.\n    $$\\beta_T(s,i) = 1 \\quad \\forall s,i$$\n\n-   **Recursion ($t < T$):**\n    We express $\\beta_t(s,i)$ in terms of $\\beta_{t+1}$ by marginalizing over the state at $t+1$:\n    $$\\beta_t(s,i) = \\sum_{s''} \\sum_{k} P(O_{t+1:T}, S_{t+1}=s'', I_{t+1}=k \\mid S_t=s, I_t=i)$$\n    $$= \\sum_{s''} \\sum_{k} P(O_{t+1:T} \\mid S_{t+1}=s'', I_{t+1}=k, S_t=s, I_t=i) P(S_{t+1}=s'', I_{t+1}=k \\mid S_t=s, I_t=i)$$\n    From the model's Markov properties, future observations are independent of past states given the current state.\n    $$P(O_{t+1:T} \\mid \\dots) = P(O_{t+1} \\mid S_{t+1}=s'') P(O_{t+2:T} \\mid S_{t+1}=s'', I_{t+1}=k) = P(O_{t+1} \\mid S_{t+1}=s'') \\beta_{t+1}(s'',k)$$\n    The second term is the state transition probability. Combining yields:\n    $$\\beta_t(s,i) = \\sum_{s''} \\sum_{k} \\beta_{t+1}(s'',k) P(O_{t+1} \\mid S_{t+1}=s'') P(S_{t+1}=s'' \\mid S_t=s, I_{t+1}=k) P(I_{t+1}=k \\mid I_t=i)$$\n\n**Smoothed Posterior Calculation**\nThe smoothed posterior of the joint latent state, $\\gamma_t(s,i) = P(S_t=s, I_t=i \\mid O_{1:T})$, is given by:\n$$\\gamma_t(s,i) = \\frac{P(S_t=s, I_t=i, O_{1:T})}{P(O_{1:T})} = \\frac{P(O_{t+1:T} \\mid S_t=s, I_t=i, O_{1:t}) P(S_t=s, I_t=i, O_{1:t})}{P(O_{1:T})}$$\nUsing conditional independence, $P(O_{t+1:T} \\mid \\dots) = P(O_{t+1:T} \\mid S_t=s, I_t=i) = \\beta_t(s,i)$. The other term is $\\alpha_t(s,i)$.\nThus, $P(S_t=s, I_t=i, O_{1:T}) = \\alpha_t(s,i) \\beta_t(s,i)$. The likelihood of the evidence is $P(O_{1:T}) = \\sum_{s'} \\sum_{j'} \\alpha_T(s',j')$.\nThe posterior is:\n$$\\gamma_t(s,i) = \\frac{\\alpha_t(s,i) \\beta_t(s,i)}{\\sum_{s'} \\sum_{j'} \\alpha_t(s', j') \\beta_t(s', j')}$$\nFinally, we obtain the marginal posterior for the input by summing over the memory states:\n$$P(I_t=i \\mid O_{1:T}) = \\sum_{s \\in \\{0,1\\}} \\gamma_t(s,i)$$\nThe decoded input is then $\\hat{I}_t = \\arg\\max_{i \\in \\{0,1\\}} P(I_t=i \\mid O_{1:T})$.\n\n**Numerical Stability**\nDirect computation of $\\alpha_t$ and $\\beta_t$ is prone to numerical underflow as probabilities are multiplied over time. We introduce scaling factors. Define scaled forward probabilities $\\hat{\\alpha}_t(s,i) = P(S_t=s, I_t=i \\mid O_{1:t})$. The recursion is maintained, but after each step, the values are normalized. Let $c_t = P(O_t \\mid O_{1:t-1})$ be the scaling factor.\n-   **Forward pass**: Compute temporary unscaled $\\alpha'_t(s,i)$ using the recursion with $\\hat{\\alpha}_{t-1}$, then find $c_t = \\sum_{s,i} \\alpha'_t(s,i)$, and finally set $\\hat{\\alpha}_t(s,i) = \\alpha'_t(s,i) / c_t$.\n-   **Backward pass**: A scaled backward variable $\\hat{\\beta}_t(s,i)$ is computed using recursion $\\hat{\\beta}_t(s,i) = \\frac{1}{c_{t+1}} \\sum_{s'',k} \\dots \\hat{\\beta}_{t+1}(s'',k)$.\nThe smoothed posterior is then proportional to the product of the scaled quantities: $\\gamma_t(s,i) \\propto \\hat{\\alpha}_t(s,i) \\hat{\\beta}_t(s,i)$. Normalizing this product at each time step $t$ gives the correct smoothed distribution.\nThis procedure is a standard and numerically robust implementation of the forward-backward algorithm.", "answer": "```python\nimport numpy as np\n\ndef run_fwd_bwd(T, A_I, pi_I, p_off, p_on, q, r, O):\n    \"\"\"\n    Implements the scaled forward-backward algorithm to decode the hidden input sequence.\n\n    Args:\n        T (int): Length of the sequence.\n        A_I (np.ndarray): Input transition matrix, shape (2, 2).\n        pi_I (np.ndarray): Initial input distribution, shape (2,).\n        p_off (float): Edit probability when input is off.\n        p_on (float): Edit probability when input is on.\n        q (float): True positive rate for observation (P(O=1|S=1)).\n        r (float): False positive rate for observation (P(O=1|S=0)).\n        O (list or np.ndarray): Observation sequence of length T.\n\n    Returns:\n        list: The decoded input sequence.\n    \"\"\"\n    # State representation: S_t in {0, 1}, I_t in {0, 1}\n    # We use 2x2 matrices for alpha, beta, where rows are S_t and cols are I_t.\n\n    # 1. Pre-compute model parameter matrices\n    # Memory transition tensor A_S[i, s_prev, s_curr] = P(S_t=s_curr | S_{t-1}=s_prev, I_t=i)\n    A_S = np.zeros((2, 2, 2))\n    A_S[0, 0, 0] = 1 - p_off  # I=0, S_prev=0 -> S_curr=0\n    A_S[0, 0, 1] = p_off      # I=0, S_prev=0 -> S_curr=1\n    A_S[0, 1, 0] = 0          # I=0, S_prev=1 -> S_curr=0 (irreversible)\n    A_S[0, 1, 1] = 1          # I=0, S_prev=1 -> S_curr=1\n    A_S[1, 0, 0] = 1 - p_on   # I=1, S_prev=0 -> S_curr=0\n    A_S[1, 0, 1] = p_on       # I=1, S_prev=0 -> S_curr=1\n    A_S[1, 1, 0] = 0          # I=1, S_prev=1 -> S_curr=0 (irreversible)\n    A_S[1, 1, 1] = 1          # I=1, S_prev=1 -> S_curr=1\n\n    # Observation matrix B[s, o] = P(O_t=o | S_t=s)\n    B = np.array([[1 - r, r], [1 - q, q]])\n\n    # 2. Forward pass (filtering)\n    alpha = np.zeros((T, 2, 2))  # alpha[t, s, i]\n    c = np.zeros(T)  # scaling factors c[t] = P(O_{t+1} | O_{1:t})\n\n    # Initialization (t=0, corresponding to time step 1)\n    # S_0 is fixed to 0\n    alpha_unscaled = np.zeros((2, 2))\n    o1 = O[0]\n    for s in range(2):\n        for i in range(2):\n            alpha_unscaled[s, i] = B[s, o1] * A_S[i, 0, s] * pi_I[i]\n    \n    c[0] = np.sum(alpha_unscaled)\n    if c[0] > 0:\n        alpha[0] = alpha_unscaled / c[0]\n\n    # Recursion (t=1 to T-1)\n    for t in range(1, T):\n        alpha_unscaled_t = np.zeros((2, 2))\n        ot = O[t]\n        for s_curr in range(2):\n            for i in range(2):\n                sum_prev = 0\n                for s_prev in range(2):\n                    for j in range(2):\n                        sum_prev += alpha[t - 1, s_prev, j] * A_I[j, i] * A_S[i, s_prev, s_curr]\n                alpha_unscaled_t[s_curr, i] = B[s_curr, ot] * sum_prev\n        \n        c[t] = np.sum(alpha_unscaled_t)\n        if c[t] > 0:\n            alpha[t] = alpha_unscaled_t / c[t]\n\n    # 3. Backward pass\n    beta = np.zeros((T, 2, 2)) # beta[t, s, i]\n    \n    # Initialization (t=T-1, corresponding to time step T)\n    beta[T - 1] = 1.0\n\n    # Recursion (t=T-2 down to 0)\n    for t in range(T - 2, -1, -1):\n        beta_unscaled_t = np.zeros((2, 2))\n        ot_plus_1 = O[t + 1]\n        for s_prev in range(2):\n            for j in range(2):\n                sum_next = 0\n                for s_curr in range(2):\n                    for i in range(2):\n                        sum_next += beta[t + 1, s_curr, i] * B[s_curr, ot_plus_1] * A_S[i, s_prev, s_curr] * A_I[j, i]\n                beta_unscaled_t[s_prev, j] = sum_next\n        \n        if c[t+1] > 0:\n            beta[t] = beta_unscaled_t / c[t+1]\n\n    # 4. Smoothing and Decoding\n    decoded_I = []\n    for t in range(T):\n        gamma_unscaled = alpha[t] * beta[t]\n        gamma_t = gamma_unscaled / np.sum(gamma_unscaled) # Normalization\n        \n        # Marginalize over S_t to get P(I_t | O_{1:T})\n        p_I_t = np.sum(gamma_t, axis=0) # Sum over s\n        \n        # Decode\n        decoded_I.append(int(np.argmax(p_I_t)))\n        \n    return decoded_I\n\n\ndef solve():\n    \"\"\"\n    Defines the test suites and runs the forward-backward algorithm for each case.\n    \"\"\"\n    test_cases = [\n        {\n            \"T\": 6,\n            \"A_I\": np.array([[0.95, 0.05], [0.10, 0.90]]),\n            \"pi_I\": np.array([0.90, 0.10]),\n            \"p_off\": 0.01, \"p_on\": 0.20,\n            \"q\": 0.95, \"r\": 0.05,\n            \"O\": [0, 0, 1, 1, 1, 1]\n        },\n        {\n            \"T\": 4,\n            \"A_I\": np.array([[0.98, 0.02], [0.02, 0.98]]),\n            \"pi_I\": np.array([0.95, 0.05]),\n            \"p_off\": 0.005, \"p_on\": 0.15,\n            \"q\": 0.98, \"r\": 0.02,\n            \"O\": [0, 0, 0, 0]\n        },\n        {\n            \"T\": 5,\n            \"A_I\": np.array([[0.90, 0.10], [0.10, 0.90]]),\n            \"pi_I\": np.array([0.50, 0.50]),\n            \"p_off\": 0.001, \"p_on\": 0.30,\n            \"q\": 0.95, \"r\": 0.05,\n            \"O\": [1, 1, 1, 1, 1]\n        },\n        {\n            \"T\": 5,\n            \"A_I\": np.array([[0.90, 0.10], [0.10, 0.90]]),\n            \"pi_I\": np.array([0.80, 0.20]),\n            \"p_off\": 0.05, \"p_on\": 0.10,\n            \"q\": 0.90, \"r\": 0.10,\n            \"O\": [0, 1, 0, 1, 0]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        decoded_seq = run_fwd_bwd(\n            case[\"T\"], case[\"A_I\"], case[\"pi_I\"],\n            case[\"p_off\"], case[\"p_on\"],\n            case[\"q\"], case[\"r\"], case[\"O\"]\n        )\n        results.append(decoded_seq)\n\n    # Format the output as a string representation of a list of lists.\n    # The default str() for lists includes spaces. We will remove them to be concise.\n    formatted_results = ','.join(str(res).replace(' ', '') for res in results)\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```", "id": "2734309"}]}