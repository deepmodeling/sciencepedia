{"hands_on_practices": [{"introduction": "Before we can design complex computational circuits, we must first accurately characterize their basic components. This exercise [@problem_id:2723242] provides hands-on practice fitting the Hill function, a cornerstone of quantitative biology, to synthetic data. By using nonlinear least squares to estimate the key parameter of cooperativity, the Hill coefficient $n$, you will develop essential skills in parameter inference and learn to quantify the uncertainty of your estimates.", "problem": "You are given a modeling task arising from the analysis of inducible promoters used to implement complex logic and computation in mammalian cells. In these systems, the steady-state reporter fluorescence under an input concentration $u$ is commonly modeled by a Hill-type input-output function that has been well tested in gene regulation studies. Assume the following conditions: (i) the steady-state mapping is instantaneous relative to the sampling interval (so dynamic transients can be neglected for the purposes of parameter estimation from the provided samples), (ii) the response follows a Hill function with known gain and half-activation parameters, and (iii) the measurement noise is independent and identically distributed Gaussian. Your task is to infer the Hill coefficient $n$ and compute a two-sided $95\\%$ confidence interval for $n$ using nonlinear least squares for several datasets.\n\nFundamental base to be used: the Central Dogma of molecular biology establishes that gene expression can be modeled as a function of transcriptional input, and promoter occupancy models yield Hill-type steady-state response functions; under independent identically distributed Gaussian noise, maximum likelihood estimation reduces to nonlinear least squares; asymptotic confidence intervals for parameters estimated by nonlinear least squares can be computed from the Jacobian-based covariance approximation and Student’s $t$ distribution.\n\nModel definition. For each data point $i \\in \\{1,\\dots,N\\}$ with input $u_i$, the observed fluorescence $y_i$ is modeled as\n$$\ny_i \\;=\\; f(u_i;n) + \\varepsilon_i, \\quad f(u;n) \\;=\\; \\alpha \\,\\frac{u^{\\,n}}{K^{\\,n} + u^{\\,n}},\n$$\nwhere $\\alpha$ and $K$ are known positive constants, $n$ is the unknown Hill coefficient to be estimated, and the noise satisfies $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ with an unknown variance $\\sigma^2$ common to all observations. All computations are unitless for this exercise.\n\nEstimation objective. Given one dataset $(u_i,y_i)_{i=1}^N$ together with known $\\alpha$ and $K$, estimate $\\hat{n}$ by minimizing the sum of squared residuals\n$$\nS(n) \\;=\\; \\sum_{i=1}^N \\left(y_i - f(u_i;n)\\right)^2.\n$$\nLet $r_i(n) = y_i - f(u_i;n)$ denote the residuals at $n$, and let $J$ denote the Jacobian of the residual vector with respect to $n$ evaluated at $\\hat{n}$, i.e., the $N \\times 1$ vector with entries\n$$\nJ_i \\;=\\; \\frac{\\partial r_i}{\\partial n}\\bigg|_{n=\\hat{n}} \\;=\\; -\\,\\frac{\\partial f(u_i;n)}{\\partial n}\\bigg|_{n=\\hat{n}}.\n$$\nDefine the residual sum of squares at the estimate as $\\mathrm{SSR} = \\sum_{i=1}^N r_i(\\hat{n})^2$, the degrees of freedom as $\\nu = N - p$ with $p=1$, and the unbiased residual variance estimate as $s^2 = \\mathrm{SSR}/\\nu$. Under standard regularity conditions, the approximate variance of $\\hat{n}$ is given by\n$$\n\\mathrm{Var}(\\hat{n}) \\;\\approx\\; s^2\\,\\left(J^\\top J\\right)^{-1},\n$$\nand a two-sided $95\\%$ confidence interval is\n$$\n\\hat{n} \\,\\pm\\, t_{0.975,\\nu}\\,\\sqrt{\\mathrm{Var}(\\hat{n})},\n$$\nwhere $t_{0.975,\\nu}$ is the $0.975$ quantile of the Student distribution with $\\nu$ degrees of freedom.\n\nAnalytic derivative. You must implement the analytic derivative $\\partial f/\\partial n$ to construct $J$:\n$$\n\\frac{\\partial f(u;n)}{\\partial n} \\;=\\; \\alpha \\,\\frac{u^{\\,n} K^{\\,n}\\,\\ln\\!\\big(u/K\\big)}{\\left(K^{\\,n} + u^{\\,n}\\right)^{2}},\n$$\nwhere $\\ln$ denotes the natural logarithm.\n\nTest suite. For each test case below, you are provided known $\\alpha$, known $K$, a vector of inputs $u_1,\\dots,u_N$, a latent true Hill coefficient $n_{\\mathrm{true}}$ (used only to synthesize $y_i$; your algorithm must not use $n_{\\mathrm{true}}$), and an additive noise sequence $\\varepsilon_1,\\dots,\\varepsilon_N$. The observed data are defined by $y_i = f(u_i;n_{\\mathrm{true}}) + \\varepsilon_i$. You must compute the estimate $\\hat{n}$ and the two-sided $95\\%$ confidence interval $[\\mathrm{lo},\\mathrm{hi}]$ for each dataset.\n\n- Case A (well-conditioned across a broad input range):\n  - $\\alpha = 1000.0$, $K = 50.0$, $n_{\\mathrm{true}} = 2.5$.\n  - Inputs $u = [5.0,10.0,20.0,30.0,50.0,80.0,120.0,200.0]$.\n  - Noise $\\varepsilon = [10.0,-5.0,8.0,-12.0,15.0,-7.0,9.0,-6.0]$.\n- Case B (inputs mostly below half-activation, challenging identifiability):\n  - $\\alpha = 800.0$, $K = 100.0$, $n_{\\mathrm{true}} = 3.0$.\n  - Inputs $u = [1.0,2.0,5.0,10.0,20.0]$.\n  - Noise $\\varepsilon = [0.5,-0.7,0.9,-1.1,1.3]$.\n- Case C (includes a point at $u=K$ yielding a locally uninformative sample for $n$):\n  - $\\alpha = 1200.0$, $K = 50.0$, $n_{\\mathrm{true}} = 2.0$.\n  - Inputs $u = [40.0,45.0,50.0,55.0,60.0,80.0]$.\n  - Noise $\\varepsilon = [-5.0,7.0,0.0,-8.0,6.0,-4.0]$.\n- Case D (high dynamic range, moderate cooperativity):\n  - $\\alpha = 1500.0$, $K = 30.0$, $n_{\\mathrm{true}} = 1.2$.\n  - Inputs $u = [2.0,5.0,10.0,20.0,30.0,60.0,120.0,300.0]$.\n  - Noise $\\varepsilon = [-2.0,3.0,-1.5,2.5,-3.0,4.0,-2.5,3.5]$.\n\nImplementation requirements.\n- For each case, construct $y_i$ exactly as $y_i = \\alpha \\, u_i^{\\,n_{\\mathrm{true}}}/(K^{\\,n_{\\mathrm{true}}} + u_i^{\\,n_{\\mathrm{true}}}) + \\varepsilon_i$ using the provided values.\n- Estimate $\\hat{n}$ by minimizing $S(n)$ via nonlinear least squares, treating $n$ as a single scalar decision variable. Use the analytic residual Jacobian based on the derivative above.\n- Compute the two-sided $95\\%$ confidence interval using the covariance approximation $s^2 (J^\\top J)^{-1}$ and the Student quantile $t_{0.975,\\nu}$ with $\\nu=N-1$.\n- To ensure numerical realism, constrain the search to $n \\in [0.1,6.0]$.\n- If $J^\\top J$ is numerically zero (e.g., below machine epsilon), return the confidence interval bounds as $\\mathrm{NaN}$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of three floating-point numbers $[\\hat{n},\\mathrm{lo},\\mathrm{hi}]$. Round every printed floating-point value to exactly $6$ decimal places. For example, the overall format must be like $[[\\hat{n}_A,\\mathrm{lo}_A,\\mathrm{hi}_A],[\\hat{n}_B,\\mathrm{lo}_B,\\mathrm{hi}_B],\\dots]$ with no spaces anywhere in the line.", "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically grounded in established principles of molecular biology and statistical inference, well-posed with a clear objective and sufficient data, and formulated with objective and precise language. The task is to perform parameter estimation for a Hill-type model from synthetic data, which is a standard and meaningful problem in quantitative biology. We proceed with the solution.\n\nThe core of the problem is to estimate the Hill coefficient, denoted by $n$, from a set of noisy measurements. The relationship between the input concentration $u$ and the measured output $y$ is given by the Hill function with additive Gaussian noise. For each of the $N$ data points in a given dataset, the model is:\n$$\ny_i = f(u_i; n) + \\varepsilon_i, \\quad \\text{for } i = 1, \\dots, N\n$$\nwhere the noise terms $\\varepsilon_i$ are independent and identically distributed draws from a normal distribution with mean $0$ and unknown variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. The function $f(u; n)$ is the deterministic Hill model:\n$$\nf(u; n) = \\alpha \\frac{u^n}{K^n + u^n}\n$$\nHere, the parameters $\\alpha$ (maximum response) and $K$ (half-activation constant) are known. The only unknown parameter to be estimated is the Hill coefficient $n$.\n\nThe estimation is performed by the method of nonlinear least squares (NLLS). This method seeks the value of $n$, denoted $\\hat{n}$, that minimizes the sum of squared residuals $S(n)$:\n$$\n\\hat{n} = \\arg\\min_{n} S(n) = \\arg\\min_{n} \\sum_{i=1}^{N} \\left( y_i - f(u_i; n) \\right)^2\n$$\nUnder the assumption of i.i.d. Gaussian noise, the NLLS estimate is equivalent to the maximum likelihood estimate. To find $\\hat{n}$, a numerical optimization algorithm is employed. We utilize the Levenberg-Marquardt algorithm, as implemented in `scipy.optimize.least_squares`. This algorithm iteratively refines an initial guess for $n$ to minimize the sum of squares. The search for $n$ is constrained to the physically realistic interval $[0.1, 6.0]$.\n\nTo accelerate convergence and ensure accuracy, we provide the optimizer with the analytic Jacobian of the residual vector. The residual vector is $r(n)$ with components $r_i(n) = y_i - f(u_i; n)$. The Jacobian is an $N \\times 1$ matrix $J$ whose entries are the derivatives of the residuals with respect to the single parameter $n$:\n$$\nJ_i(n) = \\frac{\\partial r_i(n)}{\\partial n} = - \\frac{\\partial f(u_i; n)}{\\partial n}\n$$\nThe problem provides the explicit formula for this derivative:\n$$\n\\frac{\\partial f(u; n)}{\\partial n} = \\alpha \\frac{u^n K^n \\ln(u/K)}{(K^n + u^n)^2}\n$$\nThis exact Jacobian is computed at each iteration of the optimization.\n\nOnce the optimal estimate $\\hat{n}$ is found, we proceed to compute a two-sided $95\\%$ confidence interval for it. This interval quantifies the uncertainty in our estimate. The theoretical basis for this calculation is the asymptotic distribution of the NLLS estimator. The approximate variance of $\\hat{n}$ is given by:\n$$\n\\mathrm{Var}(\\hat{n}) \\approx s^2 \\left( J^\\top J \\right)^{-1}\n$$\nHere, $J$ is the Jacobian evaluated at the final estimate $\\hat{n}$. The term $s^2$ is the unbiased estimate of the noise variance $\\sigma^2$, calculated from the residual sum of squares (SSR) at the solution:\n$$\ns^2 = \\frac{\\mathrm{SSR}}{\\nu} = \\frac{1}{N-p} \\sum_{i=1}^{N} \\left( y_i - f(u_i; \\hat{n}) \\right)^2\n$$\nThe degrees of freedom are $\\nu = N - p$, where $N$ is the number of data points and $p$ is the number of estimated parameters. In this problem, we estimate only one parameter, $n$, so $p=1$ and $\\nu=N-1$.\n\nThe term $J^\\top J$ is a $1 \\times 1$ matrix (a scalar) representing the squared Euclidean norm of the Jacobian vector at the solution. If $J^\\top J$ is numerically zero (for instance, if all $u_i = K$), the variance is undefined, and a confidence interval cannot be computed.\n\nThe confidence interval is then constructed using the quantiles of the Student's $t$-distribution, which is appropriate when the sample size is small and the population variance $\\sigma^2$ is estimated by $s^2$. The two-sided $95\\%$ confidence interval for $n$ is:\n$$\n\\left[ \\hat{n} - t_{0.975, \\nu} \\sqrt{\\mathrm{Var}(\\hat{n})}, \\quad \\hat{n} + t_{0.975, \\nu} \\sqrt{\\mathrm{Var}(\\hat{n})} \\right]\n$$\nwhere $t_{0.975, \\nu}$ is the upper $0.025$ critical value of the Student's $t$-distribution with $\\nu$ degrees of freedom.\n\nThe procedure is implemented as a Python script. For each test case, we first synthesize the observation data $y_i$ using the provided true parameters $n_{\\text{true}}$. Then, we apply the NLLS optimization to estimate $\\hat{n}$ from $(u_i,y_i)$ and subsequently compute the confidence interval as described above. The results, including the estimate $\\hat{n}$ and the lower and upper bounds of the confidence interval, are collected and formatted according to the specified output requirements.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import least_squares\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to solve the parameter estimation problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"alpha\": 1000.0, \"K\": 50.0, \"n_true\": 2.5,\n            \"u\": np.array([5.0, 10.0, 20.0, 30.0, 50.0, 80.0, 120.0, 200.0]),\n            \"eps\": np.array([10.0, -5.0, 8.0, -12.0, 15.0, -7.0, 9.0, -6.0]),\n        },\n        {\n            \"alpha\": 800.0, \"K\": 100.0, \"n_true\": 3.0,\n            \"u\": np.array([1.0, 2.0, 5.0, 10.0, 20.0]),\n            \"eps\": np.array([0.5, -0.7, 0.9, -1.1, 1.3]),\n        },\n        {\n            \"alpha\": 1200.0, \"K\": 50.0, \"n_true\": 2.0,\n            \"u\": np.array([40.0, 45.0, 50.0, 55.0, 60.0, 80.0]),\n            \"eps\": np.array([-5.0, 7.0, 0.0, -8.0, 6.0, -4.0]),\n        },\n        {\n            \"alpha\": 1500.0, \"K\": 30.0, \"n_true\": 1.2,\n            \"u\": np.array([2.0, 5.0, 10.0, 20.0, 30.0, 60.0, 120.0, 300.0]),\n            \"eps\": np.array([-2.0, 3.0, -1.5, 2.5, -3.0, 4.0, -2.5, 3.5]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = estimate_hill_params(case)\n        results.append(result)\n\n    # Format the output as specified: [[n1,lo1,hi1],[n2,lo2,hi2],...] with no spaces.\n    formatted_results = []\n    for res_tuple in results:\n        n_hat, lo, hi = res_tuple\n        if np.isnan(lo):\n            formatted_entry = f\"[{n_hat:.6f},nan,nan]\"\n        else:\n            formatted_entry = f\"[{n_hat:.6f},{lo:.6f},{hi:.6f}]\"\n        formatted_results.append(formatted_entry)\n\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef estimate_hill_params(case):\n    \"\"\"\n    Performs NLLS estimation and computes confidence interval for a single case.\n    \"\"\"\n    alpha = case[\"alpha\"]\n    K = case[\"K\"]\n    n_true = case[\"n_true\"]\n    u = case[\"u\"]\n    eps = case[\"eps\"]\n\n    # --- 1. Define model and its derivative ---\n    def hill_model(n, u_vals, alpha_val, K_val):\n        # Numerically stable version for small u\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = u_vals / K_val\n            ratio_n = ratio**n\n        return alpha_val * ratio_n / (1.0 + ratio_n)\n\n    def hill_jacobian(n, u_vals, alpha_val, K_val):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = u_vals / K_val\n            # Handle u=K case where log(1)=0\n            log_ratio = np.log(ratio, where=ratio > 0)\n            log_ratio[ratio <= 0] = -np.inf\n\n            Kn = K_val**n\n            un = u_vals**n\n            \n            numerator = alpha_val * un * Kn * log_ratio\n            denominator = (Kn + un)**2\n            # Handle potential division by zero if Kn + un is zero\n            deriv = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n        return deriv\n\n    # --- 2. Synthesize data ---\n    y_obs = hill_model(n_true, u, alpha, K) + eps\n\n    # --- 3. Define residual and Jacobian functions for the optimizer ---\n    def residuals(n_vec):\n        n = n_vec[0]\n        return y_obs - hill_model(n, u, alpha, K)\n\n    def residuals_jacobian(n_vec):\n        n = n_vec[0]\n        # Jacobian of residual r = y - f(n) is -df/dn\n        jac = -hill_jacobian(n, u, alpha, K)\n        return jac.reshape(-1, 1) # Reshape to N x 1 matrix\n\n    # --- 4. Perform Nonlinear Least Squares ---\n    n_initial = [2.0]  # Initial guess for n\n    bounds = (0.1, 6.0)\n    result = least_squares(residuals, n_initial, jac=residuals_jacobian, bounds=bounds, method='trf')\n    \n    n_hat = result.x[0]\n\n    # --- 5. Compute Confidence Interval ---\n    N = len(u)\n    p = 1  # Number of parameters\n    nu = N - p  # Degrees of freedom\n\n    J = result.jac # This is already the Jacobian of residuals at the solution\n    JtJ = J.T @ J\n\n    # Check for singularity\n    if JtJ[0, 0] < np.finfo(float).eps:\n        return [n_hat, np.nan, np.nan]\n\n    SSR = np.sum(result.fun**2)\n    s2 = SSR / nu  # Unbiased estimate of error variance\n    \n    try:\n        var_n_hat = s2 * np.linalg.inv(JtJ)\n        se_n_hat = np.sqrt(var_n_hat[0, 0])\n    except (np.linalg.LinAlgError, ValueError):\n        return [n_hat, np.nan, np.nan]\n\n    t_crit = t.ppf(0.975, df=nu)\n    ci_half_width = t_crit * se_n_hat\n\n    ci_lo = n_hat - ci_half_width\n    ci_hi = n_hat + ci_half_width\n\n    return [n_hat, ci_lo, ci_hi]\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2723242"}, {"introduction": "Synthetic circuits operate within the complex environment of the host cell, where fluctuations in resources can compromise function. In this practice [@problem_id:2723261], you will analyze the robustness of a genetic AND gate to changes in the cell's metabolic state, modeled as a global resource dependency scalar $\\rho$. You will derive and compute key robustness metrics, learning how to dissect a circuit's performance and identify potential failure points, a vital skill for engineering reliable biological systems.", "problem": "Consider a synthetic gene circuit in mammalian cells that computes an AND logic on two transcription factor (TF) inputs. Use the Central Dogma (deoxyribonucleic acid to ribonucleic acid to protein) and mass-action Ordinary Differential Equations (ODEs) with Hill-type promoter activation as the fundamental base. The cell’s metabolic state provides a dimensionless resource availability scalar $\\rho \\in \\mathbb{R}_{>0}$ that multiplicatively scales both transcription and translation rates for all genes in the circuit.\n\nThe circuit consists of two TFs, $A$ and $B$, and a reporter protein $P$. The reporter is transcribed into reporter messenger ribonucleic acid (mRNA) $m$ and then translated into protein $p$. Assume the following:\n\n- TF production is regulated upstream by external inducers and can be summarized as constant effective synthesis rates $s_A$ and $s_B$ that already encode inducer effects. Both $A$ and $B$ obey linear production and first-order degradation dynamics with resource scaling on production.\n- The reporter promoter integrates TFs via an AND logic implemented as the product of two Hill activation functions. Reporter transcription and translation are both resource-scaled. All degradation processes are first-order and are not resource-scaled.\n\nModel the dynamics with the following ODEs, where all variables and parameters are nonnegative and dimensionless unless explicitly stated:\n\n- TFs:\n$$\n\\frac{dA}{dt} = \\rho\\, s_A - \\gamma_A A, \\quad \\frac{dB}{dt} = \\rho\\, s_B - \\gamma_B B.\n$$\n\n- Reporter:\n$$\n\\frac{dm}{dt} = \\rho\\, k_{\\mathrm{tx}}\\, f(A,B) - \\gamma_m m, \\quad \\frac{dp}{dt} = \\rho\\, k_{\\mathrm{tl}}\\, m - \\gamma_p p.\n$$\n\nThe AND gate transcriptional input is\n$$\nf(A,B) = H_A(A)\\, H_B(B),\n$$\nwith Hill activations\n$$\nH_A(A) = \\frac{A^{n_A}}{K_A^{n_A} + A^{n_A}}, \\quad H_B(B) = \\frac{B^{n_B}}{K_B^{n_B} + B^{n_B}}.\n$$\n\nTask. Starting from these definitions alone, derive the steady-state reporter protein level $p^\\ast(\\rho)$ and the local logarithmic sensitivity (elasticity) of the steady-state output to $\\rho$,\n$$\nE(\\rho) \\equiv \\frac{d \\log p^\\ast(\\rho)}{d \\log \\rho}.\n$$\nUse first principles and the chain rule; do not assume any “shortcut” formulas beyond the ODEs and the Hill functions stated above. In addition, to quantify robustness of the logic computation itself (separately from the trivial overall scaling of the output stage), define the logic robustness metric over a specified resource interval,\n$$\nJ_{\\mathrm{logic}} \\equiv \\max_{\\rho \\in [\\rho_{\\min}, \\rho_{\\max}]} \\left| \\frac{f\\big(A^\\ast(\\rho), B^\\ast(\\rho)\\big)}{f\\big(A^\\ast(\\rho_0), B^\\ast(\\rho_0)\\big)} - 1 \\right|,\n$$\nwhere $A^\\ast(\\rho)$ and $B^\\ast(\\rho)$ are the TF steady states.\n\nFor numerical evaluation, use the following shared constants across all test cases:\n- $k_{\\mathrm{tx}} = 50.0$, $k_{\\mathrm{tl}} = 5.0$, $\\gamma_m = 1.0$, $\\gamma_p = 0.2$,\n- $\\rho_0 = 1.0$, $\\rho_{\\min} = 0.3$, $\\rho_{\\max} = 3.0$,\n- sampling on $\\rho$ must be performed on a logarithmic grid with $N_\\rho = 10001$ points (inclusive of endpoints),\n- robustness thresholds $E_{\\mathrm{thr}} = 3.2$ and $J_{\\mathrm{thr}} = 0.2$.\n\nDefine a robustness flag as\n$$\n\\mathrm{robust} = \\begin{cases}\n1 & \\text{if } E(\\rho_0) \\le E_{\\mathrm{thr}} \\text{ and } J_{\\mathrm{logic}} \\le J_{\\mathrm{thr}},\\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$\n\nTest suite. Evaluate the following five parameter sets, each provided as $(n_A, K_A, s_A, \\gamma_A, n_B, K_B, s_B, \\gamma_B)$:\n\n- Case $1$: $(2.0, 1.0, 10.0, 5.0, 2.0, 1.5, 12.0, 6.0)$,\n- Case $2$: $(3.0, 2.0, 100.0, 5.0, 2.0, 2.0, 90.0, 3.0)$,\n- Case $3$: $(2.0, 1.0, 1.0, 5.0, 2.0, 1.0, 1.0, 5.0)$,\n- Case $4$: $(1.0, 5.0, 20.0, 1.0, 4.0, 1.0, 1.0, 2.0)$,\n- Case $5$: $(2.0, 3.0, 3.0, 1.0, 2.0, 2.0, 2.0, 1.0)$.\n\nYour program must, for each test case, compute:\n- the elasticity $E(\\rho_0)$,\n- the logic robustness $J_{\\mathrm{logic}}$,\n- the robustness flag as an integer $0$ or $1$.\n\nFinal output format. Your program should produce a single line of output containing a Python-style list of lists\n$$\n\\big[\\,[E_1, J_1, R_1], [E_2, J_2, R_2], \\dots, [E_5, J_5, R_5]\\,\\big],\n$$\nwhere $E_i$ and $J_i$ are floats rounded to six decimal places, and $R_i$ is an integer. No other text should be printed. All computations are dimensionless; no physical units are required and none should be included in the output.", "solution": "The problem posed is a well-defined exercise in the analysis of ordinary differential equation (ODE) models, a standard practice in systems biology. It is scientifically grounded, mathematically consistent, and presents a non-trivial but solvable task. We shall therefore proceed with its solution. The objective is to derive the steady-state protein concentration $p^\\ast$ as a function of the resource availability scalar $\\rho$, derive the corresponding logarithmic sensitivity $E(\\rho)$, and then perform numerical evaluations for specified parameter sets.\n\nFirst, we determine the steady-state concentrations of all species by setting their time derivatives to zero. The system of ODEs is:\n$$\n\\frac{dA}{dt} = \\rho\\, s_A - \\gamma_A A\n$$\n$$\n\\frac{dB}{dt} = \\rho\\, s_B - \\gamma_B B\n$$\n$$\n\\frac{dm}{dt} = \\rho\\, k_{\\mathrm{tx}}\\, f(A,B) - \\gamma_m m\n$$\n$$\n\\frac{dp}{dt} = \\rho\\, k_{\\mathrm{tl}}\\, m - \\gamma_p p\n$$\nAt steady state, denoted by a superscript asterisk ($^\\ast$), we have $\\frac{dA}{dt} = \\frac{dB}{dt} = \\frac{dm}{dt} = \\frac{dp}{dt} = 0$.\n\nFor transcription factor $A$:\n$$\n\\rho\\, s_A - \\gamma_A A^\\ast = 0 \\implies A^\\ast(\\rho) = \\frac{\\rho\\, s_A}{\\gamma_A}\n$$\nSimilarly, for transcription factor $B$:\n$$\n\\rho\\, s_B - \\gamma_B B^\\ast = 0 \\implies B^\\ast(\\rho) = \\frac{\\rho\\, s_B}{\\gamma_B}\n$$\nThese show that the steady-state concentrations of the input TFs scale linearly with the resource scalar $\\rho$.\n\nFor the reporter mRNA $m$:\n$$\n\\rho\\, k_{\\mathrm{tx}}\\, f(A^\\ast, B^\\ast) - \\gamma_m m^\\ast = 0 \\implies m^\\ast(\\rho) = \\frac{\\rho\\, k_{\\mathrm{tx}}}{\\gamma_m} f\\big(A^\\ast(\\rho), B^\\ast(\\rho)\\big)\n$$\nFor the reporter protein $p$:\n$$\n\\rho\\, k_{\\mathrm{tl}}\\, m^\\ast - \\gamma_p p^\\ast = 0 \\implies p^\\ast(\\rho) = \\frac{\\rho\\, k_{\\mathrm{tl}}}{\\gamma_p} m^\\ast(\\rho)\n$$\nSubstituting the expression for $m^\\ast(\\rho)$ into the equation for $p^\\ast(\\rho)$ yields the full expression for the steady-state reporter protein level:\n$$\np^\\ast(\\rho) = \\frac{\\rho\\, k_{\\mathrm{tl}}}{\\gamma_p} \\left( \\frac{\\rho\\, k_{\\mathrm{tx}}}{\\gamma_m} f\\big(A^\\ast(\\rho), B^\\ast(\\rho)\\big) \\right) = \\frac{k_{\\mathrm{tl}} k_{\\mathrm{tx}}}{\\gamma_p \\gamma_m} \\rho^2 f\\big(A^\\ast(\\rho), B^\\ast(\\rho)\\big)\n$$\nwhere $f(A,B) = H_A(A) H_B(B)$ and $A^\\ast(\\rho)$, $B^\\ast(\\rho)$ are the linear functions of $\\rho$ derived above. The explicit form is:\n$$\np^\\ast(\\rho) = \\frac{k_{\\mathrm{tl}} k_{\\mathrm{tx}}}{\\gamma_p \\gamma_m} \\rho^2 \\left( \\frac{(A^\\ast(\\rho))^{n_A}}{K_A^{n_A} + (A^\\ast(\\rho))^{n_A}} \\right) \\left( \\frac{(B^\\ast(\\rho))^{n_B}}{K_B^{n_B} + (B^\\ast(\\rho))^{n_B}} \\right)\n$$\n\nNext, we derive the local logarithmic sensitivity, or elasticity, $E(\\rho)$, defined as:\n$$\nE(\\rho) \\equiv \\frac{d \\log p^\\ast(\\rho)}{d \\log \\rho}\n$$\nTo facilitate differentiation, we take the natural logarithm of $p^\\ast(\\rho)$:\n$$\n\\log p^\\ast(\\rho) = \\log\\left(\\frac{k_{\\mathrm{tl}} k_{\\mathrm{tx}}}{\\gamma_p \\gamma_m}\\right) + 2\\log\\rho + \\log(H_A(A^\\ast(\\rho))) + \\log(H_B(B^\\ast(\\rho)))\n$$\nDifferentiating with respect to $\\log\\rho$ term by term:\n$$\nE(\\rho) = \\frac{d}{d\\log\\rho}\\left( \\log(\\text{const.}) \\right) + \\frac{d(2\\log\\rho)}{d\\log\\rho} + \\frac{d\\log H_A(A^\\ast(\\rho))}{d\\log\\rho} + \\frac{d\\log H_B(B^\\ast(\\rho))}{d\\log\\rho}\n$$\nThe derivative of the constant is $0$, and $\\frac{d(2\\log\\rho)}{d\\log\\rho} = 2$. For the Hill function terms, we use the chain rule:\n$$\n\\frac{d\\log H_A(A^\\ast(\\rho))}{d\\log\\rho} = \\frac{d\\log H_A}{d\\log A^\\ast} \\cdot \\frac{d\\log A^\\ast}{d\\log\\rho}\n$$\nWe need the derivative $\\frac{d\\log A^\\ast}{d\\log\\rho}$. Since $A^\\ast(\\rho) = (\\frac{s_A}{\\gamma_A})\\rho$, we have $\\log A^\\ast = \\log(\\frac{s_A}{\\gamma_A}) + \\log\\rho$. Thus, $\\frac{d\\log A^\\ast}{d\\log\\rho} = 1$. The same holds for $B^\\ast$.\nThe problem reduces to computing the elasticity of the Hill function with respect to its input. Let $H(x) = \\frac{x^n}{K^n + x^n}$.\n$$\n\\log H(x) = n\\log x - \\log(K^n + x^n)\n$$\nDifferentiating with respect to $\\log x$:\n$$\n\\frac{d\\log H(x)}{d\\log x} = n - \\frac{d\\log(K^n + x^n)}{d \\log x} = n - \\frac{x}{K^n+x^n}\\frac{d(K^n+x^n)}{dx} = n - \\frac{x}{K^n+x^n}(nx^{n-1})\n$$\n$$\n\\frac{d\\log H(x)}{d\\log x} = n - \\frac{nx^n}{K^n + x^n} = n\\left(1 - \\frac{x^n}{K^n+x^n}\\right) = n\\left(\\frac{K^n}{K^n+x^n}\\right) = n(1 - H(x))\n$$\nApplying this result to our expression for $E(\\rho)$:\n$$\nE(\\rho) = 2 + \\frac{d\\log H_A}{d\\log A^\\ast} + \\frac{d\\log H_B}{d\\log B^\\ast} = 2 + n_A\\left(1 - H_A(A^\\ast(\\rho))\\right) + n_B\\left(1 - H_B(B^\\ast(\\rho))\\right)\n$$\nThis is the final analytical expression for the elasticity. It is evaluated at $\\rho = \\rho_0 = 1.0$, where $A^\\ast(\\rho_0) = s_A/\\gamma_A$ and $B^\\ast(\\rho_0) = s_B/\\gamma_B$.\n\nThe logic robustness metric $J_{\\mathrm{logic}}$ is defined as:\n$$\nJ_{\\mathrm{logic}} \\equiv \\max_{\\rho \\in [\\rho_{\\min}, \\rho_{\\max}]} \\left| \\frac{f\\big(A^\\ast(\\rho), B^\\ast(\\rho)\\big)}{f\\big(A^\\ast(\\rho_0), B^\\ast(\\rho_0)\\big)} - 1 \\right|\n$$\nThis quantity does not have a simple closed-form expression and must be computed numerically. The procedure is as follows:\n1.  Generate a logarithmic grid of $N_\\rho = 10001$ points for $\\rho$ in the interval $[\\rho_{\\min}, \\rho_{\\max}]$.\n2.  Compute the reference value $f_0 = f\\big(A^\\ast(\\rho_0), B^\\ast(\\rho_0)\\big)$.\n3.  For each point $\\rho_i$ in the grid, compute $A^\\ast(\\rho_i)$ and $B^\\ast(\\rho_i)$, then calculate $f_i = f\\big(A^\\ast(\\rho_i), B^\\ast(\\rho_i)\\big)$.\n4.  Compute the value $|f_i/f_0 - 1|$ for all points in the grid.\n5.  $J_{\\mathrm{logic}}$ is the maximum of these values.\n\nFinally, the robustness flag is determined by comparing the computed $E(\\rho_0)$ and $J_{\\mathrm{logic}}$ against their respective thresholds, $E_{\\mathrm{thr}} = 3.2$ and $J_{\\mathrm{thr}} = 0.2$.\n\nThese steps will be implemented for each test case to generate the required output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the synthetic gene circuit problem: calculates elasticity,\n    logic robustness, and a robustness flag for five parameter sets.\n    \"\"\"\n    # Shared constants\n    k_tx = 50.0\n    k_tl = 5.0\n    gamma_m = 1.0\n    gamma_p = 0.2\n    rho_0 = 1.0\n    rho_min = 0.3\n    rho_max = 3.0\n    N_rho = 10001\n    E_thr = 3.2\n    J_thr = 0.2\n\n    # Test suite: (n_A, K_A, s_A, gamma_A, n_B, K_B, s_B, gamma_B)\n    test_cases = [\n        (2.0, 1.0, 10.0, 5.0, 2.0, 1.5, 12.0, 6.0),\n        (3.0, 2.0, 100.0, 5.0, 2.0, 2.0, 90.0, 3.0),\n        (2.0, 1.0, 1.0, 5.0, 2.0, 1.0, 1.0, 5.0),\n        (1.0, 5.0, 20.0, 1.0, 4.0, 1.0, 1.0, 2.0),\n        (2.0, 3.0, 3.0, 1.0, 2.0, 2.0, 2.0, 1.0),\n    ]\n\n    results = []\n\n    def hill_function(x, n, K):\n        \"\"\"Computes the Hill activation function.\"\"\"\n        # This form is robust against potential overflow for very large x.\n        # However, direct computation is safe for the given parameters.\n        # return 1.0 / (1.0 + (K / x)**n)\n        x_n = x**n\n        return x_n / (K**n + x_n)\n\n    # Generate the rho grid for J_logic calculation\n    rho_grid = np.logspace(np.log10(rho_min), np.log10(rho_max), N_rho)\n\n    for case in test_cases:\n        nA, KA, sA, gammaA, nB, KB, sB, gammaB = case\n\n        # 1. Calculate Elasticity E(rho_0)\n        A_star_0 = sA / gammaA\n        B_star_0 = sB / gammaB\n\n        H_A_0 = hill_function(A_star_0, nA, KA)\n        H_B_0 = hill_function(B_star_0, nB, KB)\n        \n        # E(rho) = 2 + nA*(1 - H_A(A*(rho))) + nB*(1 - H_B(B*(rho)))\n        E_rho0 = 2.0 + nA * (1.0 - H_A_0) + nB * (1.0 - H_B_0)\n\n        # 2. Calculate Logic Robustness J_logic\n        f_0 = H_A_0 * H_B_0\n        \n        A_star_rho = rho_grid * sA / gammaA\n        B_star_rho = rho_grid * sB / gammaB\n\n        f_rho = hill_function(A_star_rho, nA, KA) * hill_function(B_star_rho, nB, KB)\n\n        # Avoid division by zero, although f_0 should be positive.\n        if f_0 > 1e-12:\n            deviation = np.abs(f_rho / f_0 - 1.0)\n            J_logic = np.max(deviation)\n        else:\n            # If f_0 is effectively zero, any non-zero f_rho indicates infinite relative change.\n            # This case is unlikely here but handled for robustness.\n            J_logic = np.inf\n        \n        # 3. Determine the robustness flag\n        is_robust = 1 if (E_rho0 <= E_thr and J_logic <= J_thr) else 0\n\n        # Store results for this case\n        results.append([E_rho0, J_logic, is_robust])\n\n    # Format the final output string as specified\n    output_parts = []\n    for res in results:\n        E_str = f\"{res[0]:.6f}\"\n        J_str = f\"{res[1]:.6f}\"\n        R_str = str(res[2])\n        output_parts.append(f\"[{E_str},{J_str},{R_str}]\")\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "2723261"}, {"introduction": "The function of many biological circuits is defined not by a static output, but by its behavior over time, especially in growing cell populations where dilution is a factor. This practice [@problem_id:2723238] challenges you to investigate the temporal stability of a classifier that uses microRNA-mediated repression and adaptive feedback. By numerically simulating the circuit's dynamics, you will determine if its classification output remains stable or flips over time, gaining insight into how system dynamics, feedback, and cell growth collectively shape functional reliability.", "problem": "You are tasked with assessing temporal stability of a binary classification computed by a synthetic gene circuit in proliferating mammalian cells that uses microRNA (miRNA) mediated repression with adaptive target-site affinity. The classifier reports class $C(t) \\in \\{0,1\\}$ based on a reporter protein level $P(t)$ relative to a fixed threshold $P_{\\mathrm{thr}}$. Biology provides the modeling primitives, but your task is purely mathematical and algorithmic: implement and analyze the coupled ordinary differential equations (ODEs) below and decide, for each parameter set, whether the class label remains unchanged over a specified observation window.\n\nFoundational base:\n- Central Dogma of Molecular Biology and mass-action-like turnover motivate first-order production and loss for molecular species under well-mixed conditions.\n- In proliferating cells with exponential growth rate $\\mu$, all intracellular molecular concentrations experience effective first-order dilution at rate $\\mu$.\n- Equilibrium occupancy of a single binding site by a ligand is modeled by the Michaelis–Menten-like relation $\\Theta = m/(K_d + m)$, where $m$ is the free ligand concentration and $K_d$ is the dissociation constant. Here, $m$ corresponds to miRNA and $\\Theta$ is interpreted as the fraction of target sites occupied.\n- Adaptive feedback can modulate effective $K_d$ over time toward a more repressive state when occupancy is high and relax back to a baseline when occupancy is low.\n\nModel definition:\n- Let $m(t)$ denote miRNA concentration, $K_d(t)$ the effective dissociation constant of the miRNA–target interaction, and $P(t)$ the reporter protein concentration. All time constants are in hours, but the required outputs are booleans, so you will not output any physical units. The dynamics are:\n  1. $ \\dfrac{dm}{dt} = \\alpha_m - (\\delta_m + \\mu)\\, m $,\n  2. $ \\Theta(t) = \\dfrac{m(t)}{K_d(t) + m(t)} $,\n  3. $ \\dfrac{dK_d}{dt} = -\\beta\\, \\Theta(t)\\, (K_d(t) - K_{\\min}) + \\rho\\, (K_0 - K_d(t)) $,\n  4. $ \\dfrac{dP}{dt} = \\alpha_{P0}\\, \\bigl(1 - \\Theta(t)\\bigr) - (\\delta_P + \\mu)\\, P $.\n- Initial conditions at $t=0$ are $m(0)=m_0$, $K_d(0)=K_0$, and $P(0)=P_0$.\n\nClassification rule and stability criterion:\n- Define $C(t) = 1$ if $P(t) \\ge P_{\\mathrm{thr}}$ and $C(t) = 0$ otherwise.\n- Given an observation window $[t_{\\mathrm{ref}}, T]$ with $0 < t_{\\mathrm{ref}} < T$, define the stability boolean $S$ to be true if $C(t)$ is constant for all $t \\in [t_{\\mathrm{ref}}, T]$, and false otherwise. In words, after an initial transient up to time $t_{\\mathrm{ref}}$, the classification must not change.\n\nNumerical requirements:\n- Implement a numerically stable ODE integration of the system above. You may use any standard method for non-stiff ODEs at sufficient resolution to unambiguously detect classification changes. Ensure that $\\Theta(t)$ is computed as specified and treated as bounded in $[0,1]$.\n- Your program must evaluate $C(t)$ along the numerical solution for $t \\in [t_{\\mathrm{ref}}, T]$ and compute $S$ for each test case as defined above.\n\nTest suite:\nFor each case below, simulate on $t \\in [0, T]$ and evaluate stability on $[t_{\\mathrm{ref}}, T]$. Each tuple lists $(\\alpha_m, \\delta_m, \\mu, K_0, K_{\\min}, \\beta, \\rho, \\alpha_{P0}, \\delta_P, P_{\\mathrm{thr}}, m_0, P_0, T, t_{\\mathrm{ref}})$, with the initial dissociation constant set by $K_d(0)=K_0$.\n\n- Case $1$ (happy path, always low): $(20.0, 0.1, 0.03, 50.0, 20.0, 0.2, 0.05, 1000.0, 0.05, 4000.0, 0.0, 0.0, 96.0, 24.0)$.\n- Case $2$ (no adaptation baseline): $(20.0, 0.1, 0.03, 50.0, 10.0, 0.0, 0.05, 1000.0, 0.05, 2000.0, 0.0, 0.0, 96.0, 24.0)$.\n- Case $3$ (adaptation-induced flip): $(20.0, 0.1, 0.03, 50.0, 10.0, 0.05, 0.005, 1000.0, 0.05, 2200.0, 0.0, 0.0, 96.0, 24.0)$.\n- Case $4$ (non-proliferating boundary): $(20.0, 0.1, 0.0, 50.0, 10.0, 0.0, 0.05, 1000.0, 0.05, 2000.0, 0.0, 0.0, 96.0, 24.0)$.\n- Case $5$ (high growth dilution): $(20.0, 0.1, 0.1, 50.0, 10.0, 0.2, 0.05, 1000.0, 0.05, 2300.0, 0.0, 0.0, 96.0, 24.0)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$, where each $\\text{result}_i$ is the boolean $S$ for the corresponding case in the same order as above. The booleans must be printed in the programming language’s standard boolean literal form.", "solution": "The problem presented is a well-posed initial value problem for a system of coupled ordinary differential equations (ODEs), grounded in the established principles of biochemical kinetics used in synthetic biology. The task is to determine the temporal stability of a binary classifier based on the dynamics of this system. The problem is scientifically sound, self-contained, and algorithmically specified. It is therefore deemed valid.\n\nThe system describes the dynamics of three state variables: the concentration of a microRNA, $m(t)$; the effective dissociation constant of its interaction with a target messenger RNA, $K_d(t)$; and the concentration of a reporter protein, $P(t)$. The state of the system at any time $t$ can be represented by the vector $y(t) = [m(t), K_d(t), P(t)]^T$.\n\nThe evolution of the system is governed by the following set of coupled ODEs:\n$$\n\\frac{dm}{dt} = \\alpha_m - (\\delta_m + \\mu) m(t)\n$$\n$$\n\\frac{dK_d}{dt} = -\\beta \\Theta(t) (K_d(t) - K_{\\min}) + \\rho (K_0 - K_d(t))\n$$\n$$\n\\frac{dP}{dt} = \\alpha_{P0} (1 - \\Theta(t)) - (\\delta_P + \\mu) P(t)\n$$\nwhere the term $\\Theta(t)$ represents the fractional occupancy of miRNA target sites, an intermediate quantity defined as:\n$$\n\\Theta(t) = \\frac{m(t)}{K_d(t) + m(t)}\n$$\nThe initial conditions are specified as $m(0) = m_0$, $K_d(0) = K_0$, and $P(0) = P_0$. All parameters $(\\alpha_m, \\delta_m, \\mu, K_0, K_{\\min}, \\beta, \\rho, \\alpha_{P0}, \\delta_P)$ are provided for each test case.\n\nThe solution proceeds via the following steps:\n\n1.  **Numerical Integration**: The system of ODEs is nonlinear and coupled, lacking a straightforward analytical solution. Therefore, a numerical approach is necessary. We employ a standard numerical integrator for non-stiff ODEs, such as an explicit Runge-Kutta method. The `solve_ivp` function from the SciPy library, utilizing the default `RK45` method, is perfectly suited for this task. The integration is performed over the time interval $[0, T]$ for each given test case. To ensure that any change in classification is detected, the solution is evaluated at a fine-grained temporal resolution, for example, at time steps of $0.1$ hours. This resolution is more than sufficient given the system's characteristic timescales, which are on the order of several hours.\n\n2.  **Classification and Stability Analysis**: For each test case, after obtaining the numerical solution for $P(t)$ over the interval $[0, T]$, we analyze its behavior within the specified observation window $[t_{\\mathrm{ref}}, T]$.\n    a. First, we extract the time points and corresponding protein concentrations $P(t)$ that fall within this window.\n    b. Next, we apply the classification rule, $C(t) = 1$ if $P(t) \\ge P_{\\mathrm{thr}}$ and $C(t) = 0$ otherwise, to generate a boolean time series representing the classifier's output during the observation period.\n    c. The stability criterion $S$ is then evaluated. The classification is deemed stable ($S$ is true) if and only if the boolean time series $C(t)$ is constant throughout the entire observation window, i.e., $C(t_i) = C(t_j)$ for all time points $t_i, t_j \\in [t_{\\mathrm{ref}}, T]$. This is algorithmically checked by verifying that the set of unique values in the boolean time series contains only a single element (either `True` or `False`). If the set contains both, the classification is unstable ($S$ is false).\n\n3.  **Implementation**: A Python script is constructed to automate this process. The script iterates through the five provided test cases. In each iteration, it sets up the parameters and initial conditions, calls the ODE solver, performs the stability analysis as described above, and stores the resulting boolean value of $S$. Finally, all results are collected and printed in the specified format.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the synthetic gene circuit ODEs for multiple test cases and\n    determines the stability of the classification output.\n    \"\"\"\n\n    # Test cases as tuples:\n    # (alpha_m, delta_m, mu, K0, K_min, beta, rho, alpha_P0, delta_P, P_thr, m0, P0, T, t_ref)\n    test_cases = [\n        # Case 1 (happy path, always low)\n        (20.0, 0.1, 0.03, 50.0, 20.0, 0.2, 0.05, 1000.0, 0.05, 4000.0, 0.0, 0.0, 96.0, 24.0),\n        # Case 2 (no adaptation baseline)\n        (20.0, 0.1, 0.03, 50.0, 10.0, 0.0, 0.05, 1000.0, 0.05, 2000.0, 0.0, 0.0, 96.0, 24.0),\n        # Case 3 (adaptation-induced flip)\n        (20.0, 0.1, 0.03, 50.0, 10.0, 0.05, 0.005, 1000.0, 0.05, 2200.0, 0.0, 0.0, 96.0, 24.0),\n        # Case 4 (non-proliferating boundary)\n        (20.0, 0.1, 0.0, 50.0, 10.0, 0.0, 0.05, 1000.0, 0.05, 2000.0, 0.0, 0.0, 96.0, 24.0),\n        # Case 5 (high growth dilution)\n        (20.0, 0.1, 0.1, 50.0, 10.0, 0.2, 0.05, 1000.0, 0.05, 2300.0, 0.0, 0.0, 96.0, 24.0),\n    ]\n\n    results = []\n\n    def ode_system(t, y, params):\n        \"\"\"\n        Defines the system of ordinary differential equations.\n        y: state vector [m, Kd, P]\n        params: tuple of model parameters\n        \"\"\"\n        m, Kd, P = y\n        alpha_m, delta_m, mu, K0, K_min, beta, rho, alpha_P0, delta_P = params\n\n        # Ensure Kd + m is not zero to prevent division by zero.\n        # Given m>=0 and Kd stays positive (driven towards Kmin or K0, both positive),\n        # this sum will be positive.\n        if Kd + m <= 0:\n            Theta = 0.0\n        else:\n            Theta = m / (Kd + m)\n\n        dm_dt = alpha_m - (delta_m + mu) * m\n        dKd_dt = -beta * Theta * (Kd - K_min) + rho * (K0 - Kd)\n        dP_dt = alpha_P0 * (1.0 - Theta) - (delta_P + mu) * P\n\n        return [dm_dt, dKd_dt, dP_dt]\n\n    for case in test_cases:\n        alpha_m, delta_m, mu, K0, K_min, beta, rho, alpha_P0, delta_P, P_thr, m0, P0, T, t_ref = case\n\n        ode_params = (alpha_m, delta_m, mu, K0, K_min, beta, rho, alpha_P0, delta_P)\n        \n        # Initial conditions: m(0)=m0, Kd(0)=K0, P(0)=P0\n        y0 = [m0, K0, P0]\n        \n        # Time span for integration\n        t_span = [0, T]\n        \n        # Set evaluation points for dense output, ensuring sufficient resolution\n        num_points = int(T * 10) + 1 # 10 points per hour\n        t_eval = np.linspace(0, T, num_points)\n\n        # Solve the ODE system\n        sol = solve_ivp(\n            ode_system,\n            t_span,\n            y0,\n            method='RK45',\n            t_eval=t_eval,\n            args=(ode_params,)\n        )\n\n        # Extract protein concentration P(t) from the solution\n        t_sol = sol.t\n        P_sol = sol.y[2]\n\n        # Identify indices corresponding to the observation window [t_ref, T]\n        obs_indices = (t_sol >= t_ref)\n\n        # Get protein levels and classifications in the observation window\n        P_obs = P_sol[obs_indices]\n        C_obs = (P_obs >= P_thr)\n\n        # Determine stability: True if classification is constant, False otherwise\n        # This is true if the number of unique classification values is 1.\n        is_stable = len(np.unique(C_obs)) == 1\n        results.append(is_stable)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2723238"}]}