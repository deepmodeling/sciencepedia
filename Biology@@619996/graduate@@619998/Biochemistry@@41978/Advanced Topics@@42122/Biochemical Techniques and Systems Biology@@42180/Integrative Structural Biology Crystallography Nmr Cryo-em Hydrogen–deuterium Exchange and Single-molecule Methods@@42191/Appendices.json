{"hands_on_practices": [{"introduction": "The ability to determine molecular structures using X-rays, neutrons, or electrons fundamentally relies on the principle of interference. This first exercise strips the complexity of a real experiment down to its core: the interaction of waves with two scattering centers. By deriving the scattering intensity for a simple dimer from first principles, you will gain an intuitive and mathematical grasp of how the spatial arrangement of atoms dictates the observable interference pattern, a concept that underpins all of crystallography and small-angle scattering [@problem_id:2571528].", "problem": "A rigid dimer in solution is probed under monochromatic X-ray illumination in the kinematic (single-scattering) regime, as commonly assumed in X-ray crystallography and small-angle scattering analyses. Model the dimer as two point scatterers with isotropic, energy-dependent but locally constant atomic form factors $f_A$ and $f_B$ over the scattering vector range of interest. Let the point scatterers be located at positions $\\mathbf{r}_1=-\\mathbf{r}_0/2$ and $\\mathbf{r}_2=+\\mathbf{r}_0/2$, where $\\mathbf{r}_0$ is a fixed vector specifying the inter-scatterer displacement. The scattering vector is defined as $\\mathbf{q}=\\mathbf{k}_s-\\mathbf{k}_i$, where the incident and scattered wavevectors satisfy $|\\mathbf{k}_i|=|\\mathbf{k}_s|=2\\pi/\\lambda$ for wavelength $\\lambda$.\n\nStarting only from the definition of the structure factor $S(\\mathbf{q})$ as the coherent sum of contributions from all scatterers and the fact that the differential scattering intensity is proportional to $|S(\\mathbf{q})|^2$, derive a closed-form expression for the normalized intensity\n$$\nI_{\\mathrm{rel}}(\\mathbf{q}) \\equiv \\frac{|S(\\mathbf{q})|^2}{|S(\\mathbf{0})|^2}\n$$\nas a function of $f_A$, $f_B$, and $\\mathbf{q}\\cdot\\mathbf{r}_0$. Assume the far-field approximation and neglect multiple scattering and inelastic effects. Express your final answer as a single closed-form analytic expression for $I_{\\mathrm{rel}}(\\mathbf{q})$ in terms of $f_A$, $f_B$, and $\\mathbf{q}\\cdot\\mathbf{r}_0$. No numerical evaluation is required, and no rounding is needed. Do not include units in your final answer.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of kinematic scattering theory, is well-posed with sufficient information for a unique solution, and is formulated with objective, precise language. We shall proceed with the derivation.\n\nThe structure factor, $S(\\mathbf{q})$, for a system of $N$ scatterers located at positions $\\mathbf{r}_j$ with corresponding atomic form factors $f_j$ is defined as the coherent sum of the amplitudes of the scattered waves:\n$$\nS(\\mathbf{q}) = \\sum_{j=1}^{N} f_j e^{i \\mathbf{q} \\cdot \\mathbf{r}_j}\n$$\nwhere $i$ is the imaginary unit and $\\mathbf{q}$ is the scattering vector. The form factors $f_j$ are assumed to be constant over the range of $\\mathbf{q}$ considered.\n\nThe system under consideration is a rigid dimer, which consists of two point scatterers, $A$ and $B$.\nThe number of scatterers is $N=2$.\nThe form factors are $f_A$ and $f_B$.\nThe positions are given as $\\mathbf{r}_1 = -\\frac{\\mathbf{r}_0}{2}$ and $\\mathbf{r}_2 = +\\frac{\\mathbf{r}_0}{2}$.\n\nSubstituting these givens into the general definition of the structure factor yields:\n$$\nS(\\mathbf{q}) = f_A e^{i \\mathbf{q} \\cdot \\left(-\\frac{\\mathbf{r}_0}{2}\\right)} + f_B e^{i \\mathbf{q} \\cdot \\left(+\\frac{\\mathbf{r}_0}{2}\\right)}\n$$\nThis can be rewritten as:\n$$\nS(\\mathbf{q}) = f_A e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2}\n$$\nThe scattering intensity is proportional to the squared modulus of the structure factor, $|S(\\mathbf{q})|^2$. To compute this, we multiply $S(\\mathbf{q})$ by its complex conjugate, $S^*(\\mathbf{q})$:\n$$\n|S(\\mathbf{q})|^2 = S(\\mathbf{q})S^*(\\mathbf{q})\n$$\nAssuming the atomic form factors $f_A$ and $f_B$ are real-valued, which is standard for non-anomalous X-ray scattering, the complex conjugate is:\n$$\nS^*(\\mathbf{q}) = f_A e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2}\n$$\nThe product is then:\n$$\n|S(\\mathbf{q})|^2 = \\left( f_A e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} \\right) \\left( f_A e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} \\right)\n$$\nExpanding this expression gives four terms:\n$$\n|S(\\mathbf{q})|^2 = f_A^2 e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_A f_B e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B f_A e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B^2 e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2}\n$$\nSimplifying the exponents:\n$$\n|S(\\mathbf{q})|^2 = f_A^2 e^{0} + f_A f_B e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)} + f_A f_B e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)} + f_B^2 e^{0}\n$$\n$$\n|S(\\mathbf{q})|^2 = f_A^2 + f_B^2 + f_A f_B \\left( e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)} + e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)} \\right)\n$$\nUsing Euler's formula, which states $e^{i\\theta} + e^{-i\\theta} = 2\\cos(\\theta)$, with $\\theta = \\mathbf{q} \\cdot \\mathbf{r}_0$, we obtain the Debye scattering equation for two particles:\n$$\n|S(\\mathbf{q})|^2 = f_A^2 + f_B^2 + 2 f_A f_B \\cos(\\mathbf{q} \\cdot \\mathbf{r}_0)\n$$\nNext, we must find the normalization factor, $|S(\\mathbf{0})|^2$. This is the intensity at zero scattering vector, $\\mathbf{q}=\\mathbf{0}$, which corresponds to the forward scattering direction. We evaluate $S(\\mathbf{q})$ at $\\mathbf{q}=\\mathbf{0}$:\n$$\nS(\\mathbf{0}) = f_A e^{-i (\\mathbf{0} \\cdot \\mathbf{r}_0)/2} + f_B e^{i (\\mathbf{0} \\cdot \\mathbf{r}_0)/2} = f_A e^0 + f_B e^0 = f_A + f_B\n$$\nThe squared modulus is therefore:\n$$\n|S(\\mathbf{0})|^2 = (f_A + f_B)^2 = f_A^2 + 2 f_A f_B + f_B^2\n$$\nFinally, the normalized intensity $I_{\\mathrm{rel}}(\\mathbf{q})$ is the ratio of $|S(\\mathbf{q})|^2$ to $|S(\\mathbf{0})|^2$:\n$$\nI_{\\mathrm{rel}}(\\mathbf{q}) = \\frac{|S(\\mathbf{q})|^2}{|S(\\mathbf{0})|^2} = \\frac{f_A^2 + f_B^2 + 2 f_A f_B \\cos(\\mathbf{q} \\cdot \\mathbf{r}_0)}{(f_A + f_B)^2}\n$$\nThis final expression is a function of $f_A$, $f_B$, and the scalar product $\\mathbf{q} \\cdot \\mathbf{r}_0$, as required by the problem statement.", "answer": "$$\n\\boxed{\\frac{f_A^2 + f_B^2 + 2 f_A f_B \\cos(\\mathbf{q} \\cdot \\mathbf{r}_0)}{(f_A + f_B)^2}}\n$$", "id": "2571528"}, {"introduction": "Moving from theory to practice, this exercise addresses a critical modern challenge: data validation in high-resolution cryo-electron microscopy (cryo-EM). Generating a 3D map is only the first step; ensuring that the features in the map represent true biological structure and not experimental noise is paramount for scientific rigor. This problem guides you through the reasoning behind the \"gold-standard\" practice of splitting data into independent half-sets, a cornerstone of robust cross-validation that prevents overfitting and model bias, thereby ensuring the integrity of published cryo-EM structures [@problem_id:2571522].", "problem": "A single-particle cryogenic electron microscopy (cryo-EM) dataset of a mildly flexible, oligomeric enzyme is collected at near-physiological conditions. You plan a three-dimensional (3D) classification to separate conformational states before high-resolution refinement. The experimental conditions yield a per-particle Fourier domain signal-to-noise ratio (SNR) that decreases with spatial frequency, and in the mid-resolution shells of interest the SNR is approximately $0.05$.\n\nConsider the following general setup for 3D classification in single-particle analysis (SPA). Each particle image is modeled as an additive superposition of a projection of the underlying 3D signal and noise. When reconstructing maps from particle subsets, the reconstructed Fourier coefficients can be idealized as $X = S + N$, where $S$ is the unknown signal component and $N$ represents noise. Assume $N$ has zero mean and is uncorrelated with $S$. In practice, iterative algorithms (for example, expectation–maximization with regularized likelihood) simultaneously update class assignments, orientations, and maps, which risks overfitting by adapting to noise if model complexity outpaces data support.\n\nYou are considering two alternative protocols:\n\n- Protocol $\\mathrm{P1}$ (single-set classification): Use all particles in a single pool for iterative 3D classification. For resolution assessment, compute the correlation between the final map and a reference constructed from the same particle set and processing decisions.\n\n- Protocol $\\mathrm{P2}$ (gold-standard half-set classification): Randomly split particles into $2$ statistically independent halves before any optimization. Carry out 3D classification and map reconstruction independently for each half. Assess resolution and feature reliability by correlating the two independent half-maps and by checking the reproducibility of class-specific features and occupancies across halves. Only after convergence, merge corresponding classes across halves for final maps.\n\nFrom first principles, answer the following. Use the additive model $X = S + N$ and the fact that, for statistically independent noise realizations $N$ and $\\tilde{N}$ with zero mean, cross-terms such as $\\langle S, \\tilde{N} \\rangle$ have expectation $0$, while shared noise terms can induce spurious correlation. Also use the principle of cross-validation: performance must be evaluated on data that were not used to fit the model parameters, in order to obtain an unbiased generalization estimate.\n\nWhich of the following statements are correct?\n\nA. In Protocol $\\mathrm{P1}$, iterative classification can couple reconstructions to the specific noise realization present in the particle set, so that resolution estimates based on correlating a map with a reference derived from the same data (i.e., sharing $N$) are upwardly biased. In Protocol $\\mathrm{P2}$, keeping half-sets independent ensures that, on average, only the shared signal $S$ contributes to the inter-half correlation, controlling overfitting-induced inflation.\n\nB. A sufficiently aggressive low-pass filter applied during classification is, by itself, sufficient to prevent overfitting and model bias, regardless of the number of classes, the SNR, and the number of particles.\n\nC. To test whether 3D classification features are driven by data rather than initial-model bias, one can rerun the entire gold-standard half-set classification with different random seeds and initial models, and then assess whether corresponding classes exhibit consistent features and occupancies across halves and repeats. Stability across independent halves and repeats is evidence against model bias.\n\nD. If particles are randomly split into two halves only after the final optimization in a single-set classification, then the inter-half correlation will still be a valid gold-standard estimate of resolution and feature reliability, because the split ensures statistical independence.\n\nE. Performing per-particle motion correction and contrast transfer function (CTF) refinement on each half-set while sharing alignment parameters across halves preserves the statistical independence needed for unbiased inter-half validation.\n\nSelect all that apply.\n\nImportant: Derive your reasoning from the additive signal-plus-noise model, statistical independence, and the definition of cross-validation. Avoid appeals to undocumented software-specific heuristics or black-box formulas that are not justified from these principles.", "solution": "The problem statement describes a common challenge in single-particle cryo-electron microscopy (cryo-EM): the analysis of conformational heterogeneity in a low signal-to-noise ratio (SNR) environment. The statement is scientifically sound, well-posed, and provides the necessary first principles for a rigorous evaluation. The experimental scenario is realistic; a per-particle SNR of $0.05$ is typical. The theoretical framework, modeling a particle image as an additive superposition of signal and noise ($X = S + N$), is a standard and effective idealization. The core task is to evaluate two data processing protocols, $\\mathrm{P1}$ (single-set) and $\\mathrm{P2}$ (gold-standard half-set), and related statements, based on the principles of statistical independence and cross-validation to control for overfitting. The problem is valid.\n\nThe fundamental principles to be applied are:\n1.  **Additive Model**: The measured data in Fourier space for a given projection, $X$, is the sum of the true signal, $S$, and a noise term, $N$. That is, $X = S + N$.\n2.  **Noise Properties**: The noise $N$ is a random variable with zero mean, $E[N] = 0$. The noise is uncorrelated with the signal, $E[S N] = E[S] E[N] = 0$. For two independent datasets, their noise realizations, $N_1$ and $N_2$, are statistically independent, meaning their covariance is zero, $E[N_1 N_2] = E[N_1] E[N_2] = 0$.\n3.  **Overfitting**: This occurs when a model with high complexity (e.g., a large number of 3D classes) is fit to a dataset with limited information (low SNR, insufficient number of particles). The model begins to fit the specific noise realization $N$ present in the data, rather than just the underlying signal $S$. This leads to the generation of spurious, non-reproducible features.\n4.  **Cross-Validation**: To obtain an unbiased estimate of a model's performance on new data (its generalization ability), the model must be evaluated on a dataset that was not used during the model's training or parameter fitting. This is the only robust way to detect overfitting.\n\nWith these principles, we shall evaluate each statement.\n\n**A. In Protocol $\\mathrm{P1}$, iterative classification can couple reconstructions to the specific noise realization present in the particle set, so that resolution estimates based on correlating a map with a reference derived from the same data (i.e., sharing $N$) are upwardly biased. In Protocol $\\mathrm{P2}$, keeping half-sets independent ensures that, on average, only the shared signal $S$ contributes to the inter-half correlation, controlling overfitting-induced inflation.**\n\nThis statement addresses the core issue of overfitting and its detection.\nIn Protocol $\\mathrm{P1}$, the entire dataset, which we can model as $\\{S+N\\}$, is used for classification and reconstruction. The final map, $M_{full}$, is a complex function of these data, $M_{full} = f(\\{S+N\\})$. If one assesses resolution by comparing this map to a reference, $R$, also constructed from the same data (e.g., from a subset of the same particles or using the same parameters), then $R = g(\\{S+N\\})$. Both the map and the reference are now functions of the *same* noise realization, $N$. When we compute their correlation, terms dependent on the self-correlation of the noise, $\\langle N', N' \\rangle$ (where $N'$ is the noise component in the final maps), will contribute. Since the algorithm has fit to this noise, these terms are non-zero and positive, leading to an artificially inflated correlation. This is not a true measure of signal resolution but rather a measure of how well the algorithm has memorized the noise.\n\nIn Protocol $\\mathrm{P2}$, the data is split *a priori* into two independent halves: set $1$ with data $\\{S+N_1\\}$ and set $2$ with data $\\{S+N_2\\}$. The noise realizations $N_1$ and $N_2$ are statistically independent. Two maps, $M_1 = f_1(\\{S+N_1\\})$ and $M_2 = f_2(\\{S+N_2\\})$, are reconstructed independently. When calculating their cross-correlation, we are evaluating the expectation of terms like $\\langle S+N_1, S+N_2 \\rangle$. Expanding this gives $\\langle S, S \\rangle + \\langle S, N_2 \\rangle + \\langle N_1, S \\rangle + \\langle N_1, N_2 \\rangle$. Due to the statistical independence and zero-mean properties of the noise, the expectation of the cross-terms is zero: $E[\\langle S, N_2 \\rangle] = 0$, $E[\\langle N_1, S \\rangle] = 0$, and $E[\\langle N_1, N_2 \\rangle] = 0$. Therefore, on average, the correlation is driven solely by the signal term, $\\langle S, S \\rangle$. This is the principle of gold-standard Fourier Shell Correlation (FSC), which provides an unbiased estimate of resolution by eliminating correlation arising from fitted noise. The statement accurately describes this fundamental concept.\n\nVerdict: **Correct**.\n\n**B. A sufficiently aggressive low-pass filter applied during classification is, by itself, sufficient to prevent overfitting and model bias, regardless of the number of classes, the SNR, and the number of particles.**\n\nThis statement proposes that low-pass filtering is a panacea for overfitting. A low-pass filter removes high-frequency components from the images. Since noise is often dominant at high spatial frequencies, this can indeed mitigate overfitting by preventing the algorithm from fitting high-resolution noise. However, the claim that this is *sufficient by itself* and *regardless* of other critical parameters is a gross overstatement and is incorrect.\nOverfitting is a function of the balance between data quality/quantity and model complexity. The key factors are the SNR, the number of particles, and the number of free parameters in the model (e.g., the number of 3D classes).\nIf the SNR is extremely low and the number of particles is small, but the number of classes sought is large, the algorithm can still overfit to the *low-frequency components of the noise*. An aggressive low-pass filter simply restricts the overfitting to lower resolution features, but it does not eliminate it. For example, if one tries to sort $10000$ very noisy particles into $50$ classes, the algorithm will likely produce $50$ distinct maps whose differences are driven by patterned noise, even if the data is heavily filtered. True features may be blurred with noise-driven artifacts. The filter is a regularization tool, not a guarantee against overfitting. The problem must be well-posed in the first place, with sufficient data to support the model complexity.\n\nVerdict: **Incorrect**.\n\n**C. To test whether 3D classification features are driven by data rather than initial-model bias, one can rerun the entire gold-standard half-set classification with different random seeds and initial models, and then assess whether corresponding classes exhibit consistent features and occupancies across halves and repeats. Stability across independent halves and repeats is evidence against model bias.**\n\nThis statement addresses the problem of initial model bias. Iterative refinement algorithms, like those used in cryo-EM, are susceptible to converging to local minima. If the initial model possesses strong features that are not actually present in the data, the algorithm may fail to move away from this starting point and instead reinforce these spurious features using the noise in the data. This is distinct from overfitting noise from a random start, but is an equally dangerous source of artifacts.\nThe protocol described is a robust method to diagnose and refute this bias. Protocol $\\mathrm{P2}$ (gold-standard half-set classification) already validates features against overfitting to noise *within a single run*. By re-running the *entire* P2 procedure multiple times from different starting points (e.g., different ab-initio models, or a filtered sphere with different random seeds for assigning initial parameters), one tests the robustness of the result. If the classification is driven by the true signal, $S$, in the data, then independent runs should converge to the same set of conformational states. Consistency of the resulting classes, their structural features, and their relative populations (occupancies) across these independent repeats provides strong evidence that the result is a stable, global solution dictated by the data, not a fragile, local solution dictated by the starting bias.\n\nVerdict: **Correct**.\n\n**D. If particles are randomly split into two halves only after the final optimization in a single-set classification, then the inter-half correlation will still be a valid gold-standard estimate of resolution and feature reliability, because the split ensures statistical independence.**\n\nThis statement describes a flawed procedure that violates the principle of cross-validation. The \"gold-standard\" method requires that the two datasets being compared are independent throughout the entire modeling process.\nIn the described protocol, a single-set classification is performed first. This means all parameters—importantly, the orientation and class assignment for *every single particle*—are determined using the full dataset $\\{S+N\\}$. During this optimization, the algorithm has access to all particles, and the final parameters for particle $i$ are influenced by the data from particle $j$ (for all $i, j$). This means the specific noise realization $N$ across the entire dataset has been used to determine the final parameter set for all particles.\nIf one then splits the particles into two halves *after* this has been done, the parameter assignments for the two halves are not independent. They were derived from a common optimization process that saw both halves of the data. This creates an \"information leak.\" When two maps are reconstructed from these post-facto halves, they will exhibit inflated correlation because the noise was already partially \"fit\" in a shared manner during the initial, single-set classification. The split does not retroactively create the statistical independence required for a valid cross-validation. The split must be performed *before* any step of the optimization that is to be validated.\n\nVerdict: **Incorrect**.\n\n**E. Performing per-particle motion correction and contrast transfer function (CTF) refinement on each half-set while sharing alignment parameters across halves preserves the statistical independence needed for unbiased inter-half validation.**\n\nThis statement tests a nuanced understanding of \"statistical independence.\" In the gold-standard P2 protocol, any step that involves parameter optimization must be performed independently on each half-set.\nPer-particle motion correction and CTF refinement are indeed parameter optimization steps. They refine parameters (e.g., particle shifts, defocus, beam-tilt) to maximize some quality metric of the data. The statement proposes doing this on each half-set separately, which is correct, but *while sharing alignment parameters across halves*. The act of \"sharing\" parameters means that information from one half-set is used to determine the model for the other half-set. For instance, if a global alignment parameter or a parameter of a beam-tilt model is refined using particles from *both* half-sets simultaneously, then the noise $N_1$ from the first half influences the model applied to the second half (containing noise $N_2$), and vice-versa. This explicitly breaks the statistical independence between the processing of the two halves. To maintain independence, *all* parameter refinement for half-set $1$ must use *only* data from half-set $1$, and similarly for half-set $2$. Any communication or parameter sharing between the halves during optimization constitutes an information leak and invalidates the \"gold-standard\" nature of the cross-validation.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "2571522"}, {"introduction": "Our final practice shifts focus from static structures to the dynamic nature of biomolecules, which is often crucial for their function. Nuclear Magnetic Resonance (NMR) spectroscopy offers a powerful window into these motions, and this exercise immerses you in the analysis of Carr-Purcell-Meiboom-Gill (CPMG) relaxation dispersion data to quantify conformational exchange. By implementing a numerical fitting routine for the Bloch-McConnell equations, you will directly connect experimentally measured relaxation rates to the kinetic parameters of a dynamic process ($k_{\\text{ex}}$), a key skill in quantitative biophysics [@problem_id:2571520].", "problem": "You are given Carr–Purcell–Meiboom–Gill (CPMG) relaxation dispersion data from Nuclear Magnetic Resonance (NMR) for a single amide site undergoing two-site chemical exchange between states $A$ and $B$. Assume a physically faithful two-state Bloch–McConnell description with the following core facts as your starting point:\n- Two-site exchange is characterized by a total exchange rate $k_{\\mathrm{ex}} = k_{AB} + k_{BA}$ and equilibrium populations $p_A$ and $p_B$, with $p_A = 1 - p_B$, $k_{AB} = k_{\\mathrm{ex}} p_B$, and $k_{BA} = k_{\\mathrm{ex}} p_A$.\n- Let the intrinsic transverse relaxation rate (in the absence of exchange) be $R_{2,0}$ for both states ($A$ and $B$).\n- Let the chemical shift angular frequency difference between $B$ and $A$ be $\\Delta \\omega$ (in $\\mathrm{rad}\\,\\mathrm{s}^{-1}$). In a rotating frame at the population-weighted average frequency, the offsets are $\\Omega_A = - p_B \\Delta \\omega$ and $\\Omega_B = + p_A \\Delta \\omega$.\n- Under an ideal CPMG train with perfectly instantaneous refocusing pulses at frequency $\\nu_{\\mathrm{CPMG}}$ (in $\\mathrm{Hz}$), the offsets change sign every half-interval of duration $\\tau = 1 / (2 \\nu_{\\mathrm{CPMG}})$. Over one full period of duration $T = 1/\\nu_{\\mathrm{CPMG}}$, the evolution is the product of two propagators corresponding to $+\\Omega$ and $-\\Omega$ segments.\n- The transverse magnetization obeys a linear system $\\mathrm{d}\\mathbf{M}/\\mathrm{d}t = \\mathbf{A}(\\Omega)\\,\\mathbf{M}$ with a complex $2 \\times 2$ rate matrix\n$$\n\\mathbf{A}(\\Omega) =\n\\begin{bmatrix}\n-(R_{2,0} + k_{AB}) - i\\,\\Omega_A & k_{BA} \\\\\nk_{AB} & -(R_{2,0} + k_{BA}) - i\\,\\Omega_B\n\\end{bmatrix}.\n$$\n- Over a half-interval of length $\\tau$, the propagator is $\\exp(\\mathbf{A}(\\Omega)\\,\\tau)$. Over one full period, the propagator is $\\mathbf{U} = \\exp(\\mathbf{A}(+\\Omega)\\,\\tau)\\,\\exp(\\mathbf{A}(-\\Omega)\\,\\tau)$.\n- In the long-time limit, the effective transverse relaxation rate $R_{2,\\mathrm{eff}}(\\nu_{\\mathrm{CPMG}})$ equals the negative logarithm of the spectral radius of $\\mathbf{U}$ divided by the period length, that is\n$$\nR_{2,\\mathrm{eff}}(\\nu_{\\mathrm{CPMG}}) = -\\frac{1}{T} \\ln\\left(\\rho(\\mathbf{U})\\right) = - \\nu_{\\mathrm{CPMG}} \\ln\\left(\\rho(\\mathbf{U})\\right),\n$$\nwhere $\\rho(\\mathbf{U})$ is the largest magnitude among the eigenvalues of $\\mathbf{U}$.\n\nYour task is to write a complete program that, given a set of CPMG frequencies $\\nu_{\\mathrm{CPMG}}$ (in $\\mathrm{Hz}$) and corresponding $R_{2,\\mathrm{eff}}(\\nu_{\\mathrm{CPMG}})$ (in $\\mathrm{s}^{-1}$), estimates the four unknown parameters $\\theta = (k_{\\mathrm{ex}}, p_B, \\Delta \\omega, R_{2,0})$ by nonlinear least squares fitting of the model-predicted $R_{2,\\mathrm{eff}}(\\nu_{\\mathrm{CPMG}})$ to the provided data. All parameters must satisfy $k_{\\mathrm{ex}} > 0$, $0 < p_B < 0.5$, $\\Delta \\omega > 0$, and $R_{2,0} > 0$. All rates must be reported in $\\mathrm{s}^{-1}$, and angular frequencies in $\\mathrm{rad}\\,\\mathrm{s}^{-1}$. Express $p_B$ as a unitless decimal fraction.\n\nTest Suite:\nUse the following three test cases. For each case, first generate synthetic $R_{2,\\mathrm{eff}}(\\nu_{\\mathrm{CPMG}})$ data from the model using the specified “true” parameters and the given $\\nu_{\\mathrm{CPMG}}$ values (no noise), then fit to recover the parameters.\n\n- Test Case $1$ (general case):\n  - True parameters: $k_{\\mathrm{ex}} = 600\\,\\mathrm{s}^{-1}$, $p_B = 0.08$, $\\Delta \\omega = 800\\,\\mathrm{rad}\\,\\mathrm{s}^{-1}$, $R_{2,0} = 12\\,\\mathrm{s}^{-1}$.\n  - Frequencies: $\\nu_{\\mathrm{CPMG}} \\in \\{50, 100, 200, 400, 800, 1600\\}\\,\\mathrm{Hz}$.\n\n- Test Case $2$ (fast exchange with larger minor population):\n  - True parameters: $k_{\\mathrm{ex}} = 1500\\,\\mathrm{s}^{-1}$, $p_B = 0.15$, $\\Delta \\omega = 1000\\,\\mathrm{rad}\\,\\mathrm{s}^{-1}$, $R_{2,0} = 8\\,\\mathrm{s}^{-1}$.\n  - Frequencies: $\\nu_{\\mathrm{CPMG}} \\in \\{50, 100, 200, 400, 800, 1600\\}\\,\\mathrm{Hz}$.\n\n- Test Case $3$ (slow exchange, very small minor population):\n  - True parameters: $k_{\\mathrm{ex}} = 80\\,\\mathrm{s}^{-1}$, $p_B = 0.02$, $\\Delta \\omega = 500\\,\\mathrm{rad}\\,\\mathrm{s}^{-1}$, $R_{2,0} = 10\\,\\mathrm{s}^{-1}$.\n  - Frequencies: $\\nu_{\\mathrm{CPMG}} \\in \\{50, 100, 200, 400, 800, 1600\\}\\,\\mathrm{Hz}$.\n\nRequired output:\n- For each test case, estimate and return a list $[k_{\\mathrm{ex}}, p_B, \\Delta \\omega, R_{2,0}]$ (in that order), where all four entries are real numbers. Aggregate the three per-case lists into a single list of lists.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each inner list corresponds to a test case (for example, $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],[c_1,c_2,c_3,c_4]]$)).\n- All $k_{\\mathrm{ex}}$ and $R_{2,0}$ must be expressed in $\\mathrm{s}^{-1}$, and all $\\Delta \\omega$ must be expressed in $\\mathrm{rad}\\,\\mathrm{s}^{-1}$. The quantity $p_B$ must be returned as a unitless decimal fraction.", "solution": "We start from a two-site exchange system between states $A$ and $B$ with populations $p_A$ and $p_B$ satisfying $p_A = 1 - p_B$. The chemical exchange is characterized by forward and backward rates $k_{AB}$ and $k_{BA}$ related to the observable total exchange rate $k_{\\mathrm{ex}}$ by $k_{AB} = k_{\\mathrm{ex}} p_B$ and $k_{BA} = k_{\\mathrm{ex}} p_A$, so that $k_{\\mathrm{ex}} = k_{AB} + k_{BA}$ by definition. Intrinsic transverse relaxation in each state contributes a decay rate $R_{2,0}$, assumed equal for both sites in this problem.\n\nThe transverse magnetization $\\mathbf{M}(t)$ in the rotating frame obeys the Bloch–McConnell linear system\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\n\\begin{bmatrix}\nM_A(t) \\\\\nM_B(t)\n\\end{bmatrix}\n=\n\\mathbf{A}(\\Omega)\\,\n\\begin{bmatrix}\nM_A(t) \\\\\nM_B(t)\n\\end{bmatrix},\n$$\nwhere the complex rate matrix encodes relaxation, exchange, and chemical shift offsets relative to the rotating frame:\n$$\n\\mathbf{A}(\\Omega) =\n\\begin{bmatrix}\n-(R_{2,0} + k_{AB}) - i\\,\\Omega_A & k_{BA} \\\\\nk_{AB} & -(R_{2,0} + k_{BA}) - i\\,\\Omega_B\n\\end{bmatrix}.\n$$\nWe take the rotating frame to be at the population-weighted average angular frequency, so that the offsets in the two states are\n$$\n\\Omega_A = - p_B \\Delta \\omega, \\quad \\Omega_B = + p_A \\Delta \\omega,\n$$\nwith $\\Delta \\omega$ the angular frequency difference between $B$ and $A$, in $\\mathrm{rad}\\,\\mathrm{s}^{-1}$.\n\nUnder an ideal Carr–Purcell–Meiboom–Gill (CPMG) train with perfectly instantaneous $\\pi$ pulses applied at frequency $\\nu_{\\mathrm{CPMG}}$ (in $\\mathrm{Hz}$), the effective offsets switch sign every half-interval of duration $\\tau = 1 / (2 \\nu_{\\mathrm{CPMG}})$ due to toggling-frame inversion. Over each half-interval, the system evolves under a constant generator $\\mathbf{A}(\\pm \\Omega)$, producing a propagator $\\exp(\\mathbf{A}(\\pm \\Omega)\\,\\tau)$. Over one complete CPMG period of duration $T = 1 / \\nu_{\\mathrm{CPMG}}$ (composed of two half-intervals), the full-period propagator is\n$$\n\\mathbf{U} = \\exp(\\mathbf{A}(+\\Omega)\\,\\tau)\\,\\exp(\\mathbf{A}(-\\Omega)\\,\\tau).\n$$\nFor $N$ repeated periods, the evolution is governed by $\\mathbf{U}^N$. In the long-time limit, the asymptotic decay rate is controlled by the spectral radius $\\rho(\\mathbf{U})$ (the maximum modulus of the eigenvalues of $\\mathbf{U}$). If the magnitude of the dominant eigenvalue is $\\rho(\\mathbf{U}) < 1$, then after $N$ periods the magnetization magnitude has decreased by a factor $\\rho(\\mathbf{U})^N$, yielding a net exponential decay with effective rate\n$$\nR_{2,\\mathrm{eff}}(\\nu_{\\mathrm{CPMG}}) = -\\frac{1}{T} \\ln\\left(\\rho(\\mathbf{U})\\right) = - \\nu_{\\mathrm{CPMG}} \\ln\\left(\\rho(\\mathbf{U})\\right).\n$$\nThis expression follows directly from the definition of exponential decay per unit time when one period has duration $T$ and amplitude contraction factor $\\rho(\\mathbf{U})$.\n\nAlgorithmic design:\n- Given parameters $\\theta = (k_{\\mathrm{ex}}, p_B, \\Delta \\omega, R_{2,0})$ and a frequency $\\nu_{\\mathrm{CPMG}}$, construct $p_A = 1 - p_B$, $k_{AB} = k_{\\mathrm{ex}} p_B$, $k_{BA} = k_{\\mathrm{ex}} p_A$, and offsets $\\Omega_A = - p_B \\Delta \\omega$, $\\Omega_B = + p_A \\Delta \\omega$. Form the matrices $\\mathbf{A}(+\\Omega)$ and $\\mathbf{A}(-\\Omega)$ by substituting $\\pm \\Omega_A$ and $\\pm \\Omega_B$ into the diagonal imaginary terms.\n- Compute $\\tau = 1 / (2 \\nu_{\\mathrm{CPMG}})$ and $T = 1 / \\nu_{\\mathrm{CPMG}}$. Evaluate the matrix exponentials to obtain $\\mathbf{U} = \\exp(\\mathbf{A}(+\\Omega)\\,\\tau)\\,\\exp(\\mathbf{A}(-\\Omega)\\,\\tau)$.\n- Compute the eigenvalues of $\\mathbf{U}$ and take the spectral radius $\\rho(\\mathbf{U})$ as the largest magnitude of the eigenvalues. Compute $R_{2,\\mathrm{eff}}(\\nu_{\\mathrm{CPMG}}) = - (1/T) \\ln(\\rho(\\mathbf{U}))$.\n- For a given set of $\\nu_{\\mathrm{CPMG}}$ values, assemble the vector $R_{2,\\mathrm{eff}}(\\nu_{\\mathrm{CPMG}})$ predicted by the model.\n- For each test case, generate synthetic “observations” using the true parameters without noise. Then perform nonlinear least squares to estimate $\\theta$ by minimizing the sum of squared residuals between model predictions and synthetic observations, subject to physically meaningful bounds $k_{\\mathrm{ex}} > 0$, $0 < p_B < 0.5$, $\\Delta \\omega > 0$, $R_{2,0} > 0$. This optimization is well-posed because the mapping from $\\theta$ to $R_{2,\\mathrm{eff}}(\\nu_{\\mathrm{CPMG}})$ is smooth and the test suite spans a range of $\\nu_{\\mathrm{CPMG}}$ values, capturing dispersion curvature that informs $k_{\\mathrm{ex}}$, $p_B$, and $\\Delta \\omega$ while the high-$\\nu_{\\mathrm{CPMG}}$ limit anchors $R_{2,0}$.\n\nEdge cases and consistency checks:\n- If $\\Delta \\omega \\to 0$, then $\\Omega_A = \\Omega_B = 0$ and $\\mathbf{A}(+\\Omega) = \\mathbf{A}(-\\Omega)$, giving $\\mathbf{U} = \\exp(\\mathbf{A}\\,T)$ and $R_{2,\\mathrm{eff}} \\to R_{2,0}$ for equal $R_{2,0}$ in both states, consistent with no exchange broadening in the absence of chemical shift differences.\n- The algorithm ensures $R_{2,\\mathrm{eff}}(\\nu_{\\mathrm{CPMG}})$ is computed in $\\mathrm{s}^{-1}$. All input $\\nu_{\\mathrm{CPMG}}$ are in $\\mathrm{Hz}$, and $\\Delta \\omega$ is in $\\mathrm{rad}\\,\\mathrm{s}^{-1}$. Populations are unitless fractions.\n\nImplementation details:\n- Use numerical linear algebra to compute matrix exponentials and eigenvalues.\n- Use bound-constrained nonlinear least squares to estimate parameters from the generated synthetic data for each test case.\n\nThe program will output a single line with a list of three lists, each inner list containing the estimated $[k_{\\mathrm{ex}}, p_B, \\Delta \\omega, R_{2,0}]$ for one test case, with $k_{\\mathrm{ex}}$ and $R_{2,0}$ in $\\mathrm{s}^{-1}$, $\\Delta \\omega$ in $\\mathrm{rad}\\,\\mathrm{s}^{-1}$, and $p_B$ unitless (decimal fraction).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\nfrom scipy.optimize import least_squares\n\ndef r2eff_cpmg(nu_hz, kex, pB, dw_rad_s, R2_0):\n    \"\"\"\n    Compute R2,eff at a given CPMG frequency nu_hz for a two-site exchange model\n    using Bloch–McConnell propagators with toggling offsets.\n\n    Parameters:\n        nu_hz (float): CPMG refocusing frequency in Hz.\n        kex (float): total exchange rate k_ex in s^-1.\n        pB (float): minor population fraction (unitless, 0<pB<0.5).\n        dw_rad_s (float): angular frequency difference between states in rad/s.\n        R2_0 (float): intrinsic transverse relaxation rate in s^-1.\n\n    Returns:\n        float: R2,eff(nu_hz) in s^-1.\n    \"\"\"\n    pA = 1.0 - pB\n    kAB = kex * pB\n    kBA = kex * pA\n\n    # Offsets relative to population-weighted average\n    Omega_A = -pB * dw_rad_s\n    Omega_B = +pA * dw_rad_s\n\n    tau = 1.0 / (2.0 * nu_hz)  # half-interval\n    T = 1.0 / nu_hz            # full CPMG period\n\n    # Construct A(+Omega) and A(-Omega)\n    A_plus = np.array([\n        [-(R2_0 + kAB) - 1j * Omega_A,  kBA],\n        [kAB,                           -(R2_0 + kBA) - 1j * Omega_B]\n    ], dtype=complex)\n\n    A_minus = np.array([\n        [-(R2_0 + kAB) + 1j * Omega_A,  kBA],\n        [kAB,                           -(R2_0 + kBA) + 1j * Omega_B]\n    ], dtype=complex)\n\n    # Propagator over one full period\n    U = expm(A_plus * tau) @ expm(A_minus * tau)\n\n    # Spectral radius (largest magnitude eigenvalue)\n    eigvals = np.linalg.eigvals(U)\n    rho = np.max(np.abs(eigvals))\n\n    # Effective relaxation rate\n    R2eff = - (1.0 / T) * np.log(rho)\n\n    # Numerical guard: ensure real part (imaginary may appear at ~1e-15)\n    return float(np.real(R2eff))\n\n\ndef generate_dataset(nu_array, true_params):\n    \"\"\"Generate noiseless R2,eff data for given frequencies and true parameters.\"\"\"\n    kex, pB, dw, R2_0 = true_params\n    y = np.array([r2eff_cpmg(nu, kex, pB, dw, R2_0) for nu in nu_array], dtype=float)\n    return y\n\n\ndef fit_parameters(nu_array, r2eff_obs, bounds, x0):\n    \"\"\"\n    Fit parameters (kex, pB, dw, R2_0) by minimizing residuals between model and observed R2eff.\n\n    Parameters:\n        nu_array (array): frequencies in Hz.\n        r2eff_obs (array): observed R2eff in s^-1.\n        bounds (tuple): (lower_bounds, upper_bounds) arrays.\n        x0 (array): initial guess.\n\n    Returns:\n        array: fitted parameters [kex, pB, dw, R2_0].\n    \"\"\"\n    def residuals(theta):\n        kex, pB, dw, R2_0 = theta\n        preds = np.array([r2eff_cpmg(nu, kex, pB, dw, R2_0) for nu in nu_array], dtype=float)\n        return preds - r2eff_obs\n\n    res = least_squares(residuals, x0=x0, bounds=bounds, method=\"trf\", xtol=1e-12, ftol=1e-12, gtol=1e-12, max_nfev=10000)\n    return res.x\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (nu_values_Hz, true_params)\n    # true_params = (kex [s^-1], pB [unitless], dw [rad/s], R2_0 [s^-1])\n    nu_vals = np.array([50.0, 100.0, 200.0, 400.0, 800.0, 1600.0], dtype=float)\n\n    test_cases = [\n        (nu_vals, (600.0, 0.08, 800.0, 12.0)),     # Test Case 1\n        (nu_vals, (1500.0, 0.15, 1000.0, 8.0)),    # Test Case 2\n        (nu_vals, (80.0, 0.02, 500.0, 10.0)),      # Test Case 3\n    ]\n\n    # Parameter bounds: kex in [1e-3, 1e4], pB in (1e-6, 0.5), dw in [1.0, 5000.0], R2_0 in [0.1, 50.0]\n    lb = np.array([1e-3, 1e-6, 1.0, 0.1], dtype=float)\n    ub = np.array([1e4, 0.5, 5000.0, 50.0], dtype=float)\n\n    # Initial guess\n    x0 = np.array([300.0, 0.1, 700.0, 10.0], dtype=float)\n\n    results = []\n    for nu_array, true_params in test_cases:\n        # Generate noiseless observations\n        r2eff_obs = generate_dataset(nu_array, true_params)\n\n        # Fit\n        est = fit_parameters(nu_array, r2eff_obs, bounds=(lb, ub), x0=x0)\n\n        # For readability, format to a reasonable number of decimals\n        est_list = [float(est[0]), float(est[1]), float(est[2]), float(est[3])]\n        results.append(est_list)\n\n    # Final print statement in the exact required format.\n    # Convert to string with full precision of floats as default str()\n    print(f\"[{','.join('[' + ','.join(map(str, lst)) + ']' for lst in results)}]\")\n\nsolve()\n```", "id": "2571520"}]}