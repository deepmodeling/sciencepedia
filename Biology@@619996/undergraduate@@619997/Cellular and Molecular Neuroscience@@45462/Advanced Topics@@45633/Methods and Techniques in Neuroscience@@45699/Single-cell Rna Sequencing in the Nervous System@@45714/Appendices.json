{"hands_on_practices": [{"introduction": "This practice simulates a real-world experimental design challenge. It highlights the critical trade-off between the number of cells profiled and the sequencing depth per cell, a decision constrained by budget and essential for achieving specific scientific goals like the discovery of rare cell types. Mastering this balance is the first step toward a successful single-cell experiment [@problem_id:2350884].", "problem": "A neuroscientist is planning a single-cell RNA sequencing (scRNA-seq) experiment to discover a hypothesized rare population of inhibitory interneurons in the mouse hippocampus. Based on preliminary data, she estimates that this rare cell type constitutes approximately 0.1% of all cells in the dissected tissue. Her laboratory has a fixed budget of $10,000 for this specific experiment.\n\nThe total cost of the experiment has two main components:\n1.  **Cell Preparation Cost**: This includes cell dissociation, capture, and library preparation. The cost is a fixed $1.00 per cell.\n2.  **Sequencing Cost**: The cost for sequencing the prepared libraries is $5.00 per one million sequencing reads. A sequencing depth of at least 20,000 reads per cell is considered the minimum to reliably identify major cell types.\n\nThe primary scientific goal is to capture a sufficient number of the rare interneurons to confidently identify them as a distinct cluster after computational analysis. Given the budget and the scientific objective, which of the following experimental designs represents the best plan?\n\nA. Profile 2,000 cells with an average sequencing depth of 100,000 reads per cell.\n\nB. Profile 8,000 cells with an average sequencing depth of 25,000 reads per cell.\n\nC. Profile 5,000 cells with an average sequencing depth of 50,000 reads per cell.\n\nD. Profile 8,000 cells with an average sequencing depth of 100,000 reads per cell.\n\nE. Profile 100 cells with an average sequencing depth of 1,000,000 reads per cell.", "solution": "Let the rare-cell fraction be $p=0.001$, the total budget be $B=10000$, the cell preparation cost per cell be $c_{\\text{prep}}=1$, the sequencing cost per million reads be $c_{\\text{seqM}}=5$, and the minimum required sequencing depth per cell be $r_{\\min}=20000$.\n\nFor a design with $N$ cells and mean reads per cell $r$, the total reads are\n$$\nR = N r,\n$$\nthe sequencing cost is\n$$\nC_{\\text{seq}} = \\frac{R}{10^{6}} \\, c_{\\text{seqM}},\n$$\nthe preparation cost is\n$$\nC_{\\text{prep}} = N \\, c_{\\text{prep}},\n$$\nand the total cost is\n$$\nC = C_{\\text{prep}} + C_{\\text{seq}}.\n$$\nFeasibility requires $C \\leq B$ and $r \\geq r_{\\min}$. The expected number of rare cells captured is\n$$\n\\mathbb{E}[X] = p N,\n$$\nso among feasible options with $r \\geq r_{\\min}$, maximizing $N$ maximizes $\\mathbb{E}[X]$ and thus the ability to identify the rare cluster, provided depth is not below the threshold.\n\nEvaluate each option:\n\nOption A: $N=2000$, $r=100000$.\n$$\nR=2000 \\times 100000 = 2 \\times 10^{8},\\quad C_{\\text{seq}}=\\frac{2 \\times 10^{8}}{10^{6}} \\times 5 = 200 \\times 5 = 1000,\n$$\n$$\nC_{\\text{prep}}=2000 \\times 1=2000,\\quad C=2000+1000=3000 \\leq 10000,\\quad r=100000 \\geq 20000,\n$$\n$$\n\\mathbb{E}[X]=0.001 \\times 2000=2.\n$$\n\nOption B: $N=8000$, $r=25000$.\n$$\nR=8000 \\times 25000 = 2 \\times 10^{8},\\quad C_{\\text{seq}}=\\frac{2 \\times 10^{8}}{10^{6}} \\times 5 = 1000,\n$$\n$$\nC_{\\text{prep}}=8000 \\times 1=8000,\\quad C=8000+1000=9000 \\leq 10000,\\quad r=25000 \\geq 20000,\n$$\n$$\n\\mathbb{E}[X]=0.001 \\times 8000=8.\n$$\n\nOption C: $N=5000$, $r=50000$.\n$$\nR=5000 \\times 50000 = 2.5 \\times 10^{8},\\quad C_{\\text{seq}}=\\frac{2.5 \\times 10^{8}}{10^{6}} \\times 5 = 250 \\times 5 = 1250,\n$$\n$$\nC_{\\text{prep}}=5000 \\times 1=5000,\\quad C=5000+1250=6250 \\leq 10000,\\quad r=50000 \\geq 20000,\n$$\n$$\n\\mathbb{E}[X]=0.001 \\times 5000=5.\n$$\n\nOption D: $N=8000$, $r=100000$.\n$$\nR=8000 \\times 100000 = 8 \\times 10^{8},\\quad C_{\\text{seq}}=\\frac{8 \\times 10^{8}}{10^{6}} \\times 5 = 800 \\times 5 = 4000,\n$$\n$$\nC_{\\text{prep}}=8000 \\times 1=8000,\\quad C=8000+4000=12000 > 10000,\n$$\nnot feasible.\n\nOption E: $N=100$, $r=1000000$.\n$$\nR=100 \\times 1000000 = 1 \\times 10^{8},\\quad C_{\\text{seq}}=\\frac{1 \\times 10^{8}}{10^{6}} \\times 5 = 100 \\times 5 = 500,\n$$\n$$\nC_{\\text{prep}}=100 \\times 1=100,\\quad C=100+500=600 \\leq 10000,\\quad r=1000000 \\geq 20000,\n$$\n$$\n\\mathbb{E}[X]=0.001 \\times 100=0.1.\n$$\n\nAmong feasible options (A, B, C, E), the expected rare-cell counts are $2$, $8$, $5$, and $0.1$, respectively. All these meet $r \\geq r_{\\min}$. Therefore, the design that best serves the scientific objective of capturing enough rare interneurons is the one with the largest $N$ while maintaining at least the minimum depth, namely option B.", "answer": "$$\\boxed{B}$$", "id": "2350884"}, {"introduction": "Once data is generated, the first step is rigorous quality control to ensure your conclusions are based on healthy, representative cells. A common metric is the percentage of mitochondrial gene transcripts, where high values often indicate cellular stress or damage. This exercise asks you to think critically about the biological rationale behind this crucial filtering step, connecting a computational parameter to the underlying health of the cells being studied [@problem_id:2350931].", "problem": "A neuroscientist is performing a quality control analysis on a dataset from a single-cell RNA sequencing (scRNA-seq) experiment. The experiment aims to characterize the different cell types in the mouse hippocampus. The process of scRNA-seq involves dissociating the tissue into individual cells and then capturing the messenger RNA (mRNA) from each cell to measure its gene expression profile, or transcriptome.\n\nDuring the analysis, the scientist observes that a small subset of the captured cells has an unusually high percentage of transcripts that map to genes encoded by the mitochondrial genome, sometimes exceeding 30-40% of the total transcripts for that cell, whereas the vast majority of healthy-looking cells have a mitochondrial transcript percentage below 5%. Following standard practice, the scientist filters out and removes these high-mitochondrial-percentage cells from the dataset before proceeding with cell-type classification.\n\nWhat is the most likely biological reason that justifies filtering out cells with an unusually high percentage of mitochondrial gene expression?\n\nA. These cells are a rare subtype of neuron that is characterized by an extremely high metabolic rate, and they are removed to avoid biasing the analysis towards this specific cell type.\n\nB. These cells are likely damaged or dying (undergoing apoptosis), a state in which cytoplasmic mRNA is lost while mitochondrial mRNA is relatively retained, thus artificially inflating its proportional abundance.\n\nC. These cells have been contaminated by free-floating mitochondria from other cells that ruptured during the dissociation process, meaning the measured transcriptome is not from a single cell.\n\nD. The high percentage is a known technical artifact where the reagents used for reverse transcription have a much higher affinity for mitochondrial mRNA sequences than for nuclear-encoded mRNA.\n\nE. These cells represent a developmental stage where mitochondrial biogenesis is the dominant cellular process, and these precursor cells are not relevant to the study of the adult hippocampus.", "solution": "We analyze the observation using standard single-cell RNA-seq quality control principles. A high fraction of reads mapping to mitochondrial genes is widely used as a proxy for compromised cell integrity. When a cell is damaged or undergoing apoptosis, plasma membrane integrity is lost, cytoplasmic contents (including nuclear-encoded mRNAs) diffuse out or are degraded, and the total cytoplasmic mRNA content decreases. Mitochondria, being encapsulated by double membranes, tend to retain their transcripts better during early damage, and mitochondrial mRNAs are relatively preserved in such compromised cells. Thus, even if the absolute mitochondrial RNA does not increase, its proportional contribution to the remaining captured RNA rises. This produces cells with mitochondrial transcript fractions that can reach from $0.30$ to $0.40$, whereas healthy cells typically show values below $0.05$. Therefore, these cells are filtered to remove damaged or dying cells whose transcriptomes are not representative of physiological cell states.\n\nAssessing the options:\n- Option A is unlikely because even neurons with high metabolic rates do not exhibit mitochondrial-genomeâ€“encoded transcripts dominating their polyadenylated mRNA; most oxidative phosphorylation components are nuclear-encoded. Removing a legitimate rare subtype purely for high mitochondrial fraction would be inappropriate and is not the rationale behind this QC filter.\n- Option B matches the standard biological rationale: high mitochondrial fraction indicates damaged or apoptotic cells, where cytoplasmic mRNA loss inflates the relative mitochondrial proportion.\n- Option C is less consistent with the described situation; while ambient RNA or occasional organelle contamination can affect droplets, the canonical interpretation for high mitochondrial fraction within captured cells is cell damage rather than exogenous mitochondria dominating the library.\n- Option D is incorrect; there is no standard technical artifact wherein reverse transcription preferentially amplifies mitochondrial mRNA across cells to produce this pattern. The correlation of high mitochondrial fraction with other damage markers supports a biological, not reagent-based, explanation.\n- Option E is implausible for adult hippocampus and, moreover, mitochondrial biogenesis does not cause mitochondrial-genome transcripts to constitute from $0.30$ to $0.40$ of total mRNA.\n\nThus, the most likely biological reason justifying the filtering is that these cells are damaged or dying, inflating the relative abundance of mitochondrial transcripts.", "answer": "$$\\boxed{B}$$", "id": "2350931"}, {"introduction": "Single-cell RNA sequencing's great power lies in its ability to resolve cellular diversity, but sometimes an initial analysis only reveals broad cell classes. This exercise presents a common scenario where a large cell cluster, like astrocytes, appears homogeneous at first glance but is suspected to contain hidden subtypes. It introduces the essential computational strategy of sub-clustering to \"zoom in\" on a specific population and uncover finer levels of biological heterogeneity [@problem_id:2350918].", "problem": "A research team is studying cellular diversity in the hippocampus of an adult mouse using Single-cell RNA sequencing (scRNA-seq). After processing the raw sequencing data, they perform a standard computational analysis which includes data normalization, identification of highly variable genes, dimensionality reduction using Principal Component Analysis (PCA), and finally, cell clustering. The clusters are visualized using Uniform Manifold Approximation and Projection (UMAP).\n\nThis initial analysis successfully separates major cell types, including neurons, microglia, and oligodendrocytes. One particularly large and diffuse cluster is confidently identified as representing astrocytes due to the robust and specific expression of the canonical astrocyte marker gene, `Aldh1l1`, across all cells within that cluster. The researchers hypothesize that the broad, spread-out nature of this UMAP cluster indicates the presence of multiple, transcriptionally distinct astrocyte subtypes, which were not resolved by the initial coarse-grained analysis of the entire dataset.\n\nTo investigate this hypothesis and better resolve the heterogeneity within the astrocyte population, what is the most appropriate and targeted computational next step for the team to take with their existing dataset?\n\nA. Perform differential gene expression analysis between the astrocyte cluster and all other non-astrocyte clusters combined to identify a more comprehensive list of pan-astrocyte-specific genes.\n\nB. Use a different algorithm, such as t-distributed Stochastic Neighbor Embedding (t-SNE), to re-visualize the clustering results from the full dataset, hoping it will visually separate the astrocyte subtypes.\n\nC. Conclude that the diffuse cluster is a technical artifact caused by poor cell capture and remove all cells belonging to this cluster before re-running the entire analysis pipeline.\n\nD. Isolate the cells belonging to the identified astrocyte cluster to create a new, subsetted data object, and then re-execute the analysis pipeline (i.e., re-calculate highly variable genes, re-run PCA, and re-cluster) exclusively on this astrocyte-only subset.\n\nE. Return to the original analysis of the full dataset and simply increase the resolution parameter of the clustering algorithm, forcing it to generate a much larger number of smaller clusters across all cell types.", "solution": "Goal: Resolve putative astrocyte subtype heterogeneity suggested by a large, diffuse UMAP cluster that robustly expresses the canonical astrocyte marker Aldh1l1.\n\nStep 1: Interpret the observation and define the analytical need.\n- The robust, specific expression of Aldh1l1 across the diffuse cluster supports that these cells are bona fide astrocytes rather than a technical artifact.\n- Diffuseness on UMAP commonly reflects continuous or multi-branch transcriptional variation. Resolving such withinâ€“cell-type heterogeneity requires focusing the variance modeling and dimensionality reduction on that cell type.\n\nStep 2: Apply the core principle of variance modeling and dimensionality reduction in scRNA-seq.\n- In standard workflows, highly variable gene (HVG) selection across the full dataset is dominated by interâ€“cell-type differences. Consequently, $G_{\\text{HVG,global}}$ emphasizes genes separating neurons, microglia, oligodendrocytes, and astrocytes, not genes that vary within astrocytes.\n- To resolve intra-astrocyte structure, one must recompute $G_{\\text{HVG}}$ within the astrocyte subset, i.e., $G_{\\text{HVG,astro}}$, then re-run PCA on the astrocyte-only expression matrix restricted to $G_{\\text{HVG,astro}}$, construct a $k$-nearest-neighbor graph among astrocytes, and perform clustering and UMAP on this subset. This increases sensitivity to astrocyte-specific axes of variation and is standard best practice in Seurat/Scanpy subclustering workflows.\n\nStep 3: Evaluate the proposed options against the analytical need.\n- Option A (DE astrocyte vs. non-astrocyte): This identifies pan-astrocyte markers, not genes that distinguish astrocyte subtypes; it does not improve subtype resolution.\n- Option B (switch to t-SNE on full dataset): Changing visualization without recomputing HVGs and PCA on the relevant subset will not reliably expose subtype structure; the underlying features remain dominated by global differences.\n- Option C (discard astrocyte cluster as artifact): Inconsistent with strong Aldh1l1 expression indicating genuine astrocytes; removes biologically relevant data without evidence.\n- Option E (increase global clustering resolution): This forces more clusters across all cell types and risks overclustering driven by interâ€“cell-type HVGs; it does not target astrocyte-specific variance and may fragment other populations unnecessarily.\n- Option D (subset astrocytes and re-run HVG, PCA, clustering): Directly addresses the need by modeling variance within astrocytes, maximizing power to detect subtypes and is the standard targeted approach.\n\nConclusion: The most appropriate next step is to subset the astrocyte cluster and re-run the full analysis pipeline on this subset to resolve intra-astrocyte heterogeneity.", "answer": "$$\\boxed{D}$$", "id": "2350918"}]}