{"hands_on_practices": [{"introduction": "Replica Exchange Molecular Dynamics (REMD) is a powerful method for overcoming high energy barriers by running multiple parallel simulations at different temperatures. Conformations are periodically swapped between these simulations, allowing the system to explore a wide range of states at high temperatures and then sample low-energy basins at low temperatures. This exercise [@problem_id:2109780] delves into the most critical setup decision in REMD: choosing the temperature spacing between adjacent replicas. Understanding this fundamental trade-off is key to designing an efficient simulation that balances computational cost with effective conformational sampling.", "problem": "A researcher is investigating the folding pathway of a small, 15-residue peptide using computational methods. Standard Molecular Dynamics (MD) simulations at physiological temperature (300 K) repeatedly get trapped in kinetically stable, non-native conformations, failing to sample the folded state within a reasonable simulation time. To overcome this sampling problem, the researcher decides to employ an enhanced sampling technique called Replica Exchange Molecular Dynamics (REMD).\n\nIn REMD, multiple non-interacting copies, or \"replicas,\" of the system are simulated in parallel, each at a different temperature from a predefined temperature ladder, $T_1 < T_2 < ... < T_N$. Periodically, an attempt is made to swap the spatial coordinates between replicas at adjacent temperatures (e.g., between $T_i$ and $T_{i+1}$). The success of these swaps allows conformations generated at high temperatures (which can easily cross energy barriers) to diffuse down to low temperatures, thereby enhancing the exploration of the conformational space for the replica at the temperature of interest, $T_1$.\n\nThe researcher needs to set up the temperature ladder for the REMD simulation, which will span from 300 K to 450 K. They are considering two general strategies for choosing the temperature difference, $\\Delta T = T_{i+1} - T_i$, between adjacent replicas:\n\n1.  **A small, constant $\\Delta T$**: This would require a large number of replicas to cover the range from 300 K to 450 K.\n2.  **A large, constant $\\Delta T$**: This would require a small number of replicas to cover the same range.\n\nWhich of the following statements most accurately describes the fundamental trade-off between these two strategies for setting up the REMD simulation?\n\nA. A large $\\Delta T$ is always preferable because it requires fewer replicas, saving computational cost. The reduced swap success is a minor issue that can be compensated for by running the simulation for a longer time.\n\nB. A small $\\Delta T$ is optimal because it guarantees that the high-temperature replicas explore a wider range of conformations, which then rapidly fold upon being swapped to a low temperature. The large number of replicas is a necessary cost for accuracy.\n\nC. A small $\\Delta T$ increases the probability of successful swaps between adjacent replicas, allowing a conformation to efficiently \"travel\" across the temperature range, but it requires many replicas. Conversely, a large $\\Delta T$ uses fewer replicas but drastically reduces the swap acceptance probability, potentially isolating the replicas and negating the benefit of the method.\n\nD. The choice of $\\Delta T$ is irrelevant to the simulation's efficiency; the only important factor is the total range of temperatures covered. Both strategies will converge to the same result in the same amount of time, provided the highest temperature is sufficient to overcome all energy barriers.\n\nE. A large $\\Delta T$ is dangerous because it can cause numerical instability in the simulation integrator, leading to crashes. A small $\\Delta T$ ensures stability but slows down sampling because the replicas are too similar in energy.", "solution": "In Replica Exchange Molecular Dynamics (REMD), each replica at temperature $T_{i}$ samples the canonical distribution with Boltzmann weight proportional to $\\exp(-\\beta_{i} E)$, where $\\beta_{i} = \\frac{1}{k_{B} T_{i}}$ and $k_{B}$ is the Boltzmann constant. The Metropolis acceptance probability to swap configurations with energies $E_{i}$ and $E_{i+1}$ between adjacent replicas at $\\beta_{i}$ and $\\beta_{i+1}$ is\n$$\nP_{\\text{acc}} = \\min\\left(1, \\exp\\left[(\\beta_{i} - \\beta_{i+1})(E_{i+1} - E_{i})\\right]\\right).\n$$\nThis expression shows that $P_{\\text{acc}}$ decreases as $|\\beta_{i} - \\beta_{i+1}|$ increases, because the factor multiplying the energy difference grows in magnitude with the temperature gap. The average acceptance depends on the overlap of the energy distributions at the two temperatures. Approximating the energy distribution at $T$ as Gaussian with variance\n$$\n\\sigma_{E}^{2}(T) = k_{B} T^{2} C_{V}(T),\n$$\nwhere $C_{V}$ is the heat capacity, one finds that the acceptance probability is a decreasing function of $|\\Delta \\beta| \\sigma_{E}$, with $\\Delta \\beta = \\beta_{i+1} - \\beta_{i}$. Since $C_{V}$ is extensive, $\\sigma_{E} \\propto \\sqrt{N}$ for a system with $N$ degrees of freedom, and thus maintaining a target acceptance requires\n$$\n|\\Delta \\beta| \\sim \\frac{1}{\\sigma_{E}} \\propto \\frac{1}{\\sqrt{N}}.\n$$\nIn terms of temperature spacing, this implies that a small $\\Delta T = T_{i+1} - T_{i}$ is needed to keep $|\\Delta \\beta|$ small enough for reasonable swap acceptance, especially at lower temperatures where the distributions are narrower. Consequently, using a small constant $\\Delta T$ increases the probability of successful swaps, enabling configurations to perform an efficient random walk in temperature space and thus diffuse from high to low temperatures, but it requires many replicas to span a fixed range from the lowest to the highest temperature.\n\nConversely, choosing a large constant $\\Delta T$ reduces the number of replicas needed to cover the same temperature range but makes $|\\Delta \\beta|$ large, which drastically reduces $P_{\\text{acc}}$. Poor acceptance impedes temperature-space diffusion, effectively isolating replicas at their respective temperatures and negating the main benefit of REMD. Running longer does not fundamentally resolve this, because the round-trip time in temperature space scales unfavorably as acceptance decreases; if exchanges rarely occur, configurations cannot effectively migrate between temperatures.\n\nTherefore, the fundamental trade-off is that small $\\Delta T$ improves swap acceptance and mixing at the cost of more replicas (and computational expense), while large $\\Delta T$ reduces computational cost in replicas but can cripple mixing by lowering acceptance. Options asserting that $\\Delta T$ is irrelevant, that large $\\Delta T$ merely requires longer runs, that small $\\Delta T$ alone “guarantees” optimal behavior, or that $\\Delta T$ affects integrator stability, contradict the acceptance criterion and its dependence on distribution overlap.\n\nThus, the most accurate statement is the one that explicitly describes this trade-off between swap acceptance and number of replicas.", "answer": "$$\\boxed{C}$$", "id": "2109780"}, {"introduction": "Unlike REMD, umbrella sampling focuses on mapping the free energy landscape along a specific, predefined reaction coordinate. The method involves running a series of simulations, or \"windows,\" each one applying a harmonic restraint to hold the system in a particular region of the coordinate. This practice problem [@problem_id:2109822] addresses one of the most common and critical pitfalls in this technique: ensuring sufficient overlap in the sampled conformations between adjacent windows. Properly managing this overlap is essential for accurately reconstructing a smooth Potential of Mean Force (PMF) and avoiding physically unrealistic artifacts.", "problem": "An undergraduate researcher is performing a computational study to map the energy landscape of a protein-ligand binding process. They are using umbrella sampling, an enhanced sampling technique in Molecular Dynamics (MD), to calculate the one-dimensional Potential of Mean Force (PMF). The PMF describes the free energy of the system as a function of a specific reaction coordinate, which in this case is the distance between the center of mass of the ligand and the center of mass of the protein's binding pocket.\n\nTo achieve this, the researcher runs a series of separate MD simulations, known as \"windows.\" In each window $i$, a harmonic biasing potential, $w_i(z) = \\frac{1}{2}k(z - z_i)^2$, is applied. Here, $z$ is the reaction coordinate value, $k$ is the force constant of the harmonic spring, and $z_i$ is the center of the restraining potential for that specific window. After completing the simulations, the researcher attempts to combine the biased probability distributions from all windows to reconstruct the unbiased PMF using the Weighted Histogram Analysis Method (WHAM).\n\nUpon analysis, the resulting PMF profile is found to be physically unrealistic, showing large, sharp discontinuities and extremely high statistical errors in the regions of the reaction coordinate that lie between the window centers $z_i$. The researcher has confirmed that the simulation time for each individual window was sufficient to achieve proper sampling of the states constrained by its respective harmonic potential.\n\nWhich of the following setup choices is the most direct cause for this specific failure in the PMF reconstruction?\n\nA. The force constant, $k$, of the harmonic restraining potential was chosen to be too small.\n\nB. The total number of windows used to span the reaction coordinate was too low.\n\nC. The temperature of the simulation was set too high, leading to excessive thermal fluctuations.\n\nD. The spacing between the centers of adjacent windows, $|z_{i+1} - z_i|$, was too large compared to the sampling width within each window.\n\nE. The data from the beginning of each simulation window was not discarded to account for equilibration of the system under the biasing potential.", "solution": "We are reconstructing a one-dimensional PMF $F(z)$ along reaction coordinate $z$ via umbrella sampling and WHAM. In window $i$, a harmonic bias $w_{i}(z)=\\frac{1}{2}k(z-z_{i})^{2}$ is applied. The unbiased PMF is related to the unbiased probability density by $F(z)=-k_{B}T\\ln P(z)+C$, where $\\beta=\\frac{1}{k_{B}T}$ and $C$ is an arbitrary additive constant.\n\nUnder the harmonic bias in window $i$, the sampled (biased) distribution is\n$$\nP_{i}^{\\text{bias}}(z)\\propto \\exp\\left\\{-\\beta\\left[F(z)+\\tfrac{1}{2}k(z-z_{i})^{2}\\right]\\right\\}.\n$$\nWHAM combines all $P_{i}^{\\text{bias}}(z)$ by reweighting to remove $w_{i}(z)$ and solving for normalization free energies so that the histograms agree in their overlap regions. Therefore, accurate and stable WHAM reconstruction requires sufficient overlap of adjacent biased histograms.\n\nTo quantify overlap, expand the PMF locally around $z_{i}$:\n$$\nF(z)\\approx F(z_{i})+\\tfrac{1}{2}F''(z_{i})(z-z_{i})^{2},\n$$\nso that near $z_{i}$ the effective curvature in window $i$ is $k_{\\text{eff}}=k+F''(z_{i})$, and the biased distribution is approximately Gaussian with variance\n$$\n\\sigma_{i}^{2}=\\frac{k_{B}T}{k_{\\text{eff}}}=\\frac{k_{B}T}{k+F''(z_{i})}.\n$$\nIf $|z_{i+1}-z_{i}|$ is large compared to $\\sigma_{i}$ and $\\sigma_{i+1}$, then adjacent histograms have negligible overlap. In that case, the WHAM equations become ill-conditioned between window centers, producing large statistical uncertainties and sharp discontinuities specifically in the inter-window regions—precisely the observed failure mode.\n\nNow evaluate the options:\n- A: Making $k$ too small increases $\\sigma_{i}$ (since $\\sigma_{i}\\sim\\sqrt{k_{B}T/k}$ when $|F''|\\ll k$), which improves overlap; it does not cause the observed discontinuities.\n- B: Too few windows often implies large spacing, but the direct cause of the failure is not the count per se; it is insufficient overlap relative to each window’s sampling width.\n- C: Higher temperature increases $\\sigma_{i}$ and thus improves overlap; it would not typically create the described gaps.\n- D: Excessive spacing $|z_{i+1}-z_{i}|$ relative to the sampling width $\\sigma_{i}$ is exactly the condition that destroys histogram overlap and yields discontinuities and large errors between window centers.\n- E: Not discarding initial equilibration data can bias estimates within windows but does not specifically cause systematic discontinuities localized between window centers when sufficient sampling within windows is confirmed.\n\nTherefore, the most direct cause is that the spacing between adjacent window centers is too large compared to the sampling width in each window.\n\nThe correct choice is D.", "answer": "$$\\boxed{D}$$", "id": "2109822"}, {"introduction": "Metadynamics is another widely used technique that, like umbrella sampling, reconstructs a free energy surface along chosen collective variables. It achieves this by adaptively adding a history-dependent bias potential that discourages the system from revisiting previously explored states. The resulting simulation trajectory is therefore inherently biased. This hands-on calculation [@problem_id:2109798] demonstrates the essential final step in a metadynamics study: how to use the converged bias potential to reweight the biased trajectory and recover the true, unbiased thermodynamic average of a physical observable, a core skill for extracting meaningful results.", "problem": "A computational biophysics student is analyzing the results of a molecular dynamics simulation of a small peptide in aqueous solution at a temperature of $300$ K. To overcome a large energy barrier and properly sample the transition between the peptide's compact native state and its extended unfolded state, a simulation technique called Metadynamics was employed. The simulation has successfully converged, meaning the conformational space has been exhaustively explored.\n\nThe Metadynamics simulation was run using a single Collective Variable (CV), denoted by $s$, which represents the peptide's radius of gyration. A smaller value of $s$ corresponds to a more compact structure. From the converged simulation, the total accumulated bias potential, $V_{bias}(s)$, has been determined. It is given by the following function, where $s$ is dimensionless and $V_{bias}$ is in units of kJ/mol:\n$$ V_{bias}(s) = 10 \\exp\\left(-2(s-0.8)^2\\right) + 8 \\exp\\left(-1.5(s-2.0)^2\\right) $$\nThe thermal energy for the system is given as $k_B T = 2.5$ kJ/mol.\n\nThe student has a segment of the biased trajectory data, which records the value of the CV, $s_i$, and the corresponding instantaneous number of intramolecular hydrogen bonds, $N_{HB}(i)$, at several frames $i$:\n\n| Frame ($i$) | CV value ($s_i$) | Number of H-bonds ($N_{HB}(i)$) |\n|:-----------:|:----------------:|:--------------------------------:|\n|      1      |       0.85       |                8                 |\n|      2      |       2.10       |                2                 |\n|      3      |       1.50       |                4                 |\n|      4      |       0.75       |                9                 |\n|      5      |       1.95       |                3                 |\n\nYour task is to calculate the true thermodynamic average number of hydrogen bonds, $\\langle N_{HB} \\rangle_{native}$, for the peptide when it is in its native state conformational ensemble. The native state is defined as the set of all conformations for which the radius of gyration $s$ is less than or equal to $1.2$.\n\nExpress your answer rounded to three significant figures.", "solution": "The metadynamics simulation adds a bias potential $V_{bias}(s)$ to the potential energy. The biased distribution is therefore proportional to $\\exp[-\\beta(U+V_{bias})]$, where $\\beta = 1/(k_{B}T)$. To recover unbiased ensemble averages from a biased trajectory, each frame $i$ is reweighted by the factor $w_{i} = \\exp\\left(\\beta V_{bias}(s_{i})\\right)$. Restricting to the native ensemble defined by $s \\leq 1.2$, the unbiased thermodynamic average of the number of hydrogen bonds is\n$$\n\\langle N_{HB} \\rangle_{native} = \\frac{\\sum_{i:\\, s_{i} \\leq 1.2} N_{HB}(i)\\,\\exp\\left(\\frac{V_{bias}(s_{i})}{k_{B}T}\\right)}{\\sum_{i:\\, s_{i} \\leq 1.2} \\exp\\left(\\frac{V_{bias}(s_{i})}{k_{B}T}\\right)}.\n$$\nFrom the provided frames, only $i=1$ with $s_{1}=0.85$ and $i=4$ with $s_{4}=0.75$ are in the native set. With\n$$\nV_{bias}(s) = 10\\,\\exp\\!\\left(-2(s-0.8)^{2}\\right) + 8\\,\\exp\\!\\left(-1.5(s-2.0)^{2}\\right),\n$$\nwe evaluate\n$$\nV_{bias}(0.85) = 10\\,\\exp\\!\\left(-2(0.05)^{2}\\right) + 8\\,\\exp\\!\\left(-1.5(1.15)^{2}\\right)\n= 10\\,\\exp(-0.005) + 8\\,\\exp(-1.98375),\n$$\n$$\nV_{bias}(0.75) = 10\\,\\exp\\!\\left(-2(-0.05)^{2}\\right) + 8\\,\\exp\\!\\left(-1.5(1.25)^{2}\\right)\n= 10\\,\\exp(-0.005) + 8\\,\\exp(-2.34375).\n$$\nNumerically,\n$$\n10\\,\\exp(-0.005) \\approx 9.9501247919,\\quad 8\\,\\exp(-1.98375) \\approx 1.1004195781,\\quad 8\\,\\exp(-2.34375) \\approx 0.7677489766,\n$$\nso\n$$\nV_{bias}(0.85) \\approx 11.0505443681,\\quad V_{bias}(0.75) \\approx 10.7178737685.\n$$\nWith $k_{B}T=2.5$ kJ/mol, we have $\\beta = \\frac{1}{k_{B}T} = 0.4\\,(\\text{mol/kJ})$. The weights are $w_{1}=\\exp(\\beta V_{bias}(0.85))$ and $w_{4}=\\exp(\\beta V_{bias}(0.75))$, and their ratio is\n$$\nr \\equiv \\frac{w_{1}}{w_{4}} = \\exp\\!\\left[\\beta\\big(V_{bias}(0.85)-V_{bias}(0.75)\\big)\\right] = \\exp\\!\\left(0.4\\times 0.3326705996\\right) \\approx \\exp(0.1330682398) \\approx 1.142327.\n$$\nThe native-state average is\n$$\n\\langle N_{HB} \\rangle_{native} = \\frac{8\\,w_{1} + 9\\,w_{4}}{w_{1}+w_{4}} = \\frac{8r + 9}{r+1} = 8 + \\frac{1}{1+r}.\n$$\nSubstituting $r \\approx 1.142327$ gives\n$$\n\\langle N_{HB} \\rangle_{native} \\approx 8 + \\frac{1}{2.142327} \\approx 8.4668,\n$$\nwhich, rounded to three significant figures, is $8.47$.", "answer": "$$\\boxed{8.47}$$", "id": "2109798"}]}