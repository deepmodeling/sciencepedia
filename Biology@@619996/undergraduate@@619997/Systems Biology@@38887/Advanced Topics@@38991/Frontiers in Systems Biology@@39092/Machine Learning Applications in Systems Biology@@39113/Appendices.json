{"hands_on_practices": [{"introduction": "Real-world biological data, like gene expression profiles, are rarely perfect. Missing values due to experimental error or other issues are a common challenge that must be addressed before any meaningful analysis or model training can occur. This exercise introduces one of the simplest yet most fundamental techniques for handling missing data: mean imputation. By working through this calculation, you will gain hands-on experience with a critical data preprocessing step, understanding how to make a dataset whole so that more complex machine learning algorithms can be applied [@problem_id:1443758].", "problem": "A researcher in systems biology is preparing a gene expression dataset for a machine learning model. The dataset contains expression levels for three genes (Gene A, Gene B, Gene C) across four patients (Patient 1, Patient 2, Patient 3, Patient 4).\n\nThe recorded expression levels are as follows:\n- For Gene A: 10.2 (Patient 1), 9.8 (Patient 2), 11.1 (Patient 3), 10.5 (Patient 4).\n- For Gene B: 5.4 (Patient 1), 6.1 (Patient 2), a missing value (Patient 3), 5.9 (Patient 4).\n- For Gene C: 1.3 (Patient 1), 0.9 (Patient 2), 1.5 (Patient 3), 1.1 (Patient 4).\n\nTo complete the dataset, the researcher decides to use gene-wise mean imputation. This technique involves replacing a missing value for a specific gene with the arithmetic mean of all *observed* expression values for that same gene across the other patients.\n\nFollowing this procedure, calculate the imputed expression value for Gene B in Patient 3. Report your answer as a numerical value rounded to three significant figures.", "solution": "Gene-wise mean imputation replaces a missing value for a given gene with the arithmetic mean of the observed values for that gene across other patients. Let the observed Gene B expression values be $x_{B,1} = 5.4$, $x_{B,2} = 6.1$, and $x_{B,4} = 5.9$, with $n = 3$ observed entries. The imputed value for Patient 3 is the sample mean:\n$$\n\\hat{x}_{B,3} = \\frac{1}{n}\\sum_{i \\in \\{1,2,4\\}} x_{B,i} = \\frac{5.4 + 6.1 + 5.9}{3} = \\frac{17.4}{3} = 5.8.\n$$\nRounded to three significant figures, this is $5.80$.", "answer": "$$\\boxed{5.80}$$", "id": "1443758"}, {"introduction": "A key task in systems biology is to identify which factors, such as genetic variations, have the most significant impact on a biological outcome like drug response. Decision tree models are excellent tools for this, as they build a hierarchy of predictive features. This practice dives into the core algorithm of a decision tree by using information gain, a concept from information theory, to quantitatively measure how much a given feature reduces our uncertainty about the outcome. Calculating information gain helps you understand how a machine learning model can automatically discover the most important variables from a dataset, a crucial skill in navigating the high-dimensional data common in pharmacogenomics [@problem_id:1443753].", "problem": "In a pharmacogenomic study, researchers are investigating the influence of genetic variations on patient response to a new drug. They have collected data from 10 patients, focusing on three specific biallelic Single Nucleotide Polymorphisms (SNPs), which are variations at a single position in a DNA sequence. For each SNP, the allele is represented by either 0 or 1. The drug response for each patient is categorized as either 'High' or 'Low'.\n\nThe dataset is provided below:\n\n| Patient ID | SNP1 | SNP2 | SNP3 | Drug Response |\n|------------|------|------|------|---------------|\n| 1          | 0    | 0    | 1    | Low           |\n| 2          | 1    | 0    | 0    | Low           |\n| 3          | 0    | 0    | 1    | High          |\n| 4          | 1    | 0    | 0    | Low           |\n| 5          | 0    | 1    | 1    | High          |\n| 6          | 1    | 1    | 0    | High          |\n| 7          | 0    | 1    | 1    | High          |\n| 8          | 1    | 1    | 0    | High          |\n| 9          | 0    | 0    | 1    | Low           |\n| 10         | 1    | 1    | 1    | Low           |\n\nTo build a predictive model, a decision tree algorithm is chosen. The first step is to select the most informative SNP to serve as the root of the tree. This is determined by calculating the information gain for each SNP.\n\nCalculate the information gain for the SNP that provides the most information for classifying drug response. Your calculation should use the logarithm to the base 2. Round your final answer to three significant figures.", "solution": "We model the drug response $Y \\in \\{\\text{High}, \\text{Low}\\}$ and features $A \\in \\{\\text{SNP1, SNP2, SNP3}\\}$ with binary values $v \\in \\{0,1\\}$. The entropy of a binary distribution with probabilities $p$ and $1-p$ (using base-2 logarithms) is\n$$\nH(Y) = -\\sum_{y} P(y)\\log_{2}P(y).\n$$\nThe information gain of a feature $A$ is\n$$\nIG(A) = H(Y) - \\sum_{v \\in \\{0,1\\}} P(A=v)\\,H(Y \\mid A=v),\n$$\nwhere\n$$\nH(Y \\mid A=v) = -\\sum_{y} P(y \\mid A=v)\\log_{2}P(y \\mid A=v).\n$$\n\nFrom the dataset, there are $5$ High and $5$ Low responses, so\n$$\nH(Y) = -\\left(\\frac{1}{2}\\log_{2}\\frac{1}{2} + \\frac{1}{2}\\log_{2}\\frac{1}{2}\\right) = 1.\n$$\n\nFor SNP1: The value distribution is $P(\\text{SNP1}=0)=\\frac{5}{10}$ and $P(\\text{SNP1}=1)=\\frac{5}{10}$. For $\\text{SNP1}=0$, the class counts are High $3$, Low $2$, giving\n$$\nH(Y \\mid \\text{SNP1}=0) = -\\frac{3}{5}\\log_{2}\\frac{3}{5} - \\frac{2}{5}\\log_{2}\\frac{2}{5}.\n$$\nFor $\\text{SNP1}=1$, the class counts are High $2$, Low $3$, so\n$$\nH(Y \\mid \\text{SNP1}=1) = -\\frac{2}{5}\\log_{2}\\frac{2}{5} - \\frac{3}{5}\\log_{2}\\frac{3}{5},\n$$\nwhich is equal to the previous entropy. Therefore\n$$\nH(Y \\mid \\text{SNP1}) = \\frac{1}{2}\\left[-\\frac{3}{5}\\log_{2}\\frac{3}{5} - \\frac{2}{5}\\log_{2}\\frac{2}{5}\\right] + \\frac{1}{2}\\left[-\\frac{2}{5}\\log_{2}\\frac{2}{5} - \\frac{3}{5}\\log_{2}\\frac{3}{5}\\right]\n= -\\frac{3}{5}\\log_{2}\\frac{3}{5} - \\frac{2}{5}\\log_{2}\\frac{2}{5}.\n$$\nHence\n$$\nIG(\\text{SNP1}) = 1 - \\left[-\\frac{3}{5}\\log_{2}\\frac{3}{5} - \\frac{2}{5}\\log_{2}\\frac{2}{5}\\right].\n$$\nNumerically, $H(Y \\mid \\text{SNP1}) \\approx 0.970950594$, so $IG(\\text{SNP1}) \\approx 0.029049406$.\n\nFor SNP2: The value distribution is $P(\\text{SNP2}=0)=\\frac{5}{10}$ and $P(\\text{SNP2}=1)=\\frac{5}{10}$. For $\\text{SNP2}=0$, class counts are High $1$, Low $4$, so\n$$\nH(Y \\mid \\text{SNP2}=0) = -\\frac{1}{5}\\log_{2}\\frac{1}{5} - \\frac{4}{5}\\log_{2}\\frac{4}{5}.\n$$\nFor $\\text{SNP2}=1$, class counts are High $4$, Low $1$, giving the same entropy. Therefore\n$$\nH(Y \\mid \\text{SNP2}) = -\\frac{1}{5}\\log_{2}\\frac{1}{5} - \\frac{4}{5}\\log_{2}\\frac{4}{5},\n$$\nand\n$$\nIG(\\text{SNP2}) = 1 - \\left[-\\frac{1}{5}\\log_{2}\\frac{1}{5} - \\frac{4}{5}\\log_{2}\\frac{4}{5}\\right].\n$$\nNumerically, $H(Y \\mid \\text{SNP2}) \\approx 0.721928095$, so $IG(\\text{SNP2}) \\approx 0.278071905$.\n\nFor SNP3: The value distribution is $P(\\text{SNP3}=0)=\\frac{4}{10}$ and $P(\\text{SNP3}=1)=\\frac{6}{10}$. For $\\text{SNP3}=0$, class counts are High $2$, Low $2$, so\n$$\nH(Y \\mid \\text{SNP3}=0) = -\\frac{1}{2}\\log_{2}\\frac{1}{2} - \\frac{1}{2}\\log_{2}\\frac{1}{2} = 1.\n$$\nFor $\\text{SNP3}=1$, class counts are High $3$, Low $3$, so\n$$\nH(Y \\mid \\text{SNP3}=1) = -\\frac{1}{2}\\log_{2}\\frac{1}{2} - \\frac{1}{2}\\log_{2}\\frac{1}{2} = 1.\n$$\nThus\n$$\nH(Y \\mid \\text{SNP3}) = \\frac{4}{10}\\cdot 1 + \\frac{6}{10}\\cdot 1 = 1,\n$$\nand\n$$\nIG(\\text{SNP3}) = 1 - 1 = 0.\n$$\n\nThe SNP with the largest information gain is SNP2, with information gain $IG(\\text{SNP2}) \\approx 0.278071905$. Rounding to three significant figures gives $0.278$.", "answer": "$$\\boxed{0.278}$$", "id": "1443753"}, {"introduction": "Many powerful machine learning models are considered \"black boxes\" because their internal decision-making processes are not obvious. To trust and use these models in critical applications like medicine, we need methods to explain their predictions. This exercise demonstrates how to build a local surrogate model, a technique at the heart of methods like LIME (Local Interpretable Model-agnostic Explanations), to explain a single prediction from a complex model [@problem_id:1443750]. By calculating the weights of a simple linear model that approximates the black box locally, you'll learn how we can derive understandable, feature-based explanations for even the most complex algorithms, bridging the gap between predictive power and human interpretation.", "problem": "A team of computational biologists has developed a complex, non-linear \"black-box\" machine learning model, denoted by $f(\\mathbf{x})$, to predict a patient's drug resistance score. The score is a continuous value, where higher values indicate greater resistance. The model's input, $\\mathbf{x} = (x_1, x_2)$, is a vector representing the normalized expression levels of two key genes, Gene-1 and Gene-2, derived from a patient's tumor biopsy.\n\nTo make the model's predictions interpretable for clinicians, the team uses a local surrogate model. For a specific patient, whose gene expression vector is $\\mathbf{x}_{\\text{pt}} = (10.0, 5.0)$, the black-box model predicts a high resistance score of $f(\\mathbf{x}_{\\text{pt}}) = 0.80$. To explain this prediction, they decide to approximate the behavior of $f$ in the immediate vicinity of $\\mathbf{x}_{\\text{pt}}$ with a simple linear model, $g(\\mathbf{z})$.\n\nThe linear model $g(\\mathbf{z})$ is designed to explain the deviation of the black-box model's output from the patient's predicted score. Its form is given by:\n$$g(\\mathbf{z}) = f(\\mathbf{x}_{\\text{pt}}) + w_1 (z_1 - x_{\\text{pt},1}) + w_2 (z_2 - x_{\\text{pt},2})$$\nwhere $\\mathbf{z}=(z_1, z_2)$ is a point in the neighborhood of $\\mathbf{x}_{\\text{pt}}$, and $w_1, w_2$ are the weights that represent the local importance of Gene-1 and Gene-2, respectively.\n\nThese weights are found by performing a weighted linear regression. A set of $N$ synthetic data points, $\\{\\mathbf{z}^{(i)}\\}_{i=1}^N$, are generated around $\\mathbf{x}_{\\text{pt}}$, and the black-box model $f$ is used to obtain their corresponding resistance scores, $\\{f(\\mathbf{z}^{(i)})\\}_{i=1}^N$. The weights $w_1$ and $w_2$ are those that minimize the weighted sum of squared errors:\n$$\\mathcal{L}(w_1, w_2) = \\sum_{i=1}^{N} \\pi_i \\left( f(\\mathbf{z}^{(i)}) - g(\\mathbf{z}^{(i)}) \\right)^2$$\nThe weight $\\pi_i$ for each synthetic point $\\mathbf{z}^{(i)}$ is determined by its proximity to the patient's data point $\\mathbf{x}_{\\text{pt}}$, calculated using an exponential kernel:\n$$\\pi_i = \\exp\\left(-\\frac{D(\\mathbf{x}_{\\text{pt}}, \\mathbf{z}^{(i)})^2}{\\sigma^2}\\right)$$\nwhere $D(\\mathbf{u}, \\mathbf{v})$ is the standard Euclidean distance between points $\\mathbf{u}$ and $\\mathbf{v}$, and the kernel width is given as $\\sigma = 0.2$.\n\nThe team generated the following three synthetic data points and their corresponding scores from the black-box model:\n1.  $\\mathbf{z}^{(1)} = (10.1, 5.0)$ with $f(\\mathbf{z}^{(1)}) = 0.75$\n2.  $\\mathbf{z}^{(2)} = (10.0, 5.1)$ with $f(\\mathbf{z}^{(2)}) = 0.82$\n3.  $\\mathbf{z}^{(3)} = (9.9, 5.2)$ with $f(\\mathbf{z}^{(3)}) = 0.85$\n\nYour task is to calculate the local importance weights, $w_1$ and $w_2$, for the two genes. Provide the values for $w_1$ and $w_2$ as your answer, rounded to three significant figures.", "solution": "We model the local deviation from $f(\\mathbf{x}_{\\text{pt}})$ by\n$$g(\\mathbf{z})=f(\\mathbf{x}_{\\text{pt}})+w_{1}(z_{1}-x_{\\text{pt},1})+w_{2}(z_{2}-x_{\\text{pt},2}).$$\nDefine for each synthetic point $\\mathbf{z}^{(i)}$ the offsets $d x_{i}=z^{(i)}_{1}-x_{\\text{pt},1}$, $d y_{i}=z^{(i)}_{2}-x_{\\text{pt},2}$ and the target $y_{i}=f(\\mathbf{z}^{(i)})-f(\\mathbf{x}_{\\text{pt}})$. Then the weighted least-squares problem\n$$\\min_{w_{1},w_{2}}\\sum_{i=1}^{N}\\pi_{i}\\left(y_{i}-w_{1} d x_{i}-w_{2} d y_{i}\\right)^{2}$$\nhas the solution $w=(w_{1},w_{2})^{\\top}=(X^{\\top}WX)^{-1}X^{\\top}Wy$, where\n$$X=\\begin{pmatrix}d x_{1} & d y_{1}\\\\ d x_{2} & d y_{2}\\\\ d x_{3} & d y_{3}\\end{pmatrix},\\quad W=\\operatorname{diag}(\\pi_{1},\\pi_{2},\\pi_{3}),\\quad y=\\begin{pmatrix}y_{1}\\\\ y_{2}\\\\ y_{3}\\end{pmatrix}.$$\nFrom the data, with $\\mathbf{x}_{\\text{pt}}=(10.0,5.0)$ and $f(\\mathbf{x}_{\\text{pt}})=0.80$, we have\n- $\\mathbf{z}^{(1)}=(10.1,5.0)$: $d x_{1}=0.1$, $d y_{1}=0$, $y_{1}=0.75-0.80=-0.05$.\n- $\\mathbf{z}^{(2)}=(10.0,5.1)$: $d x_{2}=0$, $d y_{2}=0.1$, $y_{2}=0.82-0.80=0.02$.\n- $\\mathbf{z}^{(3)}=(9.9,5.2)$: $d x_{3}=-0.1$, $d y_{3}=0.2$, $y_{3}=0.85-0.80=0.05$.\n\nKernel weights $\\pi_{i}$ use $\\pi_{i}=\\exp\\!\\left(-\\frac{D(\\mathbf{x}_{\\text{pt}},\\mathbf{z}^{(i)})^{2}}{\\sigma^{2}}\\right)$ with $\\sigma=0.2$ and Euclidean distance $D$. Distances are\n$$D_{1}^{2}=0.1^{2}=0.01,\\quad D_{2}^{2}=0.1^{2}=0.01,\\quad D_{3}^{2}=(-0.1)^{2}+0.2^{2}=0.05.$$\nThus\n$$\\pi_{1}=\\pi_{2}=\\exp(-0.25),\\quad \\pi_{3}=\\exp(-1.25).$$\nLet $A=\\exp(-0.25)$ and $B=\\exp(-1.25)$. Then\n$$X=\\begin{pmatrix}0.1 & 0\\\\ 0 & 0.1\\\\ -0.1 & 0.2\\end{pmatrix},\\quad W=\\operatorname{diag}(A,A,B),\\quad y=\\begin{pmatrix}-0.05\\\\ 0.02\\\\ 0.05\\end{pmatrix}.$$\nCompute the normal-equation components:\n$$S_{xx}=\\sum \\pi_{i} d x_{i}^{2}=0.01(A+B),\\quad S_{yy}=\\sum \\pi_{i} d y_{i}^{2}=0.01A+0.04B,\\quad S_{xy}=\\sum \\pi_{i} d x_{i} d y_{i}=-0.02B,$$\n$$b_{1}=\\sum \\pi_{i} d x_{i} y_{i}=-0.005(A+B),\\quad b_{2}=\\sum \\pi_{i} d y_{i} y_{i}=0.002A+0.01B.$$\nThe normal equations are\n$$\\begin{pmatrix}S_{xx} & S_{xy}\\\\ S_{xy} & S_{yy}\\end{pmatrix}\\begin{pmatrix}w_{1}\\\\ w_{2}\\end{pmatrix}=\\begin{pmatrix}b_{1}\\\\ b_{2}\\end{pmatrix},$$\nwith determinant\n$$\\Delta=S_{xx}S_{yy}-S_{xy}^{2}=0.0001A^{2}+0.0005AB=0.0001A(A+5B)>0.$$\nCramerâ€™s rule (or the $2\\times 2$ inverse) gives\n$$w_{1}=\\frac{b_{1}S_{yy}-b_{2}S_{xy}}{\\Delta}=-0.1\\cdot\\frac{5A+21B}{A+5B},\\qquad w_{2}=\\frac{S_{xx}b_{2}-S_{xy}b_{1}}{\\Delta}=0.2\\cdot\\frac{A+B}{A+5B}.$$\nSince $B=A\\exp(-1)$, writing $r=\\exp(-1)$, these simplify to\n$$w_{1}=-0.1\\cdot\\frac{5+21r}{1+5r},\\qquad w_{2}=0.2\\cdot\\frac{1+r}{1+5r}.$$\nWith $r=\\exp(-1)$, the numerical values are\n$$w_{1}\\approx -0.448175,\\qquad w_{2}\\approx 0.0963499.$$\nRounding to three significant figures yields\n$$w_{1}\\approx -0.448,\\qquad w_{2}\\approx 0.0963.$$", "answer": "$$\\boxed{w_1 = -0.448, \\ w_2 = 0.0963}$$", "id": "1443750"}]}