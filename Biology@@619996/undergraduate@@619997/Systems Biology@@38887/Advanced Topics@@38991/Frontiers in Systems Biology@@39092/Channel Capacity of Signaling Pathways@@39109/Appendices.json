{"hands_on_practices": [{"introduction": "To begin our exploration of information transmission in biology, we start with the simplest conceptual model: an idealized, perfect switch. This exercise asks you to consider a synthetic genetic circuit that responds with 'ON' or 'OFF' based on a single, sharp input threshold, with no biological noise. By calculating the maximum information this switch can transmit, this practice [@problem_id:1422329] establishes the fundamental upper limit of 1 bit for any binary output channel, providing a vital theoretical benchmark against which we can compare more realistic systems.", "problem": "In the field of synthetic biology, engineers design genetic circuits to perform specific computational tasks within a cell. Consider an idealized, noise-free genetic switch designed to respond to an input signaling molecule. Let the concentration of this input molecule be denoted by $[S]$. The switch controls the expression of a reporter gene, which can be in one of two states: 'OFF' (no expression) or 'ON' (maximal expression).\n\nThe switch is engineered to be \"ultrasensitive,\" meaning it responds at a single, sharply defined threshold concentration, $K$. Specifically, if the input concentration $[S]$ is strictly less than $K$, the reporter gene is in the 'OFF' state. If the input concentration $[S]$ is greater than or equal to $K$, the reporter gene is in the 'ON' state. The relationship between input and output is deterministic and completely free of biological noise.\n\nAn experimenter has precise control over the input concentration $[S]$ but can only observe the output state ('ON' or 'OFF') of the reporter gene. What is the maximum amount of information about the input $[S]$ that can be transmitted through this genetic switch? This quantity is known as the channel capacity of the signaling pathway. Express your answer in bits.", "solution": "Let the input be the continuous variable $X=[S]$ and the output be the binary variable $Y \\in \\{0,1\\}$, where $Y=0$ denotes OFF and $Y=1$ denotes ON. The deterministic, noise-free threshold mapping is\n$$\nY = \\begin{cases}\n0, & X < K, \\\\\n1, & X \\ge K.\n\\end{cases}\n$$\nThus $Y$ is a deterministic function of $X$, i.e., $Y=f(X)$ with probability one.\n\nThe channel capacity is defined as\n$$\nC = \\sup_{P_{X}} I(X;Y),\n$$\nwhere $I(X;Y)$ is the mutual information between $X$ and $Y$. Using the identity $I(X;Y)=H(Y)-H(Y|X)$ and the determinism of the mapping, we have\n$$\nH(Y|X)=0 \\quad \\Rightarrow \\quad I(X;Y)=H(Y).\n$$\nLet $p=\\Pr(Y=1)=\\Pr(X \\ge K)$, which can be set by choosing $P_{X}$ appropriately since the experimenter controls $[S]$. Then\n$$\nH(Y) = -p \\log_{2}(p) - (1-p)\\log_{2}(1-p).\n$$\nTo maximize $H(Y)$ over $p \\in [0,1]$, differentiate:\n$$\n\\frac{\\mathrm{d}H(Y)}{\\mathrm{d}p} = \\log_{2}\\!\\left(\\frac{1-p}{p}\\right),\n$$\nand set to zero to obtain\n$$\n\\log_{2}\\!\\left(\\frac{1-p}{p}\\right)=0 \\quad \\Rightarrow \\quad \\frac{1-p}{p}=1 \\quad \\Rightarrow \\quad p=\\frac{1}{2}.\n$$\nThe second derivative,\n$$\n\\frac{\\mathrm{d}^{2}H(Y)}{\\mathrm{d}p^{2}} = -\\frac{1}{p(1-p)\\ln 2},\n$$\nis negative for $p \\in (0,1)$, confirming a maximum at $p=\\frac{1}{2}$. Therefore,\n$$\nC = \\max_{P_{X}} H(Y) = H\\!\\left(\\tfrac{1}{2}\\right) = 1.\n$$\nThis capacity is achievable by choosing $X$ to be below $K$ with probability $\\frac{1}{2}$ and at or above $K$ with probability $\\frac{1}{2}$, so the output is equally likely OFF or ON. Hence the maximum information about $[S]$ transmitted per use of the switch is $1$ bit.", "answer": "$$\\boxed{1}$$", "id": "1422329"}, {"introduction": "Biological systems are never truly noise-free. This problem builds upon the ideal switch by introducing stochasticity, a core feature of all cellular processes. You will model a signaling kinase as a binary switch that sometimes makes errorsâ€”activating when it shouldn't or failing to activate when it should. This hands-on practice [@problem_id:1422306] is critical for understanding how noise fundamentally constrains a pathway's reliability, and you will quantify this limitation by calculating the reduced channel capacity of this noisy system.", "problem": "In a simplified model of a cellular signal transduction pathway, a kinase protein acts as a binary switch. Its activity state (output) depends on the presence or absence of a specific signaling molecule (input). Let the input state be \"present\" or \"absent\", and the output state be \"active\" or \"inactive\".\n\nThe signaling process is inherently noisy due to thermal fluctuations and stochastic molecular interactions.\n- When the signaling molecule is present, the kinase correctly becomes active with a probability of $0.90$.\n- When the signaling molecule is absent, the kinase correctly remains inactive with a probability of $0.90$.\n\nIn this system, an error occurs if the kinase is inactive when the molecule is present (a \"miss\") or if it is active when the molecule is absent (a \"false alarm\"). Assume the probabilities of these two types of errors are equal.\n\nBy modulating the frequency at which the cell is exposed to the signaling molecule, the system can be optimized to transmit the most information. What is the maximum amount of information, in bits, that the state of the kinase can convey about the presence or absence of the signaling molecule? Round your final answer to three significant figures.", "solution": "Let the input be a binary random variable $X \\in \\{0,1\\}$ with $X=1$ for molecule present and $X=0$ for absent. Let the output be $Y \\in \\{0,1\\}$ with $Y=1$ for kinase active and $Y=0$ for inactive. The problem states\n$$P(Y=1 \\mid X=1)=0.90, \\quad P(Y=0 \\mid X=0)=0.90,$$\nso the error probability in either direction is $p=0.10$. This defines a binary symmetric channel (BSC) with crossover probability $p=0.10$.\n\nLet $q=P(X=1)$ be the frequency of exposure. For a BSC, the mutual information is\n$$I(X;Y)=H(Y)-H(Y \\mid X).$$\nGiven $X \\sim \\text{Bernoulli}(q)$ and $Y=X \\oplus N$ with $N \\sim \\text{Bernoulli}(p)$ independent of $X$, we have\n$$P(Y=1)=q(1-p)+(1-q)p=p+q(1-2p),$$\nso\n$$H(Y)=H_{b}\\big(p+q(1-2p)\\big), \\quad H(Y \\mid X)=H_{b}(p),$$\nwhere the binary entropy in bits is\n$$H_{b}(x)=-x \\log_{2} x-(1-x) \\log_{2} (1-x).$$\nTherefore,\n$$I(X;Y)=H_{b}\\big(p+q(1-2p)\\big)-H_{b}(p).$$\nMaximizing over $q \\in [0,1]$, for a BSC the maximum occurs when $P(Y=1)=\\frac{1}{2}$, i.e.,\n$$p+q^{\\ast}(1-2p)=\\frac{1}{2} \\;\\;\\Rightarrow\\;\\; q^{\\ast}=\\frac{\\frac{1}{2}-p}{1-2p}=\\frac{1}{2} \\quad (\\text{for } p<\\tfrac{1}{2}).$$\nThus the channel capacity is\n$$C=\\max_{q} I(X;Y)=H_{b}\\left(\\tfrac{1}{2}\\right)-H_{b}(p)=1-H_{b}(p).$$\nFor $p=0.10$,\n$$H_{b}(0.10)=-0.10 \\log_{2}(0.10)-0.90 \\log_{2}(0.90).$$\nUsing $\\log_{2}(0.10)\\approx -3.32192809489$ and $\\log_{2}(0.90)\\approx -0.152003093445$, we get\n$$H_{b}(0.10)\\approx 0.332192809489+0.136802784101\\approx 0.468995593589.$$\nHence,\n$$C=1-H_{b}(0.10)\\approx 1-0.468995593589\\approx 0.531004406411 \\text{ bits},$$\nwhich to three significant figures is $0.531$ bits.", "answer": "$$\\boxed{0.531}$$", "id": "1422306"}, {"introduction": "Not all pathways are designed to continuously report the level of a stimulus. This final practice explores \"perfect adaptation,\" a common biological motif where a system's output returns to a baseline level even in the continued presence of a stimulus. By analyzing such a pathway, you will discover a key principle of biological information processing. This exercise [@problem_id:1422356] demonstrates that a channel capacity of zero for transmitting steady-state information is not a system failure, but rather a sophisticated design feature for detecting changes in the environment rather than absolute levels.", "problem": "In systems biology, a signaling pathway is said to exhibit \"perfect adaptation\" if its steady-state output is insensitive to the steady-state level of its input. Consider a hypothetical intracellular signaling pathway designed to respond to an external chemical stimulus.\n\nThe input to this pathway is a constant, sustained concentration of the stimulus, denoted by $S$. The output is the resulting steady-state concentration of an activated downstream protein, denoted by $R_{ss}$.\n\nThe defining characteristic of this pathway is its perfect adaptation property: for any constant stimulus concentration $S > 0$, the system's output eventually reaches a steady-state value $R_{ss}$ that is the same, regardless of the specific value of $S$. For $S=0$, the output is also this same steady-state value.\n\nChannel capacity, measured in bits, is defined as the maximum possible mutual information between the set of possible inputs and the corresponding outputs of a communication channel. What is the channel capacity for this pathway to transmit information about the steady-state value of the stimulus $S$ via the steady-state value of its response $R_{ss}$?\n\nA. 0 bits\n\nB. 1 bit\n\nC. Infinite bits\n\nD. The capacity depends on the kinetic parameters of the pathway's reactions.\n\nE. The capacity depends on the time it takes to reach the steady state.", "solution": "Let the input be the sustained stimulus concentration $S$ and the output be the steady-state response $R_{ss}$. Perfect adaptation implies that there exists a constant $R_{0}$ such that for every constant input $S \\geq 0$, the steady-state output satisfies\n$$\nR_{ss} = R_{0}.\n$$\nThus the channel mapping from $S$ to $R_{ss}$ is deterministic and constant. Equivalently, the conditional output distribution is independent of the input:\n$$\np_{R|S}(r|s) = \\delta(r - R_{0}) \\quad \\text{for all } s,\n$$\nso $p_{R|S}(r|s) = p_{R}(r)$ for all $s$, establishing statistical independence of $R$ and $S$ at steady state.\n\nThe mutual information between input $S$ and output $R$ is\n$$\nI(S;R) = \\int p_{S}(s) \\int p_{R|S}(r|s) \\ln \\left( \\frac{p_{R|S}(r|s)}{p_{R}(r)} \\right) \\, \\mathrm{d}r \\, \\mathrm{d}s.\n$$\nSince $p_{R|S}(r|s) = p_{R}(r)$ for all $s$, the logarithm is identically zero, and therefore\n$$\nI(S;R) = 0.\n$$\nEquivalently, using entropy, because $R$ is a constant almost surely, $H(R) = 0$ and for a deterministic channel $H(R|S) = 0$, hence\n$$\nI(S;R) = H(R) - H(R|S) = 0 - 0 = 0.\n$$\nChannel capacity is the supremum of mutual information over all input distributions $p_{S}$:\n$$\nC = \\sup_{p_{S}} I(S;R) = 0.\n$$\nTherefore, the channel cannot transmit any information about the steady-state value of $S$ via the steady-state output $R_{ss}$. Among the choices, this corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1422356"}]}