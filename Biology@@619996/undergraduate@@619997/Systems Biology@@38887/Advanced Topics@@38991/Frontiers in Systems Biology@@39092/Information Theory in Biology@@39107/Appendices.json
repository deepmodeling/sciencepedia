{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we first need a way to quantify uncertainty or information content. Shannon entropy is the cornerstone concept for this task, measuring the average unpredictability of a system's state. This first exercise provides a concrete starting point by asking you to calculate the entropy of a simplified amino acid distribution, a fundamental skill for analyzing variability in protein sequences or any other biological data set.", "problem": "In systems biology, information theory provides tools to quantify the complexity and information content of biological systems. Consider a simplified model for a crucial position within an enzyme's active site. This position is responsible for substrate binding, and due to structural constraints, only three specific amino acids—let's call them type A, type B, and type C—are observed to occupy this site across a large family of related enzymes.\n\nThrough extensive protein sequence analysis, the probability of finding each amino acid type at this specific position has been determined. The probability of finding amino acid A, denoted as $P(A)$, is $0.5$. The probability of finding amino acid B, $P(B)$, is $0.25$, and the probability of finding amino acid C, $P(C)$, is $0.25$.\n\nCalculate the Shannon entropy, which measures the uncertainty or average information content associated with this position's amino acid distribution. Use a base-2 logarithm for your calculation and provide the exact numerical answer in units of bits.", "solution": "Shannon entropy for a discrete distribution $\\{p_{i}\\}$ with base-2 logarithm is defined as\n$$\nH=-\\sum_{i} p_{i}\\log_{2}(p_{i}).\n$$\nFor the three-outcome distribution with $P(A)=\\frac{1}{2}$, $P(B)=\\frac{1}{4}$, and $P(C)=\\frac{1}{4}$, this becomes\n$$\nH=-\\left[\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)+\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right)+\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right)\\right].\n$$\nUsing $\\log_{2}\\!\\left(\\frac{1}{2}\\right)=-1$ and $\\log_{2}\\!\\left(\\frac{1}{4}\\right)=-2$, we evaluate\n$$\nH=-\\left[\\frac{1}{2}(-1)+\\frac{1}{4}(-2)+\\frac{1}{4}(-2)\\right]\n=-\\left[-\\frac{1}{2}-\\frac{1}{2}-\\frac{1}{2}\\right]\n=\\frac{3}{2}.\n$$\nThus, the Shannon entropy is $\\frac{3}{2}$ bits.", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "1439039"}, {"introduction": "While entropy quantifies the information in a single variable, much of systems biology is concerned with how different components of a system relate to one another. Mutual information extends the concept of entropy to measure the statistical dependency between two variables—in essence, how much knowing the state of one variable reduces our uncertainty about the other. This practice problem will guide you through calculating the mutual information between an external signal (time of day) and an internal cellular state (protein concentration), demonstrating how we can quantify the strength of biological regulation.", "problem": "In a study of circadian rhythms in a photosynthetic bacterium, a systems biologist investigates the relationship between the time of day and the concentration of a crucial regulatory protein. The time of day, denoted by the random variable $T$, is discretized into four equally likely phases: Dawn ($t_1$), Midday ($t_2$), Dusk ($t_3$), and Midnight ($t_4$). The protein concentration, denoted by the random variable $C$, is simplified into two states: Low ($c_1$) and High ($c_2$).\n\nAfter extensive observation, the biologist determines the following joint probabilities, $P(T=t_i, C=c_j)$, for the system:\n\n- $P(T=t_1, C=c_1) = 0.15$\n- $P(T=t_1, C=c_2) = 0.10$\n- $P(T=t_2, C=c_1) = 0.05$\n- $P(T=t_2, C=c_2) = 0.20$\n- $P(T=t_3, C=c_1) = 0.10$\n- $P(T=t_3, C=c_2) = 0.15$\n- $P(T=t_4, C=c_1) = 0.20$\n- $P(T=t_4, C=c_2) = 0.05$\n\nCalculate the mutual information between the time of day $T$ and the protein concentration $C$. Express your answer in bits, and round your final answer to four significant figures.", "solution": "The mutual information between discrete random variables $T$ and $C$ (in bits) is defined by\n$$\nI(T;C)=\\sum_{i,j} P(t_{i},c_{j}) \\log_{2}\\!\\left(\\frac{P(t_{i},c_{j})}{P(t_{i})P(c_{j})}\\right).\n$$\nFrom the joint probabilities, the marginals are\n$$\nP(T=t_{i})=\\sum_{j}P(t_{i},c_{j})=0.25 \\quad \\text{for all } i,\n$$\n$$\nP(C=c_{1})=\\sum_{i}P(t_{i},c_{1})=0.50,\\quad P(C=c_{2})=\\sum_{i}P(t_{i},c_{2})=0.50.\n$$\nThus $P(t_{i})P(c_{j})=0.25\\times 0.50=0.125$ for all $(i,j)$, and\n$$\nI(T;C)=\\sum_{i,j} P(t_{i},c_{j}) \\log_{2}\\!\\left(\\frac{P(t_{i},c_{j})}{0.125}\\right).\n$$\nThe eight joint probabilities take values in $\\{0.20,0.15,0.10,0.05\\}$, each appearing twice. Grouping terms,\n$$\nI(T;C)=2\\sum_{p\\in\\{0.20,0.15,0.10,0.05\\}} p \\log_{2}\\!\\left(\\frac{p}{0.125}\\right)\n=2\\sum_{p} \\left(p\\log_{2}p+3p\\right)\n=3+2\\sum_{p} p\\log_{2}p,\n$$\nsince $\\sum_{p\\in\\{0.20,0.15,0.10,0.05\\}} p=0.50$. Now evaluate the sum\n$$\n\\sum_{p} p\\log_{2}p=0.20\\log_{2}(0.20)+0.15\\log_{2}(0.15)+0.10\\log_{2}(0.10)+0.05\\log_{2}(0.05).\n$$\nUsing $\\log_{2}(0.20)=-2.3219280949$, $\\log_{2}(0.15)=-2.7369655942$, $\\log_{2}(0.10)=-3.3219280949$, and $\\log_{2}(0.05)=-4.3219280949$, we obtain\n$$\n\\sum_{p} p\\log_{2}p \\approx -0.464385619-0.410544839-0.332192809-0.216096405=-1.423219672.\n$$\nTherefore,\n$$\nI(T;C)\\approx 3+2(-1.423219672)=0.153560655 \\text{ bits}.\n$$\nRounding to four significant figures gives $0.1536$ bits.", "answer": "$$\\boxed{0.1536}$$", "id": "1438983"}, {"introduction": "Having learned to measure the information shared between two variables, we can now ask a more profound question: what is the maximum possible rate of information transmission in a biological process? This concept, known as channel capacity, is crucial for understanding the performance limits of signaling pathways, which are often subject to molecular noise. In this final exercise, we model a fundamental cellular event—protein phosphorylation—as a noisy communication channel and calculate its capacity, revealing the ultimate fidelity with which this pathway can transmit a signal.", "problem": "In cellular signaling, phosphorylation is a fundamental mechanism for information transfer. A kinase enzyme can add a phosphate group to a substrate protein, altering its state and function. Consider a simplified model of this process as a noisy communication channel.\n\nLet the \"input\" to the channel, denoted by a random variable $X$, be the intention of a kinase. $X=1$ represents the kinase actively attempting to phosphorylate a target substrate protein, and $X=0$ represents the kinase being inactive. The \"output\" of the channel, a random variable $Y$, is the resulting state of the substrate. $Y=1$ if the substrate is phosphorylated, and $Y=0$ if it is not.\n\nThe phosphorylation process is not perfectly reliable. This system can be modeled as a Binary Symmetric Channel (BSC). When the kinase intends to phosphorylate its substrate ($X=1$), there is a probability $p$ that the phosphorylation fails, and the substrate remains unphosphorylated ($Y=0$). Symmetrically, when the kinase is inactive ($X=0$), there is a baseline probability $p$ of spontaneous or non-specific phosphorylation, resulting in a phosphorylated substrate ($Y=1$). For this particular signaling pathway, the crossover probability has been experimentally determined to be $p = 0.125$.\n\nCalculate the channel capacity of this phosphorylation-based signaling pathway. Express your answer in bits, rounded to three significant figures.", "solution": "We model the phosphorylation signaling as a Binary Symmetric Channel (BSC) with crossover probability $p$. For a BSC, the mutual information between input $X$ and output $Y$ for an input distribution $\\Pr(X=1)=q$ is\n$$\nI(X;Y)=H(Y)-H(Y|X).\n$$\nSince the channel is memoryless and symmetric, the conditional output entropy given the input is\n$$\nH(Y|X)=H_{\\mathrm{b}}(p),\n$$\nwhere the binary entropy function in bits is\n$$\nH_{\\mathrm{b}}(p)=-p \\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nFor a symmetric channel, the capacity is achieved by the uniform input distribution $q=\\frac{1}{2}$, which maximizes $H(Y)$ to $1$ bit. Therefore, the channel capacity is\n$$\nC=\\max_{q} I(X;Y)=1-H_{\\mathrm{b}}(p).\n$$\n\nWith $p=\\frac{1}{8}$, compute\n$$\nH_{\\mathrm{b}}\\!\\left(\\frac{1}{8}\\right)=-\\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{8}\\right)-\\frac{7}{8}\\log_{2}\\!\\left(\\frac{7}{8}\\right).\n$$\nUsing $\\log_{2}\\!\\left(\\frac{1}{8}\\right)=-3$ and $\\log_{2}\\!\\left(\\frac{7}{8}\\right)=\\log_{2}(7)-3$, we have\n$$\nH_{\\mathrm{b}}\\!\\left(\\frac{1}{8}\\right)=-\\frac{1}{8}(-3)-\\frac{7}{8}\\bigl(\\log_{2}(7)-3\\bigr)\n=\\frac{3}{8}-\\frac{7}{8}\\log_{2}(7)+\\frac{21}{8}\n=3-\\frac{7}{8}\\log_{2}(7).\n$$\nHence the capacity is\n$$\nC=1-H_{\\mathrm{b}}\\!\\left(\\frac{1}{8}\\right)=1-\\left(3-\\frac{7}{8}\\log_{2}(7)\\right)=-2+\\frac{7}{8}\\log_{2}(7).\n$$\nNumerically,\n$$\nC\\approx -2+\\frac{7}{8}\\log_{2}(7)\\approx 0.4564355568\\ \\text{bits},\n$$\nwhich rounded to three significant figures is $0.456$ bits.", "answer": "$$\\boxed{0.456}$$", "id": "1438988"}]}