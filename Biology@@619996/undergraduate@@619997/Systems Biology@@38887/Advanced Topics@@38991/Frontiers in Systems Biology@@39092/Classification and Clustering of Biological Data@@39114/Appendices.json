{"hands_on_practices": [{"introduction": "Hierarchical clustering is a powerful unsupervised learning method for discovering the inherent structure within a dataset without prior knowledge of group labels. This exercise provides a hands-on opportunity to apply the complete-linkage agglomerative clustering algorithm, a fundamental technique for grouping data based on similarity. By working through the steps to build a dendrogram from a genetic distance matrix, you will gain a concrete understanding of how these evolutionary trees are constructed algorithmically. [@problem_id:1423409]", "problem": "A team of marine biologists is studying the microbial ecosystem of a recently discovered deep-sea hydrothermal vent. They have isolated five previously unknown bacterial species, labeled S1, S2, S3, S4, and S5. To understand their potential evolutionary relationships, they performed a genetic analysis and computed a distance matrix based on dissimilarities in their 16S ribosomal RNA (rRNA) gene sequences. A larger value in the matrix indicates greater genetic divergence between two species.\n\nThe resulting distance matrix is as follows:\n\n| | S1 | S2 | S3 | S4 | S5 |\n|:--|:--:|:--:|:--:|:--:|:--:|\n| **S1**| 0 | 0.20 | 0.50 | 0.60 | 0.70 |\n| **S2**| 0.20 | 0 | 0.40 | 0.55 | 0.65 |\n| **S3**| 0.50 | 0.40 | 0 | 0.10 | 0.30 |\n| **S4**| 0.60 | 0.55 | 0.10 | 0 | 0.25 |\n| **S5**| 0.70 | 0.65 | 0.30 | 0.25 | 0 |\n\nYour task is to determine the phylogenetic relationship between these species by performing hierarchical clustering. Specifically, you must use the **complete-linkage agglomerative clustering** method.\n\nThe resulting relationship is to be represented as a dendrogram, which can be described textually using the Newick format. In Newick format, a tree is represented by nested parentheses. For example, the string `((A,B),C);` means that species A and B are clustered together first, and this cluster is then grouped with species C.\n\nWhich of the following Newick format strings correctly represents the topology of the dendrogram obtained from the given distance matrix using complete-linkage clustering?\n\nA. `((S1,S2),((S3,S4),S5));`\n\nB. `((((S3,S4),S5),S2),S1));`\n\nC. `(((S1,S2),S5),(S3,S4));`\n\nD. `((S1,S5),((S2,S3),S4));`\n\nE. `(((S3,S4),S2),(S1,S5));`", "solution": "In complete-linkage agglomerative clustering, the distance between two clusters $A$ and $B$ is defined as\n$$\nd_{\\text{complete}}(A,B)=\\max\\{d(i,j): i\\in A,\\ j\\in B\\}.\n$$\nStart with singleton clusters $\\{S1\\},\\{S2\\},\\{S3\\},\\{S4\\},\\{S5\\}$ and the given pairwise distances.\n\n1) Identify the closest pair among singletons. The smallest entry in the matrix is $d(S3,S4)=0.10$, so merge $S3$ and $S4$ first to form $C_{34}=\\{S3,S4\\}$.\n\nCompute distances from $C_{34}$ to the remaining singletons using the complete-linkage rule:\n$$\nd(C_{34},S1)=\\max\\{d(S3,S1),d(S4,S1)\\}=\\max\\{0.50,0.60\\}=0.60,\n$$\n$$\nd(C_{34},S2)=\\max\\{d(S3,S2),d(S4,S2)\\}=\\max\\{0.40,0.55\\}=0.55,\n$$\n$$\nd(C_{34},S5)=\\max\\{d(S3,S5),d(S4,S5)\\}=\\max\\{0.30,0.25\\}=0.30.\n$$\nCurrent inter-cluster distances include $d(S1,S2)=0.20$, $d(S1,S5)=0.70$, $d(S2,S5)=0.65$, and the three computed above.\n\n2) The smallest current distance is $d(S1,S2)=0.20$, so merge $S1$ and $S2$ to form $C_{12}=\\{S1,S2\\}$.\n\nCompute distances among $C_{12}$, $C_{34}$, and $S5$:\n$$\nd(C_{12},S5)=\\max\\{d(S1,S5),d(S2,S5)\\}=\\max\\{0.70,0.65\\}=0.70,\n$$\n$$\nd(C_{12},C_{34})=\\max\\{d(S1,S3),d(S1,S4),d(S2,S3),d(S2,S4)\\}\n=\\max\\{0.50,0.60,0.40,0.55\\}=0.60,\n$$\nand from before $d(C_{34},S5)=0.30$.\n\n3) The smallest current distance is $d(C_{34},S5)=0.30$, so merge to form $C_{345}=\\{S3,S4,S5\\}$ with structure $((S3,S4),S5)$.\n\n4) Now two clusters remain: $C_{12}=\\{S1,S2\\}$ and $C_{345}=\\{S3,S4,S5\\}$. Their complete-linkage distance is\n$$\nd(C_{12},C_{345})=\\max\\{d(S1,S3),d(S1,S4),d(S1,S5),d(S2,S3),d(S2,S4),d(S2,S5)\\}\n=\\max\\{0.50,0.60,0.70,0.40,0.55,0.65\\}=0.70.\n$$\nThus the final merge is between $(S1,S2)$ and $((S3,S4),S5)$.\n\nThe resulting topology in Newick form is $((S1,S2),((S3,S4),S5));$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1423409"}, {"introduction": "The results of any data analysis are highly dependent on how the data is prepared, and this is especially true in clustering. This practice explores the critical role of normalization, a common pre-processing step, on the outcome of hierarchical clustering of gene expression data. By comparing the clustering results from raw data against two different Z-score normalization schemes, you will directly observe how these choices can reveal either the underlying biological structure or highlight technical artifacts like batch effects. [@problem_id:1423433]", "problem": "An analyst is studying a small gene expression dataset from an experiment designed to test the effect of a drug. The experiment involves four samples: a control sample from batch A (S1), a control sample from batch B (S2), a treated sample from batch A (S3), and a treated sample from batch B (S4). The intended \"biological\" grouping is therefore `{S1, S2}` (controls) versus `{S3, S4}` (treated), while a potential \"batch effect\" grouping is `{S1, S3}` (batch A) versus `{S2, S4}` (batch B).\n\nThe raw expression data for two genes, G1 and G2, are given in the following table:\n\n| Gene | S1 | S2 | S3 | S4 |\n|:----:|:--:|:--:|:--:|:--:|\n|  G1  | 1  | 6  | 4  | 9  |\n|  G2  | 4  | 9  | 1  | 6  |\n\nThe analyst wishes to compare how different data normalization strategies affect the outcome of hierarchical clustering. The clustering will be performed using the Euclidean distance as the metric and the average-linkage method, also known as Unweighted Pair Group Method with Arithmetic Mean (UPGMA). The analyst considers three cases:\n1.  **Raw Data**: No normalization is applied.\n2.  **M1 Normalization**: Each gene's expression values are independently transformed using Z-score standardization (i.e., for each gene, subtract the gene's mean and divide by the gene's standard deviation across all samples).\n3.  **M2 Normalization**: Each sample's expression values are independently transformed using Z-score standardization (i.e., for each sample, subtract the sample's mean and divide by the sample's standard deviation across all genes).\n\nIn this context, the standard deviation of a set of values $\\{x_1, ..., x_N\\}$ with mean $\\mu$ is calculated as $\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\mu)^2}$.\n\nThe final result of a hierarchical clustering can be represented by the partition of samples into two distinct groups, which corresponds to cutting the dendrogram at the highest level. Let's denote the biological partition as $P_{bio} = \\{\\{S1, S2\\}, \\{S3, S4\\}\\}$ and the batch partition as $P_{batch} = \\{\\{S1, S3\\}, \\{S2, S4\\}\\}$.\n\nWhich of the following options correctly identifies the resulting two-cluster partition for (Raw Data, M1-Normalized Data, M2-Normalized Data)?\n\nA. ($P_{batch}$, $P_{batch}$, $P_{bio}$)\n\nB. ($P_{batch}$, $P_{bio}$, $P_{bio}$)\n\nC. ($P_{bio}$, $P_{bio}$, $P_{bio}$)\n\nD. ($P_{batch}$, $P_{batch}$, $P_{batch}$)\n\nE. ($P_{bio}$, $P_{batch}$, $P_{bio}$)", "solution": "We represent each sample as a 2-dimensional vector with coordinates given by the two genes. The Euclidean distance between samples $x,y \\in \\mathbb{R}^{2}$ is\n$$\nd(x,y)=\\sqrt{(x_{1}-y_{1})^{2}+(x_{2}-y_{2})^{2}}.\n$$\nThe average-linkage (UPGMA) distance between two clusters $A$ and $B$ is\n$$\nD(A,B)=\\frac{1}{|A||B|}\\sum_{x\\in A}\\sum_{y\\in B}d(x,y).\n$$\nCutting the dendrogram at the highest level yields a partition into two clusters.\n\nRaw data. The sample vectors are $S1=(1,4)$, $S2=(6,9)$, $S3=(4,1)$, $S4=(9,6)$. Compute all pairwise distances:\n$$\n\\begin{aligned}\n&d(S1,S3)=\\sqrt{(1-4)^{2}+(4-1)^{2}}=\\sqrt{18}=3\\sqrt{2},\\\\\n&d(S2,S4)=\\sqrt{(6-9)^{2}+(9-6)^{2}}=\\sqrt{18}=3\\sqrt{2},\\\\\n&d(S1,S2)=\\sqrt{(1-6)^{2}+(4-9)^{2}}=\\sqrt{50}=5\\sqrt{2},\\\\\n&d(S3,S4)=\\sqrt{(4-9)^{2}+(1-6)^{2}}=\\sqrt{50}=5\\sqrt{2},\\\\\n&d(S1,S4)=\\sqrt{(1-9)^{2}+(4-6)^{2}}=\\sqrt{68}=2\\sqrt{17},\\\\\n&d(S2,S3)=\\sqrt{(6-4)^{2}+(9-1)^{2}}=\\sqrt{68}=2\\sqrt{17}.\n\\end{aligned}\n$$\nThe smallest distances are $d(S1,S3)=d(S2,S4)=3\\sqrt{2}$, so the first merges are the batch pairs. Suppose we merge $A=\\{S1,S3\\}$. Average-linkage distances to singletons are\n$$\nD(A,S2)=\\frac{d(S1,S2)+d(S3,S2)}{2}=\\frac{5\\sqrt{2}+2\\sqrt{17}}{2},\\quad\nD(A,S4)=\\frac{d(S1,S4)+d(S3,S4)}{2}=\\frac{2\\sqrt{17}+5\\sqrt{2}}{2}.\n$$\nSince $d(S2,S4)=3\\sqrt{2}$ is smaller than either $D(A,S2)$ or $D(A,S4)$, the next merge is $\\{S2,S4\\}$. The resulting two clusters are $\\{S1,S3\\}$ and $\\{S2,S4\\}$, which is $P_{batch}$.\n\nM1 normalization (per-gene Z-score). For each gene $g$, with values $x_{g}=\\{x_{g,S1},x_{g,S2},x_{g,S3},x_{g,S4}\\}$, compute\n$$\n\\mu_{g}=\\frac{1}{4}\\sum_{j=1}^{4}x_{g,Sj},\\qquad \\sigma_{g}=\\sqrt{\\frac{1}{4}\\sum_{j=1}^{4}\\left(x_{g,Sj}-\\mu_{g}\\right)^{2}}.\n$$\nFor $G1$, $(1,6,4,9)$ gives $\\mu_{1}=5$ and $\\sigma_{1}=\\sqrt{(16+1+1+16)/4}=\\sqrt{17/2}$. For $G2$, $(4,9,1,6)$ gives $\\mu_{2}=5$ and $\\sigma_{2}=\\sqrt{17/2}$. Thus the transformation is\n$$\nx\\mapsto \\left(\\frac{x_{1}-5}{\\sqrt{17/2}},\\frac{x_{2}-5}{\\sqrt{17/2}}\\right),\n$$\nwhich is a translation by $(5,5)$ (which leaves pairwise Euclidean distances unchanged) followed by an isotropic scaling by the positive factor $1/\\sqrt{17/2}$ (which multiplies all distances by the same constant). Therefore the relative ordering of all inter-sample distances, and hence all average-linkage inter-cluster distances, is preserved. The dendrogram is identical to the raw-data dendrogram, and the two-cluster partition is again $P_{batch}$.\n\nM2 normalization (per-sample Z-score). For each sample $Sj$, with vector $(a,b)$, compute\n$$\n\\mu=\\frac{a+b}{2},\\qquad \\sigma=\\sqrt{\\frac{1}{2}\\left((a-\\mu)^{2}+(b-\\mu)^{2}\\right)}.\n$$\nSince $a-\\mu=\\frac{a-b}{2}$ and $b-\\mu=-\\frac{a-b}{2}$, we obtain\n$$\n\\sigma=\\sqrt{\\frac{1}{2}\\left(\\frac{(a-b)^{2}}{4}+\\frac{(a-b)^{2}}{4}\\right)}=\\sqrt{\\frac{(a-b)^{2}}{4}}=\\frac{|a-b|}{2},\n$$\nand the Z-scores are\n$$\nz_{a}=\\frac{a-\\mu}{\\sigma}=\\frac{(a-b)/2}{|a-b|/2}=\\operatorname{sign}(a-b),\\quad\nz_{b}=\\frac{b-\\mu}{\\sigma}=-\\operatorname{sign}(a-b).\n$$\nApplying this to the samples:\n$$\nS1:(1,4)\\mapsto(-1,1),\\quad S2:(6,9)\\mapsto(-1,1),\\quad S3:(4,1)\\mapsto(1,-1),\\quad S4:(9,6)\\mapsto(1,-1).\n$$\nPairwise distances are $d(S1,S2)=0$, $d(S3,S4)=0$, and $d(S1,S3)=d(S1,S4)=d(S2,S3)=d(S2,S4)=\\sqrt{(2)^{2}+(-2)^{2}}=2\\sqrt{2}$. The smallest distances are zero within the biological pairs, so the first merges are $\\{S1,S2\\}$ and $\\{S3,S4\\}$. With average linkage, these zero-distance merges occur before any cross-group merge (which would occur at $2\\sqrt{2}$). Cutting the dendrogram at two clusters yields $\\{S1,S2\\}$ and $\\{S3,S4\\}$, i.e., $P_{bio}$.\n\nTherefore, the resulting partitions are $(P_{batch}, P_{batch}, P_{bio})$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1423433"}, {"introduction": "Moving from unsupervised exploration to supervised prediction, this exercise demonstrates how to build and use a classifier to assign predefined labels to new data. You will apply the Gaussian Naive Bayes model, a common and efficient probabilistic method, to classify cells into different phases of the cell cycle based on gene expression levels. This practice guides you through calculating classification scores and culminates in constructing a confusion matrix, a key tool for evaluating any classifier's performance on a test set. [@problem_id:1423429]", "problem": "A systems biologist is developing a model to classify single cells into one of three distinct cell cycle phases: Phase 1 (P1), Phase 2 (P2), and Phase 3 (P3). The classification is based on the normalized expression levels of two key regulatory genes, Gene A and Gene B.\n\nA Gaussian Naive Bayes classifier is chosen for this task. This model assumes that for a given cell cycle phase, the expression levels of Gene A and Gene B are conditionally independent. It also assumes that the expression level of each gene within each phase follows a Normal (Gaussian) distribution.\n\nFrom a large set of pre-classified cells, the following statistical summary of the training data was derived:\n- The total number of training cells is 1000. The counts for each phase are: 400 cells in P1, 350 cells in P2, and 250 cells in P3.\n- The mean ($\\mu$) and standard deviation ($\\sigma$) of the normalized expression levels for each gene in each phase are given in the table below. The expression levels are dimensionless quantities.\n\n| Phase | Gene A ($\\mu_A, \\sigma_A$) | Gene B ($\\mu_B, \\sigma_B$) |\n|---|---|---|\n| P1 | (5.0, 1.0) | (4.0, 1.2) |\n| P2 | (8.0, 1.5) | (6.0, 1.0) |\n| P3 | (6.0, 1.2) | (9.0, 1.5) |\n\nA new set of 6 test cells with known true phases is collected to evaluate the classifier's performance. Their gene expression levels and true phases are:\n\n| Cell ID | Gene A Expr. | Gene B Expr. | True Phase |\n|---|---|---|---|\n| S1 | 5.5 | 4.5 | P1 |\n| S2 | 7.5 | 5.8 | P2 |\n| S3 | 6.2 | 8.5 | P3 |\n| S4 | 4.8 | 6.5 | P1 |\n| S5 | 8.5 | 8.8 | P2 |\n| S6 | 7.0 | 7.0 | P3 |\n\nFor each test cell, determine its predicted phase using the trained Gaussian Naive Bayes classifier. Then, construct the confusion matrix for the classifier's performance on this test set. The confusion matrix should be a 3x3 matrix where the rows represent the true phases (in the order P1, P2, P3) and the columns represent the predicted phases (in the order P1, P2, P3).\n\nProvide the 9 integer elements of the confusion matrix as your final answer, listed row by row. That is, provide the element for (True P1, Predicted P1), then (True P1, Predicted P2), (True P1, Predicted P3), followed by (True P2, Predicted P1), and so on.", "solution": "We model class-conditional densities with a Gaussian Naive Bayes classifier. Let the class labels be $c \\in \\{\\text{P1}, \\text{P2}, \\text{P3}\\}$ and the features be $x = (x_{A}, x_{B})$ for Gene A and Gene B. The model assumes conditional independence given the class and univariate Normal distributions for each gene:\n$$\np(x \\mid c) \\;=\\; \\prod_{j \\in \\{A,B\\}} \\frac{1}{\\sigma_{c,j}\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(x_{j}-\\mu_{c,j})^{2}}{2\\sigma_{c,j}^{2}}\\right).\n$$\nThe class prior probabilities from counts are\n$$\n\\pi_{\\text{P1}}=\\frac{400}{1000}=0.4,\\quad \\pi_{\\text{P2}}=\\frac{350}{1000}=0.35,\\quad \\pi_{\\text{P3}}=\\frac{250}{1000}=0.25.\n$$\nFor prediction, we compare the (unnormalized) log-posterior scores\n$$\ns_{c}(x) \\;=\\; \\ln \\pi_{c} \\;+\\; \\sum_{j \\in \\{A,B\\}} \\left[-\\ln \\sigma_{c,j} \\;-\\; \\frac{(x_{j}-\\mu_{c,j})^{2}}{2\\sigma_{c,j}^{2}} \\right] \\;-\\; \\sum_{j \\in \\{A,B\\}} \\ln \\sqrt{2\\pi}.\n$$\nThe last term $-\\sum_{j} \\ln \\sqrt{2\\pi}$ is constant across classes for a fixed $x$ and can be omitted for the argmax. Define\n$$\n\\text{base}_{c} \\;=\\; \\ln \\pi_{c} \\;-\\; \\sum_{j \\in \\{A,B\\}} \\ln \\sigma_{c,j}, \\qquad Q_{c}(x) \\;=\\; \\sum_{j \\in \\{A,B\\}} \\frac{(x_{j}-\\mu_{c,j})^{2}}{2\\sigma_{c,j}^{2}},\n$$\nso that $s_{c}(x)=\\text{base}_{c} - Q_{c}(x)$ up to an additive constant independent of $c$.\n\nFrom the table, the parameters are:\n- P1: $(\\mu_{A},\\sigma_{A})=(5.0,1.0)$, $(\\mu_{B},\\sigma_{B})=(4.0,1.2)$,\n- P2: $(\\mu_{A},\\sigma_{A})=(8.0,1.5)$, $(\\mu_{B},\\sigma_{B})=(6.0,1.0)$,\n- P3: $(\\mu_{A},\\sigma_{A})=(6.0,1.2)$, $(\\mu_{B},\\sigma_{B})=(9.0,1.5)$.\n\nCompute the base terms:\n$$\n\\ln \\pi_{\\text{P1}} = \\ln(0.4) \\approx -0.916291,\\quad -\\sum \\ln \\sigma_{\\text{P1},j} = -\\ln 1.0 - \\ln 1.2 \\approx -0.182322,\\quad \\text{base}_{\\text{P1}} \\approx -1.098613,\n$$\n$$\n\\ln \\pi_{\\text{P2}} = \\ln(0.35) \\approx -1.049822,\\quad -\\sum \\ln \\sigma_{\\text{P2},j} = -\\ln 1.5 - \\ln 1.0 \\approx -0.405465,\\quad \\text{base}_{\\text{P2}} \\approx -1.455287,\n$$\n$$\n\\ln \\pi_{\\text{P3}} = \\ln(0.25) \\approx -1.386294,\\quad -\\sum \\ln \\sigma_{\\text{P3},j} = -\\ln 1.2 - \\ln 1.5 \\approx -0.587787,\\quad \\text{base}_{\\text{P3}} \\approx -1.974081.\n$$\n\nFor each test cell $x=(x_{A},x_{B})$, compute $Q_{c}(x)=\\sum_{j} \\frac{(x_{j}-\\mu_{c,j})^{2}}{2\\sigma_{c,j}^{2}}$ and then $s_{c}(x)=\\text{base}_{c}-Q_{c}(x)$; predict the class with the largest $s_{c}(x)$.\n\nS1: $(5.5,4.5)$.\n- P1: deviations $(0.5,0.5)$, variances $(1.0,1.44)$,\n$$\nQ_{\\text{P1}} = \\frac{0.5^{2}}{2\\cdot 1.0} + \\frac{0.5^{2}}{2\\cdot 1.44} = 0.125 + 0.086806 \\approx 0.211806,\\quad s_{\\text{P1}} \\approx -1.098613 - 0.211806 = -1.310419.\n$$\n- P2: deviations $(-2.5,-1.5)$, variances $(2.25,1.0)$,\n$$\nQ_{\\text{P2}} = \\frac{2.5^{2}}{2\\cdot 2.25} + \\frac{1.5^{2}}{2\\cdot 1.0} = 1.388889 + 1.125 = 2.513889,\\quad s_{\\text{P2}} \\approx -1.455287 - 2.513889 = -3.969176.\n$$\n- P3: deviations $(-0.5,-4.5)$, variances $(1.44,2.25)$,\n$$\nQ_{\\text{P3}} = \\frac{0.5^{2}}{2\\cdot 1.44} + \\frac{4.5^{2}}{2\\cdot 2.25} = 0.086806 + 4.5 = 4.586806,\\quad s_{\\text{P3}} \\approx -1.974081 - 4.586806 = -6.560887.\n$$\nPrediction: P1.\n\nS2: $(7.5,5.8)$.\n- P1: deviations $(2.5,1.8)$, variances $(1.0,1.44)$,\n$$\nQ_{\\text{P1}} = \\frac{2.5^{2}}{2\\cdot 1.0} + \\frac{1.8^{2}}{2\\cdot 1.44} = 3.125 + 1.125 = 4.25,\\quad s_{\\text{P1}} \\approx -1.098613 - 4.25 = -5.348613.\n$$\n- P2: deviations $(-0.5,-0.2)$, variances $(2.25,1.0)$,\n$$\nQ_{\\text{P2}} = \\frac{0.5^{2}}{2\\cdot 2.25} + \\frac{0.2^{2}}{2\\cdot 1.0} = 0.055556 + 0.02 = 0.075556,\\quad s_{\\text{P2}} \\approx -1.455287 - 0.075556 = -1.530843.\n$$\n- P3: deviations $(1.5,-3.2)$, variances $(1.44,2.25)$,\n$$\nQ_{\\text{P3}} = \\frac{1.5^{2}}{2\\cdot 1.44} + \\frac{3.2^{2}}{2\\cdot 2.25} = 0.78125 + 2.275556 = 3.056806,\\quad s_{\\text{P3}} \\approx -1.974081 - 3.056806 = -5.030887.\n$$\nPrediction: P2.\n\nS3: $(6.2,8.5)$.\n- P1: deviations $(1.2,4.5)$, variances $(1.0,1.44)$,\n$$\nQ_{\\text{P1}} = \\frac{1.2^{2}}{2\\cdot 1.0} + \\frac{4.5^{2}}{2\\cdot 1.44} = 0.72 + 7.03125 = 7.75125,\\quad s_{\\text{P1}} \\approx -1.098613 - 7.75125 = -8.849863.\n$$\n- P2: deviations $(-1.8,2.5)$, variances $(2.25,1.0)$,\n$$\nQ_{\\text{P2}} = \\frac{1.8^{2}}{2\\cdot 2.25} + \\frac{2.5^{2}}{2\\cdot 1.0} = 0.72 + 3.125 = 3.845,\\quad s_{\\text{P2}} \\approx -1.455287 - 3.845 = -5.300287.\n$$\n- P3: deviations $(0.2,-0.5)$, variances $(1.44,2.25)$,\n$$\nQ_{\\text{P3}} = \\frac{0.2^{2}}{2\\cdot 1.44} + \\frac{0.5^{2}}{2\\cdot 2.25} = 0.013889 + 0.055556 = 0.0694449,\\quad s_{\\text{P3}} \\approx -1.974081 - 0.0694449 = -2.043526.\n$$\nPrediction: P3.\n\nS4: $(4.8,6.5)$.\n- P1: deviations $(-0.2,2.5)$, variances $(1.0,1.44)$,\n$$\nQ_{\\text{P1}} = \\frac{0.2^{2}}{2\\cdot 1.0} + \\frac{2.5^{2}}{2\\cdot 1.44} = 0.02 + 2.170139 = 2.190139,\\quad s_{\\text{P1}} \\approx -1.098613 - 2.190139 = -3.288752.\n$$\n- P2: deviations $(-3.2,0.5)$, variances $(2.25,1.0)$,\n$$\nQ_{\\text{P2}} = \\frac{3.2^{2}}{2\\cdot 2.25} + \\frac{0.5^{2}}{2\\cdot 1.0} = 2.275556 + 0.125 = 2.400556,\\quad s_{\\text{P2}} \\approx -1.455287 - 2.400556 = -3.855843.\n$$\n- P3: deviations $(-1.2,-2.5)$, variances $(1.44,2.25)$,\n$$\nQ_{\\text{P3}} = \\frac{1.2^{2}}{2\\cdot 1.44} + \\frac{2.5^{2}}{2\\cdot 2.25} = 0.5 + 1.388889 = 1.888889,\\quad s_{\\text{P3}} \\approx -1.974081 - 1.888889 = -3.862970.\n$$\nPrediction: P1.\n\nS5: $(8.5,8.8)$.\n- P1: deviations $(3.5,4.8)$, variances $(1.0,1.44)$,\n$$\nQ_{\\text{P1}} = \\frac{3.5^{2}}{2\\cdot 1.0} + \\frac{4.8^{2}}{2\\cdot 1.44} = 6.125 + 8 = 14.125,\\quad s_{\\text{P1}} \\approx -1.098613 - 14.125 = -15.223613.\n$$\n- P2: deviations $(0.5,2.8)$, variances $(2.25,1.0)$,\n$$\nQ_{\\text{P2}} = \\frac{0.5^{2}}{2\\cdot 2.25} + \\frac{2.8^{2}}{2\\cdot 1.0} = 0.055556 + 3.92 = 3.975556,\\quad s_{\\text{P2}} \\approx -1.455287 - 3.975556 = -5.430843.\n$$\n- P3: deviations $(2.5,-0.2)$, variances $(1.44,2.25)$,\n$$\nQ_{\\text{P3}} = \\frac{2.5^{2}}{2\\cdot 1.44} + \\frac{0.2^{2}}{2\\cdot 2.25} = 2.170139 + 0.008889 = 2.179028,\\quad s_{\\text{P3}} \\approx -1.974081 - 2.179028 = -4.153109.\n$$\nPrediction: P3.\n\nS6: $(7.0,7.0)$.\n- P1: deviations $(2.0,3.0)$, variances $(1.0,1.44)$,\n$$\nQ_{\\text{P1}} = \\frac{2.0^{2}}{2\\cdot 1.0} + \\frac{3.0^{2}}{2\\cdot 1.44} = 2 + 3.125 = 5.125,\\quad s_{\\text{P1}} \\approx -1.098613 - 5.125 = -6.223613.\n$$\n- P2: deviations $(-1.0,1.0)$, variances $(2.25,1.0)$,\n$$\nQ_{\\text{P2}} = \\frac{1.0^{2}}{2\\cdot 2.25} + \\frac{1.0^{2}}{2\\cdot 1.0} = 0.222222 + 0.5 = 0.722222,\\quad s_{\\text{P2}} \\approx -1.455287 - 0.722222 = -2.177509.\n$$\n- P3: deviations $(1.0,-2.0)$, variances $(1.44,2.25)$,\n$$\nQ_{\\text{P3}} = \\frac{1.0^{2}}{2\\cdot 1.44} + \\frac{2.0^{2}}{2\\cdot 2.25} = 0.347222 + 0.888889 = 1.236111,\\quad s_{\\text{P3}} \\approx -1.974081 - 1.236111 = -3.210192.\n$$\nPrediction: P2.\n\nPredicted labels:\n- S1: P1; S4: P1 (true P1 both),\n- S2: P2; S5: P3 (true P2, one correct, one misclassified as P3),\n- S3: P3; S6: P2 (true P3, one correct, one misclassified as P2).\n\nConstruct the confusion matrix with rows as true phases (P1, P2, P3) and columns as predicted phases (P1, P2, P3). Counting:\n- True P1: predicted P1 twice, P2 zero, P3 zero → $(2,0,0)$.\n- True P2: predicted P1 zero, P2 once, P3 once → $(0,1,1)$.\n- True P3: predicted P1 zero, P2 once, P3 once → $(0,1,1)$.\n\nListed row by row, the 9 elements are $2, 0, 0, 0, 1, 1, 0, 1, 1$.", "answer": "$$\\boxed{\\begin{pmatrix}2 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1\\end{pmatrix}}$$", "id": "1423429"}]}