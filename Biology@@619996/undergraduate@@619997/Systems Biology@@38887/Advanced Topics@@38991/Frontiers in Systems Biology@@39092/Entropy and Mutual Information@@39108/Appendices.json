{"hands_on_practices": [{"introduction": "Information-theoretic entropy provides a powerful way to quantify the uncertainty or diversity within a system. This first exercise grounds this abstract concept in a tangible biological context: population genetics. By calculating the entropy of allelic frequencies, you will practice applying the fundamental Shannon entropy formula, $H = -\\sum_i p_i \\log_b(p_i)$, and develop an intuition for how it measures the genetic diversity crucial for a population's adaptability [@problem_id:1431592].", "problem": "In a population of bacteria being studied for resistance to a newly developed antibiotic, a single gene locus is identified as the primary determinant of the resistance phenotype. A genetic survey of the population reveals the existence of three distinct alleles at this locus. The allele $\\text{R}_\\text{S}$ confers strong resistance and is present with a frequency of 0.15. The allele $\\text{R}_\\text{W}$ confers weak resistance and has a frequency of 0.50. Finally, the susceptible allele, $\\text{S}$, is found with a frequency of 0.35.\n\nGenetic diversity is a key factor in a population's ability to adapt. One way to quantify this diversity at a specific locus is by calculating the information entropy of the allelic distribution. The entropy, $H$, of a discrete random variable $X$ with possible values $\\{x_1, x_2, ..., x_n\\}$ and probability mass function $P(X)$ is given by $H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_{b}(P(x_i))$, where $b$ is the base of the logarithm.\n\nCalculate the entropy of the allelic variation at this gene locus. Express your answer in bits, which corresponds to using a base $b=2$ logarithm, and round your final answer to four significant figures.", "solution": "The problem asks for the entropy of the allelic distribution at a specific gene locus. The entropy $H$ for a set of discrete probabilities is calculated using the Shannon entropy formula. The problem specifies that the answer should be in bits, which means we must use the logarithm to the base 2.\n\nThe formula for Shannon entropy is:\n$$H = -\\sum_{i=1}^{n} p_i \\log_{2}(p_i)$$\nwhere $p_i$ is the probability (or frequency) of the $i$-th state.\n\nIn this scenario, we have three alleles, which represent our three states. Their frequencies are given as:\n$p_1 = p(\\text{R}_\\text{S}) = 0.15$\n$p_2 = p(\\text{R}_\\text{W}) = 0.50$\n$p_3 = p(\\text{S}) = 0.35$\n\nWe can substitute these values into the entropy formula:\n$$H = -[ p(\\text{R}_\\text{S})\\log_2(p(\\text{R}_\\text{S})) + p(\\text{R}_\\text{W})\\log_2(p(\\text{R}_\\text{W})) + p(\\text{S})\\log_2(p(\\text{S})) ]$$\n$$H = -[ 0.15 \\log_2(0.15) + 0.50 \\log_2(0.50) + 0.35 \\log_2(0.35) ]$$\n\nTo compute the base-2 logarithm, we can use the change of base formula: $\\log_2(x) = \\frac{\\ln(x)}{\\ln(2)}$.\n\nLet's calculate each term inside the brackets:\nFor the first term:\n$0.15 \\log_2(0.15) = 0.15 \\times \\frac{\\ln(0.15)}{\\ln(2)} \\approx 0.15 \\times \\frac{-1.89712}{0.69315} \\approx 0.15 \\times (-2.73697) \\approx -0.410545$\n\nFor the second term:\n$0.50 \\log_2(0.50) = 0.50 \\times (-1) = -0.50$\n\nFor the third term:\n$0.35 \\log_2(0.35) = 0.35 \\times \\frac{\\ln(0.35)}{\\ln(2)} \\approx 0.35 \\times \\frac{-1.04982}{0.69315} \\approx 0.35 \\times (-1.51457) \\approx -0.530100$\n\nNow, we sum these three terms:\nSum = $(-0.410545) + (-0.50) + (-0.530100) = -1.440645$\n\nFinally, we apply the negative sign from the entropy formula:\n$H = -(-1.440645) = 1.440645$ bits.\n\nThe problem requires the answer to be rounded to four significant figures.\nThe first four significant figures are 1, 4, 4, 0. The fifth digit is 6, which is 5 or greater, so we round up the fourth digit.\n$H \\approx 1.441$\n\nThe entropy of the allelic variation is approximately 1.441 bits.", "answer": "$$\\boxed{1.441}$$", "id": "1431592"}, {"introduction": "While entropy measures the uncertainty of a single variable, mutual information quantifies the information shared between two variables, or the reduction in uncertainty about one given knowledge of the other. This practice problem models a basic cell signaling pathway as an information channel, with a sending cell and a receiving cell. Calculating the mutual information between the signal and the cellular response will give you a hands-on understanding of how to measure the fidelity of information transmission in biological networks [@problem_id:1431613].", "problem": "In a simplified model of cell-cell communication during embryonic development, a sending cell can secrete one of two distinct signaling molecules, which we will call Signal X and Signal Y. A nearby receiving cell detects this signal and commits to one of two possible cell fates: Proliferation or Differentiation. Let the set of signals be $S = \\{\\text{X, Y}\\}$ and the set of responses be $R = \\{\\text{Proliferate, Differentiate}\\}$.\n\nAfter extensive observation, biologists have determined the joint probability distribution, $P(s, r)$, for a specific signal $s \\in S$ being sent and a specific response $r \\in R$ occurring. The probabilities are as follows:\n- The probability of Signal X being sent and the receiving cell choosing to Proliferate is $P(\\text{X, Proliferate}) = 0.40$.\n- The probability of Signal X being sent and the receiving cell choosing to Differentiate is $P(\\text{X, Differentiate}) = 0.10$.\n- The probability of Signal Y being sent and the receiving cell choosing to Proliferate is $P(\\text{Y, Proliferate}) = 0.15$.\n- The probability of Signal Y being sent and the receiving cell choosing to Differentiate is $P(\\text{Y, Differentiate}) = 0.35$.\n\nCalculate the mutual information, $I(S; R)$, which quantifies how much information the cell's response provides about the signal that was sent. Use the base-2 logarithm for your calculation. Express your answer in bits, rounded to three significant figures.", "solution": "We are given the joint distribution for signals $S=\\{\\text{X,Y}\\}$ and responses $R=\\{\\text{Proliferate, Differentiate}\\}$. First, verify normalization:\n$$0.40+0.10+0.15+0.35=1.$$\nCompute the marginals by summing over the other variable:\n$$P(\\text{X})=0.40+0.10=0.50,\\quad P(\\text{Y})=0.15+0.35=0.50,$$\n$$P(\\text{Proliferate})=0.40+0.15=0.55,\\quad P(\\text{Differentiate})=0.10+0.35=0.45.$$\nThe mutual information (with base-2 logarithm) is\n$$I(S;R)=\\sum_{s\\in S}\\sum_{r\\in R} P(s,r)\\,\\log_{2}\\!\\left(\\frac{P(s,r)}{P(s)P(r)}\\right).$$\nEvaluate each term using the computed marginals:\n$$\\frac{P(\\text{X, Proliferate})}{P(\\text{X})P(\\text{Proliferate})}=\\frac{0.40}{0.50\\cdot 0.55}=\\frac{16}{11},$$\n$$\\frac{P(\\text{X, Differentiate})}{P(\\text{X})P(\\text{Differentiate})}=\\frac{0.10}{0.50\\cdot 0.45}=\\frac{4}{9},$$\n$$\\frac{P(\\text{Y, Proliferate})}{P(\\text{Y})P(\\text{Proliferate})}=\\frac{0.15}{0.50\\cdot 0.55}=\\frac{6}{11},$$\n$$\\frac{P(\\text{Y, Differentiate})}{P(\\text{Y})P(\\text{Differentiate})}=\\frac{0.35}{0.50\\cdot 0.45}=\\frac{14}{9}.$$\nThus,\n$$I(S;R)=0.40\\,\\log_{2}\\!\\left(\\frac{16}{11}\\right)+0.10\\,\\log_{2}\\!\\left(\\frac{4}{9}\\right)+0.15\\,\\log_{2}\\!\\left(\\frac{6}{11}\\right)+0.35\\,\\log_{2}\\!\\left(\\frac{14}{9}\\right).$$\nNumerically evaluating the base-2 logarithms and summing yields\n$$I(S;R)\\approx 0.1911649569287808\\ \\text{bits}.$$\nRounded to three significant figures, the mutual information is $0.191$ bits.", "answer": "$$\\boxed{0.191}$$", "id": "1431613"}, {"introduction": "A central goal of systems biology is to understand and predict how perturbations, such as gene knockouts, affect a system's behavior. This problem illustrates how entropy can be used to quantify the change in a system's complexity or \"configurational space\" following such a perturbation. You will calculate the change in entropy for a molecular machine assembly, learning how information theory can provide a precise measure of the functional consequences of genetic changes [@problem_id:1431562].", "problem": "A large intracellular molecular machine, which we will call the \"processor,\" is assembled from a variety of protein subunits. In its normal \"wild-type\" state, the processor consists of a central static core and can bind any combination of $N$ distinct types of peripheral subunits. Experimental data suggests that all possible combinations of these $N$ subunits binding to the core are equally likely. A state of the processor is defined by the specific subset of peripheral subunits attached to the core.\n\nA research team performs a gene-editing experiment, resulting in a \"knockout\" mutant. In this mutant, the gene for a critical adaptor protein is deleted. This adaptor is essential for a specific subset of $M$ of the peripheral subunits (where $M < N$) to attach to the central core. Consequently, in the knockout cells, these $M$ subunits can no longer bind to the processor under any circumstances. The remaining $N-M$ peripheral subunits are unaffected and can still form any combination among themselves, with each possible combination being equally likely.\n\nUsing the Shannon entropy formalism with the natural logarithm, derive a closed-form analytic expression for the change in the configurational entropy of the processor assembly, $\\Delta H = H_{\\text{knockout}} - H_{\\text{wild-type}}$, due to the gene knockout. Your final expression should be in terms of $M$ and $N$.", "solution": "We model each processor state by the subset of peripheral subunits bound to the core. In the wild-type, any of the $N$ distinct subunits may be present or absent independently, and all $2^{N}$ combinations are equally likely. In the knockout, $M$ subunits are forbidden to bind, so only the remaining $N-M$ can vary freely, yielding $2^{N-M}$ equiprobable combinations.\n\nShannon entropy (with natural logarithm) for a discrete distribution $\\{p_{i}\\}$ is\n$$\nH=-\\sum_{i} p_{i}\\ln p_{i}.\n$$\nFor $S$ equiprobable states, $p_{i}=\\frac{1}{S}$ for all $i$, hence\n$$\nH=-S\\left(\\frac{1}{S}\\right)\\ln\\!\\left(\\frac{1}{S}\\right)=\\ln S.\n$$\n\nApplying this to the wild-type:\n$$\nH_{\\text{wild-type}}=\\ln\\!\\left(2^{N}\\right)=N\\ln 2.\n$$\n\nApplying this to the knockout:\n$$\nH_{\\text{knockout}}=\\ln\\!\\left(2^{N-M}\\right)=(N-M)\\ln 2.\n$$\n\nTherefore, the change in configurational entropy is\n$$\n\\Delta H=H_{\\text{knockout}}-H_{\\text{wild-type}}=(N-M)\\ln 2 - N\\ln 2=-M\\ln 2.\n$$", "answer": "$$\\boxed{-M \\ln 2}$$", "id": "1431562"}]}