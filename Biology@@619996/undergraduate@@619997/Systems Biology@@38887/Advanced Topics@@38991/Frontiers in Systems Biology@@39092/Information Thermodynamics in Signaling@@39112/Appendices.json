{"hands_on_practices": [{"introduction": "How much can a cell truly learn from a single molecular event? This practice grounds the abstract concept of information in a concrete biological scenario: a single receptor detecting a nutrient molecule. By calculating the mutual information, you will quantify precisely how much the receptor's state (bound or unbound) tells the cell about the presence of the nutrient, directly linking the biochemical parameter $K_D$ to the principles of information theory [@problem_id:1439309]. This exercise is a first step in understanding how cells \"read\" their environment at the most fundamental level.", "problem": "A simple bacterium uses a single surface receptor to detect a specific nutrient molecule in its environment. We can model this system using information theory to quantify how much the state of the receptor tells the bacterium about its surroundings.\n\nLet the environment be a random variable $X$ with two states: nutrient 'absent' ($x_0$) or nutrient 'present' ($x_1$). Let the receptor be a random variable $Y$ with two states: 'unbound' ($y_0$) or 'bound' ($y_1$).\n\nThe prior probability that the nutrient is present is $p(X=x_1) = p_L$. When the nutrient is present, its concentration is a constant $[L]$. The interaction between the nutrient and the receptor is characterized by a dissociation constant $K_D$.\n\nThe conditional probabilities of the receptor's state are determined as follows:\n- If the nutrient is absent ($X=x_0$), the receptor is always unbound. Thus, the probability of the receptor being bound is $p(Y=y_1 | X=x_0) = 0$.\n- If the nutrient is present ($X=x_1$), the probability of the receptor being in the bound state is given by the Langmuir isotherm: $p(Y=y_1 | X=x_1) = \\frac{[L]}{[L] + K_D}$.\n\nSuppose for a particular system, the parameters are:\n- Prior probability of nutrient presence: $p_L = 0.5$\n- Ligand concentration when present: $[L] = 100 \\text{ nM}$\n- Dissociation constant: $K_D = 100 \\text{ nM}$\n\nCalculate the mutual information between the state of the environment ($X$) and the state of the receptor ($Y$). Express your answer in bits, using the base-2 logarithm. Round your final answer to three significant figures.", "solution": "We denote the environment by $X \\in \\{x_{0},x_{1}\\}$ with $p(X=x_{1})=p_{L}$ and $p(X=x_{0})=1-p_{L}$, and the receptor state by $Y \\in \\{y_{0},y_{1}\\}$. The binding probability when the ligand is present is given by the Langmuir isotherm\n$$\nq \\equiv p(Y=y_{1}\\mid X=x_{1})=\\frac{[L]}{[L]+K_{D}}.\n$$\nWhen the ligand is absent, $p(Y=y_{1}\\mid X=x_{0})=0$ and $p(Y=y_{0}\\mid X=x_{0})=1$.\n\nFirst compute the marginal distribution of $Y$:\n$$\np(Y=y_{1})=p(X=x_{1})\\,p(Y=y_{1}\\mid X=x_{1})+p(X=x_{0})\\,p(Y=y_{1}\\mid X=x_{0})\n= p_{L} q + (1-p_{L})\\cdot 0 = p_{L} q,\n$$\n$$\np(Y=y_{0})=1-p(Y=y_{1})=1-p_{L} q.\n$$\n\nThe mutual information is\n$$\nI(X;Y)=H(Y)-H(Y\\mid X),\n$$\nwith all logarithms taken base $2$ (bits). The entropy of $Y$ is\n$$\nH(Y)=-p(Y=y_{1})\\log_{2} p(Y=y_{1})-p(Y=y_{0})\\log_{2} p(Y=y_{0})\n= h_{2}(p_{L} q),\n$$\nwhere $h_{2}(u)\\equiv -u\\log_{2}u-(1-u)\\log_{2}(1-u)$ is the binary entropy function. The conditional entropy is\n$$\nH(Y\\mid X)=\\sum_{x}p(x)\\,H(Y\\mid X=x)=(1-p_{L})\\cdot 0 + p_{L}\\,h_{2}(q)=p_{L}\\,h_{2}(q).\n$$\nTherefore,\n$$\nI(X;Y)=h_{2}(p_{L} q)-p_{L}\\,h_{2}(q).\n$$\n\nNow substitute the given values $p_{L}=\\frac{1}{2}$ and $q=\\frac{[L]}{[L]+K_{D}}=\\frac{100}{100+100}=\\frac{1}{2}$:\n$$\np_{L} q=\\frac{1}{4},\\quad h_{2}\\!\\left(\\frac{1}{2}\\right)=1,\n$$\n$$\nh_{2}\\!\\left(\\frac{1}{4}\\right)=-\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right)-\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{4}\\right)\n=-\\frac{1}{4}(-2)-\\frac{3}{4}\\left(\\log_{2}3-2\\right)\n=\\frac{1}{2}+\\frac{3}{4}\\left(2-\\log_{2}3\\right).\n$$\nNumerically, $\\log_{2}3\\approx 1.5849625$, hence\n$$\nh_{2}\\!\\left(\\frac{1}{4}\\right)\\approx 0.5+\\frac{3}{4}\\times 0.4150375=0.8112781,\n$$\nand\n$$\nI(X;Y)=h_{2}\\!\\left(\\frac{1}{4}\\right)-\\frac{1}{2}\\cdot 1 \\approx 0.8112781-0.5=0.3112781\\ \\text{bits}.\n$$\nRounding to three significant figures gives $0.311$ bits.", "answer": "$$\\boxed{0.311}$$", "id": "1439309"}, {"introduction": "Signaling pathways are the communication networks of the cell, but they are not perfect; they are subject to inherent noise from stochastic biochemical events. This exercise models an entire signaling pathway as a communication channel to explore the fundamental limits of information transmission in the face of such noise. You will calculate the channel capacity, which represents the maximum rate of reliable information flow, providing a deep insight into how cellular decision-making is constrained by its own noisy internal workings [@problem_id:1439300].", "problem": "A simplified model for a cellular signaling pathway considers it as a communication channel. The input to the channel is the state of an extracellular environment, which can either be the absence of a specific ligand (input state $X=0$) or its presence (input state $X=1$). The output is the state of a downstream transcriptional regulator, which can be either inactive (output state $Y=0$) or active (output state $Y=1$).\n\nDue to thermal fluctuations and stochastic biochemical reactions, the signaling process is not perfectly reliable. The system is characterized by a symmetric error probability. The probability that the regulator becomes active even in the absence of the ligand (a \"false positive\") is $p$. Similarly, the probability that the regulator remains inactive despite the presence of the ligand (a \"false negative\") is also $p$.\n\nGiven that the error probability is $p=0.08$, determine the channel capacity of this signaling pathway. The channel capacity represents the maximum rate at which information about the ligand's presence or absence can be reliably transmitted to the transcriptional regulator. Express your answer in bits, rounded to three significant figures.", "solution": "The described system is a binary symmetric channel (BSC) with crossover probability $p$, since $P(Y \\neq X \\mid X)=p$ for both input symbols. For a BSC with input distribution $P(X=1)=q$ and $P(X=0)=1-q$, the mutual information is\n$$\nI(X;Y)=H(Y)-H(Y \\mid X).\n$$\nThe conditional entropy is\n$$\nH(Y \\mid X)=q\\,H_{2}(p)+(1-q)\\,H_{2}(p)=H_{2}(p),\n$$\nwhere $H_{2}(p)=-p \\log_{2}(p)-(1-p)\\log_{2}(1-p)$ is the binary entropy function (in bits). The output distribution is\n$$\nP(Y=1)=q(1-p)+(1-q)p=p+q(1-2p),\n$$\nso\n$$\nH(Y)=H_{2}\\!\\big(p+q(1-2p)\\big).\n$$\nTherefore,\n$$\nI(X;Y)=H_{2}\\!\\big(p+q(1-2p)\\big)-H_{2}(p).\n$$\nBecause $H_{2}(\\cdot)$ is maximized at $\\tfrac{1}{2}$ and is concave, the maximum over $q$ occurs when $p+q(1-2p)=\\tfrac{1}{2}$, which for a symmetric channel yields $q=\\tfrac{1}{2}$. Hence, the channel capacity is\n$$\nC=1-H_{2}(p).\n$$\nFor $p=0.08$,\n$$\nH_{2}(0.08)=-0.08\\,\\log_{2}(0.08)-0.92\\,\\log_{2}(0.92).\n$$\nUsing $\\log_{2}(0.08)\\approx -3.641546031$ and $\\log_{2}(0.92)\\approx -0.120294233$, we obtain\n$$\nH_{2}(0.08)\\approx -0.08(-3.641546031)-0.92(-0.120294233)\\approx 0.401994378.\n$$\nThus,\n$$\nC=1-H_{2}(0.08)\\approx 1-0.401994378\\approx 0.598005622 \\text{ bits}.\n$$\nRounded to three significant figures, the capacity is $0.598$ bits.", "answer": "$$\\boxed{0.598}$$", "id": "1439300"}, {"introduction": "Accurate sensing and reliable information processing are not free; they are metabolically expensive processes. This problem explores the profound connection between information and thermodynamics by examining a kinase-phosphatase futile cycle, a ubiquitous motif in cellular signaling that consumes energy to maintain a sensitive state. By applying a theoretical model that links information gain to energy dissipation, you will calculate the additional entropy production required to achieve a more precise cellular measurement, quantitatively demonstrating the fundamental thermodynamic cost of biological information [@problem_id:1439299].", "problem": "A biological cell employs a kinase-phosphatase futile cycle as a sensory module to measure the concentration of an external signaling molecule. In this cycle, a kinase E1 phosphorylates a substrate protein S to its active form S*, and a phosphatase E2 dephosphorylates S* back to S. This cyclic process maintains a non-equilibrium steady-state concentration of S*, consuming chemical energy and thus producing entropy. The steady-state fractional phosphorylation of the substrate, which serves as the system's output, provides information about the external signal.\n\nThe mutual information, $I$, between the external signal and the system's output quantifies the fidelity of this cellular measurement. For such sensing systems operating near the linear response regime, a theoretical model relates the mutual information $I$ (measured in nats) to the total rate of entropy production $\\dot{\\Sigma}$ (in units of $k_B$/s, where $k_B$ is the Boltzmann constant) via the expression:\n$$\nI = \\frac{1}{2} \\ln(1 + \\gamma \\dot{\\Sigma})\n$$\nHere, $\\gamma$ is a constant determined by the specific kinetic parameters of the kinase and phosphatase, which we assume remains fixed. Remember that information in nats, $I_{\\text{nat}}$, is related to information in bits, $I_{\\text{bit}}$, by the conversion $I_{\\text{nat}} = I_{\\text{bit}} \\ln(2)$.\n\nInitially, the cell's sensory module operates at a steady state that provides a mutual information of $I_1 = \\log_2(3)$ bits. To improve its sensing capabilities, the cell modifies its internal machinery, reaching a new steady state where the mutual information has increased by exactly 1 bit.\n\nAssuming the given model is valid for both the initial and final steady states with the same constant $\\gamma$, calculate the dimensionless ratio of the final entropy production rate, $\\dot{\\Sigma}_2$, to the initial entropy production rate, $\\dot{\\Sigma}_1$. Present your answer as a single real number.", "solution": "The model relates mutual information in nats to entropy production via\n$$\nI=\\frac{1}{2}\\ln\\!\\left(1+\\gamma \\dot{\\Sigma}\\right).\n$$\nInformation in nats and bits are connected by $I_{\\text{nat}}=I_{\\text{bit}}\\ln(2)$. The initial mutual information is $I_{1,\\text{bit}}=\\log_{2}(3)$ bits, so in nats\n$$\nI_{1}=I_{1,\\text{bit}}\\ln(2)=\\log_{2}(3)\\ln(2)=\\ln(3).\n$$\nThe final mutual information increases by exactly 1 bit, so\n$$\nI_{2,\\text{bit}}=\\log_{2}(3)+1=\\log_{2}(6)\\quad\\Rightarrow\\quad I_{2}=\\log_{2}(6)\\ln(2)=\\ln(6).\n$$\nSolving the model for $\\dot{\\Sigma}$ gives\n$$\n2I=\\ln\\!\\left(1+\\gamma \\dot{\\Sigma}\\right)\\quad\\Rightarrow\\quad \\exp(2I)=1+\\gamma \\dot{\\Sigma}\\quad\\Rightarrow\\quad \\dot{\\Sigma}=\\frac{\\exp(2I)-1}{\\gamma}.\n$$\nTherefore, the desired ratio is\n$$\n\\frac{\\dot{\\Sigma}_{2}}{\\dot{\\Sigma}_{1}}=\\frac{\\exp\\!\\left(2I_{2}\\right)-1}{\\exp\\!\\left(2I_{1}\\right)-1}.\n$$\nSubstituting $I_{1}=\\ln(3)$ and $I_{2}=\\ln(6)$,\n$$\n\\exp\\!\\left(2I_{1}\\right)=\\exp\\!\\left(2\\ln(3)\\right)=3^{2}=9,\\qquad \\exp\\!\\left(2I_{2}\\right)=\\exp\\!\\left(2\\ln(6)\\right)=6^{2}=36,\n$$\nso\n$$\n\\frac{\\dot{\\Sigma}_{2}}{\\dot{\\Sigma}_{1}}=\\frac{36-1}{9-1}=\\frac{35}{8}.\n$$", "answer": "$$\\boxed{\\frac{35}{8}}$$", "id": "1439299"}]}