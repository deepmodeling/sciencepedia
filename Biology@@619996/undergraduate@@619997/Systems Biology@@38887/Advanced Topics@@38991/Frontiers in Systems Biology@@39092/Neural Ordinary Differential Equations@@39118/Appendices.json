{"hands_on_practices": [{"introduction": "Before we can train or use a Neural ODE, we must first define its structure. This initial exercise focuses on the \"anatomy\" of the neural network that approximates the system's dynamics. By calculating the total number of learnable parameters, you will gain a concrete understanding of how the model's complexity is determined by the dimensions of the biological system and the architecture of the chosen network. [@problem_id:1453836]", "problem": "In the field of systems biology, researchers aim to model the complex dynamics of biological networks. A modern approach involves using a Neural Ordinary Differential Equation (Neural ODE) to learn the governing equations from time-series data.\n\nConsider a simplified signaling pathway involving $N_p = 5$ distinct protein species. The state of this system at any time $t$ is described by a state vector $\\mathbf{y}(t) \\in \\mathbb{R}^{N_p}$, where the $i$-th component of $\\mathbf{y}(t)$ represents the concentration of the $i$-th protein. The dynamics are modeled by the autonomous ODE system $\\frac{d\\mathbf{y}}{dt} = f(\\mathbf{y}; \\theta)$, where the function $f$ is represented by a fully-connected feedforward neural network. This network takes the current state vector $\\mathbf{y}$ as its input and outputs its time derivative $\\frac{d\\mathbf{y}}{dt}$. The learnable parameters of the network are denoted by $\\theta$.\n\nThe architecture of this neural network is as follows:\n1.  An input layer that accepts the state vector $\\mathbf{y}$.\n2.  A single hidden layer containing $N_h = 32$ neurons.\n3.  An output layer that produces the derivative vector $\\frac{d\\mathbf{y}}{dt}$.\n\nEach layer is fully connected to the next without any skip connections. The hidden layer and the output layer each have their own bias vectors.\n\nCalculate the total number of learnable parameters (i.e., the sum of all weights and biases) in the neural network function $f$.", "solution": "The network implements a mapping from an input of dimension $N_{p}$ to a hidden layer of size $N_{h}$ and then to an output of dimension $N_{p}$. For a fully connected layer from size $a$ to size $b$, the number of weights is $ba$ and the number of biases is $b$.\n\nHidden layer parameters:\n- Weights from input to hidden: $N_{h} \\times N_{p}$.\n- Biases in hidden: $N_{h}$.\n\nOutput layer parameters:\n- Weights from hidden to output: $N_{p} \\times N_{h}$.\n- Biases in output: $N_{p}$.\n\nThus, the total number of learnable parameters is\n$$\n(N_{h}N_{p} + N_{h}) + (N_{p}N_{h} + N_{p}) = 2N_{p}N_{h} + N_{h} + N_{p}.\n$$\nSubstituting $N_{p} = 5$ and $N_{h} = 32$ gives\n$$\n2 \\cdot 5 \\cdot 32 + 32 + 5 = 320 + 37 = 357.\n$$", "answer": "$$\\boxed{357}$$", "id": "1453836"}, {"introduction": "Once a Neural ODE model is trained, it represents the general \"rules\" governing a biological system's dynamics. To predict the trajectory for a specific individual or experiment, we must provide an initial state. This conceptual problem clarifies the crucial distinction between the model's learned parameters, $\\theta$, and the initial conditions, $\\mathbf{y}(t_0)$, which are essential for generating personalized predictions and solving the underlying initial value problem. [@problem_id:1453798]", "problem": "In the field of systems biology, researchers are developing a model to forecast the progression of a chronic viral infection in a patient. They use a continuous-time model based on a Neural Ordinary Differential Equation (NODE). The state of the patient's system at any time $t$ is described by a state vector $\\mathbf{y}(t) = [V(t), C(t), D(t)]$, where:\n- $V(t)$ is the concentration of the virus in the bloodstream.\n- $C(t)$ is the population of active cytotoxic T-lymphocytes (CTLs) that target the virus.\n- $D(t)$ is the level of a specific biomarker indicating organ damage.\n\nThe NODE learns the complex underlying dynamics directly from time-series data of these three quantities. It models the rate of change of the state vector as a function parameterized by a neural network with parameters $\\theta$:\n$$\n\\frac{d\\mathbf{y}(t)}{dt} = f(\\mathbf{y}(t), t; \\theta)\n$$\nTo make a prediction for a new patient, the model is initialized with a specific vector $\\mathbf{y}(t_0)$ at the initial time of observation, $t_0$, and then the ordinary differential equation is numerically integrated forward in time.\n\nIn the context of this specific biological model, what information is encapsulated in the initial state vector $\\mathbf{y}(t_0)$?\n\nA. The learned weights and biases ($\\theta$) of the neural network that defines the biological dynamics.\n\nB. The specific, measured values of viral concentration, CTL population, and organ damage biomarker for a patient at the beginning of the observation period.\n\nC. The predicted long-term, stable values for the viral load, CTL population, and organ damage after the disease has run its course.\n\nD. The initial rates of change for the viral concentration, CTL population, and organ damage at time $t_0$.\n\nE. Static patient-specific factors, such as genetic predispositions or age, that influence the overall disease progression.", "solution": "We are given a Neural Ordinary Differential Equation defined by\n$$\n\\frac{d\\mathbf{y}(t)}{dt} = f(\\mathbf{y}(t), t; \\theta),\n$$\nwhich is an initial value problem. To generate trajectories, the model requires an initial condition at time $t_{0}$:\n$$\n\\mathbf{y}(t_{0}) = \\mathbf{y}_{0}.\n$$\nBy definition of the state vector $\\mathbf{y}(t) = [V(t), C(t), D(t)]$, the initial state is explicitly\n$$\n\\mathbf{y}(t_{0}) = [V(t_{0}),\\, C(t_{0}),\\, D(t_{0})],\n$$\nwhich are the values of the three modeled quantities at the start of the observation.\n\nWe assess each option against this formalism:\n- Option A is incorrect because the learned weights and biases are the parameters $\\theta$ in $f(\\mathbf{y}, t; \\theta)$; they are not part of $\\mathbf{y}(t_{0})$.\n- Option B is correct because $\\mathbf{y}(t_{0})$ is precisely the vector of measured initial values $V(t_{0}), C(t_{0}), D(t_{0})$ for the new patient at the beginning of observation, which serves as the initial condition for integration.\n- Option C is incorrect because long-term or stable values (e.g., equilibria) are outcomes of the dynamics after integrating forward and are not encoded in the initial condition.\n- Option D is incorrect because the initial rates of change are given by evaluating the right-hand side at the initial state:\n$$\n\\left.\\frac{d\\mathbf{y}(t)}{dt}\\right|_{t=t_{0}} = f(\\mathbf{y}(t_{0}), t_{0}; \\theta),\n$$\nwhich is a function of $\\mathbf{y}(t_{0})$ and $\\theta$, but not equal to $\\mathbf{y}(t_{0})$ itself.\n- Option E is incorrect because static patient-specific factors are not part of the defined state vector unless explicitly included as components; as stated, $\\mathbf{y}(t)$ contains only $V(t)$, $C(t)$, and $D(t)$.\n\nTherefore, the initial state vector $\\mathbf{y}(t_{0})$ encapsulates the specific measured values of the virus concentration, CTL population, and organ damage biomarker at the start of observation for the patient.", "answer": "$$\\boxed{B}$$", "id": "1453798"}, {"introduction": "The true power of a computational model lies in its ability to perform *in silico* experiments that might be difficult or impossible in a wet lab. This final practice explores how to use a trained Neural ODE to simulate a major biological interventionâ€”a permanent gene knockout. Answering this question correctly requires understanding how to modify the simulation procedure itself to reflect a change in the system's underlying rules, demonstrating the model's value as a tool for scientific inquiry. [@problem_id:1453843]", "problem": "A systems biologist is modeling a gene regulatory network consisting of $N$ genes using a Neural Ordinary Differential Equation (Neural ODE). The state of the system at time $t$ is represented by a vector $\\mathbf{y}(t) \\in \\mathbb{R}^{N}$, where each component $y_i(t)$ corresponds to the concentration of the protein product of gene $i$. The temporal evolution of the system is governed by the differential equation:\n$$\n\\frac{d\\mathbf{y}}{dt} = f(\\mathbf{y}; \\theta)\n$$\nHere, $f: \\mathbb{R}^{N} \\to \\mathbb{R}^{N}$ is a deep neural network with learned parameters $\\theta$, which approximates the true underlying vector field of the gene regulatory dynamics. The model has been successfully trained on experimental time-series data from the wild-type organism.\n\nThe biologist now wants to use this trained model to perform an *in silico* experiment simulating a permanent, complete gene knockout of a specific gene, say gene $k$ (where $1 \\leq k \\leq N$). A permanent knockout means that the gene is rendered completely non-functional, and its corresponding protein product can no longer be synthesized. The goal is to simulate the network's behavior starting from a state where the concentration of protein $k$ is zero and ensuring it remains zero for all subsequent time.\n\nWhich of the following approaches correctly modifies the simulation procedure to achieve this permanent knockout of gene $k$, using the already trained function $f(\\mathbf{y}; \\theta)$?\n\nA. Set the initial condition for the $k$-th component to zero, $y_k(t_0) = 0$, and leave the initial conditions for all other genes $y_j(t_0)$ ($j \\neq k$) unchanged. Then, solve the original ODE $\\frac{d\\mathbf{y}}{dt} = f(\\mathbf{y}; \\theta)$.\n\nB. During each evaluation of the vector field function $f(\\mathbf{y}; \\theta)$ within the ODE solver, modify the *input vector* by always setting its $k$-th component to zero before passing it to the neural network.\n\nC. Retrain the entire neural network $f$ from scratch, using a modified training dataset in which all measured values of the concentration $y_k$ have been artificially set to zero.\n\nD. Define a new vector field function $f'(\\mathbf{y}; \\theta)$ for the simulation. During each evaluation within the ODE solver, first compute the output of the original network, $\\mathbf{v} = f(\\mathbf{y}; \\theta)$, and then construct the output of $f'$ by setting its $k$-th component to zero, $v'_k = 0$, while keeping all other components the same, $v'_j = v_j$ for $j \\neq k$. Solve the new ODE $\\frac{d\\mathbf{y}}{dt} = f'(\\mathbf{y}; \\theta)$ with the initial condition $y_k(t_0) = 0$.\n\nE. Alter the architecture of the neural network by removing all connections leading to the $k$-th output neuron and setting its bias term to zero. Then, use this modified network to solve the ODE.", "solution": "We require a permanent knockout of component $k$, meaning the simulated trajectory must satisfy $y_{k}(t_{0})=0$ and remain on the invariant subspace $S=\\{\\mathbf{y}\\in\\mathbb{R}^{N}: y_{k}=0\\}$ for all subsequent time. A necessary and sufficient condition for $S$ to be invariant under the flow is that the $k$-th component of the vector field vanishes whenever $y_{k}=0$, i.e., $f_{k}(\\mathbf{y};\\theta)=0$ for all $\\mathbf{y}\\in S$. Since the trained $f$ is not guaranteed to satisfy this, the simulation procedure must be modified so that the effective vector field used by the ODE solver enforces\n$$\n\\frac{dy_{k}}{dt}=0 \\quad \\text{and} \\quad y_{k}(t_{0})=0,\n$$\nwhile allowing the remaining components to evolve according to the learned dynamics evaluated at states with $y_{k}=0$.\n\nEvaluate the options:\n\nA. Setting only $y_{k}(t_{0})=0$ and integrating $\\frac{d\\mathbf{y}}{dt}=f(\\mathbf{y};\\theta)$ does not ensure $y_{k}(t)=0$ thereafter, because in general $f_{k}(\\mathbf{y};\\theta)\\neq 0$ on $S$. Therefore $S$ need not be invariant, and $y_{k}$ can leave zero.\n\nB. Modifying the input to $f$ by projecting $\\mathbf{y}$ to $\\Pi(\\mathbf{y})$ with $\\Pi(\\mathbf{y})_{k}=0$ yields the effective ODE\n$$\n\\frac{d\\mathbf{y}}{dt}=f(\\Pi(\\mathbf{y});\\theta).\n$$\nThis still gives $\\frac{dy_{k}}{dt}=f_{k}(\\Pi(\\mathbf{y});\\theta)$, which generically is not identically zero. Hence $y_{k}$ will not be held at zero unless an additional constraint is imposed; this does not achieve a permanent knockout by itself.\n\nC. Retraining on data with $y_{k}$ set to zero is unnecessary and does not guarantee that $\\frac{dy_{k}}{dt}=0$ in simulation; moreover, it does not satisfy the requirement to use the already trained $f(\\mathbf{y};\\theta)$.\n\nD. Define a modified vector field $f'$ by masking only the $k$-th output:\n$$\n\\mathbf{v}=f(\\mathbf{y};\\theta), \\quad v'_{k}=0, \\quad v'_{j}=v_{j}\\ \\text{for}\\ j\\neq k,\n$$\nand solve\n$$\n\\frac{d\\mathbf{y}}{dt}=f'(\\mathbf{y};\\theta), \\quad y_{k}(t_{0})=0.\n$$\nThen $\\frac{dy_{k}}{dt}=0$ for all $t$, so $y_{k}(t)\\equiv 0$. For $j\\neq k$, $\\frac{dy_{j}}{dt}=f_{j}(\\mathbf{y};\\theta)$ evaluated at states with $y_{k}=0$ (since $y_{k}$ is held at zero), correctly propagating the absence of gene $k$ to the rest of the network. This uses the already trained $f$ and only modifies the simulation procedure.\n\nE. Forcing the $k$-th output to zero by altering the network architecture also makes $\\frac{dy_{k}}{dt}\\equiv 0$ and, with $y_{k}(t_{0})=0$, would keep $y_{k}(t)=0$. However, this does not use the already trained function $f(\\mathbf{y};\\theta)$ as-is; it changes the model architecture rather than the simulation procedure. The prompt asks for a modification to the simulation using the trained $f$, which $E$ does not satisfy.\n\nTherefore, only option D correctly and directly enforces a permanent knockout using the already trained $f(\\mathbf{y};\\theta)$ in the simulation.", "answer": "$$\\boxed{D}$$", "id": "1453843"}]}