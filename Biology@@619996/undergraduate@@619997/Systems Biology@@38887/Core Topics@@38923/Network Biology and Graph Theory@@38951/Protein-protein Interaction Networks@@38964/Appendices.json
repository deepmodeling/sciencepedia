{"hands_on_practices": [{"introduction": "A protein-protein interaction (PPI) network is a map of the cellular machinery, but this map is not observed directly. Instead, it is constructed from experimental data. This exercise explores how we translate raw data from a common technique, Affinity Purification-Mass Spectrometry (AP-MS), into a network graph. You will apply two different interpretation models—the conservative \"spoke\" model and the more inclusive \"matrix\" model—to quantify how these modeling assumptions can profoundly alter the final network topology [@problem_id:1460618].", "problem": "A systems biology research team is studying the protein interaction network of a model organism. They perform a series of three Affinity Purification-Mass Spectrometry (AP-MS) experiments to identify protein complexes. In each experiment, a single \"bait\" protein is used to pull down its binding partners, which are then identified as \"prey\" proteins. The results are as follows:\n\n- Experiment 1: The bait protein is P1. The identified prey proteins are {P2, P3, P4}.\n- Experiment 2: The bait protein is P2. The identified prey proteins are {P1, P4, P5}.\n- Experiment 3: The bait protein is P5. The identified prey proteins are {P2, P6}.\n\nTo build a Protein-Protein Interaction (PPI) network from this data, the team considers two different inference models: the spoke model and the matrix model.\n\n- **Spoke Model**: In each experiment, an interaction is inferred only between the bait protein and each of the prey proteins. No interactions are inferred among prey proteins.\n- **Matrix Model**: In each experiment, it is assumed that all proteins identified (the bait and all its preys) form a fully connected complex (a clique). An interaction is thus inferred between every possible pair of proteins within that set.\n\nConsidering the data from all three experiments combined, calculate the total number of unique protein-protein interactions that would be inferred by each model. Let $N_{\\text{spoke}}$ be the total number of unique interactions from the spoke model, and $N_{\\text{matrix}}$ be the total number of unique interactions from the matrix model.\n\nPresent your answer as a row matrix with two elements, in the order $N_{\\text{spoke}}$, then $N_{\\text{matrix}}$.", "solution": "We interpret protein-protein interactions as undirected pairs. For each experiment, we form the inferred interaction set according to the specified model and then take the union across all experiments to count unique interactions.\n\nSpoke model:\nExperiment 1 (bait $\\mathrm{P1}$, preys $\\{\\mathrm{P2},\\mathrm{P3},\\mathrm{P4}\\}$) yields\n$$\nE_{1}^{s}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P1},\\mathrm{P3}),(\\mathrm{P1},\\mathrm{P4})\\}.\n$$\nExperiment 2 (bait $\\mathrm{P2}$, preys $\\{\\mathrm{P1},\\mathrm{P4},\\mathrm{P5}\\}$) yields\n$$\nE_{2}^{s}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P2},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P5})\\}.\n$$\nExperiment 3 (bait $\\mathrm{P5}$, preys $\\{\\mathrm{P2},\\mathrm{P6}\\}$) yields\n$$\nE_{3}^{s}=\\{(\\mathrm{P2},\\mathrm{P5}),(\\mathrm{P5},\\mathrm{P6})\\}.\n$$\nThe union of unique pairs is\n$$\nE^{s}=E_{1}^{s}\\cup E_{2}^{s}\\cup E_{3}^{s}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P1},\\mathrm{P3}),(\\mathrm{P1},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P5}),(\\mathrm{P5},\\mathrm{P6})\\},\n$$\nso\n$$\nN_{\\text{spoke}}=|E^{s}|=6.\n$$\n\nMatrix model:\nFor each experiment, all identified proteins form a clique. Let the identified sets be\n$$\nS_{1}=\\{\\mathrm{P1},\\mathrm{P2},\\mathrm{P3},\\mathrm{P4}\\},\\quad S_{2}=\\{\\mathrm{P1},\\mathrm{P2},\\mathrm{P4},\\mathrm{P5}\\},\\quad S_{3}=\\{\\mathrm{P2},\\mathrm{P5},\\mathrm{P6}\\}.\n$$\nTheir clique edges are\n$$\nE_{1}^{m}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P1},\\mathrm{P3}),(\\mathrm{P1},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P3}),(\\mathrm{P2},\\mathrm{P4}),(\\mathrm{P3},\\mathrm{P4})\\},\n$$\n$$\nE_{2}^{m}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P1},\\mathrm{P4}),(\\mathrm{P1},\\mathrm{P5}),(\\mathrm{P2},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P5}),(\\mathrm{P4},\\mathrm{P5})\\},\n$$\n$$\nE_{3}^{m}=\\{(\\mathrm{P2},\\mathrm{P5}),(\\mathrm{P2},\\mathrm{P6}),(\\mathrm{P5},\\mathrm{P6})\\}.\n$$\nTaking the union,\n$$\nE^{m}=E_{1}^{m}\\cup E_{2}^{m}\\cup E_{3}^{m}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P1},\\mathrm{P3}),(\\mathrm{P1},\\mathrm{P4}),(\\mathrm{P1},\\mathrm{P5}),(\\mathrm{P2},\\mathrm{P3}),(\\mathrm{P2},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P5}),(\\mathrm{P2},\\mathrm{P6}),(\\mathrm{P3},\\mathrm{P4}),(\\mathrm{P4},\\mathrm{P5}),(\\mathrm{P5},\\mathrm{P6})\\},\n$$\nso\n$$\nN_{\\text{matrix}}=|E^{m}|=11.\n$$\n\nHence, the totals are $N_{\\text{spoke}}=6$ and $N_{\\text{matrix}}=11$.", "answer": "$$\\boxed{\\begin{pmatrix}6 & 11\\end{pmatrix}}$$", "id": "1460618"}, {"introduction": "Once a network is constructed, analyzing its architecture reveals crucial functional insights. Many PPI networks exhibit a \"scale-free\" structure, characterized by a few highly connected \"hub\" proteins. This practice demonstrates a key property of such networks: their surprising combination of robustness against random errors and fragility against targeted attacks [@problem_id:1460589]. By simulating the removal of proteins from a simplified network, you will quantify why targeting hubs is far more disruptive than random failures, a vital concept for understanding disease progression and drug targeting.", "problem": "In systems biology, Protein-Protein Interaction (PPI) networks are often modeled as scale-free networks, characterized by the presence of a few highly connected proteins (hubs) and many proteins with few connections. The robustness of such a network to the removal of its constituent proteins is a critical question.\n\nConsider a simplified model of a PPI network consisting of a total of $N=1000$ proteins. The proteins are divided into two classes:\n1.  **Hubs**: There are $N_h = 100$ hub proteins, which have a very high number of connections (high degree).\n2.  **Peripheral Proteins**: There are $N_p = 900$ peripheral proteins, which have a low number of connections (low degree).\n\nThe connectivity of the network is defined as follows:\n*   The 100 hub proteins are all densely connected to each other, forming a single, stable core for the network.\n*   Each of the 900 peripheral proteins is connected to exactly one hub protein. These connections are distributed evenly, meaning each of the 100 hubs serves as an \"anchor\" for exactly $N_p/N_h = 9$ peripheral proteins.\n*   Peripheral proteins do not have any connections to each other.\n\nInitially, the entire network is fully connected, forming a single Largest Connected Component (LCC) of size 1000. A protein is considered disconnected if it is no longer part of this main LCC. The total damage to the network is measured by the total number of disconnected proteins, which is the initial LCC size minus the final LCC size after protein removal.\n\nYou are to compare two different strategies for removing 10% of the proteins from the network (i.e., removing 100 proteins).\n\n**Scenario A (Targeted Attack)**: The 100 proteins with the highest degree are deliberately removed. In this model, these are the 100 hub proteins.\n**Scenario B (Random Failure)**: 100 proteins are chosen completely at random from the entire network and removed.\n\nLet $D_A$ be the total number of proteins disconnected from the LCC in Scenario A, and $D_B$ be the total number of proteins disconnected in Scenario B. Calculate the ratio of the damage from the targeted attack to the damage from the random failure, $D_A / D_B$.\n\nGive your answer as a numerical value rounded to three significant figures.", "solution": "We analyze the two scenarios to calculate the expected damage, measured by the number of disconnected proteins. The initial network has a Largest Connected Component (LCC) of size 1000.\n\n**Scenario A (Targeted Attack):**\nThe 100 hub proteins are removed. Since all peripheral proteins are connected only to hubs, and the hubs form the network's core, removing all hubs disintegrates the network. The remaining 900 peripheral proteins become isolated nodes. The new LCC consists of a single protein, so its size is 1. The total damage is the number of disconnected proteins:\n$$\nD_{A} = (\\text{Initial LCC size}) - (\\text{Final LCC size}) = 1000 - 1 = 999.\n$$\n\n**Scenario B (Random Failure):**\n100 proteins are removed at random. We calculate the expected size of the LCC after removal. The LCC is composed of surviving hubs (which are all interconnected) and any surviving peripheral proteins whose anchor hub also survived.\n\nLet's find the expected number of nodes in the final LCC. By linearity of expectation, this is the sum of probabilities that each node is part of the LCC.\n- **Surviving Hubs:** A hub survives with probability $P(\\text{hub survives}) = \\frac{1000 - 100}{1000} = 0.9$. The expected number of surviving hubs is $100 \\times 0.9 = 90$. As long as at least one hub survives, all surviving hubs form a connected component.\n- **Surviving Peripherals in LCC:** A peripheral protein contributes to the LCC if and only if both it and its specific anchor hub survive. The probability of this for a single peripheral is the probability of choosing 100 nodes to remove from the 998 other nodes in the network:\n$$\nP(\\text{peripheral and its hub survive}) = \\frac{\\binom{998}{100}}{\\binom{1000}{100}} = \\frac{\\frac{998!}{100!898!}}{\\frac{1000!}{100!900!}} = \\frac{900 \\times 899}{1000 \\times 999}.\n$$\nThere are 900 peripheral proteins, so the expected number of peripherals remaining in the LCC is:\n$$\n\\mathbb{E}[\\text{peripherals in LCC}] = 900 \\times \\frac{900 \\times 899}{1000 \\times 999} = 810 \\times \\frac{899}{999} = \\frac{26970}{37}.\n$$\nThe expected size of the final LCC is the sum of these expectations:\n$$\n\\mathbb{E}[\\text{LCC size}] = 90 + \\frac{26970}{37} = \\frac{3330 + 26970}{37} = \\frac{30300}{37}.\n$$\nThe expected damage $D_B$ is the initial size minus the expected final LCC size:\n$$\nD_B = 1000 - \\mathbb{E}[\\text{LCC size}] = 1000 - \\frac{30300}{37} = \\frac{37000 - 30300}{37} = \\frac{6700}{37} \\approx 181.08.\n$$\nThis calculation is precise, as the case where all hubs are removed (an event with vanishingly small probability) is correctly handled by the expectation.\n\n**Ratio of Damage:**\nFinally, we calculate the ratio $D_A / D_B$:\n$$\n\\frac{D_A}{D_B} = \\frac{999}{\\frac{6700}{37}} = \\frac{999 \\times 37}{6700} = \\frac{36963}{6700} \\approx 5.5168657.\n$$\nRounded to three significant figures, the ratio is $5.52$.", "answer": "$$\\boxed{5.52}$$", "id": "1460589"}, {"introduction": "Beyond describing structure, we can use PPI networks to make powerful functional predictions based on the \"guilt-by-association\" principle. This advanced computational practice operationalizes this idea by having you implement the Random Walk with Restart (RWR) algorithm, a cornerstone of network medicine. Starting with a set of known disease-related proteins, you will use the network's wiring to rank other proteins by their relevance, a common and critical task in identifying novel disease genes or potential drug targets [@problem_id:2423157].", "problem": "A Protein-Protein Interaction (PPI) network is modeled as a finite, undirected, simple graph on $n$ nodes, represented by an adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ with $A_{ij} = 1$ if and only if there is an interaction between proteins $i$ and $j$, and $A_{ij} = 0$ otherwise. Let $S \\subset \\{0,1,\\dots,n-1\\}$ be a nonempty set of seed indices corresponding to known cancer genes. Define the seed distribution vector $s \\in \\mathbb{R}^n$ by $s_i = \\frac{1}{|S|}$ if $i \\in S$ and $s_i = 0$ otherwise. For each node $j$ with degree $d_j = \\sum_{i=1}^n A_{ij}$ equal to zero, set $A_{jj} \\leftarrow 1$ (a self-loop) prior to normalization so that a random walk is well-defined. Define the column-stochastic transition matrix $W \\in \\mathbb{R}^{n \\times n}$ by\n$$\nW = A D^{-1},\n$$\nwhere $D \\in \\mathbb{R}^{n \\times n}$ is diagonal with $D_{jj} = \\sum_{i=1}^n A_{ij}$ after the self-loop adjustment, so that for each column $j$ we have $\\sum_{i=1}^n W_{ij} = 1$.\n\nGiven a restart probability $\\alpha \\in [0,1]$, the Random Walk with Restart (RWR) process on this graph is the sequence $\\{p^{(t)}\\}_{t=0}^{\\infty}$ with $p^{(0)} = s$ and\n$$\np^{(t+1)} = (1-\\alpha) \\, W \\, p^{(t)} + \\alpha \\, s \\quad \\text{for all } t \\geq 0.\n$$\nThe process is iterated until convergence in the $\\ell_1$ norm with tolerance $\\varepsilon > 0$, that is, until $\\lVert p^{(t+1)} - p^{(t)} \\rVert_1  \\varepsilon$, or until a hard cap of $T_{\\max} \\in \\mathbb{N}$ iterations is reached. Let the converged vector be denoted by $p^\\star$ (if $\\alpha = 1$, then $p^\\star = s$ exactly). The task is to rank candidate proteins (nodes) by descending $p^\\star_i$ values, excluding all seed indices in $S$. Ties must be broken by increasing node index. Given an integer $k$ with $1 \\le k \\le n - |S|$, the required output for each test case is the list of the top-$k$ non-seed node indices according to the described ranking rule.\n\nYour program must implement the mathematics above exactly and apply it to the following test suite. All nodes are indexed by integers starting at $0$. Each test specifies $(A, S, \\alpha, \\varepsilon, T_{\\max}, k)$.\n\nTest A (general connected case):\n- Adjacency $A^{(\\mathrm{A})} \\in \\{0,1\\}^{6 \\times 6}$:\n$$\nA^{(\\mathrm{A})} =\n\\begin{bmatrix}\n0  1  0  0  0  0 \\\\\n1  0  1  0  0  1 \\\\\n0  1  0  1  0  0 \\\\\n0  0  1  0  1  0 \\\\\n0  0  0  1  0  1 \\\\\n0  1  0  0  1  0\n\\end{bmatrix}\n$$\n- Seeds $S^{(\\mathrm{A})} = \\{0,2\\}$.\n- Restart probability $\\alpha^{(\\mathrm{A})} = 0.5$.\n- Tolerance $\\varepsilon^{(\\mathrm{A})} = 10^{-10}$.\n- Maximum iterations $T_{\\max}^{(\\mathrm{A})} = 10000$.\n- Top count $k^{(\\mathrm{A})} = 3$.\n\nTest B (disconnected network to test isolation handling):\n- Adjacency $A^{(\\mathrm{B})} \\in \\{0,1\\}^{6 \\times 6}$:\n$$\nA^{(\\mathrm{B})} =\n\\begin{bmatrix}\n0  1  1  0  0  0 \\\\\n1  0  1  0  0  0 \\\\\n1  1  0  0  0  0 \\\\\n0  0  0  0  1  0 \\\\\n0  0  0  1  0  1 \\\\\n0  0  0  0  1  0\n\\end{bmatrix}\n$$\n- Seeds $S^{(\\mathrm{B})} = \\{0\\}$.\n- Restart probability $\\alpha^{(\\mathrm{B})} = 0.7$.\n- Tolerance $\\varepsilon^{(\\mathrm{B})} = 10^{-12}$.\n- Maximum iterations $T_{\\max}^{(\\mathrm{B})} = 10000$.\n- Top count $k^{(\\mathrm{B})} = 4$.\n\nTest C (high restart probability emphasizing proximity to seeds):\n- Adjacency $A^{(\\mathrm{C})} \\in \\{0,1\\}^{5 \\times 5}$:\n$$\nA^{(\\mathrm{C})} =\n\\begin{bmatrix}\n0  1  1  1  0 \\\\\n1  0  0  0  0 \\\\\n1  0  0  0  0 \\\\\n1  0  0  0  1 \\\\\n0  0  0  1  0\n\\end{bmatrix}\n$$\n- Seeds $S^{(\\mathrm{C})} = \\{0\\}$.\n- Restart probability $\\alpha^{(\\mathrm{C})} = 0.95$.\n- Tolerance $\\varepsilon^{(\\mathrm{C})} = 10^{-12}$.\n- Maximum iterations $T_{\\max}^{(\\mathrm{C})} = 10000$.\n- Top count $k^{(\\mathrm{C})} = 3$.\n\nTest D (boundary case $\\alpha = 1$):\n- Adjacency $A^{(\\mathrm{D})} \\in \\{0,1\\}^{4 \\times 4}$:\n$$\nA^{(\\mathrm{D})} =\n\\begin{bmatrix}\n0  1  0  0 \\\\\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n0  0  1  0\n\\end{bmatrix}\n$$\n- Seeds $S^{(\\mathrm{D})} = \\{1,2\\}$.\n- Restart probability $\\alpha^{(\\mathrm{D})} = 1$.\n- Tolerance $\\varepsilon^{(\\mathrm{D})} = 10^{-12}$.\n- Maximum iterations $T_{\\max}^{(\\mathrm{D})} = 10000$.\n- Top count $k^{(\\mathrm{D})} = 2$.\n\nFor each test, compute $p^\\star$ according to the definition above and return the indices of the top-$k$ non-seed nodes ranked by descending $p^\\star_i$, with ties broken by increasing index. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\,\\text{resultA},\\text{resultB},\\text{resultC},\\text{resultD}\\,]$). In this problem, each result is a list of $k$ integers, so the final output must be a single line of the form $[[i_{1},\\dots,i_{k}], [j_{1},\\dots,j_{k}], [\\dots], [\\dots]]$ where all entries are integers and lists are in the order of the tests A, B, C, D.", "solution": "The problem statement has been validated and is deemed scientifically sound, well-posed, and complete. It presents a standard application of the Random Walk with Restart (RWR) algorithm for node prioritization in a network, a common task in computational biology. We shall proceed with a rigorous, step-by-step solution.\n\nThe objective is to identify and rank non-seed proteins in a Protein-Protein Interaction (PPI) network that are most likely associated with a given set of seed proteins, which are known to be involved in a specific pathology such as cancer. The ranking is based on the steady-state probability distribution of a random walker that explores the network, with a constant probability of returning to one of the seed nodes. This methodology quantifies a form of proximity or relevance in the network context.\n\nThe algorithm proceeds in three main stages: network preprocessing, iterative calculation of the RWR probabilities, and ranking of the results.\n\n**1. Network Preprocessing and Initialization**\n\nFirst, we must construct the necessary mathematical objects from the given inputs. The inputs for each test case are an adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$, a set of seed indices $S$, a restart probability $\\alpha \\in [0,1]$, a convergence tolerance $\\varepsilon > 0$, a maximum number of iterations $T_{\\max}$, and the number of top candidates to report, $k$.\n\n**1.1. Modification of the Adjacency Matrix**\nA random walk is undefined on nodes with no outgoing edges (isolated nodes). The problem specifies a standard procedure to handle this: for any node $j$ with degree $d_j = \\sum_{i=0}^{n-1} A_{ij} = 0$, we introduce a self-loop by setting $A_{jj} \\leftarrow 1$. This ensures that every node has at least one outgoing edge, making the degree matrix invertible and the random walk well-defined everywhere. Let us denote the potentially modified adjacency matrix as $A'$.\n\n**1.2. Construction of the Transition Matrix $W$**\nThe random walker moves from a node to one of its neighbors with uniform probability. This is formalized by the column-stochastic transition matrix $W$. We first compute the degree matrix $D'$, which is a diagonal matrix where the entry $D'_{jj}$ is the degree of node $j$ calculated from $A'$:\n$$\nD'_{jj} = \\sum_{i=0}^{n-1} A'_{ij}\n$$\nThe transition matrix $W \\in \\mathbb{R}^{n \\times n}$ is then obtained by normalizing the columns of $A'$:\n$$\nW = A' (D')^{-1}\n$$\nThis is equivalent to setting each element $W_{ij}$ to:\n$$\nW_{ij} = \\frac{A'_{ij}}{D'_{jj}} = \\frac{A'_{ij}}{\\sum_{l=0}^{n-1} A'_{lj}}\n$$\nBy construction, each column of $W$ sums to $1$, i.e., $\\sum_{i=0}^{n-1} W_{ij} = 1$ for all $j \\in \\{0, \\dots, n-1\\}$, correctly representing a probability distribution for the next step of a walker currently at node $j$.\n\n**1.3. Initialization of Probability Vectors**\nThe RWR process tracks a probability distribution over the nodes of the graph, represented by a vector $p^{(t)} \\in \\mathbb{R}^n$, where $p_i^{(t)}$ is the probability of finding the walker at node $i$ at iteration step $t$. The process starts from the seed nodes. The initial distribution, $p^{(0)}$, is defined as a uniform distribution over the seed set $S$. This is captured by the seed distribution vector $s \\in \\mathbb{R}^n$:\n$$\ns_i =\n\\begin{cases}\n\\frac{1}{|S|}  \\text{if } i \\in S \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nWe initialize the process by setting $p^{(0)} = s$.\n\n**2. The Random Walk with Restart Iteration**\n\nThe core of the algorithm is the iterative update rule. At each step $t$, the walker can either move to an adjacent node according to the transition matrix $W$ with probability $1-\\alpha$, or \"restart\" by jumping back to one of the seed nodes (according to the distribution $s$) with probability $\\alpha$. This is captured by the following recurrence relation:\n$$\np^{(t+1)} = (1-\\alpha) \\, W \\, p^{(t)} + \\alpha \\, s\n$$\nThis iteration is performed until a termination condition is met:\n1.  **Convergence:** The change in the probability vector becomes negligible. We measure this using the $\\ell_1$ norm: $\\lVert p^{(t+1)} - p^{(t)} \\rVert_1 = \\sum_{i=0}^{n-1} |p^{(t+1)}_i - p^{(t)}_i|  \\varepsilon$.\n2.  **Maximum Iterations:** The number of iterations reaches the predefined limit $T_{\\max}$.\n\nThe final vector, denoted $p^\\star$, is the converged probability distribution. This vector represents the steady-state probability of finding the walker at each node. A higher probability $p^\\star_i$ suggests that node $i$ is more \"connected\" or \"relevant\" to the initial seed set $S$.\n\nA special case arises when $\\alpha = 1$. The update rule simplifies to $p^{(t+1)} = s$. In this scenario, the walker always restarts, so the distribution never evolves away from the initial seed distribution. Thus, $p^\\star = s$ without any need for iteration.\n\n**3. Ranking and Selection of Candidate Proteins**\n\nOnce the steady-state probability vector $p^\\star$ has been computed, we use it to rank potential candidate genes. The candidates are all nodes that are not in the original seed set $S$. The ranking criteria are:\n1.  **Primary Sort Key:** The score $p^\\star_i$ in descending order.\n2.  **Secondary Sort Key (Tie-breaker):** The node index $i$ in ascending order.\n\nAfter sorting all non-seed nodes according to these rules, we select the top-$k$ nodes from this sorted list. The final result for each test case is the list of these $k$ node indices.\n\nThe implementation will follow these steps precisely for each of the provided test cases.", "answer": "`[[1,3,5],[1,2,4,5],[1,2,3],[0,3]]`", "id": "2423157"}]}