{
    "hands_on_practices": [
        {
            "introduction": "A core function of modern health information infrastructure is translating data between different standards, such as from the legacy Health Level Seven (HL7) v2 format to the modern Fast Healthcare Interoperability Resources (FHIR) standard. However, this process is rarely perfect, as target systems may not fully support all the information contained in the source message. This exercise  provides a hands-on look at designing such a transformation and, more importantly, quantifying the resulting information loss when specific data types are unsupported.",
            "id": "4372614",
            "problem": "A health system ingests clinical data via Health Level Seven (HL7) version $2$ Observation Result Unsolicited (ORU$^{\\wedge}$R$01$) messages and transforms them into Fast Healthcare Interoperability Resources (FHIR) `Patient` and `Observation` resources. Consider the following scientifically realistic sample message content:\n\n- Patient Identification (PID):\n  - PID$-$3 Patient Identifier List contains two identifiers: Assigning authority \"HospitalA\", identifier value \"$A$-$12345$\", and Assigning authority \"StateRegistry\", identifier value \"$SR$-$987654$\".\n  - PID$-$5 Patient Name is \"Doe^Jane\".\n  - PID$-$7 Date/Time of Birth is \"$1997$-$08$-$14$\" (HL7 $DTM$).\n  - PID$-$8 Administrative Sex is \"F\".\n\n- Observation/Result (OBX) segments:\n  - OBX segment $1$: OBX$-$$2$ (Value Type) is \"NM\" (numeric). OBX$-$$3$ (Observation Identifier) is LOINC \"$2823$-$3$\" for \"Potassium\". OBX$-$$5$ (Observation Value) is numeric value \"$4.2$\". OBX$-$$6$ (Units) is Unified Code for Units of Measure (UCUM) \"mmol/L\". OBX$-$$14$ (Date/Time of the Observation) is \"$2025$-$10$-$02$T$09$:$15$:$00$Z\".\n  - OBX segment $2$: OBX$-$$2$ (Value Type) is \"TX\" (text). OBX$-$$3$ is LOINC \"$48767$-$8$\" \"Narrative comment\". OBX$-$$5$ is \"Specimen hemolyzed\".\n  - OBX segment $3$: OBX$-$$2$ (Value Type) is \"CWE\" (coded with exceptions). OBX$-$$3$ is LOINC \"$630$-$1$\" \"Antibiotic susceptibility\". OBX$-$$5$ contains code \"$131196009$\", coding system \"http://snomed.info/sct\", display \"Susceptible\", original text \"S\".\n  - OBX segment $4$: OBX$-$$2$ (Value Type) is \"ED\" (encapsulated data). OBX$-$$3$ is LOINC \"$55110$-$1$\" \"Imaging report\". OBX$-$$5$ is encapsulated data with components: source application \"RIS\", type of data \"application/pdf\", data subtype \"PDF\", encoding \"Base64\", and a binary payload of length \"$102400$\" bytes.\n\nDesign a transformation that produces:\n- One FHIR `Patient` resource populated from PID fields (identifiers from PID$-$3, name from PID$-$5, birthDate from PID$-$7, gender from PID$-$8).\n- Four FHIR `Observation` resources, one per OBX, where:\n  - For \"NM\" values, use `Observation.valueQuantity` with the numeric value and UCUM unit.\n  - For \"TX\" values, use `Observation.valueString`.\n  - For \"CWE\" values, use `Observation.valueCodeableConcept` when supported.\n  - For \"ED\" values, use `Observation.valueAttachment` when supported.\n\nAssume the current transformation implementation supports only \"NM\" and \"TX\" value types in OBX$-$$5$ and does not support \"CWE\" or \"ED\" value types in OBX$-$$5$; unsupported value types result in no preservation of OBX$-$$5$ content in the corresponding FHIR Observation.\n\nFor the purposes of the calculation, define the information content contributed by OBX$-$$5$ as a count of atomic elements per value type, as follows:\n- For \"NM\": two atomic elements, consisting of the numeric value ($1$) and the UCUM unit ($1$).\n- For \"TX\": one atomic element, consisting of the narrative string ($1$).\n- For \"CWE\": four atomic elements, consisting of the code ($1$), coding system ($1$), display text ($1$), and original text ($1$).\n- For \"ED\": five atomic elements, consisting of source application ($1$), type of data ($1$), data subtype ($1$), encoding ($1$), and payload ($1$).\n\nTreat only OBX$-$$5$ content (as enumerated above) as contributing to the information content metric. Define the information loss as the fraction of OBX$-$$5$ atomic elements from the message that are not preserved in the resulting FHIR Observations under the stated implementation constraints. Express the information loss as a decimal fraction and round your answer to four significant figures.",
            "solution": "The user has provided a problem statement that must be validated for scientific correctness, objectivity, and completeness before a solution can be attempted.\n\n### Step 1: Extract Givens\n\nThe problem provides the following data and definitions:\n- **Input Data Format:** Health Level Seven (HL7) version $2$ Observation Result Unsolicited (ORU$^{\\wedge}$R$01$) messages.\n- **Output Data Format:** Fast Healthcare Interoperability Resources (FHIR) Patient and Observation resources.\n- **Patient Identification (PID) Data:**\n  - PID$-$3: Two identifiers: (Authority: \"HospitalA\", Value: \"$A$-$12345$\"), (Authority: \"StateRegistry\", Value: \"$SR$-$987654$\").\n  - PID$-$5: Name: \"Doe^Jane\".\n  - PID$-$7: Date of Birth: \"$1997$-$08$-$14$\".\n  - PID$-$8: Administrative Sex: \"F\".\n- **Observation/Result (OBX) Segments Data:**\n  - **OBX segment $1$**: Value Type \"NM\" (numeric), Observation Identifier LOINC \"$2823$-$3$\", Observation Value \"$4.2$\", Units UCUM \"mmol/L\".\n  - **OBX segment $2$**: Value Type \"TX\" (text), Observation Identifier LOINC \"$48767$-$8$\", Observation Value \"Specimen hemolyzed\".\n  - **OBX segment $3$**: Value Type \"CWE\" (coded with exceptions), Observation Identifier LOINC \"$630$-$1$\", Observation Value components: code \"$131196009$\", system \"http://snomed.info/sct\", display \"Susceptible\", original text \"S\".\n  - **OBX segment $4$**: Value Type \"ED\" (encapsulated data), Observation Identifier LOINC \"$55110$-$1$\", Observation Value components: source application \"RIS\", type of data \"application/pdf\", data subtype \"PDF\", encoding \"Base64\", payload length \"$102400$\" bytes.\n- **Transformation Constraint:** The current transformation implementation supports only \"NM\" and \"TX\" value types from the OBX$-$$5$ field. Content from unsupported value types (\"CWE\", \"ED\") is not preserved.\n- **Information Content Metric (for OBX$-$$5$):**\n  - \"NM\": $2$ atomic elements (value, unit).\n  - \"TX\": $1$ atomic element (string).\n  - \"CWE\": $4$ atomic elements (code, system, display, original text).\n  - \"ED\": $5$ atomic elements (source app, data type, data subtype, encoding, payload).\n- **Objective:** Calculate the information loss, defined as the fraction of OBX$-$$5$ atomic elements not preserved in the transformation. The result must be a decimal fraction rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded:** The problem is grounded in the established field of health informatics and health systems science. It uses real-world, standardized data formats (HL7 v2, FHIR), coding systems (LOINC, SNOMED CT, UCUM), and describes a common, practical challenge in data interoperability. All terminology and scenarios are factually sound and scientifically realistic.\n- **Well-Posed:** The problem is well-posed. It provides all necessary information: the source data (four OBX segments), the explicit rules of transformation (which value types are supported), and a precise, quantitative definition for \"information content\" and \"information loss\". This ensures that a unique and meaningful solution exists.\n- **Objective:** The problem is stated in objective, technical language. The metrics for \"atomic elements\" and \"information loss\" are explicitly defined, removing any subjectivity or ambiguity from the calculation.\n- **Completeness and Consistency:** The problem is self-contained and internally consistent. It provides a complete dataset and a clear set of rules. There are no contradictions.\n- **Realism:** The scenario is realistic. Partial implementations of data standards are a frequent cause of information loss in real-world health information systems.\n\n### Step 3: Verdict and Action\n\nThe problem is deemed **valid**. It is scientifically sound, well-posed, objective, and complete. The solution process can proceed.\n\n### Solution\n\nThe objective is to calculate the information loss, $L$, which is defined as the fraction of atomic elements from the OBX$-$$5$ fields that are not preserved during the transformation. The formula for information loss is:\n$$L = \\frac{I_{\\text{lost}}}{I_{\\text{total}}}$$\nwhere $I_{\\text{total}}$ is the total number of original atomic elements and $I_{\\text{lost}}$ is the number of atomic elements lost in the transformation.\n\nFirst, we calculate the total original information content, $I_{\\text{total}}$, by summing the atomic elements from the OBX$-$$5$ field of each of the four OBX segments, according to the problem's definitions.\n\n- **OBX segment $1$ (Value Type \"NM\"):** Contains a numeric value and a unit. According to the definition, this corresponds to $I_1 = 2$ atomic elements.\n- **OBX segment $2$ (Value Type \"TX\"):** Contains a narrative string. This corresponds to $I_2 = 1$ atomic element.\n- **OBX segment $3$ (Value Type \"CWE\"):** Contains a code, coding system, display text, and original text. This corresponds to $I_3 = 4$ atomic elements.\n- **OBX segment $4$ (Value Type \"ED\"):** Contains a source application, type of data, data subtype, encoding, and a payload. This corresponds to $I_4 = 5$ atomic elements.\n\nThe total original information content is the sum of the elements from all four segments:\n$$I_{\\text{total}} = I_1 + I_2 + I_3 + I_4 = 2 + 1 + 4 + 5 = 12$$\n\nNext, we determine the total preserved information content, $I_{\\text{preserved}}$, based on the system's processing capabilities. The problem states that the implementation only supports the \"NM\" and \"TX\" value types. The OBX$-$$5$ content for unsupported types is discarded.\n\n- Content from OBX segment $1$ (\"NM\") is preserved. Preserved elements = $2$.\n- Content from OBX segment $2$ (\"TX\") is preserved. Preserved elements = $1$.\n- Content from OBX segment $3$ (\"CWE\") is not supported and thus not preserved. Preserved elements = $0$.\n- Content from OBX segment $4$ (\"ED\") is not supported and thus not preserved. Preserved elements = $0$.\n\nThe total preserved information content is:\n$$I_{\\text{preserved}} = 2 + 1 + 0 + 0 = 3$$\n\nNow, we can calculate the total lost information content, $I_{\\text{lost}}$. This is the difference between the total original content and the total preserved content.\n$$I_{\\text{lost}} = I_{\\text{total}} - I_{\\text{preserved}} = 12 - 3 = 9$$\n\nAlternatively, $I_{\\text{lost}}$ is the sum of elements from the unsupported segments: $I_{\\text{lost}} = I_3 + I_4 = 4 + 5 = 9$.\n\nFinally, we calculate the information loss fraction, $L$:\n$$L = \\frac{I_{\\text{lost}}}{I_{\\text{total}}} = \\frac{9}{12} = \\frac{3}{4}$$\n\nConverting this fraction to a decimal gives:\n$$L = 0.75$$\n\nThe problem requires the answer to be rounded to four significant figures. To express $0.75$ with four significant figures, we add trailing zeros.\n$$L = 0.7500$$\n\nThis represents a $75\\%$ loss of the defined information content from the OBX$-$$5$ fields.",
            "answer": "$$\\boxed{0.7500}$$"
        },
        {
            "introduction": "Effective health information exchange goes beyond just moving data; it requires that all systems share a common understanding of the data's meaning, a concept known as semantic interoperability. A failure in this area, such as a unit normalization error, can have direct and dangerous consequences for patient safety. Through this practical case study , you will use dimensional analysis to trace how a simple data mapping mistake in a medication's concentration can lead to a critical dosing error, highlighting the importance of data integrity.",
            "id": "4372610",
            "problem": "A regional Health Information Exchange (HIE) mediates medication orders between two Electronic Health Record (EHR) systems. One system records medications using RxNorm Semantic Clinical Drug (SCD) concepts whose canonical strengths may include a dose per standard volume (for example, milligrams per $5$ milliliters), while the other system’s local formulary requires normalization to milligrams per milliliter. Consider the RxNorm SCD for an oral suspension that is expressed as $250 \\ \\mathrm{mg}/5 \\ \\mathrm{mL}$.\n\nA pediatric clinician intends to order a single dose based on body mass using the rule $d = \\alpha \\times w$, where $d$ is the dose in $\\mathrm{mg}$, $\\alpha$ is the dosing coefficient in $\\mathrm{mg}/\\mathrm{kg}$, and $w$ is the patient weight in $\\mathrm{kg}$. The patient has weight $w = 18.4 \\ \\mathrm{kg}$, and the dosing coefficient is $\\alpha = 12.5 \\ \\mathrm{mg}/\\mathrm{kg}$. The correct concentration implied by the RxNorm SCD is $c = 250 \\ \\mathrm{mg}/5 \\ \\mathrm{mL}$, which is equivalent to $50 \\ \\mathrm{mg}/\\mathrm{mL}$.\n\nDue to a unit normalization error in the HIE mapping, the receiving system’s formulary incorrectly interprets the SCD strength as $\\tilde{c} = 250 \\ \\mathrm{mg}/\\mathrm{mL}$, omitting the division by $5$. The EHR uses this incorrect concentration $\\tilde{c}$ to compute the administration volume $v$ by dividing the intended dose $d$ by $\\tilde{c}$, that is $v = d / \\tilde{c}$. The nurse then dispenses exactly volume $v$ from the actual product (which has the true concentration $c$), so the actual delivered dose is $d_{\\mathrm{delivered}} = c \\times v$.\n\nUsing first principles of dimensional analysis and dose computation, determine the absolute dose error in milligrams, defined as $|d_{\\mathrm{delivered}} - d|$, caused by this unit normalization error. Express your final answer in milligrams (mg). Do not round; provide the exact value.",
            "solution": "The problem statement will first be validated according to the specified criteria.\n\n### Step 1: Extract Givens\n-   System 1 (Source): RxNorm Semantic Clinical Drug (SCD) concepts with canonical strengths, e.g., $250 \\ \\mathrm{mg}/5 \\ \\mathrm{mL}$.\n-   System 2 (Receiving): Local formulary requires normalization to milligrams per milliliter ($\\mathrm{mg}/\\mathrm{mL}$).\n-   Specific Drug Concentration (RxNorm SCD): $250 \\ \\mathrm{mg}/5 \\ \\mathrm{mL}$.\n-   Dosing Rule: $d = \\alpha \\times w$.\n-   $d$: dose in $\\mathrm{mg}$.\n-   $\\alpha$: dosing coefficient.\n-   $w$: patient weight in $\\mathrm{kg}$.\n-   Patient Weight: $w = 18.4 \\ \\mathrm{kg}$.\n-   Dosing Coefficient: $\\alpha = 12.5 \\ \\mathrm{mg}/\\mathrm{kg}$.\n-   Correct Concentration: $c = 250 \\ \\mathrm{mg}/5 \\ \\mathrm{mL}$, which is equivalent to $50 \\ \\mathrm{mg}/\\mathrm{mL}$.\n-   Incorrect Concentration (due to HIE error): $\\tilde{c} = 250 \\ \\mathrm{mg}/\\mathrm{mL}$.\n-   Volume Calculation by EHR: $v = d / \\tilde{c}$.\n-   Actual Delivered Dose Calculation: $d_{\\mathrm{delivered}} = c \\times v$.\n-   Objective: Determine the absolute dose error, defined as $|d_{\\mathrm{delivered}} - d|$, in $\\mathrm{mg}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is based on fundamental principles of pharmacology and dimensional analysis (dose, concentration, volume calculations). The scenario described—a unit normalization error in a Health Information Exchange (HIE) leading to a medication error—is a well-documented and realistic problem in health informatics and patient safety.\n2.  **Well-Posed**: The problem is clearly structured. All necessary variables ($w$, $\\alpha$, $c$, $\\tilde{c}$) and relationships between them are explicitly defined. The objective is to compute a single, uniquely determinable value.\n3.  **Objective**: The problem is stated using precise, quantitative language. It is free from subjective claims or ambiguity.\n4.  **Completeness**: The problem is self-contained and provides all the data required to calculate the final answer. There are no missing definitions or conditions.\n5.  **Consistency**: The data and constraints are internally consistent. There are no contradictions.\n6.  **Topical Relevance**: The problem explicitly relates to `health information infrastructure and exchange (HIE)` and errors that can occur, which is a core topic within `health systems science`.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, objective, and complete. The solution process may proceed.\n\nThe objective is to calculate the absolute dose error, $|d_{\\mathrm{delivered}} - d|$. This requires a three-step calculation: first, determine the intended dose $d$; second, determine the actual dose delivered $d_{\\mathrm{delivered}}$; and third, compute the absolute difference.\n\n**Step 1: Calculate the intended dose ($d$)**\nThe intended dose $d$ is prescribed based on the patient's weight $w$ and a dosing coefficient $\\alpha$. The relationship is given as:\n$$d = \\alpha \\times w$$\nThe given values are $\\alpha = 12.5 \\ \\mathrm{mg}/\\mathrm{kg}$ and $w = 18.4 \\ \\mathrm{kg}$. Substituting these values:\n$$d = (12.5 \\ \\mathrm{mg}/\\mathrm{kg}) \\times (18.4 \\ \\mathrm{kg})$$\nThe kilograms ($\\mathrm{kg}$) units cancel, yielding a result in milligrams ($\\mathrm{mg}$).\n$$d = 12.5 \\times 18.4 \\ \\mathrm{mg}$$\n$$d = 230 \\ \\mathrm{mg}$$\nSo, the intended dose for the patient is $230 \\ \\mathrm{mg}$.\n\n**Step 2: Calculate the actual delivered dose ($d_{\\mathrm{delivered}}$)**\nThe actual delivered dose depends on the volume $v$ administered by the nurse and the true concentration $c$ of the medication. The relationship is given as:\n$$d_{\\mathrm{delivered}} = c \\times v$$\nTo find $d_{\\mathrm{delivered}}$, we must first find the volume $v$. The problem states that the volume $v$ is incorrectly calculated by the receiving EHR system. The EHR uses the intended dose $d$ and the incorrect concentration $\\tilde{c}$ for this calculation:\n$$v = \\frac{d}{\\tilde{c}}$$\nWe use the intended dose $d = 230 \\ \\mathrm{mg}$ from Step 1 and the incorrect concentration $\\tilde{c} = 250 \\ \\mathrm{mg}/\\mathrm{mL}$ given in the problem.\n$$v = \\frac{230 \\ \\mathrm{mg}}{250 \\ \\mathrm{mg}/\\mathrm{mL}} = \\frac{230}{250} \\ \\mathrm{mL} = \\frac{23}{25} \\ \\mathrm{mL}$$\n$$v = 0.92 \\ \\mathrm{mL}$$\nThe nurse administers this volume, $v = 0.92 \\ \\mathrm{mL}$, using the medication with the correct concentration, $c$. The correct concentration is given as $c = 50 \\ \\mathrm{mg}/\\mathrm{mL}$.\nNow we can calculate the dose that was actually delivered:\n$$d_{\\mathrm{delivered}} = c \\times v = (50 \\ \\mathrm{mg}/\\mathrm{mL}) \\times (0.92 \\ \\mathrm{mL})$$\n$$d_{\\mathrm{delivered}} = 50 \\times 0.92 \\ \\mathrm{mg} = 46 \\ \\mathrm{mg}$$\nSo, the actual dose delivered to the patient was $46 \\ \\mathrm{mg}$.\n\n**Step 3: Calculate the absolute dose error**\nThe absolute dose error is defined as the absolute value of the difference between the delivered dose and the intended dose:\n$$\\text{Error} = |d_{\\mathrm{delivered}} - d|$$\nSubstituting the values calculated in the previous steps:\n$$\\text{Error} = |46 \\ \\mathrm{mg} - 230 \\ \\mathrm{mg}|$$\n$$\\text{Error} = |-184 \\ \\mathrm{mg}|$$\n$$\\text{Error} = 184 \\ \\mathrm{mg}$$\nThe absolute dose error caused by the unit normalization error is $184 \\ \\mathrm{mg}$.",
            "answer": "$$\\boxed{184}$$"
        },
        {
            "introduction": "A foundational challenge for any Health Information Exchange (HIE) is accurately identifying which records from different sources belong to the same patient. Since unique identifiers are not universally shared, HIEs rely on probabilistic matching algorithms to link records based on demographic data. This exercise  delves into the core logic of this process, allowing you to calculate a match score and set decision thresholds to balance the risks of incorrect matches versus missed links.",
            "id": "4372619",
            "problem": "A regional Health Information Exchange (HIE) deploys probabilistic record linkage based on the Fellegi–Sunter (FS) framework. For each comparison field, the FS model uses two conditional probabilities: the $m$-probability (probability of field agreement given that two records come from the same person) and the $u$-probability (probability of field agreement given that two records come from different people). Assume conditional independence of fields given the true match status. The linkage team defines the composite match score as the natural logarithm of the likelihood ratio constructed from these fieldwise comparisons.\n\nThe four comparison fields and their parameters are:\n- First name: $m_{\\mathrm{FN}} = 0.92$, $u_{\\mathrm{FN}} = 0.02$.\n- Last name: $m_{\\mathrm{LN}} = 0.96$, $u_{\\mathrm{LN}} = 0.01$.\n- Date of birth: $m_{\\mathrm{DOB}} = 0.97$, $u_{\\mathrm{DOB}} = 0.002$.\n- ZIP code: $m_{\\mathrm{ZIP}} = 0.85$, $u_{\\mathrm{ZIP}} = 0.04$.\n\nFor a particular record pair, the observed comparison outcomes are: first name agrees, last name agrees, date of birth agrees, ZIP code disagrees.\n\nThe HIE uses two decision thresholds on the composite score, $t_{\\ell}$ and $t_{u}$, with the rule: classify as an automatic match if the score is at least $t_{u}$, classify as an automatic non-match if the score is at most $t_{\\ell}$, and send to clerical review otherwise. The design targets are:\n- Automatic-match sensitivity target: the probability, under the true-match process, that the score is at least $t_{u}$ must be at least $0.85$.\n- Automatic-non-match specificity target: the probability, under the true-non-match process, that the score is at most $t_{\\ell}$ must be at least $0.96$.\n\nStarting only from the definitions of conditional probability, likelihood ratio, and conditional independence of fields given match status, derive the composite log-likelihood ratio for the observed pair. Then, using the induced discrete score distributions under the true-match and true-non-match processes, choose $t_{u}$ to be the largest value such that the automatic-match sensitivity constraint is satisfied, and choose $t_{\\ell}$ to be the smallest value such that the automatic-non-match specificity constraint is satisfied.\n\nReport your final results as a row matrix containing, in order, the composite score for the observed pair, $t_{\\ell}$, and $t_{u}$. Round all three numbers to four significant figures. No units are required, but the composite score is in natural logarithm units (nats).",
            "solution": "### Step 1: Extract Givens\nThe problem provides the following information for a probabilistic record linkage system based on the Fellegi-Sunter framework:\n- Four comparison fields: First name (FN), Last name (LN), Date of birth (DOB), ZIP code (ZIP).\n- For each field $i$, the conditional probabilities of agreement are given:\n  - $m$-probability, $P(\\text{agreement in field } i | \\text{true match}) = m_i$.\n  - $u$-probability, $P(\\text{agreement in field } i | \\text{true non-match}) = u_i$.\n- Specific parameter values:\n  - First name: $m_{\\mathrm{FN}} = 0.92$, $u_{\\mathrm{FN}} = 0.02$.\n  - Last name: $m_{\\mathrm{LN}} = 0.96$, $u_{\\mathrm{LN}} = 0.01$.\n  - Date of birth: $m_{\\mathrm{DOB}} = 0.97$, $u_{\\mathrm{DOB}} = 0.002$.\n  - ZIP code: $m_{\\mathrm{ZIP}} = 0.85$, $u_{\\mathrm{ZIP}} = 0.04$.\n- Assumption: Conditional independence of fields given the true match status.\n- Definition: The composite match score is the natural logarithm of the likelihood ratio.\n- Observation: For a specific record pair, the outcomes are: first name agrees, last name agrees, date of birth agrees, ZIP code disagrees.\n- Decision rule thresholds, $t_{\\ell}$ and $t_{u}$:\n  - Score $\\ge t_{u} \\implies$ automatic match.\n  - Score $\\le t_{\\ell} \\implies$ automatic non-match.\n  - $t_{\\ell} <$ Score $< t_{u} \\implies$ clerical review.\n- Design targets for threshold selection:\n  - Automatic-match sensitivity: $P(\\text{Score} \\ge t_{u} | \\text{true match}) \\ge 0.85$. $t_u$ should be the largest value satisfying this.\n  - Automatic-non-match specificity: $P(\\text{Score} \\le t_{\\ell} | \\text{true non-match}) \\ge 0.96$. $t_\\ell$ should be the smallest value satisfying this.\n- Required output: A row matrix containing the composite score for the observed pair, $t_{\\ell}$, and $t_{u}$, with all values rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, rooted in the established statistical theory of the Fellegi-Sunter model for probabilistic record linkage. All terms are defined, and the necessary data ($m$ and $u$ probabilities) are provided. The assumptions (conditional independence) are explicitly stated. The objectives (calculating a specific score and two thresholds based on performance constraints) are clear and well-posed. The problem is self-contained, internally consistent, and requires the application of standard principles of probability and statistics. No scientific laws are violated, and no subjective or ambiguous language is used.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution follows.\n\nThe composite match score is the sum of the individual log-likelihood ratios (or \"weights\") for each field, due to the assumption of conditional independence. For a given field $i$, there are two possible outcomes: agreement or disagreement.\n\nThe weight for agreement in field $i$ is:\n$$w_i^A = \\ln\\left(\\frac{P(\\text{agreement in } i | \\text{match})}{P(\\text{agreement in } i | \\text{non-match})}\\right) = \\ln\\left(\\frac{m_i}{u_i}\\right)$$\nThe weight for disagreement in field $i$ is:\n$$w_i^D = \\ln\\left(\\frac{P(\\text{disagreement in } i | \\text{match})}{P(\\text{disagreement in } i | \\text{non-match})}\\right) = \\ln\\left(\\frac{1 - m_i}{1 - u_i}\\right)$$\n\nLet's calculate these weights for the four fields:\n- First Name (FN):\n  - $w_{\\mathrm{FN}}^A = \\ln\\left(\\frac{0.92}{0.02}\\right) = \\ln(46) \\approx 3.82864$\n  - $w_{\\mathrm{FN}}^D = \\ln\\left(\\frac{1 - 0.92}{1 - 0.02}\\right) = \\ln\\left(\\frac{0.08}{0.98}\\right) \\approx -2.50553$\n- Last Name (LN):\n  - $w_{\\mathrm{LN}}^A = \\ln\\left(\\frac{0.96}{0.01}\\right) = \\ln(96) \\approx 4.56435$\n  - $w_{\\mathrm{LN}}^D = \\ln\\left(\\frac{1 - 0.96}{1 - 0.01}\\right) = \\ln\\left(\\frac{0.04}{0.99}\\right) \\approx -3.20883$\n- Date of Birth (DOB):\n  - $w_{\\mathrm{DOB}}^A = \\ln\\left(\\frac{0.97}{0.002}\\right) = \\ln(485) \\approx 6.18415$\n  - $w_{\\mathrm{DOB}}^D = \\ln\\left(\\frac{1 - 0.97}{1 - 0.002}\\right) = \\ln\\left(\\frac{0.03}{0.998}\\right) \\approx -3.50497$\n- ZIP code (ZIP):\n  - $w_{\\mathrm{ZIP}}^A = \\ln\\left(\\frac{0.85}{0.04}\\right) = \\ln(21.25) \\approx 3.05636$\n  - $w_{\\mathrm{ZIP}}^D = \\ln\\left(\\frac{1 - 0.85}{1 - 0.04}\\right) = \\ln\\left(\\frac{0.15}{0.96}\\right) \\approx -1.85630$\n\n**1. Composite Score for the Observed Pair**\n\nThe observed outcome is agreement on FN, LN, DOB, and disagreement on ZIP. The composite score, $S_{\\text{obs}}$, is the sum of the corresponding weights:\n$$S_{\\text{obs}} = w_{\\mathrm{FN}}^A + w_{\\mathrm{LN}}^A + w_{\\mathrm{DOB}}^A + w_{\\mathrm{ZIP}}^D$$\n$$S_{\\text{obs}} \\approx 3.82864 + 4.56435 + 6.18415 - 1.85630 = 12.72084$$\nRounding to four significant figures, $S_{\\text{obs}} \\approx 12.72$.\n\n**2. Determination of Thresholds $t_u$ and $t_\\ell$**\n\nThis requires analyzing the discrete probability distributions of the composite score under the true-match (M) and true-non-match (U) hypotheses. There are $2^4 = 16$ possible comparison patterns, each with a unique score (in this case) and a specific probability of occurrence under each hypothesis.\n\n**To find $t_u$ (Automatic-match threshold):**\nThe requirement is $P(S \\ge t_u | M) \\ge 0.85$. We seek the largest $t_u$ that satisfies this. We must identify the set of scores that should be classified as matches to meet the sensitivity target. We list the possible scores in descending order and compute their cumulative probability under the true-match process, $P(S | M)$.\n\nThe probability of a specific comparison pattern $\\gamma$ (e.g., AADA) under the match process is the product of the individual field outcome probabilities, e.g., $P(\\text{AADA}|M) = m_{\\mathrm{FN}} \\cdot m_{\\mathrm{LN}} \\cdot (1-m_{\\mathrm{DOB}}) \\cdot m_{\\mathrm{ZIP}}$.\n\nLet's find the highest scores and their probabilities under M:\n- Pattern AAAA (all agree): $S = 17.6335$. $P(\\text{AAAA}|M) = 0.92 \\cdot 0.96 \\cdot 0.97 \\cdot 0.85 = 0.72935$.\nThe cumulative probability is $0.72935$, which is less than the target of $0.85$. We must include more scores.\n- The next highest score corresponds to the pattern with the smallest disagreement weight, which is from ZIP ($w_{\\mathrm{ZIP}}^D \\approx -1.85630$). This is the AAAD pattern.\n- Pattern AAAD (FN,LN,DOB agree; ZIP disagrees): $S = 12.7208$. $P(\\text{AAAD}|M) = 0.92 \\cdot 0.96 \\cdot 0.97 \\cdot (1 - 0.85) = 0.12871$.\nThe cumulative probability of scores $\\ge 12.7208$ is $P(\\text{AAAA}|M) + P(\\text{AAAD}|M) = 0.72935 + 0.12871 = 0.85806$.\n\nSince $0.85806 \\ge 0.85$, the set of scores $\\{17.6335, 12.7208\\}$ satisfies the sensitivity criterion. To classify exactly these scores (and no others with lower values) as matches, the threshold $t_u$ must be set such that any score in this set is $\\ge t_u$ and any score not in this set is $< t_u$. The next highest score is for pattern DAAA, with $S \\approx 11.2994$. Thus, $t_u$ must be in the interval $(11.2994, 12.7208]$. The problem asks for the largest possible value for $t_u$, which is the upper bound of this interval.\n$$t_u = 12.72084$$\nRounding to four significant figures, $t_u \\approx 12.72$.\n\n**To find $t_\\ell$ (Automatic-non-match threshold):**\nThe requirement is $P(S \\le t_\\ell | U) \\ge 0.96$. We seek the smallest $t_\\ell$ that satisfies this. We list the scores in ascending order and compute their cumulative probability under the true-non-match process, $P(S | U)$.\n\nThe probability of a pattern $\\gamma$ under the non-match process is, e.g., $P(\\text{DADA}|U) = (1-u_{\\mathrm{FN}}) \\cdot u_{\\mathrm{LN}} \\cdot (1-u_{\\mathrm{DOB}}) \\cdot u_{\\mathrm{ZIP}}$.\n\nLet's find the lowest scores and their probabilities under U:\n- Pattern DDDD (all disagree): $S = -11.0756$. $P(\\text{DDDD}|U) = (1-0.02) \\cdot (1-0.01) \\cdot (1-0.002) \\cdot (1-0.04) = 0.98 \\cdot 0.99 \\cdot 0.998 \\cdot 0.96 = 0.92950$.\nThe cumulative probability is $0.92950$, which is less than the target of $0.96$. We must include more scores.\nWe proceed to the next lowest score, which corresponds to the pattern DDDA (FN,LN,DOB disagree; ZIP agrees).\n- Pattern DDDA (FN,LN,DOB disagree; ZIP agrees): $S \\approx -2.50553 - 3.20883 - 3.50497 + 3.05636 = -6.16297$. $P(\\text{DDDA}|U) = (1-0.02) \\cdot (1-0.01) \\cdot (1-0.002) \\cdot 0.04 = 0.98 \\cdot 0.99 \\cdot 0.998 \\cdot 0.04 = 0.03873$.\nThe cumulative probability of scores $\\le -6.16297$ is $P(\\text{DDDD}|U) + P(\\text{DDDA}|U) = 0.92950 + 0.03873 = 0.96823$.\n\nSince $0.96823 \\ge 0.96$, the set of scores $\\{-11.0756, -6.16297\\}$ satisfies the specificity criterion. To classify exactly these scores as non-matches, the threshold $t_\\ell$ must be set such that any score in this set is $\\le t_\\ell$. The next lowest score is for pattern ADDD, with $S \\approx -4.7415$. Thus, $t_\\ell$ must be in the interval $[-6.16297, -4.7415)$. The problem asks for the smallest possible value for $t_\\ell$, which is the lower bound of this interval.\n$$t_\\ell = -6.16297$$\nRounding to four significant figures, $t_\\ell \\approx -6.163$.\n\n**Summary of Results**\n- Composite score for the observed pair (AAAD): $S_{\\text{obs}} \\approx 12.72084$. Rounded: $12.72$.\n- Lower threshold: $t_\\ell \\approx -6.16297$. Rounded: $-6.163$.\n- Upper threshold: $t_u \\approx 12.72084$. Rounded: $12.72$.\n\nThe final answer is a row matrix containing these three values in the specified order.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n12.72 & -6.163 & 12.72\n\\end{pmatrix}\n}\n$$"
        }
    ]
}