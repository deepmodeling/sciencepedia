{
    "hands_on_practices": [
        {
            "introduction": "A screening tool's utility is not defined solely by its intrinsic accuracy, measured by sensitivity and specificity. This exercise demonstrates how to translate these abstract metrics into concrete, practical insights, such as the probability that a positive screen is correct (Positive Predictive Value) and the expected referral volume a clinic must handle. Mastering this calculation is a fundamental skill for evaluating the real-world impact and feasibility of any screening program .",
            "id": "4396192",
            "problem": "A community health center integrates a standardized social risk screening for food insecurity into routine clinical care using a brief, validated tool administered through the Electronic Health Record (EHR). The tool’s performance in the clinic population is characterized by sensitivity $Se = 0.80$, specificity $Sp = 0.90$, and the condition’s prevalence $\\pi = 0.20$ estimated from a recent community health needs assessment. The clinic policy is to automatically generate a referral to food assistance resources for every positive screen, and the clinic sees $500$ unique adult patients per week.\n\nStarting from the definitions of sensitivity $Se = P(T^{+} \\mid D)$ and specificity $Sp = P(T^{-} \\mid \\bar{D})$, where $D$ denotes the presence of food insecurity and $T^{+}$ denotes a positive test (screen), use Bayes’ theorem and the law of total probability to derive expressions for the Positive Predictive Value (PPV) $P(D \\mid T^{+})$ and the Negative Predictive Value (NPV) $P(\\bar{D} \\mid T^{-})$ in terms of $Se$, $Sp$, and $\\pi$. Then, compute the numerical values of $PPV$ and $NPV$ for the given $Se$, $Sp$, and $\\pi$. Express $PPV$ and $NPV$ as decimals and round each to four significant figures.\n\nFinally, compute the expected weekly referral load, defined as the expected number of patients per week who will receive a referral under the “refer all positive screens” policy. Report this referral load as a count of patients per week. Provide your final answers in the order $PPV$, $NPV$, expected referrals.",
            "solution": "The problem statement is evaluated for validity prior to attempting a solution.\n\n### Step 1: Extract Givens\n- Sensitivity: $Se = P(T^{+} \\mid D) = 0.80$\n- Specificity: $Sp = P(T^{-} \\mid \\bar{D}) = 0.90$\n- Prevalence of food insecurity: $\\pi = P(D) = 0.20$\n- $D$: The event that a patient has food insecurity.\n- $\\bar{D}$: The event that a patient does not have food insecurity.\n- $T^{+}$: The event of a positive test result.\n- $T^{-}$: The event of a negative test result.\n- Total number of unique adult patients per week: $N = 500$\n- Policy: A referral is generated for every positive screen ($T^{+}$).\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is based on fundamental principles of probability theory (Bayes' theorem, law of total probability) and standard epidemiological metrics (sensitivity, specificity, prevalence, predictive values). The problem is self-contained, with all necessary data and definitions provided. The numerical values ($Se=0.80$, $Sp=0.90$, $\\pi=0.20$) are realistic for a public health screening scenario. The problem asks for specific, formalizable derivations and calculations, and poses no logical contradictions or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Derivation of Predictive Values\n\nThe Positive Predictive Value ($PPV$) is the probability that a patient with a positive test result truly has the condition, defined as $P(D \\mid T^{+})$. Using Bayes' theorem:\n$$PPV = P(D \\mid T^{+}) = \\frac{P(T^{+} \\mid D) P(D)}{P(T^{+})}$$\nThe numerator contains the given terms: $P(T^{+} \\mid D) = Se$ and $P(D) = \\pi$.\nThe denominator, $P(T^{+})$, is the overall probability of a positive test. We expand this using the law of total probability:\n$$P(T^{+}) = P(T^{+} \\mid D) P(D) + P(T^{+} \\mid \\bar{D}) P(\\bar{D})$$\nWe can identify the components of this expansion:\n- $P(D) = \\pi$, so $P(\\bar{D}) = 1 - P(D) = 1 - \\pi$.\n- $P(T^{+} \\mid D) = Se$.\n- $P(T^{+} \\mid \\bar{D})$ is the false positive rate. It is the complement of the specificity, $Sp = P(T^{-} \\mid \\bar{D})$. Thus, $P(T^{+} \\mid \\bar{D}) = 1 - P(T^{-} \\mid \\bar{D}) = 1 - Sp$.\n\nSubstituting these into the expression for $P(T^{+})$:\n$$P(T^{+}) = (Se \\cdot \\pi) + ((1 - Sp) \\cdot (1 - \\pi))$$\nFinally, substituting this back into the Bayes' theorem expression for $PPV$ gives the general formula:\n$$PPV = \\frac{Se \\cdot \\pi}{Se \\cdot \\pi + (1 - Sp)(1 - \\pi)}$$\n\nThe Negative Predictive Value ($NPV$) is the probability that a patient with a negative test result is truly free of the condition, defined as $P(\\bar{D} \\mid T^{-})$. Using Bayes' theorem:\n$$NPV = P(\\bar{D} \\mid T^{-}) = \\frac{P(T^{-} \\mid \\bar{D}) P(\\bar{D})}{P(T^{-})}$$\nThe numerator contains the given terms: $P(T^{-} \\mid \\bar{D}) = Sp$ and $P(\\bar{D}) = 1 - \\pi$.\nThe denominator, $P(T^{-})$, is the overall probability of a negative test. We expand this using the law of total probability:\n$$P(T^{-}) = P(T^{-} \\mid D) P(D) + P(T^{-} \\mid \\bar{D}) P(\\bar{D})$$\nWe can identify the components of this expansion:\n- $P(D) = \\pi$ and $P(\\bar{D}) = 1 - \\pi$.\n- $P(T^{-} \\mid \\bar{D}) = Sp$.\n- $P(T^{-} \\mid D)$ is the false negative rate. It is the complement of the sensitivity, $Se = P(T^{+} \\mid D)$. Thus, $P(T^{-} \\mid D) = 1 - P(T^{+} \\mid D) = 1 - Se$.\n\nSubstituting these into the expression for $P(T^{-})$:\n$$P(T^{-}) = ((1 - Se) \\cdot \\pi) + (Sp \\cdot (1 - \\pi))$$\nFinally, substituting this back into the Bayes' theorem expression for $NPV$ gives the general formula:\n$$NPV = \\frac{Sp(1 - \\pi)}{(1 - Se)\\pi + Sp(1 - \\pi)}$$\n\n### Numerical Computation of PPV and NPV\nUsing the given values $Se = 0.80$, $Sp = 0.90$, and $\\pi = 0.20$:\n\nFor $PPV$:\n$$PPV = \\frac{0.80 \\cdot 0.20}{(0.80 \\cdot 0.20) + (1 - 0.90)(1 - 0.20)} = \\frac{0.16}{0.16 + (0.10)(0.80)} = \\frac{0.16}{0.16 + 0.08} = \\frac{0.16}{0.24} = \\frac{2}{3}$$\nAs a decimal rounded to four significant figures, $PPV \\approx 0.6667$.\n\nFor $NPV$:\n$$NPV = \\frac{0.90(1 - 0.20)}{(1 - 0.80)(0.20) + 0.90(1 - 0.20)} = \\frac{0.90 \\cdot 0.80}{(0.20)(0.20) + 0.90(0.80)} = \\frac{0.72}{0.04 + 0.72} = \\frac{0.72}{0.76} = \\frac{18}{19}$$\nAs a decimal rounded to four significant figures, $NPV \\approx 0.9474$.\n\n### Computation of Expected Weekly Referral Load\nThe clinic policy is to refer all patients who screen positive. The expected weekly referral load is the total number of patients per week, $N$, multiplied by the probability of a positive test, $P(T^{+})$.\nExpected Referrals $= N \\cdot P(T^{+})$\nWe have already derived and calculated $P(T^{+})$:\n$$P(T^{+}) = (Se \\cdot \\pi) + ((1 - Sp) \\cdot (1 - \\pi)) = 0.24$$\nWith $N = 500$ patients per week:\nExpected Referrals $= 500 \\cdot 0.24 = 120$\nThe expected weekly referral load is $120$ patients.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.6667 & 0.9474 & 120\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "As health systems increasingly rely on algorithms to identify patients with social risks, it is imperative to understand and mitigate potential biases. This practice challenges you to dissect realistic clinical scenarios and identify the specific sources of algorithmic bias—label bias, representation bias, and deployment bias. Developing this diagnostic skill is essential for ensuring that predictive models promote health equity rather than amplifying existing disparities .",
            "id": "4396139",
            "problem": "A health system plans to integrate a social risk screening model into routine clinical care to identify patients at risk of food insecurity and unstable housing. The model takes routinely collected data (demographics, prior visits, language preference, and previous social needs assessments) as input and outputs a predicted probability $\\hat{p}$ of social risk. Leadership asks the data science team to explain what counts as algorithmic bias and to classify the source of bias in several realistic situations.\n\nConsider the following situations in clinical implementation of social risk prediction:\n\nScenario One: To supervise the model during development, the team uses International Classification of Diseases codes and structured fields in the Electronic Health Record (EHR) as the target for the presence of social needs, even though many social needs are only documented in free-text notes or not recorded at all. Under-documentation is known to be more common for patients with limited access to care and for those speaking non-English languages.\n\nScenario Two: The training set comes entirely from one large urban academic medical center. In the region where the model will be used, half of patients receive care in community clinics that serve a higher proportion of uninsured and non-English-speaking patients than the academic center. The training cohort underrepresents non-English speakers and uninsured individuals relative to the deployment population.\n\nScenario Three: The model was tuned in the inpatient setting to maximize sensitivity at a fixed threshold and then deployed with the same decision threshold in primary care clinics. The prevalence of documented social needs is lower in primary care than inpatient care, and care teams in primary care have different workflows and resource constraints.\n\nWhich option both correctly defines algorithmic bias in social risk prediction and correctly classifies Scenario One, Scenario Two, and Scenario Three as label bias, representation bias, and deployment bias, respectively?\n\nA. Algorithmic bias is any random error in prediction performance. Scenario One is representation bias, Scenario Two is label bias, and Scenario Three is deployment bias.\n\nB. Algorithmic bias is a systematic pattern of error that yields inequitable model performance or decisions across groups. Scenario One is label bias, Scenario Two is representation bias, and Scenario Three is deployment bias.\n\nC. Algorithmic bias is intentional discrimination by developers baked into the code. Scenario One is deployment bias, Scenario Two is representation bias, and Scenario Three is label bias.\n\nD. Algorithmic bias is solely differences in model calibration across groups. Scenario One is label bias, Scenario Two is selection bias, and Scenario Three is deployment bias.\n\nE. Algorithmic bias is eliminated by retraining on more data. Scenario One is measurement bias, Scenario Two is representation bias, and Scenario Three is data leakage.",
            "solution": "The problem requires a critical evaluation of several scenarios involving a social risk prediction model in a clinical setting. The task is to identify the option that correctly defines algorithmic bias and correctly classifies the source of bias in each of the three given scenarios.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Context:** A health system is implementing a social risk screening model.\n-   **Model Input:** Routinely collected data (demographics, prior visits, language preference, previous social needs assessments).\n-   **Model Output:** A predicted probability, denoted as $\\hat{p}$, of social risk (e.g., food insecurity, unstable housing).\n-   **Scenario One:** The model's target variable (ground truth for social need) is defined using structured EHR data (International Classification of Diseases codes, structured fields). However, many social needs are documented only in free-text notes or not at all. This under-documentation is explicitly stated to be more common for patients with limited access to care and for those speaking non-English languages.\n-   **Scenario Two:** The model is trained on data from a single large urban academic medical center. The intended deployment population includes community clinics, which serve a higher proportion of uninsured and non-English-speaking patients. The training data underrepresents these specific groups compared to the deployment population.\n-   **Scenario Three:** The model is tuned in an inpatient setting to maximize sensitivity at a fixed threshold. It is then deployed with the same threshold in primary care clinics. The prevalence of documented social needs is lower in the primary care setting, and the clinical workflows and available resources are different.\n-   **Question:** Identify the option that provides the correct definition of algorithmic bias and correctly classifies Scenario One as label bias, Scenario Two as representation bias, and Scenario Three as deployment bias.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is evaluated against the validation criteria.\n-   **Scientifically Grounded:** The problem is firmly rooted in the established field of data science, specifically addressing fairness and ethics in machine learning. The concepts of algorithmic bias and its subtypes (label, representation, deployment) are standard terminology in the field. The clinical application is realistic and a significant area of current research and implementation. The problem is scientifically sound.\n-   **Well-Posed:** The problem provides three distinct, well-described scenarios and asks for their classification based on a provided definition. The structure is clear and leads to a determinable answer by applying standard definitions from the field.\n-   **Objective:** The scenarios are described using precise, objective language. The descriptions of patient populations and data sources are factual statements within the problem's context, free of subjective or opinion-based claims.\n\n**Flaw Checklist:**\n1.  **Scientific or Factual Unsoundness:** None. The concepts are standard.\n2.  **Non-Formalizable or Irrelevant:** None. The scenarios are concrete examples of well-defined problems in applied machine learning.\n3.  **Incomplete or Contradictory Setup:** None. Each scenario is self-contained and provides sufficient information for classification.\n4.  **Unrealistic or Infeasible:** None. These are highly realistic challenges in deploying clinical prediction models.\n5.  **Ill-Posed or Poorly Structured:** None.\n6.  **Pseudo-Profound, Trivial, or Tautological:** None. The problem requires applying specific, non-trivial knowledge.\n7.  **Outside Scientific Verifiability:** None. The classifications can be verified against the formal definitions in the machine learning literature.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will now proceed with a full solution.\n\n### Derivation and Option Analysis\n\nFirst, let us establish the formal definitions of the key concepts from the field of machine learning fairness.\n\n-   **Algorithmic Bias:** This refers to systematic and repeatable errors in a computer system that create unfair outcomes, such as privileging one arbitrary group of users over others. It is not a random error but a persistent pattern of skewed results that can lead to inequitable or discriminatory decisions.\n-   **Label Bias (or Measurement Bias):** This type of bias occurs when the data used to label the ground truth is a flawed or biased proxy for the actual outcome of interest. If the quality or accuracy of the labels systematically differs across groups, the model will learn these systematic inaccuracies. In Scenario One, the \"label\" for social need is based on structured EHR fields. The problem states that this method of labeling systematically under-documents social needs for non-English speakers and those with limited care access. Therefore, the label itself is biased against these groups. This is a clear case of **label bias**.\n-   **Representation Bias (or Sampling Bias):** This bias arises when the training dataset does not accurately reflect the target population where the model will be deployed. If certain subgroups are under- or over-represented in the training data, the model may not generalize well to these subgroups, leading to poorer performance for them. In Scenario Two, the training data comes from an academic medical center, while the deployment population also includes community clinics with a different patient demographic (more uninsured, more non-English speaking). The training data \"underrepresents non-English speakers and uninsured individuals relative to the deployment population.\" This is the definition of **representation bias**.\n-   **Deployment Bias (or Implementation Bias):** This bias occurs when there is a mismatch between the context in which the model was developed and the context in which it is deployed. This can involve changes in population characteristics (population drift), differences in how the model's predictions are used, or shifts in the underlying data generating process. In Scenario Three, the model is developed and tuned for an inpatient setting (high prevalence of social need) but deployed in a primary care setting (low prevalence). The prevalence of the outcome is a critical parameter. For a given sensitivity and specificity, a change in prevalence dramatically alters the Positive Predictive Value ($$PPV = \\frac{\\text{sensitivity} \\times \\text{prevalence}}{\\text{sensitivity} \\times \\text{prevalence} + (1 - \\text{specificity}) \\times (1 - \\text{prevalence})}$$). Using a threshold optimized for a high-prevalence setting in a low-prevalence setting will likely lead to a very low PPV and a high number of false positives, overburdening the different workflows and resource constraints of the primary care teams. This mismatch between the development and deployment context is a textbook example of **deployment bias**.\n\nBased on this analysis, the correct classifications are:\n-   Scenario One: Label Bias\n-   Scenario Two: Representation Bias\n-   Scenario Three: Deployment Bias\n\nNow, we evaluate each option.\n\n**A. Algorithmic bias is any random error in prediction performance. Scenario One is representation bias, Scenario Two is label bias, and Scenario Three is deployment bias.**\n-   **Definition of Bias:** The statement that algorithmic bias is \"any random error\" is fundamentally **Incorrect**. Bias is systematic, not random. Random, irreducible error is a component of nearly all statistical models and is distinct from bias.\n-   **Classification:** The classification swaps the labels for Scenario One and Scenario Two. Scenario One is label bias, not representation bias. Scenario Two is representation bias, not label bias. This is **Incorrect**.\n-   **Verdict:** **Incorrect**.\n\n**B. Algorithmic bias is a systematic pattern of error that yields inequitable model performance or decisions across groups. Scenario One is label bias, Scenario Two is representation bias, and Scenario Three is deployment bias.**\n-   **Definition of Bias:** This definition is **Correct**. It accurately captures the essence of algorithmic bias as a systematic, not random, phenomenon that results in inequitable outcomes.\n-   **Classification:** The classifications for all three scenarios—label bias for One, representation bias for Two, and deployment bias for Three—are **Correct**, as determined by the principled analysis above.\n-   **Verdict:** **Correct**.\n\n**C. Algorithmic bias is intentional discrimination by developers baked into the code. Scenario One is deployment bias, Scenario Two is representation bias, and Scenario Three is label bias.**\n-   **Definition of Bias:** This definition is **Incorrect**. While intentional discrimination is a possible, and egregious, source of bias, the vast majority of algorithmic bias is unintentional, arising from flawed data, inappropriate model choices, or mismatches in deployment. Defining it as solely intentional is a severe mischaracterization.\n-   **Classification:** The classification misidentifies Scenario One as deployment bias and Scenario Three as label bias. This is **Incorrect**.\n-   **Verdict:** **Incorrect**.\n\n**D. Algorithmic bias is solely differences in model calibration across groups. Scenario One is label bias, Scenario Two is selection bias, and Scenario Three is deployment bias.**\n-   **Definition of Bias:** The definition that bias is \"solely differences in model calibration\" is **Incorrect**. Poor calibration is one important manifestation of bias, but not the only one. Bias can also appear as group-wise differences in accuracy, false positive rates, false negative rates, and other performance metrics, even in a well-calibrated model. The definition is too narrow.\n-   **Classification:** The classification is mostly correct. \"Selection bias\" is a term often used synonymously with representation bias. Scenario One and Three are classified correctly. However, the flawed definition of bias makes the entire option incorrect.\n-   **Verdict:** **Incorrect**.\n\n**E. Algorithmic bias is eliminated by retraining on more data. Scenario One is measurement bias, Scenario Two is representation bias, and Scenario Three is data leakage.**\n-   **Definition of Bias:** This is not a definition of bias but an overly simplistic and often incorrect statement about its mitigation. Simply adding \"more data\" of the same kind can amplify existing biases. While more *representative* data can address representation bias, it does not fix label bias or deployment bias. This statement is **Incorrect**.\n-   **Classification:** The classification for Scenario One as \"measurement bias\" (a synonym for label bias) and Scenario Two as \"representation bias\" is correct. However, the classification of Scenario Three as \"data leakage\" is **Incorrect**. Data leakage refers to the inappropriate use of information from outside the training data during model development, leading to inflated performance metrics. Scenario Three describes a mismatch between development and deployment environments, which is deployment bias, not data leakage.\n-   **Verdict:** **Incorrect**.\n\nOnly Option B provides both an accurate definition of algorithmic bias and the correct classification for all three scenarios.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Successfully integrating social risk screening into care requires more than just an accurate tool; it demands a system that can manage the resulting patient flow. This hands-on practice introduces queuing theory as a powerful method for analyzing and optimizing clinical workflows. By modeling the referral process, you will learn to predict operational metrics like patient wait times, providing a quantitative foundation for resource planning and system design .",
            "id": "4396144",
            "problem": "A primary care clinic has integrated standardized social risk screening into routine visits. Patients with identified social needs are referred to an on-site social worker who addresses needs such as food insecurity, housing instability, or transportation barriers. Assume that referrals arrive as a Poisson process with rate $\\lambda$ (per hour), each referral requires an exponentially distributed service time with rate $\\mu$ (per hour), there is a single social worker serving referrals on a first-come, first-served basis, and the system operates in steady state with $\\lambda < \\mu$.\n\nUsing only steady-state birth–death balance and Little’s Law, derive from first principles an expression for the expected total time in the system $W$ (that is, waiting plus service) for a referral, as a function of $\\lambda$ and $\\mu$.\n\nThen evaluate this expression for a clinic in which $\\lambda = 5.4$ per hour and $\\mu = 6.75$ per hour. Express the final time in minutes and round your answer to four significant figures.",
            "solution": "The problem describes a queuing system that can be modeled as an M/M/1 queue. The arrivals are Markovian (M) following a Poisson process with rate $\\lambda$, the service times are Markovian (M) following an exponential distribution with rate $\\mu$, and there is $1$ server. The system is assumed to be in steady state, which requires the traffic intensity $\\rho = \\lambda/\\mu$ to be less than $1$. The problem statement gives the condition $\\lambda < \\mu$, which ensures this.\n\nOur objective is to derive the expected total time a referral spends in the system, $W$, using two fundamental principles: the steady-state balance equations of a birth-death process and Little's Law.\n\nLet $n$ be the number of referrals in the system (either waiting in the queue or being served). The state of the system at any time $t$ is given by $n(t)$. In steady state, we are interested in the probability distribution $P_n$, which is the long-run probability of finding $n$ referrals in the system. The state space is the set of non-negative integers $\\{0, 1, 2, \\dots\\}$.\n\nFor a birth-death process, the transition rates are:\n- Birth rate (arrival of a new referral): $\\lambda_n = \\lambda$ for all $n \\ge 0$.\n- Death rate (completion of service): $\\mu_n = \\mu$ for all $n \\ge 1$, and $\\mu_0 = 0$.\n\nThe steady-state balance equations state that for each state $n$, the rate of entering the state must equal the rate of leaving it.\n\nFor state $n=0$:\nThe system enters state $0$ from state $1$ at a rate of $\\mu P_1$. It leaves state $0$ for state $1$ at a rate of $\\lambda P_0$.\n$$ \\mu P_1 = \\lambda P_0 $$\nThis gives $P_1 = \\frac{\\lambda}{\\mu} P_0$. Let us define the traffic intensity $\\rho = \\frac{\\lambda}{\\mu}$. Then, $P_1 = \\rho P_0$.\n\nFor any state $n \\ge 1$:\nThe system can enter state $n$ from state $n-1$ (an arrival) or from state $n+1$ (a service completion). The total rate of entering state $n$ is $\\lambda P_{n-1} + \\mu P_{n+1}$. The system leaves state $n$ for state $n-1$ (a service completion) or for state $n+1$ (an arrival). The total rate of leaving state $n$ is $(\\lambda + \\mu) P_n$.\nThe balance equation is:\n$$ \\lambda P_{n-1} + \\mu P_{n+1} = (\\lambda + \\mu) P_n $$\nWe can use these equations to find a general expression for $P_n$ in terms of $P_0$. We already have $P_1 = \\rho P_0$.\nFor $n=1$, the balance equation is $\\lambda P_0 + \\mu P_2 = (\\lambda + \\mu) P_1$.\nSubstituting $P_1 = \\frac{\\lambda}{\\mu} P_0$:\n$$ \\lambda P_0 + \\mu P_2 = (\\lambda + \\mu) \\frac{\\lambda}{\\mu} P_0 $$\n$$ \\mu P_2 = \\frac{\\lambda(\\lambda+\\mu)}{\\mu} P_0 - \\lambda P_0 = \\left( \\frac{\\lambda^2+\\lambda\\mu}{\\mu} - \\frac{\\lambda\\mu}{\\mu} \\right) P_0 = \\frac{\\lambda^2}{\\mu} P_0 $$\n$$ P_2 = \\frac{\\lambda^2}{\\mu^2} P_0 = \\rho^2 P_0 $$\nBy induction, one can show that the general solution is $P_n = \\rho^n P_0$ for all $n \\ge 0$.\n\nTo find $P_0$, we use the normalization condition that the sum of all probabilities must be $1$:\n$$ \\sum_{n=0}^{\\infty} P_n = 1 $$\n$$ \\sum_{n=0}^{\\infty} \\rho^n P_0 = P_0 \\sum_{n=0}^{\\infty} \\rho^n = 1 $$\nThe sum is a geometric series. Since we are given $\\lambda < \\mu$, it follows that $\\rho < 1$, and the series converges:\n$$ \\sum_{n=0}^{\\infty} \\rho^n = \\frac{1}{1-\\rho} $$\nSubstituting this into the normalization equation:\n$$ P_0 \\left( \\frac{1}{1-\\rho} \\right) = 1 \\implies P_0 = 1-\\rho $$\nThus, the steady-state probability of having $n$ referrals in the system is:\n$$ P_n = (1-\\rho)\\rho^n, \\quad n=0, 1, 2, \\dots $$\n\nNext, we calculate the expected number of referrals in the system, denoted by $L$. This is the expectation of the distribution $P_n$:\n$$ L = E[n] = \\sum_{n=0}^{\\infty} n P_n = \\sum_{n=0}^{\\infty} n (1-\\rho)\\rho^n $$\n$$ L = (1-\\rho) \\sum_{n=0}^{\\infty} n \\rho^n = (1-\\rho) \\sum_{n=1}^{\\infty} n \\rho^n $$\nWe use the identity for the sum of an arithmetico-geometric series, $\\sum_{n=1}^{\\infty} n x^n = \\frac{x}{(1-x)^2}$ for $|x|<1$.\n$$ L = (1-\\rho) \\left( \\frac{\\rho}{(1-\\rho)^2} \\right) = \\frac{\\rho}{1-\\rho} $$\nSubstituting $\\rho = \\lambda/\\mu$:\n$$ L = \\frac{\\lambda/\\mu}{1 - \\lambda/\\mu} = \\frac{\\lambda/\\mu}{(\\mu-\\lambda)/\\mu} = \\frac{\\lambda}{\\mu-\\lambda} $$\nThis expression for $L$ was derived using only the steady-state birth-death balance equations.\n\nFinally, we apply Little's Law. Little's Law states that the long-term average number of customers in a stable system, $L$, is equal to the long-term average effective arrival rate, $\\lambda_{eff}$, multiplied by the average time a customer spends in the system, $W$.\n$$ L = \\lambda_{eff} W $$\nFor this system, no referrals are turned away, so the effective arrival rate is simply the arrival rate, $\\lambda_{eff} = \\lambda$.\n$$ L = \\lambda W $$\nWe can now solve for $W$, the expected total time in the system:\n$$ W = \\frac{L}{\\lambda} $$\nSubstituting the expression for $L$ that we derived:\n$$ W = \\frac{1}{\\lambda} \\left( \\frac{\\lambda}{\\mu-\\lambda} \\right) = \\frac{1}{\\mu-\\lambda} $$\nThis is the required expression for $W$ as a function of $\\lambda$ and $\\mu$, derived from first principles as requested.\n\nNow, we evaluate this expression for the given clinic parameters:\nArrival rate $\\lambda = 5.4$ per hour.\nService rate $\\mu = 6.75$ per hour.\n\nFirst, we confirm the steady-state condition $\\lambda < \\mu$ is satisfied: $5.4 < 6.75$, which is true.\nWe now calculate $W$:\n$$ W = \\frac{1}{\\mu - \\lambda} = \\frac{1}{6.75 - 5.4} = \\frac{1}{1.35} \\text{ hours} $$\nThe problem requires the answer in minutes. We convert hours to minutes by multiplying by $60$:\n$$ W_{\\text{minutes}} = \\frac{1}{1.35} \\times 60 = \\frac{60}{1.35} $$\nTo simplify the calculation:\n$$ W_{\\text{minutes}} = \\frac{60}{135/100} = \\frac{60 \\times 100}{135} = \\frac{6000}{135} $$\nDividing the numerator and denominator by their greatest common divisor, which is $45$:\n$6000 \\div 45 = (6000 \\div 5) \\div 9 = 1200 \\div 9 = 400/3$.\n$135 \\div 45 = 3$.\nSo,\n$$ W_{\\text{minutes}} = \\frac{400}{3} = 133.333\\dots \\text{ minutes} $$\nLet me re-check the calculation.\n$$ \\frac{6000}{135} = \\frac{1200}{27} = \\frac{400}{9} $$\nAh, a calculation error. $1200 \\div 9$ is not correct. $1200/3=400$, $27/3=9$. So $\\frac{400}{9}$.\n$$ W_{\\text{minutes}} = \\frac{400}{9} = 44.444\\dots \\text{ minutes} $$\nThe problem asks to round the answer to four significant figures. The number is $44.444\\dots$. The first four significant figures are $4$, $4$, $4$, $4$. The fifth significant figure is $4$, which is less than $5$, so we round down (i.e., truncate).\nThe result is $44.44$.",
            "answer": "$$\\boxed{44.44}$$"
        }
    ]
}