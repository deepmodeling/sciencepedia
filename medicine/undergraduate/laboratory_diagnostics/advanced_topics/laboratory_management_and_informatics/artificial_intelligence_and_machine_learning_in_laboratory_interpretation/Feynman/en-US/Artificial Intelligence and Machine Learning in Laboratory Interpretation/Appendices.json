{
    "hands_on_practices": [
        {
            "introduction": "In many diagnostic applications, such as screening for rare diseases, the number of healthy individuals vastly outweighs the number of patients with the condition. This exercise demonstrates why the Area Under the Precision-Recall Curve (PR AUC) is often a more informative metric than the more common Receiver Operating Characteristic Area Under the Curve (ROC AUC) in these imbalanced settings. By calculating the PR AUC from first principles, you will develop a crucial skill for evaluating classifier performance in a way that is directly relevant to clinical utility .",
            "id": "5207923",
            "problem": "A clinical laboratory is validating a machine learning classifier to screen for a rare autoantibody in a referral population with disease prevalence $\\pi = 0.02$. The model outputs continuous scores that are thresholded to produce binary calls. From an internal validation set, the following precision–recall operating points were obtained by sweeping the decision threshold (reported as ordered pairs of recall $R$ and precision $P$ in order of increasing recall):\n- $(R,P) = (0, 1.0)$\n- $(R,P) = (0.10, 0.60)$\n- $(R,P) = (0.30, 0.50)$\n- $(R,P) = (0.55, 0.40)$\n- $(R,P) = (0.80, 0.30)$\n- $(R,P) = (1.00, 0.20)$\n\nAssume that between each consecutive pair of points, the precision–recall curve is linearly interpolated as a function $P(R)$ over $R \\in [0,1]$, and adopt the standard definition of the area under the precision–recall curve as the integral of $P(R)$ with respect to $R$ over $[0,1]$.\n\nTasks:\n1. Using only the information above, compute the Area Under the Precision–Recall Curve (PR AUC). Give your answer as a decimal number rounded to four significant figures.\n2. Using first principles of diagnostic test metrics, briefly justify why the Precision–Recall Area Under the Curve (PR AUC) is more informative than the Receiver Operating Characteristic Area Under the Curve (ROC AUC) for rare disease screening when $\\pi$ is small. No calculation is required for this justification; base your reasoning on the definitions of precision, recall, false positive rate, and the role of prevalence in posterior probabilities.\n\nOnly the numeric value for part 1 will be graded; express it as a pure number without a percentage sign or units.",
            "solution": "The problem has been validated and is deemed sound, well-posed, and objective. It is based on established principles of machine learning model evaluation and provides all necessary information for a complete solution.\n\nThe problem asks for two tasks: first, to compute the Area Under the Precision-Recall Curve (PR AUC) from a given set of operating points, and second, to provide a conceptual justification for the utility of PR AUC over ROC AUC in the context of rare disease screening.\n\n**Part 1: Computation of the Area Under the Precision–Recall Curve (PR AUC)**\n\nThe problem specifies that the precision-recall curve, denoted as a function $P(R)$, is formed by linear interpolation between a set of given operating points $(R_i, P_i)$. The Area Under the Curve (AUC) is defined by the integral $\\int_0^1 P(R) dR$. With piecewise linear interpolation, this integral can be calculated by summing the areas of the trapezoids formed between each consecutive pair of points.\n\nThe given operating points are:\n- $(R_0, P_0) = (0, 1.0)$\n- $(R_1, P_1) = (0.10, 0.60)$\n- $(R_2, P_2) = (0.30, 0.50)$\n- $(R_3, P_3) = (0.55, 0.40)$\n- $(R_4, P_4) = (0.80, 0.30)$\n- $(R_5, P_5) = (1.00, 0.20)$\n\nThe area of a trapezoid between points $(R_i, P_i)$ and $(R_{i+1}, P_{i+1})$ is given by the formula:\n$$ A_i = \\frac{1}{2} (P_i + P_{i+1}) (R_{i+1} - R_i) $$\n\nWe compute the area for each of the five segments defined by the six points:\n\nSegment 1: from $(0, 1.0)$ to $(0.10, 0.60)$\n$$ A_1 = \\frac{1}{2} (1.0 + 0.60) (0.10 - 0) = \\frac{1}{2} (1.60)(0.10) = 0.08 $$\n\nSegment 2: from $(0.10, 0.60)$ to $(0.30, 0.50)$\n$$ A_2 = \\frac{1}{2} (0.60 + 0.50) (0.30 - 0.10) = \\frac{1}{2} (1.10)(0.20) = 0.11 $$\n\nSegment 3: from $(0.30, 0.50)$ to $(0.55, 0.40)$\n$$ A_3 = \\frac{1}{2} (0.50 + 0.40) (0.55 - 0.30) = \\frac{1}{2} (0.90)(0.25) = 0.1125 $$\n\nSegment 4: from $(0.55, 0.40)$ to $(0.80, 0.30)$\n$$ A_4 = \\frac{1}{2} (0.40 + 0.30) (0.80 - 0.55) = \\frac{1}{2} (0.70)(0.25) = 0.0875 $$\n\nSegment 5: from $(0.80, 0.30)$ to $(1.00, 0.20)$\n$$ A_5 = \\frac{1}{2} (0.30 + 0.20) (1.00 - 0.80) = \\frac{1}{2} (0.50)(0.20) = 0.05 $$\n\nThe total PR AUC is the sum of these individual areas:\n$$ \\text{PR AUC} = A_1 + A_2 + A_3 + A_4 + A_5 $$\n$$ \\text{PR AUC} = 0.08 + 0.11 + 0.1125 + 0.0875 + 0.05 $$\n$$ \\text{PR AUC} = 0.19 + 0.2000 + 0.05 = 0.44 $$\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $0.44$, which can be expressed as $0.4400$.\n\n**Part 2: Justification for PR AUC over ROC AUC in Rare Disease Screening**\n\nTo justify why the PR AUC is more informative than the ROC AUC for rare diseases (small prevalence $\\pi$), we must examine the definitions of the metrics involved. Let $D^+$ denote the presence of disease and $T^+$ denote a positive test result.\n\nThe metrics for a Receiver Operating Characteristic (ROC) curve are:\n- **True Positive Rate (TPR)**, also known as Recall ($R$) or Sensitivity: $R = \\text{TPR} = P(T^+|D^+)$. This is the fraction of actual positives correctly identified.\n- **False Positive Rate (FPR)**: $\\text{FPR} = P(T^+|\\neg D^+)$. This is the fraction of actual negatives incorrectly identified as positive.\n\nAn ROC curve plots TPR versus FPR. Both metrics are conditioned on the true disease status. Consequently, the ROC curve and its area (ROC AUC) are invariant to the disease prevalence, $\\pi = P(D^+)$.\n\nThe metrics for a Precision-Recall (PR) curve are:\n- **Recall (R)**, as defined above.\n- **Precision (P)**, also known as Positive Predictive Value (PPV): $P = P(D^+|T^+)$. This is the fraction of positive test results that are correct.\n\nPrecision is a posterior probability, and its relationship with prevalence can be made explicit using Bayes' theorem:\n$$ P(D^+|T^+) = \\frac{P(T^+|D^+)P(D^+)}{P(T^+)} $$\nThe denominator, $P(T^+)$, can be expanded using the law of total probability:\n$$ P(T^+) = P(T^+|D^+)P(D^+) + P(T^+|\\neg D^+)P(\\neg D^+) $$\nSubstituting the definitions of TPR, FPR, and prevalence $\\pi$:\n$$ P = \\frac{\\text{TPR} \\cdot \\pi}{\\text{TPR} \\cdot \\pi + \\text{FPR} \\cdot (1 - \\pi)} $$\nThis equation demonstrates that Precision is fundamentally dependent on prevalence $\\pi$.\n\nIn the context of a rare disease, $\\pi$ is very small, and thus $(1 - \\pi)$ is close to $1$. The population of non-diseased individuals is vastly larger than the population of diseased individuals. Consequently, even a very small FPR can result in a large absolute number of false positives ($FP$) that can easily outnumber the true positives ($TP$). This has a profound impact on Precision. For example, if $\\pi = 0.02$, a classifier with a high TPR of $0.9$ and a low FPR of $0.05$ would have a Precision of:\n$$ P = \\frac{0.9 \\cdot 0.02}{0.9 \\cdot 0.02 + 0.05 \\cdot 0.98} = \\frac{0.018}{0.018 + 0.049} \\approx 0.2687 $$\nDespite yielding an excellent operating point on an ROC curve, fewer than $27\\%$ of the positive classifications would be correct.\n\nThe ROC AUC, being insensitive to prevalence, can provide a misleadingly optimistic assessment of a classifier's performance in a rare disease setting. It measures the ability to distinguish between positive and negative classes without regard to their proportions in the population. The PR AUC, in contrast, directly incorporates the effect of prevalence through the Precision metric. It effectively evaluates the performance on the minority (positive) class, which is often the primary interest in screening applications. A sharp drop in the PR curve immediately signals that the classifier generates an excess of false positives relative to true positives at a given recall level, a critical piece of information that is obscured in ROC space. Therefore, for assessing classifiers on imbalanced datasets, such as in rare disease screening, the PR AUC is a more informative and relevant metric than the ROC AUC.",
            "answer": "$$\\boxed{0.4400}$$"
        },
        {
            "introduction": "While summary metrics like AUC assess a model's performance across all possible thresholds, a practical diagnostic test requires a single, fixed operating point to classify new cases. This practice introduces Youden's $J$ index, a common method for selecting an \"optimal\" threshold by balancing sensitivity and specificity. Through this calculation, you will not only apply the method but also critically analyze its underlying assumption that false positives and false negatives carry equal clinical weight .",
            "id": "5207994",
            "problem": "A hospital laboratory is piloting an Artificial Intelligence (AI) and Machine Learning (ML) classifier to assist clinical interpretation of a continuous laboratory risk score for acute kidney injury. In a prospective validation cohort, there are $N^{+} = 120$ patients with confirmed disease and $N^{-} = 180$ patients without disease. The classifier outputs a calibrated risk score in $[0,1]$, and a decision threshold $t$ converts scores to a binary positive/negative result.\n\nFor each candidate threshold $t \\in \\{0.30, 0.50, 0.70, 0.85\\}$, the number of patients predicted positive (i.e., classifier output $\\geq t$) among the diseased ($N^{+}$) and non-diseased ($N^{-}$) groups is recorded as follows:\n- At $t = 0.30$: predicted positive among diseased $= 110$; predicted positive among non-diseased $= 70$.\n- At $t = 0.50$: predicted positive among diseased $= 96$; predicted positive among non-diseased $= 36$.\n- At $t = 0.70$: predicted positive among diseased $= 80$; predicted positive among non-diseased $= 20$.\n- At $t = 0.85$: predicted positive among diseased $= 60$; predicted positive among non-diseased $= 10$.\n\nUsing only foundational definitions of sensitivity and specificity from the confusion matrix, compute, for each threshold $t$, the sensitivity $Se(t)$ and specificity $Sp(t)$, then compute Youden’s index $J(t)$ from these quantities. Identify the single threshold $t$ that maximizes $J(t)$. In your reasoning, articulate the assumptions under which maximizing $J(t)$ is appropriate for clinical decision-making in laboratory diagnostics, and explain at least two limitations of this criterion in practice.\n\nReport the final answer as the value of the threshold $t$ that maximizes $J(t)$. Do not include any units. If your computations produce exact decimal values, do not round them.",
            "solution": "The problem requires the evaluation of a machine learning classifier for acute kidney injury at four different decision thresholds. We must first validate the problem statement.\n\nThe givens are:\n- The number of patients with confirmed disease (positives): $N^{+} = 120$.\n- The number of patients without disease (negatives): $N^{-} = 180$.\n- The set of candidate decision thresholds: $t \\in \\{0.30, 0.50, 0.70, 0.85\\}$.\n- The number of true positives ($TP$) and false positives ($FP$) at each threshold:\n  - For $t = 0.30$: $TP(0.30) = 110$, $FP(0.30) = 70$.\n  - For $t = 0.50$: $TP(0.50) = 96$, $FP(0.50) = 36$.\n  - For $t = 0.70$: $TP(0.70) = 80$, $FP(0.70) = 20$.\n  - For $t = 0.85$: $TP(0.85) = 60$, $FP(0.85) = 10$.\n\nThe problem is scientifically grounded, well-posed, objective, and contains complete and consistent data. It uses standard metrics from diagnostic medicine and statistics. The number of predicted positives for each class does not exceed the total number in that class. Furthermore, as the threshold $t$ increases, the number of samples classified as positive (both $TP$ and $FP$) non-increases, which is a necessary property of a threshold-based classifier. The problem is therefore deemed valid.\n\nWe proceed with the solution by first defining the necessary performance metrics based on the confusion matrix. The elements of the confusion matrix for a given threshold $t$ are:\n- True Positives ($TP(t)$): Patients with the disease correctly classified as positive.\n- False Positives ($FP(t)$): Patients without the disease incorrectly classified as positive.\n- False Negatives ($FN(t)$): Patients with the disease incorrectly classified as negative.\n- True Negatives ($TN(t)$): Patients without the disease correctly classified as negative.\n\nThe total number of diseased and non-diseased patients are $N^{+} = TP(t) + FN(t)$ and $N^{-} = FP(t) + TN(t)$, respectively. From these, we can calculate $FN(t) = N^{+} - TP(t)$ and $TN(t) = N^{-} - FP(t)$.\n\nThe primary metrics are defined as:\n- Sensitivity ($Se(t)$), or True Positive Rate: The proportion of diseased patients correctly identified.\n$$Se(t) = \\frac{TP(t)}{N^{+}} = \\frac{TP(t)}{TP(t) + FN(t)}$$\n- Specificity ($Sp(t)$), or True Negative Rate: The proportion of non-diseased patients correctly identified.\n$$Sp(t) = \\frac{TN(t)}{N^{-}} = \\frac{TN(t)}{TN(t) + FP(t)}$$\n- Youden's Index ($J(t)$), or the Youden's J statistic: A metric that summarizes the diagnostic ability of a test. It is the sum of sensitivity and specificity minus one.\n$$J(t) = Se(t) + Sp(t) - 1$$\nWe will now compute these values for each given threshold.\n\nFor $t = 0.30$:\n- Given: $TP(0.30) = 110$, $FP(0.30) = 70$.\n- $FN(0.30) = N^{+} - TP(0.30) = 120 - 110 = 10$.\n- $TN(0.30) = N^{-} - FP(0.30) = 180 - 70 = 110$.\n- $Se(0.30) = \\frac{110}{120} = \\frac{11}{12}$.\n- $Sp(0.30) = \\frac{110}{180} = \\frac{11}{18}$.\n- $J(0.30) = \\frac{11}{12} + \\frac{11}{18} - 1 = \\frac{33 + 22}{36} - 1 = \\frac{55}{36} - \\frac{36}{36} = \\frac{19}{36}$.\n\nFor $t = 0.50$:\n- Given: $TP(0.50) = 96$, $FP(0.50) = 36$.\n- $FN(0.50) = N^{+} - TP(0.50) = 120 - 96 = 24$.\n- $TN(0.50) = N^{-} - FP(0.50) = 180 - 36 = 144$.\n- $Se(0.50) = \\frac{96}{120} = \\frac{4 \\times 24}{5 \\times 24} = \\frac{4}{5}$.\n- $Sp(0.50) = \\frac{144}{180} = \\frac{4 \\times 36}{5 \\times 36} = \\frac{4}{5}$.\n- $J(0.50) = \\frac{4}{5} + \\frac{4}{5} - 1 = \\frac{8}{5} - 1 = \\frac{3}{5}$.\n\nFor $t = 0.70$:\n- Given: $TP(0.70) = 80$, $FP(0.70) = 20$.\n- $FN(0.70) = N^{+} - TP(0.70) = 120 - 80 = 40$.\n- $TN(0.70) = N^{-} - FP(0.70) = 180 - 20 = 160$.\n- $Se(0.70) = \\frac{80}{120} = \\frac{2}{3}$.\n- $Sp(0.70) = \\frac{160}{180} = \\frac{8}{9}$.\n- $J(0.70) = \\frac{2}{3} + \\frac{8}{9} - 1 = \\frac{6 + 8}{9} - 1 = \\frac{14}{9} - \\frac{9}{9} = \\frac{5}{9}$.\n\nFor $t = 0.85$:\n- Given: $TP(0.85) = 60$, $FP(0.85) = 10$.\n- $FN(0.85) = N^{+} - TP(0.85) = 120 - 60 = 60$.\n- $TN(0.85) = N^{-} - FP(0.85) = 180 - 10 = 170$.\n- $Se(0.85) = \\frac{60}{120} = \\frac{1}{2}$.\n- $Sp(0.85) = \\frac{170}{180} = \\frac{17}{18}$.\n- $J(0.85) = \\frac{1}{2} + \\frac{17}{18} - 1 = \\frac{9 + 17}{18} - 1 = \\frac{26}{18} - \\frac{18}{18} = \\frac{8}{18} = \\frac{4}{9}$.\n\nNow, we compare the values of $J(t)$ to find the maximum. To facilitate comparison, we convert the fractions to a common denominator of $180$:\n- $J(0.30) = \\frac{19}{36} = \\frac{19 \\times 5}{36 \\times 5} = \\frac{95}{180}$.\n- $J(0.50) = \\frac{3}{5} = \\frac{3 \\times 36}{5 \\times 36} = \\frac{108}{180}$.\n- $J(0.70) = \\frac{5}{9} = \\frac{5 \\times 20}{9 \\times 20} = \\frac{100}{180}$.\n- $J(0.85) = \\frac{4}{9} = \\frac{4 \\times 20}{9 \\times 20} = \\frac{80}{180}$.\n\nComparing the numerators ($80  95  100  108$), the maximum value of Youden's Index is $J(0.50) = \\frac{108}{180} = \\frac{3}{5}$. Therefore, the threshold that maximizes $J(t)$ is $t=0.50$.\n\nDiscussion of Assumptions and Limitations:\n\nMaximizing Youden's Index $J(t) = Se(t) + Sp(t) - 1$ is appropriate for clinical decision-making under a key assumption: that the utility of the test is maximized when sensitivity and specificity are given equal weight. This implies that the cost or harm associated with a false negative (missing a true case of disease) is considered equal to the cost or harm of a false positive (incorrectly labeling a healthy individual as diseased). This criterion is thus optimal when the objective is to maximize the overall number of correctly classified patients (both diseased and non-diseased), without prioritizing one type of error over the other.\n\nHowever, this approach has at least two significant limitations in clinical practice:\n1.  **Assumption of Equal Misclassification Costs:** The core assumption that false negatives and false positives have equal consequences is rarely true in medicine. For a serious condition like acute kidney injury, a false negative ($FN$) could lead to delayed or missed treatment, resulting in severe morbidity (e.g., progression to chronic kidney disease, need for dialysis) or mortality. A false positive ($FP$), while causing patient anxiety and potentially leading to unnecessary follow-up tests and costs, generally has a much lower associated harm. A threshold chosen by maximizing Youden's index may be suboptimal and even dangerous if the cost of an $FN$ is substantially higher than the cost of an $FP$. Decision-analytic methods that explicitly model these differing costs are often more appropriate.\n2.  **Independence from Disease Prevalence:** Youden's index is calculated from sensitivity and specificity, which are intrinsic properties of the test and are independent of the prevalence of the disease in the population being tested. However, the optimal decision threshold for clinical application *should* be influenced by prevalence. For instance, in a low-prevalence setting, even a test with high specificity will generate a large number of false positives, leading to a low Positive Predictive Value ($PPV$). A clinician might therefore prefer a higher threshold to increase specificity and improve the $PPV$. Conversely, in a high-prevalence setting, a lower threshold might be acceptable to maximize sensitivity. Youden's index does not account for this context, and the threshold it identifies may not optimize the test's utility in a specific target population.\n\nIn summary, despite its mathematical simplicity, maximizing Youden's index is a-contextual and often fails to reflect the complex trade-offs inherent in clinical decision-making.",
            "answer": "$$\\boxed{0.50}$$"
        },
        {
            "introduction": "A machine learning model's performance is not static; it can change dramatically when deployed in a new clinical environment with a different patient population. This exercise tackles the critical issue of prevalence shift, where the frequency of the disease differs between the training and deployment settings. You will quantify how this shift impacts a model's Positive Predictive Value ($PPV$) and learn a fundamental method to recalibrate the model's probability outputs, ensuring they remain trustworthy and clinically meaningful .",
            "id": "5208013",
            "problem": "A machine learning classifier built to flag bloodstream infection from routine laboratory features was developed on an inpatient cohort with disease prevalence $ \\pi_{\\text{train}} $. On the training cohort, at a fixed decision threshold, the operating characteristics were measured as sensitivity $ \\mathrm{Se} $ and specificity $ \\mathrm{Sp} $. The model was calibrated so that its predicted probabilities reflect the true posterior probability under the training prevalence (that is, it is well-calibrated on the training cohort). The model is then deployed, without changing the score distribution or decision threshold, to an urgent-care cohort with a different disease prevalence $ \\pi_{\\text{deploy}} $. Assume a prior-probability (prevalence) shift only: the class-conditional score distributions, and hence $ \\mathrm{Se} $ and $ \\mathrm{Sp} $, remain unchanged between training and deployment.\n\nUsing only the base definitions of sensitivity, specificity, prevalence, and Bayes’ theorem, first derive the expression for the positive predictive value $ \\mathrm{PPV} $ as a function of $ \\mathrm{Se} $, $ \\mathrm{Sp} $, and prevalence $ \\pi $. Then, for $ \\mathrm{Se} = 0.92 $, $ \\mathrm{Sp} = 0.97 $, $ \\pi_{\\text{train}} = 0.10 $, and $ \\pi_{\\text{deploy}} = 0.025 $, compute the multiplicative change in positive predictive value upon deployment, defined as\n$$\n\\lambda \\equiv \\frac{\\mathrm{PPV}(\\pi_{\\text{deploy}})}{\\mathrm{PPV}(\\pi_{\\text{train}})}\n$$.\nNext, consider that the model’s calibration on the training cohort can be written with a logistic link as $ \\operatorname{logit}(p_{\\text{train}}(s)) = a + b\\,s $, where $ s $ is the classifier score, $ a $ is an intercept, and $ b $ is a slope. Under a pure prior shift, derive from Bayes’ theorem the required intercept adjustment $ \\Delta $ to recalibrate posterior probabilities to the deployment prevalence while keeping $ b $ unchanged, and evaluate $ \\Delta $ numerically for the given prevalences. Report as your final answer the pair $ (\\lambda, \\Delta) $.\n\nExpress both $ \\lambda $ and $ \\Delta $ as decimals and round each to four significant figures. No units are required.",
            "solution": "The problem is evaluated as valid. It is scientifically grounded in probability theory and machine learning principles, is well-posed with sufficient information for a unique solution, and is expressed in objective, formal language. We proceed with the solution.\n\nThe problem asks for two quantities: the multiplicative change in positive predictive value ($\\lambda$) and the intercept adjustment ($\\Delta$) for recalibrating a logistic classifier under a shift in disease prevalence. We will address each part in sequence, starting from first principles as requested.\n\nLet $D^+$ denote the event that a patient has the disease (bloodstream infection) and $D^-$ denote the event that the patient does not. Let $T^+$ be the event of a positive classification by the machine learning model, and $T^-$ be a negative classification.\n\nThe prevalence, $\\pi$, is the prior probability of disease: $\\pi = P(D^+)$.\nThe sensitivity, $\\mathrm{Se}$, is the true positive rate: $\\mathrm{Se} = P(T^+|D^+)$.\nThe specificity, $\\mathrm{Sp}$, is the true negative rate: $\\mathrm{Sp} = P(T^-|D^-)$.\nFrom the definition of specificity, the false positive rate is $P(T^+|D^-) = 1 - P(T^-|D^-) = 1 - \\mathrm{Sp}$.\n\n### Part 1: Derivation of PPV and Calculation of $\\lambda$\n\nThe Positive Predictive Value ($\\mathrm{PPV}$) is the posterior probability of disease given a positive test result, $P(D^+|T^+)$. We derive its expression using Bayes' theorem:\n$$ \\mathrm{PPV} = P(D^+|T^+) = \\frac{P(T^+|D^+) P(D^+)}{P(T^+)} $$\nThe denominator, $P(T^+)$, is the marginal probability of a positive test. It can be expanded using the law of total probability:\n$$ P(T^+) = P(T^+|D^+) P(D^+) + P(T^+|D^-) P(D^-) $$\nSubstituting the defined terms:\n$$ P(T^+) = (\\mathrm{Se})(\\pi) + (1-\\mathrm{Sp})(1-\\pi) $$\nSubstituting this back into the expression for $\\mathrm{PPV}$ yields the desired formula as a function of $\\pi$, $\\mathrm{Se}$, and $\\mathrm{Sp}$:\n$$ \\mathrm{PPV}(\\pi, \\mathrm{Se}, \\mathrm{Sp}) = \\frac{\\mathrm{Se} \\cdot \\pi}{\\mathrm{Se} \\cdot \\pi + (1 - \\mathrm{Sp})(1 - \\pi)} $$\nNow, we use the given numerical values to compute the PPV for the training and deployment cohorts. The given values are $\\mathrm{Se} = 0.92$, $\\mathrm{Sp} = 0.97$, $\\pi_{\\text{train}} = 0.10$, and $\\pi_{\\text{deploy}} = 0.025$. The false positive rate is $1 - \\mathrm{Sp} = 1 - 0.97 = 0.03$.\n\nFor the training cohort:\n$$ \\mathrm{PPV}(\\pi_{\\text{train}}) = \\frac{0.92 \\cdot 0.10}{0.92 \\cdot 0.10 + (0.03)(1 - 0.10)} = \\frac{0.092}{0.092 + (0.03)(0.90)} = \\frac{0.092}{0.092 + 0.027} = \\frac{0.092}{0.119} $$\n$$ \\mathrm{PPV}(\\pi_{\\text{train}}) \\approx 0.773109... $$\nFor the deployment cohort:\n$$ \\mathrm{PPV}(\\pi_{\\text{deploy}}) = \\frac{0.92 \\cdot 0.025}{0.92 \\cdot 0.025 + (0.03)(1 - 0.025)} = \\frac{0.023}{0.023 + (0.03)(0.975)} = \\frac{0.023}{0.023 + 0.02925} = \\frac{0.023}{0.05225} $$\n$$ \\mathrm{PPV}(\\pi_{\\text{deploy}}) \\approx 0.440191... $$\nThe multiplicative change, $\\lambda$, is the ratio of these two values:\n$$ \\lambda = \\frac{\\mathrm{PPV}(\\pi_{\\text{deploy}})}{\\mathrm{PPV}(\\pi_{\\text{train}})} = \\frac{0.023 / 0.05225}{0.092 / 0.119} \\approx \\frac{0.440191...}{0.773109...} \\approx 0.569398... $$\nRounding to four significant figures, we get $\\lambda \\approx 0.5694$.\n\n### Part 2: Derivation and Calculation of the Intercept Adjustment $\\Delta$\n\nThe problem postulates a logistic calibration model, which connects the model score $s$ to the posterior probability $p(s) = P(D^+|s)$ via the logit link function, $\\operatorname{logit}(p) = \\ln(\\frac{p}{1-p})$. The quantity $\\frac{p}{1-p}$ represents the posterior odds.\n\nFrom Bayes' theorem, the posterior odds of disease given a score $s$ can be written as the product of the likelihood ratio for the score and the prior odds of disease:\n$$ \\frac{P(D^+|s)}{P(D^-|s)} = \\frac{P(s|D^+)}{P(s|D^-)} \\cdot \\frac{P(D^+)}{P(D^-)} $$\nTaking the natural logarithm of both sides gives the relationship in log-odds (logit) space:\n$$ \\ln\\left(\\frac{P(D^+|s)}{P(D^-|s)}\\right) = \\ln\\left(\\frac{P(s|D^+)}{P(s|D^-)}\\right) + \\ln\\left(\\frac{P(D^+)}{P(D^-)}\\right) $$\nLet $p(s) = P(D^+|s)$ and $\\pi = P(D^+)$. The equation becomes:\n$$ \\operatorname{logit}(p(s)) = \\ln\\left(\\frac{P(s|D^+)}{P(s|D^-)}\\right) + \\operatorname{logit}(\\pi) $$\nThe assumption of a \"prior-probability shift only\" implies that the class-conditional score distributions, $P(s|D^+)$ and $P(s|D^-)$, are invariant between the training and deployment cohorts. Consequently, the log-likelihood ratio term, $\\ln\\left(\\frac{P(s|D^+)}{P(s|D^-)}\\right)$, is also invariant.\n\nFor the training cohort, the calibration model is given as $\\operatorname{logit}(p_{\\text{train}}(s)) = a + b\\,s$. Based on our derivation, this means:\n$$ a + b\\,s = \\ln\\left(\\frac{P(s|D^+)}{P(s|D^-)}\\right) + \\operatorname{logit}(\\pi_{\\text{train}}) $$\nFor the deployment cohort, the recalibrated model is $\\operatorname{logit}(p_{\\text{deploy}}(s)) = (a+\\Delta) + b\\,s$, where $b$ is unchanged and $\\Delta$ is the intercept adjustment. This must satisfy:\n$$ (a+\\Delta) + b\\,s = \\ln\\left(\\frac{P(s|D^+)}{P(s|D^-)}\\right) + \\operatorname{logit}(\\pi_{\\text{deploy}}) $$\nBy subtracting the first equation from the second, we isolate the adjustment term $\\Delta$:\n$$ ((a+\\Delta) + b\\,s) - (a + b\\,s) = \\left(\\ln\\left(\\frac{P(s|D^+)}{P(s|D^-)}\\right) + \\operatorname{logit}(\\pi_{\\text{deploy}})\\right) - \\left(\\ln\\left(\\frac{P(s|D^+)}{P(s|D^-)}\\right) + \\operatorname{logit}(\\pi_{\\text{train}})\\right) $$\nThe terms $(a+b\\,s)$ and the log-likelihood ratio cancel, yielding the expression for $\\Delta$:\n$$ \\Delta = \\operatorname{logit}(\\pi_{\\text{deploy}}) - \\operatorname{logit}(\\pi_{\\text{train}}) $$\nThis shows that the required adjustment to the logit-space intercept is precisely the difference in the logit-transformed prevalences. We now calculate its numerical value:\n$$ \\Delta = \\ln\\left(\\frac{\\pi_{\\text{deploy}}}{1 - \\pi_{\\text{deploy}}}\\right) - \\ln\\left(\\frac{\\pi_{\\text{train}}}{1 - \\pi_{\\text{train}}}\\right) = \\ln\\left(\\frac{\\pi_{\\text{deploy}}(1 - \\pi_{\\text{train}})}{\\pi_{\\text{train}}(1 - \\pi_{\\text{deploy}})}\\right) $$\nSubstituting the prevalences $\\pi_{\\text{train}} = 0.10$ and $\\pi_{\\text{deploy}} = 0.025$:\n$$ \\Delta = \\ln\\left(\\frac{0.025 \\cdot (1 - 0.10)}{0.10 \\cdot (1 - 0.025)}\\right) = \\ln\\left(\\frac{0.025 \\cdot 0.90}{0.10 \\cdot 0.975}\\right) = \\ln\\left(\\frac{0.0225}{0.0975}\\right) $$\n$$ \\Delta = \\ln\\left(\\frac{225}{975}\\right) = \\ln\\left(\\frac{3}{13}\\right) \\approx -1.466337... $$\nRounding to four significant figures, we get $\\Delta \\approx -1.466$.\n\nThe final answer is the pair $(\\lambda, \\Delta)$ with values rounded to four significant figures.\n$\\lambda \\approx 0.5694$\n$\\Delta \\approx -1.466$",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.5694  -1.466 \\end{pmatrix}}$$"
        }
    ]
}