{
    "hands_on_practices": [
        {
            "introduction": "Before an artifact can be corrected, it must first be located. A crucial first step in many advanced Metal Artifact Reduction (MAR) algorithms is to create a 'metal mask'—a map that identifies which voxels in the CT image correspond to metallic objects. This practice guides you through building a probabilistic classifier to automate this segmentation task. By applying Bayes' theorem to a set of intuitive image features, you will learn how statistical pattern recognition can be used to distinguish metal from tissue, forming the foundation for targeted artifact correction .",
            "id": "4900160",
            "problem": "You are designing a probabilistic metal mask as part of a Metal Artifact Reduction (MAR) pipeline in Computed Tomography (CT). Using a Bayesian classifier, you classify each voxel as metal or non-metal based on a feature vector consisting of intensity in Hounsfield Units (HU), image gradient magnitude in HU per millimeter, and proximity in millimeters to known high-attenuation structures (e.g., dental fillings, hip prostheses). You must formalize the classification using Bayes’ theorem and compute the posterior probability that a voxel contains metal.\n\nAssume the following:\n\n- Classes: metal, denoted by the label $M$, and non-metal, denoted by the label $N$.\n- Prior probabilities: $\\mathbb{P}(M)=0.03$ and $\\mathbb{P}(N)=0.97$.\n- Likelihood model: Given the class, the three features are conditionally independent and each feature follows a normal distribution (Gaussian) with the specified class-dependent mean and standard deviation.\n- Feature definitions and units:\n  1. Intensity $I$ in Hounsfield Units (HU).\n  2. Gradient magnitude $G$ in HU per millimeter.\n  3. Proximity $D$ in millimeters.\n\nClass-conditional Gaussian parameters (means and standard deviations) for each feature are:\n- For $M$ (metal):\n  - $I \\sim \\mathcal{N}(\\mu_{I|M}=3500,\\ \\sigma_{I|M}=400)$,\n  - $G \\sim \\mathcal{N}(\\mu_{G|M}=1200,\\ \\sigma_{G|M}=400)$,\n  - $D \\sim \\mathcal{N}(\\mu_{D|M}=0.5,\\ \\sigma_{D|M}=0.5)$.\n- For $N$ (non-metal):\n  - $I \\sim \\mathcal{N}(\\mu_{I|N}=300,\\ \\sigma_{I|N}=600)$,\n  - $G \\sim \\mathcal{N}(\\mu_{G|N}=100,\\ \\sigma_{G|N}=100)$,\n  - $D \\sim \\mathcal{N}(\\mu_{D|N}=12,\\ \\sigma_{D|N}=6)$.\n\nYour task is to:\n- Starting from Bayes’ theorem and the stated modeling assumptions, derive the posterior probability $\\mathbb{P}(M \\mid I,G,D)$ for a voxel with features $(I,G,D)$.\n- Implement a program that evaluates $\\mathbb{P}(M \\mid I,G,D)$ for each of the specified test cases and returns a binary detection decision for the presence of a metal trace, defined as $1$ if $\\mathbb{P}(M \\mid I,G,D) \\ge 0.5$ and $0$ otherwise.\n- All numerical answers must be expressed as floating point numbers. The posterior probabilities must be rounded to $6$ decimal places. The detection decisions must be integers in $\\{0,1\\}$.\n\nUse the following test suite of feature vectors $(I,G,D)$ with units:\n- Case $1$: $(4200,\\ 1800,\\ 0.2)$.\n- Case $2$: $(30,\\ 20,\\ 25)$.\n- Case $3$: $(1200,\\ 200,\\ 18)$.\n- Case $4$: $(900,\\ 700,\\ 2.0)$.\n- Case $5$: $(3500,\\ 0,\\ 5)$.\n- Case $6$: $(500,\\ 150,\\ 0.1)$.\n\nCompute:\n- The posterior probabilities $\\mathbb{P}(M \\mid I,G,D)$ for the six cases as floats rounded to $6$ decimal places.\n- The detection decisions as integers in $\\{0,1\\}$ using the threshold rule stated above.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with two lists: the first containing the six posterior probabilities in order, and the second containing the six detection decisions in order. For example, the required output format is $[[p_1,p_2,p_3,p_4,p_5,p_6],[d_1,d_2,d_3,d_4,d_5,d_6]]$ where each $p_i$ is a float rounded to $6$ decimal places and each $d_i$ is an integer equal to $0$ or $1$.",
            "solution": "The problem is reviewed and found to be valid. It is a well-posed Bayesian classification problem grounded in established principles of medical image analysis and statistical pattern recognition. All necessary data, including prior probabilities and class-conditional likelihood models, are provided, and the objective is clearly defined.\n\nThe task is to compute the posterior probability that a voxel contains metal, $\\mathbb{P}(M \\mid I,G,D)$, given a feature vector $(I,G,D)$ representing intensity, gradient magnitude, and proximity. The classification is performed using a Naive Bayes classifier.\n\nAccording to Bayes' theorem, the posterior probability of class $M$ (metal) given the evidence $(I,G,D)$ is:\n$$\n\\mathbb{P}(M \\mid I,G,D) = \\frac{\\mathbb{P}(I,G,D \\mid M) \\mathbb{P}(M)}{\\mathbb{P}(I,G,D)}\n$$\nThe term in the denominator, $\\mathbb{P}(I,G,D)$, is the marginal probability of observing the evidence, which can be expanded using the law of total probability over the two classes, $M$ (metal) and $N$ (non-metal):\n$$\n\\mathbb{P}(I,G,D) = \\mathbb{P}(I,G,D \\mid M) \\mathbb{P}(M) + \\mathbb{P}(I,G,D \\mid N) \\mathbb{P}(N)\n$$\nSubstituting this into the Bayes' formula yields:\n$$\n\\mathbb{P}(M \\mid I,G,D) = \\frac{\\mathbb{P}(I,G,D \\mid M) \\mathbb{P}(M)}{\\mathbb{P}(I,G,D \\mid M) \\mathbb{P}(M) + \\mathbb{P}(I,G,D \\mid N) \\mathbb{P}(N)}\n$$\nThe problem states that the features $I$, $G$, and $D$ are conditionally independent given the class. This is the \"naive\" assumption in the Naive Bayes classifier. This allows us to express the joint class-conditional likelihoods as the product of the individual feature likelihoods:\n$$\n\\mathbb{P}(I,G,D \\mid C) = \\mathbb{P}(I \\mid C) \\cdot \\mathbb{P}(G \\mid C) \\cdot \\mathbb{P}(D \\mid C) \\quad \\text{for } C \\in \\{M, N\\}\n$$\nEach feature's likelihood, $\\mathbb{P}(x \\mid C)$, is modeled by a normal (Gaussian) distribution, $\\mathcal{N}(\\mu_{x|C}, \\sigma_{x|C})$. The probability density function (PDF) for a normal distribution is:\n$$\nf(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n$$\nDirect computation of the posterior probability can be susceptible to numerical underflow, as the product of small likelihood values can become zero in floating-point arithmetic. A more stable approach is to work with log-probabilities. Let's define a score for each class, $S_C$, as the logarithm of the joint probability of the class and the evidence:\n$$\nS_C = \\log\\left( \\mathbb{P}(I,G,D \\mid C) \\mathbb{P}(C) \\right) = \\log\\mathbb{P}(I,G,D \\mid C) + \\log\\mathbb{P}(C)\n$$\nUsing the conditional independence assumption, this expands to:\n$$\nS_C = \\sum_{x \\in \\{I,G,D\\}} \\log\\mathbb{P}(x \\mid C) + \\log\\mathbb{P}(C)\n$$\nThe logarithm of the Gaussian PDF is:\n$$\n\\log f(x; \\mu, \\sigma) = -\\log(\\sigma) - \\frac{1}{2}\\log(2\\pi) - \\frac{(x - \\mu)^2}{2\\sigma^2}\n$$\nThe posterior probability $\\mathbb{P}(M \\mid I,G,D)$ can be expressed in terms of the scores $S_M$ and $S_N$:\n$$\n\\mathbb{P}(M \\mid I,G,D) = \\frac{e^{S_M}}{e^{S_M} + e^{S_N}}\n$$\nTo further improve numerical stability, this can be rearranged as:\n$$\n\\mathbb{P}(M \\mid I,G,D) = \\frac{1}{1 + \\frac{e^{S_N}}{e^{S_M}}} = \\frac{1}{1 + e^{S_N - S_M}}\n$$\nThis is the logistic sigmoid function applied to the difference of scores, which is robust against both underflow and overflow.\n\nThe detection decision is based on a threshold of $0.5$. A voxel is classified as metal if $\\mathbb{P}(M \\mid I,G,D) \\ge 0.5$. This condition is equivalent to:\n$$\n\\frac{1}{1 + e^{S_N - S_M}} \\ge 0.5 \\implies 1 \\ge 0.5(1 + e^{S_N - S_M}) \\implies 2 \\ge 1 + e^{S_N - S_M} \\implies 1 \\ge e^{S_N - S_M}\n$$\nTaking the natural logarithm of both sides, we get:\n$$\n\\log(1) \\ge S_N - S_M \\implies 0 \\ge S_N - S_M \\implies S_M \\ge S_N\n$$\nThis is the maximum a posteriori (MAP) decision rule: we select the class with the higher score.\n\nThe implementation will proceed as follows:\n1.  Define the means, standard deviations, and prior probabilities for both classes ($M$ and $N$) as given.\n2.  Create a function to calculate the log-PDF of a Gaussian distribution for a given value $x$, mean $\\mu$, and standard deviation $\\sigma$.\n3.  For each test case vector $(I,G,D)$:\n    a. Calculate the score $S_M$ by summing the log-PDFs of $I$, $G$, and $D$ using metal-class parameters, and adding the log of the prior $\\mathbb{P}(M)$.\n    b. Similarly, calculate the score $S_N$ for the non-metal class.\n    c. Compute the posterior probability $\\mathbb{P}(M \\mid I,G,D)$ using the formula $1 / (1 + e^{S_N - S_M})$.\n    d. Round the posterior probability to $6$ decimal places.\n    e. Determine the detection decision: $1$ if the posterior is $\\ge 0.5$, and $0$ otherwise.\n4.  Collect the computed probabilities and decisions into two separate lists.\n5.  Format the final output as a string representing a list of these two lists.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the posterior probability of a voxel being metal and makes a binary\n    detection decision using a Naive Bayes classifier.\n    \"\"\"\n\n    # Class-conditional parameters and priors from the problem statement\n    params = {\n        'M': {  # Metal\n            'prior': 0.03,\n            'I': {'mu': 3500, 'sigma': 400},\n            'G': {'mu': 1200, 'sigma': 400},\n            'D': {'mu': 0.5, 'sigma': 0.5}\n        },\n        'N': {  # Non-metal\n            'prior': 0.97,\n            'I': {'mu': 300, 'sigma': 600},\n            'G': {'mu': 100, 'sigma': 100},\n            'D': {'mu': 12, 'sigma': 6}\n        }\n    }\n\n    # Test cases: (I, G, D)\n    test_cases = [\n        (4200.0, 1800.0, 0.2),\n        (30.0, 20.0, 25.0),\n        (1200.0, 200.0, 18.0),\n        (900.0, 700.0, 2.0),\n        (3500.0, 0.0, 5.0),\n        (500.0, 150.0, 0.1)\n    ]\n\n    features = ['I', 'G', 'D']\n    \n    def log_gaussian_pdf(x, mu, sigma):\n        \"\"\"\n        Calculates the logarithm of the Gaussian Probability Density Function.\n        \"\"\"\n        # Ensure sigma is not zero to avoid division errors\n        if sigma == 0:\n            return -np.inf\n        log_pdf = -np.log(sigma * np.sqrt(2 * np.pi)) - 0.5 * ((x - mu) / sigma) ** 2\n        return log_pdf\n\n    posterior_probabilities = []\n    detection_decisions = []\n\n    for case_values in test_cases:\n        case_dict = dict(zip(features, case_values))\n        \n        # Calculate scores S_M and S_N\n        log_prob_m = np.log(params['M']['prior'])\n        log_prob_n = np.log(params['N']['prior'])\n\n        for feat in features:\n            val = case_dict[feat]\n            \n            # Add log-likelihood for Metal class\n            mu_m = params['M'][feat]['mu']\n            sigma_m = params['M'][feat]['sigma']\n            log_prob_m += log_gaussian_pdf(val, mu_m, sigma_m)\n            \n            # Add log-likelihood for Non-metal class\n            mu_n = params['N'][feat]['mu']\n            sigma_n = params['N'][feat]['sigma']\n            log_prob_n += log_gaussian_pdf(val, mu_n, sigma_n)\n\n        # Calculate posterior probability for Metal class using the numerically stable formula\n        # P(M|x) = 1 / (1 + exp(S_N - S_M))\n        score_diff = log_prob_n - log_prob_m\n        if score_diff > 700: # Avoid overflow in np.exp\n            posterior_m = 0.0\n        else:\n            posterior_m = 1.0 / (1.0 + np.exp(score_diff))\n        \n        # Round the posterior probability to 6 decimal places\n        rounded_posterior = round(posterior_m, 6)\n        \n        # Apply threshold for detection decision\n        decision = 1 if rounded_posterior >= 0.5 else 0\n        \n        posterior_probabilities.append(rounded_posterior)\n        detection_decisions.append(decision)\n\n    # Format the final output string exactly as required\n    prob_str = ','.join([f\"{p:.6f}\" for p in posterior_probabilities])\n    dec_str = ','.join(map(str, detection_decisions))\n    \n    print(f\"[[{prob_str}],[{dec_str}]]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once the data corrupted by metal has been identified, the next challenge is to replace it with plausible values. This process, known as inpainting, is often performed on the raw sinogram data before image reconstruction. This exercise introduces a powerful and intuitive inpainting method based on directional interpolation. You will derive and apply a technique that fills in missing sinogram values by respecting the local orientation of the data, providing a hands-on look at how mathematical principles of smoothness can restore information lost to metal artifacts .",
            "id": "4900085",
            "problem": "A parallel-beam computed tomography (CT) system acquires line integrals of the linear attenuation coefficient via the Radon transform, producing a two-dimensional sinogram indexed by projection angle $\\theta$ and detector coordinate $t$. A small sinogram patch around a fixed location $(\\theta_{1}, t_{1})$ is corrupted by high-attenuation metal, leaving the center sample $s(\\theta_{1}, t_{1})$ unknown while its four axis-aligned neighbors $s(\\theta_{1}, t_{1}\\pm \\Delta t)$ and $s(\\theta_{1}\\pm \\Delta \\theta, t_{1})$ remain reliable. To reduce metal-induced artifacts, consider a directional interpolation that minimizes a local anisotropic smoothness functional constructed from sinogram gradients.\n\nUse the following foundations:\n\n- The sinogram encodes line integrals: $s(\\theta, t) = \\int \\mu(x, y) \\,\\mathrm{d}\\ell$, where $\\mu$ is the linear attenuation coefficient and $\\mathrm{d}\\ell$ integrates along the ray defined by $(\\theta, t)$.\n- Local smoothness can be enforced by penalizing squared directional differences along the axes $(\\theta, t)$ with weights determined by local sinogram gradients estimated from reliable neighbor data.\n\nLet the unknown center value be denoted by $x := s(\\theta_{1}, t_{1})$. Define discrete central-difference estimates of the local sinogram gradients at $(\\theta_{1}, t_{1})$ using only reliable samples:\n$$\ng_{t} \\approx \\frac{s(\\theta_{1}, t_{1}+\\Delta t) - s(\\theta_{1}, t_{1}-\\Delta t)}{2\\,\\Delta t},\\quad\ng_{\\theta} \\approx \\frac{s(\\theta_{1}+\\Delta \\theta, t_{1}) - s(\\theta_{1}-\\Delta \\theta, t_{1})}{2\\,\\Delta \\theta}.\n$$\nConstruct an anisotropic smoothness functional for the single unknown $x$,\n$$\n\\mathcal{J}(x) = w_{t}\\Big[(x - s(\\theta_{1}, t_{1}-\\Delta t))^{2} + (s(\\theta_{1}, t_{1}+\\Delta t) - x)^{2}\\Big] + w_{\\theta}\\Big[(x - s(\\theta_{1}-\\Delta \\theta, t_{1}))^{2} + (s(\\theta_{1}+\\Delta \\theta, t_{1}) - x)^{2}\\Big],\n$$\nwhere the weights are defined from the gradient magnitudes by\n$$\nw_{t} = \\frac{1}{1 + \\left(\\frac{|g_{t}|}{g_{0t}}\\right)^{2}},\\quad\nw_{\\theta} = \\frac{1}{1 + \\left(\\frac{|g_{\\theta}|}{g_{0\\theta}}\\right)^{2}},\n$$\nwith contrast parameters $g_{0t} > 0$ and $g_{0\\theta} > 0$.\n\nAssume the following reliable neighbor data and sampling steps:\n- $s(\\theta_{1}, t_{1}-\\Delta t) = 0.51$, $s(\\theta_{1}, t_{1}+\\Delta t) = 0.63$, $s(\\theta_{1}-\\Delta \\theta, t_{1}) = 0.52$, $s(\\theta_{1}+\\Delta \\theta, t_{1}) = 0.54$.\n- $\\Delta t = 1$, $\\Delta \\theta = 0.02$ (radians).\n- $g_{0t} = 0.05$, $g_{0\\theta} = 0.50$.\n\nStarting from the above foundational definitions and the smoothness functional, derive the expression for the minimizer $x^{\\star}$ that interpolates the missing sinogram value by minimizing $\\mathcal{J}(x)$ subject to the boundary constraints provided by the reliable neighbors. Then evaluate $x^{\\star}$ numerically from the given data. Round your final numerical answer to four significant figures.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\nThe given information is as follows:\n- An unknown sinogram value $x := s(\\theta_{1}, t_{1})$.\n- Reliable neighbor data:\n  - $s(\\theta_{1}, t_{1}-\\Delta t) = 0.51$\n  - $s(\\theta_{1}, t_{1}+\\Delta t) = 0.63$\n  - $s(\\theta_{1}-\\Delta \\theta, t_{1}) = 0.52$\n  - $s(\\theta_{1}+\\Delta \\theta, t_{1}) = 0.54$\n- Sampling intervals:\n  - $\\Delta t = 1$\n  - $\\Delta \\theta = 0.02$\n- Gradient definitions for central differences:\n  - $g_{t} \\approx \\frac{s(\\theta_{1}, t_{1}+\\Delta t) - s(\\theta_{1}, t_{1}-\\Delta t)}{2\\,\\Delta t}$\n  - $g_{\\theta} \\approx \\frac{s(\\theta_{1}+\\Delta \\theta, t_{1}) - s(\\theta_{1}-\\Delta \\theta, t_{1})}{2\\,\\Delta \\theta}$\n- Anisotropic smoothness functional to be minimized:\n  - $\\mathcal{J}(x) = w_{t}\\Big[(x - s(\\theta_{1}, t_{1}-\\Delta t))^{2} + (s(\\theta_{1}, t_{1}+\\Delta t) - x)^{2}\\Big] + w_{\\theta}\\Big[(x - s(\\theta_{1}-\\Delta \\theta, t_{1}))^{2} + (s(\\theta_{1}+\\Delta \\theta, t_{1}) - x)^{2}\\Big]$\n- Weight definitions:\n  - $w_{t} = \\frac{1}{1 + \\left(\\frac{|g_{t}|}{g_{0t}}\\right)^{2}}$\n  - $w_{\\theta} = \\frac{1}{1 + \\left(\\frac{|g_{\\theta}|}{g_{0\\theta}}\\right)^{2}}$\n- Contrast parameters:\n  - $g_{0t} = 0.05$\n  - $g_{0\\theta} = 0.50$\n- The final interpolated value should be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a sinogram inpainting task using a weighted-average interpolation scheme, where the weights are derived from an anisotropic smoothness functional. This approach is a standard technique in image processing and medical imaging, often related to anisotropic diffusion. The functional $\\mathcal{J}(x)$ is a quadratic function of the unknown $x$. The weights $w_{t}$ and $w_{\\theta}$ are derived from gradient magnitudes and are always positive. Therefore, $\\mathcal{J}(x)$ is a convex parabola opening upwards, which guarantees the existence of a unique minimum. The problem is self-contained, with all necessary data and definitions provided. There are no scientific or logical contradictions. The request for units of $\\mathrm{cm}^{-1}$ on a sinogram value (which is a line integral $\\int \\mu \\, \\mathrm{d}\\ell$ and thus should have units of $\\text{length}^{-1} \\times \\text{length} = \\text{dimensionless}$) is a minor terminological ambiguity but does not affect the mathematical validity of the optimization problem itself. The problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe objective is to find the value of $x$ that minimizes the functional $\\mathcal{J}(x)$. This value, denoted $x^{\\star}$, can be found by taking the derivative of $\\mathcal{J}(x)$ with respect to $x$ and setting it to zero.\n\nLet's simplify the notation for the neighbor values:\n- $s_{t-} = s(\\theta_{1}, t_{1}-\\Delta t)$\n- $s_{t+} = s(\\theta_{1}, t_{1}+\\Delta t)$\n- $s_{\\theta-} = s(\\theta_{1}-\\Delta \\theta, t_{1})$\n- $s_{\\theta+} = s(\\theta_{1}+\\Delta \\theta, t_{1})$\n\nThe functional is:\n$$\n\\mathcal{J}(x) = w_{t}\\Big[(x - s_{t-})^{2} + (s_{t+} - x)^{2}\\Big] + w_{\\theta}\\Big[(x - s_{\\theta-})^{2} + (s_{\\theta+} - x)^{2}\\Big]\n$$\nTo find the minimum, we compute the derivative $\\frac{\\mathrm{d}\\mathcal{J}}{\\mathrm{d}x}$:\n$$\n\\frac{\\mathrm{d}\\mathcal{J}}{\\mathrm{d}x} = w_{t}\\Big[2(x - s_{t-})(1) + 2(s_{t+} - x)(-1)\\Big] + w_{\\theta}\\Big[2(x - s_{\\theta-})(1) + 2(s_{\\theta+} - x)(-1)\\Big]\n$$\n$$\n\\frac{\\mathrm{d}\\mathcal{J}}{\\mathrm{d}x} = 2w_{t}(x - s_{t-} - s_{t+} + x) + 2w_{\\theta}(x - s_{\\theta-} - s_{\\theta+} + x)\n$$\n$$\n\\frac{\\mathrm{d}\\mathcal{J}}{\\mathrm{d}x} = 2w_{t}\\Big(2x - (s_{t-} + s_{t+})\\Big) + 2w_{\\theta}\\Big(2x - (s_{\\theta-} + s_{\\theta+})\\Big)\n$$\nSetting the derivative to zero to find the critical point $x^{\\star}$:\n$$\n2w_{t}\\Big(2x^{\\star} - (s_{t-} + s_{t+})\\Big) + 2w_{\\theta}\\Big(2x^{\\star} - (s_{\\theta-} + s_{\\theta+})\\Big) = 0\n$$\nDividing by $2$:\n$$\nw_{t}\\Big(2x^{\\star} - (s_{t-} + s_{t+})\\Big) + w_{\\theta}\\Big(2x^{\\star} - (s_{\\theta-} + s_{\\theta+})\\Big) = 0\n$$\nNow, we solve for $x^{\\star}$:\n$$\n2x^{\\star}w_{t} - w_{t}(s_{t-} + s_{t+}) + 2x^{\\star}w_{\\theta} - w_{\\theta}(s_{\\theta-} + s_{\\theta+}) = 0\n$$\n$$\n2x^{\\star}(w_{t} + w_{\\theta}) = w_{t}(s_{t-} + s_{t+}) + w_{\\theta}(s_{\\theta-} + s_{\\theta+})\n$$\n$$\nx^{\\star} = \\frac{w_{t}(s_{t-} + s_{t+}) + w_{\\theta}(s_{\\theta-} + s_{\\theta+})}{2(w_{t} + w_{\\theta})}\n$$\nThis expression gives the optimal interpolated value $x^{\\star}$. It can be interpreted as a weighted average of the mean values of the neighbors along each axis.\n\nNow, we will compute the numerical value of $x^{\\star}$ using the given data.\n\nFirst, calculate the local gradients $g_{t}$ and $g_{\\theta}$:\n$$\ng_{t} = \\frac{s_{t+} - s_{t-}}{2\\Delta t} = \\frac{0.63 - 0.51}{2(1)} = \\frac{0.12}{2} = 0.06\n$$\n$$\ng_{\\theta} = \\frac{s_{\\theta+} - s_{\\theta-}}{2\\Delta \\theta} = \\frac{0.54 - 0.52}{2(0.02)} = \\frac{0.02}{0.04} = 0.5\n$$\nNext, calculate the weights $w_{t}$ and $w_{\\theta}$:\n$$\nw_{t} = \\frac{1}{1 + \\left(\\frac{|g_{t}|}{g_{0t}}\\right)^{2}} = \\frac{1}{1 + \\left(\\frac{0.06}{0.05}\\right)^{2}} = \\frac{1}{1 + (1.2)^{2}} = \\frac{1}{1 + 1.44} = \\frac{1}{2.44}\n$$\n$$\nw_{\\theta} = \\frac{1}{1 + \\left(\\frac{|g_{\\theta}|}{g_{0\\theta}}\\right)^{2}} = \\frac{1}{1 + \\left(\\frac{0.5}{0.50}\\right)^{2}} = \\frac{1}{1 + 1^{2}} = \\frac{1}{2}\n$$\nNow, substitute the weights and neighbor values into the expression for $x^{\\star}$:\n- Sum of neighbors along $t$: $s_{t-} + s_{t+} = 0.51 + 0.63 = 1.14$\n- Sum of neighbors along $\\theta$: $s_{\\theta-} + s_{\\theta+} = 0.52 + 0.54 = 1.06$\n\n$$\nx^{\\star} = \\frac{w_{t}(1.14) + w_{\\theta}(1.06)}{2(w_{t} + w_{\\theta})}\n$$\nSubstitute the values of the weights:\n$$\nx^{\\star} = \\frac{(\\frac{1}{2.44})(1.14) + (\\frac{1}{2})(1.06)}{2(\\frac{1}{2.44} + \\frac{1}{2})}\n$$\nCalculate the numerator:\n$$\n\\text{Numerator} = \\frac{1.14}{2.44} + \\frac{1.06}{2} = \\frac{1.14}{2.44} + 0.53\n$$\nCalculate the denominator:\n$$\n\\text{Denominator} = 2\\left(\\frac{1}{2.44} + \\frac{1}{2}\\right) = 2\\left(\\frac{1 + 1.22}{2.44}\\right) = 2\\left(\\frac{2.22}{2.44}\\right) = \\frac{4.44}{2.44}\n$$\nSo, we have:\n$$\nx^{\\star} = \\frac{\\frac{1.14}{2.44} + 0.53}{\\frac{4.44}{2.44}} = \\frac{1.14 + 0.53(2.44)}{4.44} = \\frac{1.14 + 1.2932}{4.44} = \\frac{2.4332}{4.44}\n$$\n$$\nx^{\\star} \\approx 0.547995495...\n$$\nThe problem requires the final answer to be rounded to four significant figures.\n$$\nx^{\\star} \\approx 0.5480\n$$\nAlthough the problem requests the unit of the interpolated value to be expressed in $\\mathrm{cm}^{-1}$, the calculation is based on dimensionless numerical inputs. The final result is a dimensionless number. Following the output format rules, units are not included in the boxed answer. The numerical value represents the interpolated sinogram data point, which is physically related to the line integral of linear attenuation coefficients.",
            "answer": "$$\\boxed{0.5480}$$"
        },
        {
            "introduction": "Metal artifacts are not caused solely by the physics of X-ray attenuation through dense materials; the characteristics of the CT scanner hardware also play a significant role. This practice explores a subtle but important source of error: detector afterglow, where a detector element continues to emit a signal briefly after being exposed to intense radiation. By modeling this phenomenon as a linear system, you will quantify how afterglow from a metal-traversing ray can 'leak' into adjacent measurements, creating characteristic streak artifacts . This exercise highlights the importance of considering the entire imaging chain when diagnosing and correcting artifacts.",
            "id": "4900088",
            "problem": "In X-ray Computed Tomography (CT), the ideal transmitted intensity at view index $n$ along a fixed detector channel is modeled by the Beer–Lambert law as $I[n] = I_{0} \\exp\\!\\left(-p[n]\\right)$, where $I_{0}$ is the incident intensity and $p[n]$ is the line integral of the linear attenuation coefficient along the corresponding ray at view $n$. Consider a detector exhibiting afterglow that can be modeled as a causal Linear Time-Invariant (LTI) system in the intensity domain with a normalized, causal impulse response $h[k]$ satisfying $\\sum_{k=0}^{\\infty} h[k] = 1$. The measured intensity is then $\\tilde{I}[n] = \\sum_{k=0}^{\\infty} h[k]\\, I[n-k]$. The system reports logarithmic measurements $m[n] = -\\ln\\!\\left(\\tilde{I}[n]/I_{0}\\right)$.\n\nAssume a uniform background object such that the true line integrals are $p[n] = p_{0}$ for all $n \\in \\mathbb{Z}$, except for a single-view metal trace at $n=0$ that adds an increment $\\Delta p > 0$: specifically, $p[n] = p_{0} + \\Delta p\\, \\delta[n]$, where $\\delta[n]$ is the discrete-time Kronecker delta. Let the afterglow impulse response be the normalized exponential kernel $h[k] = (1-\\rho)\\,\\rho^{k}$ for $k \\in \\{0,1,2,\\dots\\}$ with $0<\\rho<1$.\n\nDerive a closed-form expression for the induced bias in the logged line integral at the first adjacent view, defined as $b \\triangleq m[1] - p_{0}$, in terms of $\\rho$ and $\\Delta p$. Express your final answer in line-integral units. Provide your answer as a single, closed-form analytic expression. No numerical rounding is required.",
            "solution": "The problem asks for a closed-form expression for the bias, $b$, in the logged line integral at the first adjacent view ($n=1$) following a high-attenuation metal trace at view $n=0$. The bias is defined as $b \\triangleq m[1] - p_{0}$, where $m[1]$ is the measured line integral at view $n=1$ and $p_{0}$ is the true background line integral.\n\nFirst, we write down the definition of the measured line integral, $m[n]$, provided in the problem statement:\n$$m[n] = -\\ln\\left(\\frac{\\tilde{I}[n]}{I_{0}}\\right)$$\nWe are interested in the measurement at view $n=1$, so we have:\n$$m[1] = -\\ln\\left(\\frac{\\tilde{I}[1]}{I_{0}}\\right)$$\nTo find $m[1]$, we must first determine the measured intensity, $\\tilde{I}[1]$. The problem states that the measured intensity is the result of a convolution of the ideal intensity, $I[n]$, with the detector's afterglow impulse response, $h[k]$:\n$$\\tilde{I}[n] = \\sum_{k=0}^{\\infty} h[k] I[n-k]$$\nFor the specific view $n=1$, this becomes:\n$$\\tilde{I}[1] = \\sum_{k=0}^{\\infty} h[k] I[1-k] = h[0]I[1] + h[1]I[0] + h[2]I[-1] + \\dots$$\nNext, we need the ideal intensity values, $I[n]$. These are given by the Beer-Lambert law:\n$$I[n] = I_{0} \\exp(-p[n])$$\nThe line integrals, $p[n]$, are given for a uniform background with a single metal trace at $n=0$:\n$$p[n] = p_{0} + \\Delta p\\, \\delta[n]$$\nwhere $\\delta[n]$ is the Kronecker delta. Let's evaluate $p[n]$ at the required indices:\n- For $n=0$: $p[0] = p_{0} + \\Delta p\\, \\delta[0] = p_{0} + \\Delta p$.\n- For any $n \\neq 0$: $p[n] = p_{0} + \\Delta p\\, \\delta[n] = p_{0}$.\n\nUsing these line integrals, we find the corresponding ideal intensities:\n- For $n=0$: $I[0] = I_{0} \\exp(-p[0]) = I_{0} \\exp(-(p_{0} + \\Delta p))$.\n- For any $n \\neq 0$ (e.g., $n=1, -1, -2, \\dots$): $I[n] = I_{0} \\exp(-p[n]) = I_{0} \\exp(-p_{0})$.\n\nNow we can substitute these intensity values back into the expression for $\\tilde{I}[1]$. The arguments of $I$ in the sum are $1-k$. The argument is $0$ only when $k=1$. For all other values of $k \\ge 0$, the argument $1-k$ is non-zero.\n- For $k=1$: $I[1-1] = I[0] = I_{0} \\exp(-(p_{0} + \\Delta p))$.\n- For $k \\neq 1$: $I[1-k] = I_{0} \\exp(-p_{0})$.\n\nLet's rewrite the sum for $\\tilde{I}[1]$ by separating the term for $k=1$:\n$$\\tilde{I}[1] = h[1]I[0] + \\sum_{k=0, k \\neq 1}^{\\infty} h[k] I[1-k]$$\nSubstituting the intensity expressions:\n$$\\tilde{I}[1] = h[1] \\left(I_{0} \\exp(-(p_{0} + \\Delta p))\\right) + \\sum_{k=0, k \\neq 1}^{\\infty} h[k] \\left(I_{0} \\exp(-p_{0})\\right)$$\nWe can factor out the common term $I_{0} \\exp(-p_{0})$:\n$$\\tilde{I}[1] = I_{0} \\exp(-p_{0}) \\left( h[1] \\frac{\\exp(-(p_{0} + \\Delta p))}{\\exp(-p_{0})} + \\sum_{k=0, k \\neq 1}^{\\infty} h[k] \\right)$$\n$$\\tilde{I}[1] = I_{0} \\exp(-p_{0}) \\left( h[1] \\exp(-\\Delta p) + \\sum_{k=0, k \\neq 1}^{\\infty} h[k] \\right)$$\nThe problem states that the impulse response is normalized, i.e., $\\sum_{k=0}^{\\infty} h[k] = 1$. We can write this sum as $\\sum_{k=0, k \\neq 1}^{\\infty} h[k] + h[1] = 1$, which implies $\\sum_{k=0, k \\neq 1}^{\\infty} h[k] = 1 - h[1]$. Substituting this into the expression for $\\tilde{I}[1]$:\n$$\\tilde{I}[1] = I_{0} \\exp(-p_{0}) \\left( h[1] \\exp(-\\Delta p) + (1 - h[1]) \\right)$$\n$$\\tilde{I}[1] = I_{0} \\exp(-p_{0}) \\left( 1 + h[1] (\\exp(-\\Delta p) - 1) \\right)$$\nNow we substitute the specific a priori known form of the impulse response, $h[k] = (1-\\rho)\\rho^{k}$. For $k=1$, this gives:\n$$h[1] = (1-\\rho)\\rho^{1} = \\rho(1-\\rho)$$\nSubstituting this into the expression for $\\tilde{I}[1]$:\n$$\\tilde{I}[1] = I_{0} \\exp(-p_{0}) \\left( 1 + \\rho(1-\\rho) (\\exp(-\\Delta p) - 1) \\right)$$\nWith $\\tilde{I}[1]$ determined, we can now calculate the measured line integral $m[1]$:\n$$m[1] = -\\ln\\left(\\frac{\\tilde{I}[1]}{I_{0}}\\right) = -\\ln\\left(\\frac{I_{0} \\exp(-p_{0}) \\left( 1 + \\rho(1-\\rho) (\\exp(-\\Delta p) - 1) \\right)}{I_{0}}\\right)$$\n$$m[1] = -\\ln\\left(\\exp(-p_{0}) \\left( 1 + \\rho(1-\\rho) (\\exp(-\\Delta p) - 1) \\right)\\right)$$\nUsing the logarithm property $\\ln(ab) = \\ln(a) + \\ln(b)$:\n$$m[1] = -\\left[ \\ln(\\exp(-p_{0})) + \\ln\\left( 1 + \\rho(1-\\rho) (\\exp(-\\Delta p) - 1) \\right) \\right]$$\n$$m[1] = -\\left[ -p_{0} + \\ln\\left( 1 + \\rho(1-\\rho) (\\exp(-\\Delta p) - 1) \\right) \\right]$$\n$$m[1] = p_{0} - \\ln\\left( 1 + \\rho(1-\\rho) (\\exp(-\\Delta p) - 1) \\right)$$\nFinally, we compute the bias $b$ as defined:\n$$b = m[1] - p_{0}$$\n$$b = \\left( p_{0} - \\ln\\left( 1 + \\rho(1-\\rho) (\\exp(-\\Delta p) - 1) \\right) \\right) - p_{0}$$\n$$b = - \\ln\\left( 1 + \\rho(1-\\rho) (\\exp(-\\Delta p) - 1) \\right)$$\nThis provides the final closed-form expression for the bias in terms of $\\rho$ and $\\Delta p$.",
            "answer": "$$\\boxed{-\\ln\\left(1 + \\rho(1-\\rho)\\left(\\exp(-\\Delta p) - 1\\right)\\right)}$$"
        }
    ]
}