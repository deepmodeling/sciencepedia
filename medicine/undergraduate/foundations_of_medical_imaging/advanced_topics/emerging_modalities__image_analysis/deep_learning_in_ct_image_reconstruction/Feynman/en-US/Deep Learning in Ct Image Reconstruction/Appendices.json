{
    "hands_on_practices": [
        {
            "introduction": "To build robust models for low-dose CT, we must account for the physical properties of the acquisition, including the noise statistics. This exercise guides you through the fundamental task of deriving the gradient for the Poisson negative log-likelihood loss, which is more physically accurate than a standard squared-error loss . This calculation is at the heart of training statistically-aware reconstruction networks.",
            "id": "4875558",
            "problem": "Consider a transmission Computed Tomography (CT) system in which the detector photon counts for each ray bin are modeled as independent Poisson random variables. For a given ray bin indexed by $i$, the observed raw count is $y_i \\in \\mathbb{N}$ and the corresponding expected photon count is $\\lambda_i  0$. A deep neural network is trained directly on the raw counts $y_i$ to predict the expected count for each bin. To guarantee positivity, the network outputs an unconstrained real number $s_i \\in \\mathbb{R}$ and the predicted expected count is defined by $\\hat{\\lambda}_i = \\exp(s_i)$. Assume the measurements are independent across bins and the training objective is the negative log-likelihood (NLL) under the Poisson model, summed over bins, ignoring additive constants that do not depend on the network outputs.\n\nStarting from the Poisson probability mass function $p(y_i \\mid \\lambda_i) = \\exp(-\\lambda_i)\\,\\lambda_i^{y_i}/y_i!$ and the definition of the negative log-likelihood, derive the gradient of the total NLL with respect to the network output $s_i$ for an arbitrary bin $i$, expressed only in terms of $s_i$ and $y_i$. Provide your final gradient as a single closed-form analytic expression. No numerical approximation is required, and no units are needed.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It is based on standard statistical modeling (Poisson distribution for photon counting), common practices in machine learning for medical imaging (negative log-likelihood loss), and a standard method for ensuring parameter positivity (exponential link function). All necessary information is provided to derive the requested gradient.\n\nThe objective is to derive the gradient of the total negative log-likelihood (NLL) with respect to the unconstrained network output $s_i$ for a single ray bin $i$. The final expression should be in terms of $s_i$ and the observed count $y_i$.\n\nLet the total NLL be denoted by $L$. The problem states that this is the sum of the NLLs for each bin, indexed by $j$.\n$$\nL = \\sum_j L_j\n$$\nwhere $L_j$ is the NLL for bin $j$.\n\nThe gradient of the total NLL with respect to the network output $s_i$ for a specific bin $i$ is given by the partial derivative $\\frac{\\partial L}{\\partial s_i}$. Since the terms $L_j$ for $j \\neq i$ do not depend on $s_i$ (due to the independence of measurements across bins), their derivatives with respect to $s_i$ are zero.\n$$\n\\frac{\\partial L}{\\partial s_i} = \\frac{\\partial}{\\partial s_i} \\left( \\sum_j L_j \\right) = \\sum_j \\frac{\\partial L_j}{\\partial s_i} = \\frac{\\partial L_i}{\\partial s_i}\n$$\nThus, we only need to compute the derivative of the NLL for bin $i$.\n\nThe likelihood of observing a count $y_i$ for a given predicted mean $\\hat{\\lambda}_i$ is governed by the Poisson probability mass function (PMF):\n$$\nP(y_i \\mid \\hat{\\lambda}_i) = \\frac{\\exp(-\\hat{\\lambda}_i) \\hat{\\lambda}_i^{y_i}}{y_i!}\n$$\nThe log-likelihood, $\\ell_i$, is the natural logarithm of the PMF:\n$$\n\\ell_i(\\hat{\\lambda}_i) = \\ln \\left( \\frac{\\exp(-\\hat{\\lambda}_i) \\hat{\\lambda}_i^{y_i}}{y_i!} \\right) = \\ln(\\exp(-\\hat{\\lambda}_i)) + \\ln(\\hat{\\lambda}_i^{y_i}) - \\ln(y_i!)\n$$\n$$\n\\ell_i(\\hat{\\lambda}_i) = -\\hat{\\lambda}_i + y_i \\ln(\\hat{\\lambda}_i) - \\ln(y_i!)\n$$\nThe negative log-likelihood, $L_i$, is simply $-\\ell_i$:\n$$\nL_i(\\hat{\\lambda}_i) = \\hat{\\lambda}_i - y_i \\ln(\\hat{\\lambda}_i) + \\ln(y_i!)\n$$\nThe term $\\ln(y_i!)$ is a constant with respect to the network's parameters, as $y_i$ is a fixed observation. We can disregard it for optimization, but we will retain it here for completeness; it will vanish upon differentiation with respect to any network parameter.\n\nTo find the gradient of $L_i$ with respect to $s_i$, we must apply the chain rule, as $L_i$ depends on $\\hat{\\lambda}_i$, which in turn depends on $s_i$:\n$$\n\\frac{\\partial L_i}{\\partial s_i} = \\frac{\\partial L_i}{\\partial \\hat{\\lambda}_i} \\frac{\\partial \\hat{\\lambda}_i}{\\partial s_i}\n$$\nFirst, we compute the derivative of $L_i$ with respect to $\\hat{\\lambda}_i$:\n$$\n\\frac{\\partial L_i}{\\partial \\hat{\\lambda}_i} = \\frac{\\partial}{\\partial \\hat{\\lambda}_i} \\left( \\hat{\\lambda}_i - y_i \\ln(\\hat{\\lambda}_i) + \\ln(y_i!) \\right) = 1 - y_i \\frac{1}{\\hat{\\lambda}_i} + 0 = 1 - \\frac{y_i}{\\hat{\\lambda}_i}\n$$\nNext, we compute the derivative of $\\hat{\\lambda}_i$ with respect to $s_i$. The problem defines the relationship as $\\hat{\\lambda}_i = \\exp(s_i)$.\n$$\n\\frac{\\partial \\hat{\\lambda}_i}{\\partial s_i} = \\frac{\\partial}{\\partial s_i} (\\exp(s_i)) = \\exp(s_i)\n$$\nBy the definition provided, we also have $\\exp(s_i) = \\hat{\\lambda}_i$. Thus:\n$$\n\\frac{\\partial \\hat{\\lambda}_i}{\\partial s_i} = \\hat{\\lambda}_i\n$$\nNow, we substitute these two components back into the chain rule expression:\n$$\n\\frac{\\partial L_i}{\\partial s_i} = \\left( 1 - \\frac{y_i}{\\hat{\\lambda}_i} \\right) \\cdot \\hat{\\lambda}_i = \\hat{\\lambda}_i - y_i\n$$\nThe final step is to express this gradient solely in terms of the network output $s_i$ and the observed data $y_i$. We substitute $\\hat{\\lambda}_i = \\exp(s_i)$ into the expression for the gradient.\n$$\n\\frac{\\partial L}{\\partial s_i} = \\frac{\\partial L_i}{\\partial s_i} = \\exp(s_i) - y_i\n$$\nThis is the final closed-form analytic expression for the gradient of the total NLL with respect to the network output $s_i$ for an arbitrary bin $i$.",
            "answer": "$$\n\\boxed{\\exp(s_i) - y_i}\n$$"
        },
        {
            "introduction": "Deep learning frameworks automate gradient calculation, but they rely on the assumption that the operators within the model are implemented correctly. This exercise explores a critical pitfall: the use of a backprojector that is not the true mathematical adjoint of the forward projection operator . Understanding this issue is vital for ensuring that the network is optimizing the intended physical data-fidelity objective.",
            "id": "4875608",
            "problem": "In discrete Computed Tomography (CT), the forward model that maps an image vector $x \\in \\mathbb{R}^{n}$ to projection data $y \\in \\mathbb{R}^{m}$ is commonly modeled as a linear operator $A \\in \\mathbb{R}^{m \\times n}$ so that $y \\approx A x$. Consider an end-to-end trained reconstruction architecture that includes a data-fidelity loss $\\mathcal{L}(x) = \\tfrac{1}{2}\\|A x - y\\|_{2}^{2}$ and uses a backprojector $B \\in \\mathbb{R}^{n \\times m}$ to propagate residuals $(A x - y)$ into image space during training. In many practical toolchains, the implemented projector and backprojector are not exact adjoints of each other under the standard Euclidean inner product, for example when using a Siddon ray-driven projector for $A$ together with a voxel-driven backprojector for $B$. Starting from the definition of an adjoint operator in a Hilbert space and the definition of a gradient as the Riesz representative of the Fréchet derivative under a chosen inner product, reason about the implications of $B \\neq A^{\\top}$ for the accuracy of the gradient used in end-to-end training and for the resulting optimization dynamics.\n\nSelect all statements that are correct.\n\nA. Under the standard Euclidean inner product, the true gradient of $\\mathcal{L}(x)$ is $A^{\\top}(A x - y)$. If $\\tilde{g}(x) = B(A x - y)$ with $B \\neq A^{\\top}$, then $\\tilde{g}(x)$ is generally a biased approximation that need not be a descent direction for $\\mathcal{L}$ at $x$. Using $\\tilde{g}(x)$ can therefore cause end-to-end training to optimize a different objective than $\\mathcal{L}$.\n\nB. For any linear $B$, there always exists a symmetric positive definite (SPD) matrix $M \\in \\mathbb{R}^{n \\times n}$ such that $\\tilde{g}(x)$ equals the gradient of $\\mathcal{L}$ with respect to the $M$-weighted inner product for all residuals $(A x - y)$, implying equivalence to preconditioned gradient descent on $\\mathcal{L}$.\n\nC. A practical numerical test for adjoint correctness is to draw random $x \\in \\mathbb{R}^{n}$ and $z \\in \\mathbb{R}^{m}$ and check whether $\\langle A x, z \\rangle = \\langle x, B z \\rangle$ under the Euclidean inner product; systematic deviation indicates an implementation mismatch that will corrupt the gradient.\n\nD. Because the residual $(A x - y)$ fluctuates during training, the angle between $\\tilde{g}(x)$ and the true gradient averages to zero over iterations, making the use of $B \\neq A^{\\top}$ effectively unbiased in expectation and harmless to convergence.\n\nE. Reducing the learning rate to be sufficiently small guarantees convergence to the minimizer of $\\mathcal{L}$ even when $B \\neq A^{\\top}$, provided $A$ has full column rank.\n\nF. In an end-to-end network where the reconstruction module contains $A$ and $B$ internally and the loss is computed in the image domain, the learned parameters may partially compensate for $B \\neq A^{\\top}$, but the training signal corresponds to a surrogate objective induced by $B$, not the original physical data fidelity.",
            "solution": "The problem statement asks for an analysis of the implications of using a backprojector matrix $B$ that is not the exact adjoint of the projector matrix $A$ (i.e., $B \\neq A^{\\top}$) when training an end-to-end reconstruction network. The data-fidelity loss is given as $\\mathcal{L}(x) = \\tfrac{1}{2}\\|A x - y\\|_{2}^{2}$.\n\n### Step 1: Extract Givens\n- Image vector: $x \\in \\mathbb{R}^{n}$\n- Projection data: $y \\in \\mathbb{R}^{m}$\n- Forward model (projector): $A \\in \\mathbb{R}^{m \\times n}$\n- Backprojector: $B \\in \\mathbb{R}^{n \\times m}$\n- Data-fidelity loss function: $\\mathcal{L}(x) = \\tfrac{1}{2}\\|A x - y\\|_{2}^{2}$\n- Condition of interest: The implemented backprojector $B$ is not the exact adjoint of the projector $A$ under the standard Euclidean inner product, i.e., $B \\neq A^{\\top}$.\n- Implemented gradient approximation: $\\tilde{g}(x) = B(A x - y)$ is used to propagate residuals during training.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the principles of computational medical imaging and numerical optimization. The linear model $y = Ax$ is standard for CT. The loss function is a standard least-squares data fidelity term. The distinction between a mathematical adjoint ($A^\\top$) and a computationally implemented backprojector ($B$) that may not be a perfect adjoint (e.g., Siddon projector vs. voxel-driven backprojector) is a known and practical issue in the field. The question about the impact on gradient-based optimization is highly relevant.\n- **Well-Posed**: The problem is well-posed. It sets up a clear mathematical scenario ($B \\neq A^\\top$) and asks for its consequences on optimization dynamics based on fundamental principles of linear algebra and calculus.\n- **Objective**: The problem is stated in precise, objective mathematical language.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with the solution.\n\n### Principle-Based Derivation of the True Gradient\nThe gradient of a functional is defined in relation to its Fréchet derivative and the chosen inner product. We are in the Hilbert space $\\mathbb{R}^n$ with the standard Euclidean inner product $\\langle u, v \\rangle = u^{\\top}v$. The loss function is $\\mathcal{L}(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} = \\frac{1}{2}\\langle A x - y, A x - y \\rangle$.\n\nTo find the gradient, we first compute the Fréchet derivative, $D\\mathcal{L}(x)[h]$, which is the linear part of the change in $\\mathcal{L}$ for a small perturbation $h \\in \\mathbb{R}^n$.\n$$\n\\mathcal{L}(x+h) = \\frac{1}{2}\\langle A(x+h) - y, A(x+h) - y \\rangle \\\\\n= \\frac{1}{2}\\langle (A x - y) + Ah, (A x - y) + Ah \\rangle \\\\\n= \\frac{1}{2} \\left( \\langle A x - y, A x - y \\rangle + 2\\langle A x - y, Ah \\rangle + \\langle Ah, Ah \\rangle \\right) \\\\\n= \\mathcal{L}(x) + \\langle A x - y, Ah \\rangle + O(\\|h\\|^2)\n$$\nBy the definition of the adjoint operator $A^{\\top}$, we have $\\langle A x - y, Ah \\rangle = \\langle A^{\\top}(A x - y), h \\rangle$.\nSo, the Fréchet derivative is $D\\mathcal{L}(x)[h] = \\langle A^{\\top}(A x - y), h \\rangle$.\n\nThe gradient, $\\nabla \\mathcal{L}(x)$, is defined as the Riesz representative of the Fréchet derivative, which is the unique vector satisfying $D\\mathcal{L}(x)[h] = \\langle \\nabla \\mathcal{L}(x), h \\rangle$ for all $h$.\nBy direct comparison, the true gradient of $\\mathcal{L}(x)$ with respect to the Euclidean inner product is:\n$$\n\\nabla \\mathcal{L}(x) = A^{\\top}(A x - y)\n$$\nThe problem states that an approximation $\\tilde{g}(x) = B(A x - y)$ is used instead, where $B \\neq A^{\\top}$. This means the optimization algorithm (e.g., gradient descent) uses an incorrect update direction.\n\n### Option-by-Option Analysis\n\n**A. Under the standard Euclidean inner product, the true gradient of $\\mathcal{L}(x)$ is $A^{\\top}(A x - y)$. If $\\tilde{g}(x) = B(A x - y)$ with $B \\neq A^{\\top}$, then $\\tilde{g}(x)$ is generally a biased approximation that need not be a descent direction for $\\mathcal{L}$ at $x$. Using $\\tilde{g}(x)$ can therefore cause end-to-end training to optimize a different objective than $\\mathcal{L}$.**\n\n- The derivation above confirms that the true gradient is indeed $\\nabla \\mathcal{L}(x) = A^{\\top}(A x - y)$.\n- Since $B \\neq A^{\\top}$, the provided \"gradient\" $\\tilde{g}(x)$ differs systematically from the true gradient. This is a form of bias (a systematic error, not statistical).\n- A direction $d$ is a descent direction for $\\mathcal{L}$ at $x$ if $\\langle \\nabla \\mathcal{L}(x), -d \\rangle  0$, which for a gradient descent step using $\\tilde{g}(x)$ means we need $\\langle \\nabla \\mathcal{L}(x), \\tilde{g}(x) \\rangle  0$. Let $r = A x - y$. This condition is $\\langle A^{\\top}r, B r \\rangle = (A^{\\top}r)^{\\top}(Br) = r^{\\top}A B r  0$. There is no general guarantee that the matrix $A B$ is positive definite, so it is not guaranteed that $r^{\\top}A B r  0$ for all non-zero residuals $r$. Thus, $\\tilde{g}(x)$ may not be a descent direction.\n- An optimization algorithm using updates based on $\\tilde{g}(x)$ will converge (if it does) to a point $x^*$ where $\\tilde{g}(x^*) = B(A x^* - y) = 0$. In contrast, the true minimizer of $\\mathcal{L}(x)$ satisfies the normal equations $\\nabla \\mathcal{L}(x) = A^{\\top}(A x^* - y) = 0$. Since $B \\neq A^{\\top}$, these two conditions define different solutions. Therefore, the training process is effectively optimizing for a different goal than minimizing $\\mathcal{L}(x)$.\n\n**Verdict: Correct.**\n\n**B. For any linear $B$, there always exists a symmetric positive definite (SPD) matrix $M \\in \\mathbb{R}^{n \\times n}$ such that $\\tilde{g}(x)$ equals the gradient of $\\mathcal{L}$ with respect to the $M$-weighted inner product for all residuals $(A x - y)$, implying equivalence to preconditioned gradient descent on $\\mathcal{L}$.**\n\n- The gradient of $\\mathcal{L}(x)$ with respect to an inner product defined by an SPD matrix $M$, $\\langle u, v \\rangle_M = u^\\top M v$, is given by $\\nabla_M \\mathcal{L}(x) = M^{-1} \\nabla \\mathcal{L}(x) = M^{-1} A^{\\top}(A x - y)$. This is the update direction in preconditioned gradient descent with preconditioner $M$.\n- The statement asserts that for any $B$, there exists an SPD $M$ such that $\\tilde{g}(x) = \\nabla_M \\mathcal{L}(x)$. This means we must have $B(A x - y) = M^{-1} A^{\\top}(A x - y)$ for all possible residuals $r = Ax-y$.\n- This implies $B = M^{-1} A^{\\top}$, or $M = A^{\\top} B^{-1}$.\n- This equation has several problems. First, the matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times m}$ are typically not square. If $n \\neq m$, $B$ is not invertible, so $B^{-1}$ is not defined. Even if $n = m$, $B$ may be singular. Furthermore, even if $B$ were invertible, there is no guarantee that the resulting matrix $M = A^{\\top} B^{-1}$ would be symmetric and positive definite. Thus, the statement is false.\n\n**Verdict: Incorrect.**\n\n**C. A practical numerical test for adjoint correctness is to draw random $x \\in \\mathbb{R}^{n}$ and $z \\in \\mathbb{R}^{m}$ and check whether $\\langle A x, z \\rangle = \\langle x, B z \\rangle$ under the Euclidean inner product; systematic deviation indicates an implementation mismatch that will corrupt the gradient.**\n\n- By definition, the adjoint operator $A^{\\top}$ of a linear operator $A$ between real inner product spaces is the unique operator satisfying $\\langle Ax, z \\rangle = \\langle x, A^{\\top}z \\rangle$ for all vectors $x$ and $z$ in the respective spaces.\n- The statement suggests to test if $B$ is the adjoint of $A$ by checking if $\\langle Ax, z \\rangle = \\langle x, B z \\rangle$ for random vectors. This is the standard numerical verification method known as the \"dot-product test\".\n- A systematic failure of this test (i.e., deviation beyond floating-point precision) proves that $B \\neq A^{\\top}$.\n- As established, if $B \\neq A^{\\top}$, the implemented gradient $\\tilde{g}(x)$ is not the true gradient $\\nabla \\mathcal{L}(x)$. Using an incorrect gradient for optimization certainly qualifies as \"corrupting\" the gradient.\n\n**Verdict: Correct.**\n\n**D. Because the residual $(A x - y)$ fluctuates during training, the angle between $\\tilde{g}(x)$ and the true gradient averages to zero over iterations, making the use of $B \\neq A^{\\top}$ effectively unbiased in expectation and harmless to convergence.**\n\n- The difference between the implemented and true gradients is an error term $\\epsilon(x) = \\tilde{g}(x) - \\nabla\\mathcal{L}(x) = (B - A^{\\top})(A x - y)$.\n- The operator $(B - A^{\\top})$ is a fixed, non-zero matrix. The residual $(A x - y)$ is a deterministic function of the current iterate $x$, not a zero-mean random variable. Its \"fluctuations\" are part of the structured path taken by the optimization, not random noise.\n- There is no mathematical principle that would cause the error vector $\\epsilon(x)$ or the angle between $\\tilde{g}(x)$ and $\\nabla\\mathcal{L}(x)$ to average to zero over the optimization trajectory. The error is systematic and depends on the specific structure of the mismatch $(B - A^{\\top})$ and the content of the image $x$.\n- The claim that this is \"unbiased in expectation\" wrongly applies a concept from stochastic gradient descent (where mini-batch noise is unbiased) to a deterministic, systematic error. The claim that it is \"harmless to convergence\" is false, as it can prevent convergence or cause convergence to the wrong solution.\n\n**Verdict: Incorrect.**\n\n**E. Reducing the learning rate to be sufficiently small guarantees convergence to the minimizer of $\\mathcal{L}$ even when $B \\neq A^{\\top}$, provided $A$ has full column rank.**\n\n- The optimization update is $x_{k+1} = x_k - \\eta \\tilde{g}(x_k) = x_k - \\eta B(Ax_k - y)$.\n- If this iteration converges, it converges to a fixed point $x^*$ where $x^* = x^* - \\eta B(Ax^* - y)$, which implies $B(Ax^* - y) = 0$.\n- The minimizer of $\\mathcal{L}(x)$, let's call it $x_{LS}$, satisfies the normal equations $A^{\\top}(A x_{LS} - y) = 0$. (The condition that $A$ has full column rank ensures this minimizer is unique).\n- Since $B \\neq A^{\\top}$, the condition $B(Ax - y) = 0$ is generally not equivalent to $A^{\\top}(Ax - y) = 0$. Therefore, the fixed point of the iteration $x^*$ is not the same as the minimizer of the loss function $x_{LS}$.\n- Reducing the learning rate $\\eta$ can be necessary to ensure the stability and convergence *of the iterative process* (e.g., if the spectral radius of $I - \\eta BA$ is less than $1$), but it does not change the location of the fixed point it converges to. The algorithm will dutifully converge to the wrong answer, just with smaller steps.\n\n**Verdict: Incorrect.**\n\n**F. In an end-to-end network where the reconstruction module contains $A$ and $B$ internally and the loss is computed in the image domain, the learned parameters may partially compensate for $B \\neq A^{\\top}$, but the training signal corresponds to a surrogate objective induced by $B$, not the original physical data fidelity.**\n\n- This statement describes the situation in a broader deep learning context. Assume the data fidelity term $\\mathcal{L}(x)$ is a component of the overall loss function used to train a network with parameters $\\theta$. The gradient of this component with respect to the network's output image $x$ is incorrectly computed as $\\tilde{g}(x) = B(Ax-y)$.\n- This incorrect gradient is then backpropagated through the rest of the network to update the parameters $\\theta$. The network's learning process is driven by this faulty signal.\n- Consequently, the network parameters $\\theta$ are not being optimized to minimize the true loss function, but rather to minimize a surrogate objective whose stationary points are dictated by the operator $B$. The network learns to produce outputs that work well with the specific pair of operators $(A, B)$ used during training.\n- The learned parameters can \"partially compensate\" for this mismatch. For instance, a neural network regularizer might learn to suppress artifacts that are characteristic of the specific $B \\neq A^\\top$ mismatch.\n- The crucial point is that the training is fundamentally optimizing with respect to a different objective than the one mathematically specified by $\\mathcal{L}(x)$ and the Euclidean inner product. The physical fidelity described by $\\mathcal{L}(x)$ corresponds to the gradient $\\nabla \\mathcal{L}(x)$, but the training uses a signal from $\\tilde{g}(x)$, which is defined by $B$. The phrase \"surrogate objective induced by $B$\" accurately captures this situation. (Note: The phrase \"loss is computed in the image domain\" might be slightly imprecise as the given $\\mathcal{L}(x)$ is in the data domain, but the core argument of the statement about the training signal and surrogate objective is sound and central to the problem).\n\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ACF}$$"
        },
        {
            "introduction": "Once a model is trained, we must evaluate its performance, but what metrics should we trust? This practice challenges the reliance on standard image fidelity metrics like Peak Signal-to-Noise Ratio ($\\mathrm{PSNR}$) and the Structural Similarity Index Measure ($\\mathrm{SSIM}$) . By working through a concrete example, you will develop a more nuanced understanding of why higher scores on these metrics do not always translate to better diagnostic quality.",
            "id": "4875595",
            "problem": "A researcher evaluates two deep learning reconstructions of a small computed tomography (CT) image patch. Pixel intensities are normalized to the range $[0,1]$. The ground-truth patch $G \\in \\mathbb{R}^{4 \\times 4}$ and two reconstructions $R_1, R_2 \\in \\mathbb{R}^{4 \\times 4}$ are given by\n$$\nG \\;=\\;\n\\begin{bmatrix}\n0  0  1  1 \\\\\n0  0  1  1 \\\\\n0  0  1  1 \\\\\n0  0  1  1\n\\end{bmatrix},\\quad\nR_1 \\;=\\;\n\\begin{bmatrix}\n0.2  0.2  0.8  0.8 \\\\\n0.2  0.2  0.8  0.8 \\\\\n0.2  0.2  0.8  0.8 \\\\\n0.2  0.2  0.8  0.8\n\\end{bmatrix},\\quad\nR_2 \\;=\\;\n\\begin{bmatrix}\n0    0.2  1    0.8 \\\\\n0    0.2  1    0.8 \\\\\n0    0.2  1    0.8 \\\\\n0    0.2  1    0.8\n\\end{bmatrix}.\n$$\nAssume the peak signal value is $L=1$. Use the standard definition of mean squared error (MSE) over all $N=16$ pixels and the standard definition of Peak Signal-to-Noise Ratio (PSNR). For the Structural Similarity Index Measure (SSIM), use a single-window computation over the full $4 \\times 4$ patch with population statistics (divide by $N$ for variance and covariance), and constants $C_1=(0.01)^2$ and $C_2=(0.03)^2$.\n\nBased on correct computations of PSNR and SSIM for $R_1$ and $R_2$, and on reasoning grounded in task-based image quality for clinical decision making (e.g., detection of small, high-contrast structures), which of the following statements are true?\n\nA. The Peak Signal-to-Noise Ratio (PSNR) of $R_2$ is approximately $17\\,\\mathrm{dB}$ and exceeds that of $R_1$ (approximately $14\\,\\mathrm{dB}$).\n\nB. The Structural Similarity Index Measure (SSIM) of $R_1$ exceeds that of $R_2$ because $R_1$ better matches global brightness and contrast.\n\nC. Despite differences in PSNR and SSIM, either PSNR or SSIM alone can guarantee better clinical lesion-detection performance for any deep learning reconstruction.\n\nD. For clinical tasks sensitive to high-frequency features, a reconstruction like $R_1$ can underperform $R_2$ even if their PSNR were matched, because smoothing degrades task-relevant structure.\n\nE. With the stated constants, SSIM is approximately $0.88$ for $R_1$ and approximately $0.95$ for $R_2$.",
            "solution": "We proceed from standard definitions. The mean squared error (MSE) between an image $X$ and the ground truth $G$ over $N$ pixels is\n$$\n\\mathrm{MSE}(X,G) \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\left(X_i - G_i\\right)^2.\n$$\nThe Peak Signal-to-Noise Ratio (PSNR) in decibels for peak value $L$ is\n$$\n\\mathrm{PSNR}(X,G) \\;=\\; 10 \\log_{10}\\!\\left(\\frac{L^2}{\\mathrm{MSE}(X,G)}\\right).\n$$\nThe Structural Similarity Index Measure (SSIM) between $X$ and $G$, computed over the full patch with population statistics, is\n$$\n\\mathrm{SSIM}(X,G) \\;=\\; \\frac{\\left(2 \\mu_X \\mu_G + C_1\\right)\\left(2 \\sigma_{XG} + C_2\\right)}{\\left(\\mu_X^2 + \\mu_G^2 + C_1\\right)\\left(\\sigma_X^2 + \\sigma_G^2 + C_2\\right)},\n$$\nwhere $\\mu_X$ and $\\mu_G$ are the means, $\\sigma_X^2$ and $\\sigma_G^2$ are the variances (dividing by $N$), and $\\sigma_{XG}$ is the covariance (dividing by $N$). We take $C_1=(0.01)^2=0.0001$ and $C_2=(0.03)^2=0.0009$, and $L=1$.\n\nStep 1: Compute MSE and PSNR.\n\n- For $R_1$ versus $G$: Each $0$ in $G$ is approximated by $0.2$ in $R_1$ (error $0.2$), and each $1$ is approximated by $0.8$ (error $-0.2$). Squared error per pixel is $0.04$ for all $N=16$ pixels. Hence\n$$\n\\mathrm{MSE}(R_1,G) \\;=\\; \\frac{16 \\cdot 0.04}{16} \\;=\\; 0.04.\n$$\nThen\n$$\n\\mathrm{PSNR}(R_1,G) \\;=\\; 10 \\log_{10}\\!\\left(\\frac{1}{0.04}\\right) \\;=\\; 10 \\log_{10}(25) \\;\\approx\\; 13.98\\,\\mathrm{dB}.\n$$\n\n- For $R_2$ versus $G$: Eight pixels match exactly ($0 \\rightarrow 0$ and $1 \\rightarrow 1$, squared error $0$), and eight are off by $\\pm 0.2$ ($0 \\rightarrow 0.2$ and $1 \\rightarrow 0.8$, squared error $0.04$). Thus\n$$\n\\mathrm{MSE}(R_2,G) \\;=\\; \\frac{8 \\cdot 0 + 8 \\cdot 0.04}{16} \\;=\\; 0.02,\n$$\nand\n$$\n\\mathrm{PSNR}(R_2,G) \\;=\\; 10 \\log_{10}\\!\\left(\\frac{1}{0.02}\\right) \\;=\\; 10 \\log_{10}(50) \\;\\approx\\; 16.99\\,\\mathrm{dB}.\n$$\n\nTherefore, $\\mathrm{PSNR}(R_2,G) \\approx 16.99\\,\\mathrm{dB}$ exceeds $\\mathrm{PSNR}(R_1,G) \\approx 13.98\\,\\mathrm{dB}$.\n\nStep 2: Compute SSIM components.\n\nFirst, compute ground-truth statistics. For $G$, there are eight zeros and eight ones:\n- Mean:\n$$\n\\mu_G \\;=\\; \\frac{8 \\cdot 0 + 8 \\cdot 1}{16} \\;=\\; 0.5.\n$$\n- Variance (population):\n$$\n\\sigma_G^2 \\;=\\; \\frac{8 \\cdot (0-0.5)^2 + 8 \\cdot (1-0.5)^2}{16} \\;=\\; \\frac{8 \\cdot 0.25 + 8 \\cdot 0.25}{16} \\;=\\; 0.25.\n$$\n\nNow for $R_1$: eight values are $0.2$, eight are $0.8$.\n- Mean:\n$$\n\\mu_{R_1} \\;=\\; \\frac{8 \\cdot 0.2 + 8 \\cdot 0.8}{16} \\;=\\; 0.5.\n$$\n- Variance:\n$$\n\\sigma_{R_1}^2 \\;=\\; \\frac{8 \\cdot (0.2-0.5)^2 + 8 \\cdot (0.8-0.5)^2}{16} \\;=\\; \\frac{8 \\cdot 0.09 + 8 \\cdot 0.09}{16} \\;=\\; 0.09.\n$$\n- Covariance with $G$:\nFor each zero location, $(G-\\mu_G) = -0.5$ and $(R_1-\\mu_{R_1}) = -0.3$ gives product $0.15$ (eight times); for each one location, $(G-\\mu_G) = +0.5$ and $(R_1-\\mu_{R_1}) = +0.3$ gives product $0.15$ (eight times). Hence\n$$\n\\sigma_{R_1,G} \\;=\\; \\frac{8 \\cdot 0.15 + 8 \\cdot 0.15}{16} \\;=\\; 0.15.\n$$\nNow compute $\\mathrm{SSIM}(R_1,G)$:\n- Luminance term:\n$$\n\\frac{2 \\mu_{R_1} \\mu_G + C_1}{\\mu_{R_1}^2 + \\mu_G^2 + C_1} \\;=\\; \\frac{2 \\cdot 0.5 \\cdot 0.5 + 0.0001}{0.5^2 + 0.5^2 + 0.0001} \\;=\\; \\frac{0.5001}{0.5001} \\;=\\; 1.\n$$\n- Contrast-structure term:\n$$\n\\frac{2 \\sigma_{R_1,G} + C_2}{\\sigma_{R_1}^2 + \\sigma_G^2 + C_2} \\;=\\; \\frac{2 \\cdot 0.15 + 0.0009}{0.09 + 0.25 + 0.0009} \\;=\\; \\frac{0.3009}{0.3409} \\;\\approx\\; 0.883.\n$$\nThus\n$$\n\\mathrm{SSIM}(R_1,G) \\;\\approx\\; 0.883.\n$$\n\nFor $R_2$: the values are four $0$, four $0.2$, four $0.8$, four $1.0$.\n- Mean:\n$$\n\\mu_{R_2} \\;=\\; \\frac{4 \\cdot 0 + 4 \\cdot 0.2 + 4 \\cdot 0.8 + 4 \\cdot 1}{16} \\;=\\; 0.5.\n$$\n- Variance:\n$$\n\\sigma_{R_2}^2 \\;=\\; \\frac{4 \\cdot (0-0.5)^2 + 4 \\cdot (0.2-0.5)^2 + 4 \\cdot (0.8-0.5)^2 + 4 \\cdot (1-0.5)^2}{16}\n\\;=\\; \\frac{4 \\cdot 0.25 + 4 \\cdot 0.09 + 4 \\cdot 0.09 + 4 \\cdot 0.25}{16} \\;=\\; \\frac{2.72}{16} \\;=\\; 0.17.\n$$\n- Covariance with $G$:\nProducts $(G-\\mu_G)(R_2-\\mu_{R_2})$ are $0.25$ for exact matches ($0$ with $0$, $1$ with $1$) and $0.15$ for the perturbed cases ($0$ with $0.2$, $1$ with $0.8$), each occurring four times per half. Thus\n$$\n\\sigma_{R_2,G} \\;=\\; \\frac{4 \\cdot 0.25 + 4 \\cdot 0.15 + 4 \\cdot 0.25 + 4 \\cdot 0.15}{16} \\;=\\; \\frac{3.2}{16} \\;=\\; 0.2.\n$$\nNow compute $\\mathrm{SSIM}(R_2,G)$:\n- Luminance term is again\n$$\n\\frac{2 \\mu_{R_2} \\mu_G + C_1}{\\mu_{R_2}^2 + \\mu_G^2 + C_1} \\;=\\; 1.\n$$\n- Contrast-structure term:\n$$\n\\frac{2 \\sigma_{R_2,G} + C_2}{\\sigma_{R_2}^2 + \\sigma_G^2 + C_2} \\;=\\; \\frac{2 \\cdot 0.2 + 0.0009}{0.17 + 0.25 + 0.0009} \\;=\\; \\frac{0.4009}{0.4209} \\;\\approx\\; 0.952.\n$$\nThus\n$$\n\\mathrm{SSIM}(R_2,G) \\;\\approx\\; 0.952.\n$$\n\nStep 3: Relate metrics to clinical task performance.\n\nTask-based image quality in medical imaging emphasizes performance on a specified observer task (for example, detection of a small, high-contrast lesion). Fundamental considerations from detection theory indicate that detectability depends on the preservation of task-relevant signal components and the noise statistics across spatial frequencies. Smoothing operations reduce high-frequency content, potentially diminishing the energy of small structures critical for detection, even if pixel-wise error metrics are maintained or improved. Consequently, higher PSNR or SSIM does not, by itself, guarantee superior clinical task performance.\n\nOption-by-option analysis:\n\n- Option A: Using the computed values, $\\mathrm{PSNR}(R_1,G) \\approx 13.98\\,\\mathrm{dB}$ and $\\mathrm{PSNR}(R_2,G) \\approx 16.99\\,\\mathrm{dB}$. Hence $R_2$ exceeds $R_1$ by approximately $3\\,\\mathrm{dB}$. This statement is Correct.\n\n- Option B: We found $\\mathrm{SSIM}(R_1,G) \\approx 0.883$ and $\\mathrm{SSIM}(R_2,G) \\approx 0.952$. Despite equal means and similar contrast alignment, $R_2$ has higher SSIM due to stronger covariance and adequate variance relative to $G$. The claim that $R_1$ exceeds $R_2$ in SSIM is incorrect. Verdict: Incorrect.\n\n- Option C: No single fidelity metric (PSNR or SSIM) can guarantee better clinical lesion-detection performance across all reconstructions and tasks; task-based assessment is required. This claim of a guarantee is false. Verdict: Incorrect.\n\n- Option D: Smoothing degrades high-frequency content relevant to detecting small structures; even at matched PSNR, a smoothed image can underperform a less-smoothed image on high-frequency detection tasks. This aligns with task-based principles. Verdict: Correct.\n\n- Option E: The computed SSIM values are approximately $0.88$ for $R_1$ and $0.95$ for $R_2$, consistent with our calculations. Verdict: Correct.\n\nTherefore, the correct statements are A, D, and E.",
            "answer": "$$\\boxed{ADE}$$"
        }
    ]
}