{
    "hands_on_practices": [
        {
            "introduction": "Federated Learning's efficiency is fundamentally constrained by network bandwidth and latency, creating a significant communication bottleneck. This exercise guides you through calculating the wall-clock time for a single training round, accounting for factors like model size, sparsification, and network speeds. By working through this problem , you will develop a quantitative understanding of the practical engineering trade-offs that govern the feasibility and speed of multi-center studies.",
            "id": "4540789",
            "problem": "A multi-center radiomics study is training a predictive model using Federated Learning (FL), where each of $m$ centers transmits sparse model updates to a central server and receives the aggregated global model each round. Let the global model have $d$ trainable parameters. Each client applies top-$k$ sparsification to its local update, transmitting only the $k$ largest-magnitude parameter updates, and uses $8$-bit quantization for all transmitted values. To identify which parameters are selected under top-$k$, assume fixed-length binary coding for indices with minimal length $\\lceil \\log_{2}(d) \\rceil$ bits per index. Each uplink message includes a constant metadata header of $h_{\\text{up}}$ bits, and each downlink message includes a constant metadata header of $h_{\\text{down}}$ bits.\n\nFor a synchronous FL round, the wall-clock duration is determined by the slowest center. Assume there are $m = 8$ centers, of which $7$ have an uplink bandwidth of $12 \\times 10^{6}$ bits per second and a downlink bandwidth of $24 \\times 10^{6}$ bits per second, and $1$ constrained center has an uplink bandwidth of $6 \\times 10^{6}$ bits per second and a downlink bandwidth of $8 \\times 10^{6}$ bits per second. The one-way network latency is $L = 0.025$ seconds, and each round consists of one uplink transmission and one downlink transmission, incurring a total latency of $2L$. Ignore server computation time and any parallelization effects on downlink beyond the per-client bandwidth constraint.\n\nStarting from first principles of information representation and network throughput, derive the communication cost per client in bits for the uplink under top-$k$ sparsification with $8$-bit quantization and fixed-length index coding, and for the downlink when broadcasting the full quantized model. Then, using the constrained center’s bandwidths, calculate the per-round wall-clock time. Use the following parameters: $d = 1{,}000{,}000$, $k = 50{,}000$, $h_{\\text{up}} = 1024$ bits, $h_{\\text{down}} = 512$ bits. Express the final wall-clock time in seconds, and round your answer to three significant figures.",
            "solution": "The problem statement is subjected to rigorous validation.\n\n### Step 1: Extract Givens\nThe data and conditions explicitly provided are:\n-   Number of centers, $m = 8$.\n-   Total number of trainable parameters in the global model, $d = 1,000,000$.\n-   Sparsification parameter, $k = 50,000$.\n-   Quantization level for transmitted values, $b_{\\text{val}} = 8$ bits.\n-   Index coding scheme: fixed-length binary with size $b_{\\text{idx}} = \\lceil \\log_{2}(d) \\rceil$ bits per index.\n-   Uplink metadata header size, $h_{\\text{up}} = 1024$ bits.\n-   Downlink metadata header size, $h_{\\text{down}} = 512$ bits.\n-   Number of normal centers: $7$.\n-   Uplink bandwidth of normal centers, $B_{\\text{up, normal}} = 12 \\times 10^{6}$ bits per second.\n-   Downlink bandwidth of normal centers, $B_{\\text{down, normal}} = 24 \\times 10^{6}$ bits per second.\n-   Number of constrained centers: $1$.\n-   Uplink bandwidth of constrained center, $B_{\\text{up, constrained}} = 6 \\times 10^{6}$ bits per second.\n-   Downlink bandwidth of constrained center, $B_{\\text{down, constrained}} = 8 \\times 10^{6}$ bits per second.\n-   One-way network latency, $L = 0.025$ seconds.\n-   Total latency per round (one uplink and one downlink), $T_{\\text{latency}} = 2L$.\n-   The training is synchronous, and the total round time is determined by the slowest center.\n-   Server computation time is to be ignored.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established criteria:\n-   **Scientifically Grounded:** The problem is firmly rooted in the principles of communication-efficient federated learning, a standard topic in distributed machine learning and computer science. The use of top-$k$ sparsification, quantization, bandwidth, and latency are all standard, well-defined concepts. The setup is scientifically and technically sound.\n-   **Well-Posed:** All necessary variables and parameters for calculating the communication cost and time are provided. The objective is clear and unambiguous: to compute the wall-clock time for a single synchronous round, limited by the most resource-constrained client. The problem structure guarantees a unique and meaningful solution.\n-   **Objective:** The problem is described using precise, quantitative, and unbiased language.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is self-contained, scientifically sound, and well-posed. No flaws were detected. We may now proceed with the solution.\n\nThe objective is to calculate the per-round wall-clock time, $T_{\\text{round}}$, for a synchronous Federated Learning process. In a synchronous setting, the duration of each round is determined by the participant that takes the longest time to complete its tasks. This is the \"straggler\" or, in this case, the \"constrained center\". The total time for one round consists of the network latency and the data transmission time.\n\nThe total round time $T_{\\text{round}}$ is given by:\n$$T_{\\text{round}} = T_{\\text{latency}} + T_{\\text{transmission}}$$\nThe total latency for a round involving one uplink and one downlink is given as $T_{\\text{latency}} = 2L$. The total transmission time is the sum of the uplink time, $T_{\\text{up}}$, and the downlink time, $T_{\\text{down}}$, for the constrained center.\n$$T_{\\text{round}} = 2L + T_{\\text{up}} + T_{\\text{down}}$$\nWe must calculate $T_{\\text{up}}$ and $T_{\\text{down}}$ from first principles.\n\n**1. Uplink Communication Cost and Time**\n\nThe uplink transmission from a client consists of a metadata header and the payload. The payload contains the top-$k$ sparsified model updates. Each update comprises the value of the parameter change and its corresponding index.\n\nThe total size of the uplink message, $S_{\\text{up}}$, is:\n$$S_{\\text{up}} = h_{\\text{up}} + S_{\\text{payload, up}}$$\nThe payload size, $S_{\\text{payload, up}}$, corresponds to $k$ updates. The size of each update is the sum of the bits for the index and the bits for the quantized value.\n-   The size of each quantized value is given as $b_{\\text{val}} = 8$ bits.\n-   The size of each index, $b_{\\text{idx}}$, is determined by the minimum number of bits required to uniquely identify any of the $d$ parameters using fixed-length coding. This is calculated as:\n$$b_{\\text{idx}} = \\lceil \\log_{2}(d) \\rceil$$\nSubstituting $d = 1,000,000$:\n$$b_{\\text{idx}} = \\lceil \\log_{2}(10^6) \\rceil = \\lceil 6 \\log_{2}(10) \\rceil$$\nUsing the approximation $\\log_{2}(10) \\approx 3.3219$:\n$$b_{\\text{idx}} = \\lceil 6 \\times 3.3219 \\rceil = \\lceil 19.9314 \\rceil = 20 \\text{ bits}$$\nEach of the $k$ updates requires $(b_{\\text{idx}} + b_{\\text{val}})$ bits. The total uplink payload size is:\n$$S_{\\text{payload, up}} = k \\times (b_{\\text{idx}} + b_{\\text{val}})$$\nSubstituting the values $k = 50,000$, $b_{\\text{idx}}=20$, and $b_{\\text{val}}=8$:\n$$S_{\\text{payload, up}} = 50,000 \\times (20 + 8) = 50,000 \\times 28 = 1,400,000 \\text{ bits}$$\nThe total uplink message size, including the header $h_{\\text{up}} = 1024$ bits, is:\n$$S_{\\text{up}} = 1024 + 1,400,000 = 1,401,024 \\text{ bits}$$\nThe uplink transmission time for the constrained center, $T_{\\text{up}}$, is this size divided by its uplink bandwidth, $B_{\\text{up, constrained}} = 6 \\times 10^{6}$ bits/s:\n$$T_{\\text{up}} = \\frac{S_{\\text{up}}}{B_{\\text{up, constrained}}} = \\frac{1,401,024}{6 \\times 10^{6}} \\text{ seconds}$$\n\n**2. Downlink Communication Cost and Time**\n\nThe downlink transmission from the server to each client consists of a metadata header and the full, dense global model. The problem specifies this is a \"full quantized model\", which we interpret as all $d$ parameters being quantized to $b_{\\text{val}}$ bits each. Since the model is dense, indices are not needed; the position of each parameter is implicit.\n\nThe total size of the downlink message, $S_{\\text{down}}$, is:\n$$S_{\\text{down}} = h_{\\text{down}} + S_{\\text{payload, down}}$$\nThe payload consists of $d$ quantized parameter values:\n$$S_{\\text{payload, down}} = d \\times b_{\\text{val}}$$\nSubstituting the values $d = 1,000,000$ and $b_{\\text{val}} = 8$:\n$$S_{\\text{payload, down}} = 1,000,000 \\times 8 = 8,000,000 \\text{ bits}$$\nThe total downlink message size, including the header $h_{\\text{down}} = 512$ bits, is:\n$$S_{\\text{down}} = 512 + 8,000,000 = 8,000,512 \\text{ bits}$$\nThe downlink transmission time for the constrained center, $T_{\\text{down}}$, is this size divided by its downlink bandwidth, $B_{\\text{down, constrained}} = 8 \\times 10^{6}$ bits/s:\n$$T_{\\text{down}} = \\frac{S_{\\text{down}}}{B_{\\text{down, constrained}}} = \\frac{8,000,512}{8 \\times 10^{6}} \\text{ seconds}$$\n\n**3. Total Wall-Clock Time**\n\nNow we assemble the components to find the total round time, $T_{\\text{round}}$.\n$$T_{\\text{round}} = 2L + T_{\\text{up}} + T_{\\text{down}}$$\nSubstituting the given and derived expressions:\n$$T_{\\text{round}} = (2 \\times 0.025) + \\frac{1,401,024}{6 \\times 10^{6}} + \\frac{8,000,512}{8 \\times 10^{6}}$$\nCalculating each term:\n-   Latency: $T_{\\text{latency}} = 2 \\times 0.025 = 0.05$ s.\n-   Uplink Time: $T_{\\text{up}} = \\frac{1,401,024}{6,000,000} \\approx 0.233504$ s.\n-   Downlink Time: $T_{\\text{down}} = \\frac{8,000,512}{8,000,000} = 1.000064$ s.\n\nSumming these terms:\n$$T_{\\text{round}} \\approx 0.05 + 0.233504 + 1.000064 = 1.283568 \\text{ s}$$\nThe problem requires the answer to be rounded to three significant figures.\n$$T_{\\text{round}} \\approx 1.28 \\text{ s}$$",
            "answer": "$$\n\\boxed{1.28}\n$$"
        },
        {
            "introduction": "A primary challenge in multi-center studies is that data distributions often differ between institutions, a phenomenon known as data heterogeneity. This practice  explores 'label shift,' where the prevalence of a disease varies across centers, and asks you to derive a correction method using importance weighting. Working through this scenario will demonstrate how to ensure the global model is not biased by any single center's data distribution, while also showing how privacy can be maintained using techniques like Secure Aggregation.",
            "id": "4540746",
            "problem": "A radiomics classification study is being conducted across two clinical centers using Federated Learning (FL). Assume the scenario satisfies label shift: for all centers $k \\in \\{1,2\\}$ and classes $y \\in \\{0,1\\}$, the conditional distribution of radiomic features given the class is identical across centers, that is $p_{k}(x \\mid y) = p(x \\mid y)$, while the class priors $p_{k}(y)$ may differ by center. The goal is to train a single classifier with cross-entropy loss that is appropriate for the global population mixture characterized by a target prior $p(y)$.\n\nYou are asked to derive, from first principles, a correction scheme that reweights the per-center contributions so that the weighted empirical risk computed at each center under label shift equals the global risk under the target prior. Then propose a privacy-preserving protocol, relying on Secure Aggregation (SA), by which the target prior can be estimated without revealing any center’s individual label counts.\n\nConstruct the following synthetic example consistent with the scenario:\n- Center $1$ has $n_{1} = 800$ labeled patients, with $560$ malignant cases ($y=1$) and $240$ benign cases ($y=0$), so $p_{1}(1) = 0.7$ and $p_{1}(0) = 0.3$.\n- Center $2$ has $n_{2} = 1200$ labeled patients, with $360$ malignant cases ($y=1$) and $840$ benign cases ($y=0$), so $p_{2}(1) = 0.3$ and $p_{2}(0) = 0.7$.\n\nUsing only the assumption $p_{k}(x \\mid y) = p(x \\mid y)$ and the definition of cross-entropy risk, derive the unique class-dependent weights per center that produce an unbiased estimate of the global cross-entropy risk under the mixture prior $p(y)$, where $p(y)$ must be computed from the aggregated sample across centers via Secure Aggregation of one-hot label counts. Then, for the synthetic example, compute the numerical values of these weights for both centers and both classes.\n\nExpress your final numerical weights in a row matrix in the order $\\left(\\beta_{1}(0), \\beta_{1}(1), \\beta_{2}(0), \\beta_{2}(1)\\right)$ and round your answer to four significant figures.",
            "solution": "The problem is scientifically grounded, well-posed, and objective. It describes a realistic scenario in federated learning for medical imaging, known as label distribution shift, and asks for a standard correction method (importance sampling) and a privacy-preserving computation protocol (Secure Aggregation). The premises are consistent and the data provided is sufficient for a unique solution. Therefore, the problem is valid and I will proceed with a full solution.\n\nThe objective is to train a model with parameters $\\theta$ by minimizing a global risk function $R(\\theta)$, which is the expected loss over the target global data distribution $p(x, y)$. The loss function is the cross-entropy loss, $L(y, f(x; \\theta))$, where $f(x; \\theta)$ is the model's predicted probability for class $y=1$. The global risk is:\n$$R(\\theta) = \\mathbb{E}_{p(x,y)}[L(y, f(x; \\theta))]$$\nBy the law of total expectation, we can write the risk as a sum over the classes $y \\in \\{0,1\\}$:\n$$R(\\theta) = \\sum_{y \\in \\{0,1\\}} p(y) \\mathbb{E}_{p(x|y)}[L(y, f(x; \\theta))]$$\nwhere $p(y)$ is the target global class prior and $p(x|y)$ is the class-conditional feature distribution.\n\nIn the federated setting, each center $k$ has its own local data distribution, $p_k(x, y)$. The local risk at center $k$ is:\n$$R_k(\\theta) = \\mathbb{E}_{p_k(x,y)}[L(y, f(x; \\theta))]$$\nThe problem states the label shift assumption, which posits that the class-conditional distributions are identical across centers, i.e., $p_k(x|y) = p(x|y)$ for all $k$. The local class priors $p_k(y)$ may differ. Under this assumption, the local risk can be written as:\n$$R_k(\\theta) = \\sum_{y \\in \\{0,1\\}} p_k(y) \\mathbb{E}_{p_k(x|y)}[L(y, f(x; \\theta))] = \\sum_{y \\in \\{0,1\\}} p_k(y) \\mathbb{E}_{p(x|y)}[L(y, f(x; \\theta))]$$\nWe seek a reweighting scheme with weights $\\beta_k(y)$ applied to the loss of each sample from center $k$ with label $y$, such that the expectation of the weighted local loss equals the global risk. Let this weighted local risk be $R_k^w(\\theta)$.\n$$R_k^w(\\theta) = \\mathbb{E}_{p_k(x,y)}[\\beta_k(y) L(y, f(x; \\theta))]$$\nExpanding this expectation:\n$$R_k^w(\\theta) = \\sum_{y \\in \\{0,1\\}} \\int \\beta_k(y) L(y, f(x; \\theta)) p_k(x,y) dx$$\nUsing $p_k(x,y) = p_k(y)p_k(x|y) = p_k(y)p(x|y)$:\n$$R_k^w(\\theta) = \\sum_{y \\in \\{0,1\\}} \\int \\beta_k(y) L(y, f(x; \\theta)) p_k(y) p(x|y) dx$$\n$$R_k^w(\\theta) = \\sum_{y \\in \\{0,1\\}} \\beta_k(y) p_k(y) \\int L(y, f(x; \\theta)) p(x|y) dx$$\n$$R_k^w(\\theta) = \\sum_{y \\in \\{0,1\\}} \\beta_k(y) p_k(y) \\mathbb{E}_{p(x|y)}[L(y, f(x; \\theta))]$$\nFor this weighted local risk to equal the global risk $R(\\theta)$ for any model $f(x; \\theta)$, the coefficients of the terms $\\mathbb{E}_{p(x|y)}[L(y, f(x; \\theta))]$ must be equal. Thus, for each class $y$, we must have:\n$$\\beta_k(y) p_k(y) = p(y)$$\nThis uniquely defines the correction weight for a sample from class $y$ at center $k$ as:\n$$\\beta_k(y) = \\frac{p(y)}{p_k(y)}$$\nThis is the principle of importance sampling applied to the label shift scenario. In practice, the empirical risk at center $k$ is calculated as a weighted average: $\\hat{R}_k^w(\\theta) = \\frac{1}{n_k} \\sum_{i=1}^{n_k} \\beta_k(y_i) L(y_i, f(x_i; \\theta))$. An FL algorithm like FedAvg would then average the gradients of this weighted risk across centers.\n\nTo compute these weights, the centers need the global prior $p(y)$ and their local priors $p_k(y)$. The local priors can be computed privately by each center. To compute the global prior $p(y) = \\frac{\\sum_k n_k(y)}{\\sum_k n_k}$ without revealing the local counts $n_k(y)$, a Secure Aggregation (SA) protocol is used. Here is a proposal for such a protocol:\n1.  **Local Computation**: Each center $k$ computes its vector of label counts by summing the one-hot encoded labels of its $n_k$ patients. For a binary problem, this results in a vector $\\mathbf{v}_k = [n_k(0), n_k(1)]^T$. This vector is private. The total sample counts per center, $n_1$ and $n_2$, are considered public.\n2.  **Masking**: To hide $\\mathbf{v}_k$, each center $k$ generates a set of pairwise random masks. For a two-center scenario, Center $1$ and Center $2$ establish a shared secret (e.g., via Diffie-Hellman key exchange) to seed a pseudorandom number generator, creating a shared random vector $\\mathbf{p}_{12}$. By convention, $\\mathbf{p}_{21} = -\\mathbf{p}_{12}$.\n3.  **Transmission**: Center $1$ sends the masked vector $\\tilde{\\mathbf{v}}_1 = \\mathbf{v}_1 + \\mathbf{p}_{12}$ to the aggregator. Center $2$ sends $\\tilde{\\mathbf{v}}_2 = \\mathbf{v}_2 + \\mathbf{p}_{21} = \\mathbf{v}_2 - \\mathbf{p}_{12}$. An individual masked vector reveals no information about the underlying private vector $\\mathbf{v}_k$.\n4.  **Aggregation**: The aggregator sums the received masked vectors:\n    $$\\tilde{\\mathbf{V}} = \\tilde{\\mathbf{v}}_1 + \\tilde{\\mathbf{v}}_2 = (\\mathbf{v}_1 + \\mathbf{p}_{12}) + (\\mathbf{v}_2 - \\mathbf{p}_{12}) = \\mathbf{v}_1 + \\mathbf{v}_2$$\n    The random masks cancel out, leaving the aggregator with the sum of the private count vectors, $\\mathbf{V}_{sum} = [n_1(0)+n_2(0), n_1(1)+n_2(1)]^T = [N(0), N(1)]^T$.\n5.  **Global Prior Calculation**: The aggregator computes the total number of samples $N = N(0) + N(1)$ (or receives it as $n_1+n_2$). It then computes the target global priors $p(y) = N(y)/N$ and broadcasts them to the centers.\nWith $p(y)$, each center $k$ can compute its required weights $\\beta_k(y) = p(y)/p_k(y)$.\n\nNow we apply this to the synthetic example.\nThe givens are:\n- Center $1$: $n_1 = 800$, with $n_1(1) = 560$ and $n_1(0) = 240$.\n- Center $2$: $n_2 = 1200$, with $n_2(1) = 360$ and $n_2(0) = 840$.\n\nFirst, we compute the local priors $p_k(y) = n_k(y)/n_k$:\n- $p_1(1) = \\frac{560}{800} = 0.7$\n- $p_1(0) = \\frac{240}{800} = 0.3$\n- $p_2(1) = \\frac{360}{1200} = 0.3$\n- $p_2(0) = \\frac{840}{1200} = 0.7$\n\nNext, we compute the target global prior $p(y)$ from the aggregated data, which would be calculated via the SA protocol.\n- Total samples: $N = n_1 + n_2 = 800 + 1200 = 2000$.\n- Total malignant cases ($y=1$): $N(1) = n_1(1) + n_2(1) = 560 + 360 = 920$.\n- Total benign cases ($y=0$): $N(0) = n_1(0) + n_2(0) = 240 + 840 = 1080$.\nThe target global priors are:\n- $p(1) = \\frac{N(1)}{N} = \\frac{920}{2000} = 0.46$.\n- $p(0) = \\frac{N(0)}{N} = \\frac{1080}{2000} = 0.54$.\n\nFinally, we compute the weights $\\beta_k(y) = p(y)/p_k(y)$:\n- For Center $1$, class $0$: $\\beta_1(0) = \\frac{p(0)}{p_1(0)} = \\frac{0.54}{0.3} = 1.8$.\n- For Center $1$, class $1$: $\\beta_1(1) = \\frac{p(1)}{p_1(1)} = \\frac{0.46}{0.7} = \\frac{46}{70} = \\frac{23}{35} \\approx 0.6571428...$\n- For Center $2$, class $0$: $\\beta_2(0) = \\frac{p(0)}{p_2(0)} = \\frac{0.54}{0.7} = \\frac{54}{70} = \\frac{27}{35} \\approx 0.7714285...$\n- For Center $2$, class $1$: $\\beta_2(1) = \\frac{p(1)}{p_2(1)} = \\frac{0.46}{0.3} = \\frac{46}{30} = \\frac{23}{15} \\approx 1.5333333...$\n\nRounding these values to four significant figures, we get:\n- $\\beta_1(0) = 1.800$\n- $\\beta_1(1) = 0.6571$\n- $\\beta_2(0) = 0.7714$\n- $\\beta_2(1) = 1.533$\n\nThe final answer is presented as a row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.800 & 0.6571 & 0.7714 & 1.533\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The collaborative nature of Federated Learning, while powerful, introduces potential security vulnerabilities from malicious participants. This exercise  presents a hypothetical 'backdoor attack,' where an adversary tries to corrupt the global model for their own purposes. You are challenged to analyze the resulting gradients and devise a robust defense mechanism, learning how the central server can detect anomalous behavior without violating the privacy of the participating centers.",
            "id": "4540754",
            "problem": "A multi-center radiomics consortium is training a binary classifier to predict tumor molecular status from Magnetic Resonance Imaging (MRI) radiomics features across $K$ hospitals using Federated Learning (FL) with Stochastic Gradient Descent (SGD). Each site trains locally and sends model updates to a central server that performs aggregation. One adversarial hospital injects a backdoor: it modifies a small $3\\times 3$ high-intensity square in the lower-left corner of the tumor Region of Interest (ROI) in a fraction $5\\%$ of its local training images and flips those labels to a fixed target class $\\tau$. This creates a spurious correlation between the trigger pattern and the target label, aiming to cause misclassification at inference when the trigger is present.\n\nTo detect a backdoor without accessing private data, the server runs a dedicated detection round at iteration $t$, in which it broadcasts the current model parameters and a small, clean public reference set $\\mathcal{D}_r$ of size $n_r$ (curated from an open dataset without trigger artifacts). Each client returns a gradient of the loss on $\\mathcal{D}_r$. Under clean training objectives and a common reference batch, benign clients’ gradients should be closely aligned in direction, whereas a client optimizing a conflicting backdoor objective can yield a gradient that is nearly opposite.\n\nFor didactic purposes, assume that the model has $d=3$ trainable parameters and that four sites ($K=4$) return the following gradients on $\\mathcal{D}_r$ in the same parameter ordering:\n- Site $1$: $\\mathbf{g}_1 = (1.02,\\,-0.49,\\,0.19)^\\top$\n- Site $2$: $\\mathbf{g}_2 = (0.98,\\,-0.52,\\,0.22)^\\top$\n- Site $3$: $\\mathbf{g}_3 = (1.01,\\,-0.51,\\,0.21)^\\top$\n- Site $4$: $\\mathbf{g}_4 = (-0.90,\\,0.45,\\,-0.18)^\\top$\n\nThe server seeks to implement a cosine-gradient-similarity outlier test that is robust to one arbitrary outlier by constructing a consensus direction from the set of client gradients and then thresholding a similarity score with a robust, distribution-free criterion.\n\nWhich option both specifies an appropriate gradient similarity–based detection pipeline for this setting and correctly identifies the suspicious site to be flagged in this round?\n\nA. Compute the coordinate-wise median vector $\\mathbf{m}$ of the returned gradients, then compute each client’s cosine similarity $s_k$ to $\\mathbf{m}$. Let $s_{\\mathrm{med}}$ be the median of $\\{s_k\\}$ and $\\mathrm{MAD}$ the median absolute deviation of $\\{s_k\\}$. Flag any client with $s_k < s_{\\mathrm{med}} - 3\\cdot \\mathrm{MAD}$. With the numbers above, only site $4$ is flagged.\n\nB. Use only gradient magnitudes: compute $\\|\\mathbf{g}_k\\|_2$ for each client and flag the client with the largest magnitude as malicious, because a backdoor must push the model hardest. With the numbers above, this flags site $3$.\n\nC. Ask each client to compute cosine similarities using its own private local batch instead of the public reference set, average the pairwise similarities per client, and flag none to avoid false positives when the average is above zero.\n\nD. Augment the public reference set $\\mathcal{D}_r$ at the server with the suspected trigger pattern so that clients’ gradients emphasize the trigger feature; compute cosine similarities to the average gradient and flag site $4$ if its similarity is negative. This is necessary because otherwise the backdoor will not be detectable.",
            "solution": "The user wants me to validate and solve a problem concerning backdoor detection in a Federated Learning (FL) scenario for a multi-center radiomics study.\n\n### Step 1: Extract Givens\n\nI will begin by extracting the verbatim information provided in the problem statement.\n\n-   **Context**: A multi-center radiomics study using Federated Learning (FL) with Stochastic Gradient Descent (SGD) to train a binary classifier.\n-   **Number of Sites**: $K=4$ hospitals.\n-   **Adversarial Setup**: One adversarial hospital injects a backdoor.\n-   **Backdoor Trigger**: A small $3\\times 3$ high-intensity square in the lower-left corner of the tumor Region of Interest (ROI).\n-   **Backdoor Injection**: The trigger is applied to $5\\%$ of the local training images at the adversarial site.\n-   **Backdoor Objective**: Labels for triggered images are flipped to a fixed target class $\\tau$.\n-   **Detection Method Premise**: The server initiates a detection round by broadcasting the current model and a small, clean public reference set $\\mathcal{D}_r$. Each client returns the gradient of the loss on $\\mathcal{D}_r$. The hypothesis is that benign clients' gradients will be aligned, while the adversarial client's gradient may be in a nearly opposite direction.\n-   **Model Parameters**: The model has $d=3$ trainable parameters.\n-   **Client Gradients on $\\mathcal{D}_r$**:\n    -   Site $1$: $\\mathbf{g}_1 = (1.02,\\,-0.49,\\,0.19)^\\top$\n    -   Site $2$: $\\mathbf{g}_2 = (0.98,\\,-0.52,\\,0.22)^\\top$\n    -   Site $3$: $\\mathbf{g}_3 = (1.01,\\,-0.51,\\,0.21)^\\top$\n    -   Site $4$: $\\mathbf{g}_4 = (-0.90,\\,0.45,\\,-0.18)^\\top$\n-   **Detection Goal**: Implement a robust cosine-gradient-similarity outlier test by constructing a consensus direction and using a robust, distribution-free criterion for thresholding.\n\n### Step 2: Validate Using Extracted Givens\n\nI will now evaluate the problem statement's validity.\n\n-   **Scientific Grounding**: The problem is well-grounded in the field of machine learning security, specifically concerning adversarial attacks on federated learning systems. Backdoor attacks are a known vulnerability, and gradient-based detection on a common dataset is a recognized category of defense mechanisms. The scenario where a backdoored model produces divergent gradients on clean data is a plausible consequence of the model's parameters being shifted to learn the spurious correlation between the trigger and the target label. The setup is a standard and scientifically sound formulation of this research problem.\n-   **Well-Posedness**: The problem is well-posed. It supplies a clear objective (identify the suspicious site), all necessary numerical data (the four gradient vectors), and a clear description of the desired methodology (robust cosine similarity outlier test). A unique answer can be derived from the given information.\n-   **Objectivity**: The language is technical, precise, and free from subjective or biased statements. All terminology is standard within the domain of federated learning and adversarial machine learning.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. I will proceed with the solution derivation and option-by-option analysis.\n\n### Solution Derivation\n\nThe core task is to identify an outlier among the four gradient vectors $\\mathbf{g}_1, \\mathbf{g}_2, \\mathbf{g}_3, \\mathbf{g}_4$. The problem hypothesizes that the malicious gradient will be directionally opposite to the benign ones. A visual inspection of the vectors confirms this:\n-   $\\mathbf{g}_1 = (1.02, -0.49, 0.19)^\\top$\n-   $\\mathbf{g}_2 = (0.98, -0.52, 0.22)^\\top$\n-   $\\mathbf{g}_3 = (1.01, -0.51, 0.21)^\\top$\n-   $\\mathbf{g}_4 = (-0.90, 0.45, -0.18)^\\top$\n\nThe first three vectors are clearly clustered, with all components having the same sign and similar magnitude. The fourth vector, $\\mathbf{g}_4$, has components with signs that are opposite to those of the first three, suggesting it is pointing in a nearly opposite direction. For instance, $\\mathbf{g}_4$ is almost a negative-scalar multiple of the other vectors (e.g., $\\mathbf{g}_1 \\approx -1.13 \\cdot \\mathbf{g}_4$).\n\nThe server requires a detection method that is \"robust to one arbitrary outlier.\" This immediately suggests that simple statistics like the mean are inappropriate for constructing the \"consensus direction.\" The mean would be biased by the outlier. A robust estimator of central tendency, such as the median, is required.\n\nLet's compute the coordinate-wise median vector $\\mathbf{m}$ from the set of gradients $\\{\\mathbf{g}_1, \\mathbf{g}_2, \\mathbf{g}_3, \\mathbf{g}_4\\}$.\nLet $\\mathbf{g}_k = (g_{k,1}, g_{k,2}, g_{k,3})^\\top$. The median vector is $\\mathbf{m} = (\\text{median}(\\{g_{k,1}\\}), \\text{median}(\\{g_{k,2}\\}), \\text{median}(\\{g_{k,3}\\}))^\\top$.\n\n1.  First component values: $\\{1.02, 0.98, 1.01, -0.90\\}$. Sorted: $\\{-0.90, 0.98, 1.01, 1.02\\}$.\n    The median of an even-sized set is the average of the two middle elements: $\\text{median}_1 = \\frac{0.98 + 1.01}{2} = 0.995$.\n2.  Second component values: $\\{-0.49, -0.52, -0.51, 0.45\\}$. Sorted: $\\{-0.52, -0.51, -0.49, 0.45\\}$.\n    The median is $\\text{median}_2 = \\frac{-0.51 + (-0.49)}{2} = -0.50$.\n3.  Third component values: $\\{0.19, 0.22, 0.21, -0.18\\}$. Sorted: $\\{-0.18, 0.19, 0.21, 0.22\\}$.\n    The median is $\\text{median}_3 = \\frac{0.19 + 0.21}{2} = 0.20$.\n\nThe robust consensus vector is $\\mathbf{m} = (0.995, -0.50, 0.20)^\\top$. This vector is directionally aligned with $\\mathbf{g}_1, \\mathbf{g}_2, \\mathbf{g}_3$ and anti-aligned with $\\mathbf{g}_4$, making it a suitable reference for outlier detection.\n\nThe next step is to measure the similarity of each gradient $\\mathbf{g}_k$ to this consensus vector $\\mathbf{m}$ using cosine similarity, $s_k = \\cos(\\theta_k) = \\frac{\\mathbf{g}_k \\cdot \\mathbf{m}}{\\|\\mathbf{g}_k\\|_2 \\|\\mathbf{m}\\|_2}$.\n\nFinally, an outlier detection rule must be applied to the set of similarity scores $\\{s_k\\}$. Again, a robust method is required. A rule based on the median and the Median Absolute Deviation (MAD) of the scores is a standard, distribution-free approach. The MAD is defined as $\\text{MAD} = \\text{median}(|s_k - \\text{median}(\\{s_i\\})|)$. An observation is typically flagged as an outlier if it falls several MADs away from the median.\n\n### Option-by-Option Analysis\n\n**A. Compute the coordinate-wise median vector $\\mathbf{m}$ of the returned gradients, then compute each client’s cosine similarity $s_k$ to $\\mathbf{m}$. Let $s_{\\mathrm{med}}$ be the median of $\\{s_k\\}$ and $\\mathrm{MAD}$ the median absolute deviation of $\\{s_k\\}$. Flag any client with $s_k < s_{\\mathrm{med}} - 3\\cdot \\mathrm{MAD}$. With the numbers above, only site $4$ is flagged.**\n\nThis option describes a complete and robust pipeline that is consistent with the problem's requirements. Let's verify the calculations.\nWe have $\\mathbf{m} = (0.995, -0.50, 0.20)^\\top$.\nFirst, we compute the magnitudes:\n$\\|\\mathbf{m}\\|_2 = \\sqrt{0.995^2 + (-0.50)^2 + 0.20^2} = \\sqrt{0.990025 + 0.25 + 0.04} = \\sqrt{1.280025} \\approx 1.13138$\n$\\|\\mathbf{g}_1\\|_2 = \\sqrt{1.02^2 + (-0.49)^2 + 0.19^2} = \\sqrt{1.3166} \\approx 1.14743$\n$\\|\\mathbf{g}_2\\|_2 = \\sqrt{0.98^2 + (-0.52)^2 + 0.22^2} = \\sqrt{1.2792} \\approx 1.13102$\n$\\|\\mathbf{g}_3\\|_2 = \\sqrt{1.01^2 + (-0.51)^2 + 0.21^2} = \\sqrt{1.3243} \\approx 1.15078$\n$\\|\\mathbf{g}_4\\|_2 = \\sqrt{(-0.90)^2 + 0.45^2 + (-0.18)^2} = \\sqrt{1.0449} \\approx 1.02220$\n\nNext, the dot products $\\mathbf{g}_k \\cdot \\mathbf{m}$:\n$\\mathbf{g}_1 \\cdot \\mathbf{m} = (1.02)(0.995) + (-0.49)(-0.50) + (0.19)(0.20) = 1.0149 + 0.245 + 0.038 = 1.2979$\n$\\mathbf{g}_2 \\cdot \\mathbf{m} = (0.98)(0.995) + (-0.52)(-0.50) + (0.22)(0.20) = 0.9751 + 0.260 + 0.044 = 1.2791$\n$\\mathbf{g}_3 \\cdot \\mathbf{m} = (1.01)(0.995) + (-0.51)(-0.50) + (0.21)(0.20) = 1.00495 + 0.255 + 0.042 = 1.30195$\n$\\mathbf{g}_4 \\cdot \\mathbf{m} = (-0.90)(0.995) + (0.45)(-0.50) + (-0.18)(0.20) = -0.8955 - 0.225 - 0.036 = -1.1565$\n\nNow, the cosine similarities $s_k$:\n$s_1 = 1.2979 / (1.14743 \\cdot 1.13138) \\approx 0.9998$\n$s_2 = 1.2791 / (1.13102 \\cdot 1.13138) \\approx 0.9997$\n$s_3 = 1.30195 / (1.15078 \\cdot 1.13138) \\approx 1.0001 \\approx 1.0000$ (capped at 1)\n$s_4 = -1.1565 / (1.02220 \\cdot 1.13138) \\approx -1.0000$\n\nThe similarity scores are $\\{s_k\\} \\approx \\{0.9998, 0.9997, 1.0000, -1.0000\\}$.\nSorted scores: $\\{-1.0000, 0.9997, 0.9998, 1.0000\\}$.\nThe median score is $s_{\\mathrm{med}} = \\frac{0.9997 + 0.9998}{2} = 0.99975$.\nThe absolute deviations from the median are:\n$|0.9998 - 0.99975| = 0.00005$\n$|0.9997 - 0.99975| = 0.00005$\n$|1.0000 - 0.99975| = 0.00025$\n$|-1.0000 - 0.99975| = 1.99975$\nSorted deviations: $\\{0.00005, 0.00005, 0.00025, 1.99975\\}$.\nThe $\\mathrm{MAD}$ is the median of these deviations: $\\mathrm{MAD} = \\frac{0.00005 + 0.00025}{2} = 0.00015$.\nThe flagging threshold is $s_{\\mathrm{med}} - 3 \\cdot \\mathrm{MAD} = 0.99975 - 3 \\cdot (0.00015) = 0.99975 - 0.00045 = 0.9993$.\n\nLet's check each client:\n-   $s_1 \\approx 0.9998 > 0.9993 \\implies$ Not flagged.\n-   $s_2 \\approx 0.9997 > 0.9993 \\implies$ Not flagged.\n-   $s_3 \\approx 1.0000 > 0.9993 \\implies$ Not flagged.\n-   $s_4 \\approx -1.0000 < 0.9993 \\implies$ **Flagged**.\n\nThe pipeline is methodologically sound and the numerical conclusion is correct.\nVerdict: **Correct**.\n\n**B. Use only gradient magnitudes: compute $\\|\\mathbf{g}_k\\|_2$ for each client and flag the client with the largest magnitude as malicious, because a backdoor must push the model hardest. With the numbers above, this flags site $3$.**\n\nThis method is conceptually flawed. The defining characteristic of the outlier in the provided data is its direction, not its magnitude. An attacker can easily normalize their gradient update to have a magnitude similar to benign clients, evading such a naive check. Let's check the magnitudes: $\\|\\mathbf{g}_1\\|_2 \\approx 1.147$, $\\|\\mathbf{g}_2\\|_2 \\approx 1.131$, $\\|\\mathbf{g}_3\\|_2 \\approx 1.151$, $\\|\\mathbf{g}_4\\|_2 \\approx 1.022$. Site $3$ does have the largest magnitude, so the statement \"this flags site 3\" is arithmetically correct given its premise. However, this flags a benign client and fails to identify the clear directional outlier, Site $4$, which actually has the *smallest* magnitude. The method is not robust and is based on a weak, easily defeated heuristic.\nVerdict: **Incorrect**.\n\n**C. Ask each client to compute cosine similarities using its own private local batch instead of the public reference set, average the pairwise similarities per client, and flag none to avoid false positives when the average is above zero.**\n\nThis approach is fundamentally invalid. The entire basis for comparing gradients is that they are computed on an identical batch of data ($\\mathcal{D}_r$). If clients use their own private local data, the resulting gradients are not comparable. Differences could arise from natural data heterogeneity (non-IID data), which is a core challenge in FL, rather than malicious activity. This would lead to rampant false positives and make the detection signal meaningless. It contradicts the explicitly stated use of the \"public reference set $\\mathcal{D}_r$\". The flagging criterion is also arbitrary and lacks a sound statistical basis.\nVerdict: **Incorrect**.\n\n**D. Augment the public reference set $\\mathcal{D}_r$ at the server with the suspected trigger pattern so that clients’ gradients emphasize the trigger feature; compute cosine similarities to the average gradient and flag site $4$ if its similarity is negative. This is necessary because otherwise the backdoor will not be detectable.**\n\nThis option contains multiple flaws.\n1.  It assumes the server knows the backdoor trigger pattern (\"augment... with the suspected trigger pattern\"). This is a critical piece of information that the attacker aims to keep secret. A general-purpose defense cannot assume such knowledge.\n2.  It falsely claims this step is \"necessary because otherwise the backdoor will not be detectable.\" As demonstrated in the analysis of option A, the backdoor's influence on the model parameters can create detectable gradient divergence even on a clean reference set.\n3.  It proposes using the \"average gradient\" as the consensus direction. The average is not robust to outliers; the malicious gradient from Site $4$ would pull the average towards it, significantly degrading the ability of a cosine similarity test to identify Site $4$ as an outlier.\nThe procedure is based on unrealistic assumptions and suboptimal methods.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}