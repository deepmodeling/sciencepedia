{
    "hands_on_practices": [
        {
            "introduction": "自编码器的核心任务是重建其输入，这在处理像CT扫描这样的三维医学图像时，要求解码器能够精确地将紧凑的特征表示恢复到原始的体积维度。本练习将深入探讨转置卷积（transposed convolution）的内部工作原理，这是解码器中用于上采样和空间重建的关键构件。通过从基本原理推导输出尺寸公式，您将掌握确保自编码器架构正确性的关键计算，这是成功进行无监督特征学习的第一步。",
            "id": "4530275",
            "problem": "在影像组学中，三维计算机断层扫描 (CT) 容积通常使用自编码器进行无监督特征学习，以在没有标签的情况下捕获纹理和形态。考虑一个解码器阶段，该阶段使用单个三维转置卷积来重建原始容积的空间维度。原始容积的空间维度为 $(N_{h}, N_{w}, N_{d}) = (128, 160, 96)$。解码器输入特征图的空间维度为 $(M_{h}, M_{w}, M_{d}) = (64, 40, 32)$。该转置卷积使用沿三个轴的步幅分量 $(s_{h}, s_{w}, s_{d}) = (2, 4, 3)$、卷积核大小 $(k_{h}, k_{w}, k_{d}) = (3, 3, 3)$、膨胀 $(d_{h}, d_{w}, d_{d}) = (1, 1, 1)$ 和对称零填充 $(p_{h}, p_{w}, p_{d}) = (1, 1, 1)$。没有裁剪。该转置卷积包含一个整数输出填充 $(o_{h}, o_{w}, o_{d})$，对于每个轴 $i \\in \\{h,w,d\\}$，其约束条件为 $0 \\leq o_{i}  s_{i}$。\n\n从标准离散卷积的定义以及步幅、填充、膨胀和卷积核在一维中的含义出发，推导沿单个轴的转置卷积所产生的输出空间大小的表达式，该表达式是输入大小、步幅、填充、膨胀、卷积核大小和输出填充的函数。通过跨轴可分离性将该表达式扩展到三维。然后，使用提供的数值，确定所需的特定输出填充 $(o_{h}, o_{w}, o_{d})$，以使重建的输出精确匹配 $(N_{h}, N_{w}, N_{d}) = (128, 160, 96)$。\n\n将最终答案表示为包含 $(o_{h}, o_{w}, o_{d})$ 的单行矩阵。无需单位，也无需四舍五入。",
            "solution": "该问题是有效的，因为它在科学上基于深度学习的原理，是信息充分且一致的良定问题，并以客观、正式的语言表达。我们将开始提供解答。\n\n任务是确定三维转置卷积所需的输出填充 $(o_{h}, o_{w}, o_{d})$，以达到特定的输出空间维度。为此，我们必须首先推导转置卷积的输入大小、输出大小和参数之间的一般关系。\n\n让我们首先考虑标准的一维离散卷积。其输出大小 $N_{out}$ 由输入大小 $M_{in}$、卷积核大小 $k$、步幅 $s$、填充 $p$ 和膨胀 $d$ 决定，公式如下：\n$$N_{out} = \\left\\lfloor \\frac{M_{in} + 2p - d(k-1) - 1}{s} \\right\\rfloor + 1$$\n转置卷积，有时也称为分数步幅卷积或反卷积，可以理解为一种将输入张量映射到输出张量的操作，其输出张量的维度与标准卷积的逆操作兼容。具体来说，对于一组给定的卷积参数 $(k, s, p, d)$，相关的转置卷积将大小为 $N_{out}$ 的输入映射回大小为 $M_{in}$ 的输出。\n\n由于标准卷积公式中的向下取整函数 $\\lfloor \\cdot \\rfloor$，多个不同的标准卷积输入大小可以产生相同的输出大小。这意味着逆映射不是唯一的。为了将其形式化，我们重新整理这个关系。设转置卷积的输入大小为 $M$，输出大小为 $N$。这对应于一个标准卷积，其输入大小为 $N$，产生输出大小为 $M$。\n$$M = \\left\\lfloor \\frac{N + 2p - d(k-1) - 1}{s} \\right\\rfloor + 1$$\n这等价于不等式：\n$$M - 1 \\leq \\frac{N + 2p - d(k-1) - 1}{s}  M$$\n乘以步幅 $s$ 得：\n$$(M - 1)s \\leq N + 2p - d(k-1) - 1  (M - 1)s + s$$\n为了找到给定输入大小 $M$ 的可能输出大小 $N$ 的范围，我们分离出 $N$：\n$$(M - 1)s - 2p + d(k-1) + 1 \\leq N  (M - 1)s - 2p + d(k-1) + 1 + s$$\n这个不等式表明，输出大小 $N$ 有 $s$ 个可能的整数值。为了解决这个歧义并指定一个唯一的输出大小，引入了一个额外的参数，称为输出填充 $o$。输出填充 $o$ 是一个整数，约束条件为 $0 \\leq o  s$。最终的输出大小 $N$ 定义为推导范围的下界加上输出填充值：\n$$N = (M - 1)s - 2p + d(k-1) + 1 + o$$\n这就是一维转置卷积输出大小的通用公式。\n\n题目指出，三维操作是跨轴可分离的。因此，我们可以将此公式独立地应用于三个空间维度中的每一个：高度 $(h)$、宽度 $(w)$ 和深度 $(d)$。\n对于高度轴：\n$$N_{h} = (M_{h} - 1)s_{h} - 2p_{h} + d_{h}(k_{h}-1) + 1 + o_{h}$$\n对于宽度轴：\n$$N_{w} = (M_{w} - 1)s_{w} - 2p_{w} + d_{w}(k_{w}-1) + 1 + o_{w}$$\n对于深度轴：\n$$N_{d} = (M_{d} - 1)s_{d} - 2p_{d} + d_{d}(k_{d}-1) + 1 + o_{d}$$\n\n我们的目标是求出 $(o_{h}, o_{w}, o_{d})$ 的值。我们可以重新整理通用公式来求解 $o$：\n$$o = N - \\left( (M - 1)s - 2p + d(k-1) + 1 \\right)$$\n\n现在我们为每个维度代入给定的数值。\n\n对于高度轴 ($h$)：\n- 目标输出大小：$N_{h} = 128$\n- 输入大小：$M_{h} = 64$\n- 步幅：$s_{h} = 2$\n- 填充：$p_{h} = 1$\n- 卷积核大小：$k_{h} = 3$\n- 膨胀：$d_{h} = 1$\n\n$$o_{h} = 128 - \\left( (64 - 1) \\times 2 - 2 \\times 1 + 1 \\times (3-1) + 1 \\right)$$\n$$o_{h} = 128 - \\left( 63 \\times 2 - 2 + 1 \\times 2 + 1 \\right)$$\n$$o_{h} = 128 - \\left( 126 - 2 + 2 + 1 \\right)$$\n$$o_{h} = 128 - 127 = 1$$\n约束条件 $0 \\leq o_{h}  s_{h}$ 即 $0 \\leq 1  2$，该条件得到满足。\n\n对于宽度轴 ($w$)：\n- 目标输出大小：$N_{w} = 160$\n- 输入大小：$M_{w} = 40$\n- 步幅：$s_{w} = 4$\n- 填充：$p_{w} = 1$\n- 卷积核大小：$k_{w} = 3$\n- 膨胀：$d_{w} = 1$\n\n$$o_{w} = 160 - \\left( (40 - 1) \\times 4 - 2 \\times 1 + 1 \\times (3-1) + 1 \\right)$$\n$$o_{w} = 160 - \\left( 39 \\times 4 - 2 + 1 \\times 2 + 1 \\right)$$\n$$o_{w} = 160 - \\left( 156 - 2 + 2 + 1 \\right)$$\n$$o_{w} = 160 - 157 = 3$$\n约束条件 $0 \\leq o_{w}  s_{w}$ 即 $0 \\leq 3  4$，该条件得到满足。\n\n对于深度轴 ($d$)：\n- 目标输出大小：$N_{d} = 96$\n- 输入大小：$M_{d} = 32$\n- 步幅：$s_{d} = 3$\n- 填充：$p_{d} = 1$\n- 卷积核大小：$k_{d} = 3$\n- 膨胀：$d_{d} = 1$\n\n$$o_{d} = 96 - \\left( (32 - 1) \\times 3 - 2 \\times 1 + 1 \\times (3-1) + 1 \\right)$$\n$$o_{d} = 96 - \\left( 31 \\times 3 - 2 + 1 \\times 2 + 1 \\right)$$\n$$o_{d} = 96 - \\left( 93 - 2 + 2 + 1 \\right)$$\n$$o_{d} = 96 - 94 = 2$$\n约束条件 $0 \\leq o_{d}  s_{d}$ 即 $0 \\leq 2  3$，该条件得到满足。\n\n因此，为达到期望的输出维度，所需的输出填充向量为 $(o_{h}, o_{w}, o_{d}) = (1, 3, 2)$。",
            "answer": "$$\\boxed{\\begin{pmatrix} 1  3  2 \\end{pmatrix}}$$"
        },
        {
            "introduction": "一个设计上容量过剩（overcomplete）的自编码器，如果不加约束，很容易学会一种“无用的”恒等映射，即将输入原封不动地复制到输出，而没有学到任何有意义的数据结构。为了引导模型学习紧凑且信息丰富的放射组学特征，正则化是必不可少的。本练习通过对比两种核心的正则化策略——权重衰减（weight decay）与激活值正则化（activity regularization），来挑战您对它们在阻止平凡解和促进有效信息压缩方面根本机制的理解。",
            "id": "4530388",
            "problem": "一个放射组学小组在一个三维（$3$D）卷积自编码器上，使用未标记的计算机断层扫描（CT）图像块进行训练，以学习无监督特征，这些特征后续将用于肿瘤分层。设编码器为 $f_{\\theta}:\\mathbb{R}^{H\\times W\\times D}\\rightarrow \\mathbb{R}^{m}$，为输入图像块 $x\\in\\mathbb{R}^{H\\times W\\times D}$ 生成潜码 $z=f_{\\theta}(x)$；解码器为 $g_{\\phi}:\\mathbb{R}^{m}\\rightarrow \\mathbb{R}^{H\\times W\\times D}$，生成重构图像 $\\hat{x}=g_{\\phi}(z)$。重构损失是均方误差（MSE），$L_{\\text{rec}}(x,\\hat{x})=\\|x-\\hat{x}\\|_{2}^{2}$。该模型被有意设计为高容量模型：每层使用许多通道，并且 $m$ 不小于一个图像块中的输入体素数量，因此可以表示一个近似恒等的映射。\n\n他们比较了在训练期间应用的两种正则化策略：\n- 对所有卷积核进行权重衰减，$R_{W}=\\lambda\\|W\\|_{2}^{2}$，其中 $W$ 叠加了所有可训练的权重，且 $\\lambda0$。\n- 对潜码进行活动正则化，$R_{z}=\\gamma\\|z\\|_{1}$，其中 $\\gamma0$ 对每个样本的编码器输出应用。\n\n假设数据包含扫描仪和生理噪声，这些噪声可以理想化为体素级别的加性 $0$ 均值波动，并且不使用跳跃连接。根据核心定义，考虑每种正则化器如何与（i）在 $3$D 卷积下表示类恒等映射的难易程度，以及（ii）从 $x$ 到 $z$ 的有效信息流相互作用。哪种陈述最能解释在这种高容量设置下，哪种正则化器更直接地防止了平凡的恒等解并鼓励了紧凑的放射组学表示？\n\nA. 权重衰减更有效，因为 $3$D 卷积中的恒等映射必然使用大数值的 delta-like 核，其 $\\ell_{2}$ 范数随深度增加而增长，因此 $R_{W}$ 会强烈惩罚恒等映射并阻断信息流。\n\nB. 对 $z$ 的活动 $\\ell_{1}$ 正则化更有效，因为它直接在 $z$ 上施加稀疏性，减少了每个样本从 $x$ 传递到 $z$ 的信息量；一个精确的恒等映射需要传输几乎所有的体素自由度，这与一个强的 $\\|z\\|_{1}$ 惩罚不兼容。\n\nC. 它们在防止恒等映射方面基本等效，因为缩小 $W$ 或缩小 $z$ 都通过逐层缩减激活值的相同机制来降低 MSE 重构能力。\n\nD. 在卷积自编码器中，权重衰减更可取，因为 $\\|W\\|_{2}^{2}$ 惩罚参数数量，因此即使在 $m$ 很大时也能强制形成一个狭窄的有效瓶颈，而 $\\|z\\|_{1}$ 主要风险是产生死单元，而不限制传输的信息。",
            "solution": "用户希望我分析一个关于在放射组学背景下，应用于 $3$D 卷积自编码器的正则化方法的问题。我必须首先验证问题陈述，如果有效，则提供详细的解决方案并评估给定的选项。\n\n### 问题验证\n\n我将首先根据指定程序验证问题陈述。\n\n**步骤 1：提取已知信息**\n- **模型：** 一个三维（$3$D）卷积自编码器。\n- **应用：** 在未标记的计算机断层扫描（CT）图像块上进行训练，以学习无监督特征，用于肿瘤分层。\n- **编码器：** $f_{\\theta}:\\mathbb{R}^{H\\times W\\times D}\\rightarrow \\mathbb{R}^{m}$，其中 $z=f_{\\theta}(x)$ 是输入图像块 $x\\in\\mathbb{R}^{H\\times W\\times D}$ 的潜码。\n- **解码器：** $g_{\\phi}:\\mathbb{R}^{m}\\rightarrow \\mathbb{R}^{H\\times W\\times D}$，其中 $\\hat{x}=g_{\\phi}(z)$ 是重构结果。\n- **损失函数：** 重构损失是均方误差（MSE），$L_{\\text{rec}}(x,\\hat{x})=\\|x-\\hat{x}\\|_{2}^{2}$。\n- **模型容量：** 模型是“高容量”的，每层有许多通道。潜码维度 $m$ 不小于输入体素的数量，即 $m \\ge H \\times W \\times D$。这意味着可以表示一个近似恒等的映射。\n- **正则化策略 1：** 对所有卷积核进行权重衰减，$R_{W}=\\lambda\\|W\\|_{2}^{2}$，其中 $\\lambda0$。$W$ 代表所有可训练的权重。\n- **正则化策略 2：** 对潜码进行活动正则化，$R_{z}=\\gamma\\|z\\|_{1}$，其中 $\\gamma0$，按样本应用。\n- **数据特征：** 数据包含体素级别的加性 $0$ 均值波动（噪声）。\n- **架构约束：** 不使用跳跃连接。\n- **问题：** 问题询问哪种陈述最能解释哪种正则化器（$R_{W}$ 或 $R_{z}$）在这种特定的高容量设置下，更直接地防止了平凡的恒等解并鼓励了紧凑的放射组学表示。\n\n**步骤 2：使用提取的已知信息进行验证**\n- **科学基础：** 该问题植根于深度学习这一成熟领域，特别是其在医学成像（放射组学）中的应用。卷积自编码器、过完备表示（$m \\ge H \\times W \\times D$）、均方误差损失、权重衰减（参数上的 $\\ell_2$ 正则化）和活动正则化（激活值上的 $\\ell_1$ 正则化）都是标准且定义明确的概念。该设置描述了无监督表示学习中的一个常见挑战。该问题在科学上是合理的。\n- **良构性：** 问题要求在一个特定的、定义明确的场景中（在高容量自编码器中防止平凡解），对两种不同正则化器的*机制*进行定性比较。“平凡恒等解”和“紧凑表示”等术语在此上下文中具有明确的含义。问题的结构旨在引发对这些正则化器如何运作的理解，从而得出一个独特且有意义的概念性答案。该问题是良构的。\n- **客观性：** 问题陈述使用机器学习和计算机视觉中常见的精确技术语言表达。没有主观或含糊的术语。前提是关于模型和训练设置的客观事实。\n- **缺陷清单：**\n    1.  **科学/事实不健全：** 无。该设置是过完备自编码器的一个典型例子。\n    2.  **不可形式化/不相关：** 无。问题与主题直接相关，所有概念都可以形式化。\n    3.  **不完整/矛盾的设置：** 无。所有必要的组件都已定义：模型架构类型、损失、容量条件和正则化器。条件 $m \\ge H \\times W \\times D$ 至关重要且陈述恰当。\n    4.  **不切实际/不可行：** 无。这种设置在研究中经常用于从成像数据中学习特征。\n    5.  **病态/结构不良：** 无。问题清晰，并将分析引向机制的直接性和紧凑性的目标。\n    6.  **伪深刻/琐碎/同义反复：** 无。参数正则化和活动正则化之间的区别是神经网络中一个基本且非琐碎的概念。\n    7.  **超出科学可验证性：** 无。这些正则化器的效果在经验上和理论上都是可验证的。\n\n**步骤 3：结论与行动**\n问题陈述有效。我将继续推导解决方案。\n\n### 解决方案推导\n\n问题的核心是在过完备自编码器的背景下，比较两种正则化策略：权重衰减（$R_W$）和潜码稀疏性（$R_z$）。过完备自编码器是指潜码维度 $m$ 大于或等于输入维度（$H \\times W \\times D$）的自编码器。这类模型是“高容量”的，可以轻易地学习到一个平凡的恒等映射，$g_{\\phi}(f_{\\theta}(x)) \\approx x$，方法是简单地将输入信息复制到潜空间中，然后再复制出来。这样做可以完美地最小化重构损失 $L_{\\text{rec}}$，但得到的表示 $z$ 毫无用处，因为它没有捕捉到数据的任何底层结构。正则化在此处的作用是防止这种平凡解，并迫使模型学习一个“紧凑”或有意义的表示。数据中也含有噪声，一个平凡的恒等映射会学会重构这些噪声，而一个好的模型应该通过捕捉信号并忽略噪声来学习对输入进行去噪。\n\n让我们分析每种正则化器的机制。\n\n**1. 活动正则化 ($R_{z}=\\gamma\\|z\\|_{1}$):**\n这种正则化器对每个样本的潜码 $z$ 的 $\\ell_1$-范数施加惩罚。$\\ell_1$-范数定义为 $\\|z\\|_{1} = \\sum_{i=1}^{m} |z_i|$。从优化理论（例如，在 LASSO 回归中）中我们熟知，对向量的 $\\ell_1$-范数进行惩罚会鼓励稀疏性，这意味着它的许多分量将被驱动为恰好为零。\n\n- **机制：** 总损失函数为 $L = L_{\\text{rec}} + R_z = \\|x - \\hat{x}\\|_{2}^{2} + \\gamma\\|z\\|_{1}$。优化器现在必须平衡两个相互竞争的目标：准确的重构（低 $L_{\\text{rec}}$）和稀疏的潜码（低 $R_z$）。\n- **对恒等映射的影响：** 一个平凡的恒等映射要求潜码 $z$ 携带输入图像块 $x$ 中存在的所有信息。在过完备设置中，有很多方法可以做到这一点，但它们通常涉及一个密集的（非稀疏的）$z$。为了让 $z$ 编码 $x$ 的所有精细细节和噪声，它的 $m$ 个分量中必须有许多是非零的。一个密集的 $z$ 会产生一个大的 $\\|z\\|_{1}$ 惩罚。因此，$R_z$ 项通过增加使用许多潜在单元的成本，直接抵消了形成平凡恒等映射的趋势。\n- **对表示的影响：** 通过强制 $z$ 稀疏，模型被迫学习一个更高效的编码。它必须选择其“基函数”（由解码器表示）的一个小子集来重构输入。这鼓励了潜在特征对应于数据中有意义的、重复出现的模式，从而产生一个“紧凑”且通常是解耦的表示。这是稀疏编码的核心原则。\n\n**2. 权重衰减 ($R_{W}=\\lambda\\|W\\|_{2}^{2}$):**\n这种正则化器对模型的可训练权重 $W$ 的平方 $\\ell_2$-范数施加惩罚。$\\ell_2$-范数是 $\\|W\\|_{2} = (\\sum w_{i}^{2})^{1/2}$。这鼓励学习算法找到具有较小权重值的解。\n\n- **机制：** 总损失函数是 $L = L_{\\text{rec}} + R_W = \\|x - \\hat{x}\\|_{2}^{2} + \\lambda\\|W\\|_{2}^{2}$。正则化器作用于函数 $f_\\theta$ 和 $g_\\phi$ 的参数，而不是直接作用于表示 $z$。它对 $z$ 的影响是间接的。\n- **对恒等映射的影响：** 问题在于学习一个恒等映射是否需要大权重，从而被 $R_W$ 惩罚。在深度卷积网络中，可以近似一个类恒等变换。对于单层，一个中心的 delta 核（例如，一个 $3 \\times 3 \\times 3$ 的核，中心为 $1$，其他地方为 $0$）可以对单个通道执行类恒等操作。这样一个核的平方 $\\ell_2$ 范数是 $1^2 = 1$，这是一个非常小的值。即使在深度网络中，一个类恒等映射也不一定需要大权重。模型可以通过中等权重的组合学习一个复杂的函数来近似恒等。因此，$R_W$ 不一定或不强烈地惩罚恒等解。它的主要作用是促进从输入到输出的“更平滑”的映射，这有助于泛化，并可能顺带抑制对高频噪声的拟合。\n- **对表示的影响：** 通过鼓励较小的权重，权重衰减限制了所学函数 $f_\\theta$ 和 $g_\\phi$ 的有效复杂性。这可以防止对 $x$ 中的噪声过拟合。然而，它并不直接在潜码 $z$ 上施加像稀疏性这样的结构性约束。作为小权重的副作用，$z$ 的量级可能会变小，但它不会像使用 $\\ell_1$ 惩罚那样被强制变得稀疏或紧凑。这种影响不那么直接。\n\n**比较与结论：**\n活动正则化 $R_z = \\gamma\\|z\\|_{1}$ *直接*作用于信息通道，即潜码 $z$。它直接强制施加稀疏性约束，这与需要传递来自 $x$ 的所有信息的平凡恒等映射从根本上是不兼容的。这直接促进了一种紧凑的、基于部分的表示。\n权重衰减 $R_W = \\lambda\\|W\\|_{2}^{2}$ 通过正则化函数空间（模型参数）来*间接*作用。虽然它可以帮助防止过拟合，但它并没有提供强大或直接的压力来对抗恒等映射，也没有明确地在潜码上强制执行像稀疏性这样的紧凑结构。\n\n因此，在这种高容量设置下，对于既定目标而言，活动 $\\ell_1$ 正则化是更直接、更有效的方法。\n\n### 逐项分析\n\n**A. 权重衰减更有效，因为 $3$D 卷积中的恒等映射必然使用大数值的 delta-like 核，其 $\\ell_{2}$ 范数随深度增加而增长，因此 $R_{W}$ 会强烈惩罚恒等映射并阻断信息流。**\n- **评估：** 这个陈述基于一个错误的前提。一个“delta-like 核”是卷积层近似通道上恒等映射的好方法，但它的 $\\ell_{2}$ 范数非常*小*。对于一个只有一个值为 $1$、其余全为 $0$ 的核，其平方 $\\ell_2$ 范数为 $1$。这不是一个“大数值”的核。其范数“随深度增加而增长”的说法也未经证实，并且对于恒等映射而言通常不成立。因此，$R_W$ 强烈惩罚恒等映射的推理是有缺陷的。\n- **结论：** **不正确**。\n\n**B. 对 $z$ 的活动 $\\ell_{1}$ 正则化更有效，因为它直接在 $z$ 上施加稀疏性，减少了每个样本从 $x$ 传递到 $z$ 的信息量；一个精确的恒等映射需要传输几乎所有的体素自由度，这与一个强的 $\\|z\\|_{1}$ 惩罚不兼容。**\n- **评估：** 这个陈述准确地描述了 $\\ell_1$ 活动正则化的机制。它正确地指出正则化*直接*应用于潜码 $z$ 以强制稀疏性。它正确地推断出，一个平凡的恒等映射需要一个密集的、信息丰富的潜码，这会产生很高的 $\\ell_1$ 惩罚。在最小化重构误差和最小化对 $z$ 的 $\\ell_1$ 惩罚之间的这种冲突，迫使模型学习一个紧凑、高效的表示。整个陈述在逻辑上和技术上都是合理的。\n- **结论：** **正确**。\n\n**C. 它们在防止恒等映射方面基本等效，因为缩小 $W$ 或缩小 $z$ 都通过逐层缩减激活值的相同机制来降低 MSE 重构能力。**\n- **评估：** 这个陈述错误地将两种正则化器的机制等同起来。虽然两者都可能导致较小的激活值幅度，但它们的结构性影响根本不同。权重衰减（对 $W$ 的 $\\ell_2$ 正则化）鼓励所有权重都变小，导致一种普遍的、弥散的收缩和一个更平滑的映射。活动正则化（对 $z$ 的 $\\ell_1$ 正则化）鼓励 $z$ 的许多分量恰好为零，同时允许少数分量较大，从而产生稀疏、选择性的表示。后者是约束信息瓶颈和鼓励“基于部分”编码的更直接的方法。它们不等效，并且在它们对表示施加的结构方面，其作用机制也不同。\n- **结论：** **不正确**。\n\n**D. 在卷积自编码器中，权重衰减更可取，因为 $\\|W\\|_{2}^{2}$ 惩罚参数数量，因此即使在 $m$ 很大时也能强制形成一个狭窄的有效瓶颈，而 $\\|z\\|_{1}$ 主要风险是产生死单元，而不限制传输的信息。**\n- **评估：** 这个陈述包含两个重大错误。首先，$\\|W\\|_{2}^{2}$（权重衰减）惩罚的是参数的*值的大小*，而不是参数的*数量*。对参数数量的惩罚与 $\\ell_0$ 正则化有关。其次，声称 $\\|z\\|_{1}$ 有产生死单元的风险*而不限制传输的信息*是自相矛盾的。$\\ell_1$ 惩罚的目的和效果恰恰是通过迫使大多数通道为零（或对于给定样本是“死的”）来限制信息，从而只通过少数活动通道传输信息。这是一种直接的信息限制方法。\n- **结论：** **不正确**。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "无监督特征学习的成功不仅在于训练模型，还在于评估和提炼所学到的特征。一个理想的特征集应当是信息丰富且高度非冗余的。本练习模拟了放射组学研究中的一个关键后续步骤：通过计算特征之间的相关性来量化冗余，并根据一个明确的策略来剪除那些与其它自学习特征或已有的“手工”特征高度相关的维度，从而构建一个更精简、更具解释性的模型。",
            "id": "4530325",
            "problem": "每个数据集都提供了两个特征矩阵：一个是由自动编码器学习到的潜在表示，以及一组手工制作的放射组学特征。在放射组学的背景下，自动编码器用于无监督特征学习，生成潜在维度，这些维度能够在没有标签的情况下总结图像衍生数据中的模式。为了评估冗余并提出一种保留非冗余信息的剪枝策略，请使用以下基本依据。\n\n设 $Z \\in \\mathbb{R}^{n \\times d_z}$ 表示 $n$ 个样本和 $d_z$ 个潜在维度的潜在特征矩阵，设 $H \\in \\mathbb{R}^{n \\times d_h}$ 表示具有 $d_h$ 个特征的手工特征矩阵。对于任意特征向量 $x \\in \\mathbb{R}^{n}$，定义样本均值为\n$$\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i,\n$$\n样本方差为\n$$\ns_x^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2,\n$$\n样本标准差（Standard Deviation (SD)）为 $s_x = \\sqrt{s_x^2}$。对于两个特征向量 $x, y \\in \\mathbb{R}^{n}$，定义样本协方差\n$$\n\\operatorname{cov}(x,y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}),\n$$\n以及皮尔逊积矩相关系数（PPMCC）\n$$\nr_{x,y} = \\frac{\\operatorname{cov}(x,y)}{s_x s_y},\n$$\n当 $s_x  0$ 且 $s_y  0$ 时。如果 $s_x = 0$ 或 $s_y = 0$，则在此公式中相关性未定义。\n\n冗余度通过相关性的大小来评估。给定一个冗余阈值 $\\tau \\in [0,1]$，对潜在维度实施以下剪枝策略：\n- 零方差剪枝：如果一个潜在维度 $z_j$ 的方差 $s_{z_j} = 0$，则标记索引 $j$ 进行剪枝。\n- 潜在-潜在冗余：对于任意一对潜在维度 $z_i$ 和 $z_j$（其中 $i \\neq j$），如果 $|r_{z_i,z_j}| \\ge \\tau$，则标记样本方差 $s_{z}^2$ 较小的索引进行剪枝。如果在微小的数值容差范围内 $s_{z_i}^2 = s_{z_j}^2$，则标记较大的索引 $\\max(i,j)$ 进行剪枝。\n- 潜在-手工冗余：对于任意潜在维度 $z_i$ 和手工特征 $h_k$，如果 $|r_{z_i,h_k}| \\ge \\tau$，则标记索引 $i$ 进行剪枝。如果 $s_{h_k} = 0$，则将 $r_{z_i,h_k}$ 视为未定义，并且不应仅基于该对进行剪枝。\n\n所有特征索引均使用基于 $0$ 的索引。您的程序必须将上述策略应用于测试套件中的每个数据集，并为每个数据集返回待剪枝的潜在索引的排序列表。\n\n测试套件参数值：\n- 情况 $1$ (一般情况):\n  $$\n  Z^{(1)} = \\begin{bmatrix}\n  1  2  5 \\\\\n  2  4  4 \\\\\n  3  6  3 \\\\\n  4  8  2 \\\\\n  5  10  1\n  \\end{bmatrix},\\quad\n  H^{(1)} = \\begin{bmatrix}\n  1.1  0 \\\\\n  2.2  0 \\\\\n  3.3  1 \\\\\n  4.4  0 \\\\\n  5.5  0\n  \\end{bmatrix},\\quad\n  \\tau^{(1)} = 0.95.\n  $$\n- 情况 $2$ (边界条件，其中 $|r|$ 等于阈值):\n  $$\n  Z^{(2)} = \\begin{bmatrix}\n  1  4 \\\\\n  2  3 \\\\\n  3  2 \\\\\n  4  1\n  \\end{bmatrix},\\quad\n  H^{(2)} = \\begin{bmatrix}\n  0.5 \\\\\n  0.6 \\\\\n  0.1 \\\\\n  0.2\n  \\end{bmatrix},\\quad\n  \\tau^{(2)} = 1.0.\n  $$\n- 情况 $3$ (边缘情况，包含一个零方差潜在特征和一个零方差手工特征):\n  $$\n  Z^{(3)} = \\begin{bmatrix}\n  3  1  0 \\\\\n  3  0  1 \\\\\n  3  1  2 \\\\\n  3  0  3 \\\\\n  3  1  4\n  \\end{bmatrix},\\quad\n  H^{(3)} = \\begin{bmatrix}\n  0  1 \\\\\n  0  2 \\\\\n  0  1 \\\\\n  0  2 \\\\\n  0  1\n  \\end{bmatrix},\\quad\n  \\tau^{(3)} = 0.8.\n  $$\n- 情况 $4$ (负的跨组相关性超过阈值且潜在方差不相等):\n  $$\n  Z^{(4)} = \\begin{bmatrix}\n  1  1 \\\\\n  2  1 \\\\\n  3  2 \\\\\n  4  2 \\\\\n  5  3 \\\\\n  6  3\n  \\end{bmatrix},\\quad\n  H^{(4)} = \\begin{bmatrix}\n  -1 \\\\\n  -2 \\\\\n  -3 \\\\\n  -4 \\\\\n  -5 \\\\\n  -6\n  \\end{bmatrix},\\quad\n  \\tau^{(4)} = 0.95.\n  $$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素是对应一个情况的被剪枝潜在索引的列表。例如，输出格式如下\n$$\n[ [\\text{情况 1 的索引}], [\\text{情况 2 的索引}], [\\text{情况 3 的索引}], [\\text{情况 4 的索引}] ].\n$$\n条目必须是使用基于 $0$ 的索引的整数列表。不需要单位，因为所有量都是无量纲的。不涉及角度。任何小数阈值都必须如上所示以十进制形式提供（例如，$\\tau = 0.95$）。",
            "solution": "该问题要求在放射组学的背景下，为自动编码器学习的潜在维度制定并实施一种特征剪枝策略。其目标是通过将潜在特征相互比较，以及与一组既定的手工放射组学特征进行比较来减少冗余。通过仅保留信息最丰富且最独特的特征，该过程对于构建简约而稳健的机器学习模型至关重要。该剪枝策略基于对基本统计原理的严格应用。\n\n指定的核心统计工具是样本方差和皮尔逊积矩相关系数（PPMCC）。对于一个包含 $n$ 个样本的特征向量 $x \\in \\mathbb{R}^{n}$，样本方差 $s_x^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2$ 衡量了特征围绕其均值 $\\bar{x}$ 的变异性或离散程度。方差为零的特征是恒定的，不携带任何用于区分样本的信息。PPMCC，$r_{x,y} = \\frac{\\operatorname{cov}(x,y)}{s_x s_y}$，衡量了两个特征向量 $x$ 和 $y$ 之间线性关系的强度和方向。其取值范围从 $-1$（完全负线性相关）到 $+1$（完全正线性相关），其中 $0$ 表示没有线性相关性。当相关性的绝对值 $|r_{x,y}|$ 很高时，即认定存在冗余。\n\n所提供的剪枝策略是一种确定性算法，它使用给定的冗余阈值 $\\tau$ 应用于潜在特征矩阵 $Z \\in \\mathbb{R}^{n \\times d_z}$ 和手工特征矩阵 $H \\in \\mathbb{R}^{n \\times d_h}$。该算法包括三个不同的阶段，待剪枝的索引集合是每个阶段识别出的索引的并集。\n\n1.  **零方差剪枝**：第一步是识别并标记任何方差为零（$s_{z_j}^2 = 0$）的潜在维度 $z_j$ 进行剪枝。这类特征对于所有样本都具有相同的值，因此不具信息量。这是一个标准且必要的数据预处理步骤。\n\n2.  **潜在-潜在冗余分析**：此阶段处理学习到的潜在特征集内部的多重共线性问题。对于每一对唯一的潜在维度 $(z_i, z_j)$（其中 $i \\neq j$），计算它们的PPMCC，$r_{z_i,z_j}$。如果 $|r_{z_i,z_j}| \\ge \\tau$，则认为这两个特征是冗余的。为了决定剪枝哪一个，该策略采用一种基于信息的标准：移除样本方差（$s_z^2$）较小的特征，其原则是该特征解释了数据中较少的变异性。这样可以保留捕获更多信息的特征。在方差相等的情况下，应用一种确定性的平局决胜规则：剪枝索引较大的特征，即 $\\max(i, j)$，以确保结果的唯一性。仅当两个特征的方差都非零时，才执行此检查。\n\n3.  **潜在-手工冗余分析**：最后一个阶段评估所学特征的新颖性。将每个潜在维度 $z_i$ 与每个手工特征 $h_k$ 进行比较。如果相关性大小 $|r_{z_i,h_k}|$ 达到或超过阈值 $\\tau$，则意味着自动编码器学到的特征已经被手工特征集捕获。在这种情况下，潜在特征 $z_i$ 被标记为待剪枝，因为它没有提供超出既定放射组学特征已知信息之外的新信息。该策略正确地规定，如果一个手工特征 $h_k$ 的方差为零（$s_{h_k}=0$），则相关性未定义，该对应关系不能作为剪枝 $z_i$ 的依据。\n\n完整的算法涉及系统地应用这三条规则。使用集合（set）数据结构来收集待剪枝的潜在特征的索引是合适的，因为它可以自动处理一个索引被多次标记为待剪枝的情况。最终输出是该集合中唯一索引的排序列表。此过程确保了对特征空间进行全面且有原则的缩减。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and print the results\n    in the specified format.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"Z\": np.array([\n                [1, 2, 5],\n                [2, 4, 4],\n                [3, 6, 3],\n                [4, 8, 2],\n                [5, 10, 1]\n            ], dtype=float),\n            \"H\": np.array([\n                [1.1, 0],\n                [2.2, 0],\n                [3.3, 1],\n                [4.4, 0],\n                [5.5, 0]\n            ], dtype=float),\n            \"tau\": 0.95\n        },\n        {\n            \"Z\": np.array([\n                [1, 4],\n                [2, 3],\n                [3, 2],\n                [4, 1]\n            ], dtype=float),\n            \"H\": np.array([\n                [0.5],\n                [0.6],\n                [0.1],\n                [0.2]\n            ], dtype=float),\n            \"tau\": 1.0\n        },\n        {\n            \"Z\": np.array([\n                [3, 1, 0],\n                [3, 0, 1],\n                [3, 1, 2],\n                [3, 0, 3],\n                [3, 1, 4]\n            ], dtype=float),\n            \"H\": np.array([\n                [0, 1],\n                [0, 2],\n                [0, 1],\n                [0, 2],\n                [0, 1]\n            ], dtype=float),\n            \"tau\": 0.8\n        },\n        {\n            \"Z\": np.array([\n                [1, 1],\n                [2, 1],\n                [3, 2],\n                [4, 2],\n                [5, 3],\n                [6, 3]\n            ], dtype=float),\n            \"H\": np.array([\n                [-1],\n                [-2],\n                [-3],\n                [-4],\n                [-5],\n                [-6]\n            ], dtype=float),\n            \"tau\": 0.95\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        pruned_list = prune_latent_features(case[\"Z\"], case[\"H\"], case[\"tau\"])\n        results.append(pruned_list)\n\n    # Use map(str, ...) and join to format the list of lists correctly.\n    # e.g., for [[1, 2], [3]], this produces '[1, 2],[3]'\n    # The f-string then adds the outer brackets to get '[[1, 2],[3]]'\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef prune_latent_features(Z, H, tau):\n    \"\"\"\n    Applies the pruning policy to latent features based on redundancy.\n\n    Args:\n        Z (np.ndarray): Latent feature matrix (n_samples, d_z).\n        H (np.ndarray): Handcrafted feature matrix (n_samples, d_h).\n        tau (float): Redundancy threshold.\n\n    Returns:\n        list: A sorted list of 0-based indices of latent features to prune.\n    \"\"\"\n    if Z.ndim == 1:\n        Z = Z.reshape(-1, 1)\n    if H.ndim == 1:\n        H = H.reshape(-1, 1)\n\n    n_samples, d_z = Z.shape\n    d_h = H.shape[1] if H.size > 0 else 0\n    \n    # Use a small tolerance for floating-point comparisons\n    TOLERANCE = 1e-9\n    \n    pruned_indices = set()\n\n    # Calculate sample variances (ddof=1 for n-1 denominator)\n    latent_variances = np.var(Z, axis=0, ddof=1)\n    if d_h > 0:\n      handcrafted_variances = np.var(H, axis=0, ddof=1)\n    \n    # 1. Zero-variance pruning\n    for j in range(d_z):\n        if latent_variances[j]  TOLERANCE:\n            pruned_indices.add(j)\n\n    # 2. Latent-latent redundancy\n    for i in range(d_z):\n        for j in range(i + 1, d_z):\n            # Only calculate correlation if variances are non-zero\n            if latent_variances[i] > TOLERANCE and latent_variances[j] > TOLERANCE:\n                # np.corrcoef with two 1D arrays returns a 2x2 matrix\n                r = np.corrcoef(Z[:, i], Z[:, j])[0, 1]\n                \n                if np.abs(r) >= tau:\n                    var_i = latent_variances[i]\n                    var_j = latent_variances[j]\n                    \n                    if np.abs(var_i - var_j)  TOLERANCE:\n                        # Tie-break by pruning the higher index\n                        pruned_indices.add(j)\n                    elif var_i  var_j:\n                        pruned_indices.add(i)\n                    else:\n                        pruned_indices.add(j)\n\n    # 3. Latent-handcrafted redundancy\n    if d_h > 0:\n      for i in range(d_z):\n          for k in range(d_h):\n              # Only calculate correlation if variances are non-zero\n              if latent_variances[i] > TOLERANCE and handcrafted_variances[k] > TOLERANCE:\n                  r = np.corrcoef(Z[:, i], H[:, k])[0, 1]\n                  if np.abs(r) >= tau:\n                      pruned_indices.add(i)\n\n    return sorted(list(pruned_indices))\n\nsolve()\n```"
        }
    ]
}