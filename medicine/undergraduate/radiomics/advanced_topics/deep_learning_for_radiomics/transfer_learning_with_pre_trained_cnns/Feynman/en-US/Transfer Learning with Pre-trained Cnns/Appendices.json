{
    "hands_on_practices": [
        {
            "introduction": "Successfully applying a pre-trained model to a new domain like radiomics requires a carefully designed training strategy. A naive end-to-end training approach can lead to \"catastrophic forgetting,\" where the valuable features learned by the backbone are corrupted by large, noisy gradients from the randomly initialized classifier head. This exercise  challenges you to design a principled two-phase training schedule, a cornerstone of effective transfer learning, by first stabilizing the classifier head and then cautiously fine-tuning the backbone to avoid this pitfall.",
            "id": "4568525",
            "problem": "You are building a radiomics classifier to predict lesion malignancy from computed tomography (CT) patches. You use a Convolutional Neural Network (CNN) backbone pre-trained on natural images, denoted as a feature extractor map $f_{b}(\\cdot;\\theta_{b})$, followed by a randomly initialized fully connected classifier head $f_{h}(\\cdot;\\theta_{h})$. Let the overall model be $f(x;\\theta)=f_{h}(f_{b}(x;\\theta_{b});\\theta_{h})$. You train with cross-entropy loss $\\mathcal{L}(\\theta)$ on a dataset of $n \\approx 400$ patients, with a mini-batch size $b \\approx 8$. Assume that you will use stochastic gradient descent type updates of the form $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}\\mathcal{L}$, and that gradients are obtained by backpropagation through the composition, so that the gradient with respect to the backbone parameters propagates via the chain rule through the head. Assume Batch Normalization (BN) layers are present in the backbone.\n\nFrom first principles, reason about gradient flow magnitudes and variance in the early and late stages of training, and choose a two-phase training schedule that first trains the classifier head with a higher learning rate and then fine-tunes the backbone with a lower learning rate. The schedule should be justified by how gradient magnitudes and their variance affect expected parameter change $\\|\\Delta \\theta\\|$ and the risk of catastrophic forgetting in $\\theta_{b}$, especially given limited labeled data and small batch size.\n\nWhich option best matches these constraints?\n\n- A. Phase $1$: Freeze $\\theta_{b}$, set a higher learning rate $\\eta_{h}^{(1)} \\approx 10^{-3}$ for $\\theta_{h}$ with weight decay $\\approx 10^{-4}$, keep BN layers in the backbone in evaluation mode (freeze running statistics and affine parameters), and train until validation loss plateaus. Phase $2$: Unfreeze the last $2$ backbone stages and fine-tune them with a lower learning rate $\\eta_{b}^{(2)} \\approx 10^{-5}$ while keeping a moderate learning rate for the head $\\eta_{h}^{(2)} \\approx 5\\times 10^{-4}$; keep BN running statistics frozen due to small $b$, and use early stopping to limit drift in $\\theta_{b}$.\n\n- B. Phase $1$: Train all layers end-to-end from the start with a single high learning rate $\\eta^{(1)} \\approx 10^{-3}$ to quickly escape poor local minima; Phase $2$: Increase the learning rate to $\\eta^{(2)} \\approx 10^{-2}$ to accelerate convergence once the loss decreases, and enable BN updates throughout because they regularize training with small $b$.\n\n- C. Phase $1$: Freeze $\\theta_{h}$ and fine-tune the entire backbone with a low learning rate $\\eta_{b}^{(1)} \\approx 10^{-4}$ so features adapt to the radiomics domain before learning the classifier; Phase $2$: Unfreeze $\\theta_{h}$ and train the head only with a high learning rate $\\eta_{h}^{(2)} \\approx 10^{-3}$, allowing BN layers to update running statistics in both phases to better match the new domain.\n\n- D. Phase $1$: Freeze $\\theta_{b}$ and train the head with a very low learning rate $\\eta_{h}^{(1)} \\approx 10^{-5}$ for many epochs to avoid overfitting; Phase $2$: Unfreeze all layers and continue with the same very low learning rate $\\eta^{(2)} \\approx 10^{-5}$ for stability, keeping BN layers fully trainable to compensate for the small learning rate.\n\nSelect the single best option and be prepared to justify it using gradient descent update magnitudes, the chain rule of backpropagation, and the risk of catastrophic forgetting in the pre-trained backbone under small-data radiomics conditions.",
            "solution": "The validity of the problem statement is hereby examined.\n\n### Step 1: Extract Givens\n- **Model Architecture**: A composite function $f(x;\\theta)=f_{h}(f_{b}(x;\\theta_{b});\\theta_{h})$, where $f_{b}(\\cdot;\\theta_{b})$ is a Convolutional Neural Network (CNN) backbone pre-trained on natural images, and $f_{h}(\\cdot;\\theta_{h})$ is a randomly initialized fully connected classifier head.\n- **Task**: Radiomics classification of lesion malignancy from computed tomography (CT) patches.\n- **Dataset**: $n \\approx 400$ patients.\n- **Training Details**:\n    - Loss Function: Cross-entropy loss, $\\mathcal{L}(\\theta)$.\n    - Optimizer: Stochastic gradient descent (SGD) type updates, $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}\\mathcal{L}$.\n    - Batch Size: $b \\approx 8$.\n    - Gradient Calculation: Standard backpropagation through the composite model.\n- **Backbone Feature**: Contains Batch Normalization (BN) layers.\n- **Implicit Goal**: Devise a two-phase training schedule that is optimal under the given constraints, with a specific focus on justifying the choice based on gradient flow, parameter update magnitudes, and the risk of catastrophic forgetting.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem describes a canonical transfer learning scenario in medical image analysis. Using a pre-trained CNN, managing learning rates for different parts of a network, and addressing challenges like catastrophic forgetting and small batch sizes are all standard, well-established concepts in applied deep learning. The setup is scientifically and technically sound.\n- **Well-Posedness**: The problem is well-posed. It asks for the selection and justification of the best strategy from a given set of options, based on fundamental principles of machine learning. The constraints ($n \\approx 400$, $b \\approx 8$) provide a clear context for evaluating the trade-offs involved in each strategy.\n- **Objectivity**: The problem is stated in objective, technical language. It requires reasoning from first principles of gradient-based optimization and deep learning, not subjective opinion.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-defined and realistic problem in the domain of applied machine learning for radiomics. I will proceed with a detailed solution.\n\n### Derivation from First Principles\n\nThe core challenge is to adapt a pre-trained model to a new, small dataset with a different data distribution (natural images vs. CT scans) without destroying the valuable information encoded in the pre-trained weights $\\theta_b$. This requires careful management of parameter updates $\\Delta \\theta = -\\eta \\nabla_{\\theta}\\mathcal{L}$.\n\n**Initial State Analysis:**\n1.  **Backbone Parameters $\\theta_b$**: These are initialized to a highly effective point in the parameter space for general feature extraction, learned from a large dataset.\n2.  **Head Parameters $\\theta_h$**: These are initialized randomly. Consequently, the initial output of the model $f(x;\\theta)$ will be random, leading to a large initial loss $\\mathcal{L}(\\theta)$.\n3.  **Gradient Magnitudes**:\n    - The gradient with respect to the head, $\\nabla_{\\theta_h}\\mathcal{L}$, will be large in magnitude because the head's output is far from the target labels.\n    - The gradient for the backbone is calculated via the chain rule: $\\nabla_{\\theta_b}\\mathcal{L} = \\frac{\\partial f_b}{\\partial \\theta_b} \\frac{\\partial f_h}{\\partial f_b} \\frac{\\partial \\mathcal{L}}{\\partial f_h}$. Since the final term, representing the error signal from the head, is large and based on a random mapping, the resulting gradient $\\nabla_{\\theta_b}\\mathcal{L}$ will also be large, noisy, and not necessarily pointing in a direction that improves feature representation for the new task.\n4.  **Gradient Variance**: With a very small batch size of $b \\approx 8$, a mini-batch gradient is a high-variance estimator of the true gradient over the full dataset ($n \\approx 400$). The stochasticity is high.\n\n**Phase 1: Initial Training**\n\nThe primary objective of Phase $1$ is to train the randomly initialized head $f_h$ to perform a sensible classification based on the features provided by the frozen backbone $f_b$.\n\n-   **Risk of Catastrophic Forgetting**: If we were to unfreeze $\\theta_b$ and train end-to-end from the beginning with a significant learning rate, the large and noisy gradients would propagate into the backbone. This would cause large updates $\\|\\Delta \\theta_b\\| = \\eta \\|\\nabla_{\\theta_b}\\mathcal{L}\\|$, drastically altering the pre-trained weights and destroying the learned hierarchical features. This is the definition of catastrophic forgetting.\n-   **Optimal Phase 1 Strategy**: To mitigate this risk, the backbone parameters $\\theta_b$ must be frozen. This sets $\\nabla_{\\theta_b}\\mathcal{L} = 0$, completely protecting the pre-trained weights. Training should focus exclusively on $\\theta_h$. Since $\\nabla_{\\theta_h}\\mathcal{L}$ is large, a moderately high learning rate (e.g., $\\eta_h^{(1)} \\approx 10^{-3}$) is appropriate to allow the head to learn the new mapping efficiently. A very low learning rate would be unnecessarily slow.\n-   **Handling Batch Normalization (BN)**: BN layers normalize activations based on running statistics (mean and variance) computed during pre-training. The statistical properties of CT images are very different from natural images. However, updating these running statistics with a small batch size of $b \\approx 8$ would yield highly unstable and noisy estimates, degrading performance. Therefore, it is standard practice to keep the BN layers in evaluation mode, which means freezing their running statistics and using the ones from pre-training. The affine parameters ($\\gamma$, $\\beta$) of the BN layers are part of $\\theta_b$ and are thus also frozen in this phase.\n-   **Regularization**: Given the small dataset ($n \\approx 400$), overfitting is a major concern. Applying weight decay to the trainable parameters ($\\theta_h$) is a standard and effective regularization technique.\n\n**Phase 2: Fine-Tuning**\n\nOnce the head $f_h$ has been trained to a reasonable degree (e.g., validation loss has plateaued), the error signal $\\frac{\\partial \\mathcal{L}}{\\partial f_h}$ becomes smaller and more meaningful. It is now safer to update the backbone parameters $\\theta_b$ to adapt them to the specifics of the CT data.\n\n-   **Optimal Phase 2 Strategy**: This \"fine-tuning\" step must be done cautiously.\n    -   **Differential Learning Rate**: The learning rate for the backbone, $\\eta_b^{(2)}$, must be very small (e.g., $\\eta_b^{(2)} \\approx 10^{-5}$), typically one or two orders of magnitude smaller than the head's learning rate. This ensures that the parameter updates $\\|\\Delta \\theta_b\\|$ are small, causing only a gentle drift from the pre-trained initialization rather than a disruptive jump.\n    -   **Layer-wise Unfreezing**: It is often beneficial to unfreeze only the later stages of the backbone. Early layers learn generic features (e.g., edges, textures) that are highly transferable, while later layers learn more task-specific features. Fine-tuning only the later, more specialized layers is a robust strategy that balances adaptation and preservation of general features.\n    -   **Head's Learning Rate**: The head must continue to adapt as the features it receives from the backbone are changing. Thus, $\\theta_h$ should still be trained, typically with a learning rate $\\eta_h^{(2)}$ that is less than $\\eta_h^{(1)}$ but greater than $\\eta_b^{(2)}$.\n    -   **BN and Early Stopping**: The small batch size issue persists, so BN running statistics should remain frozen. Due to the small dataset, continuing to monitor validation performance and using early stopping is critical to prevent overfitting and limit excessive drift in $\\theta_b$.\n\n### Option-by-Option Analysis\n\n-   **A. Phase $1$: Freeze $\\theta_{b}$, set a higher learning rate $\\eta_{h}^{(1)} \\approx 10^{-3}$ for $\\theta_{h}$ with weight decay $\\approx 10^{-4}$, keep BN layers in the backbone in evaluation mode..., and train until validation loss plateaus. Phase $2$: Unfreeze the last $2$ backbone stages and fine-tune them with a lower learning rate $\\eta_{b}^{(2)} \\approx 10^{-5}$ while keeping a moderate learning rate for the head $\\eta_{h}^{(2)} \\approx 5\\times 10^{-4}$; keep BN running statistics frozen due to small $b$, and use early stopping to limit drift in $\\theta_{b}$.**\n    -   This option perfectly aligns with the derived principles. Phase $1$ correctly isolates the head for training with an appropriate learning rate and correctly handles BN layers to prevent catastrophic forgetting. Phase $2$ implements cautious fine-tuning with a very low, differential learning rate, correctly justifies freezing BN statistics due to the small batch size $b$, and incorporates best practices like partial unfreezing and early stopping.\n    -   **Verdict: Correct.**\n\n-   **B. Phase $1$: Train all layers end-to-end from the start with a single high learning rate $\\eta^{(1)} \\approx 10^{-3}$... Phase $2$: Increase the learning rate to $\\eta^{(2)} \\approx 10^{-2}$...**\n    -   This strategy is fundamentally flawed. End-to-end training with a high learning rate from the start will cause catastrophic forgetting of the pre-trained weights $\\theta_b$ due to the large, noisy gradients from the random head $\\theta_h$. Increasing the learning rate in Phase $2$ is contrary to all standard practices and will likely lead to training instability and divergence. Enabling BN updates with $b \\approx 8$ is also ill-advised.\n    -   **Verdict: Incorrect.**\n\n-   **C. Phase $1$: Freeze $\\theta_{h}$ and fine-tune the entire backbone with a low learning rate $\\eta_{b}^{(1)} \\approx 10^{-4}$... Phase $2$: Unfreeze $\\theta_{h}$ and train the head only...**\n    -   This sequence is logically incoherent. In Phase $1$, freezing the randomly initialized head $\\theta_h$ means it cannot learn. Without a learning head, there is no meaningful error signal to guide the adaptation of the backbone. The gradients flowing to $\\theta_b$ would be based on a fixed random projection and would be useless for task-specific adaptation.\n    -   **Verdict: Incorrect.**\n\n-   **D. Phase $1$: Freeze $\\theta_{b}$ and train the head with a very low learning rate $\\eta_{h}^{(1)} \\approx 10^{-5}$... Phase $2$: Unfreeze all layers and continue with the same very low learning rate $\\eta^{(2)} \\approx 10^{-5}$... keeping BN layers fully trainable...**\n    -   This strategy is suboptimal and contains errors. Using a very low learning rate of $\\eta_h^{(1)} \\approx 10^{-5}$ to train the random head from scratch would be extremely slow and inefficient. While freezing $\\theta_b$ is correct, the learning rate for $\\theta_h$ should be higher. Furthermore, keeping BN layers \"fully trainable\" (i.e., updating running statistics) is incorrect for the small batch size of $b \\approx 8$, and the justification (\"to compensate for the small learning rate\") is nonsensical.\n    -   **Verdict: Incorrect.**\n\nBased on the analysis from first principles, Option A is the only one that describes a methodologically sound, robust, and state-of-the-art training strategy for transfer learning under the specified constraints.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Data augmentation is a powerful tool for improving model generalization, but its application in radiomics demands careful consideration of the underlying data physics. Unlike in natural images, transformations applied to quantitative medical images like CT scans must be label-preserving to be valid, meaning they must not alter the diagnostic information. This practice  will test your ability to distinguish between appropriate, physically-motivated augmentations and those that would corrupt the clinical information encoded in the images, a critical skill for building reliable medical models.",
            "id": "4568487",
            "problem": "A radiomics pipeline uses transfer learning from a Residual Network (ResNet) pretrained Convolutional Neural Network (CNN) originally trained on natural images to classify the malignancy of lung nodules in Computed Tomography (CT) scans. Each input is a cropped region of interest (ROI) centered on a single nodule, resampled to isotropic voxel spacing and windowed to a fixed lung window. The clinical label indicates whether the nodule is malignant or benign and is determined by intrinsic tissue radiodensity patterns and morphology rather than global orientation. The goal is to choose augmentation families that preserve clinical label invariance while providing variability that helps fine-tune the pretrained CNN.\n\nUse the following foundational facts and definitions:\n- The clinical label is a function $y(I)$ of the underlying physical properties of the imaged lesion. A transformation $T$ is label preserving if $y(T(I))=y(I)$ for all images $I$ of the lesion under consideration.\n- The Hounsfield Unit (HU) is defined by $$\\mathrm{HU} = 1000 \\frac{\\mu - \\mu_{\\mathrm{water}}}{\\mu_{\\mathrm{water}}},$$ where $\\mu$ is the linear attenuation coefficient and $\\mu_{\\mathrm{water}}$ is that of water. Monotonic or nonlinear remapping of image intensities that breaks this calibration can alter the physical meaning of HU values.\n- Rigid motions (translations, rotations, reflections) preserve Euclidean geometry of the lesion, whereas non-rigid deformations can change shape descriptors (e.g., spiculation, margin sharpness) that are clinically predictive.\n- Acquisition noise can be modeled as small additive perturbations that do not change the expected HU value of a tissue class.\n\nWhich augmentation options below are label-preserving and appropriate for transfer learning in this CT-based radiomics task, and which should be excluded because they alter HU semantics or lesion geometry? Select all that apply.\n\nA. Apply small in-plane rigid motions: rotations of up to $\\pm 5^\\circ$ and translations of up to $\\pm 3$ voxels, performed after resampling to isotropic voxel spacing, with a constraint that the ROI fully contains the lesion post-transform.\n\nB. Apply global linear intensity remapping to simulate contrast variability: multiply all voxel intensities by a factor $\\alpha = 1.2$ and add an offset $\\beta = 100$ (in HU) before windowing.\n\nC. Add zero-mean Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ to voxel intensities with $\\sigma$ chosen such that $\\sigma \\ll W$, where $W$ is the window width used for lung windowing.\n\nD. Apply random elastic deformations parameterized by a displacement field with maximum amplitude $a = 5$ voxels to increase shape variability of the lesion.\n\nE. Apply global histogram equalization or gamma correction with $\\gamma = 0.6$ to improve local contrast prior to windowing.\n\nF. Apply left-right mirroring (reflection across the sagittal plane) while preserving voxel spacing metadata and ensuring the ROI remains centered on the lesion.\n\nSelect all correct options.",
            "solution": "The problem requires an evaluation of several data augmentation techniques for a radiomics pipeline that uses a pre-trained Convolutional Neural Network (CNN) to classify lung nodule malignancy in Computed Tomography (CT) scans. The core requirement is that any augmentation must be **label-preserving**, meaning it does not alter the fundamental characteristics of the nodule upon which the clinical label (malignant or benign) is based. The problem specifies that the label is determined by \"intrinsic tissue radiodensity patterns and morphology\" and is invariant to \"global orientation\".\n\nWe will evaluate each proposed augmentation technique against these criteria.\n\n**A. Apply small in-plane rigid motions: rotations of up to $\\pm 5^\\circ$ and translations of up to $\\pm 3$ voxels, performed after resampling to isotropic voxel spacing, with a constraint that the ROI fully contains the lesion post-transform.**\n\n- **Analysis**: Rotations and translations are rigid motions. By definition, rigid motions preserve the Euclidean geometry of an object. This means all morphological features of the nodule, such as its shape, size, margin sharpness, and spiculation, are perfectly preserved. Furthermore, rigid motions only change the spatial position of voxels; they do not alter their intensity values. Therefore, the \"intrinsic tissue radiodensity patterns\" are also preserved. The transformation is thus label-preserving, i.e., $y(T(I)) = y(I)$. Introducing small variations in orientation and position helps the CNN become robust to minor variations in nodule pose and segmentation centering, which is a desirable property.\n- **Verdict**: **Correct**. This is a standard, appropriate, and label-preserving augmentation.\n\n**B. Apply global linear intensity remapping to simulate contrast variability: multiply all voxel intensities by a factor $\\alpha = 1.2$ and add an offset $\\beta = 100$ (in HU) before windowing.**\n\n- **Analysis**: This applies a linear transformation $I' = \\alpha I + \\beta$ to the voxel intensities, which are in Hounsfield Units (HU). The HU scale is a calibrated, physical scale defined as $\\mathrm{HU} = 1000 \\frac{\\mu - \\mu_{\\mathrm{water}}}{\\mu_{\\mathrm{water}}}$, where $\\mu$ is the material's linear attenuation coefficient. This transformation breaks the physical calibration. For example, water ($0$ HU) would be mapped to $1.2 \\times 0 + 100 = 100$ HU, and air (approx. $-1000$ HU) would be mapped to $1.2 \\times (-1000) + 100 = -1100$ HU. The problem states that the label is based on \"radiodensity patterns\" and explicitly warns that remapping intensities can \"alter the physical meaning of HU values.\" Since specific tissue types (e.g., calcifications, soft tissue) are defined by their characteristic HU ranges, this transformation alters the very data used for classification in a non-physical way. It is therefore not label-preserving.\n- **Verdict**: **Incorrect**. This augmentation violates the physical semantics of CT data.\n\n**C. Add zero-mean Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ to voxel intensities with $\\sigma$ chosen such that $\\sigma \\ll W$, where $W$ is the window width used for lung windowing.**\n\n- **Analysis**: This technique simulates the random electronic noise inherent in the CT acquisition process. The problem statement notes that this is a valid physical model: \"Acquisition noise can be modeled as small additive perturbations that do not change the expected HU value of a tissue class.\" Adding zero-mean Gaussian noise does not systematically alter the radiodensity of tissues but rather introduces realistic variability. The expected value of the voxel intensity for any tissue remains unchanged. This makes the model more robust to varying noise levels in real-world scans. The condition that the standard deviation $\\sigma$ is much smaller than the window width $W$ ensures the perturbation is realistic and does not overwhelm the signal. This is a label-preserving augmentation that improves model generalization.\n- **Verdict**: **Correct**. This is a physically motivated and beneficial augmentation.\n\n**D. Apply random elastic deformations parameterized by a displacement field with maximum amplitude $a = 5$ voxels to increase shape variability of the lesion.**\n\n- **Analysis**: Elastic deformations are non-rigid transformations. The problem explicitly warns that \"non-rigid deformations can change shape descriptors (e.g., spiculation, margin sharpness) that are clinically predictive.\" Malignancy in lung nodules is often correlated with morphological features like irregular or spiculated margins. Applying a random elastic deformation would warp the nodule's shape, potentially making a smooth, benign-looking nodule appear spiculated, or vice-versa. This directly alters a key biomarker and is therefore not a label-preserving transformation, as $y(T(I))$ may not equal $y(I)$.\n- **Verdict**: **Incorrect**. This augmentation alters the predictive morphological features of the lesion.\n\n**E. Apply global histogram equalization or gamma correction with $\\gamma = 0.6$ to improve local contrast prior to windowing.**\n\n- **Analysis**: Both histogram equalization and gamma correction are non-linear intensity remapping techniques. Histogram equalization forces the intensity distribution to become uniform, completely destroying the quantitative information in the HU scale. Gamma correction applies a power-law function ($I' \\propto I^\\gamma$), which is also non-linear. Both methods break the linear relationship between voxel intensity and physical tissue density. As stated in the problem, such \"nonlinear remapping of image intensities that breaks this calibration\" is problematic because it alters the physical meaning of the data upon which the \"radiodensity patterns\" depend. These techniques are inappropriate for quantitative medical imaging tasks.\n- **Verdict**: **Incorrect**. These augmentations fundamentally alter the physical meaning of the HU values.\n\n**F. Apply left-right mirroring (reflection across the sagittal plane) while preserving voxel spacing metadata and ensuring the ROI remains centered on the lesion.**\n\n- **Analysis**: Reflection (mirroring) is a rigid motion (an improper rotation). Like other rigid motions, it preserves all geometric properties of the nodule, including its shape, size, and margin morphology. It also preserves all voxel intensities, simply rearranging their spatial locations. The problem states that the clinical label is invariant to \"global orientation,\" and a left-right reflection is a change in orientation. Therefore, this transformation is guaranteed to be label-preserving. It is a common and highly effective augmentation technique that leverages problem-specific symmetries to increase the effective size of the training dataset.\n- **Verdict**: **Correct**. This is a label-preserving rigid motion that exploits a known invariance of the problem.",
            "answer": "$$\\boxed{ACF}$$"
        },
        {
            "introduction": "After developing a sophisticated model, how do we prove it is truly better than a simpler baseline, especially when performance varies across different data sources like hospitals or scanners? Simply comparing average performance metrics can be misleading. In this exercise , you will step into the role of a machine learning researcher to implement a formal experimental protocol using paired statistical tests to rigorously quantify performance gains, a crucial step in validating any new technique.",
            "id": "4568455",
            "problem": "You are tasked with formalizing and implementing an experiment to quantify transfer gains from domain adaptation in a multi-center radiomics classification setting using pre-trained Convolutional Neural Networks (CNNs). The experimental design compares a baseline model that transfers a pre-trained CNN encoder without domain alignment to models that incorporate domain adaptation modules intended to align feature distributions across imaging centers. You will evaluate transfer gains by ablating alignment modules and statistically testing the improvements in Area Under the Receiver Operating Characteristic Curve (AUC-ROC) across centers.\n\nBase concepts to be used include the following: the AUC-ROC is a probability-valued performance measure in $[0,1]$ that is invariant to class imbalance under the assumption of proper scoring; a paired comparison across centers treats each center as a matched unit, thus differences in performance between two models at the same center form paired observations; for small to moderate sample sizes with unknown variance, inference on paired mean differences proceeds via the Student’s $t$-distribution under the assumption that center-wise differences are independent and identically distributed with finite variance.\n\nYou must perform the following steps purely in mathematical and algorithmic terms, without relying on any external data source:\n\n1. Define the transfer gain for a model $M$ relative to a baseline model $B$ at center $i$ as the paired difference $g_i = \\mathrm{AUC}_M(i) - \\mathrm{AUC}_B(i)$, with $\\mathrm{AUC}$ measured on held-out data from center $i$.\n2. For a given pair of models $(M_1, M_2)$, aggregate the paired differences $\\{d_i = \\mathrm{AUC}_{M_1}(i) - \\mathrm{AUC}_{M_2}(i)\\}$ across centers and conduct a one-sided paired Student’s $t$-test for the alternative hypothesis that the mean of $d_i$ is greater than $0$.\n3. You must handle the boundary case where the variance of $\\{d_i\\}$ is zero. In that case, define the $p$-value as $0$ if the common difference is strictly greater than $0$, and as $1$ otherwise. This convention corresponds to a limiting argument for the one-sided test and prevents undefined test statistics.\n4. Report only the $p$-values from the one-sided paired tests, rounded to six decimal places.\n\nYou are provided with three test scenarios, each reflecting a different cross-center evaluation regimen. For each scenario $s \\in \\{1,2,3\\}$, you are given four models evaluated across multiple centers:\n\n- Baseline transfer: a pre-trained CNN encoder frozen, no domain alignment.\n- Full domain adaptation (DA): pre-trained CNN with feature alignment across centers.\n- Ablation without adaptive batch normalization (BN): full DA with batch normalization adaptation removed.\n- Ablation without adversarial alignment: full DA with the adversarial alignment module removed.\n\nFor each scenario, compute the following three one-sided paired $t$-test $p$-values:\n\n- $p_{s,1}$ for Full DA versus Baseline (test whether Full DA has larger mean AUC than Baseline).\n- $p_{s,2}$ for Full DA versus No-BN (test whether Full DA has larger mean AUC than No-BN).\n- $p_{s,3}$ for Full DA versus No-Adversarial (test whether Full DA has larger mean AUC than No-Adversarial).\n\nUse only the provided AUC values. All AUCs are dimensionless and must be treated as decimals between $0$ and $1$.\n\nTest suite (AUCs by center):\n\n- Scenario $1$ with $8$ centers:\n  - Baseline: $\\{0.72, 0.75, 0.70, 0.78, 0.74, 0.69, 0.76, 0.73\\}$.\n  - Full DA: $\\{0.81, 0.83, 0.78, 0.86, 0.80, 0.77, 0.84, 0.80\\}$.\n  - No-BN: $\\{0.77, 0.79, 0.74, 0.82, 0.77, 0.73, 0.80, 0.77\\}$.\n  - No-Adversarial: $\\{0.76, 0.78, 0.73, 0.81, 0.75, 0.72, 0.79, 0.75\\}$.\n\n- Scenario $2$ with $3$ centers (small-sample boundary):\n  - Baseline: $\\{0.69, 0.71, 0.70\\}$.\n  - Full DA: $\\{0.74, 0.75, 0.73\\}$.\n  - No-BN: $\\{0.72, 0.73, 0.72\\}$.\n  - No-Adversarial: $\\{0.71, 0.72, 0.71\\}$.\n\n- Scenario $3$ with $6$ centers (zero-variance edge case):\n  - Baseline: $\\{0.80, 0.79, 0.82, 0.83, 0.81, 0.80\\}$.\n  - Full DA: $\\{0.80, 0.79, 0.82, 0.83, 0.81, 0.80\\}$.\n  - No-BN: $\\{0.80, 0.79, 0.82, 0.83, 0.81, 0.80\\}$.\n  - No-Adversarial: $\\{0.80, 0.79, 0.82, 0.83, 0.81, 0.80\\}$.\n\nYour program must compute the nine $p$-values $(p_{1,1}, p_{1,2}, p_{1,3}, p_{2,1}, p_{2,2}, p_{2,3}, p_{3,1}, p_{3,2}, p_{3,3})$ in that order and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each $p$-value rounded to six decimal places (for example, $\\left[0.012345,0.678901,\\dots\\right]$). No other output is permitted.",
            "solution": "We begin from first principles appropriate to statistical evaluation of cross-center transfer learning in radiomics using pre-trained Convolutional Neural Networks (CNNs). The Area Under the Receiver Operating Characteristic Curve (AUC-ROC), denoted $\\mathrm{AUC}$, is a performance metric valued in $[0,1]$ that can be interpreted as the probability that a randomly drawn positive instance receives a higher score than a randomly drawn negative instance under a fixed scoring function. In multi-center radiomics, differences in scanner protocols and populations induce domain shift, which motivates domain adaptation (DA) modules for feature alignment. To measure transfer gains, we compare models across centers using paired analysis, because for each center the same underlying classification task and distributional idiosyncrasies are present across models.\n\nLet there be $n$ centers indexed by $i \\in \\{1,\\dots,n\\}$. For two models $M_1$ and $M_2$, define the paired difference at center $i$ as\n$$\nd_i = \\mathrm{AUC}_{M_1}(i) - \\mathrm{AUC}_{M_2}(i).\n$$\nWe treat $\\{d_i\\}$ as independent and identically distributed realizations from a distribution with mean $\\mu_d$ and variance $\\sigma_d^2$. The null hypothesis for a one-sided superiority test is\n$$\nH_0: \\mu_d \\le 0,\n$$\nand the alternative hypothesis is\n$$\nH_1: \\mu_d > 0.\n$$\nUnder the assumption that the individual paired differences are drawn from a distribution with finite variance, we use the Student’s $t$-test for paired samples. Let\n$$\n\\bar{d} = \\frac{1}{n} \\sum_{i=1}^n d_i, \\quad s_d^2 = \\frac{1}{n-1} \\sum_{i=1}^n (d_i - \\bar{d})^2.\n$$\nWhen $s_d > 0$, the test statistic is\n$$\nt = \\frac{\\bar{d}}{s_d / \\sqrt{n}},\n$$\nwhich, under $H_0$ with unknown variance, follows a Student’s $t$-distribution with $n-1$ degrees of freedom in the sense that, if $\\mu_d = 0$, then\n$$\nt \\sim t_{n-1}.\n$$\nFor the one-sided alternative $H_1: \\mu_d > 0$, the $p$-value is computed as\n$$\np = \\Pr\\left(T_{n-1} \\ge t\\right),\n$$\nwhere $T_{n-1}$ is a Student’s $t$ random variable with $n-1$ degrees of freedom. Small values of $p$ provide evidence against $H_0$ in favor of $H_1$.\n\nA boundary condition arises when $s_d = 0$, which occurs if and only if $d_i = c$ for all $i$ for some constant $c$. In this case, the conventional $t$-statistic is undefined due to division by zero. We resolve this by a limiting argument consistent with the one-sided test: if $c > 0$, then the data are perfectly consistent with $H_1$ and we set $p = 0$; if $c \\le 0$, then we set $p = 1$, reflecting no evidence for $H_1$.\n\nAlgorithmic procedure for each scenario:\n1. For each of the three comparisons, namely Full Domain Adaptation (DA) versus Baseline, Full DA versus No-Batch-Normalization (No-BN), and Full DA versus No-Adversarial, compute the paired differences $d_i$ across centers.\n2. Compute $n$, $\\bar{d}$, and $s_d$. If $s_d = 0$:\n   - If $\\bar{d} > 0$, set $p = 0$.\n   - Else set $p = 1$.\n3. If $s_d > 0$, compute $t = \\bar{d} / (s_d / \\sqrt{n})$, degrees of freedom $\\nu = n - 1$, and the one-sided $p$-value $p = \\Pr(T_{\\nu} \\ge t)$.\n4. Round $p$ to six decimal places and store the result.\n5. Repeat for all three comparisons within each scenario and for all three scenarios.\n\nApplication to the provided test suite:\n- Scenario $1$ uses $n = 8$ centers with Baseline, Full DA, No-BN, and No-Adversarial AUCs as specified. The Full DA versus Baseline paired differences are all positive with small variability, yielding a large positive $t$ and a very small $p$. The Full DA versus No-BN differences are positive with small variability, producing a small $p$ as well. The Full DA versus No-Adversarial differences are all exactly $0.05$, so $s_d = 0$ and $\\bar{d} > 0$, giving $p = 0$ by the boundary rule.\n- Scenario $2$ uses $n = 3$ centers. The Full DA versus Baseline differences are positive and moderately sized, and the paired $t$-test yields a small one-sided $p$. Differences versus No-BN and versus No-Adversarial are positive, with corresponding small $p$-values.\n- Scenario $3$ uses $n = 6$ centers with all four models having identical AUCs. Therefore, for all three comparisons, $d_i = 0$ for all $i$, so $s_d = 0$ and $\\bar{d} = 0$, yielding $p = 1$ by the boundary rule.\n\nThe program constructs the arrays exactly as given, computes the nine one-sided paired $p$-values in the order $(p_{1,1}, p_{1,2}, p_{1,3}, p_{2,1}, p_{2,2}, p_{2,3}, p_{3,1}, p_{3,2}, p_{3,3})$, rounds them to six decimals, and prints a single line containing the comma-separated list enclosed in square brackets, as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef one_sided_paired_ttest_greater(x, y, tol=1e-12):\n    \"\"\"\n    Compute one-sided paired t-test p-value for H1: mean(x - y) > 0.\n    Handles zero-variance differences by a boundary convention:\n      - If all differences equal c > 0, return 0.0\n      - If all differences equal c <= 0, return 1.0\n    Parameters:\n        x, y: 1D arrays of equal length\n        tol: tolerance to detect zero variance in differences\n    Returns:\n        p-value (float)\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    d = x - y\n    n = d.size\n    mean_d = float(np.mean(d))\n    # Sample standard deviation with ddof=1; handle n==1 safely though not expected here\n    if n <= 1:\n        # With a single pair, if mean_d > 0, p=0, else p=1 under one-sided convention\n        return 0.0 if mean_d > 0 else 1.0\n    sd_d = float(np.std(d, ddof=1))\n    if sd_d <= tol:\n        # All differences effectively equal\n        return 0.0 if mean_d > 0 else 1.0\n    se = sd_d / np.sqrt(n)\n    t_stat = mean_d / se\n    df = n - 1\n    # One-sided p-value for greater alternative\n    p_val = float(stats.t.sf(t_stat, df))\n    # Numerical safety: bound into [0,1]\n    if p_val < 0.0:\n        p_val = 0.0\n    elif p_val > 1.0:\n        p_val = 1.0\n    return p_val\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Scenario 1: 8 centers\n    s1_baseline = np.array([0.72, 0.75, 0.70, 0.78, 0.74, 0.69, 0.76, 0.73])\n    s1_full =     np.array([0.81, 0.83, 0.78, 0.86, 0.80, 0.77, 0.84, 0.80])\n    s1_no_bn =    np.array([0.77, 0.79, 0.74, 0.82, 0.77, 0.73, 0.80, 0.77])\n    s1_no_adv =   np.array([0.76, 0.78, 0.73, 0.81, 0.75, 0.72, 0.79, 0.75])\n\n    # Scenario 2: 3 centers\n    s2_baseline = np.array([0.69, 0.71, 0.70])\n    s2_full =     np.array([0.74, 0.75, 0.73])\n    s2_no_bn =    np.array([0.72, 0.73, 0.72])\n    s2_no_adv =   np.array([0.71, 0.72, 0.71])\n\n    # Scenario 3: 6 centers (all identical AUCs)\n    s3_baseline = np.array([0.80, 0.79, 0.82, 0.83, 0.81, 0.80])\n    s3_full =     np.array([0.80, 0.79, 0.82, 0.83, 0.81, 0.80])\n    s3_no_bn =    np.array([0.80, 0.79, 0.82, 0.83, 0.81, 0.80])\n    s3_no_adv =   np.array([0.80, 0.79, 0.82, 0.83, 0.81, 0.80])\n\n    # Assemble scenarios\n    scenarios = [\n        (s1_baseline, s1_full, s1_no_bn, s1_no_adv),\n        (s2_baseline, s2_full, s2_no_bn, s2_no_adv),\n        (s3_baseline, s3_full, s3_no_bn, s3_no_adv),\n    ]\n\n    results = []\n    for (baseline, full, no_bn, no_adv) in scenarios:\n        # p_{s,1}: Full vs Baseline\n        p1 = one_sided_paired_ttest_greater(full, baseline)\n        # p_{s,2}: Full vs No-BN\n        p2 = one_sided_paired_ttest_greater(full, no_bn)\n        # p_{s,3}: Full vs No-Adversarial\n        p3 = one_sided_paired_ttest_greater(full, no_adv)\n        results.extend([p1, p2, p3])\n\n    # Round to six decimals and format\n    formatted = [f\"{p:.6f}\" for p in results]\n    print(f\"[{','.join(formatted)}]\")\n\nsolve()\n```"
        }
    ]
}