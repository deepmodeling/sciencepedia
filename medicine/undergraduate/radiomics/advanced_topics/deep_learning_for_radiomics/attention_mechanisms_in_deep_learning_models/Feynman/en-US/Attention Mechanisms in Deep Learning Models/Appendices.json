{
    "hands_on_practices": [
        {
            "introduction": "Understanding attention begins with its core computational block: the query-key-value mechanism. This practice provides a hands-on calculation of a cross-attention output, where information from a primary modality is used to query and integrate features from auxiliary sources . By manually performing the steps of scoring, scaling, and weighting, you will build an intuition for how attention dynamically selects and combines information in a multi-modal setting.",
            "id": "4529597",
            "problem": "In a multi-modality radiomics pipeline for lesion characterization, a single Computed Tomography (CT) radiomic signature is used to query complementary information from two auxiliary modalities: Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). The query, keys, and values are embedded in a common latent space of dimension $d=2$. The query vector is $Q=\\begin{bmatrix}1  1\\end{bmatrix}$, the key matrix is $K=\\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}$, and the value vector is $V=\\begin{bmatrix}2 \\\\ 5\\end{bmatrix}$, where the first component corresponds to MRI and the second to PET. According to the standard scaled similarity-based weighting used in attention mechanisms, cross-attention converts the dot-product similarities between $Q$ and the rows of $K$ into nonnegative weights that sum to one, and then aggregates $V$ as a weighted average. Using this mechanism with embedding dimension $d=2$, compute the cross-attention output for the CT query. Additionally, identify, based on the computed weights and value contributions, which modality (MRI or PET) contributes more to the final aggregated score. Provide the final aggregated score as an exact real-valued number with no units.",
            "solution": "The problem asks for the computation of a cross-attention output and an analysis of the contributions from different modalities. The underlying mechanism is the scaled dot-product attention, a standard component in transformer-based deep learning models. The formula for the attention output is given by:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V $$\nwhere $Q$ is the query matrix, $K$ is the key matrix, $V$ is the value matrix, and $d_k$ is the dimension of the key vectors.\n\nFirst, we must validate the problem statement.\nStep 1: Extract Givens.\n- The query vector, representing a CT signature, is $Q = \\begin{bmatrix}1  1\\end{bmatrix}$.\n- The key matrix, with rows corresponding to MRI and PET modalities, is $K = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}$.\n- The value vector, with components corresponding to MRI and PET, is $V = \\begin{bmatrix}2 \\\\ 5\\end{bmatrix}$.\n- The dimension of the latent space for keys and queries is $d = 2$.\n- The mechanism is scaled similarity-based weighting (scaled dot-product attention). The output is a weighted average of the components of $V$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded, as it uses a standard algorithm (attention mechanism) in a plausible context (multi-modal radiomics). It is well-posed, with all necessary data ($Q, K, V, d$) provided and a clear objective. The dimensions of the vectors and matrices are consistent for the required operations ($Q$ is $1 \\times 2$, $K$ is $2 \\times 2$, $V$ is $2 \\times 1$). The problem is objective, complete, and contains no contradictions or ambiguities. It is a valid problem.\n\nStep 3: Verdict and Action.\nThe problem is valid. We will proceed with the solution.\n\nThe calculation proceeds in several steps:\n\n1.  **Compute the dot-product similarity scores.**\n    The scores are the dot products of the query vector $Q$ with each key vector, which are the rows of the key matrix $K$. This operation is equivalent to the matrix multiplication of the query matrix $Q$ and the transpose of the key matrix $K^T$.\n    The key vectors are $K_1 = \\begin{bmatrix}1  0\\end{bmatrix}$ for MRI and $K_2 = \\begin{bmatrix}0  1\\end{bmatrix}$ for PET.\n    The transpose of the key matrix is:\n    $$ K^T = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}^T = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} $$\n    The scores vector is then computed as:\n    $$ \\text{scores} = Q K^T = \\begin{bmatrix}1  1\\end{bmatrix} \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} = \\begin{bmatrix}(1)(1) + (1)(0)  (1)(0) + (1)(1)\\end{bmatrix} = \\begin{bmatrix}1  1\\end{bmatrix} $$\n    The raw similarity score for MRI is $s_1 = 1$, and for PET is $s_2 = 1$.\n\n2.  **Scale the scores.**\n    The scores are scaled by dividing by the square root of the dimension of the key vectors, $d_k = d = 2$. The scaling factor is $\\frac{1}{\\sqrt{2}}$.\n    The scaled scores vector $z$ is:\n    $$ z = \\begin{bmatrix}z_1  z_2\\end{bmatrix} = \\begin{bmatrix}1 \\cdot \\frac{1}{\\sqrt{2}}  1 \\cdot \\frac{1}{\\sqrt{2}}\\end{bmatrix} = \\begin{bmatrix}\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}\\end{bmatrix} $$\n\n3.  **Compute the attention weights using the softmax function.**\n    The attention weights are calculated by applying the softmax function to the scaled scores. The weight $w_i$ for the $i$-th modality is given by $w_i = \\frac{\\exp(z_i)}{\\sum_{j} \\exp(z_j)}$.\n    For the MRI modality (first component):\n    $$ w_{MRI} = w_1 = \\frac{\\exp(z_1)}{\\exp(z_1) + \\exp(z_2)} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{\\exp(\\frac{1}{\\sqrt{2}}) + \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{1}{2} $$\n    For the PET modality (second component):\n    $$ w_{PET} = w_2 = \\frac{\\exp(z_2)}{\\exp(z_1) + \\exp(z_2)} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{\\exp(\\frac{1}{\\sqrt{2}}) + \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{1}{2} $$\n    The attention weights are equal, $w_{MRI} = 0.5$ and $w_{PET} = 0.5$. This indicates that the CT query vector $Q$ is equally similar to the key vectors for MRI and PET in this latent space representation.\n\n4.  **Compute the final aggregated score (attention output).**\n    The output is the weighted average of the value vector $V$, where the components of $V$ are $v_{MRI} = 2$ and $v_{PET} = 5$.\n    $$ \\text{Output} = \\sum_{i=1}^2 w_i v_i = w_{MRI} v_{MRI} + w_{PET} v_{PET} = \\left(\\frac{1}{2}\\right)(2) + \\left(\\frac{1}{2}\\right)(5) = 1 + \\frac{5}{2} = 1 + 2.5 = 3.5 $$\n    The final aggregated score is $3.5$.\n\n5.  **Identify the modality with the greater contribution.**\n    The contribution of each modality to the final score is its value multiplied by its attention weight, i.e., $w_i v_i$.\n    - Contribution from MRI: $w_{MRI} v_{MRI} = \\left(\\frac{1}{2}\\right)(2) = 1$.\n    - Contribution from PET: $w_{PET} v_{PET} = \\left(\\frac{1}{2}\\right)(5) = 2.5$.\n    Since $2.5  1$, the PET modality contributes more to the final aggregated score. This is because, while the attention mechanism found both modalities equally relevant (equal weights), the information content or signal represented by the value for PET ($v_{PET}=5$) is substantially larger than that for MRI ($v_{MRI}=2$).",
            "answer": "$$\\boxed{3.5}$$"
        },
        {
            "introduction": "Beyond spatial attention, models can also learn to focus on the most informative feature channels. This exercise delves into the architecture of a Squeeze-and-Excitation (SE) block, a popular form of channel attention, by asking you to calculate its parameter count . Understanding the relationship between the number of channels $C$ and the reduction ratio $r$ is key to designing efficient models that avoid overfitting, a common challenge in radiomics.",
            "id": "4529549",
            "problem": "In a convolutional neural network for radiomics, a channel attention module based on Squeeze-and-Excitation (SE) is inserted after a convolutional block with $C$ output channels to adaptively reweight channel-wise feature responses. The SE module performs global average pooling across spatial dimensions (which introduces no trainable parameters), followed by a two-layer Multilayer Perceptron (MLP) with a reduction ratio $r$, mapping $\\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{C/r} \\rightarrow \\mathbb{R}^{C}$. Each fully connected layer includes both weights and biases. The excitation uses a sigmoid nonlinearity, which introduces no trainable parameters.\n\nUsing the core definition that a fully connected layer mapping $\\mathbb{R}^{n_{\\mathrm{in}}} \\rightarrow \\mathbb{R}^{n_{\\mathrm{out}}}$ has $n_{\\mathrm{in}} n_{\\mathrm{out}}$ weights and $n_{\\mathrm{out}}$ biases, derive the total number of trainable parameters in the SE MLP in terms of $C$ and $r$. Then, for $C = 64$ and $r = 16$, compute the numerical value of this parameter count. Finally, based on your derived expression, reason from first principles how increasing $r$ affects parameter efficiency and overfitting risk in small radiomics cohorts with limited sample size, assuming $C$ is fixed.\n\nReport the numerical parameter count as an exact integer. No rounding is required, and no units are needed for the final numerical answer.",
            "solution": "The problem requires a three-part answer: first, a derivation of the total number of trainable parameters in a Squeeze-and-Excitation (SE) module's multilayer perceptron (MLP); second, a numerical calculation for specific values of $C$ and $r$; and third, a reasoned analysis of the effect of the reduction ratio $r$ on model characteristics. The problem is well-posed and scientifically sound, allowing for a direct solution.\n\nThe SE module's MLP consists of two fully connected (FC) layers. The total number of trainable parameters, denoted as $P_{\\text{total}}$, is the sum of the parameters from each of these two layers. We use the provided definition that a fully connected layer mapping from an input dimension $n_{\\mathrm{in}}$ to an output dimension $n_{\\mathrm{out}}$ has $n_{\\mathrm{in}} n_{\\mathrm{out}}$ weights and $n_{\\mathrm{out}}$ biases, for a total of $n_{\\mathrm{in}} n_{\\mathrm{out}} + n_{\\mathrm{out}}$ parameters.\n\nLet's analyze each layer of the MLP, which performs the mapping $\\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{C/r} \\rightarrow \\mathbb{R}^{C}$.\n\nFirst FC layer (Reduction stage):\nThis layer maps the input feature vector from a dimension of $C$ to a reduced dimension of $C/r$.\n- Input dimension: $n_{\\mathrm{in}} = C$.\n- Output dimension: $n_{\\mathrm{out}} = \\frac{C}{r}$.\nThe number of trainable weights in this layer is the product of the input and output dimensions:\n$$\nW_1 = n_{\\mathrm{in}} \\times n_{\\mathrm{out}} = C \\times \\frac{C}{r} = \\frac{C^2}{r}\n$$\nThe number of trainable biases is equal to the output dimension:\n$$\nB_1 = n_{\\mathrm{out}} = \\frac{C}{r}\n$$\nTherefore, the total number of parameters in the first FC layer, $P_1$, is:\n$$\nP_1 = W_1 + B_1 = \\frac{C^2}{r} + \\frac{C}{r}\n$$\n\nSecond FC layer (Expansion stage):\nThis layer maps the feature vector from the reduced dimension $C/r$ back to the original channel dimension $C$.\n- Input dimension: $n_{\\mathrm{in}} = \\frac{C}{r}$.\n- Output dimension: $n_{\\mathrm{out}} = C$.\nThe number of trainable weights in this layer is:\n$$\nW_2 = n_{\\mathrm{in}} \\times n_{\\mathrm{out}} = \\frac{C}{r} \\times C = \\frac{C^2}{r}\n$$\nThe number of trainable biases is:\n$$\nB_2 = n_{\\mathrm{out}} = C\n$$\nTherefore, the total number of parameters in the second FC layer, $P_2$, is:\n$$\nP_2 = W_2 + B_2 = \\frac{C^2}{r} + C\n$$\n\nThe total number of trainable parameters in the entire SE MLP is the sum of the parameters from both layers:\n$$\nP_{\\text{total}} = P_1 + P_2 = \\left(\\frac{C^2}{r} + \\frac{C}{r}\\right) + \\left(\\frac{C^2}{r} + C\\right)\n$$\nCombining terms, we get the final expression for the total number of parameters:\n$$\nP_{\\text{total}} = \\frac{2C^2}{r} + \\frac{C}{r} + C\n$$\nThis expression represents the total count of trainable parameters in the SE module's MLP in terms of the number of channels $C$ and the reduction ratio $r$.\n\nNext, we compute the numerical value for $C = 64$ and $r = 16$.\nSubstituting these values into the derived formula:\n$$\nP_{\\text{total}} = \\frac{2(64)^2}{16} + \\frac{64}{16} + 64\n$$\nFirst, we evaluate the terms:\n$64^2 = 4096$.\n$$\n\\frac{2(4096)}{16} = \\frac{8192}{16} = 512\n$$\n$$\n\\frac{64}{16} = 4\n$$\nSumming the components:\n$$\nP_{\\text{total}} = 512 + 4 + 64 = 580\n$$\nThus, for $C=64$ and $r=16$, the SE MLP contains $580$ trainable parameters.\n\nFinally, we reason from first principles how increasing the reduction ratio $r$ affects parameter efficiency and overfitting risk, assuming $C$ is fixed. Our derived expression for the total parameter count is $P_{\\text{total}}(r) = \\frac{2C^2+C}{r} + C$.\n\n1.  Effect on Parameter Efficiency:\n    Parameter efficiency refers to a model's ability to achieve high performance with a minimal number of parameters. In the expression $P_{\\text{total}}(r)$, the term $\\frac{2C^2+C}{r}$ is inversely proportional to $r$, while $C$ is a constant. As $r$ increases, the value of this fractional term decreases, thus reducing the overall parameter count $P_{\\text{total}}$. A model with fewer parameters is considered more parameter-efficient because it requires less memory and computational overhead. Therefore, increasing $r$ improves the parameter efficiency of the SE module.\n\n2.  Effect on Overfitting Risk:\n    Overfitting is a phenomenon where a model with high capacity (typically associated with a large number of parameters) learns the specific details and noise of the training data, leading to poor generalization to new, unseen data. In disciplines like radiomics, which often contend with small patient cohorts and limited sample sizes, overfitting is a significant concern. By increasing $r$, we reduce the total number of trainable parameters $P_{\\text{total}}$ in the SE module. This reduction in parameters lowers the model's overall capacity. A model with lower capacity is less able to memorize the training data and is forced to learn more robust, generalizable patterns. This acts as a form of implicit regularization. Consequently, increasing $r$ helps to mitigate the risk of overfitting, which is highly desirable for building robust models from small datasets.",
            "answer": "$$\n\\boxed{580}\n$$"
        },
        {
            "introduction": "A key advantage of attention mechanisms is their interpretability, which can be further enhanced by providing supervision during training. This practice demonstrates how to incorporate weak supervision, such as a Region of Interest (ROI) mask, to guide the model's focus by formulating an auxiliary attention loss . You will then explore a practical technique for balancing this new loss with the primary classification objective, a crucial step in preventing the model from being over-constrained and ensuring robust performance.",
            "id": "4529572",
            "problem": "A Convolutional Neural Network (CNN) for radiomics classification produces an attention vector $\\mathbf{a} = (a_{1}, \\dots, a_{N})$ over $N$ feature regions, where the attention is computed from attention logits $\\mathbf{z} = (z_{1}, \\dots, z_{N})$ via a softmax, so that $a_{i} = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{N} \\exp(z_{j})}$ and $\\sum_{i=1}^{N} a_{i} = 1$ with $a_{i} \\geq 0$. For each input, a weak Region of Interest (ROI) mask $\\mathbf{m} = (m_{1}, \\dots, m_{N})$ is available, with $m_{i} \\in \\{0, 1\\}$ indicating whether region $i$ is within the ROI.\n\nThe standard classification loss is the binary cross-entropy,\n$$\nL_{\\mathrm{cls}} = -\\left(y \\ln(p) + (1-y)\\ln(1-p)\\right),\n$$\nwhere $y \\in \\{0,1\\}$ is the ground-truth label and $p \\in (0,1)$ is the predicted probability from the model.\n\nStarting from the principle of risk minimization and the definition of expectation under a probability distribution, derive an auxiliary attention supervision loss $L_{\\mathrm{att}}$ that penalizes attention mass placed outside the ROI using the weak mask $\\mathbf{m}$. Then, form a combined loss\n$$\nL_{\\mathrm{total}} = L_{\\mathrm{cls}} + \\lambda L_{\\mathrm{att}},\n$$\nand explain how to choose the balancing coefficient $\\lambda  0$ to avoid over-constraining the attention relative to the classification objective by matching gradient scales.\n\nAssume the following scientifically plausible scenario for balancing on a held-out validation mini-batch: when $\\lambda = 1$, the Euclidean norm of the gradient of $L_{\\mathrm{cls}}$ with respect to the attention logits $\\mathbf{z}$ is measured to be $G_{\\mathrm{cls}} = 2.75$, and the Euclidean norm of the gradient of $L_{\\mathrm{att}}$ with respect to $\\mathbf{z}$ is $G_{\\mathrm{att}} = 0.44$. To avoid over-constraining, you aim to make the contribution of the attention supervision term to the gradient norm equal to a fraction $r = 0.3$ of the classification gradient norm, that is, the norm of the gradient of $\\lambda L_{\\mathrm{att}}$ with respect to $\\mathbf{z}$ should be $r G_{\\mathrm{cls}}$.\n\nUsing these quantities and first-principles reasoning, compute the value of $\\lambda$ that achieves this balance. Round your final numerical answer to four significant figures.",
            "solution": "The first task is to derive the auxiliary attention supervision loss, $L_{\\mathrm{att}}$, based on the principle of risk minimization. The attention vector $\\mathbf{a}$ can be interpreted as a probability distribution over the $N$ feature regions. We are asked to penalize attention mass placed outside the Region of Interest (ROI). The weak mask $\\mathbf{m}$ has elements $m_i = 1$ for regions inside the ROI and $m_i = 0$ for regions outside. A cost can be assigned to attending to each region: a cost of $c_i = 1$ for being outside the ROI and $c_i = 0$ for being inside. This cost function can be expressed compactly using the mask as $c_i = 1 - m_i$.\n\nAccording to the principle of risk minimization, the objective is to minimize the expected cost under the probability distribution defined by the attention weights $\\mathbf{a}$. The expectation of the cost $c$ is:\n$$\nE_{\\mathbf{a}}[c] = \\sum_{i=1}^{N} a_i c_i = \\sum_{i=1}^{N} a_i (1-m_i)\n$$\nThis expression represents the total attention weight allocated to all regions outside the specified ROI. Minimizing this value directly accomplishes the goal of penalizing attention outside the ROI. Therefore, we define the auxiliary loss $L_{\\mathrm{att}}$ as this expected cost:\n$$\nL_{\\mathrm{att}} = \\sum_{i=1}^{N} a_i (1-m_i)\n$$\nThe total loss for training the network is the sum of the classification loss and this auxiliary attention loss, weighted by a balancing coefficient $\\lambda  0$:\n$$\nL_{\\mathrm{total}} = L_{\\mathrm{cls}} + \\lambda L_{\\mathrm{att}}\n$$\nWhen training the model using gradient descent, the parameters (including the attention logits $\\mathbf{z}$) are updated based on the gradient of $L_{\\mathrm{total}}$. Due to the linearity of the gradient operator, the gradient of the total loss is the sum of the gradients of its components:\n$$\n\\nabla_{\\mathbf{z}} L_{\\mathrm{total}} = \\nabla_{\\mathbf{z}} (L_{\\mathrm{cls}} + \\lambda L_{\\mathrm{att}}) = \\nabla_{\\mathbf{z}} L_{\\mathrm{cls}} + \\lambda \\nabla_{\\mathbf{z}} L_{\\mathrm{att}}\n$$\nThe terms $\\nabla_{\\mathbf{z}} L_{\\mathrm{cls}}$ and $\\lambda \\nabla_{\\mathbf{z}} L_{\\mathrm{att}}$ are vectors that determine the direction of parameter updates. To prevent the attention-guiding objective from dominating the primary classification task, the magnitudes of these two gradient vectors should be balanced. A large $\\lambda$ could over-constrain the attention mechanism, potentially harming classification performance. The strategy proposed is to set $\\lambda$ such that the magnitude (Euclidean norm) of the gradient contribution from the attention loss, $||\\lambda \\nabla_{\\mathbf{z}} L_{\\mathrm{att}}||_2$, is a specified fraction $r$ of the magnitude of the gradient contribution from the classification loss, $||\\nabla_{\\mathbf{z}} L_{\\mathrm{cls}}||_2$.\n\nLet $G_{\\mathrm{cls}} = ||\\nabla_{\\mathbf{z}} L_{\\mathrm{cls}}||_2$ and $G_{\\mathrm{att}} = ||\\nabla_{\\mathbf{z}} L_{\\mathrm{att}}||_2$. The Euclidean norm of the gradient contribution from the attention term is:\n$$\n||\\lambda \\nabla_{\\mathbf{z}} L_{\\mathrm{att}}||_2 = |\\lambda| \\cdot ||\\nabla_{\\mathbf{z}} L_{\\mathrm{att}}||_2\n$$\nSince $\\lambda  0$, this becomes $\\lambda G_{\\mathrm{att}}$. The balancing condition is given as:\n$$\n\\lambda G_{\\mathrm{att}} = r G_{\\mathrm{cls}}\n$$\nWe can now solve for $\\lambda$:\n$$\n\\lambda = \\frac{r G_{\\mathrm{cls}}}{G_{\\mathrm{att}}}\n$$\nThe problem provides the measured values from a validation mini-batch: $G_{\\mathrm{cls}} = 2.75$, $G_{\\mathrm{att}} = 0.44$, and the desired ratio $r = 0.3$. Substituting these values:\n$$\n\\lambda = \\frac{0.3 \\times 2.75}{0.44}\n$$\nWe first compute the product in the numerator:\n$$\n0.3 \\times 2.75 = 0.825\n$$\nNext, we perform the division:\n$$\n\\lambda = \\frac{0.825}{0.44} = 1.875\n$$\nThe problem requires the final answer to be rounded to four significant figures. The calculated value $1.875$ has exactly four significant figures ($1$, $8$, $7$, $5$), so no further rounding is needed. The balancing coefficient should be set to $\\lambda = 1.875$.",
            "answer": "$$\\boxed{1.875}$$"
        }
    ]
}