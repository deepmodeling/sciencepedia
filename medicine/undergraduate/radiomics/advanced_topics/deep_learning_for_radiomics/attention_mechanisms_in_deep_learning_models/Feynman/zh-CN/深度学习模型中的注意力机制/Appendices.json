{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握注意力机制，必须从其核心计算开始。这个练习将带你动手实践缩放点积注意力（scaled dot-product attention）的完整流程，这是许多现代模型（如Transformer）的基石。通过一个多模态融合的假想场景，你将学习一个“查询”（query）向量如何与“键”（key）向量交互以计算权重，并最终加权聚合“值”（value）向量，从而直观地理解注意力机制是如何筛选和整合信息的。",
            "id": "4529597",
            "problem": "在一个用于病灶表征的多模态放射组学流程中，使用一个单一的计算机断层扫描（CT）放射组学特征，从两个辅助模态——磁共振成像（MRI）和正电子发射断层扫描（PET）——中查询互补信息。查询（query）、键（key）和值（value）被嵌入一个维度为 $d=2$ 的共同潜空间中。查询向量为 $Q=\\begin{bmatrix}1  1\\end{bmatrix}$，键矩阵为 $K=\\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}$，值向量为 $V=\\begin{bmatrix}2 \\\\ 5\\end{bmatrix}$，其中第一个分量对应MRI，第二个分量对应PET。根据注意力机制中使用的标准缩放相似度加权方法，交叉注意力将 $Q$ 与 $K$ 的行向量之间的点积相似度转换为和为1的非负权重，然后将 $V$ 聚合为加权平均值。使用此机制和嵌入维度 $d=2$，计算CT查询的交叉注意力输出。此外，根据计算出的权重和值的贡献，确定哪种模态（MRI或PET）对最终聚合分数的贡献更大。请提供最终的聚合分数，作为一个精确的实数，不带单位。",
            "solution": "该问题要求计算交叉注意力的输出，并分析不同模态的贡献。其底层机制是缩放点积注意力，这是基于Transformer的深度学习模型中的一个标准组件。注意力输出的公式如下：\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V $$\n其中 $Q$ 是查询矩阵，$K$ 是键矩阵，$V$ 是值矩阵，$d_k$ 是键向量的维度。\n\n首先，我们必须验证问题陈述。\n第1步：提取给定信息。\n- 代表CT特征的查询向量为 $Q = \\begin{bmatrix}1  1\\end{bmatrix}$。\n- 行向量分别对应MRI和PET模态的键矩阵为 $K = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}$。\n- 分量分别对应MRI和PET的值向量为 $V = \\begin{bmatrix}2 \\\\ 5\\end{bmatrix}$。\n- 键和查询的潜空间维度为 $d = 2$。\n- 机制是缩放相似度加权（缩放点积注意力）。输出是 $V$ 的分量的加权平均值。\n\n第2步：使用提取的给定信息进行验证。\n该问题在科学上是合理的，因为它在一个合理的背景（多模态放射组学）下使用了一个标准算法（注意力机制）。问题陈述清晰，所有必要的数据（$Q, K, V, d$）均已提供，目标明确。向量和矩阵的维度对于所需的操作是一致的（$Q$ 是 $1 \\times 2$，$K$ 是 $2 \\times 2$，$V$ 是 $2 \\times 1$）。该问题是客观、完整且没有矛盾或歧义的。这是一个有效的问题。\n\n第3步：结论与行动。\n问题有效。我们将继续进行求解。\n\n计算过程分几步进行：\n\n1.  **计算点积相似度分数。**\n    分数是查询向量 $Q$ 与每个键向量（即键矩阵 $K$ 的行）的点积。该操作等效于查询矩阵 $Q$ 与键矩阵转置 $K^T$ 的矩阵乘法。\n    键向量分别是 $K_1 = \\begin{bmatrix}1  0\\end{bmatrix}$（用于MRI）和 $K_2 = \\begin{bmatrix}0  1\\end{bmatrix}$（用于PET）。\n    键矩阵的转置是：\n    $$ K^T = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}^T = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} $$\n    然后计算分数向量：\n    $$ \\text{scores} = Q K^T = \\begin{bmatrix}1  1\\end{bmatrix} \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} = \\begin{bmatrix}(1)(1) + (1)(0)  (1)(0) + (1)(1)\\end{bmatrix} = \\begin{bmatrix}1  1\\end{bmatrix} $$\n    MRI的原始相似度分数为 $s_1 = 1$，PET的为 $s_2 = 1$。\n\n2.  **缩放分数。**\n    将分数除以键向量维度 $d_k = d = 2$ 的平方根进行缩放。缩放因子为 $\\frac{1}{\\sqrt{2}}$。\n    缩放后的分数向量 $z$ 是：\n    $$ z = \\begin{bmatrix}z_1  z_2\\end{bmatrix} = \\begin{bmatrix}1 \\cdot \\frac{1}{\\sqrt{2}}  1 \\cdot \\frac{1}{\\sqrt{2}}\\end{bmatrix} = \\begin{bmatrix}\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}\\end{bmatrix} $$\n\n3.  **使用softmax函数计算注意力权重。**\n    通过对缩放后的分数应用softmax函数来计算注意力权重。第 $i$ 个模态的权重 $w_i$ 由 $w_i = \\frac{\\exp(z_i)}{\\sum_{j} \\exp(z_j)}$ 给出。\n    对于MRI模态（第一个分量）：\n    $$ w_{MRI} = w_1 = \\frac{\\exp(z_1)}{\\exp(z_1) + \\exp(z_2)} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{\\exp(\\frac{1}{\\sqrt{2}}) + \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{1}{2} $$\n    对于PET模态（第二个分量）：\n    $$ w_{PET} = w_2 = \\frac{\\exp(z_2)}{\\exp(z_1) + \\exp(z_2)} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{\\exp(\\frac{1}{\\sqrt{2}}) + \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{1}{2} $$\n    注意力权重相等，$w_{MRI} = 0.5$ 且 $w_{PET} = 0.5$。这表明在该潜空间表示中，CT查询向量 $Q$ 与MRI和PET的键向量具有相同的相似度。\n\n4.  **计算最终聚合分数（注意力输出）。**\n    输出是值向量 $V$ 的加权平均值，其中 $V$ 的分量为 $v_{MRI} = 2$ 和 $v_{PET} = 5$。\n    $$ \\text{Output} = \\sum_{i=1}^2 w_i v_i = w_{MRI} v_{MRI} + w_{PET} v_{PET} = \\left(\\frac{1}{2}\\right)(2) + \\left(\\frac{1}{2}\\right)(5) = 1 + \\frac{5}{2} = 1 + 2.5 = 3.5 $$\n    最终聚合分数为 $3.5$。\n\n5.  **确定贡献更大的模态。**\n    每种模态对最终分数的贡献是其值乘以其注意力权重，即 $w_i v_i$。\n    - MRI的贡献：$w_{MRI} v_{MRI} = \\left(\\frac{1}{2}\\right)(2) = 1$。\n    - PET的贡献：$w_{PET} v_{PET} = \\left(\\frac{1}{2}\\right)(5) = 2.5$。\n    由于 $2.5 > 1$，PET模态对最终聚合分数的贡献更大。这是因为，尽管注意力机制发现两种模态同等相关（权重相等），但PET的值（$v_{PET}=5$）所代表的信息内容或信号远大于MRI的值（$v_{MRI}=2$）。",
            "answer": "$$\\boxed{3.5}$$"
        },
        {
            "introduction": "注意力机制不仅限于序列数据，它在图像分析中同样强大，尤其是在卷积神经网络（CNN）中用于增强特征图。本练习将聚焦于一种高效的通道注意力模块——压缩与激励（Squeeze-and-Excitation, SE）模块。通过推导其参数数量，你将探索架构设计（如缩减率 $r$）如何直接影响模型的参数效率和过拟合风险，这对于处理样本量有限的放射组学数据至关重要。",
            "id": "4529549",
            "problem": "在一个用于影像组学的卷积神经网络中，一个基于Squeeze-and-Excitation (SE)的通道注意力模块被插入到一个具有$C$个输出通道的卷积块之后，以自适应地重新加权逐通道的特征响应。SE模块首先在空间维度上执行全局平均池化（此过程不引入可训练参数），然后是一个具有缩减率$r$的两层多层感知机（MLP），其映射关系为 $\\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{C/r} \\rightarrow \\mathbb{R}^{C}$。每个全连接层都包含权重和偏置。激励过程使用sigmoid非线性激活函数，此过程也不引入可训练参数。\n\n根据核心定义，一个将$\\mathbb{R}^{n_{\\mathrm{in}}}$映射到$\\mathbb{R}^{n_{\\mathrm{out}}}$的全连接层具有$n_{\\mathrm{in}} n_{\\mathrm{out}}$个权重和$n_{\\mathrm{out}}$个偏置，请推导出SE MLP中可训练参数的总数（用$C$和$r$表示）。然后，对于$C = 64$和$r = 16$的情况，计算该参数数量的数值。最后，基于你推导出的表达式，从基本原理出发，论述在$C$固定的情况下，增加$r$如何影响在样本量有限的小样本影像组学队列中的参数效率和过拟合风险。\n\n报告数值参数量时应为精确整数。无需四舍五入，最终数值答案也无需单位。",
            "solution": "该问题要求分三部分作答：首先，推导Squeeze-and-Excitation (SE) 模块中多层感知机 (MLP) 的可训练参数总数；其次，根据给定的$C$和$r$值进行数值计算；第三，对缩减率$r$对模型特性的影响进行合理分析。该问题提问清晰且科学合理，可直接求解。\n\nSE模块的MLP由两个全连接 (FC) 层组成。可训练参数的总数，记为$P_{\\text{total}}$，是这两个层参数数量之和。我们使用题目给出的定义：一个从输入维度$n_{\\mathrm{in}}$映射到输出维度$n_{\\mathrm{out}}$的全连接层，有$n_{\\mathrm{in}} n_{\\mathrm{out}}$个权重和$n_{\\mathrm{out}}$个偏置，总参数数量为$n_{\\mathrm{in}} n_{\\mathrm{out}} + n_{\\mathrm{out}}$。\n\n我们来分析MLP的每一层，该MLP执行$\\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{C/r} \\rightarrow \\mathbb{R}^{C}$的映射。\n\n第一个FC层（缩减阶段）：\n该层将输入特征向量从$C$维映射到缩减后的$C/r$维。\n- 输入维度：$n_{\\mathrm{in}} = C$。\n- 输出维度：$n_{\\mathrm{out}} = \\frac{C}{r}$。\n该层的可训练权重数量是输入和输出维度的乘积：\n$$\nW_1 = n_{\\mathrm{in}} \\times n_{\\mathrm{out}} = C \\times \\frac{C}{r} = \\frac{C^2}{r}\n$$\n可训练偏置的数量等于输出维度：\n$$\nB_1 = n_{\\mathrm{out}} = \\frac{C}{r}\n$$\n因此，第一个FC层的总参数数量$P_1$为：\n$$\nP_1 = W_1 + B_1 = \\frac{C^2}{r} + \\frac{C}{r}\n$$\n\n第二个FC层（扩展阶段）：\n该层将特征向量从缩减后的$C/r$维映射回原始的通道维度$C$。\n- 输入维度：$n_{\\mathrm{in}} = \\frac{C}{r}$。\n- 输出维度：$n_{\\mathrm{out}} = C$。\n该层的可训练权重数量为：\n$$\nW_2 = n_{\\mathrm{in}} \\times n_{\\mathrm{out}} = \\frac{C}{r} \\times C = \\frac{C^2}{r}\n$$\n可训练偏置的数量为：\n$$\nB_2 = n_{\\mathrm{out}} = C\n$$\n因此，第二个FC层的总参数数量$P_2$为：\n$$\nP_2 = W_2 + B_2 = \\frac{C^2}{r} + C\n$$\n\n整个SE MLP的可训练参数总数是两层参数之和：\n$$\nP_{\\text{total}} = P_1 + P_2 = \\left(\\frac{C^2}{r} + \\frac{C}{r}\\right) + \\left(\\frac{C^2}{r} + C\\right)\n$$\n合并同类项，我们得到总参数数量的最终表达式：\n$$\nP_{\\text{total}} = \\frac{2C^2}{r} + \\frac{C}{r} + C\n$$\n该表达式表示了SE模块MLP中可训练参数的总数，用通道数$C$和缩减率$r$表示。\n\n接下来，我们计算$C = 64$和$r = 16$时的数值。\n将这些值代入推导出的公式：\n$$\nP_{\\text{total}} = \\frac{2(64)^2}{16} + \\frac{64}{16} + 64\n$$\n首先，我们计算各项：\n$(64)^2 = 4096$.\n$$\n\\frac{2(4096)}{16} = \\frac{8192}{16} = 512\n$$\n$$\n\\frac{64}{16} = 4\n$$\n将各部分相加：\n$$\nP_{\\text{total}} = 512 + 4 + 64 = 580\n$$\n因此，对于$C=64$和$r=16$，SE MLP包含580个可训练参数。\n\n最后，我们从基本原理出发，论述在$C$固定的情况下，增加缩减率$r$如何影响参数效率和过拟合风险。我们推导出的总参数数量表达式为$P_{\\text{total}}(r) = \\frac{2C^2+C}{r} + C$。\n\n1.  对参数效率的影响：\n    参数效率指模型用最少的参数达到高性能的能力。在表达式$P_{\\text{total}}(r)$中，项$\\frac{2C^2+C}{r}$与$r$成反比，而$C$是一个常数。随着$r$的增加，该分数值减小，从而减少了总参数数量$P_{\\text{total}}$。参数较少的模型被认为更具参数效率，因为它需要更少的内存和计算开销。因此，增加$r$可以提高SE模块的参数效率。\n\n2.  对过拟合风险的影响：\n    过拟合是一种现象，即具有高容量（通常与大量参数相关）的模型学习了训练数据的特定细节和噪声，导致其对新的、未见过的数据泛化能力差。在像影像组学这样经常面临小患者队列和有限样本量的学科中，过拟合是一个重要问题。通过增加$r$，我们减少了SE模块中的可训练参数总数$P_{\\text{total}}$。参数的减少降低了模型的整体容量。容量较低的模型更难记住训练数据，从而被迫学习更鲁棒、更具泛化性的模式。这起到了一种隐式正则化的作用。因此，增加$r$有助于降低过拟合的风险，这对于从小数据集构建鲁棒模型是非常有利的。",
            "answer": "$$\n\\boxed{580}\n$$"
        },
        {
            "introduction": "在医学图像分析中，一个关键的挑战是确保深度学习模型关注临床相关的区域，而不是被背景或伪影干扰。本练习将介绍一种强大的技术：利用辅助监督信息（如医生标注的感兴趣区域，ROI）来引导模型的注意力。你将学习如何设计一个辅助损失函数来惩罚“看错地方”的注意力，并探索如何通过平衡不同损失项的梯度来有效训练模型，以避免过度约束。",
            "id": "4529572",
            "problem": "一个用于放射组学分类的卷积神经网络 (CNN) 在 $N$ 个特征区域上生成一个注意力向量 $\\mathbf{a} = (a_{1}, \\dots, a_{N})$，其中注意力是通过一个 softmax 函数由注意力 logits $\\mathbf{z} = (z_{1}, \\dots, z_{N})$ 计算得出的，因此 $a_{i} = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{N} \\exp(z_{j})}$ 且 $\\sum_{i=1}^{N} a_{i} = 1$，$a_{i} \\geq 0$。对于每个输入，都有一个弱感兴趣区域 (ROI) 掩码 $\\mathbf{m} = (m_{1}, \\dots, m_{N})$，其中 $m_{i} \\in \\{0, 1\\}$ 表示区域 $i$ 是否在 ROI 内。\n\n标准的分类损失是二元交叉熵，\n$$\nL_{\\mathrm{cls}} = -\\left(y \\ln(p) + (1-y)\\ln(1-p)\\right),\n$$\n其中 $y \\in \\{0,1\\}$ 是真实标签，$p \\in (0,1)$ 是模型预测的概率。\n\n从风险最小化原则和概率分布下期望的定义出发，推导一个辅助注意力监督损失 $L_{\\mathrm{att}}$，该损失使用弱掩码 $\\mathbf{m}$ 来惩罚放置在 ROI 之外的注意力权重。然后，构建一个组合损失\n$$\nL_{\\mathrm{total}} = L_{\\mathrm{cls}} + \\lambda L_{\\mathrm{att}},\n$$\n并解释如何通过匹配梯度尺度来选择平衡系数 $\\lambda > 0$，以避免相对于分类目标过度约束注意力。\n\n假设在平衡时存在以下科学上合理的情景，该情景发生在一个留出的验证小批量上：当 $\\lambda = 1$ 时，测得 $L_{\\mathrm{cls}}$ 关于注意力 logits $\\mathbf{z}$ 的梯度的欧几里得范数为 $G_{\\mathrm{cls}} = 2.75$，而 $L_{\\mathrm{att}}$ 关于 $\\mathbf{z}$ 的梯度的欧几里得范数为 $G_{\\mathrm{att}} = 0.44$。为避免过度约束，您的目标是使注意力监督项对梯度范数的贡献等于分类梯度范数的一个分数 $r = 0.3$，也就是说，$\\lambda L_{\\mathrm{att}}$ 关于 $\\mathbf{z}$ 的梯度范数应为 $r G_{\\mathrm{cls}}$。\n\n使用这些量和第一性原理推导，计算实现这种平衡的 $\\lambda$ 值。将您的最终数值答案四舍五入到四位有效数字。",
            "solution": "用户提供了一个问题，要求推导一个用于注意力监督的辅助损失函数，解释一种梯度平衡技术，以及计算一个特定的平衡系数。\n\n首先，对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n- 注意力向量：$\\mathbf{a} = (a_{1}, \\dots, a_{N})$\n- 注意力 logits：$\\mathbf{z} = (z_{1}, \\dots, z_{N})$\n- Softmax 函数：$a_{i} = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{N} \\exp(z_{j})}$\n- 注意力属性：$\\sum_{i=1}^{N} a_{i} = 1$ 且 $a_{i} \\geq 0$\n- 弱 ROI 掩码：$\\mathbf{m} = (m_{1}, \\dots, m_{N})$，其中 $m_{i} \\in \\{0, 1\\}$\n- 分类损失：$L_{\\mathrm{cls}} = -\\left(y \\ln(p) + (1-y)\\ln(1-p)\\right)$\n- 真实标签：$y \\in \\{0,1\\}$\n- 预测概率：$p \\in (0,1)$\n- 总损失：$L_{\\mathrm{total}} = L_{\\mathrm{cls}} + \\lambda L_{\\mathrm{att}}$\n- 平衡系数：$\\lambda > 0$\n- 分类损失的测量梯度范数：$||\\nabla_{\\mathbf{z}} L_{\\mathrm{cls}}||_2 = G_{\\mathrm{cls}} = 2.75$\n- 注意力损失的测量梯度范数：$||\\nabla_{\\mathbf{z}} L_{\\mathrm{att}}||_2 = G_{\\mathrm{att}} = 0.44$\n- 目标梯度范数比率：$r = 0.3$\n- 平衡条件：$\\lambda L_{\\mathrm{att}}$ 关于 $\\mathbf{z}$ 的梯度范数应为 $r G_{\\mathrm{cls}}$。\n- 任务：推导 $L_{\\mathrm{att}}$，解释平衡策略，并计算 $\\lambda$ 至四位有效数字。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题具有科学依据，描述了深度学习中用于医学图像分析（放射组学）的一种标准且合理的技术。所有术语，如 softmax、交叉熵和基于梯度的优化，都是该领域的基础。使用辅助损失以弱监督方式引导注意力是一个常见的研究课题。该问题是适定的，为得到唯一解提供了所有必要的定义和数据。语言客观而精确。提供的数值是现实的。问题是自洽的，没有矛盾。\n\n### 步骤3：结论与行动\n问题有效。将提供一个完整的、有理有据的解决方案。\n\n第一个任务是基于风险最小化原则推导辅助注意力监督损失 $L_{\\mathrm{att}}$。注意力向量 $\\mathbf{a}$ 可以被解释为在 $N$ 个特征区域上的一个概率分布。我们需要惩罚放置在感兴趣区域 (ROI) 之外的注意力权重。弱掩码 $\\mathbf{m}$ 的元素 $m_i = 1$ 表示区域在 ROI 内，$m_i = 0$ 表示区域在 ROI 外。可以为关注每个区域分配一个成本：在 ROI 外的成本为 $c_i = 1$，在 ROI 内的成本为 $c_i = 0$。这个成本函数可以使用掩码紧凑地表示为 $c_i = 1 - m_i$。\n\n根据风险最小化原则，目标是在由注意力权重 $\\mathbf{a}$ 定义的概率分布下最小化期望成本。成本 $c$ 的期望是：\n$$\nE_{\\mathbf{a}}[c] = \\sum_{i=1}^{N} a_i c_i = \\sum_{i=1}^{N} a_i (1-m_i)\n$$\n这个表达式表示分配给指定 ROI 之外所有区域的总注意力权重。直接最小化这个值可以达到惩罚 ROI 之外注意力的目标。因此，我们将辅助损失 $L_{\\mathrm{att}}$ 定义为这个期望成本：\n$$\nL_{\\mathrm{att}} = \\sum_{i=1}^{N} a_i (1-m_i)\n$$\n用于训练网络的总损失是分类损失和这个辅助注意力损失的和，由一个平衡系数 $\\lambda > 0$ 加权：\n$$\nL_{\\mathrm{total}} = L_{\\mathrm{cls}} + \\lambda L_{\\mathrm{att}}\n$$\n在使用梯度下降法训练模型时，参数（包括注意力 logits $\\mathbf{z}$）会根据 $L_{\\mathrm{total}}$ 的梯度进行更新。由于梯度算子的线性特性，总损失的梯度是其各组成部分梯度的和：\n$$\n\\nabla_{\\mathbf{z}} L_{\\mathrm{total}} = \\nabla_{\\mathbf{z}} (L_{\\mathrm{cls}} + \\lambda L_{\\mathrm{att}}) = \\nabla_{\\mathbf{z}} L_{\\mathrm{cls}} + \\lambda \\nabla_{\\mathbf{z}} L_{\\mathrm{att}}\n$$\n项 $\\nabla_{\\mathbf{z}} L_{\\mathrm{cls}}$ 和 $\\lambda \\nabla_{\\mathbf{z}} L_{\\mathrm{att}}$ 是决定参数更新方向的向量。为了防止引导注意力的目标主导主要的分类任务，这两个梯度向量的大小应该被平衡。一个大的 $\\lambda$ 可能会过度约束注意力机制，从而可能损害分类性能。所提出的策略是设置 $\\lambda$，使得来自注意力损失的梯度贡献的大小（欧几里得范数）$||\\lambda \\nabla_{\\mathbf{z}} L_{\\mathrm{att}}||_2$ 是来自分类损失的梯度贡献的大小 $||\\nabla_{\\mathbf{z}} L_{\\mathrm{cls}}||_2$ 的一个指定分数 $r$。\n\n设 $G_{\\mathrm{cls}} = ||\\nabla_{\\mathbf{z}} L_{\\mathrm{cls}}||_2$ 且 $G_{\\mathrm{att}} = ||\\nabla_{\\mathbf{z}} L_{\\mathrm{att}}||_2$。来自注意力项的梯度贡献的欧几里得范数是：\n$$\n||\\lambda \\nabla_{\\mathbf{z}} L_{\\mathrm{att}}||_2 = |\\lambda| \\cdot ||\\nabla_{\\mathbf{z}} L_{\\mathrm{att}}||_2\n$$\n由于 $\\lambda > 0$，这变为 $\\lambda G_{\\mathrm{att}}$。平衡条件给出如下：\n$$\n\\lambda G_{\\mathrm{att}} = r G_{\\mathrm{cls}}\n$$\n我们现在可以解出 $\\lambda$：\n$$\n\\lambda = \\frac{r G_{\\mathrm{cls}}}{G_{\\mathrm{att}}}\n$$\n问题提供了来自一个验证小批量的测量值：$G_{\\mathrm{cls}} = 2.75$，$G_{\\mathrm{att}} = 0.44$，以及期望的比率 $r = 0.3$。代入这些值：\n$$\n\\lambda = \\frac{0.3 \\times 2.75}{0.44}\n$$\n我们首先计算分子中的乘积：\n$$\n0.3 \\times 2.75 = 0.825\n$$\n接下来，我们进行除法运算：\n$$\n\\lambda = \\frac{0.825}{0.44} = 1.875\n$$\n问题要求最终答案四舍五入到四位有效数字。计算出的值 $1.875$ 恰好有四位有效数字（$1$、$8$、$7$、$5$），因此不需要进一步的四舍五入。平衡系数应设置为 $\\lambda = 1.875$。",
            "answer": "$$\\boxed{1.875}$$"
        }
    ]
}