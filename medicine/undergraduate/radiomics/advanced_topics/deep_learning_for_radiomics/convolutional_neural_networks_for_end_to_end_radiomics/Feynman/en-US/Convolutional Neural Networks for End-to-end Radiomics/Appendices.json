{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of a Convolutional Neural Network's ability to learn hierarchical features is its receptive field—the specific region of the input that influences a single neuron's output. This exercise guides you through calculating the size of this field, moving beyond the standard theoretical definition to explore the more nuanced concept of the *effective* receptive field, which better reflects where the network concentrates its focus during learning . By mastering these derivations, you will gain a fundamental tool for analyzing how architectural choices like kernel size ($k$), stride ($s$), and dilation ($d$) govern the flow and aggregation of information within your model.",
            "id": "4534308",
            "problem": "A radiomics pipeline uses a one-dimensional convolutional neural network (CNN) to process intensity profiles sampled along a line within a computed tomography (CT) slice, as part of an end-to-end radiomics model for lesion characterization. Consider the following four-layer one-dimensional CNN, where layer $\\ell$ has kernel size $k_{\\ell}$, stride $s_{\\ell}$, zero-padding $p_{\\ell}$ on both ends, and dilation $d_{\\ell}$:\n\n- Layer $1$: $k_{1} = 3$, $s_{1} = 1$, $p_{1} = 1$, $d_{1} = 1$.\n- Layer $2$: $k_{2} = 3$, $s_{2} = 2$, $p_{2} = 1$, $d_{2} = 1$.\n- Layer $3$: $k_{3} = 3$, $s_{3} = 1$, $p_{3} = 2$, $d_{3} = 2$.\n- Layer $4$: $k_{4} = 5$, $s_{4} = 1$, $p_{4} = 0$, $d_{4} = 1$.\n\nAssume odd kernel sizes so that a central index is well-defined, and consider the receptive field with respect to the central output position of the final layer. Start from the definition of discrete convolution with stride, padding, and dilation, and the composition of such mappings across layers. Using only these definitions and the chain rule for gradient propagation in linearized networks, derive:\n\n1. A general closed-form expression for the theoretical receptive field size $R_{L}$ at layer $L$ as a function of $\\{k_{\\ell}, s_{\\ell}, p_{\\ell}, d_{\\ell}\\}_{\\ell=1}^{L}$, clearly identifying whether and how $p_{\\ell}$ enters the expression.\n\n2. Under a simplifying assumption that, during backpropagation, the gradient contributions from positions within each kernel are equally likely and independent across layers (approximating the effective receptive field as the distribution of path offsets), derive a closed-form expression for the effective receptive field variance $\\sigma^{2}$ at the input, in terms of $\\{k_{\\ell}, s_{\\ell}, d_{\\ell}\\}_{\\ell=1}^{L}$, where the offset positions within each kernel are treated as a symmetric discrete uniform random variable over the set of integer offsets.\n\nFinally, compute and report, for the four-layer architecture above:\n\n- The theoretical receptive field size $R_{4}$, expressed in pixels.\n- The effective receptive field variance $\\sigma^{2}$, expressed in $\\text{pixels}^{2}$.\n\nNo approximations or rounding are required; report exact values. The final answer must be a single object containing both computed quantities.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. We proceed with the solution. The solution involves two main derivations followed by a numerical calculation.\n\nFirst, we derive a general closed-form expression for the theoretical receptive field size $R_{L}$ for a one-dimensional convolutional neural network (CNN) with $L$ layers. The receptive field of a neuron is the set of input units that can influence its output value. We define $R_{\\ell}$ as the receptive field size of a neuron in layer $\\ell$ with respect to the network's input (layer $0$).\n\nLet's establish a recurrence relation. The receptive field of a single input unit (layer $0$) is $R_{0} = 1$.\nA neuron in layer $1$ applies a kernel of size $k_{1}$ with dilation $d_{1}$. The effective kernel size, which is the span of input units it connects to, is $(k_{1}-1)d_{1}+1$. Thus, the receptive field size at layer $1$ is $R_{1} = (k_{1}-1)d_{1}+1$.\n\nNow, consider a neuron at layer $\\ell > 1$. It connects to a region of neurons in layer $\\ell-1$. The size of this region is its effective kernel size, $(k_{\\ell}-1)d_{\\ell}+1$. To find the total receptive field $R_{\\ell}$, we consider the influence of this layer $\\ell-1$ region on the original input. The receptive fields of the neurons at the two ends of this region determine the total span. Let $S_{\\ell-1} = \\prod_{i=1}^{\\ell-1} s_{i}$ be the cumulative stride from layer $0$ to layer $\\ell-1$. A distance of one unit in the output of layer $i-1$ corresponds to a distance of $s_{i-1}$ units in its input. Therefore, the distance between the centers of two adjacent neurons in layer $\\ell-1$ maps to a distance of $S_{\\ell-1}$ at the input layer.\n\nThe span of the centers of the receptive fields of the neurons in layer $\\ell-1$ that are seen by a single neuron in layer $\\ell$ is given by the distance between the first and last neuron, which is $(k_{\\ell}-1)d_{\\ell}$, multiplied by the cumulative stride up to that layer, $S_{\\ell-1}$. The total receptive field $R_{\\ell}$ is this span plus the receptive field of a single neuron from layer $\\ell-1$, which is $R_{\\ell-1}$. More precisely, it is the span covered by the centers plus the \"wings\" from the first and last receptive fields. This leads to the recurrence:\n$$R_{\\ell} = R_{\\ell-1} + (k_{\\ell}-1)d_{\\ell} \\prod_{i=1}^{\\ell-1} s_{i}$$\nwith the base case $R_{0} = 1$. We define the empty product $\\prod_{i=1}^{0} s_{i} = 1$.\nUnrolling this recurrence relation gives a summation:\n$$R_{L} = R_{0} + \\sum_{\\ell=1}^{L} \\left( (k_{\\ell}-1)d_{\\ell} \\prod_{i=1}^{\\ell-1} s_{i} \\right)$$\nSubstituting $R_{0} = 1$, we obtain the general expression for the theoretical receptive field size at layer $L$:\n$$R_{L} = 1 + \\sum_{\\ell=1}^{L} (k_{\\ell}-1)d_{\\ell} \\prod_{i=1}^{\\ell-1} s_{i}$$\nThe padding parameter $p_{\\ell}$ affects the spatial dimension of the output feature map but does not alter the functional dependence of an output neuron on its inputs. Therefore, padding $p_{\\ell}$ does not enter the expression for the theoretical receptive field size.\n\nSecond, we derive the effective receptive field variance $\\sigma^{2}$. The problem posits a model where the influence of an input pixel on a central output pixel is quantified by a random walk. A path is traced from the output back to the input, and at each layer $\\ell$, a random offset $\\delta_{\\ell}$ is chosen from within the kernel. The total offset at the input is the sum of these offsets, scaled by the appropriate strides.\n\nFor a layer $\\ell$ with an odd kernel size $k_{\\ell}$, the integer offsets from the center are $\\{-\\frac{k_{\\ell}-1}{2}, \\dots, 0, \\dots, \\frac{k_{\\ell}-1}{2}\\}$. With dilation $d_{\\ell}$, the actual spatial offsets are $\\{ -d_{\\ell}\\frac{k_{\\ell}-1}{2}, \\dots, 0, \\dots, d_{\\ell}\\frac{k_{\\ell}-1}{2}\\}$. The random offset variable $\\delta_{\\ell}$ is assumed to be a symmetric discrete uniform random variable over this set.\nDue to symmetry, the mean of the offset at each layer is zero: $E[\\delta_{\\ell}] = 0$.\nThe variance of $\\delta_{\\ell}$ is $Var(\\delta_{\\ell}) = E[\\delta_{\\ell}^2]$. Let $M = (k_{\\ell}-1)/2$.\n$$Var(\\delta_{\\ell}) = \\frac{1}{k_{\\ell}} \\sum_{j=-M}^{M} (j d_{\\ell})^{2} = \\frac{d_{\\ell}^{2}}{k_{\\ell}} \\left( 2 \\sum_{j=1}^{M} j^{2} \\right) = \\frac{2 d_{\\ell}^{2}}{k_{\\ell}} \\frac{M(M+1)(2M+1)}{6} = \\frac{d_{\\ell}^{2} M(M+1)(2M+1)}{3k_{\\ell}}$$\nSubstituting $M = (k_{\\ell}-1)/2$:\n$$Var(\\delta_{\\ell}) = \\frac{d_{\\ell}^{2}}{3k_{\\ell}} \\left(\\frac{k_{\\ell}-1}{2}\\right) \\left(\\frac{k_{\\ell}+1}{2}\\right) (k_{\\ell}) = \\frac{d_{\\ell}^{2}(k_{\\ell}^{2}-1)}{12}$$\nThe total offset $\\Delta X$ at the input is the sum of offsets from each layer, scaled by the cumulative stride up to the previous layer:\n$$\\Delta X = \\sum_{\\ell=1}^{L} \\delta_{\\ell} \\left( \\prod_{i=1}^{\\ell-1} s_{i} \\right)$$\nSince the choices of offsets $\\delta_{\\ell}$ are independent across layers, the total variance $\\sigma^{2}$ is the sum of the scaled variances:\n$$\\sigma^{2} = Var(\\Delta X) = \\sum_{\\ell=1}^{L} Var\\left(\\delta_{\\ell} \\prod_{i=1}^{\\ell-1} s_{i}\\right) = \\sum_{\\ell=1}^{L} \\left(\\prod_{i=1}^{\\ell-1} s_{i}\\right)^{2} Var(\\delta_{\\ell})$$\nSubstituting the expression for $Var(\\delta_{\\ell})$, we get the final general expression:\n$$\\sigma^{2} = \\sum_{\\ell=1}^{L} \\frac{d_{\\ell}^{2}(k_{\\ell}^{2}-1)}{12} \\left(\\prod_{i=1}^{\\ell-1} s_{i}\\right)^{2}$$\n\nFinally, we compute $R_{4}$ and $\\sigma^{2}$ for the given four-layer architecture:\n- Layer $1$: $k_{1} = 3$, $s_{1} = 1$, $d_{1} = 1$.\n- Layer $2$: $k_{2} = 3$, $s_{2} = 2$, $d_{2} = 1$.\n- Layer $3$: $k_{3} = 3$, $s_{3} = 1$, $d_{3} = 2$.\n- Layer $4$: $k_{4} = 5$, $s_{4} = 1$, $d_{4} = 1$.\n\nCalculation of Theoretical Receptive Field Size $R_{4}$:\nWe first compute the cumulative strides $\\prod_{i=1}^{\\ell-1}s_{i}$:\n- For $\\ell=1$: $\\prod_{i=1}^{0} s_{i} = 1$.\n- For $\\ell=2$: $\\prod_{i=1}^{1} s_{i} = s_{1} = 1$.\n- For $\\ell=3$: $\\prod_{i=1}^{2} s_{i} = s_{1}s_{2} = 1 \\cdot 2 = 2$.\n- For $\\ell=4$: $\\prod_{i=1}^{3} s_{i} = s_{1}s_{2}s_{3} = 1 \\cdot 2 \\cdot 1 = 2$.\n\nNow we sum the terms for $R_4$:\n$$R_{4} = 1 + (k_{1}-1)d_{1}(1) + (k_{2}-1)d_{2}(s_{1}) + (k_{3}-1)d_{3}(s_{1}s_{2}) + (k_{4}-1)d_{4}(s_{1}s_{2}s_{3})$$\n$$R_{4} = 1 + (3-1)(1)(1) + (3-1)(1)(1) + (3-1)(2)(2) + (5-1)(1)(2)$$\n$$R_{4} = 1 + 2 \\cdot 1 \\cdot 1 + 2 \\cdot 1 \\cdot 1 + 2 \\cdot 2 \\cdot 2 + 4 \\cdot 1 \\cdot 2$$\n$$R_{4} = 1 + 2 + 2 + 8 + 8 = 21$$\n\nCalculation of Effective Receptive Field Variance $\\sigma^{2}$:\nWe compute the squared cumulative strides $\\left(\\prod_{i=1}^{\\ell-1}s_{i}\\right)^2$:\n- For $\\ell=1$: $1^{2} = 1$.\n- For $\\ell=2$: $s_{1}^{2} = 1^{2} = 1$.\n- For $\\ell=3$: $(s_{1}s_{2})^{2} = 2^{2} = 4$.\n- For $\\ell=4$: $(s_{1}s_{2}s_{3})^{2} = 2^{2} = 4$.\n\nNow we sum the terms for $\\sigma^{2}$:\n$$\\sigma^{2} = \\frac{1}{12} \\left[ d_{1}^{2}(k_{1}^{2}-1)(1)^{2} + d_{2}^{2}(k_{2}^{2}-1)(s_{1})^{2} + d_{3}^{2}(k_{3}^{2}-1)(s_{1}s_{2})^{2} + d_{4}^{2}(k_{4}^{2}-1)(s_{1}s_{2}s_{3})^{2} \\right]$$\n$$\\sigma^{2} = \\frac{1}{12} \\left[ 1^{2}(3^{2}-1)(1) + 1^{2}(3^{2}-1)(1) + 2^{2}(3^{2}-1)(4) + 1^{2}(5^{2}-1)(4) \\right]$$\n$$\\sigma^{2} = \\frac{1}{12} \\left[ 1(8)(1) + 1(8)(1) + 4(8)(4) + 1(24)(4) \\right]$$\n$$\\sigma^{2} = \\frac{1}{12} [8 + 8 + 128 + 96]$$\n$$\\sigma^{2} = \\frac{1}{12} (240) = 20$$\nThe theoretical receptive field size is $R_{4} = 21$ pixels, and the effective receptive field variance is $\\sigma^{2} = 20$ $\\text{pixels}^{2}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n21 & 20\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Theoretical concepts gain power when applied to solve real-world challenges. Medical images, such as MRI scans, are often anisotropic, meaning their physical resolution differs along various axes. This practice places you in the role of a network architect, tasking you with designing a 3D CNN that intelligently handles this common data characteristic without resorting to resampling, which can introduce artifacts . You will learn to make deliberate choices about strides and kernel shapes, applying your understanding of receptive fields to build a model that respects the physical reality of the input volume.",
            "id": "4534271",
            "problem": "An end-to-end radiomics pipeline processes three-dimensional Magnetic Resonance Imaging (MRI) volumes with anisotropic voxel spacing $(1.0, 1.0, 5.0)$ millimeters along $(x,y,z)$. Consider a three-layer three-dimensional Convolutional Neural Network (CNN) whose purpose is to extract features for downstream supervised learning. You must design a stride and dilation scheme that respects the acquisition anisotropy without any resampling of the input volume. The design must satisfy all of the following constraints, which reflect standard engineering choices in this setting:\n\n- Use unit stride $(1,1,1)$ in the first layer to preserve high-frequency details at the input.\n- Achieve a total in-plane downsampling factor of exactly $4$ over the three layers, applied only in the $x$ and $y$ axes, with no downsampling along $z$. Concretely, use stride $(2,2,1)$ in exactly two of the three layers, and unit stride $(1,1,1)$ in the remaining layer.\n- To respect through-slice anisotropy, forbid any through-slice mixing until the in-plane receptive field in physical units reaches at least one slice thickness. Concretely, use kernel depth $1$ along $z$ (i.e., $k_{z}=1$) in any layer where the preceding in-plane receptive field in physical units is strictly less than $5$ millimeters; use kernel depth $3$ along $z$ (i.e., $k_{z}=3$) starting at the first layer for which the preceding in-plane receptive field in physical units is at least $5$ millimeters.\n- Use the smallest positive integer dilations along all axes consistent with the above constraints.\n\nAssume all convolutional kernels have spatial size $3$ along $x$ and $y$ (i.e., $k_{x}=k_{y}=3$), zero padding is chosen so that the output is aligned to the input center voxel, and there are no skip connections. Treat each spatial axis independently when reasoning about growth of the receptive field across layers.\n\nUsing only the fundamental definition of receptive field growth under composition of convolutions with stride and dilation, determine the final effective receptive field extent in physical units (millimeters) along each axis after the third layer, under your designed scheme. For clarity, define the effective receptive field extent along a given axis to be the physical length of the minimal contiguous input segment that can influence a single output element along that axis, equal to the number of input voxels in the receptive field along that axis multiplied by the voxel spacing along that axis.\n\nExpress your final answer as a row vector $\\begin{pmatrix} R_{x} & R_{y} & R_{z} \\end{pmatrix}$ in millimeters. No rounding is required.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and internally consistent. All data and constraints required for a unique solution are provided. The problem is a valid exercise in applying fundamental principles of convolutional neural networks to a practical design scenario in medical imaging. Therefore, a full solution is warranted.\n\nThe core of the problem is to determine the design parameters of a three-layer 3D CNN—specifically the strides, kernel sizes, and dilations for each layer—based on a set of constraints, and then to calculate the resulting effective receptive field. The effective receptive field $R_i$ (in voxels) along a single axis after layer $i$ can be calculated recursively. Let $R_{i-1}$ be the receptive field after the previous layer (with $R_0=1$), $k_i$ be the kernel size of layer $i$, $d_i$ be the dilation rate of layer $i$, and $S_{i-1}$ be the cumulative product of strides up to layer $i-1$, defined as $S_{i-1} = \\prod_{j=1}^{i-1} s_j$ (with $S_0 = 1$). The recursive formula is:\n$$R_i = R_{i-1} + (k_i - 1) d_i S_{i-1}$$\nWe will apply this formula independently to each spatial axis $(x, y, z)$. The final receptive field extent in physical units is the receptive field in voxels multiplied by the corresponding voxel spacing.\n\nThe given parameters are:\n- Voxel spacing: $v = (v_x, v_y, v_z) = (1.0, 1.0, 5.0)$ mm.\n- Number of layers: $3$.\n- In-plane kernel size: $k_{ix} = k_{iy} = 3$ for $i \\in \\{1, 2, 3\\}$.\n\nWe proceed layer by layer to determine the parameters and calculate the receptive field growth.\n\n**Step 1: Determine Network Parameters**\n\n**Layer 1:**\n- **Stride ($s_1$):** The problem states to use unit stride in the first layer. Thus, $s_1 = (s_{1x}, s_{1y}, s_{1z}) = (1, 1, 1)$.\n- **Dilation ($d_1$):** The problem requires the smallest positive integer dilations. With no other constraints forcing a larger value, the smallest positive integer is $1$. Thus, $d_1 = (d_{1x}, d_{1y}, d_{1z}) = (1, 1, 1)$.\n- **Kernel Size ($k_1$):** We are given $k_{1x}=3$ and $k_{1y}=3$. For $k_{1z}$, we must evaluate the anisotropy constraint. The \"preceding in-plane receptive field\" for layer $1$ is that of the input, which is a single voxel. Its physical size is $\\max(1 \\cdot v_x, 1 \\cdot v_y) = \\max(1.0, 1.0) = 1.0$ mm. Since $1.0 < 5.0$, the constraint requires no through-slice mixing. Therefore, $k_{1z} = 1$.\n- **Layer 1 Parameters:** $s_1=(1,1,1)$, $d_1=(1,1,1)$, $k_1=(3,3,1)$.\n\n**Layer 2:**\n- **Stride ($s_2$):** The total in-plane downsampling factor must be $4$, achieved using stride $(2,2,1)$ in two of the three layers. Since layer $1$ has unit stride, layers $2$ and $3$ must have stride $(2,2,1)$. Thus, $s_2 = (s_{2x}, s_{2y}, s_{2z}) = (2, 2, 1)$.\n- **Dilation ($d_2$):** Using the smallest positive integer, $d_2 = (d_{2x}, d_{2y}, d_{2z}) = (1, 1, 1)$.\n- **Kernel Size ($k_2$):** We are given $k_{2x}=3$ and $k_{2y}=3$. To find $k_{2z}$, we first need the in-plane receptive field after layer $1$.\n  - Cumulative stride product before layer 2 is $S_1 = s_1 = (1,1,1)$.\n  - Receptive field in voxels after layer 1 ($R_1$):\n    - $R_{1x} = R_0 + (k_{1x}-1)d_{1x}S_0 = 1 + (3-1) \\cdot 1 \\cdot 1 = 3$.\n    - $R_{1y} = R_0 + (k_{1y}-1)d_{1y}S_0 = 1 + (3-1) \\cdot 1 \\cdot 1 = 3$.\n  - The preceding in-plane physical receptive field is $\\max(R_{1x} \\cdot v_x, R_{1y} \\cdot v_y) = \\max(3 \\cdot 1.0, 3 \\cdot 1.0) = 3.0$ mm. Since $3.0 < 5.0$, the constraint requires $k_{2z} = 1$.\n- **Layer 2 Parameters:** $s_2=(2,2,1)$, $d_2=(1,1,1)$, $k_2=(3,3,1)$.\n\n**Layer 3:**\n- **Stride ($s_3$):** As determined above, $s_3 = (s_{3x}, s_{3y}, s_{3z}) = (2, 2, 1)$. The total in-plane stride product is $s_{1x}s_{2x}s_{3x} = 1 \\cdot 2 \\cdot 2 = 4$, satisfying the constraint.\n- **Dilation ($d_3$):** Using the smallest positive integer, $d_3 = (d_{3x}, d_{3y}, d_{3z}) = (1, 1, 1)$.\n- **Kernel Size ($k_3$):** We are given $k_{3x}=3$ and $k_{3y}=3$. To find $k_{3z}$, we first need the in-plane receptive field after layer $2$.\n  - Cumulative stride product before layer 3 is $S_2 = (s_{1x}s_{2x}, s_{1y}s_{2y}, s_{1z}s_{2z}) = (1 \\cdot 2, 1 \\cdot 2, 1 \\cdot 1) = (2, 2, 1)$.\n  - Receptive field in voxels after layer 2 ($R_2$):\n    - $R_{2x} = R_{1x} + (k_{2x}-1)d_{2x}S_{1x} = 3 + (3-1) \\cdot 1 \\cdot 1 = 5$.\n    - $R_{2y} = R_{1y} + (k_{2y}-1)d_{2y}S_{1y} = 3 + (3-1) \\cdot 1 \\cdot 1 = 5$.\n  - The preceding in-plane physical receptive field is $\\max(R_{2x} \\cdot v_x, R_{2y} \\cdot v_y) = \\max(5 \\cdot 1.0, 5 \\cdot 1.0) = 5.0$ mm. The condition \"strictly less than $5$ millimeters\" is now false. The rule states to use $k_z=3$ starting at the first layer where the preceding field is \"at least $5$ millimeters\". This condition is met for the first time at layer $3$. Therefore, $k_{3z} = 3$.\n- **Layer 3 Parameters:** $s_3=(2,2,1)$, $d_3=(1,1,1)$, $k_3=(3,3,3)$.\n\n**Step 2: Calculate Final Receptive Field**\n\nNow we calculate the final receptive field in voxels, $R_3$, using the parameters determined above.\n\n**Along x-axis:**\n- $R_{0x}=1$\n- $R_{1x}=3$\n- $R_{2x}=5$\n- $R_{3x} = R_{2x} + (k_{3x}-1)d_{3x}S_{2x} = 5 + (3-1) \\cdot 1 \\cdot 2 = 5 + 4 = 9$.\n\n**Along y-axis:**\n- $R_{0y}=1$\n- $R_{1y}=3$\n- $R_{2y}=5$\n- By symmetry with the x-axis, $R_{3y} = 9$.\n\n**Along z-axis:**\n- We must calculate the receptive field growth from scratch.\n- $R_{0z}=1$.\n- Layer 1: $R_{1z} = R_{0z} + (k_{1z}-1)d_{1z}S_{0z} = 1 + (1-1) \\cdot 1 \\cdot 1 = 1$.\n- Layer 2: $R_{2z} = R_{1z} + (k_{2z}-1)d_{2z}S_{1z} = 1 + (1-1) \\cdot 1 \\cdot 1 = 1$.\n- Layer 3: $R_{3z} = R_{2z} + (k_{3z}-1)d_{3z}S_{2z} = 1 + (3-1) \\cdot 1 \\cdot 1 = 1 + 2 = 3$.\n\nThe final receptive field in voxels is $R_3 = (R_{3x}, R_{3y}, R_{3z}) = (9, 9, 3)$.\n\n**Step 3: Convert to Physical Units**\n\nThe problem asks for the final effective receptive field extent in physical units (mm). We multiply the voxel counts by the voxel spacing $v = (1.0, 1.0, 5.0)$ mm.\n- Extent along x: $R_x = R_{3x} \\cdot v_x = 9 \\cdot 1.0 = 9.0$ mm.\n- Extent along y: $R_y = R_{3y} \\cdot v_y = 9 \\cdot 1.0 = 9.0$ mm.\n- Extent along z: $R_z = R_{3z} \\cdot v_z = 3 \\cdot 5.0 = 15.0$ mm.\n\nThe final effective receptive field extent is $(9.0, 9.0, 15.0)$ mm.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n9.0 & 9.0 & 15.0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A predictive model is of limited clinical use if its reasoning is opaque. To build trust and validate a model's behavior, we must be able to ask: \"Why did you make this prediction?\" This exercise introduces Gradient-weighted Class Activation Mapping (Grad-CAM), a powerful and widely used technique for visualizing the input regions that a CNN deems most important for a particular decision . You will derive the method from first principles and apply it to a concrete example, learning how to generate heatmaps that illuminate the model's decision-making process and move it from a \"black box\" toward an interpretable tool.",
            "id": "4534276",
            "problem": "Consider an end-to-end radiomics system in which a three-dimensional Convolutional Neural Network (CNN) is trained to predict a scalar pre-softmax class score $y$ for a target clinical endpoint from an input volumetric image patch. Let the final convolutional block output $K$ feature maps $\\{A^{k}\\}_{k=1}^{K}$, where each $A^{k}$ is indexed by spatial coordinates $(i,j,l)$ with dimensions $I \\times J \\times L$. Define $Z = I \\cdot J \\cdot L$. The Gradient-weighted Class Activation Mapping (Grad-CAM) method assigns an importance weight to each channel based on partial derivatives of $y$ with respect to the feature maps, and produces a spatial heatmap whose positive values indicate regions contributing positively to the prediction. \n\nStarting from the chain rule of differentiation and a first-order Taylor expansion of $y$ with respect to the activations $\\{A^{k}\\}$, derive a closed-form expression for the Grad-CAM heatmap in terms of the feature maps $\\{A^{k}\\}$ and a channel-wise scalar weight constructed by globally averaging the partial derivatives $\\frac{\\partial y}{\\partial A^{k}_{i,j,l}}$ over the spatial index $(i,j,l)$. Justify the use of a rectifying nonlinearity to retain only positively contributing regions in the final heatmap. Base your derivation on well-tested facts about backpropagation and activation importance in CNNs, without assuming any pre-specified final formula.\n\nThen, for a specific case representative of a volumetric radiomics CNN, let $K = 2$, $I = 2$, $J = 2$, and $L = 1$, so that $Z = 4$. Suppose the feature maps are\n$$\nA^{1} = \\begin{pmatrix}\n1 & -2 \\\\\n0 & 3\n\\end{pmatrix}, \\quad\nA^{2} = \\begin{pmatrix}\n-1 & 2 \\\\\n1 & -2\n\\end{pmatrix},\n$$\nindexed by $(i,j,l)$ with $l=1$ for all entries, and the partial derivatives of the pre-softmax score $y$ with respect to the activations are\n$$\n\\frac{\\partial y}{\\partial A^{1}} = \\begin{pmatrix}\n4 & -1 \\\\\n2 & 0\n\\end{pmatrix}, \\quad\n\\frac{\\partial y}{\\partial A^{2}} = \\begin{pmatrix}\n-2 & 1 \\\\\n-1 & 2\n\\end{pmatrix}.\n$$\nCompute the Grad-CAM heatmap value at the voxel $(i,j,l) = (1,1,1)$ produced by globally averaged channel weights and a rectifying nonlinearity. Express your final numerical answer exactly, with no rounding. \n\nFinally, explain how the choice of the convolutional layer at which the feature maps $\\{A^{k}\\}$ are taken affects the resulting Grad-CAM heatmap in the context of volumetric radiomics, addressing resolution, semantic specificity, and gradient reliability. Your explanation should be scientifically grounded and self-consistent, but the final answer must be the single numerical value requested above, with no units.",
            "solution": "The problem statement is deemed valid. It is scientifically grounded in the principles of deep learning and model interpretability, specifically the Gradient-weighted Class Activation Mapping (Grad-CAM) method. The problem is well-posed, providing all necessary definitions and numerical data for a unique solution to be calculated. The language is objective and the setup is internally consistent and formalizable.\n\nWe are tasked with deriving the expression for a Grad-CAM heatmap and then computing its value for a specific case. The derivation must originate from fundamental principles of calculus as applied to neural networks.\n\nFirst, we derive the general form of the Grad-CAM heatmap. The pre-softmax class score, $y$, is a scalar function of the activations in the final convolutional layer's feature maps, $\\{A^{k}\\}_{k=1}^{K}$. The influence of a specific activation at voxel $(i,j,l)$ in feature map $k$, denoted $A^{k}_{i,j,l}$, on the score $y$ is captured by the partial derivative $\\frac{\\partial y}{\\partial A^{k}_{i,j,l}}$. This gradient is computed via backpropagation.\n\nA first-order Taylor expansion of $y$ around a baseline (e.g., zero activations) would approximate the score as a linear function of the activations, with the gradients serving as coefficients:\n$$ y \\approx y_0 + \\sum_{k=1}^{K} \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{l=1}^{L} \\frac{\\partial y}{\\partial A^{k}_{i,j,l}} A^{k}_{i,j,l} $$\nGrad-CAM simplifies this high-dimensional relationship by first determining the \"importance\" of an entire feature map (or channel) $k$. This is achieved by collapsing the spatial gradient information into a single scalar weight, $\\alpha_k$. As specified in the problem, this weight is obtained by performing global average pooling on the gradients for that channel:\n$$ \\alpha_k = \\frac{1}{Z} \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{l=1}^{L} \\frac{\\partial y}{\\partial A^{k}_{i,j,l}} $$\nHere, $Z = I \\cdot J \\cdot L$ is the total number of spatial locations (voxels) in a feature map. This weight, $\\alpha_k$, represents the average contribution of channel $k$ to the score $y$ per unit of activation.\n\nThe Grad-CAM heatmap, $L_{\\text{Grad-CAM}}$, is then constructed as a weighted linear combination of the feature maps, where the weights are the $\\alpha_k$ values just defined. The value of this intermediate \"raw\" heatmap at a specific voxel $(i,j,l)$ is:\n$$ L_{\\text{raw}}(i,j,l) = \\sum_{k=1}^{K} \\alpha_k A^{k}_{i,j,l} $$\nThis expression localizes the high-level semantic information captured by the channel importances, $\\alpha_k$, onto the spatial grid defined by the feature maps, $A^k$. A high positive value in $L_{\\text{raw}}$ indicates that the corresponding voxel is strongly activated in feature maps that have a strong positive influence on the class score $y$.\n\nFinally, the problem states that a rectifying nonlinearity is applied to retain only positively contributing regions. This corresponds to the application of a Rectified Linear Unit (ReLU) function, which sets all negative values to zero. The final Grad-CAM heatmap is thus:\n$$ L_{\\text{Grad-CAM}}(i,j,l) = \\text{ReLU}(L_{\\text{raw}}(i,j,l)) = \\max(0, L_{\\text{raw}}(i,j,l)) $$\nThe justification for this step is that we are typically interested in visualizing which parts of the input provide positive evidence for the class of interest. Negative values in $L_{\\text{raw}}$ can be ambiguous, arising from either positive activations in negatively-weighted channels or negative activations in positively-weighted channels. By filtering these out, the resulting heatmap highlights only the regions that support the prediction.\n\nNow, we apply this derivation to the specific case provided.\nThe givens are:\n- Number of channels: $K=2$.\n- Feature map dimensions: $I=2$, $J=2$, $L=1$.\n- Total number of voxels per map: $Z = I \\cdot J \\cdot L = 2 \\cdot 2 \\cdot 1 = 4$.\n- Feature maps:\n$$ A^{1} = \\begin{pmatrix} 1 & -2 \\\\ 0 & 3 \\end{pmatrix}, \\quad A^{2} = \\begin{pmatrix} -1 & 2 \\\\ 1 & -2 \\end{pmatrix} $$\n- Gradients of the score with respect to the feature maps:\n$$ \\frac{\\partial y}{\\partial A^{1}} = \\begin{pmatrix} 4 & -1 \\\\ 2 & 0 \\end{pmatrix}, \\quad \\frac{\\partial y}{\\partial A^{2}} = \\begin{pmatrix} -2 & 1 \\\\ -1 & 2 \\end{pmatrix} $$\n\nFirst, we compute the channel importance weights, $\\alpha_1$ and $\\alpha_2$.\nFor channel $k=1$:\n$$ \\alpha_1 = \\frac{1}{4} \\sum_{i,j,l} \\frac{\\partial y}{\\partial A^{1}_{i,j,l}} = \\frac{1}{4} (4 + (-1) + 2 + 0) = \\frac{5}{4} $$\nFor channel $k=2$:\n$$ \\alpha_2 = \\frac{1}{4} \\sum_{i,j,l} \\frac{\\partial y}{\\partial A^{2}_{i,j,l}} = \\frac{1}{4} ((-2) + 1 + (-1) + 2) = \\frac{0}{4} = 0 $$\n\nNext, we compute the raw heatmap value, $L_{\\text{raw}}$, at the specified voxel $(i,j,l) = (1,1,1)$. This corresponds to the element in the first row and first column of the given matrices (assuming $1$-based indexing for $i$ and $j$, and $l=1$ is implicit).\nFrom the given matrices, the activations at $(1,1,1)$ are:\n- $A^{1}_{1,1,1} = 1$\n- $A^{2}_{1,1,1} = -1$\n\nThe raw heatmap value is:\n$$ L_{\\text{raw}}(1,1,1) = \\alpha_1 A^{1}_{1,1,1} + \\alpha_2 A^{2}_{1,1,1} = \\left(\\frac{5}{4}\\right)(1) + (0)(-1) = \\frac{5}{4} $$\n\nFinally, we apply the ReLU function to get the final Grad-CAM value:\n$$ L_{\\text{Grad-CAM}}(1,1,1) = \\text{ReLU}\\left(\\frac{5}{4}\\right) = \\max\\left(0, \\frac{5}{4}\\right) = \\frac{5}{4} $$\nThe value of the Grad-CAM heatmap at voxel $(1,1,1)$ is $\\frac{5}{4}$.\n\nRegarding the choice of the convolutional layer, there exists a fundamental trade-off between spatial resolution and semantic specificity.\n- **Deeper Layers (near the output):** These layers have feature maps with low spatial resolution due to repeated down-sampling (e.g., pooling, strided convolutions). However, they capture high-level, complex semantic features (e.g., tumor texture, organ morphology) that are highly relevant to the clinical endpoint. A Grad-CAM heatmap generated from a deep layer will be coarse but highly class-discriminative, effectively localizing the abstract concepts the network has learned. Gradients at these layers are often cleaner as they are closer to the loss function.\n- **Shallower Layers (near the input):** These layers have high spatial resolution, preserving fine-grained detail from the input image. However, they capture low-level features like edges, corners, and simple textures, which are less semantically rich. A heatmap from a shallow layer will be detailed and precise in location but may highlight features that are not specific to the class of interest (e.g., highlighting all edges, not just those of a lesion).\nIn the context of volumetric radiomics, heatmaps are most often generated from the final convolutional layer. This is because the primary goal is to understand which high-level radiomic patterns, as encoded by the network, are driving the prediction. The loss of spatial resolution is an accepted trade-off for gaining superior semantic insight into the model's decision-making process.",
            "answer": "$$\\boxed{\\frac{5}{4}}$$"
        }
    ]
}