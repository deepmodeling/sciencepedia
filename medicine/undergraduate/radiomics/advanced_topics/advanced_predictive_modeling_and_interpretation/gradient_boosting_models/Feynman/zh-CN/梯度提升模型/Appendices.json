{
    "hands_on_practices": [
        {
            "introduction": "梯度提升模型通过顺序添加新模型来迭代地修正先前模型的误差。本练习将带您深入了解这一过程的核心，即从梯度提升分类器的逻辑损失函数出发，亲手推导出一阶和二阶导数（梯度和Hessian矩阵）。通过计算第一个（也是最简单的）决策树的叶节点权重，您将直观地理解模型是如何迈出学习第一步的，并揭示一阶与二阶提升方法在优化上的根本差异。",
            "id": "4542167",
            "problem": "一个影像组学团队正在训练一个梯度提升决策树（GBDT）分类器，以根据计算机断层扫描（CT）影像组学特征预测肺结节的恶性程度。二元标签为 $y_i \\in \\{0,1\\}$，其中 $y_i = 1$ 表示恶性，$y_i = 0$ 表示良性。该模型使用带有logistic链接函数的伯努利负对数似然。对于每个样本 $i$，模型维持一个得分 $f_i$ 和一个概率 $p_i = \\frac{1}{1 + \\exp(-f_i)}$。单个样本的损失为 $\\ell_i(f_i) = - y_i \\ln p_i - (1 - y_i) \\ln(1 - p_i)$。在迭代 $t = 0$ 时，模型对所有 $i$ 初始化 $f_i^{(0)} = 0$。\n\n任务：\n- 仅从上述定义和基础微积分出发，推导一阶导数 $g_i = \\frac{\\partial \\ell_i}{\\partial f_i}$ 和二阶导数 $h_i = \\frac{\\partial^2 \\ell_i}{\\partial f_i^2}$ 作为 $p_i$ 和 $y_i$ 的函数，然后在 $f_i^{(0)} = 0$ 处对 $y_i \\in \\{0,1\\}$ 的情况计算 $g_i$ 和 $h_i$ 的值。\n- 在第一次提升迭代中，假设树只有1个叶节点（无分裂），学习率为1，且没有正则化。对于一阶（梯度）提升，选择恒定叶节点值 $w_{\\mathrm{FO}}$ 以最小化 $\\sum_{i=1}^{n} (-g_i - w)^2$。对于二阶（牛顿）提升，选择恒定叶节点值 $w_{\\mathrm{SO}}$ 以最小化二阶泰勒近似 $\\sum_{i=1}^{n} \\left(g_i w + \\frac{1}{2} h_i w^2\\right)$。在初始化条件 $f_i^{(0)} = 0$ 下，推导 $w_{\\mathrm{FO}}$ 和 $w_{\\mathrm{SO}}$ 关于 $n$ 和 $k = \\sum_{i=1}^{n} y_i$ 的闭式表达式。\n\n在一个包含 $n = 200$ 名患者和 $k = 60$ 个恶性结节的特定CT影像组学队列中，计算第一次迭代的比率 $R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}}$。将最终答案以无单位的精确实数形式报告。",
            "solution": "首先验证问题的正确性和可解性。\n\n### 第1步：提取已知条件\n-   二元标签：$y_i \\in \\{0,1\\}$，其中 $y_i = 1$ 为恶性。\n-   模型得分：$f_i$。\n-   预测概率：$p_i = \\frac{1}{1 + \\exp(-f_i)}$。\n-   单个样本的损失函数（伯努利负对数似然）：$\\ell_i(f_i) = - y_i \\ln p_i - (1 - y_i) \\ln(1 - p_i)$。\n-   迭代 $t=0$ 时的初始模型得分：对所有样本 $i$，$f_i^{(0)} = 0$。\n-   一阶导数（梯度）：$g_i = \\frac{\\partial \\ell_i}{\\partial f_i}$。\n-   二阶导数（海森）：$h_i = \\frac{\\partial^2 \\ell_i}{\\partial f_i^2}$。\n-   一阶提升的叶节点值 $w_{\\mathrm{FO}}$ 最小化目标函数 $\\sum_{i=1}^{n} (-g_i - w)^2$。\n-   二阶提升的叶节点值 $w_{\\mathrm{SO}}$ 最小化目标函数 $\\sum_{i=1}^{n} \\left(g_i w + \\frac{1}{2} h_i w^2\\right)$。\n-   第一次迭代中的树有1个叶节点。\n-   学习率为1。没有正则化。\n-   患者总数：$n = 200$。\n-   恶性结节数量（正标签）：$k = \\sum_{i=1}^{n} y_i = 60$。\n-   目标是计算比率 $R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}}$。\n\n### 第2步：使用提取的已知条件进行验证\n该问题具有科学依据。所描述的组件是梯度提升模型理论中的标准部分。损失函数是二元交叉熵，链接函数是logistic（sigmoid）函数，梯度和二阶导数的定义是正确的。用于寻找叶节点权重的目标函数对应于GBDT中一阶（拟合残差）和二阶（牛顿法）更新的标准优化问题。该问题是适定的，提供了推导所需量和计算最终比率所需的所有信息。语言客观且数学上精确。没有矛盾、歧义或不切实际的假设。\n\n### 第3步：结论与行动\n问题有效。将提供完整解答。\n\n### 梯度和二阶导数的推导\n单个样本的损失由 $\\ell_i(f_i) = - y_i \\ln p_i - (1 - y_i) \\ln(1 - p_i)$ 给出，其中 $p_i = \\frac{1}{1 + \\exp(-f_i)}$。\n为了求 $\\ell_i$ 对 $f_i$ 的导数，我们首先求 $p_i$ 对 $f_i$ 的导数：\n$$ \\frac{\\partial p_i}{\\partial f_i} = \\frac{\\partial}{\\partial f_i} (1 + \\exp(-f_i))^{-1} = -1 (1 + \\exp(-f_i))^{-2} \\cdot (\\exp(-f_i) \\cdot -1) = \\frac{\\exp(-f_i)}{(1 + \\exp(-f_i))^2} $$\n这可以用 $p_i$ 来表示：\n$$ \\frac{\\partial p_i}{\\partial f_i} = \\frac{1}{1 + \\exp(-f_i)} \\cdot \\frac{\\exp(-f_i)}{1 + \\exp(-f_i)} = p_i (1 - p_i) $$\n这是logistic函数的一个众所周知的性质。\n\n现在，我们使用链式法则来求一阶导数 $g_i$：\n$$ g_i = \\frac{\\partial \\ell_i}{\\partial f_i} = \\frac{\\partial \\ell_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial f_i} $$\n损失函数对 $p_i$ 的导数是：\n$$ \\frac{\\partial \\ell_i}{\\partial p_i} = -\\frac{y_i}{p_i} - (1 - y_i) \\frac{1}{1 - p_i} (-1) = -\\frac{y_i}{p_i} + \\frac{1 - y_i}{1 - p_i} = \\frac{-y_i(1 - p_i) + p_i(1 - y_i)}{p_i(1 - p_i)} = \\frac{p_i - y_i}{p_i(1 - p_i)} $$\n结合这些结果：\n$$ g_i = \\left( \\frac{p_i - y_i}{p_i(1 - p_i)} \\right) \\cdot (p_i(1 - p_i)) = p_i - y_i $$\n为了求二阶导数 $h_i$，我们将 $g_i$ 对 $f_i$ 求导：\n$$ h_i = \\frac{\\partial^2 \\ell_i}{\\partial f_i^2} = \\frac{\\partial g_i}{\\partial f_i} = \\frac{\\partial}{\\partial f_i} (p_i - y_i) = \\frac{\\partial p_i}{\\partial f_i} $$\n使用我们之前得到的 $\\frac{\\partial p_i}{\\partial f_i}$ 的结果：\n$$ h_i = p_i(1 - p_i) $$\n\n### 在初始化时进行评估\n在初始迭代 $t=0$ 时，模型得分对所有 $i$ 初始化为 $f_i^{(0)} = 0$。\n每个样本的初始概率为：\n$$ p_i^{(0)} = \\frac{1}{1 + \\exp(-0)} = \\frac{1}{1 + 1} = \\frac{1}{2} $$\n我们在这个初始点评估梯度 $g_i$ 和二阶导数 $h_i$：\n$$ g_i^{(0)} = p_i^{(0)} - y_i = \\frac{1}{2} - y_i $$\n$$ h_i^{(0)} = p_i^{(0)}(1 - p_i^{(0)}) = \\frac{1}{2} \\left(1 - \\frac{1}{2}\\right) = \\frac{1}{4} $$\n这些初始值将用于计算第一棵树的叶节点权重。\n\n### 叶节点权重的推导\n对于第一次提升迭代，我们为整个数据集计算一个单一的叶节点权重 $w$。\n\n**一阶叶节点权重 ($w_{\\mathrm{FO}}$)**\n目标是找到最小化 $L_{\\mathrm{FO}}(w) = \\sum_{i=1}^{n} (-g_i - w)^2$ 的 $w_{\\mathrm{FO}}$。为了找到最小值，我们将关于 $w$ 的导数设为零：\n$$ \\frac{d L_{\\mathrm{FO}}}{d w} = \\sum_{i=1}^{n} 2(-g_i - w)(-1) = 2 \\sum_{i=1}^{n} (g_i + w) = 0 $$\n$$ \\sum_{i=1}^{n} g_i + \\sum_{i=1}^{n} w = 0 \\implies \\left(\\sum_{i=1}^{n} g_i\\right) + n w = 0 $$\n$$ w_{\\mathrm{FO}} = -\\frac{\\sum_{i=1}^{n} g_i}{n} $$\n使用初始梯度 $g_i^{(0)} = \\frac{1}{2} - y_i$ 和定义 $k = \\sum_{i=1}^{n} y_i$：\n$$ \\sum_{i=1}^{n} g_i^{(0)} = \\sum_{i=1}^{n} \\left(\\frac{1}{2} - y_i\\right) = \\sum_{i=1}^{n} \\frac{1}{2} - \\sum_{i=1}^{n} y_i = \\frac{n}{2} - k $$\n将此代入 $w_{\\mathrm{FO}}$ 的表达式中：\n$$ w_{\\mathrm{FO}} = -\\frac{\\frac{n}{2} - k}{n} = -\\left(\\frac{1}{2} - \\frac{k}{n}\\right) = \\frac{k}{n} - \\frac{1}{2} $$\n\n**二阶叶节点权重 ($w_{\\mathrm{SO}}$)**\n目标是找到最小化损失的二阶泰勒近似 $L_{\\mathrm{SO}}(w) = \\sum_{i=1}^{n} \\left(g_i w + \\frac{1}{2} h_i w^2\\right)$ 的 $w_{\\mathrm{SO}}$。将关于 $w$ 的导数设为零：\n$$ \\frac{d L_{\\mathrm{SO}}}{d w} = \\sum_{i=1}^{n} (g_i + h_i w) = 0 $$\n$$ \\sum_{i=1}^{n} g_i + w \\sum_{i=1}^{n} h_i = 0 $$\n$$ w_{\\mathrm{SO}} = -\\frac{\\sum_{i=1}^{n} g_i}{\\sum_{i=1}^{n} h_i} $$\n使用初始梯度和二阶导数 $g_i^{(0)} = \\frac{1}{2} - y_i$ 和 $h_i^{(0)} = \\frac{1}{4}$：\n分子和之前一样：$\\sum_{i=1}^{n} g_i^{(0)} = \\frac{n}{2} - k$。\n分母是：\n$$ \\sum_{i=1}^{n} h_i^{(0)} = \\sum_{i=1}^{n} \\frac{1}{4} = \\frac{n}{4} $$\n将这些和代入 $w_{\\mathrm{SO}}$ 的表达式中：\n$$ w_{\\mathrm{SO}} = -\\frac{\\frac{n}{2} - k}{\\frac{n}{4}} = -4 \\frac{\\frac{n}{2} - k}{n} = -4\\left(\\frac{1}{2} - \\frac{k}{n}\\right) = 4\\left(\\frac{k}{n} - \\frac{1}{2}\\right) $$\n\n### 比率的计算\n问题要求计算比率 $R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}}$。使用推导出的表达式：\n$$ R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}} = \\frac{4\\left(\\frac{k}{n} - \\frac{1}{2}\\right)}{\\frac{k}{n} - \\frac{1}{2}} $$\n只要分母不为零，这个比率就有效。如果 $\\frac{k}{n} = \\frac{1}{2}$，则分母为零。\n对于给定的队列，$n=200$ 且 $k=60$。\n$$ \\frac{k}{n} = \\frac{60}{200} = \\frac{3}{10} = 0.3 $$\n由于 $0.3 \\neq 0.5$，分母不为零，我们可以消去项 $\\left(\\frac{k}{n} - \\frac{1}{2}\\right)$：\n$$ R = 4 $$\n该比率是一个常数4，与 $n$ 和 $k$ 的具体值无关（只要 $2k \\neq n$）。\n最终答案是一个精确的实数。",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "在构建了模型的基础之后，我们来深入探讨决策树的构建细节。现实世界的放射组学数据往往存在缺失值，这是一个棘手的挑战。本练习将向您展示，先进的梯度提升机（如XGBoost）如何在树的构建过程中，通过一种名为“稀疏感知分裂发现”的精妙算法来原生处理缺失值。您将通过计算和比较两种不同“默认方向”下的分裂增益，来决定缺失值样本的最佳流向，从而深入理解这一强大功能的内部工作原理。",
            "id": "4542131",
            "problem": "一家医院建立了一个梯度提升机 (GBM) 来从一个计算机断层扫描放射组学特征 $x$ 预测一个二元放射组学终点。由于感兴趣区域的可变性，该特征 $x$ 经常有缺失值。训练过程是通过拟合损失函数负梯度的加性树来进行的。在某个内部节点，一个关于特征 $x$ 在阈值 $\\tau$ 处的候选分裂被评估。设对于该节点上的每个实例，损失函数相对于当前模型输出的一阶和二阶导数分别为 $g_i$ 和 $h_i$。对于任意一组实例，定义 $G=\\sum_i g_i$ 和 $H=\\sum_i h_i$。正则化包括一个系数为 $\\lambda$ 的 $\\ell_2$ 叶子节点惩罚项和一个每次分裂的复杂度惩罚项 $\\gamma$。GBM 使用损失函数的标准二阶泰勒展开来确定最优的叶子节点值和分裂增益。\n\n对于候选阈值 $\\tau$，非缺失实例（$x$ 可观测）被划分为一个“已知左”组，其聚合导数为 $G_{\\mathrm{KL}}=-12$, $H_{\\mathrm{KL}}=30$，以及一个“已知右”组，其聚合导数为 $G_{\\mathrm{KR}}=5$, $H_{\\mathrm{KR}}=20$。$x$ 值缺失的实例的聚合导数为 $G_{\\mathrm{M}}=-3$, $H_{\\mathrm{M}}=6$。超参数为 $\\lambda=1$ 和 $\\gamma=0.1$。GBM 处理缺失值的方式是，在每次分裂时学习一个默认方向，所有缺失值在训练和推理时都将遵循该方向。\n\n哪个选项正确地描述了基于树的 GBM 在这次分裂中如何处理缺失值，并在两种可能的默认方向下正确计算了分裂增益，从而确定了学习到的默认方向？\n\nA. GBM 尝试两种默认方向，将所有缺失实例分配到左子节点或右子节点，在每种情况下使用带正则化的二阶近似计算分裂增益，并选择增益较大的方向。当缺失值被送到左边时，聚合导数为 $G_{\\mathrm{L}}=G_{\\mathrm{KL}}+G_{\\mathrm{M}}=-15$, $H_{\\mathrm{L}}=H_{\\mathrm{KL}}+H_{\\mathrm{M}}=36$ 和 $G_{\\mathrm{R}}=G_{\\mathrm{KR}}=5$, $H_{\\mathrm{R}}=H_{\\mathrm{KR}}=20$，产生的增益约为 $2.66$。当缺失值被送到右边时，$G_{\\mathrm{L}}=-12$, $H_{\\mathrm{L}}=30$ 和 $G_{\\mathrm{R}}=2$, $H_{\\mathrm{R}}=26$，产生的增益约为 $1.42$。学习到的默认方向是左。\n\nB. GBM 总是将缺失实例发送到具有更多非缺失实例的子节点。分裂增益的计算完全忽略了缺失实例。此处的增益约为 $1.00$，且默认方向是右，因为 $H_{\\mathrm{KR}}=20$ 大于 $H_{\\mathrm{KL}}=30$。\n\nC. GBM 为分裂创建一个第三个“缺失”子节点，并将增益计算为三个子节点得分之和减去父节点得分。将 $G_{\\mathrm{M}}=-3$, $H_{\\mathrm{M}}=6$ 作为一个独立的子节点会使增益超过任何一种双向分配，因此学习到的默认值是“缺失分支”。\n\nD. GBM 首先将缺失的 $x$ 插补为此节点上观测到的 $x$ 的均值，然后如同没有缺失值一样继续进行；增益在插补后的完整数据上计算，而不尝试其他默认方向。计算出的增益约为 $2.00$，并且在推理时没有学习到的默认方向。",
            "solution": "用户希望我解决一个关于梯度提升机 (GBM) 的问题，具体是它们在决策树构建过程中如何处理缺失值。\n\n### 第一步：提取已知信息\n- **模型**：梯度提升机 (GBM)\n- **任务**：放射组学终点的二元分类。\n- **特征**：一个带有缺失值的单一放射组学特征 $x$。\n- **算法**：通过拟合损失函数的负梯度来构建加性树。该解法使用损失函数的二阶泰勒展开。\n- **导数符号**：对于当前节点上的一个实例 $i$，$g_i$ 是损失函数相对于模型输出的一阶导数，$h_i$ 是二阶导数。$G = \\sum g_i$ 和 $H = \\sum h_i$ 是一组实例的聚合导数。\n- **正则化**：一个系数为 $\\lambda = 1$ 的 $\\ell_2$ 叶子节点惩罚项和一个每次分裂的复杂度惩罚项 $\\gamma = 0.1$。\n- **在分裂阈值 $\\tau$ 处非缺失实例的数据**：\n    - 已知左组（实例满足 $x \\le \\tau$）：$G_{\\mathrm{KL}} = -12$, $H_{\\mathrm{KL}} = 30$。\n    - 已知右组（实例满足 $x > \\tau$）：$G_{\\mathrm{KR}} = 5$, $H_{\\mathrm{KR}} = 20$。\n- **缺失实例的数据**：\n    - 缺失组：$G_{\\mathrm{M}} = -3$, $H_{\\mathrm{M}} = 6$。\n- **缺失值处理方法**：“GBM 通过在每次分裂时学习一个默认方向来处理缺失值，所有缺失值在训练和推理时都将遵循该方向。”\n\n### 第二步：使用提取的已知信息进行验证\n问题陈述描述了稀疏感知分裂查找算法，这是像 XGBoost 这样的现代 GBM 实现的一个关键组成部分。在这种情况下，使用一阶和二阶导数（$g_i, h_i$）、$\\ell_2$ 正则化（$\\lambda$）和分裂复杂度惩罚（$\\gamma$）是标准做法。提供的值在数值上是一致的；例如，二阶导数之和（$H$）都是正数，对于常见的损失函数（如对数损失、平方误差损失），它们必须是正数。学习“默认方向”的描述是对该算法的精确技术性描述。该问题具有科学依据，提法得当，且客观。它包含了执行所需计算的所有必要信息，并且没有任何无效性缺陷。\n\n### 第三步：判断与行动\n问题有效。我将继续推导解决方案。\n\n### 推导过程\n这个问题的核心在于计算 GBM 树中一次分裂的增益。增益源于目标函数，该函数使用二阶泰勒展开进行近似。一个叶子节点的最优权重 $w^*$ 及其对应的目标函数值（分数）由以下公式给出：\n$$ w^* = -\\frac{G}{H + \\lambda} $$\n$$ \\text{score} = -\\frac{1}{2} \\frac{G^2}{H + \\lambda} $$\n其中 $G$ 和 $H$ 分别是该叶子节点中实例的一阶和二阶导数之和，$\\lambda$ 是 $\\ell_2$ 正则化参数。\n\n一次分裂的增益是目标函数的减少量，即子节点分数之和减去父节点分数，再减去引入分裂的复杂度惩罚 $\\gamma$。\n$$ \\text{Gain} = \\text{score}_{\\text{Left}} + \\text{score}_{\\text{Right}} - \\text{score}_{\\text{Parent}} - \\gamma $$\n代入分数的公式，我们得到：\n$$ \\text{Gain} = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_{\\text{Parent}}^2}{H_{\\text{Parent}} + \\lambda} \\right] - \\gamma $$\n其中下标 $L$、$R$ 和 `Parent` 分别指左子节点、右子节点和父节点。\n\n问题指出 GBM 会学习一个默认方向。这是通过评估两种情况来完成的：\n1.  所有带有缺失值的实例都被分配到左子节点。\n2.  所有带有缺失值的实例都被分配到右子节点。\n\n该算法会计算两种情况下的分裂增益，并选择产生更高增益的方向。\n\n首先，我们来计算父节点的聚合导数，该节点包含所有实例（已知左、已知右和缺失）：\n$G_{\\text{Parent}} = G_{\\mathrm{KL}} + G_{\\mathrm{KR}} + G_{\\mathrm{M}} = -12 + 5 + (-3) = -10$\n$H_{\\text{Parent}} = H_{\\mathrm{KL}} + H_{\\mathrm{KR}} + H_{\\mathrm{M}} = 30 + 20 + 6 = 56$\n\n增益公式中的父节点分数项为：\n$$ \\frac{G_{\\text{Parent}}^2}{H_{\\text{Parent}} + \\lambda} = \\frac{(-10)^2}{56 + 1} = \\frac{100}{57} $$\n\n**情况1：缺失值被发送到左子节点**\n\n左子节点将包含已知左实例和缺失值实例。右子节点将只包含已知右实例。\n- **左子节点：**\n  $G_L = G_{\\mathrm{KL}} + G_{\\mathrm{M}} = -12 + (-3) = -15$\n  $H_L = H_{\\mathrm{KL}} + H_{\\mathrm{M}} = 30 + 6 = 36$\n- **右子节点：**\n  $G_R = G_{\\mathrm{KR}} = 5$\n  $H_R = H_{\\mathrm{KR}} = 20$\n\n此情况下的增益（$\\text{Gain}_{\\text{left}}$）为：\n$$ \\text{Gain}_{\\text{left}} = \\frac{1}{2} \\left[ \\frac{(-15)^2}{36 + 1} + \\frac{5^2}{20 + 1} - \\frac{100}{57} \\right] - 0.1 $$\n$$ \\text{Gain}_{\\text{left}} = \\frac{1}{2} \\left[ \\frac{225}{37} + \\frac{25}{21} - \\frac{100}{57} \\right] - 0.1 $$\n数值计算：\n$$ \\frac{225}{37} \\approx 6.08108 $$\n$$ \\frac{25}{21} \\approx 1.19048 $$\n$$ \\frac{100}{57} \\approx 1.75439 $$\n$$ \\text{Gain}_{\\text{left}} \\approx \\frac{1}{2} [6.08108 + 1.19048 - 1.75439] - 0.1 = \\frac{1}{2} [5.51717] - 0.1 \\approx 2.7586 - 0.1 = 2.6586 $$\n所以，增益约为 $2.66$。\n\n**情况2：缺失值被发送到右子节点**\n\n左子节点将只包含已知左实例。右子节点将包含已知右实例和缺失值实例。\n- **左子节点：**\n  $G_L = G_{\\mathrm{KL}} = -12$\n  $H_L = H_{\\mathrm{KL}} = 30$\n- **右子节点：**\n  $G_R = G_{\\mathrm{KR}} + G_{\\mathrm{M}} = 5 + (-3) = 2$\n  $H_R = H_{\\mathrm{KR}} + H_{\\mathrm{M}} = 20 + 6 = 26$\n\n此情况下的增益（$\\text{Gain}_{\\text{right}}$）为：\n$$ \\text{Gain}_{\\text{right}} = \\frac{1}{2} \\left[ \\frac{(-12)^2}{30 + 1} + \\frac{2^2}{26 + 1} - \\frac{100}{57} \\right] - 0.1 $$\n$$ \\text{Gain}_{\\text{right}} = \\frac{1}{2} \\left[ \\frac{144}{31} + \\frac{4}{27} - \\frac{100}{57} \\right] - 0.1 $$\n数值计算：\n$$ \\frac{144}{31} \\approx 4.64516 $$\n$$ \\frac{4}{27} \\approx 0.14815 $$\n$$ \\text{Gain}_{\\text{right}} \\approx \\frac{1}{2} [4.64516 + 0.14815 - 1.75439] - 0.1 = \\frac{1}{2} [3.03892] - 0.1 \\approx 1.5195 - 0.1 = 1.4195 $$\n所以，增益约为 $1.42$。\n\n**结论**\n\n比较两个增益：\n$\\text{Gain}_{\\text{left}} \\approx 2.66$\n$\\text{Gain}_{\\text{right}} \\approx 1.42$\n\n由于 $\\text{Gain}_{\\text{left}} > \\text{Gain}_{\\text{right}}$，GBM 将选择将缺失值发送到左子节点。学习到的默认方向是‘左’，此次分裂的增益约为 $2.66$。\n\n### 逐项分析\n\n**A. GBM 尝试两种默认方向，将所有缺失实例分配到左子节点或右子节点，在每种情况下使用带正则化的二阶近似计算分裂增益，并选择增益较大的方向。当缺失值被送到左边时，聚合导数为 $G_{\\mathrm{L}}=G_{\\mathrm{KL}}+G_{\\mathrm{M}}=-15$, $H_{\\mathrm{L}}=H_{\\mathrm{KL}}+H_{\\mathrm{M}}=36$ 和 $G_{\\mathrm{R}}=G_{\\mathrm{KR}}=5$, $H_{\\mathrm{R}}=H_{\\mathrm{KR}}=20$，产生的增益约为 $2.66$。当缺失值被送到右边时，$G_{\\mathrm{L}}=-12$, $H_{\\mathrm{L}}=30$ 和 $G_{\\mathrm{R}}=2$, $H_{\\mathrm{R}}=26$，产生的增益约为 $1.42$。学习到的默认方向是左。**\n- 该选项完美地描述了稀疏感知分裂查找算法。\n- “缺失值向左”情况的聚合导数（$G_L=-15, H_L=36, G_R=5, H_R=20$）是正确的。计算出的增益 $\\approx 2.66$ 与我们的推导相符。\n- “缺失值向右”情况的聚合导数（$G_L=-12, H_L=30, G_R=2, H_R=26$）是正确的。计算出的增益 $\\approx 1.42$ 与我们的推导相符。\n- 基于更高增益得出学习到的默认方向是左的结论也是正确的。\n- 结论：**正确**。\n\n**B. GBM 总是将缺失实例发送到具有更多非缺失实例的子节点。分裂增益的计算完全忽略了缺失实例。此处的增益约为 $1.00$，且默认方向是右，因为 $H_{\\mathrm{KR}}=20$ 大于 $H_{\\mathrm{KL}}=30$。**\n- 该算法的描述是不正确的。决策是基于增益最大化，而不是实例数量。问题没有给出关于实例数量的信息，只有导数的和。\n- 声称分裂增益的计算完全忽略了缺失实例是不正确的。如推导所示，它们是计算中不可或缺的一部分。\n- 声称增益 $\\approx 1.00$ 是不正确的。一个忽略缺失实例的计算将得到约 $2.34$ 的增益。\n- “默认方向是右，因为 $H_{\\mathrm{KR}}=20$ 大于 $H_{\\mathrm{KL}}=30$” 这个说法在数学上是错误的（$20  30$），在算法上也是不正确的。\n- 结论：**不正确**。\n\n**C. GBM 为分裂创建一个第三个“缺失”子节点，并将增益计算为三个子节点得分之和减去父节点得分。将 $G_{\\mathrm{M}}=-3$, $H_{\\mathrm{M}}=6$ 作为一个独立的子节点会使增益超过任何一种双向分配，因此学习到的默认值是“缺失分支”。**\n- 这描述了一种不同的处理缺失值的方法（例如，在 CART 或 C4.5 中使用的方法），而不是流行的 GBM 中的标准方法，也不是短语“学习...一个默认方向”所暗示的方法。这将创建一个三路分裂，而不是一个带默认路径的二元分裂。问题的框架和增益公式都是针对二元分裂的。\n- 结论：**不正确**。\n\n**D. GBM 首先将缺失的 $x$ 插补为此节点上观测到的 $x$ 的均值，然后如同没有缺失值一样继续进行；增益在插补后的完整数据上计算，而不尝试其他默认方向。计算出的增益约为 $2.00$，并且在推理时没有学习到的默认方向。**\n- 这描述了均值插补，这是另一种可能的策略，但它不是问题陈述中描述的算法（“学习...一个默认方向”）。所描述的算法是分裂查找的一个集成部分，而不是一个预处理步骤。\n- 在不知道 $x$ 的均值和分裂阈值 $\\tau$ 的情况下，不可能知道插补后的实例会去哪里，因此无法计算增益来验证 $\\approx 2.00$ 的说法。\n- 结论：**不正确**。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "建立一个性能卓越的模型只是成功的一半，尤其在医学等高风险领域，理解模型的预测依据同样至关重要。本练习将引导您探索模型的可解释性，学习计算SHAP（Shapley Additive exPlanations）值。通过一个具体的放射组学特征实例，您将一步步计算出每个特征对最终预测结果的贡献度，并验证其核心的“局部准确性”属性，从而将复杂的黑箱模型转化为更透明、更值得信赖的决策辅助工具。",
            "id": "4542187",
            "problem": "一个基于梯度提升（GB）的放射组学回归模型经过训练，用于从计算机断层扫描（CT）成像中提取的两个标准化放射组学特征来预测一个无量纲的恶性风险评分：灰度共生矩阵对比度 $x_{1}$ 和小波高频能量 $x_{2}$。考虑该集成中的单个决策树（学习率 $= 1$，基础分数 $= 0$），其结构如下。根节点在 $x_{1}$ 的阈值 $1.5$ 处分裂：如果 $x_{1} \\leq 1.5$，左子节点在 $x_{2}$ 的阈值 $0.5$ 处进一步分裂，当 $x_{2} \\leq 0.5$ 时产生叶节点值 $0.8$，当 $x_{2}  0.5$ 时产生叶节点值 $1.6$；如果 $x_{1}  1.5$，右子节点是一个值为 $2.0$ 的叶节点。假设在 TreeSHAP 中用于边缘化的训练分布将 $x_{1}$ 和 $x_{2}$ 视为独立的，每个特征低于其各自阈值的概率为 $0.5$，高于其各自阈值的概率也为 $0.5$。对于特定实例 $x = (x_{1}, x_{2}) = (1.8, 0.4)$，该树的模型预测为 $f(x)$。\n\n使用上述独立性假设下两个特征的 Shapley 加性解释 (SHAP) 值 $\\phi_{1}$ 和 $\\phi_{2}$ 的定义，精确计算 $\\phi_{1}$ 和 $\\phi_{2}$，并验证此实例的局部准确性属性 $\\sum_{j=1}^{2} \\phi_{j} = f(x) - \\mathbb{E}[f(X)]$。提供精确值。将序对 $(\\phi_{1}, \\phi_{2})$ 以行矩阵的形式报告。",
            "solution": "### 第1步：问题验证\n\n**1. 提取已知条件：**\n- **模型：** 一个来自梯度提升 (GB) 集成的单决策树。\n- **特征：** $x_{1}$ (灰度共生矩阵对比度)，$x_{2}$ (小波高频能量)。\n- **模型参数：** 学习率 $\\eta = 1$，基础分数 $F_{0} = 0$。模型预测 $f(x)$ 是这棵单决策树的输出。\n- **树结构：**\n    - 根节点：在 $x_{1}$ 的阈值 $1.5$ 处分裂。\n    - 左子节点 ($x_{1} \\leq 1.5$)：在 $x_{2}$ 的阈值 $0.5$ 处分裂。\n        - 叶节点 1 ($x_{1} \\leq 1.5, x_{2} \\leq 0.5$)：值为 $0.8$。\n        - 叶节点 2 ($x_{1} \\leq 1.5, x_{2}  0.5$)：值为 $1.6$。\n    - 右子节点 ($x_{1}  1.5$)：是一个叶节点 (叶节点 3)，值为 $2.0$。\n- **用于 TreeSHAP 的背景分布：**\n    - 特征 $x_{1}$ 和 $x_{2}$ 被视为独立的。\n    - $P(x_{1} \\leq 1.5) = 0.5$, $P(x_{1}  1.5) = 0.5$。\n    - $P(x_{2} \\leq 0.5) = 0.5$, $P(x_{2}  0.5) = 0.5$。\n- **待解释的实例：** $x = (x_{1}, x_{2}) = (1.8, 0.4)$。\n- **任务：** 计算特征的 SHAP 值 $\\phi_{1}$ 和 $\\phi_{2}$，并验证局部准确性属性：$\\sum_{j=1}^{2} \\phi_{j} = f(x) - \\mathbb{E}[f(X)]$。\n\n**2. 使用提取的已知条件进行验证：**\n- **科学依据：** 该问题使用了已建立的概念：梯度提升、决策树和 SHAP (Shapley 加性解释) 值，特别是带有特征独立性假设的 TreeSHAP 变体。放射组学的背景是恰当的。所有概念都是机器学习和数据科学中的标准概念。\n- **问题适定性：** 问题陈述提供了一个完全指定的函数（决策树）、一个特定的输入点和一个明确定义的背景分布。这足以唯一确定 SHAP 值和期望的模型输出。\n- **客观性：** 问题以精确的数学语言陈述，没有主观性。\n\n**3. 结论与行动：**\n该问题是有效的。它是机器学习可解释性领域中一个明确定义的计算问题。我将继续进行解答。\n\n### 第2步：解题\n\n由决策树定义的预测函数 $f(x_{1}, x_{2})$ 如下：\n$$\nf(x_{1}, x_{2}) = \\begin{cases}\n0.8   \\text{if } x_{1} \\leq 1.5 \\text{ and } x_{2} \\leq 0.5 \\\\\n1.6   \\text{if } x_{1} \\leq 1.5 \\text{ and } x_{2}  0.5 \\\\\n2.0   \\text{if } x_{1}  1.5\n\\end{cases}\n$$\n\n首先，我们计算特定实例 $x = (x_{1}, x_{2}) = (1.8, 0.4)$ 的模型预测值 $f(x)$。\n由于 $x_{1} = 1.8  1.5$，该实例落入根节点的右子节点，该节点是一个叶节点。\n$$\nf(1.8, 0.4) = 2.0\n$$\n\n接下来，我们计算在背景分布下的期望预测值 $\\mathbb{E}[f(X)]$。特征 $X_{1}$ 和 $X_{2}$ 是独立的，且每个阈值被跨越的概率为 $0.5$。落入由阈值定义的任何区域的联合概率为 $0.5 \\times 0.5 = 0.25$。\n区域及其相关输出如下：\n1. $X_{1} \\leq 1.5, X_{2} \\leq 0.5$：输出为 $0.8$。概率为 $0.25$。\n2. $X_{1} \\leq 1.5, X_{2}  0.5$：输出为 $1.6$。概率为 $0.25$。\n3. $X_{1}  1.5$ (包括 $X_{2} \\leq 0.5$ 和 $X_{2}  0.5$)：输出为 $2.0$。概率为 $P(X_1  1.5) = 0.5$。\n\n因此，期望值为：\n$$\n\\mathbb{E}[f(X)] = P(X_{1} \\leq 1.5, X_{2} \\leq 0.5) \\cdot 0.8 + P(X_{1} \\leq 1.5, X_{2}  0.5) \\cdot 1.6 + P(X_{1}  1.5) \\cdot 2.0\n$$\n$$\n\\mathbb{E}[f(X)] = (0.5 \\cdot 0.5) \\cdot 0.8 + (0.5 \\cdot 0.5) \\cdot 1.6 + (0.5) \\cdot 2.0\n$$\n$$\n\\mathbb{E}[f(X)] = 0.25 \\cdot 0.8 + 0.25 \\cdot 1.6 + 0.5 \\cdot 2.0 = 0.2 + 0.4 + 1.0 = 1.6\n$$\n期望预测值（在 SHAP 中对应于基准值 $\\phi_0$）是 $\\mathbb{E}[f(X)]=1.6$。\n\n现在，我们为实例 $x=(1.8, 0.4)$ 计算 SHAP 值 $\\phi_1$ 和 $\\phi_2$。对于特征集 $F$ 中的一个特征 $j$，其 Shapley 值的定义为：\n$$\n\\phi_{j}(f, x) = \\sum_{S \\subseteq F\\setminus\\{j\\}} \\frac{|S|! (|F| - |S| - 1)!}{|F|!} [ \\mathbb{E}[f(X) | X_S=x_S, X_j=x_j] - \\mathbb{E}[f(X) | X_S=x_S] ]\n$$\n在这里， $|F|=2$，因此公式得以简化。对于一个特征 $j$，我们考虑特征集 $S=\\emptyset$ 和 $S=F\\setminus\\{j\\}$。两种情况下的权重均为 $\\frac{0!(2-1)!}{2!} = \\frac{1}{2}$。\n\n**$\\phi_1$ 的计算 (针对特征 $x_1$)：**\n两种排序是 $(\\{x_1\\}, \\{x_2\\})$ 和 $(\\{x_2\\}, \\{x_1\\})$。\n1.  首先加入 $x_1$ 时的边际贡献 (联盟 $S=\\emptyset$)：\n    贡献为 $\\mathbb{E}[f(X)|X_1=1.8] - \\mathbb{E}[f(X)]$。\n    $\\mathbb{E}[f(X)|X_1=1.8]$ 是当 $x_1$ 固定为 $1.8$ 而 $x_2$ 被边缘化时的期望输出。由于 $x_1=1.8  1.5$，无论 $x_2$ 的值是多少，决策树总是输出 $2.0$。\n    $$\n    \\mathbb{E}[f(X)|X_1=1.8] = 2.0\n    $$\n    贡献为 $2.0 - \\mathbb{E}[f(X)] = 2.0 - 1.6 = 0.4$。\n\n2.  第二个加入 $x_1$ 时的边际贡献 (联盟 $S=\\{x_2\\}$)：\n    贡献为 $\\mathbb{E}[f(X)|X_1=1.8, X_2=0.4] - \\mathbb{E}[f(X)|X_2=0.4]$。\n    $\\mathbb{E}[f(X)|X_1=1.8, X_2=0.4] = f(1.8, 0.4) = 2.0$。\n    $\\mathbb{E}[f(X)|X_2=0.4]$ 是当 $x_2$ 固定为 $0.4$ 而 $x_1$ 被边缘化时的期望输出。由于 $x_2=0.4 \\leq 0.5$：\n    - 如果 $X_1 \\leq 1.5$ (概率为 $0.5$)，输出为 $0.8$。\n    - 如果 $X_1  1.5$ (概率为 $0.5$)，输出为 $2.0$。\n    $$\n    \\mathbb{E}[f(X)|X_2=0.4] = 0.5 \\cdot 0.8 + 0.5 \\cdot 2.0 = 0.4 + 1.0 = 1.4\n    $$\n    贡献为 $2.0 - 1.4 = 0.6$。\n\n对两个贡献求平均：\n$$\n\\phi_{1} = \\frac{1}{2} (0.4) + \\frac{1}{2} (0.6) = 0.2 + 0.3 = 0.5\n$$\n\n**$\\phi_2$ 的计算 (针对特征 $x_2$)：**\n1.  首先加入 $x_2$ 时的边际贡献 (联盟 $S=\\emptyset$)：\n    贡献为 $\\mathbb{E}[f(X)|X_2=0.4] - \\mathbb{E}[f(X)]$。\n    我们已经计算出 $\\mathbb{E}[f(X)|X_2=0.4] = 1.4$。\n    贡献为 $1.4 - 1.6 = -0.2$。\n\n2.  第二个加入 $x_2$ 时的边际贡献 (联盟 $S=\\{x_1\\}$)：\n    贡献为 $\\mathbb{E}[f(X)|X_1=1.8, X_2=0.4] - \\mathbb{E}[f(X)|X_1=1.8]$。\n    我们已经知道 $f(1.8, 0.4) = 2.0$ 并且 $\\mathbb{E}[f(X)|X_1=1.8] = 2.0$。\n    贡献为 $2.0 - 2.0 = 0$。\n\n对两个贡献求平均：\n$$\n\\phi_{2} = \\frac{1}{2} (-0.2) + \\frac{1}{2} (0) = -0.1 + 0 = -0.1\n$$\n\n所以，SHAP 值为 $\\phi_1 = 0.5$ 和 $\\phi_2 = -0.1$。\n\n**验证局部准确性属性：**\n该属性表明 $\\phi_1 + \\phi_2 = f(x) - \\mathbb{E}[f(X)]$。\n- 等式左边：$\\phi_1 + \\phi_2 = 0.5 + (-0.1) = 0.4$。\n- 等式右边：$f(x) - \\mathbb{E}[f(X)] = 2.0 - 1.6 = 0.4$。\n等式 $0.4 = 0.4$ 成立，验证了局部准确性属性。\n\n最终答案是序对 $(\\phi_1, \\phi_2)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5  -0.1 \\end{pmatrix}}\n$$"
        }
    ]
}