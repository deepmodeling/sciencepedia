{
    "hands_on_practices": [
        {
            "introduction": "Wrapper methods are powerful, but their empirical approach to feature selection comes at a steep price. This exercise provides a hands-on opportunity to quantify the computational expense of a robust validation strategy, nested cross-validation, used in conjunction with a wrapper search. By calculating the total number of model fits, you will gain a concrete appreciation for the practical trade-offs between methodological rigor and computational feasibility in a typical radiomics workflow.",
            "id": "4539569",
            "problem": "A radiomics study aims to build a binary classifier for lesion malignancy using high-dimensional quantitative image features extracted from computed tomography. To control overfitting and estimate generalization performance, a wrapper-based feature selection is performed with Recursive Feature Elimination (RFE), evaluated within a nested cross-validation design. The outer cross-validation has $K_{\\mathrm{out}}$ folds to assess generalization on held-out outer test folds, and the inner cross-validation has $K_{\\mathrm{in}}$ folds to select the feature subset within each outer training partition.\n\nAssume the following protocol is adopted:\n- A \"model fit\" is counted each time the learning algorithm is trained on a dataset partition; predictions and scoring do not count as fits.\n- Within each outer fold, a stepwise wrapper search examines $E$ candidate feature subsets. For each candidate subset, performance is estimated by an inner $K_{\\mathrm{in}}$-fold cross-validation, which entails training exactly one model per inner fold (i.e., $K_{\\mathrm{in}}$ fits per inner evaluation).\n- After the inner search identifies the best feature subset for a given outer fold, a single final model is trained on the entire outer training partition using that subset, and then evaluated on the corresponding held-out outer test fold. This final model counts as exactly one additional fit per outer fold.\n- No additional refits are performed after all outer folds complete, and inference-time predictions never count toward the fit total.\n\nStarting from the foundational definition of $K$-fold cross-validation as partitioning the data into $K$ disjoint folds and, per evaluation, fitting a model $K$ times (once per held-out fold), derive a general expression for the total number of model fits required to complete the entire nested protocol as a function of $K_{\\mathrm{out}}$, $K_{\\mathrm{in}}$, and $E$, under the rules above. Then, compute the total number of model fits for $K_{\\mathrm{out}}=5$, $K_{\\mathrm{in}}=5$, and a stepwise search performing $E=50$ inner evaluations per outer fold. Provide your final answer as a single integer with no units. No rounding is required.",
            "solution": "Let $N_{\\text{fits}}$ be the total number of model fits required for the entire nested cross-validation protocol. The protocol consists of an outer loop with $K_{\\mathrm{out}}$ folds. We derive the total number of fits by analyzing the operations within a single outer fold and then multiplying by $K_{\\mathrm{out}}$.\n\nConsider a single iteration of the outer loop. The model fitting operations occur exclusively on the outer training set. There are two phases of model fitting within this single outer fold:\n1. The inner cross-validation loop for feature selection.\n2. The training of a final model for that outer fold.\n\n**Phase 1: Inner Cross-Validation for Feature Selection**\nThe problem states that a wrapper search examines $E$ candidate feature subsets. To evaluate the performance of each of these $E$ subsets, an inner $K_{\\mathrm{in}}$-fold cross-validation is performed. By definition, a single $K_{\\mathrm{in}}$-fold evaluation requires training the model $K_{\\mathrm{in}}$ times.\nTherefore, the number of model fits required to evaluate one candidate feature subset is $K_{\\mathrm{in}}$.\nSince there are $E$ candidate subsets to evaluate, the total number of model fits for the inner loop within one outer fold is:\n$$N_{\\text{inner}} = E \\times K_{\\mathrm{in}}$$\n\n**Phase 2: Final Model Training for the Outer Fold**\nAfter the inner loop identifies the best feature subset, one final model is trained on the *entire* outer training partition using that optimal subset. This counts as exactly one additional fit.\n$$N_{\\text{final\\_fit}} = 1$$\n\nThe total number of fits for a single outer fold, $N_{\\text{per\\_outer\\_fold}}$, is the sum of the fits from the inner loop and the final model fit:\n$$N_{\\text{per\\_outer\\_fold}} = N_{\\text{inner}} + N_{\\text{final\\_fit}} = (E \\times K_{\\mathrm{in}}) + 1$$\n\n**Total Number of Fits for the Entire Protocol**\nThis process is repeated for each of the $K_{\\mathrm{out}}$ folds of the outer cross-validation. Therefore, the total number of model fits, $N_{\\text{fits}}$, is the number of fits per outer fold multiplied by the number of outer folds:\n$$N_{\\text{fits}} = K_{\\mathrm{out}} \\times N_{\\text{per\\_outer\\_fold}} = K_{\\mathrm{out}} \\times (E \\times K_{\\mathrm{in}} + 1)$$\n\nNow, we compute the total number of fits for the specific values given: $K_{\\mathrm{out}}=5$, $K_{\\mathrm{in}}=5$, and $E=50$.\nSubstituting these values into the general expression:\n$$N_{\\text{fits}} = 5 \\times (50 \\times 5 + 1)$$\nFirst, we calculate the term inside the parentheses:\n$$50 \\times 5 + 1 = 250 + 1 = 251$$\nThen, we complete the calculation:\n$$N_{\\text{fits}} = 5 \\times 251 = 1255$$\n\nThus, the total number of model fits required to complete the entire nested protocol is 1255.",
            "answer": "$$\\boxed{1255}$$"
        },
        {
            "introduction": "Recursive Feature Elimination (RFE) is a popular and intuitive wrapper method, but it is not immune to the challenges posed by real-world data. This practice problem  presents a carefully constructed scenario to demonstrate how RFE's greedy, step-wise decisions can falter. By working through this example, you will see how multicollinearity and sampling variability can conspire to create a \"ranking inversion,\" leading the algorithm to discard the most valuable feature and illustrating a critical pitfall in automated feature selection.",
            "id": "4539571",
            "problem": "A radiomics team is building a binary malignancy classifier, but to reason from first principles they approximate the learning step with linear regression under squared error as the wrapper evaluator. Three standardized radiomic features are considered: $x_1$ (first-order intensity mean), $x_2$ (Gaussian-smoothed intensity mean), and $x_3$ (wavelet energy). All features are standardized to zero mean and unit variance. The Recursive Feature Elimination (RFE) procedure ranks features by absolute model coefficient magnitudes at each step. The model in the wrapper is ridge regression with penalty $\\lambda$ on the coefficients.\n\nGround-truth generative model: the malignancy score $y$ follows $y = x_1 + \\varepsilon$, where $\\varepsilon$ is zero-mean noise independent of all features with variance $\\operatorname{Var}(\\varepsilon) = \\sigma_\\varepsilon^2$. In population, the feature correlation matrix and correlations with $y$ satisfy:\n- $\\operatorname{Var}(x_i) = 1$ for $i \\in \\{1,2,3\\}$.\n- $\\operatorname{Corr}(x_1, x_2) = 0.95$, $\\operatorname{Corr}(x_1, x_3) = 0.70$, $\\operatorname{Corr}(x_2, x_3) = 0.95$.\n- $\\operatorname{Corr}(x_1, y) = 1$, $\\operatorname{Corr}(x_2, y) = 0.95$, $\\operatorname{Corr}(x_3, y) = 0.70$.\n\nOn a particular training fold of size $n$ (small), sampling variability yields the following empirical correlations used by the wrapper:\n- $\\widehat{\\operatorname{Corr}}(x_1, y) = 0.85$, $\\widehat{\\operatorname{Corr}}(x_2, y) = 0.85$, $\\widehat{\\operatorname{Corr}}(x_3, y) = 0.87$.\n- The empirical feature-to-feature correlations remain close to their population counterparts: $\\widehat{\\operatorname{Corr}}(x_1, x_2) = 0.95$, $\\widehat{\\operatorname{Corr}}(x_1, x_3) = 0.70$, $\\widehat{\\operatorname{Corr}}(x_2, x_3) = 0.95$.\n\nAssume ridge regression with penalty $\\lambda = 0.30$ is fit on this fold at each RFE step, and the ranking criterion is the absolute value of the fitted coefficients. Use the normal equation approximation for the ridge estimator in the standardized, large-sample limit, replacing $\\Sigma_{xx}$ and $\\Sigma_{xy}$ by the fold’s empirical moments:\n$$\n\\hat{w} \\approx \\left(\\Sigma_{xx} + \\lambda I\\right)^{-1} \\Sigma_{xy},\n$$\nwhere $\\Sigma_{xx}$ is the $3 \\times 3$ empirical correlation matrix of $(x_1, x_2, x_3)$ and $\\Sigma_{xy}$ is the vector of empirical correlations with $y$. In the second RFE step (after dropping one feature), use the corresponding $2 \\times 2$ submatrix and subvectors.\n\nFinally, to judge suboptimality against the ground truth $y = x_1 + \\varepsilon$, recall that for a single-feature linear predictor $\\hat{y} = \\gamma x$, the mean squared prediction error on new data is\n$$\n\\operatorname{MSE}(x) = \\sigma_\\varepsilon^2 + \\operatorname{Var}\\left(x_1 - \\gamma^\\star x\\right),\n$$\nwith $\\gamma^\\star = \\operatorname{Cov}(y, x)/\\operatorname{Var}(x)$; under standardization and $\\operatorname{Corr}(x_1, x) = \\rho$, this simplifies to\n$$\n\\operatorname{MSE}(x) = \\sigma_\\varepsilon^2 + 1 - \\rho^2.\n$$\n\nTake $\\sigma_\\varepsilon^2 = 0.20$.\n\nWhich option best describes the actual RFE elimination path on this fold and its consequence, including the quantitative suboptimality of the final subset under the ground truth?\n\nA. First eliminate $x_2$, then eliminate $x_3$, ending with $\\{x_1\\}$. No ranking inversion occurs; the final subset attains $\\operatorname{MSE} = \\sigma_\\varepsilon^2 = 0.20$.\n\nB. First eliminate $x_2$, then eliminate $x_1$, ending with $\\{x_3\\}$. A downstream ranking inversion occurs after removing a redundant stabilizer ($x_2$), and the final subset’s ground-truth error inflates to $\\operatorname{MSE} = \\sigma_\\varepsilon^2 + 1 - \\rho_{13}^2 = 0.20 + 1 - 0.49 = 0.71$.\n\nC. First eliminate $x_3$, then eliminate $x_2$, ending with $\\{x_1\\}$. The initial ranking places $x_3$ as least important; the final subset attains $\\operatorname{MSE} = 0.20$.\n\nD. Eliminate only $x_2$ and stop, because removal of a redundant feature cannot cause downstream ranking changes under ridge; the final subset is $\\{x_1, x_3\\}$ with no suboptimality relative to $\\{x_1\\}$.",
            "solution": "The problem requires us to trace the steps of a Recursive Feature Elimination (RFE) process using a ridge regression wrapper and then evaluate the outcome against the ground truth. The ranking criterion is the absolute magnitude of the fitted coefficients.\n\n**Step 1: RFE with features $\\{x_1, x_2, x_3\\}$**\n\nWe first fit a ridge regression model using all three features. The coefficients are given by $\\hat{w} = (\\Sigma_{xx} + \\lambda I)^{-1} \\Sigma_{xy}$, where $\\Sigma_{xx}$ and $\\Sigma_{xy}$ are the empirical correlation matrices and $\\lambda=0.30$.\n\nThe matrices are:\n$$ \\Sigma_{xx} = \\begin{pmatrix} 1 & 0.95 & 0.70 \\\\ 0.95 & 1 & 0.95 \\\\ 0.70 & 0.95 & 1 \\end{pmatrix}, \\quad \\Sigma_{xy} = \\begin{pmatrix} 0.85 \\\\ 0.85 \\\\ 0.87 \\end{pmatrix} $$\n$$ \\Sigma_{xx} + \\lambda I = \\begin{pmatrix} 1.30 & 0.95 & 0.70 \\\\ 0.95 & 1.30 & 0.95 \\\\ 0.70 & 0.95 & 1.30 \\end{pmatrix} $$\n\nCalculating the inverse and multiplying by $\\Sigma_{xy}$ yields the coefficient vector:\n$$ \\hat{w} \\approx \\begin{pmatrix} 0.374 \\\\ 0.083 \\\\ 0.407 \\end{pmatrix} $$\n\nWe rank features by their absolute coefficient magnitudes: $|\\hat{w}_1| \\approx 0.374$, $|\\hat{w}_2| \\approx 0.083$, and $|\\hat{w}_3| \\approx 0.407$.\nThe feature with the smallest absolute coefficient is $x_2$. Therefore, RFE eliminates $x_2$ in the first step.\n\n**Step 2: RFE with features $\\{x_1, x_3\\}$**\n\nWe refit the model on the remaining features, $x_1$ and $x_3$. The new submatrices are:\n$$ \\Sigma_{xx}^{(2)} = \\begin{pmatrix} 1 & 0.70 \\\\ 0.70 & 1 \\end{pmatrix}, \\quad \\Sigma_{xy}^{(2)} = \\begin{pmatrix} 0.85 \\\\ 0.87 \\end{pmatrix} $$\n$$ \\Sigma_{xx}^{(2)} + \\lambda I = \\begin{pmatrix} 1.30 & 0.70 \\\\ 0.70 & 1.30 \\end{pmatrix} $$\n\nThe new coefficient vector $\\hat{w}^{(2)}$ is:\n$$ \\hat{w}^{(2)} = \\begin{pmatrix} 1.30 & 0.70 \\\\ 0.70 & 1.30 \\end{pmatrix}^{-1} \\begin{pmatrix} 0.85 \\\\ 0.87 \\end{pmatrix} = \\frac{1}{1.30^2 - 0.70^2} \\begin{pmatrix} 1.30 & -0.70 \\\\ -0.70 & 1.30 \\end{pmatrix} \\begin{pmatrix} 0.85 \\\\ 0.87 \\end{pmatrix} $$\n$$ \\hat{w}^{(2)} = \\frac{1}{1.20} \\begin{pmatrix} 0.496 \\\\ 0.536 \\end{pmatrix} \\approx \\begin{pmatrix} 0.413 \\\\ 0.447 \\end{pmatrix} $$\n\nComparing the new absolute coefficient magnitudes, $|\\hat{w}_1^{(2)}| \\approx 0.413$ and $|\\hat{w}_3^{(2)}| \\approx 0.447$.\nThe smaller coefficient belongs to $x_1$. RFE therefore eliminates $x_1$ in the second step.\n\n**Conclusion and Suboptimality Analysis**\n\nThe RFE process first eliminates $x_2$, then $x_1$, leaving $\\{x_3\\}$ as the final selected feature. This is a suboptimal outcome, as the ground-truth model is $y = x_1 + \\varepsilon$, meaning $x_1$ is the only truly predictive feature.\n\nTo quantify the suboptimality, we calculate the Mean Squared Prediction Error (MSE) for a model using only $x_3$ against the ground truth. The formula provided is $\\operatorname{MSE}(x) = \\sigma_\\varepsilon^2 + 1 - \\rho^2$, where $\\rho$ is the *population* correlation between the true feature $x_1$ and the selected feature $x$.\n\nGiven $\\sigma_\\varepsilon^2 = 0.20$ and the population correlation $\\operatorname{Corr}(x_1, x_3) = \\rho_{13} = 0.70$.\n$$ \\operatorname{MSE}(x_3) = 0.20 + 1 - (0.70)^2 = 0.20 + 1 - 0.49 = 0.71 $$\n\nThe optimal single-feature model using $x_1$ would have an MSE of $\\operatorname{MSE}(x_1) = 0.20 + 1 - (1)^2 = 0.20$, which is the irreducible error $\\sigma_\\varepsilon^2$. The RFE procedure, misled by multicollinearity and sampling noise in the empirical correlations, made a \"ranking inversion\" in the second step and selected a feature that leads to a significantly higher true prediction error.\n\nThis entire sequence of events and the final calculation matches option B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "The performance of a wrapper method is critically dependent on its search strategy. Simple greedy approaches like sequential forward selection are computationally efficient but can be easily trapped in local optima, failing to identify features that are only powerful in combination. This exercise  builds a clear counterexample where a synergistic pair of features is missed by a greedy search, and then explains how a more sophisticated \"floating\" search strategy can successfully identify the optimal subset by incorporating a backtracking mechanism.",
            "id": "4539705",
            "problem": "In radiomics-based binary classification, suppose a practitioner uses a wrapper approach that evaluates feature subsets by cross-validated accuracy of a fixed classifier. Let the target be malignant versus benign lesions, encoded as a binary label $Y \\in \\{0,1\\}$ with class prior $P(Y=1)=P(Y=0)=0.5$. Consider three handcrafted features derived from texture and shape: $f_1$, $f_2$, and $f_3$.\n\nBase definitions and facts:\n- In a wrapper method, a search procedure proposes feature subsets $S \\subseteq \\{f_1,f_2,f_3,\\dots\\}$, and each subset is scored by a generalization criterion such as cross-validated accuracy of a fixed learner $\\mathcal{L}$ trained only on features in $S$.\n- Greedy sequential forward selection starts from the empty set and iteratively adds the feature that yields the largest increase in the criterion, stopping when a size constraint or a stopping rule is met.\n- Sequential forward floating selection augments forward selection with conditional deletion: after each addition, it repeatedly removes the feature whose removal most improves the criterion, as long as improvement is obtained, thereby allowing backtracking over earlier choices.\n\nConstruct a counterexample, grounded in a plausible radiomics setting, where sequential forward selection fails to include a synergistic pair of features due to individually weak marginal effects, but sequential forward floating selection can recover the pair. Your construction must satisfy all of the following:\n- Specify a concrete data-generating mechanism for $(f_1,f_2,f_3) \\mid Y$ that is scientifically plausible for radiomics and makes $f_1$ and $f_2$ individually weak but jointly strong under a fixed nonlinear learner $\\mathcal{L}$ (e.g., a Support Vector Machine with a radial basis function kernel).\n- Argue from the definitions above why forward selection, with a size budget of $k=2$ features, will fail to include the synergistic pair.\n- Explain, using the mechanics of floating search, how it can nevertheless reach the synergistic pair despite the $k=2$ budget.\n- Use explicit, self-consistent numerical performance levels (e.g., cross-validated accuracies) that are consistent with your data-generating mechanism, and ensure that every claim you make follows from the mechanism and the wrapper definitions rather than from unmotivated heuristics.\n\nWhich option below presents a valid minimal counterexample that meets all the requirements and correctly explains the failure and remedy?\n\nA. Let $Y \\in \\{0,1\\}$ with $P(Y=1)=P(Y=0)=0.5$. Conditioned on $Y$, draw $f_3 \\mid Y \\sim \\mathcal{N}(\\mu_Y,1)$ with $\\mu_1=0.4$ and $\\mu_0=0$, yielding a weak marginally predictive feature. Let $(f_1,f_2)\\mid Y$ be generated by a latent common texture factor $U \\sim \\mathcal{N}(0,1)$ and small independent noise $\\varepsilon_1,\\varepsilon_2 \\sim \\mathcal{N}(0,0.2^2)$ as follows:\n- If $Y=1$, set $f_1 = U + \\varepsilon_1$, $f_2 = U + \\varepsilon_2$ (strong positive correlation).\n- If $Y=0$, set $f_1 = U + \\varepsilon_1$, $f_2 = -U + \\varepsilon_2$ (strong negative correlation).\nThen for each $j \\in \\{1,2\\}$, the marginal $f_j \\mid Y$ has identical distribution across classes (mean $0$ and variance approximately $1.04$), so individually $f_1$ and $f_2$ are non-informative, but jointly they define two nearly orthogonal manifolds in the $(f_1,f_2)$-plane that a Support Vector Machine with radial basis function kernel can separate with high accuracy. In $10$-fold cross-validation with fixed $\\mathcal{L}$:\n- Using $\\{f_1\\}$: accuracy $\\approx 0.50$,\n- Using $\\{f_2\\}$: accuracy $\\approx 0.50$,\n- Using $\\{f_3\\}$: accuracy $\\approx 0.58$,\n- Using $\\{f_1,f_2\\}$: accuracy $\\approx 0.90$,\n- Using $\\{f_3,f_1\\}$ or $\\{f_3,f_2\\}$: accuracy $\\approx 0.59$.\nWith a budget $k=2$, sequential forward selection picks $\\{f_3\\}$ at the first step (best single feature), and then at the second step adds either $f_1$ or $f_2$ for a negligible gain (to about $0.59$), exhausting the budget and missing the synergistic pair $\\{f_1,f_2\\}$. Sequential forward floating selection, however, can add $f_3$, then add $f_1$ (reaching about $0.59$), then perform a conditional addition to include $f_2$ temporarily (forming $\\{f_1,f_2,f_3\\}$ with accuracy near $0.90$), and finally remove $f_3$ in the conditional deletion step because its removal maintains or increases the score (yielding $\\{f_1,f_2\\}$ with accuracy $\\approx 0.90$), thus honoring the $k=2$ budget at the end.\n\nB. Let $Y \\in \\{0,1\\}$ with $P(Y=1)=P(Y=0)=0.5$. Define $f_1$ and $f_2$ as thresholded texture indicators such that $Y=1$ if and only if both $f_1>1$ and $f_2>1$, otherwise $Y=0$, and $f_3$ as pure noise independent of $Y$. A linear logistic regression as $\\mathcal{L}$ will immediately capture the joint rule when both $f_1$ and $f_2$ are present; sequential forward selection will therefore always pick both $\\{f_1,f_2\\}$ within two steps because their individual p-values are small, whereas floating search is unnecessary.\n\nC. Let $Y \\in \\{0,1\\}$, take $(f_1,f_2)$ as two nearly independent weak predictors with individual accuracies about $0.55$, and $f_3$ as a strong predictor with accuracy about $0.85$. Use a linear Support Vector Machine as $\\mathcal{L}$ and recursive feature elimination (RFE). Since RFE ranks by weight magnitude, it will always retain the best pair $\\{f_1,f_2\\}$ even if they are weak alone, and forward selection’s failure cannot occur in this setup; floating search offers no advantage.\n\nD. Let $Y \\in \\{0,1\\}$ with $P(Y=1)=P(Y=0)=0.5$. Define $(f_1,f_2)\\mid Y$ exactly as in option A, and $f_3$ as pure noise with accuracy about $0.50$. Use a linear discriminant classifier as $\\mathcal{L}$. Sequential forward selection will fail to include $\\{f_1,f_2\\}$ because linear discriminants cannot exploit correlation differences, but backward elimination from the full set will remove $f_3$ first and keep $\\{f_1,f_2\\}$, so floating search is not needed and provides no remedy under any budget constraint.\n\nSelect the single best option.",
            "solution": "The problem asks for a valid counterexample where Sequential Forward Selection (SFS) fails to find an optimal 2-feature subset, but Sequential Forward Floating Selection (SFFS) succeeds. This occurs in the presence of strong feature synergy, where a feature $f_3$ is individually better than features $f_1$ or $f_2$, but the pair $\\{f_1, f_2\\}$ is vastly superior to any other 2-feature combination. Option A provides a perfect and well-specified instance of this scenario.\n\n1.  **The Setup:** It defines a plausible data-generating mechanism.\n    *   Features $f_1$ and $f_2$ are constructed to have no individual predictive power (their marginal distributions are identical for both classes, leading to an accuracy of $\\approx 0.50$). However, their relationship changes with the class label (correlated for $Y=1$, anti-correlated for $Y=0$), creating a strong synergistic pair that a non-linear classifier like an RBF-SVM can separate with high accuracy ($\\approx 0.90$).\n    *   Feature $f_3$ is a weak, but non-random, predictor with an accuracy of $\\approx 0.58$.\n    *   The combination of $f_3$ with the individually weak $f_1$ or $f_2$ yields only a minor improvement (accuracy $\\approx 0.59$), which is far inferior to the synergistic pair.\n\n2.  **SFS Failure:** SFS is a greedy, forward-only algorithm.\n    *   **Step 1 (Select 1st feature):** It compares the performance of single-feature sets: $\\text{Acc}(\\{f_1\\}) \\approx 0.50$, $\\text{Acc}(\\{f_2\\}) \\approx 0.50$, $\\text{Acc}(\\{f_3\\}) \\approx 0.58$. It will correctly choose $\\{f_3\\}$ as the best single feature.\n    *   **Step 2 (Select 2nd feature):** It evaluates all possible additions to $\\{f_3\\}$: $\\text{Acc}(\\{f_3, f_1\\}) \\approx 0.59$ and $\\text{Acc}(\\{f_3, f_2\\}) \\approx 0.59$. It selects one of these, for instance $\\{f_3, f_1\\}$.\n    *   With its budget of $k=2$ exhausted, SFS is trapped. It has found a locally optimal path but missed the globally optimal subset $\\{f_1, f_2\\}$ with accuracy $\\approx 0.90$.\n\n3.  **SFFS Success:** SFFS enhances SFS with a backtracking (deletion) step.\n    *   After an SFS step results in a candidate set (e.g., $\\{f_3, f_1\\}$), the \"floating\" nature of the algorithm allows it to test if removing a feature and adding another improves the score. This effectively allows for \"swaps.\"\n    *   From the suboptimal set $\\{f_3, f_1\\}$, a swap of $f_3$ for $f_2$ would lead to an evaluation of the set $\\{f_1, f_2\\}$. The algorithm would discover its superior accuracy of $\\approx 0.90$ and select it. The mechanism described in option A, where the algorithm temporarily forms a 3-feature set and then prunes the worst one, is another valid path for an SFFS algorithm to find the optimal subset. In both cases, the ability to backtrack is key.\n\n4.  **Other Options:**\n    *   **B** is incorrect because SFS would likely succeed in that scenario, and it mischaracterizes how linear models handle interaction.\n    *   **C** is incorrect because it discusses the wrong algorithm (RFE).\n    *   **D** is incorrect because it pairs the synergistic data with a linear learner that cannot exploit the synergy, violating a key premise of the problem.\n\nTherefore, Option A is the only one that presents a fully correct and internally consistent counterexample.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}