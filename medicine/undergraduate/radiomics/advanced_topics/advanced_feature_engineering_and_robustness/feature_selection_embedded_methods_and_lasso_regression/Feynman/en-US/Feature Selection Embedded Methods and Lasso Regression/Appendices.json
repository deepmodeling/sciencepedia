{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how LASSO performs its \"selection\" trick, we must look under the hood at its optimization algorithm. This practice guides you through the derivation of the coordinate descent update, the workhorse behind fitting LASSO models efficiently. By solving this one-dimensional problem, you will discover the soft-thresholding operator and gain a concrete understanding of how the regularization parameter $\\lambda$ forces coefficients to become exactly zero.",
            "id": "4538688",
            "problem": "A radiomics pipeline is used to predict a continuous marker of tumor aggressiveness from computed tomography texture features. To perform embedded feature selection, the model employs the Least Absolute Shrinkage and Selection Operator (LASSO) regression, which minimizes the penalized empirical risk $$\\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$ where $y \\in \\mathbb{R}^{n}$ is the response, $X \\in \\mathbb{R}^{n \\times p}$ is the feature matrix, $\\beta \\in \\mathbb{R}^{p}$ are the coefficients, $n$ is the sample size, and $\\lambda > 0$ is the regularization parameter. Features have been centered so that the intercept is unpenalized and can be ignored for this update.\n\nYou are implementing coordinate descent at the current iterate, holding all coefficients except $\\beta_{j}$ fixed. Let the partial residual with respect to feature $j$ be defined as $$r^{(j)} = y - \\sum_{k \\neq j} x_{k}\\beta_{k},$$ where $x_{k} \\in \\mathbb{R}^{n}$ denotes the $k$-th column of $X$. The one-dimensional subproblem for $\\beta_{j}$ is $$\\min_{\\beta_{j} \\in \\mathbb{R}} \\;\\; \\frac{1}{2n}\\sum_{i=1}^{n}\\left(r^{(j)}_{i} - x_{ij}\\beta_{j}\\right)^{2} + \\lambda |\\beta_{j}|.$$\n\nStarting from the stated objective and definitions, derive the coordinate descent update for $\\beta_{j}$ by solving this one-dimensional convex problem and interpreting the solution in terms of soft-thresholding of the partial residual. Then, compute the numerical update using the following data for a single feature in a cohort of $n = 8$ patients:\n- Feature column $x_{j} = [\\,0.5,\\,-1.1,\\,0.7,\\,0.0,\\,1.4,\\,-0.6,\\,0.8,\\,-1.7\\,]$,\n- Partial residual $r^{(j)} = [\\,2.3,\\,-0.9,\\,1.2,\\,0.0,\\,3.1,\\,-1.5,\\,1.0,\\,-2.8\\,]$,\n- Regularization parameter $\\lambda = 0.3$.\n\nProvide the updated value of $\\beta_{j}$ for this coordinate descent step. Round your answer to four significant figures. Finally, explain why this update mechanism is computationally efficient in high-dimensional radiomics settings where $p \\gg n$.",
            "solution": "The problem asks for three things: first, the derivation of the coordinate descent update rule for a LASSO regression coefficient $\\beta_j$; second, the numerical computation of this update for a given feature; and third, an explanation of the computational efficiency of this method in high-dimensional settings.\n\nThe problem is validated as scientifically grounded, well-posed, and objective. All definitions and premises are standard in statistical learning theory and the context of LASSO regression. The provided data is self-contained and consistent. Therefore, a solution can be derived.\n\n**Part 1: Derivation of the Coordinate Descent Update**\n\nThe objective is to solve the one-dimensional subproblem for the coefficient $\\beta_j$, holding all other coefficients $\\beta_k$ for $k \\neq j$ fixed. The objective function to minimize with respect to $\\beta_j$ is given by:\n$$L(\\beta_j) = \\frac{1}{2n}\\sum_{i=1}^{n}\\left(r^{(j)}_{i} - x_{ij}\\beta_{j}\\right)^{2} + \\lambda |\\beta_{j}|$$\nwhere the partial residual is $r^{(j)} = y - \\sum_{k \\neq j} x_{k}\\beta_{k}$.\nThis objective function is convex, as it is the sum of a convex quadratic function and a convex absolute value function. However, the absolute value term $|\\beta_j|$ makes the function non-differentiable at $\\beta_j = 0$. We can find the minimum by using subgradient calculus. The condition for $\\beta_j$ to be a minimizer is that $0$ must be contained in the subdifferential of $L(\\beta_j)$, denoted $\\partial L(\\beta_j)$.\n\nThe subdifferential is computed as:\n$$ \\partial L(\\beta_j) = \\frac{d}{d\\beta_j} \\left( \\frac{1}{2n}\\sum_{i=1}^{n}\\left(r^{(j)}_{i} - x_{ij}\\beta_{j}\\right)^{2} \\right) + \\lambda \\cdot \\partial(|\\beta_j|) $$\nThe derivative of the quadratic part is:\n$$ \\frac{1}{2n}\\sum_{i=1}^{n} 2(r^{(j)}_{i} - x_{ij}\\beta_{j})(-x_{ij}) = -\\frac{1}{n}\\sum_{i=1}^{n} (x_{ij}r^{(j)}_{i} - x_{ij}^2\\beta_j) = \\frac{1}{n} \\left( \\beta_j \\sum_{i=1}^{n}x_{ij}^2 - \\sum_{i=1}^{n}x_{ij}r^{(j)}_{i} \\right) $$\nLet's define $c_j = \\sum_{i=1}^{n}x_{ij}r^{(j)}_{i} = x_j^T r^{(j)}$ and $S_j = \\sum_{i=1}^{n}x_{ij}^2 = \\|x_j\\|_2^2$. The derivative becomes $\\frac{1}{n} (S_j \\beta_j - c_j)$.\n\nThe subdifferential of the absolute value function is:\n$$ \\partial(|\\beta_j|) = \\begin{cases} \\{1\\} & \\text{if } \\beta_j > 0 \\\\ \\{-1\\} & \\text{if } \\beta_j < 0 \\\\ [-1, 1] & \\text{if } \\beta_j = 0 \\end{cases} $$\n\nNow, we set $0 \\in \\partial L(\\beta_j)$:\n$$ 0 \\in \\frac{1}{n} (S_j \\beta_j - c_j) + \\lambda \\cdot \\partial(|\\beta_j|) $$\n\nWe analyze three cases for the value of $\\beta_j$:\n\n**Case 1: $\\beta_j > 0$**\nThe subgradient is a single value, $\\partial(|\\beta_j|) = \\{1\\}$. The condition becomes an equation:\n$$ 0 = \\frac{1}{n} (S_j \\beta_j - c_j) + \\lambda $$\n$$ S_j \\beta_j - c_j = -n\\lambda $$\n$$ \\beta_j = \\frac{c_j - n\\lambda}{S_j} $$\nFor this solution to be consistent with the assumption $\\beta_j > 0$, we must have $c_j - n\\lambda > 0$, which implies $c_j > n\\lambda$.\n\n**Case 2: $\\beta_j < 0$**\nThe subgradient is $\\partial(|\\beta_j|) = \\{-1\\}$. The condition becomes:\n$$ 0 = \\frac{1}{n} (S_j \\beta_j - c_j) - \\lambda $$\n$$ S_j \\beta_j - c_j = n\\lambda $$\n$$ \\beta_j = \\frac{c_j + n\\lambda}{S_j} $$\nFor this solution to be consistent with the assumption $\\beta_j < 0$, we must have $c_j + n\\lambda < 0$, which implies $c_j < -n\\lambda$.\n\n**Case 3: $\\beta_j = 0$**\nThe subgradient is an interval, $\\partial(|\\beta_j|) = [-1, 1]$. The condition becomes an inclusion:\n$$ 0 \\in \\frac{1}{n} (S_j \\cdot 0 - c_j) + \\lambda [-1, 1] $$\n$$ 0 \\in -\\frac{c_j}{n} + [-\\lambda, \\lambda] $$\nThis means there exists some value $g \\in [-\\lambda, \\lambda]$ such that $0 = -c_j/n + g$, or $g = c_j/n$. The condition for this case to hold is that this value of $g$ must be in its allowed range:\n$$ -\\lambda \\le \\frac{c_j}{n} \\le \\lambda \\iff |c_j| \\le n\\lambda $$\n\nCombining these three cases yields the complete solution for $\\beta_j$:\n$$ \\beta_j = \\begin{cases} \\frac{c_j - n\\lambda}{S_j} & \\text{if } c_j > n\\lambda \\\\ \\frac{c_j + n\\lambda}{S_j} & \\text{if } c_j < -n\\lambda \\\\ 0 & \\text{if } |c_j| \\le n\\lambda \\end{cases} $$\nThis can be written compactly using the soft-thresholding operator, $S_{\\alpha}(z) = \\text{sign}(z) \\max(0, |z|-\\alpha)$. The solution is:\n$$ \\beta_j = \\frac{S_{n\\lambda}(c_j)}{S_j} = \\frac{S_{n\\lambda}(x_j^T r^{(j)})}{\\|x_j\\|_2^2} $$\nThis expression represents the coordinate-wise update rule. It shows that the update is found by computing the dot product of the feature vector $x_j$ with its partial residual $r^{(j)}$, soft-thresholding this value, and then normalizing by the squared Euclidean norm of the feature vector.\n\n**Part 2: Numerical Calculation**\n\nGiven the data:\n- Sample size $n = 8$\n- Feature vector $x_{j} = [\\,0.5,\\,-1.1,\\,0.7,\\,0.0,\\,1.4,\\,-0.6,\\,0.8,\\,-1.7\\,]$\n- Partial residual $r^{(j)} = [\\,2.3,\\,-0.9,\\,1.2,\\,0.0,\\,3.1,\\,-1.5,\\,1.0,\\,-2.8\\,]$\n- Regularization parameter $\\lambda = 0.3$\n\nFirst, we compute $c_j = x_j^T r^{(j)}$:\n$$ c_j = (0.5)(2.3) + (-1.1)(-0.9) + (0.7)(1.2) + (0.0)(0.0) + (1.4)(3.1) + (-0.6)(-1.5) + (0.8)(1.0) + (-1.7)(-2.8) $$\n$$ c_j = 1.15 + 0.99 + 0.84 + 0.0 + 4.34 + 0.90 + 0.80 + 4.76 = 13.78 $$\n\nNext, we compute $S_j = \\|x_j\\|_2^2$:\n$$ S_j = (0.5)^2 + (-1.1)^2 + (0.7)^2 + (0.0)^2 + (1.4)^2 + (-0.6)^2 + (0.8)^2 + (-1.7)^2 $$\n$$ S_j = 0.25 + 1.21 + 0.49 + 0.0 + 1.96 + 0.36 + 0.64 + 2.89 = 7.80 $$\n\nThen, we calculate the threshold for the soft-thresholding operator:\n$$ n\\lambda = 8 \\times 0.3 = 2.4 $$\n\nNow we apply the update rule. We compare $|c_j|$ with $n\\lambda$:\n$$ c_j = 13.78 > n\\lambda = 2.4 $$\nThis corresponds to the first case of our derived solution.\n$$ \\beta_j = \\frac{c_j - n\\lambda}{S_j} = \\frac{13.78 - 2.4}{7.80} = \\frac{11.38}{7.80} \\approx 1.458974... $$\nRounding to four significant figures, the updated value for $\\beta_j$ is $1.459$.\n\n**Part 3: Computational Efficiency in High-Dimensional Settings ($p \\gg n$)**\n\nThe coordinate descent algorithm is highly efficient for high-dimensional problems, particularly where the number of features $p$ is much larger than the number of samples $n$, for several key reasons:\n\n1.  **Avoidance of Large Matrix Operations**: The most significant advantage is that it avoids the need to compute, store, or invert the $p \\times p$ Gram matrix $X^T X$. In a $p \\gg n$ scenario, this matrix would be enormous ($p \\times p$), and operations like inversion (costing $O(p^3)$) or even its formation (costing $O(p^2 n)$) are computationally prohibitive. Coordinate descent circumvents this by breaking the problem down into $p$ one-dimensional optimizations.\n\n2.  **Low Cost per Update**: Each coordinate update only involves operations on vectors of length $n$. The main computations for updating $\\beta_j$ are the dot product $x_j^T r$ and a vector update for the residual, each costing $O(n)$ operations. A full cycle through all $p$ features thus has a total complexity of $O(np)$. This linear scaling with $p$ is far more tractable than polynomial scaling ($O(p^2)$ or $O(p^3)$). A single step of proximal gradient descent also costs $O(np)$, but coordinate descent often converges in fewer effective passes, especially when combined with active-set strategies.\n\n3.  **Efficient Residual Updates**: The partial residual $r^{(j)}$ does not need to be recomputed from scratch for each $j$. Instead, a global residual $r = y - X\\beta$ is maintained. The term required for the update, $x_j^T r^{(j)}$, can be efficiently calculated as $x_j^T r^{(j)} = x_j^T(r + x_j \\beta_j)$, where $r$ and $\\beta_j$ are the values before the update. After updating $\\beta_j$ to $\\beta_j^{\\text{new}}$, the global residual is updated via a simple and fast rank-$1$ update: $r^{\\text{new}} = r - x_j(\\beta_j^{\\text{new}} - \\beta_j)$. This costs only $O(n)$ work.\n\n4.  **Exploitation of Sparsity**: LASSO is designed to produce sparse solutions (many $\\beta_j = 0$). Coordinate descent is particularly well-suited to this. As the algorithm proceeds, many coefficients are set to zero and may remain zero over subsequent iterations (if $|x_j^T r| \\le n\\lambda$). This allows for the use of \"active set\" methods, where the algorithm cycles only over the subset of features with non-zero coefficients, drastically reducing the effective value of $p$ and accelerating convergence.\n\nIn radiomics, where it is common to extract thousands of features ($p \\gg 1000$) from a cohort of a few hundred patients ($n \\sim 100-500$), these computational advantages make coordinate descent the workhorse algorithm for fitting LASSO models.",
            "answer": "$$\\boxed{1.459}$$"
        },
        {
            "introduction": "The $\\ell_1$ penalty treats all coefficients equally, but what about the features themselves? This exercise explores a critical prerequisite for applying the LASSO update rule (): feature standardization. Using a hypothetical radiomics scenario with features measured on vastly different scales, you will mathematically demonstrate how feature scaling directly impacts which variables are penalized, revealing why this preprocessing step is essential for fair and effective regularization.",
            "id": "4538694",
            "problem": "A radiomics analyst is fitting a linear model for a quantitative imaging biomarker, using two features measured from computed tomography scans: $x_{1}$ is the mean lesion intensity in Hounsfield Units, and $x_{2}$ is a dimensionless gray-level co-occurrence matrix energy feature. The response $y$ and each feature column are centered so that $y$ and $x_{j}$ have mean zero. The analyst uses the Least Absolute Shrinkage and Selection Operator (LASSO) penalty, whose objective for parameter vector $\\beta$ is\n$$\n\\frac{1}{2n}\\|y - X\\beta\\|^{2} + \\lambda \\sum_{j=1}^{p} |\\beta_{j}|,\n$$\nwhere $n$ is the number of cases, $X$ is the design matrix with columns $x_{1}$ and $x_{2}$, and $\\lambda \\ge 0$ is the regularization parameter. Assume that the two feature columns are orthogonal in the sense that $\\frac{1}{n} x_{1}^{\\top}x_{2} = 0$.\n\nSuppose the following empirical quantities are measured on $n$ lesions:\n$$\n\\frac{1}{n} x_{1}^{\\top} x_{1} = 10{,}000, \\quad \\frac{1}{n} x_{2}^{\\top} x_{2} = 1, \\quad \\frac{1}{n} x_{1}^{\\top} y = 500, \\quad \\frac{1}{n} x_{2}^{\\top} y = 4.\n$$\nThese values reflect that $x_{1}$ has units of Hounsfield Units and much larger variance than the dimensionless $x_{2}$.\n\nStarting from the LASSO objective definition and the given orthogonality, analyze how scaling each feature to unit variance changes both the magnitude of the LASSO coefficients and the per-feature selection thresholds in terms of $\\lambda$. Let $r_{\\text{unstd}}$ denote the ratio of the minimal penalty parameters required to set the coefficient of $x_{1}$ to zero versus that for $x_{2}$ when the original (unstandardized) features are used, and let $r_{\\text{std}}$ denote the same ratio after standardizing each feature column to have unit variance. Compute the single number $r_{\\text{unstd}} / r_{\\text{std}}$. No rounding is necessary; provide the exact value with no units.",
            "solution": "The problem requires an analysis of the effect of feature standardization on the LASSO regression penalty. We are asked to find the ratio of two quantities, $r_{\\text{unstd}}$ and $r_{\\text{std}}$, which themselves are ratios of regularization parameter thresholds.\n\nThe LASSO objective function to be minimized is given by:\n$$L(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|^{2} + \\lambda \\sum_{j=1}^{p} |\\beta_{j}|$$\nwhere $\\beta$ is the vector of coefficients, $X$ is the design matrix with columns representing features, $y$ is the response vector, $n$ is the number of samples, $p$ is the number of features (here $p=2$), and $\\lambda \\ge 0$ is the regularization parameter.\n\nThe solution to the LASSO problem, $\\hat{\\beta}$, must satisfy the Karush-Kuhn-Tucker (KKT) conditions. The subgradient of the objective function with respect to each coefficient $\\beta_j$ must contain zero at the solution. The subgradient of $L(\\beta)$ with respect to $\\beta_j$ is:\n$$\\frac{\\partial L}{\\partial \\beta_j} = \\frac{1}{n} (-x_j^\\top(y-X\\beta)) + \\lambda \\cdot s_j$$\nwhere $s_j$ is an element of the subdifferential of the absolute value function, i.e., $s_j = \\text{sgn}(\\beta_j)$ if $\\beta_j \\neq 0$, and $s_j \\in [-1, 1]$ if $\\beta_j = 0$.\nSetting the subgradient to zero gives the KKT condition for $\\beta_j$:\n$$\\frac{1}{n} x_j^\\top(y-X\\hat{\\beta}) = \\lambda s_j$$\nExpanding the term $X\\hat{\\beta} = \\sum_{k=1}^p x_k \\hat{\\beta}_k$:\n$$\\frac{1}{n} x_j^\\top y - \\sum_{k=1}^p \\hat{\\beta}_k \\left(\\frac{1}{n} x_j^\\top x_k\\right) = \\lambda s_j$$\nThe problem states that the features are orthogonal, which means $\\frac{1}{n} x_j^\\top x_k = 0$ for $j \\neq k$. This greatly simplifies the condition, as the sum collapses to a single term where $k=j$:\n$$\\frac{1}{n} x_j^\\top y - \\hat{\\beta}_j \\left(\\frac{1}{n} x_j^\\top x_j\\right) = \\lambda s_j$$\nLet's define the empirical covariance of feature $j$ with the response $y$ as $c_j = \\frac{1}{n} x_j^\\top y$, and the empirical variance of feature $j$ as $v_j = \\frac{1}{n} x_j^\\top x_j$. The condition becomes:\n$$c_j - v_j \\hat{\\beta}_j = \\lambda s_j$$\nThe problem asks for the minimal penalty parameter $\\lambda$ required to set a coefficient to zero. A coefficient $\\hat{\\beta}_j$ is zero if and only if $s_j \\in [-1, 1]$. In this case, the condition becomes $c_j = \\lambda s_j$, which is equivalent to $|c_j| \\le \\lambda$.\nThe minimal non-negative value of $\\lambda$ that ensures $\\hat{\\beta}_j = 0$ is therefore $\\lambda_j = |c_j|$. This value represents the threshold at which the coefficient $\\beta_j$ is shrunk to exactly zero.\n\nWith this general result, we can now analyze the two cases.\n\n**Case 1: Unstandardized Features**\nWe use the features $x_1$ and $x_2$ as given. The relevant empirical quantities are:\n$$c_1 = \\frac{1}{n} x_1^\\top y = 500$$\n$$c_2 = \\frac{1}{n} x_2^\\top y = 4$$\nThe minimal penalty parameter required to set $\\beta_1$ to zero is:\n$$\\lambda_{1, \\text{unstd}} = |c_1| = |500| = 500$$\nThe minimal penalty parameter required to set $\\beta_2$ to zero is:\n$$\\lambda_{2, \\text{unstd}} = |c_2| = |4| = 4$$\nThe ratio $r_{\\text{unstd}}$ is defined as the ratio of these thresholds:\n$$r_{\\text{unstd}} = \\frac{\\lambda_{1, \\text{unstd}}}{\\lambda_{2, \\text{unstd}}} = \\frac{500}{4} = 125$$\n\n**Case 2: Standardized Features**\nStandardizing a feature involves scaling it to have a unit variance. The problem states the features are already centered (mean zero). The sample variance of feature $x_j$ is $v_j = \\frac{1}{n} x_j^\\top x_j$. The scaling factor for each feature is its sample standard deviation, $s_j = \\sqrt{v_j}$.\nFrom the givens:\n$$v_1 = \\frac{1}{n} x_1^\\top x_1 = 10{,}000 \\implies s_1 = \\sqrt{10{,}000} = 100$$\n$$v_2 = \\frac{1}{n} x_2^\\top x_2 = 1 \\implies s_2 = \\sqrt{1} = 1$$\nThe standardized features, which we denote $x'_1$ and $x'_2$, are:\n$$x'_1 = \\frac{x_1}{s_1} = \\frac{x_1}{100}$$\n$$x'_2 = \\frac{x_2}{s_2} = \\frac{x_2}{1} = x_2$$\nWe now consider a new LASSO problem with these standardized features. The orthogonality condition is preserved:\n$$\\frac{1}{n} (x'_1)^\\top x'_2 = \\frac{1}{n} \\left(\\frac{x_1}{100}\\right)^\\top x_2 = \\frac{1}{100} \\left(\\frac{1}{n} x_1^\\top x_2\\right) = \\frac{1}{100} \\cdot 0 = 0$$\nThe minimal $\\lambda$ thresholds for the new coefficients $\\beta'_1$ and $\\beta'_2$ are determined by the new empirical covariances with the response, $c'_1$ and $c'_2$:\n$$c'_1 = \\frac{1}{n} (x'_1)^\\top y = \\frac{1}{n} \\left(\\frac{x_1}{100}\\right)^\\top y = \\frac{1}{100} \\left(\\frac{1}{n} x_1^\\top y\\right) = \\frac{500}{100} = 5$$\n$$c'_2 = \\frac{1}{n} (x'_2)^\\top y = \\frac{1}{n} x_2^\\top y = 4$$\nThe minimal penalty parameter to set $\\beta'_1$ to zero is:\n$$\\lambda_{1, \\text{std}} = |c'_1| = |5| = 5$$\nThe minimal penalty parameter to set $\\beta'_2$ to zero is:\n$$\\lambda_{2, \\text{std}} = |c'_2| = |4| = 4$$\nThe ratio $r_{\\text{std}}$ is:\n$$r_{\\text{std}} = \\frac{\\lambda_{1, \\text{std}}}{\\lambda_{2, \\text{std}}} = \\frac{5}{4}$$\n\n**Final Calculation**\nThe problem asks for the single number representing the ratio $r_{\\text{unstd}} / r_{\\text{std}}$.\n$$\\frac{r_{\\text{unstd}}}{r_{\\text{std}}} = \\frac{125}{\\frac{5}{4}} = 125 \\times \\frac{4}{5} = \\frac{500}{5} = 100$$\nThis result highlights how feature scaling dramatically affects the behavior of LASSO, which is sensitive to the scale of predictors. The unstandardized features suggest that $x_1$ is far more important than $x_2$ (by a factor of $125$), while standardizing reveals their importance to be much more comparable (a factor of $1.25$). The ratio of these ratios is $100$.",
            "answer": "$$\\boxed{100}$$"
        },
        {
            "introduction": "Building a robust predictive model involves more than just fitting an algorithm; it requires a rigorous process for tuning hyperparameters like $\\lambda$ and obtaining an unbiased estimate of performance. This final practice challenges you to design a complete, nested cross-validation workflow, the gold standard for this task. By carefully separating the inner loop for model selection from the outer loop for performance estimation, you will learn how to prevent information leakage and produce a trustworthy evaluation of your LASSO model.",
            "id": "4538665",
            "problem": "You are given a high-dimensional radiomics dataset with $n=240$ patients, each represented by $p=1200$ quantitative features extracted from Computed Tomography (CT) images. The binary outcome $y \\in \\{0,1\\}^n$ denotes malignancy status. You plan to train a logistic regression classifier with a Least Absolute Shrinkage and Selection Operator (LASSO) penalty, which performs embedded feature selection by shrinking some coefficients to $0$. You must select the regularization strength $\\lambda$ and estimate the generalization performance without information leakage.\n\nConstruct a scientifically sound nested Cross-Validation (CV) protocol to select $\\lambda$ and explain how it prevents information leakage from the inner model-selection process to the outer performance evaluation. Assume an outer $K=5$ fold partition and, within each outer training split, an inner $M=3$ fold CV to tune $\\lambda$. Feature standardization (mean-zero, unit-variance) is required before fitting and must be part of the protocol. The performance metric is a loss function $\\ell$ appropriate for binary classification, computed on held-out data. Choose the option that correctly specifies the steps and the leakage prevention rationale.\n\nA. Split the data into $K=5$ outer folds. For each outer fold $k$, hold out $D_{\\text{test}}^{(k)}$ and use $D_{\\text{train}}^{(k)}$ only to build a preprocessing-and-model pipeline: compute feature standardization parameters using $D_{\\text{train}}^{(k)}$; within $D_{\\text{train}}^{(k)}$, perform $M=3$ fold inner CV over a grid $\\{\\lambda_1,\\ldots,\\lambda_L\\}$, where in each inner split the standardization is fit on the inner training subset and applied to the inner validation subset; choose $\\hat{\\lambda}^{(k)}$ that minimizes the inner CV loss $\\ell$; refit the standardized LASSO logistic regression on $D_{\\text{train}}^{(k)}$ using $\\hat{\\lambda}^{(k)}$; finally, evaluate $\\ell$ on $D_{\\text{test}}^{(k)}$. Aggregate the outer test losses across the $K$ folds. This prevents leakage because $D_{\\text{test}}^{(k)}$ is not used to compute preprocessing statistics or to select $\\lambda$, so the outer evaluation is independent of inner model selection.\n\nB. Compute feature standardization parameters once using the full dataset $(X,y)$, standardize all data, then run nested CV with $K=5$ outer folds and $M=3$ inner folds to select $\\lambda$ and evaluate performance. This prevents leakage because nested CV separates tuning and evaluation, and standardizing on all samples improves stability.\n\nC. Run $K=5$ outer folds. For each fold, evaluate a grid $\\{\\lambda_1,\\ldots,\\lambda_L\\}$ by training on the outer training set and computing $\\ell$ on the outer test set for every $\\lambda$; choose the $\\lambda$ that minimizes the outer test loss, refit on the full outer training set, and then report the average outer test loss. Standardization parameters are computed on the outer training set once per fold.\n\nD. Use a single $K=5$ CV: within each fold, compute standardization on the training split, scan $\\{\\lambda_1,\\ldots,\\lambda_L\\}$, and choose the best $\\lambda$ by the same CV. Report the mean CV loss as the generalization performance. Leakage is avoided because regularization penalizes complexity, making the estimate robust.",
            "solution": "The user has provided a problem statement regarding the proper implementation of a nested cross-validation (CV) protocol for hyperparameter tuning and performance estimation of a LASSO logistic regression model in a high-dimensional radiomics setting. The key objectives are to select the regularization parameter $\\lambda$ and to estimate the generalization error without information leakage.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the specified criteria.\n\n**Step 1: Extract Givens**\n-   Dataset type: High-dimensional radiomics.\n-   Number of samples (patients): $n=240$.\n-   Number of features: $p=1200$.\n-   Feature type: Quantitative, from Computed Tomography (CT) images.\n-   Outcome variable: $y \\in \\{0,1\\}^n$, a binary variable for malignancy status.\n-   Model: Logistic regression with a Least Absolute Shrinkage and Selection Operator (LASSO) penalty.\n-   Task: Select the regularization strength $\\lambda$ and estimate generalization performance.\n-   Constraint: Avoid information leakage.\n-   Methodology: Nested Cross-Validation (CV).\n-   Outer loop folds: $K=5$.\n-   Inner loop folds: $M=3$.\n-   Preprocessing: Feature standardization (mean-zero, unit-variance) is required.\n-   Performance metric: A loss function $\\ell$ appropriate for binary classification, evaluated on held-out data.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is grounded in standard, well-established practices in machine learning and biostatistics. The scenario ($p \\gg n$, i.e., $1200 > 240$) is common in fields like radiomics and genomics, necessitating regularization techniques like LASSO. Nested cross-validation is the canonical and scientifically correct procedure for simultaneously performing hyperparameter tuning and obtaining a nearly unbiased estimate of a model's generalization performance. The concern about information leakage, particularly from preprocessing steps (standardization) and hyperparameter selection, is a critical and valid issue in model validation. The problem is scientifically sound.\n-   **Well-Posed:** The problem is well-posed. It asks for the correct specification of a nested CV protocol, for which a standard, correct procedure exists. The goal is clearly stated and all necessary parameters ($n$, $p$, $K$, $M$, model type, preprocessing step) are provided.\n-   **Objective:** The problem is stated using objective, precise, and standard terminology from statistics and machine learning. There are no subjective or ambiguous statements.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. It is scientifically sound, well-posed, objective, and describes a realistic and important task in applied machine learning. I will now proceed with the solution and option-by-option analysis.\n\n### Principle-Based Derivation\n\nThe fundamental principle of unbiased performance estimation is the strict separation of data used for any aspect of model training from the data used for final model evaluation. In a nested cross-validation setting, this principle is applied at two levels.\n\n1.  **Outer Loop (Performance Estimation):** The dataset is partitioned into $K$ folds. For each fold $k \\in \\{1, \\dots, K\\}$, the $k$-th fold serves as the outer test set, $D_{\\text{test}}^{(k)}$, which is held out. The remaining $K-1$ folds constitute the outer training set, $D_{\\text{train}}^{(k)}$. The outer loop's purpose is to generate $K$ independent (or nearly independent) estimates of performance on data completely unseen by the model-building process.\n\n2.  **Inner Loop (Model Selection/Hyperparameter Tuning):** Within each iteration of the outer loop, a complete model-building pipeline is executed using *only* the outer training set, $D_{\\text{train}}^{(k)}$. This pipeline must select the optimal hyperparameter, $\\lambda$. To do this, $D_{\\text{train}}^{(k)}$ is itself partitioned into $M$ inner folds. For a grid of candidate $\\lambda$ values, a standard $M$-fold CV is performed: for each $\\lambda$, a model is trained on $M-1$ inner folds and validated on the held-out inner fold. The average performance across the $M$ inner validation folds is computed for each $\\lambda$. The hyperparameter that yields the best average performance, denoted $\\hat{\\lambda}^{(k)}$, is selected.\n\n3.  **Preprocessing and Information Leakage:** A critical source of information leakage is data preprocessing. To prevent this, any parameters required for preprocessing (e.g., the mean and standard deviation for standardization) must be computed *exclusively* from the training data at any given stage.\n    -   In the inner loop, for each inner split, standardization parameters must be computed from the inner training data and then applied to both the inner training and inner validation data.\n    -   After the optimal hyperparameter $\\hat{\\lambda}^{(k)}$ is found, a final model for the outer fold $k$ is trained. This model is trained on the *entire* outer training set, $D_{\\text{train}}^{(k)}$, using $\\hat{\\lambda}^{(k)}$. For this step, standardization parameters are computed from the entire $D_{\\text{train}}^{(k)}$ and are applied to both $D_{\\text{train}}^{(k)}$ (for model fitting) and $D_{\\text{test}}^{(k)}$ (for evaluation).\n\n4.  **Final Evaluation:** The model trained on $D_{\\text{train}}^{(k)}$ with hyperparameter $\\hat{\\lambda}^{(k)}$ is evaluated on the pristine outer test set, $D_{\\text{test}}^{(k)}$. This yields the performance estimate for fold $k$. The final generalization performance estimate is the average of the $K$ performance estimates from each outer fold.\n\nThis nested structure ensures that the selection of $\\hat{\\lambda}^{(k)}$ and the computation of all preprocessing parameters occur without any knowledge of $D_{\\text{test}}^{(k)}$. Therefore, the evaluation on $D_{\\text{test}}^{(k)}$ provides a nearly unbiased estimate of the performance of the entire model-building *procedure* (including hyperparameter tuning) on new data.\n\n### Option-by-Option Analysis\n\n**Option A:** This option describes the nested CV protocol as follows:\n1.  Split data into $K=5$ outer folds.\n2.  For each outer fold $k$, use $D_{\\text{train}}^{(k)}$ to build the pipeline.\n3.  Within $D_{\\text{train}}^{(k)}$, perform $M=3$ fold inner CV over a grid of $\\lambda$ values.\n4.  Crucially, it specifies that \"in each inner split the standardization is fit on the inner training subset and applied to the inner validation subset\". This correctly handles preprocessing within the inner loop.\n5.  Select $\\hat{\\lambda}^{(k)}$ based on the inner CV performance.\n6.  Refit the model on the entire $D_{\\text{train}}^{(k)}$ using $\\hat{\\lambda}^{(k)}$. The description implies that standardization parameters are learned from $D_{\\text{train}}^{(k)}$ for this step.\n7.  Evaluate performance on $D_{\\text{test}}^{(k)}$.\n8.  Aggregate the $K$ test losses.\nThe rationale provided, \"$D_{\\text{test}}^{(k)}$ is not used to compute preprocessing statistics or to select $\\lambda$\", is precisely correct. This procedure adheres exactly to the principles derived above.\n**Verdict:** Correct.\n\n**Option B:** This option suggests computing standardization parameters once on the full dataset before starting any cross-validation. This is a classic and critical error that causes information leakage. Information about the data distribution of the test sets is \"leaked\" into the training sets, as the means and variances used for standardization have been influenced by all data points. This will lead to an optimistically biased performance estimate. The claim that this \"improves stability\" is a poor trade-off for methodological invalidity.\n**Verdict:** Incorrect.\n\n**Option C:** This option describes a procedure where, for each outer fold, the grid of $\\lambda$ values is evaluated directly on the outer test set ($D_{\\text{test}}^{(k)}$) to find the best $\\lambda$. This is not nested CV. It is a fundamental error in validation methodology, as it amounts to using the test set as a validation set for hyperparameter tuning. The performance reported would be the minimum loss found on the test set, which is a heavily biased and optimistic estimate of true generalization error. The test set is no longer a source of unbiased evaluation.\n**Verdict:** Incorrect.\n\n**Option D:** This option describes a single, non-nested $K=5$ fold CV. It suggests that within each fold, one tunes $\\lambda$. If it means tuning and evaluating on the same validation fold, this leads to an optimistic bias (\"winner's curse\"). The performance estimate for each fold is the best performance achieved on the validation set, not the performance on a truly held-out test set. While simple CV is a valid technique for *either* hyperparameter tuning *or* performance estimation (if the hyperparameter is fixed), it cannot be used to do both simultaneously and produce an unbiased estimate. The provided rationale, that \"regularization penalizes complexity, making the estimate robust,\" is a non-sequitur; the statistical bias in the estimation procedure is a separate issue from the variance-reduction property of the model itself.\n**Verdict:** Incorrect.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}