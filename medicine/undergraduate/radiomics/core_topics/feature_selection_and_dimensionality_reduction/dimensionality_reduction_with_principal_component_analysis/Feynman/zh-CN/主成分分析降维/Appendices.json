{
    "hands_on_practices": [
        {
            "introduction": "本练习旨在探索主成分分析（PCA）的数学核心。当特征之间存在完全相关（即共线性）时，会发生什么？这是一个基础性练习，可以帮助您理解PCA如何识别并消除冗余信息，从而揭示数据的真实“内在维度”。通过这个练习，您将明白为何PCA是如此强大的降维工具。",
            "id": "3191985",
            "problem": "考虑一个零均值随机向量 $\\mathbf{X} = (X_{1}, X_{2}, X_{3})^{\\top}$，其构造如下。设 $(X_{1}, X_{2})^{\\top}$ 是一个联合分布的零均值对，其协方差矩阵为 $\\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$，并定义 $X_{3} = X_{1} + X_{2}$ 几乎必然成立。您需要使用总体协方差矩阵对特征三元组 $(X_{1}, X_{2}, X_{3})$ 进行主成分分析 (PCA)。\n\n仅使用协方差、矩阵秩、特征值和特征向量的定义，以及主成分解释方差的 PCA 诠释，完成以下任务：\n\n1) 构建 $(X_{1}, X_{2}, X_{3})^{\\top}$ 的协方差矩阵 $\\Sigma$。\n\n2) 从第一性原理出发，判断 $\\Sigma$ 是否为秩亏的，并通过展示其零空间中的一个非平凡向量来确定任何等于 $0$ 的特征值 $\\lambda_{i}$。\n\n3) 论证确切的内在维度是多少，即 PCA 可以在不损失任何方差的情况下将 $(X_{1}, X_{2}, X_{3})$ 降至的最小目标维度 $k$，并根据 $\\Sigma$ 的结构证明您的答案。\n\n4) 计算第一个主成分所解释的总方差的确切比例（即最大特征值除以 $\\Sigma$ 的迹）。将此比例的最终答案表示为最简分数。不要使用百分号。无需四舍五入。\n\n仅提供第 $4$ 项的最终数值答案。",
            "solution": "首先根据指定标准验证问题陈述。\n\n### 步骤 1：提取已知条件\n- 一个零均值随机向量 $\\mathbf{X} = (X_{1}, X_{2}, X_{3})^{\\top}$，意味着对于 $i \\in \\{1, 2, 3\\}$，有 $\\mathbb{E}[X_i] = 0$。\n- 子向量 $(X_{1}, X_{2})^{\\top}$ 的协方差矩阵为 $\\text{Cov}((X_1, X_2)^\\top) = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$。这意味着 $\\text{Var}(X_1) = 2$，$\\text{Var}(X_2) = 2$，以及 $\\text{Cov}(X_1, X_2) = 1$。\n- 定义了一个线性相关关系：$X_{3} = X_{1} + X_{2}$ 几乎必然成立。\n- 要进行的分析是针对 $\\mathbf{X}$ 的总体协方差矩阵的主成分分析 (PCA)。\n- 任务是：1) 构建 $\\mathbf{X}$ 的协方差矩阵 $\\Sigma$。2) 确定其秩亏性并找到一个零特征值。3) 确定无损降维的内在维度。4) 计算第一个主成分解释的方差比例。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学基础：** 该问题设置在多元统计和线性代数的标准框架内。均值、协方差、PCA、特征值和秩的定义都是标准的。给定的 $2 \\times 2$ 协方差矩阵是对称且正定的（特征值为 $2+1=3$ 和 $2-1=1$），因此它是一个有效的协方差矩阵。将新随机变量构造为其他变量的线性组合是标准程序。该问题在科学上和数学上都是合理的。\n- **良态问题：** 该问题是自洽的，并提供了构建协方差矩阵和执行所要求分析的所有必要信息。问题明确，可以导出一个唯一解。\n- **客观性：** 该问题以精确的数学语言陈述，不含主观或基于意见的内容。\n\n### 步骤 3：结论与行动\n该问题被认为是**有效的**，因为它具有科学基础、是良态问题且客观。将提供完整解答。\n\n### 解答\n\n解答过程将逐一解决问题陈述中描述的四个任务。\n\n**1) 构建协方差矩阵 $\\Sigma$**\n\n随机向量 $\\mathbf{X} = (X_{1}, X_{2}, X_{3})^{\\top}$ 的协方差矩阵 $\\Sigma$ 是一个 $3 \\times 3$ 矩阵，其中元素 $\\Sigma_{ij}$ 由 $\\text{Cov}(X_i, X_j)$ 给出。由于随机变量是零均值的，$\\text{Cov}(X_i, X_j) = \\mathbb{E}[X_i X_j]$。\n\n已知信息直接给出了 $\\Sigma$ 的左上角 $2 \\times 2$ 块：\n- $\\Sigma_{11} = \\text{Var}(X_1) = 2$\n- $\\Sigma_{22} = \\text{Var}(X_2) = 2$\n- $\\Sigma_{12} = \\Sigma_{21} = \\text{Cov}(X_1, X_2) = 1$\n\n其余元素涉及 $X_3$。使用定义 $X_3 = X_1 + X_2$ 和协方差算子的线性性质：\n- $\\Sigma_{13} = \\text{Cov}(X_1, X_3) = \\text{Cov}(X_1, X_1 + X_2) = \\text{Cov}(X_1, X_1) + \\text{Cov}(X_1, X_2) = \\text{Var}(X_1) + \\text{Cov}(X_1, X_2) = 2 + 1 = 3$。根据对称性，$\\Sigma_{31} = 3$。\n- $\\Sigma_{23} = \\text{Cov}(X_2, X_3) = \\text{Cov}(X_2, X_1 + X_2) = \\text{Cov}(X_2, X_1) + \\text{Cov}(X_2, X_2) = \\text{Cov}(X_1, X_2) + \\text{Var}(X_2) = 1 + 2 = 3$。根据对称性，$\\Sigma_{32} = 3$。\n- $\\Sigma_{33} = \\text{Var}(X_3) = \\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) + 2\\text{Cov}(X_1, X_2) = 2 + 2 + 2(1) = 6$。\n\n将这些元素组合起来得到完整的协方差矩阵：\n$$ \\Sigma = \\begin{pmatrix} 2  1  3 \\\\ 1  2  3 \\\\ 3  3  6 \\end{pmatrix} $$\n\n**2) 秩亏性和零特征值**\n\n如果一个矩阵的列（或行）不是线性无关的，那么它就是秩亏的。随机变量之间的线性相关关系 $X_3 = X_1 + X_2$ 意味着数据矩阵的列之间存在线性相关，这又导致了协方差矩阵的秩亏。这种关系可以写成 $1 \\cdot X_1 + 1 \\cdot X_2 - 1 \\cdot X_3 = 0$。系数向量 $\\mathbf{v} = (1, 1, -1)^{\\top}$ 定义了结果为零的特定线性组合。这个向量 $\\mathbf{v}$ 将位于 $\\Sigma$ 的零空间中。\n\n为了从第一性原理验证这一点，我们证明 $\\mathbf{v}$ 是 $\\Sigma$ 的一个特征向量，其对应的特征值为 $0$。我们计算乘积 $\\Sigma\\mathbf{v}$：\n$$ \\Sigma\\mathbf{v} = \\begin{pmatrix} 2  1  3 \\\\ 1  2  3 \\\\ 3  3  6 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2(1) + 1(1) + 3(-1) \\\\ 1(1) + 2(1) + 3(-1) \\\\ 3(1) + 3(1) + 6(-1) \\end{pmatrix} = \\begin{pmatrix} 2 + 1 - 3 \\\\ 1 + 2 - 3 \\\\ 3 + 3 - 6 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0 \\cdot \\mathbf{v} $$\n由于对于非平凡向量 $\\mathbf{v} = (1, 1, -1)^{\\top}$，有 $\\Sigma\\mathbf{v} = \\mathbf{0}$，因此可以确定 $\\lambda = 0$ 是 $\\Sigma$ 的一个特征值。存在零特征值意味着 $\\Sigma$ 的行列式为 $0$，因此该矩阵不是满秩的。它是秩亏的。\n\n**3) 无损降维的内在维度**\n\nPCA 的目标是为数据空间找到一个正交基（即主成分）。协方差矩阵 $\\Sigma$ 的特征值表示数据在这些主成分轴上的方差。数据中的总方差是所有特征值之和，也等于 $\\Sigma$ 的迹。\n\n如果前 $k$ 个主成分捕获了总方差的 $100\\%$，那么到目标维度 $k$ 的降维是无损的。这种情况发生的充要条件是前 $k$ 个最大特征值之和等于总方差。设特征值按大小排序为 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p$。无损降维到维度 $k$ 需要 $\\sum_{i=1}^k \\lambda_i = \\sum_{i=1}^p \\lambda_i$，这意味着剩余的特征值 $\\lambda_{k+1}, \\dots, \\lambda_p$ 必须为零。\n\n因此，内在维度是非零特征值的数量，这等价于协方差矩阵 $\\Sigma$ 的秩。\n\n从第 2) 部分我们知道，$\\Sigma$ 至少有一个零特征值，所以它的秩小于 $3$。为了确定确切的秩，我们可以检查一个子矩阵。左上角的 $2 \\times 2$ 子矩阵是 $\\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$。它的行列式是 $2(2) - 1(1) = 3 \\neq 0$。由于存在一个行列式非零的 $2 \\times 2$ 子矩阵，$\\Sigma$ 的秩至少为 $2$。\n\n综合来看，我们有 $\\text{rank}(\\Sigma)  3$ 和 $\\text{rank}(\\Sigma) \\ge 2$。因此，$\\Sigma$ 的秩恰好为 $2$。这意味着恰好有两个非零特征值。为了捕获所有方差，我们必须保留与这两个非零特征值相对应的主成分。因此，无损降维的最小目标维度 $k$ 是 $2$。这是数据的内在维度，数据位于三维特征空间中由 $x_1 + x_2 - x_3 = 0$ 定义的二维平面上。\n\n**4) 第一个主成分解释的方差比例**\n\n该比例由最大特征值 $\\lambda_1$ 与总方差（即 $\\text{Tr}(\\Sigma)$）之比给出。\n总方差为：\n$$ \\text{Tr}(\\Sigma) = \\Sigma_{11} + \\Sigma_{22} + \\Sigma_{33} = 2 + 2 + 6 = 10 $$\n为了找到特征值，我们求解特征方程 $\\det(\\Sigma - \\lambda I) = 0$：\n$$ \\det \\begin{pmatrix} 2-\\lambda  1  3 \\\\ 1  2-\\lambda  3 \\\\ 3  3  6-\\lambda \\end{pmatrix} = 0 $$\n展开行列式：\n$$ (2-\\lambda)((2-\\lambda)(6-\\lambda) - 9) - 1(1(6-\\lambda) - 9) + 3(3 - 3(2-\\lambda)) = 0 $$\n$$ (2-\\lambda)(\\lambda^2 - 8\\lambda + 12 - 9) - (6-\\lambda-9) + 3(3 - 6 + 3\\lambda) = 0 $$\n$$ (2-\\lambda)(\\lambda^2 - 8\\lambda + 3) - (-\\lambda - 3) + 3(3\\lambda - 3) = 0 $$\n$$ 2\\lambda^2 - 16\\lambda + 6 - \\lambda^3 + 8\\lambda^2 - 3\\lambda + \\lambda + 3 + 9\\lambda - 9 = 0 $$\n合并同类项：\n$$ -\\lambda^3 + (2+8)\\lambda^2 + (-16-3+1+9)\\lambda + (6+3-9) = 0 $$\n$$ -\\lambda^3 + 10\\lambda^2 - 9\\lambda = 0 $$\n$$ \\lambda^3 - 10\\lambda^2 + 9\\lambda = 0 $$\n对多项式进行因式分解：\n$$ \\lambda(\\lambda^2 - 10\\lambda + 9) = 0 $$\n$$ \\lambda(\\lambda-1)(\\lambda-9) = 0 $$\n特征值为 $\\lambda = 0$, $\\lambda = 1$ 和 $\\lambda = 9$。\n排序后的特征值为 $\\lambda_1 = 9$, $\\lambda_2 = 1$ 和 $\\lambda_3 = 0$。\n最大特征值为 $\\lambda_1 = 9$。\n第一个主成分解释的总方差比例为：\n$$ \\frac{\\lambda_1}{\\text{Tr}(\\Sigma)} = \\frac{9}{10} $$\n此结果是按要求的最简分数。",
            "answer": "$$\\boxed{\\frac{9}{10}}$$"
        },
        {
            "introduction": "在放射组学等实际应用中，特征的尺度和单位通常千差万别。本练习将通过动手实践，展示为什么特征缩放是PCA一个至关重要的数据预处理步骤。您将通过计算来比较PCA在原始数据与标准化数据上的结果，并观察主成分会如何发生巨大变化，进而改变对数据方差结构的解读。这项实践对于确保PCA结果的有效性至关重要，避免分析被量纲任意巨大的特征所主导。",
            "id": "2430028",
            "problem": "给定一个数据矩阵族，其各列的数值尺度差异巨大。对于每种情况，考虑一个实值数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$，其中包含 $d=3$ 个特征和 $n$ 个样本。对于下面的每个测试用例，通过显式公式确定性地定义 $X$。任务是比较对均值中心化数据和标准化数据执行主成分分析（PCA）的结果，并量化主导主成分方向及其解释方差分数因标准化而发生的变化。此处，主成分分析（PCA）定义为对变换后数据的样本协方差矩阵进行特征分解。此处，特征的标准化定义为对每一列进行中心化（减去其样本均值）并除以其样本标准差；如果某一列的标准差为零，则在中心化后，将该标准化列恒定置为零。\n\n使用的定义：\n- 给定 $X \\in \\mathbb{R}^{n \\times d}$，令 $\\bar{\\mathbf{x}} \\in \\mathbb{R}^{d}$ 为列样本均值，并令 $X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$ 表示中心化数据，其中 $\\mathbf{1} \\in \\mathbb{R}^{n}$ 是全一向量。样本协方差矩阵为 $S = \\frac{1}{n-1} X_c^\\top X_c$。\n- PCA的特征值和特征向量是 $S$ 的特征对 $\\{(\\lambda_k,\\mathbf{v}_k)\\}_{k=1}^d$，其中 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$，特征向量 $\\{\\mathbf{v}_k\\}$ 是标准正交的。第一主成分的解释方差比为 $r_1 = \\frac{\\lambda_1}{\\sum_{j=1}^d \\lambda_j}$。\n- 对于标准化数据，计算 $X_c$ 的列样本标准差 $\\sigma_j$。对于所有 $\\sigma_j \\neq 0$ 的 $j$，通过 $Z_{:,j} = X_{c,:,j}/\\sigma_j$ 构成 $Z$，而对于任何 $\\sigma_j = 0$ 的 $j$，则 $Z_{:,j} = \\mathbf{0}$。然后定义 $S^{(z)} = \\frac{1}{n-1} Z^\\top Z$ 及其特征对 $\\{(\\mu_k,\\mathbf{u}_k)\\}_{k=1}^d$，排序为 $\\mu_1 \\ge \\mu_2 \\ge \\mu_3 \\ge 0$，其解释方差比为 $r_1^{(z)} = \\frac{\\mu_1}{\\sum_{j=1}^d \\mu_j}$。\n- 两个单位主方向 $\\mathbf{v}_1$ 和 $\\mathbf{u}_1$ 之间的一致性通过 $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1| \\in [0,1]$ 来衡量。特征向量的符号不确定性通过绝对值来处理。\n- 为量化原始坐标轴对主成分的主导程度，定义 $i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$ 和 $i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$，特征索引使用从零开始的编号。\n\n对于每个测试用例，您必须计算以下有序的量列表：\n- 从 $S$ 计算的 $r_1$，\n- 从 $S^{(z)}$ 计算的 $r_1^{(z)}$，\n- $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$，\n- $i_{\\mathrm{before}}$，\n- $i_{\\mathrm{after}}$。\n\n测试套件（每个用例定义 $n$，然后是 $i \\in \\{0,\\dots,n-1\\}$ 的 $t_i$，以及作为 $t_i$ 函数的三个特征）：\n- 用例 1：$n=200$。对于每个 $i \\in \\{0,\\dots,199\\}$，令 $t_i = \\frac{i}{199}$，并定义\n  - $x_{i1} = 1000 \\cos(2\\pi t_i)$，\n  - $x_{i2} = \\sin(2\\pi t_i) + 0.1 \\cos(4\\pi t_i)$，\n  - $x_{i3} = 0.001\\, t_i$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n- 用例 2：$n=100$。对于每个 $i \\in \\{0,\\dots,99\\}$，令 $t_i = \\frac{i}{99}$，并定义\n  - $x_{i1} = 10^6\\, t_i$，\n  - $x_{i2} = 0$，\n  - $x_{i3} = 10\\,(t_i - 0.5)$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n- 用例 3：$n=150$。对于每个 $i \\in \\{0,\\dots,149\\}$，令 $t_i = \\frac{i}{149}$，并定义\n  - $x_{i1} = 1000\\,(2 t_i - 1)$，\n  - $x_{i2} = (2 t_i - 1) + 0.1 \\sin(3\\pi t_i)$，\n  - $x_{i3} = 0.01 \\cos(5\\pi t_i)$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于每个用例，输出有序列表 $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$。将三个用例的结果聚合成一个包含三个列表的列表，顺序为用例 1、2、3。例如，整体打印结构必须是 $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$ 的形式。\n\n所有答案均为指定的无量纲实数或整数。由于所有要求的量都是纯数，因此不需要物理单位或角度单位。",
            "solution": "用户提供了一个有效且需要解决方案的问题。问题陈述在科学上基于线性代数和统计学领域，特别是主成分分析（PCA）。该问题定义明确，提供了用于构建数据的确定性指令，定义了所有必要的数学对象和程序，并要求一组特定的、可计算的量。其语言客观且无歧义。因此，可以构建一个合理的、分步的解决方案。\n\n该问题要求对三个不同案例中，在均值中心化数据与标准化数据上执行的PCA进行比较。问题的核心在于观察缩放如何影响PCA的结果。PCA识别数据集中的最大方差方向。当特征（数据矩阵的列）具有迥异的尺度时，无论底层数据结构如何，方差最大的特征将主导第一个主成分。标准化通过将每个特征重新缩放至均值为0、标准差为1，将所有特征置于同等地位，从而防止了这种情况。\n\n总体流程如下：\n1. 对于每个测试用例，构建 $n \\times d$ 数据矩阵 $X$，其中 $d=3$。\n2. 对均值中心化数据 $X_c$ 执行PCA。\n    a. 计算列样本均值向量 $\\bar{\\mathbf{x}}$。\n    b. 对数据进行中心化：$X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$。\n    c. 计算样本协方差矩阵 $S = \\frac{1}{n-1} X_c^\\top X_c$。\n    d. 通过求解特征问题 $S\\mathbf{v}_k = \\lambda_k\\mathbf{v}_k$ 来找到 $S$ 的特征值 $\\lambda_k$ 和特征向量 $\\mathbf{v}_k$。特征值已排序，$\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3$，且特征向量 $\\{\\mathbf{v}_k\\}$ 是标准正交的。第一主成分方向是 $\\mathbf{v}_1$。\n    e. 计算第一主成分的解释方差比：$r_1 = \\lambda_1 / (\\sum_{j=1}^d \\lambda_j)$。\n    f. 识别主导 $\\mathbf{v}_1$ 的原始特征：$i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$。\n\n3. 对标准化数据 $Z$ 执行PCA。\n    a. 使用 $n-1$ 作为除数，计算 $X$ 的列样本标准差 $\\sigma_j$。\n    b. 构建标准化数据矩阵 $Z$。每个列 $Z_{:,j}$ 是通过将相应的中心化列 $X_{c,:,j}$ 乘以 $1/\\sigma_j$ 得到的。如果 $\\sigma_j=0$，则列 $Z_{:,j}$ 设置为零向量。\n    c. 计算 $Z$ 的样本协方差矩阵：$S^{(z)} = \\frac{1}{n-1} Z^\\top Z$。此矩阵等价于 $X$ 的样本相关系数矩阵。对于任何非恒定特征，其对角线元素为 1。\n    d. 找到 $S^{(z)}$ 的特征值 $\\mu_k$ 和特征向量 $\\mathbf{u}_k$，并排序使得 $\\mu_1 \\ge \\mu_2 \\ge \\mu_3$。标准化数据的第一主成分方向是 $\\mathbf{u}_1$。\n    e. 计算相应的解释方差比：$r_1^{(z)} = \\mu_1 / (\\sum_{j=1}^d \\mu_j)$。分母中的和，即 $\\text{Tr}(S^{(z)})$，等于非恒定特征的数量。\n    f. 识别主导 $\\mathbf{u}_1$ 的原始特征：$i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$。\n\n4. 通过计算对齐度量 $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$ 来比较两种分析的结果，该度量衡量了两个主方向之间夹角的余弦值。\n\n5. 对于每个用例，最终输出是有序列表 $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$。\n\n具体案例分析：\n- **用例 1**：数据包含三个尺度分别为 $O(10^3)$、$O(1)$ 和 $O(10^{-3})$ 的特征。第一个特征 $x_1 = 1000 \\cos(2\\pi t_i)$ 的方差将远远大于其他特征。因此，未标准化数据的第一个主成分 $\\mathbf{v}_1$ 预计将几乎与第一个特征轴完全对齐。这将得到 $r_1 \\approx 1$ 和 $i_{\\mathrm{before}} = 0$。标准化后，所有特征的方差均为单位方差，结构关系（$(x_1, x_2)$ 平面上的椭圆轨迹）将变得明显。方差将更均匀地分布，导致 $r_1^{(z)}$ 变小，而 $\\mathbf{u}_1$ 将是特征1和2的组合。\n- **用例 2**：第一个特征 $x_1 = 10^6 t_i$ 具有巨大的尺度。第二个特征 $x_2=0$ 是常数，方差为零。第三个特征 $x_3 = 10(t_i-0.5)$ 的尺度远小于第一个。对于未标准化的数据，PCA 将由特征1主导，得到 $r_1 \\approx 1$ 和 $i_{\\mathrm{before}}=0$。标准化后，常数特征 $x_2$ 仍然是零向量。特征1和3都是 $t_i$ 的线性函数，在中心化和缩放后将变得完全相关。它们的标准化版本将是相同的，$Z_{:,1} = Z_{:,3}$。数据将塌陷到 $(Z_1, Z_3)$ 平面上的单个方向。这将导致 $r_1^{(z)}=1$（因为在非恒定特征中，有效秩为1），特征向量 $\\mathbf{u}_1$ 的形式为 $[1/\\sqrt{2}, 0, 1/\\sqrt{2}]^\\top$。\n- **用例 3**：第一个特征 $x_1 = 1000(2t_i-1)$ 尺度很大。第二个特征 $x_2 = (2t_i-1) + 0.1 \\sin(3\\pi t_i)$ 与第一个特征高度相关，但尺度小得多。第三个特征的尺度可以忽略不计。与其他情况一样，未标准化的PCA将由第一个特征的尺度决定，因此 $r_1 \\approx 1$ 且 $i_{\\mathrm{before}}=0$。标准化后，特征1和2之间的强线性关系将成为最突出的特征。第一个主成分 $\\mathbf{u}_1$ 将捕捉到这种共享方差，代表了相关数据云主轴的方向，大致在特征1和2的标准化轴之间成45度角。\n\n实现将使用 `numpy` 进行所有数值计算，特别是使用 `numpy.linalg.eigh` 对对称协方差矩阵进行特征分解。将注意处理特征值降序排序以及标准差为零的情况。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA comparison problem for three given test cases.\n    \"\"\"\n\n    def generate_case_1_data():\n        n = 200\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * np.cos(2 * np.pi * t)\n        x2 = np.sin(2 * np.pi * t) + 0.1 * np.cos(4 * np.pi * t)\n        x3 = 0.001 * t\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_2_data():\n        n = 100\n        t = np.linspace(0, 1, n)\n        x1 = 1e6 * t\n        x2 = np.zeros(n)\n        x3 = 10 * (t - 0.5)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_3_data():\n        n = 150\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * (2 * t - 1)\n        x2 = (2 * t - 1) + 0.1 * np.sin(3 * np.pi * t)\n        x3 = 0.01 * np.cos(5 * np.pi * t)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def perform_pca_analysis(X):\n        \"\"\"\n        Performs PCA on both mean-centered and standardized data, returning the required metrics.\n        \"\"\"\n        n, d = X.shape\n        \n        # --- PCA on Mean-Centered Data ---\n        X_c = X - X.mean(axis=0)\n        S = (X_c.T @ X_c) / (n - 1)\n        \n        # Eigendecomposition. eigh returns sorted eigenvalues (ascending).\n        # We reverse them to get principal components in descending order of variance.\n        evals, evecs = np.linalg.eigh(S)\n        evals = evals[::-1]\n        evecs = evecs[:, ::-1]\n        \n        lambda_1 = evals[0]\n        v1 = evecs[:, 0]\n        \n        # Explained variance ratio\n        total_variance = np.sum(evals)\n        r1 = lambda_1 / total_variance if total_variance  0 else 0\n        \n        # Dominant feature index\n        i_before = np.argmax(np.abs(v1))\n\n        # --- PCA on Standardized Data ---\n        stds = np.std(X_c, axis=0, ddof=1)\n        \n        # Handle features with zero standard deviation\n        Z = np.zeros_like(X_c)\n        non_zero_std_mask = stds  0\n        if np.any(non_zero_std_mask):\n            Z[:, non_zero_std_mask] = X_c[:, non_zero_std_mask] / stds[non_zero_std_mask]\n\n        S_z = (Z.T @ Z) / (n - 1)\n        \n        # Eigendecomposition of the correlation matrix\n        mu, u = np.linalg.eigh(S_z)\n        mu = mu[::-1]\n        u = u[:, ::-1]\n        \n        mu_1 = mu[0]\n        u1 = u[:, 0]\n        \n        # Explained variance ratio for standardized data\n        total_variance_z = np.sum(mu)\n        r1_z = mu_1 / total_variance_z if total_variance_z  0 else 0\n        \n        # Dominant feature index after standardization\n        i_after = np.argmax(np.abs(u1))\n        \n        # Alignment between the first principal components\n        alignment = np.abs(np.dot(v1, u1))\n        \n        return [r1, r1_z, alignment, int(i_before), int(i_after)]\n\n    test_cases = [\n        generate_case_1_data,\n        generate_case_2_data,\n        generate_case_3_data\n    ]\n    \n    all_results = []\n    for case_generator in test_cases:\n        X = case_generator()\n        result = perform_pca_analysis(X)\n        all_results.append(result)\n\n    # Format the output string without spaces as [[...],[...],[...]]\n    inner_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "尽管PCA功能强大，但它并非没有弱点。本练习聚焦于其一个关键的局限性：对异常值的敏感性。通过在一个结构良好的数据集中引入一个孤立的异常点，您将量化它对第一主成分方向产生的巨大影响。这项动手实践强调了在应用PCA之前进行数据清洗和异常值检测的重要性，因为极端值可能会扭曲分析结果，掩盖大部分数据中潜在的真实模式。",
            "id": "2430058",
            "problem": "您需要量化一个孤立的远距离离群点如何影响一个2维数据集中第一主成分的方向。考虑一个确定性的基准点云，它由$M$个位于以原点为中心的椭圆上的点组成，由以下参数集定义\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\tfrac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\tfrac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0,1,2,\\dots,M-1,\n$$\n其中$r_x \\gt 0$且$r_y \\gt 0$。通过在以下坐标处添加一个额外的点（离群点）来构成一个增广数据集\n$$\n\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}.\n$$\n设$\\mathbf{S}_0$为基准点云的样本协方差矩阵，$\\mathbf{S}_1$为包含$M$个椭圆点和该离群点的增广数据集的样本协方差矩阵。第一主成分定义为相应样本协方差矩阵与其最大特征值相关联的单位特征向量。记$\\mathbf{S}_0$最大特征值对应的单位特征向量为$\\mathbf{u}_0 \\in \\mathbb{R}^2$，$\\mathbf{S}_1$的类似特征向量为$\\mathbf{u}_1 \\in \\mathbb{R}^2$。由于特征向量的定义可相差一个符号，将两个主方向之间的锐角差$\\Delta$定义为\n$$\n\\Delta = \\arccos\\!\\left(\\left|\\mathbf{u}_0^\\top \\mathbf{u}_1\\right|\\right).\n$$\n您必须以弧度为单位计算$\\Delta$。所有最终数值答案必须以弧度表示，并四舍五入到$6$位小数。\n\n测试套件。对于下方的每个参数元组$(r_x, r_y, M, o_x, o_y)$，按照规定构建基准点云和增广数据集，计算$\\Delta$，并报告结果：\n- 案例 1（一般情况）：$(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 10.0, 10.0)$。\n- 案例 2（边界情况，无影响）：$(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 0.0, 0.0)$。\n- 案例 3（极端情况，离群点主导正交方向）：$(r_x, r_y, M, o_x, o_y) = (2.0, 1.0, 40, 0.0, 1000.0)$。\n- 案例 4（近各向同性基准，中等离群点）：$(r_x, r_y, M, o_x, o_y) = (1.05, 1.0, 50, 5.0, 0.2)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果顺序与上述案例相同，每个值都四舍五入到$6$位小数。例如，一个有效的输出格式是 `[x_1,x_2,x_3,x_4]`。",
            "solution": "该问题已经过验证，被认定为有效。这是一个计算物理和线性代数领域中定义明确的问题，基于已建立的科学原理。所有术语都得到了足够严格的定义，所提供的数据是一致且完整的。\n\n任务是计算当引入一个离群点时，2维数据集中第一主成分的角度偏差$\\Delta$。基准数据集是由$M$个均匀分布在椭圆上的点构成的点云，增广数据集则额外包含一个离群点。\n\n数据集的第一主成分是方差最大的方向，它对应于样本协方差矩阵最大特征值所关联的单位特征向量。设$N$个数据点的集合为$\\{\\mathbf{p}_i\\}_{i=1}^N$，其中每个$\\mathbf{p}_i \\in \\mathbb{R}^2$。样本均值为$\\bar{\\mathbf{p}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{p}_i$。样本协方差矩阵$\\mathbf{S}$由下式给出：\n$$\n\\mathbf{S} = \\frac{1}{N-1}\\sum_{i=1}^N (\\mathbf{p}_i - \\bar{\\mathbf{p}})(\\mathbf{p}_i - \\bar{\\mathbf{p}})^\\top\n$$\n请注意，分母的具体选择（是$N$还是$N-1$）无关紧要，因为它只缩放协方差矩阵，而不会改变其特征向量。数值实现将使用分母为$N-1$的常规无偏估计量。\n\n首先，我们分析由以下公式定义的$M$个点的基准点云$\\{\\mathbf{x}_k\\}_{k=0}^{M-1}$：\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\frac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\frac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0, 1, \\dots, M-1\n$$\n由于余弦和正弦函数在整个周期上的对称性，对于$M > 1$，基准点云的样本均值$\\bar{\\mathbf{x}}_0$是零向量：\n$$\n\\bar{\\mathbf{x}}_0 = \\frac{1}{M}\\sum_{k=0}^{M-1} \\mathbf{x}_k = \\begin{bmatrix} \\frac{r_x}{M}\\sum_{k=0}^{M-1}\\cos(\\frac{2\\pi k}{M}) \\\\ \\frac{r_y}{M}\\sum_{k=0}^{M-1}\\sin(\\frac{2\\pi k}{M}) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\n那么，基准点云的样本协方差矩阵$\\mathbf{S}_0$为：\n$$\n\\mathbf{S}_0 = \\frac{1}{M-1}\\sum_{k=0}^{M-1} \\mathbf{x}_k \\mathbf{x}_k^\\top = \\frac{1}{M-1} \\sum_{k=0}^{M-1} \\begin{bmatrix} r_x^2 \\cos^2(\\theta_k)  r_x r_y \\cos(\\theta_k)\\sin(\\theta_k) \\\\ r_x r_y \\cos(\\theta_k)\\sin(\\theta_k)  r_y^2 \\sin^2(\\theta_k) \\end{bmatrix}\n$$\n其中$\\theta_k = \\frac{2\\pi k}{M}$。对于$M > 2$，非对角项的和为零。对角项的和为$\\sum \\cos^2(\\theta_k) = M/2$和$\\sum \\sin^2(\\theta_k) = M/2$。因此，$\\mathbf{S}_0$是一个对角矩阵：\n$$\n\\mathbf{S}_0 = \\frac{M}{2(M-1)} \\begin{bmatrix} r_x^2  0 \\\\ 0  r_y^2 \\end{bmatrix}\n$$\n对角矩阵的特征向量是标准基向量。最大特征值对应于$r_x^2$和$r_y^2$中较大的一个。如果$r_x > r_y$，第一主成分是$\\mathbf{u}_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。如果$r_y > r_x$，则是$\\mathbf{u}_0 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。这与椭圆的半长轴方向一致。\n\n接下来，我们分析增广数据集，它包含基准点云的$M$个点以及离群点$\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}$。总点数为$N = M+1$。增广数据集的均值$\\bar{\\mathbf{x}}_1$为：\n$$\n\\bar{\\mathbf{x}}_1 = \\frac{1}{M+1}\\left(\\sum_{k=0}^{M-1}\\mathbf{x}_k + \\mathbf{z}\\right) = \\frac{1}{M+1}\\mathbf{z}\n$$\n协方差矩阵$\\mathbf{S}_1$是基于这$M+1$个点计算的。\n$$\n\\mathbf{S}_1 = \\frac{1}{M} \\left( \\sum_{k=0}^{M-1}(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)^\\top + (\\mathbf{z} - \\bar{\\mathbf{x}}_1)(\\mathbf{z} - \\bar{\\mathbf{x}}_1)^\\top \\right)\n$$\n通常情况下，$\\mathbf{S}_1$不是一个对角矩阵。离群点，特别是当其坐标$(o_x, o_y)$很大时，会显著改变均值并对协方差和贡献一个大项，从而旋转数据分布的主轴。第一主成分$\\mathbf{u}_1$是与$\\mathbf{S}_1$的最大特征值相对应的单位特征向量。这可以通过对数值计算出的$\\mathbf{S}_1$进行特征分解来找到。\n\n角差$\\Delta$计算为两个主成分向量$\\mathbf{u}_0$和$\\mathbf{u}_1$之间的锐角：\n$$\n\\Delta = \\arccos(|\\mathbf{u}_0^\\top \\mathbf{u}_1|)\n$$\n点积的绝对值确保了角度是锐角，这是考虑到特征向量与其负向量是等价的。\n\n解决每个测试案例的算法流程如下：\n1. 构建由$r_x$和$r_y$定义的椭圆上的$M$个点的基准数据集。\n2. 计算基准数据集的样本协方差矩阵$\\mathbf{S}_0$。\n3. 求$\\mathbf{S}_0$的特征向量和特征值。与最大特征值对应的特征向量$\\mathbf{u}_0$是第一主成分。\n4. 通过将离群点$\\mathbf{z}=(o_x, o_y)$添加到基准数据集中，构建增广数据集。\n5. 计算增广数据集的样本协方差矩阵$\\mathbf{S}_1$。\n6. 求$\\mathbf{S}_1$的特征向量和特征值。与最大特征值对应的特征向量$\\mathbf{u}_1$是新的第一主成分。\n7. 以弧度为单位计算角差$\\Delta = \\arccos(|\\mathbf{u}_0 \\cdot \\mathbf{u}_1|)$。点积的参数应被裁剪到$[-1, 1]$范围内，以避免数值误差。\n8. 将结果四舍五入到$6$位小数。\n对每组给定的参数都执行此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the angular difference in the first principal component of a 2D dataset\n    due to the addition of an outlier.\n    \"\"\"\n    test_cases = [\n        # (r_x, r_y, M, o_x, o_y)\n        (3.0, 1.0, 60, 10.0, 10.0),\n        (3.0, 1.0, 60, 0.0, 0.0),\n        (2.0, 1.0, 40, 0.0, 1000.0),\n        (1.05, 1.0, 50, 5.0, 0.2),\n    ]\n\n    results = []\n    \n    for r_x, r_y, M, o_x, o_y in test_cases:\n        # Step 1: Construct the base dataset\n        theta = (2 * np.pi / M) * np.arange(M)\n        x_coords = r_x * np.cos(theta)\n        y_coords = r_y * np.sin(theta)\n        base_cloud = np.stack((x_coords, y_coords), axis=1)\n\n        # Step 2  3: PCA on the base cloud\n        # Covariance matrix for a centered ellipse is diagonal, so we can determine u0 analytically.\n        # This is more robust and faster than numerical computation for this specific geometry.\n        # Although numerical computation would yield the same result.\n        # Let's use numerical computation for generality and to match the full algorithm described.\n        # The choice of divisor (N or N-1) does not affect the eigenvectors.\n        # np.cov uses N-1 by default.\n        cov_0 = np.cov(base_cloud, rowvar=False)\n        eigvals_0, eigvecs_0 = np.linalg.eigh(cov_0)\n        # eigh sorts eigenvalues in ascending order, so the last eigenvector is the principal one.\n        u_0 = eigvecs_0[:, -1]\n\n        # Step 4: Construct the augmented dataset\n        outlier = np.array([[o_x, o_y]])\n        augmented_cloud = np.vstack((base_cloud, outlier))\n\n        # Step 5  6: PCA on the augmented dataset\n        cov_1 = np.cov(augmented_cloud, rowvar=False)\n        eigvals_1, eigvecs_1 = np.linalg.eigh(cov_1)\n        u_1 = eigvecs_1[:, -1]\n\n        # Step 7: Compute the angular difference\n        # Dot product between the two unit eigenvectors\n        dot_product = np.dot(u_0, u_1)\n        \n        # Take the absolute value to find the acute angle\n        abs_dot_product = np.abs(dot_product)\n        \n        # Clip to handle potential floating point inaccuracies  1.0\n        clipped_dot = np.clip(abs_dot_product, -1.0, 1.0)\n        \n        delta = np.arccos(clipped_dot)\n\n        # Step 8: Append rounded result\n        results.append(round(delta, 6))\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"{res:.6f}\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}