{
    "hands_on_practices": [
        {
            "introduction": "决策树通过提出一系列问题来分割数据，从而构建自身。这个过程的核心是在每一步找到要问的“最佳”问题。本练习将让您亲身体验，通过最大化信息增益（一个源于信息论的基本概念），为放射组学特征计算最佳分割点 ()。",
            "id": "4535354",
            "problem": "一个用于将肺结节分类为良性或恶性的放射组学流程，使用一个基于灰度共生矩阵（GLCM）派生出的纹理特征 $x_{j}$ 的单变量决策树分裂。给定已排序的特征值 $x_{j} \\in \\{0.12, 0.14, 0.18, 0.30, 0.31\\}$ 及其对应的二元标签 $y \\in \\{0,0,1,1,1\\}$，其中 $0$ 表示良性，$1$ 表示恶性。考虑一个形式为 $x_{j} \\le \\tau$ 对比 $x_{j} > \\tau$ 的决策树分裂，其中阈值 $\\tau$ 选自连续不同特征值之间的中点。\n\n从以下定义出发，确定使信息增益最大化的阈值 $\\tau$，并通过评估所有允许的中点的信息增益来证明你的选择。\n\n使用以下基本定义：\n- 一个集合 $S$ 的熵，其类别概率为 $\\{p_{c}\\}$，定义为 $H(S) = -\\sum_{c} p_{c} \\ln p_{c}$，使用自然对数。\n- 对于将集合 $S$ 分裂为左子集 $S_{L}$ 和右子集 $S_{R}$，加权分裂后熵为 $\\frac{|S_{L}|}{|S|} H(S_{L}) + \\frac{|S_{R}|}{|S|} H(S_{R})$。\n- 信息增益为 $IG = H(S) - \\left(\\frac{|S_{L}|}{|S|} H(S_{L}) + \\frac{|S_{R}|}{|S|} H(S_{R})\\right)$。\n\n提供最终的阈值 $\\tau$ 作为一个实数。无需四舍五入。",
            "solution": "这个问题是有效的，因为它在科学上基于信息论和机器学习的原理，问题陈述清晰并包含了所有必要信息，且表述客观。我们接下来寻找使信息增益最大化的最优决策树分裂阈值 $\\tau$。\n\n数据集是一组放射组学特征值和相应的类别标签，表示为 $S$。特征值 $x_j$ 已排序，配对 $(x_j, y)$ 为：\n$S = \\{(0.12, 0), (0.14, 0), (0.18, 1), (0.30, 1), (0.31, 1)\\}$。\n样本总数是 $|S| = 5$。类别分为良性（标签 $0$）和恶性（标签 $1$）。良性样本的数量是 $n_0 = 2$，恶性样本的数量是 $n_1 = 3$。\n\n第一步是计算整个数据集 $S$ 的熵，记为 $H(S)$。类别的概率是 $p_0 = \\frac{n_0}{|S|} = \\frac{2}{5}$ 和 $p_1 = \\frac{n_1}{|S|} = \\frac{3}{5}$。\n使用给定的熵公式 $H(S) = -\\sum_{c} p_{c} \\ln p_{c}$：\n$$H(S) = -\\left( \\frac{2}{5} \\ln\\left(\\frac{2}{5}\\right) + \\frac{3}{5} \\ln\\left(\\frac{3}{5}\\right) \\right)$$\n这个值代表了在进行任何分裂前数据集的总不确定性。\n\n接下来，我们确定候选阈值 $\\tau$。这些阈值被定义为连续不同特征值之间的中点。已排序的不同特征值为 $0.12$、$0.14$、$0.18$、$0.30$ 和 $0.31$。\n候选阈值是：\n1. $\\tau_1 = \\frac{0.12 + 0.14}{2} = 0.13$\n2. $\\tau_2 = \\frac{0.14 + 0.18}{2} = 0.16$\n3. $\\tau_3 = \\frac{0.18 + 0.30}{2} = 0.24$\n4. $\\tau_4 = \\frac{0.30 + 0.31}{2} = 0.305$\n\n我们现在为每个候选阈值评估信息增益。对于在阈值 $\\tau$ 处的分裂，其信息增益 $IG(\\tau)$ 由下式给出：\n$$IG(\\tau) = H(S) - H(S|\\tau)$$\n其中 $H(S|\\tau)$ 是加权分裂后熵：\n$$H(S|\\tau) = \\frac{|S_L|}{|S|} H(S_L) + \\frac{|S_R|}{|S|} H(S_R)$$\n这里，$S_L = \\{(x_j, y) \\in S | x_j \\le \\tau \\}$ 且 $S_R = \\{(x_j, y) \\in S | x_j > \\tau \\}$。最大化 $IG(\\tau)$ 等价于最小化加权分裂后熵 $H(S|\\tau)$。\n\n我们将为四个候选阈值中的每一个计算 $H(S|\\tau)$。\n\n情况 1: $\\tau = \\tau_1 = 0.13$\n分裂为 $x_j \\le 0.13$ 对比 $x_j > 0.13$。\n$S_L = \\{(0.12, 0)\\}$。因此， $|S_L| = 1$。类别分布为 $\\{n_0=1, n_1=0\\}$。这是一个纯子集。\n熵为 $H(S_L) = -(1 \\ln(1) + 0) = 0$。注意我们使用约定 $0 \\ln(0)=0$。\n$S_R = \\{(0.14, 0), (0.18, 1), (0.30, 1), (0.31, 1)\\}$。因此， $|S_R| = 4$。类别分布为 $\\{n_0=1, n_1=3\\}$。\n熵为 $H(S_R) = -\\left(\\frac{1}{4} \\ln\\left(\\frac{1}{4}\\right) + \\frac{3}{4} \\ln\\left(\\frac{3}{4}\\right)\\right)$。\n加权分裂后熵是：\n$$H(S|\\tau_1) = \\frac{1}{5} H(S_L) + \\frac{4}{5} H(S_R) = \\frac{1}{5}(0) + \\frac{4}{5}\\left(-\\frac{1}{4} \\ln\\left(\\frac{1}{4}\\right) - \\frac{3}{4} \\ln\\left(\\frac{3}{4}\\right)\\right) = -\\frac{1}{5} \\ln\\left(\\frac{1}{4}\\right) - \\frac{3}{5} \\ln\\left(\\frac{3}{4}\\right)$$\n$$H(S|\\tau_1) = \\frac{1}{5} \\ln(4) - \\frac{3}{5}(\\ln(3) - \\ln(4)) = \\frac{4}{5} \\ln(4) - \\frac{3}{5} \\ln(3) = \\frac{8}{5} \\ln(2) - \\frac{3}{5} \\ln(3)$$\n\n情况 2: $\\tau = \\tau_2 = 0.16$\n分裂为 $x_j \\le 0.16$ 对比 $x_j > 0.16$。\n$S_L = \\{(0.12, 0), (0.14, 0)\\}$。因此， $|S_L| = 2$。类别分布为 $\\{n_0=2, n_1=0\\}$。这是一个纯子集。\n熵为 $H(S_L) = -(1 \\ln(1) + 0) = 0$。\n$S_R = \\{(0.18, 1), (0.30, 1), (0.31, 1)\\}$。因此， $|S_R| = 3$。类别分布为 $\\{n_0=0, n_1=3\\}$。这也是一个纯子集。\n熵为 $H(S_R) = -(0 + 1 \\ln(1)) = 0$。\n加权分裂后熵是：\n$$H(S|\\tau_2) = \\frac{2}{5} H(S_L) + \\frac{3}{5} H(S_R) = \\frac{2}{5}(0) + \\frac{3}{5}(0) = 0$$\n\n情况 3: $\\tau = \\tau_3 = 0.24$\n分裂为 $x_j \\le 0.24$ 对比 $x_j > 0.24$。\n$S_L = \\{(0.12, 0), (0.14, 0), (0.18, 1)\\}$。因此， $|S_L| = 3$。类别分布为 $\\{n_0=2, n_1=1\\}$。\n熵为 $H(S_L) = -\\left(\\frac{2}{3} \\ln\\left(\\frac{2}{3}\\right) + \\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right)\\right)$。\n$S_R = \\{(0.30, 1), (0.31, 1)\\}$。因此， $|S_R| = 2$。类别分布为 $\\{n_0=0, n_1=2\\}$。这是一个纯子集。\n熵为 $H(S_R) = -(0 + 1 \\ln(1)) = 0$。\n加权分裂后熵是：\n$$H(S|\\tau_3) = \\frac{3}{5} H(S_L) + \\frac{2}{5} H(S_R) = \\frac{3}{5}\\left(-\\frac{2}{3} \\ln\\left(\\frac{2}{3}\\right) - \\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right)\\right) + \\frac{2}{5}(0) = -\\frac{2}{5} \\ln\\left(\\frac{2}{3}\\right) - \\frac{1}{5} \\ln\\left(\\frac{1}{3}\\right)$$\n$$H(S|\\tau_3) = -\\frac{2}{5}(\\ln(2) - \\ln(3)) - \\frac{1}{5}(-\\ln(3)) = \\frac{3}{5} \\ln(3) - \\frac{2}{5} \\ln(2)$$\n\n情况 4: $\\tau = \\tau_4 = 0.305$\n分裂为 $x_j \\le 0.305$ 对比 $x_j > 0.305$。\n$S_L = \\{(0.12, 0), (0.14, 0), (0.18, 1), (0.30, 1)\\}$。因此， $|S_L| = 4$。类别分布为 $\\{n_0=2, n_1=2\\}$。\n熵为 $H(S_L) = -\\left(\\frac{2}{4} \\ln\\left(\\frac{2}{4}\\right) + \\frac{2}{4} \\ln\\left(\\frac{2}{4}\\right)\\right) = -\\ln\\left(\\frac{1}{2}\\right) = \\ln(2)$。\n$S_R = \\{(0.31, 1)\\}$。因此， $|S_R| = 1$。类别分布为 $\\{n_0=0, n_1=1\\}$。这是一个纯子集。\n熵为 $H(S_R) = -(0 + 1 \\ln(1)) = 0$。\n加权分裂后熵是：\n$$H(S|\\tau_4) = \\frac{4}{5} H(S_L) + \\frac{1}{5} H(S_R) = \\frac{4}{5} \\ln(2) + \\frac{1}{5}(0) = \\frac{4}{5} \\ln(2)$$\n\n为了找到最优阈值，我们比较加权分裂后熵。最低的熵对应最高的信息增益。\n- $H(S|\\tau_1) = \\frac{8}{5} \\ln(2) - \\frac{3}{5} \\ln(3) \\approx \\frac{8}{5}(0.693) - \\frac{3}{5}(1.098) = 1.109 - 0.659 = 0.450$\n- $H(S|\\tau_2) = 0$\n- $H(S|\\tau_3) = \\frac{3}{5} \\ln(3) - \\frac{2}{5} \\ln(2) \\approx \\frac{3}{5}(1.098) - \\frac{2}{5}(0.693) = 0.659 - 0.277 = 0.382$\n- $H(S|\\tau_4) = \\frac{4}{5} \\ln(2) \\approx \\frac{4}{5}(0.693) = 0.554$\n\n比较这些值：$0  0.382  0.450  0.554$。\n最小的加权分裂后熵是 $0$，它在 $\\tau_2 = 0.16$ 时出现。这个分裂产生了两个纯子集，完美地分开了各个类别。因此，信息增益在此阈值处达到最大。最大信息增益是 $IG(\\tau_2) = H(S) - 0 = H(S)$。\n\n最大化信息增益的阈值是 $\\tau_2$。",
            "answer": "$$\n\\boxed{0.16}\n$$"
        },
        {
            "introduction": "虽然单个决策树很直观，但它们可能不稳定且容易过拟合。随机森林通过平均许多树的预测来克服这个问题。本练习将通过使用偏差-方差分解，来探究这种改进背后的“原因”，估算当从单个决策树模型转换到随机森林模型时，模型的方差减少了多少 ()。",
            "id": "4535439",
            "problem": "在一项使用计算机断层扫描（CT）图像的放射组学回归研究中，一个团队从一个高维放射组学特征集中预测一个连续的影像衍生风险评分，并使用平方误差损失函数。对于一个在这些放射组学特征上训练的单决策树，观测到的训练均方误差（MSE）为 $0.10$，观测到的测试均方误差为 $0.18$。对于一个通过对同样特征构建的决策树进行自助聚合（bagging）获得的随机森林（RF）模型，观测到的测试均方误差为 $0.12$。在平方误差预测风险的偏差-方差-噪声分解下，并假设在这两个模型中，bagging 主要改变方差分量，而对偏差和不可约噪声分量的改变可以忽略不计，请估计由 bagging 带来的泛化误差中方差分量的绝对减少量。请用一个小数表示你的答案，并将结果四舍五入到三位有效数字。",
            "solution": "该问题要求计算从单个决策树模型转为随机森林模型时，泛化误差中方差分量的绝对减少量。解决方案基于平方误差损失的偏差-方差-噪声分解。\n\n预期泛化误差（由测试均方误差 $MSE_{test}$ 估计）可以分解为三个分量：偏差的平方、方差和不可约噪声。对于一个给定的预测模型，其关系表示为：\n$$\nMSE_{test} = (\\text{Bias})^2 + \\text{Variance} + \\text{Noise}\n$$\n其中：\n- $(\\text{Bias})^2$ 是偏差的平方，代表由学习算法中的错误假设引起的误差。\n- $\\text{Variance}$ 是模型预测值的方差，代表模型对训练集中微小波动的敏感度。\n- $\\text{Noise}$ 是不可约误差，它是数据生成过程的一个属性，代表了任何模型预期误差的下限。\n\n我们用相应的下标来表示单个决策树（DT）和随机森林（RF）模型的分量。\n\n对于单个决策树模型，给定的测试均方误差为 $MSE_{test, DT} = 0.18$。其分解为：\n$$\nMSE_{test, DT} = (\\text{Bias}_{DT})^2 + \\text{Variance}_{DT} + \\text{Noise}\n$$\n代入给定值，我们得到：\n$$\n0.18 = (\\text{Bias}_{DT})^2 + \\text{Variance}_{DT} + \\text{Noise} \\quad (1)\n$$\n决策树的训练均方误差为 $0.10$，此信息已被注意到，但在问题的假设下，计算并不直接需要它。训练均方误差（$0.10$）和测试均方误差（$0.18$）之间的差异表明存在过拟合，这是一种高方差的情况，而 bagging 正是为缓解这种情况而设计的。\n\n对于使用自助聚合（bagging）的随机森林模型，给定的测试均方误差为 $MSE_{test, RF} = 0.12$。其分解为：\n$$\nMSE_{test, RF} = (\\text{Bias}_{RF})^2 + \\text{Variance}_{RF} + \\text{Noise}\n$$\n代入给定值，我们得到：\n$$\n0.12 = (\\text{Bias}_{RF})^2 + \\text{Variance}_{RF} + \\text{Noise} \\quad (2)\n$$\n问题陈述了一个关键假设：“bagging 主要改变方差分量，而对偏差和不可约噪声分量的改变可以忽略不计”。这意味着两个条件：\n1. 不可约噪声项 $\\text{Noise}$ 是数据固有的，因此对于两个模型是相同的。\n2. 随机森林模型的偏差平方约等于单个决策树模型的偏差平方。即 $(\\text{Bias}_{RF})^2 \\approx (\\text{Bias}_{DT})^2$。我们可以将这个共同的偏差平方项表示为 $(\\text{Bias})^2$。\n\n应用这个假设，方程 $(1)$ 和 $(2)$ 可以重写为：\n$$\n0.18 = (\\text{Bias})^2 + \\text{Variance}_{DT} + \\text{Noise} \\quad (1')\n$$\n$$\n0.12 = (\\text{Bias})^2 + \\text{Variance}_{RF} + \\text{Noise} \\quad (2')\n$$\n我们需要求的是方差分量的绝对减少量，即差值 $\\text{Variance}_{DT} - \\text{Variance}_{RF}$。为了求这个量，我们可以用方程 $(1')$ 减去方程 $(2')$：\n$$\n0.18 - 0.12 = ((\\text{Bias})^2 + \\text{Variance}_{DT} + \\text{Noise}) - ((\\text{Bias})^2 + \\text{Variance}_{RF} + \\text{Noise})\n$$\n共同的偏差和噪声项相互抵消：\n$$\n0.06 = (\\text{Bias})^2 - (\\text{Bias})^2 + \\text{Variance}_{DT} - \\text{Variance}_{RF} + \\text{Noise} - \\text{Noise}\n$$\n$$\n0.06 = \\text{Variance}_{DT} - \\text{Variance}_{RF}\n$$\n因此，由 bagging 带来的方差分量的绝对减少量为 $0.06$。\n\n问题要求答案四舍五入到三位有效数字。计算出的值恰好是 $0.06$。为了用三位有效数字表示这个小数，我们需要添加末尾的零。第一个有效数字是 $6$。为了表示三位有效数字，我们将数字写为 $0.0600$。",
            "answer": "$$\n\\boxed{0.0600}\n$$"
        },
        {
            "introduction": "像随机森林这样的复杂模型面临的一个常见挑战是其“黑箱”性质。部分依赖图（Partial Dependence Plots, PDP）为我们提供了一个观察这些模型的窗口，展示了当单个特征变化时，模型的预测会如何随之改变。在本练习中，您将为一个放射组学特征计算并解释部分依赖图，学会识别特征与模型输出之间的有意义的关系，例如单调或U型模式 ()。",
            "id": "4535372",
            "problem": "给定一个小型放射组学场景，其中使用随机森林 (RF) 将手工制作的放射组学特征映射到一个连续的风险评分，该评分被解释为区间 $\\left[0,1\\right]$ 内的估计概率。考虑单个标准化纹理特征 $x_j$（例如，灰度共生矩阵 (GLCM) 对比度，已在队列水平上进行 $z$-score 标准化）以及两个无关特征 $v_1$ 和 $v_2$。数据集包含 $n = 12$ 个受试者，其特征三元组为 $\\left(x_j^{(i)}, v_1^{(i)}, v_2^{(i)}\\right)$，其中 $i \\in \\{1,\\dots,12\\}$。标准化的纹理特征值满足 $\\sum_{i=1}^{12} x_j^{(i)} = 0$（近似标准化为单位方差），且无关特征位于单位区间内。具体来说，数据如下：\n- $i = 1$：$x_j^{(1)} = -1.6$, $v_1^{(1)} = 0.15$, $v_2^{(1)} = 0.20$\n- $i = 2$：$x_j^{(2)} = -1.3$, $v_1^{(2)} = 0.80$, $v_2^{(2)} = 0.40$\n- $i = 3$：$x_j^{(3)} = -1.0$, $v_1^{(3)} = 0.60$, $v_2^{(3)} = 0.55$\n- $i = 4$：$x_j^{(4)} = -0.7$, $v_1^{(4)} = 0.35$, $v_2^{(4)} = 0.75$\n- $i = 5$：$x_j^{(5)} = -0.4$, $v_1^{(5)} = 0.55$, $v_2^{(5)} = 0.35$\n- $i = 6$：$x_j^{(6)} = -0.1$, $v_1^{(6)} = 0.25$, $v_2^{(6)} = 0.90$\n- $i = 7$：$x_j^{(7)} = 0.1$, $v_1^{(7)} = 0.95$, $v_2^{(7)} = 0.45$\n- $i = 8$：$x_j^{(8)} = 0.4$, $v_1^{(8)} = 0.10$, $v_2^{(8)} = 0.15$\n- $i = 9$：$x_j^{(9)} = 0.7$, $v_1^{(9)} = 0.65$, $v_2^{(9)} = 0.85$\n- $i = 10$：$x_j^{(10)} = 1.0$, $v_1^{(10)} = 0.40$, $v_2^{(10)} = 0.50$\n- $i = 11$：$x_j^{(11)} = 1.3$, $v_1^{(11)} = 0.85$, $v_2^{(11)} = 0.30$\n- $i = 12$：$x_j^{(12)} = 1.6$, $v_1^{(12)} = 0.30$, $v_2^{(12)} = 0.70$\n\n随机森林 (RF) 的预测是其决策树预测的算术平均值。每个决策树都是一个分段常数函数，每次对一个特征进行分裂，并在每个叶节点中返回一个常数值。特征 $x_j$ 的部分依赖 (PD) 函数定义为\n$$\nPD_j(z) \\equiv \\mathbb{E}_{\\mathbf{X}_{-j}}\\left[ f\\big(z, \\mathbf{X}_{-j}\\big) \\right],\n$$\n其中 $f(\\cdot)$ 是 RF 预测函数，$\\mathbf{X}_{-j}$ 表示除 $x_j$ 之外的所有特征。在具有 $n$ 个观测值的有限样本中，标准的经验近似为\n$$\n\\widehat{PD}_j(z) = \\frac{1}{n}\\sum_{i=1}^{n} f\\big(z, \\mathbf{x}^{(i)}_{-j}\\big),\n$$\n也就是说，仅将第 $j$ 个分量替换为 $z$，并在所有观测到的无关特征上对 RF 预测进行平均。\n\n您的任务是计算 $\\widehat{PD}_j(z)$，其中 $z$ 等于标准化纹理特征 $x_j$ 的五个经验分位数，即分位数水平 $q \\in \\{0.1, 0.3, 0.5, 0.7, 0.9\\}$。然后，根据以下定义自动解释这五个 $z$ 点上得到的 $\\widehat{PD}_j$ 值是单调的还是 U 形的：\n- 单调模式：在一阶差分的绝对容差 $\\tau = 10^{-6}$ 内，序列是非递减或非递增的。\n- U 形模式：存在一个内部索引 $m \\in \\{2,3,4\\}$，使得 $\\widehat{PD}_j(z_m)$ 是全局最小值，两个端点都满足 $\\widehat{PD}_j(z_1) - \\widehat{PD}_j(z_m) \\ge \\delta$ 和 $\\widehat{PD}_j(z_5) - \\widehat{PD}_j(z_m) \\ge \\delta$（其中 $\\delta = 0.05$），并且序列在达到 $m$ 之前是非递增的，从 $m$ 开始是非递减的（在容差 $\\tau$ 内）。\n\n您必须评估四个不同的随机森林，每个随机森林有三棵树。对于每棵树，请使用以下坐标轴对齐的分割和叶节点预测。设特征顺序为 $(x_j, v_1, v_2)$，以下所有阈值仅在用 $\\le$ 表示时包含右分支；否则，按书面形式应用严格不等式。\n\n- RF $1$ (预期 $\\widehat{PD}_j$ 递增)：\n  - 树 $1$：如果 $x_j  -0.5$ 则 $0.2$；否则如果 $x_j  0.5$ 则 $0.5$；否则 $0.8$。\n  - 树 $2$：如果 $x_j  0$ 则 (如果 $v_1  0.6$ 则 $0.35$ 否则 $0.45$)；否则 (如果 $v_1  0.6$ 则 $0.65$ 否则 $0.75$)。\n  - 树 $3$：如果 $x_j  0.3$ 则 (如果 $v_2  0.5$ 则 $0.4$ 否则 $0.5$)；否则 (如果 $v_2  0.5$ 则 $0.7$ 否则 $0.8$)。\n\n- RF $2$ (预期 $\\widehat{PD}_j$ 递减)：\n  - 树 $1$：如果 $x_j  -0.5$ 则 $0.8$；否则如果 $x_j  0.5$ 则 $0.5$；否则 $0.2$。\n  - 树 $2$：如果 $x_j  0$ 则 (如果 $v_1  0.6$ 则 $0.75$ 否则 $0.65$)；否则 (如果 $v_1  0.6$ 则 $0.45$ 否则 $0.35$)。\n  - 树 $3$：如果 $x_j  0.3$ 则 (如果 $v_2  0.5$ 则 $0.8$ 否则 $0.7$)；否则 (如果 $v_2  0.5$ 则 $0.5$ 否则 $0.4$)。\n\n- RF $3$ (预期 $\\widehat{PD}_j$呈 U 形)：\n  - 树 $1$：如果 $x_j  -0.5$ 则 $0.7$；否则如果 $x_j \\le 0.5$ 则 $0.2$；否则 $0.7$。\n  - 树 $2$：如果 $x_j  -0.8$ 则 $0.9$；否则如果 $x_j \\le 0.8$ 则 (如果 $v_1  0.6$ 则 $0.30$ 否则 $0.35$)；否则 $0.9$。\n  - 树 $3$：如果 $x_j  -0.3$ 则 $0.7$；否则如果 $x_j \\le 0.3$ 则 (如果 $v_2  0.5$ 则 $0.25$ 否则 $0.30$)；否则 $0.7$。\n\n- RF $4$ (预期 $\\widehat{PD}_j$ 相对于 $x_j$ 是平坦的)：\n  - 树 $1$：如果 $v_1  0.6$ 则 $0.5$；否则 $0.6$。\n  - 树 $2$：如果 $v_2  0.5$ 则 $0.55$；否则 $0.45$。\n  - 树 $3$：如果 $v_1  0.4$ 则 $0.52$；否则如果 $v_1  0.8$ 则 $0.48$；否则 $0.50$。\n\n请使用以下来自统计学习的经过充分检验的原则和定义作为您的基础：\n- 随机森林预测函数是其决策树预测的算术平均值。\n- 特征的部分依赖函数定义为对剩余特征的联合分布的期望，其经验近似是在观测数据上的样本平均值（根据大数定律）。\n- 单调和 U 形模式通过分析一阶差分和相对极值来识别，由用于数值鲁棒性的容差控制。\n\n计算 $x_j$ 在水平 $q \\in \\{0.1, 0.3, 0.5, 0.7, 0.9\\}$ 上的经验分位数，然后为四个 RF 中的每一个计算这五个 $z$ 值处的 $\\widehat{PD}_j(z)$。将每个 $\\widehat{PD}_j(z)$ 值四舍五入到小数点后四位以便报告。对于每个 RF，确定两个布尔值：序列是否根据上述定义是单调的，以及是否根据上述定义是 U 形的。\n\n测试套件和要求的最终输出：\n- 测试套件由上述四个 RF 组成。\n- 对于每个 RF，您的程序必须返回一个包含两个元素的列表：第一个是五个四舍五入的 $\\widehat{PD}_j$ 值的列表，第二个是包含两个布尔值的列表 $[\\text{is\\_monotonic}, \\text{is\\_u\\_shaped}]$。\n- 您的程序应生成单行输出，其中包含一个长度为四的列表（每个 RF 一个），格式为用方括号括起来的逗号分隔列表，不含空格。例如，整体结构必须是 $[\\,[ [a_1,a_2,a_3,a_4,a_5],[b_1,b_2] ], \\dots ]$ 的形式，\n- 其中 $a_k$ 是四舍五入到四位小数的浮点数，$b_1$，$b_2$ 是布尔值。\n- 确切的数值由您的实现根据上述规范确定。",
            "solution": "该解决方案将对问题进行验证，然后提供算法步骤。\n\n### 步骤 1：问题验证\n\n**1.1. 提取已知条件**\n\n*   **数据集**：$n=12$ 个受试者，其特征三元组为 $(x_j^{(i)}, v_1^{(i)}, v_2^{(i)})$。\n    *   $x_j$ 是一个标准化纹理特征，满足 $\\sum_{i=1}^{12} x_j^{(i)} = 0$。\n    *   $v_1, v_2$ 是位于 $[0,1]$ 区间的无关特征。\n    *   数据点已为 $i=1, \\dots, 12$ 明确提供。\n*   **模型**：随机森林 (RF) 的预测是其决策树预测的算术平均值。\n*   **任务 1：部分依赖计算**：\n    *   特征 $x_j$ 在点 $z$ 的部分依赖 (PD) 函数由以下公式近似：\n        $$ \\widehat{PD}_j(z) = \\frac{1}{n}\\sum_{i=1}^{n} f\\big(z, \\mathbf{x}^{(i)}_{-j}\\big) $$\n    *   该值必须为四个不同的 RF 模型计算。\n    *   评估点 $z$ 是观测到的 $x_j$ 值在水平 $q \\in \\{0.1, 0.3, 0.5, 0.7, 0.9\\}$ 上的经验分位数。\n*   **任务 2：模式解释**：\n    *   五个 $\\widehat{PD}_j(z)$ 值的序列必须被分类为单调或 U 形。\n    *   **单调**：序列非递减或非递增，一阶差分的绝对容差为 $\\tau = 10^{-6}$。\n    *   **U 形**：在索引 $m \\in \\{2,3,4\\}$ 处存在一个内部全局最小值。两个端点都比最小值大至少 $\\delta=0.05$。序列在达到最小值之前非递增，之后非递减，容差为 $\\tau$。\n*   **随机森林架构**：定义了四个 RF，每个 RF 有三棵决策树。每棵树的分割条件和叶节点预测都已明确指定。\n    *   RF 1：预期 PD 递增。\n    *   RF 2：预期 PD 递减。\n    *   RF 3：预期 PD呈 U 形。\n    *   RF 4：预期 PD 平坦。\n\n**1.2. 使用提取的已知条件进行验证**\n\n*   **科学依据**：该问题基于标准的机器学习和统计概念（随机森林、部分依赖图、分位数、单调性）。所提供的定义是正确且广泛使用的。\n*   **适定性**：该问题是适定的。数据已提供，模型已完全指定，计算任务清晰，解释标准明确。存在唯一解且可以计算。\n*   **客观性**：该问题以精确的数值数据、数学公式和逻辑规则客观陈述。没有主观或基于意见的内容。\n*   **完整性与一致性**：该问题是自包含的。所有必要信息都已提供。所提供的 $x_j$ 值的总和确实为 0 ($-1.6 - 1.3 - 1.0 - 0.7 - 0.4 - 0.1 + 0.1 + 0.4 + 0.7 + 1.0 + 1.3 + 1.6 = 0$)，这与标准化特征的描述相符。所有无关特征都在指定范围 $[0,1]$ 内。模型定义是完整的。\n\n**1.3. 结论与行动**\n\n问题是有效的。下面将给出完整解法。\n\n### 步骤 2：算法求解\n\n解决方案分四个阶段进行：\n1.  将数据集和四个随机森林模型的架构定义为函数。\n2.  通过计算特征 $x_j$ 的经验分位数来计算五个评估点 $z$。\n3.  对于每个 RF，计算在五个分位数点上的部分依赖值 $\\widehat{PD}_j(z)$。这涉及在无关特征的数据集上对 RF 的预测进行平均。\n4.  对于每组五个 PD 值序列，应用所提供的规则来确定该模式是单调和/或 U 形的。\n\n**2.1. 数据和模型实现**\n\n首先，我们将数据集和决策树规则表示为结构化格式。数据集 $\\mathbf{D} = \\{(\\mathbf{x}^{(i)}, y^{(i)}) \\}_{i=1}^{12}$ 由特征向量 $\\mathbf{x}^{(i)} = (x_j^{(i)}, v_1^{(i)}, v_2^{(i)})$ 组成。\n\n无关特征 $\\mathbf{x}^{(i)}_{-j} = (v_1^{(i)}, v_2^{(i)})$ 将用于边缘化。特征 $x_j$ 的值为：\n$$\n\\mathbf{x}_j = [-1.6, -1.3, -1.0, -0.7, -0.4, -0.1, 0.1, 0.4, 0.7, 1.0, 1.3, 1.6]\n$$\n无关特征的值为：\n$$\n\\mathbf{v}_1 = [0.15, 0.80, 0.60, 0.35, 0.55, 0.25, 0.95, 0.10, 0.65, 0.40, 0.85, 0.30]\n$$\n$$\n\\mathbf{v}_2 = [0.20, 0.40, 0.55, 0.75, 0.35, 0.90, 0.45, 0.15, 0.85, 0.50, 0.30, 0.70]\n$$\n$4 \\times 3 = 12$ 棵决策树中的每一棵都实现为函数 $f_T(x_j, v_1, v_2)$，返回一个标量值。随机森林函数 $f_{RF}$ 是其三个组成树函数的平均值：\n$$\nf_{RF}(x_j, v_1, v_2) = \\frac{1}{3} \\sum_{k=1}^{3} f_{T_k}(x_j, v_1, v_2)\n$$\n\n**2.2. 分位数计算**\n\n$k \\in \\{1, \\dots, 5\\}$ 的评估点 $z_k$ 是已排序 $x_j$ 数据在水平 $q \\in \\{0.1, 0.3, 0.5, 0.7, 0.9\\}$ 上的经验分位数。我们使用数据点之间的标准线性插值。对于大小为 $n$ 的数据集，分位数 $q$ 的索引计算为 $(n-1)q$。\n当 $n=12$ 时，索引为 $1.1, 3.3, 5.5, 7.7, 9.9$。这些值是从已排序的 $x_j$ 数组中插值得到的。\n*   $z_1 (q=0.1)$：$0.9 \\times x_j^{(2)} + 0.1 \\times x_j^{(3)} = 0.9 \\times (-1.3) + 0.1 \\times (-1.0) = -1.27$\n*   $z_2 (q=0.3)$：$0.7 \\times x_j^{(4)} + 0.3 \\times x_j^{(5)} = 0.7 \\times (-0.7) + 0.3 \\times (-0.4) = -0.61$\n*   $z_3 (q=0.5)$：$0.5 \\times x_j^{(6)} + 0.5 \\times x_j^{(7)} = 0.5 \\times (-0.1) + 0.5 \\times (0.1) = 0.0$\n*   $z_4 (q=0.7)$：$0.3 \\times x_j^{(8)} + 0.7 \\times x_j^{(9)} = 0.3 \\times (0.4) + 0.7 \\times (0.7) = 0.61$\n*   $z_5 (q=0.9)$：$0.1 \\times x_j^{(10)} + 0.9 \\times x_j^{(11)} = 0.1 \\times (1.0) + 0.9 \\times (1.3) = 1.27$\n五个评估点为 $z = [-1.27, -0.61, 0.0, 0.61, 1.27]$。\n\n**2.3. 部分依赖计算**\n\n对于每个 RF 和每个评估点 $z_k$，我们计算 $\\widehat{PD}_j(z_k)$:\n$$\n\\widehat{PD}_j(z_k) = \\frac{1}{12}\\sum_{i=1}^{12} f_{RF}(z_k, v_1^{(i)}, v_2^{(i)})\n$$\n根据期望（或求和）的线性性质，我们可以对每棵树的部分依赖进行平均：\n$$\n\\widehat{PD}_{j,RF}(z_k) = \\frac{1}{3} \\sum_{m=1}^{3} \\left( \\frac{1}{12}\\sum_{i=1}^{12} f_{T_m}(z_k, v_1^{(i)}, v_2^{(i)}) \\right) = \\frac{1}{3} \\sum_{m=1}^{3} \\widehat{PD}_{j,T_m}(z_k)\n$$\n这简化了计算，因为对于许多树来说，固定 $z_k$ 的预测仅取决于一个无关变量，其影响可以预先平均。\n\n让我们预先计算无关特征的平均值：\n*   $v_1  0.6$ 的受试者数量：7。\n*   $v_2  0.5$ 的受试者数量：6。\n*   $v_1  0.4$ 的受试者数量：5。\n*   $0.4 \\le v_1  0.8$ 的受试者数量：4。\n*   $v_1 \\ge 0.8$ 的受试者数量：3。\n\n这使我们能够快速计算任何在无关特征上分裂的分支的平均预测。例如，对于 RF1-Tree2，如果 $x_j  0$，预测基于分裂 $v_1  0.6$。平均预测为 $\\frac{7 \\times 0.35 + 5 \\times 0.45}{12} = \\frac{4.7}{12}$。\n\n**RF1 (递增)：**\n*   $\\widehat{PD}(z_1=-1.27) = \\frac{1}{3}(0.2 + \\frac{7 \\times 0.35 + 5 \\times 0.45}{12} + \\frac{6 \\times 0.4 + 6 \\times 0.5}{12}) = \\frac{1}{3}(0.2 + 0.39166... + 0.45) \\approx 0.3472$\n*   $\\widehat{PD}(z_2=-0.61) = \\frac{1}{3}(0.2 + 0.39166... + 0.45) \\approx 0.3472$\n*   $\\widehat{PD}(z_3=0.0) = \\frac{1}{3}(0.5 + \\frac{7 \\times 0.65 + 5 \\times 0.75}{12} + 0.45) = \\frac{1}{3}(0.5 + 0.69166... + 0.45) \\approx 0.5472$\n*   $\\widehat{PD}(z_4=0.61) = \\frac{1}{3}(0.8 + 0.69166... + \\frac{6 \\times 0.7 + 6 \\times 0.8}{12}) = \\frac{1}{3}(0.8 + 0.69166... + 0.75) \\approx 0.7472$\n*   $\\widehat{PD}(z_5=1.27) = \\frac{1}{3}(0.8 + 0.69166... + 0.75) \\approx 0.7472$\n\n**RF2 (递减)：**\n*   $\\widehat{PD}(z_1=-1.27) = \\frac{1}{3}(0.8 + \\frac{7 \\times 0.75 + 5 \\times 0.65}{12} + \\frac{6 \\times 0.8 + 6 \\times 0.7}{12}) = \\frac{1}{3}(0.8 + 0.70833... + 0.75) \\approx 0.7528$\n*   $\\widehat{PD}(z_2=-0.61) = \\frac{1}{3}(0.8 + 0.70833... + 0.75) \\approx 0.7528$\n*   $\\widehat{PD}(z_3=0.0) = \\frac{1}{3}(0.5 + \\frac{7 \\times 0.45 + 5 \\times 0.35}{12} + 0.75) = \\frac{1}{3}(0.5 + 0.40833... + 0.75) \\approx 0.5528$\n*   $\\widehat{PD}(z_4=0.61) = \\frac{1}{3}(0.2 + 0.40833... + \\frac{6 \\times 0.5 + 6 \\times 0.4}{12}) = \\frac{1}{3}(0.2 + 0.40833... + 0.45) \\approx 0.3528$\n*   $\\widehat{PD}(z_5=1.27) = \\frac{1}{3}(0.2 + 0.40833... + 0.45) \\approx 0.3528$\n\n**RF3 (U 形)：**\n*   $\\widehat{PD}(z_1=-1.27) = \\frac{1}{3}(0.7 + 0.9 + 0.7) \\approx 0.7667$\n*   $\\widehat{PD}(z_2=-0.61) = \\frac{1}{3}(0.7 + \\frac{7 \\times 0.30 + 5 \\times 0.35}{12} + 0.7) = \\frac{1}{3}(0.7 + 0.32083... + 0.7) \\approx 0.5736$\n*   $\\widehat{PD}(z_3=0.0) = \\frac{1}{3}(0.2 + 0.32083... + \\frac{6 \\times 0.25 + 6 \\times 0.30}{12}) = \\frac{1}{3}(0.2 + 0.32083... + 0.275) \\approx 0.2653$\n*   $\\widehat{PD}(z_4=0.61) = \\frac{1}{3}(0.7 + 0.9 + 0.7) \\approx 0.7667$\n*   $\\widehat{PD}(z_5=1.27) = \\frac{1}{3}(0.7 + 0.9 + 0.7) \\approx 0.7667$\n\n**RF4 (平坦)：**\nRF4 中的树不对 $x_j$ 进行分裂，因此对于所有 $z$，PD 值都是恒定的。\n*   $\\widehat{PD}_{T1} = (7 \\times 0.5 + 5 \\times 0.6)/12 = 6.5/12$\n*   $\\widehat{PD}_{T2} = (6 \\times 0.55 + 6 \\times 0.45)/12 = 6.0/12 = 0.5$\n*   $\\widehat{PD}_{T3} = (5 \\times 0.52 + 4 \\times 0.48 + 3 \\times 0.50)/12 = 6.02/12$\n*   $\\widehat{PD}_{RF4}(z) = \\frac{1}{3}(\\frac{6.5}{12} + \\frac{6.0}{12} + \\frac{6.02}{12}) = \\frac{18.52}{36} \\approx 0.5144$ 对于所有 $z$。\n\n**2.4. 模式分析**\n\n我们使用完整的浮点精度分析每组五个 PD 值的序列，然后再为最终报告进行四舍五入。\n容差为 $\\tau = 10^{-6}$ 和 $\\delta = 0.05$。\n\n*   **RF1 序列**：`[0.3472..., 0.3472..., 0.5472..., 0.7472..., 0.7472...]`。此序列是非递减的。因此，`monotonic = True`。它不是 U 形的，因为最小值不是内部值。`u_shaped = False`。\n*   **RF2 序列**：`[0.7527..., 0.7527..., 0.5527..., 0.3527..., 0.3527...]`。此序列是非递增的。因此，`monotonic = True`。它不是 U 形的。`u_shaped = False`。\n*   **RF3 序列**：`[0.7666..., 0.5736..., 0.2652..., 0.7666..., 0.7666...]`。该序列先递减后递增，所以不是单调的。`monotonic = False`。对于 U 形：\n    *   最小值位于 $m=3$（索引 2），这是一个内部点。\n    *   端点检查：$\\widehat{PD}(z_1) - \\widehat{PD}(z_3) = 0.5013... \\ge 0.05$。$\\widehat{PD}(z_5) - \\widehat{PD}(z_3) = 0.5013... \\ge 0.05$。此条件成立。\n    *   第一部分 $[0.766..., 0.573..., 0.265...]$ 是非递增的。第二部分 $[0.265..., 0.766..., 0.766...]$ 是非递减的。\n    *   所有条件都满足。`u_shaped = True`。\n*   **RF4 序列**：`[0.5144..., 0.5144..., 0.5144..., 0.5144..., 0.5144...]`。这个常数序列既是非递增的也是非递减的。`monotonic = True`。它不是 U 形的，因为不满足端点差异条件 `delta_min = 0.05`。`u_shaped = False`。\n\n实现将把这些步骤合成为一个程序。",
            "answer": "[[[0.3472,0.3472,0.5472,0.7472,0.7472],[true,false]],[[0.7528,0.7528,0.5528,0.3528,0.3528],[true,false]],[[0.7667,0.5736,0.2653,0.7667,0.7667],[false,true]],[[0.5144,0.5144,0.5144,0.5144,0.5144],[true,false]]]"
        }
    ]
}