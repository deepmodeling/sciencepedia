{
    "hands_on_practices": [
        {
            "introduction": "At the heart of any decision tree is the splitting criterion, which determines how the data is partitioned at each node. This exercise provides a hands-on calculation of information gain, a common metric based on the concept of entropy. By working through a small, hypothetical radiomics dataset, you will determine the optimal split point for a feature, gaining a fundamental understanding of how these models learn from data .",
            "id": "4535354",
            "problem": "A radiomics pipeline for classifying lung nodules as benign or malignant uses a single univariate decision tree split on a texture feature $x_{j}$ derived from the Gray-Level Co-occurrence Matrix (GLCM). You are given the sorted feature values $x_{j} \\in \\{0.12, 0.14, 0.18, 0.30, 0.31\\}$ with corresponding binary labels $y \\in \\{0,0,1,1,1\\}$, where $0$ denotes benign and $1$ denotes malignant. Consider a decision tree split of the form $x_{j} \\le \\tau$ versus $x_{j} > \\tau$, where $\\tau$ is a threshold chosen from midpoints between consecutive, distinct feature values.\n\nStarting from the definitions below, determine the threshold $\\tau$ that maximizes the information gain and justify the choice by evaluating the information gain for all admissible midpoints.\n\nUse the following fundamental base:\n- The entropy of a set $S$ with class probabilities $\\{p_{c}\\}$ is defined as $H(S) = -\\sum_{c} p_{c} \\ln p_{c}$, using the natural logarithm.\n- For a split of $S$ into left and right subsets $S_{L}$ and $S_{R}$, the weighted post-split entropy is $\\frac{|S_{L}|}{|S|} H(S_{L}) + \\frac{|S_{R}|}{|S|} H(S_{R})$.\n- The information gain is $IG = H(S) - \\left(\\frac{|S_{L}|}{|S|} H(S_{L}) + \\frac{|S_{R}|}{|S|} H(S_{R})\\right)$.\n\nProvide the final threshold $\\tau$ as a single real number. No rounding is required.",
            "solution": "The problem asks to find the optimal decision tree split threshold, $\\tau$, that maximizes the information gain for a given dataset.\n\nThe dataset, $S$, consists of 5 samples with feature-label pairs:\n$S = \\{(0.12, 0), (0.14, 0), (0.18, 1), (0.30, 1), (0.31, 1)\\}$.\nThere are 2 benign samples (label 0) and 3 malignant samples (label 1).\n\nFirst, we calculate the entropy of the entire dataset $S$. The class probabilities are $p_0 = \\frac{2}{5}$ and $p_1 = \\frac{3}{5}$.\n$$H(S) = -\\left( \\frac{2}{5} \\ln\\left(\\frac{2}{5}\\right) + \\frac{3}{5} \\ln\\left(\\frac{3}{5}\\right) \\right)$$\nThis initial entropy is a constant for all potential splits. Maximizing the information gain $IG = H(S) - H(S|\\tau)$ is equivalent to minimizing the weighted post-split entropy $H(S|\\tau)$.\n\nThe candidate thresholds $\\tau$ are the midpoints between consecutive, distinct feature values:\n1. $\\tau_1 = \\frac{0.12 + 0.14}{2} = 0.13$\n2. $\\tau_2 = \\frac{0.14 + 0.18}{2} = 0.16$\n3. $\\tau_3 = \\frac{0.18 + 0.30}{2} = 0.24$\n4. $\\tau_4 = \\frac{0.30 + 0.31}{2} = 0.305$\n\nWe now evaluate the weighted post-split entropy for each threshold.\n\n**Case 1: $\\tau = 0.13$**\n- Left subset $S_L$ ($x_j \\le 0.13$): $\\{(0.12, 0)\\}$. Pure node ($n_0=1, n_1=0$), so $H(S_L) = 0$.\n- Right subset $S_R$ ($x_j > 0.13$): 4 samples ($n_0=1, n_1=3$).\n- $H(S|\\tau_1) = \\frac{1}{5} H(S_L) + \\frac{4}{5} H(S_R) = \\frac{4}{5} \\left(-\\frac{1}{4} \\ln\\frac{1}{4} - \\frac{3}{4} \\ln\\frac{3}{4}\\right) \\approx 0.658$.\n\n**Case 2: $\\tau = 0.16$**\n- Left subset $S_L$ ($x_j \\le 0.16$): $\\{(0.12, 0), (0.14, 0)\\}$. Pure node ($n_0=2, n_1=0$), so $H(S_L) = 0$.\n- Right subset $S_R$ ($x_j > 0.16$): $\\{(0.18, 1), (0.30, 1), (0.31, 1)\\}$. Pure node ($n_0=0, n_1=3$), so $H(S_R) = 0$.\n- The weighted post-split entropy is:\n$$H(S|\\tau_2) = \\frac{2}{5} H(S_L) + \\frac{3}{5} H(S_R) = \\frac{2}{5}(0) + \\frac{3}{5}(0) = 0$$\n\n**Case 3: $\\tau = 0.24$**\n- Left subset $S_L$ ($x_j \\le 0.24$): 3 samples ($n_0=2, n_1=1$).\n- Right subset $S_R$ ($x_j > 0.24$): $\\{(0.30, 1), (0.31, 1)\\}$. Pure node ($n_0=0, n_1=2$), so $H(S_R) = 0$.\n- $H(S|\\tau_3) = \\frac{3}{5} H(S_L) + \\frac{2}{5} H(S_R) = \\frac{3}{5} \\left(-\\frac{2}{3} \\ln\\frac{2}{3} - \\frac{1}{3} \\ln\\frac{1}{3}\\right) + 0 \\approx 0.551$.\n\n**Case 4: $\\tau = 0.305$**\n- Left subset $S_L$ ($x_j \\le 0.305$): 4 samples ($n_0=2, n_1=2$).\n- Right subset $S_R$ ($x_j > 0.305$): $\\{(0.31, 1)\\}$. Pure node ($n_0=0, n_1=1$), so $H(S_R) = 0$.\n- $H(S|\\tau_4) = \\frac{4}{5} H(S_L) + \\frac{1}{5} H(S_R) = \\frac{4}{5} \\left(-\\frac{2}{4} \\ln\\frac{2}{4} - \\frac{2}{4} \\ln\\frac{2}{4}\\right) + 0 = \\frac{4}{5} \\ln(2) \\approx 0.554$.\n\nComparing the post-split entropies, the minimum value is $0$, which occurs for $\\tau_2 = 0.16$. This split perfectly separates the data into pure nodes, thus maximizing the information gain.\n\nThe threshold that maximizes the information gain is $\\tau_2$.",
            "answer": "$$\n\\boxed{0.16}\n$$"
        },
        {
            "introduction": "While single decision trees are easy to interpret, they often suffer from high variance and are prone to overfitting. Random Forests mitigate this by averaging many trees, a process known as bagging. This practice problem uses the bias-variance decomposition to quantify exactly how much of the performance improvement comes from variance reduction, providing a clear, quantitative justification for the power of ensemble methods in radiomics .",
            "id": "4535439",
            "problem": "In a radiomics regression study using computed tomography (CT) images, a team predicts a continuous imaging-derived risk score from a high-dimensional set of radiomic features using squared-error loss. For a single decision tree trained on the radiomic features, the observed training mean squared error (MSE) is $0.10$, and the observed test MSE is $0.18$. For a Random Forest (RF) model obtained by bootstrap aggregation (bagging) of decision trees built on the same features, the observed test MSE is $0.12$. Under the bias–variance–noise decomposition of squared-error predictive risk, and assuming that bagging changes the variance component while causing negligible change to the bias and irreducible noise across these two models, estimate the absolute reduction in the variance component of the generalization error attributable to bagging. Provide your answer as a single decimal number and round your result to three significant figures.",
            "solution": "The problem asks for the absolute reduction in the variance component of the generalization error when moving from a single decision tree model to a Random Forest model, based on the bias–variance–noise decomposition for squared-error loss.\n\nThe expected generalization error, estimated by the test mean squared error ($MSE_{test}$), can be decomposed as:\n$$\nMSE_{test} = (\\text{Bias})^2 + \\text{Variance} + \\text{Noise}\n$$\nwhere $(\\text{Bias})^2$ is the squared bias, $\\text{Variance}$ is the model variance, and $\\text{Noise}$ is the irreducible error.\n\nLet's denote the components for the single decision tree (DT) and the Random Forest (RF) models with respective subscripts.\n\nFor the single decision tree, the test MSE is given as $MSE_{test, DT} = 0.18$. Its decomposition is:\n$$\n0.18 = (\\text{Bias}_{DT})^2 + \\text{Variance}_{DT} + \\text{Noise} \\quad (1)\n$$\n\nFor the Random Forest model, the test MSE is given as $MSE_{test, RF} = 0.12$. Its decomposition is:\n$$\n0.12 = (\\text{Bias}_{RF})^2 + \\text{Variance}_{RF} + \\text{Noise} \\quad (2)\n$$\nThe problem states the critical assumption that bagging causes negligible change to the bias and irreducible noise. This implies:\n1. The irreducible noise, $\\text{Noise}$, is the same for both models as it is a property of the data.\n2. The squared bias is approximately equal for both models: $(\\text{Bias}_{RF})^2 \\approx (\\text{Bias}_{DT})^2$. Let's denote this common term as $(\\text{Bias})^2$.\n\nApplying these assumptions, equations $(1)$ and $(2)$ become:\n$$\n0.18 = (\\text{Bias})^2 + \\text{Variance}_{DT} + \\text{Noise} \\quad (1')\n$$\n$$\n0.12 = (\\text{Bias})^2 + \\text{Variance}_{RF} + \\text{Noise} \\quad (2')\n$$\nWe need to find the absolute reduction in variance, which is $\\text{Variance}_{DT} - \\text{Variance}_{RF}$. Subtracting equation $(2')$ from equation $(1')$:\n$$\n0.18 - 0.12 = ((\\text{Bias})^2 + \\text{Variance}_{DT} + \\text{Noise}) - ((\\text{Bias})^2 + \\text{Variance}_{RF} + \\text{Noise})\n$$\nThe common bias and noise terms cancel out, leaving:\n$$\n0.06 = \\text{Variance}_{DT} - \\text{Variance}_{RF}\n$$\nThe absolute reduction in the variance component is $0.06$.\n\nThe problem requires the answer to be rounded to three significant figures. The calculated value is $0.06$. To express this with three significant figures, we write it as $0.0600$. The first significant figure is the 6, and the two trailing zeros are also significant.",
            "answer": "$$\n\\boxed{0.0600}\n$$"
        },
        {
            "introduction": "Random Forests can often feel like \"black boxes,\" but techniques exist to understand what they have learned. This exercise introduces Partial Dependence (PD) plots, a powerful tool for visualizing the marginal effect of a single feature on the model's prediction. By computing and interpreting the PD plot for a texture feature, you will learn to uncover whether the model has identified a monotonic (e.g., higher value means higher risk) or a more complex, U-shaped relationship with the outcome .",
            "id": "4535372",
            "problem": "You are given a small radiomics scenario in which a Random Forest (RF) is used to map handcrafted radiomic features to a continuous risk score interpreted as an estimated probability in the interval $\\left[0,1\\right]$. Consider a single standardized texture feature $x_j$ (for example, a Gray-Level Co-Occurrence Matrix (GLCM) contrast, already $z$-score standardized at the cohort level), alongside two nuisance features $v_1$ and $v_2$. The data set consists of $n = 12$ subjects with feature triplets $\\left(x_j^{(i)}, v_1^{(i)}, v_2^{(i)}\\right)$ for $i \\in \\{1,\\dots,12\\}$. The standardized texture feature values satisfy $\\sum_{i=1}^{12} x_j^{(i)} = 0$ (approximately standardized with unit variance), and the nuisance features are in the unit interval. Concretely, the data are:\n- $i = 1$: $x_j^{(1)} = -1.6$, $v_1^{(1)} = 0.15$, $v_2^{(1)} = 0.20$\n- $i = 2$: $x_j^{(2)} = -1.3$, $v_1^{(2)} = 0.80$, $v_2^{(2)} = 0.40$\n- $i = 3$: $x_j^{(3)} = -1.0$, $v_1^{(3)} = 0.60$, $v_2^{(3)} = 0.55$\n- $i = 4$: $x_j^{(4)} = -0.7$, $v_1^{(4)} = 0.35$, $v_2^{(4)} = 0.75$\n- $i = 5$: $x_j^{(5)} = -0.4$, $v_1^{(5)} = 0.55$, $v_2^{(5)} = 0.35$\n- $i = 6$: $x_j^{(6)} = -0.1$, $v_1^{(6)} = 0.25$, $v_2^{(6)} = 0.90$\n- $i = 7$: $x_j^{(7)} = 0.1$, $v_1^{(7)} = 0.95$, $v_2^{(7)} = 0.45$\n- $i = 8$: $x_j^{(8)} = 0.4$, $v_1^{(8)} = 0.10$, $v_2^{(8)} = 0.15$\n- $i = 9$: $x_j^{(9)} = 0.7$, $v_1^{(9)} = 0.65$, $v_2^{(9)} = 0.85$\n- $i = 10$: $x_j^{(10)} = 1.0$, $v_1^{(10)} = 0.40$, $v_2^{(10)} = 0.50$\n- $i = 11$: $x_j^{(11)} = 1.3$, $v_1^{(11)} = 0.85$, $v_2^{(11)} = 0.30$\n- $i = 12$: $x_j^{(12)} = 1.6$, $v_1^{(12)} = 0.30$, $v_2^{(12)} = 0.70$\n\nThe Random Forest (RF) prediction is the arithmetic mean of its decision tree predictions. Each decision tree is a piecewise-constant function that splits on one feature at a time and returns a constant in each leaf. The Partial Dependence (PD) function for feature $x_j$ is defined as\n$$\nPD_j(z) \\equiv \\mathbb{E}_{\\mathbf{X}_{-j}}\\left[ f\\big(z, \\mathbf{X}_{-j}\\big) \\right],\n$$\nwhere $f(\\cdot)$ is the RF prediction function, and $\\mathbf{X}_{-j}$ denotes all features except $x_j$. In finite samples with $n$ observations, the standard empirical approximation is\n$$\n\\widehat{PD}_j(z) = \\frac{1}{n}\\sum_{i=1}^{n} f\\big(z, \\mathbf{x}^{(i)}_{-j}\\big),\n$$\nthat is, replace only the $j$-th component by $z$, and average the RF prediction across the observed nuisance features.\n\nYour task is to compute $\\widehat{PD}_j(z)$ for $z$ equal to five empirical quantiles of the standardized texture feature $x_j$, namely at quantile levels $q \\in \\{0.1, 0.3, 0.5, 0.7, 0.9\\}$. Then, automatically interpret whether the resulting $\\widehat{PD}_j$ values across these five $z$ points are monotonic or U-shaped, based on the following definitions:\n- Monotonic pattern: The sequence is nondecreasing or nonincreasing within an absolute tolerance $\\tau = 10^{-6}$ on first differences.\n- U-shaped pattern: There exists an interior index $m \\in \\{2,3,4\\}$ such that $\\widehat{PD}_j(z_m)$ is the global minimum, both endpoints satisfy $\\widehat{PD}_j(z_1) - \\widehat{PD}_j(z_m) \\ge \\delta$ and $\\widehat{PD}_j(z_5) - \\widehat{PD}_j(z_m) \\ge \\delta$ with $\\delta = 0.05$, and the sequence is nonincreasing up to $m$ and nondecreasing from $m$ onward within tolerance $\\tau$.\n\nYou must evaluate four different Random Forests, each with three trees. For each tree, use the following axis-aligned splits and leaf predictions. Let the feature order be $(x_j, v_1, v_2)$, and all thresholds below are inclusive on the right branch only when stated with $\\le$; otherwise strict inequality $<$ applies as written.\n\n- RF $1$ (expected increasing $\\widehat{PD}_j$):\n  - Tree $1$: If $x_j < -0.5$ then $0.2$; else if $x_j < 0.5$ then $0.5$; else $0.8$.\n  - Tree $2$: If $x_j < 0$ then (if $v_1 < 0.6$ then $0.35$ else $0.45$); else (if $v_1 < 0.6$ then $0.65$ else $0.75$).\n  - Tree $3$: If $x_j < 0.3$ then (if $v_2 < 0.5$ then $0.4$ else $0.5$); else (if $v_2 < 0.5$ then $0.7$ else $0.8$).\n\n- RF $2$ (expected decreasing $\\widehat{PD}_j$):\n  - Tree $1$: If $x_j < -0.5$ then $0.8$; else if $x_j < 0.5$ then $0.5$; else $0.2$.\n  - Tree $2$: If $x_j < 0$ then (if $v_1 < 0.6$ then $0.75$ else $0.65$); else (if $v_1 < 0.6$ then $0.45$ else $0.35$).\n  - Tree $3$: If $x_j < 0.3$ then (if $v_2 < 0.5$ then $0.8$ else $0.7$); else (if $v_2 < 0.5$ then $0.5$ else $0.4$).\n\n- RF $3$ (expected U-shaped $\\widehat{PD}_j$):\n  - Tree $1$: If $x_j < -0.5$ then $0.7$; else if $x_j \\le 0.5$ then $0.2$; else $0.7$.\n  - Tree $2$: If $x_j < -0.8$ then $0.9$; else if $x_j \\le 0.8$ then (if $v_1 < 0.6$ then $0.30$ else $0.35$); else $0.9$.\n  - Tree $3$: If $x_j < -0.3$ then $0.7$; else if $x_j \\le 0.3$ then (if $v_2 < 0.5$ then $0.25$ else $0.30$); else $0.7$.\n\n- RF $4$ (expected flat $\\widehat{PD}_j$ with respect to $x_j$):\n  - Tree $1$: If $v_1 < 0.6$ then $0.5$; else $0.6$.\n  - Tree $2$: If $v_2 < 0.5$ then $0.55$; else $0.45$.\n  - Tree $3$: If $v_1 < 0.4$ then $0.52$; else if $v_1 < 0.8$ then $0.48$; else $0.50$.\n\nUse the following well-tested principles and definitions from statistical learning as your foundation:\n- The Random Forest prediction function is the arithmetic mean of its decision tree predictions.\n- The Partial Dependence function for a feature is defined as an expectation over the joint distribution of the remaining features, and its empirical approximation is the sample average across the observed data (by the Law of Large Numbers).\n- Monotonic and U-shaped patterns are recognized by analyzing first differences and relative extrema, controlled by tolerances for numerical robustness.\n\nCompute the empirical quantiles of $x_j$ at levels $q \\in \\{0.1, 0.3, 0.5, 0.7, 0.9\\}$ and then compute $\\widehat{PD}_j(z)$ at these five $z$ values for each of the four RFs. Round each $\\widehat{PD}_j(z)$ value to four decimal places for reporting. For each RF, determine two booleans: whether the sequence is monotonic according to the definition above, and whether it is U-shaped according to the definition above.\n\nTest suite and required final output:\n- The test suite consists of the four RFs described above.\n- For each RF, your program must return a list with two elements: the first is the list of the five rounded $\\widehat{PD}_j$ values, and the second is a two-element list $[\\text{is\\_monotonic}, \\text{is\\_u\\_shaped}]$ of booleans.\n- Your program should produce a single line of output containing a list of length four, one per RF, formatted as a comma-separated list enclosed in square brackets with no spaces. For example, the overall structure must be of the form\n$[\\,[ [a_1,a_2,a_3,a_4,a_5],[b_1,b_2] ], \\dots ]$,\nwhere $a_k$ are floats rounded to four decimals and $b_1$, $b_2$ are booleans. The exact numerical values are determined by your implementation according to the specifications above.",
            "solution": "The problem requires calculating the Partial Dependence (PD) values for four different Random Forest (RF) models at specific quantiles of a feature $x_j$, and then classifying the resulting PD curve as monotonic or U-shaped.\n\n**Step 1: Determine Evaluation Points**\nThe evaluation points, $z$, are the empirical quantiles of the feature $x_j$ at levels $q \\in \\{0.1, 0.3, 0.5, 0.7, 0.9\\}$. The sorted $x_j$ values are $[-1.6, -1.3, -1.0, -0.7, -0.4, -0.1, 0.1, 0.4, 0.7, 1.0, 1.3, 1.6]$. With $n=12$, we use linear interpolation to find the quantiles:\n*   $z_1 (q=0.1)$: $-1.27$\n*   $z_2 (q=0.3)$: $-0.61$\n*   $z_3 (q=0.5)$: $0.0$\n*   $z_4 (q=0.7)$: $0.61$\n*   $z_5 (q=0.9)$: $1.27$\n\n**Step 2: Calculate Partial Dependence for Each RF**\nThe PD value for a given $z$ is the average prediction of the RF when the feature $x_j$ is fixed to $z$, and the nuisance features $(v_1, v_2)$ vary over their observed values in the dataset.\n$$ \\widehat{PD}_j(z) = \\frac{1}{12}\\sum_{i=1}^{12} f_{RF}\\big(z, v_1^{(i)}, v_2^{(i)}\\big) $$\nWe compute this for each of the five $z$ values and for each of the four RFs. This involves applying the tree rules for each of the 12 observations' nuisance features and averaging the results.\n\nFor example, for RF1 at $z_1 = -1.27$:\n*   Tree 1: $z_1 < -0.5$, prediction is always $0.2$. Average is $0.2$.\n*   Tree 2: $z_1 < 0$. Prediction depends on $v_1$. It is $0.35$ for the 7 subjects with $v_1 < 0.6$, and $0.45$ for the 5 subjects with $v_1 \\ge 0.6$. Average is $(7 \\times 0.35 + 5 \\times 0.45) / 12 = 4.7/12$.\n*   Tree 3: $z_1 < 0.3$. Prediction depends on $v_2$. It is $0.4$ for the 6 subjects with $v_2 < 0.5$, and $0.5$ for the 6 subjects with $v_2 \\ge 0.5$. Average is $(6 \\times 0.4 + 6 \\times 0.5) / 12 = 5.4/12$.\n*   RF1 prediction at $z_1$: $\\frac{1}{3} \\left( 0.2 + \\frac{4.7}{12} + \\frac{5.4}{12} \\right) = \\frac{12.5}{36} \\approx 0.3472$.\n\nRepeating this process for all RFs and all $z$ values yields the PD sequences.\n\n**Step 3: Analyze Patterns**\n*   **RF 1:** The PD values are $[0.3472, 0.3472, 0.5472, 0.7472, 0.7472]$. This sequence is non-decreasing, so it is **monotonic**. It is not U-shaped.\n*   **RF 2:** The PD values are $[0.7528, 0.7528, 0.5528, 0.3528, 0.3528]$. This sequence is non-increasing, so it is **monotonic**. It is not U-shaped.\n*   **RF 3:** The PD values are $[0.7667, 0.5736, 0.2653, 0.5736, 0.7667]$. The sequence is not monotonic. The minimum is the third value ($0.2653$). The endpoints are significantly higher ($0.7667 - 0.2653 \\approx 0.5 > 0.05$). The sequence decreases to the minimum and increases after. Thus, it is **U-shaped**.\n*   **RF 4:** The trees in this RF do not use feature $x_j$. The PD value is constant: $[0.5144, 0.5144, 0.5144, 0.5144, 0.5144]$. This sequence is both non-decreasing and non-increasing, so it is **monotonic**. It is not U-shaped as the endpoints are not significantly higher than the minimum.\n\nThese analyses are combined to produce the final output string.",
            "answer": "[[[0.3472,0.3472,0.5472,0.7472,0.7472],[true,false]],[[0.7528,0.7528,0.5528,0.3528,0.3528],[true,false]],[[0.7667,0.5736,0.2653,0.5736,0.7667],[false,true]],[[0.5144,0.5144,0.5144,0.5144,0.5144],[true,false]]]"
        }
    ]
}