{
    "hands_on_practices": [
        {
            "introduction": "要真正理解支持向量机（SVM）的工作原理，最好的方法莫过于亲手实现其核心算法。本实践将指导您使用次梯度下降法来解决其原始优化问题，从而从零开始构建一个线性SVM分类器。通过这个练习，您将揭示模型训练的神秘面纱，直观地看到模型如何通过迭代优化，在最小化分类误差和最大化间隔之间找到平衡，最终学习到一个决策边界。",
            "id": "2435452",
            "problem": "要求您从经验风险最小化的第一性原理出发，构建一个线性二元分类器，使用支持向量机（SVM）来预测一个家庭是否会拖欠其学生贷款。所期望的分类器必须是一个带截距项的线性软间隔支持向量机（SVM），通过对带有 Tikhonov 正则化的、基于间隔的凸损失函数进行次梯度下降来训练，并且必须在标准化的特征上运行。目标是将这个经济-金融分类任务转化为一个适定的优化问题，然后再转化为一个可以作为可运行程序实现的算法。\n\n从以下基本基础开始：\n- 对带截距项的线性决策函数进行经验风险最小化，该函数将特征向量 $x \\in \\mathbb{R}^d$ 映射到一个带符号的分数。\n- 一个基于间隔的凸损失函数，用于惩罚对期望间隔的违反，并结合对权重向量的 Tikhonov（岭）正则化，以控制复杂度和提高泛化能力。\n- 次梯度下降法，作为一种用于最小化凸的、可能不可微的目标函数的原则性算法。\n\n实现一个程序，该程序：\n- 在每个提供的训练集上训练一个线性软间隔支持向量机（SVM）分类器。\n- 仅使用训练数据，通过逐个特征的 $z$-score 对特征进行标准化：对于每个特征，减去训练均值并除以训练标准差（为避免除以零，对任何为零的标准差使用 $1$）。\n- 使用次梯度下降法，在铰链式（hinge-type）基于间隔的损失下最小化正则化经验风险，并采用用户指定的学习率调度和迭代次数。\n- 在对每个案例进行训练后，通过学习到的线性分数的符号来预测指定测试家庭的类别标签。使用 $\\{-1,+1\\}$ 中的标签，其中 $+1$ 表示违约，$-1$ 表示不违约。\n\n测试套件。共有三个案例。在每个案例中，输入包括一个训练矩阵 $X^{(k)}$（行代表家庭，列代表特征），一个标签向量 $y^{(k)} \\in \\{-1,+1\\}^{n_k}$，一个测试向量 $x_\\star^{(k)}$，以及超参数：正则化系数 $C$、迭代次数 $T$、基础步长 $\\eta_0$ 和用于学习率调度 $\\eta_t = \\eta_0/(1+\\lambda t)$ 的步长衰减 $\\lambda$。\n\n案例 $1$（可分，混合教育/金融/人口统计学特征）：\n- 特征（列）：受教育年限、家庭收入（千美元）、学生贷款余额（千美元）、信用利用率（无单位分数）、年龄。\n- 训练矩阵 $X^{(1)}$（形状 $6 \\times 5$）：\n  [\n  [$16$, $120$, $10$, $0.1$, $30$],\n  [$16$, $40$, $50$, $0.8$, $24$],\n  [$14$, $60$, $20$, $0.3$, $28$],\n  [$12$, $35$, $45$, $0.9$, $22$],\n  [$18$, $150$, $20$, $0.1$, $33$],\n  [$12$, $25$, $60$, $0.95$, $21$]\n  ]\n- 标签 $y^{(1)}$: [$-1$, $+1$, $-1$, $+1$, $-1$, $+1$]\n- 测试向量 $x_\\star^{(1)}$: [$15$, $55$, $35$, $0.6$, $25$]\n- 超参数：$C = 10.0$，$T = 2000$，$\\eta_0 = 0.5$，$\\lambda = 10^{-3}$。\n\n案例 $2$（不可分，重叠的金融/教育指标）：\n- 特征（列）：家庭收入（千美元）、学生贷款余额（千美元）、平均绩点 (GPA)（无单位）、研究生学位指示符（二进制 $0$ 或 $1$）。\n- 训练矩阵 $X^{(2)}$（形状 $8 \\times 4$）：\n  [\n  [$70$, $30$, $3.0$, $0$],\n  [$50$, $45$, $2.6$, $0$],\n  [$65$, $40$, $3.2$, $1$],\n  [$55$, $50$, $3.0$, $0$],\n  [$80$, $20$, $3.8$, $1$],\n  [$60$, $35$, $2.8$, $0$],\n  [$75$, $25$, $3.1$, $1$],\n  [$58$, $42$, $3.4$, $0$]\n  ]\n- 标签 $y^{(2)}$: [$-1$, $+1$, $-1$, $+1$, $-1$, $+1$, $-1$, $+1$]\n- 测试向量 $x_\\star^{(2)}$: [$62$, $38$, $3.0$, $0$]\n- 超参数：$C = 5.0$，$T = 2500$，$\\eta_0 = 0.4$，$\\lambda = 5 \\times 10^{-4}$。\n\n案例 $3$（特征缩放边缘案例，异构的金融/人口统计学尺度）：\n- 特征（列）：家庭收入（美元）、年龄、受抚养人数、FICO信用评分、毕业后月数。\n- 训练矩阵 $X^{(3)}$（形状 $6 \\times 5$）：\n  [\n  [$120000$, $34$, $0$, $780$, $12$],\n  [$45000$, $23$, $1$, $620$, $6$],\n  [$80000$, $29$, $2$, $700$, $24$],\n  [$35000$, $25$, $0$, $600$, $3$],\n  [$150000$, $40$, $3$, $800$, $60$],\n  [$40000$, $27$, $2$, $580$, $8$]\n  ]\n- 标签 $y^{(3)}$: [$-1$, $+1$, $-1$, $+1$, $-1$, $+1$]\n- 测试向量 $x_\\star^{(3)}$: [$65000$, $26$, $1$, $680$, $10$]\n- 超参数：$C = 8.0$，$T = 3000$，$\\eta_0 = 0.3$，$\\lambda = 10^{-4}$。\n\n算法要求：\n- 在带有铰链式（hinge-type）基于间隔损失的正则化经验风险上实现次梯度下降，同时更新权重向量和截距项。\n- 对迭代 $t \\in \\{0,1,\\dots,T-1\\}$ 使用指定的学习率调度 $\\eta_t = \\eta_0/(1+\\lambda t)$。\n- 将权重向量和截距项初始化为零。\n- 仅使用训练数据对特征进行标准化，并将相同的参数应用于测试向量。\n\n最终输出格式：\n- 对于每个案例 $k \\in \\{1,2,3\\}$，输出相应测试家庭的预测标签 $\\hat{y}_\\star^{(k)} \\in \\{-1,+1\\}$。\n- 您的程序应生成单行输出，其中包含这三个预测值，格式为用方括号括起来的逗号分隔列表，例如：\"[$\\hat{y}_\\star^{(1)}$,$\\hat{y}_\\star^{(2)}$,$\\hat{y}_\\star^{(3)}$]\"。打印的输出必须只有一行，不含其他文本。",
            "solution": "用户要求从第一性原理出发，设计并实现一个线性软间隔支持向量机（SVM）分类器。该问题是在计算经济学和金融学的背景下提出的，具体是预测学生贷款违约。我的分析分两个阶段进行：首先，对问题陈述进行严格验证；其次，基于既定的科学原理开发解决方案。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n- **模型类型**：带截距项的线性软间隔支持向量机。\n- **任务**：家庭学生贷款违约的二元分类（$y \\in \\{-1, +1\\}$）。\n- **理论基础**：基于线性决策函数的经验风险最小化（ERM）。\n- **决策函数**：$f(x) = w^\\top x + b$，将特征 $x \\in \\mathbb{R}^d$ 映射到一个分数。\n- **目标函数组成部分**：对权重向量 $w$ 的 Tikhonov（岭）正则化，以及一个基于间隔的凸损失（铰链损失）。\n- **优化算法**：次梯度下降法。\n- **预处理**：基于训练数据，对每个特征进行 z-score 标准化。标准差为零的情况通过除以 $1$ 来处理。\n- **初始化**：权重向量 $w$ 和截距项 $b$ 初始化为零。\n- **学习率调度**：对于迭代 $t \\in \\{0, 1, \\dots, T-1\\}$，$\\eta_t = \\eta_0 / (1 + \\lambda t)$。\n- **预测规则**：$\\hat{y}_\\star = \\text{sign}(w^\\top x_\\star + b)$。标签 $+1$ 代表违约，$-1$ 代表不违约。\n- **测试案例**：\n    - **案例 1**：\n        - $X^{(1)}$（训练数据，$6 \\times 5$ 矩阵），$y^{(1)}$（训练标签，长度为 $6$），$x_\\star^{(1)}$（测试向量，长度为 $5$）。\n        - 超参数：$C = 10.0$，$T = 2000$，$\\eta_0 = 0.5$，$\\lambda = 10^{-3}$。\n    - **案例 2**：\n        - $X^{(2)}$（训练数据，$8 \\times 4$ 矩阵），$y^{(2)}$（训练标签，长度为 $8$），$x_\\star^{(2)}$（测试向量，长度为 $4$）。\n        - 超参数：$C = 5.0$，$T = 2500$，$\\eta_0 = 0.4$，$\\lambda = 5 \\times 10^{-4}$。\n    - **案例 3**：\n        - $X^{(3)}$（训练数据，$6 \\times 5$ 矩阵），$y^{(3)}$（训练标签，长度为 $6$），$x_\\star^{(3)}$（测试向量，长度为 $5$）。\n        - 超参数：$C = 8.0$，$T = 3000$，$\\eta_0 = 0.3$，$\\lambda = 10^{-4}$。\n- **输出格式**：单行打印一个用方括号括起来的、逗号分隔的三个预测标签列表。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n该问题经过了严格的科学和结构验证。\n\n- **科学基础扎实**：该问题牢固地植根于统计学习理论和凸优化的既定原则。经验风险最小化、Tikhonov 正则化、铰链损失和次梯度下降是现代机器学习的经典组成部分。它们在信用风险建模中的应用是计算金融学中一个标准且被充分理解的任务。该问题遵循基本的数学和统计逻辑。\n- **适定性**：该问题是适定的。目标函数是一个严格凸函数（$\\frac{1}{2}\\|w\\|^2$）和一个凸函数（铰链损失之和）的和，因此是严格凸的。这保证了存在唯一的最小化器 $(w^*, b^*)$。次梯度下降算法，若其学习率调度满足 $\\sum_{t=0}^{\\infty} \\eta_t = \\infty$ 和 $\\sum_{t=0}^{\\infty} \\eta_t^2  \\infty$ 的条件（指定的调度 $\\eta_t \\propto 1/t$ 满足这些条件），则保证收敛到这个唯一最小值。\n- **客观性**：该问题以精确、无偏和形式化的数学语言陈述，不含任何主观或推测性内容。\n- **完整性与一致性**：所有必要的组成部分均已提供：数据集、标签、超参数、算法规范（初始化、学习率）和预处理步骤。没有内部矛盾。超参数 $C$ 的使用与软间隔 SVM 的标准原始形式一致。\n- **真实性**：所提供的数据集虽然规模较小，但代表了家庭合理的经济和人口统计学特征。尺度的异质性（例如，案例 3 中以美元计的收入与受抚养人数）正确地说明了进行特征标准化的必要性。\n\n**步骤 3：结论与行动**\n\n问题陈述是**有效的**。它在科学上是合理的，是适定的、客观的、完整的，并且内部一致。它代表了机器学习中一个标准的、非平凡的问题。我现在将着手提出一个有原则的解决方案。\n\n### 解决方案推导\n\n目标是从第一性原理出发，推导并实现一个线性软间隔 SVM 分类器。\n\n**1. 模型和目标函数**\n\n我们从正则化经验风险最小化原则开始。分类器是输入特征 $x \\in \\mathbb{R}^d$ 的一个线性函数：\n$$\nf(x; w, b) = w^\\top x + b\n$$\n其中 $w \\in \\mathbb{R}^d$ 是权重向量，$b \\in \\mathbb{R}$ 是截距项。预测类别由 $\\hat{y} = \\text{sign}(f(x; w, b))$ 给出。我们采用约定，即当 $z \\ge 0$ 时 $\\text{sign}(z) = +1$，当 $z  0$ 时 $\\text{sign}(z) = -1$。\n\n目标是找到参数 $(w, b)$，以最小化训练数据 $\\{(x_i, y_i)\\}_{i=1}^n$ 上的经验损失与一个控制模型复杂度的正则化项的组合。对于 SVM，标准选择是铰链损失，它惩罚未能达到至少为 $1$ 的间隔的预测：\n$$\nL_{\\text{hinge}}(y_i, f(x_i)) = \\max(0, 1 - y_i f(x_i)) = \\max(0, 1 - y_i(w^\\top x_i + b))\n$$\n该损失是非凸的 0-1 分类误差的一个凸上界。\n\n为了防止过拟合和提高泛化能力，我们对权重向量添加一个 Tikhonov（$\\ell_2$）正则化惩罚项，$\\frac{1}{2}\\|w\\|^2 = \\frac{1}{2} w^\\top w$。截距项 $b$ 通常不进行正则化。\n\n将这些部分组合起来，我们得到软间隔 SVM 的原始目标函数：\n$$\nJ(w, b) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\max(0, 1 - y_i(w^\\top x_i + b))\n$$\n超参数 $C > 0$ 控制着权衡：小的 $C$ 倾向于更大的间隔，代价是允许一些违反间隔的情况；而大的 $C$ 则更重地惩罚违规行为，导致间隔更小。最小化这个目标函数是一个凸优化问题。\n\n**2. 通过次梯度下降进行优化**\n\n铰链损失项使得目标函数 $J(w, b)$ 在 $y_i(w^\\top x_i + b) = 1$ 的地方不可微。因此，我们使用次梯度下降法。次梯度是梯度对不可微凸函数的推广。\n\n$J(w, b)$ 关于 $w$ 和 $b$ 的次梯度是可微正则化项的梯度与不可微铰链损失项的次梯度之和。对于单个数据点 $(x_i, y_i)$，铰链损失的次梯度为：\n$$\n\\partial \\max(0, 1 - y_i(w^\\top x_i + b)) =\n\\begin{cases}\n    \\{ \\mathbf{0} \\}  \\text{if } y_i(w^\\top x_i + b) > 1 \\\\\n    \\{ -y_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} \\}  \\text{if } y_i(w^\\top x_i + b)  1 \\\\\n    \\{ -\\alpha y_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} \\mid \\alpha \\in [0, 1] \\}  \\text{if } y_i(w^\\top x_i + b) = 1\n\\end{cases}\n$$\n对于算法实现，我们可以从次梯度集中选择任何一个元素。一个常规选择是，如果 $y_i(w^\\top x_i + b) \\le 1$，则将次梯度取为 $-y_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix}$，否则取为 $\\mathbf{0}$。然而，问题陈述仅在条件中提供了严格不等式，我们将遵循这一点。设 $\\mathcal{S}_t = \\{i \\mid y_i(w^{(t)\\top} x_i + b^{(t)})  1\\}$ 是在迭代 $t$ 时违反间隔条件的训练样本的索引集。\n\n完整目标函数 $J(w, b)$ 的次梯度为：\n$$\ng_w = \\frac{\\partial J}{\\partial w} = w - C \\sum_{i \\in \\mathcal{S}_t} y_i x_i\n$$\n$$\ng_b = \\frac{\\partial J}{\\partial b} = -C \\sum_{i \\in \\mathcal{S}_t} y_i\n$$\n迭代 $t$ 的次梯度下降更新规则为：\n$$\nw^{(t+1)} \\leftarrow w^{(t)} - \\eta_t g_w\n$$\n$$\nb^{(t+1)} \\leftarrow b^{(t)} - \\eta_t g_b\n$$\n学习率 $\\eta_t$ 由调度 $\\eta_t = \\eta_0 / (1 + \\lambda t)$ 给出。\n\n**3. 特征标准化**\n\n所提供的特征具有差异巨大的尺度（例如，以千美元计的收入与无单位的比率）。SVM 对特征尺度很敏感，因为目标函数涉及点积和范数。为确保所有特征都能适当地贡献，我们使用 z-score 变换对它们进行标准化。对于每个特征 $j \\in \\{1, \\dots, d\\}$，我们从训练数据中计算其均值 $\\mu_j$ 和标准差 $\\sigma_j$。\n$$\n\\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij} \\quad , \\quad \\sigma_j^2 = \\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2\n$$\n标准化的特征 $x'_{ij}$ 随后为：\n$$\nx'_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j'}\n$$\n其中，如果 $\\sigma_j > 0$，则 $\\sigma_j' = \\sigma_j$；如果 $\\sigma_j = 0$，则 $\\sigma_j' = 1$。必须使用从训练集派生的相同参数 $(\\mu_j, \\sigma_j')$ 对测试向量 $x_\\star$ 进行变换，然后再进行预测。\n\n该过程为训练 SVM 分类器和进行预测提供了一个有原则且完整的算法，我们现在将其实现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the SVM classification problem for all test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X_train\": np.array([\n                [16.0, 120.0, 10.0, 0.1, 30.0],\n                [16.0, 40.0, 50.0, 0.8, 24.0],\n                [14.0, 60.0, 20.0, 0.3, 28.0],\n                [12.0, 35.0, 45.0, 0.9, 22.0],\n                [18.0, 150.0, 20.0, 0.1, 33.0],\n                [12.0, 25.0, 60.0, 0.95, 21.0]\n            ]),\n            \"y_train\": np.array([-1, 1, -1, 1, -1, 1]),\n            \"x_test\": np.array([15.0, 55.0, 35.0, 0.6, 25.0]),\n            \"C\": 10.0,\n            \"T\": 2000,\n            \"eta0\": 0.5,\n            \"lmbda\": 1e-3\n        },\n        {\n            \"X_train\": np.array([\n                [70.0, 30.0, 3.0, 0.0],\n                [50.0, 45.0, 2.6, 0.0],\n                [65.0, 40.0, 3.2, 1.0],\n                [55.0, 50.0, 3.0, 0.0],\n                [80.0, 20.0, 3.8, 1.0],\n                [60.0, 35.0, 2.8, 0.0],\n                [75.0, 25.0, 3.1, 1.0],\n                [58.0, 42.0, 3.4, 0.0]\n            ]),\n            \"y_train\": np.array([-1, 1, -1, 1, -1, 1, -1, 1]),\n            \"x_test\": np.array([62.0, 38.0, 3.0, 0.0]),\n            \"C\": 5.0,\n            \"T\": 2500,\n            \"eta0\": 0.4,\n            \"lmbda\": 5e-4\n        },\n        {\n            \"X_train\": np.array([\n                [120000.0, 34.0, 0.0, 780.0, 12.0],\n                [45000.0, 23.0, 1.0, 620.0, 6.0],\n                [80000.0, 29.0, 2.0, 700.0, 24.0],\n                [35000.0, 25.0, 0.0, 600.0, 3.0],\n                [150000.0, 40.0, 3.0, 800.0, 60.0],\n                [40000.0, 27.0, 2.0, 580.0, 8.0]\n            ]),\n            \"y_train\": np.array([-1, 1, -1, 1, -1, 1]),\n            \"x_test\": np.array([65000.0, 26.0, 1.0, 680.0, 10.0]),\n            \"C\": 8.0,\n            \"T\": 3000,\n            \"eta0\": 0.3,\n            \"lmbda\": 1e-4\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X_train, y_train, x_test = case[\"X_train\"], case[\"y_train\"], case[\"x_test\"]\n        C, T, eta0, lmbda = case[\"C\"], case[\"T\"], case[\"eta0\"], case[\"lmbda\"]\n\n        # 1. Feature Standardization\n        # Using ddof=0 for population standard deviation, consistent with the formula.\n        train_mean = np.mean(X_train, axis=0)\n        train_std = np.std(X_train, axis=0, ddof=0)\n        train_std[train_std == 0] = 1.0  # Avoid division by zero\n\n        X_train_std = (X_train - train_mean) / train_std\n        x_test_std = (x_test - train_mean) / train_std\n\n        n_samples, n_features = X_train.shape\n\n        # 2. Subgradient Descent for SVM\n        # Initialize parameters\n        w = np.zeros(n_features)\n        b = 0.0\n\n        for t in range(T):\n            # Calculate learning rate\n            eta_t = eta0 / (1.0 + lmbda * t)\n\n            # Calculate scores and identify margin violators\n            scores = y_train * (X_train_std @ w + b)\n            violators_mask = scores  1.0\n            \n            # Compute subgradients\n            # Subgradient for w\n            # The sum is over samples that violate the margin condition\n            if np.any(violators_mask):\n                dw = w - C * np.sum(y_train[violators_mask, np.newaxis] * X_train_std[violators_mask], axis=0)\n                db = -C * np.sum(y_train[violators_mask])\n            else:\n                dw = w\n                db = 0.0\n\n            # Update parameters\n            w -= eta_t * dw\n            b -= eta_t * db\n\n        # 3. Prediction\n        test_score = x_test_std @ w + b\n        prediction = 1 if test_score >= 0 else -1\n        results.append(prediction)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了基本原理之后，让我们探索一种更强大、更高效的SVM训练方法。本实践将深入到SVM的对偶问题，并指导您实现一种内点法（Interior Point Method）来求解其二次规划（QP）形式。这个练习不仅能加深您对SVM与凸优化之间深刻联系的理解，也为掌握“核技巧”的精髓奠定了坚实的基础。",
            "id": "3242644",
            "problem": "您的任务是实现一个完整、可运行的程序，该程序使用内点法 (Interior Point Method, IPM) 构建并求解一系列二次规划 (Quadratic Programming, QP) 问题，并将其应用于经典支持向量机 (Support Vector Machine, SVM) 线性核二元分类问题的对偶问题。您的实现必须是自包含的，并按照下文指定的方式产生单行输出。\n\n核心优化问题是求解软间隔线性核 SVM 的对偶问题。给定训练数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和标签 $y \\in \\{-1,+1\\}^n$，对偶问题是找到一个 $ \\alpha \\in \\mathbb{R}^n $，它在一个等式约束和箱形约束的条件下，最小化严格凸二次目标函数。定义 Gram 矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 为 $K_{ij} = x_i^\\top x_j$，标签矩阵 $Y = \\operatorname{diag}(y)$，以及二次矩阵 $Q = Y K Y$。该对偶 QP 问题为\n$$\n\\begin{aligned}\n\\min_{\\alpha \\in \\mathbb{R}^n}\\quad  \\tfrac{1}{2}\\,\\alpha^\\top Q\\,\\alpha - \\mathbf{1}^\\top \\alpha \\\\\n\\text{subject to}\\quad  y^\\top \\alpha = 0, \\\\\n 0 \\le \\alpha \\le C,\n\\end{aligned}\n$$\n其中 $C \\in \\mathbb{R}$ 是一个给定的惩罚参数，满足 $C > 0$，而 $\\mathbf{1} \\in \\mathbb{R}^n$ 是全一向量。您的求解器必须实现一种基于障碍的内点法，用于求解形式为 $l \\le x \\le u$ 的带简单边界约束的等式约束 QP 问题。\n\n从凸优化的基本定义、拉格朗日最优性条件以及针对不等式约束的对数障碍法出发，实现一个可行起点障碍法。对于一个障碍参数 $\\mu > 0$，该方法最小化障碍增广目标函数\n$$\n\\phi_\\mu(\\alpha) = \\tfrac{1}{2}\\,\\alpha^\\top Q\\,\\alpha - \\mathbf{1}^\\top \\alpha - \\mu \\sum_{i=1}^n \\log(\\alpha_i - 0) - \\mu \\sum_{i=1}^n \\log(C - \\alpha_i)\n$$\n并满足等式约束 $y^\\top \\alpha = 0$。使用带等式约束的牛顿法，通过求解每个障碍参数值下的牛顿步长的 Karush–Kuhn–Tucker (KKT) 系统来计算搜索方向，并采用回溯线搜索，在确保障碍目标函数有充分下降的同时，为所有 $i \\in \\{1,\\dots,n\\}$ 保持严格可行性 $0  \\alpha_i  C$。以几何级数方式减小 $\\mu$ 直至收敛，并根据需要进行正则化以保持数值稳定性。您的算法必须在解决方案中从第一性原理出发进行论证。\n\n在求得 $\\alpha$ 后，按如下方式恢复线性核 SVM 的原始问题的分离超平面参数。计算\n$$\nw = \\sum_{i=1}^n \\alpha_i y_i x_i \\in \\mathbb{R}^d,\n$$\n并使用标准的互补松弛关系估计偏置项 $b \\in \\mathbb{R}$。如果存在索引 $i$ 使得 $0  \\alpha_i  C$（在一个小的数值容差范围内严格位于边界内部），则计算\n$$\nb = \\operatorname{mean}_{i \\,:\\, 0  \\alpha_i  C} \\left( y_i - w^\\top x_i \\right).\n$$\n如果不存在这样的 $i$，则使用基于类别间隔的鲁棒备用方案：\n$$\nb = -\\tfrac{1}{2}\\left( \\max_{i \\,:\\, y_i=-1} w^\\top x_i + \\min_{i \\,:\\, y_i=+1} w^\\top x_i \\right).\n$$\n\n对于下面的每个测试用例，使用决策规则 $\\hat{y} = \\operatorname{sign}(w^\\top x + b)$（标签在 $\\{-1,+1\\}$ 中），计算样本内分类准确率，该准确率定义为被正确分类的训练样本的比例。将每个准确率表示为一个在 $[0,1]$ 区间内的实数，四舍五入到六位小数。\n\n测试套件。请精确使用以下三个测试用例：\n\n- 用例 A（线性可分，平衡）：\n  - $X = \\begin{bmatrix}\n  2  2 \\\\\n  2  3 \\\\\n  3  2 \\\\\n  -2  -2 \\\\\n  -2  -3 \\\\\n  -3  -2\n  \\end{bmatrix}$，\n  $y = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ -1 \\\\ -1 \\\\ -1 \\end{bmatrix}$，\n  $C = 100.0$。\n- 用例 B（部分重叠，平衡）：\n  - $X = \\begin{bmatrix}\n  2  2 \\\\\n  2  0 \\\\\n  3  1 \\\\\n  3  2 \\\\\n  0  0 \\\\\n  1  0 \\\\\n  0  1 \\\\\n  1  1.5\n  \\end{bmatrix}$，\n  $y = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ +1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ -1 \\end{bmatrix}$，\n  $C = 1.0$。\n- 用例 C（线性可分，不平衡）：\n  - $X = \\begin{bmatrix}\n  2  2 \\\\\n  2  3 \\\\\n  3  2 \\\\\n  -2  -2\n  \\end{bmatrix}$，\n  $y = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ -1 \\end{bmatrix}$，\n  $C = 10.0$。\n\n最终输出格式。您的程序应产生单行输出，其中包含三个用例的结果，以逗号分隔的实数列表形式包含在一对方括号中，顺序为 A、B、C。每个数字必须四舍五入到六位小数。例如，输出必须类似于\n$$\n[\\text{acc}_A,\\text{acc}_B,\\text{acc}_C],\n$$\n其中 $\\text{acc}_A$、$\\text{acc}_B$ 和 $\\text{acc}_C$ 均为格式化为六位小数的实数。不应打印任何额外的文本或行。\n\n单位和角度单位不适用于此问题。\n\n您的实现必须是完整且无需修改即可运行的，无需任何用户输入、外部文件或网络访问。唯一允许使用的外部库是 Numerical Python (NumPy)。",
            "solution": "我们从凸优化的基本定义以及将支持向量机 (SVM) 分类器构建为凸规划的解开始。对于带有线性核的二元分类，原始软间隔 SVM 寻求一个由 $w \\in \\mathbb{R}^d$ 和 $b \\in \\mathbb{R}$ 定义的分离超平面，该超平面在间隔最大化与合页损失之间取得平衡。相应的对偶问题是一个严格凸二次规划 (QP)，带有一个线性等式约束和简单边界约束。此对偶问题对于核方法尤其方便，但在此我们专注于线性核。\n\n给定 $X \\in \\mathbb{R}^{n \\times d}$ 和标签 $y \\in \\{-1,+1\\}^n$，定义 Gram 矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 为 $K_{ij} = x_i^\\top x_j$ 以及 $Q = Y K Y$，其中 $Y = \\operatorname{diag}(y)$。带有惩罚参数 $C \\in \\mathbb{R}, C > 0$ 的软间隔对偶问题是\n$$\n\\begin{aligned}\n\\min_{\\alpha \\in \\mathbb{R}^n} \\quad  f(\\alpha) = \\tfrac{1}{2} \\alpha^\\top Q \\alpha - \\mathbf{1}^\\top \\alpha \\\\\n\\text{subject to} \\quad  y^\\top \\alpha = 0, \\\\\n 0 \\le \\alpha \\le C,\n\\end{aligned}\n$$\n其中 $\\mathbf{1} \\in \\mathbb{R}^n$ 是全一向量。原始变量中的决策函数可恢复为 $w = \\sum_{i=1}^n \\alpha_i y_i x_i$ 以及一个与 Karush–Kuhn–Tucker (KKT) 条件一致的偏置项 $b$。\n\n为求解该 QP 问题，我们使用基于对数障碍的内点法 (IPM)。不等式约束 $0 \\le \\alpha_i \\le C$ 通过给目标函数增加一个由 $\\mu > 0$ 参数化的障碍项来强制执行：\n$$\n\\phi_\\mu(\\alpha) = f(\\alpha) - \\mu \\sum_{i=1}^n \\log(\\alpha_i) - \\mu \\sum_{i=1}^n \\log(C - \\alpha_i),\n$$\n并满足等式约束 $y^\\top \\alpha = 0$。对于每个固定的 $\\mu$，我们在流形 $y^\\top \\alpha = 0$ 上使用牛顿法求解 $\\phi_\\mu(\\alpha)$ 的等式约束最小化问题。当 $\\mu \\to 0^+$ 时，解会追踪中心路径并收敛到原始 QP 问题的约束最小化解。\n\n我们从第一性原理推导牛顿步长。障碍增广目标函数的梯度是\n$$\n\\nabla \\phi_\\mu(\\alpha) = Q \\alpha - \\mathbf{1} - \\mu \\left[ \\frac{1}{\\alpha_1}, \\dots, \\frac{1}{\\alpha_n} \\right]^\\top + \\mu \\left[ \\frac{1}{C - \\alpha_1}, \\dots, \\frac{1}{C - \\alpha_n} \\right]^\\top.\n$$\n海森矩阵是\n$$\n\\nabla^2 \\phi_\\mu(\\alpha) = Q + \\operatorname{diag}\\left( \\frac{\\mu}{\\alpha_1^2} + \\frac{\\mu}{(C - \\alpha_1)^2}, \\dots, \\frac{\\mu}{\\alpha_n^2} + \\frac{\\mu}{(C - \\alpha_n)^2} \\right).\n$$\n我们通过拉格朗日乘子来实施等式约束。对于当前迭代点 $\\alpha$ 和拉格朗日乘子 $\\lambda \\in \\mathbb{R}$，牛顿步长 $(\\Delta \\alpha, \\Delta \\lambda)$ 求解线性化的 KKT 系统\n$$\n\\begin{bmatrix}\nW  y \\\\\ny^\\top  0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta \\alpha \\\\\n\\Delta \\lambda\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\ng \\\\\nr_p\n\\end{bmatrix},\n$$\n其中 $g = \\nabla \\phi_\\mu(\\alpha)$，$W = \\nabla^2 \\phi_\\mu(\\alpha)$，以及 $r_p = y^\\top \\alpha - 0$ 是原始等式残差。该系统由 $\\phi_\\mu(\\alpha)$ 的二阶泰勒展开和等式约束的线性化得出。它可以通过 Schur 补高效求解。通过求解 $W \\Delta \\alpha + y \\Delta \\lambda = -g$ 和 $y^\\top \\Delta \\alpha = -r_p$，我们得到 Schur 补方程\n$$\n(y^\\top W^{-1} y)\\, \\Delta \\lambda = -\\left( r_p + y^\\top W^{-1} g \\right),\n$$\n然后得到\n$$\n\\Delta \\alpha = - W^{-1} (g + y \\Delta \\lambda).\n$$\n因为对于 $\\mu > 0$，由于障碍函数的曲率，$W$ 是严格正定的（即使 $Q$ 只是半正定的），所以线性求解存在并且通过适度正则化可以保持数值稳定。\n\n选择步长以保持不等式约束的严格可行性，并确保障碍目标函数有充分下降。给定一个候选方向 $\\Delta \\alpha$，保持严格内部可行性的最大步长可逐分量地计算为\n$$\n\\alpha_{\\max} = \\min \\left\\{ 1,\\, 0.99 \\cdot \\min_{i:\\,\\Delta \\alpha_i  0} \\frac{\\alpha_i}{-\\Delta \\alpha_i},\\, 0.99 \\cdot \\min_{i:\\,\\Delta \\alpha_i > 0} \\frac{C - \\alpha_i}{\\Delta \\alpha_i} \\right\\}.\n$$\n然后，我们使用因子 $0  \\beta  1$ 和利用方向导数 $g^\\top \\Delta \\alpha$ 的 Armijo 条件进行回溯，以保证 $\\phi_\\mu(\\alpha)$ 的下降，同时保持可行性。\n\n初始化必须满足严格可行性和等式约束。令 $P = \\{ i \\,:\\, y_i = +1 \\}$ 和 $N = \\{ i \\,:\\, y_i = -1 \\}$。选择标量 $s, t \\in (0,1)$ 使得 $s\\,|P| = t\\,|N|$。一个能保证无论类别是否不平衡都具有严格内部性的具体选择是\n$$\ns = \\min\\!\\left(0.5,\\, 0.5 \\cdot \\frac{|N|}{|P|}\\right), \\quad t = s \\cdot \\frac{|P|}{|N|}.\n$$\n然后，对于 $i \\in P$ 设置 $\\alpha_i = C s$，对于 $j \\in N$ 设置 $\\alpha_j = C t$。这种选择对所有 $i$ 均满足 $0  \\alpha_i  C$ 和 $y^\\top \\alpha = 0$。\n\n外部障碍循环以几何级数方式减小 $\\mu$（例如，$\\mu \\leftarrow \\sigma \\mu$，其中 $\\sigma \\in (0,1)$），直到 $\\mu$ 足够小；对于每个 $\\mu$，内部等式约束牛顿法会迭代进行，直到 KKT 残差 $(g, r_p)$ 的范数低于一个容差。对 $Q$ 进行温和的对角正则化可以改善小问题的数值稳定性。\n\n收敛后，恢复原始参数 $w = \\sum_{i=1}^n \\alpha_i y_i x_i$。对于 $b$，使用 KKT 互补松弛条件。如果存在索引 $i$ 严格位于边界 $0  \\alpha_i  C$ 内部（在一个小的容差范围内），那么对于这些 $i$，我们有 $y_i (w^\\top x_i + b) \\approx 1$，这意味着 $b \\approx y_i - w^\\top x_i$。对所有可用的此类索引取平均值，可以得到一个鲁棒的估计：\n$$\nb = \\operatorname{mean}_{i \\,:\\, \\varepsilon  \\alpha_i  C - \\varepsilon} \\left( y_i - w^\\top x_i \\right).\n$$\n如果不存在这样的索引，一个鲁棒的备用方案是选择 $b$ 位于两个类别最近的投影间隔的中点：\n$$\nb = -\\tfrac{1}{2}\\left( \\max_{i \\,:\\, y_i=-1} w^\\top x_i + \\min_{i \\,:\\, y_i=+1} w^\\top x_i \\right).\n$$\n\n最后，为每个测试用例计算样本内准确率 $\\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{ \\operatorname{sign}(w^\\top x_i + b) = y_i \\}$。程序必须输出单行：一个包含三个在 $[0,1]$ 区间内的实数的列表，每个数四舍五入到六位小数，按 A、B、C 的顺序对应三个指定的测试用例。\n\n该方法将凸性、KKT 条件和对数障碍原理的核心定义整合到一个可实现的算法中：带障碍增广海森矩阵的等式约束牛顿步确保在可行域内部下降，而 Schur 补消元则利用了少数等式约束的结构。所选的初始化从第一性原理保证了严格可行性和等式满足，而 $(w,b)$ 的恢复则使用了 SVM 公式中固有的互补松弛关系。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef interior_point_qp(H, c, A, b, l, u, *,\n                      mu_start=1.0, mu_tol=1e-8, mu_decay=0.2,\n                      newton_tol=1e-8, max_newton_iter=60,\n                      backtrack_beta=0.5, armijo_c1=1e-4, ridge=1e-10):\n    \"\"\"\n    Solve: minimize 0.5 x^T H x + c^T x\n           subject to A x = b, l = x = u\n    using a log-barrier interior point method with equality-constrained Newton steps.\n\n    Parameters:\n        H: (n,n) symmetric positive semidefinite\n        c: (n,)\n        A: (m,n) equality constraints\n        b: (m,)\n        l, u: (n,) lower and upper bounds with l = u elementwise\n    Returns:\n        x: (n,) approximate solution\n    \"\"\"\n    n = H.shape[0]\n    # Symmetrize H and add small ridge for numerical stability\n    H = 0.5 * (H + H.T) + ridge * np.eye(n)\n\n    # Build strictly feasible starting point satisfying A x = b and l  x  u\n    # Here A has shape (m, n). We'll construct x0 using a problem-specific strategy when m=1,\n    # otherwise project a central point to the equality manifold with line search for feasibility.\n    m = A.shape[0]\n    # Start from midpoint\n    x = 0.5 * (l + u)\n\n    # Try to adjust to equality feasibility while staying interior\n    # Solve min ||x - x_mid|| s.t. A x = b => x = x_mid + A^T (A A^T)^{-1} (b - A x_mid)\n    # Then shrink towards midpoint if needed to maintain strict interior.\n    try:\n        At = A\n        M = At @ At.T\n        # Regularize M if near singular\n        if M.ndim == 0:\n            M = np.array([[M + 1e-12]])\n        else:\n            M = M + 1e-12 * np.eye(M.shape[0])\n        adjustment = At.T @ np.linalg.solve(M, (b - A @ x))\n        x_candidate = x + adjustment\n        # Ensure strict interior via convex combination with midpoint\n        t = 0.0\n        # If x_candidate violates bounds, pull it towards midpoint\n        while True:\n            xc = (1 - t) * x_candidate + t * x  # t increases towards midpoint\n            if np.all(xc > l) and np.all(xc  u) and np.allclose(A @ xc, b, atol=1e-10):\n                x = xc\n                break\n            t = min(1.0, t + 0.05)\n            if t >= 1.0:\n                # Fallback: stay at midpoint and enforce equality numerically later\n                x = (x + 0.0)\n                break\n    except np.linalg.LinAlgError:\n        pass\n\n    # Ensure strict interiority\n    eps = 1e-6\n    x = np.minimum(np.maximum(x, l + eps * (u - l)), u - eps * (u - l))\n\n    mu = mu_start\n\n    def barrier_grad_hess(x, mu):\n        # gradient g = H x + c - mu/(x-l) + mu/(u-x)\n        inv_xl = 1.0 / (x - l)\n        inv_ux = 1.0 / (u - x)\n        g = H @ x + c - mu * inv_xl + mu * inv_ux\n        # Hessian W = H + diag(mu/(x-l)^2 + mu/(u-x)^2)\n        diag_terms = mu * (inv_xl**2 + inv_ux**2)\n        W = H + np.diag(diag_terms)\n        return g, W\n\n    def phi(x, mu):\n        # barrier objective\n        if np.any(x = l) or np.any(x >= u):\n            return np.inf\n        return 0.5 * float(x.T @ H @ x) + float(c.T @ x) - mu * np.sum(np.log(x - l)) - mu * np.sum(np.log(u - x))\n\n    # Newton solve for each mu\n    while mu > mu_tol:\n        # Newton iterations\n        for _ in range(max_newton_iter):\n            g, W = barrier_grad_hess(x, mu)\n            r_p = A @ x - b  # primal residual\n            # Solve KKT system via Schur complement:\n            # W dx + A^T lam = -g\n            # A dx = -r_p\n            # Schur: (A W^{-1} A^T) lam = -(r_p + A W^{-1} g)\n            try:\n                Winv_A_T = np.linalg.solve(W, A.T)\n                S = A @ Winv_A_T  # (m,m)\n                rhs = -(r_p + A @ np.linalg.solve(W, g))\n                # Solve for lambda\n                lam = np.linalg.solve(S, rhs)\n                # Then dx\n                dx = -np.linalg.solve(W, g + A.T @ lam)\n            except np.linalg.LinAlgError:\n                # Add additional ridge and retry\n                W = W + 1e-8 * np.eye(n)\n                Winv_A_T = np.linalg.solve(W, A.T)\n                S = A @ Winv_A_T\n                rhs = -(r_p + A @ np.linalg.solve(W, g))\n                lam = np.linalg.solve(S, rhs)\n                dx = -np.linalg.solve(W, g + A.T @ lam)\n\n            # Check convergence of KKT residual\n            res_norm = np.sqrt(np.linalg.norm(g, 2)**2 + np.linalg.norm(r_p, 2)**2)\n            if res_norm  newton_tol:\n                break\n\n            # Compute maximum feasible step to remain strictly inside bounds\n            alpha = 1.0\n            with np.errstate(divide='ignore', invalid='ignore'):\n                idx_pos = dx > 0\n                if np.any(idx_pos):\n                    alpha = min(alpha, 0.99 * np.min((u[idx_pos] - x[idx_pos]) / dx[idx_pos]))\n                idx_neg = dx  0\n                if np.any(idx_neg):\n                    alpha = min(alpha, 0.99 * np.min((x[idx_neg] - l[idx_neg]) / (-dx[idx_neg])))\n            if not np.isfinite(alpha) or alpha = 0:\n                alpha = 0.5\n\n            # Backtracking line search for sufficient decrease\n            phi_x = phi(x, mu)\n            slope = g @ dx\n            t = alpha\n            while True:\n                x_new = x + t * dx\n                if np.any(x_new = l) or np.any(x_new >= u):\n                    t *= backtrack_beta\n                else:\n                    phi_new = phi(x_new, mu)\n                    if phi_new = phi_x + armijo_c1 * t * slope:\n                        break\n                    t *= backtrack_beta\n                if t  1e-16:\n                    break\n            x = x + t * dx\n\n        # Reduce barrier parameter\n        mu *= mu_decay\n\n    return x\n\ndef svm_linear_dual_ipm(X, y, C):\n    \"\"\"\n    Train a soft-margin linear SVM via its dual QP using an interior point method.\n    Returns: (w, b, alpha)\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n, d = X.shape\n\n    # Build Q = Y K Y with K = X X^T\n    K = X @ X.T\n    Y = np.diag(y)\n    Q = Y @ K @ Y\n\n    # c = -1, A = y^T, b = 0, l = 0, u = C\n    c = -np.ones(n)\n    A = y.reshape(1, -1)\n    b = np.array([0.0])\n    l = np.zeros(n)\n    u = C * np.ones(n)\n\n    # Strictly feasible initializer satisfying A x = b and l  x  u\n    # Construct x0 with class-balanced interior alphas\n    pos_idx = np.where(y > 0)[0]\n    neg_idx = np.where(y  0)[0]\n    if len(pos_idx) == 0 or len(neg_idx) == 0:\n        raise ValueError(\"Both classes must be present.\")\n    s = min(0.5, 0.5 * len(neg_idx) / max(1, len(pos_idx)))\n    t = s * len(pos_idx) / len(neg_idx)\n    alpha0 = np.zeros(n)\n    alpha0[pos_idx] = C * s\n    alpha0[neg_idx] = C * t\n\n    # Solve QP via interior point\n    alpha = interior_point_qp(Q, c, A, b, l, u,\n                              mu_start=1.0, mu_tol=1e-9, mu_decay=0.2,\n                              newton_tol=1e-9, max_newton_iter=80,\n                              backtrack_beta=0.5, armijo_c1=1e-4, ridge=1e-10)\n\n    # Recover w\n    w = (alpha * y) @ X\n\n    # Recover b\n    tol = 1e-6\n    support_mask = (alpha > tol)  (alpha  C - tol)\n    scores = X @ w\n    if np.any(support_mask):\n        b_vals = y[support_mask] - scores[support_mask]\n        b = float(np.mean(b_vals))\n    else:\n        # Fallback: midpoint between class projections\n        scores_pos = scores[y > 0]\n        scores_neg = scores[y  0]\n        if len(scores_pos) == 0 or len(scores_neg) == 0:\n            b = 0.0\n        else:\n            b = -0.5 * (np.max(scores_neg) + np.min(scores_pos))\n\n    return w, b, alpha\n\ndef accuracy_score(y_true, y_pred):\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    return float(np.mean(y_true == y_pred))\n\ndef run_case(X, y, C):\n    w, b, alpha = svm_linear_dual_ipm(X, y, C)\n    y_pred = np.sign(X @ w + b)\n    # Map zeros to +1 to avoid ambiguity\n    y_pred[y_pred == 0] = 1.0\n    return accuracy_score(y, y_pred)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    case_A_X = np.array([\n        [2.0, 2.0],\n        [2.0, 3.0],\n        [3.0, 2.0],\n        [-2.0, -2.0],\n        [-2.0, -3.0],\n        [-3.0, -2.0],\n    ], dtype=float)\n    case_A_y = np.array([+1.0, +1.0, +1.0, -1.0, -1.0, -1.0], dtype=float)\n    case_A_C = 100.0\n\n    case_B_X = np.array([\n        [2.0, 2.0],\n        [2.0, 0.0],\n        [3.0, 1.0],\n        [3.0, 2.0],\n        [0.0, 0.0],\n        [1.0, 0.0],\n        [0.0, 1.0],\n        [1.0, 1.5],\n    ], dtype=float)\n    case_B_y = np.array([+1.0, +1.0, +1.0, +1.0, -1.0, -1.0, -1.0, -1.0], dtype=float)\n    case_B_C = 1.0\n\n    case_C_X = np.array([\n        [2.0, 2.0],\n        [2.0, 3.0],\n        [3.0, 2.0],\n        [-2.0, -2.0],\n    ], dtype=float)\n    case_C_y = np.array([+1.0, +1.0, +1.0, -1.0], dtype=float)\n    case_C_C = 10.0\n\n    test_cases = [\n        (case_A_X, case_A_y, case_A_C),\n        (case_B_X, case_B_y, case_B_C),\n        (case_C_X, case_C_y, case_C_C),\n    ]\n\n    results = []\n    for X, y, C in test_cases:\n        acc = run_case(X, y, C)\n        results.append(f\"{acc:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "一个训练好的模型，如果缺少可靠的性能评估，其价值将大打折扣。最后一个实践将聚焦于模型评估这一关键任务，尤其是在临床研究中的重要性。您将学习如何计算受试者工作特征曲线下面积（AUC），更重要的是，使用非参数的DeLong方法来估算其置信区间。这项技能在放射组学等领域至关重要，它能够量化模型预测能力的不确定性，并为判断其临床价值提供依据。",
            "id": "4562067",
            "problem": "给定从基于影像组学特征训练的支持向量机 (SVM) 分类器中得到的预测决策函数得分，以及真实的二元结果标签。您的任务是计算受试者工作特征曲线下面积 (AUC) 和一个双侧置信区间，需要使用 DeLong 方法，并从 AUC 的概率解释和成对比较的无偏估计量理论出发。然后，您必须根据一个指定的判别阈值来解读该分类器的性能是否具有临床显著性。仅使用 AUC 作为概率、无偏样本方差和正态分位数的基础定义。除这些基础之外，不要使用任何快捷方式或预封装的例程。\n\n定义与要求：\n\n- 设正类分数的集合为 $\\mathbf{s}^+ = \\{s^+_1, s^+_2, \\dots, s^+_m\\}$，负类分数的集合为 $\\mathbf{s}^- = \\{s^-_1, s^-_2, \\dots, s^-_n\\}$。AUC 定义为一个随机选择的正类分数超过一个随机选择的负类分数的概率。具体来说，设\n$$\n\\psi(a,b) = \n\\begin{cases}\n1,  \\text{if } a > b,\\\\\n0,  \\text{if } a  b,\\\\\n\\frac{1}{2},  \\text{if } a = b,\n\\end{cases}\n$$\n则经验 AUC 为\n$$\n\\widehat{\\theta} = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\psi\\!\\left(s^+_i, s^-_j\\right).\n$$\n- 使用基于 $U$ 统计量理论的 DeLong 方法来估计 $\\widehat{\\theta}$ 的方差，无需参数假设。据此，通过正态近似构建一个双侧 $(1-\\alpha)$ 置信区间。具体而言，使用标准正态分布的临界值 $z_{1-\\alpha/2}$，并返回截断在区间 $[0,1]$ 内的界限。\n- 为了在影像组学中具有临床可解释性，定义一个判别阈值 $\\tau$（一个在 $[0,1]$ 内的浮点数）。如果置信下限大于或等于 $\\tau$，则声明该分类器“具有临床显著性”。将此决策表示为一个布尔值。\n\n实现约束：\n\n- 直接根据上述定义实现所有计算，包括成对比较函数 $\\psi$、无偏样本方差和标准正态分位数。\n- 将所有报告的浮点数结果（AUC 和置信界限）四舍五入到六位小数。\n- 不涉及物理单位。以小数形式报告任何分数数量。\n\n测试套件：\n\n计算并报告以下四个测试用例中每一个的结果。在每个用例中，都给定了 $\\mathbf{s}^+$、$\\mathbf{s}^-$、一个置信水平 $\\gamma$（因此 $\\alpha = 1-\\gamma$）和一个临床阈值 $\\tau$。\n\n- 用例 A（具有中等判别能力的典型影像组学 SVM）：\n  - $\\mathbf{s}^+ = [1.8, 1.2, 0.9, 1.5, 1.1, 1.7, 0.8, 1.3, 0.95, 1.6]$\n  - $\\mathbf{s}^- = [-0.4, -0.1, 0.2, -0.6, 0.0, -0.2, 0.1, -0.3, 0.05, -0.5]$\n  - $\\gamma = 0.95$\n  - $\\tau = 0.75$\n- 用例 B（近乎完美的分离）：\n  - $\\mathbf{s}^+ = [2.5, 2.2, 1.9, 2.1, 2.3]$\n  - $\\mathbf{s}^- = [-1.0, -0.8, -0.9, -1.2, -0.7]$\n  - $\\gamma = 0.95$\n  - $\\tau = 0.75$\n- 用例 C（大量平局，分布相同）：\n  - $\\mathbf{s}^+ = [0.5, 0.5, 0.5, 0.6, 0.4]$\n  - $\\mathbf{s}^- = [0.5, 0.5, 0.5, 0.6, 0.4]$\n  - $\\gamma = 0.95$\n  - $\\tau = 0.75$\n- 用例 D（小样本量，临界判别能力）：\n  - $\\mathbf{s}^+ = [0.7, 0.2, -0.1]$\n  - $\\mathbf{s}^- = [0.6, 0.3, 0.0, -0.2]$\n  - $\\gamma = 0.90$\n  - $\\tau = 0.70$\n\n最终输出格式：\n\n您的程序应生成单行输出，其中包含所有测试用例的结果，格式为逗号分隔的列表的列表，不含空格，并用方括号括起来。对于每个测试用例，输出列表 $[\\widehat{\\theta}, \\text{lower}, \\text{upper}, \\text{clinically\\_significant}]$，其中前三个条目是四舍五入到六位小数的浮点数，最后一个条目是布尔值。例如，最终输出应类似于\n$[[\\theta_A,\\ell_A,u_A,B_A],[\\theta_B,\\ell_B,u_B,B_B],[\\theta_C,\\ell_C,u_C,B_C],[\\theta_D,\\ell_D,u_D,B_D]]$\n，其中每个符号都被计算出的值替换。",
            "solution": "该问题要求计算受试者工作特征曲线下面积 (AUC)、使用 DeLong 的非参数方法计算其置信区间，并对几组支持向量机 (SVM) 分类器得分数据集的临床显著性进行评估。\n\n**问题验证**\n\n首先，我将根据指定标准验证问题陈述。\n\n**步骤 1：提取的已知条件**\n-   正类分数：$\\mathbf{s}^+ = \\{s^+_1, s^+_2, \\dots, s^+_m\\}$，有 $m$ 个样本。\n-   负类分数：$\\mathbf{s}^- = \\{s^-_1, s^-_2, \\dots, s^-_n\\}$，有 $n$ 个样本。\n-   成对比较函数：若 $a > b$ 则 $\\psi(a,b) = 1$，若 $a  b$ 则为 $0$，若 $a = b$ 则为 $\\frac{1}{2}$。\n-   经验 AUC 估计量：$\\widehat{\\theta} = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\psi(s^+_i, s^-_j)$。\n-   置信区间方法：基于 $U$ 统计量的 DeLong 方法，使用正态近似，其中置信水平为 $\\gamma = 1-\\alpha$ 时采用临界值 $z_{1-\\alpha/2}$。界限必须截断在 $[0,1]$ 内。\n-   临床显著性阈值：$\\tau$。如果一个分类器的置信下限 $\\ge \\tau$，则其“具有临床显著性”。\n-   四舍五入：所有浮点数输出（AUC、界限）均四舍五入到六位小数。\n-   测试用例：提供了四个具体用例（A、B、C、D），包括得分向量 $\\mathbf{s}^+$、$\\mathbf{s}^-$、置信水平 $\\gamma$ 和阈值 $\\tau$。\n\n**步骤 2：使用提取的已知条件进行验证**\n-   **科学依据**：该问题在根本上是合理的。将 AUC 定义为概率 $P(S^+ > S^-)$ 是标准做法。经验估计量 $\\widehat{\\theta}$ 是相应的 Wilcoxon-Mann-Whitney 统计量。DeLong 方法是一种成熟的、用于估计此统计量方差的非参数技术，其理论基础是广义 $U$ 统计量。这是生物统计学和机器学习评估中常用且严谨的方法。\n-   **适定性**：该问题是适定的。输入定义清晰，数学公式明确，期望的输出格式无歧义。对于每个测试用例，遵循所定义的程序可以计算出唯一、稳定且有意义的解。测试用例中的样本量（$m \\ge 3, n \\ge 4$）足以计算无偏样本方差，这要求每个组至少有两个样本。\n-   **客观性**：该问题以精确、客观的数学语言陈述，没有歧义或主观论断。虽然用例 A 被描述为具有“中等判别能力”，但所提供的数据实际上表现出完美分离。这是描述性文本中的一个微小不一致，但它不会使问题本身失效。计算必须基于所提供的明确无误的数值数据进行。\n\n**步骤 3：结论与行动**\n该问题被认为是有效的，因为它具有科学依据、适定性、客观性，并包含进行严谨求解所需的所有必要信息。我将继续进行详细的求解。\n\n**方法论与求解**\n\n求解过程分为四个阶段：(1) 计算经验 AUC，(2) 使用 DeLong 方法估计 AUC 估计量的方差，(3) 构建置信区间，以及 (4) 评估临床显著性。\n\n**1. 经验 AUC 估计**\nAUC, $\\theta$, 表示从正类中随机选择的样本得分高于从负类中随机选择的样本得分的概率，即 $\\theta = P(S^+ > S^-)$。数量 $\\widehat{\\theta}$ 是 $\\theta$ 的一个无偏估计量。它是通过对正负两类分数之间的所有 $m \\times n$ 个成对比较结果求平均值来计算的。\n比较由函数 $\\psi(a,b)$ 执行：\n$$\n\\psi(a,b) = \n\\begin{cases}\n1,  \\text{if } a > b,\\\\\n0,  \\text{if } a  b,\\\\\n\\frac{1}{2},  \\text{if } a = b.\n\\end{cases}\n$$\n那么经验 AUC 为：\n$$\n\\widehat{\\theta} = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\psi(s^+_i, s^-_j)\n$$\n\n**2. 通过 DeLong 方法估计方差**\nDeLong 方法提供了 $\\widehat{\\theta}$ 方差的非参数估计。它基于 $U$ 统计量理论。第一步是计算估计量的结构分量。\n我们定义两组分量。对于每个正类观测值 $s^+_i$，我们计算它与所有负类观测值的平均比较得分：\n$$\nV_{10}(s^+_i) = \\frac{1}{n} \\sum_{j=1}^{n} \\psi(s^+_i, s^-_j)\n$$\n有 $m$ 个这样的分量，每个 $s^+_i$ 对应一个。直观上，$V_{10}(s^+_i)$ 是仅使用第 $i$ 个正样本和所有负样本计算出的经验 AUC。\n\n类似地，对于每个负类观测值 $s^-_j$，我们计算它与所有正类观测值的平均比较得分：\n$$\nV_{01}(s^-_j) = \\frac{1}{m} \\sum_{i=1}^{m} \\psi(s^+_i, s^-_j)\n$$\n有 $n$ 个这样的分量。请注意，所有 $V_{10}$ 分量的平均值和所有 $V_{01}$ 分量的平均值都等于 $\\widehat{\\theta}$。\n\n然后通过组合这些结构分量的样本方差来估计 $\\widehat{\\theta}$ 的方差。$V_{10}$ 分量的无偏样本方差为：\n$$\nS_{10} = \\frac{1}{m-1} \\sum_{i=1}^{m} \\left( V_{10}(s^+_i) - \\widehat{\\theta} \\right)^2\n$$\n$V_{01}$ 分量的无偏样本方差为：\n$$\nS_{01} = \\frac{1}{n-1} \\sum_{j=1}^{n} \\left( V_{01}(s^-_j) - \\widehat{\\theta} \\right)^2\n$$\nAUC 估计量的估计方差 $\\widehat{\\mathrm{Var}}(\\widehat{\\theta})$ 由下式给出：\n$$\n\\widehat{\\mathrm{Var}}(\\widehat{\\theta}) = \\frac{S_{10}}{m} + \\frac{S_{01}}{n}\n$$\n$\\widehat{\\theta}$ 的标准误 (SE) 是该方差的平方根：\n$$\n\\mathrm{SE}(\\widehat{\\theta}) = \\sqrt{\\widehat{\\mathrm{Var}}(\\widehat{\\theta})}\n$$\n\n**3. 置信区间构建**\n$\\widehat{\\theta}$ 的分布可以由正态分布 $\\mathcal{N}(\\theta, \\mathrm{SE}(\\widehat{\\theta})^2)$ 来近似，特别是当 $m$ 和 $n$ 相当大时。$\\theta$ 的一个双侧 $(1-\\alpha)$ 置信区间构建如下：\n$$\n\\left[ \\widehat{\\theta} - z_{1-\\alpha/2} \\cdot \\mathrm{SE}(\\widehat{\\theta}), \\quad \\widehat{\\theta} + z_{1-\\alpha/2} \\cdot \\mathrm{SE}(\\widehat{\\theta}) \\right]\n$$\n其中 $z_{1-\\alpha/2}$ 是标准正态分布的 $(1-\\alpha/2)$-分位数。对于 $\\gamma=0.95$ 的置信水平，$\\alpha=0.05$，且 $z_{0.975} \\approx 1.96$。对于 $\\gamma=0.90$，$\\alpha=0.10$，且 $z_{0.95} \\approx 1.645$。由于 AUC 是一个概率，计算出的界限必须截断到有效范围 $[0, 1]$ 内。\n\n**4. 临床显著性**\n分类器的临床效用通常通过其在一定置信水平下区分不同类别的能力来判断。该问题将“具有临床显著性”的分类器定义为：其 AUC 置信区间的下限大于或等于预先指定的判别阈值 $\\tau$。这提供了一个保守的性能估计，以 $(1-\\alpha/2)$ 的置信度确保真实的 AUC 至少为 $\\tau$。该决策是一个基于条件 $\\text{clinically\\_significant} = (\\text{lower\\_bound} \\ge \\tau)$ 的布尔值。\n\n将为每个测试用例执行所有计算，并且最终结果（AUC、下限、上限、临床显著性）将按照规定进行四舍五入和格式化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating AUC, its CI via DeLong's method,\n    and clinical significance for given test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {\n            \"s_plus\": [1.8, 1.2, 0.9, 1.5, 1.1, 1.7, 0.8, 1.3, 0.95, 1.6],\n            \"s_minus\": [-0.4, -0.1, 0.2, -0.6, 0.0, -0.2, 0.1, -0.3, 0.05, -0.5],\n            \"gamma\": 0.95,\n            \"tau\": 0.75\n        },\n        # Case B\n        {\n            \"s_plus\": [2.5, 2.2, 1.9, 2.1, 2.3],\n            \"s_minus\": [-1.0, -0.8, -0.9, -1.2, -0.7],\n            \"gamma\": 0.95,\n            \"tau\": 0.75\n        },\n        # Case C\n        {\n            \"s_plus\": [0.5, 0.5, 0.5, 0.6, 0.4],\n            \"s_minus\": [0.5, 0.5, 0.5, 0.6, 0.4],\n            \"gamma\": 0.95,\n            \"tau\": 0.75\n        },\n        # Case D\n        {\n            \"s_plus\": [0.7, 0.2, -0.1],\n            \"s_minus\": [0.6, 0.3, 0.0, -0.2],\n            \"gamma\": 0.90,\n            \"tau\": 0.70\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        s_plus = np.array(case[\"s_plus\"])\n        s_minus = np.array(case[\"s_minus\"])\n        gamma = case[\"gamma\"]\n        tau = case[\"tau\"]\n\n        m = len(s_plus)\n        n = len(s_minus)\n\n        # 1. Calculate matrix of psi(s_plus_i, s_minus_j) values\n        # This uses broadcasting to efficiently compute pairwise differences.\n        # The formula (sign(diff) + 1) / 2 is equivalent to the psi function.\n        diff_matrix = s_plus[:, np.newaxis] - s_minus\n        psi_matrix = (np.sign(diff_matrix) + 1) / 2\n\n        # 2. Calculate empirical AUC (theta_hat)\n        theta_hat = np.mean(psi_matrix)\n\n        # 3. DeLong's method for variance\n        v_10 = np.mean(psi_matrix, axis=1) # Structural components for positive class\n        v_01 = np.mean(psi_matrix, axis=0) # Structural components for negative class\n\n        # Unbiased sample variances of the components\n        s_10 = np.var(v_10, ddof=1)\n        s_01 = np.var(v_01, ddof=1)\n        \n        # Handle cases with perfect separation where variance is 0\n        if np.isnan(s_10): s_10 = 0\n        if np.isnan(s_01): s_01 = 0\n        \n        # Total variance of theta_hat\n        var_theta = (s_10 / m) + (s_01 / n)\n        se_theta = np.sqrt(var_theta)\n\n        # 4. Construct Confidence Interval\n        alpha = 1 - gamma\n        z_quantile = norm.ppf(1 - alpha / 2)\n        margin_of_error = z_quantile * se_theta\n        \n        lower_bound = theta_hat - margin_of_error\n        upper_bound = theta_hat + margin_of_error\n\n        # Clip bounds to [0, 1]\n        lower_bound = np.clip(lower_bound, 0, 1)\n        upper_bound = np.clip(upper_bound, 0, 1)\n\n        # 5. Determine Clinical Significance\n        is_significant = lower_bound >= tau\n\n        # 6. Format results\n        result = [\n            round(theta_hat, 6),\n            round(lower_bound, 6),\n            round(upper_bound, 6),\n            is_significant\n        ]\n        results.append(str(result).replace(\" \", \"\"))\n\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}