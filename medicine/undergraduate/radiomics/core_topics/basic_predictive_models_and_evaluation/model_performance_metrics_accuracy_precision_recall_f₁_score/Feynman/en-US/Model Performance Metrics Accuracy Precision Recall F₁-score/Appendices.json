{
    "hands_on_practices": [
        {
            "introduction": "The first step in evaluating any diagnostic model is to move beyond a simple 'correct' or 'incorrect' verdict. This practice guides you through calculating the foundational metrics of precision, recall, and the $F_1$-score from a classifier's direct outputs. Through this exercise , you will develop a critical understanding of why accuracy can be a misleading metric in class-imbalanced datasets, a common scenario in radiomics, and why the $F_1$-score offers a more robust assessment of model performance.",
            "id": "4551730",
            "problem": "A radiomics system is developed to classify lesions as malignant ($+$) or benign ($-$) based on features extracted from Magnetic Resonance Imaging (MRI). On an independent, multi-institution hold-out cohort, the system produces the following confusion events: true positives (TP) $TP=40$, false positives (FP) $FP=20$, and false negatives (FN) $FN=10$. The number of true negatives (TN) is not reported because the institutions differ in benign case sampling strategies. Starting from the core definitions of event rates in binary classification, derive the expressions for precision, recall, and the F₁-score, and compute their values for this cohort. Then, using first principles, explain how the F₁-score should be interpreted relative to accuracy in a class-imbalanced screening setting typical of radiomics, where benign cases dominate and $TN$ is much larger than $TP+FN$. Express the three computed metric values as exact fractions. No rounding is required. Provide the three numbers in the order: precision, recall, F₁-score.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Classification task: Lesions as malignant ($+$) or benign ($-$).\n- True Positives ($TP$): $TP=40$.\n- False Positives ($FP$): $FP=20$.\n- False Negatives ($FN$): $FN=10$.\n- True Negatives ($TN$): Not reported.\n- Task a: Derive expressions for precision, recall, and F₁-score.\n- Task b: Compute the values for these metrics.\n- Task c: Explain the interpretation of the F₁-score relative to accuracy in a class-imbalanced setting where $TN$ is much larger than $TP+FN$.\n- Task d: Express computed metrics as exact fractions.\n- Task e: Provide the final values in the order: precision, recall, F₁-score.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is valid. It presents a standard binary classification scenario from the field of radiomics, a subfield of medical imaging analysis. The provided data ($TP$, $FP$, $FN$) are sufficient to calculate the requested metrics (precision, recall, F₁-score), as these metrics are independent of the true negative count ($TN$). The explicit mention that $TN$ is unreported is a deliberate and realistic constraint, not a flaw. The conceptual question regarding the F₁-score versus accuracy in an imbalanced setting is a fundamental and important topic in machine learning evaluation. The problem is self-contained, scientifically sound, and well-posed.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation and Computation of Metrics**\n\nWe begin by defining the fundamental classification events:\n- **True Positive ($TP$)**: A malignant lesion correctly classified as malignant.\n- **False Positive ($FP$)**: A benign lesion incorrectly classified as malignant (Type I error).\n- **True Negative ($TN$)**: A benign lesion correctly classified as benign.\n- **False Negative ($FN$)**: A malignant lesion incorrectly classified as benign (Type II error).\n\nThe total number of actual positive (malignant) instances is $P_{actual} = TP + FN$.\nThe total number of actual negative (benign) instances is $N_{actual} = TN + FP$.\nThe total number of predicted positive instances is $P_{predicted} = TP + FP$.\nThe total number of predicted negative instances is $N_{predicted} = TN + FN$.\n\n**1. Precision ($P$)**\nPrecision, or Positive Predictive Value (PPV), measures the proportion of predicted positive cases that are actually positive. It answers the question: \"Of all lesions the system flagged as malignant, what fraction were actually malignant?\"\nStarting from its definition:\n$$P = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}}$$\nIn terms of the confusion events, this is:\n$$P = \\frac{TP}{TP + FP}$$\nUsing the given values, $TP=40$ and $FP=20$:\n$$P = \\frac{40}{40 + 20} = \\frac{40}{60} = \\frac{2}{3}$$\n\n**2. Recall ($R$)**\nRecall, also known as Sensitivity or the True Positive Rate (TPR), measures the proportion of actual positive cases that were correctly identified. It answers the question: \"Of all the truly malignant lesions present, what fraction did the system correctly identify?\"\nStarting from its definition:\n$$R = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Positives}}$$\nIn terms of the confusion events, this is:\n$$R = \\frac{TP}{TP + FN}$$\nUsing the given values, $TP=40$ and $FN=10$:\n$$R = \\frac{40}{40 + 10} = \\frac{40}{50} = \\frac{4}{5}$$\n\n**3. F₁-score**\nThe F₁-score is the harmonic mean of precision and recall. It is used as a single metric that balances the concerns of both precision and recall. The harmonic mean is chosen because it penalizes extreme values more than the arithmetic mean. A high F₁-score requires both high precision and high recall.\nThe general formula for the harmonic mean of two numbers $a$ and $b$ is $2 \\frac{ab}{a+b}$. Thus, for precision $P$ and recall $R$:\n$$F_1 = 2 \\frac{P \\cdot R}{P + R}$$\nWe can derive an expression for the F₁-score directly in terms of $TP$, $FP$, and $FN$ by substituting the expressions for $P$ and $R$:\n$$F_1 = 2 \\frac{\\left(\\frac{TP}{TP + FP}\\right) \\cdot \\left(\\frac{TP}{TP + FN}\\right)}{\\left(\\frac{TP}{TP + FP}\\right) + \\left(\\frac{TP}{TP + FN}\\right)}$$\n$$F_1 = 2 \\frac{\\frac{TP^2}{(TP + FP)(TP + FN)}}{\\frac{TP(TP + FN) + TP(TP + FP)}{(TP + FP)(TP + FN)}}$$\n$$F_1 = \\frac{2 \\cdot TP^2}{TP(TP + FN) + TP(TP + FP)}$$\nFactoring out $TP$ from the denominator:\n$$F_1 = \\frac{2 \\cdot TP}{(TP + FN) + (TP + FP)} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$\nThis final expression is often more direct for computation. Using the given values, $TP=40$, $FP=20$, and $FN=10$:\n$$F_1 = \\frac{2 \\cdot 40}{2 \\cdot 40 + 20 + 10} = \\frac{80}{80 + 20 + 10} = \\frac{80}{110} = \\frac{8}{11}$$\n\nThe computed values are: Precision $P = \\frac{2}{3}$, Recall $R = \\frac{4}{5}$, and F₁-score $F_1 = \\frac{8}{11}$.\n\n**Interpretation of F₁-score vs. Accuracy in an Imbalanced Setting**\n\nFirst, we must define accuracy ($ACC$). Accuracy is the proportion of all predictions that are correct.\n$$ACC = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} = \\frac{TP + TN}{TP + TN + FP + FN}$$\nThe problem states that in a typical radiomics screening setting, benign cases dominate, meaning the number of true negatives ($TN$) is much larger than the number of cases related to the positive (malignant) class. Mathematically, $TN \\gg (TP + FN)$.\n\nLet's analyze the behavior of accuracy under this condition. As $TN$ becomes very large relative to the other terms ($TP$, $FP$, $FN$), both the numerator and denominator of the accuracy formula are dominated by $TN$:\n$$ACC = \\frac{TP + TN}{TP + TN + FP + FN} \\approx \\frac{TN}{TN + FP}$$\nFurthermore, since the model is not completely random, $FP$ is typically much smaller than $TN$. Thus, as $TN \\to \\infty$, the ratio approaches $1$:\n$$\\lim_{TN \\to \\infty} \\left( \\frac{TP + TN}{TP + TN + FP + FN} \\right) = 1$$\nThis reveals the critical flaw of accuracy in class-imbalanced scenarios. A trivial classifier that always predicts the majority class (benign) would have $TP=0$ and $FP=0$. Its accuracy would be $ACC = \\frac{TN}{TN + FN}$. If there is $1$ malignant case for every $999$ benign cases ($FN=1, TN=999$), this trivial classifier would achieve an accuracy of $\\frac{999}{1000} = 99.9\\%$, giving a dangerously misleading impression of high performance while it fails to identify any of the malignant cases it is designed to find.\n\nIn contrast, the F₁-score, along with its components precision and recall, are independent of $TN$:\n$$P = \\frac{TP}{TP+FP}, \\quad R = \\frac{TP}{TP+FN}, \\quad F_1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$\nThese metrics exclusively evaluate the model's performance on the positive class.\n- **Precision** focuses on the reliability of positive predictions, directly addressing the cost of false alarms ($FP$).\n- **Recall** focuses on the model's ability to find all positive cases, directly addressing the cost of missed cases ($FN$).\nA trivial \"always benign\" classifier would have $TP=0$, resulting in $P=0$, $R=0$, and $F_1=0$, correctly reflecting its complete lack of utility for detecting the malignant class.\n\nThe F₁-score synthesizes the trade-off between precision and recall into a single number. In medical screening, both are critical: low recall ($high FN$) means missing cancers, which is a severe failure. Low precision ($high FP$) means subjecting many healthy individuals to unnecessary, costly, and stressful follow-up procedures. The F₁-score provides a more robust and relevant measure of a model's practical utility in such imbalanced settings than accuracy, because it is not inflated by the large number of correctly identified majority-class cases ($TN$). It properly focuses the evaluation on the performance related to the minority, high-stakes class of interest.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3} & \\frac{4}{5} & \\frac{8}{11}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A model's performance metrics are not absolute; they are deeply intertwined with the characteristics of the population being tested. This exercise  explores the critical concept of prevalence and its profound impact on a classifier's real-world utility. By applying Bayes' theorem, you will see how a model's Positive Predictive Value (precision) can change dramatically when moving from a balanced research study to a low-prevalence screening environment, a crucial lesson for deploying radiomics tools responsibly.",
            "id": "4551711",
            "problem": "A radiomics classifier derived from texture and shape features of chest computed tomography (CT) images is evaluated for detecting early-stage lung malignancy. In a case-control development study, the classifier achieved sensitivity $0.85$ and specificity $0.95$, with study prevalence $0.10$. The intended deployment setting is a population screening program where the target prevalence is $0.02$.\n\nUsing only foundational probabilistic definitions and well-tested principles, namely Bayes’ theorem and the law of total probability, start from the definitions of sensitivity $P(T^{+}\\,|\\,D)$, specificity $P(T^{-}\\,|\\,\\bar{D})$, and prevalence $P(D)$, where $D$ denotes the event “malignancy present,” $\\bar{D}$ its complement, and $T^{+}$/$T^{-}$ denote the classifier’s positive/negative outputs. From this base, derive expressions for the Positive Predictive Value (PPV; also known as precision) $P(D\\,|\\,T^{+})$ and the Negative Predictive Value (NPV) $P(\\bar{D}\\,|\\,T^{-})$ as functions of sensitivity, specificity, and prevalence. Compute the numerical values of PPV and NPV under the study prevalence $0.10$ and under the target prevalence $0.02$, expressing all probabilities as decimals.\n\nFinally, report as your single numerical answer the ratio\n$$\n\\rho \\;=\\; \\frac{\\text{PPV at target prevalence}}{\\text{PPV at study prevalence}},\n$$\nrounded to four significant figures. No units are required. In your derivation, briefly explain the implications of shifting prevalence for PPV (precision), NPV, recall (sensitivity), and the F$_1$-score, in the context of deploying radiomics models from case-control studies into screening populations.",
            "solution": "The problem is scientifically grounded, well-posed, and objective. All necessary data are provided, and the task is to perform a standard analysis of diagnostic test performance under varying prevalence, based on foundational principles of probability theory. We proceed with the derivation and solution.\n\nLet $D$ be the event of malignancy presence and $\\bar{D}$ be its complement (absence of malignancy). Let $T^{+}$ and $T^{-}$ be the events of a positive and negative classifier result, respectively. The provided quantities are:\n- Sensitivity ($S_e$): The probability of a positive test given disease, $S_e = P(T^{+}|D) = 0.85$.\n- Specificity ($S_p$): The probability of a negative test given no disease, $S_p = P(T^{-}|\\bar{D}) = 0.95$.\n- Prevalence ($p$): The prior probability of disease, $p = P(D)$. We are given two values: $p_{study} = 0.10$ and $p_{target} = 0.02$.\n\nOur objective is to derive expressions for the Positive Predictive Value (PPV or precision), $P(D|T^{+})$, and the Negative Predictive Value (NPV), $P(\\bar{D}|T^{-})$, and then compute their values for the two given prevalences.\n\nFirst, we derive the expression for PPV, $P(D|T^{+})$. Using Bayes' theorem:\n$$P(D|T^{+}) = \\frac{P(T^{+}|D) P(D)}{P(T^{+})}$$\nThe numerator contains the sensitivity, $P(T^{+}|D) = S_e$, and the prevalence, $P(D) = p$. The denominator, $P(T^{+})$, is the total probability of a positive test. We expand it using the law of total probability:\n$$P(T^{+}) = P(T^{+}|D)P(D) + P(T^{+}|\\bar{D})P(\\bar{D})$$\nWe can express the terms in this expansion using the given definitions:\n- $P(T^{+}|D) = S_e$\n- $P(D) = p$\n- $P(\\bar{D}) = 1 - P(D) = 1 - p$\n- $P(T^{+}|\\bar{D})$ is the false positive rate, which is $1 - P(T^{-}|\\bar{D}) = 1 - S_p$.\n\nSubstituting these into the expansion for $P(T^{+})$:\n$$P(T^{+}) = S_e \\cdot p + (1 - S_p)(1 - p)$$\nNow, substituting this back into the Bayes' theorem expression for PPV gives its general form as a function of $S_e$, $S_p$, and $p$:\n$$\\text{PPV} = P(D|T^{+}) = \\frac{S_e \\cdot p}{S_e \\cdot p + (1 - S_p)(1 - p)}$$\n\nNext, we derive the expression for NPV, $P(\\bar{D}|T^{-})$. Again, from Bayes' theorem:\n$$P(\\bar{D}|T^{-}) = \\frac{P(T^{-}|\\bar{D}) P(\\bar{D})}{P(T^{-})}$$\nThe numerator contains the specificity, $P(T^{-}|\\bar{D}) = S_p$, and the probability of no disease, $P(\\bar{D}) = 1 - p$. The denominator, $P(T^{-})$, is the total probability of a negative test, expanded using the law of total probability:\n$$P(T^{-}) = P(T^{-}|D)P(D) + P(T^{-}|\\bar{D})P(\\bar{D})$$\nWe identify the terms:\n- $P(T^{-}|D)$ is the false negative rate, which is $1 - P(T^{+}|D) = 1 - S_e$.\n- $P(D) = p$\n- $P(T^{-}|\\bar{D}) = S_p$\n- $P(\\bar{D}) = 1 - p$\n\nSubstituting these into the expansion for $P(T^{-})$:\n$$P(T^{-}) = (1 - S_e) \\cdot p + S_p(1 - p)$$\nAnd substituting this back into the expression for NPV:\n$$\\text{NPV} = P(\\bar{D}|T^{-}) = \\frac{S_p (1 - p)}{(1 - S_e) \\cdot p + S_p(1 - p)}$$\n\nNow, we compute the numerical values for the two scenarios.\nGiven values are $S_e = 0.85$ and $S_p = 0.95$.\n\nCase 1: Study prevalence, $p_{study} = 0.10$.\n$$\\text{PPV}_{study} = \\frac{0.85 \\cdot 0.10}{0.85 \\cdot 0.10 + (1 - 0.95)(1 - 0.10)} = \\frac{0.085}{0.085 + (0.05)(0.90)} = \\frac{0.085}{0.085 + 0.045} = \\frac{0.085}{0.130} = \\frac{17}{26} \\approx 0.6538$$\n$$\\text{NPV}_{study} = \\frac{0.95 \\cdot (1 - 0.10)}{(1 - 0.85) \\cdot 0.10 + 0.95 \\cdot (1 - 0.10)} = \\frac{0.95 \\cdot 0.90}{0.15 \\cdot 0.10 + 0.95 \\cdot 0.90} = \\frac{0.855}{0.015 + 0.855} = \\frac{0.855}{0.870} = \\frac{57}{58} \\approx 0.9828$$\n\nCase 2: Target screening prevalence, $p_{target} = 0.02$.\n$$\\text{PPV}_{target} = \\frac{0.85 \\cdot 0.02}{0.85 \\cdot 0.02 + (1 - 0.95)(1 - 0.02)} = \\frac{0.017}{0.017 + (0.05)(0.98)} = \\frac{0.017}{0.017 + 0.049} = \\frac{0.017}{0.066} = \\frac{17}{66} \\approx 0.2576$$\n$$\\text{NPV}_{target} = \\frac{0.95 \\cdot (1 - 0.02)}{(1 - 0.85) \\cdot 0.02 + 0.95 \\cdot (1 - 0.02)} = \\frac{0.95 \\cdot 0.98}{0.15 \\cdot 0.02 + 0.95 \\cdot 0.98} = \\frac{0.931}{0.003 + 0.931} = \\frac{0.931}{0.934} \\approx 0.9968$$\n\nThe shift in prevalence from a case-control study setting ($p=0.10$) to a population screening setting ($p=0.02$) has profound implications for the classifier's performance metrics:\n- **PPV (Precision)**: The PPV drops precipitously from $\\approx 0.65$ to $\\approx 0.26$. This is a critical observation. In a low-prevalence setting, a positive test result is far more likely to be a false positive. The clinical implication is that most individuals flagged by the classifier will not have the disease, leading to unnecessary follow-up procedures, costs, and patient anxiety. This high false positive burden is a defining challenge of screening programs.\n- **NPV**: The NPV increases slightly, from $\\approx 0.983$ to $\\approx 0.997$. In a low-prevalence setting, a negative result becomes even more reliable in ruling out the disease. This is because the prior probability of being disease-free is already very high.\n- **Recall (Sensitivity)**: Recall, defined as $P(T^{+}|D)$, is an intrinsic property of the classifier related to its ability to detect disease when it is present. It is conditioned on the true state $D$ and is therefore independent of the disease prevalence $p$ in the population. It remains constant at $0.85$.\n- **F₁-score**: The F₁-score is the harmonic mean of precision (PPV) and recall: $F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$. Since recall is constant and precision is highly dependent on prevalence, the F₁-score is also highly dependent on prevalence. The dramatic drop in precision causes a similarly dramatic drop in the F₁-score, indicating a substantial degradation in the overall balance of the classifier's performance when moved to the screening setting.\n\nFinally, we compute the required ratio $\\rho$:\n$$\\rho \\;=\\; \\frac{\\text{PPV at target prevalence}}{\\text{PPV at study prevalence}} = \\frac{\\text{PPV}_{target}}{\\text{PPV}_{study}} = \\frac{17/66}{17/26} = \\frac{17}{66} \\cdot \\frac{26}{17} = \\frac{26}{66} = \\frac{13}{33}$$\nAs a decimal, $\\rho \\approx 0.393939...$. Rounding to four significant figures gives $0.3939$.",
            "answer": "$$\n\\boxed{0.3939}\n$$"
        },
        {
            "introduction": "Ultimately, performance metrics are not just numbers; they are tools for making informed decisions. This final practice  bridges the gap between theoretical evaluation and practical implementation by challenging you to optimize a classifier's decision threshold. You will navigate real-world trade-offs, balancing clinical constraints like follow-up capacity against the essential need for high recall, to find a solution that is not just statistically sound but also operationally feasible.",
            "id": "4551721",
            "problem": "You are given a binary classification scenario from radiomics in which a probabilistic classifier assigns to each case a malignancy score in the closed interval $[0,1]$. Each week, the clinic can perform at most $C$ follow-up actions, and a follow-up is triggered for every case predicted as positive. The weekly radiologist team requires a minimum sensitivity level, expressed as a minimum recall requirement $r_{\\min} \\in [0,1]$, to ensure that a sufficient fraction of actual malignant cases are flagged for follow-up. Your task is to choose a single decision threshold $\\tau \\in [0,1]$ such that a case is predicted positive if and only if its score is at least $\\tau$.\n\nYour program must determine $\\tau$ that maximizes precision while satisfying both the weekly follow-up capacity and the minimum recall requirement. Formally, for each candidate threshold $\\tau$, compute the confusion matrix entries based on standard definitions: True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN). Let $N$ be the total number of cases and $P$ be the total number of actual positives. The performance metrics are defined as follows, all expressed as decimals:\n- Accuracy: $\\mathrm{Acc} = \\dfrac{\\mathrm{TP} + \\mathrm{TN}}{N}$.\n- Precision: $\\mathrm{Prec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}$ with the convention that $\\mathrm{Prec} = 0$ when $\\mathrm{TP} + \\mathrm{FP} = 0$.\n- Recall: $\\mathrm{Rec} = \\dfrac{\\mathrm{TP}}{P}$ with the convention that $\\mathrm{Rec} = 0$ when $P = 0$.\n- F$_1$-score: $\\mathrm{F}_1 = \\dfrac{2 \\cdot \\mathrm{Prec} \\cdot \\mathrm{Rec}}{\\mathrm{Prec} + \\mathrm{Rec}}$ with the convention that $\\mathrm{F}_1 = 0$ when $\\mathrm{Prec} + \\mathrm{Rec} = 0$.\n\nA threshold $\\tau$ is feasible if and only if both:\n- The number of predicted positives is at most the weekly capacity $C$, i.e., $\\mathrm{TP} + \\mathrm{FP} \\le C$.\n- The recall meets the minimum requirement, i.e., $\\mathrm{Rec} \\ge r_{\\min}$.\n\nAmong all feasible thresholds, select the one that maximizes $\\mathrm{Prec}$. If there is a tie in precision, break ties by choosing the threshold with the highest F$_1$-score. If there is still a tie, choose the largest threshold $\\tau$. If no threshold satisfies both feasibility conditions, return $\\tau = -1$ and set all four metrics to $0$.\n\nUse the following test suite. In all cases, a case is predicted positive if and only if its score is at least $\\tau$. All numeric answers must be expressed as decimals (not percentages). Your program must evaluate the threshold only at the set of unique score values observed in each dataset.\n\nTest suite:\n- Case $1$: Weekly capacity $C = 20$, minimum recall $r_{\\min} = 0.7$. Scores and labels (each pair is $(\\text{score}, \\text{label})$) for $N = 30$ radiomics cases:\n  $\\{(0.95,1),(0.92,1),(0.90,1),(0.88,0),(0.86,1),(0.84,1),(0.82,0),(0.80,1),(0.78,0),(0.76,1),(0.74,1),(0.72,0),(0.70,1),(0.68,0),(0.66,1),(0.64,0),(0.62,0),(0.60,1),(0.58,0),(0.56,0),(0.54,1),(0.52,0),(0.50,0),(0.48,1),(0.46,0),(0.44,0),(0.42,0),(0.40,1),(0.38,0),(0.36,0)\\}$.\n- Case $2$: Weekly capacity $C = 5$, minimum recall $r_{\\min} = 0.9$. Scores and labels for $N = 20$ radiomics cases:\n  $\\{(0.99,1),(0.95,1),(0.93,1),(0.85,1),(0.80,1),(0.70,1),(0.60,0),(0.58,0),(0.55,0),(0.52,0),(0.50,0),(0.48,0),(0.46,0),(0.44,0),(0.42,0),(0.40,0),(0.38,0),(0.36,0),(0.34,0),(0.30,0)\\}$.\n- Case $3$: Weekly capacity $C = 3$, minimum recall $r_{\\min} = 0.5$. Scores and labels for $N = 12$ radiomics cases:\n  $\\{(0.96,1),(0.90,0),(0.88,1),(0.85,0),(0.80,1),(0.78,0),(0.60,1),(0.55,0),(0.52,0),(0.50,0),(0.40,0),(0.30,0)\\}$.\n\nFinal output format:\n- For each case, output a list $[\\tau, \\mathrm{Acc}, \\mathrm{Prec}, \\mathrm{Rec}, \\mathrm{F}_1]$ with each value rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of these lists, enclosed in square brackets, for example $[[\\cdot],[\\cdot],[\\cdot]]$.",
            "solution": "The user-provided problem is assessed to be valid. It is a well-posed optimization problem grounded in the standard principles of binary classification model evaluation. The problem statement is self-contained, objective, and scientifically sound. All necessary data, constraints, and definitions are provided, and there are no internal contradictions.\n\nThe problem asks for the determination of an optimal decision threshold, $\\tau$, for a probabilistic binary classifier used in a radiomics context. The goal is to maximize the classifier's precision ($\\mathrm{Prec}$) subject to two operational constraints: a weekly follow-up capacity, $C$, and a minimum required sensitivity level, $r_{\\min}$, expressed as a recall ($\\mathrm{Rec}$) value.\n\nThe solution methodology proceeds in a structured, stepwise manner.\n\n1.  **Problem Formalization**\n    The task is a constrained optimization problem. We must find a threshold $\\tau \\in [0,1]$ that solves:\n    $$\n    \\underset{\\tau}{\\text{maximize}} \\quad \\mathrm{Prec}(\\tau)\n    $$\n    Subject to:\n    1.  Capacity Constraint: $\\mathrm{TP}(\\tau) + \\mathrm{FP}(\\tau) \\le C$\n    2.  Recall Constraint: $\\mathrm{Rec}(\\tau) \\ge r_{\\min}$\n\n    In cases where multiple thresholds yield the same maximum precision, a tie-breaking hierarchy is enforced:\n    1.  Select the threshold with the highest $\\mathrm{F}_1$-score.\n    2.  If a tie persists, select the largest threshold value, $\\tau$.\n\n    The search space for $\\tau$ is explicitly limited to the set of unique radiomics scores present in the dataset.\n\n2.  **Systematic Evaluation of Candidate Thresholds**\n    The core of the methodology is to perform an exhaustive search over the allowed set of candidate thresholds. For each unique score value observed in the data, we treat it as a potential threshold $\\tau$.\n\n    For each candidate $\\tau$, we first classify all $N$ cases. A case is predicted as positive if its score is greater than or equal to $\\tau$, and negative otherwise. This allows us to construct a confusion matrix by comparing the predictions to the true labels. The elements of this matrix are:\n    -   $\\mathrm{TP}(\\tau)$: True Positives (score $\\ge \\tau$ and true label is $1$)\n    -   $\\mathrm{FP}(\\tau)$: False Positives (score $\\ge \\tau$ and true label is $0$)\n    -   $\\mathrm{TN}(\\tau)$: True Negatives (score $< \\tau$ and true label is $0$)\n    -   $\\mathrm{FN}(\\tau)$: False Negatives (score $< \\tau$ and true label is $1$)\n\n3.  **Performance Metric Calculation**\n    Using the confusion matrix values, we calculate the required performance metrics for each $\\tau$. Let $P = \\mathrm{TP} + \\mathrm{FN}$ be the total number of actual positive cases in the dataset.\n    -   **Accuracy**: $\\mathrm{Acc} = \\dfrac{\\mathrm{TP} + \\mathrm{TN}}{N}$\n    -   **Precision**: $\\mathrm{Prec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}$. By convention, if the denominator $\\mathrm{TP} + \\mathrm{FP} = 0$, then $\\mathrm{Prec} = 0$.\n    -   **Recall**: $\\mathrm{Rec} = \\dfrac{\\mathrm{TP}}{P}$. By convention, if $P=0$, then $\\mathrm{Rec}=0$.\n    -   **F$_1$-score**: $\\mathrm{F}_1 = \\dfrac{2 \\cdot \\mathrm{Prec} \\cdot \\mathrm{Rec}}{\\mathrm{Prec} + \\mathrm{Rec}}$. By convention, if the denominator $\\mathrm{Prec} + \\mathrm{Rec} = 0$, then $\\mathrm{F}_1 = 0$.\n\n4.  **Feasibility Analysis**\n    Each candidate threshold $\\tau$ and its corresponding metrics are tested for feasibility against the two given constraints:\n    -   $\\mathrm{TP}(\\tau) + \\mathrm{FP}(\\tau) \\le C$\n    -   $\\mathrm{Rec}(\\tau) \\ge r_{\\min}$\n\n    A threshold is considered a valid solution only if it satisfies both conditions simultaneously. All such feasible solutions are collected for the final selection step.\n\n5.  **Optimal Solution Selection**\n    If the set of feasible solutions is empty after evaluating all candidate thresholds, it is concluded that no solution exists that satisfies the clinical requirements. In this scenario, the output is $\\tau = -1$ and all performance metrics are set to $0$.\n\n    If one or more feasible solutions exist, the optimal solution is identified by applying the specified hierarchical criteria. An efficient way to implement this is to sort the set of feasible solutions. Each solution is represented by a tuple or structure containing its metrics, for instance $(\\mathrm{Prec}, \\mathrm{F}_1, \\tau)$. The sorting is performed in descending order of $\\mathrm{Prec}$, then descending order of $\\mathrm{F}_1$, and finally descending order of $\\tau$. The first element in the sorted list is the unique optimal solution according to the problem's rules.\n\n    The final output for each test case is a list containing the optimal threshold and its associated metrics: $[\\tau, \\mathrm{Acc}, \\mathrm{Prec}, \\mathrm{Rec}, \\mathrm{F}_1]$, with each value numerically formatted as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the threshold optimization problem for a series of test cases.\n    \"\"\"\n    test_cases = [\n        (\n            20, 0.7, \n            [\n                (0.95, 1), (0.92, 1), (0.90, 1), (0.88, 0), (0.86, 1), (0.84, 1), \n                (0.82, 0), (0.80, 1), (0.78, 0), (0.76, 1), (0.74, 1), (0.72, 0), \n                (0.70, 1), (0.68, 0), (0.66, 1), (0.64, 0), (0.62, 0), (0.60, 1), \n                (0.58, 0), (0.56, 0), (0.54, 1), (0.52, 0), (0.50, 0), (0.48, 1), \n                (0.46, 0), (0.44, 0), (0.42, 0), (0.40, 1), (0.38, 0), (0.36, 0)\n            ]\n        ),\n        (\n            5, 0.9, \n            [\n                (0.99, 1), (0.95, 1), (0.93, 1), (0.85, 1), (0.80, 1), (0.70, 1), \n                (0.60, 0), (0.58, 0), (0.55, 0), (0.52, 0), (0.50, 0), (0.48, 0), \n                (0.46, 0), (0.44, 0), (0.42, 0), (0.40, 0), (0.38, 0), (0.36, 0), \n                (0.34, 0), (0.30, 0)\n            ]\n        ),\n        (\n            3, 0.5, \n            [\n                (0.96, 1), (0.90, 0), (0.88, 1), (0.85, 0), (0.80, 1), (0.78, 0), \n                (0.60, 1), (0.55, 0), (0.52, 0), (0.50, 0), (0.40, 0), (0.30, 0)\n            ]\n        )\n    ]\n\n    all_results = []\n    for C, r_min, data in test_cases:\n        scores = np.array([d[0] for d in data])\n        labels = np.array([d[1] for d in data])\n        \n        N = len(labels)\n        P = np.sum(labels)\n        \n        # Per problem statement, evaluate only at unique score values.\n        unique_scores = np.unique(scores)\n        \n        feasible_solutions = []\n        \n        for tau in unique_scores:\n            predictions = (scores >= tau).astype(int)\n            TP = np.sum((predictions == 1) & (labels == 1))\n            FP = np.sum((predictions == 1) & (labels == 0))\n            \n            predicted_positives = TP + FP\n\n            if P == 0:\n                rec = 0.0\n            else:\n                rec = TP / P\n\n            # Check feasibility first.\n            is_feasible = (predicted_positives <= C) and (rec >= r_min)\n\n            if is_feasible:\n                # Calculate remaining metrics only for feasible thresholds.\n                TN = np.sum((predictions == 0) & (labels == 0))\n                \n                if predicted_positives == 0:\n                    prec = 0.0\n                else:\n                    prec = TP / predicted_positives\n                \n                if (prec + rec) == 0:\n                    f1 = 0.0\n                else:\n                    f1 = 2 * prec * rec / (prec + rec)\n\n                acc = (TP + TN) / N\n                \n                # Store all metrics for this feasible solution.\n                feasible_solutions.append({\n                    'prec': prec, \n                    'f1': f1, \n                    'tau': tau, \n                    'acc': acc, \n                    'rec': rec\n                })\n        \n        if not feasible_solutions:\n            result = [-1.0, 0.0, 0.0, 0.0, 0.0]\n        else:\n            # Sort using a key that implements the hierarchical tie-breaking rules:\n            # 1. Maximize precision (sort by -prec)\n            # 2. Maximize F1-score (sort by -f1)\n            # 3. Maximize threshold tau (sort by -tau)\n            best_solution = sorted(feasible_solutions, key=lambda x: (-x['prec'], -x['f1'], -x['tau']))[0]\n            \n            result = [\n                best_solution['tau'],\n                best_solution['acc'],\n                best_solution['prec'],\n                best_solution['rec'],\n                best_solution['f1']\n            ]\n        \n        # Format the result list into the required string format with 6 decimal places.\n        formatted_result = f\"[{','.join([f'{v:.6f}' for v in result])}]\"\n        all_results.append(formatted_result)\n\n    # Print a single line with all results, as specified.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}