{
    "hands_on_practices": [
        {
            "introduction": "This exercise connects the physical world of tissue morphology to the digital world of pixels. We will apply the foundational Nyquist-Shannon sampling theorem to determine the minimum scanning resolution required to faithfully capture diagnostically important features. Mastering this concept is the first step in ensuring that a whole-slide image is fit for purpose, preventing the loss of critical information before any analysis even begins .",
            "id": "4949003",
            "problem": "In virtual slide microscopy for histopathology, the smallest diagnostically relevant nuclear feature to be captured is a micronucleus-like nuclear fragment of diameter $1.5\\,\\mu\\mathrm{m}$ at the specimen plane. The scanner reports sampling density as an object-space pixel size in $\\mu\\mathrm{m}/\\mathrm{pixel}$. Using the Nyquist–Shannon sampling theorem for band-limited signals and the definition that faithful digital capture of a spatial feature requires at least $2$ samples across the minimum feature width, derive the relationship between the required object-space pixel size and the minimum feature width. Then compute the minimum object-space pixel size needed to faithfully capture a $1.5\\,\\mu\\mathrm{m}$ diameter feature at $2$ samples per minimum feature width. Finally, based on your derivation, determine whether an object-space pixel size of $0.25\\,\\mu\\mathrm{m}/\\mathrm{pixel}$ is adequate for this task, justifying your conclusion in words.\n\nExpress the computed minimum object-space pixel size in $\\mu\\mathrm{m}/\\mathrm{pixel}$ and round your numerical answer to three significant figures.",
            "solution": "The problem is evaluated to be valid as it is scientifically grounded in the principles of signal processing (Nyquist–Shannon sampling theorem) as applied to digital imaging, is well-posed with sufficient and consistent data, and is expressed in objective, precise language.\n\nThe problem requires the derivation of a relationship between sampling pixel size and minimum feature size, a calculation based on this relationship, and an assessment of a given scanner's adequacy. This will be addressed sequentially.\n\nThe foundation for this problem is the Nyquist–Shannon sampling theorem, which dictates the conditions necessary to reconstruct a continuous band-limited signal from a discrete set of samples without loss of information. For a spatial signal, such as a microscopic image, the theorem can be expressed in terms of spatial frequencies.\n\nLet $d_{min}$ be the smallest feature size (diameter) that must be resolved. This corresponds to the highest spatial frequency, $k_{max}$, present in the signal that we wish to capture, where the relationship is approximately $k_{max} \\approx \\frac{1}{d_{min}}$. The theorem states that the sampling frequency, $k_s$, must be at least twice the highest frequency component of the signal.\n$$k_s \\ge 2k_{max}$$\nThe spatial sampling frequency, $k_s$, is determined by the object-space pixel size, $p_s$. The sampling frequency is the number of samples per unit distance, so it is the reciprocal of the pixel size.\n$$k_s = \\frac{1}{p_s}$$\nSubstituting the expressions for $k_s$ and $k_{max}$ into the Nyquist criterion gives:\n$$\\frac{1}{p_s} \\ge 2 \\left( \\frac{1}{d_{min}} \\right)$$\nSolving this inequality for the object-space pixel size, $p_s$, yields the required relationship:\n$$p_s \\le \\frac{d_{min}}{2}$$\nThis relationship states that for faithful capture, the center-to-center distance between adjacent sampling points (the pixel size) must be less than or equal to half the width of the smallest feature of interest. This corresponds directly to the problem's definition of requiring at least $N_{samples} = 2$ samples across the minimum feature width. The maximum permissible pixel size, which I will denote as $p_{s,max}$, occurs at the limit of this inequality.\n\nNext, we compute this maximum permissible object-space pixel size needed to faithfully capture a feature of diameter $d_{min} = 1.5\\,\\mu\\mathrm{m}$, using the critical sampling condition of exactly $2$ samples per feature width.\n$$p_{s,max} = \\frac{d_{min}}{2} = \\frac{1.5\\,\\mu\\mathrm{m}}{2} = 0.75\\,\\mu\\mathrm{m}$$\nThe units are implicitly $\\mu\\mathrm{m}/\\mathrm{pixel}$ as per the problem's definition of object-space pixel size. The problem requires this value to be rounded to three significant figures.\n$$p_{s,max} = 0.750\\,\\mu\\mathrm{m}/\\mathrm{pixel}$$\n\nFinally, we must determine whether a scanner with an object-space pixel size of $p_{s,actual} = 0.25\\,\\mu\\mathrm{m}/\\mathrm{pixel}$ is adequate for this task. The condition for adequacy is that the actual pixel size must be less than or equal to the maximum permissible pixel size derived from the Nyquist criterion.\n$$p_{s,actual} \\le p_{s,max}$$\nSubstituting the values:\n$$0.25\\,\\mu\\mathrm{m}/\\mathrm{pixel} \\le 0.750\\,\\mu\\mathrm{m}/\\mathrm{pixel}$$\nThis inequality is true.\n\nJustification: An object-space pixel size of $0.25\\,\\mu\\mathrm{m}/\\mathrm{pixel}$ is adequate. The derived maximum allowable pixel size to satisfy the Nyquist criterion for a $1.5\\,\\mu\\mathrm{m}$ feature is $0.750\\,\\mu\\mathrm{m}/\\mathrm{pixel}$. Since the scanner's actual pixel size ($0.25\\,\\mu\\mathrm{m}/\\mathrm{pixel}$) is smaller than this maximum limit, the sampling rate is higher than the minimum required rate. Specifically, the number of samples across the $1.5\\,\\mu\\mathrm{m}$ feature would be:\n$$N_{samples} = \\frac{d_{min}}{p_{s,actual}} = \\frac{1.5\\,\\mu\\mathrm{m}}{0.25\\,\\mu\\mathrm{m}/\\mathrm{pixel}} = 6\\,\\mathrm{pixels}$$\nSince $6$ samples is greater than the required minimum of $2$ samples, the scanner provides more than sufficient sampling density to faithfully capture the $1.5\\,\\mu\\mathrm{m}$ nuclear fragment. This represents oversampling, which is beneficial for image quality and subsequent analysis.",
            "answer": "$$\\boxed{0.750}$$"
        },
        {
            "introduction": "A single whole-slide image can be gigapixels in size, making it too large to handle as one monolithic file for interactive viewing. The solution is a multi-resolution pyramid structure, which stores the image at several levels of detail. This practice problem delves into the simple but powerful mathematics behind constructing these pyramids, giving you a concrete understanding of how digital pathology systems achieve their fluid zoom and pan capabilities .",
            "id": "4948984",
            "problem": "In virtual microscopy and digital pathology, Whole Slide Images (WSI) are stored as resolution pyramids to enable rapid multi-scale navigation. Consider a WSI acquired at $40\\times$ optical magnification with a spatial sampling of $0.25\\,\\mu\\mathrm{m}/\\mathrm{pixel}$. A contiguous square tissue region on this slide measures $15\\,\\mathrm{mm}\\times 15\\,\\mathrm{mm}$ in physical space. The pyramid contains $4$ levels indexed by $\\ell=0,1,2,3$, where each successive level is downsampled by a factor of $2$ relative to the previous level. Assume that the pixel dimension along each axis is determined solely by the physical size and sampling rate at that level, and that downsampling yields exact integer pixel counts without padding or cropping.\n\nUsing the fundamental relation between physical length and sampling, and standard dimensional conversion $1\\,\\mathrm{mm}=1000\\,\\mu\\mathrm{m}$, compute the pixel dimensions (width and height) at each pyramid level. Report your final answer as a single row matrix containing $(w_0,h_0,w_1,h_1,w_2,h_2,w_3,h_3)$, where $w_\\ell$ and $h_\\ell$ are the width and height in pixels at level $\\ell$. Do not include units in your final answer.",
            "solution": "Whole Slide Image (WSI) pyramids rely on the sampling relation that connects physical length to the number of discrete samples (pixels). The foundational definition is that if a one-dimensional extent has physical length $L$ and the sampling interval is $s$ (in $\\mu\\mathrm{m}/\\mathrm{pixel}$), then the number of pixels $N$ along that axis is\n$$\nN \\;=\\; \\frac{L}{s}.\n$$\nFor a square region, the width and height in pixels are equal and each given by this relation, applied independently to each axis.\n\nFirst, convert the given physical size from millimeters to micrometers using the well-tested dimensional identity $1\\,\\mathrm{mm}=1000\\,\\mu\\mathrm{m}$. The tissue region is $15\\,\\mathrm{mm}\\times 15\\,\\mathrm{mm}$, so each side length is\n$$\nL \\;=\\; 15\\,\\mathrm{mm} \\times 1000\\,\\frac{\\mu\\mathrm{m}}{\\mathrm{mm}} \\;=\\; 15000\\,\\mu\\mathrm{m}.\n$$\nAt the base level (level $\\ell=0$), the sampling interval is the native scan sampling $s_0 = 0.25\\,\\mu\\mathrm{m}/\\mathrm{pixel}$. Therefore, the pixel count along one axis at level $0$ is\n$$\nN_0 \\;=\\; \\frac{L}{s_0} \\;=\\; \\frac{15000}{0.25} \\;=\\; 60000.\n$$\nSince the region is square, the width and height at level $0$ are\n$$\nw_0 \\;=\\; 60000, \\quad h_0 \\;=\\; 60000.\n$$\n\nEach successive pyramid level is downsampled by a factor of $2$ relative to the previous level. Downsampling by a factor of $2$ along each axis reduces the pixel count by the same factor. Thus, the general relationship for level $\\ell$ is\n$$\nN_{\\ell} \\;=\\; \\frac{N_0}{2^{\\ell}},\n$$\nand since the region remains square at each level,\n$$\nw_{\\ell} \\;=\\; N_{\\ell}, \\quad h_{\\ell} \\;=\\; N_{\\ell}.\n$$\nCompute these for $\\ell=1,2,3$:\n- For $\\ell=1$:\n$$\nN_1 \\;=\\; \\frac{60000}{2^{1}} \\;=\\; 30000, \\quad w_1 \\;=\\; 30000, \\quad h_1 \\;=\\; 30000.\n$$\n- For $\\ell=2$:\n$$\nN_2 \\;=\\; \\frac{60000}{2^{2}} \\;=\\; 15000, \\quad w_2 \\;=\\; 15000, \\quad h_2 \\;=\\; 15000.\n$$\n- For $\\ell=3$:\n$$\nN_3 \\;=\\; \\frac{60000}{2^{3}} \\;=\\; 7500, \\quad w_3 \\;=\\; 7500, \\quad h_3 \\;=\\; 7500.\n$$\n\nCollecting the ordered sequence $(w_0,h_0,w_1,h_1,w_2,h_2,w_3,h_3)$ as requested yields\n$$\n(60000,\\,60000,\\,30000,\\,30000,\\,15000,\\,15000,\\,7500,\\,7500).\n$$\nAll entries are integers, consistent with the assumption that downsampling yields exact integer pixel counts.",
            "answer": "$$\\boxed{\\begin{pmatrix}60000  60000  30000  30000  15000  15000  7500  7500\\end{pmatrix}}$$"
        },
        {
            "introduction": "In the real world, no two HE-stained slides are exactly alike, with variations in color and intensity posing a significant challenge for automated analysis algorithms. This hands-on coding exercise guides you through implementing a classic technique, Reinhard stain normalization, to standardize images in a perceptually uniform color space. By writing the code to transform a source image's color profile to match a target, you will gain practical experience in a crucial pre-processing step for robust computational pathology .",
            "id": "4948985",
            "problem": "You are given two synthetic Whole Slide Image (WSI) patches per test case that represent Hematoxylin and Eosin (HE) stained tissue with different stain intensities. Your task is to implement Reinhard stain normalization in CIE $L^*a^*b^*$ (Lab) color space to transform the source patch to match the target patch's color distribution, and to quantify the effect of normalization on nuclei segmentation precision and recall using a fixed decision rule defined from the target patch. You must produce a complete, runnable program that constructs the synthetic images, performs the normalization, conducts segmentation, and computes the metrics for a provided test suite.\n\nFundamental base:\n- Color and color space conversion definitions from the International Commission on Illumination (CIE) and the standard red-green-blue (sRGB) space:\n  - sRGB gamma decoding: if $s \\in [0,1]$ denotes sRGB intensity per channel, then the linear intensity $l$ is given by\n    $$l = \\begin{cases}\n    \\dfrac{s}{12.92},  s \\le 0.04045 \\\\\n    \\left(\\dfrac{s+0.055}{1.055}\\right)^{2.4},  s  0.04045\n    \\end{cases}$$\n  - Linear RGB to CIE XYZ under the standard illuminant D65:\n    $$\\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix} =\n    \\begin{bmatrix}\n    0.4124564  0.3575761  0.1804375 \\\\\n    0.2126729  0.7151522  0.0721750 \\\\\n    0.0193339  0.1191920  0.9503041\n    \\end{bmatrix}\n    \\begin{bmatrix} R_{\\text{lin}} \\\\ G_{\\text{lin}} \\\\ B_{\\text{lin}} \\end{bmatrix}$$\n  - CIE XYZ to CIE $L^*a^*b^*$ with white point $(X_n, Y_n, Z_n)$ for D65: $X_n = 0.95047$, $Y_n = 1.0$, $Z_n = 1.08883$. Let $\\epsilon = \\dfrac{216}{24389}$ and $\\kappa = \\dfrac{24389}{27}$. Define\n    $$f(t) = \\begin{cases}\n    t^{1/3},  t  \\epsilon \\\\\n    \\dfrac{\\kappa t + 16}{116},  \\text{otherwise}\n    \\end{cases}$$\n    Then\n    $$L^* = 116 f\\!\\left(\\dfrac{Y}{Y_n}\\right) - 16,\\quad\n    a^* = 500\\left[f\\!\\left(\\dfrac{X}{X_n}\\right) - f\\!\\left(\\dfrac{Y}{Y_n}\\right)\\right],\\quad\n    b^* = 200\\left[f\\!\\left(\\dfrac{Y}{Y_n}\\right) - f\\!\\left(\\dfrac{Z}{Z_n}\\right)\\right].$$\n- Reinhard normalization in Lab space:\n  For each channel $c \\in \\{L^*, a^*, b^*\\}$, let $\\mu_{\\text{src},c}$ and $\\sigma_{\\text{src},c}$ be the mean and standard deviation of the source patch channel, and $\\mu_{\\text{tgt},c}$ and $\\sigma_{\\text{tgt},c}$ be the corresponding statistics of the target patch channel. The normalized source channel is\n  $$I_{\\text{norm},c} = \\left(\\dfrac{I_{\\text{src},c} - \\mu_{\\text{src},c}}{\\max(\\sigma_{\\text{src},c}, \\varepsilon)}\\right)\\sigma_{\\text{tgt},c} + \\mu_{\\text{tgt},c},$$\n  where $\\varepsilon$ is a small positive constant to prevent division by zero.\n- Segmentation decision rule and metrics:\n  - A fixed threshold on the blue-yellow channel $b^*$ is used to produce a binary prediction mask: pixels are classified as nuclei if $b^*  b_t$.\n  - The threshold $b_t$ is computed from the target patch as the midpoint between the mean $b^*$ of nuclei and the mean $b^*$ of non-nuclei background computed from the target ground-truth mask:\n    $$b_t = \\dfrac{\\overline{b^*}_{\\text{nuc}} + \\overline{b^*}_{\\text{bg}}}{2}.$$\n  - If either the nuclei or background class is empty in the target patch, compute $b_t$ using Otsu's method on the target $b^*$ channel histogram with $256$ bins to maximize the between-class variance.\n  - Precision and recall are defined as follows for predicted mask $\\hat{M}$ and ground truth $M$:\n    $$\\text{TP} = \\sum (\\hat{M} \\land M),\\quad \\text{FP} = \\sum (\\hat{M} \\land \\lnot M),\\quad \\text{FN} = \\sum (\\lnot \\hat{M} \\land M),$$\n    $$\\text{precision} = \\begin{cases}\n    \\dfrac{\\text{TP}}{\\text{TP} + \\text{FP}},  \\text{TP} + \\text{FP}  0 \\\\\n    1.0,  \\text{otherwise}\n    \\end{cases},\\quad\n    \\text{recall} = \\begin{cases}\n    \\dfrac{\\text{TP}}{\\text{TP} + \\text{FN}},  \\text{TP} + \\text{FN}  0 \\\\\n    1.0,  \\text{otherwise}\n    \\end{cases}.$$\n\nImplementation requirements:\n1. Construct synthetic source and target sRGB images of size $N \\times N$ with predefined nuclei masks (circular nuclei) and homogeneous background using the parameters below. Intensities must be treated as sRGB values in $[0,1]$.\n2. Convert both source and target images to CIE $L^*a^*b^*$, compute Reinhard normalization to map source to target in $L^*a^*b^*$, and convert back to sRGB if needed only for sanity checks; segmentation will be performed directly in $L^*a^*b^*$.\n3. Compute the threshold $b_t$ from the target patch using the ground truth mask and then classify nuclei in the source patch before normalization and after normalization using the same $b_t$.\n4. Compute precision and recall for both the pre-normalization and post-normalization predictions against the source ground-truth mask.\n\nTest suite:\nUse three parameter sets that define the image size, nuclei geometry, and sRGB colors for nuclei and background in source and target patches.\n\n- Test Case 1 (general case):\n  - Image size $N = 128$.\n  - Nuclei mask: union of three circles with centers $(32, 32)$, $(96, 40)$, $(64, 96)$ and radii $12$, $10$, $14$ pixels, respectively.\n  - Target sRGB colors (nuclei/background): nuclei $\\mathbf{c}_{\\text{tgt,nuc}} = [0.35, 0.28, 0.60]$, background $\\mathbf{c}_{\\text{tgt,bg}} = [0.95, 0.78, 0.86]$.\n  - Source sRGB colors (nuclei/background, faded and shifted): nuclei $\\mathbf{c}_{\\text{src,nuc}} = [0.50, 0.45, 0.68]$, background $\\mathbf{c}_{\\text{src,bg}} = [0.98, 0.86, 0.90]$.\n\n- Test Case 2 (edge case: no nuclei):\n  - Image size $N = 128$.\n  - Nuclei mask: empty set (no nuclei).\n  - Target sRGB colors: background only $\\mathbf{c}_{\\text{tgt,bg}} = [0.95, 0.78, 0.86]$.\n  - Source sRGB colors: background only $\\mathbf{c}_{\\text{src,bg}} = [0.97, 0.80, 0.88]$.\n\n- Test Case 3 (boundary case: identical source and target):\n  - Image size $N = 128$.\n  - Nuclei mask: same as Test Case 1.\n  - Target sRGB colors: nuclei $\\mathbf{c}_{\\text{tgt,nuc}} = [0.35, 0.28, 0.60]$, background $\\mathbf{c}_{\\text{tgt,bg}} = [0.95, 0.78, 0.86]$.\n  - Source sRGB colors: identical to target, nuclei $\\mathbf{c}_{\\text{src,nuc}} = [0.35, 0.28, 0.60]$, background $\\mathbf{c}_{\\text{src,bg}} = [0.95, 0.78, 0.86]$.\n\nAnswer specification and final output format:\n- For each test case, compute four floats: precision before normalization, recall before normalization, precision after normalization, recall after normalization.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets in the following order:\n  $$[\\text{prec}_1^{\\text{pre}},\\text{rec}_1^{\\text{pre}},\\text{prec}_1^{\\text{post}},\\text{rec}_1^{\\text{post}},\\text{prec}_2^{\\text{pre}},\\text{rec}_2^{\\text{pre}},\\text{prec}_2^{\\text{post}},\\text{rec}_2^{\\text{post}},\\text{prec}_3^{\\text{pre}},\\text{rec}_3^{\\text{pre}},\\text{prec}_3^{\\text{post}},\\text{rec}_3^{\\text{post}}].$$\nAll numerical results must be floats. No physical units are involved in this problem.",
            "solution": "The problem requires the implementation and evaluation of the Reinhard stain normalization algorithm on synthetic histological images. The evaluation is performed by quantifying the precision and recall of a simple nucleus segmentation task before and after normalization. The entire process is grounded in well-defined principles of color science and image analysis. The solution is structured into four main components: synthetic image generation, color space transformations, Reinhard normalization, and segmentation with metric evaluation.\n\n### Step 1: Synthetic Image Generation and Ground Truth\n\nThe first step is to generate the synthetic Whole Slide Image (WSI) patches for both source and target, along with the ground-truth nucleus mask. For each test case, an image of size $N \\times N$ is created. A boolean ground-truth mask, $M$, of the same size, is first generated based on the specified geometry of the nuclei. For Test Case 1 and 3, the mask is the union of pixels inside three circles defined by their centers $(x_c, y_c)$ and radii $r$. A pixel at coordinate $(x, y)$ is part of a nucleus if $(x-x_c)^2 + (y-y_c)^2 \\le r^2$ for any of the given circles. For Test Case 2, the mask is an empty set, represented by an $N \\times N$ matrix of all `False` values.\n\nOnce the mask $M$ is defined, the sRGB images are constructed. Each image is a $3$-channel array where pixels corresponding to nuclei (where $M$ is `True`) are assigned the specified nucleus sRGB color vector $\\mathbf{c}_{\\text{nuc}}$, and background pixels (where $M$ is `False`) are assigned the background color vector $\\mathbf{c}_{\\text{bg}}$. The sRGB values are specified in the range $[0, 1]$. This procedure results in a source image $I_{\\text{src}}$, a target image $I_{\\text{tgt}}$, and a shared ground-truth mask $M$.\n\n### Step 2: Color Space Transformations (sRGB to CIE $L^*a^*b^*$)\n\nThe Reinhard normalization and the segmentation are performed in the CIE $L^*a^*b^*$ (Lab) color space, which is designed to be more perceptually uniform than the sRGB space. This requires a three-step conversion process for both source and target images.\n\n**1. sRGB to Linear RGB:** The gamma compression of the sRGB standard is reversed to obtain linear intensity values. For each channel value $s \\in [0, 1]$, the linear value $l$ is calculated according to the piecewise function:\n$$l = \\begin{cases}\n\\dfrac{s}{12.92},  s \\le 0.04045 \\\\\n\\left(\\dfrac{s+0.055}{1.055}\\right)^{2.4},  s  0.04045\n\\end{cases}$$\nThis operation is applied element-wise to each channel of the sRGB image.\n\n**2. Linear RGB to CIE XYZ:** The linear RGB values are transformed into the CIE XYZ space, which is a device-independent representation of color. This is a linear transformation defined by a matrix multiplication, using the standard D65 illuminant:\n$$\\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix} =\n\\begin{bmatrix}\n0.4124564  0.3575761  0.1804375 \\\\\n0.2126729  0.7151522  0.0721750 \\\\\n0.0193339  0.1191920  0.9503041\n\\end{bmatrix}\n\\begin{bmatrix} R_{\\text{lin}} \\\\ G_{\\text{lin}} \\\\ B_{\\text{lin}} \\end{bmatrix}$$\nComputationally, the $N \\times N \\times 3$ image is reshaped to $(N^2) \\times 3$, right-multiplied by the transpose of the above matrix, and then reshaped back to $N \\times N \\times 3$.\n\n**3. CIE XYZ to CIE $L^*a^*b^*$:** The final conversion to Lab space is non-linear. First, the XYZ values are normalized by the D65 white point reference values: $X_n = 0.95047$, $Y_n = 1.0$, and $Z_n = 1.08883$. A function $f(t)$ is applied to each of the normalized ratios $X/X_n$, $Y/Y_n$, and $Z/Z_n$:\n$$f(t) = \\begin{cases}\nt^{1/3},  t  \\epsilon_{Lab} \\\\\n\\dfrac{\\kappa t + 16}{116},  \\text{otherwise}\n\\end{cases}$$\nwhere $\\epsilon_{Lab} = \\frac{216}{24389}$ and $\\kappa = \\frac{24389}{27}$. The $L^*$, $a^*$, and $b^*$ components are then computed as:\n$$L^* = 116 f\\!\\left(\\dfrac{Y}{Y_n}\\right) - 16$$\n$$a^* = 500\\left[f\\!\\left(\\dfrac{X}{X_n}\\right) - f\\!\\left(\\dfrac{Y}{Y_n}\\right)\\right]$$\n$$b^* = 200\\left[f\\!\\left(\\dfrac{Y}{Y_n}\\right) - f\\!\\left(\\dfrac{Z}{Z_n}\\right)\\right]$$\nThis full pipeline is applied to both the source and target images to yield $I_{\\text{src,Lab}}$ and $I_{\\text{tgt,Lab}}$.\n\n### Step 3: Reinhard Stain Normalization\n\nThe core of the problem logic is to apply Reinhard normalization to the source image, forcing its color distribution to match that of the target image. This is done independently for each of the $L^*$, $a^*$, and $b^*$ channels. For a given channel $c$, the mean $\\mu_c$ and standard deviation $\\sigma_c$ are calculated across all pixels for both the source and target Lab images. The normalized source image channel, $I_{\\text{norm},c}$, is then computed pixel by pixel using the formula:\n$$I_{\\text{norm},c} = \\left(\\dfrac{I_{\\text{src},c} - \\mu_{\\text{src},c}}{\\max(\\sigma_{\\text{src},c}, \\epsilon_{norm})}\\right)\\sigma_{\\text{tgt},c} + \\mu_{\\text{tgt,c}}$$\nA small positive constant, $\\epsilon_{norm}$ (e.g., $10^{-6}$), is used to prevent division by zero in cases where a channel has no variation (i.e., $\\sigma_{\\text{src},c} = 0$), which occurs in the synthetic images of the test suite. This results in the normalized source image, $I_{\\text{norm,Lab}}$.\n\n### Step 4: Segmentation and Evaluation\n\nThe effect of normalization is quantified by comparing the performance of a fixed segmentation rule applied to the source image before and after normalization.\n\n**Segmentation Threshold:** The segmentation rule is a simple threshold on the blue-yellow channel, $b^*$. A pixel is classified as a nucleus if its $b^*$ value is less than a threshold, $b_t$. This threshold is determined from the target image and its ground-truth mask $M$.\n- Ordinarily, $b_t$ is the midpoint between the mean $b^*$ of target nuclei and the mean $b^*$ of the target background: $b_t = (\\overline{b^*}_{\\text{nuc}} + \\overline{b^*}_{\\text{bg}})/2$.\n- In edge cases where the target image lacks either nuclei or background pixels (as in Test Case 2), the problem specifies using Otsu's method on the target's $b^*$ channel histogram (with $256$ bins) to find an optimal threshold that maximizes between-class variance.\n\n**Prediction and Metrics:** The same threshold $b_t$ is used to generate two binary prediction masks from the source image:\n1.  $\\hat{M}_{\\text{pre}}$: from the original source Lab image, $I_{\\text{src,Lab}}$.\n2.  $\\hat{M}_{\\text{post}}$: from the normalized source Lab image, $I_{\\text{norm,Lab}}$.\n\nFor both predicted masks, we calculate true positives (TP), false positives (FP), and false negatives (FN) by comparing them to the ground-truth mask $M$:\n$$\\text{TP} = \\sum (\\hat{M} \\land M), \\quad \\text{FP} = \\sum (\\hat{M} \\land \\lnot M), \\quad \\text{FN} = \\sum (\\lnot \\hat{M} \\land M)$$\nFrom these counts, precision and recall are computed, with special handling for zero-denominator cases as specified:\n$$\\text{precision} = \\begin{cases} 1.0,  \\text{if } \\text{TP} + \\text{FP} = 0 \\\\ \\dfrac{\\text{TP}}{\\text{TP} + \\text{FP}},  \\text{otherwise} \\end{cases}$$\n$$\\text{recall} = \\begin{cases} 1.0,  \\text{if } \\text{TP} + \\text{FN} = 0 \\\\ \\dfrac{\\text{TP}}{\\text{TP} + \\text{FN}},  \\text{otherwise} \\end{cases}$$\nThis procedure yields four metrics per test case: pre-normalization precision, pre-normalization recall, post-normalization precision, and post-normalization recall. The final output is an aggregation of these metrics for all test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the complete pipeline for all test cases.\n    \"\"\"\n    \n    # --- Color Space Conversion Constants ---\n    XYZ_FROM_RGB = np.array([\n        [0.4124564, 0.3575761, 0.1804375],\n        [0.2126729, 0.7151522, 0.0721750],\n        [0.0193339, 0.1191920, 0.9503041]\n    ])\n    D65_WHITEPOINT = np.array([0.95047, 1.0, 1.08883])\n    LAB_EPSILON = 216 / 24389\n    LAB_KAPPA = 24389 / 27\n    \n    # --- Helper Functions ---\n    def generate_image_and_mask(N, circles, c_nuc, c_bg):\n        \"\"\"Generates a synthetic image and its ground truth mask.\"\"\"\n        coords = np.arange(N)\n        xx, yy = np.meshgrid(coords, coords)\n        mask = np.zeros((N, N), dtype=bool)\n        if circles:\n            for xc, yc, r in circles:\n                mask |= ((xx - xc)**2 + (yy - yc)**2) = r**2\n        \n        image = np.zeros((N, N, 3), dtype=np.float64)\n        if c_bg is not None:\n             image[~mask] = c_bg\n        if c_nuc is not None and np.any(mask):\n            image[mask] = c_nuc\n            \n        return image, mask\n\n    def srgb_to_linear(srgb):\n        \"\"\"Converts sRGB image to linear RGB.\"\"\"\n        return np.where(srgb = 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055)**2.4)\n\n    def linear_to_xyz(linear_rgb):\n        \"\"\"Converts linear RGB image to CIE XYZ.\"\"\"\n        h, w, c = linear_rgb.shape\n        reshaped_rgb = linear_rgb.reshape(-1, 3)\n        xyz = np.dot(reshaped_rgb, XYZ_FROM_RGB.T)\n        return xyz.reshape(h, w, 3)\n\n    def xyz_to_lab(xyz):\n        \"\"\"Converts CIE XYZ image to CIE L*a*b*.\"\"\"\n        norm_xyz = xyz / D65_WHITEPOINT\n        \n        def f(t):\n            return np.where(t  LAB_EPSILON, np.cbrt(t), (LAB_KAPPA * t + 16) / 116)\n        \n        f_xyz = f(norm_xyz)\n        \n        L_star = 116 * f_xyz[..., 1] - 16\n        a_star = 500 * (f_xyz[..., 0] - f_xyz[..., 1])\n        b_star = 200 * (f_xyz[..., 1] - f_xyz[..., 2])\n        \n        return np.stack([L_star, a_star, b_star], axis=-1)\n\n    def srgb_to_lab(srgb_img):\n        \"\"\"Full conversion pipeline from sRGB to CIE L*a*b*.\"\"\"\n        return xyz_to_lab(linear_to_xyz(srgb_to_linear(srgb_img)))\n        \n    def reinhard_normalize(source_lab, target_lab, epsilon_norm=1e-6):\n        \"\"\"Performs Reinhard normalization in Lab space.\"\"\"\n        src_mean = np.mean(source_lab, axis=(0, 1))\n        src_std = np.std(source_lab, axis=(0, 1))\n        tgt_mean = np.mean(target_lab, axis=(0, 1))\n        tgt_std = np.std(target_lab, axis=(0, 1))\n        \n        normalized_lab = np.copy(source_lab)\n        for i in range(3):\n            # Regularize std dev to prevent division by zero\n            safe_src_std = max(src_std[i], epsilon_norm)\n            normalized_lab[..., i] = ((source_lab[..., i] - src_mean[i]) / safe_src_std) * tgt_std[i] + tgt_mean[i]\n            \n        return normalized_lab\n        \n    def calculate_threshold(target_lab_b, target_mask):\n        \"\"\"Calculates segmentation threshold b_t from target b* channel.\"\"\"\n        nuclei_pixels = target_mask\n        background_pixels = ~target_mask\n        \n        has_nuclei = np.any(nuclei_pixels)\n        has_background = np.any(background_pixels)\n        \n        if has_nuclei and has_background:\n            b_star_nuc_mean = np.mean(target_lab_b[nuclei_pixels])\n            b_star_bg_mean = np.mean(target_lab_b[background_pixels])\n            return (b_star_nuc_mean + b_star_bg_mean) / 2\n        else:\n            # Otsu's method for single-class images\n            # For this problem's synthetic data, this results in a single-valued image.\n            # Any threshold will produce zero between-class variance.\n            # np.argmax on an array of zeros returns 0.\n            # Using a fixed range for b* values to make histogram binning stable.\n            b_range = (-128.0, 128.0)\n            bins = 256\n            hist, bin_edges = np.histogram(target_lab_b.flatten(), bins=bins, range=b_range)\n            bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n            \n            pixel_total = target_lab_b.size\n            \n            weight_b = np.cumsum(hist)\n            weight_f = pixel_total - weight_b\n            \n            mean_b = np.cumsum(hist * bin_centers)\n            mean_f = np.sum(hist * bin_centers) - mean_b\n\n            # To avoid division by zero\n            with np.errstate(divide='ignore', invalid='ignore'):\n              m_b = mean_b / weight_b\n              m_f = mean_f / weight_f\n            \n            # fill nans with 0\n            m_b[np.isnan(m_b)] = 0\n            m_f[np.isnan(m_f)] = 0\n            \n            between_class_variance = weight_b * weight_f * (m_b - m_f)**2\n            \n            # The maximum variance for a single-valued image will be 0.\n            # argmax returns the first occurrence of the maximum value.\n            optimal_k = np.argmax(between_class_variance)\n            return bin_centers[optimal_k]\n            \n    def calculate_metrics(predicted_mask, ground_truth_mask):\n        \"\"\"Calculates precision and recall.\"\"\"\n        tp = np.sum(predicted_mask  ground_truth_mask)\n        fp = np.sum(predicted_mask  ~ground_truth_mask)\n        fn = np.sum(~predicted_mask  ground_truth_mask)\n        \n        precision = 1.0 if (tp + fp) == 0 else tp / (tp + fp)\n        recall = 1.0 if (tp + fn) == 0 else tp / (tp + fn)\n        \n        return precision, recall\n\n    # --- Test Suite Definition ---\n    test_cases = [\n        { # Case 1\n            \"N\": 128,\n            \"circles\": [(32, 32, 12), (96, 40, 10), (64, 96, 14)],\n            \"target_colors\": {\"nuc\": [0.35, 0.28, 0.60], \"bg\": [0.95, 0.78, 0.86]},\n            \"source_colors\": {\"nuc\": [0.50, 0.45, 0.68], \"bg\": [0.98, 0.86, 0.90]},\n        },\n        { # Case 2\n            \"N\": 128,\n            \"circles\": [],\n            \"target_colors\": {\"nuc\": None, \"bg\": [0.95, 0.78, 0.86]},\n            \"source_colors\": {\"nuc\": None, \"bg\": [0.97, 0.80, 0.88]},\n        },\n        { # Case 3\n            \"N\": 128,\n            \"circles\": [(32, 32, 12), (96, 40, 10), (64, 96, 14)],\n            \"target_colors\": {\"nuc\": [0.35, 0.28, 0.60], \"bg\": [0.95, 0.78, 0.86]},\n            \"source_colors\": {\"nuc\": [0.35, 0.28, 0.60], \"bg\": [0.95, 0.78, 0.86]},\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # 1. Generate images and mask\n        source_srgb, source_mask = generate_image_and_mask(case[\"N\"], case[\"circles\"], case[\"source_colors\"][\"nuc\"], case[\"source_colors\"][\"bg\"])\n        target_srgb, target_mask = generate_image_and_mask(case[\"N\"], case[\"circles\"], case[\"target_colors\"][\"nuc\"], case[\"target_colors\"][\"bg\"])\n        \n        # 2. Convert to Lab space\n        source_lab = srgb_to_lab(source_srgb)\n        target_lab = srgb_to_lab(target_srgb)\n        \n        # 3. Calculate segmentation threshold from target\n        b_t = calculate_threshold(target_lab[..., 2], target_mask)\n        \n        # 4. Pre-normalization evaluation\n        pred_mask_pre = source_lab[..., 2]  b_t\n        prec_pre, rec_pre = calculate_metrics(pred_mask_pre, source_mask)\n        \n        # 5. Post-normalization evaluation\n        source_lab_norm = reinhard_normalize(source_lab, target_lab)\n        pred_mask_post = source_lab_norm[..., 2]  b_t\n        prec_post, rec_post = calculate_metrics(pred_mask_post, source_mask)\n        \n        all_results.extend([prec_pre, rec_pre, prec_post, rec_post])\n        \n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```"
        }
    ]
}