{
    "hands_on_practices": [
        {
            "introduction": "A fundamental challenge in designing public health programs is deciding who should receive an intervention. This practice guides you through the process of setting a rational, evidence-based threshold for initiating preventive cardiovascular therapy. By balancing the expected benefits of treatment (preventing disease) against its costs and harms (side effects, resource use), you will derive the specific 10-year risk level at which initiating therapy becomes a net positive, a core skill in health policy and resource allocation.",
            "id": "4969510",
            "problem": "A Ministry of Health in a low-income country plans to set a single initiation threshold for primary prevention therapy against major Cardiovascular Disease (CVD) events using World Health Organization (WHO) absolute risk charts. Adults whose estimated 10-year CVD risk $p$ exceeds a threshold $p^{\\ast}$ will be offered a once-daily fixed-dose combination antihypertensive for $10$ years. Assume one major CVD event per person at most over $10$ years, no discounting, and independence of harms and benefits, so that linearity of expectations applies.\n\nUnder program conditions, for an individual with baseline 10-year risk $p$:\n- The therapy yields a constant relative risk reduction $r$ in the probability of a first major CVD event over $10$ years.\n- Each first major CVD event leads to an average loss of $L$ Disability-Adjusted Life Years (DALYs), where Disability-Adjusted Life Year (DALY) is defined in the standard way as one lost year of healthy life.\n- Adverse effects attributable to the therapy occur with probability $q$ over $10$ years and cause an average loss of $h$ DALYs if they occur.\n- The daily pill imposes a small ongoing disutility of $d$ DALYs per person-year while taken.\n- Delivering the therapy uses health system resources with an opportunity cost equivalent to $c$ DALYs per treated person over $10$ years.\n\nStarting only from the definition of expected value and the definition of relative risk reduction, derive the condition on $p$ under which the expected net DALYs averted per person treated are positive. Then, find the risk threshold $p^{\\ast}$ that makes the expected net DALYs averted exactly zero.\n\nUsing the parameter values $r=0.25$, $L=8$, $q=0.02$, $h=0.3$, $d=0.003$, and $c=0.08$, compute $p^{\\ast}$. Round your final result to three significant figures. Express the threshold as a decimal fraction of 10-year risk (do not use a percent sign).",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Problem Goal:** To set a single initiation threshold, $p^{\\ast}$, for primary prevention therapy against major Cardiovascular Disease (CVD) events.\n- **Population:** Adults in a low-income country.\n- **Decision Rule:** Individuals with an estimated $10$-year CVD risk $p$ greater than $p^{\\ast}$ are treated.\n- **Treatment:** A once-daily fixed-dose combination antihypertensive for $10$ years.\n- **Assumptions:**\n  - At most one major CVD event per person over $10$ years.\n  - No discounting.\n  - Independence of harms and benefits, allowing for the linearity of expectations.\n- **Parameters for an individual with baseline $10$-year risk $p$:**\n  - $r$: constant relative risk reduction in the probability of a first major CVD event over $10$ years.\n  - $L$: average loss of Disability-Adjusted Life Years (DALYs) per first major CVD event.\n  - $q$: probability of adverse effects from therapy over $10$ years.\n  - $h$: average loss of DALYs if adverse effects occur.\n  - $d$: disutility of $d$ DALYs per person-year while on therapy.\n  - $c$: opportunity cost of $c$ DALYs per treated person over $10$ years.\n- **Tasks:**\n  1. Derive the condition on $p$ for which expected net DALYs averted per person treated are positive.\n  2. Find the risk threshold $p^{\\ast}$ where expected net DALYs averted are zero.\n  3. Compute $p^{\\ast}$ using the following values: $r=0.25$, $L=8$, $q=0.02$, $h=0.3$, $d=0.003$, and $c=0.08$.\n  4. Round the result to three significant figures and express it as a decimal fraction.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in health economics and decision analysis, specifically cost-effectiveness analysis using DALYs as the outcome measure.\n- **Scientifically Grounded:** The problem is based on established principles of epidemiology (absolute risk, relative risk reduction) and health economics (DALYs, opportunity cost, expected value). The model is a simplification of reality (e.g., no discounting, constant RRR) but does not violate any fundamental scientific or mathematical laws. It is scientifically realistic.\n- **Well-Posed:** The problem is self-contained, providing all necessary variables and a clear objective: to find the break-even risk threshold. The existence of a unique, stable, and meaningful solution is demonstrable.\n- **Objective:** The language is precise and quantitative. It uses standard, well-defined terms from the field of global health. There are no subjective or opinion-based claims.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is a valid, formalizable problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe objective is to find the condition under which the expected net Disability-Adjusted Life Years (DALYs) averted by the therapy are positive. This can be expressed as finding the condition for which the expected DALYs lost without treatment are greater than the expected DALYs lost with treatment. Let $\\Delta E[\\text{DALY}]$ be the expected net DALYs averted.\n\nLet's define the expected DALY loss in two scenarios: no treatment and treatment.\n\n1.  **Expected DALY loss without treatment:**\n    For an individual with a baseline $10$-year risk of a CVD event of $p$, the only source of DALY loss in the model is the event itself, which causes a loss of $L$ DALYs. By the definition of expected value:\n    $$E[\\text{loss}_{\\text{no treat}}] = p \\cdot L + (1-p) \\cdot 0 = pL$$\n\n2.  **Expected DALY loss with treatment:**\n    With treatment, there are multiple sources of DALY loss.\n    - **CVD events:** The therapy provides a relative risk reduction of $r$. The new probability of a CVD event, $p'$, is defined by the relation $r = \\frac{p - p'}{p}$, which gives $p' = p(1-r)$. The expected DALY loss from CVD events under treatment is $p'L = p(1-r)L$.\n    - **Adverse effects:** The therapy causes adverse effects with probability $q$, resulting in an average loss of $h$ DALYs. The expected DALY loss from adverse effects is $qh$.\n    - **Disutility of medication:** The daily pill imposes a disutility of $d$ DALYs per year. The therapy is taken for $10$ years. This is a certain cost, not a probabilistic one. The total DALY loss from disutility is $10d$.\n    - **Opportunity cost:** The health system resources used have an opportunity cost of $c$ DALYs per person treated. This is also a certain cost.\n    \n    The total expected DALY loss with treatment is the sum of these components, based on the assumption of independence and linearity of expectations:\n    $$E[\\text{loss}_{\\text{treat}}] = p(1-r)L + qh + 10d + c$$\n\n3.  **Net DALYs Averted:**\n    The expected net DALYs averted, $\\Delta E[\\text{DALY}]$, is the difference between the expected loss without treatment and the expected loss with treatment.\n    $$\\Delta E[\\text{DALY}] = E[\\text{loss}_{\\text{no treat}}] - E[\\text{loss}_{\\text{treat}}]$$\n    $$\\Delta E[\\text{DALY}] = pL - \\left( p(1-r)L + qh + 10d + c \\right)$$\n    $$\\Delta E[\\text{DALY}] = pL - pL + prL - qh - 10d - c$$\n    $$\\Delta E[\\text{DALY}] = prL - (qh + 10d + c)$$\n    This final expression represents the expected DALYs averted (the benefit, $prL$) minus the total expected DALYs incurred from therapy (the harms and costs, $qh + 10d + c$).\n\n4.  **Condition for Positive Net DALYs Averted:**\n    The therapy is beneficial if the expected net DALYs averted are positive.\n    $$\\Delta E[\\text{DALY}] > 0$$\n    $$prL - (qh + 10d + c) > 0$$\n    $$prL > qh + 10d + c$$\n    The condition on the baseline risk $p$ is:\n    $$p > \\frac{qh + 10d + c}{rL}$$\n\n5.  **Derivation of the Risk Threshold $p^{\\ast}$:**\n    The risk threshold $p^{\\ast}$ is the specific value of $p$ at which the expected net DALYs averted are exactly zero, meaning the benefits equal the costs.\n    $$p^{\\ast}rL - (qh + 10d + c) = 0$$\n    Solving for $p^{\\ast}$:\n    $$p^{\\ast} = \\frac{qh + 10d + c}{rL}$$\n\n6.  **Numerical Calculation of $p^{\\ast}$:**\n    We substitute the given parameter values: $r=0.25$, $L=8$, $q=0.02$, $h=0.3$, $d=0.003$, and $c=0.08$.\n\n    First, calculate the numerator (total harms and costs):\n    $$\\text{Numerator} = qh + 10d + c = (0.02)(0.3) + 10(0.003) + 0.08$$\n    $$\\text{Numerator} = 0.006 + 0.030 + 0.080 = 0.116$$\n\n    Next, calculate the denominator (benefit per unit of risk):\n    $$\\text{Denominator} = rL = (0.25)(8) = 2$$\n\n    Finally, compute the threshold $p^{\\ast}$:\n    $$p^{\\ast} = \\frac{0.116}{2} = 0.058$$\n\n    The problem requires rounding the result to three significant figures. The calculated value $0.058$ has two significant figures. To express it with three, we add a trailing zero, which is significant in this context.\n    $$p^{\\ast} = 0.0580$$\n    This is the 10-year risk threshold, expressed as a decimal fraction.",
            "answer": "$$ \\boxed{0.0580} $$"
        },
        {
            "introduction": "The risk models used to make treatment decisions are only as good as the data they are built on. This exercise delves into the crucial statistical challenge of regression dilution bias, where random measurement error in risk factors like blood pressure can cause us to underestimate their true health impact. By working through this problem, you will learn how to use data from a validation study to correct for this bias, ensuring our risk estimates are as accurate as possible.",
            "id": "4969559",
            "problem": "A population-based cohort study in a Low- and Middle-Income Countries (LMICs) setting enrolled $10{,}000$ adults and followed them for incident cardiovascular disease (CVD). The exposure of interest was systolic blood pressure (SBP), measured once at baseline. The primary analysis used a proportional hazards model with a log-linear specification for SBP and reported that the estimated log hazard ratio increased by $0.12$ per $10$ mmHg increment in SBP measured at baseline.\n\nTo assess and correct for regression dilution arising from within-person variability and random measurement error in SBP, a validation substudy was conducted in a simple random sample of $300$ cohort participants. Each participant in the substudy had $m=2$ replicate SBP measurements taken $1$ year apart under comparable conditions. A one-way random effects analysis of variance on the replicate SBP measurements yielded a mean square within-person of $MS_{W}=100$ (in $\\text{mmHg}^2$) and a mean square between-person of $MS_{B}=400$ (in $\\text{mmHg}^2$).\n\nAssume the following foundational conditions:\n- The true SBP for participant $i$, denoted $T_{i}$, is stable over the short term of replicates and varies across participants with variance $\\sigma_{b}^{2}$.\n- Each observed replicate $X_{ij}$ satisfies the classical additive measurement error model $X_{ij}=T_{i}+U_{ij}$ with $U_{ij}$ independent of $T_{i}$ and the outcome, and with variance $\\sigma_{w}^{2}$ identical across replicates.\n- The log hazard is a linear function of the true SBP in units of $10$ mmHg, and the proportional hazards analysis using baseline SBP is interpreted in terms of the corresponding regression of a linear predictor on a mismeasured exposure.\n\nUsing only fundamental definitions of variance decomposition, properties of covariance under additive independent noise, and the definition of a regression slope as a covariance-to-variance ratio, derive a correction for the baseline-analysis log hazard slope that accounts for regression dilution due to within-person variation and random error. Then compute the corrected hazard ratio for a $10$ mmHg higher true SBP.\n\nRound your final answer to four significant figures. Express the hazard ratio as a pure number (dimensionless).",
            "solution": "The problem requires us to correct an estimated log hazard ratio from a proportional hazards model for the effects of regression dilution due to measurement error in the exposure, systolic blood pressure (SBP). We will first derive the general correction formula based on the principles of linear regression and variance decomposition, and then use the data from the validation substudy to compute the corrected hazard ratio.\n\nLet $T$ be the true long-term average SBP for a participant, and let $X$ be the single baseline measurement of SBP. The problem states that the observed measurement $X$ is related to the true value $T$ by the classical additive measurement error model:\n$$X = T + U$$\nwhere $U$ is a random error term. Key assumptions are that $U$ is independent of $T$, $\\mathbb{E}[U] = 0$, and the variance of $U$ is $\\text{Var}(U) = \\sigma_w^2$. The variance of the true SBP across the population is $\\text{Var}(T) = \\sigma_b^2$.\n\nThe true underlying model assumes that the log hazard of cardiovascular disease (CVD), denoted by $L$, is a linear function of the true SBP, $T$:\n$$L = \\alpha + \\beta_{\\text{true}} T$$\nHowever, the analysis was conducted using the observed, mismeasured SBP, $X$. The proportional hazards model effectively performs a regression of the log hazard on $X$. The estimated log hazard ratio, $\\hat{\\beta}_{\\text{obs}}$, is an estimate of the slope in the regression of $L$ on $X$:\n$$L \\approx \\alpha' + \\beta_{\\text{obs}} X$$\nThe slope of a simple linear regression of a dependent variable $Y$ (here, $L$) on a predictor $Z$ is given by the ratio of their covariance to the variance of the predictor, $\\text{Cov}(Y, Z) / \\text{Var}(Z)$.\n\nThe true slope is:\n$$\\beta_{\\text{true}} = \\frac{\\text{Cov}(L, T)}{\\text{Var}(T)}$$\nThe observed slope is:\n$$\\beta_{\\text{obs}} = \\frac{\\text{Cov}(L, X)}{\\text{Var}(X)}$$\nWe can relate these two expressions. First, let's analyze the covariance term, $\\text{Cov}(L, X)$.\nSince $X = T+U$ and the error $U$ is independent of the true value $T$ and the outcome (implying it's independent of the log hazard $L$), we have:\n$$\\text{Cov}(L, X) = \\text{Cov}(L, T+U) = \\text{Cov}(L, T) + \\text{Cov}(L, U)$$\nBecause $L$ is a function of $T$, and $U$ is independent of $T$ and the factors determining the outcome, $\\text{Cov}(L, U) = 0$. Therefore:\n$$\\text{Cov}(L, X) = \\text{Cov}(L, T)$$\nNext, let's analyze the variance term, $\\text{Var}(X)$. Since $T$ and $U$ are independent:\n$$\\text{Var}(X) = \\text{Var}(T+U) = \\text{Var}(T) + \\text{Var}(U) = \\sigma_b^2 + \\sigma_w^2$$\nSubstituting these back into the expression for $\\beta_{\\text{obs}}$:\n$$\\beta_{\\text{obs}} = \\frac{\\text{Cov}(L, T)}{\\text{Var}(T) + \\text{Var}(U)} = \\frac{\\text{Cov}(L, T)}{\\sigma_b^2 + \\sigma_w^2}$$\nWe can rewrite this in terms of $\\beta_{\\text{true}}$:\n$$\\beta_{\\text{obs}} = \\left(\\frac{\\text{Cov}(L, T)}{\\text{Var}(T)}\\right) \\left(\\frac{\\text{Var}(T)}{\\text{Var}(T) + \\text{Var}(U)}\\right) = \\beta_{\\text{true}} \\left(\\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_w^2}\\right)$$\nThe term $\\lambda = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_w^2}$ is the regression dilution factor, also known as the reliability coefficient or intraclass correlation coefficient. It represents the proportion of the total variance in the observed measurements that is due to true between-person differences. Since $\\sigma_w^2 > 0$, we have $\\lambda  1$, which means $|\\beta_{\\text{obs}}|  |\\beta_{\\text{true}}|$. This attenuation of the regression slope towards zero is known as regression dilution.\n\nThe correction formula to find the true slope from the observed slope is:\n$$\\beta_{\\text{true}} = \\frac{\\beta_{\\text{obs}}}{\\lambda} = \\beta_{\\text{obs}} \\left(\\frac{\\sigma_b^2 + \\sigma_w^2}{\\sigma_b^2}\\right)$$\nTo apply this correction, we need to estimate the variance components $\\sigma_b^2$ (between-person variance, i.e., $\\text{Var}(T)$) and $\\sigma_w^2$ (within-person variance, i.e., $\\text{Var}(U)$). The problem provides results from a one-way random effects analysis of variance (ANOVA) on $m=2$ replicate measurements for $300$ participants. The expected values of the mean squares are:\n$$E(MS_W) = \\sigma_w^2$$\n$$E(MS_B) = \\sigma_w^2 + m \\sigma_b^2$$\nwhere $MS_W$ is the mean square within-person and $MS_B$ is the mean square between-person. We use the observed mean squares as estimates for their expected values.\nGiven data:\n$MS_{W} = 100$ mmHg$^2$\n$MS_{B} = 400$ mmHg$^2$\n$m = 2$\n\nFrom these, we can estimate the variance components:\nAn estimate for the within-person variance is $\\hat{\\sigma}_w^2 = MS_W = 100$.\nUsing the formula for $MS_B$, we can solve for an estimate of the between-person variance, $\\hat{\\sigma}_b^2$:\n$$MS_B = \\hat{\\sigma}_w^2 + m \\hat{\\sigma}_b^2$$\n$$400 = 100 + 2 \\hat{\\sigma}_b^2$$\n$$2 \\hat{\\sigma}_b^2 = 400 - 100 = 300$$\n$$\\hat{\\sigma}_b^2 = \\frac{300}{2} = 150$$\nSo, we have $\\hat{\\sigma}_b^2 = 150$ mmHg$^2$ and $\\hat{\\sigma}_w^2 = 100$ mmHg$^2$.\n\nNow we can estimate the reliability coefficient $\\lambda$:\n$$\\hat{\\lambda} = \\frac{\\hat{\\sigma}_b^2}{\\hat{\\sigma}_b^2 + \\hat{\\sigma}_w^2} = \\frac{150}{150 + 100} = \\frac{150}{250} = \\frac{3}{5} = 0.6$$\nThe problem states that the observed log hazard ratio increases by $0.12$ per $10$ mmHg increment in SBP. This is our observed slope, $\\hat{\\beta}_{\\text{obs}, 10} = 0.12$.\n\nWe apply the correction to find the true log hazard ratio for a $10$ mmHg increment:\n$$\\hat{\\beta}_{\\text{true}, 10} = \\frac{\\hat{\\beta}_{\\text{obs}, 10}}{\\hat{\\lambda}} = \\frac{0.12}{0.6} = 0.2$$\nThe corrected log hazard ratio for a $10$ mmHg increase in true SBP is $0.2$.\n\nThe final step is to compute the corrected hazard ratio (HR) for this increment:\n$$HR_{\\text{true}, 10} = \\exp(\\hat{\\beta}_{\\text{true}, 10}) = \\exp(0.2)$$\nCalculating the numerical value:\n$$\\exp(0.2) \\approx 1.221402758...$$\nRounding to four significant figures, we get $1.221$.\nThis is the corrected hazard ratio for a $10$ mmHg higher true SBP.",
            "answer": "$$\\boxed{1.221}$$"
        },
        {
            "introduction": "Implementing a large-scale screening program, especially in a low-resource setting, involves difficult choices. This practice presents a common operational dilemma: choosing between two different diagnostic tests for diabetes when the budget for treatment is limited. You will analyze how test characteristics like sensitivity and specificity interact with a fixed treatment capacity to determine which strategy ultimately averts more cardiovascular events, revealing that the \"best\" test on paper may not be the best choice in practice.",
            "id": "4969546",
            "problem": "A Ministry of Health in a low- and middle-income country (LMIC) is planning a national screening campaign to reduce five-year cardiovascular events by identifying undiagnosed diabetes and immediately initiating a standardized cardio-protective treatment package (for example, statin plus blood pressure control for those who test positive). Two index tests are under consideration: Hemoglobin A1c (HbA1c) and Fasting Plasma Glucose (FPG). The program will screen a cohort of $10{,}000$ adults aged $40$â€“$65$ years. Due to budget constraints, the program can treat at most $800$ individuals who screen positive.\n\nUse only the following foundational elements in your reasoning: the definitions of disease prevalence, sensitivity, and specificity; the law of total probability; and the definition of relative risk reduction as a multiplicative factor on baseline risk. Assume independence across individuals, and that treatment has no effect on non-diabetics who are treated in error.\n\nEpidemiologic and program parameters are as follows:\n- Prevalence of undiagnosed diabetes in the target group: $p = 0.12$.\n- Five-year cardiovascular event risk among untreated diabetics: $R_{d} = 0.18$.\n- Five-year cardiovascular event risk among non-diabetics (treated or untreated): $R_{nd} = 0.05$; however, non-diabetics who receive treatment experience no change in their risk under this program assumption.\n- Relative risk reduction in five-year cardiovascular events for true diabetics who receive treatment: $r = 0.30$ (so treated risk becomes $(1 - r)R_d$).\n- For Hemoglobin A1c (HbA1c): sensitivity $Se_{H} = 0.55$, specificity $Sp_{H} = 0.98$.\n- For Fasting Plasma Glucose (FPG): sensitivity $Se_{F} = 0.70$, specificity $Sp_{F} = 0.94$.\n\nOperational rule under the treatment cap: if the number of test positives for a given test is less than or equal to $800$, all test-positive individuals are treated. If the number of test positives exceeds $800$, a fraction $q$ of all positives equal to \n$$q = \\frac{800}{\\text{number of positives}}$$ \nis treated, applied uniformly at random across positives.\n\nStarting from the core definitions and probability laws listed above, derive an expression for the expected number of five-year cardiovascular events averted per $10{,}000$ people screened for each test, and compute the expected additional events averted by using HbA1c instead of FPG. Express your final answer as a single real number equal to the expected number of additional cardiovascular events averted per $10{,}000$ people screened (HbA1c minus FPG). Round your answer to four significant figures.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in established epidemiological principles, well-posed with a complete and consistent set of givens, and is objective in its language. A unique, meaningful solution can be derived.\n\nThe objective is to calculate the expected additional cardiovascular events averted by using the Hemoglobin A1c (HbA1c) test instead of the Fasting Plasma Glucose (FPG) test for screening. This is the difference between the expected events averted by each test strategy.\n\nLet's define the symbolic parameters provided:\n- Total number of individuals in the screening cohort: $N = 10,000$.\n- Prevalence of undiagnosed diabetes: $p = P(D) = 0.12$. The probability of not having diabetes is $P(D^c) = 1-p = 0.88$.\n- Five-year cardiovascular event risk in untreated diabetics: $R_d = 0.18$.\n- Five-year cardiovascular event risk in non-diabetics: $R_{nd} = 0.05$.\n- Relative risk reduction from treatment for diabetics: $r = 0.30$. The risk for a treated diabetic is $R_{d, \\text{treat}} = (1-r)R_d$.\n- Treatment capacity cap: $C = 800$.\n- Test parameters for HbA1c (denoted by subscript $H$): sensitivity $Se_H = 0.55$, specificity $Sp_H = 0.98$.\n- Test parameters for FPG (denoted by subscript $F$): sensitivity $Se_F = 0.70$, specificity $Sp_F = 0.94$.\n\nThe core of the problem is to determine the expected number of cardiovascular events averted, $E_{\\text{averted}}$, for each testing strategy. According to the problem statement, treatment is effective only for true diabetics and has no effect on non-diabetics. Therefore, events are averted only when a true diabetic is correctly identified (a true positive, $TP$) and subsequently receives treatment.\n\nThe absolute risk reduction ($ARR$) for a treated diabetic is the difference between the risk without treatment and the risk with treatment:\n$$ARR = R_d - R_{d, \\text{treat}} = R_d - (1-r)R_d = rR_d$$\n\nThe total number of events averted under a given testing strategy $i$ (where $i \\in \\{H, F\\}$) is the product of the expected number of treated true diabetics and the absolute risk reduction per person.\n$$E_{\\text{averted}, i} = E[\\text{Number of treated } TP_i] \\times ARR$$\n\nLet's first find the expected number of true positives ($TP_i$) and false positives ($FP_i$) for a generic test $i$.\nUsing the definition of sensitivity, $Se_i = P(T_i^+ | D)$, the expected number of true positives is:\n$$E[TP_i] = N \\times P(T_i^+ \\text{ and } D) = N \\times P(T_i^+ | D) \\times P(D) = N \\cdot Se_i \\cdot p$$\nUsing the definition of specificity, $Sp_i = P(T_i^- | D^c)$, the probability of a positive test in a non-diabetic is $P(T_i^+ | D^c) = 1 - Sp_i$. The expected number of false positives is:\n$$E[FP_i] = N \\times P(T_i^+ \\text{ and } D^c) = N \\times P(T_i^+ | D^c) \\times P(D^c) = N \\cdot (1 - Sp_i) \\cdot (1-p)$$\n\nThe total expected number of individuals who test positive, $N_{\\text{pos}, i}$, is the sum of expected true and false positives:\n$$N_{\\text{pos}, i} = E[TP_i] + E[FP_i]$$\n\nThe operational rule for treatment is determined by comparing $N_{\\text{pos}, i}$ to the treatment cap $C$. The fraction of test-positive individuals who receive treatment, $q_i$, is:\n$$q_i = \\min\\left(1, \\frac{C}{N_{\\text{pos}, i}}\\right)$$\n\nThe expected number of treated true diabetics is the expected number of true positives, $E[TP_i]$, multiplied by this fraction $q_i$.\n$$E[\\text{Number of treated } TP_i] = q_i \\cdot E[TP_i]$$\n\nCombining these findings, the general expression for the expected number of events averted for test $i$ is:\n$$E_{\\text{averted}, i} = \\min\\left(1, \\frac{C}{N \\cdot (Se_i \\cdot p + (1 - Sp_i) \\cdot (1-p))}\\right) \\cdot (N \\cdot Se_i \\cdot p) \\cdot (r \\cdot R_d)$$\n\nNow, we will apply this formula to each test.\n\n**1. Calculation for HbA1c (Test H)**\nFirst, we calculate the expected number of true and false positives.\n$$E[TP_H] = N \\cdot Se_H \\cdot p = 10,000 \\times 0.55 \\times 0.12 = 660$$\n$$E[FP_H] = N \\cdot (1 - Sp_H) \\cdot (1-p) = 10,000 \\times (1 - 0.98) \\times (1 - 0.12) = 10,000 \\times 0.02 \\times 0.88 = 176$$\nThe total expected number of positives is:\n$$N_{\\text{pos}, H} = E[TP_H] + E[FP_H] = 660 + 176 = 836$$\nSince $N_{\\text{pos}, H} = 836 > C = 800$, not all positives can be treated. The fraction treated is:\n$$q_H = \\frac{C}{N_{\\text{pos}, H}} = \\frac{800}{836}$$\nThe expected number of events averted using HbA1c is:\n$$E_{\\text{averted}, H} = q_H \\cdot E[TP_H] \\cdot r \\cdot R_d = \\frac{800}{836} \\times 660 \\times 0.30 \\times 0.18 \\approx 34.105263$$\n\n**2. Calculation for FPG (Test F)**\nNext, we perform the same calculation for the FPG test.\n$$E[TP_F] = N \\cdot Se_F \\cdot p = 10,000 \\times 0.70 \\times 0.12 = 840$$\n$$E[FP_F] = N \\cdot (1 - Sp_F) \\cdot (1-p) = 10,000 \\times (1 - 0.94) \\times (1 - 0.12) = 10,000 \\times 0.06 \\times 0.88 = 528$$\nThe total expected number of positives is:\n$$N_{\\text{pos}, F} = E[TP_F] + E[FP_F] = 840 + 528 = 1368$$\nSince $N_{\\text{pos}, F} = 1368 > C = 800$, the fraction treated is:\n$$q_F = \\frac{C}{N_{\\text{pos}, F}} = \\frac{800}{1368}$$\nThe expected number of events averted using FPG is:\n$$E_{\\text{averted}, F} = q_F \\cdot E[TP_F] \\cdot r \\cdot R_d = \\frac{800}{1368} \\times 840 \\times 0.30 \\times 0.18 \\approx 26.526315$$\n\n**3. Final Comparison**\nThe expected additional events averted by using HbA1c instead of FPG is the difference:\n$$\\Delta E_{\\text{averted}} = E_{\\text{averted}, H} - E_{\\text{averted}, F}$$\n$$\\Delta E_{\\text{averted}} \\approx 34.105263 - 26.526315 = 7.578948$$\nThe problem requires rounding the final answer to four significant figures.\n$$\\Delta E_{\\text{averted}} \\approx 7.579$$\nThe analysis demonstrates that, despite FPG having a higher sensitivity, the HbA1c test averts more cardiovascular events under the specified resource constraint. This is because FPG's lower specificity generates a large number of false positives, which exhausts the treatment budget. The rationing of treatment ($q_F$ is much smaller than $q_H$) is so severe that fewer true positives are ultimately treated compared to the more specific HbA1c test.",
            "answer": "$$\\boxed{7.579}$$"
        }
    ]
}