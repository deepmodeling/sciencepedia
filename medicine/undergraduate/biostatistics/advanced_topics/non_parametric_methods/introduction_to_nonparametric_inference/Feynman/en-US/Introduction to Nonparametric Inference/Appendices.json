{
    "hands_on_practices": [
        {
            "introduction": "This first exercise brings us to the very foundation of nonparametric inference: the randomization test. We explore a scenario in a clinical trial where the treatment assignment itself provides the basis for constructing a hypothesis test . By embracing the sharp null hypothesis—the idea that the treatment has no effect on any individual—we can determine the significance of our results by comparing them to all possible outcomes that could have occurred under the study's specific randomization scheme. This practice builds intuition for how probability enters our analysis not through assumed population distributions, but through the known, physical process of randomization.",
            "id": "4920236",
            "problem": "A biostatistics team runs a Randomized Controlled Trial (RCT) to evaluate whether a new treatment increases a continuous biomarker response relative to control. Randomization uses permuted blocks with unequal allocation: within each block of size $3$, exactly $2$ participants are assigned to treatment and $1$ to control. There are $3$ blocks. The three post-treatment biomarker outcomes in each block, along with the actual observed assignment in this trial, are as follows.\nBlock $1$: outcomes $8$, $4$, $2$; the subjects with outcomes $8$ and $4$ were assigned to treatment, and the subject with outcome $2$ was assigned to control.\nBlock $2$: outcomes $7$, $6$, $1$; the subjects with outcomes $7$ and $6$ were assigned to treatment, and the subject with outcome $1$ was assigned to control.\nBlock $3$: outcomes $9$, $3$, $0$; the subjects with outcomes $9$ and $0$ were assigned to treatment, and the subject with outcome $3$ was assigned to control.\nYou will construct a randomization-based permutation test that respects the randomization scheme. Use the following fundamental base:\n- Under the null hypothesis $H_0$ of no treatment effect, the observed outcomes are fixed, and only the treatment labels are random according to the randomization mechanism.\n- To respect the randomization scheme, permutations must be restricted to relabelings that could have arisen from the actual design; here, within each block of size $3$, exactly $2$ labels must be treatment and $1$ label must be control.\nDefine the test statistic $T$ to be the difference in mean outcomes between the treatment group and the control group, $T = \\bar{Y}_T - \\bar{Y}_C$, and consider the one-sided alternative $H_1$ that the treatment increases the mean outcome. Construct the exact permutation distribution of $T$ under $H_0$ by enumerating all labelings consistent with the block randomization and the 2:1 allocation within each block, and compute the exact one-sided $p$-value $\\Pr(T \\ge T_{\\text{obs}} \\mid H_0)$. Express your final answer as a single fraction in lowest terms. No rounding is required.",
            "solution": "The problem requires the computation of an exact one-sided $p$-value for a permutation test based on a block randomization design. The null hypothesis, $H_0$, posits that the treatment has no effect on the biomarker outcomes. This is the sharp null hypothesis, which implies that the observed outcome for each participant would have been the same regardless of the treatment assignment they received. Under $H_0$, the set of all $9$ outcomes is considered fixed. The only source of randomness is the assignment of treatment labels, which follows the specified randomization scheme.\n\nThe randomization scheme consists of permuted blocks of size $3$. Within each of the $3$ blocks, $2$ participants are assigned to treatment and $1$ to control. The choice of which participant receives the control assignment within a block is random. For a block with $3$ participants, there are $\\binom{3}{1} = 3$ possible ways to assign $1$ control label and $2$ treatment labels. Since there are $3$ independent blocks, the total number of possible treatment assignments consistent with the randomization scheme is $3 \\times 3 \\times 3 = 27$. Under $H_0$, each of these $27$ assignments is equally likely, with probability $\\frac{1}{27}$.\n\nThe test statistic is defined as the difference in mean outcomes between the treatment group (T) and the control group (C): $T = \\bar{Y}_T - \\bar{Y}_C$. The total number of participants is $N=9$, with $N_T = 3 \\times 2 = 6$ in the treatment group and $N_C = 3 \\times 1 = 3$ in the control group.\n\nFirst, we calculate the observed value of the test statistic, $T_{\\text{obs}}$, based on the actual assignment given in the problem.\nThe outcomes in each block are:\nBlock $1$: $\\{8, 4, 2\\}$\nBlock $2$: $\\{7, 6, 1\\}$\nBlock $3$: $\\{9, 3, 0\\}$\n\nThe observed assignment is:\nTreatment group outcomes: $Y_T = \\{8, 4, 7, 6, 9, 0\\}$\nControl group outcomes: $Y_C = \\{2, 1, 3\\}$\n\nThe sum and mean for the observed treatment group are:\n$$ \\sum Y_T = 8 + 4 + 7 + 6 + 9 + 0 = 34 $$\n$$ \\bar{Y}_T = \\frac{34}{6} = \\frac{17}{3} $$\n\nThe sum and mean for the observed control group are:\n$$ \\sum Y_C = 2 + 1 + 3 = 6 $$\n$$ \\bar{Y}_C = \\frac{6}{3} = 2 $$\n\nThe observed test statistic is:\n$$ T_{\\text{obs}} = \\bar{Y}_T - \\bar{Y}_C = \\frac{17}{3} - 2 = \\frac{17}{3} - \\frac{6}{3} = \\frac{11}{3} $$\n\nThe one-sided $p$-value is the probability of observing a test statistic at least as extreme as $T_{\\text{obs}}$ under the null hypothesis, i.e., $p = \\Pr(T \\ge T_{\\text{obs}} \\mid H_0)$. To find this, we must consider all $27$ possible randomizations.\n\nWe can simplify the calculation by expressing $T$ in terms of the sum of the control outcomes, $\\sum Y_C$. The total sum of all outcomes is constant: $S_{\\text{total}} = (8+4+2) + (7+6+1) + (9+3+0) = 40$. Since $\\sum Y_T = 40 - \\sum Y_C$, the test statistic can be rewritten as:\n$$ T = \\frac{\\sum Y_T}{N_T} - \\frac{\\sum Y_C}{N_C} = \\frac{40 - \\sum Y_C}{6} - \\frac{\\sum Y_C}{3} = \\frac{40 - 3\\sum Y_C}{6} $$\nThe condition $T \\ge T_{\\text{obs}}$ is equivalent to $\\sum Y_C \\le 6$. Thus, the $p$-value is the proportion of the $27$ possible randomizations for which the sum of the control group outcomes is less than or equal to $6$.\n\nWe need to enumerate the combinations of control assignments that satisfy this condition. For each randomization, one outcome from each block is chosen to be the control outcome. Let $y_{c1}$, $y_{c2}$, and $y_{c3}$ be the control outcomes from Block $1$, Block $2$, and Block $3$ respectively.\nThe possible control outcomes are:\n- From Block $1$: $y_{c1} \\in \\{2, 4, 8\\}$\n- From Block $2$: $y_{c2} \\in \\{1, 6, 7\\}$\n- From Block $3$: $y_{c3} \\in \\{0, 3, 9\\}$\n\nWe find all combinations $(y_{c1}, y_{c2}, y_{c3})$ such that $y_{c1} + y_{c2} + y_{c3} \\le 6$.\n1.  $(2, 1, 0) \\implies \\text{sum} = 3 \\le 6$. This is one such randomization.\n2.  $(2, 1, 3) \\implies \\text{sum} = 6 \\le 6$. This is a second such randomization (this is the observed case).\n3.  $(4, 1, 0) \\implies \\text{sum} = 5 \\le 6$. This is a third such randomization.\n\nNo other combinations satisfy the condition. For example, if $y_{c1}=8$, the minimum sum is $8+1+0=9$. If $y_{c2}=6$, the minimum sum is $2+6+0=8$. Therefore, there are exactly $3$ possible randomizations (out of $27$) for which $\\sum Y_C \\le 6$.\n\nEach of these $3$ randomizations results in a test statistic $T$ greater than or equal to $T_{\\text{obs}}$. Since each of the $27$ randomizations is equally likely, the exact one-sided $p$-value is the ratio of the number of favorable outcomes to the total number of outcomes.\n$$ p \\text{-value} = \\frac{\\text{Number of permutations with } T \\ge T_{\\text{obs}}}{\\text{Total number of permutations}} = \\frac{3}{27} $$\nExpressing this as a fraction in lowest terms, we get:\n$$ p \\text{-value} = \\frac{1}{9} $$",
            "answer": "$$\\boxed{\\frac{1}{9}}$$"
        },
        {
            "introduction": "Moving from permutation tests on raw data, we now explore one of the most classic and powerful nonparametric tools: the Wilcoxon signed-rank test. This practice focuses on a matched-pairs study, a common design in biostatistics, and challenges us to test for a treatment effect using ranks instead of the original values . This approach not only frees us from normality assumptions but also introduces robust methods for handling common data issues like ties and zero-differences, providing a practical lesson in conducting inference with the messy data often encountered in real-world research.",
            "id": "4920252",
            "problem": "A matched-pairs biostatistics study evaluates the within-subject change in a continuous biomarker between baseline and 12 weeks after intervention. Let the paired differences be denoted by $d_{i} = \\text{post}_{i} - \\text{pre}_{i}$ for $i=1,\\dots,25$. Due to instrument rounding and a detection limit, several observed differences are exactly zero and multiple nonzero absolute differences are tied. The observed differences are\n$$\n\\{0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,2,\\,2,\\,3,\\,3,\\,6,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0\\}.\n$$\nAssume the standard conditions for the Wilcoxon signed-rank test hold under the null hypothesis of symmetry of the difference distribution about $0$, with zero differences removed and tied absolute differences assigned average ranks.\n\nUsing only fundamental probabilistic reasoning under the null hypothesis (random signs with equal probability and independence across nonzero pairs), perform the exact Wilcoxon signed-rank test and compute the two-sided mid-p value. Proceed by:\n- Removing the zero differences.\n- Ranking the nonzero absolute differences from smallest to largest, assigning average ranks for ties.\n- Computing the statistic $W^{+}$ as the sum of ranks associated with positive differences.\n- Using the exact distribution of $W^{+}$ induced by independent random signs to obtain the two-sided mid-p value, defined by tail probabilities beyond the observed statistic plus one-half of the probability at equality on the relevant tail(s).\n\nExpress the final mid-p value as an exact fraction or as a decimal rounded to four significant figures. Briefly explain how the presence of ties in the absolute differences affects the distribution of the signed-rank statistic and the interpretation of the mid-p value. The final numerical answer must be a single value (no intervals or inequalities).",
            "solution": "To perform the Wilcoxon signed-rank test, we follow the standard procedure. First, we handle the zero differences. The initial set of $n=25$ differences contains $m_0 = 16$ zeros, which are removed from the analysis. This leaves an effective sample size of $n'=8$ non-zero differences: $\\{1,\\,1,\\,1,\\,2,\\,2,\\,3,\\,3,\\,6\\}$.\n\nNext, we rank the absolute values of these non-zero differences, assigning average ranks for ties:\n- The three values of $1$ occupy ranks $1, 2, 3$. Their average rank is $(1+2+3)/3 = 2$.\n- The two values of $2$ occupy ranks $4, 5$. Their average rank is $(4+5)/2 = 4.5$.\n- The two values of $3$ occupy ranks $6, 7$. Their average rank is $(6+7)/2 = 6.5$.\n- The value of $6$ occupies rank $8$. Its rank is $8$.\nThe resulting set of ranks is $\\{2,\\,2,\\,2,\\,4.5,\\,4.5,\\,6.5,\\,6.5,\\,8\\}$. The sum of these ranks is $36$, which correctly matches the formula $\\frac{n'(n'+1)}{2} = \\frac{8(9)}{2}=36$.\n\nThe test statistic $W^{+}$ is the sum of the ranks corresponding to the positive differences. In this case, all non-zero differences are positive, so the observed statistic, $W_{obs}^{+}$, is the sum of all ranks:\n$$ W_{obs}^{+} = 2+2+2+4.5+4.5+6.5+6.5+8 = 36 $$\n\nUnder the null hypothesis, each of the $n'=8$ non-zero differences has an equal probability of being positive or negative. This creates $2^{n'} = 2^8 = 256$ equally likely sign combinations. The distribution of $W^{+}$ is symmetric around its mean, $E[W^{+}] = \\frac{1}{2} \\sum R_i = 36/2 = 18$. The observed statistic $W_{obs}^{+} = 36$ is the maximum possible value, representing the most extreme outcome in the upper tail. The equally extreme outcome in the lower tail is the minimum value, $W_{symm}^{-} = 0$.\n\nThe two-sided mid-$p$ value is calculated as:\n$$p_{\\text{mid}} = P(W^{+} > W_{obs}^{+}) + \\frac{1}{2}P(W^{+} = W_{obs}^{+}) + P(W^{+} < W_{symm}^{-}) + \\frac{1}{2}P(W^{+} = W_{symm}^{-})$$\nSubstituting $W_{obs}^{+} = 36$ and $W_{symm}^{-} = 0$:\n$$p_{\\text{mid}} = P(W^{+} > 36) + \\frac{1}{2}P(W^{+} = 36) + P(W^{+} < 0) + \\frac{1}{2}P(W^{+} = 0)$$\n- $P(W^{+} > 36) = 0$, since $36$ is the maximum value.\n- $P(W^{+} = 36)$ occurs only if all differences are positive, which is one outcome out of 256. Thus, $P(W^{+} = 36) = 1/256$.\n- $P(W^{+}  0) = 0$, since $0$ is the minimum value.\n- $P(W^{+} = 0)$ occurs only if all differences are negative, which is one outcome out of 256. Thus, $P(W^{+} = 0) = 1/256$.\n\nSubstituting these probabilities:\n$$p_{\\text{mid}} = 0 + \\frac{1}{2} \\left( \\frac{1}{256} \\right) + 0 + \\frac{1}{2} \\left( \\frac{1}{256} \\right) = \\frac{1}{512} + \\frac{1}{512} = \\frac{2}{512} = \\frac{1}{256}$$\n\nThe presence of ties in the absolute differences means that average ranks must be used. This alters the null distribution of $W^{+}$, as the set of possible values for the statistic changes. Standard tables assuming no ties are inapplicable, and the exact distribution must be derived from the specific rank set obtained from the data. The mid-p value is a general technique for discrete test statistics that provides a test size closer to the nominal significance level by effectively \"splitting\" the probability mass at the observed value between the rejection and non-rejection regions.",
            "answer": "$$\\boxed{\\frac{1}{256}}$$"
        },
        {
            "introduction": "Our final practice expands our toolkit beyond tests of central tendency to a method that compares the entire distributions of two independent samples. The Kolmogorov-Smirnov (KS) test asks a more general question: do these two sets of data come from the same underlying distribution? This exercise requires you to construct Empirical Cumulative Distribution Functions (ECDFs) from the ground up and find the maximum difference between them, providing a powerful and intuitive measure of how much the two samples diverge . Mastering this technique allows you to detect a wider range of differences between groups than simple tests of means or medians can reveal.",
            "id": "4920263",
            "problem": "A clinical study compares baseline biomarker concentrations between two independent cohorts. Assume the two cohorts are independent simple random samples from continuous distributions. Let the treatment cohort be a sample of size $n$ and the control cohort be a sample of size $m$. The observed values (in $\\mathrm{mg}/\\mathrm{dL}$) are:\n\n- Treatment cohort $\\mathcal{A}$ ($n = 8$): $3.1$, $4.7$, $5.0$, $5.2$, $6.3$, $6.8$, $7.4$, $9.0$.\n- Control cohort $\\mathcal{B}$ ($m = 7$): $2.9$, $3.5$, $4.4$, $5.5$, $6.0$, $7.9$, $8.2$.\n\nStarting from the fundamental definition of the empirical cumulative distribution function (ECDF) for a sample $X_{1}, \\dots, X_{n}$, given by $F_{n}(x)$ equal to the proportion of sample points not exceeding $x$, and analogously for $G_{m}(x)$ for the second sample, compute the two-sample Kolmogorov–Smirnov statistic, which is the supremum over $x$ of the absolute difference between the two ECDFs.\n\nThen, under the null hypothesis that both samples are drawn from the same continuous distribution, use the large-sample asymptotic null distribution of the scaled Kolmogorov–Smirnov statistic to obtain an approximation to the $p$-value. Round the $p$-value to four significant figures. Report your final answer as a row matrix containing, in order, the Kolmogorov–Smirnov statistic and the approximated $p$-value, with no units inside the matrix.",
            "solution": "To compute the two-sample Kolmogorov-Smirnov (KS) statistic, we first define the Empirical Cumulative Distribution Functions (ECDFs) for the two cohorts. Let $F_n(x)$ be the ECDF for the treatment cohort $\\mathcal{A}$ (with $n=8$) and $G_m(x)$ for the control cohort $\\mathcal{B}$ (with $m=7$). The KS statistic is $D_{n,m} = \\sup_x |F_n(x) - G_m(x)|$.\n\nThe value of the supremum is found at one of the observed data points from the combined sample. We evaluate the ECDFs at each point, where $F_n(x)$ increases by $1/8$ at each point from $\\mathcal{A}$, and $G_m(x)$ increases by $1/7$ at each point from $\\mathcal{B}$. By computing the absolute difference $|F_n(x) - G_m(x)|$ at every value in the combined sample, we find that the maximum difference occurs at $x=4.4$. At this point, the ECDF for cohort $\\mathcal{A}$ is $F_8(4.4) = 1/8$ (since only one value, 3.1, is $\\le 4.4$) and the ECDF for cohort $\\mathcal{B}$ is $G_7(4.4) = 3/7$ (since three values, 2.9, 3.5, and 4.4, are $\\le 4.4$). The maximum difference is:\n$$ D_{8,7} = |F_8(4.4) - G_7(4.4)| = \\left|\\frac{1}{8} - \\frac{3}{7}\\right| = \\left|\\frac{7-24}{56}\\right| = \\frac{17}{56} $$\n\nNext, we compute the approximate $p$-value using the large-sample asymptotic distribution. The scaled KS statistic is:\n$$ K_{n,m} = \\sqrt{\\frac{nm}{n+m}} D_{n,m} = \\sqrt{\\frac{8 \\times 7}{8+7}} \\times \\frac{17}{56} = \\sqrt{\\frac{56}{15}} \\times \\frac{17}{56} = \\frac{17}{\\sqrt{840}} $$\nThe observed value of the scaled statistic is $k_{\\text{obs}} \\approx 0.5866$.\n\nUnder the null hypothesis, the distribution of $K_{n,m}$ converges to the Kolmogorov distribution. The $p$-value is given by $\\Pr(K > k_{\\text{obs}})$, which can be calculated with the series:\n$$ p = 2 \\sum_{i=1}^{\\infty} (-1)^{i-1} \\exp(-2i^2 k_{\\text{obs}}^2) $$\nUsing $k_{\\text{obs}}^2 = (17/\\sqrt{840})^2 = 289/840$, we compute the first few terms:\n$$ p \\approx 2 \\left( \\exp\\left(-2 \\cdot \\frac{289}{840}\\right) - \\exp\\left(-8 \\cdot \\frac{289}{840}\\right) + \\dots \\right) $$\n$$ p \\approx 2 (\\exp(-0.6881) - \\exp(-2.7524) + \\dots) $$\n$$ p \\approx 2 (0.502534 - 0.063781 + 0.002044 - \\dots) \\approx 0.8816 $$\nRounding to four significant figures, the approximated $p$-value is $0.8816$.\n\nThe final result is the KS statistic and the approximated $p$-value, presented as a row matrix.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{17}{56}  0.8816 \\end{pmatrix} } $$"
        }
    ]
}