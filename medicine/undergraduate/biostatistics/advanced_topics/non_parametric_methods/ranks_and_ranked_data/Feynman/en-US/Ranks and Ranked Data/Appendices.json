{
    "hands_on_practices": [
        {
            "introduction": "Pearson's correlation is a cornerstone of statistics, but its reliance on raw data values makes it sensitive to outliers and non-linear relationships. Rank-based methods like Spearman's correlation offer a robust alternative by measuring the strength of a monotonic association, independent of the data's original scale. This simulation exercise  allows you to empirically verify this principle, demonstrating how Spearman's correlation remains stable even when data is afflicted with heavy-tailed noise and extreme contamination, providing a much more reliable picture of the underlying relationship.",
            "id": "4946608",
            "problem": "You will implement a simulation study to compare the empirical robustness of Spearman’s rank correlation coefficient (Spearman’s $\\rho_s$) and Pearson’s product-moment correlation coefficient (Pearson’s $\\rho$) under monotone nonlinear relationships with heavy-tailed noise and point-mass contamination. The study must be based on first principles: definitions of sample correlation, rank transformation, and the basic properties of monotone increasing functions and heavy-tailed distributions.\n\nUse the following foundational bases appropriate to ranked data in biostatistics:\n- The sample Pearson correlation between two real-valued vectors $x$ and $y$ of common length $n$ is defined as the standardized sample covariance of $x$ and $y$.\n- The sample Spearman correlation between two real-valued vectors $x$ and $y$ is the sample Pearson correlation computed on the rank-transformed data, where each observation is replaced by its rank among all observations (with average ranks for ties).\n- A function $g:\\mathbb{R}\\to\\mathbb{R}$ is monotone increasing if for all $x_1,x_2\\in\\mathbb{R}$, $x_1\\le x_2$ implies $g(x_1)\\le g(x_2)$.\n- Heavy-tailed noise is represented by a Student’s $t$ distribution with low degrees of freedom.\n\nYour program must perform the following steps exactly:\n1. For each test case, simulate $R$ independent replicates of a dataset $\\{(X_i,Y_i)\\}_{i=1}^n$ as follows:\n   - Generate $X_i \\sim \\mathcal{N}(0,1)$ independently for $i=1,\\dots,n$.\n   - Generate independent noise $E_i \\sim t_{\\nu}$ (Student’s $t$ with $\\nu$ degrees of freedom). Multiply the noise by a fixed scale $s$ to control its magnitude, resulting in $s\\,E_i$.\n   - Choose a monotone increasing function $g$ and set the clean response as $Y_i^{(\\mathrm{clean})}=g(X_i)+s\\,E_i$.\n   - Create a contaminated response $Y_i^{(\\mathrm{cont})}$ by selecting $m=\\lfloor p\\,n\\rfloor$ indices uniformly without replacement and replacing those $m$ values via $Y_i^{(\\mathrm{cont})}=Y_i^{(\\mathrm{clean})}+M\\cdot S_i$, where $S_i$ are independent Rademacher signs taking values in $\\{-1,+1\\}$ with equal probability.\n2. For each replicate, compute:\n   - The sample Pearson correlation on the clean data, $\\widehat{\\rho}_{P}^{(\\mathrm{clean})}$, and on the contaminated data, $\\widehat{\\rho}_{P}^{(\\mathrm{cont})}$.\n   - The sample Spearman correlation on the clean data, $\\widehat{\\rho}_{S}^{(\\mathrm{clean})}$, and on the contaminated data, $\\widehat{\\rho}_{S}^{(\\mathrm{cont})}$.\n3. For each replicate, compute the absolute changes\n   $$\\Delta_P=\\left|\\widehat{\\rho}_{P}^{(\\mathrm{cont})}-\\widehat{\\rho}_{P}^{(\\mathrm{clean})}\\right|,\\qquad \\Delta_S=\\left|\\widehat{\\rho}_{S}^{(\\mathrm{cont})}-\\widehat{\\rho}_{S}^{(\\mathrm{clean})}\\right|.$$\n4. Average these absolute changes across the $R$ replicates to obtain $\\overline{\\Delta}_P$ and $\\overline{\\Delta}_S$ for the test case.\n5. For each test case, return a boolean indicating whether $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$.\n\nScientific rationale that your implementation must adhere to:\n- Monotone increasing transformations preserve ranks. Therefore, in the absence of contamination, Spearman’s correlation is invariant under strictly monotone increasing transformations of the marginals, while Pearson’s correlation is not.\n- Heavy-tailed noise increases the frequency and magnitude of extreme values, inflating sensitivity of estimators relying on finite second moments. Pearson’s correlation is known to be more sensitive to such extremes compared to rank-based measures.\n- Point-mass contamination introduces extreme outliers that primarily stress robustness.\n\nUse the following fixed constants across all test cases:\n- Number of replicates $R = 400$.\n- Noise scale $s = 0.5$.\n- Contamination magnitude $M = 25$.\n- Random number generator seed fixed to an integer of your choice to ensure reproducibility.\n\nUse the following monotone increasing functions $g$:\n- $g_{\\mathrm{lin}}(x)=x$.\n- $g_{\\exp}(x)=\\exp(x/2)$.\n- $g_{\\mathrm{logit}}(x)=\\frac{1}{1+\\exp(-x)}$.\n- $g_{\\mathrm{cub}}(x)=x^3$.\n\nTest Suite (provide results in this exact order):\n- Case $1$: $g=g_{\\mathrm{lin}}$, $n=200$, $\\nu=3$, $p=0.1$.\n- Case $2$: $g=g_{\\exp}$, $n=200$, $\\nu=3$, $p=0.1$.\n- Case $3$: $g=g_{\\mathrm{logit}}$, $n=200$, $\\nu=1$, $p=0.1$.\n- Case $4$: $g=g_{\\mathrm{cub}}$, $n=80$, $\\nu=3$, $p=0.3$.\n- Case $5$: $g=g_{\\mathrm{lin}}$, $n=200$, $\\nu=3$, $p=0.0$.\n\nFinal output specification:\n- Your program should produce a single line of output containing a list of $5$ booleans corresponding to the $5$ cases above in order. The $k$-th boolean must be $\\mathrm{True}$ if and only if $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ for case $k$, and $\\mathrm{False}$ otherwise.\n- The line must be formatted as a Python-style list with commas and no spaces, for example: $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{False}]$.\n- No additional text should be printed.\n\nNo physical units are involved. Angles are not involved. Any proportions must be returned as decimals, but your final output here consists only of booleans. The program must be fully self-contained and require no user input, external files, or network access.",
            "solution": "The user has requested the implementation of a simulation study to evaluate the robustness of Pearson's and Spearman's correlation coefficients. The problem is scientifically sound, well-posed, and all parameters and procedures are explicitly defined. Therefore, a solution is warranted.\n\n### Principle-Based Design\n\nThe core of this problem lies in comparing the robustness of two common measures of statistical association: the Pearson product-moment correlation coefficient, $\\widehat{\\rho}_P$, and the Spearman rank correlation coefficient, $\\widehat{\\rho}_S$. Robustness, in this context, refers to the stability of an estimator in the presence of deviations from ideal assumptions, such as outliers or non-normal error distributions.\n\n**1. Theoretical Foundations**\n\n**Pearson Correlation ($\\widehat{\\rho}_P$)**: The sample Pearson correlation coefficient measures the linear association between two continuous variables, $x$ and $y$. For a sample of size $n$, $\\{(x_i, y_i)\\}_{i=1}^n$, it is defined as the sample covariance divided by the product of the sample standard deviations:\n$$ \\widehat{\\rho}_P(x,y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}} $$\nwhere $\\bar{x}$ and $\\bar{y}$ are the sample means. This formula reveals that $\\widehat{\\rho}_P$ is computed directly from the raw data values. Consequently, it is highly sensitive to outliers. A single extreme data point can arbitrarily inflate or deflate the sums of squares and cross-products, disproportionately influencing the means, variances, and covariance, thereby distorting the correlation estimate.\n\n**Spearman Correlation ($\\widehat{\\rho}_S$)**: The sample Spearman correlation coefficient assesses the strength of a monotonic relationship between two variables. It is a non-parametric measure. Its key feature is that it operates not on the raw data, but on the ranks of the data. The procedure involves:\n1. Transforming the vector $x$ into a vector of its ranks, $R_x$.\n2. Transforming the vector $y$ into a vector of its ranks, $R_y$. In case of ties, the average rank is assigned.\n3. Calculating the Pearson correlation coefficient of these rank-transformed vectors:\n$$ \\widehat{\\rho}_S(x,y) = \\widehat{\\rho}_P(R_x, R_y) $$\nBy converting data to ranks, the influence of the actual magnitudes is removed. An extreme outlier is simply assigned the highest or lowest rank (e.g., $1$ or $n$), a value on the same integer scale as all other ranks. This transformation bounds the influence of any single observation, making $\\widehat{\\rho}_S$ inherently more robust to outliers than $\\widehat{\\rho}_P$.\n\n**2. Simulation Methodology**\n\nThe simulation is designed to create datasets where these theoretical differences in robustness can be empirically quantified.\n\n**Data Generation**:\n- The predictor variable $X_i$ is drawn from a standard normal distribution, $X_i \\sim \\mathcal{N}(0,1)$, which is a common baseline.\n- A monotone increasing function $g$ is applied, $g(X_i)$, to create a non-linear (or linear, for $g(x)=x$) relationship. Since $\\widehat{\\rho}_S$ measures monotonic association, it is expected to perform well even when the relationship is not linear, unlike $\\widehat{\\rho}_P$.\n- Heavy-tailed noise, $s \\cdot E_i$ where $E_i \\sim t_{\\nu}$, is added. A Student's $t$-distribution with low degrees of freedom $\\nu$ (e.g., $\\nu=1$ for the Cauchy distribution, $\\nu=3$) has heavier tails than a normal distribution, meaning it produces extreme values more frequently. This tests the estimators' sensitivity to non-Gaussian noise.\n- The resulting \"clean\" data is $Y_i^{(\\mathrm{clean})} = g(X_i) + s \\cdot E_i$.\n- Point-mass contamination is introduced to a fraction $p$ of the data. A large magnitude shock, $M \\cdot S_i$, is added to $m = \\lfloor p \\cdot n\\rfloor$ randomly chosen observations to create $Y_i^{(\\mathrm{cont})}$. This models gross measurement errors or data corruption, creating extreme outliers designed to stress-test the estimators.\n\n**Quantifying Robustness**:\nThe simulation measures robustness by quantifying how much each correlation estimate changes when the data is contaminated. For each of the $R=400$ replicates, we compute:\n- The change in Pearson correlation: $\\Delta_P = |\\widehat{\\rho}_{P}^{(\\mathrm{cont})} - \\widehat{\\rho}_{P}^{(\\mathrm{clean})}|$.\n- The change in Spearman correlation: $\\Delta_S = |\\widehat{\\rho}_{S}^{(\\mathrm{cont})} - \\widehat{\\rho}_{S}^{(\\mathrm{clean})}|$.\n\nA smaller $\\Delta$ indicates greater robustness. By averaging these changes over many replicates, we obtain stable estimates, $\\overline{\\Delta}_P$ and $\\overline{\\Delta}_S$, of the expected impact of contamination on each coefficient. The final comparison, $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$, provides a direct verdict on which estimator is more robust under the specified conditions.\n\n**3. Implementation Plan**\n\nThe program will be implemented in Python using the `numpy` and `scipy` libraries.\n- A fixed integer seed for the random number generator (`numpy.random.default_rng`) ensures reproducibility.\n- The data generation process follows the specified steps precisely. `numpy.random.Generator.normal` and `numpy.random.Generator.standard_t` will be used to generate the random variates.\n- The contamination is applied by first creating a copy of the clean response vector, then selecting indices without replacement using `numpy.random.Generator.choice`, and finally adding the scaled Rademacher noise.\n- To compute Pearson correlation, `numpy.corrcoef` is used.\n- To compute Spearman correlation as per its definition, `scipy.stats.rankdata` (with `method='average'` for ties) is first used to obtain the ranks of $X$ and $Y$, and then `numpy.corrcoef` is applied to these rank vectors.\n- The main logic iterates through the five test cases, performing $R$ replicates for each. Within each case, the absolute changes $\\Delta_P$ and $\\Delta_S$ are calculated in each replicate and accumulated.\n- After all replicates for a case are complete, the averages $\\overline{\\Delta}_P$ and $\\overline{\\Delta}_S$ are computed, and the boolean result of the comparison $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ is stored.\n- Finally, the list of booleans for all test cases is printed in the specified format. The special case where $p=0.0$ serves as a control; with no contamination, both $\\Delta_P$ and $\\Delta_S$ should be zero, making the inequality $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ hold true. For all other cases, theory predicts a similar outcome due to the superior robustness of the rank-based Spearman correlation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndef solve():\n    \"\"\"\n    Implements a simulation study to compare the robustness of Pearson's and \n    Spearman's correlation coefficients under various conditions.\n    \"\"\"\n    # Define fixed constants for the simulation\n    R = 400  # Number of replicates\n    S_SCALE = 0.5  # Noise scale s\n    M_CONTAMINATION = 25.0  # Contamination magnitude M\n    SEED = 42  # Fixed seed for reproducibility\n\n    # Initialize the random number generator\n    rng = np.random.default_rng(SEED)\n\n    # Define the monotone increasing functions\n    def g_lin(x):\n        return x\n\n    def g_exp(x):\n        return np.exp(x / 2.0)\n\n    def g_logit(x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    def g_cub(x):\n        return x**3\n\n    # Define the test suite\n    test_cases = [\n        {'g': g_lin, 'n': 200, 'nu': 3, 'p': 0.1, 'name': 'g_lin'},\n        {'g': g_exp, 'n': 200, 'nu': 3, 'p': 0.1, 'name': 'g_exp'},\n        {'g': g_logit, 'n': 200, 'nu': 1, 'p': 0.1, 'name': 'g_logit'},\n        {'g': g_cub, 'n': 80, 'nu': 3, 'p': 0.3, 'name': 'g_cub'},\n        {'g': g_lin, 'n': 200, 'nu': 3, 'p': 0.0, 'name': 'g_lin_no_cont'},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        g, n, nu, p = case['g'], case['n'], case['nu'], case['p']\n        \n        total_delta_p = 0.0\n        total_delta_s = 0.0\n\n        for _ in range(R):\n            # Step 1: Generate data\n            # Generate X_i ~ N(0, 1)\n            x_data = rng.normal(loc=0.0, scale=1.0, size=n)\n            \n            # Generate noise E_i ~ t_nu\n            noise = rng.standard_t(df=nu, size=n)\n            \n            # Generate clean response Y_i^(clean)\n            y_clean = g(x_data) + S_SCALE * noise\n            \n            # Create contaminated response Y_i^(cont)\n            y_cont = np.copy(y_clean)\n            m = int(np.floor(p * n))\n            \n            if m > 0:\n                indices_to_contaminate = rng.choice(n, size=m, replace=False)\n                signs = rng.choice([-1, 1], size=m)\n                y_cont[indices_to_contaminate] += M_CONTAMINATION * signs\n            \n            # Step 2: Compute correlations\n            # Pearson correlations\n            rho_p_clean = np.corrcoef(x_data, y_clean)[0, 1]\n            rho_p_cont = np.corrcoef(x_data, y_cont)[0, 1]\n\n            # Spearman correlations (as Pearson on ranks)\n            rank_x = rankdata(x_data, method='average')\n            rank_y_clean = rankdata(y_clean, method='average')\n            rank_y_cont = rankdata(y_cont, method='average')\n            \n            rho_s_clean = np.corrcoef(rank_x, rank_y_clean)[0, 1]\n            rho_s_cont = np.corrcoef(rank_x, rank_y_cont)[0, 1]\n\n            # Step 3: Compute absolute changes\n            delta_p = np.abs(rho_p_cont - rho_p_clean)\n            delta_s = np.abs(rho_s_cont - rho_s_clean)\n            \n            total_delta_p += delta_p\n            total_delta_s += delta_s\n\n        # Step 4: Average absolute changes\n        avg_delta_p = total_delta_p / R\n        avg_delta_s = total_delta_s / R\n\n        # Step 5: Compare and store boolean result\n        results.append(avg_delta_s <= avg_delta_p)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A key advantage of rank-based methods is their distribution-free nature, but this elegance is complicated when the data contains tied values. This practice  moves beyond simply applying a formula and challenges you to derive the standard tie-correction factor from first principles. By exploring how assigning midranks to tied observations systematically reduces the total variance of the rank population, you will gain a deep, foundational understanding of how non-parametric tests like the Kruskal-Wallis test are adjusted to maintain their accuracy.",
            "id": "4946622",
            "problem": "A clinical researcher pools observations from $k$ independent treatment groups to perform a Kruskal–Wallis (KW) test. Under the null hypothesis of identical continuous distributions, the pooled ranks $1,2,\\dots,n$ are exchangeable and group rank sums behave as sums from simple random samples without replacement from the set of ranks. When tied measurements occur, midranks are assigned within each tied block. The researcher seeks a principled variance adjustment for the group rank sums that depends only on the pooled sample size and the tie structure, not on detailed group composition.\n\nStarting from the following foundational base:\n\n- The population mean and variance of the integers $1,2,\\dots,n$ are $(n+1)/2$ and $(n^{2}-1)/12$.\n- For sampling without replacement from a finite population of size $n$ with population variance $S^{2}$, the variance of the sum of $n_{i}$ sampled values is $n_{i}\\,(n-n_{i})\\,S^{2}/(n-1)$.\n- Midranks are the arithmetic means of the consecutive rank positions in each tied block.\n\nDerive, from first principles, the multiplicative tie correction factor that converts the no-ties variance of a group rank sum into the correct variance under midranks. Your derivation must explain why the correction arises from the change in the population variance of the pooled ranks caused by replacing each tied block of consecutive ranks by a constant midrank, and it must identify how the correction depends on the sizes of the tied blocks.\n\nThen, evaluate the tie correction factor for the following unfamiliar pooled data scenario: there are $n=43$ pooled observations with five tie blocks of sizes $t_{1}=2$, $t_{2}=2$, $t_{3}=5$, $t_{4}=3$, and $t_{5}=4$ (all other observations are distinct). Report the factor as a single number. Round your answer to four significant figures.",
            "solution": "The problem requires the derivation of a multiplicative tie correction factor for the variance of a group rank sum in a Kruskal-Wallis test framework. The derivation must proceed from first principles as laid out in the problem statement.\n\nLet the total number of pooled observations be $n$. In the absence of ties, the ranks assigned to the observations would be the set of integers $\\{1, 2, \\dots, n\\}$.\nLet the population of ranks be denoted by $\\{r_1, r_2, \\dots, r_n\\}$.\nThe problem provides that the variance of the sum of $n_i$ values sampled without replacement from a finite population of size $n$ is given by\n$$ \\text{Var}(\\text{sum}) = \\frac{n_i(n-n_i)}{n-1} S^2 $$\nwhere $S^2$ is the population variance. In the context of rank statistics, this sum is the rank sum $R_i$ for a group of size $n_i$, and $S^2$ is the variance of the population of $n$ ranks.\n$$ \\text{Var}(R_i) = \\frac{n_i(n-n_i)}{n-1} S_{ranks}^2 $$\nThe problem defines the population variance of the integers $1, 2, \\dots, n$ as $S_{no-ties}^2 = \\frac{n^2-1}{12}$. This corresponds to the formula $S^2 = \\frac{1}{n} \\sum_{j=1}^n (r_j - \\bar{r})^2$.\nThus, for the no-ties case, the variance of a rank sum is:\n$$ \\text{Var}_{no-ties}(R_i) = \\frac{n_i(n-n_i)}{n-1} S_{no-ties}^2 $$\nWhen ties are present, the original ranks are replaced by a new set of values $\\{r'_1, r'_2, \\dots, r'_n\\}$ which include midranks. The variance of the rank sum is computed using the same structure, but with the variance of this new population of values, $S_{ties}^2$:\n$$ \\text{Var}_{ties}(R_i) = \\frac{n_i(n-n_i)}{n-1} S_{ties}^2 $$\nThe multiplicative tie correction factor, let's call it $C_T$, is defined such that $\\text{Var}_{ties}(R_i) = C_T \\cdot \\text{Var}_{no-ties}(R_i)$. Substituting the expressions for the variances, we get:\n$$ \\frac{n_i(n-n_i)}{n-1} S_{ties}^2 = C_T \\cdot \\frac{n_i(n-n_i)}{n-1} S_{no-ties}^2 $$\nThis relationship simplifies to:\n$$ C_T = \\frac{S_{ties}^2}{S_{no-ties}^2} $$\nThis shows that the correction factor is the ratio of the population variance of the ranks with ties to the population variance of the ranks without ties. The core of the derivation is to determine $S_{ties}^2$.\n\nThe population variance is defined as $S^2 = \\left(\\frac{1}{n}\\sum_{j=1}^n r_j^2\\right) - \\bar{r}^2$. We must analyze how assigning midranks affects the mean rank $\\bar{r}$ and the sum of squared ranks $\\sum r_j^2$.\n\nFirst, consider the mean rank. A block of $t$ tied observations, which would have occupied consecutive ranks $k, k+1, \\dots, k+t-1$, are all assigned their midrank, $m = \\frac{1}{t}\\sum_{j=0}^{t-1}(k+j) = k + \\frac{t-1}{2}$. The sum of the original ranks in this block is $\\sum_{j=0}^{t-1}(k+j) = tk + \\frac{t(t-1)}{2}$. The sum of the new midranks for these $t$ observations is $t \\cdot m = t(k + \\frac{t-1}{2}) = tk + \\frac{t(t-1)}{2}$. Since the sum of ranks within any tied block is conserved, the total sum of all $n$ ranks remains unchanged.\n$$ \\sum_{j=1}^n r'_j = \\sum_{j=1}^n j = \\frac{n(n+1)}{2} $$\nConsequently, the mean rank is also unchanged by the presence of ties: $\\bar{r}_{ties} = \\bar{r}_{no-ties} = \\frac{n+1}{2}$.\n\nNext, we analyze the sum of squares of the ranks, $\\sum r^2$. The change in $\\sum r^2$ due to one tied block is the difference between the sum of squares of the midranks and the sum of squares of the original ranks:\n$$ \\Delta\\left(\\sum r^2\\right)_{\\text{block}} = t m^2 - \\sum_{j=0}^{t-1} (k+j)^2 $$\nSubstituting $m=k+\\frac{t-1}{2}$ and using the formula for the sum of squares, $\\sum_{j=0}^{t-1} j^2 = \\frac{(t-1)t(2t-1)}{6}$:\n$$ \\sum_{j=0}^{t-1} (k+j)^2 = \\sum_{j=0}^{t-1} (k^2+2kj+j^2) = tk^2 + 2k\\frac{t(t-1)}{2} + \\frac{(t-1)t(2t-1)}{6} $$\n$$ t m^2 = t \\left(k+\\frac{t-1}{2}\\right)^2 = t \\left(k^2 + k(t-1) + \\frac{(t-1)^2}{4}\\right) $$\nThe difference is:\n$$ \\Delta\\left(\\sum r^2\\right)_{\\text{block}} = t \\frac{(t-1)^2}{4} - \\frac{t(t-1)(2t-1)}{6} = \\frac{t(t-1)}{12} [3(t-1) - 2(2t-1)] $$\n$$ = \\frac{t(t-1)}{12} [3t-3-4t+2] = \\frac{t(t-1)(-t-1)}{12} = -\\frac{t(t^2-1)}{12} $$\nThis reduction in the sum of squares depends only on the size $t$ of the tied block, not its location. If there are $g$ tie blocks with sizes $t_1, t_2, \\dots, t_g$, the total reduction in the sum of squares of ranks is the sum of the reductions from each block:\n$$ \\left(\\sum r^2\\right)_{ties} = \\left(\\sum r^2\\right)_{no-ties} - \\sum_{i=1}^g \\frac{t_i(t_i^2-1)}{12} $$\nSince $\\bar{r}$ remains constant, the change in variance $S^2$ is directly proportional to the change in $\\sum r^2$:\n$$ S_{ties}^2 = \\frac{1}{n}\\left(\\sum r^2\\right)_{ties} - \\bar{r}^2 = \\frac{1}{n}\\left(\\left(\\sum r^2\\right)_{no-ties} - \\sum_{i=1}^g \\frac{t_i(t_i^2-1)}{12}\\right) - \\bar{r}^2 $$\n$$ S_{ties}^2 = \\left(\\frac{1}{n}\\left(\\sum r^2\\right)_{no-ties} - \\bar{r}^2\\right) - \\frac{1}{12n}\\sum_{i=1}^g (t_i^3-t_i) $$\n$$ S_{ties}^2 = S_{no-ties}^2 - \\frac{1}{12n}\\sum_{i=1}^g (t_i^3-t_i) $$\nNow we can derive the correction factor $C_T$:\n$$ C_T = \\frac{S_{ties}^2}{S_{no-ties}^2} = \\frac{S_{no-ties}^2 - \\frac{1}{12n}\\sum_{i=1}^g (t_i^3-t_i)}{S_{no-ties}^2} = 1 - \\frac{\\frac{1}{12n}\\sum_{i=1}^g (t_i^3-t_i)}{S_{no-ties}^2} $$\nSubstituting $S_{no-ties}^2 = \\frac{n^2-1}{12}$:\n$$ C_T = 1 - \\frac{\\frac{1}{12n}\\sum_{i=1}^g (t_i^3-t_i)}{\\frac{n^2-1}{12}} = 1 - \\frac{\\sum_{i=1}^g (t_i^3-t_i)}{n(n^2-1)} = 1 - \\frac{\\sum_{i=1}^g (t_i^3-t_i)}{n^3-n} $$\nThis expression confirms that the correction is a reduction from $1$ that depends only on the total sample size $n$ and the sizes of the tied blocks $t_i$.\n\nWe now evaluate this factor for the given data: $n=43$ and tie blocks of sizes $t_1=2$, $t_2=2$, $t_3=5$, $t_4=3$, and $t_5=4$.\nFirst, calculate the sum $\\sum (t_i^3-t_i)$:\n$$ t_1=2: 2^3-2 = 8-2 = 6 $$\n$$ t_2=2: 2^3-2 = 8-2 = 6 $$\n$$ t_3=5: 5^3-5 = 125-5 = 120 $$\n$$ t_4=3: 3^3-3 = 27-3 = 24 $$\n$$ t_5=4: 4^3-4 = 64-4 = 60 $$\nThe sum is $\\sum (t_i^3-t_i) = 6+6+120+24+60 = 216$.\nNext, calculate the denominator $n^3-n$:\n$$ n^3-n = 43^3 - 43 = 79507 - 43 = 79464 $$\nFinally, compute the correction factor $C_T$:\n$$ C_T = 1 - \\frac{216}{79464} $$\n$$ C_T \\approx 1 - 0.002718155... $$\n$$ C_T \\approx 0.997281844... $$\nRounding to four significant figures, the tie correction factor is $0.9973$.",
            "answer": "$$\\boxed{0.9973}$$"
        },
        {
            "introduction": "When dealing with discrete ordinal data, such as responses on a Likert scale, ties are not an exception but a fundamental feature of the data. In such cases, the assumptions behind standard normal approximations can be strained. This exercise  introduces the gold standard for inference in this setting: the exact permutation test. You will implement an enumeration algorithm to construct the true null distribution of the Wilcoxon rank-sum statistic, allowing for the calculation of exact p-values without any reliance on approximations.",
            "id": "4946639",
            "problem": "You are asked to implement an exact permutation-based adjustment of null distributions for rank statistics when outcomes are discrete, as in Likert scales. Under the null hypothesis of exchangeability, the labels of group membership are uniformly random over the pooled multiset of responses. With discrete outcomes, tied ranks occur, and midranks must be used. Your task is to derive, implement, and evaluate an exact enumeration strategy that respects the observed pooled category counts of the Likert variable.\n\nFundamental base:\n- Under the null hypothesis of no group effect, group labels are exchangeable across the pooled observations.\n- Ties are handled by assigning midranks: all observations in the same category receive the average of the ranks they would occupy if broken arbitrarily.\n- The Wilcoxon rank-sum statistic $W$ for a group of size $n_1$ is the sum of the midranks of the observations in that group.\n- The total number of pooled observations is $N = n_1 + n_2$, and the mean of the null distribution of $W$ is $\\mu = n_1 (N + 1)/2$.\n\nYour program must:\n1. Compute midranks from the pooled data by sorting unique Likert categories in ascending order and assigning, for each tie block of pooled size $c_k$, the midrank $m_k$ equal to the average of the block’s ranks.\n2. Express the Wilcoxon rank-sum statistic as $W = \\sum_{k} x_k \\, m_k$, where $x_k$ is the number of group-$A$ observations in category $k$. The vector $(x_k)$ ranges over all integer allocations satisfying $0 \\le x_k \\le c_k$ and $\\sum_k x_k = n_1$.\n3. Use exact enumeration weighted by combinatorial counts to construct the exact null distribution of $W$. Each allocation $(x_k)$ corresponds to $\\prod_k \\binom{c_k}{x_k}$ distinct labelings, each equally likely under the null. The total number of labelings is $\\binom{N}{n_1}$. Therefore, the probability of an allocation $(x_k)$ is $\\left(\\prod_k \\binom{c_k}{x_k}\\right) \\big/ \\binom{N}{n_1}$.\n4. Aggregate allocations with the same $W$ to get the exact probability mass function of $W$, and compute:\n   - The observed statistic $w_{\\text{obs}}$ from the actual group assignments.\n   - The left one-sided $p$-value $p_{\\text{left}} = \\mathbb{P}(W \\le w_{\\text{obs}})$.\n   - The right one-sided $p$-value $p_{\\text{right}} = \\mathbb{P}(W \\ge w_{\\text{obs}})$.\n   - The two-sided $p$-value defined by the distance from the mean: $p_{\\text{two}} = \\mathbb{P}(|W - \\mu| \\ge |w_{\\text{obs}} - \\mu|)$.\n5. To avoid fractional midranks in computation, scale all midranks by a factor of $2$ so that $W$ becomes an integer-valued statistic $W^{\\ast} = 2 W$, and similarly $\\mu^{\\ast} = 2 \\mu = n_1 (N + 1)$. Use this scaled form to perform exact enumeration and compute probabilities.\n\nTest suite:\nImplement your program to process the following four test cases. In each case, group $A$ and group $B$ are provided as lists of Likert responses. All Likert values are integers, and midranks must be computed from the pooled data.\n\n- Case $1$ (happy path with multiple ties):\n  - Group $A$: $[$1$,$2$,$4$,$4$,$5$]$\n  - Group $B$: $[$1$,$1$,$3$,$4$,$5$,$5$,$5$]$\n\n- Case $2$ (degenerate all-equal category):\n  - Group $A$: $[$3$,$3$]$\n  - Group $B$: $[$3$,$3$,$3$]$\n\n- Case $3$ (no ties in the pooled data):\n  - Group $A$: $[$1$,$2$]$\n  - Group $B$: $[$3$,$4$,$5$]$\n\n- Case $4$ (balanced sizes with central ties):\n  - Group $A$: $[$2$,$2$,$3$,$4$]$\n  - Group $B$: $[$1$,$2$,$3$,$3$]$\n\nRequired output:\n- For each case, output a list $[p_{\\text{two}}, p_{\\text{left}}, p_{\\text{right}}]$ where each entry is a float rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of these per-case lists, enclosed in square brackets. For example: $[[p_{1,\\text{two}},p_{1,\\text{left}},p_{1,\\text{right}}],[p_{2,\\text{two}},p_{2,\\text{left}},p_{2,\\text{right}}],\\dots]$.",
            "solution": "The user-provided problem is a valid and well-posed question in the domain of biostatistics, specifically concerning non-parametric hypothesis testing with discrete data. All provided information is scientifically sound, self-consistent, and sufficient for deriving a unique solution. The core of the problem is to implement an exact permutation test for the Wilcoxon rank-sum statistic, accounting for tied ranks, a standard procedure in statistical analysis.\n\nThe methodology is outlined as follows:\n\n1.  **Data Pooling and Rank Calculation**: For each test case, the data from group $A$ (size $n_1$) and group $B$ (size $n_2$) are pooled into a single dataset of size $N = n_1 + n_2$. The unique values (Likert categories) are identified and sorted. Let there be $K$ unique categories. For each category $k \\in \\{1, \\dots, K\\}$, we count its total frequency $c_k$ in the pooled data.\n    The ranks of the $c_k$ observations in category $k$ occupy the positions from $\\left(1 + \\sum_{j=1}^{k-1} c_j\\right)$ to $\\left(\\sum_{j=1}^{k} c_j\\right)$. The midrank $m_k$ for category $k$ is the average of these ranks:\n    $$m_k = \\frac{\\left(1 + \\sum_{j=1}^{k-1} c_j\\right) + \\left(\\sum_{j=1}^{k} c_j\\right)}{2} = \\left(\\sum_{j=1}^{k-1} c_j\\right) + \\frac{c_k + 1}{2}$$\n    To avoid floating-point arithmetic during enumeration, we use a scaled midrank $m_k^* = 2m_k$, which is guaranteed to be an integer:\n    $$m_k^* = 2\\left(\\sum_{j=1}^{k-1} c_j\\right) + c_k + 1$$\n\n2.  **Observed Statistic and Null Hypothesis Parameters**: The Wilcoxon rank-sum statistic for group $A$, $W$, is the sum of the ranks of its observations. This can be expressed in terms of category counts. Let $x_k$ be the number of observations from group $A$ that fall into category $k$. The scaled statistic $W^*$ is then:\n    $$W^* = \\sum_{k=1}^{K} x_k m_k^*$$\n    We compute the observed value of this statistic, $w_{\\text{obs}}^*$, using the actual counts $x_{k, \\text{obs}}$ from the provided group $A$ data.\n    Under the null hypothesis of exchangeability, the mean of the distribution of $W$ is $\\mu = \\frac{n_1(N+1)}{2}$. The scaled mean is $\\mu^* = 2\\mu = n_1(N+1)$, which is an integer.\n\n3.  **Exact Enumeration of the Null Distribution**: The crucial step is to construct the exact probability mass function (PMF) of $W^*$ under the null hypothesis. The null hypothesis implies that any subset of $n_1$ observations from the pooled set of $N$ is equally likely to form group $A$. The total number of ways to choose group $A$ is $\\binom{N}{n_1}$.\n    An equivalent perspective, which is computationally more efficient, is to consider all possible allocations of the $n_1$ observations of group $A$ into the $K$ categories. An allocation is a vector of counts $(x_1, x_2, \\dots, x_K)$ satisfying the constraints:\n    $$0 \\le x_k \\le c_k \\quad \\text{for all } k=1, \\dots, K$$\n    $$\\sum_{k=1}^{K} x_k = n_1$$\n    For each such valid allocation, the number of ways to form group $A$ is the product of binomial coefficients $\\prod_{k=1}^{K} \\binom{c_k}{x_k}$. The probability of this allocation is:\n    $$\\mathbb{P}(x_1, \\dots, x_K) = \\frac{\\prod_{k=1}^{K} \\binom{c_k}{x_k}}{\\binom{N}{n_1}}$$\n    We generate all valid allocation vectors $(x_k)$ using a recursive backtracking algorithm. For each allocation, we compute the corresponding statistic $W^* = \\sum_k x_k m_k^*$ and its probability. The full PMF of $W^*$ is constructed by summing the probabilities of all allocations that yield the same value of $W^*$.\n\n4.  **P-value Calculation**: With the complete PMF of $W^*$, we calculate the required p-values based on the observed statistic $w_{\\text{obs}}^*$ and the scaled mean $\\mu^*$:\n    -   **Left-sided p-value**: $p_{\\text{left}} = \\mathbb{P}(W^* \\le w_{\\text{obs}}^*) = \\sum_{w^* \\le w_{\\text{obs}}^*} \\mathbb{P}(W^*=w^*)$\n    -   **Right-sided p-value**: $p_{\\text{right}} = \\mathbb{P}(W^* \\ge w_{\\text{obs}}^*) = \\sum_{w^* \\ge w_{\\text{obs}}^*} \\mathbb{P}(W^*=w^*)$\n    -   **Two-sided p-value**: This is based on the distance from the mean. Let $d_{\\text{obs}} = |w_{\\text{obs}}^* - \\mu^*|$. The p-value is the probability of observing a result at least as extreme:\n        $$p_{\\text{two}} = \\mathbb{P}(|W^* - \\mu^*| \\ge d_{\\text{obs}}) = \\sum_{w^* \\text{ s.t. } |w^*-\\mu^*| \\ge d_{\\text{obs}}} \\mathbb{P}(W^*=w^*)$$\n    Since $W^*$, $\\mu^*$, and $w_{\\text{obs}}^*$ are all integers, these calculations are exact and do not involve floating-point comparison issues.\n\nThe implementation will follow this logic. A recursive function will handle the enumeration of allocations. The `scipy.special.comb` function will be used for calculating binomial coefficients to ensure accuracy and handle potentially large numbers, although the test cases are small enough that this is not a major concern. Finally, the computed p-values are rounded to $6$ decimal places as required.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nfrom collections import defaultdict\n\ndef calculate_p_values(group_a, group_b):\n    \"\"\"\n    Computes exact permutation p-values for the Wilcoxon rank-sum test with ties.\n    \n    Args:\n        group_a (list): List of observations for group A.\n        group_b (list): List of observations for group B.\n        \n    Returns:\n        list: A list containing [p_two, p_left, p_right] rounded to 6 decimal places.\n    \"\"\"\n    \n    # Step 1: Initialize data and basic parameters\n    n1 = len(group_a)\n    n2 = len(group_b)\n    N = n1 + n2\n    \n    # Handle trivial case where a group is empty\n    if n1 == 0 or n2 == 0:\n        return [1.0, 1.0, 1.0]\n\n    pooled_data = np.concatenate((group_a, group_b))\n    \n    # Step 2: Compute category counts and scaled midranks\n    unique_cats, cat_indices = np.unique(pooled_data, return_inverse=True)\n    num_cats = len(unique_cats)\n    \n    # c_k: counts of each unique category in the pooled data\n    c_k = np.bincount(cat_indices, minlength=num_cats)\n    \n    # x_k_obs: observed counts of each unique category in group A\n    group_a_cats = cat_indices[:n1]\n    x_k_obs = np.bincount(group_a_cats, minlength=num_cats)\n    \n    # m_star_k: scaled midranks for each category\n    cum_c = np.concatenate(([0], np.cumsum(c_k)))\n    m_star_k = 2 * cum_c[:-1] + c_k + 1\n\n    # Step 3: Compute observed statistic and mean\n    w_star_obs = np.sum(x_k_obs * m_star_k)\n    mu_star = n1 * (N + 1)\n\n    # Step 4: Enumerate all possible allocations and build the null distribution\n    distribution = defaultdict(float)\n    total_perms = comb(N, n1, exact=True)\n\n    allocations = []\n    def find_allocations(k_idx, n1_rem, current_alloc):\n        \"\"\"\n        Recursively find all valid allocations (x_k vectors).\n        \"\"\"\n        if k_idx == num_cats:\n            if n1_rem == 0:\n                allocations.append(current_alloc)\n            return\n\n        # Iterate through possible counts x_k for the current category\n        max_count = min(c_k[k_idx], n1_rem)\n        for x_k in range(max_count + 1):\n            find_allocations(k_idx + 1, n1_rem - x_k, current_alloc + [x_k])\n\n    find_allocations(0, n1, [])\n\n    for alloc_vec in allocations:\n        # Calculate the statistic W* for this allocation\n        w_star = np.sum(np.array(alloc_vec) * m_star_k)\n        \n        # Calculate the number of permutations for this allocation\n        prob_numerator = 1\n        for i in range(num_cats):\n            prob_numerator *= comb(c_k[i], alloc_vec[i], exact=True)\n            \n        # Add probability to the distribution map\n        probability = prob_numerator / total_perms\n        distribution[w_star] += probability\n\n    # Step 5: Calculate p-values from the null distribution\n    p_left = 0.0\n    p_right = 0.0\n    p_two = 0.0\n    \n    # d_obs is integer, as are all w_star and mu_star\n    d_obs = abs(w_star_obs - mu_star)\n\n    for w_star, prob in distribution.items():\n        if w_star <= w_star_obs:\n            p_left += prob\n        if w_star >= w_star_obs:\n            p_right += prob\n        if abs(w_star - mu_star) >= d_obs:\n            p_two += prob\n            \n    # Round results to 6 decimal places as required\n    return [round(p_two, 6), round(p_left, 6), round(p_right, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path with multiple ties)\n        {'group_a': [1, 2, 4, 4, 5], 'group_b': [1, 1, 3, 4, 5, 5, 5]},\n        \n        # Case 2 (degenerate all-equal category)\n        {'group_a': [3, 3], 'group_b': [3, 3, 3]},\n        \n        # Case 3 (no ties in the pooled data)\n        {'group_a': [1, 2], 'group_b': [3, 4, 5]},\n        \n        # Case 4 (balanced sizes with central ties)\n        {'group_a': [2, 2, 3, 4], 'group_b': [1, 2, 3, 3]}\n    ]\n    \n    results = []\n    for case in test_cases:\n        p_values = calculate_p_values(case['group_a'], case['group_b'])\n        results.append(p_values)\n        \n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}