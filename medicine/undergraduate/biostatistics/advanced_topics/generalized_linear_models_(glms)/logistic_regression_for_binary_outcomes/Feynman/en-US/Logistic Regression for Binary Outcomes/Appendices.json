{
    "hands_on_practices": [
        {
            "introduction": "A logistic regression model's direct output for a given set of predictors is a number on the log-odds scale, which isn't immediately intuitive. To make sense of a model's prediction, we must first convert this log-odds value back into a probability. This foundational exercise  walks you through deriving and applying the inverse-logit (or logistic) function, a critical skill for translating model outputs into clinically meaningful risk estimates.",
            "id": "4970702",
            "problem": "A clinical research team is developing a prognostic model for $30$-day mortality among patients admitted to the intensive care unit (ICU) with acute decompensated heart failure. Let $Y \\in \\{0,1\\}$ denote the binary outcome where $Y=1$ indicates death within $30$ days and $Y=0$ indicates survival. Assume $Y \\mid \\boldsymbol{X}$ follows a Bernoulli distribution with parameter $p(\\boldsymbol{X})$, and the model is a generalized linear model with the canonical logit link. For a particular patient with covariates $\\boldsymbol{X}$, the fitted linear predictor (also called the log-odds) is $\\eta=-1.386$.\n\nStarting only from the Bernoulli model and the definition of the canonical logit link in generalized linear models, derive the mapping from the linear predictor $\\eta$ to the probability $p(\\boldsymbol{X})$, and compute the numerical value of $p$ for this patient. Briefly state the clinical meaning of the obtained $p$ for $30$-day mortality risk.\n\nProvide the value of $p$ as a decimal rounded to three significant figures. Do not use a percentage sign.",
            "solution": "The problem statement is evaluated for validity.\n\n**Step 1: Extract Givens**\n-   Outcome variable: $Y \\in \\{0,1\\}$, where $Y=1$ denotes death within $30$ days and $Y=0$ denotes survival.\n-   Conditional distribution of the outcome: $Y \\mid \\boldsymbol{X} \\sim \\text{Bernoulli}(p(\\boldsymbol{X}))$.\n-   Model framework: Generalized linear model (GLM).\n-   Link function: Canonical logit link.\n-   Fitted linear predictor for a specific patient: $\\eta = -1.386$.\n-   Tasks:\n    1.  Derive the mapping from the linear predictor $\\eta$ to the probability $p(\\boldsymbol{X})$.\n    2.  Compute the numerical value of $p$ for the given patient.\n    3.  Briefly state the clinical meaning of the computed probability.\n    4.  Provide $p$ rounded to three significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the theory of generalized linear models, a standard and fundamental topic in statistical modeling. The use of a Bernoulli distribution for a binary outcome and the logit link function are canonical choices within this framework. The problem is well-posed, providing all necessary information to perform the requested derivation and calculation, leading to a unique solution. The terminology is precise and objective. The provided value for the linear predictor, $\\eta = -1.386$, is a plausible real-world value. There are no contradictions, ambiguities, or violations of scientific principles.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n***\n\nThe problem requires the derivation of the relationship between the linear predictor, $\\eta$, and the event probability, $p$, within the framework of a generalized linear model (GLM) for a Bernoulli-distributed outcome.\n\nA GLM consists of three components:\n1.  A random component: The distribution of the response variable $Y$ is a member of the exponential family.\n2.  A systematic component: A linear predictor, $\\eta = \\boldsymbol{X}^T\\boldsymbol{\\beta}$.\n3.  A link function: A function $g$ that relates the expected value of the random component, $\\mu = E[Y]$, to the systematic component, such that $g(\\mu) = \\eta$.\n\nFor the binary outcome $Y \\in \\{0,1\\}$, the conditional distribution is Bernoulli with parameter $p = P(Y=1 \\mid \\boldsymbol{X})$. The probability mass function is $f(y;p) = p^y(1-p)^{1-y}$. To identify the canonical link function, we must express this in the canonical form of the exponential family:\n$$f(y; \\theta) = \\exp \\left( \\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right)$$\nRewriting the Bernoulli PMF:\n$$f(y;p) = p^y(1-p)^{1-y} = \\exp\\left( \\ln\\left(p^y(1-p)^{1-y}\\right) \\right)$$\n$$= \\exp\\left( y\\ln(p) + (1-y)\\ln(1-p) \\right)$$\n$$= \\exp\\left( y\\ln(p) + \\ln(1-p) - y\\ln(1-p) \\right)$$\n$$= \\exp\\left( y\\left[\\ln(p) - \\ln(1-p)\\right] + \\ln(1-p) \\right)$$\n$$= \\exp\\left( y\\ln\\left(\\frac{p}{1-p}\\right) + \\ln(1-p) \\right)$$\nComparing this to the canonical form, we can identify the canonical parameter $\\theta = \\ln\\left(\\frac{p}{1-p}\\right)$. The quantity $\\frac{p}{1-p}$ is the odds of the event $Y=1$, and its natural logarithm is the log-odds. The expected value of a Bernoulli variable is $\\mu = E[Y] = p$.\n\nThe canonical link function is the function that maps the mean $\\mu$ to the canonical parameter $\\theta$. For the Bernoulli distribution, this is:\n$$g(p) = \\theta = \\ln\\left(\\frac{p}{1-p}\\right)$$\nThis function, $g(p)$, is known as the logit function.\n\nIn a GLM, the link function relates the mean of the outcome to the linear predictor, $\\eta$. Using the canonical logit link, we have:\n$$\\eta = g(p) = \\ln\\left(\\frac{p}{1-p}\\right)$$\nThe first task is to derive the mapping from $\\eta$ to $p$. This requires inverting the logit function. We solve the above equation for $p$:\n$$\\exp(\\eta) = \\frac{p}{1-p}$$\n$$\\exp(\\eta)(1-p) = p$$\n$$\\exp(\\eta) - p\\exp(\\eta) = p$$\n$$\\exp(\\eta) = p + p\\exp(\\eta)$$\n$$\\exp(\\eta) = p(1 + \\exp(\\eta))$$\n$$p = \\frac{\\exp(\\eta)}{1 + \\exp(\\eta)}$$\nThis expression provides the mapping from the linear predictor $\\eta$ to the probability $p$. This function is commonly known as the logistic sigmoid function. An equivalent and often more numerically stable form is obtained by dividing the numerator and denominator by $\\exp(\\eta)$:\n$$p = \\frac{1}{\\exp(-\\eta) + 1}$$\nThis completes the derivation.\n\nThe second task is to compute the numerical value of $p$ for the patient with a linear predictor of $\\eta = -1.386$. Using the derived formula:\n$$p = \\frac{1}{1 + \\exp(-\\eta)} = \\frac{1}{1 + \\exp(-(-1.386))}$$\n$$p = \\frac{1}{1 + \\exp(1.386)}$$\nWe calculate the value of $\\exp(1.386)$:\n$$\\exp(1.386) \\approx 3.998854$$\nSubstituting this back into the expression for $p$:\n$$p \\approx \\frac{1}{1 + 3.998854} = \\frac{1}{4.998854} \\approx 0.20004585$$\nThe problem requires the answer to be rounded to three significant figures. The first significant figure is $2$, the second is $0$, and the third is $0$. The fourth digit is $0$, so we round down.\n$$p \\approx 0.200$$\n\nThe final task is to state the clinical meaning of this value. The probability $p$ represents the model's prediction for the likelihood of the event $Y=1$ (death within $30$ days) for a patient with the given set of covariates $\\boldsymbol{X}$. Therefore, a value of $p=0.200$ means that for this particular patient, the prognostic model estimates a $20.0\\%$ probability of death within $30$ days of admission to the ICU. This is the patient's individual predicted risk.",
            "answer": "$$\n\\boxed{0.200}\n$$"
        },
        {
            "introduction": "Once we can predict the probability of an event, we often want to understand how a specific risk factor influences that probability. In logistic regression, the model coefficients ($\\beta$) quantify this relationship on the log-odds scale. This practice  demonstrates the crucial step of exponentiating a coefficient to find the odds ratio, which provides a clear, multiplicative interpretation of a covariate's effect on the outcome's odds.",
            "id": "4970670",
            "problem": "A prospective cohort study of adult surgical patients models the binary outcome $Y \\in \\{0,1\\}$, where $Y=1$ denotes severe postoperative infection within $30$ days. Let $\\mathbf{x} = (x_1,\\dots,x_p)$ denote patient covariates including high-sensitivity C-reactive protein (hs-CRP). The hs-CRP covariate is encoded as $x_j$, defined so that a one-unit increase in $x_j$ corresponds to an increase of $10$ mg/L in hs-CRP. Investigators fit a logistic regression model with a logit link, where the probability of infection given covariates is $p(\\mathbf{x}) = \\Pr(Y=1 \\mid \\mathbf{x})$, the odds of infection are $p(\\mathbf{x})/(1 - p(\\mathbf{x}))$, and the log-odds (logit) are $\\ln\\!\\left(\\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\\right)$. Suppose the estimated regression coefficient for $x_j$ is $\\beta_j = 0.693$.\n\nStarting from the definitions of odds and the logit link, and treating all other covariates as fixed, derive the multiplicative change in the odds of severe postoperative infection associated with a one-unit increase in $x_j$. Then, compute this multiplicative change for the given $\\beta_j$. Round your final numeric answer to four significant figures and do not include units. In addition, state the clinical interpretation of an odds ratio of $2$ per one-unit increase in $x_j$ (this interpretation is not part of the final numeric answer).",
            "solution": "The problem is valid as it is scientifically grounded in the principles of biostatistics, specifically logistic regression, and is well-posed with all necessary information provided for a unique solution.\n\nThe problem asks to derive the multiplicative change in the odds of an event for a unit increase in a covariate in a logistic regression model, compute its value for a given coefficient, and provide a clinical interpretation.\n\nLet the binary outcome be $Y \\in \\{0, 1\\}$, where $Y=1$ represents the event of interest (severe postoperative infection). The probability of this event, conditional on a vector of covariates $\\mathbf{x} = (x_1, \\dots, x_p)$, is denoted by $p(\\mathbf{x}) = \\Pr(Y=1 \\mid \\mathbf{x})$.\n\nThe logistic regression model uses a logit link function, which models the natural logarithm of the odds of the event as a linear function of the covariates. The odds are defined as the ratio of the probability of the event occurring to the probability of it not occurring:\n$$\n\\text{Odds}(\\mathbf{x}) = \\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\n$$\nThe log-odds (or logit) is therefore:\n$$\n\\text{logit}(p(\\mathbf{x})) = \\ln\\left(\\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\\right)\n$$\nIn the logistic regression model, the logit is expressed as:\n$$\n\\ln\\left(\\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\\right) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_j x_j + \\dots + \\beta_p x_p = \\beta_0 + \\sum_{k=1}^{p} \\beta_k x_k\n$$\nwhere $\\beta_0, \\beta_1, \\dots, \\beta_p$ are the regression coefficients.\n\nTo derive the multiplicative change in the odds associated with a one-unit increase in a specific covariate $x_j$, we must compare the odds at two different levels of $x_j$, while holding all other covariates $(x_k \\text{ for } k \\neq j)$ constant.\n\nLet the odds for a patient with covariates $\\mathbf{x}$ be denoted by $\\text{Odds}(\\mathbf{x})$. By exponentiating the logit equation, we get:\n$$\n\\text{Odds}(\\mathbf{x}) = \\exp\\left(\\beta_0 + \\sum_{k=1}^{p} \\beta_k x_k\\right)\n$$\nNow, consider a second scenario where the covariate $x_j$ is increased by one unit to $x_j + 1$. The new vector of covariates is $\\mathbf{x}' = (x_1, \\dots, x_j+1, \\dots, x_p)$. The odds for this scenario are:\n$$\n\\text{Odds}(\\mathbf{x}') = \\exp\\left(\\beta_0 + \\sum_{k \\neq j} \\beta_k x_k + \\beta_j(x_j + 1)\\right)\n$$\nThe multiplicative change in the odds is the ratio of the new odds to the original odds. This ratio is defined as the odds ratio (OR) for a one-unit change in $x_j$.\n$$\n\\text{OR}_j = \\frac{\\text{Odds}(\\mathbf{x}')}{\\text{Odds}(\\mathbf{x})} = \\frac{\\exp\\left(\\beta_0 + \\sum_{k \\neq j} \\beta_k x_k + \\beta_j(x_j + 1)\\right)}{\\exp\\left(\\beta_0 + \\sum_{k=1}^{p} \\beta_k x_k\\right)}\n$$\nWe can rewrite the exponent in the numerator:\n$$\n\\beta_0 + \\sum_{k \\neq j} \\beta_k x_k + \\beta_j x_j + \\beta_j = \\left(\\beta_0 + \\sum_{k=1}^{p} \\beta_k x_k\\right) + \\beta_j\n$$\nSubstituting this back into the expression for the odds ratio:\n$$\n\\text{OR}_j = \\frac{\\exp\\left(\\left(\\beta_0 + \\sum_{k=1}^{p} \\beta_k x_k\\right) + \\beta_j\\right)}{\\exp\\left(\\beta_0 + \\sum_{k=1}^{p} \\beta_k x_k\\right)}\n$$\nUsing the property $\\exp(a+b) = \\exp(a)\\exp(b)$, we get:\n$$\n\\text{OR}_j = \\frac{\\exp\\left(\\beta_0 + \\sum_{k=1}^{p} \\beta_k x_k\\right) \\exp(\\beta_j)}{\\exp\\left(\\beta_0 + \\sum_{k=1}^{p} \\beta_k x_k\\right)}\n$$\nThe term $\\exp\\left(\\beta_0 + \\sum_{k=1}^{p} \\beta_k x_k\\right)$ cancels from the numerator and denominator, yielding the fundamental result:\n$$\n\\text{OR}_j = \\exp(\\beta_j)\n$$\nThis derivation shows that the multiplicative change in the odds for a one-unit increase in $x_j$ is equal to the exponential of the coefficient $\\beta_j$.\n\nThe problem states that the estimated coefficient for the covariate $x_j$ (representing hs-CRP) is $\\beta_j = 0.693$. We can now compute the multiplicative change in the odds:\n$$\n\\text{OR}_j = \\exp(0.693)\n$$\nWe calculate this value and round to four significant figures as requested:\n$$\n\\exp(0.693) \\approx 1.99970535 \\approx 2.000\n$$\nThe multiplicative change in the odds is $2.000$.\n\nThe problem also requests the clinical interpretation of an odds ratio of $2$ per one-unit increase in $x_j$. The covariate $x_j$ is encoded such that a one-unit increase corresponds to a $10$ mg/L increase in hs-CRP. Therefore, an odds ratio of $2$ for $x_j$ means that for every $10$ mg/L increase in a patient's hs-CRP level, the odds of developing a severe postoperative infection within $30$ days are estimated to be multiplied by $2$ (i.e., they are doubled), assuming all other patient characteristics in the model are held constant.",
            "answer": "$$\\boxed{2.000}$$"
        },
        {
            "introduction": "We have seen how to interpret a fitted logistic regression model, but how are the model's coefficients estimated in the first place? Unlike linear regression, there is no simple closed-form solution; instead, an iterative procedure is used. This exercise  provides a rare hands-on look under the hood at the Iteratively Reweighted Least Squares (IRLS) algorithm, allowing you to perform one complete step of the fitting process and demystifying how statistical software arrives at its final estimates.",
            "id": "4970685",
            "problem": "Consider a binary outcome model for a short perioperative study where each patient $i \\in \\{1,2,3\\}$ contributes a single Bernoulli trial $y_{i} \\in \\{0,1\\}$ indicating occurrence of an acute postoperative complication. Let the covariate vector be $x_{i} \\in \\mathbb{R}^{2}$ with an intercept and a standardized operative complexity score, and denote the parameter vector by $\\beta \\in \\mathbb{R}^{2}$. The generalized linear model uses the logistic link so that the conditional mean is $p_{i} = \\mathbb{E}(y_{i} \\mid x_{i}, \\beta)$ satisfying $\\operatorname{logit}(p_{i}) = x_{i}^{\\top} \\beta$. The data are given by the design matrix $X = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$ and the outcome vector $y = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$. Starting at the initial parameter $\\beta^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, perform one iteration of Iteratively Reweighted Least Squares (IRLS) (first defining the score and observed information from the Bernoulli log-likelihood with the logistic link and then deriving the IRLS quantities from these first principles), and compute the following objects at the initial iterate: the vector of model-predicted probabilities $p^{(0)}$, the diagonal weight matrix $W^{(0)}$, the working response $z^{(0)}$, and the updated parameter vector $\\beta^{(1)}$. Express all computed quantities exactly as rational numbers or integers; no rounding is required. Provide your final answer as a single composite expression containing all four results in the order $\\left(p^{(0)}, W^{(0)}, z^{(0)}, \\beta^{(1)}\\right)$.",
            "solution": "The user wants to find the predicted probabilities $p^{(0)}$, the weight matrix $W^{(0)}$, the working response $z^{(0)}$, and the updated parameter vector $\\beta^{(1)}$ after one iteration of the Iteratively Reweighted Least Squares (IRLS) algorithm for a logistic regression model.\n\n### Step 1: Problem Validation\n\nThe problem is a standard, well-defined exercise in computational statistics for generalized linear models (GLMs).\n\n*   **Extracted Givens**:\n    *   Model: Logistic regression for a binary outcome $y_i \\in \\{0, 1\\}$.\n    *   Number of observations: $N=3$.\n    *   Link function: $\\operatorname{logit}(p_i) = \\ln\\left(\\frac{p_i}{1-p_i}\\right) = x_i^\\top \\beta$.\n    *   Design matrix: $X = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$.\n    *   Outcome vector: $y = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n    *   Initial parameter vector: $\\beta^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n    *   Task: Perform one IRLS iteration starting from $\\beta^{(0)}$ and compute $p^{(0)}$, $W^{(0)}$, $z^{(0)}$, and $\\beta^{(1)}$.\n\n*   **Validation Verdict**:\n    *   The problem is **scientifically grounded** in the theory of GLMs.\n    *   It is **well-posed**, providing all necessary information for a single, unique iteration of the IRLS algorithm.\n    *   The language is **objective** and mathematically precise.\n    *   The problem is **valid**.\n\n### Step 2: Derivation of the IRLS Algorithm from First Principles\n\nThe IRLS algorithm for GLMs can be derived as a Newton-Raphson method for maximizing the log-likelihood. For a binary outcome $y_i \\sim \\text{Bernoulli}(p_i)$, the log-likelihood for a single observation $i$ is:\n$$\n\\ell_i(\\beta) = y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)\n$$\nThe total log-likelihood is $\\ell(\\beta) = \\sum_{i=1}^{N} \\ell_i(\\beta)$. The link function relates the probability $p_i$ to the linear predictor $\\eta_i = x_i^\\top \\beta$:\n$$\n\\eta_i = \\ln\\left(\\frac{p_i}{1-p_i}\\right) \\implies p_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(-\\eta_i)}\n$$\nThis is the logistic sigmoid function, $\\sigma(\\eta_i)$. Its derivative is $\\frac{d p_i}{d \\eta_i} = p_i(1-p_i)$.\n\n**1. Score Function (Gradient of the Log-Likelihood)**\nWe find the gradient of $\\ell(\\beta)$ with respect to the parameter vector $\\beta$. Using the chain rule for a single observation's contribution:\n$$\n\\frac{\\partial \\ell_i}{\\partial \\beta_j} = \\frac{\\partial \\ell_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nThe components are:\n$$\n\\frac{\\partial \\ell_i}{\\partial p_i} = \\frac{y_i}{p_i} - \\frac{1-y_i}{1-p_i} = \\frac{y_i - p_i}{p_i(1-p_i)}\n$$\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = p_i(1-p_i)\n$$\n$$\n\\frac{\\partial \\eta_i}{\\partial \\beta_j} = \\frac{\\partial (x_i^\\top \\beta)}{\\partial \\beta_j} = x_{ij}\n$$\nCombining these gives:\n$$\n\\frac{\\partial \\ell_i}{\\partial \\beta_j} = \\frac{y_i - p_i}{p_i(1-p_i)} \\cdot p_i(1-p_i) \\cdot x_{ij} = (y_i - p_i) x_{ij}\n$$\nThe score vector for the full dataset is the sum over all observations, which can be written in matrix form:\n$$\nS(\\beta) = \\nabla_\\beta \\ell(\\beta) = \\sum_{i=1}^{N} (y_i - p_i) x_i = X^\\top (y - p)\n$$\n\n**2. Observed Information Matrix (Negative Hessian)**\nNext, we compute the Hessian matrix $\\mathcal{H}(\\beta) = \\frac{\\partial^2 \\ell(\\beta)}{\\partial \\beta \\partial \\beta^\\top}$. The entry $(\\mathcal{H})_{jk}$ is:\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\beta_k \\partial \\beta_j} = \\sum_{i=1}^{N} \\frac{\\partial}{\\partial \\beta_k} \\left[ (y_i - p_i) x_{ij} \\right] = \\sum_{i=1}^{N} - \\frac{\\partial p_i}{\\partial \\beta_k} x_{ij}\n$$\nUsing the chain rule again, $\\frac{\\partial p_i}{\\partial \\beta_k} = \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = p_i(1-p_i) x_{ik}$. Substituting this in:\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\beta_k \\partial \\beta_j} = \\sum_{i=1}^{N} -p_i(1-p_i) x_{ik} x_{ij}\n$$\nIn matrix form, the Hessian is $\\mathcal{H}(\\beta) = -X^\\top W X$, where $W$ is a diagonal matrix with diagonal entries $W_{ii} = p_i(1-p_i)$. The observed information matrix is $J(\\beta) = -\\mathcal{H}(\\beta) = X^\\top W X$.\n\n**3. IRLS Update Step**\nThe Newton-Raphson update to find the maximum of $\\ell(\\beta)$ is:\n$$\n\\beta^{(t+1)} = \\beta^{(t)} - [\\mathcal{H}(\\beta^{(t)})]^{-1} S(\\beta^{(t)})\n$$\nSubstituting our expressions for the score $S$ and Hessian $\\mathcal{H}$:\n$$\n\\beta^{(t+1)} = \\beta^{(t)} - [-X^\\top W^{(t)} X]^{-1} [X^\\top (y - p^{(t)})] = \\beta^{(t)} + (X^\\top W^{(t)} X)^{-1} X^\\top (y - p^{(t)})\n$$\nTo see this as a weighted least squares problem, we rearrange the terms:\n$$\n(X^\\top W^{(t)} X) \\beta^{(t+1)} = (X^\\top W^{(t)} X) \\beta^{(t)} + X^\\top (y - p^{(t)})\n$$\nLet us define a \"working response\" vector $z^{(t)}$ such that the update is the solution to the weighted normal equations: $\\beta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)}$. This implies $X^\\top W^{(t)} z^{(t)} = (X^\\top W^{(t)} X) \\beta^{(t)} + X^\\top (y - p^{(t)})$. We can solve for $z^{(t)}$:\n$$\nW^{(t)} z^{(t)} = W^{(t)} X \\beta^{(t)} + (y - p^{(t)}) \\implies z^{(t)} = X \\beta^{(t)} + (W^{(t)})^{-1} (y - p^{(t)})\n$$\nRecalling that $\\eta^{(t)} = X \\beta^{(t)}$, the working response is:\n$$\nz^{(t)} = \\eta^{(t)} + (W^{(t)})^{-1} (y - p^{(t)})\n$$\n\n### Step 3: Calculation for the First Iteration\n\nGiven the initial parameter vector $\\beta^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\n**1. Compute Probabilities $p^{(0)}$**\nFirst, calculate the linear predictors $\\eta^{(0)} = X \\beta^{(0)}$:\n$$\n\\eta^{(0)} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nNow, compute the probabilities $p_i^{(0)}$ using the inverse link function $p_i = \\sigma(\\eta_i)$:\n$$\np_i^{(0)} = \\frac{1}{1+\\exp(-\\eta_i^{(0)})} = \\frac{1}{1+\\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2} \\quad \\text{for } i=1,2,3\n$$\nThus, the vector of probabilities is:\n$$\np^{(0)} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}\n$$\n\n**2. Compute Weight Matrix $W^{(0)}$**\nThe diagonal weights are $W_{ii}^{(0)} = p_i^{(0)}(1 - p_i^{(0)})$:\n$$\nW_{ii}^{(0)} = \\frac{1}{2} \\left(1 - \\frac{1}{2}\\right) = \\frac{1}{4} \\quad \\text{for } i=1,2,3\n$$\nThe weight matrix is:\n$$\nW^{(0)} = \\begin{pmatrix} 1/4 & 0 & 0 \\\\ 0 & 1/4 & 0 \\\\ 0 & 0 & 1/4 \\end{pmatrix}\n$$\n\n**3. Compute Working Response $z^{(0)}$**\nThe working response is $z^{(0)} = \\eta^{(0)} + (W^{(0)})^{-1} (y - p^{(0)})$:\n$$\ny - p^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}\n$$\nThe inverse of the weight matrix is $(W^{(0)})^{-1} = \\text{diag}(4, 4, 4)$.\n$$\nz^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} -1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 2 \\\\ 2 \\end{pmatrix}\n$$\n\n**4. Compute Updated Parameters $\\beta^{(1)}$**\nThe update is $\\beta^{(1)} = (X^\\top W^{(0)} X)^{-1} X^\\top W^{(0)} z^{(0)}$.\nFirst, compute $X^\\top W^{(0)} X$:\n$$\nX^\\top W^{(0)} X = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1/4 & 0 & 0 \\\\ 0 & 1/4 & 0 \\\\ 0 & 0 & 1/4 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 3 & 3 \\\\ 3 & 5 \\end{pmatrix}\n$$\nNext, find the inverse of this matrix:\n$$\n(X^\\top W^{(0)} X)^{-1} = \\left(\\frac{1}{4} \\begin{pmatrix} 3 & 3 \\\\ 3 & 5 \\end{pmatrix}\\right)^{-1} = 4 \\cdot \\frac{1}{3(5)-3(3)} \\begin{pmatrix} 5 & -3 \\\\ -3 & 3 \\end{pmatrix} = \\frac{4}{6} \\begin{pmatrix} 5 & -3 \\\\ -3 & 3 \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} 5 & -3 \\\\ -3 & 3 \\end{pmatrix}\n$$\nNow, compute $X^\\top W^{(0)} z^{(0)}$:\n$$\nX^\\top W^{(0)} z^{(0)} = \\frac{1}{4} \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} -2+2+2 \\\\ 0+2+4 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 2 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 3/2 \\end{pmatrix}\n$$\nFinally, compute $\\beta^{(1)}$:\n$$\n\\beta^{(1)} = \\left(\\frac{2}{3} \\begin{pmatrix} 5 & -3 \\\\ -3 & 3 \\end{pmatrix}\\right) \\begin{pmatrix} 1/2 \\\\ 3/2 \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} 5(1/2) - 3(3/2) \\\\ -3(1/2) + 3(3/2) \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} 5/2 - 9/2 \\\\ -3/2 + 9/2 \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} -4/2 \\\\ 6/2 \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} -2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -4/3 \\\\ 2 \\end{pmatrix}\n$$\n\nThe four requested quantities at the initial iterate are:\n$p^{(0)} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}$, $W^{(0)} = \\begin{pmatrix} 1/4 & 0 & 0 \\\\ 0 & 1/4 & 0 \\\\ 0 & 0 & 1/4 \\end{pmatrix}$, $z^{(0)} = \\begin{pmatrix} -2 \\\\ 2 \\\\ 2 \\end{pmatrix}$, and $\\beta^{(1)} = \\begin{pmatrix} -4/3 \\\\ 2 \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\n\\begin{aligned}\np^{(0)} &= \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix} \\\\\nW^{(0)} &= \\begin{pmatrix} 1/4 & 0 & 0 \\\\ 0 & 1/4 & 0 \\\\ 0 & 0 & 1/4 \\end{pmatrix} \\\\\nz^{(0)} &= \\begin{pmatrix} -2 \\\\ 2 \\\\ 2 \\end{pmatrix} \\\\\n\\beta^{(1)} &= \\begin{pmatrix} -4/3 \\\\ 2 \\end{pmatrix}\n\\end{aligned}\n}\n$$"
        }
    ]
}