{
    "hands_on_practices": [
        {
            "introduction": "To truly understand a model's fit, we must first examine how well it predicts individual outcomes. This exercise tasks you with deriving and calculating deviance residuals, which are the fundamental building blocks of many goodness-of-fit statistics. By linking the log-likelihood of the model to the contribution of each observation, you will develop a powerful intuition for identifying points where the model's predictions diverge most significantly from reality .",
            "id": "4914512",
            "problem": "A biostatistics study uses logistic regression to model a binary outcome $Y_i \\in \\{0,1\\}$ given covariates $x_i$, yielding fitted probabilities $\\hat{p}_i = \\Pr(Y_i=1 \\mid x_i)$ for $i=1,\\dots,n$. Consider $n=8$ independent observations with outcomes and fitted probabilities\n- $(y_1,\\hat{p}_1)=(1,0.8)$,\n- $(y_2,\\hat{p}_2)=(0,0.8)$,\n- $(y_3,\\hat{p}_3)=(1,0.2)$,\n- $(y_4,\\hat{p}_4)=(0,0.2)$,\n- $(y_5,\\hat{p}_5)=(1,0.95)$,\n- $(y_6,\\hat{p}_6)=(0,0.95)$,\n- $(y_7,\\hat{p}_7)=(1,0.05)$,\n- $(y_8,\\hat{p}_8)=(0,0.05)$.\n\nStarting from the Bernoulli log-likelihood and the definition of the model deviance as twice the difference between the saturated and fitted log-likelihoods, derive the individual deviance residuals $r_i$ for each observation $i=1,\\dots,8$. Then, compute the root-mean-square of the deviance residuals across the $8$ observations. Briefly interpret the magnitudes of the residuals in terms of goodness-of-fit, referring to how they relate to calibration assessments such as the Hosmer-Lemeshow goodness-of-fit test (HL) and to classification metrics such as accuracy and the Receiver Operating Characteristic (ROC) curve. Round your final numerical answer (the root-mean-square deviance residual) to four significant figures.",
            "solution": "The user wants me to validate the following problem and, if it is valid, provide a solution.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n-   A logistic regression model is used for a binary outcome $Y_i \\in \\{0,1\\}$ given covariates $x_i$.\n-   The fitted probabilities are $\\hat{p}_i = \\Pr(Y_i=1 \\mid x_i)$ for $i=1,\\dots,n$.\n-   The number of independent observations is $n=8$.\n-   The observed outcomes and fitted probabilities are:\n    -   $(y_1,\\hat{p}_1)=(1,0.8)$\n    -   $(y_2,\\hat{p}_2)=(0,0.8)$\n    -   $(y_3,\\hat{p}_3)=(1,0.2)$\n    -   $(y_4,\\hat{p}_4)=(0,0.2)$\n    -   $(y_5,\\hat{p}_5)=(1,0.95)$\n    -   $(y_6,\\hat{p}_6)=(0,0.95)$\n    -   $(y_7,\\hat{p}_7)=(1,0.05)$\n    -   $(y_8,\\hat{p}_8)=(0,0.05)$\n-   The task is to:\n    1.  Derive the individual deviance residuals $r_i$ for each observation.\n    2.  Compute the root-mean-square of the deviance residuals.\n    3.  Briefly interpret the magnitudes of the residuals in relation to goodness-of-fit (calibration, Hosmer-Lemeshow test) and classification metrics (accuracy, ROC curve).\n    4.  Round the final numerical answer to four significant figures.\n\n#### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is firmly grounded in the statistical theory of generalized linear models, specifically logistic regression. Deviance residuals, the Hosmer-Lemeshow test, and classification metrics are standard tools for model assessment in this context. The premises and concepts are factually sound.\n2.  **Well-Posed**: The problem provides all necessary data and definitions to calculate the requested quantities. The question is unambiguous and leads to a unique numerical answer and a standard interpretation.\n3.  **Objective**: The problem is stated in precise, objective language. It requests calculations and interpretations based on established statistical principles, not subjective opinions.\n4.  **Completeness and Consistency**: The provided data are complete and internally consistent. There are no contradictions.\n5.  **Relevance**: The problem is directly relevant to the topic of goodness-of-fit assessment and classification metrics in logistic regression within biostatistics.\n\n#### Step 3: Verdict and Action\nThe problem is valid. It is a well-posed, scientifically sound exercise in applying and interpreting diagnostic measures for a logistic regression model. I will proceed with the solution.\n\n### Solution Derivation\n\nThe analysis begins with the log-likelihood function for a single observation $i$ from a Bernoulli distribution, which is the basis for logistic regression. Given the binary outcome $y_i \\in \\{0, 1\\}$ and the model's predicted probability $p_i = \\Pr(Y_i=1)$, the log-likelihood is:\n$$\nl(p_i; y_i) = y_i \\ln(p_i) + (1-y_i)\\ln(1-p_i)\n$$\nThe total log-likelihood for the fitted model across $n$ observations is the sum of individual log-likelihoods, evaluated at the fitted probabilities $\\hat{p}_i$:\n$$\nL(\\hat{\\mathbf{p}}) = \\sum_{i=1}^n \\left[ y_i \\ln(\\hat{p}_i) + (1-y_i)\\ln(1-\\hat{p}_i) \\right]\n$$\nA saturated model is a model that fits the data perfectly. For binary data, this is achieved by setting the predicted probability for each observation equal to its observed outcome, i.e., $\\hat{p}_i^{\\text{sat}} = y_i$. The log-likelihood of the saturated model is:\n$$\nL(\\mathbf{y}) = \\sum_{i=1}^n \\left[ y_i \\ln(y_i) + (1-y_i)\\ln(1-y_i) \\right]\n$$\nFor any $y_i \\in \\{0,1\\}$, the term inside the sum is $1\\ln(1) + (1-1)\\ln(0)$ or $0\\ln(0) + (1-0)\\ln(1)$. Since $\\lim_{x\\to 0} x\\ln(x) = 0$, each term in the sum is $0$. Therefore, the log-likelihood of the saturated model is $L(\\mathbf{y}) = 0$.\n\nThe model deviance, $D$, is defined as twice the difference between the log-likelihood of the saturated model and the fitted model. It measures the extent to which the fitted model deviates from the perfect fit of the saturated model.\n$$\nD = 2[L(\\mathbf{y}) - L(\\hat{\\mathbf{p}})] = 2[0 - L(\\hat{\\mathbf{p}})] = -2 \\sum_{i=1}^n \\left[ y_i \\ln(\\hat{p}_i) + (1-y_i)\\ln(1-\\hat{p}_i) \\right]\n$$\nThe total deviance $D$ is the sum of the individual deviance components, $d_i$, for each observation, where $D = \\sum_{i=1}^n d_i$. The individual deviance component is given by:\n$$\nd_i = -2 \\left[ y_i \\ln(\\hat{p}_i) + (1-y_i)\\ln(1-\\hat{p}_i) \\right]\n$$\nThe deviance residual, $r_i$, is the signed square root of the individual deviance component. The sign is determined by the sign of the raw residual, $y_i - \\hat{p}_i$. This ensures that the residual is positive for an unexpected success ($y_i=1$ with small $\\hat{p}_i$) and negative for an unexpected failure ($y_i=0$ with large $\\hat{p}_i$).\n$$\nr_i = \\text{sign}(y_i - \\hat{p}_i) \\sqrt{d_i}\n$$\nWe can write this out for the two possible outcomes:\n-   If $y_i=1$: $d_i = -2 \\ln(\\hat{p}_i)$. Since $0  \\hat{p}_i  1$, $\\text{sign}(1-\\hat{p}_i) = +1$. The residual is $r_i = \\sqrt{-2\\ln(\\hat{p}_i)}$.\n-   If $y_i=0$: $d_i = -2 \\ln(1-\\hat{p}_i)$. Since $0  \\hat{p}_i  1$, $\\text{sign}(0-\\hat{p}_i) = -1$. The residual is $r_i = -\\sqrt{-2\\ln(1-\\hat{p}_i)}$.\n\nNow, we apply these formulas to the $n=8$ observations provided.\n\n-   For $(y_1, \\hat{p}_1) = (1, 0.8)$: $r_1 = \\sqrt{-2\\ln(0.8)} \\approx \\sqrt{-2(-0.22314)} \\approx \\sqrt{0.44629} \\approx 0.6680$\n-   For $(y_2, \\hat{p}_2) = (0, 0.8)$: $r_2 = -\\sqrt{-2\\ln(1-0.8)} = -\\sqrt{-2\\ln(0.2)} \\approx -\\sqrt{-2(-1.60944)} \\approx -\\sqrt{3.21888} \\approx -1.7941$\n-   For $(y_3, \\hat{p}_3) = (1, 0.2)$: $r_3 = \\sqrt{-2\\ln(0.2)} \\approx \\sqrt{3.21888} \\approx 1.7941$\n-   For $(y_4, \\hat{p}_4) = (0, 0.2)$: $r_4 = -\\sqrt{-2\\ln(1-0.2)} = -\\sqrt{-2\\ln(0.8)} \\approx -\\sqrt{0.44629} \\approx -0.6680$\n-   For $(y_5, \\hat{p}_5) = (1, 0.95)$: $r_5 = \\sqrt{-2\\ln(0.95)} \\approx \\sqrt{-2(-0.05129)} \\approx \\sqrt{0.10259} \\approx 0.3203$\n-   For $(y_6, \\hat{p}_6) = (0, 0.95)$: $r_6 = -\\sqrt{-2\\ln(1-0.95)} = -\\sqrt{-2\\ln(0.05)} \\approx -\\sqrt{-2(-2.99573)} \\approx -\\sqrt{5.99146} \\approx -2.4477$\n-   For $(y_7, \\hat{p}_7) = (1, 0.05)$: $r_7 = \\sqrt{-2\\ln(0.05)} \\approx \\sqrt{5.99146} \\approx 2.4477$\n-   For $(y_8, \\hat{p}_8) = (0, 0.05)$: $r_8 = -\\sqrt{-2\\ln(1-0.05)} = -\\sqrt{-2\\ln(0.95)} \\approx -\\sqrt{0.10259} \\approx -0.3203$\n\nThe next step is to compute the root-mean-square (RMS) of these deviance residuals. The RMS is defined as:\n$$\n\\text{RMS} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n r_i^2}\n$$\nSince $r_i^2 = d_i$, the sum $\\sum_{i=1}^n r_i^2$ is simply the total deviance $D$. So, $\\text{RMS} = \\sqrt{D/n}$.\nLet's compute the total deviance $D$ by summing the individual squared residuals ($d_i=r_i^2$):\n$D = r_1^2+r_2^2+r_3^2+r_4^2+r_5^2+r_6^2+r_7^2+r_8^2$\n$D = (-2\\ln(0.8)) + (-2\\ln(0.2)) + (-2\\ln(0.2)) + (-2\\ln(0.8)) + (-2\\ln(0.95)) + (-2\\ln(0.05)) + (-2\\ln(0.05)) + (-2\\ln(0.95))$\n$D = -4(\\ln(0.8) + \\ln(0.2) + \\ln(0.95) + \\ln(0.05))$\n$D \\approx 0.44629 + 3.21888 + 3.21888 + 0.44629 + 0.10259 + 5.99146 + 5.99146 + 0.10259 \\approx 19.5184$\n\nWith $n=8$, the RMS is:\n$$\n\\text{RMS} = \\sqrt{\\frac{19.5184}{8}} \\approx \\sqrt{2.4398} \\approx 1.5620499\n$$\nRounding to four significant figures, the RMS of the deviance residuals is $1.562$.\n\n### Interpretation\nThe magnitudes of the deviance residuals provide insight into the model's goodness-of-fit at both the individual and aggregate levels. For a well-fitting model, deviance residuals are approximately standard normal, so values with magnitude greater than $2$ are considered large and indicative of poor fit for that specific observation.\n\n-   Observations $1$, $4$, $5$, and $8$ have small residuals ($|r_i|  1$), indicating the model's predictions were reasonable for these outcomes (e.g., for obs. 1, $\\hat{p}_1=0.8$ and $y_1=1$).\n-   Observations $2$ and $3$ have moderately large residuals ($|r_i| \\approx 1.79$), indicating a notable discrepancy between prediction and outcome (e.g., for obs. 3, the model predicted a low probability $\\hat{p}_3=0.2$ for an event that occurred, $y_3=1$).\n-   Observations $6$ and $7$ have large residuals ($|r_i| \\approx 2.45$), highlighting a severe lack of fit. For these cases, the model was very confident in its prediction (e.g., $\\hat{p}_6=0.95$), but the outcome was the opposite of what was expected ($y_6=0$). These points are \"surprising\" to the model and contribute heavily to the overall deviance.\n\n**Relation to Calibration and Hosmer-Lemeshow (HL) Test:**\nGoodness-of-fit assessment focuses heavily on **calibration**, which is a model's ability to produce predicted probabilities that match the observed event rates. The HL test formalizes this by grouping subjects by predicted risk and comparing observed to expected event counts in each group. The large residuals we observe, especially for observations $6$ and $7$, signify poor calibration. The model predicts probabilities near the boundaries ($0.05$ and $0.95$) where the opposite outcomes occurred. If these observations were part of an HL test, they would create large discrepancies between observed and expected counts in the corresponding risk groups, leading to a large HL chi-square statistic and a small p-value, indicating that the model is poorly calibrated. The large overall RMS deviance residual ($1.562$) also suggests a global lack of fit.\n\n**Relation to Classification Metrics (Accuracy, ROC Curve):**\nClassification metrics assess **discrimination**, the model's ability to distinguish between the two outcome classes ($Y=1$ vs. $Y=0$). This is distinct from calibration.\n-   **Accuracy:** Using a standard threshold of $0.5$, the model correctly classifies only $4$ out of $8$ observations (correct: $1,4,5,8$; incorrect: $2,3,6,7$), for an accuracy of $50\\%$. This is no better than random guessing.\n-   **ROC Curve:** The ROC curve is generated by plotting the true positive rate against the false positive rate at all possible thresholds. The area under this curve (AUC) quantifies discrimination. The set of predicted probabilities for the subjects with $y=1$ is $\\{\\hat{p}_1, \\hat{p}_3, \\hat{p}_5, \\hat{p}_7\\} = \\{0.8, 0.2, 0.95, 0.05\\}$. The set for subjects with $y=0$ is $\\{\\hat{p}_2, \\hat{p}_4, \\hat{p}_6, \\hat{p}_8\\} = \\{0.8, 0.2, 0.95, 0.05\\}$. The distributions of predicted probabilities for the two classes are identical. This means the model has absolutely no ability to discriminate between cases and controls. The ROC curve would lie on the diagonal line of no discrimination, and the AUC would be exactly $0.5$.\n\nIn conclusion, the deviance residuals reveal a model with a severe pathology. The large magnitudes point to poor calibration (a failure in goodness-of-fit), and the specific structure of the data shows a complete failure of discrimination as well. The model is effectively useless for both prediction and classification.",
            "answer": "$$\n\\boxed{1.562}\n$$"
        },
        {
            "introduction": "While individual residuals are insightful, we often need aggregate statistics to formally assess a model's calibration. This problem guides you through the manual calculation of the Hosmer-Lemeshow statistic, a standard method for evaluating if predicted probabilities align with observed event rates across different risk strata. You will also compute and compare two fundamental scoring rules, the cross-entropy (log-loss) and Brier score, to appreciate their distinct properties in penalizing prediction errors .",
            "id": "4914555",
            "problem": "A biostatistics team fits a logistic regression model to a binary clinical outcome $y_{i} \\in \\{0,1\\}$ for $n=20$ patients and obtains predicted probabilities $\\hat{p}_{i} \\in (0,1)$. The model is evaluated using goodness-of-fit and classification quality metrics derived from first principles.\n\nStarting from the Bernoulli likelihood for independent observations and its logarithm, define the entropy-based measure of classification quality as the negative average log-likelihood per observation. Also consider the mean squared error between predicted probabilities and outcomes. Use these foundations to compute both metrics for the dataset below and to discuss their local relationship via quadratic approximation.\n\nIn addition, assess model calibration using the Hosmer-Lemeshow (HL) goodness-of-fit statistic by grouping patients into $G=5$ risk groups of equal size based on sorting by $\\hat{p}_{i}$, and by comparing observed to expected counts of events within each group. Use the canonical HL grouping procedure and the Pearson-type grouping statistic.\n\nThe dataset is given as ordered pairs $(\\hat{p}_{i}, y_{i})$ for patients $i=1,\\dots,20$:\n$(0.05,0)$, $(0.08,0)$, $(0.10,0)$, $(0.12,1)$, $(0.18,0)$, $(0.22,0)$, $(0.25,1)$, $(0.28,0)$, $(0.32,0)$, $(0.35,1)$, $(0.45,1)$, $(0.50,0)$, $(0.55,1)$, $(0.60,1)$, $(0.65,0)$, $(0.70,1)$, $(0.78,1)$, $(0.82,1)$, $(0.88,1)$, $(0.92,1)$.\n\nTasks:\n- Derive, from the Bernoulli likelihood, the entropy-based negative average log-likelihood per observation and compute its value for the dataset.\n- Define the Brier score as the mean squared difference between $\\hat{p}_{i}$ and $y_{i}$ and compute its value for the dataset.\n- Using a second-order Taylor expansion around correct classification, explain the local relationship between the entropy-based measure and the Brier score.\n- Compute the Hosmer-Lemeshow statistic for $G=5$ equal-sized groups (sorted by $\\hat{p}_{i}$), showing expected and observed counts and the contribution of each group to the statistic.\n\nFinally, report the single quantity $R$ defined as the ratio of the entropy-based negative average log-likelihood to the Brier score. Round $R$ to four significant figures. No units are required, and express any intermediate proportions or probabilities in decimal form rather than as percentages.",
            "solution": "The problem is valid as it is scientifically grounded in established biostatistical principles, well-posed with all necessary data and clear objectives, and expressed in precise, objective language.\n\n### Part 1: Entropy-Based Negative Average Log-Likelihood\n\nThe model is for a binary outcome $y_i \\in \\{0, 1\\}$ for $i=1, \\dots, n$ independent patients. The predicted probability of the event ($y_i=1$) is $\\hat{p}_i$. The probability of observing the outcome $y_i$ is given by the Bernoulli probability mass function:\n$$ P(Y_i = y_i | \\hat{p}_i) = \\hat{p}_i^{y_i} (1-\\hat{p}_i)^{1-y_i} $$\nFor $n$ independent observations, the total likelihood is the product of the individual probabilities:\n$$ L(\\{\\hat{p}_i\\}; \\{y_i\\}) = \\prod_{i=1}^{n} \\hat{p}_i^{y_i} (1-\\hat{p}_i)^{1-y_i} $$\nThe log-likelihood, $\\ell$, is the natural logarithm of the likelihood:\n$$ \\ell = \\ln(L) = \\sum_{i=1}^{n} \\ln\\left( \\hat{p}_i^{y_i} (1-\\hat{p}_i)^{1-y_i} \\right) = \\sum_{i=1}^{n} [y_i \\ln(\\hat{p}_i) + (1-y_i) \\ln(1-\\hat{p}_i)] $$\nThe entropy-based measure of classification quality, which we denote as $E$, is defined as the negative average log-likelihood per observation. This is also known as the cross-entropy loss.\n$$ E = -\\frac{1}{n} \\ell = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\ln(\\hat{p}_i) + (1-y_i) \\ln(1-\\hat{p}_i)] $$\nTo compute its value for the given dataset, we have $n=20$. We separate the sum into two parts: one for observations where $y_i=1$ and one for $y_i=0$.\n\nFor cases with $y_i=1$ ($11$ observations):\n$$ \\sum_{y_i=1} \\ln(\\hat{p}_i) = \\ln(0.12) + \\ln(0.25) + \\ln(0.35) + \\ln(0.45) + \\ln(0.55) + \\ln(0.60) + \\ln(0.70) + \\ln(0.78) + \\ln(0.82) + \\ln(0.88) + \\ln(0.92) \\approx -7.478345 $$\nFor cases with $y_i=0$ ($9$ observations):\n$$ \\sum_{y_i=0} \\ln(1-\\hat{p}_i) = \\ln(1-0.05) + \\ln(1-0.08) + \\ln(1-0.10) + \\ln(1-0.18) + \\ln(1-0.22) + \\ln(1-0.28) + \\ln(1-0.32) + \\ln(1-0.50) + \\ln(1-0.65) \\approx -3.144083 $$\nThe total log-likelihood is $\\ell \\approx -7.478345 - 3.144083 = -10.622428$.\nThe entropy-based measure is:\n$$ E = -\\frac{1}{20} (-10.622428) \\approx 0.5311214 $$\n\n### Part 2: Brier Score (Mean Squared Error)\n\nThe Brier score, $BS$, is defined as the mean squared difference between the predicted probabilities $\\hat{p}_i$ and the actual outcomes $y_i$.\n$$ BS = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{p}_i)^2 $$\nTo compute its value for the dataset ($n=20$):\nFor cases with $y_i=1$:\n$$ \\sum_{y_i=1} (1-\\hat{p}_i)^2 = (1-0.12)^2 + (1-0.25)^2 + \\dots + (1-0.92)^2 = 0.88^2 + 0.75^2 + 0.65^2 + 0.55^2 + 0.45^2 + 0.40^2 + 0.30^2 + 0.22^2 + 0.18^2 + 0.12^2 + 0.08^2 = 2.616 $$\nFor cases with $y_i=0$:\n$$ \\sum_{y_i=0} (0-\\hat{p}_i)^2 = (0.05)^2 + (0.08)^2 + \\dots + (0.65)^2 = 0.05^2 + 0.08^2 + 0.10^2 + 0.18^2 + 0.22^2 + 0.28^2 + 0.32^2 + 0.50^2 + 0.65^2 = 0.953 $$\nThe total sum of squares is $2.616 + 0.953 = 3.569$.\nThe Brier score is:\n$$ BS = \\frac{1}{20} (3.569) = 0.17845 $$\n\n### Part 3: Local Relationship between Entropy and Brier Score\n\nLet's analyze the per-observation loss for the entropy measure, $E_i$, and the Brier score, $BS_i$.\n$$ E_i = -[y_i \\ln(\\hat{p}_i) + (1-y_i) \\ln(1-\\hat{p}_i)] $$\n$$ BS_i = (y_i - \\hat{p}_i)^2 $$\nWe examine the local behavior of $E_i$ around \"correct classification,\" which means the predicted probability $\\hat{p}_i$ is very close to the true outcome $y_i$. Let the deviation be $\\epsilon = |y_i - \\hat{p}_i|$, where $\\epsilon \\to 0^+$.\n\nCase 1: $y_i=1$. Correct classification implies $\\hat{p}_i \\to 1$. Let $\\hat{p}_i = 1-\\epsilon$.\nThe entropy loss is $E_i = -\\ln(1-\\epsilon)$.\nThe Brier score is $BS_i = (1 - (1-\\epsilon))^2 = \\epsilon^2$.\nUsing the second-order Taylor expansion of $-\\ln(1-\\epsilon)$ around $\\epsilon=0$:\n$$ E_i = - \\left( -\\epsilon - \\frac{\\epsilon^2}{2} - O(\\epsilon^3) \\right) = \\epsilon + \\frac{\\epsilon^2}{2} + O(\\epsilon^3) $$\nCase 2: $y_i=0$. Correct classification implies $\\hat{p}_i \\to 0$. Let $\\hat{p}_i = \\epsilon$.\nThe entropy loss is $E_i = -\\ln(1-\\epsilon)$.\nThe Brier score is $BS_i = (0 - \\epsilon)^2 = \\epsilon^2$.\nThe Taylor expansion is identical:\n$$ E_i = \\epsilon + \\frac{\\epsilon^2}{2} + O(\\epsilon^3) $$\nIn both cases, for a small deviation $\\epsilon = |y_i - \\hat{p}_i|$, we have $BS_i = \\epsilon^2$. Substituting $\\epsilon = \\sqrt{BS_i}$ into the expansion for $E_i$:\n$$ E_i \\approx \\sqrt{BS_i} + \\frac{1}{2} BS_i $$\nThis relationship shows that for predictions very close to the true outcome, the entropy loss is dominated by a term linear in $\\epsilon$ ($\\sqrt{BS_i}$), while the Brier score is quadratic ($\\epsilon^2$). This means the entropy loss penalizes small prediction errors more severely than the Brier score.\n\n### Part 4: Hosmer-Lemeshow Statistic\n\nWe sort patients by $\\hat{p}_i$ and form $G=5$ groups of equal size, $n_g = 20/5=4$. For each group $g$, we calculate the observed number of events ($O_g = \\sum y_i$) and the expected number of events ($E_g = \\sum \\hat{p}_i$). The Hosmer-Lemeshow statistic $H$ is a Pearson-type chi-squared statistic calculated from these counts:\n$$ H = \\sum_{g=1}^{G} \\left[ \\frac{(O_g - E_g)^2}{E_g} + \\frac{((n_g - O_g) - (n_g - E_g))^2}{n_g - E_g} \\right] $$\nThe calculations are summarized in the following table.\n\n| Group (g) | Patients (by index $i$) | Predicted Probabilities ($\\hat{p}_i$) | Outcomes ($y_i$) | $n_g$ | $O_g$ | $E_g$ | $O_g-E_g$ | Contribution to $H$ |\n| :---: | :---: | :--- | :--- | :---: | :---: | :---: | :---: | :---: |\n| 1 | 1-4 | $0.05, 0.08, 0.10, 0.12$ | $0, 0, 0, 1$ | 4 | 1 | 0.35 | 0.65 | $\\frac{0.65^2}{0.35} + \\frac{(-0.65)^2}{3.65} \\approx 1.32290$ |\n| 2 | 5-8 | $0.18, 0.22, 0.25, 0.28$ | $0, 0, 1, 0$ | 4 | 1 | 0.93 | 0.07 | $\\frac{0.07^2}{0.93} + \\frac{(-0.07)^2}{3.07} \\approx 0.00687$ |\n| 3 | 9-12 | $0.32, 0.35, 0.45, 0.50$ | $0, 1, 1, 0$ | 4 | 2 | 1.62 | 0.38 | $\\frac{0.38^2}{1.62} + \\frac{(-0.38)^2}{2.38} \\approx 0.14981$ |\n| 4 | 13-16 | $0.55, 0.60, 0.65, 0.70$ | $1, 1, 0, 1$ | 4 | 3 | 2.50 | 0.50 | $\\frac{0.50^2}{2.50} + \\frac{(-0.50)^2}{1.50} \\approx 0.26667$ |\n| 5 | 17-20 | $0.78, 0.82, 0.88, 0.92$ | $1, 1, 1, 1$ | 4 | 4 | 3.40 | 0.60 | $\\frac{0.60^2}{3.40} + \\frac{(-0.60)^2}{0.60} \\approx 0.70588$ |\n\nSumming the contributions from each group:\n$$ H \\approx 1.32290 + 0.00687 + 0.14981 + 0.26667 + 0.70588 \\approx 2.45213 $$\n\n### Part 5: Final Ratio Calculation\n\nThe final task is to compute the ratio $R$ of the entropy-based negative average log-likelihood to the Brier score.\n$$ R = \\frac{E}{BS} = \\frac{0.5311214}{0.17845} \\approx 2.976296 $$\nRounding to four significant figures, we get:\n$$ R \\approx 2.976 $$",
            "answer": "$$\n\\boxed{2.976}\n$$"
        },
        {
            "introduction": "The final step in mastering model assessment is to translate theoretical knowledge into computational practice. This capstone problem challenges you to build a complete analysis pipeline, from fitting a logistic regression model to evaluating its performance with simulation-based methods. By implementing the Pearson chi-squared test and using a parametric bootstrap to assess its significance, you will gain hands-on experience with a powerful technique for generating reliable inferences when standard theoretical assumptions may not hold .",
            "id": "4914521",
            "problem": "Consider a binary response setting with observations $\\{(y_i, x_i)\\}_{i=1}^n$, where $y_i \\in \\{0,1\\}$ and $x_i \\in \\mathbb{R}^p$. The logistic regression model assumes $\\Pr(Y_i = 1 \\mid x_i) = \\sigma(x_i^\\top \\beta)$ with the logistic function $\\sigma(u) = 1/(1+\\exp(-u))$. The parameters $\\beta \\in \\mathbb{R}^{p+1}$ (including an intercept) are estimated by maximum likelihood via optimization of the Bernoulli log-likelihood. For numerical stability, you may include a small $\\ell_2$ (ridge) penalty, but the inferential targets remain those implied by the unpenalized Bernoulli likelihood.\n\nAfter fitting and obtaining $\\hat{\\beta}$, your task is to assess goodness-of-fit, perform the Hosmer-Lemeshow grouping test, and compute classification metrics, using principled constructions from first principles:\n\n1. Fit the logistic regression model by maximizing the Bernoulli log-likelihood to obtain $\\hat{\\beta}$ and predicted probabilities $\\hat{p}_i = \\sigma(x_i^\\top \\hat{\\beta})$.\n2. Construct the Pearson-type goodness-of-fit statistic $X^2$ from standardized residuals that use the Bernoulli variance model. This statistic must be derived from the residual form $(y_i - \\hat{p}_i)$ and the variance $\\hat{p}_i(1-\\hat{p}_i)$ under the model.\n3. Implement a parametric bootstrap to assess the sampling distribution of $X^2$ under the fitted model by generating bootstrap outcomes $y_i^\\ast \\sim \\operatorname{Bernoulli}(\\hat{p}_i)$, refitting the logistic model to each bootstrap dataset to obtain new predicted probabilities $\\hat{p}_i^\\ast$, and recomputing the bootstrap statistic $X^{2\\ast}$. Use the right-tail fraction $\\Pr^\\ast(X^{2\\ast} \\ge X^2)$ as the bootstrap $p$-value.\n4. Implement the Hosmer-Lemeshow (HL) test by grouping observations into $G$ bins based on quantiles of $\\hat{p}_i$ (groups should be formed to be as equal in size as possible). For each group, compute observed and model-expected event counts and derive the HL statistic using the binomial variance at the group level. Use the chi-square approximation with degrees of freedom $G-2$ to compute the HL $p$-value.\n5. For a given classification threshold $t \\in (0,1)$, compute accuracy, sensitivity (true positive rate), and specificity (true negative rate) using the fitted $\\hat{p}_i$ and the empirical outcomes $y_i$. If a denominator in any rate is zero, treat the corresponding rate as a real number following the conventional limiting definitions applicable to the confusion matrix.\n\nYour program must implement the above principled steps and evaluate the following test suite. In each case, generate covariates $x_i$ with an intercept and independent standard normal predictors, then generate outcomes $y_i$ from a known logistic model with specified coefficients. Use the specified random number generator seed to ensure reproducibility. Specifically, for each test case, do the following:\n- Generate $n$ independent $p$-dimensional predictors with entries drawn from the standard normal distribution, prepend an intercept of $1$ to form $(p+1)$-dimensional covariate vectors, and compute true probabilities $p_i^{\\text{true}} = \\sigma(x_i^\\top \\beta_{\\text{true}})$.\n- Generate the empirical outcomes $y_i \\sim \\operatorname{Bernoulli}(p_i^{\\text{true}})$.\n- Fit the logistic model to $(x_i, y_i)$ to obtain $\\hat{\\beta}$ and $\\hat{p}_i$.\n- Compute $X^2$, its bootstrap $p$-value with $B$ replicates, the HL statistic with $G$ groups and its chi-square $p$-value, and accuracy, sensitivity, specificity at threshold $t$.\n\nTest suite parameter sets:\n- Case A (general case): seed $= 42$, $n = 200$, $p = 2$, $\\beta_{\\text{true}} = [-1.0, 0.8, -0.5]$, $G = 10$, $B = 300$, $t = 0.5$.\n- Case B (rare events): seed $= 314$, $n = 150$, $p = 2$, $\\beta_{\\text{true}} = [-3.0, 1.2, 0.0]$, $G = 8$, $B = 300$, $t = 0.3$.\n- Case C (moderate extremes): seed $= 7$, $n = 120$, $p = 3$, $\\beta_{\\text{true}} = [-0.5, 1.5, -1.0, 0.5]$, $G = 6$, $B = 250$, $t = 0.5$.\n\nOutput specification:\n- For each test case, return a list of seven real numbers in the following order: $[X^2, p_{\\text{boot}}, H_{\\text{HL}}, p_{\\text{HL}}, \\text{accuracy}, \\text{sensitivity}, \\text{specificity}]$.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of these per-case lists, enclosed in square brackets, with no spaces. For example, the output should look like $[[r_{11},r_{12},\\dots,r_{17}],[r_{21},\\dots,r_{27}],[r_{31},\\dots,r_{37}]]$, where each $r_{jk}$ is a real number. Express all real numbers as decimals (not percentages).\n\nAll quantities are unitless. Angles are not involved. The entire computation must be self-contained and reproducible from the specification above, using the seeds provided.",
            "solution": "The user-provided problem is a well-defined task in biostatistics, requiring the implementation of a complete workflow for logistic regression analysis, from model fitting to goodness-of-fit assessment and performance evaluation. The problem is scientifically grounded, internally consistent, and requires no information beyond what is provided. The steps outlined are standard statistical procedures. A minor ambiguity regarding the magnitude of a small $\\ell_2$ penalty for numerical stability is resolved by selecting a standard small value, which aligns with the problem's intent. The problem is deemed valid and a full solution is presented below.\n\nThe core of the problem is to analyze a binary response $y_i \\in \\{0, 1\\}$ as a function of covariates $x_i \\in \\mathbb{R}^p$ using the logistic regression model. An intercept is included, so the covariate vectors are $(p+1)$-dimensional. The model postulates that the probability of a positive outcome ($y_i=1$) is given by:\n$$\n\\Pr(Y_i = 1 \\mid x_i) = p_i = \\sigma(x_i^\\top \\beta) = \\frac{1}{1 + \\exp(-x_i^\\top \\beta)}\n$$\nwhere $\\beta \\in \\mathbb{R}^{p+1}$ is the vector of model coefficients.\n\n**1. Logistic Regression Model Fitting**\n\nThe coefficients $\\hat{\\beta}$ are estimated by maximizing the log-likelihood of the observed data. For $n$ independent observations, the likelihood function is the product of Bernoulli probabilities:\n$$\nL(\\beta) = \\prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}\n$$\nThe log-likelihood is:\n$$\n\\ell(\\beta) = \\log L(\\beta) = \\sum_{i=1}^n \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n$$\nTo enhance numerical stability, particularly in cases of near-perfect separation in the data, a small $\\ell_2$ (ridge) penalty is added. The penalized log-likelihood is:\n$$\n\\ell_{\\text{pen}}(\\beta) = \\ell(\\beta) - \\frac{\\lambda}{2} \\|\\beta\\|_2^2\n$$\nwhere $\\lambda$ is a small, positive regularization parameter. We seek $\\hat{\\beta} = \\arg\\max_{\\beta} \\ell_{\\text{pen}}(\\beta)$, which is equivalent to minimizing the negative penalized log-likelihood, $J(\\beta) = - \\ell_{\\text{pen}}(\\beta)$.\n\nWe use the Newton-Raphson algorithm, an iterative optimization method that uses second-order information. The update rule is:\n$$\n\\beta_{k+1} = \\beta_k - H_k^{-1} g_k\n$$\nwhere $g_k = \\nabla J(\\beta_k)$ is the gradient and $H_k = \\nabla^2 J(\\beta_k)$ is the Hessian of the objective function, both evaluated at $\\beta_k$.\n\nThe gradient is:\n$$\ng(\\beta) = \\nabla J(\\beta) = - \\nabla \\ell_{\\text{pen}}(\\beta) = - \\left( \\sum_{i=1}^n (y_i - p_i) x_i - \\lambda \\beta \\right) = X^\\top(p-y) + \\lambda\\beta\n$$\nThe Hessian is:\n$$\nH(\\beta) = \\nabla^2 J(\\beta) = - \\nabla^2 \\ell_{\\text{pen}}(\\beta) = X^\\top W X + \\lambda I\n$$\nwhere $X$ is the $n \\times (p+1)$ design matrix, $y$ is the vector of outcomes, $p$ is the vector of probabilities $\\sigma(X\\beta)$, $I$ is the identity matrix, and $W$ is a diagonal matrix with diagonal entries $W_{ii} = p_i(1-p_i)$. The iterations start with an initial guess, typically $\\beta_0 = 0$, and continue until convergence. Once $\\hat{\\beta}$ is found, the fitted probabilities are $\\hat{p}_i = \\sigma(x_i^\\top \\hat{\\beta})$.\n\n**2. Pearson Goodness-of-Fit Statistic ($X^2$)**\n\nThe Pearson-type goodness-of-fit statistic, $X^2$, measures the discrepancy between observed outcomes and predicted probabilities. It is the sum of squared Pearson residuals:\n$$\nX^2 = \\sum_{i=1}^n \\frac{(y_i - \\hat{p}_i)^2}{\\hat{p}_i(1-\\hat{p}_i)}\n$$\nThe denominator $\\hat{p}_i(1-\\hat{p}_i)$ is the estimated variance of the Bernoulli random variable $Y_i$. A large value of $X^2$ suggests a poor model fit.\n\n**3. Parametric Bootstrap for $X^2$ p-value**\n\nThe theoretical asymptotic distribution of $X^2$ (typically $\\chi^2_{n-p-1}$) is often a poor approximation in practice. A parametric bootstrap provides a more accurate assessment of the sampling distribution of $X^2$ under the assumption that the fitted model is correct. The procedure is as follows:\n1.  Fit the model to the original data $(X, y)$ to obtain $\\hat{\\beta}$ and $\\hat{p}$. Calculate the observed statistic $X^2$.\n2.  For each of $B$ bootstrap replications:\n    a. Generate a bootstrap sample of outcomes $y^\\ast$ where each $y_i^\\ast \\sim \\operatorname{Bernoulli}(\\hat{p}_i)$.\n    b. Fit the logistic regression model to the bootstrap data $(X, y^\\ast)$ to obtain new estimates $\\hat{\\beta}^\\ast$ and $\\hat{p}^\\ast$.\n    c. Compute the bootstrap statistic $X^{2\\ast} = \\sum_{i=1}^n \\frac{(y_i^\\ast - \\hat{p}_i^\\ast)^2}{\\hat{p}_i^\\ast(1-\\hat{p}_i^\\ast)}$.\n3.  The bootstrap p-value is the proportion of bootstrap statistics that are greater than or equal to the observed statistic:\n$$\np_{\\text{boot}} = \\frac{1}{B} \\sum_{j=1}^B \\mathbb{I}(X^{2\\ast}_j \\ge X^2)\n$$\n\n**4. Hosmer-Lemeshow (HL) Test**\n\nThe Hosmer-Lemeshow test is another goodness-of-fit test that groups observations based on their predicted probabilities.\n1.  The observations are sorted according to their fitted probabilities $\\hat{p}_i$.\n2.  The data is partitioned into $G$ groups of as nearly equal size as possible.\n3.  For each group $g \\in \\{1, \\dots, G\\}$, we calculate:\n    -   $N_g$: the number of subjects in group $g$.\n    -   $O_g = \\sum_{i \\in g} y_i$: the observed number of events (outcomes $y_i=1$).\n    -   $E_g = \\sum_{i \\in g} \\hat{p}_i$: the expected number of events under the model.\n4.  The HL statistic is calculated as a Pearson-like chi-square statistic on the $2 \\times G$ table of observed and expected counts (events vs. non-events across groups):\n$$\nH_{\\text{HL}} = \\sum_{g=1}^G \\frac{(O_g - E_g)^2}{N_g \\bar{p}_g (1-\\bar{p}_g)}\n$$\nwhere $\\bar{p}_g = E_g / N_g$ is the average estimated probability in group $g$. The denominator is the variance of the sum of $N_g$ Bernoulli trials, approximated using the average probability $\\bar{p}_g$.\n5.  Under the null hypothesis that the model is well-calibrated, $H_{\\text{HL}}$ approximately follows a chi-square distribution with $G-2$ degrees of freedom. The p-value $p_{\\text{HL}}$ is computed from this distribution: $p_{\\text{HL}} = \\Pr(\\chi_{G-2}^2 \\ge H_{\\text{HL}})$.\n\n**5. Classification Metrics**\n\nGiven a classification threshold $t$, an observation is classified as positive ($\\hat{y}_i=1$) if $\\hat{p}_i \\ge t$, and negative ($\\hat{y}_i=0$) otherwise. Performance is assessed using a confusion matrix, which tabulates:\n-   True Positives (TP): $\\sum_i \\mathbb{I}(\\hat{y}_i = 1 \\text{ and } y_i = 1)$\n-   True Negatives (TN): $\\sum_i \\mathbb{I}(\\hat{y}_i = 0 \\text{ and } y_i = 0)$\n-   False Positives (FP): $\\sum_i \\mathbb{I}(\\hat{y}_i = 1 \\text{ and } y_i = 0)$\n-   False Negatives (FN): $\\sum_i \\mathbb{I}(\\hat{y}_i = 0 \\text{ and } y_i = 1)$\n\nFrom these values, we compute:\n-   **Accuracy**: The proportion of correct classifications.\n    $$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n-   **Sensitivity** (True Positive Rate, Recall): The proportion of actual positives that are correctly identified.\n    $$ \\text{Sensitivity} = \\frac{TP}{TP + FN} $$\n-   **Specificity** (True Negative Rate): The proportion of actual negatives that are correctly identified.\n    $$ \\text{Specificity} = \\frac{TN}{TN + FP} $$\nIf a denominator is zero (i.e., no actual positives for sensitivity or no actual negatives for specificity), the corresponding rate is taken to be $0.0$, as no instances could be correctly classified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test suite for logistic regression analysis.\n    \"\"\"\n\n    LAMBDA_REG = 1e-6 # Small L2 regularization parameter\n\n    def _sigmoid(u):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-u))\n\n    def _fit_logistic_regression(X, y, max_iter=25, tol=1e-8):\n        \"\"\"\n        Fits a logistic regression model using Newton-Raphson.\n\n        Args:\n            X (np.ndarray): Design matrix of shape (n_samples, n_features+1).\n            y (np.ndarray): Response vector of shape (n_samples,).\n            max_iter (int): Maximum number of iterations.\n            tol (float): Convergence tolerance for log-likelihood change.\n\n        Returns:\n            tuple: A tuple containing:\n                - np.ndarray: Estimated coefficients beta_hat.\n                - np.ndarray: Predicted probabilities p_hat.\n        \"\"\"\n        n_samples, n_features = X.shape\n        beta = np.zeros(n_features)\n        \n        # Penalize all coefficients, including intercept, for stability\n        I_reg = np.eye(n_features) * LAMBDA_REG\n\n        # Newton-Raphson iterations\n        ll_old = -np.inf\n        for _ in range(max_iter):\n            u = X @ beta\n            p = _sigmoid(u)\n            \n            # Avoid p=0 or p=1 for numerical stability in W\n            p = np.clip(p, 1e-10, 1 - 1e-10)\n\n            # Gradient and Hessian of negative penalized log-likelihood\n            grad = X.T @ (p - y) + LAMBDA_REG * beta\n            W = np.diag(p * (1 - p))\n            hessian = X.T @ W @ X + I_reg\n\n            # Update step\n            try:\n                # Use psuedo-inverse for more stability\n                step = np.linalg.pinv(hessian) @ grad\n                beta = beta - step\n            except np.linalg.LinAlgError:\n                break # Stop if Hessian is singular\n\n            # Check for convergence\n            ll_new = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p)) - 0.5 * LAMBDA_REG * np.dot(beta, beta)\n            if np.abs(ll_new - ll_old)  tol:\n                break\n            ll_old = ll_new\n            \n        final_p = _sigmoid(X @ beta)\n        return beta, final_p\n\n    test_cases = [\n        {'seed': 42, 'n': 200, 'p': 2, 'beta_true': [-1.0, 0.8, -0.5], 'G': 10, 'B': 300, 't': 0.5},\n        {'seed': 314, 'n': 150, 'p': 2, 'beta_true': [-3.0, 1.2, 0.0], 'G': 8, 'B': 300, 't': 0.3},\n        {'seed': 7, 'n': 120, 'p': 3, 'beta_true': [-0.5, 1.5, -1.0, 0.5], 'G': 6, 'B': 250, 't': 0.5},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        seed, n, p, beta_true, G, B, t = case.values()\n        \n        # Reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        X_cov = rng.standard_normal((n, p))\n        X = np.hstack([np.ones((n, 1)), X_cov])\n        beta_true = np.array(beta_true)\n        p_true = _sigmoid(X @ beta_true)\n        y = rng.binomial(1, p_true)\n\n        # 1. Fit the model\n        beta_hat, p_hat = _fit_logistic_regression(X, y)\n        \n        # Clip p_hat to avoid division by zero in stats\n        p_hat_clipped = np.clip(p_hat, 1e-10, 1 - 1e-10)\n\n        # 2. Pearson Chi-squared statistic\n        X2 = np.sum((y - p_hat)**2 / (p_hat_clipped * (1 - p_hat_clipped)))\n\n        # 3. Parametric Bootstrap\n        bootstrap_stats = []\n        for _ in range(B):\n            y_boot = rng.binomial(1, p_hat)\n            try:\n                _, p_boot = _fit_logistic_regression(X, y_boot)\n                p_boot_clipped = np.clip(p_boot, 1e-10, 1 - 1e-10)\n                X2_boot = np.sum((y_boot - p_boot)**2 / (p_boot_clipped * (1 - p_boot_clipped)))\n                bootstrap_stats.append(X2_boot)\n            except np.linalg.LinAlgError:\n                # Skip if a bootstrap fit fails\n                continue\n        \n        if len(bootstrap_stats)  0:\n            p_boot = np.mean(np.array(bootstrap_stats) = X2)\n        else:\n            p_boot = np.nan # Should not happen with regularization\n\n        # 4. Hosmer-Lemeshow Test\n        sorted_indices = np.argsort(p_hat)\n        groups = np.array_split(sorted_indices, G)\n        \n        hl_stat = 0.0\n        for group_indices in groups:\n            y_group = y[group_indices]\n            p_hat_group = p_hat[group_indices]\n            N_g = len(y_group)\n            O_g = np.sum(y_group)\n            E_g = np.sum(p_hat_group)\n            \n            # Using the formulation based on binomial variance at group level\n            avg_p_g = E_g / N_g if N_g  0 else 0\n            denom = N_g * avg_p_g * (1 - avg_p_g)\n            \n            if denom  1e-9: # Avoid division by zero\n                hl_stat += (O_g - E_g)**2 / denom\n        \n        p_hl = chi2.sf(hl_stat, df=G - 2)\n\n        # 5. Classification Metrics\n        y_pred = (p_hat = t).astype(int)\n        \n        # Confusion matrix elements\n        tp = np.sum((y_pred == 1)  (y == 1))\n        tn = np.sum((y_pred == 0)  (y == 0))\n        fp = np.sum((y_pred == 1)  (y == 0))\n        fn = np.sum((y_pred == 0)  (y == 1))\n\n        accuracy = (tp + tn) / n\n        \n        sensitivity_denom = tp + fn\n        sensitivity = tp / sensitivity_denom if sensitivity_denom  0 else 0.0\n        \n        specificity_denom = tn + fp\n        specificity = tn / specificity_denom if specificity_denom  0 else 0.0\n        \n        case_results = [X2, p_boot, hl_stat, p_hl, accuracy, sensitivity, specificity]\n        all_results.append(case_results)\n    \n    # Final print statement in the exact required format.\n    print(str(all_results).replace(' ', ''))\n\nsolve()\n```"
        }
    ]
}