{
    "hands_on_practices": [
        {
            "introduction": "The Negative Binomial (NB) model is a cornerstone for analyzing overdispersed count data. Rather than simply accepting its formula, this exercise challenges you to build it from first principles. By deriving the NB distribution as a Poisson-Gamma mixture, you will gain a deeper intuition for how unobserved heterogeneity among subjects gives rise to variance that exceeds the mean, a hallmark of overdispersion. This practice will solidify your understanding of how the NB model naturally accommodates a higher proportion of zero counts compared to a standard Poisson model with the same mean. ",
            "id": "4935401",
            "problem": "A biostatistics study records counts of nematode eggs per gram of stool across individuals. Let $Y$ denote the count for an individual. A standard baseline model assumes a Poisson distribution with mean $\\mu$, whose probability mass function is $P(Y=y)=\\exp(-\\mu)\\mu^{y}/y!$ for $y\\in\\{0,1,2,\\dots\\}$. To accommodate overdispersion relative to the Poisson, a widely used alternative is the Negative Binomial type $2$ (NB2), defined as the Poissonâ€“Gamma mixture: conditional on a latent rate $\\Lambda$, $Y\\mid \\Lambda\\sim\\text{Poisson}(\\Lambda)$, and the latent rate follows a Gamma distribution $\\Lambda\\sim\\text{Gamma}(\\text{shape}=r,\\text{scale}=s)$, with $r=1/\\alpha$ and $s=\\alpha\\mu$, so that $E(Y)=\\mu$ and $\\operatorname{Var}(Y)=\\mu(1+\\alpha\\mu)$. Here $\\alpha>0$ is the overdispersion coefficient.\n\nStarting from these definitions and without using any pre-stated shortcut formulas, derive the marginal probability $P(Y=0)$ under the NB2 model. Then, compare this to the Poisson zero probability $P(Y=0)=\\exp(-\\mu)$ by forming the ratio $R=\\frac{P_{\\text{NB2}}(Y=0)}{P_{\\text{Poisson}}(Y=0)}$.\n\nSuppose the observed sample mean and variance of the counts are $\\mu=1.5$ and $\\sigma^{2}=4.5$, consistent with overdispersion. Estimate the overdispersion coefficient by the method of moments via $\\alpha=\\frac{\\sigma^{2}-\\mu}{\\mu^{2}}$, and use this value together with $\\mu=1.5$ to compute the ratio $R$.\n\nRound your final numeric answer for $R$ to four significant figures. Express your answer as a pure number (no units).",
            "solution": "The problem requires the derivation of the marginal probability of observing a zero count, $P(Y=0)$, under a Negative Binomial type $2$ ($NB2$) model, which is defined as a Poisson-Gamma mixture. Subsequently, this probability is to be compared to that from a standard Poisson model by forming a ratio, $R$. Finally, a numerical value for this ratio is to be computed using provided sample moments.\n\nFirst, we derive the marginal probability $P_{\\text{NB2}}(Y=y)$. The problem states that conditional on a latent rate $\\Lambda$, the count $Y$ follows a Poisson distribution, $Y \\mid \\Lambda \\sim \\text{Poisson}(\\Lambda)$. Its probability mass function (PMF) is $P(Y=y \\mid \\Lambda=\\lambda) = \\frac{\\exp(-\\lambda)\\lambda^y}{y!}$. The latent rate $\\Lambda$ is a random variable that follows a Gamma distribution, $\\Lambda \\sim \\text{Gamma}(\\text{shape}=r, \\text{scale}=s)$, with its probability density function (PDF) given by $f_{\\Lambda}(\\lambda) = \\frac{1}{\\Gamma(r)s^r} \\lambda^{r-1} \\exp(-\\lambda/s)$ for $\\lambda > 0$. The parameters of the Gamma distribution are specified as $r=1/\\alpha$ and $s=\\alpha\\mu$.\n\nThe marginal probability of $Y$ is obtained by integrating the joint probability of $Y$ and $\\Lambda$ over all possible values of $\\Lambda$. This is an application of the law of total probability:\n$$P(Y=y) = \\int_{0}^{\\infty} P(Y=y \\mid \\Lambda=\\lambda) f_{\\Lambda}(\\lambda) \\, d\\lambda$$\nWe are tasked with finding this probability for the specific case where the count is zero, i.e., $y=0$. For $y=0$, the conditional Poisson probability simplifies to:\n$$P(Y=0 \\mid \\Lambda=\\lambda) = \\frac{\\exp(-\\lambda)\\lambda^0}{0!} = \\exp(-\\lambda)$$\nSubstituting this and the Gamma PDF into the integral for the marginal probability gives:\n$$P_{\\text{NB2}}(Y=0) = \\int_{0}^{\\infty} \\exp(-\\lambda) \\left( \\frac{1}{\\Gamma(r)s^r} \\lambda^{r-1} \\exp\\left(-\\frac{\\lambda}{s}\\right) \\right) \\, d\\lambda$$\nWe can rearrange the terms by pulling the constants outside the integral and combining the exponential functions:\n$$P_{\\text{NB2}}(Y=0) = \\frac{1}{\\Gamma(r)s^r} \\int_{0}^{\\infty} \\lambda^{r-1} \\exp\\left(-\\lambda - \\frac{\\lambda}{s}\\right) \\, d\\lambda$$\n$$P_{\\text{NB2}}(Y=0) = \\frac{1}{\\Gamma(r)s^r} \\int_{0}^{\\infty} \\lambda^{r-1} \\exp\\left(-\\lambda\\left(1 + \\frac{1}{s}\\right)\\right) \\, d\\lambda$$\n$$P_{\\text{NB2}}(Y=0) = \\frac{1}{\\Gamma(r)s^r} \\int_{0}^{\\infty} \\lambda^{r-1} \\exp\\left(-\\lambda\\frac{s+1}{s}\\right) \\, d\\lambda$$\nThe integral is a standard form related to the Gamma function. The general form of this integral is $\\int_0^\\infty x^{k-1} \\exp(-ax) \\, dx = \\frac{\\Gamma(k)}{a^k}$. In our context, the shape parameter is $k=r$ and the rate parameter is $a = \\frac{s+1}{s}$. The integral thus evaluates to:\n$$\\int_{0}^{\\infty} \\lambda^{r-1} \\exp\\left(-\\lambda \\frac{s+1}{s}\\right) \\, d\\lambda = \\frac{\\Gamma(r)}{\\left(\\frac{s+1}{s}\\right)^r} = \\Gamma(r) \\left(\\frac{s}{s+1}\\right)^r$$\nSubstituting this result back into our expression for $P_{\\text{NB2}}(Y=0)$:\n$$P_{\\text{NB2}}(Y=0) = \\frac{1}{\\Gamma(r)s^r} \\left[ \\Gamma(r) \\left(\\frac{s}{s+1}\\right)^r \\right]$$\nThe $\\Gamma(r)$ terms cancel out, simplifying the expression significantly:\n$$P_{\\text{NB2}}(Y=0) = \\frac{1}{s^r} \\left(\\frac{s}{s+1}\\right)^r = \\left(\\frac{s}{s(s+1)}\\right)^r = \\left(\\frac{1}{s+1}\\right)^r$$\nNow, we substitute the model-specific parameterizations for $r$ and $s$, which are $r=1/\\alpha$ and $s=\\alpha\\mu$:\n$$P_{\\text{NB2}}(Y=0) = \\left(\\frac{1}{1+\\alpha\\mu}\\right)^{1/\\alpha} = (1+\\alpha\\mu)^{-1/\\alpha}$$\nThis completes the derivation of the marginal probability for a zero count in the NB2 model.\n\nNext, we form the ratio $R$ of the zero probability under the NB2 model to that under the Poisson model. For a Poisson distribution with mean $\\mu$, the probability of a zero count is $P_{\\text{Poisson}}(Y=0) = \\exp(-\\mu)$. The ratio $R$ is therefore:\n$$R = \\frac{P_{\\text{NB2}}(Y=0)}{P_{\\text{Poisson}}(Y=0)} = \\frac{(1+\\alpha\\mu)^{-1/\\alpha}}{\\exp(-\\mu)}$$\n$$R = \\exp(\\mu) (1+\\alpha\\mu)^{-1/\\alpha}$$\nThe ratio $R$ quantifies the excess of zeros in the NB2 model relative to the Poisson model for a given mean $\\mu$ and overdispersion $\\alpha$.\n\nFinally, we compute the numerical value of $R$. The problem provides the observed sample mean $\\mu=1.5$ and sample variance $\\sigma^2=4.5$. The overdispersion coefficient $\\alpha$ is estimated by the method of moments using the formula:\n$$\\alpha = \\frac{\\sigma^{2}-\\mu}{\\mu^{2}} = \\frac{4.5 - 1.5}{(1.5)^{2}} = \\frac{3.0}{2.25} = \\frac{4}{3}$$\nWith the values of $\\mu=1.5$ and $\\alpha=4/3$, we can compute $R$:\n$$R = \\exp(1.5) \\left(1 + \\frac{4}{3} \\times 1.5\\right)^{-1/(4/3)}$$\nFirst, we simplify the base of the power term:\n$$1 + \\frac{4}{3} \\times 1.5 = 1 + \\frac{4}{3} \\times \\frac{3}{2} = 1 + 2 = 3$$\nThe exponent is $-1/\\alpha = -1/(4/3) = -3/4$. So the expression for $R$ becomes:\n$$R = \\exp(1.5) \\cdot (3)^{-3/4}$$\nNow, we calculate the numerical value:\n$$\\exp(1.5) \\approx 4.48168907$$\n$$(3)^{-3/4} = \\frac{1}{3^{0.75}} \\approx \\frac{1}{2.27950705} \\approx 0.4386913$$\n$$R \\approx 4.48168907 \\times 0.4386913 \\approx 1.9660773$$\nRounding the result to four significant figures as requested gives $1.966$.",
            "answer": "$$\\boxed{1.966}$$"
        },
        {
            "introduction": "When count data exhibit an excess of zeros, both zero-inflated and hurdle models are powerful tools, but they tell different conceptual stories about the data-generating process. This exercise asks you to dissect the mathematical machinery of the Zero-Inflated Poisson (ZIP) and the Hurdle Poisson models. By comparing how each model constructs its probabilities for positive counts, you will uncover their subtle yet critical differences and similarities, sharpening your ability to select the most appropriate model for a given scientific question. ",
            "id": "4935349",
            "problem": "A biostatistics team is modeling the number of parasite eggs per gram in stool samples, denoted by the count variable $Y$. The empirical histogram shows a large spike at $0$ and a long tail of positive counts, indicating excess zeros and overdispersion. Two competing models are considered based on foundational definitions: the Zero-Inflated Poisson (ZIP) model and the hurdle Poisson model. The ZIP model is defined as a mixture between a degenerate distribution at zero and a Poisson distribution: with probability $\\pi$ the process yields a structural zero, and with probability $1-\\pi$ it yields a Poisson count with rate $\\lambda$. The hurdle Poisson model is defined as a two-part model: with probability $p$ the process yields a structural zero, and with probability $1-p$ the process yields a positive count generated from a zero-truncated Poisson distribution with rate $\\lambda$.\n\nStarting from the core definition of the Poisson distribution with rate $\\lambda$, which assigns probability $P(N=k)=\\exp(-\\lambda)\\lambda^{k}/k!$ for $k\\in\\{0,1,2,\\dots\\}$, and from the mixture and truncation principles described above, do the following:\n\n1. Derive, for each model, the unconditional distribution over the positive support $\\{1,2,\\dots\\}$, that is, the probabilities $P(Y=k)$ for all $k\\geq 1$. Explain how these distributions over positive counts differ between the ZIP and the hurdle Poisson constructions.\n\n2. Using those derivations, compute the conditional probability $P(Y=1\\mid Y>0)$ under each model and simplify your result to a closed-form expression in $\\lambda$.\n\n3. Evaluate both conditional probabilities at $\\lambda=2.5$ and report the two numerical values. Round your two reported values to four significant figures. Express the final answer with no units.",
            "solution": "This problem proceeds in three parts as requested. Let a random variable $N$ follow a Poisson distribution with rate $\\lambda > 0$, denoted $N \\sim \\text{Poisson}(\\lambda)$. Its probability mass function (PMF) is given by $P(N=k) = \\frac{\\exp(-\\lambda)\\lambda^k}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\n**1. Unconditional Distributions for Positive Counts**\n\nThis part requires deriving the probability $P(Y=k)$ for integers $k \\geq 1$ under both the ZIP and Hurdle models, and explaining the difference.\n\n**Zero-Inflated Poisson (ZIP) Model**\n\nThe ZIP model is defined as a mixture. With probability $\\pi$, the outcome is a structural zero, $Y=0$. With probability $1-\\pi$, the outcome is drawn from a $\\text{Poisson}(\\lambda)$ distribution.\n\nFor any positive integer count $k \\geq 1$, the observation cannot be a structural zero. Therefore, it must arise from the Poisson component of the mixture. The probability of this is the probability of selecting the Poisson component, $1-\\pi$, multiplied by the probability of observing the value $k$ from that Poisson distribution.\n\n$$\nP(Y=k) = P(\\text{from Poisson}) \\times P(N=k \\mid \\text{from Poisson})\n$$\n\nWhere $N \\sim \\text{Poisson}(\\lambda)$.\nThus, for $k \\in \\{1, 2, 3, \\dots\\}$:\n$$\nP_{\\text{ZIP}}(Y=k) = (1-\\pi) \\frac{\\exp(-\\lambda)\\lambda^k}{k!}\n$$\nThe probability of a zero under the ZIP model combines the structural zeros and the sampling zeros from the Poisson component: $P_{\\text{ZIP}}(Y=0) = \\pi + (1-\\pi)\\exp(-\\lambda)$.\n\n**Hurdle Poisson Model**\n\nThe hurdle model is a two-part model. With probability $p$, the outcome is a structural zero (the process does not \"clear the hurdle\"). With probability $1-p$, the outcome is a positive count drawn from a zero-truncated Poisson distribution with rate $\\lambda$.\n\nFirst, we define the PMF of a zero-truncated Poisson distribution. Let $N \\sim \\text{Poisson}(\\lambda)$. The probability of observing a positive count from this distribution is $P(N>0) = 1 - P(N=0) = 1 - \\exp(-\\lambda)$.\nThe PMF of a zero-truncated Poisson variable, let's call it $N_{>0}$, for $k \\in \\{1, 2, 3, \\dots\\}$ is:\n$$\nP(N_{>0}=k) = P(N=k \\mid N>0) = \\frac{P(N=k \\text{ and } N>0)}{P(N>0)} = \\frac{\\frac{\\exp(-\\lambda)\\lambda^k}{k!}}{1-\\exp(-\\lambda)}\n$$\nIn the hurdle model, an outcome $k \\geq 1$ occurs if the process clears the hurdle (with probability $1-p$) and then takes the value $k$ from the zero-truncated distribution.\nThus, for $k \\in \\{1, 2, 3, \\dots\\}$:\n$$\nP_{\\text{Hurdle}}(Y=k) = (1-p) P(N_{>0}=k) = (1-p) \\frac{\\exp(-\\lambda)\\lambda^k}{k!(1-\\exp(-\\lambda))}\n$$\nThe probability of a zero is simply $P_{\\text{Hurdle}}(Y=0) = p$.\n\n**Comparison of Distributions for Positive Counts**\n\nThe two derived distributions for positive counts ($k \\geq 1$) are:\n- ZIP: $P_{\\text{ZIP}}(Y=k) = (1-\\pi) \\frac{\\exp(-\\lambda)\\lambda^k}{k!}$\n- Hurdle: $P_{\\text{Hurdle}}(Y=k) = \\left(\\frac{1-p}{1-\\exp(-\\lambda)}\\right) \\frac{\\exp(-\\lambda)\\lambda^k}{k!}$\n\nBoth distributions share the same Poisson kernel, $\\frac{\\exp(-\\lambda)\\lambda^k}{k!}$. Consequently, for both models, the ratio of probabilities for any two positive counts, $\\frac{P(Y=k_1)}{P(Y=k_2)}$ for $k_1, k_2 \\geq 1$, is identical to that of a standard Poisson distribution.\n\nThe fundamental difference lies in the scaling constants. In the ZIP model, the unconditional probabilities for positive counts are simply the standard Poisson probabilities scaled down by a factor of $(1-\\pi)$. In the Hurdle model, the standard Poisson probabilities for positive counts are first re-normalized by the factor $1/(1-\\exp(-\\lambda))$ to create a valid zero-truncated distribution, and then scaled by the probability of clearing the hurdle, $(1-p)$. The overall scaling factor is $\\frac{1-p}{1-\\exp(-\\lambda)}$.\n\n**2. Conditional Probability $P(Y=1 \\mid Y>0)$**\n\nWe now compute the conditional probability of observing a count of $1$ given that the count is positive, for each model.\n\n**ZIP Model**\n\nWe use the definition of conditional probability: $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$.\nHere, $A$ is the event $\\{Y=1\\}$ and $B$ is the event $\\{Y>0\\}$. The intersection $\\{Y=1\\} \\cap \\{Y>0\\}$ is just $\\{Y=1\\}$.\n$$\nP_{\\text{ZIP}}(Y=1 \\mid Y>0) = \\frac{P_{\\text{ZIP}}(Y=1)}{P_{\\text{ZIP}}(Y>0)}\n$$\nThe numerator is $P_{\\text{ZIP}}(Y=1) = (1-\\pi)\\lambda\\exp(-\\lambda)$.\nThe denominator is the total probability of a positive count:\n$$\nP_{\\text{ZIP}}(Y>0) = \\sum_{k=1}^{\\infty} P_{\\text{ZIP}}(Y=k) = \\sum_{k=1}^{\\infty} (1-\\pi) \\frac{\\exp(-\\lambda)\\lambda^k}{k!} = (1-\\pi) \\sum_{k=1}^{\\infty} \\frac{\\exp(-\\lambda)\\lambda^k}{k!}\n$$\nThe sum is the probability that a standard Poisson variable is positive, which is $1-\\exp(-\\lambda)$.\n$$\nP_{\\text{ZIP}}(Y>0) = (1-\\pi)(1-\\exp(-\\lambda))\n$$\nNow, we compute the ratio:\n$$\nP_{\\text{ZIP}}(Y=1 \\mid Y>0) = \\frac{(1-\\pi)\\lambda\\exp(-\\lambda)}{(1-\\pi)(1-\\exp(-\\lambda))} = \\frac{\\lambda\\exp(-\\lambda)}{1-\\exp(-\\lambda)}\n$$\nThe mixing parameter $\\pi$ cancels.\n\n**Hurdle Model**\n\nWe follow the same procedure.\n$$\nP_{\\text{Hurdle}}(Y=1 \\mid Y>0) = \\frac{P_{\\text{Hurdle}}(Y=1)}{P_{\\text{Hurdle}}(Y>0)}\n$$\nThe numerator is $P_{\\text{Hurdle}}(Y=1) = (1-p) \\frac{\\lambda\\exp(-\\lambda)}{1-\\exp(-\\lambda)}$.\nThe denominator, the probability of clearing the hurdle, is simply $P_{\\text{Hurdle}}(Y>0) = 1-p$.\nNow, we compute the ratio:\n$$\nP_{\\text{Hurdle}}(Y=1 \\mid Y>0) = \\frac{(1-p) \\frac{\\lambda\\exp(-\\lambda)}{1-\\exp(-\\lambda)}}{1-p} = \\frac{\\lambda\\exp(-\\lambda)}{1-\\exp(-\\lambda)}\n$$\nThe hurdle probability $p$ also cancels.\n\nFor both models, the conditional probability of $Y=1$ given $Y>0$ is identical and simplifies to the same closed-form expression in $\\lambda$:\n$$\nP(Y=1 \\mid Y>0) = \\frac{\\lambda\\exp(-\\lambda)}{1-\\exp(-\\lambda)}\n$$\nThis is the probability of observing a $1$ in a zero-truncated Poisson distribution.\n\n**3. Numerical Evaluation**\n\nWe are asked to evaluate the derived conditional probability at $\\lambda=2.5$.\n$$\nP(Y=1 \\mid Y>0) = \\frac{2.5 \\times \\exp(-2.5)}{1-\\exp(-2.5)}\n$$\nFirst, calculate the value of $\\exp(-2.5)$:\n$$\n\\exp(-2.5) \\approx 0.0820849986\n$$\nNow, substitute this value into the expression:\n$$\nP(Y=1 \\mid Y>0) \\approx \\frac{2.5 \\times 0.0820849986}{1 - 0.0820849986} = \\frac{0.2052124965}{0.9179150014} \\approx 0.22356534\n$$\nRounding this result to four significant figures gives $0.2236$. Since the expression is identical for both the ZIP and Hurdle models, the numerical value is the same for both.\n- Conditional probability for ZIP model at $\\lambda=2.5$: $0.2236$.\n- Conditional probability for Hurdle model at $\\lambda=2.5$: $0.2236$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.2236 & 0.2236 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Fitting a statistical model often involves a \"black box\" of computational algorithms. This practice takes you \"under the hood\" of Maximum Likelihood Estimation (MLE) for the Zero-Inflated Poisson (ZIP) model. By deriving the complete log-likelihood function and the resulting score equations, you will understand the objective function that statistical software optimizes to find the best-fitting parameter values. This foundational exercise moves you from being a user of statistical methods to an informed modeler who grasps the core mechanics of inference. ",
            "id": "4935354",
            "problem": "A biostatistics study records the count of unscheduled clinic visits $y_i$ for $n$ independent patients $i=1,\\dots,n$ over a fixed follow-up period. Many patients are structurally not at risk of having any visits, resulting in more zeros than expected under a simple Poisson model. To address this, assume a Zero-Inflated Poisson (ZIP) model, in which each observation arises from a two-component mixture: with probability $\\pi_i$ the count is a structural zero, and with probability $1-\\pi_i$ the count follows a Poisson distribution with mean $\\lambda_i$. The model uses a logistic link for the inflation probability and a log link for the Poisson mean,\n$$\n\\pi_i \\;=\\; \\operatorname{logit}^{-1}(\\mathbf{z}_i^{\\top}\\boldsymbol{\\gamma}) \\;=\\; \\frac{1}{1+\\exp(-\\mathbf{z}_i^{\\top}\\boldsymbol{\\gamma})}\n\\quad\\text{and}\\quad\n\\lambda_i \\;=\\; \\exp(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}),\n$$\nwhere $\\mathbf{z}_i$ and $\\mathbf{x}_i$ are known covariate vectors and $\\boldsymbol{\\gamma}$ and $\\boldsymbol{\\beta}$ are parameter vectors.\n\nStarting only from the fundamental definitions of the Poisson probability mass function and the mixture model construction described above, do the following:\n- Derive the complete log-likelihood $\\ell(\\boldsymbol{\\beta},\\boldsymbol{\\gamma}\\,|\\,\\{y_i,\\mathbf{x}_i,\\mathbf{z}_i\\}_{i=1}^{n})$ for the ZIP model with the specified links.\n- Write the score equations by taking the gradient of the log-likelihood with respect to $\\boldsymbol{\\gamma}$ and $\\boldsymbol{\\beta}$ and setting each to zero.\n\nThen, for the following dataset and parameter values, evaluate the log-likelihood numerically and report its value, rounded to four significant figures:\n- $n=5$ patients with observed counts $(y_1,y_2,y_3,y_4,y_5)=(0,0,3,1,0)$.\n- Covariates are two-dimensional with an intercept, $\\mathbf{x}_i=(1,t_i)$ and $\\mathbf{z}_i=(1,t_i)$, where $(t_1,t_2,t_3,t_4,t_5)=(0,1,2,1,3)$.\n- Parameter values are $\\boldsymbol{\\beta}=(0.2,-0.1)$ and $\\boldsymbol{\\gamma}=(-1.0,0.5)$.\n\nExpress the final numerical answer as a real number with no units and round to four significant figures.",
            "solution": "This problem proceeds in three parts as requested: derivation of the log-likelihood, derivation of the score equations, and numerical evaluation of the log-likelihood for the given dataset.\n\n### Part 1: Derivation of the Log-Likelihood\n\nLet $Y_i$ be the random variable representing the count for patient $i$, for $i=1, \\dots, n$. The Zero-Inflated Poisson (ZIP) model specifies that $Y_i$ is a mixture of two components: a structural zero and a Poisson-distributed count.\n\nThe probability mass function (PMF) of a Poisson distribution with mean $\\lambda$ is given by:\n$$ P(Y=k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\quad \\text{for } k=0, 1, 2, \\ldots $$\nIn the ZIP model, for each observation $y_i$, the count is a structural zero with probability $\\pi_i$, or it is drawn from a Poisson($\\lambda_i$) distribution with probability $1-\\pi_i$.\n\nWe can derive the PMF for $Y_i$ by considering two cases:\n\nCase 1: The observed count is $y_i=0$.\nThis outcome can occur in two mutually exclusive ways:\n1.  The observation is a structural zero. The probability of this is $\\pi_i$.\n2.  The observation is from the Poisson component and the realized value is $0$. The probability of being from the Poisson component is $1-\\pi_i$, and the probability of a Poisson random variable being $0$ is $P(Y_i=0|\\lambda_i) = \\frac{\\lambda_i^0 e^{-\\lambda_i}}{0!} = e^{-\\lambda_i}$. The joint probability of this path is $(1-\\pi_i) e^{-\\lambda_i}$.\n\nThe total probability of observing $y_i=0$ is the sum of these probabilities:\n$$ P(Y_i = 0) = \\pi_i + (1-\\pi_i) e^{-\\lambda_i} $$\n\nCase 2: The observed count is $y_i > 0$.\nThis outcome can only occur if the observation is from the Poisson component. The probability of this is $1-\\pi_i$. Given that it is from the Poisson component, the probability of observing the specific count $y_i$ is $\\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!}$.\n\nThe total probability of observing $y_i=k$ where $k \\in \\{1, 2, 3, \\dots\\}$ is:\n$$ P(Y_i = k) = (1-\\pi_i) \\frac{\\lambda_i^{k} e^{-\\lambda_i}}{k!} $$\n\nWe can write a single expression for the PMF of $Y_i$ using an indicator function $I(y_i=0)$, which is $1$ if $y_i=0$ and $0$ otherwise.\n$$ P(y_i) = \\left[\\pi_i + (1-\\pi_i)e^{-\\lambda_i}\\right]^{I(y_i=0)} \\left[(1-\\pi_i) \\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!}\\right]^{1-I(y_i=0)} $$\nSince the $n$ observations are independent, the total likelihood is the product of the individual probabilities:\n$$ L(\\boldsymbol{\\beta},\\boldsymbol{\\gamma}) = \\prod_{i=1}^{n} P(y_i) $$\nThe log-likelihood, $\\ell(\\boldsymbol{\\beta},\\boldsymbol{\\gamma}) = \\ln L(\\boldsymbol{\\beta},\\boldsymbol{\\gamma})$, is the sum of the individual log-probabilities:\n$$ \\ell(\\boldsymbol{\\beta},\\boldsymbol{\\gamma}) = \\sum_{i=1}^{n} \\ln P(y_i) $$\nIt is more convenient to split the sum over the set of observations with zero counts, $I_0 = \\{i | y_i=0\\}$, and the set of observations with positive counts, $I_+ = \\{i | y_i>0\\}$.\n$$ \\ell(\\boldsymbol{\\beta},\\boldsymbol{\\gamma}) = \\sum_{i \\in I_0} \\ln\\left( \\pi_i + (1-\\pi_i)e^{-\\lambda_i} \\right) + \\sum_{i \\in I_+} \\ln\\left( (1-\\pi_i) \\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!} \\right) $$\nExpanding the second term, we get:\n$$ \\sum_{i \\in I_+} \\left[ \\ln(1-\\pi_i) + y_i\\ln(\\lambda_i) - \\lambda_i - \\ln(y_i!) \\right] $$\nThus, the complete log-likelihood is:\n$$ \\ell(\\boldsymbol{\\beta},\\boldsymbol{\\gamma}) = \\sum_{i \\in I_0} \\ln\\left( \\pi_i + (1-\\pi_i)e^{-\\lambda_i} \\right) + \\sum_{i \\in I_+} \\left[ \\ln(1-\\pi_i) + y_i\\ln(\\lambda_i) - \\lambda_i - \\ln(y_i!) \\right] $$\nwhere $\\pi_i = \\frac{1}{1+\\exp(-\\mathbf{z}_i^{\\top}\\boldsymbol{\\gamma})}$ and $\\lambda_i = \\exp(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta})$.\n\n### Part 2: Derivation of the Score Equations\n\nThe score equations are obtained by setting the partial derivatives of the log-likelihood with respect to the parameters $\\boldsymbol{\\beta}$ and $\\boldsymbol{\\gamma}$ to zero. We use the chain rule and the following derivatives:\nLet $\\eta_{i\\beta} = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$ and $\\eta_{i\\gamma} = \\mathbf{z}_i^{\\top}\\boldsymbol{\\gamma}$.\n$$ \\frac{\\partial \\lambda_i}{\\partial \\boldsymbol{\\beta}} = \\frac{\\partial \\exp(\\eta_{i\\beta})}{\\partial \\boldsymbol{\\beta}} = \\exp(\\eta_{i\\beta}) \\frac{\\partial \\eta_{i\\beta}}{\\partial \\boldsymbol{\\beta}} = \\lambda_i \\mathbf{x}_i $$\n$$ \\frac{\\partial \\pi_i}{\\partial \\boldsymbol{\\gamma}} = \\frac{\\partial}{\\partial \\boldsymbol{\\gamma}}\\left(\\frac{1}{1+\\exp(-\\eta_{i\\gamma})}\\right) = \\frac{\\exp(-\\eta_{i\\gamma})}{(1+\\exp(-\\eta_{i\\gamma}))^2} \\frac{\\partial \\eta_{i\\gamma}}{\\partial \\boldsymbol{\\gamma}} = \\pi_i(1-\\pi_i)\\mathbf{z}_i $$\n\n**Score with respect to $\\boldsymbol{\\beta}$:**\nThe parameter vector $\\boldsymbol{\\beta}$ only appears in $\\lambda_i$.\n$$ \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}} = \\sum_{i \\in I_0} \\frac{1}{\\pi_i + (1-\\pi_i)e^{-\\lambda_i}} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}}((1-\\pi_i)e^{-\\lambda_i}) + \\sum_{i \\in I_+} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(y_i\\ln(\\lambda_i) - \\lambda_i) $$\nFor $i \\in I_0$:\n$$ \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\ln\\left( \\pi_i + (1-\\pi_i)e^{-\\lambda_i} \\right) = \\frac{(1-\\pi_i)e^{-\\lambda_i}(-1)}{\\pi_i + (1-\\pi_i)e^{-\\lambda_i}} \\frac{\\partial \\lambda_i}{\\partial \\boldsymbol{\\beta}} = -\\frac{(1-\\pi_i)e^{-\\lambda_i}\\lambda_i}{\\pi_i + (1-\\pi_i)e^{-\\lambda_i}} \\mathbf{x}_i $$\nFor $i \\in I_+$:\n$$ \\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(y_i\\ln(\\lambda_i) - \\lambda_i) = \\left(\\frac{y_i}{\\lambda_i} - 1\\right) \\frac{\\partial \\lambda_i}{\\partial \\boldsymbol{\\beta}} = \\left(\\frac{y_i - \\lambda_i}{\\lambda_i}\\right) \\lambda_i\\mathbf{x}_i = (y_i - \\lambda_i)\\mathbf{x}_i $$\nCombining these terms, the score equation for $\\boldsymbol{\\beta}$ is:\n$$ \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}} = \\sum_{i \\in I_+} (y_i - \\lambda_i)\\mathbf{x}_i - \\sum_{i \\in I_0} \\frac{(1-\\pi_i)\\lambda_i e^{-\\lambda_i}}{\\pi_i + (1-\\pi_i)e^{-\\lambda_i}} \\mathbf{x}_i = \\mathbf{0} $$\n\n**Score with respect to $\\boldsymbol{\\gamma}$:**\nThe parameter vector $\\boldsymbol{\\gamma}$ only appears in $\\pi_i$.\n$$ \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\gamma}} = \\sum_{i \\in I_0} \\frac{1}{\\pi_i + (1-\\pi_i)e^{-\\lambda_i}} \\frac{\\partial}{\\partial \\boldsymbol{\\gamma}}(\\pi_i + (1-\\pi_i)e^{-\\lambda_i}) + \\sum_{i \\in I_+} \\frac{\\partial}{\\partial \\boldsymbol{\\gamma}}(\\ln(1-\\pi_i)) $$\nFor $i \\in I_0$:\n$$ \\frac{\\partial}{\\partial \\boldsymbol{\\gamma}} \\ln\\left( \\pi_i + (1-\\pi_i)e^{-\\lambda_i} \\right) = \\frac{1 - e^{-\\lambda_i}}{\\pi_i + (1-\\pi_i)e^{-\\lambda_i}} \\frac{\\partial \\pi_i}{\\partial \\boldsymbol{\\gamma}} = \\frac{(1-e^{-\\lambda_i})\\pi_i(1-\\pi_i)}{\\pi_i + (1-\\pi_i)e^{-\\lambda_i}} \\mathbf{z}_i $$\nFor $i \\in I_+$:\n$$ \\frac{\\partial}{\\partial \\boldsymbol{\\gamma}}(\\ln(1-\\pi_i)) = \\frac{-1}{1-\\pi_i} \\frac{\\partial \\pi_i}{\\partial \\boldsymbol{\\gamma}} = \\frac{-1}{1-\\pi_i} \\pi_i(1-\\pi_i)\\mathbf{z}_i = -\\pi_i \\mathbf{z}_i $$\nCombining these terms, the score equation for $\\boldsymbol{\\gamma}$ is:\n$$ \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\gamma}} = \\sum_{i \\in I_0} \\frac{(1-e^{-\\lambda_i})\\pi_i(1-\\pi_i)}{\\pi_i + (1-\\pi_i)e^{-\\lambda_i}} \\mathbf{z}_i - \\sum_{i \\in I_+} \\pi_i \\mathbf{z}_i = \\mathbf{0} $$\n\n### Part 3: Numerical Evaluation of the Log-Likelihood\n\nGiven data:\n- $n=5$\n- Counts $y = (0,0,3,1,0)$\n- Covariate values $t = (0,1,2,1,3)$\n- Model parameters $\\boldsymbol{\\beta}=(0.2,-0.1)^{\\top}$ and $\\boldsymbol{\\gamma}=(-1.0,0.5)^{\\top}$\n- Covariate vectors are $\\mathbf{x}_i=\\mathbf{z}_i=(1,t_i)^{\\top}$\n\n**Step 1: Calculate linear predictors $\\eta_{i\\beta}$ and $\\eta_{i\\gamma}$**\n- $i=1$: $t_1=0$. $\\eta_{1\\beta} = 1(0.2) + 0(-0.1) = 0.2$. $\\eta_{1\\gamma} = 1(-1.0) + 0(0.5) = -1.0$.\n- $i=2$: $t_2=1$. $\\eta_{2\\beta} = 1(0.2) + 1(-0.1) = 0.1$. $\\eta_{2\\gamma} = 1(-1.0) + 1(0.5) = -0.5$.\n- $i=3$: $t_3=2$. $\\eta_{3\\beta} = 1(0.2) + 2(-0.1) = 0.0$. $\\eta_{3\\gamma} = 1(-1.0) + 2(0.5) = 0.0$.\n- $i=4$: $t_4=1$. $\\eta_{4\\beta} = 1(0.2) + 1(-0.1) = 0.1$. $\\eta_{4\\gamma} = 1(-1.0) + 1(0.5) = -0.5$.\n- $i=5$: $t_5=3$. $\\eta_{5\\beta} = 1(0.2) + 3(-0.1) = -0.1$. $\\eta_{5\\gamma} = 1(-1.0) + 3(0.5) = 0.5$.\n\n**Step 2: Calculate $\\lambda_i$ and $\\pi_i$**\n- $i=1$: $\\lambda_1 = \\exp(0.2) \\approx 1.22140$. $\\pi_1 = (1+\\exp(1.0))^{-1} \\approx 0.26894$.\n- $i=2$: $\\lambda_2 = \\exp(0.1) \\approx 1.10517$. $\\pi_2 = (1+\\exp(0.5))^{-1} \\approx 0.37754$.\n- $i=3$: $\\lambda_3 = \\exp(0.0) = 1.0$. $\\pi_3 = (1+\\exp(0.0))^{-1} = 0.5$.\n- $i=4$: $\\lambda_4 = \\exp(0.1) \\approx 1.10517$. $\\pi_4 = (1+\\exp(0.5))^{-1} \\approx 0.37754$.\n- $i=5$: $\\lambda_5 = \\exp(-0.1) \\approx 0.90484$. $\\pi_5 = (1+\\exp(-0.5))^{-1} \\approx 0.62246$.\n\n**Step 3: Calculate the log-likelihood contribution for each observation**\nThe set of zero observations is $I_0 = \\{1, 2, 5\\}$, and the set of non-zero observations is $I_+ = \\{3, 4\\}$.\n- For $i \\in I_0$, the contribution is $\\ell_i = \\ln(\\pi_i + (1-\\pi_i)e^{-\\lambda_i})$.\n  - $\\ell_1 = \\ln(0.26894 + (1-0.26894)\\exp(-1.22140)) = \\ln(0.48447) \\approx -0.72465$.\n  - $\\ell_2 = \\ln(0.37754 + (1-0.37754)\\exp(-1.10517)) = \\ln(0.58365) \\approx -0.53853$.\n  - $\\ell_5 = \\ln(0.62246 + (1-0.62246)\\exp(-0.90484)) = \\ln(0.77522) \\approx -0.25464$.\n- For $i \\in I_+$, the contribution is $\\ell_i = \\ln(1-\\pi_i) + y_i\\ln(\\lambda_i) - \\lambda_i - \\ln(y_i!)$.\n  - $\\ell_3$ ($y_3=3$): $\\ln(1-0.5) + 3\\ln(1.0) - 1.0 - \\ln(3!) = \\ln(0.5) + 0 - 1.0 - \\ln(6) \\approx -0.69315 - 1.0 - 1.79176 = -3.48491$.\n  - $\\ell_4$ ($y_4=1$): $\\ln(1-0.37754) + 1\\ln(1.10517) - 1.10517 - \\ln(1!) = \\ln(0.62246) + 0.1 - 1.10517 - 0 \\approx -0.47413 + 0.1 - 1.10517 = -1.47930$.\n\n**Step 4: Sum the contributions**\n$$ \\ell = \\ell_1 + \\ell_2 + \\ell_3 + \\ell_4 + \\ell_5 $$\n$$ \\ell \\approx -0.72465 - 0.53853 - 3.48491 - 1.47930 - 0.25464 = -6.48203 $$\nRounding to four significant figures, the log-likelihood is $-6.482$.",
            "answer": "$$\\boxed{-6.482}$$"
        }
    ]
}