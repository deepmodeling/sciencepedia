{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp what partial correlation represents, there is no substitute for calculating it from first principles. This exercise guides you through the foundational procedure: removing the linear influence of a control variable ($Z$) from two variables of interest ($X$ and $Y$) by fitting simple linear regressions and then calculating the Pearson correlation of the resulting residuals. This hands-on process demystifies the concept, showing you precisely what it means to 'control for' a variable in a statistical sense.",
            "id": "4937002",
            "problem": "A biostatistics study investigates whether changes in systolic blood pressure, denoted by $X$ (in arbitrary standardized units), are associated with changes in an inflammatory biomarker, denoted by $Y$, after accounting for participant age, denoted by $Z$ (age centered at the cohort mean). You are given a sample of $n=6$ participants with observed triplets $(x_i,y_i,z_i)$:\n- $i=1$: $(x_1,y_1,z_1)=(-17,\\,12,\\, -5)$\n- $i=2$: $(x_2,y_2,z_2)=(-15,\\,4,\\, -3)$\n- $i=3$: $(x_3,y_3,z_3)=(-3,\\,5,\\, -1)$\n- $i=4$: $(x_4,y_4,z_4)=(3,\\, -5,\\, 1)$\n- $i=5$: $(x_5,y_5,z_5)=(14,\\, -5,\\, 3)$\n- $i=6$: $(x_6,y_6,z_6)=(18,\\, -11,\\, 5)$\n\nStarting from fundamental definitions, estimate the partial correlation of $X$ and $Y$ controlling for $Z$, denoted by $\\hat{\\rho}_{XY\\cdot Z}$, by performing the following steps:\n\n1. Fit the simple linear regression of $X$ on $Z$ using ordinary least squares (OLS), and extract the residuals $\\hat{e}_{Xi}=x_i-(\\hat{\\alpha}_X+\\hat{\\beta}_X z_i)$.\n2. Fit the simple linear regression of $Y$ on $Z$ using OLS, and extract the residuals $\\hat{e}_{Yi}=y_i-(\\hat{\\alpha}_Y+\\hat{\\beta}_Y z_i)$.\n3. Compute the Pearson product-moment correlation coefficient (PCC) between the residuals, $\\hat{\\rho}_{XY\\cdot Z}$, using only the foundational definitions of sample mean, sample covariance, and sample variance.\n\nReport $\\hat{\\rho}_{XY\\cdot Z}$ as a decimal and round your final answer to four significant figures. No units are required for the final answer.",
            "solution": "The problem requires the estimation of the partial correlation coefficient $\\hat{\\rho}_{XY\\cdot Z}$ from a sample of $n=6$ observations. The procedure outlined is to first regress $X$ on $Z$ and $Y$ on $Z$ separately using ordinary least squares (OLS), and then to compute the Pearson product-moment correlation coefficient of the resulting sets of residuals.\n\nThe provided data points $(x_i, y_i, z_i)$ are:\n$(-17, 12, -5)$, $(-15, 4, -3)$, $(-3, 5, -1)$, $(3, -5, 1)$, $(14, -5, 3)$, $(18, -11, 5)$.\n\nFirst, we calculate the sample means of the variables $X$, $Y$, and $Z$.\n$\\bar{x} = \\frac{1}{6} \\sum_{i=1}^{6} x_i = \\frac{1}{6}(-17 - 15 - 3 + 3 + 14 + 18) = \\frac{0}{6} = 0$.\n$\\bar{y} = \\frac{1}{6} \\sum_{i=1}^{6} y_i = \\frac{1}{6}(12 + 4 + 5 - 5 - 5 - 11) = \\frac{0}{6} = 0$.\n$\\bar{z} = \\frac{1}{6} \\sum_{i=1}^{6} z_i = \\frac{1}{6}(-5 - 3 - 1 + 1 + 3 + 5) = \\frac{0}{6} = 0$.\nThe fact that all variables are mean-centered simplifies the OLS calculations.\n\n### Step 1: Fit the linear regression of $X$ on $Z$ and find the residuals.\nThe simple linear regression model is $X = \\alpha_X + \\beta_X Z + \\epsilon_X$.\nThe OLS estimators for the coefficients are given by:\n$\\hat{\\beta}_X = \\frac{\\sum_{i=1}^n (z_i - \\bar{z})(x_i - \\bar{x})}{\\sum_{i=1}^n (z_i - \\bar{z})^2}$\n$\\hat{\\alpha}_X = \\bar{x} - \\hat{\\beta}_X \\bar{z}$\nSince $\\bar{x} = 0$ and $\\bar{z} = 0$, these formulas simplify to:\n$\\hat{\\beta}_X = \\frac{\\sum_{i=1}^n x_i z_i}{\\sum_{i=1}^n z_i^2}$\n$\\hat{\\alpha}_X = 0$\n\nWe compute the necessary sums:\n$\\sum_{i=1}^6 z_i^2 = (-5)^2 + (-3)^2 + (-1)^2 + 1^2 + 3^2 + 5^2 = 25 + 9 + 1 + 1 + 9 + 25 = 70$.\n$\\sum_{i=1}^6 x_i z_i = (-17)(-5) + (-15)(-3) + (-3)(-1) + (3)(1) + (14)(3) + (18)(5) = 85 + 45 + 3 + 3 + 42 + 90 = 268$.\n\nThe estimated slope is:\n$\\hat{\\beta}_X = \\frac{268}{70} = \\frac{134}{35}$.\nThe fitted regression line is $\\hat{x}_i = \\hat{\\alpha}_X + \\hat{\\beta}_X z_i = \\frac{134}{35} z_i$.\n\nThe residuals $\\hat{e}_{Xi}$ are calculated as $x_i - \\hat{x}_i$:\n$\\hat{e}_{X1} = -17 - \\frac{134}{35}(-5) = -17 + \\frac{134}{7} = \\frac{-119 + 134}{7} = \\frac{15}{7}$\n$\\hat{e}_{X2} = -15 - \\frac{134}{35}(-3) = -15 + \\frac{402}{35} = \\frac{-525 + 402}{35} = -\\frac{123}{35}$\n$\\hat{e}_{X3} = -3 - \\frac{134}{35}(-1) = -3 + \\frac{134}{35} = \\frac{-105 + 134}{35} = \\frac{29}{35}$\n$\\hat{e}_{X4} = 3 - \\frac{134}{35}(1) = \\frac{105 - 134}{35} = -\\frac{29}{35}$\n$\\hat{e}_{X5} = 14 - \\frac{134}{35}(3) = 14 - \\frac{402}{35} = \\frac{490 - 402}{35} = \\frac{88}{35}$\n$\\hat{e}_{X6} = 18 - \\frac{134}{35}(5) = 18 - \\frac{134}{7} = \\frac{126 - 134}{7} = -\\frac{8}{7}$\n\n### Step 2: Fit the linear regression of $Y$ on $Z$ and find the residuals.\nThe model is $Y = \\alpha_Y + \\beta_Y Z + \\epsilon_Y$.\nWith $\\bar{y} = 0$ and $\\bar{z} = 0$, the estimators simplify to:\n$\\hat{\\beta}_Y = \\frac{\\sum_{i=1}^n y_i z_i}{\\sum_{i=1}^n z_i^2}$\n$\\hat{\\alpha}_Y = 0$\n\nWe compute the sum of products for $y_i$ and $z_i$:\n$\\sum_{i=1}^6 y_i z_i = (12)(-5) + (4)(-3) + (5)(-1) + (-5)(1) + (-5)(3) + (-11)(5) = -60 - 12 - 5 - 5 - 15 - 55 = -152$.\n\nThe estimated slope is:\n$\\hat{\\beta}_Y = \\frac{-152}{70} = -\\frac{76}{35}$.\nThe fitted regression line is $\\hat{y}_i = \\hat{\\alpha}_Y + \\hat{\\beta}_Y z_i = -\\frac{76}{35} z_i$.\n\nThe residuals $\\hat{e}_{Yi}$ are calculated as $y_i - \\hat{y}_i$:\n$\\hat{e}_{Y1} = 12 - (-\\frac{76}{35})(-5) = 12 - \\frac{76}{7} = \\frac{84 - 76}{7} = \\frac{8}{7}$\n$\\hat{e}_{Y2} = 4 - (-\\frac{76}{35})(-3) = 4 - \\frac{228}{35} = \\frac{140 - 228}{35} = -\\frac{88}{35}$\n$\\hat{e}_{Y3} = 5 - (-\\frac{76}{35})(-1) = 5 - \\frac{76}{35} = \\frac{175 - 76}{35} = \\frac{99}{35}$\n$\\hat{e}_{Y4} = -5 - (-\\frac{76}{35})(1) = -5 + \\frac{76}{35} = \\frac{-175 + 76}{35} = -\\frac{99}{35}$\n$\\hat{e}_{Y5} = -5 - (-\\frac{76}{35})(3) = -5 + \\frac{228}{35} = \\frac{-175 + 228}{35} = \\frac{53}{35}$\n$\\hat{e}_{Y6} = -11 - (-\\frac{76}{35})(5) = -11 + \\frac{76}{7} = \\frac{-77 + 76}{7} = -\\frac{1}{7}$\n\n### Step 3: Compute the Pearson correlation between the residuals.\nThe Pearson correlation coefficient between two variables $A$ and $B$ is defined using sample covariance and variance. It can be computed as:\n$\\hat{\\rho}_{AB} = \\frac{\\sum_{i=1}^n (a_i - \\bar{a})(b_i - \\bar{b})}{\\sqrt{\\sum_{i=1}^n (a_i - \\bar{a})^2 \\sum_{i=1}^n (b_i - \\bar{b})^2}}$\nFor OLS residuals with an intercept in the model, the sample mean is zero. Let's verify: $\\sum \\hat{e}_{Xi} = \\frac{15}{7}-\\frac{123}{35}+\\frac{29}{35}-\\frac{29}{35}+\\frac{88}{35}-\\frac{8}{7} = \\frac{75-123+29-29+88-40}{35} = \\frac{192-192}{35} = 0$. Similarly, $\\sum \\hat{e}_{Yi} = 0$.\nSo, $\\bar{\\hat{e}}_X = 0$ and $\\bar{\\hat{e}}_Y = 0$. The formula for the correlation of the residuals simplifies to:\n$\\hat{\\rho}_{XY\\cdot Z} = \\hat{\\rho}_{\\hat{e}_X\\hat{e}_Y} = \\frac{\\sum_{i=1}^n \\hat{e}_{Xi} \\hat{e}_{Yi}}{\\sqrt{(\\sum_{i=1}^n \\hat{e}_{Xi}^2) (\\sum_{i=1}^n \\hat{e}_{Yi}^2)}}$\n\nWe compute the required sums:\nSum of cross-products of residuals:\n$\\sum \\hat{e}_{Xi} \\hat{e}_{Yi} = (\\frac{15}{7})(\\frac{8}{7}) + (-\\frac{123}{35})(-\\frac{88}{35}) + (\\frac{29}{35})(\\frac{99}{35}) + (-\\frac{29}{35})(-\\frac{99}{35}) + (\\frac{88}{35})(\\frac{53}{35}) + (-\\frac{8}{7})(-\\frac{1}{7})$\n$= \\frac{120}{49} + \\frac{10824}{1225} + \\frac{2871}{1225} + \\frac{2871}{1225} + \\frac{4664}{1225} + \\frac{8}{49}$\nSince $1225 = 35^2 = (5 \\times 7)^2 = 25 \\times 49$, we can use a common denominator of $1225$:\n$= \\frac{120 \\times 25}{1225} + \\frac{10824}{1225} + \\frac{2871}{1225} + \\frac{2871}{1225} + \\frac{4664}{1225} + \\frac{8 \\times 25}{1225}$\n$= \\frac{3000 + 10824 + 2871 + 2871 + 4664 + 200}{1225} = \\frac{24430}{1225}$\n\nSum of squared residuals for $X$:\n$\\sum \\hat{e}_{Xi}^2 = (\\frac{15}{7})^2 + (-\\frac{123}{35})^2 + (\\frac{29}{35})^2 + (-\\frac{29}{35})^2 + (\\frac{88}{35})^2 + (-\\frac{8}{7})^2$\n$= \\frac{225}{49} + \\frac{15129}{1225} + \\frac{841}{1225} + \\frac{841}{1225} + \\frac{7744}{1225} + \\frac{64}{49}$\n$= \\frac{225 \\times 25}{1225} + \\frac{15129}{1225} + \\frac{841}{1225} + \\frac{841}{1225} + \\frac{7744}{1225} + \\frac{64 \\times 25}{1225}$\n$= \\frac{5625 + 15129 + 841 + 841 + 7744 + 1600}{1225} = \\frac{31780}{1225}$\n\nSum of squared residuals for $Y$:\n$\\sum \\hat{e}_{Yi}^2 = (\\frac{8}{7})^2 + (-\\frac{88}{35})^2 + (\\frac{99}{35})^2 + (-\\frac{99}{35})^2 + (\\frac{53}{35})^2 + (-\\frac{1}{7})^2$\n$= \\frac{64}{49} + \\frac{7744}{1225} + \\frac{9801}{1225} + \\frac{9801}{1225} + \\frac{2809}{1225} + \\frac{1}{49}$\n$= \\frac{64 \\times 25}{1225} + \\frac{7744}{1225} + \\frac{9801}{1225} + \\frac{9801}{1225} + \\frac{2809}{1225} + \\frac{1 \\times 25}{1225}$\n$= \\frac{1600 + 7744 + 9801 + 9801 + 2809 + 25}{1225} = \\frac{31780}{1225}$\n\nNow we compute the correlation:\n$\\hat{\\rho}_{XY\\cdot Z} = \\frac{\\frac{24430}{1225}}{\\sqrt{(\\frac{31780}{1225}) (\\frac{31780}{1225})}} = \\frac{\\frac{24430}{1225}}{\\frac{31780}{1225}} = \\frac{24430}{31780} = \\frac{2443}{3178}$\n\nFinally, we calculate the decimal value and round to four significant figures:\n$\\hat{\\rho}_{XY\\cdot Z} = \\frac{2443}{3178} \\approx 0.76872246...$\nRounding to four significant figures gives $0.7687$.",
            "answer": "$$\\boxed{0.7687}$$"
        },
        {
            "introduction": "After estimating a partial correlation, a critical next step in any biostatistical analysis is to assess its statistical significance. An observed correlation could be a genuine association or simply the result of random sampling variability. This practice demonstrates how to perform a hypothesis test for a partial correlation coefficient, connecting it to the familiar framework of t-tests in multiple regression and allowing you to determine if your finding is statistically robust.",
            "id": "4937067",
            "problem": "A biostatistics team investigates the association between systolic blood pressure and urinary sodium excretion while controlling for demographic and lifestyle variables. In a cohort of $n=48$ adults, the partial correlation between systolic blood pressure ($Y$) and urinary sodium ($X$) controlling for $p=5$ covariates (age, sex, Body Mass Index (BMI), smoking status, and physical activity) is estimated as $r=0.35$. Assume that the joint distribution of all variables is multivariate normal and that the usual linear model assumptions (independent observations, linearity, homoscedasticity) hold.\n\nStarting from fundamental definitions, justify how the exact finite-sample null distribution for a test of $H_{0}: \\rho_{XY\\cdot Z}=0$ can be obtained by exploiting the equivalence between partial correlation and the significance test for a regression coefficient in a linear model that adjusts for the $p$ control variables. Then, using this exact null distribution, compute the two-sided p-value for the observed $r$.\n\nYour final answer should be a single decimal number corresponding to the two-sided p-value, rounded to $3$ significant figures.",
            "solution": "The primary task is to justify the use of a t-test for a partial correlation coefficient by relating it to a test on a regression coefficient, and then to compute the corresponding p-value.\n\nLet $Y$ represent systolic blood pressure, $X$ represent urinary sodium, and $Z = (Z_1, Z_2, \\dots, Z_p)$ represent the vector of $p=5$ covariates. The partial correlation coefficient, $\\rho_{XY\\cdot Z}$, quantifies the linear association between $Y$ and $X$ after adjusting for the linear effects of the variables in $Z$. The null hypothesis to be tested is $H_0: \\rho_{XY\\cdot Z} = 0$.\n\nThe equivalence between partial correlation and multiple regression provides the justification for the exact null distribution. This equivalence arises because testing for a partial correlation is conceptually identical to testing for the significance of a predictor in a multiple linear regression model that includes the variables being controlled for.\n\nConsider the multiple linear regression model where $Y$ is predicted by $X$ and all the covariates in $Z$:\n$$\nY = \\beta_0 + \\beta_X X + \\sum_{j=1}^{p} \\beta_j Z_j + \\epsilon\n$$\nIn this model, the coefficient $\\beta_X$ represents the expected change in $Y$ for a one-unit increase in $X$, while holding all the covariates $Z_1, \\dots, Z_p$ constant. This is precisely the interpretation of the association between $Y$ and $X$ after controlling for $Z$. Therefore, testing the null hypothesis that there is no partial correlation between $Y$ and $X$ given $Z$ ($H_0: \\rho_{XY\\cdot Z} = 0$) is mathematically equivalent to testing the null hypothesis that the regression coefficient for $X$ is zero ($H_0: \\beta_X = 0$) in the specified multiple regression model.\n\nFor a multiple linear regression model, the significance of any individual coefficient $\\hat{\\beta}_k$ is tested using a t-test. The test statistic is given by:\n$$\nt = \\frac{\\hat{\\beta}_k - 0}{\\text{SE}(\\hat{\\beta}_k)}\n$$\nwhere $\\text{SE}(\\hat{\\beta}_k)$ is the standard error of the estimated coefficient. Under the null hypothesis $H_0: \\beta_k=0$ and the standard assumptions of linear regression (which are given in the problem), this test statistic follows a Student's t-distribution.\n\nThe degrees of freedom for this t-distribution are given by $df = n - k - 1$, where $n$ is the sample size and $k$ is the total number of predictors in the model. In our model, we have $X$ as one predictor and the $p$ covariates in $Z$ as the other predictors. Thus, the total number of predictors is $k = 1 + p$. The number of parameters estimated in the model is $(1+p)$ slope coefficients plus one intercept, for a total of $p+2$ parameters. The degrees of freedom for the error term, and consequently for the t-test, are:\n$$\ndf = n - (1 + p) - 1 = n - p - 2\n$$\nIt can be shown that the t-statistic for testing $H_0: \\beta_X=0$ can be expressed directly in terms of the sample partial correlation coefficient, $r$ (which is an estimate of $\\rho_{XY\\cdot Z}$). The formula is:\n$$\nt = r \\sqrt{\\frac{n - p - 2}{1 - r^2}}\n$$\nThis formula provides the test statistic for $H_0: \\rho_{XY\\cdot Z}=0$, and its null distribution is a Student's t-distribution with $df = n - p - 2$ degrees of freedom. This constitutes the exact finite-sample null distribution required.\n\nWe can now compute the p-value for the observed data. The givens are:\nSample size, $n = 48$\nNumber of covariates, $p = 5$\nObserved partial correlation, $r = 0.35$\n\nFirst, we calculate the degrees of freedom for the t-distribution:\n$$\ndf = n - p - 2 = 48 - 5 - 2 = 41\n$$\nNext, we calculate the value of the t-statistic using the observed partial correlation:\n$$\nt_{obs} = r \\sqrt{\\frac{df}{1 - r^2}} = 0.35 \\sqrt{\\frac{41}{1 - (0.35)^2}}\n$$\n$$\nt_{obs} = 0.35 \\sqrt{\\frac{41}{1 - 0.1225}} = 0.35 \\sqrt{\\frac{41}{0.8775}}\n$$\n$$\nt_{obs} \\approx 0.35 \\sqrt{46.723646} \\approx 0.35 \\times 6.83547\n$$\n$$\nt_{obs} \\approx 2.392414\n$$\nThe problem requires a two-sided p-value, which corresponds to the hypothesis test $H_0: \\rho_{XY\\cdot Z} = 0$ against the alternative $H_a: \\rho_{XY\\cdot Z} \\neq 0$. The p-value is the probability of observing a t-statistic at least as extreme as the one calculated:\n$$\n\\text{p-value} = P(|T_{41}| \\ge |t_{obs}|) = 2 \\times P(T_{41} \\ge 2.392414)\n$$\nwhere $T_{41}$ is a random variable following a t-distribution with $41$ degrees of freedom. Using a statistical calculator for this probability, we find:\n$$\nP(T_{41} \\ge 2.392414) \\approx 0.01069\n$$\nTherefore, the two-sided p-value is:\n$$\n\\text{p-value} = 2 \\times 0.01069 \\approx 0.02138\n$$\nRounding this value to $3$ significant figures gives $0.0214$.",
            "answer": "$$\\boxed{0.0214}$$"
        },
        {
            "introduction": "Partial correlation is a powerful tool, but it has a crucial limitation: it only measures the *linear* component of an association. This thought experiment is designed to build your critical thinking skills by exploring a scenario where the partial correlation is zero, yet a strong, non-linear conditional dependence persists. Understanding this pitfall is essential for correctly interpreting your results and avoiding erroneous conclusions in real-world data analysis.",
            "id": "4937024",
            "problem": "A biostatistician is studying a nonlinear exposureâ€“response relationship in a cohort where the baseline biomarker $X$ is standard normal, $X \\sim \\mathcal{N}(0,1)$. The outcome $Y$ is generated by a nonlinear mechanism with additive noise, $Y = X^{2} + \\epsilon$, where the measurement error $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ is independent of $X$. A duplicate laboratory channel records $Z$ as the exact biomarker, $Z = X$. The biostatistician wishes to quantify the linear association between $X$ and $Y$ after adjusting for $Z$ using partial correlation.\n\nStarting from the population definition of partial correlation as the Pearson correlation between residuals from Ordinary Least Squares (OLS) projections onto the closed linear span of the conditioning variable(s), proceed as follows:\n\n1. To avoid degeneracy induced by the exact duplicate $Z = X$, introduce a vanishingly small perturbation $\\delta \\sim \\mathcal{N}(0,\\tau^{2})$, independent of $(X,\\epsilon)$, and define $Z_{\\tau} = X + \\delta$. Work with $Z_{\\tau}$ and compute the population partial correlation $\\rho_{XY \\cdot Z_{\\tau}}$ via the correlation between the OLS residual of $X$ regressed on $Z_{\\tau}$ and the OLS residual of $Y$ regressed on $Z_{\\tau}$. Then take the limit as $\\tau \\to 0$ to obtain the value of $\\rho_{XY \\cdot Z}$ for the idealized duplicate $Z = X$.\n\n2. Using the law of total expectation and properties of conditional distributions, give a mathematical argument that, despite the computed partial correlation, the conditional dependence between $X$ and $Y$ persists under this data-generating mechanism.\n\nExpress the final numerical value of $\\rho_{XY \\cdot Z}$ as your answer. No rounding is required.",
            "solution": "Part 1: Computation of the Partial Correlation $\\rho_{XY \\cdot Z}$\n\nThe partial correlation $\\rho_{XY \\cdot Z}$ is defined as the Pearson correlation between the residuals of an Ordinary Least Squares (OLS) regression of $X$ on $Z$ and $Y$ on $Z$. The problem introduces a perturbed variable $Z_{\\tau} = X + \\delta$ to avoid the degeneracy of $Z=X$, where $X \\sim \\mathcal{N}(0,1)$, $\\delta \\sim \\mathcal{N}(0, \\tau^2)$, and $X, \\delta, \\epsilon$ are mutually independent. We first compute $\\rho_{XY \\cdot Z_{\\tau}}$ and then take the limit as $\\tau \\to 0$.\n\nLet $\\tilde{X}_{\\tau}$ be the residual from the OLS regression of $X$ on $Z_{\\tau}$, and $\\tilde{Y}_{\\tau}$ be the residual from the OLS regression of $Y$ on $Z_{\\tau}$. The partial correlation is given by:\n$$\n\\rho_{XY \\cdot Z_{\\tau}} = \\text{Corr}(\\tilde{X}_{\\tau}, \\tilde{Y}_{\\tau}) = \\frac{\\text{Cov}(\\tilde{X}_{\\tau}, \\tilde{Y}_{\\tau})}{\\sqrt{\\text{Var}(\\tilde{X}_{\\tau}) \\text{Var}(\\tilde{Y}_{\\tau})}}\n$$\nThe residual for a variable $A$ regressed on $B$ is $A_{res} = A - (\\beta_0 + \\beta_1 B)$, where $\\beta_1 = \\frac{\\text{Cov}(A,B)}{\\text{Var}(B)}$ and $\\beta_0 = \\text{E}[A] - \\beta_1 \\text{E}[B]$.\n\nFirst, we compute the necessary expectations, variances, and covariances.\nGiven $X \\sim \\mathcal{N}(0,1)$, we have $\\text{E}[X] = 0$ and $\\text{Var}(X) = \\text{E}[X^2] = 1$. The odd moments of a standard normal distribution are zero, so $\\text{E}[X^3] = 0$. The fourth moment is $\\text{E}[X^4] = 3$.\nGiven $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, we have $\\text{E}[\\epsilon] = 0$ and $\\text{Var}(\\epsilon) = \\sigma^2$.\nGiven $\\delta \\sim \\mathcal{N}(0, \\tau^2)$, we have $\\text{E}[\\delta] = 0$ and $\\text{Var}(\\delta) = \\tau^2$.\n\nFor the predictor variable $Z_{\\tau} = X + \\delta$:\n$\\text{E}[Z_{\\tau}] = \\text{E}[X] + \\text{E}[\\delta] = 0 + 0 = 0$.\n$\\text{Var}(Z_{\\tau}) = \\text{Var}(X+\\delta) = \\text{Var}(X) + \\text{Var}(\\delta) = 1 + \\tau^2$ (due to independence of $X, \\delta$).\n\nFor the outcome variable $Y = X^2 + \\epsilon$:\n$\\text{E}[Y] = \\text{E}[X^2 + \\epsilon] = \\text{E}[X^2] + \\text{E}[\\epsilon] = 1 + 0 = 1$.\n\nNow we compute the covariances needed for the OLS coefficients:\n$\\text{Cov}(X, Z_{\\tau}) = \\text{Cov}(X, X+\\delta) = \\text{Cov}(X,X) + \\text{Cov}(X,\\delta) = \\text{Var}(X) + 0 = 1$.\n$\\text{Cov}(Y, Z_{\\tau}) = \\text{Cov}(X^2+\\epsilon, X+\\delta) = \\text{Cov}(X^2, X) + \\text{Cov}(X^2, \\delta) + \\text{Cov}(\\epsilon, X) + \\text{Cov}(\\epsilon, \\delta)$.\nDue to independence, $\\text{Cov}(X^2, \\delta)=0$, $\\text{Cov}(\\epsilon, X)=0$, and $\\text{Cov}(\\epsilon, \\delta)=0$.\n$\\text{Cov}(X^2, X) = \\text{E}[X^2 \\cdot X] - \\text{E}[X^2]\\text{E}[X] = \\text{E}[X^3] - (1)(0) = 0 - 0 = 0$.\nTherefore, $\\text{Cov}(Y, Z_{\\tau}) = 0$.\n\nNow we can find the OLS residuals.\nFor the regression of $X$ on $Z_{\\tau}$:\nThe slope is $\\beta_{X|Z_{\\tau}} = \\frac{\\text{Cov}(X,Z_{\\tau})}{\\text{Var}(Z_{\\tau})} = \\frac{1}{1+\\tau^2}$.\nThe intercept is $\\beta_{0, X|Z_{\\tau}} = \\text{E}[X] - \\beta_{X|Z_{\\tau}} \\text{E}[Z_{\\tau}] = 0$.\nThe residual is $\\tilde{X}_{\\tau} = X - \\beta_{X|Z_{\\tau}}Z_{\\tau} = X - \\frac{1}{1+\\tau^2}(X+\\delta) = \\frac{\\tau^2}{1+\\tau^2}X - \\frac{1}{1+\\tau^2}\\delta$.\n\nFor the regression of $Y$ on $Z_{\\tau}$:\nThe slope is $\\beta_{Y|Z_{\\tau}} = \\frac{\\text{Cov}(Y,Z_{\\tau})}{\\text{Var}(Z_{\\tau})} = \\frac{0}{1+\\tau^2} = 0$.\nThe intercept is $\\beta_{0, Y|Z_{\\tau}} = \\text{E}[Y] - \\beta_{Y|Z_{\\tau}}\\text{E}[Z_{\\tau}] = 1 - 0 \\cdot 0 = 1$.\nThe OLS 'fit' is just the mean of $Y$.\nThe residual is $\\tilde{Y}_{\\tau} = Y - (\\beta_{0, Y|Z_{\\tau}} + \\beta_{Y|Z_{\\tau}}Z_{\\tau}) = Y - 1$.\n\nNext, we compute the covariance of the residuals, $\\text{Cov}(\\tilde{X}_{\\tau}, \\tilde{Y}_{\\tau})$.\n$$\n\\text{Cov}(\\tilde{X}_{\\tau}, \\tilde{Y}_{\\tau}) = \\text{Cov}\\left(\\frac{\\tau^2}{1+\\tau^2}X - \\frac{1}{1+\\tau^2}\\delta, Y-1\\right)\n$$\nSince $Y-1 = X^2+\\epsilon-1$, we have:\n$$\n\\text{Cov}(\\tilde{X}_{\\tau}, \\tilde{Y}_{\\tau}) = \\text{Cov}\\left(\\frac{\\tau^2}{1+\\tau^2}X - \\frac{1}{1+\\tau^2}\\delta, X^2+\\epsilon-1\\right)\n$$\nUsing the bilinearity of covariance and removing the constant $-1$:\n$$\n= \\frac{\\tau^2}{1+\\tau^2}\\text{Cov}(X, X^2+\\epsilon) - \\frac{1}{1+\\tau^2}\\text{Cov}(\\delta, X^2+\\epsilon)\n$$\nWe expand the covariance terms:\n$\\text{Cov}(X, X^2+\\epsilon) = \\text{Cov}(X,X^2) + \\text{Cov}(X,\\epsilon) = 0 + 0 = 0$.\n$\\text{Cov}(\\delta, X^2+\\epsilon) = \\text{Cov}(\\delta,X^2) + \\text{Cov}(\\delta,\\epsilon) = 0 + 0 = 0$ (due to mutual independence).\nThus, $\\text{Cov}(\\tilde{X}_{\\tau}, \\tilde{Y}_{\\tau}) = 0$ for any $\\tau^2 > 0$.\n\nSince the numerator of the partial correlation is zero, $\\rho_{XY \\cdot Z_{\\tau}} = 0$ for all $\\tau > 0$, provided the variances of the residuals are non-zero.\n$\\text{Var}(\\tilde{Y}_{\\tau}) = \\text{Var}(Y-1) = \\text{Var}(X^2+\\epsilon) = \\text{Var}(X^2)+\\text{Var}(\\epsilon) = (\\text{E}[X^4] - (\\text{E}[X^2])^2) + \\sigma^2 = (3 - 1^2) + \\sigma^2 = 2+\\sigma^2 > 0$.\n$\\text{Var}(\\tilde{X}_{\\tau}) = \\text{Var}(\\frac{\\tau^2}{1+\\tau^2}X - \\frac{1}{1+\\tau^2}\\delta) = (\\frac{\\tau^2}{1+\\tau^2})^2\\text{Var}(X) + (\\frac{1}{1+\\tau^2})^2\\text{Var}(\\delta) = \\frac{\\tau^4}{(1+\\tau^2)^2}(1) + \\frac{1}{(1+\\tau^2)^2}(\\tau^2) = \\frac{\\tau^4+\\tau^2}{(1+\\tau^2)^2} = \\frac{\\tau^2(1+\\tau^2)}{(1+\\tau^2)^2} = \\frac{\\tau^2}{1+\\tau^2} > 0$ for $\\tau > 0$.\nSince the variances are non-zero, the partial correlation is well-defined and equal to zero.\n\nFinally, we take the limit as $\\tau \\to 0$:\n$$\n\\rho_{XY \\cdot Z} = \\lim_{\\tau \\to 0} \\rho_{XY \\cdot Z_{\\tau}} = \\lim_{\\tau \\to 0} 0 = 0\n$$\n\nPart 2: Argument for Persistent Conditional Dependence\n\nThe result $\\rho_{XY \\cdot Z} = 0$ indicates that there is no *linear* association between $X$ and $Y$ after linearly adjusting for $Z=X$. However, partial correlation is not a general measure of conditional dependence. The equivalence between zero partial correlation and conditional independence ($\\rho_{AB \\cdot C}=0 \\iff A \\perp B | C$) holds only for variables that follow a multivariate normal distribution.\n\nIn this problem, the joint distribution of $(X,Y,Z)$ is not multivariate normal because the relationship $Y = X^2 + \\epsilon$ is nonlinear. Therefore, we cannot conclude conditional independence from the zero partial correlation.\n\nTo demonstrate the persistence of conditional dependence, we must examine the conditional distribution of one variable given the others. Let us analyze the conditional distribution of $Y$ given $Z=z$. Since $Z = X$, this is equivalent to examining the conditional distribution of $Y$ given $X=x$.\n\nThe data-generating model is $Y = X^2 + \\epsilon$. When we condition on a specific value $X=x$, the model becomes:\n$$\nY | (X=x) = x^2 + \\epsilon\n$$\nSince $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is independent of $X$, the distribution of $Y$ conditional on $X=x$ is a normal distribution with mean $x^2$ and variance $\\sigma^2$:\n$$\nY | (X=x) \\sim \\mathcal{N}(x^2, \\sigma^2)\n$$\nThe conditional expectation of $Y$ given $X=x$ is:\n$$\n\\text{E}[Y | X=x] = x^2\n$$\nStatistical dependence between two variables $Y$ and $X$ is defined by the property that the conditional distribution of $Y$ given $X=x$ changes with $x$. A direct consequence is that if $\\text{E}[Y | X=x]$ is not a constant, the variables are dependent.\nIn this case, $\\text{E}[Y | X=x] = x^2$ is a non-constant function of $x$. For example, $\\text{E}[Y | X=1] = 1$ while $\\text{E}[Y | X=2] = 4$. This demonstrates that knowing the value of $X$ changes our expectation of the value of $Y$. This is the very definition of statistical dependence.\n\nIn conclusion, although the partial correlation $\\rho_{XY \\cdot Z}$ is zero, the variables $Y$ and $X$ (and therefore $Y$ and $Z$) are strongly dependent. The partial correlation method, being based on linear regressions, fails to detect the purely quadratic nature of the relationship. The conditional dependence between $Y$ and $X$ not only persists but is a defining feature of the data-generating mechanism.",
            "answer": "$$\n\\boxed{0}\n$$"
        }
    ]
}