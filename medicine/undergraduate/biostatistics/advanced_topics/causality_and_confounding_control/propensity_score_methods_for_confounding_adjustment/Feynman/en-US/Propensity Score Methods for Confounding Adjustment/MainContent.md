## Introduction
In scientific inquiry, determining cause and effect is a central challenge. While [randomized controlled trials](@entry_id:905382) (RCTs) represent the gold standard by ensuring comparability between groups, they are often not feasible. This leaves researchers with observational data, where systematic differences between treated and untreated groups—a problem known as confounding—can severely bias conclusions about a treatment's true effect. How can we draw reliable causal conclusions when randomization is not an option?

This article introduces Propensity Score Methods, a powerful statistical framework designed to address this very knowledge gap. By modeling the probability of receiving treatment, these methods allow us to adjust for pre-existing differences, effectively mimicking the balance achieved in an RCT. Over the next three chapters, you will embark on a comprehensive journey into this essential topic. In "Principles and Mechanisms," you will explore the foundational theory, from the logic of [potential outcomes](@entry_id:753644) to the elegant properties of the [propensity score](@entry_id:635864) itself. Following this, "Applications and Interdisciplinary Connections" will demonstrate how these methods are used to solve real-world problems across diverse fields like medicine and economics. Finally, "Hands-On Practices" will provide you with exercises to solidify your understanding and apply these concepts yourself, transforming theory into practical skill.

## Principles and Mechanisms

In our quest to understand the world, few questions are more fundamental than those of cause and effect. Does a new drug cure a disease? Does a policy change improve social welfare? The gold standard for answering such questions is the **Randomized Controlled Trial (RCT)**. By randomly assigning individuals to either a treatment or a control group, we create two populations that are, on average, identical in every conceivable way—both known and unknown—except for the single factor we are studying. Randomization magically erases systematic differences, ensuring that any observed difference in outcomes can be confidently attributed to the treatment. In the language of statistics, [randomization](@entry_id:198186) achieves **[exchangeability](@entry_id:263314)**: the outcome that a person *would have had* under treatment (or no treatment) is independent of the treatment they actually received.

But what happens when an RCT is unethical, impractical, or impossible? We are often left with a messy jumble of **observational data**. Here, people choose—or are chosen for—treatments for all sorts of reasons. Patients who receive a new heart medication might be sicker to begin with than those who don't. Students who attend a tutoring program might be more motivated than those who don't. The treated and untreated groups are no longer exchangeable. Their outcomes differ not just because of the treatment, but because of these pre-existing differences. This tangled web of cause and effect is what we call **[confounding](@entry_id:260626)**, and it is the central villain in the story of [observational research](@entry_id:906079).

Propensity score methods are one of the most elegant and powerful ideas ever devised to confront this villain. They provide a recipe for taking messy observational data and creating a "pseudo-randomized trial" where fair comparisons can once again be made. To appreciate this beautiful idea, we must first build our story from the ground up, starting with the very logic of causal reasoning itself.

### A Bridge to a Causal World

Before we can even talk about [confounding](@entry_id:260626), we need to establish a clear framework that connects the data we *observe* to the causal effects we *wish to know*. This requires a few foundational assumptions, which act as a bridge between the real world and the theoretical world of "what if."

First, for every individual, we must imagine there are well-defined **[potential outcomes](@entry_id:753644)**. For a binary treatment $A$ (where $A=1$ is treatment and $A=0$ is control), we imagine that each person has two [potential outcomes](@entry_id:753644): $Y(1)$, the outcome they would have if they received the treatment, and $Y(0)$, the outcome they would have if they did not. The causal effect for that individual is the difference, $Y(1) - Y(0)$, a quantity we can never directly observe because a person can only take one path.

To make these [potential outcomes](@entry_id:753644) meaningful, we rely on two core principles, often bundled under the **Stable Unit Treatment Value Assumption (SUTVA)** . The first part is **no interference**: the [potential outcomes](@entry_id:753644) for one person do not depend on the treatment assigned to anyone else. My outcome from a vaccine depends only on whether *I* got the shot, not whether my neighbor did. The second part is **no hidden versions of treatment**: when we say "treatment," we mean one specific thing. If $A=1$ means taking a pill, it's the same pill, at the same dose, for everyone who gets it.

With well-defined [potential outcomes](@entry_id:753644), we need one final link: the **consistency** assumption. This states that the outcome we actually observe for an individual is precisely the potential outcome corresponding to the treatment they received. If person $i$ took the treatment ($A_i=1$), their observed outcome $Y_i$ is equal to their potential outcome under treatment, $Y_i(1)$. It's a simple-sounding but crucial link that allows our [real-world data](@entry_id:902212) to speak about the unobserved causal world .

### Taming the Confounder: The "As-If" Randomization

With our logical bridge in place, we can now tackle confounding. In an [observational study](@entry_id:174507), the treated and untreated groups are not exchangeable. But what if we could break them down into smaller, more comparable subgroups? Imagine we are studying a drug and we know that age is a confounder—older people are both more likely to take the drug and more likely to have a poor outcome. While the entire group of drug-takers isn't comparable to the non-takers, the 65-year-old drug-takers might be quite comparable to the 65-year-old non-takers.

This is the central strategy: **conditioning**. We assume that if we can identify and measure a sufficient set of pre-treatment covariates, let's call them $\mathbf{X}$, that capture all the common causes of treatment and outcome, then *within strata defined by these covariates*, the treatment assignment is "as-if" random. This is the assumption of **[conditional exchangeability](@entry_id:896124)**, or **ignorability** . Formally, we assume $(Y(0), Y(1)) \perp A \mid \mathbf{X}$. This is a bold, untestable claim that there are no important *unmeasured* confounders, but it is the cornerstone of [causal inference](@entry_id:146069) from observational data.

Selecting the right covariates for $\mathbf{X}$ is a delicate art .
- We must include all known **confounders**—variables that are common causes of both the treatment and the outcome (like disease severity).
- We should also include variables that are strong predictors of the outcome, even if they don't predict treatment, as this can increase the precision of our estimate.
- Critically, we must *exclude* certain variables. We must not adjust for anything that happens *after* treatment begins. A variable on the causal pathway from treatment to outcome is a **mediator**. Adjusting for it would be like trying to estimate a parachute's effectiveness but controlling for the "variable" of a soft landing—you'd block the very effect you want to measure.
- We also must not adjust for **colliders**, which are variables that are the common *effect* of two other variables. Adjusting for a [collider](@entry_id:192770) can create a spurious [statistical association](@entry_id:172897) where none exists, a nasty form of bias.

This leads us to a new challenge. To achieve "as-if" [randomization](@entry_id:198186), we need to compare people who have the exact same values for all covariates in $\mathbf{X}$. But what if $\mathbf{X}$ includes dozens of variables? The "[curse of dimensionality](@entry_id:143920)" kicks in; it becomes practically impossible to find enough people who match perfectly on every single one. It seems we've traded one problem for another.

### The Propensity Score: A Magical Scalar

This is where the genius of the **[propensity score](@entry_id:635864)** enters the scene. Defined by Paul Rosenbaum and Donald Rubin in 1983, the [propensity score](@entry_id:635864), $e(\mathbf{X})$, is simply the probability of an individual receiving the treatment, given their vector of pre-treatment covariates $\mathbf{X}$.

$$e(\mathbf{X}) = P(A=1 \mid \mathbf{X})$$

On the surface, this looks like just another statistical calculation. But this single number, this scalar, possesses two almost magical properties.

First is the **balancing property**. The [propensity score](@entry_id:635864) is a *[balancing score](@entry_id:911689)*. This means that if you condition on the value of the [propensity score](@entry_id:635864), the distribution of the entire vector of covariates, $\mathbf{X}$, becomes the same between the treated and untreated groups . In other words, if you take a group of people who all had, say, a $70\%$ chance of receiving the treatment, the treated members of that group and the untreated members of that group will have, on average, the same age, the same [blood pressure](@entry_id:177896), the same everything that went into $\mathbf{X}$. The [propensity score](@entry_id:635864) acts as a summary that miraculously balances all the covariates simultaneously.

The second, and more profound, property follows directly from the first. If our assumption of [conditional exchangeability](@entry_id:896124) held for the high-dimensional vector $\mathbf{X}$, it now holds for the one-dimensional [propensity score](@entry_id:635864), $e(\mathbf{X})$  .

$$(Y(1), Y(0)) \perp A \mid \mathbf{X} \quad \implies \quad (Y(1), Y(0)) \perp A \mid e(\mathbf{X})$$

This is a breathtaking result. The entire [confounding](@entry_id:260626) problem, spread across potentially dozens of variables, has been collapsed into a single dimension. We no longer need to find a treated person and an untreated person with the exact same covariate profile; we only need to find two people who had the same *probability* of being treated. It tells us that for [confounding adjustment](@entry_id:914495), it is not the specific combination of covariates that matters, but simply the resulting propensity for treatment. In fact, any [one-to-one transformation](@entry_id:148028) of the [propensity score](@entry_id:635864) (like the log-odds, or logit) is also a [balancing score](@entry_id:911689), showing that it's the *information* about treatment probability that does the work, not the specific $0$-to-$1$ scale .

### Putting the Score to Work

Now that we have this miraculous summary score, how do we use it to estimate a causal effect? There are several elegant mechanisms.

**1. Matching and Stratification:** The most intuitive approach is to use the score to create matched groups, just as we imagined. For each treated individual, we can find one or more untreated individuals with a very similar [propensity score](@entry_id:635864). By creating a new dataset of these matched pairs, we have constructed a pseudo-experiment where the two groups are now well-balanced. Alternatively, we can use **stratification** (or subclassification), dividing the entire study population into, say, five or ten strata based on the [propensity score](@entry_id:635864) (e.g., $0-0.2$, $0.2-0.4$, etc.). Within each stratum, the covariates are roughly balanced. We can calculate the [treatment effect](@entry_id:636010) in each stratum and then compute a weighted average to get the overall effect. As we imagine these strata getting narrower and narrower, this method asymptotically removes all [confounding bias](@entry_id:635723) .

**2. Inverse Probability of Treatment Weighting (IPTW):** This is perhaps the most powerful and flexible use of [propensity scores](@entry_id:913832). The core idea is to re-weight the individuals in the sample to create a new "pseudo-population" in which confounding no longer exists.

Think of it this way: in our observational sample, individuals are not there by chance. A person with characteristics that make them highly likely to receive the treatment (say, a [propensity score](@entry_id:635864) of $0.9$) is "over-represented" in the treated group compared to what they would be in a randomized trial. Conversely, a person with a low [propensity score](@entry_id:635864) of $0.1$ who nonetheless received the treatment is a "surprise"—they are more like someone who was randomly assigned treatment.

IPTW corrects for this by assigning a weight to each person that is the inverse of the probability of receiving the treatment they actually received.
- For a treated person ($A=1$), the weight is $\frac{1}{e(\mathbf{X})}$.
- For an untreated person ($A=0$), the weight is $\frac{1}{1 - e(\mathbf{X})}$.

A "surprising" treated person with $e(\mathbf{X}) = 0.1$ gets a large weight of $1/0.1 = 10$, so they represent 10 people in the pseudo-population. An "expected" treated person with $e(\mathbf{X}) = 0.9$ gets a small weight of $1/0.9 \approx 1.1$. By applying these weights, we create a new population where the treatment is no longer associated with the covariates $\mathbf{X}$. We can then simply compute the difference in the [weighted mean](@entry_id:894528) outcomes between the treated and control groups to get an unbiased estimate of the causal effect.

What's more, by subtly changing the weights, we can answer different causal questions .
- The weights above target the **Average Treatment Effect (ATE)**: the effect if everyone in the population were treated versus if no one were.
- By using weights of $1$ for the treated and $\frac{e(\mathbf{X})}{1-e(\mathbf{X})}$ for the untreated, we can estimate the **Average Treatment Effect on the Treated (ATT)**: the effect specifically for the type of people who actually chose to receive the treatment. This is often the more relevant policy question.

### A Dose of Reality: Diagnostics and Overlap

This statistical machinery is powerful, but it's not magic. It relies on a critical assumption we have not yet discussed: **positivity**, also known as **common support** . This means that for any given set of covariates $\mathbf{X}$, there must be a non-zero probability of being both treated and untreated. That is, $0  e(\mathbf{X})  1$.

If this assumption is violated—for instance, if every person with a certain characteristic always receives the treatment, so $e(\mathbf{X})=1$ for them—then we have no untreated individuals in that group to serve as a comparison. Causal inference for that subgroup is impossible. This logical failure shows up in the math: the IPTW weight for a hypothetical untreated person in that group would be $\frac{1}{1-e(\mathbf{X})} = \frac{1}{1-1} = \frac{1}{0}$, an infinite weight. The method breaks down precisely where the logic does.

This raises a final, crucial question: How do we know if we did a good job? The answer is **diagnostics**. After applying our [propensity score](@entry_id:635864) method (matching or weighting), we must check if it actually worked. The goal of the [propensity score](@entry_id:635864) model is *not* to predict treatment as accurately as possible. In fact, a model that predicts treatment perfectly (e.g., with a [c-statistic](@entry_id:906510) or AUC of $1.0$) is a disaster—it implies a complete lack of positivity and makes causal inference impossible .

The true test is **balance**. Did our adjustment procedure make the covariate distributions similar between the treated and control groups in the new matched or weighted sample? To check this, we use metrics like the **Absolute Standardized Mean Difference (ASMD)** . For each covariate, this metric calculates the difference in means between the two groups, scaled by a [pooled standard deviation](@entry_id:198759). Unlike a [p-value](@entry_id:136498), it's independent of sample size and provides a pure measure of the magnitude of the imbalance. A common rule of thumb is that an ASMD below $0.1$ for all covariates suggests adequate balance has been achieved, and our pseudo-population now successfully mimics the properties of a randomized trial.

In the end, the journey through [propensity scores](@entry_id:913832) reveals a deep and satisfying unity. It begins with the fundamental chaos of observational data. By applying a clear logical framework, we diagnose the problem of confounding. We then use our knowledge of the world to build a model for treatment selection, which yields the remarkable [propensity score](@entry_id:635864). This single score, in turn, allows us to re-shape our data through matching or weighting, creating a balanced, analyzable dataset where the fog of [confounding](@entry_id:260626) has been lifted, and the path to a causal conclusion is finally illuminated.