{
    "hands_on_practices": [
        {
            "introduction": "The bias-variance tradeoff can often feel abstract. This first exercise provides a direct, hands-on experience with the phenomenon by asking you to build a simulation from the ground up . By generating data and fitting both a simple and a complex model, you will concretely observe how increasing model complexity reduces training error but can lead to worse generalization performance, a classic sign of overfitting. This practice is fundamental for building intuition about why we need robust evaluation techniques like cross-validation.",
            "id": "4897610",
            "problem": "You are to design and implement a complete simulation study that exposes the bias-variance tradeoff using polynomial regression models under a realistic data-generating mechanism, and to use Cross-Validation (CV) to estimate expected test error. The goal is to construct scenarios where a model with more parameters achieves lower training error but worse expected test error, and to demonstrate that this behavior is driven by an increase in the explicit variance term.\n\nFundamental base and setup:\n- Let the predictor be a real-valued covariate $x \\in [-1,1]$ distributed as $x \\sim \\mathrm{Uniform}([-1,1])$.\n- Let the response be generated by $y = f(x) + \\epsilon$, where $f(x)$ is a deterministic function and $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is independent noise with variance $\\sigma^2$.\n- Define the true signal $f(x)$ by $f(x) = \\alpha \\sin(\\pi x) + \\beta x$, where $\\alpha$ and $\\beta$ are fixed real constants.\n- Consider polynomial regression models of degree $d$ (including an intercept), which estimate a function $\\hat{f}_d(x)$ via least squares on the training data. The “simple” model uses degree $d_s$ and the “complex” model uses degree $d_c$ with $d_c > d_s$.\n\nCore definitions to be used:\n- The training mean squared error (MSE) is $\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i - \\hat{f}(x_i)\\right)^2$ for a training dataset of size $n$.\n- The $K$-fold cross-validation estimator of expected test MSE is the average of held-out fold MSEs over $K$ folds formed by partitioning the training dataset.\n- The bias-variance decomposition at a fixed point $x$ under the data-generating model $y = f(x) + \\epsilon$ states\n$$\n\\mathbb{E}\\left[(\\hat{f}(x) - y)^2\\right] = \\left(\\mathbb{E}[\\hat{f}(x)] - f(x)\\right)^2 + \\mathrm{Var}(\\hat{f}(x)) + \\sigma^2,\n$$\nwhere the expectation and variance are taken over the randomness in the training sample (and $\\epsilon$ when applicable). For this simulation, you will estimate the variance term $\\mathrm{Var}(\\hat{f}(x))$ explicitly by Monte Carlo replicates.\n\nTask requirements:\n1. For each specified test case, generate $B$ independent training datasets of size $n$ according to $x \\sim \\mathrm{Uniform}([-1,1])$ and $y = f(x) + \\epsilon$, with $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ and fixed $f(x) = \\alpha \\sin(\\pi x) + \\beta x$. Use $\\alpha = 1.5$ and $\\beta = 0.5$.\n2. For each dataset and each model degree $d \\in \\{d_s, d_c\\}$:\n   - Fit the least squares polynomial model of degree $d$.\n   - Compute the training MSE.\n   - Compute the $K$-fold cross-validation MSE, with $K=5$ folds.\n3. To estimate the variance term $\\mathrm{Var}(\\hat{f}(x))$ for each degree $d$, construct a fixed grid of $m$ points $\\{x_j\\}_{j=1}^m$ covering $[-1,1]$, compute $\\hat{f}_d^{(b)}(x_j)$ for each Monte Carlo replicate $b \\in \\{1,\\dots,B\\}$, then estimate $\\mathrm{Var}(\\hat{f}(x_j))$ by the sample variance across replicates. Average these variances over the grid to obtain a single scalar summary\n$$\nV_d = \\frac{1}{m} \\sum_{j=1}^m \\widehat{\\mathrm{Var}}\\left(\\hat{f}_d(x_j)\\right).\n$$\n4. For each test case, aggregate across the $B$ replicates to compute:\n   - The average training MSE for the simple model and the complex model.\n   - The average $K$-fold CV MSE for the simple model and the complex model.\n   - The variance summaries $V_{d_s}$ and $V_{d_c}$.\n5. Define the overfitting manifestation indicator for a test case as $1$ (true) if all of the following three conditions hold, and $0$ (false) otherwise:\n   - The average training MSE of the complex model is strictly less than that of the simple model.\n   - The average $K$-fold CV MSE of the complex model is strictly greater than that of the simple model.\n   - The variance summary $V_{d_c}$ is strictly greater than $V_{d_s}$.\n\nParameter values (test suite):\n- Use $B = 200$ and $m = 201$ grid points.\n- Use $K = 5$ folds.\n- Use the following three test cases, which together probe a general scenario, a large-sample boundary, and a noise-free edge case:\n  1. Case A (happy path): $n = 60$, $\\sigma = 0.5$, $d_s = 3$, $d_c = 18$.\n  2. Case B (large sample boundary): $n = 400$, $\\sigma = 0.5$, $d_s = 3$, $d_c = 18$.\n  3. Case C (noise-free edge case): $n = 60$, $\\sigma = 0$, $d_s = 3$, $d_c = 18$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$), where each $resultk$ is the integer $0$ or $1$ for the $k$-th test case, evaluated according to the indicator described above. No other output or text should be printed.",
            "solution": "The problem presents a valid and well-posed simulation study in the field of biostatistics, designed to explore the bias-variance tradeoff using polynomial regression. All parameters and procedures are clearly defined, scientifically grounded in established statistical principles, and computationally feasible. The problem is objective and free of ambiguity, satisfying all criteria for validation.\n\nThe core of the task is to simulate data from a known true function, fit models of varying complexity, and evaluate their performance to illustrate the concept of overfitting. The simulation framework is defined as follows:\n\nThe data-generating process is based on a predictor variable $x$ sampled uniformly from the interval $[-1, 1]$, i.e., $x \\sim \\mathrm{Uniform}([-1,1])$. The corresponding response variable $y$ is generated by the model $y = f(x) + \\epsilon$, where $f(x)$ is the deterministic true signal and $\\epsilon$ is a random noise term. The signal is given by the function $f(x) = \\alpha \\sin(\\pi x) + \\beta x$, with specified constants $\\alpha = 1.5$ and $\\beta = 0.5$. The noise follows a normal distribution, $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, with its variance $\\sigma^2$ being one of the parameters of the simulation.\n\nWe compare two models: a \"simple\" polynomial regression model of degree $d_s$ and a \"complex\" model of degree $d_c$, where $d_c > d_s$. For each training dataset of size $n$, we fit the models $\\hat{f}_{d_s}(x)$ and $\\hat{f}_{d_c}(x)$ by minimizing the residual sum of squares (ordinary least squares).\n\nThe simulation protocol is executed for $B=200$ Monte Carlo replicates for each of the three test cases provided. Each replicate involves these steps:\n1.  **Data Generation**: A new training dataset of size $n$, $\\{(x_i, y_i)\\}_{i=1}^n$, is created according to the specified data-generating process.\n2.  **Model Fitting and Evaluation**: For each degree $d \\in \\{d_s, d_c\\}$:\n    -   The polynomial model $\\hat{f}_d(x)$ is fitted to the training data.\n    -   The **training mean squared error (MSE)** is calculated as $\\mathrm{MSE}_{\\text{train}} = \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i - \\hat{f}_d(x_i)\\right)^2$. This measures how well the model fits the data it was trained on.\n    -   The expected test MSE is estimated using **$K$-fold cross-validation (CV)**, with $K=5$. The training set is partitioned into $K$ disjoint subsets (folds). For each fold, a model is trained on the remaining $K-1$ folds and tested on the held-out fold. The CV error, $\\mathrm{MSE}_{\\text{CV}}$, is the average of the MSEs from these $K$ validation folds. This metric provides a more robust estimate of the model's generalization performance on unseen data.\n\n3.  **Explicit Variance Estimation**: The variance of the model itself is a key component of the bias-variance tradeoff. We estimate it directly via the Monte Carlo simulation. A fixed grid of $m=201$ points, $\\{x_j\\}_{j=1}^m$, is established over the domain $[-1,1]$. For each replicate $b$ and each model degree $d$, we compute the predictions $\\hat{f}_d^{(b)}(x_j)$ on this grid. The variance of the estimator at a point $x_j$ is estimated by the sample variance of predictions at that point across all $B$ replicates. A single scalar summary of the model variance, $V_d$, is then computed by averaging these point-wise variance estimates over the entire grid: $V_d = \\frac{1}{m} \\sum_{j=1}^m \\widehat{\\mathrm{Var}}\\left(\\hat{f}_d(x_j)\\right)$.\n\nAfter all $B$ replicates are completed, the collected metrics are averaged to yield $\\overline{\\mathrm{MSE}}_{\\text{train}}$, $\\overline{\\mathrm{MSE}}_{\\text{CV}}$, and the aggregate variance term $V_d$ for both the simple and complex models.\n\nFinally, an **overfitting manifestation indicator** is computed for each test case. This binary indicator is set to $1$ (true) if and only if all three of the following conditions are simultaneously satisfied, and $0$ (false) otherwise:\n1.  $\\overline{\\mathrm{MSE}}_{\\text{train}, d_c} < \\overline{\\mathrm{MSE}}_{\\text{train}, d_s}$: The complex model achieves a lower error on the training data.\n2.  $\\overline{\\mathrm{MSE}}_{\\text{CV}, d_c} > \\overline{\\mathrm{MSE}}_{\\text{CV}, d_s}$: The complex model exhibits a higher estimated test error, indicating poor generalization (overfitting).\n3.  $V_{d_c} > V_{d_s}$: The complex model is more variable (less stable) than the simple model.\n\nThis structured analysis is applied to three distinct test cases to probe the behavior of the models under different conditions of sample size and noise level. The implementation will proceed by systematically executing this simulation design and reporting a list of the resulting indicator values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs a simulation study to demonstrate the bias-variance tradeoff\n    and computes an overfitting indicator for three test cases.\n    \"\"\"\n    # Set a random seed for reproducibility of the simulation results.\n    np.random.seed(42)\n\n    # --- Simulation parameters from the problem statement ---\n    B = 200      # Number of Monte Carlo replicates\n    m = 201      # Number of grid points for variance estimation\n    K = 5        # Number of folds for cross-validation\n    alpha = 1.5  # Parameter for the true function f(x)\n    beta = 0.5   # Parameter for the true function f(x)\n\n    # Test cases: (n, sigma, d_s, d_c)\n    test_cases = [\n        (60, 0.5, 3, 18),   # Case A: Canonical overfitting scenario\n        (400, 0.5, 3, 18),  # Case B: Large sample size scenario\n        (60, 0.0, 3, 18),   # Case C: Noise-free scenario\n    ]\n\n    def calculate_cv_mse(x, y, d, k_folds):\n        \"\"\"\n        Calculates the K-fold cross-validation MSE for a polynomial model.\n        \"\"\"\n        fold_mses = []\n        n_samples = len(x)\n        indices = np.arange(n_samples)\n        \n        # Shuffle indices to ensure random distribution of data across folds.\n        np.random.shuffle(indices)\n        \n        # Split indices into k_folds.\n        fold_indices = np.array_split(indices, k_folds)\n\n        for k in range(k_folds):\n            val_idx = fold_indices[k]\n            train_idx = np.concatenate([fold_indices[i] for i in range(k_folds) if i != k])\n\n            x_fold_train, y_fold_train = x[train_idx], y[train_idx]\n            x_fold_val, y_fold_val = x[val_idx], y[val_idx]\n\n            # Fit model on the training part of the fold.\n            # numpy might issue a RankWarning for high-degree polynomials, \n            # which is expected and can be ignored for this problem.\n            coeffs = np.polyfit(x_fold_train, y_fold_train, d)\n            \n            # Predict on the validation part.\n            y_pred_val = np.polyval(coeffs, x_fold_val)\n\n            # Calculate and store MSE for the fold.\n            fold_mse = np.mean((y_fold_val - y_pred_val)**2)\n            fold_mses.append(fold_mse)\n        \n        return np.mean(fold_mses)\n\n    final_results = []\n\n    for case in test_cases:\n        n, sigma, d_s, d_c = case\n\n        # Accumulators for metrics over B replicates\n        train_mses_s, train_mses_c = [], []\n        cv_mses_s, cv_mses_c = [], []\n        \n        # Grid for variance estimation\n        grid = np.linspace(-1, 1, m)\n        \n        # Storage for predictions on the grid across all replicates\n        all_preds_s = np.zeros((B, m))\n        all_preds_c = np.zeros((B, m))\n\n        for b in range(B):\n            # 1. Generate data for the current replicate\n            x_train = np.random.uniform(-1, 1, n)\n            y_true = alpha * np.sin(np.pi * x_train) + beta * x_train\n            if sigma > 0:\n                noise = np.random.normal(0, sigma, n)\n                y_train = y_true + noise\n            else:\n                y_train = y_true\n\n            # 2. Process simple model (degree d_s)\n            coeffs_s = np.polyfit(x_train, y_train, d_s)\n            y_pred_train_s = np.polyval(coeffs_s, x_train)\n            train_mses_s.append(np.mean((y_train - y_pred_train_s)**2))\n            cv_mses_s.append(calculate_cv_mse(x_train, y_train, d_s, K))\n            all_preds_s[b, :] = np.polyval(coeffs_s, grid)\n\n            # 3. Process complex model (degree d_c)\n            coeffs_c = np.polyfit(x_train, y_train, d_c)\n            y_pred_train_c = np.polyval(coeffs_c, x_train)\n            train_mses_c.append(np.mean((y_train - y_pred_train_c)**2))\n            cv_mses_c.append(calculate_cv_mse(x_train, y_train, d_c, K))\n            all_preds_c[b, :] = np.polyval(coeffs_c, grid)\n\n        # 4. Aggregate results across B replicates\n        avg_train_mse_s = np.mean(train_mses_s)\n        avg_train_mse_c = np.mean(train_mses_c)\n        avg_cv_mse_s = np.mean(cv_mses_s)\n        avg_cv_mse_c = np.mean(cv_mses_c)\n\n        # Calculate variance summary V_d using sample variance (ddof=1)\n        V_s = np.mean(np.var(all_preds_s, axis=0, ddof=1))\n        V_c = np.mean(np.var(all_preds_c, axis=0, ddof=1))\n        \n        # 5. Apply the overfitting manifestation indicator logic\n        cond1 = avg_train_mse_c < avg_train_mse_s\n        cond2 = avg_cv_mse_c > avg_cv_mse_s\n        cond3 = V_c > V_s\n        \n        indicator = 1 if (cond1 and cond2 and cond3) else 0\n        final_results.append(indicator)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Cross-validation is not just for estimating a final error rate; it is a powerful framework for tuning models to meet specific, real-world objectives. This practice moves into a realistic biostatistics scenario where the goal is not merely to be \"accurate,\" but to optimize a classifier for a specific clinical need: maximizing sensitivity while guaranteeing a minimum level of specificity . You will derive a custom-penalized loss function that translates this clinical goal into a mathematical objective, demonstrating how to leverage a robust, stratified cross-validation scheme for nuanced model selection.",
            "id": "4897592",
            "problem": "A biostatistics team is building a binary risk classifier for an imbalanced case-control study, with cases denoted by $Y=1$ and controls denoted by $Y=0$. For each subject $i \\in \\{1,\\dots,n\\}$, a model produces a real-valued score $\\hat f(X_i)$ intended to be larger for cases than for controls. The classifier will be implemented by thresholding this score at a tunable threshold $\\tau \\in \\mathbb{R}$ via the rule: predict $1$ if $\\hat f(X_i) \\ge \\tau$, and $0$ otherwise. The clinical goal is to choose $\\tau$ to maximize sensitivity subject to achieving at least a target specificity $\\alpha \\in (0,1)$. The team wants to estimate performance and tune $\\tau$ using stratified repeated $K$-fold cross-validation (CV), with $K \\in \\{2,3,\\dots\\}$ folds and $R \\in \\{1,2,\\dots\\}$ independent repetitions.\n\nStarting from the definitions of sensitivity and specificity and the definition of $K$-fold cross-validation, design a stratified repeated CV scheme appropriate for imbalanced case-control data that uses out-of-fold predictions only, and aggregates performance estimates across repetitions in a way that controls variance. Then, using only these foundational definitions and avoiding any in-sample reuse, derive an empirical loss function $\\widehat{L}(\\tau)$ suitable for threshold tuning that encodes the objective “maximize sensitivity subject to specificity at least $\\alpha$” by means of a one-sided linear penalty that is zero when the specificity constraint is met and increases linearly with the shortfall when it is violated. Your loss must be written explicitly in terms of indicator functions, the out-of-fold predictions from the repeated stratified $K$-fold CV scheme, and the quantities $n$, $K$, $R$, and $\\alpha$, together with a penalty weight $\\lambda>0$ controlling the strength of the constraint.\n\nAssumptions and conventions:\n- For each repetition $r \\in \\{1,\\dots,R\\}$, partition the data indices $\\{1,\\dots,n\\}$ into $K$ folds with stratification on $Y$ so that each fold preserves the empirical class proportions as closely as possible. For each subject $i$ in a validation fold within repetition $r$, let $\\hat f^{(-i,r)}(X_i)$ denote the score produced by training the model on the $K-1$ training folds (excluding the fold containing $i$) in repetition $r$ and predicting on $X_i$.\n- Use the micro-averaged aggregation across all out-of-fold predictions from all repetitions to estimate sensitivity and specificity.\n- Use the classification rule “predict $1$ if and only if $\\hat f^{(-i,r)}(X_i) \\ge \\tau$” when evaluating out-of-fold predictions.\n\nProvide your final answer as a single closed-form analytic expression for the cross-validated penalized loss $\\widehat{L}(\\tau)$, using the notation $\\mathbb{1}\\{\\cdot\\}$ for indicator functions and $(x)_{+}=\\max\\{x,0\\}$. No numerical computation is required, and no units apply.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard, albeit non-trivial, task in biostatistical modeling: deriving a penalized loss function for tuning a classifier threshold based on performance estimates from stratified repeated cross-validation. All definitions and constraints are clear, consistent, and sufficient for deriving a unique analytical solution.\n\nThe solution is developed in three steps. First, we outline the stratified repeated cross-validation (CV) and aggregation scheme as specified. Second, we formalize the micro-averaged estimates of sensitivity and specificity based on the out-of-fold predictions from this scheme. Finally, we construct the penalized empirical loss function $\\widehat{L}(\\tau)$ that encodes the specified optimization objective.\n\n**Step 1: Stratified Repeated K-Fold Cross-Validation Scheme**\n\nThe problem requires a stratified repeated $K$-fold CV scheme. This procedure is designed to produce reliable out-of-sample performance estimates, particularly with imbalanced data. Let $n$ be the total number of subjects, with $N_1 = \\sum_{i=1}^n \\mathbb{1}\\{Y_i=1\\}$ cases ($Y=1$) and $N_0 = \\sum_{i=1}^n \\mathbb{1}\\{Y_i=0\\}$ controls ($Y=0$).\n\nThe procedure is as follows:\n1.  The entire process is repeated $R$ times, indexed by $r \\in \\{1,\\dots,R\\}$. Each repetition involves a new, independent random partitioning of the data.\n2.  For each repetition $r$, the dataset of $n$ subjects is partitioned into $K$ disjoint folds. The partitioning is stratified by the outcome variable $Y$. This ensures that each of the $K$ folds has approximately the same proportion of cases and controls as the full dataset, i.e., each fold contains approximately $N_1/K$ cases and $N_0/K$ controls.\n3.  For each fold $k \\in \\{1,\\dots,K\\}$ in repetition $r$, the subjects in that fold are designated as the validation set, while the subjects in the remaining $K-1$ folds form the training set.\n4.  A model is trained on the training set. This model is then used to generate a real-valued score $\\hat f(X_i)$ for each subject $i$ in the corresponding validation set. As per the problem's notation, we denote this out-of-fold score for subject $i$ in repetition $r$ as $\\hat f^{(-i,r)}(X_i)$.\n5.  After completing this process for all $K$ folds in all $R$ repetitions, we obtain a total of $n \\times R$ out-of-fold predictions. For each subject $i$, there are $R$ predictions, $\\{\\hat f^{(-i,1)}(X_i), \\dots, \\hat f^{(-i,R)}(X_i)\\}$, one from each repetition.\n\n**Step 2: Micro-Averaged Performance Metrics**\n\nThe problem specifies micro-averaged aggregation. This means we pool all $n \\times R$ out-of-fold predictions and their true labels before calculating performance metrics. This is equivalent to summing the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) over all predictions.\n\nThe classification rule for a given threshold $\\tau$ is to predict a subject as a case (1) if their score is greater than or equal to $\\tau$.\n- Predicted Positive: $\\hat f^{(-i,r)}(X_i) \\ge \\tau$\n- Predicted Negative: $\\hat f^{(-i,r)}(X_i) < \\tau$\n\nWe can define the total counts for TP, FN, TN, and FP as functions of $\\tau$ by summing over all subjects $i \\in \\{1,\\dots,n\\}$ and all repetitions $r \\in \\{1,\\dots,R\\}$:\n- Total True Positives: $\\text{TP}(\\tau) = \\sum_{r=1}^R \\sum_{i=1}^n \\mathbb{1}\\{Y_i=1 \\text{ and } \\hat f^{(-i,r)}(X_i) \\ge \\tau\\} = \\sum_{r=1}^R \\sum_{i=1}^n \\mathbb{1}\\{Y_i=1\\}\\mathbb{1}\\{\\hat f^{(-i,r)}(X_i) \\ge \\tau\\}$\n- Total True Negatives: $\\text{TN}(\\tau) = \\sum_{r=1}^R \\sum_{i=1}^n \\mathbb{1}\\{Y_i=0 \\text{ and } \\hat f^{(-i,r)}(X_i) < \\tau\\} = \\sum_{r=1}^R \\sum_{i=1}^n \\mathbb{1}\\{Y_i=0\\}\\mathbb{1}\\{\\hat f^{(-i,r)}(X_i) < \\tau\\}$\n\nThe total number of case instances across all repetitions is $R \\cdot N_1 = R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=1\\}$.\nThe total number of control instances across all repetitions is $R \\cdot N_0 = R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=0\\}$.\n\nThe micro-averaged sensitivity, $\\widehat{\\text{Sens}}(\\tau)$, is the ratio of total true positives to total actual positives:\n$$ \\widehat{\\text{Sens}}(\\tau) = \\frac{\\text{TP}(\\tau)}{R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=1\\}} = \\frac{\\sum_{r=1}^R \\sum_{i=1}^n \\mathbb{1}\\{Y_i=1\\}\\mathbb{1}\\{\\hat f^{(-i,r)}(X_i) \\ge \\tau\\}}{R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=1\\}} $$\nThe micro-averaged specificity, $\\widehat{\\text{Spec}}(\\tau)$, is the ratio of total true negatives to total actual negatives:\n$$ \\widehat{\\text{Spec}}(\\tau) = \\frac{\\text{TN}(\\tau)}{R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=0\\}} = \\frac{\\sum_{r=1}^R \\sum_{i=1}^n \\mathbb{1}\\{Y_i=0\\}\\mathbb{1}\\{\\hat f^{(-i,r)}(X_i) < \\tau\\}}{R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=0\\}} $$\n\n**Step 3: Derivation of the Penalized Loss Function**\n\nThe objective is to maximize sensitivity subject to the constraint that specificity is at least $\\alpha$. This is a constrained optimization problem. We can reformulate it as an unconstrained problem by minimizing a penalized loss function. Maximizing sensitivity is equivalent to minimizing negative sensitivity. The constraint is $\\widehat{\\text{Spec}}(\\tau) \\ge \\alpha$. A penalty should be applied when this constraint is violated, i.e., when $\\widehat{\\text{Spec}}(\\tau) < \\alpha$.\n\nThe loss function $\\widehat{L}(\\tau)$ will have the form:\n$ \\widehat{L}(\\tau) = (\\text{Objective Term}) + (\\text{Penalty Term}) $\nThe objective term corresponds to minimizing negative sensitivity: $-\\widehat{\\text{Sens}}(\\tau)$.\nThe penalty term is active when $\\alpha - \\widehat{\\text{Spec}}(\\tau) > 0$. The problem asks for a linear penalty in the shortfall, so the penalty term is $\\lambda \\cdot \\max\\{0, \\alpha - \\widehat{\\text{Spec}}(\\tau)\\}$, where $\\lambda > 0$ is a penalty weight. Using the notation $(x)_+ = \\max\\{x,0\\}$, this is $\\lambda (\\alpha - \\widehat{\\text{Spec}}(\\tau))_+$.\n\nSo, the loss function is:\n$ \\widehat{L}(\\tau) = -\\widehat{\\text{Sens}}(\\tau) + \\lambda (\\alpha - \\widehat{\\text{Spec}}(\\tau))_+ $\nIt is often more convenient to work with the False Positive Rate (FPR), where $\\text{Spec} = 1 - \\text{FPR}$. The constraint $\\widehat{\\text{Spec}}(\\tau) \\ge \\alpha$ is equivalent to $1 - \\widehat{\\text{FPR}}(\\tau) \\ge \\alpha$, or $\\widehat{\\text{FPR}}(\\tau) \\le 1-\\alpha$.\nThe penalty term can then be rewritten in terms of the FPR:\n$ (\\alpha - \\widehat{\\text{Spec}}(\\tau))_+ = (\\alpha - (1 - \\widehat{\\text{FPR}}(\\tau)))_+ = (\\widehat{\\text{FPR}}(\\tau) - (1-\\alpha))_+ $\nThe estimated false positive rate, $\\widehat{\\text{FPR}}(\\tau)$, is the fraction of controls incorrectly classified as cases:\n$$ \\widehat{\\text{FPR}}(\\tau) = \\frac{\\text{FP}(\\tau)}{R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=0\\}} = \\frac{\\sum_{r=1}^R \\sum_{i=1}^n \\mathbb{1}\\{Y_i=0\\}\\mathbb{1}\\{\\hat f^{(-i,r)}(X_i) \\ge \\tau\\}}{R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=0\\}} $$\nThis form is computationally direct as it, like sensitivity, depends on counting scores exceeding the threshold $\\tau$.\n\nSubstituting the expressions for $-\\widehat{\\text{Sens}}(\\tau)$ and the FPR-based penalty term, we arrive at the final expression for the empirical loss function $\\widehat{L}(\\tau)$:\n$$ \\widehat{L}(\\tau) = - \\frac{\\sum_{r=1}^R \\sum_{i=1}^n \\mathbb{1}\\{Y_i=1\\}\\mathbb{1}\\{\\hat f^{(-i,r)}(X_i) \\ge \\tau\\}}{R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=1\\}} + \\lambda \\left( \\frac{\\sum_{r=1}^R \\sum_{i=1}^n \\mathbb{1}\\{Y_i=0\\}\\mathbb{1}\\{\\hat f^{(-i,r)}(X_i) \\ge \\tau\\}}{R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=0\\}} - (1-\\alpha) \\right)_{+} $$\nThis expression fulfills all requirements of the problem statement. Minimizing this loss function with respect to $\\tau$ will yield an optimal threshold that balances the primary goal of maximizing sensitivity with the clinical constraint on specificity. The parameter $\\lambda$ controls the hardness of the constraint: as $\\lambda \\to \\infty$, any violation of the specificity constraint becomes infinitely costly, forcing the solution to satisfy $\\widehat{\\text{Spec}}(\\tau) \\ge \\alpha$.",
            "answer": "$$\\boxed{- \\frac{\\sum_{r=1}^R \\sum_{i=1}^n \\mathbb{1}\\{Y_i=1\\} \\mathbb{1}\\{\\hat f^{(-i,r)}(X_i) \\ge \\tau\\}}{R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=1\\}} + \\lambda \\left( \\frac{\\sum_{r=1}^R \\sum_{i=1}^n \\mathbb{1}\\{Y_i=0\\} \\mathbb{1}\\{\\hat f^{(-i,r)}(X_i) \\ge \\tau\\}}{R \\sum_{j=1}^n \\mathbb{1}\\{Y_j=0\\}} - (1-\\alpha) \\right)_{+}}$$"
        },
        {
            "introduction": "The validity of a cross-validation estimate hinges on the strict separation of data used for training from data used for evaluation. This final practice explores one of the most common and insidious errors in machine learning pipelines: \"data leakage,\" where information from the validation set inadvertently contaminates the training process . By analytically deriving the bias that results from performing feature selection on the full dataset before cross-validation, you will quantify how this mistake leads to overly optimistic performance estimates and understand why *all* data-dependent modeling steps must be encapsulated within the CV loop.",
            "id": "4897575",
            "problem": "A biostatistics team is evaluating a linear predictor to forecast a standardized quantitative phenotype. For each subject $i \\in \\{1,\\dots,n\\}$ and each biomarker $j \\in \\{1,\\dots,p\\}$, assume the following data-generating process under a null model with no true associations:\n- $y_i \\sim \\mathcal{N}(0,1)$ independently across $i$.\n- $x_{ij} \\sim \\mathcal{N}(0,1)$ independently across $i$ and $j$, and independent of $y_i$.\n\nThey mistakenly perform feature selection using the full dataset before model evaluation as follows.\n- Step $1$ (leaky feature selection on full data): compute the sample correlation $r_j = \\frac{1}{n}\\sum_{i=1}^{n} x_{ij} y_i$ for each biomarker $j$ on the full dataset, and select the single biomarker $j^{\\star}$ with the largest absolute correlation, that is $j^{\\star} \\in \\arg\\max_{1 \\leq j \\leq p} |r_j|$.\n- Step $2$ (evaluation by cross-validation reusing the selected feature): estimate the mean squared prediction error by Leave-One-Out Cross-Validation (LOOCV), refitting ordinary least squares without an intercept on the training set of size $n-1$ in each fold to obtain a coefficient $\\hat{\\beta}_{(-i)}$ for the fixed biomarker $x_{ij^{\\star}}$, and predicting $\\hat{y}_i = \\hat{\\beta}_{(-i)} x_{ij^{\\star}}$ for the left-out subject $i$.\n\nLet $\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}$ denote the resulting LOOCV estimate of mean squared error produced by the above leakage-prone procedure, and let $\\mathrm{MSE}_{\\mathrm{test}}$ denote the true expected mean squared prediction error on an independent new subject when the procedure is trained on $n$ observations and then applied to the new subject (still using the feature $j^{\\star}$ selected on the full $n$ training observations as above). Define the bias\n$$\nB(n,p) \\equiv \\mathbb{E}\\!\\left[\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}\\right] \\;-\\; \\mathbb{E}\\!\\left[\\mathrm{MSE}_{\\mathrm{test}}\\right].\n$$\n\nAssume $n$ is large, $p$ may grow with $n$ with $\\ln p = o(n)$, and ignore all terms that are lower order than $\\frac{\\ln p}{n}$. Starting from fundamental definitions and standard large-sample approximations, derive the leading-order asymptotic expression for $B(n,p)$ as a function of $n$ and $p$ under the null model described. Provide your answer as a single closed-form analytic expression. No rounding is required and no units are needed.",
            "solution": "The user wants to find the leading-order asymptotic expression for the bias $B(n,p) = \\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}] - \\mathbb{E}[\\mathrm{MSE}_{\\text{test}}]$. We will derive expressions for the two expectations separately and then compute their difference.\n\n### Step 1: Preliminary Definitions and Approximations\n\nLet $X$ be the $n \\times p$ matrix of biomarker data, and $y$ be the $n \\times 1$ vector of phenotype data. The elements $x_{ij}$ and $y_i$ are i.i.d. $\\mathcal{N}(0,1)$.\nThe sample correlation for biomarker $j$ is $r_j = \\frac{1}{n}\\sum_{i=1}^{n} x_{ij} y_i$. Let $C_j = \\sum_{i=1}^{n} x_{ij} y_i = n r_j$.\nThe sum of squares for predictor $j$ is $S_j = \\sum_{i=1}^{n} x_{ij}^2$. By the law of large numbers, for large $n$, $S_j/n \\to \\mathbb{E}[x_{ij}^2] = 1$. We will use the approximation $S_j \\approx n$. This holds for all $j$, including the selected one $j^{\\star}$.\nThe feature selection rule is $j^{\\star} = \\arg\\max_{1 \\leq j \\leq p} |r_j| = \\arg\\max_{1 \\leq j \\leq p} |C_j|$.\n\nTo analyze the selection process, we define normalized variables $\\tilde{r}_j = C_j / \\sqrt{S_j}$. Conditional on $X$, each $\\tilde{r}_j$ is a linear combination of i.i.d. standard normal variables $y_i$: $\\tilde{r}_j = \\sum_i (x_{ij}/\\sqrt{S_j}) y_i$. Thus, $\\mathbb{E}[\\tilde{r}_j|X] = 0$ and $\\mathrm{Var}(\\tilde{r}_j|X) = \\sum_i (x_{ij}/\\sqrt{S_j})^2 \\mathrm{Var}(y_i) = S_j/S_j = 1$. So, $\\tilde{r}_j | X \\sim \\mathcal{N}(0,1)$.\nThe covariance is $\\mathrm{Cov}(\\tilde{r}_j, \\tilde{r}_k|X) = \\frac{\\sum_i x_{ij}x_{ik}}{\\sqrt{S_j S_k}}$, which for large $n$ converges to $0$ for $j \\neq k$. Therefore, for large $n$, the variables $\\{\\tilde{r}_j\\}_{j=1}^p$ can be treated as i.i.d. $\\mathcal{N}(0,1)$ random variables.\n\nThe selection rule $j^{\\star} = \\arg\\max_j |C_j| = \\arg\\max_j |\\sqrt{S_j} \\tilde{r}_j|$ is approximately equivalent to $j^{\\star} \\approx \\arg\\max_j |\\tilde{r}_j|$ since $S_j \\approx n$ for all $j$. Let $M_p^2 = \\max_{1 \\leq j \\leq p} \\tilde{r}_j^2$. For large $p$, the expectation of the maximum of $p$ i.i.d. $\\chi^2(1)$ variables is well-approximated by $\\mathbb{E}[M_p^2] \\approx 2 \\ln p$.\n\n### Step 2: Derivation of $\\mathbb{E}[\\mathrm{MSE}_{\\text{test}}]$\n\nThe true test error, $\\mathrm{MSE}_{\\text{test}}$, is the expected squared prediction error for a new subject $(x_{\\text{new}}, y_{\\text{new}})$ drawn from the same distribution, independent of the training data. The prediction is $\\hat{y}_{\\text{new}} = \\hat{\\beta} x_{\\text{new}, j^{\\star}}$, where $\\hat{\\beta}$ is the OLS coefficient estimated from the full training set of size $n$.\n$$\n\\mathrm{MSE}_{\\text{test}} = \\mathbb{E}_{\\text{new}}[(y_{\\text{new}} - \\hat{\\beta} x_{\\text{new}, j^{\\star}})^2 | X, y]\n$$\nExpanding this and using the independence of the new subject ($\\mathbb{E}[y_{\\text{new}}] = 0$, $\\mathbb{E}[y_{\\text{new}}^2]=1$, $\\mathbb{E}[x_{\\text{new}, j^{\\star}}^2]=1$):\n$$\n\\mathrm{MSE}_{\\text{test}} = \\mathbb{E}_{\\text{new}}[y_{\\text{new}}^2] - 2\\hat{\\beta}\\mathbb{E}_{\\text{new}}[y_{\\text{new}}x_{\\text{new}, j^{\\star}}] + \\hat{\\beta}^2\\mathbb{E}_{\\text{new}}[x_{\\text{new}, j^{\\star}}^2] = 1 - 0 + \\hat{\\beta}^2 = 1 + \\hat{\\beta}^2\n$$\nThe OLS estimate without an intercept is $\\hat{\\beta} = \\frac{\\sum_i x_{ij^{\\star}}y_i}{\\sum_i x_{ij^{\\star}}^2} = \\frac{C_{j^{\\star}}}{S_{j^{\\star}}}$.\nThus, $\\mathrm{MSE}_{\\text{test}} = 1 + \\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}^2}$.\nWe need to compute the expectation over the training data distribution:\n$$\n\\mathbb{E}[\\mathrm{MSE}_{\\text{test}}] = 1 + \\mathbb{E}\\left[\\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}^2}\\right]\n$$\nUsing the approximation $S_{j^{\\star}} \\approx n$:\n$$\n\\mathbb{E}[\\mathrm{MSE}_{\\text{test}}] \\approx 1 + \\frac{1}{n^2}\\mathbb{E}[C_{j^{\\star}}^2]\n$$\nWe have $C_{j^{\\star}} = \\sqrt{S_{j^{\\star}}} \\tilde{r}_{j^{\\star}}$, where $|\\tilde{r}_{j^{\\star}}|^2 \\approx M_p^2$.\nSo, $\\mathbb{E}[C_{j^{\\star}}^2] = \\mathbb{E}[S_{j^{\\star}} \\tilde{r}_{j^{\\star}}^2] \\approx \\mathbb{E}[S_{j^{\\star}}] \\mathbb{E}[\\tilde{r}_{j^{\\star}}^2] \\approx n \\mathbb{E}[M_p^2]$.\nUsing $\\mathbb{E}[M_p^2] \\approx 2 \\ln p$, we get $\\mathbb{E}[C_{j^{\\star}}^2] \\approx 2n \\ln p$.\nSubstituting this into the expression for the expected test error:\n$$\n\\mathbb{E}[\\mathrm{MSE}_{\\text{test}}] \\approx 1 + \\frac{1}{n^2}(2n \\ln p) = 1 + \\frac{2 \\ln p}{n}\n$$\n\n### Step 3: Derivation of $\\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}]$\n\nThe LOOCV error estimate is $\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$, where $\\hat{y}_i = \\hat{\\beta}_{(-i)} x_{ij^{\\star}}$.\nA standard result for OLS relates the LOOCV residual to the residual from the full model fit. For a no-intercept model, this identity is:\n$$\ny_i - \\hat{y}_i = \\frac{y_i - \\hat{\\beta} x_{ij^{\\star}}}{1 - h_{ii}}\n$$\nwhere $h_{ii}$ is the $i$-th diagonal element of the hat matrix $H = x_{j^{\\star}}(x_{j^{\\star}}^Tx_{j^{\\star}})^{-1}x_{j^{\\star}}^T$.\n$h_{ii} = \\frac{x_{ij^{\\star}}^2}{\\sum_{k=1}^n x_{kj^{\\star}}^2} = \\frac{x_{ij^{\\star}}^2}{S_{j^{\\star}}}$.\nFor large $n$, $S_{j^{\\star}} \\approx n$, so $h_{ii} = O_p(1/n)$.\nThe LOOCV estimate can be written as:\n$$\n\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{(y_i - \\hat{\\beta}x_{ij^{\\star}})^2}{(1-h_{ii})^2}\n$$\nLet $\\widehat{\\mathrm{MSE}}_{\\text{in}} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{\\beta}x_{ij^{\\star}})^2$ be the in-sample (training) MSE. Using the expansion $(1-h_{ii})^{-2} = 1 + 2h_{ii} + O_p(h_{ii}^2)$:\n$$\n\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}} \\approx \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\beta}x_{ij^{\\star}})^2(1+2h_{ii}) = \\widehat{\\mathrm{MSE}}_{\\text{in}} + \\frac{2}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\beta}x_{ij^{\\star}})^2 h_{ii}\n$$\nFirst, we compute the expectation of the in-sample MSE:\n$$\n\\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\text{in}}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_i(y_i - \\hat{\\beta}x_{ij^{\\star}})^2\\right] = \\frac{1}{n}\\mathbb{E}\\left[\\sum_i y_i^2 - \\hat{\\beta}^2 S_{j^{\\star}}\\right]\n$$\nThis uses the orthogonality property of OLS residuals. $\\mathbb{E}[\\sum y_i^2]=n$.\n$\\hat{\\beta}^2 S_{j^{\\star}} = (\\frac{C_{j^{\\star}}}{S_{j^{\\star}}})^2 S_{j^{\\star}} = \\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}}$.\n$\\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\text{in}}] = \\frac{1}{n}\\left(n - \\mathbb{E}\\left[\\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}}\\right]\\right) = 1 - \\frac{1}{n}\\mathbb{E}\\left[\\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}}\\right]$.\nAs established before, $\\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}} \\approx M_p^2$, so $\\mathbb{E}[\\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}}] \\approx 2 \\ln p$.\nTherefore, the expected in-sample error is highly optimistic:\n$$\n\\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\text{in}}] \\approx 1 - \\frac{2 \\ln p}{n}\n$$\nNext, consider the correction term $\\frac{2}{n}\\mathbb{E}[\\sum_i (y_i - \\hat{\\beta}x_{ij^{\\star}})^2 h_{ii}]$.\nBy symmetry, this is $2\\mathbb{E}[(y_1 - \\hat{\\beta}x_{1j^{\\star}})^2 h_{11}]$.\n$h_{11} = x_{1j^{\\star}}^2/S_{j^{\\star}} \\approx x_{1j^{\\star}}^2/n$. The term is $\\frac{2}{n}\\mathbb{E}[(y_1 - \\hat{\\beta}x_{1j^{\\star}})^2 x_{1j^{\\star}}^2]$.\nFor large $n$, $\\hat{\\beta} = O_p(\\sqrt{\\ln p / n})$ which goes to $0$. Thus, $y_1 - \\hat{\\beta}x_{1j^{\\star}} \\approx y_1$. The expectation becomes $\\frac{2}{n}\\mathbb{E}[y_1^2 x_{1j^{\\star}}^2]$. Since $y_1$ and $x_{1j^{\\star}}$ are independent standard normals (selection of $j^{\\star}$ is a low-probability event for any single row), $\\mathbb{E}[y_1^2 x_{1j^{\\star}}^2] = \\mathbb{E}[y_1^2]\\mathbb{E}[x_{1j^{\\star}}^2]=1$.\nSo the correction term is approximately $2/n$.\nCombining the terms for the expected LOOCV error:\n$$\n\\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}] \\approx \\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\text{in}}] + \\frac{2}{n} \\approx \\left(1 - \\frac{2 \\ln p}{n}\\right) + \\frac{2}{n}\n$$\n\n### Step 4: Calculation of the Bias $B(n,p)$\n\nThe bias is the difference between the two derived expectations.\n$$\nB(n,p) = \\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}] - \\mathbb{E}[\\mathrm{MSE}_{\\text{test}}]\n$$\nSubstituting the derived expressions:\n$$\nB(n,p) \\approx \\left(1 - \\frac{2 \\ln p}{n} + \\frac{2}{n}\\right) - \\left(1 + \\frac{2 \\ln p}{n}\\right)\n$$\n$$\nB(n,p) \\approx -\\frac{4 \\ln p}{n} + \\frac{2}{n}\n$$\nThe problem asks for the leading-order asymptotic expression and to ignore terms of lower order than $\\frac{\\ln p}{n}$. Assuming $p$ is large enough or grows with $n$ such that $\\ln p \\to \\infty$, the term $\\frac{2}{n}$ is of lower order than $\\frac{4 \\ln p}{n}$. We therefore drop it.\nThe leading-order asymptotic expression for the bias is:\n$$\nB(n,p) \\approx -\\frac{4 \\ln p}{n}\n$$\nThis negative bias indicates that the leaky cross-validation procedure is substantially optimistic, underestimating the true prediction error due to the feature selection being performed on the full dataset before cross-validation.",
            "answer": "$$\n\\boxed{-\\frac{4 \\ln(p)}{n}}\n$$"
        }
    ]
}