{
    "hands_on_practices": [
        {
            "introduction": "Understanding multicollinearity begins with its primary diagnostic tool: the Variance Inflation Factor (VIF). This exercise provides a direct application of the VIF formula, connecting it to the coefficient of determination ($R_j^2$) from an auxiliary regression. By working through this fundamental calculation , you will solidify your understanding of how VIF precisely quantifies the degree to which a predictor's variance is inflated due to its linear relationship with other predictors in a model.",
            "id": "1938245",
            "problem": "In the context of multiple linear regression, multicollinearity is a phenomenon where one predictor variable in a model can be linearly predicted from the others with a substantial degree of accuracy. A key metric used to quantify the severity of multicollinearity is the Variance Inflation Factor (VIF).\n\nAn analyst is studying the factors that influence the price of a certain commodity. They build a multiple linear regression model with several predictor variables. To assess multicollinearity, they focus on a specific predictor, $X_j$. The analyst performs an auxiliary regression where $X_j$ is treated as the response variable and all other predictor variables in the original model are used as its predictors. The coefficient of determination, $R_j^2$, from this auxiliary regression is found to be $0.96$.\n\nBased on this information, calculate the Variance Inflation Factor for the regression coefficient $\\hat{\\beta}_j$ associated with the predictor $X_j$. Provide your answer as a single number.",
            "solution": "In multiple linear regression, the Variance Inflation Factor for coefficient $\\hat{\\beta}_{j}$ associated with predictor $X_{j}$ is defined by\n$$\n\\text{VIF}_{j}=\\frac{1}{1-R_{j}^{2}},\n$$\nwhere $R_{j}^{2}$ is the coefficient of determination from the auxiliary regression of $X_{j}$ on all the other predictors. This follows from the variance formula\n$$\n\\operatorname{Var}(\\hat{\\beta}_{j})=\\frac{\\sigma^{2}}{S_{X_{j}}^{2}\\left(1-R_{j}^{2}\\right)},\n$$\nso the inflation relative to the case with no collinearity among predictors is the factor $\\left(1-R_{j}^{2}\\right)^{-1}$.\n\nGiven $R_{j}^{2}=0.96$, compute the denominator:\n$$\n1-R_{j}^{2}=1-0.96=0.04.\n$$\nTherefore,\n$$\n\\text{VIF}_{j}=\\frac{1}{0.04}=25.\n$$",
            "answer": "$$\\boxed{25}$$"
        },
        {
            "introduction": "A high VIF score is more than just a number; it is a warning that your model's coefficients may be unreliable and even counter-intuitive. This practice problem explores a classic consequence of severe multicollinearity: a predictor's coefficient taking on a sign opposite to what domain knowledge and simple correlations would suggest. By calculating the coefficient under these conditions , you will gain first-hand insight into how collinearity can distort a model's interpretation and obscure the true relationship between predictors and the outcome.",
            "id": "1938238",
            "problem": "An agricultural scientist is studying the yield of a new variety of corn. They conduct an experiment with varying levels of two different liquid fertilizers, \"Gro-Fast\" (applied in mL per plant) and \"Yield-Max\" (applied in mL per plant). The scientist proposes a multiple linear regression model for the corn yield (in kg per plant), $Y$, based on the amounts of the two fertilizers applied, $X_1$ for Gro-Fast and $X_2$ for Yield-Max:\n$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon$$\nBased on prior knowledge of crop science, both fertilizers are expected to have a positive impact on yield. However, the two fertilizers have similar chemical compositions, and the experimental design resulted in a high positive correlation between the amounts of $X_1$ and $X_2$ applied.\n\nAfter the experiment, the following summary statistics are calculated from the collected data:\n- Sample correlation between yield ($Y$) and Gro-Fast ($X_1$): $r_{y1} = 0.80$\n- Sample correlation between yield ($Y$) and Yield-Max ($X_2$): $r_{y2} = 0.90$\n- Sample correlation between Gro-Fast ($X_1$) and Yield-Max ($X_2$): $r_{12} = 0.95$\n\n- Sample standard deviation of yield ($Y$): $s_y = 1.5$ kg\n- Sample standard deviation of Gro-Fast ($X_1$): $s_1 = 10.0$ mL\n- Sample standard deviation of Yield-Max ($X_2$): $s_2 = 12.0$ mL\n\nDespite the strong positive simple correlation between Gro-Fast and yield ($r_{y1} = 0.80$), the multiple regression analysis surprisingly produces a negative estimated coefficient for Gro-Fast, $\\hat{\\beta}_1$. Calculate the value of $\\hat{\\beta}_1$. Express your answer in kg/mL, rounded to three significant figures.",
            "solution": "We start from the multiple linear regression of $Y$ on $X_{1}$ and $X_{2}$. Let $Z_{y}=(Y-\\bar{Y})/s_{y}$, $Z_{1}=(X_{1}-\\bar{X}_{1})/s_{1}$, and $Z_{2}=(X_{2}-\\bar{X}_{2})/s_{2}$ be standardized variables. The standardized regression takes the form\n$$\nZ_{y}=b_{1}^{\\ast}Z_{1}+b_{2}^{\\ast}Z_{2}+\\varepsilon,\n$$\nwhere the standardized coefficients satisfy\n$$\n\\begin{pmatrix}\nb_{1}^{\\ast}\\\\\nb_{2}^{\\ast}\n\\end{pmatrix}\n=R^{-1}\n\\begin{pmatrix}\nr_{y1}\\\\\nr_{y2}\n\\end{pmatrix},\n\\quad\nR=\n\\begin{pmatrix}\n1 & r_{12}\\\\\nr_{12} & 1\n\\end{pmatrix}.\n$$\nThe inverse of the $2\\times 2$ correlation matrix is\n$$\nR^{-1}=\\frac{1}{1-r_{12}^{2}}\n\\begin{pmatrix}\n1 & -r_{12}\\\\\n-r_{12} & 1\n\\end{pmatrix}.\n$$\nTherefore,\n$$\nb_{1}^{\\ast}=\\frac{r_{y1}-r_{12}r_{y2}}{1-r_{12}^{2}},\n\\qquad\nb_{2}^{\\ast}=\\frac{r_{y2}-r_{12}r_{y1}}{1-r_{12}^{2}}.\n$$\nThe unstandardized coefficient for $X_{1}$ is obtained by rescaling:\n$$\n\\hat{\\beta}_{1}=b_{1}^{\\ast}\\frac{s_{y}}{s_{1}}=\\frac{r_{y1}-r_{12}r_{y2}}{1-r_{12}^{2}}\\cdot\\frac{s_{y}}{s_{1}}.\n$$\nSubstituting the given values $r_{y1}=0.80$, $r_{y2}=0.90$, $r_{12}=0.95$, $s_{y}=1.5$, and $s_{1}=10.0$, we compute\n$$\nr_{y1}-r_{12}r_{y2}=0.80-0.95\\cdot 0.90=0.80-0.855=-0.055,\n$$\n$$\n1-r_{12}^{2}=1-0.95^{2}=1-0.9025=0.0975,\n$$\nso\n$$\nb_{1}^{\\ast}=\\frac{-0.055}{0.0975}=-\\frac{22}{39}\\approx -0.56410256.\n$$\nThen\n$$\n\\hat{\\beta}_{1}=b_{1}^{\\ast}\\frac{s_{y}}{s_{1}}=-\\frac{22}{39}\\cdot\\frac{1.5}{10}=-\\frac{22}{39}\\cdot 0.15\\approx -0.08461538.\n$$\nRounding to three significant figures gives\n$$\n\\hat{\\beta}_{1}\\approx -0.0846.\n$$\nThis negative coefficient arises despite the positive simple correlation due to the strong positive collinearity between $X_{1}$ and $X_{2}$, which shifts the partial effect estimate.",
            "answer": "$$\\boxed{-0.0846}$$"
        },
        {
            "introduction": "While diagnosing multicollinearity is critical, an equally important skill is knowing how to mitigate it. This hands-on practice addresses \"structural\" multicollinearity, a common issue in polynomial regression, and introduces centering as an effective solution. You will calculate VIFs for a quadratic model both before and after centering the predictor variable , providing a clear demonstration of how this simple data transformation can resolve collinearity and improve model stability without altering its predictive capacity.",
            "id": "4929491",
            "problem": "Consider a quadratic polynomial regression in a biostatistics study of a continuous covariate $X$ on an outcome $Y$, specified as $Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} X^{2} + \\varepsilon$, where $\\varepsilon$ is a mean-zero error term independent of $X$. Investigators are concerned about multicollinearity between $X$ and $X^{2}$ and wish to quantify it via the Variance Inflation Factor (VIF).\n\nYou are given a simple, scientifically plausible covariate sample of size $n=7$: $X_{i} \\in \\{1, 2, 3, 4, 5, 6, 7\\}$ for $i=1,\\dots,7$. Using only the fundamental definitions of least squares regression and the coefficient of determination $R^{2}$ (without relying on any shortcut formulas not derived from these definitions), compute the VIFs for the predictors $X$ and $X^{2}$ in the following two design specifications:\n\n- Before centering: the design matrix has columns corresponding to the intercept, $X$, and $X^{2}$.\n- After centering: define $Z = X - \\bar{X}$, where $\\bar{X}$ is the sample mean of $X$. The design matrix has columns corresponding to the intercept, $Z$, and $Z^{2}$.\n\nIn each specification, treat multicollinearity according to its foundational definition via the regression of each predictor on the remaining predictor(s) and the intercept to obtain the relevant $R_{j}^{2}$ for the predictor indexed by $j$, and then compute the corresponding VIF for $X$ and for $X^{2}$ (or for $Z$ and $Z^{2}$ after centering). Finally, explain the observed differences in $R_{j}^{2}$ between the uncentered and centered specifications based on properties of moments of $X$.\n\nExpress your final answer as four numbers in a single row matrix in the order: VIF for $X$ before centering, VIF for $X^{2}$ before centering, VIF for $Z$ after centering, VIF for $Z^{2}$ after centering. No rounding is required.",
            "solution": "The problem asks for the calculation of Variance Inflation Factors (VIFs) for the predictors in a quadratic regression model, $Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} X^{2} + \\varepsilon$, both before and after centering the covariate $X$. The VIF for a predictor $j$, denoted $VIF_j$, is a measure of multicollinearity and is defined by the formula:\n$$VIF_j = \\frac{1}{1 - R_j^2}$$\nHere, $R_j^2$ is the coefficient of determination from an auxiliary ordinary least squares (OLS) regression of the predictor $X_j$ on all other predictors in the model, including the intercept. For a simple linear regression of a variable $U$ on a variable $V$, $U = \\gamma_0 + \\gamma_1 V + \\text{error}$, the coefficient of determination $R^2$ is given by the square of the Pearson correlation coefficient between $U$ and $V$. It can be calculated fundamentally as:\n$$R^2 = \\frac{\\left( \\sum_{i=1}^n (U_i - \\bar{U})(V_i - \\bar{V}) \\right)^2}{\\left( \\sum_{i=1}^n (U_i - \\bar{U})^2 \\right) \\left( \\sum_{i=1}^n (V_i - \\bar{V})^2 \\right)}$$\nWe are given a sample of size $n=7$ for the covariate $X$: $X_i \\in \\{1, 2, 3, 4, 5, 6, 7\\}$.\n\nFirst, we compile the necessary sample sums and moments for the given data.\nThe values for $X$ are $\\{1, 2, 3, 4, 5, 6, 7\\}$.\n$\\sum_{i=1}^7 X_i = 1+2+3+4+5+6+7 = 28$.\nThe sample mean of $X$ is $\\bar{X} = \\frac{28}{7} = 4$.\nThe values for $X^2$ are $\\{1^2, 2^2, \\dots, 7^2\\} = \\{1, 4, 9, 16, 25, 36, 49\\}$.\n$\\sum_{i=1}^7 X_i^2 = 1+4+9+16+25+36+49 = 140$.\nThe sample mean of $X^2$ is $\\overline{X^2} = \\frac{140}{7} = 20$.\nWe will also need higher moments for cross-product calculations:\n$\\sum_{i=1}^7 X_i^3 = 1^3+2^3+\\dots+7^3 = 1+8+27+64+125+216+343 = 784$.\n$\\sum_{i=1}^7 X_i^4 = 1^4+2^4+\\dots+7^4 = 1+16+81+256+625+1296+2401 = 4676$.\n\n**Part 1: VIFs Before Centering**\n\nThe predictors are the intercept, $P_1 = X$, and $P_2 = X^2$.\n\nTo find the VIF for $X$, we must compute $R_X^2$ from the regression of $X$ on $X^2$ and an intercept.\nLet $U=X$ and $V=X^2$. We calculate the terms for the $R^2$ formula:\nThe total sum of squares for $X$ is:\n$$SS_{tot}(X) = \\sum_{i=1}^7 (X_i - \\bar{X})^2 = \\sum X_i^2 - n\\bar{X}^2 = 140 - 7(4^2) = 140 - 112 = 28$$\nThe total sum of squares for $X^2$ is:\n$$SS_{tot}(X^2) = \\sum_{i=1}^7 (X_i^2 - \\overline{X^2})^2 = \\sum X_i^4 - n(\\overline{X^2})^2 = 4676 - 7(20^2) = 4676 - 2800 = 1876$$\nThe sum of cross-products between $X$ and $X^2$ is:\n$$S_{X,X^2} = \\sum_{i=1}^7 (X_i - \\bar{X})(X_i^2 - \\overline{X^2}) = \\sum X_i^3 - n\\bar{X}\\overline{X^2} = 784 - 7(4)(20) = 784 - 560 = 224$$\nNow, we compute $R_X^2$:\n$$R_X^2 = \\frac{S_{X,X^2}^2}{SS_{tot}(X) \\cdot SS_{tot}(X^2)} = \\frac{224^2}{28 \\cdot 1876} = \\frac{50176}{52528} = \\frac{448}{469}$$\nThe VIF for $X$ is therefore:\n$$VIF_X = \\frac{1}{1 - R_X^2} = \\frac{1}{1 - \\frac{448}{469}} = \\frac{1}{\\frac{469 - 448}{469}} = \\frac{469}{21}$$\nTo find the VIF for $X^2$, we need $R_{X^2}^2$ from the regression of $X^2$ on $X$ and an intercept. Because the coefficient of determination $R^2$ in a simple linear regression is symmetric with respect to the two variables, $R_{X^2}^2 = R_X^2 = \\frac{448}{469}$.\nThus, the VIF for $X^2$ is identical to the VIF for $X$:\n$$VIF_{X^2} = \\frac{1}{1 - R_{X^2}^2} = \\frac{469}{21}$$\n\n**Part 2: VIFs After Centering**\n\nWe define the centered variable $Z = X - \\bar{X} = X - 4$. The predictors are now the intercept, $P'_1 = Z$, and $P'_2 = Z^2$.\nThe values for $Z$ are $\\{1-4, 2-4, \\dots, 7-4\\} = \\{-3, -2, -1, 0, 1, 2, 3\\}$.\nThe sample mean of $Z$ is $\\bar{Z} = \\frac{1}{7}\\sum Z_i = \\frac{0}{7} = 0$.\nThe values for $Z^2$ are $\\{(-3)^2, (-2)^2, \\dots, 3^2\\} = \\{9, 4, 1, 0, 1, 4, 9\\}$.\n$\\sum_{i=1}^7 Z_i^2 = 9+4+1+0+1+4+9 = 28$.\nThe sample mean of $Z^2$ is $\\overline{Z^2} = \\frac{28}{7} = 4$.\n\nTo find the VIF for $Z$, we compute $R_Z^2$ from the regression of $Z$ on $Z^2$ and an intercept.\nLet $U=Z$ and $V=Z^2$. We calculate the sum of cross-products $S_{Z,Z^2}$:\n$$S_{Z,Z^2} = \\sum_{i=1}^7 (Z_i - \\bar{Z})(Z_i^2 - \\overline{Z^2})$$\nSince $\\bar{Z}=0$, this simplifies to:\n$$S_{Z,Z^2} = \\sum_{i=1}^7 Z_i(Z_i^2 - \\overline{Z^2}) = \\sum_{i=1}^7 Z_i^3 - \\overline{Z^2} \\sum_{i=1}^7 Z_i$$\nWe know $\\sum Z_i = 0$. We calculate $\\sum Z_i^3$:\n$\\sum_{i=1}^7 Z_i^3 = (-3)^3 + (-2)^3 + (-1)^3 + 0^3 + 1^3 + 2^3 + 3^3 = -27 - 8 - 1 + 0 + 1 + 8 + 27 = 0$.\nTherefore, the sum of cross-products is:\n$$S_{Z,Z^2} = 0 - 4(0) = 0$$\nSince the numerator of the $R^2$ formula is $S_{Z,Z^2}^2 = 0^2 = 0$, it immediately follows that $R_Z^2 = 0$.\nThe VIF for $Z$ is:\n$$VIF_Z = \\frac{1}{1 - R_Z^2} = \\frac{1}{1 - 0} = 1$$\nBy the same symmetry argument as in the uncentered case, $R_{Z^2}^2 = R_Z^2 = 0$. Thus, the VIF for $Z^2$ is:\n$$VIF_{Z^2} = \\frac{1}{1 - R_{Z^2}^2} = 1$$\n\n**Part 3: Explanation of Differences**\n\nThe substantial difference in VIFs between the uncentered and centered specifications arises from the sample correlation between a variable and its square, which is fundamentally tied to the symmetry properties of the variable's sample distribution.\n\nIn the **uncentered case**, the predictors are $X$ and $X^2$. The high VIFs ($\\frac{469}{21} \\approx 22.3$) indicate strong multicollinearity. This collinearity is quantified by the non-zero sample correlation between $X$ and $X^2$. The correlation is non-zero because the sum of cross-products $S_{X,X^2} = \\sum (X_i-\\bar{X})(X_i^2-\\overline{X^2})$ is non-zero. The term $(X_i-\\bar{X})$ represents a variable that is symmetric about zero, but the term $(X_i^2-\\overline{X^2})$ is not symmetric. The product of these terms summed over the sample does not cancel to zero, resulting in a non-zero covariance and thus a high correlation.\n\nIn the **centered case**, the predictors are $Z = X-\\bar{X}$ and $Z^2$. The VIFs are both $1$, the minimum possible value, indicating a complete absence of multicollinearity between $Z$ and $Z^2$. This is because the sample correlation between $Z$ and $Z^2$ is exactly zero. The sum of cross-products, $S_{Z,Z^2} = \\sum (Z_i-\\bar{Z})(Z_i^2-\\overline{Z^2})$, is zero. As shown in the calculation, with $\\bar{Z}=0$, this sum simplifies to $\\sum Z_i^3$. The sample data for $Z$, $\\{-3, -2, -1, 0, 1, 2, 3\\}$, is perfectly symmetric about its mean of $0$. A fundamental property of any data distribution that is symmetric about zero is that all its odd-ordered sample moments are zero. Thus, the third sample moment $\\sum Z_i^3$ is zero. This zero covariance leads to a zero correlation and $R^2=0$, which in turn yields $VIF=1$.\n\nIn summary, centering the covariate $X$ creates a new variable $Z$ whose sample distribution is symmetric about zero. This specific symmetry property makes $Z$ and $Z^2$ uncorrelated (orthogonal in the regression context, given an intercept), thereby resolving the multicollinearity problem that exists between the original uncentered predictor $X$ and its square $X^2$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{469}{21} & \\frac{469}{21} & 1 & 1\n\\end{pmatrix}\n}\n$$"
        }
    ]
}