{
    "hands_on_practices": [
        {
            "introduction": "The foundation of regression is the principle of least squares, where we find the line that minimizes the sum of squared errors. This practice extends that core idea to the more general case of Weighted Least Squares (WLS), a powerful technique used when observations have differing levels of precision. By working through this problem , you will derive the estimators from first principles, solidifying your understanding of how regression models can be adapted to handle real-world data complexities like heteroscedasticity.",
            "id": "4916039",
            "problem": "A biostatistician is modeling the relationship between a biomarker concentration $x$ and a continuous clinical outcome $y$ across two independent patients, where measurement errors are heteroscedastic due to differing assay precision. Under the simple linear regression model with intercept, the outcome is modeled as $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$ for $i=1,2$, with independent errors satisfying $\\mathbb{E}[\\varepsilon_{i} \\mid x_{i}] = 0$ and $\\operatorname{Var}(\\varepsilon_{i} \\mid x_{i}) \\propto 1/w_{i}$, where $w_{i} > 0$ is a known weight proportional to the inverse variance of $\\varepsilon_{i}$. The Weighted Least Squares (WLS) estimator is defined as the minimizer of the weighted residual sum of squares $\\sum_{i=1}^{2} w_{i} \\left(y_{i} - \\beta_{0} - \\beta_{1} x_{i}\\right)^{2}$.\n\nYou are given the data vectors $w=(1,4)$, $x=(0,1)$, and $y=(1,3)$. Starting only from the WLS definition above and general properties of least squares minimization, compute the WLS estimates of the intercept $\\beta_{0}$ and slope $\\beta_{1}$ for this model. Report your final answer as the ordered pair $(\\beta_{0}, \\beta_{1})$. No rounding is required, and no units should be included in the final answer.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It provides a complete and consistent setup for a standard biostatistical calculation.\n\nThe objective is to find the Weighted Least Squares (WLS) estimates of the intercept $\\beta_{0}$ and the slope $\\beta_{1}$ for the simple linear regression model $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$. The WLS estimates are the values of $\\beta_{0}$ and $\\beta_{1}$ that minimize the weighted residual sum of squares function, $S(\\beta_{0}, \\beta_{1})$.\n\nThe weighted residual sum of squares is defined as:\n$$S(\\beta_{0}, \\beta_{1}) = \\sum_{i=1}^{2} w_{i} (y_{i} - \\beta_{0} - \\beta_{1} x_{i})^{2}$$\n\nThe problem provides the following data for two patients ($i=1, 2$):\n- Weights: $w = (w_{1}, w_{2}) = (1, 4)$\n- Biomarker concentrations: $x = (x_{1}, x_{2}) = (0, 1)$\n- Clinical outcomes: $y = (y_{1}, y_{2}) = (1, 3)$\n\nSubstituting these values into the expression for $S(\\beta_{0}, \\beta_{1})$:\n$$S(\\beta_{0}, \\beta_{1}) = w_{1}(y_{1} - \\beta_{0} - \\beta_{1} x_{1})^{2} + w_{2}(y_{2} - \\beta_{0} - \\beta_{1} x_{2})^{2}$$\n$$S(\\beta_{0}, \\beta_{1}) = 1 \\cdot (1 - \\beta_{0} - \\beta_{1} \\cdot 0)^{2} + 4 \\cdot (3 - \\beta_{0} - \\beta_{1} \\cdot 1)^{2}$$\n$$S(\\beta_{0}, \\beta_{1}) = (1 - \\beta_{0})^{2} + 4(3 - \\beta_{0} - \\beta_{1})^{2}$$\n\nTo find the values of $\\beta_{0}$ and $\\beta_{1}$ that minimize this function, we must find the critical points by taking the partial derivatives with respect to $\\beta_{0}$ and $\\beta_{1}$ and setting them equal to zero. This yields a system of two linear equations known as the normal equations.\n\nFirst, we compute the partial derivative with respect to $\\beta_{0}$:\n$$\\frac{\\partial S}{\\partial \\beta_{0}} = \\frac{\\partial}{\\partial \\beta_{0}} \\left[ (1 - \\beta_{0})^{2} + 4(3 - \\beta_{0} - \\beta_{1})^{2} \\right]$$\n$$\\frac{\\partial S}{\\partial \\beta_{0}} = 2(1 - \\beta_{0}) \\cdot (-1) + 4 \\cdot 2(3 - \\beta_{0} - \\beta_{1}) \\cdot (-1)$$\n$$\\frac{\\partial S}{\\partial \\beta_{0}} = -2(1 - \\beta_{0}) - 8(3 - \\beta_{0} - \\beta_{1})$$\nSetting this derivative to zero to find the minimum:\n$$-2(1 - \\beta_{0}) - 8(3 - \\beta_{0} - \\beta_{1}) = 0$$\nDividing by $-2$:\n$$(1 - \\beta_{0}) + 4(3 - \\beta_{0} - \\beta_{1}) = 0$$\n$$1 - \\beta_{0} + 12 - 4\\beta_{0} - 4\\beta_{1} = 0$$\n$$13 - 5\\beta_{0} - 4\\beta_{1} = 0$$\nThis gives us our first normal equation:\n$$5\\beta_{0} + 4\\beta_{1} = 13 \\quad (1)$$\n\nNext, we compute the partial derivative with respect to $\\beta_{1}$:\n$$\\frac{\\partial S}{\\partial \\beta_{1}} = \\frac{\\partial}{\\partial \\beta_{1}} \\left[ (1 - \\beta_{0})^{2} + 4(3 - \\beta_{0} - \\beta_{1})^{2} \\right]$$\nThe first term, $(1 - \\beta_{0})^{2}$, does not depend on $\\beta_{1}$, so its derivative is zero.\n$$\\frac{\\partial S}{\\partial \\beta_{1}} = 0 + 4 \\cdot 2(3 - \\beta_{0} - \\beta_{1}) \\cdot (-1)$$\n$$\\frac{\\partial S}{\\partial \\beta_{1}} = -8(3 - \\beta_{0} - \\beta_{1})$$\nSetting this derivative to zero:\n$$-8(3 - \\beta_{0} - \\beta_{1}) = 0$$\n$$3 - \\beta_{0} - \\beta_{1} = 0$$\nThis gives us our second normal equation:\n$$\\beta_{0} + \\beta_{1} = 3 \\quad (2)$$\n\nNow we must solve the system of two linear equations:\n$$1) \\quad 5\\beta_{0} + 4\\beta_{1} = 13$$\n$$2) \\quad \\beta_{0} + \\beta_{1} = 3$$\n\nFrom equation (2), we can express $\\beta_{0}$ in terms of $\\beta_{1}$:\n$$\\beta_{0} = 3 - \\beta_{1}$$\nSubstitute this expression for $\\beta_{0}$ into equation (1):\n$$5(3 - \\beta_{1}) + 4\\beta_{1} = 13$$\n$$15 - 5\\beta_{1} + 4\\beta_{1} = 13$$\n$$15 - \\beta_{1} = 13$$\n$$\\beta_{1} = 15 - 13$$\n$$\\beta_{1} = 2$$\n\nFinally, substitute the value of $\\beta_{1}$ back into the expression for $\\beta_{0}$:\n$$\\beta_{0} = 3 - \\beta_{1} = 3 - 2$$\n$$\\beta_{0} = 1$$\n\nThe WLS estimates are $\\hat{\\beta}_{0} = 1$ and $\\hat{\\beta}_{1} = 2$. The question asks for the ordered pair $(\\beta_{0}, \\beta_{1})$.",
            "answer": "$$\\boxed{(1, 2)}$$"
        },
        {
            "introduction": "Once we have estimated the coefficients of a regression model, a common next step is to test hypotheses about them. A frequent question in clinical trials is whether two different treatments or dosages have equivalent effects. This practice  guides you through the process of constructing and calculating a test statistic to formally compare two regression coefficients, a fundamental skill for drawing meaningful conclusions from statistical models.",
            "id": "4916000",
            "problem": "A randomized clinical study investigates the effect of two dosing regimens of a new anti-inflammatory drug on the logarithm of C-reactive protein concentration, adjusted for baseline covariates. A multiple linear regression is fit with design matrix $X$ comprising an intercept, two binary indicators for the dosing regimens (relative to placebo), and additional covariates (age, sex, baseline body mass index, and site), yielding a parameter vector $\\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\dots, \\beta_p)^{\\top}$. The regression model is $Y = X \\beta + \\varepsilon$, where $\\varepsilon$ are independent errors with mean $0$ and variance $\\sigma^{2}$, and coefficients are estimated by ordinary least squares (OLS).\n\nThe clinical question is whether the two dosing regimens have the same effect on the outcome. This can be expressed as the null hypothesis $H_{0} : \\beta_{1} = \\beta_{2}$, which corresponds to the linear contrast $c = (0, 1, -1, 0, \\dots, 0)^{\\top}$ and $H_{0} : c^{\\top} \\beta = 0$. Suppose the OLS estimates satisfy $\\hat{\\beta}_{1} = 0.5$ and $\\hat{\\beta}_{2} = 0.1$. From the fitted model, the estimated variance of the contrast is reported as $\\widehat{\\operatorname{Var}}(\\hat{\\beta}_{1} - \\hat{\\beta}_{2}) = 0.09$.\n\nStarting from first principles of the linear model and the sampling distribution of linear contrasts of OLS coefficients, derive the appropriate test statistic for $H_{0} : \\beta_{1} = \\beta_{2}$ and then compute its numerical value using the provided quantities. Give your final numerical answer as an exact value (do not round).",
            "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded problem in biostatistics.\n\nThe problem asks for the derivation of a test statistic for a linear contrast of coefficients in a multiple linear regression model, and for the computation of its value.\n\nLet the multiple linear regression model be specified as:\n$$\nY = X \\beta + \\varepsilon\n$$\nwhere $Y$ is the $n \\times 1$ vector of outcomes, $X$ is the $n \\times (p+1)$ design matrix of rank $p+1$, $\\beta$ is the $(p+1) \\times 1$ vector of unknown parameters, and $\\varepsilon$ is the $n \\times 1$ vector of random errors. The problem states that the errors $\\varepsilon_i$ are independent with mean $0$ and constant variance $\\sigma^2$. For exact finite-sample inference, we make the standard assumption that the errors are normally distributed. Thus, $\\varepsilon \\sim N(0, \\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix.\n\nThe coefficients are estimated using ordinary least squares (OLS). The OLS estimator for $\\beta$ is given by:\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}Y\n$$\nSince $\\hat{\\beta}$ is a linear transformation of the normally distributed vector $Y$, $\\hat{\\beta}$ is also normally distributed. Its distribution is:\n$$\n\\hat{\\beta} \\sim N(\\beta, \\sigma^2 (X^{\\top}X)^{-1})\n$$\nThe mean $\\mathbb{E}[\\hat{\\beta}] = \\beta$ shows that $\\hat{\\beta}$ is an unbiased estimator of $\\beta$. The variance-covariance matrix is $\\operatorname{Var}(\\hat{\\beta}) = \\sigma^2 (X^{\\top}X)^{-1}$.\n\nThe null hypothesis of interest is $H_{0} : \\beta_{1} = \\beta_{2}$, which can be written as a linear contrast $H_{0} : c^{\\top}\\beta = 0$, where the contrast vector $c$ is given by $c = (0, 1, -1, 0, \\dots, 0)^{\\top}$. The corresponding estimator for the contrast is $c^{\\top}\\hat{\\beta} = \\hat{\\beta}_{1} - \\hat{\\beta}_{2}$.\n\nAs a linear combination of the elements of $\\hat{\\beta}$, the contrast estimator $c^{\\top}\\hat{\\beta}$ is also normally distributed:\n$$\n\\mathbb{E}[c^{\\top}\\hat{\\beta}] = c^{\\top}\\mathbb{E}[\\hat{\\beta}] = c^{\\top}\\beta\n$$\n$$\n\\operatorname{Var}(c^{\\top}\\hat{\\beta}) = c^{\\top}\\operatorname{Var}(\\hat{\\beta})c = c^{\\top}(\\sigma^2 (X^{\\top}X)^{-1})c = \\sigma^2 c^{\\top}(X^{\\top}X)^{-1}c\n$$\nThus, $c^{\\top}\\hat{\\beta} \\sim N(c^{\\top}\\beta, \\sigma^2 c^{\\top}(X^{\\top}X)^{-1}c)$.\n\nUnder the null hypothesis $H_{0} : c^{\\top}\\beta = 0$, we have $\\mathbb{E}[c^{\\top}\\hat{\\beta}] = 0$. If $\\sigma^2$ were known, we could form the following standard normal statistic:\n$$\nZ = \\frac{c^{\\top}\\hat{\\beta}}{\\sqrt{\\operatorname{Var}(c^{\\top}\\hat{\\beta})}} = \\frac{c^{\\top}\\hat{\\beta}}{\\sigma\\sqrt{c^{\\top}(X^{\\top}X)^{-1}c}} \\sim N(0, 1)\n$$\nHowever, the error variance $\\sigma^2$ is typically unknown. It must be estimated from the data. The unbiased estimator for $\\sigma^2$ is the mean squared error (MSE), denoted $\\hat{\\sigma}^2$:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-(p+1)} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{SSE}{n-(p+1)}\n$$\nwhere $SSE$ is the sum of squared errors and $df = n-(p+1)$ are the residual degrees of freedom. It is a fundamental result in linear model theory that the random variable $\\frac{(n-(p+1))\\hat{\\sigma}^2}{\\sigma^2} = \\frac{SSE}{\\sigma^2}$ follows a chi-squared distribution with $n-(p+1)$ degrees of freedom, i.e., $\\chi^2_{n-(p+1)}$. It is also known that $\\hat{\\sigma}^2$ is statistically independent of $\\hat{\\beta}$.\n\nTo form a test statistic that does not depend on the unknown $\\sigma^2$, we replace $\\sigma^2$ with its estimate $\\hat{\\sigma}^2$. This leads to the Student's t-statistic. The general form of a t-distributed random variable with $df$ degrees of freedom is the ratio of a standard normal random variable to the square root of an independent chi-squared random variable divided by its degrees of freedom.\n$$\nT = \\frac{Z}{\\sqrt{\\chi^2_{df}/df}} \\sim t_{df}\n$$\nIn our context, the statistic is constructed as:\n$$\nT = \\frac{\\frac{c^{\\top}\\hat{\\beta} - c^{\\top}\\beta}{\\sigma\\sqrt{c^{\\top}(X^{\\top}X)^{-1}c}}}{\\sqrt{\\frac{(n-(p+1))\\hat{\\sigma}^2}{\\sigma^2} / (n-(p+1))}} = \\frac{\\frac{c^{\\top}\\hat{\\beta} - c^{\\top}\\beta}{\\sigma\\sqrt{c^{\\top}(X^{\\top}X)^{-1}c}}}{\\sqrt{\\hat{\\sigma}^2/\\sigma^2}} = \\frac{c^{\\top}\\hat{\\beta} - c^{\\top}\\beta}{\\hat{\\sigma}\\sqrt{c^{\\top}(X^{\\top}X)^{-1}c}}\n$$\nThe denominator is the estimated standard error of the contrast estimator:\n$$\n\\operatorname{SE}(c^{\\top}\\hat{\\beta}) = \\sqrt{\\widehat{\\operatorname{Var}}(c^{\\top}\\hat{\\beta})} = \\sqrt{\\hat{\\sigma}^2 c^{\\top}(X^{\\top}X)^{-1}c}\n$$\nSo, the statistic $T = \\frac{c^{\\top}\\hat{\\beta} - c^{\\top}\\beta}{\\operatorname{SE}(c^{\\top}\\hat{\\beta})}$ follows a t-distribution with $n-(p+1)$ degrees of freedom.\n\nTo test the specific null hypothesis $H_{0} : c^{\\top}\\beta = 0$, we set $c^{\\top}\\beta = 0$ in the expression for $T$. The appropriate test statistic is therefore:\n$$\nt = \\frac{c^{\\top}\\hat{\\beta}}{\\operatorname{SE}(c^{\\top}\\hat{\\beta})} = \\frac{\\hat{\\beta}_{1} - \\hat{\\beta}_{2}}{\\sqrt{\\widehat{\\operatorname{Var}}(\\hat{\\beta}_{1} - \\hat{\\beta}_{2})}}\n$$\nThis is the required derived test statistic.\n\nWe now compute its numerical value using the information provided in the problem statement.\nThe given quantities are:\n- Estimated coefficient for regimen 1: $\\hat{\\beta}_{1} = 0.5$\n- Estimated coefficient for regimen 2: $\\hat{\\beta}_{2} = 0.1$\n- Estimated variance of the contrast: $\\widehat{\\operatorname{Var}}(\\hat{\\beta}_{1} - \\hat{\\beta}_{2}) = 0.09$\n\nFirst, compute the value of the estimated contrast:\n$$\n\\hat{\\beta}_{1} - \\hat{\\beta}_{2} = 0.5 - 0.1 = 0.4\n$$\nNext, compute the standard error of the contrast, which is the square root of the estimated variance:\n$$\n\\operatorname{SE}(\\hat{\\beta}_{1} - \\hat{\\beta}_{2}) = \\sqrt{\\widehat{\\operatorname{Var}}(\\hat{\\beta}_{1} - \\hat{\\beta}_{2})} = \\sqrt{0.09} = 0.3\n$$\nFinally, compute the value of the t-statistic:\n$$\nt = \\frac{0.4}{0.3} = \\frac{4/10}{3/10} = \\frac{4}{3}\n$$\nThe numerical value of the test statistic is $\\frac{4}{3}$.",
            "answer": "$$\\boxed{\\frac{4}{3}}$$"
        },
        {
            "introduction": "Real-world data rarely follows a perfect linear relationship, meaning our models are often approximations of a more complex reality. This raises a critical question: what do our estimated coefficients represent when the model is 'wrong'? This conceptual exercise  challenges you to interpret a slope coefficient as the 'best linear approximation' to a nonlinear function, exploring the subtle but vital distinction between a linear association and any association at all.",
            "id": "4916046",
            "problem": "A biostatistician collects independent observations of a continuous biomarker-exposure pair $(X_1,Y)$ where the data-generating mechanism satisfies $Y=\\sin(X_1)+\\varepsilon$, with $X_1\\sim \\mathcal{N}(0,1)$ and $\\varepsilon\\sim \\mathcal{N}(0,\\sigma^2)$ independent of $X_1$. They fit the misspecified linear model $Y=\\beta_0+\\beta_1 X_1+\\text{error}$ by ordinary least squares (OLS). Interpret $\\beta_1$ as the population best linear approximation coefficient, defined as the value that minimizes the expected squared error between $Y$ and its linear predictor in $X_1$. They then consider the usual large-sample test of $H_0:\\beta_1=0$.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. In this setting, the population slope equals the derivative of $\\sin(x)$ at $x=0$, so $\\beta_1^\\star=\\cos(0)=1$.\n\nB. In this setting, the population slope equals $\\mathrm{Cov}(X_1,\\sin(X_1))/\\mathrm{Var}(X_1)$; because $X_1\\sim \\mathcal{N}(0,1)$, this yields $\\beta_1^\\star=\\mathbb{E}[\\cos(X_1)]=e^{-1/2}$.\n\nC. A large-sample test of $H_0:\\beta_1=0$ in the misspecified linear model is a test that there is no association at all between $X_1$ and $Y=\\sin(X_1)+\\varepsilon$.\n\nD. It is possible that $H_0:\\beta_1=0$ is not rejected even if $X_1$ has a strong nonlinear effect on $Y$; the test only targets the zero linear component in the best linear approximation of $\\sin(X_1)$ by $X_1$.\n\nE. Under the stated independence and moment conditions, the OLS estimator $\\hat{\\beta}_1$ is consistent for the population projection coefficient $\\beta_1^\\star$ even though the linear model is misspecified.",
            "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\n- Data Source: Independent observations of a continuous biomarker-exposure pair $(X_1, Y)$.\n- Data-Generating Mechanism: $Y = \\sin(X_1) + \\varepsilon$.\n- Distribution of $X_1$: $X_1 \\sim \\mathcal{N}(0, 1)$.\n- Distribution of $\\varepsilon$: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n- Independence: $X_1$ and $\\varepsilon$ are independent.\n- Fitted Model: $Y = \\beta_0 + \\beta_1 X_1 + \\text{error}$ (misspecified linear model).\n- Estimation Method: Ordinary Least Squares (OLS).\n- Definition of Population Coefficient $\\beta_1$: The value that minimizes the expected squared error, $\\mathbb{E}[(Y - (\\beta_0 + \\beta_1 X_1))^2]$. This coefficient will be denoted $\\beta_1^\\star$.\n- Hypothesis Test: A large-sample test of $H_0: \\beta_1 = 0$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in statistical theory, specifically in the area of regression analysis and model misspecification. The concepts of OLS, population projection coefficients, normal distributions, and trigonometric functions are standard in mathematics and statistics.\n- **Well-Posed**: The problem is well-posed. The definition of the population coefficient $\\beta_1^\\star$ as the minimizer of the expected squared error is precise and leads to a unique solution. The question asks for an evaluation of several statements based on this setup, which is a standard format for a conceptual problem in statistics.\n- **Objective**: The problem is stated in objective, formal language without ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a standard theoretical question in biostatistics and econometrics concerning the interpretation of regression coefficients under model misspecification. I will now proceed to solve the problem.\n\n---\n\n## SOLUTION DERIVATION\n\nThe problem asks us to evaluate several statements concerning a misspecified linear regression. The true data-generating process is $Y = \\sin(X_1) + \\varepsilon$, where $X_1 \\sim \\mathcal{N}(0, 1)$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ are independent. We are fitting the model $Y = \\beta_0 + \\beta_1 X_1 + \\text{error}$ via OLS. The population coefficients $(\\beta_0^\\star, \\beta_1^\\star)$ are defined as the values that minimize the mean squared error of the linear approximation:\n$$L(\\beta_0, \\beta_1) = \\mathbb{E}[(Y - (\\beta_0 + \\beta_1 X_1))^2]$$\n\nTo find the minimum, we take the partial derivatives with respect to $\\beta_0$ and $\\beta_1$ and set them to zero. The first-order condition for $\\beta_1$ is:\n$$\\frac{\\partial L}{\\partial \\beta_1} = \\mathbb{E}[-2 X_1 (Y - \\beta_0 - \\beta_1 X_1)] = 0$$\n$$\\mathbb{E}[X_1 Y] - \\beta_0 \\mathbb{E}[X_1] - \\beta_1 \\mathbb{E}[X_1^2] = 0$$\nThe first-order condition for $\\beta_0$ gives $\\beta_0 = \\mathbb{E}[Y] - \\beta_1 \\mathbb{E}[X_1]$.\nSubstituting this into the equation for $\\beta_1$:\n$$\\mathbb{E}[X_1 Y] - (\\mathbb{E}[Y] - \\beta_1 \\mathbb{E}[X_1]) \\mathbb{E}[X_1] - \\beta_1 \\mathbb{E}[X_1^2] = 0$$\n$$\\mathbb{E}[X_1 Y] - \\mathbb{E}[X_1]\\mathbb{E}[Y] = \\beta_1 (\\mathbb{E}[X_1^2] - (\\mathbb{E}[X_1])^2)$$\nThis simplifies to the well-known formula for the population slope coefficient:\n$$\\beta_1^\\star = \\frac{\\mathrm{Cov}(X_1, Y)}{\\mathrm{Var}(X_1)}$$\n\nNow, we compute the components using the given information:\n- $\\mathrm{Var}(X_1)$: Since $X_1 \\sim \\mathcal{N}(0, 1)$, $\\mathrm{Var}(X_1) = 1$.\n- $\\mathrm{Cov}(X_1, Y)$: We substitute $Y = \\sin(X_1) + \\varepsilon$.\n$$\\mathrm{Cov}(X_1, Y) = \\mathrm{Cov}(X_1, \\sin(X_1) + \\varepsilon) = \\mathrm{Cov}(X_1, \\sin(X_1)) + \\mathrm{Cov}(X_1, \\varepsilon)$$\nSince $X_1$ and $\\varepsilon$ are independent, $\\mathrm{Cov}(X_1, \\varepsilon) = 0$.\n$$\\mathrm{Cov}(X_1, \\sin(X_1)) = \\mathbb{E}[X_1 \\sin(X_1)] - \\mathbb{E}[X_1] \\mathbb{E}[\\sin(X_1)]$$\nGiven $X_1 \\sim \\mathcal{N}(0, 1)$, we have $\\mathbb{E}[X_1] = 0$.\nAlso, the function $\\sin(x) \\cdot \\phi(x)$, where $\\phi(x)$ is the standard normal probability density function, is an odd function. Therefore, its integral over the symmetric interval $(-\\infty, \\infty)$ is zero.\n$$\\mathbb{E}[\\sin(X_1)] = \\int_{-\\infty}^{\\infty} \\sin(x) \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx = 0$$\nThus, $\\mathrm{Cov}(X_1, \\sin(X_1)) = \\mathbb{E}[X_1 \\sin(X_1)]$.\n\nPutting it all together:\n$$\\beta_1^\\star = \\frac{\\mathbb{E}[X_1 \\sin(X_1)]}{1} = \\mathbb{E}[X_1 \\sin(X_1)]$$\n\nTo calculate this expectation, we use Stein's Lemma. For a random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and a differentiable function $g$ with $\\mathbb{E}|g'(X)| < \\infty$, the lemma states $\\mathbb{E}[(X-\\mu)g(X)] = \\sigma^2 \\mathbb{E}[g'(X)]$.\nIn our case, $X_1 \\sim \\mathcal{N}(0, 1)$, so $\\mu=0$ and $\\sigma^2=1$. Let $g(X_1) = \\sin(X_1)$, so $g'(X_1) = \\cos(X_1)$. The condition $\\mathbb{E}|\\cos(X_1)| < \\infty$ is met since $|\\cos(X_1)| \\le 1$.\nApplying Stein's Lemma:\n$$\\beta_1^\\star = \\mathbb{E}[X_1 \\sin(X_1)] = (1) \\cdot \\mathbb{E}[\\cos(X_1)]$$\n\nFinally, we compute $\\mathbb{E}[\\cos(X_1)]$. The characteristic function of a standard normal random variable is $\\phi_{X_1}(t) = \\mathbb{E}[e^{itX_1}] = e^{-t^2/2}$.\nBy Euler's formula, $\\mathbb{E}[e^{itX_1}] = \\mathbb{E}[\\cos(tX_1) + i\\sin(tX_1)] = \\mathbb{E}[\\cos(tX_1)] + i\\mathbb{E}[\\sin(tX_1)]$.\nSince the distribution of $X_1$ is symmetric about $0$, $\\mathbb{E}[\\sin(tX_1)]$ is $0$ for any $t$.\nTherefore, $\\mathbb{E}[\\cos(tX_1)] = e^{-t^2/2}$.\nWe need $\\mathbb{E}[\\cos(X_1)]$, which corresponds to setting $t=1$:\n$$\\mathbb{E}[\\cos(X_1)] = e^{-1^2/2} = e^{-1/2}$$\nSo, the population slope is $\\beta_1^\\star = e^{-1/2}$.\n\nWith this result, $\\beta_1^\\star = e^{-1/2} \\approx 0.6065$, we can evaluate the options.\n\n## OPTION-BY-OPTION ANALYSIS\n\n**A. In this setting, the population slope equals the derivative of $\\sin(x)$ at $x=0$, so $\\beta_1^\\star=\\cos(0)=1$.**\nThis statement proposes that the population slope is determined by a first-order Taylor expansion of the true mean function $g(x)=\\sin(x)$ around the mean of the predictor, $\\mathbb{E}[X_1]=0$. The derivative is $g'(x) = \\cos(x)$, so $g'(0) = \\cos(0) = 1$. However, the population OLS slope is a global property of the joint distribution of $(X_1, Y)$, averaging over the entire support of $X_1$, not a local property at a single point. Our calculation showed that $\\beta_1^\\star = e^{-1/2}$, which is not equal to $1$. The approximation only holds if the distribution of $X_1$ is highly concentrated around $0$.\nVerdict: **Incorrect**.\n\n**B. In this setting, the population slope equals $\\mathrm{Cov}(X_1,\\sin(X_1))/\\mathrm{Var}(X_1)$; because $X_1\\sim \\mathcal{N}(0,1)$, this yields $\\beta_1^\\star=\\mathbb{E}[\\cos(X_1)]=e^{-1/2}$.**\nThis statement makes three claims:\n1. $\\beta_1^\\star = \\mathrm{Cov}(X_1,\\sin(X_1))/\\mathrm{Var}(X_1)$: As shown in our derivation, $\\beta_1^\\star = \\mathrm{Cov}(X_1, Y)/\\mathrm{Var}(X_1)$ and since $Y = \\sin(X_1) + \\varepsilon$ with $\\varepsilon$ independent of $X_1$, this correctly simplifies to $\\mathrm{Cov}(X_1,\\sin(X_1))/\\mathrm{Var}(X_1)$.\n2. This expression equals $\\mathbb{E}[\\cos(X_1)]$: As shown using Stein's Lemma with $\\mathrm{Var}(X_1)=1$, $\\mathrm{Cov}(X_1, \\sin(X_1)) = \\mathbb{E}[X_1 \\sin(X_1)] = \\mathbb{E}[\\cos(X_1)]$. This is correct.\n3. The value is $e^{-1/2}$: As calculated using the characteristic function of the standard normal distribution, $\\mathbb{E}[\\cos(X_1)] = e^{-1/2}$. This is also correct.\nAll parts of the statement are mathematically correct and follow from our derivation.\nVerdict: **Correct**.\n\n**C. A large-sample test of $H_0:\\beta_1=0$ in the misspecified linear model is a test that there is no association at all between $X_1$ and $Y=\\sin(X_1)+\\varepsilon$.**\nThe hypothesis test for $H_0: \\beta_1=0$ assesses whether the population coefficient of the *best linear approximation* is zero. It tests for the absence of a linear component of association. \"No association at all\" between $X_1$ and $Y$ would imply that the conditional distribution of $Y$ given $X_1$ is the same as the marginal distribution of $Y$, or at least that $\\mathbb{E}[Y|X_1]$ is constant. In this problem, $\\mathbb{E}[Y|X_1] = \\mathbb{E}[\\sin(X_1) + \\varepsilon | X_1] = \\sin(X_1)$, which is a non-constant function of $X_1$. There is a clear, strong nonlinear association. The test of $H_0:\\beta_1=0$ is not a test for any and all associations; it is specific to the linear projection.\nVerdict: **Incorrect**.\n\n**D. It is possible that $H_0:\\beta_1=0$ is not rejected even if $X_1$ has a strong nonlinear effect on $Y$; the test only targets the zero linear component in the best linear approximation of $\\sin(X_1)$ by $X_1$.**\nThis statement correctly distinguishes between the overall relationship and the specific linear component targeted by OLS. First, there is indeed a strong nonlinear effect, as $\\mathbb{E}[Y|X_1] = \\sin(X_1)$. Second, a statistical test for $H_0: \\beta_1=0$ may fail to reject the null. This can happen if the null is true (i.e., $\\beta_1^\\star=0$, as in the case of $Y=X_1^2$ with $X_1 \\sim \\mathcal{N}(0,1)$), or if the null is false but the test lacks statistical power (a Type II error), which is always possible with finite samples. The statement correctly identifies that the test's scope is limited to the \"zero linear component in the best linear approximation,\" which is precisely what $\\beta_1=0$ means in this context.\nVerdict: **Correct**.\n\n**E. Under the stated independence and moment conditions, the OLS estimator $\\hat{\\beta}_1$ is consistent for the population projection coefficient $\\beta_1^\\star$ even though the linear model is misspecified.**\nThis is a standard and fundamental result in the theory of OLS. The OLS estimator for the slope is $\\hat{\\beta}_1 = \\frac{\\sum_i (X_{1i}-\\bar{X}_1)(Y_i-\\bar{Y})}{\\sum_i (X_{1i}-\\bar{X}_1)^2}$. By the Law of Large Numbers, the sample covariance in the numerator converges in probability to $\\mathrm{Cov}(X_1, Y)$, and the sample variance in the denominator converges in probability to $\\mathrm{Var}(X_1)$. Therefore, their ratio $\\hat{\\beta}_1$ converges in probability to $\\frac{\\mathrm{Cov}(X_1, Y)}{\\mathrm{Var}(X_1)}$, which is the definition of the population projection coefficient $\\beta_1^\\star$. This consistency holds as long as the necessary moments exist and the observations are independent, which is true under the problem's setup. The fact that the linear model is misspecified does not affect the consistency of the OLS estimator for its population analogue, $\\beta_1^\\star$.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{BDE}$$"
        }
    ]
}