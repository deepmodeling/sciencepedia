{
    "hands_on_practices": [
        {
            "introduction": "Regression models operate on numerical inputs, which means categorical variables like treatment groups or disease stages must be translated into a set of numerical predictors. This exercise demonstrates how different valid coding schemes—such as treatment, effect, and Helmert coding—can be used to represent the same categorical factor. By working through the calculations, you will confirm a fundamental principle: while the coefficients and their interpretations change with the coding scheme, the model's overall fit and predictions for $y_i=\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}+\\varepsilon_i$ remain identical .",
            "id": "4899207",
            "problem": "A single-factor linear regression model with a categorical predictor having $K=3$ levels is fit by ordinary least squares (OLS) with an intercept. Consider the following dataset of $n=6$ independent observations, each belonging to exactly one of three levels $L_1$, $L_2$, $L_3$ of the categorical predictor:\n- Observation $1$: $y_1=4$, level $L_1$\n- Observation $2$: $y_2=6$, level $L_1$\n- Observation $3$: $y_3=8$, level $L_2$\n- Observation $4$: $y_4=10$, level $L_2$\n- Observation $5$: $y_5=5$, level $L_3$\n- Observation $6$: $y_6=3$, level $L_3$\n\nStart from the core definitions of the linear model, least squares projection, and the coefficient of determination. Use three different full-rank coding schemes for the factor (always including an intercept) and fit the model $y_i=\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}+\\varepsilon_i$ by OLS under each scheme:\n1. Treatment (dummy) coding with $L_1$ as the reference level. Use regressors $\\{1, D_2, D_3\\}$ where $D_k=1$ if the observation is in level $L_k$ and $0$ otherwise.\n2. Effect (sum-to-zero) coding with regressors $\\{1, E_1, E_2\\}$ defined by the per-level codes\n   - $L_1$: $(E_1, E_2)=(-1,-1)$,\n   - $L_2$: $(E_1, E_2)=(1,0)$,\n   - $L_3$: $(E_1, E_2)=(0,1)$.\n3. Helmert coding with regressors $\\{1, H_1, H_2\\}$ defined by the per-level codes\n   - $L_1$: $(H_1, H_2)=(-1,-1)$,\n   - $L_2$: $(H_1, H_2)=(1,-1)$,\n   - $L_3$: $(H_1, H_2)=(0,2)$.\n\nFor each coding:\n- Compute the OLS coefficient vector $\\boldsymbol{\\beta}$.\n- Compute the fitted value $\\hat{y}_i$ for every observation $i\\in\\{1,\\dots,6\\}$.\n- Compute the coefficient of determination (R-squared), defined as $R^2=1-\\mathrm{SSE}/\\mathrm{SST}$, where $\\mathrm{SSE}=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$ and $\\mathrm{SST}=\\sum_{i=1}^{n}(y_i-\\bar{y})^2$ with $\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i$.\n\nUsing first principles, justify why the fitted values and $R^2$ are invariant across these full-rank codings, even though the individual coefficients differ in interpretation and numerical value.\n\nReport as your final answer the common numerical value of $R^2$ for this dataset. Round your answer to four significant figures.",
            "solution": "The user wants me to solve a problem.\nMy analysis of the user's request is as follows:\n1.  **Extract Givens**:\n    -   Model: Single-factor linear regression, $y_{i}=\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}+\\varepsilon_{i}$, fit by OLS with an intercept.\n    -   Categorical predictor with $K=3$ levels: $L_{1}$, $L_{2}$, $L_{3}$.\n    -   Dataset ($n=6$ observations):\n        -   $y_{1}=4$, level $L_{1}$\n        -   $y_{2}=6$, level $L_{1}$\n        -   $y_{3}=8$, level $L_{2}$\n        -   $y_{4}=10$, level $L_{2}$\n        -   $y_{5}=5$, level $L_{3}$\n        -   $y_{6}=3$, level $L_{3}$\n    -   Coding Scheme 1 (Treatment/Dummy): Reference level $L_{1}$. Regressors $\\{1,D_{2},D_{3}\\}$ where $D_{k}=1$ if in level $L_{k}$, $0$ otherwise.\n    -   Coding Scheme 2 (Effect/Sum-to-zero): Regressors $\\{1,E_{1},E_{2}\\}$ with per-level codes: $L_{1}: (-1,-1)$, $L_{2}: (1,0)$, $L_{3}: (0,1)$.\n    -   Coding Scheme 3 (Helmert): Regressors $\\{1,H_{1},H_{2}\\}$ with per-level codes: $L_{1}: (-1,-1)$, $L_{2}: (1,-1)$, $L_{3}: (0,2)$.\n    -   Tasks: For each coding scheme, compute $\\boldsymbol{\\beta}$, $\\hat{y}_{i}$ for all $i$, and $R^{2}$.\n    -   Definition of $R^2$: $R^{2}=1-\\mathrm{SSE}/\\mathrm{SST}$, where $\\mathrm{SSE}=\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$ and $\\mathrm{SST}=\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}$.\n    -   Justification: Explain from first principles why $\\hat{y}_{i}$ and $R^{2}$ are invariant.\n\n2.  **Validate Using Extracted Givens**:\n    -   **Scientifically Grounded**: The problem is a standard exercise in linear regression analysis, a fundamental topic in statistics and biostatistics. It is based on established principles of OLS.\n    -   **Well-Posed**: All data, model forms, and definitions required for the solution are provided. The coding schemes specified are full-rank, ensuring a unique OLS solution exists for each.\n    -   **Objective**: The problem is stated using precise, standard mathematical and statistical terminology, free of subjectivity.\n    -   The problem does not violate any of the invalidity criteria. It is complete, consistent, realistic, and well-structured.\n\n3.  **Verdict and Action**:\n    -   The problem is **valid**. A full solution will be provided.\n\nThe problem asks for an analysis of a single-factor linear model under three different parameterizations. We will first provide the theoretical justification for the invariance of fitted values and $R^2$, then perform the calculations for the specific dataset.\n\n**Theoretical Justification for Invariance**\n\nThe general linear model is expressed in matrix form as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y}$ is the $n \\times 1$ vector of responses, $\\mathbf{X}$ is the $n \\times p$ design matrix, $\\boldsymbol{\\beta}$ is the $p \\times 1$ vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is the vector of errors. The Ordinary Least Squares (OLS) estimator for $\\boldsymbol{\\beta}$ is given by $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$, assuming $\\mathbf{X}$ has full column rank.\n\nThe vector of fitted values, $\\hat{\\mathbf{y}}$, is given by:\n$$ \\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} $$\nThis can be written as $\\hat{\\mathbf{y}} = \\mathbf{P}\\mathbf{y}$, where the matrix $\\mathbf{P} = \\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}$ is the projection matrix. Geometrically, $\\hat{\\mathbf{y}}$ is the orthogonal projection of the data vector $\\mathbf{y}$ onto the column space of the design matrix, $C(\\mathbf{X})$. The projection matrix $\\mathbf{P}$ is uniquely determined by the subspace $C(\\mathbf{X})$, not by the specific choice of basis vectors (columns) for that subspace.\n\nIn this problem, we have a single categorical factor with $K=3$ levels. Any full-rank parameterization with an intercept will require $p=3$ columns in the design matrix $\\mathbf{X}$. Let $\\mathbf{X}_A$ and $\\mathbf{X}_B$ be two such design matrices corresponding to different coding schemes (e.g., treatment and effect coding). Both matrices are constructed to span the same subspace: the subspace of vectors that are constant within each of the $K$ levels. This subspace has dimension $K=3$. Because both $\\mathbf{X}_A$ and $\\mathbf{X}_B$ are $n \\times K$ matrices of rank $K$, their columns form a basis for this same $K$-dimensional subspace. This means that their column spaces are identical: $C(\\mathbf{X}_A) = C(\\mathbf{X}_B)$.\n\nSince the column spaces are identical, the projection matrix $\\mathbf{P}$ onto this space is also identical for both codings. Consequently, the vector of fitted values, $\\hat{\\mathbf{y}} = \\mathbf{P}\\mathbf{y}$, must be invariant to the choice of a full-rank coding scheme.\n\nThis invariance directly extends to the Sum of Squared Errors ($\\mathrm{SSE}$) and the coefficient of determination ($R^2$):\n- $\\mathrm{SSE} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = ||\\mathbf{y} - \\hat{\\mathbf{y}}||^2$. Since $\\mathbf{y}$ is fixed and $\\hat{\\mathbf{y}}$ is invariant, $\\mathrm{SSE}$ is invariant.\n- The Total Sum of Squares, $\\mathrm{SST} = \\sum_{i=1}^{n}(y_i - \\bar{y})^2$, depends only on the response data $\\mathbf{y}$ and is therefore constant regardless of the model.\n- The coefficient of determination, $R^2 = 1 - \\mathrm{SSE}/\\mathrm{SST}$, must therefore also be invariant across these different coding schemes.\n\nFor any one-way ANOVA model, the OLS projection yields fitted values equal to the group means. Let's find these first as they will be the common set of fitted values.\nThe data are $y = (4, 6, 8, 10, 5, 3)$. The levels are $(L_1, L_1, L_2, L_2, L_3, L_3)$.\n- Mean of level $L_1$: $\\bar{y}_1 = \\frac{4+6}{2} = 5$.\n- Mean of level $L_2$: $\\bar{y}_2 = \\frac{8+10}{2} = 9$.\n- Mean of level $L_3$: $\\bar{y}_3 = \\frac{5+3}{2} = 4$.\nThe vector of fitted values for all schemes will be $\\hat{\\mathbf{y}} = (5, 5, 9, 9, 4, 4)^{\\top}$.\n\n**Computation of $R^2$**\n\nFirst, we compute the overall mean $\\bar{y}$:\n$$ \\bar{y} = \\frac{1}{6}(4+6+8+10+5+3) = \\frac{36}{6} = 6 $$\nNext, we compute the Total Sum of Squares ($\\mathrm{SST}$):\n$$ \\mathrm{SST} = \\sum_{i=1}^{6}(y_i - \\bar{y})^2 = (4-6)^2 + (6-6)^2 + (8-6)^2 + (10-6)^2 + (5-6)^2 + (3-6)^2 $$\n$$ \\mathrm{SST} = (-2)^2 + 0^2 + 2^2 + 4^2 + (-1)^2 + (-3)^2 = 4 + 0 + 4 + 16 + 1 + 9 = 34 $$\nUsing the invariant fitted values $\\hat{\\mathbf{y}} = (5, 5, 9, 9, 4, 4)^{\\top}$, we compute the Sum of Squared Errors ($\\mathrm{SSE}$):\n$$ \\mathrm{SSE} = \\sum_{i=1}^{6}(y_i - \\hat{y}_i)^2 = (4-5)^2 + (6-5)^2 + (8-9)^2 + (10-9)^2 + (5-4)^2 + (3-4)^2 $$\n$$ \\mathrm{SSE} = (-1)^2 + 1^2 + (-1)^2 + 1^2 + 1^2 + (-1)^2 = 1+1+1+1+1+1 = 6 $$\nFinally, we compute the coefficient of determination $R^2$:\n$$ R^2 = 1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}} = 1 - \\frac{6}{34} = 1 - \\frac{3}{17} = \\frac{14}{17} $$\n\nNow, we demonstrate the calculations for each coding scheme as required. The response vector is $\\mathbf{y} = (4, 6, 8, 10, 5, 3)^{\\top}$.\n\n**1. Treatment (Dummy) Coding with $L_1$ as Reference**\n\nThe model is $y_i = \\beta_0 + \\beta_1 D_{i2} + \\beta_2 D_{i3} + \\varepsilon_i$. The design matrix $\\mathbf{X}_D$ is:\n$$ \\mathbf{X}_D = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix} $$\nWe compute $\\mathbf{X}_D^{\\top}\\mathbf{X}_D$ and $\\mathbf{X}_D^{\\top}\\mathbf{y}$:\n$$ \\mathbf{X}_D^{\\top}\\mathbf{X}_D = \\begin{pmatrix} 6 & 2 & 2 \\\\ 2 & 2 & 0 \\\\ 2 & 0 & 2 \\end{pmatrix}, \\quad \\mathbf{X}_D^{\\top}\\mathbf{y} = \\begin{pmatrix} 36 \\\\ 18 \\\\ 8 \\end{pmatrix} $$\nThe inverse of $\\mathbf{X}_D^{\\top}\\mathbf{X}_D$ is:\n$$ (\\mathbf{X}_D^{\\top}\\mathbf{X}_D)^{-1} = \\frac{1}{8}\\begin{pmatrix} 4 & -4 & -4 \\\\ -4 & 8 & 4 \\\\ -4 & 4 & 8 \\end{pmatrix} = \\begin{pmatrix} 1/2 & -1/2 & -1/2 \\\\ -1/2 & 1 & 1/2 \\\\ -1/2 & 1/2 & 1 \\end{pmatrix} $$\nThe OLS coefficients are:\n$$ \\hat{\\boldsymbol{\\beta}}_D = (\\mathbf{X}_D^{\\top}\\mathbf{X}_D)^{-1}\\mathbf{X}_D^{\\top}\\mathbf{y} = \\begin{pmatrix} 1/2 & -1/2 & -1/2 \\\\ -1/2 & 1 & 1/2 \\\\ -1/2 & 1/2 & 1 \\end{pmatrix} \\begin{pmatrix} 36 \\\\ 18 \\\\ 8 \\end{pmatrix} = \\begin{pmatrix} 18 - 9 - 4 \\\\ -18 + 18 + 4 \\\\ -18 + 9 + 8 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 4 \\\\ -1 \\end{pmatrix} $$\nThe fitted values are $\\hat{\\mathbf{y}}_D = \\mathbf{X}_D\\hat{\\boldsymbol{\\beta}}_D$:\n$$ \\hat{\\mathbf{y}}_D = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 4 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 5 \\\\ 5+4 \\\\ 5+4 \\\\ 5-1 \\\\ 5-1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 5 \\\\ 9 \\\\ 9 \\\\ 4 \\\\ 4 \\end{pmatrix} $$\nThese are indeed the group means.\n\n**2. Effect (Sum-to-Zero) Coding**\n\nThe model is $y_i = \\beta_0 + \\beta_1 E_{i1} + \\beta_2 E_{i2} + \\varepsilon_i$. The design matrix $\\mathbf{X}_E$ is:\n$$ \\mathbf{X}_E = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & -1 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix} $$\nWe compute $\\mathbf{X}_E^{\\top}\\mathbf{X}_E$ and $\\mathbf{X}_E^{\\top}\\mathbf{y}$:\n$$ \\mathbf{X}_E^{\\top}\\mathbf{X}_E = \\begin{pmatrix} 6 & 0 & 0 \\\\ 0 & 4 & 2 \\\\ 0 & 2 & 4 \\end{pmatrix}, \\quad \\mathbf{X}_E^{\\top}\\mathbf{y} = \\begin{pmatrix} 36 \\\\ 8 \\\\ -2 \\end{pmatrix} $$\nThe inverse is $(\\mathbf{X}_E^{\\top}\\mathbf{X}_E)^{-1} = \\frac{1}{72}\\begin{pmatrix} 12 & 0 & 0 \\\\ 0 & 24 & -12 \\\\ 0 & -12 & 24 \\end{pmatrix} = \\begin{pmatrix} 1/6 & 0 & 0 \\\\ 0 & 1/3 & -1/6 \\\\ 0 & -1/6 & 1/3 \\end{pmatrix}$.\nThe OLS coefficients are:\n$$ \\hat{\\boldsymbol{\\beta}}_E = \\begin{pmatrix} 1/6 & 0 & 0 \\\\ 0 & 1/3 & -1/6 \\\\ 0 & -1/6 & 1/3 \\end{pmatrix} \\begin{pmatrix} 36 \\\\ 8 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 8/3 + 2/6 \\\\ -8/6 - 2/3 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 3 \\\\ -2 \\end{pmatrix} $$\nThe fitted values are $\\hat{\\mathbf{y}}_E = \\mathbf{X}_E\\hat{\\boldsymbol{\\beta}}_E$:\n$$ \\hat{\\mathbf{y}}_E = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & -1 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 3 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 6 - 3 + 2 \\\\ 6 - 3 + 2 \\\\ 6 + 3 + 0 \\\\ 6 + 3 + 0 \\\\ 6 + 0 - 2 \\\\ 6 + 0 - 2 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 5 \\\\ 9 \\\\ 9 \\\\ 4 \\\\ 4 \\end{pmatrix} $$\nThe fitted values are identical to the previous case.\n\n**3. Helmert Coding**\n\nThe model is $y_i = \\beta_0 + \\beta_1 H_{i1} + \\beta_2 H_{i2} + \\varepsilon_i$. The design matrix is $\\mathbf{X}_H$:\n$$ \\mathbf{X}_H = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & -1 \\\\ 1 & 1 & -1 \\\\ 1 & 1 & -1 \\\\ 1 & 0 & 2 \\\\ 1 & 0 & 2 \\end{pmatrix} $$\nThe columns corresponding to the contrasts $H_1$ and $H_2$ are orthogonal to each other and to the intercept column. Thus, $\\mathbf{X}_H^{\\top}\\mathbf{X}_H$ is a diagonal matrix.\n$$ \\mathbf{X}_H^{\\top}\\mathbf{X}_H = \\begin{pmatrix} 6 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 12 \\end{pmatrix}, \\quad \\mathbf{X}_H^{\\top}\\mathbf{y} = \\begin{pmatrix} 36 \\\\ 8 \\\\ -12 \\end{pmatrix} $$\nThe inverse is trivial: $(\\mathbf{X}_H^{\\top}\\mathbf{X}_H)^{-1} = \\begin{pmatrix} 1/6 & 0 & 0 \\\\ 0 & 1/4 & 0 \\\\ 0 & 0 & 1/12 \\end{pmatrix}$.\nThe OLS coefficients are:\n$$ \\hat{\\boldsymbol{\\beta}}_H = \\begin{pmatrix} 1/6 & 0 & 0 \\\\ 0 & 1/4 & 0 \\\\ 0 & 0 & 1/12 \\end{pmatrix} \\begin{pmatrix} 36 \\\\ 8 \\\\ -12 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 2 \\\\ -1 \\end{pmatrix} $$\nThe fitted values are $\\hat{\\mathbf{y}}_H = \\mathbf{X}_H\\hat{\\boldsymbol{\\beta}}_H$:\n$$ \\hat{\\mathbf{y}}_H = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & -1 \\\\ 1 & 1 & -1 \\\\ 1 & 1 & -1 \\\\ 1 & 0 & 2 \\\\ 1 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 6-2+1 \\\\ 6-2+1 \\\\ 6+2+1 \\\\ 6+2+1 \\\\ 6-2 \\\\ 6-2 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 5 \\\\ 9 \\\\ 9 \\\\ 4 \\\\ 4 \\end{pmatrix} $$\nThe fitted values are again identical.\n\nAs demonstrated, while the coefficient vectors $\\hat{\\boldsymbol{\\beta}}_D$, $\\hat{\\boldsymbol{\\beta}}_E$, and $\\hat{\\boldsymbol{\\beta}}_H$ are numerically different and have different interpretations, they all produce the same vector of fitted values $\\hat{\\mathbf{y}}$. This confirms the theoretical principle that the projection of $\\mathbf{y}$ onto the model's column space is invariant to the choice of basis for that space. As $\\hat{\\mathbf{y}}$ is invariant, so are $\\mathrm{SSE}$ and $R^2$.\n\nThe final numerical value for $R^2$ is $\\frac{14}{17} \\approx 0.8235294...$. Rounded to four significant figures, this is $0.8235$.",
            "answer": "$$ \\boxed{0.8235} $$"
        },
        {
            "introduction": "After fitting a regression model, the primary task is to translate its coefficients into scientifically meaningful quantities. This practice focuses on a logistic regression model with a multi-level categorical predictor and shows how to derive an odds ratio comparing two non-reference exposure groups . This requires you to combine multiple model coefficients, reinforcing the idea that the parameters are building blocks for answering specific research questions.",
            "id": "4899234",
            "problem": "A clinical study models the probability of a binary outcome $Y$ indicating $28$-day mortality in an intensive care unit as a function of a $3$-level exposure $A \\in \\{1,2,3\\}$ and a continuous confounder $Z$ (a severity score). The modeling approach is logistic regression with treatment coding that uses level $A=1$ as the baseline. There are no interaction terms in the model. The fitted model is\n$$\n\\operatorname{logit}\\!\\left\\{ \\Pr(Y=1 \\mid A,Z) \\right\\} \\;=\\; \\beta_0 \\;+\\; \\beta_2 \\,\\mathbf{1}(A=2) \\;+\\; \\beta_3 \\,\\mathbf{1}(A=3) \\;+\\; \\gamma Z,\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function. The maximum likelihood estimates and their estimated covariance matrix are:\n$$\n\\widehat{\\boldsymbol{\\theta}} \\;=\\; \\big(\\widehat{\\beta}_0,\\, \\widehat{\\beta}_2,\\, \\widehat{\\beta}_3,\\, \\widehat{\\gamma}\\big)^{\\top} \\;=\\; (-1.10,\\, 0.40,\\, 0.95,\\, 0.30)^{\\top},\n$$\n$$\n\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\theta}}) \\;=\\; \n\\begin{pmatrix}\n0.0900 & -0.0100 & -0.0150 & 0.0050\\\\\n-0.0100 & 0.0324 & 0.0120 & -0.0060\\\\\n-0.0150 & 0.0120 & 0.0484 & -0.0080\\\\\n0.0050 & -0.0060 & -0.0080 & 0.0225\n\\end{pmatrix}.\n$$\nStarting from the definitions of the logistic regression model and the odds ratio, and without invoking any pre-stated shortcut results, derive the closed-form expression for the odds ratio comparing $A=3$ versus $A=2$ in terms of the treatment-coded coefficients. Then, using the provided estimates and covariance matrix, compute:\n- the maximum likelihood estimate of that odds ratio, and\n- the delta-method standard error (SE) of the log-odds ratio and of the odds ratio.\n\nRound all numerical quantities to four significant figures and express standard errors as decimals. Important: For the final response to this question, report only the closed-form symbolic expression for the odds ratio as your final answer; do not include any numerical values in the final response.",
            "solution": "The problem is first validated for correctness and completeness.\n\n**Step 1: Extract Givens**\n- Outcome variable: $Y$, a binary variable for $28$-day mortality.\n- Exposure variable: $A$, a $3$-level categorical variable, $A \\in \\{1, 2, 3\\}$.\n- Confounder variable: $Z$, a continuous severity score.\n- Model: A logistic regression model given by\n$$\n\\operatorname{logit}\\!\\left\\{ \\Pr(Y=1 \\mid A,Z) \\right\\} \\;=\\; \\beta_0 \\;+\\; \\beta_2 \\,\\mathbf{1}(A=2) \\;+\\; \\beta_3 \\,\\mathbf{1}(A=3) \\;+\\; \\gamma Z\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function and level $A=1$ is the reference category.\n- Maximum likelihood estimates of the parameters:\n$$\n\\widehat{\\boldsymbol{\\theta}} \\;=\\; \\big(\\widehat{\\beta}_0,\\, \\widehat{\\beta}_2,\\, \\widehat{\\beta}_3,\\, \\widehat{\\gamma}\\big)^{\\top} \\;=\\; (-1.10,\\, 0.40,\\, 0.95,\\, 0.30)^{\\top}\n$$\n- Estimated covariance matrix of the estimators:\n$$\n\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\theta}}) \\;=\\; \n\\begin{pmatrix}\n0.0900 & -0.0100 & -0.0150 & 0.0050\\\\\n-0.0100 & 0.0324 & 0.0120 & -0.0060\\\\\n-0.0150 & 0.0120 & 0.0484 & -0.0080\\\\\n0.0050 & -0.0060 & -0.0080 & 0.0225\n\\end{pmatrix}\n$$\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is based on logistic regression, maximum likelihood estimation, and the delta method, which are fundamental and standard concepts in biostatistics. The setup is scientifically sound.\n- **Well-Posedness:** The problem is well-posed. It provides a clear model, all necessary parameter estimates, and their covariance matrix. The tasks are specific and lead to a unique, derivable solution.\n- **Objectivity:** The problem is stated in precise and objective mathematical language, free from ambiguity or subjective claims.\n- **Completeness and Consistency:** The problem is self-contained. The provided covariance matrix is symmetric, and its diagonal elements (variances) are positive, which is consistent with the properties of a valid covariance matrix.\n- **Other Flaws:** The problem is a standard application of statistical theory and does not exhibit any other flaws such as being trivial, tautological, or unverifiable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution is warranted.\n\n**Derivation of the Odds Ratio Expression**\n\nThe odds of the outcome are defined as $\\text{Odds} = \\frac{\\Pr(Y=1)}{1-\\Pr(Y=1)}$. The logistic regression model states that the natural logarithm of the odds (the logit) is a linear function of the predictors:\n$$\n\\ln\\left(\\frac{\\Pr(Y=1 \\mid A,Z)}{1-\\Pr(Y=1 \\mid A,Z)}\\right) = \\beta_0 + \\beta_2 \\mathbf{1}(A=2) + \\beta_3 \\mathbf{1}(A=3) + \\gamma Z\n$$\nBy exponentiating both sides, we find the odds:\n$$\n\\text{Odds}(A,Z) = \\exp\\left(\\beta_0 + \\beta_2 \\mathbf{1}(A=2) + \\beta_3 \\mathbf{1}(A=3) + \\gamma Z\\right)\n$$\nThe odds ratio ($\\text{OR}$) comparing exposure level $A=3$ to $A=2$ is the ratio of their respective odds, holding the confounder $Z$ constant at an arbitrary level $z$:\n$$\n\\text{OR}_{3 \\text{ vs } 2} = \\frac{\\text{Odds}(A=3, Z=z)}{\\text{Odds}(A=2, Z=z)}\n$$\nFor a subject with $A=3$ and $Z=z$, the indicator functions are $\\mathbf{1}(A=2)=0$ and $\\mathbf{1}(A=3)=1$. The odds are:\n$$\n\\text{Odds}(A=3, Z=z) = \\exp(\\beta_0 + \\beta_2(0) + \\beta_3(1) + \\gamma z) = \\exp(\\beta_0 + \\beta_3 + \\gamma z)\n$$\nFor a subject with $A=2$ and $Z=z$, the indicator functions are $\\mathbf{1}(A=2)=1$ and $\\mathbf{1}(A=3)=0$. The odds are:\n$$\n\\text{Odds}(A=2, Z=z) = \\exp(\\beta_0 + \\beta_2(1) + \\beta_3(0) + \\gamma z) = \\exp(\\beta_0 + \\beta_2 + \\gamma z)\n$$\nThe odds ratio is the ratio of these two expressions:\n$$\n\\text{OR}_{3 \\text{ vs } 2} = \\frac{\\exp(\\beta_0 + \\beta_3 + \\gamma z)}{\\exp(\\beta_0 + \\beta_2 + \\gamma z)}\n$$\nUsing the property of exponents $\\frac{\\exp(a)}{\\exp(b)} = \\exp(a-b)$, we get:\n$$\n\\text{OR}_{3 \\text{ vs } 2} = \\exp\\left( (\\beta_0 + \\beta_3 + \\gamma z) - (\\beta_0 + \\beta_2 + \\gamma z) \\right) = \\exp(\\beta_3 - \\beta_2)\n$$\nThis is the required closed-form expression.\n\n**Numerical Computations**\n\n1.  **Maximum Likelihood Estimate of the Odds Ratio:**\nBy the invariance property of maximum likelihood estimators (MLEs), the MLE of $\\text{OR}_{3 \\text{ vs } 2}$ is obtained by substituting the MLEs of the parameters into the expression:\n$$\n\\widehat{\\text{OR}}_{3 \\text{ vs } 2} = \\exp(\\widehat{\\beta}_3 - \\widehat{\\beta}_2)\n$$\nUsing the given values $\\widehat{\\beta}_2 = 0.40$ and $\\widehat{\\beta}_3 = 0.95$:\n$$\n\\widehat{\\text{OR}}_{3 \\text{ vs } 2} = \\exp(0.95 - 0.40) = \\exp(0.55) \\approx 1.73325\n$$\nRounding to four significant figures, the estimated odds ratio is $1.733$.\n\n2.  **Standard Error of the Log-Odds Ratio:**\nLet $\\lambda = \\beta_3 - \\beta_2$ be the log-odds ratio. Its MLE is $\\widehat{\\lambda} = \\widehat{\\beta}_3 - \\widehat{\\beta}_2$. The variance of this linear combination of estimators is:\n$$\n\\operatorname{Var}(\\widehat{\\lambda}) = \\operatorname{Var}(\\widehat{\\beta}_3 - \\widehat{\\beta}_2) = \\operatorname{Var}(\\widehat{\\beta}_3) + \\operatorname{Var}(\\widehat{\\beta}_2) - 2\\operatorname{Cov}(\\widehat{\\beta}_2, \\widehat{\\beta}_3)\n$$\nFrom the provided covariance matrix $\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\theta}})$, we extract the required estimated variances and covariance:\n$\\widehat{\\operatorname{Var}}(\\widehat{\\beta}_2) = 0.0324$ (element $(2,2)$)\n$\\widehat{\\operatorname{Var}}(\\widehat{\\beta}_3) = 0.0484$ (element $(3,3)$)\n$\\widehat{\\operatorname{Cov}}(\\widehat{\\beta}_2, \\widehat{\\beta}_3) = 0.0120$ (element $(2,3)$)\nSubstituting these values:\n$$\n\\widehat{\\operatorname{Var}}(\\widehat{\\lambda}) = 0.0484 + 0.0324 - 2(0.0120) = 0.0808 - 0.0240 = 0.0568\n$$\nThe standard error (SE) is the square root of the variance:\n$$\n\\text{SE}(\\widehat{\\lambda}) = \\sqrt{\\widehat{\\operatorname{Var}}(\\widehat{\\lambda})} = \\sqrt{0.0568} \\approx 0.2383275\n$$\nRounding to four significant figures, $\\text{SE}(\\widehat{\\lambda}) = 0.2383$.\n\n3.  **Standard Error of the Odds Ratio:**\nWe use the delta method to find the approximate SE of the odds ratio, which is a function $g(\\lambda) = \\exp(\\lambda)$ of the log-odds ratio $\\lambda$. The approximate variance is given by $\\operatorname{Var}(g(\\widehat{\\lambda})) \\approx [g'(\\lambda)]^2 \\operatorname{Var}(\\widehat{\\lambda})$, evaluated at the estimates.\nThe derivative is $g'(\\lambda) = \\exp(\\lambda)$. At the estimate $\\widehat{\\lambda}$, this is $g'(\\widehat{\\lambda}) = \\exp(\\widehat{\\lambda}) = \\widehat{\\text{OR}}$.\nThe estimated variance of the odds ratio is:\n$$\n\\widehat{\\operatorname{Var}}(\\widehat{\\text{OR}}) \\approx (\\widehat{\\text{OR}})^2 \\widehat{\\operatorname{Var}}(\\widehat{\\lambda})\n$$\nThe standard error is the square root:\n$$\n\\text{SE}(\\widehat{\\text{OR}}) \\approx \\sqrt{(\\widehat{\\text{OR}})^2 \\widehat{\\operatorname{Var}}(\\widehat{\\lambda})} = |\\widehat{\\text{OR}}| \\sqrt{\\widehat{\\operatorname{Var}}(\\widehat{\\lambda})} = \\widehat{\\text{OR}} \\times \\text{SE}(\\widehat{\\lambda})\n$$\n(since the odds ratio is positive). Using the previously computed values:\n$$\n\\text{SE}(\\widehat{\\text{OR}}) \\approx 1.73325 \\times 0.2383275 \\approx 0.413088\n$$\nRounding to four significant figures, $\\text{SE}(\\widehat{\\text{OR}}) = 0.4131$.",
            "answer": "$$\n\\boxed{\\exp(\\beta_3 - \\beta_2)}\n$$"
        },
        {
            "introduction": "A key question in biostatistics is whether the effect of an exposure is consistent across different subgroups, a concept known as interaction or effect modification. This exercise guides you through the formal process of testing for interaction in a logistic regression model using a multivariate Wald test . You will learn how to set up the null hypothesis for no interaction and use the estimated coefficients and their covariance matrix to compute a single test statistic that evaluates the joint significance of all interaction terms.",
            "id": "4899236",
            "problem": "A randomized epidemiologic study investigates whether the association between a binary treatment indicator $A \\in \\{0,1\\}$ and a binary clinical outcome $Y \\in \\{0,1\\}$ varies across levels of a categorical biomarker $B$ having $J=4$ levels $\\{B_1,B_2,B_3,B_4\\}$, while adjusting for a continuous confounder $C$. A logistic Generalized Linear Model (GLM) with logit link is fit using reference cell coding for $B$ with $B_1$ as the reference and including first-order interactions between $A$ and $B$. The model is\n$$\n\\log\\left(\\frac{\\Pr(Y=1 \\mid A,B,C)}{1-\\Pr(Y=1 \\mid A,B,C)}\\right)\n=\n\\beta_{0}+\\beta_{A}A+\\beta_{B_{2}}\\mathbf{1}(B=B_{2})+\\beta_{B_{3}}\\mathbf{1}(B=B_{3})+\\beta_{B_{4}}\\mathbf{1}(B=B_{4})\n+\\beta_{AB_{2}}\\,A\\,\\mathbf{1}(B=B_{2})+\\beta_{AB_{3}}\\,A\\,\\mathbf{1}(B=B_{3})+\\beta_{AB_{4}}\\,A\\,\\mathbf{1}(B=B_{4})\n+\\beta_{C}C.\n$$\nLet the regression coefficient vector be ordered as\n$$\n\\boldsymbol{\\beta}=\n\\begin{pmatrix}\n\\beta_{0} & \\beta_{A} & \\beta_{B_{2}} & \\beta_{B_{3}} & \\beta_{B_{4}} & \\beta_{AB_{2}} & \\beta_{AB_{3}} & \\beta_{AB_{4}} & \\beta_{C}\n\\end{pmatrix}^{\\top}.\n$$\nYou are interested in testing whether there is any interaction between $A$ and $B$ on the log-odds scale. Formulate the null hypothesis that all interaction terms are zero. Then, using the Wald test based on Maximum Likelihood Estimation (MLE), construct the appropriate contrast matrix that tests the interaction terms jointly with degrees of freedom equal to $J-1$.\n\nFrom the fitted model, the estimated interaction coefficients and their estimated covariance matrix are:\n$$\n\\widehat{\\boldsymbol{\\beta}}_{AB}=\n\\begin{pmatrix}\n\\widehat{\\beta}_{AB_{2}} \\\\\n\\widehat{\\beta}_{AB_{3}} \\\\\n\\widehat{\\beta}_{AB_{4}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.42 \\\\\n-0.18 \\\\\n0.30\n\\end{pmatrix},\n\\qquad\n\\widehat{\\boldsymbol{\\Sigma}}_{AB}=\n\\begin{pmatrix}\n0.040 & 0.005 & -0.004 \\\\\n0.005 & 0.050 & 0.006 \\\\\n-0.004 & 0.006 & 0.045\n\\end{pmatrix}.\n$$\nUse the large-sample Wald test to compute the test statistic for the interaction, taking the contrast to target these three interaction coefficients. Explicitly specify the contrast matrix relative to the ordering of $\\boldsymbol{\\beta}$ above, and then compute the Wald test statistic\n$$\nW=(\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}})^{\\top}\\left[\\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top}\\right]^{-1}(\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}}),\n$$\nnoting that $\\widehat{\\boldsymbol{\\Sigma}}_{AB}$ is the submatrix $\\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top}$ for the interaction terms. State the degrees of freedom and report the value of $W$.\n\nRound your final numerical answer for $W$ to four significant figures. The final answer you provide must be the Wald statistic $W$ only, with no units.",
            "solution": "The user wants me to solve a biostatistics problem involving a logistic regression model.\n\n### Step 1: Extract Givens\n-   **Model:** A logistic GLM with a logit link is given by:\n    $$\n    \\log\\left(\\frac{\\Pr(Y=1 \\mid A,B,C)}{1-\\Pr(Y=1 \\mid A,B,C)}\\right)\n    =\n    \\beta_{0}+\\beta_{A}A+\\beta_{B_{2}}\\mathbf{1}(B=B_{2})+\\beta_{B_{3}}\\mathbf{1}(B=B_{3})+\\beta_{B_{4}}\\mathbf{1}(B=B_{4})\n    +\\beta_{AB_{2}}\\,A\\,\\mathbf{1}(B=B_{2})+\\beta_{AB_{3}}\\,A\\,\\mathbf{1}(B=B_{3})+\\beta_{AB_{4}}\\,A\\,\\mathbf{1}(B=B_{4})\n    +\\beta_{C}C.\n    $$\n-   **Variables and Parameters:**\n    -   $Y$: binary clinical outcome, $Y \\in \\{0,1\\}$.\n    -   $A$: binary treatment indicator, $A \\in \\{0,1\\}$.\n    -   $B$: categorical biomarker with $J=4$ levels, $\\{B_{1}, B_{2}, B_{3}, B_{4}\\}$. $B_{1}$ is the reference level.\n    -   $C$: continuous confounder.\n    -   $\\mathbf{1}(\\cdot)$: indicator function.\n-   **Coefficient Vector:** The ordered coefficient vector is:\n    $$\n    \\boldsymbol{\\beta}=\n    \\begin{pmatrix}\n    \\beta_{0} & \\beta_{A} & \\beta_{B_{2}} & \\beta_{B_{3}} & \\beta_{B_{4}} & \\beta_{AB_{2}} & \\beta_{AB_{3}} & \\beta_{AB_{4}} & \\beta_{C}\n    \\end{pmatrix}^{\\top}.\n    $$\n-   **Estimated Interaction Coefficients and Covariance Matrix:**\n    $$\n    \\widehat{\\boldsymbol{\\beta}}_{AB}=\n    \\begin{pmatrix}\n    \\widehat{\\beta}_{AB_{2}} \\\\\n    \\widehat{\\beta}_{AB_{3}} \\\\\n    \\widehat{\\beta}_{AB_{4}}\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n    0.42 \\\\\n    -0.18 \\\\\n    0.30\n    \\end{pmatrix},\n    \\qquad\n    \\widehat{\\boldsymbol{\\Sigma}}_{AB}=\n    \\begin{pmatrix}\n    0.040 & 0.005 & -0.004 \\\\\n    0.005 & 0.050 & 0.006 \\\\\n    -0.004 & 0.006 & 0.045\n    \\end{pmatrix}.\n    $$\n-   **Task:** Formulate the null hypothesis for no interaction between $A$ and $B$. Construct the contrast matrix $\\mathbf{C}$ for a Wald test. Compute the Wald test statistic $W$ and state its degrees of freedom.\n-   **Wald Test Statistic Formula:**\n    $$\n    W=(\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}})^{\\top}\\left[\\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top}\\right]^{-1}(\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}}).\n    $$\n-   **Rounding:** The final numerical answer for $W$ must be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is a standard application of generalized linear models (logistic regression) in biostatistics. The model specification, use of reference cell coding for a categorical predictor, inclusion of interaction terms, and the application of a multivariate Wald test are all well-established and fundamental statistical concepts. The problem is scientifically and mathematically sound.\n-   **Well-Posed:** All necessary information is provided. A clear question is asked (compute the Wald statistic for interaction), and the required data (estimated coefficients and their covariance matrix) are supplied. The model structure is explicit. A unique, stable, and meaningful solution exists.\n-   **Objective:** The problem is posed using precise, standard statistical terminology. There are no subjective or ambiguous statements.\n-   The problem does not exhibit any of the flaws listed in the instructions (e.g., non-formalizable, incomplete, unrealistic). The given covariance matrix is symmetric with positive diagonal elements, as required for a valid covariance matrix.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\nThe problem asks for a joint test of the interaction between the treatment $A$ and the categorical biomarker $B$. The specified logistic regression model includes terms for this interaction.\n\nFirst, we formulate the null hypothesis. The interaction between $A$ and $B$ is captured by the coefficients $\\beta_{AB_{j}}$ for $j=2, 3, 4$. The interaction term for the reference level, $\\beta_{AB_{1}}$, is structurally zero due to the reference cell coding scheme used. Therefore, the null hypothesis of no interaction is that all non-redundant interaction coefficients are simultaneously zero.\n$$ H_{0}: \\beta_{AB_{2}} = 0, \\beta_{AB_{3}} = 0, \\beta_{AB_{4}} = 0. $$\nThis can be written in matrix form as $\\mathbf{C}\\boldsymbol{\\beta} = \\mathbf{0}$, where $\\mathbf{C}$ is a contrast matrix and $\\boldsymbol{\\beta}$ is the vector of all model coefficients.\n\nThe full coefficient vector $\\boldsymbol{\\beta}$ is a $9 \\times 1$ vector ordered as:\n$$ \\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_{0} & \\beta_{A} & \\beta_{B_{2}} & \\beta_{B_{3}} & \\beta_{B_{4}} & \\beta_{AB_{2}} & \\beta_{AB_{3}} & \\beta_{AB_{4}} & \\beta_{C} \\end{pmatrix}^{\\top} $$\nThe coefficients of interest, $\\beta_{AB_{2}}$, $\\beta_{AB_{3}}$, and $\\beta_{AB_{4}}$, are the 6th, 7th, and 8th elements of this vector. To test the null hypothesis that these three coefficients are zero, we construct a $3 \\times 9$ contrast matrix $\\mathbf{C}$ where each row selects one of these coefficients.\n$$ \\mathbf{C} = \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\end{pmatrix} $$\nThe degrees of freedom for this test are equal to the number of rows in $\\mathbf{C}$, which is the number of parameters being jointly tested. Here, the biomarker $B$ has $J=4$ levels, so there are $J-1=3$ interaction terms. The degrees of freedom are $3$.\n\nThe large-sample Wald test statistic is given by:\n$$ W = (\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}})^{\\top}\\left[\\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top}\\right]^{-1}(\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}}) $$\nBased on our definition of $\\mathbf{C}$:\n- The term $\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}}$ extracts the estimated interaction coefficients:\n$$ \\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\widehat{\\beta}_{AB_{2}} \\\\ \\widehat{\\beta}_{AB_{3}} \\\\ \\widehat{\\beta}_{AB_{4}} \\end{pmatrix} = \\widehat{\\boldsymbol{\\beta}}_{AB} = \\begin{pmatrix} 0.42 \\\\ -0.18 \\\\ 0.30 \\end{pmatrix} $$\n- The term $\\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top}$ extracts the submatrix of the full covariance matrix corresponding to these interaction coefficients. This submatrix is provided in the problem as $\\widehat{\\boldsymbol{\\Sigma}}_{AB}$:\n$$ \\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top} = \\widehat{\\boldsymbol{\\Sigma}}_{AB} = \\begin{pmatrix} 0.040 & 0.005 & -0.004 \\\\ 0.005 & 0.050 & 0.006 \\\\ -0.004 & 0.006 & 0.045 \\end{pmatrix} $$\nThus, the Wald statistic calculation simplifies to:\n$$ W = \\widehat{\\boldsymbol{\\beta}}_{AB}^{\\top} \\widehat{\\boldsymbol{\\Sigma}}_{AB}^{-1} \\widehat{\\boldsymbol{\\beta}}_{AB} $$\nWe must first compute the inverse of $\\widehat{\\boldsymbol{\\Sigma}}_{AB}$. The determinant of $\\widehat{\\boldsymbol{\\Sigma}}_{AB}$ is:\n$$ \\det(\\widehat{\\boldsymbol{\\Sigma}}_{AB}) = 0.040(0.050 \\times 0.045 - 0.006 \\times 0.006) - 0.005(0.005 \\times 0.045 - 0.006 \\times (-0.004)) + (-0.004)(0.005 \\times 0.006 - 0.050 \\times (-0.004)) $$\n$$ \\det(\\widehat{\\boldsymbol{\\Sigma}}_{AB}) = 0.040(0.002214) - 0.005(0.000249) - 0.004(0.000230) $$\n$$ \\det(\\widehat{\\boldsymbol{\\Sigma}}_{AB}) = 0.00008856 - 0.000001245 - 0.00000092 = 0.000086395 $$\nThe adjugate matrix, $\\operatorname{adj}(\\widehat{\\boldsymbol{\\Sigma}}_{AB})$, is the transpose of the cofactor matrix. Since $\\widehat{\\boldsymbol{\\Sigma}}_{AB}$ is symmetric, its cofactor matrix is also symmetric, and thus equals the adjugate.\n$$ \\operatorname{adj}(\\widehat{\\boldsymbol{\\Sigma}}_{AB}) = \\begin{pmatrix} 0.002214 & -0.000249 & 0.000230 \\\\ -0.000249 & 0.001784 & -0.000260 \\\\ 0.000230 & -0.000260 & 0.001975 \\end{pmatrix} $$\nThe inverse matrix is $\\widehat{\\boldsymbol{\\Sigma}}_{AB}^{-1} = \\frac{1}{\\det(\\widehat{\\boldsymbol{\\Sigma}}_{AB})} \\operatorname{adj}(\\widehat{\\boldsymbol{\\Sigma}}_{AB})$:\n$$ \\widehat{\\boldsymbol{\\Sigma}}_{AB}^{-1} = \\frac{1}{0.000086395} \\begin{pmatrix} 0.002214 & -0.000249 & 0.000230 \\\\ -0.000249 & 0.001784 & -0.000260 \\\\ 0.000230 & -0.000260 & 0.001975 \\end{pmatrix} \\approx \\begin{pmatrix} 25.6263 & -2.8821 & 2.6622 \\\\ -2.8821 & 20.6493 & -3.0094 \\\\ 2.6622 & -3.0094 & 22.8601 \\end{pmatrix} $$\nNow we compute the quadratic form for $W$:\n$$ W = \\begin{pmatrix} 0.42 & -0.18 & 0.30 \\end{pmatrix} \\widehat{\\boldsymbol{\\Sigma}}_{AB}^{-1} \\begin{pmatrix} 0.42 \\\\ -0.18 \\\\ 0.30 \\end{pmatrix} $$\nFirst, we compute the product $\\widehat{\\boldsymbol{\\beta}}_{AB}^{\\top} \\widehat{\\boldsymbol{\\Sigma}}_{AB}^{-1}$:\n$$ \\begin{pmatrix} 0.42 & -0.18 & 0.30 \\end{pmatrix} \\begin{pmatrix} 25.6263 & -2.8821 & 2.6622 \\\\ -2.8821 & 20.6493 & -3.0094 \\\\ 2.6622 & -3.0094 & 22.8601 \\end{pmatrix} $$\nThis gives a row vector:\n$$ \\approx \\begin{pmatrix} (10.7630 + 0.5188 + 0.7987) & (-1.2105 - 3.7169 - 0.9028) & (1.1181 + 0.5417 + 6.8580) \\end{pmatrix} $$\n$$ \\approx \\begin{pmatrix} 12.0805 & -5.8302 & 8.5178 \\end{pmatrix} $$\nFinally, we multiply this row vector by $\\widehat{\\boldsymbol{\\beta}}_{AB}$:\n$$ W \\approx \\begin{pmatrix} 12.0805 & -5.8302 & 8.5178 \\end{pmatrix} \\begin{pmatrix} 0.42 \\\\ -0.18 \\\\ 0.30 \\end{pmatrix} $$\n$$ W \\approx 12.0805 \\times 0.42 + (-5.8302) \\times (-0.18) + 8.5178 \\times 0.30 $$\n$$ W \\approx 5.07381 + 1.04944 + 2.55534 = 8.67859 $$\nUnder the null hypothesis, $W$ follows a chi-squared distribution with $3$ degrees of freedom. The calculated value of the statistic is approximately $8.67859$.\nRounding to four significant figures, we get $W = 8.679$.",
            "answer": "$$\n\\boxed{8.679}\n$$"
        }
    ]
}