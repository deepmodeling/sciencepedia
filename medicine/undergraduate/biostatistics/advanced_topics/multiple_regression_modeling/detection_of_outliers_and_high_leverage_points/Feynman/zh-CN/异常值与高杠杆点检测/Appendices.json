{
    "hands_on_practices": [
        {
            "introduction": "要理解一个数据点对回归模型的影响力，首先需要掌握杠杆值（leverage）的概念。本练习将通过一个简单的线性回归模型，从第一性原理出发推导杠杆值的计算公式 。通过这个过程，我们将建立一个清晰的直觉：为什么某些数据点，特别是那些预测变量值远离数据中心的数据点，对回归线有更大的潜在影响力。",
            "id": "4908310",
            "problem": "一项生物统计学研究使用带截距项的简单线性回归模型，对一个连续的临床结局 $y$ 和一个经过中心化处理的单个预测变量 $x$（使得 $\\sum_{i=1}^{n} x_{i} = 0$）进行建模。用于普通最小二乘（OLS）回归的设计矩阵有两列：一列全为1，代表截距项；另一列是中心化后的预测变量 $x$。考虑一个包含 $n = 6$ 个受试者的特定数据集，其中心化预测变量的值为\n$$\nx = \\{-4,\\,-2,\\,-1,\\,1,\\,3,\\,3\\}。\n$$\n从 OLS 帽子矩阵 $H$ 的定义 $H = X\\left(X^{\\top}X\\right)^{-1}X^{\\top}$ 出发，推导在这种中心化设计下对角杠杆值元素 $h_{ii}$ 的表达式，并计算 $x_{6} = 3$ 的受试者所对应的杠杆值 $h_{66}$。将你的最终数值答案表示为一个精确分数。在推导过程中，请解释在这种中心化预测变量的设计中，哪些 $x_i$ 值会驱动杠杆值，并说明原因。",
            "solution": "用户希望我解决以下问题：\n一项生物统计学研究使用带截距项的简单线性回归模型，对一个连续的临床结局 $y$ 和一个经过中心化处理的单个预测变量 $x$（使得 $\\sum_{i=1}^{n} x_{i} = 0$）进行建模。用于普通最小二乘（OLS）回归的设计矩阵有两列：一列全为1，代表截距项；另一列是中心化后的预测变量 $x$。考虑一个包含 $n = 6$ 个受试者的特定数据集，其中心化预测变量的值为\n$$\nx = \\{-4,\\,-2,\\,-1,\\,1,\\,3,\\,3\\}。\n$$\n从 OLS 帽子矩阵 $H$ 的定义 $H = X\\left(X^{\\top}X\\right)^{-1}X^{\\top}$ 出发，推导在这种中心化设计下对角杠杆值元素 $h_{ii}$ 的表达式，并计算 $x_{6} = 3$ 的受试者所对应的杠杆值 $h_{66}$。将你的最终数值答案表示为一个精确分数。在推导过程中，请解释在这种中心化预测变量的设计中，哪些 $x_i$ 值会驱动杠杆值，并说明原因。\n\n### 第1步：问题验证\n\n**1.1. 提取已知条件：**\n*   **模型：** 带截距项的简单线性回归：$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$。\n*   **预测变量属性：** 预测变量 $x$ 是中心化的，即 $\\sum_{i=1}^{n} x_{i} = 0$。\n*   **设计矩阵 $X$：** 有两列，一列是代表截距项的全1列，另一列是中心化后的预测变量 $x$。\n*   **数据集大小：** $n = 6$。\n*   **预测变量值：** $x = \\{-4,\\,-2,\\,-1,\\,1,\\,3,\\,3\\}$。\n*   **定义：** 帽子矩阵为 $H = X\\left(X^{\\top}X\\right)^{-1}X^{\\top}$。\n*   **任务1：** 推导中心化设计下对角杠杆值元素 $h_{ii}$ 的通用公式。\n*   **任务2：** 计算 $x_6 = 3$ 的受试者的具体杠杆值 $h_{66}$。\n*   **任务3：** 将最终答案表示为精确分数。\n*   **任务4：** 解释在该设计中是什么驱动了杠杆值。\n\n**1.2. 依据标准进行验证：**\n*   **科学依据：** 该问题是线性回归分析中的一个标准练习，这是生物统计学的核心主题。帽子矩阵、杠杆值和中心化预测变量等概念都是基础性的，并且表述正确。\n*   **适定性：** 该问题是适定的。它提供了执行推导和计算所需的所有数据和定义。指定的预测变量值 $x = \\{-4,\\,-2,\\,-1,\\,1,\\,3,\\,3\\}$ 满足中心化条件：$-4 - 2 - 1 + 1 + 3 + 3 = 0$。\n*   **客观性：** 该问题以精确、客观、无歧义的数学语言陈述。\n*   **结论：** 该问题有效。它在科学上是合理的、适定的和客观的。它没有违反任何指定的无效标准。\n\n### 第2步：求解推导\n\n求解过程首先在带中心化预测变量的简单线性回归模型中推导杠杆值 $h_{ii}$ 的通用公式，然后将此公式应用于给定的特定数据。\n\n模型为 $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$。对于 $n$ 个观测值，设计矩阵 $X$ 是一个 $n \\times 2$ 的矩阵，其中第一列是代表截距项 $\\beta_0$ 的全1向量，第二列包含中心化预测变量 $x$ 的值。\n$$\nX = \\begin{pmatrix}\n1  x_1 \\\\\n1  x_2 \\\\\n\\vdots  \\vdots \\\\\n1  x_n\n\\end{pmatrix}\n$$\n帽子矩阵定义为 $H = X\\left(X^{\\top}X\\right)^{-1}X^{\\top}$。其对角元素 $h_{ii}$ 是每个观测值的杠杆分数。杠杆值 $h_{ii}$ 可以通过 $X$ 的第 $i$ 行、矩阵 $\\left(X^{\\top}X\\right)^{-1}$ 以及 $X^{\\top}$ 的第 $i$ 列（即 $X$ 的第 $i$ 行的转置）的乘积来计算。令 $X_{i\\cdot}$ 表示 $X$ 的第 $i$ 行，因此 $X_{i\\cdot} = \\begin{pmatrix} 1  x_i \\end{pmatrix}$。那么，$h_{ii} = X_{i\\cdot}\\left(X^{\\top}X\\right)^{-1}X_{i\\cdot}^{\\top}$。\n\n首先，我们计算矩阵 $X^{\\top}X$：\n$$\nX^{\\top}X = \\begin{pmatrix}\n1  1  \\cdots  1 \\\\\nx_1  x_2  \\cdots  x_n\n\\end{pmatrix}\n\\begin{pmatrix}\n1  x_1 \\\\\n1  x_2 \\\\\n\\vdots  \\vdots \\\\\n1  x_n\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\sum_{j=1}^{n} 1  \\sum_{j=1}^{n} x_j \\\\\n\\sum_{j=1}^{n} x_j  \\sum_{j=1}^{n} x_j^2\n\\end{pmatrix}\n$$\n题目指出预测变量 $x$ 是中心化的，这意味着 $\\sum_{j=1}^{n} x_j = 0$。将此代入矩阵中，得到：\n$$\nX^{\\top}X = \\begin{pmatrix}\nn  0 \\\\\n0  \\sum_{j=1}^{n} x_j^2\n\\end{pmatrix}\n$$\n预测变量的中心化使得 $X^{\\top}X$ 矩阵成为一个对角矩阵，这简化了求逆过程。其逆矩阵 $\\left(X^{\\top}X\\right)^{-1}$ 可以通过取每个对角元素的倒数得到：\n$$\n\\left(X^{\\top}X\\right)^{-1} = \\begin{pmatrix}\n\\frac{1}{n}  0 \\\\\n0  \\frac{1}{\\sum_{j=1}^{n} x_j^2}\n\\end{pmatrix}\n$$\n现在，我们可以推导 $h_{ii}$ 的公式：\n$$\nh_{ii} = X_{i\\cdot}\\left(X^{\\top}X\\right)^{-1}X_{i\\cdot}^{\\top} =\n\\begin{pmatrix} 1  x_i \\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{n}  0 \\\\\n0  \\frac{1}{\\sum_{j=1}^{n} x_j^2}\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}\n$$\n从左到右执行矩阵乘法：\n$$\nh_{ii} = \\begin{pmatrix} 1 \\cdot \\frac{1}{n} + x_i \\cdot 0  1 \\cdot 0 + x_i \\cdot \\frac{1}{\\sum_{j=1}^{n} x_j^2} \\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}\n= \\begin{pmatrix} \\frac{1}{n}  \\frac{x_i}{\\sum_{j=1}^{n} x_j^2} \\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}\n$$\n完成最后的乘法：\n$$\nh_{ii} = \\left(\\frac{1}{n}\\right)(1) + \\left(\\frac{x_i}{\\sum_{j=1}^{n} x_j^2}\\right)(x_i) = \\frac{1}{n} + \\frac{x_i^2}{\\sum_{j=1}^{n} x_j^2}\n$$\n这就是在带有中心化预测变量的简单线性回归中，第 $i$ 个观测值的杠杆值的通用表达式。\n\n这个公式直接回答了是什么驱动杠杆值的问题。观测值 $i$ 的杠杆值 $h_{ii}$ 由两项组成。第一项 $\\frac{1}{n}$ 对数据集中的所有观测值都是一个常数。第二项 $\\frac{x_i^2}{\\sum_{j=1}^{n} x_j^2}$ 是特定于观测值 $i$ 的。由于分母 $\\sum_{j=1}^{n} x_j^2$ 对于给定的数据集是一个常数，因此第二项的值与 $x_i^2$ 成正比。因为预测变量 $x$ 是中心化的，其均值为 $0$。因此，$x_i^2 = (x_i - \\bar{x})^2$ 是预测变量值与其均值之间距离的平方。因此，预测变量值 $x_i$ 远离数据中心（即 $|x_i|$ 较大）的观测值将具有较大的 $x_i^2$，从而具有高杠杆值。这些点具有影响力，因为它们在预测变量空间中的位置使其更有可能将回归线拉向自己。\n\n现在，我们为给定的数据集计算具体的 $h_{66}$ 值。\n数据为 $n=6$，预测变量值为 $x = \\{-4,\\,-2,\\,-1,\\,1,\\,3,\\,3\\}$。\n首先，我们计算中心化预测变量值的平方和：\n$$\n\\sum_{j=1}^{6} x_j^2 = (-4)^2 + (-2)^2 + (-1)^2 + 1^2 + 3^2 + 3^2\n$$\n$$\n\\sum_{j=1}^{6} x_j^2 = 16 + 4 + 1 + 1 + 9 + 9 = 40\n$$\n我们感兴趣的观测值是第六个受试者，其 $x_6 = 3$。\n使用推导出的 $h_{ii}$ 公式，代入 $i=6$，$n=6$，$x_6=3$ 和 $\\sum x_j^2 = 40$：\n$$\nh_{66} = \\frac{1}{n} + \\frac{x_6^2}{\\sum_{j=1}^{6} x_j^2} = \\frac{1}{6} + \\frac{3^2}{40} = \\frac{1}{6} + \\frac{9}{40}\n$$\n为了将其表示为单个分数，我们找到一个公分母，即 $6$ 和 $40$ 的最小公倍数。$\\text{lcm}(6, 40) = 120$。\n$$\nh_{66} = \\frac{1 \\cdot 20}{6 \\cdot 20} + \\frac{9 \\cdot 3}{40 \\cdot 3} = \\frac{20}{120} + \\frac{27}{120} = \\frac{47}{120}\n$$\n$x_6=3$ 的受试者的杠杆值为 $\\frac{47}{120}$。",
            "answer": "$$\\boxed{\\frac{47}{120}}$$"
        },
        {
            "introduction": "一个高杠杆点不一定是离群点，但它会使离群点的检测变得复杂。本练习将揭示一个被称为“掩盖效应”（masking effect）的关键现象：一个高杠杆观测点会把它自己“拉向”回归线，导致其原始残差看起来很小 。我们将计算并比较内学生化残差和外学生化残差，看看后者如何修正这种效应，从而为识别真实离群点提供一个更可靠的工具。",
            "id": "4908256",
            "problem": "一项生物统计学研究使用多元线性回归来建立一个连续生物标志物测量值与几个协变量之间的关系。假设采用标准同方差高斯线性模型：对于索引为 $i=1,\\dots,n$ 的受试者，模型为 $y_i = \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta} + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ 独立同分布。最小二乘估计量得出拟合值 $\\hat{y}_i$ 和残差 $e_i = y_i - \\hat{y}_i$。“帽子矩阵” $\\boldsymbol{H}$ 通过 $\\hat{\\boldsymbol{y}} = \\boldsymbol{H}\\boldsymbol{y}$ 将观测响应值映射到拟合值，观测值 $i$ 的杠杆值为 $h_{ii}$，即 $\\boldsymbol{H}$ 的第 $i$ 个对角元素。对于一个被怀疑是高杠杆值的观测点，从完整模型的拟合中给出以下信息：\n- 样本量 $n = 30$，拟合系数的数量 $p = 3$（包括截距），\n- 观测点级别的数据：$y_i = 12.5$，$\\hat{y}_i = 10.8$，$h_{ii} = 0.82$，\n- 来自完整模型的无偏误差方差估计为 $\\hat{\\sigma}^2 = 1.44$，它等于均方误差 (MSE)，即 $\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n-p}$，其中 $\\text{SSE} = \\sum_{j=1}^{n} e_j^2$ 是残差平方和 (SSE)。\n\n从线性模型的基本原理和上述定义出发，计算观测点 $i$ 的内学生化残差和外学生化（删除）残差。将这两个值表示为行向量 $\\begin{pmatrix}\\text{内}  \\text{外}\\end{pmatrix}$。将您的数值答案四舍五入到四位有效数字。不需要单位。",
            "solution": "该问题是有效的。这是一个回归诊断中的标准、良构问题，提供了所有必要信息，并且没有科学或逻辑上的不一致之处。\n\n该问题要求计算特定观测值 $i$ 的内学生化残差和外学生化残差。我们将从这些量的基本定义出发。\n\n线性模型定义为 $y_i = \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta} + \\varepsilon_i$，其中误差 $\\varepsilon_i$ 独立同分布于 $\\mathcal{N}(0, \\sigma^2)$。$\\boldsymbol{\\beta}$ 的普通最小二乘 (OLS) 估计得到拟合值 $\\hat{y}_i$ 和原始残差 $e_i = y_i - \\hat{y}_i$。\n\n第 $i$ 个残差的方差可以从帽子矩阵 $\\boldsymbol{H}$ 的性质推导得出。残差向量为 $\\boldsymbol{e} = (\\boldsymbol{I}-\\boldsymbol{H})\\boldsymbol{y}$。由于 $\\mathbb{E}[\\boldsymbol{y}] = \\boldsymbol{X}\\boldsymbol{\\beta}$ 且 $\\boldsymbol{H}\\boldsymbol{X} = \\boldsymbol{X}$，我们有 $\\mathbb{E}[\\boldsymbol{e}] = (\\boldsymbol{I}-\\boldsymbol{H})\\boldsymbol{X}\\boldsymbol{\\beta} = \\boldsymbol{X}\\boldsymbol{\\beta} - \\boldsymbol{X}\\boldsymbol{\\beta} = \\boldsymbol{0}$。$\\boldsymbol{e}$ 的方差-协方差矩阵为 $\\text{Var}(\\boldsymbol{e}) = (\\boldsymbol{I}-\\boldsymbol{H})\\text{Var}(\\boldsymbol{y})(\\boldsymbol{I}-\\boldsymbol{H})^{\\top}$。由于 $\\text{Var}(\\boldsymbol{y}) = \\sigma^2\\boldsymbol{I}$ 且 $\\boldsymbol{I}-\\boldsymbol{H}$ 是对称且幂等的，该式可简化为 $\\text{Var}(\\boldsymbol{e}) = \\sigma^2(\\boldsymbol{I}-\\boldsymbol{H})$。单个残差 $e_i$ 的方差是该矩阵的第 $i$ 个对角元素，即 $\\text{Var}(e_i) = \\sigma^2(1 - h_{ii})$，其中 $h_{ii}$ 是观测值 $i$ 的杠杆值。\n\n未知误差方差 $\\sigma^2$ 由均方误差 (MSE) 估计，即 $\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n-p}$，其中 $\\text{SSE} = \\sum_{j=1}^{n} e_j^2$ 是残差平方和，$n$ 是样本量，$p$ 是估计系数的数量。\n\n首先，我们根据给定数据计算原始残差 $e_i$：$y_i = 12.5$ 和 $\\hat{y}_i = 10.8$。\n$$e_i = y_i - \\hat{y}_i = 12.5 - 10.8 = 1.7$$\n\n接下来，我们从给定的 $\\hat{\\sigma}^2 = 1.44$ 确定估计的误差标准差 $\\hat{\\sigma}$ 的值。\n$$\\hat{\\sigma} = \\sqrt{1.44} = 1.2$$\n\n内学生化残差记为 $r_i$，是原始残差除以其估计的标准误，其中估计值 $\\hat{\\sigma}$ 来自完整模型。\n$$r_i = \\frac{e_i}{\\widehat{\\text{SE}}(e_i)} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}$$\n代入给定值 $h_{ii} = 0.82$、$n = 30$ 和 $p = 3$：\n$$r_i = \\frac{1.7}{1.2\\sqrt{1-0.82}} = \\frac{1.7}{1.2\\sqrt{0.18}}$$\n$$r_i \\approx \\frac{1.7}{1.2 \\times 0.424264} \\approx \\frac{1.7}{0.509117} \\approx 3.33905$$\n四舍五入到四位有效数字，内学生化残差为 $3.339$。\n\n外学生化残差记为 $t_i$，在概念上类似，但它使用的是从移除观测值 $i$ 后的数据集中获得的 $\\sigma^2$ 的估计值。该估计值记为 $\\hat{\\sigma}_{(i)}^2$。其公式为：\n$$t_i = \\frac{e_i}{\\hat{\\sigma}_{(i)}\\sqrt{1-h_{ii}}}$$\n为了在不重新运行回归的情况下找到 $\\hat{\\sigma}_{(i)}^2$，我们使用留一法残差平方和 $\\text{SSE}_{(i)}$ 与完整模型残差平方和 $\\text{SSE}$ 之间的关系。其恒等式为 $\\text{SSE}_{(i)} = \\text{SSE} - \\frac{e_i^2}{1-h_{ii}}$。\n留一法方差估计为 $\\hat{\\sigma}_{(i)}^2 = \\frac{\\text{SSE}_{(i)}}{n-1-p}$。\n我们可以用完整模型中的量来表示它：\n$$\\hat{\\sigma}_{(i)}^2 = \\frac{(n-p)\\hat{\\sigma}^2 - \\frac{e_i^2}{1-h_{ii}}}{n-p-1}$$\n使用给定数据：\n- $(n-p)\\hat{\\sigma}^2 = (30-3) \\times 1.44 = 27 \\times 1.44 = 38.88$\n- $e_i^2 = (1.7)^2 = 2.89$\n- $\\frac{e_i^2}{1-h_{ii}} = \\frac{2.89}{1-0.82} = \\frac{2.89}{0.18} \\approx 16.0556$\n留一法模型的自由度为 $n-p-1 = 30-3-1=26$。\n$$\\hat{\\sigma}_{(i)}^2 = \\frac{38.88 - 16.0555...}{26} = \\frac{22.8244...}{26} \\approx 0.87786$$\n然后，$\\hat{\\sigma}_{(i)} = \\sqrt{0.87786...} \\approx 0.93694$。\n现在我们计算 $t_i$：\n$$t_i = \\frac{1.7}{0.93694... \\times \\sqrt{0.18}} \\approx \\frac{1.7}{0.93694... \\times 0.424264...} \\approx \\frac{1.7}{0.39750...} \\approx 4.27666$$\n四舍五入到四位有效数字，外学生化残差为 $4.277$。\n\n为了验证，我们可以使用 $t_i$ 和 $r_i$ 之间的直接关系：\n$$t_i = r_i \\sqrt{\\frac{n-p-1}{n-p-r_i^2}}$$\n使用未四舍五入的值 $r_i \\approx 3.33905$：\n$$t_i \\approx 3.33905 \\sqrt{\\frac{30 - 3 - 1}{30 - 3 - (3.33905)^2}} = 3.33905 \\sqrt{\\frac{26}{27 - 11.14929}}$$\n$$t_i \\approx 3.33905 \\sqrt{\\frac{26}{15.85071}} \\approx 3.33905 \\sqrt{1.64029} \\approx 3.33905 \\times 1.28074 \\approx 4.27666$$\n结果是一致的。\n\n内学生化残差为 $3.339$，外学生化残差为 $4.277$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3.339  4.277\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "为了真正掌握这些概念，亲眼见证它们在实践中的表现是无价的。这项计算练习将指导您通过模拟数据集，来创建高杠杆和离群点的受控情景 。通过分析这些模拟数据并观察不同诊断统计量的行为，您将对残差的掩盖效应以及学生化残差和库克距离等工具的综合威力，获得一个实践性的、动态的理解。",
            "id": "4908265",
            "problem": "您必须编写一个完整、可运行的程序，构建并分析模拟的线性回归数据集，以展示杠杆值如何在普通最小二乘法 (OLS) 的残差图中掩盖大的残差。该分析必须基于普通最小二乘法、线性模型、残差、杠杆值和标准化诊断的核心定义，而不依赖于本说明中任何预先提供的快捷公式。\n\n在一个带截距项的简单线性模型下构建数据集，其中响应向量由一条直线加上加性高斯噪声生成。自变量的值必须按照下文为每个测试用例指定的方式进行模拟。为保证可复现性，请使用给定的种子。程序必须为每个数据集拟合一个带截距项的普通最小二乘模型，然后为每个情况计算以下内容：所有观测值的杠杆值、普通残差、外学生化残差和库克距离。使用这些诊断指标来评估两个邻近的极端自变量值是否能在普通最小二乘法残差图中掩盖大的残差。\n\n需要依赖的基本原理和假设：\n- 普通最小二乘模型最小化残差平方和。\n- 拟合值是观测响应的线性函数，由带截距项的模型矩阵确定。\n- 杠杆值量化了每个观测值的拟合值对观测响应的敏感度。\n- 外学生化残差通过排除每个观测值后的残差方差估计来调整普通残差，并考虑了杠杆值。\n- 库克距离综合了残差大小和杠杆值来量化影响力。\n\n对于下述每个测试用例，从直线 $y = \\beta_{0} + \\beta_{1} x + \\varepsilon$（其中 $\\varepsilon$ 是均值为 $0$、标准差为 $\\sigma$ 的高斯噪声）中模拟一个大小为 $n_{0}$ 的基础数据集。基础自变量值 $x$ 必须在区间 $[x_{\\min}, x_{\\max}]$ 内均匀采样。然后，根据情况，可选择性地追加两个额外的观测值，它们的自变量值 $x_{\\text{ext},1}$ 和 $x_{\\text{ext},2}$ 极端且邻近，均远在基础范围的右侧。在追加极端点的情况下，要么使其响应值以小噪声紧靠生成直线，要么通过一个固定的偏移量进行平移，以造成一种强烈的偏差，该偏差的方向使两个极端点都受到影响。\n\n您必须使用以下包含三个案例的测试套件，每个案例由 $(n_{0}, \\beta_{0}, \\beta_{1}, \\sigma, x_{\\min}, x_{\\max}, \\text{seed}, \\text{extreme\\_mode})$ 定义，并在适用时附带极端点配置参数 $(x_{\\text{ext},1}, x_{\\text{ext},2}, \\sigma_{\\text{ext}}, \\Delta)$：\n- 案例 A (带掩蔽效应的理想路径): $(n_{0} = 30, \\beta_{0} = 1.0, \\beta_{1} = 2.0, \\sigma = 1.0, x_{\\min} = 0.0, x_{\\max} = 10.0, \\text{seed} = 20231111, \\text{extreme\\_mode} = \\text{\"aligned\"})$，参数为 $(x_{\\text{ext},1} = 20.0, x_{\\text{ext},2} = 20.1, \\sigma_{\\text{ext}} = 0.3, \\Delta = 0.0)$。\n- 案例 B (无极端值的边界情况): $(n_{0} = 30, \\beta_{0} = 1.0, \\beta_{1} = 2.0, \\sigma = 1.0, x_{\\min} = 0.0, x_{\\max} = 10.0, \\text{seed} = 20231112, \\text{extreme\\_mode} = \\text{\"none\"})$；此案例不添加额外点。\n- 案例 C (带强方向性偏移的边缘情况): $(n_{0} = 30, \\beta_{0} = 1.0, \\beta_{1} = 2.0, \\sigma = 1.0, x_{\\min} = 0.0, x_{\\max} = 10.0, \\text{seed} = 20231113, \\text{extreme\\_mode} = \\text{\"shifted\"})$，参数为 $(x_{\\text{ext},1} = 20.0, x_{\\text{ext},2} = 20.1, \\sigma_{\\text{ext}} = 0.0, \\Delta = 8.0)$。\n\n对于每个案例，对完整数据集拟合一个带截距项的普通最小二乘模型。设 $n$ 表示拟合数据集中观测值的总数（包括任何追加的极端点），$p$ 表示包括截距项在内的回归参数数量，即 $p = 2$。设 $h_{i}$ 表示观测值 $i$ 的杠杆值，$r_{i}$ 为其普通残差，$t_{i}$ 为其外学生化残差，$D_{i}$ 为其库克距离，并设 $\\alpha = 0.05$。\n\n为每个案例定义以下决策量：\n- 高杠杆值阈值：$h_{\\text{thr}} = \\dfrac{2p}{n}$。\n- 经 Bonferroni 校正的双侧外学生化残差阈值：$t_{\\text{crit}}$ 是自由度为 $n - p - 1$ 的学生t分布的 $(1 - \\alpha/(2n))$ 分位数。\n- 掩蔽布尔值，当且仅当以下所有条件同时成立时为真：\n  1. 案例中恰好存在两个追加的极端点，并且两者都满足 $h_{i} > h_{\\text{thr}}$。\n  2. 两个追加极端点的 $|r_{i}|$ 都严格小于 $\\{|r_{j}|\\}_{j=1}^{n}$ 的中位数。\n  3. $\\max_{1 \\le j \\le n} |r_{j}|$ 的索引不是两个极端点的索引之一。\n\n为每个案例计算并报告一个元组 $[\\text{masking}, \\text{count}_{\\text{high-lev}}, \\text{count}_{\\text{bonf-out}}, \\text{argmax}_{\\text{Cook}}]$，其中：\n- $\\text{masking}$ 是如上定义的布尔值。\n- $\\text{count}_{\\text{high-lev}}$ 是满足 $h_{i} > h_{\\text{thr}}$ 的观测值数量。\n- $\\text{count}_{\\text{bonf-out}}$ 是满足 $|t_{i}| > t_{\\text{crit}}$ 的观测值数量。\n- $\\text{argmax}_{\\text{Cook}}$ 是具有最大 $D_{i}$ 的观测值的从零开始的索引。\n\n您的程序应生成单行输出，其中包含三个案例的结果，形式为一个由这些元组组成的逗号分隔列表，并用方括号括起来，例如 $[[\\text{maskA},\\text{cA1},\\text{cA2},\\text{iA}],[\\text{maskB},\\text{cB1},\\text{cB2},\\text{iB}],[\\text{maskC},\\text{cC1},\\text{cC2},\\text{iC}]]$。布尔值必须打印为编程语言的原生布尔文字，计数和索引必须为整数。不涉及物理单位。如果出现任何角度，应将其纯粹视为无单位的抽象数学量。不应打印百分比；概率水平 $\\alpha$ 必须按上述指定的小数值处理。",
            "solution": "该问题是有效的。它提出了一个在生物统计学领域中定义明确的计算任务，具体涉及线性模型中的回归诊断。该问题具有科学依据，是客观的，并提供了所有必要信息以产生唯一、可验证的解决方案。\n\n### 基于原理的解决方案设计\n\n目标是展示残差掩蔽现象，即具有高杠杆值的观测值能不适当地影响拟合的回归线，导致它们自身的残差小得出人意料。此分析通过在不同条件下模拟数据集并计算标准的普通最小二乘法 (OLS) 回归诊断指标来进行。\n\n#### 1. OLS 和诊断的理论框架\n\n分析基于一个带截距项的简单线性回归模型：\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i $$\n其中 $\\varepsilon_i$ 是来自正态分布 $\\mathcal{N}(0, \\sigma^2)$ 的独立同分布误差。以矩阵形式表示为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{y}$ 是响应的 $n \\times 1$ 向量，$\\mathbf{X}$ 是 $n \\times p$ 的模型矩阵（有 $n$ 个观测值和 $p=2$ 个参数），$\\boldsymbol{\\beta}$ 是系数的 $p \\times 1$ 向量，$\\boldsymbol{\\varepsilon}$ 是误差向量。对于我们的模型，矩阵 $\\mathbf{X}$ 是：\n$$\n\\mathbf{X} = \\begin{pmatrix} 1  x_1 \\\\ 1  x_2 \\\\ \\vdots  \\vdots \\\\ 1  x_n \\end{pmatrix}\n$$\n\nOLS 方法找到最小化残差平方和 (RSS) $\\sum r_i^2 = \\mathbf{r}^T\\mathbf{r}$ 的系数估计值 $\\hat{\\boldsymbol{\\beta}}$。解由下式给出：\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n\n拟合值 $\\hat{\\mathbf{y}}$ 是观测值 $\\mathbf{y}$ 的线性变换：\n$$ \\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} = \\mathbf{H}\\mathbf{y} $$\n矩阵 $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$ 被称为“帽子矩阵”，因为它将 $\\mathbf{y}$ 投影到其估计值 $\\hat{\\mathbf{y}}$ 上。\n\n**杠杆值 (Leverage)**：帽子矩阵的对角元素 $h_i = H_{ii}$ 是杠杆值。杠杆值 $h_i$ 量化了观测响应 $y_i$ 对其自身拟合值 $\\hat{y}_i$ 的影响，因为 $\\frac{\\partial \\hat{y}_i}{\\partial y_i} = h_i$。高杠杆值表示一个观测值具有异常的预测变量值 $x_i$（即，远离 $x$ 值的均值），因此有可能对回归线产生高度影响。一个常用的启发式规则将 $h_i > 2p/n$ 的观测值标记为高杠杆值。\n\n**普通残差 (Ordinary Residuals)**：残差是观测值与拟合值之间的差异，$r_i = y_i - \\hat{y}_i$。残差图是评估模型拟合度的主要工具。然而，高杠杆点可以“拉动”回归线向自身靠拢，导致其残差 $r_i$ 变小，从而掩盖了它们与由其他数据点拟合出的模型之间的真实差异。\n\n**外学生化残差 (Externally Studentized Residuals)**：为了解决普通残差的问题（方差非恒定和掩蔽效应），我们使用外学生化残差 ($t_i$)。第 $i$ 个残差通过其标准差的估计值进行缩放，该估计值是使用移除了观测值 $i$ 的数据拟合的模型计算得出的。这为观测值的“离群性”提供了一个更客观的度量。公式为：\n$$ t_i = \\frac{r_i}{s_{(i)}\\sqrt{1-h_i}} = r_i \\sqrt{\\frac{n-p-1}{\\text{RSS}(1-h_i) - r_i^2}} $$\n其中 $s_{(i)}$ 是不包含观测值 $i$ 的模型的残差标准误。在观测值 $i$ 不是离群点的零假设下，$t_i$ 服从自由度为 $n-p-1$ 的学生t分布。为了在整个数据集中检验离群点，将 Bonferroni 校正应用于显著性水平 $\\alpha$，从而得到在 $(1 - \\alpha/(2n))$ 分位数处的临界值 $t_{\\text{crit}}$，该值来自 $t_{n-p-1}$ 分布。\n\n**库克距离 (Cook's Distance)**：该诊断指标衡量单个观测值对整套估计系数的总体影响。它结合了观测值的残差和杠杆值信息。大的库克距离表示移除该观测值将显著改变模型的系数估计。公式为：\n$$ D_i = \\frac{(\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)})^T (\\mathbf{X}^T\\mathbf{X}) (\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)})}{p s^2} = \\frac{r_i^2}{p s^2} \\left[ \\frac{h_i}{(1-h_i)^2} \\right] $$\n其中 $s^2 = \\text{RSS}/(n-p)$。`argmax_Cook` 标识了根据此度量标准最具影响力的单个点。\n\n#### 2. 模拟与分析策略\n\n程序将对三个不同的测试用例实施上述原理。\n\n- **数据生成**：对于每个案例，使用一个以特定种子初始化的 `numpy` 随机数生成器来生成基础数据集，以确保可复现性。自变量 $x$ 从均匀分布中抽取，响应 $y$ 根据指定的线性方程和附加的高斯噪声计算得出。在案例 A 和 C 中，会追加两个具有极端 $x$ 值的额外点。\n    - **案例 A (对齐的极端点)**：两个极端点的 $y$ 值与 underlying 线性关系一致。它们被设计为具有高杠杆值并强力拉动回归线，但因为其位置“正确”，自身的残差会很小，从而展示出遮蔽效应。\n    - **案例 B (无极端点)**：作为基线，代表一个没有故意引入影响点的标准数据集。\n    - **案例 C (偏移的极端点)**：两个极端点的 $y$ 值系统地偏离了真实直线。这些是同时具有高杠杆值的真实离群点。它们的残差预计会很大，并且它们将被识别为影响点，而不会出现掩蔽效应。\n\n- **计算**：一个单独的函数将处理每个案例。\n    1. 构建模型矩阵 $\\mathbf{X}$ 和响应向量 $\\mathbf{y}$。\n    2. 计算 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$。\n    3. 计算帽子矩阵 $\\mathbf{H}$ 并从其对角线提取杠杆值 $h_i$。\n    4. 使用上述公式计算普通残差 $\\mathbf{r}$、外学生化残差 $\\mathbf{t}$ 和库克距离 $\\mathbf{D}$。\n    5. 评估决策量：\n        - `masking`：通过应用三个指定条件确定的布尔值，这些条件检查是否存在具有反常小残差的高杠杆极端点。\n        - `count_high-lev`：$h_i > 2p/n$ 的点数。\n        - `count_bonf-out`：$|t_i|$ 超过经 Bonferroni 校正的临界值 $t_{\\text{crit}}$ 的点数，该值从 `scipy.stats.t.ppf` 获得。\n        - `argmax_Cook`：具有最大库克距离的观测值的索引，使用 `numpy.argmax` 找到。\n    6. 每个案例的最终结果被整理并格式化为指定的字符串输出。\n\n这种基于原理的方法确保输出是回归诊断理论在模拟数据上的直接和正确的应用，从而忠实地解决了问题陈述。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import t as student_t\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the analysis for all test cases.\n    \"\"\"\n    \n    # Define the test cases as per the problem statement.\n    # Format: (n0, b0, b1, sigma, x_min, x_max, seed, extreme_mode, ext_params)\n    # ext_params: (x_ext1, x_ext2, sigma_ext, delta)\n    test_cases = [\n        (30, 1.0, 2.0, 1.0, 0.0, 10.0, 20231111, \"aligned\", (20.0, 20.1, 0.3, 0.0)),\n        (30, 1.0, 2.0, 1.0, 0.0, 10.0, 20231112, \"none\", None),\n        (30, 1.0, 2.0, 1.0, 0.0, 10.0, 20231113, \"shifted\", (20.0, 20.1, 0.0, 8.0)),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = analyze_case(case_params)\n        results.append(result)\n        \n    # Format the final output string.\n    # str() on a list produces the desired '[...]' format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef analyze_case(case_params):\n    \"\"\"\n    Analyzes a single regression case to compute diagnostic statistics.\n    \"\"\"\n    n0, b0, b1, sigma, x_min, x_max, seed, extreme_mode, ext_params = case_params\n    \n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    \n    # Base dataset\n    x_base = rng.uniform(x_min, x_max, n0)\n    eps_base = rng.normal(0, sigma, n0)\n    y_base = b0 + b1 * x_base + eps_base\n    \n    x_full, y_full = x_base, y_base\n    \n    if extreme_mode != \"none\":\n        x_ext1, x_ext2, sigma_ext, delta = ext_params\n        x_extreme = np.array([x_ext1, x_ext2])\n        eps_extreme = rng.normal(0, sigma_ext, 2)\n        y_extreme = (b0 + b1 * x_extreme) + eps_extreme + delta\n        \n        x_full = np.concatenate((x_base, x_extreme))\n        y_full = np.concatenate((y_base, y_extreme))\n        \n    n = len(x_full)\n    p = 2  # number of parameters: intercept and slope\n\n    # 2. OLS Regression\n    X = np.vstack((np.ones(n), x_full)).T\n    \n    # Compute OLS estimator and related quantities\n    XTX_inv = np.linalg.inv(X.T @ X)\n    beta_hat = XTX_inv @ X.T @ y_full\n    y_hat = X @ beta_hat\n    r = y_full - y_hat\n    \n    # 3. Diagnostic Statistics Calculation\n    # Hat matrix H and leverages h_i\n    H = X @ XTX_inv @ X.T\n    h = np.diag(H)\n    \n    # Residual sum of squares (RSS) and residual standard error (s)\n    RSS = r.T @ r\n    s2 = RSS / (n - p)\n    \n    # Externally Studentized residuals t_i\n    # Formula: t_i = r_i * sqrt((n - p - 1) / (RSS * (1 - h_i) - r_i^2))\n    denom_sq = RSS * (1 - h) - r**2\n    # Clamp small negative values from floating point error to 0\n    denom_sq[denom_sq  0] = 0\n    \n    with np.errstate(divide='ignore', invalid='ignore'):\n        t = r * np.sqrt((n - p - 1) / denom_sq)\n        t[np.isnan(t)] = 0.0 # Handle case where denominator and numerator are zero\n    \n    # Cook's distance D_i\n    # Formula: D_i = (r_i^2 / (p * s2)) * (h_i / (1 - h_i)^2)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        D = (r**2 / (p * s2)) * (h / (1 - h)**2)\n        D[np.isnan(D)] = 0.0\n\n    # 4. Compute Output Quantities\n    # High-leverage count\n    h_thr = (2 * p) / n\n    count_high_lev = np.sum(h > h_thr)\n    \n    # Bonferroni-corrected outlier count\n    alpha = 0.05\n    df = n - p - 1\n    if df > 0:\n        t_crit = student_t.ppf(1 - alpha / (2 * n), df=df)\n        count_bonf_out = np.sum(np.abs(t) > t_crit)\n    else:\n        count_bonf_out = 0\n        \n    # Index of max Cook's Distance\n    argmax_cook = np.argmax(D)\n    \n    # Masking boolean evaluation\n    masking = False\n    if extreme_mode != \"none\":\n        extreme_indices = [n - 2, n - 1]\n        \n        # Condition 1: Both extreme points have high leverage\n        cond1 = np.all(h[extreme_indices] > h_thr)\n        \n        # Condition 2: Both extreme points have small absolute residuals\n        median_abs_r = np.median(np.abs(r))\n        cond2 = np.all(np.abs(r[extreme_indices])  median_abs_r)\n        \n        # Condition 3: The max absolute residual is not at an extreme point\n        idx_max_abs_r = np.argmax(np.abs(r))\n        cond3 = idx_max_abs_r not in extreme_indices\n        \n        if cond1 and cond2 and cond3:\n            masking = True\n            \n    return [bool(masking), int(count_high_lev), int(count_bonf_out), int(argmax_cook)]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}