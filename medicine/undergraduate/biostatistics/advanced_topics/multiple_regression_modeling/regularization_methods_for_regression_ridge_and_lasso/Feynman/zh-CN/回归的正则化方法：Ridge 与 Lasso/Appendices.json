{
    "hands_on_practices": [
        {
            "introduction": "要真正理解正则化的威力，我们首先需要了解标准回归方法在何处会遇到困难。这个练习将通过一个具体案例，揭示多重共线性问题：当预测变量高度相关时，普通最小二乘法 (OLS) 的估计会变得极不稳定。通过亲手计算在这种情境下一个 OLS 系数的方差，你将具体地体会到这种不稳定性是如何产生的，并理解为何我们需要像岭回归和 LASSO 这样的正则化方法。",
            "id": "4947371",
            "problem": "一个生物统计学团队正在使用两种已知其检测结果高度相关的实验室生物标志物来为一个连续临床结局 $y$ 建模。$n=5$ 名患者和 $p=2$ 种生物标志物的数据被整合到设计矩阵 $X$ 中（其列已被缩放到可比较的单位）：\n$$\nX=\\begin{pmatrix}\n1  1.002 \\\\\n2  2.001 \\\\\n3  3.000 \\\\\n4  4.001 \\\\\n5  5.002\n\\end{pmatrix}.\n$$\n假设标准线性模型为 $y = X\\beta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$，$\\mathcal{N}$ 表示正态分布，$I$ 是单位矩阵。假设先前的测量研究为残差方差提供了一个可靠的估计值 $\\sigma^2 = 0.25$。仅使用这些模型定义和普通最小二乘法 (OLS) 的性质，推导第一个系数 $\\beta_1$ 的 OLS 估计量的抽样方差，然后计算在给定 $X$ 和 $\\sigma^2$ 的情况下的数值。将最终数值答案四舍五入至四位有效数字。将最终答案表示为不带单位的实数。此外，在共线性的背景下解释该方差的大小，并说明为何岭回归和最小绝对收缩和选择算子 (LASSO) 等正则化方法在此处是相关的。",
            "solution": "该问题要求推导和计算一个普通最小二乘法 (OLS) 系数估计的抽样方差，然后联系多重共线性和正则化技术的相关性对结果进行解释。\n\n**第一部分：OLS 估计量抽样方差的推导**\n\n标准线性模型由 $y = X\\beta + \\varepsilon$ 给出，其中 $y$ 是 $n \\times 1$ 的结局向量，$X$ 是 $n \\times p$ 的设计矩阵，$\\beta$ 是 $p \\times 1$ 的系数向量，$\\varepsilon$ 是 $n \\times 1$ 的误差向量。OLS 估计量 $\\hat{\\beta}$ 是通过最小化残差平方和 $SSR = (y - X\\beta)^T(y - X\\beta)$ 来推导的。这会得到正规方程 $X^T X \\hat{\\beta} = X^T y$。假设 $X^T X$ 是可逆的，则 OLS 估计量为：\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\n为了求出 $\\hat{\\beta}$ 的抽样分布，我们将真实模型 $y = X\\beta + \\varepsilon$ 代入 $\\hat{\\beta}$ 的表达式中：\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T (X\\beta + \\varepsilon) = (X^T X)^{-1} (X^T X) \\beta + (X^T X)^{-1} X^T \\varepsilon = \\beta + (X^T X)^{-1} X^T \\varepsilon\n$$\n问题陈述误差服从正态分布 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$。这意味着误差向量的期望值为 $E[\\varepsilon] = 0$，误差的协方差矩阵为 $E[\\varepsilon \\varepsilon^T] = \\sigma^2 I$。\n估计量的期望值为：\n$$\nE[\\hat{\\beta}] = E[\\beta + (X^T X)^{-1} X^T \\varepsilon] = \\beta + (X^T X)^{-1} X^T E[\\varepsilon] = \\beta + 0 = \\beta\n$$\n这表明 OLS 估计量是无偏的。$\\hat{\\beta}$ 的协方差矩阵定义为 $\\text{Cov}(\\hat{\\beta}) = E[(\\hat{\\beta} - E[\\hat{\\beta}])(\\hat{\\beta} - E[\\hat{\\beta}])^T]$。由于 $E[\\hat{\\beta}] = \\beta$，我们有 $\\hat{\\beta} - \\beta = (X^T X)^{-1} X^T \\varepsilon$。因此：\n$$\n\\text{Cov}(\\hat{\\beta}) = E[((X^T X)^{-1} X^T \\varepsilon)((X^T X)^{-1} X^T \\varepsilon)^T]\n$$\n使用性质 $(AB)^T = B^T A^T$，上式变为：\n$$\n\\text{Cov}(\\hat{\\beta}) = E[(X^T X)^{-1} X^T \\varepsilon \\varepsilon^T X ((X^T X)^{-1})^T]\n$$\n由于 $X$ 被视为固定的（非随机的），我们可以将其移到期望符号外面：\n$$\n\\text{Cov}(\\hat{\\beta}) = (X^T X)^{-1} X^T E[\\varepsilon \\varepsilon^T] X ((X^T X)^{-1})^T\n$$\n代入 $E[\\varepsilon \\varepsilon^T] = \\sigma^2 I$：\n$$\n\\text{Cov}(\\hat{\\beta}) = (X^T X)^{-1} X^T (\\sigma^2 I) X ((X^T X)^{-1})^T = \\sigma^2 (X^T X)^{-1} X^T X ((X^T X)^{-1})^T\n$$\n这可以简化为 $\\sigma^2 (X^T X)^{-1} I ((X^T X)^{-1})^T$。因为 $X^T X$ 是对称的，它的逆矩阵 $(X^T X)^{-1}$ 也是对称的，所以 $((X^T X)^{-1})^T = (X^T X)^{-1}$。OLS 估计量协方差矩阵的最终表达式是：\n$$\n\\text{Cov}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\n$$\n第一个系数的估计量 $\\hat{\\beta}_1$ 的抽样方差是该协方差矩阵的第一个对角元素。如果我们记 $C = (X^T X)^{-1}$，那么：\n$$\n\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 C_{11} = \\sigma^2 [(X^T X)^{-1}]_{11}\n$$\n这就是我们将用于计算的通用公式。\n\n**第二部分：数值计算**\n\n给定的数据是 $n=5$，$p=2$，$\\sigma^2 = 0.25$，以及设计矩阵：\n$$\nX = \\begin{pmatrix}\n1  1.002 \\\\\n2  2.001 \\\\\n3  3.000 \\\\\n4  4.001 \\\\\n5  5.002\n\\end{pmatrix}\n$$\n首先，我们计算矩阵 $X^T X$：\n$$\nX^T X = \\begin{pmatrix}\n1  2  3  4  5 \\\\\n1.002  2.001  3.000  4.001  5.002\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1.002 \\\\\n2  2.001 \\\\\n3  3.000 \\\\\n4  4.001 \\\\\n5  5.002\n\\end{pmatrix}\n$$\n各元素为：\n$(X^T X)_{11} = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 1+4+9+16+25 = 55$\n$(X^T X)_{12} = (X^T X)_{21} = 1(1.002) + 2(2.001) + 3(3.000) + 4(4.001) + 5(5.002) = 1.002 + 4.002 + 9 + 16.004 + 25.010 = 55.018$\n$(X^T X)_{22} = 1.002^2 + 2.001^2 + 3.000^2 + 4.001^2 + 5.002^2 = 1.004004 + 4.004001 + 9 + 16.008001 + 25.020004 = 55.03601$\n所以，该矩阵为：\n$$\nX^T X = \\begin{pmatrix} 55  55.018 \\\\ 55.018  55.03601 \\end{pmatrix}\n$$\n接下来，我们计算这个 $2 \\times 2$ 矩阵的逆矩阵。矩阵 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ 的逆是 $\\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$。\n行列式是：\n$$\n\\det(X^T X) = (55)(55.03601) - (55.018)^2 = 3026.98055 - 3026.980324 = 0.000226\n$$\n逆矩阵是：\n$$\n(X^T X)^{-1} = \\frac{1}{0.000226} \\begin{pmatrix} 55.03601  -55.018 \\\\ -55.018  55 \\end{pmatrix}\n$$\n我们需要这个矩阵的 (1,1) 元素，即：\n$$\n[(X^T X)^{-1}]_{11} = \\frac{55.03601}{0.000226}\n$$\n现在我们可以使用 $\\sigma^2 = 0.25$ 计算 $\\hat{\\beta}_1$ 的方差：\n$$\n\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 [(X^T X)^{-1}]_{11} = 0.25 \\times \\frac{55.03601}{0.000226} \\approx 60880.542\n$$\n四舍五入到四位有效数字，我们得到 $60880$。用科学记数法表示，即为 $6.088 \\times 10^4$。\n\n**第三部分：解释与正则化的相关性**\n\n计算出的方差 $\\text{Var}(\\hat{\\beta}_1) \\approx 6.088 \\times 10^4$ 是一个非常大的值。系数估计量的大方差意味着该估计非常不精确和不稳定。如果我们从同一总体中收集另一组不同的数据样本，$\\hat{\\beta}_1$ 的估计值可能会发生剧烈变化。这使得该系数估计对于推断或预测是不可靠的。\n\n这种高方差的根本原因是严重的多重共线性。当回归模型中的预测变量高度相关时，就会发生多重共线性。在本问题中，设计矩阵 $X$ 的两列几乎相同，表明其相关性极高。两种生物标志物测量值之间的相关系数几乎为 $1$。当预测变量高度相关时，矩阵 $X^T X$ 变得近乎奇异（即其行列式接近于零）。在我们的例子中，$\\det(X^T X) = 0.000226$，这是一个非常小的数。对一个近乎奇异的矩阵求逆的过程在数值上是不稳定的，并会导致逆矩阵的元素值非常大。由于 OLS 估计量的方差与该逆矩阵的对角元素成正比（如公式 $\\text{Cov}(\\hat{\\beta}) = \\sigma^2(X^T X)^{-1}$ 所示），方差会被“放大”到非常大的值。这种现象通常通过方差膨胀因子 (VIF) 来量化。\n\n正则化方法是专门为解决多重共线性问题而设计的。OLS 在这种情况下会失效，因为它寻求的是无偏估计量，而这带来的代价是巨大的方差。正则化方法在估计中引入少量偏差，以实现方差的大幅降低，从而得到一个低得多的总均方误差（$MSE = \\text{方差} + \\text{偏差}^2$）。\n\n-   **岭回归 (Ridge Regression)**：岭回归在 OLS 目标函数中增加了一个惩罚项，最小化 $\\sum (y_i - X_i\\beta)^2 + \\lambda \\sum \\beta_j^2$。得到的估计量是 $\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\lambda I)^{-1} X^T y$，其中 $\\lambda > 0$ 是一个调整参数。即使 $X^T X$ 不是良态的，向 $X^T X$ 添加 $\\lambda I$ 项也使得待求逆的矩阵变为良态且非奇异的。这稳定了求逆过程，并产生方差小得多的系数估计。\n-   **Lasso（最小绝对收缩和选择算子）**：Lasso 最小化 $\\sum (y_i - X_i\\beta)^2 + \\lambda \\sum |\\beta_j|$。与岭回归类似，它也将系数向零收缩以降低方差。然而，$L_1$ 惩罚项具有将某些系数精确地收缩到零的独特属性。在像本例这样两个预测变量高度相关的情况下，Lasso 可能会选择其中一个生物标志物并为其分配一个非零系数，同时通过将其系数设为零来消除另一个。这通过执行自动变量选择提供了一个更简单、更易于解释的模型。\n\n总之，OLS 估计量的极端方差凸显了其在存在严重多重共线性时的失效。像岭回归和 Lasso 这样的正则化技术是关键的统计工具，在此类常见的生物统计学场景中能够提供稳定、可靠且更易于解释的模型。",
            "answer": "$$\\boxed{6.088 \\times 10^4}$$"
        },
        {
            "introduction": "在认识到问题的存在之后，我们来探索一种解决方案——岭回归的内在机制。这个练习将深入探讨岭回归的工作原理，而不仅仅是停留在系数的计算公式上。通过推导“帽子矩阵”及其迹（即有效自由度），我们将学习如何量化正则化参数 $\\lambda$ 是如何控制模型复杂度并收缩预测变量影响力的。",
            "id": "4947422",
            "problem": "考虑一个生物统计线性回归情景，其中使用由生物预测变量（例如，标准化的基因表达或临床协变量）组成的中心化设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 对中心化响应向量 $y \\in \\mathbb{R}^{n}$ 进行建模，因此不需要截距项。为了减轻多重共线性和过拟合，岭回归估计量 $\\hat{\\beta}^{\\text{ridge}} \\in \\mathbb{R}^{p}$ 被定义为惩罚最小二乘目标函数的唯一最小化子：\n$$\n\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2},\n$$\n其中惩罚参数 $\\lambda > 0$。从这个定义出发，仅使用线性代数知识，例如导数性质、对称性、正交对角化和奇异值分解（SVD）的存在性，按以下步骤进行：\n\n1. 推导刻画 $\\hat{\\beta}^{\\text{ridge}}$ 的正规方程，并获得用 $X$、$y$ 和 $\\lambda$ 表示的 $\\hat{\\beta}^{\\text{ridge}}$ 的显式表达式。\n2. 推导拟合值 $\\hat{y} = X \\hat{\\beta}^{\\text{ridge}}$，并证明存在一个线性映射 $H_{\\lambda} \\in \\mathbb{R}^{n \\times n}$ 使得 $\\hat{y} = H_{\\lambda} y$。明确地用 $X$ 和 $\\lambda$ 表示 $H_{\\lambda}$。\n3. 令 $r = \\operatorname{rank}(X)$，并设 $X$ 的奇异值分解（SVD）为 $X = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{p \\times p}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{n \\times p}$ 的主对角线上有非零奇异值 $\\sigma_{1}, \\dots, \\sigma_{r} > 0$。仅使用这些定义和迹的基本性质，推导 $\\operatorname{tr}(H_{\\lambda})$ 作为 $\\lambda$ 和非零奇异值 $\\{\\sigma_{j}\\}_{j=1}^{r}$ 的函数的闭式解析表达式。\n\n将你的最终答案以 $\\operatorname{tr}(H_{\\lambda})$ 关于 $\\lambda$ 和 $\\{\\sigma_{j}\\}_{j=1}^{r}$ 的单个闭式解析表达式的形式给出。不需要数值近似或四舍五入。",
            "solution": "所述问题是有效的。它在科学上植根于已建立的正则化线性模型理论，特别是岭回归，这是现代生物统计学和机器学习的基石。该问题是适定的，提供了推导唯一、有意义解所需的所有必要条件和定义。它是客观的，没有任何模糊性或事实错误。我们可以开始解题。\n\n问题分为三个部分。我们将按顺序解决它们。\n\n第1部分：推导正规方程和 $\\hat{\\beta}^{\\text{ridge}}$ 的表达式。\n\n岭回归的目标函数由下式给出\n$$L(\\beta) = \\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}$$\n其中 $y \\in \\mathbb{R}^{n}$ 是中心化响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是中心化设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda > 0$ 是正则化参数。我们可以使用向量转置来表示欧几里得范数的平方：\n$$L(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta) + \\lambda \\beta^{\\top}\\beta$$\n展开第一项，我们得到：\n$$L(\\beta) = y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta + \\lambda \\beta^{\\top}\\beta$$\n由于 $y^{\\top}X\\beta$ 是一个标量（$1 \\times 1$ 矩阵），它等于其转置 $(\\beta^{\\top}X^{\\top}y)$。这使我们可以合并两个交叉乘积项：\n$$L(\\beta) = y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta + \\lambda \\beta^{\\top}I\\beta$$\n我们可以提出因子 $\\beta^{\\top}$ 和 $\\beta$，将函数写成关于 $\\beta$ 的二次型：\n$$L(\\beta) = y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}(X^{\\top}X + \\lambda I)\\beta$$\n为了找到最小化 $L(\\beta)$ 的系数向量 $\\hat{\\beta}^{\\text{ridge}}$，我们计算 $L(\\beta)$ 关于 $\\beta$ 的梯度并将其设为零向量。当 $\\lambda > 0$ 时，目标函数是严格凸的，这保证了存在唯一的全局最小值。使用向量微积分的法则（对于对称矩阵 $A$，有 $\\frac{\\partial}{\\partial x} c^{\\top}x = c$ 和 $\\frac{\\partial}{\\partial x} x^{\\top}Ax = 2Ax$），梯度为：\n$$\\frac{\\partial L(\\beta)}{\\partial \\beta} = -2X^{\\top}y + 2(X^{\\top}X + \\lambda I)\\beta$$\n在 $\\beta = \\hat{\\beta}^{\\text{ridge}}$ 处将梯度设为零，得到条件：\n$$-2X^{\\top}y + 2(X^{\\top}X + \\lambda I)\\hat{\\beta}^{\\text{ridge}} = 0$$\n两边除以 $2$ 并重新整理，得到岭回归的正规方程：\n$$(X^{\\top}X + \\lambda I)\\hat{\\beta}^{\\text{ridge}} = X^{\\top}y$$\n为了获得 $\\hat{\\beta}^{\\text{ridge}}$ 的显式表达式，我们必须对矩阵 $(X^{\\top}X + \\lambda I)$ 求逆。矩阵 $X^{\\top}X$ 是半正定的。由于 $\\lambda > 0$，矩阵 $\\lambda I$ 是正定的。一个半正定矩阵与一个正定矩阵之和是正定的，因此总是可逆的。左乘 $(X^{\\top}X + \\lambda I)$ 的逆矩阵，得到显式解：\n$$\\hat{\\beta}^{\\text{ridge}} = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}y$$\n\n第2部分：推导拟合值 $\\hat{y}$ 和帽子矩阵 $H_{\\lambda}$。\n\n拟合值，记作 $\\hat{y}$，是由模型使用估计出的系数预测的：\n$$\\hat{y} = X\\hat{\\beta}^{\\text{ridge}}$$\n代入第1部分中 $\\hat{\\beta}^{\\text{ridge}}$ 的表达式：\n$$\\hat{y} = X \\left( (X^{\\top}X + \\lambda I)^{-1}X^{\\top}y \\right)$$\n使用矩阵乘法的结合律，我们可以重新组合各项，将 $\\hat{y}$ 表示为 $y$ 的一个线性变换：\n$$\\hat{y} = \\left( X(X^{\\top}X + \\lambda I)^{-1}X^{\\top} \\right) y$$\n这表明存在一个线性映射 $H_{\\lambda} \\in \\mathbb{R}^{n \\times n}$ 使得 $\\hat{y} = H_{\\lambda} y$。通过观察可知，这个矩阵，被称为岭回归的帽子矩阵或平滑矩阵，是：\n$$H_{\\lambda} = X(X^{\\top}X + \\lambda I)^{-1}X^{\\top}$$\n\n第3部分：推导 $\\operatorname{tr}(H_{\\lambda})$ 的闭式表达式。\n\n我们的目标是计算 $\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}\\left(X(X^{\\top}X + \\lambda I)^{-1}X^{\\top}\\right)$。我们利用迹的循环性质，即对于矩阵 $A \\in \\mathbb{R}^{n \\times p}$ 和 $B \\in \\mathbb{R}^{p \\times n}$，有 $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$。令 $A = X$ 且 $B = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}$。那么，\n$$\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}\\left((X^{\\top}X + \\lambda I)^{-1}X^{\\top}X\\right)$$\n现在我们使用 $X$ 的奇异值分解（SVD），即 $X = U \\Sigma V^{\\top}$。这里，$U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{p \\times p}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{n \\times p}$ 是一个矩形对角矩阵，其上有 $r = \\operatorname{rank}(X)$ 个非零奇异值 $\\sigma_1, \\dots, \\sigma_r > 0$。首先，我们用 SVD 表示 $X^{\\top}X$：\n$$X^{\\top}X = (U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top}) = V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top} = V\\Sigma^{\\top}I_n\\Sigma V^{\\top} = V(\\Sigma^{\\top}\\Sigma) V^{\\top}$$\n矩阵 $\\Sigma^{\\top}\\Sigma \\in \\mathbb{R}^{p \\times p}$ 是一个对角矩阵，其对角元素为 $(\\sigma_1^2, \\dots, \\sigma_r^2, 0, \\dots, 0)$。现在将其代入迹的表达式中。首先，考虑项 $(X^{\\top}X + \\lambda I)$：\n$$X^{\\top}X + \\lambda I = V(\\Sigma^{\\top}\\Sigma) V^{\\top} + \\lambda I = V(\\Sigma^{\\top}\\Sigma) V^{\\top} + \\lambda VV^{\\top} = V(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top}$$\n那么其逆矩阵为：\n$$(X^{\\top}X + \\lambda I)^{-1} = (V(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top})^{-1} = V(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}V^{\\top}$$\n现在我们来组合迹函数内部的项：\n$$(X^{\\top}X + \\lambda I)^{-1}X^{\\top}X = \\left(V(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}V^{\\top}\\right) \\left(V(\\Sigma^{\\top}\\Sigma) V^{\\top}\\right)$$\n$$= V(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}(V^{\\top}V)(\\Sigma^{\\top}\\Sigma) V^{\\top} = V\\left((\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}(\\Sigma^{\\top}\\Sigma)\\right)V^{\\top}$$\n令 $M = (\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}(\\Sigma^{\\top}\\Sigma)$。对 $\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}(VMV^{\\top})$ 使用迹的循环性质：\n$$\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}(V^{\\top}VM) = \\operatorname{tr}(I_pM) = \\operatorname{tr}(M)$$\n矩阵 $M$ 是两个可交换对角矩阵的乘积，因此它本身也是一个对角矩阵。其第 $j$ 个对角元素 $M_{jj}$ 是 $(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}$ 和 $\\Sigma^{\\top}\\Sigma$ 相应对角元素的乘积：\n$$M_{jj} = \\frac{1}{(\\Sigma^{\\top}\\Sigma)_{jj} + \\lambda} \\times (\\Sigma^{\\top}\\Sigma)_{jj}$$\n对于 $j = 1, \\dots, r$，$\\Sigma^{\\top}\\Sigma$ 的第 $j$ 个对角元素是 $\\sigma_j^2$。因此，\n$$M_{jj} = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} \\quad \\text{for } j = 1, \\dots, r$$\n对于 $j = r+1, \\dots, p$，第 $j$ 个对角元素是 $0$。因此，\n$$M_{jj} = \\frac{0}{0 + \\lambda} = 0 \\quad \\text{for } j = r+1, \\dots, p$$\n$M$ 的迹是其对角元素之和：\n$$\\operatorname{tr}(M) = \\sum_{j=1}^{p} M_{jj} = \\sum_{j=1}^{r} \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} + \\sum_{j=r+1}^{p} 0$$\n因此，$H_{\\lambda}$ 的迹的最终闭式解析表达式是：\n$$\\operatorname{tr}(H_{\\lambda}) = \\sum_{j=1}^{r} \\frac{\\sigma_{j}^{2}}{\\sigma_{j}^{2} + \\lambda}$$\n这个量也被称为岭回归拟合的有效自由度。",
            "answer": "$$\\boxed{\\sum_{j=1}^{r} \\frac{\\sigma_{j}^{2}}{\\sigma_{j}^{2} + \\lambda}}$$"
        },
        {
            "introduction": "理论是根基，但在实践中这些模型是如何被拟合的呢？尤其是在处理大规模数据时，高效的算法至关重要。最后一个练习将带我们从理论走向实践，探索坐标下降法——一种拟合正则化模型的基础算法。通过为岭回归和 LASSO 实现单步的坐标更新，你将掌握计算机高效求解最优系数的核心机制，即便面对海量数据集也能游刃有余。",
            "id": "4947418",
            "problem": "考虑惩罚线性回归中的单坐标更新，这是一种在生物统计学中广泛使用的方法，用于在高维生物数据中控制方差和提高预测精度。设有 $n$ 个观测值。令响应向量为 $y \\in \\mathbb{R}^n$，并假设当前拟合模型的残差向量为 $r \\in \\mathbb{R}^n$，定义为 $r = y - X \\beta$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是当前系数向量。关注对应于一个坐标的单个预测变量列 $x \\in \\mathbb{R}^n$，其当前系数为 $b \\in \\mathbb{R}$。一次单步坐标更新将 $b$ 改变一个量 $d \\in \\mathbb{R}$，得到新系数 $b_{\\text{new}} = b + d$，从而产生新的残差向量 $r' = r - x d$ 和新的残差平方和（RSS），由 $RSS' = \\lVert r' \\rVert_2^2$ 给出。\n\n仅从最小二乘法和凸惩罚的定义出发：\n- 残差平方和为 $RSS(\\beta) = \\lVert y - X \\beta \\rVert_2^2$。\n- 岭回归 (Ridge) 的目标函数是 $\\tfrac{1}{2} \\lVert y - X \\beta \\rVert_2^2 + \\tfrac{\\alpha}{2} \\lVert \\beta \\rVert_2^2$，其中惩罚参数 $\\alpha \\ge 0$。\n- Lasso 的目标函数是 $\\tfrac{1}{2} \\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1$，其中惩罚参数 $\\lambda \\ge 0$。\n\n你的任务是实现对 $b$ 的单坐标更新，并计算残差平方和随此更新产生的精确变化。更新量必须通过最小化相应惩罚目标函数对单个坐标的值来确定，同时保持所有其他坐标固定。概念上使用偏残差 $r_j = y - X_{-j} \\beta_{-j}$，但在计算中，给定的是 $r$ 和 $b$，因此你应该使用 $r_j = r + x b$ 来构造 $r_j$，以消除对未知子模型的依赖。然后对以下惩罚项之一执行更新：\n- 岭回归 (Ridge)：使用惩罚参数 $\\alpha$。\n- Lasso：使用惩罚参数 $\\lambda$。\n\n计算出 $b_{\\text{new}}$ 后，定义 $d = b_{\\text{new}} - b$，并计算残差平方和的精确变化量 $\\Delta RSS = RSS' - RSS$，将其表示为 $d$、$x$ 和 $r$ 的函数。\n\n实现一个程序，对下面的每个测试用例执行以下操作：\n- 构造 $r_j = r + x b$。\n- 计算单步坐标更新 $b_{\\text{new}}$，该更新最小化所选惩罚目标函数在坐标 $b$ 上的值。\n- 计算 $d = b_{\\text{new}} - b$。\n- 计算将 $d$ 应用于 $b$ 后（不改变任何其他坐标）残差平方和的精确变化量 $\\Delta RSS$。\n\n将每个测试用例的 $\\Delta RSS$ 作为浮点数报告。\n\n测试套件（每个用例给出 $n$、$x$、$r$、$b$ 和惩罚项规格）：\n1. 岭回归 (Ridge) 正常路径：\n   - $n = 5$\n   - $x = [1.2, -0.3, 0.5, 0.0, 2.1]$\n   - $r = [0.8, -1.1, 0.3, 0.0, 2.0]$\n   - $b = 0.4$\n   - 岭回归，$\\alpha = 0.7$。\n2. Lasso 正常路径（预期非零更新）：\n   - $n = 6$\n   - $x = [0.5, -1.0, 0.3, 1.2, -0.7, 0.4]$\n   - $r = [1.0, 0.5, -0.2, 0.3, -1.5, 0.1]$\n   - $b = -0.1$\n   - Lasso，$\\lambda = 0.2$。\n3. Lasso 阈值化至零（边界情况）：\n   - $n = 4$\n   - $x = [0.2, -0.1, 0.05, 0.0]$\n   - $r = [0.01, -0.02, 0.03, -0.04]$\n   - $b = 0.5$\n   - Lasso，$\\lambda = 0.2$。\n4. 岭回归 (Ridge)，预测变量列为零（边界情况）：\n   - $n = 5$\n   - $x = [0.0, 0.0, 0.0, 0.0, 0.0]$\n   - $r = [1.0, -1.0, 0.5, -0.5, 2.0]$\n   - $b = 3.0$\n   - 岭回归，$\\alpha = 1.0$。\n5. Lasso，预测变量数值极小（数值稳定性情况）：\n   - $n = 3$\n   - $x = [10^{-8}, -2 \\cdot 10^{-8}, 3 \\cdot 10^{-8}]$\n   - $r = [0.0, 0.0, 0.0]$\n   - $b = 1.0$\n   - Lasso，$\\lambda = 0.1$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，例如 $[result_1,result_2,\\dots]$，顺序与上面列出的测试用例完全一致。",
            "solution": "该问题要求推导并实现岭回归 (ridge) 和 Lasso 正则化线性回归的单坐标更新，并计算由此产生的残差平方和 ($RSS$) 的变化。这个问题具有科学依据、是良定的且内容自洽。所提供的定义和测试用例是一致且有效的。因此，我们可以着手解决。\n\n惩罚线性回归的一般目标函数是：\n$$L(\\beta) = \\frac{1}{2}\\lVert y - X \\beta \\rVert_2^2 + P(\\beta)$$\n其中 $P(\\beta)$ 是一个惩罚函数。我们的目标是更新对应于预测变量列 $x$ 的单个系数，记为 $b$。所有其他系数保持不变。新系数将为 $b_{\\text{new}}$。目标函数，仅视为 $b_{\\text{new}}$ 的函数，可以使用偏残差 $r_j = y - \\sum_{k \\neq j} x_k \\beta_k$ 来表示。按照题目要求，这可以从当前的完整残差 $r = y - X\\beta$ 计算得出，即 $r_j = r + xb$。\n\n目标函数中的 $RSS$ 部分可以写成：\n$$\\frac{1}{2}\\lVert r_j - x b_{\\text{new}} \\rVert_2^2 = \\frac{1}{2}(r_j - x b_{\\text{new}})^T(r_j - x b_{\\text{new}}) = \\frac{1}{2} (\\lVert r_j \\rVert_2^2 - 2 b_{\\text{new}} (r_j^T x) + b_{\\text{new}}^2 \\lVert x \\rVert_2^2)$$\n这是 $b_{\\text{new}}$ 的一个二次函数。项 $\\frac{1}{2}\\lVert r_j \\rVert_2^2$ 相对于 $b_{\\text{new}}$ 是一个常数，在最小化过程中可以忽略。\n\n**1. 岭回归更新**\n\n对于岭回归，惩罚项为 $P(\\beta) = \\frac{\\alpha}{2} \\lVert \\beta \\rVert_2^2 = \\frac{\\alpha}{2} \\sum_k \\beta_k^2$。单个坐标 $b_{\\text{new}}$ 的目标函数为：\n$$L_{\\text{ridge}}(b_{\\text{new}}) = \\frac{1}{2} \\lVert r_j - x b_{\\text{new}} \\rVert_2^2 + \\frac{\\alpha}{2} b_{\\text{new}}^2 + \\text{const}$$\n这个函数是可微且凸的。为了找到最小值，我们将其关于 $b_{\\text{new}}$ 的导数设为零：\n$$\\frac{\\partial L_{\\text{ridge}}}{\\partial b_{\\text{new}}} = -r_j^T x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\alpha b_{\\text{new}} = 0$$\n求解 $b_{\\text{new}}$：\n$$b_{\\text{new}} (\\lVert x \\rVert_2^2 + \\alpha) = r_j^T x$$\n$$b_{\\text{new}} = \\frac{r_j^T x}{\\lVert x \\rVert_2^2 + \\alpha}$$\n代入 $r_j = r + xb$：\n$$r_j^T x = (r + xb)^T x = r^T x + b(x^T x) = r^T x + b \\lVert x \\rVert_2^2$$\n因此，新系数的更新规则是：\n$$b_{\\text{new}} = \\frac{r^T x + b \\lVert x \\rVert_2^2}{\\lVert x \\rVert_2^2 + \\alpha}$$\n如果预测变量列 $x$ 是一个零向量，则 $\\lVert x \\rVert_2^2 = 0$ 且 $r^T x = 0$，从而得到 $b_{\\text{new}} = 0$。由于 $\\alpha \\ge 0$，如果 $\\alpha > 0$ 或 $\\lVert x \\rVert_2^2 > 0$，分母总是正的。\n\n**2. Lasso 回归更新**\n\n对于 Lasso 回归，惩罚项为 $P(\\beta) = \\lambda \\lVert \\beta \\rVert_1 = \\lambda \\sum_k |\\beta_k|$。单个坐标 $b_{\\text{new}}$ 的目标函数是：\n$$L_{\\text{lasso}}(b_{\\text{new}}) = \\frac{1}{2} \\lVert r_j - x b_{\\text{new}} \\rVert_2^2 + \\lambda |b_{\\text{new}}| + \\text{const}$$\n这个目标函数是凸的，但在 $b_{\\text{new}} = 0$ 处不可微。我们使用次梯度优化。目标函数的次梯度是：\n$$\\partial L_{\\text{lasso}}(b_{\\text{new}}) = -r_j^T x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\lambda \\cdot \\partial |b_{\\text{new}}|$$\n其中 $\\partial |u|$ 是绝对值函数的次梯度：当 $u \\neq 0$ 时为 $\\text{sgn}(u)$，当 $u=0$ 时为区间 $[-1, 1]$。\n在最小值点，次梯度集合必须包含 $0$：\n$$0 \\in -r_j^T x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\lambda \\cdot \\text{sgn}(b_{\\text{new}})$$\n令 $z = r_j^T x = r^T x + b \\lVert x \\rVert_2^2$ 并假设 $\\lVert x \\rVert_2^2 > 0$。条件变为：\n$$z - b_{\\text{new}} \\lVert x \\rVert_2^2 \\in \\lambda \\cdot \\text{sgn}(b_{\\text{new}})$$\n这会导出以下解：\n- 如果 $z > \\lambda$，那么 $b_{\\text{new}} > 0$：$z - b_{\\text{new}} \\lVert x \\rVert_2^2 = \\lambda \\implies b_{\\text{new}} = (z - \\lambda) / \\lVert x \\rVert_2^2$。\n- 如果 $z < -\\lambda$，那么 $b_{\\text{new}} < 0$：$z - b_{\\text{new}} \\lVert x \\rVert_2^2 = -\\lambda \\implies b_{\\text{new}} = (z + \\lambda) / \\lVert x \\rVert_2^2$。\n- 如果 $|z| \\le \\lambda$，那么 $b_{\\text{new}} = 0$。\n\n这就是软阈值函数，$S(z, \\lambda) = \\text{sgn}(z) \\max(|z|-\\lambda, 0)$。更新规则是：\n$$b_{\\text{new}} = \\frac{S(r^T x + b \\lVert x \\rVert_2^2, \\lambda)}{\\lVert x \\rVert_2^2}$$\n如果 $\\lVert x \\rVert_2^2 = 0$，需要最小化的目标函数就简化为 $\\lambda |b_{\\text{new}}|$（加上一个常数），它在 $b_{\\text{new}} = 0$ 时最小化。这种情况必须在实现中单独处理。\n\n**3. 残差平方和的变化（$\\Delta RSS$）**\n\n问题要求 $RSS$ 的精确变化量，定义为 $\\Delta RSS = RSS' - RSS$。\n当前的 $RSS$ 是 $\\lVert r \\rVert_2^2$。\n新的残差是 $r' = r - xd$，其中 $d = b_{\\text{new}} - b$ 是系数的变化量。\n新的 $RSS'$ 是：\n$$RSS' = \\lVert r' \\rVert_2^2 = \\lVert r - xd \\rVert_2^2 = (r - xd)^T(r - xd)$$\n$$RSS' = r^T r - (xd)^T r - r^T(xd) + (xd)^T(xd)$$\n由于 $r^T(xd) = d(r^T x)$ 是一个标量，它等于其转置 $(xd)^T r = d(x^T r)$。\n$$RSS' = \\lVert r \\rVert_2^2 - 2d(r^T x) + d^2(x^T x) = RSS - 2d(r^T x) + d^2 \\lVert x \\rVert_2^2$$\n因此，$RSS$ 的变化量是：\n$$\\Delta RSS = RSS' - RSS = d^2 \\lVert x \\rVert_2^2 - 2d(r^T x)$$\n这个公式允许直接从更新量 $d$、预测变量 $x$ 和当前残差 $r$ 计算 $\\Delta RSS$，而无需计算新的残差向量 $r'$。\n\n**算法摘要**\n对于每个测试用例：\n1.  给定 $x, r, b$ 和惩罚参数。\n2.  计算标量值：$r^T x$ 和 $\\lVert x \\rVert_2^2$。\n3.  根据惩罚类型（岭回归或 Lasso），使用推导出的公式计算更新后的系数 $b_{\\text{new}}$。处理 $\\lVert x \\rVert_2^2 = 0$ 的特殊情况。\n4.  计算系数的变化量：$d = b_{\\text{new}} - b$。\n5.  计算 $RSS$ 的变化量：$\\Delta RSS = d^2 \\lVert x \\rVert_2^2 - 2d(r^T x)$。\n6.  报告 $\\Delta RSS$ 的值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of single-coordinate update problems for Ridge and Lasso\n    regression and computes the change in Residual Sum of Squares (RSS).\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Ridge happy path\n        {'x': [1.2, -0.3, 0.5, 0.0, 2.1],\n         'r': [0.8, -1.1, 0.3, 0.0, 2.0],\n         'b': 0.4,\n         'penalty': 'ridge',\n         'param': 0.7},\n\n        # Case 2: Lasso happy path (nonzero update expected)\n        {'x': [0.5, -1.0, 0.3, 1.2, -0.7, 0.4],\n         'r': [1.0, 0.5, -0.2, 0.3, -1.5, 0.1],\n         'b': -0.1,\n         'penalty': 'lasso',\n         'param': 0.2},\n\n        # Case 3: Lasso thresholding to zero (edge case)\n        {'x': [0.2, -0.1, 0.05, 0.0],\n         'r': [0.01, -0.02, 0.03, -0.04],\n         'b': 0.5,\n         'penalty': 'lasso',\n         'param': 0.2},\n\n        # Case 4: Ridge with zero predictor column (boundary case)\n        {'x': [0.0, 0.0, 0.0, 0.0, 0.0],\n         'r': [1.0, -1.0, 0.5, -0.5, 2.0],\n         'b': 3.0,\n         'penalty': 'ridge',\n         'param': 1.0},\n\n        # Case 5: Lasso with numerically tiny predictor (numerical stability case)\n        {'x': [1e-8, -2e-8, 3e-8],\n         'r': [0.0, 0.0, 0.0],\n         'b': 1.0,\n         'penalty': 'lasso',\n         'param': 0.1}\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x_vec = np.array(case['x'], dtype=np.float64)\n        r_vec = np.array(case['r'], dtype=np.float64)\n        b = case['b']\n        penalty_type = case['penalty']\n        param = case['param']\n        \n        b_new = 0.0\n\n        # Compute scalar quantities needed for the updates\n        x_sq_norm = np.dot(x_vec, x_vec) # Corresponds to ||x||_2^2\n        r_dot_x = np.dot(r_vec, x_vec)   # Corresponds to r^T x\n\n        if penalty_type == 'ridge':\n            # Ridge update: b_new = (r^T x + b ||x||^2) / (||x||^2 + alpha)\n            alpha = param\n            numerator = r_dot_x + b * x_sq_norm\n            denominator = x_sq_norm + alpha\n            # Denominator is guaranteed to be non-zero since alpha >= 0 and we\n            # handle the pure ||x||^2 = 0 case implicitly. If ||x||^2 is 0,\n            # numerator is 0, so b_new becomes 0 correctly.\n            if denominator != 0:\n                b_new = numerator / denominator\n            else:\n                 # This case only happens if x_sq_norm=0 and alpha=0,\n                 # which is unpenalized OLS and ill-defined for a zero predictor.\n                 # With alpha>0, this will not occur.\n                 b_new = 0.0\n\n        elif penalty_type == 'lasso':\n            # Lasso update: b_new = S(r^T x + b ||x||^2, lambda) / ||x||^2\n            # where S is the soft-thresholding operator.\n            lmbda = param\n            \n            # Handle the case where the predictor is a zero vector.\n            # In this case, the coordinate has no effect on RSS, so the\n            # L1 penalty lambda * |b_new| is minimized at b_new = 0.\n            if x_sq_norm == 0:\n                b_new = 0.0\n            else:\n                z = r_dot_x + b * x_sq_norm\n                if z > lmbda:\n                    b_new = (z - lmbda) / x_sq_norm\n                elif z  -lmbda:\n                    b_new = (z + lmbda) / x_sq_norm\n                else: # |z| = lmbda\n                    b_new = 0.0\n        \n        # Compute the change in coefficient\n        d = b_new - b\n        \n        # Compute the exact change in RSS: Delta_RSS = d^2 ||x||^2 - 2d (r^T x)\n        delta_rss = d**2 * x_sq_norm - 2 * d * r_dot_x\n        \n        results.append(delta_rss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}