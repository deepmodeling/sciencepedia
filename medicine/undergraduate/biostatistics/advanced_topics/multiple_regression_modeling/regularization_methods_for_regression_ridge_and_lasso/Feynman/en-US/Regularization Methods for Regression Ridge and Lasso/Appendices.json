{
    "hands_on_practices": [
        {
            "introduction": "Before diving into the solutions that regularization offers, it's crucial to understand the problem it is designed to solve. In many biostatistical applications, predictor variables like biomarker levels are often highly correlated, a condition known as multicollinearity. This exercise demonstrates firsthand how multicollinearity destabilizes Ordinary Least Squares (OLS) regression by dramatically inflating the variance of coefficient estimates, making them unreliable for interpretation or prediction. By working through this calculation , you will gain a tangible appreciation for why alternative methods like Ridge and Lasso are essential.",
            "id": "4947371",
            "problem": "A biostatistics team is modeling a continuous clinical outcome $y$ from two laboratory biomarkers whose assays are known to be highly correlated. The data for $n=5$ patients and $p=2$ biomarkers are assembled into the design matrix $X$ (columns scaled to comparable units):\n$$\nX=\\begin{pmatrix}\n1 & 1.002 \\\\\n2 & 2.001 \\\\\n3 & 3.000 \\\\\n4 & 4.001 \\\\\n5 & 5.002\n\\end{pmatrix}.\n$$\nAssume the standard linear model $y = X\\beta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$, where $\\mathcal{N}$ denotes the normal distribution and $I$ is the identity matrix. Suppose prior measurement studies provide a reliable estimate of the residual variance $\\sigma^2 = 0.25$. Using only these model definitions and properties of Ordinary Least Squares (OLS), derive the sampling variance of the OLS estimator for the first coefficient $\\beta_1$, and then compute its numerical value for the given $X$ and $\\sigma^2$. Round your final numerical answer to four significant figures. Express the final answer as a real number with no units. Additionally, interpret the magnitude of this variance in the context of collinearity and explain why regularization methods such as ridge regression and the least absolute shrinkage and selection operator (lasso) are relevant here.",
            "solution": "The problem requires the derivation and calculation of the sampling variance for an Ordinary Least Squares (OLS) coefficient estimate, followed by an interpretation of the result in the context of multicollinearity and the relevance of regularization techniques.\n\n**Part 1: Derivation of the Sampling Variance of an OLS Estimator**\n\nThe standard linear model is given by $y = X\\beta + \\varepsilon$, where $y$ is the $n \\times 1$ vector of outcomes, $X$ is the $n \\times p$ design matrix, $\\beta$ is the $p \\times 1$ vector of coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of errors. The OLS estimator $\\hat{\\beta}$ is derived by minimizing the sum of squared residuals, $SSR = (y - X\\beta)^\\top(y - X\\beta)$. This yields the normal equations $X^\\top X \\hat{\\beta} = X^\\top y$. Assuming $X^\\top X$ is invertible, the OLS estimator is:\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n$$\nTo find the sampling distribution of $\\hat{\\beta}$, we substitute the true model $y = X\\beta + \\varepsilon$ into the expression for $\\hat{\\beta}$:\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top (X\\beta + \\varepsilon) = (X^\\top X)^{-1} (X^\\top X) \\beta + (X^\\top X)^{-1} X^\\top \\varepsilon = \\beta + (X^\\top X)^{-1} X^\\top \\varepsilon\n$$\nThe problem states that the errors follow a normal distribution $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$. This means the expected value of the error vector is $E[\\varepsilon] = 0$ and the covariance matrix of the errors is $E[\\varepsilon \\varepsilon^\\top] = \\sigma^2 I$.\nThe expected value of the estimator is:\n$$\nE[\\hat{\\beta}] = E[\\beta + (X^\\top X)^{-1} X^\\top \\varepsilon] = \\beta + (X^\\top X)^{-1} X^\\top E[\\varepsilon] = \\beta + 0 = \\beta\n$$\nThis shows that the OLS estimator is unbiased. The covariance matrix of $\\hat{\\beta}$ is defined as $\\text{Cov}(\\hat{\\beta}) = E[(\\hat{\\beta} - E[\\hat{\\beta}])(\\hat{\\beta} - E[\\hat{\\beta}])^\\top]$. Since $E[\\hat{\\beta}] = \\beta$, we have $\\hat{\\beta} - \\beta = (X^\\top X)^{-1} X^\\top \\varepsilon$. Therefore:\n$$\n\\text{Cov}(\\hat{\\beta}) = E[((X^\\top X)^{-1} X^\\top \\varepsilon)((X^\\top X)^{-1} X^\\top \\varepsilon)^\\top]\n$$\nUsing the property $(AB)^\\top = B^\\top A^\\top$, this becomes:\n$$\n\\text{Cov}(\\hat{\\beta}) = E[(X^\\top X)^{-1} X^\\top \\varepsilon \\varepsilon^\\top X ((X^\\top X)^{-1})^\\top]\n$$\nSince $X$ is treated as fixed (non-stochastic), we can move it outside the expectation:\n$$\n\\text{Cov}(\\hat{\\beta}) = (X^\\top X)^{-1} X^\\top E[\\varepsilon \\varepsilon^\\top] X ((X^\\top X)^{-1})^\\top\n$$\nSubstituting $E[\\varepsilon \\varepsilon^\\top] = \\sigma^2 I$:\n$$\n\\text{Cov}(\\hat{\\beta}) = (X^\\top X)^{-1} X^\\top (\\sigma^2 I) X ((X^\\top X)^{-1})^\\top = \\sigma^2 (X^\\top X)^{-1} X^\\top X ((X^\\top X)^{-1})^\\top\n$$\nThis simplifies to $\\sigma^2 (X^\\top X)^{-1} I ((X^\\top X)^{-1})^\\top$. Since $X^\\top X$ is symmetric, its inverse $(X^\\top X)^{-1}$ is also symmetric, so $((X^\\top X)^{-1})^\\top = (X^\\top X)^{-1}$. The final expression for the covariance matrix of the OLS estimator is:\n$$\n\\text{Cov}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}\n$$\nThe sampling variance of the estimator for the first coefficient, $\\hat{\\beta}_1$, is the first diagonal element of this covariance matrix. If we denote $C = (X^\\top X)^{-1}$, then:\n$$\n\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 C_{11} = \\sigma^2 [(X^\\top X)^{-1}]_{11}\n$$\nThis is the general formula we will use for the calculation.\n\n**Part 2: Numerical Calculation**\n\nThe given data are $n=5$, $p=2$, $\\sigma^2 = 0.25$, and the design matrix:\n$$\nX = \\begin{pmatrix}\n1 & 1.002 \\\\\n2 & 2.001 \\\\\n3 & 3.000 \\\\\n4 & 4.001 \\\\\n5 & 5.002\n\\end{pmatrix}\n$$\nFirst, we compute the matrix $X^\\top X$:\n$$\nX^\\top X = \\begin{pmatrix}\n1 & 2 & 3 & 4 & 5 \\\\\n1.002 & 2.001 & 3.000 & 4.001 & 5.002\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 1.002 \\\\\n2 & 2.001 \\\\\n3 & 3.000 \\\\\n4 & 4.001 \\\\\n5 & 5.002\n\\end{pmatrix}\n$$\nThe elements are:\n$(X^\\top X)_{11} = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 1+4+9+16+25 = 55$\n$(X^\\top X)_{12} = (X^\\top X)_{21} = 1(1.002) + 2(2.001) + 3(3.000) + 4(4.001) + 5(5.002) = 1.002 + 4.002 + 9 + 16.004 + 25.010 = 55.018$\n$(X^\\top X)_{22} = 1.002^2 + 2.001^2 + 3.000^2 + 4.001^2 + 5.002^2 = 1.004004 + 4.004001 + 9 + 16.008001 + 25.020004 = 55.03601$\nSo, the matrix is:\n$$\nX^\\top X = \\begin{pmatrix} 55 & 55.018 \\\\ 55.018 & 55.03601 \\end{pmatrix}\n$$\nNext, we compute the inverse of this $2 \\times 2$ matrix. The inverse of a matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is $\\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant is:\n$$\n\\det(X^\\top X) = (55)(55.03601) - (55.018)^2 = 3026.98055 - 3026.980324 = 0.000226\n$$\nThe inverse is:\n$$\n(X^\\top X)^{-1} = \\frac{1}{0.000226} \\begin{pmatrix} 55.03601 & -55.018 \\\\ -55.018 & 55 \\end{pmatrix}\n$$\nWe need the $(1,1)$ element of this matrix, which is:\n$$\n[(X^\\top X)^{-1}]_{11} = \\frac{55.03601}{0.000226}\n$$\nNow we can compute the variance of $\\hat{\\beta}_1$ using $\\sigma^2 = 0.25$:\n$$\n\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 [(X^\\top X)^{-1}]_{11} = 0.25 \\times \\frac{55.03601}{0.000226} \\approx 60880.542\n$$\nRounding to four significant figures, we get $60880$. In scientific notation, this is $6.088 \\times 10^4$.\n\n**Part 3: Interpretation and Relevance of Regularization**\n\nThe calculated variance, $\\text{Var}(\\hat{\\beta}_1) \\approx 6.088 \\times 10^4$, is an exceptionally large value. A large variance for a coefficient estimator implies that the estimate is very imprecise and unstable. If we were to collect a different sample of data from the same population, the estimated value of $\\hat{\\beta}_1$ could change dramatically. This makes the coefficient estimate unreliable for inference or prediction.\n\nThe root cause of this high variance is severe multicollinearity. Multicollinearity occurs when predictor variables in a regression model are highly correlated. In this problem, the two columns of the design matrix $X$ are nearly identical, indicating an extremely high correlation. The correlation coefficient between the two biomarker measurements is virtually $1$. When predictors are highly correlated, the matrix $X^\\top X$ becomes nearly singular (i.e., its determinant is close to zero). In our case, $\\det(X^\\top X) = 0.000226$, which is a very small number. The process of inverting a nearly-singular matrix is numerically unstable and leads to an inverse matrix with very large entries. Since the variances of the OLS estimators are proportional to the diagonal elements of this inverse matrix (as shown by the formula $\\text{Cov}(\\hat{\\beta}) = \\sigma^2(X^\\top X)^{-1}$), the variances are \"inflated\" to very large values. This phenomenon is often quantified by the Variance Inflation Factor (VIF).\n\nRegularization methods are specifically designed to address the problem of multicollinearity. OLS fails in this situation because it seeks an unbiased estimator, which comes at the cost of enormous variance. Regularization methods introduce a small amount of bias in the estimates to achieve a large reduction in variance, leading to a much lower overall mean squared error ($MSE = \\text{Variance} + \\text{Bias}^2$).\n\n-   **Ridge Regression**: Ridge regression adds a penalty to the OLS objective function, minimizing $\\sum (y_i - X_i\\beta)^2 + \\lambda \\sum \\beta_j^2$. The resulting estimator is $\\hat{\\beta}_{\\text{ridge}} = (X^\\top X + \\lambda I)^{-1} X^\\top y$, where $\\lambda > 0$ is a tuning parameter. The addition of the term $\\lambda I$ to $X^\\top X$ makes the matrix to be inverted well-conditioned and non-singular, even when $X^\\top X$ is not. This stabilizes the inverse and produces coefficient estimates with much lower variance.\n-   **Lasso (Least Absolute Shrinkage and Selection Operator)**: The lasso minimizes $\\sum (y_i - X_i\\beta)^2 + \\lambda \\sum |\\beta_j|$. Like ridge, it shrinks coefficients toward zero to reduce variance. However, the $\\ell_1$ penalty has the unique property of shrinking some coefficients to exactly zero. In a case of two highly correlated predictors like this, the lasso is likely to select one of the biomarkers and assign it a non-zero coefficient while eliminating the other by setting its coefficient to zero. This provides a simpler, more interpretable model by performing automatic variable selection.\n\nIn conclusion, the extreme variance of the OLS estimator highlights its breakdown in the presence of severe multicollinearity. Regularization techniques like ridge and lasso are crucial statistical tools that provide stable, reliable, and more interpretable models in such common biostatistical scenarios.",
            "answer": "$$\\boxed{6.088 \\times 10^4}$$"
        },
        {
            "introduction": "Having seen the pitfalls of OLS with correlated data, we now turn to a powerful solution: Ridge regression. This practice moves beyond simply applying the Ridge formula to exploring its deep mathematical structure. You will derive the Ridge estimator and its associated \"hat matrix,\" which linearly transforms observed outcomes into fitted values, and then uncover how the regularization parameter $\\lambda$ controls the model's flexibility by calculating the effective degrees of freedom . This provides a fundamental insight into the bias-variance trade-off at the heart of regularization.",
            "id": "4947422",
            "problem": "Consider a biostatistical linear regression setting where a centered response vector $y \\in \\mathbb{R}^{n}$ is modeled using a centered design matrix $X \\in \\mathbb{R}^{n \\times p}$ of biological predictors (for example, standardized gene expression or clinical covariates), so that no intercept term is needed. To mitigate multicollinearity and overfitting, the ridge regression estimator $\\hat{\\beta}^{\\text{ridge}} \\in \\mathbb{R}^{p}$ is defined as the unique minimizer of the penalized least-squares objective\n$$\n\\lVert y - X\\beta\\rVert_{2}^{2} + \\lambda \\lVert\\beta\\rVert_{2}^{2},\n$$\nwith penalty parameter $\\lambda > 0$. Starting from this definition and using only linear algebra facts such as properties of derivatives, symmetry, orthogonal diagonalization, and the existence of a singular value decomposition (SVD), proceed as follows:\n\n1. Derive the normal equations that characterize $\\hat{\\beta}^{\\text{ridge}}$ and obtain an explicit expression for $\\hat{\\beta}^{\\text{ridge}}$ in terms of $X$, $y$, and $\\lambda$.\n2. Derive the fitted values $\\hat{y} = X \\hat{\\beta}^{\\text{ridge}}$ and show that there exists a linear map $H_{\\lambda} \\in \\mathbb{R}^{n \\times n}$ such that $\\hat{y} = H_{\\lambda} y$. Identify $H_{\\lambda}$ explicitly in terms of $X$ and $\\lambda$.\n3. Let $r = \\operatorname{rank}(X)$, and let the singular value decomposition (SVD) of $X$ be $X = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{p \\times p}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times p}$ has nonzero singular values $\\sigma_{1}, \\dots, \\sigma_{r} > 0$ on its main diagonal. Using only these definitions and fundamental trace properties, derive a closed-form analytic expression for $\\operatorname{tr}(H_{\\lambda})$ as a function of $\\lambda$ and the nonzero singular values $\\{\\sigma_{j}\\}_{j=1}^{r}$. \n\nProvide your final answer as a single closed-form analytic expression for $\\operatorname{tr}(H_{\\lambda})$ in terms of $\\lambda$ and $\\{\\sigma_{j}\\}_{j=1}^{r}$. No numerical approximation or rounding is required.",
            "solution": "The problem is structured in three parts. We will address them in sequence.\n\nPart 1: Derivation of the normal equations and the expression for $\\hat{\\beta}^{\\text{ridge}}$.\n\nThe objective function for ridge regression is given by\n$$L(\\beta) = \\lVert y - X\\beta\\rVert_{2}^{2} + \\lambda \\lVert\\beta\\rVert_{2}^{2}$$\nwhere $y \\in \\mathbb{R}^{n}$ is the centered response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the centered design matrix, $\\beta \\in \\mathbb{R}^{p}$ is the vector of coefficients, and $\\lambda > 0$ is the regularization parameter. We can express the squared Euclidean norms using vector transposes:\n$$L(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta) + \\lambda \\beta^{\\top}\\beta$$\nExpanding the first term, we get:\n$$L(\\beta) = y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta + \\lambda \\beta^{\\top}\\beta$$\nSince $y^{\\top}X\\beta$ is a scalar ($1 \\times 1$ matrix), it is equal to its transpose, $(\\beta^{\\top}X^{\\top}y)$. This allows us to combine the two cross-product terms:\n$$L(\\beta) = y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta + \\lambda \\beta^{\\top}I\\beta$$\nWe can factor out $\\beta^{\\top}$ and $\\beta$ to write the function as a quadratic form in $\\beta$:\n$$L(\\beta) = y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}(X^{\\top}X + \\lambda I)\\beta$$\nTo find the coefficient vector $\\hat{\\beta}^{\\text{ridge}}$ that minimizes $L(\\beta)$, we compute the gradient of $L(\\beta)$ with respect to $\\beta$ and set it to the zero vector. The objective function is strictly convex for $\\lambda > 0$, guaranteeing a unique global minimum. Using the rules of vector calculus ($\\frac{\\partial}{\\partial x} c^{\\top}x = c$ and $\\frac{\\partial}{\\partial x} x^{\\top}Ax = 2Ax$ for symmetric $A$), the gradient is:\n$$\\frac{\\partial L(\\beta)}{\\partial \\beta} = -2X^{\\top}y + 2(X^{\\top}X + \\lambda I)\\beta$$\nSetting the gradient to zero at $\\beta = \\hat{\\beta}^{\\text{ridge}}$ gives the condition:\n$$-2X^{\\top}y + 2(X^{\\top}X + \\lambda I)\\hat{\\beta}^{\\text{ridge}} = 0$$\nDividing by $2$ and rearranging leads to the normal equations for ridge regression:\n$$(X^{\\top}X + \\lambda I)\\hat{\\beta}^{\\text{ridge}} = X^{\\top}y$$\nTo obtain an explicit expression for $\\hat{\\beta}^{\\text{ridge}}$, we must invert the matrix $(X^{\\top}X + \\lambda I)$. The matrix $X^{\\top}X$ is positive semi-definite. Since $\\lambda > 0$, the matrix $\\lambda I$ is positive definite. The sum of a positive semi-definite matrix and a positive definite matrix is positive definite, and thus always invertible. Pre-multiplying by the inverse of $(X^{\\top}X + \\lambda I)$ gives the explicit solution:\n$$\\hat{\\beta}^{\\text{ridge}} = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}y$$\n\nPart 2: Derivation of the fitted values $\\hat{y}$ and the hat matrix $H_{\\lambda}$.\n\nThe fitted values, denoted $\\hat{y}$, are predicted by the model using the estimated coefficients:\n$$\\hat{y} = X\\hat{\\beta}^{\\text{ridge}}$$\nSubstituting the expression for $\\hat{\\beta}^{\\text{ridge}}$ from Part 1:\n$$\\hat{y} = X \\left( (X^{\\top}X + \\lambda I)^{-1}X^{\\top}y \\right)$$\nUsing the associativity of matrix multiplication, we can regroup the terms to express $\\hat{y}$ as a linear transformation of $y$:\n$$\\hat{y} = \\left( X(X^{\\top}X + \\lambda I)^{-1}X^{\\top} \\right) y$$\nThis shows the existence of a linear map $H_{\\lambda} \\in \\mathbb{R}^{n \\times n}$ such that $\\hat{y} = H_{\\lambda} y$. By inspection, this matrix, known as the hat matrix or smoother matrix for ridge regression, is:\n$$H_{\\lambda} = X(X^{\\top}X + \\lambda I)^{-1}X^{\\top}$$\n\nPart 3: Derivation of a closed-form expression for $\\operatorname{tr}(H_{\\lambda})$.\n\nOur goal is to compute $\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}\\left(X(X^{\\top}X + \\lambda I)^{-1}X^{\\top}\\right)$. We employ the cyclic property of the trace, which states that $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$ for matrices $A \\in \\mathbb{R}^{n \\times p}$ and $B \\in \\mathbb{R}^{p \\times n}$. Let $A = X$ and $B = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}$. Then,\n$$\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}\\left((X^{\\top}X + \\lambda I)^{-1}X^{\\top}X\\right)$$\nWe now use the singular value decomposition (SVD) of $X$, given as $X = U \\Sigma V^{\\top}$. Here, $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{p \\times p}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times p}$ is a rectangular diagonal matrix with $r = \\operatorname{rank}(X)$ non-zero singular values $\\sigma_1, \\dots, \\sigma_r > 0$. First, we express $X^{\\top}X$ in terms of the SVD:\n$$X^{\\top}X = (U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top}) = V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top} = V\\Sigma^{\\top}I_n\\Sigma V^{\\top} = V(\\Sigma^{\\top}\\Sigma) V^{\\top}$$\nThe matrix $\\Sigma^{\\top}\\Sigma \\in \\mathbb{R}^{p \\times p}$ is a diagonal matrix with diagonal entries $(\\sigma_1^2, \\dots, \\sigma_r^2, 0, \\dots, 0)$. Now substitute this into the trace expression. First, consider the term $(X^{\\top}X + \\lambda I)$:\n$$X^{\\top}X + \\lambda I = V(\\Sigma^{\\top}\\Sigma) V^{\\top} + \\lambda I = V(\\Sigma^{\\top}\\Sigma) V^{\\top} + \\lambda VV^{\\top} = V(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top}$$\nThe inverse is then:\n$$(X^{\\top}X + \\lambda I)^{-1} = (V(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top})^{-1} = V(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}V^{\\top}$$\nNow we assemble the term inside the trace:\n$$(X^{\\top}X + \\lambda I)^{-1}X^{\\top}X = \\left(V(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}V^{\\top}\\right) \\left(V(\\Sigma^{\\top}\\Sigma) V^{\\top}\\right)$$\n$$= V(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}(V^{\\top}V)(\\Sigma^{\\top}\\Sigma) V^{\\top} = V\\left((\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}(\\Sigma^{\\top}\\Sigma)\\right)V^{\\top}$$\nLet $M = (\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}(\\Sigma^{\\top}\\Sigma)$. Using the cyclic property of the trace on $\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}(VMV^{\\top})$:\n$$\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}(V^{\\top}VM) = \\operatorname{tr}(I_pM) = \\operatorname{tr}(M)$$\nThe matrix $M$ is a product of two commuting diagonal matrices, and is therefore itself a diagonal matrix. Its $j$-th diagonal element $M_{jj}$ is the product of the corresponding diagonal elements of $(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}$ and $\\Sigma^{\\top}\\Sigma$:\n$$M_{jj} = \\frac{1}{(\\Sigma^{\\top}\\Sigma)_{jj} + \\lambda} \\times (\\Sigma^{\\top}\\Sigma)_{jj}$$\nFor $j = 1, \\dots, r$, the $j$-th diagonal element of $\\Sigma^{\\top}\\Sigma$ is $\\sigma_j^2$. Thus,\n$$M_{jj} = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} \\quad \\text{for } j = 1, \\dots, r$$\nFor $j = r+1, \\dots, p$, the $j$-th diagonal element is $0$. Thus,\n$$M_{jj} = \\frac{0}{0 + \\lambda} = 0 \\quad \\text{for } j = r+1, \\dots, p$$\nThe trace of $M$ is the sum of its diagonal elements:\n$$\\operatorname{tr}(M) = \\sum_{j=1}^{p} M_{jj} = \\sum_{j=1}^{r} \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} + \\sum_{j=r+1}^{p} 0$$\nThus, the final closed-form analytic expression for the trace of $H_{\\lambda}$ is:\n$$\\operatorname{tr}(H_{\\lambda}) = \\sum_{j=1}^{r} \\frac{\\sigma_{j}^{2}}{\\sigma_{j}^{2} + \\lambda}$$\nThis quantity is also known as the effective degrees of freedom of the ridge regression fit.",
            "answer": "$$\\boxed{\\sum_{j=1}^{r} \\frac{\\sigma_{j}^{2}}{\\sigma_{j}^{2} + \\lambda}}$$"
        },
        {
            "introduction": "The theoretical formulas for Ridge and Lasso are elegant, but how do we compute the coefficients for a real dataset, especially one with thousands of predictors? This hands-on coding practice demystifies the fitting process by guiding you through a single step of the coordinate descent algorithm, the workhorse behind modern regularized regression solvers. By implementing an update for one coefficient at a time , you will see exactly how the Ridge and Lasso penalties operate algorithmically and build an intuitive understanding of concepts like soft-thresholding.",
            "id": "4947418",
            "problem": "Consider a single-coordinate update in penalized linear regression, a method widely used in biostatistics for controlling variance and improving prediction in high-dimensional biological data. Let there be $n$ observations. Let the response vector be $y \\in \\mathbb{R}^n$, and suppose the current fitted model has residual vector $r \\in \\mathbb{R}^n$ defined by $r = y - X \\beta$, where $X \\in \\mathbb{R}^{n \\times p}$ is a design matrix and $\\beta \\in \\mathbb{R}^p$ is the current coefficient vector. Focus on a single predictor column $x \\in \\mathbb{R}^n$ corresponding to one coordinate, whose current coefficient is $b \\in \\mathbb{R}$. A one-step coordinate update changes $b$ by an amount $d \\in \\mathbb{R}$ to a new coefficient $b_{\\text{new}} = b + d$, producing a new residual vector $r' = r - x d$ and a new residual sum of squares (RSS) given by $RSS' = \\lVert r' \\rVert_2^2$.\n\nStarting only from the definitions of least squares and convex penalties:\n- The residual sum of squares is $RSS(\\beta) = \\lVert y - X \\beta \\rVert_2^2$.\n- The ridge objective is $\\tfrac{1}{2} \\lVert y - X \\beta \\rVert_2^2 + \\tfrac{\\alpha}{2} \\lVert \\beta \\rVert_2^2$ for a penalty parameter $\\alpha \\ge 0$.\n- The lasso objective is $\\tfrac{1}{2} \\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1$ for a penalty parameter $\\lambda \\ge 0$.\n\nYour task is to implement a single-coordinate update for $b$ and compute the exact change in residual sum of squares as a function of the update. The update must be determined by minimizing the appropriate penalized objective with respect to the single coordinate, holding all other coordinates fixed. Use the partial residual $r_j = y - X_{-j} \\beta_{-j}$ conceptually, but in computation you are given $r$ and $b$, so you should form $r_j$ using $r_j = r + x b$ to eliminate dependence on the unknown submodel. Then perform the update for one of the following penalties:\n- Ridge: use the penalty parameter $\\alpha$.\n- Lasso: use the penalty parameter $\\lambda$.\n\nAfter computing $b_{\\text{new}}$, define $d = b_{\\text{new}} - b$, and compute the exact change in residual sum of squares $\\Delta RSS = RSS' - RSS$ as a function of $d$, $x$, and $r$.\n\nImplement a program that, for each test case below, performs:\n- Construct $r_j = r + x b$.\n- Compute the one-step coordinate update $b_{\\text{new}}$ minimizing the chosen penalized objective along coordinate $b$.\n- Compute $d = b_{\\text{new}} - b$.\n- Compute the exact change in residual sum of squares $\\Delta RSS$ after applying $d$ to $b$ (without changing any other coordinates).\n\nReport $\\Delta RSS$ for each test case as a float.\n\nTest suite (each case gives $n$, $x$, $r$, $b$, and the penalty specification):\n1. Ridge happy path:\n   - $n = 5$\n   - $x = [1.2, -0.3, 0.5, 0.0, 2.1]$\n   - $r = [0.8, -1.1, 0.3, 0.0, 2.0]$\n   - $b = 0.4$\n   - Ridge with $\\alpha = 0.7$.\n2. Lasso happy path (nonzero update expected):\n   - $n = 6$\n   - $x = [0.5, -1.0, 0.3, 1.2, -0.7, 0.4]$\n   - $r = [1.0, 0.5, -0.2, 0.3, -1.5, 0.1]$\n   - $b = -0.1$\n   - Lasso with $\\lambda = 0.2$.\n3. Lasso thresholding to zero (edge case):\n   - $n = 4$\n   - $x = [0.2, -0.1, 0.05, 0.0]$\n   - $r = [0.01, -0.02, 0.03, -0.04]$\n   - $b = 0.5$\n   - Lasso with $\\lambda = 0.2$.\n4. Ridge with zero predictor column (boundary case):\n   - $n = 5$\n   - $x = [0.0, 0.0, 0.0, 0.0, 0.0]$\n   - $r = [1.0, -1.0, 0.5, -0.5, 2.0]$\n   - $b = 3.0$\n   - Ridge with $\\alpha = 1.0$.\n5. Lasso with numerically tiny predictor (numerical stability case):\n   - $n = 3$\n   - $x = [10^{-8}, -2 \\cdot 10^{-8}, 3 \\cdot 10^{-8}]$\n   - $r = [0.0, 0.0, 0.0]$\n   - $b = 1.0$\n   - Lasso with $\\lambda = 0.1$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[result_1,result_2,\\dots]$, in the exact order of the test cases listed above.",
            "solution": "The general objective function for penalized linear regression is:\n$$L(\\beta) = \\frac{1}{2}\\lVert y - X \\beta \\rVert_2^2 + P(\\beta)$$\nwhere $P(\\beta)$ is a penalty function. We aim to update a single coefficient, denoted by $b$, corresponding to a predictor column $x$. All other coefficients are held fixed. The new coefficient will be $b_{\\text{new}}$. The objective function, viewed as a function of $b_{\\text{new}}$ alone, can be expressed using the partial residual $r_j = y - \\sum_{k \\neq j} x_k \\beta_k$. As specified, this can be computed from the full current residual $r = y - X\\beta$ as $r_j = r + xb$.\n\nThe $RSS$ portion of the objective can be written as:\n$$\\frac{1}{2}\\lVert r_j - x b_{\\text{new}} \\rVert_2^2 = \\frac{1}{2}(r_j - x b_{\\text{new}})^\\top(r_j - x b_{\\text{new}}) = \\frac{1}{2} (\\lVert r_j \\rVert_2^2 - 2 b_{\\text{new}} (r_j^\\top x) + b_{\\text{new}}^2 \\lVert x \\rVert_2^2)$$\nThis is a quadratic function of $b_{\\text{new}}$. The term $\\frac{1}{2}\\lVert r_j \\rVert_2^2$ is constant with respect to $b_{\\text{new}}$ and can be ignored during minimization.\n\n**1. Ridge Regression Update**\n\nFor ridge regression, the penalty is $P(\\beta) = \\frac{\\alpha}{2} \\lVert \\beta \\rVert_2^2 = \\frac{\\alpha}{2} \\sum_k \\beta_k^2$. The objective function for the single coordinate $b_{\\text{new}}$ is:\n$$L_{\\text{ridge}}(b_{\\text{new}}) = \\frac{1}{2} \\lVert r_j - x b_{\\text{new}} \\rVert_2^2 + \\frac{\\alpha}{2} b_{\\text{new}}^2 + \\text{const}$$\nThis function is differentiable and convex. To find the minimum, we set its derivative with respect to $b_{\\text{new}}$ to zero:\n$$\\frac{\\partial L_{\\text{ridge}}}{\\partial b_{\\text{new}}} = -r_j^\\top x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\alpha b_{\\text{new}} = 0$$\nSolving for $b_{\\text{new}}$:\n$$b_{\\text{new}} (\\lVert x \\rVert_2^2 + \\alpha) = r_j^\\top x$$\n$$b_{\\text{new}} = \\frac{r_j^\\top x}{\\lVert x \\rVert_2^2 + \\alpha}$$\nSubstituting $r_j = r + xb$:\n$$r_j^\\top x = (r + xb)^\\top x = r^\\top x + b(x^\\top x) = r^\\top x + b \\lVert x \\rVert_2^2$$\nThus, the update rule for the new coefficient is:\n$$b_{\\text{new}} = \\frac{r^\\top x + b \\lVert x \\rVert_2^2}{\\lVert x \\rVert_2^2 + \\alpha}$$\nIf the predictor column $x$ is a zero vector, $\\lVert x \\rVert_2^2 = 0$ and $r^\\top x = 0$, leading to $b_{\\text{new}} = 0$. Since $\\alpha \\ge 0$, the denominator is always positive if $\\alpha > 0$ or $\\lVert x \\rVert_2^2 > 0$.\n\n**2. Lasso Regression Update**\n\nFor lasso regression, the penalty is $P(\\beta) = \\lambda \\lVert \\beta \\rVert_1 = \\lambda \\sum_k |\\beta_k|$. The objective for the single coordinate $b_{\\text{new}}$ is:\n$$L_{\\text{lasso}}(b_{\\text{new}}) = \\frac{1}{2} \\lVert r_j - x b_{\\text{new}} \\rVert_2^2 + \\lambda |b_{\\text{new}}| + \\text{const}$$\nThis objective is convex but not differentiable at $b_{\\text{new}} = 0$. We use subgradient optimization. The subgradient of the objective is:\n$$\\partial L_{\\text{lasso}}(b_{\\text{new}}) = -r_j^\\top x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\lambda \\cdot \\partial |b_{\\text{new}}|$$\nwhere $\\partial |u|$ is the subgradient of the absolute value function: $\\text{sgn}(u)$ for $u \\neq 0$ and the interval $[-1, 1]$ for $u=0$.\nAt the minimum, the subgradient set must contain $0$:\n$$0 \\in -r_j^\\top x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\lambda \\cdot \\text{sgn}(b_{\\text{new}})$$\nLet $z = r_j^\\top x = r^\\top x + b \\lVert x \\rVert_2^2$ and assume $\\lVert x \\rVert_2^2 > 0$. The condition becomes:\n$$z - b_{\\text{new}} \\lVert x \\rVert_2^2 \\in \\lambda \\cdot \\text{sgn}(b_{\\text{new}})$$\nThis yields the solution:\n- If $z > \\lambda$, then $b_{\\text{new}} > 0$: $z - b_{\\text{new}} \\lVert x \\rVert_2^2 = \\lambda \\implies b_{\\text{new}} = (z - \\lambda) / \\lVert x \\rVert_2^2$.\n- If $z  -\\lambda$, then $b_{\\text{new}}  0$: $z - b_{\\text{new}} \\lVert x \\rVert_2^2 = -\\lambda \\implies b_{\\text{new}} = (z + \\lambda) / \\lVert x \\rVert_2^2$.\n- If $|z| \\le \\lambda$, then $b_{\\text{new}} = 0$.\n\nThis is the soft-thresholding function, $S(z, \\lambda) = \\text{sgn}(z) \\max(|z|-\\lambda, 0)$. The update rule is:\n$$b_{\\text{new}} = \\frac{S(r^\\top x + b \\lVert x \\rVert_2^2, \\lambda)}{\\lVert x \\rVert_2^2}$$\nIf $\\lVert x \\rVert_2^2 = 0$, the objective to minimize is simply $\\lambda |b_{\\text{new}}|$ (plus a constant), which is minimized at $b_{\\text{new}} = 0$. This case must be handled separately in implementation.\n\n**3. Change in Residual Sum of Squares ($\\Delta RSS$)**\n\nThe problem asks for the exact change in $RSS$, defined as $\\Delta RSS = RSS' - RSS$.\nThe current $RSS$ is $\\lVert r \\rVert_2^2$.\nThe new residual is $r' = r - xd$, where $d = b_{\\text{new}} - b$ is the change in the coefficient.\nThe new $RSS'$ is:\n$$RSS' = \\lVert r' \\rVert_2^2 = \\lVert r - xd \\rVert_2^2 = (r - xd)^\\top(r - xd)$$\n$$RSS' = r^\\top r - (xd)^\\top r - r^\\top(xd) + (xd)^\\top(xd)$$\nSince $r^\\top(xd) = d(r^\\top x)$ is a scalar, it equals its transpose $(xd)^\\top r = d(x^\\top r)$.\n$$RSS' = \\lVert r \\rVert_2^2 - 2d(r^\\top x) + d^2(x^\\top x) = RSS - 2d(r^\\top x) + d^2 \\lVert x \\rVert_2^2$$\nTherefore, the change in $RSS$ is:\n$$\\Delta RSS = RSS' - RSS = d^2 \\lVert x \\rVert_2^2 - 2d(r^\\top x)$$\nThis formula allows for direct computation of $\\Delta RSS$ from the update $d$, the predictor $x$, and the current residual $r$, without needing to compute the new residual vector $r'$.\n\n**Algorithm Summary**\nFor each test case:\n1.  Given $x, r, b$ and penalty parameters.\n2.  Compute scalar quantities: $r^\\top x$ and $\\lVert x \\rVert_2^2$.\n3.  Based on the penalty type (Ridge or Lasso), compute the updated coefficient $b_{\\text{new}}$ using the derived formulae. Handle the special case $\\lVert x \\rVert_2^2 = 0$.\n4.  Compute the change in the coefficient: $d = b_{\\text{new}} - b$.\n5.  Compute the change in $RSS$: $\\Delta RSS = d^2 \\lVert x \\rVert_2^2 - 2d(r^\\top x)$.\n6.  Report the value of $\\Delta RSS$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of single-coordinate update problems for Ridge and Lasso\n    regression and computes the change in Residual Sum of Squares (RSS).\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Ridge happy path\n        {'x': [1.2, -0.3, 0.5, 0.0, 2.1],\n         'r': [0.8, -1.1, 0.3, 0.0, 2.0],\n         'b': 0.4,\n         'penalty': 'ridge',\n         'param': 0.7},\n\n        # Case 2: Lasso happy path (nonzero update expected)\n        {'x': [0.5, -1.0, 0.3, 1.2, -0.7, 0.4],\n         'r': [1.0, 0.5, -0.2, 0.3, -1.5, 0.1],\n         'b': -0.1,\n         'penalty': 'lasso',\n         'param': 0.2},\n\n        # Case 3: Lasso thresholding to zero (edge case)\n        {'x': [0.2, -0.1, 0.05, 0.0],\n         'r': [0.01, -0.02, 0.03, -0.04],\n         'b': 0.5,\n         'penalty': 'lasso',\n         'param': 0.2},\n\n        # Case 4: Ridge with zero predictor column (boundary case)\n        {'x': [0.0, 0.0, 0.0, 0.0, 0.0],\n         'r': [1.0, -1.0, 0.5, -0.5, 2.0],\n         'b': 3.0,\n         'penalty': 'ridge',\n         'param': 1.0},\n\n        # Case 5: Lasso with numerically tiny predictor (numerical stability case)\n        {'x': [1e-8, -2e-8, 3e-8],\n         'r': [0.0, 0.0, 0.0],\n         'b': 1.0,\n         'penalty': 'lasso',\n         'param': 0.1}\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x_vec = np.array(case['x'], dtype=np.float64)\n        r_vec = np.array(case['r'], dtype=np.float64)\n        b = case['b']\n        penalty_type = case['penalty']\n        param = case['param']\n        \n        b_new = 0.0\n\n        # Compute scalar quantities needed for the updates\n        x_sq_norm = np.dot(x_vec, x_vec) # Corresponds to ||x||_2^2\n        r_dot_x = np.dot(r_vec, x_vec)   # Corresponds to r^T x\n\n        if penalty_type == 'ridge':\n            # Ridge update: b_new = (r^T x + b ||x||^2) / (||x||^2 + alpha)\n            alpha = param\n            numerator = r_dot_x + b * x_sq_norm\n            denominator = x_sq_norm + alpha\n            # Denominator is guaranteed to be non-zero since alpha = 0 and we\n            # handle the pure ||x||^2 = 0 case implicitly. If ||x||^2 is 0,\n            # numerator is 0, so b_new becomes 0 correctly.\n            if denominator != 0:\n                b_new = numerator / denominator\n            else:\n                 # This case only happens if x_sq_norm=0 and alpha=0,\n                 # which is unpenalized OLS and ill-defined for a zero predictor.\n                 # With alpha0, this will not occur.\n                 b_new = 0.0\n\n        elif penalty_type == 'lasso':\n            # Lasso update: b_new = S(r^T x + b ||x||^2, lambda) / ||x||^2\n            # where S is the soft-thresholding operator.\n            lmbda = param\n            \n            # Handle the case where the predictor is a zero vector.\n            # In this case, the coordinate has no effect on RSS, so the\n            # L1 penalty lambda * |b_new| is minimized at b_new = 0.\n            if x_sq_norm == 0:\n                b_new = 0.0\n            else:\n                z = r_dot_x + b * x_sq_norm\n                if z  lmbda:\n                    b_new = (z - lmbda) / x_sq_norm\n                elif z  -lmbda:\n                    b_new = (z + lmbda) / x_sq_norm\n                else: # |z| = lmbda\n                    b_new = 0.0\n        \n        # Compute the change in coefficient\n        d = b_new - b\n        \n        # Compute the exact change in RSS: Delta_RSS = d^2 ||x||^2 - 2d (r^T x)\n        delta_rss = d**2 * x_sq_norm - 2 * d * r_dot_x\n        \n        results.append(delta_rss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}