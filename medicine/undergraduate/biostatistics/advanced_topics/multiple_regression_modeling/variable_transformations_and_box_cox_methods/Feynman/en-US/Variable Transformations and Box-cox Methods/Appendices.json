{
    "hands_on_practices": [
        {
            "introduction": "Before applying transformations, it's crucial to understand *why* they work. A primary goal is to stabilize variance, helping to meet the assumption of homoscedasticity in many statistical models. This practice provides a theoretical foundation by asking you to use the delta method, a fundamental approximation technique in statistics, to prove how the square-root transformation effectively stabilizes the variance of Poisson-distributed count data . Mastering this derivation offers deep insight into the principled selection of transformations for specific data types.",
            "id": "4965046",
            "problem": "A biostatistician models event counts per subject as a Poisson random variable $Y$ with mean $\\mu > 0$, so that $\\mathbb{E}[Y] = \\mu$ and $\\operatorname{Var}(Y) = \\mu$. To facilitate linear modeling assumptions that require approximately constant variance across levels of $\\mu$, the biostatistician considers the square-root transformation $g(Y) = \\sqrt{Y}$, which is a special case of the Box–Cox family of transformations. Using a first-order Taylor expansion justification (the delta method), derive the approximate variance of $g(Y)$ in terms of $\\mu$ and determine whether this transformation stabilizes the variance, in the sense that the leading-order approximation does not depend on $\\mu$. Provide the approximate variance as a single exact constant. No rounding is required and no units are involved.",
            "solution": "The problem is to derive the approximate variance of the square-root transformation of a Poisson-distributed random variable using a first-order Taylor expansion (the delta method) and to determine if this transformation is variance-stabilizing.\n\nThe problem statement has been validated and is deemed sound. It is a well-posed problem in biostatistics that is scientifically grounded and internally consistent.\n\nLet $Y$ be a random variable representing the event counts. The problem states that $Y$ follows a Poisson distribution with mean $\\mu > 0$. The key properties of the Poisson distribution are its mean and variance:\n$$ \\mathbb{E}[Y] = \\mu $$\n$$ \\operatorname{Var}(Y) = \\mu $$\nThe goal is to analyze the variance of a new random variable, $Z = g(Y)$, where the transformation is given by $g(y) = \\sqrt{y}$.\n\nWe will use the delta method, which is based on a first-order Taylor series expansion of the function $g(Y)$ around the mean of $Y$, which is $\\mu$. The expansion is:\n$$ g(Y) \\approx g(\\mu) + g'(\\mu)(Y - \\mu) $$\nHere, $g'(\\mu)$ is the first derivative of $g(y)$ with respect to $y$, evaluated at $y = \\mu$.\n\nThe variance of this linear approximation of $g(Y)$ is used to approximate the variance of $g(Y)$ itself. Using the properties of variance, where $\\operatorname{Var}(aX + b) = a^2 \\operatorname{Var}(X)$ for constants $a$ and $b$, we have:\n$$ \\operatorname{Var}(g(Y)) \\approx \\operatorname{Var}(g(\\mu) + g'(\\mu)(Y - \\mu)) $$\nSince $g(\\mu)$ and $g'(\\mu)$ are constants with respect to the random variable $Y$, the term $g(\\mu)$ does not contribute to the variance. Thus,\n$$ \\operatorname{Var}(g(Y)) \\approx \\operatorname{Var}(g'(\\mu)(Y - \\mu)) = [g'(\\mu)]^2 \\operatorname{Var}(Y - \\mu) $$\nFurthermore, since $\\operatorname{Var}(Y - c) = \\operatorname{Var}(Y)$ for any constant $c$, we arrive at the general formula for the delta method approximation of variance:\n$$ \\operatorname{Var}(g(Y)) \\approx [g'(\\mu)]^2 \\operatorname{Var}(Y) $$\n\nNow, we apply this formula to the specific problem.\nThe transformation is $g(y) = \\sqrt{y} = y^{1/2}$.\nThe first derivative of $g(y)$ is:\n$$ g'(y) = \\frac{d}{dy}(y^{1/2}) = \\frac{1}{2} y^{-1/2} = \\frac{1}{2\\sqrt{y}} $$\nWe evaluate this derivative at the mean of $Y$, $y = \\mu$:\n$$ g'(\\mu) = \\frac{1}{2\\sqrt{\\mu}} $$\nWe are given that for the Poisson distribution, $\\operatorname{Var}(Y) = \\mu$. Substituting these components into the delta method formula:\n$$ \\operatorname{Var}(\\sqrt{Y}) \\approx [g'(\\mu)]^2 \\operatorname{Var}(Y) = \\left( \\frac{1}{2\\sqrt{\\mu}} \\right)^2 \\cdot \\mu $$\nNow, we simplify the expression:\n$$ \\operatorname{Var}(\\sqrt{Y}) \\approx \\left( \\frac{1}{4\\mu} \\right) \\cdot \\mu $$\n$$ \\operatorname{Var}(\\sqrt{Y}) \\approx \\frac{\\mu}{4\\mu} = \\frac{1}{4} $$\n\nThe result of this first-order approximation is that the variance of the transformed variable $\\sqrt{Y}$ is approximately $\\frac{1}{4}$.\nA transformation is said to be variance-stabilizing if the variance of the transformed variable is approximately constant, i.e., it does not depend on the mean $\\mu$ of the original variable. In this case, the approximate variance is the constant $\\frac{1}{4}$, which is independent of $\\mu$. Therefore, the square-root transformation $g(Y) = \\sqrt{Y}$ is a variance-stabilizing transformation for the Poisson distribution, at least to the first order of approximation provided by the delta method.\n\nThe problem asks for the approximate variance as a single exact constant. This value is $\\frac{1}{4}$.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "From theory, we move to a powerful real-world application: the evaluation of childhood growth. The Lambda-Mu-Sigma (LMS) method, used globally for pediatric growth charts, is a direct application of the Box-Cox transformation to standardize skewed measurements like weight and height. In this exercise, you will step into the role of a biostatistician by using provided LMS parameters ($L$, $M$, and $S$) to calculate a weight-for-age $z$-score, demonstrating how transformations convert raw data into a clinically meaningful and interpretable metric .",
            "id": "5216265",
            "problem": "A pediatric clinician is evaluating a child using the World Health Organization (WHO) Growth Standards based on the Lambda-Mu-Sigma (LMS) method. The LMS method models an anthropometric measurement $Y$ at a given age as approximately normal after a Box-Cox family transformation controlled by three age- and sex-specific parameters: the Box-Cox power $L$, the median $M$, and a generalized coefficient of variation $S$. The standardized value is interpreted as a $z$-score, which represents the number of standard deviations the observation lies from the age- and sex-specific reference. Starting only from the fundamental definition that the Box-Cox transformation of $Y$ about the reference $M$ linearly rescales skewness and dispersion to approximate a standard normal variable, derive the appropriate standardization and compute the weight-for-age $z$-score for a child aged $24$ months with weight $y = 11.8$ kilograms, given the LMS parameters $L = -0.1$, $M = 12.5$, and $S = 0.09$ for this age and sex. Provide your final numerical answer as a pure number with no unit, rounded to four significant figures.",
            "solution": "The problem statement is scientifically grounded, self-contained, and well-posed. The Lambda-Mu-Sigma (LMS) method is a standard and well-documented statistical technique used by the World Health Organization (WHO) for developing growth charts. All necessary parameters and data for the calculation are provided, and there are no internal contradictions or scientifically unsound premises. The problem is therefore deemed valid and a solution will be provided.\n\nThe task is to derive the formula for the standardized $z$-score from the fundamental principles of the LMS method and then use it to compute a specific value. The LMS method posits that a given anthropometric measurement $Y$ at a specific age can be transformed into a variable that follows a standard normal distribution, $Z \\sim N(0, 1)$. This transformation is based on the Box-Cox power transformation and involves three parameters: the median $M$, a generalized coefficient of variation $S$, and a Box-Cox power parameter $L$.\n\nThe derivation proceeds in three steps:\n1.  **Handling Scale and Location**: The measurement $Y$ is first scaled by the age- and sex-specific median $M$. This is achieved by considering the ratio $\\frac{Y}{M}$. This step makes the measurement independent of its absolute scale and centers it around $1$.\n\n2.  **Handling Skewness**: The skewed distribution of $\\frac{Y}{M}$ is normalized using a Box-Cox transformation. For a variable $X$, the Box-Cox transformation with power $\\lambda$ is defined as:\n    $$ T(X; \\lambda) = \\begin{cases} \\frac{X^\\lambda - 1}{\\lambda}  \\text{if } \\lambda \\neq 0 \\\\ \\ln(X)  \\text{if } \\lambda = 0 \\end{cases} $$\n    The case for $\\lambda=0$ is derived from the limit $\\lim_{\\lambda \\to 0} \\frac{X^\\lambda - 1}{\\lambda} = \\ln(X)$. In the LMS method, the power is denoted by $L$. Applying this transformation to the scaled measurement $\\frac{Y}{M}$, we get the transformed variable, which we will call $Y_{T}$:\n    $$ Y_{T} = \\frac{\\left(\\frac{Y}{M}\\right)^L - 1}{L} \\quad (\\text{for } L \\neq 0) $$\n    This transformation is designed to produce a variable $Y_T$ that is approximately normally distributed with a mean of approximately $0$.\n\n3.  **Handling Dispersion (Standardization)**: The problem states that the final standardized value is a $z$-score, which by definition must have a standard deviation of $1$. The transformed variable $Y_T$ has a standard deviation that is given by the parameter $S$, the generalized coefficient of variation. To standardize $Y_T$ to a variable $Z$ with a standard deviation of $1$, we must divide $Y_T$ by its standard deviation, $S$.\n    $$ Z = \\frac{Y_T}{\\text{std.dev}(Y_T)} = \\frac{Y_T}{S} $$\n    Substituting the expression for $Y_T$, we derive the formula for the $z$-score for the case where $L \\neq 0$:\n    $$ Z = \\frac{\\frac{\\left(\\frac{Y}{M}\\right)^L - 1}{L}}{S} = \\frac{\\left(\\frac{Y}{M}\\right)^L - 1}{LS} $$\n    For the special case where $L=0$, the transformation is logarithmic, and the formula becomes:\n    $$ Z = \\frac{\\ln\\left(\\frac{Y}{M}\\right)}{S} $$\nThe derivation is now complete. We can proceed with the calculation using the provided data.\n\nThe given values are:\n-   Actual weight: $y = 11.8$ kilograms\n-   Age- and sex-specific median weight: $M = 12.5$\n-   Age- and sex-specific Box-Cox power: $L = -0.1$\n-   Age- and sex-specific generalized coefficient of variation: $S = 0.09$\n\nSince $L = -0.1 \\neq 0$, we use the first derived formula:\n$$ Z = \\frac{\\left(\\frac{y}{M}\\right)^L - 1}{LS} $$\nSubstitute the given values into the formula:\n$$ Z = \\frac{\\left(\\frac{11.8}{12.5}\\right)^{-0.1} - 1}{(-0.1)(0.09)} $$\nFirst, calculate the ratio $\\frac{y}{M}$:\n$$ \\frac{11.8}{12.5} = 0.944 $$\nNow, substitute this back into the equation for $Z$:\n$$ Z = \\frac{(0.944)^{-0.1} - 1}{-0.009} $$\nNext, we evaluate the term in the numerator. The power term $(0.944)^{-0.1}$ is calculated as:\n$$ (0.944)^{-0.1} = \\exp(-0.1 \\times \\ln(0.944)) $$\n$$ \\ln(0.944) \\approx -0.0576352 $$\n$$ -0.1 \\times \\ln(0.944) \\approx 0.00576352 $$\n$$ \\exp(0.00576352) \\approx 1.0057801 $$\nNow, we can compute the numerator:\n$$ (0.944)^{-0.1} - 1 \\approx 1.0057801 - 1 = 0.0057801 $$\nFinally, we can calculate the $z$-score $Z$:\n$$ Z \\approx \\frac{0.0057801}{-0.009} \\approx -0.6422367 $$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $6$, $4$, $2$, and $2$. The fifth digit is $3$, so we round down.\n$$ Z \\approx -0.6422 $$\nThis is the weight-for-age $z$-score for the child.",
            "answer": "$$\\boxed{-0.6422}$$"
        },
        {
            "introduction": "Our final practice bridges theory and computation by tackling the central question of the Box-Cox method: how do we find the optimal transformation parameter, $\\lambda$, for a given dataset? This problem guides you through deriving the profile log-likelihood function, which provides the statistical criterion for selecting the best $\\lambda$. You will then implement a numerical optimization algorithm to find the value of $\\lambda$ that maximizes this function, thereby making the transformed data as close to normally distributed as possible and equipping you with the complete workflow for applying this powerful technique .",
            "id": "4965153",
            "problem": "A biostatistician is evaluating monotone power transformations for a strictly positive response variable to improve approximate normality. For a sample of $n$ independent and identically distributed (i.i.d.) observations $\\{y_i\\}_{i=1}^n$ with $y_i  0$, consider the Box–Cox family of transformations, defined by\n$$\nz_i(\\lambda)=\n\\begin{cases}\n\\dfrac{y_i^{\\lambda}-1}{\\lambda},  \\lambda \\neq 0, \\\\[6pt]\n\\log(y_i),  \\lambda = 0,\n\\end{cases}\n$$\nand suppose that, after transformation, the working model assumes $z_i(\\lambda)$ are i.i.d. normal with mean $\\mu$ and variance $\\sigma^2$. The inferential target is the value of the transformation parameter $\\lambda$ that maximizes the profile log-likelihood under this model.\n\nYour tasks are as follows:\n\n1) Starting from the normal likelihood for transformed data, the independence assumption, and the Jacobian for the change of variables $y \\mapsto z(\\lambda)$, derive the profile log-likelihood for $\\lambda$ by eliminating the nuisance parameters $\\mu$ and $\\sigma^2$ via their maximum likelihood estimates (MLEs). Express the result as a function of $\\lambda$ and the data $\\{y_i\\}_{i=1}^n$ only, up to an additive constant that does not depend on $\\lambda$.\n\n2) Design an algorithm to maximize the derived function over $\\lambda$ using:\n   - A coarse grid search over a bounded interval to obtain an initial estimate.\n   - A refinement using the Newton–Raphson (NR) method applied to the profile log-likelihood function. For Newton–Raphson, use numerical differentiation with symmetric finite differences for the first and second derivatives at step size $h$, incorporate a backtracking line search to ensure ascent, and specify clear stopping criteria.\n\n3) Implement a complete program that:\n   - Contains no input and defines the following test suite internally. Each test case is a list of strictly positive real numbers representing a sample $\\{y_i\\}$.\n     - Test case A (log-symmetric, expected solution near $\\lambda=0$): $y = \\exp([-2,-1,0,1,2,-2,-1,0,1,2])$.\n     - Test case B (nearly symmetric on the original scale, expected solution near $\\lambda=1$): $y = [8.0,9.0,9.5,10.0,10.5,11.0,12.0,12.5,13.0,14.0]$.\n     - Test case C (highly right-skewed, wide dynamic range): $y = [0.1,0.2,0.3,0.5,1.0,2.0,4.0,8.0,16.0,32.0]$.\n     - Test case D (log-spread values, expected solution near $\\lambda=0$): $y = \\exp([-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0])$.\n   - For each test case, performs:\n     - A grid search over $\\lambda \\in [-2,2]$ with step size $0.01$ to find an initial maximizer.\n     - A Newton–Raphson refinement initialized at the grid maximizer, using symmetric finite differences with step size $h = 10^{-4}$, a maximum of $50$ iterations, a numerical tolerance of $10^{-10}$ on the parameter update, clipping $\\lambda$ to the range $[-5,5]$ after each update, and backtracking line search with step-halving until the profile log-likelihood increases.\n     - A numerically stable evaluation of the Box–Cox transform near $\\lambda = 0$ using its continuous limit.\n   - Rounds each final $\\lambda$ estimate to six digits after the decimal point.\n\n4) Final output format:\n   - Your program should produce a single line of output containing the four final $\\lambda$ estimates as a comma-separated list enclosed in square brackets (e.g., \"[x1,x2,x3,x4]\"), with each value printed as a floating-point number rounded to six digits after the decimal point.\n\nAll mathematical symbols must be interpreted in the following way: $\\lambda$ denotes the Box–Cox power parameter, $\\{y_i\\}_{i=1}^n$ are observations on the original positive scale, and $z_i(\\lambda)$ are the transformed observations. Angles are not used in this problem, and there are no physical units to report. The answers for each test case are floats. The problem is purely mathematical and algorithmic and does not require any domain-specific units or context beyond the definitions given here. Ensure all numerical computations are consistent with the standard assumptions and that the data remain strictly positive throughout.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a complete solution. It requires the derivation of the profile log-likelihood for the Box-Cox transformation parameter $\\lambda$ and its numerical maximization.\n\n### 1. Derivation of the Profile Log-Likelihood\n\nThe problem starts with a set of $n$ independent and identically distributed (i.i.d.) positive observations $\\{y_i\\}_{i=1}^n$. The Box-Cox transformation is applied to each observation:\n$$\nz_i(\\lambda) =\n\\begin{cases}\n\\dfrac{y_i^{\\lambda}-1}{\\lambda},  \\lambda \\neq 0, \\\\\n\\log(y_i),  \\lambda = 0.\n\\end{cases}\n$$\nThe working model assumes that the transformed variables $z_i(\\lambda)$ are i.i.d. following a normal distribution $N(\\mu, \\sigma^2)$.\n\nThe probability density function (PDF) of a single transformed variable $z_i$ is given by:\n$$\nf_{Z}(z_i; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z_i - \\mu)^2}{2\\sigma^2}\\right)\n$$\nTo find the likelihood function in terms of the original data $y_i$, we perform a change of variables. The relationship is $z_i = z_i(y_i, \\lambda)$. The PDF of $Y_i$ is $f_Y(y_i) = f_Z(z_i(\\lambda)) \\cdot |J|$, where $J$ is the Jacobian of the transformation.\n\nThe derivative of $z_i$ with respect to $y_i$ is:\n$$\n\\frac{dz_i}{dy_i} = \\frac{d}{dy_i}\\left(\\frac{y_i^{\\lambda}-1}{\\lambda}\\right) = \\frac{1}{\\lambda} (\\lambda y_i^{\\lambda-1}) = y_i^{\\lambda-1}\n$$\nThis result also holds in the limit as $\\lambda \\to 0$, where $\\frac{dz_i}{dy_i} = \\frac{1}{y_i}$. Since $y_i  0$, the Jacobian is $|J| = \\left|\\frac{dz_i}{dy_i}\\right| = y_i^{\\lambda-1}$.\n\nThe likelihood function for the entire sample $\\{y_i\\}_{i=1}^n$ is the product of the individual densities, due to the i.i.d. assumption:\n$$\nL(\\lambda, \\mu, \\sigma^2 | \\{y_i\\}) = \\prod_{i=1}^n f_Y(y_i) = \\prod_{i=1}^n \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z_i(\\lambda) - \\mu)^2}{2\\sigma^2}\\right) \\cdot y_i^{\\lambda-1} \\right]\n$$\nThe log-likelihood function, $\\ell = \\log L$, is:\n$$\n\\ell(\\lambda, \\mu, \\sigma^2) = \\sum_{i=1}^n \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(z_i(\\lambda) - \\mu)^2}{2\\sigma^2} + (\\lambda-1)\\log(y_i) \\right]\n$$\n$$\n\\ell(\\lambda, \\mu, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (z_i(\\lambda) - \\mu)^2 + (\\lambda-1)\\sum_{i=1}^n \\log(y_i)\n$$\n\nTo obtain the profile log-likelihood for $\\lambda$, we must first find the maximum likelihood estimates (MLEs) of the nuisance parameters $\\mu$ and $\\sigma^2$ for a fixed value of $\\lambda$. We differentiate $\\ell$ with respect to $\\mu$ and $\\sigma^2$ and set the results to zero.\n\nFor $\\mu$:\n$$\n\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{1}{\\sigma^2}\\sum_{i=1}^n (z_i(\\lambda) - \\mu) = 0 \\implies \\sum_{i=1}^n z_i(\\lambda) - n\\mu = 0\n$$\n$$\n\\hat{\\mu}(\\lambda) = \\frac{1}{n}\\sum_{i=1}^n z_i(\\lambda)\n$$\nThis is the sample mean of the transformed data.\n\nFor $\\sigma^2$:\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (z_i(\\lambda) - \\mu)^2 = 0\n$$\nSubstituting $\\hat{\\mu}(\\lambda)$ for $\\mu$:\n$$\n\\hat{\\sigma}^2(\\lambda) = \\frac{1}{n}\\sum_{i=1}^n (z_i(\\lambda) - \\hat{\\mu}(\\lambda))^2\n$$\nThis is the sample variance (with denominator $n$) of the transformed data.\n\nSubstituting these MLEs back into the log-likelihood function gives the profile log-likelihood $\\ell_p(\\lambda)$:\n$$\n\\ell_p(\\lambda) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) - \\frac{1}{2\\hat{\\sigma}^2(\\lambda)}\\sum_{i=1}^n (z_i(\\lambda) - \\hat{\\mu}(\\lambda))^2 + (\\lambda-1)\\sum_{i=1}^n \\log(y_i)\n$$\nUsing the definition of $\\hat{\\sigma}^2(\\lambda)$, the sum of squares term simplifies: $\\sum_{i=1}^n (z_i(\\lambda) - \\hat{\\mu}(\\lambda))^2 = n\\hat{\\sigma}^2(\\lambda)$.\n$$\n\\ell_p(\\lambda) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) - \\frac{n\\hat{\\sigma}^2(\\lambda)}{2\\hat{\\sigma}^2(\\lambda)} + (\\lambda-1)\\sum_{i=1}^n \\log(y_i)\n$$\n$$\n\\ell_p(\\lambda) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2} - \\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) + (\\lambda-1)\\sum_{i=1}^n \\log(y_i)\n$$\nTo maximize $\\ell_p(\\lambda)$, we can drop the terms that are constant with respect to $\\lambda$, which are $-\\frac{n}{2}\\log(2\\pi)$ and $-\\frac{n}{2}$. Let the resulting function to be maximized be $L_{prof}(\\lambda)$:\n$$\nL_{prof}(\\lambda) = -\\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) + (\\lambda-1)\\sum_{i=1}^n \\log(y_i)\n$$\nThis is the desired profile log-likelihood function expressed in terms of $\\lambda$ and the data $\\{y_i\\}$, up to an additive constant.\n\n### 2. Algorithm Design\n\nThe maximization of $L_{prof}(\\lambda)$ is performed in two stages as specified.\n\n**Stage 1: Coarse Grid Search**\nAn initial estimate for $\\lambda$ is found by evaluating $L_{prof}(\\lambda)$ on a discrete grid of points.\n1.  Define a grid of $\\lambda$ values over the interval $[-2, 2]$ with a step size of $0.01$.\n2.  For each $\\lambda_j$ in the grid, compute $L_{prof}(\\lambda_j)$.\n3.  The initial estimate, $\\lambda_{init}$, is the grid point that yields the maximum value of $L_{prof}(\\lambda_j)$.\n\n**Stage 2: Newton-Raphson Refinement**\nStarting from $\\lambda_{init}$, the estimate is refined using the Newton-Raphson method to find the root of the derivative of $L_{prof}(\\lambda)$. The iterative update rule is:\n$$\n\\lambda_{k+1} = \\lambda_k - \\frac{g(\\lambda_k)}{H(\\lambda_k)}\n$$\nwhere $g(\\lambda) = \\frac{d}{d\\lambda}L_{prof}(\\lambda)$ is the gradient (first derivative) and $H(\\lambda) = \\frac{d^2}{d\\lambda^2}L_{prof}(\\lambda)$ is the Hessian (second derivative).\n\n- **Numerical Derivatives**: The derivatives are computed numerically using symmetric finite differences with a step size $h = 10^{-4}$:\n  - Gradient: $g(\\lambda) \\approx \\frac{L_{prof}(\\lambda+h) - L_{prof}(\\lambda-h)}{2h}$\n  - Hessian: $H(\\lambda) \\approx \\frac{L_{prof}(\\lambda+h) - 2L_{prof}(\\lambda) + L_{prof}(\\lambda-h)}{h^2}$\n\n- **Backtracking Line Search**: A pure Newton step is not guaranteed to increase the function value, especially if far from the maximum or if the function is not locally concave. A backtracking line search is employed to ensure ascent.\n  1.  Calculate the Newton direction: $p_k = -g(\\lambda_k)/H(\\lambda_k)$. For maximization, this is an ascent direction only if $H(\\lambda_k)  0$. If $H(\\lambda_k) \\ge 0$, the method is not applicable in its pure form; the iteration should stop, as we are not in a region of local concavity.\n  2.  Initialize a step size factor $\\alpha = 1$.\n  3.  Repeatedly halve $\\alpha$ until the ascent condition $L_{prof}(\\lambda_k + \\alpha p_k)  L_{prof}(\\lambda_k)$ is met.\n  4.  The update is then $\\lambda_{k+1} = \\lambda_k + \\alpha p_k$.\n\n- **Clipping**: After each update, the new estimate $\\lambda_{k+1}$ is clipped to the interval $[-5, 5]$ to prevent divergent steps and maintain a reasonable parameter range.\n\n- **Stopping Criteria**: The iteration terminates when either of the following conditions is met:\n  1.  The absolute change in the parameter is below a tolerance: $|\\lambda_{k+1} - \\lambda_k|  10^{-10}$.\n  2.  A maximum of $50$ iterations is reached.\n\n- **Numerical Stability**: For values of $\\lambda$ close to zero (e.g., $|\\lambda|  10^{-6}$), the computation of $z_i(\\lambda)$ can be numerically unstable. Using the second-order Taylor series expansion of $y_i^\\lambda = \\exp(\\lambda \\log y_i)$ avoids this issue:\n$$\nz_i(\\lambda) = \\frac{\\exp(\\lambda \\log y_i) - 1}{\\lambda} \\approx \\frac{(1 + \\lambda \\log y_i + \\frac{1}{2}(\\lambda \\log y_i)^2) - 1}{\\lambda} = \\log y_i + \\frac{\\lambda}{2}(\\log y_i)^2\n$$\nThis approximation provides a smooth and accurate transition through $\\lambda=0$.",
            "answer": "```python\nimport numpy as np\n\ndef box_cox_transform(y: np.ndarray, lambda_val: float, log_y: np.ndarray) - np.ndarray:\n    \"\"\"\n    Computes the Box-Cox transformation z(lambda) for a given y and lambda.\n    Uses a Taylor expansion for lambda close to 0 for numerical stability.\n\n    Args:\n        y: A numpy array of strictly positive data.\n        lambda_val: The Box-Cox power parameter.\n        log_y: The precomputed natural logarithm of y.\n\n    Returns:\n        A numpy array of the transformed data.\n    \"\"\"\n    if abs(lambda_val)  1e-6:\n        # Taylor series approximation for lambda near 0\n        # (y^lambda - 1)/lambda approx log(y) + (lambda/2)*(log(y))^2\n        return log_y + 0.5 * lambda_val * log_y**2\n    else:\n        return (y**lambda_val - 1) / lambda_val\n\ndef profile_log_likelihood(lambda_val: float, y: np.ndarray, n: int, sum_log_y: float, log_y: np.ndarray) - float:\n    \"\"\"\n    Computes the profile log-likelihood for the Box-Cox parameter lambda.\n\n    Args:\n        lambda_val: The Box-Cox power parameter to evaluate.\n        y: The original data array.\n        n: The number of observations.\n        sum_log_y: The precomputed sum of the natural logarithms of y.\n        log_y: The precomputed natural logarithm of y.\n\n    Returns:\n        The value of the profile log-likelihood (up to a constant).\n    \"\"\"\n    # 1. Transform data\n    z = box_cox_transform(y, lambda_val, log_y)\n    \n    # 2. Compute MLEs for mu and sigma^2 for the given lambda\n    mu_hat = np.mean(z)\n    # Using ddof=0 for MLE variance (denominator n)\n    sigma2_hat = np.var(z, ddof=0)\n\n    # 3. Handle case where variance is zero\n    if sigma2_hat = 0:\n        return -np.inf # Log of non-positive number is undefined or -inf\n\n    # 4. Compute profile log-likelihood\n    # L_prof = -n/2 * log(sigma^2) + (lambda - 1) * sum(log(y_i))\n    log_lik = -0.5 * n * np.log(sigma2_hat) + (lambda_val - 1) * sum_log_y\n    \n    return log_lik\n\ndef solve():\n    \"\"\"\n    Main function to find the optimal lambda for each test case.\n    \"\"\"\n    test_cases = [\n        # Test case A (log-symmetric, expected lambda ~ 0)\n        np.exp([-2, -1, 0, 1, 2, -2, -1, 0, 1, 2]).tolist(),\n        # Test case B (nearly symmetric, expected lambda ~ 1)\n        [8.0, 9.0, 9.5, 10.0, 10.5, 11.0, 12.0, 12.5, 13.0, 14.0],\n        # Test case C (highly right-skewed, expected lambda ~ 0)\n        [0.1, 0.2, 0.3, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0],\n        # Test case D (log-spread, expected lambda ~ 0)\n        np.exp([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]).tolist(),\n    ]\n\n    final_lambdas = []\n    \n    for case_data in test_cases:\n        y = np.array(case_data, dtype=float)\n        n = len(y)\n        log_y = np.log(y)\n        sum_log_y = np.sum(log_y)\n\n        # === Stage 1: Coarse Grid Search ===\n        grid_lambdas = np.arange(-2.0, 2.001, 0.01)\n        \n        # Using a lambda function to pass extra arguments easily\n        likelihood_func = lambda l: profile_log_likelihood(l, y, n, sum_log_y, log_y)\n        \n        grid_likelihoods = [likelihood_func(l) for l in grid_lambdas]\n        \n        lambda_init = grid_lambdas[np.argmax(grid_likelihoods)]\n\n        # === Stage 2: Newton-Raphson Refinement ===\n        lambda_k = lambda_init\n        h = 1e-4  # Step size for numerical differentiation\n        max_iter = 50\n        tolerance = 1e-10\n        clip_range = [-5.0, 5.0]\n\n        for _ in range(max_iter):\n            l_k = likelihood_func(lambda_k)\n            \n            # Numerical derivatives using symmetric finite differences\n            l_plus_h = likelihood_func(lambda_k + h)\n            l_minus_h = likelihood_func(lambda_k - h)\n            \n            grad = (l_plus_h - l_minus_h) / (2 * h)\n            hess = (l_plus_h - 2 * l_k + l_minus_h) / (h**2)\n\n            # Check for non-concavity: hessian must be negative for a maximum.\n            # If not, Newton's method is not guaranteed to find an ascent direction.\n            if hess = 0 or np.isclose(hess, 0):\n                break\n\n            # Newton step\n            step = -grad / hess\n            \n            # Backtracking line search to ensure ascent\n            alpha = 1.0\n            while True:\n                lambda_next_prop = lambda_k + alpha * step\n                if likelihood_func(lambda_next_prop)  l_k:\n                    break\n                alpha /= 2.0\n                if alpha  1e-8:  # Failsafe to prevent infinite loop\n                    alpha = 0.0\n                    break\n            \n            lambda_next = lambda_k + alpha * step\n            \n            # Clipping\n            lambda_next = np.clip(lambda_next, clip_range[0], clip_range[1])\n            \n            # Check for convergence\n            if abs(lambda_next - lambda_k)  tolerance:\n                lambda_k = lambda_next\n                break\n            \n            lambda_k = lambda_next\n\n        # Round to six decimal places for the final result\n        final_lambdas.append(round(lambda_k, 6))\n\n    # Format output as specified\n    print(f\"[{','.join(f'{l:.6f}' for l in final_lambdas)}]\")\n\nsolve()\n```"
        }
    ]
}