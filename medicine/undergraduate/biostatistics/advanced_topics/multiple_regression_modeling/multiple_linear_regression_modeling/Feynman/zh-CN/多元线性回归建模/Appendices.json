{
    "hands_on_practices": [
        {
            "introduction": "在解释模型之前，我们必须首先正确地构建它。本练习将解决回归分析中一个常见的陷阱：“虚拟变量陷阱”，它发生于我们不恰当地编码分类预测变量时。通过理解如何为分类因子及其交互作用构建有效的设计矩阵，你将能确保你的模型是可识别的，其参数可以被唯一地估计。",
            "id": "4930791",
            "problem": "一位生物统计学家正在拟合一个多元线性回归模型，以研究一个临床数据集中的连续结果 $Y$。该模型包含以下预测变量：一个有 $3$ 个水平的分类治疗因子 $A$，一个有 $2$ 个水平的分类性别因子 $B$，以及一个连续协变量 $Z$。研究者希望模型中包含截距项、$A$ 和 $B$ 的主效应、$A \\times B$ 的交互作用以及协变量 $Z$。设计矩阵 $X$ 的构建必须避免虚拟变量陷阱，同时确保参数是可识别的。\n\n从线性模型的基本原理出发，我们知道普通最小二乘 (OLS) 估计量存在且唯一的条件是设计矩阵 $X$ 的列是线性无关的，这等价于 $X$ 具有满列秩。特别地，满列秩等价于 $X^\\top X$ 在 $X$ 的列空间上是正定的。当使用指示（虚拟）变量表示分类预测变量时，将一个因子的所有水平指示变量与截距项一起包含在模型中，通常会引发完全多重共线性，因为截距项列等于该因子各指示变量列之和。交互作用会引入额外的结构性依赖，必须通过适当的约束来解决，以保持可识别性。\n\n选择所有正确描述了通过对 $X$ 施加有效线性约束来防止虚拟变量陷阱的方法，并指定了验证 $\\operatorname{rank}(X)$ 等于预期参数数量的正确方法的选项。\n\nA. 包含截距项、$A$ 的所有 $3$ 个指示列、$B$ 的所有 $2$ 个指示列、$A \\times B$ 的所有 $6$ 个交互作用指示列，以及协变量 $Z$。通过确认 $X^\\top X$ 的对角线元素均为严格正数来验证可识别性。\n\nB. 使用参考单元格编码：包含截距项；对于 $A$，包含对应于两个非参考水平的 $2$ 个指示列；对于 $B$，包含对应于非参考水平的 $1$ 个指示列；对于 $A \\times B$，包含由非参考指示变量的乘积构成的 $(3-1)(2-1)=2$ 个交互作用列；包含 $Z$。通过检查 $X^\\top X$ 的所有特征值 $\\lambda_i$ 均为严格正数来验证满秩，这意味着 $\\operatorname{rank}(X)$ 等于列数。\n\nC. 在保留所有主效应和交互作用指示列的同时，对 $A$ 和 $B$ 的水平系数施加和为一的约束，并通过检查 $\\det(X)\\neq 0$ 来验证满秩。\n\nD. 使用带有和为零约束的效应编码：用 $2$ 个对比列对 $A$ 进行参数化，其编码值在 $3$ 个水平上总和为 $0$；用 $1$ 个对比列对 $B$ 进行参数化，其编码值在 $2$ 个水平上总和为 $0$；并以逐元素乘积的方式形成 $(3-1)(2-1)=2$ 个交互作用对比列，使得交互作用参数满足对于每个 $k$，$\\sum_{j=1}^{2}\\gamma_{kj}=0$ 以及对于每个 $j$，$\\sum_{k=1}^{3}\\gamma_{kj}=0$；包含截距项和 $Z$。通过计算奇异值分解 (SVD) $X=U\\Sigma V^\\top$ 并确认 $\\Sigma$ 中最小的奇异值为严格正数来验证满秩。",
            "solution": "从线性模型设置开始。多元线性回归模型可以写成\n$$\nY = X\\beta + \\varepsilon,\n$$\n其中 $Y$ 是 $n \\times 1$ 的结果向量，$X$ 是 $n \\times p$ 的设计矩阵，$\\beta$ 是 $p \\times 1$ 的参数向量，$\\varepsilon$ 是 $n \\times 1$ 的误差向量。普通最小二乘 (OLS) 估计量是\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y,\n$$\n其存在且唯一的充要条件是 $X$ 具有满列秩 $p$，等价地，$X^\\top X$ 是正定的。正定性意味着对于任何非零向量 $v \\in \\mathbb{R}^p$，都有\n$$\nv^\\top (X^\\top X) v > 0,\n$$\n这也意味着 $X^\\top X$ 的所有特征值 $\\lambda_i$ 都是严格正数。\n\n当分类预测变量用指示变量编码时，虚拟变量陷阱源于完全多重共线性。对于一个有 $a$ 个水平的因子 $A$，其 $a$ 个指示列对于每个观测值都满足：这 $a$ 个水平指示变量的和等于元素全为1的截距项列。因此，在有截距项的情况下，包含所有 $a$ 个指示变量会使这些列线性相关：\n$$\n\\mathbf{1} = \\sum_{k=1}^{a} \\mathbb{I}\\{A = k\\}.\n$$\n类似地，对于一个有 $b$ 个水平的因子 $B$，包含所有 $b$ 个指示变量和截距项会产生另一个线性依赖关系。$A$ 和 $B$ 之间的交互作用会产生 $ab$ 个指示变量乘积列；在主效应不受约束的情况下，除非施加约束以消除冗余，否则许多这些交互作用列是主效应指示变量和截距项的线性组合。为避免这些依赖关系，可以施加线性约束，将截距项、主效应和交互作用的参数化自由度减少到 $1 + (a-1) + (b-1) + (a-1)(b-1)$，再加上任何连续协变量（如 $Z$）的自由度。\n\n两种标准方法可以施加这些约束：\n\n- 参考单元格编码：为每个因子移除一个指示变量，留下 $a-1$ 和 $b-1$ 个独立的主效应列，以及由非参考指示变量的乘积构成的 $(a-1)(b-1)$ 个交互作用列。这消除了等于截距项的线性组合，并确保交互作用项与主效应不冗余。\n\n- 效应编码：使用在各水平上满足和为零约束的对比列，确保编码后的列与截距项线性无关。在效应编码下，交互作用参数服从分层和为零约束，例如对于每个 $k$ 有 $\\sum_{j=1}^{b}\\gamma_{kj}=0$ 以及对于每个 $j$ 有 $\\sum_{k=1}^{a}\\gamma_{kj}=0$。这确保了交互作用空间具有 $(a-1)(b-1)$ 个自由度，并且不与主效应或截距项共线。\n\n满秩的验证应使用有效的线性代数标准：要么确认 $X^\\top X$ 是正定的（所有特征值均为严格正数），要么等价地，确认 $X$ 的最小奇异值是严格正数（来自奇异值分解 (SVD) $X=U\\Sigma V^\\top$）。\n\n逐个选项分析：\n\nA. 此选项建议包含截距项、$A$ 的所有 $3$ 个指示列、$B$ 的所有 $2$ 个指示列、$A \\times B$ 交互作用的所有 $6$ 个指示列，以及 $Z$。这种构造保证了完全多重共线性，因为截距项列等于 $A$ 的指示列之和，也等于 $B$ 的指示列之和，并且交互作用指示列是主效应指示列的线性组合。建议的验证方法——检查 $X^\\top X$ 的对角线元素为正——不足以确定满秩。对角线元素为正并不能排除非对角线上的依赖关系，$X^\\top X$ 即使对角线元素为正也可能是奇异的。结论：错误。\n\nB. 此选项指定了参考单元格编码：包含截距项；$A$ 的 $2$ 个虚拟列（因为 $3-1=2$），$B$ 的 $1$ 个虚拟列（因为 $2-1=1$）；由非参考指示变量的乘积构成的 $(3-1)(2-1)=2$ 个交互作用列；以及包含 $Z$。这移除了冗余的指示列，并将交互作用空间限制在 $(a-1)(b-1)$ 个自由度，防止了过度参数化。验证方法——检查 $X^\\top X$ 的所有特征值 $\\lambda_i$ 均为严格正数——是判断正定性（并因此判断满列秩）的正确标准。结论：正确。\n\nC. 此选项建议在保留所有指示列的同时，对水平系数施加和为一的约束，并通过 $\\det(X)\\neq 0$ 来验证满秩。和为一约束不是带截距项的线性模型中的标准可识别性约束；它们不能消除截距项与全套指示变量之间的依赖关系，并且，如果在系数层面实施而没有适当的对比编码，可能仍会使 $X$ 秩亏。此外，$\\det(X)$ 不是一个有效的秩判据，因为 $X$ 通常是一个 $n \\times p$ 的矩形矩阵，其中 $n \\ge p$；除非 $n=p$（这并非通用的回归设定），否则其行列式未定义。秩的验证应该在 $X^\\top X$ 上进行，或者通过 $X$ 的奇异值进行。结论：错误。\n\nD. 此选项使用带有和为零约束的效应编码，为 $A$ 生成 $2$ 个对比列，为 $B$ 生成 $1$ 个对比列，并构建 $(3-1)(2-1)=2$ 个交互作用对比列，其中交互作用参数满足分层和为零约束，所有这些都与截距项和 $Z$ 一起包含在内。这种编码确保了这些列与截距项线性无关，并且交互作用空间具有正确的 $(a-1)(b-1)$ 维数，从而避免了虚拟变量陷阱。通过奇异值分解 (SVD) $X=U\\Sigma V^\\top$ 并检查最小奇异值是否为严格正数来进行验证，是检验满列秩的正确方法。结论：正确。\n\n因此，正确的选项是 B 和 D。",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "向模型中添加更多的预测变量几乎总会提高标准的 $R^2$ 值，即使新变量毫无用处。本练习将探讨一个更可靠的模型性能度量——调整后的 $R^2$——它会对模型的复杂性进行惩罚。从第一性原理出发推导这个公式，将巩固你对自由度以及模型拟合度与简约性之间权衡的理解。",
            "id": "4930824",
            "problem": "一位生物统计学家正在建立一个用于空腹血糖（由响应变量 $Y$ 表示）的多元线性回归模型，使用的预测变量为年龄 $X_{1}$、身体质量指数 $X_{2}$ 和睡眠时长 $X_{3}$。设线性模型为 $Y = X\\beta + \\varepsilon$，其中 $X$ 包含一个截距列和 $p$ 个预测变量列（因此回归系数总数为 $p+1$），样本量为 $n$，$\\varepsilon$ 是一个均值为 $0$ 且方差恒定的随机误差项。决定系数 $R^2$ 由总平方和 (TSS) 和残差平方和 (RSS) 定义为 $R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$，其中 $\\text{TSS} = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}$ 且 $\\text{RSS} = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$。从 $\\text{TSS}$ 和 $\\text{RSS}$ 的基本定义出发，并根据方差的无偏估计量是通过将平方和除以其相应自由度得到的原理，推导出调整后决定系数关于 $R^2$、$n$ 和 $p$（其中 $p$ 为不包括截距的预测变量个数）的闭式表达式。然后，使用你推导出的表达式，用文字解释为什么当增加一个不能显著改善模型拟合度的预测变量时，调整后的决定系数可能会下降，并说明自由度的作用。你的最终答案必须是调整后决定系数的单一解析表达式。",
            "solution": "问题陈述是有效的。这是一个适定且有科学依据的生物统计学问题，要求从第一性原理出发，推导一个标准的统计公式——调整后决定系数($R^2_{\\text{adj}}$)，并解释其性质。所有给出的定义和条件都是标准且一致的。\n\n目标是推导调整后决定系数 $R^2_{\\text{adj}}$ 的表达式，并解释其行为。推导始于这样一个原理：调整后的度量指标应使用方差的无偏估计量，而无偏估计量是通过将平方和除以其相应的自由度得到的。\n\n决定系数 $R^2$ 定义为 $R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$，其中 $\\text{TSS}$ 是总平方和，$\\text{RSS}$ 是残差平方和。这个指标衡量了响应变量中可由预测变量预测的方差比例。然而，每当模型中增加一个新的预测变量时，无论该预测变量的实际解释能力如何，$R^2$ 都会机械地增加或保持不变。这是因为最小二乘法优化总是会减少或保持 $\\text{RSS}$ 不变，即使只是偶然。调整后的 $R^2$ 通过惩罚包含非信息性预测变量来纠正这一点。\n\n我们从响应变量总方差 $\\sigma^2_Y$ 和模型随机误差项方差 $\\sigma^2_{\\varepsilon}$ 的无偏估计量开始。\n\n总平方和 $\\text{TSS} = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}$ 具有 $n-1$ 个自由度，因为在估计样本均值 $\\bar{y}$ 时损失了一个自由度。因此，$Y$ 的总方差的一个无偏估计量是总均方 $\\text{MST}$：\n$$ \\text{MST} = \\frac{\\text{TSS}}{n-1} $$\n\n残差平方和 $\\text{RSS} = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$ 具有 $n-(p+1)$ 个自由度。这是因为在多元线性回归模型中估计了 $p+1$ 个参数：$p$ 个预测变量的系数和一个截距项。误差方差 $\\sigma^2_{\\varepsilon}$ 的一个无偏估计量是均方误差 $\\text{MSE}$：\n$$ \\text{MSE} = \\frac{\\text{RSS}}{n-p-1} $$\n\n调整后决定系数 $R^2_{\\text{adj}}$ 的定义与 $R^2$ 类似，但它使用的是这些无偏方差估计量的比率，而不是平方和的比率：\n$$ R^2_{\\text{adj}} = 1 - \\frac{\\text{MSE}}{\\text{MST}} $$\n代入 $\\text{MSE}$ 和 $\\text{MST}$ 的表达式，我们得到：\n$$ R^2_{\\text{adj}} = 1 - \\frac{\\frac{\\text{RSS}}{(n-p-1)}}{\\frac{\\text{TSS}}{(n-1)}} $$\n这个表达式可以重排为：\n$$ R^2_{\\text{adj}} = 1 - \\left(\\frac{\\text{RSS}}{\\text{TSS}}\\right) \\left(\\frac{n-1}{n-p-1}\\right) $$\n根据标准决定系数的定义 $R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$，我们可以用 $R^2$ 来表示比率 $\\frac{\\text{RSS}}{\\text{TSS}}$：\n$$ \\frac{\\text{RSS}}{\\text{TSS}} = 1 - R^2 $$\n将此代入我们关于 $R^2_{\\text{adj}}$ 的方程，得到关于 $R^2$、$n$ 和 $p$ 的最终表达式：\n$$ R^2_{\\text{adj}} = 1 - (1 - R^2)\\left(\\frac{n-1}{n-p-1}\\right) $$\n\n现在，我们用这个表达式来解释为什么当增加一个不能显著改善模型拟合度的预测变量时，$R^2_{\\text{adj}}$ 会下降。当模型中增加一个新的预测变量时，预测变量的数量 $p$ 增加 1。这对 $R^2_{\\text{adj}}$ 的公式有两个相互竞争的影响：\n\n1.  对 $R^2$ 的影响：增加任何预测变量（无论有用与否）都会导致标准 $R^2$ 增加，或者在极少数情况下保持不变。这意味着 $(1 - R^2)$ 项会减少或保持不变。表达式的这一部分会推动 $R^2_{\\text{adj}}$ 增加。\n\n2.  对惩罚因子的影响：项 $\\left(\\frac{n-1}{n-p-1}\\right)$ 是对模型复杂度的惩罚。当 $p$ 增加到 $p+1$ 时，分母 $(n-p-1)$ 减小，导致整个惩罚因子增加。对于 $p \\ge 0$，该因子总是大于或等于 1。表达式的这一部分会推动 $R^2_{\\text{adj}}$ 减少。\n\n$R^2_{\\text{adj}}$ 的总体变化取决于这两种效应之间的平衡。如果新的预测变量“不能显著改善模型拟合度”，这意味着由此导致的 $R^2$ 增加非常小。相应的 $(1 - R^2)$ 项的减少也很轻微。然而，随着新预测变量的加入，惩罚因子 $\\left(\\frac{n-1}{n-p-1}\\right)$ 会严格增加。如果 $(1 - R^2)$ 的微小减少不足以抵消惩罚因子的增加，它们的乘积 $(1 - R^2)\\left(\\frac{n-1}{n-p-1}\\right)$ 将会增加。由于这个乘积是从 1 中减去的，其值的增加将导致 $R^2_{\\text{adj}}$ 下降。\n\n本质上，只有当拟合度的改善（即 $R^2$ 的增加）足够大，能够证明为新的预测变量花费一个额外自由度的“成本”是合理的时，调整后的 $R^2$ 才会增加。这种机制确保了 $R^2_{\\text{adj}}$ 是一个更诚实的模型质量度量，因为它在奖励拟合优度的同时惩罚了模型的复杂度。",
            "answer": "$$\n\\boxed{1 - (1 - R^2) \\frac{n-1}{n-p-1}}\n$$"
        },
        {
            "introduction": "当模型中加入另一个变量时，回归系数可能会发生巨大变化——甚至改变符号。这个令人惊讶的现象被称为辛普森悖论，是混杂效应的一种表现。本练习将挑战你探索系数在何种数学条件下是稳定的（即可折叠的），以及在何种条件下不稳定，为谨慎解释观测数据提供了至关重要的一课。",
            "id": "4930822",
            "problem": "一位生物医学研究者将一个连续结果 $Y$（例如，血浆生物标志物浓度）建模为一个暴露 $X$（例如，营养摄入量）和一个协变量 $Z$（例如，年龄）的线性函数，并带有加性噪声。数据生成机制指定为\n$$\nY = \\alpha + \\beta_X X + \\beta_Z Z + \\varepsilon,\n$$\n其中 $\\varepsilon$ 满足 $\\mathrm{E}[\\varepsilon \\mid X,Z] = 0$，且 $\\varepsilon$ 与 $(X,Z)$ 独立。系数 $\\beta_X$ 代表了当 $X$ 和 $Z$ 都被纳入模型时 $X$ 的多元回归系数。在整个问题中，假设 $\\mathrm{E}[X]=0$ 和 $\\mathrm{E}[Z]=0$。\n\n考虑两种科学上现实的研究设计：\n\n研究A（随机化暴露）：$X$ 是随机化的且独立于 $Z$，因此 $X \\perp Z$。$(X,Z)$ 的联合分布满足 $\\operatorname{Var}(X)>0$ 和 $\\operatorname{Var}(Z)>0$。\n\n研究B（具有混杂结构的观察性研究）：$X$ 和 $Z$ 相关，且 $\\operatorname{Cov}(X,Z) \\neq 0$。为具体起见，假设 $\\operatorname{Var}(X) = 1$，$\\operatorname{Var}(Z) = 1$，$\\operatorname{Cov}(X,Z) = -0.6$，并且 $\\beta_X = 0.5$ 和 $\\beta_Z = 2.0$。\n\n$X$ 的系数关于 $Z$ 的可折叠性意味着，仅通过 $Y$ 对 $X$ 回归得到的 $X$ 的系数（即简单回归系数）等于 $Y$ 对 $(X,Z)$ 回归时得到的多元回归系数 $\\beta_X$。\n\n下列哪个陈述是正确的？选择所有适用项。\n\nA. 在研究A中，根据所述模型，仅通过 $Y$ 对 $X$ 回归得到的 $X$ 的简单回归系数等于通过 $Y$ 对 $(X,Z)$ 回归得到的 $X$ 的多元回归系数。\n\nB. 在研究A中，即使 $X$ 独立于 $Z$， $X$ 的简单回归系数通常也会与其多元回归系数不同，除非 $\\beta_Z = 0$。\n\nC. 在研究B中，只要 $\\operatorname{Cov}(X,Z)  0$ 且 $\\beta_Z > 0$ 的量级足够大，即使多元回归系数 $\\beta_X$ 为正， $X$ 的简单回归系数也可能为负。\n\nD. 辛普森悖论指的是当 $X$ 和 $Z$ 相关且 $Z$ 影响 $Y$ 时，在对 $Z$ 进行调整后，$X$ 和 $Y$ 之间的关联方向可能发生逆转的现象。\n\nE. 如果 $\\operatorname{Cov}(X,Z) = 0$，那么 $X$ 的简单回归系数等于其多元回归系数 $\\beta_X$，而与 $\\beta_Z$ 的量级无关。",
            "solution": "用户希望我验证并解决一个关于简单线性回归系数和多元线性回归系数之间关系的问题，该问题涉及对预测变量协方差结构的不同假设。\n\n### 问题验证\n\n**第一步：提取已知条件**\n\n问题提供了以下信息：\n-   **数据生成模型**：$Y \\;=\\; \\alpha \\;+\\; \\beta_X X \\;+\\; \\beta_Z Z \\;+\\; \\varepsilon$。\n-   **结果**：$Y$ 是一个连续变量。\n-   **预测变量**：$X$（暴露）和 $Z$（协变量）是变量。\n-   **误差项**：$\\varepsilon$ 是一个随机误差，满足 $\\mathrm{E}[\\varepsilon \\mid X,Z] = 0$ 且 $\\varepsilon$ 与 $(X,Z)$ 独立。\n-   **目标系数**：$\\beta_X$ 是 $X$ 的多元回归系数。\n-   **假设**：$\\mathrm{E}[X]=0$ 和 $\\mathrm{E}[Z]=0$。\n-   **研究A（随机化暴露）**：$X$ 独立于 $Z$（$X \\perp Z$）。并且 $\\operatorname{Var}(X)0$ 和 $\\operatorname{Var}(Z)0$。\n-   **研究B（具有混杂的观察性研究）**：$X$ 和 $Z$ 相关。具体来说，$\\operatorname{Var}(X) = 1$，$\\operatorname{Var}(Z) = 1$，$\\operatorname{Cov}(X,Z) = -0.6$，$\\beta_X = 0.5$，以及 $\\beta_Z = 2.0$。\n-   **可折叠性定义**：$X$ 的简单回归系数（来自 $Y$ 对 $X$ 的单独回归）等于多元回归系数 $\\beta_X$。\n\n**第二步：使用提取的已知条件进行验证**\n\n-   **科学依据**：该问题基于多元线性回归的基本原理，这是生物统计学和许多其他科学领域的核心课题。混杂、可折叠性、随机化和观察性研究等概念是统计建模和因果推断的核心。指定的模型是标准的线性模型。所有方面在科学上都是合理的。\n-   **定义明确**：问题陈述清晰。它要求在明确定义的条件下评估 $X$ 的简单回归系数和多元回归系数之间的关系。所提供的信息足以推导出必要的公式并评估每个陈述。存在一个唯一的、稳定的、有意义的解。\n-   **客观性**：术语精确且在统计学中是标准的（例如，“多元回归系数”、“协方差”、“独立”）。问题没有主观或含糊的语言。研究B中的数值是一致的；$(X, Z)$ 的指定协方差矩阵为 $\\begin{pmatrix} 1  -0.6 \\\\ -0.6  1 \\end{pmatrix}$，该矩阵是正定的，因为其行列式为 $1 - (-0.6)^2 = 1 - 0.36 = 0.64  0$。\n\n**第三步：结论和行动**\n\n问题陈述是有效的。它科学合理、定义明確、客观，并包含足够的信息以进行严谨的求解。我将继续推导解决方案。\n\n### 解决方案的推导\n\n问题的核心是比较模型 $Y = \\alpha + \\beta_X X + \\beta_Z Z + \\varepsilon$ 中的多元回归系数 $\\beta_X$与省略 $Z$ 的模型中得到的简单回归系数。\n\n设简单线性回归模型为 $Y = \\alpha' + \\beta_{X, \\text{simple}} X + \\eta$。\n总体简单回归系数 $\\beta_{X, \\text{simple}}$ 由以下公式给出：\n$$\n\\beta_{X, \\text{simple}} = \\frac{\\operatorname{Cov}(X,Y)}{\\operatorname{Var}(X)}\n$$\n我们必须使用真实的数据生成模型 $Y = \\alpha + \\beta_X X + \\beta_Z Z + \\varepsilon$ 来计算 $\\operatorname{Cov}(X,Y)$。\n\n利用协方差的性质：\n$$\n\\operatorname{Cov}(X,Y) = \\operatorname{Cov}(X, \\alpha + \\beta_X X + \\beta_Z Z + \\varepsilon)\n$$\n$$\n\\operatorname{Cov}(X,Y) = \\operatorname{Cov}(X, \\alpha) + \\operatorname{Cov}(X, \\beta_X X) + \\operatorname{Cov}(X, \\beta_Z Z) + \\operatorname{Cov}(X, \\varepsilon)\n$$\n因为 $\\alpha$ 和 $\\beta_X, \\beta_Z$ 是常数：\n$$\n\\operatorname{Cov}(X,Y) = 0 + \\beta_X \\operatorname{Cov}(X,X) + \\beta_Z \\operatorname{Cov}(X,Z) + \\operatorname{Cov}(X, \\varepsilon)\n$$\n$$\n\\operatorname{Cov}(X,Y) = \\beta_X \\operatorname{Var}(X) + \\beta_Z \\operatorname{Cov}(X,Z) + \\operatorname{Cov}(X, \\varepsilon)\n$$\n问题陈述 $\\varepsilon$ 独立于 $(X,Z)$，这意味着 $\\operatorname{Cov}(X, \\varepsilon) = 0$。因此：\n$$\n\\operatorname{Cov}(X,Y) = \\beta_X \\operatorname{Var}(X) + \\beta_Z \\operatorname{Cov}(X,Z)\n$$\n将此代入 $\\beta_{X, \\text{simple}}$ 的公式中：\n$$\n\\beta_{X, \\text{simple}} = \\frac{\\beta_X \\operatorname{Var}(X) + \\beta_Z \\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}\n$$\n$$\n\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}\n$$\n这个基本方程将简单回归系数与多元回归系数联系起来。$\\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}$ 这一项被称为遗漏变量偏误。\n当这个偏误项为零时，即 $\\beta_{X, \\text{simple}} = \\beta_X$，就出现可折叠性。这种情况发生当且仅当：\n1.  $\\beta_Z = 0$（在多元回归模型中，被省略的变量 $Z$ 对结果 $Y$ 没有影响）。\n2.  $\\operatorname{Cov}(X,Z) = 0$（暴露 $X$ 与被省略的变量 $Z$ 不相关）。\n\n现在我们来评估每个选项。\n\n### 逐项分析\n\n**A. 在研究A中，根据所述模型，仅通过 $Y$ 对 $X$ 回归得到的 $X$ 的简单回归系数等于通过 $Y$ 对 $(X,Z)$ 回归得到的 $X$ 的多元回归系数。**\n\n在研究A中，给定 $X$ 独立于 $Z$（$X \\perp Z$）。独立性的一个结果是协方差为零，即 $\\operatorname{Cov}(X,Z) = 0$。将此代入我们推导的公式：\n$$\n\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{0}{\\operatorname{Var}(X)} = \\beta_X\n$$\n由于 $\\beta_{X, \\text{simple}}$ 等于 $\\beta_X$，该陈述是正确的。这是随机化的关键优势：它通过打破暴露与其他协变量之间的关联来消除混杂。\n**结论：正确。**\n\n**B. 在研究A中，即使 $X$ 独立于 $Z$， $X$ 的简单回归系数通常也会与其多元回归系数不同，除非 $\\beta_Z = 0$。**\n\n这个陈述与对选项A的分析相矛盾。如上所示，在研究A中，$X \\perp Z$ 的条件意味着 $\\operatorname{Cov}(X,Z) = 0$。这使得偏误项 $\\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}$ 对于*任何* $\\beta_Z$ 的值都等于零。简单回归系数和多元回归系数将相等。声称除非 $\\beta_Z = 0$ 否则它们通常会不同的说法是错误的。\n**结论：不正确。**\n\n**C. 在研究B中，只要 $\\operatorname{Cov}(X,Z)  0$ 且 $\\beta_Z  0$ 的量级足够大，即使多元回归系数 $\\beta_X$ 为正， $X$ 的简单回归系数也可能为负。**\n\n让我们使用公式 $\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}$。\n问题为研究B提供了具体数值：$\\beta_X = 0.5$，$\\beta_Z = 2.0$，$\\operatorname{Cov}(X,Z) = -0.6$，以及 $\\operatorname{Var}(X) = 1$。让我们计算 $\\beta_{X, \\text{simple}}$：\n$$\n\\beta_{X, \\text{simple}} = 0.5 + (2.0) \\frac{-0.6}{1} = 0.5 - 1.2 = -0.7\n$$\n在这种情况下，多元回归系数 $\\beta_X$ 是正的（$0.5$），但简单回归系数 $\\beta_{X, \\text{simple}}$ 是负的（$-0.7$）。这是因为选项中指定的条件（$\\operatorname{Cov}(X,Z)  0$ 和 $\\beta_Z  0$）产生了一个负的偏误项。如果这个偏误的绝对值（$|-1.2|=1.2$）大于真实系数 $\\beta_X$（$0.5$），符号就会反转。这证实了这种可能性。\n**结论：正确。**\n\n**D. 辛普森悖论指的是当 $X$ 和 $Z$ 相关且 $Z$ 影响 $Y$ 时，在对 $Z$ 进行调整后，$X$ 和 $Y$ 之间的关联方向可能发生逆转的现象。**\n\n这是一个定性陈述，描述了线性回归背景下的辛普森悖论现象。“关联的方向”对应于回归系数的符号。未经调整的关联由 $\\beta_{X, \\text{simple}}$ 衡量，而调整后的关联由 $\\beta_X$ 衡量。当 $\\mathrm{sign}(\\beta_{X, \\text{simple}}) \\neq \\mathrm{sign}(\\beta_X)$ 时，悖论就发生了。正如对选项C的分析所示，当模型中省略了一个变量 $Z$ 时，如果 $Z$ 与 $X$ 相关（$\\operatorname{Cov}(X,Z) \\neq 0$）且 $Z$ 是 $Y$ 的一个预测变量（$\\beta_Z \\neq 0$），这种情况就可能发生。这个陈述正确地描述了这些条件和由此产生的现象。\n**结论：正确。**\n\n**E. 如果 $\\operatorname{Cov}(X,Z) = 0$，那么 $X$ 的简单回归系数等于其多元回归系数 $\\beta_X$，而与 $\\beta_Z$ 的量级无关。**\n\n这个陈述提出了可折叠性的两个一般条件之一。使用我们推导的公式：\n$$\n\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}\n$$\n如果我们设置 $\\operatorname{Cov}(X,Z) = 0$，偏误项变为 $\\beta_Z \\frac{0}{\\operatorname{Var}(X)} = 0$。这对任何有限的 $\\beta_Z$ 值都成立（并且前提是 $\\operatorname{Var}(X) \\neq 0$，这本身就是估计系数的必要条件）。因此，在 $\\operatorname{Cov}(X,Z) = 0$ 的条件下，$\\beta_{X, \\text{simple}} = \\beta_X$ 总是成立的。\n**结论：正确。**",
            "answer": "$$\\boxed{ACDE}$$"
        }
    ]
}