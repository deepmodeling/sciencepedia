{
    "hands_on_practices": [
        {
            "introduction": "Building a robust multiple linear regression model begins with correctly representing your variables. When dealing with categorical predictors like treatment groups or sex, a common pitfall is the \"dummy variable trap,\" which introduces perfect multicollinearity and makes the model impossible to solve. This practice  challenges you to think like a data scientist, exploring valid coding strategies to ensure your model's parameters are identifiable and laying the linear algebra foundation for a sound analysis.",
            "id": "4930791",
            "problem": "A biostatistician is fitting a multiple linear regression model to study a continuous outcome $Y$ in a clinical dataset with the following predictors: a categorical treatment factor $A$ with $3$ levels, a categorical sex factor $B$ with $2$ levels, and a continuous covariate $Z$. The investigator wants to include an intercept, the main effects of $A$ and $B$, the interaction $A \\times B$, and the covariate $Z$. The design matrix $X$ must be constructed to avoid the dummy variable trap while ensuring the parameters are identifiable.\n\nStarting from first principles in linear models, recall that the Ordinary Least Squares (OLS) estimator exists and is unique only if the columns of the design matrix $X$ are linearly independent, which is equivalent to $X^\\top X$ being positive definite on the column space of $X$. When categorical predictors are represented with indicator (dummy) variables, including all level indicators for a factor together with an intercept generally induces perfect multicollinearity because the intercept column equals the sum of the factor’s indicator columns. Interactions introduce additional structured dependencies that must be resolved by appropriate constraints to preserve identifiability.\n\nSelect all options that correctly describe approaches that prevent the dummy variable trap by enforcing valid linear constraints on $X$ and that specify a correct method to verify that $\\operatorname{rank}(X)$ equals the intended number of parameters.\n\nA. Include the intercept, all $3$ indicator columns for $A$, all $2$ indicator columns for $B$, all $6$ interaction indicator columns for $A \\times B$, and the covariate $Z$. Verify identifiability by confirming that the diagonal entries of $X^\\top X$ are strictly positive.\n\nB. Use reference cell coding: include the intercept; for $A$, include $2$ indicator columns corresponding to two non-reference levels; for $B$, include $1$ indicator column for the non-reference level; for $A \\times B$, include $(3-1)(2-1)=2$ interaction columns built as products of the non-reference indicators; include $Z$. Verify full rank by checking that all eigenvalues $\\lambda_i$ of $X^\\top X$ are strictly positive, which implies $\\operatorname{rank}(X)$ equals the number of columns.\n\nC. Impose sum-to-one constraints on the level coefficients for $A$ and $B$ while keeping all main-effect and interaction indicator columns, and verify full rank by checking that $\\det(X)\\neq 0$.\n\nD. Use effect coding with sum-to-zero constraints: parameterize $A$ with $2$ contrast columns whose coded values sum to $0$ across the $3$ levels, parameterize $B$ with $1$ contrast column whose coded values sum to $0$ across the $2$ levels, and form $(3-1)(2-1)=2$ interaction contrast columns as elementwise products such that interaction parameters satisfy $\\sum_{j=1}^{2}\\gamma_{kj}=0$ for each $k$ and $\\sum_{k=1}^{3}\\gamma_{kj}=0$ for each $j$; include the intercept and $Z$. Verify full rank by computing the Singular Value Decomposition (SVD) $X=U\\Sigma V^\\top$ and confirming that the smallest singular value in $\\Sigma$ is strictly positive.",
            "solution": "Begin with the linear model setup. The multiple linear regression model can be written as\n$$\nY = X\\beta + \\varepsilon,\n$$\nwhere $Y$ is the $n \\times 1$ outcome vector, $X$ is the $n \\times p$ design matrix, $\\beta$ is the $p \\times 1$ parameter vector, and $\\varepsilon$ is the $n \\times 1$ error vector. The Ordinary Least Squares (OLS) estimator is\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y,\n$$\nwhich exists and is unique if and only if $X$ has full column rank $p$, equivalently $X^\\top X$ is positive definite. Positive definiteness means that for any nonzero vector $v \\in \\mathbb{R}^p$,\n$$\nv^\\top (X^\\top X) v > 0,\n$$\nand implies that all eigenvalues $\\lambda_i$ of $X^\\top X$ are strictly positive.\n\nWhen categorical predictors are coded with indicator variables, the dummy variable trap arises from perfect multicollinearity. For a factor $A$ with $a$ levels, the $a$ indicator columns satisfy, for each observation, that the sum across the $a$ level indicators equals the intercept column of ones. Thus, with an intercept, including all $a$ indicators renders the columns linearly dependent:\n$$\n\\mathbf{1} = \\sum_{k=1}^{a} \\mathbb{I}\\{A = k\\}.\n$$\nSimilarly, for a factor $B$ with $b$ levels, including all $b$ indicators with an intercept yields another linear dependence. Interactions between $A$ and $B$ generate $ab$ indicator product columns; with unconstrained main effects, many of these interaction columns are linear combinations of the main-effect indicators and the intercept unless constraints are imposed to remove redundancies. To avoid these dependencies, one enforces linear constraints that reduce the parameterization to $1 + (a-1) + (b-1) + (a-1)(b-1)$ degrees of freedom for the intercept, main effects, and interaction, plus any continuous covariates such as $Z$.\n\nTwo standard approaches enforce these constraints:\n\n- Reference cell coding removes one indicator per factor, leaving $a-1$ and $b-1$ independent main-effect columns and $(a-1)(b-1)$ interaction columns formed from products of the non-reference indicators. This eliminates the linear combination that equals the intercept and ensures that the interaction terms are not redundant with the main effects.\n\n- Effect coding uses contrast columns that satisfy sum-to-zero constraints across levels, ensuring that the coded columns are linearly independent of the intercept. For interactions under effect coding, the interaction parameters obey hierarchical sum-to-zero constraints, such as $\\sum_{j=1}^{b}\\gamma_{kj}=0$ for each $k$ and $\\sum_{k=1}^{a}\\gamma_{kj}=0$ for each $j$, which ensures that the interaction space has $(a-1)(b-1)$ degrees of freedom and is not collinear with main effects or the intercept.\n\nVerification of full rank should use valid linear algebraic criteria: either confirm that $X^\\top X$ is positive definite (all eigenvalues strictly positive), or equivalently, that the smallest singular value of $X$ is strictly positive (from the Singular Value Decomposition (SVD) $X=U\\Sigma V^\\top$).\n\nOption-by-option analysis:\n\nA. This option proposes including the intercept, all $3$ indicators for $A$, all $2$ indicators for $B$, all $6$ indicators for the $A \\times B$ interaction, and $Z$. This construction guarantees perfect multicollinearity because the intercept column equals the sum of the $A$ indicators and also equals the sum of the $B$ indicators, and the interaction indicators are linear combinations of main-effect indicators. The suggested verification method—checking that the diagonal entries of $X^\\top X$ are positive—is not sufficient to establish full rank. The diagonal entries being positive do not preclude off-diagonal dependencies, and $X^\\top X$ can be singular despite positive diagonals. Verdict: Incorrect.\n\nB. This option specifies reference cell coding: include the intercept; $2$ dummy columns for $A$ (since $3-1=2$), $1$ dummy column for $B$ (since $2-1=1$); $(3-1)(2-1)=2$ interaction columns built as products of the non-reference indicators; and include $Z$. This removes the redundant indicator columns and limits the interaction space to $(a-1)(b-1)$ degrees of freedom, preventing overparameterization. The verification method—checking that all eigenvalues $\\lambda_i$ of $X^\\top X$ are strictly positive—is a correct criterion for positive definiteness and therefore full column rank. Verdict: Correct.\n\nC. This option proposes sum-to-one constraints on level coefficients while retaining all indicator columns, and verifying full rank via $\\det(X)\\neq 0$. Sum-to-one constraints are not the standard identifiability constraints in linear models with an intercept; they do not eliminate the dependency between the intercept and the full set of indicators, and, implemented at the coefficient level without appropriate contrast coding, they may still leave $X$ rank-deficient. Moreover, $\\det(X)$ is not a valid rank criterion because $X$ is typically an $n \\times p$ rectangular matrix with $n \\ge p$; its determinant is not defined unless $n=p$, which is not the generic regression setting. Rank verification should be performed on $X^\\top X$ or via the singular values of $X$. Verdict: Incorrect.\n\nD. This option uses effect coding with sum-to-zero constraints for $A$ and $B$, producing $2$ contrast columns for $A$ and $1$ contrast column for $B$, and constructs $(3-1)(2-1)=2$ interaction contrast columns with hierarchical sum-to-zero constraints on the interaction parameters, all together with the intercept and $Z$. This coding ensures that the columns are linearly independent of the intercept and that the interaction space has the correct $(a-1)(b-1)$ dimensionality, avoiding the dummy variable trap. The verification via the Singular Value Decomposition (SVD) $X=U\\Sigma V^\\top$ and checking that the smallest singular value is strictly positive is a correct test for full column rank. Verdict: Correct.\n\nTherefore, the correct options are B and D.",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "Once your model structure is defined, the next step is often to preprocess the predictors, which may be measured on vastly different scales. This exercise  explores the practical consequences of centering and standardizing your predictors, two common data transformation techniques. You will investigate how these methods affect the interpretation of your regression coefficients, help manage multicollinearity when including interaction terms, and improve the numerical stability of your calculations.",
            "id": "4930773",
            "problem": "A biostatistician models a continuous clinical outcome $Y$ (for example, systolic blood pressure) using multiple linear regression with an intercept and three predictors: age $A$ (in years), body mass index $B$ (in $\\text{kg/m}^2$), and dietary sodium intake $S$ (in $\\text{mg/day}$). Ordinary least squares (OLS) is used under the standard assumption that the error vector has mean $0$ and constant variance. Because the predictors are measured on different scales, she considers two common preprocessing steps applied to the predictor columns (not to $Y$): centering (subtracting each predictor’s sample mean so that its mean is $0$) and standardizing (subtracting each predictor’s sample mean and dividing by its sample standard deviation so that its mean is $0$ and variance is $1$). She fits models that include only linear main effects, and she also considers models that add interaction and polynomial terms (for example, $A \\times S$ and $B^2$), potentially formed after centering.\n\nAssume that multicollinearity is assessed using the variance inflation factor (VIF), defined via $R^2$ from regressing one predictor on the remaining predictors. Which of the following statements about the effects of centering and standardizing the predictor matrix $X$ on the interpretation of regression coefficients $\\beta$ and on multicollinearity are correct? Select all that apply.\n\nA. In a model with only linear main effects, centering $A$, $B$, and $S$ so that each has mean $0$ leaves the slope estimates for $A$, $B$, and $S$ numerically unchanged, but changes the intercept to estimate the expected value of $Y$ when $A$, $B$, and $S$ are at their sample means.\n\nB. Standardizing $A$, $B$, and $S$ (mean $0$, variance $1$) changes the fitted values and residuals because the predictors are on a different scale.\n\nC. In a model with only linear main effects, after standardizing, each slope can be interpreted as the expected change in $Y$ associated with a $1$ standard deviation increase in the corresponding predictor, and numerically equals the original unstandardized slope multiplied by that predictor’s sample standard deviation.\n\nD. Centering or standardizing linear predictors by themselves reduces the VIFs for those predictors because it reduces correlations among the predictors.\n\nE. Centering predictors before creating interaction or polynomial terms (for example, replacing $A$ by $A - \\bar{A}$ before forming $(A - \\bar{A}) \\times (S - \\bar{S})$ or $(B - \\bar{B})^2$) tends to reduce multicollinearity between main effects and those higher-order terms, without changing the fitted values.\n\nF. Standardizing predictors improves the numerical conditioning of the normal equations matrix used to compute OLS estimates, even though statistical multicollinearity measures based on $R^2$ (and VIFs) are unchanged by such linear rescaling of the original linear predictors.",
            "solution": "The general form of the multiple linear regression model is $Y = X\\beta + \\epsilon$, where $Y$ is the $n \\times 1$ vector of outcomes, $X$ is the $n \\times (p+1)$ design matrix (with a leading column of ones for an intercept and $p$ predictor columns), $\\beta$ is the $(p+1) \\times 1$ vector of coefficients, and $\\epsilon$ is the $n \\times 1$ vector of errors. The Ordinary Least Squares (OLS) estimate of $\\beta$ is $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y$.\n\nThe predictors are age ($A$), body mass index ($B$), and dietary sodium intake ($S$). The sample means are denoted $\\bar{A}$, $\\bar{B}$, $\\bar{S}$, and the sample standard deviations are $s_A$, $s_B$, $s_S$.\n\nCentering a predictor $A$ results in a new predictor $A_c = A - \\bar{A}$.\nStandardizing a predictor $A$ results in a new predictor $A_s = (A - \\bar{A}) / s_A$.\n\nWe will now evaluate each statement.\n\nA. In a model with only linear main effects, centering $A$, $B$, and $S$ so that each has mean $0$ leaves the slope estimates for $A$, $B$, and $S$ numerically unchanged, but changes the intercept to estimate the expected value of $Y$ when $A$, $B$, and $S$ are at their sample means.\n\nLet the original model be $Y_i = \\beta_0 + \\beta_A A_i + \\beta_B B_i + \\beta_S S_i + \\epsilon_i$.\nThe centered model is $Y_i = \\beta_0^* + \\beta_A^* (A_i - \\bar{A}) + \\beta_B^* (B_i - \\bar{B}) + \\beta_S^* (S_i - \\bar{S}) + \\epsilon_i$.\n\nAccording to the Frisch-Waugh-Lovell theorem, the coefficient for a given predictor in a multiple regression is equivalent to the coefficient obtained by regressing the residuals of the outcome (from a regression on all other predictors) on the residuals of the given predictor (from a regression on all other predictors). Centering all predictors is equivalent to residualizing them with respect to the intercept. The slope coefficients in a model with an intercept measure the effect of each predictor after accounting for the intercept. Therefore, adding a centered intercept term (by centering the predictors) to a model that already contains an intercept does not change the estimated slope coefficients. Thus, $\\hat{\\beta}_A = \\hat{\\beta}_A^*$, $\\hat{\\beta}_B = \\hat{\\beta}_B^*$, and $\\hat{\\beta}_S = \\hat{\\beta}_S^*$. The slope estimates are unchanged.\n\nFor the intercept in the centered model, the predictor-columns of the design matrix are orthogonal to the intercept-column (a vector of ones), since their sum (and mean) is $0$. This block-orthogonality simplifies the OLS estimation, and the intercept estimate becomes $\\hat{\\beta}_0^* = \\bar{Y}$. This is the predicted value of $Y$ when the centered predictors are all $0$, which occurs when $A_i = \\bar{A}$, $B_i = \\bar{B}$, and $S_i = \\bar{S}$.\nThe original intercept, $\\hat{\\beta}_0$, can be related to the new one:\n$\\hat{\\beta}_0^* + \\hat{\\beta}_A (A - \\bar{A}) = \\hat{\\beta}_0 + \\hat{\\beta}_A A$\n$\\hat{\\beta}_0 = \\hat{\\beta}_0^* - \\hat{\\beta}_A \\bar{A}$.\nFor all predictors, $\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_A \\bar{A} - \\hat{\\beta}_B \\bar{B} - \\hat{\\beta}_S \\bar{S}$. This is the predicted value of $Y$ when $A=0$, $B=0$, and $S=0$, which is different from $\\bar{Y}$ in general. The statement is correct on all points.\n\nVerdict: **Correct**.\n\nB. Standardizing $A$, $B$, and $S$ (mean $0$, variance $1$) changes the fitted values and residuals because the predictors are on a different scale.\n\nLet the original design matrix be $X = [\\mathbf{1}, X_A, X_B, X_S]$.\nLet the standardized design matrix be $X_{\\text{std}} = [\\mathbf{1}, X_{A,s}, X_{B,s}, X_{S,s}]$, where $X_{A,s} = (X_A - \\bar{A}\\mathbf{1})/s_A$.\nEach column of $X_{\\text{std}}$ is a linear combination of the columns of $X$. For example, $X_{A,s}$ is a linear combination of $X_A$ and $\\mathbf{1}$. Conversely, each column of $X$ is a linear combination of the columns of $X_{\\text{std}}$. For example, $X_A = s_A X_{A,s} + \\bar{A}\\mathbf{1}$.\nThis means that the column space of $X$ is identical to the column space of $X_{\\text{std}}$, i.e., $\\text{Col}(X) = \\text{Col}(X_{\\text{std}})$.\nThe fitted values $\\hat{Y}$ are the orthogonal projection of the outcome vector $Y$ onto the column space of the design matrix. The projection matrix, or hat matrix, is given by $H = X(X^\\top X)^{-1} X^\\top$. Since the column space is unchanged, the projection matrix is also unchanged. Therefore, the fitted values, $\\hat{Y} = HY$, are identical for both models.\nThe residuals are given by $e = Y - \\hat{Y}$. Since both $Y$ and $\\hat{Y}$ are unchanged, the residuals are also identical.\n\nVerdict: **Incorrect**.\n\nC. In a model with only linear main effects, after standardizing, each slope can be interpreted as the expected change in $Y$ associated with a $1$ standard deviation increase in the corresponding predictor, and numerically equals the original unstandardized slope multiplied by that predictor’s sample standard deviation.\n\nLet the standardized model be $Y = \\beta_0^{\\text{std}} + \\beta_A^{\\text{std}} A_s + \\dots + \\epsilon$. The predictor is $A_s = (A - \\bar{A})/s_A$.\nThe coefficient $\\beta_A^{\\text{std}}$ represents the change in $E[Y]$ for a $1$-unit increase in $A_s$, holding other standardized predictors constant. A $1$-unit increase in $A_s$ corresponds to an $s_A$-unit increase in the original predictor $A$. Thus, the interpretation is correct.\n\nNow, let's find the numerical relationship. Let the unstandardized model be $Y = \\beta_0 + \\beta_A A + \\dots + \\epsilon$.\nWe know from (A) that centering does not change the slope estimates. So, if we regress $Y$ on centered predictors $A_c = A - \\bar{A}$, the slope estimate is still $\\hat{\\beta}_A$.\nThe relationship between centered and standardized predictors is $A_c = s_A A_s$.\nSubstituting this into the centered model equation:\n$Y = \\dots + \\hat{\\beta}_A A_c + \\dots = \\dots + \\hat{\\beta}_A (s_A A_s) + \\dots$\nComparing this with the standardized model form, $Y = \\dots + \\hat{\\beta}_A^{\\text{std}} A_s + \\dots$, we must have:\n$\\hat{\\beta}_A^{\\text{std}} = \\hat{\\beta}_A s_A$.\nThe standardized slope is the original unstandardized slope multiplied by the predictor's sample standard deviation. The statement is fully correct.\n\nVerdict: **Correct**.\n\nD. Centering or standardizing linear predictors by themselves reduces the VIFs for those predictors because it reduces correlations among the predictors.\n\nThe Variance Inflation Factor for predictor $X_j$ is $\\text{VIF}_j = 1/(1 - R_j^2)$, where $R_j^2$ is the coefficient of determination from regressing $X_j$ on the other $p-1$ predictors.\nThe Pearson correlation coefficient between two variables $U$ and $V$ is invariant to scaling and shifting, i.e., $\\text{Corr}(aU+b, cV+d) = \\text{Corr}(U,V)$ for $a, c$ of the same sign. Centering ($A_c = A - \\bar{A}$) and standardizing ($A_s = (A-\\bar{A})/s_A$) are linear transformations.\nTherefore, the correlation between any two predictors $A$ and $B$ is unchanged by centering or standardizing them:\n$\\text{Corr}(A, B) = \\text{Corr}(A_c, B_c) = \\text{Corr}(A_s, B_s)$.\nThe value $R_j^2$ depends on the correlations among the predictors. Since these correlations are not changed, $R_j^2$ is not changed. Consequently, $\\text{VIF}_j$ is not changed. The premise \"because it reduces correlations\" is false, and the conclusion \"reduces the VIFs\" is also false.\n\nVerdict: **Incorrect**.\n\nE. Centering predictors before creating interaction or polynomial terms (for example, replacing $A$ by $A - \\bar{A}$ before forming $(A - \\bar{A}) \\times (S - \\bar{S})$ or $(B - \\bar{B})^2$) tends to reduce multicollinearity between main effects and those higher-order terms, without changing the fitted values.\n\nConsider a model with $A$ and $A^2$. The correlation between $A$ and $A^2$ is often high. Now consider the model with the centered predictor $A_c = A - \\bar{A}$ and its square $A_c^2 = (A - \\bar{A})^2$. The covariance is $E[A_c \\cdot A_c^2] - E[A_c]E[A_c^2] = E[(A-\\bar{A})^3]$, which is related to the skewness of $A$. For a symmetric distribution, this correlation is $0$. For many real-world distributions, $\\text{Corr}(A_c, A_c^2)$ is substantially smaller than $\\text{Corr}(A, A^2)$. Similarly, the correlation between a main effect $A_c$ and an interaction term $A_c S_c$ is often much lower than the correlation between $A$ and $AS$. So, centering does tend to reduce this \"structural\" multicollinearity.\n\nNow, consider whether the fitted values change. Let's compare Model 1 ($Y \\sim 1 + A + S + AS$) with Model 2 ($Y \\sim 1 + A_c + S_c + A_c S_c$).\nThe predictors in Model 2 are $1$, $A - \\bar{A}$, $S - \\bar{S}$, and $(A - \\bar{A})(S - \\bar{S}) = AS - \\bar{A}S - A\\bar{S} + \\bar{A}\\bar{S}$.\nEach of these predictors is a linear combination of the predictors in Model 1, which are $1$, $A$, $S$, and $AS$. For example, $A-\\bar{A} = 1 \\cdot A - \\bar{A} \\cdot 1$.\nConversely, we can express the predictors of Model 1 as linear combinations of those in Model 2. For example, $A = (A-\\bar{A}) + \\bar{A} \\cdot 1$. And $AS = (A_c S_c) + \\bar{S}A_c + \\bar{A}S_c + \\bar{A}\\bar{S} \\cdot 1$.\nSince the set of predictors for each model can be formed by linear combinations of the predictors from the other model, their design matrices span the same vector space. As established in the analysis of (B), if the column spaces are identical, the projection matrix $H$ is identical, and thus the fitted values $\\hat{Y}=HY$ and residuals $e=Y-\\hat{Y}$ are identical.\n\nVerdict: **Correct**.\n\nF. Standardizing predictors improves the numerical conditioning of the normal equations matrix used to compute OLS estimates, even though statistical multicollinearity measures based on $R^2$ (and VIFs) are unchanged by such linear rescaling of the original linear predictors.\n\nThe matrix to be inverted in OLS is $X^\\top X$. The numerical stability of this inversion is related to the matrix's condition number, $\\kappa(X^\\top X)$. A high condition number indicates an ill-conditioned matrix, prone to numerical errors.\nIf predictors are on vastly different scales (e.g., age in tens, sodium intake in thousands), the elements of $X^\\top X$ (which are sums of squares and cross-products) will have vastly different magnitudes. This often leads to a high condition number.\nWhen predictors are standardized, they all have mean $0$ and variance $1$. The predictor part of the design matrix, $X_p$, has columns with roughly equal norms. The matrix $(X_{p,\\text{std}})^\\top X_{p,\\text{std}}$ is $(n-1)$ times the correlation matrix of the predictors. Its diagonal elements are all equal ($(n-1)$), and off-diagonal elements are bounded. This brings the matrix elements to a similar scale, which is a standard technique for preconditioning a matrix. It generally reduces the condition number of $X^\\top X$ significantly, thus improving numerical stability.\nThe second part of the statement, \"even though statistical multicollinearity measures based on $R^2$ (and VIFs) are unchanged by such linear rescaling of the original linear predictors,\" is a restatement of the finding in (D) for a model with only main effects. As shown, VIFs are indeed unchanged. The statement correctly contrasts the numerical benefit with the unchanged statistical measures of collinearity.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ACEF}$$"
        },
        {
            "introduction": "After fitting a model, how do you measure its success? While the coefficient of determination, $R^2$, is widely used, it has a significant drawback: it never decreases when you add more predictors, tempting you to build overly complex models. This exercise  guides you through the derivation of the adjusted $R^2$, a more sophisticated metric that balances goodness-of-fit with model parsimony by penalizing the inclusion of non-informative variables.",
            "id": "4930824",
            "problem": "A biostatistician is building a multiple linear regression model for fasting plasma glucose, denoted by the response variable $Y$, using predictors age $X_{1}$, body mass index $X_{2}$, and sleep duration $X_{3}$. Let the linear model be $Y = X\\beta + \\varepsilon$, where $X$ includes an intercept column and $p$ predictor columns (so the total number of regression coefficients is $p+1$), the sample size is $n$, and $\\varepsilon$ is a random error term with mean $0$ and constant variance. The coefficient of determination $R^{2}$ is defined using the Total Sum of Squares (TSS) and the Residual Sum of Squares (RSS) by $R^{2} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$, where $\\text{TSS} = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}$ and $\\text{RSS} = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$. Starting from the fundamental definitions of $\\text{TSS}$ and $\\text{RSS}$ and the principle that unbiased estimators of variances divide sums of squares by their appropriate degrees of freedom, derive a closed-form expression for the adjusted coefficient of determination in terms of $R^{2}$, $n$, and $p$ (with $p$ counting predictors excluding the intercept). Then, using your derived expression, explain in words why the adjusted coefficient of determination can decrease when adding a predictor that does not materially improve model fit, referring to the role of degrees of freedom. Your final answer must be the single analytic expression for the adjusted coefficient of determination.",
            "solution": "The objective is to derive an expression for the adjusted coefficient of determination, $R^2_{\\text{adj}}$, and explain its behavior. The derivation begins from the principle that adjusted metrics should use unbiased estimators of variance, which are obtained by dividing a sum of squares by its corresponding degrees of freedom.\n\nThe coefficient of determination, $R^2$, is defined as $R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$, where $\\text{TSS}$ is the Total Sum of Squares and $\\text{RSS}$ is the Residual Sum of Squares. This metric measures the proportion of variance in the response variable that is predictable from the predictor variables. However, $R^2$ mechanically increases or remains constant whenever a new predictor is added to the model, regardless of the predictor's actual explanatory power. This is because the least-squares optimization will always reduce or hold constant the $\\text{RSS}$, even if just by chance. The adjusted $R^2$ corrects for this by penalizing the inclusion of non-informative predictors.\n\nWe start with the unbiased estimators for the total variance of the response variable, $\\sigma^2_Y$, and the variance of the model's random error term, $\\sigma^2_{\\varepsilon}$.\n\nThe Total Sum of Squares, $\\text{TSS} = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}$, has $n-1$ degrees of freedom, because one degree of freedom is lost in the estimation of the sample mean $\\bar{y}$. Therefore, an unbiased estimator for the total variance of $Y$ is the Mean Square Total, $\\text{MST}$:\n$$ \\text{MST} = \\frac{\\text{TSS}}{n-1} $$\n\nThe Residual Sum of Squares, $\\text{RSS} = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$, has $n-(p+1)$ degrees of freedom. This is because $p+1$ parameters are estimated in the multiple linear regression model: $p$ coefficients for the predictor variables and one intercept term. An unbiased estimator for the error variance $\\sigma^2_{\\varepsilon}$ is the Mean Square Error, $\\text{MSE}$:\n$$ \\text{MSE} = \\frac{\\text{RSS}}{n-p-1} $$\n\nThe adjusted coefficient of determination, $R^2_{\\text{adj}}$, is defined analogously to $R^2$, but it uses the ratio of these unbiased variance estimators instead of the ratio of the sums of squares:\n$$ R^2_{\\text{adj}} = 1 - \\frac{\\text{MSE}}{\\text{MST}} $$\nSubstituting the expressions for $\\text{MSE}$ and $\\text{MST}$, we get:\n$$ R^2_{\\text{adj}} = 1 - \\frac{\\frac{\\text{RSS}}{(n-p-1)}}{\\frac{\\text{TSS}}{(n-1)}} $$\nThis expression can be rearranged as:\n$$ R^2_{\\text{adj}} = 1 - \\left(\\frac{\\text{RSS}}{\\text{TSS}}\\right) \\left(\\frac{n-1}{n-p-1}\\right) $$\nFrom the definition of the standard coefficient of determination, $R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$, we can express the ratio $\\frac{\\text{RSS}}{\\text{TSS}}$ in terms of $R^2$:\n$$ \\frac{\\text{RSS}}{\\text{TSS}} = 1 - R^2 $$\nSubstituting this into our equation for $R^2_{\\text{adj}}$ yields the final expression in terms of $R^2$, $n$, and $p$:\n$$ R^2_{\\text{adj}} = 1 - (1 - R^2)\\left(\\frac{n-1}{n-p-1}\\right) $$\n\nNow, we use this expression to explain why $R^2_{\\text{adj}}$ can decrease when a predictor that does not materially improve the model fit is added. When a new predictor is added to the model, the number of predictors, $p$, increases by $1$. This has two competing effects on the formula for $R^2_{\\text{adj}}$:\n\n1.  **Effect on $R^2$**: The addition of any predictor, useful or not, will cause the standard $R^2$ to increase or, in a trivial case, stay the same. This means the term $(1 - R^2)$ will decrease or stay the same. This part of the expression pushes $R^2_{\\text{adj}}$ to increase.\n\n2.  **Effect on the Penalty Factor**: The term $\\left(\\frac{n-1}{n-p-1}\\right)$ is a penalty for model complexity. When $p$ increases to $p+1$, the denominator $(n-p-1)$ decreases, causing the entire penalty factor to increase. This factor is always greater than or equal to $1$ for $p \\ge 0$. This part of the expression pushes $R^2_{\\text{adj}}$ to decrease.\n\nThe overall change in $R^2_{\\text{adj}}$ depends on the balance between these two effects. If the new predictor \"does not materially improve model fit,\" it means the resulting increase in $R^2$ is very small. The corresponding decrease in the $(1 - R^2)$ term is slight. However, the penalty factor $\\left(\\frac{n-1}{n-p-1}\\right)$ strictly increases with the addition of the new predictor. If the small reduction in $(1 - R^2)$ is not large enough to overcome the increase in the penalty factor, their product, $(1 - R^2)\\left(\\frac{n-1}{n-p-1}\\right)$, will increase. Since this product is subtracted from $1$, an increase in its value will cause $R^2_{\\text{adj}}$ to decrease.\n\nIn essence, the adjusted $R^2$ only increases if the improvement in fit (the increase in $R^2$) is substantial enough to justify the \"cost\" of spending an additional degree of freedom on the new predictor. This mechanism ensures that $R^2_{\\text{adj}}$ is a more honest measure of model quality, as it rewards goodness of fit while penalizing model complexity.",
            "answer": "$$\n\\boxed{1 - (1 - R^2) \\frac{n-1}{n-p-1}}\n$$"
        }
    ]
}