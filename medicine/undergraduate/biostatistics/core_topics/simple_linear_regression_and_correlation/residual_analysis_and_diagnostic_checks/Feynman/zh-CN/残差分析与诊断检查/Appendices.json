{
    "hands_on_practices": [
        {
            "introduction": "杠杆率（leverage）是衡量单个观测点对回归模型具有多大*潜在*影响力的关键指标。这种潜力仅取决于解释变量的取值，而与响应变量无关。本练习  将通过推导简单线性回归中的杠杆率公式，来具体化这一抽象概念，揭示观测点与数据中心的距离如何决定其潜在影响力。",
            "id": "4949183",
            "problem": "一项生物统计学研究将一个连续的生物标志物响应建模为一个连续剂量的线性函数，并带有一个截距。具体来说，对于索引为 $i \\in \\{1,\\dots,n\\}$ 的观测值，模型为 $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$，其中误差 $\\varepsilon_{i}$ 满足用于证明拟合值的投影性质的常规普通最小二乘 (OLS) 假设。设 $n \\times 2$ 的设计矩阵为 $X = \\begin{pmatrix} 1  x_{1} \\\\ \\vdots  \\vdots \\\\ 1  x_{n} \\end{pmatrix}$，并设投影（帽子）矩阵是将在 OLS 下观测到的响应映射到其拟合值的唯一线性算子。该投影矩阵的对角线元素即为杠杆值，记作 $h_{ii}$，它量化了观测值 $i$ 影响其自身拟合值的潜力。\n\n请仅从标准线性模型定义和 $2 \\times 2$ 矩阵的线性代数事实出发，推导出杠杆值 $h_{ii}$ 的闭式表达式，用 $n$、$x_{i}$、$\\bar{x}$ 和 $\\sum_{j=1}^{n} (x_{j} - \\bar{x})^{2}$ 来表示，其中 $\\bar{x} = \\frac{1}{n} \\sum_{j=1}^{n} x_{j}$。你的推导应明确说明为使用 $\\bar{x}$ 和 $\\sum_{j=1}^{n} (x_{j} - \\bar{x})^{2}$ 来替代无关求和项而必需的任何中间代数步骤。\n\n此外，请根据你的推导，定性地解释设计点 $\\{x_{j}\\}_{j=1}^{n}$ 的间距如何决定一个观测值的潜在影响，以及为什么相对于 $\\bar{x}$ 更极端的 $x_{i}$ 值具有更高的杠杆值。\n\n请提供 $h_{ii}$ 的简化解析表达式作为你的最终答案。无需进行数值四舍五入，也不涉及单位。",
            "solution": "该问题要求推导简单线性回归模型的杠杆值 $h_{ii}$，并对其性质进行定性解释。推导必须从线性模型理论的基本定义开始。\n\n简单线性回归模型由 $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$ 给出。其矩阵形式为 $\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{y}$ 是响应值的 $n \\times 1$ 向量，$X$ 是 $n \\times 2$ 的设计矩阵，$\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}$ 是系数向量，$\\boldsymbol{\\varepsilon}$ 是误差向量。\n\n设计矩阵 $X$ 如下所示：\n$$\nX = \\begin{pmatrix} 1  x_{1} \\\\ 1  x_{2} \\\\ \\vdots  \\vdots \\\\ 1  x_{n} \\end{pmatrix}\n$$\n普通最小二乘 (OLS) 拟合值向量 $\\hat{\\mathbf{y}}$ 是通过将观测响应向量 $\\mathbf{y}$ 投影到 $X$ 的列空间上得到的。投影矩阵，或称帽子矩阵，用 $H$ 表示，定义为：\n$$\nH = X(X^T X)^{-1} X^T\n$$\n第 $i$ 个观测值的杠杆值 $h_{ii}$ 是 $H$ 的第 $i$ 个对角线元素。它可以计算为 $h_{ii} = \\mathbf{x}_i^T (X^T X)^{-1} \\mathbf{x}_i$，其中 $\\mathbf{x}_i^T = \\begin{pmatrix} 1  x_i \\end{pmatrix}$ 是设计矩阵 $X$ 的第 $i$ 行。\n\n我们的推导分几个步骤进行。\n\n**步骤 1：计算矩阵 $X^T X$。**\n矩阵 $X^T X$ 是一个 $2 \\times 2$ 的对称矩阵。\n$$\nX^T X = \\begin{pmatrix} 1  1  \\dots  1 \\\\ x_1  x_2  \\dots  x_n \\end{pmatrix} \\begin{pmatrix} 1  x_{1} \\\\ 1  x_{2} \\\\ \\vdots  \\vdots \\\\ 1  x_{n} \\end{pmatrix} = \\begin{pmatrix} \\sum_{j=1}^{n} 1  \\sum_{j=1}^{n} x_j \\\\ \\sum_{j=1}^{n} x_j  \\sum_{j=1}^{n} x_j^2 \\end{pmatrix}\n$$\n使用样本均值的定义 $\\bar{x} = \\frac{1}{n} \\sum_{j=1}^{n} x_j$，我们可以写出 $\\sum_{j=1}^{n} x_j = n\\bar{x}$。\n因此，\n$$\nX^T X = \\begin{pmatrix} n  n\\bar{x} \\\\ n\\bar{x}  \\sum_{j=1}^{n} x_j^2 \\end{pmatrix}\n$$\n\n**步骤 2：计算 $X^T X$ 的行列式及其逆矩阵。**\n$X^T X$ 的行列式为：\n$$\n\\det(X^T X) = n \\left(\\sum_{j=1}^{n} x_j^2\\right) - (n\\bar{x})^2 = n \\sum_{j=1}^{n} x_j^2 - n^2\\bar{x}^2\n$$\n为了用所要求的项来表示它，我们使用均值离差平方和的定义，记作 $S_{xx}$：\n$$\nS_{xx} = \\sum_{j=1}^{n} (x_j - \\bar{x})^2 = \\sum_{j=1}^{n} (x_j^2 - 2x_j\\bar{x} + \\bar{x}^2) = \\left(\\sum_{j=1}^{n} x_j^2\\right) - 2\\bar{x}\\left(\\sum_{j=1}^{n} x_j\\right) + \\sum_{j=1}^{n} \\bar{x}^2\n$$\n代入 $\\sum_{j=1}^{n} x_j = n\\bar{x}$，我们得到：\n$$\nS_{xx} = \\left(\\sum_{j=1}^{n} x_j^2\\right) - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2 = \\left(\\sum_{j=1}^{n} x_j^2\\right) - n\\bar{x}^2\n$$\n因此，行列式可以简化为：\n$$\n\\det(X^T X) = n \\left( \\left(\\sum_{j=1}^{n} x_j^2\\right) - n\\bar{x}^2 \\right) = n S_{xx} = n \\sum_{j=1}^{n} (x_j - \\bar{x})^2\n$$\n现在，我们使用通用 $2 \\times 2$ 矩阵的逆矩阵公式 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}^{-1} = \\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$ 来求 $X^T X$ 的逆矩阵：\n$$\n(X^T X)^{-1} = \\frac{1}{n S_{xx}} \\begin{pmatrix} \\sum_{j=1}^{n} x_j^2  -n\\bar{x} \\\\ -n\\bar{x}  n \\end{pmatrix}\n$$\n为了简化这个表达式，我们代入 $\\sum_{j=1}^{n} x_j^2 = S_{xx} + n\\bar{x}^2$：\n$$\n(X^T X)^{-1} = \\frac{1}{n S_{xx}} \\begin{pmatrix} S_{xx} + n\\bar{x}^2  -n\\bar{x} \\\\ -n\\bar{x}  n \\end{pmatrix} = \\begin{pmatrix} \\frac{S_{xx} + n\\bar{x}^2}{n S_{xx}}  -\\frac{n\\bar{x}}{n S_{xx}} \\\\ -\\frac{n\\bar{x}}{n S_{xx}}  \\frac{n}{n S_{xx}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}  -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}}  \\frac{1}{S_{xx}} \\end{pmatrix}\n$$\n\n**步骤 3：计算杠杆值 $h_{ii}$。**\n使用公式 $h_{ii} = \\mathbf{x}_i^T (X^T X)^{-1} \\mathbf{x}_i$ 和 $\\mathbf{x}_i^T = \\begin{pmatrix} 1  x_i \\end{pmatrix}$：\n$$\nh_{ii} = \\begin{pmatrix} 1  x_i \\end{pmatrix} \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}  -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}}  \\frac{1}{S_{xx}} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}\n$$\n首先，将行向量与矩阵相乘：\n$$\n\\begin{pmatrix} 1  x_i \\end{pmatrix} \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}  -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}}  \\frac{1}{S_{xx}} \\end{pmatrix} = \\begin{pmatrix} \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} - \\frac{x_i \\bar{x}}{S_{xx}}\\right)  \\left(-\\frac{\\bar{x}}{S_{xx}} + \\frac{x_i}{S_{xx}}\\right) \\end{pmatrix}\n$$\n接下来，将得到的 $1 \\times 2$ 向量与列向量 $\\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}$ 相乘：\n$$\nh_{ii} = \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} - \\frac{x_i \\bar{x}}{S_{xx}}\\right) \\cdot 1 + \\left(\\frac{x_i - \\bar{x}}{S_{xx}}\\right) \\cdot x_i\n$$\n$$\nh_{ii} = \\frac{1}{n} + \\frac{\\bar{x}^2 - x_i \\bar{x} + x_i^2 - \\bar{x} x_i}{S_{xx}} = \\frac{1}{n} + \\frac{x_i^2 - 2x_i\\bar{x} + \\bar{x}^2}{S_{xx}}\n$$\n注意到第二项的分子是 $(x_i - \\bar{x})^2$，我们得到最终表达式：\n$$\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}\n$$\n代回 $S_{xx}$ 的定义：\n$$\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}\n$$\n这是观测值 $i$ 的杠杆值的所求闭式表达式。\n\n**杠杆值的定性解释**\n\n推导出的杠杆值表达式 $h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}$ 直接揭示了决定一个观测值对其自身拟合值潜在影响的因素。杠杆值由两个非负项组成：\n\n$1$. 项 $\\frac{1}{n}$ 代表了对所有观测值都均一的杠杆值基线分量。它只取决于样本量 $n$。随着观测数量的增加，这个基线影响会减小，反映出在更大的数据集中任何单个数据点的重要性都会降低。\n\n$2$. 第二项 $\\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}$ 代表了由于预测变量值 $x_i$ 相对于其他预测变量值的位置而产生的杠杆分量。\n   - 分子 $(x_i - \\bar{x})^2$ 是 $x_i$ 到预测变量数据中心 $\\bar{x}$ 的距离的平方。这一项表明，当一个观测值的预测变量值变得更极端（即离均值 $\\bar{x}$ 更远）时，其杠杆值会呈平方增长。具有较大 $|x_i - \\bar{x}|$ 值的观测值被称为高杠杆点。这些点就像长杠杆臂一样，使它们具有更大的潜力来撬动拟合的回归线。\n   - 分母 $\\sum_{j=1}^{n} (x_j - \\bar{x})^2$ 是所有预测变量值的离差平方和。它量化了设计点 $\\{x_j\\}$ 的总散布程度或变异性。这一项设定了用以判断给定点 $x_i$ “极端性”的尺度。\n     - 如果设计点间距很宽，分母就会很大。这会减小所有点的位置杠杆分量的大小，意味着回归线被分散的数据“锚定”得更稳固，任何单个数据点的影响都会减弱。\n     - 相反，如果设计点紧密聚集，分母就会很小。这会放大了位置杠杆分量。在这种情况下，一个离 $\\bar{x}$ 仅有中等距离的观测值也可能具有非常高的杠杆值，因为它相对于其余数据的狭窄分布而言是一个异常值。\n\n总之，如果一个观测值的预测变量值远离所有预测变量值的均值，特别是当大多数其他预测变量值分布不广时，该观测值的杠杆值就很高。这为预测变量空间中的远端数据点在回归分析中最具影响力这一直观概念提供了精确的定量基础。",
            "answer": "$$\n\\boxed{\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}}\n$$"
        },
        {
            "introduction": "仅有高杠杆率并不足以产生实际影响；该观测点的响应值还必须“出人意料”（即具有较大的残差）。库克距离（Cook's distance）正是这样一个结合了杠杆率和残差大小的度量，用以量化单个观测点对模型系数的*实际*影响。本练习  将推导库克距离的常用计算公式，清晰地展示其如何同时依赖于杠杆率和学生化残差。",
            "id": "4949167",
            "problem": "一位生物统计学家拟合了一个普通最小二乘 (OLS) 线性回归模型，其响应向量为 $y \\in \\mathbb{R}^{n}$，设计矩阵为满秩矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其中 $n > p \\ge 2$。假设经典线性模型为 $y = X\\beta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。令 $\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y$ 为 OLS 估计量，$\\hat{y} = X \\hat{\\beta}$ 为拟合值，$e = y - \\hat{y}$ 为残差向量。定义帽子矩阵 $H = X (X^{\\top} X)^{-1} X^{\\top}$，并令 $h_{ii}$ 表示其第 $i$ 个对角元素（观测值 $i$ 的杠杆值）。令 $s^{2} = \\frac{1}{n-p} \\|y - X \\hat{\\beta}\\|_{2}^{2}$ 为均方误差。观测值 $i$ 的内部学生化残差定义为 $t_{i} = \\frac{e_{i}}{s \\sqrt{1 - h_{ii}}}$。对于省略观测值 $i$ 的留一法拟合，将其系数估计量记为 $\\hat{\\beta}_{(i)}$。观测值 $i$ 的库克距离 (Cook’s distance) 定义为\n$$\nD_{i} \\;=\\; \\frac{\\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)^{\\top} X^{\\top} X \\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)}{p\\, s^{2}}.\n$$\n仅从这些定义和关于秩一更新的标准线性代数恒等式出发，推导一个仅用 $p$、$t_{i}$ 和 $h_{ii}$ 表示的 $D_{i}$ 的显式表达式。然后，根据你的推导，简要解释杠杆值 $h_{ii}$ 和残差大小（由 $t_{i}$ 体现）各自如何影响 $D_{i}$。\n\n以 $D_{i}$ 关于 $p$、$t_{i}$ 和 $h_{ii}$ 的闭式解析表达式的形式提供你的最终答案。无需进行数值计算。",
            "solution": "这个问题提法得当，科学上基于线性模型理论，并包含了获得唯一解析解所需的所有信息。所给定义和前提都是标准的且内部一致。\n\n目标是推导库克距离 $D_{i}$ 的表达式，该表达式用预测变量数 $p$、内部学生化残差 $t_{i}$ 和杠杆值 $h_{ii}$ 表示。为观测值 $i$ 提供的库克距离定义是：\n$$ D_{i} \\;=\\; \\frac{\\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)^{\\top} X^{\\top} X \\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)}{p\\, s^{2}} $$\n推导的核心是找到因省略第 $i$ 个观测值而导致的系数向量变化 $\\hat{\\beta} - \\hat{\\beta}_{(i)}$ 的简化表达式。留一法估计量 $\\hat{\\beta}_{(i)}$ 是通过使用数据矩阵 $X_{(i)}$ 和 $y_{(i)}$ 计算得出的，这两个矩阵分别是通过从 $X$ 和 $y$ 中移除第 $i$ 行而形成的。令 $x_i^{\\top}$ 为 $X$ 的第 $i$ 行。留一法估计量所需的矩阵乘积是全数据乘积的秩一修正：\n$$ X_{(i)}^{\\top} X_{(i)} = X^{\\top}X - x_i x_i^{\\top} $$\n$$ X_{(i)}^{\\top} y_{(i)} = X^{\\top}y - x_i y_i $$\n为了计算 $\\hat{\\beta}_{(i)} = (X_{(i)}^{\\top} X_{(i)})^{-1} X_{(i)}^{\\top} y_{(i)}$，我们首先需要 $X_{(i)}^{\\top} X_{(i)}$ 的逆。我们使用关于秩一更新的 Sherman-Morrison-Woodbury 公式，该公式指出 $(A - uv^{\\top})^{-1} = A^{-1} + \\frac{A^{-1}uv^{\\top}A^{-1}}{1 - v^{\\top}A^{-1}u}$。设 $A = X^{\\top}X$ 且 $u = v = x_i$，我们得到：\n$$ (X^{\\top}X - x_i x_i^{\\top})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - x_i^{\\top}(X^{\\top}X)^{-1}x_i} $$\n分母中的项 $x_i^{\\top}(X^{\\top}X)^{-1}x_i$ 对应于帽子矩阵 $H = X(X^{\\top}X)^{-1}X^{\\top}$ 的第 $i$ 个对角元素，即杠杆值 $h_{ii}$。因此，逆矩阵为：\n$$ (X_{(i)}^{\\top} X_{(i)})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - h_{ii}} $$\n现在，我们可以表示 $\\hat{\\beta}_{(i)}$：\n$$ \\hat{\\beta}_{(i)} = \\left( (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - h_{ii}} \\right) (X^{\\top}y - x_i y_i) $$\n展开此表达式可得：\n$$ \\hat{\\beta}_{(i)} = (X^{\\top}X)^{-1}(X^{\\top}y) - (X^{\\top}X)^{-1}x_i y_i + \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} \\left( x_i^{\\top}(X^{\\top}X)^{-1}X^{\\top}y - x_i^{\\top}(X^{\\top}X)^{-1}x_i y_i \\right) $$\n我们识别出几个项：$\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$、第 $i$ 个拟合值 $\\hat{y}_i = x_i^{\\top}\\hat{\\beta} = x_i^{\\top}(X^{\\top}X)^{-1}X^{\\top}y$ 和 $h_{ii} = x_i^{\\top}(X^{\\top}X)^{-1}x_i$。代入这些可得：\n$$ \\hat{\\beta}_{(i)} = \\hat{\\beta} - (X^{\\top}X)^{-1}x_i y_i + \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} (\\hat{y}_i - h_{ii} y_i) $$\n重新整理以求差分向量 $\\hat{\\beta} - \\hat{\\beta}_{(i)}$：\n$$ \\hat{\\beta} - \\hat{\\beta}_{(i)} = (X^{\\top}X)^{-1}x_i y_i - \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} (\\hat{y}_i - h_{ii} y_i) = (X^{\\top}X)^{-1}x_i \\left( y_i - \\frac{\\hat{y}_i - h_{ii} y_i}{1 - h_{ii}} \\right) $$\n括号中的项简化为：\n$$ \\frac{y_i(1 - h_{ii}) - (\\hat{y}_i - h_{ii} y_i)}{1 - h_{ii}} = \\frac{y_i - y_i h_{ii} - \\hat{y}_i + y_i h_{ii}}{1 - h_{ii}} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} = \\frac{e_i}{1-h_{ii}} $$\n其中 $e_i = y_i - \\hat{y}_i$ 是第 $i$ 个残差。这导出了系数变化的关键结果：\n$$ \\hat{\\beta} - \\hat{\\beta}_{(i)} = \\frac{(X^{\\top}X)^{-1}x_i e_i}{1 - h_{ii}} $$\n接下来，我们将其代入库克距离的分子中：\n$$ (\\hat{\\beta} - \\hat{\\beta}_{(i)})^{\\top} X^{\\top} X (\\hat{\\beta} - \\hat{\\beta}_{(i)}) = \\left( \\frac{e_i}{1-h_{ii}} x_i^{\\top}(X^{\\top}X)^{-1} \\right) (X^{\\top}X) \\left( \\frac{(X^{\\top}X)^{-1}x_i e_i}{1-h_{ii}} \\right) $$\n$$ = \\left( \\frac{e_i}{1-h_{ii}} \\right)^2 x_i^{\\top} \\left( (X^{\\top}X)^{-1} (X^{\\top}X) (X^{\\top}X)^{-1} \\right) x_i = \\frac{e_i^2}{(1-h_{ii})^2} x_i^{\\top} (X^{\\top}X)^{-1} x_i $$\n使用 $h_{ii} = x_i^{\\top}(X^{\\top}X)^{-1}x_i$，分子变为 $\\frac{e_i^2 h_{ii}}{(1-h_{ii})^2}$。将其代入 $D_i$ 的定义中：\n$$ D_i = \\frac{1}{p s^2} \\left[ \\frac{e_i^2 h_{ii}}{(1-h_{ii})^2} \\right] $$\n最后一步是引入内部学生化残差 $t_{i} = \\frac{e_i}{s \\sqrt{1 - h_{ii}}}$。平方此式可得 $t_i^2 = \\frac{e_i^2}{s^2(1-h_{ii})}$，由此我们得到 $e_i^2 = t_i^2 s^2 (1 - h_{ii})$。代入 $e_i^2$ 的这个表达式：\n$$ D_i = \\frac{1}{p s^2} \\left[ \\frac{t_i^2 s^2 (1 - h_{ii}) h_{ii}}{(1-h_{ii})^2} \\right] $$\n消去 $s^2$ 和一个因子 $(1-h_{ii})$，得到库克距离的最终表达式：\n$$ D_i = \\frac{t_i^2}{p} \\frac{h_{ii}}{1-h_{ii}} $$\n这个推导出的表达式阐明了观测值影响力的本质。由 $D_i$ 衡量的影响力是两个因素的乘积：一个与残差大小 ($t_i^2$) 相关，另一个与观测值的杠杆值 ($h_{ii}$) 相关。\n项 $t_i^2$ 表明 $D_i$ 与学生化残差的平方成正比。远离回归超平面（响应空间中的离群点）的点将具有较大的 $|t_i|$ 值，从而导致较大的 $D_i$。\n项 $\\frac{h_{ii}}{1-h_{ii}}$ 捕捉了杠杆值的影响。杠杆值 $h_{ii}$ 量化了观测值的预测变量值 $x_i$ 与预测变量空间中心的距离。当杠杆值 $h_{ii}$ 接近 $1$ 时，函数 $\\frac{h_{ii}}{1-h_{ii}}$ 非线性地增加。这意味着高杠杆点对残差项起到了强烈的乘数效应。\n总之，如果一个观测值具有大残差、高杠杆值或两者兼有，那么它就是有影响力的。一个高杠杆点具有高影响力的潜力；如果该点同时具有不可忽略的残差，这种潜力就会实现。反之，一个杠杆值低的点必须有非常大的残差才能产生影响力。",
            "answer": "$$\\boxed{\\frac{t_{i}^{2}}{p} \\frac{h_{ii}}{1 - h_{ii}}}$$"
        },
        {
            "introduction": "在深入探讨库克距离中的“残差大小”部分时，我们面临一个严峻挑战：离群点遮蔽效应（outlier masking）。这种效应指离群点本身会夸大方差的估计值，从而使自己更难被检测出来。这项概念性练习  对比了内学生化残差和外学生化残差，阐明了为何后者是揭示单个离群点的更强大工具，从而提高了我们诊断检验的可靠性。",
            "id": "4949136",
            "problem": "一位生物统计学家拟合了一个线性模型 $y = X\\beta + \\varepsilon$ 来研究一个连续生物标志物响应 $y$ 与一组协变量 $X$（包括截距项）之间的关系，其中 $\\varepsilon$ 是均值为 $0$、方差为 $\\sigma^{2}$ 的独立误差项。普通最小二乘法（OLS）得到拟合值 $\\hat{y}$ 和残差 $e = y - \\hat{y}$。设 $H = X(X^{\\top}X)^{-1}X^{\\top}$ 为帽子矩阵，其对角元素为 $h_{ii}$，并且我们知道在标准OLS假设下，$\\operatorname{Var}(e_{i}) = \\sigma^{2}(1 - h_{ii})$。\n\n为了筛查异常值，该分析师考虑了在观测点 $i$ 处的两个标准化残差统计量：内学生化统计量 $t_{i}$，它使用全数据的均方误差（MSE）来估计 $\\sigma^{2}$；以及外学生化（剔除）统计量 $t_{i}^{*}$，它使用移除观测点 $i$ 后计算的留一法MSE。这两种统计量都通过 $h_{ii}$ 对杠杆率进行了调整，但它们在误差方差的估计方式上有所不同。\n\n假设恰好有一个观测点（索引为 $i$）是异常的，即其误差 $\\varepsilon_{i}$ 发生了偏移，导致相应的残差 $e_{i}$ 的量级远大于通常水平，而所有其他误差 $\\varepsilon_{j}$（对于 $j \\neq i$）都遵循模型假设。在基于这些统计量的异常值检验的固定显著性水平下，就其方差估计的差异而言，以下哪个陈述最能描述 $t_{i}$ 相对于 $t_{i}^{*}$ 的相对功效（正确标记异常观测值的概率）？\n\n选择唯一的最佳选项。\n\nA. 当只有一个观测值异常时，$t_{i}^{*}$ 通常比 $t_{i}$ 有更大的功效，因为留一法方差估计排除了该异常观测值，因此不会被放大；而全数据方差估计被异常残差所放大，从而减小了 $t_{i}$ 的量级并导致掩盖效应。\n\nB. 当只有一个观测值异常时，$t_{i}$ 和 $t_{i}^{*}$ 具有相等的功效，因为两者都通过 $h_{ii}$ 对杠杆率进行了调整，因此按相同的有效方差进行缩放。\n\nC. $t_{i}$ 比 $t_{i}^{*}$ 有更大的功效，因为使用所有 $n$ 个观测值会得到比留一法估计更小的全数据方差估计，从而使 $t_{i}$ 的量级更大。\n\nD. $t_{i}^{*}$ 比 $t_{i}$ 的功效更低，因为移除观测值会减少自由度，从而使留一法方差估计增加的程度足以抵消任何掩盖效应。",
            "solution": "该问题陈述是回归诊断领域中一个恰当提出的问题，这是生物统计学和广义统计学中的一个标准课题。所有术语都是标准的，设置清晰且内部一致。它描述了一个比较异常值检测统计量的经典场景。因此，该问题是有效的。\n\n问题的核心是比较两种不同类型的学生化残差在检测单个异常值时的统计功效。让我们形式化地定义所涉及的量。\n\n线性模型由 $y = X\\beta + \\varepsilon$ 给出，其中误差项 $\\varepsilon_i$ 是独立的，且 $\\mathrm{E}[\\varepsilon_i] = 0$ 和 $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2$。使用普通最小二乘法（OLS）拟合模型后，我们得到残差 $e = y - \\hat{y}$。第 $i$ 个残差的方差是 $\\operatorname{Var}(e_i) = \\sigma^2(1 - h_{ii})$，其中 $h_{ii}$ 是帽子矩阵 $H = X(X^{\\top}X)^{-1}X^{\\top}$ 的第 $i$ 个对角元素。\n\n用于筛查第 $i$ 个观测值是否为异常值的两个统计量是：\n1.  **内学生化残差**，$t_i$：该统计量使用误差方差的全数据估计值 $\\hat{\\sigma}^2$，即均方误差（MSE）。\n    $$ \\hat{\\sigma}^2 = \\text{MSE} = \\frac{\\sum_{j=1}^{n} e_j^2}{n-p} = \\frac{SSE}{n-p} $$\n    其中 $n$ 是观测值的数量，$p$ 是模型中的参数数量（即 $X$ 的列数）。该统计量为：\n    $$ t_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}} $$\n\n2.  **外学生化（或剔除）残差**，$t_i^*$：该统计量使用误差方差的留一法估计值 $\\hat{\\sigma}_{(i)}^2$。这个估计值是通过对移除了第 $i$ 个观测值的数据拟合回归模型来计算的。\n    $$ \\hat{\\sigma}_{(i)}^2 = \\text{MSE}_{(i)} = \\frac{SSE_{(i)}}{n-1-p} $$\n    其中 $SSE_{(i)}$ 是不包含观测值 $i$ 的模型拟合所产生的残差平方和。该统计量为：\n    $$ t_i^* = \\frac{e_i}{\\hat{\\sigma}_{(i)}\\sqrt{1-h_{ii}}} $$\n    请注意，两个统计量在分子中使用相同的原始残差 $e_i$，在分母中使用相同的杠杆率调整 $\\sqrt{1-h_{ii}}$。唯一的区别在于方差估计：$\\hat{\\sigma}$ 与 $\\hat{\\sigma}_{(i)}$。\n\n问题设定了一个特定情景：恰好有一个观测点（索引为 $i$）是异常的，以至于其残差 $e_i$ 的量级“远大于通常水平”。这意味着 $e_i^2$ 与其他平方残差 $e_j^2$（对于 $j \\neq i$）相比非常大。\n\n让我们分析两个方差估计值 $\\hat{\\sigma}^2$ 和 $\\hat{\\sigma}_{(i)}^2$ 之间的关系。有一个著名的恒等式，它将全数据残差平方和（$SSE$）与留一法残差平方和（$SSE_{(i)}$）联系起来：\n$$ SSE = SSE_{(i)} + \\frac{e_i^2}{1-h_{ii}} $$\n这个恒等式表明，全数据 $SSE$ 由不包含观测值 $i$ 的数据的 $SSE$ 加上一个与该观测值残差平方成正比的项组成，该项还被其杠杆率放大（因为 $0  h_{ii}  1$）。\n\n现在考虑方差估计：\n- 全数据估计：$\\hat{\\sigma}^2 = \\frac{SSE}{n-p} = \\frac{SSE_{(i)} + e_i^2/(1-h_{ii})}{n-p}$\n- 留一法估计：$\\hat{\\sigma}_{(i)}^2 = \\frac{SSE_{(i)}}{n-p-1}$\n\n由于观测值 $i$ 是一个异常值，$e_i^2$ 很大。这个大的 $e_i^2$ 值贡献给了 $\\hat{\\sigma}^2$ 的分子，导致它成为对真实误差方差 $\\sigma^2$ 的一个被放大的估计。异常值实际上将方差估计“拉”向其自身。相比之下，$\\hat{\\sigma}_{(i)}^2$ 是由剩余的 $n-1$ 个观测值计算得出的，根据假设，这些观测值不是异常的。因此，$\\hat{\\sigma}_{(i)}^2$ 是对真实方差 $\\sigma^2$ 的一个未被放大且更准确的估计。$e_i^2$ 的巨大数值使得 $SSE \\gg SSE_{(i)}$，并且这个效应通常远超过自由度上的微小差异（$n-p$ vs. $n-p-1$）。因此，$\\hat{\\sigma}^2$ 将显著大于 $\\hat{\\sigma}_{(i)}^2$。\n\n现在，我们比较两个检验统计量的量级：\n$$ |t_i| = \\frac{|e_i|}{\\hat{\\sigma}\\sqrt{1-h_{ii}}} \\quad \\text{与} \\quad |t_i^*| = \\frac{|e_i|}{\\hat{\\sigma}_{(i)}\\sqrt{1-h_{ii}}} $$\n由于分子相同，并且我们已经确定 $\\hat{\\sigma} > \\hat{\\sigma}_{(i)}$，可得出 $|t_i|$ 的分母大于 $|t_i^*|$ 的分母。因此：\n$$ |t_i|  |t_i^*| $$\n这种现象被称为**掩盖效应**（masking）。单个异常值放大了全数据方差估计，这进而减小了其自身的内学生化残差的量级，使其更不容易被标记为异常值。外学生化残差 $t_i^*$ 通过使用一个未被异常值本身污染的方差估计来避免这个问题。\n\n异常值检验的**功效**（power）指的是其正确检测出真实异常值的能力。对于给定的显著性水平，如果一个观测值的检验统计量超过一个临界值，它就会被标记。由于 $|t_i^*|$ 大于 $|t_i|$，它更有可能超过任何给定的临界值。因此，基于 $t_i^*$ 的检验在检测单个异常观测值方面具有更大的功效。\n\n在零假设（即没有异常值且误差服从正态分布）下，$t_i^*$ 服从自由度为 $n-p-1$ 的学生t分布。这为正式的假设检验提供了直接依据。\n\n现在，我们评估每个选项：\n\n**A. 当只有一个观测值异常时，$t_{i}^{*}$ 通常比 $t_{i}$ 有更大的功效，因为留一法方差估计排除了该异常观测值，因此不会被放大；而全数据方差估计被异常残差所放大，从而减小了 $t_{i}$ 的量级并导致掩盖效应。**\n该陈述与我们的推导完全一致。它正确地指出 $t_i^*$ 具有更大的功效，正确地将其归因于留一法方差估计的未被放大性质与被放大的全数据估计的对比，并正确地描述了其后果是 $t_i$ 的量级减小（掩盖效应）。此选项是**正确的**。\n\n**B. 当只有一个观测值异常时，$t_{i}$ 和 $t_{i}^{*}$ 具有相等的功效，因为两者都通过 $h_{ii}$ 对杠杆率进行了调整，因此按相同的有效方差进行缩放。**\n这是不正确的。虽然两种统计量都对杠杆率进行了调整，但它们在方差估计部分（$\\hat{\\sigma}$ vs. $\\hat{\\sigma}_{(i)}$）上存在关键差异。在存在异常值的情况下，它们不会按相同的有效方差进行缩放。因此，它们的功效不相等。此选项是**不正确的**。\n\n**C. $t_{i}$ 比 $t_{i}^{*}$ 有更大的功效，因为使用所有 $n$ 个观测值会得到比留一法估计更小的全数据方差估计，从而使 $t_{i}$ 的量级更大。**\n该陈述犯了两个错误。首先，它声称 $t_i$ 具有更大的功效，这与事实相反。其次，其推理是有缺陷的：在存在异常值的情况下，全数据方差估计 $\\hat{\\sigma}^2$ 通常比留一法估计 $\\hat{\\sigma}_{(i)}^2$ *更大*，而不是更小。这使得 $|t_i|$ 更小，而不是更大。此选项是**不正确的**。\n\n**D. $t_{i}^{*}$ 比 $t_{i}$ 的功效更低，因为移除观测值会减少自由度，从而使留一法方差估计增加的程度足以抵消任何掩盖效应。**\n该陈述错误地声称 $t_i^*$ 的功效更低。其提供的推理也是有缺陷的。虽然留一法估计的自由度确实更低（$n-p-1$ vs. $n-p$），但与移除异常值的巨大贡献所导致的残差平方和的巨大减少（$SSE_{(i)}$ vs. $SSE$）相比，这个影响是次要的。因此，留一法方差估计比全数据估计更小，而不是更大。此选项是**不正确的**。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}