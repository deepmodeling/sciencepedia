{
    "hands_on_practices": [
        {
            "introduction": "Before we dive into the methods for testing normality, it's essential to understand *why* this assumption is so important. This practice builds that foundational motivation by exploring the practical consequences of non-normality on one of statistics' most common estimators: the sample mean. By comparing the variance of the sample mean for data from a normal distribution versus a heavy-tailed one, you will quantify the profound impact that distribution shape has on estimator precision, making the need for normality diagnostics crystal clear .",
            "id": "4894228",
            "problem": "A biostatistician is comparing the behavior of the sample mean for two biomarker measurements collected in independent and identically distributed samples of size $n$: one biomarker whose measurement errors are modeled by a heavy-tailed Student's $t$ distribution with degrees of freedom $\\nu=3$, location $0$, and scale parameter $s=1$, and another biomarker modeled by a normal distribution with mean $0$ and variance $1$. Using only general properties of variance, independence, and well-established distributional facts, compute the ratio of the variance of the sample mean under the heavy-tailed Student's $t$ model to the variance of the sample mean under the normal model. Round your final answer to $4$ significant figures. In your reasoning, briefly explain how this ratio connects to the practical need for normality diagnostics such as quantile-quantile (Q-Q) plots and the Shapiro–Wilk test when assessing biomarker distributions.",
            "solution": "The problem asks us to compute the ratio of the variance of the sample mean for a sample from a location-scale Student's $t$ distribution to that of a sample from a standard normal distribution.\n\nLet the two sets of biomarker measurements be represented by two independent and identically distributed (i.i.d.) random samples of size $n$.\n\nThe first sample, $X_1, X_2, \\dots, X_n$, is drawn from a location-scale Student's $t$ distribution. Let's denote the random variable for a single observation as $X$. The parameters are given as:\n- Degrees of freedom: $\\nu = 3$\n- Location parameter: $\\mu = 0$\n- Scale parameter: $s = 1$\n\nThe second sample, $Y_1, Y_2, \\dots, Y_n$, is drawn from a normal distribution. Let's denote the random variable for a single observation as $Y$. The parameters are given as:\n- Mean: $\\mu_Y = 0$\n- Variance: $\\sigma^2_Y = 1$\n\nLet $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ be the sample mean for the Student's $t$ model.\nLet $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$ be the sample mean for the normal model.\n\nThe variance of the sample mean for an i.i.d. sample of size $n$ from a population with variance $\\sigma^2$ is given by the general formula:\n$$\n\\text{Var}(\\bar{X}) = \\frac{\\text{Var}(X)}{n}\n$$\nThis is derived from the properties of variance. Since the observations $X_i$ are independent, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(\\bar{X}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_i) = \\frac{1}{n^2} (n \\cdot \\text{Var}(X)) = \\frac{\\text{Var}(X)}{n}\n$$\nSimilarly, for the normal model, $\\text{Var}(\\bar{Y}) = \\frac{\\text{Var}(Y)}{n}$.\n\nThe ratio we need to compute is:\n$$\n\\frac{\\text{Var}(\\bar{X})}{\\text{Var}(\\bar{Y})} = \\frac{\\text{Var}(X)/n}{\\text{Var}(Y)/n} = \\frac{\\text{Var}(X)}{\\text{Var}(Y)}\n$$\nWe must now find the population variances, $\\text{Var}(X)$ and $\\text{Var}(Y)$.\n\nFor the normal model, the variance is explicitly given as $\\text{Var}(Y) = \\sigma^2_Y = 1$.\n\nFor the Student's $t$ model, a random variable $X$ that follows a location-scale Student's $t$ distribution can be expressed as $X = \\mu + s \\cdot T$, where $T$ is a random variable from the standard Student's $t$ distribution with $\\nu$ degrees of freedom. The variance of the standard Student's $t$ distribution, $\\text{Var}(T)$, is defined for $\\nu > 2$ and is given by:\n$$\n\\text{Var}(T) = \\frac{\\nu}{\\nu - 2}\n$$\nThe variance of the location-scale variable $X$ is then:\n$$\n\\text{Var}(X) = \\text{Var}(\\mu + s \\cdot T) = s^2 \\cdot \\text{Var}(T)\n$$\nIn our problem, we have $\\nu = 3$, $\\mu = 0$, and $s = 1$. Since $\\nu=3 > 2$, the variance is well-defined.\nFirst, we find the variance of the standard $t$-distribution with $\\nu=3$ degrees of freedom:\n$$\n\\text{Var}(T) = \\frac{3}{3 - 2} = \\frac{3}{1} = 3\n$$\nNow, we find the variance of our specific biomarker measurement variable $X$:\n$$\n\\text{Var}(X) = s^2 \\cdot \\text{Var}(T) = 1^2 \\cdot 3 = 3\n$$\nSo, the population variance for the heavy-tailed model is $\\text{Var}(X) = 3$.\n\nNow we can compute the required ratio:\n$$\n\\frac{\\text{Var}(\\bar{X})}{\\text{Var}(\\bar{Y})} = \\frac{\\text{Var}(X)}{\\text{Var}(Y)} = \\frac{3}{1} = 3\n$$\nThe problem requires the answer to be rounded to $4$ significant figures. The exact answer is $3$, which can be written as $3.000$.\n\nThe practical connection of this ratio to normality diagnostics is profound. The ratio of $3$ demonstrates that for a sample of any size $n$, the sample mean derived from the heavy-tailed $t(\\nu=3)$ distribution has a variance three times larger than that of a sample mean from a standard normal distribution. This is despite both parent distributions being symmetric and centered at $0$. The higher variance is a direct consequence of the \"heavy tails\" of the Student's $t$ distribution, which allows for more frequent extreme observations (outliers) compared to the normal distribution.\n\nIf a researcher assumes normality when the underlying data are in fact heavy-tailed, they will be using a model that dramatically underestimates the true variability of their sample mean. This leads to invalid statistical inferences: confidence intervals for the mean will be erroneously narrow, and hypothesis tests will have an inflated Type I error rate, leading to false-positive findings.\n\nThis is precisely why diagnostics for assessing normality are a critical step in data analysis.\n- **Quantile-Quantile (Q-Q) plots** are a graphical tool to check for normality. For data from a heavy-tailed distribution like our $t(\\nu=3)$ case, the points on the Q-Q plot will deviate systematically from the straight reference line, particularly at the ends, visually revealing the presence of heavier-than-normal tails.\n- **The Shapiro–Wilk test** is a formal hypothesis test where the null hypothesis is that the data are drawn from a normally distributed population. For data from a heavy-tailed distribution, this test would likely yield a low $p$-value, leading to a rejection of the normality assumption.\nBy using these tools, a biostatistician can identify departures from normality, such as the heavy-tailedness explored here. This detection allows them to switch to more appropriate statistical methods, such as robust statistics or non-parametric tests, that are less sensitive to outliers, thereby ensuring the validity and reliability of their scientific conclusions. The calculated ratio of $3$ quantifies the high cost of failing to do so.",
            "answer": "$$\n\\boxed{3.000}\n$$"
        },
        {
            "introduction": "The Shapiro-Wilk test is a powerful and widely used formal test for normality, but it is not without its limitations. This exercise challenges you to investigate the test's robustness by analyzing its behavior in the presence of a single extreme outlier, a common issue in biomarker data. Through a focused theoretical derivation, you will uncover how the test statistic, $W$, responds to such an outlier, providing critical insight into why relying solely on a single test can be misleading and why robust methods are a necessary part of a biostatistician's toolkit .",
            "id": "4894182",
            "problem": "A biostatistics researcher is assessing normality of a univariate biomarker using the Shapiro–Wilk test. Let $x_{(1)} \\leq x_{(2)} \\leq \\dots \\leq x_{(n)}$ denote the ordered sample of size $n \\geq 5$. The Shapiro–Wilk statistic $W$ is based on a linear contrast of the order statistics and is defined by the well-tested construction:\n$$\nW \\;=\\; \\frac{\\left(\\sum_{i=1}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right)\\right)^{2}}{\\sum_{i=1}^{n}\\left(x_{i} - \\bar{x}\\right)^{2}},\n$$\nwhere $m = \\lfloor n/2 \\rfloor$, the weights $a_{i}$ depend only on $n$ through the expected values and covariance structure of the order statistics of a standard normal distribution, and $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}$ is the sample mean. Assume $a_{1} \\geq a_{2} \\geq \\dots \\geq a_{m} > 0$.\n\nSuppose a single extreme observation occurs in the upper tail, replacing $x_{(n)}$ by a value $t$ that increases without bound while all other observations remain fixed and finite. Define $W(t)$ as the Shapiro–Wilk statistic computed on the sample with $x_{(n)}=t$.\n\nUsing only the fundamental definitions above, derive the closed-form analytic expression for the limit\n$$\n\\lim_{t \\to \\infty} W(t)\n$$\nin terms of $n$ and $a_{1}$, where $a_{1}$ is the largest Shapiro–Wilk weight in the contrast. Express your final answer as a single analytic expression. No rounding is required.\n\nBriefly propose at least two robust alternatives for assessing normality that mitigate the influence of a single extreme observation, but note that these proposals do not affect the required final expression for the limit.",
            "solution": "The problem requires us to find the limit of the Shapiro–Wilk statistic, $W(t)$, as a single extreme observation $t$ in the upper tail of a sample goes to infinity. The sample consists of $n-1$ fixed, finite observations, which we can denote as $\\{y_1, y_2, \\dots, y_{n-1}\\}$, and the variable observation $t$. As $t \\to \\infty$, it will eventually exceed all fixed observations, so we can set $x_{(n)} = t$. The remaining order statistics, $x_{(1)}, \\ldots, x_{(n-1)}$, are the sorted values of the fixed set $\\{y_1, y_2, \\dots, y_{n-1}\\}$ and are thus constant with respect to $t$.\n\nThe Shapiro–Wilk statistic is given by:\n$$ W(t) \\;=\\; \\frac{\\left(\\sum_{i=1}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right)\\right)^{2}}{\\sum_{i=1}^{n}\\left(x_{i} - \\bar{x}\\right)^{2}} $$\nwhere $m = \\lfloor n/2 \\rfloor$ and the sample under consideration is $\\{y_1, \\dots, y_{n-1}, t\\}$. We will analyze the asymptotic behavior of the numerator and the denominator separately as $t \\to \\infty$.\n\n**Analysis of the Numerator**\n\nLet $N(t)$ denote the numerator of $W(t)$. The term inside the square is the linear contrast, which we denote by $S(t)$:\n$$ S(t) = \\sum_{i=1}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right) $$\nWe can separate the term involving $x_{(n)}$ from the sum. This occurs when $i=1$, as $x_{(n+1-1)} = x_{(n)}$.\n$$ S(t) = a_{1}\\left(x_{(n)} - x_{(1)}\\right) + \\sum_{i=2}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right) $$\nSubstituting $x_{(n)} = t$, we have:\n$$ S(t) = a_{1}\\left(t - x_{(1)}\\right) + \\sum_{i=2}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right) $$\nThe order statistics $x_{(1)}, x_{(2)}, \\dots, x_{(n-1)}$ are fixed values, independent of $t$. The weights $a_i$ are also constant for a given $n$. Therefore, the expression can be written as a linear function of $t$:\n$$ S(t) = a_{1}t + \\left( -a_{1}x_{(1)} + \\sum_{i=2}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right) \\right) $$\nLet $C_{\\text{num}}$ be the constant term:\n$$ C_{\\text{num}} = -a_{1}x_{(1)} + \\sum_{i=2}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right) $$\nThus, $S(t) = a_1t + C_{\\text{num}}$. The numerator is $N(t) = (S(t))^2 = (a_1t + C_{\\text{num}})^2$.\nAs $t \\to \\infty$, $N(t)$ is a quadratic polynomial in $t$. The leading term determines its asymptotic behavior:\n$$ N(t) = a_1^2 t^2 + 2a_1 C_{\\text{num}} t + C_{\\text{num}}^2 $$\nThe dominant term for large $t$ is $a_1^2 t^2$.\n\n**Analysis of the Denominator**\n\nLet $D(t)$ denote the denominator, which is the sum of squared deviations from the sample mean $\\bar{x}$.\n$$ D(t) = \\sum_{j=1}^{n}\\left(x_{j} - \\bar{x}\\right)^{2} $$\nThe sample mean, $\\bar{x}(t)$, depends on $t$:\n$$ \\bar{x}(t) = \\frac{1}{n} \\left( \\sum_{j=1}^{n-1} y_j + t \\right) $$\nLet $S_{n-1} = \\sum_{j=1}^{n-1} y_j$. Since the $y_j$ values are fixed, $S_{n-1}$ is a constant.\n$$ \\bar{x}(t) = \\frac{S_{n-1} + t}{n} $$\nFor analytical convenience, we use the identity for the sum of squares:\n$$ D(t) = \\sum_{j=1}^{n} x_j^2 - n(\\bar{x}(t))^2 $$\nThe sum of squares of the observations is:\n$$ \\sum_{j=1}^{n} x_j^2 = \\sum_{j=1}^{n-1} y_j^2 + t^2 $$\nLet $S_{sq, n-1} = \\sum_{j=1}^{n-1} y_j^2$, which is also a constant.\nSo, $\\sum_{j=1}^{n} x_j^2 = S_{sq, n-1} + t^2$.\nNow, substitute the expressions for $\\sum x_j^2$ and $\\bar{x}(t)$ into the formula for $D(t)$:\n$$ D(t) = (S_{sq, n-1} + t^2) - n\\left(\\frac{S_{n-1} + t}{n}\\right)^2 $$\n$$ D(t) = (S_{sq, n-1} + t^2) - \\frac{1}{n}(S_{n-1} + t)^2 $$\n$$ D(t) = S_{sq, n-1} + t^2 - \\frac{1}{n}(S_{n-1}^2 + 2S_{n-1}t + t^2) $$\nCollecting terms by powers of $t$:\n$$ D(t) = \\left(1 - \\frac{1}{n}\\right)t^2 - \\frac{2S_{n-1}}{n}t + \\left(S_{sq, n-1} - \\frac{S_{n-1}^2}{n}\\right) $$\n$$ D(t) = \\frac{n-1}{n}t^2 - \\frac{2S_{n-1}}{n}t + C_{\\text{den}} $$\nwhere $C_{\\text{den}} = S_{sq, n-1} - \\frac{S_{n-1}^2}{n}$ is a constant. As $t \\to \\infty$, $D(t)$ is also a quadratic polynomial in $t$. Its asymptotic behavior is governed by the leading term $\\frac{n-1}{n}t^2$.\n\n**Calculation of the Limit**\n\nWe now compute the limit of $W(t) = N(t)/D(t)$ as $t \\to \\infty$. This is the ratio of two quadratic polynomials in $t$. The limit is the ratio of their leading coefficients.\n$$ \\lim_{t \\to \\infty} W(t) = \\lim_{t \\to \\infty} \\frac{a_1^2 t^2 + 2a_1 C_{\\text{num}} t + C_{\\text{num}}^2}{\\frac{n-1}{n}t^2 - \\frac{2S_{n-1}}{n}t + C_{\\text{den}}} $$\nTo formalize this, we divide the numerator and denominator by $t^2$, the highest power of $t$:\n$$ \\lim_{t \\to \\infty} W(t) = \\lim_{t \\to \\infty} \\frac{a_1^2 + \\frac{2a_1 C_{\\text{num}}}{t} + \\frac{C_{\\text{num}}^2}{t^2}}{\\frac{n-1}{n} - \\frac{2S_{n-1}}{nt} + \\frac{C_{\\text{den}}}{t^2}} $$\nAs $t \\to \\infty$, all terms with $t$ or $t^2$ in the denominator approach zero:\n$$ \\lim_{t \\to \\infty} W(t) = \\frac{a_1^2 + 0 + 0}{\\frac{n-1}{n} - 0 + 0} = \\frac{a_1^2}{\\frac{n-1}{n}} $$\nSimplifying this expression gives the final result:\n$$ \\lim_{t \\to \\infty} W(t) = \\frac{n a_1^2}{n-1} $$\nThis result depends only on the sample size $n$ and the largest weight coefficient $a_1$, as required.\n\n**Robust Alternatives for Assessing Normality**\n\nThe calculation above demonstrates that a single extreme observation causes the Shapiro-Wilk statistic to converge to a specific non-zero, non-unity value, typically leading to a rejection of the null hypothesis of normality. This highlights the test's lack of robustness. To mitigate the influence of such outliers, one can employ robust methods for assessing normality. Two such alternatives are:\n\n$1$. **Quantile-Quantile (Q-Q) Plot with Robust Scaling**: Standard Q-Q plots are susceptible to distortion by outliers, which can affect the visual scaling of the plot and the interpretation of the pattern for the bulk of the data. A more robust approach involves standardizing the data not with the sample mean and standard deviation, but with robust estimators of location and scale, such as the median and the Median Absolute Deviation (MAD). The transformed data points $z_i = (x_i - \\text{median}(X)) / \\text{MAD}(X)$ are then plotted against the quantiles of a standard normal distribution. This method contains the influence of the outlier and provides a clearer picture of the normality of the central data.\n\n$2$. **Normality Testing on a Trimmed Sample**: Another strategy is to apply a normality test, such as the Shapiro-Wilk test itself, to a trimmed version of the data. For example, a $5\\%$ symmetrically trimmed sample would involve removing the lowest $5\\%$ and highest $5\\%$ of the observations before computing the test statistic. This explicitly removes extreme values, allowing the test to assess the distributional properties of the core, or bulk, of the data, providing a more robust test of normality for the underlying population, assuming the outliers are contaminants.",
            "answer": "$$\\boxed{\\frac{n a_{1}^{2}}{n-1}}$$"
        },
        {
            "introduction": "Identifying non-normality is only the first step; often, we must then transform the data to meet the assumptions of subsequent analyses. This hands-on coding practice guides you through an end-to-end workflow for both transforming and assessing data, centering on the powerful Box-Cox transformation. You will learn to programmatically find the optimal transformation parameter, $\\hat{\\lambda}$, and then validate the result using both a quantitative measure of Q-Q plot linearity and the formal Shapiro-Wilk test, synthesizing multiple techniques into a cohesive practical skill .",
            "id": "4894199",
            "problem": "You are given four strictly positive biomarker datasets. Your task is to implement an end-to-end normality assessment pipeline that, for each dataset, computes the optimal Box–Cox transformation parameter $\\hat{\\lambda}$ by maximizing the normal profile log-likelihood, transforms the data with the estimated $\\hat{\\lambda}$, and then assesses normality using a quantile–quantile (Q-Q) correlation summary and the Shapiro–Wilk (SW) test.\n\nBase definitions and assumptions:\n- For strictly positive measurements $x_1,\\dots,x_n$, the Box–Cox transformation is defined by\n$$\nT_{\\lambda}(x)=\n\\begin{cases}\n\\dfrac{x^{\\lambda}-1}{\\lambda}, & \\lambda \\neq 0,\\\\\n\\log(x), & \\lambda = 0.\n\\end{cases}\n$$\n- Assume there exists a $\\lambda$ such that $Z_i = T_{\\lambda}(x_i)$ are independent and identically distributed as a Normal distribution with mean $\\mu$ and variance $\\sigma^2$.\n- The optimal $\\hat{\\lambda}$ is the maximizer of the normal profile log-likelihood in $\\lambda$ under this model.\n\nQuantifying the Q–Q linearity without plotting:\n- Let $z_{(1)} \\le \\dots \\le z_{(n)}$ denote the order statistics of the transformed sample $z_i = T_{\\hat{\\lambda}}(x_i)$.\n- Define Blom plotting positions $p_i = \\dfrac{i - 0.375}{n + 0.25}$ for $i=1,\\dots,n$, and let $q_i = \\Phi^{-1}(p_i)$ where $\\Phi^{-1}$ is the inverse cumulative distribution function of the standard Normal distribution.\n- Compute the Pearson correlation between the vectors $(z_{(1)},\\dots,z_{(n)})$ and $(q_1,\\dots,q_n)$; denote this correlation by $r_{\\text{QQ}}$. Values of $r_{\\text{QQ}}$ closer to $1$ indicate stronger linearity in the Q–Q relationship.\n\nNormality test:\n- Apply the Shapiro–Wilk test to the transformed data $\\{z_i\\}$ and record the test’s $p$-value. Use significance level $\\alpha = 0.05$ and declare “normal” if the $p$-value is greater than or equal to $\\alpha$.\n\nYour program must:\n- For each dataset, compute $\\hat{\\lambda}$ by maximizing the normal profile log-likelihood in $\\lambda$ over a bounded interval, transform the data with $T_{\\hat{\\lambda}}$, compute $r_{\\text{QQ}}$, compute the Shapiro–Wilk $p$-value, and finally report a boolean normality decision using $\\alpha = 0.05$.\n- Round each real-valued output ($\\hat{\\lambda}$, $p$-value, $r_{\\text{QQ}}$) to $4$ decimal places.\n\nTest suite (datasets):\nAll datasets use $n = 30$ points with $i \\in \\{1,\\dots,30\\}$ and $p_i^{(u)} = \\dfrac{i - 0.5}{n}$. Let $\\Phi^{-1}$ denote the inverse cumulative distribution function of the standard Normal distribution, and let $G^{-1}_{k,\\theta}$ denote the inverse cumulative distribution function of the Gamma distribution with shape $k$ and scale $\\theta$.\n\n- Dataset $\\mathcal{D}_1$ (log-normal-like): $x_i^{(1)} = \\exp\\!\\big(2.0 + 0.5 \\cdot \\Phi^{-1}(p_i^{(u)})\\big)$.\n- Dataset $\\mathcal{D}_2$ (approximately Normal but positive): $x_i^{(2)} = 10.0 + 1.2 \\cdot \\Phi^{-1}(p_i^{(u)})$.\n- Dataset $\\mathcal{D}_3$ (Gamma-like skew): $x_i^{(3)} = G^{-1}_{2.0,\\,3.0}(p_i^{(u)})$.\n- Dataset $\\mathcal{D}_4$ (small-magnitude positive): $x_i^{(4)} = 0.02 + \\exp\\!\\big(-2.0 + 0.5 \\cdot \\Phi^{-1}(p_i^{(u)})\\big)$.\n\nOutput specification:\n- For each dataset $\\mathcal{D}_j$ with $j \\in \\{1,2,3,4\\}$, return a list $[\\hat{\\lambda}_j, p\\_j, r_{\\text{QQ},j}, \\text{normal}_j]$ where $\\hat{\\lambda}_j$ is the estimated Box–Cox parameter, $p\\_j$ is the Shapiro–Wilk $p$-value on the transformed data, $r_{\\text{QQ},j}$ is the Q–Q correlation, and $\\text{normal}_j$ is a boolean that is true if $p\\_j \\ge 0.05$ and false otherwise.\n- Round $\\hat{\\lambda}_j$, $p\\_j$, and $r_{\\text{QQ},j}$ to $4$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The line must be of the form:\n`[[\\hat{\\lambda}_1,p_1,r_{\\text{QQ},1},\\text{normal}_1],[\\hat{\\lambda}_2,p_2,r_{\\text{QQ},2},\\text{normal}_2],[\\hat{\\lambda}_3,p_3,r_{\\text{QQ},3},\\text{normal}_3],[\\hat{\\lambda}_4,p_4,r_{\\text{QQ},4},\\text{normal}_4]]`.",
            "solution": "### 1. Theoretical Framework\n\n#### 1.1. Box-Cox Transformation and Normality Assumption\nGiven a set of strictly positive data points $x_1, x_2, \\dots, x_n$, the Box-Cox transformation is a power transformation parameterized by $\\lambda$. It is defined as:\n$$\nT_{\\lambda}(x) =\n\\begin{cases}\n\\frac{x^{\\lambda}-1}{\\lambda}, & \\text{if } \\lambda \\neq 0, \\\\\n\\log(x), & \\text{if } \\lambda = 0.\n\\end{cases}\n$$\nThe case for $\\lambda=0$ is the limit of the expression for $\\lambda \\neq 0$ as $\\lambda \\to 0$. The core assumption is that for some value of $\\lambda$, the transformed variables $Z_i = T_{\\lambda}(x_i)$ are independent and identically distributed (i.i.d.) samples from a normal distribution, $N(\\mu, \\sigma^2)$.\n\n#### 1.2. Estimation of the Optimal Parameter $\\hat{\\lambda}$\nThe optimal parameter $\\hat{\\lambda}$ is determined by the method of maximum likelihood. The probability density function of the original data $x_i$ can be found from the density of the transformed data $z_i$ using the change of variables formula. The log-likelihood function for the original observations $\\mathbf{x} = (x_1, \\dots, x_n)$ given the parameters $\\mu$, $\\sigma^2$, and $\\lambda$ is:\n$$\n\\ell(\\mu, \\sigma^2, \\lambda | \\mathbf{x}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (T_\\lambda(x_i) - \\mu)^2 + (\\lambda-1)\\sum_{i=1}^{n} \\log(x_i)\n$$\nFor a fixed $\\lambda$, the maximum likelihood estimators (MLEs) for $\\mu$ and $\\sigma^2$ are the sample mean and variance of the transformed data $z_i(\\lambda) = T_\\lambda(x_i)$:\n$$\n\\hat{\\mu}(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} z_i(\\lambda) \\quad \\text{and} \\quad \\hat{\\sigma}^2(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} (z_i(\\lambda) - \\hat{\\mu}(\\lambda))^2\n$$\nSubstituting these back into the log-likelihood function yields the profile log-likelihood for $\\lambda$. Maximizing this is equivalent to maximizing the simplified function:\n$$\nL_{\\text{profile}}(\\lambda) = -\\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) + (\\lambda-1)\\sum_{i=1}^{n} \\log(x_i)\n$$\nThis maximization will be performed numerically over a bounded interval, for instance $\\lambda \\in [-5, 5]$.\n\n#### 1.3. Assessment of Normality of Transformed Data\n\nOnce $\\hat{\\lambda}$ is found, the data are transformed as $z_i = T_{\\hat{\\lambda}}(x_i)$. The normality of this new sample $\\{z_i\\}$ is then assessed.\n\n**Quantile-Quantile (Q-Q) Correlation, $r_{\\text{QQ}}$:**\nThis metric quantifies the linearity of a Q-Q plot.\n1.  Compute the order statistics of the transformed data: $z_{(1)} \\le z_{(2)} \\le \\dots \\le z_{(n)}$.\n2.  Calculate the Blom plotting positions: $p_i = \\frac{i - 0.375}{n + 0.25}$ for $i=1, \\dots, n$.\n3.  Determine the corresponding theoretical quantiles from the standard normal distribution: $q_i = \\Phi^{-1}(p_i)$.\n4.  Compute the Pearson correlation coefficient between the vector of ordered statistics $\\mathbf{z}_{\\text{ord}} = (z_{(1)}, \\dots, z_{(n)})$ and the vector of theoretical quantiles $\\mathbf{q} = (q_1, \\dots, q_n)$. This correlation is denoted $r_{\\text{QQ}}$.\n\n**Shapiro-Wilk (SW) Test:**\nThe Shapiro-Wilk test is a formal hypothesis test for normality. The null hypothesis ($H_0$) is that the data sample comes from a normally distributed population.\n-   If the $p$-value is less than $\\alpha=0.05$, $H_0$ is rejected.\n-   If the $p$-value is greater than or equal to $\\alpha=0.05$, there is insufficient evidence to reject $H_0$.\n\n### 2. Computational Procedure\n\nThe following steps are performed for each of the four datasets:\n\n1.  **Generate Dataset**: The data points for each dataset $\\mathcal{D}_j$ are generated according to the specified formulas.\n2.  **Estimate $\\hat{\\lambda}$**: The negative of the profile log-likelihood, $-L_{\\text{profile}}(\\lambda)$, is defined as an objective function. A numerical optimization routine is used to find the value $\\hat{\\lambda}$ that minimizes this function.\n3.  **Transform Data**: The original dataset $x_i$ is transformed into $z_i = T_{\\hat{\\lambda}}(x_i)$ using the estimated $\\hat{\\lambda}$.\n4.  **Calculate $r_{\\text{QQ}}$**: The Q-Q correlation is computed for the transformed data $\\{z_i\\}$.\n5.  **Perform Shapiro-Wilk Test**: The Shapiro-Wilk test is applied to the transformed data $\\{z_i\\}$ to obtain a $p$-value.\n6.  **Report Results**: A boolean decision on normality is made based on whether the $p$-value is $\\ge 0.05$. The final results for the dataset—$\\hat{\\lambda}$, the SW $p$-value, $r_{\\text{QQ}}$, and the normality decision—are collected and rounded.\n7.  **Final Output**: The collected results for all four datasets are assembled into a list of lists and printed in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats, optimize\n\ndef solve():\n    \"\"\"\n    Main function to perform normality assessment on four datasets.\n    \"\"\"\n\n    def generate_datasets():\n        \"\"\"Generates the four datasets as specified in the problem.\"\"\"\n        n = 30\n        i = np.arange(1, n + 1)\n        p_u = (i - 0.5) / n\n        norm_quantiles = stats.norm.ppf(p_u)\n\n        # Dataset D1 (log-normal-like)\n        d1 = np.exp(2.0 + 0.5 * norm_quantiles)\n\n        # Dataset D2 (approximately Normal but positive)\n        d2 = 10.0 + 1.2 * norm_quantiles\n\n        # Dataset D3 (Gamma-like skew)\n        gamma_quantiles = stats.gamma.ppf(p_u, a=2.0, scale=3.0)\n        d3 = gamma_quantiles\n\n        # Dataset D4 (small-magnitude positive)\n        d4 = 0.02 + np.exp(-2.0 + 0.5 * norm_quantiles)\n\n        return [d1, d2, d3, d4]\n\n    def boxcox_transform(x, lam):\n        \"\"\"Applies the Box-Cox transformation.\"\"\"\n        if lam == 0:\n            return np.log(x)\n        else:\n            return (x**lam - 1) / lam\n\n    def profile_log_likelihood(lam, x):\n        \"\"\"\n        Computes the profile log-likelihood for a given lambda.\n        Note: The function returns the *negative* for minimization purposes.\n        \"\"\"\n        n = len(x)\n        z = boxcox_transform(x, lam)\n        # Using n-ddof=0 for MLE of variance (division by n)\n        log_lik = -n / 2 * np.log(np.var(z, ddof=0)) + (lam - 1) * np.sum(np.log(x))\n        return -log_lik\n\n    def analyze_dataset(x):\n        \"\"\"\n        Performs the full analysis pipeline for a single dataset.\n        \"\"\"\n        # 1. Compute optimal lambda\n        # We use a bounded search for robustness, as is common practice.\n        res = optimize.minimize_scalar(\n            profile_log_likelihood, \n            args=(x,), \n            bounds=(-5, 5), \n            method='bounded'\n        )\n        lambda_hat = res.x\n\n        # 2. Transform the data\n        z_transformed = boxcox_transform(x, lambda_hat)\n\n        # 3. Compute Q-Q correlation (r_QQ)\n        n = len(z_transformed)\n        z_ordered = np.sort(z_transformed)\n        i = np.arange(1, n + 1)\n        p_blom = (i - 0.375) / (n + 0.25)\n        q_theoretical = stats.norm.ppf(p_blom)\n        r_qq = np.corrcoef(z_ordered, q_theoretical)[0, 1]\n\n        # 4. Compute Shapiro-Wilk test p-value\n        _, p_value = stats.shapiro(z_transformed)\n\n        # 5. Make normality decision\n        alpha = 0.05\n        is_normal = p_value >= alpha\n\n        # 6. Format results\n        return [\n            round(lambda_hat, 4),\n            round(p_value, 4),\n            round(r_qq, 4),\n            is_normal\n        ]\n\n    datasets = generate_datasets()\n    all_results = [analyze_dataset(d) for d in datasets]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\".replace(\" \", \"\"))\n\nsolve()\n\n```"
        }
    ]
}