{
    "hands_on_practices": [
        {
            "introduction": "在生物统计学中，我们常常需要检验数据是否服从正态分布。然而，经典的Kolmogorov-Smirnov检验有一个局限：它假设总体的真实均值和方差是已知的，但这在现实中几乎不可能。本练习  介绍了一项关键的修正方法——Lilliefors检验。你将学习当均值和标准差是从样本中估计出来时，如何通过蒙特卡洛模拟来生成正确的$p$值，这是进行稳健统计检验的一项基本技能。",
            "id": "4894176",
            "problem": "你需要编写一个完整的、自包含的程序，通过计算单样本柯尔莫哥洛夫-斯米尔诺夫（KS）统计量，并通过蒙特卡洛模拟近似Lilliefors p值，来评估位置和尺度参数未知的样本对正态分布的拟合优度。正态性检验是生物统计学中的一个核心诊断方法，当参数未知时，近似方法必须考虑到参数估计这一步骤。\n\n请使用以下基础来设计您的方法：根据观测样本构建的经验累积分布函数、标准正态分布的累积分布函数，以及累积分布函数之间逐点差异的上确界概念。当参数未知时，将它们视为讨厌参数，并使用从样本中导出的一致估计量。蒙特卡洛方法必须在正态性原假设下，复制确切的估计过程，以近似这种复合情况下KS统计量的抽样分布。\n\n每个测试用例需要实现的任务：\n- 从输入样本中，使用除数为 $(n-1)$ 的无偏样本标准差来估计未知的均值和标准差，其中 $n$ 是样本大小。\n- 使用估计的参数对数据进行标准化，然后通过计算标准化数据的经验累积分布函数与标准正态分布的累积分布函数之间绝对差的上确界，来计算用于正态性检验的单样本柯尔莫哥洛夫-斯米尔诺夫统计量。\n- 在原假设下通过蒙特卡洛模拟来近似Lilliefors p值：从标准正态分布中抽取 $M$ 个独立的样本，每个样本大小为 $n$；对每个模拟样本使用相同的程序重新估计均值和标准差；以与观测数据相同的方式为每个模拟样本重新计算KS统计量；最后，计算模拟的KS统计量中大于或等于观测KS统计量的比例。这个比例即为十进制表示的蒙特卡洛p值。\n\n模拟可复现性要求：\n- 对所有模拟使用固定的伪随机数生成器种子 $123456$，以确保每次运行的输出都是确定性的。\n\n最终输出要求：\n- 对于每个测试用-例，输出一个列表 $[D, p]$，其中 $D$ 是KS统计量，$p$ 是近似的Lilliefors p值，两者都四舍五入到六位小数，并以十进制形式表示（不带百分号）。\n- 您的程序应生成单行输出，包含所有测试用例的结果，格式为一个由逗号分隔的列表，并用方括号括起来。预期的最终输出格式为 $[[D_1,p_1],[D_2,p_2],[D_3,p_3]]$，其中每个 $D_i$ 和 $p_i$ 都四舍五入到六位小数。\n\n测试套件：\n- 案例A（中等样本，近似正态）：样本 $x_A$，其中 $n=30$，数据为 $[8.6, 9.8, 12.1, 10.5, 7.9, 11.3, 9.2, 10.1, 12.5, 8.8, 9.9, 10.4, 11.0, 8.3, 10.9, 11.7, 9.0, 10.2, 12.0, 7.5, 8.1, 9.7, 10.8, 11.5, 9.4, 10.0, 12.3, 8.7, 9.1, 10.6]$，$M=4000$。\n- 案例B（偏态样本，非正态）：样本 $x_B$，其中 $n=25$，数据为 $[5.1, 5.3, 5.4, 5.6, 5.7, 6.0, 6.2, 6.5, 7.0, 7.2, 7.6, 8.4, 5.2, 5.8, 5.9, 6.1, 6.3, 6.9, 7.5, 8.1, 9.0, 9.5, 10.8, 11.3, 12.7]$，$M=4000$。\n- 案例C（小样本，临界情况）：样本 $x_C$，其中 $n=8$，数据为 $[-0.2, 0.1, -1.1, 0.7, 1.0, -0.5, 0.3, -0.8]$，$M=4000$。\n\n您的程序必须严格按照 $[[D_1,p_1],[D_2,p_2],[D_3,p_3]]$ 的格式生成一行输出，其中每个浮点数都四舍五入到六位小数，使用指定的种子，且无其他输出。此问题不涉及任何物理单位或角度；所有值都是无单位的，p值必须以十进制表示。",
            "solution": "该问题要求实现Lilliefors正态性检验，这是柯尔莫哥洛夫-斯米尔诺夫（KS）拟合优度检验的一种特殊应用，其中假设的正态分布的参数（均值和标准差）是未知的，必须从数据中估计。检验统计量的p值将使用蒙特卡洛模拟进行近似，因为参数的估计会改变该统计量的原分布。\n\n验证过程确认了该问题在科学上是合理的、适定的、客观的，并包含获得唯一且可验证解所需的所有信息。它准确地描述了生物统计学及相关领域的标准程序。\n\n解决方案首先定义检验统计量，然后概述用于p值近似的蒙特卡洛方法。\n\n设观测样本表示为 $X = \\{x_1, x_2, \\ldots, x_n\\}$。原假设 $H_0$ 假定该样本来自一个正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$，其中均值 $\\mu$ 和标准差 $\\sigma$ 是未知的。\n\n第一步是从数据中估计这些讨厌参数。样本均值 $\\bar{x}$ 作为 $\\mu$ 的估计量，无偏样本标准差 $s$ 作为 $\\sigma$ 的估计量。它们的计算公式如下：\n$$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i $$\n$$ s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} $$\n使用除数 $n-1$ 提供了对总体方差的无偏估计。\n\n接下来，使用这些估计值对观测数据进行标准化，以产生一组新的值 $Z = \\{z_1, z_2, \\ldots, z_n\\}$：\n$$ z_i = \\frac{x_i - \\bar{x}}{s} $$\n在原假设下，如果 $\\bar{x}$ 和 $s$ 是真实的总体参数 $\\mu$ 和 $\\sigma$，则标准化数据 $Z$ 将服从标准正态分布 $\\mathcal{N}(0, 1)$。\n\nLilliefors检验统计量 $D$ 是对该标准化数据计算的单样本KS统计量。它被定义为标准化样本的经验累积分布函数（ECDF），记为 $F_n(z)$，与标准正态分布的累积分布函数（CDF），记为 $\\Phi(z)$，之间绝对差的上确界。\n$$ D = \\sup_{z} |F_n(z) - \\Phi(z)| $$\n为了计算 $D$，我们首先对标准化数据进行排序，得到顺序统计量 $z_{(1)} \\le z_{(2)} \\le \\ldots \\le z_{(n)}$。ECDF $F_n(z)$ 是一个阶梯函数，在每个 $z_{(i)}$ 处增加 $1/n$。差值 $|F_n(z) - \\Phi(z)|$ 的上确界将出现在某个观测数据点上。因此，可以使用以下公式高效地计算 $D$ 的值，该公式在ECDF的每一步之前和之时评估差值：\n$$ D = \\max_{i=1, \\ldots, n} \\left( \\max \\left( \\frac{i}{n} - \\Phi(z_{(i)}), \\Phi(z_{(i)}) - \\frac{i-1}{n} \\right) \\right) $$\n\n由于参数 $\\mu$ 和 $\\sigma$ 是从数据中估计的，因此在 $H_0$ 下统计量 $D$ 的分布与标准KS分布不同。数据经过其自身的样本矩进行中心化和缩放后，往往比来自标准正态分布的真实随机样本更贴合正态分布。因此，Lilliefors检验的临界值小于标准KS检验的临界值。我们通过蒙特卡洛模拟来近似正确的原分布和相应的p值。\n\n蒙特卡洛程序如下：\n1.  计算原始观测数据的检验统计量，记为 $D_{obs}$。\n2.  进行大量的重复，次数为 $M$（此处 $M=4000$），执行以下步骤来模拟原假设：\n    a. 从标准正态分布 $\\mathcal{N}(0, 1)$ 中生成一个大小为 $n$ 的合成样本 $\\{y_1, y_2, \\ldots, y_n\\}$。\n    b. 对此合成样本执行与观测数据完全相同的估计和标准化程序。计算其样本均值 $\\bar{y}$ 和样本标准差 $s_y$，然后将其标准化得到 $\\{w_1, w_2, \\ldots, w_n\\}$，其中 $w_j = (y_j - \\bar{y})/s_y$。\n    c. 为该合成样本 $\\{w_j\\}$ 计算Lilliefors统计量 $D_{sim}$。\n3.  此过程产生一组 $M$ 个模拟统计量 $\\{D_{sim,1}, D_{sim,2}, \\ldots, D_{sim,M}\\}$。该集合可作为大小为 $n$ 的样本的统计量 $D$ 的原分布的经验近似。\n4.  p值随后被估计为模拟统计量中大于或等于观测统计量的比例：\n    $$ p \\approx \\frac{\\sum_{j=1}^{M} \\mathbb{I}(D_{sim,j} \\ge D_{obs})}{M} $$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数。伪随机数生成器的固定种子确保了此模拟的可复现性。\n\n实现将遵循这一逻辑。将设计一个辅助函数来为任何给定的数据样本计算 $D$ 统计量。主程序将遍历所提供的测试用例。对于每个案例，它将计算 $D_{obs}$，然后运行蒙特卡洛循环以生成原分布并计算p值。每个案例的最终结果 $[D_{obs}, p]$ 将四舍五入到六位小数，并格式化为指定的输出字符串。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the Lilliefors test statistic for normality and its p-value via\n    Monte Carlo simulation for a series of test cases.\n    \"\"\"\n\n    def lilliefors_statistic(data):\n        \"\"\"\n        Calculates the Lilliefors statistic for a given data sample.\n\n        This involves:\n        1. Estimating the mean and standard deviation from the data.\n        2. Standardizing the data using these estimates.\n        3. Computing the Kolmogorov-Smirnov statistic against a standard normal distribution.\n\n        Args:\n            data (np.ndarray): A 1D array of sample data.\n\n        Returns:\n            float: The computed Lilliefors test statistic D.\n        \"\"\"\n        n = len(data)\n        if n == 0:\n            return 0.0\n\n        # Estimate parameters: sample mean and unbiased sample standard deviation\n        mu_hat = np.mean(data)\n        sigma_hat = np.std(data, ddof=1)\n\n        # Handle degenerate case of zero standard deviation\n        if sigma_hat == 0:\n            return 1.0\n\n        # Standardize the data and sort it\n        sorted_data = np.sort(data)\n        z_scores = (sorted_data - mu_hat) / sigma_hat\n\n        # Calculate the CDF of the standard normal distribution at each z-score\n        cdf_values = norm.cdf(z_scores)\n\n        # The ECDF steps are at i/n\n        i = np.arange(1, n + 1)\n\n        # Compute the statistic D = max(D+, D-)\n        # D+ = max_i (i/n - Phi(z_i))\n        # D- = max_i (Phi(z_i) - (i-1)/n)\n        d_plus = np.max(i / n - cdf_values)\n        d_minus = np.max(cdf_values - (i - 1) / n)\n\n        return np.max([d_plus, d_minus])\n\n    # Simulation reproducibility requirement\n    seed = 123456\n    rng = np.random.default_rng(seed)\n\n    # Test suite\n    test_cases = [\n        {\n            \"data\": np.array([8.6, 9.8, 12.1, 10.5, 7.9, 11.3, 9.2, 10.1, 12.5, 8.8, 9.9, 10.4, 11.0, 8.3, 10.9, 11.7, 9.0, 10.2, 12.0, 7.5, 8.1, 9.7, 10.8, 11.5, 9.4, 10.0, 12.3, 8.7, 9.1, 10.6]),\n            \"M\": 4000\n        },\n        {\n            \"data\": np.array([5.1, 5.3, 5.4, 5.6, 5.7, 6.0, 6.2, 6.5, 7.0, 7.2, 7.6, 8.4, 5.2, 5.8, 5.9, 6.1, 6.3, 6.9, 7.5, 8.1, 9.0, 9.5, 10.8, 11.3, 12.7]),\n            \"M\": 4000\n        },\n        {\n            \"data\": np.array([-0.2, 0.1, -1.1, 0.7, 1.0, -0.5, 0.3, -0.8]),\n            \"M\": 4000\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        observed_data = case[\"data\"]\n        n = len(observed_data)\n        M = case[\"M\"]\n\n        # 1. Compute the observed KS statistic\n        d_obs = lilliefors_statistic(observed_data)\n\n        # 2. Approximate p-value through Monte Carlo simulation\n        simulated_stats = np.zeros(M)\n        for i in range(M):\n            # a. Draw a sample of size n from N(0,1)\n            sim_sample = rng.normal(loc=0.0, scale=1.0, size=n)\n            # b, c, d. Calculate KS statistic for this simulated sample\n            simulated_stats[i] = lilliefors_statistic(sim_sample)\n\n        # 3. Compute the p-value\n        p_value = np.sum(simulated_stats >= d_obs) / M\n\n        # 4. Store rounded results\n        all_results.append([round(d_obs, 6), round(p_value, 6)])\n\n    # Final print statement in the exact required format\n    # Example: [[0.123456,0.123456],[0.234567,0.234567],[0.345678,0.345678]]\n    result_str = \"[\" + \",\".join([f\"[{d},{p}]\" for d, p in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "当正态性检验失败时，我们该怎么办？许多强大的统计方法都要求数据呈正态分布，但生物学数据常常是偏态的。本练习  提供了一个建设性的解决方案，介绍了Box-Cox变换这一强大的工具，它可以用来稳定方差，使数据更接近正态分布。你将实现一个完整的流程，学习如何使用最大似然法找到最优变换参数，并随后通过诊断检验来验证变换的成功。",
            "id": "4894199",
            "problem": "您将获得四个严格为正的生物标志物数据集。您的任务是实现一个端到端的正态性评估流程，该流程对每个数据集计算最优的 Box–Cox 变换参数 $\\hat{\\lambda}$（通过最大化正态剖面对数似然），使用估计的 $\\hat{\\lambda}$ 对数据进行变换，然后使用分位数-分位数（Q–Q, quantile–quantile）相关性摘要和 Shapiro–Wilk（SW, Shapiro–Wilk）检验来评估其正态性。\n\n基本定义与假设：\n- 对于严格为正的测量值 $x_1,\\dots,x_n$，Box–Cox 变换定义如下\n$$\nT_{\\lambda}(x)=\n\\begin{cases}\n\\dfrac{x^{\\lambda}-1}{\\lambda},  \\lambda \\neq 0,\\\\\n\\log(x),  \\lambda = 0.\n\\end{cases}\n$$\n- 假设存在一个 $\\lambda$，使得 $Z_i = T_{\\lambda}(x_i)$ 是独立同分布的，服从均值为 $\\mu$、方差为 $\\sigma^2$ 的正态分布。\n- 最优的 $\\hat{\\lambda}$ 是该模型下关于 $\\lambda$ 的正态剖面对数似然的最大化者。\n\n在不绘图的情况下量化 Q–Q 线性度：\n- 令 $z_{(1)} \\le \\dots \\le z_{(n)}$ 表示变换后样本 $z_i = T_{\\hat{\\lambda}}(x_i)$ 的顺序统计量。\n- 定义 Blom 标绘点 $p_i = \\dfrac{i - 0.375}{n + 0.25}$（对于 $i=1,\\dots,n$），并令 $q_i = \\Phi^{-1}(p_i)$，其中 $\\Phi^{-1}$ 是标准正态分布的逆累积分布函数。\n- 计算向量 $(z_{(1)},\\dots,z_{(n)})$ 和 $(q_1,\\dots,q_n)$ 之间的 Pearson 相关系数；将此相关性记为 $r_{\\text{QQ}}$。$r_{\\text{QQ}}$ 的值越接近 1，表示 Q–Q 关系中的线性度越强。\n\n正态性检验：\n- 对变换后的数据 $\\{z_i\\}$ 应用 Shapiro–Wilk 检验，并记录检验的 $p$ 值。使用显著性水平 $\\alpha = 0.05$，如果 $p$ 值大于或等于 $\\alpha$，则判定为“正态”。\n\n您的程序必须：\n- 对于每个数据集，通过在一个有界区间上最大化关于 $\\lambda$ 的正态剖面对数似然来计算 $\\hat{\\lambda}$，使用 $T_{\\hat{\\lambda}}$ 变换数据，计算 $r_{\\text{QQ}}$，计算 Shapiro–Wilk 的 $p$ 值，最后使用 $\\alpha = 0.05$ 报告一个布尔值的正态性判定。\n- 将每个实值输出（$\\hat{\\lambda}$，$p$ 值，$r_{\\text{QQ}}$）四舍五入到小数点后 4 位。\n\n测试套件（数据集）：\n所有数据集都使用 $n = 30$ 个点，其中 $i \\in \\{1,\\dots,30\\}$ 且 $p_i^{(u)} = \\dfrac{i - 0.5}{n}$。令 $\\Phi^{-1}$ 表示标准正态分布的逆累积分布函数，令 $G^{-1}_{k,\\theta}$ 表示形状参数为 $k$、尺度参数为 $\\theta$ 的 Gamma 分布的逆累积分布函数。\n\n- 数据集 $\\mathcal{D}_1$（类对数正态）：$x_i^{(1)} = \\exp\\!\\big(2.0 + 0.5 \\cdot \\Phi^{-1}(p_i^{(u)})\\big)$。\n- 数据集 $\\mathcal{D}_2$（近似正态但为正值）：$x_i^{(2)} = 10.0 + 1.2 \\cdot \\Phi^{-1}(p_i^{(u)})$。\n- 数据集 $\\mathcal{D}_3$（类 Gamma 分布偏态）：$x_i^{(3)} = G^{-1}_{2.0,\\,3.0}(p_i^{(u)})$。\n- 数据集 $\\mathcal{D}_4$（小量级正值）：$x_i^{(4)} = 0.02 + \\exp\\!\\big(-2.0 + 0.5 \\cdot \\Phi^{-1}(p_i^{(u)})\\big)$。\n\n输出规格：\n- 对于每个数据集 $\\mathcal{D}_j$（其中 $j \\in \\{1,2,3,4\\}$），返回一个列表 $[\\hat{\\lambda}_j, p\\_j, r_{\\text{QQ},j}, \\text{normal}_j]$，其中 $\\hat{\\lambda}_j$ 是估计的 Box–Cox 参数，$p\\_j$ 是对变换后数据进行 Shapiro–Wilk 检验的 $p$ 值，$r_{\\text{QQ},j}$ 是 Q–Q 相关性，而 $\\text{normal}_j$ 是一个布尔值，如果 $p\\_j \\ge 0.05$ 则为 true，否则为 false。\n- 将 $\\hat{\\lambda}_j$、$p\\_j$ 和 $r_{\\text{QQ},j}$ 四舍五入到小数点后 4 位。\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的、逗号分隔的列表。该行的格式必须如下：\n$[\\,[\\hat{\\lambda}_1,p_1,r_{\\text{QQ},1},\\text{normal}_1],[\\hat{\\lambda}_2,p_2,r_{\\text{QQ},2},\\text{normal}_2],[\\hat{\\lambda}_3,p_3,r_{\\text{QQ},3},\\text{normal}_3],[\\hat{\\lambda}_4,p_4,r_{\\text{QQ},4},\\text{normal}_4]\\,]$.",
            "solution": "用户提供了一个有效的、定义明确的生物统计学问题。任务是构建并应用一个统计流程，以评估四个给定数据集的正态性，其中包含一个初步的 Box-Cox 变换步骤以改善正态性。解决方案首先详细阐述所需方法的理论基础，然后通过计算实现它们。\n\n### 1. 理论框架\n\n#### 1.1. Box-Cox 变换与正态性假设\n给定一组严格为正的数据点 $x_1, x_2, \\dots, x_n$，Box-Cox 变换是一种由 $\\lambda$ 参数化的幂变换。其定义如下：\n$$\nT_{\\lambda}(x) =\n\\begin{cases}\n\\frac{x^{\\lambda}-1}{\\lambda},  \\text{if } \\lambda \\neq 0, \\\\\n\\log(x),  \\text{if } \\lambda = 0.\n\\end{cases}\n$$\n$\\lambda=0$ 的情况是当 $\\lambda \\to 0$ 时 $\\lambda \\neq 0$ 表达式的极限，这可以使用 L'Hôpital 法则来证明。该程序的核心假设是，对于某个 $\\lambda$ 值，变换后的变量 $Z_i = T_{\\lambda}(x_i)$ 是来自正态分布 $N(\\mu, \\sigma^2)$ 的独立同分布（i.i.d.）样本。\n\n#### 1.2. 最优参数 $\\hat{\\lambda}$ 的估计\n最优参数 $\\hat{\\lambda}$ 是通过最大似然法确定的。原始数据 $x_i$ 的概率密度函数可以通过使用变量替换公式从变换后数据 $z_i$ 的密度中得到。给定参数 $\\mu$、$\\sigma^2$ 和 $\\lambda$ 时，原始观测值 $\\mathbf{x} = (x_1, \\dots, x_n)$ 的似然函数为：\n$$\nL(\\mu, \\sigma^2, \\lambda | \\mathbf{x}) = \\left( \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(T_\\lambda(x_i) - \\mu)^2}{2\\sigma^2}\\right) \\right) \\cdot J(\\lambda; \\mathbf{x})\n$$\n项 $J(\\lambda; \\mathbf{x})$ 是从 $\\mathbf{x}$ 到 $\\mathbf{z}$ 变换的雅可比行列式：\n$$\nJ(\\lambda; \\mathbf{x}) = \\prod_{i=1}^{n} \\left| \\frac{d T_\\lambda(x_i)}{d x_i} \\right| = \\prod_{i=1}^{n} x_i^{\\lambda-1}\n$$\n因此，对数似然函数为：\n$$\n\\ell(\\mu, \\sigma^2, \\lambda | \\mathbf{x}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (T_\\lambda(x_i) - \\mu)^2 + (\\lambda-1)\\sum_{i=1}^{n} \\log(x_i)\n$$\n对于固定的 $\\lambda$，$\\mu$ 和 $\\sigma^2$ 的最大似然估计量（MLEs）是变换后数据 $z_i(\\lambda) = T_\\lambda(x_i)$ 的样本均值和样本方差：\n$$\n\\hat{\\mu}(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} z_i(\\lambda) = \\bar{z}(\\lambda)\n$$\n$$\n\\hat{\\sigma}^2(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} (z_i(\\lambda) - \\bar{z}(\\lambda))^2\n$$\n将这些代入对数似然函数，得到关于 $\\lambda$ 的剖面对数似然：\n$$\n\\ell_p(\\lambda | \\mathbf{x}) = -\\frac{n}{2}\\log(2\\pi) -\\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) - \\frac{n}{2} + (\\lambda-1)\\sum_{i=1}^{n} \\log(x_i)\n$$\n为了找到最优的 $\\hat{\\lambda}$，我们对 $\\ell_p(\\lambda | \\mathbf{x})$ 关于 $\\lambda$ 进行最大化。由于项 $-\\frac{n}{2}\\log(2\\pi)$ 和 $-\\frac{n}{2}$ 相对于 $\\lambda$ 是常数，这等价于最大化简化的函数：\n$$\nL_{\\text{profile}}(\\lambda) = -\\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) + (\\lambda-1)\\sum_{i=1}^{n} \\log(x_i)\n$$\n这个最大化过程将通过最小化该函数的负数，在一个有界区间（例如 $\\lambda \\in [-5, 5]$）上进行数值计算。\n\n#### 1.3. 变换后数据的正态性评估\n\n一旦找到 $\\hat{\\lambda}$，数据就被变换为 $z_i = T_{\\hat{\\lambda}}(x_i)$。然后使用两种方法评估这个新样本 $\\{z_i\\}$ 的正态性。\n\n**分位数-分位数（Q-Q）相关性, $r_{\\text{QQ}}$：**\n该度量量化了 Q-Q 图的线性程度。它涉及将排序后的数据与标准正态分布的理论分位数进行比较。\n1.  计算变换后数据的顺序统计量：$z_{(1)} \\le z_{(2)} \\le \\dots \\le z_{(n)}$。\n2.  计算 Blom 标绘点：$p_i = \\frac{i - 0.375}{n + 0.25}$（对于 $i=1, \\dots, n$）。\n3.  从标准正态分布中确定相应的理论分位数：$q_i = \\Phi^{-1}(p_i)$，其中 $\\Phi^{-1}$ 是 $N(0, 1)$ 的逆累积分布函数（CDF）。\n4.  计算顺序统计量向量 $\\mathbf{z}_{\\text{ord}} = (z_{(1)}, \\dots, z_{(n)})$ 与理论分位数向量 $\\mathbf{q} = (q_1, \\dots, q_n)$ 之间的 Pearson 相关系数。此相关性记为 $r_{\\text{QQ}}$。$r_{\\text{QQ}}$ 的值接近 1 表示存在强线性关系，从而支持正态性假设。\n\n**Shapiro-Wilk (SW) 检验：**\nShapiro-Wilk 检验是一种用于检验正态性的正式假设检验。其零假设（$H_0$）是数据样本来自一个正态分布的总体。计算检验统计量并将其转换为 $p$ 值。\n-   如果 $p$ 值小于选定的显著性水平 $\\alpha$（此处 $\\alpha=0.05$），则拒绝 $H_0$，数据被认为不服从正态分布。\n-   如果 $p$ 值大于或等于 $\\alpha$，则没有足够证据拒绝 $H_0$，并且为了此问题的目的，数据被判定为与正态分布一致。\n\n### 2. 计算步骤\n\n对提供的四个数据集中的每一个执行以下步骤：\n\n1.  **生成数据集**：根据指定的公式为每个数据集 $\\mathcal{D}_j$ 生成数据点。\n2.  **估计 $\\hat{\\lambda}$**：将剖面对数似然的负数 $-L_{\\text{profile}}(\\lambda)$ 定义为目标函数。使用数值优化程序（`scipy.optimize.minimize_scalar`）在如 $[-5, 5]$ 的有界区间内找到使该函数最小化的值 $\\hat{\\lambda}$。\n3.  **变换数据**：使用估计的 $\\hat{\\lambda}$ 将原始数据集 $x_i$ 变换为 $z_i = T_{\\hat{\\lambda}}(x_i)$。\n4.  **计算 $r_{\\text{QQ}}$**：如第 1.3 节所述，为变换后的数据 $\\{z_i\\}$ 计算 Q-Q 相关性。\n5.  **执行 Shapiro-Wilk 检验**：对变换后的数据 $\\{z_i\\}$ 应用 Shapiro-Wilk 检验以获得 $p$ 值。\n6.  **报告结果**：根据 $p$ 值是否 $\\ge 0.05$ 做出关于正态性的布尔决策。收集数据集的最终结果——$\\hat{\\lambda}$、SW 的 $p$ 值、$r_{\\text{QQ}}$ 和正态性判定。实数值四舍五入到小数点后四位。\n7.  **最终输出**：将所有四个数据集的收集结果组合成一个列表的列表，并按指定格式打印。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats, optimize\n\ndef solve():\n    \"\"\"\n    Main function to perform normality assessment on four datasets.\n    \"\"\"\n\n    def generate_datasets():\n        \"\"\"Generates the four datasets as specified in the problem.\"\"\"\n        n = 30\n        i = np.arange(1, n + 1)\n        p_u = (i - 0.5) / n\n        norm_quantiles = stats.norm.ppf(p_u)\n\n        # Dataset D1 (log-normal-like)\n        d1 = np.exp(2.0 + 0.5 * norm_quantiles)\n\n        # Dataset D2 (approximately Normal but positive)\n        d2 = 10.0 + 1.2 * norm_quantiles\n\n        # Dataset D3 (Gamma-like skew)\n        gamma_quantiles = stats.gamma.ppf(p_u, a=2.0, scale=3.0)\n        d3 = gamma_quantiles\n\n        # Dataset D4 (small-magnitude positive)\n        d4 = 0.02 + np.exp(-2.0 + 0.5 * norm_quantiles)\n\n        return [d1, d2, d3, d4]\n\n    def boxcox_transform(x, lam):\n        \"\"\"Applies the Box-Cox transformation.\"\"\"\n        if lam == 0:\n            return np.log(x)\n        else:\n            return (x**lam - 1) / lam\n\n    def profile_log_likelihood(lam, x):\n        \"\"\"\n        Computes the profile log-likelihood for a given lambda.\n        Note: The function returns the *negative* for minimization purposes.\n        \"\"\"\n        n = len(x)\n        z = boxcox_transform(x, lam)\n        # Using n-ddof=0 for MLE of variance (division by n)\n        log_lik = -n / 2 * np.log(np.var(z, ddof=0)) + (lam - 1) * np.sum(np.log(x))\n        return -log_lik\n\n    def analyze_dataset(x):\n        \"\"\"\n        Performs the full analysis pipeline for a single dataset.\n        \"\"\"\n        # 1. Compute optimal lambda\n        # We use a bounded search for robustness, as is common practice.\n        res = optimize.minimize_scalar(\n            profile_log_likelihood, \n            args=(x,), \n            bounds=(-5, 5), \n            method='bounded'\n        )\n        lambda_hat = res.x\n\n        # 2. Transform the data\n        z_transformed = boxcox_transform(x, lambda_hat)\n\n        # 3. Compute Q-Q correlation (r_QQ)\n        n = len(z_transformed)\n        z_ordered = np.sort(z_transformed)\n        i = np.arange(1, n + 1)\n        p_blom = (i - 0.375) / (n + 0.25)\n        q_theoretical = stats.norm.ppf(p_blom)\n        r_qq = np.corrcoef(z_ordered, q_theoretical)[0, 1]\n\n        # 4. Compute Shapiro-Wilk test p-value\n        _, p_value = stats.shapiro(z_transformed)\n\n        # 5. Make normality decision\n        alpha = 0.05\n        is_normal = p_value >= alpha\n\n        # 6. Format results\n        return [\n            round(lambda_hat, 4),\n            round(p_value, 4),\n            round(r_qq, 4),\n            is_normal\n        ]\n\n    datasets = generate_datasets()\n    all_results = [analyze_dataset(d) for d in datasets]\n    \n    # Final print statement in the exact required format.\n    # The str() representation of a list includes spaces after commas,\n    # which matches the visual style of the problem's output format example.\n    print(str(all_results).replace(\"True\", \"true\").replace(\"False\", \"false\"))\n\nsolve()\n\n```"
        },
        {
            "introduction": "正态性评估不仅是一个孤立的步骤，更是模型诊断的关键组成部分。在线性回归中，误差项服从正态分布是一项基本假设，而非正态的残差往往预示着模型设定存在更深层次的问题。这项高级练习  将让你沉浸在真实的场景中，体验从不正确的函数形式到遗漏变量等各种模型设定错误如何导致非正态的残差，并教会你如何应用针对性的修正措施来提高模型的有效性和可靠性。",
            "id": "4894203",
            "problem": "您必须编写一个完整、可运行的程序，该程序模拟多种生物统计学上现实的场景，以评估在模型设定错误的情况下模型残差的正态性，然后应用纠正措施。目标是使用源于经典线性模型第一性原理的诊断方法来评估残差的正态性，并推理特定的纠正措施如何以及为何能改善正态性。\n\n从以下核心定义和经过充分检验的事实出发。假设经典线性模型具有响应向量 $y \\in \\mathbb{R}^n$、设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$、参数向量 $\\beta \\in \\mathbb{R}^p$ 以及满足 $y = X \\beta + \\varepsilon$ 的误差向量 $\\varepsilon \\in \\mathbb{R}^n$。残差定义为 $r = y - X \\hat{\\beta}$，其中 $\\hat{\\beta}$ 是使残差平方和最小的最小二乘估计。在正确设定且误差为独立同分布的正态误差的情况下，残差是正态向量的线性变换，因此服从正态分布。偏离正态性的情况可能由均值函数设定错误、方差函数设定错误、重尾误差或遗漏变量引起。残差的正态性可以使用 Shapiro–Wilk 检验进行评估，该检验在正态性原假设下提供一个 $p$ 值；标准化残差计算为 $z_i = r_i / \\hat{\\sigma}$，其中 $\\hat{\\sigma}^2 = \\sum_{i=1}^n r_i^2 / (n - p)$。\n\n您必须纯粹以数学和逻辑术语实现以下模拟协议：\n\n1. 对于每个测试用例，根据指定的数据生成过程生成合成数据，使用普通最小二乘法 (OLS) 拟合一个基准的设定错误模型，计算标准化残差，并计算这些残差的 Shapiro–Wilk $p$ 值。\n\n2. 根据设定错误的类型应用特定的纠正措施并重新拟合模型：\n   - 均值设定错误：用适当的非线性项丰富均值函数。\n   - 方差设定错误：使用加权最小二乘法 (WLS)，其权重根据方差函数的理论推导得出。\n   - 重尾误差：使用带有 Huber 权重的迭代重加权最小二乘法 (IRLS)，以减少离群值和重尾的影响。\n   - 遗漏变量：包含遗漏的预测变量以正确设定均值函数。\n\n3. 纠正后重新计算标准化残差和 Shapiro–Wilk $p$ 值。报告基准和纠正后的 $p$ 值及其差异，以量化改进程度。\n\n您的程序必须通过正规方程从第一性原理实现 OLS 和 WLS。对于带有 Huber 权重的 IRLS，使用 Huber 影响函数定义的权重 $w_i = 1$（如果 $|r_i| \\leq k s$）和 $w_i = k s / |r_i|$（否则），其中 $s$ 是一个稳健的尺度，而 $k$ 是一个调整常数。使用无偏残差标准差 $\\hat{\\sigma} = \\sqrt{\\sum r_i^2 / (n - p)}$ 来标准化残差。\n\n测试套件规范：\n您必须实现以下五个测试用例，每个用例的参数涵盖不同方面（一般情况、边界情况和边缘情况）。所有随机性必须使用固定的种子以保证可复现性。此问题不涉及任何物理单位。\n\n- 案例 $1$（正确设定，理想情况）：生成 $n = 200$ 个观测值，其中 $x \\sim \\mathrm{Uniform}(-3, 3)$，参数 $\\beta_0 = 2, \\beta_1 = 0.5$，同方差误差 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $\\sigma = 1$。拟合正确的线性模型 $y = \\beta_0 + \\beta_1 x + \\varepsilon$。使用种子 $314159$。\n\n- 案例 $2$（均值设定错误）：生成 $n = 200$ 个观测值，其中 $x \\sim \\mathrm{Uniform}(-2, 2)$，参数 $\\beta_0 = 1, \\beta_1 = 1, \\beta_2 = 0.8$，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $\\sigma = 1$。真实均值为 $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\varepsilon$，但拟合时基准线性模型忽略了 $x^2$。纠正措施：包含 $x^2$。使用种子 $271828$。\n\n- 案例 $3$（方差设定错误，异方差性）：生成 $n = 300$ 个观测值，其中 $x \\sim \\mathrm{Uniform}(0, 3)$，参数 $\\beta_0 = 0, \\beta_1 = 1.5$，基础尺度 $\\sigma_0 = 0.5$，异方差方差 $\\mathrm{Var}(\\varepsilon \\mid x) = \\sigma_0^2 (1 + c x^2)$ 且 $c = 2$。拟合基准 OLS，纠正措施为使用权重 $w_i = 1 / (1 + c x_i^2)$ 的 WLS。使用种子 $161803$。\n\n- 案例 $4$（重尾误差，边缘情况）：生成 $n = 250$ 个观测值，其中 $x \\sim \\mathrm{Uniform}(-2, 2)$，参数 $\\beta_0 = 0, \\beta_1 = 1$，自由度 $df = 3$，尺度 $\\sigma = 1$。误差 $\\varepsilon$ 来自自由度为 $df$ 的学生 t 分布，并经过缩放以使方差为 $\\sigma^2$（即，将标准 $t$ 分布乘以 $\\sqrt{(df - 2)/df} \\cdot \\sigma$）。基准模型使用 OLS；纠正措施使用带有 Huber 权重和调整常数 $k = 1.345$ 的 IRLS。使用种子 $101$。\n\n- 案例 $5$（遗漏变量，相关预测变量）：生成 $n = 200$ 个观测值，其中 $x_1 \\sim \\mathrm{Uniform}(-2, 2)$ 且 $x_2 = \\rho x_1 + \\eta$，其中 $\\rho = 0.7$ 且 $\\eta \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$ 且 $\\sigma_\\eta = 0.3$。参数为 $\\beta_0 = 0, \\beta_1 = 1, \\beta_2 = 1$，同方差误差 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $\\sigma = 0.5$。真实均值为 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$，但基准模型拟合时忽略了 $x_2$。纠正措施：包含 $x_2$。使用种子 $2024$。\n\n对于每个案例，计算：\n- 基准标准化残差的 Shapiro–Wilk $p$ 值，\n- 纠正后标准化残差的 Shapiro–Wilk $p$ 值，\n- 差异 $d = p_{\\mathrm{corrected}} - p_{\\mathrm{baseline}}$。\n\n最终输出格式：\n您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表，其中每个元素是三个浮点数的列表 $[p_{\\mathrm{baseline}}, p_{\\mathrm{corrected}}, d]$，顺序与测试用例相同，无空格。例如，生成类似 $[[p_{1b},p_{1c},d_1],[p_{2b},p_{2c},d_2],\\dots,[p_{5b},p_{5c},d_5]]$ 形式的输出。\n\n所有输出都必须是浮点数。此问题不涉及任何物理单位、角度单位或百分比，因此无需进行单位转换。",
            "solution": "评估统计模型有效性的问题是所有经验科学学科的核心。经典线性模型的一个关键诊断程序是检查残差，即观测数据与模型预测之间的差异。在模型的理想假设下——特别是误差是来自正态分布的独立同分布随机变量——模型的残差也应呈正态分布。残差偏离正态性通常表明模型的一个或多个基本假设被违反。本模拟研究旨在展示不同类型的模型设定错误如何在残差中表现为非正态性，以及有针对性的纠正措施如何恢复正态性（通过 Shapiro-Wilk 检验来衡量）。\n\n经典线性模型的一般形式由以下方程给出：\n$$ y = X \\beta + \\varepsilon $$\n其中 $y$ 是观测值的 $n \\times 1$ 向量，$X$ 是预测变量（包括一个截距项）的 $n \\times p$ 设计矩阵，$\\beta$ 是未知参数的 $p \\times 1$ 向量，$\\varepsilon$ 是未观测到的随机误差的 $n \\times 1$ 向量。我们假设 $\\mathrm{E}[\\varepsilon] = 0$ 和 $\\mathrm{Var}(\\varepsilon) = \\sigma^2 I_n$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\n$\\beta$ 的普通最小二乘法 (OLS) 估计量，记为 $\\hat{\\beta}$，是通过最小化残差平方和 $\\|y - X\\beta\\|^2$ 得到的。这产生了著名的正规方程解：\n$$ \\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T y $$\n然后残差计算为 $r = y - X \\hat{\\beta}$。为了诊断目的，通常使用标准化残差，问题将其定义为 $z_i = r_i / \\hat{\\sigma}$，其中 $\\hat{\\sigma}$ 是误差标准差的无偏估计量：\n$$ \\hat{\\sigma} = \\sqrt{\\frac{\\sum_{i=1}^n r_i^2}{n - p}} $$\n这里，$n$ 是观测值的数量，$p$ 是参数的数量（即 $X$ 的列数）。这些标准化残差的正态性使用 Shapiro-Wilk 检验进行评估，该检验测试数据来自正态分布的原假设。一个小的 $p$ 值（通常  0.05）提供了反对正态性的证据。\n\n该模拟协议处理了几种常见的模型设定错误：\n\n$1$. **均值设定错误**：预测变量与响应变量之间关系的函数形式不正确。例如，当真实关系是二次方时，拟合了一个线性关系。这种设定错误通常会在残差中引入系统性模式，可能导致其分布偏离正态性。纠正措施是通过向设计矩阵 $X$ 中添加适当的非线性项（例如 $x^2$）来丰富模型。\n\n$2$. **方差设定错误（异方差性）**：误差方差恒定（同方差性）的假设被违反。误差的方差取决于预测变量的值。OLS 仍然是无偏的，但不再是最佳线性无偏估计量 (BLUE)。残差将不具有恒定的方差，这可能导致正态性检验失败。纠正措施是使用加权最小二乘法 (WLS)，它在这种情况下是 BLUE。WLS 估计量为：\n$$ \\hat{\\beta}_{WLS} = (X^T W X)^{-1} X^T W y $$\n其中 $W$ 是一个对角权重矩阵，$W = \\mathrm{diag}(w_1, w_2, \\dots, w_n)$，每个权重 $w_i$ 与该观测的误差方差成反比，即 $w_i \\propto 1/\\mathrm{Var}(\\varepsilon_i)$。\n\n$3$. **重尾误差**：误差来自比正态分布具有更重尾部的分布，例如自由度较少的学生 t 分布。这会导致离群值，这些离群值会严重影响 OLS 拟合，并导致残差分布非正态。纠正措施涉及稳健回归，特别是迭代重加权最小二乘法 (IRLS)。IRLS 是一种迭代执行加权最小二乘法的算法，其中权重由前一次迭代的残差确定。这个过程降低了具有大残差（离群值）的观测值的影响。对于 Huber M-估计，权重定义为：\n$$ w_i = \\begin{cases} 1  \\text{if } |r_i| \\le k s \\\\ \\frac{k s}{|r_i|}  \\text{if } |r_i| > k s \\end{cases} $$\n这里，$k$ 是一个调整常数（指定为 $k=1.345$，以便在正态数据上达到约 $95\\%$ 的效率），$s$ 是对残差尺度的稳健估计。我们将采用的一个常见选择是初始 OLS 残差的缩放中位数绝对偏差 (MAD)：$s = \\text{median}(|r_{\\text{initial}}|) / 0.6745$。迭代过程从 OLS 拟合开始，一直持续到估计的系数收敛为止。\n\n$4$. **遗漏变量**：一个相关的预测变量被排除在模型之外。如果遗漏的变量与包含的预测变量相关，那么包含变量的估计系数将是有偏的（遗漏变量偏误）。这种偏误会传播到残差中，残差将包含缺失变量的影响，通常导致非正态性。直接的纠正措施是在模型设定中包含遗漏的变量。\n\n该算法通过为五个指定的测试用例中的每一个模拟数据来进行。对于每个案例，它首先拟合规定的基准（设定错误）模型，计算标准化残差，并计算 Shapiro-Wilk $p$ 值 ($p_{\\mathrm{baseline}}$)。然后，它应用指定的纠正措施，重新拟合模型，并重新计算标准化残差和相应的 Shapiro-Wilk $p$ 值 ($p_{\\mathrm{corrected}}$)。差异 $d = p_{\\mathrm{corrected}} - p_{\\mathrm{baseline}}$ 量化了残差正态性的改善程度。所有计算，包括 OLS、WLS 和 IRLS，都按照问题要求从第一性原理实现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef fit_ols(X, y):\n    \"\"\"\n    Fits an Ordinary Least Squares (OLS) model using the normal equations.\n    \"\"\"\n    try:\n        # beta_hat = (X'X)^-1 * X'y\n        XtX_inv = np.linalg.inv(X.T @ X)\n        XtY = X.T @ y\n        beta = XtX_inv @ XtY\n        return beta\n    except np.linalg.LinAlgError:\n        return np.linalg.lstsq(X, y, rcond=None)[0]\n\ndef fit_wls(X, y, w):\n    \"\"\"\n    Fits a Weighted Least Squares (WLS) model.\n    \"\"\"\n    W = np.diag(w)\n    try:\n        # beta_hat = (X'WX)^-1 * X'Wy\n        XtWX_inv = np.linalg.inv(X.T @ W @ X)\n        XtWY = X.T @ W @ y\n        beta = XtWX_inv @ XtWY\n        return beta\n    except np.linalg.LinAlgError:\n        # Fallback for singular matrix\n        sqrt_W = np.sqrt(W)\n        return np.linalg.lstsq(sqrt_W @ X, sqrt_W @ y, rcond=None)[0]\n\ndef fit_irls_huber(X, y, k=1.345, max_iter=100, tol=1e-6):\n    \"\"\"\n    Fits a robust regression model using Iteratively Reweighted Least Squares (IRLS)\n    with Huber T weights.\n    \"\"\"\n    # 1. Initial fit\n    beta = fit_ols(X, y)\n    \n    # 2. Calculate robust scale s (MAD, scaled for asymptotic normality)\n    residuals_initial = y - X @ beta\n    # MAD is median(|res - median(res)|). median(res) is close to 0 for a good fit.\n    mad = np.median(np.abs(residuals_initial - np.median(residuals_initial)))\n    s = mad / 0.6745  # Scale for consistency\n    if s  1e-10: s = 1.0 # Protect against zero scale\n\n    for _ in range(max_iter):\n        beta_old = beta\n        \n        # 3. Compute residuals\n        r = y - X @ beta\n        \n        # 4. Compute Huber weights\n        abs_r = np.abs(r)\n        weights = np.ones_like(r)\n        idx = abs_r > k * s\n        weights[idx] = (k * s) / abs_r[idx]\n        \n        # 5. WLS step\n        beta = fit_wls(X, y, weights)\n        \n        # 6. Check for convergence\n        if np.linalg.norm(beta - beta_old)  tol:\n            break\n            \n    return beta\n\ndef calculate_diagnostics(X, y, beta):\n    \"\"\"\n    Calculates residuals, standardized residuals, and Shapiro-Wilk p-value.\n    \"\"\"\n    n, p = X.shape\n    r = y - X @ beta\n    \n    # Unbiased estimate of error variance\n    sigma_sq_hat = np.sum(r**2) / (n - p)\n    sigma_hat = np.sqrt(sigma_sq_hat)\n    \n    # Avoid division by zero if residuals are all zero\n    if sigma_hat  1e-10:\n        return 1.0 # Perfect fit, residuals are constant, not normal, but p-value=1 is reasonable\n    \n    # Standardized residuals as per problem statement\n    z = r / sigma_hat\n    \n    # Shapiro-Wilk test\n    if len(z)  3: return np.nan # Shapiro-Wilk needs at least 3 samples\n    p_value = stats.shapiro(z).pvalue\n    return p_value\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases_params = [\n        # Case 1: Correct specification\n        {'seed': 314159, 'n': 200},\n        # Case 2: Mean misspecification\n        {'seed': 271828, 'n': 200},\n        # Case 3: Variance misspecification\n        {'seed': 161803, 'n': 300},\n        # Case 4: Heavy-tailed errors\n        {'seed': 101, 'n': 250},\n        # Case 5: Omitted variable\n        {'seed': 2024, 'n': 200},\n    ]\n\n    results = []\n\n    # --- Case 1: Correct Specification ---\n    spec = test_cases_params[0]\n    rng = np.random.default_rng(spec['seed'])\n    n = spec['n']\n    x = rng.uniform(-3, 3, n)\n    X_base = np.c_[np.ones(n), x]\n    beta_true = np.array([2, 0.5])\n    eps = rng.normal(0, 1, n)\n    y = X_base @ beta_true + eps\n    \n    # Baseline (which is also the correct model here)\n    beta_base = fit_ols(X_base, y)\n    p_base = calculate_diagnostics(X_base, y, beta_base)\n    \n    # Correction (same model, so same result)\n    p_corr = p_base\n    results.append([p_base, p_corr, p_corr - p_base])\n\n    # --- Case 2: Mean Misspecification ---\n    spec = test_cases_params[1]\n    rng = np.random.default_rng(spec['seed'])\n    n = spec['n']\n    x = rng.uniform(-2, 2, n)\n    X_true = np.c_[np.ones(n), x, x**2]\n    beta_true = np.array([1, 1, 0.8])\n    eps = rng.normal(0, 1, n)\n    y = X_true @ beta_true + eps\n    \n    # Baseline\n    X_base = np.c_[np.ones(n), x]\n    beta_base = fit_ols(X_base, y)\n    p_base = calculate_diagnostics(X_base, y, beta_base)\n    \n    # Correction\n    X_corr = X_true\n    beta_corr = fit_ols(X_corr, y)\n    p_corr = calculate_diagnostics(X_corr, y, beta_corr)\n    results.append([p_base, p_corr, p_corr - p_base])\n    \n    # --- Case 3: Variance Misspecification ---\n    spec = test_cases_params[2]\n    rng = np.random.default_rng(spec['seed'])\n    n = spec['n']\n    c = 2.0\n    sigma0 = 0.5\n    x = rng.uniform(0, 3, n)\n    X_base = np.c_[np.ones(n), x]\n    beta_true = np.array([0, 1.5])\n    error_std_dev = sigma0 * np.sqrt(1 + c * x**2)\n    eps = rng.normal(0, 1, n) * error_std_dev\n    y = X_base @ beta_true + eps\n    \n    # Baseline\n    beta_base = fit_ols(X_base, y)\n    p_base = calculate_diagnostics(X_base, y, beta_base)\n    \n    # Correction\n    weights = 1 / (1 + c * x**2)\n    beta_corr = fit_wls(X_base, y, weights)\n    p_corr = calculate_diagnostics(X_base, y, beta_corr)\n    results.append([p_base, p_corr, p_corr - p_base])\n\n    # --- Case 4: Heavy-Tailed Errors ---\n    spec = test_cases_params[3]\n    rng = np.random.default_rng(spec['seed'])\n    n = spec['n']\n    df = 3\n    sigma = 1\n    x = rng.uniform(-2, 2, n)\n    X_base = np.c_[np.ones(n), x]\n    beta_true = np.array([0, 1])\n    scale_factor = sigma * np.sqrt((df - 2) / df)\n    eps = stats.t.rvs(df=df, size=n, random_state=rng) * scale_factor\n    y = X_base @ beta_true + eps\n    \n    # Baseline\n    beta_base = fit_ols(X_base, y)\n    p_base = calculate_diagnostics(X_base, y, beta_base)\n    \n    # Correction\n    beta_corr = fit_irls_huber(X_base, y, k=1.345)\n    p_corr = calculate_diagnostics(X_base, y, beta_corr)\n    results.append([p_base, p_corr, p_corr - p_base])\n\n    # --- Case 5: Omitted Variable ---\n    spec = test_cases_params[4]\n    rng = np.random.default_rng(spec['seed'])\n    n = spec['n']\n    rho = 0.7\n    sigma_eta = 0.3\n    sigma_eps = 0.5\n    x1 = rng.uniform(-2, 2, n)\n    eta = rng.normal(0, sigma_eta, n)\n    x2 = rho * x1 + eta\n    X_true = np.c_[np.ones(n), x1, x2]\n    beta_true = np.array([0, 1, 1])\n    eps = rng.normal(0, sigma_eps, n)\n    y = X_true @ beta_true + eps\n\n    # Baseline\n    X_base = np.c_[np.ones(n), x1]\n    beta_base = fit_ols(X_base, y)\n    p_base = calculate_diagnostics(X_base, y, beta_base)\n    \n    # Correction\n    X_corr = X_true\n    beta_corr = fit_ols(X_corr, y)\n    p_corr = calculate_diagnostics(X_corr, y, beta_corr)\n    results.append([p_base, p_corr, p_corr - p_base])\n\n    # Final print statement in the exact required format.\n    # Convert list of lists to string, then remove spaces\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}