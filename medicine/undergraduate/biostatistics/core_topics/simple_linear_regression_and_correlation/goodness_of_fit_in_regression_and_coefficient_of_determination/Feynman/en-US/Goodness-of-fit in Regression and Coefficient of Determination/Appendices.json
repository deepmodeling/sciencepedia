{
    "hands_on_practices": [
        {
            "introduction": "To build a strong foundation, we begin with the direct computation of the coefficient of determination, $R^2$. This first exercise  requires you to calculate $R^2$ from a small dataset where a logarithmic transformation is used to achieve linearity—a common practice in biostatistics. By working through the fundamental formulas, you will gain a concrete understanding of how $R^2$ quantifies the proportion of variance in the response that is explained by the predictor.",
            "id": "4914681",
            "problem": "A biostatistics study investigates how a serum biomarker $y$ varies with drug dose $x$ when the relationship is multiplicative. The scientific premise is that multiplicative error in $y$ is stabilized by the natural logarithm, and the transformed relationship between the biomarker and dose is modeled linearly. Specifically, the working model is\n$$\\ln(y_i) = a + b \\ln(x_i) + e_i,$$\nwhere $a$ and $b$ are unknown parameters and $e_i$ represents zero-mean random error on the log scale. Ordinary Least Squares (OLS) is used to fit the line on the transformed scale.\n\nA set of $n=5$ paired observations was collected under exponentially increasing doses chosen to span a wide dynamic range. For each subject $i$, both $\\ln(x_i)$ and $\\ln(y_i)$ were recorded as follows (ordered by increasing $\\ln(x_i)$):\n$$(\\ln(x_i), \\ln(y_i)) \\in \\{(0, 0.45), (1, 1.25), (2, 2.20), (3, 3.05), (4, 3.95)\\}.$$\n\nStarting from the core definitions of least squares fitting and the variance decomposition for the transformed response, compute the coefficient of determination $R^2$ for this fitted linear model on the log-transformed scale. Express your final answer as a pure number (no units), and round your answer to four significant figures.",
            "solution": "The problem statement is evaluated and found to be valid. It is a well-posed, scientifically grounded problem in biostatistics that provides a complete and consistent set of data and a clear objective. The model specified, which involves a logarithmic transformation to linearize a power-law relationship and stabilize variance, is a standard and respected technique in statistical modeling. All necessary information is provided to compute the requested quantity.\n\nThe problem asks for the coefficient of determination, denoted as $R^2$, for a linear model fitted to log-transformed data. The model is given by:\n$$ \\ln(y_i) = a + b \\ln(x_i) + e_i $$\nFor clarity and convenience in the calculations that follow, let us define the transformed variables as $X_i' = \\ln(x_i)$ and $Y_i' = \\ln(y_i)$. The model is then a simple linear regression model:\n$$ Y_i' = a + b X_i' + e_i $$\nThe dataset consists of $n=5$ paired observations $(X_i', Y_i')$:\n$$ (X_i', Y_i') \\in \\{(0, 0.45), (1, 1.25), (2, 2.20), (3, 3.05), (4, 3.95)\\} $$\n\nThe coefficient of determination, $R^2$, represents the proportion of the total variance in the dependent variable, $Y_i'$, that is explained by the linear relationship with the independent variable, $X_i'$. It is defined as:\n$$ R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}} $$\nwhere $\\text{SS}_{\\text{tot}}$ is the total sum of squares and $\\text{SS}_{\\text{res}}$ is the residual sum of squares.\n\nThe total sum of squares measures the total variability in the response variable $Y_i'$:\n$$ \\text{SS}_{\\text{tot}} = \\sum_{i=1}^{n} (Y_i' - \\overline{Y'})^2 $$\nwhere $\\overline{Y'}$ is the sample mean of the $Y_i'$ values.\n\nThe residual sum of squares measures the variability that remains unexplained after fitting the regression line:\n$$ \\text{SS}_{\\text{res}} = \\sum_{i=1}^{n} (Y_i' - \\hat{Y}_i')^2 $$\nwhere $\\hat{Y}_i' = \\hat{a} + \\hat{b}X_i'$ are the fitted values obtained from the Ordinary Least Squares (OLS) regression.\n\nFor a simple linear regression model with a single predictor, $R^2$ is equivalent to the square of the Pearson product-moment correlation coefficient, $r$, between $X'$ and $Y'$. This provides a more direct computational path. The formula for $r^2$ is:\n$$ R^2 = r^2 = \\left( \\frac{\\sum_{i=1}^{n} (X_i' - \\overline{X'})(Y_i' - \\overline{Y'})}{\\sqrt{\\sum_{i=1}^{n} (X_i' - \\overline{X'})^2 \\sum_{i=1}^{n} (Y_i' - \\overline{Y'})^2}} \\right)^2 = \\frac{S_{X'Y'}^2}{S_{X'X'} S_{Y'Y'}} $$\nwhere we have defined the corrected sums of squares and cross-products:\n$S_{X'X'} = \\sum_{i=1}^{n} (X_i' - \\overline{X'})^2$\n$S_{Y'Y'} = \\sum_{i=1}^{n} (Y_i' - \\overline{Y'})^2$\n$S_{X'Y'} = \\sum_{i=1}^{n} (X_i' - \\overline{X'})(Y_i' - \\overline{Y'})$\n\nTo compute these quantities, we first calculate the necessary sums and means from the data.\nThe data points are:\n$X': \\{0, 1, 2, 3, 4\\}$\n$Y': \\{0.45, 1.25, 2.20, 3.05, 3.95\\}$\n\nFirst, we compute the sums:\n$$ \\sum_{i=1}^{5} X_i' = 0 + 1 + 2 + 3 + 4 = 10 $$\n$$ \\sum_{i=1}^{5} Y_i' = 0.45 + 1.25 + 2.20 + 3.05 + 3.95 = 10.90 $$\nNext, we compute the sample means:\n$$ \\overline{X'} = \\frac{1}{n}\\sum_{i=1}^{n} X_i' = \\frac{10}{5} = 2 $$\n$$ \\overline{Y'} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i' = \\frac{10.90}{5} = 2.18 $$\n\nNext, we use the computational formulas for the corrected sums of squares and cross-products, which are more convenient than using the definitional formulas.\n$S_{X'X'} = \\sum_{i=1}^{n} (X_i')^2 - \\frac{(\\sum_{i=1}^{n} X_i')^2}{n}$\n$S_{Y'Y'} = \\sum_{i=1}^{n} (Y_i')^2 - \\frac{(\\sum_{i=1}^{n} Y_i')^2}{n}$\n$S_{X'Y'} = \\sum_{i=1}^{n} X_i' Y_i' - \\frac{(\\sum_{i=1}^{n} X_i')(\\sum_{i=1}^{n} Y_i')}{n}$\n\nWe need the following additional sums:\n$$ \\sum_{i=1}^{5} (X_i')^2 = 0^2 + 1^2 + 2^2 + 3^2 + 4^2 = 0 + 1 + 4 + 9 + 16 = 30 $$\n$$ \\sum_{i=1}^{5} (Y_i')^2 = (0.45)^2 + (1.25)^2 + (2.20)^2 + (3.05)^2 + (3.95)^2 $$\n$$ = 0.2025 + 1.5625 + 4.84 + 9.3025 + 15.6025 = 31.51 $$\n$$ \\sum_{i=1}^{5} X_i' Y_i' = (0)(0.45) + (1)(1.25) + (2)(2.20) + (3)(3.05) + (4)(3.95) $$\n$$ = 0 + 1.25 + 4.40 + 9.15 + 15.80 = 30.60 $$\n\nNow we can compute $S_{X'X'}$, $S_{Y'Y'}$, and $S_{X'Y'}$:\n$$ S_{X'X'} = 30 - \\frac{(10)^2}{5} = 30 - \\frac{100}{5} = 30 - 20 = 10 $$\n$$ S_{Y'Y'} = 31.51 - \\frac{(10.90)^2}{5} = 31.51 - \\frac{118.81}{5} = 31.51 - 23.762 = 7.748 $$\n$$ S_{X'Y'} = 30.60 - \\frac{(10)(10.90)}{5} = 30.60 - \\frac{109}{5} = 30.60 - 21.80 = 8.80 $$\n\nFinally, we substitute these values into the formula for $R^2$:\n$$ R^2 = \\frac{S_{X'Y'}^2}{S_{X'X'} S_{Y'Y'}} = \\frac{(8.80)^2}{(10)(7.748)} = \\frac{77.44}{77.48} $$\n$$ R^2 \\approx 0.9994837377... $$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $9, 9, 9, 4$. The fifth significant figure is $8$, so we round the fourth one up.\n$$ R^2 \\approx 0.9995 $$\nThis value indicates that approximately $99.95\\%$ of the variance in the logarithm of the biomarker concentration is explained by the logarithm of the drug dose, signifying an exceptionally strong linear fit on the transformed scale.",
            "answer": "$$ \\boxed{0.9995} $$"
        },
        {
            "introduction": "In multiple regression, a key challenge is to attribute the explained variance to individual predictors. This next practice  introduces the concepts of partial and semi-partial coefficients of determination, which measure a variable's contribution after accounting for other predictors. By calculating these values from the sums of squared errors of nested models, you will learn how to isolate and quantify the unique explanatory power of a single predictor within a larger model.",
            "id": "4914694",
            "problem": "A biostatistics study analyzes systolic blood pressure using a multiple linear regression model with predictors age, body mass index, and plasma sodium concentration. The response variable is continuous systolic blood pressure. Assume the standard linear model with independent, homoscedastic errors holds. Let the model with all three predictors be referred to as the full model and the model excluding plasma sodium (but including age and body mass index) be referred to as the reduced model. You are given the following least squares summary quantities: the total sum of squares $SST = 24000$, the sum of squared errors (SSE) for the reduced model $SSE_{\\mathrm{red}} = 18000$, and the sum of squared errors for the full model $SSE_{\\mathrm{full}} = 17100$. Starting from the core definitions of the total sum of squares, residual sum of squares, and the coefficient of determination ($R^2$), derive the expressions needed to compute both the partial coefficient of determination for plasma sodium (controlling for age and body mass index) and the semi-partial coefficient of determination for plasma sodium. Using these derivations, compute the ratio of the partial coefficient of determination to the semi-partial coefficient of determination for plasma sodium. Express your final answer as a pure number (no units).",
            "solution": "The problem statement is valid. It presents a clear, well-defined scenario in biostatistics and provides all the necessary numerical values ($SST$, $SSE_{\\mathrm{red}}$, and $SSE_{\\mathrm{full}}$) to calculate the requested quantities. The concepts of partial and semi-partial coefficients of determination are central to understanding variable contribution in multiple regression.\n\nLet $Y$ be the response variable (systolic blood pressure) and $\\mathbf{X}_{\\mathrm{red}}$ be the set of predictors {age, body mass index}. Let $X_p$ be the additional predictor (plasma sodium). The full set of predictors is $\\mathbf{X}_{\\mathrm{full}} = \\{ \\mathbf{X}_{\\mathrm{red}}, X_p \\}$. We are given:\n*   Total Sum of Squares: $SST = 24000$\n*   Sum of Squared Errors (Reduced Model): $SSE_{\\mathrm{red}} = 18000$\n*   Sum of Squared Errors (Full Model): $SSE_{\\mathrm{full}} = 17100$\n\nThe **partial coefficient of determination** for $X_p$, controlling for $\\mathbf{X}_{\\mathrm{red}}$, measures the proportion of variance in $Y$ that was *unexplained by the reduced model* but is subsequently explained by adding $X_p$. The variance unexplained by the reduced model is $SSE_{\\mathrm{red}}$. The reduction in this unexplained variance upon adding $X_p$ is $SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}$.\nTherefore, the partial coefficient of determination, denoted $r^2_{Y,p|\\mathrm{red}}$, is:\n$$ r^2_{Y,p|\\mathrm{red}} = \\frac{SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}}{SSE_{\\mathrm{red}}} $$\nNumerically, this is:\n$$ r^2_{Y,p|\\mathrm{red}} = \\frac{18000 - 17100}{18000} = \\frac{900}{18000} = \\frac{1}{20} = 0.05 $$\n\nThe **semi-partial coefficient of determination** for $X_p$ measures the proportion of the *total variance* in $Y$ that is *uniquely* explained by $X_p$ (after accounting for the influence of $\\mathbf{X}_{\\mathrm{red}}$). The unique variance explained by $X_p$ is also $SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}$. This is expressed as a fraction of the total sum of squares, $SST$.\nTherefore, the semi-partial coefficient of determination, denoted $r^2_{Y(p|\\mathrm{red})}$, is:\n$$ r^2_{Y(p|\\mathrm{red})} = \\frac{SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}}{SST} $$\nNumerically, this is:\n$$ r^2_{Y(p|\\mathrm{red})} = \\frac{18000 - 17100}{24000} = \\frac{900}{24000} = \\frac{9}{240} = \\frac{3}{80} = 0.0375 $$\n\nThe problem asks for the ratio of the partial coefficient of determination to the semi-partial coefficient of determination for plasma sodium.\n$$ \\text{Ratio} = \\frac{r^2_{Y,p|\\mathrm{red}}}{r^2_{Y(p|\\mathrm{red})}} = \\frac{\\left( \\frac{SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}}{SSE_{\\mathrm{red}}} \\right)}{\\left( \\frac{SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}}{SST} \\right)} $$\nThe term $(SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}})$ cancels out, simplifying the expression to:\n$$ \\text{Ratio} = \\frac{SST}{SSE_{\\mathrm{red}}} $$\nSubstituting the given values:\n$$ \\text{Ratio} = \\frac{24000}{18000} = \\frac{24}{18} = \\frac{4}{3} $$",
            "answer": "$$\n\\boxed{\\frac{4}{3}}\n$$"
        },
        {
            "introduction": "The principle of partitioning variance is not limited to regression with continuous predictors. This final exercise  demonstrates how the coefficient of determination extends naturally to the Analysis of Variance (ANOVA) framework, where predictors are categorical. You will use summary statistics from an ANOVA to calculate $R^2$ as the ratio of between-groups variability to total variability, solidifying its interpretation as the proportion of variance explained and linking it to the widely used Cohen’s $f$ effect size.",
            "id": "4914695",
            "problem": "A biostatistics team investigates the mean reduction in systolic blood pressure after eight weeks under three antihypertensive regimens using a one-way fixed-effects Analysis of Variance (ANOVA). Group-specific summaries are:\n\n- Treatment A: sample size $n_{1} = 40$, group mean $\\bar{y}_{1} = 8.2$, unbiased sample variance $s_{1}^{2} = 14.0$.\n- Treatment B: sample size $n_{2} = 35$, group mean $\\bar{y}_{2} = 5.7$, unbiased sample variance $s_{2}^{2} = 11.5$.\n- Treatment C: sample size $n_{3} = 45$, group mean $\\bar{y}_{3} = 10.1$, unbiased sample variance $s_{3}^{2} = 18.3$.\n\nTreat the data as arising from the canonical one-way ANOVA model $y_{ij} = \\mu + \\tau_{j} + \\varepsilon_{ij}$ with independent errors and homogeneous group-level variances estimated by the provided unbiased sample variances. Using the core definition that the coefficient of determination (denoted $R^{2}$) is the proportion of total variability in the outcome accounted for by the model, and that standardized ANOVA effect size is defined through the ratio of explained to unexplained variability, compute the Cohen’s $f$ effect size for this design.\n\nRound your final answer for Cohen’s $f$ to four significant figures. Express the final value as a pure number with no units.",
            "solution": "The one-way fixed-effects Analysis of Variance (ANOVA) model $y_{ij} = \\mu + \\tau_{j} + \\varepsilon_{ij}$ partitions the total variability of the outcome into variability between group means and variability within groups. Let the total sample size be $N = \\sum_{j=1}^{3} n_{j}$. The weighted overall mean is\n$$\n\\bar{y} \\;=\\; \\frac{\\sum_{j=1}^{3} n_{j}\\,\\bar{y}_{j}}{N}.\n$$\nThe between-groups sum of squares is\n$$\n\\mathrm{SSB} \\;=\\; \\sum_{j=1}^{3} n_{j}\\,(\\bar{y}_{j} - \\bar{y})^{2},\n$$\nand the within-groups (error) sum of squares is, by the definition of unbiased sample variance within each group,\n$$\n\\mathrm{SSE} \\;=\\; \\sum_{j=1}^{3} (n_{j} - 1)\\,s_{j}^{2}.\n$$\nThe total sum of squares is $\\mathrm{SST} = \\mathrm{SSB} + \\mathrm{SSE}$. The coefficient of determination is, by definition of proportion of variability explained,\n$$\nR^{2} \\;=\\; \\frac{\\mathrm{SSB}}{\\mathrm{SST}}.\n$$\nFor the standardized ANOVA effect size, Cohen’s $f$ is defined by $f^{2} = \\frac{R^{2}}{1 - R^{2}}$, and $f = \\sqrt{f^{2}}$.\n\nWe compute each quantity from the provided summaries.\n\nFirst, the total sample size is\n$$\nN \\;=\\; n_{1} + n_{2} + n_{3} \\;=\\; 40 + 35 + 45 \\;=\\; 120.\n$$\nThe weighted overall mean is\n$$\n\\bar{y} \\;=\\; \\frac{n_{1}\\bar{y}_{1} + n_{2}\\bar{y}_{2} + n_{3}\\bar{y}_{3}}{N}\n\\;=\\; \\frac{40 \\cdot 8.2 + 35 \\cdot 5.7 + 45 \\cdot 10.1}{120}\n\\;=\\; \\frac{328 + 199.5 + 454.5}{120}\n\\;=\\; \\frac{982.0}{120}\n\\;=\\; 8.183\\overline{3}.\n$$\nBetween-groups sum of squares:\n$$\n\\begin{aligned}\n\\mathrm{SSB}\n&= 40\\,(8.2 - 8.183\\overline{3})^{2}\n+ 35\\,(5.7 - 8.183\\overline{3})^{2}\n+ 45\\,(10.1 - 8.183\\overline{3})^{2}\n\\\\\n&= 40\\,(0.016\\overline{6})^{2}\n+ 35\\,(-2.483\\overline{3})^{2}\n+ 45\\,(1.916\\overline{6})^{2}.\n\\end{aligned}\n$$\nCompute each term:\n$$\n40\\,(0.016\\overline{6})^{2} \\;=\\; 40 \\cdot 0.000277\\overline{7} \\;=\\; 0.011111\\ldots,\n$$\n$$\n35\\,(-2.483\\overline{3})^{2} \\;\\approx\\; 35 \\cdot 6.166944\\ldots \\;=\\; 215.843055\\ldots,\n$$\n$$\n45\\,(1.916\\overline{6})^{2} \\;=\\; 45 \\cdot 3.673611\\ldots \\;=\\; 165.3125.\n$$\nHence,\n$$\n\\mathrm{SSB} \\;\\approx\\; 0.011111 + 215.843056 + 165.3125 \\;=\\; 381.166667.\n$$\nWithin-groups sum of squares:\n$$\n\\mathrm{SSE} \\;=\\; (40-1)\\cdot 14.0 \\;+\\; (35-1)\\cdot 11.5 \\;+\\; (45-1)\\cdot 18.3\n\\;=\\; 39\\cdot 14.0 \\;+\\; 34\\cdot 11.5 \\;+\\; 44\\cdot 18.3,\n$$\nso\n$$\n\\mathrm{SSE} \\;=\\; 546 \\;+\\; 391 \\;+\\; 805.2 \\;=\\; 1{,}742.2.\n$$\nTotal sum of squares:\n$$\n\\mathrm{SST} \\;=\\; \\mathrm{SSB} + \\mathrm{SSE} \\;\\approx\\; 381.166667 + 1{,}742.2 \\;=\\; 2{,}123.366667.\n$$\nTherefore,\n$$\nR^{2} \\;=\\; \\frac{\\mathrm{SSB}}{\\mathrm{SST}} \\;\\approx\\; \\frac{381.166667}{2{,}123.366667}\n\\;\\approx\\; 0.1795.\n$$\nNow,\n$$\nf^{2} \\;=\\; \\frac{R^{2}}{1 - R^{2}} \\;=\\; \\frac{\\mathrm{SSB}}{\\mathrm{SSE}}\n\\;\\approx\\; \\frac{381.166667}{1742.2}\n\\;\\approx\\; 0.2187847,\n$$\nand\n$$\nf \\;=\\; \\sqrt{f^{2}} \\;\\approx\\; \\sqrt{0.2187847} \\;\\approx\\; 0.467744.\n$$\nRounded to four significant figures, the Cohen’s $f$ effect size is\n$$\n0.4677.\n$$",
            "answer": "$$\\boxed{0.4677}$$"
        }
    ]
}