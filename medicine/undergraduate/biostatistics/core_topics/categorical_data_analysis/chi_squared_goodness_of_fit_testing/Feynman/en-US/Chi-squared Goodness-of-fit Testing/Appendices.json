{
    "hands_on_practices": [
        {
            "introduction": "The chi-squared goodness-of-fit test is a powerful tool for comparing observed data to a theoretical model, but its application becomes more nuanced when the model's parameters must be estimated from the data itself. This practice problem  guides you through a classic case from population genetics: testing for Hardy–Weinberg equilibrium. You will derive the expected genotype frequencies based on an estimated allele frequency and learn how this estimation step affects the degrees of freedom for the test statistic.",
            "id": "4899472",
            "problem": "A biostatistician is testing whether a biallelic locus with alleles $A$ and $a$ is in Hardy–Weinberg equilibrium (HWE) in a large, randomly mating population. A simple random sample of $n=500$ unrelated diploid individuals yields the following observed genotype counts: $n_{AA}=310$, $n_{Aa}=150$, $n_{aa}=40$. Under the null hypothesis that the population is in HWE, the genotype probabilities are determined by a single allele frequency parameter $p$ for allele $A$.\n\nStarting from the multinomial sampling model for the genotype counts and the Hardy–Weinberg equilibrium principle, do the following:\n\n- Derive the maximum likelihood estimate $\\hat{p}$ of the allele $A$ frequency under the null hypothesis that the population is in HWE.\n- Using this estimate, construct the expected genotype counts under HWE for the three genotype categories.\n- Using the Pearson chi-squared goodness-of-fit framework, compute the test statistic that compares the observed counts to the expected counts under HWE.\n\nExpress your final answer as the single numeric value of the chi-squared test statistic, rounded to four significant figures. Do not compute a $p$-value. No units are required for the final answer.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. All necessary data for a unique solution are provided: a sample size of $n=500$ individuals and observed genotype counts $n_{AA}=310$, $n_{Aa}=150$, and $n_{aa}=40$. The sum of counts $310+150+40=500$ matches the total sample size $n$. The task is to perform a standard Pearson's chi-squared goodness-of-fit test for Hardy–Weinberg equilibrium (HWE), which is a conventional procedure in biostatistics. We proceed with the solution.\n\nThe solution involves three main steps as requested:\n1.  Derivation of the maximum likelihood estimate (MLE) of the allele $A$ frequency, denoted as $p$.\n2.  Calculation of the expected genotype counts under the HWE null hypothesis using the MLE.\n3.  Computation of the Pearson's chi-squared test statistic.\n\n**Step 1: Derivation of the Maximum Likelihood Estimate ($\\hat{p}$)**\n\nUnder the null hypothesis of HWE, the probabilities of observing the genotypes $AA$, $Aa$, and $aa$ are given by $p^2$, $2p(1-p)$, and $(1-p)^2$ respectively, where $p$ is the frequency of allele $A$. The observed counts $(n_{AA}, n_{Aa}, n_{aa})$ follow a multinomial distribution with total size $n$ and these probabilities.\n\nThe likelihood function $L(p)$ for the observed data is proportional to the probability mass function of the multinomial distribution:\n$$L(p | n_{AA}, n_{Aa}, n_{aa}) \\propto (p^2)^{n_{AA}} [2p(1-p)]^{n_{Aa}} [(1-p)^2]^{n_{aa}}$$\nTo find the MLE, we maximize the log-likelihood function, $\\ell(p) = \\ln(L(p))$:\n$$\\ell(p) = \\ln\\left( (p^2)^{n_{AA}} [2p(1-p)]^{n_{Aa}} [(1-p)^2]^{n_{aa}} \\right) + C$$\nwhere $C$ is a constant that does not depend on $p$.\n$$\\ell(p) = n_{AA}\\ln(p^2) + n_{Aa}\\ln(2p(1-p)) + n_{aa}\\ln((1-p)^2) + C$$\n$$\\ell(p) = 2n_{AA}\\ln(p) + n_{Aa}(\\ln(2) + \\ln(p) + \\ln(1-p)) + 2n_{aa}\\ln(1-p) + C$$\nGrouping terms involving $p$:\n$$\\ell(p) = (2n_{AA} + n_{Aa})\\ln(p) + (n_{Aa} + 2n_{aa})\\ln(1-p) + n_{Aa}\\ln(2) + C$$\nTo find the value of $p$ that maximizes $\\ell(p)$, we take the first derivative with respect to $p$ and set it to zero:\n$$\\frac{d\\ell}{dp} = \\frac{2n_{AA} + n_{Aa}}{p} - \\frac{n_{Aa} + 2n_{aa}}{1-p} = 0$$\nSolving for the MLE, $\\hat{p}$:\n$$\\frac{2n_{AA} + n_{Aa}}{\\hat{p}} = \\frac{n_{Aa} + 2n_{aa}}{1-\\hat{p}}$$\n$$(1-\\hat{p})(2n_{AA} + n_{Aa}) = \\hat{p}(n_{Aa} + 2n_{aa})$$\n$$2n_{AA} + n_{Aa} - \\hat{p}(2n_{AA} + n_{Aa}) = \\hat{p}(n_{Aa} + 2n_{aa})$$\n$$2n_{AA} + n_{Aa} = \\hat{p}(2n_{AA} + n_{Aa} + n_{Aa} + 2n_{aa})$$\n$$2n_{AA} + n_{Aa} = \\hat{p}(2n_{AA} + 2n_{Aa} + 2n_{aa})$$\nSince $n = n_{AA} + n_{Aa} + n_{aa}$, the term in the parenthesis is $2n$.\n$$2n_{AA} + n_{Aa} = \\hat{p}(2n)$$\nThus, the MLE for the frequency of allele $A$ is:\n$$\\hat{p} = \\frac{2n_{AA} + n_{Aa}}{2n}$$\nThis is the \"gene counting\" estimator: the total number of $A$ alleles in the sample divided by the total number of alleles ($2n$).\n\nSubstituting the given values $n_{AA}=310$, $n_{Aa}=150$, and $n=500$:\n$$\\hat{p} = \\frac{2(310) + 150}{2(500)} = \\frac{620 + 150}{1000} = \\frac{770}{1000} = 0.77$$\nThe estimated frequency of allele $a$ is $\\hat{q} = 1 - \\hat{p} = 1 - 0.77 = 0.23$.\n\n**Step 2: Construction of Expected Genotype Counts**\n\nThe expected genotype counts under HWE are calculated using the estimated allele frequencies.\nThe expected probabilities are:\n$P(AA) = \\hat{p}^2 = (0.77)^2 = 0.5929$\n$P(Aa) = 2\\hat{p}\\hat{q} = 2(0.77)(0.23) = 0.3542$\n$P(aa) = \\hat{q}^2 = (0.23)^2 = 0.0529$\n\nThe expected counts ($E$) are found by multiplying these probabilities by the total sample size $n=500$:\n$E_{AA} = n \\times \\hat{p}^2 = 500 \\times 0.5929 = 296.45$\n$E_{Aa} = n \\times 2\\hat{p}\\hat{q} = 500 \\times 0.3542 = 177.1$\n$E_{aa} = n \\times \\hat{q}^2 = 500 \\times 0.0529 = 26.45$\nAs a check, the sum of expected counts is $296.45 + 177.1 + 26.45 = 500$, which equals the total sample size.\n\n**Step 3: Computation of the Chi-Squared Test Statistic**\n\nThe Pearson's chi-squared ($\\chi^2$) test statistic is calculated as the sum of the squared differences between observed ($O$) and expected ($E$) counts, divided by the expected counts for each category:\n$$\\chi^2 = \\sum_{i \\in \\{AA, Aa, aa\\}} \\frac{(O_i - E_i)^2}{E_i}$$\nThe observed counts are $O_{AA}=310$, $O_{Aa}=150$, and $O_{aa}=40$.\nThe expected counts are $E_{AA}=296.45$, $E_{Aa}=177.1$, and $E_{aa}=26.45$.\n\nSubstituting these values into the formula:\n$$\\chi^2 = \\frac{(310 - 296.45)^2}{296.45} + \\frac{(150 - 177.1)^2}{177.1} + \\frac{(40 - 26.45)^2}{26.45}$$\n$$\\chi^2 = \\frac{(13.55)^2}{296.45} + \\frac{(-27.1)^2}{177.1} + \\frac{(13.55)^2}{26.45}$$\n$$\\chi^2 = \\frac{183.6025}{296.45} + \\frac{734.41}{177.1} + \\frac{183.6025}{26.45}$$\nNow, we compute the value of each term:\n$$\\chi^2 \\approx 0.619335 + 4.146866 + 6.941493$$\n$$\\chi^2 \\approx 11.707694$$\nRounding to four significant figures as requested gives $11.71$. This statistic would be compared to a $\\chi^2$ distribution with one degree of freedom, but the problem does not require this final step. The degrees of freedom are calculated as (number of categories) - $1$ - (number of estimated parameters), which is $3 - 1 - 1 = 1$.",
            "answer": "$$\\boxed{11.71}$$"
        },
        {
            "introduction": "A significant chi-squared statistic tells us that our model does not fit the data, but it doesn't tell us why. To diagnose the specific sources of this lack of fit, we must examine the contribution of each category using residuals. This exercise  introduces the concept of Pearson residuals, showing you how their sign and magnitude can pinpoint which categories have observed counts that deviate most from the model's expectations.",
            "id": "4899481",
            "problem": "A biostatistician is assessing the fit of a five-category model for mutation types in a large population. The null model specifies category probabilities $\\{p_1, p_2, p_3, p_4, p_5\\} = \\{0.10, 0.20, 0.25, 0.30, 0.15\\}$. A sample of size $n = 400$ yields observed counts $\\{O_1, O_2, O_3, O_4, O_5\\} = \\{48, 70, 110, 130, 42\\}$. The expected counts under the null are $\\{E_1, E_2, E_3, E_4, E_5\\} = \\{40, 80, 100, 120, 60\\}$ because $E_i = n p_i$. Starting from the core setup of the chi-squared goodness-of-fit test—namely that, under the null model for large samples, each cell count is modeled as a random variable with mean $E_i$ and a known variance determined by the sampling scheme—construct a cell-level standardized residual that aggregates to the Pearson chi-squared statistic. Then, interpret how the sign and magnitude of this residual diagnose lack of fit in individual cells. Finally, apply your result to the data above.\n\nSelect all statements that are correct.\n\nA. Under an independent Poisson sampling scheme for cell counts with mean $E_i$, the appropriate cell-wise standardized residual equals the difference between observed and expected divided by the square root of the variance, and therefore the residual is $(O_i - E_i)/\\sqrt{E_i}$. A positive value indicates more counts than the model expects; the contribution of cell $i$ to the Pearson chi-squared statistic is the squared residual $r_i^2$.\n\nB. Under a fixed-total multinomial model, the variance of the cell count $O_i$ leads to the standardized residual $(O_i - E_i)/\\sqrt{E_i \\left(1 - E_i/n\\right)}$, and this quantity is the Pearson residual used in chi-squared goodness-of-fit testing.\n\nC. For the given data, the residual for category $5$ is approximately $-2.32$, which indicates about $2.32$ standard deviations fewer counts than expected under the model; among the five categories, category $5$ has the largest absolute residual and is the primary driver of lack of fit.\n\nD. The sign of the Pearson residual is not meaningful; negative residuals indicate more counts than expected, and only the absolute value $|r_i|$ matters for diagnosing lack of fit.\n\nE. For the Pearson chi-squared statistic computed from these five categories, the degrees of freedom is $5$ regardless of whether any parameters are estimated in the model for the cell probabilities.",
            "solution": "To determine the correct statements, we analyze the definition and properties of Pearson residuals in the context of a chi-squared goodness-of-fit test. The Pearson residual for a category *i* is defined as $r_i = (O_i - E_i) / \\sqrt{E_i}$. The sum of the squares of these residuals, $\\sum r_i^2$, equals the Pearson chi-squared statistic, $\\chi^2$.\n\nLet's evaluate each option:\n\n**A. Correct.** This statement accurately describes the Pearson residual under the common Poisson sampling model approximation, where the variance of a count $O_i$ is equal to its mean, $E_i$. The formula for the residual, $r_i = (O_i - E_i)/\\sqrt{E_i}$, is correct. A positive value implies $O_i > E_i$ (more counts than expected), and its square, $r_i^2$, is indeed the contribution of cell *i* to the total $\\chi^2$ statistic.\n\n**B. Incorrect.** The variance of a cell count $O_i$ in a multinomial model is $n p_i (1-p_i) = E_i(1 - E_i/n)$. The residual standardized by this variance, $(O_i - E_i)/\\sqrt{E_i(1 - E_i/n)}$, is known as the \"standardized residual\" or \"adjusted residual,\" not the \"Pearson residual.\" The Pearson residual uses $\\sqrt{E_i}$ in the denominator, which corresponds to the Poisson variance approximation.\n\n**C. Correct.** We must calculate the residuals for all categories to verify this claim.\n- Observed counts $O = \\{48, 70, 110, 130, 42\\}$\n- Expected counts $E = \\{40, 80, 100, 120, 60\\}$\n- Residuals:\n    - $r_1 = (48 - 40) / \\sqrt{40} \\approx 1.26$\n    - $r_2 = (70 - 80) / \\sqrt{80} \\approx -1.12$\n    - $r_3 = (110 - 100) / \\sqrt{100} = 1.00$\n    - $r_4 = (130 - 120) / \\sqrt{120} \\approx 0.91$\n    - $r_5 = (42 - 60) / \\sqrt{60} \\approx -2.32$\nThe residual for category 5 is approximately $-2.32$, indicating a deficit of counts of about 2.32 standard deviations. Comparing the absolute values $\\{1.26, 1.12, 1.00, 0.91, 2.32\\}$, category 5 indeed has the largest absolute residual and is therefore the primary contributor to the lack of fit.\n\n**D. Incorrect.** This statement makes two false claims. First, the sign of the residual is highly meaningful, indicating whether the observed count is a surplus ($+$) or a deficit ($-$) compared to the expectation. Second, it incorrectly states that negative residuals indicate more counts than expected; the opposite is true.\n\n**E. Incorrect.** The degrees of freedom ($df$) for a chi-squared goodness-of-fit test are calculated as $df = k - 1 - m$, where $k$ is the number of categories and $m$ is the number of independent parameters estimated from the data. Here, $k=5$ and the probabilities are pre-specified ($m=0$), so $df = 5 - 1 - 0 = 4$. The statement incorrectly gives the degrees of freedom as $5$ (which is $k$) and falsely claims the value is independent of parameter estimation.\n\nTherefore, statements A and C are the only correct ones.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "The Chi-squared goodness-of-fit framework is not limited to testing simple, fully specified probability models, but can be adapted to evaluate more complex scientific hypotheses that impose structural relationships among category probabilities. In this problem , you will test a null hypothesis with equality constraints. This challenge requires a careful derivation of the maximum likelihood estimates for the constrained parameters and a correct determination of the test's degrees of freedom, showcasing the test's versatility.",
            "id": "4899436",
            "problem": "A public health laboratory classifies bacterial isolates into $6$ phenotypic categories based on biochemical tests. In a surveillance sample of $n=300$ isolates, the observed counts are:\n- Category $1$: $42$\n- Category $2$: $58$\n- Category $3$: $30$\n- Category $4$: $41$\n- Category $5$: $29$\n- Category $6$: $100$\n\nModel the vector of counts as a single draw from a multinomial distribution with parameter vector $\\mathbf{p}=(p_{1},p_{2},p_{3},p_{4},p_{5},p_{6})$ and total $n=300$. You wish to test a scientifically motivated null hypothesis that imposes equality constraints among certain cell probabilities: under $H_{0}$, the first two categories are equally likely and the next three categories are equally likely, that is,\n$$\nH_{0}:\\; p_{1}=p_{2},\\quad p_{3}=p_{4}=p_{5},\\quad p_{i}\\ge 0,\\quad \\sum_{i=1}^{6}p_{i}=1.\n$$\n\nStarting only from the multinomial model definition and the definition of the Maximum Likelihood Estimator (MLE), and without assuming any prederived formulas for test degrees of freedom:\n1) Derive the MLE of the constrained parameter under $H_{0}$ and the corresponding expected counts.\n2) Construct the Pearson goodness-of-fit statistic from first principles.\n3) Explain how the equality constraints change the dimensionality of the parameter space under $H_{0}$ and use this to derive the appropriate large-sample reference distribution for the test statistic, including its degrees of freedom (df), justifying your result from the asymptotic behavior of multinomial counts and parameter estimation under constraints.\n\nFinally, using your derived reference distribution and the observed data, compute the large-sample $p$-value for the test. Round your final numerical answer to four significant figures and express it as a pure decimal (no percent sign).",
            "solution": "**1. MLE of the Constrained Parameter and Expected Counts**\n\nThe vector of observed counts is $\\mathbf{O} = (O_1, O_2, O_3, O_4, O_5, O_6) = (42, 58, 30, 41, 29, 100)$. The total number of isolates is $n=300$. The data are modeled as a single draw from a multinomial distribution with $k=6$ categories and parameter vector $\\mathbf{p} = (p_1, p_2, p_3, p_4, p_5, p_6)$. The likelihood function is given by:\n$$\nL(\\mathbf{p} | \\mathbf{O}) \\propto \\prod_{i=1}^{6} p_i^{O_i} = p_1^{42} p_2^{58} p_3^{30} p_4^{41} p_5^{29} p_6^{100}\n$$\nThe null hypothesis $H_0$ imposes the constraints $p_1=p_2$ and $p_3=p_4=p_5$. We introduce a smaller set of parameters to reflect these constraints. Let $\\theta_A = p_1 = p_2$, $\\theta_B = p_3 = p_4 = p_5$, and $\\theta_C = p_6$. The probability vector under $H_0$ is $(\\theta_A, \\theta_A, \\theta_B, \\theta_B, \\theta_B, \\theta_C)$. The condition $\\sum p_i = 1$ becomes a constraint on the new parameters:\n$$\n2\\theta_A + 3\\theta_B + \\theta_C = 1\n$$\nTo find the Maximum Likelihood Estimator (MLE) of the parameters under $H_0$, we maximize the likelihood function subject to this constraint. It is equivalent and simpler to maximize the log-likelihood function, $\\ell(\\mathbf{p}) = \\ln(L(\\mathbf{p}))$. Substituting the constrained parameters:\n$$\n\\ell(\\theta_A, \\theta_B, \\theta_C) = 42\\ln(\\theta_A) + 58\\ln(\\theta_A) + 30\\ln(\\theta_B) + 41\\ln(\\theta_B) + 29\\ln(\\theta_B) + 100\\ln(\\theta_C)\n$$\n$$\n\\ell(\\theta_A, \\theta_B, \\theta_C) = (42+58)\\ln(\\theta_A) + (30+41+29)\\ln(\\theta_B) + 100\\ln(\\theta_C)\n$$\n$$\n\\ell(\\theta_A, \\theta_B, \\theta_C) = 100\\ln(\\theta_A) + 100\\ln(\\theta_B) + 100\\ln(\\theta_C)\n$$\nWe use the method of Lagrange multipliers to maximize $\\ell$ subject to $g(\\theta_A, \\theta_B, \\theta_C) = 2\\theta_A + 3\\theta_B + \\theta_C - 1 = 0$. The Lagrangian is:\n$$\n\\mathcal{L}(\\theta_A, \\theta_B, \\theta_C, \\lambda) = 100\\ln(\\theta_A) + 100\\ln(\\theta_B) + 100\\ln(\\theta_C) - \\lambda(2\\theta_A + 3\\theta_B + \\theta_C - 1)\n$$\nWe find the partial derivatives and set them to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_A} = \\frac{100}{\\theta_A} - 2\\lambda = 0 \\implies \\theta_A = \\frac{100}{2\\lambda} = \\frac{50}{\\lambda}\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_B} = \\frac{100}{\\theta_B} - 3\\lambda = 0 \\implies \\theta_B = \\frac{100}{3\\lambda}\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_C} = \\frac{100}{\\theta_C} - \\lambda = 0 \\implies \\theta_C = \\frac{100}{\\lambda}\n$$\nSubstitute these into the constraint equation:\n$$\n2\\left(\\frac{50}{\\lambda}\\right) + 3\\left(\\frac{100}{3\\lambda}\\right) + \\frac{100}{\\lambda} = 1\n$$\n$$\n\\frac{100}{\\lambda} + \\frac{100}{\\lambda} + \\frac{100}{\\lambda} = 1 \\implies \\frac{300}{\\lambda} = 1 \\implies \\lambda = 300\n$$\nNow we find the MLEs for the parameters under $H_0$:\n$$\n\\hat{\\theta}_A = \\frac{50}{300} = \\frac{1}{6}\n$$\n$$\n\\hat{\\theta}_B = \\frac{100}{3 \\times 300} = \\frac{1}{9}\n$$\n$$\n\\hat{\\theta}_C = \\frac{100}{300} = \\frac{1}{3}\n$$\nThe MLE for the full parameter vector $\\mathbf{p}$ under $H_0$ is:\n$$\n\\hat{\\mathbf{p}}_{H_0} = (\\hat{p}_1, \\hat{p}_2, \\hat{p}_3, \\hat{p}_4, \\hat{p}_5, \\hat{p}_6) = \\left(\\frac{1}{6}, \\frac{1}{6}, \\frac{1}{9}, \\frac{1}{9}, \\frac{1}{9}, \\frac{1}{3}\\right)\n$$\nThe expected counts, $E_i$, under $H_0$ are calculated as $E_i = n\\hat{p}_i$:\n- $E_1 = n\\hat{p}_1 = 300 \\times \\frac{1}{6} = 50$\n- $E_2 = n\\hat{p}_2 = 300 \\times \\frac{1}{6} = 50$\n- $E_3 = n\\hat{p}_3 = 300 \\times \\frac{1}{9} = \\frac{100}{3}$\n- $E_4 = n\\hat{p}_4 = 300 \\times \\frac{1}{9} = \\frac{100}{3}$\n- $E_5 = n\\hat{p}_5 = 300 \\times \\frac{1}{9} = \\frac{100}{3}$\n- $E_6 = n\\hat{p}_6 = 300 \\times \\frac{1}{3} = 100$\n\n**2. Pearson Goodness-of-Fit Statistic**\n\nThe Pearson goodness-of-fit statistic, $\\chi^2$, is defined as the sum of squared differences between observed and expected counts, standardized by the expected counts:\n$$\n\\chi^2 = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}\n$$\nUsing the observed counts $\\mathbf{O}=(42, 58, 30, 41, 29, 100)$ and the derived expected counts:\n$$\n\\chi^2 = \\frac{(42 - 50)^2}{50} + \\frac{(58 - 50)^2}{50} + \\frac{(30 - 100/3)^2}{100/3} + \\frac{(41 - 100/3)^2}{100/3} + \\frac{(29 - 100/3)^2}{100/3} + \\frac{(100 - 100)^2}{100}\n$$\n$$\n\\chi^2 = \\frac{(-8)^2}{50} + \\frac{8^2}{50} + \\frac{(-10/3)^2}{100/3} + \\frac{(23/3)^2}{100/3} + \\frac{(-13/3)^2}{100/3} + 0\n$$\n$$\n\\chi^2 = \\frac{64}{50} + \\frac{64}{50} + \\frac{100/9}{100/3} + \\frac{529/9}{100/3} + \\frac{169/9}{100/3}\n$$\n$$\n\\chi^2 = 1.28 + 1.28 + \\frac{100}{9}\\frac{3}{100} + \\frac{529}{9}\\frac{3}{100} + \\frac{169}{9}\\frac{3}{100}\n$$\n$$\n\\chi^2 = 2.56 + \\frac{1}{3} + \\frac{529}{300} + \\frac{169}{300}\n$$\nTo sum precisely, we use a common denominator of $300$:\n$$\n\\chi^2 = \\frac{256 \\times 3}{100 \\times 3} + \\frac{1 \\times 100}{3 \\times 100} + \\frac{529}{300} + \\frac{169}{300} = \\frac{768}{300} + \\frac{100}{300} + \\frac{529}{300} + \\frac{169}{300}\n$$\n$$\n\\chi^2 = \\frac{768 + 100 + 529 + 169}{300} = \\frac{1566}{300} = 5.22\n$$\n\n**3. Derivation of the Reference Distribution and Degrees of Freedom**\n\nThe large-sample reference distribution for the Pearson statistic is a chi-squared ($\\chi^2$) distribution. The degrees of freedom (df) are determined by the dimensionality of the parameter space.\n\n- **Unconstrained Model:** The full parameter space consists of the vector $\\mathbf{p} = (p_1, \\dots, p_6)$ subject to the single constraint $\\sum_{i=1}^6 p_i = 1$. The number of free parameters, and thus the dimension of this space, is $6 - 1 = 5$.\n\n- **Constrained Model ($H_0$):** The null hypothesis imposes additional equality constraints: $p_1 = p_2$ and $p_3 = p_4 = p_5$.\n  - The constraint $p_1 = p_2$ can be written as one linear equation: $p_1 - p_2 = 0$.\n  - The constraint $p_3 = p_4 = p_5$ can be written as two independent linear equations, for example: $p_3 - p_4 = 0$ and $p_4 - p_5 = 0$.\n  - These are $1+2=3$ independent constraints imposed on the parameters by $H_0$.\n\nThe dimension of the parameter space under $H_0$ is the dimension of the unconstrained space minus the number of independent constraints imposed by the hypothesis.\n$$\n\\text{dim}(H_0) = (6 - 1) - 3 = 2\n$$\nAlternatively, under $H_0$, the probability vector is determined by three parameters $(\\theta_A, \\theta_B, \\theta_C)$ subject to one constraint $2\\theta_A + 3\\theta_B + \\theta_C = 1$. The number of free parameters is thus $3 - 1 = 2$. For example, if we specify $\\theta_A$ and $\\theta_B$, then $\\theta_C$ is fixed.\n\nThe degrees of freedom for the chi-squared goodness-of-fit test are given by the formula:\n$$\ndf = (\\text{number of categories} - 1) - (\\text{number of independent parameters estimated})\n$$\nHere, the number of categories is $k=6$. The number of independent parameters estimated from the data to define the null hypothesis probabilities is the dimension of the parameter space under $H_0$, which is $2$.\n$$\ndf = (6 - 1) - 2 = 3\n$$\nThis result stems from the asymptotic theory of maximum likelihood estimators. The vector of standardized cell counts asymptotically follows a multivariate normal distribution. The Pearson statistic is a quadratic form of this vector, which follows a $\\chi^2$ distribution. Each parameter estimated from the data imposes an additional linear constraint on the cell count residuals, reducing the degrees of freedom of the resulting distribution by one. Since we estimated two independent parameters ($\\hat{\\theta}_A$ and $\\hat{\\theta}_B$) to find the expected counts, we lose two degrees of freedom from the baseline of $k-1=5$.\n\nThus, the appropriate large-sample reference distribution for the test statistic is a chi-squared distribution with $3$ degrees of freedom, denoted $\\chi^2_3$.\n\n**4. Large-Sample p-value**\n\nThe p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one calculated, assuming the null hypothesis is true. We compute $P(\\chi^2_3 \\ge 5.22)$.\nUsing a computational tool or a detailed chi-squared distribution table for $df=3$:\n$$\np\\text{-value} = P(\\chi^2_3 \\ge 5.22) \\approx 0.15638\n$$\nRounding to four significant figures as requested, the p-value is $0.1564$.\nSince this p-value is greater than conventional significance levels (e.g., $\\alpha = 0.05$), we would not reject the null hypothesis. The observed data are consistent with the proposed model where $p_1=p_2$ and $p_3=p_4=p_5$.",
            "answer": "$$\\boxed{0.1564}$$"
        }
    ]
}