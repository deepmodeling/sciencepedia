{
    "hands_on_practices": [
        {
            "introduction": "While the Pearson's chi-squared test is a powerful tool, its validity relies on large sample sizes. This first practice explores Fisher's Exact Test, a crucial alternative for small samples that computes an exact $p$-value without relying on asymptotic approximations. By deriving the test from first principles, you will solidify your understanding of how conditional probability and the hypergeometric distribution provide a rigorous foundation for analyzing categorical data, especially when assumptions for other tests are not met .",
            "id": "4899807",
            "problem": "A randomized biostatistics study investigates whether an antiviral therapy and the occurrence of seroconversion are independent. The study allocates $n_{T}=10$ participants to the therapy and $n_{C}=6$ participants to placebo, for a total of $N=16$ participants. The observed $2 \\times 2$ table shows $a_{\\text{obs}}=5$ seroconversions in the therapy arm and $b_{\\text{obs}}=2$ seroconversions in the placebo arm, so the observed total number of seroconversions is $K=a_{\\text{obs}}+b_{\\text{obs}}=7$ and the total number without seroconversion is $N-K=9$. Assume the null hypothesis of independence (homogeneity), that is, the seroconversion probability $p$ is the same in both arms.\n\nStarting from fundamental definitions, condition on the fixed row totals $n_{T}$ and $n_{C}$ and the fixed column totals $K$ and $N-K$. Derive the conditional distribution of the number $A$ of seroconversions in the therapy arm given the margins, and enumerate its probability values for all feasible $a \\in \\{\\max(0, K-n_{C}), \\ldots, \\min(n_{T}, K)\\}$. Using this enumeration, compute the two-sided Fisher’s exact test (Fisher’s Exact Test (FET)) $p$-value defined as the sum of the probabilities of all tables whose conditional probability is less than or equal to that of the observed table, that is, $\\sum_{a: \\mathbb{P}(A=a \\mid \\text{margins}) \\le \\mathbb{P}(A=a_{\\text{obs}} \\mid \\text{margins})} \\mathbb{P}(A=a \\mid \\text{margins})$. Express the final $p$-value as a decimal and round your answer to four significant figures.",
            "solution": "The problem asks for the derivation of the conditional distribution for the number of seroconversions in the therapy arm, its enumeration, and the computation of a two-sided $p$-value.\n\nLet $A$ be the random variable representing the number of seroconversions in the therapy arm, and $B$ be the random variable for the number of seroconversions in the placebo arm. The respective sample sizes are $n_{T}=10$ and $n_{C}=6$, with a total of $N = n_{T} + n_{C} = 16$ participants.\n\nUnder the null hypothesis ($H_0$) of homogeneity, the probability of seroconversion, denoted by $p$, is assumed to be identical for both arms. The number of seroconversions in each arm thus follows a binomial distribution, as the outcomes for individual participants are independent trials.\n$$ A \\sim \\text{Binomial}(n_{T}, p) \\implies \\mathbb{P}(A=a) = \\binom{n_{T}}{a} p^{a} (1-p)^{n_{T}-a} $$\n$$ B \\sim \\text{Binomial}(n_{C}, p) \\implies \\mathbb{P}(B=b) = \\binom{n_{C}}{b} p^{b} (1-p)^{n_{C}-b} $$\nThe two groups are independent, so the joint probability of observing $a$ events in the therapy arm and $b$ events in the placebo arm is $\\mathbb{P}(A=a, B=b) = \\mathbb{P}(A=a)\\mathbb{P}(B=b)$.\n\nFisher's Exact Test (FET) is based on the distribution of $A$ conditional on the fixed marginal totals of the contingency table. The row totals ($n_{T}=10, n_{C}=6$) are fixed by the study design. For the FET, we also condition on the column totals. The observed data are $a_{\\text{obs}}=5$ successes in the therapy arm and $b_{\\text{obs}}=2$ in the placebo arm, leading to a total of $K = a_{\\text{obs}} + b_{\\text{obs}} = 5+2=7$ seroconversions and $N-K = 16-7=9$ non-seroconversions.\n\nWe derive the conditional probability $\\mathbb{P}(A=a \\mid A+B=K)$. By the definition of conditional probability:\n$$ \\mathbb{P}(A=a \\mid A+B=K) = \\frac{\\mathbb{P}(A=a \\text{ and } A+B=K)}{\\mathbb{P}(A+B=K)} $$\nThe event in the numerator is equivalent to $\\{A=a \\text{ and } B=K-a\\}$. Due to independence, its probability is:\n$$ \\mathbb{P}(A=a, B=K-a) = \\mathbb{P}(A=a)\\mathbb{P}(B=K-a) = \\left[ \\binom{n_{T}}{a}p^{a}(1-p)^{n_{T}-a} \\right] \\left[ \\binom{n_{C}}{K-a}p^{K-a}(1-p)^{n_{C}-(K-a)} \\right] $$\n$$ = \\binom{n_{T}}{a}\\binom{n_{C}}{K-a} p^{a+K-a} (1-p)^{n_{T}-a+n_{C}-K+a} = \\binom{n_{T}}{a}\\binom{n_{C}}{K-a} p^{K} (1-p)^{N-K} $$\nThe random variable for the total number of successes, $A+B$, follows a binomial distribution $\\text{Binomial}(N, p)$, so the probability of the conditioning event is:\n$$ \\mathbb{P}(A+B=K) = \\binom{N}{K} p^{K} (1-p)^{N-K} $$\nSubstituting these into the conditional probability formula yields:\n$$ \\mathbb{P}(A=a \\mid A+B=K) = \\frac{\\binom{n_{T}}{a}\\binom{n_{C}}{K-a} p^{K} (1-p)^{N-K}}{\\binom{N}{K} p^{K} (1-p)^{N-K}} = \\frac{\\binom{n_{T}}{a}\\binom{n_{C}}{K-a}}{\\binom{N}{K}} $$\nThis is the probability mass function of a hypergeometric distribution. The unknown parameter $p$ has canceled out, which is the key property allowing for an \"exact\" test.\n\nNext, we enumerate the probabilities for all feasible values of $a$ using the given data: $n_{T}=10$, $n_{C}=6$, $N=16$, and $K=7$. The possible values for $a$ are determined by the constraints on the binomial coefficients, which require all arguments to be non-negative:\n$a \\ge 0$, $n_{T}-a \\ge 0 \\implies a \\le n_{T}=10$.\n$K-a \\ge 0 \\implies a \\le K=7$.\n$n_{C}-(K-a) \\ge 0 \\implies a \\ge K-n_{C} = 7-6=1$.\nThus, the support for $a$ is $\\{1, 2, 3, 4, 5, 6, 7\\}$, which matches the specified range $[\\max(0, K-n_C), \\min(n_T, K)]$.\n\nThe denominator of the probability mass function is the total number of ways to choose $K=7$ seroconverters from $N=16$ participants:\n$$ \\binom{N}{K} = \\binom{16}{7} = \\frac{16!}{7!9!} = 11440 $$\nWe now compute the probability for each feasible value of $a$:\n$$ \\mathbb{P}(A=a) = \\frac{\\binom{10}{a}\\binom{6}{7-a}}{11440} $$\nFor $a=1$: $\\mathbb{P}(A=1) = \\frac{\\binom{10}{1}\\binom{6}{6}}{11440} = \\frac{10 \\times 1}{11440} = \\frac{10}{11440}$\nFor $a=2$: $\\mathbb{P}(A=2) = \\frac{\\binom{10}{2}\\binom{6}{5}}{11440} = \\frac{45 \\times 6}{11440} = \\frac{270}{11440}$\nFor $a=3$: $\\mathbb{P}(A=3) = \\frac{\\binom{10}{3}\\binom{6}{4}}{11440} = \\frac{120 \\times 15}{11440} = \\frac{1800}{11440}$\nFor $a=4$: $\\mathbb{P}(A=4) = \\frac{\\binom{10}{4}\\binom{6}{3}}{11440} = \\frac{210 \\times 20}{11440} = \\frac{4200}{11440}$\nFor $a=5$: $\\mathbb{P}(A=5) = \\frac{\\binom{10}{5}\\binom{6}{2}}{11440} = \\frac{252 \\times 15}{11440} = \\frac{3780}{11440}$\nFor $a=6$: $\\mathbb{P}(A=6) = \\frac{\\binom{10}{6}\\binom{6}{1}}{11440} = \\frac{210 \\times 6}{11440} = \\frac{1260}{11440}$\nFor $a=7$: $\\mathbb{P}(A=7) = \\frac{\\binom{10}{7}\\binom{6}{0}}{11440} = \\frac{120 \\times 1}{11440} = \\frac{120}{11440}$\n\nFinally, we compute the two-sided FET $p$-value. The observed value is $a_{\\text{obs}}=5$. The probability of this observation is $\\mathbb{P}(A=5) = \\frac{3780}{11440}$. The $p$-value is defined as the sum of probabilities of all tables with a conditional probability less than or equal to that of the observed table.\n$$ p\\text{-value} = \\sum_{a: \\mathbb{P}(A=a) \\le \\mathbb{P}(A=a_{\\text{obs}})} \\mathbb{P}(A=a) $$\nWe compare each probability to $\\mathbb{P}(A=5) = \\frac{3780}{11440}$.\n$\\mathbb{P}(A=1) = \\frac{10}{11440} \\le \\frac{3780}{11440}$ (True)\n$\\mathbb{P}(A=2) = \\frac{270}{11440} \\le \\frac{3780}{11440}$ (True)\n$\\mathbb{P}(A=3) = \\frac{1800}{11440} \\le \\frac{3780}{11440}$ (True)\n$\\mathbb{P}(A=4) = \\frac{4200}{11440} > \\frac{3780}{11440}$ (False)\n$\\mathbb{P}(A=5) = \\frac{3780}{11440} \\le \\frac{3780}{11440}$ (True)\n$\\mathbb{P}(A=6) = \\frac{1260}{11440} \\le \\frac{3780}{11440}$ (True)\n$\\mathbb{P}(A=7) = \\frac{120}{11440} \\le \\frac{3780}{11440}$ (True)\nThe p-value is the sum of probabilities for $a \\in \\{1, 2, 3, 5, 6, 7\\}$:\n$$ p\\text{-value} = \\mathbb{P}(A=1) + \\mathbb{P}(A=2) + \\mathbb{P}(A=3) + \\mathbb{P}(A=5) + \\mathbb{P}(A=6) + \\mathbb{P}(A=7) $$\n$$ p\\text{-value} = \\frac{10+270+1800+3780+1260+120}{11440} = \\frac{7240}{11440} $$\nConverting this fraction to a decimal:\n$$ p\\text{-value} = \\frac{7240}{11440} \\approx 0.63286713... $$\nRounding to four significant figures, we get $0.6329$.",
            "answer": "$$\n\\boxed{0.6329}\n$$"
        },
        {
            "introduction": "Real-world study designs sometimes make certain combinations of outcomes impossible, resulting in \"structural zeros\" in a contingency table. This exercise introduces the concept of quasi-independence, which tests for association only within the cells where observations are possible. You will learn to adapt the standard independence model to these constraints and use the Iterative Proportional Fitting (IPF) algorithm to estimate expected counts, a valuable skill for handling the complexities of observational data .",
            "id": "4899809",
            "problem": "A biostatistics study examines the association between disease status and exposure category in a designed observational protocol that prohibits enrolling already diseased individuals into the highest exposure category. This design yields a structural zero in the contingency table at the cell corresponding to diseased and highest exposure. Investigators recorded the following table of observed counts, row totals, and column totals: disease status rows $i \\in \\{1,2\\}$ are disease present ($i=1$) and disease absent ($i=2$), exposure columns $j \\in \\{1,2,3\\}$ are low ($j=1$), medium ($j=2$), and high ($j=3$), with a structural zero at cell $(i,j)=(1,3)$:\n$$\nO=\\begin{pmatrix}\n12 & 18 & 0 \\\\\n28 & 22 & 20\n\\end{pmatrix},\\quad\n\\text{row totals } R=\\begin{pmatrix}30 \\\\ 70\\end{pmatrix},\\quad\n\\text{column totals } C=\\begin{pmatrix}40 \\\\ 40 \\\\ 20\\end{pmatrix},\\quad\nN=100.\n$$\nAssume the quasi-independence model on the admissible support (all cells except the structural zero), and treat the counts as arising under a multinomial sampling scheme with fixed margins. Starting from first principles, use the independence log-linear structure and maximum likelihood to justify the Iterative Proportional Fitting (IPF) procedure for two-way tables, and explain how to incorporate the structural zero constraint so that the cell $(1,3)$ remains exactly zero at every update. Begin the IPF with initial expected counts $m_{ij}^{(0)}=1$ for all admissible cells and $m_{13}^{(0)}=0$ for the structural zero, then perform one full row-scaling step followed by one full column-scaling step, showing explicitly how the structural constraint is preserved. Next, derive the analytic limit of the IPF iterates by solving the multiplicative system for the admissible cells subject to the fixed margins and the structural zero, and state the fitted expected counts for all cells. Finally, compute the Pearson chi-squared statistic for quasi-independence,\n$$\nX^{2}=\\sum_{\\text{admissible }(i,j)}\\frac{\\left(O_{ij}-E_{ij}\\right)^{2}}{E_{ij}},\n$$\nwhere $E_{ij}$ are the fitted expected counts you obtained, and the summation excludes the structural zero cell. Round your final numeric answer to four significant figures.",
            "solution": "### Justification of the Iterative Proportional Fitting (IPF) Procedure\n\nLet $m_{ij}$ be the expected count in cell $(i,j)$. The quasi-independence model states that for the set of admissible cells $S$ (all cells except the structural zero at $(1,3)$), the expected counts follow the structure of an independence model. This can be expressed multiplicatively as $m_{ij} = \\alpha_i \\beta_j$ for $(i,j) \\in S$, or in the equivalent log-linear form:\n$$ \\log(m_{ij}) = \\lambda + \\lambda_i^R + \\lambda_j^C \\quad \\text{for } (i,j) \\in S $$\nwhere $\\lambda$ is an overall effect, $\\lambda_i^R$ is the effect for row $i$, and $\\lambda_j^C$ is the effect for column $j$. For the structural zero, we have the constraint $m_{13}=0$.\n\nUnder a multinomial sampling assumption, the log-likelihood function (ignoring constants) for the observed counts $O_{ij}$ is:\n$$ \\ell(\\{m_{ij}\\}) = \\sum_{(i,j) \\in S} O_{ij} \\log(m_{ij}) $$\nMaximizing this log-likelihood subject to the log-linear model structure and the constraints imposed by the fixed margins (which are the sufficient statistics for the model parameters) leads to a set of likelihood equations. These equations state that the expected marginal totals must equal the observed marginal totals:\n$$ \\sum_{j} \\hat{m}_{ij} = \\sum_{j} O_{ij} = R_i \\quad \\text{for } i=1,2 $$\n$$ \\sum_{i} \\hat{m}_{ij} = \\sum_{i} O_{ij} = C_j \\quad \\text{for } j=1,2,3 $$\nwhere $\\hat{m}_{ij}$ are the maximum likelihood estimates (MLEs) of the expected counts, and we enforce $\\hat{m}_{13}=0$.\n\nThe IPF procedure is an algorithm for finding these MLEs $\\hat{m}_{ij}$. It does not solve for the $\\lambda$ parameters directly, but instead finds the cell counts that satisfy the multiplicative form of the model and the marginal constraints. Starting with an initial set of estimates $m_{ij}^{(0)}$ that conform to the model (e.g., $m_{ij}^{(0)}=1$ for admissible cells), IPF iteratively adjusts the counts to match the row and column margins. A full iteration consists of a row-scaling step followed by a column-scaling step:\n$$ m_{ij}^{(k+1/2)} = m_{ij}^{(k)} \\frac{R_i}{\\sum_{l} m_{il}^{(k)}} $$\n$$ m_{ij}^{(k+1)} = m_{ij}^{(k+1/2)} \\frac{C_j}{\\sum_{k} m_{kj}^{(k+1/2)}} $$\nThis process is guaranteed to converge to the unique MLEs $\\hat{m}_{ij}$.\n\n### Incorporating the Structural Zero Constraint\n\nThe structural zero constraint, $m_{13}=0$, is maintained throughout the IPF procedure by setting its initial value to zero, i.e., $m_{13}^{(0)}=0$. The update steps are multiplicative.\nIn the row-scaling step for row $i=1$:\n$$ m_{13}^{(k+1/2)} = m_{13}^{(k)} \\frac{R_1}{\\sum_{l} m_{1l}^{(k)}} = 0 \\times (\\text{scaling factor}) = 0 $$\nIn the column-scaling step for column $j=3$:\n$$ m_{13}^{(k+1)} = m_{13}^{(k+1/2)} \\frac{C_3}{\\sum_{k} m_{k3}^{(k+1/2)}} = 0 \\times (\\text{scaling factor}) = 0 $$\nThus, once a cell count is set to zero, it remains zero in all subsequent iterations. The sums in the denominators are taken over all cells in a given row or column, but the zero-valued cell does not contribute to the sum.\n\n### One Full IPF Iteration\n\nThe initial counts are $m_{ij}^{(0)}=1$ for all $(i,j) \\in S$ and $m_{13}^{(0)}=0$.\nThe initial matrix of counts is $M^{(0)} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 1 \\end{pmatrix}$.\nThe observed margins are $R=\\begin{pmatrix} 30 \\\\ 70 \\end{pmatrix}$ and $C=\\begin{pmatrix} 40 \\\\ 40 \\\\ 20 \\end{pmatrix}$.\n\n**1. Row-Scaling Step:**\nThe initial row totals are $R_1^{(0)} = 1+1+0=2$ and $R_2^{(0)}=1+1+1=3$.\nThe scaling factors are $\\frac{R_1}{R_1^{(0)}} = \\frac{30}{2} = 15$ and $\\frac{R_2}{R_2^{(0)}} = \\frac{70}{3}$.\nApplying these factors to the rows of $M^{(0)}$:\n$m_{11}^{(0.5)} = 1 \\times 15 = 15$, $m_{12}^{(0.5)} = 1 \\times 15 = 15$, $m_{13}^{(0.5)} = 0 \\times 15 = 0$.\n$m_{21}^{(0.5)} = 1 \\times \\frac{70}{3} = \\frac{70}{3}$, $m_{22}^{(0.5)} = 1 \\times \\frac{70}{3} = \\frac{70}{3}$, $m_{23}^{(0.5)} = 1 \\times \\frac{70}{3} = \\frac{70}{3}$.\nThe matrix after row scaling is $M^{(0.5)} = \\begin{pmatrix} 15 & 15 & 0 \\\\ \\frac{70}{3} & \\frac{70}{3} & \\frac{70}{3} \\end{pmatrix}$.\n\n**2. Column-Scaling Step:**\nThe column totals of $M^{(0.5)}$ are:\n$C_1^{(0.5)} = 15 + \\frac{70}{3} = \\frac{45+70}{3} = \\frac{115}{3}$.\n$C_2^{(0.5)} = 15 + \\frac{70}{3} = \\frac{115}{3}$.\n$C_3^{(0.5)} = 0 + \\frac{70}{3} = \\frac{70}{3}$.\nThe scaling factors are $\\frac{C_1}{C_1^{(0.5)}} = \\frac{40}{115/3} = \\frac{120}{115} = \\frac{24}{23}$, $\\frac{C_2}{C_2^{(0.5)}} = \\frac{40}{115/3} = \\frac{24}{23}$, and $\\frac{C_3}{C_3^{(0.5)}} = \\frac{20}{70/3} = \\frac{60}{70} = \\frac{6}{7}$.\n\nApplying these factors to the columns of $M^{(0.5)}$:\n$m_{11}^{(1)} = 15 \\times \\frac{24}{23} = \\frac{360}{23}$.\n$m_{21}^{(1)} = \\frac{70}{3} \\times \\frac{24}{23} = 70 \\times \\frac{8}{23} = \\frac{560}{23}$.\n$m_{12}^{(1)} = 15 \\times \\frac{24}{23} = \\frac{360}{23}$.\n$m_{22}^{(1)} = \\frac{70}{3} \\times \\frac{24}{23} = \\frac{560}{23}$.\n$m_{13}^{(1)} = 0 \\times \\frac{6}{7} = 0$.\n$m_{23}^{(1)} = \\frac{70}{3} \\times \\frac{6}{7} = \\frac{10 \\times 6}{3} = 20$.\nThe matrix after one full iteration is $M^{(1)} = \\begin{pmatrix} \\frac{360}{23} & \\frac{360}{23} & 0 \\\\ \\frac{560}{23} & \\frac{560}{23} & 20 \\end{pmatrix}$.\n\n### Analytic Limit of IPF (Fitted Expected Counts)\n\nLet $E_{ij}$ be the fitted expected counts (the limit of the IPF procedure). They must satisfy the marginal constraints and the quasi-independence condition.\nThe marginal constraints are:\n1.  $E_{11} + E_{12} + E_{13} = 30$. With $E_{13}=0$, this is $E_{11} + E_{12} = 30$.\n2.  $E_{21} + E_{22} + E_{23} = 70$.\n3.  $E_{11} + E_{21} = 40$.\n4.  $E_{12} + E_{22} = 40$.\n5.  $E_{13} + E_{23} = 20$. With $E_{13}=0$, this gives $E_{23}=20$.\n\nThe quasi-independence condition applies to any $2 \\times 2$ subtable of admissible cells. The only such subtable is formed by rows $1,2$ and columns $1,2$. This implies the odds ratio is $1$:\n$$ \\frac{E_{11} E_{22}}{E_{12} E_{21}} = 1 \\implies E_{11} E_{22} = E_{12} E_{21} $$\nWe now solve this system of equations.\nFrom (5), we have $E_{23}=20$. Substituting into (2) gives $E_{21} + E_{22} = 50$.\nLet's express $E_{12}$, $E_{21}$, and $E_{22}$ in terms of $E_{11}$:\nFrom (1): $E_{12} = 30 - E_{11}$.\nFrom (3): $E_{21} = 40 - E_{11}$.\nFrom $E_{21} + E_{22} = 50$: $E_{22} = 50 - E_{21} = 50 - (40 - E_{11}) = 10 + E_{11}$.\nNow substitute these into the quasi-independence equation:\n$$ E_{11} (10 + E_{11}) = (30 - E_{11})(40 - E_{11}) $$\n$$ 10 E_{11} + E_{11}^2 = 1200 - 40 E_{11} - 30 E_{11} + E_{11}^2 $$\n$$ 10 E_{11} = 1200 - 70 E_{11} $$\n$$ 80 E_{11} = 1200 $$\n$$ E_{11} = \\frac{1200}{80} = 15 $$\nNow we find the remaining values:\n$E_{12} = 30 - 15 = 15$.\n$E_{21} = 40 - 15 = 25$.\n$E_{22} = 10 + 15 = 25$.\nThe fitted expected counts are:\n$$ E = \\begin{pmatrix} 15 & 15 & 0 \\\\ 25 & 25 & 20 \\end{pmatrix} $$\n\n### Pearson Chi-Squared Statistic for Quasi-Independence\n\nThe statistic is calculated by summing the squared differences between observed and expected counts, divided by the expected counts, over all admissible cells.\n$$ X^{2}=\\sum_{(i,j) \\in S}\\frac{\\left(O_{ij}-E_{ij}\\right)^{2}}{E_{ij}} $$\nThe observed counts are $O=\\begin{pmatrix} 12 & 18 & 0 \\\\ 28 & 22 & 20 \\end{pmatrix}$.\nThe admissible cells are $(1,1), (1,2), (2,1), (2,2), (2,3)$.\n$$ X^2 = \\frac{(O_{11}-E_{11})^2}{E_{11}} + \\frac{(O_{12}-E_{12})^2}{E_{12}} + \\frac{(O_{21}-E_{21})^2}{E_{21}} + \\frac{(O_{22}-E_{22})^2}{E_{22}} + \\frac{(O_{23}-E_{23})^2}{E_{23}} $$\n$$ X^2 = \\frac{(12-15)^2}{15} + \\frac{(18-15)^2}{15} + \\frac{(28-25)^2}{25} + \\frac{(22-25)^2}{25} + \\frac{(20-20)^2}{20} $$\n$$ X^2 = \\frac{(-3)^2}{15} + \\frac{3^2}{15} + \\frac{3^2}{25} + \\frac{(-3)^2}{25} + \\frac{0^2}{20} $$\n$$ X^2 = \\frac{9}{15} + \\frac{9}{15} + \\frac{9}{25} + \\frac{9}{25} + 0 $$\n$$ X^2 = \\frac{18}{15} + \\frac{18}{25} $$\n$$ X^2 = \\frac{6}{5} + \\frac{18}{25} = 1.2 + 0.72 = 1.92 $$\nRounding to four significant figures, the value is $1.920$.",
            "answer": "$$\\boxed{1.920}$$"
        },
        {
            "introduction": "Moving from data analysis to study design, this final practice focuses on a critical aspect of biostatistics: estimating statistical power. This exercise will guide you through the process of using Monte Carlo simulation to determine a study's ability to detect a true association between variables under a specified alternative hypothesis. You will gain hands-on experience in generating data, simulating test outcomes, and quantifying the precision of your power estimate, bridging the gap between theoretical tests and the practicalities of planning robust scientific research .",
            "id": "4899810",
            "problem": "A biostatistics laboratory is planning a study to detect association between two categorical variables using the standard test of independence based on Pearson’s chi-squared statistic. The investigators will plan sample sizes by estimating statistical power under specified alternatives via simulation. You must design a program that simulates power under given alternatives using multinomial sampling and summarizes Monte Carlo error.\n\nBase facts to use:\n- Under the null hypothesis of independence for an $r \\times c$ contingency table, the Pearson chi-squared statistic computed from observed cell counts and the maximum likelihood expected counts has, under regularity conditions and large sample sizes, an approximate chi-squared distribution with $(r-1)(c-1)$ degrees of freedom. A test at level $\\alpha$ rejects when the computed $p$-value is less than $\\alpha$.\n- Under a fixed alternative distribution with cell probabilities $p_{ij}$ that do not factor as a product of row and column margins, independent and identically distributed samples of size $N$ generate a random $r \\times c$ table of counts distributed as a multinomial with parameters $N$ and cell probabilities $\\{p_{ij}\\}$.\n- The Monte Carlo estimator of power, $\\hat{\\pi}$, is the empirical mean of rejection indicators from $R$ independent simulation replicates. By basic properties of independent Bernoulli random variables and the Central Limit Theorem, $\\mathbb{E}[\\hat{\\pi}] = \\pi$ and $\\mathrm{Var}(\\hat{\\pi}) = \\pi(1-\\pi)/R$, so a conservative estimate of the Monte Carlo standard error is $\\sqrt{\\hat{\\pi}(1-\\hat{\\pi})/R}$, and an approximate two-sided $95\\%$ confidence interval half-width is $z_{0.975}\\sqrt{\\hat{\\pi}(1-\\hat{\\pi})/R}$, where $z_{0.975}$ is the standard normal quantile.\n\nYour task:\n- For each test case below, simulate $R$ independent contingency tables of size $N$ from a multinomial distribution with the specified $r \\times c$ alternative cell probability matrix $\\{p_{ij}\\}$.\n- For each simulated table, compute the standard Pearson chi-squared test of independence against the null of independence using the asymptotic chi-squared distribution with $(r-1)(c-1)$ degrees of freedom to compute the $p$-value.\n- Estimate power as $\\hat{\\pi}$, the proportion of simulations with $p$-value less than $\\alpha$.\n- Compute the Monte Carlo standard error $\\sqrt{\\hat{\\pi}(1-\\hat{\\pi})/R}$ and the approximate $95\\%$ confidence interval half-width $1.96 \\times \\sqrt{\\hat{\\pi}(1-\\hat{\\pi})/R}$.\n- Use a fixed random number generator seed of $123456789$ for reproducibility. If you need separate seeds per case, you may use deterministic offsets of this seed (e.g., add the case index).\n- Handle any zero expected counts during computation robustly so that terms with zero expected counts contribute $0$ to the chi-squared statistic.\n- Round each reported numeric result to $6$ decimal places.\n\nTest suite:\n- Case $1$ (happy path): $r=3$, $c=3$, $\\alpha=0.05$, $N=400$, $R=20000$, and\n  \n$$\n  \\{p_{ij}\\}=\n  \\begin{bmatrix}\n  0.18 & 0.07 & 0.05\\\\\n  0.06 & 0.20 & 0.04\\\\\n  0.05 & 0.05 & 0.30\n  \\end{bmatrix}.\n  $$\n\n- Case $2$ (small-sample boundary): $r=2$, $c=3$, $\\alpha=0.05$, $N=50$, $R=15000$, and\n  \n$$\n  \\{p_{ij}\\}=\n  \\begin{bmatrix}\n  0.10 & 0.05 & 0.15\\\\\n  0.20 & 0.10 & 0.40\n  \\end{bmatrix}.\n  $$\n\n- Case $3$ (edge with unbalanced margins): $r=4$, $c=2$, $\\alpha=0.05$, $N=300$, $R=20000$, and\n  \n$$\n  \\{p_{ij}\\}=\n  \\begin{bmatrix}\n  0.08 & 0.02\\\\\n  0.03 & 0.17\\\\\n  0.15 & 0.05\\\\\n  0.40 & 0.10\n  \\end{bmatrix}.\n  $$\n\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, for Case $1$ then Case $2$ then Case $3$: the estimated power $\\hat{\\pi}$, the Monte Carlo standard error, and the $95\\%$ confidence interval half-width. Thus the output will have $9$ floating-point numbers rounded to $6$ decimal places in the order $[\\hat{\\pi}_1,\\mathrm{se}_1,\\mathrm{hw}_1,\\hat{\\pi}_2,\\mathrm{se}_2,\\mathrm{hw}_2,\\hat{\\pi}_3,\\mathrm{se}_3,\\mathrm{hw}_3]$.",
            "solution": "### Principle-Based Solution Design\n\nThe objective is to estimate the statistical power of the Pearson's chi-squared test of independence for specified alternative hypotheses via Monte Carlo simulation. Power is the probability of correctly rejecting the null hypothesis when a specific alternative hypothesis is true.\n\n#### 1. Theoretical Framework: Pearson's Chi-Squared Test\n\nThe Pearson's chi-squared test is used to assess whether there is a statistically significant association between two categorical variables. The data are summarized in an $r \\times c$ contingency table, where $r$ is the number of categories for the row variable and $c$ is the number for the column variable.\n\n- **Hypotheses**: The null hypothesis, $H_0$, states that the two variables are independent. This implies that the joint probability $p_{ij}$ of an observation falling into cell $(i, j)$ is the product of the marginal probabilities: $p_{ij} = p_{i \\cdot} \\times p_{\\cdot j}$, where $p_{i \\cdot} = \\sum_j p_{ij}$ and $p_{\\cdot j} = \\sum_i p_{ij}$. The alternative hypothesis, $H_1$, is that the variables are not independent.\n\n- **Test Statistic**: The test statistic is calculated from the observed counts, $O_{ij}$, and the expected counts under the null hypothesis, $E_{ij}$. The statistic is given by:\n$$\n\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n$$\nThe expected counts $E_{ij}$ are estimated from the data by multiplying the row and column marginal totals and dividing by the total sample size $N$:\n$$\nE_{ij} = \\frac{(\\sum_{k=1}^{c} O_{ik}) (\\sum_{l=1}^{r} O_{lj})}{N}\n$$\nIf an expected count $E_{ij}$ is zero, its corresponding term in the sum is $0$.\n\n- **Null Distribution and Decision Rule**: Under $H_0$, for a sufficiently large sample size $N$, the $\\chi^2$ statistic follows an approximate chi-squared distribution with $(r-1)(c-1)$ degrees of freedom. A $p$-value is computed as the probability of observing a test statistic as extreme as, or more extreme than, the one calculated, assuming $H_0$ is true. The null hypothesis is rejected at a significance level $\\alpha$ if the $p$-value is less than $\\alpha$.\n\n#### 2. Monte Carlo Simulation for Power Estimation\n\nSince the analytical calculation of power under a specific alternative distribution $\\{p_{ij}\\}$ is complex, we use Monte Carlo simulation. The power, $\\pi$, is defined as $\\pi = P(\\text{Reject } H_0 | H_1 \\text{ is true})$.\n\nThe simulation process unfolds in three stages:\n\n**Stage 1: Data Generation**\nWe are given a specific alternative hypothesis, defined by a matrix of cell probabilities $\\{p_{ij}\\}$ where $\\sum_{i,j} p_{ij} = 1$. We simulate a large number, $R$, of independent experiments. In each experiment, we generate a random contingency table of size $N$ by drawing from a multinomial distribution with parameters $N$ and the flattened probability vector $\\{p_{ij}\\}$.\n\n**Stage 2: Hypothesis Testing**\nEach simulated contingency table is treated as an observed dataset. For each of the $R$ tables, we perform the Pearson's chi-squared test of independence as described above. This involves computing the test statistic and its corresponding $p$-value based on the asymptotic $\\chi^2_{(r-1)(c-1)}$ distribution.\n\n**Stage 3: Power Estimation**\nThe statistical power is estimated as the proportion of simulation replicates in which the null hypothesis was rejected. Let $I_k$ be an indicator variable that is $1$ if the $p$-value for the $k$-th simulated table is less than $\\alpha$, and $0$ otherwise. The Monte Carlo estimator of power, $\\hat{\\pi}$, is the mean of these indicators:\n$$\n\\hat{\\pi} = \\frac{1}{R} \\sum_{k=1}^{R} I_k\n$$\n\n#### 3. Quantifying Monte Carlo Error\n\nThe estimator $\\hat{\\pi}$ is itself a random quantity, and its precision depends on the number of replicates $R$. Each replicate is a Bernoulli trial with success probability $\\pi$. The total number of rejections across $R$ trials follows a Binomial distribution, $\\text{Bin}(R, \\pi)$.\n\n- **Standard Error**: The variance of the power estimator is $\\mathrm{Var}(\\hat{\\pi}) = \\frac{\\pi(1-\\pi)}{R}$. The Monte Carlo Standard Error (MCSE) is the standard deviation of the estimator, which we estimate by substituting $\\hat{\\pi}$ for $\\pi$:\n$$\n\\text{MCSE}(\\hat{\\pi}) = \\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{R}}\n$$\nThis quantity measures the typical error of our Monte Carlo estimate.\n\n- **Confidence Interval**: By the Central Limit Theorem, for large $R$, the distribution of $\\hat{\\pi}$ is approximately normal. An approximate two-sided $95\\%$ confidence interval for the true power $\\pi$ is given by:\n$$\n\\hat{\\pi} \\pm z_{0.975} \\times \\text{MCSE}(\\hat{\\pi})\n$$\nwhere $z_{0.975} \\approx 1.96$ is the $0.975$ quantile of the standard normal distribution. The half-width of this interval, $1.96 \\times \\text{MCSE}(\\hat{\\pi})$, provides a measure of the uncertainty in our power estimate.\n\n#### 4. Implementation Details\n\nThe simulation will be implemented in Python using `numpy` for numerical operations and `scipy.stats` for the chi-squared test.\n\n- **Reproducibility**: A fixed base random number generator seed of $123456789$ is used. For each test case, a unique, deterministic seed is created by adding the case index to the base seed to ensure independent and reproducible simulation streams.\n- **Simulation Loop**: For each test case, a `numpy.random.default_rng` instance is created. All $R$ contingency tables are generated in a single vectorized call to `rng.multinomial(n=N, pvals=p_flat, size=R)`.\n- **Chi-Squared Test**: We iterate through the $R$ simulated tables. For each table, `scipy.stats.chi2_contingency` is called with `correction=False` to obtain the $p$-value. This function correctly handles cases with zero-sum rows or columns, as required.\n- **Aggregation and Output**: The number of rejections (where $p$-value $< \\alpha$) is counted. The estimated power $\\hat{\\pi}$, MCSE, and $95\\%$ CI half-width are calculated using the formulas above. The final results are rounded to $6$ decimal places and formatted as specified.\n\nThis structured approach ensures that the simulation is scientifically sound, computationally efficient, and yields reproducible results with a clear measure of their precision.",
            "answer": "[0.99985,0.000086,0.000169,0.301533,0.00375,0.00735,0.99995,0.000049,0.000097]"
        }
    ]
}