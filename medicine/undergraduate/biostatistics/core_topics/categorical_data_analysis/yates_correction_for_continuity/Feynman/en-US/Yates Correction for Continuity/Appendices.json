{
    "hands_on_practices": [
        {
            "introduction": "Mastering any statistical method begins with a solid understanding of its mechanics. This practice  guides you through the fundamental calculation of the Yates-corrected chi-square statistic, $X_Y^2$. By first computing the statistic from its cell-wise residual definition and then algebraically reconciling it with the common cross-product formula, you will build both computational skill and a deeper conceptual insight into how the correction operates.",
            "id": "4966735",
            "problem": "A case-control study investigated whether a new prophylactic protocol is associated with a reduced occurrence of a postoperative infection. The $2 \\times 2$ table of observed counts is as follows: treatment group infections $a = 12$, treatment group no infections $b = 8$, control group infections $c = 5$, and control group no infections $d = 15$. Let the row totals be $r_{1} = a + b$ and $r_{2} = c + d$, the column totals be $c_{1} = a + c$ and $c_{2} = b + d$, and the grand total be $n = r_{1} + r_{2} = c_{1} + c_{2}$. Under the null hypothesis of independence, the expected counts satisfy $E_{ij} = \\frac{(\\text{row } i \\text{ total})(\\text{column } j \\text{ total})}{n}$.\n\nUsing the definition of the Pearson chi-square statistic with Yates's correction for continuity (that is, subtracting $0.5$ from the absolute cellwise deviation before squaring), compute the Yates-corrected statistic\n$$\nX_{Y}^{2} = \\sum_{i=1}^{2}\\sum_{j=1}^{2} \\frac{\\left(|O_{ij} - E_{ij}| - 0.5\\right)^{2}}{E_{ij}},\n$$\nwhere $O_{ij}$ are the observed counts and $E_{ij}$ are the expected counts. Then, starting only from the fundamental definitions above, reconcile this cellwise residual computation with the standard cross-product-based expression for a $2 \\times 2$ table (sometimes called the determinant form) by algebraic manipulation, and show that both approaches yield the same numerical value for $X_{Y}^{2}$ for the table given.\n\nReport the final numerical value of $X_{Y}^{2}$ rounded to four significant figures.",
            "solution": "The problem is deemed valid as it is scientifically grounded in established statistical theory, well-posed with a complete and consistent set of data and definitions, and objectively stated. It presents a standard, non-trivial task in biostatistics that is verifiable through mathematical derivation and computation.\n\nThe task is to compute the Yates-corrected chi-square statistic, $X_{Y}^{2}$, for a given $2 \\times 2$ contingency table using two different but equivalent formulas, demonstrating their algebraic and numerical equivalence.\n\nThe observed counts are given as a $2 \\times 2$ table:\n$$\nO = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix} = \\begin{pmatrix} 12  8 \\\\ 5  15 \\end{pmatrix}\n$$\n\nFirst, we calculate the row totals, column totals, and the grand total, $n$.\nRow totals:\n$r_{1} = a + b = 12 + 8 = 20$\n$r_{2} = c + d = 5 + 15 = 20$\n\nColumn totals:\n$c_{1} = a + c = 12 + 5 = 17$\n$c_{2} = b + d = 8 + 15 = 23$\n\nGrand total:\n$n = r_{1} + r_{2} = 20 + 20 = 40$. As a check, $n = c_{1} + c_{2} = 17 + 23 = 40$.\n\nUnder the null hypothesis of independence, the expected counts, $E_{ij}$, are calculated as $E_{ij} = \\frac{(\\text{row } i \\text{ total})(\\text{column } j \\text{ total})}{n}$.\n$E_{11} = \\frac{r_{1} c_{1}}{n} = \\frac{20 \\times 17}{40} = 8.5$\n$E_{12} = \\frac{r_{1} c_{2}}{n} = \\frac{20 \\times 23}{40} = 11.5$\n$E_{21} = \\frac{r_{2} c_{1}}{n} = \\frac{20 \\times 17}{40} = 8.5$\n$E_{22} = \\frac{r_{2} c_{2}}{n} = \\frac{20 \\times 23}{40} = 11.5$\n\nThe matrix of expected counts is:\n$$\nE = \\begin{pmatrix} 8.5  11.5 \\\\ 8.5  11.5 \\end{pmatrix}\n$$\n\n**Part 1: Computation using the Cellwise Residual Formula**\n\nThe Yates-corrected chi-square statistic is defined as:\n$$\nX_{Y}^{2} = \\sum_{i=1}^{2}\\sum_{j=1}^{2} \\frac{\\left(|O_{ij} - E_{ij}| - 0.5\\right)^{2}}{E_{ij}}\n$$\nWe compute the absolute deviation $|O_{ij} - E_{ij}|$ for each cell:\n$|O_{11} - E_{11}| = |12 - 8.5| = 3.5$\n$|O_{12} - E_{12}| = |8 - 11.5| = |-3.5| = 3.5$\n$|O_{21} - E_{21}| = |5 - 8.5| = |-3.5| = 3.5$\n$|O_{22} - E_{22}| = |15 - 11.5| = 3.5$\n\nThe absolute deviation is $3.5$ for all cells. The term in the numerator of the sum is therefore constant:\n$(|O_{ij} - E_{ij}| - 0.5)^{2} = (3.5 - 0.5)^{2} = 3.0^2 = 9.0$\n\nNow, we compute $X_{Y}^{2}$:\n$$\nX_{Y}^{2} = \\frac{9.0}{E_{11}} + \\frac{9.0}{E_{12}} + \\frac{9.0}{E_{21}} + \\frac{9.0}{E_{22}}\n$$\n$$\nX_{Y}^{2} = \\frac{9.0}{8.5} + \\frac{9.0}{11.5} + \\frac{9.0}{8.5} + \\frac{9.0}{11.5} = 2 \\times \\frac{9.0}{8.5} + 2 \\times \\frac{9.0}{11.5}\n$$\n$$\nX_{Y}^{2} = 18.0 \\left( \\frac{1}{8.5} + \\frac{1}{11.5} \\right) = 18.0 \\left( \\frac{2}{17} + \\frac{2}{23} \\right) = 36.0 \\left( \\frac{1}{17} + \\frac{1}{23} \\right)\n$$\n$$\nX_{Y}^{2} = 36.0 \\left( \\frac{23 + 17}{17 \\times 23} \\right) = 36.0 \\left( \\frac{40}{391} \\right) = \\frac{1440}{391} \\approx 3.682864...\n$$\n\n**Part 2: Algebraic Reconciliation with the Cross-Product Formula**\n\nWe start from the cellwise definition and show its equivalence to the cross-product form.\nFor a general $2 \\times 2$ table, the deviation $O_{ij} - E_{ij}$ for cell $(1,1)$ is:\n$O_{11} - E_{11} = a - \\frac{r_{1}c_{1}}{n} = a - \\frac{(a+b)(a+c)}{n} = \\frac{a(a+b+c+d) - (a^2+ac+ab+bc)}{n} = \\frac{ad-bc}{n}$.\nSimilarly, the deviations for cells $(1,2)$, $(2,1)$, and $(2,2)$ are $\\frac{bc-ad}{n}$, $\\frac{bc-ad}{n}$, and $\\frac{ad-bc}{n}$, respectively.\nThus, the absolute deviation is the same for all four cells:\n$$\n|O_{ij} - E_{ij}| = \\left| \\frac{ad-bc}{n} \\right| = \\frac{|ad-bc|}{n}\n$$\nThe numerator term in the $X_Y^2$ sum is therefore constant:\n$$\n\\left(|O_{ij} - E_{ij}| - 0.5\\right)^{2} = \\left(\\frac{|ad-bc|}{n} - \\frac{1}{2}\\right)^{2}\n$$\nWe can factor this constant numerator out of the sum:\n$$\nX_{Y}^{2} = \\left(\\frac{|ad-bc|}{n} - \\frac{1}{2}\\right)^{2} \\sum_{i,j} \\frac{1}{E_{ij}}\n$$\nNow, we simplify the sum of the reciprocals of the expected counts:\n$$\n\\sum_{i,j} \\frac{1}{E_{ij}} = \\frac{1}{E_{11}} + \\frac{1}{E_{12}} + \\frac{1}{E_{21}} + \\frac{1}{E_{22}} = \\frac{n}{r_1 c_1} + \\frac{n}{r_1 c_2} + \\frac{n}{r_2 c_1} + \\frac{n}{r_2 c_2}\n$$\nFactoring out common terms:\n$$\n= n \\left( \\left(\\frac{1}{r_1 c_1} + \\frac{1}{r_1 c_2}\\right) + \\left(\\frac{1}{r_2 c_1} + \\frac{1}{r_2 c_2}\\right) \\right) = n \\left( \\frac{1}{r_1}\\left(\\frac{1}{c_1} + \\frac{1}{c_2}\\right) + \\frac{1}{r_2}\\left(\\frac{1}{c_1} + \\frac{1}{c_2}\\right) \\right)\n$$\n$$\n= n \\left( \\frac{1}{r_1} + \\frac{1}{r_2} \\right) \\left( \\frac{1}{c_1} + \\frac{1}{c_2} \\right) = n \\left( \\frac{r_1+r_2}{r_1 r_2} \\right) \\left( \\frac{c_1+c_2}{c_1 c_2} \\right)\n$$\nSince $r_1+r_2=n$ and $c_1+c_2=n$, this simplifies to:\n$$\n\\sum_{i,j} \\frac{1}{E_{ij}} = n \\left( \\frac{n}{r_1 r_2} \\right) \\left( \\frac{n}{c_1 c_2} \\right) = \\frac{n^3}{r_1 r_2 c_1 c_2}\n$$\nSubstitute this back into the expression for $X_{Y}^{2}$:\n$$\nX_{Y}^{2} = \\left(\\frac{|ad-bc|}{n} - \\frac{1}{2}\\right)^{2} \\left( \\frac{n^3}{r_1 r_2 c_1 c_2} \\right) = \\left( \\frac{|ad-bc| - n/2}{n} \\right)^{2} \\left( \\frac{n^3}{r_1 r_2 c_1 c_2} \\right)\n$$\n$$\nX_{Y}^{2} = \\frac{(|ad-bc| - n/2)^2}{n^2} \\frac{n^3}{r_1 r_2 c_1 c_2} = \\frac{n(|ad-bc| - n/2)^2}{r_1 r_2 c_1 c_2}\n$$\nThis is the standard cross-product formula for the Yates-corrected chi-square statistic. The algebraic reconciliation is complete.\n\n**Part 3: Computation using the Cross-Product Formula**\n\nTo verify, we now use this derived formula with the given data:\n$a=12$, $b=8$, $c=5$, $d=15$.\n$r_1=20$, $r_2=20$, $c_1=17$, $c_2=23$, $n=40$.\nThe cross-product term is $|ad-bc| = |(12)(15) - (8)(5)| = |180 - 40| = 140$.\nThe correction term is $n/2 = 40/2 = 20$.\n\n$$\nX_{Y}^{2} = \\frac{40(|140| - 20)^2}{(20)(20)(17)(23)} = \\frac{40(120)^2}{400(17)(23)}\n$$\n$$\nX_{Y}^{2} = \\frac{40 \\times 14400}{400 \\times 391} = \\frac{576000}{156400} = \\frac{5760}{1564} = \\frac{1440}{391}\n$$\nThis is the same fraction obtained in Part 1. Numerically, this is:\n$X_{Y}^{2} \\approx 3.682864...$\n\nBoth approaches yield the identical result, confirming the algebraic equivalence. The problem asks for the final numerical value rounded to four significant figures.\n\n$X_{Y}^{2} \\approx 3.683$",
            "answer": "$$\n\\boxed{3.683}\n$$"
        },
        {
            "introduction": "After learning how to apply a statistical correction, it is crucial to understand its limitations. This practice  presents a case study involving a table with sparse data, a common scenario in pilot studies or research on rare events. By working through this example, you will discover a situation where the continuity correction can behave counter-intuitively, reinforcing the vital principle that statistical tools must be applied with critical judgment, not just mechanical calculation.",
            "id": "4966732",
            "problem": "A clinical pilot study cross-classifies the presence of a transient adverse event ($\\text{Yes}$, $\\text{No}$) by exposure to a new prophylactic protocol ($\\text{Exposed}$, $\\text{Unexposed}$). The resulting $2 \\times 2$ table has cell counts labeled in the standard order as $a$, $b$, $c$, $d$ with $a = 0$, $b = 1$, $c = 4$, and $d = 15$. Consider the large-sample Pearson chi-square test of independence with one degree of freedom. Starting from the definition of the Pearson chi-square statistic in terms of observed and expected frequencies under independence, and the general principle of continuity correction for approximating a discrete distribution by a continuous reference, do the following:\n\n1. Derive the expected counts under independence and write the Pearson chi-square statistic in terms of these expected counts.\n2. Apply Yatesâ€™s correction for continuity for a $2 \\times 2$ table by reducing the absolute deviation $|O - E|$ by $1/2$ in each contributing term before squaring, and evaluate the continuity-corrected test statistic for the given table.\n3. Briefly interpret whether, in this sparse table, the continuity correction meaningfully addresses the discreteness of the data, justifying your reasoning from first principles about discreteness and approximation.\n\nReport your final continuity-corrected chi-square statistic as a single real number, rounded to four significant figures. No units are required.",
            "solution": "The problem requires the calculation of the Yates's continuity-corrected chi-square statistic for a given $2 \\times 2$ contingency table and an interpretation of the correction's effect. The analysis will proceed by first establishing the observed and expected frequencies, then applying the definition of the corrected statistic.\n\nThe given observed counts are $a = 0$, $b = 1$, $c = 4$, and $d = 15$. We can arrange these into a $2 \\times 2$ table, representing the cross-classification of the adverse event by exposure group. Let the rows represent exposure (Row 1: Exposed, Row 2: Unexposed) and the columns represent the presence of the adverse event (Column 1: Yes, Column 2: No). The observed frequencies, $O_{ij}$, are:\n$$\n\\begin{pmatrix} O_{11}  O_{12} \\\\ O_{21}  O_{22} \\end{pmatrix} = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix} = \\begin{pmatrix} 0  1 \\\\ 4  15 \\end{pmatrix}\n$$\n\nFirst, we calculate the marginal totals and the grand total, $N$:\nRow 1 total: $R_1 = a + b = 0 + 1 = 1$.\nRow 2 total: $R_2 = c + d = 4 + 15 = 19$.\nColumn 1 total: $C_1 = a + c = 0 + 4 = 4$.\nColumn 2 total: $C_2 = b + d = 1 + 15 = 16$.\nGrand total: $N = a + b + c + d = 0 + 1 + 4 + 15 = 20$. The totals are consistent, as $R_1+R_2 = 1+19=20$ and $C_1+C_2 = 4+16=20$.\n\n**1. Expected Counts and Pearson Chi-Square Statistic**\n\nUnder the null hypothesis of independence between the row and column variables, the expected frequency for each cell, $E_{ij}$, is calculated as $E_{ij} = \\frac{R_i C_j}{N}$.\n\nThe expected counts are:\n$E_{11} = \\frac{R_1 C_1}{N} = \\frac{1 \\times 4}{20} = 0.2$\n$E_{12} = \\frac{R_1 C_2}{N} = \\frac{1 \\times 16}{20} = 0.8$\n$E_{21} = \\frac{R_2 C_1}{N} = \\frac{19 \\times 4}{20} = \\frac{76}{20} = 3.8$\n$E_{22} = \\frac{R_2 C_2}{N} = \\frac{19 \\times 16}{20} = \\frac{304}{20} = 15.2$\n\nThe Pearson chi-square statistic, $\\chi^2$, is defined as the sum of the squared differences between observed and expected frequencies, divided by the expected frequencies:\n$$\n\\chi^2 = \\sum_{i=1}^{2} \\sum_{j=1}^{2} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n$$\n\n**2. Yates's Correction for Continuity**\n\nYates's correction adjusts this formula by subtracting a value of $0.5$ from the absolute difference between the observed and expected frequencies before squaring. This is intended to correct for the error introduced when approximating the discrete sampling distribution of the test statistic with a continuous chi-square distribution.\n\nThe continuity-corrected chi-square statistic, denoted $\\chi^2_Y$, is given by:\n$$\n\\chi^2_Y = \\sum_{i=1}^{2} \\sum_{j=1}^{2} \\frac{(|O_{ij} - E_{ij}| - 0.5)^2}{E_{ij}}\n$$\nA key property for any $2 \\times 2$ table is that the absolute deviation $|O_{ij} - E_{ij}|$ is constant for all four cells. Let's calculate this value:\n$|O_{11} - E_{11}| = |0 - 0.2| = 0.2$\n$|O_{12} - E_{12}| = |1 - 0.8| = 0.2$\n$|O_{21} - E_{21}| = |4 - 3.8| = 0.2$\n$|O_{22} - E_{22}| = |15 - 15.2| = 0.2$\nThis common absolute deviation can also be calculated as $\\frac{|ad-bc|}{N} = \\frac{|(0)(15) - (1)(4)|}{20} = \\frac{|-4|}{20} = 0.2$.\n\nNow, we apply the correction. The numerator term for each cell is $(|O_{ij} - E_{ij}| - 0.5)^2 = (0.2 - 0.5)^2 = (-0.3)^2 = 0.09$.\n\nWe can now compute $\\chi^2_Y$:\n$$\n\\chi^2_Y = \\frac{(0.09)}{0.2} + \\frac{(0.09)}{0.8} + \\frac{(0.09)}{3.8} + \\frac{(0.09)}{15.2}\n$$\nAlternatively, the algebraically equivalent and computationally simpler formula for the corrected statistic is:\n$$\n\\chi^2_Y = \\frac{N(|ad - bc| - \\frac{N}{2})^2}{(a+b)(c+d)(a+c)(b+d)}\n$$\nUsing the given values:\n$a=0, b=1, c=4, d=15, N=20$.\n$|ad - bc| = |(0)(15) - (1)(4)| = |-4| = 4$.\n$a+b = 1$, $c+d = 19$, $a+c = 4$, $b+d = 16$.\n$$\n\\chi^2_Y = \\frac{20(|4| - \\frac{20}{2})^2}{(1)(19)(4)(16)} = \\frac{20(4 - 10)^2}{1216} = \\frac{20(-6)^2}{1216} = \\frac{20(36)}{1216} = \\frac{720}{1216}\n$$\nSimplifying the fraction:\n$\\frac{720}{1216} = \\frac{360}{608} = \\frac{180}{304} = \\frac{90}{152} = \\frac{45}{76}$.\n\nTo provide the final answer as a real number rounded to four significant figures:\n$\\chi^2_Y = \\frac{45}{76} \\approx 0.59210526...$\nRounding to four significant figures gives $0.5921$.\n\n**3. Interpretation of the Continuity Correction**\n\nThe fundamental purpose of a continuity correction is to improve the approximation of the test statistic's discrete sampling distribution by the continuous chi-square distribution. This typically results in a *smaller* test statistic value, leading to a more conservative test (i.e., a larger p-value and a lower Type I error rate).\n\nIn this specific case, the table is sparse (one cell count is $0$, another is $1$). Let's compare the corrected statistic with the uncorrected Pearson chi-square statistic:\n$$\n\\chi^2 = \\frac{N(ad-bc)^2}{(a+b)(c+d)(a+c)(b+d)} = \\frac{20((0)(15)-(1)(4))^2}{1216} = \\frac{20(-4)^2}{1216} = \\frac{20(16)}{1216} = \\frac{320}{1216} = \\frac{5}{19} \\approx 0.2632\n$$\nWe observe that the continuity-corrected statistic, $\\chi^2_Y \\approx 0.5921$, is more than twice as large as the uncorrected statistic, $\\chi^2 \\approx 0.2632$. This is a pathological result that runs counter to the intention of the correction.\n\nThis anomaly occurs because the absolute deviation between observed and expected counts, $|O-E|=0.2$, is less than the correction factor of $0.5$. The formula subtracts $0.5$ from this small value, resulting in a negative number ($-0.3$), which upon squaring ($( -0.3)^2=0.09$) becomes larger than the original squared deviation ($( \\pm 0.2)^2=0.04$). Consequently, the overall statistic increases.\n\nTherefore, for this sparse table where the observed data are already very close to the expectation under the null hypothesis, Yates's correction does not meaningfully address the discreteness of the data. Instead, its mechanistic application produces a distorted and counter-intuitive inflation of the test statistic. This highlights a well-known flaw of Yates's correction: it can severely over-correct, especially in tables with small marginal totals or when the deviation from expectation is small. For such sparse tables, an exact method like Fisher's exact test, which does not rely on a continuous approximation, is the preferred analytical tool.",
            "answer": "$$\\boxed{0.5921}$$"
        },
        {
            "introduction": "Modern statistical practice often moves beyond analyzing a single dataset to understanding how methods perform in the long run across a variety of conditions. This advanced hands-on practice  introduces this concept through simulation. By writing a program to compare the significance decisions of the Yates-corrected test against the 'gold standard' Fisher's exact test, you will gain an empirical understanding of the correction's conservatism and the specific scenarios where these two important methods may lead to different conclusions.",
            "id": "4966736",
            "problem": "You are to write a complete program that uses simulation to quantify how often the Pearson chi-square independence test with Yates's correction for continuity and Fisher's exact test disagree on statistical significance at level $\\alpha=0.05$ for realistic biostatistical sample sizes in $2 \\times 2$ contingency tables. The derivation and implementation must start from fundamental definitions of independence testing in contingency tables, the notion of discrete sampling variability, and the role of continuity correction in approximating a discrete distribution by a continuous one. You must not rely on precomputed datasets or external inputs.\n\nConsider two independent groups with binary outcomes. Let group $1$ have sample size $n_1$ and success probability $p_1$, and let group $2$ have sample size $n_2$ and success probability $p_2$. On each simulation replicate, draw $X_1 \\sim \\text{Binomial}(n_1,p_1)$ and $X_2 \\sim \\text{Binomial}(n_2,p_2)$ to represent the count of successes in each group, and construct a $2 \\times 2$ table\n$$\n\\begin{pmatrix}\na  b \\\\\nc  d\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nX_1  n_1 - X_1 \\\\\nX_2  n_2 - X_2\n\\end{pmatrix},\n$$\nwith row totals $r_1=a+b$, $r_2=c+d$, column totals $c_1=a+c$, $c_2=b+d$, and grand total $N=r_1+r_2$. Under the null hypothesis of independence, the exact test compares the observed table to the hypergeometric distribution induced by fixed margins, whereas the Pearson chi-square test evaluates the deviation between observed and expected counts. Yates's correction for continuity adjusts the Pearson chi-square framework to account for discrete cell counts by applying a half-count continuity correction in the $2 \\times 2$ case.\n\nYour task is to perform the following for each scenario in the test suite:\n- Simulate $R$ independent replicates with a fixed random seed $42$ for reproducibility. On each replicate, compute:\n  - The two-sided Fisher's exact test $p$-value based on the hypergeometric model.\n  - The Pearson chi-square test $p$-value with Yates's continuity correction, using the $\\chi^2$ distribution with one degree of freedom (degrees of freedom (df) equals $1$). If any marginal total is zero, set the Yates-corrected test $p$-value to $1$ by convention.\n- Determine significance decisions by the rule \"reject when $p \\le \\alpha$\" for both tests at $\\alpha=0.05$.\n- Record whether the two tests disagree on significance on that replicate, and compute the proportion of replicates with disagreement. Express this proportion as a decimal in $[0,1]$.\n\nTest suite:\n- Case $1$ (balanced, null, moderate event rate): $(n_1,n_2,p_1,p_2,R)=(50,50,0.20,0.20,4000)$.\n- Case $2$ (unbalanced, null, moderate event rate): $(n_1,n_2,p_1,p_2,R)=(30,120,0.20,0.20,4000)$.\n- Case $3$ (balanced, null, rare events, small sample): $(n_1,n_2,p_1,p_2,R)=(25,25,0.02,0.02,6000)$.\n- Case $4$ (balanced, alternative, moderate effect): $(n_1,n_2,p_1,p_2,R)=(60,60,0.20,0.40,4000)$.\n- Case $5$ (balanced, null, large sample, common events): $(n_1,n_2,p_1,p_2,R)=(300,300,0.50,0.50,3000)$.\n- Case $6$ (balanced, alternative, rare events): $(n_1,n_2,p_1,p_2,R)=(100,100,0.01,0.05,5000)$.\n\nFinal output specification:\n- For each case, compute the disagreement proportion and round it to $4$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,...]\"). The results must appear in the exact order of the test suite cases $1$ through $6$.",
            "solution": "We begin from fundamental definitions of independence testing in $2 \\times 2$ contingency tables. Let the observed counts be $a$, $b$, $c$, and $d$, forming the table\n$$\n\\begin{pmatrix}\na  b \\\\\nc  d\n\\end{pmatrix},\n$$\nwith row totals $r_1=a+b$, $r_2=c+d$, column totals $c_1=a+c$, $c_2=b+d$, and grand total $N=r_1+r_2$. In the biostatistical setting, each group $i \\in \\{1,2\\}$ comprises $n_i$ independent Bernoulli trials with success probability $p_i$, so $X_i \\sim \\text{Binomial}(n_i,p_i)$ yields the number of successes, and the table is constructed via $a=X_1$, $b=n_1-X_1$, $c=X_2$, $d=n_2-X_2$.\n\nUnder the null hypothesis that the success probabilities are equal across groups (independence in the sense of equal row conditional probabilities), the exact distribution of the table given fixed margins is hypergeometric. Fisher's exact test evaluates the probability of observing a table as extreme or more extreme than the observed table with respect to an appropriate tail criterion under the hypergeometric model, yielding a two-sided $p$-value. This test is exact in finite samples because it uses the discrete distribution induced by fixed margins.\n\nThe Pearson chi-square test is constructed by comparing observed cell counts to expected cell counts under independence. For the $2 \\times 2$ table, the classical Pearson chi-square statistic without correction is\n$$\n\\chi^2_{\\text{Pearson}} = \\sum_{i=1}^{2}\\sum_{j=1}^{2} \\frac{(O_{ij}-E_{ij})^2}{E_{ij}},\n$$\nwhere $O_{ij}$ are observed counts and $E_{ij} = \\frac{r_i c_j}{N}$ are expected counts under independence. The test statistic follows approximately a chi-square ($\\chi^2$) distribution with one degree of freedom ($df=1$) under the null hypothesis, by large-sample approximations such as the Central Limit Theorem (CLT) applied to properly standardized deviations.\n\nHowever, for small or moderate sample sizes and especially for rare events, the discrete nature of $O_{ij}$ leads to a non-negligible approximation error. Yates's correction for continuity addresses this by introducing a half-count continuity correction to better approximate the discrete distribution by the continuous $\\chi^2$ distribution. In the $2 \\times 2$ case, an equivalent and widely used formulation of the continuity-corrected statistic simplifies to\n$$\n\\chi^2_{\\text{Yates}} = \\frac{N\\left(\\left|ad - bc\\right| - \\frac{N}{2}\\right)^2}{r_1 r_2 c_1 c_2}.\n$$\nThis expression can be derived from the continuity-corrected deviation in each cell, $\\left(|O_{ij}-E_{ij}|-\\frac{1}{2}\\right)^2/E_{ij}$, exploiting algebraic relationships among the four cells and their margins in the $2 \\times 2$ case. The associated $p$-value is computed as the upper tail probability of the chi-square distribution with $df=1$, namely $p_{\\text{Yates}} = \\Pr\\left(\\chi^2_{1} \\ge \\chi^2_{\\text{Yates}}\\right)$, which is given by the survival function of the $\\chi^2$ distribution evaluated at $\\chi^2_{\\text{Yates}}$. If any marginal total is zero, the denominator $r_1 r_2 c_1 c_2$ equals zero and the statistic is undefined; in such degenerate cases, there is no information to infer association, and we set $p_{\\text{Yates}}=1$ by convention (non-significant).\n\nFor Fisher's exact test, we compute the two-sided $p$-value $p_{\\text{Fisher}}$ by summing hypergeometric probabilities of tables that are at least as extreme as the observed table under the null hypothesis of fixed margins. In practice, we use a well-tested numerical implementation that returns the two-sided $p$-value for the $2 \\times 2$ table.\n\nSimulation algorithm:\n- Fix $\\alpha=0.05$ and a pseudorandom number generator seed of $42$.\n- For each scenario $(n_1,n_2,p_1,p_2,R)$:\n  1. For $R$ replicates, draw $X_1 \\sim \\text{Binomial}(n_1,p_1)$ and $X_2 \\sim \\text{Binomial}(n_2,p_2)$, form $(a,b,c,d)$, and compute $p_{\\text{Yates}}$ and $p_{\\text{Fisher}}$ as described above.\n  2. Make significance decisions by the rule \"reject when $p \\le \\alpha$\" for each test.\n  3. Record disagreement when one test rejects and the other does not.\n  4. After $R$ replicates, compute the disagreement proportion as the number of disagreements divided by $R$, yielding a value in $[0,1]$.\n- Round each proportion to $4$ decimal places for the final reported values.\n\nTest suite coverage rationale:\n- Case $1$ provides a balanced design under the null with a moderate event probability ($p_1=p_2=0.20$), where both tests should be similar but small-sample discreteness can matter.\n- Case $2$ introduces unbalanced group sizes under the null, testing sensitivity to unequal margins.\n- Case $3$ uses rare events in a small balanced sample under the null, a situation where continuity correction is often impactful and exact methods are strongly preferred.\n- Case $4$ provides a balanced alternative with a moderate effect size ($p_1=0.20$, $p_2=0.40$), where power differences and discreteness may cause disagreements.\n- Case $5$ uses a large balanced sample with common events under the null, where approximations are typically accurate and disagreements should be rare.\n- Case $6$ provides a balanced alternative with rare events, combining low expected counts with an effect to probe conservative behavior of continuity correction versus exact testing.\n\nThe program implements the above simulation and prints a single line containing the six rounded disagreement proportions in the order of the test suite. No external inputs are needed, and reproducibility is ensured via the fixed seed $42$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import fisher_exact, chi2\n\ndef yates_p_value(a, b, c, d):\n    \"\"\"\n    Compute the Yates-corrected chi-square p-value for a 2x2 table.\n    If any marginal total is zero, return 1.0 by convention.\n    \"\"\"\n    r1 = a + b\n    r2 = c + d\n    c1 = a + c\n    c2 = b + d\n    N = r1 + r2\n\n    # Degenerate margins: no information to infer association\n    if r1 == 0 or r2 == 0 or c1 == 0 or c2 == 0:\n        return 1.0\n\n    # Continuity-corrected chi-square statistic for 2x2\n    cross = abs(a * d - b * c)\n    stat = N * (cross - N / 2.0) ** 2 / (r1 * r2 * c1 * c2)\n\n    # Upper tail p-value with df=1\n    p = chi2.sf(stat, df=1)\n    return float(p)\n\ndef fisher_p_value(a, b, c, d):\n    \"\"\"\n    Compute two-sided Fisher's exact test p-value for a 2x2 table.\n    \"\"\"\n    _, p = fisher_exact([[a, b], [c, d]], alternative=\"two-sided\")\n    return float(p)\n\ndef simulate_disagreement(n1, n2, p1, p2, R, rng, alpha=0.05):\n    disagreements = 0\n    for _ in range(R):\n        x1 = rng.binomial(n1, p1)\n        x2 = rng.binomial(n2, p2)\n        a = x1\n        b = n1 - x1\n        c = x2\n        d = n2 - x2\n\n        p_y = yates_p_value(a, b, c, d)\n        p_f = fisher_p_value(a, b, c, d)\n\n        sig_y = p_y = alpha\n        sig_f = p_f = alpha\n\n        if sig_y != sig_f:\n            disagreements += 1\n\n    return disagreements / R\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (n1, n2, p1, p2, R)\n    test_cases = [\n        (50, 50, 0.20, 0.20, 4000),   # Case 1: balanced, null, moderate event rate\n        (30, 120, 0.20, 0.20, 4000),  # Case 2: unbalanced, null, moderate event rate\n        (25, 25, 0.02, 0.02, 6000),   # Case 3: balanced, null, rare events, small sample\n        (60, 60, 0.20, 0.40, 4000),   # Case 4: balanced, alternative, moderate effect\n        (300, 300, 0.50, 0.50, 3000), # Case 5: balanced, null, large sample, common events\n        (100, 100, 0.01, 0.05, 5000), # Case 6: balanced, alternative, rare events\n    ]\n\n    rng = np.random.default_rng(42)\n    alpha = 0.05\n\n    results = []\n    for n1, n2, p1, p2, R in test_cases:\n        rate = simulate_disagreement(n1, n2, p1, p2, R, rng, alpha=alpha)\n        # Round to 4 decimal places as required\n        results.append(f\"{rate:.4f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}