{
    "hands_on_practices": [
        {
            "introduction": "A sound statistical analysis begins not with a test, but with a principled investigation of its underlying assumptions. This practice problem simulates a common scenario in clinical research, guiding you through a workflow to assess the assumptions of normality and equal variance. By comparing graphical methods with formal statistical tests, you will learn to make an informed decision between a pooled-variance t-test and a more robust Welch's t-test .",
            "id": "4895839",
            "problem": "A clinical researcher plans to compare baseline plasma low-density lipoprotein cholesterol concentrations between $2$ independent cohorts: a primary prevention group ($n_1 = 24$) and a secondary prevention group ($n_2 = 26$). The intended inferential method is a $2$-sample $t$ procedure on the difference in group means. The validity of the pooled-variance $2$-sample $t$ test requires that, for small to moderate $n$, each group’s outcome distribution is approximately normal and the population variances are equal; when variances are unequal, a Welch-type $t$ procedure is appropriate. Before selecting a test, the researcher wants a principled workflow that uses quantile–quantile (Q–Q) plots, the Shapiro–Wilk normality test, Levene’s equal variances test, and the Brown–Forsythe equal variances test, and a justification of each tool’s sensitivity and specificity.\n\nBase definitions to keep in mind:\n- Sensitivity is the probability of correctly detecting a violated assumption when it is present, that is, $\\mathbb{P}(\\text{reject } H_0 \\mid H_1 \\text{ true})$.\n- Specificity is the probability of correctly not flagging a violation when the assumption holds, that is, $\\mathbb{P}(\\text{not reject } H_0 \\mid H_0 \\text{ true}) = 1 - \\alpha$ for a level-$\\alpha$ test.\n\nThe Shapiro–Wilk test evaluates $H_0$: “the sample is drawn from a normal distribution” and is known to be among the most powerful omnibus tests for normality across a range of non-normal alternatives for small to moderate $n$. Levene’s test assesses equality of variances by testing group differences in transformed deviations $|X_{ij} - \\bar{X}_j|$ (deviation from the group mean $\\bar{X}_j$), while Brown–Forsythe uses $|X_{ij} - \\tilde{X}_j|$ (deviation from the group median $\\tilde{X}_j$), making it more robust to skewness and outliers.\n\nWhich option best specifies a principled workflow and correctly justifies the relative sensitivity and specificity of these tools for checking assumptions prior to choosing a pooled-variance $2$-sample $t$ test versus a Welch-type $t$ test?\n\nA. Begin with Q–Q plots of each group to visually assess linearity: if points track the reference line with only mild tail departures, proceed to Shapiro–Wilk at $\\alpha = 0.05$ for each group, interpreting $p$-values with caution because with small to moderate $n$ the test may have limited sensitivity to subtle deviations but good specificity under true normality. For equal variances, first consult the Q–Q plots for evidence of skewness or outliers; if such features are present, use Brown–Forsythe because median-based deviations provide higher specificity (better type I error control) under non-normality with comparable sensitivity to variance differences, whereas Levene’s mean-based deviations can suffer inflated type I error under skewness. If normality appears plausible and no outliers are evident, Levene is acceptable. Choose the pooled $t$ if equal variances are not rejected; otherwise use Welch’s $t$.\n\nB. Skip Q–Q plots and rely exclusively on Shapiro–Wilk with $\\alpha = 0.10$ to “increase both sensitivity and specificity” for normality screening. For variances, always prefer Levene because it is more sensitive than Brown–Forsythe, and always use the pooled $t$ test whenever the Levene $p$-value exceeds $0.10$.\n\nC. After Q–Q plots, if any deviation from linearity is seen, immediately transform the data and do not perform Shapiro–Wilk because it lacks sensitivity for moderate $n$. For variances, always use Bartlett’s test because it has the highest specificity regardless of distributional shape, and use the pooled $t$ test whenever Bartlett’s $p$-value exceeds $0.05$.\n\nD. Use Q–Q plots followed by Shapiro–Wilk. If non-normality is detected, switch to the Mann–Whitney test. For variances, run both Levene and Brown–Forsythe but prefer Levene when outliers are present because mean-based deviations “emphasize extremes” and thus increase sensitivity, and note that Q–Q plots have low specificity so they should be down-weighted in decisions about normality.\n\nSelect the single best option.",
            "solution": "The user has requested a critical validation of the problem statement, followed by a detailed solution and evaluation of the provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Groups:** Two independent cohorts: a primary prevention group (Group $1$) and a secondary prevention group (Group $2$).\n-   **Sample Sizes:** $n_1 = 24$, $n_2 = 26$.\n-   **Objective:** Compare baseline plasma low-density lipoprotein cholesterol concentrations via the difference in group means.\n-   **Intended Inferential Method:** A $2$-sample $t$ procedure.\n-   **Assumptions for Pooled-Variance $2$-sample $t$-test:** For small to moderate sample sizes ($n$), outcomes in each group are approximately normal, and population variances are equal.\n-   **Alternative Method:** A Welch-type $t$ procedure is used for unequal variances.\n-   **Tools for Assumption Checking:** Quantile–quantile (Q–Q) plots, Shapiro–Wilk normality test, Levene’s equal variances test, Brown–Forsythe equal variances test.\n-   **Definitions:**\n    -   Sensitivity: $\\mathbb{P}(\\text{reject } H_0 \\mid H_1 \\text{ true})$.\n    -   Specificity: $\\mathbb{P}(\\text{not reject } H_0 \\mid H_0 \\text{ true}) = 1 - \\alpha$ for a level-$\\alpha$ test.\n-   **Test Descriptions:**\n    -   Shapiro–Wilk test: Evaluates the null hypothesis, $H_0$: “the sample is drawn from a normal distribution.” It is powerful for small to moderate $n$.\n    -   Levene’s test: Assesses equal variances using absolute deviations from the group mean, $|X_{ij} - \\bar{X}_j|$.\n    -   Brown–Forsythe test: Assesses equal variances using absolute deviations from the group median, $|X_{ij} - \\tilde{X}_j|$, making it more robust to skewness and outliers.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding:** The problem is firmly rooted in standard biostatistical methodology. The distinction between pooled-variance and Welch-Satterthwaite ($t$-type) procedures, along with their underlying assumptions of normality and homogeneity of variance, are fundamental concepts. The tools listed for assumption-checking (Q–Q plots, Shapiro–Wilk, Levene, Brown–Forsythe) are all standard, widely accepted methods. The provided definitions of sensitivity, specificity, and the descriptions of the tests are factually correct and consistent with statistical theory.\n-   **Well-Posed:** The problem is well-posed. It asks for the \"best\" option describing a \"principled workflow,\" which is a request to evaluate the statistical a-priori reasoning presented in each option against established best practices. A unique best answer can be determined by identifying which option presents the most statistically sound and nuanced approach.\n-   **Objectivity:** The problem is stated in objective, precise language. All terms are either standard in the field or explicitly defined.\n-   **Flaw Check:**\n    1.  **Scientific/Factual Unsoundness:** None. The premises are sound.\n    2.  **Non-Formalizable/Irrelevant:** None. The problem is directly relevant to biostatistics and is formalizable.\n    3.  **Incomplete/Contradictory:** None. The information provided is sufficient to evaluate the options.\n    4.  **Unrealistic/Infeasible:** None. The scenario is a realistic representation of a common task in clinical research.\n    5.  **Ill-Posed/Poorly Structured:** None.\n    6.  **Pseudo-Profound/Trivial:** None. The question requires a nuanced understanding of the properties and limitations of various statistical tests.\n    7.  **Outside Scientific Verifiability:** None. The claims within the options are verifiable against statistical literature and theory.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the solution.\n\n### Derivation and Option Analysis\n\nA principled workflow for checking the assumptions of a two-sample $t$-test involves a combination of graphical assessment and formal hypothesis testing, with an awareness of the limitations of each approach, especially with small to moderate sample sizes as given ($n_1 = 24$, $n_2 = 26$).\n\n**Principles of a Valid Workflow:**\n1.  **Graphical First:** Analysis should always begin with data visualization (e.g., Q–Q plots for normality, boxplots for spread and symmetry). Graphical methods provide crucial context that single $p$-values from formal tests cannot.\n2.  **Normality Assessment:** The $t$-test is reasonably robust to mild-to-moderate deviations from normality, particularly for symmetric distributions. The primary concerns are heavy skewness and the presence of outliers.\n    -   Q–Q plots are the standard graphical tool. For small samples, some deviation from the reference line is expected even under true normality.\n    -   The Shapiro–Wilk test is a powerful formal test for normality. Its null hypothesis is that the data *are* normal. A key issue is that for small $n$, its power (sensitivity) to detect non-normality is limited. Thus, a non-significant result ($p > \\alpha$) does not prove normality, it only signifies a failure to detect non-normality. The specificity of the test, $\\mathbb{P}(\\text{not reject } H_0 \\mid H_0 \\text{ true})$, is by definition equal to $1-\\alpha$.\n3.  **Homogeneity of Variance Assessment:** This assumption is critical for the pooled-variance $t$-test. Its violation can lead to significant inflation of the Type I error rate.\n    -   Levene’s test uses an ANOVA on the absolute deviations from the mean ($|X_{ij} - \\bar{X}_j|$). Because the mean is sensitive to skewness and outliers, Levene’s test can have an inflated Type I error rate (i.e., low specificity) when the underlying distributions are skewed, incorrectly rejecting the null of equal variances.\n    -   The Brown–Forsythe test modifies Levene's test by using the absolute deviations from the median ($|X_{ij} - \\tilde{X}_j|$). Since the median is robust to skewness and outliers, this test maintains better control over the Type I error rate (i.e., has better specificity) in the presence of non-normality. It is therefore generally preferred over Levene’s test, especially if there is any doubt about the symmetry of the distributions.\n    -   Bartlett's test is another option, but it is highly sensitive to non-normality and should only be used when the data are known to be normally distributed.\n4.  **Decision Logic:** Based on the assumption checks, a decision is made. If normality is plausible and variances are deemed equal, the pooled $t$-test is used. If normality is plausible but variances are unequal, the Welch $t$-test is the appropriate choice. The Welch test is generally a safer default, as its performance is close to the pooled test when variances are equal and far superior when they are not.\n\n**Evaluation of Options:**\n\n**A. Begin with Q–Q plots of each group to visually assess linearity: if points track the reference line with only mild tail departures, proceed to Shapiro–Wilk at $\\alpha = 0.05$ for each group, interpreting $p$-values with caution because with small to moderate $n$ the test may have limited sensitivity to subtle deviations but good specificity under true normality. For equal variances, first consult the Q–Q plots for evidence of skewness or outliers; if such features are present, use Brown–Forsythe because median-based deviations provide higher specificity (better type I error control) under non-normality with comparable sensitivity to variance differences, whereas Levene’s mean-based deviations can suffer inflated type I error under skewness. If normality appears plausible and no outliers are evident, Levene is acceptable. Choose the pooled $t$ if equal variances are not rejected; otherwise use Welch’s $t$.**\nThis option describes a comprehensive and statistically sound workflow.\n-   It correctly prioritizes visual inspection with Q–Q plots.\n-   It provides a correct and nuanced interpretation of the Shapiro–Wilk test, acknowledging its limited sensitivity with small $n$ and its defined specificity ($1-\\alpha$).\n-   It correctly links the choice between Levene's and Brown–Forsythe's tests to the shape of the distribution (skewness/outliers) noted from the initial visual checks.\n-   The justification for preferring Brown–Forsythe (robustness, better Type I error control/specificity under non-normality) is correct.\n-   The final decision logic is consistent with the goal of the problem.\n**Verdict: Correct**\n\n**B. Skip Q–Q plots and rely exclusively on Shapiro–Wilk with $\\alpha = 0.10$ to “increase both sensitivity and specificity” for normality screening. For variances, always prefer Levene because it is more sensitive than Brown–Forsythe, and always use the pooled $t$ test whenever the Levene $p$-value exceeds $0.10$.**\nThis option contains multiple fundamental errors.\n-   Skipping graphical checks is poor statistical practice.\n-   The statement that increasing $\\alpha$ increases both sensitivity and specificity is false. Increasing the significance level $\\alpha$ from, for example, $0.05$ to $0.10$ increases the test's sensitivity (power) but *decreases* its specificity, which is $1-\\alpha$ (from $0.95$ to $0.90$).\n-   \"Always prefer Levene\" is poor advice; the Brown–Forsythe test is generally more robust and often preferred. The claim that Levene is more sensitive is an oversimplification that ignores its critical lack of robustness (poor specificity) in non-normal situations.\n**Verdict: Incorrect**\n\n**C. After Q–Q plots, if any deviation from linearity is seen, immediately transform the data and do not perform Shapiro–Wilk because it lacks sensitivity for moderate $n$. For variances, always use Bartlett’s test because it has the highest specificity regardless of distributional shape, and use the pooled $t$ test whenever Bartlett’s $p$-value exceeds $0.05$.**\nThis option is based on flawed reasoning and incorrect facts.\n-   Recommending immediate transformation for *any* deviation on a Q–Q plot is too aggressive, as small-sample plots are rarely perfect.\n-   The claim that Bartlett's test has \"the highest specificity regardless of distributional shape\" is factually wrong. Bartlett's test is notoriously sensitive to departures from normality, meaning it has poor specificity when the data are not normal.\n-   The reasoning for not using Shapiro–Wilk is weak; its limitations call for caution, not complete avoidance in a thorough assessment.\n**Verdict: Incorrect**\n\n**D. Use Q–Q plots followed by Shapiro–Wilk. If non-normality is detected, switch to the Mann–Whitney test. For variances, run both Levene and Brown–Forsythe but prefer Levene when outliers are present because mean-based deviations “emphasize extremes” and thus increase sensitivity, and note that Q–Q plots have low specificity so they should be down-weighted in decisions about normality.**\nThis option contains incorrect justifications.\n-   The advice to \"prefer Levene when outliers are present\" is backwards and dangerous. The mean's sensitivity to outliers is exactly why Levene's test is *not* robust and can lead to spurious rejections of the equal-variance hypothesis (low specificity). The robust Brown–Forsythe test is the correct choice in the presence of outliers.\n-   Applying the formal term \"specificity\" to a graphical plot is imprecise. More importantly, advising that Q–Q plots should be \"down-weighted\" is contrary to good practice; visual diagnostics are essential for understanding data and providing context for formal tests.\n-   While switching to a Mann–Whitney test is a possible strategy, the core of the variance-test justification is critically flawed.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Beyond simply reacting to assumption violations, a skilled biostatistician can proactively design studies to be more efficient and powerful. This exercise moves from assumption *checking* to study *optimization* in a scenario where population variances are known to be unequal. You will derive the optimal allocation of participants to two groups that minimizes the variance of the mean difference, thereby maximizing the study's statistical power for a fixed sample size .",
            "id": "4895832",
            "problem": "A clinical biostatistics study aims to compare the mean concentration of a blood biomarker between two independent cohorts: a treatment group and a control group. Let the treatment group observations be denoted $\\{X_{1i}\\}_{i=1}^{n_1}$ with population mean $\\mu_1$ and variance $\\sigma_1^{2}$, and the control group observations be denoted $\\{X_{2j}\\}_{j=1}^{n_2}$ with population mean $\\mu_2$ and variance $\\sigma_2^{2}$. Assume:\n- Within each cohort, observations are independent and identically distributed with finite second moments.\n- The cohorts are independent of each other.\n- Either the observations are normally distributed or the Central Limit Theorem (CLT) applies so that the sample means are approximately normal for sufficiently large sample sizes.\n\nUnder heteroscedasticity (that is, $\\sigma_1^{2} \\neq \\sigma_2^{2}$), the investigator plans a fixed total sample size $N$ and must choose $n_1$ and $n_2$ subject to the constraint $n_1 + n_2 = N$. Starting from the above assumptions and fundamental properties of variance, first derive the variance of the difference of sample means, $\\bar{X}_1 - \\bar{X}_2$, in terms of $n_1$, $n_2$, $\\sigma_1^{2}$, and $\\sigma_2^{2}$. Then, treating $n_1$ and $n_2$ as positive real decision variables, determine the allocation $(n_1, n_2)$ that minimizes the variance of $\\bar{X}_1 - \\bar{X}_2$ subject to $n_1 + n_2 = N$.\n\nExpress your final answer as a single closed-form analytic expression for the optimal allocation vector using the LaTeX row matrix notation $\\begin{pmatrix} \\cdot & \\cdot \\end{pmatrix}$. No rounding is required.",
            "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Treatment Group:** A set of observations $\\{X_{1i}\\}_{i=1}^{n_1}$ from a population with mean $\\mu_1$ and variance $\\sigma_1^{2}$. The sample size is $n_1$.\n- **Control Group:** A set of observations $\\{X_{2j}\\}_{j=1}^{n_2}$ from a population with mean $\\mu_2$ and variance $\\sigma_2^{2}$. The sample size is $n_2$.\n- **Assumption 1:** Within each group, observations are independent and identically distributed (i.i.d.) with finite second moments.\n- **Assumption 2:** The two groups (cohorts) are independent of each other.\n- **Assumption 3:** The observations are either from a normal distribution, or the sample sizes are large enough for the Central Limit Theorem (CLT) to apply to the sample means.\n- **Condition:** The variances are unequal (heteroscedasticity), i.e., $\\sigma_1^{2} \\neq \\sigma_2^{2}$.\n- **Constraint:** The total sample size is fixed at $N$, such that $n_1 + n_2 = N$.\n- **Objective:** First, derive the variance of the difference of sample means, $\\operatorname{Var}(\\bar{X}_1 - \\bar{X}_2)$. Second, find the allocation of sample sizes, $(n_1, n_2)$, that minimizes this variance, treating $n_1$ and $n_2$ as positive real variables.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a classic and fundamental exercise in statistical experimental design. The objective of minimizing the variance of an estimator (in this case, the difference in sample means) is a core principle for achieving maximum statistical power and precision for a fixed total sample size. The assumptions provided are standard for two-sample t-procedures. The problem described is known as Neyman allocation.\n- **Well-Posed:** The problem is well-posed. It specifies a clear objective function, $\\operatorname{Var}(\\bar{X}_1 - \\bar{X}_2)$, and a well-defined constraint, $n_1 + n_2 = N$. The treatment of $n_1$ and $n_2$ as continuous variables is a standard mathematical technique for solving this optimization problem, with the understanding that the resulting real numbers would be rounded to integers in a practical application. A unique minimum is expected.\n- **Objective:** The problem is stated using precise mathematical and statistical language, free from ambiguity or subjective content.\n\nThe problem does not exhibit any of the flaws listed in the invalidation criteria. It is scientifically sound, formally structured, complete, and poses a meaningful question in the field of biostatistics.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Derivation of the Solution\n\nThe first task is to derive an expression for the variance of the difference between the two sample means, $\\operatorname{Var}(\\bar{X}_1 - \\bar{X}_2)$.\n\nThe sample mean for the treatment group is defined as $\\bar{X}_1 = \\frac{1}{n_1} \\sum_{i=1}^{n_1} X_{1i}$. The variance of this sample mean, given that the observations $X_{1i}$ are i.i.d. with variance $\\sigma_1^2$, is:\n$$ \\operatorname{Var}(\\bar{X}_1) = \\operatorname{Var}\\left(\\frac{1}{n_1} \\sum_{i=1}^{n_1} X_{1i}\\right) $$\nUsing the property $\\operatorname{Var}(aY) = a^2 \\operatorname{Var}(Y)$ and that the variance of a sum of independent variables is the sum of their variances:\n$$ \\operatorname{Var}(\\bar{X}_1) = \\frac{1}{n_1^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n_1} X_{1i}\\right) = \\frac{1}{n_1^2} \\sum_{i=1}^{n_1} \\operatorname{Var}(X_{1i}) = \\frac{1}{n_1^2} (n_1 \\sigma_1^2) = \\frac{\\sigma_1^2}{n_1} $$\nSimilarly, for the control group, the variance of the sample mean $\\bar{X}_2$ is:\n$$ \\operatorname{Var}(\\bar{X}_2) = \\frac{\\sigma_2^2}{n_2} $$\nSince the two cohorts are independent, the random variables $\\bar{X}_1$ and $\\bar{X}_2$ are independent. The variance of the difference of two independent random variables is the sum of their variances:\n$$ \\operatorname{Var}(\\bar{X}_1 - \\bar{X}_2) = \\operatorname{Var}(\\bar{X}_1) + \\operatorname{Var}(\\bar{X}_2) $$\nSubstituting the expressions for the variances of the sample means gives the objective function to be minimized, which we denote as $V$:\n$$ V(n_1, n_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} $$\nThe second task is to minimize this function subject to the constraint $n_1 + n_2 = N$. We can solve this constrained optimization problem by substituting the constraint into the objective function. From the constraint, we have $n_2 = N - n_1$. Substituting this into the variance expression yields a function of a single variable, $n_1$:\n$$ V(n_1) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{N - n_1} $$\nTo find the value of $n_1$ that minimizes $V$, we take the first derivative of $V(n_1)$ with respect to $n_1$ and set it to zero. The domain for $n_1$ is $(0, N)$.\n$$ \\frac{dV}{dn_1} = \\frac{d}{dn_1} \\left( \\sigma_1^2 n_1^{-1} + \\sigma_2^2 (N - n_1)^{-1} \\right) = -\\sigma_1^2 n_1^{-2} - \\sigma_2^2 (N - n_1)^{-2} (-1) $$\n$$ \\frac{dV}{dn_1} = -\\frac{\\sigma_1^2}{n_1^2} + \\frac{\\sigma_2^2}{(N - n_1)^2} $$\nSetting the derivative to zero to find the critical point:\n$$ \\frac{\\sigma_2^2}{(N - n_1)^2} = \\frac{\\sigma_1^2}{n_1^2} $$\nSince standard deviations $\\sigma_1, \\sigma_2$ are non-negative and sample sizes $n_1, n_2$ are positive, we can take the square root of both sides:\n$$ \\frac{\\sigma_2}{N - n_1} = \\frac{\\sigma_1}{n_1} $$\nNow, we solve for $n_1$:\n$$ n_1 \\sigma_2 = (N - n_1) \\sigma_1 = N\\sigma_1 - n_1\\sigma_1 $$\n$$ n_1 \\sigma_2 + n_1 \\sigma_1 = N\\sigma_1 $$\n$$ n_1 (\\sigma_1 + \\sigma_2) = N\\sigma_1 $$\n$$ n_1 = N \\frac{\\sigma_1}{\\sigma_1 + \\sigma_2} $$\nTo confirm that this is a minimum, we can check the second derivative:\n$$ \\frac{d^2V}{dn_1^2} = \\frac{d}{dn_1} \\left( -\\sigma_1^2 n_1^{-2} + \\sigma_2^2 (N - n_1)^{-2} \\right) = 2\\sigma_1^2 n_1^{-3} + 2\\sigma_2^2(N-n_1)^{-3} $$\nSince $\\sigma_1^2, \\sigma_2^2, n_1, N-n_1$ are all positive, $\\frac{d^2V}{dn_1^2} > 0$, confirming the critical point corresponds to a minimum.\n\nNow we find the optimal value for $n_2$ using the constraint $n_2 = N - n_1$:\n$$ n_2 = N - N \\frac{\\sigma_1}{\\sigma_1 + \\sigma_2} = N \\left( 1 - \\frac{\\sigma_1}{\\sigma_1 + \\sigma_2} \\right) = N \\left( \\frac{\\sigma_1 + \\sigma_2 - \\sigma_1}{\\sigma_1 + \\sigma_2} \\right) $$\n$$ n_2 = N \\frac{\\sigma_2}{\\sigma_1 + \\sigma_2} $$\nThis result, known as Neyman allocation, indicates that the optimal sample size for each group is proportional to its population standard deviation.\n\nThe optimal allocation vector $(n_1, n_2)$ is therefore:\n$$ \\left( N \\frac{\\sigma_1}{\\sigma_1 + \\sigma_2}, N \\frac{\\sigma_2}{\\sigma_1 + \\sigma_2} \\right) $$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nN \\frac{\\sigma_1}{\\sigma_1 + \\sigma_2} & N \\frac{\\sigma_2}{\\sigma_1 + \\sigma_2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The assumption of independence is a cornerstone of the two-sample t-test, yet it is often violated in practice by study designs involving clustering, such as patients within the same clinic or multiple measurements from the same subject. This advanced problem provides a hands-on opportunity to quantify the consequences of such a violation. You will calculate the exact bias introduced into the pooled variance estimator when intra-cluster correlation is present, revealing how ignoring this structure can compromise your statistical conclusions .",
            "id": "4895853",
            "problem": "A biostatistics team plans to compare mean biomarker levels between two treatment groups using a two-sample Student’s $t$ test, which assumes independence of observations within each group. To examine the consequences of violating this assumption due to clustering, consider the following data-generating mechanism for each group $g \\in \\{1,2\\}$.\n\nWithin group $g$, there are $m_g$ distinct clusters, each of size $K_g$, so the total number of observations is $n_g = m_g K_g$. Observations follow the random-effects model\n$$\nX_{g,c,i} = \\mu_g + U_{g,c} + \\epsilon_{g,c,i},\n$$\nwhere $c \\in \\{1,\\dots,m_g\\}$ indexes clusters and $i \\in \\{1,\\dots,K_g\\}$ indexes individuals within cluster. The cluster effects $U_{g,c}$ are independent and identically distributed with $\\mathbb{E}[U_{g,c}] = 0$ and $\\operatorname{Var}(U_{g,c}) = \\tau_g^{2}$. The individual errors $\\epsilon_{g,c,i}$ are independent and identically distributed with $\\mathbb{E}[\\epsilon_{g,c,i}] = 0$ and $\\operatorname{Var}(\\epsilon_{g,c,i}) = \\sigma_g^{2}$, and are independent of all $U_{g,c}$. Consequently, two individuals in the same cluster within group $g$ have covariance $\\tau_g^{2}$, while individuals from different clusters have covariance $0$. The marginal variance in group $g$ is $v_g = \\tau_g^{2} + \\sigma_g^{2}$. Assume the groups have equal marginal variance $v_1 = v_2 = v$ but may have different clustering parameters.\n\nLet $S_g^{2}$ denote the usual sample variance computed across all $n_g$ observations in group $g$ ignoring clustering, and let the pooled variance estimator be\n$$\nS_{p}^{2} = \\frac{(n_{1}-1) S_{1}^{2} + (n_{2}-1) S_{2}^{2}}{n_{1} + n_{2} - 2}.\n$$\nDefine the intraclass correlation coefficient (ICC) as $\\rho_g = \\tau_g^{2}/v$. You will analyze the bias in $S_{p}^{2}$ when clustering is ignored.\n\nStarting from the definitions of variance, covariance, and the sample variance, and using only the properties of expectations and covariances for the model given above, derive an analytic expression for the bias $\\mathbb{E}[S_{p}^{2}] - v$ in terms of $(n_1,n_2,K_1,K_2,\\rho_1,\\rho_2,v)$. Then, in the specific counterexample where $m_{1} = 10$, $K_{1} = 5$, $m_{2} = 8$, $K_{2} = 5$, $v = 1$, $\\rho_{1} = 0.1$, and $\\rho_{2} = 0.2$, compute the numerical value of the bias as a single exact number. If you obtain an exact rational number, report it exactly; no rounding is required.",
            "solution": "The two-sample Student’s $t$ procedures assume independence of observations within each group. Under the given clustered model, observations within the same cluster share the random effect $U_{g,c}$, so they are positively correlated and the independence assumption is violated. We quantify how this affects the pooled variance estimator $S_{p}^{2}$.\n\nWe begin with a general identity for the expectation of the usual sample variance under possible dependence. For a collection of $N$ observations $\\{Y_{1},\\dots,Y_{N}\\}$ with common mean $\\mu$, define the sample variance\n$$\nS^{2} = \\frac{1}{N-1} \\sum_{i=1}^{N} \\left( Y_{i} - \\bar{Y} \\right)^{2}, \\quad \\text{where } \\bar{Y} = \\frac{1}{N} \\sum_{i=1}^{N} Y_{i}.\n$$\nUsing $\\sum_{i=1}^{N}(Y_i - \\bar{Y})^{2} = \\sum_{i=1}^{N}Y_i^{2} - N \\bar{Y}^{2}$ and linearity of expectation,\n$$\n\\mathbb{E}[S^{2}] = \\frac{1}{N-1} \\left( \\sum_{i=1}^{N} \\mathbb{E}[Y_i^{2}] - N \\,\\mathbb{E}[\\bar{Y}^{2}] \\right).\n$$\nWe write $\\mathbb{E}[Y_i^{2}] = \\operatorname{Var}(Y_i) + \\mu^{2}$ and\n$$\n\\mathbb{E}[\\bar{Y}^{2}] = \\operatorname{Var}(\\bar{Y}) + \\mu^{2} = \\frac{1}{N^{2}} \\left( \\sum_{i=1}^{N} \\operatorname{Var}(Y_i) + \\sum_{\\substack{i,j=1 \\\\ i \\neq j}}^{N} \\operatorname{Cov}(Y_i,Y_j) \\right) + \\mu^{2}.\n$$\nSubstituting these into the expression for $\\mathbb{E}[S^{2}]$ and simplifying yields the general formula\n$$\n\\mathbb{E}[S^{2}] = \\frac{(N-1)\\,\\overline{v} - \\frac{1}{N} \\sum_{\\substack{i,j=1 \\\\ i \\neq j}}^{N} \\operatorname{Cov}(Y_i,Y_j)}{N-1},\n$$\nwhere $\\overline{v} = \\frac{1}{N} \\sum_{i=1}^{N} \\operatorname{Var}(Y_i)$. In the special case where all $Y_i$ share the same variance $v$,\n$$\n\\mathbb{E}[S^{2}] = v - \\frac{1}{N(N-1)} \\sum_{\\substack{i,j=1 \\\\ i \\neq j}}^{N} \\operatorname{Cov}(Y_i,Y_j).\n$$\n\nApply this to each group $g \\in \\{1,2\\}$. Within group $g$, $N = n_g$, each observation $X_{g,c,i}$ has variance $\\operatorname{Var}(X_{g,c,i}) = v_g = \\tau_g^{2} + \\sigma_g^{2}$, and the covariance between two distinct observations is\n$$\n\\operatorname{Cov}(X_{g,c,i}, X_{g,c',j}) = \n\\begin{cases}\n\\tau_g^{2}, & \\text{if } c = c' \\text{ and } i \\neq j, \\\\\n0, & \\text{if } c \\neq c'.\n\\end{cases}\n$$\nThe sum of covariances over ordered pairs $i \\neq j$ in group $g$ equals the number of ordered within-cluster pairs times $\\tau_g^{2}$. In a single cluster of size $K_g$, the number of ordered distinct pairs is $K_g(K_g-1)$. With $m_g$ clusters, the total over all clusters is $m_g K_g (K_g - 1)$. Therefore,\n$$\n\\sum_{\\substack{(c,i),(c',j) \\\\ (c,i) \\neq (c',j)}} \\operatorname{Cov}(X_{g,c,i}, X_{g,c',j}) = \\tau_g^{2}\\, m_g K_g (K_g - 1) = \\tau_g^{2}\\, n_g (K_g - 1).\n$$\nPlugging into the expectation formula for the sample variance in group $g$,\n$$\n\\mathbb{E}[S_{g}^{2}] = v_g - \\frac{1}{n_g(n_g - 1)} \\left( \\tau_g^{2}\\, n_g (K_g - 1) \\right) = v_g - \\tau_g^{2}\\, \\frac{K_g - 1}{n_g - 1}.\n$$\nUnder the assumption $v_1 = v_2 = v$, we obtain a clean expression for each group:\n$$\n\\mathbb{E}[S_{g}^{2}] = v - \\tau_g^{2}\\, \\frac{K_g - 1}{n_g - 1}, \\quad g \\in \\{1,2\\}.\n$$\n\nNow consider the pooled variance estimator\n$$\nS_{p}^{2} = \\frac{(n_{1}-1) S_{1}^{2} + (n_{2}-1) S_{2}^{2}}{n_{1} + n_{2} - 2}.\n$$\nTaking expectations and using linearity,\n$$\n\\mathbb{E}[S_{p}^{2}] = \\frac{(n_{1}-1)\\, \\mathbb{E}[S_{1}^{2}] + (n_{2}-1)\\, \\mathbb{E}[S_{2}^{2}]}{n_{1} + n_{2} - 2}.\n$$\nSubstitute the expressions for $\\mathbb{E}[S_{g}^{2}]$:\n$$\n\\mathbb{E}[S_{p}^{2}] = \\frac{(n_{1}-1)\\left( v - \\tau_{1}^{2}\\, \\frac{K_{1} - 1}{n_{1} - 1} \\right) + (n_{2}-1)\\left( v - \\tau_{2}^{2}\\, \\frac{K_{2} - 1}{n_{2} - 1} \\right)}{n_{1} + n_{2} - 2}.\n$$\nSimplifying,\n$$\n\\mathbb{E}[S_{p}^{2}] = \\frac{(n_{1}-1)v + (n_{2}-1)v - \\left[ \\tau_{1}^{2}(K_{1} - 1) + \\tau_{2}^{2}(K_{2} - 1) \\right]}{n_{1} + n_{2} - 2} = v - \\frac{\\tau_{1}^{2}(K_{1} - 1) + \\tau_{2}^{2}(K_{2} - 1)}{n_{1} + n_{2} - 2}.\n$$\nHence the bias is\n$$\n\\mathbb{E}[S_{p}^{2}] - v = - \\frac{\\tau_{1}^{2}(K_{1} - 1) + \\tau_{2}^{2}(K_{2} - 1)}{n_{1} + n_{2} - 2}.\n$$\nExpressing in terms of the intraclass correlation coefficient (ICC) $\\rho_g = \\tau_g^{2}/v$,\n$$\n\\mathbb{E}[S_{p}^{2}] - v = - \\, v \\, \\frac{\\rho_{1}(K_{1} - 1) + \\rho_{2}(K_{2} - 1)}{n_{1} + n_{2} - 2}.\n$$\n\nWe now compute the requested numerical value in the specified counterexample: $m_{1} = 10$, $K_{1} = 5$, $m_{2} = 8$, $K_{2} = 5$, $v = 1$, $\\rho_{1} = 0.1$, and $\\rho_{2} = 0.2$. First, $n_{1} = m_{1} K_{1} = 10 \\cdot 5 = 50$ and $n_{2} = m_{2} K_{2} = 8 \\cdot 5 = 40$. With $v = 1$, we have $\\tau_{1}^{2} = \\rho_{1} v = 0.1$ and $\\tau_{2}^{2} = \\rho_{2} v = 0.2$. Then\n$$\n\\mathbb{E}[S_{p}^{2}] - v = - \\frac{ \\tau_{1}^{2}(K_{1} - 1) + \\tau_{2}^{2}(K_{2} - 1) }{n_{1} + n_{2} - 2} = - \\frac{ 0.1 \\cdot 4 + 0.2 \\cdot 4 }{50 + 40 - 2}.\n$$\nCompute the numerator and denominator:\n$$\n0.1 \\cdot 4 + 0.2 \\cdot 4 = 0.4 + 0.8 = 1.2, \\quad 50 + 40 - 2 = 88,\n$$\nso\n$$\n\\mathbb{E}[S_{p}^{2}] - v = - \\frac{1.2}{88} = - \\frac{6/5}{88} = - \\frac{6}{440} = - \\frac{3}{220}.\n$$\nThis exact rational number shows a downward bias due to positive within-cluster correlation and violation of independence.",
            "answer": "$$\\boxed{-\\frac{3}{220}}$$"
        }
    ]
}