{
    "hands_on_practices": [
        {
            "introduction": "When an ANOVA test reveals a significant difference among group means, the next step is to identify which specific pairs of groups differ. Performing multiple t-tests inflates the probability of making at least one Type I error, an issue known as the multiple comparisons problem. This exercise introduces the Bonferroni correction, a fundamental and universally applicable method for controlling this family-wise error rate (FWER). By deriving the correction from the basic principles of probability and applying it to a set of p-values, you will gain a foundational understanding of how multiple testing adjustments work .",
            "id": "4938829",
            "problem": "A biostatistics study compares mean serum biomarker levels across $k$ treatment groups using a one-way Analysis of Variance (ANOVA). Post-hoc pairwise comparisons among all groups are performed using two-sample $t$-tests, yielding $m=10$ raw $p$-values for the set of all pairwise contrasts: $p=(0.001,\\,0.02,\\,0.07,\\,0.11,\\,0.23,\\,0.045,\\,0.0045,\\,0.39,\\,0.003,\\,0.12)$. The investigator wishes to control the Family-Wise Error Rate (FWER) at level $\\alpha=0.05$ across these $m$ comparisons. Starting from the union bound on probabilities and the definition of FWER, derive the Bonferroni family-wise control and the corresponding adjusted $p$-value mapping for a collection of $m$ simultaneous tests. Then apply this adjustment to the given raw $p$-values and determine the total number of rejected pairwise null hypotheses at level $\\alpha=0.05$. Report only the total count of rejections as your final numerical answer. No rounding is needed. Briefly justify, based on first principles, why the Bonferroni approach is applicable in this setting compared to Tukey, Scheffé, or Dunnett procedures, without using any shortcut formulas in the problem statement.",
            "solution": "The problem is assessed as valid. It is scientifically grounded in the principles of statistical hypothesis testing, well-posed with sufficient information for a unique solution, and objective in its formulation.\n\nThe problem requires the derivation of the Bonferroni correction for controlling the Family-Wise Error Rate (FWER), its application to a given set of $p$-values, a count of the resulting rejections, and a justification for its use.\n\nFirst, we derive the Bonferroni correction from first principles. Let there be a family of $m$ null hypotheses, denoted by $H_{0,i}$ for $i \\in \\{1, 2, \\dots, m\\}$. The Family-Wise Error Rate (FWER) is defined as the probability of making at least one Type I error. A Type I error is the event of rejecting a true null hypothesis. Let $I_0$ be the subset of indices $\\{1, 2, \\dots, m\\}$ for which the corresponding null hypothesis $H_{0,i}$ is true. The FWER is then:\n$$\n\\text{FWER} = P\\left(\\bigcup_{i \\in I_0} \\{\\text{reject } H_{0,i}\\}\\right)\n$$\nSince the set $I_0$ is unknown in a real experimental setting, we seek to control the FWER by bounding the probability of rejecting any of the $m$ hypotheses, which is a conservative upper bound on the true FWER:\n$$\n\\text{FWER} \\le P\\left(\\bigcup_{i=1}^{m} \\{\\text{reject } H_{0,i}\\}\\right)\n$$\nWe invoke the union bound, also known as Boole's inequality, which states that for any collection of events $A_1, A_2, \\dots, A_m$, the probability of their union is less than or equal to the sum of their individual probabilities: $P(\\cup_{i=1}^m A_i) \\le \\sum_{i=1}^m P(A_i)$. Applying this to our expression for the FWER upper bound, where $A_i$ is the event {reject $H_{0,i}$}:\n$$\n\\text{FWER} \\le \\sum_{i=1}^{m} P(\\text{reject } H_{0,i})\n$$\nFor a single hypothesis test $i$, we reject $H_{0,i}$ if its associated $p$-value, $p_i$, is less than or equal to a chosen significance level, $\\alpha_i$. Under a true null hypothesis, the probability of rejecting it is controlled by this level: $P(\\text{reject } H_{0,i} | H_{0,i} \\text{ is true}) = \\alpha_i$. Therefore, $P(\\text{reject } H_{0,i}) \\le \\alpha_i$ for any $i$. Substituting this into the inequality gives:\n$$\n\\text{FWER} \\le \\sum_{i=1}^{m} \\alpha_i\n$$\nTo control the overall FWER at a desired level $\\alpha$, we must ensure that $\\sum_{i=1}^{m} \\alpha_i \\le \\alpha$. The simplest and most general way to satisfy this condition is to set the significance level for each individual test to be equal, i.e., $\\alpha_i = \\alpha^*$ for all $i$. This yields:\n$$\nm \\alpha^* \\le \\alpha \\implies \\alpha^* \\le \\frac{\\alpha}{m}\n$$\nThe Bonferroni procedure sets this individual significance threshold to $\\alpha^* = \\frac{\\alpha}{m}$. Therefore, the decision rule is to reject the null hypothesis $H_{0,i}$ if its raw $p$-value $p_i \\le \\frac{\\alpha}{m}$.\n\nThe corresponding adjusted $p$-value, $p_i^{\\text{adj}}$, is defined as the smallest FWER level $\\alpha$ at which the hypothesis $H_{0,i}$ would be rejected. Based on the Bonferroni rule, we reject if $p_i \\le \\frac{\\alpha}{m}$, which is equivalent to $\\alpha \\ge m \\cdot p_i$. The smallest value of $\\alpha$ that satisfies this inequality is $\\alpha = m \\cdot p_i$. Since a probability cannot exceed $1$, the Bonferroni-adjusted $p$-value is formally defined as:\n$$\np_i^{\\text{adj}} = \\min(m \\cdot p_i, 1)\n$$\nA hypothesis is rejected if its adjusted $p$-value is less than or equal to the desired FWER level, $p_i^{\\text{adj}} \\le \\alpha$.\n\nNow, we apply this to the given problem. We have $m=10$ pairwise comparisons and a desired FWER of $\\alpha=0.05$. The Bonferroni-corrected significance level for each individual test is:\n$$\n\\alpha^* = \\frac{\\alpha}{m} = \\frac{0.05}{10} = 0.005\n$$\nThe set of raw $p$-values is $p=(0.001, 0.02, 0.07, 0.11, 0.23, 0.045, 0.0045, 0.39, 0.003, 0.12)$. We must compare each raw $p$-value to the corrected threshold $\\alpha^*=0.005$ and count the number of hypotheses that are rejected.\n1. $p_1 = 0.001 \\le 0.005 \\implies$ Reject $H_{0,1}$\n2. $p_2 = 0.02 > 0.005 \\implies$ Do not reject $H_{0,2}$\n3. $p_3 = 0.07 > 0.005 \\implies$ Do not reject $H_{0,3}$\n4. $p_4 = 0.11 > 0.005 \\implies$ Do not reject $H_{0,4}$\n5. $p_5 = 0.23 > 0.005 \\implies$ Do not reject $H_{0,5}$\n6. $p_6 = 0.045 > 0.005 \\implies$ Do not reject $H_{0,6}$\n7. $p_7 = 0.0045 \\le 0.005 \\implies$ Reject $H_{0,7}$\n8. $p_8 = 0.39 > 0.005 \\implies$ Do not reject $H_{0,8}$\n9. $p_9 = 0.003 \\le 0.005 \\implies$ Reject $H_{0,9}$\n10. $p_{10} = 0.12 > 0.005 \\implies$ Do not reject $H_{0,10}$\n\nThe hypotheses corresponding to the raw $p$-values of $0.001$, $0.0045$, and $0.003$ are rejected. The total number of rejected pairwise null hypotheses is $3$.\n\nFinally, we justify the applicability of the Bonferroni method. The Bonferroni correction's derivation relies solely on the union bound of probability. This makes it universally applicable to any collection of $m$ hypothesis tests, as it makes no assumptions about the dependence structure between the tests or the nature of the data that generated the $p$-values. In contrast, other methods are more specialized:\n- **Tukey's HSD** is specifically designed for testing all pairwise comparisons following an ANOVA. Its validity rests on the assumptions of the ANOVA model, such as normality, independence of observations, and homogeneity of variances, and it performs best with equal sample sizes per group. The problem provides only the final $p$-values, not confirmation that these underlying assumptions are met.\n- **Scheffé's Method** is designed to control the FWER for all possible linear contrasts among group means, not just pairwise comparisons. It is unnecessarily conservative if the only goal is to test all pairwise differences.\n- **Dunnett's Method** is designed for the specific situation of comparing multiple treatment groups to a single control group, not for all possible pairwise comparisons among all groups.\nGiven that the problem only provides a set of $p$-values and the general goal of controlling the FWER for $m$ simultaneous tests, the Bonferroni method is the most appropriate procedure to apply from first principles, as its validity does not depend on any information beyond what is provided.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "While the Bonferroni correction is versatile, more powerful methods exist for specific situations. For the common task of comparing all possible pairs of means after an ANOVA, the Tukey-Kramer method is often preferred. It is specifically designed for this \"all-pairwise\" scenario and is generally less conservative than Bonferroni, offering greater power to detect true differences. This practice demonstrates how to apply the Tukey-Kramer method, which uses the studentized range distribution and importantly, adapts to the common real-world complication of unequal sample sizes among groups .",
            "id": "4938779",
            "problem": "A biostatistics team has conducted an Analysis of Variance (ANOVA) on a clinical assay comparing $k=6$ treatment groups with unequal sample sizes $n=(8,12,9,15,10,11)$. The residual within-group mean square is $MS_{within}=7.1$, and the residual degrees of freedom are $df=60$. To control the family-wise error at $\\alpha=0.05$ for all pairwise comparisons, they plan to use the Tukey–Kramer method. The laboratory’s statistical software reports the studentized range critical value for these settings as $q^{\\star}=4.05$ for $k=6$ and $df=60$ at $\\alpha=0.05$.\n\nStarting from the standard ANOVA assumptions of normally distributed errors and common variance across groups, derive the Tukey–Kramer critical difference for a pair of group means and then compute the Tukey–Kramer critical differences for the three pairs formed by the three smallest sample sizes, specifically the pairs with $(n_i,n_j)=(8,9)$, $(8,10)$, and $(9,10)$. Round each critical difference to four significant figures. Provide your three numerical answers ordered as $(8,9)$, $(8,10)$, and $(9,10)$.",
            "solution": "The problem is validated as sound and well-posed. All necessary data are provided to compute the required statistical quantities. While a minor inconsistency exists between the stated residual degrees of freedom ($df=60$) and the value calculated from the given sample sizes ($df = \\sum n_i - k = (8+12+9+15+10+11) - 6 = 65 - 6 = 59$), the problem explicitly provides both $df=60$ and the corresponding studentized range critical value $q^{\\star}=4.05$. It is standard practice to proceed using the given parameters, as they form a self-contained and solvable problem set. The use of an adjacent integer value for degrees of freedom (e.g., $60$ instead of $59$) is common when using statistical tables or software interpolation, and does not invalidate the problem's structure.\n\nThe objective is to derive and compute the Tukey-Kramer critical difference for specific pairs of groups with unequal sample sizes. The Tukey-Kramer method is an adaptation of Tukey's Honestly Significant Difference (HSD) test designed to handle unequal sample sizes while controlling the family-wise error rate (FWER) for all pairwise comparisons following an ANOVA.\n\nThe core of the method is the studentized range statistic, $q$, which for a comparison between the means of group $i$ ($\\bar{y}_i$) and group $j$ ($\\bar{y}_j$) is defined as:\n$$\nq = \\frac{|\\bar{y}_i - \\bar{y}_j|}{SE_{ij}}\n$$\nwhere $SE_{ij}$ is the standard error of the difference between the two means. Under the standard ANOVA assumption of homoscedasticity (common variance $\\sigma^2$ across all groups), the best estimate for $\\sigma^2$ is the within-group mean square, $MS_{within}$.\n\nThe variance of the difference between two independent sample means is the sum of their individual variances:\n$$\n\\text{Var}(\\bar{y}_i - \\bar{y}_j) = \\text{Var}(\\bar{y}_i) + \\text{Var}(\\bar{y}_j) = \\frac{\\sigma^2}{n_i} + \\frac{\\sigma^2}{n_j} = \\sigma^2 \\left(\\frac{1}{n_i} + \\frac{1}{n_j}\\right)\n$$\nEstimating $\\sigma^2$ with $MS_{within}$, the standard error for the difference is:\n$$\nSE_{ij} = \\sqrt{MS_{within} \\left(\\frac{1}{n_i} + \\frac{1}{n_j}\\right)}\n$$\nThe Tukey-Kramer test statistic adjusts this standard error to fit the studentized range distribution. The pairwise statistic is often expressed as:\n$$\nq_{ij} = \\frac{|\\bar{y}_i - \\bar{y}_j|}{\\sqrt{\\frac{MS_{within}}{2} \\left(\\frac{1}{n_i} + \\frac{1}{n_j}\\right)}}\n$$\nA difference between means $|\\bar{y}_i - \\bar{y}_j|$ is declared statistically significant at the $\\alpha$ level if the calculated statistic $q_{ij}$ exceeds the critical value from the studentized range distribution, $q^{\\star}_{\\alpha, k, df}$, where $k$ is the number of groups and $df$ is the residual degrees of freedom.\n\nThe Tukey-Kramer critical difference, which we can denote as $CD_{ij}$, is the minimum absolute difference between two means that would be considered significant. It is derived by rearranging the inequality $|\\bar{y}_i - \\bar{y}_j| > CD_{ij}$:\n$$\nCD_{ij} = q^{\\star}_{\\alpha, k, df} \\sqrt{\\frac{MS_{within}}{2} \\left(\\frac{1}{n_i} + \\frac{1}{n_j}\\right)}\n$$\nThis is the general formula for the Tukey-Kramer critical difference.\n\nWe are given the following values:\nNumber of groups, $k=6$.\nWithin-group mean square, $MS_{within}=7.1$.\nResidual degrees of freedom, $df=60$.\nFamily-wise error rate, $\\alpha=0.05$.\nStudentized range critical value, $q^{\\star} = 4.05$.\n\nWe need to compute $CD_{ij}$ for the three pairs of sample sizes $(n_i, n_j)$: $(8, 9)$, $(8, 10)$, and $(9, 10)$.\n\nCase 1: Pair with sample sizes $(n_i, n_j) = (8, 9)$.\n$$\nCD_{8,9} = 4.05 \\sqrt{\\frac{7.1}{2} \\left(\\frac{1}{8} + \\frac{1}{9}\\right)}\n$$\n$$\nCD_{8,9} = 4.05 \\sqrt{3.55 \\left(\\frac{9+8}{72}\\right)} = 4.05 \\sqrt{3.55 \\left(\\frac{17}{72}\\right)}\n$$\n$$\nCD_{8,9} = 4.05 \\sqrt{0.8381944...} = 4.05 \\times 0.915530...\n$$\n$$\nCD_{8,9} = 3.70789...\n$$\nRounding to four significant figures, we get $3.708$.\n\nCase 2: Pair with sample sizes $(n_i, n_j) = (8, 10)$.\n$$\nCD_{8,10} = 4.05 \\sqrt{\\frac{7.1}{2} \\left(\\frac{1}{8} + \\frac{1}{10}\\right)}\n$$\n$$\nCD_{8,10} = 4.05 \\sqrt{3.55 \\left(\\frac{10+8}{80}\\right)} = 4.05 \\sqrt{3.55 \\left(\\frac{18}{80}\\right)}\n$$\n$$\nCD_{8,10} = 4.05 \\sqrt{0.79875} = 4.05 \\times 0.893728...\n$$\n$$\nCD_{8,10} = 3.6196...\n$$\nRounding to four significant figures, we get $3.620$.\n\nCase 3: Pair with sample sizes $(n_i, n_j) = (9, 10)$.\n$$\nCD_{9,10} = 4.05 \\sqrt{\\frac{7.1}{2} \\left(\\frac{1}{9} + \\frac{1}{10}\\right)}\n$$\n$$\nCD_{9,10} = 4.05 \\sqrt{3.55 \\left(\\frac{10+9}{90}\\right)} = 4.05 \\sqrt{3.55 \\left(\\frac{19}{90}\\right)}\n$$\n$$\nCD_{9,10} = 4.05 \\sqrt{0.749444...} = 4.05 \\times 0.865704...\n$$\n$$\nCD_{9,10} = 3.5061...\n$$\nRounding to four significant figures, we get $3.506$.\n\nThe three critical differences, ordered as requested for pairs $(8,9)$, $(8,10)$, and $(9,10)$, are $3.708$, $3.620$, and $3.506$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3.708 & 3.620 & 3.506\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The choice of a post-hoc test should be driven by the specific research hypotheses. This exercise explores Dunnett's test, a procedure optimized for experiments with a \"many-to-one\" comparison structure, such as comparing several new treatments against a standard control. By focusing only on the comparisons of interest, Dunnett's test provides more statistical power than methods designed for all-pairwise comparisons like Tukey's HSD or the highly general Bonferroni correction. This practice will guide you through the application of Dunnett's test, reinforcing the crucial concept of matching your statistical tool to your experimental design .",
            "id": "4938839",
            "problem": "A biostatistics team analyzes a one-way study with $g=4$ treatment groups and one control group. Each group has $n_{0}=15$ for control and $n_{i}=15$ for each treatment $i \\in \\{1,2,3,4\\}$. A standard one-way Analysis of Variance (ANOVA) is used under the usual Gaussian model with common variance across groups, and the residual mean square (within-group mean square) is reported as $MS_{\\text{within}}=5.2$. From the fitted model, the estimated mean differences for each treatment versus control (treatment mean minus control mean) are observed to be $(1.1,\\,2.0,\\,-0.3,\\,1.7)$. Using Dunnett’s multiple comparison procedure to control the family-wise error rate (FWER) at $\\alpha=0.05$ for two-sided comparisons, determine how many treatments are declared significantly different from the control.\n\nUse the following information:\n- The comparison standard error under the common-variance normal model is given by $\\sqrt{MS_{\\text{within}}\\left(\\frac{1}{n_{0}}+\\frac{1}{n_{i}}\\right)}$.\n- The within-group degrees of freedom are $\\nu=\\sum_{j=0}^{g}(n_{j}-1)$.\n- The two-sided Dunnett critical value for $g=4$ and $\\nu=70$ at $\\alpha=0.05$ is $2.45$.\n\nExpress your final answer as a single integer equal to the number of significant treatments. No rounding of the final answer is required.",
            "solution": "The problem requires us to determine the number of treatments that are declared significantly different from a control group using Dunnett's multiple comparison procedure. The study involves $g=4$ treatment groups and one control group, making a total of $k=g+1=5$ groups. The sample size is equal for all groups, with $n_j=15$ for $j \\in \\{0, 1, 2, 3, 4\\}$. The family-wise error rate for the two-sided comparisons is to be controlled at $\\alpha=0.05$.\n\nFirst, we verify the within-group degrees of freedom, $\\nu$, using the provided formula $\\nu=\\sum_{j=0}^{g}(n_{j}-1)$. Given $g=4$ and $n_j=15$ for all $j=0, \\dots, 4$, the calculation is:\n$$\n\\nu = \\sum_{j=0}^{4} (15-1) = 5 \\times (15-1) = 5 \\times 14 = 70\n$$\nThis result matches the value of $\\nu=70$ for which the problem supplies a critical value, confirming the internal consistency of the problem's data.\n\nDunnett's procedure is designed to compare each of the $g=4$ treatment means, denoted $\\bar{y}_i$ for $i \\in \\{1, 2, 3, 4\\}$, with the control group mean, $\\bar{y}_0$. A mean difference is declared statistically significant if the absolute value of the corresponding test statistic, $t_i$, exceeds the two-sided Dunnett critical value, which we denote as $d_{\\alpha, g, \\nu}$.\n\nThe test statistic for the $i$-th comparison is defined as:\n$$\nt_i = \\frac{|\\bar{y}_i - \\bar{y}_0|}{SE}\n$$\nwhere $SE$ represents the standard error of the difference between a treatment mean and the control mean. The problem provides the formula for this standard error:\n$$\nSE = \\sqrt{MS_{\\text{within}}\\left(\\frac{1}{n_{0}}+\\frac{1}{n_{i}}\\right)}\n$$\nWe are given $MS_{\\text{within}} = 5.2$, and the sample sizes are $n_0 = 15$ and $n_i = 15$. We can therefore compute the standard error:\n$$\nSE = \\sqrt{5.2 \\left(\\frac{1}{15}+\\frac{1}{15}\\right)} = \\sqrt{5.2 \\times \\frac{2}{15}} = \\sqrt{\\frac{10.4}{15}}\n$$\n\nThe condition for declaring the $i$-th comparison statistically significant is $t_i > d_{\\alpha, g, \\nu}$. With the provided critical value $d_{0.05, 4, 70} = 2.45$, the inequality becomes:\n$$\n\\frac{|\\bar{y}_i - \\bar{y}_0|}{SE} > 2.45\n$$\nAn equivalent approach is to calculate a \"critical difference,\" $D$, and compare the observed absolute mean differences directly against it. The difference is significant if $|\\bar{y}_i - \\bar{y}_0| > D$, where:\n$$\nD = d_{\\alpha, g, \\nu} \\times SE = 2.45 \\times \\sqrt{\\frac{10.4}{15}}\n$$\nTo maintain precision and avoid rounding errors from the square root, we can work with the squares of these quantities. The significance condition is thus $|\\bar{y}_i - \\bar{y}_0|^2 > D^2$. Let us calculate $D^2$:\n$$\nD^2 = (2.45)^2 \\times \\left(\\frac{10.4}{15}\\right) = 6.0025 \\times \\frac{10.4}{15} = \\frac{62.426}{15}\n$$\n$D^2 \\approx 4.16173$.\n\nThe observed mean differences, $(\\bar{y}_i - \\bar{y}_0)$, are given as the set $(1.1,\\,2.0,\\,-0.3,\\,1.7)$. We compute the square of the absolute value for each of these differences:\n- For treatment $1$: $|\\bar{y}_1 - \\bar{y}_0|^2 = |1.1|^2 = 1.21$\n- For treatment $2$: $|\\bar{y}_2 - \\bar{y}_0|^2 = |2.0|^2 = 4.00$\n- For treatment $3$: $|\\bar{y}_3 - \\bar{y}_0|^2 = |-0.3|^2 = 0.09$\n- For treatment $4$: $|\\bar{y}_4 - \\bar{y}_0|^2 = |1.7|^2 = 2.89$\n\nFinally, we compare each of these squared differences to the squared critical difference, $D^2 \\approx 4.16173$:\n- Treatment $1$: $1.21 < 4.16173$. The difference is not significant.\n- Treatment $2$: $4.00 < 4.16173$. The difference is not significant.\n- Treatment $3$: $0.09 < 4.16173$. The difference is not significant.\n- Treatment $4$: $2.89 < 4.16173$. The difference is not significant.\n\nSince none of the squared absolute mean differences exceed the squared critical difference, we fail to reject the null hypothesis for all four comparisons. No treatment mean is found to be significantly different from the control mean at the $\\alpha=0.05$ significance level.\n\nTherefore, the number of treatments declared significantly different from the control is $0$.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}