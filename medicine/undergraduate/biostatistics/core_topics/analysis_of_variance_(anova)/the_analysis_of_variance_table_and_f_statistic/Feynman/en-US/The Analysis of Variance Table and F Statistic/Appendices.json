{
    "hands_on_practices": [
        {
            "introduction": "The foundation of Analysis of Variance (ANOVA) lies in partitioning the total variation in a dataset into distinct sources. This first practice guides you through the fundamental mechanics of constructing an ANOVA table from summary statistics, a core skill for any biostatistician. By calculating the sums of squares and mean squares from first principles, you will gain a concrete understanding of how the F-statistic quantifies the ratio of between-group variation to within-group variation. ",
            "id": "4957544",
            "problem": "A biostatistician is studying enzyme activity measured in arbitrary activity units across four independent genotypes using a one-way Analysis of Variance (ANOVA). For each genotype, the sample size, sample mean, and unbiased sample variance are reported as follows: genotype $1$: $n_{1}=15$, $\\bar{x}_{1}=12.0$, $s_{1}^{2}=1.2$; genotype $2$: $n_{2}=15$, $\\bar{x}_{2}=11.5$, $s_{2}^{2}=0.9$; genotype $3$: $n_{3}=15$, $\\bar{x}_{3}=10.0$, $s_{3}^{2}=1.5$; genotype $4$: $n_{4}=15$, $\\bar{x}_{4}=13.0$, $s_{4}^{2}=2.1$. Assume the standard one-way layout with independent, normally distributed errors having a common variance across genotypes. Starting from the core definitions of variability attributable to group differences and to within-group scatter, construct the ANOVA table by partitioning the total variability into “between groups” and “within groups” components and matching each component with its appropriate degrees of freedom. Then, compute the ANOVA $F$ statistic for testing equality of the genotype means.\n\nRound your final numeric answer to four significant figures.",
            "solution": "The core principle of Analysis of Variance (ANOVA) is to partition the total variability in a dataset into components attributable to different sources of variation. In a one-way ANOVA, the total sum of squares ($SST$) is partitioned into the sum of squares between groups ($SSB$) and the sum of squares within groups ($SSW$).\n\n$$SST = SSB + SSW$$\n\nThe $SSB$ component measures the variability of the group means around the overall grand mean, representing the effect of the treatment or factor (in this case, genotype). The $SSW$ component measures the pooled variability of the individual observations within each group, representing the random error or unexplained scatter.\n\nFirst, we determine the constants for the analysis.\n- The number of groups is $k=4$.\n- The sample sizes are equal for all groups: $n_1=n_2=n_3=n_4=15$.\n- The total number of observations is $N = \\sum_{i=1}^{k} n_i = 15+15+15+15 = 60$.\n\nNext, we calculate the grand mean, $\\bar{x}_{grand}$, which is the mean of all observations. For a balanced design where all $n_i$ are equal, this simplifies to the average of the group means.\n$$ \\bar{x}_{grand} = \\frac{1}{N} \\sum_{i=1}^{k} n_i \\bar{x}_i = \\frac{1}{4} \\sum_{i=1}^{4} \\bar{x}_i $$\n$$ \\bar{x}_{grand} = \\frac{12.0 + 11.5 + 10.0 + 13.0}{4} = \\frac{46.5}{4} = 11.625 $$\n\nNow, we compute the sums of squares.\n\n1.  **Sum of Squares Within Groups ($SSW$)**: This represents the pooled within-group scatter. It is the sum of the sums of squared deviations of observations from their respective group means. The unbiased sample variance for group $i$ is given by $s_i^2 = \\frac{1}{n_i-1} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x}_i)^2$. The sum of squares for group $i$ is therefore $(n_i-1)s_i^2$. The total $SSW$ is the sum of these values across all groups.\n    $$ SSW = \\sum_{i=1}^{k} (n_i-1) s_i^2 $$\n    $$ SSW = (15-1)(1.2) + (15-1)(0.9) + (15-1)(1.5) + (15-1)(2.1) $$\n    $$ SSW = 14(1.2 + 0.9 + 1.5 + 2.1) = 14(5.7) = 79.8 $$\n\n2.  **Sum of Squares Between Groups ($SSB$)**: This represents the variability attributable to differences between the group means. It is the sum of the squared differences between each group mean and the grand mean, weighted by the sample size of each group.\n    $$ SSB = \\sum_{i=1}^{k} n_i (\\bar{x}_i - \\bar{x}_{grand})^2 $$\n    $$ SSB = 15(12.0 - 11.625)^2 + 15(11.5 - 11.625)^2 + 15(10.0 - 11.625)^2 + 15(13.0 - 11.625)^2 $$\n    $$ SSB = 15 [ (0.375)^2 + (-0.125)^2 + (-1.625)^2 + (1.375)^2 ] $$\n    $$ SSB = 15 [ 0.140625 + 0.015625 + 2.640625 + 1.890625 ] $$\n    $$ SSB = 15 [ 4.6875 ] = 70.3125 $$\n\nNext, we determine the degrees of freedom ($df$) associated with each sum of squares.\n-   Degrees of Freedom Between Groups: $df_B = k - 1 = 4 - 1 = 3$.\n-   Degrees of Freedom Within Groups: $df_W = N - k = 60 - 4 = 56$.\n-   Total Degrees of Freedom: $df_T = N - 1 = 60 - 1 = 59$. Note that $df_T = df_B + df_W$.\n\nNow we compute the Mean Squares ($MS$), which are the sums of squares divided by their respective degrees of freedom.\n-   Mean Square Between Groups: $MSB = \\frac{SSB}{df_B} = \\frac{70.3125}{3} = 23.4375$.\n-   Mean Square Within Groups: $MSW = \\frac{SSW}{df_W} = \\frac{79.8}{56} = 1.425$. The $MSW$ is the pooled estimate of the common variance $\\sigma^2$.\n\nFinally, we compute the ANOVA $F$ statistic, which is the ratio of the between-group variance estimate to the within-group variance estimate.\n$$ F = \\frac{MSB}{MSW} = \\frac{23.4375}{1.425} \\approx 16.44736842... $$\n\nThe complete ANOVA table is constructed as follows:\n| Source of Variation | Sum of Squares ($SS$) | Degrees of Freedom ($df$) | Mean Square ($MS$) | $F$ Statistic |\n|---------------------|-----------------------|---------------------------|--------------------|---------------|\n| Between Groups      | $70.3125$             | $3$                       | $23.4375$          | $16.45$       |\n| Within Groups       | $79.8$                | $56$                      | $1.425$            |               |\n| Total               | $150.1125$            | $59$                      |                    |               |\n\nThe problem requires the final answer to be rounded to four significant figures.\n$$ F \\approx 16.447368... \\rightarrow 16.45 $$",
            "answer": "$$\n\\boxed{16.45}\n$$"
        },
        {
            "introduction": "While the overall F-test tells us whether any group means differ, it doesn't specify the nature of that difference. This exercise introduces the powerful technique of linear contrasts, which allow us to test specific, planned hypotheses, such as whether a treatment effect increases linearly with dose. You will learn to construct an F-statistic for a specific contrast, providing a much more focused and interpretable conclusion than the omnibus ANOVA test alone. ",
            "id": "4957537",
            "problem": "A randomized clinical experiment compares the mean serum cytokine level across $4$ treatment arms: control $(i=1)$ and three increasing dose levels $(i=2,3,4)$. Let the observed cytokine levels in group $i$ be $Y_{ij}$ for $j=1,\\dots,n_i$. Adopt the one-way cell means parameterization of the linear model with homoscedastic Gaussian errors: $Y_{ij} = \\mu_i + \\varepsilon_{ij}$ with independent $\\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$ and $i=1,\\dots,4$. The study reports the following summary statistics (sample size $n_i$, sample mean $\\bar{Y}_i$, and sample standard deviation $s_i$):\n- Group $1$ (control): $n_1 = 15$, $\\bar{Y}_1 = 22.4$, $s_1 = 4.8$.\n- Group $2$ (low dose): $n_2 = 12$, $\\bar{Y}_2 = 18.9$, $s_2 = 5.1$.\n- Group $3$ (medium dose): $n_3 = 17$, $\\bar{Y}_3 = 17.3$, $s_3 = 4.5$.\n- Group $4$ (high dose): $n_4 = 14$, $\\bar{Y}_4 = 16.8$, $s_4 = 4.2$.\n\nConsider the contrast of cell means that encodes a linear dose trend,\n$$L = -3\\mu_1 - 1\\mu_2 + 1\\mu_3 + 3\\mu_4,$$\nwhich satisfies $\\sum_{i=1}^{4} c_i = 0$ with coefficients $c_1=-3$, $c_2=-1$, $c_3=1$, $c_4=3$. Starting from the model assumptions and the analysis of variance decomposition for one-way experiments, derive from first principles an appropriate $F$ statistic to test the null hypothesis $H_0: L = 0$ versus $H_1: L \\neq 0$, and then compute its numerical value using the given summary statistics. Express the final $F$ statistic and round your answer to four significant figures.",
            "solution": "The problem describes a one-way analysis of variance (ANOVA) model, $Y_{ij} = \\mu_i + \\varepsilon_{ij}$ with $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$, and asks for the construction and computation of an $F$ statistic for a specific linear contrast $L = \\sum_{i=1}^{4} c_i \\mu_i$.\n\nThe derivation of the $F$ statistic proceeds from the foundational principle that an $F$-distributed random variable is the ratio of two independent chi-squared random variables, each divided by its respective degrees of freedom. We must therefore construct a numerator term related to the contrast $L$ and a denominator term related to the error variance $\\sigma^2$ that satisfy these conditions.\n\n**1. Numerator: Sum of Squares for the Contrast ($SS_L$)**\n\nThe contrast of interest is $L = \\sum_{i=1}^{4} c_i \\mu_i$. The natural estimator for $L$ is $\\hat{L} = \\sum_{i=1}^{4} c_i \\bar{Y}_i$, where $\\bar{Y}_i$ is the sample mean for group $i$.\n\nUnder the model assumptions, the sample means are independent and normally distributed: $\\bar{Y}_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2/n_i)$.\nSince $\\hat{L}$ is a linear combination of independent normal variables, it is also normally distributed. Its expectation and variance are:\n$E[\\hat{L}] = E[\\sum_{i=1}^{4} c_i \\bar{Y}_i] = \\sum_{i=1}^{4} c_i E[\\bar{Y}_i] = \\sum_{i=1}^{4} c_i \\mu_i = L$.\n$\\operatorname{Var}(\\hat{L}) = \\operatorname{Var}(\\sum_{i=1}^{4} c_i \\bar{Y}_i) = \\sum_{i=1}^{4} c_i^2 \\operatorname{Var}(\\bar{Y}_i) = \\sum_{i=1}^{4} c_i^2 \\frac{\\sigma^2}{n_i} = \\sigma^2 \\sum_{i=1}^{4} \\frac{c_i^2}{n_i}$.\n\nSo, $\\hat{L} \\sim \\mathcal{N} \\left( L, \\sigma^2 \\sum_{i=1}^{4} \\frac{c_i^2}{n_i} \\right)$.\n\nUnder the null hypothesis, $H_0: L=0$, the distribution simplifies to $\\hat{L} \\sim \\mathcal{N} \\left( 0, \\sigma^2 \\sum_{i=1}^{4} \\frac{c_i^2}{n_i} \\right)$.\nStandardizing this variable yields a standard normal distribution:\n$$Z = \\frac{\\hat{L} - 0}{\\sqrt{\\operatorname{Var}(\\hat{L})}} = \\frac{\\hat{L}}{\\sigma \\sqrt{\\sum_{i=1}^{4} \\frac{c_i^2}{n_i}}} \\sim \\mathcal{N}(0, 1)$$\nSquaring a standard normal variable produces a chi-squared distribution with $1$ degree of freedom:\n$$Z^2 = \\frac{\\hat{L}^2}{\\sigma^2 \\sum_{i=1}^{4} \\frac{c_i^2}{n_i}} \\sim \\chi^2_1$$\nWe define the Sum of Squares for the Contrast, $SS_L$, as the part of this expression that does not depend on the unknown $\\sigma^2$:\n$$SS_L = \\frac{\\hat{L}^2}{\\sum_{i=1}^{4} \\frac{c_i^2}{n_i}}$$\nFrom this definition, it follows that $\\frac{SS_L}{\\sigma^2} \\sim \\chi^2_1$. The degrees of freedom for the numerator is $df_L = 1$. The Mean Square for the contrast is $MS_L = \\frac{SS_L}{df_L} = SS_L$.\n\n**2. Denominator: Mean Squared Error ($MS_E$)**\n\nWe need an independent estimator for the error variance $\\sigma^2$. This is provided by the Mean Squared Error, $MS_E$, which is the pooled variance from all $k=4$ groups.\nThe Sum of Squared Errors ($SS_E$) is the sum of the squared deviations from each observation to its group mean:\n$$SS_E = \\sum_{i=1}^{4} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2$$\nUsing the definition of the sample variance for each group, $s_i^2 = \\frac{1}{n_i-1} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2$, we can write $SS_E$ in terms of the given summary statistics:\n$$SS_E = \\sum_{i=1}^{4} (n_i-1) s_i^2$$\nThe degrees of freedom for error is $df_E = \\sum_{i=1}^{4} (n_i-1) = N-k$, where $N=\\sum n_i$ is the total sample size.\nBy Cochran's theorem, the quantity $\\frac{SS_E}{\\sigma^2}$ follows a chi-squared distribution with $df_E$ degrees of freedom: $\\frac{SS_E}{\\sigma^2} \\sim \\chi^2_{N-k}$.\nThe Mean Squared Error is the unbiased estimator of $\\sigma^2$: $MS_E = \\frac{SS_E}{df_E} = \\frac{\\sum (n_i-1)s_i^2}{N-k}$.\nCrucially, Cochran's theorem also establishes that $SS_L$ and $SS_E$ are independent random variables.\n\n**3. The F-Statistic**\n\nThe $F$ statistic is formed as the ratio of the two independent mean squares:\n$$F = \\frac{MS_L}{MS_E} = \\frac{SS_L/df_L}{SS_E/df_E} = \\frac{SS_L}{MS_E}$$\nSubstituting the derived expressions:\n$$F = \\frac{ \\frac{\\hat{L}^2}{\\sum_{i=1}^{4} (c_i^2/n_i)} }{ \\frac{\\sum_{i=1}^{4} (n_i-1)s_i^2}{N-k} }$$\nUnder $H_0$, this statistic follows an $F$-distribution with $df_1 = 1$ and $df_2 = N-k$ degrees of freedom.\n\n**4. Numerical Calculation**\n\nFirst, calculate the total sample size $N$ and the error degrees of freedom $df_E$:\n$N = n_1 + n_2 + n_3 + n_4 = 15 + 12 + 17 + 14 = 58$.\n$df_E = N-k = 58 - 4 = 54$.\n\nNext, compute the estimated contrast $\\hat{L}$:\n$\\hat{L} = c_1\\bar{Y}_1 + c_2\\bar{Y}_2 + c_3\\bar{Y}_3 + c_4\\bar{Y}_4$\n$\\hat{L} = (-3)(22.4) + (-1)(18.9) + (1)(17.3) + (3)(16.8) = -67.2 - 18.9 + 17.3 + 50.4 = -18.4$.\n\nNow, calculate the terms for the numerator sum of squares, $SS_L$:\n$\\sum_{i=1}^{4} \\frac{c_i^2}{n_i} = \\frac{(-3)^2}{15} + \\frac{(-1)^2}{12} + \\frac{1^2}{17} + \\frac{3^2}{14} = \\frac{9}{15} + \\frac{1}{12} + \\frac{1}{17} + \\frac{9}{14}$\n$\\sum_{i=1}^{4} \\frac{c_i^2}{n_i} \\approx 0.6 + 0.083333 + 0.058824 + 0.642857 \\approx 1.385014$.\n$SS_L = MS_L = \\frac{\\hat{L}^2}{\\sum (c_i^2/n_i)} = \\frac{(-18.4)^2}{1.385014} = \\frac{338.56}{1.385014} \\approx 244.4447$.\n\nNext, compute the terms for the denominator mean square, $MS_E$:\n$SS_E = (15-1)(4.8)^2 + (12-1)(5.1)^2 + (17-1)(4.5)^2 + (14-1)(4.2)^2$\n$SS_E = 14(23.04) + 11(26.01) + 16(20.25) + 13(17.64)$\n$SS_E = 322.56 + 286.11 + 324.00 + 229.32 = 1161.99$.\n$MS_E = \\frac{SS_E}{df_E} = \\frac{1161.99}{54} \\approx 21.518333$.\n\nFinally, compute the $F$ statistic:\n$F = \\frac{MS_L}{MS_E} = \\frac{244.4447}{21.518333} \\approx 11.36000$.\n\nRounding to four significant figures, the value of the $F$ statistic is $11.36$.",
            "answer": "$$\n\\boxed{11.36}\n$$"
        },
        {
            "introduction": "Biological experiments often involve more than one factor, and their effects can be intertwined. This advanced practice moves from a one-way to a two-way ANOVA, introducing the critical concept of interaction—where the effect of one factor depends on the level of another. You will apply a hierarchical testing strategy, first assessing the interaction and then, if appropriate, pooling variance to increase the power for testing the main effects, a common and practical approach in experimental data analysis. ",
            "id": "4957533",
            "problem": "A randomized biostatistical experiment investigates how a pro-inflammatory biomarker response, denoted by $Y$, varies under combinations of two fixed factors: a three-level diet factor $A$ (levels $A_1$, $A_2$, $A_3$) and a two-level exercise factor $B$ (levels $B_1$, $B_2$). The design is balanced with $n=5$ independent subjects per cell. The observed cell sample means $\\bar{Y}_{ij\\cdot}$ (averaged over the $n$ subjects in cell $(i,j)$) and unbiased within-cell sample variances $s_{ij}^2$ are as follows:\n- $A_1$: $\\bar{Y}_{11\\cdot}=10.1$ with $s_{11}^2=1.44$; $\\bar{Y}_{12\\cdot}=12.0$ with $s_{12}^2=0.81$.\n- $A_2$: $\\bar{Y}_{21\\cdot}=11.0$ with $s_{21}^2=1.21$; $\\bar{Y}_{22\\cdot}=13.2$ with $s_{22}^2=1.00$.\n- $A_3$: $\\bar{Y}_{31\\cdot}=12.2$ with $s_{31}^2=1.00$; $\\bar{Y}_{32\\cdot}=14.1$ with $s_{32}^2=1.21$.\n\nAssume the standard two-way fixed-effects model with interaction and homoscedastic individual errors: for subjects $k=1,\\dots,n$ in cell $(i,j)$,\n$$\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk},\n$$\nwhere $E[\\varepsilon_{ijk}]=0$, $\\operatorname{Var}(\\varepsilon_{ijk})=\\sigma^2$, and $\\varepsilon_{ijk}$ are independent.\n\nUsing only the definitions of sums of squares derived from orthogonal decomposition of variation in a balanced two-way layout and the fact that mean squares are sums of squares divided by their corresponding degrees of freedom, proceed as follows:\n- Construct the Analysis of Variance (ANOVA) table components for factor $A$, factor $B$, the $A\\times B$ interaction, and the error: explicitly determine the degrees of freedom and the sums of squares for each source by working from the cell means, marginal means, and the within-cell variability.\n- Apply hierarchical testing: first test for the presence of an $A\\times B$ interaction using the appropriate $F$ statistic. If the interaction is not statistically significant at significance level $\\alpha=0.05$, pool the interaction sum of squares with the error sum of squares to form a new error term, and then test the main effect of factor $A$ using this pooled error term.\n\nWhat is the numerical value of the $F$ statistic used to test the main effect of factor $A$ under this hierarchical testing rule? Round your answer to four significant figures. State only the $F$ statistic; do not report a $p$-value or a decision.",
            "solution": "The problem describes a balanced two-way fixed-effects ANOVA. The factors are $A$ (diet) with $I=3$ levels and $B$ (exercise) with $J=2$ levels. There are $n=5$ replicates per cell. The total number of observations is $N = I \\times J \\times n = 3 \\times 2 \\times 5 = 30$.\n\nFirst, we calculate the marginal means and the grand mean from the given cell sample means.\nThe levels of factor $A$ are indexed by $i \\in \\{1, 2, 3\\}$ and levels of factor $B$ are indexed by $j \\in \\{1, 2\\}$.\nThe cell means $\\bar{Y}_{ij\\cdot}$ are:\n$\\bar{Y}_{11\\cdot}=10.1$, $\\bar{Y}_{12\\cdot}=12.0$\n$\\bar{Y}_{21\\cdot}=11.0$, $\\bar{Y}_{22\\cdot}=13.2$\n$\\bar{Y}_{31\\cdot}=12.2$, $\\bar{Y}_{32\\cdot}=14.1$\n\nThe marginal means for factor $A$, $\\bar{Y}_{i\\cdot\\cdot} = \\frac{1}{J}\\sum_{j=1}^J \\bar{Y}_{ij\\cdot}$:\n$\\bar{Y}_{1\\cdot\\cdot} = \\frac{10.1 + 12.0}{2} = 11.05$\n$\\bar{Y}_{2\\cdot\\cdot} = \\frac{11.0 + 13.2}{2} = 12.10$\n$\\bar{Y}_{3\\cdot\\cdot} = \\frac{12.2 + 14.1}{2} = 13.15$\n\nThe marginal means for factor $B$, $\\bar{Y}_{\\cdot j\\cdot} = \\frac{1}{I}\\sum_{i=1}^I \\bar{Y}_{ij\\cdot}$:\n$\\bar{Y}_{\\cdot 1\\cdot} = \\frac{10.1 + 11.0 + 12.2}{3} = \\frac{33.3}{3} = 11.1$\n$\\bar{Y}_{\\cdot 2\\cdot} = \\frac{12.0 + 13.2 + 14.1}{3} = \\frac{39.3}{3} = 13.1$\n\nThe grand mean, $\\bar{Y}_{\\cdot\\cdot\\cdot} = \\frac{1}{I}\\sum_{i=1}^I \\bar{Y}_{i\\cdot\\cdot}$:\n$\\bar{Y}_{\\cdot\\cdot\\cdot} = \\frac{11.05 + 12.10 + 13.15}{3} = \\frac{36.3}{3} = 12.1$\n\nNext, we construct the ANOVA table components.\nThe degrees of freedom (DF) are:\n$DF_A = I - 1 = 3 - 1 = 2$\n$DF_B = J - 1 = 2 - 1 = 1$\n$DF_{AB} = (I-1)(J-1) = (2)(1) = 2$\n$DF_E = IJ(n-1) = (3)(2)(5-1) = 24$\n\nThe Sums of Squares (SS) are calculated as follows:\nSum of Squares for Error ($SSE$):\n$SSE = \\sum_{i=1}^I \\sum_{j=1}^J (n-1)s_{ij}^2 = 4 \\times (1.44 + 0.81 + 1.21 + 1.00 + 1.00 + 1.21)$\n$SSE = 4 \\times 6.67 = 26.68$\nThe Mean Square Error is $MSE = \\frac{SSE}{DF_E} = \\frac{26.68}{24} \\approx 1.1117$.\n\nSum of Squares for factor A ($SSA$):\n$SSA = nJ \\sum_{i=1}^I (\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2$\n$SSA = (5)(2) \\left[ (11.05 - 12.1)^2 + (12.10 - 12.1)^2 + (13.15 - 12.1)^2 \\right]$\n$SSA = 10 \\left[ (-1.05)^2 + (0)^2 + (1.05)^2 \\right]$\n$SSA = 10 [1.1025 + 0 + 1.1025] = 10 \\times 2.205 = 22.05$\nThe Mean Square for A is $MSA = \\frac{SSA}{DF_A} = \\frac{22.05}{2} = 11.025$.\n\nSum of Squares for the interaction ($SS_{AB}$):\n$SS_{AB} = n \\sum_{i=1}^I \\sum_{j=1}^J (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot j\\cdot} + \\bar{Y}_{\\cdot\\cdot\\cdot})^2$\nThe interaction terms are:\n- $(i=1, j=1): 10.1 - 11.05 - 11.1 + 12.1 = 0.05$\n- $(i=1, j=2): 12.0 - 11.05 - 13.1 + 12.1 = -0.05$\n- $(i=2, j=1): 11.0 - 12.10 - 11.1 + 12.1 = -0.10$\n- $(i=2, j=2): 13.2 - 12.10 - 13.1 + 12.1 = 0.10$\n- $(i=3, j=1): 12.2 - 13.15 - 11.1 + 12.1 = 0.05$\n- $(i=3, j=2): 14.1 - 13.15 - 13.1 + 12.1 = -0.05$\n$SS_{AB} = 5 \\left[ (0.05)^2 + (-0.05)^2 + (-0.10)^2 + (0.10)^2 + (0.05)^2 + (-0.05)^2 \\right]$\n$SS_{AB} = 5 [0.0025 + 0.0025 + 0.0100 + 0.0100 + 0.0025 + 0.0025] = 5 [0.0300] = 0.15$\nThe Mean Square for the interaction is $MS_{AB} = \\frac{SS_{AB}}{DF_{AB}} = \\frac{0.15}{2} = 0.075$.\n\nNow, we proceed with the hierarchical testing. First, test for the $A \\times B$ interaction.\nThe $F$ statistic for the interaction is $F_{AB} = \\frac{MS_{AB}}{MSE} = \\frac{0.075}{26.68 / 24} \\approx 0.06746$.\nThe critical value for this test at $\\alpha=0.05$ is $F_{crit} = F_{0.05, 2, 24} \\approx 3.4028$.\nSince $F_{AB} < F_{crit}$, the interaction is not statistically significant.\n\nAccording to the instructions, we pool the interaction sum of squares with the error sum of squares.\nThe pooled sum of squares for error is $SS'_{Error} = SS_{AB} + SSE = 0.15 + 26.68 = 26.83$.\nThe pooled degrees of freedom for error is $DF'_{Error} = DF_{AB} + DF_E = 2 + 24 = 26$.\nThe new pooled mean square error is $MS'_{Error} = \\frac{SS'_{Error}}{DF'_{Error}} = \\frac{26.83}{26} \\approx 1.031923$.\n\nFinally, we test the main effect of factor $A$ using this pooled error term.\nThe $F$ statistic for factor $A$ is $F_A = \\frac{MSA}{MS'_{Error}} = \\frac{11.025}{26.83 / 26} \\approx 10.6839$.\n\nRounding to four significant figures, the result is $10.68$.",
            "answer": "$$\n\\boxed{10.68}\n$$"
        }
    ]
}