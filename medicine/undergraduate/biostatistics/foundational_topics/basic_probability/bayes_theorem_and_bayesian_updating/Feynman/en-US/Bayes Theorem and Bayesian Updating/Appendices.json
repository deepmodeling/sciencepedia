{
    "hands_on_practices": [
        {
            "introduction": "The fundamental task in Bayesian inference is updating our beliefs in light of new evidence. This first exercise provides a foundational walkthrough of this process for a continuous parameter, using the common scenario of estimating a population mean from normally distributed data. By deriving the posterior distribution from first principles, you will gain a concrete understanding of how a prior belief, represented by a normal distribution, is mathematically combined with data to yield an updated posterior belief, which is also a normal distribution . This practice is essential for mastering the core mechanics of Bayesian updating.",
            "id": "4896445",
            "problem": "A clinical study evaluates the change in systolic blood pressure (in millimeters of mercury (mm Hg)) after a four-week intervention. Let the changes for independent participants be modeled as $y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$, where $\\mu$ is the unknown population mean change and $\\sigma^2$ is known from prior calibration. Suppose a prior belief about $\\mu$ is $\\mu \\sim \\mathcal{N}(m_0,s_0^2)$ based on earlier trials.\n\nIn this study, the known calibration variance is $\\sigma^2 = 16$ and the prior parameters are $m_0 = -2$ and $s_0^2 = 9$. The observed changes are the $n=8$ values\n$$\n\\{4,\\,-1,\\,3,\\,2,\\,0,\\,1,\\,1,\\,2\\}.\n$$\n\nStarting from Bayes' theorem and the standard form of the normal probability density function, use first principles (i.e., write the likelihood $p(y\\mid \\mu)$ and the prior $p(\\mu)$, and then algebraically complete the square in $\\mu$) to derive the posterior density $p(\\mu\\mid y)$, identify its family, and express its parameters as functions of $m_0$, $s_0^2$, $\\sigma^2$, and the observed data. Then, using the provided numerical values, compute the posterior mean of $\\mu$ for this dataset.\n\nExpress the final numerical answer (the posterior mean) in millimeters of mercury and round your answer to four significant figures.",
            "solution": "The problem is valid. It presents a standard, well-posed problem in Bayesian biostatistics involving a conjugate normal-normal model, with all necessary parameters and data provided for a unique solution.\n\nThe problem requires the derivation of the posterior distribution for the population mean $\\mu$ from first principles, followed by a numerical calculation of the posterior mean.\n\nLet the observed data be $y = \\{y_1, y_2, \\dots, y_n\\}$. The model specifies that the data points $y_i$ are independent and identically distributed according to a normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$.\n$$ y_i \\mid \\mu \\sim \\mathcal{N}(\\mu, \\sigma^2) $$\nThe prior belief about $\\mu$ is also modeled by a normal distribution with mean $m_0$ and variance $s_0^2$.\n$$ \\mu \\sim \\mathcal{N}(m_0, s_0^2) $$\n\nAccording to Bayes' theorem, the posterior probability density function (PDF) $p(\\mu \\mid y)$ is proportional to the product of the likelihood function $p(y \\mid \\mu)$ and the prior PDF $p(\\mu)$.\n$$ p(\\mu \\mid y) \\propto p(y \\mid \\mu) p(\\mu) $$\n\nFirst, we write the PDF for the prior distribution of $\\mu$:\n$$ p(\\mu) = \\frac{1}{\\sqrt{2\\pi s_0^2}} \\exp\\left(-\\frac{(\\mu - m_0)^2}{2s_0^2}\\right) $$\nSince the observations $y_i$ are independent, the likelihood function is the product of the individual densities:\n$$ p(y \\mid \\mu) = \\prod_{i=1}^{n} p(y_i \\mid \\mu) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - \\mu)^2\\right) $$\nNow, we find the posterior PDF by multiplying the prior and the likelihood. We can drop any constants that do not depend on $\\mu$.\n$$ p(\\mu \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - \\mu)^2\\right) \\exp\\left(-\\frac{(\\mu - m_0)^2}{2s_0^2}\\right) $$\nCombining the exponents gives:\n$$ p(\\mu \\mid y) \\propto \\exp\\left[-\\frac{1}{2}\\left(\\frac{\\sum_{i=1}^{n}(y_i - \\mu)^2}{\\sigma^2} + \\frac{(\\mu - m_0)^2}{s_0^2}\\right)\\right] $$\nTo identify the posterior distribution, we expand the terms in the exponent and treat it as a quadratic function of $\\mu$. Let $E$ be the term inside the parentheses:\n$$ E = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(y_i^2 - 2y_i\\mu + \\mu^2) + \\frac{1}{s_0^2}(\\mu^2 - 2m_0\\mu + m_0^2) $$\nWe collect terms involving $\\mu^2$ and $\\mu$:\n$$ E = \\mu^2\\left(\\frac{n}{\\sigma^2} + \\frac{1}{s_0^2}\\right) - 2\\mu\\left(\\frac{\\sum_{i=1}^{n}y_i}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) + \\left(\\frac{\\sum_{i=1}^{n}y_i^2}{\\sigma^2} + \\frac{m_0^2}{s_0^2}\\right) $$\nLet $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i$ be the sample mean. The expression for $E$ can be written as:\n$$ E = \\mu^2\\left(\\frac{n}{\\sigma^2} + \\frac{1}{s_0^2}\\right) - 2\\mu\\left(\\frac{n\\bar{y}}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) + C $$\nwhere $C$ contains all terms not depending on $\\mu$.\nThe structure of the exponent, a quadratic in $\\mu$, shows that the posterior distribution is also normal. Let the posterior be $\\mu \\mid y \\sim \\mathcal{N}(m_n, s_n^2)$. Its PDF is proportional to $\\exp\\left(-\\frac{(\\mu-m_n)^2}{2s_n^2}\\right)$. Expanding the exponent:\n$$ -\\frac{(\\mu-m_n)^2}{2s_n^2} = -\\frac{1}{2s_n^2}(\\mu^2 - 2m_n\\mu + m_n^2) = -\\frac{1}{2}\\left(\\frac{1}{s_n^2}\\mu^2 - \\frac{2m_n}{s_n^2}\\mu + \\frac{m_n^2}{s_n^2}\\right) $$\nBy comparing the coefficients of $\\mu^2$ and $\\mu$ in our two expressions for the exponent, we can identify the posterior parameters $m_n$ and $s_n^2$.\nComparing the $\\mu^2$ coefficients:\n$$ \\frac{1}{s_n^2} = \\frac{n}{\\sigma^2} + \\frac{1}{s_0^2} $$\nThis gives the posterior precision (inverse variance). The posterior variance $s_n^2$ is:\n$$ s_n^2 = \\left(\\frac{n}{\\sigma^2} + \\frac{1}{s_0^2}\\right)^{-1} $$\nComparing the $\\mu$ coefficients:\n$$ \\frac{m_n}{s_n^2} = \\frac{n\\bar{y}}{\\sigma^2} + \\frac{m_0}{s_0^2} $$\nSolving for the posterior mean $m_n$:\n$$ m_n = s_n^2 \\left(\\frac{n\\bar{y}}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) = \\frac{\\frac{n\\bar{y}}{\\sigma^2} + \\frac{m_0}{s_0^2}}{\\frac{n}{\\sigma^2} + \\frac{1}{s_0^2}} $$\nThus, the posterior distribution $p(\\mu \\mid y)$ is a normal distribution with mean $m_n$ and variance $s_n^2$ as derived above.\n\nNow, we substitute the provided numerical values to compute the posterior mean.\nThe given data are:\nKnown variance: $\\sigma^2 = 16$.\nPrior parameters: $m_0 = -2$ and $s_0^2 = 9$.\nSample size: $n=8$.\nObserved changes: $y = \\{4, -1, 3, 2, 0, 1, 1, 2\\}$.\n\nFirst, we calculate the sample mean $\\bar{y}$:\n$$ \\sum_{i=1}^{8} y_i = 4 - 1 + 3 + 2 + 0 + 1 + 1 + 2 = 12 $$\n$$ \\bar{y} = \\frac{\\sum_{i=1}^{8} y_i}{n} = \\frac{12}{8} = 1.5 $$\nNext, we substitute these values into the formula for the posterior mean $m_n$:\n$$ m_n = \\frac{\\frac{n\\bar{y}}{\\sigma^2} + \\frac{m_0}{s_0^2}}{\\frac{n}{\\sigma^2} + \\frac{1}{s_0^2}} = \\frac{\\frac{8 \\times 1.5}{16} + \\frac{-2}{9}}{\\frac{8}{16} + \\frac{1}{9}} $$\nLet's compute the numerator and denominator separately.\nDenominator:\n$$ \\frac{8}{16} + \\frac{1}{9} = \\frac{1}{2} + \\frac{1}{9} = \\frac{9}{18} + \\frac{2}{18} = \\frac{11}{18} $$\nNumerator:\n$$ \\frac{8 \\times 1.5}{16} + \\frac{-2}{9} = \\frac{12}{16} - \\frac{2}{9} = \\frac{3}{4} - \\frac{2}{9} = \\frac{27}{36} - \\frac{8}{36} = \\frac{19}{36} $$\nNow, we compute the ratio:\n$$ m_n = \\frac{\\frac{19}{36}}{\\frac{11}{18}} = \\frac{19}{36} \\times \\frac{18}{11} = \\frac{19}{2 \\times 11} = \\frac{19}{22} $$\nFinally, we convert this fraction to a decimal and round to four significant figures:\n$$ m_n = \\frac{19}{22} \\approx 0.86363636\\dots $$\nRounding to four significant figures, we get $0.8636$. The units are millimeters of mercury (mm Hg).\nThe posterior mean of $\\mu$ is $0.8636$ mm Hg.",
            "answer": "$$\\boxed{0.8636}$$"
        },
        {
            "introduction": "A key aspect of Bayesian analysis is understanding the interplay between prior knowledge and observed data. This exercise uses the Beta-Binomial model, a cornerstone for analyzing proportions in biostatistics, to explore this relationship through a sensitivity analysis. You will demonstrate that the posterior mean is a weighted average of the prior mean and the sample proportion, with the weights depending on the sample size and the \"strength\" of the prior . This practice builds crucial intuition about how to interpret the posterior and how the influence of the prior diminishes as more data is collected.",
            "id": "4896446",
            "problem": "A public health team is estimating the prevalence $p$ of prior infection in a community using a perfectly specific and sensitive serological assay. They test a simple random sample of $n$ individuals and observe $y$ positives. They adopt a Beta prior for $p$ with hyperparameters $(\\alpha,\\beta)$, where the prior mean is fixed at $m_{0}=\\alpha/(\\alpha+\\beta)$ while the prior strength $s=\\alpha+\\beta$ is allowed to vary as part of a sensitivity analysis. The data model is a Binomial sampling model for $y$ given $p$ and $n$.\n\nIn a particular survey, the team observes $y=36$ positives out of $n=240$ individuals, and they fix the prior mean at $m_{0}=0.1$. Using Bayesâ€™ theorem with the Binomial likelihood and the Beta prior, derive the posterior mean of $p$ as a function of the prior strength $s$ (with $m_{0}$ fixed), and show that the posterior mean is a weighted average of $m_{0}$ and the sample proportion $y/n$ with weights that depend on $s$ and $n$. Then, determine the value of $s$ for which the posterior mean equals the simple arithmetic average of the prior mean $m_{0}$ and the sample proportion $y/n$.\n\nProvide your final answer as an exact integer for the case $y=36$, $n=240$, and $m_{0}=0.1$.",
            "solution": "The problem requires a Bayesian analysis of a proportion using a Beta-Binomial conjugate model. We need to derive the posterior mean of the prevalence $p$ and find the prior strength $s$ that makes this mean equal to the simple average of the prior mean and the sample proportion.\n\nThe prior distribution for the prevalence $p$ is $p \\sim \\mathrm{Beta}(\\alpha, \\beta)$. The likelihood for observing $y$ successes in $n$ trials is proportional to $p^y (1-p)^{n-y}$. By Bayes' theorem, the posterior distribution is found by multiplying the prior and the likelihood kernels:\n$$ p | y, n \\propto p^{\\alpha-1} (1-p)^{\\beta-1} \\times p^y (1-p)^{n-y} = p^{\\alpha+y-1} (1-p)^{\\beta+n-y-1} $$\nThis shows the posterior distribution is also a Beta distribution: $p | y, n \\sim \\mathrm{Beta}(\\alpha+y, \\beta+n-y)$.\n\nThe mean of a $\\mathrm{Beta}(a,b)$ distribution is $\\frac{a}{a+b}$. Thus, the posterior mean $m_1$ is:\n$$ m_1 = E[p|y,n] = \\frac{\\alpha+y}{(\\alpha+y) + (\\beta+n-y)} = \\frac{\\alpha+y}{\\alpha+\\beta+n} $$\nThe problem defines the prior mean as $m_0 = \\frac{\\alpha}{\\alpha+\\beta}$ and the prior strength as $s = \\alpha+\\beta$. From these, we can express $\\alpha = m_0 s$. Substituting these into the expression for the posterior mean:\n$$ m_1 = \\frac{m_0 s + y}{s+n} $$\nTo show this is a weighted average of the prior mean $m_0$ and the sample proportion $\\hat{p} = \\frac{y}{n}$, we can rearrange the terms:\n$$ m_1 = \\frac{s \\cdot m_0 + n \\cdot \\frac{y}{n}}{s+n} = \\left(\\frac{s}{s+n}\\right) m_0 + \\left(\\frac{n}{s+n}\\right) \\hat{p} $$\nThis expression clearly shows the posterior mean as a weighted average of the prior mean and the sample proportion, with weights proportional to the prior strength $s$ and the sample size $n$.\n\nThe problem asks for the value of $s$ where the posterior mean $m_1$ equals the simple arithmetic average of the prior mean and the sample proportion, i.e., $m_1 = \\frac{m_0 + \\hat{p}}{2}$.\n$$ \\left(\\frac{s}{s+n}\\right) m_0 + \\left(\\frac{n}{s+n}\\right) \\hat{p} = \\frac{1}{2} m_0 + \\frac{1}{2} \\hat{p} $$\nFor this equality to hold for arbitrary distinct values of $m_0$ and $\\hat{p}$, the weights must be equal:\n$$ \\frac{s}{s+n} = \\frac{1}{2} \\quad \\text{and} \\quad \\frac{n}{s+n} = \\frac{1}{2} $$\nBoth equations lead to the same conclusion:\n$$ 2s = s+n \\implies s = n $$\nThe posterior mean is the simple arithmetic average of the prior mean and sample proportion if and only if the prior strength $s$ equals the sample size $n$.\n\nGiven the sample size $n=240$, the required value for the prior strength is $s=240$. The specific values of $y=36$ and $m_0=0.1$ define the context but are not needed to find the relationship between $s$ and $n$.",
            "answer": "$$ \\boxed{240} $$"
        },
        {
            "introduction": "Beyond estimating parameters, a powerful application of Bayesian modeling is making predictions about future observations. This exercise demonstrates how to calculate the posterior predictive probability, which formalizes this process. Building on the Beta-Binomial framework, you will integrate over the posterior uncertainty of the success probability parameter $p$ to find the probability of a success in a new, unseen trial . This practice showcases how Bayesian methods provide a natural way to quantify our uncertainty about future outcomes, a critical task in clinical and public health decision-making.",
            "id": "4896497",
            "problem": "A biostatistician is modeling the probability that a new patient exhibits a binary clinical outcome labeled \"success\" in a molecular assay. Let the unknown success probability be denoted by $p$, and let the data consist of $n$ independent and identically distributed binary outcomes with $y$ observed successes. The prior belief about $p$ is modeled by a Beta distribution with parameters $\\alpha$ and $\\beta$, written $p \\sim \\mathrm{Beta}(\\alpha,\\beta)$. The observed data likelihood, given $p$, follows the Binomial model.\n\nUsing Bayes' theorem and integrating out the parameter $p$, derive from first principles the posterior predictive probability that the next, new Bernoulli trial will be a success, expressed as a single closed-form analytic expression in terms of $\\alpha$, $\\beta$, $y$, and $n$. Do not substitute numerical values. No rounding is required. Provide the final expression only, simplified as far as possible.",
            "solution": "The problem requires the derivation of the posterior predictive probability of a success for a new Bernoulli trial, given a set of existing observations. Let the unknown success probability be $p$. The prior belief about $p$ is described by a Beta distribution, and the data likelihood is modeled by a Binomial distribution. This is a classic application of Bayesian inference using conjugate priors.\n\nLet $D$ represent the observed data, which consists of $y$ successes in $n$ independent and identically distributed (i.i.d.) trials. We are given:\n1.  **Prior Distribution**: The prior probability density function (PDF) for $p$ is a Beta distribution with parameters $\\alpha$ and $\\beta$.\n    $$f(p|\\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha-1} (1-p)^{\\beta-1}$$\n    where $\\Gamma(\\cdot)$ is the Gamma function. This is denoted as $p \\sim \\mathrm{Beta}(\\alpha, \\beta)$.\n\n2.  **Likelihood Function**: The probability of observing $y$ successes in $n$ trials, given the success probability $p$, follows a Binomial distribution. The likelihood function is:\n    $$L(p|D) = P(y, n|p) = \\binom{n}{y} p^y (1-p)^{n-y}$$\n\nOur goal is to find the posterior predictive probability of a success in a new trial, which we denote as $\\tilde{y}=1$. This probability is conditioned on the observed data $D$. By the law of total probability, we can express this by integrating over all possible values of the unknown parameter $p$, weighted by its posterior probability density.\n$$P(\\tilde{y}=1|D) = \\int_0^1 P(\\tilde{y}=1|p, D) f(p|D) dp$$\nSince the trials are i.i.d. conditional on $p$, the probability of the next outcome depends only on $p$, not on the past data $D$. Thus, $P(\\tilde{y}=1|p, D) = P(\\tilde{y}=1|p)$. For a single Bernoulli trial with success probability $p$, this is simply $p$.\n$$P(\\tilde{y}=1|D) = \\int_0^1 p \\cdot f(p|D) dp$$\nThis integral is, by definition, the expected value of $p$ with respect to its posterior distribution, $E[p|D]$.\n\nThe first step is to derive the posterior distribution, $f(p|D)$, using Bayes' theorem:\n$$f(p|D) \\propto L(p|D) \\cdot f(p|\\alpha, \\beta)$$\nSubstituting the expressions for the likelihood and prior, and ignoring the normalization constants that do not depend on $p$:\n$$f(p|D) \\propto \\left(p^y (1-p)^{n-y}\\right) \\cdot \\left(p^{\\alpha-1} (1-p)^{\\beta-1}\\right)$$\nCombining the terms with base $p$ and $(1-p)$:\n$$f(p|D) \\propto p^{y+\\alpha-1} (1-p)^{n-y+\\beta-1}$$\nThis functional form is the kernel of a Beta distribution. We can identify the updated parameters for the posterior distribution:\nThe posterior shape parameter $\\alpha'$ is $y+\\alpha$.\nThe posterior shape parameter $\\beta'$ is $n-y+\\beta$.\nThus, the posterior distribution of $p$ given the data $D$ is a Beta distribution:\n$$p|D \\sim \\mathrm{Beta}(y+\\alpha, n-y+\\beta)$$\nThe normalized posterior PDF is:\n$$f(p|D) = \\frac{\\Gamma((y+\\alpha)+(n-y+\\beta))}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} p^{y+\\alpha-1} (1-p)^{n-y+\\beta-1}$$\n$$f(p|D) = \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} p^{y+\\alpha-1} (1-p)^{n-y+\\beta-1}$$\nNow we compute the posterior predictive probability by calculating the expectation $E[p|D]$:\n$$P(\\tilde{y}=1|D) = \\int_0^1 p \\cdot f(p|D) dp$$\n$$= \\int_0^1 p \\cdot \\left( \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} p^{y+\\alpha-1} (1-p)^{n-y+\\beta-1} \\right) dp$$\nWe can pull the constant normalization factor out of the integral:\n$$= \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} \\int_0^1 p \\cdot p^{y+\\alpha-1} (1-p)^{n-y+\\beta-1} dp$$\n$$= \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} \\int_0^1 p^{y+\\alpha} (1-p)^{n-y+\\beta-1} dp$$\nThe integral is recognized as the Beta function, $B(a,b) = \\int_0^1 t^{a-1}(1-t)^{b-1}dt = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$. In our case, the parameters for the Beta function in the integral are $a=y+\\alpha+1$ and $b=n-y+\\beta$.\n$$\\int_0^1 p^{(y+\\alpha+1)-1} (1-p)^{(n-y+\\beta)-1} dp = B(y+\\alpha+1, n-y+\\beta)$$\n$$= \\frac{\\Gamma(y+\\alpha+1)\\Gamma(n-y+\\beta)}{\\Gamma((y+\\alpha+1)+(n-y+\\beta))}$$\nSubstituting this back into the expression for the posterior predictive probability:\n$$P(\\tilde{y}=1|D) = \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} \\cdot \\frac{\\Gamma(y+\\alpha+1)\\Gamma(n-y+\\beta)}{\\Gamma(n+\\alpha+\\beta+1)}$$\nWe can cancel the term $\\Gamma(n-y+\\beta)$.\n$$P(\\tilde{y}=1|D) = \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)} \\cdot \\frac{\\Gamma(y+\\alpha+1)}{\\Gamma(n+\\alpha+\\beta+1)}$$\nUsing the fundamental property of the Gamma function, $\\Gamma(z+1) = z\\Gamma(z)$, we can rewrite $\\Gamma(y+\\alpha+1)$ and $\\Gamma(n+\\alpha+\\beta+1)$:\n$$\\Gamma(y+\\alpha+1) = (y+\\alpha)\\Gamma(y+\\alpha)$$\n$$\\Gamma(n+\\alpha+\\beta+1) = (n+\\alpha+\\beta)\\Gamma(n+\\alpha+\\beta)$$\nSubstituting these into the expression:\n$$P(\\tilde{y}=1|D) = \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)} \\cdot \\frac{(y+\\alpha)\\Gamma(y+\\alpha)}{(n+\\alpha+\\beta)\\Gamma(n+\\alpha+\\beta)}$$\nNow, we can cancel the terms $\\Gamma(n+\\alpha+\\beta)$ and $\\Gamma(y+\\alpha)$:\n$$P(\\tilde{y}=1|D) = \\frac{y+\\alpha}{n+\\alpha+\\beta}$$\nThis is the final, simplified closed-form expression for the posterior predictive probability that the next trial will be a success. It represents the mean of the posterior Beta distribution.",
            "answer": "$$\\boxed{\\frac{y+\\alpha}{n+\\alpha+\\beta}}$$"
        }
    ]
}