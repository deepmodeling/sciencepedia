{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of biostatistics is the ability to update our assessment of a patient's condition as new diagnostic information becomes available. This exercise provides a practical application of fundamental probability rules to a realistic clinical scenario. By working through it, you will gain hands-on experience using the law of total probability and Bayes' theorem to combine prior knowledge about disease prevalence with new test results, ultimately calculating the revised probability of a specific disease .",
            "id": "4913399",
            "problem": "A hospital surveillance study of adult patients admitted with acute lower respiratory infection has determined that, conditional on admission with this syndrome, exactly one of three etiologies is present: Influenza A ($A$), Respiratory Syncytial Virus ($B$), or Bacterial Pneumonia ($C$). Based on historical prevalence in this admitted population, the prior probabilities are $P(A)=0.24$, $P(B)=0.31$, and $P(C)=0.45$.\n\nEach admitted patient undergoes two diagnostic assessments:\n1. A molecular respiratory panel (polymerase chain reaction (PCR)-based), which returns either positive $(M^{+})$ or negative $(M^{-})$ for viral targets.\n2. A serum procalcitonin test, dichotomized at a clinically validated threshold, which returns either high $(P^{+})$ or low $(P^{-})$.\n\nFrom independent clinical validation data, the following conditional probabilities are known:\n- Given $A$: $P(M^{+}\\mid A)=0.94$ and $P(P^{+}\\mid A)=0.28$.\n- Given $B$: $P(M^{+}\\mid B)=0.88$ and $P(P^{+}\\mid B)=0.35$.\n- Given $C$: $P(M^{+}\\mid C)=0.17$ and $P(P^{+}\\mid C)=0.81$.\n\nIt is reasonable to assume that, conditional on the true etiology, the two test outcomes are independent.\n\nUsing only the core definitions of probability and conditional probability, and the independence assumption stated above, do the following:\n- Derive the full joint probability table over the sample space consisting of disease $\\{A,B,C\\}$ and test outcome pairs $\\{(M^{+},P^{+}), (M^{+},P^{-}), (M^{-},P^{+}), (M^{-},P^{-})\\}$.\n- Then, compute the posterior probability that an admitted patient has Bacterial Pneumonia $(C)$ given that both tests are positive, that is, $P(C\\mid M^{+},P^{+})$.\n\nExpress the final numeric answer as a decimal rounded to four significant figures. Do not use a percentage sign.",
            "solution": "The problem is deemed valid.\n**Step 1: Extract Givens:** The problem provides a set of mutually exclusive and exhaustive etiologies $\\{A, B, C\\}$ with prior probabilities $P(A)=0.24$, $P(B)=0.31$, and $P(C)=0.45$. It specifies two diagnostic tests with binary outcomes, $(M^{+}, M^{-})$ and $(P^{+}, P^{-})$. It provides conditional probabilities of positive test results for each etiology: $P(M^{+}\\mid A)=0.94$, $P(P^{+}\\mid A)=0.28$; $P(M^{+}\\mid B)=0.88$, $P(P^{+}\\mid B)=0.35$; $P(M^{+}\\mid C)=0.17$, $P(P^{+}\\mid C)=0.81$. A key assumption of conditional independence of the tests given the etiology is stated. The tasks are to derive a joint probability table and compute a specific posterior probability, $P(C\\mid M^{+},P^{+})$, rounded to four significant figures.\n**Step 2: Validate Using Extracted Givens:** The problem is scientifically grounded, describing a standard scenario in clinical diagnostics and epidemiology. It is well-posed, as all necessary probabilities are provided, the priors sum to $1$, and the tasks are clearly defined, leading to a unique solution. The language is objective and precise. The problem is a direct application of fundamental probability theory, specifically the law of total probability and Bayes' theorem, and is not trivial or ill-posed. All conditions for a valid problem are met.\n\nThe solution proceeds as follows. Let $E$ be a random variable for the etiology, taking values in $\\{A, B, C\\}$. Let $M$ and $P$ be random variables for the outcomes of the molecular panel and procalcitonin test, respectively.\n\nThe given prior probabilities are:\n$$P(A) = 0.24$$\n$$P(B) = 0.31$$\n$$P(C) = 0.45$$\nThese are mutually exclusive and exhaustive, as $0.24 + 0.31 + 0.45 = 1$.\n\nThe given conditional probabilities for positive test results are:\n- For etiology $A$: $P(M^{+} \\mid A) = 0.94$ and $P(P^{+} \\mid A) = 0.28$.\n- For etiology $B$: $P(M^{+} \\mid B) = 0.88$ and $P(P^{+} \\mid B) = 0.35$.\n- For etiology $C$: $P(M^{+} \\mid C) = 0.17$ and $P(P^{+} \\mid C) = 0.81$.\n\nThe problem states that, conditional on the true etiology, the two test outcomes are independent. This can be expressed for any etiology $E \\in \\{A, B, C\\}$ and test outcomes $M_i \\in \\{M^{+}, M^{-}\\}$, $P_j \\in \\{P^{+}, P^{-}\\}$ as:\n$$P(M_i, P_j \\mid E) = P(M_i \\mid E) P(P_j \\mid E)$$\n\nThe first task is to derive the full joint probability table for $P(E, M, P)$. The joint probability is found using the chain rule: $P(E, M, P) = P(M, P \\mid E)P(E)$. Applying the conditional independence assumption, this becomes:\n$$P(E, M, P) = P(M \\mid E) P(P \\mid E) P(E)$$\n\nFirst, we compute the conditional probabilities for negative test outcomes using the complement rule, $P(\\text{event}^c) = 1 - P(\\text{event})$:\n- $P(M^{-} \\mid A) = 1 - P(M^{+} \\mid A) = 1 - 0.94 = 0.06$\n- $P(P^{-} \\mid A) = 1 - P(P^{+} \\mid A) = 1 - 0.28 = 0.72$\n- $P(M^{-} \\mid B) = 1 - P(M^{+} \\mid B) = 1 - 0.88 = 0.12$\n- $P(P^{-} \\mid B) = 1 - P(P^{+} \\mid B) = 1 - 0.35 = 0.65$\n- $P(M^{-} \\mid C) = 1 - P(M^{+} \\mid C) = 1 - 0.17 = 0.83$\n- $P(P^{-} \\mid C) = 1 - P(P^{+} \\mid C) = 1 - 0.81 = 0.19$\n\nNow we can compute all $3 \\times 2 \\times 2 = 12$ joint probabilities:\n\nFor etiology $A$ (with $P(A) = 0.24$):\n- $P(A, M^{+}, P^{+}) = P(M^{+} \\mid A) P(P^{+} \\mid A) P(A) = 0.94 \\times 0.28 \\times 0.24 = 0.063168$\n- $P(A, M^{+}, P^{-}) = P(M^{+} \\mid A) P(P^{-} \\mid A) P(A) = 0.94 \\times 0.72 \\times 0.24 = 0.162432$\n- $P(A, M^{-}, P^{+}) = P(M^{-} \\mid A) P(P^{+} \\mid A) P(A) = 0.06 \\times 0.28 \\times 0.24 = 0.004032$\n- $P(A, M^{-}, P^{-}) = P(M^{-} \\mid A) P(P^{-} \\mid A) P(A) = 0.06 \\times 0.72 \\times 0.24 = 0.010368$\n\nFor etiology $B$ (with $P(B) = 0.31$):\n- $P(B, M^{+}, P^{+}) = P(M^{+} \\mid B) P(P^{+} \\mid B) P(B) = 0.88 \\times 0.35 \\times 0.31 = 0.095480$\n- $P(B, M^{+}, P^{-}) = P(M^{+} \\mid B) P(P^{-} \\mid B) P(B) = 0.88 \\times 0.65 \\times 0.31 = 0.177320$\n- $P(B, M^{-}, P^{+}) = P(M^{-} \\mid B) P(P^{+} \\mid B) P(B) = 0.12 \\times 0.35 \\times 0.31 = 0.013020$\n- $P(B, M^{-}, P^{-}) = P(M^{-} \\mid B) P(P^{-} \\mid B) P(B) = 0.12 \\times 0.65 \\times 0.31 = 0.024180$\n\nFor etiology $C$ (with $P(C) = 0.45$):\n- $P(C, M^{+}, P^{+}) = P(M^{+} \\mid C) P(P^{+} \\mid C) P(C) = 0.17 \\times 0.81 \\times 0.45 = 0.061965$\n- $P(C, M^{+}, P^{-}) = P(M^{+} \\mid C) P(P^{-} \\mid C) P(C) = 0.17 \\times 0.19 \\times 0.45 = 0.014535$\n- $P(C, M^{-}, P^{+}) = P(M^{-} \\mid C) P(P^{+} \\mid C) P(C) = 0.83 \\times 0.81 \\times 0.45 = 0.302535$\n- $P(C, M^{-}, P^{-}) = P(M^{-} \\mid C) P(P^{-} \\mid C) P(C) = 0.83 \\times 0.19 \\times 0.45 = 0.070965$\n\nThe full joint probability table is as follows:\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Etiology } E & \\text{Outcomes } (M,P) & \\text{Joint Probability } P(E,M,P) \\\\\n\\hline\n\\hline\nA & (M^{+}, P^{+}) & 0.063168 \\\\\nA & (M^{+}, P^{-}) & 0.162432 \\\\\nA & (M^{-}, P^{+}) & 0.004032 \\\\\nA & (M^{-}, P^{-}) & 0.010368 \\\\\n\\hline\nB & (M^{+}, P^{+}) & 0.095480 \\\\\nB & (M^{+}, P^{-}) & 0.177320 \\\\\nB & (M^{-}, P^{+}) & 0.013020 \\\\\nB & (M^{-}, P^{-}) & 0.024180 \\\\\n\\hline\nC & (M^{+}, P^{+}) & 0.061965 \\\\\nC & (M^{+}, P^{-}) & 0.014535 \\\\\nC & (M^{-}, P^{+}) & 0.302535 \\\\\nC & (M^{-}, P^{-}) & 0.070965 \\\\\n\\hline\n\\end{array}\n$$\n\nThe second task is to compute the posterior probability $P(C \\mid M^{+}, P^{+})$. Using the definition of conditional probability, which is the foundation of Bayes' theorem:\n$$P(C \\mid M^{+}, P^{+}) = \\frac{P(C, M^{+}, P^{+})}{P(M^{+}, P^{+})}$$\nThe numerator is a joint probability we have already calculated:\n$$P(C, M^{+}, P^{+}) = 0.061965$$\nThe denominator, $P(M^{+}, P^{+})$, is the marginal probability of observing the outcome $(M^{+}, P^{+})$. It is obtained by summing the joint probabilities of this outcome over all possible etiologies, according to the law of total probability:\n$$P(M^{+}, P^{+}) = P(A, M^{+}, P^{+}) + P(B, M^{+}, P^{+}) + P(C, M^{+}, P^{+})$$\nUsing the values from our calculations:\n$$P(M^{+}, P^{+}) = 0.063168 + 0.095480 + 0.061965 = 0.220613$$\nNow we can compute the posterior probability:\n$$P(C \\mid M^{+}, P^{+}) = \\frac{0.061965}{0.220613} \\approx 0.2808775$$\nThe problem requires the answer to be rounded to four significant figures. The fifth significant figure is $7$, so we round up the fourth.\n$$P(C \\mid M^{+}, P^{+}) \\approx 0.2809$$\nThis is the posterior probability that a patient has Bacterial Pneumonia given a positive molecular panel and a high procalcitonin level.",
            "answer": "$$\\boxed{0.2809}$$"
        },
        {
            "introduction": "Many statistical models, including the diagnostic framework in the previous exercise, rely on assumptions of independence between different events or measurements. This computational practice shifts our focus from theoretical calculation to empirical verification. You will write a simulation to generate data under different conditions—one where independence is true by design, and one where it is not—and use the data to test the mathematical definition of independence, $P(A, B) = P(A)P(B)$ . This exercise provides a concrete understanding of how abstract theoretical properties are observed and validated in practice.",
            "id": "4913365",
            "problem": "Consider two binary random variables $X \\in \\{0,1\\}$ and $Y \\in \\{0,1\\}$ representing the presence ($1$) or absence ($0$) of two distinct biomarkers recorded per trial in a biostatistical simulation study. The foundational base is the axiomatic definition of probability on a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$, the definition of independence of random variables, and the frequency-based estimator of probabilities. Independence is defined as $X$ independent of $Y$ if and only if for all $x \\in \\{0,1\\}$ and $y \\in \\{0,1\\}$, the equality $\\mathbb{P}(X=x, Y=y) = \\mathbb{P}(X=x)\\,\\mathbb{P}(Y=y)$ holds.\n\nYou will generate synthetic data across repeated trials, estimate empirical probabilities from the generated data, and verify independence by demonstrating equality of joint and product marginals to within a specified tolerance. For each parameter set, simulate $N$ independent and identically distributed trials producing pairs $(X_t, Y_t)$ for $t = 1, 2, \\ldots, N$. Let the empirical joint probabilities be\n$$\n\\hat{p}_{ij} = \\frac{1}{N}\\sum_{t=1}^{N} \\mathbf{1}\\{X_t=i, Y_t=j\\}, \\quad i,j \\in \\{0,1\\},\n$$\nand let the empirical marginals be\n$$\n\\hat{p}_{i\\cdot} = \\sum_{j=0}^{1}\\hat{p}_{ij} \\quad \\text{and} \\quad \\hat{p}_{\\cdot j} = \\sum_{i=0}^{1}\\hat{p}_{ij}.\n$$\nDefine the verification statistic\n$$\nD = \\max_{i \\in \\{0,1\\},\\, j \\in \\{0,1\\}} \\left| \\hat{p}_{ij} - \\hat{p}_{i\\cdot}\\,\\hat{p}_{\\cdot j} \\right|.\n$$\nFor a user-specified tolerance $\\varepsilon > 0$, declare independence verified if and only if $D \\le \\varepsilon$.\n\nSimulation models:\n- Independent Bernoulli marginals: draw $X_t \\sim \\text{Bernoulli}(p_X)$ and $Y_t \\sim \\text{Bernoulli}(p_Y)$ independently for each $t$.\n- Specified joint distribution with given marginals and dependence parameter: construct a joint probability table using\n$$\n\\begin{align*}\np_{11} &= p_X p_Y + c,\\\\\np_{10} &= p_X(1 - p_Y) - c,\\\\\np_{01} &= (1 - p_X)p_Y - c,\\\\\np_{00} &= (1 - p_X)(1 - p_Y) + c,\n\\end{align*}\n$$\nsubject to the constraint that all $p_{ij} \\ge 0$ and $\\sum_{i,j} p_{ij} = 1$. When $c \\neq 0$ and the constraints hold, this yields a dependent model with fixed marginals $p_X$ and $p_Y$.\n\nYour task is to implement a program that, for each parameter set in the test suite below, performs the simulation, computes $D$, and outputs a boolean indicating whether independence is verified to within the supplied $\\varepsilon$. The program must be self-contained and must not read any input.\n\nThe Random Number Generator (RNG) must be seeded per test case for reproducibility. Use an RNG that produces independent draws across the $N$ trials and adheres to the specified model.\n\nTest suite parameter sets (each test case is a tuple of parameters):\n1. $(\\text{model}=\\text{independent},\\, N=200000,\\, p_X=0.3,\\, p_Y=0.6,\\, \\varepsilon=0.002,\\, \\text{seed}=12345)$\n2. $(\\text{model}=\\text{dependent},\\, N=200000,\\, p_X=0.3,\\, p_Y=0.6,\\, c=0.05,\\, \\varepsilon=0.002,\\, \\text{seed}=54321)$\n3. $(\\text{model}=\\text{independent},\\, N=500,\\, p_X=0.3,\\, p_Y=0.6,\\, \\varepsilon=0.06,\\, \\text{seed}=111)$\n4. $(\\text{model}=\\text{independent},\\, N=50000,\\, p_X=0.02,\\, p_Y=0.03,\\, \\varepsilon=0.001,\\, \\text{seed}=222)$\n5. $(\\text{model}=\\text{dependent},\\, N=200000,\\, p_X=0.02,\\, p_Y=0.03,\\, c=0.0004,\\, \\varepsilon=0.0003,\\, \\text{seed}=333)$\n\nNotes:\n- For the dependent model, the parameter $c$ must satisfy\n$$\n- \\min\\left\\{p_X(1-p_Y),\\, (1-p_X)p_Y \\right\\} \\le c \\le \\min\\left\\{p_X p_Y,\\, (1-p_X)(1-p_Y) \\right\\}\n$$\nto maintain nonnegativity of all $p_{ij}$.\n- The outputs are booleans. There are no physical units, no angles, and no percentages; all probabilities must be handled as decimals in $[0,1]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,\\text{ }result_2,\\text{ }result_3,\\text{ }result_4,\\text{ }result_5]$), where each $result_k$ is a boolean indicating whether independence is verified under the $k$-th parameter set.",
            "solution": "The problem requires the verification of statistical independence between two binary random variables, $X, Y \\in \\{0,1\\}$, through Monte Carlo simulation. The process is grounded in the axiomatic foundations of probability theory and employs standard statistical estimation techniques.\n\nThe theoretical basis for this problem is the definition of independence for two random variables. On a given probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$, two random variables $X$ and $Y$ are said to be independent if and only if their joint probability distribution function is equal to the product of their marginal distribution functions. For discrete binary variables, this condition simplifies to:\n$$\n\\mathbb{P}(X=i, Y=j) = \\mathbb{P}(X=i)\\,\\mathbb{P}(Y=j) \\quad \\forall i,j \\in \\{0,1\\}\n$$\nLet $p_{ij} = \\mathbb{P}(X=i, Y=j)$ denote the joint probabilities, and let $p_{i\\cdot} = \\mathbb{P}(X=i)$ and $p_{\\cdot j} = \\mathbb{P}(Y=j)$ denote the marginal probabilities. The independence condition is thus $p_{ij} = p_{i\\cdot} p_{\\cdot j}$ for all $i,j$.\n\nIn a simulation study, the true probabilities $\\{p_{ij}\\}$ are defined by the data-generating model. We draw a sample of size $N$ of independent and identically distributed pairs $(X_t, Y_t)_{t=1}^N$. From this sample, we estimate the true probabilities using the frequency-based estimators. The empirical joint probabilities, $\\hat{p}_{ij}$, are calculated as the proportion of trials that result in the outcome $(i,j)$:\n$$\n\\hat{p}_{ij} = \\frac{1}{N} \\sum_{t=1}^{N} \\mathbf{1}\\{X_t=i, Y_t=j\\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The empirical marginal probabilities, $\\hat{p}_{i\\cdot}$ and $\\hat{p}_{\\cdot j}$, are subsequently calculated by summing over the empirical joint probabilities, in accordance with the law of total probability:\n$$\n\\hat{p}_{i\\cdot} = \\sum_{j=0}^{1} \\hat{p}_{ij} = \\hat{p}_{i0} + \\hat{p}_{i1} \\quad \\text{and} \\quad \\hat{p}_{\\cdot j} = \\sum_{i=0}^{1} \\hat{p}_{ij} = \\hat{p}_{0j} + \\hat{p}_{1j}\n$$\nDue to finite sample size $N$, sampling variability will cause the estimated probabilities to deviate from their true values. Consequently, even if the underlying data-generating process is perfectly independent, we do not expect the equality $\\hat{p}_{ij} = \\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j}$ to hold exactly. Instead, we assess whether the deviation from this equality is small. The verification statistic $D$ is defined as the maximum absolute deviation over all four possible outcomes:\n$$\nD = \\max_{i,j \\in \\{0,1\\}} |\\hat{p}_{ij} - \\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j}|\n$$\nIndependence is considered \"verified\" if this maximum deviation is less than or equal to a specified tolerance, $\\varepsilon$, i.e., $D \\le \\varepsilon$.\n\nThe problem specifies two simulation models:\n1.  **Independent Model**: Samples for $X_t$ and $Y_t$ are drawn independently from Bernoulli distributions with parameters $p_X$ and $p_Y$, respectively. By construction, the true joint probabilities are $p_{ij} = p_{i\\cdot}p_{\\cdot j}$, so any measured deviation $D > 0$ arises purely from sampling error.\n2.  **Dependent Model**: A joint probability distribution is constructed using marginal probabilities $p_X$, $p_Y$, and a dependence parameter $c$. The joint probabilities are given by:\n$$\n\\begin{align*}\np_{11} &= p_X p_Y + c, \\\\\np_{10} &= p_X(1 - p_Y) - c, \\\\\np_{01} &= (1 - p_X)p_Y - c, \\\\\np_{00} &= (1 - p_X)(1 - p_Y) + c.\n\\end{align*}\n$$\n    This construction preserves the marginals, i.e., $p_{1\\cdot} = p_{11}+p_{10} = p_X$ and $p_{\\cdot 1} = p_{11}+p_{01} = p_Y$. The parameter $c$ is equal to the covariance between $X$ and $Y$, $\\text{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = p_{11} - p_X p_Y = c$. When $c \\neq 0$, the variables are dependent, and the theoretical deviation is $|c|$.\n\nThe algorithmic implementation proceeds on a case-by-case basis. For each test case, the random number generator is seeded for reproducibility.\n-   Data Generation: For the independent model, two separate arrays of random numbers are generated. For the dependent model, the four joint probabilities $\\{p_{00}, p_{01}, p_{10}, p_{11}\\}$ are first calculated. These probabilities define a categorical distribution over the four outcomes $\\{(0,0), (0,1), (1,0), (1,1)\\}$. $N$ samples are drawn from this distribution. A vectorized approach maps the sampled category index (from $0$ to $3$) back to the corresponding $(X,Y)$ pair.\n-   Computation of $D$: A $2 \\times 2$ contingency table of counts is efficiently constructed from the generated $X$ and $Y$ data. This table is normalized by $N$ to produce the empirical joint probability matrix, $\\hat{P} = (\\hat{p}_{ij})$. The marginal probability vectors, $\\hat{p}_{i\\cdot}$ and $\\hat{p}_{\\cdot j}$, are found by summing the rows and columns of $\\hat{P}$, respectively. The matrix of products of marginals, $(\\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j})$, is computed via the outer product of these vectors. Finally, $D$ is obtained by finding the maximum element in the matrix of absolute differences between $\\hat{P}$ and the product-of-marginals matrix. The resulting $D$ is compared to $\\varepsilon$ to produce the final boolean output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by running simulations for each test case\n    and verifying independence based on the specified criterion.\n    \"\"\"\n\n    test_cases = [\n        ('independent', 200000, 0.3, 0.6, 0.002, 12345),\n        ('dependent', 200000, 0.3, 0.6, 0.05, 0.002, 54321),\n        ('independent', 500, 0.3, 0.6, 0.06, 111),\n        ('independent', 50000, 0.02, 0.03, 0.001, 222),\n        ('dependent', 200000, 0.02, 0.03, 0.0004, 0.0003, 333)\n    ]\n\n    results = []\n\n    def _calculate_verification(X, Y, N, epsilon):\n        \"\"\"\n        Common logic to calculate the verification statistic D and check against epsilon.\n        \"\"\"\n        # Create a 2x2 contingency table of counts\n        counts = np.zeros((2, 2), dtype=np.int64)\n        np.add.at(counts, (X, Y), 1)\n\n        # Calculate empirical joint probabilities\n        p_hat_ij = counts / N\n\n        # Calculate empirical marginal probabilities\n        p_hat_i_dot = np.sum(p_hat_ij, axis=1)\n        p_hat_dot_j = np.sum(p_hat_ij, axis=0)\n\n        # Calculate the matrix of product-of-marginals\n        p_hat_prod = np.outer(p_hat_i_dot, p_hat_dot_j)\n\n        # Calculate the verification statistic D\n        D = np.max(np.abs(p_hat_ij - p_hat_prod))\n\n        # Check if D is within the tolerance epsilon\n        return D <= epsilon\n\n    for case in test_cases:\n        model = case[0]\n        seed = case[-1]\n        epsilon = case[-2]\n        N = case[1]\n        p_X = case[2]\n        p_Y = case[3]\n        \n        # Initialize the random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        if model == 'independent':\n            # Generate X and Y from independent Bernoulli distributions\n            X = (rng.random(N) < p_X).astype(np.int8)\n            Y = (rng.random(N) < p_Y).astype(np.int8)\n        \n        elif model == 'dependent':\n            c = case[4]\n            \n            # Calculate the joint probability distribution\n            p11 = p_X * p_Y + c\n            p10 = p_X * (1 - p_Y) - c\n            p01 = (1 - p_X) * p_Y - c\n            p00 = 1.0 - p11 - p10 - p01\n            \n            # Probabilities for outcomes (0,0), (0,1), (1,0), (1,1)\n            probs = [p00, p01, p10, p11]\n            \n            # Draw N samples from the categorical distribution\n            choices = rng.choice(4, size=N, p=probs)\n            \n            # Map choice indices to (X, Y) pairs efficiently\n            # choice 0 -> (0,0), 1 -> (0,1), 2 -> (1,0), 3 -> (1,1)\n            X = (choices // 2).astype(np.int8)\n            Y = (choices % 2).astype(np.int8)\n\n        # Calculate the result for the current case\n        result = _calculate_verification(X, Y, N, epsilon)\n        results.append(result.item())\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Probability is not only about frequencies of events but also a powerful language for quantifying uncertainty and belief. In Bayesian biostatistics, we often begin an analysis by formally encoding expert knowledge into a prior probability distribution. This exercise demonstrates how to translate a clinician's qualitative beliefs—expressed as a plausible range for an unknown parameter—into the precise hyperparameters $(\\alpha, \\beta)$ of a Beta distribution, which is a common and flexible choice for modeling probabilities . This practice is a key step in building Bayesian models and reasoning under uncertainty.",
            "id": "4913394",
            "problem": "A biostatistics study is planning a small safety assessment of a new antihypertensive medication. Before observing any data, clinicians wish to encode their prior beliefs about the unknown probability $p$ that a randomly selected patient experiences a clinically significant adverse event on first exposure. To maintain interpretability and leverage Bayesian conjugacy, model $p$ with a Beta prior $\\mathrm{Beta}(\\alpha,\\beta)$, which is conjugate to the Binomial likelihood for event counts.\n\nClinicians state that, based on similar compounds and early pharmacovigilance, there is a $0.95$ prior probability that $p$ lies in the interval $[0.04, 0.12]$, and that this interval is centered at $0.08$ in the sense that $0.08$ is the midpoint of the elicited bounds. Using only foundational definitions (the cumulative distribution function and quantiles) to connect these prior quantiles to the hyperparameters $(\\alpha,\\beta)$ of a conjugate $\\mathrm{Beta}(\\alpha,\\beta)$ prior, derive the system that maps the elicited quantiles to $(\\alpha,\\beta)$ and then solve for $(\\alpha,\\beta)$ under a principled approximation appropriate for large-shape Beta distributions.\n\nReport the hyperparameters $(\\alpha,\\beta)$ as a pair, rounded to four significant figures. Express all probabilities as decimals (e.g., $0.08$ rather than $8\\%$).",
            "solution": "We begin from the foundational setup for a Bernoulli parameter $p$. If $X$ is the number of adverse events in $n$ patients, then, conditional on $p$, $X$ follows a Binomial distribution with likelihood proportional to $p^{x}(1-p)^{n-x}$. A Beta prior $\\mathrm{Beta}(\\alpha,\\beta)$ is conjugate to the Binomial likelihood, yielding a posterior $\\mathrm{Beta}(\\alpha+x,\\beta+n-x)$. The prior distribution for $p$ is thus characterized by its cumulative distribution function (CDF) $F(p\\,;\\alpha,\\beta)$ and its quantiles.\n\nBy definition of quantiles and the cumulative distribution function, the clinicians’ statements imply that the lower and upper quantiles satisfy\n$$\nF(0.04\\,;\\alpha,\\beta) \\approx 0.025\n\\quad\\text{and}\\quad\nF(0.12\\,;\\alpha,\\beta) \\approx 0.975,\n$$\nso that the central $95\\%$ interval is approximately $[0.04,0.12]$. Because the interval is stated to be centered at the midpoint $0.08$, we take the prior mass to be symmetric around $0.08$ in the sense that the deviations from the center are equal: $0.12-0.08=0.08-0.04=0.04$. For large-shape Beta distributions (i.e., with $\\alpha+\\beta$ sufficiently large), the distribution of $p$ is well-approximated near its center by a Normal distribution with mean\n$$\n\\mu \\equiv \\mathbb{E}[p] = \\frac{\\alpha}{\\alpha+\\beta}\n$$\nand variance\n$$\nV \\equiv \\mathrm{Var}(p) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^{2}(\\alpha+\\beta+1)} = \\frac{\\mu(1-\\mu)}{\\alpha+\\beta+1}.\n$$\nThese mean and variance expressions are fundamental properties of the Beta distribution obtained from its moments.\n\nUnder the Normal approximation, the $\\gamma$-quantile of $p$ is approximately\n$$\nq_{\\gamma} \\approx \\mu + z_{\\gamma}\\sqrt{V},\n$$\nwhere $z_{\\gamma}$ is the $\\gamma$-quantile of the standard Normal distribution. For a central $95\\%$ interval, $z_{0.975}\\approx 1.96$ and $z_{0.025}\\approx -1.96$. With the elicited symmetric bounds and center,\n$$\n\\mu \\approx \\frac{0.04+0.12}{2} = 0.08,\n$$\nand the half-width equals\n$$\n0.12 - 0.08 = 0.04 \\approx z_{0.975}\\sqrt{V} \\quad\\Rightarrow\\quad \\sqrt{V} \\approx \\frac{0.04}{1.96}.\n$$\nThus,\n$$\nV \\approx \\left(\\frac{0.04}{1.96}\\right)^{2}.\n$$\nUsing $V = \\mu(1-\\mu)/(\\alpha+\\beta+1)$ with $\\mu=0.08$, we solve for the prior precision $\\alpha+\\beta$:\n$$\n\\alpha+\\beta+1 \\approx \\frac{\\mu(1-\\mu)}{V}\n= \\frac{0.08\\cdot 0.92}{\\left(\\frac{0.04}{1.96}\\right)^{2}},\n$$\nso\n$$\n\\alpha+\\beta \\approx \\frac{0.08\\cdot 0.92}{\\left(\\frac{0.04}{1.96}\\right)^{2}} - 1.\n$$\nFinally, using $\\mu = \\alpha/(\\alpha+\\beta)$, we recover\n$$\n\\alpha \\approx \\mu(\\alpha+\\beta) \\quad\\text{and}\\quad \\beta \\approx (1-\\mu)(\\alpha+\\beta).\n$$\n\nWe now carry out the calculation numerically at the end. First compute the variance proxy:\n$$\n\\sqrt{V} \\approx \\frac{0.04}{1.96} \\approx 0.020408163, \\quad\nV \\approx (0.020408163)^{2} \\approx 0.000416493.\n$$\nThen\n$$\n\\alpha+\\beta+1 \\approx \\frac{0.08\\cdot 0.92}{0.000416493} \\approx \\frac{0.0736}{0.000416493} \\approx 176.7136,\n$$\nso\n$$\n\\alpha+\\beta \\approx 175.7136.\n$$\nUsing $\\mu=0.08$,\n$$\n\\alpha \\approx 0.08\\times 175.7136 \\approx 14.0571, \\quad\n\\beta \\approx 0.92\\times 175.7136 \\approx 161.6565.\n$$\n\nRounded to four significant figures, the elicited hyperparameters are\n$$\n\\alpha \\approx 14.06, \\quad \\beta \\approx 161.7.\n$$\n\nThis procedure demonstrates how clinically meaningful prior quantiles on $p$ can be mapped to the hyperparameters of a conjugate $\\mathrm{Beta}(\\alpha,\\beta)$ prior using the definitions of quantiles and a Normal approximation grounded in the first two moments of the Beta distribution. In practice, one could refine these by numerically solving $F(0.04\\,;\\alpha,\\beta)=0.025$ and $F(0.12\\,;\\alpha,\\beta)=0.975$ via the regularized incomplete beta function, starting from the above values as initial guesses.",
            "answer": "$$\\boxed{\\begin{pmatrix}14.06 & 161.7\\end{pmatrix}}$$"
        }
    ]
}