{
    "hands_on_practices": [
        {
            "introduction": "我们将从一个基础练习开始，巩固卡方分布的基本性质。此练习将引导你计算一个经过其自由度归一化的卡方随机变量的方差。理解这种常见的统计变换如何影响数据的离散程度，是进行统计推断时的一项核心技能。",
            "id": "2330",
            "problem": "设 $X$ 是一个服从自由度为 $k$ 的卡方分布的随机变量，记为 $X \\sim \\chi^2(k)$。卡方分布是一种在统计推断中有应用的连续概率分布。\n\n对于随机变量 $X \\sim \\chi^2(k)$，已知其期望值（均值）为 $\\mathbb{E}[X] = k$，其方差为 $\\mathrm{Var}(X) = 2k$。\n\n考虑一个新的随机变量 $Y$，它是由 $X$ 除以其自由度进行归一化得到的：\n$$\nY = \\frac{X}{k}\n$$\n你的任务是利用已知的 $X$ 的性质，推导出随机变量 $Y$ 的方差（记为 $\\mathrm{Var}(Y)$）的表达式。请用参数 $k$ 表示你的最终答案。",
            "solution": "我们有 $Y = \\frac{X}{k}$。使用方差在标量乘法下的性质，\n$$\n\\mathrm{Var}(aX) = a^2\\,\\mathrm{Var}(X),\n$$\n令 $a = \\frac{1}{k}$，可得\n$$\n\\mathrm{Var}\\bigl(Y\\bigr)\n= \\mathrm{Var}\\Bigl(\\frac{X}{k}\\Bigr)\n= \\frac{1}{k^2}\\,\\mathrm{Var}(X).\n$$\n由于 $\\mathrm{Var}(X)=2k$，我们得到\n$$\n\\mathrm{Var}(Y)\n= \\frac{1}{k^2}\\,(2k)\n= \\frac{2}{k}.\n$$",
            "answer": "$$\\boxed{\\frac{2}{k}}$$"
        },
        {
            "introduction": "在掌握了基本代数性质后，我们将深入探索卡方分布的概率密度函数（PDF）的几何形态。通过这个练习，你将运用微积分来定位分布的峰值（即众数）。将众数与均值进行比较，能够让你从分析的视角深刻理解为何卡方分布在自由度较低时会呈现出明显的右偏斜形态。",
            "id": "4958326",
            "problem": "一位生物统计学家正在分析列联表数据的拟合优度检验。在原假设下，Pearson 检验统计量 $X$ 服从自由度为 $k$ 的卡方分布，记作 $\\chi^{2}_{k}$。从卡方分布的基本概率密度函数（PDF）和伽马分布的标准均值出发，通过直接在其支撑集上优化其PDF，并适当考虑在 $x=0$ 处的边界情况，来确定 $\\chi^{2}_{k}$ 分布的众数作为 $k$ 的函数。然后，对于 $k \\ge 2$ 的情况，解释该众数的位置与均值的比较，并根据分布的偏度来解释这一比较。将您的最终答案表示为众数的单个闭式解析表达式；无需四舍五入。",
            "solution": "该问题要求推导自由度为 $k$ 的卡方分布（记作 $\\chi^2_k$）的众数，并随后在 $k \\ge 2$ 的情况下将此众数与分布的均值进行比较，以解释其偏度。\n\n首先，我们确定 $\\chi^2_k$ 分布的均值。$\\chi^2_k$ 分布是伽马分布的一个特例。一个服从形状参数为 $\\alpha$、速率参数为 $\\beta$ 的伽马分布的随机变量 $X$，记作 $X \\sim \\text{Gamma}(\\alpha, \\beta)$，其均值为 $E[X] = \\frac{\\alpha}{\\beta}$。$\\chi^2_k$ 分布对应于参数为 $\\alpha = \\frac{k}{2}$ 和 $\\beta = \\frac{1}{2}$ 的伽马分布。因此，$\\chi^2_k$ 分布的均值为 $\\mu = E[X] = \\frac{k/2}{1/2} = k$。\n\n接下来，我们通过最大化 $\\chi^2_k$ 分布的概率密度函数（PDF）来确定众数。对于随机变量 $X \\sim \\chi^2_k$，其PDF由下式给出：\n$$ f(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{k/2 - 1} \\exp(-x/2) $$\n其中 $x > 0$，$k$ 是表示自由度的正整数，$\\Gamma(\\cdot)$ 是伽马函数。该分布的支撑集是区间 $(0, \\infty)$。众数是在此支撑集上使 $f(x; k)$ 最大化的 $x$ 值。\n\n为了简化最大化过程，我们可以处理PDF的自然对数 $\\ln f(x; k)$，因为对数函数是严格单调递增的函数。最大化 $\\ln f(x; k)$ 将得到与最大化 $f(x; k)$ 相同的 $x$ 值。\n$$ \\ln f(x; k) = \\ln\\left(\\frac{1}{2^{k/2} \\Gamma(k/2)}\\right) + \\ln\\left(x^{k/2 - 1}\\right) + \\ln\\left(\\exp(-x/2)\\right) $$\n$$ \\ln f(x; k) = -\\frac{k}{2}\\ln(2) - \\ln\\left(\\Gamma(k/2)\\right) + \\left(\\frac{k}{2} - 1\\right)\\ln(x) - \\frac{x}{2} $$\n为了找到最大值，我们计算关于 $x$ 的一阶导数并将其设为零。不含 $x$ 的项在求导时是常数。\n$$ \\frac{d}{dx} \\ln f(x; k) = \\frac{d}{dx} \\left[ \\left(\\frac{k}{2} - 1\\right)\\ln(x) - \\frac{x}{2} \\right] $$\n$$ \\frac{d}{dx} \\ln f(x; k) = \\left(\\frac{k}{2} - 1\\right)\\frac{1}{x} - \\frac{1}{2} $$\n将导数设为零以寻找临界点：\n$$ \\left(\\frac{k}{2} - 1\\right)\\frac{1}{x} - \\frac{1}{2} = 0 $$\n$$ \\frac{k-2}{2x} = \\frac{1}{2} $$\n假设 $x \\neq 0$（这在支撑集上成立），我们找到临界点：\n$$ x = k - 2 $$\n现在我们必须根据 $k$ 的值来分析这个结果，同时考虑到 $k$ 是一个正整数。\n\n情况1：$k > 2$。\n对于 $k > 2$，临界点 $x=k-2$ 是正数，因此它位于支撑集 $(0, \\infty)$ 内。为了确定这是否为最大值，我们检验二阶导数：\n$$ \\frac{d^2}{dx^2} \\ln f(x; k) = \\frac{d}{dx} \\left[ \\left(\\frac{k-2}{2}\\right)x^{-1} - \\frac{1}{2} \\right] = -\\left(\\frac{k-2}{2}\\right)x^{-2} = -\\frac{k-2}{2x^2} $$\n由于 $k > 2$，项 $k-2$ 是正数。对于任何 $x > 0$，$2x^2$ 也是正数。因此，对于支撑集中的所有 $x$，二阶导数都是负数。这证实了对数PDF是凹的，并且临界点 $x=k-2$ 是唯一的最大值点。众数是 $k-2$。\n\n情况2：$k \\le 2$。由于 $k$ 必须是正整数，这种情况包括 $k=1$ 和 $k=2$。\n如果 $k=2$，一阶导数变为：\n$$ \\frac{d}{dx} \\ln f(x; 2) = \\left(\\frac{2-2}{2}\\right)\\frac{1}{x} - \\frac{1}{2} = 0 - \\frac{1}{2} = -\\frac{1}{2} $$\n由于对于 $x>0$，导数恒为负，函数 $f(x; 2)$ 在其整个支撑集上是严格递减的。当 $x$ 趋近于支撑集的左边界，即 $x \\to 0^+$ 时，函数值趋近于最大值。因此，众数位于 $x=0$。注意，公式 $k-2$ 给出 $2-2=0$。\n\n如果 $k=1$，一阶导数变为：\n$$ \\frac{d}{dx} \\ln f(x; 1) = \\left(\\frac{1-2}{2}\\right)\\frac{1}{x} - \\frac{1}{2} = -\\frac{1}{2x} - \\frac{1}{2} $$\n对于 $x > 0$，$-\\frac{1}{2x}$ 和 $-\\frac{1}{2}$ 都是负数，因此导数是严格为负的。同样，函数 $f(x; 1)$ 是严格递减的。PDF $f(x;1) = (\\sqrt{2\\pi})^{-1} x^{-1/2} \\exp(-x/2)$ 在 $x=0$ 处有一条垂直渐近线。函数的上确界在 $x=0$ 处，因此众数是 $0$。在这种情况下，临界点公式 $k-2$ 给出 $1-2 = -1$，这在支撑集之外，证实了最大值不是一个内点。\n\n总结众数的结果：\n- 如果 $k > 2$，众数是 $k-2$。\n- 如果 $k \\in \\{1, 2\\}$，众数是 $0$。\n\n这可以表示为 $k$ 的单个函数：$\\text{众数} = \\max(0, k-2)$。\n\n问题接着要求在 $k \\ge 2$ 的情况下比较众数与均值，并根据偏度进行解释。\n均值为 $\\mu = k$。\n众数为 $\\text{众数} = \\max(0, k-2)$。\n\n对于 $k \\ge 2$：\n- 如果 $k=2$，均值 $= 2$，众数 $= \\max(0, 2-2) = 0$。此时，均值 $>$ 众数。\n- 如果 $k>2$，均值 $= k$，众数 $= \\max(0, k-2) = k-2$。显然，$k > k-2$，因此 均值 $>$ 众数。\n\n在所有 $k \\ge 2$ 的情况下，卡方分布的均值都严格大于其众数。\n\n这种 均值 $>$ 众数 的比较是右偏或正偏分布的一个典型特征。对于单峰分布，这个不等式表明大部分概率质量集中在较低的值处，而一个较长的尾部延伸到较高的值处。均值被尾部这些较大但出现频率较低的值向右（朝向较高的值）拉动。$\\chi^2_k$ 分布的偏度的正式度量是 $\\gamma_1 = \\frac{2\\sqrt{2}}{\\sqrt{k}}$，对于 $k>0$ 它始终为正，这证实了该分布总是右偏的。随着 $k$ 的增加，偏度减小，均值 ($k$) 和众数 ($k-2$) 变得更接近，反映了该分布向对称的正态分布收敛。\n\n所要求的最终答案是众数作为 $k$ 的函数的单个闭式表达式。",
            "answer": "$$ \\boxed{\\max(0, k-2)} $$"
        },
        {
            "introduction": "这个计算实践练习旨在连接抽象理论与实际应用，让你“看到”统计定理的运作。你将通过编程来模拟经典的皮尔逊卡方拟合优度检验，并经验性地验证其检验统计量的抽样分布确实收敛于理论卡方分布。这项动手实践为卡方检验的核心地位提供了强有力的证据，并展示了模拟在理解复杂统计概念中的威力。",
            "id": "2405617",
            "problem": "您将通过模拟研究皮尔逊卡方拟合优度检验统计量的渐近零分布，并量化其向理论卡方分布收敛的情况。考虑一个具有 $K$ 个类别和概率向量 $\\mathbf{p} = (p_{1},\\ldots,p_{K})$ 的分类模型，其中每个 $p_{i} \\in (0,1)$ 且 $\\sum_{i=1}^{K} p_{i} = 1$。对于一个大小为 $n$ 的样本，令 $\\mathbf{O} = (O_{1},\\ldots,O_{K})$ 表示类别计数，假设在原假设下服从多项分布，并令 $\\mathbf{E} = (E_{1},\\ldots,E_{K})$ 为期望计数，其中对于每个 $i \\in \\{1,\\ldots,K\\}$ 都有 $E_{i} = n p_{i}$。定义皮尔逊卡方统计量\n$$\nQ_{n} \\;=\\; \\sum_{i=1}^{K} \\frac{(O_{i}-E_{i})^{2}}{E_{i}}.\n$$\n在原假设下，当 $K$ 固定且所有 $p_{i} \\in (0,1)$ 时，已知当 $n \\to \\infty$ 时，$Q_{n}$ 的分布收敛到自由度为 $K-1$ 的卡方分布。\n\n您的任务是编写一个确定性的基于模拟的程序，该程序为每个指定的测试用例，使用蒙特卡洛复制来近似 $Q_{n}$ 的分布，然后报告 $Q_{n}$ 的经验分布与自由度为 $K-1$ 的理论卡方分布之间的柯尔莫哥洛夫-斯米尔诺夫距离。柯尔莫哥洛夫-斯米尔诺夫（KS）距离是两个累积分布函数（CDF）之间绝对差的上确界：\n$$\nD \\;=\\; \\sup_{x \\in \\mathbb{R}} \\left| \\widehat{F}_{Q_{n}}(x) \\;-\\; F_{\\chi^{2}_{K-1}}(x) \\right|.\n$$\n您的程序必须是确定性的，通过在每个测试用例中使用提供的种子来控制伪随机性。\n\n请使用以下测试套件。对于每个测试用例，给定 $(K,\\mathbf{p},n,R,s)$，其中 $K$ 是类别数，$\\mathbf{p}$ 是概率向量，$n$ 是样本量，$R$ 是蒙特卡洛复制次数，$s$ 是伪随机数生成器的种子。\n\n- 测试用例 $1$ (一般情况，中等 $n$):\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 50$\n  - $R = 20000$\n  - $s = 12345$\n- 测试用例 $2$ (相同模型，更大 $n$):\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 200$\n  - $R = 20000$\n  - $s = 12345$\n- 测试用例 $3$ (相同模型，大得多 $n$):\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 2000$\n  - $R = 20000$\n  - $s = 12345$\n- 测试用例 $4$ (边界情况，小 $n$ 且类别数相对于 $n$ 较多):\n  - $K = 8$\n  - $\\mathbf{p} = (1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8)$\n  - $n = 16$\n  - $R = 20000$\n  - $s = 67890$\n- 测试用例 $5$ (高维模型):\n  - $K = 20$\n  - $\\mathbf{p} = (1/20,\\,\\ldots,\\,1/20)$\n  - $n = 1000$\n  - $R = 15000$\n  - $s = 13579$\n\n对于每个测试用例，您的程序必须计算一个实数，该实数等于 $Q_{n}$ 的经验分布（基于 $R$ 次复制）与自由度为 $K-1$ 的卡方分布之间的柯尔莫哥洛夫-斯米尔诺夫距离 $D$。答案必须是精确到 $6$ 位小数的实数。\n\n最终输出格式：您的程序应生成一行，其中包含五个测试用例的结果，格式为方括号括起来的逗号分隔列表（例如，$[d_{1},d_{2},d_{3},d_{4},d_{5}]$），其中每个 $d_{j}$ 是测试用例 $j$ 的四舍五入后的柯尔莫哥洛夫-斯米尔诺夫距离。",
            "solution": "所提出的问题是计算统计学中一个明确定义的练习，并且被认为是有效的。它在科学上基于已建立的统计理论，即皮尔逊卡方检验及其渐近性质。每个测试用例的参数都是完整、一致的，并允许一个唯一的、可验证的解决方案。该任务要求对一个基本极限定理进行数值研究，这是量化领域中一个标准且有意义的程序。\n\n解决方案如下。我们处理量化皮尔逊卡方统计量 $Q_{n}$ 向其渐近 $\\chi^2$ 分布收敛的问题。\n\n首先，我们建立理论基础。在原假设下，从具有 $K$ 个类别和概率向量 $\\mathbf{p}=(p_1, \\ldots, p_K)$ 的分类分布中抽取的样本量为 $n$ 的观测计数 $\\mathbf{O} = (O_{1},\\ldots,O_{K})$ 服从多项分布，记作 $\\text{Multinomial}(n, \\mathbf{p})$。类别 $i$ 的期望计数是 $E_i=np_i$。皮尔逊卡方统计量由下式给出\n$$\nQ_{n} = \\sum_{i=1}^{K} \\frac{(O_{i}-E_{i})^{2}}{E_{i}}.\n$$\n统计学中的一个基本结果，即皮尔逊定理，指出当样本量 $n$ 趋于无穷大时，$Q_n$ 的分布收敛于自由度为 $K-1$ 的卡方分布，记作 $\\chi^2_{K-1}$。这种收敛是我们研究的主题。收敛速度取决于 $n$、$K$ 和具体的概率 $p_i$。当所有期望计数 $E_i$ 都足够大时（通常 $E_i \\ge 5$），该近似被认为是可靠的。\n\n为了对有限 $n$ 的情况下该近似的质量进行数值评估，我们采用蒙特卡洛模拟。对于由参数 $(K, \\mathbf{p}, n, R, s)$ 定义的每个测试用例，步骤如下：\n\n1.  **初始化**：我们将伪随机数生成器的种子固定为指定值 $s$。这确保了模拟是确定性的，其结果是完全可复现的。\n\n2.  **$Q_n$ 的模拟**：我们生成 $R$ 个统计量 $Q_n$ 的独立样本。这通过执行以下步骤的 $R$ 次复制来实现：\n    a. 从 $\\text{Multinomial}(n, \\mathbf{p})$ 分布中抽取一个观测计数向量 $\\mathbf{O} = (O_1, \\ldots, O_K)$。使用向量化实现，我们可以一次性生成所有 $R$ 个计数向量，得到一个 $R \\times K$ 的观测计数矩阵。\n    b. 对于 $R$ 个观测计数向量中的每一个，我们计算相应的 $Q_n$ 值。期望计数 $\\mathbf{E} = n\\mathbf{p}$ 在所有复制中都是恒定的。此计算也进行向量化，以高效地计算一个包含 $R$ 个 $Q_n$ 统计量值的数组。\n\n3.  **分布距离的量化**：生成 $R$ 个 $Q_n$ 样本后，我们得到一个经验累积分布函数（CDF），记作 $\\widehat{F}_{Q_n}(x)$。我们必须测量这个经验 CDF 与目标分布的理论 CDF $F_{\\chi^2_{K-1}}(x)$ 之间的“距离”。问题指定使用柯尔莫哥洛夫-斯米尔诺夫（KS）距离，定义为两个 CDF 之间绝对差的上确界：\n    $$\n    D = \\sup_{x \\in \\mathbb{R}} \\left| \\widehat{F}_{Q_{n}}(x) - F_{\\chi^{2}_{K-1}}(x) \\right|.\n    $$\n    $D$ 值越小，表示 $Q_n$ 的经验分布与理论 $\\chi^2_{K-1}$ 分布之间的拟合越好。此计算使用 `scipy.stats` 库中的 `kstest` 函数执行，该函数直接将生成的 $Q_n$ 值样本与自由度为 $K-1$ 的理论卡方分布进行比较。\n\n这整个过程应用于五个测试用例中的每一个。每个用例的最终输出是计算出的 KS 距离 $D$，四舍五入到 $6$ 位小数。测试用例的结果说明了渐近近似的性质：随着 $n$ 的增加，$D$ 距离预计会减小（用例 1-3），而当期望单元格计数较小时，近似效果预计会很差（用例 4）。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Computes the Kolmogorov-Smirnov distance between the empirical distribution\n    of the Pearson chi-square statistic and the theoretical chi-square\n    distribution for a suite of test cases.\n    \"\"\"\n\n    def compute_ks_distance(K, p_vec, n, R, seed):\n        \"\"\"\n        Runs a Monte Carlo simulation to compute the KS distance for one test case.\n\n        Args:\n            K (int): Number of categories.\n            p_vec (list or np.ndarray): Probability vector.\n            n (int): Sample size.\n            R (int): Number of Monte Carlo replications.\n            seed (int): Seed for the pseudorandom number generator.\n\n        Returns:\n            float: The computed KS distance, rounded to 6 decimal places.\n        \"\"\"\n        # 1. Initialize the pseudorandom number generator for deterministic results.\n        rng = np.random.default_rng(seed)\n\n        # 2. Define model parameters and calculate theoretical expected counts.\n        p_vec = np.array(p_vec)\n        expected_counts = n * p_vec\n        df = K - 1  # Degrees of freedom for the chi-square distribution.\n\n        # 3. Generate R sets of observed counts from the multinomial distribution.\n        # This is a vectorized operation, creating an (R, K) array.\n        observed_counts = rng.multinomial(n, p_vec, size=R)\n\n        # 4. Calculate the Pearson chi-square statistic for each of the R replicates.\n        # This calculation is also vectorized for efficiency.\n        # We sum over the K categories (axis=1).\n        q_n_samples = np.sum((observed_counts - expected_counts)**2 / expected_counts, axis=1)\n\n        # 5. Compute the Kolmogorov-Smirnov statistic.\n        # This compares the empirical distribution of the simulated Q_n values\n        # against the theoretical chi-square CDF with K-1 degrees of freedom.\n        ks_statistic, _ = kstest(q_n_samples, 'chi2', args=(df,), N=R)\n\n        # 6. Return the result rounded to the specified precision.\n        return round(ks_statistic, 6)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (K, p_vector, n, R, seed)\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 50, 20000, 12345),\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 200, 20000, 12345),\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 2000, 20000, 12345),\n        (8, [1/8] * 8, 16, 20000, 67890),\n        (20, [1/20] * 20, 1000, 15000, 13579)\n    ]\n\n    results = []\n    for case in test_cases:\n        K, p, n, R, s = case\n        result = compute_ks_distance(K, p, n, R, s)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}