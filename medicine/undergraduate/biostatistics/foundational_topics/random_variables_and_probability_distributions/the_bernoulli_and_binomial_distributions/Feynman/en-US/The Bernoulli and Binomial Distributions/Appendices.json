{
    "hands_on_practices": [
        {
            "introduction": "The Wald confidence interval is often the first method students learn for estimating a population proportion, thanks to its simplicity and direct connection to the Central Limit Theorem. However, it is crucial for a practitioner to understand not just how to construct an interval, but also how it performs in practice. This exercise () provides a hands-on opportunity to investigate the Wald interval's limitations, particularly its tendency to have an *actual* coverage probability that is lower than its *nominal* level in common scenarios.",
            "id": "4957599",
            "problem": "Consider a sequence of $n$ independent and identically distributed Bernoulli random variables, each with success probability $p \\in (0,1)$. Let $K$ denote the total number of successes in the $n$ trials, so $K$ follows a binomial distribution with parameters $n$ and $p$, written $K \\sim \\mathrm{Binomial}(n,p)$. The sample proportion is $\\hat{p} = K/n$. Using the Central Limit Theorem (CLT), the distribution of a suitable standardization of $\\hat{p}$ can be approximated by the standard Gaussian distribution, and this approximation can be used to construct a two-sided confidence interval for $p$ with nominal confidence level $1 - \\alpha$. The Wald interval is obtained by substituting the observed sample proportion $\\hat{p}$ into the estimated standard error when forming this normal-approximation-based interval.\n\nYour tasks are:\n\n- Construct the Wald interval for $p$ at nominal confidence level $1 - \\alpha$ based on an observed binomial count $K$. Your construction must start from the CLT-based reasoning: treat the standardized $\\hat{p}$ as approximately standard normal, and derive a symmetric two-sided interval that has nominal coverage $1 - \\alpha$ under the normal approximation. Then, substitute the estimated standard error that uses $\\hat{p}$ in place of the unknown $p$.\n- Analyze the actual (finite-sample) coverage of the Wald interval as a function of the true parameter $p_0 \\in (0,1)$ by computing the exact coverage probability under the binomial model. Define the coverage probability as the probability, under $K \\sim \\mathrm{Binomial}(n,p_0)$, that the constructed interval based on $K$ contains the true parameter $p_0$.\n- Identify and quantify failure modes, especially when $n p_0 (1-p_0)$ is small or when $K \\in \\{0,n\\}$, by computing the probability of these degenerate outcomes under the binomial model.\n\nFormally, for given $(n, p_0, \\alpha)$, define the Wald interval $I(K)$ constructed from $K$, and compute:\n1. The actual coverage probability\n$$\n\\mathrm{Cov}(n,p_0,\\alpha) \\;=\\; \\sum_{k=0}^{n} \\mathbf{1}\\{p_0 \\in I(k)\\} \\, \\Pr(K=k \\mid n,p_0),\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function and $\\Pr(K=k \\mid n,p_0)$ is the binomial probability mass function.\n2. The nominal confidence level $1 - \\alpha$.\n3. The probability of the degenerate outcomes $K \\in \\{0,n\\}$,\n$$\n\\mathrm{Deg}(n,p_0) \\;=\\; \\Pr(K=0 \\mid n,p_0) + \\Pr(K=n \\mid n,p_0).\n$$\n\nAll probabilities must be expressed as decimals in $[0,1]$ and not as percentages. No physical units or angle units are involved.\n\nImplement a program that, for each test case in the following test suite, computes and returns the triple $[\\mathrm{Cov}(n,p_0,\\alpha), \\, 1-\\alpha, \\, \\mathrm{Deg}(n,p_0)]$:\n\n- Case 1 (happy path): $n = 50$, $p_0 = 0.5$, $\\alpha = 0.05$.\n- Case 2 (near-boundary, small variance): $n = 20$, $p_0 = 0.05$, $\\alpha = 0.05$.\n- Case 3 (near-boundary, symmetric to Case 2): $n = 20$, $p_0 = 0.95$, $\\alpha = 0.05$.\n- Case 4 (small sample, near-boundary): $n = 5$, $p_0 = 0.10$, $\\alpha = 0.05$.\n- Case 5 (moderate sample, very small variance): $n = 100$, $p_0 = 0.02$, $\\alpha = 0.05$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of the five triples, each triple enclosed in square brackets, and the whole list enclosed in square brackets. For example, an output line with two cases would look like $[[a_1,b_1,c_1],[a_2,b_2,c_2]]$, where $a_i$, $b_i$, $c_i$ are decimals. Your program must hard-code the above test suite and produce the corresponding single-line output on execution.",
            "solution": "The user problem is valid as it is scientifically grounded in statistical theory, well-posed with all necessary information, and objective in its formulation. We will proceed with the three specified tasks: deriving the Wald confidence interval, analyzing its performance, and implementing the calculations.\n\n### 1. Construction of the Wald Confidence Interval\n\nLet $K$ be the number of successes in $n$ independent Bernoulli trials, with success probability $p$. The random variable $K$ follows a binomial distribution, $K \\sim \\mathrm{Binomial}(n,p)$. The sample proportion, $\\hat{p} = K/n$, is an unbiased estimator of $p$. Its expectation is $E[\\hat{p}] = p$, and its variance is $\\mathrm{Var}(\\hat{p}) = p(1-p)/n$.\n\nThe Central Limit Theorem (CLT) states that for a sufficiently large sample size $n$, the distribution of the standardized sample proportion converges to a standard normal distribution:\n$$\nZ = \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/n}} \\xrightarrow{d} N(0,1)\n$$\nwhere $N(0,1)$ is the standard normal distribution. Based on this, an approximate $100(1-\\alpha)\\%$ two-sided confidence interval for $p$ is constructed from the pivotal quantity $Z$. Let $z_{\\alpha/2}$ be the critical value from the standard normal distribution such that $\\Pr(Z > z_{\\alpha/2}) = \\alpha/2$. Then, we have:\n$$\n\\Pr(-z_{\\alpha/2} \\le Z \\le z_{\\alpha/2}) \\approx 1 - \\alpha\n$$\nSubstituting the expression for $Z$:\n$$\n\\Pr\\left(-z_{\\alpha/2} \\le \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/n}} \\le z_{\\alpha/2}\\right) \\approx 1 - \\alpha\n$$\nSolving the inequality for $p$ is algebraically complex because $p$ appears in both the numerator and the denominator (as part of the standard error). The Wald interval simplifies this by replacing the true standard error, $\\mathrm{SE}(p) = \\sqrt{p(1-p)/n}$, with its estimate, $\\widehat{\\mathrm{SE}} = \\mathrm{SE}(\\hat{p}) = \\sqrt{\\hat{p}(1-\\hat{p})/n}$. This yields a new approximate pivotal quantity:\n$$\nZ' = \\frac{\\hat{p} - p}{\\sqrt{\\hat{p}(1-\\hat{p})/n}}\n$$\nWe assume $Z'$ also follows an approximate standard normal distribution. The confidence interval is then derived by isolating $p$ in the inequality $-z_{\\alpha/2} \\le Z' \\le z_{\\alpha/2}$:\n$$\n-z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\le \\hat{p} - p \\le z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n$$\n$$\n\\hat{p} - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\le p \\le \\hat{p} + z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n$$\nThus, the Wald interval for $p$, denoted $I(K)$, is given by:\n$$\nI(K) = \\left[ \\hat{p} - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\quad \\hat{p} + z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right]\n$$\nwhere $\\hat{p} = K/n$ and $z_{\\alpha/2}$ is the quantile function of the standard normal distribution evaluated at $1 - \\alpha/2$.\n\n### 2. Analysis of Actual Coverage and Failure Modes\n\nThe nominal confidence level of the interval is $1 - \\alpha$. However, due to the approximation, the actual coverage probability for a finite sample size $n$ may differ. The actual coverage probability for a true parameter value $p_0$ is the probability that the random interval $I(K)$ contains $p_0$. This is calculated by summing the probabilities of all outcomes $k \\in \\{0, 1, \\dots, n\\}$ for which the constructed interval $I(k)$ covers $p_0$:\n$$\n\\mathrm{Cov}(n,p_0,\\alpha) = \\sum_{k=0}^{n} \\mathbf{1}\\{p_0 \\in I(k)\\} \\, \\Pr(K=k \\mid n,p_0)\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function and $\\Pr(K=k \\mid n,p_0)$ is the binomial probability mass function:\n$$\n\\Pr(K=k \\mid n,p_0) = \\binom{n}{k} p_0^k (1-p_0)^{n-k}\n$$\nA significant failure mode of the Wald interval occurs at the boundaries of the sample space, i.e., when $K=0$ or $K=n$.\n- If $K=0$, then $\\hat{p}=0/n=0$. The estimated standard error $\\widehat{\\mathrm{SE}} = \\sqrt{0(1-0)/n} = 0$. The resulting interval is $I(0) = [0-0, 0+0] = [0,0]$, a point interval.\n- If $K=n$, then $\\hat{p}=n/n=1$. The estimated standard error $\\widehat{\\mathrm{SE}} = \\sqrt{1(1-1)/n} = 0$. The resulting interval is $I(n) = [1-0, 1+0] = [1,1]$, also a point interval.\n\nSince the problem statement specifies that the true parameter $p_0 \\in (0,1)$, these degenerate, zero-width intervals at $[0,0]$ and $[1,1]$ will never contain $p_0$. Consequently, whenever an experiment yields $K=0$ or $K=n$, the Wald interval fails to cover the true parameter. The total probability of these failures is:\n$$\n\\mathrm{Deg}(n,p_0) = \\Pr(K=0 \\mid n,p_0) + \\Pr(K=n \\mid n,p_0) = (1-p_0)^n + p_0^n\n$$\nThis probability can be substantial when $n$ is small or $p_0$ is close to $0$ or $1$, leading to significant under-coverage compared to the nominal level $1-\\alpha$.\n\n### 3. Computational Procedure\n\nFor each test case given by the tuple $(n, p_0, \\alpha)$, we compute the following three values:\n1.  **Actual Coverage Probability $\\mathrm{Cov}(n,p_0,\\alpha)$**: We iterate through each possible outcome $k$ from $0$ to $n$. For each $k$, we compute $\\hat{p}=k/n$ and construct the Wald interval $I(k)$. We check if $p_0 \\in I(k)$. If it is, we add the binomial probability $\\Pr(K=k | n, p_0)$ to a running total. The final sum is the actual coverage.\n2.  **Nominal Confidence Level**: This is simply $1-\\alpha$.\n3.  **Probability of Degenerate Outcomes $\\mathrm{Deg}(n,p_0)$**: This is the sum of the binomial probabilities for $k=0$ and $k=n$.\n\nThe critical value $z_{\\alpha/2}$ is obtained from the inverse cumulative distribution function (or quantile function) of the standard normal distribution, specifically $z_{\\alpha/2} = \\Phi^{-1}(1 - \\alpha/2)$. The binomial probabilities are computed using the probability mass function. The implementation will use functions from the `scipy.stats` library for these calculations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import binom, norm\n\ndef solve():\n    \"\"\"\n    Computes the actual coverage, nominal confidence, and degenerate outcome probability\n    for the Wald confidence interval for a binomial proportion for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, p_0, alpha)\n        (50, 0.5, 0.05),  # Case 1 (happy path)\n        (20, 0.05, 0.05), # Case 2 (near-boundary, small variance)\n        (20, 0.95, 0.05), # Case 3 (near-boundary, symmetric to Case 2)\n        (5, 0.10, 0.05),  # Case 4 (small sample, near-boundary)\n        (100, 0.02, 0.05) # Case 5 (moderate sample, very small variance)\n    ]\n\n    results = []\n    for n, p_0, alpha in test_cases:\n        # Array of all possible success counts\n        k_values = np.arange(0, n + 1)\n\n        # 1. Calculate the binomial probability mass function for each k\n        # Pr(K=k | n, p_0)\n        pmf_values = binom.pmf(k_values, n, p_0)\n\n        # 2. Construct the Wald interval for each k\n        p_hat_values = k_values / n\n        \n        # Calculate critical value z_{alpha/2}\n        z_alpha_2 = norm.ppf(1 - alpha / 2.0)\n        \n        # Calculate estimated standard error for each k.\n        # Use np.errstate to handle warnings for k=0 and k=n where p_hat*(1-p_hat) is 0.\n        with np.errstate(invalid='ignore'):\n            se_hat = np.sqrt(p_hat_values * (1 - p_hat_values) / n)\n        # For k=0 or k=n, p_hat*(1-p_hat) is 0, so se_hat is 0. This is the correct behavior.\n        se_hat = np.nan_to_num(se_hat) # Replace potential NaN with 0\n\n        # Calculate margin of error\n        margin_of_error = z_alpha_2 * se_hat\n\n        # Calculate interval bounds\n        lower_bounds = p_hat_values - margin_of_error\n        upper_bounds = p_hat_values + margin_of_error\n        \n        # 3. Compute the actual coverage probability\n        # Check for which k values the interval contains the true p_0\n        # A confidence interval is a closed set, so we use <=\n        is_covered = (lower_bounds <= p_0) & (p_0 <= upper_bounds)\n        \n        # Sum the probabilities of the outcomes k for which the interval covers p_0\n        actual_coverage = np.sum(pmf_values[is_covered])\n        \n        # 4. Compute the nominal confidence level\n        nominal_level = 1.0 - alpha\n        \n        # 5. Compute the probability of degenerate outcomes (k=0 or k=n)\n        prob_degenerate = pmf_values[0] + pmf_values[n]\n        \n        results.append([actual_coverage, nominal_level, prob_degenerate])\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) will call str() on each sublist, which is exactly the format needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Given the known performance issues of the Wald interval, statisticians have developed more reliable alternatives. This practice () delves into two of the most important: the Wilson score interval, derived by inverting the score test, and the \"exact\" Clopper-Pearson interval. By implementing and comparing these three methods, you will gain a deeper appreciation for the trade-offs between them in terms of coverage accuracy and interval length, empowering you to choose the appropriate tool for a given analytical task.",
            "id": "4957555",
            "problem": "A sequence of $n$ independent and identically distributed Bernoulli trials with success probability $p$ yields a total of $k$ successes, where $k \\in \\{0,1,\\dots,n\\}$. Let $\\ell(p)$ denote the binomial log-likelihood, $U(p)$ the score function, and $I(p)$ the Fisher information. Starting only from the following foundational elements:\n- The definition of the Bernoulli and binomial models.\n- The binomial log-likelihood and its derivatives.\n- The large-sample normal approximation for the score test statistic under a simple null hypothesis.\nyou must derive an algorithm that constructs the Wilson score interval for $p$ by inverting the two-sided score test of level $\\alpha$. Specifically, construct the acceptance region for the score test of $H_0: p = p_0$ at level $\\alpha$ and mathematically invert it to obtain an interval in $p$ that depends on $n$ and the observed $k$.\n\nThen implement three interval estimators for $p$ given $n$ and $k$:\n1. The Wald interval based on the maximum likelihood estimate (MLE) $\\hat{p}$ and the large-sample normal approximation.\n2. The Wilson score interval obtained by inverting the score test as derived above.\n3. The Clopper–Pearson interval obtained by inverting the exact binomial cumulative distribution function (CDF).\n\nFor numerical validity, for any interval estimator with endpoints outside the unit interval, truncate endpoints to the unit interval $[0,1]$. For the Clopper–Pearson interval, handle the extreme outcomes $k=0$ and $k=n$ in a way consistent with the exact binomial inversion so that admissible endpoints are always within $[0,1]$.\n\nDefine the coverage probability of an interval procedure for a fixed true $p$ and sample size $n$ as the probability (with respect to the binomial sampling distribution for $K \\sim \\text{Binomial}(n,p)$) that the random interval contains the true parameter $p$:\n$$\n\\text{coverage}(n,p,\\alpha) \\;=\\; \\sum_{k=0}^{n} \\mathbb{I}\\{L(n,k,\\alpha) \\le p \\le U(n,k,\\alpha)\\} \\cdot \\binom{n}{k} p^k (1-p)^{n-k},\n$$\nwhere $[L(n,k,\\alpha),U(n,k,\\alpha)]$ is the interval produced by the method for the realized $k$, and $\\mathbb{I}\\{\\cdot\\}$ is the indicator function. Define the expected length of an interval method at $(n,p,\\alpha)$ as\n$$\n\\text{Elen}(n,p,\\alpha) \\;=\\; \\sum_{k=0}^{n} \\left(U(n,k,\\alpha) - L(n,k,\\alpha)\\right) \\cdot \\binom{n}{k} p^k (1-p)^{n-k}.\n$$\n\nYour program must, for each test case $(n,p,\\alpha)$, compute two floats for each method: the coverage probability and the expected length, using exact summation over $k \\in \\{0,\\dots,n\\}$ and the binomial probability mass function. No Monte Carlo simulation is permitted.\n\nTest Suite:\n- Case 1: $n=10$, $p=0.2$, $\\alpha=0.05$.\n- Case 2: $n=40$, $p=0.5$, $\\alpha=0.05$.\n- Case 3: $n=5$, $p=0.05$, $\\alpha=0.05$.\n- Case 4: $n=30$, $p=0.9$, $\\alpha=0.05$.\n- Case 5: $n=2$, $p=0.5$, $\\alpha=0.10$.\n- Case 6: $n=100$, $p=0.01$, $\\alpha=0.05$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one sub-list per test case. For each test case, output a list of six numbers in the order\n$[\\text{Wald coverage}, \\text{Wald expected length}, \\text{Wilson coverage}, \\text{Wilson expected length}, \\text{Clopper–Pearson coverage}, \\text{Clopper–Pearson expected length}]$.\nThe final line must look like\n$$\n[\\,[w_1,w_2,w_3,w_4,w_5,w_6],\\,[\\dots],\\dots\\,]\n$$\nwith all numeric quantities expressed as decimals (no percentage signs), and the entire output produced on a single line with no additional text.",
            "solution": "The problem requires the derivation and implementation of three different confidence interval estimators for a binomial proportion $p$, and their subsequent evaluation based on coverage probability and expected length. We will first provide the theoretical background and derivation for each method, as stipulated.\n\n### 1. Derivation and Formulation of Confidence Intervals\n\nLet $K$ be the number of successes in a sequence of $n$ independent and identically distributed Bernoulli trials with success probability $p$. The random variable $K$ follows a binomial distribution, $K \\sim \\text{Binomial}(n,p)$, with probability mass function (PMF) $f(k;p) = \\binom{n}{k} p^k (1-p)^{n-k}$ for $k \\in \\{0, 1, \\dots, n\\}$. Given an observed count of successes $k$, we aim to construct a $100(1-\\alpha)\\%$ confidence interval for $p$.\n\n#### 1.1. Wilson Score Interval\n\nThis interval is derived by inverting the score test for the null hypothesis $H_0: p = p_0$. The derivation proceeds from the binomial log-likelihood function.\n\nThe likelihood for an observed $k$ is $L(p) = \\binom{n}{k} p^k (1-p)^{n-k}$. The log-likelihood is:\n$$ \\ell(p) = \\log L(p) = \\log\\binom{n}{k} + k \\log(p) + (n-k) \\log(1-p) $$\nThe score function, $U(p)$, is the first derivative of the log-likelihood with respect to $p$:\n$$ U(p) = \\frac{d\\ell(p)}{dp} = \\frac{k}{p} - \\frac{n-k}{1-p} = \\frac{k - np}{p(1-p)} $$\nThe Fisher information, $I(p)$, is the negative expectation of the second derivative of the log-likelihood. The second derivative is:\n$$ \\frac{d^2\\ell(p)}{dp^2} = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2} $$\nTaking the negative expectation, with $E[K] = np$, we get:\n$$ I(p) = -E\\left[\\frac{d^2\\ell(p)}{dp^2}\\right] = E\\left[\\frac{K}{p^2} + \\frac{n-K}{(1-p)^2}\\right] = \\frac{np}{p^2} + \\frac{n-np}{(1-p)^2} = \\frac{n}{p} + \\frac{n}{1-p} = \\frac{n}{p(1-p)} $$\nThe score test statistic for the null hypothesis $H_0: p = p_0$ is given by $Z = U(p_0) / \\sqrt{I(p_0)}$. For large $n$, $Z$ is approximately standard normal, $Z \\sim N(0,1)$.\n$$ Z = \\frac{ (k - np_0) / (p_0(1-p_0)) }{ \\sqrt{n / (p_0(1-p_0))} } = \\frac{k - np_0}{\\sqrt{n p_0 (1-p_0)}} $$\nLet $\\hat{p} = k/n$ be the maximum likelihood estimate (MLE). The statistic can be written as $Z = (\\hat{p}-p_0)/\\sqrt{p_0(1-p_0)/n}$. The two-sided score test at significance level $\\alpha$ does not reject $H_0$ if $|Z| \\le z_{\\alpha/2}$, where $z_{\\alpha/2}$ is the upper $\\alpha/2$ quantile of the standard normal distribution. The acceptance region for $p_0$ is defined by the inequality:\n$$ \\frac{(\\hat{p} - p_0)^2}{p_0(1-p_0)/n} \\le z_{\\alpha/2}^2 $$\nTo obtain the confidence interval for $p$, we solve this inequality for $p_0$, which we now treat as the variable $p$:\n$$ n(\\hat{p}-p)^2 \\le z_{\\alpha/2}^2 p(1-p) $$\n$$ n(\\hat{p}^2 - 2\\hat{p}p + p^2) \\le z_{\\alpha/2}^2(p-p^2) $$\nRearranging the terms yields a quadratic inequality in $p$:\n$$ (n + z_{\\alpha/2}^2)p^2 - (2n\\hat{p} + z_{\\alpha/2}^2)p + n\\hat{p}^2 \\le 0 $$\nThe endpoints of the confidence interval are the roots of the corresponding quadratic equation $Ap^2 + Bp + C = 0$, where $A = n+z_{\\alpha/2}^2$, $B = -(2n\\hat{p} + z_{\\alpha/2}^2)$, and $C = n\\hat{p}^2$. Using the quadratic formula $p = (-B \\pm \\sqrt{B^2-4AC})/(2A)$, the endpoints are:\n$$ p = \\frac{2n\\hat{p} + z_{\\alpha/2}^2 \\pm \\sqrt{(2n\\hat{p} + z_{\\alpha/2}^2)^2 - 4(n+z_{\\alpha/2}^2)(n\\hat{p}^2)}}{2(n+z_{\\alpha/2}^2)} $$\nThe discriminant simplifies to $B^2 - 4AC = z_{\\alpha/2}^2(4n\\hat{p}(1-\\hat{p}) + z_{\\alpha/2}^2)$. Substituting $\\hat{p}=k/n$:\n$$ p = \\frac{2k + z_{\\alpha/2}^2 \\pm z_{\\alpha/2}\\sqrt{4k(1-k/n) + z_{\\alpha/2}^2}}{2(n+z_{\\alpha/2}^2)} $$\nThis formula defines the lower and upper bounds of the Wilson score interval, $[L_W(k,n,\\alpha), U_W(k,n,\\alpha)]$. The resulting interval is contained within $[0,1]$ by construction.\n\n#### 1.2. Wald Interval\nThe Wald interval is based on the large-sample normal approximation of the distribution of the MLE, $\\hat{p}=k/n$. The variance of $\\hat{p}$ is $p(1-p)/n$, which is estimated by plugging in the MLE, yielding the estimated standard error $\\text{SE}(\\hat{p}) = \\sqrt{\\hat{p}(1-\\hat{p})/n}$.\nThe Wald interval is centered at $\\hat{p}$ with a margin of error determined by $z_{\\alpha/2}$:\n$$ \\left[ \\hat{p} - z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\quad \\hat{p} + z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right] $$\nThis interval performs poorly for small $n$ and for $p$ near $0$ or $1$. When $k=0$ or $k=n$, $\\hat{p}$ is $0$ or $1$ respectively, causing the standard error to become $0$ and the interval to collapse to a single point. Per the problem statement, endpoints are truncated to the interval $[0,1]$.\n\n#### 1.3. Clopper–Pearson Interval\nThe Clopper–Pearson interval is an \"exact\" method based on inverting two one-sided binomial tests, ensuring the coverage probability is at least $1-\\alpha$ for all $p$. The endpoints $[p_L, p_U]$ are defined by:\n1. The lower bound $p_L$ is the solution to $P(K \\ge k | p=p_L) = \\sum_{i=k}^{n} \\binom{n}{i}p_L^i(1-p_L)^{n-i} = \\alpha/2$.\n2. The upper bound $p_U$ is the solution to $P(K \\le k | p=p_U) = \\sum_{i=0}^{k} \\binom{n}{i}p_U^i(1-p_U)^{n-i} = \\alpha/2$.\n\nThese equations can be solved using the relationship between the binomial CDF and the regularized incomplete Beta function. The solutions are given by quantiles of the Beta distribution:\n- $p_L = \\text{BetaInv}(\\alpha/2; k, n-k+1)$\n- $p_U = \\text{BetaInv}(1-\\alpha/2; k+1, n-k)$\nwhere $\\text{BetaInv}(q; a, b)$ is the $q$-th quantile of a Beta distribution with shape parameters $a$ and $b$.\nFor the extreme cases:\n- If $k=0$, the lower bound equation is $P(K \\ge 0 | p_L) = 1 = \\alpha/2$, which has no solution. The lower bound is taken to be $p_L=0$.\n- If $k=n$, the upper bound equation is $P(K \\le n | p_U) = 1 = \\alpha/2$, again with no solution. The upper bound is taken to be $p_U=1$.\nThese conventions are consistent with the limits of the Beta distribution quantile function.\n\n### 2. Evaluation Metrics and Algorithm\n\nThe performance of these interval estimators is evaluated using two metrics, calculated by summing over all possible outcomes $k \\in \\{0, 1, \\dots, n\\}$ weighted by their binomial probabilities.\n\n- **Coverage Probability**: The probability that the random interval $[L(K), U(K)]$ contains the true parameter $p$.\n$$ \\text{coverage}(n,p,\\alpha) = \\sum_{k=0}^{n} \\mathbb{I}\\{L(n,k,\\alpha) \\le p \\le U(n,k,\\alpha)\\} \\cdot \\binom{n}{k} p^k (1-p)^{n-k} $$\n- **Expected Length**: The average length of the interval.\n$$ \\text{Elen}(n,p,\\alpha) = \\sum_{k=0}^{n} \\left(U(n,k,\\alpha) - L(n,k,\\alpha)\\right) \\cdot \\binom{n}{k} p^k (1-p)^{n-k} $$\nHere, $\\mathbb{I}\\{\\cdot\\}$ is the indicator function.\n\nThe algorithm to compute these metrics for a given $(n, p, \\alpha)$ is as follows:\n1. Initialize six accumulators to $0$: one for coverage and one for length for each of the three methods (Wald, Wilson, Clopper-Pearson).\n2. Calculate the standard normal quantile $z_{\\alpha/2}$.\n3. For each integer $k$ from $0$ to $n$:\n   a. Calculate the binomial probability $P(K=k) = \\binom{n}{k} p^k (1-p)^{n-k}$.\n   b. For each of the three methods, compute the confidence interval $[L, U]$ corresponding to the observed $k$.\n   c. For each method, if the true parameter $p$ lies within its computed interval $[L, U]$, add the probability from step 3a to the method's coverage accumulator.\n   d. For each method, add the product of the interval length $(U-L)$ and the probability from step 3a to the method's expected length accumulator.\n4. After iterating through all $k$, the accumulators will hold the final values for the coverage probability and expected length for each method. The implementation follows this exact summation procedure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, binom, beta\n\ndef wald_interval(k: int, n: int, z_alpha_2: float) -> tuple[float, float]:\n    \"\"\"Calculates the Wald confidence interval for a binomial proportion.\"\"\"\n    if n == 0:\n        return 0.0, 1.0 # Undefined, return full range\n    \n    p_hat = k / n\n    \n    # For k=0 or k=n, the standard error is 0, leading to a zero-width interval.\n    if p_hat == 0 or p_hat == 1:\n        return p_hat, p_hat\n        \n    se = np.sqrt(p_hat * (1 - p_hat) / n)\n    margin = z_alpha_2 * se\n    lower = p_hat - margin\n    upper = p_hat + margin\n    \n    # Truncate interval to [0, 1] as required.\n    return max(0.0, lower), min(1.0, upper)\n\ndef wilson_interval(k: int, n: int, z_alpha_2: float) -> tuple[float, float]:\n    \"\"\"Calculates the Wilson score confidence interval for a binomial proportion.\"\"\"\n    if n == 0:\n        return 0.0, 1.0\n        \n    z2 = z_alpha_2**2\n    \n    # Use the derived formula which is more numerically stable\n    numerator_center = 2 * k + z2\n    denominator = 2 * (n + z2)\n    \n    sqrt_full = z_alpha_2 * np.sqrt(4*k*(1-k/n) + z2)\n    \n    lower = (numerator_center - sqrt_full) / denominator\n    upper = (numerator_center + sqrt_full) / denominator\n    \n    return lower, upper\n\ndef clopper_pearson_interval(k: int, n: int, alpha: float) -> tuple[float, float]:\n    \"\"\"Calculates the Clopper-Pearson exact confidence interval.\"\"\"\n    if n == 0:\n        return 0.0, 1.0\n        \n    alpha_2 = alpha / 2.0\n    \n    # Lower bound: Handle k=0 case explicitly as beta.ppf requires shape > 0.\n    if k == 0:\n        lower = 0.0\n    else:\n        lower = beta.ppf(alpha_2, k, n - k + 1)\n        \n    # Upper bound: Handle k=n case explicitly.\n    if k == n:\n        upper = 1.0\n    else:\n        upper = beta.ppf(1 - alpha_2, k + 1, n - k)\n        \n    return lower, upper\n\ndef calculate_metrics(n: int, p_true: float, alpha: float) -> list[float]:\n    \"\"\"\n    Calculates coverage probability and expected length for the three interval types\n    by summing over all possible outcomes k.\n    \"\"\"\n    z_alpha_2 = norm.ppf(1 - alpha / 2.0)\n    \n    k_values = np.arange(0, n + 1)\n    pmf_values = binom.pmf(k_values, n, p_true)\n    \n    # Initialize metrics accumulators\n    metrics = {\n        'wald': {'coverage': 0.0, 'length': 0.0},\n        'wilson': {'coverage': 0.0, 'length': 0.0},\n        'cp': {'coverage': 0.0, 'length': 0.0},\n    }\n    \n    for k, prob in zip(k_values, pmf_values):\n        # Wald interval\n        lw, uw = wald_interval(k, n, z_alpha_2)\n        if lw <= p_true <= uw:\n            metrics['wald']['coverage'] += prob\n        metrics['wald']['length'] += (uw - lw) * prob\n        \n        # Wilson score interval\n        lwi, uwi = wilson_interval(k, n, z_alpha_2)\n        if lwi <= p_true <= uwi:\n            metrics['wilson']['coverage'] += prob\n        metrics['wilson']['length'] += (uwi - lwi) * prob\n        \n        # Clopper-Pearson interval\n        lcp, ucp = clopper_pearson_interval(k, n, alpha)\n        if lcp <= p_true <= ucp:\n            metrics['cp']['coverage'] += prob\n        metrics['cp']['length'] += (ucp - lcp) * prob\n        \n    return [\n        metrics['wald']['coverage'], metrics['wald']['length'],\n        metrics['wilson']['coverage'], metrics['wilson']['length'],\n        metrics['cp']['coverage'], metrics['cp']['length'],\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results in the specified format.\n    \"\"\"\n    test_cases = [\n        (10, 0.2, 0.05),\n        (40, 0.5, 0.05),\n        (5, 0.05, 0.05),\n        (30, 0.9, 0.05),\n        (2, 0.5, 0.10),\n        (100, 0.01, 0.05),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n, p, alpha = case\n        case_results = calculate_metrics(n, p, alpha)\n        all_results.append(case_results)\n\n    # Format the final output string as per the problem specification.\n    # It must be a single line: [[res1_1,...,res1_6],[res2_1,...,res2_6],...]\n    formatted_results = []\n    for res in all_results:\n        # Format each number to a reasonable number of decimal places for consistency\n        formatted_res_list = [f\"{x:.10f}\".rstrip('0').rstrip('.') for x in res]\n        formatted_res = f\"[{','.join(formatted_res_list)}]\"\n        formatted_results.append(formatted_res)\n    \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Confidence intervals are not just for post-hoc analysis; they are a cornerstone of effective study design. Before collecting any data, a researcher must determine the sample size needed to achieve a desired level of precision. This exercise () guides you through the derivation of a sample size formula by controlling the margin of error for a proportion's confidence interval, connecting statistical theory directly to the practical planning of a scientific investigation.",
            "id": "4957558",
            "problem": "In a cross-sectional biostatistics study, you will estimate the prevalence $p$ of a binary biomarker using independent and identically distributed Bernoulli observations. You plan to report a two-sided confidence interval for $p$ with nominal confidence level $1-\\alpha$ under a normal approximation justified by the Central Limit Theorem (CLT). Starting from the definitions of a Bernoulli random variable and a binomial sample proportion, and using the fact that for large $n$ the standardized sample proportion is approximately standard normal, derive a design expression for the required sample size $n$ that ensures the absolute margin of error does not exceed a prespecified tolerance $\\epsilon$ with confidence $1-\\alpha$. Then, identify the choice of $p$ that yields a conservative (worst-case) design and use it to compute the minimal integer $n$ ensuring the margin of error is at most $\\epsilon$ for any true $p \\in [0,1]$.\n\nAssume $\\alpha = 0.05$ and $\\epsilon = 0.04$, and take the standard normal upper quantile $z_{1-\\alpha/2}$ corresponding to the confidence level $1-\\alpha$ as the usual value used in practice. Report the minimal integer $n$ that meets the requirement. Do not include units. No rounding by significant figures is required; report the exact minimal integer that satisfies the design criterion.",
            "solution": "The user has provided a valid problem statement. The problem is scientifically grounded in standard statistical theory, is well-posed, and is stated using objective and precise language. All necessary information is provided.\n\nThe problem asks for the derivation of a sample size formula for estimating a population proportion $p$ and its application to a specific case. We begin by defining the statistical framework.\n\nLet $X_1, X_2, \\dots, X_n$ be a set of $n$ independent and identically distributed (i.i.d.) random variables from a Bernoulli distribution with parameter $p$, denoted as $X_i \\sim \\text{Bernoulli}(p)$. The parameter $p$ represents the true prevalence of the biomarker in the population. For each observation $i$, $X_i=1$ if the biomarker is present (with probability $p$) and $X_i=0$ if it is absent (with probability $1-p$). The expected value of each observation is $E[X_i] = p$ and the variance is $\\text{Var}(X_i) = p(1-p)$.\n\nThe sample proportion, $\\hat{p}$, is the point estimator for $p$ and is defined as the mean of the observations:\n$$ \\hat{p} = \\frac{1}{n} \\sum_{i=1}^{n} X_i $$\nAs an estimator, $\\hat{p}$ has an expected value $E[\\hat{p}] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right] = \\frac{1}{n}\\sum_{i=1}^{n} E[X_i] = \\frac{1}{n}(np) = p$, making it an unbiased estimator of $p$. The variance of $\\hat{p}$ is given by $\\text{Var}(\\hat{p}) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n} \\text{Var}(X_i) = \\frac{1}{n^2}(np(1-p)) = \\frac{p(1-p)}{n}$, due to the independence of the observations.\n\nBy the Central Limit Theorem (CLT), for a sufficiently large sample size $n$, the sampling distribution of $\\hat{p}$ is approximately normal:\n$$ \\hat{p} \\approx N\\left(p, \\frac{p(1-p)}{n}\\right) $$\nStandardizing the random variable $\\hat{p}$ yields a variable that is approximately standard normal:\n$$ Z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\approx N(0,1) $$\nA two-sided confidence interval for $p$ with a confidence level of $1-\\alpha$ is constructed based on the pivotal quantity $Z$. We seek to find the bounds that satisfy:\n$$ P\\left(-z_{1-\\alpha/2} \\le \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\le z_{1-\\alpha/2}\\right) \\approx 1-\\alpha $$\nwhere $z_{1-\\alpha/2}$ is the upper $1-\\alpha/2$ quantile of the standard normal distribution.\n\nThe absolute margin of error, which we can denote by $E_{margin}$, is the half-width of the confidence interval. From the inequality above, it is clear that the expression for the margin of error in estimating $p$ is $E_{margin} = z_{1-\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}}$. The problem states a requirement that this margin of error should not exceed a prespecified tolerance $\\epsilon$.\n$$ z_{1-\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}} \\le \\epsilon $$\nOur goal is to find the minimum sample size $n$ that satisfies this condition. We rearrange the inequality to solve for $n$:\n$$ \\sqrt{\\frac{p(1-p)}{n}} \\le \\frac{\\epsilon}{z_{1-\\alpha/2}} $$\n$$ \\frac{p(1-p)}{n} \\le \\left(\\frac{\\epsilon}{z_{1-\\alpha/2}}\\right)^2 $$\n$$ n \\ge p(1-p) \\left(\\frac{z_{1-\\alpha/2}}{\\epsilon}\\right)^2 $$\nThis is the general design expression for the required sample size. However, this expression depends on the unknown proportion $p$. To guarantee that the margin of error requirement is met for *any* possible value of $p \\in [0, 1]$, we must adopt a conservative (worst-case) approach. This involves finding the value of $p$ that maximizes the required sample size $n$. The sample size $n$ is maximized when the term $p(1-p)$ is maximized.\n\nLet $f(p) = p(1-p) = p - p^2$. To find the maximum of this function on the interval $[0, 1]$, we can use calculus. The first derivative is $f'(p) = 1 - 2p$. Setting the derivative to zero, $1 - 2p = 0$, yields $p = \\frac{1}{2} = 0.5$. Since the second derivative $f''(p) = -2$ is negative, $p=0.5$ is a local maximum. This is the global maximum on the interval $[0, 1]$, with a maximum value of $f(0.5) = 0.5(1-0.5) = 0.25$.\n\nBy substituting this worst-case value for $p(1-p)$ into the sample size inequality, we obtain a conservative formula for $n$ that is independent of $p$:\n$$ n \\ge 0.25 \\left(\\frac{z_{1-\\alpha/2}}{\\epsilon}\\right)^2 $$\nWe are given the following values:\n- Confidence level $1-\\alpha = 1 - 0.05 = 0.95$.\n- This implies $\\alpha/2 = 0.025$, so we need the $1 - 0.025 = 0.975$ quantile of the standard normal distribution. The problem specifies to use the \"usual value,\" which is $z_{0.975} = 1.96$.\n- Maximum tolerable margin of error $\\epsilon = 0.04$.\n\nSubstituting these values into the conservative sample size formula:\n$$ n \\ge 0.25 \\left(\\frac{1.96}{0.04}\\right)^2 $$\nFirst, we compute the ratio inside the parentheses:\n$$ \\frac{1.96}{0.04} = \\frac{196}{4} = 49 $$\nNow, we substitute this back into the inequality:\n$$ n \\ge 0.25 \\times (49)^2 $$\n$$ n \\ge 0.25 \\times 2401 $$\n$$ n \\ge 600.25 $$\nSince the sample size $n$ must be an integer, we must choose the smallest integer that satisfies this condition. Therefore, we take the ceiling of the calculated value:\n$$ n_{min} = \\lceil 600.25 \\rceil = 601 $$\nThe minimal integer sample size required to ensure the margin of error is at most $0.04$ with $95\\%$ confidence, for any true prevalence $p$, is $601$.",
            "answer": "$$ \\boxed{601} $$"
        }
    ]
}