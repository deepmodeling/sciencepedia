{
    "hands_on_practices": [
        {
            "introduction": "我们从一个基础练习开始：为指数分布推导最大似然估计量（MLE）。这个过程将帮助您掌握应用最大似然原理的核心步骤——构建似然函数，并通过微积分找到使数据出现概率最大化的参数值。这是将理论模型转化为实际估计量的关键技能。",
            "id": "4937856",
            "problem": "在一项关于罕见生物事件到达间隔时间的研究中，假设您观测到一个来自率参数为 $\\theta$（其中 $\\theta>0$）的指数模型的独立同分布样本 $X_{1}, X_{2}, \\ldots, X_{n}$。其概率密度函数为 $f(x\\mid\\theta)=\\theta \\exp(-\\theta x)$，其中 $x>0$。请仅使用独立观测的似然函数的定义以及关于参数最大化其值的原则，根据所述假设从基本原理推导 $\\theta$ 的最大似然估计量 $\\hat{\\theta}$。请仔细证明您的解确实是参数空间 $\\{\\theta>0\\}$ 上的最大值点。请用样本均值 $\\bar{X}$（其中 $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$）的单个闭式表达式来表示您的最终答案。无需数值近似，也不涉及单位。",
            "solution": "该问题要求基于一个独立同分布 (i.i.d.) 样本 $X_{1}, X_{2}, \\ldots, X_{n}$，推导指数分布率参数 $\\theta$ 的最大似然估计量 (MLE)。推导过程必须从基本原理出发，并且必须严格证明该解是参数空间 $\\theta > 0$ 上的最大值。\n\n首先，我们建立似然函数 $L(\\theta)$。对于一个 i.i.d. 样本，似然函数是在每个样本点上求值的各个概率密度函数 (PDF) 的乘积。给定的 PDF 为 $f(x_i \\mid \\theta) = \\theta \\exp(-\\theta x_i)$，其中 $x_i > 0$。\n因此，似然函数为：\n$$L(\\theta) = \\prod_{i=1}^{n} f(x_i \\mid \\theta) = \\prod_{i=1}^{n} \\left[ \\theta \\exp(-\\theta x_i) \\right]$$\n根据乘积和指数的性质，可将其简化为：\n$$L(\\theta) = \\theta^n \\exp\\left(-\\theta \\sum_{i=1}^{n} x_i\\right)$$\n\n为了找到使 $L(\\theta)$ 最大化的 $\\theta$ 值，计算上更方便的是最大化似然函数的自然对数，即对数似然函数 $\\ell(\\theta) = \\ln(L(\\theta))$。由于自然对数是严格递增函数，使 $\\ell(\\theta)$ 最大化的 $\\theta$ 值也将使 $L(\\theta)$ 最大化。\n对数似然函数为：\n$$\\ell(\\theta) = \\ln\\left( \\theta^n \\exp\\left(-\\theta \\sum_{i=1}^{n} x_i\\right) \\right)$$\n利用对数的性质 $\\ln(ab) = \\ln(a) + \\ln(b)$ 和 $\\ln(a^b) = b\\ln(a)$，我们得到：\n$$\\ell(\\theta) = \\ln(\\theta^n) + \\ln\\left(\\exp\\left(-\\theta \\sum_{i=1}^{n} x_i\\right)\\right)$$\n$$\\ell(\\theta) = n \\ln(\\theta) - \\theta \\sum_{i=1}^{n} x_i$$\n此函数在指定的参数空间 $\\theta > 0$ 上有定义。\n\n为了求最大值，我们首先通过求 $\\ell(\\theta)$ 关于 $\\theta$ 的一阶导数并将其设为零来找到临界点。这个导数通常被称为得分函数。\n$$\\frac{d\\ell(\\theta)}{d\\theta} = \\frac{d}{d\\theta}\\left( n \\ln(\\theta) - \\theta \\sum_{i=1}^{n} x_i \\right) = \\frac{n}{\\theta} - \\sum_{i=1}^{n} x_i$$\n将导数设为零以找到候选估计量，我们将其记为 $\\hat{\\theta}$：\n$$\\frac{n}{\\hat{\\theta}} - \\sum_{i=1}^{n} x_i = 0$$\n$$\\frac{n}{\\hat{\\theta}} = \\sum_{i=1}^{n} x_i$$\n解出 $\\hat{\\theta}$ 可得：\n$$\\hat{\\theta} = \\frac{n}{\\sum_{i=1}^{n} x_i}$$\n问题要求用样本均值 $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ 来表示答案。我们可以将分母重写为 $\\sum_{i=1}^{n} x_i = n \\bar{x}$。将其代入 $\\hat{\\theta}$ 的表达式中：\n$$\\hat{\\theta} = \\frac{n}{n\\bar{x}} = \\frac{1}{\\bar{x}}$$\n因此，$\\theta$ 的 MLE 是样本均值的倒数，即 $\\hat{\\theta} = \\frac{1}{\\bar{X}}$。\n\n接下来，我们必须证明这个临界点对应一个最大值。我们使用二阶导数检验。对数似然函数的二阶导数是：\n$$\\frac{d^2\\ell(\\theta)}{d\\theta^2} = \\frac{d}{d\\theta}\\left( \\frac{n}{\\theta} - \\sum_{i=1}^{n} x_i \\right) = -\\frac{n}{\\theta^2}$$\n$\\theta$ 的参数空间是 $\\{\\theta \\mid \\theta > 0\\}$。样本大小 $n$ 是一个正整数。因此，对于参数空间中的任何 $\\theta$，都有 $\\theta^2 > 0$ 和 $n > 0$，这意味着二阶导数总是负的：\n$$\\frac{d^2\\ell(\\theta)}{d\\theta^2} = -\\frac{n}{\\theta^2}  0$$\n在整个定义域上二阶导数为负，表明对数似然函数 $\\ell(\\theta)$ 是严格凹函数。一个严格凹函数至多有一个临界点，如果存在，该点必定是唯一的全局最大值点。我们的计算找到了这个唯一的临界点位于 $\\hat{\\theta} = \\frac{1}{\\bar{X}}$。\n为了完全严谨，我们还考察 $\\ell(\\theta)$ 在参数空间 $(0, \\infty)$ 边界处的行为。\n当 $\\theta \\to 0^+$ 时，$n\\ln(\\theta)$ 项趋于 $-\\infty$，因此 $\\ell(\\theta) \\to -\\infty$。\n当 $\\theta \\to \\infty$ 时，我们分析 $\\ell(\\theta) = n \\ln(\\theta) - \\theta \\sum x_i$。在此极限下，负的线性项 $-\\theta \\sum x_i$ 的增长速度远快于对数项 $n\\ln(\\theta)$，导致 $\\ell(\\theta) \\to -\\infty$。（这里假设 $\\sum x_i  0$，对于一个非平凡的到达间隔时间样本，其中每个 $x_i  0$，这个条件必须成立）。\n由于该函数在定义域的两个边界处都趋于 $-\\infty$，并且在函数为凹的定义域内只有一个临界点，因此这个临界点必定是全局最大值点。\n所以，$\\theta$ 的最大似然估计量确实是 $\\hat{\\theta} = \\frac{1}{\\bar{X}}$。",
            "answer": "$$\\boxed{\\frac{1}{\\bar{X}}}$$"
        },
        {
            "introduction": "推导出估计量后，评估其质量至关重要。此练习探讨了估计量的一个关键属性：偏倚（bias），并以高斯分布的方差为例，揭示了最大似然估计量有时可能存在系统性偏差。理解并修正这种偏差是统计推断中的一个核心概念，也是著名的“$n-1$”修正的由来。",
            "id": "4159922",
            "problem": "一位神经科学家正在量化来自一个皮层区域的诱发局部场电位 (LFP) 幅值的逐次试验间的变异性。在 $n$ 次独立试验中，幅值测量值被建模为来自一个均值 $\\mu$ 和方差 $\\sigma^{2}$ 未知的高斯分布的独立同分布样本 $y_{1},\\dots,y_{n}$，即，对于 $i=1,\\dots,n$，$y_{i} \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ 独立成立。令 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$ 表示样本均值。\n\n从高斯模型的似然函数以及对于标量参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的偏差定义 $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$ 出发，完成以下任务：\n\n1. 在均值 $\\mu$ 未知的假设下，推导方差 $\\sigma^{2}$ 的最大似然估计量 (MLE)，并用 $y_{i}$ 和 $\\bar{y}$ 表示。\n2. 利用独立高斯随机变量的期望和方差的核心性质，推导 $\\sigma^{2}$ 的 MLE 的期望，并得到其偏差（作为 $n$ 和 $\\sigma^{2}$ 的函数）。你的推导应仅依赖于基本原理，如独立性、期望的线性性以及高斯数据样本均值的方差。\n3. 基于用 $\\bar{y}$ 构成的残差平方和，提出一个 $\\sigma^{2}$ 的无偏估计量，并证明其是无偏的。\n\n作为你的最终答案，报告用 $n$ 和 $\\sigma^{2}$ 表示的 $\\sigma^{2}$ 的 MLE 偏差的闭式解析表达式。无需四舍五入。最终答案必须是单个表达式。",
            "solution": "该问题被认为是有效的，因为它有科学依据、提法明确、客观且自洽。它在神经科学数据分析的背景下，提出了一个估计理论中的标准、可解问题。我们将进行完整的推导。\n\n该问题要求我们完成三项任务，这些任务与从一组 $n$ 个独立同分布 (i.i.d.) 样本 $y_1, \\dots, y_n$ 中估计高斯分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的方差 $\\sigma^2$ 相关，其中 $\\mu$ 和 $\\sigma^2$ 均未知。\n\n**1. $\\sigma^2$ 的最大似然估计量 (MLE) 的推导**\n\n从均值为 $\\mu$、方差为 $\\sigma^2$ 的高斯分布中抽取的单个观测值 $y_i$ 的概率密度函数 (PDF) 为：\n$$f(y_i \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)$$\n鉴于样本 $y_1, \\dots, y_n$ 是独立同分布的，似然函数 $L(\\mu, \\sigma^2 \\mid y_1, \\dots, y_n)$ 是各个 PDF 的乘积：\n$$L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} f(y_i \\mid \\mu, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right)$$\n为了找到最大似然估计量，使用对数似然函数 $\\ell(\\mu, \\sigma^2) = \\ln(L(\\mu, \\sigma^2))$ 会更方便：\n$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\n我们通过求 $\\ell$ 关于 $\\mu$ 和 $\\sigma^2$ 的偏导数并将它们设为零来找到 MLE。\n\n首先，对于均值 $\\mu$：\n$$\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\mu)(-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)$$\n将其设为零可得 $\\sum_{i=1}^{n} (y_i - \\mu) = 0$，这意味着 $\\sum_{i=1}^{n} y_i - n\\mu = 0$。因此 $\\mu$ 的 MLE 是：\n$$\\hat{\\mu}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n} y_i = \\bar{y}$$\n接下来，对于方差 $\\sigma^2$。为方便标记，我们令 $\\theta = \\sigma^2$。\n$$\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{n}{2\\theta} + \\frac{1}{2\\theta^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\n为了找到似然函数的联合最大值，我们将 $\\mu$ 的 MLE $\\hat{\\mu}_{\\text{MLE}} = \\bar{y}$ 代入关于 $\\theta$ 的导数中，并将其设为零：\n$$\\left. \\frac{\\partial \\ell}{\\partial \\theta} \\right|_{\\mu=\\bar{y}, \\theta=\\hat{\\theta}_{\\text{MLE}}} = -\\frac{n}{2\\hat{\\theta}_{\\text{MLE}}} + \\frac{1}{2\\hat{\\theta}_{\\text{MLE}}^2}\\sum_{i=1}^{n} (y_i - \\bar{y})^2 = 0$$\n乘以 $2\\hat{\\theta}_{\\text{MLE}}^2$ 得：\n$$-n\\hat{\\theta}_{\\text{MLE}} + \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = 0$$\n解出 $\\hat{\\theta}_{\\text{MLE}}$，我们得到方差 $\\sigma^2$ 的 MLE：\n$$\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n\n**2. $\\sigma^2$ 的 MLE 的偏差**\n\n参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的偏差定义为 $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$。我们必须计算 $\\hat{\\sigma}^2_{\\text{MLE}}$ 的期望。\n$$\\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = \\mathbb{E}\\left[ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\frac{1}{n} \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right]$$\n我们通过加上和减去真实均值 $\\mu$ 来分析这个平方和项：\n$$\\sum_{i=1}^{n} (y_i - \\bar{y})^2 = \\sum_{i=1}^{n} ((y_i - \\mu) - (\\bar{y} - \\mu))^2$$\n展开平方：\n$$= \\sum_{i=1}^{n} \\left[ (y_i - \\mu)^2 - 2(y_i - \\mu)(\\bar{y} - \\mu) + (\\bar{y} - \\mu)^2 \\right]$$\n$$= \\sum_{i=1}^{n} (y_i - \\mu)^2 - 2(\\bar{y} - \\mu) \\sum_{i=1}^{n} (y_i - \\mu) + \\sum_{i=1}^{n} (\\bar{y} - \\mu)^2$$\n认识到 $\\sum_{i=1}^{n} (y_i - \\mu) = n(\\bar{y} - \\mu)$，表达式变为：\n$$= \\sum_{i=1}^{n} (y_i - \\mu)^2 - 2n(\\bar{y} - \\mu)^2 + n(\\bar{y} - \\mu)^2 = \\sum_{i=1}^{n} (y_i - \\mu)^2 - n(\\bar{y} - \\mu)^2$$\n现在，我们利用期望的线性性来对这个表达式取期望：\n$$\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right] - n\\mathbb{E}\\left[ (\\bar{y} - \\mu)^2 \\right]$$\n我们分别计算每一项。\n根据定义，$y_i$ 的方差为 $\\mathrm{Var}(y_i) = \\mathbb{E}[(y_i - \\mathbb{E}[y_i])^2] = \\mathbb{E}[(y_i - \\mu)^2] = \\sigma^2$。因此：\n$$\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right] = \\sum_{i=1}^{n} \\mathbb{E}[(y_i - \\mu)^2] = \\sum_{i=1}^{n} \\sigma^2 = n\\sigma^2$$\n项 $\\mathbb{E}[(\\bar{y} - \\mu)^2]$ 是样本均值的方差 $\\mathrm{Var}(\\bar{y})$，因为 $\\mathbb{E}[\\bar{y}] = \\mu$。我们从基本原理推导这个结论。由于样本是独立的：\n$$\\mathrm{Var}(\\bar{y}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2} \\mathrm{Var}\\left(\\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(y_i) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sigma^2 = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$$\n将这些结果代回：\n$$\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = n\\sigma^2 - n\\left(\\frac{\\sigma^2}{n}\\right) = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2$$\n现在我们可以计算 MLE 的期望：\n$$\\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = \\frac{1}{n} \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\frac{1}{n}(n-1)\\sigma^2 = \\frac{n-1}{n}\\sigma^2$$\n最后，$\\sigma^2$ 的 MLE 的偏差是：\n$$\\mathrm{Bias}(\\hat{\\sigma}^2_{\\text{MLE}}) = \\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\n\n**3. $\\sigma^2$ 的无偏估计量**\n\n我们已经证明 $\\hat{\\sigma}^2_{\\text{MLE}}$ 是一个有偏估计量。我们可以通过对 $\\hat{\\sigma}^2_{\\text{MLE}}$ 进行缩放来构造一个无偏估计量。令无偏估计量为 $s^2$。我们希望 $\\mathbb{E}[s^2] = \\sigma^2$。\n我们知道 $\\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = \\frac{n-1}{n}\\sigma^2$。我们定义 $s^2 = c \\cdot \\hat{\\sigma}^2_{\\text{MLE}}$，其中 $c$ 是某个常数。\n$$\\mathbb{E}[s^2] = c \\cdot \\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = c \\left(\\frac{n-1}{n}\\right)\\sigma^2$$\n为了使其无偏，我们必须有 $c \\left(\\frac{n-1}{n}\\right) = 1$，这意味着 $c = \\frac{n}{n-1}$。\n因此，基于残差平方和 $\\sum_{i=1}^{n} (y_i - \\bar{y})^2$ 提出的无偏估计量是：\n$$s^2 = \\frac{n}{n-1} \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{n}{n-1} \\left( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right) = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n这就是众所周知的样本方差。为了证明它是无偏的，我们计算它的期望：\n$$\\mathbb{E}[s^2] = \\mathbb{E}\\left[ \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\frac{1}{n-1} \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right]$$\n使用我们第 2 部分的结果，$\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = (n-1)\\sigma^2$。\n$$\\mathbb{E}[s^2] = \\frac{1}{n-1} (n-1)\\sigma^2 = \\sigma^2$$\n由于 $\\mathbb{E}[s^2] - \\sigma^2 = 0$，估计量 $s^2$ 是无偏的。\n\n问题要求的是 $\\sigma^2$ 的 MLE 偏差的闭式表达式。这在分析的第二部分已经推导出来了。",
            "answer": "$$\\boxed{-\\frac{1}{n}\\sigma^2}$$"
        },
        {
            "introduction": "在无偏估计量和有偏估计量之间如何选择？本练习引入了均方误差（MSE）作为评估估计量总体性能的综合指标。通过比较两种不同估计量的MSE，您将亲身体会到统计学中最重要的概念之一——偏倚-方差权衡（bias-variance tradeoff），并理解为何有时一个有偏估计量可能比无偏估计量更优。",
            "id": "4937858",
            "problem": "一个公共卫生实验室进行了 $n$ 次独立检测，每次检测使用来自同一总体的不同血样，以估计一个二元生物标志物（存在与否）的流行率 $p$。设 $X_{1},\\dots,X_{n}$ 为独立同分布 (I.I.D.) 的随机变量，服从参数为 $p$ 的伯努利分布，即 $X_{i}\\sim \\mathrm{Bernoulli}(p)$，其中 $X_{i}=1$ 表示存在，$X_{i}=0$ 表示不存在。设样本比例为 $\\hat{p}=\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$。考虑收缩估计量 $\\tilde{p}=\\frac{n\\bar{X}+\\alpha}{n+\\beta}$，其中 $\\alpha\\ge 0$ 和 $\\beta\\ge 0$ 是不依赖于数据的固定常数，并用于伪计数正则化。\n\n从期望、方差、偏差和均方误差 (MSE) 的定义出发，其中参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的均方误差为 $\\mathrm{MSE}(\\hat{\\theta})= \\mathrm{Var}(\\hat{\\theta}) + \\left(\\mathrm{Bias}(\\hat{\\theta})\\right)^{2}$，偏差为 $\\mathrm{Bias}(\\hat{\\theta})=\\mathbb{E}[\\hat{\\theta}]-\\theta$，推导：\n- $\\hat{p}$ 的偏差和方差，\n- $\\tilde{p}$ 的偏差和方差，\n- 作为 $p$ 的函数的均方误差 $\\mathrm{MSE}(\\hat{p})$ 和 $\\mathrm{MSE}(\\tilde{p})$。\n\n然后，通过将差值 $\\mathrm{MSE}(\\tilde{p})-\\mathrm{MSE}(\\hat{p})$ 化简为关于 $p$、$n$、$\\alpha$ 和 $\\beta$ 的单一闭式解析表达式来比较这两个均方误差。作为最终答案，报告 $\\mathrm{MSE}(\\tilde{p})-\\mathrm{MSE}(\\hat{p})$ 的化简表达式。无需四舍五入。",
            "solution": "问题陈述已经过严格验证，被认为是有效的。它在科学上基于生物统计学和点估计理论的既定原则。该问题是适定的、客观的、自洽的，为得到唯一且有意义的解提供了所有必要的定义和数据。未检测到科学、逻辑或结构上的缺陷。\n\n问题要求推导伯努利比例 $p$ 的两个估计量的偏差、方差和均方误差 (MSE)，然后求出它们均方误差之差的化简表达式。设 $X_{1},\\dots,X_{n}$ 是来自 $\\mathrm{Bernoulli}(p)$ 分布的独立同分布 (I.I.D.) 随机变量。我们知道对于每个 $X_i$，其期望为 $\\mathbb{E}[X_i] = p$，方差为 $\\mathrm{Var}(X_i) = p(1-p)$。设 $S_n = \\sum_{i=1}^{n}X_{i}$。由于独立同分布的性质，$S_n$ 服从二项分布，$S_n \\sim \\mathrm{Binomial}(n, p)$，其期望为 $\\mathbb{E}[S_n] = np$，方差为 $\\mathrm{Var}(S_n) = np(1-p)$。\n\n首先，我们分析标准样本比例 $\\hat{p} = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i} = \\frac{S_n}{n}$。\n\n$\\hat{p}$ 的期望是：\n$$\\mathbb{E}[\\hat{p}] = \\mathbb{E}\\left[\\frac{S_n}{n}\\right] = \\frac{1}{n}\\mathbb{E}[S_n] = \\frac{1}{n}(np) = p$$\n参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的偏差定义为 $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$。因此，$\\hat{p}$ 的偏差是：\n$$\\mathrm{Bias}(\\hat{p}) = \\mathbb{E}[\\hat{p}] - p = p - p = 0$$\n估计量 $\\hat{p}$ 是无偏的。\n\n$\\hat{p}$ 的方差是：\n$$\\mathrm{Var}(\\hat{p}) = \\mathrm{Var}\\left(\\frac{S_n}{n}\\right) = \\frac{1}{n^2}\\mathrm{Var}(S_n) = \\frac{1}{n^2}(np(1-p)) = \\frac{p(1-p)}{n}$$\nMSE 定义为 $\\mathrm{MSE}(\\hat{\\theta}) = \\mathrm{Var}(\\hat{\\theta}) + (\\mathrm{Bias}(\\hat{\\theta}))^2$。对于 $\\hat{p}$，其 MSE 是：\n$$\\mathrm{MSE}(\\hat{p}) = \\mathrm{Var}(\\hat{p}) + (\\mathrm{Bias}(\\hat{p}))^2 = \\frac{p(1-p)}{n} + 0^2 = \\frac{p(1-p)}{n}$$\n\n接下来，我们分析收缩估计量 $\\tilde{p} = \\frac{n\\bar{X}+\\alpha}{n+\\beta} = \\frac{n\\hat{p}+\\alpha}{n+\\beta}$。\n\n$\\tilde{p}$ 的期望是：\n$$\\mathbb{E}[\\tilde{p}] = \\mathbb{E}\\left[\\frac{n\\hat{p}+\\alpha}{n+\\beta}\\right] = \\frac{1}{n+\\beta}\\mathbb{E}[n\\hat{p}+\\alpha] = \\frac{1}{n+\\beta}(n\\mathbb{E}[\\hat{p}]+\\alpha) = \\frac{np+\\alpha}{n+\\beta}$$\n$\\tilde{p}$ 的偏差是：\n$$\\mathrm{Bias}(\\tilde{p}) = \\mathbb{E}[\\tilde{p}] - p = \\frac{np+\\alpha}{n+\\beta} - p = \\frac{np+\\alpha - p(n+\\beta)}{n+\\beta} = \\frac{np+\\alpha - np - p\\beta}{n+\\beta} = \\frac{\\alpha - \\beta p}{n+\\beta}$$\n$\\tilde{p}$ 的方差是：\n$$\\mathrm{Var}(\\tilde{p}) = \\mathrm{Var}\\left(\\frac{n\\hat{p}+\\alpha}{n+\\beta}\\right) = \\left(\\frac{n}{n+\\beta}\\right)^2 \\mathrm{Var}(\\hat{p}) = \\frac{n^2}{(n+\\beta)^2}\\left(\\frac{p(1-p)}{n}\\right) = \\frac{np(1-p)}{(n+\\beta)^2}$$\n$\\tilde{p}$ 的 MSE 是其方差和偏差平方的和：\n$$\\mathrm{MSE}(\\tilde{p}) = \\mathrm{Var}(\\tilde{p}) + (\\mathrm{Bias}(\\tilde{p}))^2 = \\frac{np(1-p)}{(n+\\beta)^2} + \\left(\\frac{\\alpha - \\beta p}{n+\\beta}\\right)^2 = \\frac{np(1-p) + (\\alpha - \\beta p)^2}{(n+\\beta)^2}$$\n\n最后，题目要求我们求出差值 $\\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p})$，并将其化简为单一的闭式表达式。\n$$\\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p}) = \\frac{np(1-p) + (\\alpha - \\beta p)^2}{(n+\\beta)^2} - \\frac{p(1-p)}{n}$$\n为了合并这些项，我们使用公分母 $n(n+\\beta)^2$。\n$$\\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p}) = \\frac{n\\left[np(1-p) + (\\alpha - \\beta p)^2\\right] - (n+\\beta)^2 p(1-p)}{n(n+\\beta)^2}$$\n我们来分析分子。我们可以将包含 $p(1-p)$ 的项分组：\n$$ \\text{分子} = n^2 p(1-p) - (n+\\beta)^2 p(1-p) + n(\\alpha - \\beta p)^2 $$\n$$ = \\left[n^2 - (n+\\beta)^2\\right]p(1-p) + n(\\alpha - \\beta p)^2 $$\n$$ = \\left[n^2 - (n^2 + 2n\\beta + \\beta^2)\\right]p(1-p) + n(\\alpha - \\beta p)^2 $$\n$$ = (-2n\\beta - \\beta^2)p(1-p) + n(\\alpha - \\beta p)^2 $$\n现在我们展开剩余的项，并根据 $p$ 的幂次合并系数：\n$$ \\text{分子} = -\\beta(2n+\\beta)(p-p^2) + n(\\alpha^2 - 2\\alpha\\beta p + \\beta^2 p^2) $$\n$$ = - (2n\\beta + \\beta^2)p + (2n\\beta + \\beta^2)p^2 + n\\alpha^2 - 2n\\alpha\\beta p + n\\beta^2 p^2 $$\n合并 $p^2$ 的系数：\n$$(2n\\beta + \\beta^2 + n\\beta^2)p^2 = (n\\beta^2 + 2n\\beta + \\beta^2)p^2$$\n合并 $p$ 的系数：\n$$(-(2n\\beta + \\beta^2) - 2n\\alpha\\beta)p = -(2n\\alpha\\beta + 2n\\beta + \\beta^2)p$$\n常数项是 $n\\alpha^2$。\n\n将分子整理成关于 $p$ 的多项式：\n$$ \\text{分子} = (n\\beta^2 + 2n\\beta + \\beta^2)p^2 - (2n\\alpha\\beta + 2n\\beta + \\beta^2)p + n\\alpha^2 $$\n因此，MSE 的差值的化简形式是：\n$$ \\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p}) = \\frac{(n\\beta^2 + 2n\\beta + \\beta^2)p^2 - (2n\\alpha\\beta + 2n\\beta + \\beta^2)p + n\\alpha^2}{n(n+\\beta)^2} $$",
            "answer": "$$\n\\boxed{\\frac{(n\\beta^2 + 2n\\beta + \\beta^2)p^2 - (2n\\alpha\\beta + 2n\\beta + \\beta^2)p + n\\alpha^2}{n(n+\\beta)^2}}\n$$"
        }
    ]
}