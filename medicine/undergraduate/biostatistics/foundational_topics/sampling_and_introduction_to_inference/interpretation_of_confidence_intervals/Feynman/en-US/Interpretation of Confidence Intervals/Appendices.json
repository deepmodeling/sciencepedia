{
    "hands_on_practices": [
        {
            "introduction": "Estimating a population mean from a small sample is a cornerstone of statistical inference. This exercise guides you through the construction of a confidence interval for a mean when the population variance is unknown, a common scenario in practice. By using the Student's $t$-distribution, you will learn to account for the additional uncertainty that arises from estimating the variance and explore how the sample size directly influences the precision of your estimate .",
            "id": "4918389",
            "problem": "A biostatistician draws a simple random sample of size $n=12$ from a population assumed to follow a Normal distribution $\\mathcal{N}(\\mu,\\sigma^{2})$, where $\\sigma^{2}$ is unknown. The sample has mean $\\bar{x}=5.1$ and sample standard deviation $s=1.9$. Using only foundational principles about sampling distributions and pivotal quantities, construct the two-sided 95% confidence interval for the mean $\\mu$ based on the Student’s $t$ distribution, and briefly explain, from first principles, how and why the degrees of freedom $n-1$ affect the interval’s width. For grading purposes, report as your final numerical answer the margin of error (that is, the half-width of the 95% confidence interval). Round your final numerical answer to four significant figures.",
            "solution": "The problem is well-posed and scientifically sound, containing all necessary information to construct the confidence interval and analyze its properties.\n\nWe are given a simple random sample of size $n=12$ from a population assumed to follow a Normal distribution, $\\mathcal{N}(\\mu, \\sigma^2)$, with unknown mean $\\mu$ and unknown variance $\\sigma^2$. The sample mean is $\\bar{x}=5.1$ and the sample standard deviation is $s=1.9$. We are tasked with constructing a two-sided 95% confidence interval for $\\mu$.\n\nThe foundational principle for constructing this interval is the use of a pivotal quantity, which is a function of the sample data and the parameter of interest whose sampling distribution is known and does not depend on any unknown parameters.\n\nSince the population variance $\\sigma^2$ is unknown, we cannot use the standard normal ($Z$) statistic. Instead, we use the sample standard deviation $s$ as an estimate for $\\sigma$. The resulting pivotal quantity is the $t$-statistic, defined as:\n$$\nT = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}}\n$$\nwhere $\\bar{X}$ is the sample mean random variable and $S$ is the sample standard deviation random variable. By a fundamental theorem of statistics (originally from W.S. Gosset), this quantity $T$ follows a Student's $t$-distribution with $\\nu = n-1$ degrees of freedom.\n\nFor a two-sided 95% confidence interval, the level of significance is $\\alpha = 1 - 0.95 = 0.05$. We need to find the critical values $\\pm t_{\\alpha/2, \\nu}$ from the $t$-distribution with $\\nu = n-1$ degrees of freedom that bound the central $1-\\alpha=0.95$ of the probability mass. This is expressed as:\n$$\nP(-t_{\\alpha/2, \\nu} < T < t_{\\alpha/2, \\nu}) = 1-\\alpha\n$$\nSubstituting the expression for $T$, we have:\n$$\nP\\left(-t_{\\alpha/2, \\nu} < \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} < t_{\\alpha/2, \\nu}\\right) = 1-\\alpha\n$$\nTo find the interval for $\\mu$, we rearrange the inequalities:\n$$\n-t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}} < \\bar{X} - \\mu < t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}}\n$$\n$$\n-\\bar{X} - t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}} < -\\mu < -\\bar{X} + t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}}\n$$\nMultiplying by $-1$ reverses the inequalities:\n$$\n\\bar{X} - t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}} < \\mu < \\bar{X} + t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}}\n$$\nThe confidence interval is therefore given by $\\bar{x} \\pm E$, where $E$ is the margin of error. The margin of error is defined as:\n$$\nE = t_{\\alpha/2, \\nu} \\frac{s}{\\sqrt{n}}\n$$\nNow, we substitute the given values:\nSample size $n=12$.\nSample standard deviation $s=1.9$.\nDegrees of freedom $\\nu = n-1 = 12-1 = 11$.\nSignificance level $\\alpha = 0.05$, so $\\alpha/2 = 0.025$.\n\nWe need the critical value $t_{0.025, 11}$ from the Student's $t$-distribution with $11$ degrees of freedom. This value is the point such that the area in the upper tail is $0.025$. Consulting a $t$-distribution table or using statistical software, we find:\n$$\nt_{0.025, 11} \\approx 2.201\n$$\nNext, we calculate the standard error of the mean (SEM):\n$$\n\\text{SEM} = \\frac{s}{\\sqrt{n}} = \\frac{1.9}{\\sqrt{12}}\n$$\nThe margin of error is then:\n$$\nE = t_{0.025, 11} \\cdot \\frac{s}{\\sqrt{n}} \\approx 2.201 \\times \\frac{1.9}{\\sqrt{12}}\n$$\n$$\nE \\approx 2.201 \\times \\frac{1.9}{3.46410} \\approx 2.201 \\times 0.54856 \\approx 1.20736\n$$\nRounding to four significant figures, the margin of error is $1.207$.\n\nThe second part of the problem asks for an explanation of how and why the degrees of freedom ($df = n-1$) affect the interval's width. The width of the confidence interval is $2E = 2 \\cdot t_{\\alpha/2, n-1} \\cdot \\frac{s}{\\sqrt{n}}$. The degrees of freedom, $\\nu = n-1$, influence the width primarily through the critical value $t_{\\alpha/2, \\nu}$.\n\nFrom first principles, the Student's $t$-distribution is used instead of the Normal distribution to account for the additional uncertainty introduced by estimating the unknown population standard deviation $\\sigma$ with the sample standard deviation $s$. The shape of the $t$-distribution depends on the degrees of freedom, $\\nu$. For small values of $\\nu$ (i.e., small sample sizes), the $t$-distribution has heavier tails compared to the standard normal distribution. This means there is more probability in the tails, reflecting the greater uncertainty in the estimate $s$.\n\nAs the degrees of freedom $\\nu=n-1$ increase, the sample standard deviation $s$ becomes a more reliable estimate of $\\sigma$. Consequently, the $t$-distribution converges in shape to the standard normal distribution $\\mathcal{N}(0,1)$. For a a fixed confidence level $1-\\alpha$, the critical value $t_{\\alpha/2, \\nu}$ is a strictly decreasing function of $\\nu$. A larger $\\nu$ corresponds to a smaller critical value $t_{\\alpha/2, \\nu}$, which moves closer to the corresponding $z_{\\alpha/2}$ value from the normal distribution.\nA smaller critical value directly results in a smaller margin of error $E$, and thus a narrower confidence interval. In summary, increasing the degrees of freedom reduces the penalty for estimating $\\sigma$, leading to a smaller critical value and a more precise (narrower) confidence interval for the mean $\\mu$.",
            "answer": "$$\\boxed{1.207}$$"
        },
        {
            "introduction": "Not all statistical formulas are universally applicable, and this practice illustrates the critical importance of choosing the right tool for the job. You will compare the widely taught Wald interval for a proportion with the more reliable Wilson score interval, particularly in a challenging scenario where events are rare. This exercise demonstrates how simple approximations can fail near the boundaries of the parameter space and how a more theoretically sound method provides more trustworthy results .",
            "id": "4918349",
            "problem": "A public health laboratory screens a random sample of $n=100$ specimens for a pathogen and observes $x=2$ positives. Let $X \\sim \\mathrm{Bin}(n,p)$ denote the number of positives under independent, identically distributed Bernoulli trials with success probability $p$, and let $\\hat{p}=x/n$ be the sample proportion. Using only foundational principles (including the Central Limit Theorem (CLT) for independent, identically distributed Bernoulli variables and test inversion logic), construct the 95% Wald confidence interval and the 95% Wilson score confidence interval for $p$. Then, compute the ratio of the Wilson interval length to the Wald interval length, explicitly treating the Wald interval as untruncated (so its lower end may lie outside $[0,1]$). Round your ratio to four significant figures and express it as a decimal. Finally, explain from first principles why coverage issues arise near the parameter boundary for the Wald interval and how the Wilson interval addresses these issues. The final numerical answer must be the ratio of lengths, rounded to four significant figures.",
            "solution": "The problem requires the construction and comparison of two types of confidence intervals for a binomial proportion, $p$. We are given a sample size of $n=100$ and an observed number of positive specimens $x=2$. The confidence level is specified as 95%.\n\nFirst, we establish the basic quantities. The sample proportion is $\\hat{p} = \\frac{x}{n} = \\frac{2}{100} = 0.02$. For a 95% confidence level, the significance level is $\\alpha = 1 - 0.95 = 0.05$. The critical value from the standard normal distribution is $z_{\\alpha/2} = z_{0.025}$. From the cumulative distribution function of the standard normal distribution, $\\Phi(z_{0.025}) = 1 - 0.025 = 0.975$, which gives $z_{0.025} \\approx 1.95996$. We will use the common approximation $z_{0.025} = 1.96$.\n\nThe construction of both intervals is based on the principle of test inversion, starting from a test statistic that is approximately standard normal. By the Central Limit Theorem applied to a sum of independent and identically distributed Bernoulli random variables, the sample proportion $\\hat{p}$ is approximately normally distributed for large $n$:\n$$ \\hat{p} \\approx_d N\\left(p, \\frac{p(1-p)}{n}\\right) $$\nwhere $N(\\mu, \\sigma^2)$ denotes a normal distribution with mean $\\mu$ and variance $\\sigma^2$. This leads to the standardized pivotal quantity:\n$$ Z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\approx_d N(0,1) $$\n\n**1. Wald Confidence Interval**\n\nThe Wald interval is derived by inverting the Wald test. This method simplifies the pivotal quantity by substituting the sample estimate $\\hat{p}$ for the unknown parameter $p$ in the standard error term. The estimated standard error is $\\widehat{\\mathrm{SE}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$. The test statistic\n$$ Z_W = \\frac{\\hat{p} - p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} $$\nis assumed to be approximately standard normal. The $(1-\\alpha)$ confidence interval is the set of all values of $p$ for which we would not reject the null hypothesis $H_0: \\text{proportion} = p$ at significance level $\\alpha$. This corresponds to the inequality $|Z_W| \\le z_{\\alpha/2}$, or:\n$$ -z_{\\alpha/2} \\le \\frac{\\hat{p} - p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} \\le z_{\\alpha/2} $$\nSolving this inequality for $p$ yields the Wald interval:\n$$ \\mathrm{CI}_W = \\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} $$\nUsing the given values:\n$\\hat{p} = 0.02$, $n=100$, and $z_{0.025}=1.96$.\nThe margin of error is:\n$$ E_W = 1.96 \\times \\sqrt{\\frac{0.02(1-0.02)}{100}} = 1.96 \\times \\sqrt{\\frac{0.0196}{100}} = 1.96 \\times \\frac{0.14}{10} = 1.96 \\times 0.014 = 0.02744 $$\nThe interval bounds are:\nLower bound: $p_{W, \\text{lower}} = 0.02 - 0.02744 = -0.00744$.\nUpper bound: $p_{W, \\text{upper}} = 0.02 + 0.02744 = 0.04744$.\nAs per the problem statement, we use the untruncated interval. The length of the Wald interval is $L_W = p_{W, \\text{upper}} - p_{W, \\text{lower}} = 2 \\times E_W = 2 \\times 0.02744 = 0.05488$.\n\n**2. Wilson Score Confidence Interval**\n\nThe Wilson score interval is derived by inverting the score test. Crucially, it does not substitute $\\hat{p}$ into the standard error term in the pivotal quantity. Instead, it solves for the values of $p$ that satisfy:\n$$ -z_{\\alpha/2} \\le \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\le z_{\\alpha/2} $$\nThis is equivalent to solving the quadratic inequality:\n$$ (\\hat{p} - p)^2 \\le z_{\\alpha/2}^2 \\left( \\frac{p(1-p)}{n} \\right) $$\nRearranging this into the standard quadratic form $Ap^2+Bp+C=0$ gives the endpoints of the interval:\n$$ n(\\hat{p}-p)^2 - z_{\\alpha/2}^2 p(1-p) = 0 $$\n$$ n(\\hat{p}^2 - 2\\hat{p}p + p^2) - z_{\\alpha/2}^2(p - p^2) = 0 $$\n$$ (n+z_{\\alpha/2}^2)p^2 - (2n\\hat{p} + z_{\\alpha/2}^2)p + n\\hat{p}^2 = 0 $$\nThe roots of this quadratic equation are found using the quadratic formula, $p = \\frac{-B \\pm \\sqrt{B^2-4AC}}{2A}$, which gives the interval endpoints:\n$$ \\mathrm{CI}_S = \\frac{2n\\hat{p} + z_{\\alpha/2}^2 \\pm z_{\\alpha/2}\\sqrt{4n\\hat{p}(1-\\hat{p}) + z_{\\alpha/2}^2}}{2(n+z_{\\alpha/2}^2)} $$\nLet's substitute the values: $n=100$, $\\hat{p}=0.02$, and $z_{0.025}=1.96$. Let $z=1.96$, so $z^2 = 3.8416$.\nThe common denominator is $2(n+z^2) = 2(100+3.8416) = 207.6832$.\nThe first term in the numerator is $2n\\hat{p} + z^2 = 2(100)(0.02) + 3.8416 = 4 + 3.8416 = 7.8416$.\nThe term under the square root in the numerator is $4n\\hat{p}(1-\\hat{p}) + z^2 = 4(100)(0.02)(0.98) + 3.8416 = 7.84 + 3.8416 = 11.6816$.\nThe second term in the numerator is $z\\sqrt{11.6816} = 1.96 \\times \\sqrt{11.6816} \\approx 1.96 \\times 3.4178356 \\approx 6.7009578$.\nThe interval is therefore:\n$$ p_S = \\frac{7.8416 \\pm 6.7009578}{207.6832} $$\nLower bound: $p_{S, \\text{lower}} = \\frac{7.8416 - 6.7009578}{207.6832} = \\frac{1.1406422}{207.6832} \\approx 0.0054921$.\nUpper bound: $p_{S, \\text{upper}} = \\frac{7.8416 + 6.7009578}{207.6832} = \\frac{14.5425578}{207.6832} \\approx 0.0700223$.\nThe length of the Wilson score interval is $L_S = p_{S, \\text{upper}} - p_{S, \\text{lower}} \\approx 0.0700223 - 0.0054921 = 0.0645302$.\n\n**3. Ratio of Interval Lengths**\n\nThe ratio of the Wilson interval length to the Wald interval length is:\n$$ R = \\frac{L_S}{L_W} = \\frac{0.0645302}{0.05488} \\approx 1.1758418 $$\nRounding to four significant figures, the ratio is $1.176$.\n\n**4. Explanation of Coverage Issues**\n\nThe substantial difference in interval lengths and bounds, especially the negative lower bound of the Wald interval, highlights fundamental theoretical differences.\n\nThe primary flaw of the **Wald interval** arises from its reliance on estimating the standard error, $\\mathrm{SE} = \\sqrt{p(1-p)/n}$, using the point estimate $\\hat{p}$. The variance of a Bernoulli trial, $p(1-p)$, is maximized at $p=0.5$ and approaches zero as $p$ approaches the boundaries of the parameter space, $p=0$ or $p=1$. When the true $p$ is small, as in this problem, the observed $\\hat{p}$ is also likely to be small. This leads to a systematic underestimation of the true standard error. An underestimated SE produces an interval that is artificially narrow, which causes its actual coverage probability to fall below the nominal level (e.g., below 95%). In the extreme cases where $x=0$ or $x=n$, we have $\\hat{p}=0$ or $\\hat{p}=1$, resulting in an estimated standard error of zero and a zero-width interval. Such an interval has a $0\\%$ chance of covering any true parameter $p \\in (0,1)$. Furthermore, the Wald interval is symmetric about $\\hat{p}$, which is a poor approximation for the highly skewed sampling distribution of $\\hat{p}$ when $p$ is near $0$ or $1$. The fact that it can produce nonsensical bounds (like $p < 0$) is a direct symptom of these theoretical deficiencies.\n\nThe **Wilson score interval** remedies these issues by not substituting the point estimate into the standard error. By solving the quadratic inequality involving the true parameter $p$ in the variance term, it accounts for the dependence of the variance on $p$. This approach has several benefits:\n1.  **Avoids SE Underestimation:** It does not use the potentially small $\\hat{p}$ to estimate variance, leading to a more appropriate interval width and better coverage properties, especially near the boundaries.\n2.  **Asymmetry:** The interval is not symmetric around $\\hat{p}$. Its center, $\\frac{n\\hat{p} + z_{\\alpha/2}^2/2}{n + z_{\\alpha/2}^2}$, is a weighted average of $\\hat{p}$ and $0.5$. This center is pulled towards $0.5$, which is particularly effective when $\\hat{p}$ is near $0$ or $1$, creating an asymmetric interval that better reflects the skewness of the underlying sampling distribution. For our data, this center is approximately $0.038$, higher than $\\hat{p}=0.02$.\n3.  **Boundary Integrity:** The Wilson interval is guaranteed to lie within the $(0,1)$ parameter space, except for the trivial endpoints of $0$ or $1$ when $x=0$ or $x=n$, respectively. It never collapses to a zero-width interval for observed proportions of $0$ or $1$, thus maintaining reasonable performance across the entire parameter space. Its mean coverage probability is consistently closer to the nominal level than that of the Wald interval.\n\nIn this specific problem, where $np=2$, the normal approximation is on questionable ground. The Wald interval is visibly too narrow and incorrectly positioned, as evidenced by its negative lower bound. The Wilson interval, by contrast, is wider and shifted to the right, providing a more plausible and theoretically sound range for the true proportion $p$.",
            "answer": "$$\\boxed{1.176}$$"
        },
        {
            "introduction": "How do we make confident statements when we observe zero events? This problem tackles a frequent challenge in fields like safety monitoring and clinical trials, where standard interval formulas break down. By returning to the fundamental definition of a confidence interval—inverting a hypothesis test—you will derive an \"exact\" one-sided upper bound for a proportion. This powerful technique provides a meaningful conclusion even from sparse data and deepens your understanding of the core principles of frequentist coverage .",
            "id": "4918350",
            "problem": "A single-arm Phase II trial monitors a binary adverse event during an observation window following a vaccine dose. Among $n=40$ independent participants, zero events are observed ($x=0$). Assume a Binomial model $X \\sim \\mathrm{Binomial}(n,p)$ with event probability $p$ that is common across participants and independent across trials.\n\nUsing only the definition of a one-sided upper confidence bound through its frequentist coverage property and the distributional assumption above (that is, without normal approximations), derive an exact one-sided upper confidence bound for $p$ at confidence level $0.95$. Your derivation should start from the definition that a one-sided upper $(1-\\alpha)$ confidence bound $U(X)$ must satisfy $\\inf_{p \\in [0,1]} \\Pr_{p}\\big(p \\le U(X)\\big) \\ge 1-\\alpha$, and proceed by inverting an appropriate family of hypothesis tests for $H_{0}: p=p_{0}$ versus a one-sided alternative.\n\nThen compute the numerical value of the bound for $x=0$ and $n=40$ at confidence level $0.95$. Express your final bound as a decimal, and round your answer to four significant figures.\n\nFinally, briefly interpret, in clinical terms, what this bound means about plausible values of $p$ given the data, and what its coverage statement means in repeated sampling.",
            "solution": "The problem as stated is formally sound. It presents a standard scenario in biostatistics for calculating an exact confidence interval for a binomial proportion. The givens are complete and consistent, the objective is clearly defined, and the task is grounded in established statistical theory. Therefore, we proceed with the solution.\n\nThe problem asks for the derivation and calculation of a one-sided upper confidence bound for a binomial proportion $p$, based on an observation of $X=x$ events in $n$ trials. The confidence level is specified as $1-\\alpha$.\n\nThe derivation begins with the frequentist definition of a confidence interval, which is constructed by inverting a family of hypothesis tests. For a one-sided upper bound, we are looking for an interval of the form $[0, U(x)]$. This interval should contain all values of the parameter $p_0$ for which the null hypothesis $H_0: p=p_0$ is not rejected in favor of a specific alternative, given the observed data $x$.\n\nSince we seek an upper bound for $p$, a high posited value $p_0$ would be considered less plausible if the observed number of events $x$ is small. This leads us to formulate the hypothesis test for each possible value $p_0 \\in [0, 1]$ as:\n- Null Hypothesis $H_0: p = p_0$\n- Alternative Hypothesis $H_1: p < p_0$\n\nThe test statistic is the number of observed events, $X$, which follows a binomial distribution, $X \\sim \\mathrm{Binomial}(n, p)$. Under the null hypothesis, $X \\sim \\mathrm{Binomial}(n, p_0)$. Evidence against $H_0$ in favor of $H_1$ is provided by small values of $X$.\n\nThe p-value for this test is the probability of observing a result as extreme as or more extreme than the observed data $x$, under the assumption that $H_0$ is true. \"More extreme\" in this context means a value of the test statistic that provides stronger evidence for the alternative, which corresponds to values of $X \\le x$.\nTherefore, the p-value is given by:\n$$ \\text{p-value} = \\Pr_{p_0}(X \\le x) = \\sum_{k=0}^{x} \\binom{n}{k} p_0^k (1-p_0)^{n-k} $$\n\nWe reject the null hypothesis $H_0: p=p_0$ at a significance level $\\alpha$ if the p-value is less than or equal to $\\alpha$.\n$$ \\Pr_{p_0}(X \\le x) \\le \\alpha $$\n\nThe $(1-\\alpha)$ confidence set for $p$ is the collection of all values $p_0$ for which we do *not* reject $H_0$. That is, the set of all $p_0$ such that:\n$$ \\Pr_{p_0}(X \\le x) > \\alpha $$\n\nThe one-sided upper confidence bound, which we denote as $p_U$, is the supremum of this set. The function $f(p_0) = \\Pr_{p_0}(X \\le x)$ is a monotonically decreasing function of $p_0$. Consequently, the upper bound $p_U$ is the value of $p$ that satisfies the boundary condition:\n$$ \\Pr_{p_U}(X \\le x) = \\alpha $$\nThis method of constructing an interval is known as the Clopper-Pearson method for exact confidence intervals.\n\nNow, we apply this general formula to the specific data provided in the problem:\n- Sample size: $n = 40$\n- Observed events: $x = 0$\n- Confidence level: $1-\\alpha = 0.95$, which implies $\\alpha = 0.05$\n\nSubstituting these values into the equation for the upper bound $p_U$:\n$$ \\Pr_{p_U}(X \\le 0) = 0.05 $$\nFor a binomial distribution, the event $X \\le 0$ is equivalent to the event $X=0$. Thus, the equation simplifies to:\n$$ \\Pr_{p_U}(X = 0) = 0.05 $$\nThe probability mass function for a binomial random variable at $k=0$ is $\\binom{n}{0} p^0 (1-p)^{n-0} = (1-p)^n$. Applying this to our case:\n$$ (1-p_U)^{40} = 0.05 $$\nTo solve for $p_U$, we take the $40$-th root of both sides:\n$$ 1 - p_U = (0.05)^{1/40} $$\nFinally, we isolate $p_U$:\n$$ p_U = 1 - (0.05)^{1/40} $$\n\nWe now compute the numerical value:\n$$ p_U = 1 - (0.05)^{0.025} $$\n$$ p_U \\approx 1 - 0.92784158 $$\n$$ p_U \\approx 0.07215842 $$\nRounding to four significant figures, the result is $0.07216$.\n\nThe one-sided 95% upper confidence bound for the adverse event probability $p$ is approximately $0.07216$.\n\nThe interpretation of this result is twofold:\n1.  **Interpretation of the Bound**: Given the observation of zero adverse events in $40$ participants, we are 95% confident that the true, unknown probability $p$ of an adverse event is no more than $0.07216$, or $7.216\\%$. Values of $p$ greater than $0.07216$ are considered inconsistent with the observed data, because if the true rate were higher than this, the probability of observing zero events would have been less than $5\\%$.\n2.  **Interpretation of the Coverage Property**: The statement \"95% confident\" refers to the long-run performance of the statistical procedure used to generate the bound. If we were to repeat this study (i.e., recruit many new samples of $40$ participants) and calculate a one-sided 95% upper bound from each sample, we would expect at least 95% of these calculated upper bounds to be greater than or equal to the true, fixed value of $p$. It is a statement about the reliability of the method, not a probabilistic statement about the true parameter $p$ itself. The true parameter is a fixed, not a random, quantity.",
            "answer": "$$\n\\boxed{0.07216}\n$$"
        }
    ]
}