{
    "hands_on_practices": [
        {
            "introduction": "理解一个理论最好的方式就是亲手实践。我们的第一个练习将从生物统计学中最常见的情景——二元结果（例如，疾病发生或未发生）——开始。我们将使用基本的伯努利分布，从第一性原理出发，一步步构建似然函数并推导出其最大似然估计量。这个练习不仅能帮助你牢固掌握似然性的核心思想，还将通过分析边界情况，让你深刻理解参数空间的定义为何至关重要 ()。",
            "id": "4969170",
            "problem": "在一项关于二元临床结局的多中心观察性研究中，每位患者在固定的随访期内或者经历特定的不良事件，或者不经历。令 $X_{1},\\dots,X_{n}$ 表示独立同分布（i.i.d.）的伯努利随机变量，其未知事件概率为 $p$，其中 $X_{i}=1$ 表示患者 $i$ 发生了不良事件，否则 $X_{i}=0$。$p$ 的参数空间是闭区间 $[0,1]$，这是一个概率的自然空间，但您也应该分析当参数空间取为开区间 $(0,1)$ 时会发生什么。\n\n仅使用伯努利概率质量函数的定义和独立同分布数据的似然函数定义，从第一性原理推导 $p$ 的最大似然估计量（MLE） $\\hat{p}$。然后，在参数空间为 $(0,1)$ 和 $[0,1]$ 两种选择下，仔细分析在所有观测值为 $0$（即对所有 $i$，$X_{i}=0$）或所有观测值为 $1$（即对所有 $i$，$X_{i}=1$）的非正则边界情况下，最大似然估计量的存在性和唯一性。您的分析应通过借助似然函数或对数似然函数的适当性质，来证明在每种边界情况下，指定参数空间中是否存在最大化者以及其是否唯一。\n\n以 $\\hat{p}$ 的单个闭式表达式形式提供最终答案。无需进行数值舍入。",
            "solution": "该问题要求推导伯努利分布参数 $p$ 的最大似然估计量 (MLE)，并仔细分析在两种不同参数空间 $[0, 1]$ 和 $(0, 1)$ 下，该估计量在边界情况下的存在性和唯一性。\n\n设 $X_{1}, \\dots, X_{n}$ 为来自参数为 $p$ 的伯努利分布的独立同分布 (i.i.d.) 随机变量，其中 $p$ 是事件（记为 $X_i=1$）的概率。单个观测值 $X_i$ 的概率质量函数 (PMF) 由下式给出：\n$$ P(X_i = x_i; p) = p^{x_i} (1-p)^{1-x_i} \\quad \\text{for } x_i \\in \\{0, 1\\} $$\n参数 $p$ 代表一个概率，因此其自然参数空间是闭区间 $[0, 1]$。\n\n首先，我们构造似然函数 $L(p)$。由于观测值是独立同分布的，似然函数是各个概率质量函数的乘积：\n$$ L(p; x_1, \\dots, x_n) = \\prod_{i=1}^{n} P(X_i = x_i; p) = \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i} $$\n这可以通过合并指数来简化：\n$$ L(p) = p^{\\sum_{i=1}^{n} x_i} (1-p)^{\\sum_{i=1}^{n} (1-x_i)} = p^{\\sum x_i} (1-p)^{n - \\sum x_i} $$\n设 $S = \\sum_{i=1}^{n} X_i$ 为观测到的事件总数。$S$ 是 $p$ 的一个充分统计量。似然函数可以写成：\n$$ L(p) = p^{S} (1-p)^{n-S} $$\n为了找到使 $L(p)$ 最大化的 $p$ 值，计算上更方便的是最大化似然函数的自然对数，即对数似然，记为 $\\ell(p)$。由于自然对数是一个严格单调递增函数，使 $\\ell(p)$ 最大化的 $p$ 值也会使 $L(p)$ 最大化。\n$$ \\ell(p) = \\ln(L(p)) = \\ln(p^{S} (1-p)^{n-S}) = S \\ln(p) + (n-S) \\ln(1-p) $$\n对数似然函数在 $p \\in (0, 1)$ 上有定义。我们将首先找到解存在于此开区间内情况下的 MLE，然后分析边界点 $p=0$ 和 $p=1$。\n\n我们通过对 $\\ell(p)$ 关于 $p$ 求导并将导数设为零来找到最大值。\n$$ \\frac{d\\ell}{dp} = \\frac{S}{p} - \\frac{n-S}{1-p} $$\n将导数设为 $0$：\n$$ \\frac{S}{p} = \\frac{n-S}{1-p} $$\n$$ S(1-p) = p(n-S) $$\n$$ S - Sp = np - Sp $$\n$$ S = np $$\n$$ p = \\frac{S}{n} $$\n这个临界点 $\\hat{p} = \\frac{S}{n}$ 是样本均值 $\\bar{X}$。为验证这是一个最大值，我们检查二阶导数：\n$$ \\frac{d^2\\ell}{dp^2} = -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2} $$\n对于非边界情况，我们有 $0  S  n$。这意味着 $S > 0$ 且 $n-S > 0$。由于 $p^2 > 0$ 和 $(1-p)^2 > 0$，二阶导数中的两项都是负的。因此，对于所有 $p \\in (0, 1)$，$\\frac{d^2\\ell}{dp^2}  0$，这证实了对数似然函数是严格凹的。因此，临界点 $\\hat{p} = \\frac{S}{n}$ 是唯一的最大值点。在这种情况下，由于 $0  S  n$，我们有 $0  \\hat{p}  1$，所以 MLE 在两个参数空间 $(0, 1)$ 和 $[0, 1]$ 中都存在且唯一。\n\n现在，我们必须按要求分析边界情况。\n情况1：所有观测值都为 $0$。\n这对应于 $S = \\sum_{i=1}^{n} X_i = 0$。似然函数变为：\n$$ L(p) = p^0 (1-p)^{n-0} = (1-p)^n $$\n对于 $p \\in (0, 1)$，对数似然为 $\\ell(p) = n \\ln(1-p)$。\n在参数空间 $[0, 1]$ 上的分析：\n函数 $L(p) = (1-p)^n$ 在区间 $[0, 1]$ 上是 $p$ 的严格单调递减函数。一个在闭区间上的严格单调递减函数在区间的左端点处达到其唯一的最大值。因此，似然函数在 $p=0$ 时最大化。MLE 是 $\\hat{p} = 0$。这个值在参数空间 $[0, 1]$ 内。似然函数的最大值是 $L(0) = (1-0)^n = 1$。所以，对于空间 $[0, 1]$，存在唯一的 MLE，即 $\\hat{p}=0$。这与通用公式 $\\hat{p} = S/n = 0/n = 0$ 一致。\n\n在参数空间 $(0, 1)$ 上的分析：\n在开区间 $(0, 1)$ 上，函数 $L(p) = (1-p)^n$ 仍然是严格单调递减的。函数的上确界是 $\\sup_{p \\in (0, 1)} (1-p)^n = 1$，当 $p \\to 0^+$ 时趋近于此值。然而，对于开区间 $(0, 1)$ 内的任何 $p$，这个上确界都永远无法达到。对于任何提议的最大化者 $p^* \\in (0, 1)$，我们总能找到一个值 $p^{**} = p^*/2$，使得 $0  p^{**}  p^*$ 并且 $L(p^{**}) = (1-p^*/2)^n > (1-p^*)^n = L(p^*)$。因此，在参数空间 $(0, 1)$ 内不存在最大值。最大似然估计量不存在。\n\n情况2：所有观测值都为 $1$。\n这对应于 $S = \\sum_{i=1}^{n} X_i = n$。似然函数变为：\n$$ L(p) = p^n (1-p)^{n-n} = p^n $$\n对于 $p \\in (0, 1)$，对数似然为 $\\ell(p) = n \\ln(p)$。\n在参数空间 $[0, 1]$ 上的分析：\n函数 $L(p) = p^n$ 在区间 $[0, 1]$ 上是 $p$ 的严格单调递增函数。一个在闭区间上的严格单调递增函数在区间的右端点处达到其唯一的最大值。因此，似然函数在 $p=1$ 时最大化。MLE 是 $\\hat{p} = 1$。这个值在参数空间 $[0, 1]$ 内。似然函数的最大值是 $L(1) = 1^n = 1$。所以，对于空间 $[0, 1]$，存在唯一的 MLE，即 $\\hat{p}=1$。这与通用公式 $\\hat{p} = S/n = n/n = 1$ 一致。\n\n在参数空间 $(0, 1)$ 上的分析：\n在开区间 $(0, 1)$ 上，函数 $L(p) = p^n$ 仍然是严格单调递增的。函数的上确界是 $\\sup_{p \\in (0, 1)} p^n = 1$，当 $p \\to 1^-$ 时趋近于此值。然而，对于开区间 $(0, 1)$ 内的任何 $p$，这个上确界都永远无法达到。对于任何提议的最大化者 $p^* \\in (0, 1)$，我们总能找到一个值 $p^{**} = (p^*+1)/2$，使得 $p^*  p^{**}  1$ 并且 $L(p^{**}) = ((p^*+1)/2)^n > (p^*)^n = L(p^*)$。因此，在参数空间 $(0, 1)$ 内不存在最大值。最大似然估计量不存在。\n\n总之，当且仅当参数空间是闭区间 $[0, 1]$ 时，对于所有可能的观测集 $\\{X_1, \\dots, X_n\\}$，$p$ 的 MLE 都是良定、存在且唯一的。这是伯努利参数的自然且标准的选择。通用表达式 $\\hat{p} = \\frac{\\sum_{i=1}^{n} X_i}{n}$ 在此参数空间选择下涵盖了所有三种情况（内部、全为 $0$、全为 $1$）。",
            "answer": "$$\n\\boxed{\\frac{\\sum_{i=1}^{n} X_{i}}{n}}\n$$"
        },
        {
            "introduction": "在掌握了基础之后，我们现在转向统计学中无处不在的正态分布模型。这个练习将我们的技能从单参数的离散分布扩展到双参数的连续分布，你需要同时为均值 $\\mu$ 和方差 $\\sigma^2$ 寻找最大似然估计。更重要的是，通过这个过程，你将发现一个关于最大似然估计的关键性质：它并非总是无偏的。理解方差估计量的偏差 () 是从理论走向严谨统计实践的关键一步。",
            "id": "4969243",
            "problem": "一个生物医学实验室校准一种新的检测方法，该方法报告每个患者样本的连续生物标志物值。假设患者间的测量误差被建模为来自未知均值偏差 $\\mu$ 和未知方差 $\\sigma^{2}$ 的正态分布 $N(\\mu,\\sigma^{2})$ 的独立同分布（i.i.d.）抽样。您观察到一次校准运行的值为 $x_{1},\\dots,x_{n}$，其中 $n \\geq 2$。从独立同分布正态观测的似然定义以及期望和方差的基本性质出发，推导最大似然估计量（Maximum Likelihood Estimators, MLE） $\\hat{\\mu}$ 和 $\\hat{\\sigma}^{2}$，并用 $x_{1},\\dots,x_{n}$ 表示。然后，相对于通常的无偏方差估计量 $s^{2}_{\\text{unb}}=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}$，确定最大似然估计量 $\\hat{\\sigma}^{2}$ 的确切偏差，其定义为 $\\operatorname{Bias}(\\hat{\\sigma}^{2})=\\mathbb{E}[\\hat{\\sigma}^{2}]-\\sigma^{2}$。请用 $n$ 和 $\\sigma^{2}$ 将偏差的最终答案表示为单个闭式表达式。无需进行数值舍入。此处 $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$ 表示样本均值。",
            "solution": "求解过程主要分为两部分：首先，推导正态分布均值 $\\mu$ 和方差 $\\sigma^2$ 的最大似然估计量 (MLE)；其次，计算方差的最大似然估计量 $\\hat{\\sigma}^2$ 的偏差。\n\n设 $x_1, \\dots, x_n$ 是来自正态分布 $N(\\mu, \\sigma^2)$ 的一个独立同分布 (i.i.d.) 样本。单个观测值 $x_i$ 的概率密度函数 (PDF) 由下式给出：\n$$f(x_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)$$\n\n由于观测值的独立性，似然函数 $L(\\mu, \\sigma^2 | \\mathbf{x})$ 是这 $n$ 个观测值各自概率密度函数的乘积：\n$$L(\\mu, \\sigma^2 | \\mathbf{x}) = \\prod_{i=1}^{n} f(x_i | \\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)$$\n$$L(\\mu, \\sigma^2 | \\mathbf{x}) = \\left(2\\pi\\sigma^2\\right)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right)$$\n\n为了简化最大化过程，我们使用似然函数的自然对数，即对数似然函数 $\\mathcal{L}(\\mu, \\sigma^2 | \\mathbf{x})$：\n$$\\mathcal{L} = \\ln[L(\\mu, \\sigma^2 | \\mathbf{x})] = \\ln\\left[ \\left(2\\pi\\sigma^2\\right)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right) \\right]$$\n$$\\mathcal{L} = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\n$$\\mathcal{L} = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\n\n为了求最大似然估计量，我们将 $\\mathcal{L}$ 分别对 $\\mu$ 和 $\\sigma^2$ 求偏导数，并令其等于零。\n\n首先，对于均值 $\\mu$：\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left[ -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right]$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(x_i - \\mu)(-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)$$\n将导数设为零，求解 $\\hat{\\mu}$：\n$$\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\hat{\\mu}) = 0 \\implies \\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} \\hat{\\mu} = 0$$\n$$\\sum_{i=1}^{n} x_i - n\\hat{\\mu} = 0 \\implies \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\bar{x}$$\n$\\mu$ 的最大似然估计量是样本均值，$\\hat{\\mu} = \\bar{x}$。\n\n接下来，对于方差 $\\sigma^2$。直接对 $\\sigma^2$ 求导会更方便。\n$$\\frac{\\partial \\mathcal{L}}{\\partial (\\sigma^2)} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left[ -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right]$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial (\\sigma^2)} = -\\frac{n}{2\\sigma^2} - \\left(-\\frac{1}{2(\\sigma^2)^2}\\right) \\sum_{i=1}^{n} (x_i - \\mu)^2 = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\n将导数设为零，并代入估计量 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^2$：\n$$-\\frac{n}{2\\hat{\\sigma}^2} + \\frac{1}{2(\\hat{\\sigma}^2)^2} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2 = 0$$\n两边乘以 $2(\\hat{\\sigma}^2)^2$ 得：\n$$-n\\hat{\\sigma}^2 + \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2 = 0$$\n代入 $\\hat{\\mu} = \\bar{x}$，我们求解 $\\hat{\\sigma}^2$：\n$$\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2$$\n\n现在，我们确定该估计量的偏差。偏差定义为 $\\operatorname{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2$。我们必须计算 $\\hat{\\sigma}^2$ 的期望。\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\frac{1}{n} \\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right]$$\n我们展开平方和项：\n$$\\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\sum_{i=1}^{n} (x_i^2 - 2x_i\\bar{x} + \\bar{x}^2) = \\sum_{i=1}^{n} x_i^2 - 2\\bar{x}\\sum_{i=1}^{n} x_i + n\\bar{x}^2$$\n由于 $\\sum_{i=1}^{n} x_i = n\\bar{x}$，上式可简化为：\n$$\\sum_{i=1}^{n} x_i^2 - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2 = \\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2$$\n现在我们对该表达式求期望：\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2\\right] = \\sum_{i=1}^{n} \\mathbb{E}[x_i^2] - n\\mathbb{E}[\\bar{x}^2]$$\n我们使用一般性质 $\\mathbb{E}[Y^2] = \\operatorname{Var}(Y) + (\\mathbb{E}[Y])^2$。\n对于每个 $x_i$，$\\mathbb{E}[x_i] = \\mu$ 且 $\\operatorname{Var}(x_i) = \\sigma^2$，所以 $\\mathbb{E}[x_i^2] = \\sigma^2 + \\mu^2$。\n对于样本均值 $\\bar{x}$，$\\mathbb{E}[\\bar{x}] = \\mathbb{E}[\\frac{1}{n}\\sum x_i] = \\mu$ 且 $\\operatorname{Var}(\\bar{x}) = \\operatorname{Var}(\\frac{1}{n}\\sum x_i) = \\frac{1}{n^2}\\sum\\operatorname{Var}(x_i) = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$。\n因此，$\\mathbb{E}[\\bar{x}^2] = \\operatorname{Var}(\\bar{x}) + (\\mathbb{E}[\\bar{x}])^2 = \\frac{\\sigma^2}{n} + \\mu^2$。\n将这些期望代回：\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\sum_{i=1}^{n} (\\sigma^2 + \\mu^2) - n\\left(\\frac{\\sigma^2}{n} + \\mu^2\\right)$$\n$$= n(\\sigma^2 + \\mu^2) - (\\sigma^2 + n\\mu^2) = n\\sigma^2 + n\\mu^2 - \\sigma^2 - n\\mu^2 = (n-1)\\sigma^2$$\n现在我们求 $\\hat{\\sigma}^2$ 的期望：\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\frac{1}{n} \\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\frac{1}{n}(n-1)\\sigma^2$$\n最后，我们计算偏差：\n$$\\operatorname{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\n方差的最大似然估计量的偏差为 $-\\frac{1}{n}\\sigma^2$。这表明最大似然估计量是一个有偏估计量，它倾向于低估真实方差。随着样本量 $n$ 的增加，该偏差会减小。",
            "answer": "$$ \\boxed{-\\frac{1}{n}\\sigma^2} $$"
        },
        {
            "introduction": "在许多现实世界的生物统计学应用中，例如使用逻辑回归来预测疾病风险，最大似然估计量并没有简单的封闭解。这最后一个练习将带你从理论走向实践，探索如何通过数值优化方法求解这些模型。我们将聚焦于逻辑回归模型，并推导求解其参数的牛顿-拉弗森（Newton-Raphson）算法的完整步骤，最后通过一个具体的数据集完成一次迭代计算 ()。这个练习将为你展示现代统计软件内部求解复杂模型的核心机制。",
            "id": "4969348",
            "problem": "一项临床研究旨在探究术后并发症的概率与某一标准化生物标志物之间的函数关系。设 $y_i \\in \\{0,1\\}$ 表示患者 $i$ 是否出现并发症，设 $x_i \\in \\mathbb{R}$ 表示该生物标志物的值。假设一个参数向量为 $\\theta \\in \\mathbb{R}^{p}$ 的参数模型通过最大似然估计（MLE）的原理进行拟合，该原理选择 $\\theta$ 以最大化观测数据的对数似然 $l(\\theta)$。从得分函数 $U(\\theta)$（定义为 $l(\\theta)$ 的梯度）和 $U(\\theta)$ 的雅可比矩阵 $J(\\theta)$ 的定义出发，推导用于迭代求解得分方程 $U(\\theta)=0$ 以获得MLE的牛顿-拉弗森（Newton-Raphson）更新规则。\n\n然后，将问题具体化到带单个预测变量的二元逻辑回归，其中对于每个患者\n$$\np_i(\\beta_0,\\beta_1) \\equiv \\Pr(y_i=1 \\mid x_i) = \\frac{1}{1+\\exp\\!\\big(-\\eta_i\\big)}, \\quad \\text{其中 } \\eta_i = \\beta_0 + \\beta_1 x_i,\n$$\n且独立观测值的对数似然为\n$$\nl(\\beta_0,\\beta_1) = \\sum_{i=1}^{n} \\left[ y_i \\ln p_i + (1-y_i)\\ln(1-p_i) \\right].\n$$\n使用第一性原理，并且不引入任何快捷公式，推导该模型的得分向量 $U(\\beta_0,\\beta_1)$ 和雅可比矩阵 $J(\\beta_0,\\beta_1)$。\n\n最后，从初始猜测值 $\\beta^{(0)} = (\\beta_0^{(0)}, \\beta_1^{(0)})^{\\top} = (0,0)^{\\top}$ 开始，对以下包含 $n=6$ 名患者的数据集应用一次牛顿-拉弗森迭代：\n- 生物标志物值 $x = (-2,\\, -1,\\, 0,\\, 1,\\, 2,\\, 0)$，\n- 结局 $y = (0,\\, 0,\\, 0,\\, 1,\\, 1,\\, 0)$。\n\n计算一次牛顿-拉弗森步骤后更新的参数向量 $\\beta^{(1)}$。将你的最终答案表示为一个 $1 \\times 2$ 的行矩阵，并将每个元素四舍五入到四位有效数字。",
            "solution": "此问题分为三个部分：首先，是最大似然估计（MLE）的牛顿-拉弗森更新规则的一般性推导；其次，是二元逻辑回归模型的得分向量和雅可比矩阵的具体推导；最后，是对给定数据集应用一次牛顿-拉弗森步骤的数值计算。\n\n### 第一部分：牛顿-拉弗森更新规则的推导\n\n最大似然估计（MLE）的目标是找到使对数似然函数 $l(\\theta)$ 最大化的参数向量 $\\theta$。这个最大值是在对数似然函数的梯度为零的临界点上找到的。得分函数 $U(\\theta)$ 定义为该梯度：\n$$\nU(\\theta) = \\nabla l(\\theta)\n$$\n因此，找到MLE $\\hat\\theta$ 等价于求解得分方程 $U(\\hat\\theta) = 0$。\n\n牛顿-拉弗森法是一种迭代求根算法。为了求解 $U(\\theta) = 0$，我们从一个初始猜测值 $\\theta^{(k)}$ 开始，旨在找到一个更好的近似值 $\\theta^{(k+1)}$。我们通过在 $\\theta^{(k)}$ 附近的一阶泰勒级数展开来近似 $U(\\theta)$：\n$$\nU(\\theta) \\approx U(\\theta^{(k)}) + J(\\theta^{(k)}) (\\theta - \\theta^{(k)})\n$$\n其中 $J(\\theta^{(k)})$ 是得分函数 $U(\\theta)$ 在 $\\theta^{(k)}$ 处求值的雅可比矩阵。雅可比矩阵的元素由 $J_{ij} = \\frac{\\partial U_i}{\\partial \\theta_j} = \\frac{\\partial^2 l}{\\partial \\theta_j \\partial \\theta_i}$ 给出。请注意，这是对数似然函数 $l(\\theta)$ 的海森矩阵（Hessian matrix）。\n\n为了找到下一个迭代值 $\\theta^{(k+1)}$，我们在此近似中令 $U(\\theta^{(k+1)}) = 0$：\n$$\n0 \\approx U(\\theta^{(k)}) + J(\\theta^{(k)}) (\\theta^{(k+1)} - \\theta^{(k)})\n$$\n重新整理此方程以求解更新步长 $(\\theta^{(k+1)} - \\theta^{(k)})$，我们得到：\n$$\nJ(\\theta^{(k)}) (\\theta^{(k+1)} - \\theta^{(k)}) = -U(\\theta^{(k)})\n$$\n假设雅可比矩阵 $J(\\theta^{(k)})$ 是可逆的，我们可以乘以其逆矩阵 $[J(\\theta^{(k)})]^{-1}$：\n$$\n\\theta^{(k+1)} - \\theta^{(k)} = -[J(\\theta^{(k)})]^{-1} U(\\theta^{(k)})\n$$\n这就导出了迭代求解MLE的牛顿-拉弗森更新规则：\n$$\n\\theta^{(k+1)} = \\theta^{(k)} - [J(\\theta^{(k)})]^{-1} U(\\theta^{(k)})\n$$\n\n### 第二部分：逻辑回归的得分与雅可比矩阵\n\n我们现在将问题具体化到参数为 $\\beta = (\\beta_0, \\beta_1)^{\\top}$ 的二元逻辑回归模型。对于 $n$ 个独立观测值，对数似然由下式给出：\n$$\nl(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} \\left[ y_i \\ln p_i + (1-y_i)\\ln(1-p_i) \\right]\n$$\n其中 $p_i = \\Pr(y_i=1 \\mid x_i) = \\frac{1}{1+\\exp(-\\eta_i)}$ 且 $\\eta_i = \\beta_0 + \\beta_1 x_i$。\n\n首先，我们求 $p_i$ 相对于 $\\eta_i$ 的一个关键导数：\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( (1+\\exp(-\\eta_i))^{-1} \\right) = -1 \\cdot (1+\\exp(-\\eta_i))^{-2} \\cdot (-\\exp(-\\eta_i)) = \\frac{\\exp(-\\eta_i)}{(1+\\exp(-\\eta_i))^2}\n$$\n这可以重写为：\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = \\frac{1}{1+\\exp(-\\eta_i)} \\cdot \\frac{\\exp(-\\eta_i)}{1+\\exp(-\\eta_i)} = p_i \\cdot \\left( \\frac{1+\\exp(-\\eta_i)-1}{1+\\exp(-\\eta_i)} \\right) = p_i \\cdot \\left( 1 - \\frac{1}{1+\\exp(-\\eta_i)} \\right) = p_i(1-p_i)\n$$\n\n得分向量 $U(\\beta_0, \\beta_1)$ 的分量是 $\\frac{\\partial l}{\\partial \\beta_0}$ 和 $\\frac{\\partial l}{\\partial \\beta_1}$。我们使用链式法则：\n$$\n\\frac{\\partial l}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n第 $i$ 个对数似然项关于 $p_i$ 的导数是：\n$$\n\\frac{\\partial l_i}{\\partial p_i} = \\frac{y_i}{p_i} - \\frac{1-y_i}{1-p_i} = \\frac{y_i(1-p_i) - (1-y_i)p_i}{p_i(1-p_i)} = \\frac{y_i - p_i}{p_i(1-p_i)}\n$$\n结合这些结果：\n$$\n\\frac{\\partial l_i}{\\partial \\beta_j} = \\left( \\frac{y_i - p_i}{p_i(1-p_i)} \\right) \\cdot (p_i(1-p_i)) \\cdot \\frac{\\partial \\eta_i}{\\partial \\beta_j} = (y_i - p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n$\\eta_i$ 的导数是 $\\frac{\\partial \\eta_i}{\\partial \\beta_0} = 1$ 和 $\\frac{\\partial \\eta_i}{\\partial \\beta_1} = x_i$。\n得分向量的分量是：\n$$\nU_0 = \\frac{\\partial l}{\\partial \\beta_0} = \\sum_{i=1}^{n} (y_i - p_i) \\cdot 1 = \\sum_{i=1}^{n} (y_i - p_i)\n$$\n$$\nU_1 = \\frac{\\partial l}{\\partial \\beta_1} = \\sum_{i=1}^{n} (y_i - p_i) \\cdot x_i = \\sum_{i=1}^{n} x_i(y_i - p_i)\n$$\n所以得分向量是 $U(\\beta_0, \\beta_1) = \\begin{pmatrix} \\sum_{i=1}^{n} (y_i - p_i) \\\\ \\sum_{i=1}^{n} x_i(y_i - p_i) \\end{pmatrix}$。\n\n接下来，我们推导雅可比矩阵 $J(\\beta_0, \\beta_1)$，其元素为 $J_{jk} = \\frac{\\partial U_j}{\\partial \\beta_k} = \\frac{\\partial^2 l}{\\partial \\beta_k \\partial \\beta_j}$。\n$$\n\\frac{\\partial^2 l}{\\partial \\beta_k \\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^{n} (y_i - p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\right) = \\sum_{i=1}^{n} \\left( -\\frac{\\partial p_i}{\\partial \\beta_k} \\right) \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n再次使用链式法则，$\\frac{\\partial p_i}{\\partial \\beta_k} = \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = p_i(1-p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k}$。\n代入此式可得雅可比矩阵元素的一般形式：\n$$\nJ_{jk} = -\\sum_{i=1}^{n} p_i(1-p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n现在我们计算具体元素：\n$$\nJ_{00} = \\frac{\\partial^2 l}{\\partial \\beta_0^2} = -\\sum_{i=1}^{n} p_i(1-p_i) (1)(1) = -\\sum_{i=1}^{n} p_i(1-p_i)\n$$\n$$\nJ_{01} = \\frac{\\partial^2 l}{\\partial \\beta_1 \\partial \\beta_0} = -\\sum_{i=1}^{n} p_i(1-p_i) (x_i)(1) = -\\sum_{i=1}^{n} x_i p_i(1-p_i)\n$$\n根据对称性（克莱罗定理，Clairaut's theorem），$J_{10} = J_{01}$。\n$$\nJ_{11} = \\frac{\\partial^2 l}{\\partial \\beta_1^2} = -\\sum_{i=1}^{n} p_i(1-p_i) (x_i)(x_i) = -\\sum_{i=1}^{n} x_i^2 p_i(1-p_i)\n$$\n因此，雅可比矩阵是 $J(\\beta_0, \\beta_1) = \\begin{pmatrix} -\\sum p_i(1-p_i)  -\\sum x_i p_i(1-p_i) \\\\ -\\sum x_i p_i(1-p_i)  -\\sum x_i^2 p_i(1-p_i) \\end{pmatrix}$。\n\n### 第三部分：数值应用\n\n我们从 $\\beta^{(0)} = (\\beta_0^{(0)}, \\beta_1^{(0)})^{\\top} = (0,0)^{\\top}$ 开始，应用一次牛顿-拉弗森步骤。\n数据集为 $n=6$ 时的 $x = (-2, -1, 0, 1, 2, 0)$ 和 $y = (0, 0, 0, 1, 1, 0)$。\n\n首先，在 $\\beta^{(0)}$ 处评估概率。对于任何 $x_i$，$\\eta_i^{(0)} = \\beta_0^{(0)} + \\beta_1^{(0)} x_i = 0 + 0 \\cdot x_i = 0$。\n每个患者的概率为 $p_i^{(0)} = \\frac{1}{1+\\exp(-0)} = \\frac{1}{1+1} = 0.5$。\n\n_接下来，计算得分向量 $U(\\beta^{(0)})$：_\n我们需要以下总和：\n$\\sum_{i=1}^6 y_i = 0+0+0+1+1+0 = 2$。\n$\\sum_{i=1}^6 x_i = -2-1+0+1+2+0 = 0$。\n$\\sum_{i=1}^6 x_i y_i = (-2)(0) + (-1)(0) + (0)(0) + (1)(1) + (2)(1) + (0)(0) = 3$。\n得分向量的分量是：\n$U_0^{(0)} = \\sum_{i=1}^6 (y_i - p_i^{(0)}) = \\sum y_i - \\sum p_i^{(0)} = 2 - 6 \\times 0.5 = 2 - 3 = -1$。\n$U_1^{(0)} = \\sum_{i=1}^6 x_i (y_i - p_i^{(0)}) = \\sum x_i y_i - p_i^{(0)} \\sum x_i = 3 - 0.5 \\times 0 = 3$。\n所以，$U(\\beta^{(0)}) = \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}$。\n\n_接下来，计算雅可比矩阵 $J(\\beta^{(0)})$：_\n对于所有 $i$，$p_i^{(0)}(1-p_i^{(0)}) = 0.5 \\times (1-0.5) = 0.25$。\n我们需要 $x_i$ 的平方和：\n$\\sum_{i=1}^6 x_i^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 + 0^2 = 4+1+0+1+4+0 = 10$。\n雅可比矩阵的元素是：\n$J_{00}^{(0)} = -\\sum p_i^{(0)}(1-p_i^{(0)}) = -6 \\times 0.25 = -1.5$。\n$J_{01}^{(0)} = -\\sum x_i p_i^{(0)}(1-p_i^{(0)}) = -0.25 \\sum x_i = -0.25 \\times 0 = 0$。\n$J_{11}^{(0)} = -\\sum x_i^2 p_i^{(0)}(1-p_i^{(0)}) = -0.25 \\sum x_i^2 = -0.25 \\times 10 = -2.5$。\n所以，$J(\\beta^{(0)}) = \\begin{pmatrix} -1.5  0 \\\\ 0  -2.5 \\end{pmatrix}$。\n\n最后，执行更新以找到 $\\beta^{(1)} = (\\beta_0^{(1)}, \\beta_1^{(1)})^{\\top}$：\n$$\n\\beta^{(1)} = \\beta^{(0)} - [J(\\beta^{(0)})]^{-1} U(\\beta^{(0)})\n$$\n对角雅可比矩阵的逆矩阵是：\n$$\n[J(\\beta^{(0)})]^{-1} = \\begin{pmatrix} 1/(-1.5)  0 \\\\ 0  1/(-2.5) \\end{pmatrix} = \\begin{pmatrix} -2/3  0 \\\\ 0  -2/5 \\end{pmatrix}\n$$\n现在，我们计算 $\\beta^{(1)}$：\n$$\n\\beta^{(1)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} -2/3  0 \\\\ 0  -2/5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix} = -\\begin{pmatrix} (-2/3)(-1) + (0)(3) \\\\ (0)(-1) + (-2/5)(3) \\end{pmatrix} = -\\begin{pmatrix} 2/3 \\\\ -6/5 \\end{pmatrix} = \\begin{pmatrix} -2/3 \\\\ 6/5 \\end{pmatrix}\n$$\n用十进制形式表示，即 $\\beta^{(1)} = \\begin{pmatrix} -0.6666... \\\\ 1.2 \\end{pmatrix}$。\n将每个条目四舍五入到四位有效数字，我们得到：\n$\\beta_0^{(1)} \\approx -0.6667$\n$\\beta_1^{(1)} = 1.200$\n\n更新后的参数向量为 $\\beta^{(1)} \\approx (-0.6667, 1.200)^{\\top}$。按要求表示为 $1 \\times 2$ 的行矩阵：\n$\\begin{pmatrix} -0.6667  1.200 \\end{pmatrix}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -0.6667  1.200 \\end{pmatrix}}\n$$"
        }
    ]
}