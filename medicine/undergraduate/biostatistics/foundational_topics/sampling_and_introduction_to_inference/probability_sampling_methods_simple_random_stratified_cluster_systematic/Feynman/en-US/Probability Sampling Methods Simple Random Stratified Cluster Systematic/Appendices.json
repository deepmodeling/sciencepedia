{
    "hands_on_practices": [
        {
            "introduction": "Deciding to use stratified sampling is only the first step; the next crucial decision is how to distribute your sample across the different strata. This exercise provides a hands-on comparison between two fundamental strategies: proportional allocation, where the sample size in a stratum is proportional to its population size, and Neyman allocation, which aims to minimize variance. By calculating the variance for each method, you will quantify the efficiency gained by using Neyman's optimal approach, which cleverly directs more sampling effort towards strata that are larger and more internally heterogeneous .",
            "id": "4942771",
            "problem": "A biostatistics team is planning a stratified study to estimate the population mean of a continuous biomarker across three strata defined by clinical risk categories. The strata have population sizes $N_{1} = 1200$, $N_{2} = 800$, and $N_{3} = 500$, giving a total population size $N = 2500$. The stratum population standard deviations for the biomarker are $S_{1} = 12$, $S_{2} = 20$, and $S_{3} = 8$. The total sample size is fixed at $n = 300$, and sampling within each stratum is by Simple Random Sampling Without Replacement (SRSWOR).\n\nUsing only first principles and well-tested facts about stratified sampling, compute the ratio\n$$\\frac{\\operatorname{Var}_{\\text{prop}}}{\\operatorname{Var}_{\\text{Neyman}}}$$\nwhere $\\operatorname{Var}_{\\text{prop}}$ is the variance of the stratified estimator of the overall mean under proportional allocation $n_{h} = n \\cdot N_{h}/N$, and $\\operatorname{Var}_{\\text{Neyman}}$ is the variance under Neyman allocation $n_{h} \\propto N_{h} S_{h}$, both evaluated with the finite population correction within strata. For the purpose of calculation, treat $n_{h}$ as real-valued (ignore integer rounding). Round your final ratio to four significant figures. Then briefly interpret this ratio in terms of the efficiency gain achieved by Neyman allocation relative to proportional allocation, expressing any fractional reduction as a decimal (do not use the percentage sign).",
            "solution": "The problem is valid as it is scientifically grounded in the principles of biostatistics, self-contained, and well-posed. All necessary data for calculation are provided, and the objective is clearly defined. We can proceed with the solution.\n\nThe objective is to compute the ratio $\\frac{\\operatorname{Var}_{\\text{prop}}}{\\operatorname{Var}_{\\text{Neyman}}}$, where $\\operatorname{Var}_{\\text{prop}}$ and $\\operatorname{Var}_{\\text{Neyman}}$ are the variances of the stratified mean estimator under proportional and Neyman allocation, respectively.\n\nThe general formula for the variance of the stratified mean estimator, $\\bar{y}_{st} = \\sum_{h=1}^{L} W_h \\bar{y}_h$, with a finite population correction (FPC) is:\n$$ \\operatorname{Var}(\\bar{y}_{st}) = \\sum_{h=1}^{L} W_h^2 \\operatorname{Var}(\\bar{y}_h) = \\sum_{h=1}^{L} W_h^2 \\left(1-\\frac{n_h}{N_h}\\right) \\frac{S_h^2}{n_h} $$\nwhere $L$ is the number of strata, $W_h = N_h/N$ is the weight of stratum $h$, $N_h$ is the population size of stratum $h$, $N$ is the total population size, $n_h$ is the sample size in stratum $h$, and $S_h$ is the population standard deviation in stratum $h$. This can be rewritten as:\n$$ \\operatorname{Var}(\\bar{y}_{st}) = \\sum_{h=1}^{L} W_h^2 \\left(\\frac{S_h^2}{n_h} - \\frac{S_h^2}{N_h}\\right) = \\sum_{h=1}^{L} \\frac{W_h^2 S_h^2}{n_h} - \\sum_{h=1}^{L} \\frac{W_h^2 S_h^2}{N_h} $$\nSince $W_h = N_h/N$, the second term simplifies to $\\frac{1}{N^2} \\sum_{h=1}^{L} N_h S_h^2$, or equivalently $\\frac{1}{N} \\sum_{h=1}^{L} W_h S_h^2$. This term is constant regardless of the sample allocation method.\n\nThe given data are:\nTotal population size $N = 2500$.\nTotal sample size $n = 300$.\nStratum $1$: $N_1 = 1200$, $S_1 = 12$.\nStratum $2$: $N_2 = 800$, $S_2 = 20$.\nStratum $3$: $N_3 = 500$, $S_3 = 8$.\n\nFirst, we calculate the stratum weights $W_h$:\n$W_1 = \\frac{N_1}{N} = \\frac{1200}{2500} = 0.48$\n$W_2 = \\frac{N_2}{N} = \\frac{800}{2500} = 0.32$\n$W_3 = \\frac{N_3}{N} = \\frac{500}{2500} = 0.20$\n\nNext, we evaluate the variance for each allocation scheme.\n\n**1. Proportional Allocation**\nUnder proportional allocation, the sample size in each stratum is $n_h^{\\text{prop}} = n \\frac{N_h}{N} = n W_h$.\n$n_1^{\\text{prop}} = 300 \\cdot 0.48 = 144$\n$n_2^{\\text{prop}} = 300 \\cdot 0.32 = 96$\n$n_3^{\\text{prop}} = 300 \\cdot 0.20 = 60$\nFor proportional allocation, the FPC term $(1 - n_h/N_h)$ is constant across strata: $1 - \\frac{n}{N} = 1 - \\frac{300}{2500} = 1 - 0.12 = 0.88$.\nThe variance formula simplifies to:\n$$ \\operatorname{Var}_{\\text{prop}} = \\left(1 - \\frac{n}{N}\\right) \\sum_{h=1}^{L} \\frac{W_h^2 S_h^2}{n_h^{\\text{prop}}} = \\left(1 - \\frac{n}{N}\\right) \\sum_{h=1}^{L} \\frac{W_h^2 S_h^2}{n W_h} = \\frac{1}{n}\\left(1 - \\frac{n}{N}\\right) \\sum_{h=1}^{L} W_h S_h^2 $$\nWe calculate the term $\\sum W_h S_h^2$:\n$$ \\sum_{h=1}^{3} W_h S_h^2 = (0.48)(12^2) + (0.32)(20^2) + (0.20)(8^2) $$\n$$ = (0.48)(144) + (0.32)(400) + (0.20)(64) = 69.12 + 128 + 12.8 = 209.92 $$\nNow we calculate $\\operatorname{Var}_{\\text{prop}}$:\n$$ \\operatorname{Var}_{\\text{prop}} = \\frac{1}{300}\\left(1 - \\frac{300}{2500}\\right) (209.92) = \\frac{0.88}{300}(209.92) = \\frac{184.7296}{300} \\approx 0.61576533 $$\n\n**2. Neyman Allocation**\nUnder Neyman allocation, the sample size is $n_h^{\\text{Neyman}} = n \\frac{N_h S_h}{\\sum_{k=1}^{L} N_k S_k}$.\nFirst, calculate the denominator $\\sum N_k S_k$:\n$$ \\sum_{k=1}^{3} N_k S_k = (1200)(12) + (800)(20) + (500)(8) = 14400 + 16000 + 4000 = 34400 $$\nThe allocatable sample sizes (as real values) are:\n$n_1^{\\text{Neyman}} = 300 \\cdot \\frac{14400}{34400} = \\frac{5400}{43} \\approx 125.58$\n$n_2^{\\text{Neyman}} = 300 \\cdot \\frac{16000}{34400} = \\frac{6000}{43} \\approx 139.53$\n$n_3^{\\text{Neyman}} = 300 \\cdot \\frac{4000}{34400} = \\frac{1500}{43} \\approx 34.88$\nThe variance under Neyman allocation can be calculated using a simplified formula that incorporates the FPC:\n$$ \\operatorname{Var}_{\\text{Neyman}} = \\frac{1}{n}\\left(\\sum_{h=1}^{L} W_h S_h\\right)^2 - \\frac{1}{N}\\sum_{h=1}^{L} W_h S_h^2 $$\nWe need the terms $\\sum W_h S_h$ and $\\sum W_h S_h^2$. We already found $\\sum W_h S_h^2 = 209.92$.\nLet's find $\\sum W_h S_h$:\n$$ \\sum_{h=1}^{3} W_h S_h = (0.48)(12) + (0.32)(20) + (0.20)(8) = 5.76 + 6.4 + 1.6 = 13.76 $$\nNow we calculate $\\operatorname{Var}_{\\text{Neyman}}$:\n$$ \\operatorname{Var}_{\\text{Neyman}} = \\frac{1}{300}(13.76)^2 - \\frac{1}{2500}(209.92) $$\n$$ = \\frac{189.3376}{300} - \\frac{209.92}{2500} = 0.63112533... - 0.083968 = 0.54715733... $$\n\n**3. Compute the Ratio**\nNow we compute the ratio $\\frac{\\operatorname{Var}_{\\text{prop}}}{\\operatorname{Var}_{\\text{Neyman}}}$:\n$$ \\text{Ratio} = \\frac{0.61576533...}{0.54715733...} \\approx 1.125390 $$\nRounding to four significant figures, the ratio is $1.125$.\n\n**Interpretation**\nThe ratio of the variances, $\\frac{\\operatorname{Var}_{\\text{prop}}}{\\operatorname{Var}_{\\text{Neyman}}} \\approx 1.125$, quantifies the relative efficiency of Neyman allocation compared to proportional allocation. A ratio greater than $1$ indicates that Neyman allocation is more efficient, resulting in a smaller variance for the same total sample size. The efficiency gain can be expressed as the fractional reduction in variance when using Neyman allocation instead of proportional allocation. This fractional reduction is given by $1 - \\frac{\\operatorname{Var}_{\\text{Neyman}}}{\\operatorname{Var}_{\\text{prop}}} = 1 - \\frac{1}{\\text{Ratio}}$. Using the unrounded ratio for better precision:\n$$ \\text{Fractional Reduction} = 1 - \\frac{1}{1.125390...} \\approx 1 - 0.88858 = 0.11142 $$\nThus, shifting from proportional to Neyman allocation for this study design would reduce the variance of the estimated population mean by a fraction of approximately $0.1114$. This gain in precision is achieved because Neyman allocation directs more sampling effort to strata that are larger (larger $N_h$) and more variable (larger $S_h$), which is the optimal strategy for minimizing the overall variance of the estimator.",
            "answer": "$$\\boxed{1.125}$$"
        },
        {
            "introduction": "Cluster sampling is often a practical necessity, saving time and resources, but it comes with a critical statistical subtlety: the intracluster correlation (ICC). Measurements from units within the same cluster are rarely independent, and ignoring this fact can lead to dangerously misleading conclusions. This practice problem demonstrates the severe consequences of this oversight by calculating the *actual* coverage of a confidence interval that was naively constructed assuming simple random sampling . You will see firsthand how positive correlation inflates variance, a phenomenon known as the design effect, and undermines the reliability of your statistical inference.",
            "id": "4942744",
            "problem": "A public health team conducts a multi-center biostatistics study to estimate the population mean of a continuous biomarker measured on patients. The study uses one-stage cluster sampling: from a large frame of clinics, $b$ clinics are selected by probability sampling, and then $m$ patients are measured within each selected clinic, yielding a total sample size $n = b m$. Because patients share clinic-level environments, measurements from patients within the same clinic are positively correlated. Let the Intracluster Correlation Coefficient (ICC) be denoted by $\\rho$, defined as $\\rho = \\frac{\\operatorname{Cov}(Y_{ci}, Y_{cj})}{\\operatorname{Var}(Y_{ci})}$ for $i \\neq j$ within the same clinic $c$, and assume $\\operatorname{Var}(Y_{ci}) = \\sigma^{2}$ is finite and constant across all units, with $\\operatorname{Cov}(Y_{ci}, Y_{dj}) = 0$ for $c \\neq d$. The analyst, however, mistakenly treats the sample as a simple random sample and reports a two-sided confidence interval with nominal coverage $0.95$ using the large-sample normal critical value $z_{0.975} \\approx 1.96$ and the independence-based standard error formula.\n\nStarting from the definitions of variance and covariance and the structure of the one-stage cluster sample described above, derive the variance of the sample mean under clustering in terms of $n$, $b$, $m$, $\\sigma^{2}$, and $\\rho$ and relate it to the variance under simple random sampling by a multiplicative factor. Then, using this relationship, determine the actual coverage probability of the analyst’s naive two-sided confidence interval that ignores clustering when $n=240$, $b=24$, and $\\rho=0.06$. Assume $m$ is constant across clusters, the Central Limit Theorem (CLT) applies so that the sampling distribution of the mean is approximately normal, and ignore finite population corrections.\n\nProvide your final answer as a single decimal number representing the actual coverage probability. Round your answer to four significant figures and do not use a percentage sign.",
            "solution": "We are asked to quantify the effect of clustering on the variance of the sample mean and its implications for the coverage probability of a naive confidence interval that assumes independence. We begin with the sample structure: there are $b$ clusters (clinics), each contributing $m$ units (patients), so the total sample size is $n = b m$. Denote the measurements by $Y_{c k}$ for cluster $c \\in \\{1,\\dots,b\\}$ and unit $k \\in \\{1,\\dots,m\\}$, and let the sample mean be\n$$\n\\bar{Y} \\;=\\; \\frac{1}{n} \\sum_{c=1}^{b} \\sum_{k=1}^{m} Y_{c k}.\n$$\nWe assume $\\operatorname{Var}(Y_{c k}) = \\sigma^{2}$ for all $c,k$, $\\operatorname{Cov}(Y_{c i}, Y_{c j}) = \\rho \\sigma^{2}$ for $i \\neq j$ within the same cluster $c$, and $\\operatorname{Cov}(Y_{c i}, Y_{d j}) = 0$ for $c \\neq d$.\n\nBy the properties of variance and covariance,\n$$\n\\operatorname{Var}\\!\\left(\\sum_{c=1}^{b} \\sum_{k=1}^{m} Y_{c k}\\right)\n= \\sum_{c=1}^{b} \\operatorname{Var}\\!\\left(\\sum_{k=1}^{m} Y_{c k}\\right)\n$$\nbecause cross-cluster covariances are zero. For a fixed cluster $c$,\n$$\n\\operatorname{Var}\\!\\left(\\sum_{k=1}^{m} Y_{c k}\\right)\n= \\sum_{k=1}^{m} \\operatorname{Var}(Y_{c k}) \\;+\\; 2 \\sum_{1 \\leq i  j \\leq m} \\operatorname{Cov}(Y_{c i}, Y_{c j})\n= m \\sigma^{2} \\;+\\; 2 \\binom{m}{2} \\rho \\sigma^{2}\n= m \\sigma^{2} \\;+\\; m(m-1) \\rho \\sigma^{2}.\n$$\nSumming over the $b$ clusters,\n$$\n\\operatorname{Var}\\!\\left(\\sum_{c=1}^{b} \\sum_{k=1}^{m} Y_{c k}\\right)\n= b \\left[ m \\sigma^{2} + m(m-1) \\rho \\sigma^{2} \\right]\n= b m \\sigma^{2} \\left[ 1 + (m-1) \\rho \\right].\n$$\nTherefore, the variance of the sample mean is\n$$\n\\operatorname{Var}(\\bar{Y})\n= \\frac{1}{n^{2}} \\operatorname{Var}\\!\\left(\\sum_{c=1}^{b} \\sum_{k=1}^{m} Y_{c k}\\right)\n= \\frac{b m \\sigma^{2} \\left[ 1 + (m-1) \\rho \\right]}{(b m)^{2}}\n= \\frac{\\sigma^{2}}{n} \\left[ 1 + (m-1) \\rho \\right].\n$$\nUnder simple random sampling with independence (i.e., $\\rho = 0$), the variance of $\\bar{Y}$ is $\\operatorname{Var}_{\\text{SRS}}(\\bar{Y}) = \\sigma^{2}/n$. Thus, the ratio of the clustered variance to the simple random sampling variance is\n$$\n\\text{Design Effect} \\;=\\; \\frac{\\operatorname{Var}(\\bar{Y})}{\\operatorname{Var}_{\\text{SRS}}(\\bar{Y})}\n= 1 + (m - 1) \\rho.\n$$\nThis multiplicative inflation factor captures how intracluster correlation increases the variance relative to independence.\n\nNow consider the analyst’s naive two-sided confidence interval based on the independence assumption. Let the naive standard error be $s_{\\text{naive}} = \\sigma/\\sqrt{n}$ (ignoring estimation error in practice for coverage calculations). The analyst forms the interval\n$$\n\\bar{Y} \\pm z_{0.975} \\, s_{\\text{naive}},\n$$\nwith $z_{0.975} \\approx 1.96$ for nominal coverage $0.95$. The actual sampling distribution of $\\bar{Y}$ under clustering is approximately normal by the Central Limit Theorem (CLT),\n$$\n\\bar{Y} \\sim \\mathcal{N}\\!\\left(\\mu, \\frac{\\sigma^{2}}{n} \\left[ 1 + (m-1) \\rho \\right] \\right).\n$$\nDefine the pivot used by the analyst:\n$$\nT \\;=\\; \\frac{\\bar{Y} - \\mu}{s_{\\text{naive}}}\n\\;=\\; \\frac{\\bar{Y} - \\mu}{\\sigma/\\sqrt{n}}.\n$$\nUnder the true clustered variance, $T$ has distribution\n$$\nT \\sim \\mathcal{N}\\!\\left(0, \\, 1 + (m - 1) \\rho \\right),\n$$\nbecause scaling by $\\sigma/\\sqrt{n}$ leaves a variance factor of $1 + (m-1) \\rho$. The naive interval covers if $|T| \\leq z_{0.975}$. Thus the actual coverage probability is\n$$\n\\Pr\\!\\left( |T| \\leq z_{0.975} \\right)\n= \\Pr\\!\\left( |Z| \\leq \\frac{z_{0.975}}{\\sqrt{1 + (m - 1) \\rho}} \\right)\n= 2 \\,\\Phi\\!\\left( \\frac{z_{0.975}}{\\sqrt{1 + (m - 1) \\rho}} \\right) - 1,\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ and $\\Phi$ is the standard normal cumulative distribution function.\n\nWe now substitute the given values. The problem specifies $n = 240$, $b = 24$, and $\\rho = 0.06$, with equal cluster sizes $m = n/b = 240/24 = 10$. The design effect is\n$$\n\\text{DE} = 1 + (m - 1)\\rho = 1 + (10 - 1)\\cdot 0.06 = 1 + 9 \\cdot 0.06 = 1.54.\n$$\nCompute the scaling factor:\n$$\n\\sqrt{\\text{DE}} = \\sqrt{1.54} \\approx 1.240968,\n\\quad\n\\frac{z_{0.975}}{\\sqrt{\\text{DE}}} \\approx \\frac{1.96}{1.240968} \\approx 1.57941.\n$$\nEvaluate the standard normal cumulative distribution function at this value:\n$$\n\\Phi\\!\\left(1.57941\\right) \\approx 0.942833,\n$$\nso the actual coverage probability is\n$$\n2 \\times 0.942833 - 1 = 0.885666 \\approx 0.8857\n$$\nwhen rounded to four significant figures. This demonstrates undercoverage relative to the nominal $0.95$ when intracluster correlation is ignored.",
            "answer": "$$\\boxed{0.8857}$$"
        },
        {
            "introduction": "Having understood the theoretical impact of clustering on variance, we now turn to a practical challenge: how do we estimate this complex variance from a single sample? This coding exercise bridges theory and application by asking you to implement two workhorse techniques in modern survey statistics: Taylor series linearization and the cluster bootstrap . By writing the code to compare these methods on given datasets, you will gain invaluable experience in the computational side of biostatistics and a deeper appreciation for how we handle the complexities of real-world sample designs.",
            "id": "4942737",
            "problem": "You are given a one-stage cluster sample consisting of Primary Sampling Units (PSUs), where each PSU contains a count of individuals and a count of individuals with a binary outcome of interest. You will estimate the population proportion and its variance under a with-replacement PSU sampling framework using two approaches: a cluster-level bootstrap that resamples PSUs with replacement, and a first-order Taylor linearization approximation for the ratio estimator. All quantities must be expressed as decimals (not percentages). The design-based logic must start from the following fundamental base: unbiasedness of the Horvitz–Thompson total under equal-probability sampling, the ratio estimator formed as a ratio of estimated totals for a proportion, and first-order Taylor expansion for variance linearization.\n\nDefinitions to use. Let there be $m$ PSUs indexed by $i \\in \\{1,\\dots,m\\}$. For PSU $i$, let $n_i$ be the number of individuals observed and $y_i$ be the number of individuals with the binary outcome. The estimator of the proportion is the sample ratio of totals,\n$$\n\\hat{p} \\;=\\; \\frac{\\sum_{i=1}^{m} y_i}{\\sum_{i=1}^{m} n_i}.\n$$\nThe cluster bootstrap (resampling PSUs with replacement) proceeds by generating $B$ bootstrap replicates. In replicate $b \\in \\{1,\\dots,B\\}$, sample $m$ PSUs with replacement from the set $\\{1,\\dots,m\\}$, compute\n$$\n\\hat{p}^{*(b)} \\;=\\; \\frac{\\sum_{j=1}^{m} y_{I^{(b)}_j}}{\\sum_{j=1}^{m} n_{I^{(b)}_j}},\n$$\nand then estimate the bootstrap variance as the sample variance across replicates:\n$$\n\\hat{V}_{\\text{boot}}(\\hat{p}) \\;=\\; \\frac{1}{B-1}\\sum_{b=1}^{B}\\Big(\\hat{p}^{*(b)} - \\bar{\\hat{p}}^{*}\\Big)^2,\\quad \\bar{\\hat{p}}^{*} \\;=\\; \\frac{1}{B}\\sum_{b=1}^{B}\\hat{p}^{*(b)}.\n$$\nThe first-order Taylor linearization for the ratio estimator uses the linearized variable\n$$\nu_i \\;=\\; y_i \\;-\\; \\hat{p}\\, n_i.\n$$\nUnder a with-replacement PSU sampling approximation, an estimator of the variance of $\\hat{p}$ is\n$$\n\\hat{V}_{\\text{lin}}(\\hat{p}) \\;=\\; \\frac{m}{m-1}\\,\\frac{\\sum_{i=1}^{m} u_i^2}{\\Big(\\sum_{i=1}^{m} n_i\\Big)^2}.\n$$\n\nImplement a program that, for each specified test case, computes $\\hat{p}$, the bootstrap variance $\\hat{V}_{\\text{boot}}(\\hat{p})$, and the linearization variance $\\hat{V}_{\\text{lin}}(\\hat{p})$, and returns the triple $[\\hat{p}, \\hat{V}_{\\text{boot}}(\\hat{p}), \\hat{V}_{\\text{lin}}(\\hat{p})]$ with each value rounded to $6$ decimal places.\n\nUse the following numeric datasets and test suite. Each dataset is a list of PSU pairs $(n_i,y_i)$ given explicitly, and each test specifies the number of bootstrap replicates $B$ and a pseudo-random seed value to ensure reproducibility. All numbers appear as integers; your computations must produce decimal outputs.\n\nDataset A (moderately sized, balanced cluster sizes):\n- (85, 12), (73, 9), (120, 20), (64, 6), (95, 14), (102, 13), (58, 5), (80, 10), (140, 25), (77, 8), (66, 7), (90, 11).\n\nDataset B (heterogeneous sizes and includes a zero-outcome PSU):\n- (150, 28), (45, 2), (60, 4), (110, 15), (90, 8), (200, 35), (30, 0), (75, 7).\n\nDataset C (small number of PSUs, boundary case):\n- (40, 3), (55, 7), (35, 2).\n\nTest suite to implement:\n- Case 1: Dataset A, B = 10000, seed = 12345.\n- Case 2: Dataset A, B = 500, seed = 12345.\n- Case 3: Dataset B, B = 10000, seed = 202311.\n- Case 4: Dataset C, B = 10000, seed = 777.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case’s triple is an inner list, for example:\n\"[[$p_1$, $v_{b,1}$, $v_{\\ell,1}$],[$p_2$, $v_{b,2}$, $v_{\\ell,2}$],...]\".\nYou must round each float to 6 decimal places and return decimals (not percentages). No other text should be printed.\n\nYour implementation must be fully self-contained, take no input, and not access any files or networks. You must adhere to the specified test suite and output format.",
            "solution": "The problem is valid. It presents a clearly defined computational task within the established principles of biostatistics and survey sampling theory. All necessary data, formulas, and parameters are provided, and the problem is self-contained, objective, and scientifically sound.\n\nThe task is to estimate a proportion $\\hat{p}$ from one-stage cluster sampling data and to compute the variance of this estimate using two different methods: a cluster-level bootstrap and first-order Taylor series linearization.\n\nLet the data consist of $m$ Primary Sampling Units (PSUs), or clusters. For each PSU $i \\in \\{1, \\dots, m\\}$, we are given the cluster size, $n_i$, and the count of individuals with a binary outcome, $y_i$.\n\n**1. Proportion Estimation**\n\nThe estimator for the population proportion, $\\hat{p}$, is the ratio of two estimated totals: the total count of positive outcomes over the total count of individuals in the sample. This is a ratio estimator, which is consistent for the true population proportion. The formula is:\n$$\n\\hat{p} \\;=\\; \\frac{\\sum_{i=1}^{m} y_i}{\\sum_{i=1}^{m} n_i}\n$$\nThis calculation involves summing all $y_i$ values across the clusters and dividing by the sum of all $n_i$ values.\n\n**2. Variance Estimation via First-Order Taylor Linearization**\n\nThe Taylor linearization method is an analytical technique to approximate the variance of a complex, non-linear estimator like $\\hat{p}$. The variance of a ratio of two random variables $\\hat{Y}/\\hat{N}$ can be approximated by linearizing the function $f(\\hat{Y}, \\hat{N}) = \\hat{Y}/\\hat{N}$ around the true population totals $(Y, N)$. This yields:\n$$\nV(\\hat{p}) \\approx V\\left(\\frac{\\hat{Y}}{N} - \\frac{Y}{N^2}\\hat{N}\\right) = \\frac{1}{N^2} V(\\hat{Y} - p\\hat{N})\n$$\nwhere $p = Y/N$. To form an estimator, we substitute sample estimates for population parameters. We define a \"linearized variable\" $u_i$ for each cluster that represents the cluster's contribution to the numerator's variance:\n$$\nu_i \\;=\\; y_i - \\hat{p}\\,n_i\n$$\nThe problem specifies a variance estimator under a with-replacement sampling approximation for the PSUs. Under simple random sampling of $m$ clusters with replacement, the variance of a sample total $\\sum_{i=1}^{m} t_i$ is estimated by $m$ times the sample variance of the $t_i$ values. Since $\\sum u_i = 0$, the sample variance of the $u_i$ values simplifies. The variance of the linearized numerator, $\\sum_{i=1}^{m} u_i$, is estimated by $\\frac{m}{m-1}\\sum_{i=1}^{m} u_i^2$. Dividing by the square of the estimated denominator total, $(\\sum_{i=1}^{m} n_i)^2$, gives the final variance estimator for $\\hat{p}$:\n$$\n\\hat{V}_{\\text{lin}}(\\hat{p}) \\;=\\; \\frac{m}{m-1}\\,\\frac{\\sum_{i=1}^{m} u_i^2}{\\left(\\sum_{i=1}^{m} n_i\\right)^2}\n$$\nThe term $\\frac{m}{m-1}$ is an unbiasedness correction factor for the variance of a single mean/total from a simple random sample.\n\nThe algorithm is:\na. Compute $\\hat{p}$.\nb. Compute $u_i$ for each cluster $i=1, \\dots, m$.\nc. Compute $\\sum_{i=1}^{m} u_i^2$ and $\\sum_{i=1}^{m} n_i$.\nd. Substitute these quantities into the formula for $\\hat{V}_{\\text{lin}}(\\hat{p})$.\n\n**3. Variance Estimation via Cluster Bootstrap**\n\nThe bootstrap is a non-parametric, computational method for estimating the variance of an estimator. For clustered data, it is critical to resample the entire clusters (PSUs) to preserve the within-cluster correlation structure. The process is as follows:\n\na. Generate a large number, $B$, of bootstrap replicates. For each replicate $b$ from $1$ to $B$:\n    i. Create a bootstrap sample by drawing $m$ PSUs with replacement from the original set of $m$ PSUs. This results in a new dataset of $m$ clusters, $\\{(n_{I_j^{(b)}}, y_{I_j^{(b)}})\\}_{j=1}^m$, where each index $I_j^{(b)}$ is drawn uniformly from $\\{1, \\dots, m\\}$.\n    ii. Compute the proportion estimate, $\\hat{p}^{*(b)}$, for this bootstrap sample:\n    $$\n    \\hat{p}^{*(b)} \\;=\\; \\frac{\\sum_{j=1}^{m} y_{I^{(b)}_j}}{\\sum_{j=1}^{m} n_{I^{(b)}_j}}\n    $$\nb. The collection of $B$ estimates, $\\{\\hat{p}^{*(1)}, \\dots, \\hat{p}^{*(B)}\\}$, forms an empirical sampling distribution for $\\hat{p}$. The bootstrap variance estimate is the sample variance of these replicate estimates:\n$$\n\\hat{V}_{\\text{boot}}(\\hat{p}) \\;=\\; \\frac{1}{B-1}\\sum_{b=1}^{B}\\left(\\hat{p}^{*(b)} - \\bar{\\hat{p}}^{*}\\right)^2, \\quad \\text{where } \\bar{\\hat{p}}^{*} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{p}^{*(b)}\n$$\nTo ensure reproducibility of the random sampling process, a pseudo-random number generator is initialized with a specific seed for each test case.\n\nThe implementation will process each test case by first calculating $\\hat{p}$ and $\\hat{V}_{\\text{lin}}(\\hat{p})$. Then, it will run the bootstrap procedure for the specified number of replicates $B$ using the given seed to calculate $\\hat{V}_{\\text{boot}}(\\hat{p})$. Finally, it will format the three resulting values, rounded to $6$ decimal places, as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes proportion and variance estimates for cluster-sampled data\n    using both bootstrap and linearization methods for specified test cases.\n    \"\"\"\n\n    # Define the datasets from the problem statement.\n    dataset_A = [\n        (85, 12), (73, 9), (120, 20), (64, 6), (95, 14), (102, 13),\n        (58, 5), (80, 10), (140, 25), (77, 8), (66, 7), (90, 11)\n    ]\n    dataset_B = [\n        (150, 28), (45, 2), (60, 4), (110, 15), (90, 8), (200, 35),\n        (30, 0), (75, 7)\n    ]\n    dataset_C = [\n        (40, 3), (55, 7), (35, 2)\n    ]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'dataset': dataset_A, 'B': 10000, 'seed': 12345},\n        {'dataset': dataset_A, 'B': 500, 'seed': 12345},\n        {'dataset': dataset_B, 'B': 10000, 'seed': 202311},\n        {'dataset': dataset_C, 'B': 10000, 'seed': 777}\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        dataset = case['dataset']\n        B = case['B']\n        seed = case['seed']\n\n        # Convert dataset to numpy arrays for efficient computation\n        data = np.array(dataset, dtype=np.float64)\n        n_i = data[:, 0]\n        y_i = data[:, 1]\n        m = len(n_i)\n\n        # 1. Estimate the proportion, p_hat\n        sum_y_i = np.sum(y_i)\n        sum_n_i = np.sum(n_i)\n        p_hat = sum_y_i / sum_n_i\n\n        # 2. Calculate linearization variance (V_lin)\n        u_i = y_i - p_hat * n_i\n        sum_u_i_sq = np.sum(u_i**2)\n        v_lin = (m / (m - 1.0)) * sum_u_i_sq / (sum_n_i**2)\n\n        # 3. Calculate bootstrap variance (V_boot)\n        rng = np.random.default_rng(seed)\n        p_hat_star_b = np.empty(B)\n        original_indices = np.arange(m)\n        \n        for b in range(B):\n            # Draw m indices with replacement\n            bootstrap_indices = rng.choice(original_indices, size=m, replace=True)\n            \n            # Create bootstrap sample of clusters\n            y_i_star = y_i[bootstrap_indices]\n            n_i_star = n_i[bootstrap_indices]\n            \n            # Calculate proportion for the bootstrap sample\n            # Handle potential division by zero if all sampled clusters have size 0\n            sum_n_i_star = np.sum(n_i_star)\n            if sum_n_i_star == 0:\n                # This is unlikely with the given data but is good practice.\n                # A proportion is ill-defined. Can be set to 0 or NaN.\n                # Given the problem's data, this branch is not hit.\n                p_hat_star_b[b] = 0.0\n            else:\n                p_hat_star_b[b] = np.sum(y_i_star) / sum_n_i_star\n        \n        # Compute the sample variance of the bootstrap estimates\n        # ddof=1 ensures division by (B-1)\n        v_boot = np.var(p_hat_star_b, ddof=1)\n        \n        # Round results to 6 decimal places\n        p_hat_rounded = round(p_hat, 6)\n        v_boot_rounded = round(v_boot, 6)\n        v_lin_rounded = round(v_lin, 6)\n        \n        result_list = [p_hat_rounded, v_boot_rounded, v_lin_rounded]\n        \n        # Format for final output string. Use .6f to ensure 6 decimal places printed.\n        str_list = [f'{val:.6f}' for val in result_list]\n        all_results_str.append(f\"[{','.join(str_list)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}