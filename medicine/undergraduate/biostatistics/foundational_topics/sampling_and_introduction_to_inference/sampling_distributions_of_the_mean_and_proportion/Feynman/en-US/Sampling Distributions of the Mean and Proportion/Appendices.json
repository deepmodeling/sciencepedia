{
    "hands_on_practices": [
        {
            "introduction": "Comparing the means of two groups is one of the most common tasks in biostatistics. While the standard Student's $t$-test is a powerful tool, it rests on the assumption that the variances of the two groups are equal—an assumption that is often violated in real data. This exercise confronts the famous Behrens-Fisher problem by exploring the theoretical foundation of Welch's $t$-test, which does not require equal variances. You will derive the Satterthwaite approximation for the degrees of freedom, gaining a deeper understanding of why this approximation is necessary and how it is constructed through the clever technique of moment-matching. ",
            "id": "4951504",
            "problem": "A biostatistics team is comparing the mean change in systolic blood pressure between two independent cohorts: a treatment group and a control group. The change (follow-up minus baseline) in each group is assumed to be a random sample from a normal population with potentially unequal variances. Let the treatment group have sample size $n_1$ and sample variance $s_1^{2}$, and the control group have sample size $n_2$ and sample variance $s_2^{2}$. The sample means are $\\bar{X}_1$ and $\\bar{X}_2$, respectively. Consider the standardized mean difference statistic\n$$\nT \\;=\\; \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^{2}}{n_1} + \\frac{s_2^{2}}{n_2}}}.\n$$\nStarting only from the following foundational facts and definitions:\n- If $X_1,\\dots,X_{n}$ are independent and identically distributed as normal with mean $\\mu$ and variance $\\sigma^{2}$, then the sample mean $\\bar{X}$ is normal with mean $\\mu$ and variance $\\sigma^{2}/n$.\n- For a normal sample, the unbiased sample variance $S^{2}$ satisfies $(n-1)\\,S^{2}/\\sigma^{2} \\sim \\chi^{2}_{n-1}$, and $S^{2}$ is independent of $\\bar{X}$.\n- If $U \\sim \\chi^{2}_{k}$, then $E[U] = k$ and $\\operatorname{Var}(U) = 2k$.\n- The Central Limit Theorem (CLT) ensures approximate normality of sums and averages under finite variance.\n\nTasks:\n1. Explain why, when variances are unequal, the ratio $T$ is not exactly distributed as a Student’s $t$ random variable, even under normality, and why an approximation is needed.\n2. Using moment-matching to a scaled chi-square distribution for the random denominator $\\frac{s_1^{2}}{n_1} + \\frac{s_2^{2}}{n_2}$, derive the Satterthwaite Degrees of Freedom (df) approximation for the sampling distribution of $T$ under unequal variances.\n3. A study records the following: $n_1 = 42$, $s_1^{2} = 12.1$, $n_2 = 67$, $s_2^{2} = 18.7$. Under the assumptions above, compute the approximate Satterthwaite df for the sampling distribution of $T$.\n\nRound your final numerical answer to four significant figures. Do not include any units in your final answer.",
            "solution": "This problem is a valid exercise in biostatistics concerning the Behrens-Fisher problem, which deals with the comparison of means from two normal populations with unknown and potentially unequal variances. The problem is well-posed, scientifically grounded, and provides all necessary information for its solution.\n\nThe problem asks for three tasks: an explanation of the distributional nature of the test statistic $T$, a derivation of the Satterthwaite approximation for the degrees of freedom, and a numerical calculation of these degrees of freedom for a given dataset.\n\n**Task 1: Why the Welch's $t$-statistic $T$ is not exactly Student's $t$-distributed**\n\nA random variable is defined to follow a Student's $t$-distribution with $k$ degrees of freedom, denoted $t_k$, if it can be written as the ratio $t_k = \\frac{Z}{\\sqrt{U/k}}$, where $Z$ is a standard normal random variable ($Z \\sim N(0,1)$), $U$ is a chi-square random variable with $k$ degrees of freedom ($U \\sim \\chi_k^2$), and $Z$ and $U$ are independent.\n\nLet's analyze the statistic $T = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^{2}}{n_1} + \\frac{s_2^{2}}{n_2}}}$.\n\nThe numerator is $\\bar{X}_1 - \\bar{X}_2$. Based on the provided foundational facts, since the samples are drawn from normal populations, $\\bar{X}_1 \\sim N(\\mu_1, \\sigma_1^2/n_1)$ and $\\bar{X}_2 \\sim N(\\mu_2, \\sigma_2^2/n_2)$. Since the two cohorts are independent, $\\bar{X}_1$ and $\\bar{X}_2$ are independent. The difference of two independent normal variables is also normal. Therefore, the distribution of the difference is $\\bar{X}_1 - \\bar{X}_2 \\sim N(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2})$.\n\nTo construct a standard normal variable $Z$, we standardize this difference:\n$$\nZ = \\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\sim N(0,1)\n$$\nThe numerator of the statistic $T$, under the null hypothesis $H_0: \\mu_1=\\mu_2$, is $\\bar{X}_1 - \\bar{X}_2$.\n\nThe denominator of $T$ is the square root of the quantity $V = \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}$. This term $V$ is an estimator for the true variance of the numerator, $\\operatorname{Var}(\\bar{X}_1 - \\bar{X}_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$. The independence of $\\bar{X}$ and $S^2$ for a normal sample implies that the numerator $\\bar{X}_1 - \\bar{X}_2$ is independent of the denominator term $V$.\n\nNow, let's examine the distribution of $V$. From the foundational facts, we know that for each sample $i \\in \\{1, 2\\}$, the quantity $\\frac{(n_i-1)s_i^2}{\\sigma_i^2}$ follows a chi-square distribution with $n_i-1$ degrees of freedom, i.e., $\\frac{(n_i-1)s_i^2}{\\sigma_i^2} \\sim \\chi_{n_i-1}^2$. This can be rewritten as $s_i^2 \\sim \\frac{\\sigma_i^2}{n_i-1}\\chi_{n_i-1}^2$.\n\nThe term $V$ is therefore a linear combination of two independent, scaled chi-square variables:\n$$\nV = \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\sim \\frac{\\sigma_1^2}{n_1(n_1-1)}\\chi_{n_1-1}^2 + \\frac{\\sigma_2^2}{n_2(n_2-1)}\\chi_{n_2-1}^2\n$$\nA fundamental theorem in statistics states that a linear combination of independent chi-square variables, $Y = \\sum a_i \\chi_{k_i}^2$, is itself a scaled chi-square variable only in specific cases, such as when all the scaling coefficients $a_i$ are equal. In our case, the scaling coefficients are $\\frac{\\sigma_1^2}{n_1(n_1-1)}$ and $\\frac{\\sigma_2^2}{n_2(n_2-1)}$. These coefficients are equal only under very restrictive conditions on the population variances and sample sizes, which are not generally met, particularly when $\\sigma_1^2 \\neq \\sigma_2^2$.\n\nSince the random variable $V$ is not, in general, a single scaled chi-square variable, the ratio $T$ does not exactly follow a Student's $t$-distribution. Its exact distribution is complex, which necessitates the use of an approximation, such as the Satterthwaite approximation for the effective degrees of freedom.\n\n**Task 2: Derivation of the Satterthwaite Degrees of Freedom**\n\nThe Satterthwaite approximation aims to approximate the distribution of $V = \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}$ by a scaled chi-square distribution of the form $c \\cdot \\chi_\\nu^2$, where $\\nu$ represents the effective degrees of freedom. The approximation is achieved by matching the first two moments (mean and variance) of $V$ with those of $c \\cdot \\chi_\\nu^2$.\n\nFirst, let's find the mean and variance of $V$.\nThe expectation of $V$ is:\n$E[V] = E\\left[\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right] = \\frac{1}{n_1}E[s_1^2] + \\frac{1}{n_2}E[s_2^2]$.\nSince $s_i^2$ is an unbiased estimator of the population variance $\\sigma_i^2$, we have $E[s_i^2] = \\sigma_i^2$.\nThus, $E[V] = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$.\n\nThe variance of $V$, using the independence of $s_1^2$ and $s_2^2$, is:\n$\\operatorname{Var}(V) = \\operatorname{Var}\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right) = \\frac{1}{n_1^2}\\operatorname{Var}(s_1^2) + \\frac{1}{n_2^2}\\operatorname{Var}(s_2^2)$.\nTo find $\\operatorname{Var}(s_i^2)$, we use the fact that $\\frac{(n_i-1)s_i^2}{\\sigma_i^2} \\sim \\chi_{n_i-1}^2$.\nThis implies $s_i^2 = \\frac{\\sigma_i^2}{n_i-1}\\chi_{n_i-1}^2$.\n$\\operatorname{Var}(s_i^2) = \\operatorname{Var}\\left(\\frac{\\sigma_i^2}{n_i-1}\\chi_{n_i-1}^2\\right) = \\left(\\frac{\\sigma_i^2}{n_i-1}\\right)^2 \\operatorname{Var}(\\chi_{n_i-1}^2)$.\nUsing the given fact that $\\operatorname{Var}(\\chi_k^2) = 2k$, we get:\n$\\operatorname{Var}(s_i^2) = \\frac{\\sigma_i^4}{(n_i-1)^2} \\cdot 2(n_i-1) = \\frac{2\\sigma_i^4}{n_i-1}$.\nSubstituting this back into the expression for $\\operatorname{Var}(V)$:\n$\\operatorname{Var}(V) = \\frac{1}{n_1^2}\\left(\\frac{2\\sigma_1^4}{n_1-1}\\right) + \\frac{1}{n_2^2}\\left(\\frac{2\\sigma_2^4}{n_2-1}\\right)$.\n\nNow, consider the approximating distribution $A = c \\cdot \\chi_\\nu^2$. Its moments are:\n$E[A] = c \\cdot E[\\chi_\\nu^2] = c\\nu$.\n$\\operatorname{Var}(A) = c^2 \\cdot \\operatorname{Var}(\\chi_\\nu^2) = c^2 (2\\nu) = 2c^2\\nu$.\n\nEquating the moments of $V$ and $A$:\n1. $E[V] = E[A] \\implies \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} = c\\nu$\n2. $\\operatorname{Var}(V) = \\operatorname{Var}(A) \\implies \\frac{2\\sigma_1^4}{n_1^2(n_1-1)} + \\frac{2\\sigma_2^4}{n_2^2(n_2-1)} = 2c^2\\nu$\n\nFrom equation (1), we solve for $c$: $c = \\frac{1}{\\nu}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)$.\nSubstitute this into equation (2), after simplifying by a factor of 2:\n$\\frac{\\sigma_1^4}{n_1^2(n_1-1)} + \\frac{\\sigma_2^4}{n_2^2(n_2-1)} = c^2\\nu = \\left[\\frac{1}{\\nu}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)\\right]^2\\nu = \\frac{1}{\\nu}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2$.\n\nNow, we solve for the degrees of freedom, $\\nu$:\n$$\n\\nu = \\frac{\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2}{\\frac{\\sigma_1^4}{n_1^2(n_1-1)} + \\frac{\\sigma_2^4}{n_2^2(n_2-1)}}\n$$\nThis can be rewritten in a more memorable form:\n$$\n\\nu = \\frac{\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{\\sigma_1^2}{n_1}\\right)^2}{n_1-1} + \\frac{\\left(\\frac{\\sigma_2^2}{n_2}\\right)^2}{n_2-1}}\n$$\nSince the population variances $\\sigma_1^2$ and $\\sigma_2^2$ are unknown, we substitute their unbiased sample estimates, $s_1^2$ and $s_2^2$, to obtain the estimated degrees of freedom, which we will denote as $df$:\n$$\ndf = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{s_1^2}{n_1}\\right)^2}{n_1-1} + \\frac{\\left(\\frac{s_2^2}{n_2}\\right)^2}{n_2-1}}\n$$\nThis is the Welch-Satterthwaite equation for the approximate degrees of freedom.\n\n**Task 3: Numerical Calculation of Degrees of Freedom**\n\nThe problem provides the following data: $n_1 = 42$, $s_1^2 = 12.1$, $n_2 = 67$, and $s_2^2 = 18.7$. We substitute these values into the derived formula for $df$.\n\nFirst, calculate the individual terms $\\frac{s_i^2}{n_i}$:\n$v_1 = \\frac{s_1^2}{n_1} = \\frac{12.1}{42} \\approx 0.288095$\n$v_2 = \\frac{s_2^2}{n_2} = \\frac{18.7}{67} \\approx 0.279104$\n\nNow, substitute these into the Satterthwaite formula:\n$$\ndf = \\frac{(v_1 + v_2)^2}{\\frac{v_1^2}{n_1-1} + \\frac{v_2^2}{n_2-1}} = \\frac{\\left(\\frac{12.1}{42} + \\frac{18.7}{67}\\right)^2}{\\frac{\\left(\\frac{12.1}{42}\\right)^2}{42-1} + \\frac{\\left(\\frac{18.7}{67}\\right)^2}{67-1}}\n$$\n$$\ndf = \\frac{\\left(0.288095238... + 0.279104477...\\right)^2}{\\frac{\\left(0.288095238...\\right)^2}{41} + \\frac{\\left(0.279104477...\\right)^2}{66}}\n$$\n$$\ndf = \\frac{\\left(0.567199715...\\right)^2}{\\frac{0.083000997...}{41} + \\frac{0.077900003...}{66}}\n$$\n$$\ndf = \\frac{0.321715115...}{0.002024414... + 0.001180303...}\n$$\n$$\ndf = \\frac{0.321715115...}{0.003204717...}\n$$\n$$\ndf \\approx 100.38797\n$$\nRounding the result to four significant figures, we get $100.4$.",
            "answer": "$$\\boxed{100.4}$$"
        },
        {
            "introduction": "Just as we compare means for continuous outcomes, we often need to compare proportions for binary outcomes, such as the prevalence of a disease or the success rate of a treatment in two different populations. This practice demonstrates how the Central Limit Theorem can be applied to sample proportions derived from Bernoulli trials, allowing us to construct a large-sample sampling distribution for their difference. By deriving the asymptotic variance of the difference between two independent sample proportions, you will reinforce fundamental principles of how sampling variability combines across independent studies, a cornerstone of meta-analysis and comparative effectiveness research. ",
            "id": "4951512",
            "problem": "A biostatistician is comparing the presence of a specific biomarker in two independent cohorts. In cohort $1$, each subject’s biomarker status is modeled as an independent Bernoulli random variable $X_{1,i}$ with success probability $p_{1}$, and in cohort $2$, each subject’s biomarker status is modeled as an independent Bernoulli random variable $X_{2,j}$ with success probability $p_{2}$. Let the sample sizes be $n_{1}$ and $n_{2}$, respectively, and define the sample proportions as $\\hat{p}_{1} = \\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}} X_{1,i}$ and $\\hat{p}_{2} = \\frac{1}{n_{2}}\\sum_{j=1}^{n_{2}} X_{2,j}$. Assume independence both within and across cohorts.\n\nUsing only the definitions of Bernoulli random variables and the Central Limit Theorem (CLT) for independent sums, derive the asymptotic distribution of $\\hat{p}_{1} - \\hat{p}_{2}$ as $n_{1} \\to \\infty$ and $n_{2} \\to \\infty$, and obtain its asymptotic variance in terms of $p_{1}$, $p_{2}$, $n_{1}$, and $n_{2}$.\n\nThen, evaluate the asymptotic variance numerically for $p_{1} = 0.12$, $p_{2} = 0.08$, $n_{1} = 800$, and $n_{2} = 1000$. Round your answer to four significant figures.",
            "solution": "The problem requires the derivation of the asymptotic distribution for the difference in sample proportions, $\\hat{p}_{1} - \\hat{p}_{2}$. This can be accomplished by first determining the asymptotic distribution of each sample proportion individually and then combining them.\n\nFor cohort $1$, each $X_{1,i}$ is an independent Bernoulli random variable with success probability $p_{1}$. The mean and variance of $X_{1,i}$ are:\n$$\nE[X_{1,i}] = p_{1}\n$$\n$$\n\\text{Var}(X_{1,i}) = p_{1}(1 - p_{1})\n$$\nThe sample proportion $\\hat{p}_{1}$ is the sample mean of $n_{1}$ independent and identically distributed (i.i.d.) random variables. The mean of $\\hat{p}_{1}$ is:\n$$\nE[\\hat{p}_{1}] = E\\left[\\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}} X_{1,i}\\right] = \\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}} E[X_{1,i}] = \\frac{1}{n_{1}}(n_{1}p_{1}) = p_{1}\n$$\nThe variance of $\\hat{p}_{1}$, due to the independence of the $X_{1,i}$ variables, is:\n$$\n\\text{Var}(\\hat{p}_{1}) = \\text{Var}\\left(\\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}} X_{1,i}\\right) = \\frac{1}{n_{1}^{2}}\\sum_{i=1}^{n_{1}} \\text{Var}(X_{1,i}) = \\frac{1}{n_{1}^{2}}(n_{1}p_{1}(1 - p_{1})) = \\frac{p_{1}(1 - p_{1})}{n_{1}}\n$$\nAccording to the Central Limit Theorem, as $n_{1} \\to \\infty$, the sampling distribution of $\\hat{p}_{1}$ converges to a Normal distribution with mean $p_{1}$ and variance $\\frac{p_{1}(1 - p_{1})}{n_{1}}$. We can write this as:\n$$\n\\hat{p}_{1} \\overset{d}{\\to} N\\left(p_{1}, \\frac{p_{1}(1 - p_{1})}{n_{1}}\\right)\n$$\nwhere $\\overset{d}{\\to}$ denotes convergence in distribution.\n\nSimilarly, for cohort $2$, we have $X_{2,j} \\sim \\text{Bernoulli}(p_{2})$ with $E[X_{2,j}] = p_{2}$ and $\\text{Var}(X_{2,j}) = p_{2}(1 - p_{2})$. The sample proportion $\\hat{p}_{2}$ has mean and variance:\n$$\nE[\\hat{p}_{2}] = p_{2}\n$$\n$$\n\\text{Var}(\\hat{p}_{2}) = \\frac{p_{2}(1 - p_{2})}{n_{2}}\n$$\nBy the CLT, as $n_{2} \\to \\infty$, the asymptotic distribution of $\\hat{p}_{2}$ is:\n$$\n\\hat{p}_{2} \\overset{d}{\\to} N\\left(p_{2}, \\frac{p_{2}(1 - p_{2})}{n_{2}}\\right)\n$$\nThe problem states that the two cohorts are independent. Therefore, the random variables $\\hat{p}_{1}$ and $\\hat{p}_{2}$ are independent. A property of Normal distributions is that the difference of two independent Normally distributed random variables is also Normally distributed.\n\nWe are interested in the distribution of the difference $D = \\hat{p}_{1} - \\hat{p}_{2}$. The mean of the difference is:\n$$\nE[D] = E[\\hat{p}_{1} - \\hat{p}_{2}] = E[\\hat{p}_{1}] - E[\\hat{p}_{2}] = p_{1} - p_{2}\n$$\nDue to the independence of $\\hat{p}_{1}$ and $\\hat{p}_{2}$, the variance of the difference is the sum of their variances:\n$$\n\\text{Var}(D) = \\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = \\text{Var}(\\hat{p}_{1}) + \\text{Var}(\\hat{p}_{2}) = \\frac{p_{1}(1 - p_{1})}{n_{1}} + \\frac{p_{2}(1 - p_{2})}{n_{2}}\n$$\nThus, the asymptotic distribution of $\\hat{p}_{1} - \\hat{p}_{2}$ as $n_{1} \\to \\infty$ and $n_{2} \\to \\infty$ is a Normal distribution with mean $p_{1} - p_{2}$ and variance $\\frac{p_{1}(1 - p_{1})}{n_{1}} + \\frac{p_{2}(1 - p_{2})}{n_{2}}$. Formally:\n$$\n\\hat{p}_{1} - \\hat{p}_{2} \\overset{d}{\\to} N\\left(p_{1} - p_{2}, \\frac{p_{1}(1 - p_{1})}{n_{1}} + \\frac{p_{2}(1 - p_{2})}{n_{2}}\\right)\n$$\nThe asymptotic variance of $\\hat{p}_{1} - \\hat{p}_{2}$ is:\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = \\frac{p_{1}(1 - p_{1})}{n_{1}} + \\frac{p_{2}(1 - p_{2})}{n_{2}}\n$$\nNow, we evaluate this variance numerically using the given values: $p_{1} = 0.12$, $p_{2} = 0.08$, $n_{1} = 800$, and $n_{2} = 1000$.\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = \\frac{0.12(1 - 0.12)}{800} + \\frac{0.08(1 - 0.08)}{1000}\n$$\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = \\frac{0.12(0.88)}{800} + \\frac{0.08(0.92)}{1000}\n$$\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = \\frac{0.1056}{800} + \\frac{0.0736}{1000}\n$$\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = 0.000132 + 0.0000736\n$$\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = 0.0002056\n$$\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $0.0002056$. The first significant figure is the first non-zero digit, which is $2$. The four significant figures are $2$, $0$, $5$, and $6$. The number as calculated, $0.0002056$, already has exactly four significant figures. Therefore, no additional rounding is needed.\nThe result is $0.0002056$.",
            "answer": "$$\n\\boxed{0.0002056}\n$$"
        },
        {
            "introduction": "Often in biostatistics, the parameter of interest is not a simple mean or proportion, but a more complex function of them, such as a ratio. For instance, we might want to estimate the ratio of mean biomarker levels between a treatment and a control group. This exercise introduces the Delta method, a versatile and powerful technique for deriving the approximate sampling distribution of a function of one or more random variables. By applying it to find the asymptotic variance of a ratio of two sample means, you will learn a generalizable skill that is essential for statistical inference on a vast array of derived quantities like risk ratios and hazard ratios. ",
            "id": "4951505",
            "problem": "A biostatistics study compares two independent cohorts to estimate the ratio of population mean biomarker levels. Let $\\{X_{i}\\}_{i=1}^{n_{X}}$ be independent and identically distributed real-valued observations from a population with mean $\\mu_{X}$ and variance $\\sigma_{X}^{2}$, and let $\\{Y_{j}\\}_{j=1}^{n_{Y}}$ be independent and identically distributed real-valued observations from a second, independent population with mean $\\mu_{Y} \\neq 0$ and variance $\\sigma_{Y}^{2}$. Assume all fourth moments are finite. Define the sample means $\\bar{X} = \\frac{1}{n_{X}} \\sum_{i=1}^{n_{X}} X_{i}$ and $\\bar{Y} = \\frac{1}{n_{Y}} \\sum_{j=1}^{n_{Y}} Y_{j}$, and consider the estimator of the ratio of population means $R = \\bar{X}/\\bar{Y}$, which targets $\\mu_{X}/\\mu_{Y}$.\n\nUsing only the Central Limit Theorem (CLT) for sample means and first-order differentiability of $g(x,y) = x/y$, derive the first-order asymptotic distribution of $R$ under the independence and finite variance assumptions. Then, explicitly compute the asymptotic variance of $R$ in closed form in terms of $\\mu_{X}$, $\\mu_{Y}$, $\\sigma_{X}^{2}$, $\\sigma_{Y}^{2}$, $n_{X}$, and $n_{Y}$. Provide your final answer as a single simplified analytic expression for the asymptotic variance of $R$ only. No numerical evaluation is required.",
            "solution": "The objective is to find the asymptotic variance of the estimator $R = \\bar{X}/\\bar{Y}$ for the parameter $\\mu_X/\\mu_Y$. The derivation relies on the Central Limit Theorem (CLT) and the first-order Taylor expansion of a function of random variables, a technique commonly known as the Delta method.\n\nFirst, we establish the asymptotic distribution of the sample means. According to the Central Limit Theorem, for a sample $\\{X_i\\}_{i=1}^{n_X}$ of independent and identically distributed (i.i.d.) random variables with population mean $\\mu_X$ and finite variance $\\sigma_X^2$, the sample mean $\\bar{X}$ is asymptotically normally distributed. Specifically, as $n_X \\to \\infty$:\n$$ \\sqrt{n_X}(\\bar{X} - \\mu_X) \\xrightarrow{d} N(0, \\sigma_X^2) $$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution. This implies that for large $n_X$, $\\bar{X}$ is approximately normally distributed with mean $\\mu_X$ and variance $\\frac{\\sigma_X^2}{n_X}$.\nSimilarly, for the independent sample $\\{Y_j\\}_{j=1}^{n_Y}$ with population mean $\\mu_Y$ and variance $\\sigma_Y^2$, as $n_Y \\to \\infty$:\n$$ \\sqrt{n_Y}(\\bar{Y} - \\mu_Y) \\xrightarrow{d} N(0, \\sigma_Y^2) $$\nThis implies that for large $n_Y$, $\\bar{Y}$ is approximately normally distributed with mean $\\mu_Y$ and variance $\\frac{\\sigma_Y^2}{n_Y}$.\n\nSince the two samples $\\{X_i\\}$ and $\\{Y_j\\}$ are independent, their respective sample means $\\bar{X}$ and $\\bar{Y}$ are also independent random variables.\nWe can combine these two sample means into a vector statistic $T = \\begin{pmatrix} \\bar{X} \\\\ \\bar{Y} \\end{pmatrix}$. The corresponding parameter vector is $\\theta = \\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix}$.\nThe asymptotic distribution of the vector $T$ is a bivariate normal distribution. The mean vector is $E[T] = \\begin{pmatrix} E[\\bar{X}] \\\\ E[\\bar{Y}] \\end{pmatrix} = \\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix} = \\theta$.\nThe covariance matrix of $T$, denoted $\\Sigma_T$, is:\n$$ \\Sigma_T = \\text{Cov}(T) = \\begin{pmatrix} \\text{Var}(\\bar{X}) & \\text{Cov}(\\bar{X}, \\bar{Y}) \\\\ \\text{Cov}(\\bar{X}, \\bar{Y}) & \\text{Var}(\\bar{Y}) \\end{pmatrix} $$\nFrom the CLT, $\\text{Var}(\\bar{X}) = \\frac{\\sigma_X^2}{n_X}$ and $\\text{Var}(\\bar{Y}) = \\frac{\\sigma_Y^2}{n_Y}$. Due to the independence of the two samples, $\\text{Cov}(\\bar{X}, \\bar{Y}) = 0$.\nThus, the asymptotic covariance matrix is:\n$$ \\Sigma_T = \\begin{pmatrix} \\frac{\\sigma_X^2}{n_X} & 0 \\\\ 0 & \\frac{\\sigma_Y^2}{n_Y} \\end{pmatrix} $$\nSo, for large $n_X$ and $n_Y$, the vector $T = \\begin{pmatrix} \\bar{X} \\\\ \\bar{Y} \\end{pmatrix}$ is approximately distributed as a bivariate normal:\n$$ \\begin{pmatrix} \\bar{X} \\\\ \\bar{Y} \\end{pmatrix} \\stackrel{a}{\\sim} N\\left( \\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix}, \\begin{pmatrix} \\frac{\\sigma_X^2}{n_X} & 0 \\\\ 0 & \\frac{\\sigma_Y^2}{n_Y} \\end{pmatrix} \\right) $$\n\nThe estimator of interest is $R = \\bar{X}/\\bar{Y}$, which is a function of $\\bar{X}$ and $\\bar{Y}$. Let this function be $g(x,y) = x/y$. Our estimator is $R = g(\\bar{X}, \\bar{Y})$. We use the multivariate Delta method to find its asymptotic distribution.\nThe Delta method states that for a function $g$ with continuous first partial derivatives, the asymptotic distribution of $g(T)$ is normal with mean $g(\\theta)$ and variance given by $D_g(\\theta) \\Sigma_T (D_g(\\theta))^T$, where $D_g(\\theta)$ is the gradient of $g$ evaluated at $\\theta$.\n\nThe function is $g(x,y) = x/y$. We compute its gradient, which is a row vector of its partial derivatives:\n$$ D_g(x,y) = \\begin{pmatrix} \\frac{\\partial g}{\\partial x} & \\frac{\\partial g}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{y} & -\\frac{x}{y^2} \\end{pmatrix} $$\nWe evaluate this gradient at the mean vector $\\theta = (\\mu_X, \\mu_Y)$. The problem states $\\mu_Y \\neq 0$, so the derivatives are well-defined at this point.\n$$ D_g(\\mu_X, \\mu_Y) = \\begin{pmatrix} \\frac{1}{\\mu_Y} & -\\frac{\\mu_X}{\\mu_Y^2} \\end{pmatrix} $$\nThe asymptotic variance of $R$, denoted $\\text{AsyVar}(R)$, is given by the formula:\n$$ \\text{AsyVar}(R) = D_g(\\mu_X, \\mu_Y) \\Sigma_T (D_g(\\mu_X, \\mu_Y))^T $$\nSubstituting the expressions for the gradient and the covariance matrix:\n$$ \\text{AsyVar}(R) = \\begin{pmatrix} \\frac{1}{\\mu_Y} & -\\frac{\\mu_X}{\\mu_Y^2} \\end{pmatrix} \\begin{pmatrix} \\frac{\\sigma_X^2}{n_X} & 0 \\\\ 0 & \\frac{\\sigma_Y^2}{n_Y} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\mu_Y} \\\\ -\\frac{\\mu_X}{\\mu_Y^2} \\end{pmatrix} $$\nWe perform the matrix multiplication. First, multiply the row vector (gradient) by the covariance matrix:\n$$ \\begin{pmatrix} \\frac{1}{\\mu_Y} & -\\frac{\\mu_X}{\\mu_Y^2} \\end{pmatrix} \\begin{pmatrix} \\frac{\\sigma_X^2}{n_X} & 0 \\\\ 0 & \\frac{\\sigma_Y^2}{n_Y} \\end{pmatrix} = \\begin{pmatrix} \\left(\\frac{1}{\\mu_Y}\\right)\\left(\\frac{\\sigma_X^2}{n_X}\\right) + (0) & (0) + \\left(-\\frac{\\mu_X}{\\mu_Y^2}\\right)\\left(\\frac{\\sigma_Y^2}{n_Y}\\right) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sigma_X^2}{n_X \\mu_Y} & -\\frac{\\mu_X \\sigma_Y^2}{n_Y \\mu_Y^2} \\end{pmatrix} $$\nNext, multiply this resulting row vector by the transposed (column) gradient vector:\n$$ \\text{AsyVar}(R) = \\begin{pmatrix} \\frac{\\sigma_X^2}{n_X \\mu_Y} & -\\frac{\\mu_X \\sigma_Y^2}{n_Y \\mu_Y^2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\mu_Y} \\\\ -\\frac{\\mu_X}{\\mu_Y^2} \\end{pmatrix} $$\n$$ \\text{AsyVar}(R) = \\left(\\frac{\\sigma_X^2}{n_X \\mu_Y}\\right)\\left(\\frac{1}{\\mu_Y}\\right) + \\left(-\\frac{\\mu_X \\sigma_Y^2}{n_Y \\mu_Y^2}\\right)\\left(-\\frac{\\mu_X}{\\mu_Y^2}\\right) $$\n$$ \\text{AsyVar}(R) = \\frac{\\sigma_X^2}{n_X \\mu_Y^2} + \\frac{\\mu_X^2 \\sigma_Y^2}{n_Y \\mu_Y^4} $$\nThis expression represents the asymptotic variance of $R$.\nThe full asymptotic distribution is thus:\n$$ R \\stackrel{a}{\\sim} N\\left( \\frac{\\mu_X}{\\mu_Y}, \\frac{\\sigma_X^2}{n_X \\mu_Y^2} + \\frac{\\mu_X^2 \\sigma_Y^2}{n_Y \\mu_Y^4} \\right) $$\nTo provide the final answer as a single simplified analytic expression, we can combine the terms in the variance expression by finding a common denominator, which is $n_X n_Y \\mu_Y^4$:\n$$ \\text{AsyVar}(R) = \\frac{\\sigma_X^2 (n_Y \\mu_Y^2)}{n_X \\mu_Y^2 (n_Y \\mu_Y^2)} + \\frac{\\mu_X^2 \\sigma_Y^2 (n_X)}{n_Y \\mu_Y^4 (n_X)} = \\frac{n_Y \\sigma_X^2 \\mu_Y^2}{n_X n_Y \\mu_Y^4} + \\frac{n_X \\mu_X^2 \\sigma_Y^2}{n_X n_Y \\mu_Y^4} $$\n$$ \\text{AsyVar}(R) = \\frac{n_Y \\sigma_X^2 \\mu_Y^2 + n_X \\mu_X^2 \\sigma_Y^2}{n_X n_Y \\mu_Y^4} $$\nThis is the closed-form expression for the asymptotic variance of $R$ in terms of the given population and sample size parameters.",
            "answer": "$$\n\\boxed{\\frac{n_Y \\sigma_X^2 \\mu_Y^2 + n_X \\mu_X^2 \\sigma_Y^2}{n_X n_Y \\mu_Y^4}}\n$$"
        }
    ]
}