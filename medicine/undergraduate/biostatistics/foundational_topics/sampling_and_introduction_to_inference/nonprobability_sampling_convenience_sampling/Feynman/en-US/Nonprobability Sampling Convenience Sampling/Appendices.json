{
    "hands_on_practices": [
        {
            "introduction": "To effectively address bias in convenience samples, we must first build a solid intuition for how it arises. This exercise provides a foundational, hands-on opportunity to simulate a data-generating process where selection depends on a known covariate . By deriving the resulting bias in the sample mean and the regression slope from first principles, you will gain a clear, quantitative understanding of how non-random selection systematically distorts some estimators while leaving others unaffected in specific scenarios.",
            "id": "4932651",
            "problem": "You are to implement a program that formalizes a nonprobability convenience sampling mechanism and computes estimator biases arising from selection on a covariate. Consider a data-generating process with covariate $X \\sim \\mathcal{N}(0,1)$, outcome $Y = \\beta X + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independent of $X$, and selection indicator $S \\sim \\text{Bernoulli}\\big(\\operatorname{logit}^{-1}(\\alpha + \\gamma X)\\big)$, where $\\operatorname{logit}^{-1}(t) = \\frac{1}{1 + e^{-t}}$. The convenience sample is the subset with $S=1$. Your goal is to compute, from first principles, the biases of two estimators under this selection:\n- The sample mean of $Y$ computed in the selected sample, compared to the population mean of $Y$.\n- The Ordinary Least Squares (OLS) regression slope of $Y$ on $X$ computed in the selected sample, compared to the true slope $\\beta$.\n\nBase your reasoning on the following fundamental definitions and facts:\n- The bias of an estimator $\\hat{\\theta}$ for a target parameter $\\theta$ is $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$.\n- For the linear model $Y=\\beta X + \\epsilon$ with $\\mathbb{E}[\\epsilon \\mid X] = 0$, the population slope under OLS equals $\\frac{\\operatorname{Cov}(X,Y)}{\\operatorname{Var}(X)}$.\n- Conditioning on selection $S=1$ changes the distribution of $X$ according to the selection mechanism, which can be expressed through expectations with respect to the base distribution of $X$ weighted by $\\operatorname{logit}^{-1}(\\alpha + \\gamma X)$.\n\nYou must not rely on any shortcut formula not derivable from these bases. Use numerical integration with respect to the standard normal distribution to evaluate expectations of the form $\\mathbb{E}[g(X)]$ for appropriately chosen functions $g(\\cdot)$.\n\nYour program should:\n- For each test case, compute the bias of the selected-sample mean of $Y$, defined as $\\mathbb{E}[\\bar{Y} \\mid S=1] - \\mathbb{E}[Y]$, expressed as a real number (float). Note that $\\mathbb{E}[Y]$ is over the full population (unselected).\n- For each test case, compute the asymptotic bias of the selected-sample OLS slope $\\hat{\\beta}_{\\text{OLS}}$ of $Y$ on $X$, defined as $\\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] - \\beta$, expressed as a real number (float). Treat $\\hat{\\beta}_{\\text{OLS}}$ in large samples as the ratio $\\frac{\\operatorname{Cov}(X,Y \\mid S=1)}{\\operatorname{Var}(X \\mid S=1)}$, where both covariance and variance are conditional on $S=1$.\n\nRequirements for numerical evaluation:\n- Compute expectations with respect to $X \\sim \\mathcal{N}(0,1)$ using deterministic numerical integration. One valid way is Gauss–Hermite quadrature, using the identity\n$$\n\\mathbb{E}[g(X)] \\;=\\; \\int_{-\\infty}^{\\infty} g(x)\\,\\phi(x)\\,dx \\;=\\; \\frac{1}{\\sqrt{\\pi}} \\sum_{k=1}^{K} w_k\\, g\\!\\left(\\sqrt{2}\\, z_k\\right),\n$$\nwhere $\\phi(\\cdot)$ is the standard normal density, and $\\{z_k,w_k\\}_{k=1}^K$ are the Gauss–Hermite nodes and weights for the weight function $\\exp(-x^2)$ with some finite $K$ chosen to achieve numerical stability.\n\nTest suite:\n- Use the following six test cases, each specified as $(\\alpha,\\gamma,\\beta,\\sigma)$:\n    1. $(0, 1, 0.8, 1)$\n    2. $(0.2, 0, 1.2, 2)$\n    3. $(-0.5, 2.0, -0.5, 1.5)$\n    4. $(1.0, -1.5, 1.0, 0.5)$\n    5. $(-3.0, 1.0, 0.4, 1.0)$\n    6. $(0, 1, 0, 1)$\n\nNotes:\n- All angles, if any, are not applicable to this problem.\n- No physical units are involved.\n- When reporting results, do not use percentage signs; return decimal numbers.\n- The parameter $\\sigma$ is included to verify invariance properties; ensure your solution logic respects the model definition.\n\nFinal output specification:\n- Your program should produce a single line of output containing a flattened list of real numbers, where for each test case you append the two biases in order: first the bias of the selected-sample mean of $Y$, then the bias of the selected-sample OLS slope, for that test case.\n- Round each number to $6$ decimal places.\n- The final printed line must be a single comma-separated Python-style list, for example: $[\\text{result}_1,\\text{result}_2,\\ldots,\\text{result}_{12}]$ for the six cases above.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in statistical theory, well-posed with a clear objective and all necessary parameters, and formulated objectively. It presents a non-trivial but solvable problem in the analysis of selection bias.\n\nThe solution proceeds by deriving the analytical expressions for the two biases from first principles, followed by a description of the numerical implementation.\n\nLet the selection probability for a given covariate value $x$ be denoted by $p(x) = P(S=1 \\mid X=x) = \\operatorname{logit}^{-1}(\\alpha + \\gamma x) = \\frac{1}{1 + e^{-(\\alpha + \\gamma x)}}$. The covariate $X$ follows a standard normal distribution, $X \\sim \\mathcal{N}(0,1)$, with probability density function $\\phi(x)$.\n\nA fundamental tool for this analysis is the formula for the expectation of a function $g(X)$ conditional on selection ($S=1$). Using the definition of conditional expectation:\n$$ \\mathbb{E}[g(X) \\mid S=1] = \\frac{\\mathbb{E}[g(X) \\cdot \\mathbb{I}(S=1)]}{P(S=1)} $$\nwhere $\\mathbb{I}(S=1)$ is the indicator function for selection. The denominator is the overall probability of selection, $P(S=1) = \\mathbb{E}[\\mathbb{I}(S=1)]$. The numerator and denominator can be expressed as expectations over the distribution of $X$:\n$$ P(S=1) = \\mathbb{E}_X[\\mathbb{E}[\\mathbb{I}(S=1) \\mid X]] = \\mathbb{E}_X[P(S=1 \\mid X)] = \\mathbb{E}_X[p(X)] $$\n$$ \\mathbb{E}[g(X) \\cdot \\mathbb{I}(S=1)] = \\mathbb{E}_X[\\mathbb{E}[g(X) \\cdot \\mathbb{I}(S=1) \\mid X]] = \\mathbb{E}_X[g(X) \\cdot P(S=1 \\mid X)] = \\mathbb{E}_X[g(X)p(X)] $$\nCombining these gives the key result:\n$$ \\mathbb{E}[g(X) \\mid S=1] = \\frac{\\mathbb{E}[g(X)p(X)]}{\\mathbb{E}[p(X)]} $$\nwhere the expectations are taken over the original (unselected) distribution of $X \\sim \\mathcal{N}(0,1)$.\n\n### Bias of the Selected-Sample Mean of Y\n\nThe first objective is to compute the bias of the sample mean of $Y$ in the selected sample, which is $\\mathbb{E}[\\bar{Y} \\mid S=1] - \\mathbb{E}[Y]$. Assuming a large sample, $\\mathbb{E}[\\bar{Y} \\mid S=1]$ approaches $\\mathbb{E}[Y \\mid S=1]$.\n\n1.  **Population Mean $\\mathbb{E}[Y]$**:\n    The outcome model is $Y = \\beta X + \\epsilon$. The population mean of $Y$ is:\n    $$ \\mathbb{E}[Y] = \\mathbb{E}[\\beta X + \\epsilon] = \\beta\\mathbb{E}[X] + \\mathbb{E}[\\epsilon] $$\n    Given $X \\sim \\mathcal{N}(0,1)$ and $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, we have $\\mathbb{E}[X]=0$ and $\\mathbb{E}[\\epsilon]=0$.\n    Therefore, $\\mathbb{E}[Y] = 0$.\n\n2.  **Selected-Sample Mean $\\mathbb{E}[Y \\mid S=1]$**:\n    $$ \\mathbb{E}[Y \\mid S=1] = \\mathbb{E}[\\beta X + \\epsilon \\mid S=1] = \\beta\\mathbb{E}[X \\mid S=1] + \\mathbb{E}[\\epsilon \\mid S=1] $$\n    We must evaluate $\\mathbb{E}[\\epsilon \\mid S=1]$. The selection mechanism $S$ depends only on $X$. Since $\\epsilon$ is independent of $X$, it is also independent of $S$. Formally:\n    $$ \\mathbb{E}[\\epsilon \\mid S=1] = \\frac{\\mathbb{E}[\\epsilon \\cdot \\mathbb{I}(S=1)]}{P(S=1)} = \\frac{\\mathbb{E}_X[\\mathbb{E}_\\epsilon[\\epsilon \\cdot p(X) \\mid X]]}{P(S=1)} = \\frac{\\mathbb{E}_X[p(X)\\mathbb{E}_\\epsilon[\\epsilon \\mid X]]}{P(S=1)} $$\n    As $\\epsilon$ is independent of $X$, $\\mathbb{E}[\\epsilon \\mid X] = \\mathbb{E}[\\epsilon] = 0$. Thus, $\\mathbb{E}[\\epsilon \\mid S=1] = 0$.\n    This simplifies the selected-sample mean to:\n    $$ \\mathbb{E}[Y \\mid S=1] = \\beta\\mathbb{E}[X \\mid S=1] $$\n\n3.  **Bias Calculation**:\n    The bias of the sample mean is:\n    $$ \\mathrm{Bias}(\\bar{Y}_S) = \\mathbb{E}[Y \\mid S=1] - \\mathbb{E}[Y] = \\beta\\mathbb{E}[X \\mid S=1] - 0 = \\beta\\mathbb{E}[X \\mid S=1] $$\n    Using our general formula for conditional expectations:\n    $$ \\mathbb{E}[X \\mid S=1] = \\frac{\\mathbb{E}[X \\cdot p(X)]}{\\mathbb{E}[p(X)]} $$\n    Let us define the following expectations which will be computed numerically:\n    $E_0 = \\mathbb{E}[p(X)] = \\mathbb{E}[\\operatorname{logit}^{-1}(\\alpha + \\gamma X)]$\n    $E_1 = \\mathbb{E}[X \\cdot p(X)] = \\mathbb{E}[X \\cdot \\operatorname{logit}^{-1}(\\alpha + \\gamma X)]$\n    The bias of the mean is then:\n    $$ \\mathrm{Bias}_1 = \\beta \\frac{E_1}{E_0} $$\n\n### Asymptotic Bias of the Selected-Sample OLS Slope\n\nThe second objective is to compute the asymptotic bias of the OLS slope, defined as $\\frac{\\operatorname{Cov}(X,Y \\mid S=1)}{\\operatorname{Var}(X \\mid S=1)} - \\beta$.\n\n1.  **Conditional Covariance and Variance**:\n    We need to compute $\\operatorname{Var}(X \\mid S=1)$ and $\\operatorname{Cov}(X,Y \\mid S=1)$.\n    The variance is given by $\\operatorname{Var}(X \\mid S=1) = \\mathbb{E}[X^2 \\mid S=1] - (\\mathbb{E}[X \\mid S=1])^2$.\n    The covariance is $\\operatorname{Cov}(X,Y \\mid S=1) = \\mathbb{E}[XY \\mid S=1] - \\mathbb{E}[X \\mid S=1]\\mathbb{E}[Y \\mid S=1]$.\n\n2.  **Derivation of the Conditional Slope**:\n    A more direct approach utilizes the law of total expectation. The regression function of $Y$ on $X$ in the selected subpopulation is $\\mathbb{E}[Y \\mid X, S=1]$.\n    $$ \\mathbb{E}[Y \\mid X, S=1] = \\mathbb{E}[\\beta X + \\epsilon \\mid X, S=1] = \\beta X + \\mathbb{E}[\\epsilon \\mid X, S=1] $$\n    Since selection $S$ depends only on $X$, conditioning on $X$ makes $S$ a constant. The independence of $\\epsilon$ and $X$ implies $\\epsilon$ is independent of the event $S=1$ given $X$. Therefore, $\\mathbb{E}[\\epsilon \\mid X, S=1] = \\mathbb{E}[\\epsilon \\mid X] = \\mathbb{E}[\\epsilon] = 0$.\n    This yields a crucial result:\n    $$ \\mathbb{E}[Y \\mid X, S=1] = \\beta X $$\n    This means that even within the selected subpopulation, the conditional expectation of $Y$ given $X$ remains linear in $X$ with the same slope $\\beta$. The OLS estimator converges in probability to the slope of the best linear predictor of $Y$ from $X$. Since the relationship is exactly linear, OLS is consistent for $\\beta$.\n    To formally prove this using the covariance/variance ratio:\n    - $\\mathbb{E}[Y \\mid S=1] = \\mathbb{E}[\\mathbb{E}[Y \\mid X, S=1] \\mid S=1] = \\mathbb{E}[\\beta X \\mid S=1] = \\beta \\mathbb{E}[X \\mid S=1]$.\n    - $\\mathbb{E}[XY \\mid S=1] = \\mathbb{E}[\\mathbb{E}[XY \\mid X, S=1] \\mid S=1] = \\mathbb{E}[X \\mathbb{E}[Y \\mid X, S=1] \\mid S=1] = \\mathbb{E}[X(\\beta X) \\mid S=1] = \\beta \\mathbb{E}[X^2 \\mid S=1]$.\n    Substituting these into the covariance formula:\n    $$ \\operatorname{Cov}(X,Y \\mid S=1) = \\beta \\mathbb{E}[X^2 \\mid S=1] - \\mathbb{E}[X \\mid S=1](\\beta \\mathbb{E}[X \\mid S=1]) $$\n    $$ = \\beta (\\mathbb{E}[X^2 \\mid S=1] - (\\mathbb{E}[X \\mid S=1])^2) = \\beta \\operatorname{Var}(X \\mid S=1) $$\n    The OLS slope in the selected sample is therefore:\n    $$ \\hat{\\beta}_{\\text{OLS},S} \\xrightarrow{p} \\frac{\\operatorname{Cov}(X,Y \\mid S=1)}{\\operatorname{Var}(X \\mid S=1)} = \\frac{\\beta \\operatorname{Var}(X \\mid S=1)}{\\operatorname{Var}(X \\mid S=1)} = \\beta $$\n    The asymptotic bias is:\n    $$ \\mathrm{Bias}_2 = \\beta - \\beta = 0 $$\n    This holds for any values of the parameters $(\\alpha, \\gamma, \\beta, \\sigma)$, provided $\\operatorname{Var}(X \\mid S=1) > 0$. Notice that the noise variance $\\sigma^2$ does not appear in the formulas for either bias.\n\n### Numerical Evaluation\n\nThe expectations $E_0$ and $E_1$ are computed using Gauss-Hermite quadrature as specified. For a function $g(X)$ with $X \\sim \\mathcal{N}(0,1)$:\n$$ \\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x)\\frac{e^{-x^2/2}}{\\sqrt{2\\pi}}dx $$\nThe problem provides a formula using nodes and weights $\\{z_k, w_k\\}_{k=1}^K$ for the weight function $e^{-x^2}$:\n$$ \\mathbb{E}[g(X)] = \\frac{1}{\\sqrt{\\pi}} \\sum_{k=1}^{K} w_k\\, g(\\sqrt{2}\\, z_k) $$\nWe use this formula for $g_0(x) = p(x)$ and $g_1(x) = x \\cdot p(x)$ to find $E_0$ and $E_1$ respectively. The implementation will use a sufficiently large number of nodes, $K$, for accuracy.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_hermite\n\ndef solve():\n    \"\"\"\n    Computes the biases of the sample mean and OLS slope under convenience sampling.\n    \"\"\"\n    # Test cases as specified in the problem statement.\n    test_cases = [\n        (0.0, 1.0, 0.8, 1.0),\n        (0.2, 0.0, 1.2, 2.0),\n        (-0.5, 2.0, -0.5, 1.5),\n        (1.0, -1.5, 1.0, 0.5),\n        (-3.0, 1.0, 0.4, 1.0),\n        (0.0, 1.0, 0.0, 1.0)\n    ]\n\n    # --- Setup for Gauss-Hermite Quadrature ---\n    # Number of quadrature points for numerical stability and precision.\n    K = 100\n    \n    # Get nodes (z) and weights (w) for the weight function exp(-x^2).\n    # These are for the \"physicists'\" Hermite polynomials.\n    z, w = roots_hermite(K)\n    \n    # The integration formula requires evaluating the function at sqrt(2)*z_k.\n    x_nodes = np.sqrt(2.0) * z\n    \n    # The formula also has a pre-factor of 1/sqrt(pi).\n    pi_inv_sqrt = 1.0 / np.sqrt(np.pi)\n\n    def logit_inv(t):\n        \"\"\"Computes the inverse logit (logistic sigmoid) function.\"\"\"\n        # Use np.exp which is robust to large arguments.\n        return 1.0 / (1.0 + np.exp(-t))\n\n    results = []\n    \n    for case in test_cases:\n        alpha, gamma, beta, sigma = case\n        \n        # --- Bias of the Sample Mean of Y ---\n        # The bias is given by beta * E[X | S=1].\n        # E[X | S=1] = E[X * p(X)] / E[p(X)], where p(X) is the selection probability.\n\n        # Let p(x) = logit_inv(alpha + gamma * x).\n        # We need to compute E_0 = E[p(X)] and E_1 = E[X * p(X)].\n        \n        # Evaluate p(x) at the quadrature nodes.\n        p_x_nodes = logit_inv(alpha + gamma * x_nodes)\n        \n        # Function g(x) = p(x) for expectation E_0.\n        # Evaluate g(x_k) for all nodes k. The values are simply p_x_nodes.\n        \n        # E_0 = (1/sqrt(pi)) * sum(w_k * p(x_k))\n        E0 = np.sum(w * p_x_nodes) * pi_inv_sqrt\n\n        # Function g(x) = x * p(x) for expectation E_1.\n        # Evaluate g(x_k) = x_k * p(x_k) for all nodes k.\n        g1_nodes = x_nodes * p_x_nodes\n        \n        # E_1 = (1/sqrt(pi)) * sum(w_k * x_k * p(x_k))\n        E1 = np.sum(w * g1_nodes) * pi_inv_sqrt\n        \n        # If the selection probability is zero almost everywhere, E0 will be near zero.\n        # In this situation, the conditional expectation is ill-defined, but practically\n        # the bias would be considered zero as no sample is selected.\n        if np.isclose(E0, 0.0):\n            bias_mean_y = 0.0\n        else:\n            # E[X|S=1] = E1 / E0\n            # Bias = beta * E[X|S=1]\n            bias_mean_y = beta * (E1 / E0)\n\n        # --- Bias of the OLS Slope ---\n        # As derived in the solution, when selection depends only on the covariate X\n        # and not the outcome Y or the error term epsilon, the OLS estimator for the slope\n        # remains consistent. Its asymptotic bias is zero.\n        bias_ols_slope = 0.0\n\n        results.append(round(bias_mean_y, 6))\n        results.append(round(bias_ols_slope, 6))\n\n    # Format the final output as a comma-separated list in brackets.\n    output_str = \",\".join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Before applying any adjustment technique, a crucial first step is to diagnose the extent of the problem. This practice introduces the absolute standardized difference (ASD), a fundamental metric used in biostatistics to quantify the imbalance between a convenience sample and a target population across key covariates . By calculating the ASD for both continuous and binary variables, you will learn a standard procedure for assessing sample representativeness and identifying which covariates require the most urgent attention for adjustment.",
            "id": "4932704",
            "problem": "A biostatistics team evaluates the representativeness of a nonprobability convenience sample of adult volunteers recruited via a smartphone application relative to a known target population in the same region. Four covariates are assessed: age, body mass index, sex (female), and current smoking status. For the convenience sample of $n_{s} = 600$ registrants, the observed summaries are: mean age $\\bar{x}_{\\text{age}, s} = 35$ years with sample standard deviation $s_{\\text{age}, s} = 12$ years; mean body mass index $\\bar{x}_{\\text{BMI}, s} = 26.8$ $\\mathrm{kg}/\\mathrm{m}^{2}$ with sample standard deviation $s_{\\text{BMI}, s} = 5.0$ $\\mathrm{kg}/\\mathrm{m}^{2}$; number of females $= 390$; number of current smokers $= 60$. From a validated population registry (treated as the reference population) with $N_{p} = 50{,}000$, the reference summaries are: mean age $\\mu_{\\text{age}, p} = 42$ years with population standard deviation $\\sigma_{\\text{age}, p} = 14$ years; mean body mass index $\\mu_{\\text{BMI}, p} = 27.5$ $\\mathrm{kg}/\\mathrm{m}^{2}$ with population standard deviation $\\sigma_{\\text{BMI}, p} = 5.2$ $\\mathrm{kg}/\\mathrm{m}^{2}$; number of females $= 26{,}000$; number of current smokers $= 9{,}000$.\n\nStarting from core definitions of mean, variance, and proportion, compute the absolute standardized differences for each covariate comparing the convenience sample to the population reference, treating continuous covariates (age, body mass index) as measured on a numeric scale and binary covariates (female, current smoker) as proportions. Adopt the following thresholds to flag poor representativeness: an absolute standardized difference exceeding $0.2$ indicates poor representativeness; values less than or equal to $0.1$ indicate acceptable representativeness. Report the single integer equal to the number of covariates that are flagged for poor representativeness under these thresholds. No rounding instruction is needed because the required final answer is a count without units.",
            "solution": "The problem statement is assessed for validity.\n\n### Step 1: Extract Givens\nThe following data and conditions are provided:\n\n**Convenience Sample ($s$) Data:**\n- Sample size: $n_{s} = 600$\n- Mean age: $\\bar{x}_{\\text{age}, s} = 35$ years\n- Sample standard deviation of age: $s_{\\text{age}, s} = 12$ years\n- Mean body mass index (BMI): $\\bar{x}_{\\text{BMI}, s} = 26.8$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- Sample standard deviation of BMI: $s_{\\text{BMI}, s} = 5.0$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- Number of females: $390$\n- Number of current smokers: $60$\n\n**Reference Population ($p$) Data:**\n- Population size: $N_{p} = 50{,}000$\n- Population mean age: $\\mu_{\\text{age}, p} = 42$ years\n- Population standard deviation of age: $\\sigma_{\\text{age}, p} = 14$ years\n- Population mean BMI: $\\mu_{\\text{BMI}, p} = 27.5$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- Population standard deviation of BMI: $\\sigma_{\\text{BMI}, p} = 5.2$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- Population number of females: $26{,}000$\n- Population number of current smokers: $9{,}000$\n\n**Thresholds and Task:**\n- Threshold for poor representativeness: absolute standardized difference $> 0.2$\n- Threshold for acceptable representativeness: absolute standardized difference $\\le 0.1$\n- Task: Compute the number of covariates flagged for poor representativeness.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, employing the established biostatistical concept of standardized difference to evaluate sample representativeness, a common task in assessing selection bias. All provided data for human demographics and health metrics (age, BMI, sex, smoking status) are realistic. The problem is well-posed, providing all necessary information and clear, objective criteria for reaching a unique conclusion. It is free of ambiguity, contradictions, or subjective claims. The problem does not violate any of the specified invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe primary task is to compute the absolute standardized difference (ASD) for each of the four covariates. The definition of the ASD depends on whether the covariate is continuous or binary.\n\nFor a continuous covariate, the ASD is defined as the absolute difference between the sample mean ($\\bar{x}_s$) and the population mean ($\\mu_p$), standardized by the population standard deviation ($\\sigma_p$).\n$$\n\\text{ASD}_{\\text{continuous}} = \\frac{|\\bar{x}_s - \\mu_p|}{\\sigma_p}\n$$\nThis is the appropriate formulation when comparing a sample to a known reference population for which the true parameter $\\sigma_p$ is available.\n\nFor a binary covariate, characterized by a proportion, the ASD is defined as the absolute difference between the sample proportion ($p_s$) and the population proportion ($P_p$), standardized by the standard deviation of the Bernoulli distribution in the population, which is $\\sqrt{P_p(1-P_p)}$.\n$$\n\\text{ASD}_{\\text{binary}} = \\frac{|p_s - P_p|}{\\sqrt{P_p(1 - P_p)}}\n$$\n\nWe will now calculate the ASD for each of the four covariates.\n\n**1. Covariate: Age (Continuous)**\nThe givens are $\\bar{x}_{\\text{age}, s} = 35$, $\\mu_{\\text{age}, p} = 42$, and $\\sigma_{\\text{age}, p} = 14$.\n$$\n\\text{ASD}_{\\text{age}} = \\frac{|\\bar{x}_{\\text{age}, s} - \\mu_{\\text{age}, p}|}{\\sigma_{\\text{age}, p}} = \\frac{|35 - 42|}{14} = \\frac{|-7|}{14} = \\frac{7}{14} = 0.5\n$$\n\n**2. Covariate: Body Mass Index (BMI) (Continuous)**\nThe givens are $\\bar{x}_{\\text{BMI}, s} = 26.8$, $\\mu_{\\text{BMI}, p} = 27.5$, and $\\sigma_{\\text{BMI}, p} = 5.2$.\n$$\n\\text{ASD}_{\\text{BMI}} = \\frac{|\\bar{x}_{\\text{BMI}, s} - \\mu_{\\text{BMI}, p}|}{\\sigma_{\\text{BMI}, p}} = \\frac{|26.8 - 27.5|}{5.2} = \\frac{|-0.7|}{5.2} = \\frac{0.7}{5.2} \\approx 0.1346\n$$\n\n**3. Covariate: Sex (Female) (Binary)**\nFirst, we compute the sample and population proportions.\nThe sample proportion of females, $p_{\\text{female}, s}$, is:\n$$\np_{\\text{female}, s} = \\frac{\\text{Number of females in sample}}{\\text{Sample size}} = \\frac{390}{600} = \\frac{39}{60} = \\frac{13}{20} = 0.65\n$$\nThe population proportion of females, $P_{\\text{female}, p}$, is:\n$$\nP_{\\text{female}, p} = \\frac{\\text{Number of females in population}}{\\text{Population size}} = \\frac{26{,}000}{50{,}000} = \\frac{26}{50} = \\frac{13}{25} = 0.52\n$$\nNow, we compute the ASD:\n$$\n\\text{ASD}_{\\text{female}} = \\frac{|p_{\\text{female}, s} - P_{\\text{female}, p}|}{\\sqrt{P_{\\text{female}, p}(1 - P_{\\text{female}, p})}} = \\frac{|0.65 - 0.52|}{\\sqrt{0.52(1 - 0.52)}} = \\frac{0.13}{\\sqrt{0.52 \\times 0.48}} = \\frac{0.13}{\\sqrt{0.2496}} \\approx 0.2602\n$$\n\n**4. Covariate: Current Smoking Status (Binary)**\nFirst, we compute the sample and population proportions.\nThe sample proportion of smokers, $p_{\\text{smoker}, s}$, is:\n$$\np_{\\text{smoker}, s} = \\frac{\\text{Number of smokers in sample}}{\\text{Sample size}} = \\frac{60}{600} = \\frac{1}{10} = 0.1\n$$\nThe population proportion of smokers, $P_{\\text{smoker}, p}$, is:\n$$\nP_{\\text{smoker}, p} = \\frac{\\text{Number of smokers in population}}{\\text{Population size}} = \\frac{9{,}000}{50{,}000} = \\frac{9}{50} = 0.18\n$$\nNow, we compute the ASD:\n$$\n\\text{ASD}_{\\text{smoker}} = \\frac{|p_{\\text{smoker}, s} - P_{\\text{smoker}, p}|}{\\sqrt{P_{\\text{smoker}, p}(1 - P_{\\text{smoker}, p})}} = \\frac{|0.1 - 0.18|}{\\sqrt{0.18(1 - 0.18)}} = \\frac{|-0.08|}{\\sqrt{0.18 \\times 0.82}} = \\frac{0.08}{\\sqrt{0.1476}} \\approx 0.2082\n$$\n\nFinally, we compare each ASD to the threshold for poor representativeness, which is $> 0.2$.\n\n- **Age**: $\\text{ASD}_{\\text{age}} = 0.5$. Since $0.5 > 0.2$, this covariate is flagged.\n- **BMI**: $\\text{ASD}_{\\text{BMI}} \\approx 0.1346$. Since $0.1346 \\le 0.2$, this covariate is not flagged.\n- **Sex (Female)**: $\\text{ASD}_{\\text{female}} \\approx 0.2602$. Since $0.2602 > 0.2$, this covariate is flagged.\n- **Smoking**: $\\text{ASD}_{\\text{smoker}} \\approx 0.2082$. Since $0.2082 > 0.2$, this covariate is flagged.\n\nThe covariates flagged for poor representativeness are age, sex (female), and current smoking status. The total count of flagged covariates is $3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "Having understood and diagnosed selection bias, we now turn to a powerful correction method: Multilevel Regression and Post-stratification (MRP). This advanced practice guides you through implementing an MRP estimator, which combines the strengths of statistical modeling with known population data to produce robust, debiased estimates . You will build a hierarchical model to predict outcomes within demographic strata and then post-stratify these predictions using population weights, a state-of-the-art approach for analysis of nonprobability samples.",
            "id": "4932631",
            "problem": "You are given a convenience sample drawn from a finite population that is stratified by categorical covariates collectively denoted by $X$. The outcome $Y$ is binary. Because the sample is a convenience sample, the sample distribution of $X$ may not equal the population distribution of $X$, leading to selection bias in naive estimates of the population mean of $Y$. Your task is to construct a Multilevel Regression and Post-Stratification (MRP) estimator that combines a hierarchical outcome model with external population marginals for $X$.\n\nFundamental basis and modeling assumptions:\n- By the law of total expectation, the population mean of $Y$ satisfies $\\mathbb{E}[Y] = \\sum_{s=1}^{S} \\mathbb{E}[Y \\mid X=s] \\Pr(X=s)$, where $s$ indexes strata defined by $X$ and $S$ is the total number of strata.\n- In the presence of nonprobability (convenience) sampling, the sample distribution of $X$ does not necessarily equal the population distribution. However, if we can model $\\mathbb{E}[Y \\mid X=s]$ and we have external information on $\\Pr_{\\text{pop}}(X=s)$, then we can estimate $\\mathbb{E}[Y]$ by combining model-based estimates of $\\mathbb{E}[Y \\mid X=s]$ with population marginals $\\Pr_{\\text{pop}}(X=s)$.\n- Use a hierarchical outcome model with partial pooling across strata. Specifically, parameterize the stratum-level mean via a logistic link: for stratum $s$, let $p_s = \\Pr(Y=1 \\mid X=s)$ and model $\\text{logit}(p_s) = \\alpha + b_s$, where $\\alpha$ is a global intercept and $b_s$ are stratum-specific random effects. Impose Gaussian shrinkage by maximizing a penalized log-likelihood that corresponds to $b_s \\sim \\mathcal{N}(0, \\sigma^2)$, equivalently a ridge penalty with tuning parameter $\\lambda = 1/\\sigma^2$.\n\nPenalized likelihood with aggregated data:\n- You observe, for each stratum $s$, a convenience sample size $n_s$ and the number of observed successes $y_s$. Let $\\sigma(z) = 1/(1 + e^{-z})$ denote the logistic function. The penalized log-likelihood for parameters $(\\alpha, b_1, \\dots, b_S)$ is\n$$\n\\ell(\\alpha, b) = \\sum_{s=1}^{S} \\left[ y_s \\log\\left(\\sigma(\\alpha + b_s)\\right) + (n_s - y_s) \\log\\left(1 - \\sigma(\\alpha + b_s)\\right) \\right] - \\frac{\\lambda}{2} \\sum_{s=1}^{S} b_s^2.\n$$\nYou must find the maximizer $(\\hat{\\alpha}, \\hat{b})$ using Newton–Raphson on the full parameter vector and then compute $\\hat{p}_s = \\sigma(\\hat{\\alpha} + \\hat{b}_s)$.\n\nPost-stratification:\n- Given external population marginals $w_s = \\Pr_{\\text{pop}}(X=s)$ with $\\sum_{s=1}^{S} w_s = 1$, compute the MRP estimator of the population mean of $Y$ as\n$$\n\\hat{\\mu}_{\\text{MRP}} = \\sum_{s=1}^{S} w_s \\hat{p}_s.\n$$\n\nAlgorithmic specification for Newton–Raphson:\n- Define the parameter vector $\\theta = (\\alpha, b_1, \\dots, b_S)^\\top$.\n- At a current iterate, compute $p_s = \\sigma(\\alpha + b_s)$ for all $s$.\n- The gradient components are\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{s=1}^{S} (y_s - n_s p_s), \\quad\n\\frac{\\partial \\ell}{\\partial b_s} = (y_s - n_s p_s) - \\lambda b_s.\n$$\n- The Hessian entries are\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = -\\sum_{s=1}^{S} n_s p_s (1 - p_s), \\quad\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\, \\partial b_s} = - n_s p_s (1 - p_s), \\quad\n\\frac{\\partial^2 \\ell}{\\partial b_s^2} = - n_s p_s (1 - p_s) - \\lambda,\n$$\nwith $\\frac{\\partial^2 \\ell}{\\partial b_s \\, \\partial b_{s'}} = 0$ for $s \\neq s'$.\n- Solve the linear system $H(\\theta) \\, \\Delta = - \\nabla \\ell(\\theta)$ and update $\\theta \\leftarrow \\theta + \\Delta$ until convergence.\n\nTest suite and required outputs:\n- There are $S = 6$ strata in each case. For each test case, you are given $(n_s)$, $(y_s)$, $(w_s)$, a true stratum probability vector $(p^{\\star}_s)$ used only for evaluation, and a penalty $\\lambda$. Use the provided $(n_s)$ and $(y_s)$ to fit the hierarchical model, use $(w_s)$ to post-stratify, and compute $\\hat{\\mu}_{\\text{MRP}}$.\n\n- Test Case $1$ (happy path, moderate bias):\n    - $n = [20, 15, 15, 25, 15, 60]$\n    - $y = [2, 2, 2, 8, 5, 24]$\n    - $w = [0.2, 0.15, 0.25, 0.15, 0.1, 0.15]$\n    - $p^{\\star} = [0.1, 0.12, 0.15, 0.3, 0.35, 0.4]$\n    - $\\lambda = 1.0$\n- Test Case $2$ (boundary with a missing stratum in the sample):\n    - $n = [10, 5, 40, 0, 10, 5]$\n    - $y = [2, 1, 20, 0, 1, 1]$\n    - $w = [0.1, 0.25, 0.1, 0.2, 0.2, 0.15]$\n    - $p^{\\star} = [0.18, 0.22, 0.5, 0.05, 0.1, 0.12]$\n    - $\\lambda = 2.5$\n- Test Case $3$ (severe imbalance, oversampling low-risk strata):\n    - $n = [200, 150, 10, 5, 2, 1]$\n    - $y = [10, 12, 1, 1, 1, 1]$\n    - $w = [0.15, 0.15, 0.2, 0.2, 0.15, 0.15]$\n    - $p^{\\star} = [0.05, 0.08, 0.12, 0.25, 0.5, 0.55]$\n    - $\\lambda = 0.5$\n\nCompute, for each test case, the MRP estimator $\\hat{\\mu}_{\\text{MRP}}$ and produce them as the final program output. All outputs are unitless floats. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1, r_2, r_3]$), where $r_i$ is $\\hat{\\mu}_{\\text{MRP}}$ for Test Case $i$ in the order given. No other text should be printed.",
            "solution": "The problem requires the construction and application of a Multilevel Regression and Post-stratification (MRP) estimator to correct for selection bias in a convenience sample. The solution involves maximizing a penalized log-likelihood using the Newton-Raphson method and then re-weighting the model-based predictions to match known population demographics.\n\nFirst, we formalize the problem. We are given data from a convenience sample stratified into $S$ distinct strata. For each stratum $s \\in \\{1, \\dots, S\\}$, we have the sample size $n_s$ and the number of \"successes\" $y_s$ for a binary outcome $Y$. We are also provided with the population-level proportions $w_s$ for each stratum. The goal is to estimate the population mean, $\\mu = \\mathbb{E}[Y]$.\n\nThe core of the MRP approach lies in the Law of Total Expectation:\n$$\n\\mu = \\mathbb{E}[Y] = \\sum_{s=1}^{S} \\mathbb{E}[Y \\mid X=s] \\Pr_{\\text{pop}}(X=s) = \\sum_{s=1}^{S} p_s w_s\n$$\nwhere $p_s = \\Pr(Y=1 \\mid X=s)$ is the true success probability in stratum $s$, and $w_s$ is the true population proportion of that stratum. While the sample distribution may be biased, we can estimate $\\mu$ if we can obtain reliable estimates of $p_s$ and know the true $w_s$.\n\nWe model the stratum-specific probabilities $p_s$ using a hierarchical logistic regression model. This allows for partial pooling of information across strata, which is particularly useful for strata with small sample sizes. The model is specified as:\n$$\n\\text{logit}(p_s) = \\log\\left(\\frac{p_s}{1-p_s}\\right) = \\alpha + b_s\n$$\nHere, $\\alpha$ is a global intercept representing the average log-odds of success across all strata, and $b_s$ is a stratum-specific random effect capturing the deviation of stratum $s$ from this average. The model assumes these random effects are drawn from a common distribution, $b_s \\sim \\mathcal{N}(0, \\sigma^2)$. This assumption introduces a shrinkage effect, pulling the estimates for $b_s$ towards $0$, which helps to regularize the model and prevent overfitting, especially in strata with sparse data.\n\nThe estimation of the parameters $(\\alpha, b_1, \\dots, b_S)$ is performed by maximizing a penalized log-likelihood. The likelihood of the observed data $(y_s, n_s)$ across all strata is given by the product of binomial likelihoods. The penalty term arises from the prior on the random effects, $b_s \\sim \\mathcal{N}(0, \\sigma^2)$, which is equivalent to an $L_2$ (ridge) penalty on the coefficients $b_s$. Letting $\\lambda = 1/\\sigma^2$, the penalized log-likelihood function to be maximized is:\n$$\n\\ell(\\alpha, b) = \\sum_{s=1}^{S} \\left[ y_s \\log\\left(\\sigma(\\alpha + b_s)\\right) + (n_s - y_s) \\log\\left(1 - \\sigma(\\alpha + b_s)\\right) \\right] - \\frac{\\lambda}{2} \\sum_{s=1}^{S} b_s^2\n$$\nwhere $\\sigma(z) = 1/(1 + e^{-z})$ is the logistic function. Strata where $n_s=0$ contribute nothing to the sum over the data likelihood but are still affected by the penalty on their corresponding $b_s$, which effectively shrinks $\\hat{b}_s$ towards $0$.\n\nTo find the parameters $(\\hat{\\alpha}, \\hat{b})$ that maximize this function, we employ the Newton-Raphson algorithm, an iterative second-order optimization method. Let the full parameter vector be $\\theta = (\\alpha, b_1, \\dots, b_S)^\\top$. The iterative update rule is:\n$$\n\\theta_{k+1} = \\theta_k - [H(\\theta_k)]^{-1} \\nabla \\ell(\\theta_k)\n$$\nwhere $\\nabla \\ell(\\theta_k)$ is the gradient vector and $H(\\theta_k)$ is the Hessian matrix of the penalized log-likelihood, both evaluated at the current parameter estimate $\\theta_k$.\n\nThe gradient vector $\\nabla \\ell(\\theta)$ has components:\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{s=1}^{S} (y_s - n_s p_s)\n$$\n$$\n\\frac{\\partial \\ell}{\\partial b_s} = (y_s - n_s p_s) - \\lambda b_s \\quad \\text{for } s = 1, \\dots, S\n$$\nwhere $p_s = \\sigma(\\alpha + b_s)$.\n\nThe Hessian matrix $H(\\theta)$ is a symmetric $(S+1) \\times (S+1)$ matrix. Its entries are given by the second partial derivatives of $\\ell$. Let $W_s = n_s p_s(1-p_s)$. The entries are:\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = -\\sum_{s=1}^{S} W_s\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\, \\partial b_s} = -W_s\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial b_s^2} = -W_s - \\lambda\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial b_s \\, \\partial b_{s'}} = 0 \\quad \\text{for } s \\neq s'\n$$\nThe Hessian has a bordered diagonal structure. The Newton-Raphson update step $\\Delta_k = -[H(\\theta_k)]^{-1} \\nabla \\ell(\\theta_k)$ is computed by solving the linear system $H(\\theta_k) \\Delta_k = -\\nabla \\ell(\\theta_k)$.\n\nThe algorithm proceeds as follows:\n1. Initialize parameters, e.g., $\\theta_0 = \\mathbf{0}$.\n2. Iterate for $k=0, 1, 2, \\dots$ until convergence:\n   a. For the current $\\theta_k = (\\alpha_k, b_{1,k}, \\dots, b_{S,k})^\\top$, compute $p_s = \\sigma(\\alpha_k + b_{s,k})$ for all strata.\n   b. Compute the gradient vector $\\nabla \\ell(\\theta_k)$ and the Hessian matrix $H(\\theta_k)$.\n   c. Solve the linear system $H(\\theta_k) \\Delta_k = -\\nabla \\ell(\\theta_k)$ for the update step $\\Delta_k$.\n   d. Update the parameters: $\\theta_{k+1} = \\theta_k + \\Delta_k$.\n3. Convergence is achieved when the norm of the update vector, $\\|\\Delta_k\\|$, falls below a small tolerance.\n\nOnce the algorithm converges to the optimal parameters $(\\hat{\\alpha}, \\hat{b})$, we compute the estimated stratum probabilities:\n$$\n\\hat{p}_s = \\sigma(\\hat{\\alpha} + \\hat{b}_s) \\quad \\text{for } s = 1, \\dots, S\n$$\nFor a stratum $s$ with $n_s=0$, the estimate $\\hat{b}_s$ is driven to $0$ by the penalty term, so its predicted probability will be $\\hat{p}_s \\approx \\sigma(\\hat{\\alpha})$, borrowing strength from all other strata.\n\nFinally, the post-stratification step combines these model-based probability estimates with the known population weights $w_s$ to compute the MRP estimator for the population mean:\n$$\n\\hat{\\mu}_{\\text{MRP}} = \\sum_{s=1}^{S} w_s \\hat{p}_s\n$$\nThis final value is the debiased estimate of the population mean of $Y$.",
            "answer": "```python\nimport numpy as np\n\ndef _compute_mrp_estimate(n, y, w, lambda_val, tol=1e-8, max_iter=25):\n    \"\"\"\n    Computes the MRP estimate by maximizing a penalized logistic regression model.\n\n    Args:\n        n (list): List of sample sizes per stratum.\n        y (list): List of success counts per stratum.\n        w (list): List of population proportions per stratum.\n        lambda_val (float): The ridge penalty parameter.\n        tol (float): Convergence tolerance for the Newton-Raphson algorithm.\n        max_iter (int): Maximum number of iterations for Newton-Raphson.\n\n    Returns:\n        float: The Multilevel Regression and Post-stratification (MRP) estimate.\n    \"\"\"\n    n_arr = np.array(n, dtype=float)\n    y_arr = np.array(y, dtype=float)\n    w_arr = np.array(w, dtype=float)\n    S = len(n_arr)\n\n    # Initialize parameters theta = (alpha, b_1, ..., b_S)\n    theta = np.zeros(S + 1)\n\n    for i in range(max_iter):\n        alpha = theta[0]\n        b = theta[1:]\n\n        # Linear predictor eta = alpha + b_s for each stratum s\n        eta = alpha + b\n        \n        # Probabilities p_s = sigma(eta_s)\n        # Clip eta to prevent overflow in exp\n        eta = np.clip(eta, -500, 500)\n        p = 1.0 / (1.0 + np.exp(-eta))\n\n        # Gradient of the penalized log-likelihood\n        grad = np.zeros(S + 1)\n        # Residuals for the likelihood part\n        residual = y_arr - n_arr * p\n        # d(ell)/d(alpha)\n        grad[0] = np.sum(residual)\n        # d(ell)/d(b_s)\n        grad[1:] = residual - lambda_val * b\n\n        # Hessian matrix of the penalized log-likelihood\n        H = np.zeros((S + 1, S + 1))\n        # Weights for the Hessian W_s = n_s * p_s * (1 - p_s)\n        W = n_arr * p * (1.0 - p)\n        \n        # Fill the Hessian matrix based on its structure\n        # H_00 = d^2(ell)/d(alpha^2)\n        H[0, 0] = -np.sum(W)\n        # H_0s = H_s0 = d^2(ell)/d(alpha)d(b_s)\n        H[0, 1:] = -W\n        H[1:, 0] = -W\n        \n        # Diagonal elements for the b_s block: H_ss = d^2(ell)/d(b_s^2)\n        # This uses array broadcasting in a slightly non-obvious way. A direct\n        # assignment to the diagonal of a submatrix is safer.\n        H_sub = H[1:, 1:]\n        np.fill_diagonal(H_sub, -W - lambda_val)\n\n        # Newton-Raphson step: solve H * delta = -grad\n        try:\n            delta = np.linalg.solve(H, -grad)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if Hessian is singular, though unlikely with lambda > 0\n            # For this problem, we assume H is always invertible.\n            # Returning NaN on failure.\n            return np.nan\n        \n        # Update parameters\n        theta += delta\n\n        # Check for convergence\n        if np.linalg.norm(delta)  tol:\n            break\n\n    # After convergence, compute final estimates\n    final_alpha = theta[0]\n    final_b = theta[1:]\n\n    # Final stratum probabilities\n    final_eta = final_alpha + final_b\n    final_eta = np.clip(final_eta, -500, 500)\n    final_p = 1.0 / (1.0 + np.exp(-final_eta))\n\n    # Post-stratification to get the MRP estimate of the population mean\n    mu_mrp = np.sum(w_arr * final_p)\n    return mu_mrp\n\ndef solve():\n    \"\"\"\n    Main function to run the MRP estimation for the given test cases.\n    \"\"\"\n    # Test cases from the problem statement\n    test_cases = [\n        {\n            \"n\": [20, 15, 15, 25, 15, 60],\n            \"y\": [2, 2, 2, 8, 5, 24],\n            \"w\": [0.2, 0.15, 0.25, 0.15, 0.1, 0.15],\n            \"lambda_val\": 1.0\n        },\n        {\n            \"n\": [10, 5, 40, 0, 10, 5],\n            \"y\": [2, 1, 20, 0, 1, 1],\n            \"w\": [0.1, 0.25, 0.1, 0.2, 0.2, 0.15],\n            \"lambda_val\": 2.5\n        },\n        {\n            \"n\": [200, 150, 10, 5, 2, 1],\n            \"y\": [10, 12, 1, 1, 1, 1],\n            \"w\": [0.15, 0.15, 0.2, 0.2, 0.15, 0.15],\n            \"lambda_val\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _compute_mrp_estimate(\n            n=case[\"n\"],\n            y=case[\"y\"],\n            w=case[\"w\"],\n            lambda_val=case[\"lambda_val\"]\n        )\n        results.append(result)\n\n    # Format the output as a comma-separated list in brackets.\n    # We use a reasonable precision for floating point results.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}