{
    "hands_on_practices": [
        {
            "introduction": "在尝试纠正方便样本中的偏差之前，首先量化问题的严重程度至关重要。本练习将介绍绝对标准化差异，这是一个基本工具，用于比较样本特征与已知目标群体的特征，从而清晰地衡量样本的代表性。通过这个练习 ，你将学会如何为连续和二元协变量计算这一关键指标，并根据既定阈值评估样本的不平衡程度。",
            "id": "4932704",
            "problem": "一个生物统计团队评估了一个通过智能手机应用程序招募的成年志愿者非概率便利样本，相对于同一地区已知目标人群的代表性。评估了四个协变量：年龄、身体质量指数、性别（女性）和当前吸烟状况。对于$n_{s} = 600$名注册者的便利样本，观测到的摘要数据如下：平均年龄$\\bar{x}_{\\text{age}, s} = 35$岁，样本标准差$s_{\\text{age}, s} = 12$岁；平均身体质量指数$\\bar{x}_{\\text{BMI}, s} = 26.8$ $\\mathrm{kg}/\\mathrm{m}^{2}$，样本标准差$s_{\\text{BMI}, s} = 5.0$ $\\mathrm{kg}/\\mathrm{m}^{2}$；女性人数$= 390$；当前吸烟者人数$= 60$。从一个经过验证的人口登记数据（被视为参考人群），其中$N_{p} = 50{,}000$，参考摘要数据如下：平均年龄$\\mu_{\\text{age}, p} = 42$岁，总体标准差$\\sigma_{\\text{age}, p} = 14$岁；平均身体质量指数$\\mu_{\\text{BMI}, p} = 27.5$ $\\mathrm{kg}/\\mathrm{m}^{2}$，总体标准差$\\sigma_{\\text{BMI}, p} = 5.2$ $\\mathrm{kg}/\\mathrm{m}^{2}$；女性人数$= 26{,}000$；当前吸烟者人数$= 9{,}000$。\n\n从均值、方差和比例的核心定义出发，计算每个协变量的绝对标准化差异，以比较便利样本与人群参考。将连续协变量（年龄、身体质量指数）视为数值尺度测量，将二元协变量（女性、当前吸烟者）视为比例。采用以下阈值来标记代表性差：绝对标准化差异超过$0.2$表示代表性差；值小于或等于$0.1$表示代表性可接受。报告在这些阈值下被标记为代表性差的协变量的数量，结果为一个整数。无需四舍五入说明，因为要求的最终答案是一个没有单位的计数。",
            "solution": "评估问题陈述的有效性。\n\n### 第一步：提取已知信息\n提供了以下数据和条件：\n\n**便利样本 ($s$) 数据：**\n- 样本量：$n_{s} = 600$\n- 平均年龄：$\\bar{x}_{\\text{age}, s} = 35$ 岁\n- 年龄的样本标准差：$s_{\\text{age}, s} = 12$ 岁\n- 平均身体质量指数 (BMI)：$\\bar{x}_{\\text{BMI}, s} = 26.8$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- BMI的样本标准差：$s_{\\text{BMI}, s} = 5.0$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- 女性人数：$390$\n- 当前吸烟者人数：$60$\n\n**参考人群 ($p$) 数据：**\n- 人群规模：$N_{p} = 50{,}000$\n- 人群平均年龄：$\\mu_{\\text{age}, p} = 42$ 岁\n- 人群年龄标准差：$\\sigma_{\\text{age}, p} = 14$ 岁\n- 人群平均BMI：$\\mu_{\\text{BMI}, p} = 27.5$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- 人群BMI标准差：$\\sigma_{\\text{BMI}, p} = 5.2$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- 人群中女性人数：$26{,}000$\n- 人群中当前吸烟者人数：$9{,}000$\n\n**阈值和任务：**\n- 代表性差的阈值：绝对标准化差异 $> 0.2$\n- 代表性可接受的阈值：绝对标准化差异 $\\le 0.1$\n- 任务：计算被标记为代表性差的协变量数量。\n\n### 第二步：使用提取的已知信息进行验证\n该问题具有科学依据，采用了公认的生物统计学概念“标准化差异”来评估样本代表性，这是评估选择偏倚的常见任务。所有提供的人口统计学和健康指标数据（年龄、BMI、性别、吸烟状况）都是现实的。问题是适定的，提供了所有必要信息和清晰、客观的标准以达成唯一结论。问题没有歧义、矛盾或主观声明。该问题不违反任何指定的无效标准。\n\n### 第三步：结论与行动\n问题有效。将提供完整解答。\n\n主要任务是计算四个协变量中每一个的绝对标准化差异（ASD）。ASD的定义取决于协变量是连续的还是二元的。\n\n对于连续协变量，ASD定义为样本均值（$\\bar{x}_s$）与总体均值（$\\mu_p$）之间的绝对差值，并用总体标准差（$\\sigma_p$）进行标准化。\n$$\n\\text{ASD}_{\\text{连续}} = \\frac{|\\bar{x}_s - \\mu_p|}{\\sigma_p}\n$$\n当将样本与已知真实参数 $\\sigma_p$ 的参考人群进行比较时，这是合适的公式。\n\n对于以比例为特征的二元协变量，ASD定义为样本比例（$p_s$）与总体比例（$P_p$）之间的绝对差值，并用总体中伯努利分布的标准差 $\\sqrt{P_p(1-P_p)}$ 进行标准化。\n$$\n\\text{ASD}_{\\text{二元}} = \\frac{|p_s - P_p|}{\\sqrt{P_p(1 - P_p)}}\n$$\n\n我们现在将为四个协变量中的每一个计算ASD。\n\n**1. 协变量：年龄（连续）**\n已知数据为 $\\bar{x}_{\\text{age}, s} = 35$，$\\mu_{\\text{age}, p} = 42$，以及 $\\sigma_{\\text{age}, p} = 14$。\n$$\n\\text{ASD}_{\\text{age}} = \\frac{|\\bar{x}_{\\text{age}, s} - \\mu_{\\text{age}, p}|}{\\sigma_{\\text{age}, p}} = \\frac{|35 - 42|}{14} = \\frac{|-7|}{14} = \\frac{7}{14} = 0.5\n$$\n\n**2. 协变量：身体质量指数（BMI）（连续）**\n已知数据为 $\\bar{x}_{\\text{BMI}, s} = 26.8$，$\\mu_{\\text{BMI}, p} = 27.5$，以及 $\\sigma_{\\text{BMI}, p} = 5.2$。\n$$\n\\text{ASD}_{\\text{BMI}} = \\frac{|\\bar{x}_{\\text{BMI}, s} - \\mu_{\\text{BMI}, p}|}{\\sigma_{\\text{BMI}, p}} = \\frac{|26.8 - 27.5|}{5.2} = \\frac{|-0.7|}{5.2} = \\frac{0.7}{5.2} \\approx 0.1346\n$$\n\n**3. 协变量：性别（女性）（二元）**\n首先，我们计算样本和总体比例。\n女性的样本比例，$p_{\\text{female}, s}$，为：\n$$\np_{\\text{female}, s} = \\frac{\\text{样本中女性人数}}{\\text{样本量}} = \\frac{390}{600} = \\frac{39}{60} = \\frac{13}{20} = 0.65\n$$\n女性的总体比例，$P_{\\text{female}, p}$，为：\n$$\nP_{\\text{female}, p} = \\frac{\\text{人群中女性人数}}{\\text{人群规模}} = \\frac{26{,}000}{50{,}000} = \\frac{26}{50} = \\frac{13}{25} = 0.52\n$$\n现在，我们计算ASD：\n$$\n\\text{ASD}_{\\text{female}} = \\frac{|p_{\\text{female}, s} - P_{\\text{female}, p}|}{\\sqrt{P_{\\text{female}, p}(1 - P_{\\text{female}, p})}} = \\frac{|0.65 - 0.52|}{\\sqrt{0.52(1 - 0.52)}} = \\frac{0.13}{\\sqrt{0.52 \\times 0.48}} = \\frac{0.13}{\\sqrt{0.2496}} \\approx 0.2602\n$$\n\n**4. 协变量：当前吸烟状况（二元）**\n首先，我们计算样本和总体比例。\n吸烟者的样本比例，$p_{\\text{smoker}, s}$，为：\n$$\np_{\\text{smoker}, s} = \\frac{\\text{样本中吸烟者人数}}{\\text{样本量}} = \\frac{60}{600} = \\frac{1}{10} = 0.1\n$$\n吸烟者的总体比例，$P_{\\text{smoker}, p}$，为：\n$$\nP_{\\text{smoker}, p} = \\frac{\\text{人群中吸烟者人数}}{\\text{人群规模}} = \\frac{9{,}000}{50{,}000} = \\frac{9}{50} = 0.18\n$$\n现在，我们计算ASD：\n$$\n\\text{ASD}_{\\text{smoker}} = \\frac{|p_{\\text{smoker}, s} - P_{\\text{smoker}, p}|}{\\sqrt{P_{\\text{smoker}, p}(1 - P_{\\text{smoker}, p})}} = \\frac{|0.1 - 0.18|}{\\sqrt{0.18(1 - 0.18)}} = \\frac{|-0.08|}{\\sqrt{0.18 \\times 0.82}} = \\frac{0.08}{\\sqrt{0.1476}} \\approx 0.2082\n$$\n\n最后，我们将每个ASD与代表性差的阈值（$> 0.2$）进行比较。\n\n- **年龄**：$\\text{ASD}_{\\text{age}} = 0.5$。因为 $0.5 > 0.2$，此协变量被标记。\n- **BMI**：$\\text{ASD}_{\\text{BMI}} \\approx 0.1346$。因为 $0.1346 \\le 0.2$，此协变量未被标记。\n- **性别（女性）**：$\\text{ASD}_{\\text{female}} \\approx 0.2602$。因为 $0.2602 > 0.2$，此协变量被标记。\n- **吸烟**：$\\text{ASD}_{\\text{smoker}} \\approx 0.2082$。因为 $0.2082 > 0.2$，此协变量被标记。\n\n被标记为代表性差的协变量是年龄、性别（女性）和当前吸烟状况。被标记的协变量总数为$3$。",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "在诊断出不平衡后，一种有效的校正技术是重新加权样本数据。本练习  将指导你实现一个现代化的校准程序，该程序旨在找到最佳权重，以确保加权后样本的特征与已知的人口基准相符，同时通过正则化保持权重的稳定性。你将通过最小化一个结合了矩匹配误差和惩罚项的复杂损失函数，来探索偏差与方差之间的权衡。",
            "id": "4932764",
            "problem": "在生物统计学中，考虑一个方便样本，其中观察了 $n$ 个个体的标量协变量 $X \\in \\mathbb{R}$，但其入样概率未知。令 $S \\in \\{0,1\\}$ 表示选择指示符，当个体被包含在方便样本中时为 $1$，否则为 $0$。在非概率抽样中，选择概率（也称为倾向性）是未知的，并且可能依赖于 $X$。假设目标（外部）总体为 $X$ 的前两个原始矩提供了可靠的基准，即 $m_1 = \\mathbb{E}[X]$ 和 $m_2 = \\mathbb{E}[X^2]$。目标是校准一个参数化倾向性模型，使得方便样本的逆概率加权矩与这些总体矩对齐。\n\n采用逻辑斯谛倾向性模型 $p(x;\\boldsymbol{\\theta}) = \\operatorname{logit}^{-1}(\\theta_0 + \\theta_1 x)$，其参数向量为 $\\boldsymbol{\\theta} = (\\theta_0,\\theta_1)^\\top$，其中 $\\operatorname{logit}^{-1}(z) = \\frac{1}{1 + e^{-z}}$。为每个样本单位 $i=1,\\dots,n$ 定义逆倾向性权重 $w_i(\\boldsymbol{\\theta}) = \\frac{1}{p(x_i;\\boldsymbol{\\theta})}$。给定权重 $w_i(\\boldsymbol{\\theta})$，定义 $k \\in \\{1,2\\}$ 阶的加权原始矩为\n$$\n\\widehat{M}_k(\\boldsymbol{\\theta}) = \\frac{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta}) \\, x_i^k}{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})}.\n$$\n为了抑制不稳定的权重和极端的倾向性，引入一个基于权重的平方变异系数的惩罚项，\n$$\n\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) = \\frac{\\operatorname{Var}\\big(w_1(\\boldsymbol{\\theta}),\\dots,w_n(\\boldsymbol{\\theta})\\big)}{\\left(\\frac{1}{n}\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})\\right)^2},\n$$\n以及一个倾向性的障碍项，\n$$\nB(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left[-\\log\\big(p(x_i;\\boldsymbol{\\theta})\\big) - \\log\\big(1 - p(x_i;\\boldsymbol{\\theta})\\big)\\right].\n$$\n通过最小化以下损失来校准 $\\boldsymbol{\\theta}$，\n$$\nL(\\boldsymbol{\\theta}) = \\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2 + \\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) + \\beta \\, B(\\boldsymbol{\\theta}),\n$$\n其中 $\\alpha \\ge 0$ 和 $\\beta \\ge 0$ 是用户选择的惩罚权重。\n\n从逆概率加权（Inverse Probability Weighting (IPW)）的定义和逻辑斯谛倾向性模型出发，对于给定的方便样本 $x_1,\\dots,x_n$、总体基准 $m_1,m_2$ 和惩罚权重 $\\alpha,\\beta$，推导一个计算 $\\widehat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta})$ 的算法。对于每个测试用例，该算法必须生成校准后的参数和诊断值。\n\n实现一个程序，对以下每个测试用例执行校准并返回校准后的参数和诊断值：\n- 案例 A（一般情况）：$X = [ -0.21, 0.05, 0.37, -0.62, 0.12, -0.33, -0.08, 0.41, 0.79, -0.45, 1.12, 0.58, -0.14, 0.27, -0.71, 0.93, 0.36, -0.50, 0.10, 0.02 ]$, $m_1 = 0.50$, $m_2 = 1.25$, $\\alpha = 0.10$, $\\beta = 0.01$。\n- 案例 B（接近对齐边界）：$X = [ -0.20, 0.30, 0.90, 0.60, 1.40, -0.10, 0.70, 0.20, 0.50, -0.30, 1.10, 0.40, 0.00, 1.60, -0.50, 0.80, -0.10, 0.30, 0.90, 0.40 ]$, $m_1 = 0.50$, $m_2 = 1.25$, $\\alpha = 0.01$, $\\beta = 0.01$。\n- 案例 C（极端协变量边缘）：$X = [ 5.00, -4.00, 3.50, -3.20, 2.80, -2.50, 2.30, -2.10, 1.90, -1.70, 1.50, -1.30, 1.10, -0.90, 0.70, -0.50, 0.30, -0.20, 0.10, -0.10 ]$, $m_1 = 0.00$, $m_2 = 2.00$, $\\alpha = 1.00$, $\\beta = 0.05$。\n\n在所有案例中，使用相同的逻辑斯谛链接函数，从 $\\boldsymbol{\\theta}^{(0)} = (0.00, 0.00)^\\top$ 开始初始化，并将参数约束在界限 $\\theta_0 \\in [-10.00, 10.00]$，$\\theta_1 \\in [-10.00, 10.00]$ 内。如果为了数值稳定性需要，在计算 $B(\\boldsymbol{\\theta})$ 和权重 $w_i(\\boldsymbol{\\theta})$ 时，使用 $\\epsilon = 10^{-6}$ 将倾向性裁剪到开区间 $(\\epsilon, 1 - \\epsilon)$ 内。\n\n对于每个案例，您的程序应计算校准后的参数向量 $\\widehat{\\boldsymbol{\\theta}} = (\\widehat{\\theta}_0,\\widehat{\\theta}_1)$、优化后的加权原始矩 $\\widehat{M}_1(\\widehat{\\boldsymbol{\\theta}})$ 和 $\\widehat{M}_2(\\widehat{\\boldsymbol{\\theta}})$，以及最小化的损失 $L(\\widehat{\\boldsymbol{\\theta}})$。所有三个测试用例的最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，由按顺序连接每个案例的五个浮点数形成：\n$[ \\widehat{\\theta}_{0,A}, \\widehat{\\theta}_{1,A}, \\widehat{M}_{1,A}, \\widehat{M}_{2,A}, L_A, \\widehat{\\theta}_{0,B}, \\widehat{\\theta}_{1,B}, \\widehat{M}_{1,B}, \\widehat{M}_{2,B}, L_B, \\widehat{\\theta}_{0,C}, \\widehat{\\theta}_{1,C}, \\widehat{M}_{1,C}, \\widehat{M}_{2,C}, L_C ]$。",
            "solution": "### 问题验证\n\n#### 步骤 1：提取已知信息\n\n问题提供了以下数据、定义和约束：\n\n*   **模型**：一个逻辑斯谛倾向性模型 $p(x;\\boldsymbol{\\theta}) = \\operatorname{logit}^{-1}(\\theta_0 + \\theta_1 x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x)}}$，其参数向量为 $\\boldsymbol{\\theta} = (\\theta_0,\\theta_1)^\\top$。\n*   **逆倾向性权重**：对于方便样本 $x_1, \\dots, x_n$，权重为 $w_i(\\boldsymbol{\\theta}) = \\frac{1}{p(x_i;\\boldsymbol{\\theta})}$。\n*   **加权原始矩**：对于 $k \\in \\{1,2\\}$，$\\widehat{M}_k(\\boldsymbol{\\theta}) = \\frac{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta}) \\, x_i^k}{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})}$。\n*   **总体基准**：目标矩 $m_1 = \\mathbb{E}[X]$ 和 $m_2 = \\mathbb{E}[X^2]$。\n*   **惩罚项**：\n    *   权重的平方变异系数：$\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) = \\frac{\\operatorname{Var}\\big(w_1(\\boldsymbol{\\theta}),\\dots,w_n(\\boldsymbol{\\theta})\\big)}{\\left(\\frac{1}{n}\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})\\right)^2}$。\n    *   倾向性障碍项：$B(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left[-\\log\\big(p(x_i;\\boldsymbol{\\theta})\\big) - \\log\\big(1 - p(x_i;\\boldsymbol{\\theta})\\big)\\right]$。\n*   **损失函数**：$L(\\boldsymbol{\\theta}) = \\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2 + \\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) + \\beta \\, B(\\boldsymbol{\\theta})$，其中 $\\alpha \\ge 0$ 和 $\\beta \\ge 0$ 是惩罚权重。\n*   **任务**：找到 $\\widehat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta})$。\n*   **数值参数**：\n    *   初始猜测值：$\\boldsymbol{\\theta}^{(0)} = (0.00, 0.00)^\\top$。\n    *   参数界限：$\\theta_0 \\in [-10.00, 10.00]$，$\\theta_1 \\in [-10.00, 10.00]$。\n    *   倾向性裁剪阈值：$\\epsilon = 10^{-6}$。\n*   **测试用例**：\n    *   案例 A：$X = [ -0.21, 0.05, 0.37, -0.62, 0.12, -0.33, -0.08, 0.41, 0.79, -0.45, 1.12, 0.58, -0.14, 0.27, -0.71, 0.93, 0.36, -0.50, 0.10, 0.02 ]$, $m_1 = 0.50$, $m_2 = 1.25$, $\\alpha = 0.10$, $\\beta = 0.01$。\n    *   案例 B：$X = [ -0.20, 0.30, 0.90, 0.60, 1.40, -0.10, 0.70, 0.20, 0.50, -0.30, 1.10, 0.40, 0.00, 1.60, -0.50, 0.80, -0.10, 0.30, 0.90, 0.40 ]$, $m_1 = 0.50$, $m_2 = 1.25$, $\\alpha = 0.01$, $\\beta = 0.01$。\n    *   案例 C：$X = [ 5.00, -4.00, 3.50, -3.20, 2.80, -2.50, 2.30, -2.10, 1.90, -1.70, 1.50, -1.30, 1.10, -0.90, 0.70, -0.50, 0.30, -0.20, 0.10, -0.10 ]$, $m_1 = 0.00$, $m_2 = 2.00$, $\\alpha = 1.00$, $\\beta = 0.05$。\n\n#### 步骤 2：使用提取的已知信息进行验证\n\n根据验证标准对问题进行评估：\n*   **科学基础**：该问题在非概率抽样和调查数据校准的统计理论中有坚实的基础。逆概率加权、使用逻辑斯谛回归进行倾向性建模以及通过惩罚函数进行正则化都是生物统计学及相关领域中标准且成熟的技术。\n*   **良态问题**：该问题是良态的。它指定了一个明确的目标函数 $L(\\boldsymbol{\\theta})$，该函数需要关于参数向量 $\\boldsymbol{\\theta}$ 进行最小化。所有必要的数据、模型、初始条件和约束（参数界限）都已提供，以定义一个可解的数值优化问题。损失函数是连续的，其组成部分旨在确保一个行为良好的优化景观，这表明存在一个稳定且有意义的解。\n*   **客观性**：问题以精确、客观的数学语言陈述。所有术语都有正式定义。测试用例由无歧义的数值数据组成。\n\n问题陈述没有显示出科学上不健全、不完整、矛盾或模棱两可等缺陷。\n\n#### 步骤 3：结论与行动\n\n问题是**有效的**。将提供解决方案。\n\n### 求解推导\n\n问题的核心是找到最小化指定损失函数 $L(\\boldsymbol{\\theta})$ 的参数向量 $\\widehat{\\boldsymbol{\\theta}} = (\\widehat{\\theta}_0, \\widehat{\\theta}_1)^\\top$。这是一个数值优化问题。算法包括定义损失函数并使用数值求解器找到最小值。\n\n**1. 目标函数构建**\n\n要最小化的目标函数是：\n$$\nL(\\boldsymbol{\\theta}) = \\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2 + \\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) + \\beta \\, B(\\boldsymbol{\\theta})\n$$\n该函数有三个关键组成部分：\n*   **矩匹配项**：$\\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2$。该项惩罚逆倾向性加权样本矩 $\\widehat{M}_1(\\boldsymbol{\\theta})$ 和 $\\widehat{M}_2(\\boldsymbol{\\theta})$ 与已知总体矩 $m_1$ 和 $m_2$ 的偏差。最小化该项驱动了校准过程。\n*   **权重变异惩罚**：$\\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big)$。这是一个由超参数 $\\alpha$ 控制的正则化项。它惩罚权重 $w_i(\\boldsymbol{\\theta})$ 的高变异性，通过其平方变异系数来衡量。这可以避免少数个体权重极大，从而导致不稳定和高方差估计的解。\n*   **倾向性障碍惩罚**：$\\beta \\, B(\\boldsymbol{\\theta})$。该项由 $\\beta$ 控制，惩罚过于接近 $0$ 或 $1$ 的倾向性 $p(x_i; \\boldsymbol{\\theta})$。当 $p \\to 0$ 或 $p \\to 1$ 时，函数 $-\\log(p) - \\log(1-p)$ 趋于无穷大，起到障碍作用。这增强了数值稳定性，并使模型保持在合理范围内。\n\n**2. 组件定义**\n\n$L(\\boldsymbol{\\theta})$ 的组件是建立在逻辑斯谛倾向性模型之上的。对于每个观测值 $x_i$ 和参数向量 $\\boldsymbol{\\theta} = (\\theta_0, \\theta_1)^\\top$：\n*   线性预测器为 $\\eta_i = \\theta_0 + \\theta_1 x_i$。\n*   倾向性为 $p_i(\\boldsymbol{\\theta}) = \\frac{1}{1 + e^{-\\eta_i}}$。为保证数值稳定性，$p_i$ 被裁剪到区间 $[\\epsilon, 1-\\epsilon]$ 内，其中 $\\epsilon=10^{-6}$。\n*   权重为 $w_i(\\boldsymbol{\\theta}) = 1 / p_i(\\boldsymbol{\\theta})$。\n*   加权矩为 $\\widehat{M}_1(\\boldsymbol{\\theta}) = \\frac{\\sum_i w_i x_i}{\\sum_i w_i}$ 和 $\\widehat{M}_2(\\boldsymbol{\\theta}) = \\frac{\\sum_i w_i x_i^2}{\\sum_i w_i}$。\n*   平方 CV 的计算方式为 $\\frac{\\text{np.var}(w)}{\\text{np.mean}(w)^2}$，作用于权重集合 $\\{w_i\\}_{i=1}^n$。\n*   障碍项为 $B(\\boldsymbol{\\theta}) = \\sum_i [-\\log(p_i) - \\log(1-p_i)]$。\n\n**3. 优化算法**\n\n损失函数 $L(\\boldsymbol{\\theta})$ 是 $\\boldsymbol{\\theta}$ 的一个非线性函数。$\\arg\\min_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta})$ 的解析解是不可行的。因此，需要一种数值优化算法。\n\n该问题是一个约束优化问题，因为参数有界：$\\theta_0, \\theta_1 \\in [-10.00, 10.00]$。一个合适的算法是能处理箱式约束的拟牛顿法，例如 **L-BFGS-B（带界限的有限内存 Broyden–Fletcher–Goldfarb–Shanno）** 算法。该算法使用梯度和 Hessian 矩阵的近似值来迭代逼近函数的最小值。\n\n计算过程如下：\n1.  实现一个函数，用于计算给定向量 $\\boldsymbol{\\theta}$ 和输入数据（$X$, $m_1$, $m_2$, $\\alpha$, $\\beta$）的损失 $L(\\boldsymbol{\\theta})$。该函数将包括计算倾向性、权重和损失所有组件的逻辑。\n2.  使用数值优化例程，例如 SciPy 库中的 `scipy.optimize.minimize`，并配置为使用 `L-BFGS-B` 方法。\n3.  向优化器提供损失函数、初始参数猜测值 $\\boldsymbol{\\theta}^{(0)} = (0, 0)^\\top$、参数界限 $[(-10, 10), (-10, 10)]$ 以及其他所需的参数数据。\n4.  优化器将返回估计的最优参数向量 $\\widehat{\\boldsymbol{\\theta}}$。\n5.  使用 $\\widehat{\\boldsymbol{\\theta}}$，计算最终的诊断值：优化后的加权矩 $\\widehat{M}_1(\\widehat{\\boldsymbol{\\theta}})$、$\\widehat{M}_2(\\widehat{\\boldsymbol{\\theta}})$ 和最小化的损失 $L(\\widehat{\\boldsymbol{\\theta}})$。\n\n此过程应用于所提供的三个测试用例中的每一个。以下程序实现了这个推导出的算法。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the propensity score calibration problem for three test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"X\": np.array([ -0.21, 0.05, 0.37, -0.62, 0.12, -0.33, -0.08, 0.41, 0.79, -0.45, 1.12, 0.58, -0.14, 0.27, -0.71, 0.93, 0.36, -0.50, 0.10, 0.02 ]),\n            \"m1\": 0.50, \"m2\": 1.25, \"alpha\": 0.10, \"beta\": 0.01\n        },\n        {\n            \"name\": \"Case B\",\n            \"X\": np.array([ -0.20, 0.30, 0.90, 0.60, 1.40, -0.10, 0.70, 0.20, 0.50, -0.30, 1.10, 0.40, 0.00, 1.60, -0.50, 0.80, -0.10, 0.30, 0.90, 0.40 ]),\n            \"m1\": 0.50, \"m2\": 1.25, \"alpha\": 0.01, \"beta\": 0.01\n        },\n        {\n            \"name\": \"Case C\",\n            \"X\": np.array([ 5.00, -4.00, 3.50, -3.20, 2.80, -2.50, 2.30, -2.10, 1.90, -1.70, 1.50, -1.30, 1.10, -0.90, 0.70, -0.50, 0.30, -0.20, 0.10, -0.10 ]),\n            \"m1\": 0.00, \"m2\": 2.00, \"alpha\": 1.00, \"beta\": 0.05\n        }\n    ]\n\n    # Universal parameters\n    epsilon = 1e-6\n    theta_initial = np.array([0.0, 0.0])\n    bounds = ((-10.0, 10.0), (-10.0, 10.0))\n\n    def calculate_diagnostics(theta, X):\n        \"\"\"Calculates propensities, weights, and weighted moments.\"\"\"\n        theta0, theta1 = theta\n        linear_term = theta0 + theta1 * X\n        \n        # Sigmoid function for propensities\n        propensities = 1 / (1 + np.exp(-linear_term))\n        \n        # Clip for numerical stability\n        propensities = np.clip(propensities, epsilon, 1 - epsilon)\n        \n        weights = 1 / propensities\n        \n        sum_w = np.sum(weights)\n        \n        # Handle potential division by zero, although unlikely\n        if sum_w == 0:\n            M1_hat = np.nan\n            M2_hat = np.nan\n        else:\n            M1_hat = np.sum(weights * X) / sum_w\n            M2_hat = np.sum(weights * X**2) / sum_w\n            \n        return propensities, weights, M1_hat, M2_hat\n\n    def loss_function(theta, X, m1, m2, alpha, beta):\n        \"\"\"Computes the total loss L(theta).\"\"\"\n        propensities, weights, M1_hat, M2_hat = calculate_diagnostics(theta, X)\n\n        # 1. Moment matching loss\n        moment_loss = (M1_hat - m1)**2 + (M2_hat - m2)**2\n\n        # 2. Squared CV penalty\n        mean_w = np.mean(weights)\n        var_w = np.var(weights)  # ddof=0 by default\n        cv_sq_penalty = alpha * var_w / mean_w**2 if mean_w > 1e-9 else 0.0\n\n        # 3. Barrier penalty\n        barrier_penalty = beta * np.sum(-np.log(propensities) - np.log(1 - propensities))\n\n        total_loss = moment_loss + cv_sq_penalty + barrier_penalty\n        return total_loss\n\n    all_results = []\n    for case in test_cases:\n        X, m1, m2, alpha, beta = case['X'], case['m1'], case['m2'], case['alpha'], case['beta']\n        \n        # Perform optimization\n        opt_result = minimize(\n            fun=loss_function,\n            x0=theta_initial,\n            args=(X, m1, m2, alpha, beta),\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n        \n        theta_hat = opt_result.x\n        loss_hat = opt_result.fun\n        \n        # Recalculate final moments with optimal theta\n        _, _, M1_hat, M2_hat = calculate_diagnostics(theta_hat, X)\n        \n        # Append results in the specified order\n        all_results.extend([theta_hat[0], theta_hat[1], M1_hat, M2_hat, loss_hat])\n\n    # Format the final output string\n    # Using a format specifier for consistent output across floating point representations\n    formatted_results = [f\"{val:.8f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "基于模型的方法为偏差校正提供了一种比传统加权更复杂的替代方案，尤其适用于复杂场景或稀疏数据。在本练习  中，你将构建一个多层回归与后分层（MRP）估计器，它将分层结果模型与已知的人口数据相结合，以生成一个稳健且经过偏差校正的总体估计。这种方法通过在不同分层之间“借用”信息，能够有效处理样本量小的分层，从而提供更精确的预测。",
            "id": "4932631",
            "problem": "给定一个从有限总体中抽取的便利样本，该总体按分类协变量（统称为 $X$）进行分层。结果 $Y$ 是二元的。由于该样本是便利样本，$X$ 的样本分布可能不等于 $X$ 的总体分布，导致对 $Y$ 的总体均值的朴素估计存在选择偏差。您的任务是构建一个多层回归与后分层（MRP）估计量，该估计量将分层结果模型与 $X$ 的外部总体边缘分布相结合。\n\n基本依据和建模假设：\n- 根据全期望定律，$Y$ 的总体均值满足 $\\mathbb{E}[Y] = \\sum_{s=1}^{S} \\mathbb{E}[Y \\mid X=s] \\Pr(X=s)$，其中 $s$ 是由 $X$ 定义的层的索引，$S$ 是层的总数。\n- 在非概率（便利）抽样的情况下，$X$ 的样本分布不一定等于总体分布。然而，如果我们能够对 $\\mathbb{E}[Y \\mid X=s]$ 进行建模，并且我们拥有关于 $\\Pr_{\\text{pop}}(X=s)$ 的外部信息，那么我们可以通过将 $\\mathbb{E}[Y \\mid X=s]$ 的基于模型的估计与总体边缘分布 $\\Pr_{\\text{pop}}(X=s)$ 相结合来估计 $\\mathbb{E}[Y]$。\n- 使用一个跨层进行部分汇集的分层结果模型。具体来说，通过逻辑链接对层级均值进行参数化：对于层 $s$，令 $p_s = \\Pr(Y=1 \\mid X=s)$ 并建模 $\\text{logit}(p_s) = \\alpha + b_s$，其中 $\\alpha$ 是全局截距，$b_s$ 是特定于层的随机效应。通过最大化一个惩罚对数似然来施加高斯收缩，这对应于 $b_s \\sim \\mathcal{N}(0, \\sigma^2)$，等价于一个调整参数为 $\\lambda = 1/\\sigma^2$ 的岭惩罚。\n\n带有聚合数据的惩罚似然：\n- 对于每个层 $s$，您观察到一个便利样本大小 $n_s$ 和观察到的成功次数 $y_s$。令 $\\sigma(z) = 1/(1 + e^{-z})$ 表示逻辑函数。参数 $(\\alpha, b_1, \\dots, b_S)$ 的惩罚对数似然为\n$$\n\\ell(\\alpha, b) = \\sum_{s=1}^{S} \\left[ y_s \\log\\left(\\sigma(\\alpha + b_s)\\right) + (n_s - y_s) \\log\\left(1 - \\sigma(\\alpha + b_s)\\right) \\right] - \\frac{\\lambda}{2} \\sum_{s=1}^{S} b_s^2.\n$$\n您必须在完整参数向量上使用 Newton–Raphson 方法找到最大化器 $(\\hat{\\alpha}, \\hat{b})$，然后计算 $\\hat{p}_s = \\sigma(\\hat{\\alpha} + \\hat{b}_s)$。\n\n后分层：\n- 给定外部总体边缘分布 $w_s = \\Pr_{\\text{pop}}(X=s)$ 且 $\\sum_{s=1}^{S} w_s = 1$，计算 $Y$ 的总体均值的 MRP 估计量为\n$$\n\\hat{\\mu}_{\\text{MRP}} = \\sum_{s=1}^{S} w_s \\hat{p}_s.\n$$\n\nNewton–Raphson 的算法规范：\n- 定义参数向量 $\\theta = (\\alpha, b_1, \\dots, b_S)^\\top$。\n- 在当前迭代中，为所有 $s$ 计算 $p_s = \\sigma(\\alpha + b_s)$。\n- 梯度分量是\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{s=1}^{S} (y_s - n_s p_s), \\quad\n\\frac{\\partial \\ell}{\\partial b_s} = (y_s - n_s p_s) - \\lambda b_s.\n$$\n- Hessian 矩阵的元素是\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = -\\sum_{s=1}^{S} n_s p_s (1 - p_s), \\quad\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\, \\partial b_s} = - n_s p_s (1 - p_s), \\quad\n\\frac{\\partial^2 \\ell}{\\partial b_s^2} = - n_s p_s (1 - p_s) - \\lambda,\n$$\n其中，对于 $s \\neq s'$，有 $\\frac{\\partial^2 \\ell}{\\partial b_s \\, \\partial b_{s'}} = 0$。\n- 求解线性系统 $H(\\theta) \\, \\Delta = - \\nabla \\ell(\\theta)$ 并更新 $\\theta \\leftarrow \\theta + \\Delta$ 直至收敛。\n\n测试套件和要求的输出：\n- 每种情况下都有 $S = 6$ 个层。对于每个测试用例，您将获得 $(n_s)$、$(y_s)$、$(w_s)$、一个仅用于评估的真实层概率向量 $(p^{\\star}_s)$ 以及一个惩罚项 $\\lambda$。使用提供的 $(n_s)$ 和 $(y_s)$ 来拟合分层模型，使用 $(w_s)$ 进行后分层，并计算 $\\hat{\\mu}_{\\text{MRP}}$。\n\n- 测试用例 1（理想路径，中度偏差）：\n    - $n = [20, 15, 15, 25, 15, 60]$\n    - $y = [2, 2, 2, 8, 5, 24]$\n    - $w = [0.2, 0.15, 0.25, 0.15, 0.1, 0.15]$\n    - $p^{\\star} = [0.1, 0.12, 0.15, 0.3, 0.35, 0.4]$\n    - $\\lambda = 1.0$\n- 测试用例 2（边界情况，样本中存在缺失层）：\n    - $n = [10, 5, 40, 0, 10, 5]$\n    - $y = [2, 1, 20, 0, 1, 1]$\n    - $w = [0.1, 0.25, 0.1, 0.2, 0.2, 0.15]$\n    - $p^{\\star} = [0.18, 0.22, 0.5, 0.05, 0.1, 0.12]$\n    - $\\lambda = 2.5$\n- 测试用例 3（严重不平衡，对低风险层进行过采样）：\n    - $n = [200, 150, 10, 5, 2, 1]$\n    - $y = [10, 12, 1, 1, 1, 1]$\n    - $w = [0.15, 0.15, 0.2, 0.2, 0.15, 0.15]$\n    - $p^{\\star} = [0.05, 0.08, 0.12, 0.25, 0.5, 0.55]$\n    - $\\lambda = 0.5$\n\n为每个测试用例计算 MRP 估计量 $\\hat{\\mu}_{\\text{MRP}}$，并将其作为最终程序输出。所有输出都是无单位的浮点数。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，$[r_1, r_2, r_3]$），其中 $r_i$ 是按给定顺序排列的测试用例 $i$ 的 $\\hat{\\mu}_{\\text{MRP}}$。不应打印其他任何文本。",
            "solution": "该问题要求构建并应用多层回归与后分层（MRP）估计量，以校正便利样本中的选择偏差。解决方案涉及使用 Newton-Raphson 方法最大化惩罚对数似然，然后重新加权基于模型的预测，以匹配已知的人口统计数据。\n\n首先，我们将问题形式化。我们拥有来自一个便利样本的数据，该样本被分层为 $S$ 个不同的层。对于每个层 $s \\in \\{1, \\dots, S\\}$，我们有一个二元结果 $Y$ 的样本量 $n_s$ 和“成功”次数 $y_s$。我们还被提供了每个层的人口水平比例 $w_s$。目标是估计总体均值 $\\mu = \\mathbb{E}[Y]$。\n\nMRP方法的核心在于全期望定律：\n$$\n\\mu = \\mathbb{E}[Y] = \\sum_{s=1}^{S} \\mathbb{E}[Y \\mid X=s] \\Pr_{\\text{pop}}(X=s) = \\sum_{s=1}^{S} p_s w_s\n$$\n其中 $p_s = \\Pr(Y=1 \\mid X=s)$ 是层 $s$ 中的真实成功概率，$w_s$ 是该层的真实总体比例。虽然样本分布可能存在偏差，但如果我们能获得 $p_s$ 的可靠估计并知道真实的 $w_s$，我们就可以估计 $\\mu$。\n\n我们使用分层逻辑回归模型对特定于层的概率 $p_s$ 进行建模。这允许跨层部分汇集信息，对于样本量小的层尤其有用。该模型指定如下：\n$$\n\\text{logit}(p_s) = \\log\\left(\\frac{p_s}{1-p_s}\\right) = \\alpha + b_s\n$$\n在这里，$\\alpha$ 是一个全局截距，表示所有层的平均成功对数几率，$b_s$ 是一个特定于层的随机效应，捕捉层 $s$ 与此平均值的偏差。该模型假设这些随机效应来自一个共同的分布，$b_s \\sim \\mathcal{N}(0, \\sigma^2)$。这一假设引入了收缩效应，将 $b_s$ 的估计值拉向 $0$，这有助于正则化模型并防止过拟合，尤其是在数据稀疏的层中。\n\n参数 $(\\alpha, b_1, \\dots, b_S)$ 的估计是通过最大化惩罚对数似然来进行的。所有层中观测数据 $(y_s, n_s)$ 的似然由二项式似然的乘积给出。惩罚项源于随机效应的先验，$b_s \\sim \\mathcal{N}(0, \\sigma^2)$，这等同于对系数 $b_s$ 的 $L_2$（岭）惩罚。令 $\\lambda = 1/\\sigma^2$，需要最大化的惩罚对数似然函数为：\n$$\n\\ell(\\alpha, b) = \\sum_{s=1}^{S} \\left[ y_s \\log\\left(\\sigma(\\alpha + b_s)\\right) + (n_s - y_s) \\log\\left(1 - \\sigma(\\alpha + b_s)\\right) \\right] - \\frac{\\lambda}{2} \\sum_{s=1}^{S} b_s^2\n$$\n其中 $\\sigma(z) = 1/(1 + e^{-z})$ 是逻辑函数。$n_s=0$ 的层对数据似然的总和没有贡献，但仍然受到其相应 $b_s$ 上的惩罚的影响，这有效地将 $\\hat{b}_s$ 收缩到 $0$。\n\n为了找到最大化此函数的参数 $(\\hat{\\alpha}, \\hat{b})$，我们采用 Newton-Raphson 算法，这是一种迭代的二阶优化方法。设完整参数向量为 $\\theta = (\\alpha, b_1, \\dots, b_S)^\\top$。迭代更新规则是：\n$$\n\\theta_{k+1} = \\theta_k - [H(\\theta_k)]^{-1} \\nabla \\ell(\\theta_k)\n$$\n其中 $\\nabla \\ell(\\theta_k)$ 是梯度向量，$H(\\theta_k)$ 是惩罚对数似然的 Hessian 矩阵，两者都在当前参数估计 $\\theta_k$ 处求值。\n\n梯度向量 $\\nabla \\ell(\\theta)$ 的分量为：\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{s=1}^{S} (y_s - n_s p_s)\n$$\n$$\n\\frac{\\partial \\ell}{\\partial b_s} = (y_s - n_s p_s) - \\lambda b_s \\quad \\text{for } s = 1, \\dots, S\n$$\n其中 $p_s = \\sigma(\\alpha + b_s)$。\n\nHessian 矩阵 $H(\\theta)$ 是一个对称的 $(S+1) \\times (S+1)$ 矩阵。其元素由 $\\ell$ 的二阶偏导数给出。令 $W_s = n_s p_s(1-p_s)$。这些元素是：\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = -\\sum_{s=1}^{S} W_s\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\, \\partial b_s} = -W_s\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial b_s^2} = -W_s - \\lambda\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial b_s \\, \\partial b_{s'}} = 0 \\quad \\text{for } s \\neq s'\n$$\nHessian 矩阵具有带边对角结构。Newton-Raphson 更新步骤 $\\Delta_k = -[H(\\theta_k)]^{-1} \\nabla \\ell(\\theta_k)$ 是通过求解线性系统 $H(\\theta_k) \\Delta_k = -\\nabla \\ell(\\theta_k)$ 来计算的。\n\n该算法按以下步骤进行：\n1. 初始化参数，例如 $\\theta_0 = \\mathbf{0}$。\n2. 从 $k=0, 1, 2, \\dots$ 开始迭代，直到收敛：\n   a. 对于当前的 $\\theta_k = (\\alpha_k, b_{1,k}, \\dots, b_{S,k})^\\top$，为所有层计算 $p_s = \\sigma(\\alpha_k + b_{s,k})$。\n   b. 计算梯度向量 $\\nabla \\ell(\\theta_k)$ 和 Hessian 矩阵 $H(\\theta_k)$。\n   c. 求解线性系统 $H(\\theta_k) \\Delta_k = -\\nabla \\ell(\\theta_k)$ 以获得更新步长 $\\Delta_k$。\n   d. 更新参数：$\\theta_{k+1} = \\theta_k + \\Delta_k$。\n3. 当更新向量的范数 $\\|\\Delta_k\\|$ 小于一个很小的容差时，即达到收敛。\n\n一旦算法收敛到最优参数 $(\\hat{\\alpha}, \\hat{b})$，我们计算估计的层概率：\n$$\n\\hat{p}_s = \\sigma(\\hat{\\alpha} + \\hat{b}_s) \\quad \\text{for } s = 1, \\dots, S\n$$\n对于 $n_s=0$ 的层 $s$，估计值 $\\hat{b}_s$ 被惩罚项驱动至 $0$，因此其预测概率将为 $\\hat{p}_s \\approx \\sigma(\\hat{\\alpha})$，这是从所有其他层中借用信息的结果。\n\n最后，后分层步骤将这些基于模型的概率估计与已知的总体权重 $w_s$ 相结合，以计算总体均值的 MRP 估计量：\n$$\n\\hat{\\mu}_{\\text{MRP}} = \\sum_{s=1}^{S} w_s \\hat{p}_s\n$$\n这个最终值是 $Y$ 的总体均值的去偏估计。",
            "answer": "```python\nimport numpy as np\n\ndef _compute_mrp_estimate(n, y, w, lambda_val, tol=1e-8, max_iter=25):\n    \"\"\"\n    Computes the MRP estimate by maximizing a penalized logistic regression model.\n\n    Args:\n        n (list): List of sample sizes per stratum.\n        y (list): List of success counts per stratum.\n        w (list): List of population proportions per stratum.\n        lambda_val (float): The ridge penalty parameter.\n        tol (float): Convergence tolerance for the Newton-Raphson algorithm.\n        max_iter (int): Maximum number of iterations for Newton-Raphson.\n\n    Returns:\n        float: The Multilevel Regression and Post-stratification (MRP) estimate.\n    \"\"\"\n    n_arr = np.array(n, dtype=float)\n    y_arr = np.array(y, dtype=float)\n    w_arr = np.array(w, dtype=float)\n    S = len(n_arr)\n\n    # Initialize parameters theta = (alpha, b_1, ..., b_S)\n    theta = np.zeros(S + 1)\n\n    for i in range(max_iter):\n        alpha = theta[0]\n        b = theta[1:]\n\n        # Linear predictor eta = alpha + b_s for each stratum s\n        eta = alpha + b\n        \n        # Probabilities p_s = sigma(eta_s)\n        # Clip eta to prevent overflow in exp\n        eta = np.clip(eta, -500, 500)\n        p = 1.0 / (1.0 + np.exp(-eta))\n\n        # Gradient of the penalized log-likelihood\n        grad = np.zeros(S + 1)\n        # Residuals for the likelihood part\n        residual = y_arr - n_arr * p\n        # d(ell)/d(alpha)\n        grad[0] = np.sum(residual)\n        # d(ell)/d(b_s)\n        grad[1:] = residual - lambda_val * b\n\n        # Hessian matrix of the penalized log-likelihood\n        H = np.zeros((S + 1, S + 1))\n        # Weights for the Hessian W_s = n_s * p_s * (1 - p_s)\n        W = n_arr * p * (1.0 - p)\n        \n        # Fill the Hessian matrix based on its structure\n        # H_00 = d^2(ell)/d(alpha^2)\n        H[0, 0] = -np.sum(W)\n        # H_0s = H_s0 = d^2(ell)/d(alpha)d(b_s)\n        H[0, 1:] = -W\n        H[1:, 0] = -W\n        \n        # Diagonal elements for the b_s block: H_ss = d^2(ell)/d(b_s^2)\n        # This uses array broadcasting in a slightly non-obvious way. A direct\n        # assignment to the diagonal of a submatrix is safer.\n        H_sub = H[1:, 1:]\n        np.fill_diagonal(H_sub, -W - lambda_val)\n\n        # Newton-Raphson step: solve H * delta = -grad\n        try:\n            delta = np.linalg.solve(H, -grad)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if Hessian is singular, though unlikely with lambda > 0\n            # For this problem, we assume H is always invertible.\n            # Returning NaN on failure.\n            return np.nan\n        \n        # Update parameters\n        theta += delta\n\n        # Check for convergence\n        if np.linalg.norm(delta)  tol:\n            break\n\n    # After convergence, compute final estimates\n    final_alpha = theta[0]\n    final_b = theta[1:]\n\n    # Final stratum probabilities\n    final_eta = final_alpha + final_b\n    final_eta = np.clip(final_eta, -500, 500)\n    final_p = 1.0 / (1.0 + np.exp(-final_eta))\n\n    # Post-stratification to get the MRP estimate of the population mean\n    mu_mrp = np.sum(w_arr * final_p)\n    return mu_mrp\n\ndef solve():\n    \"\"\"\n    Main function to run the MRP estimation for the given test cases.\n    \"\"\"\n    # Test cases from the problem statement\n    test_cases = [\n        {\n            \"n\": [20, 15, 15, 25, 15, 60],\n            \"y\": [2, 2, 2, 8, 5, 24],\n            \"w\": [0.2, 0.15, 0.25, 0.15, 0.1, 0.15],\n            \"lambda_val\": 1.0\n        },\n        {\n            \"n\": [10, 5, 40, 0, 10, 5],\n            \"y\": [2, 1, 20, 0, 1, 1],\n            \"w\": [0.1, 0.25, 0.1, 0.2, 0.2, 0.15],\n            \"lambda_val\": 2.5\n        },\n        {\n            \"n\": [200, 150, 10, 5, 2, 1],\n            \"y\": [10, 12, 1, 1, 1, 1],\n            \"w\": [0.15, 0.15, 0.2, 0.2, 0.15, 0.15],\n            \"lambda_val\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _compute_mrp_estimate(\n            n=case[\"n\"],\n            y=case[\"y\"],\n            w=case[\"w\"],\n            lambda_val=case[\"lambda_val\"]\n        )\n        results.append(result)\n\n    # Format the output as a comma-separated list in brackets.\n    # We use a reasonable precision for floating point results.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}