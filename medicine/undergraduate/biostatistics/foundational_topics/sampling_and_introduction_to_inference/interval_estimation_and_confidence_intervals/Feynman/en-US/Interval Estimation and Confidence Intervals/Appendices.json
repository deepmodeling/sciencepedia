{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in designing a rigorous quantitative study is determining how many subjects are needed to achieve a desired level of precision. This exercise provides fundamental practice in this area by asking you to derive the relationship between sample size $n$, population variance $\\sigma^2$, and the width of a confidence interval. By applying this formula to a practical clinical research scenario, you will develop a core intuition for the trade-offs involved in study planning. ",
            "id": "4560495",
            "problem": "A clinical biomarker of systemic inflammation is quantified in milligrams per liter (mg/L). Investigators plan a study to estimate the population mean biomarker level, denoted by $\\mu$, using a two-sided confidence interval (CI) at confidence level $1 - \\alpha = 0.95$. Based on a prior meta-analysis across independent cohorts, the within-subject measurement process and biological variability for this biomarker are well-modeled by independent and identically distributed (i.i.d.) Gaussian observations $X_{1}, X_{2}, \\dots, X_{n}$ with mean $\\mu$ and known variance $\\sigma^{2}$. The meta-analysis provides an externally validated variance estimate $\\sigma^{2} = 64$ (mg/L)$^{2}$ that can be treated as known for planning purposes.\n\nStarting from the core definitions of a two-sided confidence interval and the sampling distribution of the sample mean under the Gaussian model with known variance, derive a relationship connecting the CI width $W$, the sample size $n$, the variance $\\sigma^{2}$, and the appropriate quantile of the standard normal distribution. Then, using this relationship, determine the smallest integer $n$ that guarantees a planned two-sided CI for $\\mu$ has total width $W = 3$ mg/L at confidence level $0.95$.\n\nReport the final sample size $n$ as a single integer. If non-integer calculations occur, choose the smallest integer $n$ that satisfies the width constraint $W \\leq 3$ mg/L. Do not include any units in your final numeric answer.",
            "solution": "The problem asks for the determination of the minimum sample size, $n$, required to achieve a specified confidence interval width for a population mean, $\\mu$, given a known population variance, $\\sigma^2$.\n\nThe estimator for the population mean $\\mu$ is the sample mean, $\\bar{X}$, defined as:\n$$\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n$$\nGiven that $X_i$ are i.i.d. random variables drawn from a normal distribution $N(\\mu, \\sigma^2)$, the sampling distribution of the sample mean $\\bar{X}$ is also normal. The mean of $\\bar{X}$ is $E[\\bar{X}] = \\mu$, and its variance is $\\text{Var}(\\bar{X}) = \\frac{\\sigma^2}{n}$. Thus, the sampling distribution is:\n$$\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\nTo construct a confidence interval, we use a pivotal quantity whose distribution does not depend on the unknown parameter $\\mu$. We standardize $\\bar{X}$ to obtain a standard normal random variable, $Z$:\n$$\nZ = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim N(0, 1)\n$$\nA two-sided confidence interval with confidence level $1 - \\alpha$ is constructed from the probability statement:\n$$\nP(-z_{\\alpha/2} \\le Z \\le z_{\\alpha/2}) = 1 - \\alpha\n$$\nwhere $z_{\\alpha/2}$ is the upper $\\alpha/2$ quantile of the standard normal distribution, defined by $P(Z > z_{\\alpha/2}) = \\alpha/2$.\n\nSubstituting the expression for $Z$ gives:\n$$\nP\\left(-z_{\\alpha/2} \\le \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\le z_{\\alpha/2}\\right) = 1 - \\alpha\n$$\nWe rearrange the inequality to isolate the parameter $\\mu$:\n$$\n-z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\le \\bar{X} - \\mu \\le z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n$$\n$$\n-\\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\le -\\mu \\le -\\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n$$\n$$\n\\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\le \\mu \\le \\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n$$\nThis defines the $100(1-\\alpha)\\%$ confidence interval for $\\mu$:\n$$\n\\text{CI} = \\left[ \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right]\n$$\nThe total width, $W$, of this confidence interval is the difference between the upper bound and the lower bound:\n$$\nW = \\left( \\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right) - \\left( \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right) = 2 z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n$$\nThis equation provides the required relationship connecting the width $W$, the sample size $n$, the standard deviation $\\sigma$, and the standard normal quantile $z_{\\alpha/2}$.\n\nThe problem requires that the total width be $W = 3$ mg/L. To be more precise, the planned width should be at most $3$ mg/L. Thus, we have the inequality:\n$$\nW \\le 3 \\implies 2 z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\le 3\n$$\nWe can now solve for $n$:\n$$\n\\sqrt{n} \\ge \\frac{2 z_{\\alpha/2} \\sigma}{3}\n$$\n$$\nn \\ge \\left( \\frac{2 z_{\\alpha/2} \\sigma}{3} \\right)^2\n$$\nNow we substitute the given values.\nThe confidence level is $1 - \\alpha = 0.95$, so $\\alpha = 0.05$ and $\\alpha/2 = 0.025$. The corresponding quantile for the standard normal distribution is $z_{0.025}$. This is the value such that the cumulative probability is $1 - 0.025 = 0.975$. The standard value for $z_{0.025}$ is approximately $1.96$.\nThe known variance is $\\sigma^2 = 64$ (mg/L)$^2$, so the standard deviation is $\\sigma = \\sqrt{64} = 8$ mg/L.\nThe maximum desired width is $W = 3$ mg/L.\n\nSubstituting these values into the inequality for $n$:\n$$\nn \\ge \\left( \\frac{2 \\times 1.96 \\times 8}{3} \\right)^2\n$$\n$$\nn \\ge \\left( \\frac{31.36}{3} \\right)^2\n$$\n$$\nn \\ge (10.4533...)^2\n$$\n$$\nn \\ge 109.2718...\n$$\nSince the sample size $n$ must be an integer, we must take the smallest integer greater than or equal to $109.2718...$. This is necessary to ensure the width constraint $W \\le 3$ is met. Therefore, the minimum required sample size is $n=110$.",
            "answer": "$$\\boxed{110}$$"
        },
        {
            "introduction": "While normal approximations are useful, many outcomes in biostatistics are discrete counts, requiring more precise methods. This practice challenges you to derive the \"exact\" Clopper-Pearson confidence interval for a binomial proportion by applying the fundamental principle of inverting a hypothesis test.  Engaging with this derivation builds a deeper understanding of where confidence intervals come from and introduces the important concept of conservatism, a key property of methods dealing with discrete data.",
            "id": "4805618",
            "problem": "A single-arm early-phase clinical study seeks to estimate the true adverse event probability $p$ for a new therapy. Suppose the data arise as $Y \\sim \\mathrm{Bin}(n,p)$, where $Y$ is the number of patients with at least one adverse event among $n$ independent patients. You are asked to construct a $(1-\\alpha)$ confidence interval for $p$ using the exact inversion of the binomial test, also known as the Clopper–Pearson (CP) method.\n\nTasks:\n1. Starting only from the binomial probability mass function $P(Y=y \\mid p)=\\binom{n}{y} p^{y} (1-p)^{n-y}$, the definition of an exact one-sided test at level $\\alpha/2$, and the principle of inversion of a family of tests, derive the CP interval by finding the lower endpoint $L$ and upper endpoint $U$ as the unique solutions to tail-probability equations in $p$. Express your final endpoints in terms of the inverse of a standard special function, and state clearly how these endpoints are defined for $y \\in \\{0,1,\\dots,n\\}$.\n2. Provide a concise argument explaining why the CP interval is conservative in the sense that its coverage probability is at least $(1-\\alpha)$ for all $p \\in (0,1)$, and strictly greater than $(1-\\alpha)$ for many $p$ due to discreteness.\n3. Apply your general result to the following pilot-safety setting: in $n=2$ patients, $y=1$ experienced the adverse event, and a $(1-\\alpha)=0.90$ interval is desired. Compute the length of the CP interval, defined as $U-L$, and report the numerical value rounded to four significant figures. Provide your final answer as a single real number without units.",
            "solution": "This solution is organized according to the three tasks specified in the problem statement.\n\n#### 1. Derivation of the Clopper–Pearson (CP) Interval\n\nThe principle of constructing a $(1-\\alpha)$ confidence interval by inverting a family of hypothesis tests states that the confidence set for a parameter $p$ consists of all values $p_0$ for which the null hypothesis $H_0: p = p_0$ is *not rejected* at significance level $\\alpha$ given the observed data $y$.\n\nThe Clopper-Pearson method is derived by inverting two separate one-sided exact binomial tests, each at a significance level of $\\alpha/2$.\n\n**Derivation of the Lower Endpoint $L$**\n\nTo find the lower endpoint $L$, we consider for each possible value $p_0 \\in (0,1)$ the one-sided hypothesis test $H_0: p = p_0$ versus $H_1: p > p_0$. This test is rejected if the observed outcome $y$ is surprisingly large, indicating that the true $p$ is likely greater than $p_0$. The p-value for this test is the probability of observing an outcome as extreme or more extreme than $y$, under the null hypothesis:\n$$ \\text{p-value}_U(y, p_0) = P(Y \\ge y \\mid p=p_0) = \\sum_{k=y}^{n} \\binom{n}{k} p_0^k (1-p_0)^{n-k} $$\nThe lower bound $L$ of the confidence interval is the smallest value of $p_0$ that is *not* rejected. This means we are looking for the value $L$ such that if $p_0 < L$, the hypothesis $H_0: p=p_0$ is rejected. Rejection occurs if the p-value is less than or equal to $\\alpha/2$. To find the boundary of the acceptance region, we set the p-value equal to $\\alpha/2$.\nThe lower endpoint $L$ is the solution to the equation:\n$$ P(Y \\ge y \\mid p=L) = \\sum_{k=y}^{n} \\binom{n}{k} L^k (1-L)^{n-k} = \\frac{\\alpha}{2} $$\nThis equation holds for $y \\in \\{1, \\dots, n\\}$. If $y=0$, the sum is $1$, so the equation $1 = \\alpha/2$ has no solution. In this case, the test of $H_0: p = p_0$ is never rejected based on an observation of $y=0$, for any $p_0 \\ge 0$. Thus, by convention and logic of the test, the lower bound is $L=0$ when $y=0$.\n\n**Derivation of the Upper Endpoint $U$**\n\nTo find the upper endpoint $U$, we consider for each $p_0 \\in (0,1)$ the one-sided hypothesis test $H_0: p = p_0$ versus $H_1: p < p_0$. This test is rejected if the observed outcome $y$ is surprisingly small. The p-value is:\n$$ \\text{p-value}_L(y, p_0) = P(Y \\le y \\mid p=p_0) = \\sum_{k=0}^{y} \\binom{n}{k} p_0^k (1-p_0)^{n-k} $$\nThe upper bound $U$ is the largest value of $p_0$ that is *not* rejected. We find this boundary by setting the p-value equal to $\\alpha/2$.\nThe upper endpoint $U$ is the solution to the equation:\n$$ P(Y \\le y \\mid p=U) = \\sum_{k=0}^{y} \\binom{n}{k} U^k (1-U)^{n-k} = \\frac{\\alpha}{2} $$\nThis equation holds for $y \\in \\{0, \\dots, n-1\\}$. If $y=n$, the sum is $1$, so the equation $1 = \\alpha/2$ has no solution. For an observation of $y=n$, we never reject $H_0: p=p_0$ in favor of a smaller $p$. Thus, by convention, the upper bound is $U=1$ when $y=n$.\n\n**Expression in Terms of a Special Function**\n\nThe binomial sum is related to the regularized incomplete beta function, $I_x(a,b)$, which is the cumulative distribution function (CDF) of the Beta distribution, $\\mathrm{Beta}(a,b)$. The relevant identities are:\n$$ P(Y \\ge y \\mid p) = I_p(y, n-y+1) $$\n$$ P(Y \\le y \\mid p) = I_{1-p}(n-y, y+1) $$\nLet $F_{\\beta}^{-1}(q; a, b)$ denote the quantile function (the inverse CDF) of the $\\mathrm{Beta}(a,b)$ distribution, which returns the value $x$ such that $I_x(a,b)=q$.\n\nUsing these identities, we can express $L$ and $U$:\n- For $L$, the equation $P(Y \\ge y \\mid p=L) = \\alpha/2$ becomes $I_L(y, n-y+1) = \\alpha/2$. Solving for $L$ gives $L = F_{\\beta}^{-1}(\\alpha/2; y, n-y+1)$.\n- For $U$, the equation $P(Y \\le y \\mid p=U) = \\alpha/2$ becomes $I_{1-U}(n-y, y+1) = \\alpha/2$. Let $q=1-U$. Then $I_q(n-y, y+1) = \\alpha/2$. So, $q = F_{\\beta}^{-1}(\\alpha/2; n-y, y+1)$. Then $U = 1-q = 1 - F_{\\beta}^{-1}(\\alpha/2; n-y, y+1)$. Using the identity $1-F_{\\beta}^{-1}(q; a, b) = F_{\\beta}^{-1}(1-q; b, a)$, we get $U = F_{\\beta}^{-1}(1-\\alpha/2; y+1, n-y)$.\n\nThe complete definition of the endpoints is:\n- For $y=0$: $L=0$ and $U = F_{\\beta}^{-1}(1-\\alpha/2; 1, n)$.\n- For $y \\in \\{1, \\dots, n-1\\}$: $L = F_{\\beta}^{-1}(\\alpha/2; y, n-y+1)$ and $U = F_{\\beta}^{-1}(1-\\alpha/2; y+1, n-y)$.\n- For $y=n$: $L=F_{\\beta}^{-1}(\\alpha/2; n, 1)$ and $U=1$.\n\n#### 2. Conservatism of the CP Interval\n\nThe coverage probability of a confidence interval for a parameter $p$ is the probability, under the assumption that $p$ is the true value, that the interval contains $p$. The CP interval $[L(Y), U(Y)]$ is conservative, meaning its coverage probability $C(p) = P(L(Y) \\le p \\le U(Y))$ is at least $(1-\\alpha)$ for all $p \\in (0,1)$.\n\nBy construction, the interval $[L(y), U(y)]$ contains a value $p_0$ if and only if the hypothesis $H_0: p=p_0$ is not rejected. This occurs when both one-sided p-values are greater than or equal to $\\alpha/2$:\n$$ L(y) \\le p \\iff P(Y \\ge y \\mid p) \\ge \\alpha/2 $$\n$$ p \\le U(y) \\iff P(Y \\le y \\mid p) \\ge \\alpha/2 $$\nThe interval fails to cover the true value $p$ if either $p < L(Y)$ or $p > U(Y)$. This corresponds to the rejection of $H_0: p$. The probability of non-coverage is the sum of the probabilities of two disjoint events (disjointness shown below): Type I error for the upper-tailed test and Type I error for the lower-tailed test.\n$$ P(\\text{non-coverage}) = P_p(Y \\in \\{y \\mid p < L(y)\\}) + P_p(Y \\in \\{y \\mid p > U(y)\\}) $$\nThe condition $p > U(y)$ is equivalent to $P_p(Y \\le y) < \\alpha/2$. The actual probability of this event, let's call it the lower tail rejection probability $\\alpha_L(p)$, is $\\sum_{y \\text{ s.t. } P_p(Y \\le y) < \\alpha/2} P_p(Y=y)$. Let $y_{max}$ be the largest $y$ satisfying the condition. This sum is $P_p(Y \\le y_{max})$, which by definition is less than $\\alpha/2$.\nSimilarly, the condition $p < L(y)$ is equivalent to $P_p(Y \\ge y) < \\alpha/2$. The probability of this event, the upper tail rejection probability $\\alpha_U(p)$, is $\\sum_{y \\text{ s.t. } P_p(Y \\ge y) < \\alpha/2} P_p(Y=y)$. Let $y_{min}$ be the smallest $y$ satisfying this. This sum is $P_p(Y \\ge y_{min})$, which is less than $\\alpha/2$.\n\nThe exact size of a test based on a discrete distribution must be less than or equal to the nominal level. This is because the p-value distribution is discrete, and it is not generally possible to define a rejection region whose probability is exactly $\\alpha/2$. We must choose a rejection region whose probability is the largest possible value that does not exceed $\\alpha/2$. Thus, we have $\\alpha_L(p) \\le \\alpha/2$ and $\\alpha_U(p) \\le \\alpha/2$.\n\nThe total non-coverage probability is $\\alpha_L(p) + \\alpha_U(p) \\le \\alpha/2 + \\alpha/2 = \\alpha$.\nTherefore, the coverage probability is $C(p) = 1 - (\\alpha_L(p) + \\alpha_U(p)) \\ge 1-\\alpha$.\n\nThe coverage is strictly greater than $(1-\\alpha)$ for many values of $p$ precisely because of this discreteness. For most combinations of $(n, p)$, there is no integer $y$ for which $P_p(Y \\le y)$ or $P_p(Y \\ge y)$ is exactly equal to $\\alpha/2$. As a result, the actual rejection probabilities $\\alpha_L(p)$ and $\\alpha_U(p)$ are often strictly less than $\\alpha/2$, leading to a total non-coverage probability strictly less than $\\alpha$ and a coverage probability strictly greater than $(1-\\alpha)$.\n\n#### 3. Application to a Pilot-Safety Setting\n\nWe are given $n=2$, $y=1$, and $(1-\\alpha)=0.90$. This implies $\\alpha=0.10$ and $\\alpha/2=0.05$.\n\nFor the lower endpoint $L$, we solve for $p=L$ in $P(Y \\ge 1 \\mid p) = 0.05$.\n$$ P(Y \\ge 1 \\mid p) = P(Y=1 \\mid p) + P(Y=2 \\mid p) = \\binom{2}{1} p^1 (1-p)^1 + \\binom{2}{2} p^2 (1-p)^0 $$\n$$ = 2p(1-p) + p^2 = 2p - 2p^2 + p^2 = 2p - p^2 $$\nWe set $2L - L^2 = 0.05$, which rearranges to the quadratic equation $L^2 - 2L + 0.05 = 0$.\nUsing the quadratic formula, $L = \\frac{2 \\pm \\sqrt{4 - 4(1)(0.05)}}{2} = 1 \\pm \\frac{\\sqrt{3.8}}{2}$. Since $L$ must be in $[0,1]$, we take the smaller root:\n$$ L = 1 - \\frac{\\sqrt{3.8}}{2} \\approx 1 - \\frac{1.94935886...}{2} = 1 - 0.97467943... \\approx 0.02532057... $$\nFor the upper endpoint $U$, we solve for $p=U$ in $P(Y \\le 1 \\mid p) = 0.05$.\n$$ P(Y \\le 1 \\mid p) = P(Y=0 \\mid p) + P(Y=1 \\mid p) = \\binom{2}{0} p^0 (1-p)^2 + \\binom{2}{1} p^1 (1-p)^1 $$\n$$ = (1-p)^2 + 2p(1-p) = (1-2p+p^2) + (2p-2p^2) = 1 - p^2 $$\nWe set $1 - U^2 = 0.05$, which gives $U^2 = 0.95$. Since $U$ must be in $[0,1]$, we take the positive root:\n$$ U = \\sqrt{0.95} \\approx 0.97467943... $$\nThe length of the confidence interval is $U-L$:\n$$ U-L = \\sqrt{0.95} - \\left(1 - \\frac{\\sqrt{3.8}}{2}\\right) \\approx 0.97467943 - 0.02532057 = 0.94935886... $$\nRounding to four significant figures, the length is $0.9494$.",
            "answer": "$$\\boxed{0.9494}$$"
        },
        {
            "introduction": "Theoretical properties of statistical methods must be verified in practice, and simulation is the modern tool for doing so. This hands-on coding exercise guides you to build a Monte Carlo simulation to empirically evaluate the performance of different confidence intervals for a proportion, including the Wald, Wilson, and Clopper-Pearson methods.  By directly observing how often each interval \"captures\" the true parameter value under different conditions, you will gain a tangible and robust understanding of why certain methods are preferred and how concepts like nominal coverage translate into real-world performance.",
            "id": "4805580",
            "problem": "Consider a binary outcome model in which the number of events $X$ observed in $n$ independent trials follows a Binomial distribution $X \\sim \\mathrm{Binomial}(n,p)$ with unknown event probability $p \\in (0,1)$. In the frequentist framework, the coverage of a confidence interval for $p$ is defined as the probability, under repeated sampling from the true model, that the interval contains the true parameter value. Formally, for a confidence interval procedure that maps data $X$ to an interval $I(X)$, the coverage at a particular $p$ is $C(p) = \\mathbb{P}_p\\{p \\in I(X)\\}$.\n\nDesign a simulation study to empirically assess the coverage of three different two-sided confidence intervals for the binomial proportion $p$, each at nominal confidence level $1-\\alpha$:\n\n- The Wald interval obtained from asymptotic normality of the maximum likelihood estimator $\\hat{p}=X/n$.\n- The Wilson score interval obtained by inverting the score test for $H_0: p=p_0$.\n- The exact Clopper–Pearson interval obtained by inverting the exact binomial test.\n\nUse the following fundamental base to derive and implement each interval:\n\n- The data-generating mechanism $X \\sim \\mathrm{Binomial}(n,p)$ and the maximum likelihood estimator $\\hat{p}=X/n$.\n- Asymptotic normality of $\\hat{p}$: under regularity, $\\sqrt{n}(\\hat{p}-p)$ is approximately normal with mean $0$ and variance $p(1-p)$, and a plug-in estimate may be used for $p$ in the variance term.\n- The score test for $H_0: p=p_0$ in the binomial model, and confidence intervals obtained by inverting this test.\n- Exact test inversion for the binomial model leading to equal-tailed confidence intervals, computable via quantiles of the Beta distribution.\n\nYou must implement a Monte Carlo simulation that, for each test case, generates $M$ independent realizations $X_1,\\dots,X_M$ from the Binomial model with the specified $(n,p)$, constructs the three confidence intervals for each realization, and estimates coverage for each method as the fraction of realizations whose interval contains the true $p$. Intervals must be clipped to the parameter space $[0,1]$ to ensure scientific realism. Randomness must be made reproducible by fixing the random number generator seed.\n\nSimulation parameters:\n\n- Nominal confidence: $1-\\alpha = 0.95$ (that is, $\\alpha = 0.05$).\n- Number of Monte Carlo replicates per test case: $M = 10000$.\n- Random seed: $12345$.\n\nTest suite (each test case is a pair $(n,p)$):\n\n- Case $1$: $(n,p) = (20,\\,0.01)$.\n- Case $2$: $(n,p) = (20,\\,0.10)$.\n- Case $3$: $(n,p) = (20,\\,0.50)$.\n- Case $4$: $(n,p) = (50,\\,0.01)$.\n- Case $5$: $(n,p) = (50,\\,0.10)$.\n- Case $6$: $(n,p) = (50,\\,0.50)$.\n- Case $7$: $(n,p) = (200,\\,0.01)$.\n- Case $8$: $(n,p) = (200,\\,0.10)$.\n- Case $9$: $(n,p) = (200,\\,0.50)$.\n\nFor each test case, report the empirical coverage for the three methods in the fixed order Wilson, Clopper–Pearson, Wald. Express each coverage as a decimal rounded to $4$ places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a three-element list corresponding to a test case, for example, $[\\,[c_{1,\\mathrm{Wilson}},c_{1,\\mathrm{CP}},c_{1,\\mathrm{Wald}}],\\,[c_{2,\\mathrm{Wilson}},c_{2,\\mathrm{CP}},c_{2,\\mathrm{Wald}}],\\,\\dots\\,]$.",
            "solution": "The objective is to conduct a Monte Carlo simulation study to evaluate the empirical coverage probability of three common confidence intervals for a binomial proportion, $p$. The coverage probability of an interval procedure $I(X)$ for a parameter $p$ is defined as the probability, over repeated sampling, that the computed interval contains the true value of $p$. For a nominal confidence level of $1-\\alpha$, a good interval procedure should have its actual coverage $C(p) = \\mathbb{P}_p\\{p \\in I(X)\\}$ close to $1-\\alpha$ for all possible values of $p$.\n\nThe simulation will proceed as follows. For each test case, defined by a number of trials $n$ and a true probability $p_{\\text{true}}$, we generate $M=10000$ independent outcomes $X_i \\sim \\mathrm{Binomial}(n, p_{\\text{true}})$. For each outcome $X_i$, we compute the three specified confidence intervals. The empirical coverage for each method is then estimated as the fraction of these $M$ intervals that contain $p_{\\text{true}}$. The nominal confidence level is $1-\\alpha = 0.95$, so $\\alpha = 0.05$. The critical value from the standard normal distribution is $z_{\\alpha/2} = z_{0.025}$, which is the $(1 - 0.025)$-th quantile of $\\mathcal{N}(0,1)$.\n\nLet $X$ be the number of successes in $n$ trials. The maximum likelihood estimator (MLE) for $p$ is $\\hat{p} = X/n$.\n\n### 1. Wald Interval\n\nThe Wald interval is derived from the asymptotic normality of the MLE. The Central Limit Theorem implies that for large $n$, the distribution of $\\hat{p}$ is approximately normal: $\\hat{p} \\sim \\mathcal{N}(p, \\frac{p(1-p)}{n})$. By Slutsky's theorem, we can substitute the consistent estimator $\\hat{p}$ for $p$ in the variance term, leading to the standardized statistic:\n$$ Z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} $$\nwhich is approximately distributed as a standard normal $\\mathcal{N}(0,1)$. A $100(1-\\alpha)\\%$ confidence interval for $p$ is formed by finding the set of $p$ values for which $|Z| \\le z_{\\alpha/2}$:\n$$ \\mathbb{P}\\left(-z_{\\alpha/2} \\le \\frac{\\hat{p} - p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} \\le z_{\\alpha/2}\\right) \\approx 1-\\alpha $$\nSolving for $p$ yields the Wald interval:\n$$ \\left[ \\hat{p} - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\; \\hat{p} + z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right] $$\nA practical issue arises when $X=0$ or $X=n$, making $\\hat{p}=0$ or $\\hat{p}=1$. In these cases, the standard error term $\\sqrt{\\hat{p}(1-\\hat{p})/n}$ becomes $0$, and the interval collapses to a point, $[0,0]$ or $[1,1]$, which unrealistically suggests perfect certainty. The computed interval will be clipped to $[0,1]$.\n\n### 2. Wilson Score Interval\n\nThe Wilson score interval is derived by inverting the score test. Unlike the Wald interval, the score test uses the variance under the null hypothesis, $p_0(1-p_0)/n$, which avoids the issue of using $\\hat{p}$ in the standard error. The interval is the set of all possible values $p_0$ for which the null hypothesis $H_0: p=p_0$ is not rejected at level $\\alpha$. This corresponds to the set of $p$ satisfying:\n$$ \\left| \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\right| \\le z_{\\alpha/2} $$\nSquaring both sides and rearranging terms gives a quadratic inequality in $p$:\n$$ (\\hat{p} - p)^2 \\le z_{\\alpha/2}^2 \\frac{p(1-p)}{n} $$\n$$ n(\\hat{p}^2 - 2\\hat{p}p + p^2) \\le z_{\\alpha/2}^2 (p - p^2) $$\n$$ (n + z_{\\alpha/2}^2)p^2 - (2n\\hat{p} + z_{\\alpha/2}^2)p + n\\hat{p}^2 \\le 0 $$\nThe roots of the corresponding quadratic equation $Ap^2+Bp+C=0$ define the endpoints of the interval. Using the quadratic formula $\\frac{-B \\pm \\sqrt{B^2-4AC}}{2A}$ with $A = n+z_{\\alpha/2}^2$, $B = -(2n\\hat{p} + z_{\\alpha/2}^2)$, and $C = n\\hat{p}^2$, the interval is given by:\n$$ \\frac{2n\\hat{p} + z_{\\alpha/2}^2 \\pm z_{\\alpha/2}\\sqrt{4n\\hat{p}(1-\\hat{p}) + z_{\\alpha/2}^2}}{2(n+z_{\\alpha/2}^2)} $$\nThis can be expressed as:\n$$ \\frac{1}{1 + z_{\\alpha/2}^2/n} \\left( \\hat{p} + \\frac{z_{\\alpha/2}^2}{2n} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z_{\\alpha/2}^2}{4n^2}} \\right) $$\nThis interval has better performance than the Wald interval, especially for small sample sizes or when $p$ is close to $0$ or $1$.\n\n### 3. Clopper-Pearson Interval\n\nThe Clopper-Pearson interval is an \"exact\" method derived by inverting two one-sided binomial tests. It is constructed to guarantee that the coverage probability is at least $1-\\alpha$ for all values of $p$.\nThe lower endpoint, $p_L$, is the solution to:\n$$ \\mathbb{P}(X \\ge x | p=p_L) = \\sum_{k=x}^{n} \\binom{n}{k} p_L^k (1-p_L)^{n-k} = \\frac{\\alpha}{2} $$\nThe upper endpoint, $p_U$, is the solution to:\n$$ \\mathbb{P}(X \\le x | p=p_U) = \\sum_{k=0}^{x} \\binom{n}{k} p_U^k (1-p_U)^{n-k} = \\frac{\\alpha}{2} $$\nThese equations can be solved using the relationship between the binomial cumulative distribution function and the regularized incomplete beta function. The interval endpoints are given by quantiles of the Beta distribution:\n- The lower bound $p_L$ is the $\\alpha/2$ quantile of a Beta distribution with parameters $(x, n-x+1)$: $p_L = \\mathrm{Beta}^{-1}(\\alpha/2; x, n-x+1)$. For the special case $x=0$, $p_L=0$.\n- The upper bound $p_U$ is the $1-\\alpha/2$ quantile of a Beta distribution with parameters $(x+1, n-x)$: $p_U = \\mathrm{Beta}^{-1}(1-\\alpha/2; x+1, n-x)$. For the special case $x=n$, $p_U=1$.\n\nThis method is known to be conservative, meaning its actual coverage often exceeds the nominal $1-\\alpha$ level.\n\nThe simulation will be implemented in Python using the `numpy` library for vectorized computations and random number generation, and `scipy.stats` for statistical functions (quantiles of normal and beta distributions). The random number generator will be seeded with $12345$ for reproducibility.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, beta\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo simulation to estimate the coverage of Wald, Wilson,\n    and Clopper-Pearson confidence intervals for a binomial proportion.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (20, 0.01),\n        (20, 0.10),\n        (20, 0.50),\n        (50, 0.01),\n        (50, 0.10),\n        (50, 0.50),\n        (200, 0.01),\n        (200, 0.10),\n        (200, 0.50),\n    ]\n\n    # Simulation parameters\n    M = 10000\n    alpha = 0.05\n    seed = 12345\n    rng = np.random.default_rng(seed)\n    \n    # Pre-calculate the standard normal critical value\n    z_crit = norm.ppf(1 - alpha / 2)\n    \n    all_results = []\n    \n    for n, p_true in test_cases:\n        # Generate M binomial random variates\n        # Shape: (M,)\n        X = rng.binomial(n, p_true, size=M)\n        \n        # Estimate of p\n        # Shape: (M,)\n        p_hat = X / n\n        \n        # --- 1. Wald Interval ---\n        # Standard error for the Wald interval\n        wald_se = np.sqrt(p_hat * (1 - p_hat) / n)\n        wald_lower = p_hat - z_crit * wald_se\n        wald_upper = p_hat + z_crit * wald_se\n        \n        # Clip interval to [0, 1]\n        wald_lower = np.maximum(0, wald_lower)\n        wald_upper = np.minimum(1, wald_upper)\n        \n        # Calculate coverage\n        wald_covered = (wald_lower <= p_true) & (p_true <= wald_upper)\n        wald_coverage = np.mean(wald_covered)\n        \n        # --- 2. Wilson Score Interval ---\n        z2 = z_crit**2\n        denom = 1 + z2 / n\n        center_adj = (p_hat + z2 / (2 * n))\n        \n        term = z_crit * np.sqrt( (p_hat * (1 - p_hat) / n) + (z2 / (4 * n**2)) )\n        \n        wilson_lower = (center_adj - term) / denom\n        wilson_upper = (center_adj + term) / denom\n\n        # Clip interval to [0, 1]\n        wilson_lower = np.maximum(0, wilson_lower)\n        wilson_upper = np.minimum(1, wilson_upper)\n        \n        # Calculate coverage\n        wilson_covered = (wilson_lower <= p_true) & (p_true <= wilson_upper)\n        wilson_coverage = np.mean(wilson_covered)\n\n        # --- 3. Clopper-Pearson Interval ---\n        alpha_half = alpha / 2\n        \n        # Lower bound\n        cp_lower = np.zeros_like(p_hat)\n        # Handle cases where X > 0, otherwise beta.ppf fails for a=0\n        mask_pos_x = X > 0\n        cp_lower[mask_pos_x] = beta.ppf(alpha_half, X[mask_pos_x], n - X[mask_pos_x] + 1)\n        \n        # Upper bound\n        cp_upper = np.ones_like(p_hat)\n        # Handle cases where X  n, otherwise beta.ppf fails for b=0\n        mask_lt_n = X  n\n        cp_upper[mask_lt_n] = beta.ppf(1 - alpha_half, X[mask_lt_n] + 1, n - X[mask_lt_n])\n        \n        # Calculate coverage\n        cp_covered = (cp_lower = p_true)  (p_true = cp_upper)\n        cp_coverage = np.mean(cp_covered)\n        \n        # Store results rounded to 4 decimal places in the specified order\n        # (Wilson, Clopper-Pearson, Wald)\n        case_results = [\n            round(wilson_coverage, 4),\n            round(cp_coverage, 4),\n            round(wald_coverage, 4)\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string as specified\n    sub_results_str = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output_str = f\"[{','.join(sub_results_str)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}