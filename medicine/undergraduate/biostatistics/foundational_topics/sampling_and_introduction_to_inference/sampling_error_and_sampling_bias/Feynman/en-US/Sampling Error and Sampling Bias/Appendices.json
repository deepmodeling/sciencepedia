{
    "hands_on_practices": [
        {
            "introduction": "In biostatistical studies, sampling strategies are often designed for efficiency. However, a seemingly practical approach, such as sampling more individuals from a subregion with higher expected disease prevalence, can introduce a systematic error if the data are not analyzed correctly. This exercise provides a concrete counterexample to illustrate how such a disproportionate sampling plan leads to sampling bias in a naive estimate of regional prevalence and allows you to quantify the exact magnitude of this bias .",
            "id": "4951835",
            "problem": "A public health agency aims to estimate the regional prevalence of a binary disease outcome by pooling samples collected from two subregions, denoted $A$ and $B$. The true subregion prevalences are constant within each subregion. The agency’s field teams, seeking efficiency, sampled more densely in the subregion they perceived to have higher prevalence. Consider the following explicit scenario, which is intended to serve as a counterexample illustrating how such a sampling plan can inflate the estimated regional prevalence if naive pooling is used.\n\n- Subregion $A$ has population size $N_{A} = 6000$ and true prevalence $p_{A} = 0.12$.\n- Subregion $B$ has population size $N_{B} = 14000$ and true prevalence $p_{B} = 0.03$.\n- The true regional prevalence is defined as the population-size-weighted mean of subregion prevalences.\n- The sampling plan draws $m_{A} = 400$ individuals from subregion $A$ and $m_{B} = 100$ individuals from subregion $B$, using simple random sampling without replacement within each subregion.\n- Let $\\hat{p}_{A}$ and $\\hat{p}_{B}$ denote the sample proportions of disease in the two subregions. The agency reports the pooled, unweighted sample proportion $\\hat{p}_{\\text{naive}} = \\dfrac{m_{A}\\hat{p}_{A} + m_{B}\\hat{p}_{B}}{m_{A} + m_{B}}$ as the regional estimate.\n\nUsing only core definitions from sampling theory and probability (definition of prevalence as a population mean of indicator variables, expectation of a sample proportion under simple random sampling without replacement, and the definition of bias as the difference between an estimator’s expectation and the target parameter), analytically show that this sampling design inflates the expected naive pooled estimate relative to the true regional prevalence. Then compute the bias magnitude, defined as $|\\mathbb{E}[\\hat{p}_{\\text{naive}}] - p_{\\text{true}}|$, where $p_{\\text{true}}$ denotes the true regional prevalence. Express your final answer as a decimal (no percent sign). No rounding is required.",
            "solution": "The problem is well-posed and scientifically sound, allowing for a complete analytical solution. We begin by formally defining the parameter of interest, the true regional prevalence, and the estimator proposed by the agency. We will then analyze the estimator's statistical properties, specifically its expectation, to quantify its bias.\n\nThe total population of the region is $N = N_{A} + N_{B}$. The number of individuals with the disease in subregion $A$ is $N_{A}p_{A}$, and in subregion $B$ is $N_{B}p_{B}$. The true regional prevalence, $p_{\\text{true}}$, is defined as the total number of diseased individuals in the region divided by the total population size. This is equivalent to the population-size-weighted mean of the subregion prevalences.\n$$p_{\\text{true}} = \\frac{N_{A}p_{A} + N_{B}p_{B}}{N_{A} + N_{B}}$$\nLet us define the population weight for subregion $A$ as $W_{A} = \\frac{N_{A}}{N_{A}+N_{B}}$ and for subregion $B$ as $W_{B} = \\frac{N_{B}}{N_{A}+N_{B}}$, where $W_{A}+W_{B}=1$. Then, the true prevalence can be written as:\n$$p_{\\text{true}} = W_{A}p_{A} + W_{B}p_{B}$$\n\nThe agency's proposed estimator, $\\hat{p}_{\\text{naive}}$, is the unweighted proportion of diseased individuals in the pooled sample. Let $X_{A} = m_{A}\\hat{p}_{A}$ be the number of diseased individuals in the sample from subregion $A$, and $X_{B} = m_{B}\\hat{p}_{B}$ be the number in the sample from subregion $B$. The estimator is:\n$$\\hat{p}_{\\text{naive}} = \\frac{X_{A} + X_{B}}{m_{A} + m_{B}} = \\frac{m_{A}\\hat{p}_{A} + m_{B}\\hat{p}_{B}}{m_{A} + m_{B}}$$\nTo assess the bias of this estimator, we first compute its expected value, $\\mathbb{E}[\\hat{p}_{\\text{naive}}]$. Using the linearity of the expectation operator:\n$$\\mathbb{E}[\\hat{p}_{\\text{naive}}] = \\mathbb{E}\\left[\\frac{m_{A}\\hat{p}_{A} + m_{B}\\hat{p}_{B}}{m_{A} + m_{B}}\\right] = \\frac{m_{A}\\mathbb{E}[\\hat{p}_{A}] + m_{B}\\mathbb{E}[\\hat{p}_{B}]}{m_{A} + m_{B}}$$\nThe sampling within each subregion is simple random sampling without replacement (SRSWOR). A fundamental result of sampling theory is that the sample proportion, $\\hat{p}$, is an unbiased estimator of the population proportion, $p$. Therefore, $\\mathbb{E}[\\hat{p}_{A}] = p_{A}$ and $\\mathbb{E}[\\hat{p}_{B}] = p_{B}$. Substituting these into the equation for the expected value gives:\n$$\\mathbb{E}[\\hat{p}_{\\text{naive}}] = \\frac{m_{A}p_{A} + m_{B}p_{B}}{m_{A} + m_{B}}$$\nThis expression shows that the expected value of the naive estimator is a weighted average of the subregion prevalences, where the weights are determined by the sample sizes, not the population sizes. Let us define the sample weights as $w_{A} = \\frac{m_{A}}{m_{A}+m_{B}}$ and $w_{B} = \\frac{m_{B}}{m_{A}+m_{B}}$, where $w_{A}+w_{B}=1$. Then,\n$$\\mathbb{E}[\\hat{p}_{\\text{naive}}] = w_{A}p_{A} + w_{B}p_{B}$$\n\nThe bias of the estimator is the difference between its expectation and the true parameter value:\n$$\\text{Bias}(\\hat{p}_{\\text{naive}}) = \\mathbb{E}[\\hat{p}_{\\text{naive}}] - p_{\\text{true}} = (w_{A}p_{A} + w_{B}p_{B}) - (W_{A}p_{A} + W_{B}p_{B})$$\nUsing $w_{B} = 1 - w_{A}$ and $W_{B} = 1 - W_{A}$, we can rearrange the terms:\n$$\\text{Bias}(\\hat{p}_{\\text{naive}}) = (w_{A} - W_{A})p_{A} + ((1-w_{A}) - (1-W_{A}))p_{B} = (w_{A} - W_{A})p_{A} - (w_{A} - W_{A})p_{B}$$\n$$\\text{Bias}(\\hat{p}_{\\text{naive}}) = (w_{A} - W_{A})(p_{A} - p_{B})$$\nThe problem states that the sampling plan oversampled the subregion with higher prevalence. The given data are $p_{A} = 0.12$ and $p_{B} = 0.03$, so $p_{A} > p_{B}$, which makes the term $(p_{A} - p_{B})$ positive.\nThe sampling weight for subregion $A$ is $w_{A} = \\frac{m_{A}}{m_{A}+m_{B}} = \\frac{400}{400+100} = \\frac{400}{500} = 0.8$.\nThe population weight for subregion $A$ is $W_{A} = \\frac{N_{A}}{N_{A}+N_{B}} = \\frac{6000}{6000+14000} = \\frac{6000}{20000} = 0.3$.\nSince $w_{A} > W_{A}$, the term $(w_{A} - W_{A})$ is also positive. Thus, the product $(w_{A} - W_{A})(p_{A} - p_{B})$ is positive, which analytically demonstrates that the bias is positive. This means $\\mathbb{E}[\\hat{p}_{\\text{naive}}] > p_{\\text{true}}$, and the naive estimator is inflated as claimed. The inflation arises because the sampling proportion from subregion $A$ ($80\\%$) is much larger than its population proportion ($30\\%$), giving undue weight to its higher prevalence.\n\nNow, we compute the numerical value of the bias magnitude.\nFirst, we calculate the true regional prevalence, $p_{\\text{true}}$:\n$$p_{\\text{true}} = \\frac{N_{A}p_{A} + N_{B}p_{B}}{N_{A} + N_{B}} = \\frac{(6000)(0.12) + (14000)(0.03)}{6000 + 14000} = \\frac{720 + 420}{20000} = \\frac{1140}{20000} = 0.057$$\nNext, we calculate the expected value of the naive pooled estimate, $\\mathbb{E}[\\hat{p}_{\\text{naive}}]$:\n$$\\mathbb{E}[\\hat{p}_{\\text{naive}}] = \\frac{m_{A}p_{A} + m_{B}p_{B}}{m_{A} + m_{B}} = \\frac{(400)(0.12) + (100)(0.03)}{400 + 100} = \\frac{48 + 3}{500} = \\frac{51}{500} = 0.102$$\nThe bias is the difference between these two values:\n$$\\text{Bias} = \\mathbb{E}[\\hat{p}_{\\text{naive}}] - p_{\\text{true}} = 0.102 - 0.057 = 0.045$$\nThe bias magnitude is the absolute value of the bias:\n$$|\\text{Bias}| = |0.045| = 0.045$$\nThis confirms a substantial positive bias, leading to an overestimation of the true regional prevalence by $4.5$ percentage points on average.",
            "answer": "$$\\boxed{0.045}$$"
        },
        {
            "introduction": "Beyond systematic bias, we must also manage sampling error—the random variation that occurs simply because we are observing a sample instead of the whole population. This practice addresses one of the most fundamental questions in study design: \"How large a sample do I need?\" You will derive the formula for the minimum sample size required to achieve a desired precision, paying special attention to the finite population correction (FPC), an essential adjustment when the sample constitutes a non-trivial fraction of the total population .",
            "id": "4951762",
            "problem": "A biostatistics team plans to estimate the mean body mass index in a finite registry of adult patients using Simple Random Sampling Without Replacement (SRSWOR). The registry contains a known, fixed population of size $N = 5000$. From a prior census of the same registry, the population variance is known to be $S^{2} = 36$ and is believed to have remained stable. The team desires a two-sided confidence interval for the population mean at confidence level $1 - \\alpha = 0.95$ and requires that the margin of error be at most $m = 0.5$ (in body mass index units). Assume that under SRSWOR the sampling estimator is unbiased and that a normal approximation is appropriate for the sampling distribution of the sample mean. Explicitly include the finite population correction in your derivation and computations.\n\nWork from fundamental definitions that distinguish sampling error from sampling bias, and from the core fact that under SRSWOR the sample mean is unbiased. Starting from the definition of a two-sided $(1 - \\alpha)$ confidence interval for the mean in terms of a standard normal quantile $z_{1-\\alpha/2}$ and the standard error of the sample mean under SRSWOR, derive an expression for the continuous (real-valued) sample size $n_{\\text{cont}}$ that achieves a margin of error exactly equal to $m$. Then, identify the minimal integer sample size $n_{\\min}$ that guarantees the margin of error requirement by taking the smallest integer that meets the inequality implied by your derivation.\n\nUse $z_{1-\\alpha/2} = z_{0.975}$ for $\\alpha = 0.05$ and evaluate numerically with $z_{0.975} \\approx 1.96$. Report the minimal integer sample size $n_{\\min}$. Do not round by significant figures; instead, provide the smallest integer $n$ that satisfies the requirement.",
            "solution": "The problem requires the determination of the minimum integer sample size, $n_{\\min}$, needed to estimate a population mean with a specified margin of error and confidence level, drawing from a finite population using Simple Random Sampling Without Replacement (SRSWOR). The solution must be rigorously derived, beginning with fundamental statistical principles.\n\nFirst, we must distinguish between sampling bias and sampling error. Sampling bias refers to a systematic discrepancy between an estimator's expected value and the true population parameter. An estimator $\\hat{\\theta}$ for a parameter $\\theta$ is unbiased if its expectation equals the true value, i.e., $E[\\hat{\\theta}] = \\theta$. Conversely, sampling error is the random variability of an estimator's value from one sample to another, inherent in the process of sampling. It is not a systematic error but a consequence of observing a subset, rather than the entire population. The magnitude of sampling error is typically quantified by the standard error of the estimator.\n\nThe problem specifies the use of SRSWOR, for which the sample mean, $\\bar{y}$, is a well-established unbiased estimator of the population mean, $\\mu$. Thus, $E[\\bar{y}] = \\mu$, and there is no sampling bias to consider. Our task is to control the sampling error by ensuring the sample size is sufficiently large.\n\nA two-sided confidence interval for the population mean $\\mu$ at a confidence level of $1 - \\alpha$ is constructed as:\n$$ \\bar{y} \\pm m $$\nwhere $m$ is the margin of error. Under the assumption that the sampling distribution of the sample mean is approximately normal, the margin of error is defined as:\n$$ m = z_{1-\\alpha/2} \\times \\text{SE}(\\bar{y}) $$\nHere, $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution, and $\\text{SE}(\\bar{y})$ is the standard error of the sample mean.\n\nFor Simple Random Sampling Without Replacement from a finite population of size $N$, the variance of the sample mean is given by:\n$$ \\text{Var}(\\bar{y}) = \\frac{S^2}{n} \\left( \\frac{N-n}{N} \\right) $$\nwhere $S^2$ is the population variance, $n$ is the sample size, and the term $\\left( \\frac{N-n}{N} \\right)$ is the finite population correction (FPC). The standard error is the square root of the variance:\n$$ \\text{SE}(\\bar{y}) = \\sqrt{\\frac{S^2}{n} \\left( \\frac{N-n}{N} \\right)} $$\nThe problem requires the margin of error to be at most $m = 0.5$. To find the minimum sample size, we set the margin of error to its maximum permissible value:\n$$ m = z_{1-\\alpha/2} \\sqrt{\\frac{S^2}{n} \\left( \\frac{N-n}{N} \\right)} $$\nWe now solve this equation for $n$. Squaring both sides yields:\n$$ m^2 = z_{1-\\alpha/2}^2 \\left( \\frac{S^2}{n} \\right) \\left( \\frac{N-n}{N} \\right) $$\n$$ m^2 = z_{1-\\alpha/2}^2 S^2 \\left( \\frac{1}{n} - \\frac{1}{N} \\right) $$\nTo isolate $n$, we first solve for $\\frac{1}{n}$:\n$$ \\frac{m^2}{z_{1-\\alpha/2}^2 S^2} = \\frac{1}{n} - \\frac{1}{N} $$\n$$ \\frac{1}{n} = \\frac{m^2}{z_{1-\\alpha/2}^2 S^2} + \\frac{1}{N} $$\nLet us define $n_0$ as the sample size that would be required for an infinite population (or, equivalently, if the FPC were ignored). This is found by setting the FPC factor to $1$:\n$$ n_0 = \\frac{z_{1-\\alpha/2}^2 S^2}{m^2} $$\nSubstituting this definition into our equation for $\\frac{1}{n}$:\n$$ \\frac{1}{n} = \\frac{1}{n_0} + \\frac{1}{N} $$\nThis elegantly relates the required sample size $n$ to the infinite-population size $n_0$ and the population size $N$. To find the expression for the continuous sample size, $n_{\\text{cont}}$, we solve for $n$:\n$$ \\frac{1}{n} = \\frac{N + n_0}{n_0 N} $$\n$$ n = n_{\\text{cont}} = \\frac{n_0 N}{n_0 + N} = \\frac{n_0}{1 + \\frac{n_0}{N}} $$\nThis is the derived expression for the continuous sample size $n_{\\text{cont}}$ that achieves a margin of error exactly equal to $m$.\n\nNow, we substitute the given numerical values:\nPopulation size $N = 5000$.\nPopulation variance $S^2 = 36$.\nDesired margin of error $m = 0.5$.\nConfidence level $1 - \\alpha = 0.95$, which gives $\\alpha = 0.05$. The corresponding standard normal quantile is $z_{1-\\alpha/2} = z_{0.975} \\approx 1.96$.\n\nFirst, we calculate the initial sample size estimate, $n_0$:\n$$ n_0 = \\frac{(1.96)^2 \\times 36}{(0.5)^2} = \\frac{3.8416 \\times 36}{0.25} = 3.8416 \\times 144 = 553.1904 $$\nNext, we use this value to calculate the continuous sample size $n_{\\text{cont}}$ including the finite population correction:\n$$ n_{\\text{cont}} = \\frac{n_0}{1 + \\frac{n_0}{N}} = \\frac{553.1904}{1 + \\frac{553.1904}{5000}} = \\frac{553.1904}{1 + 0.11063808} = \\frac{553.1904}{1.11063808} \\approx 498.0835 $$\nThe value $n_{\\text{cont}} \\approx 498.0835$ is the exact real-valued sample size that would produce a margin of error of precisely $m=0.5$. However, a sample size must be an integer. The margin of error is a decreasing function of the sample size $n$. To ensure that the margin of error is *at most* $m=0.5$, the chosen integer sample size $n$ must be greater than or equal to $n_{\\text{cont}}$. Therefore, the minimal integer sample size, $n_{\\min}$, is the smallest integer that satisfies this condition, which is the ceiling of $n_{\\text{cont}}$:\n$$ n_{\\min} = \\lceil n_{\\text{cont}} \\rceil = \\lceil 498.0835 \\rceil = 499 $$\nA sample size of $n=498$ would result in a margin of error slightly larger than $0.5$, thus failing to meet the requirement. A sample size of $n=499$ is the minimum integer value that guarantees the margin of error will be no more than $0.5$.",
            "answer": "$$\\boxed{499}$$"
        },
        {
            "introduction": "Choosing the 'best' estimator involves more than just eliminating bias; it often requires navigating the fundamental bias-variance tradeoff. This exercise demonstrates a scenario where a 'calibrated' estimator successfully removes bias but at the cost of increased variance compared to a simpler, naive estimator. By calculating the Mean Squared Error ($MSE$) for both, you will learn to use this comprehensive metric to evaluate the overall performance of an estimator and make a more informed choice .",
            "id": "4951758",
            "problem": "A finite population generates independent and identically distributed observations $(X, Y)$ with $X \\in \\{0,1\\}$ and a conditional model $Y \\mid X = 1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$ and $Y \\mid X = 0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$. The population proportion of $X=1$ is known and equals $\\pi \\in (0,1)$. A convenience sample of size $n$ is drawn independently from a frame whose composition is distorted: the sampling distribution of $X$ in the frame is $\\tilde{\\pi} = \\mathbb{P}(X=1 \\text{ in the frame})$, with $\\tilde{\\pi} \\neq \\pi$. Assume $Y$ is independent of the sampling distortion conditional on $X$ (so within each $X$-group, the sample is representative for $Y$). Let the target be the population mean of $Y$, namely $\\mu_{Y} = \\pi \\mu_{1} + (1-\\pi)\\mu_{0}$.\n\nConsider two estimators for $\\mu_{Y}$ based on the distorted sample:\n1. The naive estimator $\\hat{\\mu}_{N} = \\bar{Y}$, the unweighted sample mean across all $n$ observations.\n2. The calibrated post-stratified estimator $\\hat{\\mu}_{C} = \\pi \\bar{Y}_{1} + (1-\\pi)\\bar{Y}_{0}$, where $\\bar{Y}_{1}$ and $\\bar{Y}_{0}$ are the sample means within the $X=1$ and $X=0$ groups, respectively.\n\nStarting from the definitions of bias $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$ and variance $\\operatorname{Var}(\\hat{\\theta}) = \\mathbb{E}\\!\\left[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^{2}\\right]$, and using the law of total expectation and the law of total variance, derive expressions for the bias and variance of $\\hat{\\mu}_{N}$ and $\\hat{\\mu}_{C}$ under the following assumptions:\n- The sample draws are independent and identically distributed from the frame with $X \\sim \\operatorname{Bernoulli}(\\tilde{\\pi})$.\n- Conditional on $X$, $Y$ follows the specified normal models with common conditional variance $\\sigma^{2}$ and is independent across individuals.\n- For the variance of $\\hat{\\mu}_{C}$, write the conditional variance given the group counts $N_{1}$ and $N_{0}$, and then approximate the unconditional variance by replacing $\\mathbb{E}\\!\\left[N_{1}^{-1}\\right]$ with $(n\\tilde{\\pi})^{-1}$ and $\\mathbb{E}\\!\\left[N_{0}^{-1}\\right]$ with $(n(1-\\tilde{\\pi}))^{-1}$.\n\nUse the following numerical values for a toy example: $\\pi = 0.7$, $\\tilde{\\pi} = 0.9$, $\\mu_{1} = 5$, $\\mu_{0} = 7$, $\\sigma^{2} = 1$, $n = 40$. Compute the mean squared error (MSE), defined as $\\operatorname{MSE}(\\hat{\\theta}) = \\operatorname{Var}(\\hat{\\theta}) + \\operatorname{Bias}(\\hat{\\theta})^{2}$, for both $\\hat{\\mu}_{N}$ and $\\hat{\\mu}_{C}$, and then report the numerical value of the MSE impact, defined as $\\operatorname{MSE}(\\hat{\\mu}_{C}) - \\operatorname{MSE}(\\hat{\\mu}_{N})$, expressed as a real number. Round your final numeric answer to four significant figures.",
            "solution": "The problem statement is assessed to be valid. It is a well-posed, scientifically grounded, and objective problem in the field of biostatistics, specifically concerning survey sampling theory. It provides a complete and consistent setup for deriving and comparing the bias, variance, and mean squared error (MSE) of two estimators for a population mean under sampling bias. All required definitions, models, and parameters are explicitly supplied.\n\nThe target parameter is the population mean of $Y$, given by the law of total expectation as $\\mu_Y = \\mathbb{E}_{\\text{pop}}[Y] = \\pi \\mu_1 + (1-\\pi)\\mu_0$. The sample is drawn from a frame where the proportion of subjects with $X=1$ is $\\tilde{\\pi} \\neq \\pi$. Thus, observations $(X_i, Y_i)$ for $i=1, \\dots, n$ are independent and identically distributed (i.i.d.) draws from the frame, where $X_i \\sim \\operatorname{Bernoulli}(\\tilde{\\pi})$ and the conditional distribution of $Y_i$ given $X_i$ is as specified in the population model.\n\nWe will analyze each estimator's bias and variance to compute its MSE. The MSE of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\operatorname{MSE}(\\hat{\\theta}) = \\operatorname{Var}(\\hat{\\theta}) + (\\operatorname{Bias}(\\hat{\\theta}))^2$.\n\n**1. Analysis of the Naive Estimator $\\hat{\\mu}_{N} = \\bar{Y}$**\n\nFirst, we compute the expectation of $\\hat{\\mu}_{N}$. Since the observations are i.i.d. draws from the frame, the expectation of the sample mean is the expectation of a single observation $Y$ from the frame. We use the law of total expectation, conditioning on $X$:\n$$\n\\mathbb{E}[\\hat{\\mu}_{N}] = \\mathbb{E}[\\bar{Y}] = \\mathbb{E}[Y] = \\mathbb{E}[\\mathbb{E}[Y \\mid X]]\n$$\nIn the sampling frame, $\\mathbb{P}(X=1) = \\tilde{\\pi}$ and $\\mathbb{P}(X=0) = 1-\\tilde{\\pi}$.\n$$\n\\mathbb{E}[Y] = \\mathbb{P}(X=1)\\mathbb{E}[Y \\mid X=1] + \\mathbb{P}(X=0)\\mathbb{E}[Y \\mid X=0] = \\tilde{\\pi}\\mu_1 + (1-\\tilde{\\pi})\\mu_0\n$$\nThe bias of $\\hat{\\mu}_N$ is the difference between its expectation and the true population mean $\\mu_Y$:\n$$\n\\operatorname{Bias}(\\hat{\\mu}_{N}) = \\mathbb{E}[\\hat{\\mu}_{N}] - \\mu_Y = (\\tilde{\\pi}\\mu_1 + (1-\\tilde{\\pi})\\mu_0) - (\\pi\\mu_1 + (1-\\pi)\\mu_0)\n$$\n$$\n\\operatorname{Bias}(\\hat{\\mu}_{N}) = (\\tilde{\\pi} - \\pi)\\mu_1 - (\\tilde{\\pi} - \\pi)\\mu_0 = (\\tilde{\\pi} - \\pi)(\\mu_1 - \\mu_0)\n$$\nNext, we compute the variance of $\\hat{\\mu}_{N}$. Since the observations are i.i.d., $\\operatorname{Var}(\\hat{\\mu}_{N}) = \\operatorname{Var}(\\bar{Y}) = \\frac{1}{n} \\operatorname{Var}(Y)$. We use the law of total variance for a single observation $Y$ from the frame:\n$$\n\\operatorname{Var}(Y) = \\mathbb{E}[\\operatorname{Var}(Y \\mid X)] + \\operatorname{Var}(\\mathbb{E}[Y \\mid X])\n$$\nThe conditional variance is $\\operatorname{Var}(Y \\mid X=1) = \\sigma^2$ and $\\operatorname{Var(Y \\mid X=0)} = \\sigma^2$. Therefore, the term $\\operatorname{Var}(Y \\mid X)$ is a constant $\\sigma^2$, and its expectation is $\\mathbb{E}[\\operatorname{Var}(Y \\mid X)] = \\sigma^2$.\nThe conditional expectation $\\mathbb{E}[Y \\mid X]$ is a random variable that takes the value $\\mu_1$ with probability $\\tilde{\\pi}$ and $\\mu_0$ with probability $1-\\tilde{\\pi}$. Its variance is:\n$$\n\\operatorname{Var}(\\mathbb{E}[Y \\mid X]) = \\tilde{\\pi}(1-\\tilde{\\pi})(\\mu_1 - \\mu_0)^2\n$$\nCombining these results, the variance of a single observation $Y$ is:\n$$\n\\operatorname{Var}(Y) = \\sigma^2 + \\tilde{\\pi}(1-\\tilde{\\pi})(\\mu_1 - \\mu_0)^2\n$$\nAnd the variance of the naive estimator is:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{N}) = \\frac{1}{n} \\left[ \\sigma^2 + \\tilde{\\pi}(1-\\tilde{\\pi})(\\mu_1 - \\mu_0)^2 \\right]\n$$\nThe MSE of the naive estimator is the sum of its variance and squared bias:\n$$\n\\operatorname{MSE}(\\hat{\\mu}_{N}) = \\frac{1}{n} \\left[ \\sigma^2 + \\tilde{\\pi}(1-\\tilde{\\pi})(\\mu_1 - \\mu_0)^2 \\right] + ((\\tilde{\\pi} - \\pi)(\\mu_1 - \\mu_0))^2\n$$\n\n**2. Analysis of the Calibrated Estimator $\\hat{\\mu}_{C} = \\pi \\bar{Y}_{1} + (1-\\pi)\\bar{Y}_{0}$**\n\nFirst, we compute the expectation of $\\hat{\\mu}_{C}$. Let $N_1$ be the number of observations with $X=1$ and $N_0$ with $X=0$. $N_1 \\sim \\operatorname{Binomial}(n, \\tilde{\\pi})$. We use the law of total expectation, conditioning on the counts $(N_1, N_0)$:\n$$\n\\mathbb{E}[\\hat{\\mu}_{C}] = \\mathbb{E}[\\mathbb{E}[\\pi \\bar{Y}_{1} + (1-\\pi)\\bar{Y}_{0} \\mid N_1, N_0]]\n$$\nConditional on $N_1 > 0$ and $N_0 > 0$, the sample means are unbiased for the stratum means: $\\mathbb{E}[\\bar{Y}_1 \\mid N_1] = \\mu_1$ and $\\mathbb{E}[\\bar{Y}_0 \\mid N_0] = \\mu_0$.\n$$\n\\mathbb{E}[\\hat{\\mu}_{C} \\mid N_1, N_0] = \\pi \\mathbb{E}[\\bar{Y}_1 \\mid N_1] + (1-\\pi) \\mathbb{E}[\\bar{Y}_0 \\mid N_0] = \\pi \\mu_1 + (1-\\pi) \\mu_0 = \\mu_Y\n$$\nSince the conditional expectation is the constant $\\mu_Y$, the unconditional expectation is also $\\mu_Y$.\n$$\n\\mathbb{E}[\\hat{\\mu}_{C}] = \\mathbb{E}[\\mu_Y] = \\mu_Y\n$$\nTherefore, the calibrated estimator is unbiased:\n$$\n\\operatorname{Bias}(\\hat{\\mu}_{C}) = \\mathbb{E}[\\hat{\\mu}_{C}] - \\mu_Y = 0\n$$\nNext, we compute the variance of $\\hat{\\mu}_{C}$. Since the bias is zero, the MSE equals the variance. We use the law of total variance:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{C}) = \\mathbb{E}[\\operatorname{Var}(\\hat{\\mu}_{C} \\mid N_1, N_0)] + \\operatorname{Var}(\\mathbb{E}[\\hat{\\mu}_{C} \\mid N_1, N_0])\n$$\nThe second term is $\\operatorname{Var}(\\mu_Y) = 0$. For the first term, we find the conditional variance. Given $(N_1, N_0)$, $\\bar{Y}_1$ and $\\bar{Y}_0$ are independent.\n$$\n\\operatorname{Var}(\\hat{\\mu}_{C} \\mid N_1, N_0) = \\operatorname{Var}(\\pi \\bar{Y}_{1} + (1-\\pi)\\bar{Y}_{0} \\mid N_1, N_0) = \\pi^2 \\operatorname{Var}(\\bar{Y}_1 \\mid N_1) + (1-\\pi)^2 \\operatorname{Var}(\\bar{Y}_0 \\mid N_0)\n$$\n$$\n\\operatorname{Var}(\\hat{\\mu}_{C} \\mid N_1, N_0) = \\pi^2 \\frac{\\sigma^2}{N_1} + (1-\\pi)^2 \\frac{\\sigma^2}{N_0} = \\sigma^2 \\left(\\frac{\\pi^2}{N_1} + \\frac{(1-\\pi)^2}{n-N_1}\\right)\n$$\nThe unconditional variance is the expectation of this quantity over the distribution of $N_1$:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{C}) = \\mathbb{E}\\left[\\sigma^2 \\left(\\frac{\\pi^2}{N_1} + \\frac{(1-\\pi)^2}{N_0}\\right)\\right] = \\sigma^2 \\left(\\pi^2 \\mathbb{E}\\left[\\frac{1}{N_1}\\right] + (1-\\pi)^2 \\mathbb{E}\\left[\\frac{1}{N_0}\\right]\\right)\n$$\nUsing the specified approximation, $\\mathbb{E}[N_1^{-1}] \\approx (n\\tilde{\\pi})^{-1}$ and $\\mathbb{E}[N_0^{-1}] \\approx (n(1-\\tilde{\\pi}))^{-1}$:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{C}) \\approx \\sigma^2 \\left(\\frac{\\pi^2}{n\\tilde{\\pi}} + \\frac{(1-\\pi)^2}{n(1-\\tilde{\\pi})}\\right) = \\frac{\\sigma^2}{n} \\left(\\frac{\\pi^2}{\\tilde{\\pi}} + \\frac{(1-\\pi)^2}{1-\\tilde{\\pi}}\\right)\n$$\nSince the bias is zero, the MSE is equal to this variance:\n$$\n\\operatorname{MSE}(\\hat{\\mu}_{C}) \\approx \\frac{\\sigma^2}{n} \\left(\\frac{\\pi^2}{\\tilde{\\pi}} + \\frac{(1-\\pi)^2}{1-\\tilde{\\pi}}\\right)\n$$\n\n**3. Numerical Calculation**\n\nWe are given the values: $\\pi = 0.7$, $\\tilde{\\pi} = 0.9$, $\\mu_1 = 5$, $\\mu_0 = 7$, $\\sigma^2 = 1$, and $n = 40$.\n\nFirst, calculate the MSE of the naive estimator $\\hat{\\mu}_N$:\n$$\n\\operatorname{Bias}(\\hat{\\mu}_{N}) = (0.9 - 0.7)(5 - 7) = (0.2)(-2) = -0.4\n$$\n$$\n(\\operatorname{Bias}(\\hat{\\mu}_{N}))^2 = (-0.4)^2 = 0.16\n$$\n$$\n\\operatorname{Var}(\\hat{\\mu}_{N}) = \\frac{1}{40} \\left[ 1 + 0.9(1-0.9)(5-7)^2 \\right] = \\frac{1}{40} \\left[ 1 + 0.9(0.1)(-2)^2 \\right] = \\frac{1}{40} [1 + 0.09(4)] = \\frac{1 + 0.36}{40} = \\frac{1.36}{40} = 0.034\n$$\n$$\n\\operatorname{MSE}(\\hat{\\mu}_{N}) = 0.034 + 0.16 = 0.194\n$$\n\nNext, calculate the MSE of the calibrated estimator $\\hat{\\mu}_C$:\n$$\n\\operatorname{Bias}(\\hat{\\mu}_{C}) = 0 \\implies (\\operatorname{Bias}(\\hat{\\mu}_{C}))^2 = 0\n$$\n$$\n\\operatorname{MSE}(\\hat{\\mu}_{C}) = \\operatorname{Var}(\\hat{\\mu}_{C}) \\approx \\frac{1}{40} \\left( \\frac{0.7^2}{0.9} + \\frac{(1-0.7)^2}{1-0.9} \\right) = \\frac{1}{40} \\left( \\frac{0.49}{0.9} + \\frac{0.3^2}{0.1} \\right)\n$$\n$$\n\\operatorname{MSE}(\\hat{\\mu}_{C}) \\approx \\frac{1}{40} \\left( \\frac{0.49}{0.9} + \\frac{0.09}{0.1} \\right) = \\frac{1}{40} \\left( \\frac{49}{90} + 0.9 \\right) = \\frac{1}{40} \\left( \\frac{49}{90} + \\frac{81}{90} \\right) = \\frac{1}{40} \\left( \\frac{130}{90} \\right) = \\frac{13}{360}\n$$\nNumerically, $\\operatorname{MSE}(\\hat{\\mu}_{C}) \\approx \\frac{13}{360} \\approx 0.036111...$\n\nFinally, we compute the MSE impact, defined as the difference $\\operatorname{MSE}(\\hat{\\mu}_{C}) - \\operatorname{MSE}(\\hat{\\mu}_{N})$:\n$$\n\\text{MSE Impact} \\approx \\frac{13}{360} - 0.194 \\approx 0.036111... - 0.194 = -0.157888...\n$$\nRounding to four significant figures, the MSE impact is $-0.1579$. This negative value indicates that the calibrated estimator has a substantially lower mean squared error than the naive estimator in this scenario, primarily due to the elimination of the large bias term.",
            "answer": "$$\\boxed{-0.1579}$$"
        }
    ]
}