{
    "hands_on_practices": [
        {
            "introduction": "Averaging rates or ratios across different groups is a common task in biostatistics, but it holds a subtle trap. Simply taking the arithmetic mean of individual rates can produce a misleading summary if the groups are of different sizes or were studied under specific constraints. This practice problem demonstrates how the harmonic mean naturally emerges as the correct way to average rates when the numerator of each rate (in this case, the number of deaths) is held constant, a design sometimes used in epidemiological surveillance. By working through this scenario, you will gain a concrete understanding of why the harmonic mean is essential for accurately summarizing certain types of rate data. ",
            "id": "4965964",
            "problem": "A multi-center audit aims to summarize the Case Fatality Rate (CFR) across $3$ hospitals under a fixed-deaths surveillance design. In each hospital, surveillance continued until exactly $5$ deaths had occurred, after which the number of observed cases during that period was recorded. Let the observed numbers of cases be $80$, $200$, and $20$ in hospitals $1$, $2$, and $3$, respectively. For each hospital $i$, the unit-specific CFR is defined as the proportion $r_i = d_i / n_i$, where $d_i$ is the number of deaths and $n_i$ is the number of cases observed during the surveillance period. Here, $d_i = 5$ for all $i$.\n\nStarting only from the definitions of proportion and of statistical means, and without invoking any pre-stated formulas for specialized means, determine which cross-hospital summary of $\\{r_i\\}$ corresponds to the overall CFR implied by aggregating deaths and cases across hospitals under this fixed-deaths design. Then, compute the absolute bias that would be incurred if, instead, the unweighted arithmetic mean of the unit-specific CFRs were used as the cross-hospital summary. Express the bias as a decimal, and round your final answer to four significant figures.",
            "solution": "The problem requires us to first identify the correct method for summarizing unit-specific Case Fatality Rates (CFRs) from a multi-center study with a fixed-deaths design, and then to quantify the bias introduced by using a simple unweighted arithmetic mean instead.\n\nLet the number of hospitals be $k=3$. For each hospital $i \\in \\{1, 2, 3\\}$, we are given the number of deaths, $d_i$, and the number of observed cases, $n_i$. The study design specifies that surveillance in each hospital continued until a fixed number of deaths, $d$, was observed. Therefore, $d_1 = d_2 = d_3 = d = 5$. The observed numbers of cases are $n_1 = 80$, $n_2 = 200$, and $n_3 = 20$.\n\nThe unit-specific CFR for hospital $i$ is defined as the proportion $r_i = \\frac{d_i}{n_i}$.\nThe individual CFRs are:\nFor hospital 1: $r_1 = \\frac{d_1}{n_1} = \\frac{5}{80} = \\frac{1}{16} = 0.0625$.\nFor hospital 2: $r_2 = \\frac{d_2}{n_2} = \\frac{5}{200} = \\frac{1}{40} = 0.025$.\nFor hospital 3: $r_3 = \\frac{d_3}{n_3} = \\frac{5}{20} = \\frac{1}{4} = 0.25$.\n\nThe problem states that the overall CFR, let us denote it $R_{overall}$, is implied by aggregating deaths and cases across all hospitals. This is the definition of a pooled proportion.\n$$R_{overall} = \\frac{\\text{Total Deaths}}{\\text{Total Cases}} = \\frac{\\sum_{i=1}^{k} d_i}{\\sum_{i=1}^{k} n_i}$$\nThis $R_{overall}$ represents the true, correctly weighted summary of the CFR across the entire audited population.\n\nNow, we must determine which type of statistical mean of the individual rates $\\{r_i\\}$ corresponds to this $R_{overall}$. Let's express $R_{overall}$ in terms of the individual rates $r_i$.\n\nA general way to relate the overall proportion to the individual proportions is to recognize it as a weighted arithmetic mean. Since $d_i = n_i r_i$, we can substitute this into the numerator:\n$$R_{overall} = \\frac{\\sum_{i=1}^{k} n_i r_i}{\\sum_{i=1}^{k} n_i}$$\nThis expression is the definition of a weighted arithmetic mean of the rates $r_i$, where the weight for each rate $r_i$ is its denominator, $n_i$ (the number of cases). This shows that the true overall proportion is always a weighted average of the individual proportions, with weights given by the sample size of each subgroup.\n\nHowever, the problem specifies a fixed-deaths design, where $d_i = d$ is a constant for all $i$. This special condition allows for an alternative and more specific interpretation. We can express the number of cases, $n_i$, in terms of the rate $r_i$ and the fixed number of deaths $d$: $n_i = \\frac{d_i}{r_i} = \\frac{d}{r_i}$. Substituting this into the denominator of the expression for $R_{overall}$:\n$$R_{overall} = \\frac{\\sum_{i=1}^{k} d}{\\sum_{i=1}^{k} \\frac{d}{r_i}}$$\nSince $d$ is a constant, we can factor it out from both the numerator and the denominator:\n$$R_{overall} = \\frac{k \\cdot d}{d \\cdot \\sum_{i=1}^{k} \\frac{1}{r_i}} = \\frac{k}{\\sum_{i=1}^{k} \\frac{1}{r_i}}$$\nThis is the definition of the harmonic mean of the $k$ rates $\\{r_1, r_2, \\dots, r_k\\}$. Thus, for the specific case of a fixed-deaths (or more generally, a fixed-numerator) design, the correct overall proportion corresponds to the harmonic mean of the individual proportions.\n\nLet's compute the value of $R_{overall}$ using the aggregated data:\nTotal deaths: $\\sum_{i=1}^{3} d_i = 5 + 5 + 5 = 15$.\nTotal cases: $\\sum_{i=1}^{3} n_i = 80 + 200 + 20 = 300$.\n$$R_{overall} = \\frac{15}{300} = \\frac{1}{20} = 0.05$$\n\nNext, we are asked to consider the summary that would be obtained by using the unweighted arithmetic mean of the unit-specific CFRs. Let's denote this by $R_{arith}$.\n$$R_{arith} = \\frac{1}{k} \\sum_{i=1}^{k} r_i = \\frac{r_1 + r_2 + r_3}{3}$$\nSubstituting the numerical values for the individual rates:\n$$R_{arith} = \\frac{0.0625 + 0.025 + 0.25}{3} = \\frac{0.3375}{3} = 0.1125$$\nThe unweighted arithmetic mean gives a substantially different, and incorrect, estimate of the overall CFR. This is because it gives equal importance to the CFR from hospital 3 ($r_3=0.25$), which was based on only $20$ cases, and the CFR from hospital 2 ($r_2=0.025$), which was based on $200$ cases. The correct summary, $R_{overall}$, appropriately weights the contribution of each hospital by its number of cases.\n\nThe final step is to compute the absolute bias incurred by using the unweighted arithmetic mean. The bias is the difference between the estimated value ($R_{arith}$) and the true value ($R_{overall}$).\n$$\\text{Bias} = R_{arith} - R_{overall}$$\n$$\\text{Bias} = 0.1125 - 0.05 = 0.0625$$\nThe absolute bias is the absolute value of this difference:\n$$\\text{Absolute Bias} = |0.1125 - 0.05| = |0.0625| = 0.0625$$\nThe problem asks for this value to be rounded to four significant figures. The number $0.0625$ has three significant figures ($6$, $2$, $5$). To express it with four significant figures, we append a zero.\n$$\\text{Absolute Bias} \\approx 0.06250$$",
            "answer": "$$\\boxed{0.06250}$$"
        },
        {
            "introduction": "Biological data, such as biomarker concentrations, are often right-skewed, meaning that a simple arithmetic mean can be a poor measure of central tendency. This exercise challenges you to move beyond rote calculation and engage in principled statistical reasoning to select an appropriate summary statistic. You will compare the sample geometric mean and the sample median for a skewed dataset, justifying your choice not just on intuition, but by considering the underlying data-generating model (the Log-Normal distribution) and a formal decision criterion (a loss function based on relative error). ",
            "id": "4965919",
            "problem": "An investigator is summarizing a strictly positive plasma biomarker measured in a cross-sectional study. The sample consists of $n=8$ independent measurements: $x=\\{2,3,4,5,50,60,70,80\\}$. The histogram of $x$ is highly right-skewed. The study team needs a single-number summary to communicate the biomarker’s central tendency in a way that aligns with multiplicative biological variability and relative error, and they are willing to assume that the population distribution of the biomarker is Log-Normal, meaning that if $Y=\\ln X$, then $Y$ is Normally distributed.\n\nTasks:\n- Using only fundamental definitions, compute the sample median of $x$ and the sample geometric mean of $x$ (report approximate numerical values).\n- Under the Log-Normal assumption, and taking a risk perspective in which the loss for reporting a summary $t0$ when observing $X0$ is the squared deviation on the logarithmic scale, $L(t,X)=(\\ln X-\\ln t)^2$, decide which summary is preferred and why.\n\nChoose the single best choice that correctly interprets the computed summaries and justifies the preferred summary under the stated Log-Normal and risk assumptions:\n\nA. The sample median should be preferred, because it minimizes expected squared error on the logarithmic scale and is less biased than the geometric mean for estimating the population median under a Log-Normal model.\n\nB. The sample geometric mean should be preferred, because under a Log-Normal model it targets the same population quantity as the median, minimizes expected squared error on the logarithmic scale, and is the maximum likelihood estimator that is asymptotically more efficient than the sample median when the model is correct.\n\nC. The arithmetic mean should be preferred, because for a Log-Normal population it equals the population median and therefore minimizes squared error on the original scale.\n\nD. The harmonic mean should be preferred, because it is invariant under logarithmic transformation and equals the Log-Normal population median.\n\nE. The sample geometric mean always equals the sample median for any dataset drawn from a Log-Normal distribution, so either is equally valid under the Log-Normal assumption.",
            "solution": "The user has requested a detailed analysis of a biostatistics problem, following a strict validation and solution protocol.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Sample size: $n=8$ independent measurements.\n- Sample data: $x=\\{2, 3, 4, 5, 50, 60, 70, 80\\}$.\n- All measurements are strictly positive.\n- The data's histogram is highly right-skewed.\n- Objective: Find a single-number summary for central tendency that aligns with multiplicative biological variability and relative error.\n- Assumption: The population distribution of the biomarker $X$ is Log-Normal.\n- Definition of Log-Normal: If $Y=\\ln X$, then $Y$ is Normally distributed.\n- Loss Function: For a summary statistic $t0$ and an observation $X0$, the loss is $L(t,X)=(\\ln X-\\ln t)^2$.\n- Tasks:\n    1. Compute the sample median of $x$.\n    2. Compute the sample geometric mean of $x$.\n    3. Decide the preferred summary under the Log-Normal assumption and the given loss function.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Soundness**: The problem is scientifically sound. It uses standard statistical concepts such as the sample median, geometric mean, the Log-Normal distribution, maximum likelihood estimation, and decision theory (loss functions). The Log-Normal distribution is a very common and appropriate model for strictly positive, right-skewed data in biology and medicine, such as biomarker concentrations. The specified loss function, $(\\ln X - \\ln t)^2$, represents the squared error on the logarithmic scale, which is a standard way to formalize the idea of prioritizing relative error over absolute error.\n- **Well-Posed nature**: The problem is well-posed. It provides all necessary data and assumptions to compute the required quantities and to make a principled decision. The question is unambiguous and leads to a single, justifiable conclusion based on statistical theory.\n- **Objectivity**: The problem statement is objective, using precise mathematical and statistical language. It does not contain subjective or opinion-based claims.\n- **Completeness and Consistency**: The problem is self-contained. The data, sample size, distributional assumption, and decision criterion (loss function) are all specified. There are no internal contradictions. The data are positive, consistent with a Log-Normal model.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. It is a well-formulated problem in applied statistics that tests fundamental concepts. I will proceed with the solution.\n\n### Solution Derivation\n\nThe problem requires us to perform two calculations and then use statistical principles to justify the choice of a summary statistic.\n\n**1. Computation of Sample Statistics**\n\nFirst, we compute the sample median and the sample geometric mean for the given data set $x=\\{2, 3, 4, 5, 50, 60, 70, 80\\}$. The sample size is $n=8$. The data are already sorted in ascending order.\n\n*   **Sample Median**: For an even sample size $n$, the median is the arithmetic mean of the two central values, which are the $(n/2)$-th and $(n/2 + 1)$-th observations.\n    Here, $n=8$, so we need the $4$-th and $5$-th values.\n    $x_4 = 5$\n    $x_5 = 50$\n    The sample median, denoted $m$, is:\n    $$m = \\frac{x_4 + x_5}{2} = \\frac{5 + 50}{2} = \\frac{55}{2} = 27.5$$\n\n*   **Sample Geometric Mean**: The geometric mean ($GM$) of a set of $n$ positive numbers $\\{x_1, \\dots, x_n\\}$ is the $n$-th root of their product.\n    $$GM = \\left(\\prod_{i=1}^{n} x_i\\right)^{1/n}$$\n    For the given data:\n    $$GM = (2 \\times 3 \\times 4 \\times 5 \\times 50 \\times 60 \\times 70 \\times 80)^{1/8}$$\n    The product is $2 \\times 3 \\times 4 \\times 5 \\times 50 \\times 60 \\times 70 \\times 80 = 2,016,000,000$.\n    $$GM = (2,016,000,000)^{1/8}$$\n    This calculation is more conveniently performed using logarithms. The logarithm of the geometric mean is the arithmetic mean of the logarithms of the values:\n    $$\\ln(GM) = \\frac{1}{n} \\sum_{i=1}^{n} \\ln(x_i)$$\n    $$\\ln(GM) = \\frac{\\ln(2) + \\ln(3) + \\ln(4) + \\ln(5) + \\ln(50) + \\ln(60) + \\ln(70) + \\ln(80)}{8}$$\n    Using approximate values:\n    $$\\ln(GM) \\approx \\frac{0.693 + 1.099 + 1.386 + 1.609 + 3.912 + 4.094 + 4.248 + 4.382}{8} = \\frac{21.423}{8} \\approx 2.678$$\n    Therefore, the geometric mean is:\n    $$GM = e^{\\ln(GM)} \\approx e^{2.678} \\approx 14.556$$\n\nSo, the sample median is $27.5$ and the sample geometric mean is approximately $14.56$.\n\n**2. Justification of the Preferred Summary**\n\nThe problem asks us to choose a summary statistic $t$ that is optimal under the loss function $L(t,X) = (\\ln X - \\ln t)^2$. In statistical decision theory, the optimal choice is the one that minimizes the expected loss, or risk, $R(t) = E[L(t,X)]$.\n$$R(t) = E[(\\ln X - \\ln t)^2]$$\nLet $Y = \\ln X$. The problem states that $X$ is Log-Normally distributed, so $Y$ is Normally distributed. Let the mean and variance of $Y$ be $\\mu_Y = E[Y]$ and $\\sigma_Y^2 = \\text{Var}(Y)$, respectively. The risk function can be rewritten in terms of $Y$ and a constant $c = \\ln t$:\n$$R(c) = E[(Y - c)^2]$$\nThis is the mean squared error for estimating the random variable $Y$ with a constant $c$. It is a fundamental result in statistics that this expression is minimized when $c = E[Y]$.\nTo show this, we can expand the expression and differentiate:\n$R(c) = E[Y^2 - 2cY + c^2] = E[Y^2] - 2cE[Y] + c^2$.\n$\\frac{dR}{dc} = -2E[Y] + 2c$. Setting the derivative to zero gives $c = E[Y]$.\nSince $c = \\ln t$, the optimal population summary statistic $t$ must satisfy $\\ln t = E[Y] = E[\\ln X]$. This implies the optimal summary is $t_{opt} = e^{E[\\ln X]}$. This quantity is, by definition, the **population geometric mean**.\n\nNow, let's consider the properties of the Log-Normal distribution. If $X \\sim \\text{Log-Normal}(\\mu_Y, \\sigma_Y^2)$, then:\n- The median of the distribution of $X$ is $e^{\\mu_Y}$.\n- The geometric mean of the distribution of $X$ is $e^{E[\\ln X]} = e^{\\mu_Y}$.\nTherefore, for a Log-Normal distribution, the population median and the population geometric mean are identical. The task is to find the best *sample* statistic to estimate this common population parameter, $e^{\\mu_Y}$.\n\nWe are estimating $e^{\\mu_Y}$ from a sample $x_1, \\dots, x_n$. This is equivalent to estimating $e^{\\mu_Y}$ from the log-transformed sample $y_1, \\dots, y_n$ where $y_i = \\ln x_i$ and $Y_i \\sim N(\\mu_Y, \\sigma_Y^2)$.\n- The **sample geometric mean** of the $x_i$ is $GM = (\\prod x_i)^{1/n} = e^{(\\frac{1}{n}\\sum \\ln x_i)} = e^{\\bar{y}}$.\n- The sample mean $\\bar{y}$ is the maximum likelihood estimator (MLE) for the mean $\\mu_Y$ of a normal distribution. By the invariance property of MLEs, $e^{\\bar{y}}$ (the sample geometric mean) is the MLE for $e^{\\mu_Y}$. The sample mean $\\bar{y}$ is also the most efficient unbiased estimator of $\\mu_Y$.\n\n- The **sample median** of $x$ is another estimator for the population median $e^{\\mu_Y}$. However, for a normal distribution (the $y_i$), the sample mean $\\bar{y}$ is a more efficient estimator of the population mean $\\mu_Y$ than the sample median of the $y_i$. The asymptotic variance of the sample mean is $\\sigma_Y^2/n$, while the asymptotic variance of the sample median is $(\\pi/2)(\\sigma_Y^2/n)$. Since $\\pi/2  1$, the sample mean is more efficient. This superior efficiency carries over when estimating $e^{\\mu_Y}$, meaning the sample geometric mean is asymptotically more efficient than the sample median under the Log-Normal assumption.\n\nIn summary, the sample geometric mean is the preferred estimator because it is the MLE for the parameter that minimizes the specified loss function, and it is a more efficient estimator than the sample median when the Log-Normal model holds.\n\n### Option-by-Option Analysis\n\n**A. The sample median should be preferred, because it minimizes expected squared error on the logarithmic scale and is less biased than the geometric mean for estimating the population median under a Log-Normal model.**\n- \"minimizes expected squared error on the logarithmic scale\": This is incorrect. The value $t$ that minimizes the *sample* sum of squared logarithmic errors, $\\sum_i (\\ln x_i - \\ln t)^2$, is the sample geometric mean, not the sample median.\n- \"is less biased\": The sample geometric mean $e^{\\bar{Y}}$ is a biased estimator of $e^{\\mu_Y}$, with $E[e^{\\bar{Y}}] = e^{\\mu_Y + \\sigma_Y^2/(2n)}$. The sample median is also generally biased for small samples from skewed distributions. There is no general theoretical basis to claim the sample median is less biased in this context. The primary justification for an estimator here stems from efficiency and the loss function, not small-sample bias properties.\n**Verdict: Incorrect.**\n\n**B. The sample geometric mean should be preferred, because under a Log-Normal model it targets the same population quantity as the median, minimizes expected squared error on the logarithmic scale, and is the maximum likelihood estimator that is asymptotically more efficient than the sample median when the model is correct.**\n- \"targets the same population quantity as the median\": Correct. For a Log-Normal distribution, the population GM and population median are both $e^{\\mu_Y}$.\n- \"minimizes expected squared error on the logarithmic scale\": Correct. This statement correctly links the sample GM to the principle of minimizing the specified loss function. The sample GM is the empirical risk minimizer.\n- \"is the maximum likelihood estimator\": Correct. The sample GM is the MLE of $e^{\\mu_Y}$.\n- \"asymptotically more efficient than the sample median\": Correct. This follows from the superior efficiency of the sample mean over the sample median for a Normal distribution (on the log scale).\nAll claims in this option are accurate and provide a complete and correct justification.\n**Verdict: Correct.**\n\n**C. The arithmetic mean should be preferred, because for a Log-Normal population it equals the population median and therefore minimizes squared error on the original scale.**\n- \"it equals the population median\": Incorrect. The population arithmetic mean of a Log-Normal distribution is $e^{\\mu_Y + \\sigma_Y^2/2}$, which is strictly greater than the population median $e^{\\mu_Y}$ for $\\sigma_Y^2  0$.\n- \"minimizes squared error on the original scale\": While the arithmetic mean does minimize the expected squared error on the original scale, $E[(X-t)^2]$, the problem explicitly specifies a loss function on the *logarithmic* scale. This justification is irrelevant to the stated goals.\n**Verdict: Incorrect.**\n\n**D. The harmonic mean should be preferred, because it is invariant under logarithmic transformation and equals the Log-Normal population median.**\n- \"invariant under logarithmic transformation\": This claim is mathematically false. The harmonic mean does not possess such an invariance property.\n- \"equals the Log-Normal population median\": This is incorrect. The population harmonic mean has a different value, related to $e^{\\mu_Y - \\sigma_Y^2/2}$.\n**Verdict: Incorrect.**\n\n**E. The sample geometric mean always equals the sample median for any dataset drawn from a Log-Normal distribution, so either is equally valid under the Log-Normal assumption.**\n- \"sample geometric mean always equals the sample median\": This is false. Our own calculations show the sample GM is $\\approx 14.56$ and the sample median is $27.5$. While their population counterparts are equal for a Log-Normal distribution, the sample estimators are almost never equal for a finite sample.\n- \"so either is equally valid\": This conclusion is based on a false premise. Furthermore, as established in the analysis for B, they are not equally valid from an efficiency standpoint; the geometric mean is preferred.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "A correct formula is only useful if it can be reliably computed. This final practice bridges the gap between theoretical understanding and practical implementation, a critical skill for any data analyst. You will discover that the standard formula for the geometric mean can fail spectacularly when applied to data with the extreme dynamic ranges common in fields like genomics, due to numerical underflow or overflow. This hands-on coding challenge guides you to design and validate a numerically stable algorithm, reinforcing the importance of working on the logarithmic scale to create robust biostatistical tools. ",
            "id": "4965927",
            "problem": "A biostatistics analyst needs to compute a multiplicative average of strictly positive measurements that appear in gene expression and rate modeling. Consider a set of $n$ strictly positive values $\\{x_i\\}_{i=1}^n$ with nonnegative weights $\\{w_i\\}_{i=1}^n$ that encode sample importance. The task focuses on designing a numerically stable algorithm for the multiplicative average $G$ that remains accurate under extreme dynamic ranges typical in biological data. The algorithm must start from two foundational bases: (i) for all $x_i  0$, the natural logarithm $\\log(x_i)$ is defined and strictly increasing, and (ii) the natural logarithm maps products to sums, namely $\\log\\left(\\prod_{i=1}^n x_i\\right) = \\sum_{i=1}^n \\log(x_i)$. Build your derivation upon these facts alone.\n\nYour program must implement two computational strategies:\n- A naive multiplicative approach that operates directly on the original scale.\n- A numerically stable approach that operates on the logarithmic scale, uses a centering constant for logarithms to reduce cancellation, and applies compensated summation (Kahan’s method) in all critical accumulations.\n\nThe numerically stable approach must:\n1. Validate inputs to ensure $x_i  0$ and $w_i \\ge 0$ for all $i$.\n2. Use a centering constant $c$ computed from the $\\log(x_i)$ to control numerical cancellation.\n3. Accumulate weighted centered logarithms using compensated summation.\n4. Return the multiplicative average on the original scale by appropriately inverting the logarithmic transformation.\n\nYour program must also implement a plain, non-compensated log-scale method to serve as a comparator for the effectiveness of compensated summation.\n\nDesign a validation suite with extreme datasets that reflect plausible biostatistical scenarios. Use the following five test cases, where each case provides $(\\{x_i\\}, \\{w_i\\})$:\n\n- Case A (moderate fold-changes, varied weights): $x = [\\,0.8,\\, 1.2,\\, 1.1,\\, 0.9,\\, 1.05\\,]$, $w = [\\,10,\\, 20,\\, 15,\\, 5,\\, 50\\,]$.\n- Case B (near underflow regime, uniformly weighted): $x = [\\,10^{-300},\\, 2\\times 10^{-300},\\, 3\\times 10^{-300}\\,]$, $w = [\\,1,\\, 1,\\, 1\\,]$.\n- Case C (near overflow regime, uniformly weighted): $x = [\\,10^{308},\\, 5\\times 10^{307},\\, 9\\times 10^{307}\\,]$, $w = [\\,1,\\, 1,\\, 1\\,]$.\n- Case D (cancellation in centered sums with very large weights): $x = [\\,e^{100},\\, e^{100+10^{-10}},\\, e^{100-10^{-10}}\\,]$, $w = [\\,10^{12},\\, 10^{12},\\, 10^{12}\\,]$.\n- Case E (heteroscedastic weights, wide dynamic range): $x = [\\,10^{-50},\\, 10^{-20},\\, 1,\\, 10^{10}\\,]$, $w = [\\,1,\\, 10^{3},\\, 10^{6},\\, 10^{9}\\,]$.\n\nFor each case, compute four quantities:\n- The naive multiplicative average.\n- The numerically stable average using centered logs and compensated summation.\n- The plain log-scale average using simple summation without compensation.\n- A high-accuracy reference computed using exact rounding summation on the log-scale (use correctly-rounded summation, not pairwise or naive accumulation).\n\nFrom these, evaluate the following boolean criteria per case (produce exactly one boolean per case):\n- Case A: The numerically stable result matches the high-accuracy reference within a relative tolerance of $10^{-12}$.\n- Case B: The naive result numerically underflows to $0$ while the stable result is strictly positive and finite.\n- Case C: The naive result is not finite (either $\\infty$ or $\\mathrm{NaN}$) while the stable result is finite.\n- Case D: The absolute error of the compensated-summation log-scale estimate (relative to the high-accuracy reference in log-space) is less than or equal to the absolute error of the plain summation log-scale estimate.\n- Case E: The numerically stable result matches the high-accuracy reference within a relative tolerance of $10^{-12}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\,\\text{result}_1,\\,\\text{result}_2,\\,\\text{result}_3,\\,\\text{result}_4,\\,\\text{result}_5\\,]$). The output booleans must appear in the order of the cases listed above.\n\nAll quantities are dimensionless; do not attach physical units. Express every computed tolerance comparison in decimal form, not using a percentage sign. The algorithmic derivation must avoid shortcut formulas and must start from the logarithmic product-to-sum principle and the definition of weighted aggregation.",
            "solution": "The user has requested the design and implementation of an algorithm for computing the weighted multiplicative average (weighted geometric mean) of a set of strictly positive values. The design must be numerically stable, especially for data with extreme dynamic ranges, and its derivation must be grounded in fundamental logarithmic properties.\n\n### 1. Problem Formalization and Derivation\n\nLet $\\{x_i\\}_{i=1}^n$ be a set of $n$ strictly positive measurements, $x_i  0$. Let $\\{w_i\\}_{i=1}^n$ be a set of corresponding non-negative weights, $w_i \\ge 0$, with the constraint that the total weight $W = \\sum_{i=1}^n w_i$ is strictly positive. The multiplicative average, or weighted geometric mean $G$, is the value whose logarithm is the weighted arithmetic mean of the logarithms of the individual values. This is a direct consequence of the problem's foundational principle that logarithms map products to sums.\n\nThe weighted arithmetic mean of the logarithms $\\log(x_i)$ is given by:\n$$ \\overline{\\log(x)} = \\frac{\\sum_{i=1}^n w_i \\log(x_i)}{\\sum_{i=1}^n w_i} $$\nBy definition, the logarithm of the geometric mean $G$ is this average:\n$$ \\log(G) = \\frac{\\sum_{i=1}^n w_i \\log(x_i)}{W} $$\nUsing the property of logarithms that $a \\log(b) = \\log(b^a)$, we can rewrite the numerator:\n$$ \\log(G) = \\frac{1}{W} \\sum_{i=1}^n \\log(x_i^{w_i}) $$\nApplying the second foundational principle, $\\sum \\log(a_i) = \\log(\\prod a_i)$:\n$$ \\log(G) = \\frac{1}{W} \\log\\left(\\prod_{i=1}^n x_i^{w_i}\\right) $$\nAgain, using $a \\log(b) = \\log(b^a)$:\n$$ \\log(G) = \\log\\left(\\left(\\prod_{i=1}^n x_i^{w_i}\\right)^{1/W}\\right) $$\nExponentiating both sides yields the familiar formula for the weighted geometric mean:\n$$ G = \\left(\\prod_{i=1}^n x_i^{w_i}\\right)^{1/W} $$\nThis derivation, originating from the log-sum principle, provides the basis for our computational strategies.\n\n### 2. Computational Strategies\n\n#### Strategy 1: Naive Multiplicative Approach\nThis method directly implements the formula $G = (\\prod x_i^{w_i})^{1/W}$.\n1.  Calculate the total weight $W = \\sum_{i=1}^n w_i$.\n2.  Calculate the product $P = \\prod_{i=1}^n x_i^{w_i}$.\n3.  Compute $G_{naive} = P^{1/W}$.\n\nThis approach is numerically unstable. The intermediate product $P$ can easily become smaller than the smallest representable positive number (underflow, resulting in $0.0$) or larger than the largest representable finite number (overflow, resulting in $\\infty$) when the $x_i$ values are very small or very large, respectively. This is a common issue with biological data, which can span many orders of magnitude.\n\n#### Strategy 2: Numerically Stable Logarithmic Approach\nThis method avoids the pitfalls of the naive approach by operating entirely in the logarithmic domain, where products become sums and the dynamic range of the values is compressed. It incorporates two specific techniques for enhanced accuracy: a centering constant and compensated summation.\n\n1.  **Input Validation**: The algorithm first ensures that all $x_i  0$ and $w_i \\ge 0$, and that $W = \\sum w_i  0$.\n2.  **Logarithmic Transformation**: Convert all $x_i$ to the log scale: $y_i = \\log(x_i)$. The formula for $\\log(G)$ is now $\\log(G) = (\\sum w_i y_i) / W$.\n3.  **Centering**: To reduce numerical errors during summation, we introduce a centering constant $c$. By choosing $c$ to be close to the mean of the $y_i$ values (e.g., $c = y_1$), we compute the sum on smaller-magnitude numbers, $d_i = y_i - c$.\n    The expression for $\\log(G)$ becomes:\n    $$ \\log(G) = \\frac{\\sum_{i=1}^n w_i (d_i + c)}{W} = \\frac{\\sum w_i d_i}{W} + \\frac{\\sum w_i c}{W} = \\frac{\\sum w_i d_i}{W} + c $$\n4.  **Compensated Summation**: Standard floating-point summation, `sum = sum + term`, can suffer from loss of precision when `term` is much smaller in magnitude than `sum`. Kahan's compensated summation algorithm mitigates this by tracking a running compensation term for the lost low-order bits. We apply this method to compute both $W = \\sum w_i$ and the centered sum $S_d = \\sum w_i d_i$ with high accuracy.\n5.  **Reconstruction**: After computing the accurate log-space average, $\\log(G)_{stable} = S_d/W + c$, we exponentiate to return to the original scale:\n    $$ G_{stable} = \\exp\\left(\\frac{S_d}{W} + c\\right) $$\n    To further improve stability, we can rewrite this as $G_{stable} = \\exp(c) \\exp(S_d/W)$. Since $c = \\log(x_1)$, this is:\n    $$ G_{stable} = x_1 \\exp\\left(\\frac{S_d}{W}\\right) $$\n    This formulation avoids potentially adding a large number $c$ back to a small ratio $S_d/W$ before exponentiation.\n\n#### Strategy 3: Plain Log-Scale Approach (Comparator)\nThis method serves as a baseline to demonstrate the effectiveness of compensated summation. It operates on the log scale but uses naive summation.\n1.  Compute $y_i = \\log(x_i)$.\n2.  Compute $W_{plain} = \\sum w_i$ and $S_{plain} = \\sum w_i y_i$ using standard, uncompensated summation.\n3.  Compute $G_{plain} = \\exp(S_{plain} / W_{plain})$.\n\n#### Strategy 4: High-Accuracy Reference\nTo provide a ground truth for comparison, we use Python's `math.fsum()`, which implements an algorithm for correctly-rounded summation that minimizes floating-point error.\n1.  Compute $y_i = \\log(x_i)$.\n2.  Compute $W_{ref} = \\text{math.fsum}(w_i)$ and $S_{ref} = \\text{math.fsum}(w_i y_i)$.\n3.  Compute $G_{ref} = \\exp(S_{ref} / W_{ref})$.\n\nBy comparing the results of these four methods on extreme test cases, we can validate the superior stability and accuracy of the proposed stable algorithm.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef kahan_sum(iterable):\n    \"\"\"\n    Computes the sum of a sequence of floating-point numbers using Kahan's\n    compensated summation algorithm to minimize numerical error.\n    \"\"\"\n    total = 0.0\n    compensation = 0.0\n    for value in iterable:\n        y = value - compensation\n        t = total + y\n        compensation = (t - total) - y\n        total = t\n    return total\n\ndef naive_gmean(x, w):\n    \"\"\"\n    Computes the weighted geometric mean using a naive multiplicative approach.\n    This method is prone to underflow and overflow.\n    \"\"\"\n    # handle_zeros_in_weights\n    non_zero_indices = w > 0\n    if not np.any(non_zero_indices):\n        return np.nan, np.nan\n    x_filt, w_filt = x[non_zero_indices], w[non_zero_indices]\n\n    total_weight = np.sum(w_filt)\n    if total_weight == 0:\n        return np.nan, np.nan\n        \n    # Use a try-except block to catch overflow in power calculation\n    try:\n        # np.power can result in overflow, which is caught.\n        # It can also result in underflow, manually checked later.\n        with np.errstate(over='raise'):\n             product = np.prod(np.power(x_filt, w_filt))\n    except (FloatingPointError, OverflowError):\n        return np.inf, np.nan # Overflow detected\n\n    if product == 0: # Underflow check\n        return 0.0, np.nan\n\n    return np.power(product, 1.0 / total_weight), np.nan\n\ndef stable_gmean(x, w):\n    \"\"\"\n    Computes the weighted geometric mean using a numerically stable\n    logarithmic approach with a centering constant and Kahan summation.\n    \"\"\"\n    if not (np.all(x > 0) and np.all(w >= 0)):\n        raise ValueError(\"Inputs are invalid: x must be > 0 and w must be >= 0.\")\n    \n    non_zero_indices = w > 0\n    if not np.any(non_zero_indices):\n        return np.nan, np.nan\n    x_filt, w_filt = x[non_zero_indices], w[non_zero_indices]\n\n    sum_w = kahan_sum(w_filt)\n    if sum_w == 0:\n        return np.nan, np.nan\n\n    log_x = np.log(x_filt)\n    \n    # Centering constant\n    c = log_x[0]\n    \n    # Centered logs\n    d = log_x - c\n    \n    # Weighted centered logs\n    wd = w_filt * d\n    \n    sum_wd = kahan_sum(wd)\n    \n    log_g_mean = sum_wd / sum_w + c\n    result = np.exp(log_g_mean)\n    \n    return result, log_g_mean\n\ndef plain_log_gmean(x, w):\n    \"\"\"\n    Computes the weighted geometric mean on the log-scale using naive summation.\n    \"\"\"\n    non_zero_indices = w > 0\n    if not np.any(non_zero_indices):\n        return np.nan, np.nan\n    x_filt, w_filt = x[non_zero_indices], w[non_zero_indices]\n    \n    sum_w = np.sum(w_filt)\n    if sum_w == 0:\n        return np.nan, np.nan\n    \n    log_x = np.log(x_filt)\n    w_log_x = w_filt * log_x\n    \n    sum_w_log_x = np.sum(w_log_x)\n    \n    log_g_mean = sum_w_log_x / sum_w\n    result = np.exp(log_g_mean)\n    \n    return result, log_g_mean\n\ndef reference_gmean(x, w):\n    \"\"\"\n    Computes the weighted geometric mean using high-precision summation (math.fsum).\n    \"\"\"\n    non_zero_indices = w > 0\n    if not np.any(non_zero_indices):\n        return np.nan, np.nan\n    x_filt, w_filt = x[non_zero_indices], w[non_zero_indices]\n\n    sum_w = math.fsum(w_filt)\n    if sum_w == 0:\n        return np.nan, np.nan\n        \n    log_x = np.log(x_filt)\n    w_log_x = [wi * li for wi, li in zip(w_filt, log_x)]\n    \n    sum_w_log_x = math.fsum(w_log_x)\n    \n    log_g_mean = sum_w_log_x / sum_w\n    result = np.exp(log_g_mean)\n\n    return result, log_g_mean\n\ndef solve():\n    test_cases = [\n        # Case A: Moderate fold-changes, varied weights\n        {'x': np.array([0.8, 1.2, 1.1, 0.9, 1.05], dtype=np.float64),\n         'w': np.array([10, 20, 15, 5, 50], dtype=np.float64)},\n        # Case B: Near underflow regime, uniformly weighted\n        {'x': np.array([1e-300, 2e-300, 3e-300], dtype=np.float64),\n         'w': np.array([1, 1, 1], dtype=np.float64)},\n        # Case C: Near overflow regime, uniformly weighted\n        {'x': np.array([1e308, 5e307, 9e307], dtype=np.float64),\n         'w': np.array([1, 1, 1], dtype=np.float64)},\n        # Case D: Cancellation in centered sums with very large weights\n        {'x': np.array([np.exp(100), np.exp(100 + 1e-10), np.exp(100 - 1e-10)], dtype=np.float64),\n         'w': np.array([1e12, 1e12, 1e12], dtype=np.float64)},\n        # Case E: Heteroscedastic weights, wide dynamic range\n        {'x': np.array([1e-50, 1e-20, 1, 1e10], dtype=np.float64),\n         'w': np.array([1, 1e3, 1e6, 1e9], dtype=np.float64)}\n    ]\n\n    criteria_defs = [\n        # Case A: Stable result matches reference within tolerance\n        lambda r: abs(r['stable'][0] - r['ref'][0]) / abs(r['ref'][0]) = 1e-12,\n        # Case B: Naive underflows, stable does not\n        lambda r: r['naive'][0] == 0.0 and r['stable'][0] > 0 and np.isfinite(r['stable'][0]),\n        # Case C: Naive overflows, stable is finite\n        lambda r: not np.isfinite(r['naive'][0]) and np.isfinite(r['stable'][0]),\n        # Case D: Compensated-sum log-error is = plain-sum log-error\n        lambda r: abs(r['stable'][1] - r['ref'][1]) = abs(r['plain'][1] - r['ref'][1]),\n        # Case E: Stable result matches reference within tolerance\n        lambda r: abs(r['stable'][0] - r['ref'][0]) / abs(r['ref'][0]) = 1e-12\n    ]\n    \n    results = []\n    for i, case in enumerate(test_cases):\n        x, w = case['x'], case['w']\n        \n        # Dictionary to store results for the current case\n        case_results = {\n            'naive': naive_gmean(x, w),\n            'stable': stable_gmean(x, w),\n            'plain': plain_log_gmean(x, w),\n            'ref': reference_gmean(x, w)\n        }\n        \n        # Evaluate criterion for the current case\n        is_criterion_met = criteria_defs[i](case_results)\n        results.append(is_criterion_met)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}