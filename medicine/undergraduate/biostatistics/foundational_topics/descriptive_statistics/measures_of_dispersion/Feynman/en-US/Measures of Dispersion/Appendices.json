{
    "hands_on_practices": [
        {
            "introduction": "The first step in analyzing any dataset is to summarize its key features, including its central tendency and spread. This exercise provides fundamental practice in calculating the three most common measures of dispersion: the range, sample variance, and sample standard deviation. By working with a small, manageable clinical dataset of blood pressure readings, you will translate theoretical formulas into concrete values and learn to interpret what these numbers mean in a real-world health context .",
            "id": "4812293",
            "problem": "A clinical research team collects systolic blood pressure (SBP) values in millimeters of mercury (mmHg) from a simple random sample of $n=6$ adult patients in a hypertension clinic: $[120,126,130,140,142,150]$. Using a foundations-first approach grounded in the core definitions of variability for independent and identically distributed random variables, compute the three standard measures of dispersion for this dataset: the range, the sample variance, and the sample standard deviation. Justify the choice of the finite-sample variance estimator based on the concept of unbiasedness and the constraint introduced by estimating the mean from the same data. Then, interpret each measure in clinical terms, explicitly relating the magnitude of dispersion to variability in SBP across patients in the sample.\n\nExpress your final numerical values for the range, the sample variance, and the sample standard deviation rounded to four significant figures. Use millimeters of mercury (mmHg) when discussing clinical interpretation. The final numerical answer must contain only the three values, without units.",
            "solution": "The problem requires the calculation and interpretation of three measures of dispersion—the range, sample variance, and sample standard deviation—for a given dataset of systolic blood pressure (SBP) values. It also demands a justification for the formula used for the sample variance, grounded in the concept of unbiased estimation. The problem is scientifically and statistically sound, well-posed, and contains all necessary information.\n\nThe given data are a simple random sample of $n=6$ SBP values: $X = \\{120, 126, 130, 140, 142, 150\\}$. The units are millimeters of mercury (mmHg).\n\nFirst, we calculate the sample mean ($\\bar{x}$), which is a prerequisite for calculating the sample variance and standard deviation. The sample mean is the sum of the observations divided by the number of observations.\n$$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i $$\n$$ \\bar{x} = \\frac{120 + 126 + 130 + 140 + 142 + 150}{6} = \\frac{808}{6} = \\frac{404}{3} \\approx 134.67 \\text{ mmHg} $$\n\n**1. Range**\n\nThe range ($R$) is the simplest measure of dispersion, defined as the difference between the maximum and minimum values in the dataset.\n$$ R = x_{\\text{max}} - x_{\\text{min}} $$\nFrom the dataset, the maximum value is $x_{\\text{max}} = 150$ and the minimum value is $x_{\\text{min}} = 120$.\n$$ R = 150 - 120 = 30 $$\nTo four significant figures, the range is $30.00$.\n\nClinical Interpretation: The range of $30$ mmHg indicates that the total spread of SBP values within this sample of six patients is $30$ mmHg. This is the difference between the patient with the highest blood pressure ($150$ mmHg) and the patient with the lowest ($120$ mmHg).\n\n**2. Sample Variance ($s^2$)**\n\nThe problem requires a foundational justification for the choice of the sample variance estimator. The sample variance, denoted $s^2$, is an estimator for the true but unknown population variance, $\\sigma^2$. A critical property of a good estimator is that it be unbiased, meaning its expected value over all possible samples of size $n$ is equal to the parameter it is estimating.\n\nIf the population mean $\\mu$ were known, an unbiased estimator for $\\sigma^2$ would be $\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2$. However, $\\mu$ is unknown and must be estimated from the data using the sample mean, $\\bar{x}$. The sum of squared deviations from the sample mean, $\\sum(x_i - \\bar{x})^2$, is minimized at $\\bar{x}$, meaning it is always less than or equal to the sum of squared deviations from the true mean $\\mu$ (i.e., $\\sum(x_i - \\bar{x})^2 \\leq \\sum(x_i - \\mu)^2$ for any sample).\n\nConsequently, using a denominator of $n$ with the sample mean, as in $\\frac{1}{n}\\sum(x_i - \\bar{x})^2$, would systematically underestimate the true population variance. The expected value of this biased estimator is $E\\left[\\frac{1}{n}\\sum(x_i - \\bar{x})^2\\right] = \\frac{n-1}{n}\\sigma^2$.\n\nTo correct for this underestimation, we use the denominator $n-1$ instead of $n$. This is known as Bessel's correction. The resulting estimator for the sample variance is:\n$$ s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 $$\nThis estimator is unbiased, as its expected value is equal to the population variance: $E[s^2] = \\sigma^2$. The term $n-1$ represents the degrees of freedom. Since one degree of freedom is 'used up' to estimate the mean from the data (as the deviations $x_i - \\bar{x}$ must sum to zero), only $n-1$ of these deviations are free to vary.\n\nNow, we compute the value of $s^2$ for the given data. We first calculate the sum of squared deviations (SS):\n$$ \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\sum_{i=1}^{6} \\left(x_i - \\frac{404}{3}\\right)^2 $$\n$$ = \\left(120 - \\frac{404}{3}\\right)^2 + \\left(126 - \\frac{404}{3}\\right)^2 + \\left(130 - \\frac{404}{3}\\right)^2 + \\left(140 - \\frac{404}{3}\\right)^2 + \\left(142 - \\frac{404}{3}\\right)^2 + \\left(150 - \\frac{404}{3}\\right)^2 $$\n$$ = \\left(-\\frac{44}{3}\\right)^2 + \\left(-\\frac{26}{3}\\right)^2 + \\left(-\\frac{14}{3}\\right)^2 + \\left(\\frac{16}{3}\\right)^2 + \\left(\\frac{22}{3}\\right)^2 + \\left(\\frac{46}{3}\\right)^2 $$\n$$ = \\frac{1936}{9} + \\frac{676}{9} + \\frac{196}{9} + \\frac{256}{9} + \\frac{484}{9} + \\frac{2116}{9} = \\frac{5664}{9} = 629.333... $$\nNow, we divide by the degrees of freedom, $n-1 = 6-1=5$:\n$$ s^2 = \\frac{629.333...}{5} = 125.8666... $$\nRounded to four significant figures, the sample variance is $125.9$.\n\nClinical Interpretation: The sample variance is $125.9$ mmHg$^2$. The units of variance (squared units of measurement) are not directly intuitive in a clinical context. Its primary role is as a mathematically fundamental measure of dispersion that forms the basis for statistical tests and for the standard deviation.\n\n**3. Sample Standard Deviation ($s$)**\n\nThe sample standard deviation ($s$) is the positive square root of the sample variance. It is preferred for interpretation because its units are the same as the original data.\n$$ s = \\sqrt{s^2} $$\n$$ s = \\sqrt{125.8666...} \\approx 11.21903... $$\nRounded to four significant figures, the sample standard deviation is $11.22$.\n\nClinical Interpretation: The sample standard deviation is $11.22$ mmHg. This value represents a typical or average amount by which an individual patient's SBP deviates from the sample mean SBP of $134.7$ mmHg. A patient with an SBP of $145.9$ mmHg ($134.7+11.2$) would be considered one standard deviation above the mean for this sample. A larger standard deviation would signify greater heterogeneity in blood pressure readings among the patients, while a smaller value would indicate the readings are more tightly clustered around the average.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 30.00 & 125.9 & 11.22 \\end{pmatrix} } $$"
        },
        {
            "introduction": "In the previous practice, we calculated the sample variance. A crucial detail in that formula is the use of $n-1$ in the denominator, a concept known as Bessel's correction. This exercise challenges you to explore the theoretical justification for this correction by directly comparing the estimator that divides by $n$ with the one that divides by $n-1$ . By deriving the expected values of both estimators, you will gain a first-principles understanding of bias and why the unbiased sample variance is the standard for inferential statistics.",
            "id": "4812244",
            "problem": "A clinical investigator analyzes changes in a continuous biomarker across $n=5$ independent patients enrolled in a pilot study of a new treatment. The observed changes are $[8, 9, 10, 11, 12]$. Assume the measurements are independent and identically distributed (i.i.d.) draws from a population with finite mean $\\mu$ and finite variance $\\sigma^{2}$. Using only core definitions from probability and statistical inference, and without appealing to any pre-stated estimator formulas, do the following:\n\n1. Compute the sample mean and then compute two estimators of the population variance constructed from deviations around the sample mean: the estimator that divides by $n$ (sometimes called the maximum likelihood estimator under a Normal model) and the estimator that divides by $n-1$ (the unbiased sample variance). Report both numerical values for this dataset.\n\n2. Using first-principles arguments based on the definition of expectation, the variance of the sample mean, and the orthogonal decomposition of total variation around $\\mu$ into variation around the sample mean plus the variation of the sample mean around $\\mu$, derive the expected value under repeated sampling of each of the two estimators in terms of $\\sigma^{2}$ and $n$. Then specialize your expressions to $n=5$.\n\nProvide your final answer in the form of a row matrix containing, in order: the estimator that divides by $n$ for the given data, the estimator that divides by $n-1$ for the given data, the expected value of the estimator that divides by $n$ when $n=5$ expressed in terms of $\\sigma^{2}$, and the expected value of the estimator that divides by $n-1$ when $n=5$ expressed in terms of $\\sigma^{2}$. Do not include units, and do not round any values.",
            "solution": "The problem statement is evaluated to be valid. It is a well-posed, scientifically grounded, and objective problem in elementary statistical inference. It provides all necessary data and assumptions and requests a standard computation and a fundamental theoretical derivation.\n\nThe solution is divided into two parts as requested by the problem.\n\n**Part 1: Numerical Computations**\n\nThe given dataset consists of $n=5$ biomarker change observations: $x_1=8$, $x_2=9$, $x_3=10$, $x_4=11$, and $x_5=12$.\n\nFirst, we compute the sample mean, denoted by $\\bar{x}$:\n$$\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{5} (8 + 9 + 10 + 11 + 12) = \\frac{50}{5} = 10\n$$\n\nNext, we compute the sum of squared deviations of the observations from the sample mean, which is $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$. The deviations $(x_i - \\bar{x})$ are $(8-10)=-2$, $(9-10)=-1$, $(10-10)=0$, $(11-10)=1$, and $(12-10)=2$.\nThe sum of the squares of these deviations is:\n$$\n\\sum_{i=1}^{5} (x_i - \\bar{x})^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 4 + 1 + 0 + 1 + 4 = 10\n$$\n\nWe now calculate the two estimators of the population variance, $\\sigma^2$.\n\nThe first estimator, denoted here as $\\hat{\\sigma}_n^2$, is constructed by dividing the sum of squared deviations by $n$:\n$$\n\\hat{\\sigma}_n^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\frac{1}{5} (10) = 2\n$$\n\nThe second estimator is the unbiased sample variance, denoted by $s^2$, which divides the sum of squared deviations by $n-1$:\n$$\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\frac{1}{5-1} (10) = \\frac{10}{4} = 2.5\n$$\n\n**Part 2: Derivation of Expected Values**\n\nLet $X_1, X_2, \\dots, X_n$ represent the random variables corresponding to the $n$ i.i.d. observations. By assumption, for each $i$, $E[X_i] = \\mu$ and $\\text{Var}(X_i) = E[(X_i - \\mu)^2] = \\sigma^2$. The sample mean is the random variable $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nThe derivation is based on the orthogonal decomposition of the total sum of squares. The fundamental identity is:\n$$\n\\sum_{i=1}^{n} (X_i - \\mu)^2 = \\sum_{i=1}^{n} (X_i - \\bar{X})^2 + n(\\bar{X} - \\mu)^2\n$$\nThis identity is derived by writing $(X_i - \\mu)$ as $(X_i - \\bar{X}) + (\\bar{X} - \\mu)$ and expanding the square. The cross-product term, $2\\sum_{i=1}^{n}(X_i - \\bar{X})(\\bar{X} - \\mu)$, vanishes because $\\sum_{i=1}^{n}(X_i - \\bar{X}) = 0$.\n\nWe take the expectation of both sides of the identity, using the linearity of the expectation operator:\n$$\nE\\left[\\sum_{i=1}^{n} (X_i - \\mu)^2\\right] = E\\left[\\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right] + E\\left[n(\\bar{X} - \\mu)^2\\right]\n$$\n\nWe evaluate the expectation of the terms on the left and far-right.\nThe left-hand side is:\n$$\nE\\left[\\sum_{i=1}^{n} (X_i - \\mu)^2\\right] = \\sum_{i=1}^{n} E[(X_i - \\mu)^2] = \\sum_{i=1}^{n} \\text{Var}(X_i) = \\sum_{i=1}^{n} \\sigma^2 = n\\sigma^2\n$$\n\nFor the second term on the right-hand side, we note that $E[(\\bar{X} - \\mu)^2]$ is the variance of the sample mean, $\\text{Var}(\\bar{X})$. The expectation of the sample mean is $E[\\bar{X}] = E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n}\\sum_{i=1}^n E[X_i] = \\frac{1}{n}(n\\mu) = \\mu$. The variance of the sample mean for i.i.d. variables is:\n$$\n\\text{Var}(\\bar{X}) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_i) = \\frac{1}{n^2} (n\\sigma^2) = \\frac{\\sigma^2}{n}\n$$\nTherefore, the expectation of this term is:\n$$\nE\\left[n(\\bar{X} - \\mu)^2\\right] = n E[(\\bar{X} - \\mu)^2] = n \\text{Var}(\\bar{X}) = n \\left(\\frac{\\sigma^2}{n}\\right) = \\sigma^2\n$$\n\nSubstituting these results back into the main expectation equation gives:\n$$\nn\\sigma^2 = E\\left[\\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right] + \\sigma^2\n$$\nRearranging to solve for the expected sum of squared deviations from the sample mean:\n$$\nE\\left[\\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right] = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2\n$$\n\nNow, we can find the expected value of the two variance estimators, which are the random variables $\\hat{\\sigma}_n^2 = \\frac{1}{n}\\sum_{i=1}^{n} (X_i - \\bar{X})^2$ and $S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\bar{X})^2$.\n\nFor the estimator dividing by $n$:\n$$\nE[\\hat{\\sigma}_n^2] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right] = \\frac{1}{n} E\\left[\\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right] = \\frac{1}{n}(n-1)\\sigma^2 = \\frac{n-1}{n}\\sigma^2\n$$\nThis estimator is biased, as its expectation is not $\\sigma^2$.\n\nFor the estimator dividing by $n-1$:\n$$\nE[S^2] = E\\left[\\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right] = \\frac{1}{n-1}E\\left[\\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right] = \\frac{1}{n-1}(n-1)\\sigma^2 = \\sigma^2\n$$\nThis estimator is unbiased for $\\sigma^2$.\n\nFinally, we specialize these general results for the case $n=5$:\nThe expected value of the estimator dividing by $n=5$ is:\n$$\nE[\\hat{\\sigma}_n^2] = \\frac{5-1}{5}\\sigma^2 = \\frac{4}{5}\\sigma^2\n$$\nThe expected value of the estimator dividing by $n-1=4$ is:\n$$\nE[S^2] = \\sigma^2\n$$\n\nThe final answer will consist of the two numerical values from Part 1 and the two expressions from Part 2, in a specific order.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2 & 2.5 & \\frac{4}{5}\\sigma^2 & \\sigma^2 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Biostatistics often involves comparing two or more groups, such as a treatment group and a control group. When we can assume that the variability within each group is the same, we can 'pool' their data to create a single, more robust estimate of this common variance. This practice guides you through deriving the pooled variance estimator and applying it to data from a comparative clinical study . More importantly, it prompts you to consider the critical assumption of equal variances and the consequences of its violation, a key skill for any careful analyst.",
            "id": "4812171",
            "problem": "In a randomized comparative effectiveness study of two antihypertensive therapies, the primary endpoint is the standardized change in systolic blood pressure from baseline to week $12$, where “standardized” means each change has been scaled by the baseline standard deviation so that the outcome is unitless. Let group $A$ denote patients on therapy $A$ and group $B$ denote patients on therapy $B$. You are given that group $A$ has sample size $n_A = 25$ and sample variance $s_A^2 = 9$, and group $B$ has sample size $n_B = 35$ and sample variance $s_B^2 = 16$.\n\nStarting only from the core definitions of sample variance as the average squared deviation about the sample mean and the additivity of independent sums of squares, derive from first principles an estimator for the common variance when the two groups are assumed to have a single shared population variance. Then compute its value for the data above. Finally, delineate the minimal statistical assumptions under which pooling of information across groups to estimate a common variance is justified in this clinical context, and describe one realistic way those assumptions might fail in practice along with an appropriate analytical remedy.\n\nReport the pooled variance as an exact number (do not round) and omit any units, since the endpoint has been standardized and is dimensionless.",
            "solution": "The problem requires the derivation of an estimator for a common variance from two independent samples, the calculation of its value for the given data, and a discussion of the underlying statistical assumptions.\n\nThe validation of the problem statement finds it to be scientifically grounded, well-posed, and objective. It is a standard problem in statistical inference with clear instructions and sufficient data. Therefore, a full solution is warranted.\n\n**1. Derivation of the Pooled Variance Estimator**\n\nThe derivation begins from the definition of the sample variance for a single group. The unbiased sample variance, $s^2$, for a sample of size $n$ with observations $x_i$ and sample mean $\\bar{x}$, is defined as the sum of squared deviations from the mean divided by the degrees of freedom:\n$$ s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 $$\nThis quantity is an unbiased estimator of the population variance $\\sigma^2$. The numerator, $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$, is known as the sum of squares ($SS$) about the mean, and it is associated with $df = n-1$ degrees of freedom.\n\nFor group A, with sample size $n_A$ and sample variance $s_A^2$, the sum of squares is:\n$$ SS_A = (n_A - 1)s_A^2 $$\nThis sum of squares has $df_A = n_A - 1$ degrees of freedom.\n\nSimilarly, for group B, with sample size $n_B$ and sample variance $s_B^2$, the sum of squares is:\n$$ SS_B = (n_B - 1)s_B^2 $$\nThis sum of squares has $df_B = n_B - 1$ degrees of freedom.\n\nThe problem assumes the existence of a single shared population variance, $\\sigma^2$, common to both groups. To create a single, more precise estimator for this common variance, we combine, or \"pool,\" the information from both independent samples. As stipulated, we invoke the principle of the additivity of independent sums of squares. Since the two groups in a randomized study are independent, we can sum their respective sums of squares to get a total or pooled sum of squares, $SS_p$:\n$$ SS_p = SS_A + SS_B = (n_A - 1)s_A^2 + (n_B - 1)s_B^2 $$\nThe degrees of freedom associated with this pooled sum of squares are also additive due to independence:\n$$ df_p = df_A + df_B = (n_A - 1) + (n_B - 1) = n_A + n_B - 2 $$\nThe pooled variance estimator, denoted $s_p^2$, is defined as this pooled sum of squares divided by the pooled degrees of freedom. This provides a weighted average of the individual sample variances, with the weights being their respective degrees of freedom.\n$$ s_p^2 = \\frac{SS_p}{df_p} = \\frac{(n_A - 1)s_A^2 + (n_B - 1)s_B^2}{(n_A - 1) + (n_B - 1)} = \\frac{(n_A - 1)s_A^2 + (n_B - 1)s_B^2}{n_A + n_B - 2} $$\nThis expression is the estimator for the common variance derived from first principles.\n\n**2. Computation of the Pooled Variance**\n\nThe given data are:\nFor group A: $n_A = 25$, $s_A^2 = 9$.\nFor group B: $n_B = 35$, $s_B^2 = 16$.\n\nSubstituting these values into the derived formula for $s_p^2$:\n$$ s_p^2 = \\frac{(25 - 1) \\cdot 9 + (35 - 1) \\cdot 16}{25 + 35 - 2} $$\n$$ s_p^2 = \\frac{24 \\cdot 9 + 34 \\cdot 16}{58} $$\n$$ s_p^2 = \\frac{216 + 544}{58} $$\n$$ s_p^2 = \\frac{760}{58} $$\nTo simplify the fraction, we can divide both the numerator and the denominator by their greatest common divisor. Both are even, so dividing by $2$:\n$$ s_p^2 = \\frac{380}{29} $$\nSince $29$ is a prime number and $380 = 2^2 \\cdot 5 \\cdot 19$ is not divisible by $29$, this fraction is in its simplest form.\n\n**3. Minimal Statistical Assumptions**\n\nThe justification for pooling variance estimates rests on three primary assumptions:\n\n1.  **Homogeneity of Variance (Homoscedasticity)**: This is the most critical assumption. It posits that the true population variances of the two groups are equal, i.e., $\\sigma_A^2 = \\sigma_B^2 = \\sigma^2$. The pooled variance $s_p^2$ is an estimator of this single common variance $\\sigma^2$. If this assumption is violated, the pooled estimator is biased and can lead to incorrect statistical inferences.\n2.  **Independence of Samples**: The observations from group A must be statistically independent of the observations from group B. In the context of a randomized comparative study, randomization of participants to treatment arms is the standard procedure to ensure this independence holds.\n3.  **Normality**: For the pooled variance estimator to be used in formal inferential procedures such as the two-sample t-test or the construction of confidence intervals, it is assumed that the data within each group are sampled from a normal distribution. That is, the standardized changes in systolic blood pressure for group A follow a $N(\\mu_A, \\sigma^2)$ distribution, and for group B, a $N(\\mu_B, \\sigma^2)$ distribution. While the point estimate $s_p^2$ can be calculated without this assumption, its sampling distribution (a scaled chi-square distribution) and subsequent use in hypothesis testing depend on normality.\n\n**4. Realistic Failure of Assumptions and Analytical Remedy**\n\nA realistic way for these assumptions to fail in this clinical context is the violation of the homogeneity of variance. The two antihypertensive therapies might not only have different mean effects but also different effects on the variability of the response. For instance, therapy A might have a consistent, moderate effect across all patients, resulting in a smaller variance. Therapy B might be a novel drug that is highly effective for a subset of patients (e.g., those with a particular genetic marker) but has little or no effect on others. This differential response would lead to a much larger variance in the outcomes for group B compared to group A, meaning $\\sigma_A^2 \\neq \\sigma_B^2$.\n\nThe appropriate analytical remedy when there is evidence of heteroscedasticity is to use statistical methods that do not rely on the assumption of equal variances. For comparing the means of two independent groups, the standard remedy is **Welch's t-test**. Unlike the pooled-variance t-test, Welch's test does not pool the sample variances. Instead, the test statistic is computed using the individual sample variances:\n$$ t_{Welch} = \\frac{\\bar{x}_A - \\bar{x}_B}{\\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}}} $$\nThe degrees of freedom for this statistic are not simply $n_A + n_B - 2$, but are instead approximated using the **Welch-Satterthwaite equation**. This approach provides a more robust and reliable inference about the difference in means when the population variances cannot be assumed to be equal.",
            "answer": "$$\\boxed{\\frac{380}{29}}$$"
        }
    ]
}