{
    "hands_on_practices": [
        {
            "introduction": "The distinction between data that are Missing At Random (MAR) versus Missing Not At Random (MNAR) is fundamental, but it can be subtle, as the classification of a mechanism depends on the information available to the analyst. This exercise  uses a carefully constructed hypothetical scenario to demonstrate how a missingness pattern can be considered MAR when a key predictive variable is included in the analysis. By showing that the mechanism becomes MNAR if that same variable is overlooked, this practice highlights why correctly specifying the imputation model is critical for avoiding biased results from methods like complete-case analysis.",
            "id": "4928144",
            "problem": "Consider a cohort study with a binary outcome $Y$ indicating the presence ($Y=1$) or absence ($Y=0$) of a condition and a continuous covariate $X$ representing a standardized risk score. Let $X \\sim \\mathcal{N}(0,1)$, and suppose the data collection protocol records the outcome $Y$ only for participants whose covariate exceeds a threshold. Define the missingness indicator $R$ for $Y$ by $R=1$ if $Y$ is observed and $R=0$ if $Y$ is missing. Assume the following data-generating process:\n- The outcome is determined by a threshold rule $Y=\\mathbf{1}\\{X>0\\}$.\n- The missingness mechanism is $R=\\mathbf{1}\\{X>-0.5\\}$.\n\nUsing the standard definitions of Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR), treat the mechanism $R$ as MAR when conditioning on $X$ but MNAR when $X$ is omitted from the conditioning set, and show this formally by computing $P(R=1 \\mid Y=1)$ and $P(R=1 \\mid Y=0)$ under the specified joint distribution.\n\nThen, consider the complete-case estimator of the marginal prevalence $p=\\Pr(Y=1)$, defined as $p_{\\mathrm{cc}}=\\Pr(Y=1 \\mid R=1)$. Derive $p$ and $p_{\\mathrm{cc}}$, and compute the bias $b = p_{\\mathrm{cc}} - p$ as a closed-form analytic expression in terms of the standard normal cumulative distribution function $\\Phi$. Finally, briefly explain the implications of this mechanism for complete-case analysis and for the validity of Multiple Imputation (MI).\n\nYour final answer should be a single closed-form analytic expression for $b$ in terms of $\\Phi$.",
            "solution": "We begin with the fundamental definitions. Let $R$ denote the missingness indicator for $Y$. Missing Completely At Random (MCAR) is defined by $P(R \\mid Y_{\\mathrm{mis}}, Y_{\\mathrm{obs}}, X)=P(R)$, Missing At Random (MAR) is defined by $P(R \\mid Y_{\\mathrm{mis}}, Y_{\\mathrm{obs}}, X)=P(R \\mid Y_{\\mathrm{obs}}, X)$, and Missing Not At Random (MNAR) is the complement where the distribution of $R$ depends on unobserved data even after conditioning on observed quantities.\n\nIn the given setup, $X \\sim \\mathcal{N}(0,1)$, the outcome is $Y=\\mathbf{1}\\{X>0\\}$, and missingness is $R=\\mathbf{1}\\{X>-0.5\\}$. Because $R$ depends only on $X$, we have\n$$\nP(R=1 \\mid X,Y)=P(R=1 \\mid X)=\\mathbf{1}\\{X>-0.5\\},\n$$\nwhich shows that, conditional on $X$, missingness does not depend on unobserved values of $Y$, satisfying Missing At Random (MAR) when conditioning on $X$.\n\nTo show that the mechanism is Missing Not At Random (MNAR) when $X$ is omitted, we compute $P(R=1 \\mid Y)$ marginalizing $X$ but conditioning on $Y$. First observe that $Y=1 \\iff X>0$ and $Y=0 \\iff X \\le 0$ due to the threshold rule. Then:\n- For $Y=1$, we have $X>0$, which implies $X>-0.5$, thus\n$$\nP(R=1 \\mid Y=1)=P(X>-0.5 \\mid X>0)=1.\n$$\n- For $Y=0$, we have $X \\le 0$, and missingness is $R=1$ if $X>-0.5$. Therefore\n$$\nP(R=1 \\mid Y=0)=P(-0.5 < X \\le 0 \\mid X \\le 0)\n=\\frac{P(-0.5 < X \\le 0)}{P(X \\le 0)}\n=\\frac{\\Phi(0)-\\Phi(-0.5)}{\\Phi(0)},\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function. Using the symmetry $\\Phi(-a)=1-\\Phi(a)$ and $\\Phi(0)=\\frac{1}{2}$, this simplifies to\n$$\nP(R=1 \\mid Y=0)=\\frac{\\frac{1}{2}-\\left(1-\\Phi(0.5)\\right)}{\\frac{1}{2}}\n=\\frac{\\Phi(0.5)-\\frac{1}{2}}{\\frac{1}{2}}\n=2\\Phi(0.5)-1.\n$$\nSince $P(R=1 \\mid Y=1)=1$ and $P(R=1 \\mid Y=0)=2\\Phi(0.5)-1<1$, we have $P(R=1 \\mid Y=1) \\neq P(R=1 \\mid Y=0)$, which demonstrates MNAR when $X$ is omitted.\n\nNext, we derive the true marginal prevalence $p=\\Pr(Y=1)$. Because $Y=\\mathbf{1}\\{X>0\\}$ and $X \\sim \\mathcal{N}(0,1)$, we have\n$$\np=\\Pr(Y=1)=\\Pr(X>0)=1-\\Phi(0)=\\frac{1}{2}.\n$$\n\nWe now derive the complete-case prevalence $p_{\\mathrm{cc}}=\\Pr(Y=1 \\mid R=1)$. Since $R=1$ if and only if $X>-0.5$, and $Y=1$ if and only if $X>0$, we have\n$$\np_{\\mathrm{cc}}=\\Pr(Y=1 \\mid R=1)=\\Pr(X>0 \\mid X>-0.5)\n=\\frac{\\Pr(X>0)}{\\Pr(X>-0.5)}=\\frac{\\frac{1}{2}}{\\Phi(0.5)}.\n$$\nTherefore, the bias of the complete-case estimator of the marginal prevalence is\n$$\nb = p_{\\mathrm{cc}} - p \n= \\frac{\\frac{1}{2}}{\\Phi(0.5)} - \\frac{1}{2}\n= \\frac{1}{2}\\left(\\frac{1}{\\Phi(0.5)} - 1\\right).\n$$\n\nImplications for analysis:\n- Complete-case analysis estimates $p_{\\mathrm{cc}}$ instead of $p$ and is biased because the observed data are enriched for larger $X$ values, which deterministically correspond to $Y=1$ under the threshold rule; hence $p_{\\mathrm{cc}}>p$ and the bias $b$ is positive.\n- Multiple Imputation (MI) would yield valid inference under the Missing At Random (MAR) assumption if the imputation model conditions on $X$ (the variable driving missingness). Because $R$ depends only on $X$, including $X$ in the imputation model aligns with MAR and allows consistent recovery of the marginal distribution of $Y$. In contrast, omitting $X$ would mis-specify the missingness as Missing Not At Random (MNAR) relative to the imputation model, generally leading to biased imputations and biased estimates of $p$.\n\nThe required closed-form analytic expression for the bias $b$ is\n$$\nb=\\frac{1}{2}\\left(\\frac{1}{\\Phi(0.5)} - 1\\right).\n$$",
            "answer": "$$\\boxed{\\frac{1}{2}\\left(\\frac{1}{\\Phi(0.5)} - 1\\right)}$$"
        },
        {
            "introduction": "After identifying the likely missing data mechanism and the variables that drive it, the next practical step is to specify the imputation model itself. When multiple candidate models exist, we need a principled way to choose the best one for imputing the missing values. This problem  guides you through using a standard statistical tool, the Bayesian Information Criterion (BIC), to select the most appropriate imputation model by balancing its goodness-of-fit with its complexity.",
            "id": "4611910",
            "problem": "A cohort study aims to estimate associations between a continuous inflammatory biomarker $Y$ measured at baseline and two covariates: age $X$ (in years) and smoking status $Z$ (binary, $Z \\in \\{0,1\\}$). The biomarker $Y$ is missing for some participants due to nonresponse. Investigators judge, based on study design and auxiliary information, that the missingness is Missing At Random (MAR), meaning $P(R=1 \\mid Y, X, Z) = P(R=1 \\mid X, Z)$, where $R$ indicates response for $Y$. To perform Multiple Imputation (MI), they will impute $Y$ under a homoscedastic normal linear model for the conditional distribution $Y \\mid X, Z$.\n\nThey consider three candidate imputation models for $Y \\mid X, Z$, all with independent, identically distributed normal errors and an unknown residual variance:\n\n- Model $M_{1}$: $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$.\n- Model $M_{2}$: $Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z + \\varepsilon$.\n- Model $M_{3}$: $Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z + \\beta_{3} XZ + \\beta_{4} X^{2} + \\varepsilon$.\n\nAll three models include an unknown error variance $\\sigma^{2}$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$, independent across participants. Using only the observed cases for $Y$, with $n_{\\text{obs}} = 420$, the maximum observed-data log-likelihoods for the three models are:\n$$\\ell_{1} = -605.3,\\quad \\ell_{2} = -598.9,\\quad \\ell_{3} = -597.2.$$\n\nChoose an imputation model by a large-sample, likelihood-based selection principle that is consistent under MAR and suitable for selecting among parametric models for the observed-data distribution of $Y \\mid X, Z$. Report the index $m \\in \\{1,2,3\\}$ of the selected model as a single number. No rounding is required for the final answer.",
            "solution": "The problem requires selecting an imputation model using a large-sample, consistent, likelihood-based criterion. The Bayesian Information Criterion (BIC) meets these requirements. The BIC is defined as:\n$$BIC = k \\ln(n) - 2\\ell$$\nwhere $k$ is the number of estimated parameters, $n$ is the number of observations, and $\\ell$ is the maximized log-likelihood. The model with the lowest BIC is preferred.\n\nThe models are fitted on the observed data, so $n = n_{\\text{obs}} = 420$. We calculate the BIC for each candidate model, remembering to count the residual variance $\\sigma^2$ as a parameter.\n\n1.  **Model $M_{1}$**: Has parameters $\\beta_0$, $\\beta_1$, and $\\sigma^2$, so $k_1 = 3$. The log-likelihood is $\\ell_1 = -605.3$.\n    $$BIC_1 = 3 \\ln(420) - 2(-605.3) \\approx 3(6.040) + 1210.6 = 18.12 + 1210.6 = 1228.72$$\n\n2.  **Model $M_{2}$**: Has parameters $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\sigma^2$, so $k_2 = 4$. The log-likelihood is $\\ell_2 = -598.9$.\n    $$BIC_2 = 4 \\ln(420) - 2(-598.9) \\approx 4(6.040) + 1197.8 = 24.16 + 1197.8 = 1221.96$$\n\n3.  **Model $M_{3}$**: Has parameters $\\beta_0$, $\\beta_1$, $\\beta_2$, $\\beta_3$, $\\beta_4$, and $\\sigma^2$, so $k_3 = 6$. The log-likelihood is $\\ell_3 = -597.2$.\n    $$BIC_3 = 6 \\ln(420) - 2(-597.2) \\approx 6(6.040) + 1194.4 = 36.24 + 1194.4 = 1230.64$$\n\nComparing the values, $BIC_2 \\approx 1221.96$ is the minimum. Therefore, we select model $M_2$. The index of the selected model is 2.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Multiple imputation achieves its goal of providing valid inferences by generating several versions of the complete data to reflect the uncertainty caused by missing values. A crucial question in practice is determining just how many imputed datasets, $M$, are sufficient for a reliable analysis. This exercise  provides a quantitative framework for answering this question by connecting the \"fraction of missing information\" to the practical goal of achieving a desired level of statistical efficiency.",
            "id": "4928119",
            "problem": "A prospective cohort study investigates the association between baseline systolic blood pressure and a continuous inflammation biomarker using linear regression. Due to survey logistics, a subset of biomarker measurements is missing. Exploratory analyses and study procedures suggest that the missingness mechanism is Missing At Random (MAR), conditional on observed covariates collected for all participants. The investigators plan to use Multiple Imputation (MI) for handling the missing data and must choose the number of imputations $M$ to ensure that inference is stable relative to the ideal of infinitely many imputations. \n\nThey carry out a pilot MI with a large temporary number of imputations to estimate the fraction of missing information for the regression slope of interest, obtaining an estimate $\\hat{\\lambda} = 0.35$. The team wishes to select the smallest $M$ such that the relative efficiency (defined as the efficiency of MI with finite $M$ relative to MI with infinitely many imputations) is at least $\\tau = 0.99$.\n\nWorking from core definitions of missingness mechanisms (Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR)), the concept of complete case analysis, and the standard large-sample relationship between the fraction of missing information and the relative efficiency under MI, address the following:\n\n1. Explain qualitatively how the overall missingness rate influences the fraction of missing information $\\lambda$ under MCAR and MAR, and how this, in turn, affects the required number of imputations $M$ for stable inference. Contrast this with complete case analysis, focusing on information loss.\n2. Using a well-tested approximation that links relative efficiency under MI to the fraction of missing information, derive an inequality for $M$ in terms of $\\lambda$ and $\\tau$ that guarantees the desired efficiency threshold.\n3. Compute the minimal integer $M$ that satisfies the criterion when $\\hat{\\lambda} = 0.35$ and $\\tau = 0.99$. Report the minimal integer value of $M$. No additional rounding beyond selecting this minimal integer is required.",
            "solution": "The solution addresses the three parts of the problem concerning the number of imputations, $M$.\n\n1. **Qualitative Explanation:** The fraction of missing information, $\\lambda$, quantifies how much statistical information about a parameter is lost due to missing data. Under both MCAR and MAR, $\\lambda$ generally increases with the proportion of missing values. A higher $\\lambda$ indicates greater uncertainty, requiring a larger $M$ to obtain a stable estimate of the variance due to missingness. In contrast, complete case analysis (CCA) discards all incomplete records, leading to a loss of statistical power and, critically, biased results under MAR if the complete cases are not representative of the full sample. MI avoids this by using all available data to make principled inferences.\n\n2. **Derivation of the Inequality:** The relative efficiency, $\\tau$, of an MI analysis with $M$ imputations is approximated by $\\tau \\approx (1 + \\lambda/M)^{-1}$. We require this to be at least a certain threshold.\n$$ \\frac{1}{1 + \\lambda/M} \\ge \\tau $$\nRearranging to solve for $M$:\n$$ 1 \\ge \\tau \\left(1 + \\frac{\\lambda}{M}\\right) $$\n$$ \\frac{1}{\\tau} - 1 \\ge \\frac{\\lambda}{M} $$\n$$ \\frac{1 - \\tau}{\\tau} \\ge \\frac{\\lambda}{M} $$\n$$ M \\ge \\lambda \\left(\\frac{\\tau}{1 - \\tau}\\right) $$\nThis inequality provides the minimum $M$ required for a desired efficiency $\\tau$ given $\\lambda$.\n\n3. **Calculation of Minimal $M$:** Using the given values $\\hat{\\lambda} = 0.35$ and $\\tau = 0.99$, we apply the derived formula:\n$$ M \\ge 0.35 \\left(\\frac{0.99}{1 - 0.99}\\right) = 0.35 \\left(\\frac{0.99}{0.01}\\right) = 0.35 \\times 99 = 34.65 $$\nSince $M$ must be an integer, the smallest number of imputations that satisfies this condition is 35.",
            "answer": "$$\\boxed{35}$$"
        }
    ]
}