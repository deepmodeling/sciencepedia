{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the Kappa statistic, we must first master its calculation from raw data. This foundational exercise guides you through the essential steps of computing Cohen's Kappa for a scenario with three classification categories . By calculating the observed agreement ($P_o$), the expected chance agreement ($P_e$), and the final Kappa value ($\\kappa$), you will build a solid computational base for more advanced applications.",
            "id": "4604233",
            "problem": "Two independent raters each classify the same set of clinical cases into three mutually exclusive categories: positive finding, indeterminate, and negative finding. The joint counts $n_{ij}$ of ratings by Rater A (rows) versus Rater B (columns) for $N$ cases are given by the $3 \\times 3$ matrix\n$$\n\\begin{pmatrix}\n42  6  12 \\\\\n8  15  5 \\\\\n10  7  55\n\\end{pmatrix},\n$$\nwhere the first row and column correspond to the positive category, the second to the indeterminate category, and the third to the negative category.\n\nUsing only foundational definitions from probability and epidemiologic agreement, proceed as follows:\n- Compute the empirical joint proportions $p_{ij}$ from the counts $n_{ij}$ and the total $N$.\n- From these empirical joint proportions, determine the observed agreement proportion $P_{o}$ as the probability that the raters exactly agree on category.\n- Under the chance model that the raters’ assignments are statistically independent while preserving their marginal category distributions, determine the expected agreement proportion $P_{e}$.\n- Define the chance-corrected agreement coefficient (Cohen’s kappa) $\\kappa$ as the proportionate improvement of $P_{o}$ over $P_{e}$, and compute its value for these data.\n\nRound the final value of $\\kappa$ to four significant figures and express it as a decimal. The final answer must be a single real number.",
            "solution": "The user wants to solve the problem, so my first step is to validate the problem statement to ensure it is scientifically grounded, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\n- A set of clinical cases is classified by two independent raters (Rater A, Rater B).\n- There are three mutually exclusive rating categories: positive, indeterminate, and negative.\n- The joint counts of ratings, $n_{ij}$, are given by the $3 \\times 3$ matrix:\n$$\n\\begin{pmatrix}\n42  6  12 \\\\\n8  15  5 \\\\\n10  7  55\n\\end{pmatrix}\n$$\n- The rows correspond to Rater A and the columns correspond to Rater B. The first row/column is 'positive', the second is 'indeterminate', and the third is 'negative'.\n- The task is to compute the empirical joint proportions $p_{ij}$, the observed agreement $P_{o}$, the expected agreement under independence $P_{e}$, and Cohen's kappa coefficient $\\kappa$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem requires the calculation of Cohen's kappa, a standard and widely used statistic for measuring interrater reliability. The concepts of observed proportion of agreement ($P_{o}$) and expected proportion of agreement by chance ($P_{e}$) are foundational in this context. The methodology is firmly established in statistics and epidemiology. The problem is scientifically sound.\n- **Well-Posed**: The problem provides all necessary data (the contingency table of counts) and a clear, sequential set of instructions for the calculation. The definitions provided for $P_{o}$, $P_{e}$, and $\\kappa$ are standard and lead to a unique, stable, and meaningful solution. The problem is well-posed.\n- **Objective**: The problem is stated in precise, quantitative terms. There is no ambiguity, subjectivity, or bias in the description or the requested tasks.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, and objective. I will now proceed with the full solution.\n\nThe solution will follow the steps outlined in the problem statement. First, we denote the given matrix of counts as $N_{ij}$.\n$$\nN_{ij} = \\begin{pmatrix}\nn_{11}  n_{12}  n_{13} \\\\\nn_{21}  n_{22}  n_{23} \\\\\nn_{31}  n_{32}  n_{33}\n\\end{pmatrix} = \\begin{pmatrix}\n42  6  12 \\\\\n8  15  5 \\\\\n10  7  55\n\\end{pmatrix}\n$$\nThe total number of cases, $N$, is the sum of all elements in the matrix.\n$$\nN = \\sum_{i=1}^{3} \\sum_{j=1}^{3} n_{ij} = 42 + 6 + 12 + 8 + 15 + 5 + 10 + 7 + 55 = 160\n$$\n\nThe empirical joint proportions, $p_{ij}$, are calculated by dividing each count $n_{ij}$ by the total number of cases $N$.\n$$\np_{ij} = \\frac{n_{ij}}{N}\n$$\n$$\nP_{ij} = \\begin{pmatrix}\np_{11}  p_{12}  p_{13} \\\\\np_{21}  p_{22}  p_{23} \\\\\np_{31}  p_{32}  p_{33}\n\\end{pmatrix} = \\frac{1}{160} \\begin{pmatrix}\n42  6  12 \\\\\n8  15  5 \\\\\n10  7  55\n\\end{pmatrix} = \\begin{pmatrix}\n0.2625  0.0375  0.075 \\\\\n0.05  0.09375  0.03125 \\\\\n0.0625  0.04375  0.34375\n\\end{pmatrix}\n$$\n\nThe observed agreement proportion, $P_{o}$, is the probability that the two raters classify a case into the same category. This corresponds to the sum of the proportions on the main diagonal of the matrix $P_{ij}$.\n$$\nP_{o} = \\sum_{i=1}^{3} p_{ii} = p_{11} + p_{22} + p_{33} = \\frac{n_{11} + n_{22} + n_{33}}{N}\n$$\n$$\nP_{o} = \\frac{42 + 15 + 55}{160} = \\frac{112}{160} = 0.7\n$$\n\nNext, we determine the expected agreement proportion, $P_{e}$, under a model of chance where the raters' judgments are statistically independent. This requires the marginal proportions for each rater. Let $p_{i.}$ be the proportion of cases Rater A assigned to category $i$, and $p_{.j}$ be the proportion of cases Rater B assigned to category $j$.\n\nThe row totals (Rater A's counts) are:\n$n_{1.} = n_{11} + n_{12} + n_{13} = 42 + 6 + 12 = 60$\n$n_{2.} = n_{21} + n_{22} + n_{23} = 8 + 15 + 5 = 28$\n$n_{3.} = n_{31} + n_{32} + n_{33} = 10 + 7 + 55 = 72$\n\nThe column totals (Rater B's counts) are:\n$n_{.1} = n_{11} + n_{21} + n_{31} = 42 + 8 + 10 = 60$\n$n_{.2} = n_{12} + n_{22} + n_{32} = 6 + 15 + 7 = 28$\n$n_{.3} = n_{13} + n_{23} + n_{33} = 12 + 5 + 55 = 72$\n\nThe marginal proportions are:\nFor Rater A: $p_{1.} = \\frac{60}{160}$, $p_{2.} = \\frac{28}{160}$, $p_{3.} = \\frac{72}{160}$\nFor Rater B: $p_{.1} = \\frac{60}{160}$, $p_{.2} = \\frac{28}{160}$, $p_{.3} = \\frac{72}{160}$\n\nUnder the assumption of independence, the expected proportion of agreement for any category $i$ is the product of the marginal proportions for that category, $p_{i.} \\times p_{.i}$. The total expected agreement proportion $P_{e}$ is the sum of these expected proportions over all categories.\n$$\nP_{e} = \\sum_{i=1}^{3} p_{i.} p_{.i} = p_{1.}p_{.1} + p_{2.}p_{.2} + p_{3.}p_{.3}\n$$\n$$\nP_{e} = \\left(\\frac{60}{160}\\right)\\left(\\frac{60}{160}\\right) + \\left(\\frac{28}{160}\\right)\\left(\\frac{28}{160}\\right) + \\left(\\frac{72}{160}\\right)\\left(\\frac{72}{160}\\right)\n$$\n$$\nP_{e} = \\frac{60^2 + 28^2 + 72^2}{160^2} = \\frac{3600 + 784 + 5184}{25600} = \\frac{9568}{25600} = 0.37375\n$$\n\nFinally, we compute Cohen’s kappa coefficient, $\\kappa$. It is defined as the proportionate improvement of observed agreement over chance agreement. It measures the agreement beyond what would be expected by chance. The formula is:\n$$\n\\kappa = \\frac{P_{o} - P_{e}}{1 - P_{e}}\n$$\nSubstituting the calculated values for $P_{o}$ and $P_{e}$:\n$$\n\\kappa = \\frac{0.7 - 0.37375}{1 - 0.37375} = \\frac{0.32625}{0.62625}\n$$\nPerforming the division:\n$$\n\\kappa \\approx 0.52095808...\n$$\nThe problem requires rounding the final value to four significant figures. The fifth significant figure is $5$, so we round up the fourth digit.\n$$\n\\kappa \\approx 0.5210\n$$\nThe trailing zero is significant and must be included.",
            "answer": "$$\n\\boxed{0.5210}\n$$"
        },
        {
            "introduction": "The Kappa statistic's real power lies in its ability to account for agreement that occurs purely by chance. This practice explores a crucial, and often subtle, feature of Kappa: its sensitivity to the marginal distributions, or the base rates at which each rater assigns categories . By comparing two hypothetical scenarios with identical observed agreement but different marginals, you will uncover why Kappa provides a more nuanced measure of reliability than simple percent agreement.",
            "id": "4604226",
            "problem": "A hospital quality study evaluates agreement between two independent clinical raters who classify a binary finding (present versus absent) for a cohort of $N=200$ patients. Consider two alternative $2 \\times 2$ contingency tables of patient-level ratings that produce the same observed proportion of agreement but different marginal distributions.\n\nTable A (balanced margins): $a=70$, $b=30$, $c=30$, $d=70$, with $N=a+b+c+d=200$.\n\nTable B (mismatched margins): $a=80$, $b=60$, $c=0$, $d=60$, with $N=a+b+c+d=200$.\n\nStarting from the fundamental definitions of the observed agreement proportion and the expected agreement by chance derived from the marginal probabilities, construct the chance-corrected agreement measure commonly used in epidemiology, Cohen’s kappa (the kappa statistic), and compute it for Table A and Table B. Then, using first principles, explain why the identical observed agreement proportion still yields different kappa values across the two tables.\n\nReport, as your final answer, the numerical difference between the two kappa values, computed as the kappa for Table B minus the kappa for Table A. Round your answer to four significant figures. Express the final answer as a decimal (do not use a percentage sign).",
            "solution": "The problem requires the calculation and comparison of Cohen's kappa statistic for two different $2 \\times 2$ contingency tables, Table A and Table B, which share the same observed proportion of agreement but differ in their marginal distributions.\n\nFirst, we establish the general framework for a $2 \\times 2$ table representing the ratings of two independent raters on a binary outcome (e.g., Present/Absent). Let the cells of the table be denoted as follows:\n- $a$: Number of cases where both Rater 1 and Rater 2 classify the finding as 'Present'.\n- $b$: Number of cases where Rater 1 classifies as 'Present' and Rater 2 as 'Absent'.\n- $c$: Number of cases where Rater 1 classifies as 'Absent' and Rater 2 as 'Present'.\n- $d$: Number of cases where both Rater 1 and Rater 2 classify the finding as 'Absent'.\n\nThe total number of patients is $N = a+b+c+d$.\n\nThe Cohen's kappa statistic, $\\kappa$, is defined as the chance-corrected measure of agreement:\n$$ \\kappa = \\frac{P_o - P_e}{1 - P_e} $$\nwhere:\n- $P_o$ is the observed proportion of agreement. This is the sum of cases where both raters agree, divided by the total number of cases.\n$$ P_o = \\frac{a+d}{N} $$\n- $P_e$ is the expected proportion of agreement that would occur by chance. This is calculated based on the marginal probabilities of each rater's classifications, under the assumption of independence between the raters.\n\nThe marginal totals are:\n- Rater 1 classifies 'Present': $R_1_P = a+b$\n- Rater 1 classifies 'Absent': $R_1_A = c+d$\n- Rater 2 classifies 'Present': $C_2_P = a+c$\n- Rater 2 classifies 'Absent': $C_2_A = b+d$\n\nThe corresponding marginal probabilities are:\n- $P(R_1=\\text{Present}) = \\frac{a+b}{N}$\n- $P(R_1=\\text{Absent}) = \\frac{c+d}{N}$\n- $P(R_2=\\text{Present}) = \\frac{a+c}{N}$\n- $P(R_2=\\text{Absent}) = \\frac{b+d}{N}$\n\nThe expected probability of agreement by chance, $P_e$, is the sum of the probabilities of agreeing on 'Present' and agreeing on 'Absent':\n$$ P_e = P(R_1=\\text{Present}) \\times P(R_2=\\text{Present}) + P(R_1=\\text{Absent}) \\times P(R_2=\\text{Absent}) $$\n$$ P_e = \\left(\\frac{a+b}{N}\\right) \\left(\\frac{a+c}{N}\\right) + \\left(\\frac{c+d}{N}\\right) \\left(\\frac{b+d}{N}\\right) $$\n\nNow, we apply these formulas to the two tables provided.\n\n**Analysis of Table A (balanced margins):**\nThe given values are $a=70$, $b=30$, $c=30$, $d=70$, with $N=200$.\n\n1.  **Observed Agreement ($P_{o,A}$):**\n    $$ P_{o,A} = \\frac{a+d}{N} = \\frac{70+70}{200} = \\frac{140}{200} = 0.7 $$\n\n2.  **Expected Agreement ($P_{e,A}$):**\n    First, we calculate the marginal totals for Table A:\n    - Rater 1 'Present': $a+b = 70+30 = 100$\n    - Rater 1 'Absent': $c+d = 30+70 = 100$\n    - Rater 2 'Present': $a+c = 70+30 = 100$\n    - Rater 2 'Absent': $b+d = 30+70 = 100$\n    The marginal probabilities are:\n    - $P(R_1=\\text{Present}) = \\frac{100}{200} = 0.5$\n    - $P(R_1=\\text{Absent}) = \\frac{100}{200} = 0.5$\n    - $P(R_2=\\text{Present}) = \\frac{100}{200} = 0.5$\n    - $P(R_2=\\text{Absent}) = \\frac{100}{200} = 0.5$\n    Now, we compute $P_{e,A}$:\n    $$ P_{e,A} = (0.5 \\times 0.5) + (0.5 \\times 0.5) = 0.25 + 0.25 = 0.5 $$\n\n3.  **Cohen's Kappa ($\\kappa_A$):**\n    $$ \\kappa_A = \\frac{P_{o,A} - P_{e,A}}{1 - P_{e,A}} = \\frac{0.7 - 0.5}{1 - 0.5} = \\frac{0.2}{0.5} = 0.4 $$\n\n**Analysis of Table B (mismatched margins):**\nThe given values are $a=80$, $b=60$, $c=0$, $d=60$, with $N=200$.\n\n1.  **Observed Agreement ($P_{o,B}$):**\n    $$ P_{o,B} = \\frac{a+d}{N} = \\frac{80+60}{200} = \\frac{140}{200} = 0.7 $$\n    As specified in the problem, $P_{o,B} = P_{o,A}$.\n\n2.  **Expected Agreement ($P_{e,B}$):**\n    First, we calculate the marginal totals for Table B:\n    - Rater 1 'Present': $a+b = 80+60 = 140$\n    - Rater 1 'Absent': $c+d = 0+60 = 60$\n    - Rater 2 'Present': $a+c = 80+0 = 80$\n    - Rater 2 'Absent': $b+d = 60+60 = 120$\n    The marginal probabilities are:\n    - $P(R_1=\\text{Present}) = \\frac{140}{200} = 0.7$\n    - $P(R_1=\\text{Absent}) = \\frac{60}{200} = 0.3$\n    - $P(R_2=\\text{Present}) = \\frac{80}{200} = 0.4$\n    - $P(R_2=\\text{Absent}) = \\frac{120}{200} = 0.6$\n    Now, we compute $P_{e,B}$:\n    $$ P_{e,B} = \\left(\\frac{140}{200}\\right)\\left(\\frac{80}{200}\\right) + \\left(\\frac{60}{200}\\right)\\left(\\frac{120}{200}\\right) = (0.7 \\times 0.4) + (0.3 \\times 0.6) = 0.28 + 0.18 = 0.46 $$\n\n3.  **Cohen's Kappa ($\\kappa_B$):**\n    $$ \\kappa_B = \\frac{P_{o,B} - P_{e,B}}{1 - P_{e,B}} = \\frac{0.7 - 0.46}{1 - 0.46} = \\frac{0.24}{0.54} = \\frac{24}{54} = \\frac{4}{9} $$\n    As a decimal, $\\kappa_B \\approx 0.4444...$\n\n**Explanation of the Difference in Kappa Values:**\nThe kappa statistic $\\kappa = (P_o - P_e) / (1 - P_e)$ is a function of both the observed agreement $P_o$ and the chance-expected agreement $P_e$. In this problem, $P_o$ is held constant at $0.7$ for both tables. Therefore, the difference in the kappa values, $\\kappa_A$ and $\\kappa_B$, arises solely from the difference in their respective expected agreement proportions, $P_{e,A}$ and $P_{e,B}$.\n\nThe expected agreement, $P_e$, is calculated from the products of the marginal probabilities. These marginal probabilities reflect the base rates or prevalence with which each rater uses the categories 'Present' and 'Absent'.\n- For Table A, the marginal distributions are balanced: both raters classify $50\\%$ of cases as 'Present' and $50\\%$ as 'Absent'. This leads to an expected agreement $P_{e,A} = 0.5$.\n- For Table B, the marginal distributions are mismatched: Rater 1 classifies $70\\%$ as 'Present', while Rater 2 classifies only $40\\%$ as 'Present'. This imbalance results in a lower expected agreement, $P_{e,B} = 0.46$.\n\nThe core principle is that for a fixed sum of marginal probabilities for a given category across raters, the expected agreement is maximized when the marginals are equal (balanced) and minimized as they become more dissimilar. Here, $P_{e,A} > P_{e,B}$.\n\nBecause $\\kappa$ is a monotonically decreasing function of $P_e$ for a fixed $P_o$ (where $P_o  1$), a lower value of $P_e$ yields a higher value of $\\kappa$. Intuitively, when chance agreement is less likely (i.e., $P_e$ is small due to mismatched marginals), the observed agreement $P_o$ is considered more substantial, leading to a higher chance-corrected agreement $\\kappa$. Conversely, when chance agreement is more likely ($P_e$ is large due to balanced marginals), a larger portion of the observed agreement is discounted as being due to chance, resulting in a lower $\\kappa$. This illustrates the well-known property of the kappa statistic being sensitive to the prevalence of the rated categories as reflected in the marginal totals, often referred to as the \"prevalence paradox\".\n\n**Final Calculation:**\nThe problem asks for the numerical difference $\\kappa_B - \\kappa_A$.\n$$ \\kappa_B - \\kappa_A = \\frac{4}{9} - 0.4 = \\frac{4}{9} - \\frac{4}{10} = \\frac{4}{9} - \\frac{2}{5} $$\n$$ \\kappa_B - \\kappa_A = \\frac{4 \\times 5 - 2 \\times 9}{9 \\times 5} = \\frac{20 - 18}{45} = \\frac{2}{45} $$\nConverting this fraction to a decimal gives:\n$$ \\frac{2}{45} \\approx 0.044444... $$\nRounding to four significant figures, we get $0.04444$.",
            "answer": "$$\n\\boxed{0.04444}\n$$"
        },
        {
            "introduction": "Precision in statistical calculation is paramount, and the formula for expected agreement ($P_e$) can sometimes be a source of confusion. This exercise is designed to sharpen your understanding by contrasting the correct calculation of $P_e$ with a common but flawed alternative . By working through both methods, you will solidify your grasp of why expected agreement must be based on the individual marginal probabilities of each rater, not an average of the two.",
            "id": "4604183",
            "problem": "Two independent raters, a hospital pulmonologist and an infectious disease specialist, classify the severity of community-acquired pneumonia in a cohort of $200$ patients into three mutually exclusive categories: \"None\", \"Mild\", and \"Severe\". The cross-classification of their ratings is summarized by the following counts, where each ordered pair denotes $(\\text{Rater 1 category}, \\text{Rater 2 category})$:\n\n- (\"None\",\"None\"): $50$\n- (\"None\",\"Mild\"): $10$\n- (\"None\",\"Severe\"): $5$\n- (\"Mild\",\"None\"): $20$\n- (\"Mild\",\"Mild\"): $30$\n- (\"Mild\",\"Severe\"): $10$\n- (\"Severe\",\"None\"): $10$\n- (\"Severe\",\"Mild\"): $25$\n- (\"Severe\",\"Severe\"): $40$\n\nA junior analyst reports the \"expected agreement by chance\" $P_{e}$ by first averaging, category-by-category, the two raters’ marginal proportions of \"None\", \"Mild\", and \"Severe\" across the sample, and then summing the squared averages across the three categories. Using the foundational definitions of observed agreement and chance agreement for interrater reliability, compute:\n\n1. The correctly specified Cohen’s kappa statistic (commonly abbreviated as $\\kappa$) using chance agreement derived from the raters’ individual marginal distributions.\n2. The incorrectly specified $\\kappa$ using the junior analyst’s procedure described above.\n\nFinally, provide the single numerical quantity equal to the difference between the incorrect and correct $\\kappa$ estimates (incorrect minus correct). Round your final answer to four significant figures and express it as a decimal. Do not use a percentage sign.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the established statistical theory of interrater reliability, specifically Cohen's kappa statistic. The problem is well-posed, providing a complete and consistent set of data in the form of a contingency table of rater classifications. The total number of observations from the cross-classification table ($50+10+5+20+30+10+10+25+40=200$) matches the stated cohort size of $200$ patients, confirming internal consistency. The definitions for the correct and incorrect calculation procedures are clear and unambiguous. Thus, a unique, meaningful solution can be derived.\n\nLet the three categories be indexed as $i, j \\in \\{1, 2, 3\\}$, corresponding to \"None\", \"Mild\", and \"Severe\", respectively. Rater 1's judgments define the rows of the contingency table, and Rater 2's judgments define the columns. The number of patients classified into category $i$ by Rater 1 and category $j$ by Rater 2 is denoted by $n_{ij}$. The total number of patients is $N=200$.\n\nThe contingency table of counts, $n_{ij}$, is as follows:\n$$\n\\begin{array}{c|ccc|c}\n\\text{Rater 1 \\ Rater 2}  \\text{None (1)}  \\text{Mild (2)}  \\text{Severe (3)}  \\text{Row Total} \\\\\n\\hline\n\\text{None (1)}  50  10  5  65 \\\\\n\\text{Mild (2)}  20  30  10  60 \\\\\n\\text{Severe (3)}  10  25  40  75 \\\\\n\\hline\n\\text{Column Total}  80  65  55  200\n\\end{array}\n$$\n\nThe general formula for Cohen's kappa statistic ($\\kappa$) is:\n$$\n\\kappa = \\frac{P_o - P_e}{1 - P_e}\n$$\nwhere $P_o$ is the observed proportional agreement and $P_e$ is the expected proportional agreement by chance.\n\nFirst, we calculate the observed agreement, $P_o$. This is the proportion of patients on which the two raters agree. Agreement occurs along the main diagonal of the table.\n$$\nP_o = \\frac{\\sum_{i=1}^3 n_{ii}}{N} = \\frac{n_{11} + n_{22} + n_{33}}{N} = \\frac{50 + 30 + 40}{200} = \\frac{120}{200} = 0.6\n$$\n\nNext, we address the two methods for calculating the expected agreement, $P_e$.\n\n**1. Correctly specified Cohen’s kappa ($\\kappa_{correct}$)**\n\nThe correct procedure for calculating the expected agreement, $P_{e,correct}$, uses the product of the marginal probabilities for each category, summed over all categories. Let $p_{i.}$ be the proportion of patients Rater 1 classified into category $i$, and $p_{.j}$ be the proportion of patients Rater 2 classified into category $j$.\n\nThe marginal totals for Rater 1 (rows) are $n_{1.} = 65$, $n_{2.} = 60$, and $n_{3.} = 75$.\nThe corresponding marginal proportions are:\n$p_{1.} = \\frac{65}{200} = 0.325$\n$p_{2.} = \\frac{60}{200} = 0.300$\n$p_{3.} = \\frac{75}{200} = 0.375$\n\nThe marginal totals for Rater 2 (columns) are $n_{.1} = 80$, $n_{.2} = 65$, and $n_{.3} = 55$.\nThe corresponding marginal proportions are:\n$p_{.1} = \\frac{80}{200} = 0.400$\n$p_{.2} = \\frac{65}{200} = 0.325$\n$p_{.3} = \\frac{55}{200} = 0.275$\n\nThe expected agreement by chance is the sum of the products of these marginals for each category:\n$$\nP_{e,correct} = \\sum_{i=1}^3 p_{i.} p_{.i} = (p_{1.} \\times p_{.1}) + (p_{2.} \\times p_{.2}) + (p_{3.} \\times p_{.3})\n$$\n$$\nP_{e,correct} = (0.325 \\times 0.400) + (0.300 \\times 0.325) + (0.375 \\times 0.275)\n$$\n$$\nP_{e,correct} = 0.13000 + 0.09750 + 0.103125 = 0.330625\n$$\nNow, we compute the correctly specified kappa:\n$$\n\\kappa_{correct} = \\frac{P_o - P_{e,correct}}{1 - P_{e,correct}} = \\frac{0.6 - 0.330625}{1 - 0.330625} = \\frac{0.269375}{0.669375} \\approx 0.4024276377\n$$\n\n**2. Incorrectly specified kappa ($\\kappa_{incorrect}$)**\n\nThe junior analyst's procedure calculates an incorrect expected agreement, $P_{e,incorrect}$. First, average the marginal proportions for each category:\nLet $\\bar{p}_i = \\frac{p_{i.} + p_{.i}}{2}$.\n$\\bar{p}_1 = \\frac{0.325 + 0.400}{2} = 0.3625$\n$\\bar{p}_2 = \\frac{0.300 + 0.325}{2} = 0.3125$\n$\\bar{p}_3 = \\frac{0.375 + 0.275}{2} = 0.3250$\n\nNext, sum the squares of these averaged proportions:\n$$\nP_{e,incorrect} = \\sum_{i=1}^3 (\\bar{p}_i)^2 = (0.3625)^2 + (0.3125)^2 + (0.3250)^2\n$$\n$$\nP_{e,incorrect} = 0.13140625 + 0.09765625 + 0.105625 = 0.3346875\n$$\nNow, we compute the incorrectly specified kappa using this $P_e$:\n$$\n\\kappa_{incorrect} = \\frac{P_o - P_{e,incorrect}}{1 - P_{e,incorrect}} = \\frac{0.6 - 0.3346875}{1 - 0.3346875} = \\frac{0.2653125}{0.6653125} \\approx 0.3987787694\n$$\nNote that the analyst's procedure is equivalent to assuming the raters have the same marginal distribution, which is an average of their actual, distinct distributions. This assumption is generally incorrect unless the marginal distributions are identical.\n\n**3. Difference between incorrect and correct $\\kappa$ estimates**\n\nThe final required quantity is the difference $\\Delta\\kappa = \\kappa_{incorrect} - \\kappa_{correct}$.\n$$\n\\Delta\\kappa = 0.3987787694... - 0.4024276377...\n$$\n$$\n\\Delta\\kappa = -0.0036488683...\n$$\nThe problem requires this value to be rounded to four significant figures. The first four significant digits are $3$, $6$, $4$, $8$. The fifth significant digit is $8$, so we round up the fourth digit.\n$$\n\\Delta\\kappa \\approx -0.003649\n$$",
            "answer": "$$\\boxed{-0.003649}$$"
        }
    ]
}