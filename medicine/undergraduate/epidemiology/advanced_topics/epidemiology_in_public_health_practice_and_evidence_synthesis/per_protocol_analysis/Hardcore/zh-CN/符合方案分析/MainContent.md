## 引言
在临床研究中，一个核心问题随之出现：如果患者完美地遵循治疗方案，治疗的真实效果是什么？虽然意向性治疗（ITT）分析评估了真实世界中一项治疗“策略”的效果，但遵循方案（PP）分析则试图揭示这种“纯粹”的生物学或药理学效应。然而，简单地分析依从方案的患者子集充满了方法学上的风险，因为它破坏了随机化的基本原则并引入了严重的偏倚。这个在理想科学问题与简单分析方法的有效性之间的鸿沟，为研究人员带来了重大挑战。

本文为理解和正确实施遵循方案分析提供了一份全面的指南。在第一章“原理与机制”中，我们将使用潜结果框架来形式化定义遵循方案效应，剖析常见的陷阱，如[对撞机](@entry_id:192770)分层偏倚和时变混杂，并介绍为克服这些问题而设计的统计工具，如[逆概率](@entry_id:196307)加权。接下来，“应用与跨学科关联”将把这种方法置于真实世界的背景中，探讨其在监管科学中的关键作用，特别是在[非劣效性试验](@entry_id:176667)中，并展示ITT和PP分析如何为临床医生、政策制定者和科学家提供互补信息。最后，“动手实践”部分将提供实际练习来巩固您的理解，指导您计算调整权重并评估研究结果的稳健性。这些章节将共同使您具备超越朴素比较、进行严谨且有意义的遵循方案分析的能力。

## 原理与机制

在[临床试验分析](@entry_id:172914)中，一个核心挑战是如何评估当参与者未能完全遵循预设方案时的治疗效果。意向性治疗（Intention-to-Treat, ITT）分析通过比较最初随机分配的组别来评估“分配治疗”策略的效果，这一方法因其保留了随机化的完整性而备受推崇。然而，研究者和临床医生通常更关心一个实际问题：“如果患者确实遵循了治疗方案，那么治疗的真实效果是什么？” 这就是“遵循方案分析”（Per-protocol Analysis）试图回答的问题。本章将深入探讨遵循方案分析的核心原理、其固有的方法学挑战，以及为克服这些挑战而发展的先进统计机制。

### 遵循方案 estimand 的形式化定义

为了精确地探讨遵循方案效果，我们必须超越简单的关联性描述，进入因果推断的领域。这需要借助**潜结果（potential outcomes）**框架。对于研究中的每个个体，我们可以设想其在不同治疗路径下的[潜在结果](@entry_id:753644)。假设一项试验比较两种持续的治疗策略：$\bar a$（例如，持续服用活性药物）和 $\bar a'$（例如，持续服用安慰剂）。对于每个个体，都存在一个[潜在结果](@entry_id:753644) $Y^{\bar a}$，表示如果该个体遵循策略 $\bar a$ 将观察到的结局；同样，$Y^{\bar a'}$ 表示如果遵循策略 $\bar a'$ 将观察到的结局。

基于此，**遵循方案因果 estimand（per-protocol causal estimand）** 被形式化定义为在整个人群中，两种完美依从策略下平均潜在结果的差异：

$$
E[Y^{\bar a}] - E[Y^{\bar a'}]
$$

这个定义的关键在于，它是一个关于假设性干预的对比，即“假如人群中的每个人都完美遵循方案 $\bar a$”与“假如每个人都完美遵循方案 $\bar a'$”之间的比较。每个个体的 $Y^{\bar a}$ 和 $Y^{\bar a'}$ 都是预先定义好的，无论他们实际上接受了什么治疗，甚至无论他们被分配到哪个组 。

这与 ITT estimand $E[Y^{Z=1}] - E[Y^{Z=0}]$ 形成了鲜明对比，后者评估的是随机 *分配* 到治疗组（$Z=1$）与[对照组](@entry_id:188599)（$Z=0$）的效果。由于随机化保证了治疗分配与所有基线特征（无论已知或未知）的独立性，ITT 分析能够无偏地估计 *分配* 效果，即使存在大量不依从行为。然而，不依从（例如，治疗组患者停药，安慰剂组患者寻求外部治疗）通常会“稀释”观察到的组间差异，使得 ITT 效应的量级可能小于真实的遵循方案效应。但需要注意的是，这种稀释并非绝对规律。在特定情况下，例如当治疗效果存在异质性，且不依从行为与这种异质性相关时（比如对药物反应最差的患者最有可能停药），ITT 效应的量级甚至可能大于遵循方案效应 。

### 依从性的操作化定义：一个基础步骤

在能够估计遵循方案效果之前，我们必须首先精确地、无[歧义](@entry_id:276744)地定义什么是“依从”。这看似简单，但在实际操作中充满挑战。例如，一个方案要求每日服药，但患者偶尔会早服或晚服几个小时，这是否算作不依从？

一个严谨的方法是将方案定义为一组随时间变化的允许行为集合。例如，考虑一个方案，要求在预定时间点 $s_k$（例如，第1天，第2天，...）服药，并允许一个长度为 $g$ 的对称**宽限窗（grace window）**。这意味着在时间区间 $[s_k - g, s_k + g]$ 内的任何一次服药都被视为对第 $k$ 次计划服药的依从。

为了在分析中处理依从性，我们可以构建一个随时间变化的二进制依从性指示器 $D_t$，其值为1表示截至时间 $t$ 之前个体始终遵循方案，为0则表示已发生方案偏离。这个指示器必须是**单调不增**的，即一旦 $D_t$ 变为0，它在所有后续时间点都将保持为0。此外，一个合理的定义是，只有当一个计划行为的整个宽限窗都已经过去而患者仍未完成该行为时，才判定其发生了方案偏离。

形式上，在时间 $t$，只有那些宽限窗已经完全关闭的计划服药（即 $s_k + g \le t$）才应被评估。因此，截至时间 $t$ 的依从性指示器可以定义为：

$$
D_t = \prod_{k: \, s_k + g \le t} \mathbf{1}\! \left( \exists m \text{ s.t. } r_m \in [s_k - g, s_k + g] \right)
$$

其中 $\{r_m\}$ 是观察到的实际服药时间集合，$\mathbf{1}(\cdot)$ 是[指示函数](@entry_id:186820)。这个公式的含义是：只有当所有在时间 $t$ 之前其宽限窗已关闭的计划服药，都在其各自的宽限窗内被至少一次实际服药所覆盖时，$D_t$ 才为1 。这种严谨的操作化是进行任何有效遵循方案分析的必要前提。

### 朴素分析的陷阱：随机化的失效与偏倚的产生

最直观的遵循方案分析方法可能是：在试验结束后，找出两组中各自“依从”的患者，然后比较他们的结局。例如，比较随机分配到治疗组且坚持服药的患者与分配到安慰剂组且坚持服药（或不服药）的患者。然而，这种“朴素”的分析方法存在根本性的缺陷，因为它破坏了随机化赋予的组间可比性。

依从行为本身是一个发生在随机化之后的变量。决定一个患者是否坚持服药的因素（如他们的总体健康状况、对治疗副作用的耐受性、生活习惯等）很可能也与他们的临床结局直接相关。这意味着，通过只选择依从者进行比较，我们实际上是在比较两个经过“自我选择”的、在预后特征上不再具有可比性的群体。这就引入了**选择偏倚（selection bias）**和**混杂（confounding）**。

#### 对撞机分层偏倚

这种偏倚的产生机制可以通过因果图（Directed Acyclic Graphs, DAGs）清晰地阐释。让我们考虑一个简化的结构，其中 $Z$ 是随机化分组，$D$ 是依从性指示器（$D=1$ 表示依从），$U$ 代表所有可能影响依从性和结局的*未测量*基线预后因素（如潜在的健康状况、个人动机等）。

在这个结构中，随机化 $Z$ 会影响依从性 $D$（例如，活性药物的副作用可能导致依从性降低），即 $Z \rightarrow D$。同时，未测量的预后因素 $U$ 也会影响依从性 $D$（例如，更健康的患者可能更倾向于坚持服药）和结局 $Y$（更健康的患者结局更好），即 $U \rightarrow D$ 和 $U \rightarrow Y$。

这里的关键结构是 $Z \rightarrow D \leftarrow U$。在因果图理论中，$D$ 被称为**对撞机（collider）**，因为它被两个箭头同时指向。随机化的一个重要成果是保证了在整个研究人群中，$Z$ 和 $U$ 在基线时是独立的（$Z \perp U$）。然而，当我们通过选择分析子集来**以 $D$ 为条件（conditioning on $D$）**时——例如，只分析依从者（$D=1$）——我们就在[对撞机](@entry_id:192770)上打开了一个非因果的关联通路。这会导致在依从者子集中，$Z$ 和 $U$ 不再独立（$Z \not\perp U \mid D=1$）。

举例来说，假设活性药物（$Z=1$）有副作用，使得依从变得更困难。那么，在依从者（$D=1$）中，一个接受了活性药物的患者更有可能是一个具有非常强的健康动机（$U$值高）的人，因为他/她克服了副作用的障碍。相比之下，一个依从于安慰剂（$Z=0$）的患者则不需要克服这种障碍。因此，在依从者子集中，治疗组（$Z=1$）的平均健康动机（$U$）会高于安慰剂组（$Z=0$）。由于 $U$ 本身也预测了更好的结局，这就使得两组不再具有可比性，从而产生了偏倚  。这种偏倚区别于传统的混杂，被称为**对撞机分层偏倚（collider-stratification bias）**。

#### 永生时间偏倚

在生存分析（time-to-event analysis）中，一种特殊但常见的偏倚形式是**永生时间偏倚（immortal time bias）**。当依从者的定义要求个体必须生存并保持依从一段时间才能被归类时，这种偏倚就会出现。例如，在一项评估药物预防住院效果的研究中，如果我们将“依从者”定义为“成功完成了前4周服药”的患者，那么这个定义本身就隐含了一个条件：被归为依从者的个体必须至少生存了4周且未住院。

这意味着，在对“依从者”组进行分析时，从研究开始到第4周结束的这段时间，对于该组来说是“永生”的——根据定义，他们不可能在这段时间内发生结局事件（住院）。分析时，这段零事件的“永生时间”被人为地计入了风险时间的总分母中，从而错误地拉低了该组的事件发生率（[风险率](@entry_id:266388)），导致对治疗效果的过度乐观估计。

正确的做法是，必须将依从性视为一个**随时间变化的暴露（time-varying exposure）**。个体的暴露状态在每个时间点都应根据其即时行为来划分。例如，在第1周依从的患者，其在第1周的“人-时（person-time）”应被归为“暴露”状态；如果他在第2周未服药，其在第2周的“人-时”则应被归为“未暴露”状态。这种动态的划分确保了任何时间点的暴露[状态分类](@entry_id:276397)仅依赖于过去的信息，从而避免了永生时间偏倚 。

### 纵向研究中的高级挑战：时变混杂

在许多纵向研究中，情况比上述静态的对撞机偏倚更为复杂。我们常常面临**时变混杂（time-varying confounding）**的挑战。这里的时变混杂物 $L_t$（例如，某项临床指标如血压、或副作用的出现）具有双重特性：

1.  它受**过去治疗**（$\bar A_{t-1}$）的影响。例如，服用降压药会降低后续的血压值。
2.  它又会影响**未来治疗**（$A_t$）的决策和**最终结局**（$Y$）。例如，血压值会影响医生是否调整剂量，同时血压本身也是心血管结局的风险因子。

这种[因果结构](@entry_id:159914)可以表示为 $\bar A_{t-1} \rightarrow L_t \rightarrow A_t$ 以及 $L_t \rightarrow Y$。这种变量 $L_t$ 既是过去治疗效果的**中介（mediator）**，又是未来治疗决策的**混杂物（confounder）**。

这种结构给传统统计方法带来了巨大困难。如果我们为了控制 $A_t$ 和 $Y$ 之间的混杂而在回归模型中调整 $L_t$，我们会错误地阻断了过去治疗 $\bar A_{t-1}$ 通过 $L_t$ 影响结局 $Y$ 的部分因果路径，从而引入偏倚。但如果我们不调整 $L_t$，则 $A_t$ 与 $Y$ 之间的关系又会受到 $L_t$ 的混杂。

为了在这种复杂情况下识别因果效应，我们需要一个更强的假设，即**序贯[可交换性](@entry_id:263314)（sequential exchangeability）**。该假设要求，在任何时间点 $t$，给定所有已观察到的过去历史（包括过去的治疗史 $\bar A_{t-1}$ 和协变量史 $\bar L_t$），当前所接受的治疗 $A_t$ 与所有潜在结局 $Y^{\bar a}$ 都是独立的：

$$
Y^{\bar a} \perp A_t \mid \bar L_t, \bar A_{t-1}
$$

这个假设的直观含义是：在任何时刻，控制了所有可用的过去信息后，决定患者接受何种治疗的因素与他们未来的潜在健康结局之间没有未测量的[共同原因](@entry_id:266381)。这本质上是要求在每个时间点上都实现“条件下的随机化”。这个假设是G方法（如逆概率加权和G-formula）等现代因果推断技术能够处理时变混杂的基础 。在存在时变混杂的情况下，仅仅满足基线时的可交换性是远远不够的。即使是对撞机偏倚，在纵向背景下也会变得更加复杂，偏倚可能通过时变中介变量（$L$）从依从性（$D$）传播回最初的随机分配（$Z$），形成如 $Z \rightarrow A \rightarrow L \rightarrow D \leftarrow U \rightarrow Y$ 这样的偏倚路径 。

### 调整原理：[逆概率](@entry_id:196307)加权法

既然朴素分析会因选择偏倚和混杂而失败，我们需要使用统计方法来调整这些偏倚。**[逆概率](@entry_id:196307)加权（Inverse Probability Weighting, IPW）**是其中一种核心技术。其基本思想是创建一个加权的“伪人群（pseudo-population）”，在这个伪人群中，导致依从性与结局相关的混杂因素被平衡掉，从而模拟出一个所有人都遵循特定方案的理想化试验。

在遵循方案分析中，方案偏离通常被处理为一种**信息性删失（informative censoring）**，因为决定偏离（即被“删失”）的因素也与结局相关。因此，我们应用**[逆概率](@entry_id:196307)删失加权（Inverse Probability of Censoring Weighting, IPCW）**。

对于一个在整个研究期间都保持依从的个体，其权重被设定为“他/她保持依从的概率”的倒数。具体来说，对于个体 $i$，其在时间 $t$ 的权重 $w_i(t)$ 是其截至该时间点，在给定其协变量和治疗史的条件下，持续保持未被删失（即依从）的累积概率的倒数。这个累积概率可以通过[链式法则](@entry_id:190743)分解为一系列[条件概率](@entry_id:151013)的乘积：

$$
w_i(T) = \frac{1}{\prod_{k=0}^{T} P(C_i(k)=0 \mid \bar L_i(k-1), \bar A_i(k-1), C_i(k-1)=0)}
$$

其中 $C_i(k)=1$ 表示在时间 $k$ 发生删失。通过给依从概率低的个体（即那些在相似条件下的大多数人都已偏离方案的个体）赋予更高的权重，IPCW 使得留存下来的依从者能够代表整个目标人群 。

#### 稳定权重

上述“非稳定”权重虽然在理论上能校正偏倚，但在有限样本中可能导致极大的方差，特别是在某些个体保持依从的概率非常低时。为了提高统计稳定性，我们通常使用**稳定权重（stabilized weights）**。稳定权重通过在分子的位置上乘以一个“稳定因子”来实现，该因子是删失概率但条件于一个简化的、通常只包含基线或不随时间变化的协变量集。其公式为：

$$
sw_i(t) = \prod_{k=0}^{t} \frac{P(C_i(k)=0 \mid \text{稳定协变量集})}{P(C_i(k)=0 \mid \bar{L}_i(k-1), \bar{A}_i(k-1))}
$$

例如，假设在时间点 $t=0,1,2$，个体 $i$ 保持依从（$C(k)=0$）。我们拥有模型预测的分子概率（例如，条件于基线协变量）分别为 $0.95, 0.96, 0.98$，以及分母概率（条件于完整的时变历史）分别为 $0.90, 0.80, 0.70$。那么，该个体在 $t=2$ 时的稳定权重为：

$$
w_i = \left(\frac{0.95}{0.90}\right) \left(\frac{0.96}{0.80}\right) \left(\frac{0.98}{0.70}\right) \approx 1.773
$$

这个权重直观地反映了，由于该个体的完整历史（分母）使其比仅基于基线特征（分子）的预测更容易偏离方案，因此需要上调其权重以在伪人群中补偿这种差异 。

#### 关键假设：正定性

IPW方法的有效性依赖于三个核心假设：一致性（Consistency）、序贯可交换性（Sequential Exchangeability，即无未测混杂）和**正定性（Positivity）**。[正定性](@entry_id:149643)假设要求，在协变量历史的任何一种组合下，个体接受每种治疗（或保持依从）的条件概率都必须严格大于零。

$$
P(A_t=a_t \mid \bar L_t, \bar A_{t-1}) > 0 \quad \text{and} \quad P(C_{t+1}=0 \mid \bar L_t, \bar A_t) > 0
$$

这个假设至关重要。如果对于某类人群，他们遵循某个方案的概率为零（**理论违反**），那么在数据中将完全没有关于这类人在该方案下结局的信息，因果效应在该子人群中是不可识别的。如果这个概率虽然不为零但非常接近零（**实际违反**），那么逆概率权重将会变得极大，导致估计结果的方差爆炸，变得极不稳定且高度依赖于所使用的[概率模型](@entry_id:265150)的准确性 。

最后，值得强调的是，所有基于IPW的方法都依赖于对权重模型（即依从/删失概率模型）的**正确设定**。如果模型被错误设定，权重将无法完全校正混杂，导致残留的偏倚 。因此，遵循方案分析虽然能够回答重要的临床问题，但它是一个需要审慎应用高级统计方法并仔细检验其 underlying 假设的复杂过程。