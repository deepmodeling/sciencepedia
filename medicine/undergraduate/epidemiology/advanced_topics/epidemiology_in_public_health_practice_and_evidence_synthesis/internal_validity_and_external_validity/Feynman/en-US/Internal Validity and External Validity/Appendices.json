{
    "hands_on_practices": [
        {
            "introduction": "A core challenge to the internal validity of any observational study is the potential for unmeasured confounding. This exercise introduces a powerful technique known as a 'tipping point' analysis, which allows you to move beyond simply stating that confounding is a limitation. By performing this calculation , you will learn to quantitatively assess how strong an unmeasured confounder's effects would need to be to fully explain away an observed association, providing a crucial tool for judging the robustness of research findings.",
            "id": "4603866",
            "problem": "A prospective cohort study evaluates whether a sustained occupational exposure to volatile solvents is associated with incident chronic kidney disease over a follow-up of several years. After adjustment for all measured covariates, the investigator reports an observed risk ratio (RR) comparing exposed versus unexposed of $RR_{\\text{obs}} = 1.9$. Concern remains about an unmeasured binary confounder $U$ (for example, long-term dehydration habits) that could be associated with both exposure and disease. Let the prevalence of $U$ among the exposed be $p_{1}$ and among the unexposed be $p_{0}$. Suppose that, conditional on $U$, the disease risk multiplies by a factor $RR_{UY}$ for individuals with $U=1$ compared to those with $U=0$, and that this multiplicative effect does not differ by exposure status (no effect modification by the exposure on the risk ratio scale). The investigator judges based on design constraints that the imbalance in $U$ between exposure groups cannot exceed $\\Delta = 0.3$, that is $|p_{1} - p_{0}| \\leq 0.3$. \n\nStarting from the definition of the risk ratio and the structure of confounding, derive from first principles the tipping point analysis that identifies the minimal confounder effect size $RR_{UY}$ such that there exist prevalences $p_{1}$ and $p_{0}$ satisfying $|p_{1} - p_{0}| \\leq 0.3$ for which the unmeasured confounding alone could fully explain the observed association (i.e., reduce the causal risk ratio to the null, so that the observed $RR_{\\text{obs}}$ could be produced purely by confounding). Report the threshold value of $RR_{UY}$ as a single number. No rounding is required, and no units are to be reported.",
            "solution": "The problem asks for a derivation from first principles of the minimal confounder-disease effect size, denoted as $RR_{UY}$, that could fully account for an observed risk ratio, $RR_{\\text{obs}}$, under a specified constraint on the imbalance of the confounder's prevalence.\n\nLet $E$ be the binary variable for exposure, where $E=1$ for the exposed group and $E=0$ for the unexposed group. Let $Y$ be the binary variable for the disease outcome, where $Y=1$ indicates the occurrence of the disease. Let $U$ be the unmeasured binary confounder, with $U=1$ indicating its presence and $U=0$ its absence.\n\nThe observed risk ratio is defined as the ratio of the risk of the disease in the exposed group to the risk in the unexposed group:\n$$RR_{\\text{obs}} = \\frac{P(Y=1|E=1)}{P(Y=1|E=0)}$$\nWe are given that $RR_{\\text{obs}} = 1.9$.\n\nUsing the law of total probability, we can express the risk in each exposure group by stratifying on the confounder $U$:\n$$P(Y=1|E=e) = P(Y=1|E=e, U=1)P(U=1|E=e) + P(Y=1|E=e, U=0)P(U=0|E=e)$$\nfor $e \\in \\{0, 1\\}$.\n\nLet us define the parameters provided in the problem statement:\n1.  The prevalence of the confounder among the exposed: $p_1 = P(U=1|E=1)$.\n2.  The prevalence of the confounder among the unexposed: $p_0 = P(U=1|E=0)$.\n3.  The risk ratio for the association between the confounder $U$ and the outcome $Y$, conditional on exposure status $E$. This is given as $RR_{UY}$ and is assumed not to be modified by $E$:\n    $$RR_{UY} = \\frac{P(Y=1|E=e, U=1)}{P(Y=1|E=e, U=0)} \\quad \\text{for } e \\in \\{0, 1\\}$$\n\nThe condition that the unmeasured confounding by $U$ could fully explain the observed association means that the causal risk ratio of $E$ on $Y$ is null, i.e., $RR_{\\text{causal}} = 1$. The causal risk ratio is the effect of the exposure on the outcome within strata of the confounder. The assumption of no causal effect translates to:\n$$\\frac{P(Y=1|E=1, U=u)}{P(Y=1|E=0, U=u)} = 1 \\quad \\text{for } u \\in \\{0, 1\\}$$\nThis implies $P(Y=1|E=1, U=u) = P(Y=1|E=0, U=u)$ for each stratum $u$.\n\nLet's denote the risk in the unexposed, unconfounded stratum ($E=0, U=0$) as $r_{00} = P(Y=1|E=0, U=0)$. Based on our definitions and the null causal effect assumption, we can express all stratum-specific risks in terms of $r_{00}$ and $RR_{UY}$:\n-   $P(Y=1|E=0, U=0) = r_{00}$\n-   $P(Y=1|E=1, U=0) = P(Y=1|E=0, U=0) = r_{00}$ (due to $RR_{\\text{causal}}=1$)\n-   $P(Y=1|E=0, U=1) = RR_{UY} \\times P(Y=1|E=0, U=0) = RR_{UY} \\cdot r_{00}$\n-   $P(Y=1|E=1, U=1) = RR_{UY} \\times P(Y=1|E=1, U=0) = RR_{UY} \\cdot r_{00}$\n\nNow, we substitute these expressions back into the equations for the observed risks:\nFor the exposed group ($E=1$):\n$$P(Y=1|E=1) = P(Y=1|E=1, U=1)p_1 + P(Y=1|E=1, U=0)(1-p_1)$$\n$$P(Y=1|E=1) = (RR_{UY} \\cdot r_{00})p_1 + r_{00}(1-p_1) = r_{00}[p_1 RR_{UY} - p_1 + 1] = r_{00}[1 + p_1(RR_{UY}-1)]$$\nFor the unexposed group ($E=0$):\n$$P(Y=1|E=0) = P(Y=1|E=0, U=1)p_0 + P(Y=1|E=0, U=0)(1-p_0)$$\n$$P(Y=1|E=0) = (RR_{UY} \\cdot r_{00})p_0 + r_{00}(1-p_0) = r_{00}[p_0 RR_{UY} - p_0 + 1] = r_{00}[1 + p_0(RR_{UY}-1)]$$\n\nThe observed risk ratio $RR_{\\text{obs}}$ is the ratio of these two probabilities. The term $r_{00}$ cancels out:\n$$RR_{\\text{obs}} = \\frac{r_{00}[1 + p_1(RR_{UY}-1)]}{r_{00}[1 + p_0(RR_{UY}-1)]} = \\frac{1 + p_1(RR_{UY}-1)}{1 + p_0(RR_{UY}-1)}$$\nThis equation provides the relationship between the observed association and the parameters of the confounding structure. Our goal is to find the minimal effect size $RR_{UY}$ that satisfies this equation for some valid pair $(p_1, p_0)$. We solve for $RR_{UY}$:\n$$RR_{\\text{obs}}[1 + p_0(RR_{UY}-1)] = 1 + p_1(RR_{UY}-1)$$\n$$RR_{\\text{obs}} + p_0 RR_{\\text{obs}}(RR_{UY}-1) = 1 + p_1(RR_{UY}-1)$$\n$$RR_{\\text{obs}} - 1 = (RR_{UY}-1)[p_1 - p_0 RR_{\\text{obs}}]$$\n$$RR_{UY}-1 = \\frac{RR_{\\text{obs}} - 1}{p_1 - p_0 RR_{\\text{obs}}}$$\n$$RR_{UY} = 1 + \\frac{RR_{\\text{obs}} - 1}{p_1 - p_0 RR_{\\text{obs}}}$$\nThe problem states that the investigator's judgment constrains the imbalance in confounder prevalence: $|p_1 - p_0| \\leq \\Delta$, where $\\Delta = 0.3$. Also, $p_1$ and $p_0$ are prevalences, so $0 \\leq p_1 \\leq 1$ and $0 \\leq p_0 \\leq 1$.\n\nWe seek the minimal confounder effect size, which is commonly interpreted as the minimal value of $RR_{UY} > 1$. The observed $RR_{\\text{obs}} = 1.9$ is greater than $1$, so the numerator $RR_{\\text{obs}} - 1 = 0.9$ is positive. For $RR_{UY}$ to be greater than $1$, the denominator $p_1 - p_0 RR_{\\text{obs}}$ must also be positive.\n\nTo find the minimum value of $RR_{UY} > 1$, we must maximize the positive denominator, $p_1 - p_0 RR_{\\text{obs}}$, subject to the given constraints. We want to maximize the function $f(p_0, p_1) = p_1 - p_0 RR_{\\text{obs}}$ over the feasible region defined by:\n1. $0 \\leq p_0 \\leq 1$\n2. $0 \\leq p_1 \\leq 1$\n3. $|p_1 - p_0| \\leq \\Delta$\n\nSince $RR_{\\text{obs}} = 1.9 > 1$, the function $f(p_0, p_1)$ increases with $p_1$ and decreases with $p_0$. Therefore, to maximize this function, we should choose the largest possible value for $p_1$ and the smallest possible value for $p_0$.\nThe confounding scenario that produces an inflated risk ratio ($RR_{\\text{obs}} > 1$ from $RR_{\\text{causal}}=1$) with a risk factor confounder ($RR_{UY} > 1$) is one where the confounder is more prevalent in the exposed group, i.e., $p_1 > p_0$. The constraint thus becomes $p_1 - p_0 \\leq \\Delta$. To maximize $p_1$ and minimize $p_0$, we should set $p_0$ to its absolute minimum, which is $p_0=0$. The constraint then simplifies to $p_1 \\leq \\Delta$. To maximize $p_1$, we choose its maximum allowed value, $p_1 = \\Delta$.\nThe pair $(p_1, p_0) = (\\Delta, 0)$ satisfies all constraints: $|p_1-p_0| = |\\Delta-0| = \\Delta \\leq \\Delta$, and since $\\Delta=0.3$, both $p_1$ and $p_0$ are within the $[0, 1]$ interval. This pair represents the most extreme confounding imbalance permitted by the design constraints and will thus require the smallest confounder-outcome association $RR_{UY}$ to explain the observation.\n\nSubstituting this worst-case scenario $(p_1, p_0) = (\\Delta, 0)$ into the expression for $RR_{UY}$:\n$$RR_{UY, \\text{min}} = 1 + \\frac{RR_{\\text{obs}} - 1}{\\Delta - 0 \\cdot RR_{\\text{obs}}} = 1 + \\frac{RR_{\\text{obs}} - 1}{\\Delta}$$\nWe are given $RR_{\\text{obs}} = 1.9$ and $\\Delta = 0.3$. Plugging in these values gives the tipping point for $RR_{UY}$:\n$$RR_{UY, \\text{min}} = 1 + \\frac{1.9 - 1}{0.3} = 1 + \\frac{0.9}{0.3} = 1 + 3 = 4$$\nThus, for an unmeasured confounder with a prevalence difference no greater than $0.3$ to fully explain the observed risk ratio of $1.9$, its association with the outcome must be at least a risk ratio of $4.0$. Any smaller effect size for $RR_{UY}$ would be insufficient to account for the entire observed association under the given constraint.",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "Beyond classical confounding, internal validity can be threatened in more subtle ways, such as through the very act of selecting a study population. This practice uses simulation to demonstrate 'collider-stratification bias,' a counter-intuitive phenomenon where conditioning on a common effect of the exposure and outcome creates a spurious association. By working through this simulation , you will gain a concrete understanding of how selection mechanisms can generate bias, a critical lesson for interpreting studies based on specific, non-random subgroups.",
            "id": "4603844",
            "problem": "Consider a source population in which there is a binary exposure $A \\in \\{0,1\\}$ and a binary outcome $Y \\in \\{0,1\\}$. Assume $A$ and $Y$ are generated independently as Bernoulli random variables with probabilities $p_A$ and $p_Y$, respectively, so that $A \\sim \\text{Bernoulli}(p_A)$ and $Y \\sim \\text{Bernoulli}(p_Y)$. Let $S \\in \\{0,1\\}$ be a binary indicator of clinic attendance, where the attendance probability depends on both $A$ and $Y$ through a logistic mechanism,\n$$\n\\Pr(S=1 \\mid A,Y) = \\text{expit}\\left(\\alpha + \\beta_A A + \\beta_Y Y + \\beta_{AY} A Y\\right),\n$$\nwhere $\\text{expit}(z) = \\frac{1}{1+e^{-z}}$.\n\nIn the source population, the true marginal association between $A$ and $Y$ is null because they are generated independently. However, conditioning on $S=1$ (observing only clinic attendees) can induce an association between $A$ and $Y$ via a common effect (collider) mechanism, affecting internal validity (bias in the estimated association) and external validity (generalizability from attendees to the source population).\n\nYour task is to use simulation to determine the direction of bias in the estimated association between $A$ and $Y$ when conditioning on $S=1$. For each parameter set, simulate $N$ individuals, generate $A$ and $Y$ independently as specified, generate $S$ using the attendance model, subset to individuals with $S=1$, and then estimate the association between $A$ and $Y$ in the subset.\n\nDefine the estimated association as the log odds ratio,\n$$\n\\widehat{\\theta} = \\log\\left(\\frac{n_{11} \\cdot n_{00}}{n_{10} \\cdot n_{01}}\\right),\n$$\nwhere $n_{ab}$ is the count of individuals in the $S=1$ subset with $A=a$ and $Y=b$. If any $n_{ab}$ is zero, apply the Haldane–Anscombe continuity correction by adding $0.5$ to each cell before computing the odds ratio. Classify the direction of bias relative to the null ($\\theta=0$) using a tolerance $\\varepsilon = 10^{-3}$:\n- Return $+1$ if $\\widehat{\\theta} > \\varepsilon$ (positive bias),\n- Return $-1$ if $\\widehat{\\theta}  -\\varepsilon$ (negative bias),\n- Return $0$ if $|\\widehat{\\theta}| \\le \\varepsilon$ (negligible bias).\n\nAll probabilities must be treated as decimals in $[0,1]$.\n\nTest suite. Use the following parameter sets, each specified as $(p_A, p_Y, \\alpha, \\beta_A, \\beta_Y, \\beta_{AY}, N)$:\n- Case $1$: $(0.5, 0.5, -2.0, 1.0, 1.0, 0.0, 500000)$,\n- Case $2$: $(0.5, 0.5, -2.0, 1.5, -1.5, 0.0, 500000)$,\n- Case $3$: $(0.6, 0.4, 0.0, 0.0, 0.0, 0.0, 500000)$,\n- Case $4$: $(0.5, 0.5, -1.0, 2.0, 0.0, 0.0, 500000)$,\n- Case $5$: $(0.5, 0.5, -3.0, 1.0, 1.0, 3.0, 500000)$,\n- Case $6$: $(0.5, 0.5, -5.0, 2.0, 2.0, 0.0, 500000)$,\n- Case $7$: $(0.5, 0.5, 5.0, 2.0, -2.0, 0.0, 500000)$.\n\nFinal output format. Your program should produce a single line of output containing the classification results for the $7$ cases as a comma-separated list enclosed in square brackets (for example, $[1,-1,0,1,1,1,0]$). No other output should be produced. Note: Although the question involves epidemiological concepts, your program must focus solely on the specified probabilistic simulation and classification logic.",
            "solution": "The problem requires us to simulate a scenario where two independent variables, an exposure $A$ and an outcome $Y$, influence a third variable, $S$, which represents selection into a study (e.g., clinic attendance). Conditioning on $S=1$ means we are analyzing only the selected subgroup. In the language of directed acyclic graphs (DAGs), if $A \\to S$ and $Y \\to S$, then $S$ is a \"collider\" on the path $A \\to S \\leftarrow Y$. In the source population, $A$ and $Y$ are independent. However, conditioning on the collider $S$ can induce a statistical association between $A$ and $Y$, a phenomenon known as collider bias or selection bias. This bias can distort the estimated association away from the true null association, threatening the internal validity of a study conducted on the selected subgroup. The goal of this task is to quantify the direction of this induced bias through simulation for several parameter configurations. The direction of bias depends on the parameters of the selection model.\n\nThe simulation process unfolds as follows for each parameter set $(p_A, p_Y, \\alpha, \\beta_A, \\beta_Y, \\beta_{AY}, N)$:\n\n1.  **Data Generation for the Source Population**: We simulate a source population of size $N$. For each of the $N$ individuals, we generate the binary exposure $A$ and outcome $Y$ independently from Bernoulli distributions:\n    $$A \\sim \\text{Bernoulli}(p_A)$$\n    $$Y \\sim \\text{Bernoulli}(p_Y)$$\n    By construction, in this complete source population, the true odds ratio between $A$ and $Y$ is $1$, and the true log odds ratio $\\theta$ is $\\log(1) = 0$.\n\n2.  **Selection into the Study**: For each individual, we determine their selection status $S \\in \\{0,1\\}$. The probability of being selected, $\\Pr(S=1)$, is conditional on their values of $A$ and $Y$. This probability is governed by a logistic model:\n    $$\\Pr(S=1 \\mid A, Y) = \\frac{1}{1 + \\exp\\left(-(\\alpha + \\beta_A A + \\beta_Y Y + \\beta_{AY} A Y)\\right)}$$\n    We generate a binary value for $S$ for each individual according to this conditional probability.\n\n3.  **Subsetting the Population**: We restrict our analysis to the sub-population for which $S=1$, mimicking an observational study that only includes, for example, clinic attendees.\n\n4.  **Estimating the Association**: In this selected sub-population ($S=1$), we construct a $2 \\times 2$ contingency table for $A$ and $Y$. The cells of the table contain the counts $n_{ab}$, representing the number of individuals with $A=a$ and $Y=b$.\n    $$\n    \\begin{array}{c|cc|c}\n              Y=1  Y=0  \\text{Total} \\\\\n    \\hline\n    A=1       n_{11}  n_{10}  n_{1\\cdot} \\\\\n    A=0       n_{01}  n_{00}  n_{0\\cdot} \\\\\n    \\hline\n    \\text{Total}  n_{\\cdot 1}  n_{\\cdot 0}  n\n    \\end{array}\n    $$\n    The association between $A$ and $Y$ in this subgroup is estimated using the log odds ratio, $\\widehat{\\theta}$. The odds ratio is calculated as $(n_{11} n_{00}) / (n_{10} n_{01})$.\n    $$\n    \\widehat{\\theta} = \\log\\left(\\frac{n_{11} \\cdot n_{00}}{n_{10} \\cdot n_{01}}\\right)\n    $$\n    If any of the four counts $n_{ab}$ is zero, we apply the Haldane–Anscombe continuity correction, adding $0.5$ to each cell count before calculating the odds ratio. This prevents division by zero and stabilizes the estimate.\n\n5.  **Classifying the Bias**: The true log odds ratio in the source population is $\\theta = 0$. The estimated value $\\widehat{\\theta}$ from the $S=1$ subgroup will, in general, be non-zero due to collider bias. We classify the direction of this bias based on the value of $\\widehat{\\theta}$:\n    - If $\\widehat{\\theta} > \\varepsilon$, where $\\varepsilon=10^{-3}$, we classify the bias as positive ($+1$).\n    - If $\\widehat{\\theta}  -\\varepsilon$, we classify the bias as negative ($-1$).\n    - If $|\\widehat{\\theta}| \\le \\varepsilon$, we classify the bias as negligible ($0$).\n\nThe direction of bias can often be predicted analytically. The induced odds ratio in the stratum $S=1$ is approximately given by:\n$$\n\\text{OR}_{AY|S=1} \\approx \\frac{\\Pr(S=1|A=1,Y=1) \\cdot \\Pr(S=1|A=0,Y=0)}{\\Pr(S=1|A=1,Y=0) \\cdot \\Pr(S=1|A=0,Y=1)}\n$$\nThe bias is positive if this ratio is greater than $1$ and negative if it is less than $1$. In the specific case where the interaction term $\\beta_{AY}=0$, the direction of bias is determined by the product $\\beta_A \\beta_Y$. If $\\beta_A$ and $\\beta_Y$ have the same sign ($\\beta_A \\beta_Y > 0$), the induced association is typically negative ($\\widehat{\\theta}  0$). If they have opposite signs ($\\beta_A \\beta_Y  0$), the induced association is typically positive ($\\widehat{\\theta} > 0$). If either $\\beta_A=0$ or $\\beta_Y=0$, no path is opened and no bias is induced. The simulation will confirm these theoretical expectations and evaluate cases where $\\beta_{AY} \\neq 0$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates collider bias and determines the direction of the resulting association.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of the simulation.\n    np.random.seed(0)\n\n    # Test suite: (p_A, p_Y, alpha, beta_A, beta_Y, beta_AY, N)\n    test_cases = [\n        (0.5, 0.5, -2.0, 1.0, 1.0, 0.0, 500000),\n        (0.5, 0.5, -2.0, 1.5, -1.5, 0.0, 500000),\n        (0.6, 0.4, 0.0, 0.0, 0.0, 0.0, 500000),\n        (0.5, 0.5, -1.0, 2.0, 0.0, 0.0, 500000),\n        (0.5, 0.5, -3.0, 1.0, 1.0, 3.0, 500000),\n        (0.5, 0.5, -5.0, 2.0, 2.0, 0.0, 500000),\n        (0.5, 0.5, 5.0, 2.0, -2.0, 0.0, 500000),\n    ]\n\n    results = []\n    epsilon = 1e-3\n\n    for case in test_cases:\n        p_A, p_Y, alpha, beta_A, beta_Y, beta_AY, N = case\n\n        # Step 1: Generate independent variables A and Y for the source population.\n        A = np.random.binomial(1, p_A, N)\n        Y = np.random.binomial(1, p_Y, N)\n\n        # Step 2: Generate selection status S based on the logistic model.\n        linear_predictor = alpha + beta_A * A + beta_Y * Y + beta_AY * A * Y\n        # Using the numpy implementation of the expit function.\n        selection_prob = 1 / (1 + np.exp(-linear_predictor))\n        S = (np.random.rand(N)  selection_prob).astype(int)\n\n        # Step 3: Subset the population to those with S=1.\n        selected_mask = (S == 1)\n        A_selected = A[selected_mask]\n        Y_selected = Y[selected_mask]\n\n        if len(A_selected) == 0:\n            # If the selected sample is empty, no association can be computed.\n            # This is highly unlikely with the given parameters and N.\n            results.append(0)\n            continue\n\n        # Step 4: Construct the 2x2 contingency table.\n        n11 = np.sum((A_selected == 1)  (Y_selected == 1))\n        n10 = np.sum((A_selected == 1)  (Y_selected == 0))\n        n01 = np.sum((A_selected == 0)  (Y_selected == 1))\n        n00 = np.sum((A_selected == 0)  (Y_selected == 0))\n\n        # Apply Haldane-Anscombe continuity correction if any cell is zero.\n        if n11 == 0 or n10 == 0 or n01 == 0 or n00 == 0:\n            n11 += 0.5\n            n10 += 0.5\n            n01 += 0.5\n            n00 += 0.5\n\n        # Step 5: Estimate the log odds ratio.\n        # The correction ensures n10 and n01 are not zero.\n        theta_hat = np.log((n11 * n00) / (n10 * n01))\n\n        # Step 6: Classify the direction of bias.\n        if theta_hat > epsilon:\n            results.append(1)\n        elif theta_hat  -epsilon:\n            results.append(-1)\n        else:\n            results.append(0)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once we are reasonably confident in a study's internal validity, we face the next critical question: do the results apply elsewhere? This practice explores the core concept of external validity by examining how a causal effect, estimated in one population, can be 'transported' to another. You will use the fundamental definitions of risk ratio and risk difference to see how the absolute impact of an intervention can change in a new population, even when the relative effect is stable , a crucial insight for applying evidence in the real world.",
            "id": "4603832",
            "problem": "A randomized controlled trial (RCT) with high internal validity estimates a risk ratio $RR$ for a preventive intervention. Let $p_{0}^{\\text{study}}$ denote the baseline (control) risk in the study population, and let $p_{0}^{\\text{target}}$ denote the baseline (control) risk in a distinct target population to which results may be transported. Assume the following:\n- The study’s internally valid estimate of the risk ratio is $RR = 0.7$.\n- The baseline risk in the study population is $p_{0}^{\\text{study}} = 0.20$.\n- The baseline risk in the target population is $p_{0}^{\\text{target}} = 0.10$.\n- The risk ratio is constant across these populations (that is, there is no effect modification on the risk ratio scale).\n\nStarting from the core definitions of risk ratio and risk difference, use the constant risk ratio assumption to derive the target population’s treated risk $p_{1}^{\\text{target}}$ and then compute the target risk difference $RD^{\\text{target}} = p_{1}^{\\text{target}} - p_{0}^{\\text{target}}$. Express the final risk difference as a decimal. No rounding is necessary.\n\nFinally, briefly explain, using first principles, how the constancy of the risk ratio affects external validity and why the risk difference can legitimately change across populations even when the risk ratio is constant.",
            "solution": "### Solution Derivation\nThe core definitions for risk, risk ratio, and risk difference are the starting points. Let $p_1$ denote the risk of the outcome in a group receiving an intervention (treated group), and let $p_0$ denote the risk of the outcome in a group not receiving the intervention (control or baseline group).\n\nThe risk ratio ($RR$) is defined as the ratio of the risk in the treated group to the risk in the control group:\n$$RR = \\frac{p_1}{p_0}$$\nThe risk difference ($RD$) is defined as the absolute difference in risk between the two groups:\n$$RD = p_1 - p_0$$\n\nThe problem states that a randomized controlled trial (RCT) produced an internally valid estimate of the risk ratio, $RR = 0.7$. The study also measured the baseline risk in its control group as $p_{0}^{\\text{study}} = 0.20$. From these values, we can determine the risk in the treated group within the study population, $p_{1}^{\\text{study}}$:\n$$p_{1}^{\\text{study}} = RR \\times p_{0}^{\\text{study}} = 0.7 \\times 0.20 = 0.14$$\nThe risk difference within the study was therefore:\n$$RD^{\\text{study}} = p_{1}^{\\text{study}} - p_{0}^{\\text{study}} = 0.14 - 0.20 = -0.06$$\n\nThe core of the problem lies in \"transporting\" this result to a target population with a different baseline risk, $p_{0}^{\\text{target}} = 0.10$. The critical assumption provided is that the risk ratio is constant across these populations. This means we assume $RR^{\\text{target}} = RR^{\\text{study}} = 0.7$.\n\nUsing this constant risk ratio, we can derive the expected risk in the treated group of the target population, $p_{1}^{\\text{target}}$:\n$$p_{1}^{\\text{target}} = RR^{\\text{target}} \\times p_{0}^{\\text{target}}$$\nSubstituting the known values:\n$$p_{1}^{\\text{target}} = 0.7 \\times 0.10 = 0.07$$\n\nNow, we can compute the risk difference in the target population, $RD^{\\text{target}}$, as requested:\n$$RD^{\\text{target}} = p_{1}^{\\text{target}} - p_{0}^{\\text{target}}$$\n$$RD^{\\text{target}} = 0.07 - 0.10 = -0.03$$\n\nThe final risk difference in the target population is $-0.03$.\n\n### Conceptual Explanation\nThe problem also requests an explanation of how the constancy of the risk ratio affects external validity and why the risk difference changes.\n\n**External Validity and the Constant Risk Ratio Assumption**: External validity concerns the extent to which the results of a study can be generalized or transported to other populations, settings, or times. In epidemiology, effect measures can be relative (like the risk ratio or odds ratio) or absolute (like the risk difference). The assumption that a specific effect measure is constant, or transportable, across populations is a key judgment in assessing external validity. In this problem, the assumption that $RR$ is constant allows us to take the relative effect ($RR=0.7$) found in the study and apply it to the target population, which has different underlying characteristics (specifically, a different baseline risk $p_{0}^{\\text{target}}$). This allows for a quantitative prediction of what the intervention's effect would be in this new context. Without such an assumption of transportability for some effect measure, any attempt to generalize results would be baseless.\n\n**Why the Risk Difference Changes When the Risk Ratio is Constant**: The relationship between the risk difference ($RD$) and the risk ratio ($RR$) can be derived directly from their definitions. We start with the definition of the risk difference:\n$$RD = p_1 - p_0$$\nFrom the definition of the risk ratio, we can express $p_1$ as $p_1 = RR \\times p_0$. Substituting this into the equation for $RD$:\n$$RD = (RR \\times p_0) - p_0$$\nBy factoring out the baseline risk, $p_0$, we obtain a direct relationship between $RD$ and $p_0$ for a constant $RR$:\n$$RD = p_0 (RR - 1)$$\nThis equation makes it explicit that if the risk ratio ($RR$) is a constant different from $1$, the risk difference ($RD$) is a linear function of the baseline risk ($p_0$). Therefore, if two populations have different baseline risks (i.e., $p_{0}^{\\text{study}} \\neq p_{0}^{\\text{target}}$), their corresponding risk differences will necessarily be different (i.e., $RD^{\\text{study}} \\neq RD^{\\text{target}}$), even while the relative effect ($RR$) remains the same. The risk ratio captures the proportionate reduction in risk, which is assumed stable, whereas the risk difference captures the absolute reduction in risk, which scales with the baseline level of risk in the population. A higher baseline risk means there is more risk to be reduced, leading to a larger absolute benefit for a given relative effect. In this problem, since $p_{0}^{\\text{study}} = 2 \\times p_{0}^{\\text{target}}$, the magnitude of the risk difference in the study is also twice that in the target population ( $|-0.06| = 2 \\times |-0.03|$ ), consistent with this principle.",
            "answer": "$$\\boxed{-0.03}$$"
        }
    ]
}