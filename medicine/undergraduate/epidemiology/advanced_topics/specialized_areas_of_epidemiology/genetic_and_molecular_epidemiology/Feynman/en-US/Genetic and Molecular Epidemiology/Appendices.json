{
    "hands_on_practices": [
        {
            "introduction": "Before interpreting genetic associations, it is crucial to ensure data quality. A cornerstone of this process is testing for Hardy-Weinberg Equilibrium (HWE), which describes the expected genotype frequencies in a randomly mating population. This exercise  will guide you through constructing an exact test for HWE, a necessary tool when small sample sizes make standard approximations unreliable, allowing you to validate the genetic integrity of your control group.",
            "id": "4595384",
            "problem": "A biallelic single-nucleotide polymorphism with alleles $A$ and $a$ is genotyped in a case-control study. In the cases, the observed genotype counts are $(n_{AA}^{\\mathrm{case}}, n_{Aa}^{\\mathrm{case}}, n_{aa}^{\\mathrm{case}}) = (18, 12, 2)$. In the controls, the observed genotype counts are $(n_{AA}^{\\mathrm{ctrl}}, n_{Aa}^{\\mathrm{ctrl}}, n_{aa}^{\\mathrm{ctrl}}) = (16, 0, 4)$. Focusing solely on the controls, use first principles of Mendelian segregation and random mating to construct the exact test for Hardy-Weinberg Equilibrium (HWE). Proceed as follows:\n\n- Start from the definition of HWE in a randomly mating population, where if the allele $a$ has population frequency $p$, then the genotype probabilities are $(1-p)^{2}$ for $AA$, $2p(1-p)$ for $Aa$, and $p^{2}$ for $aa$. \n- Model the control genotype counts $(n_{AA}^{\\mathrm{ctrl}}, n_{Aa}^{\\mathrm{ctrl}}, n_{aa}^{\\mathrm{ctrl}})$ as a multinomial sample of size $n$ under HWE. Show that by conditioning on the total number of $a$ alleles observed in controls, the nuisance parameter $p$ is eliminated and the conditional distribution of the heterozygote count is obtained.\n- Using this conditional distribution, compute the exact two-sided $p$-value defined as the sum of probabilities of all control genotype configurations (consistent with the observed total number of $a$ alleles) that have probability less than or equal to that of the observed control configuration. Express your final $p$-value as a decimal and round to three significant figures.\n\nFinally, justify based on first principles and expected genotype counts when the large-sample chi-square ($\\chi^{2}$) approximation to the HWE test in controls is valid, and evaluate its validity for the given control counts. Your final reported answer should be only the numerical $p$-value for the exact test, rounded as specified.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It presents a standard problem in statistical genetics: performing an exact test for Hardy-Weinberg Equilibrium (HWE). All necessary data and definitions are provided.\n\nThe problem requires an exact test for HWE in the control group. The observed genotype counts for the controls are given as $(n_{AA}^{\\mathrm{ctrl}}, n_{Aa}^{\\mathrm{ctrl}}, n_{aa}^{\\mathrm{ctrl}}) = (16, 0, 4)$.\n\nThe total number of individuals in the control sample is $n = n_{AA}^{\\mathrm{ctrl}} + n_{Aa}^{\\mathrm{ctrl}} + n_{aa}^{\\mathrm{ctrl}} = 16 + 0 + 4 = 20$.\n\nUnder the null hypothesis of HWE, for a population with allele frequencies $p_A$ and $p_a$ for alleles $A$ and $a$ respectively ($p_A + p_a = 1$), the genotype frequencies are given by $P(AA) = p_A^2$, $P(Aa) = 2p_A p_a$, and $P(aa) = p_a^2$. The problem defines the frequency of allele $a$ as $p_a = p$, so the frequencies are $P(AA) = (1-p)^2$, $P(Aa) = 2p(1-p)$, and $P(aa) = p^2$.\n\nThe genotype counts $(N_{AA}, N_{Aa}, N_{aa})$ in a sample of size $n$ are modeled as a draw from a multinomial distribution. The probability of observing a specific set of counts $(n_{AA}, n_{Aa}, n_{aa})$ is:\n$$ P(N_{AA}=n_{AA}, N_{Aa}=n_{Aa}, N_{aa}=n_{aa}) = \\frac{n!}{n_{AA}! n_{Aa}! n_{aa}!} [P(AA)]^{n_{AA}} [P(Aa)]^{n_{Aa}} [P(aa)]^{n_{aa}} $$\nSubstituting the HWE probabilities, we get the likelihood function for the unknown parameter $p$:\n$$ L(p; n_{AA}, n_{Aa}, n_{aa}) = \\frac{n!}{n_{AA}! n_{Aa}! n_{aa}!} ((1-p)^2)^{n_{AA}} (2p(1-p))^{n_{Aa}} (p^2)^{n_{aa}} $$\n$$ L(p) = \\frac{n!}{n_{AA}! n_{Aa}! n_{aa}!} 2^{n_{Aa}} (1-p)^{2n_{AA}+n_{Aa}} p^{n_{Aa}+2n_{aa}} $$\nLet $N_a$ be the total count of allele $a$ in the sample, where $N_a = n_{Aa} + 2n_{aa}$. The total number of alleles in the sample is $2n$. The count of allele $A$ is $N_A = 2n_{AA} + n_{Aa} = 2n - N_a$. The likelihood can be rewritten as:\n$$ L(p) = \\frac{n!}{n_{AA}! n_{Aa}! n_{aa}!} 2^{n_{Aa}} (1-p)^{2n-N_a} p^{N_a} $$\nBy the Fisher-Neyman factorization theorem, $N_a$ is a sufficient statistic for the nuisance parameter $p$. To create a test that is independent of $p$, we condition on the observed value of the sufficient statistic. For the given control data, the observed count of allele $a$ is $N_a^{\\mathrm{obs}} = n_{Aa}^{\\mathrm{ctrl}} + 2n_{aa}^{\\mathrm{ctrl}} = 0 + 2(4) = 8$.\n\nWe need to find the conditional probability of the genotype counts given $n=20$ and $N_a=8$:\n$$ P(N_{AA}=n_{AA}, N_{Aa}=n_{Aa}, N_{aa}=n_{aa} \\mid N_a=8, n=20) $$\nThis is the ratio of the probability of observing the specific genotype counts to the probability of observing the total allele count $N_a=8$. The total number of $a$ alleles in a sample of $2n$ alleles follows a binomial distribution $N_a \\sim \\mathrm{Binomial}(2n, p)$. Thus, $P(N_a=k) = \\binom{2n}{k} p^k (1-p)^{2n-k}$.\nThe conditional probability is:\n$$ P(\\text{counts} \\mid N_a=8) = \\frac{P(\\text{counts})}{P(N_a=8)} = \\frac{\\frac{n!}{n_{AA}! n_{Aa}! n_{aa}!} 2^{n_{Aa}} p^8 (1-p)^{32}}{\\binom{2n}{8} p^8 (1-p)^{32}} = \\frac{n! 2^{n_{Aa}}}{n_{AA}! n_{Aa}! n_{aa}! \\binom{2n}{8}} $$\nThis probability is free of $p$. Given that $n=20$ and $N_a=8$, the genotype counts are constrained:\n$n_{AA} + n_{Aa} + n_{aa} = 20$\n$2n_{AA} + n_{Aa} = 2(20) - 8 = 32$\n$n_{Aa} + 2n_{aa} = 8$\n\nFrom these constraints, we can express $n_{AA}$ and $n_{aa}$ in terms of $n_{Aa}$:\n$n_{aa} = (8 - n_{Aa})/2$\n$n_{AA} = (32 - n_{Aa})/2$\nFor $n_{AA}$ and $n_{aa}$ to be integers, $n_{Aa}$ must be an even number. Also, all counts must be non-negative, so $n_{Aa} \\ge 0$, $8 - n_{Aa} \\ge 0 \\implies n_{Aa} \\le 8$.\nThus, the possible values for the heterozygote count $n_{Aa}$ are $\\{0, 2, 4, 6, 8\\}$.\n\nThe two-sided exact $p$-value is the sum of probabilities of all possible genotype configurations (for the fixed $N_a=8, n=20$) that have a probability less than or equal to that of the observed configuration $(16, 0, 4)$. The observed configuration corresponds to $n_{Aa}=0$.\n\nLet $f(k)$ be the term in the probability proportional to the configuration with $n_{Aa}=k$:\n$f(n_{Aa}) = \\frac{2^{n_{Aa}}}{n_{AA}! n_{Aa}! n_{aa}!}$. The conditional probability is $P(n_{Aa}=k) = \\frac{n!}{\\binom{2n}{N_a}} f(k)$.\nWe compute the relative values of $f(k)$ for $k \\in \\{0, 2, 4, 6, 8\\}$.\nThe possible configurations $(n_{AA}, n_{Aa}, n_{aa})$ are:\n- $n_{Aa} = 0$: $(16, 0, 4)$ (Observed)\n- $n_{Aa} = 2$: $(15, 2, 3)$\n- $n_{Aa} = 4$: $(14, 4, 2)$\n- $n_{Aa} = 6$: $(13, 6, 1)$\n- $n_{Aa} = 8$: $(12, 8, 0)$\n\nLet's compute the ratios of probabilities:\n$\\frac{P(n_{Aa}=2)}{P(n_{Aa}=0)} = \\frac{f(2)}{f(0)} = \\frac{2^2/(15!2!3!)}{2^0/(16!0!4!)} = \\frac{4 \\cdot 16! \\cdot 4!}{15! \\cdot 2! \\cdot 3!} = 4 \\cdot 16 \\cdot \\frac{24}{12} = 128$.\n$\\frac{P(n_{Aa}=4)}{P(n_{Aa}=2)} = \\frac{f(4)}{f(2)} = \\frac{2^4/(14!4!2!)}{2^2/(15!2!3!)} = \\frac{4 \\cdot 15! \\cdot 2! \\cdot 3!}{14! \\cdot 4! \\cdot 2!} = 4 \\cdot 15 \\cdot \\frac{6}{24} = 15$.\n$\\frac{P(n_{Aa}=6)}{P(n_{Aa}=4)} = \\frac{f(6)}{f(4)} = \\frac{2^6/(13!6!1!)}{2^4/(14!4!2!)} = \\frac{4 \\cdot 14! \\cdot 4! \\cdot 2!}{13! \\cdot 6!} = 4 \\cdot 14 \\cdot \\frac{48}{720} = \\frac{56}{15}$.\n$\\frac{P(n_{Aa}=8)}{P(n_{Aa}=6)} = \\frac{f(8)}{f(6)} = \\frac{2^8/(12!8!0!)}{2^6/(13!6!1!)} = \\frac{4 \\cdot 13! \\cdot 6!}{12! \\cdot 8!} = 4 \\cdot 13 \\cdot \\frac{1}{8 \\cdot 7} = \\frac{13}{14}$.\n\nThese ratios show that $P(n_{Aa}=0) < P(n_{Aa}=2) < P(n_{Aa}=4) < P(n_{Aa}=6)$, and $P(n_{Aa}=8) < P(n_{Aa}=6)$. A full comparison reveals that $P(n_{Aa}=0)$ is the minimum probability among all possible configurations.\nTherefore, the only configuration with probability less than or equal to the observed one is the observed one itself. The $p$-value is the conditional probability of observing $n_{Aa}=0$.\n$$p\\text{-value} = P(n_{Aa}=0 \\mid N_a=8) = \\frac{f(0)}{\\sum_{k \\in \\{0,2,4,6,8\\}} f(k)}$$\nLet $f(0)=1$ (as a reference unit). The other values are:\n$f(2) = \\frac{P(n_{Aa}=2)}{P(n_{Aa}=0)} f(0) = 128 \\cdot f(0)$.\n$f(4) = \\frac{P(n_{Aa}=4)}{P(n_{Aa}=2)} f(2) = 15 \\cdot 128 \\cdot f(0) = 1920 \\cdot f(0)$.\n$f(6) = \\frac{P(n_{Aa}=6)}{P(n_{Aa}=4)} f(4) = \\frac{56}{15} \\cdot 1920 \\cdot f(0) = 56 \\cdot 128 \\cdot f(0) = 7168 \\cdot f(0)$.\n$f(8) = \\frac{P(n_{Aa}=8)}{P(n_{Aa}=6)} f(6) = \\frac{13}{14} \\cdot 7168 \\cdot f(0) = 13 \\cdot 512 \\cdot f(0) = 6656 \\cdot f(0)$.\n\nThe sum of the relative probabilities is:\n$\\sum f(k) = (1 + 128 + 1920 + 7168 + 6656) \\cdot f(0) = 15873 \\cdot f(0)$.\nThe $p$-value is:\n$$ p\\text{-value} = \\frac{f(0)}{15873 \\cdot f(0)} = \\frac{1}{15873} \\approx 0.00006299943... $$\nRounding to three significant figures, the $p$-value is $0.0000630$.\n\nFinally, we assess the validity of the large-sample chi-square ($\\chi^2$) approximation. The $\\chi^2$ test statistic for HWE, $\\sum (O-E)^2/E$, is approximately $\\chi^2$-distributed with one degree of freedom. This approximation is based on the normal approximation to the multinomial distribution of genotype counts. The validity of this approximation fundamentally requires that the expected count for each cell (genotype) is not too small. A widely accepted guideline is that all expected counts should be at least $5$.\n\nTo calculate the expected counts for the control group, we first estimate the allele frequency $p$ from the sample data. The maximum likelihood estimate is the sample allele frequency:\n$$\\hat{p} = \\frac{n_{Aa}^{\\mathrm{ctrl}} + 2n_{aa}^{\\mathrm{ctrl}}}{2n} = \\frac{0 + 2(4)}{2(20)} = \\frac{8}{40} = 0.2$$\nThe expected genotype counts under HWE are:\n$E_{AA} = n(1-\\hat{p})^2 = 20(0.8)^2 = 20(0.64) = 12.8$\n$E_{Aa} = n(2\\hat{p}(1-\\hat{p})) = 20(2 \\cdot 0.2 \\cdot 0.8) = 20(0.32) = 6.4$\n$E_{aa} = n(\\hat{p}^2) = 20(0.2)^2 = 20(0.04) = 0.8$\n\nThe expected count for the $aa$ genotype is $E_{aa} = 0.8$, which is substantially less than the conventional threshold of $5$. Because one of the expected counts is very small, the theoretical conditions for the $\\chi^2$ approximation are not met. Therefore, the large-sample $\\chi^2$ test is not valid for this dataset, and the use of an exact test is necessary and appropriate.",
            "answer": "$$\\boxed{6.30 \\times 10^{-5}}$$"
        },
        {
            "introduction": "Interpreting the results of a Genome-Wide Association Study (GWAS) requires carefully dissecting the sources of observed association signals. A key challenge is distinguishing between true polygenic architecture, where many variants have small effects, and inflation from confounding factors like population stratification. This practice  introduces Linkage Disequilibrium (LD) Score Regression, a powerful technique that uses the correlation structure between genetic variants to partition test statistic inflation into these distinct components.",
            "id": "4595371",
            "problem": "A genome-wide association study (GWAS) of a complex trait was conducted in a homogeneous population using single-nucleotide polymorphisms (SNPs) after standard quality control, resulting in $M$ independent SNP-level association tests. The sample size was $N$. The association test statistic for SNP $j$ is a one-degree-of-freedom chi-square statistic, denoted $\\chi_{j}^{2}$, computed from standardized genotypes and phenotypes. A quantile-quantile (Q-Q) plot indicates deviation from the null line, consistent with inflation of test statistics. To distinguish whether this inflation arises from true polygenicity or confounding (for example, population stratification or cryptic relatedness), you are asked to reason from first principles of the polygenic model and Linkage Disequilibrium (LD) properties, and then use LD Score Regression (LDSC).\n\nUse the following foundational bases:\n- Under the null of no association without confounding, the one-degree-of-freedom chi-square statistic has expected value $1$.\n- Under a polygenic additive model with many small independent SNP effects and standardized inputs, the expected variance in phenotype tagged by SNP $j$ increases with its LD score $l_{j}$, where the LD score is defined as the sum of squared correlations of SNP $j$ with other SNPs in the region. This creates an expected increase in the noncentrality of $\\chi_{j}^{2}$ with $l_{j}$.\n- Confounding that affects all SNPs approximately uniformly contributes an additive inflation to the expected $\\chi_{j}^{2}$ that does not depend on $l_{j}$.\n\nStarting from these bases, derive a linear decomposition of the expected $\\chi_{j}^{2}$ into a baseline, a component proportional to $l_{j}$ due to polygenicity, and an $l_{j}$-independent component due to confounding. Show that fitting an ordinary least squares (OLS) regression of $\\chi_{j}^{2}$ on $l_{j}$ across SNPs estimates a slope that captures the polygenic contribution per unit LD score and an intercept that captures the baseline plus confounding. Then, using the provided aggregated genome-wide sample moments, compute the LDSC intercept and slope, and interpret the slope in terms of SNP-heritability and the intercept in terms of the fraction of observed inflation attributable to confounding:\n\n- Number of SNPs: $M = 1{,}000{,}000$.\n- Sample size: $N = 200{,}000$.\n- Sample mean of LD scores: $\\bar{l} = 85$.\n- Sample variance of LD scores: $\\operatorname{Var}(l) = 65$.\n- Sample covariance between LD scores and chi-square statistics: $\\operatorname{Cov}(l, \\chi^{2}) = 0.39$.\n- Sample mean of chi-square statistics: $\\overline{\\chi^{2}} = 1.56$.\n\nDefine your estimates as follows:\n- The OLS slope $\\hat{b}$ estimates the per-unit LD score increase in expected $\\chi^{2}$ due to polygenicity.\n- The OLS intercept $\\hat{a}$ estimates the baseline plus confounding component of expected $\\chi^{2}$ at $l_{j} = 0$.\n- Assuming the standard LDSC identification, interpret the slope as $\\hat{b} \\approx N h^{2} / M$ to estimate SNP-heritability $h^{2}$.\n- Interpret the intercept as $\\hat{a} \\approx 1 +$ confounding, and compute the fraction of the observed inflation $\\overline{\\chi^{2}} - 1$ attributable to confounding as $(\\hat{a} - 1) / (\\overline{\\chi^{2}} - 1)$.\n\nRound your answers to four significant figures. Express the final result as a row matrix with entries, in order: LDSC intercept $\\hat{a}$, LDSC slope $\\hat{b}$, SNP-heritability estimate $\\hat{h}^{2}$, and the fraction of observed inflation attributable to confounding $(\\hat{a} - 1) / (\\overline{\\chi^{2}} - 1)$. No physical units are required.",
            "solution": "The user has provided a problem in genetic epidemiology concerning the interpretation of genome-wide association study (GWAS) results using LD Score Regression (LDSC). The problem requires deriving the fundamental linear relationship underlying LDSC and then applying it to a set of provided summary statistics to estimate key parameters.\n\n### Step 1: Problem Validation\n\nFirst, I will extract the givens and validate the problem statement.\n\n**Givens:**\n- Number of independent SNPs: $M = 1{,}000{,}000$\n- Sample size: $N = 200{,}000$\n- Association test statistic for SNP $j$: $\\chi_{j}^{2}$, a one-degree-of-freedom chi-square statistic.\n- LD score for SNP $j$: $l_{j} = \\sum_{k} r_{jk}^{2}$, where $r_{jk}$ is the correlation between SNP $j$ and SNP $k$.\n- Foundational bases:\n    1. Under the null hypothesis of no genetic association and no confounding, the expected value of the test statistic is $E[\\chi_{j}^{2}] = 1$.\n    2. Under a polygenic additive model, the expected noncentrality of $\\chi_{j}^{2}$ increases proportionally with the LD score $l_{j}$.\n    3. Confounding introduces an additive inflation to the expected $\\chi_{j}^{2}$ that is independent of $l_{j}$.\n- Aggregated genome-wide sample moments:\n    - Sample mean of LD scores: $\\bar{l} = 85$\n    - Sample variance of LD scores: $\\operatorname{Var}(l) = 65$\n    - Sample covariance between LD scores and chi-square statistics: $\\operatorname{Cov}(l, \\chi^{2}) = 0.39$\n    - Sample mean of chi-square statistics: $\\overline{\\chi^{2}} = 1.56$\n- Required computations and interpretations:\n    - OLS slope $\\hat{b}$ and intercept $\\hat{a}$ from the regression of $\\chi^{2}$ on $l$.\n    - SNP-heritability estimate $\\hat{h}^{2}$ using the relation $\\hat{b} \\approx N h^{2} / M$.\n    - Fraction of observed inflation attributable to confounding: $(\\hat{a} - 1) / (\\overline{\\chi^{2}} - 1)$.\n    - Final answers to be rounded to four significant figures.\n\n**Validation:**\n- **Scientific Grounding:** The problem is firmly based on the established principles of Linkage Disequilibrium (LD) Score Regression, a standard and widely used statistical method in genetic epidemiology. The description of polygenicity, confounding, LD scores, and their relationship to GWAS chi-square statistics is correct and reflects the foundational theory of the method.\n- **Well-Posedness:** The problem is well-posed. It asks for the derivation of a linear model and the estimation of its parameters using ordinary least squares (OLS). It provides precisely the necessary sample moments ($\\operatorname{Cov}(l, \\chi^2)$, $\\operatorname{Var}(l)$, $\\bar{l}$, $\\overline{\\chi^2}$) to calculate the OLS slope and intercept. All constants required for subsequent interpretations ($N$, $M$) are also provided. A unique solution exists.\n- **Objectivity:** The problem is stated in precise, objective, and technical language, free from ambiguity or subjective claims.\n\n**Verdict:**\nThe problem is valid. It is scientifically sound, well-posed, objective, and contains all necessary information for a complete solution. I will proceed with the derivation and calculation.\n\n### Step 2: Derivation and Solution\n\n**Derivation of the LD Score Regression Model**\n\nThe problem asks to derive a linear decomposition of the expected $\\chi_{j}^{2}$ statistic. We start from the provided foundational bases.\n\n1.  **Baseline:** In the absence of any genetic effect or confounding, a one-degree-of-freedom chi-square test statistic is expected to have a mean of $1$. This establishes the baseline for our model: $E[\\chi_{j}^{2}]_{\\text{null}} = 1$.\n\n2.  **Polygenicity:** The problem states that under a polygenic model, the expected noncentrality of $\\chi_{j}^{2}$ increases with its LD score, $l_{j}$. Let's formalize this. The $\\chi_{j}^{2}$ statistic for SNP $j$ from a simple linear regression with standardized genotype $G_j$ and phenotype $Y$ is approximately $N \\hat{\\beta}_{j, \\text{marginal}}^{2}$, where $\\hat{\\beta}_{j, \\text{marginal}}$ is the estimated marginal effect size. The expected value of the statistic is approximately $1 + N E[\\hat{\\beta}_{j, \\text{marginal}}^{2}]$. The marginal effect of SNP $j$ captures its own causal effect, $\\beta_j$, plus effects from all other SNPs ($k$) that are in LD with it, weighted by the correlation $r_{jk}$: $\\beta_{j, \\text{marginal}} \\approx \\sum_{k} r_{jk} \\beta_k$. The expected squared marginal effect is $E[\\beta_{j, \\text{marginal}}^{2}] = E[(\\sum_{k} r_{jk} \\beta_k)^2]$. Assuming causal effects $\\beta_k$ are independent with mean $0$ and variance $h^2/M$ (where $h^2$ is the total SNP-heritability distributed across $M$ causal SNPs), this simplifies to $E[\\beta_{j, \\text{marginal}}^{2}] = \\sum_{k} r_{jk}^{2} E[\\beta_k^2] = (\\sum_{k} r_{jk}^{2}) \\frac{h^2}{M} = l_j \\frac{h^2}{M}$.\n    Therefore, the contribution to the expected chi-square statistic from polygenicity is $N l_j \\frac{h^2}{M}$.\n\n3.  **Confounding:** The problem states that confounding (e.g., from population stratification or cryptic relatedness) adds a constant inflation term, $C$, that does not depend on the LD score $l_j$. This term affects all SNPs approximately equally.\n\nCombining these three components, we arrive at the linear model for the expected value of the chi-square statistic for SNP $j$:\n$$E[\\chi_{j}^{2}] = 1 + N \\frac{h^{2}}{M} l_{j} + C$$\nThis equation is a linear model of the form $E[Y_j] = a + b X_j$, where the dependent variable is $Y_j = \\chi_j^2$ and the independent variable is $X_j = l_j$. The model parameters are:\n-   **Slope:** $b = \\frac{N h^{2}}{M}$. This term quantifies the contribution of polygenicity. A higher LD score $l_j$ means the SNP tags more genetic variation, leading to a stronger expected association signal.\n-   **Intercept:** $a = 1 + C$. This term captures the baseline expectation of $1$ plus any inflation $C$ due to confounding.\n\n**Estimation using Ordinary Least Squares (OLS)**\n\nWe are asked to fit a regression of $\\chi_j^2$ on $l_j$ across all $M$ SNPs:\n$$ \\chi_{j}^{2} = a + b l_{j} + \\epsilon_{j} $$\nThe OLS estimators for the slope ($\\hat{b}$) and intercept ($\\hat{a}$) are calculated from sample moments. The formula for the slope estimator is:\n$$ \\hat{b} = \\frac{\\operatorname{Cov}(l, \\chi^{2})}{\\operatorname{Var}(l)} $$\nThe formula for the intercept estimator is:\n$$ \\hat{a} = \\overline{\\chi^{2}} - \\hat{b} \\bar{l} $$\nThese estimators provide estimates for the true slope $b$ and intercept $a$.\n\n**Numerical Calculation**\n\nUsing the provided sample moments:\n- $\\operatorname{Cov}(l, \\chi^{2}) = 0.39$\n- $\\operatorname{Var}(l) = 65$\n- $\\overline{\\chi^{2}} = 1.56$\n- $\\bar{l} = 85$\n\nFirst, we compute the OLS slope, $\\hat{b}$:\n$$ \\hat{b} = \\frac{0.39}{65} = 0.006 $$\n\nNext, we compute the OLS intercept, $\\hat{a}$:\n$$ \\hat{a} = 1.56 - (0.006 \\times 85) = 1.56 - 0.51 = 1.05 $$\n\nRounding to four significant figures as required:\n- $\\hat{b} = 0.006000$\n- $\\hat{a} = 1.050$\n\n**Interpretation and Final Computations**\n\n1.  **SNP-heritability ($h^2$)**:\n    We use the relationship $\\hat{b} \\approx \\frac{N h^{2}}{M}$ to estimate $h^2$.\n    $$ \\hat{h}^{2} = \\frac{M \\hat{b}}{N} $$\n    Substituting the values for $M$, $N$, and our estimated $\\hat{b}$:\n    $$ \\hat{h}^{2} = \\frac{1{,}000{,}000 \\times 0.006}{200{,}000} = \\frac{6000}{200000} = \\frac{6}{200} = 0.03 $$\n    Rounded to four significant figures, $\\hat{h}^2 = 0.03000$.\n\n2.  **Fraction of inflation from confounding**:\n    The total observed inflation of the mean test statistic is $\\overline{\\chi^{2}} - 1 = 1.56 - 1 = 0.56$.\n    The intercept $\\hat{a}$ is an estimate of $1 + C$, where $C$ is the inflation due to confounding. Thus, the portion of inflation attributable to confounding is estimated by $\\hat{a} - 1$.\n    $$ \\text{Confounding Inflation} = \\hat{a} - 1 = 1.050 - 1 = 0.050 $$\n    The fraction of the total observed inflation that is due to confounding is:\n    $$ \\frac{\\hat{a} - 1}{\\overline{\\chi^{2}} - 1} = \\frac{0.050}{0.56} \\approx 0.0892857... $$\n    Rounded to four significant figures, this fraction is $0.08929$.\n\nThis analysis indicates that the vast majority of the observed inflation in test statistics ($\\overline{\\chi^2}=1.56$) is due to true polygenic signal, not confounding. Specifically, the intercept of $1.050$ suggests only a modest level of confounding, while the non-zero slope of $0.006000$ provides strong evidence for a polygenic architecture, yielding a SNP-heritability estimate of $3\\%$.\n\n**Summary of Results:**\n- LDSC intercept $\\hat{a} = 1.050$\n- LDSC slope $\\hat{b} = 0.006000$\n- SNP-heritability estimate $\\hat{h}^2 = 0.03000$\n- Fraction of inflation from confounding = $0.08929$\n\nThese values will be placed into a row matrix for the final answer.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 1.050 & 0.006000 & 0.03000 & 0.08929 \\end{pmatrix} } $$"
        },
        {
            "introduction": "Genetic epidemiology extends beyond identifying disease risk to personalizing medicine, a field known as pharmacogenetics. A classic example is warfarin, an anticoagulant whose effective dose varies widely among individuals due to both genetic and clinical factors. In this hands-on exercise , you will build a dose prediction model integrating variants in the `CYP2C9` and `VKORC1` genes with clinical data, and learn to quantify the distinct contribution of genetics to predicting this important clinical outcome.",
            "id": "4595342",
            "problem": "A cohort study collected warfarin maintenance dose data along with genetic and clinical covariates to investigate pharmacogenetic determinants of dose requirements. Warfarin dose is measured in milligrams per day, and genotypes for cytochrome P450 family 2 subfamily C member 9 (CYP2C9) and vitamin K epoxide reductase complex subunit 1 (VKORC1) were recorded as variant-allele counts per individual. Clinical covariates include age in years, weight in kilograms, and an indicator for concomitant amiodarone therapy. Your task is to implement a program that, for a set of modeling scenarios, fits an ordinary least squares dose prediction model and quantifies the proportion of variance explained by genetic and clinical factors.\n\nUse the following foundational principles and definitions as the starting point:\n- The central dogma of molecular biology (deoxyribonucleic acid to ribonucleic acid to protein) and pharmacogenetics provide a mechanistic rationale that genetic variants can alter drug metabolism and sensitivity, affecting dose requirements. In epidemiologic modeling, these effects are represented through covariates in a regression model.\n- The linear regression model specifies the outcome as a linear combination of predictors: for outcome vector $\\mathbf{y} \\in \\mathbb{R}^{n}$ and predictor matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, the ordinary least squares estimator solves the minimization of the residual sum of squares, i.e., minimize $\\mathrm{RSS}(\\boldsymbol{\\beta}) = \\lVert \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\rVert_2^2$ over $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$, yielding $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$ when $\\mathbf{X}^{\\top}\\mathbf{X}$ is invertible, and fitted values $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$.\n- The total sum of squares is $\\mathrm{TSS} = \\sum_{i=1}^{n}(y_i - \\bar{y})^2$, where $\\bar{y}$ is the sample mean of $\\mathbf{y}$. For a fitted model, the residual sum of squares is $\\mathrm{RSS} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$. The coefficient of determination is $R^2 = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}}$.\n- To partition explained variance between two predictor blocks, clinical ($\\mathbf{X}_c$) and genetic ($\\mathbf{X}_g$), use the averaged sequential sums of squares (also known as Lindeman–Merenda–Gold for two blocks): define $R^2_{\\mathrm{clin}}$ as the $R^2$ from a model with only clinical predictors (and an intercept), $R^2_{\\mathrm{gen}}$ as the $R^2$ from a model with only genetic predictors (and an intercept), and $R^2_{\\mathrm{full}}$ as the $R^2$ from the full model with both clinical and genetic predictors (and a single intercept). Then the clinical contribution is $C_{\\mathrm{clin}} = \\frac{1}{2}\\left(R^2_{\\mathrm{clin}} + (R^2_{\\mathrm{full}} - R^2_{\\mathrm{gen}})\\right)$ and the genetic contribution is $C_{\\mathrm{gen}} = \\frac{1}{2}\\left(R^2_{\\mathrm{gen}} + (R^2_{\\mathrm{full}} - R^2_{\\mathrm{clin}})\\right)$, which satisfy $C_{\\mathrm{clin}} + C_{\\mathrm{gen}} = R^2_{\\mathrm{full}}$.\n\nYou will analyze three datasets and four modeling scenarios. In all cases, the clinical predictor block $\\mathbf{X}_c$ contains an intercept and the following covariates: age (years), weight (kilograms), and amiodarone indicator ($0$ if no, $1$ if yes). The genetic predictor block $\\mathbf{X}_g$ contains an intercept and the genetic covariates as encoded according to the scenario. Genetic encoding options are:\n- Additive coding: use the variant-allele counts directly, i.e., if the count is $0$, $1$, or $2$, then the encoded value is $0$, $1$, or $2$ respectively.\n- Dominant coding: use $1$ if the variant-allele count is >= 1, else $0$.\n\nDose transformation options are:\n- None: use the dose vector $\\mathbf{y}$ as given.\n- Log: use the transformed dose $\\tilde{\\mathbf{y}}$ with $\\tilde{y}_i = \\ln(y_i)$.\n\nDatasets (each dose is in milligrams per day; ages in years; weights in kilograms; amiodarone, CYP2C9, and VKORC1 are coded as described above):\n- Dataset A ($n = 12$):\n  - Dose: $\\{4.1, 5.36, 3.08, 4.54, 3.1, 4.92, 4.4, 3.3, 5.6, 4.7, 5.1, 4.1\\}$\n  - Age: $\\{45, 52, 60, 68, 70, 55, 40, 80, 65, 50, 58, 72\\}$\n  - Weight: $\\{65, 80, 72, 90, 75, 68, 55, 85, 70, 60, 78, 82\\}$\n  - Amiodarone: $\\{0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0\\}$\n  - CYP2C9 variant count: $\\{1, 0, 1, 2, 1, 0, 1, 0, 0, 0, 1, 0\\}$\n  - VKORC1 variant count: $\\{1, 1, 1, 0, 2, 0, 0, 2, 0, 1, 0, 2\\}$\n- Dataset B ($n = 10$):\n  - Dose: $\\{5.88, 5.8, 4.9, 5.9, 5.62, 5.5, 4.76, 4.8, 4.8, 5.62\\}$\n  - Age: $\\{50, 55, 60, 65, 70, 45, 58, 62, 68, 52\\}$\n  - Weight: $\\{72, 75, 70, 80, 78, 60, 68, 74, 76, 66\\}$\n  - Amiodarone: $\\{0, 0, 1, 0, 0, 0, 1, 0, 0, 0\\}$\n  - CYP2C9 variant count: $\\{0, 0, 0, 0, 0, 0, 0, 1, 0, 0\\}$\n  - VKORC1 variant count: $\\{0, 0, 0, 0, 0, 0, 0, 0, 1, 0\\}$\n- Dataset C ($n = 12$):\n  - Dose: $\\{5.55, 3.8, 3.76, 3.84, 3.85, 4.0, 3.7, 3.84, 3.84, 3.72, 3.98, 3.81\\}$\n  - Age: $\\{60, 62, 58, 61, 59, 60, 60, 59, 61, 62, 58, 60\\}$\n  - Weight: $\\{70, 71, 69, 70, 70, 71, 69, 70, 69, 71, 70, 70\\}$\n  - Amiodarone: $\\{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\\}$\n  - CYP2C9 variant count: $\\{0, 1, 2, 0, 1, 0, 2, 1, 0, 2, 0, 1\\}$\n  - VKORC1 variant count: $\\{0, 1, 0, 2, 1, 2, 0, 1, 2, 0, 2, 1\\}$\n\nModeling scenarios (test suite):\n- Scenario $1$: Dataset A, dose transformation = log, genetic encoding = additive.\n- Scenario $2$: Dataset A, dose transformation = none, genetic encoding = dominant.\n- Scenario $3$: Dataset B, dose transformation = log, genetic encoding = additive.\n- Scenario $4$: Dataset C, dose transformation = log, genetic encoding = additive.\n\nFor each scenario:\n- Construct $\\mathbf{X}_c$ with an intercept and the clinical covariates in the stated order.\n- Construct $\\mathbf{X}_g$ with an intercept and the genetic covariates encoded per the scenario.\n- Construct the full model matrix $\\mathbf{X}_{\\mathrm{full}}$ with a single intercept and all clinical and genetic covariates.\n- Fit ordinary least squares for the clinical-only, genetic-only, and full models.\n- Compute $R^2_{\\mathrm{clin}}$, $R^2_{\\mathrm{gen}}$, and $R^2_{\\mathrm{full}}$ using the definitions above.\n- Compute $C_{\\mathrm{clin}}$ and $C_{\\mathrm{gen}}$ as the averaged sequential contributions.\n\nYour program should produce a single line of output containing the results for the four scenarios as a comma-separated list enclosed in square brackets, where each scenario’s result is a sublist of three floats in the order $[R^2_{\\mathrm{full}}, C_{\\mathrm{gen}}, C_{\\mathrm{clin}}]$. Round each float to four decimal places. For example, the output format must be like $[[a,b,c],[d,e,f],[g,h,i],[j,k,l]]$ where each symbol is a float rounded to four decimal places. No other text should be printed.",
            "solution": "The problem requires the implementation of an ordinary least squares (OLS) regression analysis to quantify the proportion of variance in warfarin dose requirements explained by genetic and clinical factors. This task is grounded in the principles of pharmacogenetics and statistical modeling. The analysis will proceed by constructing and evaluating several linear models for specified datasets and scenarios, culminating in the calculation of partitioned variance components.\n\nThe foundational statistical model is the multiple linear regression model, which posits a linear relationship between a dependent variable vector $\\mathbf{y} \\in \\mathbb{R}^{n}$ and a set of predictor variables represented by the design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$. The model is expressed as:\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n$$\nwhere $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ is the vector of regression coefficients and $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^{n}$ is the vector of random errors. The OLS method finds the coefficient estimate $\\hat{\\boldsymbol{\\beta}}$ that minimizes the residual sum of squares (RSS):\n$$\n\\mathrm{RSS}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - (\\mathbf{X}\\boldsymbol{\\beta})_i)^2 = \\lVert \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\rVert_2^2\n$$\nAssuming the matrix $\\mathbf{X}^{\\top}\\mathbf{X}$ is invertible (i.e., $\\mathbf{X}$ has full column rank), the unique solution to this minimization problem is given by the normal equations:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\nThe fitted values are then $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$.\n\nThe goodness of fit of the model is assessed using the coefficient of determination, $R^2$. It measures the proportion of the total variance in the outcome variable that is predictable from the predictor variables. First, the total sum of squares (TSS) is calculated, which represents the total variance in the outcome:\n$$\n\\mathrm{TSS} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n$$\nwhere $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i$ is the sample mean of the outcome. The RSS, calculated using the fitted values $\\hat{y}_i$, represents the variance not explained by the model. The coefficient of determination is then:\n$$\nR^2 = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n$$\n\nTo partition the explained variance ($R^2$) between two blocks of predictors, clinical ($\\mathbf{X}_c$) and genetic ($\\mathbf{X}_g$), we employ the method of averaged sequential sums of squares, also known as the Lindeman–Merenda–Gold (LMG) method. This approach provides a balanced attribution of variance, especially when predictors are correlated. The procedure requires fitting three models:\n$1$. A model with only clinical predictors (and an intercept), yielding $R^2_{\\mathrm{clin}}$.\n$2$. A model with only genetic predictors (and an intercept), yielding $R^2_{\\mathrm{gen}}$.\n$3$. A full model with both clinical and genetic predictors (and a single intercept), yielding $R^2_{\\mathrm{full}}$.\n\nThe contributions of the clinical and genetic blocks to the total explained variance of the full model are then calculated as:\n$$\nC_{\\mathrm{clin}} = \\frac{1}{2}\\left(R^2_{\\mathrm{clin}} + (R^2_{\\mathrm{full}} - R^2_{\\mathrm{gen}})\\right)\n$$\n$$\nC_{\\mathrm{gen}} = \\frac{1}{2}\\left(R^2_{\\mathrm{gen}} + (R^2_{\\mathrm{full}} - R^2_{\\mathrm{clin}})\\right)\n$$\nThese contributions sum to the total explained variance of the full model, i.e., $C_{\\mathrm{clin}} + C_{\\mathrm{gen}} = R^2_{\\mathrm{full}}$.\n\nThe computational procedure for each specified scenario is as follows:\n$1$. **Data Preparation**: For the given dataset, the outcome vector $\\mathbf{y}$ is prepared, applying a natural logarithm transformation ($\\tilde{y}_i = \\ln(y_i)$) if specified. The genetic covariates are encoded based on the scenario's rule (additive or dominant).\n$2$. **Matrix Construction**: Three design matrices are assembled:\n    - $\\mathbf{X}_c$: An intercept column (all $1$s), followed by columns for age, weight, and amiodarone status.\n    - $\\mathbf{X}_g$: An intercept column, followed by columns for the encoded `CYP2C9` and `VKORC1` covariates.\n    - $\\mathbf{X}_{\\mathrm{full}}$: An intercept column, followed by all clinical and genetic covariates.\n$3$. **Model Fitting and Evaluation**: An OLS regression is performed for each of the three models $(\\mathbf{y}, \\mathbf{X}_c)$, $(\\mathbf{y}, \\mathbf{X}_g)$, and $(\\mathbf{y}, \\mathbf{X}_{\\mathrm{full}})$. For each fit, the corresponding $R^2$ value ($R^2_{\\mathrm{clin}}$, $R^2_{\\mathrm{gen}}$, $R^2_{\\mathrm{full}}$) is computed. While the theoretical solution for $\\hat{\\boldsymbol{\\beta}}$ involves matrix inversion, a numerically more stable method like solving the normal equations or using a least-squares solver based on matrix decomposition (e.g., QR or SVD) is preferable in implementation.\n$4$. **Variance Partitioning**: Using the calculated $R^2$ values, the contributions $C_{\\mathrm{clin}}$ and $C_{\\mathrm{gen}}$ are calculated using the LMG formulas.\n$5$. **Output Formulation**: The final results for each scenario, $[R^2_{\\mathrm{full}}, C_{\\mathrm{gen}}, C_{\\mathrm{clin}}]$, are rounded to four decimal places and compiled into the specified list format.\nThis systematic process will be applied to each of the four modeling scenarios described in the problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the warfarin dose prediction problem by fitting OLS models and partitioning variance\n    for a set of specified scenarios.\n    \"\"\"\n\n    # Define the datasets as provided in the problem statement.\n    datasets = {\n        'A': {\n            'n': 12,\n            'dose': np.array([4.1, 5.36, 3.08, 4.54, 3.1, 4.92, 4.4, 3.3, 5.6, 4.7, 5.1, 4.1]),\n            'age': np.array([45, 52, 60, 68, 70, 55, 40, 80, 65, 50, 58, 72]),\n            'weight': np.array([65, 80, 72, 90, 75, 68, 55, 85, 70, 60, 78, 82]),\n            'amiodarone': np.array([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0]),\n            'cyp2c9': np.array([1, 0, 1, 2, 1, 0, 1, 0, 0, 0, 1, 0]),\n            'vkorc1': np.array([1, 1, 1, 0, 2, 0, 0, 2, 0, 1, 0, 2]),\n        },\n        'B': {\n            'n': 10,\n            'dose': np.array([5.88, 5.8, 4.9, 5.9, 5.62, 5.5, 4.76, 4.8, 4.8, 5.62]),\n            'age': np.array([50, 55, 60, 65, 70, 45, 58, 62, 68, 52]),\n            'weight': np.array([72, 75, 70, 80, 78, 60, 68, 74, 76, 66]),\n            'amiodarone': np.array([0, 0, 1, 0, 0, 0, 1, 0, 0, 0]),\n            'cyp2c9': np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n            'vkorc1': np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0]),\n        },\n        'C': {\n            'n': 12,\n            'dose': np.array([5.55, 3.8, 3.76, 3.84, 3.85, 4.0, 3.7, 3.84, 3.84, 3.72, 3.98, 3.81]),\n            'age': np.array([60, 62, 58, 61, 59, 60, 60, 59, 61, 62, 58, 60]),\n            'weight': np.array([70, 71, 69, 70, 70, 71, 69, 70, 69, 71, 70, 70]),\n            'amiodarone': np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n            'cyp2c9': np.array([0, 1, 2, 0, 1, 0, 2, 1, 0, 2, 0, 1]),\n            'vkorc1': np.array([0, 1, 0, 2, 1, 2, 0, 1, 2, 0, 2, 1]),\n        }\n    }\n\n    # Define the modeling scenarios.\n    scenarios = [\n        {'dataset': 'A', 'dose_trans': 'log', 'gen_encode': 'additive'},\n        {'dataset': 'A', 'dose_trans': 'none', 'gen_encode': 'dominant'},\n        {'dataset': 'B', 'dose_trans': 'log', 'gen_encode': 'additive'},\n        {'dataset': 'C', 'dose_trans': 'log', 'gen_encode': 'additive'},\n    ]\n\n    def calculate_r_squared(X, y):\n        \"\"\"\n        Calculates the R-squared value for a linear regression model.\n        \n        Args:\n            X (np.ndarray): The design matrix (n x p), including an intercept.\n            y (np.ndarray): The outcome vector (n,).\n        \n        Returns:\n            float: The R-squared value.\n        \"\"\"\n        # Use np.linalg.lstsq for a numerically stable OLS solution.\n        # It minimizes ||y - Xb||^2.\n        beta_hat, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        y_hat = X @ beta_hat\n        \n        tss = np.sum((y - np.mean(y))**2)\n        rss = np.sum((y - y_hat)**2)\n        \n        if tss == 0:\n            return 1.0 if rss == 0 else 0.0\n            \n        return 1 - (rss / tss)\n\n    results = []\n\n    for sc in scenarios:\n        # 1. Prepare data for the current scenario\n        data = datasets[sc['dataset']]\n        n = data['n']\n        \n        # Apply dose transformation\n        if sc['dose_trans'] == 'log':\n            y = np.log(data['dose'])\n        else:\n            y = data['dose']\n            \n        # Apply genetic encoding\n        if sc['gen_encode'] == 'additive':\n            cyp2c9_encoded = data['cyp2c9']\n            vkorc1_encoded = data['vkorc1']\n        elif sc['gen_encode'] == 'dominant':\n            cyp2c9_encoded = (data['cyp2c9'] >= 1).astype(int)\n            vkorc1_encoded = (data['vkorc1'] >= 1).astype(int)\n        \n        # 2. Construct design matrices\n        intercept = np.ones(n)\n        \n        X_c = np.stack([intercept, data['age'], data['weight'], data['amiodarone']], axis=1)\n        X_g = np.stack([intercept, cyp2c9_encoded, vkorc1_encoded], axis=1)\n        X_full = np.stack([intercept, data['age'], data['weight'], data['amiodarone'],\n                           cyp2c9_encoded, vkorc1_encoded], axis=1)\n                           \n        # 3. Fit models and compute R-squared values\n        r2_clin = calculate_r_squared(X_c, y)\n        r2_gen = calculate_r_squared(X_g, y)\n        r2_full = calculate_r_squared(X_full, y)\n        \n        # 4. Compute averaged sequential contributions\n        c_clin = 0.5 * (r2_clin + (r2_full - r2_gen))\n        c_gen = 0.5 * (r2_gen + (r2_full - r2_clin))\n        \n        # 5. Store results rounded to four decimal places\n        results.append([\n            round(r2_full, 4),\n            round(c_gen, 4),\n            round(c_clin, 4)\n        ])\n    \n    # Final print statement in the exact required format.\n    print(f\"[[{results[0][0]}, {results[0][1]}, {results[0][2]}],[{results[1][0]}, {results[1][1]}, {results[1][2]}],[{results[2][0]}, {results[2][1]}, {results[2][2]}],[{results[3][0]}, {results[3][1]}, {results[3][2]}]]\")\n\nsolve()\n\n```"
        }
    ]
}