{
    "hands_on_practices": [
        {
            "introduction": "在进行复杂的回归不连续设计（RDD）估计之前，一个关键的步骤是数据的可视化。本练习将指导你学习如何创建信息丰富的RDD图，并进行关键的视觉诊断检查，以评估设计的有效性。掌握这些技巧对于识别真实的处理效应和避免错误结论至关重要。",
            "id": "4629748",
            "problem": "一个流行病学团队评估一项公共资助的流感疫苗接种计划的资格对一年内住院率的因果效应。资格由 $c = 65$ 岁的年龄分界点确定性地分配：个体的驱动变量 $X_i$ 为其精确年龄（以年为单位，精确到天），当且仅当 $X_i \\ge c$ 时，该个体接受处理。设观测结果为 $Y_i$，处理指标为 $D_i = \\mathbf{1}\\{X_i \\ge c\\}$，潜在结果为 $Y_i(1)$ 和 $Y_i(0)$。回归断点设计依赖于连续性假设，即 $\\mathbb{E}[Y_i(d)\\mid X_i = x]$ 在 $x = c$ 处对 $d \\in \\{0,1\\}$ 是连续的，因此目标参数是分界点处的跳跃，\n$$\n\\tau = \\lim_{x \\downarrow c} \\mathbb{E}[Y_i \\mid X_i = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y_i \\mid X_i = x].\n$$\n该团队希望构建一个信息丰富的回归断点图，图中包含分箱散点和在 $c$ 两侧叠加的局部多项式拟合，以直观地评估该设计及其假设的合理性。\n\n以下哪些程序和解释正确地描述了如何构建这样的图，以及哪些视觉特征支持或削弱了该设计的有效性？\n\nA. 创建不跨越 $c$ 的驱动变量 $X_i$ 的分箱；在每个分箱内，计算 $Y_i$ 的均值，并在该分箱的中点处绘制该均值。在 $c$ 的左侧和右侧叠加独立的局部多项式拟合（例如，局部线性拟合），该拟合仅使用相应一侧满足 $|X_i - c| \\le h$ 的观测值进行估计，并使用核权重来降低离 $c$ 较远的点的影响，同时不强制曲线在 $x = c$ 处相遇。\n\nB. 汇集 $c$ 两侧的所有观测值，并在 $X_i$ 的整个支撑集上拟合一个次数为 $k \\ge 4$ 的高阶全局多项式；使用拟合的曲线推断 $c$ 处的跳跃，因为更高次的多项式可以灵活地在任何地方近似条件均值函数。\n\nC. 设置足够大的分箱宽度，使得大多数分箱跨越很宽的 $X_i$ 范围（例如，分箱宽度远大于局部多项式所用的带宽 $h$），因为较大的分箱可以减少分箱均值中的噪声，并使绘制的曲线在 $x = c$ 附近更平滑。\n\nD. 目视检查叠加的局部拟合在远离 $x = c$ 的地方是否平滑，以及在 $x = c$ 处 $Y_i$ 是否存在明显的离散跳跃，同时，附带的图表显示，对于预先确定的协变量或 $X_i$ 的密度，在 $x = c$ 处没有可见的不连续性；$X_i$ 在 $c$ 附近缺乏聚集现象支持了设计的有效性。\n\nE. 为了避免图中分界点处的选择偏差，丢弃所有满足 $|X_i - c| \\le \\delta$ (对于某个小的 $\\delta > 0$) 的观测值，这样绘制的不连续性就不会由靠近 $c$ 的特殊观测值驱动。\n\nF. 在每一侧使用分位数等距的分箱（即每个分箱的观测数大致相等）是可以接受的；计算分箱内 $Y_i$ 的均值，并根据分箱内 $X_i$ 的均值或中位数进行绘制，同时确保分箱不跨越 $c$。作为稳健性检验，应使用几个合理的带宽 $h$ 和多项式次数重复绘图，以评估表观的跳跃和远离 $c$ 处的平滑性是否稳定。\n\n选择所有适用项。",
            "solution": "问题陈述描述了一个清晰回归断点（RD）设计，这是一种用于因果推断的准实验方法。该设置在科学上是有效的且定义明确。RD设计的核心是比较驱动变量 $X_i$ 恰好在分界点 $c$ 以下的观测值与恰好在分界点 $c$ 以上的观测值。关键的识别假设是潜在结果的条件期望 $\\mathbb{E}[Y_i(d) \\mid X_i = x]$ 在 $x = c$ 处的连续性。目标是评估构建和解释RD图的程序，RD图是可视化处理效应和评估设计合理性的主要工具。\n\n让我们根据回归断点分析的既定原则来分析每个选项。\n\n**A. 创建不跨越 $c$ 的驱动变量 $X_i$ 的分箱；在每个分箱内，计算 $Y_i$ 的均值，并在该分箱的中点处绘制该均值。在 $c$ 的左侧和右侧叠加独立的局部多项式拟合（例如，局部线性拟合），该拟合仅使用相应一侧满足 $|X_i - c| \\le h$ 的观测值进行估计，并使用核权重来降低离 $c$ 较远的点的影响，同时不强制曲线在 $x = c$ 处相遇。**\n\n这个选项精确而正确地描述了创建RD图的现代最佳实践。\n1.  **分箱**：原始数据通常过于密集，难以有效可视化。将数据分箱到驱动变量 $X_i$ 的离散区间中，并为每个分箱绘制平均结果 $\\bar{Y}$ 是一种标准技术。关键在于任何分箱都不能跨越分界点 $c$，因为这会混合处理组和未处理组的单位，从而掩盖不连续性。在分箱的中点（或者更准确地说，在分箱内 $X_i$ 的均值处）绘制分箱均值是正确的程序。\n2.  **局部多项式回归**：用于估计分界点两侧回归函数的最先进方法是局部多项式回归（最常见的是局部线性回归，即多项式次数 $p=1$）。这种方法优于全局多项式，因为RD估计量是在点 $x=c$ 处定义的*局部*参数。\n3.  **独立拟合**：回归函数必须对 $X_i  c$ 和 $X_i \\ge c$ 的数据分开估计。强制曲线在 $c$ 处连续将意味着假设没有处理效应的原假设（$\\tau=0$），这会违背分析的目的。\n4.  **带宽和核函数**：局部多项式回归是在分界点的一个带宽 $h$ 内的数据子集上执行的（$c-h \\le X_i  c$ 和 $c \\le X_i \\le c+h$）。通常应用核权重，以给予更靠近分界点 $c$ 的观测值更大的影响，这是该估计量局部性质的核心。\n\n整个程序旨在为分界点处的跳跃提供一个稳健的视觉和定量估计。\n\n**对A的结论：正确。**\n\n**B. 汇集 $c$ 两侧的所有观测值，并在 $X_i$ 的整个支撑集上拟合一个次数为 $k \\ge 4$ 的高阶全局多项式；使用拟合的曲线推断 $c$ 处的跳跃，因为更高次的多项式可以灵活地在任何地方近似条件均值函数。**\n\n这描述了一种过时的、现在被强烈不推荐的方法。虽然理论上可以使用带有处理指标的全局多项式来建模不连续性（例如，$Y_i = P_k(X_i) + \\tau D_i + \\epsilon_i$），但这种方法已被证明是高度不可靠的。\n1.  **不稳定性**：高阶多项式（$k \\ge 2$，尤其是 $k \\ge 4$）是众所周知的容易产生人为的“摆动”，并在数据支撑集的边界附近表现不佳。估计的跳跃 $\\hat{\\tau}$ 对多项式次数 $k$ 的选择可能极其敏感。\n2.  **局部与全局**：RD效应本质上是分界点 $c$ 处的一个局部现象。全局多项式使用远离分界点的观测值来推断分界点处的函数形状，这可能导致显著的偏差。如选项A所述的局部多项式方法更稳健，并且与估计量的局部性质更一致。该领域的顶尖方法学家强烈建议不要在RD分析中使用高阶全局多项式。\n\n**对B的结论：不正确。**\n\n**C. 设置足够大的分箱宽度，使得大多数分箱跨越很宽的 $X_i$ 范围（例如，分箱宽度远大于局部多项式所用的带宽 $h$），因为较大的分箱可以减少分箱均值中的噪声，并使绘制的曲线在 $x = c$ 附近更平滑。**\n\n这个程序是不明智的。虽然较大的分箱确实可以减少分箱均值的方差（减少“噪声”），但这样做的代价是引入了巨大的偏差。RD图旨在可视化分界点周围条件均值函数的局部行为。使用非常大的分箱宽度会将结果 $Y_i$ 在很宽的 $X_i$ 值范围内取平均。如果基础函数 $\\mathbb{E}[Y_i \\mid X_i = x]$ 不是平坦的，这种平均将产生一个不能准确代表该函数在分箱中心值的箱均值。这在分界点附近尤其成问题，因为大的分箱可能会模糊并隐藏我们希望观察到的不连续性。分箱是一种视觉辅助工具；其分辨率应足够精细以追踪局部多项式拟合，而不是粗糙到与之相矛盾。使用比估计带宽 $h$ 宽得多的分箱在概念上是不一致的。\n\n**对C的结论：不正确。**\n\n**D. 目视检查叠加的局部拟合在远离 $x = c$ 的地方是否平滑，以及在 $x = c$ 处 $Y_i$ 是否存在明显的离散跳跃，同时，附带的图表显示，对于预先确定的协变量或 $X_i$ 的密度，在 $x = c$ 处没有可见的不连续性；$X_i$ 在 $c$ 附近缺乏聚集现象支持了设计的有效性。**\n\n这个选项正确地描述了RD图的解释和必要的附带诊断检验。\n1.  **结果图检查**：如果存在处理效应，$Y_i$ 对 $X_i$ 的主图应在 $c$ 处显示出清晰的跳跃，并且两侧的函数应相当平滑，没有其他跳跃或奇怪的模式。\n2.  **协变量平衡检验**：一个关键的有效性检查是为预先确定的协变量（在处理决定之前其值已确定的变量，例如性别、基线健康状况）创建RD图。连续性假设意味着潜在结果的分布在阈值附近是平滑的。推而广之，任何预先确定的特征的分布也应该是平滑的。协变量在 $c$ 处的跳跃表明，分界点上方和下方的人群在处理分配之外的其他方面存在差异，这削弱了因果解释。\n3.  **密度检验（操纵检验）**：另一个关键检验是检查驱动变量 $X_i$ 的密度。如果个体能够精确地操纵他们的 $X_i$ 值以获得或避免处理，我们可能会看到一个非随机的排序模式，例如在 $c$ 下方密度下降，在 $c$ 上方密度激增。这将违反个体无法完美控制驱动变量的假设。平滑地穿过分界点的密度支持了设计的有效性。“缺乏聚集”与平滑的密度是同义词。\n\n**对D的结论：正确。**\n\n**E. 为了避免图中分界点处的选择偏差，丢弃所有满足 $|X_i - c| \\le \\delta$ (对于某个小的 $\\delta  0$) 的观测值，这样绘制的不连续性就不会由靠近 $c$ 的特殊观测值驱动。**\n\n这描述了一种“甜甜圈洞”RD。这不是一种标准或推荐的主要分析策略。RD的全部逻辑依赖于分界点*两侧*个体的可比性。丢弃这些信息最丰富的观测值，就等于扔掉了识别策略核心的数据。如果回归函数不是平坦的，这种方法会从根本上改变估计量，并可能引入其自身的偏差。虽然在极少数情况下，当对分界点附近的数据质量有特定担忧时，它可能被用作补充的敏感性分析，但将其框定为“避免选择偏差”的标准方法是不正确的。标准的RD框架已经通过连续性假设解释了基于 $X_i$ 的处理选择。\n\n**对E的结论：不正确。**\n\n**F. 在每一侧使用分位数等距的分箱（即每个分箱的观测数大致相等）是可以接受的；计算分箱内 $Y_i$ 的均值，并根据分箱内 $X_i$ 的均值或中位数进行绘制，同时确保分箱不跨越 $c$。作为稳健性检验，应使用几个合理的带宽 $h$ 和多项式次数重复绘图，以评估表观的跳跃和远离 $c$ 处的平滑性是否稳定。**\n\n这个选项描述了RD分析中其他有效且重要的程序。\n1.  **分位数等距分箱**：与等宽分箱（如果数据稀疏，可能导致某些分箱观测值很少）不同，创建包含相同数量观测值的分箱是一种常见且可接受的做法。这有助于在 $X_i$ 的支撑集上稳定分箱均值的方差。与任何分箱方法一样，必须将分箱的结果均值与分箱的 $X_i$ 均值进行绘图，并确保没有分箱跨越分界点 $c$。\n2.  **稳健性检验**：一个可信的RD分析必须证明其主要发现不过分依赖于任意的模型选择。因此，通过使用不同的带宽（$h$）和不同的局部多项式次数（例如，比较局部线性，$p=1$，与局部二次，$p=2$）重新估计效应来检查结果的稳健性是至关重要的。如果估计的跳跃和图的整体形状在这些变化中保持稳定，就会增加对研究结果的信心。\n\n**对F的结论：正确。**",
            "answer": "$$\\boxed{ADF}$$"
        },
        {
            "introduction": "回归不连续设计的核心思想是利用断点处的局部比较。本编码练习将通过一个强有力的动手实践，展示为什么局部估计方法至关重要。通过比较一个正确的局部线性估计器和一个不恰当的全局平滑器，你将亲眼看到忽略设计的局部性如何完全掩盖真实的处理效应。",
            "id": "3168530",
            "problem": "给定一个驱动变量 $X$、一个结果变量 $Y$ 和一个断点 $c$。考虑一个断点回归（RD）设定，其中处理指标为 $T = \\mathbf{1}\\{X \\ge c\\}$，结果变量 $Y$ 遵循结构模型 $Y = g(X) + \\tau T + \\varepsilon$，其中 $g$ 是一个平滑的基线函数，$\\tau$ 是目标 RD 跳跃，$\\varepsilon$ 是噪声。RD 估计量定义为在断点处的条件期望之差，\n$$\n\\tau = \\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x].\n$$\n你将构建一个合成示例，其中在 $X$ 上应用局部加权散点平滑（LOESS）会隐藏在 $c$ 处的断点，并且你将量化全局平滑器对 RD 的风险。\n\n数据生成过程定义如下：\n- 驱动变量是 $X \\sim \\text{Uniform}([-1, 1])$，每个观测值都是独立的。\n- 基线函数是 $g(x) = x^2$。\n- 断点是 $c = 0$。\n- 真实的 RD 跳跃是 $\\tau = 1$。\n- 噪声是 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$，并且是独立的。\n\n将 LOESS 预测器 $\\hat{f}(x)$ 定义为在目标点 $x_0$ 处使用比例为 $s$ 的最近邻进行加权局部线性回归的解。对于每个 $x_0$：\n1. 对于所有观测到的 $x_i$，计算距离 $d_i = |x_i - x_0|$。\n2. 令 $k = \\lceil s n \\rceil$，其中 $n$ 是样本量，并令 $d_{\\max}$ 为第 $k$ 小的 $d_i$。\n3. 为 $d_i \\le d_{\\max}$ 的点定义三立方权重 $w_i = \\left(1 - \\left(\\frac{d_i}{d_{\\max}}\\right)^3\\right)^3$，否则 $w_i = 0$。\n4. 通过最小化 $\\sum_i w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$ 来拟合加权局部线性模型 $y_i = \\beta_0 + \\beta_1 x_i$，并设置 $\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0$。\n\n由于 LOESS 对断点两侧的数据拟合一条单一的平滑曲线，拟合函数 $\\hat{f}(x)$ 在 $x$ 上是连续的。因此，度量为\n$$\n\\hat{J}_{\\text{LOESS}} = \\hat{f}(c + \\epsilon) - \\hat{f}(c - \\epsilon)\n$$\n的“可见”断点，对于一个小的 $\\epsilon  0$，其值预计约为 $0$，而不管真实的跳跃 $\\tau$ 是多少，从而隐藏了断点。\n\n作为一个有原则的 RD 估计器，通过在断点两侧的带宽 $h$ 范围内拟合两个独立的普通最小二乘法（OLS）回归来定义局部线性 RD 估计器：\n- 左侧回归使用 $x \\in [c - h, c)$ 的数据并拟合 $y = \\alpha_- + \\beta_- x$，得到 $\\hat{m}_-(c) = \\hat{\\alpha}_- + \\hat{\\beta}_- c$。\n- 右侧回归使用 $x \\in [c, c + h]$ 的数据并拟合 $y = \\alpha_+ + \\beta_+ x$，得到 $\\hat{m}_+(c) = \\hat{\\alpha}_+ + \\hat{\\beta}_+ c$。\n- RD 估计值为 $\\hat{\\tau} = \\hat{m}_+(c) - \\hat{m}_-(c)$。\n\n你的程序必须：\n1. 使用指定的参数为每个测试用例生成数据。\n2. 使用上述 LOESS 过程，根据跨度 $s$ 和小偏移量 $\\epsilon$ 计算 $\\hat{J}_{\\text{LOESS}}$。\n3. 使用带宽 $h$ 计算局部线性 RD 估计值 $\\hat{\\tau}$。\n4. 计算绝对偏差 $B_L = |\\hat{J}_{\\text{LOESS}} - \\tau|$ 和 $B_R = |\\hat{\\tau} - \\tau|$。\n5. 对于每个测试用例，输出比率 $R = \\frac{B_L}{B_R}$。\n\n解释：当 $R$ 大于 $1$ 时，基于 LOESS 的可见跳跃比局部线性 RD 估计更具偏差，这说明了在断点处强制连续性并可能隐藏真实断点的全局平滑器的风险。\n\n测试套件（每个元组为 $(n, \\sigma, s, h, \\epsilon, \\text{seed})$）：\n- 用例 1：$(500, 0.1, 0.9, 0.2, 0.001, 17)$。\n- 用例 2：$(500, 1.0, 0.9, 0.2, 0.001, 23)$。\n- 用例 3：$(300, 0.1, 0.5, 0.15, 0.001, 31)$。\n- 用例 4：$(300, 0.1, 0.2, 0.1, 0.001, 47)$。\n\n最终输出格式：你的程序应生成单行输出，其中包含所有测试用例的比率，格式为逗号分隔的列表，并用方括号括起来，例如 $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$。不允许有其他输出。",
            "solution": "用户提供的问题是统计学习和计量经济学领域中一个明确定义的模拟练习，特别关注断点回归（RD）设计。该问题在科学上是合理的、自洽的，并且在算法上是明确的。它要求比较两种断点估计器：一种是基于全局平滑器（LOESS），已知其在这种情况下表现不佳；另一种是基于为 RD 分析量身定制的有原则的局部线性回归方法。\n\n问题陈述要求我们通过模拟来验证这种理论行为。我们将遵循针对几个测试用例的指定步骤：数据生成、用两种方法进行估计，以及计算它们的绝对偏差之比。\n\n### 步骤 1：数据生成\n对于每个测试用例，我们给定一个样本量 $n$、一个噪声标准差 $\\sigma$ 和一个随机种子。我们根据指定的数据生成过程（DGP）生成数据：\n- 驱动变量 $X$ 从 $[-1, 1]$ 上的均匀分布中抽取。\n- 处理分配 $T$ 是基于 $X$ 和断点 $c = 0$ 的确定性分配，其中 $T = \\mathbf{1}\\{X \\ge c\\}$。\n- 结果变量 $Y$ 由结构模型 $Y = g(X) + \\tau T + \\varepsilon$ 生成，其中基线是 $g(x) = x^2$，真实的处理效应（跳跃）是 $\\tau = 1$，噪声项 $\\varepsilon$ 从正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽取。\n\n### 步骤 2：计算基于 LOESS 的可见跳跃 $\\hat{J}_{\\text{LOESS}}$\n问题定义了一个基于局部加权散点平滑（LOESS）的“可见”跳跃。LOESS 在 $X$ 的整个范围内拟合一个单一的平滑函数 $\\hat{f}(x)$，忽略了在断点 $c$ 处的断点。在目标点 $x_0$ 计算 LOESS 预测值 $\\hat{f}(x_0)$ 的过程如下：\n1.  对于给定的跨度参数 $s$，确定要使用的最近邻的数量：$k = \\lceil sn \\rceil$。\n2.  根据距离 $|x_i - x_0|$，找出离 $x_0$ 最近的 $k$ 个数据点 $(x_i, y_i)$。令这些邻居中最远者的距离为 $d_{\\max}$。\n3.  为每个数据点 $i$ 分配一个三立方权重：对于邻域内的点 ($|x_i - x_0| \\le d_{\\max}$)，$w_i = \\left(1 - \\left(\\frac{|x_i - x_0|}{d_{\\max}}\\right)^3\\right)^3$，对所有其他点，$w_i = 0$。\n4.  通过最小化加权残差平方和 $\\sum_i w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$，拟合一个加权局部线性回归模型 $y_i = \\beta_0 + \\beta_1 x_i + e_i$。\n5.  $x_0$ 处的 LOESS 预测值为 $\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0$。\n\n然后，通过计算跨越断点的两个点的 LOESS 预测值之差来计算“可见跳跃”：对于一个小的 $\\epsilon0$，$\\hat{J}_{\\text{LOESS}} = \\hat{f}(c + \\epsilon) - \\hat{f}(c - \\epsilon)$。由于 $\\hat{f}(x)$ 是一个连续函数，这个量预计会接近 $0$。\n\n### 步骤 3：计算局部线性 RD 估计值 $\\hat{\\tau}$\n这是 RD 估计的标准、有原则的方法。它通过在断点 $c$ 的两侧拟合独立的模型来承认断点的存在：\n1.  **断点左侧：** 使用 $x \\in [c-h, c)$ 的数据，我们拟合一个普通最小二乘法（OLS）回归 $y = \\alpha_- + \\beta_- x$。从左侧在断点处的预测值为 $\\hat{m}_-(c) = \\hat{\\alpha}_- + \\hat{\\beta}_- c$。\n2.  **断点右侧：** 使用 $x \\in [c, c+h]$ 的数据，我们拟合另一个 OLS 回归 $y = \\alpha_+ + \\beta_+ x$。从右侧在断点处的预测值为 $\\hat{m}_+(c) = \\hat{\\alpha}_+ + \\hat{\\beta}_+ c$。\n3.  RD 估计值是这两个预测值之差：$\\hat{\\tau} = \\hat{m}_+(c) - \\hat{m}_-(c)$。由于 $c=0$，这可以简化为 $\\hat{\\tau} = \\hat{\\alpha}_+ - \\hat{\\alpha}_-$，即两个局部回归的截距之差。\n\n### 步骤 4：偏差和比率计算\n在真实跳跃 $\\tau=1$ 的情况下，我们计算两种估计器的绝对偏差：\n- LOESS 偏差：$B_L = |\\hat{J}_{\\text{LOESS}} - \\tau|$。\n- RD 估计器偏差：$B_R = |\\hat{\\tau} - \\tau|$。\n\n每个测试用例的最终输出是这些偏差的比率，$R = B_L / B_R$。一个大的 $R$ 值表明基于 LOESS 的方法偏差要大得多，并有效地隐藏了真实的断点，从而证实了在 RD 背景下“全局平滑器的风险”。\n整个过程在提供的 Python 代码中实现，该代码遍历指定的测试用例并为每个用例计算比率 $R$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Regression Discontinuity problem by comparing a LOESS-based approach\n    with a standard local linear RD estimator.\n    \"\"\"\n\n    def _loess_predict(x0, X, Y, s):\n        \"\"\"\n        Computes the LOESS prediction for a single point x0 based on the provided\n        data (X, Y) and span s.\n        \"\"\"\n        n = len(X)\n        k = int(np.ceil(s * n))\n\n        # Clamp k to be within valid range [1, n] for array indexing.\n        if k > n: k = n\n        if k  1: k = 1\n\n        # 1. Compute distances and find the k-th neighbor's distance (d_max).\n        distances = np.abs(X - x0)\n        sorted_distances = np.sort(distances)\n        d_max = sorted_distances[k-1]\n        \n        # 2. Define tricube weights.\n        weights = np.zeros(n)\n        if d_max > 0:\n            u = distances / d_max\n            mask = u = 1.0\n            weights[mask] = (1 - u[mask]**3)**3\n        else:  # d_max is 0, only points at x0 get weight 1.\n            weights = (distances == 0).astype(float)\n\n        # 3. Fit weighted local linear model.\n        valid_mask = weights > 0\n        \n        # Fallback for insufficient data points within the window.\n        if np.sum(valid_mask) == 0:\n            return Y[np.argmin(distances)] # Return value of the single closest point.\n        \n        X_w = X[valid_mask]\n        Y_w = Y[valid_mask]\n        weights_w = weights[valid_mask]\n        \n        # If not enough unique X values, fall back to local constant model (weighted mean).\n        if len(np.unique(X_w))  2:\n            return np.average(Y_w, weights=weights_w)\n            \n        X_des = np.vstack([np.ones_like(X_w), X_w]).T\n        \n        # WLS can be solved by transforming to an OLS problem.\n        W_sqrt = np.sqrt(weights_w)\n        X_prime = X_des * W_sqrt[:, np.newaxis]\n        Y_prime = Y_w * W_sqrt\n        \n        # np.linalg.lstsq is robust and handles rank-deficient cases.\n        beta_hat = np.linalg.lstsq(X_prime, Y_prime, rcond=None)[0]\n        prediction = beta_hat[0] + beta_hat[1] * x0\n\n        return prediction\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, sigma, s, h, epsilon, seed)\n        (500, 0.1, 0.9, 0.2, 0.001, 17),\n        (500, 1.0, 0.9, 0.2, 0.001, 23),\n        (300, 0.1, 0.5, 0.15, 0.001, 31),\n        (300, 0.1, 0.2, 0.1, 0.001, 47),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, sigma, s, h, epsilon, seed = case\n        \n        # --- Parameter setup ---\n        c = 0.0\n        tau_true = 1.0\n        rng = np.random.default_rng(seed)\n\n        # --- Data Generation ---\n        X = rng.uniform(-1, 1, n)\n        T = (X >= c).astype(int)\n        g_X = X**2\n        noise = rng.normal(0, sigma, n)\n        Y = g_X + tau_true * T + noise\n        \n        # --- LOESS Calculation ---\n        f_hat_plus = _loess_predict(c + epsilon, X, Y, s)\n        f_hat_minus = _loess_predict(c - epsilon, X, Y, s)\n        J_loess = f_hat_plus - f_hat_minus\n\n        # --- Local Linear RD Estimator ---\n        # Left regression\n        mask_left = (X >= c - h)  (X  c)\n        X_left, Y_left = X[mask_left], Y[mask_left]\n        if len(X_left)  2: raise ValueError(\"Not enough data points left of cutoff.\")\n        X_des_left = np.vstack([np.ones_like(X_left), X_left]).T\n        alpha_minus, beta_minus = np.linalg.lstsq(X_des_left, Y_left, rcond=None)[0]\n        m_hat_minus = alpha_minus + beta_minus * c\n\n        # Right regression\n        mask_right = (X >= c)  (X = c + h)\n        X_right, Y_right = X[mask_right], Y[mask_right]\n        if len(X_right)  2: raise ValueError(\"Not enough data points right of cutoff.\")\n        X_des_right = np.vstack([np.ones_like(X_right), X_right]).T\n        alpha_plus, beta_plus = np.linalg.lstsq(X_des_right, Y_right, rcond=None)[0]\n        m_hat_plus = alpha_plus + beta_plus * c\n        \n        tau_hat = m_hat_plus - m_hat_minus\n        \n        # --- Bias and Ratio Calculation ---\n        B_L = np.abs(J_loess - tau_true)\n        B_R = np.abs(tau_hat - tau_true)\n        \n        if B_R == 0.0:\n            R = 1.0 if B_L == 0.0 else np.inf\n        else:\n            R = B_L / B_R\n            \n        results.append(R)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "本练习将理论应用于一个真实的医疗场景，让你亲手实现一个标准的局部线性RDD估计器。你还将探索应用研究中的一个关键环节：检验结果的稳健性。我们将通过在模型中加入一个重要的协变量，来观察它如何影响我们的估计结果以及我们对结论的信心。",
            "id": "3168528",
            "problem": "考虑一个在医疗分诊场景中的清晰回归断点设计 (RDD)，在该场景中，重症监护是根据一个阈值规则分配的。设驱动变量为临床风险评分 $X \\in \\mathbb{R}$，断点为 $c \\in \\mathbb{R}$，并定义处理指标为 $T = \\mathbb{1}\\{X \\geq c\\}$。结果 $Y \\in \\{0,1\\}$ 表示死亡率（其中 $Y=1$ 表示死亡），风险调整协变量 $Z \\in \\mathbb{R}$ 概括了基线风险。感兴趣的因果估计量是断点处的局部平均处理效应，即 $Y$ 的条件期望在 $X=c$ 处的跳跃。您的任务是实现一个局部线性估计量，使用核加权最小二乘法来估计断点处的不连续性，包括不使用 $Z$ 进行风险调整和使用 $Z$ 进行风险调整两种情况，然后评估估计效应对于此调整的稳健性。\n\n该设计的基本基础如下：\n- 令 $Y(1)$ 和 $Y(0)$ 分别表示处理和控制下的潜在结果，并假设稳定单位处理价值假定 (SUTVA)。在清晰分配机制 $T=\\mathbb{1}\\{X \\ge c\\}$ 和条件期望 $\\mathbb{E}[Y(0)\\mid X=x]$ 与 $\\mathbb{E}[Y(1)\\mid X=x]$ 在 $x=c$ 处连续的连续性假设下，断点处的局部平均处理效应等于 $\\mathbb{E}[Y\\mid X=x]$ 在 $x=c$ 处的不连续性。\n- 局部线性估计量通过中心化驱动变量 $R = X - c$ 中的线性函数来近似 $c$ 两侧的条件期望，该拟合在带宽 $h0$ 内进行，并使用核权重来降低离断点较远的数据点权重。\n\n实现要求：\n- 在带宽 $h$ 内，使用三角核权重 $K(u) = (1 - |u|)\\mathbb{1}\\{|u|\\le 1\\}$，其中 $u = R/h$，因此点权重为 $w_i = (1 - |R_i|/h)\\mathbb{1}\\{|R_i|\\le h\\}$。\n- 对满足 $|R|\\le h$ 的观测值，通过加权最小二乘法拟合以下局部线性设定：\n  1. 未调整：将 $Y$ 对列 $(1, T, R, T\\cdot R)$ 进行回归，并将 $T$ 的系数作为估计的不连续性。\n  2. 风险调整后：将 $Y$ 对 $(1, T, R, T\\cdot R, Z)$ 进行回归，并再次将 $T$ 的系数作为估计的不连续性。\n- 对于每个测试用例，报告三个量：未调整的估计值（一个浮点数）、风险调整后的估计值（一个浮点数）和一个稳健性指标（一个布尔值）。如果调整后和未调整的估计值之间的绝对差小于或等于容忍度 $\\delta0$，则该指标为真，否则为假。\n\n测试套件和输入：\n对于每个测试用例，您将获得 $X$、$Y$、$Z$ 的数组、断点 $c$、带宽 $h$ 和稳健性容忍度 $\\delta$。这些数组是：\n\n测试用例 1（正常路径，中等带宽）：\n- $X = [\\,45,\\,47,\\,49,\\,50,\\,51,\\,53,\\,55,\\,46,\\,48,\\,52,\\,54,\\,44,\\,56,\\,57,\\,43,\\,58\\,]$\n- $Y = [\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0\\,]$\n- $Z = [\\,0.6,\\,0.5,\\,0.55,\\,0.58,\\,0.62,\\,0.65,\\,0.7,\\,0.52,\\,0.57,\\,0.63,\\,0.66,\\,0.48,\\,0.72,\\,0.74,\\,0.46,\\,0.76\\,]$\n- $c = 50$, $h = 5$, $\\delta = 0.05$。\n\n测试用例 2（边界情况，小带宽）：\n- $X = [\\,45,\\,47,\\,49,\\,50,\\,51,\\,53,\\,55,\\,46,\\,48,\\,52,\\,54,\\,44,\\,56,\\,57,\\,43,\\,58\\,]$\n- $Y = [\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0\\,]$\n- $Z = [\\,0.6,\\,0.5,\\,0.55,\\,0.58,\\,0.62,\\,0.65,\\,0.7,\\,0.52,\\,0.57,\\,0.63,\\,0.66,\\,0.48,\\,0.72,\\,0.74,\\,0.46,\\,0.76\\,]$\n- $c = 50$, $h = 2$, $\\delta = 0.10$。\n\n测试用例 3（边缘情况，强协变量影响）：\n- $X = [\\,48,\\,49,\\,50,\\,51,\\,52,\\,53,\\,47,\\,46,\\,54,\\,55,\\,45,\\,56\\,]$\n- $Y = [\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,0,\\,0,\\,1,\\,0\\,]$\n- $Z = [\\,0.9,\\,0.85,\\,0.88,\\,0.86,\\,0.83,\\,0.8,\\,0.92,\\,0.95,\\,0.78,\\,0.75,\\,0.97,\\,0.74\\,]$\n- $c = 50$, $h = 6$, $\\delta = 0.08$。\n\n算法规格：\n- 构建中心化的驱动变量 $R = X - c$。\n- 计算权重 $w_i = (1 - |R_i|/h)\\mathbb{1}\\{|R_i|\\le h\\}$ 并将分析限制在 $w_i0$ 的观测值上。\n- 如上所述，为未调整和调整后的模型构建设计矩阵 $D$。\n- 使用 Moore–Penrose 伪逆计算加权最小二乘估计量，以处理近奇异矩阵：估计 $\\hat{\\beta} = (D^{\\top} W D)^{+} D^{\\top} W Y$，其中 $W$ 是权重的对角矩阵，$(\\cdot)^{+}$ 表示伪逆。提取 $T$ 上的系数作为估计的不连续性。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含所有三个测试用例的连接结果，形式为方括号括起来的逗号分隔列表。顺序必须是，对于测试用例 1、测试用例 2、然后测试用例 3：$[\\text{unadjusted}_1,\\text{adjusted}_1,\\text{robust}_1,\\text{unadjusted}_2,\\text{adjusted}_2,\\text{robust}_2,\\text{unadjusted}_3,\\text{adjusted}_3,\\text{robust}_3]$。布尔值必须以编程语言的原生布尔表示（true 或 false）打印，浮点数必须以十进制形式打印。不涉及物理单位或角度单位。百分比（如有）必须表示为小数。",
            "solution": "该问题被评估为**有效的**。其科学基础在于因果推断和统计学习的原理，特别是回归断点设计 (RDD)。该问题是适定的，提供了所有必要的数据、参数以及一个清晰、数学上合理的估计算法。语言客观精确，设定自成一体且一致。\n\n任务是使用局部线性估计量来估计清晰 RDD 断点处的局部平均处理效应 (LATE)。这在两种设定下进行：一个未调整模型和一个针对风险协变量 $Z$ 调整的模型。然后评估估计对于此调整的稳健性。估计过程基于核加权最小二乘法。\n\n分步流程如下：\n\n1.  **数据筛选和准备**：对于每个观测值 $i$，我们首先计算中心化的驱动变量 $R_i = X_i - c$，其中 $X_i$ 是临床风险评分，$c$ 是断点。分析被限制在断点周围的一个局部邻域内，该邻域由带宽 $h  0$ 定义。问题指定使用三角核，权重由 $w_i = (1 - |R_i|/h)\\mathbb{1}\\{|R_i|\\le h\\}$ 给出。“限制于 $w_i  0$ 的观测值”这一指令意味着只有满足 $|R_i|  h$ 的观测值才被包含在回归中。对于这些选定的观测值，我们定义处理指标 $T_i = \\mathbb{1}\\{X_i \\geq c\\}$，如果观测值被处理则为 $1$，否则为 $0$。\n\n2.  **未调整的局部线性 RDD 估计**：第一个模型在不调整协变量 $Z$ 的情况下估计处理效应。我们在带宽内拟合一个加权最小二乘 (WLS) 回归。模型设定为：\n    $$\n    Y_i = \\beta_0 + \\tau T_i + \\beta_1 R_i + \\beta_2 (T_i \\cdot R_i) + \\epsilon_i\n    $$\n    该模型在断点两侧分别拟合了线性趋势。参数 $\\tau$ 代表了在断点 $c$ (其中 $R_i=0$) 处截距的跳跃，这是 LATE 的 RDD 估计值。回归变量是一个截距 ($1$)、处理指标 ($T$)、中心化的驱动变量 ($R$) 和一个交互项 ($T \\cdot R$)。未调整模型的设计矩阵是 $D_{\\text{unadj}} = [1, T, R, T \\cdot R]$。\n\n3.  **风险调整后的局部线性 RDD 估计**：第二个模型将基线风险协变量 $Z$ 作为额外的线性控制变量包含进来。这样做是为了评估处理效应估计是否对包含其他预后因素敏感。模型设定为：\n    $$\n    Y_i = \\beta_0 + \\tau_{adj} T_i + \\beta_1 R_i + \\beta_2 (T_i \\cdot R_i) + \\beta_3 Z_i + \\nu_i\n    $$\n    感兴趣的参数同样是处理指标上的系数 $\\tau_{adj}$。包含 $Z$ 可以提高估计的精度并检验稳健性。调整后模型的设计矩阵是 $D_{\\text{adj}} = [1, T, R, T \\cdot R, Z]$。\n\n4.  **加权最小二乘 (WLS) 计算**：两个模型都使用 WLS 进行估计。系数向量 $\\hat{\\beta}$ 使用问题陈述中指定的公式计算，该公式涉及 Moore-Penrose 伪逆 $(\\cdot)^{+}$ 以确保稳定性，尤其是在存在共线性或观测值很少的情况下：\n    $$\n    \\hat{\\beta} = (D^{\\top} W D)^{+} D^{\\top} W Y\n    $$\n    这里，$Y$ 是筛选后观测值的结果向量，$D$ 是相应的设计矩阵（$D_{\\text{unadj}}$ 或 $D_{\\text{adj}}$），$W$ 是一个对角矩阵，其对角线上是三角核权重 $w_i$。估计的处理效应（$\\hat{\\tau}$ 或 $\\hat{\\tau}_{adj}$）是得到的 $\\hat{\\beta}$ 向量的第二个元素，对应于处理指标 $T$ 的系数。\n\n5.  **稳健性评估**：最后一步是比较未调整的估计值 $\\hat{\\tau}_{\\text{unadj}}$ 与风险调整后的估计值 $\\hat{\\tau}_{\\text{adj}}$。如果两者之间的绝对差在给定的容忍度 $\\delta$ 之内，则认为估计是稳健的：\n    $$\n    \\text{robust} = (|\\hat{\\tau}_{\\text{adj}} - \\hat{\\tau}_{\\text{unadj}}| \\le \\delta)\n    $$\n    这提供了一个关于研究发现稳定性的布尔指标。对于每个测试用例，最终输出将是三元组 $(\\hat{\\tau}_{\\text{unadj}}, \\hat{\\tau}_{\\text{adj}}, \\text{robust})$。",
            "answer": "```python\nimport numpy as np\n\ndef _calculate_rdd_estimate(X: np.ndarray, Y: np.ndarray, Z: np.ndarray, c: float, h: float, adjusted: bool) -> float:\n    \"\"\"\n    Calculates the RDD estimate using local linear regression via WLS.\n    \"\"\"\n    # 1. Center the running variable\n    R = X - c\n    \n    # 2. Filter data to be strictly within the bandwidth (where weight w_i > 0)\n    mask = np.abs(R)  h\n    \n    # If no data points are within the bandwidth, return NaN.\n    if not np.any(mask):\n        return np.nan\n\n    R_filt = R[mask]\n    Y_filt = Y[mask]\n    X_filt = X[mask]\n    Z_filt = Z[mask]\n\n    # 3. Compute triangular kernel weights\n    weights = 1 - np.abs(R_filt) / h\n    W = np.diag(weights)\n\n    # 4. Construct the design matrix D\n    intercept = np.ones_like(R_filt)\n    T_filt = (X_filt >= c).astype(float)\n    TR_interaction = T_filt * R_filt\n\n    if adjusted:\n        # Columns: (1, T, R, T*R, Z)\n        D = np.stack([intercept, T_filt, R_filt, TR_interaction, Z_filt], axis=1)\n    else:\n        # Columns: (1, T, R, T*R)\n        D = np.stack([intercept, T_filt, R_filt, TR_interaction], axis=1)\n\n    # Check for underdetermined system (fewer obs than params)\n    # The use of pinv is specified to handle this case.\n    n_obs, n_params = D.shape\n    if n_obs == 0:\n        return np.nan\n\n    # 5. Compute the WLS estimator using the Moore-Penrose pseudoinverse\n    try:\n        D_T_W = D.T @ W\n        # The core formula: beta = (D'WD)^+ D'WY\n        pinv_term = np.linalg.pinv(D_T_W @ D)\n        beta_hat = pinv_term @ D_T_W @ Y_filt\n    except np.linalg.LinAlgError:\n        # This branch is unlikely with pinv but included for robustness\n        return np.nan\n\n    # 6. The RDD estimate is the coefficient on the treatment indicator T, which is the second regressor (index 1).\n    rdd_estimate = beta_hat[1]\n    \n    return rdd_estimate\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": np.array([45, 47, 49, 50, 51, 53, 55, 46, 48, 52, 54, 44, 56, 57, 43, 58]),\n            \"Y\": np.array([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0]),\n            \"Z\": np.array([0.6, 0.5, 0.55, 0.58, 0.62, 0.65, 0.7, 0.52, 0.57, 0.63, 0.66, 0.48, 0.72, 0.74, 0.46, 0.76]),\n            \"c\": 50.0,\n            \"h\": 5.0,\n            \"delta\": 0.05\n        },\n        {\n            \"X\": np.array([45, 47, 49, 50, 51, 53, 55, 46, 48, 52, 54, 44, 56, 57, 43, 58]),\n            \"Y\": np.array([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0]),\n            \"Z\": np.array([0.6, 0.5, 0.55, 0.58, 0.62, 0.65, 0.7, 0.52, 0.57, 0.63, 0.66, 0.48, 0.72, 0.74, 0.46, 0.76]),\n            \"c\": 50.0,\n            \"h\": 2.0,\n            \"delta\": 0.10\n        },\n        {\n            \"X\": np.array([48, 49, 50, 51, 52, 53, 47, 46, 54, 55, 45, 56]),\n            \"Y\": np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0]),\n            \"Z\": np.array([0.9, 0.85, 0.88, 0.86, 0.83, 0.8, 0.92, 0.95, 0.78, 0.75, 0.97, 0.74]),\n            \"c\": 50.0,\n            \"h\": 6.0,\n            \"delta\": 0.08\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X, Y, Z, c, h, delta = case[\"X\"], case[\"Y\"], case[\"Z\"], case[\"c\"], case[\"h\"], case[\"delta\"]\n\n        # Calculate unadjusted estimate\n        unadjusted_est = _calculate_rdd_estimate(X, Y, Z, c, h, adjusted=False)\n        \n        # Calculate risk-adjusted estimate\n        adjusted_est = _calculate_rdd_estimate(X, Y, Z, c, h, adjusted=True)\n        \n        # Assess robustness\n        is_robust = np.abs(adjusted_est - unadjusted_est) = delta\n        \n        results.extend([unadjusted_est, adjusted_est, str(is_robust).lower()])\n\n    # Final print statement in the exact required format.\n    # str() converts booleans to 'True'/'False' and floats to decimal strings.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}