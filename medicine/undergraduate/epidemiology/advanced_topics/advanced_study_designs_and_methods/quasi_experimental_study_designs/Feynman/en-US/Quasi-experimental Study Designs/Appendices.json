{
    "hands_on_practices": [
        {
            "introduction": "The Difference-in-Differences (DiD) design is a cornerstone of quasi-experimental research, prized for its intuitive approach to estimating policy effects. Its validity, however, hinges critically on the parallel trends assumption—the idea that the treatment and control groups would have followed similar trends in the absence of the intervention. This practice problem challenges you to derive the mathematical consequences of violating this assumption, providing a fundamental understanding of how unobserved differential trends can bias your results .",
            "id": "4626142",
            "problem": "Consider a canonical two-period and two-group Difference-in-Differences (DiD) setup in epidemiology, where a policy exposure affects only the treated group in the post period. Let groups be indexed by $g \\in \\{T, C\\}$ for treated and control, and time by $t \\in \\{0,1\\}$ for pre and post. Let $Y_{gt}(d)$ denote the potential outcome for group $g$ at time $t$ under exposure status $d \\in \\{0,1\\}$. Assume the Stable Unit Treatment Value Assumption (SUTVA) holds and the additive treatment response is constant for the treated group only in the post period, so that $Y_{T1}(1) - Y_{T1}(0) = \\tau$, $Y_{T0}(1) - Y_{T0}(0) = 0$, $Y_{C1}(1) - Y_{C1}(0) = 0$, and $Y_{C0}(1) - Y_{C0}(0) = 0$. The Average Treatment Effect on the Treated (ATT) is defined as $\\tau$. The Difference-in-Differences (DiD) estimator is constructed from observed outcomes $Y_{gt}$ as the post-minus-pre difference in the treated group, minus the post-minus-pre difference in the control group.\n\nSuppose the parallel trends assumption is violated by a constant difference in untreated trends between the treated and control groups, quantified by a scalar $\\Delta$, defined as\n$$\\left[E\\!\\left(Y_{T1}(0) - Y_{T0}(0)\\right)\\right] - \\left[E\\!\\left(Y_{C1}(0) - Y_{C0}(0)\\right)\\right] = \\Delta.$$\nStarting from the fundamental potential outcomes framework and the definition of the DiD estimator, derive the expected value of the DiD estimator and compute its bias with respect to $\\tau$. Show how the sign and magnitude of the bias depend on $\\Delta$. Express your final answer as a single closed-form expression in terms of $\\Delta$; do not include any units and do not round.",
            "solution": "The problem asks us to derive the expected value and bias of the Difference-in-Differences (DiD) estimator under a specified violation of the parallel trends assumption. We begin by formalizing the DiD estimator and then use the potential outcomes framework to derive its properties.\n\nLet the observed outcome for group $g \\in \\{T, C\\}$ at time $t \\in \\{0,1\\}$ be denoted by the random variable $Y_{gt}$. The DiD estimator, $\\hat{\\tau}_{\\text{DiD}}$, is constructed from the sample means of these outcomes. The estimator is given by:\n$$\n\\hat{\\tau}_{\\text{DiD}} = (\\bar{Y}_{T1} - \\bar{Y}_{T0}) - (\\bar{Y}_{C1} - \\bar{Y}_{C0})\n$$\nwhere $\\bar{Y}_{gt}$ is the sample mean outcome for group $g$ at time $t$. We are interested in the expected value of this estimator, $E[\\hat{\\tau}_{\\text{DiD}}]$. By the linearity of expectation and the property that the expectation of a sample mean is the population mean ($E[\\bar{Y}_{gt}] = E[Y_{gt}]$), we have:\n$$\nE[\\hat{\\tau}_{\\text{DiD}}] = (E[Y_{T1}] - E[Y_{T0}]) - (E[Y_{C1}] - E[Y_{C0}])\n$$\n\nNext, we relate the observed outcomes to the potential outcomes, $Y_{gt}(d)$, where $d \\in \\{0,1\\}$ indicates exposure status. The problem states that the policy exposure affects only the treated group ($T$) in the post-period ($t=1$). This defines the exposure status for each group-period combination:\n\\begin{itemize}\n    \\item Group $T$, time $t=1$: Exposed, so the observed outcome is $Y_{T1} = Y_{T1}(1)$.\n    \\item Group $T$, time $t=0$: Not exposed, so the observed outcome is $Y_{T0} = Y_{T0}(0)$.\n    \\item Group $C$, time $t=1$: Not exposed, so the observed outcome is $Y_{C1} = Y_{C1}(0)$.\n    \\item Group $C$, time $t=0$: Not exposed, so the observed outcome is $Y_{C0} = Y_{C0}(0)$.\n\\end{itemize}\nThe Stable Unit Treatment Value Assumption (SUTVA) ensures this mapping is well-defined.\n\nSubstituting the potential outcomes into the expression for the expected value of the estimator, we get:\n$$\nE[\\hat{\\tau}_{\\text{DiD}}] = (E[Y_{T1}(1)] - E[Y_{T0}(0)]) - (E[Y_{C1}(0)] - E[Y_{C0}(0)])\n$$\n\nThe problem provides a key assumption about the treatment effect. It is an additive constant, $\\tau$, for the treated group in the post-period:\n$$\nY_{T1}(1) - Y_{T1}(0) = \\tau\n$$\nThis implies $Y_{T1}(1) = Y_{T1}(0) + \\tau$. Taking the expectation of both sides gives:\n$$\nE[Y_{T1}(1)] = E[Y_{T1}(0) + \\tau] = E[Y_{T1}(0)] + \\tau\n$$\nsince $\\tau$ is a constant. The parameter of interest, the Average Treatment Effect on the Treated (ATT), is given as $\\tau$.\n\nNow we substitute this expression for $E[Y_{T1}(1)]$ back into our equation for $E[\\hat{\\tau}_{\\text{DiD}}]$:\n$$\nE[\\hat{\\tau}_{\\text{DiD}}] = ( (E[Y_{T1}(0)] + \\tau) - E[Y_{T0}(0)]) - (E[Y_{C1}(0)] - E[Y_{C0}(0)])\n$$\nRearranging the terms to isolate $\\tau$, we have:\n$$\nE[\\hat{\\tau}_{\\text{DiD}}] = \\tau + \\left[ (E[Y_{T1}(0)] - E[Y_{T0}(0)]) - (E[Y_{C1}(0)] - E[Y_{C0}(0)]) \\right]\n$$\n\nThe term in the square brackets represents the difference between the time trend for the treated group and the time trend for the control group, both under the non-exposed condition ($d=0$). The standard parallel trends assumption posits that this term is zero. However, the problem states that this assumption is violated and defines this difference as a scalar constant $\\Delta$:\n$$\n\\left[E\\!\\left(Y_{T1}(0) - Y_{T0}(0)\\right)\\right] - \\left[E\\!\\left(Y_{C1}(0) - Y_{C0}(0)\\right)\\right] = \\Delta\n$$\nSubstituting $\\Delta$ into our expression for the expected value of the estimator gives the first part of our result:\n$$\nE[\\hat{\\tau}_{\\text{DiD}}] = \\tau + \\Delta\n$$\n\nThe bias of an estimator is defined as the difference between its expected value and the true parameter it is intended to estimate. In this case, the true parameter is the ATT, $\\tau$.\n$$\n\\text{Bias}(\\hat{\\tau}_{\\text{DiD}}) = E[\\hat{\\tau}_{\\text{DiD}}] - \\tau\n$$\nSubstituting the derived expression for $E[\\hat{\\tau}_{\\text{DiD}}]$:\n$$\n\\text{Bias}(\\hat{\\tau}_{\\text{DiD}}) = (\\tau + \\Delta) - \\tau = \\Delta\n$$\n\nThis result shows that the bias of the DiD estimator is precisely equal to $\\Delta$, the magnitude of the violation of the parallel trends assumption. The sign and magnitude of the bias are therefore a direct function of $\\Delta$:\n\\begin{itemize}\n    \\item If $\\Delta > 0$, it means that the outcome for the treated group would have trended upwards more steeply than the outcome for the control group, even in the absence of treatment. The DiD estimator mistakes this differential trend for a treatment effect, leading to a positive bias. The estimator overestimates the true effect $\\tau$.\n    \\item If $\\Delta < 0$, the outcome for the treated group would have trended downwards (or less steeply upwards) relative to the control group in the absence of treatment. This leads to a negative bias, and the estimator underestimates the true effect $\\tau$.\n    \\item If $\\Delta = 0$, the parallel trends assumption holds, the bias is zero, and the DiD estimator is an unbiased estimator for the ATT, $\\tau$.\n\\end{itemize}\nThe magnitude of the bias is directly given by $|\\Delta|$. A larger absolute value of $\\Delta$ corresponds to a larger bias in the DiD estimate.\n\nThe final answer requested is the closed-form expression for the bias in terms of $\\Delta$. As derived, this is simply $\\Delta$.",
            "answer": "$$\n\\boxed{\\Delta}\n$$"
        },
        {
            "introduction": "Interrupted Time Series (ITS) analysis offers a powerful way to evaluate interventions by modeling trends in an outcome before and after a policy change. A common pitfall in ITS is failing to account for underlying patterns in the data, such as seasonality, which can lead to omitted variable bias. This exercise guides you through quantifying the bias from an unmodeled seasonal component and then asks you to specify a correctly-adjusted model, honing your skills in robust time-series model specification .",
            "id": "4626097",
            "problem": "A public health department evaluates a quasi-experimental policy that began in month $19$ of a $36$-month observation period, using an interrupted time series framework in epidemiology. Let monthly outcome $Y_{t}$ denote influenza-like illness clinic visits per $100{,}000$ population at month $t$, where $t \\in \\{1,2,\\ldots,36\\}$. The true data-generating process is\n$$\nY_{t} \\;=\\; \\alpha \\;+\\; \\beta D_{t} \\;+\\; \\delta S_{t} \\;+\\; \\varepsilon_{t},\n$$\nwhere $D_{t}$ is a deterministic treatment indicator equal to $0$ for $t \\leq 18$ and $1$ for $t \\geq 19$, $S_{t}$ is a deterministic seasonality component given by\n$$\nS_{t} \\;=\\; \\cos\\!\\left(\\frac{2\\pi t}{12}\\right),\n$$\n$\\delta$ is the magnitude of seasonal fluctuation, and $\\varepsilon_{t}$ is an error term with $\\mathbb{E}[\\varepsilon_{t} \\mid D_{t}, S_{t}] = 0$. Suppose $\\delta = 27$.\n\nAn analyst mistakenly omits the seasonality component and fits an ordinary least squares regression of $Y_{t}$ on an intercept and $D_{t}$ only. Starting from the definitions of unbiasedness and the Ordinary Least Squares (OLS) normal equations, derive and compute the expected bias in the estimated treatment effect for this misspecified model. Express the final bias as a single real number in the unit “visits per $100{,}000$”.\n\nThen, specify a scientifically reasonable model that would mitigate this bias by including either Fourier series terms or monthly seasonal indicator (“dummy”) variables. You must write the model explicitly in symbolic form.\n\nNo rounding is necessary; provide the exact value for the bias.",
            "solution": "The problem requires the derivation and computation of the omitted variable bias for an estimated treatment effect in a misspecified interrupted time series model, and the specification of a correctly specified model.\n\nFirst, let us formalize the models. The true data-generating process (DGP) is given by:\n$$ Y_{t} = \\alpha + \\beta D_{t} + \\delta S_{t} + \\varepsilon_{t} \\quad (*)$$\nwhere $t \\in \\{1, 2, \\ldots, 36\\}$, $D_{t} = 1$ for $t \\geq 19$ and $0$ otherwise, $S_{t} = \\cos(\\frac{2\\pi t}{12})$, $\\delta = 27$, and $\\mathbb{E}[\\varepsilon_{t} \\mid D_{t}, S_{t}] = 0$.\n\nThe analyst fits a misspecified model that omits the seasonality component $S_t$:\n$$ Y_{t} = a + b D_{t} + u_{t} \\quad (**) $$\nWe are tasked with finding the expected bias of the Ordinary Least Squares (OLS) estimator of $b$, denoted $\\hat{b}$, relative to the true effect $\\beta$. The bias is defined as $\\text{Bias}(\\hat{b}) = \\mathbb{E}[\\hat{b}] - \\beta$.\n\nThe OLS estimators for model $(**)$, which we denote $\\hat{a}$ and $\\hat{b}$, are derived from the normal equations:\n$$ \\sum_{t=1}^{N} (Y_t - \\hat{a} - \\hat{b}D_t) = 0 $$\n$$ \\sum_{t=1}^{N} D_t (Y_t - \\hat{a} - \\hat{b}D_t) = 0 $$\nwhere $N=36$ is the total number of observations. Solving this system yields the standard formula for the slope coefficient estimator:\n$$ \\hat{b} = \\frac{\\sum_{t=1}^{N} (D_t - \\bar{D})(Y_t - \\bar{Y})}{\\sum_{t=1}^{N} (D_t - \\bar{D})^2} $$\nwhere $\\bar{D} = \\frac{1}{N}\\sum_{t=1}^{N} D_t$ and $\\bar{Y} = \\frac{1}{N}\\sum_{t=1}^{N} Y_t$. Using the property $\\sum_{t=1}^{N} (D_t - \\bar{D})\\bar{Y} = 0$, this simplifies to:\n$$ \\hat{b} = \\frac{\\sum_{t=1}^{N} (D_t - \\bar{D})Y_t}{\\sum_{t=1}^{N} (D_t - \\bar{D})^2} $$\nTo find the bias, we first compute the expectation of $\\hat{b}$. We substitute the true model for $Y_t$ from equation $(*)$ into the expression for $\\hat{b}$:\n$$ \\hat{b} = \\frac{\\sum_{t=1}^{N} (D_t - \\bar{D})(\\alpha + \\beta D_t + \\delta S_t + \\varepsilon_t)}{\\sum_{t=1}^{N} (D_t - \\bar{D})^2} $$\nLet's expand the numerator:\n$$ \\sum_{t=1}^{N} (D_t - \\bar{D})\\alpha + \\sum_{t=1}^{N} (D_t - \\bar{D})\\beta D_t + \\sum_{t=1}^{N} (D_t - \\bar{D})\\delta S_t + \\sum_{t=1}^{N} (D_t - \\bar{D})\\varepsilon_t $$\nThe terms simplify as follows:\n1.  $\\alpha \\sum_{t=1}^{N} (D_t - \\bar{D}) = 0$ by the properties of deviations from the mean.\n2.  $\\beta \\sum_{t=1}^{N} (D_t - \\bar{D})D_t = \\beta \\sum_{t=1}^{N} (D_t - \\bar{D})(D_t - \\bar{D}) = \\beta \\sum_{t=1}^{N} (D_t - \\bar{D})^2$.\n3.  $\\delta \\sum_{t=1}^{N} (D_t - \\bar{D})S_t$.\n4.  $\\sum_{t=1}^{N} (D_t - \\bar{D})\\varepsilon_t$.\n\nSubstituting these back into the expression for $\\hat{b}$:\n$$ \\hat{b} = \\frac{\\beta \\sum_{t=1}^{N} (D_t - \\bar{D})^2 + \\delta \\sum_{t=1}^{N} (D_t - \\bar{D})S_t + \\sum_{t=1}^{N} (D_t - \\bar{D})\\varepsilon_t}{\\sum_{t=1}^{N} (D_t - \\bar{D})^2} $$\n$$ \\hat{b} = \\beta + \\delta \\frac{\\sum_{t=1}^{N} (D_t - \\bar{D})S_t}{\\sum_{t=1}^{N} (D_t - \\bar{D})^2} + \\frac{\\sum_{t=1}^{N} (D_t - \\bar{D})\\varepsilon_t}{\\sum_{t=1}^{N} (D_t - \\bar{D})^2} $$\nNow we take the expectation, conditional on the deterministic regressors $D_t$ and $S_t$. Since $\\mathbb{E}[\\varepsilon_t] = 0$, the expectation of the last term is zero.\n$$ \\mathbb{E}[\\hat{b}] = \\beta + \\delta \\frac{\\sum_{t=1}^{N} (D_t - \\bar{D})S_t}{\\sum_{t=1}^{N} (D_t - \\bar{D})^2} $$\nThe bias is therefore:\n$$ \\text{Bias}(\\hat{b}) = \\mathbb{E}[\\hat{b}] - \\beta = \\delta \\frac{\\sum_{t=1}^{N} (D_t - \\bar{D})S_t}{\\sum_{t=1}^{N} (D_t - \\bar{D})^2} $$\nTo compute the value of the bias, we must evaluate the sums.\nThe observation period is $N=36$ months. The intervention starts at $t=19$. There are $18$ pre-intervention months ($t=1, \\dots, 18$) and $18$ post-intervention months ($t=19, \\dots, 36$).\nThe mean of the treatment indicator $D_t$ is $\\bar{D} = \\frac{18 \\times 0 + 18 \\times 1}{36} = \\frac{18}{36} = \\frac{1}{2}$.\n\nThe denominator is:\n$$ \\sum_{t=1}^{36} (D_t - \\bar{D})^2 = \\sum_{t=1}^{18} (0 - \\frac{1}{2})^2 + \\sum_{t=19}^{36} (1 - \\frac{1}{2})^2 = 18 \\left(\\frac{1}{4}\\right) + 18 \\left(\\frac{1}{4}\\right) = \\frac{18}{2} = 9 $$\nThe numerator requires calculating $\\sum_{t=1}^{36} (D_t - \\frac{1}{2})S_t$. We can expand this:\n$$ \\sum_{t=1}^{36} (D_t - \\frac{1}{2}) S_t = \\sum_{t=1}^{18} (0 - \\frac{1}{2}) \\cos\\left(\\frac{2\\pi t}{12}\\right) + \\sum_{t=19}^{36} (1 - \\frac{1}{2}) \\cos\\left(\\frac{2\\pi t}{12}\\right) $$\n$$ = -\\frac{1}{2} \\sum_{t=1}^{18} \\cos\\left(\\frac{\\pi t}{6}\\right) + \\frac{1}{2} \\sum_{t=19}^{36} \\cos\\left(\\frac{\\pi t}{6}\\right) = \\frac{1}{2} \\left( \\sum_{t=19}^{36} S_t - \\sum_{t=1}^{18} S_t \\right) $$\nThe function $S_t = \\cos(\\frac{\\pi t}{6})$ has a period of $12$. The sum over any $12$ consecutive integers is $0$.\nLet's compute the sum for the pre-intervention period ($t=1, \\dots, 18$):\n$$ \\sum_{t=1}^{18} S_t = \\sum_{t=1}^{12} \\cos\\left(\\frac{\\pi t}{6}\\right) + \\sum_{t=13}^{18} \\cos\\left(\\frac{\\pi t}{6}\\right) = 0 + \\sum_{j=1}^{6} \\cos\\left(\\frac{\\pi (j+12)}{6}\\right) = \\sum_{j=1}^{6} \\cos\\left(\\frac{\\pi j}{6} + 2\\pi\\right) $$\n$$ = \\sum_{j=1}^{6} \\cos\\left(\\frac{\\pi j}{6}\\right) = \\cos(\\frac{\\pi}{6}) + \\cos(\\frac{\\pi}{3}) + \\cos(\\frac{\\pi}{2}) + \\cos(\\frac{2\\pi}{3}) + \\cos(\\frac{5\\pi}{6}) + \\cos(\\pi) $$\n$$ = \\frac{\\sqrt{3}}{2} + \\frac{1}{2} + 0 - \\frac{1}{2} - \\frac{\\sqrt{3}}{2} - 1 = -1 $$\nNow, let's compute the sum for the post-intervention period ($t=19, \\dots, 36$). This is also an $18$-month period.\n$$ \\sum_{t=19}^{36} S_t = \\sum_{t=19}^{30} S_t + \\sum_{t=31}^{36} S_t = 0 + \\sum_{j=1}^{6} \\cos\\left(\\frac{\\pi (j+30)}{6}\\right) = \\sum_{j=1}^{6} \\cos\\left(\\frac{\\pi j}{6} + 5\\pi\\right) $$\n$$ = \\sum_{j=1}^{6} -\\cos\\left(\\frac{\\pi j}{6}\\right) = - \\left( \\sum_{j=1}^{6} \\cos\\left(\\frac{\\pi j}{6}\\right) \\right) = -(-1) = 1 $$\nPlugging these results back into the numerator expression:\n$$ \\sum_{t=1}^{36} (D_t - \\frac{1}{2})S_t = \\frac{1}{2} (1 - (-1)) = \\frac{1}{2}(2) = 1 $$\nFinally, we compute the bias using $\\delta=27$:\n$$ \\text{Bias}(\\hat{b}) = (27) \\frac{1}{9} = 3 $$\nThe expected bias in the estimated treatment effect is $3$ visits per $100,000$.\n\nThe second task is to specify a scientifically reasonable model to mitigate this bias. The bias arises because the omitted seasonality term $S_t$ is correlated with the treatment indicator $D_t$. To correct this, the model must include terms that account for the 12-month seasonal cycle. A standard approach is to use Fourier series terms (sine and cosine pairs) for the fundamental frequency of the cycle. For a 12-month period, the fundamental frequency is $\\omega = \\frac{2\\pi}{12}$. A model incorporating these terms is:\n$$ Y_t = c_0 + c_1 D_t + c_2 \\cos\\left(\\frac{2\\pi t}{12}\\right) + c_3 \\sin\\left(\\frac{2\\pi t}{12}\\right) + e_t $$\nIn this model, the coefficients $c_2$ and $c_3$ will capture the amplitude and phase of the seasonal pattern. By including these terms as regressors, we control for the confounding effect of seasonality, and the OLS estimator of the treatment effect, $\\hat{c}_1$, will be an unbiased estimator of the true effect $\\beta$. This is because the omitted variable $S_t$ (or a sufficient representation of it) is now included in the regression, satisfying the OLS assumption of no omitted confounding variables.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "When unmeasured confounding plagues the relationship between an exposure and an outcome, Instrumental Variables (IV) analysis provides a potential solution by leveraging a third variable—the instrument. The entire method fails, however, if the instrument is only weakly related to the exposure, a condition known as the \"weak instrument\" problem. In this exercise, you will calculate a key diagnostic, the first-stage $F$-statistic, and explore why a weak instrument can lead to misleading results, teaching you a vital step in validating any IV study .",
            "id": "4626140",
            "problem": "A health system evaluates the effect of initiating a new lipid-lowering therapy on six-month low-density lipoprotein cholesterol using an instrumental variables design in an observational cohort. Let the exposure be therapy initiation, denoted by $X_{i}$, and let the instrument be the treating physician’s prior-month prescribing preference for the therapy, denoted by $Z_{i}$, measured as the fraction of that physician’s prior-month patients who were initiated on the therapy. The first-stage linear model conditions on patient covariates and site fixed effects, so that\n$$\nX_{i}=\\pi_{0}+\\pi_{1}Z_{i}+W_{i}^{\\top}\\pi_{w}+u_{i},\n$$\nwhere $W_{i}$ includes baseline covariates and site indicators. In a sample of $n=2{,}400$ patients, the estimated coefficient on the instrument is $\\widehat{\\pi}_{1}=0.0885$ with an associated robust standard error $\\operatorname{SE}(\\widehat{\\pi}_{1})=0.0300$.\n\nUsing the standard linear-model testing framework for instrument relevance in the first stage, compute the first-stage $F$-statistic for testing the null hypothesis $H_{0}:\\pi_{1}=0$ in this single-instrument setting. Your final numerical answer must be rounded to four significant figures and expressed as a pure number.\n\nIn your reasoning, start from the core definitions of instrumental variables in quasi-experiments (relevance, independence, exclusion), explain the role of the first-stage $F$-statistic as a diagnostic for instrument strength, and justify why small values such as $F<10$ imply finite-sample bias of Two-Stage Least Squares (2SLS) toward Ordinary Least Squares (OLS). Briefly name a methodological remedy that mitigates weak-instrument bias or provides valid inference under weak identification, such as Limited Information Maximum Likelihood (LIML) or weak-instrument-robust inferential procedures. Only the computed $F$-statistic should be reported as your final answer.",
            "solution": "Instrumental variables (IV) analysis is a statistical method used to estimate causal relationships when controlled experiments are not feasible and the relationship between an exposure and an outcome is confounded by unmeasured variables. For a variable $Z_{i}$ to serve as a valid instrument for the causal effect of an exposure $X_{i}$ on an outcome $Y_{i}$, it must satisfy three core assumptions, conditional on any covariates $W_{i}$:\n1.  **Relevance:** The instrument must be causally associated with the exposure. In the context of the a linear first-stage model, this translates to the coefficient on the instrument being non-zero. For the given model, $X_{i}=\\pi_{0}+\\pi_{1}Z_{i}+W_{i}^{\\top}\\pi_{w}+u_{i}$, the relevance condition requires that $\\pi_{1} \\neq 0$.\n2.  **Independence (or Ignorability):** The instrument must be independent of any unmeasured confounders that affect both the exposure and the outcome. This means the instrument's assignment is as-if random with respect to the potential outcomes and sources of confounding.\n3.  **Exclusion Restriction:** The instrument must affect the outcome only through its effect on the exposure. There is no direct causal pathway from the instrument $Z_{i}$ to the outcome $Y_{i}$ that bypasses the exposure $X_{i}$.\n\nThe problem focuses on the first condition: instrument relevance. The first-stage regression models the exposure $X_{i}$ as a function of the instrument $Z_{i}$ and other covariates $W_{i}$. The strength of the instrument is a critical diagnostic in any IV analysis. A weak instrument, one that is only weakly correlated with the exposure, can lead to severe problems in estimation.\n\nThe strength of the instrument is formally tested using the first-stage $F$-statistic. This statistic tests the null hypothesis that all coefficients on the instruments in the first-stage regression are equal to zero. In this problem, there is only a single instrument, $Z_{i}$, so we are testing the null hypothesis $H_{0}: \\pi_{1}=0$ against the alternative $H_{1}: \\pi_{1} \\neq 0$.\n\nFor a single instrument, the $F$-statistic is equivalent to the square of the $t$-statistic for the coefficient of that instrument. The $t$-statistic for the estimated coefficient $\\widehat{\\pi}_{1}$ is calculated as the ratio of the estimate to its standard error:\n$$\nt = \\frac{\\widehat{\\pi}_{1} - \\pi_{1, H_0}}{\\operatorname{SE}(\\widehat{\\pi}_{1})}\n$$\nUnder the null hypothesis $H_{0}: \\pi_{1}=0$, this simplifies to:\n$$\nt = \\frac{\\widehat{\\pi}_{1}}{\\operatorname{SE}(\\widehat{\\pi}_{1})}\n$$\nConsequently, the first-stage $F$-statistic is given by:\n$$\nF = t^2 = \\left( \\frac{\\widehat{\\pi}_{1}}{\\operatorname{SE}(\\widehat{\\pi}_{1})} \\right)^2\n$$\nThe problem provides the following values from a sample of $n=2,400$ patients:\n- The estimated coefficient on the instrument: $\\widehat{\\pi}_{1} = 0.0885$.\n- The robust standard error of the estimate: $\\operatorname{SE}(\\widehat{\\pi}_{1}) = 0.0300$.\n\nSubstituting these values into the formula for the $F$-statistic:\n$$\nF = \\left( \\frac{0.0885}{0.0300} \\right)^2\n$$\nFirst, we compute the ratio, which is the $t$-statistic:\n$$\nt = \\frac{0.0885}{0.0300} = 2.95\n$$\nNext, we square this value to obtain the $F$-statistic:\n$$\nF = (2.95)^2 = 8.7025\n$$\nThe problem requires the answer to be rounded to four significant figures. Thus, $F \\approx 8.703$.\n\nThis value of the $F$-statistic, being less than the commonly cited rule-of-thumb threshold of $F=10$ (proposed by Staiger and Stock, 1997), suggests that the instrument is \"weak.\" Weak instruments pose a significant problem for the standard Two-Stage Least Squares (2SLS) estimator. In finite samples, the 2SLS estimator is biased in the direction of the Ordinary Least Squares (OLS) estimator. The magnitude of this bias is inversely related to the first-stage $F$-statistic. Since the OLS estimate is itself biased due to confounding, a weak instrument means that the 2SLS estimate may not adequately correct for this confounding, compromising the validity of the causal inference.\n\nTo address the issues of weak instruments, methodologists have developed alternative estimators and inferential procedures. One such alternative estimator is Limited Information Maximum Likelihood (LIML), which is known to be more robust to weak instruments in terms of median bias compared to 2SLS. Another approach is to use weak-instrument-robust inferential procedures, such as the Anderson-Rubin (AR) test, which provide valid hypothesis tests and confidence intervals for the causal parameter even when instruments are weak, although they may be less precise.\n\nThe final computation for the requested value is $8.7025$, which rounds to $8.703$.",
            "answer": "$$\n\\boxed{8.703}\n$$"
        }
    ]
}