{
    "hands_on_practices": [
        {
            "introduction": "Before performing any statistical adjustment, it is essential to first think causally about the system under study. This exercise introduces the use of a Directed Acyclic Graph (DAG) to visualize the assumed relationships between an exposure $A$, an outcome $Y$, and other covariates. By applying the backdoor criterion, you will practice identifying a valid set of variables for adjustment, a process crucial for isolating a causal effect from confounding bias .",
            "id": "4612668",
            "problem": "An investigator wants to identify the total causal effect of an exposure $A$ on an outcome $Y$ using a Directed Acyclic Graph (DAG). The measured covariates are $L_1$, $L_2$, and $L_3$. There is also an unmeasured variable $U$. The causal structure is as follows:\n- $L_1 \\to A$, $L_1 \\to Y$,\n- $L_3 \\to A$, $L_3 \\to Y$,\n- $A \\to Y$,\n- $A \\to L_2$, $U \\to L_2$, and $U \\to Y$,\nwith no other arrows. All variables $L_1$, $L_2$, and $L_3$ are distinct baseline or follow-up measurements as indicated by the arrows; specifically, $L_1$ and $L_3$ temporally precede $A$ and $Y$, while $L_2$ occurs after $A$. The variable $U$ is unmeasured and cannot be adjusted for. The goal is to decide whether adjusting for the set $S=\\{L_1,L_3\\}$ is valid for identifying the total causal effect of $A$ on $Y$, and to justify whether $L_2$ should be excluded from the adjustment set.\n\nUse the definitions of backdoor paths, colliders, and $d$-separation as your fundamental base: a backdoor path between $A$ and $Y$ is any path that starts with an arrow into $A$; a path is blocked by a set if it contains an unconditioned collider or any noncollider that is conditioned on; and a set $S$ is a valid adjustment set if it blocks all backdoor paths from $A$ to $Y$ without conditioning on descendants of $A$ when identifying the total causal effect.\n\nWhich statement is most accurate?\n\nA. $S=\\{L_1,L_3\\}$ is a valid adjustment set. $L_2$ should be excluded because it is a collider on the path $A \\to L_2 \\leftarrow U \\to Y$, and conditioning on $L_2$ would open that noncausal path.\n\nB. $S=\\{L_1,L_3\\}$ is not valid; $L_2$ must be included because it lies on a path between $A$ and $Y$, and adjusting for all variables associated with $A$ and $Y$ is required to remove confounding.\n\nC. $S=\\{L_1,L_3\\}$ is not valid because $L_1$ induces $M$-bias when adjusted for; one should exclude $L_1$ and adjust for $L_2$ instead.\n\nD. $S=\\{L_1,L_3\\}$ is valid only if one also adjusts for $L_2$ to block the path through $U$, since $U$ is unmeasured.",
            "solution": "The validity of the problem statement is established as follows.\n\n**Step 1: Extract Givens**\n-   **Exposure:** $A$\n-   **Outcome:** $Y$\n-   **Measured Covariates:** $L_1$, $L_2$, $L_3$\n-   **Unmeasured Covariate:** $U$\n-   **Proposed Adjustment Set:** $S = \\{L_1, L_3\\}$\n-   **Causal Structure (Directed Acyclic Graph - DAG):**\n    -   $L_1 \\to A$\n    -   $L_1 \\to Y$\n    -   $L_3 \\to A$\n    -   $L_3 \\to Y$\n    -   $A \\to Y$\n    -   $A \\to L_2$\n    -   $U \\to L_2$\n    -   $U \\to Y$\n-   **Temporal Information:** $L_1$ and $L_3$ precede $A$; $L_2$ occurs after $A$.\n-   **Definitions:**\n    -   A backdoor path from $A$ to $Y$ is a path starting with an arrow into $A$.\n    -   A path is blocked by a set if it contains an unconditioned collider or a conditioned non-collider.\n    -   A valid adjustment set $S$ for the total causal effect must block all backdoor paths from $A$ to $Y$ and must not contain any descendants of $A$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the established theory of causal inference using DAGs. The provided definitions are standard and internally consistent. The causal structure is explicitly given, and the question is precise and unambiguous. The problem is well-posed, as a unique solution can be derived from the given structure and rules. The temporal information is consistent with the DAG structure (e.g., $A \\to L_2$ is consistent with $L_2$ occurring after $A$). There are no contradictions, missing pieces of information, or violations of scientific principles.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. We will proceed with a full derivation and analysis.\n\n**Derivation**\n\nThe primary goal is to determine if the set $S = \\{L_1, L_3\\}$ is a valid adjustment set for estimating the total causal effect of $A$ on $Y$. According to the backdoor criterion for total effects, a set of covariates $S$ is valid if and only if two conditions are met:\n1.  $S$ blocks all backdoor paths between the exposure $A$ and the outcome $Y$.\n2.  No variable in $S$ is a descendant of the exposure $A$.\n\nFirst, we must identify all paths between $A$ and $Y$ in the provided DAG.\n\n**Path Identification and Analysis:**\n\n1.  **Causal Path:** $A \\to Y$. This is the direct effect of $A$ on $Y$, which is part of the total causal effect we wish to identify. This path should remain open.\n\n2.  **Backdoor Paths:** These are non-causal paths that have an arrow pointing into $A$.\n    -   **Path 1:** $A \\leftarrow L_1 \\to Y$. This is a backdoor path because it starts with an arrow into $A$. The variable $L_1$ is a common cause of $A$ and $Y$, thus acting as a confounding variable. This path is open by default because $L_1$ is a non-collider.\n    -   **Path 2:** $A \\leftarrow L_3 \\to Y$. This is also a backdoor path for the same reason as Path 1. $L_3$ is a common cause of $A$ and $Y$, acting as a confounder. This path is open by default.\n    -   There are no other backdoor paths. Any path that begins by moving backwards from $A$ must go via $L_1$ or $L_3$. From those nodes, the only path to $Y$ is the direct arrow.\n\n3.  **Other Non-Causal Paths:**\n    -   **Path 3:** $A \\to L_2 \\leftarrow U \\to Y$. This path is not a backdoor path because it begins with an arrow leaving $A$. The node $L_2$ on this path is a **collider** because two arrows point into it ($A \\to L_2$ and $U \\to L_2$). According to the rules of d-separation, a path containing an unconditioned collider is **blocked**. Therefore, this path is naturally blocked and does not create a spurious association between $A$ and $Y$.\n\n**Evaluating the Proposed Adjustment Set $S = \\{L_1, L_3\\}$:**\n\n-   **Condition 1 (Blocking Backdoor Paths):**\n    -   To block the backdoor path $A \\leftarrow L_1 \\to Y$, we must condition on the non-collider $L_1$. The set $S$ includes $L_1$, so this path is successfully blocked.\n    -   To block the backdoor path $A \\leftarrow L_3 \\to Y$, we must condition on the non-collider $L_3$. The set $S$ includes $L_3$, so this path is also blocked.\n    -   Since these are the only two backdoor paths, the set $S = \\{L_1, L_3\\}$ successfully blocks all backdoor paths. This condition is satisfied.\n\n-   **Condition 2 (No Descendants of A):**\n    -   A descendant of $A$ is any variable that can be reached by following a sequence of directed arrows starting from $A$.\n    -   From the DAG, the descendants of $A$ are $Y$ and $L_2$ (due to the arrow $A \\to L_2$).\n    -   The proposed set $S = \\{L_1, L_3\\}$ contains neither $Y$ nor $L_2$. The variables $L_1$ and $L_3$ are ancestors of $A$, not descendants.\n    -   Therefore, this condition is also satisfied.\n\n-   **Conclusion on $S$:** Because both conditions of the backdoor criterion are met, the set $S = \\{L_1, L_3\\}$ is a **valid adjustment set** for identifying the total causal effect of $A$ on $Y$.\n\n**Evaluating the Role of $L_2$:**\n\nNow, we must consider whether $L_2$ should be included or excluded from the adjustment set.\n-   As established, $L_2$ is a descendant of $A$. Adjusting for a descendant of the exposure is forbidden when identifying the *total* causal effect, as it would block a portion of the effect we aim to measure.\n-   Furthermore, consider the path $A \\to L_2 \\leftarrow U \\to Y$. We identified $L_2$ as a collider on this path, which means the path is blocked by default. If we were to condition on (adjust for) $L_2$, we would open this path. Opening this path creates a spurious, non-causal association between $A$ and $Y$ through the unmeasured common cause $U$. This phenomenon is known as collider-stratification bias or M-bias.\n-   Since $U$ is unmeasured, we cannot then block this newly opened path by adjusting for $U$. Thus, adjusting for $L_2$ would introduce bias into the estimate of the effect of $A$ on $Y$.\n-   Therefore, $L_2$ **must be excluded** from the adjustment set.\n\n**Option-by-Option Analysis**\n\n**A. $S=\\{L_1,L_3\\}$ is a valid adjustment set. $L_2$ should be excluded because it is a collider on the path $A \\to L_2 \\leftarrow U \\to Y$, and conditioning on $L_2$ would open that noncausal path.**\n-   Our analysis concluded that $S=\\{L_1,L_3\\}$ is a valid adjustment set.\n-   Our analysis also concluded that $L_2$ must be excluded, and the reason provided (it is a collider on a path involving an unmeasured variable $U$, and conditioning on it would open this path and induce bias) is precisely correct.\n-   Verdict: **Correct**.\n\n**B. $S=\\{L_1,L_3\\}$ is not valid; $L_2$ must be included because it lies on a path between $A$ and $Y$, and adjusting for all variables associated with $A$ and $Y$ is required to remove confounding.**\n-   This statement claims $S=\\{L_1,L_3\\}$ is not valid, which is false.\n-   It claims $L_2$ must be included, which is false.\n-   The reasoning \"adjusting for all variables associated with $A$ and $Y$\" is a common but incorrect heuristic. As shown, adjusting for $L_2$ would introduce bias, not remove it.\n-   Verdict: **Incorrect**.\n\n**C. $S=\\{L_1,L_3\\}$ is not valid because $L_1$ induces $M$-bias when adjusted for; one should exclude $L_1$ and adjust for $L_2$ instead.**\n-   This statement claims $S=\\{L_1,L_3\\}$ is not valid, which is false.\n-   The claim that adjusting for $L_1$ induces M-bias is incorrect. M-bias requires the adjusted variable to be a collider, but in the given DAG, no arrows point into $L_1$. $L_1$ is a standard confounder that must be adjusted for.\n-   The suggestion to adjust for $L_2$ is also incorrect.\n-   Verdict: **Incorrect**.\n\n**D. $S=\\{L_1,L_3\\}$ is valid only if one also adjusts for $L_2$ to block the path through $U$, since $U$ is unmeasured.**\n-   The premise that validity requires adjusting for $L_2$ is false. Adjusting for $L_2$ would introduce bias.\n-   The reasoning is also flawed. The path through $U$ ($A \\to L_2 \\leftarrow U \\to Y$) is already blocked because $L_2$ is an unconditioned collider. Adjusting for $L_2$ *opens* this path; it does not block it.\n-   Verdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once a valid adjustment set is identified, we use statistical models to estimate the adjusted effect. This practice demystifies the inner workings of multiple linear regression, one of the most common tools for multivariable adjustment. By starting from the underlying covariance structure of the variables, you will derive the adjusted effect estimate $\\hat{\\beta}_{X}$, gaining a deeper appreciation for how a model mathematically isolates the association of interest .",
            "id": "4612709",
            "problem": "An observational cohort study investigates the association between a continuous outcome $Y$ and an exposure $X$, adjusting for two confounders $Z_{1}$ and $Z_{2}$. All variables are mean-centered so that the intercept in the linear model is zero. The analysis fits a multiple linear regression using Ordinary Least Squares (OLS), estimating coefficients $\\hat{\\beta}_{X}$, $\\hat{\\beta}_{Z_{1}}$, and $\\hat{\\beta}_{Z_{2}}$ in the model\n$$\nY = \\beta_{X} X + \\beta_{Z_{1}} Z_{1} + \\beta_{Z_{2}} Z_{2} + \\varepsilon,\n$$\nwhere $\\varepsilon$ is the random error term with mean zero.\n\nYou are given the sample covariance matrix of the regressors $\\Sigma$,\n$$\n\\Sigma = \n\\begin{pmatrix}\n4 & 3 & -4 \\\\\n3 & 9 & -6 \\\\\n-4 & -6 & 16\n\\end{pmatrix},\n$$\nwhere each entry is the empirical covariance $n^{-1}\\sum_{i=1}^{n} W_{i}^{(j)} W_{i}^{(k)}$ for $W^{(j)}, W^{(k)} \\in \\{X, Z_{1}, Z_{2}\\}$. You are also given the vector of empirical covariances between $Y$ and each regressor,\n$$\n\\begin{pmatrix}\n\\operatorname{Cov}(Y,X) \\\\\n\\operatorname{Cov}(Y,Z_{1}) \\\\\n\\operatorname{Cov}(Y,Z_{2})\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n8 \\\\\n-6 \\\\\n2\n\\end{pmatrix}.\n$$\n\nStarting from the defining properties of Ordinary Least Squares and the covariance identities for mean-centered variables, derive the normal equations that determine the OLS estimates and compute the adjusted effect estimate $\\hat{\\beta}_{X}$. Express your final answer as an exact value without rounding. No units are required.",
            "solution": "The problem requires the computation of the Ordinary Least Squares (OLS) estimate $\\hat{\\beta}_{X}$ for the coefficient of the exposure $X$ in the multiple linear regression model $Y = \\beta_{X} X + \\beta_{Z_{1}} Z_{1} + \\beta_{Z_{2}} Z_{2} + \\varepsilon$. All variables are stipulated to be mean-centered, which simplifies the underlying calculations.\n\nThe OLS method determines the coefficient estimates $(\\hat{\\beta}_{X}, \\hat{\\beta}_{Z_{1}}, \\hat{\\beta}_{Z_{2}})$ by minimizing the sum of squared residuals (SSR). For a sample of size $n$, the SSR is defined as:\n$$\nSSR = \\sum_{i=1}^{n} \\varepsilon_i^2 = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^{n} (Y_i - (\\hat{\\beta}_{X} X_i + \\hat{\\beta}_{Z_{1}} Z_{1i} + \\hat{\\beta}_{Z_{2}} Z_{2i}))^2\n$$\nTo find the coefficients that minimize this quantity, we must take the partial derivative of the $SSR$ with respect to each coefficient and set the resulting expression to zero. This yields a system of equations known as the normal equations.\n\nThe partial derivative with respect to $\\hat{\\beta}_{X}$ is:\n$$\n\\frac{\\partial SSR}{\\partial \\hat{\\beta}_{X}} = -2 \\sum_{i=1}^{n} X_i (Y_i - \\hat{\\beta}_{X} X_i - \\hat{\\beta}_{Z_{1}} Z_{1i} - \\hat{\\beta}_{Z_{2}} Z_{2i}) = 0\n$$\nDividing by $-2n$ and applying the distributive property of the summation, we get:\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} X_i Y_i - \\hat{\\beta}_{X} \\frac{1}{n} \\sum_{i=1}^{n} X_i^2 - \\hat{\\beta}_{Z_{1}} \\frac{1}{n} \\sum_{i=1}^{n} X_i Z_{1i} - \\hat{\\beta}_{Z_{2}} \\frac{1}{n} \\sum_{i=1}^{n} X_i Z_{2i} = 0\n$$\nFor any two mean-centered variables $A$ and $B$, their sample covariance is given by the formula $\\operatorname{Cov}(A,B) = \\frac{1}{n} \\sum_{i=1}^{n} A_i B_i$. The sample variance is a special case where $A=B$, i.e., $\\operatorname{Var}(A) = \\operatorname{Cov}(A,A) = \\frac{1}{n} \\sum_{i=1}^{n} A_i^2$. Using this, the equation transforms into:\n$$\n\\operatorname{Cov}(Y,X) = \\hat{\\beta}_{X} \\operatorname{Var}(X) + \\hat{\\beta}_{Z_{1}} \\operatorname{Cov}(X, Z_1) + \\hat{\\beta}_{Z_{2}} \\operatorname{Cov}(X, Z_2)\n$$\nThis derivation can be repeated for $\\hat{\\beta}_{Z_{1}}$ and $\\hat{\\beta}_{Z_{2}}$, yielding a system of three linear equations:\n$$\n\\operatorname{Cov}(Y,X) = \\hat{\\beta}_{X} \\operatorname{Var}(X) + \\hat{\\beta}_{Z_1} \\operatorname{Cov}(X, Z_1) + \\hat{\\beta}_{Z_{2}} \\operatorname{Cov}(X, Z_2)\n$$\n$$\n\\operatorname{Cov(Y,Z_1)} = \\hat{\\beta}_{X} \\operatorname{Cov}(Z_1, X) + \\hat{\\beta}_{Z_1} \\operatorname{Var}(Z_1) + \\hat{\\beta}_{Z_{2}} \\operatorname{Cov}(Z_1, Z_2)\n$$\n$$\n\\operatorname{Cov(Y,Z_2)} = \\hat{\\beta}_{X} \\operatorname{Cov}(Z_2, X) + \\hat{\\beta}_{Z_1} \\operatorname{Cov}(Z_2, Z_1) + \\hat{\\beta}_{Z_{2}} \\operatorname{Var}(Z_2)\n$$\nThis system can be written compactly in matrix form as $\\Sigma \\hat{\\boldsymbol{\\beta}} = \\mathbf{c}$, where $\\Sigma$ is the empirical covariance matrix of the regressors, $\\hat{\\boldsymbol{\\beta}}$ is the vector of coefficient estimates, and $\\mathbf{c}$ is the vector of empirical covariances between the outcome and the regressors.\n$$\n\\Sigma = \n\\begin{pmatrix}\n\\operatorname{Var}(X) & \\operatorname{Cov}(X,Z_1) & \\operatorname{Cov}(X,Z_2) \\\\\n\\operatorname{Cov}(Z_1,X) & \\operatorname{Var}(Z_1) & \\operatorname{Cov}(Z_1,Z_2) \\\\\n\\operatorname{Cov(Z_2,X)} & \\operatorname{Cov}(Z_2,Z_1) & \\operatorname{Var}(Z_2)\n\\end{pmatrix},\n\\quad\n\\hat{\\boldsymbol{\\beta}} = \n\\begin{pmatrix}\n\\hat{\\beta}_{X} \\\\\n\\hat{\\beta}_{Z_{1}} \\\\\n\\hat{\\beta}_{Z_{2}}\n\\end{pmatrix},\n\\quad\n\\mathbf{c} =\n\\begin{pmatrix}\n\\operatorname{Cov}(Y,X) \\\\\n\\operatorname{Cov}(Y,Z_{1}) \\\\\n\\operatorname{Cov}(Y,Z_{2})\n\\end{pmatrix}\n$$\nSubstituting the values provided in the problem statement, we obtain the specific linear system:\n$$\n\\begin{pmatrix}\n4 & 3 & -4 \\\\\n3 & 9 & -6 \\\\\n-4 & -6 & 16\n\\end{pmatrix}\n\\begin{pmatrix}\n\\hat{\\beta}_{X} \\\\\n\\hat{\\beta}_{Z_{1}} \\\\\n\\hat{\\beta}_{Z_{2}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n8 \\\\\n-6 \\\\\n2\n\\end{pmatrix}\n$$\nTo solve for the component $\\hat{\\beta}_{X}$, we can utilize Cramer's rule, which states that $\\hat{\\beta}_{X} = \\frac{\\det(\\Sigma_X)}{\\det(\\Sigma)}$, where $\\Sigma_X$ is the matrix formed by replacing the first column of $\\Sigma$ with the vector $\\mathbf{c}$. First, we compute the determinant of $\\Sigma$:\n$$\n\\det(\\Sigma) = 4(9 \\cdot 16 - (-6)^2) - 3(3 \\cdot 16 - (-4)(-6)) + (-4)(3(-6) - 9(-4))\n$$\n$$\n\\det(\\Sigma) = 4(144 - 36) - 3(48 - 24) - 4(-18 + 36)\n$$\n$$\n\\det(\\Sigma) = 4(108) - 3(24) - 4(18) = 432 - 72 - 72 = 288\n$$\nNext, we form the matrix $\\Sigma_X$ and compute its determinant:\n$$\n\\Sigma_X = \n\\begin{pmatrix}\n8 & 3 & -4 \\\\\n-6 & 9 & -6 \\\\\n2 & -6 & 16\n\\end{pmatrix}\n$$\n$$\n\\det(\\Sigma_X) = 8(9 \\cdot 16 - (-6)^2) - 3((-6) \\cdot 16 - (-6) \\cdot 2) + (-4)((-6)(-6) - 9 \\cdot 2)\n$$\n$$\n\\det(\\Sigma_X) = 8(144 - 36) - 3(-96 + 12) - 4(36 - 18)\n$$\n$$\n\\det(\\Sigma_X) = 8(108) - 3(-84) - 4(18) = 864 + 252 - 72 = 1044\n$$\nFinally, we compute $\\hat{\\beta}_{X}$ as the ratio of these determinants and simplify the resulting fraction:\n$$\n\\hat{\\beta}_{X} = \\frac{\\det(\\Sigma_X)}{\\det(\\Sigma)} = \\frac{1044}{288} = \\frac{261}{72} = \\frac{29}{8}\n$$",
            "answer": "$$\\boxed{\\frac{29}{8}}$$"
        },
        {
            "introduction": "In studies with many potential confounders, adjusting for each one individually in a regression model can be challenging. Propensity score methods offer a powerful alternative by summarizing all measured confounders $L$ into a single variable, the propensity score $e(L)$. This advanced practice guides you through the full computational workflow of estimating propensity scores and, critically, using them to check whether the adjustment has successfully balanced the confounders between treatment groups .",
            "id": "4612673",
            "problem": "You are given a binary treatment variable $A \\in \\{0,1\\}$ and a vector of baseline covariates $L \\in \\mathbb{R}^p$ for $n$ independent individuals. The propensity score is defined as $e(L) = \\mathbb{P}(A=1 \\mid L)$, and it is modeled via logistic regression so that $e(L) \\in (0,1)$ for all observed $L$. The goal is to construct a logistic regression model to estimate the propensity score $e(L)$ from the observed data and then to specify diagnostics to verify covariate balance using standardized mean differences within propensity score strata.\n\nFundamental base:\n- Binary outcomes arise from a Bernoulli model, so for individual $i$ with covariates $L_i$, the conditional likelihood is $\\mathbb{P}(A_i \\mid L_i) = e(L_i)^{A_i} (1-e(L_i))^{1-A_i}$.\n- The logistic regression model specifies the log odds of the probability as linear in the covariates, which ensures $e(L)$ is bounded between $0$ and $1$.\n- The standardized mean difference (SMD) is a scale-free measure of imbalance between two groups defined as the difference in means scaled by the pooled standard deviation.\n\nYou must implement an algorithm that:\n1. Fits a logistic regression by maximum likelihood to estimate $\\hat{e}(L)$ from the observed $(A_i, L_i)$, $i=1,\\ldots,n$.\n2. Stratifies the estimated propensity scores $\\hat{e}(L)$ into $K$ strata using quantiles at equally spaced probabilities between $0$ and $1$.\n3. For each stratum and each covariate, computes the standardized mean difference between the treated ($A=1$) and untreated ($A=0$) groups. For a covariate indexed by $j$, within a given stratum $s$, define the group-specific sample means $\\bar{L}_{1,js}$ and $\\bar{L}_{0,js}$ and the sample variances $S_{1,js}^2$ and $S_{0,js}^2$. The standardized mean difference is\n$$\n\\mathrm{SMD}_{js} = \\frac{\\bar{L}_{1,js} - \\bar{L}_{0,js}}{\\sqrt{\\frac{S_{1,js}^2 + S_{0,js}^2}{2}}}.\n$$\nIf a stratum has no treated or no untreated observations, you must treat every $\\mathrm{SMD}_{js}$ in that stratum as undefined and set it to $+\\infty$ for diagnostic purposes. If the pooled variance in the denominator is $0$, then set $\\mathrm{SMD}_{js}=0$ only when the means are equal; otherwise set $\\mathrm{SMD}_{js}=+\\infty$.\n\nDiagnostics:\n- For each test case, compute the maximum absolute standardized mean difference across all strata and all covariates,\n$$\nM = \\max_{j,s} \\left|\\mathrm{SMD}_{js}\\right|,\n$$\nwhere $M$ may be $+\\infty$ under the rules above.\n- For each test case, also compute a boolean flag indicating whether all computed $\\left|\\mathrm{SMD}_{js}\\right|$ are finite and less than or equal to a threshold $\\tau$ (use the case-specific value of $\\tau$).\n\nTest suite:\nImplement your program to run the following three test cases. In each case, generate data according to the specified distributions and treatment assignment mechanism, then fit the logistic regression model and compute $M$ and the boolean balance flag.\n\n- Case $1$ (general case):\n  - Sample size $n=1000$, number of strata $K=5$, threshold $\\tau=0.1$, random seed $s=128$.\n  - Covariates:\n    - $L_1 \\sim \\mathcal{N}(0,1)$,\n    - $L_2 \\sim \\mathrm{Bernoulli}(0.5)$,\n    - $L_3 \\sim \\mathrm{Uniform}(-1,1)$.\n  - Treatment assignment mechanism:\n    - Intercept $\\beta_0 = 0.2$,\n    - Coefficients $\\beta = (0.7, -0.6, 0.4)$,\n    - For individual $i$, $A_i \\sim \\mathrm{Bernoulli}(e(L_i))$ with $e(L_i)$ given by the logistic model.\n\n- Case $2$ (boundary case: randomization-like assignment independent of covariates):\n  - Sample size $n=80$, number of strata $K=4$, threshold $\\tau=0.1$, random seed $s=999$.\n  - Covariates:\n    - $L_1 \\sim \\mathcal{N}(0,1)$,\n    - $L_2 \\sim \\mathrm{Bernoulli}(0.5)$,\n    - $L_3 \\sim \\mathrm{Uniform}(-1,1)$.\n  - Treatment assignment mechanism:\n    - Intercept $\\beta_0 = 0$,\n    - Coefficients $\\beta = (0, 0, 0)$.\n\n- Case $3$ (edge case: near-separation causing lack of overlap):\n  - Sample size $n=400$, number of strata $K=5$, threshold $\\tau=0.1$, random seed $s=17$.\n  - Covariates:\n    - $L_1 \\sim \\mathcal{N}(0,1)$,\n    - $L_2 \\sim \\mathrm{Bernoulli}(0.7)$,\n    - $L_3 \\sim \\mathcal{N}(0, 0.2^2)$.\n  - Treatment assignment mechanism:\n    - Intercept $\\beta_0 = 2.0$,\n    - Coefficients $\\beta = (2.5, -2.0, 0.0)$.\n\nAlgorithmic requirements:\n- Fit the logistic regression with an intercept term using maximum likelihood via a numerically stable iterative method based on second-order information.\n- Use equal-probability quantile cutpoints to form $K$ strata from $\\hat{e}(L)$.\n- Compute the standardized mean differences within each stratum for each covariate, then aggregate to $M$ and the boolean flag.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes an inner two-element list $[M,\\text{flag}]$. For example, the output structure is $[[M_1,\\text{flag}_1],[M_2,\\text{flag}_2],[M_3,\\text{flag}_3]]$ with $M_k$ a float (possibly $+\\infty$) and $\\text{flag}_k$ a boolean.\n\nNo physical units or angles are involved. All numerical outputs must be raw numbers, not percentages.",
            "solution": "The problem is valid. It presents a clear, self-contained, and scientifically grounded task in the field of epidemiology and biostatistics. The objective is to implement a standard workflow for propensity score analysis: estimating propensity scores via logistic regression and then evaluating covariate balance across strata defined by these scores using the standardized mean difference. The problem provides all necessary data-generating parameters, algorithmic specifications, and rules for handling edge cases, making it well-posed and objective.\n\nThe solution will be implemented following the sequence of steps outlined in the problem.\n\n### Step 1: Data Generation\nFor each test case, we first generate a dataset of size $n$ with $p=3$ covariates, $L = (L_1, L_2, L_3)$. The covariates are drawn from specified distributions. We then generate the binary treatment assignment $A \\in \\{0, 1\\}$ for each individual $i$ based on their covariates $L_i$ and a true propensity score model. The true propensity score $e(L_i) = \\mathbb{P}(A_i=1 \\mid L_i)$ is given by the logistic function:\n$$\ne(L_i) = \\frac{1}{1 + \\exp(-(\\beta_0 + L_i^T \\beta))}\n$$\nwhere $\\beta_0$ is the intercept and $\\beta$ is the vector of coefficients. The treatment $A_i$ for each individual is then drawn from a Bernoulli distribution with parameter $e(L_i)$.\n\n### Step 2: Propensity Score Estimation via Logistic Regression\nThe next step is to estimate the propensity scores from the generated data $(A_i, L_i)_{i=1}^n$. We fit a logistic regression model with an intercept, which assumes the same functional form as the true data-generating model. The vector of coefficients, let's denote it $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\ldots, \\beta_p)^T$, is estimated by maximizing the log-likelihood function of the Bernoulli outcomes:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( A_i \\log(e(L_i)) + (1-A_i) \\log(1-e(L_i)) \\right)\n$$\nSubstituting the logistic model for $e(L_i)$ and letting $X_i = (1, L_{i1}, \\ldots, L_{ip})$ be the design vector for individual $i$:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( A_i (X_i \\boldsymbol{\\beta}) - \\log(1 + \\exp(X_i \\boldsymbol{\\beta})) \\right)\n$$\nMaximizing this function is a convex optimization problem. As required, we use an iterative method based on second-order information. The Iteratively Reweighted Least Squares (IRLS) algorithm is equivalent to the Newton-Raphson method for this problem. At each iteration $t$, the coefficient vector $\\boldsymbol{\\beta}^{(t)}$ is updated to $\\boldsymbol{\\beta}^{(t+1)}$:\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - (\\nabla^2 \\ell(\\boldsymbol{\\beta}^{(t)}))^{-1} \\nabla \\ell(\\boldsymbol{\\beta}^{(t)})\n$$\nLet $X$ be the $n \\times (p+1)$ design matrix, $\\mathbf{A}$ be the vector of treatments, and $\\mathbf{p}^{(t)}$ be the vector of predicted probabilities using $\\boldsymbol{\\beta}^{(t)}$. Let $W^{(t)}$ be a diagonal matrix with entries $W_{ii}^{(t)} = p_i^{(t)}(1_i-p_i^{(t)})$. The gradient and Hessian are $\\nabla \\ell = X^T(\\mathbf{A} - \\mathbf{p})$ and $\\nabla^2 \\ell = -X^T W X$. The update step becomes:\n$$\n\\boldsymbol{\\beta}^{(t+1)} = (X^T W^{(t)} X)^{-1} X^T W^{(t)} \\mathbf{z}^{(t)}\n$$\nwhere $\\mathbf{z}^{(t)} = X \\boldsymbol{\\beta}^{(t)} + (W^{(t)})^{-1}(\\mathbf{A} - \\mathbf{p}^{(t)})$ is the working response. This iterative process is repeated for a fixed number of steps or until convergence. Once the final estimate $\\hat{\\boldsymbol{\\beta}}$ is obtained, the estimated propensity scores are calculated for all individuals:\n$$\n\\hat{e}(L_i) = \\frac{1}{1 + \\exp(-X_i \\hat{\\boldsymbol{\\beta}})}\n$$\n\n### Step 3: Stratification by Propensity Score\nThe continuous estimated propensity scores $\\hat{e}(L)$ are used to stratify the population into $K$ groups of approximately equal size. This is achieved by defining strata based on the quantiles of the distribution of $\\hat{e}(L)$. We compute $K-1$ cut-points corresponding to the $(100/K), (200/K), \\ldots, (100 \\cdot (K-1)/K)$ percentiles of the sample of $\\hat{e}(L_i)$ values. These cut-points define $K$ strata. For instance, for $K=5$, the cut-points are the $20^{th}$, $40^{th}$, $60^{th}$, and $80^{th}$ percentiles.\n\n### Step 4: Calculation of Standardized Mean Differences (SMD)\nWithin each stratum, we assess the balance of each covariate between the treated ($A=1$) and untreated ($A=0$) groups. For each covariate $j \\in \\{1,\\ldots,p\\}$ and each stratum $s \\in \\{1,\\ldots,K\\}$, the standardized mean difference is calculated. Let $n_{1s}$ and $n_{0s}$ be the number of treated and untreated individuals in stratum $s$. We compute the sample means ($\\bar{L}_{1,js}$, $\\bar{L}_{0,js}$) and sample variances ($S_{1,js}^2$, $S_{0,js}^2$) for covariate $L_j$ in each group within the stratum. The SMD is given by:\n$$\n\\mathrm{SMD}_{js} = \\frac{\\bar{L}_{1,js} - \\bar{L}_{0,js}}{\\sqrt{\\frac{S_{1,js}^2 + S_{0,js}^2}{2}}}\n$$\nThe problem specifies precise rules for handling edge cases:\n1.  If a stratum $s$ contains no treated individuals ($n_{1s}=0$) or no untreated individuals ($n_{0s}=0$), balance cannot be assessed. For such strata, $\\mathrm{SMD}_{js}$ for all covariates $j$ is set to $+\\infty$.\n2.  If the pooled variance in the denominator is zero (i.e., $S_{1,js}^2 = 0$ and $S_{0,js}^2 = 0$), this means the covariate is constant within both groups in that stratum. If the means are also equal ($\\bar{L}_{1,js} = \\bar{L}_{0,js}$), the groups are identical on this covariate, so $\\mathrm{SMD}_{js}=0$. If the means differ, there is a perfect separation, and $\\mathrm{SMD}_{js}$ is set to $+\\infty$.\n\n### Step 5: Diagnostic Aggregation\nFinally, the computed SMDs are summarized into two diagnostic measures for each test case:\n1.  The maximum absolute standardized mean difference, $M = \\max_{j,s} |\\mathrm{SMD}_{js}|$. This single value provides an overall measure of the worst-case imbalance across all covariates and strata. An infinite value for any $\\mathrm{SMD}_{js}$ will result in $M = +\\infty$.\n2.  A boolean balance flag, which is `True` if and only if all computed $|\\mathrm{SMD}_{js}|$ values are finite and do not exceed a given threshold $\\tau$. That is, the flag is `True` if $M \\leq \\tau$ and $M$ is finite. Otherwise, the flag is `False`. This provides a simple pass/fail diagnostic for achieving a desired level of covariate balance.\n\nThese steps constitute a complete and rigorous procedure for evaluating the effectiveness of propensity score stratification in balancing observed covariates, a cornerstone of modern observational research.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for propensity score analysis.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"n\": 1000, \"K\": 5, \"tau\": 0.1, \"seed\": 128,\n            \"L_dists\": [lambda n, rng: rng.normal(0, 1, n),\n                        lambda n, rng: rng.binomial(1, 0.5, n),\n                        lambda n, rng: rng.uniform(-1, 1, n)],\n            \"beta0\": 0.2, \"beta_coeffs\": np.array([0.7, -0.6, 0.4])\n        },\n        # Case 2 (randomization-like)\n        {\n            \"n\": 80, \"K\": 4, \"tau\": 0.1, \"seed\": 999,\n            \"L_dists\": [lambda n, rng: rng.normal(0, 1, n),\n                        lambda n, rng: rng.binomial(1, 0.5, n),\n                        lambda n, rng: rng.uniform(-1, 1, n)],\n            \"beta0\": 0.0, \"beta_coeffs\": np.array([0.0, 0.0, 0.0])\n        },\n        # Case 3 (near-separation)\n        {\n            \"n\": 400, \"K\": 5, \"tau\": 0.1, \"seed\": 17,\n            \"L_dists\": [lambda n, rng: rng.normal(0, 1, n),\n                        lambda n, rng: rng.binomial(1, 0.7, n),\n                        lambda n, rng: rng.normal(0, 0.2**2, n)],\n            \"beta0\": 2.0, \"beta_coeffs\": np.array([2.5, -2.0, 0.0])\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        result = run_one_case(**case)\n        results.append(result)\n\n    # Format the final output as specified\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    print(output_str)\n\ndef run_one_case(n, K, tau, seed, L_dists, beta0, beta_coeffs):\n    \"\"\"\n    Executes a single test case for propensity score analysis.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Data Generation\n    p = len(L_dists)\n    L = np.zeros((n, p))\n    for j in range(p):\n        L[:, j] = L_dists[j](n, rng)\n\n    eta = beta0 + L @ beta_coeffs\n    true_ps = expit(eta)\n    A = rng.binomial(1, true_ps, n)\n\n    # 2. Logistic Regression (IRLS)\n    X = np.c_[np.ones(n), L]\n    coeffs = np.zeros(p + 1)\n    \n    # IRLS iterations for MLE of logistic regression\n    for _ in range(25): # 25 iterations are generally sufficient\n        eta_hat = X @ coeffs\n        p_hat = expit(eta_hat)\n        \n        # Add a small epsilon for numerical stability\n        weights_diag = p_hat * (1 - p_hat) + 1e-8\n        \n        # Working response for IRLS\n        z = eta_hat + (A - p_hat) / weights_diag\n        \n        # Weighted least squares step\n        XT_W = X.T * weights_diag[np.newaxis, :]\n        XT_W_X = XT_W @ X\n        XT_W_z = XT_W @ z\n        \n        try:\n            new_coeffs = np.linalg.solve(XT_W_X, XT_W_z)\n            if np.linalg.norm(new_coeffs - coeffs) < 1e-8:\n                coeffs = new_coeffs\n                break\n            coeffs = new_coeffs\n        except np.linalg.LinAlgError:\n            # In case of singularity, stop updating\n            break\n\n    e_hat = expit(X @ coeffs)\n\n    # 3. Stratification\n    if K > 1:\n        quantiles = np.linspace(0, 100, K + 1)[1:-1]\n        cut_points = np.percentile(e_hat, quantiles)\n        # np.digitize returns 0 for x <= cut_points[0], 1 for cut_points[0] < x <= cut_points[1], etc.\n        stratum_indices = np.digitize(e_hat, cut_points)\n    else: # K=1 means one stratum for all data\n        stratum_indices = np.zeros(n, dtype=int)\n\n\n    # 4. SMD Calculation\n    smd_values = []\n    for s in range(K):\n        stratum_mask = (stratum_indices == s)\n        A_stratum = A[stratum_mask]\n        L_stratum = L[stratum_mask]\n\n        treated_mask = (A_stratum == 1)\n        untreated_mask = (A_stratum == 0)\n\n        n_treated = np.sum(treated_mask)\n        n_untreated = np.sum(untreated_mask)\n\n        # Handle strata with no overlap\n        if n_treated == 0 or n_untreated == 0:\n            for j in range(p):\n                smd_values.append(np.inf)\n            continue\n\n        L_treated = L_stratum[treated_mask]\n        L_untreated = L_stratum[untreated_mask]\n\n        for j in range(p):\n            mean_treated = np.mean(L_treated[:, j])\n            mean_untreated = np.mean(L_untreated[:, j])\n            \n            # Use ddof=1 for sample variance\n            var_treated = np.var(L_treated[:, j], ddof=1) if n_treated > 1 else 0\n            var_untreated = np.var(L_untreated[:, j], ddof=1) if n_untreated > 1 else 0\n\n            pooled_var = (var_treated + var_untreated) / 2\n            \n            numerator = mean_treated - mean_untreated\n            denominator = np.sqrt(pooled_var)\n\n            if denominator == 0:\n                smd = 0.0 if numerator == 0 else np.inf\n            else:\n                smd = numerator / denominator\n            \n            smd_values.append(smd)\n\n    # 5. Diagnostic Aggregation\n    if not smd_values: # This can happen if n=0 or other weird cases\n        M = 0.0\n    else:\n        M = np.max(np.abs(smd_values)) if smd_values else 0.0\n        \n    balance_flag = np.isfinite(M) and M <= tau\n    \n    return [M, balance_flag]\n\n\nsolve()\n```"
        }
    ]
}