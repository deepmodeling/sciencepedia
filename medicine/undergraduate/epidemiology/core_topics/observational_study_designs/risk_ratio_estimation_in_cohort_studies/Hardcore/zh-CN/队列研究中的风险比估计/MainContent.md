## 引言
在流行病学和公共卫生研究中，量化暴露与疾病之间的关联强度是做出[科学推断](@entry_id:155119)和制定有效干预措施的基石。在各类关联性度量中，风险比（Risk Ratio, RR）因其直观易懂的特性，在队列研究中扮演着至关重要的角色。然而，从观察数据中准确[估计风险](@entry_id:139340)比并进行有效的因果推断，远非简单的公式计算所能概括。研究者常常面临着由混杂、偏倚和复杂[数据结构](@entry_id:262134)带来的严峻挑战，这些问题构成了从理论到实践的关键知识鸿沟。

本文旨在系统性地指导读者跨越这一鸿沟。我们将从风险比估计的基础出发，逐步深入探讨其背后的复杂性与解决方案。在第一章“原则与机制”中，我们将奠定理论基础，详细阐述风险比的计算、与其他度量的区别，以及处理混杂、删失和竞争性风险等核心方法学问题。随后，在第二章“应用与跨学科联系”中，我们将展示风险比如何在流行病学、临床研究和公共卫生政策等领域发挥作用，并介绍用于调整混杂和处理偏倚的高级模型。最后，通过第三章“动手实践”，你将有机会运用所学知识，解决具体的分析案例，从而巩固对关键概念的理解。通过这三章的学习，你将能够自信地在队列研究中估计和解释风险比，并审慎地评估其因果意义。

## 原则与机制

在上一章介绍性讨论的基础上，本章将深入探讨队列研究中风险比（Risk Ratio, RR）估计的核心原则与机制。我们将从风险的基本定义出发，系统性地阐述其在流行病学研究中的计算、解释以及与其他关联性度量的区别。此外，本章还将探讨从关联到因果推断的关键概念，并解决在实际数据分析中遇到的复杂问题，如混杂、删失、竞争性风险和时依性偏倚。

### 风险与风险比的基本概念

在流行病学中，衡量疾病发生频率的最基本指标之一是**累积发病率（cumulative incidence）**，通常简称为**风险（risk）**。它衡量的是在一个特定时间段内，初始无病的人群中发生特定健康结果（如疾病）的比例。风险是一个概率，其值介于 $0$ 和 $1$ 之间。

为了精确地计算风险，必须明确两个关键要素：分子和分母。

1.  **分子**：在指定随访期内新出现（即**新发**）的结局事件数量。
2.  **分母**：在随访开始时（基线），所有可能发生该结局事件的**风险人群（at-risk population）**的总数。

“风险人群”的定义至关重要。它必须仅包括在研究开始时没有患上目标疾病，但有潜力发展成该疾病的个体。已经患病的人（患病病例）不能成为新发病例，因此必须从分母中排除。

设想一个理想化的场景：在一个职业健康研究中，研究人员在一个固定的队列（称为**封闭队列**）中追踪一群工人，从时间 $t=0$ 开始，持续一个固定的随访期 $T$ 。在这个队列中，成员资格在基线时就已确定，没有新成员加入，也没有失访。假设我们想计算该队列在时间段 $[0, T]$ 内发生某种[呼吸系统](@entry_id:163483)疾病的风险。正确的计算方法是：将此期间新发病例的数量，除以在 $t=0$ 时所有易感且无该疾病的工人的总数。这个基线风险人群是在整个随访期间或直到他们发生事件为止的追踪对象。

在队列研究中，我们通常的目标是比较不同暴露组之间的风险。最常用的相对比较度量是**风险比（Risk Ratio, RR）**。它被定义为暴露组的累积发病率（$R_1$）与非暴露组的累积发病率（$R_0$）之比：

$$RR = \frac{R_1}{R_0}$$

风险比的解释非常直观。例如，如果 $RR = 2$，意味着暴露组发生该疾病的风险是非暴露组的两倍。如果 $RR = 0.5$，则意味着暴露使风险减半，表明该暴露是一个保护性因素。当 $RR = 1$ 时，表明暴露与疾病风险之间没有关联。

### 风险比的比较与特性

虽然风险比因其解释的直接性而备受青睐，但在流行病学中还有其他重要的关联性度量，如**优势比（Odds Ratio, OR）**和**风险差（Risk Difference, RD）**。理解它们之间的关系和各自的特性对于正确选择和解释分析结果至关重要。

优势（odds）定义为事件发生的概率与不发生的概率之比，即 $O = \frac{p}{1-p}$。相应地，**优势比**是暴露组的优势与非暴露组的优势之比：

$$OR = \frac{O_1}{O_0} = \frac{R_1 / (1-R_1)}{R_0 / (1-R_0)}$$

通过简单的代数变换，我们可以揭示风险比和优势比之间的精确数学关系 ：

$$OR = RR \times \frac{1-R_0}{1-R_1}$$

这个公式揭示了几个重要特性：

1.  **罕见病假设（Rare Outcome Assumption）**：当研究的结局在所有组中都非常罕见时（即 $R_1$ 和 $R_0$ 都接近于 $0$），那么 $1-R_1$ 和 $1-R_0$ 都将接近于 $1$。在这种情况下，分数项 $\frac{1-R_0}{1-R_1}$ 近似等于 $1$，因此 $OR \approx RR$。这是优势比能够在病例对照研究中用来近似风险比的理论基础 。

2.  **OR的极端性**：当疾病不罕见时，OR总是比RR更“极端”，即离 $1$ 更远。如果暴露是风险因素（$RR > 1$），则 $R_1 > R_0$，导致 $\frac{1-R_0}{1-R_1} > 1$，因此 $OR > RR$。反之，如果暴露是保护性因素（$RR  1$），则 $R_1  R_0$，导致 $\frac{1-R_0}{1-R_1}  1$，因此 $OR  RR$。这意味着OR会比RR夸大效应的强度（无论是风险还是保护作用） 。

#### 可折叠性

**可折叠性（Collapsibility）** 是指在一个分层变量（如年龄、性别）的各个层内恒定的关联度量，在合并（或“折叠”）数据后，其粗略估计值是否仍然等于该恒定值。风险比是一个可折叠的度量。具体来说，如果一个分层变量与暴露无关，并且层内的风险比是恒定的，那么粗略的风险比将等于这个共同的层内风险比 。

相比之下，优势比是**不可折叠的（non-collapsible）**。即使在分层变量与暴露无关的情况下，如果该变量本身是结局的一个风险因素，那么粗略的OR通常也不会等于共同的层内OR。粗略OR会偏离层内OR，通常更接近于1。这种特性使得在解释OR时需要特别小心，因为即使没有传统意义上的混杂（即混杂因素与暴露无关），分层和粗略的OR也可能不一致。

#### 抽样不变性

另一个关键区别在于不同度量在不同研究设计下的稳定性。优势比具有一个非常重要的**不变性（invariance）**属性。在一个基于结局的抽样设计中（如病例对照研究，我们选择一定数量的病例和一定数量的非病例进行研究），计算出的OR仍然是对源队列中OR的无偏估计。然而，风险比和风险差在这种抽样方案下会被扭曲，其估计值依赖于病例和非病例的抽样比例 。正是这种对病例对照抽样的不变性，使得优势比成为该类研究设计的核心关联度量。

### 从关联到因果：混杂与目标试验模拟

队列研究计算出的风险比本质上是一个**关联性（associational）**度量。它告诉我们，在观察到的数据中，一个群体的风险与另一个群体的风险有何不同。然而，我们通常更关心**因果性（causal）**问题：如果我们将暴露施加于人群，将会对风险产生什么影响？

为了形式化这一区别，我们可以使用**潜在结局（potential outcomes）**框架 。对于每个个体，我们可以设想两个潜在结局：$Y^1$ 是该个体在接受暴露（$A=1$）下的结局，而 $Y^0$ 是在未接受暴露（$A=0$）下的结局。**因果风险比**被定义为在整个人群中，如果每个人都接受暴露的风险与每个人都不接受暴露的风险之比：

$$RR^{\text{causal}} = \frac{P(Y^1=1)}{P(Y^0=1)}$$

而我们从数据中直接计算的**关联风险比**是：

$$RR^{\text{associational}} = \frac{P(Y=1 | A=1)}{P(Y=1 | A=0)}$$

这两个量在大多数[观察性研究](@entry_id:174507)中并不相等。其根本原因在于**混杂（confounding）**。混杂发生于暴露组和非暴露组在基线时就存在系统性差异，而这些差异本身也与结局相关。

#### [辛普森悖论](@entry_id:136589)：混杂的极端例证

混杂最引人注目的表现之一是**辛普森悖论（Simpson's Paradox）**。在这种情况下，关联的方向在分层分析和合并分析中完全相反。例如，一个暴露可能在每一个亚组（如高风险人群和低风险人群）中都显示为有害的（$RR  1$），但在合并整个数据后，却表现为保护性的（$RR  1$） 。

这种悖论的发生，是因为暴露和混杂因素（即分层变量）的分布不均衡。设想一个分层变量$Z$（例如，高风险基础疾病），它同时影响结局风险和接受暴露的可能性。如果在非暴露组中，高风险个体（$Z=1$）的比例（$\beta$）远高于暴露组中的比例（$\alpha$），那么非暴露组的整体（粗略）风险会被高风险个体“拉高”。即使在每个风险层内暴露都轻微增加了风险，但由于暴露组主要由低风险个体构成，其整体风险可能反而低于那个被高风险个体主导的非暴露组 。

#### 识别因果效应的条件

为了使[观察性研究](@entry_id:174507)中的关联风险比能够等同于因果风险比，或通过调整来估计因果风险比，必须满足三个核心的**可识别性假设（identifiability assumptions）** ：

1.  **一致性（Consistency）**：一个个体观察到的结局，就是其在所接受的暴露水平下的潜在结局。
2.  **可交换性（Exchangeability）**（或称无混杂）：在给定一组测量的基线协变量 $L$ 的条件下，暴露分配与潜在结局无关。这意味着在相同的 $L$ 水平内，暴露组和非暴露组是可以相互比较的，就好像是随机分配的一样。这是“无未测量混杂”的数学表达。
3.  **正性（Positivity）**：在协变量 $L$ 的每个层内，接受或不接受暴露的概率都大于零。这确保了在所有类型的个体中都存在暴露组和非暴露组，使得比较成为可能。

#### 目标试验模拟

为了系统地应用这些原则并减少观察性研究中的偏倚，现代流行病学提倡**目标试验模拟（Target Trial Emulation）**框架 。这个方法要求研究者首先明确地设计一个他们想要模拟的、理想化的随机对照试验（即“目标试验”），然后使用观察性数据来“模拟”这个试验的每个组成部分。关键步骤包括：

*   **资格标准**：明确定义谁可以入组，所有标准必须在随访开始（时间零点）前确定。
*   **时间零点**：为所有参与者定义一个统一的随访起始点。
*   **干预策略**：明确定义要比较的干预措施。
*   **结局**：精确定义要测量的结局。
*   **随访**：明确随访的起止时间。
*   **分析计划**：指定分析方法，通常是“意向治疗分析（intention-to-treat）”，即根据最初的（模拟）分配进行分析，无论后续是否依从。

通过严谨地定义这些要素，研究者可以更清晰地识别和处理观察性数据中的潜在偏倚，从而更接近于估计真实的因果效应。

### 估计中的实际挑战与解决方案

在实际应用中，从队列数据中[估计风险](@entry_id:139340)比及其[置信区间](@entry_id:138194)会遇到一系列统计和方法学上的挑战。

#### [统计推断](@entry_id:172747)与[置信区间](@entry_id:138194)

我们从样本数据中计算出的风险比是一个**点估计（point estimate）**，它本身会受到抽样变异的影响。为了量化这种不确定性，我们必须计算**[置信区间](@entry_id:138194)（Confidence Interval, CI）**。

直接为风险比构建[置信区间](@entry_id:138194)是困难的，因为其抽样分布通常是[偏态](@entry_id:178163)的，并且其值被限制在正数范围内。一种标准且稳健的方法是先对风险比进行**[对数变换](@entry_id:267035)（log transformation）** 。$\ln(RR)$的抽样分布更接近正态分布，且其取值范围是整个[实数轴](@entry_id:148276)，这使得基于[正态近似](@entry_id:261668)的Wald方法更为适用。

构建$95\%$[置信区间](@entry_id:138194)的步骤如下：
1.  计算风险比的点估计值 $\hat{RR} = \hat{R}_1 / \hat{R}_0$。
2.  计算对数风险比 $\ln(\hat{RR})$。
3.  估计 $\ln(\hat{RR})$ 的标准误（Standard Error, SE）。对于来自两个独立二项分布的数据，其方差的估计公式为：
    $$\widehat{\text{Var}}(\ln(\hat{RR})) = \frac{1 - \hat{R}_1}{a} + \frac{1 - \hat{R}_0}{c}$$
    其中 $a$ 和 $c$ 分别是暴露组和非暴露组的事件数。标准误即为方差的平方根 $SE(\ln(\hat{RR})) = \sqrt{\widehat{\text{Var}}(\ln(\hat{RR}))}$。
4.  在对数尺度上构建Wald[置信区间](@entry_id:138194)：
    $$\ln(\hat{RR}) \pm 1.96 \times SE(\ln(\hat{RR}))$$
5.  将区间的上下限通过**指数变换（exponentiation）**转换回原始尺度：
    $$CI_{RR} = [\exp(\text{下限}), \exp(\text{上限})]$$

值得注意的是，通过这种方法得到的[置信区间](@entry_id:138194)在原始风险比尺度上是**不对称的** 。这是因为对数尺度上的加性对称性（$\ln(\hat{RR}) \pm \delta$）在转换回原始尺度后，变成了[乘性](@entry_id:187940)对称性（$[\hat{RR} / e^{\delta}, \hat{RR} \times e^{\delta}]$）。因此，[点估计](@entry_id:174544)值到[置信区间](@entry_id:138194)上限的距离通常不等于其到下限的距离。

#### [生存数据分析](@entry_id:190868)中的挑战

在许多队列研究中，数据以“生存时间”或“事件发生时间”的形式出现，这引入了更复杂的挑战。

**1. 删失（Censoring）**

当我们在随访结束前就失去了某个研究对象的结局信息时，就会发生**删失**（最常见的是[右删失](@entry_id:164686)）。例如，患者可能搬家失联，或研究在他们发生事件前就结束了。处理删失数据的标准方法，如[Kaplan-Meier](@entry_id:169317)方法，依赖于一个关键假设：**独立删失（independent censoring）**或称**非信息性删失（non-informative censoring）** 。该假设要求在任何时间点，被删失的个体在未来的事件风险与那些仍在随访中的个体是相同的。如果删失的原因与个体的事件风险相关（例如，病情更重的患者更可能退出研究），那么删失就是**信息性的（informative）**，此时使用标准方法将导致有偏的结果。

**2. 竞争性风险（Competing Risks）**

在某些研究中，个体可能经历多种类型的结局事件，而任何一种事件的发生都会妨碍其他事件的发生。这些事件被称为**竞争性风险**。例如，在研究某种特定原因的死亡率时，“其他原因导致的死亡”就是一个竞争性事件 。

一个常见的错误是将竞争性事件当作普通的[右删失](@entry_id:164686)来处理。这种做法严重违反了独立删失假设。因为一个经历了竞争性事件的个体，其发生主要结局的概率立即变为零，这与仍在随访的个体的风险完全不同。使用[Kaplan-Meier](@entry_id:169317)方法（计算 $1 - S_{KM}$）在这种情况下会人为地高估主要结局的风险，因为它错误地将本已不可能发生事件的个体（那些经历了竞争性事件的）的“风险”重新分配给了仍在随访的人群。

正确的处理方法是使用竞争性风险模型来估计**累积发生函数（Cumulative Incidence Function, CIF）**。CIF估计的是在存在竞争性事件的情况下，到某个时间点为止，发生特定类型事件的真实概率。在一个没有其他失访的封闭队列中，特定事件的CIF可以简单地通过将该事件的总发生数除以基线时的总人数来计算 。

**3. 时依性暴露与永恒时间偏倚（Immortal Time Bias）**

在许多观察性研究中，暴露（如开始服药）并不是在基线时就确定的，而是在随访过程中的不同时间点（**交错起始**）发生的。分析这类**时依性暴露（time-varying exposure）**时，一个严重的陷阱是**永恒时间偏倚（immortal time bias）** 。

这种偏倚通常源于一种错误的分析方法：将个体分为“在随访期间曾暴露过”和“从未暴露过”两组。在这种分类下，“曾暴露过”组中的每个成员都必须存活到他们实际开始暴露的那一刻。从研究开始到他们暴露开始的这段时间，他们是“永恒的”，因为根据定义，他们不可能在这段时间内死亡（否则他们会被分到“从未暴露过”组）。这种人为地为暴露组赋予了一段无风险的生存时间，从而系统地偏向于暴露是有益的结论。

避免永恒时间偏倚的正确方法是将暴露视为随时间变化的变量。在分析中，个体的随访时间应该被切分：在暴露开始前的时间段，他们对非暴露组的风险做贡献；在暴露开始后的时间段，他们对暴露组的风险做贡献。这种时依性分析方法，例如使用时依性协变量的生存模型，能够正确地归因风险时间，从而避免永恒时间偏倚 。