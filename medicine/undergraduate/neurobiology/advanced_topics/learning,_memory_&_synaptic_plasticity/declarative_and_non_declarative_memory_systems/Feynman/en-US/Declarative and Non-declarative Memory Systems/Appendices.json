{
    "hands_on_practices": [
        {
            "introduction": "The formation of associations, a cornerstone of non-declarative memory, can be described with mathematical precision. This exercise  introduces the Rescorla-Wagner model, a foundational theory where learning is driven by prediction error. By working through this problem, you will derive the trial-by-trial change in associative strength, giving you a hands-on understanding of how theoretical models generate learning curves that can be quantitatively compared against empirical observations.",
            "id": "5011414",
            "problem": "A central distinction in neurobiology separates declarative (explicit) memory from non-declarative (implicit) memory. Associative conditioning, such as a conditioned stimulus (CS) paired with an unconditioned stimulus (US) producing a conditioned response (CR), is a form of non-declarative memory. Error-driven learning in Pavlovian conditioning is well captured by a widely used and experimentally supported rule in associative learning. Consider a scenario in which a subject undergoes CS–US pairings in a delay conditioning paradigm, and the associative strength of the CS–CR linkage after trial $n$ is denoted by $V_n$. The learning update is driven by the difference between the expected outcome and the experienced outcome. Let the learning rate parameters be $\\alpha$ (CS salience) and $\\beta$ (US effectiveness), and let $\\lambda$ be the asymptotic associative strength corresponding to the US magnitude.\n\nStarting from a well-tested error-correction learning rule, the associative strength updates across trials according to a first-order recursion driven by prediction error. The initial associative strength is $V_0 = 0$.\n\nYou are provided the following experimental configuration:\n- Parameters: $\\alpha = 0.3$, $\\beta = 0.2$, and $\\lambda = 1.0$.\n- An empirically observed CR growth curve across trials is described by the function $V_{\\mathrm{emp}}(n) = \\lambda \\left(1 - \\exp(-\\gamma n)\\right)$ with $\\gamma = 0.1$.\n- Consider $N = 10$ consecutive conditioning trials.\n\nUsing the foundational error-correction principle for non-declarative associative learning and the above parameters, derive the expected $V_n$ as a function of $n$ and then compute the mean squared discrepancy across the first $N$ trials,\n$$\nS_N = \\sum_{n=1}^{N} \\left(V_n - V_{\\mathrm{emp}}(n)\\right)^{2}.\n$$\nExpress the final value of $S_N$ as a dimensionless decimal and round your answer to $4$ significant figures.",
            "solution": "Non-declarative associative learning in Pavlovian conditioning is often modeled by a prediction error rule in which the change in associative strength on trial $n$ is proportional to the difference between the experienced outcome and the current expectation. A well-tested formulation is the Rescorla–Wagner update rule\n$$\n\\Delta V_n = \\alpha \\beta \\left(\\lambda - V_n\\right),\n$$\nwhere $\\alpha$ captures conditioned stimulus (CS) salience, $\\beta$ captures unconditioned stimulus (US) effectiveness, and $\\lambda$ is the asymptotic associative strength corresponding to the US magnitude. Writing this as a recursion,\n$$\nV_{n+1} = V_n + \\alpha \\beta \\left(\\lambda - V_n\\right) = (1 - \\alpha \\beta) V_n + \\alpha \\beta \\lambda.\n$$\nThis is a linear, constant-coefficient first-order difference equation. Define $c = 1 - \\alpha \\beta$. Then\n$$\nV_{n+1} - \\lambda = c \\left(V_n - \\lambda\\right).\n$$\nIterating yields\n$$\nV_n - \\lambda = c^{n} \\left(V_0 - \\lambda\\right),\n$$\nand therefore the closed-form solution is\n$$\nV_n = \\lambda - (\\lambda - V_0)\\, c^{n} = \\lambda \\left[1 - c^{n}\\right] \\quad \\text{when } V_0 = 0.\n$$\nWith the provided parameters $\\alpha = 0.3$, $\\beta = 0.2$, $\\lambda = 1.0$, we have\n$$\n\\alpha \\beta = 0.3 \\times 0.2 = 0.06, \\quad c = 1 - \\alpha \\beta = 0.94,\n$$\nand thus\n$$\nV_n = 1.0 \\times \\left[1 - (0.94)^{n}\\right] = 1 - (0.94)^{n}.\n$$\n\nThe empirical CR curve is\n$$\nV_{\\mathrm{emp}}(n) = \\lambda \\left(1 - \\exp(-\\gamma n)\\right) = 1 - \\exp(-0.1 n).\n$$\nThe discrepancy on trial $n$ is\n$$\nD_n = V_n - V_{\\mathrm{emp}}(n) = \\left[1 - (0.94)^{n}\\right] - \\left[1 - \\exp(-0.1 n)\\right] = \\exp(-0.1 n) - (0.94)^{n}.\n$$\nThe mean squared discrepancy across the first $N = 10$ trials is\n$$\nS_{10} = \\sum_{n=1}^{10} \\left[D_n\\right]^2 = \\sum_{n=1}^{10} \\left(\\exp(-0.1 n) - (0.94)^{n}\\right)^2.\n$$\n\nWe now compute the numerical values needed for $n = 1$ through $n = 10$:\n- For $n = 1$: $\\exp(-0.1) \\approx 0.9048374180$, $(0.94)^{1} = 0.94$, $D_1 \\approx -0.0351625820$, $D_1^2 \\approx 0.0012364072$.\n- For $n = 2$: $\\exp(-0.2) \\approx 0.8187307531$, $(0.94)^{2} = 0.8836$, $D_2 \\approx -0.0648692469$, $D_2^2 \\approx 0.0042080192$.\n- For $n = 3$: $\\exp(-0.3) \\approx 0.7408182207$, $(0.94)^{3} \\approx 0.830584$, $D_3 \\approx -0.0897657793$, $D_3^2 \\approx 0.0080578948$.\n- For $n = 4$: $\\exp(-0.4) \\approx 0.6703200460$, $(0.94)^{4} \\approx 0.78074896$, $D_4 \\approx -0.1104289140$, $D_4^2 \\approx 0.0121945440$.\n- For $n = 5$: $\\exp(-0.5) \\approx 0.6065306597$, $(0.94)^{5} \\approx 0.7339040224$, $D_5 \\approx -0.1273733627$, $D_5^2 \\approx 0.0162239735$.\n- For $n = 6$: $\\exp(-0.6) \\approx 0.5488116361$, $(0.94)^{6} \\approx 0.6898697809$, $D_6 \\approx -0.1410581448$, $D_6^2 \\approx 0.0198974000$.\n- For $n = 7$: $\\exp(-0.7) \\approx 0.4965853038$, $(0.94)^{7} \\approx 0.6484786056$, $D_7 \\approx -0.1518933018$, $D_7^2 \\approx 0.0230715750$.\n- For $n = 8$: $\\exp(-0.8) \\approx 0.4493289641$, $(0.94)^{8} \\approx 0.6095608892$, $D_8 \\approx -0.1602319251$, $D_8^2 \\approx 0.0256742700$.\n- For $n = 9$: $\\exp(-0.9) \\approx 0.4065696597$, $(0.94)^{9} \\approx 0.5729872359$, $D_9 \\approx -0.1664175762$, $D_9^2 \\approx 0.0276948090$.\n- For $n = 10$: $\\exp(-1.0) \\approx 0.3678794412$, $(0.94)^{10} \\approx 0.5386080018$, $D_{10} \\approx -0.1707285606$, $D_{10}^2 \\approx 0.0291482310$.\n\nSumming these squared discrepancies,\n$$\nS_{10} \\approx 0.0012364072 + 0.0042080192 + 0.0080578948 + 0.0121945440 + 0.0162239735 + 0.0198974000 + 0.0230715750 + 0.0256742700 + 0.0276948090 + 0.0291482310,\n$$\nwhich yields\n$$\nS_{10} \\approx 0.1674071237.\n$$\nRounded to $4$ significant figures, the final value is\n$$\nS_{10} \\approx 0.1674.\n$$\nThis dimensionless value quantifies how closely the error-driven non-declarative associative learning prediction matches the empirical curve across the specified trials.",
            "answer": "$$\\boxed{0.1674}$$"
        },
        {
            "introduction": "To study declarative memory, we must distinguish a subject's true memory accuracy from their tendency to guess. This practice  applies Signal Detection Theory (SDT), a powerful framework from psychophysics, to a recognition memory task. You will learn to derive and calculate the core SDT metrics of sensitivity ($d'$) and criterion ($c$), and see how these quantitative measures allow us to interpret the distinct roles of the hippocampus and perirhinal cortex in declarative memory.",
            "id": "5011420",
            "problem": "A recognition-memory experiment probes declarative memory by asking participants to judge whether test items are previously studied (\"old\") or novel (\"new\"). Behavioral performance is summarized by the hit rate $H$ (probability of responding \"old\" to old items) and the false alarm rate $FA$ (probability of responding \"old\" to new items). Assume an equal-variance Gaussian Signal Detection Theory (SDT) model in which the \"new\" (noise) evidence distribution is $\\mathcal{N}(0,1)$ and the \"old\" (signal) evidence distribution is $\\mathcal{N}(d',1)$, and a single decision criterion separates \"old\" from \"new\" responses. The hippocampus is known to support recollection, while the perirhinal cortex is known to support familiarity within the declarative memory system.\n\nIn three sessions with the same participants, the following rates were observed:\n- Control: $H_{\\mathrm{ctrl}} = 0.78$, $FA_{\\mathrm{ctrl}} = 0.22$.\n- Perirhinal disruption: $H_{\\mathrm{peri}} = 0.70$, $FA_{\\mathrm{peri}} = 0.30$.\n- Hippocampal disruption: $H_{\\mathrm{hip}} = 0.76$, $FA_{\\mathrm{hip}} = 0.24$.\n\nStarting from the equal-variance Gaussian SDT framework and basic probabilistic definitions, derive from first principles how to compute the sensitivity index $d'$ and the decision criterion $c$ for each condition from $H$ and $FA$ without assuming any pre-given formula. Then, compute $d'$ and $c$ for each of the three conditions. Finally, define the net relative disruption metric $\\delta$ as the difference in $d'$ change from control between the perirhinal and hippocampal disruptions:\n$$\\delta \\equiv \\left(d'_{\\mathrm{peri}} - d'_{\\mathrm{ctrl}}\\right) - \\left(d'_{\\mathrm{hip}} - d'_{\\mathrm{ctrl}}\\right).$$\nProvide $\\delta$ as a single closed-form analytic expression in terms of the inverse cumulative distribution function of the standard normal distribution $\\Phi^{-1}(\\cdot)$. Do not numerically approximate $\\delta$ in your final answer.",
            "solution": "The problem is framed within the equal-variance Gaussian Signal Detection Theory model. Let $x$ be the continuous variable representing memory evidence. According to the model, evidence for \"new\" items (noise) is drawn from a normal distribution with mean $\\mu_N = 0$ and standard deviation $\\sigma_N = 1$. The probability density function (PDF) is $f_N(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2})$. Evidence for \"old\" items (signal) is drawn from a normal distribution with mean $\\mu_S = d'$ and standard deviation $\\sigma_S = 1$. The PDF is $f_S(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{(x-d')^2}{2})$.\n\nA participant adopts a decision criterion, $c$. If the memory evidence $x$ for a test item exceeds this criterion ($x > c$), the participant responds \"old\". Otherwise ($x \\le c$), the participant responds \"new\".\n\nThe hit rate, $H$, is the probability of responding \"old\" to a genuinely old item.\n$$H = P(\\text{response is \"old\"} | \\text{item is old}) = P(x > c | x \\sim \\mathcal{N}(d',1))$$\nThe false alarm rate, $FA$, is the probability of responding \"old\" to a new item.\n$$FA = P(\\text{response is \"old\"} | \\text{item is new}) = P(x > c | x \\sim \\mathcal{N}(0,1))$$\n\nTo derive the expressions for $d'$ and $c$, we utilize the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z) = P(Z \\le z)$ where $Z \\sim \\mathcal{N}(0,1)$. The probability $P(Z > z)$ is $1 - \\Phi(z)$.\n\nFirst, consider the false alarm rate, $FA$. The evidence $x$ is drawn from the standard normal distribution $\\mathcal{N}(0,1)$.\n$$FA = P(x > c) = 1 - P(x \\le c) = 1 - \\Phi(c)$$\nRearranging for $\\Phi(c)$ gives $\\Phi(c) = 1 - FA$. Applying the inverse CDF, $\\Phi^{-1}(\\cdot)$, to both sides yields the expression for the criterion $c$:\n$$c = \\Phi^{-1}(1 - FA)$$\nUsing the symmetry property of the standard normal distribution, $\\Phi^{-1}(1-p) = -\\Phi^{-1}(p)$, we can write $c$ as:\n$$c = -\\Phi^{-1}(FA)$$\n\nNext, consider the hit rate, $H$. The evidence $x$ is drawn from $\\mathcal{N}(d',1)$. To use the standard normal CDF, we standardize the variable by subtracting the mean $d'$.\n$$H = P(x > c) = P(x - d' > c - d')$$\nSince $x \\sim \\mathcal{N}(d',1)$, the variable $z = x - d'$ follows the standard normal distribution $\\mathcal{N}(0,1)$. Thus,\n$$H = P(z > c - d') = 1 - P(z \\le c - d') = 1 - \\Phi(c - d')$$\nRearranging gives $\\Phi(c - d') = 1 - H$. Applying the inverse CDF gives:\n$$c - d' = \\Phi^{-1}(1 - H)$$\nUsing the symmetry property, we get:\n$$c - d' = -\\Phi^{-1}(H)$$\n\nWe now have a system of two equations for the two unknowns, $d'$ and $c$:\n1. $c = -\\Phi^{-1}(FA)$\n2. $c - d' = -\\Phi^{-1}(H)$\n\nTo find $d'$, we substitute the first equation into the second:\n$$-\\Phi^{-1}(FA) - d' = -\\Phi^{-1}(H)$$\nSolving for $d'$, the sensitivity index, we obtain:\n$$d' = \\Phi^{-1}(H) - \\Phi^{-1}(FA)$$\nThis completes the derivation from first principles.\n\nNow, we compute $d'$ and $c$ for each of the three experimental conditions.\n\n**Control Condition:**\n$H_{\\mathrm{ctrl}} = 0.78$, $FA_{\\mathrm{ctrl}} = 0.22$.\n$d'_{\\mathrm{ctrl}} = \\Phi^{-1}(0.78) - \\Phi^{-1}(0.22)$\n$c_{\\mathrm{ctrl}} = -\\Phi^{-1}(0.22)$\nNoting that $0.78 = 1 - 0.22$, we use the symmetry property $\\Phi^{-1}(0.78) = -\\Phi^{-1}(0.22)$.\nSo, $d'_{\\mathrm{ctrl}} = -\\Phi^{-1}(0.22) - \\Phi^{-1}(0.22) = -2\\Phi^{-1}(0.22)$, or $d'_{\\mathrm{ctrl}} = 2\\Phi^{-1}(0.78)$.\nAnd $c_{\\mathrm{ctrl}} = \\Phi^{-1}(0.78)$. This implies $c_{\\mathrm{ctrl}} = d'_{\\mathrm{ctrl}}/2$.\n\n**Perirhinal Disruption Condition:**\n$H_{\\mathrm{peri}} = 0.70$, $FA_{\\mathrm{peri}} = 0.30$.\n$d'_{\\mathrm{peri}} = \\Phi^{-1}(0.70) - \\Phi^{-1}(0.30)$\n$c_{\\mathrm{peri}} = -\\Phi^{-1}(0.30)$\nSince $0.70 = 1 - 0.30$, we have $\\Phi^{-1}(0.70) = -\\Phi^{-1}(0.30)$.\nSo, $d'_{\\mathrm{peri}} = \\Phi^{-1}(0.70) - (-\\Phi^{-1}(0.70)) = 2\\Phi^{-1}(0.70)$.\nAnd $c_{\\mathrm{peri}} = \\Phi^{-1}(0.70)$, which implies $c_{\\mathrm{peri}} = d'_{\\mathrm{peri}}/2$.\n\n**Hippocampal Disruption Condition:**\n$H_{\\mathrm{hip}} = 0.76$, $FA_{\\mathrm{hip}} = 0.24$.\n$d'_{\\mathrm{hip}} = \\Phi^{-1}(0.76) - \\Phi^{-1}(0.24)$\n$c_{\\mathrm{hip}} = -\\Phi^{-1}(0.24)$\nSince $0.76 = 1 - 0.24$, we have $\\Phi^{-1}(0.76) = -\\Phi^{-1}(0.24)$.\nSo, $d'_{\\mathrm{hip}} = \\Phi^{-1}(0.76) - (-\\Phi^{-1}(0.76)) = 2\\Phi^{-1}(0.76)$.\nAnd $c_{\\mathrm{hip}} = \\Phi^{-1}(0.76)$, which implies $c_{\\mathrm{hip}} = d'_{\\mathrm{hip}}/2$.\n\nThe net relative disruption metric $\\delta$ is defined as the difference between the changes in $d'$ from control for the two disruption conditions:\n$$\\delta \\equiv (d'_{\\mathrm{peri}} - d'_{\\mathrm{ctrl}}) - (d'_{\\mathrm{hip}} - d'_{\\mathrm{ctrl}})$$\nThis expression simplifies to the direct difference between the sensitivities in the two disruption conditions:\n$$\\delta = d'_{\\mathrm{peri}} - d'_{\\mathrm{hip}}$$\nSubstituting the expressions we found for $d'_{\\mathrm{peri}}$ and $d'_{\\mathrm{hip}}$:\n$$\\delta = \\left(\\Phi^{-1}(0.70) - \\Phi^{-1}(0.30)\\right) - \\left(\\Phi^{-1}(0.76) - \\Phi^{-1}(0.24)\\right)$$\nUsing the simplified forms derived from the symmetry of the given data:\n$$d'_{\\mathrm{peri}} = 2\\Phi^{-1}(0.70)$$\n$$d'_{\\mathrm{hip}} = 2\\Phi^{-1}(0.76)$$\nSubstituting these into the simplified expression for $\\delta$:\n$$\\delta = 2\\Phi^{-1}(0.70) - 2\\Phi^{-1}(0.76)$$\nFactoring out the common term, we arrive at the final closed-form analytic expression for $\\delta$:\n$$\\delta = 2(\\Phi^{-1}(0.70) - \\Phi^{-1}(0.76))$$\nThis expression is in the required form, solely in terms of the inverse CDF of the standard normal distribution and the provided numerical rates.",
            "answer": "$$\\boxed{2\\left(\\Phi^{-1}(0.70) - \\Phi^{-1}(0.76)\\right)}$$"
        },
        {
            "introduction": "The gold standard for demonstrating that two cognitive functions rely on separate neural substrates is the double dissociation. This exercise  elevates this concept from a qualitative observation to a rigorous statistical test. You will use a nested model comparison to quantitatively assess whether a two-system model (separating declarative and non-declarative memory) provides a statistically superior explanation for patient performance data than a simpler, single-system model.",
            "id": "5011413",
            "problem": "A research team is investigating whether the neurobiological distinction between declarative memory and non-declarative memory is reflected in task performance across clinical groups. Declarative memory (explicit memory for facts and events) depends critically on the medial temporal lobe, including the hippocampus, whereas non-declarative memory (implicit skill and habit learning) depends critically on cortico-striatal circuits, including the basal ganglia. A hallmark of distinct systems is a double dissociation: amnesic patients with medial temporal lobe lesions show impaired free recall but intact mirror-tracing skill learning, whereas Parkinsonian patients with basal ganglia dysfunction show impaired mirror-tracing skill learning but intact free recall.\n\nTo evaluate whether a two-system model (distinct effects of group on each task) explains variance better than a single-factor model (a single group effect that is the same across tasks), the investigators collected the following dataset: $N = 48$ task observations were obtained by testing $12$ amnesic patients and $12$ Parkinsonian patients on two tasks each, free recall and mirror tracing. Both tasks were standardized within task to $z$-scores before analysis to place them on a common scale.\n\nThey fit two nested linear models to the stacked (long-format) standardized scores, assuming the General Linear Model with independent, normally distributed errors and constant variance across groups and tasks. The restricted single-factor model includes an intercept and independent main effects of group and task; the full two-system model adds the group-by-task interaction to allow task-specific group effects. The restricted model has $p_{R} = 3$ parameters (intercept, group main effect, task main effect), and the full model has $p_{F} = 4$ parameters (intercept, group main effect, task main effect, group-by-task interaction). The sum of squared residuals (error sum of squares) from each fit are:\n- Restricted model error sum of squares: $\\mathrm{SSE}_{R} = 371.8$.\n- Full model error sum of squares: $\\mathrm{SSE}_{F} = 310.0$.\n\nUsing a nested model comparison under the General Linear Model assumptions, compute the appropriate $F$-statistic to test whether the two-system model explains the variance better than the single-factor model. Express your final test statistic as a dimensionless number and round your answer to four significant figures. No units are required.",
            "solution": "The problem requires the computation of an $F$-statistic to compare two nested linear models within the framework of the General Linear Model. The goal is to determine if a full model, which includes a group-by-task interaction, provides a significantly better explanation of the data than a restricted model that omits this interaction. This statistical comparison directly addresses the neurobiological hypothesis of a double dissociation, where the effect of the clinical group (amnesic vs. Parkinsonian) is different for the two tasks (free recall vs. mirror tracing).\n\nThe general formula for the $F$-statistic in a nested model comparison is:\n$$ F = \\frac{(\\mathrm{SSE}_{R} - \\mathrm{SSE}_{F}) / (p_{F} - p_{R})}{\\mathrm{SSE}_{F} / (N - p_{F})} $$\nwhere:\n- $\\mathrm{SSE}_{R}$ is the sum of squared errors (residuals) for the restricted model.\n- $\\mathrm{SSE}_{F}$ is the sum of squared errors for the full model.\n- $p_{R}$ is the number of parameters in the restricted model.\n- $p_{F}$ is the number of parameters in the full model.\n- $N$ is the total number of observations.\n\nThe provided data are:\n- Total number of observations: $N = 48$.\n- Parameters in the restricted model: $p_{R} = 3$.\n- Sum of squared errors for the restricted model: $\\mathrm{SSE}_{R} = 371.8$.\n- Parameters in the full model: $p_{F} = 4$.\n- Sum of squared errors for the full model: $\\mathrm{SSE}_{F} = 310.0$.\n\nFirst, we calculate the numerator of the $F$-statistic. The numerator represents the reduction in the sum of squared errors achieved by adding the new parameter(s), normalized by the number of parameters added. This quantity is the mean square for the effect being tested (the group-by-task interaction).\n\nThe change in the sum of squared errors is:\n$$ \\Delta \\mathrm{SSE} = \\mathrm{SSE}_{R} - \\mathrm{SSE}_{F} = 371.8 - 310.0 = 61.8 $$\n\nThe change in the number of parameters (degrees of freedom for the numerator) is:\n$$ \\Delta p = p_{F} - p_{R} = 4 - 3 = 1 $$\n\nThe mean square for the effect is:\n$$ \\mathrm{MS}_{\\text{Effect}} = \\frac{\\Delta \\mathrm{SSE}}{\\Delta p} = \\frac{61.8}{1} = 61.8 $$\n\nNext, we calculate the denominator of the $F$-statistic. The denominator is the mean squared error of the full model, which serves as an estimate of the error variance ($\\sigma^2$) under the assumption that the full model is correct.\n\nThe degrees of freedom for the error in the full model (degrees of freedom for the denominator) are:\n$$ df_{\\text{Error}} = N - p_{F} = 48 - 4 = 44 $$\n\nThe mean square error for the full model is:\n$$ \\mathrm{MS}_{\\text{Error}} = \\frac{\\mathrm{SSE}_{F}}{N - p_{F}} = \\frac{310.0}{44} $$\n\nNow, we can compute the $F$-statistic by taking the ratio of the mean square for the effect to the mean square error:\n$$ F = \\frac{\\mathrm{MS}_{\\text{Effect}}}{\\mathrm{MS}_{\\text{Error}}} = \\frac{61.8}{310.0 / 44} $$\n$$ F = \\frac{61.8 \\times 44}{310.0} = \\frac{2719.2}{310.0} \\approx 8.7716129... $$\n\nThe problem requires the final answer to be rounded to four significant figures.\n$$ F \\approx 8.772 $$\nThis $F$-statistic, with $1$ and $44$ degrees of freedom, would be used to test the null hypothesis that the group-by-task interaction term is zero. A sufficiently large value would lead to the rejection of the null hypothesis, supporting the two-system model.",
            "answer": "$$\\boxed{8.772}$$"
        }
    ]
}