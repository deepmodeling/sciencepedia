{
    "hands_on_practices": [
        {
            "introduction": "感知本质上是一个推断过程。大脑必须根据嘈杂和不完整的感官输入，对外部世界的状态做出最佳猜测。这个练习将探讨一个理想贝叶斯观察者如何在不确定的情况下做出决策，通过一个基础性的双选择任务，我们将贝叶斯大脑的抽象理论与心理物理学中经典的信号检测论（Signal Detection Theory, SDT）联系起来。通过这个实践，你将推导出最优决策规则，并理解它如何与可测量的心理物理学量（如敏感度 $d'$ 和决策标准 $c$）相关联 。",
            "id": "5052174",
            "problem": "一个分层预测编码模型中的简化皮层回路执行贝叶斯推断，以在单次试验观察中识别两种刺激类别之一。在每次试验中，测量值 $y$ 是一个固定窗口内的群体放电率，潜在的刺激类别是 $x \\in \\{0,1\\}$。在生成模型下，观测值是条件高斯分布，且方差相等：$y \\mid x=i \\sim \\mathcal{N}(\\mu_{i}, \\sigma^{2})$ (对于 $i \\in \\{0,1\\}$)，先验概率相等 $p(x=0)=p(x=1)=\\frac{1}{2}$，决策成本对称。大脑被假定通过似然比 $\\Lambda(y)=\\frac{p(y \\mid x=1)}{p(y \\mid x=0)}$ 比较后验概率来执行最优贝叶斯决策规则。\n\n从贝叶斯定理和这些生成假设出发，推导最优决策规则，并证明它是一个关于 $y$ 的阈值规则。然后，将此阈值规则与标准信号检测理论 (SDT) 的量联系起来，定义 $d'$ 和 SDT 决策标准 $c$ 如下：$d'=\\frac{\\mu_{1}-\\mu_{0}}{\\sigma}$ 和 $c=\\frac{k-\\mu_{0}}{\\sigma}$，其中 $k$ 是 $y$ 在物理单位上的决策阈值。最后，对于一个在这两个类别之间的双择一强迫选择 (2AFC) 任务，其参数为 $\\mu_{0}=10$ spikes/s，$\\mu_{1}=30$ spikes/s，和 $\\sigma=8$ spikes/s，计算决策阈值 $k$（以 spikes/s 表示）、敏感性 $d'$ 和标准 $c$ 的数值。\n\n将您的数值答案四舍五入到四位有效数字。以 spikes/s 为单位表示 $k$。以有序三元组 $(k, d', c)$ 的形式提供您的最终结果。",
            "solution": "该问题要求推导用于将刺激分类为两个类别之一的最优贝叶斯决策规则，然后计算相关的信号检测理论 (SDT) 的量。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- 潜在的刺激类别是 $x \\in \\{0,1\\}$。\n- 观测值是群体放电率 $y$。\n- 观测值的生成模型是条件高斯分布：$p(y \\mid x=i) \\sim \\mathcal{N}(\\mu_{i}, \\sigma^{2})$ (对于 $i \\in \\{0,1\\}$)。\n- 两个类别的方差相等。\n- 先验概率相等：$p(x=0) = p(x=1) = \\frac{1}{2}$。\n- 决策成本对称。\n- 似然比定义为 $\\Lambda(y)=\\frac{p(y \\mid x=1)}{p(y \\mid x=0)}$。\n- SDT 敏感性定义为 $d'=\\frac{\\mu_{1}-\\mu_{0}}{\\sigma}$。\n- SDT 决策标准定义为 $c=\\frac{k-\\mu_{0}}{\\sigma}$，其中 $k$ 是 $y$ 的决策阈值。\n- 用于数值计算的参数：$\\mu_{0}=10$ spikes/s，$\\mu_{1}=30$ spikes/s，$\\sigma=8$ spikes/s。\n- 任务是双择一强迫选择 (2AFC) 任务。\n- 数值答案必须四舍五入到四位有效数字。\n- 最终答案为有序三元组 $(k, d', c)$。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题牢固地植根于贝叶斯决策理论和信号检测理论，这两者都是计算神经科学和心理物理学中用于建模感知和决策的标准和基本框架。高斯分布和等方差的假设是常见的，并构成了标准 SDT 的基础。\n- **问题定义明确：** 提供了所有必要的信息。生成模型、先验概率、成本结构（由“对称决策成本”暗示）和定义都已指定，从而导出一个唯一的最优决策规则。为最终计算提供了数值参数。\n- **客观性：** 问题以精确的数学和科学语言陈述，没有歧义或主观性。\n\n**步骤 3：结论与行动**\n该问题是有效的。它在科学上是合理的、定义明确且客观的。我将继续进行解答。\n\n### 最优决策规则的推导\n\n目标是找到一个在对称成本条件下最小化错误概率的决策规则。这通过选择在给定观测值 $y$ 的情况下具有最大后验概率 (MAP) 的刺激类别 $x$ 来实现。决策规则是：如果 $p(x=1|y) > p(x=0|y)$，则选择类别 $x=1$，否则选择类别 $x=0$。\n\n使用贝叶斯定理，每个类别的后验概率为：\n$$p(x=i|y) = \\frac{p(y|x=i)p(x=i)}{p(y)}$$\n其中 $p(y) = \\sum_{j=0}^{1} p(y|x=j)p(x=j)$ 是观测值的边际概率，也称为证据。\n\n决策规则 $p(x=1|y) > p(x=0|y)$ 可以重写为：\n$$\\frac{p(y|x=1)p(x=1)}{p(y)} > \\frac{p(y|x=0)p(x=0)}{p(y)}$$\n由于 $p(y)$ 是一个正的公因子，可以消去：\n$$p(y|x=1)p(x=1) > p(y|x=0)p(x=0)$$\n这可以用似然比 $\\Lambda(y)$ 来表示：\n$$\\frac{p(y|x=1)}{p(y|x=0)} > \\frac{p(x=0)}{p(x=1)}$$\n鉴于先验概率相等 $p(x=0) = p(x=1) = \\frac{1}{2}$，先验概率之比为 $\\frac{p(x=0)}{p(x=1)} = 1$。决策规则简化为如果 $\\Lambda(y) > 1$ 则选择 $x=1$。\n\n现在，我们代入高斯似然的表达式。给定 $x=i$ 时 $y$ 的概率密度函数是：\n$$p(y|x=i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu_i)^2}{2\\sigma^2}\\right)$$\n因此，似然比为：\n$$\\Lambda(y) = \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu_1)^2}{2\\sigma^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu_0)^2}{2\\sigma^2}\\right)} = \\exp\\left(-\\frac{(y-\\mu_1)^2}{2\\sigma^2} + \\frac{(y-\\mu_0)^2}{2\\sigma^2}\\right)$$\n决策规则是 $\\Lambda(y) > 1$。由于自然对数是单调函数，我们可以对两边取对数而不改变不等式：\n$$\\ln(\\Lambda(y)) > \\ln(1) \\implies \\ln(\\Lambda(y)) > 0$$\n$$\\frac{(y-\\mu_0)^2 - (y-\\mu_1)^2}{2\\sigma^2} > 0$$\n由于 $2\\sigma^2 > 0$，我们可以用它乘以不等式两边：\n$$(y-\\mu_0)^2 - (y-\\mu_1)^2 > 0$$\n展开平方项：\n$$(y^2 - 2y\\mu_0 + \\mu_0^2) - (y^2 - 2y\\mu_1 + \\mu_1^2) > 0$$\n$$y^2 - 2y\\mu_0 + \\mu_0^2 - y^2 + 2y\\mu_1 - \\mu_1^2 > 0$$\n$$2y\\mu_1 - 2y\\mu_0 > \\mu_1^2 - \\mu_0^2$$\n$$2y(\\mu_1 - \\mu_0) > (\\mu_1 - \\mu_0)(\\mu_1 + \\mu_0)$$\n假设 $\\mu_1 \\neq \\mu_0$，并且根据给定的数值具体为 $\\mu_1 > \\mu_0$，我们可以用正项 $2(\\mu_1 - \\mu_0)$ 除以不等式两边：\n$$y > \\frac{\\mu_1 + \\mu_0}{2}$$\n这证明了最优决策规则是关于观测值 $y$ 的阈值规则。如果 $y > k$，系统应判定刺激类别为 $x=1$；如果 $y  k$，则判定为 $x=0$，其中决策阈值 $k$ 为：\n$$k = \\frac{\\mu_0 + \\mu_1}{2}$$\n\n### SDT 量的计算\n\n我们给出的数值是：$\\mu_{0}=10$ spikes/s，$\\mu_{1}=30$ spikes/s，和 $\\sigma=8$ spikes/s。\n\n1.  **决策阈值, $k$**：\n    使用推导出的最优阈值表达式：\n    $$k = \\frac{10 + 30}{2} = \\frac{40}{2} = 20$$\n    单位是 spikes/s。四舍五入到四位有效数字，$k = 20.00$ spikes/s。\n\n2.  **敏感性, $d'$**：\n    使用问题中提供的定义：\n    $$d' = \\frac{\\mu_1 - \\mu_0}{\\sigma} = \\frac{30 - 10}{8} = \\frac{20}{8} = 2.5$$\n    这是一个无量纲的量。四舍五入到四位有效数字，$d' = 2.500$。\n\n3.  **标准, $c$**：\n    使用问题中提供的定义和我们计算出的 $k$ 值：\n    $$c = \\frac{k - \\mu_0}{\\sigma} = \\frac{20 - 10}{8} = \\frac{10}{8} = 1.25$$\n    这也是一个无量纲的量。四舍五入到四位有效数字，$c = 1.250$。\n\n在这种情况下，最优贝叶斯观察者是无偏的，将标准精确地置于两个刺激分布均值的中点。这导致标准值为 $c = d'/2$，我们的计算证实了这一点：$1.250 = 2.500/2$。\n\n最终的有序三元组 $(k, d', c)$ 是 $(20.00, 2.500, 1.250)$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n20.00  2.500  1.250\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在一个动态变化的世界中，大脑必须持续地更新其关于世界状态的信念。这个练习将带你深入预测编码框架的“引擎盖”之下，从第一性原理推导其核心数学工具——卡尔曼滤波器（Kalman filter）。通过这个推导，你将清晰地看到预测编码的核心思想是如何运作的：系统根据“精度加权的预测误差”（precision-weighted prediction error）来修正其内部模型 。这不仅是一次数学练习，更是理解大脑如何主动预测并从错误中学习的关键一步。",
            "id": "5052165",
            "problem": "在预测编码和贝叶斯大脑框架中，一个核心假设是皮层回路通过在生成模型中执行近似贝叶斯推断来编码关于感官输入潜在原因的信念。考虑线性高斯生成模型\n$$x_t = A x_{t-1} + \\omega_t,\\quad \\omega_t \\sim \\mathcal{N}(0,Q),$$\n$$y_t = C x_t + \\eta_t,\\quad \\eta_t \\sim \\mathcal{N}(0,R),$$\n其初始先验为 $p(x_0)=\\mathcal{N}(\\mu_0,P_0)$。从贝叶斯法则和多元正态分布的性质出发，且不假设任何已有的滤波公式，请从第一性原理推导单步预测分布 $p(x_t\\mid y_{1:t-1})$ 和后验分布 $p(x_t\\mid y_{1:t})$ 的闭合形式解。你的推导过程应为：(i) 通过对 $x_{t-1}$ 进行边缘化来形成预测先验；(ii) 通过将预测先验与似然相乘，并在指数项中进行配方，来执行观测更新，从而提取后验均值和协方差。解释观测更新在预测编码中如何体现为由观测精度加权的预测误差。\n\n然后，将问题特化到所有变量均为实数值的标量情况，并根据给定参数计算在时间 $t=1$ 时的后验均值\n$$A=\\frac{4}{5},\\quad C=\\frac{6}{5},\\quad Q=\\frac{1}{2},\\quad R=\\frac{3}{10},\\quad \\mu_0=1,\\quad P_0=\\frac{3}{2},\\quad y_1=2.$$\n将 $t=1$ 时后验均值的最终数值结果表示为精确分数。不要包含任何单位。",
            "solution": "问题陈述是有效的。它提出了一个在状态空间模型和贝叶斯推断背景下的标准但并不平凡的推导，这是诸如预测编码等现代计算神经科学理论的基石。该问题是自洽的、有科学依据且良定的。我们开始解答。\n\n该问题要求推导线性高斯状态空间模型的递归贝叶斯更新，这是卡尔曼滤波器的数学基础。我们将按要求从第一性原理分两步进行推导：预测步骤和更新步骤。\n\n设时间 $t-1$ 的后验分布由 $p(x_{t-1}\\mid y_{1:t-1})=\\mathcal{N}(x_{t-1};\\mu_{t-1|t-1}, P_{t-1|t-1})$ 给出。对于 $t=1$ 的初始步骤，这对应于给定的先验 $p(x_0) = \\mathcal{N}(x_0;\\mu_0, P_0)$。\n\n**第一部分：预测步骤（推导预测先验）**\n\n第一步是计算单步预测分布 $p(x_t\\mid y_{1:t-1})$，也称为时间 $t$ 的预测先验。该分布通过将联合分布 $p(x_t, x_{t-1}\\mid y_{1:t-1})$ 对前一状态 $x_{t-1}$ 进行边缘化得到：\n$$p(x_t\\mid y_{1:t-1}) = \\int p(x_t, x_{t-1}\\mid y_{1:t-1}) \\, dx_{t-1}$$\n使用概率的乘法法则，我们可以将联合分布写为：\n$$p(x_t, x_{t-1}\\mid y_{1:t-1}) = p(x_t\\mid x_{t-1}, y_{1:t-1})p(x_{t-1}\\mid y_{1:t-1})$$\n生成模型的状态方程 $x_t = A x_{t-1} + \\omega_t$ 意味着，在给定紧邻的前一个状态 $x_{t-1}$ 的条件下，$x_t$ 与过去的观测值 $y_{1:t-1}$ 条件独立。这就是马尔可夫性质。因此，$p(x_t\\mid x_{t-1}, y_{1:t-1}) = p(x_t\\mid x_{t-1})$。根据状态方程，这个分布是高斯的：\n$$p(x_t\\mid x_{t-1}) = \\mathcal{N}(x_t; A x_{t-1}, Q)$$\n我们正在对两个高斯分布进行卷积：$p(x_t\\mid x_{t-1})$ 和 $p(x_{t-1}\\mid y_{1:t-1})$。结果是另一个高斯分布。随机变量 $x_t$ 是高斯变量 $x_{t-1}$ 的线性变换加上一个独立的高斯噪声项 $\\omega_t$。$x_t$ 的均值和协方差（以 $y_{1:t-1}$ 为条件）可以使用全期望定律和全方差定律找到。\n\n预测分布的均值，我们记为 $\\mu_{t|t-1}$，是：\n$$\\mu_{t|t-1} = E[x_t\\mid y_{1:t-1}] = E[A x_{t-1} + \\omega_t \\mid y_{1:t-1}] = A E[x_{t-1}\\mid y_{1:t-1}] + E[\\omega_t] = A \\mu_{t-1|t-1} + 0$$\n所以，$\\mu_{t|t-1} = A \\mu_{t-1|t-1}$。\n\n预测分布的协方差，记为 $P_{t|t-1}$，是：\n$$P_{t|t-1} = \\text{Cov}(x_t\\mid y_{1:t-1}) = \\text{Cov}(A x_{t-1} + \\omega_t \\mid y_{1:t-1})$$\n因为 $x_{t-1}$（在给定 $y_{1:t-1}$ 的条件下）和 $\\omega_t$ 是独立的，所以它们和的协方差是它们协方差的和：\n$$P_{t|t-1} = \\text{Cov}(A x_{t-1}\\mid y_{1:t-1}) + \\text{Cov}(\\omega_t) = A \\, \\text{Cov}(x_{t-1}\\mid y_{1:t-1}) \\, A^T + Q$$\n所以，$P_{t|t-1} = A P_{t-1|t-1} A^T + Q$。\n\n因此，单步预测分布是：\n$$p(x_t\\mid y_{1:t-1}) = \\mathcal{N}(x_t; \\mu_{t|t-1}, P_{t|t-1})$$\n其均值为 $\\mu_{t|t-1} = A\\mu_{t-1|t-1}$，协方差为 $P_{t|t-1} = AP_{t-1|t-1}A^T+Q$。\n\n**第二部分：更新步骤（推导后验分布）**\n\n第二步是在观测到 $y_t$ 后更新我们关于 $x_t$ 的信念。我们使用贝叶斯法则来找到后验分布 $p(x_t\\mid y_{1:t}) = p(x_t\\mid y_t, y_{1:t-1})$：\n$$p(x_t\\mid y_{1:t}) \\propto p(y_t\\mid x_t, y_{1:t-1}) p(x_t\\mid y_{1:t-1})$$\n观测值 $y_t$ 仅依赖于当前状态 $x_t$，因此 $p(y_t\\mid x_t, y_{1:t-1}) = p(y_t\\mid x_t)$。这是似然，由观测方程给出，为 $\\mathcal{N}(y_t; C x_t, R)$。先验是第一部分中推导出的预测分布，$\\mathcal{N}(x_t; \\mu_{t|t-1}, P_{t|t-1})$。\n\n后验是两个高斯分布的乘积，这是一个未归一化的高斯分布。我们通过检查其对数来分析其形式：\n$$\\ln p(x_t\\mid y_{1:t}) = \\ln p(y_t\\mid x_t) + \\ln p(x_t\\mid y_{1:t-1}) + \\text{const.}$$\n$$= -\\frac{1}{2}(y_t - C x_t)^T R^{-1} (y_t - C x_t) - \\frac{1}{2}(x_t - \\mu_{t|t-1})^T P_{t|t-1}^{-1} (x_t - \\mu_{t|t-1}) + \\text{const.}$$\n我们展开二次型并合并包含 $x_t$ 的项，以确定后验分布的均值和协方差。一个通用的多元正态分布 $\\mathcal{N}(x; \\mu, P)$ 的指数项形式为 $-\\frac{1}{2}x^T P^{-1}x + x^T P^{-1}\\mu - \\frac{1}{2}\\mu^T P^{-1}\\mu$。\n\n展开指数项得到：\n$$\\text{exp-term} = -\\frac{1}{2}(x_t^T C^T R^{-1} C x_t - 2x_t^T C^T R^{-1} y_t) - \\frac{1}{2}(x_t^T P_{t|t-1}^{-1} x_t - 2x_t^T P_{t|t-1}^{-1} \\mu_{t|t-1}) + \\dots$$\n其中 `...` 包含不依赖于 $x_t$ 的项。\n\n$x_t$ 的二次项是 $-\\frac{1}{2}x_t^T(C^T R^{-1} C + P_{t|t-1}^{-1})x_t$。这告诉我们后验分布 $p(x_t \\mid y_{1:t})=\\mathcal{N}(x_t; \\mu_{t|t}, P_{t|t})$ 的逆协方差（精度）是：\n$$P_{t|t}^{-1} = P_{t|t-1}^{-1} + C^T R^{-1} C$$\n\n$x_t$ 的一次项是 $x_t^T(C^T R^{-1} y_t + P_{t|t-1}^{-1} \\mu_{t|t-1})$。将其与通用形式 $x_t^T P_{t|t}^{-1} \\mu_{t|t}$ 进行比较，我们发现：\n$$P_{t|t}^{-1} \\mu_{t|t} = P_{t|t-1}^{-1} \\mu_{t|t-1} + C^T R^{-1} y_t$$\n乘以 $P_{t|t}$ 得到后验均值：\n$$\\mu_{t|t} = P_{t|t} (P_{t|t-1}^{-1} \\mu_{t|t-1} + C^T R^{-1} y_t)$$\n为了得到预测编码中使用的形式，我们对这个表达式进行处理。从后验精度方程，我们有 $P_{t|t-1}^{-1} = P_{t|t}^{-1} - C^T R^{-1} C$。将其代入 $\\mu_{t|t}$ 的方程中：\n$$P_{t|t}^{-1} \\mu_{t|t} = (P_{t|t}^{-1} - C^T R^{-1} C) \\mu_{t|t-1} + C^T R^{-1} y_t$$\n$$P_{t|t}^{-1} \\mu_{t|t} = P_{t|t}^{-1} \\mu_{t|t-1} - C^T R^{-1} C \\mu_{t|t-1} + C^T R^{-1} y_t$$\n$$P_{t|t}^{-1} \\mu_{t|t} = P_{t|t}^{-1} \\mu_{t|t-1} + C^T R^{-1} (y_t - C \\mu_{t|t-1})$$\n从左侧乘以 $P_{t|t}$ 得到：\n$$\\mu_{t|t} = \\mu_{t|t-1} + P_{t|t} C^T R^{-1} (y_t - C \\mu_{t|t-1})$$\n定义卡尔曼增益为 $K_t = P_{t|t} C^T R^{-1}$，我们得到最终的更新方程：\n$$p(x_t\\mid y_{1:t}) = \\mathcal{N}(x_t; \\mu_{t|t}, P_{t|t})$$\n$$\\mu_{t|t} = \\mu_{t|t-1} + K_t(y_t - C \\mu_{t|t-1})$$\n$$P_{t|t} = (P_{t|t-1}^{-1} + C^T R^{-1} C)^{-1}$$\n这就完成了从第一性原理的推导。\n\n**在预测编码中的解释**\n\n后验均值更新方程 $\\mu_{t|t} = \\mu_{t|t-1} + K_t(y_t - C \\mu_{t|t-1})$ 在预测编码的背景下有清晰的解释。\n1.  **预测：** $\\mu_{t|t-1}$ 是在观测到 $y_t$ 之前关于状态 $x_t$ 的先验信念或预测。项 $C\\mu_{t|t-1}$ 是基于此信念预测的感官输入。\n2.  **预测误差：** 项 $\\delta_t = y_t - C \\mu_{t|t-1}$ 是预测误差，即实际感官观测值 $y_t$ 与预测观测值 $C \\mu_{t|t-1}$ 之间的差异。\n3.  **精度加权：** 卡尔曼增益 $K_t$ 对该预测误差进行加权。该增益可以被证明等价于 $K_t = P_{t|t-1} C^T (C P_{t|t-1} C^T + R)^{-1}$。它最优地平衡了先验信念的不确定性（编码在 $P_{t|t-1}$ 中）和感官数据的不确定性（编码在 $R$ 中）。高的观测噪声（大的 $R$）导致小的增益，意味着意外的观测被赋予较低的权重。高的先验不确定性（大的 $P_{t|t-1}$）导致大的增益，意味着新数据更被信任。\n因此，更新机制是根据预测误差按比例修正先验信念，比例常数（增益）由先验和似然的相对精度（逆方差）决定。这种通过精度加权的预测误差来更新信念的过程是预测编码框架的核心计算原理。\n\n**数值计算**\n\n我们被要求用以下参数计算标量后验均值 $\\mu_{1|1}$：\n$A=\\frac{4}{5}$, $C=\\frac{6}{5}$, $Q=\\frac{1}{2}$, $R=\\frac{3}{10}$, $\\mu_0=1$, $P_0=\\frac{3}{2}$, $y_1=2$.\n\n首先，我们从 $t=0$ 时的先验开始，执行 $t=1$ 的预测步骤：$\\mu_{0|0} = \\mu_0 = 1$ 且 $P_{0|0} = P_0 = \\frac{3}{2}$。\n\n预测均值 $\\mu_{1|0}$：\n$$\\mu_{1|0} = A \\mu_{0|0} = \\frac{4}{5} \\times 1 = \\frac{4}{5}$$\n\n预测方差 $P_{1|0}$：\n$$P_{1|0} = A^2 P_0 + Q = \\left(\\frac{4}{5}\\right)^2 \\left(\\frac{3}{2}\\right) + \\frac{1}{2} = \\frac{16}{25} \\times \\frac{3}{2} + \\frac{1}{2} = \\frac{24}{25} + \\frac{1}{2} = \\frac{48}{50} + \\frac{25}{50} = \\frac{73}{50}$$\n\n接下来，我们执行 $t=1$ 的更新步骤。我们需要计算卡尔曼增益 $K_1$ 和预测误差。\n\n标量情况下的卡尔曼增益 $K_1$ 是：\n$$K_1 = \\frac{P_{1|0} C}{C^2 P_{1|0} + R}$$\n分子：\n$$P_{1|0} C = \\frac{73}{50} \\times \\frac{6}{5} = \\frac{438}{250} = \\frac{219}{125}$$\n分母：\n$$C^2 P_{1|0} + R = \\left(\\frac{6}{5}\\right)^2 \\left(\\frac{73}{50}\\right) + \\frac{3}{10} = \\frac{36}{25} \\times \\frac{73}{50} + \\frac{3}{10} = \\frac{2628}{1250} + \\frac{3}{10} = \\frac{1314}{625} + \\frac{3}{10}$$\n$$= \\frac{2628}{1250} + \\frac{375}{1250} = \\frac{3003}{1250}$$\n所以，增益是：\n$$K_1 = \\frac{\\frac{219}{125}}{\\frac{3003}{1250}} = \\frac{219}{125} \\times \\frac{1250}{3003} = 219 \\times \\frac{10}{3003} = \\frac{2190}{3003}$$\n我们简化分数。$219 = 3 \\times 73$ 且 $3003 = 3 \\times 1001 = 3 \\times 7 \\times 11 \\times 13$。\n$$K_1 = \\frac{3 \\times 73 \\times 10}{3 \\times 1001} = \\frac{730}{1001}$$\n\n预测误差是：\n$$y_1 - C \\mu_{1|0} = 2 - \\left(\\frac{6}{5}\\right)\\left(\\frac{4}{5}\\right) = 2 - \\frac{24}{25} = \\frac{50}{25} - \\frac{24}{25} = \\frac{26}{25}$$\n\n最后，我们计算后验均值 $\\mu_{1|1}$：\n$$\\mu_{1|1} = \\mu_{1|0} + K_1 (y_1 - C \\mu_{1|0}) = \\frac{4}{5} + \\left(\\frac{730}{1001}\\right) \\left(\\frac{26}{25}\\right)$$\n我们简化乘积项：\n$$K_1 (y_1 - C \\mu_{1|0}) = \\frac{730}{1001} \\times \\frac{26}{25} = \\frac{73 \\times 10}{7 \\times 11 \\times 13} \\times \\frac{2 \\times 13}{25} = \\frac{73 \\times 2 \\times 5}{7 \\times 11 \\times 13} \\times \\frac{2 \\times 13}{5 \\times 5}$$\n$$= \\frac{73 \\times 2 \\times 2}{7 \\times 11 \\times 5} = \\frac{292}{385}$$\n现在我们将此修正项加到先验均值上：\n$$\\mu_{1|1} = \\frac{4}{5} + \\frac{292}{385}$$\n公分母是 $385$。$385 = 5 \\times 77$。\n$$\\mu_{1|1} = \\frac{4 \\times 77}{5 \\times 77} + \\frac{292}{385} = \\frac{308}{385} + \\frac{292}{385} = \\frac{600}{385}$$\n通过将分子和分母除以它们的最大公约数 5 来简化最终的分数：\n$$\\mu_{1|1} = \\frac{600 \\div 5}{385 \\div 5} = \\frac{120}{77}$$\n数字 $120$ 和 $77$ 互质 ($120=2^3 \\cdot 3 \\cdot 5$, $77=7 \\cdot 11$)。",
            "answer": "$$\\boxed{\\frac{120}{77}}$$"
        },
        {
            "introduction": "理论模型最终需要与神经实现联系起来，而神经系统在编码信息时面临着固有的权衡。本练习通过一个简化的单神经元模型，探讨了神经编码中一个根本性的两难困境：如何在有效传递信息（编码效率）与抵抗噪声干扰（稳健性）之间取得平衡。你将通过数学推导和计算机编程，亲手量化先验精度（prior precision，$\\Pi_p$）和似然精度（likelihood precision，$\\Pi_l$）如何共同决定这一权衡，从而对预测编码的计算原理建立起具体而深刻的直觉 。",
            "id": "5052097",
            "problem": "考虑贝叶斯大脑 (Bayesian Brain, BB) 框架下的单神经元预测编码 (Predictive Coding, PC) 模型。潜在的感觉原因 $x$ 使用高斯先验 $x \\sim \\mathcal{N}(\\mu_0, \\sigma_p^2)$ 进行建模，测量值 $y$ 由线性高斯似然 $y = x + \\epsilon$ 生成，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_l^2)$。将先验精度定义为 $\\Pi_p = 1/\\sigma_p^2$，似然精度定义为 $\\Pi_l = 1/\\sigma_l^2$，两者均为严格正数。在预测编码中，神经元通过最小化精度加权的预测误差来估计 $x$。\n\n您的任务如下：\n\n1. 从贝叶斯定理和高斯恒等式出发，为指定的线性高斯模型推导后验分布 $p(x \\mid y)$。然后，从预测编码最小化精度加权预测误差的原理出发，用 $\\Pi_p$、$\\Pi_l$、$\\mu_0$ 和 $y$ 推导出稳态时的不动点估计值 $\\hat{x}$。\n\n2. 将编码效率定义为 $x$ 和 $y$ 之间的互信息 (Mutual Information, MI)，单位为奈特 (nats)。仅用 $\\Pi_p$ 和 $\\Pi_l$ 推导出 $\\mathrm{MI}(x; y)$ 的表达式，不使用任何无关常数，且不假设 $\\mu_0$ 的任何特殊值。\n\n3. 将对感觉噪声的鲁棒性定义为估计量 $\\hat{x}$ 对加性测量噪声的敏感度，量化为将 $y = x + \\epsilon$ 代入 $\\hat{x}$ 后 $\\epsilon$ 的乘法系数。仅用 $\\Pi_p$ 和 $\\Pi_l$ 推导出此敏感度 $S$。\n\n4. 实现一个完整的、可运行的程序，为下面列出的每个测试用例计算两个量：编码效率（单位：奈特）和鲁棒性 $S$。每个计算量必须四舍五入到 $6$ 位小数。最终输出必须是单行，包含一个逗号分隔的列表的列表，其中每个内部列表为 $[\\mathrm{MI}, S]$，并严格按照指定顺序排列。\n\n使用以下精度对 $(\\Pi_p, \\Pi_l)$ 的测试套件，其选择旨在探讨平衡情况、强先验、强似然以及在保持严格正性的前提下的近简并边缘情况：\n\n- 情况 A (平衡): $(\\Pi_p, \\Pi_l) = (1, 1)$。\n- 情况 B (强先验): $(\\Pi_p, \\Pi_l) = (100, 1)$。\n- 情况 C (强似然): $(\\Pi_p, \\Pi_l) = (1, 100)$。\n- 情况 D (弱先验): $(\\Pi_p, \\Pi_l) = (10^{-6}, 1)$。\n- 情况 E (弱似然): $(\\Pi_p, \\Pi_l) = (1, 10^{-6})$。\n\n所有量均为无量纲。您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，内部列表按照上述测试套件的顺序排列（例如 $[[\\mathrm{MI}_A,S_A],[\\mathrm{MI}_B,S_B],\\dots]$）。",
            "solution": "该问题具有科学依据，提法明确且客观。它提出了计算神经科学中的一个典型问题，提供了所有必要的定义和约束以推导出唯一且有意義的解。这些任务要求应用贝叶斯统计、信息论和优化中的基本原理，这些都是预测编码和贝叶斯大脑研究的核心。\n\n**1. 后验和预测编码不动点估计的推导**\n\n首先，我们使用贝叶斯定理推导后验分布 $p(x \\mid y)$。该定理指出，后验与似然和先验的乘积成正比：$p(x \\mid y) \\propto p(y \\mid x)p(x)$。\n\n潜在原因 $x$ 的先验分布为高斯分布：\n$$p(x) = \\mathcal{N}(x; \\mu_0, \\sigma_p^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_p^2}} \\exp\\left(-\\frac{(x - \\mu_0)^2}{2\\sigma_p^2}\\right)$$\n给定 $x$ 时测量值 $y$ 的似然也是高斯分布，从模型 $y = x + \\epsilon$ 和 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_l^2)$ 推导得出：\n$$p(y \\mid x) = \\mathcal{N}(y; x, \\sigma_l^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_l^2}} \\exp\\left(-\\frac{(y - x)^2}{2\\sigma_l^2}\\right)$$\n因此，后验概率与这两个高斯函数的乘积成正比。由于两个高斯函数的乘积是另一个高斯函数，我们可以通过关注结果分布指数中的项来找到其参数，忽略常数归一化因子：\n$$\\log p(x \\mid y) \\propto -\\frac{(x - \\mu_0)^2}{2\\sigma_p^2} - \\frac{(y - x)^2}{2\\sigma_l^2}$$\n使用精度定义 $\\Pi_p = 1/\\sigma_p^2$ 和 $\\Pi_l = 1/\\sigma_l^2$，表达式变为：\n$$\\log p(x \\mid y) \\propto -\\frac{1}{2}\\left(\\Pi_p(x - \\mu_0)^2 + \\Pi_l(y - x)^2\\right)$$\n为了找到后验高斯的均值和方差，我们展开二次项并对 $x$ 进行配方：\n$$-\\frac{1}{2}\\left(\\Pi_p(x^2 - 2x\\mu_0 + \\mu_0^2) + \\Pi_l(y^2 - 2yx + x^2)\\right)$$\n$$-\\frac{1}{2}\\left( (\\Pi_p + \\Pi_l)x^2 - 2(\\Pi_p\\mu_0 + \\Pi_ly)x + \\text{const} \\right)$$\n这个关于 $x$ 的二次型对应于一个高斯分布 $\\mathcal{N}(x; \\mu_{post}, \\sigma_{post}^2)$，其对数概率具有形式 $-\\frac{(x-\\mu_{post})^2}{2\\sigma_{post}^2} = -\\frac{1}{2\\sigma_{post}^2}(x^2 - 2x\\mu_{post} + \\mu_{post}^2)$。\n通过比较 $x^2$ 项的系数，我们确定后验精度 $\\Pi_{post} = 1/\\sigma_{post}^2$：\n$$\\Pi_{post} = \\Pi_p + \\Pi_l$$\n通过比较 $x$ 项的系数，我们找到后验均值 $\\mu_{post}$：\n$$\\Pi_{post}\\mu_{post} = \\Pi_p\\mu_0 + \\Pi_ly \\implies \\mu_{post} = \\frac{\\Pi_p\\mu_0 + \\Pi_ly}{\\Pi_p + \\Pi_l}$$\n因此，后验分布为：\n$$p(x \\mid y) = \\mathcal{N}\\left(x; \\frac{\\Pi_p\\mu_0 + \\Pi_ly}{\\Pi_p + \\Pi_l}, \\frac{1}{\\Pi_p + \\Pi_l}\\right)$$\n\n接下来，我们在预测编码框架下推导不动点估计值 $\\hat{x}$。预测编码假定大脑会最小化一个成本函数，该函数对应于精度加权的预测误差平方和。对于此模型，误差是估计值 $\\hat{x}$ 与先验均值 $\\mu_0$ 的偏差，以及感觉预测值 $\\hat{x}$ 与实际感觉输入 $y$ 的偏差。成本函数 $L(\\hat{x})$ 是：\n$$L(\\hat{x}) = \\frac{1}{2}\\Pi_p(\\hat{x} - \\mu_0)^2 + \\frac{1}{2}\\Pi_l(y - \\hat{x})^2$$\n为了找到最小化此成本的最优估计值 $\\hat{x}$，我们对 $L(\\hat{x})$ 关于 $\\hat{x}$ 求导并令其为零，这定义了系统动力学的不动点：\n$$\\frac{dL}{d\\hat{x}} = \\Pi_p(\\hat{x} - \\mu_0) - \\Pi_l(y - \\hat{x}) = 0$$\n$$\\Pi_p\\hat{x} - \\Pi_p\\mu_0 - \\Pi_ly + \\Pi_l\\hat{x} = 0$$\n$$\\hat{x}(\\Pi_p + \\Pi_l) = \\Pi_p\\mu_0 + \\Pi_ly$$\n$$\\hat{x} = \\frac{\\Pi_p\\mu_0 + \\Pi_ly}{\\Pi_p + \\Pi_l}$$\n这个不动点估计值 $\\hat{x}$ 与后验均值 $\\mu_{post}$ 完全相同。这证明了在线性高斯模型中，最小化预测编码中的精度加权预测误差与执行最优贝叶斯推断是等价的。估计值 $\\hat{x}$ 是先验均值和感觉证据的精度加权平均。\n\n**2. 编码效率（互信息）的推导**\n\n编码效率定义为互信息 $\\mathrm{MI}(x; y)$。我们使用关系式 $\\mathrm{MI}(x; y) = H(x) - H(x \\mid y)$，其中 $H$ 表示微分熵。\n一维高斯变量 $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ 的熵是 $H(z) = \\frac{1}{2}\\log(2\\pi e \\sigma^2)$。\n先验分布 $p(x) \\sim \\mathcal{N}(\\mu_0, \\sigma_p^2)$ 的熵是：\n$$H(x) = \\frac{1}{2}\\log(2\\pi e \\sigma_p^2) = \\frac{1}{2}\\log\\left(\\frac{2\\pi e}{\\Pi_p}\\right)$$\n条件熵 $H(x \\mid y)$ 是后验分布 $p(x \\mid y)$ 的熵。根据我们之前的推导，后验方差为 $\\sigma_{post}^2 = 1/(\\Pi_p + \\Pi_l)$。因此，后验的熵是：\n$$H(x \\mid y) = \\frac{1}{2}\\log(2\\pi e \\sigma_{post}^2) = \\frac{1}{2}\\log\\left(\\frac{2\\pi e}{\\Pi_p + \\Pi_l}\\right)$$\n互信息是这两个熵的差值：\n$$\\mathrm{MI}(x; y) = H(x) - H(x \\mid y) = \\frac{1}{2}\\log\\left(\\frac{2\\pi e}{\\Pi_p}\\right) - \\frac{1}{2}\\log\\left(\\frac{2\\pi e}{\\Pi_p + \\Pi_l}\\right)$$\n使用对数性质 $\\log(a) - \\log(b) = \\log(a/b)$：\n$$\\mathrm{MI}(x; y) = \\frac{1}{2}\\log\\left(\\frac{2\\pi e / \\Pi_p}{2\\pi e / (\\Pi_p + \\Pi_l)}\\right) = \\frac{1}{2}\\log\\left(\\frac{\\Pi_p + \\Pi_l}{\\Pi_p}\\right)$$\n编码效率（单位：奈特）的最终表达式仅取决于精度，为：\n$$\\mathrm{MI}(x; y) = \\frac{1}{2}\\log\\left(1 + \\frac{\\Pi_l}{\\Pi_p}\\right)$$\n\n**3. 对感觉噪声的鲁棒性推导**\n\n鲁棒性定义为估计量 $\\hat{x}$ 对加性测量噪声 $\\epsilon$ 的敏感度 $S$，其中 $y = x + \\epsilon$。我们通过将 $y$ 的表达式代入 $\\hat{x}$ 的方程来找到这一点：\n$$\\hat{x} = \\frac{\\Pi_p\\mu_0 + \\Pi_l y}{\\Pi_p + \\Pi_l} = \\frac{\\Pi_p\\mu_0 + \\Pi_l (x + \\epsilon)}{\\Pi_p + \\Pi_l}$$\n我们可以分离各项以独立出噪声项 $\\epsilon$ 的贡献：\n$$\\hat{x} = \\frac{\\Pi_p\\mu_0 + \\Pi_l x}{\\Pi_p + \\Pi_l} + \\left(\\frac{\\Pi_l}{\\Pi_p + \\Pi_l}\\right)\\epsilon$$\n敏感度 $S$ 是噪声项 $\\epsilon$ 的乘法系数。因此：\n$$S = \\frac{\\Pi_l}{\\Pi_p + \\Pi_l}$$\n该值表示应用于新感觉信息的增益。相对于先验精度 $\\Pi_p$，较高的似然精度 $\\Pi_l$ 会导致较高的敏感度 $S$，这意味着估计值受（可能含噪声的）测量影响很大。相反，强先验（高 $\\Pi_p$）会降低 $S$，使估计值通过更多地依赖先验信念而对感觉噪声更具鲁棒性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes coding efficiency (MI) and robustness (S) for a single-neuron\n    Predictive Coding model based on a set of precision pairs.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple (Pi_p, Pi_l).\n    test_cases = [\n        (1.0, 1.0),            # Case A (balanced)\n        (100.0, 1.0),          # Case B (strong prior)\n        (1.0, 100.0),          # Case C (strong likelihood)\n        (1e-6, 1.0),           # Case D (weak prior)\n        (1.0, 1e-6),           # Case E (weak likelihood)\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        pi_p, pi_l = case\n        \n        # 1. Calculate Coding Efficiency (Mutual Information)\n        # MI = 0.5 * log(1 + Pi_l / Pi_p)\n        # Using np.log for the natural logarithm (for units of nats).\n        # This formula is numerically stable even for very small pi_p.\n        mutual_information = 0.5 * np.log(1 + pi_l / pi_p)\n        \n        # 2. Calculate Robustness (Sensitivity to noise)\n        # S = Pi_l / (Pi_p + Pi_l)\n        sensitivity = pi_l / (pi_p + pi_l)\n        \n        # 3. Format the results for the current case.\n        # The problem requires rounding to 6 decimal places and a specific\n        # string format for each inner list without extra spaces.\n        # e.g., \"[MI,S]\"\n        # Using f-string formatting '{:.6f}' ensures 6 decimal places.\n        formatted_result = f\"[{mutual_information:.6f},{sensitivity:.6f}]\"\n        results.append(formatted_result)\n\n    # Final print statement must produce a single line in the exact required format.\n    # The format is a comma-separated list of the formatted inner lists,\n    # all enclosed in a single pair of square brackets.\n    # e.g., [[MI_A,S_A],[MI_B,S_B],...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}