{
    "hands_on_practices": [
        {
            "introduction": "The dorsal and ventral visual streams are not only functionally distinct but may also differ in their processing speed, a crucial factor for real-time interaction with the world. This practice guides you through building a simple biophysical model to estimate and compare the total time it takes for a visual signal to reach area MT (dorsal) versus area V4 (ventral). By treating axonal conduction and synaptic transmission as sources of variable delay, you will apply principles of probability and approximation to quantify a fundamental, hypothesized difference between the two pathways .",
            "id": "5013693",
            "problem": "A brief, high-contrast visual flash evokes feedforward activity that propagates from the retina to higher visual cortical areas along partially segregated pathways. Consider two such pathways: the dorsal stream to the middle temporal area (MT) and the ventral stream to area V4. Model the total onset latency for each pathway as the sum of an axonal conduction time and a sum of synaptic delays across serial relays. Use the following foundational bases: (i) axonal conduction delay equals path length divided by conduction velocity, (ii) synaptic delays across serial relays add, and (iii) for independent stages, total latency is the sum of the component latencies.\n\nAssume the following biophysically plausible properties.\n- The effective axonal path length for MT is $L_{\\mathrm{MT}} = 0.10\\,\\mathrm{m}$ and for V4 is $L_{\\mathrm{V4}} = 0.080\\,\\mathrm{m}$.\n- The effective conduction velocity for MT is a random variable $V_{\\mathrm{MT}}$ with approximately normal distribution having mean $\\mu_{\\mathrm{MT}} = 2.5\\,\\mathrm{m/s}$ and standard deviation $\\sigma_{\\mathrm{MT}} = 0.50\\,\\mathrm{m/s}$. For V4, $V_{\\mathrm{V4}}$ is approximately normal with mean $\\mu_{\\mathrm{V4}} = 1.8\\,\\mathrm{m/s}$ and standard deviation $\\sigma_{\\mathrm{V4}} = 0.36\\,\\mathrm{m/s}$. Coefficients of variation are small enough that a second-order Taylor expansion about the mean provides an accurate approximation to expectations of smooth functions of velocity.\n- Synaptic delay at each relay is an independent normal random variable with mean $\\mu_{s} = 1.2\\,\\mathrm{ms}$ and standard deviation $\\sigma_{s} = 0.25\\,\\mathrm{ms}$. The number of serial synapses is $n_{\\mathrm{MT}} = 4$ for MT and $n_{\\mathrm{V4}} = 5$ for V4.\n- All random components are mutually independent.\n\nTasks:\n1. Starting from the above definitions and assumptions, derive an approximate expression for the expected value and variance of the total onset latency for each pathway, expressed in terms of $L$, $\\mu$, $\\sigma$, $n$, $\\mu_{s}$, and $\\sigma_{s}$. State the resulting approximate latency distribution for each pathway.\n2. Using the provided numerical parameters, compute the predicted onset time (the expected latency) for MT and for V4, and then compute the difference $\\Delta t = \\mathbb{E}[T_{\\mathrm{V4}}] - \\mathbb{E}[T_{\\mathrm{MT}}]$. Round your final numeric answer to three significant figures. Express the final result in milliseconds (ms). The final answer must be a single real-valued number.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in neurobiology, well-posed with a clear objective and all necessary parameters, and uses standard mathematical modeling and approximation techniques.\n\nThe total onset latency, $T$, for a given visual pathway is modeled as the sum of the axonal conduction latency, $T_{\\mathrm{axon}}$, and the total synaptic delay, $T_{\\mathrm{synapse}}$.\n$$\nT = T_{\\mathrm{axon}} + T_{\\mathrm{synapse}}\n$$\nThe axonal conduction latency is the ratio of the effective path length, $L$, to the effective conduction velocity, $V$.\n$$\nT_{\\mathrm{axon}} = \\frac{L}{V}\n$$\nThe total synaptic delay is the sum of delays at each of the $n$ serial relays. Each synaptic delay, $D_i$, is an independent random variable.\n$$\nT_{\\mathrm{synapse}} = \\sum_{i=1}^{n} D_i\n$$\nThe problem states that all random components are mutually independent. This implies that the velocity $V$ and all synaptic delays $D_i$ are mutually independent. Consequently, $T_{\\mathrm{axon}}$ and $T_{\\mathrm{synapse}}$ are independent random variables.\n\n### Task 1: Derivation of Latency Expectation, Variance, and Distribution\n\nFirst, we analyze the total synaptic delay, $T_{\\mathrm{synapse}}$. The synaptic delays $D_i$ are independent and identically distributed (i.i.d.) normal random variables with mean $\\mu_s$ and variance $\\sigma_s^2$, i.e., $D_i \\sim \\mathcal{N}(\\mu_s, \\sigma_s^2)$.\n\nThe expected value of the sum of random variables is the sum of their expected values:\n$$\n\\mathbb{E}[T_{\\mathrm{synapse}}] = \\mathbb{E}\\left[\\sum_{i=1}^{n} D_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[D_i] = n \\mu_s\n$$\nSince the synaptic delays are independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(T_{\\mathrm{synapse}}) = \\mathrm{Var}\\left(\\sum_{i=1}^{n} D_i\\right) = \\sum_{i=1}^{n} \\mathrm{Var}(D_i) = n \\sigma_s^2\n$$\nThe sum of independent normal random variables is itself a normal random variable. Therefore, the distribution of the total synaptic delay is:\n$$\nT_{\\mathrm{synapse}} \\sim \\mathcal{N}(n\\mu_s, n\\sigma_s^2)\n$$\n\nNext, we analyze the axonal conduction latency, $T_{\\mathrm{axon}} = L/V$. The velocity $V$ is a random variable, so $T_{\\mathrm{axon}}$ is also a random variable. We are instructed to use a second-order Taylor series expansion of the function $f(V) = L/V$ around the mean velocity, $\\mu = \\mathbb{E}[V]$, to find an approximation for its expectation.\nThe function and its first two derivatives with respect to $V$ are:\n$$\nf(V) = \\frac{L}{V}, \\quad f'(V) = -\\frac{L}{V^2}, \\quad f''(V) = \\frac{2L}{V^3}\n$$\nThe second-order Taylor expansion of $f(V)$ around $\\mu$ is:\n$$\nf(V) \\approx f(\\mu) + f'(\\mu)(V-\\mu) + \\frac{1}{2}f''(\\mu)(V-\\mu)^2\n$$\nSubstituting the derivatives evaluated at $\\mu$:\n$$\nT_{\\mathrm{axon}} = \\frac{L}{V} \\approx \\frac{L}{\\mu} - \\frac{L}{\\mu^2}(V-\\mu) + \\frac{1}{2}\\left(\\frac{2L}{\\mu^3}\\right)(V-\\mu)^2 = \\frac{L}{\\mu} - \\frac{L}{\\mu^2}(V-\\mu) + \\frac{L}{\\mu^3}(V-\\mu)^2\n$$\nTaking the expectation of this expression:\n$$\n\\mathbb{E}[T_{\\mathrm{axon}}] \\approx \\mathbb{E}\\left[\\frac{L}{\\mu} - \\frac{L}{\\mu^2}(V-\\mu) + \\frac{L}{\\mu^3}(V-\\mu)^2\\right] = \\frac{L}{\\mu} - \\frac{L}{\\mu^2}\\mathbb{E}[V-\\mu] + \\frac{L}{\\mu^3}\\mathbb{E}[(V-\\mu)^2]\n$$\nBy definition, $\\mathbb{E}[V-\\mu] = \\mathbb{E}[V] - \\mu = \\mu - \\mu = 0$, and $\\mathbb{E}[(V-\\mu)^2] = \\mathrm{Var}(V) = \\sigma^2$. Substituting these yields the approximate expected axonal latency:\n$$\n\\mathbb{E}[T_{\\mathrm{axon}}] \\approx \\frac{L}{\\mu} + \\frac{L\\sigma^2}{\\mu^3} = \\frac{L}{\\mu}\\left(1 + \\frac{\\sigma^2}{\\mu^2}\\right)\n$$\nFor the variance of the axonal latency, we use the first-order approximation (delta method), which is standard when the coefficient of variation is small:\n$$\n\\mathrm{Var}(T_{\\mathrm{axon}}) = \\mathrm{Var}(f(V)) \\approx |f'(\\mu)|^2 \\mathrm{Var}(V)\n$$\n$$\n\\mathrm{Var}(T_{\\mathrm{axon}}) \\approx \\left(-\\frac{L}{\\mu^2}\\right)^2 \\sigma^2 = \\frac{L^2 \\sigma^2}{\\mu^4}\n$$\nNow we combine the results for the total latency $T = T_{\\mathrm{axon}} + T_{\\mathrm{synapse}}$. Due to the independence of the components, the expected value and variance of the total latency are the sums of the respective parts.\n\nExpected total latency:\n$$\n\\mathbb{E}[T] = \\mathbb{E}[T_{\\mathrm{axon}}] + \\mathbb{E}[T_{\\mathrm{synapse}}] \\approx \\frac{L}{\\mu}\\left(1 + \\frac{\\sigma^2}{\\mu^2}\\right) + n \\mu_s\n$$\nVariance of total latency:\n$$\n\\mathrm{Var}(T) = \\mathrm{Var}(T_{\\mathrm{axon}}) + \\mathrm{Var}(T_{\\mathrm{synapse}}) \\approx \\frac{L^2 \\sigma^2}{\\mu^4} + n \\sigma_s^2\n$$\nThe total latency $T$ is the sum of a normally distributed variable ($T_{\\mathrm{synapse}}$) and another random variable ($T_{\\mathrm{axon}}$) which is a function of an approximately normal variable. A common and reasonable approximation in such cases is to model the total latency itself as a normal random variable.\nTherefore, the approximate latency distribution for each pathway is:\n$$\nT \\sim \\mathcal{N}\\left(\\frac{L}{\\mu}\\left(1 + \\frac{\\sigma^2}{\\mu^2}\\right) + n \\mu_s, \\frac{L^2 \\sigma^2}{\\mu^4} + n \\sigma_s^2\\right)\n$$\n\n### Task 2: Numerical Computations\n\nWe are given the following parameters, ensuring consistent units (seconds for time, meters for length).\nFor the MT pathway:\n$L_{\\mathrm{MT}} = 0.10\\,\\mathrm{m}$\n$\\mu_{\\mathrm{MT}} = 2.5\\,\\mathrm{m/s}$\n$\\sigma_{\\mathrm{MT}} = 0.50\\,\\mathrm{m/s}$\n$n_{\\mathrm{MT}} = 4$\n\nFor the V4 pathway:\n$L_{\\mathrm{V4}} = 0.080\\,\\mathrm{m}$\n$\\mu_{\\mathrm{V4}} = 1.8\\,\\mathrm{m/s}$\n$\\sigma_{\\mathrm{V4}} = 0.36\\,\\mathrm{m/s}$\n$n_{\\mathrm{V4}} = 5$\n\nFor synaptic delays (common to both pathways):\n$\\mu_s = 1.2\\,\\mathrm{ms} = 1.2 \\times 10^{-3}\\,\\mathrm{s}$\n$\\sigma_s = 0.25\\,\\mathrm{ms} = 0.25 \\times 10^{-3}\\,\\mathrm{s}$\n\nWe now compute the predicted onset time (expected latency) for each pathway using the derived formula for $\\mathbb{E}[T]$.\n\nFor the MT pathway:\n$$\n\\mathbb{E}[T_{\\mathrm{MT}}] \\approx \\frac{L_{\\mathrm{MT}}}{\\mu_{\\mathrm{MT}}}\\left(1 + \\frac{\\sigma_{\\mathrm{MT}}^2}{\\mu_{\\mathrm{MT}}^2}\\right) + n_{\\mathrm{MT}} \\mu_s\n$$\n$$\n\\mathbb{E}[T_{\\mathrm{MT}}] \\approx \\frac{0.10}{2.5}\\left(1 + \\frac{(0.50)^2}{(2.5)^2}\\right) + 4 \\times (1.2 \\times 10^{-3})\n$$\n$$\n\\mathbb{E}[T_{\\mathrm{MT}}] \\approx 0.04\\left(1 + \\left(\\frac{0.50}{2.5}\\right)^2\\right) + 0.0048 = 0.04(1 + (0.2)^2) + 0.0048\n$$\n$$\n\\mathbb{E}[T_{\\mathrm{MT}}] \\approx 0.04(1 + 0.04) + 0.0048 = 0.04(1.04) + 0.0048 = 0.0416 + 0.0048 = 0.0464\\,\\mathrm{s}\n$$\n\nFor the V4 pathway:\n$$\n\\mathbb{E}[T_{\\mathrm{V4}}] \\approx \\frac{L_{\\mathrm{V4}}}{\\mu_{\\mathrm{V4}}}\\left(1 + \\frac{\\sigma_{\\mathrm{V4}}^2}{\\mu_{\\mathrm{V4}}^2}\\right) + n_{\\mathrm{V4}} \\mu_s\n$$\n$$\n\\mathbb{E}[T_{\\mathrm{V4}}] \\approx \\frac{0.080}{1.8}\\left(1 + \\frac{(0.36)^2}{(1.8)^2}\\right) + 5 \\times (1.2 \\times 10^{-3})\n$$\n$$\n\\mathbb{E}[T_{\\mathrm{V4}}] \\approx \\frac{2}{45}\\left(1 + \\left(\\frac{0.36}{1.8}\\right)^2\\right) + 0.0060 = \\frac{2}{45}(1 + (0.2)^2) + 0.0060\n$$\n$$\n\\mathbb{E}[T_{\\mathrm{V4}}] \\approx \\frac{2}{45}(1 + 0.04) + 0.0060 = \\frac{2}{45}(1.04) + 0.0060 = \\frac{2.08}{45} + 0.0060\n$$\n$$\n\\mathbb{E}[T_{\\mathrm{V4}}] \\approx 0.046222... + 0.0060 = 0.052222...\\,\\mathrm{s}\n$$\nThe predicted onset time for MT is $\\mathbb{E}[T_{\\mathrm{MT}}] = 46.4\\,\\mathrm{ms}$, and for V4 is $\\mathbb{E}[T_{\\mathrm{V4}}] \\approx 52.2\\,\\mathrm{ms}$.\n\nFinally, we compute the difference $\\Delta t = \\mathbb{E}[T_{\\mathrm{V4}}] - \\mathbb{E}[T_{\\mathrm{MT}}]$.\n$$\n\\Delta t \\approx 0.052222... - 0.0464 = 0.0058222...\\,\\mathrm{s}\n$$\nConverting this result to milliseconds:\n$$\n\\Delta t \\approx 0.0058222...\\,\\mathrm{s} \\times \\frac{1000\\,\\mathrm{ms}}{1\\,\\mathrm{s}} \\approx 5.8222...\\,\\mathrm{ms}\n$$\nRounding to three significant figures, we get $5.82\\,\\mathrm{ms}$.",
            "answer": "$$\n\\boxed{5.82}\n$$"
        },
        {
            "introduction": "A key function of the dorsal stream is to translate sensory information into plans for action, which requires transforming coordinates from the retina's point of view to a more stable, body-centered frame of reference. This exercise explores a widely-supported neural mechanism for this process, known as a 'gain field,' where a neuron's visual response is multiplicatively modulated by eye position. You will work with a mathematical model of a neuron in the parietal cortex to understand how this interaction allows for complex computations essential for visually guided behavior .",
            "id": "5013675",
            "problem": "In the primate visual system, the dorsal visual stream transforms retinotopic sensory signals into body-centered representations needed for action. Neurons in the Lateral Intraparietal area (LIP) of posterior parietal cortex exhibit gain fields, meaning their visually driven responses are multiplicatively modulated by eye position. Consider a single LIP neuron that receives a retinal motion input $r(t)$ and an eye position signal $e(t)$, both expressed in normalized, dimensionless units so that their amplitudes are comparable. The neuron’s firing rate $f(t)$ is assumed to be well approximated by the lowest-order interaction consistent with gain-field modulation that can implement a coordinate transformation from retinal to body-centered signals.\n\nYou are given that the retinal input is $r(t) = A_{r}\\cos(\\omega t)$ and the eye position signal is $e(t) = A_{e}\\cos(\\omega t + \\phi)$, where $A_{r}$, $A_{e}$, $\\omega$, and $\\phi$ are constants. Under small-signal conditions and stationarity over one cycle, assume the neuron's firing rate can be expanded to the minimal nontrivial order that captures multiplicative modulation by eye position of visual input and that this expansion is parameterized by coefficients $\\alpha$, $\\beta$, and $\\gamma$ (with units chosen so that $f(t)$ is in spikes per second). The constants are $A_{r} = 0.6$, $A_{e} = 0.8$, $\\phi = \\frac{\\pi}{3}$, $\\alpha = 12$, $\\beta = 8$, and $\\gamma = 52$. The angular frequency $\\omega$ is nonzero but otherwise unspecified; the analysis should hold for any $\\omega$ over one full cycle.\n\nStarting from core definitions of gain fields and the requirement for a transformation from retinal coordinates to body-centered coordinates, derive the minimal bilinear firing-rate form implied by gain-field modulation and use it to compute the cycle-averaged firing rate $\\bar{f}$ over one full period $T = \\frac{2\\pi}{\\omega}$. Round your final numerical answer to four significant figures. Express your final numerical answer in spikes per second.",
            "solution": "The problem requires the calculation of the cycle-averaged firing rate of a neuron in the Lateral Intraparietal area (LIP) modeled with gain-field dynamics. The solution proceeds in two stages: first, deriving the mathematical form of the neuron's firing rate based on the provided neurobiological principles, and second, calculating the time-average of this rate over one cycle using the given input signals.\n\n**1. Derivation of the Firing Rate Model**\n\nThe problem states that the neuron's firing rate, $f(t)$, is a function of a retinal motion input, $r(t)$, and an eye position signal, $e(t)$. The core concept is that of a \"gain field,\" where the neuron's response to the visual stimulus is multiplicatively modulated by eye position. This is a mechanism for transforming sensory signals from a retinotopic (eye-centered) frame to a different, for instance body-centered, reference frame.\n\nWe are asked to find the \"lowest-order interaction consistent with gain-field modulation\" or the \"minimal non-trivial order that captures multiplicative modulation.\" This points to a Taylor series expansion of the firing rate function $f(r, e)$ in terms of its inputs $r$ and $e$, truncated to include the necessary terms. A general two-variable Taylor expansion around the point $(r=0, e=0)$ is:\n$$ f(r, e) = f(0,0) + \\frac{\\partial f}{\\partial r}r + \\frac{\\partial f}{\\partial e}e + \\frac{\\partial^2 f}{\\partial r \\partial e}re + \\text{higher-order terms} $$\nHere, all partial derivatives are evaluated at $(r=0, e=0)$.\nThe term $f(0,0)$ represents a constant baseline firing rate, $C$.\nThe term $\\frac{\\partial f}{\\partial r}r$ represents the neuron's response to the retinal stimulus alone.\nThe term $\\frac{\\partial f}{\\partial e}e$ represents the neuron's modulation by eye position alone.\nThe term $\\frac{\\partial^2 f}{\\partial r \\partial e}re$ is the crucial one. It is the lowest-order term that captures the multiplicative interaction between the retinal signal and the eye position signal. Its existence, i.e., $\\frac{\\partial^2 f}{\\partial r \\partial e} \\neq 0$, is the mathematical signature of a gain field in this simplified model.\n\nThe problem specifies that the model expansion is \"parameterized by coefficients $\\alpha$, $\\beta$, and $\\gamma$\". It is logical to associate these with the coefficients of the linear and interaction terms. Thus, we identify:\n$\\alpha = \\frac{\\partial f}{\\partial r}$ (sensitivity to retinal input)\n$\\beta = \\frac{\\partial f}{\\partial e}$ (sensitivity to eye position)\n$\\gamma = \\frac{\\partial^2 f}{\\partial r \\partial e}$ (gain-field modulation strength)\n\nThis leads to the minimal bilinear firing-rate form:\n$$ f(t) = C + \\alpha r(t) + \\beta e(t) + \\gamma r(t)e(t) $$\nThe problem does not provide a value for the baseline firing rate $C$. However, a numerical answer is required for the cycle-averaged firing rate $\\bar{f}$. The average of the term $C$ is simply $C$. The average of the terms $\\alpha r(t)$ and $\\beta e(t)$ will be zero, as they are pure sinusoids. The final average will thus be $C + \\gamma \\overline{r(t)e(t)}$. For a unique numerical solution to exist, the constant $C$ must be specified. Since it is not, we must infer the problem's intent. The phrase \"expansion is parameterized by coefficients $\\alpha, \\beta, \\text{ and } \\gamma$\" suggests that the model to be used is composed only of the terms associated with these parameters. This implies we are to analyze the dynamic, input-dependent part of the firing rate, which is equivalent to assuming a baseline rate of $C=0$. With this well-justified assumption, the model becomes:\n$$ f(t) = \\alpha r(t) + \\beta e(t) + \\gamma r(t)e(t) $$\n\n**2. Calculation of the Cycle-Averaged Firing Rate**\n\nThe cycle-averaged firing rate, $\\bar{f}$, is defined as the integral of $f(t)$ over one period $T = \\frac{2\\pi}{\\omega}$, divided by the period:\n$$ \\bar{f} = \\frac{1}{T} \\int_0^T f(t) dt = \\frac{1}{T} \\int_0^T \\left( \\alpha r(t) + \\beta e(t) + \\gamma r(t)e(t) \\right) dt $$\nBy linearity of integration, this becomes:\n$$ \\bar{f} = \\alpha \\left(\\frac{1}{T} \\int_0^T r(t) dt\\right) + \\beta \\left(\\frac{1}{T} \\int_0^T e(t) dt\\right) + \\gamma \\left(\\frac{1}{T} \\int_0^T r(t)e(t) dt\\right) $$\nThe inputs are given as $r(t) = A_r \\cos(\\omega t)$ and $e(t) = A_e \\cos(\\omega t + \\phi)$. The time average of any sinusoidal function of the form $\\cos(\\Omega t + \\delta)$ over a full period is zero. Therefore, the averages of $r(t)$ and $e(t)$ are both zero:\n$$ \\frac{1}{T} \\int_0^T r(t) dt = 0 \\quad \\text{and} \\quad \\frac{1}{T} \\int_0^T e(t) dt = 0 $$\nThe expression for $\\bar{f}$ simplifies to the average of the interaction term:\n$$ \\bar{f} = \\gamma \\left( \\frac{1}{T} \\int_0^T r(t)e(t) dt \\right) $$\nWe now compute the integral of the product $r(t)e(t)$:\n$$ \\int_0^T r(t)e(t) dt = \\int_0^T \\left( A_r \\cos(\\omega t) \\right) \\left( A_e \\cos(\\omega t + \\phi) \\right) dt = A_r A_e \\int_0^T \\cos(\\omega t) \\cos(\\omega t + \\phi) dt $$\nWe use the trigonometric product-to-sum identity $\\cos(A)\\cos(B) = \\frac{1}{2}(\\cos(A-B) + \\cos(A+B))$:\n$$ \\cos(\\omega t) \\cos(\\omega t + \\phi) = \\frac{1}{2} \\left( \\cos(\\phi) + \\cos(2\\omega t + \\phi) \\right) $$\nSubstituting this into the integral:\n$$ \\int_0^T A_r A_e \\left( \\frac{1}{2} (\\cos(\\phi) + \\cos(2\\omega t + \\phi)) \\right) dt = \\frac{A_r A_e}{2} \\left[ \\int_0^T \\cos(\\phi) dt + \\int_0^T \\cos(2\\omega t + \\phi) dt \\right] $$\nThe first integral evaluates to $T \\cos(\\phi)$, since $\\cos(\\phi)$ is a constant. The second integral is zero, as it is the integral of a sinusoid with frequency $2\\omega$ over an interval $T = \\frac{2\\pi}{\\omega}$, which is two full periods of the integrand.\n$$ \\int_0^T r(t)e(t) dt = \\frac{A_r A_e}{2} [T \\cos(\\phi) + 0] = \\frac{1}{2} A_r A_e T \\cos(\\phi) $$\nNow, we can find the average value:\n$$ \\bar{f} = \\gamma \\left( \\frac{1}{T} \\left[ \\frac{1}{2} A_r A_e T \\cos(\\phi) \\right] \\right) = \\frac{1}{2} \\gamma A_r A_e \\cos(\\phi) $$\nThis is the final analytical expression for the cycle-averaged firing rate.\n\n**3. Numerical Calculation**\n\nWe substitute the given numerical values:\n$A_r = 0.6$\n$A_e = 0.8$\n$\\phi = \\frac{\\pi}{3}$\n$\\gamma = 52$\n\nFirst, $\\cos(\\phi) = \\cos(\\frac{\\pi}{3}) = \\frac{1}{2} = 0.5$.\nNow, we compute $\\bar{f}$:\n$$ \\bar{f} = \\frac{1}{2} (52) (0.6) (0.8) (0.5) $$\n$$ \\bar{f} = (26) (0.6) (0.8) (0.5) $$\n$$ \\bar{f} = (26) (0.48) (0.5) $$\n$$ \\bar{f} = (26) (0.24) $$\n$$ \\bar{f} = 6.24 $$\nThe problem states the units are spikes per second and requires the answer to be rounded to four significant figures.\n$$ \\bar{f} = 6.240 \\text{ spikes/second} $$\nThe other coefficients, $\\alpha = 12$ and $\\beta = 8$, do not contribute to the cycle-averaged firing rate under stationary sinusoidal inputs, as their corresponding terms average to zero over a full cycle.",
            "answer": "$$\\boxed{6.240}$$"
        },
        {
            "introduction": "Why does the brain have two separate visual streams? This practice explores a powerful computational explanation: they provide distinct but complementary sources of evidence about the world, which the brain can intelligently combine to form a robust percept. Using the framework of Bayesian inference, you will model how an observer integrates a motion cue (from the dorsal stream) and a shape cue (from the ventral stream), weighting each by its reliability to arrive at an optimal estimate of heading direction . This exercise demonstrates how formal mathematical models can provide deep insights into the logic of brain design and perceptual decision-making.",
            "id": "5013729",
            "problem": "In the primate visual system, the dorsal visual stream is predominantly tuned to motion-derived information, whereas the ventral visual stream is tuned to shape-derived information. Consider a perceptual decision about a single latent heading angle $\\theta$ (in degrees) relative to straight ahead during a brief visual presentation of a moving textured object. Assume the following:\n\n- The dorsal motion cue produces a measurement $\\theta_{m}$ with a Gaussian likelihood $p(\\theta_{m} \\mid \\theta)$ that has mean $\\theta$ and variance $\\sigma_{m}^{2}$. In a particular trial, the motion measurement equals $\\theta_{m} = 10$ and the dorsal motion variance equals $\\sigma_{m}^{2} = 4$.\n- The ventral shape cue produces a measurement $\\theta_{s}$ with a Gaussian likelihood $p(\\theta_{s} \\mid \\theta)$ that has mean $\\theta$ and variance $\\sigma_{s}^{2}$. In the same trial, the shape measurement equals $\\theta_{s} = -2$ and the ventral shape variance equals $\\sigma_{s}^{2} = 16$.\n- The observer has a Gaussian prior over the heading angle $p(\\theta)$ with mean $\\mu_{p} = 0$ and variance $\\sigma_{p}^{2} = 64$.\n- Assume conditional independence of the cues given $\\theta$.\n\nUsing Bayes’ theorem and the assumptions above, derive the posterior distribution $p(\\theta \\mid \\theta_{m}, \\theta_{s})$ over the heading angle $\\theta$ and interpret the predicted behavior under cue conflict by identifying the Maximum A Posteriori (MAP) estimate. Quantify the stream-specific weightings for the dorsal motion and ventral shape cues, defined as the normalized contributions that each cue’s reliability (inverse variance) makes to the MAP estimate under the Gaussian assumptions.\n\nReport only the MAP estimate of the heading angle as your final answer, expressed in degrees. Round your final answer to four significant figures.",
            "solution": "The problem asks for the Maximum A Posteriori (MAP) estimate of a latent heading angle, $\\theta$, based on two noisy sensory cues and a prior belief. This is a problem of Bayesian inference, specifically cue integration.\n\nThe problem has been validated and is deemed valid. It is scientifically grounded in computational neuroscience, well-posed with all necessary information provided, and objective in its formulation.\n\nAccording to Bayes' theorem, the posterior probability distribution of the angle $\\theta$ given the motion measurement $\\theta_{m}$ and the shape measurement $\\theta_{s}$ is proportional to the product of the likelihood and the prior:\n$$p(\\theta \\mid \\theta_{m}, \\theta_{s}) \\propto p(\\theta_{m}, \\theta_{s} \\mid \\theta) p(\\theta)$$\nThe problem states that the two cues are conditionally independent given the true heading angle $\\theta$. Therefore, the joint likelihood can be factored into the product of the individual likelihoods:\n$$p(\\theta_{m}, \\theta_{s} \\mid \\theta) = p(\\theta_{m} \\mid \\theta) p(\\theta_{s} \\mid \\theta)$$\nSubstituting this into the first equation, we get the expression for the posterior:\n$$p(\\theta \\mid \\theta_{m}, \\theta_{s}) \\propto p(\\theta_{m} \\mid \\theta) p(\\theta_{s} \\mid \\theta) p(\\theta)$$\nWe are given that all three distributions on the right-hand side are Gaussian:\n1.  The dorsal motion cue likelihood: $p(\\theta_{m} \\mid \\theta) = \\mathcal{N}(\\theta, \\sigma_{m}^{2})$, where the measurement is $\\theta_{m} = 10$ and the variance is $\\sigma_{m}^{2} = 4$.\n2.  The ventral shape cue likelihood: $p(\\theta_{s} \\mid \\theta) = \\mathcal{N}(\\theta, \\sigma_{s}^{2})$, where the measurement is $\\theta_{s} = -2$ and the variance is $\\sigma_{s}^{2} = 16$.\n3.  The prior distribution: $p(\\theta) = \\mathcal{N}(\\mu_{p}, \\sigma_{p}^{2})$, where the mean is $\\mu_{p} = 0$ and the variance is $\\sigma_{p}^{2} = 64$.\n\nThe probability density function for a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is proportional to $\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$. Thus, the posterior distribution is:\n$$p(\\theta \\mid \\theta_{m}, \\theta_{s}) \\propto \\exp\\left(-\\frac{(\\theta_{m}-\\theta)^2}{2\\sigma_{m}^{2}}\\right) \\exp\\left(-\\frac{(\\theta_{s}-\\theta)^2}{2\\sigma_{s}^{2}}\\right) \\exp\\left(-\\frac{(\\theta-\\mu_{p})^2}{2\\sigma_{p}^{2}}\\right)$$\nCombining the exponential terms:\n$$p(\\theta \\mid \\theta_{m}, \\theta_{s}) \\propto \\exp\\left( -\\frac{(\\theta-\\theta_{m})^2}{2\\sigma_{m}^{2}} -\\frac{(\\theta-\\theta_{s})^2}{2\\sigma_{s}^{2}} -\\frac{(\\theta-\\mu_{p})^2}{2\\sigma_{p}^{2}} \\right)$$\nThe MAP estimate, $\\theta_{\\text{MAP}}$, is the value of $\\theta$ that maximizes this posterior probability. Maximizing the posterior is equivalent to minimizing the negative of its logarithm. Let $J(\\theta)$ be the term in the exponent (also known as the negative log posterior, ignoring constant terms):\n$$J(\\theta) = \\frac{(\\theta-\\theta_{m})^2}{2\\sigma_{m}^{2}} + \\frac{(\\theta-\\theta_{s})^2}{2\\sigma_{s}^{2}} + \\frac{(\\theta-\\mu_{p})^2}{2\\sigma_{p}^{2}}$$\nTo find the minimum of $J(\\theta)$, we take its derivative with respect to $\\theta$ and set it to $0$:\n$$\\frac{dJ}{d\\theta} = \\frac{2(\\theta-\\theta_{m})}{2\\sigma_{m}^{2}} + \\frac{2(\\theta-\\theta_{s})}{2\\sigma_{s}^{2}} + \\frac{2(\\theta-\\mu_{p})}{2\\sigma_{p}^{2}} = 0$$\n$$\\frac{\\theta-\\theta_{m}}{\\sigma_{m}^{2}} + \\frac{\\theta-\\theta_{s}}{\\sigma_{s}^{2}} + \\frac{\\theta-\\mu_{p}}{\\sigma_{p}^{2}} = 0$$\nWe can separate terms involving $\\theta$:\n$$\\theta\\left(\\frac{1}{\\sigma_{m}^{2}} + \\frac{1}{\\sigma_{s}^{2}} + \\frac{1}{\\sigma_{p}^{2}}\\right) = \\frac{\\theta_{m}}{\\sigma_{m}^{2}} + \\frac{\\theta_{s}}{\\sigma_{s}^{2}} + \\frac{\\mu_{p}}{\\sigma_{p}^{2}}$$\nSolving for $\\theta$ gives the MAP estimate, $\\theta_{\\text{MAP}}$:\n$$\\theta_{\\text{MAP}} = \\frac{\\frac{\\theta_{m}}{\\sigma_{m}^{2}} + \\frac{\\theta_{s}}{\\sigma_{s}^{2}} + \\frac{\\mu_{p}}{\\sigma_{p}^{2}}}{\\frac{1}{\\sigma_{m}^{2}} + \\frac{1}{\\sigma_{s}^{2}} + \\frac{1}{\\sigma_{p}^{2}}}$$\nThis equation shows that the MAP estimate is a weighted average of the measurements and the prior mean, where the weights are the reliabilities (inverse variances) of each source of information. Let $r_{i} = 1/\\sigma_{i}^{2}$ be the reliability.\n$$r_{m} = \\frac{1}{\\sigma_{m}^{2}} = \\frac{1}{4}$$\n$$r_{s} = \\frac{1}{\\sigma_{s}^{2}} = \\frac{1}{16}$$\n$$r_{p} = \\frac{1}{\\sigma_{p}^{2}} = \\frac{1}{64}$$\nThe MAP estimate equation becomes:\n$$\\theta_{\\text{MAP}} = \\frac{r_{m}\\theta_{m} + r_{s}\\theta_{s} + r_{p}\\mu_{p}}{r_{m} + r_{s} + r_{p}}$$\nSubstituting the given numerical values:\n$\\theta_{\\text{MAP}} = \\frac{(\\frac{1}{4})(10) + (\\frac{1}{16})(-2) + (\\frac{1}{64})(0)}{\\frac{1}{4} + \\frac{1}{16} + \\frac{1}{64}}$\nFirst, we calculate the numerator:\n$$\\text{Numerator} = \\frac{10}{4} - \\frac{2}{16} + 0 = 2.5 - 0.125 = 2.375$$\nIn fractional form: $\\frac{160}{64} - \\frac{8}{64} = \\frac{152}{64}$.\nNext, we calculate the denominator:\n$$\\text{Denominator} = \\frac{1}{4} + \\frac{1}{16} + \\frac{1}{64} = \\frac{16}{64} + \\frac{4}{64} + \\frac{1}{64} = \\frac{21}{64}$$\nNow, we compute the final value for $\\theta_{\\text{MAP}}$:\n$$\\theta_{\\text{MAP}} = \\frac{\\frac{152}{64}}{\\frac{21}{64}} = \\frac{152}{21}$$\nAs a decimal, this is approximately $7.2380952...$\n\nThe normalized weighting for each cue is its reliability divided by the total reliability.\nTotal reliability: $r_{\\text{total}} = r_{m} + r_{s} + r_{p} = \\frac{21}{64}$.\nWeight for dorsal motion cue: $w_{m} = \\frac{r_{m}}{r_{\\text{total}}} = \\frac{1/4}{21/64} = \\frac{16}{21} \\approx 0.762$.\nWeight for ventral shape cue: $w_{s} = \\frac{r_{s}}{r_{\\text{total}}} = \\frac{1/16}{21/64} = \\frac{4}{21} \\approx 0.190$.\nThe high reliability of the dorsal motion cue ($\\sigma_{m}^{2} = 4$) gives it a large weight, pulling the MAP estimate of $\\approx 7.24$ degrees much closer to its measurement $\\theta_{m}=10$ than to the conflicting ventral shape cue measurement $\\theta_{s}=-2$ or the prior mean $\\mu_{p}=0$.\n\nThe problem asks for the MAP estimate of the heading angle, rounded to four significant figures.\n$\\theta_{\\text{MAP}} = \\frac{152}{21} \\approx 7.2380952...$\nRounding to four significant figures gives $7.238$.",
            "answer": "$$\n\\boxed{7.238}\n$$"
        }
    ]
}