{
    "hands_on_practices": [
        {
            "introduction": "Clinical diagnosis in neurobiology is increasingly supported by quantitative models that integrate various sources of data. This practice provides a hands-on look at how this is done by using a multinomial logistic regression model, a powerful statistical tool, to classify aphasia subtypes. By inputting a hypothetical patient's scores on standardized language tests, you will calculate the posterior probability of them having receptive aphasia, gaining insight into the mechanics of modern, evidence-based diagnostics .",
            "id": "5079533",
            "problem": "Language comprehension, spontaneous fluency, repetition, and confrontation naming are standardized behavioral measures of language function closely linked to the integrity of Wernicke’s area in the posterior superior temporal cortex and its distributed network connections. Receptive aphasia (also known as Wernicke’s aphasia) is characterized by impaired comprehension with relatively fluent speech and variable deficits in repetition and naming. Consider a discriminative classifier based on multinomial logistic modeling that assigns posterior probabilities to four diagnostic categories from observed test scores: receptive aphasia (Wernicke’s aphasia), Broca’s aphasia (expressive aphasia), anomic aphasia, and neurotypical language function. The patient’s scores are $(\\text{comprehension}=30,\\ \\text{fluency}=85,\\ \\text{repetition}=40,\\ \\text{naming}=50)$ on $0$–$100$ scales. The model uses a feature vector $x$ obtained by rescaling each score to the unit interval via $x=\\left(\\frac{c}{100},\\frac{f}{100},\\frac{r}{100},\\frac{n}{100}\\right)$, where $c$, $f$, $r$, and $n$ denote comprehension, fluency, repetition, and naming respectively. The multinomial logistic model specifies a linear predictor $z_{k}=\\beta_{k,0}+\\beta_{k}^{\\top}x$ for each class $k\\in\\{\\text{Wernicke},\\ \\text{Broca},\\ \\text{Anomic},\\ \\text{Healthy}\\}$ with the following parameters:\n- Wernicke: $\\beta_{\\text{W},0}=1.0$, $\\beta_{\\text{W}}=\\left(-2.5,\\ 1.8,\\ -1.2,\\ -0.8\\right)$.\n- Broca: $\\beta_{\\text{B},0}=0.2$, $\\beta_{\\text{B}}=\\left(1.4,\\ -2.0,\\ -0.8,\\ -0.3\\right)$.\n- Anomic: $\\beta_{\\text{A},0}=0.0$, $\\beta_{\\text{A}}=\\left(0.1,\\ 0.4,\\ -0.4,\\ -2.2\\right)$.\n- Healthy: $\\beta_{\\text{H},0}=-3.5$, $\\beta_{\\text{H}}=\\left(3.0,\\ 1.0,\\ 2.0,\\ 1.0\\right)$.\n\nUsing the standard probabilistic interpretation for multinomial logistic models consistent with Bayes’ theorem and maximum-entropy assumptions, compute the posterior probability of receptive aphasia given the patient’s scores. Round your final answer to four significant figures and express it as a decimal fraction (no percent sign).",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded within the fields of clinical neuroscience and statistical modeling, well-posed with a complete and consistent set of data and parameters, and objectively stated. The task is to compute a posterior probability using a standard multinomial logistic regression model, for which all necessary inputs are provided.\n\nThe first step is to construct the feature vector $x$ from the patient's test scores. The scores are given as comprehension $c=30$, fluency $f=85$, repetition $r=40$, and naming $n=50$. These scores are on $0$–$100$ scales and are rescaled to the unit interval $[0, 1]$ according to the formula $x=\\left(\\frac{c}{100},\\frac{f}{100},\\frac{r}{100},\\frac{n}{100}\\right)$.\nSubstituting the given scores:\n$$\nx = \\left(\\frac{30}{100}, \\frac{85}{100}, \\frac{40}{100}, \\frac{50}{100}\\right) = (0.3, 0.85, 0.4, 0.5)\n$$\n\nThe multinomial logistic regression model defines a linear predictor, $z_k$, for each diagnostic class $k$. The formula for the linear predictor is $z_k = \\beta_{k,0} + \\beta_k^\\top x$, where $\\beta_{k,0}$ is the intercept and $\\beta_k$ is the vector of weights for class $k$. We calculate $z_k$ for each of the four classes: Wernicke (W), Broca (B), Anomic (A), and Healthy (H).\n\nFor Wernicke's aphasia ($k=\\text{W}$):\n$\\beta_{\\text{W},0} = 1.0$ and $\\beta_{\\text{W}} = (-2.5, 1.8, -1.2, -0.8)$.\n$$\nz_{\\text{W}} = 1.0 + (-2.5)(0.3) + (1.8)(0.85) + (-1.2)(0.4) + (-0.8)(0.5)\n$$\n$$\nz_{\\text{W}} = 1.0 - 0.75 + 1.53 - 0.48 - 0.40 = 0.90\n$$\n\nFor Broca's aphasia ($k=\\text{B}$):\n$\\beta_{\\text{B},0} = 0.2$ and $\\beta_{\\text{B}} = (1.4, -2.0, -0.8, -0.3)$.\n$$\nz_{\\text{B}} = 0.2 + (1.4)(0.3) + (-2.0)(0.85) + (-0.8)(0.4) + (-0.3)(0.5)\n$$\n$$\nz_{\\text{B}} = 0.2 + 0.42 - 1.70 - 0.32 - 0.15 = -1.55\n$$\n\nFor anomic aphasia ($k=\\text{A}$):\n$\\beta_{\\text{A},0} = 0.0$ and $\\beta_{\\text{A}} = (0.1, 0.4, -0.4, -2.2)$.\n$$\nz_{\\text{A}} = 0.0 + (0.1)(0.3) + (0.4)(0.85) + (-0.4)(0.4) + (-2.2)(0.5)\n$$\n$$\nz_{\\text{A}} = 0.03 + 0.34 - 0.16 - 1.10 = -0.89\n$$\n\nFor neurotypical language function (Healthy, $k=\\text{H}$):\n$\\beta_{\\text{H},0} = -3.5$ and $\\beta_{\\text{H}} = (3.0, 1.0, 2.0, 1.0)$.\n$$\nz_{\\text{H}} = -3.5 + (3.0)(0.3) + (1.0)(0.85) + (2.0)(0.4) + (1.0)(0.5)\n$$\n$$\nz_{\\text{H}} = -3.5 + 0.90 + 0.85 + 0.80 + 0.50 = -0.45\n$$\n\nThe posterior probability for a given class $k$, $P(y=k|x)$, is calculated using the softmax function, which normalizes the exponentiated linear predictors:\n$$\nP(y=k|x) = \\frac{\\exp(z_k)}{\\sum_{j \\in \\{\\text{W, B, A, H}\\}} \\exp(z_j)}\n$$\nWe need to compute the posterior probability of receptive aphasia, which corresponds to the Wernicke class ($k=\\text{W}$). Let's denote this probability as $P_{\\text{W}}$.\n$$\nP_{\\text{W}} = \\frac{\\exp(z_{\\text{W}})}{\\exp(z_{\\text{W}}) + \\exp(z_{\\text{B}}) + \\exp(z_{\\text{A}}) + \\exp(z_{\\text{H}})}\n$$\nSubstituting the calculated values of $z_k$:\n$$\nP_{\\text{W}} = \\frac{\\exp(0.90)}{\\exp(0.90) + \\exp(-1.55) + \\exp(-0.89) + \\exp(-0.45)}\n$$\nNow, we evaluate the exponential terms:\n$\\exp(0.90) \\approx 2.459603$\n$\\exp(-1.55) \\approx 0.212248$\n$\\exp(-0.89) \\approx 0.410656$\n$\\exp(-0.45) \\approx 0.637628$\n\nThe sum in the denominator is:\n$$\n\\sum_{j} \\exp(z_j) \\approx 2.459603 + 0.212248 + 0.410656 + 0.637628 = 3.720135\n$$\nFinally, the posterior probability for Wernicke's aphasia is:\n$$\nP_{\\text{W}} \\approx \\frac{2.459603}{3.720135} \\approx 0.66115904\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $6, 6, 1, 1$. The fifth significant digit is $5$, so we round up the fourth digit.\n$$\nP_{\\text{W}} \\approx 0.6612\n$$",
            "answer": "$$\\boxed{0.6612}$$"
        },
        {
            "introduction": "Beyond diagnosis, a key goal in neurobiology is to understand the functional role of brain regions like Wernicke's area. This exercise delves into the heart of functional Magnetic Resonance Imaging (fMRI) analysis by introducing the General Linear Model (GLM). You will first conceptualize how the brain's response to language stimuli is modeled and then apply this framework to calculate a $t$-statistic for a contrast designed to isolate lexical processing, demonstrating how we can statistically test for the specific functions of a brain area .",
            "id": "5079579",
            "problem": "In a rapid event-related functional Magnetic Resonance Imaging (fMRI) experiment aimed at probing lexical processing in the posterior Superior Temporal Gyrus (STG; Wernicke’s area), participants view visually presented words and pronounceable pseudowords. Assume the Blood Oxygenation Level Dependent (BOLD) system is approximately Linear Time-Invariant (LTI) over the task timescale, with a hemodynamic response function (HRF) that acts as the impulse response. Starting from the LTI assumption and the definition of convolution, construct a signal model in which the predicted BOLD response to words and pseudowords is formed by convolving their respective stimulus time series with a canonical HRF. Show how this leads to a General Linear Model (GLM) of the form $y = X \\beta + \\varepsilon$ for a single left posterior STG voxel, where $y$ is the sampled BOLD time series, $X$ contains at least two columns corresponding to the convolved word and pseudoword regressors, and additional nuisance columns (such as motion and low-frequency drift) may be included. Clearly state any assumptions needed to justify ordinary least squares estimation at the voxel level, and specify a contrast vector that isolates lexical processing in STG in the sense of greater response to words than pseudowords.\n\nFor a particular left posterior STG voxel, suppose ordinary least squares yields the coefficient estimates for the first two regressors (word and pseudoword) as $\\hat{\\beta}_{\\text{word}} = 0.92$ and $\\hat{\\beta}_{\\text{pseudo}} = 0.50$. Assume the sampling covariance matrix for these two coefficients (i.e., the corresponding $2 \\times 2$ principal submatrix of $\\operatorname{Var}(\\hat{\\beta})$) is\n$$\n\\begin{pmatrix}\n0.0100 & 0.0025\\\\\n0.0025 & 0.0100\n\\end{pmatrix}.\n$$\nUse your derived contrast to compute the corresponding $t$-statistic for the lexical effect in this voxel. Round your final numerical answer to four significant figures. The final answer must be a single real number with no units.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of fMRI data analysis, is well-posed with sufficient and consistent information, and is stated objectively. We can proceed with a solution.\n\nThe solution is structured in two main parts. First, we will derive the signal model based on the Linear Time-Invariant (LTI) system assumption and construct the General Linear Model (GLM). Second, we will use the provided numerical data to compute the requested $t$-statistic.\n\n**Part 1: The General Linear Model for fMRI**\n\nThe core assumption is that the system mapping neural activity to the observed Blood Oxygenation Level Dependent (BOLD) signal is approximately Linear and Time-Invariant (LTI). In an LTI system, the output signal is the convolution of the input signal with the system's impulse response.\n\nLet $s(t)$ represent the neural activity over time, which acts as the input to the hemodynamic system. In event-related fMRI, this input is modeled as a series of impulses (Dirac delta functions) at the onset times of the stimuli. Let $h(t)$ be the hemodynamic response function (HRF), which is the BOLD response to a single, infinitesimally brief impulse of neural activity. The predicted BOLD signal, $y_{\\text{predicted}}(t)$, is then the convolution of $s(t)$ and $h(t)$, denoted by $(s * h)(t)$:\n$$y_{\\text{predicted}}(t) = (s * h)(t) = \\int_{-\\infty}^{\\infty} s(\\tau) h(t - \\tau) d\\tau$$\n\nIn this experiment, there are two distinct stimulus types: words and pseudowords. Let their respective stimulus time series be $s_{\\text{word}}(t)$ and $s_{\\text{pseudo}}(t)$. Each function is a train of delta functions representing the onsets of word or pseudoword stimuli. Due to the linearity property of the system, the total response is the sum of the responses to each stimulus type, scaled by their respective response amplitudes, $\\beta_{\\text{word}}$ and $\\beta_{\\text{pseudo}}$:\n$$y_{\\text{predicted}}(t) = \\beta_{\\text{word}} (s_{\\text{word}} * h)(t) + \\beta_{\\text{pseudo}} (s_{\\text{pseudo}} * h)(t)$$\n\nfMRI measures the BOLD signal at discrete time points, $t_i$, for $i = 1, 2, \\dots, N$, where $N$ is the total number of scans. The measured BOLD signal in a voxel, $y(t_i)$, is modeled as the sum of the predicted signal, contributions from nuisance sources (e.g., head motion, low-frequency drift), and random error, $\\varepsilon(t_i)$. The complete model is:\n$$y(t_i) = \\beta_{\\text{word}} (s_{\\text{word}} * h)(t_i) + \\beta_{\\text{pseudo}} (s_{\\text{pseudo}} * h)(t_i) + \\sum_{j=1}^{M} \\beta_j g_j(t_i) + \\varepsilon(t_i)$$\nwhere $g_j(t_i)$ are the values of the $M$ nuisance regressors at time $t_i$, with corresponding coefficients $\\beta_j$.\n\nThis equation can be expressed in matrix form, which defines the General Linear Model (GLM). Let $\\mathbf{y}$ be an $N \\times 1$ column vector of the observed BOLD time series, where $y_i = y(t_i)$. Let $\\mathbf{x}_{\\text{word}}$ be an $N \\times 1$ column vector where the $i$-th element is $(s_{\\text{word}} * h)(t_i)$. This is the \"word regressor\". Similarly, let $\\mathbf{x}_{\\text{pseudo}}$ be the \"pseudoword regressor\". Let $\\mathbf{x}_j$ be the column vectors for the nuisance regressors. The design matrix, $\\mathbf{X}$, is formed by concatenating these column vectors:\n$$\\mathbf{X} = \\begin{pmatrix} \\mathbf{x}_{\\text{word}} & \\mathbf{x}_{\\text{pseudo}} & \\mathbf{x}_1 & \\cdots & \\mathbf{x}_M \\end{pmatrix}$$\nLet $\\boldsymbol{\\beta}$ be the column vector of parameters to be estimated:\n$$\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_{\\text{word}} & \\beta_{\\text{pseudo}} & \\beta_1 & \\cdots & \\beta_M \\end{pmatrix}^T$$\nLet $\\boldsymbol{\\varepsilon}$ be the $N \\times 1$ column vector of errors. The GLM is then expressed as:\n$$\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$$\n\nFor ordinary least squares (OLS) estimation of $\\boldsymbol{\\beta}$ to provide the Best Linear Unbiased Estimator (BLUE), the Gauss-Markov assumptions regarding the error term $\\boldsymbol{\\varepsilon}$ must hold:\n1.  **Zero Mean**: The expectation of the errors is zero, $E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$.\n2.  **Homoscedasticity and No Autocorrelation**: The errors are uncorrelated and have constant variance. This is expressed as the covariance matrix of the errors being a scalar multiple of the identity matrix: $\\operatorname{Var}(\\boldsymbol{\\varepsilon}) = E[\\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon}^T] = \\sigma^2 \\mathbf{I}$, where $\\mathbf{I}$ is the $N \\times N$ identity matrix and $\\sigma^2$ is the error variance.\nFor statistical inference (such as the $t$-test below), an additional assumption is that the errors are normally distributed, i.e., $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$.\n\nThe research question is to isolate lexical processing, specified as a greater response to words than to pseudowords. This translates to the statistical hypothesis $H_1: \\beta_{\\text{word}} > \\beta_{\\text{pseudo}}$, or equivalently, $\\beta_{\\text{word}} - \\beta_{\\text{pseudo}} > 0$. This is a linear contrast on the model parameters. This contrast can be written as $\\mathbf{c}^T \\boldsymbol{\\beta}$, where $\\mathbf{c}$ is a contrast vector. Given the ordering of parameters in $\\boldsymbol{\\beta}$ as $(\\beta_{\\text{word}}, \\beta_{\\text{pseudo}}, \\dots)$, the contrast vector to test this hypothesis is:\n$$\\mathbf{c} = \\begin{pmatrix} 1 & -1 & 0 & \\cdots & 0 \\end{pmatrix}^T$$\n\n**Part 2: Calculation of the $t$-statistic**\n\nThe $t$-statistic for a general linear contrast $\\mathbf{c}^T \\boldsymbol{\\beta}$ is given by:\n$$t = \\frac{\\mathbf{c}^T \\hat{\\boldsymbol{\\beta}}}{\\text{SE}(\\mathbf{c}^T \\hat{\\boldsymbol{\\beta}})} = \\frac{\\mathbf{c}^T \\hat{\\boldsymbol{\\beta}}}{\\sqrt{\\mathbf{c}^T \\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) \\mathbf{c}}}$$\nwhere $\\hat{\\boldsymbol{\\beta}}$ is the OLS estimate of $\\boldsymbol{\\beta}$, and $\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}})$ is its sampling covariance matrix.\n\nWe are given the following information for a specific voxel:\nThe coefficient estimates for the first two regressors:\n$$\\hat{\\beta}_{\\text{word}} = 0.92$$\n$$\\hat{\\beta}_{\\text{pseudo}} = 0.50$$\nLet $\\hat{\\boldsymbol{\\beta}}_{\\text{sub}} = \\begin{pmatrix} 0.92 \\\\ 0.50 \\end{pmatrix}$.\n\nThe corresponding $2 \\times 2$ sampling covariance submatrix is:\n$$\\mathbf{C}_{\\text{sub}} = \\operatorname{Var} \\left( \\begin{pmatrix} \\hat{\\beta}_{\\text{word}} \\\\ \\hat{\\beta}_{\\text{pseudo}} \\end{pmatrix} \\right) = \\begin{pmatrix} 0.0100 & 0.0025 \\\\ 0.0025 & 0.0100 \\end{pmatrix}$$\nThe relevant part of the contrast vector is $\\mathbf{c}_{\\text{sub}} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nFirst, we compute the value of the contrast using the estimated coefficients (the numerator of the $t$-statistic):\n$$\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 0.92 \\\\ 0.50 \\end{pmatrix} = (1)(0.92) + (-1)(0.50) = 0.92 - 0.50 = 0.42$$\n\nNext, we compute the variance of the contrast estimate (the term under the square root in the denominator):\n$$\\text{Var}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = \\mathbf{c}_{\\text{sub}}^T \\mathbf{C}_{\\text{sub}} \\mathbf{c}_{\\text{sub}}$$\n$$\\text{Var}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 0.0100 & 0.0025 \\\\ 0.0025 & 0.0100 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\n$$\\text{Var}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = \\begin{pmatrix} 1 \\times 0.0100 - 1 \\times 0.0025 & 1 \\times 0.0025 - 1 \\times 0.0100 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\n$$\\text{Var}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = \\begin{pmatrix} 0.0075 & -0.0075 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\n$$\\text{Var}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = (0.0075)(1) + (-0.0075)(-1) = 0.0075 + 0.0075 = 0.0150$$\n\nAlternatively, the variance can be computed as:\n$$\\text{Var}(\\hat{\\beta}_{\\text{word}} - \\hat{\\beta}_{\\text{pseudo}}) = \\text{Var}(\\hat{\\beta}_{\\text{word}}) + \\text{Var}(\\hat{\\beta}_{\\text{pseudo}}) - 2 \\text{Cov}(\\hat{\\beta}_{\\text{word}}, \\hat{\\beta}_{\\text{pseudo}})$$\n$$= 0.0100 + 0.0100 - 2(0.0025) = 0.0200 - 0.0050 = 0.0150$$\n\nThe standard error (SE) of the contrast is the square root of this variance:\n$$\\text{SE}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = \\sqrt{0.0150}$$\n\nFinally, we compute the $t$-statistic:\n$$t = \\frac{0.42}{\\sqrt{0.0150}} \\approx \\frac{0.42}{0.122474487} \\approx 3.429281$$\n\nRounding the result to four significant figures, we get $3.429$.",
            "answer": "$$\\boxed{3.429}$$"
        },
        {
            "introduction": "To understand the impact of damage to Wernicke's area, it is crucial to compare the neural activity of patients with that of neurotypical individuals. This practice explores this comparative approach using data from Electroencephalography (EEG) and a key neural signature of semantic processing, the N400 event-related potential. You will work with summary statistics from a hypothetical study to compute an effect size (Cohen's $d$) and perform a $t$-test, providing a complete walkthrough of how researchers quantify and establish the significance of neural differences between clinical and control groups .",
            "id": "5079632",
            "problem": "A cohort study investigates semantic processing in individuals with chronic receptive aphasia due to lesions centered on Wernicke's area (posterior superior temporal cortex) compared with neurotypical controls using Electroencephalography (EEG) event-related potentials (ERP). The N400 component, which indexes lexical-semantic integration to sentence context, is measured as mean amplitude in a centro-parietal region of interest over the time window from $300$ to $500$ milliseconds. Each participant reads sentences with either congruent or incongruent final words. For each participant, define the N400 effect as the within-subject difference in mean amplitude, computed as incongruent minus congruent (more negative values indicate a larger N400 effect).\n\nGroup-level descriptive statistics are as follows:\n\n- Control group: sample size $n_C = 26$; mean N400 amplitude for congruent sentences $-3.0$ microvolts with standard deviation $2.0$ microvolts; mean N400 amplitude for incongruent sentences $-6.0$ microvolts with standard deviation $2.5$ microvolts; within-subject Pearson correlation between congruent and incongruent amplitudes $r_C = 0.60$.\n\n- Receptive aphasia group (lesions involving Wernicke's area): sample size $n_A = 20$; mean N400 amplitude for congruent sentences $-2.0$ microvolts with standard deviation $2.2$ microvolts; mean N400 amplitude for incongruent sentences $-3.0$ microvolts with standard deviation $2.3$ microvolts; within-subject Pearson correlation between congruent and incongruent amplitudes $r_A = 0.50$.\n\nUsing these data and starting from first principles of variance for linear combinations of correlated measurements and the definition of standardized mean differences, do the following:\n\n1. Derive the group mean N400 effect for each group.\n2. Derive the standard deviation of the within-subject N400 effect for each group from the provided per-condition standard deviations and within-subject correlations.\n3. Compute the between-group Cohen's $d$ for the difference in N400 effect between the receptive aphasia and control groups using a pooled standard deviation of the effect scores, assuming equal variances.\n4. Test whether the between-group difference in the N400 effect is statistically significant at $\\alpha = 0.050$ (two-tailed) using an independent-samples $t$-test on the effect scores under the equal-variance assumption, and state your conclusion.\n\nReport Cohen's $d$ as your final answer, rounded to three significant figures. Express no units for Cohen's $d$ in the final answer.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in established principles of neurophysiology (Event-Related Potentials, specifically the N400 component) and neuropsychology (receptive aphasia and Wernicke's area). It is well-posed, providing all necessary statistical data ($n$, mean, standard deviation, and correlation for two conditions in two groups) to perform the requested calculations. The problem is objective, complete, and its components are internally consistent. The data provided are realistic for this type of research. We may therefore proceed with the solution.\n\nThe problem requires a four-part analysis: $1$) calculation of the mean N400 effect for each group, $2$) calculation of the standard deviation of this effect for each group, $3$) calculation of the between-group Cohen's $d$ for the effect, and $4$) a statistical significance test of the between-group difference.\n\nLet $X_{cong}$ and $X_{incong}$ be the random variables representing the measured mean EEG amplitude for congruent and incongruent sentences, respectively. The N400 effect, which we denote as $E$, is defined as the within-subject difference:\n$$E = X_{incong} - X_{cong}$$\n\nThe problem provides the following sample statistics for the Control (C) and Aphasia (A) groups:\n\nControl Group:\n- Sample size: $n_C = 26$\n- Mean (congruent): $\\bar{x}_{C,cong} = -3.0$\n- Standard deviation (congruent): $s_{C,cong} = 2.0$\n- Mean (incongruent): $\\bar{x}_{C,incong} = -6.0$\n- Standard deviation (incongruent): $s_{C,incong} = 2.5$\n- Correlation: $r_C = 0.60$\n\nAphasia Group:\n- Sample size: $n_A = 20$\n- Mean (congruent): $\\bar{x}_{A,cong} = -2.0$\n- Standard deviation (congruent): $s_{A,cong} = 2.2$\n- Mean (incongruent): $\\bar{x}_{A,incong} = -3.0$\n- Standard deviation (incongruent): $s_{A,incong} = 2.3$\n- Correlation: $r_A = 0.50$\n\n**1. Derive the group mean N400 effect for each group.**\n\nThe mean of a difference of two random variables is the difference of their respective means. Thus, the mean N400 effect, $\\bar{x}_E$, is calculated as:\n$$\\bar{x}_E = \\bar{x}_{incong} - \\bar{x}_{cong}$$\n\nFor the Control group (C):\n$$\\bar{x}_{E,C} = \\bar{x}_{C,incong} - \\bar{x}_{C,cong} = -6.0 - (-3.0) = -3.0$$\n\nFor the Aphasia group (A):\n$$\\bar{x}_{E,A} = \\bar{x}_{A,incong} - \\bar{x}_{A,cong} = -3.0 - (-2.0) = -1.0$$\n\n**2. Derive the standard deviation of the within-subject N400 effect for each group.**\n\nThe variance of a difference of two correlated random variables, $X$ and $Y$, is given by the formula:\n$$\\text{Var}(X - Y) = \\text{Var}(X) + \\text{Var}(Y) - 2 \\cdot \\text{Cov}(X, Y)$$\nThe covariance, $\\text{Cov}(X, Y)$, can be expressed in terms of the Pearson correlation coefficient, $r$, and the standard deviations, $s_X$ and $s_Y$, as $\\text{Cov}(X, Y) = r \\cdot s_X \\cdot s_Y$. Substituting this into the variance formula gives the variance of the effect scores, $s_E^2$:\n$$s_E^2 = s_{incong}^2 + s_{cong}^2 - 2 \\cdot r \\cdot s_{incong} \\cdot s_{cong}$$\nThe standard deviation, $s_E$, is the square root of the variance.\n\nFor the Control group (C):\n$$s_{E,C}^2 = s_{C,incong}^2 + s_{C,cong}^2 - 2 \\cdot r_C \\cdot s_{C,incong} \\cdot s_{C,cong}$$\n$$s_{E,C}^2 = (2.5)^2 + (2.0)^2 - 2 \\cdot (0.60) \\cdot (2.5) \\cdot (2.0)$$\n$$s_{E,C}^2 = 6.25 + 4.00 - 6.00 = 4.25$$\nThe standard deviation of the effect for the Control group is:\n$$s_{E,C} = \\sqrt{4.25}$$\n\nFor the Aphasia group (A):\n$$s_{E,A}^2 = s_{A,incong}^2 + s_{A,cong}^2 - 2 \\cdot r_A \\cdot s_{A,incong} \\cdot s_{A,cong}$$\n$$s_{E,A}^2 = (2.3)^2 + (2.2)^2 - 2 \\cdot (0.50) \\cdot (2.3) \\cdot (2.2)$$\n$$s_{E,A}^2 = 5.29 + 4.84 - 5.06 = 5.07$$\nThe standard deviation of the effect for the Aphasia group is:\n$$s_{E,A} = \\sqrt{5.07}$$\n\n**3. Compute the between-group Cohen's $d$ for the difference in N400 effect.**\n\nCohen's $d$ is a standardized mean difference, calculated as the difference between two means divided by a pooled standard deviation. Here, the means are the mean N400 effects of the two groups, $\\bar{x}_{E,A}$ and $\\bar{x}_{E,C}$. The difference of interest is $\\bar{x}_{E,A} - \\bar{x}_{E,C}$.\n\nThe pooled standard deviation, $s_p$, is calculated using the variances of the effect scores ($s_{E,A}^2$ and $s_{E,C}^2$) under the assumption of equal variances:\n$$s_p = \\sqrt{\\frac{(n_A - 1)s_{E,A}^2 + (n_C - 1)s_{E,C}^2}{n_A + n_C - 2}}$$\nSubstituting the known values:\n$$s_p^2 = \\frac{(20 - 1)(5.07) + (26 - 1)(4.25)}{20 + 26 - 2}$$\n$$s_p^2 = \\frac{19 \\cdot (5.07) + 25 \\cdot (4.25)}{44}$$\n$$s_p^2 = \\frac{96.33 + 106.25}{44} = \\frac{202.58}{44} \\approx 4.60409$$\nSo, the pooled standard deviation is:\n$$s_p = \\sqrt{\\frac{202.58}{44}} \\approx 2.14571$$\n\nNow, we compute Cohen's $d$:\n$$d = \\frac{\\bar{x}_{E,A} - \\bar{x}_{E,C}}{s_p}$$\n$$d = \\frac{-1.0 - (-3.0)}{2.14571} = \\frac{2.0}{2.14571} \\approx 0.93209$$\nRounding to three significant figures, Cohen's $d$ is $0.932$.\n\n**4. Test the statistical significance of the between-group difference.**\n\nWe use an independent-samples $t$-test on the effect scores, assuming equal variances. The test statistic, $t$, is calculated as:\n$$t = \\frac{\\bar{x}_{E,A} - \\bar{x}_{E,C}}{s_p \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_C}}}$$\nThe denominator is the standard error of the difference between the means.\n$$SE_{diff} = s_p \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_C}} = 2.14571 \\sqrt{\\frac{1}{20} + \\frac{1}{26}} = 2.14571 \\sqrt{0.050 + 0.03846...} = 2.14571 \\sqrt{0.08846...} \\approx 0.63814$$\nThe $t$-statistic is:\n$$t = \\frac{2.0}{0.63814} \\approx 3.134$$\n\nThe degrees of freedom ($df$) for this test are:\n$$df = n_A + n_C - 2 = 20 + 26 - 2 = 44$$\nFor a two-tailed test with a significance level of $\\alpha = 0.050$ and $df = 44$, the critical $t$-value, $t_{crit}$, is found from a $t$-distribution table or calculator. $t_{crit(44, 0.05/2)} \\approx 2.015$.\n\nSince the absolute value of our calculated $t$-statistic, $|3.134|$, is greater than the critical value, $2.015$, we reject the null hypothesis.\n\nThe conclusion is that there is a statistically significant difference in the N400 effect between the aphasia group and the control group ($t(44) \\approx 3.134$, $p < 0.050$). Specifically, the N400 effect (a more negative amplitude for incongruent vs. congruent words) is significantly smaller in magnitude for the individuals with receptive aphasia compared to the neurotypical controls.\n\nThe problem asks for Cohen's $d$ as the final answer.\n$$d \\approx 0.93209 \\approx 0.932$$",
            "answer": "$$\\boxed{0.932}$$"
        }
    ]
}