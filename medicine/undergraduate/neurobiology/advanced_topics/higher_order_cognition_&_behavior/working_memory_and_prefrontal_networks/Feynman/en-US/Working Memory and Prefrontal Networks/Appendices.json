{
    "hands_on_practices": [
        {
            "introduction": "A central debate in cognitive neuroscience concerns the nature of working memory capacity. Is it a system of discrete, all-or-none \"slots,\" or a continuous, divisible \"resource\"? This exercise places you in the role of a neuroscientist tasked with interpreting experimental data. By analyzing converging behavioral and neural evidence from a hypothetical experiment, you will learn to distinguish the signatures of these two competing models and connect them to plausible circuit mechanisms in the prefrontal cortex, such as attractor networks and divisive normalization .",
            "id": "5080027",
            "problem": "A laboratory studies Working Memory (WM) for simple visual features using a delayed-estimation task. On each trial, participants view $N$ colored oriented bars, maintain them across a delay, and then reproduce the orientation of a probed item. Simultaneous recordings are obtained from dorsolateral prefrontal cortex (dlPFC) in nonhuman primates performing an analogous task. Two distinct experimental contexts, X and Y, are designed to selectively engage different circuit regimes (e.g., via neuromodulatory state and task demands). The following observations are made:\n\nBehavioral observations:\n- Context X: As $N$ increases from $N=1$ to $N=6$, the distribution of recall errors appears to be a mixture: a narrow, stimulus-centered component of approximately constant width for stored items, plus an increasingly prominent uniform-like component for trials where memory appears absent. The fraction of trials with near-random responses remains near $0$ for $N \\le 3$ and then increases sharply for $N \\ge 4$. For the non-random responses, precision shows little change from $N=1$ to $N=4$.\n- Context Y: As $N$ increases from $N=1$ to $N=6$, recall precision degrades smoothly without an obvious uniform-like component; the standard deviation of errors increases approximately proportionally to $\\sqrt{N}$, and response distributions remain unimodal around the target across all $N$.\n\nNeural observations from dlPFC during the delay:\n- Context X: Item-selective neurons show persistent activity bumps that are spatially separated in preferred feature space. The number of simultaneously active item-specific ensembles saturates around $K \\approx 3$–$4$ as $N$ increases, with each maintained bump retaining similar peak firing rate and tuning width as when $N=1$. The total gamma-band bursts per delay epoch plateau as $N$ exceeds $3$, and blood-oxygen-level-dependent signals in lateral prefrontal cortex (PFC) similarly plateau with $N$.\n- Context Y: Item-selective neurons exhibit reduced peak firing rates and broadened tuning as $N$ increases; total spiking across the local population remains approximately conserved across $N$. Noise correlations between neurons tuned to different items increase with $N$. No saturation is seen in the count of active ensembles; instead, activity per item scales down gradually.\n\nUse the following foundational bases to reason about the underlying WM architecture:\n- Definitions of WM capacity frameworks: “slot” models posit a finite number $K$ of item representations that are either stored or not; “resource” models posit a continuously divisible representational resource per item that diminishes with increasing $N$.\n- Well-tested properties of cortical population codes: estimation precision is bounded by Fisher information $J$, and for approximately independent, Poisson-like spiking neurons with divisive normalization, per-item gain and thus $J$ typically decrease as load increases, leading to continuous precision loss with $N$.\n- Well-established network motifs in PFC: recurrent excitation with lateral inhibition can generate discrete attractor “bumps” whose number is limited by global inhibition; divisive normalization distributes fixed total activity across multiple stimuli, reducing per-item gain as $N$ increases.\n\nWhich option best identifies the model-context mapping and specifies plausible neural implementations consistent with each framework?\n\nA. Context X reflects a “slot” model implemented by a discrete multi-bump attractor network in PFC with a finite $K$ constrained by lateral inhibition; Context Y reflects a “resource” model implemented by a population code under divisive normalization, yielding per-item Fisher information that decreases with $N$ and smoothly reduced precision.\n\nB. Context X reflects a “resource” model implemented by divisive normalization; Context Y reflects a “slot” model implemented by a discrete attractor network with finite $K$.\n\nC. Both contexts X and Y reflect “slot” models, but Context X has stronger lateral inhibition than Context Y; the continuous drop in precision in Y arises from slots with variable fidelity rather than resource division.\n\nD. Both contexts X and Y reflect “resource” models; the apparent random-guess mixture in X arises purely from accumulating Poisson noise without discrete failures of storage, and the saturation of active ensembles is an artifact of measurement rather than a network limit.",
            "solution": "The problem statement is evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Task**: Delayed-estimation for $N$ colored oriented bars. Recordings from dorsolateral prefrontal cortex (dlPFC).\n- **Context X - Behavior**:\n    - As set size $N$ increases from $N=1$ to $N=6$.\n    - Recall error distribution is a mixture of a narrow, stimulus-centered component (constant width) and a uniform-like component (memory absent).\n    - Fraction of random responses is near $0$ for $N \\le 3$, increases sharply for $N \\ge 4$.\n    - For non-random responses, precision shows little change from $N=1$ to $N=4$.\n- **Context X - Neural (dlPFC)**:\n    - Item-selective neurons show persistent, spatially separated activity bumps.\n    - Number of simultaneously active item-specific ensembles saturates around $K \\approx 3$–$4$.\n    - Maintained bumps retain similar peak firing rate and tuning width as for $N=1$.\n    - Total gamma-band bursts and BOLD signals in lateral PFC plateau as $N$ exceeds $3$.\n- **Context Y - Behavior**:\n    - As set size $N$ increases from $N=1$ to $N=6$.\n    - Recall precision degrades smoothly.\n    - No obvious uniform-like component; distributions remain unimodal.\n    - Standard deviation of errors increases approximately proportionally to $\\sqrt{N}$.\n- **Context Y - Neural (dlPFC)**:\n    - Item-selective neurons exhibit reduced peak firing rates and broadened tuning as $N$ increases.\n    - Total spiking across the local population remains approximately conserved across $N$.\n    - Noise correlations between neurons tuned to different items increase with $N$.\n    - No saturation in the count of active ensembles; activity per item scales down gradually.\n- **Foundational Bases**:\n    - **“Slot” model**: Finite number $K$ of all-or-none item representations.\n    - **“Resource” model**: Continuously divisible representational resource; precision per item diminishes with increasing $N$.\n    - **Population codes**: Precision is bounded by Fisher information, $J$. With divisive normalization, per-item gain and $J$ decrease with load $N$, causing continuous precision loss.\n    - **PFC motifs**: Recurrent excitation and lateral inhibition can create a limited number of discrete attractor bumps. Divisive normalization distributes fixed total activity, reducing per-item gain as $N$ increases.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective.\n- **Scientific Groundedness**: The problem describes a canonical working memory paradigm and presents behavioral and neural findings that are highly representative of data published in leading neuroscience journals. The theoretical frameworks (\"slot\" vs. \"resource\" models) and neural mechanisms (attractor networks, divisive normalization, population coding, Fisher information) are standard, well-established concepts in computational and systems neuroscience. The problem is a valid and realistic synthesis of experimental observations and theoretical models.\n- **Well-Posedness**: The problem provides two distinct sets of converging behavioral and neural data (Context X and Context Y) and asks for the best mapping to a set of competing theoretical models and their neural implementations. The information is sufficient and internally consistent within each context, allowing for a logical deduction of the best-fitting explanation.\n- **Objectivity**: The language is technical and descriptive, free of subjective claims. The data are described with qualitative and quantitative precision (e.g., \"constant width\", \"saturates around $K \\approx 3$–$4$\", \"proportionally to $\\sqrt{N}$\").\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation and Option Analysis\n\nThe task is to map the observations in Context X and Context Y to the appropriate cognitive models (\"slot\" vs. \"resource\") and their plausible neural implementations.\n\n**Analysis of Context X:**\n- **Behavioral Evidence**: The key observation is the mixture distribution of recall errors, comprising a high-precision component and a uniform (random guess) component. The fraction of random guesses is negligible for small set sizes ($N \\le 3$) and then increases sharply for larger set sizes ($N \\ge 4$). This pattern signifies a catastrophic failure of memory for items exceeding a certain capacity, which is the defining characteristic of a **\"slot\" model**. In this framework, items are either successfully stored in one of a finite number of slots (here, $K \\approx 3$–$4$) and recalled with high precision, or they are not stored at all, leading to a random guess. The observation that precision for remembered items shows little change for $N=1$ to $N=4$ further supports this, as items that occupy a slot are maintained with a fixed quality.\n- **Neural Evidence**: The neural data provide a direct substrate for a slot model. The presence of discrete, persistent activity \"bumps\" is the hallmark of an attractor network, where recurrent excitation maintains a representation and lateral inhibition keeps representations separate. The saturation of the number of these bumps at $K \\approx 3$–$4$ provides a clear neural basis for the behavioral capacity limit. The fact that the properties of each bump (peak firing rate, tuning width) are preserved as long as the capacity is not exceeded explains the constant precision of remembered items. The plateau of gamma-band activity and BOLD signals further reinforces the idea of a saturating network resource. This mechanism, based on recurrent excitation and lateral inhibition limiting the number of stable states, is a canonical implementation of a discrete, finite-capacity system.\n- **Conclusion for Context X**: The data strongly support a **\"slot\" model** implemented by a **discrete multi-bump attractor network** with a capacity limit $K$ imposed by network dynamics like global inhibition.\n\n**Analysis of Context Y:**\n- **Behavioral Evidence**: The key observation is the smooth, continuous degradation of recall precision as set size $N$ increases. There is no evidence of a catastrophic failure or a random-guess component. This graceful degradation is the defining characteristic of a **\"resource\" model**, where a finite representational resource (e.g., total neural firing rate) is divided among all items being remembered. As $N$ increases, the share of the resource per item decreases, resulting in a less precise representation. The error distribution remains unimodal, consistent with a representation that is always present but becomes noisier. The specific relationship that the standard deviation of errors $\\sigma$ increases approximately as $\\sqrt{N}$ has theoretical significance. Since precision is often defined as $1/\\sigma^2$, this implies precision $\\propto 1/N$. This is precisely what a simple resource model predicts if a total resource is divided equally among $N$ items.\n- **Neural Evidence**: The neural data align perfectly with a resource model implemented by divisive normalization. The reduction in peak firing rates and broadening of tuning curves with increasing $N$ are direct neural correlates of a lower-quality, less precise representation for each item. The conservation of total spiking activity across the population is the signature of divisive normalization, a mechanism that allocates a fixed \"budget\" of activity. As more items ($N$) are represented, the activity (gain) for each must decrease. The foundational principles state that precision is bounded by Fisher information, $J$, which for such population codes is proportional to the gain. Therefore, as $N$ increases, per-item gain decreases, $J$ decreases, and precision degrades. This mechanism naturally produces the smooth decline in performance seen behaviorally. The lack of saturation in the number of active ensembles further distinguishes it from the discrete attractor model.\n- **Conclusion for Context Y**: The data strongly support a **\"resource\" model** implemented by a **population code operating under divisive normalization**, where a fixed total activity is distributed among items, leading to a continuous decline in per-item precision.\n\n**Evaluation of Options:**\n\n- **A. Context X reflects a “slot” model implemented by a discrete multi-bump attractor network in PFC with a finite $K$ constrained by lateral inhibition; Context Y reflects a “resource” model implemented by a population code under divisive normalization, yielding per-item Fisher information that decreases with $N$ and smoothly reduced precision.**\nThis option presents a perfect match to the analysis above. It correctly identifies Context X with a \"slot\" model and its corresponding attractor network implementation. It also correctly identifies Context Y with a \"resource\" model and its corresponding implementation via divisive normalization in a population code. The descriptions of the mechanisms and their consequences are accurate.\n**Verdict: Correct.**\n\n- **B. Context X reflects a “resource” model implemented by divisive normalization; Context Y reflects a “slot” model implemented by a discrete attractor network with finite $K$.**\nThis option swaps the correct mappings. Context X's data (sharp capacity limit, mixture model errors) are inconsistent with a resource model. Context Y's data (smooth degradation, unimodal errors, conserved total activity) are inconsistent with a slot model.\n**Verdict: Incorrect.**\n\n- **C. Both contexts X and Y reflect “slot” models, but Context X has stronger lateral inhibition than Context Y; the continuous drop in precision in Y arises from slots with variable fidelity rather than resource division.**\nThis option fails to account for the neural data in Context Y. The observations of conserved total activity, gradually decreasing firing rates per item, and a lack of bump saturation are signature features of divisive normalization (a resource mechanism), not an attractor-based slot mechanism, even one with variable fidelity. A resource model provides a more direct and parsimonious explanation for the entirety of the data in Context Y.\n**Verdict: Incorrect.**\n\n- **D. Both contexts X and Y reflect “resource” models; the apparent random-guess mixture in X arises purely from accumulating Poisson noise without discrete failures of storage, and the saturation of active ensembles is an artifact of measurement rather than a network limit.**\nThis option attempts to explain the \"slot-like\" data of Context X using a resource model. It dismisses crucial evidence—the saturation of neural ensembles, gamma bursts, and BOLD signals across multiple measurement modalities—as a \"measurement artifact.\" This is not a scientifically rigorous explanation; it ignores strong, converging evidence for a physical network limit. The sharp behavioral threshold at $N \\approx 4$ is also not well explained by a gradual accumulation of noise.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The \"resource model\" proposes that representational quality degrades gracefully as memory load increases. But what does this \"degradation\" look like at the level of a single neuron? This practice moves from a qualitative concept to a precise mathematical formulation . You will derive how the division of a finite neural resource propagates as increased trial-to-trial variability in a neuron's firing rate, providing a concrete link between cognitive theory, population coding principles, and measurable neural statistics.",
            "id": "5079997",
            "problem": "Consider a population-coding account of working memory in the prefrontal cortex (PFC), in which a single pyramidal neuron is tuned to a scalar feature $\\theta$ (for example, motion direction) that must be maintained in an $n$-back task with set size $N$ (number of distinct items concurrently maintained). Assume the following widely used bases:\n\n- Resource models of working memory posit that the per-item precision $J(N)$ of the mnemonic representation declines inversely with set size, so that $J(N) \\propto \\frac{1}{N}$. Equivalently, the variance of the mnemonic estimation error is the inverse of precision.\n- Trial-to-trial variability of PFC single-neuron firing around its mean is well described by a conditionally Gaussian distribution when averaging over short analysis windows.\n- Smooth neuronal tuning permits local linear approximation over small perturbations of the encoded feature.\n\nLet the neuron's mean firing rate given a mnemonic representation $\\theta_{\\mathrm{m}}$ be $f(\\theta_{\\mathrm{m}})$, where $f$ is differentiable at the true feature $\\theta$. Model the mnemonic error as $\\theta_{\\mathrm{m}} = \\theta + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}\\!\\left(0, \\frac{N}{J_{0}}\\right)$, where $J_{0} > 0$ is a constant setting the total available precision-resource magnitude. Conditional on $\\theta_{\\mathrm{m}}$, let the observed firing rate $r$ be distributed as $r \\mid \\theta_{\\mathrm{m}} \\sim \\mathcal{N}\\!\\left(f(\\theta_{\\mathrm{m}}), \\sigma_{r}^{2}\\right)$, where $\\sigma_{r}^{2} > 0$ is an intrinsic variability parameter that does not depend on $N$.\n\nUnder the assumption that the mnemonic error magnitude is small enough that a first-order approximation of $f$ around $\\theta$ is valid, derive and provide, as a single closed-form analytic expression, the probability density function of the marginal distribution $p(r \\mid \\theta, N)$ of firing rates across trials at fixed $\\theta$ and set size $N$, expressed in terms of $r$, $N$, $J_{0}$, $\\sigma_{r}^{2}$, $f(\\theta)$, and $f'(\\theta)$. Do not introduce any new parameters. Express your final answer as the explicit density function in terms of $r$ and $N$. No numerical approximation or rounding is required.",
            "solution": "The user wants to solve the following problem.\n\n### PROBLEM VALIDATION\n\n#### Step 1: Extract Givens\n- **Model**: Population-coding account of working memory in the prefrontal cortex (PFC).\n- **Target Variable**: A scalar feature $\\theta$.\n- **Task Parameters**: Set size $N$.\n- **Precision Model**: Per-item precision $J(N) \\propto \\frac{1}{N}$.\n- **Mnemonic Error**: A mnemonic representation $\\theta_{\\mathrm{m}}$ of the true feature $\\theta$ is modeled as $\\theta_{\\mathrm{m}} = \\theta + \\varepsilon$.\n- **Error Distribution**: The mnemonic error $\\varepsilon$ follows a normal distribution, $\\varepsilon \\sim \\mathcal{N}\\!\\left(0, \\frac{N}{J_{0}}\\right)$, where $J_0 > 0$ is a constant.\n- **Neuronal Tuning**: The mean firing rate is a function $f(\\theta_{\\mathrm{m}})$ of the mnemonic representation, and $f$ is differentiable at $\\theta$.\n- **Firing Rate Variability**: Conditional on $\\theta_{\\mathrm{m}}$, the observed firing rate $r$ is distributed as $r \\mid \\theta_{\\mathrm{m}} \\sim \\mathcal{N}\\!\\left(f(\\theta_{\\mathrm{m}}), \\sigma_{r}^{2}\\right)$, where $\\sigma_{r}^{2} > 0$ is a constant.\n- **Approximation**: A first-order approximation of $f$ around $\\theta$ is valid for the given range of mnemonic errors.\n- **Objective**: Derive the probability density function (PDF) of the marginal distribution $p(r \\mid \\theta, N)$ as a single closed-form expression in terms of $r$, $N$, $J_{0}$, $\\sigma_{r}^{2}$, $f(\\theta)$, and $f'(\\theta)$.\n\n#### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is well-grounded in computational neuroscience. The concepts of population coding, resource models of working memory ($J \\propto 1/N$), Gaussian noise for neural firing and memory, and local linear approximations of tuning curves are standard and established tools in the field.\n- **Well-Posedness**: The problem is well-posed. It requires the derivation of a marginal probability distribution from a hierarchical model where all component distributions and their relationships are explicitly defined. This structure guarantees a unique and meaningful solution.\n- **Objectivity**: The problem is stated in precise, objective mathematical language, free from any subjective claims.\n- **Consistency and Completeness**: The provided information is self-contained and consistent. All necessary parameters and distributional forms are specified for the derivation. No contradictions are present.\n- **Feasibility**: The assumptions, including the linear approximation, are standard for simplifying complex neural models and are physically and mathematically plausible within the specified context.\n\n#### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is a standard, well-posed problem in theoretical neurobiology that can be solved using principles of probability theory.\n\n### SOLUTION\n\nThe objective is to find the marginal probability density function $p(r \\mid \\theta, N)$. This is obtained by integrating the joint density $p(r, \\theta_{\\mathrm{m}} \\mid \\theta, N)$ over all possible values of the latent mnemonic representation $\\theta_{\\mathrm{m}}$:\n$$p(r \\mid \\theta, N) = \\int_{-\\infty}^{\\infty} p(r, \\theta_{\\mathrm{m}} \\mid \\theta, N) \\, d\\theta_{\\mathrm{m}}$$\nUsing the chain rule of probability, we can write the joint density as $p(r, \\theta_{\\mathrmm} \\mid \\theta, N) = p(r \\mid \\theta_{\\mathrmm}, \\theta, N) p(\\theta_{\\mathrmm} \\mid \\theta, N)$. The problem's structure implies that the firing rate $r$ depends directly on $\\theta_{\\mathrmm}$, not independently on $\\theta$ or $N$ when $\\theta_{\\mathrmm}$ is given. Thus, $p(r \\mid \\theta_{\\mathrmm}, \\theta, N) = p(r \\mid \\theta_{\\mathrmm})$. The integral becomes:\n$$p(r \\mid \\theta, N) = \\int_{-\\infty}^{\\infty} p(r \\mid \\theta_{\\mathrm{m}}) p(\\theta_{\\mathrm{m}} \\mid \\theta, N) \\, d\\theta_{\\mathrmm}$$\nThis is an expression for the convolution of the two distributions.\n\nWe are given the distributions:\n$1$. The distribution of the mnemonic representation $\\theta_{\\mathrmm}$. Since $\\theta_{\\mathrmm} = \\theta + \\varepsilon$ and $\\varepsilon \\sim \\mathcal{N}\\!\\left(0, \\frac{N}{J_0}\\right)$, it follows that $\\theta_{\\mathrmm}$ is a Gaussian random variable with mean $\\theta$ and variance $\\frac{N}{J_0}$:\n$$\\theta_{\\mathrmm} \\sim \\mathcal{N}\\!\\left(\\theta, \\frac{N}{J_0}\\right)$$\n$2$. The conditional distribution of the firing rate $r$ given $\\theta_{\\mathrmm}$:\n$$r \\mid \\theta_{\\mathrmm} \\sim \\mathcal{N}\\!\\left(f(\\theta_{\\mathrmm}), \\sigma_r^2\\right)$$\nThe problem states that a first-order Taylor approximation of $f(\\theta_{\\mathrmm})$ around the true feature value $\\theta$ is valid. We write:\n$$f(\\theta_{\\mathrmm}) \\approx f(\\theta) + (\\theta_{\\mathrmm} - \\theta) f'(\\theta)$$\nSince $\\varepsilon = \\theta_{\\mathrmm} - \\theta$, this is equivalent to:\n$$f(\\theta_{\\mathrmm}) \\approx f(\\theta) + \\varepsilon f'(\\theta)$$\nSubstituting this linear approximation into the conditional distribution for $r$, the mean of $r$ becomes approximately $f(\\theta) + \\varepsilon f'(\\theta)$.\n\nThis structure defines a hierarchical Gaussian model. The observed firing rate $r$ can be expressed as the sum of its approximate mean and a zero-mean Gaussian noise term, let's call it $\\delta_r \\sim \\mathcal{N}(0, \\sigma_r^2)$:\n$$r \\approx \\left( f(\\theta) + \\varepsilon f'(\\theta) \\right) + \\delta_r$$\nRearranging the equation to isolate the random components:\n$$r - f(\\theta) \\approx \\varepsilon f'(\\theta) + \\delta_r$$\nThe term on the right, $\\varepsilon f'(\\theta) + \\delta_r$, is the sum of two independent Gaussian random variables. Let's find the distribution of each.\nThe variable $\\varepsilon f'(\\theta)$ is a scaled Gaussian. Given $\\varepsilon \\sim \\mathcal{N}\\!\\left(0, \\frac{N}{J_0}\\right)$, the distribution of $\\varepsilon f'(\\theta)$ is:\n$$\\varepsilon f'(\\theta) \\sim \\mathcal{N}\\!\\left( f'(\\theta) \\cdot 0, (f'(\\theta))^2 \\cdot \\frac{N}{J_0} \\right) = \\mathcal{N}\\!\\left(0, \\frac{N (f'(\\theta))^2}{J_0} \\right)$$\nThe second variable is the intrinsic noise, $\\delta_r \\sim \\mathcal{N}\\!\\left(0, \\sigma_r^2\\right)$.\n\nThe sum of two independent Gaussian random variables is also a Gaussian random variable. The mean of the sum is the sum of the means, and the variance of the sum is the sum of the variances. Therefore, the distribution of $r - f(\\theta)$ is:\n$$r - f(\\theta) \\sim \\mathcal{N}\\!\\left(0+0, \\frac{N (f'(\\theta))^2}{J_0} + \\sigma_r^2 \\right)$$\nThe variable $r$ is simply a shifted version of $r-f(\\theta)$. Adding the constant $f(\\theta)$ shifts the mean by $f(\\theta)$ but does not change the variance. Thus, the marginal distribution of $r$ is:\n$$r \\mid \\theta, N \\sim \\mathcal{N}\\!\\left(f(\\theta), \\sigma_r^2 + \\frac{N (f'(\\theta))^2}{J_0}\\right)$$\nThis result is also obtainable by applying the law of total variance. The mean is $E[r] = E[E[r|\\theta_{\\mathrmm}]] \\approx E[f(\\theta) + \\varepsilon f'(\\theta)] = f(\\theta)$. The variance is $\\text{Var}(r) = E[\\text{Var}(r|\\theta_{\\mathrmm})] + \\text{Var}(E[r|\\theta_{\\mathrmm}]) = \\sigma_r^2 + \\text{Var}(f(\\theta_{\\mathrmm})) \\approx \\sigma_r^2 + \\text{Var}(f(\\theta) + \\varepsilon f'(\\theta)) = \\sigma_r^2 + (f'(\\theta))^2 \\text{Var}(\\varepsilon) = \\sigma_r^2 + \\frac{N(f'(\\theta))^2}{J_0}$.\n\nThe probability density function (PDF) for a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is given by $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)$.\nFor the distribution of $r$, we have the mean $\\mu = f(\\theta)$ and the variance $\\sigma^2 = \\sigma_r^2 + \\frac{N(f'(\\theta))^2}{J_0}$.\nSubstituting these quantities into the general formula for the PDF, we get the final expression for $p(r \\mid \\theta, N)$:\n$$p(r \\mid \\theta, N) = \\frac{1}{\\sqrt{2\\pi \\left(\\sigma_r^2 + \\frac{N (f'(\\theta))^2}{J_0}\\right)}} \\exp\\left(-\\frac{(r - f(\\theta))^2}{2 \\left(\\sigma_r^2 + \\frac{N (f'(\\theta))^2}{J_0}\\right)}\\right)$$\nThis expression represents the marginal probability of observing a firing rate $r$ for a neuron tuned to stimulus $\\theta$, when $N$ items are held in working memory. The total variance of the firing rate is the sum of the intrinsic neuronal variance $\\sigma_r^2$ and a term that reflects the propagation of mnemonic uncertainty through the neuron's tuning curve, which increases with set size $N$.",
            "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{2\\pi \\left(\\sigma_{r}^{2} + \\frac{N (f'(\\theta))^{2}}{J_{0}}\\right)}} \\exp\\left(-\\frac{(r - f(\\theta))^{2}}{2 \\left(\\sigma_{r}^{2} + \\frac{N (f'(\\theta))^{2}}{J_{0}}\\right)}\\right)}\n$$"
        },
        {
            "introduction": "In contrast to resource models, \"slot\" models are often implemented by attractor networks that maintain stable, persistent activity. This hands-on problem drills down to the biophysical level to ask what makes this persistent firing possible within a single neuron . By balancing synaptic currents and accounting for noise, you will calculate the critical ratio of NMDA-to-AMPA receptor conductance required to create a self-sustaining memory trace, revealing how specific molecular properties of synapses enable network-level cognitive functions.",
            "id": "5080017",
            "problem": "Consider a single-compartment excitatory pyramidal neuron in the primate prefrontal cortex that participates in a working memory microcircuit. Its membrane dynamics are modeled by a conductance-based equation with leak and recurrent excitatory synapses mediated by α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) receptors and N-methyl-D-aspartate (NMDA) receptors. Assume the following well-tested bases:\n- The membrane equation states that the net inward current determines the firing rate; near a chosen operating point, the frequency-current (f-I) curve is approximately linear: $r = \\beta \\left(I_{\\text{net}} - I_{0}\\right)$, where $r$ is firing rate, $I_{\\text{net}}$ is the mean net inward current (inward excitatory plus external minus outward leak), $I_{0}$ is the rheobase offset (the effective current at which firing begins), and $\\beta$ is the local slope of the f-I curve.\n- For a Poisson spike train at rate $r$ driving an exponential synapse of peak conductance increment $q$ and decay time constant $\\tau$, the mean conductance is $r\\,q\\,\\tau$. With reversal potential $E_{\\text{exc}}$ and membrane voltage $V_{m}$ held near a working point, the mean inward excitatory current is $I_{\\text{exc}} = \\bar{g}_{\\text{exc}} \\left(E_{\\text{exc}} - V_{m}\\right)$, with $\\bar{g}_{\\text{exc}}$ the mean excitatory conductance.\n- Leak current is outward and given by $I_{\\text{leak}} = g_{L} \\left(V_{m} - E_{L}\\right)$.\n\nYou will compute the minimal NMDA-to-AMPA peak conductance ratio needed for the recurrent excitation to sustain a target delay firing rate under specified leak and noise. Assume a fixed total recurrent peak conductance budget per presynaptic spike, $q_{\\text{tot}}$, that is partitioned between AMPA and NMDA such that $q_{\\text{A}} + q_{\\text{N}} = q_{\\text{tot}}$. The operating point is $V_{m} = -55$ millivolts, with excitatory reversal potential $E_{\\text{exc}} = 0$ millivolts and leak reversal $E_{L} = -70$ millivolts. The leak conductance is $g_{L} = 5$ nanosiemens. The synaptic kinetics are $\\tau_{\\text{A}} = 2$ milliseconds and $\\tau_{\\text{N}} = 100$ milliseconds. The total recurrent peak conductance budget is $q_{\\text{tot}} = 3$ nanosiemens. The target delay firing rate is $r_{\\text{target}} = 20$ hertz. The f-I parameters near this operating point are $\\beta = 0.2$ hertz per picoampere and $I_{0} = 50$ picoamperes. There is a tonic external inward current $I_{\\text{ext}} = 50$ picoamperes. Background synaptic and channel noise can be modeled as zero-mean Gaussian current with standard deviation $\\sigma_{I} = 15$ picoamperes; to ensure robust persistence, require a safety margin equal to $k = 2$ standard deviations added to the mean inward current requirement.\n\nUnder these assumptions, use the conductance-based framework and the linearized f-I relation to derive and calculate the minimal ratio $q_{\\text{N}}/q_{\\text{A}}$ such that the recurrent excitatory input (AMPA plus NMDA), together with $I_{\\text{ext}}$, overcomes leak and achieves $r_{\\text{target}}$ with the specified noise margin. Treat inward current as positive and outward current as negative in summations. Express your final ratio as a dimensionless number. Round your final answer to three significant figures.",
            "solution": "The problem is to determine the minimal ratio of NMDA to AMPA peak conductance, $q_{\\text{N}}/q_{\\text{A}}$, required for a single-compartment neuron model to sustain a target firing rate with a specified noise margin.\n\nFirst, we establish the core relationship governing the neuron's firing rate. The problem states that the firing rate $r$ is a linear function of the mean net inward current $I_{\\text{net}}$:\n$$r = \\beta \\left(I_{\\text{net}} - I_{0}\\right)$$\nTo achieve a target firing rate $r_{\\text{target}}$, a specific mean net current is required. Rearranging the equation gives:\n$$I_{\\text{net, req}} = I_{0} + \\frac{r_{\\text{target}}}{\\beta}$$\n\nThe problem also introduces a robustness requirement against noise. This is specified as a safety margin of $k$ standard deviations ($\\sigma_I$) \"added to the mean inward current requirement.\" This implies that the net current available from the neuron's biophysical properties must meet not just the basic requirement for firing at $r_{\\text{target}}$, but this requirement augmented by the safety margin. We interpret this as a condition on the steady-state current balance:\n$$I_{\\text{net, avail}} = I_{\\text{net, req}} + k\\sigma_{I}$$\nTherefore, the balance equation that must be satisfied at the operating point ($r = r_{\\text{target}}$) is:\n$$I_{\\text{net, avail}} = I_{0} + \\frac{r_{\\text{target}}}{\\beta} + k\\sigma_{I}$$\n\nNext, we define the available mean net current, $I_{\\text{net, avail}}$. The problem defines this as the sum of inward excitatory and external currents, minus the outward leak current. Using the sign convention that inward currents are positive:\n$$I_{\\text{net, avail}} = I_{\\text{exc}} + I_{\\text{ext}} - I_{\\text{leak}}$$\nHere, $I_{\\text{exc}}$ is the recurrent excitatory current, $I_{\\text{ext}}$ is the external tonic current, and $I_{\\text{leak}}$ is the outward leak current.\n\nWe now substitute the expressions for each current component.\nThe leak current is given by:\n$$I_{\\text{leak}} = g_{L} \\left(V_{m} - E_{L}\\right)$$\nThe total recurrent excitatory current $I_{\\text{exc}}$ is the sum of AMPA and NMDA components. It depends on the total mean excitatory conductance $\\bar{g}_{\\text{exc}}$ and the driving force:\n$$I_{\\text{exc}} = \\bar{g}_{\\text{exc}} \\left(E_{\\text{exc}} - V_{m}\\right)$$\nThe mean conductance $\\bar{g}_{\\text{exc}}$ is the sum of mean AMPA ($\\bar{g}_{\\text{A}}$) and NMDA ($\\bar{g}_{\\text{N}}$) conductances. For a firing rate $r$, these are:\n$$\\bar{g}_{\\text{A}} = r q_{\\text{A}} \\tau_{\\text{A}}$$\n$$\\bar{g}_{\\text{N}} = r q_{\\text{N}} \\tau_{\\text{N}}$$\nAt the target firing rate $r_{\\text{target}}$, the total mean excitatory conductance is:\n$$\\bar{g}_{\\text{exc}} = r_{\\text{target}} \\left(q_{\\text{A}} \\tau_{\\text{A}} + q_{\\text{N}} \\tau_{\\text{N}}\\right)$$\nSo, the excitatory current is:\n$$I_{\\text{exc}} = r_{\\text{target}} \\left(q_{\\text{A}} \\tau_{\\text{A}} + q_{\\text{N}} \\tau_{\\text{N}}\\right) \\left(E_{\\text{exc}} - V_{m}\\right)$$\n\nCombining these expressions, our primary balance equation becomes:\n$$r_{\\text{target}} \\left(q_{\\text{A}} \\tau_{\\text{A}} + q_{\\text{N}} \\tau_{\\text{N}}\\right) \\left(E_{\\text{exc}} - V_{m}\\right) + I_{\\text{ext}} - g_{L} \\left(V_{m} - E_{L}\\right) = I_{0} + \\frac{r_{\\text{target}}}{\\beta} + k\\sigma_{I}$$\nWe can isolate the required recurrent excitatory current, $I_{\\text{exc}}$:\n$$I_{\\text{exc}} = \\left(I_{0} + \\frac{r_{\\text{target}}}{\\beta}\\right) + g_{L} \\left(V_{m} - E_{L}\\right) - I_{\\text{ext}} + k\\sigma_{I}$$\n\nLet's compute the numerical value of the required $I_{\\text{exc}}$ using the provided parameters. All currents are expressed in picoamperes ($pA$).\n- $I_0 = 50 \\, \\text{pA}$\n- $r_{\\text{target}} = 20 \\, \\text{Hz}$\n- $\\beta = 0.2 \\, \\text{Hz/pA} \\implies \\frac{r_{\\text{target}}}{\\beta} = \\frac{20}{0.2} = 100 \\, \\text{pA}$\n- $g_{L} = 5 \\, \\text{nS} = 5 \\times 10^{-9} \\, \\text{S}$\n- $V_{m} = -55 \\, \\text{mV} = -55 \\times 10^{-3} \\, \\text{V}$\n- $E_{L} = -70 \\, \\text{mV} = -70 \\times 10^{-3} \\, \\text{V}$\n- $I_{\\text{leak}} = (5 \\times 10^{-9} \\, \\text{S}) \\left( (-55 - (-70)) \\times 10^{-3} \\, \\text{V} \\right) = (5 \\times 10^{-9}) (15 \\times 10^{-3}) \\, \\text{A} = 75 \\times 10^{-12} \\, \\text{A} = 75 \\, \\text{pA}$\n- $I_{\\text{ext}} = 50 \\, \\text{pA}$\n- $k = 2$\n- $\\sigma_I = 15 \\, \\text{pA} \\implies k\\sigma_I = 30 \\, \\text{pA}$\n\nSubstituting these values:\n$$I_{\\text{exc}} = (50 + 100) + 75 - 50 + 30 = 205 \\, \\text{pA}$$\n\nNow, we relate this required current to the synaptic parameters. Let the ratio be $R = q_{\\text{N}}/q_{\\text{A}}$. The conductance budget is $q_{\\text{A}} + q_{\\text{N}} = q_{\\text{tot}}$.\nFrom these two relations, we express $q_{\\text{A}}$ and $q_{\\text{N}}$ in terms of $R$ and $q_{\\text{tot}}$:\n$$q_{\\text{A}} \\left(1 + R\\right) = q_{\\text{tot}} \\implies q_{\\text{A}} = \\frac{q_{\\text{tot}}}{1+R}$$\n$$q_{\\text{N}} = \\frac{R \\, q_{\\text{tot}}}{1+R}$$\nHowever, it is simpler to solve for $q_{\\text{A}}$ and $q_{\\text{N}}$ first. Let $K_{q\\tau} = q_{\\text{A}}\\tau_{\\text{A}} + q_{\\text{N}}\\tau_{\\text{N}}$. The equation for $I_{\\text{exc}}$ is:\n$$I_{\\text{exc}} = r_{\\text{target}} K_{q\\tau} (E_{\\text{exc}} - V_{m})$$\nWe solve for $K_{q\\tau}$:\n$$K_{q\\tau} = \\frac{I_{\\text{exc}}}{r_{\\text{target}}(E_{\\text{exc}} - V_{m})}$$\nLet's compute the value of $K_{q\\tau}$:\n- $I_{\\text{exc}} = 205 \\, \\text{pA} = 205 \\times 10^{-12} \\, \\text{A}$\n- $r_{\\text{target}} = 20 \\, \\text{s}^{-1}$\n- $E_{\\text{exc}} = 0 \\, \\text{V}$\n- $V_{m} = -55 \\times 10^{-3} \\, \\text{V}$\n- $E_{\\text{exc}} - V_m = 55 \\times 10^{-3} \\, \\text{V}$\n$$K_{q\\tau} = \\frac{205 \\times 10^{-12} \\, \\text{A}}{20 \\, \\text{s}^{-1} \\times 55 \\times 10^{-3} \\, \\text{V}} = \\frac{205 \\times 10^{-12}}{1.1} \\, \\text{S}\\cdot\\text{s} \\approx 1.8636 \\times 10^{-10} \\, \\text{S}\\cdot\\text{s}$$\nNow we use the budget constraint $q_{\\text{N}} = q_{\\text{tot}} - q_{\\text{A}}$ to solve for $q_{\\text{A}}$:\n$$K_{q\\tau} = q_{\\text{A}}\\tau_{\\text{A}} + (q_{\\text{tot}} - q_{\\text{A}})\\tau_{\\text{N}} = q_{\\text{A}}(\\tau_{\\text{A}} - \\tau_{\\text{N}}) + q_{\\text{tot}}\\tau_{\\text{N}}$$\n$$q_{\\text{A}} = \\frac{K_{q\\tau} - q_{\\text{tot}}\\tau_{\\text{N}}}{\\tau_{\\text{A}} - \\tau_{\\text{N}}}$$\nLet's compute the terms with the given values:\n- $q_{\\text{tot}} = 3 \\, \\text{nS} = 3 \\times 10^{-9} \\, \\text{S}$\n- $\\tau_{\\text{A}} = 2 \\, \\text{ms} = 2 \\times 10^{-3} \\, \\text{s}$\n- $\\tau_{\\text{N}} = 100 \\, \\text{ms} = 100 \\times 10^{-3} \\, \\text{s}$\n- $q_{\\text{tot}}\\tau_{\\text{N}} = (3 \\times 10^{-9} \\, \\text{S})(100 \\times 10^{-3} \\, \\text{s}) = 3 \\times 10^{-10} \\, \\text{S}\\cdot\\text{s}$\n- $\\tau_{\\text{A}} - \\tau_{\\text{N}} = (2 - 100) \\times 10^{-3} \\, \\text{s} = -98 \\times 10^{-3} \\, \\text{s}$\nSubstituting these into the expression for $q_{\\text{A}}$:\n$$q_{\\text{A}} = \\frac{1.8636 \\times 10^{-10} \\, \\text{S}\\cdot\\text{s} - 3 \\times 10^{-10} \\, \\text{S}\\cdot\\text{s}}{-98 \\times 10^{-3} \\, \\text{s}} = \\frac{-1.1364 \\times 10^{-10}}{-98 \\times 10^{-3}} \\, \\text{S} \\approx 1.1596 \\times 10^{-9} \\, \\text{S}$$\nSo, $q_{\\text{A}} \\approx 1.1596 \\, \\text{nS}$.\nNow we find $q_{\\text{N}}$:\n$$q_{\\text{N}} = q_{\\text{tot}} - q_{\\text{A}} = 3 \\, \\text{nS} - 1.1596 \\, \\text{nS} = 1.8404 \\, \\text{nS}$$\nFinally, we compute the minimal ratio $q_{\\text{N}}/q_{\\text{A}}$:\n$$\\frac{q_{\\text{N}}}{q_{\\text{A}}} = \\frac{1.8404 \\, \\text{nS}}{1.1596 \\, \\text{nS}} \\approx 1.5871$$\nRounding to three significant figures, the ratio is $1.59$.",
            "answer": "$$\\boxed{1.59}$$"
        }
    ]
}