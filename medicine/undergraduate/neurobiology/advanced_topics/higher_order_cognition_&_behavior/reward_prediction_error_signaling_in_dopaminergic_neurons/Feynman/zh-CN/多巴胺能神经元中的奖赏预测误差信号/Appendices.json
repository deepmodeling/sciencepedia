{
    "hands_on_practices": [
        {
            "introduction": "奖励预测误差（RPE）是理解多巴胺神经元功能的基石。本练习将引导你使用时间差分（TD）学习的核心公式，来亲手计算这一关键信号。通过这个基础计算，你将能够量化“结果超出预期”的程度，并理解为何它会产生一个正的RPE信号。",
            "id": "5058200",
            "problem": "在一个由时间差分（TD）学习建模的巴甫洛夫条件反射任务中，通常假设腹侧被盖区的多巴胺能神经元编码奖励预测误差（RPE），该误差基于状态值比较实际结果与预期未来回报。一个动物从状态 $s_t$ 转移到 $s_{t+1}$，接收到即时奖励 $r_t$，并且拥有状态值估计 $V(s_t)$ 和 $V(s_{t+1})$。折扣因子为 $\\gamma$。\n\n给定 $V(s_t)=0.5$，$V(s_{t+1})=0.6$，$r_t=1$以及$\\gamma=0.9$，计算 RPE $\\delta_t$。然后，根据 $\\delta_t$ 的符号，说明多巴胺能放电会暂时高于基线还是低于基线，并从意外奖励的角度解释原因。\n\n将 $\\delta_t$ 表示为一个不带单位的纯数。无需四舍五入。",
            "solution": "该问题要求在时间差分（TD）学习框架下计算奖励预测误差（RPE），记为 $\\delta_t$，并在多巴胺能神经元活动的背景下解释该信号。\n\n首先，我们必须定义 RPE。在 TD 学习中，时间 $t$ 的 RPE $\\delta_t$ 量化了观察到的奖励加上后续状态的折扣估计值与当前状态的估计值之间的差异。这就是“TD误差”信号。其公式为：\n$$\n\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n$$\n其中：\n- $r_t$ 是从状态 $s_t$ 转移时获得的即时奖励。\n- $\\gamma$ 是折扣因子，决定了未来奖励的现值。\n- $V(s_{t+1})$ 是后续状态 $s_{t+1}$ 的估计值。\n- $V(s_t)$ 是当前状态 $s_t$ 的估计值。\n\n术语 $r_t + \\gamma V(s_{t+1})$ 称为 TD 目标。它代表了基于转移结果对状态 $s_t$ 的值的更新、更准确的估计。因此，RPE $\\delta_t$ 是这个新估计（目标）与旧估计 $V(s_t)$ 之间的误差。\n\n问题给出了以下数值：\n- $V(s_t) = 0.5$\n- $V(s_{t+1}) = 0.6$\n- $r_t = 1$\n- $\\gamma = 0.9$\n\n将这些值代入 RPE 公式：\n$$\n\\delta_t = 1 + (0.9)(0.6) - 0.5\n$$\n我们首先计算折扣因子与下一个状态值的乘积：\n$$\n(0.9)(0.6) = 0.54\n$$\n现在，我们将此结果代回主方程：\n$$\n\\delta_t = 1 + 0.54 - 0.5\n$$\n$$\n\\delta_t = 1.54 - 0.5\n$$\n$$\n\\delta_t = 1.04\n$$\n计算出的奖励预测误差为 $\\delta_t = 1.04$。\n\n接下来，我们必须在多巴胺能神经元放电的背景下解释 $\\delta_t$ 的符号。多巴胺奖励预测误差假说认为，中脑多巴胺能神经元的阶段性活动（phasic activity）发出了 RPE 信号。\n- 一个正的 RPE（$\\delta_t > 0$）表示结果好于预期。实际奖励（$r_t$）与下一个状态的折扣值（$\\gamma V(s_{t+1})$）之和大于在状态 $s_t$ 时预测的值。这种“正向意外”导致多巴胺能神经元的放电频率在其基线紧张性活动（tonic activity）之上出现短暂的、阶段性的*增加*。\n- 一个负的 RPE（$\\delta_t < 0$）表示结果差于预期，导致放电频率短暂*下降*（暂停或降低）至基线以下。\n- 一个零 RPE（$\\delta_t = 0$）表示结果与预期完全一致，导致基线放电频率没有变化。\n\n在本例中，$\\delta_t = 1.04$，这是一个正值。因此，结果显著好于预测。智能体预期从状态 $s_t$ 获得的总未来回报为 $V(s_t) = 0.5$，但它收到了一个即时奖励 $r_t = 1$ 并转移到了一个折扣值为 $\\gamma V(s_{t+1}) = 0.54$ 的状态，总的“实际”回报为 $1.54$。这个 $1.04$ 的正差异构成了一个正向意外。根据该假说，这将导致腹侧被盖区的多巴胺能放电暂时增加到其基线水平之上，标志着一个出乎意料的好结果。",
            "answer": "$$\n\\boxed{1.04}\n$$"
        },
        {
            "introduction": "奖励预测误差不仅仅是一个信号，它还是驱动学习的“教学信号”。本练习将带你探究经典的“阻断效应”，这是一个揭示了学习与“意外”之间深刻联系的著名心理学现象。你将运用RPE的计算来解释为何当一个奖励被完全预测时，新的线索无法与之建立关联，从而体会到“没有意外，就没有学习”这一深刻原理。",
            "id": "5058233",
            "problem": "一项经典的巴甫洛夫条件反射实验分两个阶段进行，实验中使用的线索和奖励是研究多巴胺能神经元信号传递的标准范式。在阶段 $1$ 中，单个线索 $A$ 与大小为 $R_1$（无量纲归一化单位）的奖励重复配对，直至训练达到渐近稳定状态。根据经验和理论，在渐近稳定时，为线索 $A$ 编码的期望值等于所给的奖励，即 $V(A)=R_1$。在阶段 $2$ 中，复合线索 $AB$ 在两种不同的条件下呈现。在条件 $B_{\\text{lock}}$（阻断）下，当呈现 $AB$ 时，给出的奖励仍然是 $R_1$。在条件 $U_{\\text{nblock}}$（解除阻断）下，当呈现 $AB$ 时，给出的奖励增加到 $R_2 > R_1$。假设复合线索的期望值是其关联强度的加和，因此 $V(AB)=V(A)+V(B)$，并且由于新线索 $B$ 从未与奖励配对过，在阶段 $2$ 开始时其初始值为 $V(B)=0$。设给出的奖励为 $R_1=1.0$ 和 $R_2=1.4$（无量纲归一化单位）。\n\n多巴胺能神经元编码奖励预测误差，该误差定义为在奖励送达时，获得的奖励与期望的奖励之间的差值。以此为基础，通过从基本原理计算奖励预测误差，推导出在条件 $B_{\\text{lock}}$ 和条件 $U_{\\text{nblock}}$ 下，奖励送达时刻的预测多巴胺信号。将你的最终答案以行矩阵的形式报告，其中包含分别对应于条件 $B_{\\text{lock}}$ 和 $U_{\\text{nblock}}$ 的两个奖励预测误差的实数（无量纲归一化单位）。无需四舍五入。",
            "solution": "首先根据所需标准对问题进行验证。\n\n### 步骤 1：提取已知条件\n- 在阶段 $1$ 中，线索 $A$ 与奖励 $R_1$ 配对，直至达到渐近稳定状态。\n- 在渐近稳定时，线索 $A$ 的期望值为 $V(A) = R_1$。\n- 在阶段 $2$ 中，呈现复合线索 $AB$。\n- 在阶段 $2$ 开始时，新线索 $B$ 的初始值为 $V(B) = 0$。\n- 复合线索 $AB$ 的期望值是关联强度的加和：$V(AB) = V(A) + V(B)$。\n- 在条件 $B_{\\text{lock}}$（阻断）下，线索 $AB$ 获得的奖励为 $R_1$。\n- 在条件 $U_{\\text{nblock}}$（解除阻断）下，线索 $AB$ 获得的奖励为 $R_2$。\n- 奖励之间的关系是 $R_2 > R_1$。\n- 奖励的具体数值为 $R_1 = 1.0$ 和 $R_2 = 1.4$。\n- 奖励预测误差（$\\delta$）的定义是获得的奖励（$R$）与期望的奖励（$V$）之间的差值：$\\delta = R - V$。\n- 任务是计算条件 $B_{\\text{lock}}$ 和条件 $U_{\\text{nblock}}$ 下的奖励预测误差。\n\n### 步骤 2：使用提取的已知条件进行验证\n评估问题的有效性。\n- **科学依据：**该问题是计算神经科学中的一个经典表述，具体涉及多巴胺功能的强化学习模型，如 Rescorla-Wagner 模型。关联值（$V$）、复合线索、阻断现象和奖励预测误差（$\\delta$）等概念是该领域的基石。给出的关系式 $\\delta = R - V$ 是奖励预测误差的经典定义。\n- **适定性：**该问题是完全自洽的。它提供了计算两种指定条件下唯一解所需的所有必要定义（$V(AB) = V(A) + V(B)$，$\\delta = R - V$）、初始条件（来自阶段 $1$ 的 $V(A) = R_1$，阶段 $2$ 开始时的 $V(B) = 0$）和参数（$R_1$，$R_2$）。\n- **客观性：**该问题以精确、正式的语言陈述，没有歧义或主观论断。\n\n该问题不存在任何已列出的无效性缺陷。它在科学上是合理的，是适定的，并且是客观的。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将推导解答。\n\n问题的核心是计算在两种不同实验条件下，奖励送达时刻的奖励预测误差（此处用 $\\delta$ 表示）。给出的基本方程是：\n$$\n\\delta = R_{\\text{obtained}} - V_{\\text{expected}}\n$$\n其中 $R_{\\text{obtained}}$ 是实际送达的奖励，而 $V_{\\text{expected}}$ 是基于当前线索的总期望奖励。\n\n在阶段 $2$ 开始时，系统已经过训练，使得线索 $A$ 的值达到了一个渐近稳定状态，其值等于与之配对的奖励。\n$$\nV(A) = R_1\n$$\n线索 $B$ 是新的，意味着它之前没有与奖励建立关联。\n$$\nV(B) = 0\n$$\n在阶段 $2$ 中，呈现了复合线索 $AB$。根据问题陈述，这个复合线索的期望值是其各个组成部分值的总和。\n$$\nV_{\\text{expected}} = V(AB) = V(A) + V(B)\n$$\n代入阶段 $2$ 开始时 $V(A)$ 和 $V(B)$ 的已知值：\n$$\nV_{\\text{expected}} = R_1 + 0 = R_1\n$$\n这个期望值 $V_{\\text{expected}} = R_1$ 对 $B_{\\text{lock}}$ 和 $U_{\\text{nblock}}$ 两种条件都是相同的，因为期望是在新奖励送达之前基于先前的学习形成的。\n\n现在，我们分别计算每种条件下的奖励预测误差。\n\n**条件 $B_{\\text{lock}}$（阻断）：**\n在这种条件下，送达的奖励 $R_{\\text{obtained}}$ 是 $R_1$。\n奖励预测误差 $\\delta_{\\text{lock}}$ 为：\n$$\n\\delta_{\\text{lock}} = R_{\\text{obtained}} - V_{\\text{expected}} = R_1 - R_1 = 0\n$$\n这个结果，即预测误差为零，是阻断效应的核心特征。因为奖励被预先训练的线索 $A$ 完全预测，所以结果中没有“意外”。因此，关于线索 $B$ 的新学习不会发生。预测多巴胺信号将不会显示出与基线放电率相比的变化。\n\n**条件 $U_{\\text{nblock}}$（解除阻断）：**\n在这种条件下，送达的奖励 $R_{\\text{obtained}}$ 增加到 $R_2$。\n奖励预测误差 $\\delta_{\\text{unblock}}$ 为：\n$$\n\\delta_{\\text{unblock}} = R_{\\text{obtained}} - V_{\\text{expected}} = R_2 - R_1\n$$\n这是一个正向的预测误差，因为问题陈述了 $R_2 > R_1$。这个“意外”信号表明结果好于预期。这种正向误差信号被认为可以驱动学习，导致当前线索（在这种情况下是线索 $B$，因为 $V(A)$ 相对于 $R_1$ 已经达到其渐近稳定状态）的关联强度增加。预测多巴胺信号将显示出阶段性的爆发活动。\n\n最后，我们代入给定的数值，$R_1 = 1.0$ 和 $R_2 = 1.4$。\n\n对于条件 $B_{\\text{lock}}$：\n$$\n\\delta_{\\text{lock}} = 0\n$$\n\n对于条件 $U_{\\text{nblock}}$：\n$$\n\\delta_{\\text{unblock}} = R_2 - R_1 = 1.4 - 1.0 = 0.4\n$$\n\n最终答案应以包含这两个值 $[\\delta_{\\text{lock}}, \\delta_{\\text{unblock}}]$ 的行矩阵形式报告。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  0.4 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "我们已经知道RPE信号能够驱动学习，但大脑是如何在突触层面实现这一过程的呢？本练习将引导你探索一个被称为“三因子学习法则”的生物学模型。你将看到，多巴胺所携带的全局RPE信号如何与局部的突触活动（资格痕迹）相互作用，从而精确地调整突触权重，实现长期增强（LTP）或长期抑制（LTD）。",
            "id": "5058243",
            "problem": "在经典条件反射范式中，中脑的多巴胺能神经元会广播一个奖励预测误差（RPE）信号，记作 $\\delta_t$，该信号报告了接收到的奖励与预期奖励之间的瞬时差异。一个投射到中型多棘神经元上的皮质纹状体突触维持着一个资格迹，记作 $e_t$，它反映了近期的突触前和突触后活动，并随时间衰减。经验和理论研究支持一种三因子学习规则：在时间 $t$ 的突触权重变化，记作 $\\Delta w$，取决于一个局部资格因子、一个全局调制性教学信号和一个学习率。假设遵循以下基本原则：\n- 如果资格迹或教学信号任一为零，则那一刻没有可塑性发生。\n- 在操作点附近，突触更新可以通过资格迹和教学信号之间的最低阶（线性）相互作用来建模。\n- 一个正的学习率 $\\alpha$ 决定了变化的总体幅度。\n\n已知在特定时间 $t$，资格迹为 $e_t = 0.4$，奖励预测误差为 $\\delta_t = 0.2$，学习率为 $\\alpha = 0.05$（所有量均为无单位量），请完成以下任务：\n1. 根据上述原则，写出与三因子规则一致的 $\\Delta w$ 的最简线性表达式。\n2. 使用所提供的值，计算 $\\Delta w$ 的数值。\n3. 根据 $\\Delta w$ 的符号，说明此时突触应发生长时程增强还是长时程抑制。\n\n将 $\\Delta w$ 的数值作为你的最终答案。最终答案以精确小数形式表示（无需四舍五入），并注意突触权重是无单位的。将你的推理和解释与最终数值答案分开提供。",
            "solution": "问题陈述已经过验证，被认为是合理的。它在科学上基于计算神经科学的原理，特别是应用于突触可塑性的强化学习理论。该问题提法恰当、客观，并包含所有推导出唯一解而无矛盾所需的信息。\n\n问题要求三部分内容：学习规则的推导、突触权重变化 $\\Delta w$ 的计算，以及对这一变化的解释。\n\n1.  **突触更新规则的构建**\n\n任务是找到与所述三因子学习规则一致的突触权重变化 $\\Delta w$ 的最简数学表达式。这三个因子是：\n-   学习率，$\\alpha$。\n-   资格迹，$e_t$。\n-   教学信号，即奖励预测误差（RPE），$\\delta_t$。\n\n问题提供了三个基本原则来指导构建：\n-   原则1：如果资格迹（$e_t$）或教学信号（$\\delta_t$）任一为零，则没有可塑性发生。在数学上，如果 $e_t = 0$ 或 $\\delta_t = 0$，那么 $\\Delta w = 0$。\n-   原则2：更新规则由资格迹 $e_t$ 和教学信号 $\\delta_t$ 之间的“最低阶（线性）相互作用”来建模。在双变量函数 $f(x, y)$ 在 $(0,0)$ 点附近的泰勒展开中，最低阶的相互作用项形式为 $c \\cdot x \\cdot y$。更高阶的相互作用项会是像 $x^2 y$，$x y^2$ 等。因此，$e_t$ 和 $\\delta_t$ 之间的相互作用意味着一个与它们乘积 $e_t \\delta_t$ 成比例的项。这种乘积形式也满足原则1，因为如果 $e_t$ 或 $\\delta_t$ 任一为零，表达式即为零。而加法形式，如 $A e_t + B \\delta_t$，则在另一个变量为任意非零值时，除非系数 $A$ 和 $B$ 为零，否则无法满足原则1。\n-   原则3：一个正的学习率 $\\alpha$ 决定了变化的总体幅度。这意味着 $\\alpha$ 作为一个比例常数。\n\n结合这些原则，突触权重变化 $\\Delta w$ 与学习率、资格迹和奖励预测误差的乘积成正比。满足所有三个条件的最简表达式是：\n$$\n\\Delta w = \\alpha \\cdot e_t \\cdot \\delta_t\n$$\n\n这个方程是基底神经节强化学习模型的基石。\n\n2.  **计算 $\\Delta w$ 的数值**\n\n问题提供了在特定时间 $t$ 的以下数值：\n-   学习率: $\\alpha = 0.05$\n-   资格迹: $e_t = 0.4$\n-   奖励预测误差: $\\delta_t = 0.2$\n\n将这些值代入推导出的 $\\Delta w$ 方程中：\n$$\n\\Delta w = (0.05) \\cdot (0.4) \\cdot (0.2)\n$$\n\n计算过程如下：\n$$\n\\Delta w = (0.020) \\cdot (0.2)\n$$\n$$\n\\Delta w = 0.004\n$$\n所有量都是无单位的，因此得到的突触权重变化也是无单位的。\n\n3.  **$\\Delta w$ 符号的解释**\n\n项 $\\Delta w$ 代表突触强度（或权重）的变化。$\\Delta w$ 的符号决定了可塑性的方向。\n-   如果 $\\Delta w > 0$，突触权重增加。这种突触的增强被称为**长时程增强（LTP）**。\n-   如果 $\\Delta w < 0$，突触权重减少。这种突触的减弱被称为**长时程抑制（LTD）**。\n-   如果 $\\Delta w = 0$，突触权重没有变化。\n\n在本例中，计算值为 $\\Delta w = 0.004$，是一个正数。因此，该突触应发生增强。\n\n这个结果与强化学习理论一致。正的奖励预测误差（$\\delta_t = 0.2 > 0$）表明结果好于预期。学习规则的作用是增强最近活跃的突触（由非零的资格迹 $e_t=0.4$ 表示），以使导致这一积极结果的行为在未来更有可能发生。",
            "answer": "$$\n\\boxed{0.004}\n$$"
        }
    ]
}