{
    "hands_on_practices": [
        {
            "introduction": "The journey of a neural signal in a brain-computer interface begins with its acquisition. Modern microelectrode arrays can record from hundreds of neurons simultaneously, generating a massive amount of raw data. This exercise () provides a foundational understanding of the engineering constraints by tasking you to calculate the data rate and storage requirements for a typical high-channel-count recording system. Mastering this calculation is the first step in appreciating the significant hardware and data management challenges inherent in building practical BCIs.",
            "id": "4457855",
            "problem": "A multichannel extracellular recording system in a Brain-Computer Interface (BCI) application uses a microelectrode array (MEA) with $96$ independent channels to acquire neuronal spike waveforms. Each channel is digitized at a constant sampling frequency of $30\\,\\mathrm{kHz}$ with an analog-to-digital converter (ADC) resolution of $12$ bits per sample. Assume continuous acquisition, synchronous sampling across channels, and raw streaming with no compression, framing, metadata, or line-coding overhead; assume that samples are densely packed bitwise so that exactly $12$ bits are stored per sample.\n\nStarting from fundamental definitions of sampling frequency (samples per second) and information representation in digital systems (bits per sample), derive the expressions needed to compute:\n1. The raw data rate of the entire system.\n2. The total storage required for $1$ hour of recording.\n\nExpress the raw data rate in megabytes per second (MB/s) and the total storage in gigabytes (GB), using the decimal definitions $1\\,\\mathrm{MB} = 10^{6}\\,\\mathrm{bytes}$ and $1\\,\\mathrm{GB} = 10^{9}\\,\\mathrm{bytes}$. Provide the final numerical values without rounding.",
            "solution": "The foundational definitions we use are:\n- The sampling frequency $f_{s}$ gives the number of samples produced per second per channel.\n- The analog-to-digital converter resolution $n_{b}$ gives the number of bits used to represent each sample.\n- For $N_{c}$ channels sampled independently and synchronously, the total number of samples per second is $N_{c} f_{s}$.\n- The raw bit rate is the product of the number of samples per second and the number of bits per sample.\n\nLet $f_{s} = 30\\,\\mathrm{kHz} = 30{,}000\\,\\mathrm{s}^{-1}$, $n_{b} = 12\\,\\mathrm{bits/sample}$, and $N_{c} = 96$ channels.\n\nThe raw bit rate $R_{\\mathrm{bits}}$ is\n$$\nR_{\\mathrm{bits}} = f_{s} \\times n_{b} \\times N_{c}.\n$$\nSubstituting symbols and then values:\n$$\nR_{\\mathrm{bits}} = 30{,}000 \\times 12 \\times 96 = 34{,}560{,}000 \\ \\text{bits/s}.\n$$\n\nTo convert to bytes per second, use $1\\,\\mathrm{byte} = 8\\,\\mathrm{bits}$:\n$$\nR_{\\mathrm{bytes}} = \\frac{R_{\\mathrm{bits}}}{8} = \\frac{34{,}560{,}000}{8} = 4{,}320{,}000 \\ \\text{bytes/s}.\n$$\n\nTo express the raw data rate in megabytes per second (decimal), use $1\\,\\mathrm{MB} = 10^{6}\\,\\mathrm{bytes}$:\n$$\nR_{\\mathrm{MB/s}} = \\frac{R_{\\mathrm{bytes}}}{10^{6}} = \\frac{4{,}320{,}000}{1{,}000{,}000} = 4.32.\n$$\n\nNext, compute the total storage for a recording duration $T = 1\\,\\mathrm{hour} = 3600\\,\\mathrm{s}$. The total bytes $S_{\\mathrm{bytes}}$ stored are\n$$\nS_{\\mathrm{bytes}} = R_{\\mathrm{bytes}} \\times T = 4{,}320{,}000 \\times 3{,}600 = 15{,}552{,}000{,}000 \\ \\text{bytes}.\n$$\n\nConvert to gigabytes (decimal) using $1\\,\\mathrm{GB} = 10^{9}\\,\\mathrm{bytes}$:\n$$\nS_{\\mathrm{GB}} = \\frac{S_{\\mathrm{bytes}}}{10^{9}} = \\frac{15{,}552{,}000{,}000}{1{,}000{,}000{,}000} = 15.552.\n$$\n\nThus, the raw data rate is $4.32\\,\\mathrm{MB/s}$ and the storage required for one hour is $15.552\\,\\mathrm{GB}$, under the stated assumptions and decimal unit definitions.",
            "answer": "$$\\boxed{\\begin{pmatrix}4.32  15.552\\end{pmatrix}}$$"
        },
        {
            "introduction": "Once neural data is acquired, the central challenge is to decode the user's intent. This practice () guides you through the core of this process by developing a linear decoder using ridge regression, a robust machine learning technique used to prevent overfitting. By deriving the decoder and analyzing its performance, you will explore the fundamental bias-variance trade-off, learning how the regularization parameter $ \\lambda $ helps create a model that generalizes well from a limited set of training data to new, unseen neural patterns.",
            "id": "5002219",
            "problem": "A laboratory is developing a Brain-Computer Interface (BCI) decoder that maps neuronal firing rates to the handâ€™s scalar tangential velocity during a center-out reaching task. In each time bin indexed by $t \\in \\{1,\\ldots,N\\}$ of width $\\Delta t$, the preprocessed firing rate vector is $\\mathbf{r}_{t} \\in \\mathbb{R}^{p}$ (already z-scored and whitened across neurons so that the sample covariance is the identity), and the simultaneously measured hand velocity is $v_{t} \\in \\mathbb{R}$. Assume a linear-Gaussian encoding model $v_{t} = \\mathbf{r}_{t}^{\\top} \\boldsymbol{\\beta} + \\varepsilon_{t}$ where $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$ are independent across $t$ and independent of $\\mathbf{r}_{t}$. Stack the data into the design matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$ with rows $\\mathbf{r}_{t}^{\\top}$ and target vector $\\mathbf{y} \\in \\mathbb{R}^{N}$ with entries $v_{t}$. Because the neuronal features were whitened using the training data, you may assume the empirical second moment satisfies $\\mathbf{X}^{\\top}\\mathbf{X} = N \\mathbf{I}_{p}$.\n\nYou train a ridge regression decoder by minimizing the penalized least-squares objective\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2},\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter and $\\|\\cdot\\|$ denotes the Euclidean norm.\n\nTasks:\n1) Starting from the model $ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{N})$, derive the closed-form solution $\\widehat{\\mathbf{w}}$ that minimizes $J(\\mathbf{w};\\lambda)$.\n\n2) For a new, independent test sample $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$ drawn from the same distribution, with $\\mathbf{r}_{\\mathrm{new}}$ independent of training data and satisfying $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}] = \\mathbf{0}$ and $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$, derive the expected out-of-sample mean squared prediction error\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]\n$$\nas an explicit function of $\\lambda$, $N$, $p$, $\\sigma^{2}$, and $\\boldsymbol{\\beta}$. Your derivation must start from the definitions above and the assumptions on $\\mathbf{X}$ and $\\mathbf{r}_{\\mathrm{new}}$, and it must expose the bias-variance decomposition that depends on $\\lambda$.\n\n3) To make the trade-off explicit and independent of a particular unknown $\\boldsymbol{\\beta}$, assume a hierarchical prior consistent with neural population codes: $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$ with $\\tau^{2}  0$. Average your expression for $\\mathcal{E}(\\lambda)$ over this prior and simplify to a scalar function of $\\lambda$, $N$, $p$, $\\sigma^{2}$, and $\\tau^{2}$.\n\n4) Using your averaged expression, determine the value $\\lambda^{\\star}$ that minimizes the expected out-of-sample mean squared prediction error. Then, evaluate this optimum numerically for\n- $p = 100$,\n- $N = 10000$,\n- $\\sigma^{2} = 0.04$,\n- $\\tau^{2} = 0.01$.\nExpress the final value of $\\lambda^{\\star}$ as a pure number without units. If rounding is necessary, round to four significant figures. If not, provide the exact value.",
            "solution": "The problem asks for a multi-step analysis of a ridge regression decoder in the context of a Brain-Computer Interface (BCI). The analysis involves deriving the decoder, its out-of-sample error, and the optimal regularization parameter under a specific data model and prior.\n\n### Task 1: Derivation of the Ridge Regression Estimator $\\widehat{\\mathbf{w}}$\nThe ridge regression estimator $\\widehat{\\mathbf{w}}$ is found by minimizing the objective function\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2}\n$$\nwhere $\\|\\cdot\\|$ is the Euclidean norm. We can write the squared norms in terms of vector transposes:\n$$\nJ(\\mathbf{w};\\lambda) = (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^{\\top}(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\nExpanding the first term gives:\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - \\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w} - \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{w} + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\nSince $\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y}$ is a scalar, it equals its transpose $\\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w}$. Thus, we can combine the cross-terms:\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\nTo find the minimum, we take the gradient of $J(\\mathbf{w};\\lambda)$ with respect to $\\mathbf{w}$ and set it to zero. Using standard matrix calculus results ($\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{a} = \\mathbf{a}$ and $\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{M}\\mathbf{w} = 2\\mathbf{M}\\mathbf{w}$ for symmetric $\\mathbf{M}$):\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w};\\lambda) = -2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\nSetting the gradient to the zero vector gives the solution $\\widehat{\\mathbf{w}}$:\n$$\n-2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{0}\n$$\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{X}^{\\top}\\mathbf{y}\n$$\nThe formal solution is $\\widehat{\\mathbf{w}} = (\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$. The problem states the assumption that the empirical second moment is $\\mathbf{X}^{\\top}\\mathbf{X} = N\\mathbf{I}_{p}$. Substituting this into the expression for $\\widehat{\\mathbf{w}}$:\n$$\n\\widehat{\\mathbf{w}} = (N\\mathbf{I}_{p} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = ((N+\\lambda)\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{I}_{p}^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\nThis is the closed-form solution for $\\widehat{\\mathbf{w}}$ under the given assumption.\n\n### Task 2: Expected Out-of-Sample Mean Squared Prediction Error\nWe are asked to derive $\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]$. The expectation is taken over the randomness in the training data (which makes $\\widehat{\\mathbf{w}}$ random) and the new test sample $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$.\nThe true model for the new sample is $v_{\\mathrm{new}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}$, where $\\varepsilon_{\\mathrm{new}} \\sim \\mathcal{N}(0,\\sigma^{2})$. Substituting this into the error term:\n$$\nv_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}) - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}\n$$\nSquaring this expression:\n$$\n(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2} + 2\\varepsilon_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}^{2}\n$$\nNow we take the expectation. The new noise term $\\varepsilon_{\\mathrm{new}}$ is independent of the training data (and thus $\\widehat{\\mathbf{w}}$) and the new features $\\mathbf{r}_{\\mathrm{new}}$. Since $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}] = 0$, the cross-term vanishes. We have $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}^{2}] = \\sigma^{2}$.\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] + \\sigma^{2}\n$$\nThe remaining expectation is over $\\mathbf{r}_{\\mathrm{new}}$ and $\\widehat{\\mathbf{w}}$. We can rewrite the term inside the expectation using the trace trick: $(\\mathbf{a}^{\\top}\\mathbf{b})^2 = \\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b} = \\mathrm{tr}(\\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}) = \\mathrm{tr}(\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}\\mathbf{b}^{\\top})$. It is simpler to use $\\mathbb{E}[x^2] = \\mathbb{E}[\\mathrm{tr}(x^2)]$ where $x$ is a scalar.\n$$\n\\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] = \\mathbb{E}\\left[\\mathrm{tr}\\left((\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top} (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right)\\right]\n$$\nBy linearity of trace and expectation, and since $\\widehat{\\mathbf{w}}$ (from training data) is independent of $\\mathbf{r}_{\\mathrm{new}}$:\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right)\n$$\nUsing the assumption $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$:\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top}\\mathbf{I}_{p}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right) = \\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right]\n$$\nSo, $\\mathcal{E}(\\lambda) = \\mathbb{E}[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}] + \\sigma^2$. The expectation $\\mathbb{E}[\\cdot]$ is now only over the training data randomness. We now perform a bias-variance decomposition:\n$$\n\\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right] = \\left\\|\\boldsymbol{\\beta} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\right\\|^{2} + \\mathbb{E}\\left[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^{2}\\right] = \\text{Bias}(\\widehat{\\mathbf{w}})^{2} + \\text{Var}(\\widehat{\\mathbf{w}})\n$$\nWe derive the bias and variance terms. First, substitute $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ into the expression for $\\widehat{\\mathbf{w}}$:\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\n$$\nThe expectation of $\\widehat{\\mathbf{w}}$ (over $\\boldsymbol{\\varepsilon}$) is:\n$$\n\\mathbb{E}[\\widehat{\\mathbf{w}}] = \\mathbb{E}\\left[\\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\\right] = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}]) = \\frac{N}{N+\\lambda}\\boldsymbol{\\beta}\n$$\nThe squared bias is:\n$$\n\\text{Bias}(\\widehat{\\mathbf{w}})^{2} = \\left\\|\\mathbb{E}[\\widehat{\\mathbf{w}}] - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|\\frac{N}{N+\\lambda}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|-\\frac{\\lambda}{N+\\lambda}\\boldsymbol{\\beta}\\right\\|^{2} = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2}\n$$\nThe variance term is $\\mathbb{E}[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^2] = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}}))$.\n$$\n\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}] = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\n$$\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\mathbb{E}\\left[(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])^{\\top}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbb{E}\\left[\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]\\mathbf{X}\n$$\nUsing $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]=\\sigma^2\\mathbf{I}_N$ and $\\mathbf{X}^{\\top}\\mathbf{X}=N\\mathbf{I}_p$:\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{I}_{N}\\mathbf{X} = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{X} = \\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\n$$\nThe variance is the trace of this covariance matrix:\n$$\n\\text{Var}(\\widehat{\\mathbf{w}}) = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}})) = \\mathrm{tr}\\left(\\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\\right) = \\frac{pN\\sigma^2}{(N+\\lambda)^2}\n$$\nCombining all terms, the expected out-of-sample error is:\n$$\n\\mathcal{E}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### Task 3: Averaging Over the Prior for $\\boldsymbol{\\beta}$\nWe are given a prior distribution over the true weights, $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$. We need to compute $\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\mathbb{E}_{\\boldsymbol{\\beta}}[\\mathcal{E}(\\lambda)]$. The only term in $\\mathcal{E}(\\lambda)$ that depends on $\\boldsymbol{\\beta}$ is $\\|\\boldsymbol{\\beta}\\|^{2}$. We compute its expectation under the prior:\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\sum_{i=1}^{p}\\beta_{i}^{2}\\right] = \\sum_{i=1}^{p}\\mathbb{E}_{\\boldsymbol{\\beta}}[\\beta_{i}^{2}]\n$$\nFor each component $\\beta_i \\sim \\mathcal{N}(0, \\tau^{2})$, the second moment is $\\mathbb{E}[\\beta_{i}^{2}] = \\mathrm{Var}(\\beta_i) + (\\mathbb{E}[\\beta_i])^{2} = \\tau^{2} + 0^{2} = \\tau^{2}$.\nTherefore, the expected squared norm is:\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\sum_{i=1}^{p}\\tau^{2} = p\\tau^{2}\n$$\nSubstituting this into the expression for $\\mathcal{E}(\\lambda)$:\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}p\\tau^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{p\\lambda^{2}\\tau^{2} + pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### Task 4: Optimal Regularization Parameter $\\lambda^{\\star}$\nTo find the $\\lambda$ that minimizes $\\mathcal{E}_{\\mathrm{avg}}(\\lambda)$, we differentiate with respect to $\\lambda$ and set the derivative to zero. The constant term $\\sigma^{2}$ can be ignored during minimization.\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{d}{d\\lambda}\\left( \\frac{p(\\lambda^{2}\\tau^{2} + N\\sigma^2)}{(N+\\lambda)^2} \\right)\n$$\nUsing the quotient rule $\\frac{d}{dx}(\\frac{u}{v}) = \\frac{u'v - uv'}{v^2}$:\nLet $u(\\lambda) = p(\\lambda^{2}\\tau^{2} + N\\sigma^2)$ and $v(\\lambda) = (N+\\lambda)^2$.\nThen $u'(\\lambda) = 2p\\lambda\\tau^{2}$ and $v'(\\lambda) = 2(N+\\lambda)$.\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{(2p\\lambda\\tau^{2})(N+\\lambda)^2 - p(\\lambda^{2}\\tau^{2} + N\\sigma^2) \\cdot 2(N+\\lambda)}{(N+\\lambda)^4}\n$$\nSetting the derivative to zero and assuming $\\lambda \\ge 0, N \\ge 1$, we can simplify by dividing by the non-zero factor $2p(N+\\lambda)$:\n$$\n(\\lambda\\tau^{2})(N+\\lambda) - (\\lambda^{2}\\tau^{2} + N\\sigma^2) = 0\n$$\n$$\nN\\lambda\\tau^{2} + \\lambda^{2}\\tau^{2} - \\lambda^{2}\\tau^{2} - N\\sigma^2 = 0\n$$\n$$\nN\\lambda\\tau^{2} = N\\sigma^2\n$$\nSince $N \\ge 1$ and $\\tau^{2}  0$ (given), we can divide by $N\\tau^{2}$ to find the optimal $\\lambda^{\\star}$:\n$$\n\\lambda^{\\star} = \\frac{\\sigma^2}{\\tau^2}\n$$\nThis result is elegantly simple and represents the ratio of the noise variance in the measurements to the prior variance of the model parameters. The second derivative can be checked to confirm this is a minimum. The numerator of the derivative simplified to $2pN(\\lambda\\tau^2 - \\sigma^2)(N+\\lambda)$, which is negative for $\\lambda  \\sigma^2/\\tau^2$ and positive for $\\lambda  \\sigma^2/\\tau^2$, confirming a minimum.\n\nFinally, we evaluate this expression numerically with the provided values:\n- $\\sigma^{2} = 0.04$\n- $\\tau^{2} = 0.01$\nThe values for $p$ and $N$ are not needed to find $\\lambda^{\\star}$ in this idealized setting.\n$$\n\\lambda^{\\star} = \\frac{0.04}{0.01} = 4\n$$\nThe optimal value is exactly $4$.",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "After building a decoder, we need a rigorous way to measure its performance. Simply stating the accuracy is not enough, as it doesn't account for the speed of communication or the number of possible choices. This problem () introduces the Information Transfer Rate (ITR), the gold standard for BCI evaluation, which is derived from the principles of Shannon's information theory. By calculating the ITR for a BCI speller, you will learn how to quantify the true communication bandwidth of a BCI channel, providing a powerful metric for comparing different system designs and user proficiency.",
            "id": "5002103",
            "problem": "A Brain-Computer Interface (BCI) speller presents a user with a fixed set of $N$ equally likely symbols. Each selection trial takes $T$ seconds from cue onset to finalized output, and the classifier selects the intended symbol with probability $P$; when an error occurs, it is uniformly distributed over the $N-1$ non-target symbols. Assume trials are independent and the symbol prior is uniform.\n\nStarting from the fundamental definitions of Shannon entropy and mutual information, derive expressions that formalize:\n- the symbol throughput (expected correctly communicated symbols per minute),\n- the raw bitrate upper bound (bits per minute assuming error-free communication of $N$-ary symbols),\n- the information transfer rate (mutual information per minute through the selection channel under the stated error model).\n\nThen, for a speller with $N=36$, $T=4\\,\\mathrm{s}$, and $P=0.85$, compute the numerical values of the symbol throughput, the raw bitrate, and the information transfer rate. Report your final numerical value for the Information Transfer Rate in bits per minute. Round your final answer to four significant figures and express it in bits per minute.",
            "solution": "### Derivation of Performance Metrics\n\nWe will derive the expressions for the three requested metrics based on the provided parameters $N$, $T$, and $P$. All information-theoretic quantities will be calculated using the base-$2$ logarithm, yielding results in bits. The number of trials (selections) that can be made per minute is given by $\\frac{60}{T}$.\n\n**1. Symbol Throughput (ST)**\nThe symbol throughput is the expected number of correctly communicated symbols per unit of time.\nA single trial results in a correct symbol with probability $P$. The expected number of correct symbols per trial is thus $1 \\times P + 0 \\times (1-P) = P$.\nMultiplying the expected correct symbols per trial by the number of trials per minute gives the symbol throughput in symbols per minute.\n$$\n\\text{ST} = P \\times \\frac{60}{T}\n$$\n\n**2. Raw Bitrate Upper Bound (RBUB)**\nThe raw bitrate represents the theoretical maximum information transfer rate assuming error-free communication ($P=1$). For a source with $N$ equally likely symbols, the Shannon entropy (information content per symbol) is $H = \\log_2(N)$.\nThe raw bitrate is the information per symbol multiplied by the number of symbols selected per minute.\n$$\n\\text{RBUB} = \\log_2(N) \\times \\frac{60}{T}\n$$\n\n**3. Information Transfer Rate (ITR)**\nThe ITR quantifies the actual amount of information successfully transmitted through the noisy channel. It is defined as the mutual information between the intended symbol and the classifier's output, expressed per unit of time.\n\nLet $X$ be the random variable representing the symbol intended by the user, and $Y$ be the random variable for the symbol selected by the BCI classifier. Both $X$ and $Y$ can take any of the $N$ possible symbol values.\n\nThe mutual information per trial, $I(X;Y)$, is given by:\n$$\nI(X;Y) = H(Y) - H(Y|X)\n$$\nwhere $H(Y)$ is the entropy of the output distribution and $H(Y|X)$ is the conditional entropy of the output given the input.\n\nDue to the uniform prior on the intended symbols ($p(x) = \\frac{1}{N}$) and the symmetry of the error model, the output distribution $p(y)$ is also uniform. We can verify this:\n$$\np(y) = \\sum_{x} p(y|x)p(x) = p(y|x=y)p(x=y) + \\sum_{x \\neq y} p(y|x \\neq y)p(x \\neq y)\n$$\n$$\np(y) = P \\cdot \\frac{1}{N} + (N-1) \\cdot \\left(\\frac{1-P}{N-1}\\right) \\cdot \\frac{1}{N} = \\frac{P}{N} + \\frac{1-P}{N} = \\frac{1}{N}\n$$\nSince the output distribution is uniform, its entropy is:\n$$\nH(Y) = -\\sum_{y} p(y)\\log_2(p(y)) = -\\sum_{i=1}^N \\frac{1}{N}\\log_2\\left(\\frac{1}{N}\\right) = \\log_2(N)\n$$\n\nNext, we find the conditional entropy $H(Y|X)$, which represents the uncertainty in the output given the input is known.\n$$\nH(Y|X) = -\\sum_{x,y} p(x,y) \\log_2(p(y|x)) = -\\sum_{x} p(x) \\sum_{y} p(y|x)\\log_2(p(y|x))\n$$\nDue to the channel's symmetry, the inner sum is the same for all $x$. For any given input $x$, the probability of a correct output is $p(y=x|x) = P$, and the probability of any specific incorrect output $y \\neq x$ is $p(y \\neq x|x) = \\frac{1-P}{N-1}$.\nThe entropy for a given $x$ is:\n$$\nH(Y|X=x) = -p(y=x|x)\\log_2(p(y=x|x)) - \\sum_{y \\neq x} p(y \\neq x|x)\\log_2(p(y \\neq x|x))\n$$\n$$\nH(Y|X=x) = -P\\log_2(P) - (N-1)\\left(\\frac{1-P}{N-1}\\right)\\log_2\\left(\\frac{1-P}{N-1}\\right)\n$$\n$$\nH(Y|X=x) = -P\\log_2(P) - (1-P)\\log_2\\left(\\frac{1-P}{N-1}\\right)\n$$\nSince this is constant for all $x$, $H(Y|X) = H(Y|X=x)$.\n\nNow we can compute the mutual information per trial, often denoted as $B$:\n$$\nB = I(X;Y) = H(Y) - H(Y|X) = \\log_2(N) - \\left[-P\\log_2(P) - (1-P)\\log_2\\left(\\frac{1-P}{N-1}\\right)\\right]\n$$\n$$\nB = \\log_2(N) + P\\log_2(P) + (1-P)\\log_2\\left(\\frac{1-P}{N-1}\\right)\n$$\nThis expression gives the information transferred in bits per trial. The ITR is this quantity multiplied by the number of trials per minute:\n$$\n\\text{ITR} = \\left[ \\log_2(N) + P\\log_2(P) + (1-P)\\log_2\\left(\\frac{1-P}{N-1}\\right) \\right] \\times \\frac{60}{T}\n$$\n\n### Numerical Computations\nGiven the values $N=36$, $T=4\\,\\mathrm{s}$, and $P=0.85$.\nThe number of trials per minute is $\\frac{60}{4} = 15$.\n\n**1. Symbol Throughput (ST)**\n$$\n\\text{ST} = 0.85 \\times 15 = 12.75 \\text{ symbols/min}\n$$\n\n**2. Raw Bitrate Upper Bound (RBUB)**\n$$\n\\text{RBUB} = \\log_2(36) \\times 15 = \\frac{\\ln(36)}{\\ln(2)} \\times 15 \\approx 5.1699 \\times 15 \\approx 77.55 \\text{ bits/min}\n$$\n\n**3. Information Transfer Rate (ITR)**\nFirst, we compute the information per trial, $B$:\n$$\nB = \\log_2(36) + 0.85\\log_2(0.85) + (1-0.85)\\log_2\\left(\\frac{1-0.85}{36-1}\\right)\n$$\n$$\nB = \\log_2(36) + 0.85\\log_2(0.85) + 0.15\\log_2\\left(\\frac{0.15}{35}\\right)\n$$\nWe evaluate the terms:\n- $\\log_2(36) \\approx 5.169925$\n- $0.85\\log_2(0.85) = 0.85 \\times \\frac{\\ln(0.85)}{\\ln(2)} \\approx 0.85 \\times (-0.234453) \\approx -0.199285$\n- $0.15\\log_2\\left(\\frac{0.15}{35}\\right) = 0.15 \\times \\frac{\\ln(0.15/35)}{\\ln(2)} \\approx 0.15 \\times (-7.86613) \\approx -1.179920$\nSumming these terms to get bits per trial:\n$$\nB \\approx 5.169925 - 0.199285 - 1.179920 \\approx 3.79072 \\text{ bits/trial}\n$$\nFinally, we compute the ITR in bits per minute:\n$$\n\\text{ITR} = B \\times 15 \\approx 3.79072 \\times 15 \\approx 56.8608 \\text{ bits/min}\n$$\nRounding the final answer for the ITR to four significant figures gives $56.86$ bits/min.",
            "answer": "$$\n\\boxed{56.86}\n$$"
        }
    ]
}