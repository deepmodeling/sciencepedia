{
    "hands_on_practices": [
        {
            "introduction": "To understand how the cerebellum processes information, we must first appreciate the scale of synaptic integration within its principal neurons. This exercise provides a foundational look at how a Purkinje cell sums its vast array of inputs from parallel fibers to modulate its output firing rate. By performing this calculation , you will develop a quantitative intuition for the degree of synaptic cooperativity required to drive a meaningful change in neuronal activity, a cornerstone of cerebellar computation.",
            "id": "5005921",
            "problem": "A cerebellar Purkinje cell (PC) receives approximately $10^{5}$ parallel fiber (PF) synapses. Each PF synapse produces a unitary excitatory postsynaptic potential (EPSP) of amplitude $0.05\\,\\text{mV}$ at the spine head. Because of passive dendritic attenuation, only a fixed fraction $\\alpha=0.1$ of a spine-head depolarization reaches the soma. Consider a synchronous volley of $M=1000$ PF synapses distributed across the dendritic tree. Assume that (i) the compound depolarization remains sufficiently small that membrane conductances do not saturate, (ii) summation at the soma is linear, and (iii) dendritic attenuation is well approximated by a constant multiplicative factor $\\alpha$ for these inputs.\n\nPurkinje cell simple spike (SS) firing near baseline can be approximated by a locally linear dependence on somatic membrane potential: a small change in somatic potential $\\Delta V$ produces a change in firing rate $\\Delta r$ given by $\\Delta r = g\\,\\Delta V$, where $g$ is the local gain. Suppose the baseline firing rate is $r_{0}=75\\,\\text{Hz}$ and the gain is $g=12\\,\\text{Hz}/\\text{mV}$.\n\nUsing only the assumptions above and first principles of linear superposition and passive attenuation, compute the predicted instantaneous firing rate during the synchronous PF volley. Express your final firing rate in $\\text{Hz}$ and round your answer to three significant figures.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- Total number of parallel fiber (PF) synapses on a Purkinje cell (PC): approximately $10^{5}$.\n- Unitary excitatory postsynaptic potential (EPSP) amplitude at the spine head: $V_{\\text{EPSP, spine}} = 0.05\\,\\text{mV}$.\n- Dendritic attenuation factor: $\\alpha = 0.1$.\n- Number of synchronously firing PF synapses in a volley: $M = 1000$.\n- Assumption (i): The compound depolarization is small enough to prevent saturation of membrane conductances.\n- Assumption (ii): Summation of potentials at the soma is linear.\n- Assumption (iii): Dendritic attenuation is approximated by the constant multiplicative factor $\\alpha$.\n- Baseline simple spike (SS) firing rate: $r_{0} = 75\\,\\text{Hz}$.\n- Local gain of the firing rate with respect to somatic membrane potential: $g = 12\\,\\text{Hz}/\\text{mV}$.\n- Relationship between change in somatic potential ($\\Delta V$) and change in firing rate ($\\Delta r$): $\\Delta r = g\\,\\Delta V$.\n- Task: Compute the predicted instantaneous firing rate during the synchronous PF volley. The final answer must be in $\\text{Hz}$ and rounded to three significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity:\n- **Scientifically Grounded:** Yes. The problem describes a simplified but standard model of synaptic integration and firing rate modulation in a Purkinje cell. The values for synaptic potentials, number of inputs, firing rates, and gain are all within physiologically realistic ranges. The assumptions of linear summation and constant attenuation are common simplifications used in introductory computational neuroscience to make problems tractable.\n- **Well-Posed:** Yes. The problem provides all necessary data and a clear set of assumptions required to arrive at a unique, meaningful solution. The question is unambiguous.\n- **Objective:** Yes. The problem is stated using precise, quantitative, and unbiased language.\n- **Completeness and Consistency:** The problem is self-contained. The provided data ($V_{\\text{EPSP, spine}}$, $\\alpha$, $M$, $r_{0}$, $g$) and assumptions (linear summation, constant attenuation) are sufficient and consistent. The figure of $10^{5}$ total synapses is contextual information and does not contradict the volley size of $M=1000$.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, and objective. A solution will be derived.\n\nThe objective is to calculate the final instantaneous firing rate, $r_{\\text{final}}$, which is the sum of the baseline rate, $r_{0}$, and the change in firing rate, $\\Delta r$, induced by the synaptic volley.\n$$r_{\\text{final}} = r_{0} + \\Delta r$$\nThe change in firing rate, $\\Delta r$, is linearly related to the change in somatic membrane potential, $\\Delta V$, by the gain, $g$:\n$$\\Delta r = g\\,\\Delta V$$\nTo find $\\Delta r$, we must first determine the total somatic depolarization, $\\Delta V$.\n\nA single parallel fiber synapse generates an EPSP of amplitude $V_{\\text{EPSP, spine}}$ at the dendritic spine head. This potential is attenuated as it passively propagates to the soma. The fraction of the potential that reaches the soma is given by the attenuation factor $\\alpha$. Therefore, the contribution of a single synapse to the somatic potential, $\\Delta V_{\\text{single}}$, is:\n$$\\Delta V_{\\text{single}} = \\alpha \\cdot V_{\\text{EPSP, spine}}$$\nThe problem states that a volley of $M$ synapses fires synchronously. A core assumption is that the resulting potentials sum linearly at the soma. Thus, the total change in somatic membrane potential, $\\Delta V$, is the sum of the contributions from all $M$ active synapses:\n$$\\Delta V = \\sum_{i=1}^{M} \\Delta V_{\\text{single}} = M \\cdot \\Delta V_{\\text{single}}$$\nSubstituting the expression for $\\Delta V_{\\text{single}}$, we get:\n$$\\Delta V = M \\cdot \\alpha \\cdot V_{\\text{EPSP, spine}}$$\nNow we can substitute the given numerical values:\n- $M = 1000$\n- $\\alpha = 0.1$\n- $V_{\\text{EPSP, spine}} = 0.05\\,\\text{mV}$\n\n$$\\Delta V = 1000 \\cdot 0.1 \\cdot 0.05\\,\\text{mV} = 100 \\cdot 0.05\\,\\text{mV} = 5\\,\\text{mV}$$\nThe synchronous volley of $1000$ PF synapses produces a total depolarization of $5\\,\\text{mV}$ at the soma.\n\nNext, we calculate the change in firing rate, $\\Delta r$, resulting from this depolarization using the given gain, $g = 12\\,\\text{Hz}/\\text{mV}$:\n$$\\Delta r = g\\,\\Delta V = (12\\,\\text{Hz}/\\text{mV}) \\cdot (5\\,\\text{mV}) = 60\\,\\text{Hz}$$\nThe firing rate increases by $60\\,\\text{Hz}$ from its baseline level.\n\nFinally, we compute the new instantaneous firing rate, $r_{\\text{final}}$, by adding this change to the baseline rate, $r_{0} = 75\\,\\text{Hz}$:\n$$r_{\\text{final}} = r_{0} + \\Delta r = 75\\,\\text{Hz} + 60\\,\\text{Hz} = 135\\,\\text{Hz}$$\nThe predicted instantaneous firing rate during the volley is $135\\,\\text{Hz}$. This value is already presented with three significant figures, as requested by the problem.",
            "answer": "$$\\boxed{135}$$"
        },
        {
            "introduction": "Motor learning in the cerebellum is not static; it relies on the ability to modify synaptic strengths based on performance errors. This practice delves into the core mechanism of cerebellar plasticity: long-term depression (LTD) at parallel fiber-to-Purkinje cell synapses. By exploring a mathematical model of the temporal learning window , you will quantify how the precise timing between inputs dictates the magnitude of synaptic change, a principle essential for error-driven adaptation.",
            "id": "5005981",
            "problem": "In a simplified theoretical model of cerebellar parallel fiber (PF) to Purkinje cell synaptic plasticity, conjunctive activation of a PF spike and a climbing fiber (CF)-evoked complex spike (CS) induces long-term depression (LTD) at the PF synapse. The LTD amplitude per PF spike is taken to be proportional to an eligibility kernel $K(\\Delta t)$ that depends on the time difference $\\Delta t$ between the PF spike and the CS, where the CS occurs at time $t=0$ and $\\Delta t$ is defined as $\\Delta t = t_{\\mathrm{PF}} - t_{\\mathrm{CS}}$. Assume a symmetric, decaying kernel\n$$\nK(\\Delta t) = \\exp\\!\\left(-\\frac{|\\Delta t|}{\\tau}\\right),\n$$\nwith time constant $\\tau = 50\\,\\text{ms}$. This kernel is dimensionless and encodes the temporal specificity of LTD induction by weighting PF spikes according to their temporal proximity to the CS.\n\nUsing only the definition of $K(\\Delta t)$ and the interpretation that the LTD magnitude per PF spike is proportional to $K(\\Delta t)$, compute the relative LTD magnitude for PF spikes occurring at $\\Delta t = 10\\,\\text{ms}$ versus $\\Delta t = 100\\,\\text{ms}$ by forming the ratio\n$$\nr \\equiv \\frac{K(10\\,\\text{ms})}{K(100\\,\\text{ms})}.\n$$\nProvide your final result as a single closed-form analytic expression with no numerical approximation. Then, based on the value of $r$ and the given $\\tau$, briefly interpret the temporal specificity of learning in one sentence. The requested final answer is the expression for $r$ only. The ratio $r$ is dimensionless, so no physical units are required in the final answer.",
            "solution": "The problem statement is first validated for scientific soundness, completeness, and objectivity.\n\n**Step 1: Extract Givens**\n-   **Model**: A simplified theoretical model of cerebellar long-term depression (LTD) at the parallel fiber (PF) to Purkinje cell synapse.\n-   **Mechanism**: LTD is induced by conjunctive activation of a PF spike and a climbing fiber (CF)-evoked complex spike (CS).\n-   **Time references**: The CS occurs at time $t_{\\mathrm{CS}} = 0$. The time difference is $\\Delta t = t_{\\mathrm{PF}} - t_{\\mathrm{CS}}$.\n-   **Eligibility Kernel**: The LTD amplitude per PF spike is proportional to an eligibility kernel $K(\\Delta t)$. The functional form is given as $K(\\Delta t) = \\exp\\left(-\\frac{|\\Delta t|}{\\tau}\\right)$.\n-   **Time Constant**: $\\tau = 50\\,\\text{ms}$.\n-   **Task**: Compute the ratio $r \\equiv \\frac{K(10\\,\\text{ms})}{K(100\\,\\text{ms})}$.\n-   **Answer Format**: The final result for $r$ must be a single closed-form analytic expression with no numerical approximation. A brief interpretation is also requested as part of the reasoning.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed against the required validation criteria.\n-   **Scientifically Grounded**: The problem describes a canonical model of spike-timing-dependent plasticity (STDP) at a specific, well-studied synapse. The Marr-Albus-Ito theory of cerebellar learning posits that conjunctive CF and PF activity drives LTD, and computational models frequently employ an exponential eligibility kernel to formalize the temporal window for this plasticity. The time constant $\\tau = 50\\,\\text{ms}$ is biophysically plausible for such synaptic processes. The model, while simplified, is a cornerstone of theoretical neuroscience and does not violate any scientific principles.\n-   **Well-Posed**: The problem is well-posed. The function $K(\\Delta t)$ is explicitly defined, all necessary parameters ($\\tau$, and the two values of $\\Delta t$) are provided, and the task is a direct, unambiguous calculation. A unique solution exists.\n-   **Objective**: The problem is stated in precise, quantitative, and unbiased language.\n-   **Completeness and Consistency**: The problem is self-contained and internally consistent. No information is missing or contradictory.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. It is scientifically sound, well-posed, and complete. A solution will be derived as requested.\n\nThe task is to compute the ratio $r$, defined as the LTD magnitude for a PF spike at $\\Delta t_1 = 10\\,\\text{ms}$ relative to that of a PF spike at $\\Delta t_2 = 100\\,\\text{ms}$. The LTD magnitude is proportional to the eligibility kernel $K(\\Delta t)$, so the ratio is given by:\n$$\nr = \\frac{K(\\Delta t_1)}{K(\\Delta t_2)} = \\frac{K(10\\,\\text{ms})}{K(100\\,\\text{ms})}\n$$\nThe eligibility kernel is defined as:\n$$\nK(\\Delta t) = \\exp\\left(-\\frac{|\\Delta t|}{\\tau}\\right)\n$$\nwith the time constant $\\tau = 50\\,\\text{ms}$.\n\nFirst, we evaluate the numerator, $K(10\\,\\text{ms})$. We substitute $\\Delta t = 10\\,\\text{ms}$ and $\\tau = 50\\,\\text{ms}$ into the expression for the kernel.\n$$\nK(10\\,\\text{ms}) = \\exp\\left(-\\frac{|10\\,\\text{ms}|}{50\\,\\text{ms}}\\right) = \\exp\\left(-\\frac{10}{50}\\right) = \\exp\\left(-\\frac{1}{5}\\right)\n$$\nNote that the units of milliseconds in the numerator and denominator of the exponent's argument cancel, rendering the argument dimensionless, as required for an exponent.\n\nNext, we evaluate the denominator, $K(100\\,\\text{ms})$. We substitute $\\Delta t = 100\\,\\text{ms}$ and $\\tau = 50\\,\\text{ms}$.\n$$\nK(100\\,\\text{ms}) = \\exp\\left(-\\frac{|100\\,\\text{ms}|}{50\\,\\text{ms}}\\right) = \\exp\\left(-\\frac{100}{50}\\right) = \\exp(-2)\n$$\n\nNow, we compute the ratio $r$ by dividing the two results.\n$$\nr = \\frac{K(10\\,\\text{ms})}{K(100\\,\\text{ms})} = \\frac{\\exp\\left(-\\frac{1}{5}\\right)}{\\exp(-2)}\n$$\nUsing the property of exponential functions $\\frac{\\exp(a)}{\\exp(b)} = \\exp(a-b)$, we can simplify the expression for $r$.\n$$\nr = \\exp\\left(-\\frac{1}{5} - (-2)\\right) = \\exp\\left(2 - \\frac{1}{5}\\right)\n$$\nTo combine the terms in the exponent, we find a common denominator.\n$$\n2 - \\frac{1}{5} = \\frac{10}{5} - \\frac{1}{5} = \\frac{9}{5}\n$$\nTherefore, the final expression for the ratio $r$ is:\n$$\nr = \\exp\\left(\\frac{9}{5}\\right)\n$$\nThis is the required closed-form analytic expression.\n\nFor the requested interpretation, the value of $r = \\exp(9/5) = \\exp(1.8) \\approx 6.05$. The resulting ratio $r=\\exp(9/5)$, which is significantly greater than $1$, indicates that the LTD induction mechanism is highly sensitive to the timing of presynaptic spikes, strongly favoring those that are nearly coincident with the postsynaptic complex spike.",
            "answer": "$$\n\\boxed{\\exp\\left(\\frac{9}{5}\\right)}\n$$"
        },
        {
            "introduction": "Bridging the gap between cellular mechanisms and observable behavior is a central challenge in neuroscience. This problem guides you through the construction of a state-space model, a powerful theoretical tool for understanding motor adaptation over time. By modeling learning as a combination of fast and slow processes , you will see how this framework can elegantly explain complex behavioral phenomena, such as multi-timescale learning and the spontaneous recovery of memory.",
            "id": "5005892",
            "problem": "A primate performs a vestibulo-ocular reflex (VOR) adaptation experiment in which a constant visual-vestibular mismatch is imposed over many trials, generating a retinal slip error on trial $n$ denoted $e_n$ that is relayed by climbing fibers to cerebellar Purkinje cells. The behavioral VOR gain on trial $n$ is $g_n$, defined as eye velocity divided by head velocity (target gain is $g^\\ast$). Consider these fundamental bases:\n\n- The Marr–Albus–Ito hypothesis: error-related climbing fiber activity drives plasticity at parallel fiber–Purkinje cell synapses, altering Purkinje cell output, which in turn changes vestibular nucleus motor commands and thus the VOR gain.\n- Adaptation is error-driven: changes in the neural substrate on trial $n$ depend on the error $e_n$.\n- Cerebellar memory is not unitary: behavioral data show at least two timescales of learning and forgetting, suggesting a fast but labile component and a slow but stable component.\n- Retention reflects passive decay or consolidation between trials: an internal state $x$ retains a fraction $\\rho$ of its value from one trial to the next, with $0<\\rho<1$.\n\nUsing only these bases, do the following:\n\n- Introduce two internal adaptive states, a fast component $x_f$ and a slow component $x_s$, both contributing additively to the observed gain $g_n$.\n- Assume state-dependent retention factors $\\rho_f$ and $\\rho_s$ with $0<\\rho_f<\\rho_s<1$, and error sensitivities (learning rates) $\\alpha$ and $\\beta$ with $\\alpha>\\beta>0$.\n- Derive a discrete-time, linear state-space model that updates $x_f$ and $x_s$ from trial $n$ to $n+1$ based only on $e_n$ and retention, and that yields $g_n$ as the sum of the states.\n- From this model, derive the trial-by-trial change in gain $\\Delta g_{n+1}=g_{n+1}-g_n$ as a function of $x_f(n)$, $x_s(n)$, $\\rho_f$, $\\rho_s$, and $e_n$, and identify the coefficient linking $e_n$ to $\\Delta g_{n+1}$.\n- Using the model, state two qualitative behavioral predictions for a constant-step error schedule: the shape of the learning curve across trials and a signature observed after brief counter-adaptation (e.g., spontaneous recovery or savings), and rationalize them from the model’s structure.\n\nWhich option below correctly specifies the model and its trial-by-trial implication, and draws the correct qualitative predictions?\n\nA. Define $g_n=x_f(n)+x_s(n)$ with updates:\n$$x_f(n+1)=\\rho_f x_f(n)+\\alpha e_n,\\quad x_s(n+1)=\\rho_s x_s(n)+\\beta e_n,$$\nwhere $0<\\rho_f<\\rho_s<1$ and $\\alpha>\\beta>0$. Then\n$$\\Delta g_{n+1}=(\\rho_f-1)x_f(n)+(\\rho_s-1)x_s(n)+(\\alpha+\\beta)e_n,$$\nso the coefficient of $e_n$ in $\\Delta g_{n+1}$ is $\\alpha+\\beta$. The model predicts a double-exponential approach to asymptote under a constant error, and after brief counter-adaptation, spontaneous recovery due to the slow state’s persistence.\n\nB. Define a single state $g_n$ with\n$$g_{n+1}=\\rho\\, g_n+(\\alpha+\\beta)e_n,$$\nso two learning rates generate two timescales. Then\n$$\\Delta g_{n+1}=(\\rho-1)g_n+(\\alpha+\\beta)e_n.$$\nThe model predicts a single-exponential learning curve and no spontaneous recovery after counter-adaptation.\n\nC. Define $g_n=x_f(n)+x_s(n)$ with\n$$x_f(n+1)=x_f(n)+\\rho_f \\alpha e_n,\\quad x_s(n+1)=x_s(n)+\\rho_s \\beta e_n,$$\nso retention acts on the error input rather than the state. Then\n$$\\Delta g_{n+1}=\\rho_f \\alpha e_n+\\rho_s \\beta e_n.$$\nThis predicts that retention accelerates learning but does not affect forgetting, and that counter-adaptation produces no spontaneous recovery.\n\nD. Define $g_n=x_f(n)+x_s(n)$ with\n$$x_f(n+1)=\\rho_f x_f(n)+\\beta e_n,\\quad x_s(n+1)=\\rho_s x_s(n)+\\alpha e_n,$$\nwhere $\\rho_f>\\rho_s$ and $\\alpha>\\beta>0$. Then\n$$\\Delta g_{n+1}=(\\rho_f-1)x_f(n)+(\\rho_s-1)x_s(n)+(\\alpha+\\beta)e_n,$$\nso the coefficient of $e_n$ in $\\Delta g_{n+1}$ equals $\\max\\{\\alpha,\\beta\\}$. The model predicts a single-exponential curve because the fast state has higher retention.\n\nChoose the single best option.",
            "solution": "The problem statement is scrutinized for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n\nThe problem provides the following information and constraints for building a model of vestibulo-ocular reflex (VOR) adaptation:\n1.  **System Variables:**\n    -   $e_n$: Retinal slip error on trial $n$.\n    -   $g_n$: VOR gain on trial $n$.\n    -   $g^\\ast$: Target VOR gain.\n2.  **Fundamental Bases (Modeling Principles):**\n    -   **Marr–Albus–Ito hypothesis:** Climbing fiber error signals ($e_n$) drive synaptic plasticity, which alters motor commands and VOR gain ($g_n$).\n    -   **Error-driven adaptation:** Changes on trial $n$ depend on the error $e_n$.\n    -   **Multiple memory components:** At least two timescales exist: a fast/labile component and a slow/stable component.\n    -   **Retention mechanism:** An internal state $x$ retains a fraction $\\rho$ of its value from one trial to the next, where $0 < \\rho < 1$. This represents passive decay/consolidation.\n3.  **Modeling Tasks:**\n    -   Introduce two additive states: a fast component $x_f$ and a slow component $x_s$, such that $g_n = x_f(n) + x_s(n)$.\n    -   Use retention factors $\\rho_f$ and $\\rho_s$ with the constraint $0 < \\rho_f < \\rho_s < 1$.\n    -   Use error sensitivities (learning rates) $\\alpha$ and $\\beta$ with the constraint $\\alpha > \\beta > 0$.\n    -   Derive a discrete-time, linear state-space model for the trial-to-trial updates of $x_f$ and $x_s$.\n    -   Derive the trial-by-trial change in gain, $\\Delta g_{n+1} = g_{n+1} - g_n$.\n    -   Identify the coefficient of $e_n$ in the expression for $\\Delta g_{n+1}$.\n    -   State two qualitative predictions: the shape of the learning curve and a signature after brief counter-adaptation.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientific Grounding:** The problem is firmly rooted in computational neuroscience and motor control. The Marr–Albus–Ito hypothesis is a central theory of cerebellar function. The two-state model of motor adaptation is a standard and empirically supported framework (e.g., Smith, Shadmehr, et al., 2006) used to explain phenomena like savings and spontaneous recovery. The VOR is a classic experimental paradigm for these studies. The problem is scientifically sound.\n-   **Well-Posed:** The problem provides a clear set of constraints and definitions to construct a specific mathematical model. The terms \"fast\" and \"slow\" are quantitatively defined by the constraints on the retention factors and learning rates. \"Fast\" implies faster decay (lower retention, $\\rho_f$) and faster learning (higher rate, $\\alpha$). \"Slow\" implies slower decay (higher retention, $\\rho_s$) and slower learning (lower rate, $\\beta$). The constraints $0 < \\rho_f < \\rho_s < 1$ and $\\alpha > \\beta > 0$ are consistent with these definitions. The tasks are all mathematically tractable derivations from the model.\n-   **Objective:** The language is technical, precise, and free of subjectivity.\n-   **Consistency and Completeness:** The problem is self-contained and its constraints are mutually consistent. All information required to derive the model and its properties is provided.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is a well-posed, scientifically grounded problem in theoretical neuroscience. The solution process will now proceed.\n\n### Derivation of the Model and its Implications\n\n1.  **Model Formulation:**\n    Based on the provided principles, we construct a linear state-space model.\n    -   The total gain is the sum of a fast state $x_f$ and a slow state $x_s$:\n        $$g_n = x_f(n) + x_s(n)$$\n    -   The update for each state from trial $n$ to $n+1$ consists of two parts: a retention of the current state and an update based on the error $e_n$.\n    -   The retention for a state $x$ is given as $\\rho x$. The error-driven update is linear in $e_n$, taking the form of $(\\text{learning rate}) \\times e_n$.\n    -   The fast state $x_f$ is \"fast but labile,\" meaning it learns quickly (high learning rate) and forgets quickly (low retention). Thus, it is associated with learning rate $\\alpha$ and retention factor $\\rho_f$.\n    -   The slow state $x_s$ is \"slow but stable,\" meaning it learns slowly (low learning rate) and forgets slowly (high retention). It is associated with learning rate $\\beta$ and retention factor $\\rho_s$.\n    -   The constraints are $0 < \\rho_f < \\rho_s < 1$ (fast state forgets more) and $\\alpha > \\beta > 0$ (fast state learns more per trial).\n    -   This yields the following system of equations:\n        $$x_f(n+1) = \\rho_f x_f(n) + \\alpha e_n$$\n        $$x_s(n+1) = \\rho_s x_s(n) + \\beta e_n$$\n\n2.  **Derivation of Gain Change ($\\Delta g_{n+1}$):**\n    The change in gain from trial $n$ to trial $n+1$ is $\\Delta g_{n+1} = g_{n+1} - g_n$.\n    -   First, express $g_{n+1}$ in terms of the states at trial $n$:\n        $$g_{n+1} = x_f(n+1) + x_s(n+1) = (\\rho_f x_f(n) + \\alpha e_n) + (\\rho_s x_s(n) + \\beta e_n)$$\n    -   Now, compute the difference:\n        $$\\Delta g_{n+1} = g_{n+1} - g_n$$\n        $$\\Delta g_{n+1} = [(\\rho_f x_f(n) + \\alpha e_n) + (\\rho_s x_s(n) + \\beta e_n)] - [x_f(n) + x_s(n)]$$\n    -   Group terms by $x_f(n)$, $x_s(n)$, and $e_n$:\n        $$\\Delta g_{n+1} = (\\rho_f x_f(n) - x_f(n)) + (\\rho_s x_s(n) - x_s(n)) + (\\alpha e_n + \\beta e_n)$$\n        $$\\Delta g_{n+1} = (\\rho_f - 1)x_f(n) + (\\rho_s - 1)x_s(n) + (\\alpha + \\beta)e_n$$\n    -   From this expression, the coefficient linking the error $e_n$ to the change in gain $\\Delta g_{n+1}$ is clearly $(\\alpha + \\beta)$.\n\n3.  **Qualitative Predictions:**\n    -   **Learning Curve Shape:** For a constant error schedule, $e_n = E$, the system is a linear, time-invariant system driven by a constant input. The solution for each state is the sum of a homogeneous solution (an exponential decay) and a particular solution (a constant asymptote).\n        -   $x_f(n)$ approaches its asymptote $x_{f,ss} = \\frac{\\alpha E}{1-\\rho_f}$ with dynamics governed by $\\rho_f^n$.\n        -   $x_s(n)$ approaches its asymptote $x_{s,ss} = \\frac{\\beta E}{1-\\rho_s}$ with dynamics governed by $\\rho_s^n$.\n        -   Since $g_n = x_f(n) + x_s(n)$, the total gain $g_n$ is the sum of two exponential functions with different time constants (related to $\\rho_f$ and $\\rho_s$). This produces a **double-exponential** learning curve. The initial phase is fast, dominated by $x_f$, and the later phase is slow, dominated by $x_s$.\n    -   **Signature after Counter-Adaptation:**\n        -   **Adaptation:** Let the system adapt to a positive error $E > 0$. Both $x_f$ and $x_s$ will become positive.\n        -   **Counter-Adaptation:** Introduce a brief, opposing error (e.g., $e_n = -E'$, where $E' > 0$). Because $\\alpha > \\beta$, the fast state $x_f$ will be strongly driven in the negative direction, potentially becoming negative itself. The slow state $x_s$, with its small learning rate $\\beta$, will change very little and remain positive. The overall gain $g_n = x_f(n) + x_s(n)$ might be driven back to baseline or even become negative (over-compensation).\n        -   **Post-Counter-Adaptation (Washout):** Set the error to zero, $e_n = 0$. The dynamics become $x_f(n+1) = \\rho_f x_f(n)$ and $x_s(n+1) = \\rho_s x_s(n)$. The negative value of $x_f$ will decay to zero very quickly (since $\\rho_f$ is small, corresponding to rapid forgetting). The positive value of the slow state $x_s$ will decay very slowly (since $\\rho_s$ is close to $1$). The total gain $g_n = x_f(n) + x_s(n)$ will therefore increase from its post-counter-adaptation value back towards a positive value, \"unmasking\" the persistent memory in the slow state. This effect, where the gain appears to revert towards its previously learned state without any error driving it, is known as **spontaneous recovery**.\n\n### Option-by-Option Analysis\n\n**A. Define $g_n=x_f(n)+x_s(n)$ with updates: $x_f(n+1)=\\rho_f x_f(n)+\\alpha e_n,\\quad x_s(n+1)=\\rho_s x_s(n)+\\beta e_n,$ where $0<\\rho_f<\\rho_s<1$ and $\\alpha>\\beta>0$. Then $\\Delta g_{n+1}=(\\rho_f-1)x_f(n)+(\\rho_s-1)x_s(n)+(\\alpha+\\beta)e_n,$ so the coefficient of $e_n$ in $\\Delta g_{n+1}$ is $\\alpha+\\beta$. The model predicts a double-exponential approach to asymptote under a constant error, and after brief counter-adaptation, spontaneous recovery due to the slow state’s persistence.**\n\n-   **Model:** This model correctly implements the principles. The fast state $x_f$ has the faster learning rate $\\alpha$ and faster forgetting rate ($1-\\rho_f$). The slow state $x_s$ has the slower learning rate $\\beta$ and slower forgetting rate ($1-\\rho_s$).\n-   **$\\Delta g_{n+1}$ Derivation:** The expression for $\\Delta g_{n+1}$ and the identification of the coefficient of $e_n$ as $\\alpha+\\beta$ are correct, as shown in the derivation above.\n-   **Predictions:** The predictions of a double-exponential learning curve and spontaneous recovery are the correct hallmarks of this two-state model.\n-   **Verdict:** **Correct**.\n\n**B. Define a single state $g_n$ with $g_{n+1}=\\rho\\, g_n+(\\alpha+\\beta)e_n,$ so two learning rates generate two timescales. Then $\\Delta g_{n+1}=(\\rho-1)g_n+(\\alpha+\\beta)e_n.$ The model predicts a single-exponential learning curve and no spontaneous recovery after counter-adaptation.**\n\n-   **Model:** This is a single-state model. It violates the explicit instruction to use two internal states based on the \"Cerebellar memory is not unitary\" principle. The statement \"two learning rates generate two timescales\" is nonsensical in the context of this single-state equation.\n-   **Verdict:** **Incorrect**.\n\n**C. Define $g_n=x_f(n)+x_s(n)$ with $x_f(n+1)=x_f(n)+\\rho_f \\alpha e_n,\\quad x_s(n+1)=x_s(n)+\\rho_s \\beta e_n,$ so retention acts on the error input rather than the state. Then $\\Delta g_{n+1}=\\rho_f \\alpha e_n+\\rho_s \\beta e_n.$ This predicts that retention accelerates learning but does not affect forgetting, and that counter-adaptation produces no spontaneous recovery.**\n\n-   **Model:** This model form is incorrect. It models the passive decay/retention mechanism improperly. The update equations $x(n+1)=x(n)+...$ imply perfect memory of the state ($x(n)$ is passed to $x(n+1)$ with a coefficient of $1$), which contradicts the principle that \"an internal state $x$ retains a fraction $\\rho$ of its value from one trial to the next, with $0<\\rho<1$\". Because there is no passive decay, spontaneous recovery is impossible, as the option correctly notes for its own flawed model.\n-   **Verdict:** **Incorrect**.\n\n**D. Define $g_n=x_f(n)+x_s(n)$ with $x_f(n+1)=\\rho_f x_f(n)+\\beta e_n,\\quad x_s(n+1)=\\rho_s x_s(n)+\\alpha e_n,$ where $\\rho_f>\\rho_s$ and $\\alpha>\\beta>0$. Then $\\Delta g_{n+1}=(\\rho_f-1)x_f(n)+(\\rho_s-1)x_s(n)+(\\alpha+\\beta)e_n,$ so the coefficient of $e_n$ in $\\Delta g_{n+1}$ equals $\\max\\{\\alpha,\\beta\\}$. The model predicts a single-exponential curve because the fast state has higher retention.**\n\n-   **Model:** This option has multiple errors.\n    1.  It misassigns the learning rates: the fast state $x_f$ is given the slow rate $\\beta$, and the slow state $x_s$ is given the fast rate $\\alpha$. This is inconsistent with the labels \"fast\" and \"slow\".\n    2.  It states $\\rho_f > \\rho_s$, which contradicts the problem's explicit instruction to use $0 < \\rho_f < \\rho_s < 1$. This condition would mean the \"fast\" state forgets more slowly than the \"slow\" state.\n    3.  The coefficient of $e_n$ in $\\Delta g_{n+1}$ is $\\alpha+\\beta$, not $\\max\\{\\alpha,\\beta\\}$.\n    4.  A two-state model, regardless of parameter assignment, will produce a double-exponential curve, not a single-exponential one.\n-   **Verdict:** **Incorrect**.\n\nBased on the rigorous derivation and option analysis, Option A is the only one that correctly formulates the model according to the given principles, correctly derives its mathematical properties, and correctly states its key behavioral predictions.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}