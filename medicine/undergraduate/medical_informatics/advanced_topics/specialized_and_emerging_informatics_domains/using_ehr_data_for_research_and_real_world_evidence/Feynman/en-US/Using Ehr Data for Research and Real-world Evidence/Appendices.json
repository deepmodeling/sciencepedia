{
    "hands_on_practices": [
        {
            "introduction": "Electronic Health Record (EHR) data often aggregates information from diverse sources, leading to inconsistencies such as different units for the same laboratory measurement. This exercise simulates a crucial data cleaning task known as harmonization. By converting all serum creatinine measurements to a standard unit before applying the official CKD-EPI formula for kidney function, you will directly quantify how failure to harmonize data can lead to profoundly incorrect clinical assessments .",
            "id": "4862785",
            "problem": "Electronic Health Record (EHR) data frequently contain laboratory measurements recorded in different units across time, sites, or instruments. To generate real-world evidence about kidney function, these measurements must be harmonized to a common unit before applying clinically validated equations. This problem requires you to implement unit harmonization and compute estimated glomerular filtration rate (eGFR) using the Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) 2021 creatinine-based equation, then quantify the distributional impact of unit harmonization.\n\nFundamental base:\n- Measurement unit conversion between mass concentration and amount of substance concentration is a well-tested rule. For serum creatinine, the accepted conversion is $1\\ \\mathrm{mg/dL} = 88.4\\ \\mathrm{\\mu mol/L}$, implying $$\\mathrm{mg/dL} = \\frac{\\mathrm{\\mu mol/L}}{88.4}.$$\n- The CKD-EPI equation is a well-tested clinical formula mapping serum creatinine, age, and sex to an eGFR in $\\mathrm{mL/min/1.73\\,m^2}$; it must be used with serum creatinine in $\\mathrm{mg/dL}$.\n- The Kolmogorov–Smirnov (KS) distance between two empirical distributions is defined by $$D = \\sup_x \\left|F_n(x) - G_m(x)\\right|,$$ where $F_n$ and $G_m$ are the empirical cumulative distribution functions of samples of sizes $n$ and $m$.\n\nTask:\n- For each dataset, compute two versions of eGFR in $\\mathrm{mL/min/1.73\\,m^2}$:\n    1. A naive version in which each recorded creatinine value is treated as if it were in $\\mathrm{mg/dL}$ regardless of its recorded unit.\n    2. A harmonized version in which values recorded in $\\mathrm{\\mu mol/L}$ are converted to $\\mathrm{mg/dL}$ using $\\mathrm{mg/dL} = \\mathrm{\\mu mol/L} / 88.4$ before computing eGFR.\n- Use the CKD-EPI 2021 creatinine equation for adults, without a race coefficient. Implement the standard sex-specific constants and age dependence as defined in that equation. Serum creatinine must be handled in $\\mathrm{mg/dL}$.\n- For each dataset, compute the following summary statistics comparing naive and harmonized eGFR distributions:\n    1. The difference in the mean eGFR, defined as $\\overline{eGFR}_{\\text{naive}} - \\overline{eGFR}_{\\text{harmonized}}$ (expressed in $\\mathrm{mL/min/1.73\\,m^2}$).\n    2. The difference in the median eGFR, defined as $\\operatorname{median}(eGFR_{\\text{naive}}) - \\operatorname{median}(eGFR_{\\text{harmonized}})$ (expressed in $\\mathrm{mL/min/1.73\\,m^2}$).\n    3. The Kolmogorov–Smirnov distance $D$ between the naive and harmonized eGFR samples.\n\nAngle units are not applicable. Express all eGFR quantities in $\\mathrm{mL/min/1.73\\,m^2}$ and report numerical outputs as decimal floats. Round each reported float in the final output to $4$ decimal places.\n\nInput data (embedded in the program; no external input):\nEach record is a tuple $(\\text{age in years}, \\text{sex}, \\text{serum creatinine value}, \\text{unit})$ where sex is either $\\text{\"F\"}$ for female or $\\text{\"M\"}$ for male. The test suite comprises three datasets:\n\n- Dataset $1$ (mixed units, typical values):\n    - $(65, \"F\", 1.1, \"mg/dL\")$\n    - $(50, \"M\", 90, \"μmol/L\")$\n    - $(80, \"F\", 1.8, \"mg/dL\")$\n    - $(40, \"M\", 75, \"μmol/L\")$\n    - $(30, \"F\", 0.7, \"mg/dL\")$\n    - $(70, \"M\", 110, \"μmol/L\")$\n    - $(55, \"F\", 95, \"μmol/L\")$\n    - $(45, \"M\", 1.3, \"mg/dL\")$\n- Dataset $2$ (all values already in $\\mathrm{mg/dL}$):\n    - $(18, \"M\", 0.9, \"mg/dL\")$\n    - $(22, \"F\", 0.6, \"mg/dL\")$\n    - $(90, \"M\", 2.5, \"mg/dL\")$\n    - $(75, \"F\", 1.2, \"mg/dL\")$\n    - $(65, \"M\", 1.0, \"mg/dL\")$\n    - $(50, \"F\", 0.8, \"mg/dL\")$\n- Dataset $3$ (all values in $\\mathrm{\\mu mol/L}$):\n    - $(35, \"F\", 80, \"μmol/L\")$\n    - $(55, \"M\", 120, \"μmol/L\")$\n    - $(85, \"F\", 200, \"μmol/L\")$\n    - $(45, \"M\", 70, \"μmol/L\")$\n    - $(60, \"F\", 100, \"μmol/L\")$\n    - $(28, \"M\", 60, \"μmol/L\")$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order:\n$[$difference in mean for dataset $1$, difference in median for dataset $1$, KS distance for dataset $1$, difference in mean for dataset $2$, difference in median for dataset $2$, KS distance for dataset $2$, difference in mean for dataset $3$, difference in median for dataset $3$, KS distance for dataset $3]$.\nRound each float to $4$ decimal places. The numerical values represent $\\mathrm{mL/min/1.73\\,m^2}$ for mean and median differences and a unitless decimal for the KS distance $D$.",
            "solution": "The problem requires the implementation of a data processing pipeline to demonstrate the importance of unit harmonization in clinical calculations using Electronic Health Record (EHR) data. The pipeline involves three key stages: data harmonization, application of a clinical model, and statistical comparison.\n\nFirst, we must define the computational model for estimating the glomerular filtration rate (eGFR). The problem specifies the use of the Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) 2021 creatinine-based equation, which is the current standard for assessing kidney function in adults. This equation relates serum creatinine ($S_{Cr}$), age, and sex to eGFR. A critical prerequisite is that serum creatinine must be in units of milligrams per deciliter ($\\mathrm{mg/dL}$). The 2021 formula, excluding the race-based coefficient, is expressed as:\n$$ eGFR = 142 \\times \\min\\left(\\frac{S_{Cr}}{k}, 1\\right)^\\alpha \\times \\max\\left(\\frac{S_{Cr}}{k}, 1\\right)^{-1.200} \\times 0.9938^{Age} \\times F_{sex} $$\nThe parameters $k$, $\\alpha$, and $F_{sex}$ are sex-dependent. For females, $k = 0.7$, $\\alpha = -0.241$, and the sex-specific factor $F_{sex} = 1.012$. For males, $k = 0.9$, $\\alpha = -0.302$, and $F_{sex} = 1.0$. This single, compact formula correctly captures the piecewise nature of the original CKD-EPI definition.\n\nSecond, we address the core task of unit harmonization. The input data may contain serum creatinine values in two different units: mass concentration ($\\mathrm{mg/dL}$) and amount of substance concentration ($\\mathrm{\\mu mol/L}$). To use the CKD-EPI equation, all values must be in $\\mathrm{mg/dL}$. The problem provides the standard conversion factor: $1\\ \\mathrm{mg/dL} = 88.4\\ \\mathrm{\\mu mol/L}$. Therefore, a creatinine value $S_{Cr, \\mu mol/L}$ given in $\\mathrm{\\mu mol/L}$ can be converted to $S_{Cr, mg/dL}$ in $\\mathrm{mg/dL}$ using the formula:\n$$ S_{Cr, mg/dL} = \\frac{S_{Cr, \\mu mol/L}}{88.4} $$\nThe problem requires calculating two sets of eGFR values. The \"harmonized\" set is computed by first applying this conversion to any creatinine value recorded in $\\mathrm{\\mu mol/L}$, ensuring all inputs to the CKD-EPI equation are correct. The \"naive\" set is computed by ignoring the stated units and treating all numerical creatinine values as if they were already in $\\mathrm{mg/dL}$. This is designed to simulate a common data processing error and quantify its impact.\n\nThird, we must quantify the discrepancy between the distributions of naive and harmonized eGFR values. Three statistical measures are specified:\n1.  The difference in the mean: $\\Delta_{\\text{mean}} = \\overline{eGFR}_{\\text{naive}} - \\overline{eGFR}_{\\text{harmonized}}$. A non-zero value indicates a systematic shift in the central tendency of the distribution caused by the unit error.\n2.  The difference in the median: $\\Delta_{\\text{median}} = \\operatorname{median}(eGFR_{\\text{naive}}) - \\operatorname{median}(eGFR_{\\text{harmonized}})$. This provides a robust measure of the shift in central tendency, less sensitive to outliers than the mean.\n3.  The Kolmogorov–Smirnov (KS) distance: $D = \\sup_x |F_{\\text{naive}}(x) - F_{\\text{harmonized}}(x)|$, where $F$ denotes the empirical cumulative distribution function (ECDF) of a sample. The KS distance measures the maximum absolute difference between the ECDFs of the two samples, providing a global measure of the difference in their distributions. A value of $D=0$ implies identical distributions, while $D=1$ implies no overlap between the distributions.\n\nThe algorithm proceeds by iterating through each provided dataset. For each dataset, we iterate through every patient record. For each patient, we compute two eGFR values: one using the naive creatinine value and one using the harmonized creatinine value. After processing all patients in a dataset, we collect the two resulting sets of eGFR values. We then use standard numerical library functions to calculate the mean of each set, the median of each set, and the KS distance between the two sets. Finally, we compute the required differences and format all results for output. For Dataset $2$, where all units are already $\\mathrm{mg/dL}$, the naive and harmonized calculations are identical, correctly yielding differences and a KS distance of $0$. For Dataset $3$, where all units are $\\mathrm{\\mu mol/L}$, the naive calculation uses creatinine values that are approximately $88.4$ times too high, leading to extremely low, non-physiological eGFRs and a large discrepancy from the harmonized results, resulting in a KS distance of $1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import ks_2samp\n\ndef solve():\n    \"\"\"\n    Solves the eGFR harmonization problem by calculating naive and harmonized eGFR values\n    and comparing their distributions for three datasets.\n    \"\"\"\n\n    # Define the datasets as specified in the problem statement.\n    datasets = [\n        # Dataset 1 (mixed units)\n        [\n            (65, \"F\", 1.1, \"mg/dL\"),\n            (50, \"M\", 90, \"μmol/L\"),\n            (80, \"F\", 1.8, \"mg/dL\"),\n            (40, \"M\", 75, \"μmol/L\"),\n            (30, \"F\", 0.7, \"mg/dL\"),\n            (70, \"M\", 110, \"μmol/L\"),\n            (55, \"F\", 95, \"μmol/L\"),\n            (45, \"M\", 1.3, \"mg/dL\"),\n        ],\n        # Dataset 2 (all mg/dL)\n        [\n            (18, \"M\", 0.9, \"mg/dL\"),\n            (22, \"F\", 0.6, \"mg/dL\"),\n            (90, \"M\", 2.5, \"mg/dL\"),\n            (75, \"F\", 1.2, \"mg/dL\"),\n            (65, \"M\", 1.0, \"mg/dL\"),\n            (50, \"F\", 0.8, \"mg/dL\"),\n        ],\n        # Dataset 3 (all μmol/L)\n        [\n            (35, \"F\", 80, \"μmol/L\"),\n            (55, \"M\", 120, \"μmol/L\"),\n            (85, \"F\", 200, \"μmol/L\"),\n            (45, \"M\", 70, \"μmol/L\"),\n            (60, \"F\", 100, \"μmol/L\"),\n            (28, \"M\", 60, \"μmol/L\"),\n        ],\n    ]\n\n    def ckd_epi_2021(scr_mgdl, age, sex):\n        \"\"\"\n        Calculates eGFR using the CKD-EPI 2021 creatinine equation.\n        \n        Args:\n            scr_mgdl (float): Serum creatinine in mg/dL.\n            age (int): Age in years.\n            sex (str): \"F\" for female, \"M\" for male.\n\n        Returns:\n            float: Estimated GFR in mL/min/1.73m^2.\n        \"\"\"\n        if sex == \"F\":\n            k = 0.7\n            alpha = -0.241\n            sex_factor = 1.012\n        else:  # sex == \"M\"\n            k = 0.9\n            alpha = -0.302\n            sex_factor = 1.0\n\n        scr_over_k = scr_mgdl / k\n        min_term = min(scr_over_k, 1.0)**alpha\n        max_term = max(scr_over_k, 1.0)**(-1.200)\n        age_term = 0.9938**age\n        egfr = 142 * min_term * max_term * age_term * sex_factor\n        return egfr\n\n    final_results = []\n    \n    # Conversion factor from μmol/L to mg/dL is division by 88.4\n    CONVERSION_FACTOR = 88.4\n\n    for dataset in datasets:\n        egfr_naive = []\n        egfr_harmonized = []\n\n        for age, sex, scr_value, unit in dataset:\n            # 1. Naive eGFR calculation\n            # Treats all creatinine values as if they were in mg/dL.\n            naive_egfr_val = ckd_epi_2021(scr_value, age, sex)\n            egfr_naive.append(naive_egfr_val)\n\n            # 2. Harmonized eGFR calculation\n            # Converts μmol/L to mg/dL before calculation.\n            if unit == \"μmol/L\":\n                harmonized_scr = scr_value / CONVERSION_FACTOR\n            else: # unit == \"mg/dL\"\n                harmonized_scr = scr_value\n            \n            harmonized_egfr_val = ckd_epi_2021(harmonized_scr, age, sex)\n            egfr_harmonized.append(harmonized_egfr_val)\n        \n        # Convert lists to NumPy arrays for vectorized operations\n        egfr_naive_arr = np.array(egfr_naive)\n        egfr_harmonized_arr = np.array(egfr_harmonized)\n\n        # 3. Compute summary statistics\n        # Difference in means\n        mean_diff = np.mean(egfr_naive_arr) - np.mean(egfr_harmonized_arr)\n        \n        # Difference in medians\n        median_diff = np.median(egfr_naive_arr) - np.median(egfr_harmonized_arr)\n        \n        # Kolmogorov-Smirnov distance\n        ks_distance = ks_2samp(egfr_naive_arr, egfr_harmonized_arr).statistic\n        \n        final_results.extend([mean_diff, median_diff, ks_distance])\n\n    # Format the final output string as required\n    # Each float must be rounded to 4 decimal places.\n    formatted_results = [f\"{val:.4f}\" for val in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A vast amount of critical patient information is embedded in unstructured clinical notes, and extracting it is a fundamental challenge in generating real-world evidence. This process, called electronic phenotyping, often begins with Natural Language Processing (NLP) techniques to identify mentions of conditions like pneumonia. This practice will guide you in building a simple but effective rule-based NLP algorithm and, just as importantly, evaluating its accuracy using standard metrics like sensitivity and Positive Predictive Value (PPV) .",
            "id": "4862788",
            "problem": "You are given a text-processing and evaluation task grounded in Electronic Health Record (EHR) data for generating real-world evidence. The task is to implement a case-insensitive Natural Language Processing (NLP) rule to extract affirmed diagnoses of pneumonia from dated clinical notes and then compute sensitivity and Positive Predictive Value (PPV) of the extraction against chart-validated labels. Your program must be a complete, runnable program. The task is to be framed and solved in purely mathematical and logical terms without relying on any external data sources.\n\nFundamental base and core definitions: Clinical text mining can be modeled as a binary classification problem over patients, where a patient is predicted positive if an affirmed mention is present and predicted negative otherwise. Using the standard diagnostic test framework, sensitivity and Positive Predictive Value (PPV) are defined via the confusion matrix quantities true positives, false negatives, and false positives. Dates are used to restrict mentions to a clinically relevant observation window around a patient-specific index date.\n\nAffirmed mention rule: For a given note, split the note into sentences where a sentence is delimited by period, semicolon, or newline. A sentence affirms pneumonia if it contains the substring \"pneumonia\" and does not contain the substring \"no evidence of\"; both checks must be case-insensitive. A note affirms pneumonia if any of its sentences affirms pneumonia. A patient is predicted positive if any note dated within an inclusive window of $\\pm W$ days around the patient’s index date contains an affirmed mention according to the sentence rule.\n\nEvaluation quantities: For a set of patients, let $TP$ be the number of true positives, $FN$ the number of false negatives, and $FP$ the number of false positives, determined by comparing the program’s predicted patient-level labels to the chart-validated labels. Using standard definitions from diagnostic test evaluation, sensitivity and Positive Predictive Value (PPV) are to be computed as decimals. In edge cases where denominators would be $0$ (for example, no chart-validated positives or no predicted positives), define the corresponding metric to be $0.0$.\n\nYour program must implement the above rules and compute the metrics for the following test suite. In each test case, each patient has an index date, a binary chart-validated label, and a list of dated notes. A date is a string in ISO format \"YYYY-MM-DD\". The window size $W$ is supplied per test case and is in days.\n\nTest Suite:\nTest Case $1$ with window size $W = 30$:\n- Patient \"A\": index date 2021-03-15, chart-validated label $1$. Notes: (2021-03-10, \"Patient with pneumonia, started antibiotics.\"), (2021-02-01, \"No evidence of pneumonia.\")\n- Patient \"B\": index date 2021-03-20, chart-validated label $0$. Notes: (2021-03-19, \"CXR shows no evidence of pneumonia.\"), (2021-03-01, \"Follow-up visit.\")\n- Patient \"C\": index date 2021-03-25, chart-validated label $1$. Notes: (2021-03-26, \"Pneumonia suspected.\")\n- Patient \"D\": index date 2021-03-05, chart-validated label $0$. Notes: (2021-04-10, \"No evidence of pneumonia.\"), (2021-03-04, \"No fever.\")\n\nTest Case $2$ with window size $W = 14$:\n- Patient \"E\": index date 2020-11-01, chart-validated label $1$. Notes: (2020-11-02, \"No evidence of pneumonia.\")\n- Patient \"F\": index date 2020-11-01, chart-validated label $0$. Notes: (2020-11-03, \"No evidence of pneumonia on CT.\")\n- Patient \"G\": index date 2020-11-10, chart-validated label $1$. Notes: (2020-10-01, \"Pneumonia confirmed.\")\n\nTest Case $3$ with window size $W = 7$:\n- Patient \"H\": index date 2022-01-15, chart-validated label $1$. Notes: (2022-01-15, \"No evidence of pneumonia. However, later that day, pneumonia developed.\")\n- Patient \"I\": index date 2022-01-15, chart-validated label $0$. Notes: (2022-01-16, \"No evidence of pneumonia; monitoring ongoing.\")\n- Patient \"J\": index date 2022-01-10, chart-validated label $1$. Notes: (2022-01-18, \"Pneumonia.\")\n\nTest Case $4$ with window size $W = 90$:\n- Patient \"K\": index date 2019-05-01, chart-validated label $0$. Notes: (2019-04-01, \"pNeUmOnIa noted in past.\")\n- Patient \"L\": index date 2019-05-01, chart-validated label $1$. Notes: (2019-05-20, \"No evidence of severe pneumonia?\"), (2019-05-21, \"possible pneumonia vs bronchitis\")\n- Patient \"M\": index date 2019-05-01, chart-validated label $1$. Notes: (2019-01-01, \"Pneumonia.\")\n- Patient \"N\": index date 2019-05-01, chart-validated label $0$. Notes: (2019-05-02, \"No evidence of pneumonia.\")\n\nYour program must:\n- Implement the case-insensitive sentence-level affirmed mention rule as specified.\n- For each test case, predict patient-level labels based on notes within the specified window around the index date.\n- Compute sensitivity and PPV as decimals for each test case, applying the $0.0$ rule for zero denominators.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the results are ordered by test case and within each test case by sensitivity followed by PPV. Express each metric as a decimal rounded to $4$ decimal places. For example, if there are $4$ test cases, the output format must be \"[sens$_1$,ppv$_1$,sens$_2$,ppv$_2$,sens$_3$,ppv$_3$,sens$_4$,ppv$_4$]\".",
            "solution": "The problem requires the implementation and evaluation of a rule-based Natural Language Processing (NLP) system for detecting affirmed diagnoses of pneumonia from clinical notes. The evaluation is to be conducted using the standard metrics of sensitivity and Positive Predictive Value (PPV). The problem is well-defined, scientifically grounded in the principles of clinical informatics and diagnostic test evaluation, and provides all necessary data and rules for a unique, verifiable solution.\n\nThe solution can be constructed by formalizing a sequence of logical operations, which we will apply to each patient in the provided test suite.\n\nThe core of the problem is a binary classification task. For each patient, we predict a label, $y_{pred} \\in \\{0, 1\\}$, where $y_{pred}=1$ signifies a predicted pneumonia diagnosis and $y_{pred}=0$ signifies its absence. This prediction is then compared to a ground-truth, chart-validated label, $y_{true} \\in \\{0, 1\\}$.\n\nThe components of the algorithmic solution are as follows:\n\n1.  **Date and Window Logic**: A clinical note is considered relevant only if its date, $d_{note}$, falls within an inclusive window of $\\pm W$ days around a patient's index date, $d_{index}$. Mathematically, this condition is expressed as:\n    $$ |d_{note} - d_{index}| \\le W $$\n    where the date difference is measured in days. Dates are provided in \"YYYY-MM-DD\" format and must be parsed into a date object capable of arithmetic.\n\n2.  **Sentence-Level Affirmation Rule**: A single sentence is deemed to affirm pneumonia if it satisfies two conditions simultaneously. Let $s$ be a sentence. Let $s_{lower}$ be the lowercase version of $s$. The sentence affirms pneumonia if and only if:\n    $$ (\\text{\"pneumonia\"} \\in s_{lower}) \\land (\\text{\"no evidence of\"} \\notin s_{lower}) $$\n    Sentences within a note are delimited by a period (.), a semicolon (;), or a newline character (`\\n`).\n\n3.  **Note-Level and Patient-Level Prediction**: A clinical note affirms pneumonia if at least one of its sentences affirms pneumonia. A patient is predicted to be positive ($y_{pred}=1$) if at least one of their relevant clinical notes (i.e., a note within the specified date window) affirms pneumonia. If no relevant note affirms pneumonia, the patient is predicted to be negative ($y_{pred}=0$).\n\n4.  **Evaluation Metrics**: The performance of the prediction algorithm is measured by comparing the set of predicted labels, $\\{y_{pred}\\}$, to the set of true labels, $\\{y_{true}\\}$. This comparison yields the counts of true positives ($TP$), false positives ($FP$), and false negatives ($FN$).\n    *   $TP$: The number of patients for whom $y_{true}=1$ and $y_{pred}=1$.\n    *   $FP$: The number of patients for whom $y_{true}=0$ and $y_{pred}=1$.\n    *   $FN$: The number of patients for whom $y_{true}=1$ and $y_{pred}=0$.\n\n    From these quantities, sensitivity and PPV are calculated:\n    $$ \\text{Sensitivity} = \\frac{TP}{TP + FN} $$\n    $$ \\text{Positive Predictive Value (PPV)} = \\frac{TP}{TP + FP} $$\n    As per the problem specification, if the denominator of either metric is $0$, the value of that metric is defined as $0.0$.\n\nWe now apply this formal procedure to each test case.\n\n**Test Case 1: Window Size $W = 30$ days**\n*   Patient A ($y_{true}=1$): Note from 2021-03-10 is within the $30$-day window of index date 2021-03-15. The note \"Patient with pneumonia...\" affirms pneumonia. Thus, $y_{pred}=1$. This is a True Positive ($TP$).\n*   Patient B ($y_{true}=0$): Note from 2021-03-19 is within the window of index date 2021-03-20. The sentence \"...no evidence of pneumonia\" contains the negation phrase and does not affirm pneumonia. Thus, $y_{pred}=0$. This is a True Negative ($TN$).\n*   Patient C ($y_{true}=1$): Note from 2021-03-26 is a day after index date 2021-03-25, within the window. \"Pneumonia suspected.\" affirms pneumonia. Thus, $y_{pred}=1$. This is a True Positive ($TP$).\n*   Patient D ($y_{true}=0$): Note from 2021-04-10 is more than $30$ days after index date 2021-03-05 and is ignored. Note from 2021-03-04 is within the window but does not mention pneumonia. Thus, $y_{pred}=0$. This is a True Negative ($TN$).\n*   **Metrics**: $TP=2$, $FP=0$, $FN=0$. Total positives $TP+FN=2$. Predicted positives $TP+FP=2$.\n    *   Sensitivity = $2 / (2+0) = 1.0$.\n    *   PPV = $2 / (2+0) = 1.0$.\n\n**Test Case 2: Window Size $W = 14$ days**\n*   Patient E ($y_{true}=1$): Note from 2020-11-02 is within the window of index date 2020-11-01. \"No evidence of pneumonia.\" is negated. Thus, $y_{pred}=0$. This is a False Negative ($FN$).\n*   Patient F ($y_{true}=0$): Note from 2020-11-03 is within the window of index date 2020-11-01. \"No evidence of pneumonia on CT.\" is negated. Thus, $y_{pred}=0$. This is a True Negative ($TN$).\n*   Patient G ($y_{true}=1$): Note from 2020-10-01 is outside the $14$-day window of index date 2020-11-10 and is ignored. Thus, $y_{pred}=0$. This is a False Negative ($FN$).\n*   **Metrics**: $TP=0$, $FP=0$, $FN=2$. Total positives $TP+FN=2$. Predicted positives $TP+FP=0$.\n    *   Sensitivity = $0 / (0+2) = 0.0$.\n    *   PPV: Denominator $TP+FP=0$. By rule, PPV = $0.0$.\n\n**Test Case 3: Window Size $W = 7$ days**\n*   Patient H ($y_{true}=1$): Note from 2022-01-15 is on the index date. The note text is \"No evidence of pneumonia. However, later that day, pneumonia developed.\". The first sentence is negated. The second sentence, \"However, ... pneumonia developed.\", affirms pneumonia. Since one sentence affirms, the note affirms. Thus, $y_{pred}=1$. This is a True Positive ($TP$).\n*   Patient I ($y_{true}=0$): Note from 2022-01-16 is within the window of index date 2022-01-15. The text \"No evidence of pneumonia; monitoring ongoing.\" contains a negated sentence and another without the keyword. The note does not affirm pneumonia. Thus, $y_{pred}=0$. This is a True Negative ($TN$).\n*   Patient J ($y_{true}=1$): Note from 2022-01-18 is $8$ days after index date 2022-01-10, which is outside the $7$-day window. The note is ignored. Thus, $y_{pred}=0$. This is a False Negative ($FN$).\n*   **Metrics**: $TP=1$, $FP=0$, $FN=1$. Total positives $TP+FN=2$. Predicted positives $TP+FP=1$.\n    *   Sensitivity = $1 / (1+1) = 0.5$.\n    *   PPV = $1 / (1+0) = 1.0$.\n\n**Test Case 4: Window Size $W = 90$ days**\n*   Patient K ($y_{true}=0$): Note from 2019-04-01 is within the window of index date 2019-05-01. The text \"pNeUmOnIa noted in past.\" affirms pneumonia (case-insensitive check). Thus, $y_{pred}=1$. This is a False Positive ($FP$).\n*   Patient L ($y_{true}=1$): Note from 2019-05-20 contains a negated sentence. Note from 2019-05-21 is also in the window and its text \"possible pneumonia vs bronchitis\" affirms pneumonia. Thus, $y_{pred}=1$. This is a True Positive ($TP$).\n*   Patient M ($y_{true}=1$): Note from 2019-01-01 is outside the $90$-day window of index date 2019-05-01 and is ignored. Thus, $y_{pred}=0$. This is a False Negative ($FN$).\n*   Patient N ($y_{true}=0$): Note from 2019-05-02 is within the window. \"No evidence of pneumonia.\" is negated. Thus, $y_{pred}=0$. This is a True Negative ($TN$).\n*   **Metrics**: $TP=1$, $FP=1$, $FN=1$. Total positives $TP+FN=2$. Predicted positives $TP+FP=2$.\n    *   Sensitivity = $1 / (1+1) = 0.5$.\n    *   PPV = $1 / (1+1) = 0.5$.\n\nThe final results, rounded to $4$ decimal places, are compiled for the final output.",
            "answer": "```python\nimport numpy as np\nimport re\nfrom datetime import datetime, timedelta\n\ndef solve():\n    \"\"\"\n    Solves the pneumonia detection and evaluation task based on the provided rules and test suite.\n    \"\"\"\n    test_suite = [\n        {\n            \"W\": 30,\n            \"patients\": [\n                {\"id\": \"A\", \"index_date\": \"2021-03-15\", \"label\": 1, \"notes\": [(\"2021-03-10\", \"Patient with pneumonia, started antibiotics.\"), (\"2021-02-01\", \"No evidence of pneumonia.\")]},\n                {\"id\": \"B\", \"index_date\": \"2021-03-20\", \"label\": 0, \"notes\": [(\"2021-03-19\", \"CXR shows no evidence of pneumonia.\"), (\"2021-03-01\", \"Follow-up visit.\")]},\n                {\"id\": \"C\", \"index_date\": \"2021-03-25\", \"label\": 1, \"notes\": [(\"2021-03-26\", \"Pneumonia suspected.\")]},\n                {\"id\": \"D\", \"index_date\": \"2021-03-05\", \"label\": 0, \"notes\": [(\"2021-04-10\", \"No evidence of pneumonia.\"), (\"2021-03-04\", \"No fever.\")]}\n            ]\n        },\n        {\n            \"W\": 14,\n            \"patients\": [\n                {\"id\": \"E\", \"index_date\": \"2020-11-01\", \"label\": 1, \"notes\": [(\"2020-11-02\", \"No evidence of pneumonia.\")]},\n                {\"id\": \"F\", \"index_date\": \"2020-11-01\", \"label\": 0, \"notes\": [(\"2020-11-03\", \"No evidence of pneumonia on CT.\")]},\n                {\"id\": \"G\", \"index_date\": \"2020-11-10\", \"label\": 1, \"notes\": [(\"2020-10-01\", \"Pneumonia confirmed.\")]}\n            ]\n        },\n        {\n            \"W\": 7,\n            \"patients\": [\n                {\"id\": \"H\", \"index_date\": \"2022-01-15\", \"label\": 1, \"notes\": [(\"2022-01-15\", \"No evidence of pneumonia. However, later that day, pneumonia developed.\")]},\n                {\"id\": \"I\", \"index_date\": \"2022-01-15\", \"label\": 0, \"notes\": [(\"2022-01-16\", \"No evidence of pneumonia; monitoring ongoing.\")]},\n                {\"id\": \"J\", \"index_date\": \"2022-01-10\", \"label\": 1, \"notes\": [(\"2022-01-18\", \"Pneumonia.\")]}\n            ]\n        },\n        {\n            \"W\": 90,\n            \"patients\": [\n                {\"id\": \"K\", \"index_date\": \"2019-05-01\", \"label\": 0, \"notes\": [(\"2019-04-01\", \"pNeUmOnIa noted in past.\")]},\n                {\"id\": \"L\", \"index_date\": \"2019-05-01\", \"label\": 1, \"notes\": [(\"2019-05-20\", \"No evidence of severe pneumonia?\"), (\"2019-05-21\", \"possible pneumonia vs bronchitis\")]},\n                {\"id\": \"M\", \"index_date\": \"2019-05-01\", \"label\": 1, \"notes\": [(\"2019-01-01\", \"Pneumonia.\")]},\n                {\"id\": \"N\", \"index_date\": \"2019-05-01\", \"label\": 0, \"notes\": [(\"2019-05-02\", \"No evidence of pneumonia.\")]}\n            ]\n        }\n    ]\n\n    final_results = []\n\n    def sentence_affirms_pneumonia(sentence):\n        \"\"\"Checks if a single sentence affirms pneumonia.\"\"\"\n        s_lower = sentence.lower()\n        return \"pneumonia\" in s_lower and \"no evidence of\" not in s_lower\n\n    def note_affirms_pneumonia(note_text):\n        \"\"\"Checks if any sentence in a note affirms pneumonia.\"\"\"\n        sentences = re.split(r'[.;\\n]', note_text)\n        for sentence in sentences:\n            if sentence_affirms_pneumonia(sentence):\n                return True\n        return False\n\n    for test_case in test_suite:\n        W = test_case[\"W\"]\n        patients = test_case[\"patients\"]\n        \n        tp, fp, fn = 0, 0, 0\n\n        for patient in patients:\n            index_date = datetime.strptime(patient[\"index_date\"], \"%Y-%m-%d\")\n            true_label = patient[\"label\"]\n            \n            predicted_positive = False\n            for note_date_str, note_text in patient[\"notes\"]:\n                note_date = datetime.strptime(note_date_str, \"%Y-%m-%d\")\n                \n                # Check if the note is within the inclusive date window\n                if abs((note_date - index_date).days) = W:\n                    if note_affirms_pneumonia(note_text):\n                        predicted_positive = True\n                        break # A single affirming note is sufficient\n            \n            pred_label = 1 if predicted_positive else 0\n\n            if true_label == 1 and pred_label == 1:\n                tp += 1\n            elif true_label == 0 and pred_label == 1:\n                fp += 1\n            elif true_label == 1 and pred_label == 0:\n                fn += 1\n\n        # Calculate metrics for the test case\n        total_positives = tp + fn\n        predicted_positives = tp + fp\n\n        sensitivity = 0.0\n        if total_positives > 0:\n            sensitivity = tp / total_positives\n\n        ppv = 0.0\n        if predicted_positives > 0:\n            ppv = tp / predicted_positives\n            \n        final_results.append(f\"{sensitivity:.4f}\")\n        final_results.append(f\"{ppv:.4f}\")\n\n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "When analyzing observational data to assess treatment effects, researchers can fall into subtle but critical methodological traps. This exercise focuses on one of the most notorious pitfalls: immortal time bias, which can create a false appearance of treatment benefit. By comparing a naive (and incorrect) time-fixed analysis with a methodologically sound time-updated analysis, you will gain a tangible understanding of how this bias originates and why correctly handling time-dependent exposures is essential for valid causal inference from EHR data .",
            "id": "4862751",
            "problem": "You are given a cohort represented by Electronic Health Records (EHR) data, where each individual has a baseline time of $t=0$ months, a treatment initiation time $\\tau_i$ (which may be $\\infty$ if the individual never initiates treatment), a follow-up time $y_i$ measured from baseline to either the event or censoring in months, and a binary event indicator $\\delta_i \\in \\{0,1\\}$ where $\\delta_i=1$ denotes an event occurring at time $y_i$ and $\\delta_i=0$ denotes censoring at time $y_i$. Assume that, conditional on exposure state, the hazard within a group is approximately constant over time so that the empirical hazard in any exposure group $g \\in \\{0,1\\}$ can be estimated by $\\hat{h}_g = E_g / T_g$, where $E_g$ is the total number of observed events in group $g$ and $T_g$ is the total person-time in months accumulated by individuals while in group $g$. Define the hazard ratio as $\\widehat{HR} = \\hat{h}_1 / \\hat{h}_0$. You must compute $\\widehat{HR}$ twice: first under a time-fixed exposure definition, and second under a time-updated exposure definition, and then quantify the bias introduced by immortal time under the time-fixed exposure definition.\n\nThe time-fixed exposure definition classifies individuals based on whether they ever initiate treatment during follow-up. Specifically, for the time-fixed analysis, assign an individual to exposed group $g=1$ if $\\tau_i  \\infty$ and to unexposed group $g=0$ if $\\tau_i = \\infty$. For this time-fixed analysis, compute $T_1^{\\mathrm{fixed}}$ as the sum of $y_i$ over all individuals with $\\tau_i  \\infty$, and $T_0^{\\mathrm{fixed}}$ as the sum of $y_i$ over all individuals with $\\tau_i = \\infty$. Compute $E_1^{\\mathrm{fixed}}$ as the sum of $\\delta_i$ over all individuals with $\\tau_i  \\infty$, and $E_0^{\\mathrm{fixed}}$ as the sum of $\\delta_i$ over all individuals with $\\tau_i = \\infty$. The time-fixed hazard ratio is then $\\widehat{HR}_{\\mathrm{fixed}} = \\left(E_1^{\\mathrm{fixed}} / T_1^{\\mathrm{fixed}}\\right) \\big/ \\left(E_0^{\\mathrm{fixed}} / T_0^{\\mathrm{fixed}}\\right)$.\n\nThe time-updated exposure definition treats exposure as a time-dependent covariate. For an individual with finite $\\tau_i$, the unexposed person-time is the interval $[0, \\min(y_i,\\tau_i))$ and the exposed person-time is the interval $[\\tau_i, y_i)$ if $\\tau_i  y_i$. For an individual with $\\tau_i = \\infty$, all person-time $[0, y_i)$ is unexposed. Allocate events as occurring in the exposure state immediately preceding $y_i$; if $\\tau_i = y_i$, count the event as unexposed. Formally, compute $T_0^{\\mathrm{upd}}$ and $T_1^{\\mathrm{upd}}$ by summing the appropriate interval lengths in months as above across all individuals, and compute $E_0^{\\mathrm{upd}}$ and $E_1^{\\mathrm{upd}}$ by allocating events to the state active just before $y_i$ using the rule: if $\\delta_i=1$ and $y_i \\le \\tau_i$, increment $E_0^{\\mathrm{upd}}$ by $1$; if $\\delta_i=1$ and $y_i  \\tau_i$, increment $E_1^{\\mathrm{upd}}$ by $1$; if $\\delta_i=0$, do not increment any $E_g$. The time-updated hazard ratio is then $\\widehat{HR}_{\\mathrm{upd}} = \\left(E_1^{\\mathrm{upd}} / T_1^{\\mathrm{upd}}\\right) \\big/ \\left(E_0^{\\mathrm{upd}} / T_0^{\\mathrm{upd}}\\right)$.\n\nTo quantify immortal time bias, compute the relative bias as $b = \\left(\\widehat{HR}_{\\mathrm{fixed}} - \\widehat{HR}_{\\mathrm{upd}}\\right) / \\widehat{HR}_{\\mathrm{upd}}$, expressed as a decimal. All time quantities must be handled in months, and hazard ratios and bias are unitless. Your program must output each hazard ratio rounded to $3$ decimal places, and each bias rounded to $3$ decimal places.\n\nUse the following test suite of cohorts. Each cohort is a list of individuals given by tuples $(\\tau_i, y_i, \\delta_i)$, where all times are in months, $\\tau_i = \\infty$ denotes never treated, $y_i$ is the event or censoring time, and $\\delta_i \\in \\{0,1\\}$ is the event indicator.\n\nTest Case $1$ (happy path, mixed initiation times, some censoring):\n- Patients: $\\left[(2, 10, 1), (5, 12, 0), (\\infty, 8, 1), (0, 9, 1), (\\infty, 7, 0), (3, 6, 1)\\right]$.\n\nTest Case $2$ (late treatment initiation creating substantial immortal time, with events in both groups):\n- Patients: $\\left[(9, 12, 1), (10, 12, 0), (11, 14, 1), (\\infty, 6, 1), (\\infty, 12, 0)\\right]$.\n\nTest Case $3$ (event exactly at initiation boundary, immediate initiators, and never-treated individuals):\n- Patients: $\\left[(5, 5, 1), (0, 8, 0), (4, 9, 1), (\\infty, 7, 1), (\\infty, 10, 0)\\right]$.\n\nTest Case $4$ (early initiations and never-treated mix):\n- Patients: $\\left[(0, 4, 1), (1, 6, 0), (2, 3, 1), (\\infty, 5, 1), (\\infty, 6, 0)\\right]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, flattening the triplets $\\left[\\widehat{HR}_{\\mathrm{fixed}}, \\widehat{HR}_{\\mathrm{upd}}, b\\right]$ for the $4$ test cases in order. For example, the output format must be $\\left[\\widehat{HR}_{\\mathrm{fixed}}^{(1)}, \\widehat{HR}_{\\mathrm{upd}}^{(1)}, b^{(1)}, \\widehat{HR}_{\\mathrm{fixed}}^{(2)}, \\widehat{HR}_{\\mathrm{upd}}^{(2)}, b^{(2)}, \\widehat{HR}_{\\mathrm{fixed}}^{(3)}, \\widehat{HR}_{\\mathrm{upd}}^{(3)}, b^{(3)}, \\widehat{HR}_{\\mathrm{fixed}}^{(4)}, \\widehat{HR}_{\\mathrm{upd}}^{(4)}, b^{(4)}\\right]$, with all values rounded to $3$ decimal places. Times must be treated in months, and hazard ratios and bias are unitless floats.",
            "solution": "The user has provided a problem that is scientifically grounded in the principles of biostatistics and epidemiology, specifically concerning survival analysis and the methodological artifact known as immortal time bias. The problem is well-posed, providing explicit mathematical definitions and complete data for all calculations. There are no contradictions, ambiguities, or factual inaccuracies. Therefore, the problem is valid and a solution will be provided.\n\nThe core of this problem lies in comparing two different methods for defining exposure in a cohort study where treatment initiation is delayed. The goal is to calculate a hazard ratio ($\\widehat{HR}$) under each definition and quantify the discrepancy, which illustrates the concept of immortal time bias.\n\n**Fundamental Concepts**\n\nThe hazard rate, $h(t)$, represents the instantaneous risk of an event at time $t$, given survival up to time $t$. In this problem, we assume the hazard is constant within a specific exposure group $g \\in \\{0, 1\\}$, where $g=0$ denotes the unexposed state and $g=1$ denotes the exposed state. This assumption allows us to estimate the hazard empirically as:\n$$\n\\hat{h}_g = \\frac{E_g}{T_g}\n$$\nwhere $E_g$ is the total number of events observed in group $g$, and $T_g$ is the total person-time (in this case, person-months) of follow-up accumulated by individuals while they are in group $g$.\n\nThe hazard ratio ($\\widehat{HR}$) is the ratio of the hazard in the exposed group to the hazard in the unexposed group:\n$$\n\\widehat{HR} = \\frac{\\hat{h}_1}{\\hat{h}_0}\n$$\nAn $\\widehat{HR}  1$ suggests the exposure is associated with an increased risk of the event, an $\\widehat{HR}  1$ suggests a protective association, and an $\\widehat{HR} = 1$ suggests no association.\n\n**Analysis 1: Time-Fixed Exposure Definition**\n\nThis is a naive and methodologically flawed approach. An individual is classified into an exposure group based on their entire history, specifically whether they *ever* initiate treatment.\n- **Exposed Group ($g=1$)**: Individuals with a finite treatment initiation time, $\\tau_i  \\infty$.\n- **Unexposed Group ($g=0$)**: Individuals who never initiate treatment, $\\tau_i = \\infty$.\n\nFor this analysis, we calculate the total events and total person-time for each group:\n- $E_1^{\\mathrm{fixed}} = \\sum_{i: \\tau_i  \\infty} \\delta_i$\n- $T_1^{\\mathrm{fixed}} = \\sum_{i: \\tau_i  \\infty} y_i$\n- $E_0^{\\mathrm{fixed}} = \\sum_{i: \\tau_i = \\infty} \\delta_i$\n- $T_0^{\\mathrm{fixed}} = \\sum_{i: \\tau_i = \\infty} y_i$\n\nThe resulting time-fixed hazard ratio is:\n$$\n\\widehat{HR}_{\\mathrm{fixed}} = \\frac{E_1^{\\mathrm{fixed}} / T_1^{\\mathrm{fixed}}}{E_0^{\\mathrm{fixed}} / T_0^{\\mathrm{fixed}}}\n$$\n\nThe critical flaw here is the misattribution of person-time. For an individual who initiates treatment at $\\tau_i  0$, the period from baseline $t=0$ to $\\tau_i$ is \"immortal time\" with respect to the exposed state. The individual *must* survive event-free to reach $\\tau_i$ and become treated. By classifying their entire follow-up time $y_i$ as \"exposed\" person-time, we are including a period ($[0, \\tau_i)$) during which an exposed event could not have possibly occurred. This practice artificially inflates the denominator $T_1^{\\mathrm{fixed}}$ without a corresponding possibility of increasing the numerator $E_1^{\\mathrm{fixed}}$, thereby biasing the estimated hazard $\\hat{h}_1^{\\mathrm{fixed}}$ downwards and biasing $\\widehat{HR}_{\\mathrm{fixed}}$ towards the null value of $1$.\n\n**Analysis 2: Time-Updated Exposure Definition**\n\nThis is the correct approach, treating exposure as a time-varying covariate. An individual can contribute person-time to both the unexposed and exposed states.\n- For an individual $i$, the time interval $[0, \\min(y_i, \\tau_i))$ is unexposed person-time.\n- If $\\tau_i  y_i$, the time interval $[\\tau_i, y_i)$ is exposed person-time.\n\nThe total person-times for the time-updated analysis are calculated by summing these contributions across all individuals:\n- $T_0^{\\mathrm{upd}} = \\sum_{i} \\min(y_i, \\tau_i)$\n- $T_1^{\\mathrm{upd}} = \\sum_{i} \\max(0, y_i - \\tau_i)$\n\nEvents are allocated to the exposure state the individual was in immediately prior to the event time $y_i$.\n- An event ($\\delta_i=1$) is counted in the unexposed group ($E_0^{\\mathrm{upd}}$) if it occurs at or before treatment initiation ($y_i \\le \\tau_i$).\n- An event ($\\delta_i=1$) is counted in the exposed group ($E_1^{\\mathrm{upd}}$) if it occurs after treatment initiation ($y_i  \\tau_i$).\n\nThe time-updated hazard ratio, which is the correct estimate under the given model, is:\n$$\n\\widehat{HR}_{\\mathrm{upd}} = \\frac{E_1^{\\mathrm{upd}} / T_1^{\\mathrm{upd}}}{E_0^{\\mathrm{upd}} / T_0^{\\mathrm{upd}}}\n$$\n\n**Quantifying Immortal Time Bias**\n\nThe bias introduced by the incorrect time-fixed method is quantified by comparing its result to the correct time-updated result. The relative bias, $b$, is given by:\n$$\nb = \\frac{\\widehat{HR}_{\\mathrm{fixed}} - \\widehat{HR}_{\\mathrm{upd}}}{\\widehat{HR}_{\\mathrm{upd}}}\n$$\nA negative value for $b$ indicates that the time-fixed analysis underestimates the hazard ratio, which is the expected direction of immortal time bias.\n\n**Algorithmic Implementation**\n\nThe solution will be implemented by a function that processes a given cohort of patients. This function will perform the following steps:\n1. Initialize counters for both fixed and updated analyses: $E_0^{\\mathrm{fixed}}$, $T_0^{\\mathrm{fixed}}$, $E_1^{\\mathrm{fixed}}$, $T_1^{\\mathrm{fixed}}$, $E_0^{\\mathrm{upd}}$, $T_0^{\\mathrm{upd}}$, $E_1^{\\mathrm{upd}}$, $T_1^{\\mathrm{upd}}$, all to $0$.\n2. Iterate through each patient $(\\tau_i, y_i, \\delta_i)$ in the cohort.\n3. For the **time-fixed analysis**, classify the patient based on $\\tau_i$. If $\\tau_i  \\infty$, add $y_i$ to $T_1^{\\mathrm{fixed}}$ and $\\delta_i$ to $E_1^{\\mathrm{fixed}}$. If $\\tau_i = \\infty$, add $y_i$ to $T_0^{\\mathrm{fixed}}$ and $\\delta_i$ to $E_0^{\\mathrm{fixed}}$.\n4. For the **time-updated analysis**, partition the patient's person-time. Add $\\min(y_i, \\tau_i)$ to $T_0^{\\mathrm{upd}}$ and $y_i - \\min(y_i, \\tau_i)$ to $T_1^{\\mathrm{upd}}$. If $\\delta_i=1$, allocate the event: increment $E_0^{\\mathrm{upd}}$ if $y_i \\le \\tau_i$, or increment $E_1^{\\mathrm{upd}}$ if $y_i  \\tau_i$.\n5. After iterating through all patients, calculate the four hazards ($\\hat{h}_0^{\\mathrm{fixed}}, \\hat{h}_1^{\\mathrm{fixed}}, \\hat{h}_0^{\\mathrm{upd}}, \\hat{h}_1^{\\mathrm{upd}}$), taking care to handle potential division by zero if a person-time denominator is $0$.\n6. Compute $\\widehat{HR}_{\\mathrm{fixed}}$ and $\\widehat{HR}_{\\mathrm{upd}}$.\n7. Compute the relative bias $b$.\n8. Return the triplet ($\\widehat{HR}_{\\mathrm{fixed}}, \\widehat{HR}_{\\mathrm{upd}}, b$).\n\nThis procedure will be applied to each test case, and the results will be collected and formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes time-fixed and time-updated hazard ratios and the resulting\n    relative bias for several EHR cohorts to demonstrate immortal time bias.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1 (happy path, mixed initiation times, some censoring)\n        [(2, 10, 1), (5, 12, 0), (np.inf, 8, 1), (0, 9, 1), (np.inf, 7, 0), (3, 6, 1)],\n        \n        # Test Case 2 (late treatment initiation creating substantial immortal time)\n        [(9, 12, 1), (10, 12, 0), (11, 14, 1), (np.inf, 6, 1), (np.inf, 12, 0)],\n        \n        # Test Case 3 (event at initiation boundary, immediate initiators)\n        [(5, 5, 1), (0, 8, 0), (4, 9, 1), (np.inf, 7, 1), (np.inf, 10, 0)],\n        \n        # Test Case 4 (early initiations and never-treated mix)\n        [(0, 4, 1), (1, 6, 0), (2, 3, 1), (np.inf, 5, 1), (np.inf, 6, 0)],\n    ]\n\n    all_results = []\n\n    for cohort in test_cases:\n        # --- Time-Fixed Analysis ---\n        E0_fixed, T0_fixed = 0.0, 0.0\n        E1_fixed, T1_fixed = 0.0, 0.0\n\n        for tau_i, y_i, delta_i in cohort:\n            if tau_i == np.inf:  # Unexposed group (never-treated)\n                T0_fixed += y_i\n                E0_fixed += delta_i\n            else:  # Exposed group (ever-treated)\n                T1_fixed += y_i\n                E1_fixed += delta_i\n\n        h0_fixed = E0_fixed / T0_fixed if T0_fixed > 0 else 0.0\n        h1_fixed = E1_fixed / T1_fixed if T1_fixed > 0 else 0.0\n        \n        hr_fixed = h1_fixed / h0_fixed if h0_fixed > 0 else np.inf\n\n        # --- Time-Updated Analysis ---\n        E0_upd, T0_upd = 0.0, 0.0\n        E1_upd, T1_upd = 0.0, 0.0\n\n        for tau_i, y_i, delta_i in cohort:\n            # Partition person-time\n            unexposed_pt = min(y_i, tau_i)\n            exposed_pt = y_i - unexposed_pt\n            \n            T0_upd += unexposed_pt\n            T1_upd += exposed_pt\n\n            # Allocate events\n            if delta_i == 1:\n                if y_i = tau_i:\n                    E0_upd += 1\n                else:  # y_i > tau_i\n                    E1_upd += 1\n        \n        h0_upd = E0_upd / T0_upd if T0_upd > 0 else 0.0\n        h1_upd = E1_upd / T1_upd if T1_upd > 0 else 0.0\n        \n        hr_upd = h1_upd / h0_upd if h0_upd > 0 else np.inf\n\n        # --- Bias Calculation ---\n        # The problem cases ensure hr_upd is not zero\n        bias = (hr_fixed - hr_upd) / hr_upd\n        \n        all_results.extend([\n            round(hr_fixed, 3),\n            round(hr_upd, 3),\n            round(bias, 3)\n        ])\n\n    # Format the final output string\n    formatted_results = f\"[{','.join(map(str, all_results))}]\"\n    print(formatted_results)\n\nsolve()\n```"
        }
    ]
}