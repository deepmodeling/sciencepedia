{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in designing any backup and disaster recovery plan is to understand and project its cost. Modern backup solutions use sophisticated data reduction techniques like deduplication to minimize the physical storage required, but it's essential to account for all factors, including metadata overhead, to create an accurate financial model. This first exercise  will guide you through a practical calculation of monthly storage costs for a large-scale Electronic Health Record (EHR) system, translating logical data size into a real-world financial figure.",
            "id": "4823559",
            "problem": "A regional hospital is designing a disaster recovery plan for its Electronic Health Record (EHR) backups. The logical backup footprint is $800\\,\\text{TB}$ measured as the sum of unique and duplicate logical data across all protected systems. The backup software achieves an effective deduplication ratio of $10:1$, where an effective deduplication ratio $R:1$ means a logical dataset of size $L$ requires $L/R$ of physical storage capacity before metadata overhead. The storage provider charges $\\$0.02$ per $\\text{GB}$ per month and bills using decimal units such that $1\\,\\text{TB}=1000\\,\\text{GB}$. Deduplication metadata and index overhead add $10\\%$ to the physical capacity after deduplication. Ignore compression beyond what is already reflected in the effective deduplication ratio and ignore any transient capacity for backup windows.\n\nUsing only these fundamentals, compute the monthly storage cost for retaining the deduplicated backups. Express your final answer in United States dollars (USD) and round your answer to four significant figures.",
            "solution": "The problem requires the computation of the monthly storage cost for retaining deduplicated backups. We begin by validating the problem statement.\n\n### Step 1: Extract Givens\n- Logical backup footprint, $L = 800\\,\\text{TB}$.\n- Effective deduplication ratio: $10:1$. Let the ratio factor be $R = 10$.\n- Physical capacity required for a logical dataset of size $L$: $L/R$.\n- Storage cost: $C_{\\text{GB}} = \\$0.02$ per GB per month.\n- Unit conversion specified by provider: $1\\,\\text{TB} = 1000\\,\\text{GB}$.\n- Metadata and index overhead: $M_{\\text{overhead}} = 10\\%$ of the physical capacity after deduplication.\n- Final answer requirement: Round to four significant figures and express in USD.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the principles of data storage management, specifically deduplication and cost calculation, which are standard concepts in medical informatics and IT infrastructure. It is well-posed, providing all necessary data and definitions for a unique solution. The language is objective and precise. The problem is free of scientific flaws, contradictions, and ambiguities. The values provided are realistic for a hospital's IT operations.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A complete solution will be provided.\n\nThe solution proceeds by first calculating the physical storage size after deduplication, then adding the metadata overhead, converting the total size to the billing unit (GB), and finally calculating the total monthly cost.\n\nLet $L$ be the logical backup footprint, where $L = 800\\,\\text{TB}$.\nThe effective deduplication ratio is $10:1$, with the ratio factor $R = 10$. The physical capacity required after deduplication, which we shall denote as $S_{\\text{dedup}}$, is calculated by dividing the logical footprint by the deduplication factor $R$:\n$$S_{\\text{dedup}} = \\frac{L}{R}$$\nSubstituting the given values:\n$$S_{\\text{dedup}} = \\frac{800\\,\\text{TB}}{10} = 80\\,\\text{TB}$$\nNext, we account for the metadata and index overhead. This overhead, $M_{\\text{overhead}}$, adds $10\\%$ to the physical capacity *after* deduplication. The total physical capacity required, $S_{\\text{total}}$, is therefore:\n$$S_{\\text{total}} = S_{\\text{dedup}} + (M_{\\text{overhead}} \\times S_{\\text{dedup}}) = S_{\\text{dedup}} \\times (1 + M_{\\text{overhead}})$$\nGiven $M_{\\text{overhead}} = 10\\% = 0.10$:\n$$S_{\\text{total}} = 80\\,\\text{TB} \\times (1 + 0.10) = 80\\,\\text{TB} \\times 1.1 = 88\\,\\text{TB}$$\nThe storage provider's billing is based on a cost per gigabyte (GB). We must convert the total physical capacity from terabytes (TB) to gigabytes. The problem specifies the decimal unit conversion $1\\,\\text{TB} = 1000\\,\\text{GB}$.\n$$S_{\\text{total, GB}} = 88\\,\\text{TB} \\times \\frac{1000\\,\\text{GB}}{1\\,\\text{TB}} = 88000\\,\\text{GB}$$\nThe monthly cost per gigabyte is given as $C_{\\text{GB}} = \\$0.02$. The total monthly cost, $C_{\\text{monthly}}$, is the product of the total physical capacity in gigabytes and the cost per gigabyte.\n$$C_{\\text{monthly}} = S_{\\text{total, GB}} \\times C_{\\text{GB}}$$\nSubstituting the numerical values:\n$$C_{\\text{monthly}} = 88000 \\times 0.02 = 1760$$\nThe cost is therefore $\\$1760$. The problem requires the final answer to be rounded to four significant figures. The calculated value is $1760$. In scientific notation, this is $1.760 \\times 10^3$. The four significant figures are $1$, $7$, $6$, and the trailing $0$. Thus, the value $1760$ is already presented with the correct number of significant figures, and no rounding is necessary.",
            "answer": "$$\\boxed{1760}$$"
        },
        {
            "introduction": "Beyond storage costs, a successful disaster recovery strategy depends on the ability to move data efficiently to a safe, remote location. This transfer must be completed within a strict \"backup window\" to ensure that recovery points are up-to-date and production systems are not impacted during business hours. In this practice problem , you will determine the minimum network bandwidth required for nightly data replication, learning to factor in real-world variables like data compression and network protocol overhead.",
            "id": "4823533",
            "problem": "A regional hospital replicates changed electronic health record data nightly from its on-premises primary data center to a cloud-based disaster recovery site. The storage team reports that the nightly change set is $3\\ \\text{TB}$, measured in decimal terabytes where $1\\ \\text{TB} = 10^{12}\\ \\text{bytes}$. The data reduction applied by the backup software achieves a sustained compression ratio of $2:1$ on the wire. The networking team measures that transport security and protocol headers introduce a constant protocol overhead of $10\\%$ on the transmitted payload. Assume that $1\\ \\text{byte} = 8\\ \\text{bits}$, $1\\ \\text{Mb/s} = 10^{6}\\ \\text{bits/s}$, and that the replication runs continuously at a steady rate across the entire window with negligible packet loss and no competing traffic.\n\nUsing the fundamental relation that a constant data rate equals total transmitted volume divided by total transmission time, and the definition that a compression ratio of $a:b$ reduces payload size by a factor of $\\frac{b}{a}$ while a protocol overhead of $p$ increases transmitted volume to a factor of $(1+p)$ of the payload, compute the minimum wide area network bandwidth required to guarantee that the entire compressed and protocol-encapsulated nightly change set is transmitted within a $6$-hour window.\n\nExpress your answer in megabits per second $\\left(\\text{Mb/s}\\right)$ and round to three significant figures.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of data communications, is well-posed with sufficient and consistent data, and is expressed in objective, unambiguous language. All required definitions, constants, and constraints are provided. We may therefore proceed with the calculation.\n\nThe objective is to compute the minimum required network bandwidth in megabits per second ($\\text{Mb/s}$) to transfer a given volume of data within a specified time frame, accounting for data compression and protocol overhead. We will follow a systematic process of first determining the total volume of data to be transmitted and then dividing by the total time available for transmission.\n\nLet $V_{raw}$ be the initial size of the nightly change set.\nThe problem states $V_{raw} = 3\\ \\text{TB}$.\nUsing the provided definition, $1\\ \\text{TB} = 10^{12}\\ \\text{bytes}$, we have:\n$$V_{raw} = 3 \\times 10^{12}\\ \\text{bytes}$$\n\nThe data is compressed with a ratio of $2:1$. The problem specifies that a compression ratio of $a:b$ reduces the payload size by a factor of $\\frac{b}{a}$. Here, $a=2$ and $b=1$. The compression factor is $\\frac{1}{2}$.\nLet $V_{payload}$ be the size of the data after compression.\n$$V_{payload} = V_{raw} \\times \\frac{1}{2} = (3 \\times 10^{12}\\ \\text{bytes}) \\times \\frac{1}{2} = 1.5 \\times 10^{12}\\ \\text{bytes}$$\nThis is the size of the compressed payload that needs to be transmitted.\n\nNext, we account for protocol overhead. The overhead is given as $p = 10\\%$, which is equivalent to a decimal value of $p=0.1$. The problem defines that this overhead increases the transmitted volume by a factor of $(1+p)$.\nLet $V_{tx}$ be the total volume of data that must be transmitted over the network, including the payload and the overhead.\n$$V_{tx} = V_{payload} \\times (1 + p) = (1.5 \\times 10^{12}\\ \\text{bytes}) \\times (1 + 0.1) = (1.5 \\times 10^{12}\\ \\text{bytes}) \\times 1.1$$\n$$V_{tx} = 1.65 \\times 10^{12}\\ \\text{bytes}$$\n\nNetwork bandwidth is typically measured in bits per second. Therefore, we must convert the total transmitted volume from bytes to bits. Using the given conversion $1\\ \\text{byte} = 8\\ \\text{bits}$:\n$$V_{tx, bits} = V_{tx} \\times 8\\ \\frac{\\text{bits}}{\\text{byte}} = (1.65 \\times 10^{12}\\ \\text{bytes}) \\times 8\\ \\frac{\\text{bits}}{\\text{byte}}$$\n$$V_{tx, bits} = 13.2 \\times 10^{12}\\ \\text{bits} = 1.32 \\times 10^{13}\\ \\text{bits}$$\n\nThe transmission must be completed within a $6$-hour time window. Let this time be $T$.\n$$T = 6\\ \\text{hours}$$\nTo calculate the data rate in bits per second, we must convert this time window into seconds.\n$$T_{sec} = 6\\ \\text{hours} \\times \\left(\\frac{60\\ \\text{minutes}}{1\\ \\text{hour}}\\right) \\times \\left(\\frac{60\\ \\text{seconds}}{1\\ \\text{minute}}\\right) = 6 \\times 3600\\ \\text{seconds}$$\n$$T_{sec} = 21600\\ \\text{s} = 2.16 \\times 10^4\\ \\text{s}$$\n\nThe required bandwidth, or data rate $R$, is the total transmitted volume in bits divided by the transmission time in seconds.\n$$R = \\frac{V_{tx, bits}}{T_{sec}} = \\frac{1.32 \\times 10^{13}\\ \\text{bits}}{2.16 \\times 10^4\\ \\text{s}}$$\n$$R = \\left(\\frac{1.32}{2.16}\\right) \\times 10^{13-4}\\ \\frac{\\text{bits}}{\\text{s}} = \\left(\\frac{11}{18}\\right) \\times 10^9\\ \\frac{\\text{bits}}{\\text{s}}$$\nNumerically, this is approximately $0.6111... \\times 10^9\\ \\text{b/s}$, or $611,111,111.11...\\ \\text{b/s}$.\n\nThe problem requires the answer in megabits per second ($\\text{Mb/s}$), where $1\\ \\text{Mb/s} = 10^6\\ \\text{b/s}$.\n$$R_{Mbps} = \\frac{R}{10^6} = \\frac{\\left(\\frac{11}{18}\\right) \\times 10^9\\ \\text{b/s}}{10^6\\ \\text{b/s/Mb/s}} = \\left(\\frac{11}{18}\\right) \\times 10^3\\ \\text{Mb/s}$$\n$$R_{Mbps} \\approx 0.6111... \\times 10^3\\ \\text{Mb/s} = 611.111...\\ \\text{Mb/s}$$\n\nFinally, we round the result to three significant figures. The first three significant figures are $6$, $1$, and $1$. The fourth figure is $1$, which is less than $5$, so we round down.\n$$R_{rounded} = 611\\ \\text{Mb/s}$$\nThis is the minimum wide area network bandwidth required to meet the specified disaster recovery objective.",
            "answer": "$$\\boxed{611}$$"
        },
        {
            "introduction": "Backup and disaster recovery planning is rarely about finding a single 'best' solution; it is an exercise in optimization and balancing competing objectives. Different backup strategies offer different trade-offs between restore speed, the impact on production systems, and the load on backup infrastructure. This advanced exercise  challenges you to move beyond simple arithmetic and build a mathematical model to analyze these trade-offs, allowing you to determine the conditions under which a modern synthetic backup strategy is superior to a traditional one.",
            "id": "4823600",
            "problem": "A hospital’s Electronic Health Record (EHR) primary dataset has size $S$ and is protected by nightly backups. Each day, a fraction $c$ of the dataset changes, so the daily incremental volume is $cS$. The hospital is considering switching from a weekly full plus daily incremental backup policy to a daily synthetic full backup policy constructed by merging the previous full with the day’s incremental on the backup repository. The objective is to reduce expected restore time while not extending the production backup window, subject to I/O budget constraints on both the production system and the backup repository.\n\nAssume the following well-tested bases:\n- The time to transfer volume $V$ over a channel of sustained throughput $\\Theta$ is $t = V / \\Theta$.\n- I/O budget constraints impose upper bounds on sustained throughput available to backup processes within specified time windows.\n- A restore that replays a chain of incrementals introduces an average per-incremental latency penalty due to index lookups and random I/O.\n\nParameters:\n- Dataset size: $S = 12{,}000{,}000$ MB.\n- Daily change rate (unknown): $c \\in [0,1]$.\n- Production I/O budget for backup reads: $B_{p} = 650$ MB/s.\n- Network throughput from production to the backup gateway: $T_{n} = 500$ MB/s.\n- Nightly production backup window duration: $W = 4$ hours.\n- Backup repository I/O budget available to the synthetic merge: $U_{b} = 600$ MB/s, available for $M = 12$ hours each day (outside the production window).\n- Restore read throughput from backup repository to the restore target: $R_{b} = 800$ MB/s.\n- Traditional policy: a full backup once per week and daily incrementals thereafter; consider a worst-case restore at day index $i = 6$ since the last full.\n- Average per-incremental restore latency penalty (traditional chain rehydration): $L = 20$ s.\n- Synthetic full read slowdown factor (due to deduplication fragmentation): $\\phi = 1.3$ (unitless).\n- Traditional chain read slowdown factor: $\\psi = 1.05$ (unitless).\n\nUse the bases above to:\n- Derive the condition under which the synthetic full restore time is strictly less than the traditional chain restore time on day index $i$, in terms of $c$ and the given parameters.\n- Formalize the production backup window constraint using the effective production-to-backup throughput $T_{p} = \\min(B_{p}, T_{n})$.\n- Formalize the backup repository I/O budget constraint for the daily synthetic merge by accounting for reads of the previous full and the day’s incremental and the write of the new full.\n\nThen, using the provided parameter values, compute the maximum daily change rate $c_{\\star}$ such that:\n- A daily synthetic full backup strictly reduces restore time relative to the traditional chain on day index $i$, and\n- The production backup window is not extended, and\n- The synthetic merge stays within the backup repository’s daily I/O budget.\n\nExpress $c_{\\star}$ as a unitless decimal. Round your final answer to four significant figures.",
            "solution": "The problem asks for the maximum daily change rate, denoted as $c_{\\star}$, for an Electronic Health Record (EHR) dataset that satisfies three specific conditions. These conditions relate to restore time, the production backup window, and the I/O budget of the backup repository. We will formulate each condition as an inequality involving the change rate $c$ and then solve for the maximum value of $c$ that satisfies all constraints.\n\nFirst, we establish the time units for consistency. All throughputs are given in MB/s, so we will convert time windows from hours to seconds.\nThe nightly production backup window is $W = 4 \\text{ hours} = 4 \\times 3600 \\text{ s} = 14400 \\text{ s}$.\nThe backup repository's daily processing window is $M = 12 \\text{ hours} = 12 \\times 3600 \\text{ s} = 43200 \\text{ s}$.\n\nThe problem is solved by deriving three inequalities for $c$ based on the given constraints.\n\n1.  **Restore Time Constraint**\n    The first condition is that the restore time for the daily synthetic full backup policy, $T_{\\text{restore,synth}}$, must be strictly less than the worst-case restore time for the traditional policy, $T_{\\text{restore,trad}}$.\n\n    The traditional policy involves restoring one full backup of size $S$ and $i$ incremental backups, each of average size $cS$. The worst case is given for day index $i=6$. The total data volume to be read is $S + i(cS) = S(1+ic)$. This process is subject to a read slowdown factor $\\psi$ and a latency penalty $L$ for each of the $i$ incrementals. The restore is performed at a read throughput of $R_b$.\n    $$T_{\\text{restore,trad}} = \\psi \\frac{S(1+ic)}{R_b} + iL$$\n\n    The synthetic full backup policy involves restoring a single file of size $S$. This process is subject to a read slowdown factor $\\phi$ due to deduplication fragmentation.\n    $$T_{\\text{restore,synth}} = \\phi \\frac{S}{R_b}$$\n\n    The constraint is $T_{\\text{restore,synth}} < T_{\\text{restore,trad}}$:\n    $$\\phi \\frac{S}{R_b} < \\psi \\frac{S(1+ic)}{R_b} + iL$$\n    We can rearrange this inequality to solve for $c$:\n    $$\\phi \\frac{S}{R_b} - \\psi \\frac{S}{R_b} - iL < \\frac{\\psi Sic}{R_b}$$\n    $$\\frac{S}{R_b}(\\phi - \\psi) - iL < c \\left(\\frac{\\psi Si}{R_b}\\right)$$\n    $$c > \\frac{\\frac{S}{R_b}(\\phi - \\psi) - iL}{\\frac{\\psi Si}{R_b}} = \\frac{S(\\phi - \\psi) - iLR_b}{\\psi Si}$$\n    $$c > \\frac{\\phi - \\psi}{\\psi i} - \\frac{L R_b}{\\psi S}$$\n    This inequality provides a lower bound for $c$.\n\n2.  **Production Backup Window Constraint**\n    The second condition is that the daily backup of incremental changes from the production system must not exceed the available backup window $W$. The volume of data to be transferred is the daily change, $cS$. The transfer is limited by the effective production-to-backup throughput, $T_p$, which is the minimum of the production I/O budget for backup reads, $B_p$, and the network throughput, $T_n$.\n    $$T_p = \\min(B_p, T_n)$$\n    The time required for this backup is $t_{\\text{backup}} = \\frac{cS}{T_p}$. This must be less than or equal to $W$:\n    $$\\frac{cS}{T_p} \\le W$$\n    Solving for $c$, we get an upper bound:\n    $$c \\le \\frac{W T_p}{S}$$\n\n3.  **Backup Repository I/O Budget Constraint**\n    The third condition is that the daily synthetic merge process on the backup repository must be completed within its allotted time window $M$. This process involves reading the previous full backup (volume $S$), reading the day's incremental (volume $cS$), and writing the new synthetic full backup (volume $S$). The total I/O volume is the sum of these operations:\n    $$V_{\\text{merge,IO}} = S + cS + S = S(2+c)$$\n    This entire operation is sustained by the backup repository's I/O budget, which provides a throughput of $U_b$. The time taken for the merge is $t_{\\text{merge}} = \\frac{V_{\\text{merge,IO}}}{U_b}$. This must be less than or equal to $M$:\n    $$\\frac{S(2+c)}{U_b} \\le M$$\n    Solving for $c$, we obtain another upper bound:\n    $$S(2+c) \\le M U_b \\implies 2+c \\le \\frac{M U_b}{S} \\implies c \\le \\frac{M U_b}{S} - 2$$\n\n**Calculation of $c_{\\star}$**\nWe are looking for the maximum value $c_{\\star}$ that satisfies all three inequalities. This value will be the minimum of the derived upper bounds, provided this value also satisfies the lower bound inequality.\n$$c_{\\star} = \\min\\left(\\frac{W T_p}{S}, \\frac{M U_b}{S} - 2\\right)$$\n\nLet's now substitute the given parameter values:\n$S = 12,000,000$ MB\n$c \\in [0,1]$\n$B_p = 650$ MB/s\n$T_n = 500$ MB/s\n$W = 14400$ s\n$U_b = 600$ MB/s\n$M = 43200$ s\n$R_b = 800$ MB/s\n$i = 6$\n$L = 20$ s\n$\\phi = 1.3$\n$\\psi = 1.05$\n\nFirst, we calculate the lower bound for $c$ from the restore time constraint:\n$$c > \\frac{1.3 - 1.05}{1.05 \\times 6} - \\frac{20 \\text{ s} \\times 800 \\text{ MB/s}}{1.05 \\times 12,000,000 \\text{ MB}} = \\frac{0.25}{6.3} - \\frac{16000}{12,600,000}$$\n$$c > 0.0396825... - 0.0012698... = 0.0384127...$$\n\nNext, we calculate the upper bounds.\nFor the production window constraint, we first find $T_p$:\n$$T_p = \\min(650 \\text{ MB/s}, 500 \\text{ MB/s}) = 500 \\text{ MB/s}$$\nThe upper bound from this constraint is:\n$$c \\le \\frac{W T_p}{S} = \\frac{14400 \\text{ s} \\times 500 \\text{ MB/s}}{12,000,000 \\text{ MB}} = \\frac{7,200,000}{12,000,000} = 0.6$$\n\nFor the backup repository budget constraint, the upper bound is:\n$$c \\le \\frac{M U_b}{S} - 2 = \\frac{43200 \\text{ s} \\times 600 \\text{ MB/s}}{12,000,000 \\text{ MB}} - 2 = \\frac{25,920,000}{12,000,000} - 2$$\n$$c \\le 2.16 - 2 = 0.16$$\n\nThe set of all valid $c$ must satisfy $c > 0.0384127...$, $c \\le 0.6$, and $c \\le 0.16$.\nCombining these, the feasible range for $c$ is:\n$$0.0384127... < c \\le \\min(0.6, 0.16)$$\n$$0.0384127... < c \\le 0.16$$\nThe maximum daily change rate $c_{\\star}$ is the supremum of this interval, which is the upper bound.\n$$c_{\\star} = 0.16$$\nThe problem requires the answer to be rounded to four significant figures.\n$$c_{\\star} = 0.1600$$",
            "answer": "$$\\boxed{0.1600}$$"
        }
    ]
}