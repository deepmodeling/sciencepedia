{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of differential privacy is adding calibrated noise to query results. This first exercise guides you through the fundamental process of calculating the \"sensitivity\" of a query—the maximum change a single individual can cause—and using it to determine the correct amount of Laplace noise needed to achieve a specific privacy guarantee, $\\epsilon$. Mastering this calculation  is essential for implementing even the simplest differentially private mechanisms.",
            "id": "4835475",
            "problem": "A research hospital intends to release an aggregate quality metric derived from its Electronic Health Record (EHR) system: the sum of adult patients’ serum creatinine measurements recorded in a one-month period. For each patient $i$, suppose there are $m_i$ draws with measured values $x_{i1}, x_{i2}, \\dots, x_{im_i}$ in $\\text{mg/dL}$. The patient’s raw total is defined as $t_i = \\sum_{j=1}^{m_i} x_{ij}$. To enforce privacy and robustness, the hospital clips each patient’s contribution at a fixed cap $C$ in $\\text{mg/dL}$, forming $s_i = \\min(t_i, C)$. The released statistic is the sum query\n$$\nf(D) = \\sum_{i=1}^{n} s_i,\n$$\nwhere $D$ is the dataset of $n$ patients.\n\nThe hospital will use pure Differential Privacy (DP) with privacy parameter $\\epsilon$ under the add/remove neighboring relation: two datasets $D$ and $D'$ are neighbors if one can be obtained from the other by adding or removing a single patient’s entire EHR record. The randomized mechanism adds noise to the query output drawn from a Laplace distribution centered at zero with a scale parameter $b$ (in the same unit as the query, $\\text{mg/dL}$).\n\nUsing only first principles—the formal definition of Differential Privacy, the concept of global $\\ell_1$ sensitivity for a query under the specified neighboring relation, and the probability density function of the Laplace distribution—derive the global $\\ell_1$ sensitivity of $f$ and then determine the scale parameter $b$ required to satisfy $\\epsilon$-DP for this sum query. Take $C = 10\\,\\text{mg/dL}$ and $\\epsilon = 0.8$. Round your final value for $b$ to four significant figures, and express it in $\\text{mg/dL}$. Report only the value of $b$.",
            "solution": "We begin from the formal definition of Differential Privacy (DP). A randomized mechanism $\\mathcal{M}$ satisfies $\\epsilon$-DP under the add/remove neighboring relation if, for all neighboring datasets $D$ and $D'$ and for all measurable subsets $S$ of outputs,\n$$\n\\Pr[\\mathcal{M}(D) \\in S] \\leq \\exp(\\epsilon)\\,\\Pr[\\mathcal{M}(D') \\in S].\n$$\n\nThe mechanism of interest is the Laplace mechanism applied to the sum query $f(D)$, namely\n$$\n\\mathcal{M}(D) = f(D) + Y,\n$$\nwhere $Y$ is a random variable distributed according to the Laplace distribution with mean $0$ and scale parameter $b$. The probability density function (pdf) of $Y$ is\n$$\np_{Y}(y) = \\frac{1}{2b}\\,\\exp\\!\\left(-\\frac{|y|}{b}\\right).\n$$\n\nTo use the Laplace mechanism, we need the global $\\ell_1$ sensitivity of the query $f$ under the specified neighboring relation. The global $\\ell_1$ sensitivity $\\Delta_{1}(f)$ is defined as\n$$\n\\Delta_{1}(f) = \\max_{D \\sim D'} |f(D) - f(D')|,\n$$\nwhere $D \\sim D'$ denotes that $D$ and $D'$ are neighboring datasets (differing by the add/remove of one patient’s record).\n\nGiven $f(D) = \\sum_{i=1}^{n} s_i$ with $s_i = \\min(t_i, C)$ and $t_i = \\sum_{j=1}^{m_i} x_{ij}$, under add/remove adjacency, $D'$ differs from $D$ by the presence or absence of a single patient $p$. Then\n$$\nf(D) - f(D') = \n\\begin{cases}\ns_p & \\text{if } D' \\text{ is } D \\text{ with patient } p \\text{ removed},\\\\\n- s_p & \\text{if } D \\text{ is } D' \\text{ with patient } p \\text{ removed}.\n\\end{cases}\n$$\nTherefore,\n$$\n|f(D) - f(D')| = |s_p|.\n$$\nBy construction, each $s_p$ is clipped: $s_p = \\min(t_p, C)$, implying $0 \\leq s_p \\leq C$. Hence,\n$$\n|f(D) - f(D')| \\leq C,\n$$\nand since there exist datasets where a patient attains $s_p = C$, the maximum possible change is achieved at $C$. Thus,\n$$\n\\Delta_{1}(f) = C.\n$$\n\nNext, we derive the scale parameter $b$ required for the Laplace mechanism to satisfy $\\epsilon$-DP. Consider the ratio of output densities at a fixed value $z \\in \\mathbb{R}$:\n$$\n\\frac{p_{\\mathcal{M}(D)}(z)}{p_{\\mathcal{M}(D')}(z)} \n= \n\\frac{\\frac{1}{2b}\\exp\\!\\left(-\\frac{|z - f(D)|}{b}\\right)}{\\frac{1}{2b}\\exp\\!\\left(-\\frac{|z - f(D')|}{b}\\right)}\n=\n\\exp\\!\\left(\\frac{|z - f(D')| - |z - f(D)|}{b}\\right).\n$$\nUsing the reverse triangle inequality, $||a|-|b|| \\leq |a-b|$, we obtain\n$$\n\\left||z - f(D')| - |z - f(D)|\\right| \\leq |f(D) - f(D')|.\n$$\nTherefore, for all $z$,\n$$\n\\frac{p_{\\mathcal{M}(D)}(z)}{p_{\\mathcal{M}(D')}(z)} \n\\leq \\exp\\!\\left(\\frac{|f(D) - f(D')|}{b}\\right)\n\\leq \\exp\\!\\left(\\frac{\\Delta_{1}(f)}{b}\\right).\n$$\nTo ensure the DP condition $\\Pr[\\mathcal{M}(D) \\in S] \\leq \\exp(\\epsilon)\\,\\Pr[\\mathcal{M}(D') \\in S]$ for all measurable $S$, it suffices to require\n$$\n\\exp\\!\\left(\\frac{\\Delta_{1}(f)}{b}\\right) \\leq \\exp(\\epsilon),\n$$\nwhich is equivalent to\n$$\n\\frac{\\Delta_{1}(f)}{b} \\leq \\epsilon\n\\quad\\Rightarrow\\quad\nb \\geq \\frac{\\Delta_{1}(f)}{\\epsilon}.\n$$\n\nUsing the derived sensitivity $\\Delta_{1}(f) = C$ and the given parameters $C = 10\\,\\text{mg/dL}$ and $\\epsilon = 0.8$, the minimal scale is\n$$\nb = \\frac{C}{\\epsilon} = \\frac{10}{0.8} = 12.5\\,\\text{mg/dL}.\n$$\nRounded to four significant figures (and expressed in $\\text{mg/dL}$ as required), this is\n$$\nb = 12.50\\,\\text{mg/dL}.\n$$\n\nWe were asked to report only the value of $b$, rounded to four significant figures.",
            "answer": "$$\\boxed{12.50}$$"
        },
        {
            "introduction": "The strength of a privacy guarantee depends critically on how we define \"an individual's data.\" This practice problem explores the crucial distinction between protecting individual events (like a single hospital visit) and protecting all data belonging to a user. By calculating the sensitivity under both event-level and user-level privacy models , you will see firsthand how this choice impacts the amount of noise required and, consequently, the utility of the released data.",
            "id": "4835559",
            "problem": "A health system maintains an Electronic Health Record (EHR) database of visit events, where each visit record contains a patient identifier and a timestamp. Consider a $30$-day reporting window and the following counting query: for a dataset $D$ consisting of visit records in the window, define\n$$\nq(D) = \\sum_{u \\in \\mathcal{U}} \\min\\{v_u(D), K\\},\n$$\nwhere $\\mathcal{U}$ is the set of users (patients) who have at least one visit in the window, $v_u(D)$ is the number of visit records for user $u$ in $D$ during the window, and $K$ is a policy cap that limits each user’s contribution to at most $K$ visits via clipping. Assume all visits have unit weight and that the system enforces clipping at $K = 7$ for each user in the $30$-day window.\n\nTwo notions of adjacency are used for differential privacy (DP). Under event-level adjacency, two datasets $D$ and $D'$ are adjacent if they differ by exactly one visit record. Under user-level adjacency, two datasets $D$ and $D'$ are adjacent if they differ by the complete set of visit records of exactly one user.\n\nStarting from the core definition of global sensitivity for a function $q$, namely\n$$\n\\Delta = \\max_{D \\sim D'} |q(D) - q(D')|,\n$$\nwhere $D \\sim D'$ denotes adjacency under a specified notion, derive the global sensitivities for $q$ under event-level and user-level adjacency in this health data sharing context, and then compute the difference between the user-level global sensitivity and the event-level global sensitivity. Express your final answer as a single real number. No rounding is required.",
            "solution": "The problem asks for the difference between the user-level and event-level global sensitivities for a given counting query. The query is defined as:\n$$\nq(D) = \\sum_{u \\in \\mathcal{U}} \\min\\{v_u(D), K\\}\n$$\nwhere $D$ is a dataset of visit records, $\\mathcal{U}$ is the set of users in the dataset, $v_u(D)$ is the number of visits for user $u$ in $D$, and $K$ is a clipping parameter set to $K=7$.\n\nThe global sensitivity, $\\Delta$, of a function $q$ is defined as the maximum absolute difference in the function's output over all possible pairs of adjacent datasets $D$ and $D'$.\n$$\n\\Delta = \\max_{D \\sim D'} |q(D) - q(D')|\n$$\nThe definition of adjacency, $D \\sim D'$, depends on the privacy model. We will analyze the sensitivity under two such models: event-level and user-level adjacency.\n\nFirst, we derive the event-level global sensitivity, which we denote as $\\Delta_{\\text{event}}$.\nUnder event-level adjacency, two datasets $D$ and $D'$ are adjacent if they differ by exactly one visit record. Let's assume, without loss of generality, that $D' = D \\cup \\{r\\}$, where $r$ is a single visit record for some user $u^*$. The case where $D' = D \\setminus \\{r\\}$ is symmetric.\n\nWhen adding the record $r$ for user $u^*$, the visit count for any other user $u \\neq u^*$ remains unchanged, i.e., $v_u(D') = v_u(D)$. The visit count for user $u^*$ increases by one: $v_{u^*}(D') = v_{u^*}(D) + 1$. The difference in the query output is therefore determined solely by the change in the term for user $u^*$.\n$$\n|q(D') - q(D)| = \\left| \\left( \\sum_{u \\neq u^*} \\min\\{v_u(D'), K\\} + \\min\\{v_{u^*}(D'), K\\} \\right) - \\left( \\sum_{u \\neq u^*} \\min\\{v_u(D), K\\} + \\min\\{v_{u^*}(D), K\\} \\right) \\right|\n$$\n$$\n|q(D') - q(D)| = |\\min\\{v_{u^*}(D'), K\\} - \\min\\{v_{u^*}(D), K\\}|\n$$\nSubstituting $v_{u^*}(D') = v_{u^*}(D) + 1$, we get:\n$$\n|q(D') - q(D)| = |\\min\\{v_{u^*}(D) + 1, K\\} - \\min\\{v_{u^*}(D), K\\}|\n$$\nTo find the global sensitivity, we must maximize this quantity over all possible datasets $D$, which is equivalent to maximizing over all possible initial visit counts $v_{u^*}(D)$. Let $v = v_{u^*}(D)$, where $v$ is a non-negative integer. We analyze the expression $|\\min\\{v+1, K\\} - \\min\\{v, K\\}|$ for $K=7$.\n\nCase 1: $v \\ge K$. In this case, $v \\ge 7$. Then $v+1 > K$.\nThe difference is $|\\min\\{v+1, 7\\} - \\min\\{v, 7\\}| = |7 - 7| = 0$.\n\nCase 2: $v < K$. In this case, $v \\le 6$.\nIf $v = K-1 = 6$, the difference is $|\\min\\{6+1, 7\\} - \\min\\{6, 7\\}| = |\\min\\{7, 7\\} - \\min\\{6, 7\\}| = |7 - 6| = 1$.\nIf $v < K-1$, so $v \\le 5$, then $v+1 < K=7$. The difference is $|\\min\\{v+1, 7\\} - \\min\\{v, 7\\}| = |(v+1) - v| = 1$.\n\nThe maximum possible change when adding or removing a single record is therefore $1$. Thus, the event-level global sensitivity is:\n$$\n\\Delta_{\\text{event}} = 1\n$$\n\nNext, we derive the user-level global sensitivity, denoted as $\\Delta_{\\text{user}}$.\nUnder user-level adjacency, two datasets $D$ and $D'$ are adjacent if they differ by the complete set of visit records of a single user. Let's analyze the maximum possible change by considering the addition of a user $u^*$ to a dataset. Let $D$ be a dataset that contains no records for user $u^*$, so $v_{u^*}(D)=0$. Let $D'$ be the dataset $D$ plus all of the visit records for user $u^*$. The number of visits for this user in $D'$ is $v_{u^*}(D') = n$, where $n$ can be any integer greater than or equal to $1$. The visit counts for all other users $u \\neq u^*$ are unaffected.\n\nThe change in the query output is:\n$$\n|q(D') - q(D)| = |\\min\\{v_{u^*}(D'), K\\} - \\min\\{v_{u^*}(D), K\\}|\n$$\nSubstituting $v_{u^*}(D')=n$ and $v_{u^*}(D)=0$:\n$$\n|q(D') - q(D)| = |\\min\\{n, K\\} - \\min\\{0, K\\}| = |\\min\\{n, K\\} - 0| = \\min\\{n, K\\}\n$$\nTo find the global sensitivity, we must maximize this quantity over all possible adjacent datasets $D$ and $D'$. This is equivalent to choosing the number of visits $n$ for the user $u^*$ that maximizes $\\min\\{n, K\\}$. Since the number of visits $n$ for a user in the $30$-day window is not bounded by the problem statement, we can assume $n$ can be arbitrarily large. The function $\\min\\{n, K\\}$ increases with $n$ until $n=K$, and then remains constant at $K$ for all $n \\ge K$.\nThe maximum value of $\\min\\{n, K\\}$ is $K$. This maximum is achieved for any user with $n \\ge K$ visits.\n\nTherefore, the user-level global sensitivity is:\n$$\n\\Delta_{\\text{user}} = K\n$$\nGiven the policy cap $K=7$, we have:\n$$\n\\Delta_{\\text{user}} = 7\n$$\n\nFinally, the problem asks for the difference between the user-level global sensitivity and the event-level global sensitivity.\n$$\n\\text{Difference} = \\Delta_{\\text{user}} - \\Delta_{\\text{event}} = 7 - 1 = 6\n$$\nThe resulting difference is a single real number.",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "Real-world data systems must answer many queries, not just one, which raises a critical question: how does privacy loss accumulate over multiple releases? This exercise introduces the Basic Composition Theorem, a rule that shows how the total privacy cost, $\\epsilon$, adds up, and challenges you to use it to manage a \"privacy budget\" for a health surveillance dashboard . This concept is fundamental to building sustainable, privacy-preserving data systems.",
            "id": "4835438",
            "problem": "A regional health agency operates a public surveillance dashboard that releases a fixed set of aggregate statistics computed from a protected patient database each day. Each released statistic is a counting query (for example, the number of new influenza admissions in the past $1$ day), which has global sensitivity $\\Delta f = 1$. To protect individuals, the agency uses the Laplace mechanism for each statistic, calibrated to a per-release privacy parameter $\\epsilon_{0}$, meaning the noise scale is $b = \\Delta f / \\epsilon_{0}$.\n\nStarting from the definition of Differential Privacy (DP): a randomized mechanism $M$ is $\\epsilon$-Differentially Private (DP) if for any pair of neighboring datasets $D$ and $D'$ and any measurable set $S$ of outputs, $\\Pr[M(D) \\in S] \\le \\exp(\\epsilon)\\,\\Pr[M(D') \\in S]$, derive how the cumulative privacy parameter $\\epsilon_{\\text{tot}}$ grows under simple composition when $k$ statistics are released sequentially and possibly adaptively from the same dataset (i.e., each release may depend on previous releases). Then, apply your derivation to determine the maximum number $k$ of distinct statistics that can be released per day if the per-release privacy parameter is $\\epsilon_{0} = 0.05$ and the daily privacy budget is $\\epsilon_{\\text{budget}} = 1.2$.\n\nReport only the maximum integer $k$ as your final answer. No units are required. If rounding were necessary, round to the nearest integer; however, if the value is exactly an integer, report that integer exactly.",
            "solution": "**Derivation of Cumulative Privacy Loss under Composition**\n\nLet there be a sequence of $k$ randomized mechanisms, $M_1, M_2, \\dots, M_k$. Each mechanism $M_i$ is assumed to be $\\epsilon_0$-differentially private. These mechanisms are applied sequentially to the same database $D$. The composition can be adaptive, meaning that the choice of mechanism $M_i$ (for $i > 1$) can depend on the outputs of the previous mechanisms $M_1, \\dots, M_{i-1}$. Let the sequence of outputs be $o_1, o_2, \\dots, o_k$, where $o_i$ is the output of $M_i$.\n\nThe composite mechanism, $M_{\\text{comp}}(D)$, produces the tuple of outputs $O = (o_1, o_2, \\dots, o_k)$. We want to determine the total privacy parameter, $\\epsilon_{\\text{tot}}$, for $M_{\\text{comp}}$.\n\nLet $D$ and $D'$ be two neighboring datasets, differing in at most one individual's record. Let $p(O|D)$ and $p(O|D')$ be the probability density (or mass) functions for the output tuple $O$ when the mechanism is run on dataset $D$ and $D'$, respectively.\n\nUsing the chain rule of probability, we can write the joint probability density as a product of conditional probabilities:\n$$p(O|D) = p(o_1|D) \\cdot p(o_2|D, o_1) \\cdot p(o_3|D, o_1, o_2) \\cdots p(o_k|D, o_1, \\dots, o_{k-1})$$\nAnd similarly for dataset $D'$:\n$$p(O|D') = p(o_1|D') \\cdot p(o_2|D', o_1) \\cdot p(o_3|D', o_1, o_2) \\cdots p(o_k|D', o_1, \\dots, o_{k-1})$$\n\nNow, we analyze the ratio of these two probabilities:\n$$\\frac{p(O|D)}{p(O|D')} = \\left(\\frac{p(o_1|D)}{p(o_1|D')}\\right) \\cdot \\left(\\frac{p(o_2|D, o_1)}{p(o_2|D', o_1)}\\right) \\cdots \\left(\\frac{p(o_k|D, o_1, \\dots, o_{k-1})}{p(o_k|D', o_1, \\dots, o_{k-1})}\\right)$$\n\nFor the first mechanism, $M_1$, because it is $\\epsilon_0$-DP, we have for any possible output $o_1$:\n$$\\frac{p(o_1|D)}{p(o_1|D')} \\le \\exp(\\epsilon_0)$$\nFor the second mechanism, $M_2$, the $\\epsilon_0$-DP guarantee holds for any fixed conditioning information. The previous output $o_1$ can be treated as public, auxiliary information. Therefore, for any $o_2$:\n$$\\frac{p(o_2|D, o_1)}{p(o_2|D', o_1)} \\le \\exp(\\epsilon_0)$$\nThis reasoning extends to all subsequent mechanisms. For any mechanism $M_i$ in the sequence, its $\\epsilon_0$-DP guarantee holds conditional on the outputs of the previous mechanisms $o_1, \\dots, o_{i-1}$:\n$$\\frac{p(o_i|D, o_1, \\dots, o_{i-1})}{p(o_i|D', o_1, \\dots, o_{i-1})} \\le \\exp(\\epsilon_0)$$\nSubstituting these individual bounds back into the product for the ratio of joint probabilities:\n$$\\frac{p(O|D)}{p(O|D')} \\le \\exp(\\epsilon_0) \\cdot \\exp(\\epsilon_0) \\cdots \\exp(\\epsilon_0) \\quad (k \\text{ times})$$\n$$\\frac{p(O|D)}{p(O|D')} \\le \\left(\\exp(\\epsilon_0)\\right)^k = \\exp(k \\cdot \\epsilon_0)$$\nThis inequality must hold for any pair of neighboring datasets $D, D'$ and any possible sequence of outputs $O$. This is the definition of $(k \\cdot \\epsilon_0)$-differential privacy. Therefore, the cumulative privacy parameter for the composite mechanism is:\n$$\\epsilon_{\\text{tot}} = k \\cdot \\epsilon_0$$\nThis result is known as the Basic Composition Theorem of differential privacy.\n\n**Application to the Problem**\n\nThe problem states that a total daily privacy budget of $\\epsilon_{\\text{budget}} = 1.2$ must not be exceeded. The cumulative privacy loss after $k$ releases, $\\epsilon_{\\text{tot}}$, must be less than or equal to this budget.\n$$\\epsilon_{\\text{tot}} \\le \\epsilon_{\\text{budget}}$$\nUsing the composition rule derived above:\n$$k \\cdot \\epsilon_0 \\le \\epsilon_{\\text{budget}}$$\nWe are given the per-release privacy parameter $\\epsilon_0 = 0.05$. We need to find the maximum integer number of statistics, $k$, that can be released.\n$$k \\le \\frac{\\epsilon_{\\text{budget}}}{\\epsilon_0}$$\nSubstituting the given numerical values:\n$$k \\le \\frac{1.2}{0.05}$$\nTo compute the fraction, we can multiply the numerator and denominator by $100$:\n$$k \\le \\frac{120}{5}$$\n$$k \\le 24$$\nSince $k$ must be an integer, the maximum number of statistics that can be released per day is $24$.",
            "answer": "$$\\boxed{24}$$"
        }
    ]
}