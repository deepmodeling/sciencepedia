{
    "hands_on_practices": [
        {
            "introduction": "Before a machine learning model can learn to identify clinical entities, we must first translate human annotations into a precise, token-level format. The Begin, Inside, Last, Outside, Unit-length (BILOU) scheme is a powerful method for this, as it explicitly marks the boundaries of single and multi-word entities. This practice will give you hands-on experience applying the BILOU scheme to a clinical snippet, a fundamental skill for preparing data for NER model training and evaluation. ",
            "id": "4849564",
            "problem": "In clinical Natural Language Processing, Named Entity Recognition (NER) identifies spans of text referring to clinically meaningful concepts such as problems, tests, and medications. A widely used tagging scheme for span-level labeling is the Begin, Inside, Last, Outside, Unit-length (BILOU) scheme, which assigns a category letter to each token depending on its position in an entity: Begin for the first token of a multi-token entity, Inside for any interior token of a multi-token entity, Last for the final token of a multi-token entity, Outside for tokens not part of any entity, and Unit-length for a single-token entity. In this problem, you will apply the BILOU scheme to a tokenized clinical note snippet with entity spans annotated at the text level and then compute a numeric functional summary of the resulting category sequence.\n \nUse the following foundations:\n- Tokens are produced by splitting on whitespace and treating punctuation marks such as periods as separate tokens.\n- Braced annotations are meta-markup and not part of the token stream; they demarcate entity spans and their types. The text inside braces starts with the entity type name followed by a space and then the exact span tokens. For example, “{Problem chest pain}” marks a “Problem” entity spanning “chest pain” across two tokens.\n- The BILOU scheme assigns category letters at the token level, conditioned on each span’s exact token boundaries. Use the entity types only to decide span membership and to ensure correct BILOU position tagging; for the numeric computation below, you will use only the category letters and ignore the entity types.\n\nConsider the snippet (with annotations) from a clinical note:\nPt reports {Problem chest pain}. Started {Medication aspirin} {Dose 81 mg} daily. {Test ECG} normal. {Test CT chest} negative for {Problem PE}. Discharged home.\n\nAcronym definitions used in the snippet include Electrocardiogram (ECG), Computed Tomography (CT), and Pulmonary Embolism (PE).\n\nTask:\n1. Tokenize the snippet as specified and apply the BILOU scheme to produce the per-token category letters $c_{1}, c_{2}, \\dots, c_{N} \\in \\{\\text{B}, \\text{I}, \\text{L}, \\text{O}, \\text{U}\\}$, where $N$ is the number of tokens.\n2. Define the weight function $w$ on category letters by $w(\\text{B})=2$, $w(\\text{I})=3$, $w(\\text{L})=5$, $w(\\text{O})=0$, and $w(\\text{U})=7$.\n3. Compute the scalar\n$$\nS \\;=\\; \\sum_{i=1}^{N} i \\cdot w(c_{i}).\n$$\n\nReport the value of $S$ as an integer with no units. No rounding is necessary. Clearly show how single-token entities are represented under the BILOU scheme when deriving $c_{1},\\dots,c_{N}$.",
            "solution": "The problem statement is validated as sound and well-posed. It presents a clear, procedural task within the domain of computational linguistics and medical informatics, specifically concerning Named Entity Recognition (NER). All terms are defined, the given data are consistent, and the required computation is unambiguous. The problem is scientifically grounded, formalizable, and verifiable.\n\nThe solution proceeds in three steps: tokenization of the clinical note, application of the BILOU tagging scheme, and computation of the weighted sum $S$.\n\n### Step 1: Tokenization\n\nThe input snippet is:\n`Pt reports {Problem chest pain}. Started {Medication aspirin} {Dose 81 mg} daily. {Test ECG} normal. {Test CT chest} negative for {Problem PE}. Discharged home.`\n\nThe text content, excluding the meta-markup for annotations, is:\n`Pt reports chest pain. Started aspirin 81 mg daily. ECG normal. CT chest negative for PE. Discharged home.`\n\nAccording to the specified rules, this text is tokenized by splitting on whitespace, and treating punctuation marks (periods in this case) as separate tokens. This process yields a sequence of $N=23$ tokens. The 1-indexed list of tokens, $t_i$, is as follows:\n\n- $t_1$: `Pt`\n- $t_2$: `reports`\n- $t_3$: `chest`\n- $t_4$: `pain`\n- $t_5$: `.`\n- $t_6$: `Started`\n- $t_7$: `aspirin`\n- $t_8$: `81`\n- $t_9$: `mg`\n- $t_{10}$: `daily`\n- $t_{11}$: `.`\n- $t_{12}$: `ECG`\n- $t_{13}$: `normal`\n- $t_{14}$: `.`\n- $t_{15}$: `CT`\n- $t_{16}$: `chest`\n- $t_{17}$: `negative`\n- $t_{18}$: `for`\n- $t_{19}$: `PE`\n- $t_{20}$: `.`\n- $t_{21}$: `Discharged`\n- $t_{22}$: `home`\n- $t_{23}$: `.`\n\n### Step 2: BILOU Tagging\n\nThe BILOU tagging scheme assigns a category letter $c_i \\in \\{\\text{B}, \\text{I}, \\text{L}, \\text{O}, \\text{U}\\}$ to each token $t_i$. The tag depends on the token's position within an annotated entity span.\n- **B**: Begin token of a multi-token entity.\n- **I**: Inside token of a multi-token entity.\n- **L**: Last token of a multi-token entity.\n- **O**: Outside of any entity.\n- **U**: Unit-length (single-token) entity.\n\nThe annotated entity spans and their corresponding token indices are:\n- `{Problem chest pain}`: spans tokens $t_3$ (`chest`) and $t_4$ (`pain`).\n- `{Medication aspirin}`: spans token $t_7$ (`aspirin`).\n- `{Dose 81 mg}`: spans tokens $t_8$ (`81`) and $t_9$ (`mg`).\n- `{Test ECG}`: spans token $t_{12}$ (`ECG`).\n- `{Test CT chest}`: spans tokens $t_{15}$ (`CT`) and $t_{16}$ (`chest`).\n- `{Problem PE}`: spans token $t_{19}$ (`PE`).\n\nApplying the BILOU rules to this token sequence:\n- For `{Problem chest pain}` ($t_3, t_4$): $t_3$ is the first token, so its category is $c_3 = \\text{B}$. $t_4$ is the last token, so its category is $c_4 = \\text{L}$.\n- For `{Medication aspirin}` ($t_7$): This is a single-token entity. As specified, its category is $c_7 = \\text{U}$.\n- For `{Dose 81 mg}` ($t_8, t_9$): $t_8$ is the first token, so $c_8 = \\text{B}$. $t_9$ is the last token, so $c_9 = \\text{L}$.\n- For `{Test ECG}` ($t_{12}$): This is a single-token entity, so its category is $c_{12} = \\text{U}$.\n- For `{Test CT chest}` ($t_{15}, t_{16}$): $t_{15}$ is the first token, so $c_{15} = \\text{B}$. $t_{16}$ is the last token, so $c_{16} = \\text{L}$.\n- For `{Problem PE}` ($t_{19}$): This is a single-token entity, so its category is $c_{19} = \\text{U}$.\n- All other tokens are not part of any annotated entity. Therefore, their category is $\\text{O}$.\n\n### Step 3: Computation of S\n\nThe scalar $S$ is defined as $S = \\sum_{i=1}^{N} i \\cdot w(c_i)$, where weights are given by $w(\\text{B})=2$, $w(\\text{I})=3$, $w(\\text{L})=5$, $w(\\text{O})=0$, and $w(\\text{U})=7$. Since $w(\\text{O})=0$, tokens with category $\\text{O}$ do not contribute to the sum. We only need to sum the terms for tokens with categories $\\text{B}$, $\\text{L}$, or $\\text{U}$.\n\nThe following table summarizes the non-zero contributions to the sum:\n| Index $i$ | Token $t_i$ | Category $c_i$ | Weight $w(c_i)$ | Term $i \\cdot w(c_i)$ |\n|:---:|:---|:---:|:---:|:---:|\n| $3$ | `chest` | $\\text{B}$ | $2$ | $3 \\times 2 = 6$ |\n| $4$ | `pain` | $\\text{L}$ | $5$ | $4 \\times 5 = 20$ |\n| $7$ | `aspirin` | $\\text{U}$ | $7$ | $7 \\times 7 = 49$ |\n| $8$ | `81` | $\\text{B}$ | $2$ | $8 \\times 2 = 16$ |\n| $9$ | `mg` | $\\text{L}$ | $5$ | $9 \\times 5 = 45$ |\n| $12$ | `ECG` | $\\text{U}$ | $7$ | $12 \\times 7 = 84$ |\n| $15$ | `CT` | $\\text{B}$ | $2$ | $15 \\times 2 = 30$ |\n| $16$ | `chest` | $\\text{L}$ | $5$ | $16 \\times 5 = 80$ |\n| $19$ | `PE` | $\\text{U}$ | $7$ | $19 \\times 7 = 133$ |\n\nThe total sum $S$ is the sum of the values in the last column.\n$$\nS = (3 \\cdot w(\\text{B})) + (4 \\cdot w(\\text{L})) + (7 \\cdot w(\\text{U})) + (8 \\cdot w(\\text{B})) + (9 \\cdot w(\\text{L})) + (12 \\cdot w(\\text{U})) + (15 \\cdot w(\\text{B})) + (16 \\cdot w(\\text{L})) + (19 \\cdot w(\\text{U}))\n$$\n$$\nS = (3 \\cdot 2) + (4 \\cdot 5) + (7 \\cdot 7) + (8 \\cdot 2) + (9 \\cdot 5) + (12 \\cdot 7) + (15 \\cdot 2) + (16 \\cdot 5) + (19 \\cdot 7)\n$$\n$$\nS = 6 + 20 + 49 + 16 + 45 + 84 + 30 + 80 + 133\n$$\nSumming these terms:\n$$\nS = 26 + 49 + 16 + 45 + 84 + 30 + 80 + 133\n$$\n$$\nS = 75 + 16 + 45 + 84 + 30 + 80 + 133\n$$\n$$\nS = 91 + 45 + 84 + 30 + 80 + 133\n$$\n$$\nS = 136 + 84 + 30 + 80 + 133\n$$\n$$\nS = 220 + 30 + 80 + 133\n$$\n$$\nS = 250 + 80 + 133\n$$\n$$\nS = 330 + 133\n$$\n$$\nS = 463\n$$\nThe value of the scalar $S$ is $463$.",
            "answer": "$$\n\\boxed{463}\n$$"
        },
        {
            "introduction": "Clinical text is rife with abbreviations and acronyms that can have multiple meanings, a challenge known as word sense disambiguation. For an NER system to be effective, it must infer the correct meaning from context. This practice uses a hypothetical scenario involving the ambiguous acronym \"MS\" to demonstrate how a Naive Bayes classifier, a foundational probabilistic model, leverages contextual cues to make a prediction based on Bayesian principles. ",
            "id": "4849600",
            "problem": "A clinical Named Entity Recognition (NER) system must disambiguate the acronym \"MS\" in Electronic Health Record (EHR) notes between the disease multiple sclerosis and the medication morphine sulfate. Consider a binary classification variable $Y \\in \\{\\text{multiple sclerosis}, \\text{morphine sulfate}\\}$ and a Bernoulli feature vector $\\mathbf{x} = (x_{1}, x_{2}, x_{3}, x_{4}, x_{5})$ extracted from the local sentence context around the occurrence of \"MS.\" Each $x_{i} \\in \\{0,1\\}$ indicates the presence or absence of a specific lexical cue in the same sentence as the acronym. Assume the following features:\n- $x_{1}$: presence of a numeric dosage pattern (for example, the token \"mg\") within three tokens of \"MS\".\n- $x_{2}$: presence of a route-of-administration token (for example, intravenous (IV), by mouth (PO), or frequency such as \"q4h\") within three tokens of \"MS\".\n- $x_{3}$: presence of neurology-specific terms (for example, \"relapse\", \"optic neuritis\", \"demyelinating\", \"MRI lesions\") in the same sentence as \"MS\".\n- $x_{4}$: presence of a medication administration verb (for example, \"titrated\", \"started\", \"administered\") in the same sentence as \"MS\".\n- $x_{5}$: presence of \"pain\" or a numeric pain score in the same sentence as \"MS\".\n\nSuppose the priors over senses in this inpatient corpus are $P(Y=\\text{multiple sclerosis})=0.4$ and $P(Y=\\text{morphine sulfate})=0.6$. Further suppose the following conditional likelihoods for the presence of each feature under a Bernoulli model:\n- $P(x_{1}=1 \\mid Y=\\text{multiple sclerosis})=0.02$, $P(x_{1}=1 \\mid Y=\\text{morphine sulfate})=0.85$.\n- $P(x_{2}=1 \\mid Y=\\text{multiple sclerosis})=0.01$, $P(x_{2}=1 \\mid Y=\\text{morphine sulfate})=0.90$.\n- $P(x_{3}=1 \\mid Y=\\text{multiple sclerosis})=0.75$, $P(x_{3}=1 \\mid Y=\\text{morphine sulfate})=0.05$.\n- $P(x_{4}=1 \\mid Y=\\text{multiple sclerosis})=0.05$, $P(x_{4}=1 \\mid Y=\\text{morphine sulfate})=0.60$.\n- $P(x_{5}=1 \\mid Y=\\text{multiple sclerosis})=0.10$, $P(x_{5}=1 \\mid Y=\\text{morphine sulfate})=0.55$.\n\nYou are asked to disambiguate a particular occurrence of \"MS\" appearing in the sentence: “MS 2 mg IV q4h, titrated for pain control.” The extracted features for this sentence are $\\mathbf{x} = (1, 1, 0, 1, 1)$.\n\nAssume conditional independence of features given the class (the Naive Bayes assumption) and use Bayes’ theorem as the foundational starting point. Derive the posterior probability $P(Y=\\text{multiple sclerosis} \\mid \\mathbf{x})$ from first principles and compute its numerical value. Express the final probability as a decimal and round your answer to four significant figures.",
            "solution": "The problem is valid. It is a well-defined application of Naive Bayes classification, a fundamental concept in probability theory and machine learning. All necessary data, including prior probabilities and conditional likelihoods, are provided, and there are no internal contradictions or scientific inaccuracies.\n\nThe objective is to compute the posterior probability $P(Y=\\text{multiple sclerosis} \\mid \\mathbf{x})$ for a given feature vector $\\mathbf{x} = (1, 1, 0, 1, 1)$. To simplify the notation, let $Y_{MS}$ represent the class 'multiple sclerosis' and $Y_{MP}$ represent the class 'morphine sulfate'. We are asked to calculate $P(Y=Y_{MS} \\mid \\mathbf{x})$.\n\nThe derivation begins with Bayes' theorem, which states:\n$$ P(Y=Y_{MS} \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid Y=Y_{MS}) P(Y=Y_{MS})}{P(\\mathbf{x})} $$\nThe term $P(Y=Y_{MS} \\mid \\mathbf{x})$ is the posterior probability of the class given the evidence. The term $P(\\mathbf{x} \\mid Y=Y_{MS})$ is the likelihood of the evidence given the class. The term $P(Y=Y_{MS})$ is the prior probability of the class. The term $P(\\mathbf{x})$ is the marginal probability of the evidence.\n\nThe marginal probability of the evidence, $P(\\mathbf{x})$, can be calculated using the law of total probability by summing over all possible classes:\n$$ P(\\mathbf{x}) = \\sum_{y \\in \\{Y_{MS}, Y_{MP}\\}} P(\\mathbf{x} \\mid Y=y) P(Y=y) $$\n$$ P(\\mathbf{x}) = P(\\mathbf{x} \\mid Y=Y_{MS}) P(Y=Y_{MS}) + P(\\mathbf{x} \\mid Y=Y_{MP}) P(Y=Y_{MP}) $$\nSubstituting this into Bayes' theorem gives:\n$$ P(Y=Y_{MS} \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid Y=Y_{MS}) P(Y=Y_{MS})}{P(\\mathbf{x} \\mid Y=Y_{MS}) P(Y=Y_{MS}) + P(\\mathbf{x} \\mid Y=Y_{MP}) P(Y=Y_{MP})} $$\nThe problem states to assume conditional independence of features given the class (the Naive Bayes assumption). This allows us to express the likelihood term $P(\\mathbf{x} \\mid Y)$ as a product of individual feature likelihoods:\n$$ P(\\mathbf{x} \\mid Y) = P(x_1, x_2, x_3, x_4, x_5 \\mid Y) = \\prod_{i=1}^{5} P(x_i \\mid Y) $$\nThe given feature vector is $\\mathbf{x} = (1, 1, 0, 1, 1)$. The probabilities for $x_i=1$ are provided. Since each feature $x_i$ is a Bernoulli variable, the probability for $x_i=0$ is given by $P(x_i=0 \\mid Y) = 1 - P(x_i=1 \\mid Y)$. For our specific feature vector, we need $P(x_3=0 \\mid Y)$.\n\nFirst, we calculate the likelihood for the class $Y=Y_{MS}$:\nThe necessary conditional probabilities are:\n$P(x_1=1 \\mid Y_{MS}) = 0.02$\n$P(x_2=1 \\mid Y_{MS}) = 0.01$\n$P(x_3=0 \\mid Y_{MS}) = 1 - P(x_3=1 \\mid Y_{MS}) = 1 - 0.75 = 0.25$\n$P(x_4=1 \\mid Y_{MS}) = 0.05$\n$P(x_5=1 \\mid Y_{MS}) = 0.10$\n\nThe likelihood is the product of these probabilities:\n$$ P(\\mathbf{x}=(1,1,0,1,1) \\mid Y_{MS}) = (0.02) \\times (0.01) \\times (0.25) \\times (0.05) \\times (0.10) $$\n$$ P(\\mathbf{x} \\mid Y_{MS}) = (2 \\times 10^{-2}) \\times (1 \\times 10^{-2}) \\times (0.25) \\times (5 \\times 10^{-2}) \\times (0.1) = 2.5 \\times 10^{-7} $$\n\nNext, we calculate the likelihood for the class $Y=Y_{MP}$:\nThe necessary conditional probabilities are:\n$P(x_1=1 \\mid Y_{MP}) = 0.85$\n$P(x_2=1 \\mid Y_{MP}) = 0.90$\n$P(x_3=0 \\mid Y_{MP}) = 1 - P(x_3=1 \\mid Y_{MP}) = 1 - 0.05 = 0.95$\n$P(x_4=1 \\mid Y_{MP}) = 0.60$\n$P(x_5=1 \\mid Y_{MP}) = 0.55$\n\nThe likelihood is the product of these probabilities:\n$$ P(\\mathbf{x}=(1,1,0,1,1) \\mid Y_{MP}) = (0.85) \\times (0.90) \\times (0.95) \\times (0.60) \\times (0.55) $$\n$$ P(\\mathbf{x} \\mid Y_{MP}) = 0.2398275 $$\n\nNow we can compute the terms in the Bayes' formula. The priors are given as $P(Y_{MS}) = 0.4$ and $P(Y_{MP}) = 0.6$.\n\nThe numerator is $P(\\mathbf{x} \\mid Y_{MS}) P(Y_{MS})$:\n$$ P(\\mathbf{x} \\mid Y_{MS}) P(Y_{MS}) = (2.5 \\times 10^{-7}) \\times 0.4 = 1.0 \\times 10^{-7} $$\n\nThe denominator is the marginal probability of the evidence, $P(\\mathbf{x})$:\n$$ P(\\mathbf{x}) = P(\\mathbf{x} \\mid Y_{MS}) P(Y_{MS}) + P(\\mathbf{x} \\mid Y_{MP}) P(Y_{MP}) $$\n$$ P(\\mathbf{x}) = (1.0 \\times 10^{-7}) + (0.2398275 \\times 0.6) $$\n$$ P(\\mathbf{x}) = 1.0 \\times 10^{-7} + 0.1438965 $$\n$$ P(\\mathbf{x}) = 0.1438966 $$\n\nFinally, we compute the posterior probability:\n$$ P(Y=Y_{MS} \\mid \\mathbf{x}) = \\frac{1.0 \\times 10^{-7}}{0.1438966} $$\n$$ P(Y=Y_{MS} \\mid \\mathbf{x}) \\approx 6.949450 \\times 10^{-7} $$\n\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $6$, $9$, $4$, and $9$. The fifth significant digit is $4$, which is less than $5$, so we do not round up the last digit.\n\nThe final numerical value is $6.949 \\times 10^{-7}$.",
            "answer": "$$\\boxed{6.949 \\times 10^{-7}}$$"
        },
        {
            "introduction": "Evaluating an NER system is more complex than simply marking predictions as right or wrong, especially when dealing with entities that span multiple words. The choice of evaluation metric can significantly change our perception of a model's performance. This practice will guide you through calculating and comparing strict and relaxed F1 scores, illustrating how they penalize different types of errors and providing a deeper understanding of how to assess an NER model's real-world utility. ",
            "id": "4849523",
            "problem": "A clinical note has been tokenized into indexed tokens. A human-annotated gold standard set of entities with types and token spans is provided, along with a model’s predicted entities. Consider two evaluation regimes for Named Entity Recognition (NER): strict matching and relaxed matching. Use the following scientifically grounded definitions as the base of your derivation: True Positive (TP) counts a predicted entity that correctly matches a gold entity under the regime’s matching rule; False Positive (FP) counts a predicted entity that does not match any gold entity under the regime’s rule; False Negative (FN) counts a gold entity that does not have any matching predicted entity under the regime’s rule. Precision $P$ is defined as $P=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$, Recall $R$ is defined as $R=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$, and the $F1$ score is defined as the harmonic mean $F1=\\frac{2PR}{P+R}$.\n\nMatch definitions are as follows. Under strict matching, a predicted entity is considered a match if and only if its type exactly equals the gold entity’s type and its token span is identical to the gold entity’s span. Under relaxed matching, a predicted entity is considered a match if and only if its type equals the gold entity’s type and its token span overlaps the gold entity’s span in at least one token; apply a one-to-one constraint so that each gold entity can be matched to at most one predicted entity and each predicted entity can be matched to at most one gold entity. When multiple overlapping candidates exist, assume the evaluation selects matches to maximize the number of true positives.\n\nThe gold entities are:\n- $G_1$: type $\\text{Problem}$, span $[5,7]$.\n- $G_2$: type $\\text{Test}$, span $[10,11]$.\n- $G_3$: type $\\text{Treatment}$, span $[15,15]$.\n- $G_4$: type $\\text{Problem}$, span $[20,21]$.\n- $G_5$: type $\\text{Test}$, span $[25,27]$.\n- $G_6$: type $\\text{Treatment}$, span $[30,32]$.\n\nThe predicted entities are:\n- $P_1$: type $\\text{Problem}$, span $[5,7]$.\n- $P_2$: type $\\text{Test}$, span $[10,12]$.\n- $P_3$: type $\\text{Treatment}$, span $[14,15]$.\n- $P_4$: type $\\text{Problem}$, span $[20,22]$.\n- $P_5$: type $\\text{Test}$, span $[25,26]$.\n- $P_6$: type $\\text{Treatment}$, span $[31,32]$.\n- $P_7$: type $\\text{Problem}$, span $[2,3]$.\n- $P_8$: type $\\text{Test}$, span $[40,41]$.\n- $P_9$: type $\\text{Problem}$, span $[25,27]$.\n\nCompute the strict $F1$ score and the relaxed $F1$ score from first principles using the provided definitions and matching rules. Then compute the difference $\\Delta = F1_{\\text{relaxed}} - F1_{\\text{strict}}$. Express your final answer for $\\Delta$ as a reduced fraction. No rounding is required. Additionally, within your solution, explain the principal reason for any discrepancy between the strict and relaxed $F1$ scores in terms of boundary detection and type agreement in clinical NER.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of definitions, data, and constraints required to compute the specified named entity recognition (NER) metrics. The rules for strict and relaxed matching are clearly defined, including the optimization criterion for the relaxed case, ensuring a unique solution can be determined. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe problem requires the calculation of the $F1$ score under two different evaluation regimes: strict and relaxed matching. We are given $N_G = 6$ gold standard entities and $N_P = 9$ predicted entities. We will first calculate the scores for the strict regime, then for the relaxed regime, and finally compute their difference.\n\nThe fundamental quantities are True Positives ($TP$), False Positives ($FP$), and False Negatives ($FN$). From these, we compute Precision ($P$) and Recall ($R$):\n$$P = \\frac{TP}{TP+FP}$$\n$$R = \\frac{TP}{TP+FN}$$\nThe $F1$ score is the harmonic mean of Precision and Recall:\n$$F1 = \\frac{2PR}{P+R} = \\frac{2 \\cdot \\frac{TP}{TP+FP} \\cdot \\frac{TP}{TP+FN}}{\\frac{TP}{TP+FP} + \\frac{TP}{TP+FN}} = \\frac{2(TP)^2}{TP(TP+FN) + TP(TP+FP)} = \\frac{2TP}{2TP+FP+FN}$$\n\nFirst, we analyze the performance under the **strict matching** regime, which requires that both the entity type and the token span be identical for a match to occur.\n\nWe compare each predicted entity ($P_i$) against the set of gold entities ($G_j$):\n- $P_1$ (type $\\text{Problem}$, span $[5,7]$) is an exact match for $G_1$ (type $\\text{Problem}$, span $[5,7]$). This is a True Positive.\n- $P_2$ (type $\\text{Test}$, span $[10,12]$) has the same type as $G_2$ (type $\\text{Test}$, span $[10,11]$), but the span is different. Not a strict match.\n- $P_3$ (type $\\text{Treatment}$, span $[14,15]$) has the same type as $G_3$ (type $\\text{Treatment}$, span $[15,15]$), but the span is different. Not a strict match.\n- $P_4$ (type $\\text{Problem}$, span $[20,22]$) has the same type as $G_4$ (type $\\text{Problem}$, span $[20,21]$), but the span is different. Not a strict match.\n- $P_5$ (type $\\text{Test}$, span $[25,26]$) has the same type as $G_5$ (type $\\text{Test}$, span $[25,27]$), but the span is different. Not a strict match.\n- $P_6$ (type $\\text{Treatment}$, span $[31,32]$) has the same type as $G_6$ (type $\\text{Treatment}$, span $[30,32]$), but the span is different. Not a strict match.\n- $P_7$ (type $\\text{Problem}$, span $[2,3]$) does not correspond to any gold entity. Not a match.\n- $P_8$ (type $\\text{Test}$, span $[40,41]$) does not correspond to any gold entity. Not a match.\n- $P_9$ (type $\\text{Problem}$, span $[25,27]$) has the same span as $G_5$ (type $\\text{Test}$, span $[25,27]$), but the type is different ($\\text{Problem}$ vs. $\\text{Test}$). Not a strict match.\n\nBased on this analysis for strict matching:\n- The number of True Positives is $TP_{\\text{strict}} = 1$ (from the match between $P_1$ and $G_1$).\n- The total number of predictions is $N_P = 9$. The number of False Positives is the count of predicted entities that are not TPs: $FP_{\\text{strict}} = N_P - TP_{\\text{strict}} = 9 - 1 = 8$.\n- The total number of gold entities is $N_G = 6$. The number of False Negatives is the count of gold entities that are not matched: $FN_{\\text{strict}} = N_G - TP_{\\text{strict}} = 6 - 1 = 5$.\n\nNow we can compute the strict precision, recall, and $F1$ score:\n$P_{\\text{strict}} = \\frac{TP_{\\text{strict}}}{TP_{\\text{strict}} + FP_{\\text{strict}}} = \\frac{1}{1+8} = \\frac{1}{9}$\n$R_{\\text{strict}} = \\frac{TP_{\\text{strict}}}{TP_{\\text{strict}} + FN_{\\text{strict}}} = \\frac{1}{1+5} = \\frac{1}{6}$\n$F1_{\\text{strict}} = \\frac{2 P_{\\text{strict}} R_{\\text{strict}}}{P_{\\text{strict}} + R_{\\text{strict}}} = \\frac{2 \\cdot \\frac{1}{9} \\cdot \\frac{1}{6}}{\\frac{1}{9} + \\frac{1}{6}} = \\frac{\\frac{2}{54}}{\\frac{2+3}{18}} = \\frac{\\frac{1}{27}}{\\frac{5}{18}} = \\frac{1}{27} \\cdot \\frac{18}{5} = \\frac{2}{15}$\n\nNext, we analyze the performance under the **relaxed matching** regime. A match occurs if the entity types are identical and their spans overlap by at least one token. The matching is one-to-one and is chosen to maximize the number of TPs.\n\nWe identify all potential matches based on type agreement and span overlap. A span $[a, b]$ overlaps with $[c, d]$ if $\\max(a, c) \\le \\min(b, d)$.\n- $P_1$ (Problem, $[5,7]$) and $G_1$ (Problem, $[5,7]$): Types match, spans overlap. Potential match $(P_1, G_1)$.\n- $P_2$ (Test, $[10,12]$) and $G_2$ (Test, $[10,11]$): Types match, spans overlap. Potential match $(P_2, G_2)$.\n- $P_3$ (Treatment, $[14,15]$) and $G_3$ (Treatment, $[15,15]$): Types match, spans overlap. Potential match $(P_3, G_3)$.\n- $P_4$ (Problem, $[20,22]$) and $G_4$ (Problem, $[20,21]$): Types match, spans overlap. Potential match $(P_4, G_4)$.\n- $P_5$ (Test, $[25,26]$) and $G_5$ (Test, $[25,27]$): Types match, spans overlap. Potential match $(P_5, G_5)$.\n- $P_6$ (Treatment, $[31,32]$) and $G_6$ (Treatment, $[30,32]$): Types match, spans overlap. Potential match $(P_6, G_6)$.\n- $P_7$ (Problem, $[2,3]$): No overlapping gold entity. No match.\n- $P_8$ (Test, $[40,41]$): No overlapping gold entity. No match.\n- $P_9$ (Problem, $[25,27]$) and $G_5$ (Test, $[25,27]$): Spans overlap, but types differ. No match.\n\nThe potential matches are $(P_1, G_1)$, $(P_2, G_2)$, $(P_3, G_3)$, $(P_4, G_4)$, $(P_5, G_5)$, and $(P_6, G_6)$. Each involves a unique predicted entity and a unique gold entity, so there are no conflicts. To maximize the number of TPs, we accept all $6$ of these matches.\n\nBased on this analysis for relaxed matching:\n- The number of True Positives is $TP_{\\text{relaxed}} = 6$.\n- The number of False Positives is $FP_{\\text{relaxed}} = N_P - TP_{\\text{relaxed}} = 9 - 6 = 3$. The FPs are $P_7$, $P_8$, and $P_9$.\n- The number of False Negatives is $FN_{\\text{relaxed}} = N_G - TP_{\\text{relaxed}} = 6 - 6 = 0$.\n\nNow we compute the relaxed precision, recall, and $F1$ score:\n$P_{\\text{relaxed}} = \\frac{TP_{\\text{relaxed}}}{TP_{\\text{relaxed}} + FP_{\\text{relaxed}}} = \\frac{6}{6+3} = \\frac{6}{9} = \\frac{2}{3}$\n$R_{\\text{relaxed}} = \\frac{TP_{\\text{relaxed}}}{TP_{\\text{relaxed}} + FN_{\\text{relaxed}}} = \\frac{6}{6+0} = 1$\n$F1_{\\text{relaxed}} = \\frac{2 P_{\\text{relaxed}} R_{\\text{relaxed}}}{P_{\\text{relaxed}} + R_{\\text{relaxed}}} = \\frac{2 \\cdot \\frac{2}{3} \\cdot 1}{\\frac{2}{3} + 1} = \\frac{\\frac{4}{3}}{\\frac{5}{3}} = \\frac{4}{5}$\n\nFinally, we compute the difference $\\Delta = F1_{\\text{relaxed}} - F1_{\\text{strict}}$:\n$\\Delta = \\frac{4}{5} - \\frac{2}{15} = \\frac{12}{15} - \\frac{2}{15} = \\frac{10}{15}$\nReducing the fraction gives:\n$\\Delta = \\frac{2}{3}$\n\nThe principal reason for the substantial discrepancy between the strict and relaxed $F1$ scores lies in how they evaluate boundary detection versus type agreement. The strict score $F1_{\\text{strict}} = \\frac{2}{15}$ is very low because it penalizes any inaccuracy in boundary detection. In this problem, $5$ out of the $6$ correctly typed predictions ($P_2, P_3, P_4, P_5, P_6$) had minor boundary errors. The strict metric treats each of these as a complete failure, contributing to both a false positive and a false negative, drastically reducing the TP count to $1$. In contrast, the relaxed metric is designed to forgive such boundary inaccuracies. As long as the entity type is correct and the spans overlap, it counts the prediction as a success. This is why the TP count for the relaxed metric, $TP_{\\text{relaxed}}$, rises to $6$, leading to a much higher recall ($R_{\\text{relaxed}}=1$) and precision ($P_{\\text{relaxed}}=\\frac{2}{3}$), and consequently a high $F1_{\\text{relaxed}}$ score of $\\frac{4}{5}$. The discrepancy is therefore almost entirely attributable to the strict metric's intolerance for imperfect boundary detection, a task where models often produce near-misses that the relaxed metric correctly credits. Type agreement is a necessary condition for both regimes, as shown by $P_9$ being an FP in both cases.",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        }
    ]
}