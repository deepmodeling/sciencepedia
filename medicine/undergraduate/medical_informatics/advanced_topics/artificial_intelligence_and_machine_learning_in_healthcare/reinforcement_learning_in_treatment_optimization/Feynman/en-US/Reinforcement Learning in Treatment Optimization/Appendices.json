{
    "hands_on_practices": [
        {
            "introduction": "To begin, we must first understand how to define an optimal treatment strategy when we have a perfect model of the clinical environment. This exercise  asks you to model a simplified acute care scenario as a Markov Decision Process (MDP) and find the best policy using dynamic programming. Mastering this helps build the theoretical foundation for all reinforcement learning, showing how to calculate long-term value in sequential decision-making under certainty.",
            "id": "4855005",
            "problem": "A clinician is designing a Reinforcement Learning (RL) strategy for sequential treatment in an acute care setting. Model the problem as a discounted finite Markov Decision Process (MDP) with states, actions, transition probabilities, and a clinically motivated reward function. The state space consists of $3$ health states: $s_{1}$ (Critical), $s_{2}$ (Stable), and $s_{3}$ (Discharged). The action space consists of $2$ treatment choices available in $s_{1}$ and $s_{2}$: $a_{1}$ (Aggressive) and $a_{2}$ (Conservative). State $s_{3}$ is absorbing with a single passive action that leaves the state unchanged.\n\nThe transition dynamics are as follows for $s_{1}$ (Critical):\n- If action $a_{1}$ (Aggressive) is chosen: $\\mathbb{P}(s_{1} \\rightarrow s_{1}) = 0.2$, $\\mathbb{P}(s_{1} \\rightarrow s_{2}) = 0.6$, $\\mathbb{P}(s_{1} \\rightarrow s_{3}) = 0.2$.\n- If action $a_{2}$ (Conservative) is chosen: $\\mathbb{P}(s_{1} \\rightarrow s_{1}) = 0.5$, $\\mathbb{P}(s_{1} \\rightarrow s_{2}) = 0.4$, $\\mathbb{P}(s_{1} \\rightarrow s_{3}) = 0.1$.\n\nFor $s_{2}$ (Stable):\n- If action $a_{1}$ (Aggressive) is chosen: $\\mathbb{P}(s_{2} \\rightarrow s_{1}) = 0.1$, $\\mathbb{P}(s_{2} \\rightarrow s_{2}) = 0.2$, $\\mathbb{P}(s_{2} \\rightarrow s_{3}) = 0.7$.\n- If action $a_{2}$ (Conservative) is chosen: $\\mathbb{P}(s_{2} \\rightarrow s_{1}) = 0.05$, $\\mathbb{P}(s_{2} \\rightarrow s_{2}) = 0.45$, $\\mathbb{P}(s_{2} \\rightarrow s_{3}) = 0.5$.\n\nState $s_{3}$ (Discharged) is absorbing: $\\mathbb{P}(s_{3} \\rightarrow s_{3}) = 1$.\n\nThe one-step reward $r(s,a,s')$ is defined to encode clinical outcome and treatment side-effects. Upon transitioning to $s' \\in \\{s_{1}, s_{2}, s_{3}\\}$ from a non-absorbing state $s \\in \\{s_{1}, s_{2}\\}$ under action $a \\in \\{a_{1}, a_{2}\\}$, the reward is\n- Outcome component: $+10$ if $s' = s_{3}$ (Discharged), $+2$ if $s' = s_{2}$ (Stable), $-3$ if $s' = s_{1}$ (Critical).\n- Action cost: $-2$ for $a = a_{1}$ (Aggressive), $-0.5$ for $a = a_{2}$ (Conservative).\n\nFor transitions originating from the absorbing state $s_{3}$, all rewards are $0$. The discount factor is $\\gamma = 0.9$.\n\nStarting from the initial state $s_{1}$ (Critical) at time $t=0$, determine the optimal stationary deterministic policy that maximizes the expected discounted sum of rewards and compute the corresponding optimal state-value at $s_{1}$, denoted $V^{\\ast}(s_{1})$.\n\nBase your derivation on first principles for Markov Decision Processes and reinforcement learning, beginning from the definition of expected discounted return and the principle of optimality. You must explicitly justify the optimal policy you choose before computing $V^{\\ast}(s_{1})$.\n\nAnswer specification:\n- Report only the optimal expected discounted value $V^{\\ast}(s_{1})$ as a single real number.\n- Round your final numerical answer to four significant figures.\n- Do not include units in your final numeric answer.",
            "solution": "To find the optimal policy $\\pi^*$ and its corresponding state-values $V^*$, we use the Bellman optimality equations. The optimal value of a state, $V^*(s)$, is the maximum expected return achievable from that state. For the absorbing state $s_3$, the value is $V^*(s_3) = 0$. For the other states, the Bellman optimality equation is:\n$$V^*(s) = \\max_{a \\in A} Q^*(s, a) = \\max_{a \\in A} \\sum_{s'} \\mathbb{P}(s'|s,a) \\left[ r(s,a,s') + \\gamma V^*(s') \\right]$$\nThe reward $r(s,a,s')$ is the sum of an outcome component and an action cost. Let's pre-calculate these total rewards:\n- $r(s, a_1, s_1) = -3 - 2 = -5$\n- $r(s, a_1, s_2) = 2 - 2 = 0$\n- $r(s, a_1, s_3) = 10 - 2 = 8$\n- $r(s, a_2, s_1) = -3 - 0.5 = -3.5$\n- $r(s, a_2, s_2) = 2 - 0.5 = 1.5$\n- $r(s, a_2, s_3) = 10 - 0.5 = 9.5$\n\nThe optimal policy can be found using methods like Value Iteration or Policy Iteration. Let's assume we have found the optimal policy to be $\\pi^*(s_1) = a_1$ (Aggressive) and $\\pi^*(s_2) = a_2$ (Conservative). We can verify this guess by first evaluating this policy (finding $V^{\\pi^*}$) and then checking if any other action improves the value (Policy Improvement).\n\n**Policy Evaluation:**\nFor the policy $\\pi^* = \\{s_1 \\to a_1, s_2 \\to a_2\\}$, the state-values $V^{\\pi^*}(s_1)$ and $V^{\\pi^*}(s_2)$ must satisfy the following system of linear equations derived from the Bellman expectation equation:\n$V(s_1) = 0.2(-5 + \\gamma V(s_1)) + 0.6(0 + \\gamma V(s_2)) + 0.2(8 + \\gamma V(s_3))$\n$V(s_2) = 0.05(-3.5 + \\gamma V(s_1)) + 0.45(1.5 + \\gamma V(s_2)) + 0.5(9.5 + \\gamma V(s_3))$\n\nSubstituting $\\gamma=0.9$ and $V(s_3)=0$:\n$V(s_1) = 0.2(-5 + 0.9 V(s_1)) + 0.6(0.9 V(s_2)) + 0.2(8)$\n$V(s_1) = -1 + 0.18 V(s_1) + 0.54 V(s_2) + 1.6 \\implies (1 - 0.18) V(s_1) - 0.54 V(s_2) = 0.6$\n$0.82 V(s_1) - 0.54 V(s_2) = 0.6$  (1)\n\n$V(s_2) = 0.05(-3.5 + 0.9 V(s_1)) + 0.45(1.5 + 0.9 V(s_2)) + 0.5(9.5)$\n$V(s_2) = -0.175 + 0.045 V(s_1) + 0.675 + 0.405 V(s_2) + 4.75 \\implies -0.045 V(s_1) + (1 - 0.405) V(s_2) = 5.25$\n$-0.045 V(s_1) + 0.595 V(s_2) = 5.25$  (2)\n\nSolving this system of two linear equations:\nFrom (2), $V(s_2) = \\frac{5.25 + 0.045 V(s_1)}{0.595}$.\nSubstitute into (1): $0.82 V(s_1) - 0.54 \\left( \\frac{5.25 + 0.045 V(s_1)}{0.595} \\right) = 0.6$\n$0.82 V(s_1) - \\frac{2.835}{0.595} - \\frac{0.0243 V(s_1)}{0.595} = 0.6$\n$0.82 V(s_1) - 4.76470... - 0.04084... V(s_1) = 0.6$\n$0.779159... V(s_1) = 5.36470...$\n$V(s_1) = \\frac{5.36470...}{0.779159...} \\approx 6.88514$\nUsing this, we find $V(s_2) \\approx \\frac{5.25 + 0.045(6.88514)}{0.595} \\approx 9.3444$.\n\n**Policy Improvement (Verification):**\nNow we must check if our assumed policy is indeed optimal. We check if any other action can yield a better Q-value.\nFor state $s_1$, is $Q(s_1, a_1) \\ge Q(s_1, a_2)$? We already know $Q(s_1, a_1) = V(s_1) \\approx 6.885$.\n$Q(s_1, a_2) = 0.5(-3.5 + 0.9 V(s_1)) + 0.4(1.5 + 0.9 V(s_2)) + 0.1(9.5) \\approx 0.5(-3.5 + 0.9 \\cdot 6.885) + 0.4(1.5 + 0.9 \\cdot 9.344) + 0.95 \\approx 6.262$. Since $6.885 > 6.262$, $a_1$ is the optimal action in $s_1$.\n\nFor state $s_2$, is $Q(s_2, a_2) \\ge Q(s_2, a_1)$? We know $Q(s_2, a_2) = V(s_2) \\approx 9.344$.\n$Q(s_2, a_1) = 0.1(-5 + 0.9 V(s_1)) + 0.2(0 + 0.9 V(s_2)) + 0.7(8) \\approx 0.1(-5 + 0.9 \\cdot 6.885) + 0.2(0.9 \\cdot 9.344) + 5.6 \\approx 7.408$. Since $9.344 > 7.408$, $a_2$ is the optimal action in $s_2$.\n\nThe policy is stable and thus optimal. The optimal state-value for state $s_1$ is $V^*(s_1) \\approx 6.88514$.\nRounding to four significant figures, we get $6.885$.",
            "answer": "$$\\boxed{6.885}$$"
        },
        {
            "introduction": "In reality, we rarely have a perfect model of patient responses. This practice  moves from planning to learning by introducing Temporal-Difference (TD) methods, which learn directly from observed patient trajectories. You will apply the TD($\\lambda$) algorithm to update value estimates, gaining hands-on experience with how RL agents learn from experience and assign credit for outcomes to past actions using eligibility traces.",
            "id": "4855004",
            "problem": "A health system is evaluating a fixed dosing policy for managing a chronic condition using a Markov Decision Process (MDP). The goal is to estimate the state-value function under this policy via Temporal-Difference (TD) learning with eligibility traces, specifically TD($\\lambda$). Consider a single patient episode with the following observed trajectory of visit-level states and rewards (under the fixed policy):\n- At time $t=0$: state $S_{0}=\\text{baseline}$, then reward $R_{1}=0$ and next state $S_{1}=\\text{mild-response}$.\n- At time $t=1$: state $S_{1}=\\text{mild-response}$, then reward $R_{2}=1$ and next state $S_{2}=\\text{adverse-managed}$.\n- At time $t=2$: state $S_{2}=\\text{adverse-managed}$, then reward $R_{3}=2$ and terminal absorption $S_{3}=\\text{terminal}$.\n\nAssume the terminal state has value $V(\\text{terminal})=0$. The initial value estimates before processing this episode are $V(\\text{baseline})=0.5$, $V(\\text{mild-response})=0.8$, and $V(\\text{adverse-managed})=0.3$. Use discount factor $\\gamma=0.9$, trace-decay parameter $\\lambda=0.5$, and learning rate $\\alpha=0.2$. Use on-line TD($\\lambda$) with accumulating eligibility traces for a single left-to-right pass through the episode.\n\nCompute the updated estimate of $V(\\text{baseline})$ immediately after processing this episode once. Round your answer to four significant figures. Express the answer as a pure number with no units.",
            "solution": "The user wants to compute the updated state-value estimate for a specific state, $V(\\text{baseline})$, after processing a single episode using the on-line Temporal-Difference learning algorithm with eligibility traces, known as TD($\\lambda$). The problem provides all necessary parameters and initial conditions.\n\nFirst, we establish the framework and formalize the given information. The state space consists of four states: $S = \\{\\text{baseline}, \\text{mild-response}, \\text{adverse-managed}, \\text{terminal}\\}$. For brevity, we denote these as $s_b, s_m, s_a,$ and $s_T$ respectively.\n\nThe parameters for the TD($\\lambda$) algorithm are:\n- Learning rate: $\\alpha = 0.2$\n- Discount factor: $\\gamma = 0.9$\n- Trace-decay parameter: $\\lambda = 0.5$\n\nThe initial state-value estimates before processing the episode are:\n- $V_0(s_b) = 0.5$\n- $V_0(s_m) = 0.8$\n- $V_0(s_a) = 0.3$\n- The value of the terminal state is fixed: $V(s_T) = 0$.\n\nThe eligibility traces for all states are initialized to zero at the beginning of the episode:\n- $e_0(s_b) = 0$\n- $e_0(s_m) = 0$\n- $e_0(s_a) = 0$\n\nThe observed trajectory for the episode is a sequence of (state, reward, next state) transitions:\n- At $t=0$: $(S_0=s_b, R_1=0, S_1=s_m)$\n- At $t=1$: $(S_1=s_m, R_2=1, S_2=s_a)$\n- At $t=2$: $(S_2=s_a, R_3=2, S_3=s_T)$\n\nThe on-line TD($\\lambda$) algorithm with accumulating traces updates the value estimates and eligibility traces at each step $t$ of the episode. Let $V_t$ and $e_t$ represent the value function and eligibility traces at the beginning of step $t$. The update process for each step $t$ is as follows:\n1.  Calculate the TD error, $\\delta_t$:\n    $$ \\delta_t = R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t) $$\n2.  Increment the eligibility trace for the current state $S_t$. We denote the trace vector after this increment but before the decay as $e'_t$:\n    $$ e'_t(S_t) = e_t(S_t) + 1 $$\n    $$ e'_t(s) = e_t(s) \\quad \\text{for } s \\neq S_t $$\n3.  Update the value estimates for all states $s$ to obtain $V_{t+1}$:\n    $$ V_{t+1}(s) = V_t(s) + \\alpha \\delta_t e'_t(s) $$\n4.  Decay all traces to obtain $e_{t+1}$ for the next step:\n    $$ e_{t+1}(s) = \\gamma \\lambda e'_t(s) $$\n\nWe now apply this process sequentially for each step of the episode.\n\n**Step $t=0$:**\n- State transition: $(S_0=s_b, R_1=0, S_1=s_m)$.\n- Calculate TD error $\\delta_0$:\n  $$ \\delta_0 = R_1 + \\gamma V_0(S_1) - V_0(S_0) = 0 + (0.9)(0.8) - 0.5 = 0.72 - 0.5 = 0.22 $$\n- Update eligibility traces to get $e'_0$:\n  - $e'_0(s_b) = e_0(s_b) + 1 = 0 + 1 = 1$\n  - $e'_0(s_m) = e_0(s_m) = 0$\n  - $e'_0(s_a) = e_0(s_a) = 0$\n- Update value estimates to get $V_1$:\n  - $V_1(s_b) = V_0(s_b) + \\alpha \\delta_0 e'_0(s_b) = 0.5 + (0.2)(0.22)(1) = 0.5 + 0.044 = 0.544$\n  - $V_1(s_m) = V_0(s_m) + \\alpha \\delta_0 e'_0(s_m) = 0.8 + (0.2)(0.22)(0) = 0.8$\n  - $V_1(s_a) = V_0(s_a) + \\alpha \\delta_0 e'_0(s_a) = 0.3 + (0.2)(0.22)(0) = 0.3$\n- Decay traces to get $e_1$:\n  - $e_1(s_b) = \\gamma \\lambda e'_0(s_b) = (0.9)(0.5)(1) = 0.45$\n  - $e_1(s_m) = \\gamma \\lambda e'_0(s_m) = 0$\n  - $e_1(s_a) = \\gamma \\lambda e'_0(s_a) = 0$\n\n**Step $t=1$:**\n- State transition: $(S_1=s_m, R_2=1, S_2=s_a)$.\n- Calculate TD error $\\delta_1$ using the updated values $V_1$:\n  $$ \\delta_1 = R_2 + \\gamma V_1(S_2) - V_1(S_1) = 1 + (0.9)(0.3) - 0.8 = 1 + 0.27 - 0.8 = 0.47 $$\n- Update eligibility traces to get $e'_1$:\n  - $e'_1(s_b) = e_1(s_b) = 0.45$\n  - $e'_1(s_m) = e_1(s_m) + 1 = 0 + 1 = 1$\n  - $e'_1(s_a) = e_1(s_a) = 0$\n- Update value estimates to get $V_2$:\n  - $V_2(s_b) = V_1(s_b) + \\alpha \\delta_1 e'_1(s_b) = 0.544 + (0.2)(0.47)(0.45) = 0.544 + 0.0423 = 0.5863$\n  - $V_2(s_m) = V_1(s_m) + \\alpha \\delta_1 e'_1(s_m) = 0.8 + (0.2)(0.47)(1) = 0.8 + 0.094 = 0.894$\n  - $V_2(s_a) = V_1(s_a) + \\alpha \\delta_1 e'_1(s_a) = 0.3 + (0.2)(0.47)(0) = 0.3$\n- Decay traces to get $e_2$:\n  - $e_2(s_b) = \\gamma \\lambda e'_1(s_b) = (0.9)(0.5)(0.45) = 0.2025$\n  - $e_2(s_m) = \\gamma \\lambda e'_1(s_m) = (0.9)(0.5)(1) = 0.45$\n  - $e_2(s_a) = \\gamma \\lambda e'_1(s_a) = 0$\n\n**Step $t=2$:**\n- State transition: $(S_2=s_a, R_3=2, S_3=s_T)$.\n- Calculate TD error $\\delta_2$ using values $V_2$:\n  $$ \\delta_2 = R_3 + \\gamma V(S_3) - V_2(S_2) = 2 + (0.9)(0) - 0.3 = 1.7 $$\n- Update eligibility traces to get $e'_2$:\n  - $e'_2(s_b) = e_2(s_b) = 0.2025$\n  - $e'_2(s_m) = e_2(s_m) = 0.45$\n  - $e'_2(s_a) = e_2(s_a) + 1 = 0 + 1 = 1$\n- Update value estimates to get $V_3$. We are only required to find the final value for $V(s_b)$:\n  $$ V_3(s_b) = V_2(s_b) + \\alpha \\delta_2 e'_2(s_b) = 0.5863 + (0.2)(1.7)(0.2025) = 0.5863 + (0.34)(0.2025) = 0.5863 + 0.06885 = 0.65515 $$\nThe episode terminates as $S_3$ is a terminal state. The final estimated value for the state 'baseline' after this episode is $V_3(s_b) = 0.65515$.\n\nThe problem requires the answer to be rounded to four significant figures.\n$$ V_3(s_b) \\approx 0.6552 $$",
            "answer": "$$\\boxed{0.6552}$$"
        },
        {
            "introduction": "A critical challenge in medical AI is evaluating a new treatment policy's potential without deploying it in a live clinical setting. This exercise  addresses this by tasking you with Off-Policy Evaluation (OPE), using historical data to estimate the value of a novel policy. By calculating the Doubly Robust (DR) estimate, you will learn a powerful and practical technique that combines model-based predictions and importance sampling to achieve reliable and low-variance policy evaluation from offline electronic health records.",
            "id": "4855025",
            "problem": "Consider a one-step treatment decision problem in Reinforcement Learning (RL) for treatment optimization in Medical Informatics. The clinical context is antibiotic choice for sepsis based on a binary risk group indicator $x \\in \\{0,1\\}$, where $x=1$ denotes high risk and $x=0$ denotes low risk. An Off-Policy Evaluation (OPE) task is to estimate the expected normalized clinical improvement under a deterministic target policy $\\pi$ using retrospective Electronic Health Record (EHR) data generated by a behavior policy $b$. You are given a small dataset of $n=5$ independent patient episodes $\\{(x_i,a_i,y_i)\\}_{i=1}^{5}$, where $x_i$ is the risk group, $a_i \\in \\{\\mathrm{T0},\\mathrm{T1}\\}$ is the antibiotic chosen, and $y_i \\in [0,1]$ is the observed normalized improvement. The deterministic target policy prescribes $\\pi(\\mathrm{T1}\\mid x=1)=1$ for high risk and $\\pi(\\mathrm{T0}\\mid x=0)=1$ for low risk, and $\\pi(\\mathrm{T0}\\mid x=1)=\\pi(\\mathrm{T1}\\mid x=0)=0$.\n\nYou also have an outcome regression model $m(x,a)$ trained from historical data that provides the following action-value predictions:\n- For high risk $x=1$: $m(1,\\mathrm{T1})=0.75$, $m(1,\\mathrm{T0})=0.55$.\n- For low risk $x=0$: $m(0,\\mathrm{T0})=0.68$, $m(0,\\mathrm{T1})=0.52$.\n\nThe following episodes and pre-computed importance weights $w_i=\\pi(a_i\\mid x_i)/b(a_i\\mid x_i)$ are provided:\n- Episode $i=1$: $(x_1=1,\\ a_1=\\mathrm{T1},\\ y_1=0.80)$, with $w_1=\\frac{10}{7}$.\n- Episode $i=2$: $(x_2=1,\\ a_2=\\mathrm{T0},\\ y_2=0.50)$, with $w_2=0$.\n- Episode $i=3$: $(x_3=0,\\ a_3=\\mathrm{T0},\\ y_3=0.70)$, with $w_3=\\frac{5}{3}$.\n- Episode $i=4$: $(x_4=0,\\ a_4=\\mathrm{T1},\\ y_4=0.40)$, with $w_4=0$.\n- Episode $i=5$: $(x_5=1,\\ a_5=\\mathrm{T1},\\ y_5=0.60)$, with $w_5=\\frac{10}{7}$.\n\nUsing only fundamental definitions of policy value, importance sampling, and the law of total expectation, derive an expression for the one-step Doubly Robust (DR) estimator of the target policy value $V(\\pi)$ and then compute its empirical estimate on the given dataset. Clearly state the value you compute. Round your final numerical answer to four significant figures. Express your answer as a unitless real number.",
            "solution": "The user has requested the derivation and computation of the Doubly Robust (DR) estimator for the value of a target policy, $V(\\pi)$, in a one-step off-policy evaluation setting.\n\nFirst, we will derive the general form of the Doubly Robust estimator. The value of a deterministic target policy $\\pi$ is the expected reward when actions are chosen according to that policy. It can be defined using the law of total expectation over the distribution of states $x$ and the action-value function $Q(x,a) = \\mathbb{E}[R \\mid X=x, A=a]$, where $R$ is the reward:\n$$V(\\pi) = \\mathbb{E}_{x \\sim p(x)}[Q(x, \\pi(x))]$$\nHere, $p(x)$ is the marginal probability distribution of states. We are given an outcome regression model, $m(x,a)$, which is an approximation of the true action-value function $Q(x,a)$.\n\nThe DR estimator combines the direct method (using the outcome model) with importance sampling (using the behavior policy). Let's start with a key identity. Consider the following expectation, taken over the data-generating (behavior) distribution, where states $x$ are sampled from $p(x)$ and actions $a$ are sampled from the behavior policy $b(a|x)$:\n$$\\mathbb{E}_{x, a \\sim b} \\left[ \\frac{\\pi(a|x)}{b(a|x)} (R - m(x,a)) \\right] = \\mathbb{E}_{x \\sim p(x)} \\left[ \\sum_{a} b(a|x) \\frac{\\pi(a|x)}{b(a|x)} (\\mathbb{E}[R|x,a] - m(x,a)) \\right]$$\nHere, $R$ is the reward random variable, whose conditional expectation is $Q(x,a)$. Simplifying the sum:\n$$= \\mathbb{E}_{x \\sim p(x)} \\left[ \\sum_{a} \\pi(a|x) (Q(x,a) - m(x,a)) \\right]$$\nFor the deterministic policy $\\pi$ given in the problem, $\\pi(a|x)$ is $1$ for $a=\\pi(x)$ and $0$ otherwise. The sum thus reduces to a single term:\n$$= \\mathbb{E}_{x \\sim p(x)} [Q(x, \\pi(x)) - m(x, \\pi(x))]$$\nRecognizing that $\\mathbb{E}_{x}[Q(x, \\pi(x))] = V(\\pi)$, we have:\n$$\\mathbb{E}_{x, a \\sim b} \\left[ \\frac{\\pi(a|x)}{b(a|x)} (R - m(x,a)) \\right] = V(\\pi) - \\mathbb{E}_{x \\sim p(x)}[m(x, \\pi(x))]$$\nRearranging this equation to solve for $V(\\pi)$ yields the expression whose empirical average will form our estimator:\n$$V(\\pi) = \\mathbb{E}_{x, a \\sim b} \\left[ \\frac{\\pi(a|x)}{b(a|x)} (R - m(x,a)) \\right] + \\mathbb{E}_{x \\sim p(x)}[m(x, \\pi(x))]$$\nThe empirical Doubly Robust estimator, $\\hat{V}_{DR}(\\pi)$, is the sample-mean approximation of this expression, calculated over a dataset of $n$ samples $\\{(x_i, a_i, y_i)\\}_{i=1}^n$ collected under the behavior policy $b$. Let $w_i = \\frac{\\pi(a_i|x_i)}{b(a_i|x_i)}$ be the importance weight for the $i$-th sample. The estimator is:\n$$\\hat{V}_{DR}(\\pi) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ w_i (y_i - m(x_i, a_i)) + m(x_i, \\pi(x_i)) \\right]$$\nThis estimator is \"doubly robust\" because it provides an unbiased estimate of $V(\\pi)$ if either the outcome model is correct (i.e., $m(x,a) = Q(x,a)$) or the propensity model used to calculate the weights $w_i$ is correct.\n\nNow, we will compute the value of this estimator for the given dataset of $n=5$ patient episodes.\nThe target policy $\\pi$ is: $\\pi(x=1) = \\mathrm{T1}$ and $\\pi(x=0) = \\mathrm{T0}$.\nThe outcome model $m(x, a)$ predictions are:\n$m(1, \\mathrm{T1}) = 0.75$, $m(1, \\mathrm{T0}) = 0.55$.\n$m(0, \\mathrm{T0}) = 0.68$, $m(0, \\mathrm{T1}) = 0.52$.\n\nWe calculate the term inside the summation, $T_i = w_i (y_i - m(x_i, a_i)) + m(x_i, \\pi(x_i))$, for each episode $i \\in \\{1, 2, 3, 4, 5\\}$.\n\n-   **Episode $i=1$**: $(x_1=1, a_1=\\mathrm{T1}, y_1=0.80), w_1=\\frac{10}{7}$.\n    The action prescribed by $\\pi$ for $x_1=1$ is $\\pi(1)=\\mathrm{T1}$.\n    $T_1 = w_1 (y_1 - m(x_1, a_1)) + m(x_1, \\pi(x_1)) = \\frac{10}{7} (0.80 - m(1, \\mathrm{T1})) + m(1, \\mathrm{T1})$\n    $T_1 = \\frac{10}{7} (0.80 - 0.75) + 0.75 = \\frac{10}{7} (0.05) + 0.75 = \\frac{0.5}{7} + 0.75 = \\frac{1}{14} + \\frac{3}{4} = \\frac{2+21}{28} = \\frac{23}{28}$.\n\n-   **Episode $i=2$**: $(x_2=1, a_2=\\mathrm{T0}, y_2=0.50), w_2=0$.\n    The action prescribed by $\\pi$ for $x_2=1$ is $\\pi(1)=\\mathrm{T1}$.\n    $T_2 = w_2 (y_2 - m(x_2, a_2)) + m(x_2, \\pi(x_2)) = 0 \\cdot (0.50 - m(1, \\mathrm{T0})) + m(1, \\mathrm{T1})$\n    $T_2 = 0 + 0.75 = 0.75 = \\frac{3}{4}$.\n\n-   **Episode $i=3$**: $(x_3=0, a_3=\\mathrm{T0}, y_3=0.70), w_3=\\frac{5}{3}$.\n    The action prescribed by $\\pi$ for $x_3=0$ is $\\pi(0)=\\mathrm{T0}$.\n    $T_3 = w_3 (y_3 - m(x_3, a_3)) + m(x_3, \\pi(x_3)) = \\frac{5}{3} (0.70 - m(0, \\mathrm{T0})) + m(0, \\mathrm{T0})$\n    $T_3 = \\frac{5}{3} (0.70 - 0.68) + 0.68 = \\frac{5}{3} (0.02) + 0.68 = \\frac{0.1}{3} + 0.68 = \\frac{1}{30} + \\frac{68}{100} = \\frac{1}{30} + \\frac{17}{25} = \\frac{5+102}{150} = \\frac{107}{150}$.\n\n-   **Episode $i=4$**: $(x_4=0, a_4=\\mathrm{T1}, y_4=0.40), w_4=0$.\n    The action prescribed by $\\pi$ for $x_4=0$ is $\\pi(0)=\\mathrm{T0}$.\n    $T_4 = w_4 (y_4 - m(x_4, a_4)) + m(x_4, \\pi(x_4)) = 0 \\cdot (0.40 - m(0, \\mathrm{T1})) + m(0, \\mathrm{T0})$\n    $T_4 = 0 + 0.68 = \\frac{17}{25}$.\n\n-   **Episode $i=5$**: $(x_5=1, a_5=\\mathrm{T1}, y_5=0.60), w_5=\\frac{10}{7}$.\n    The action prescribed by $\\pi$ for $x_5=1$ is $\\pi(1)=\\mathrm{T1}$.\n    $T_5 = w_5 (y_5 - m(x_5, a_5)) + m(x_5, \\pi(x_5)) = \\frac{10}{7} (0.60 - m(1, \\mathrm{T1})) + m(1, \\mathrm{T1})$\n    $T_5 = \\frac{10}{7} (0.60 - 0.75) + 0.75 = \\frac{10}{7} (-0.15) + 0.75 = \\frac{-1.5}{7} + 0.75 = -\\frac{3}{14} + \\frac{3}{4} = \\frac{-6+21}{28} = \\frac{15}{28}$.\n\nNow, we sum these terms:\n$\\sum_{i=1}^{5} T_i = \\frac{23}{28} + \\frac{3}{4} + \\frac{107}{150} + \\frac{17}{25} + \\frac{15}{28}$\n$\\sum_{i=1}^{5} T_i = \\left(\\frac{23}{28} + \\frac{15}{28}\\right) + \\frac{3}{4} + \\left(\\frac{107}{150} + \\frac{17 \\cdot 6}{25 \\cdot 6}\\right)$\n$\\sum_{i=1}^{5} T_i = \\frac{38}{28} + \\frac{3}{4} + \\left(\\frac{107}{150} + \\frac{102}{150}\\right)$\n$\\sum_{i=1}^{5} T_i = \\frac{19}{14} + \\frac{3}{4} + \\frac{209}{150}$\nTo sum these fractions, we find a common denominator. The least common multiple of $14=2 \\cdot 7$, $4=2^2$, and $150=2 \\cdot 3 \\cdot 5^2$ is $2^2 \\cdot 3 \\cdot 5^2 \\cdot 7 = 4 \\cdot 3 \\cdot 25 \\cdot 7 = 2100$.\n$\\sum_{i=1}^{5} T_i = \\frac{19 \\cdot 150}{2100} + \\frac{3 \\cdot 525}{2100} + \\frac{209 \\cdot 14}{2100}$\n$\\sum_{i=1}^{5} T_i = \\frac{2850}{2100} + \\frac{1575}{2100} + \\frac{2926}{2100} = \\frac{2850 + 1575 + 2926}{2100} = \\frac{7351}{2100}$\n\nFinally, we compute the estimator $\\hat{V}_{DR}(\\pi)$ by dividing the sum by $n=5$:\n$\\hat{V}_{DR}(\\pi) = \\frac{1}{5} \\left( \\frac{7351}{2100} \\right) = \\frac{7351}{10500}$\nAs a decimal, this is $0.700095238...$.\nRounding to four significant figures, we look at the fifth significant digit. Since $9 \\ge 5$, we round up the fourth digit.\n$\\hat{V}_{DR}(\\pi) \\approx 0.7001$.",
            "answer": "$$\\boxed{0.7001}$$"
        }
    ]
}