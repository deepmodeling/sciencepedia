{
    "hands_on_practices": [
        {
            "introduction": "A key challenge in deploying clinical machine learning models is \"prevalence shift,\" where the frequency of a disease in the deployment population differs from the training data. While metrics like the Area Under the Receiver Operating Characteristic (AUROC) curve are valuable for assessing a model's general discriminative power, they are famously insensitive to class prevalence. This exercise will guide you through a fundamental calculation to demonstrate why a high AUROC can be dangerously misleading, showing how a model's practical utility can plummet in a real-world, low-prevalence setting .",
            "id": "4841072",
            "problem": "A hospital develops a binary classifier to predict a rare adverse event from routinely collected clinical variables. Let $Y \\in \\{0,1\\}$ denote the true event label, $X$ the observed features, and $\\hat{S}(X) \\in [0,1]$ a risk score; at deployment, the hospital will issue alerts when $\\hat{S}(X) \\ge \\tau$ for some threshold $\\tau$. The training cohort has adverse event prevalence $P_{\\text{train}}(Y=1) = \\pi_{\\text{train}} = 0.01$, while the target ward where the model will be deployed has prevalence $P_{\\text{deploy}}(Y=1) = \\pi_{\\text{deploy}} = 0.002$. On internal validation drawn from the training distribution, the classifier achieves True Positive Rate (TPR) $= 0.80$ and False Positive Rate (FPR) $= 0.05$ at an operating threshold selected for clinical workflow, and an Area Under the Receiver Operating Characteristic (AUROC) of $0.92$ overall. Receiver Operating Characteristic (ROC) curves summarize the trade-off of TPR versus FPR across thresholds.\n\nFrom first principles of probability, and without relying on shortcut formulas, reason about the implications of the class proportions and the shift in prevalence for deployment. Consider that clinical utility depends on the posterior probability $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$ achieved when issuing alerts, and that ranking-based discrimination summarized by AUROC aggregates $TPR(\\tau)$ versus $FPR(\\tau)$ across $\\tau$.\n\nWhich option most accurately defines class imbalance and prevalence shift in this setting, and correctly explains why a high AUROC can be misleading for rare adverse events under the stated deployment prevalence?\n\nA. Class imbalance means $P(Y=1)$ is much smaller than $P(Y=0)$ in the training data; prevalence shift means $P(Y)$ changes between training and deployment while $P(X \\mid Y)$ remains stable. Because AUROC summarizes $TPR$ versus $FPR$ across thresholds and is invariant to $P(Y)$, a high AUROC can still correspond to low $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$ when $\\pi$ is very small, since even modest $FPR$ generates many false positives that dominate true positives.\n\nB. Class imbalance refers to any difference in feature distributions $P(X)$ between classes; prevalence shift means the conditional distributions $P(X \\mid Y)$ change over time while $P(Y)$ stays constant. AUROC will necessarily decrease as prevalence decreases, so a high AUROC guarantees high $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$ even in rare-event settings.\n\nC. Class imbalance is caused by label noise; prevalence shift occurs when the threshold $\\tau$ is changed at deployment. AUROC is highly sensitive to the number of negatives and therefore is the preferred measure for rare-event calibration, ensuring that $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$ is high when AUROC is high.\n\nD. Class imbalance means $P(Y=1)$ is much smaller than $P(Y=0)$, and prevalence shift means that both $P(Y)$ and $P(X \\mid Y)$ change arbitrarily at deployment. AUROC is misleading for rare events because it underestimates false negatives, not because false positives can outnumber true positives at operational thresholds.\n\nE. Class imbalance is when $P(Y=1)$ equals $P(Y=0)$ but $P(X)$ differs between training and deployment; prevalence shift is eliminated by calibration so AUROC fully determines $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$ regardless of how rare the event is.",
            "solution": "Here's my thought process for solving this problem:\n\n1.  **Initial Read-Through:** I'll first read the entire problem and all the options to get a general sense of what's being asked. The problem is about evaluating a machine learning classifier in a medical context. Key terms are jumping out: binary classifier, rare event, prevalence, TPR, FPR, AUROC, and the posterior probability (or Positive Predictive Value, PPV). The core of the question is about the relationship between a high AUROC and the actual clinical usefulness (PPV) when the prevalence of the disease/event changes from training to deployment.\n\n2.  **Deconstruct the Problem Statement:** I'll break down the given information into concrete, mathematical terms.\n    *   The event is binary: $Y \\in \\{0, 1\\}$.\n    *   We have a score model $\\hat{S}(X)$.\n    *   A positive prediction (alert) is made if $\\hat{S}(X) \\ge \\tau$.\n    *   Training prevalence: $\\pi_{\\text{train}} = P(Y=1) = 0.01$.\n    *   Deployment prevalence: $\\pi_{\\text{deploy}} = P(Y=1) = 0.002$.\n    *   At a specific operating threshold $\\tau$, we have:\n        *   True Positive Rate (TPR): $P(\\hat{S}(X) \\ge \\tau | Y=1) = 0.80$. This is also called sensitivity or recall.\n        *   False Positive Rate (FPR): $P(\\hat{S}(X) \\ge \\tau | Y=0) = 0.05$. This is also $1 - \\text{specificity}$.\n    *   Overall discrimination: Area Under the ROC curve (AUROC) = $0.92$.\n    *   The key clinical utility metric is the posterior probability: $P(Y=1 | \\hat{S}(X) \\ge \\tau)$. This is the PPV.\n\n3.  **Formulate a Plan Based on \"First Principles\":** The prompt explicitly asks to reason from first principles. This means I should not just pull a formula out of thin air, but derive it or at least justify it.\n    *   **Define the concepts:** I need to have a clear, formal definition for \"class imbalance\" and \"prevalence shift.\"\n        *   *Class imbalance:* This is a property of the data distribution, specifically the prior probability of the classes. It means $P(Y=1)$ is very different from $P(Y=0)$. In this case, since the event is rare, $P(Y=1)$ is much smaller than $P(Y=0)$.\n        *   *Prevalence shift:* This is a type of *dataset shift*. Specifically, it's a *prior probability shift*, where the prior $P(Y)$ changes between training and testing, but the class-conditional distributions $P(X|Y)$ are assumed to remain stable. The problem gives us $\\pi_{\\text{train}} \\neq \\pi_{\\text{deploy}}$, so this is exactly what's happening. The stability of $P(X|Y)$ is a crucial assumption because it implies that the TPR and FPR of the classifier (at a fixed threshold $\\tau$) also remain stable, as they are defined based on these conditional distributions.\n    *   **Analyze the AUROC:** What is AUROC and how does it relate to prevalence? The ROC curve plots TPR vs. FPR. Both TPR ($P(\\text{alert}|Y=1)$) and FPR ($P(\\text{alert}|Y=0)$) are conditioned on the true label $Y$. Therefore, they *do not depend on the prevalence $P(Y)$*. Since the ROC curve is independent of prevalence, its area (AUROC) must also be independent of prevalence. This is a critical point. A high AUROC ($0.92$) is a good sign of *discriminative* ability (the model is good at ranking positive cases higher than negative cases), but it tells us nothing about performance at a *specific* prevalence.\n    *   **Analyze the PPV:** Now, let's look at the clinical utility metric, $P(Y=1 | \\hat{S}(X) \\ge \\tau)$. This is a posterior probability. The go-to tool for relating priors and posteriors is **Bayes' Theorem**.\n        Let A be the event of an alert, i.e., $\\hat{S}(X) \\ge \\tau$. We want to find $P(Y=1 | A)$.\n        Bayes' Theorem states:\n        $$P(Y=1 | A) = \\frac{P(A | Y=1) P(Y=1)}{P(A)}$$\n        Let's substitute our terms:\n        *   $P(A | Y=1)$ is the TPR.\n        *   $P(Y=1)$ is the prevalence, $\\pi$.\n        *   $P(A)$ is the overall probability of an alert. We can find this using the law of total probability: $P(A) = P(A | Y=1)P(Y=1) + P(A | Y=0)P(Y=0)$.\n        *   $P(A | Y=0)$ is the FPR.\n        *   $P(Y=0) = 1 - \\pi$.\n        So, the denominator is $P(A) = (TPR \\cdot \\pi) + (FPR \\cdot (1-\\pi))$.\n        Putting it all together:\n        $$PPV = P(Y=1 | A) = \\frac{TPR \\cdot \\pi}{TPR \\cdot \\pi + FPR \\cdot (1-\\pi)}$$\n        This formula shows that PPV is *highly dependent* on the prevalence $\\pi$.\n\n4.  **Execute the Plan (Calculations):** Now I'll plug in the numbers from the problem to see the effect of the prevalence shift.\n    *   **At training prevalence ($\\pi_{\\text{train}} = 0.01$):**\n        $$PPV_{\\text{train}} = \\frac{0.80 \\cdot 0.01}{0.80 \\cdot 0.01 + 0.05 \\cdot (1 - 0.01)} = \\frac{0.008}{0.008 + 0.05 \\cdot 0.99} = \\frac{0.008}{0.008 + 0.0495} = \\frac{0.008}{0.0575} \\approx 0.139$$\n        So, in the training data, about 14% of alerts are correct. This is not great, but perhaps acceptable depending on the context.\n    *   **At deployment prevalence ($\\pi_{\\text{deploy}} = 0.002$):**\n        $$PPV_{\\text{deploy}} = \\frac{0.80 \\cdot 0.002}{0.80 \\cdot 0.002 + 0.05 \\cdot (1 - 0.002)} = \\frac{0.0016}{0.0016 + 0.05 \\cdot 0.998} = \\frac{0.0016}{0.0016 + 0.0499} = \\frac{0.0016}{0.0515} \\approx 0.031$$\n        Here, the PPV crashes to about 3%. This means for every 100 alerts the system generates, only 3 will be for actual adverse events. The other 97 will be false alarms. This is likely to cause \"alert fatigue\" and would make the system clinically useless, despite its high AUROC.\n\n5.  **Synthesize the \"Why\":** The calculation shows *what* happens, but the prompt asks *why*. The \"why\" is in the PPV formula. Look at the two terms in the denominator:\n    *   True Positives (rate): $TPR \\cdot \\pi$\n    *   False Positives (rate): $FPR \\cdot (1-\\pi)$\n    When $\\pi$ is very small (like $0.002$), $1-\\pi$ is very close to $1$. So the number of false positives is roughly proportional to $FPR$. The number of true positives is proportional to $TPR \\cdot \\pi$.\n    The ratio of false positives to true positives in the alerted group is $\\frac{FPR \\cdot (1-\\pi)}{TPR \\cdot \\pi}$.\n    For the deployment scenario, this is $\\frac{0.05 \\cdot 0.998}{0.80 \\cdot 0.002} = \\frac{0.0499}{0.0016} \\approx 31.2$.\n    This means that for every true positive alert, we get over 31 false positive alerts. The sheer number of negative patients overwhelms the classifier, even with a seemingly low FPR. This is the core reason why a high AUROC can be misleading for rare events. AUROC is prevalence-invariant, but PPV is not.\n\n6.  **Evaluate the Options:** Now I'll go through each option and check it against my derivation.\n\n    *   **A:**\n        *   _\"Class imbalance means $P(Y=1)$ is much smaller than $P(Y=0)$...\"_ - **Correct**.\n        *   _\"...prevalence shift means $P(Y)$ changes... while $P(X \\mid Y)$ remains stable.\"_ - **Correct**, this is the standard definition.\n        *   _\"Because AUROC... is invariant to $P(Y)$...\"_ - **Correct**, as I reasoned.\n        *   _\"a high AUROC can still correspond to low $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$ when $\\pi$ is very small...\"_ - **Correct**, as my calculation showed.\n        *   _\"since even modest FPR generates many false positives that dominate true positives.\"_ - **Correct**, this is the \"why\" I synthesized. This option looks perfect.\n\n    *   **B:**\n        *   _\"Class imbalance refers to any difference in feature distributions $P(X)$ between classes...\"_ - **Incorrect**. This is required for classification, not the definition of imbalance.\n        *   _\"prevalence shift means the conditional distributions $P(X \\mid Y)$ change... while $P(Y)$ stays constant.\"_ - **Incorrect**. This is the opposite of prevalence shift. It's a form of concept drift.\n        *   The rest of the option is based on these false premises, and is also factually wrong (\"AUROC will necessarily decrease,\" \"guarantees high P(Y=1|...)\").\n\n    *   **C:**\n        *   _\"Class imbalance is caused by label noise...\"_ - **Incorrect**. Unrelated concepts.\n        *   _\"prevalence shift occurs when the threshold $\\tau$ is changed...\"_ - **Incorrect**.\n        *   _\"AUROC is highly sensitive to the number of negatives...\"_ - **Incorrect**. It is famously *in*sensitive. Precision-recall is the sensitive one.\n        *   The rest is also wrong.\n\n    *   **D:**\n        *   _\"Class imbalance means $P(Y=1)$ is much smaller than $P(Y=0)$...\"_ - **Correct**.\n        *   _\"...prevalence shift means that both $P(Y)$ and $P(X \\mid Y)$ change...\"_ - This is not the standard definition of \"prevalence shift,\" but a more general \"dataset shift.\" The problem is constructed to isolate the effect of *prevalence* shift, where $P(X|Y)$ is stable.\n        *   _\"AUROC is misleading for rare events because it underestimates false negatives, not because false positives can outnumber true positives...\"_ - **Incorrect**. The primary issue is the explosion of *false positives* in absolute numbers, which leads to a low PPV. The statement about \"underestimating false negatives\" is confused.\n\n    *   **E:**\n        *   _\"Class imbalance is when $P(Y=1)$ equals $P(Y=0)$...\"_ - **Incorrect**. This is a balanced dataset.\n        *   Everything else that follows is also wrong.\n\n7.  **Final Conclusion:** Option A is the only one that correctly defines all terms and provides the correct, first-principles-based reasoning. I will select A as the final answer.\n\n\nThis detailed, step-by-step process, starting from first principles and definitions, moving to a formal derivation, applying it to the specific numbers, synthesizing the explanation, and then methodically checking each option, ensures I arrive at the correct and well-justified answer. It directly follows the prompt's requirements.\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   A binary classifier predicts an event, with true label $Y \\in \\{0,1\\}$.\n-   The classifier outputs a risk score $\\hat{S}(X) \\in [0,1]$ from features $X$.\n-   An alert is issued if $\\hat{S}(X) \\ge \\tau$ for a threshold $\\tau$.\n-   Prevalence in the training cohort: $P_{\\text{train}}(Y=1) = \\pi_{\\text{train}} = 0.01$.\n-   Prevalence in the deployment ward: $P_{\\text{deploy}}(Y=1) = \\pi_{\\text{deploy}} = 0.002$.\n-   At the chosen threshold $\\tau$, on the training distribution, the classifier has:\n    -   True Positive Rate (TPR) $= P(\\hat{S}(X) \\ge \\tau \\mid Y=1) = 0.80$.\n    -   False Positive Rate (FPR) $= P(\\hat{S}(X) \\ge \\tau \\mid Y=0) = 0.05$.\n-   Overall classifier performance: Area Under the Receiver Operating Characteristic (AUROC) $= 0.92$.\n-   The question asks for the correct definitions of class imbalance and prevalence shift, and an explanation of why a high AUROC can be misleading for rare events, focusing on the posterior probability $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is grounded in the standard, fundamental principles of probability theory and classifier evaluation in machine learning and medical informatics. All terms (TPR, FPR, AUROC, prevalence, prevalence shift) are standard and used correctly.\n-   **Well-Posed**: The problem is well-posed. It presents a clear, realistic scenario and asks for a conceptual explanation based on first principles, supported by the provided data. A unique, correct explanation exists.\n-   **Objective**: The problem is stated using objective, precise, and standard terminology. It is free from subjective or opinion-based claims.\n-   **Completeness and Consistency**: The problem is self-contained. It provides all necessary information to reason about the relationship between prevalence, rank-based metrics (AUROC), and threshold-based, clinically-relevant metrics (posterior probability). The implicit assumption that the class-conditional distributions $P(X \\mid Y)$ are stable between training and deployment is the standard definition of \"prevalence shift,\" which the question asks to define. Thus, it is not an omission but part of the question itself. The values are internally consistent and plausible.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. It is a sound and well-formulated question on a critical topic in applied machine learning. I will proceed to derive the solution.\n\n### Derivation from First Principles\n\nThe core of the problem is the relationship between a classifier's discrimination (measured by AUROC) and its practical utility in a clinical setting (measured by a posterior probability, like the Positive Predictive Value).\n\n1.  **Definitions and First Principles**\n    -   **Class Imbalance**: A dataset exhibits class imbalance when the prior probabilities of the classes are highly unequal. For this binary classification problem, this means $P(Y=1)$ is much smaller than $P(Y=0)$, which is true for the given rare adverse event ($P(Y=1) = 0.01$ or $P(Y=1) = 0.002$).\n    -   **Prevalence Shift**: This is a specific type of dataset drift where the class prior probabilities $P(Y)$ change between the training and deployment distributions, but the class-conditional feature distributions $P(X|Y)$ remain stable. The stability of $P(X|Y)$ implies that the classifier's intrinsic properties, TPR and FPR, which are conditioned on $Y$, remain unchanged for a fixed threshold $\\tau$.\n    -   **AUROC Invariance**: The ROC curve is a plot of $TPR(\\tau) = P(\\hat{S}(X) \\ge \\tau \\mid Y=1)$ versus $FPR(\\tau) = P(\\hat{S}(X) \\ge \\tau \\mid Y=0)$ for all possible thresholds $\\tau$. Since both TPR and FPR are conditional on the true class $Y$, they are independent of the class prevalence $P(Y)$. Consequently, the entire ROC curve and its area (AUROC) are invariant to changes in prevalence. An AUROC of $0.92$ is a measure of the model's ability to rank positive cases higher than negative cases, and this ranking ability does not change when the proportion of positive cases in the population changes.\n    -   **Posterior Probability (PPV)**: The clinical utility is given by $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$, the probability that a patient who gets an alert truly has the adverse event. This is the Positive Predictive Value (PPV). We can derive this using Bayes' theorem. Let $A$ be the event of an alert, $A \\equiv (\\hat{S}(X) \\ge \\tau)$.\n        \n        $$P(Y=1 \\mid A) = \\frac{P(A \\mid Y=1) P(Y=1)}{P(A)}$$\n        \n        Using the law of total probability for the denominator:\n        $P(A) = P(A \\mid Y=1)P(Y=1) + P(A \\mid Y=0)P(Y=0)$\n        \n        Let $\\pi = P(Y=1)$ be the prevalence. Substituting the definitions of TPR and FPR, we get:\n        \n        $$PPV = \\frac{TPR \\cdot \\pi}{TPR \\cdot \\pi + FPR \\cdot (1-\\pi)}$$\n        \n        This expression demonstrates that the PPV is a direct function of the prevalence $\\pi$.\n\n2.  **Application to the Scenario**\n    We can now calculate the PPV for both the training and deployment settings, using the given values: $TPR = 0.80$ and $FPR = 0.05$.\n\n    -   **In the training cohort** ($\\pi_{\\text{train}} = 0.01$):\n        $$PPV_{\\text{train}} = \\frac{0.80 \\cdot 0.01}{0.80 \\cdot 0.01 + 0.05 \\cdot (1-0.01)} = \\frac{0.008}{0.008 + 0.05 \\cdot 0.99} = \\frac{0.008}{0.008 + 0.0495} = \\frac{0.008}{0.0575} \\approx 0.139$$\n        In the training environment, approximately $13.9\\%$ of alerts are for true events.\n\n    -   **In the deployment ward** ($\\pi_{\\text{deploy}} = 0.002$):\n        $$PPV_{\\text{deploy}} = \\frac{0.80 \\cdot 0.002}{0.80 \\cdot 0.002 + 0.05 \\cdot (1-0.002)} = \\frac{0.0016}{0.0016 + 0.05 \\cdot 0.998} = \\frac{0.0016}{0.0016 + 0.0499} = \\frac{0.0016}{0.0515} \\approx 0.031$$\n        In the deployment environment, the PPV drops to approximately $3.1\\%$.\n\n3.  **Explanation of the Discrepancy**\n    Despite the high and unchanging AUROC ($0.92$), the clinical utility (PPV) plummets from $\\approx 14\\%$ to $\\approx 3\\%$ due to the decrease in prevalence. The reason lies in the absolute number of false positives versus true positives.\n    -   The number of true positives in a population of size $N$ is $N \\cdot \\pi \\cdot TPR$.\n    -   The number of false positives is $N \\cdot (1-\\pi) \\cdot FPR$.\n    When $\\pi$ is very small, the number of negative cases, $N(1-\\pi)$, is very large. Even a modest FPR (here, $0.05$) applied to this large population generates a large absolute number of false positives. As $\\pi$ decreases, the number of true positives shrinks, while the number of false positives remains high. The false positives \"dominate\" or \"overwhelm\" the true positives, leading to a very low PPV. The high AUROC only tells us the model is good at ranking, but it does not guarantee that any given threshold will provide a high-purity set of alerts when the event is extremely rare.\n\n### Option-by-Option Analysis\n\n**A. Class imbalance means $P(Y=1)$ is much smaller than $P(Y=0)$ in the training data; prevalence shift means $P(Y)$ changes between training and deployment while $P(X \\mid Y)$ remains stable. Because AUROC summarizes $TPR$ versus $FPR$ across thresholds and is invariant to $P(Y)$, a high AUROC can still correspond to low $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$ when $\\pi$ is very small, since even modest $FPR$ generates many false positives that dominate true positives.**\nThis option correctly defines class imbalance. It correctly defines prevalence shift as a change in $P(Y)$ with stable $P(X|Y)$. It correctly states that AUROC is invariant to prevalence. It correctly concludes that PPV can be low despite high AUROC in rare-event settings, and provides the correct reason: the large number of false positives overwhelming the true positives. This aligns perfectly with our derivation.\n**Verdict: Correct.**\n\n**B. Class imbalance refers to any difference in feature distributions $P(X)$ between classes; prevalence shift means the conditional distributions $P(X \\mid Y)$ change over time while $P(Y)$ stays constant. AUROC will necessarily decrease as prevalence decreases, so a high AUROC guarantees high $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$ even in rare-event settings.**\nThe definition of class imbalance is incorrect; a difference in $P(X|Y=0)$ and $P(X|Y=1)$ is what makes classification possible, not what defines imbalance. The definition of prevalence shift is incorrect; it describes concept drift and falsely claims $P(Y)$ is constant. The statement that AUROC decreases with prevalence is incorrect. The final conclusion is the opposite of the truth.\n**Verdict: Incorrect.**\n\n**C. Class imbalance is caused by label noise; prevalence shift occurs when the threshold $\\tau$ is changed at deployment. AUROC is highly sensitive to the number of negatives and therefore is the preferred measure for rare-event calibration, ensuring that $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$ is high when AUROC is high.**\nThe definition of class imbalance is incorrect; it is unrelated to label noise. The definition of prevalence shift is incorrect; it is unrelated to changing $\\tau$. The statement that AUROC is \"highly sensitive to the number of negatives\" is false; it is known for its insensitivity to class balance, unlike metrics based on a Precision-Recall curve. AUROC is a measure of discrimination, not calibration.\n**Verdict: Incorrect.**\n\n**D. Class imbalance means $P(Y=1)$ is much smaller than $P(Y=0)$, and prevalence shift means that both $P(Y)$ and $P(X \\mid Y)$ change arbitrarily at deployment. AUROC is misleading for rare events because it underestimates false negatives, not because false positives can outnumber true positives at operational thresholds.**\nThe definition of class imbalance is correct. The definition of prevalence shift is too general (describing a broader dataset shift) and contradicts the standard assumption of stable $P(X|Y)$ used to analyze prevalence shift's effects. The reasoning is incorrect; the core issue is the high number of *false positives* relative to true positives, not an underestimation of *false negatives*.\n**Verdict: Incorrect.**\n\n**E. Class imbalance is when $P(Y=1)$ equals $P(Y=0)$ but $P(X)$ differs between training and deployment; prevalence shift is eliminated by calibration so AUROC fully determines $P(Y=1 \\mid \\hat{S}(X) \\ge \\tau)$ regardless of how rare the event is.**\nThe definition of class imbalance is incorrect; $P(Y=1) = P(Y=0)$ describes a balanced dataset. The claim that calibration \"eliminates\" prevalence shift is false; prevalence is a property of the population, not the model. The claim that AUROC \"fully determines\" PPV is false; our derivation shows PPV is critically dependent on prevalence, while AUROC is not.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A good clinical model must do more than just rank patients by risk; its predicted probabilities should also be reliable. This introduces the crucial distinction between a model's *discrimination* (its ability to separate high-risk from low-risk patients, often measured by AUROC) and its *calibration* (the accuracy of its probability estimates). In this practice, you will discover through a first-principles derivation that these two properties are not the same; a model can be perfectly calibrated yet have an AUROC barely better than random chance . This highlights the importance of using multiple metrics to holistically evaluate a model's performance.",
            "id": "4841086",
            "problem": "A hospital is developing a sepsis risk model using a single continuous laboratory biomarker. Let the binary outcome be denoted by $Y \\in \\{0,1\\}$, where $Y=1$ indicates a patient develops sepsis within $48$ hours. The biomarker is $X \\in \\mathbb{R}$. In the target population, the class prior (prevalence) is $p = \\mathbb{P}(Y=1) = 0.01$. Empirically, the class-conditional feature distributions are well described by normal distributions with equal variance: $X \\mid Y=1 \\sim \\mathcal{N}(\\mu_1,\\sigma^2)$ and $X \\mid Y=0 \\sim \\mathcal{N}(\\mu_0,\\sigma^2)$, with $\\mu_0 = 0$, $\\mu_1 = 0.1$, and $\\sigma = 1$. The model outputs the posterior probability score $S = s(X)$, where $s(x) = \\mathbb{P}(Y=1 \\mid X=x)$ is computed using Bayes’ theorem with the true parameters and the true prior $p$. Thus, the model’s scores are numerically equal to the true conditional probability of the outcome given $X$.\n\nUse only foundational definitions in probability and statistics: the definition of calibration as $\\mathbb{P}(Y=1 \\mid S=s) = s$ for almost every $s$, Bayes’ theorem, and the definition of the Area Under the Receiver Operating Characteristic (AUROC) curve as the probability that a randomly drawn positive case receives a strictly higher score than a randomly drawn negative case when ties have probability zero. Assume independence when drawing one positive and one negative case.\n\nStarting from these definitions (and without appealing to any pre-memorized “shortcut” formulas), derive and evaluate the AUROC of this model under the stated data-generating mechanism. Then interpret how this value demonstrates that, under heavy class imbalance and weakly separated class-conditional distributions, a model can be perfectly calibrated yet have poor discrimination.\n\nProvide the final AUROC as a single real number rounded to four significant figures. No units are required.",
            "solution": "The problem is first validated against the specified criteria.\n\n**Step 1: Extract Givens**\n-   Outcome variable: $Y \\in \\{0,1\\}$, where $Y=1$ indicates sepsis.\n-   Continuous biomarker: $X \\in \\mathbb{R}$.\n-   Class prior for sepsis: $p = \\mathbb{P}(Y=1) = 0.01$.\n-   Class-conditional distribution for positive cases ($Y=1$): $X \\mid Y=1 \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$.\n-   Class-conditional distribution for negative cases ($Y=0$): $X \\mid Y=0 \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$.\n-   Parameters: $\\mu_0 = 0$, $\\mu_1 = 0.1$, $\\sigma = 1$.\n-   Model score: $S = s(X)$, where $s(x) = \\mathbb{P}(Y=1 \\mid X=x)$ is computed using the true parameters.\n-   Definition of AUROC: The probability that a randomly drawn positive case receives a strictly higher score than a randomly drawn negative case, i.e., $\\mathbb{P}(S_1 > S_0)$, where $S_1$ is the score for a positive case and $S_0$ is for a negative case.\n-   Assumption: Independence between the randomly drawn positive and negative cases.\n-   Task: Derive and evaluate the AUROC from these definitions and interpret the result.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and mathematically sound. It presents a standard scenario in statistical classification and model evaluation, using established concepts like Bayes' theorem, normal distributions, class imbalance, calibration, and the AUROC. The provided information is complete, consistent, and well-defined, allowing for a unique solution to be derived from first principles. The parameters are realistic for a medical diagnostic problem with a weak biomarker. The problem is well-posed and objective.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be derived.\n\n**Derivation of the AUROC**\n\nThe Area Under the Receiver Operating Characteristic (AUROC) curve is defined as the probability that a randomly selected positive case will have a higher risk score than a randomly selected negative case. Let $X_1$ be a random draw from the distribution of the biomarker for the positive class ($Y=1$), and $X_0$ be a random draw from the distribution for the negative class ($Y=0$). The corresponding scores are $S_1 = s(X_1)$ and $S_0 = s(X_0)$. The AUROC is then $\\mathbb{P}(S_1 > S_0)$.\n\nFirst, we must characterize the score function $s(x) = \\mathbb{P}(Y=1 \\mid X=x)$. Using Bayes' theorem:\n$$s(x) = \\frac{f(x \\mid Y=1) \\mathbb{P}(Y=1)}{f(x \\mid Y=1) \\mathbb{P}(Y=1) + f(x \\mid Y=0) \\mathbb{P}(Y=0)}$$\nwhere $f(x \\mid Y=1)$ and $f(x \\mid Y=0)$ are the probability density functions (PDFs) of the class-conditional distributions. Let $p = \\mathbb{P}(Y=1)=0.01$, so $\\mathbb{P}(Y=0) = 1-p = 0.99$. The PDFs are for normal distributions:\n$$f(x \\mid Y=1) = f_1(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x-\\mu_1)^2}{2\\sigma^2}\\right)$$\n$$f(x \\mid Y=0) = f_0(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x-\\mu_0)^2}{2\\sigma^2}\\right)$$\nThe score function can be written as:\n$$s(x) = \\frac{p f_1(x)}{p f_1(x) + (1-p) f_0(x)} = \\frac{1}{1 + \\frac{1-p}{p} \\frac{f_0(x)}{f_1(x)}}$$\n\nWe must determine if $s(x)$ is a monotonic function of $x$. The relationship $\\mathbb{P}(S_1 > S_0)$ depends on the monotonicity of $s(x)$. The ratio of the PDFs is:\n$$\\frac{f_0(x)}{f_1(x)} = \\frac{\\exp\\left(-\\frac{(x-\\mu_0)^2}{2\\sigma^2}\\right)}{\\exp\\left(-\\frac{(x-\\mu_1)^2}{2\\sigma^2}\\right)} = \\exp\\left(\\frac{(x-\\mu_1)^2 - (x-\\mu_0)^2}{2\\sigma^2}\\right)$$\nThe term in the exponent is:\n$$\\frac{1}{2\\sigma^2} \\left( (x^2 - 2x\\mu_1 + \\mu_1^2) - (x^2 - 2x\\mu_0 + \\mu_0^2) \\right) = \\frac{1}{2\\sigma^2} \\left( 2x(\\mu_0 - \\mu_1) + \\mu_1^2 - \\mu_0^2 \\right) = \\frac{\\mu_0 - \\mu_1}{\\sigma^2} x + \\frac{\\mu_1^2 - \\mu_0^2}{2\\sigma^2}$$\nThis is a linear function of $x$. Let $g(x) = \\frac{f_0(x)}{f_1(x)}$. The derivative of its logarithm with respect to $x$ is $\\frac{d}{dx} \\ln(g(x)) = \\frac{\\mu_0 - \\mu_1}{\\sigma^2}$.\nGiven $\\mu_1 = 0.1$ and $\\mu_0 = 0$, this derivative is $\\frac{0 - 0.1}{1^2} = -0.1$, which is negative. Therefore, $\\ln(g(x))$ is a strictly decreasing function of $x$, which implies $g(x) = \\frac{f_0(x)}{f_1(x)}$ is also a strictly decreasing function of $x$.\n\nThe score function is $s(x) = \\left(1 + \\frac{1-p}{p}g(x)\\right)^{-1}$. Since $g(x)$ is a positive, strictly decreasing function, and $z \\mapsto (1+c z)^{-1}$ for $c>0, z>0$ is a strictly decreasing function, the composition results in $s(x)$ being a strictly **increasing** function of $x$.\n\nBecause $s(x)$ is strictly monotonic, the inequality $s(X_1) > s(X_0)$ is equivalent to the inequality $X_1 > X_0$. Thus, the AUROC can be calculated as:\n$$\\text{AUROC} = \\mathbb{P}(S_1 > S_0) = \\mathbb{P}(X_1 > X_0)$$\nWe are given that $X_1$ and $X_0$ are independent random variables, with distributions:\n$$X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$$\n$$X_0 \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$$\nLet the difference be $D = X_1 - X_0$. Since $X_1$ and $X_0$ are independent normal variables, their difference $D$ is also normally distributed.\nThe mean of $D$ is $\\mathbb{E}[D] = \\mathbb{E}[X_1] - \\mathbb{E}[X_0] = \\mu_1 - \\mu_0$.\nThe variance of $D$ is $\\text{Var}(D) = \\text{Var}(X_1) + \\text{Var}(-X_0) = \\text{Var}(X_1) + (-1)^2 \\text{Var}(X_0) = \\sigma^2 + \\sigma^2 = 2\\sigma^2$.\nSo, $D \\sim \\mathcal{N}(\\mu_1 - \\mu_0, 2\\sigma^2)$.\n\nWe need to compute $\\mathbb{P}(D > 0)$. To do this, we standardize the variable $D$:\n$$\\mathbb{P}(D > 0) = \\mathbb{P}\\left(\\frac{D - \\mathbb{E}[D]}{\\sqrt{\\text{Var}(D)}} > \\frac{0 - \\mathbb{E}[D]}{\\sqrt{\\text{Var}(D)}}\\right)$$\nLet $Z = \\frac{D - \\mathbb{E}[D]}{\\sqrt{\\text{Var}(D)}}$, where $Z \\sim \\mathcal{N}(0,1)$ is a standard normal variable.\n$$\\mathbb{P}(D > 0) = \\mathbb{P}\\left(Z > \\frac{-(\\mu_1 - \\mu_0)}{\\sqrt{2\\sigma^2}}\\right) = \\mathbb{P}\\left(Z > -\\frac{\\mu_1 - \\mu_0}{\\sigma\\sqrt{2}}\\right)$$\nBy the symmetry of the standard normal distribution, $\\mathbb{P}(Z > -z) = \\mathbb{P}(Z < z)$. Let $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution.\n$$\\text{AUROC} = \\mathbb{P}\\left(Z < \\frac{\\mu_1 - \\mu_0}{\\sigma\\sqrt{2}}\\right) = \\Phi\\left(\\frac{\\mu_1 - \\mu_0}{\\sigma\\sqrt{2}}\\right)$$\n\nNow, we substitute the given numerical values: $\\mu_1 = 0.1$, $\\mu_0 = 0$, and $\\sigma = 1$.\n$$\\text{AUROC} = \\Phi\\left(\\frac{0.1 - 0}{1 \\cdot \\sqrt{2}}\\right) = \\Phi\\left(\\frac{0.1}{\\sqrt{2}}\\right)$$\nNumerically, the argument is $\\frac{0.1}{\\sqrt{2}} \\approx 0.070710678$.\nEvaluating the standard normal CDF at this value gives:\n$$\\text{AUROC} = \\Phi(0.070710678) \\approx 0.528186$$\nRounding to four significant figures, we get $0.5282$.\n\n**Interpretation**\n\nThe problem states that the model is perfectly calibrated. This is true by construction: the score $s(x)$ is defined as the true conditional probability $\\mathbb{P}(Y=1 \\mid X=x)$. This means that for any group of patients for whom the model outputs a score of, say, $s=0.05$, the true proportion of those patients who will develop sepsis is exactly $5\\%$. Calibration measures the reliability of the model's probability outputs.\n\nThe calculated AUROC is approximately $0.5282$. The AUROC is a measure of the model's **discrimination**, its ability to separate the positive and negative classes. An AUROC of $1.0$ means perfect separation, while an AUROC of $0.5$ means the model has no discriminative ability beyond random chance. Our value of $0.5282$ is extremely close to $0.5$, indicating very poor discriminative power. The model is only slightly better than a coin flip at ranking a randomly chosen septic patient higher than a randomly chosen non-septic patient.\n\nThis result demonstrates a critical principle in model evaluation: **perfect calibration does not imply good discrimination**. The underlying reason for the poor discrimination is the data-generating mechanism itself. The class-conditional distributions, $\\mathcal{N}(0.1, 1)$ and $\\mathcal{N}(0, 1)$, are \"weakly separated\" because their means ($\\mu_1=0.1$, $\\mu_0=0$) are very close relative to their standard deviation ($\\sigma=1$). This causes a massive overlap between the biomarker values for septic and non-septic patients, making it inherently difficult to distinguish between the two groups based on the biomarker $X$ alone. The heavy class imbalance ($p=0.01$) further means that the majority of patients have low biomarker values regardless of their outcome, making discrimination challenging.\n\nThe model correctly learns the true (and limited) relationship between the biomarker and the outcome, producing perfectly reliable probability scores. However, because the intrinsic signal in the data is so weak, these reliable probabilities cannot effectively separate the classes, resulting in a low AUROC. This scenario is common in medicine, where risk factors may be statistically significant but have small effect sizes, leading to calibrated models that are not very discriminative.",
            "answer": "$$\\boxed{0.5282}$$"
        },
        {
            "introduction": "Beyond just evaluating a model, we must be able to explain its decisions, especially when a patient is denied a critical intervention like an ICU transfer. This hands-on practice shifts our focus from evaluation to interpretability by introducing counterfactual explanations: specific, minimal changes to a patient's profile that would have changed the model's outcome. You will solve a constrained optimization problem to generate these actionable \"what-if\" scenarios, learning how to translate a model's abstract logic into concrete steps that respect clinical plausibility .",
            "id": "4841103",
            "problem": "You are given a binary decision model for Intensive Care Unit (ICU) transfer in a healthcare setting, represented as a logistic regression classifier. The model outputs a probability of acceptance according to the logistic function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, where $z = \\mathbf{w}^\\top \\mathbf{x} + b$. A transfer is accepted when $\\sigma(z) \\geq 0.5$, which is equivalent to the threshold condition $\\mathbf{w}^\\top \\mathbf{x} + b \\geq 0$. A counterfactual explanation is a minimally altered feature vector $\\mathbf{x}_{\\mathrm{cf}} = \\mathbf{x} + \\boldsymbol{\\delta}$ that flips a denied decision to acceptance while respecting clinical plausibility constraints.\n\nConstruct counterfactual explanations that minimally alter the input features under the following clinically grounded constraints:\n- Immutable features: certain features cannot be altered at all.\n- Monotonic direction constraints: some features can only be increased (e.g., improving oxygen saturation), while others can only be decreased (e.g., reducing lactate).\n- Physiologically plausible ranges: every feature must remain within a predefined lower and upper bound after alteration.\n\nFormally, given a patient feature vector $\\mathbf{x} \\in \\mathbb{R}^n$, find $\\boldsymbol{\\delta} \\in \\mathbb{R}^n$ that solves the constrained minimization\n$$\n\\begin{aligned}\n\\min_{\\boldsymbol{\\delta}} \\quad & \\|\\boldsymbol{\\delta}\\|_2 \\\\\n\\text{s.t.} \\quad & \\mathbf{w}^\\top(\\mathbf{x} + \\boldsymbol{\\delta}) + b \\geq 0, \\\\\n& x_i + \\delta_i \\in [\\ell_i, u_i] \\quad \\forall i \\in \\{1,\\dots,n\\}, \\\\\n& \\delta_i = 0 \\quad \\text{if feature } i \\text{ is immutable}, \\\\\n& \\delta_i \\geq 0 \\quad \\text{if feature } i \\text{ is restricted to increase only}, \\\\\n& \\delta_i \\leq 0 \\quad \\text{if feature } i \\text{ is restricted to decrease only}.\n\\end{aligned}\n$$\nUse the following model parameters, feature ordering, and constraints. All features are normalized to the interval $[0,1]$.\n\nFeature order and meaning (normalized):\n$1$: age (immutable), $2$: comorbidity index (immutable), $3$: heart rate (decrease only), $4$: oxygen saturation (increase only), $5$: lactate (decrease only), $6$: mean arterial pressure (increase only), $7$: urine output (increase only), $8$: fraction of inspired oxygen (decrease only).\n\nModel parameters:\n$\\mathbf{w} = [-1.0,-0.8,-0.5,0.9,-1.2,1.0,0.7,-0.9]$ and $b = -0.4$.\n\nPhysiologically plausible ranges (lower and upper bounds per feature):\n$\\boldsymbol{\\ell} = [0.0, 0.0, 0.3, 0.85, 0.0, 0.4, 0.2, 0.3]$, $\\boldsymbol{u} = [1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 0.9]$.\n\nMonotonic direction and immutability:\n- Immutable: features $1$ and $2$ have $\\delta_1 = 0$ and $\\delta_2 = 0$.\n- Decrease only: features $3$, $5$, $8$ have $\\delta_3 \\leq 0$, $\\delta_5 \\leq 0$, $\\delta_8 \\leq 0$.\n- Increase only: features $4$, $6$, $7$ have $\\delta_4 \\geq 0$, $\\delta_6 \\geq 0$, $\\delta_7 \\geq 0$.\n\nTest suite of patient feature vectors $\\mathbf{x}$:\n- Case $1$ (denied but near threshold): $\\mathbf{x}^{(1)} = [0.6, 0.5, 0.6, 0.95, 0.4, 0.6, 0.5, 0.4]$.\n- Case $2$ (already accepted): $\\mathbf{x}^{(2)} = [0.3, 0.2, 0.5, 0.98, 0.2, 0.8, 0.7, 0.3]$.\n- Case $3$ (denied and likely infeasible): $\\mathbf{x}^{(3)} = [0.95, 0.9, 0.8, 0.88, 0.95, 0.5, 0.3, 0.85]$.\n\nYour program must, for each case, compute a counterfactual feature vector $\\mathbf{x}_{\\mathrm{cf}}$ meeting the above constraints that minimally changes $\\mathbf{x}$ to yield acceptance. If the patient is already accepted, return the original $\\mathbf{x}$. If no feasible counterfactual exists under the constraints, return an empty list for that case.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is either a bracketed list of floats representing $\\mathbf{x}_{\\mathrm{cf}}$ for a case or an empty list if infeasible, for example, $[[x_{1},\\dots,x_{8}],[],[x_{1},\\dots,x_{8}]]$. No additional text should be printed.",
            "solution": "The problem requires the computation of a counterfactual explanation, $\\mathbf{x}_{\\mathrm{cf}}$, for a logistic regression model used in a clinical decision context. A counterfactual explanation is a minimally perturbed version of an original feature vector $\\mathbf{x}$, denoted $\\mathbf{x}_{\\mathrm{cf}} = \\mathbf{x} + \\boldsymbol{\\delta}$, that alters the model's prediction from a \"denied\" to an \"accepted\" outcome, subject to a set of plausibility constraints. The goal is to find the change vector $\\boldsymbol{\\delta}$ with the minimum Euclidean norm, $\\|\\boldsymbol{\\delta}\\|_2$.\n\nThis problem can be rigorously formulated as a constrained optimization problem. Specifically, it is a Quadratic Program (QP), which involves minimizing a quadratic objective function subject to linear constraints.\n\nFirst, let us analyze the objective function. The objective is to minimize the Euclidean norm $\\|\\boldsymbol{\\delta}\\|_2 = \\sqrt{\\sum_{i=1}^n \\delta_i^2}$. Minimizing this norm is equivalent to minimizing its square, $\\|\\boldsymbol{\\delta}\\|_2^2 = \\sum_{i=1}^n \\delta_i^2 = \\boldsymbol{\\delta}^\\top \\boldsymbol{\\delta}$, because the square root function is strictly monotonic for non-negative values. The squared form is a convex quadratic function, which is preferable for standard optimization solvers. Thus, our objective function is:\n$$\n\\min_{\\boldsymbol{\\delta}} \\quad f(\\boldsymbol{\\delta}) = \\boldsymbol{\\delta}^\\top \\boldsymbol{\\delta}\n$$\n\nNext, we formalize the constraints. The problem specifies several types of constraints on the perturbation vector $\\boldsymbol{\\delta}$.\n\n1.  **Decision Boundary Constraint**: The counterfactual point $\\mathbf{x}_{\\mathrm{cf}} = \\mathbf{x} + \\boldsymbol{\\delta}$ must result in an \"accepted\" decision. The model accepts a transfer when $\\mathbf{w}^\\top \\mathbf{x}_{\\mathrm{cf}} + b \\geq 0$. Substituting $\\mathbf{x}_{\\mathrm{cf}}$ gives:\n    $$\n    \\mathbf{w}^\\top (\\mathbf{x} + \\boldsymbol{\\delta}) + b \\geq 0\n    $$\n    This can be rearranged into a linear inequality constraint on $\\boldsymbol{\\delta}$:\n    $$\n    \\mathbf{w}^\\top \\boldsymbol{\\delta} \\geq -(\\mathbf{w}^\\top \\mathbf{x} + b)\n    $$\n    Let $z = \\mathbf{w}^\\top \\mathbf{x} + b$ be the model's linear score for the original vector $\\mathbf{x}$. For a denied case, $z < 0$. The constraint becomes $\\mathbf{w}^\\top \\boldsymbol{\\delta} \\geq -z$. This is a single linear inequality constraint.\n\n2.  **Combined Plausibility Constraints**: The problem defines three types of feature-specific constraints: physiologically plausible ranges, immutability, and monotonic direction. These can be combined into a set of box constraints, i.e., lower and upper bounds for each component $\\delta_i$ of the vector $\\boldsymbol{\\delta}$.\n    - The range constraint is $x_i + \\delta_i \\in [\\ell_i, u_i]$, which implies $\\ell_i - x_i \\leq \\delta_i \\leq u_i - x_i$.\n    - For immutable features ($i \\in \\{1, 2\\}$), $\\delta_i = 0$. This sets the lower and upper bounds for $\\delta_i$ to $0$. So, $\\delta_i \\in [0, 0]$.\n    - For features restricted to increase only ($i \\in \\{4, 6, 7\\}$), $\\delta_i \\geq 0$. This modifies the lower bound from the range constraint. The resulting bounds are $\\max(\\ell_i - x_i, 0) \\leq \\delta_i \\leq u_i - x_i$.\n    - For features restricted to decrease only ($i \\in \\{3, 5, 8\\}$), $\\delta_i \\leq 0$. This modifies the upper bound. The resulting bounds are $\\ell_i - x_i \\leq \\delta_i \\leq \\min(u_i - x_i, 0)$.\n\nLet the final lower and upper bounds for each $\\delta_i$ be $L_i$ and $U_i$ respectively. The full optimization problem is:\n$$\n\\begin{aligned}\n\\min_{\\boldsymbol{\\delta}} \\quad & \\sum_{i=1}^{8} \\delta_i^2 \\\\\n\\text{s.t.} \\quad & \\mathbf{w}^\\top \\boldsymbol{\\delta} \\geq -(\\mathbf{w}^\\top \\mathbf{x} + b) \\\\\n& L_i \\leq \\delta_i \\leq U_i \\quad \\forall i \\in \\{1,\\dots,8\\}\n\\end{aligned}\n$$\n\nThe solution strategy for each given patient vector $\\mathbf{x}$ is as follows:\n1.  First, evaluate the initial linear score $z = \\mathbf{w}^\\top \\mathbf{x} + b$. If $z \\geq 0$, the patient is already classified as \"accepted,\" and no counterfactual is necessary. The original vector $\\mathbf{x}$ is returned.\n2.  If $z < 0$, the optimization problem is constructed. We calculate the specific bounds $[L_i, U_i]$ for each $\\delta_i$ based on the given $\\mathbf{x}$, $\\boldsymbol{\\ell}$, $\\boldsymbol{u}$, and the specified feature constraints.\n3.  The resulting QP is solved using a suitable numerical solver. The Sequential Least Squares Programming (SLSQP) method is appropriate for this task, as it can handle both inequality constraints and box constraints. We use the `scipy.optimize.minimize` function with the `'SLSQP'` method.\n4.  The solver's outcome determines the result. If the solver finds a feasible solution (indicated by a success status), the optimal perturbation $\\boldsymbol{\\delta}^*$ is used to compute the counterfactual $\\mathbf{x}_{\\mathrm{cf}} = \\mathbf{x} + \\boldsymbol{\\delta}^*$. If the constraints are mutually incompatible (i.e., the feasible set is empty), the solver will fail to find a solution. In this case, no feasible counterfactual exists, and an empty list is returned, as stipulated for Case $3$. This can be pre-emptively checked by verifying if the maximum possible value of $\\mathbf{w}^\\top\\boldsymbol{\\delta}$ under the box constraints is sufficient to meet the decision boundary requirement. For Case $3$, $\\max(\\mathbf{w}^\\top\\boldsymbol{\\delta}) \\approx 2.813$, while the requirement is $\\mathbf{w}^\\top\\boldsymbol{\\delta} \\geq 2.873$, confirming infeasibility.\nThe implementation encapsulates this logic, iterating through each test case and applying the appropriate procedure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Computes counterfactual explanations for a logistic regression model\n    by solving a constrained quadratic optimization problem.\n    \"\"\"\n    # Model parameters and constraints\n    w = np.array([-1.0, -0.8, -0.5, 0.9, -1.2, 1.0, 0.7, -0.9])\n    b = -0.4\n    \n    # Physiologically plausible ranges (lower and upper bounds)\n    l_bounds = np.array([0.0, 0.0, 0.3, 0.85, 0.0, 0.4, 0.2, 0.3])\n    u_bounds = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 0.9])\n    \n    # Feature constraint definitions (0-indexed)\n    immutable_idx = [0, 1]\n    decrease_only_idx = [2, 4, 7]\n    increase_only_idx = [3, 5, 6]\n    \n    # Test suite of patient feature vectors\n    test_cases = [\n        [0.6, 0.5, 0.6, 0.95, 0.4, 0.6, 0.5, 0.4],\n        [0.3, 0.2, 0.5, 0.98, 0.2, 0.8, 0.7, 0.3],\n        [0.95, 0.9, 0.8, 0.88, 0.95, 0.5, 0.3, 0.85],\n    ]\n\n    results = []\n    \n    for x_list in test_cases:\n        x = np.array(x_list)\n        z = w.T @ x + b\n        \n        # Case: Already accepted, no counterfactual needed\n        if z >= 0:\n            results.append(x.tolist())\n            continue\n            \n        # Case: Denied, find a counterfactual by solving the optimization problem\n        \n        # 1. Define bounds for the perturbation vector delta\n        delta_bounds = []\n        for i in range(len(x)):\n            # Base bounds from plausible ranges\n            lower_delta = l_bounds[i] - x[i]\n            upper_delta = u_bounds[i] - x[i]\n            \n            if i in immutable_idx:\n                delta_bounds.append((0.0, 0.0))\n            elif i in decrease_only_idx:\n                # delta must be <= 0 and within range\n                delta_bounds.append((lower_delta, min(upper_delta, 0.0)))\n            elif i in increase_only_idx:\n                # delta must be >= 0 and within range\n                delta_bounds.append((max(lower_delta, 0.0), upper_delta))\n            else:\n                # Should not be reached given problem definition\n                delta_bounds.append((lower_delta, upper_delta))\n        \n        # 2. Define objective function (||delta||_2^2) and its jacobian\n        objective_fn = lambda delta: np.dot(delta, delta)\n        objective_jac = lambda delta: 2 * delta\n        \n        # 3. Define the linear inequality constraint: w.T @ (x + delta) + b >= 0\n        # This simplifies to: w.T @ delta >= -(w.T @ x + b) which is w.T @ delta >= -z\n        # For scipy's 'ineq' type, which is C(x) >= 0, our function is C(delta) = w.T @ delta + z\n        constraint_fun = lambda delta: w.T @ delta + z\n        constraint_jac = lambda delta: w\n        \n        constraints = [{'type': 'ineq', 'fun': constraint_fun, 'jac': constraint_jac}]\n        \n        # 4. Initial guess for delta\n        delta0 = np.zeros(len(x))\n        \n        # 5. Solve the Quadratic Program\n        opt_result = minimize(\n            fun=objective_fn,\n            x0=delta0,\n            jac=objective_jac,\n            bounds=delta_bounds,\n            constraints=constraints,\n            method='SLSQP'\n        )\n        \n        # 6. Process the result\n        if opt_result.success:\n            delta_sol = opt_result.x\n            x_cf = x + delta_sol\n            results.append(x_cf.tolist())\n        else:\n            # If optimization fails, no feasible counterfactual was found\n            results.append([])\n            \n    # Format the final output string exactly as specified\n    output_parts = []\n    for res in results:\n        if not res:\n            output_parts.append(\"[]\")\n        else:\n            output_parts.append(f\"[{','.join(f'{val:.8f}' for val in res)}]\")\n            \n    final_output_str = f\"[{','.join(output_parts)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}