{
    "hands_on_practices": [
        {
            "introduction": "Before we can analyze data from diverse sources, we must first ensure it speaks a common language. This practice focuses on value-level standardization, a fundamental step where we convert measurements into consistent units. Here, you will tackle a common scenario of harmonizing glucose lab values, applying dimensional analysis to ensure data consistency within a Common Data Model .",
            "id": "4829224",
            "problem": "A hospital is harmonizing laboratory results from Informatics for Integrating Biology and the Bedside (i2b2) to the Observational Medical Outcomes Partnership (OMOP) Common Data Model. As part of unit standardization guided by the Unified Code for Units of Measure (UCUM) and Logical Observation Identifiers Names and Codes (LOINC), two fasting plasma glucose observations for the same patient encounter are being evaluated: one recorded as $180\\,\\text{mg/dL}$ and the other recorded as $10\\,\\text{mmol/L}$. For glucose, it is a well-tested clinical fact that $1\\,\\text{mmol/L} = 18\\,\\text{mg/dL}$ based on its molecular mass.\n\nUsing dimensional analysis consistent with UCUM, convert both observations to the common unit $\\text{mg/dL}$ and compute the absolute difference $\\Delta$ between the two standardized values, defined as\n$$\n\\Delta = \\left|C_{1,\\text{mg/dL}} - C_{2,\\text{mg/dL}}\\right|.\n$$\nExpress your final answer in $\\text{mg/dL}$. No rounding is required. The purpose of $\\Delta$ is to determine whether the two observations are consistent representations of the same concentration under OMOP unit standardization.",
            "solution": "The problem requires the standardization of two fasting plasma glucose concentration measurements to a common unit, $\\text{mg/dL}$, and the subsequent calculation of the absolute difference between them. This is a standard procedure in clinical data harmonization, particularly when migrating or integrating data between different electronic health record systems or research databases like i2b2 and OMOP. The process relies on dimensional analysis, using conversion factors derived from fundamental physicochemical properties, in this case, the molar mass of glucose.\n\nThe givens are:\n- First concentration, $C_1 = 180\\,\\text{mg/dL}$.\n- Second concentration, $C_2 = 10\\,\\text{mmol/L}$.\n- The conversion factor for glucose: $1\\,\\text{mmol/L} = 18\\,\\text{mg/dL}$. This is stated as a well-tested clinical fact for the purposes of this problem.\n- The target unit for both concentrations is $\\text{mg/dL}$.\n- The quantity to be computed is the absolute difference, $\\Delta = |C_{1,\\text{mg/dL}} - C_{2,\\text{mg/dL}}|$.\n\nThe first step is to ensure both measurements are expressed in the same units. The first measurement, $C_1$, is already in the target units:\n$$\nC_{1,\\text{mg/dL}} = 180\\,\\text{mg/dL}\n$$\n\nThe second step is to convert the second measurement, $C_2$, from $\\text{mmol/L}$ to $\\text{mg/dL}$. We use the provided conversion factor. Dimensional analysis dictates that we multiply the original value by a form of the conversion factor that cancels the original units and introduces the desired units. The conversion factor can be expressed as a ratio equal to unity:\n$$\n\\frac{18\\,\\text{mg/dL}}{1\\,\\text{mmol/L}} = 1\n$$\nMultiplying $C_2$ by this ratio, we get:\n$$\nC_{2,\\text{mg/dL}} = C_2 \\times \\frac{18\\,\\text{mg/dL}}{1\\,\\text{mmol/L}} = 10\\,\\text{mmol/L} \\times \\frac{18\\,\\text{mg/dL}}{1\\,\\text{mmol/L}}\n$$\nThe units of $\\text{mmol/L}$ cancel out, leaving the desired units of $\\text{mg/dL}$:\n$$\nC_{2,\\text{mg/dL}} = 10 \\times 18\\,\\text{mg/dL} = 180\\,\\text{mg/dL}\n$$\nThus, after standardization, the second concentration is $180\\,\\text{mg/dL}$.\n\nThe final step is to compute the absolute difference, $\\Delta$, between the two standardized concentrations.\n$$\n\\Delta = |C_{1,\\text{mg/dL}} - C_{2,\\text{mg/dL}}|\n$$\nSubstituting the standardized values:\n$$\n\\Delta = |180\\,\\text{mg/dL} - 180\\,\\text{mg/dL}|\n$$\n$$\n\\Delta = |0\\,\\text{mg/dL}|\n$$\n$$\n\\Delta = 0\\,\\text{mg/dL}\n$$\nThe absolute difference between the two measurements is $0\\,\\text{mg/dL}$. This indicates that, according to the provided clinical conversion factor, the two observations are perfectly consistent and represent the identical value. In the context of a data quality assessment during a migration from i2b2 to OMOP, a difference of $0$ would confirm that the data harmonization for this specific observation is correct and no data discrepancy exists.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Real-world clinical data is often messy, and the same event can be recorded multiple times with slight variations. This exercise challenges you to act as a data quality expert, defining a robust algorithm to deduplicate records and select the single most informative one based on principles like data granularity and provenance. This skill is critical for building a reliable and trustworthy dataset for research .",
            "id": "4829231",
            "problem": "A hospital research data warehouse stores laboratory results under both the Observational Medical Outcomes Partnership (OMOP) Common Data Model and the Informatics for Integrating Biology and the Bedside (i2b2) star schema. In OMOP, a laboratory result is represented as a `MEASUREMENT` record whose core identity comprises the patient identifier, the visit occurrence, the measurement concept (e.g., Logical Observation Identifiers Names and Codes (LOINC) term), the measurement datetime, the value (numeric or categorical), and the unit in Unified Code for Units of Measure (UCUM). In i2b2, the analogous information resides in `OBSERVATION_FACT` rows keyed by patient, encounter, concept, modifier, and datetime. In an Extract-Transform-Load (ETL) process, repeated feeds from multiple sources can create repeated lab results within a single encounter for the same patient and the same LOINC concept, with slight differences due to unit systems, rounding, or provenance.\n\nConsider a single patient with identifier $p$, a single visit occurrence $v$, and the LOINC concept for serum or plasma glucose $\\ell$. Within this encounter, the ETL produced three candidate `MEASUREMENT` records for the same laboratory event. Each record has attributes as follows:\n\n- Record $R_1$: `measurement_datetime` $t_1$ is $09{:}15{:}00$, `value_as_number` $x_1 = 105$, `unit` $\\mathrm{mg/dL}$, `normal_range_low` $= 70$, `normal_range_high` $= 99$, provenance reliability score $r_1 = 0.90$, device origin from the central laboratory analyzer, `unit_concept_id` is a standard UCUM concept, `measurement_concept_id` equals $\\ell$, and specimen identifier present.\n- Record $R_2$: `measurement_datetime` $t_2$ is $09{:}15{:}00$, `value_as_concept` $y_2 = \\text{High}$, `unit` is missing, normal ranges missing, provenance reliability score $r_2 = 0.60$, manual transcription origin, `measurement_concept_id` equals $\\ell$, and specimen identifier missing.\n- Record $R_3$: `measurement_datetime` $t_3$ is $09{:}14{:}30$, `value_as_number` $x_3 = 5.8$, `unit` $\\mathrm{mmol/L}$, `normal_range_low` $= 3.9$, `normal_range_high` $= 5.5$, provenance reliability score $r_3 = 0.95$, device origin from the point-of-care analyzer, `unit_concept_id` is a standard UCUM concept, `measurement_concept_id` equals $\\ell$, and specimen identifier present.\n\nAssume the widely accepted conversion for glucose between $\\mathrm{mmol/L}$ and $\\mathrm{mg/dL}$ is $1~\\mathrm{mmol/L} \\approx 18~\\mathrm{mg/dL}$. Let the ETL deduplication window be a symmetric time tolerance $\\Delta t = 300~\\mathrm{s}$, and define a numeric value tolerance $\\epsilon = 2~\\mathrm{mg/dL}$ after unit normalization to a canonical unit. The goal is to specify deduplication criteria that correctly group these records as duplicates within the encounter and to formalize a tie-breaking algorithm that preserves the single most informative OMOP `MEASUREMENT` record, consistent with common data model semantics and data quality principles (completeness, granularity, standardization, and provenance).\n\nWhich option below correctly specifies both (i) a scientifically sound deduplication criterion for repeated lab results within an encounter and (ii) a tie-breaking algorithm that preserves the most informative `MEASUREMENT` record according to the stated principles?\n\nA. Treat records as duplicates if they share $(p, v, \\ell)$, satisfy $\\lvert t_i - t_j \\rvert \\le \\Delta t$, and have normalized numeric values within tolerance after unit conversion: write numeric values in a canonical unit $\\mathrm{mg/dL}$ as $x_i^{\\ast}$, where $x_i^{\\ast} = x_i$ for $\\mathrm{mg/dL}$ and $x_i^{\\ast} = 18 \\times x_i$ for $\\mathrm{mmol/L}$; then require $\\lvert x_i^{\\ast} - x_j^{\\ast} \\rvert \\le \\epsilon$ when both are numeric. Select the single record by lexicographic maximization of the tuple $(g_i, s_i, q_i, c_i, r_i)$, where $g_i = 1$ if `value_as_number` is non-null and $g_i = 0$ otherwise, $s_i = 1$ if `unit_concept_id` is a standard UCUM concept and `measurement_concept_id` is a standard LOINC, $q_i$ is the numeric precision measured by the count of decimal places of $x_i$ (with $q_i = 0$ for categorical), $c_i$ is the count of non-null informative fields among {`value`, `unit`, normal ranges, specimen identifier}, and $r_i$ is the provenance reliability score in $[0,1]$. Break ties by earliest device-derived $t_i$.\n\nB. Treat records as duplicates if they share $(p, v, \\ell)$ regardless of time or unit, and preserve a single record by computing the arithmetic mean of all normalized numeric values: $\\bar{x}^{\\ast} = \\frac{1}{n}\\sum_{i=1}^{n} x_i^{\\ast}$, where categorical values are mapped to midrange numeric surrogates; select the latest timestamp $t_{\\max}$ and discard all original sources to retain only the averaged `MEASUREMENT`.\n\nC. Treat records as duplicates only if `measurement_datetime` is exactly equal, and select the record with the latest timestamp, ignoring unit standardization and value type; if multiple records share the latest timestamp, choose arbitrarily.\n\nD. Treat records as duplicates if they share $(p, v, \\ell)$ and select the record with maximal provenance reliability $r_i$, ignoring whether the value is numeric or categorical and ignoring unit standardization; break ties by arbitrary source preference.\n\nE. Treat records as duplicates if they share $(p, v, \\ell)$ and $\\lvert t_i - t_j \\rvert \\le \\Delta t$, and select the record whose value is farthest from the provided normal range in its native unit, on the premise that it carries the highest information content; if a record lacks normal ranges, set its deviation to $0$ so it is never selected.",
            "solution": "To solve this problem, we must first apply the deduplication criteria to identify which records represent the same clinical event, and then apply the tie-breaking algorithm to select the single most informative record.\n\n**1. Deduplication:**\nThe criteria are: shared patient ($p$), visit ($v$), and concept ($\\ell$); temporal proximity ($\\lvert t_i - t_j \\rvert \\le 300~\\mathrm{s}$); and value consistency for numeric records ($\\lvert x_i^{\\ast} - x_j^{\\ast} \\rvert \\le 2~\\mathrm{mg/dL}$).\n- **Contextual Match:** All three records share $(p, v, \\ell)$.\n- **Temporal Proximity:** $\\lvert t_1 - t_3 \\rvert = 30~\\mathrm{s} \\le 300~\\mathrm{s}$. $\\lvert t_1 - t_2 \\rvert = 0~\\mathrm{s} \\le 300~\\mathrm{s}$. All three records are within the time window.\n- **Value Consistency:** We normalize numeric values to $\\mathrm{mg/dL}$.\n    - $R_1$: $x_1^{\\ast} = 105~\\mathrm{mg/dL}$.\n    - $R_3$: $x_3^{\\ast} = 5.8~\\mathrm{mmol/L} \\times 18 = 104.4~\\mathrm{mg/dL}$.\n    - The difference is $\\lvert 105 - 104.4 \\rvert = 0.6~\\mathrm{mg/dL}$, which is $\\le 2~\\mathrm{mg/dL}$. The values are consistent. $R_2$ is categorical ('High'), which is also consistent with the numeric values being above the normal range.\nTherefore, all three records ($R_1, R_2, R_3$) correctly form a single duplicate group.\n\n**2. Tie-Breaking (Evaluating Option A's Algorithm):**\nOption A proposes a hierarchical, multi-factor algorithm for selecting the best record, which aligns with data quality principles.\n- **Value Granularity:** $R_1$ and $R_3$ are numeric, while $R_2$ is categorical. Numeric data is more granular and informative for a lab test, so $R_2$ is eliminated.\n- **Provenance & Precision:** Comparing the remaining records, $R_1$ and $R_3$:\n    - **Provenance:** $R_3$ has a higher reliability score ($r_3=0.95$) than $R_1$ ($r_1=0.90$), indicating a more trustworthy source.\n    - **Precision:** $R_3$'s value ($5.8$) has one decimal place, while $R_1$'s value ($105$) has zero, suggesting higher precision for $R_3$.\n    - **Completeness & Standardization:** Both are highly complete and use standard concepts.\nBased on superior provenance and precision, $R_3$ is the most informative record. The systematic algorithm proposed in Option A formalizes this type of logic and correctly selects $R_3$.\n\n**3. Evaluating Other Options:**\n- **B:** Averaging values fabricates data and destroys crucial provenance information, which is a poor practice in clinical data management.\n- **C:** Using an exact timestamp match for deduplication is too strict. It would fail to group $R_1$ and $R_3$, which clearly represent the same event.\n- **D:** Ignoring the critical difference between numeric and categorical values is a major flaw, as quantitative information is lost.\n- **E:** Selecting a record based on the extremity of its value is not a sound data quality principle; it conflates clinical significance with data quality.\n\nThus, Option A provides the only scientifically sound and comprehensive methodology that correctly handles both the deduplication and tie-breaking steps according to established principles of clinical data management.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A key power of a Common Data Model is its ability to summarize a patient's long-term history from individual records. In this final practice, you will apply the OMOP CDM's logic to transform discrete drug purchase records into continuous \"drug eras.\" This process is essential for creating the longitudinal features needed to study treatment patterns and adherence over time .",
            "id": "4829283",
            "problem": "In the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM), the table `DRUG_ERA` is derived from `DRUG_EXPOSURE`. A `DRUG_EXPOSURE` record with a start date and a days-supply defines an exposure interval from its start date through the inclusive end date computed from the days-supply. A `DRUG_ERA` is formed by merging `DRUG_EXPOSURE` intervals that either overlap or are separated by a gap that does not exceed a fixed persistence window. Consider a single patient with three `DRUG_EXPOSURE` records for the same ingredient, with start-day offsets relative to an index date of $0$, $25$, and $90$, each having $30$ days of supply. The persistence window is $30$ days.\n\nStarting from the definitions above and without invoking any shortcut formulas, derive the resulting `DRUG_ERA` intervals by:\n- constructing each `DRUG_EXPOSURE` interval from its start day and days-supply, and\n- applying the persistence-window rule to determine whether intervals should be merged into a single `DRUG_ERA`.\n\nExpress the final `DRUG_ERA` start and end day offsets as exact integers, ordered chronologically, and provide the four numbers $(s_{1}, e_{1}, s_{2}, e_{2})$ in a single row matrix. No rounding is required. Report day offsets relative to the index date as plain integers without units.",
            "solution": "The goal is to derive `DRUG_ERA` intervals from a series of `DRUG_EXPOSURE` records based on the OMOP CDM definitions.\n\n**Step 1: Define `DRUG_EXPOSURE` Intervals**\nAn exposure interval runs from its start day to its end day, inclusive. The end day is calculated as `start_day + days_supply - 1`.\nGiven a `days_supply` of 30 for all records:\n- Exposure 1 ($E_1$): Starts at day 0. Interval is $[0, 0 + 30 - 1] = [0, 29]$.\n- Exposure 2 ($E_2$): Starts at day 25. Interval is $[25, 25 + 30 - 1] = [25, 54]$.\n- Exposure 3 ($E_3$): Starts at day 90. Interval is $[90, 90 + 30 - 1] = [90, 119]$.\n\n**Step 2: Merge Intervals into `DRUG_ERA`s**\nIntervals are merged if the gap between them is less than or equal to the `persistence_window` of 30 days. The gap is calculated as `next_start_day - previous_end_day - 1`.\n\n1.  **Start with $E_1$**: The first era begins with $E_1$, so the current era under construction is $[0, 29]$.\n2.  **Consider $E_2$**: The gap between the current era (ending day 29) and $E_2$ (starting day 25) is $25 - 29 - 1 = -5$ days. A negative gap indicates the intervals overlap. Since $-5 \\le 30$, we merge them. The new, combined era spans from the start of the first interval to the end of the second: $[0, 54]$.\n3.  **Consider $E_3$**: The gap between the current era (ending day 54) and $E_3$ (starting day 90) is $90 - 54 - 1 = 35$ days.\nSince $35 > 30$, the gap exceeds the persistence window. Therefore, we do not merge $E_3$ with the current era.\n\n**Step 3: Finalize Eras**\n- The first merge process stops. The first `DRUG_ERA` is finalized as **[0, 54]**.\n- A new `DRUG_ERA` begins with the unmerged exposure, $E_3$. Since there are no more exposures to consider, this second era is finalized as **[90, 119]**.\n\nThe resulting two drug eras have start and end days of (0, 54) and (90, 119). The final answer combines these four numbers into the required matrix format.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 54 & 90 & 119\n\\end{pmatrix}\n}\n$$"
        }
    ]
}