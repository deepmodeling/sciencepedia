{
    "hands_on_practices": [
        {
            "introduction": "In prenatal screening, a raw measurement like nuchal translucency (NT) is only interpretable when compared to the expected value for a given gestational age. This exercise  provides hands-on practice in converting a raw NT value into standardized forms, such as the Multiple of the Median (MoM) and the z-score. Mastering this fundamental conversion is the first and most critical step in quantifying how unusual a marker is, paving the way for its use in risk calculation.",
            "id": "5057004",
            "problem": "A fetus is assessed at a Crown–Rump Length (CRL) of $60$ mm. The sonographer measures a Nuchal Translucency (NT) thickness of $2.8$ mm. The population reference for this CRL indicates an expected median NT of $1.5$ mm. In first-trimester genetic screening, risk modeling commonly uses standardized representations of markers: the difference from the expected median (denoted $\\Delta \\mathrm{NT}$), the Multiples of the Median (MoM), and a $z$-score that reflects how many Standard Deviations (SD) the measurement lies above or below the population mean at that gestational size. Assume the between-individual variability of NT at this CRL can be approximated by a constant SD of $\\sigma = 0.5$ mm.\n\nUsing these data, compute $\\Delta \\mathrm{NT}$ in mm, NT MoM, and the standardized $z$-score. Round the MoM and $z$-score to two decimal places. Select the option that lists all three values in the order: $\\Delta \\mathrm{NT}$ (mm), MoM, $z$.\n\nA. $\\Delta \\mathrm{NT} = 1.30$ mm; MoM $= 1.87$; $z = 2.60$\n\nB. $\\Delta \\mathrm{NT} = -1.30$ mm; MoM $= 0.54$; $z = -2.60$\n\nC. $\\Delta \\mathrm{NT} = 1.30$ mm; MoM $= 0.87$; $z = 2.60$\n\nD. $\\Delta \\mathrm{NT} = 1.30$ mm; MoM $= 1.87$; $z = 5.60$",
            "solution": "The task draws on fundamental statistical standardization and screening marker conventions used in prenatal genetics:\n\n1. By definition, $\\Delta \\mathrm{NT}$ is the deviation of the measured NT from the CRL-specific expected central tendency (here the median). Therefore, $\\Delta \\mathrm{NT}$ is computed as the measured value minus the expected median. With measured NT $= 2.8$ mm and expected median $= 1.5$ mm,\n$$\n\\Delta \\mathrm{NT} = 2.8 - 1.5 = 1.3 \\text{ mm}.\n$$\n\n2. The Multiples of the Median (MoM) represents a scale-free ratio comparing the measured marker to the expected median at the same gestational size. Hence,\n$$\n\\mathrm{MoM} = \\frac{2.8}{1.5} = 1.866\\overline{6} \\approx 1.87.\n$$\n\n3. The standardized $z$-score quantifies the number of Standard Deviations (SD) that the observation lies from the population mean (here, the expected median is used as the central value for standardization at this CRL). Using the standardization rule,\n$$\nz = \\frac{(2.8 - 1.5)}{0.5} = \\frac{1.3}{0.5} = 2.6 \\approx 2.60.\n$$\n\nThus, the correct triple is $\\Delta \\mathrm{NT} = 1.30$ mm, $\\mathrm{MoM} = 1.87$, and $z = 2.60$.\n\nOption-by-option analysis:\n\n- Option A: $\\Delta \\mathrm{NT} = 1.30$ mm; $\\mathrm{MoM} = 1.87$; $z = 2.60$. These match the calculations above. Verdict — Correct.\n\n- Option B: $\\Delta \\mathrm{NT} = -1.30$ mm; $\\mathrm{MoM} = 0.54$; $z = -2.60$. This inverts the difference and the ratio: $\\Delta \\mathrm{NT}$ should be $+1.30$ mm, not negative, and MoM should be $\\frac{2.8}{1.5}$, not $\\frac{1.5}{2.8}$. The $z$ also takes the wrong sign. Verdict — Incorrect.\n\n- Option C: $\\Delta \\mathrm{NT} = 1.30$ mm; $\\mathrm{MoM} = 0.87$; $z = 2.60$. The $z$ is correct, but the MoM is incorrectly computed as a fractional change $\\frac{2.8 - 1.5}{1.5} = 0.87$ rather than the ratio $\\frac{2.8}{1.5}$. Verdict — Incorrect.\n\n- Option D: $\\Delta \\mathrm{NT} = 1.30$ mm; $\\mathrm{MoM} = 1.87$; $z = 5.60$. The $\\Delta \\mathrm{NT}$ and MoM are correct, but the $z$ is incorrectly computed as $\\frac{2.8}{0.5} = 5.6$ instead of using the deviation from the central value. Verdict — Incorrect.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The \"median\" used to calculate a MoM is not a fixed value; it is influenced by various maternal characteristics. This practice  delves into the critical process of adjusting raw MoMs for patient-specific covariates, such as maternal weight and smoking status. By applying multiplicative correction factors, you will learn how to normalize marker levels to a common reference, ensuring that the final risk assessment is accurate and personalized.",
            "id": "5056968",
            "problem": "A maternal serum screening protocol expresses observed biomarker concentrations as Multiples of the Median (MoM) to standardize values across gestational ages. In first-trimester screening, typical biomarkers include Pregnancy-Associated Plasma Protein A (PAPP-A), free beta subunit of Human Chorionic Gonadotropin (free $\\beta$-hCG), and Nuchal Translucency (NT). The MoM for a marker is defined as $ \\text{MoM} = \\dfrac{\\text{observed concentration}}{\\text{expected median concentration at the same gestational age}} $, where the expected median concentration is estimated for a defined reference population. It is well established that maternal covariates such as maternal weight, smoking status, and Type 1 Diabetes Mellitus (DM1) alter the expected median concentrations through empirically determined scaling factors derived from population models, and that these effects are treated multiplicatively under the assumption that each covariate independently scales the expected median.\n\nStarting from the definition of MoM and the concept that covariates scale the expected median by strictly positive multiplicative factors, derive the mathematical relationship between a raw MoM (computed against a reference median without covariate adjustment) and an adjusted MoM that normalizes the raw MoM to the reference population with no covariate effects. Assume the following covariates apply: maternal weight of 90 kg, smoker, and Type 1 Diabetes Mellitus, and that the corresponding correction factors are given as $c_w = 0.85$, $c_{\\text{smk}} = 1.1$, and $c_{\\text{DM1}} = 0.9$. The correction factors are defined such that each one represents the multiplicative scaling of the expected median attributable to the covariate relative to the reference state, with all factors strictly positive.\n\nYour task is to implement a program that, for each test case below, computes the adjusted MoMs for a fixed marker order: PAPP-A, free $\\beta$-hCG, NT, using the covariate correction factors specified above. You must express each adjusted MoM as a decimal rounded to $3$ decimal places. The adjusted MoM is dimensionless; no physical units are required.\n\nTest suite (each tuple is the ordered triple of raw MoMs for PAPP-A, free $\\beta$-hCG, NT):\n\n- Case $1$: $(0.75, 1.25, 1.30)$\n- Case $2$: $(1.00, 1.00, 1.00)$\n- Case $3$: $(3.50, 0.40, 2.20)$\n- Case $4$: $(0.05, 2.80, 1.05)$\n\nDesign for coverage:\n- Case $1$ is a typical \"happy path\" with heterogeneous raw MoMs.\n- Case $2$ tests the normalization effect when all raw MoMs equal the median $1.00$.\n- Case $3$ includes edge extremes for high and low raw MoMs.\n- Case $4$ includes a very small positive raw MoM and a high value.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case produces a list of three adjusted MoMs in the specified order and rounding. For example, your output should look like $[[a_1,a_2,a_3],[b_1,b_2,b_3],\\dots]$ with no spaces. The answers for each test case must be lists of decimal numbers rounded to $3$ decimals.",
            "solution": "The problem is scientifically and mathematically well-posed, providing a sound basis for deriving the relationship between raw and adjusted Multiples of the Median (MoM). We begin by formally defining the quantities involved.\n\nLet $C_{obs}$ represent the observed concentration of a given biomarker.\nLet $M_{ref}$ be the expected median concentration for a reference population at a specific gestational age, with no covariate effects.\nLet $M_{subj}$ be the expected median concentration for a specific subject, adjusted for her individual covariates (maternal weight, smoking status, etc.).\n\nThe problem defines the raw MoM, which we denote as $\\text{MoM}_{raw}$, as the observed concentration divided by the reference median. This is computed without accounting for patient-specific covariates.\n$$ \\text{MoM}_{raw} = \\frac{C_{obs}}{M_{ref}} $$\n\nThe adjusted MoM, which we denote as $\\text{MoM}_{adj}$, is the value used for risk assessment and is defined as the observed concentration divided by the subject-specific expected median concentration. This MoM value is normalized for the subject's covariates.\n$$ \\text{MoM}_{adj} = \\frac{C_{obs}}{M_{subj}} $$\n\nThe core principle stated is that covariates multiplicatively scale the expected median of the reference population. The correction factors provided are $c_w = 0.85$ for a maternal weight of $90$ $\\text{kg}$, $c_{\\text{smk}} = 1.1$ for smoking, and $c_{\\text{DM1}} = 0.9$ for Type $1$ Diabetes Mellitus. Since these effects are assumed to be independent and multiplicative, the subject-specific median $M_{subj}$ is related to the reference median $M_{ref}$ by the product of these factors. Let us define a total correction factor, $C_{total}$:\n$$ C_{total} = c_w \\times c_{\\text{smk}} \\times c_{\\text{DM1}} $$\nThus, the subject-specific median is:\n$$ M_{subj} = M_{ref} \\times C_{total} $$\n\nOur goal is to derive an expression for $\\text{MoM}_{adj}$ in terms of $\\text{MoM}_{raw}$. We can rearrange the definition of $\\text{MoM}_{raw}$ to solve for $C_{obs}$:\n$$ C_{obs} = \\text{MoM}_{raw} \\times M_{ref} $$\n\nNow, we substitute the expressions for $C_{obs}$ and $M_{subj}$ into the definition of $\\text{MoM}_{adj}$:\n$$ \\text{MoM}_{adj} = \\frac{C_{obs}}{M_{subj}} = \\frac{\\text{MoM}_{raw} \\times M_{ref}}{M_{ref} \\times C_{total}} $$\n\nThe term $M_{ref}$ appears in both the numerator and the denominator, allowing for cancellation. This yields the final relationship:\n$$ \\text{MoM}_{adj} = \\frac{\\text{MoM}_{raw}}{C_{total}} $$\nThis derived formula shows that to obtain the adjusted MoM, one must divide the raw MoM by the total correction factor derived from the product of all applicable individual covariate correction factors.\n\nWe now calculate the numerical value of $C_{total}$ using the provided factors:\n$$ C_{total} = 0.85 \\times 1.1 \\times 0.9 = 0.8415 $$\n\nThis correction factor applies to all three markers (PAPP-A, free $\\beta$-hCG, NT) as per the problem's setup. We can now compute the adjusted MoMs for each test case by dividing the raw MoMs by $C_{total} = 0.8415$. The results must be rounded to $3$ decimal places.\n\nFor Case $1$ with raw MoMs ($0.75$, $1.25$, $1.30$):\n- Adjusted PAPP-A MoM: $\\frac{0.75}{0.8415} \\approx 0.89126... \\rightarrow 0.891$\n- Adjusted free $\\beta$-hCG MoM: $\\frac{1.25}{0.8415} \\approx 1.48544... \\rightarrow 1.485$\n- Adjusted NT MoM: $\\frac{1.30}{0.8415} \\approx 1.54486... \\rightarrow 1.545$\nThe resulting adjusted triple is ($0.891$, $1.485$, $1.545$).\n\nApplying the same procedure to all test cases yields the following results:\n\n- Case $1$: raw MoMs ($0.75$, $1.25$, $1.30$) $\\rightarrow$ adjusted MoMs ($0.891$, $1.485$, $1.545$)\n- Case $2$: raw MoMs ($1.00$, $1.00$, $1.00$) $\\rightarrow$ adjusted MoMs ($1.188$, $1.188$, $1.188$)\n- Case $3$: raw MoMs ($3.50$, $0.40$, $2.20$) $\\rightarrow$ adjusted MoMs ($4.159$, $0.475$, $2.614$)\n- Case $4$: raw MoMs ($0.05$, $2.80$, $1.05$) $\\rightarrow$ adjusted MoMs ($0.059$, $3.327$, $1.248$)\n\nThe algorithm implemented in the final answer will systematically apply these calculations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates adjusted Multiples of the Median (MoM) for maternal serum screening\n    biomarkers based on given raw MoMs and covariate correction factors.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple contains raw MoMs for (PAPP-A, free beta-hCG, NT).\n    test_cases = [\n        (0.75, 1.25, 1.30),  # Case 1\n        (1.00, 1.00, 1.00),  # Case 2\n        (3.50, 0.40, 2.20),  # Case 3\n        (0.05, 2.80, 1.05),  # Case 4\n    ]\n\n    # Covariate correction factors for maternal weight, smoking, and DM1.\n    c_w = 0.85\n    c_smk = 1.1\n    c_dm1 = 0.9\n\n    # The total correction factor is the product of individual factors, as their\n    # effects on the expected median concentration are multiplicative.\n    c_total = c_w * c_smk * c_dm1\n\n    # List to store the results for all test cases.\n    all_results = []\n\n    for raw_moms in test_cases:\n        # Convert the tuple of raw MoMs to a numpy array for vectorized division.\n        raw_moms_array = np.array(raw_moms)\n        \n        # The adjusted MoM is the raw MoM divided by the total correction factor.\n        # This relationship was derived from the definitions of raw and adjusted MoM.\n        adjusted_moms_array = raw_moms_array / c_total\n        \n        # Round the results to 3 decimal places as required.\n        rounded_moms = np.round(adjusted_moms_array, 3)\n        \n        # Convert the numpy array back to a list for the final output format.\n        all_results.append(rounded_moms.tolist())\n\n    # The final print statement must match the exact specified format:\n    # A string representation of a list of lists, with no spaces.\n    final_output_string = str(all_results).replace(' ', '')\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "A final risk assessment requires synthesizing multiple, often correlated, streams of evidence into a single, coherent probability. This capstone practice  guides you through the construction of a complete Bayesian risk calculator, the engine at the heart of modern prenatal screening. You will implement a multivariate Gaussian model to generate a likelihood ratio from marker data and use it to update a prior risk, providing a powerful demonstration of how statistical inference translates complex biological data into a clinically meaningful result.",
            "id": "5056999",
            "problem": "You are to design and implement a general-purpose Bayesian risk calculator for first-trimester maternal serum screening and nuchal translucency assessment in medical genetics. The calculator must ingest marker Multiple of the Median (MoM) values, covariates, and a prior risk, and then compute a posterior risk by deriving a multivariate Gaussian Likelihood Ratio (LR) from first principles.\n\nFoundational bases to use:\n- Bayes’ theorem: if the hypothesis of interest has prior probability $p$, prior odds $o = \\dfrac{p}{1-p}$, and the data yield likelihood ratio $LR$, then posterior odds $o^{\\prime} = o \\cdot LR$ and posterior risk $p^{\\prime} = \\dfrac{o^{\\prime}}{1 + o^{\\prime}}$.\n- The multivariate normal distribution: for a $k$-dimensional random vector $\\mathbf{x}$ with mean $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$, the probability density is proportional to $\\exp\\!\\big(-\\tfrac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\big)$ and scales with $\\lvert \\boldsymbol{\\Sigma} \\rvert^{-\\tfrac{1}{2}}$.\n\nTarget concept to derive and implement:\n- Construct the likelihood ratio by modeling the natural logarithms of MoMs as multivariate Gaussian under two classes: unaffected (null) and affected (condition of interest). Let the observed MoM vector be $\\mathbf{m} \\in \\mathbb{R}^{k}$, and define $\\mathbf{x} = \\ln(\\mathbf{m})$ elementwise. Covariate-adjusted mean vectors are given by $\\boldsymbol{\\mu}_{0}(\\mathbf{c}) = \\boldsymbol{\\mu}_{0} + \\mathbf{B}_{0}\\mathbf{c}$ for the unaffected class and $\\boldsymbol{\\mu}_{1}(\\mathbf{c}) = \\boldsymbol{\\mu}_{1} + \\mathbf{B}_{1}\\mathbf{c}$ for the affected class, where $\\mathbf{c} \\in \\mathbb{R}^{p}$ is the covariate vector, $\\mathbf{B}_{0} \\in \\mathbb{R}^{k \\times p}$ and $\\mathbf{B}_{1} \\in \\mathbb{R}^{k \\times p}$ are covariate adjustment matrices, and $\\boldsymbol{\\Sigma}_{0}, \\boldsymbol{\\Sigma}_{1} \\in \\mathbb{R}^{k \\times k}$ are symmetric positive-definite covariance matrices. Derive the multivariate Gaussian likelihood ratio $LR$ between the affected and unaffected classes based on $\\mathbf{x}$, $\\boldsymbol{\\mu}_{j}(\\mathbf{c})$, and $\\boldsymbol{\\Sigma}_{j}$ for $j \\in \\{0,1\\}$, and then compute the posterior risk using Bayes’ theorem.\n\nArchitecture and algorithmic constraints:\n- Accept arbitrary dimension $k$ for markers and $p$ for covariates as long as matrix dimensions are compatible.\n- Use the natural logarithm for MoMs, i.e., elementwise transform $x_{i} = \\ln(m_{i})$.\n- Compute the likelihood ratio rigorously from the multivariate normal model, and combine with the prior via Bayes’ theorem to obtain the posterior risk.\n- Perform calculations in the log domain to maintain numerical stability. Use the matrix determinant logarithm via a numerically stable method when needed.\n- Units: covariate for maternal weight must be expressed in kilograms (kg); covariate for gestational age must be expressed in weeks. Risk must be expressed as a decimal (for example, $0.0123$), not using a percentage sign.\n\nInput specification for each test case:\n- The marker MoM vector $\\mathbf{m}$ of length $k$ (unitless).\n- The covariate vector $\\mathbf{c}$ of length $p$ (with entries ordered as maternal weight deviation from a reference in kilograms, followed by gestational age deviation from a reference in weeks; deviations are defined as observed minus reference).\n- The prior risk $p$ as a decimal in $(0,1)$.\n- Base mean vectors $\\boldsymbol{\\mu}_{0}, \\boldsymbol{\\mu}_{1}$ of length $k$ (specified on the natural log scale).\n- Covariate adjustment matrices $\\mathbf{B}_{0}, \\mathbf{B}_{1}$ of shape $k \\times p$.\n- Covariance matrices $\\boldsymbol{\\Sigma}_{0}, \\boldsymbol{\\Sigma}_{1}$ of shape $k \\times k$.\n\nYour program must compute the posterior risk $p^{\\prime}$ for each test case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $\\big[0.123,0.456\\big]$).\n\nTest suite:\n- Case $1$ (happy path, $k=3$, markers Free $\\beta$-human chorionic gonadotropin (Free $\\beta$-hCG), Pregnancy-Associated Plasma Protein A (PAPP-A), Nuchal Translucency (NT); $p=2$ covariates: maternal weight deviation in kg from reference 70 kg, and gestational age deviation in weeks from reference 12 weeks):\n  - $\\mathbf{m} = [\\,1.8,\\,0.55,\\,1.4\\,]$\n  - $\\mathbf{c} = [\\,-5,\\,0\\,]$\n  - $p = 0.01$\n  - $\\boldsymbol{\\mu}_{0} = [\\,0.0,\\,0.0,\\,0.0\\,]$\n  - $\\boldsymbol{\\mu}_{1} = [\\,0.6931471805599453,\\,-0.6931471805599453,\\,0.4054651081081644\\,]$\n  - $\\mathbf{B}_{0} = \\begin{bmatrix} -0.001  0.002 \\\\ -0.002  -0.001 \\\\ 0.0  0.01 \\end{bmatrix}$\n  - $\\mathbf{B}_{1} = \\begin{bmatrix} -0.001  0.002 \\\\ -0.002  -0.001 \\\\ 0.0  0.01 \\end{bmatrix}$\n  - $\\boldsymbol{\\Sigma}_{0} = \\begin{bmatrix} 0.04  0.01  0.008 \\\\ 0.01  0.03  0.006 \\\\ 0.008  0.006  0.02 \\end{bmatrix}$\n  - $\\boldsymbol{\\Sigma}_{1} = \\begin{bmatrix} 0.05  0.015  0.01 \\\\ 0.015  0.035  0.008 \\\\ 0.01  0.008  0.025 \\end{bmatrix}$\n- Case $2$ (boundary: very small prior, neutral markers, same model as Case $1$):\n  - $\\mathbf{m} = [\\,1.0,\\,1.0,\\,1.0\\,]$\n  - $\\mathbf{c} = [\\,0,\\,0\\,]$\n  - $p = 0.0001$\n  - Use the same $\\boldsymbol{\\mu}_{0}, \\boldsymbol{\\mu}_{1}, \\mathbf{B}_{0}, \\mathbf{B}_{1}, \\boldsymbol{\\Sigma}_{0}, \\boldsymbol{\\Sigma}_{1}$ as Case $1$.\n- Case $3$ (high prior, mildly normal markers, covariates off-reference, same model as Case $1$):\n  - $\\mathbf{m} = [\\,1.05,\\,0.95,\\,1.0\\,]$\n  - $\\mathbf{c} = [\\,10,\\,1\\,]$\n  - $p = 0.5$\n  - Use the same $\\boldsymbol{\\mu}_{0}, \\boldsymbol{\\mu}_{1}, \\mathbf{B}_{0}, \\mathbf{B}_{1}, \\boldsymbol{\\Sigma}_{0}, \\boldsymbol{\\Sigma}_{1}$ as Case $1$.\n- Case $4$ (edge case: single marker NT only, $k=1$, $p=2$):\n  - $\\mathbf{m} = [\\,1.1\\,]$\n  - $\\mathbf{c} = [\\,0,\\,1\\,]$\n  - $p = 0.02$\n  - $\\boldsymbol{\\mu}_{0} = [\\,0.0\\,]$\n  - $\\boldsymbol{\\mu}_{1} = [\\,0.4054651081081644\\,]$\n  - $\\mathbf{B}_{0} = \\begin{bmatrix} 0.0  0.01 \\end{bmatrix}$\n  - $\\mathbf{B}_{1} = \\begin{bmatrix} 0.0  0.01 \\end{bmatrix}$\n  - $\\boldsymbol{\\Sigma}_{0} = \\begin{bmatrix} 0.02 \\end{bmatrix}$\n  - $\\boldsymbol{\\Sigma}_{1} = \\begin{bmatrix} 0.025 \\end{bmatrix}$\n- Case $5$ (two markers: Free $\\beta$-hCG and PAPP-A, $k=2$, $p=2$):\n  - $\\mathbf{m} = [\\,2.2,\\,0.4\\,]$\n  - $\\mathbf{c} = [\\,-15,\\,0\\,]$\n  - $p = 0.02$\n  - $\\boldsymbol{\\mu}_{0} = [\\,0.0,\\,0.0\\,]$\n  - $\\boldsymbol{\\mu}_{1} = [\\,0.6931471805599453,\\,-0.6931471805599453\\,]$\n  - $\\mathbf{B}_{0} = \\begin{bmatrix} -0.001  0.002 \\\\ -0.002  -0.001 \\end{bmatrix}$\n  - $\\mathbf{B}_{1} = \\begin{bmatrix} -0.001  0.002 \\\\ -0.002  -0.001 \\end{bmatrix}$\n  - $\\boldsymbol{\\Sigma}_{0} = \\begin{bmatrix} 0.04  0.01 \\\\ 0.01  0.03 \\end{bmatrix}$\n  - $\\boldsymbol{\\Sigma}_{1} = \\begin{bmatrix} 0.05  0.015 \\\\ 0.015  0.035 \\end{bmatrix}$\n- Case $6$ (sanity check: identical class models imply $LR \\approx 1$, $k=2$, $p=2$):\n  - $\\mathbf{m} = [\\,1.2,\\,0.8\\,]$\n  - $\\mathbf{c} = [\\,3,\\,-2\\,]$\n  - $p = 0.12345$\n  - $\\boldsymbol{\\mu}_{0} = [\\,0.0,\\,0.0\\,]$\n  - $\\boldsymbol{\\mu}_{1} = [\\,0.0,\\,0.0\\,]$\n  - $\\mathbf{B}_{0} = \\begin{bmatrix} -0.001  0.001 \\\\ 0.001  -0.001 \\end{bmatrix}$\n  - $\\mathbf{B}_{1} = \\begin{bmatrix} -0.001  0.001 \\\\ 0.001  -0.001 \\end{bmatrix}$\n  - $\\boldsymbol{\\Sigma}_{0} = \\begin{bmatrix} 0.02  0.0 \\\\ 0.0  0.02 \\end{bmatrix}$\n  - $\\boldsymbol{\\Sigma}_{1} = \\begin{bmatrix} 0.02  0.0 \\\\ 0.0  0.02 \\end{bmatrix}$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the cases above, for example: $\\big[\\text{case1},\\text{case2},\\ldots,\\text{case6}\\big]$. Each entry must be a decimal representing the posterior risk for that case. No other text should be printed.",
            "solution": "The problem requires the design and implementation of a Bayesian risk calculator. The methodology is grounded in Bayes' theorem and the modeling of biological marker data using multivariate Gaussian distributions. This approach is a cornerstone of modern medical diagnostics and risk assessment, particularly in the context of prenatal screening. The solution will be presented in two parts: first, a rigorous derivation of the necessary mathematical formulas from first principles, and second, an algorithmic specification for implementation.\n\n### 1. Theoretical Foundation: Bayesian Inference\n\nThe core of the problem is to update a prior belief about a hypothesis in light of new evidence. Let $H_1$ be the hypothesis that a condition is present (e.g., a chromosomal aneuploidy) and $H_0$ be the null hypothesis that the condition is absent.\n\nBayes' theorem relates the posterior probability $P(H_1 | \\text{data})$ to the prior probability $P(H_1)$ and the likelihood of the data under each hypothesis. It is often more convenient to work with odds, where the odds of an event with probability $p$ are defined as $o = \\frac{p}{1-p}$.\n\nThe Bayesian update in terms of odds is given by:\n$$ \\text{Posterior Odds} = \\text{Prior Odds} \\times \\text{Likelihood Ratio} $$\nOr, letting $o = \\frac{P(H_1)}{1-P(H_1)}$ be the prior odds and $o'$ be the posterior odds:\n$$ o' = o \\cdot LR $$\nwhere the Likelihood Ratio ($LR$) is defined as the ratio of the probability of observing the data if the condition is present to the probability of observing the data if the condition is absent:\n$$ LR = \\frac{P(\\text{data} | H_1)}{P(\\text{data} | H_0)} $$\nOnce the posterior odds $o'$ are calculated, they can be converted back to a posterior probability (risk), $p'$, using the relation:\n$$ p' = \\frac{o'}{1+o'} $$\n\n### 2. Likelihood Model: The Multivariate Gaussian Distribution\n\nThe problem states that the evidence, or data, consists of a vector of $k$ marker Multiple of the Median (MoM) values, $\\mathbf{m}$. For statistical modeling, it is standard practice to use the natural logarithm of these values, so we define a vector $\\mathbf{x} = \\ln(\\mathbf{m})$, where the logarithm is applied element-wise.\n\nThe vector $\\mathbf{x}$ is modeled as being drawn from a $k$-dimensional multivariate Gaussian (normal) distribution. The parameters of this distribution (mean vector and covariance matrix) depend on whether the hypothesis $H_1$ (affected) or $H_0$ (unaffected) is true. Furthermore, these means are adjusted for $p$ covariates (e.g., maternal weight, gestational age) contained in a vector $\\mathbf{c}$.\n\nThe probability density function (PDF) for a $k$-variate normal random vector $\\mathbf{z}$ with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$ is:\n$$ f(\\mathbf{z}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{k/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{z} - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{z} - \\boldsymbol{\\mu})\\right) $$\nwhere $|\\boldsymbol{\\Sigma}|$ is the determinant of the covariance matrix and $\\boldsymbol{\\Sigma}^{-1}$ is its inverse.\n\nFor our specific problem, we have two such distributions:\n- **Unaffected Class ($H_0$):** $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0(\\mathbf{c}), \\boldsymbol{\\Sigma}_0)$\n- **Affected Class ($H_1$):** $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_1(\\mathbf{c}), \\boldsymbol{\\Sigma}_1)$\n\nThe covariate-adjusted mean vectors are defined by a linear model:\n$$ \\boldsymbol{\\mu}_0(\\mathbf{c}) = \\boldsymbol{\\mu}_0 + \\mathbf{B}_0 \\mathbf{c} $$\n$$ \\boldsymbol{\\mu}_1(\\mathbf{c}) = \\boldsymbol{\\mu}_1 + \\mathbf{B}_1 \\mathbf{c} $$\nwhere $\\boldsymbol{\\mu}_0$ and $\\boldsymbol{\\mu}_1$ are the base mean vectors, and $\\mathbf{B}_0$ and $\\mathbf{B}_1$ are the $k \\times p$ covariate adjustment matrices.\n\n### 3. Derivation of the Likelihood Ratio\n\nUsing the PDF of the multivariate normal distribution, the likelihood ratio $LR$ for an observed vector $\\mathbf{x}$ is:\n$$ LR(\\mathbf{x}) = \\frac{f(\\mathbf{x}; \\boldsymbol{\\mu}_1(\\mathbf{c}), \\boldsymbol{\\Sigma}_1)}{f(\\mathbf{x}; \\boldsymbol{\\mu}_0(\\mathbf{c}), \\boldsymbol{\\Sigma}_0)} = \\frac{\\frac{1}{(2\\pi)^{k/2} |\\boldsymbol{\\Sigma}_1|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_1(\\mathbf{c}))^{\\top} \\boldsymbol{\\Sigma}_1^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1(\\mathbf{c}))\\right)}{\\frac{1}{(2\\pi)^{k/2} |\\boldsymbol{\\Sigma}_0|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_0(\\mathbf{c}))^{\\top} \\boldsymbol{\\Sigma}_0^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_0(\\mathbf{c}))\\right)} $$\nThe constant factor $(2\\pi)^{k/2}$ cancels. Grouping the determinant and exponential terms yields:\n$$ LR(\\mathbf{x}) = \\left(\\frac{|\\boldsymbol{\\Sigma}_0|}{|\\boldsymbol{\\Sigma}_1|}\\right)^{1/2} \\exp\\left[\\frac{1}{2} \\left( (\\mathbf{x} - \\boldsymbol{\\mu}_0(\\mathbf{c}))^{\\top} \\boldsymbol{\\Sigma}_0^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_0(\\mathbf{c})) - (\\mathbf{x} - \\boldsymbol{\\mu}_1(\\mathbf{c}))^{\\top} \\boldsymbol{\\Sigma}_1^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1(\\mathbf{c})) \\right) \\right] $$\n\n### 4. Numerical Stability and Log-Domain Computation\n\nDirect computation of the $LR$ can be numerically unstable, as determinants can be very small or large, and the exponential function can lead to overflow or underflow. The problem correctly mandates computation in the logarithmic domain.\n\nTaking the natural logarithm of the $LR$ expression gives the log-likelihood ratio, $\\ln(LR)$:\n$$ \\ln(LR) = \\frac{1}{2} \\ln\\left(\\frac{|\\boldsymbol{\\Sigma}_0|}{|\\boldsymbol{\\Sigma}_1|}\\right) + \\frac{1}{2} \\left[ (\\mathbf{x} - \\boldsymbol{\\mu}_0(\\mathbf{c}))^{\\top} \\boldsymbol{\\Sigma}_0^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_0(\\mathbf{c})) - (\\mathbf{x} - \\boldsymbol{\\mu}_1(\\mathbf{c}))^{\\top} \\boldsymbol{\\Sigma}_1^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1(\\mathbf{c})) \\right] $$\nRearranging the terms, we get the final form for computation:\n$$ \\ln(LR) = \\frac{1}{2} \\left( \\ln|\\boldsymbol{\\Sigma}_0| - \\ln|\\boldsymbol{\\Sigma}_1| - M_1^2 + M_0^2 \\right) $$\nwhere $M_j^2 = (\\mathbf{x} - \\boldsymbol{\\mu}_j(\\mathbf{c}))^{\\top} \\boldsymbol{\\Sigma}_j^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_j(\\mathbf{c}))$ is the squared Mahalanobis distance of the observation $\\mathbf{x}$ from the mean of class $j$.\n\nThe Bayesian update is also performed in the log domain. Let the prior risk be $p$. The prior odds are $o = p/(1-p)$, so the log-prior-odds are $\\ln(o) = \\ln(p) - \\ln(1-p)$. This is the logit function, $\\text{logit}(p)$.\nThe log-posterior-odds are then:\n$$ \\ln(o') = \\ln(o) + \\ln(LR) = \\text{logit}(p) + \\ln(LR) $$\nFinally, to recover the posterior risk $p'$, we invert the logit function. The inverse logit function is the logistic sigmoid function:\n$$ p' = \\frac{e^{\\ln(o')}}{1+e^{\\ln(o')}} = \\frac{1}{1+e^{-\\ln(o')}} $$\nThis formulation is numerically robust.\n\n### 5. Algorithmic Procedure\n\nFor each test case, the calculation proceeds as follows:\n\n1.  **Initialization**: Receive the input parameters: MoM vector $\\mathbf{m}$, covariate vector $\\mathbf{c}$, prior risk $p$, and the Gaussian model parameters $\\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\mathbf{B}_0, \\mathbf{B}_1, \\boldsymbol{\\Sigma}_0, \\boldsymbol{\\Sigma}_1$.\n2.  **Data Transformation**: Compute the log-MoM vector: $\\mathbf{x} = \\ln(\\mathbf{m})$.\n3.  **Mean Adjustment**: Calculate the covariate-adjusted mean vectors for both classes:\n    - $\\boldsymbol{\\mu}_{0,adj} = \\boldsymbol{\\mu}_0 + \\mathbf{B}_0 \\mathbf{c}$\n    - $\\boldsymbol{\\mu}_{1,adj} = \\boldsymbol{\\mu}_1 + \\mathbf{B}_1 \\mathbf{c}$\n4.  **Log-Determinant Calculation**: Compute the natural logarithm of the determinants of the covariance matrices, $\\ln|\\boldsymbol{\\Sigma}_0|$ and $\\ln|\\boldsymbol{\\Sigma}_1|$. This should be done using a numerically stable function that avoids computing the determinant directly before taking the logarithm.\n5.  **Mahalanobis Distance Calculation**: For each class $j \\in \\{0, 1\\}$, calculate the squared Mahalanobis distance $M_j^2$. A computationally stable method is to solve the linear system $\\boldsymbol{\\Sigma}_j \\mathbf{y}_j = (\\mathbf{x} - \\boldsymbol{\\mu}_{j,adj})$ for $\\mathbf{y}_j$, and then compute the dot product $M_j^2 = (\\mathbf{x} - \\boldsymbol{\\mu}_{j,adj})^{\\top} \\mathbf{y}_j$. This avoids explicit matrix inversion.\n6.  **Log-Likelihood Ratio Calculation**: Assemble the $\\ln(LR)$ using the formula derived in Section 4.\n    $$ \\ln(LR) = \\frac{1}{2} \\left( \\ln|\\boldsymbol{\\Sigma}_0| - \\ln|\\boldsymbol{\\Sigma}_1| - M_1^2 + M_0^2 \\right) $$\n7.  **Bayesian Update**:\n    a. Compute the log-prior-odds: $\\ln(o) = \\text{logit}(p)$.\n    b. Compute the log-posterior-odds: $\\ln(o') = \\ln(o) + \\ln(LR)$.\n8.  **Posterior Risk Calculation**: Convert the log-posterior-odds back to a risk probability using the logistic sigmoid function: $p' = \\frac{1}{1 + \\exp(-\\ln(o'))}$.\n\nThis sequence of operations provides a robust and general algorithm for computing the posterior risk as specified by the problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logit, expit\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian risk calculation problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path, k=3, p=2)\n        {\n            \"m\": np.array([1.8, 0.55, 1.4]),\n            \"c\": np.array([-5.0, 0.0]),\n            \"p\": 0.01,\n            \"mu0\": np.array([0.0, 0.0, 0.0]),\n            \"mu1\": np.array([0.6931471805599453, -0.6931471805599453, 0.4054651081081644]),\n            \"B0\": np.array([[-0.001, 0.002], [-0.002, -0.001], [0.0, 0.01]]),\n            \"B1\": np.array([[-0.001, 0.002], [-0.002, -0.001], [0.0, 0.01]]),\n            \"Sigma0\": np.array([[0.04, 0.01, 0.008], [0.01, 0.03, 0.006], [0.008, 0.006, 0.02]]),\n            \"Sigma1\": np.array([[0.05, 0.015, 0.01], [0.015, 0.035, 0.008], [0.01, 0.008, 0.025]]),\n        },\n        # Case 2 (boundary: very small prior, neutral markers)\n        {\n            \"m\": np.array([1.0, 1.0, 1.0]),\n            \"c\": np.array([0.0, 0.0]),\n            \"p\": 0.0001,\n            \"mu0\": np.array([0.0, 0.0, 0.0]),\n            \"mu1\": np.array([0.6931471805599453, -0.6931471805599453, 0.4054651081081644]),\n            \"B0\": np.array([[-0.001, 0.002], [-0.002, -0.001], [0.0, 0.01]]),\n            \"B1\": np.array([[-0.001, 0.002], [-0.002, -0.001], [0.0, 0.01]]),\n            \"Sigma0\": np.array([[0.04, 0.01, 0.008], [0.01, 0.03, 0.006], [0.008, 0.006, 0.02]]),\n            \"Sigma1\": np.array([[0.05, 0.015, 0.01], [0.015, 0.035, 0.008], [0.01, 0.008, 0.025]]),\n        },\n        # Case 3 (high prior, mildly normal markers, covariates off-reference)\n        {\n            \"m\": np.array([1.05, 0.95, 1.0]),\n            \"c\": np.array([10.0, 1.0]),\n            \"p\": 0.5,\n            \"mu0\": np.array([0.0, 0.0, 0.0]),\n            \"mu1\": np.array([0.6931471805599453, -0.6931471805599453, 0.4054651081081644]),\n            \"B0\": np.array([[-0.001, 0.002], [-0.002, -0.001], [0.0, 0.01]]),\n            \"B1\": np.array([[-0.001, 0.002], [-0.002, -0.001], [0.0, 0.01]]),\n            \"Sigma0\": np.array([[0.04, 0.01, 0.008], [0.01, 0.03, 0.006], [0.008, 0.006, 0.02]]),\n            \"Sigma1\": np.array([[0.05, 0.015, 0.01], [0.015, 0.035, 0.008], [0.01, 0.008, 0.025]]),\n        },\n        # Case 4 (edge case: single marker, k=1, p=2)\n        {\n            \"m\": np.array([1.1]),\n            \"c\": np.array([0.0, 1.0]),\n            \"p\": 0.02,\n            \"mu0\": np.array([0.0]),\n            \"mu1\": np.array([0.4054651081081644]),\n            \"B0\": np.array([[0.0, 0.01]]),\n            \"B1\": np.array([[0.0, 0.01]]),\n            \"Sigma0\": np.array([[0.02]]),\n            \"Sigma1\": np.array([[0.025]]),\n        },\n        # Case 5 (two markers, k=2, p=2)\n        {\n            \"m\": np.array([2.2, 0.4]),\n            \"c\": np.array([-15.0, 0.0]),\n            \"p\": 0.02,\n            \"mu0\": np.array([0.0, 0.0]),\n            \"mu1\": np.array([0.6931471805599453, -0.6931471805599453]),\n            \"B0\": np.array([[-0.001, 0.002], [-0.002, -0.001]]),\n            \"B1\": np.array([[-0.001, 0.002], [-0.002, -0.001]]),\n            \"Sigma0\": np.array([[0.04, 0.01], [0.01, 0.03]]),\n            \"Sigma1\": np.array([[0.05, 0.015], [0.015, 0.035]]),\n        },\n        # Case 6 (sanity check: identical models)\n        {\n            \"m\": np.array([1.2, 0.8]),\n            \"c\": np.array([3.0, -2.0]),\n            \"p\": 0.12345,\n            \"mu0\": np.array([0.0, 0.0]),\n            \"mu1\": np.array([0.0, 0.0]),\n            \"B0\": np.array([[-0.001, 0.001], [0.001, -0.001]]),\n            \"B1\": np.array([[-0.001, 0.001], [0.001, -0.001]]),\n            \"Sigma0\": np.array([[0.02, 0.0], [0.0, 0.02]]),\n            \"Sigma1\": np.array([[0.02, 0.0], [0.0, 0.02]]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        p_posterior = calculate_posterior_risk(\n            m=case[\"m\"],\n            c=case[\"c\"],\n            p_prior=case[\"p\"],\n            mu0=case[\"mu0\"],\n            mu1=case[\"mu1\"],\n            B0=case[\"B0\"],\n            B1=case[\"B1\"],\n            Sigma0=case[\"Sigma0\"],\n            Sigma1=case[\"Sigma1\"],\n        )\n        results.append(p_posterior)\n\n    print(f\"[{','.join(f'{r:.15f}'.rstrip('0').rstrip('.') for r in results)}]\")\n\ndef calculate_posterior_risk(m, c, p_prior, mu0, mu1, B0, B1, Sigma0, Sigma1):\n    \"\"\"\n    Computes the posterior risk using a multivariate Gaussian likelihood ratio.\n\n    Args:\n        m (np.ndarray): Marker MoM vector (k,).\n        c (np.ndarray): Covariate vector (p,).\n        p_prior (float): Prior risk.\n        mu0 (np.ndarray): Base mean vector for unaffected class (k,).\n        mu1 (np.ndarray): Base mean vector for affected class (k,).\n        B0 (np.ndarray): Covariate adjustment matrix for unaffected class (k, p).\n        B1 (np.ndarray): Covariate adjustment matrix for affected class (k, p).\n        Sigma0 (np.ndarray): Covariance matrix for unaffected class (k, k).\n        Sigma1 (np.ndarray): Covariance matrix for affected class (k, k).\n\n    Returns:\n        float: The calculated posterior risk.\n    \"\"\"\n    # Step 1: Transform marker MoMs to log-space\n    x = np.log(m)\n\n    # Step 2: Calculate covariate-adjusted means\n    mu0_adj = mu0 + B0 @ c\n    mu1_adj = mu1 + B1 @ c\n\n    # Step 3: Calculate components for the log-likelihood ratio\n    \n    # Log-determinants using a numerically stable method\n    sign0, log_det0 = np.linalg.slogdet(Sigma0)\n    sign1, log_det1 = np.linalg.slogdet(Sigma1)\n\n    # The covariance matrices must be positive-definite, so signs must be +1.\n    # An error here would indicate an invalid (e.g., non-PD) matrix.\n    if sign0 = 0 or sign1 = 0:\n        raise ValueError(\"Covariance matrix is not positive-definite.\")\n\n    # Squared Mahalanobis distances\n    # M_j^2 = (x - mu_j)^T * Sigma_j^-1 * (x - mu_j)\n    # Calculated by solving the linear system Sigma_j * y = (x - mu_j)\n    # and then taking the dot product y^T * (x - mu_j) to avoid matrix inversion.\n    \n    # Class 0 (unaffected)\n    d0 = x - mu0_adj\n    y0 = np.linalg.solve(Sigma0, d0)\n    m_dist_sq_0 = d0 @ y0\n    \n    # Class 1 (affected)\n    d1 = x - mu1_adj\n    y1 = np.linalg.solve(Sigma1, d1)\n    m_dist_sq_1 = d1 @ y1\n\n    # Step 4: Assemble the log-likelihood ratio (log_LR)\n    log_lr = 0.5 * (log_det0 - log_det1 - m_dist_sq_1 + m_dist_sq_0)\n\n    # Step 5: Perform the Bayesian update in log-odds space\n    # logit(p) gives the log-prior-odds\n    log_prior_odds = logit(p_prior)\n    \n    # Add log-LR to get log-posterior-odds\n    log_posterior_odds = log_prior_odds + log_lr\n\n    # Step 6: Convert log-posterior-odds back to posterior risk\n    # expit(x) is the inverse of logit(x), i.e., the logistic sigmoid function\n    p_posterior = expit(log_posterior_odds)\n\n    return p_posterior\n\nsolve()\n```"
        }
    ]
}