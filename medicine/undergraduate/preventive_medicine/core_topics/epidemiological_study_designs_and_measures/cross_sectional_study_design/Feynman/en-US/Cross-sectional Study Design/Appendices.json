{
    "hands_on_practices": [
        {
            "introduction": "A well-designed study begins long before any data is collected. A crucial first step is to determine the appropriate sample size, which is the number of participants needed to achieve the study's objectives. This practice exercise  will guide you through the fundamental principles of sample size calculation for a cross-sectional study, ensuring your findings are both statistically meaningful and resource-efficient.",
            "id": "4517839",
            "problem": "A public health team plans a cross-sectional survey to estimate the point prevalence of daily electronic cigarette use among adults in a large metropolitan region using simple random sampling (SRS) without replacement from a population so large that the sampling fraction is negligible. The team will report a two-sided confidence interval (CI) for the prevalence based on the normal approximation to the binomial, justified by the Central Limit Theorem (CLT), and will design the sample size so that the CI has a prespecified half-width (margin of error) no larger than a target value.\n\nStarting only from the following fundamental bases:\n- Under the CLT, if $X \\sim \\operatorname{Binomial}(n,p)$ and $\\hat{p} = X/n$, then for large $n$,\n$$\n\\frac{\\hat{p}-p}{\\sqrt{p(1-p)/n}} \\approx Z,\n$$\nwhere $Z$ has the standard normal distribution with cumulative distribution function $\\Phi$.\n- A two-sided CI with nominal confidence level $1-\\alpha$ has the form $\\hat{p} \\pm z_{\\alpha/2}\\,\\sqrt{p(1-p)/n}$ when the population size is large relative to $n$, where $z_{\\alpha/2}$ satisfies $\\Pr(|Z|\\le z_{\\alpha/2}) = 1-\\alpha$.\n- The margin of error is the half-width of the CI.\n\nDerive, from these bases, an expression for the minimum integer sample size $n$ required so that the absolute margin of error does not exceed a chosen design value $E$ at nominal confidence level $1-\\alpha$, when planning is based on a prior estimate of prevalence $p^{\\ast}$. Then compute the minimum integer $n$ for the design values $p^{\\ast} = 0.12$, $E = 0.015$, and $1-\\alpha = 0.95$. Report the minimum integer sample size that guarantees the design margin of error target under these assumptions. The final answer must be a single number.",
            "solution": "The problem statement is scrutinized for validity according to the specified criteria.\n\n### Step 1: Extract Givens\n- Study type: cross-sectional survey\n- Sampling method: simple random sampling (SRS) without replacement\n- Population characteristic: large metropolitan region, such that the sampling fraction is negligible\n- Objective: estimate the point prevalence, $p$, of daily electronic cigarette use\n- Statistical method: two-sided confidence interval (CI) based on the normal approximation to the binomial, justified by the Central Limit Theorem (CLT)\n- Design constraint: the CI half-width (margin of error) must not exceed a target value, $E$.\n- Fundamental Basis 1: For $X \\sim \\operatorname{Binomial}(n,p)$ and $\\hat{p} = X/n$, the standardized sample proportion $\\frac{\\hat{p}-p}{\\sqrt{p(1-p)/n}}$ is approximately distributed as $Z$, where $Z$ is a standard normal random variable.\n- Fundamental Basis 2: A two-sided CI with nominal confidence level $1-\\alpha$ is given by $\\hat{p} \\pm z_{\\alpha/2}\\,\\sqrt{p(1-p)/n}$, where $z_{\\alpha/2}$ satisfies $\\Pr(|Z|\\le z_{\\alpha/2}) = 1-\\alpha$.\n- Fundamental Basis 3: The margin of error is the half-width of the CI.\n- Task 1: Derive an expression for the minimum integer sample size, $n$, required for the margin of error to not exceed a design value $E$ at confidence level $1-\\alpha$, using a prior estimate of prevalence $p^{\\ast}$.\n- Task 2: Compute the minimum integer $n$ for the specific design values: $p^{\\ast} = 0.12$, $E = 0.015$, and $1-\\alpha = 0.95$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for scientific grounding, well-posedness, and objectivity.\n- **Scientific and Factual Soundness:** The problem is a standard, fundamental exercise in biostatistics and epidemiology. The use of the normal approximation to the binomial distribution for constructing a confidence interval for a proportion is a classical application of the Central Limit Theorem. The formula for the confidence interval and the concept of a margin of error are textbook definitions. The entire premise is scientifically sound.\n- **Well-Posedness and Completeness:** The problem is well-posed. It requests a derivation from specified first principles and then a calculation using provided numerical values. All necessary information is present: the form of the CI, the definition of margin of error, the desired confidence level, the target margin of error, and the planning value for the prevalence. The problem structure leads to a unique, meaningful solution.\n- **Objectivity and Clarity:** The problem is stated in precise, objective, and unambiguous language common to statistics and public health research. Terms like \"point prevalence,\" \"simple random sampling,\" \"confidence level,\" and \"margin of error\" have clear, universally accepted definitions.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined, scientifically sound, and clear problem in applied statistics. The solution process will now proceed.\n\n### Derivation of the Sample Size Formula\n\nThe problem requires the derivation of the minimum sample size, $n$, from the provided bases.\n\nFrom Fundamental Basis 2 and 3, the margin of error, which we will denote as $M$, is the half-width of the confidence interval.\n$$\nM = z_{\\alpha/2}\\,\\sqrt{\\frac{p(1-p)}{n}}\n$$\nThis expression depends on the true population prevalence, $p$, which is unknown before the study is conducted. For planning purposes, as specified in the problem, we must use a prior estimate, $p^{\\ast}$, in place of $p$. The planned margin of error is therefore:\n$$\nM_{\\text{planned}} = z_{\\alpha/2}\\,\\sqrt{\\frac{p^{\\ast}(1-p^{\\ast})}{n}}\n$$\nThe design specification requires that this planned margin of error does not exceed a target value, $E$. This establishes the following inequality:\n$$\nz_{\\alpha/2}\\,\\sqrt{\\frac{p^{\\ast}(1-p^{\\ast})}{n}} \\le E\n$$\nTo find the required sample size $n$, we solve this inequality for $n$. We assume $E > 0$, $z_{\\alpha/2} > 0$, and $0  p^{\\ast}  1$, which are all true in any practical application.\n\nFirst, we square both sides of the inequality:\n$$\n\\left(z_{\\alpha/2}\\right)^2 \\frac{p^{\\ast}(1-p^{\\ast})}{n} \\le E^2\n$$\nNext, we rearrange the terms to isolate $n$. Since $n$ must be positive, we can multiply both sides by $n$ and divide by $E^2$ without changing the direction of the inequality:\n$$\n\\left(z_{\\alpha/2}\\right)^2 p^{\\ast}(1-p^{\\ast}) \\le n E^2\n$$\n$$\nn \\ge \\frac{\\left(z_{\\alpha/2}\\right)^2 p^{\\ast}(1-p^{\\ast})}{E^2}\n$$\nThe sample size, $n$, must be an integer. The inequality specifies the minimum value that $n$ can take. Therefore, the minimum integer sample size that satisfies this condition is the smallest integer greater than or equal to the expression on the right-hand side. This corresponds to the ceiling function.\n$$\nn_{\\text{min}} = \\left\\lceil \\frac{z_{\\alpha/2}^2 p^{\\ast}(1-p^{\\ast})}{E^2} \\right\\rceil\n$$\nThis is the required derived expression for the minimum integer sample size.\n\n### Computation of the Minimum Sample Size\n\nNow, we compute the value of $n$ using the provided design values:\n- Prior prevalence estimate: $p^{\\ast} = 0.12$\n- Target margin of error: $E = 0.015$\n- Nominal confidence level: $1-\\alpha = 0.95$\n\nFirst, we must determine the value of $z_{\\alpha/2}$. The confidence level is $1-\\alpha = 0.95$, which implies $\\alpha = 0.05$ and $\\alpha/2 = 0.025$. The value $z_{0.025}$ is the critical value from the standard normal distribution $Z$ that satisfies $\\Pr(|Z| \\le z_{0.025}) = 0.95$. This is equivalent to finding the value such that the area in the upper tail is $0.025$, i.e., $\\Pr(Z > z_{0.025}) = 0.025$. The cumulative probability is $\\Phi(z_{0.025}) = 1 - 0.025 = 0.975$. This is a standard quantile, and its value is approximately $1.96$. Thus, $z_{0.025} \\approx 1.96$.\n\nWe now substitute the numerical values into the derived inequality for $n$:\n$$\nn \\ge \\frac{(1.96)^2 \\times 0.12 \\times (1-0.12)}{(0.015)^2}\n$$\nWe compute the components of this expression:\n- $(1.96)^2 = 3.8416$\n- $p^{\\ast}(1-p^{\\ast}) = 0.12 \\times (0.88) = 0.1056$\n- $E^2 = (0.015)^2 = 0.000225$\n\nSubstituting these back into the inequality:\n$$\nn \\ge \\frac{3.8416 \\times 0.1056}{0.000225}\n$$\n$$\nn \\ge \\frac{0.405673056}{0.000225}\n$$\n$$\nn \\ge 1802.99136\n$$\nSince the sample size $n$ must be an integer, the minimum number of participants required is the smallest integer that is greater than or equal to $1802.99136$.\n$$\nn = \\lceil 1802.99136 \\rceil = 1803\n$$\nTherefore, the minimum integer sample size required to guarantee that the margin of error does not exceed $0.015$ at a $95\\%$ confidence level, based on a prevalence estimate of $0.12$, is $1803$.",
            "answer": "$$\n\\boxed{1803}\n$$"
        },
        {
            "introduction": "After collecting data, the core task of a cross-sectional study is to quantify the association between an exposure and an outcome. Different measures tell different parts of the story, from the absolute public health burden to the relative strength of an association. In this exercise , you will calculate and interpret the Prevalence Difference ($PD$), Prevalence Ratio ($PR$), and Prevalence Odds Ratio ($POR$) from a hypothetical dataset, learning to distinguish their unique meanings in a public health context.",
            "id": "4517878",
            "problem": "A public health team conducts a cross-sectional screening survey of adult residents to assess whether living in neighborhoods with high fine particulate matter exposure is associated with current physician-diagnosed asthma. A total of $2400$ adults are sampled at one point in time. Exposure is defined at the neighborhood level as either high or low average fine particulate matter concentration. The outcome is the presence of current physician-diagnosed asthma at the time of survey assessment. The observed counts are:\n\n- High exposure (exposed): $176$ with asthma, $924$ without asthma.\n- Low exposure (unexposed): $130$ with asthma, $1170$ without asthma.\n\nStarting only from core definitions appropriate for cross-sectional studies—that prevalence is the proportion of persons with the outcome at a specified time in a defined population, that prevalence difference is the difference in prevalence between exposure groups, that prevalence ratio is the ratio of these prevalences, and that prevalence odds ratio is the ratio of the prevalence odds between exposure groups—do the following:\n\n1. Compute the prevalence of current asthma in the exposed group and in the unexposed group from the data above.\n2. From these prevalences, compute the prevalence ratio $PR$, the prevalence difference $PD$, and the prevalence odds ratio $POR$.\n3. Provide a brief interpretation for $PR$, $PD$, and $POR$ in the context of preventive medicine, emphasizing what each measure conveys about the burden of disease and potential implications for prevention in a cross-sectional design.\n\nFinally, define the scalar quantity\n$$Q \\equiv \\ln(POR) + PR - PD,$$\nwhere $\\ln$ denotes the natural logarithm. Compute $Q$ from your results. Round your final numeric answer for $Q$ to $4$ significant figures. This quantity is dimensionless; do not include units.",
            "solution": "The problem statement is evaluated and deemed valid. It is scientifically grounded in the principles of epidemiology, well-posed with sufficient and consistent data, and objectively stated.\n\nThe problem provides data from a cross-sectional study of $2400$ adults to assess the association between exposure to high fine particulate matter and the prevalence of physician-diagnosed asthma. The data can be organized into a $2 \\times 2$ contingency table. Let $E$ denote the exposed group (high exposure) and $\\bar{E}$ denote the unexposed group (low exposure). Let $D$ denote the presence of the disease (asthma) and $\\bar{D}$ denote its absence.\n\nThe observed counts are:\n- Exposed group ($E$): $a = 176$ with asthma ($D$), $b = 924$ without asthma ($\\bar{D}$).\n- Unexposed group ($\\bar{E}$): $c = 130$ with asthma ($D$), $d = 1170$ without asthma ($\\bar{D}$).\n\nThe total number of individuals in the exposed group is $N_E = a + b = 176 + 924 = 1100$.\nThe total number of individuals in the unexposed group is $N_{\\bar{E}} = c + d = 130 + 1170 = 1300$.\nThe total sample size is $N = N_E + N_{\\bar{E}} = 1100 + 1300 = 2400$, which matches the given total.\n\n**1. Compute the prevalence of current asthma in the exposed and unexposed groups.**\n\nPrevalence is defined as the proportion of persons with the outcome at a specified time in a defined population.\n\nThe prevalence of asthma in the exposed group ($P_E$) is the number of asthma cases in the exposed group divided by the total number of individuals in the exposed group:\n$$P_E = \\frac{a}{a+b} = \\frac{176}{1100} = 0.16$$\n\nThe prevalence of asthma in the unexposed group ($P_{\\bar{E}}$) is the number of asthma cases in the unexposed group divided by the total number of individuals in the unexposed group:\n$$P_{\\bar{E}} = \\frac{c}{c+d} = \\frac{130}{1300} = 0.10$$\n\nSo, the prevalence of asthma is $16\\%$ in the high-exposure group and $10\\%$ in the low-exposure group.\n\n**2. Compute the prevalence ratio ($PR$), prevalence difference ($PD$), and prevalence odds ratio ($POR$).**\n\nThe prevalence difference ($PD$) is the absolute difference in prevalence between the two groups.\n$$PD = P_E - P_{\\bar{E}} = 0.16 - 0.10 = 0.06$$\n\nThe prevalence ratio ($PR$) is the ratio of the prevalence in the exposed group to the prevalence in the unexposed group.\n$$PR = \\frac{P_E}{P_{\\bar{E}}} = \\frac{0.16}{0.10} = 1.6$$\n\nThe prevalence odds ratio ($POR$) is the ratio of the odds of having asthma in the exposed group to the odds of having asthma in the unexposed group.\nThe odds of asthma in the exposed group are $Odds_E = \\frac{P_E}{1 - P_E} = \\frac{a/N_E}{b/N_E} = \\frac{a}{b}$.\n$$Odds_E = \\frac{176}{924} = \\frac{16 \\times 11}{84 \\times 11} = \\frac{16}{84} = \\frac{4}{21}$$\nThe odds of asthma in the unexposed group are $Odds_{\\bar{E}} = \\frac{P_{\\bar{E}}}{1 - P_{\\bar{E}}} = \\frac{c/N_{\\bar{E}}}{d/N_{\\bar{E}}} = \\frac{c}{d}$.\n$$Odds_{\\bar{E}} = \\frac{130}{1170} = \\frac{13}{117} = \\frac{1}{9}$$\nThe prevalence odds ratio is the ratio of these odds.\n$$POR = \\frac{Odds_E}{Odds_{\\bar{E}}} = \\frac{a/b}{c/d} = \\frac{ad}{bc} = \\frac{4/21}{1/9} = \\frac{4}{21} \\times 9 = \\frac{36}{21} = \\frac{12}{7}$$\nAs a decimal, $POR = \\frac{12}{7} \\approx 1.7142857$.\n\n**3. Interpretation of $PR$, $PD$, and $POR$.**\n\n- **Prevalence Difference ($PD$)**: The $PD$ is $0.06$, or $6$ percentage points. This is an absolute measure of effect. It signifies that there are $6$ excess cases of asthma for every $100$ people in the high-exposure neighborhood compared to the low-exposure neighborhood at the time of the survey. From a preventive medicine perspective, the $PD$ quantifies the absolute public health burden associated with the exposure. If the association were causal, this measure would imply that eliminating high exposure could prevent $6$ cases of asthma per $100$ people in the population.\n\n- **Prevalence Ratio ($PR$)**: The $PR$ is $1.6$. This is a relative measure of effect. It indicates that the prevalence of asthma is $1.6$ times higher (or $60\\%$ higher) in the high-exposure group compared to the low-exposure group. The $PR$ is used to describe the strength of the association between the exposure and the outcome. A value greater than $1$ suggests a positive association. In etiological research, it helps to assess the magnitude of an association.\n\n- **Prevalence Odds Ratio ($POR$)**: The $POR$ is $12/7 \\approx 1.71$. This is also a relative measure of effect. It means that the odds of having asthma are approximately $1.71$ times greater for individuals in the high-exposure group compared to those in the low-exposure group. In cross-sectional studies, the $POR$ is often used as an estimate of the association, particularly in the context of logistic regression models. When the outcome is not rare (as in this case, where prevalences are $10\\%$ and $16\\%$), the $POR$ is not a good approximation of the $PR$ ($1.71$ vs. $1.6$), and it tends to overestimate the strength of the association as measured by the $PR$. The $PR$ is generally considered more directly interpretable for prevalence data.\n\n**4. Final Calculation of $Q$.**\n\nThe scalar quantity $Q$ is defined as:\n$$Q \\equiv \\ln(POR) + PR - PD$$\nSubstituting the computed values:\n$$PR = 1.6$$\n$$PD = 0.06$$\n$$POR = \\frac{12}{7}$$\nWe have:\n$$Q = \\ln\\left(\\frac{12}{7}\\right) + 1.6 - 0.06$$\n$$Q = \\ln\\left(\\frac{12}{7}\\right) + 1.54$$\nNow, we compute the numerical value:\n$$\\ln\\left(\\frac{12}{7}\\right) \\approx \\ln(1.7142857...) \\approx 0.5389965...$$\n$$Q \\approx 0.5389965... + 1.54 = 2.0789965...$$\nRounding the result to $4$ significant figures, we get:\n$$Q \\approx 2.079$$",
            "answer": "$$\\boxed{2.079}$$"
        },
        {
            "introduction": "In the real world, our measurement tools, such as surveys, are not always perfectly accurate. It is often necessary to validate them against a more reliable \"gold standard,\" but doing so for every participant can be prohibitively expensive. This final practice problem  places you in the role of a study designer, challenging you to select a valid and feasible strategy for a verification substudy, a critical skill for ensuring the credibility of your research findings.",
            "id": "4517845",
            "problem": "A national cross-sectional survey of $N=10{,}000$ adults collects a self-reported indicator $S \\in \\{0,1\\}$ of a chronic condition. A clinical adjudication using standardized diagnostic criteria is available as the reference (gold) standard for true disease status $D \\in \\{0,1\\}$, but due to cost constraints only up to $1{,}000$ participants can undergo gold-standard verification. In the survey responses, $600$ participants report $S=1$ and $9{,}400$ report $S=0$. The public health objective is to estimate, at the population level, the sensitivity and specificity of self-report relative to the gold standard, while avoiding verification (work-up) bias and maintaining scientific validity under the cross-sectional design.\n\nYou may assume that the gold standard has no measurement error, and that any probability sampling scheme used for verification is implemented as designed. Select all options that both (i) are feasible under the stated constraint of at most $1{,}000$ verifications, and (ii) yield unbiased estimates of the population sensitivity and specificity when analyzed as described.\n\nA. Verify all $S=1$ participants and do not verify any $S=0$ participants. Estimate sensitivity as the proportion of verified $S=1$ participants who are truly diseased, and estimate specificity by treating unverified $S=0$ participants as truly non-diseased.\n\nB. Select a Simple Random Sample (SRS) of $1{,}000$ participants from the entire survey, independent of $S$, and verify them with the gold standard. Estimate sensitivity and specificity from the verified subsample as the corresponding conditional proportions.\n\nC. Verify all $S=1$ participants and additionally select a Simple Random Sample of $400$ participants from the $S=0$ group for verification. Estimate sensitivity and specificity using Inverse Probability Weighting (IPW), with verification probabilities $\\pi_{1}=1$ for $S=1$ and $\\pi_{0}=400/9{,}400$ for $S=0$, applied to reconstruct unbiased population cell counts across the joint distribution of $(S,D)$.\n\nD. Verify all participants with high predicted risk based on age, clinic blood pressure, and other risk factors, regardless of $S$, until $1{,}000$ verifications are reached. Estimate sensitivity and specificity by computing the corresponding conditional proportions in the verified set without any weighting or adjustment.\n\nFor concreteness, suppose option C is implemented and, among the verified participants, the following counts are observed: among the $S=1$ group, $480$ are $D=1$ and $120$ are $D=0$; among the sampled $S=0$ group, $40$ are $D=1$ and $360$ are $D=0$. Use these numbers only if needed to justify whether option C yields unbiased estimators within the constraint. Select all correct options.",
            "solution": "The user has requested a critical validation of a problem statement from the field of preventive medicine and biostatistics, followed by a complete solution and evaluation of the provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   Total survey participants: $N=10,000$ adults.\n*   Self-reported indicator: $S \\in \\{0,1\\}$.\n*   True disease status (gold standard): $D \\in \\{0,1\\}$.\n*   Maximum number of gold-standard verifications: $1,000$.\n*   Survey counts: $600$ participants report $S=1$ and $9,400$ report $S=0$.\n*   Objective: Estimate population-level sensitivity, $Se = P(S=1|D=1)$, and specificity, $Sp = P(S=0|D=0)$.\n*   Constraints: Avoid verification (work-up) bias; maintain scientific validity.\n*   Assumptions: The gold standard is perfect (no error); any probability sampling is executed as designed.\n*   The options must be evaluated on two criteria: (i) feasibility (at most $1,000$ verifications) and (ii) yielding unbiased estimates of sensitivity and specificity.\n*   For option C, hypothetical observed counts are provided: among $S=1$ verified, $480$ are $D=1$ and $120$ are $D=0$; among a sample of $400$ from $S=0$, $40$ are $D=1$ and $360$ are $D=0$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientific Grounding**: The problem is well-grounded in the principles of epidemiology and biostatistics. It describes a classic \"two-phase\" or \"biphasic\" sampling design, a standard approach for evaluating diagnostic tests when the gold standard is expensive. The concepts of sensitivity, specificity, cross-sectional studies, verification bias, and estimation techniques like Inverse Probability Weighting (IPW) are fundamental to this field.\n2.  **Well-Posedness**: The problem is well-posed. It provides all necessary information (population size, initial screen results, verification budget) and a clear objective (unbiased estimation of $Se$ and $Sp$). The constraints are explicit. A solution can be derived and the options can be unambiguously evaluated.\n3.  **Objectivity**: The language is clear, quantitative, and free of subjective statements. All terms are standard within the relevant scientific discipline.\n\nThe problem statement does not violate any of the invalidity criteria. It is scientifically sound, formally structured, complete, and realistic.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. The solution process will now proceed.\n\n### Derivation of Solution\n\nThe objective is to obtain unbiased estimates of population-level sensitivity, $Se = P(S=1|D=1)$, and specificity, $Sp = P(S=0|D=0)$. To do this, we need unbiased estimates of the four cell counts in the full population's $2 \\times 2$ table of $S$ versus $D$. Let $N_{sd}$ be the total number of individuals in the population of $N=10,000$ with self-report status $s$ and true disease status $d$.\n\nThe true parameters are:\n$$\nSe = \\frac{N_{11}}{N_{11} + N_{01}}\n\\quad \\text{and} \\quad\nSp = \\frac{N_{00}}{N_{00} + N_{10}}\n$$\nWe know the row marginals from the survey: $N_1 = N_{11} + N_{10} = 600$ and $N_0 = N_{01} + N_{00} = 9,400$. The challenge is to estimate the unknown counts $N_{11}, N_{10}, N_{01}, N_{00}$ using at most $1,000$ verifications, in a way that avoids bias. Verification bias occurs when the probability of being selected for gold-standard verification depends on factors that are also associated with the true disease status, and this selection mechanism is not properly accounted for in the analysis.\n\n### Option-by-Option Analysis\n\n**Option A: Verify all $S=1$ participants and do not verify any $S=0$ participants. Estimate sensitivity as the proportion of verified $S=1$ participants who are truly diseased, and estimate specificity by treating unverified $S=0$ participants as truly non-diseased.**\n\n*   **Feasibility**: This strategy requires verifying all $600$ participants who reported $S=1$. Since $600 \\le 1,000$, this is feasible.\n*   **Unbiasedness**: This strategy is fundamentally flawed and will produce biased estimates.\n    1.  The estimate for sensitivity is described as \"the proportion of verified $S=1$ participants who are truly diseased.\" This is an estimate of $P(D=1|S=1)$, the Positive Predictive Value (PPV), not sensitivity, $P(S=1|D=1)$. To estimate sensitivity, one would need to know the total number of diseased individuals in the population, $N_D = N_{11} + N_{01}$. This strategy provides no information about $N_{01}$ (diseased individuals who reported $S=0$).\n    2.  The estimate for specificity relies on \"treating unverified $S=0$ participants as truly non-diseased.\" This is an extreme and scientifically invalid assumption. It implies that the self-report has perfect sensitivity (i.e., $N_{01}=0$), which is precisely what we are trying to measure. This assumption forces the estimate of false negatives to be zero, leading to a biased estimate of any parameter that depends on $N_{01}$ or $N_{00}$ (since it assumes $N_{00}=9,400$). This is a textbook example of verification bias.\n*   **Verdict**: **Incorrect**.\n\n**Option B: Select a Simple Random Sample (SRS) of $1,000$ participants from the entire survey, independent of $S$, and verify them with the gold standard. Estimate sensitivity and specificity from the verified subsample as the corresponding conditional proportions.**\n\n*   **Feasibility**: This strategy requires verifying a Simple Random Sample (SRS) of $1,000$ individuals. This is feasible as it meets the budget constraint of $1,000$.\n*   **Unbiasedness**: An SRS from the full population of $N=10,000$ creates a sub-population of $n=1,000$ that is, in expectation, perfectly representative of the full population. The joint distribution of $(S,D)$ in this random subsample is an unbiased estimator for the joint distribution in the full population. Let the counts in the subsample be $n_{sd}$. Then, the estimators for sensitivity and specificity calculated from this subsample,\n    $$\n    \\hat{Se} = \\frac{n_{11}}{n_{11} + n_{01}}\n    \\quad \\text{and} \\quad\n    \\hat{Sp} = \\frac{n_{00}}{n_{00} + n_{10}}\n    $$\n    are consistent (and approximately unbiased) estimators of the true population parameters. The key is that the selection for verification is independent of the test result $S$ and the (unknown) disease status $D$, thereby avoiding verification bias. This is a valid study design.\n*   **Verdict**: **Correct**.\n\n**Option C: Verify all $S=1$ participants and additionally select a Simple Random Sample of $400$ participants from the $S=0$ group for verification. Estimate sensitivity and specificity using Inverse Probability Weighting (IPW), with verification probabilities $\\pi_{1}=1$ for $S=1$ and $\\pi_{0}=400/9{,}400$ for $S=0$, applied to reconstruct unbiased population cell counts across the joint distribution of $(S,D)$.**\n\n*   **Feasibility**: This strategy requires verifying all $600$ people with $S=1$ and a random sample of $400$ people with $S=0$. The total number of verifications is $600 + 400 = 1,000$. This is feasible.\n*   **Unbiasedness**: This is a stratified sampling design, often called a two-phase design, where the strata are defined by the self-report status $S$. This is a valid and often efficient method.\n    *   The probability of verification for a person in the $S=1$ stratum is $\\pi_1 = \\frac{600}{600} = 1$.\n    *   The probability of verification for a person in the $S=0$ stratum is $\\pi_0 = \\frac{400}{9,400}$.\n    *   To obtain unbiased estimates of the population cell counts ($N_{sd}$), we use Inverse Probability Weighting. Each verified individual is weighted by the inverse of their probability of selection.\n    *   Let $n_{11}^v, n_{10}^v$ be the observed diseased/non-diseased counts in the verified $S=1$ group. The weighted estimates of the population counts are $\\hat{N}_{11} = n_{11}^v / \\pi_1 = n_{11}^v$ and $\\hat{N}_{10} = n_{10}^v / \\pi_1 = n_{10}^v$.\n    *   Let $n_{01}^v, n_{00}^v$ be the observed counts in the verified $S=0$ sample. The weighted estimates are $\\hat{N}_{01} = n_{01}^v / \\pi_0 = n_{01}^v \\times \\frac{9,400}{400}$ and $\\hat{N}_{00} = n_{00}^v / \\pi_0 = n_{00}^v \\times \\frac{9,400}{400}$.\n    *   These estimators $\\hat{N}_{sd}$ are unbiased for the true population counts $N_{sd}$.\n    *   Estimators for sensitivity and specificity can then be formed:\n        $$\n        \\hat{Se} = \\frac{\\hat{N}_{11}}{\\hat{N}_{11} + \\hat{N}_{01}}\n        \\quad \\text{and} \\quad\n        \\hat{Sp} = \\frac{\\hat{N}_{00}}{\\hat{N}_{00} + \\hat{N}_{10}}\n        $$\n    *   Because these are functions of unbiased estimators, they are consistent estimators of the true parameters. This is a standard, valid statistical procedure to correct for the planned differential verification rates. The use of the concrete numbers provided in the problem confirms the mechanics: $\\hat{N}_{11}=480$, $\\hat{N}_{10}=120$, $\\hat{N}_{01}=40 \\times (9400/400) = 940$, and $\\hat{N}_{00}=360 \\times (9400/400) = 8460$. These numbers form a consistent basis for estimation.\n*   **Verdict**: **Correct**.\n\n**D. Verify all participants with high predicted risk based on age, clinic blood pressure, and other risk factors, regardless of $S$, until $1,000$ verifications are reached. Estimate sensitivity and specificity by computing the corresponding conditional proportions in the verified set without any weighting or adjustment.**\n\n*   **Feasibility**: Verifying $1,000$ participants is feasible.\n*   **Unbiasedness**: This strategy is invalid and will lead to severe bias.\n    1.  The selection for verification is based on risk factors (e.g., age, blood pressure) which are almost certainly correlated with the true disease status $D$. This is a non-probability, purposive sampling scheme.\n    2.  The verified sample will be heavily enriched with individuals who have the disease, both those who reported $S=1$ and those who reported $S=0$. It is not representative of the full population, nor of the $S=1$ stratum, nor of the $S=0$ stratum.\n    3.  The proposal to estimate $Se$ and $Sp$ by \"computing the corresponding conditional proportions in the verified set without any weighting or adjustment\" is fatally flawed. This is analogous to performing a case-control study and attempting to estimate prevalence from it. The sampling probabilities are unknown and are not accounted for. This method introduces a profound verification bias, and the resulting estimates of sensitivity and specificity would be incorrect and meaningless for the general population.\n*   **Verdict**: **Incorrect**.",
            "answer": "$$\\boxed{BC}$$"
        }
    ]
}