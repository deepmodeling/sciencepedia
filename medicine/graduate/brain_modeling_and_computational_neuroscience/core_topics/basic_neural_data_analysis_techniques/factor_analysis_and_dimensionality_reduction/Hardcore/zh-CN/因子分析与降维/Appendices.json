{
    "hands_on_practices": [
        {
            "introduction": "在分析任何数据集的变异性之前，我们必须首先定义一个基准。主成分分析（PCA）旨在捕捉数据中方差最大的方向，但如果不先移除数据的均值，结果可能会被误导。本练习探讨了为什么均值中心化不仅仅是一个技术步骤，而是确保PCA捕捉神经活动动态变化而非静态基线属性的关键前提。",
            "id": "3979655",
            "problem": "一个神经元群体的记录产生一个数据矩阵 $Y \\in \\mathbb{R}^{T \\times N}$，其中 $T$ 表示时间窗或试验的次数，$N$ 表示神经元的数量。设 $y_t \\in \\mathbb{R}^{N}$ 表示在时间或试验索引 $t$ 处的群体放电率向量，并设跨时间的样本均值为 $\\bar{y} = \\frac{1}{T} \\sum_{t=1}^{T} y_t$。考虑生成结构 $y_t = \\mu + z_t$，其中 $\\mu \\in \\mathbb{R}^{N}$ 是一个基线（强直性）放电率向量，$z_t \\in \\mathbb{R}^{N}$ 是零均值波动，即 $\\frac{1}{T} \\sum_{t=1}^{T} z_t = 0$，其协方差 $\\Sigma_z = \\frac{1}{T} \\sum_{t=1}^{T} z_t z_t^{\\top}$ 是一个半正定矩阵。假设 $\\|\\mu\\|^2$ 相对于 $\\Sigma_z$ 的最大特征值很大，这反映了与调制相比，基线值很大。\n\n主成分分析（PCA）寻找最大化投影方差的方向。标准做法是根据均值中心化的数据计算样本协方差，而另一种（不正确的）做法是直接对未中心化的 $Y$ 应用 PCA。仅使用样本协方差、二阶矩和主成分分析（PCA）的方差最大化特性的定义，推断中心化如何改变优化过程和主方向的几何形状。\n\n在这种情况下，下列哪些陈述是正确的？\n\nA. 对数据进行中心化对于标准主成分分析（PCA）是必要的，因为它移除了均值的秩一贡献，确保主成分捕捉的是 $z_t$ 中的变异性；如果数据未中心化且 $\\|\\mu\\|^2$ 很大，则第一个主成分会与 $\\mu / \\|\\mu\\|$ 紧密对齐，并可能掩盖与任务相关的变异性。\n\nB. 不对数据进行中心化仅通过一个加性常数影响主成分得分，而主成分载荷（方向）保持不变。\n\nC. 直接对未中心化的数据矩阵 $Y$ 执行奇异值分解（SVD），当 $\\|\\mu\\|^2$ 占主导时，会产生一个与均值方向对齐的顶层右奇异向量，并通过 $\\|\\bar{y}\\|^2$ 的贡献夸大了可解释方差；因此，若不进行中心化，PCA 对大的基线值过于敏感。\n\nD. 在具有大强直性基线的神经数据中，省略中心化会产生一个主导分解的伪全局共激活分量，可能导致下游动力学模型推断出集中在基线方向的误导性低维结构；中心化通过移除直流偏移量来缓解此问题。\n\nE. 基于从未中心化观测值计算的样本协方差进行白化会自动移除均值，因此即使基线很大，也无需进行中心化。\n\n选择所有正确的选项。",
            "solution": "### 推导\n\n问题的核心在于比较两种情况下 PCA 所需寻找特征向量的矩阵：有数据中心化和没有数据中心化。\n\n**情况 1：标准 PCA（带中心化）**\n\n标准 PCA 是在均值中心化的数据上进行的。设中心化后的数据点为 $\\tilde{y}_t$。\n$$ \\tilde{y}_t = y_t - \\bar{y} $$\n利用 $\\bar{y} = \\mu$ 和 $y_t = \\mu + z_t$，我们得到：\n$$ \\tilde{y}_t = (\\mu + z_t) - \\mu = z_t $$\n样本协方差矩阵 $S_{centered}$ 是根据这些中心化数据点计算的：\n$$ S_{centered} = \\frac{1}{T} \\sum_{t=1}^{T} \\tilde{y}_t \\tilde{y}_t^{\\top} = \\frac{1}{T} \\sum_{t=1}^{T} z_t z_t^{\\top} = \\Sigma_z $$\n在这种情况下，PCA 找到的是 $\\Sigma_z$ 的特征向量和特征值。主成分（PCs）是波动 $z_t$ 的最大方差方向，这些方向通常包含神经数据中动态的、携带信息的信号。\n\n**情况 2：对未中心化数据进行 PCA**\n\n如果直接对未中心化的数据 $y_t$ 应用 PCA，算法会找到关于原点的二阶矩矩阵的特征向量，我们称之为 $S_{uncentered}$。\n$$ S_{uncentered} = \\frac{1}{T} \\sum_{t=1}^{T} y_t y_t^{\\top} $$\n代入生成模型 $y_t = \\mu + z_t$：\n$$ S_{uncentered} = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu + z_t)(\\mu + z_t)^{\\top} = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu\\mu^{\\top} + \\mu z_t^{\\top} + z_t\\mu^{\\top} + z_t z_t^{\\top}) $$\n分配求和：\n$$ S_{uncentered} = \\frac{1}{T} \\left( T\\mu\\mu^{\\top} + \\mu \\left(\\sum_{t=1}^{T} z_t\\right)^{\\top} + \\left(\\sum_{t=1}^{T} z_t\\right) \\mu^{\\top} + \\sum_{t=1}^{T} z_t z_t^{\\top} \\right) $$\n鉴于 $\\sum_{t=1}^{T} z_t = 0$，中间两项消失。我们剩下：\n$$ S_{uncentered} = \\mu\\mu^{\\top} + \\frac{1}{T} \\sum_{t=1}^{T} z_t z_t^{\\top} = \\mu\\mu^{\\top} + \\Sigma_z $$\n由于 $\\bar{y} = \\mu$，这也可以写成 $S_{uncentered} = \\bar{y}\\bar{y}^{\\top} + \\Sigma_z$。\n\n矩阵 $\\mu\\mu^{\\top}$ 是一个秩一矩阵。其唯一的非零特征值是 $\\|\\mu\\|^2$，对应的特征向量是 $\\mu / \\|\\mu\\|$。问题陈述中提到，$\\|\\mu\\|^2$ 相对于 $\\Sigma_z$ 的最大特征值很大。我们实质上是在波动的协方差矩阵（$\\Sigma_z$）上，添加了一个沿特定方向（$\\mu$）的大的秩一矩阵。\n\n根据矩阵扰动理论，如果我们有一个矩阵 $A = v v^{\\top} + B$，其中 $\\|v\\|^2$ 远大于 $B$ 的谱范数，那么 $A$ 的最大特征值将约等于 $\\|v\\|^2$，其对应的特征向量将接近于 $v / \\|v\\|$。\n在我们的例子中，$v = \\mu$ 且 $B = \\Sigma_z$。因此，$S_{uncentered}$ 的顶层特征向量将与均值向量的方向 $\\mu / \\|\\mu\\|$ 紧密对齐。相应的特征值将约等于 $\\|\\mu\\|^2$。\n\n**推导结论**：\n- **中心化 PCA** 通过找到 $\\Sigma_z$ 的特征向量来识别波动 $z_t$ 的主轴。\n- **未中心化 PCA** 识别原始数据云 $y_t$ 相对于原点的主轴。当数据云的均值 $\\mu$ 远离原点时（即 $\\|\\mu\\|$ 很大），变异的主要轴线就简化为从原点到数据云中心的方向。这意味着第一个 PC 约等于 $\\mu / \\|\\mu\\|$，其相关的方差（特征值）约等于 $\\|\\mu\\|^2$。这个分量掩盖了 $\\Sigma_z$ 中更细微的方差结构。\n\n### 逐项分析\n\n**A. 对数据进行中心化对于标准主成分分析（PCA）是必要的，因为它移除了均值的秩一贡献，确保主成分捕捉的是 $z_t$ 中的变异性；如果数据未中心化且 $\\|\\mu\\|^2$ 很大，则第一个主成分会与 $\\mu / \\|\\mu\\|$ 紧密对齐，并可能掩盖与任务相关的变异性。**\n\n这个陈述准确地总结了我们的推导。对未中心化数据应用 PCA 涉及对角化 $S_{uncentered} = \\mu\\mu^{\\top} + \\Sigma_z$。中心化移除了 $\\mu\\mu^{\\top}$ 这一项，即“均值的秩一贡献”。中心化后，PCA 在 $\\Sigma_z$ 上操作，其分量描述了 $z_t$ 中的变异性。如上所示，如果 $\\|\\mu\\|^2$ 很大，未中心化数据的第一个 PC 会与 $\\mu / \\|\\mu\\|$ 对齐。由于有趣的、与任务相关的变异性通常编码在波动 $z_t$ 中，让主导 PC 仅仅反映静态基线 $\\mu$ 会有效地掩盖这种变异性。\n\n**结论：正确。**\n\n**B. 不对数据进行中心化仅通过一个加性常数影响主成分得分，而主成分载荷（方向）保持不变。**\n\n这是不正确的。主成分载荷（方向）是特征向量。对于中心化数据，它们是 $\\Sigma_z$ 的特征向量。对于未中心化数据，它们是 $\\mu\\mu^{\\top} + \\Sigma_z$ 的特征向量。这是不同的矩阵，通常会有不同的特征向量。我们的分析显示第一个特征向量发生了显著变化。由于载荷（方向）改变，得分（在这些方向上的投影）也会以一种不是简单的加性常数的方式改变。\n\n**结论：不正确。**\n\n**C. 直接对未中心化的数据矩阵 $Y$ 执行奇异值分解（SVD），当 $\\|\\mu\\|^2$ 占主导时，会产生一个与均值方向对齐的顶层右奇异向量，并通过 $\\|\\bar{y}\\|^2$ 的贡献夸大了可解释方差；因此，若不进行中心化，PCA 对大的基线值过于敏感。**\n\n设数据矩阵 $Y$ 的 SVD 为 $Y = U \\Sigma_{SVD} V^{\\top}$，其中 $V$ 包含右奇异向量。对未中心化数据进行 PCA 就是找到二阶矩矩阵 $S_{uncentered} = \\frac{1}{T} Y^{\\top}Y$ 的特征向量。\n我们来分析 $Y^{\\top}Y$：\n$$ Y^{\\top}Y = (U \\Sigma_{SVD} V^{\\top})^{\\top} (U \\Sigma_{SVD} V^{\\top}) = V \\Sigma_{SVD}^{\\top} U^{\\top} U \\Sigma_{SVD} V^{\\top} = V \\Sigma_{SVD}^{2} V^{\\top} $$\n这是 $Y^{\\top}Y$ 的特征分解。$Y^{\\top}Y$ 的特征向量（也就是 $S_{uncentered}$ 的特征向量）是 $Y$ 的右奇异向量（$V$ 的列）。因此，对未中心化数据进行 PCA 等价于找到数据矩阵 $Y$ 的右奇异向量。\n我们的推导表明，未中心化数据的顶层 PC 与 $\\mu / \\|\\mu\\|$ 对齐，也就是 $\\bar{y} / \\|\\bar{y}\\|$。因此，$Y$ 的顶层右奇异向量与均值方向对齐。与此分量相关的“方差”是 $S_{uncentered}$ 的顶层特征值，约等于 $\\|\\mu\\|^2 = \\|\\bar{y}\\|^2$。这个大值夸大了总“可解释方差”，使得分析被基线所主导。该陈述完全正确。\n\n**结论：正确。**\n\n**D. 在具有大强直性基线的神经数据中，省略中心化会产生一个主导分解的伪全局共激活分量，可能导致下游动力学模型推断出集中在基线方向的误导性低维结构；中心化通过移除直流偏移量来缓解此问题。**\n\n这个陈述在神经科学的背景下，对数学结果给出了一个正确且有见地的解释。\n- “伪全局共激活分量”：第一个 PC 方向是 $\\mu$，即基线放电率向量。如果这些放电率都是正的，这个方向对应于所有神经元同步增加或减少其活动。它是“伪”的，因为它反映的是静态均值，而不是相关波动的模式。\n- “主导分解”：其相关特征值约为 $\\|\\mu\\|^2$，根据假设，这个值很大。\n- “误导性低维结构”：如果一个下游模型（例如，用于系统辨识）使用来自未中心化数据的顶层 PCs，其状态空间将被均值向量所主导。该模型将捕捉到投影到这个静态方向上的动态，完全错误地表征了发生在波动子空间中的真实动态。\n- “中心化通过移除直流偏移量来缓解…”：中心化减去 $\\mu$，即信号的 DC（直流）或静态分量，从而解决了这个问题。\n\n**结论：正确。**\n\n**E. 基于从未中心化观测值计算的样本协方差进行白化会自动移除均值，因此即使基线很大，也无需进行中心化。**\n\n白化是一个线性变换 $W$，应用于数据向量 $y_t$ 以产生新向量 $y'_t = W y_t$，使得 $y'_t$ 的协方差是单位矩阵。白化矩阵 $W$ 是从原始数据的协方差矩阵推导出来的。在这种情况下，它将基于 $S_{uncentered} = \\mu\\mu^{\\top} + \\Sigma_z$。具体来说，$W$ 可以选择为 $S_{uncentered}^{-1/2}$。\n白化后数据的均值为：\n$$ \\mathbb{E}[y'_t] = \\mathbb{E}[W y_t] = W \\mathbb{E}[y_t] = W \\mu $$\n要移除均值，我们需要 $W\\mu = 0$。然而，$W$ 是一个可逆矩阵（因为对于任何非平凡的 $\\Sigma_z$，$S_{uncentered}$ 都是正定的），而 $\\mu$ 是一个非零向量。因此，$W\\mu$ 不可能是零向量。白化使变量去相关并进行缩放；它不移除它们的均值。中心化是一个独立的、必需的步骤。\n\n**结论：不正确。**",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "中心化数据后，我们面临下一个关键选择：是否应该对每个神经元的贡献同等看待？本练习深入探讨了数据标准化（即对相关矩阵而非协方差矩阵进行PCA）的含义和后果。当处理具有不同放电率和方差的神经元时，这是一个至关重要的考量，因为它决定了我们的分析是侧重于绝对的协同变化还是相对的协同模式。",
            "id": "3979654",
            "problem": "在一次伸手任务中，你记录了一组$N$个皮层神经元在$T$个时间窗内的脉冲发放计数，形成一个数据矩阵$X \\in \\mathbb{R}^{N \\times T}$，其中$X_{i t}$是神经元$i$在时间窗$t$内的脉冲发放计数。你通过减去每个神经元在时间上的均值来将其活动中心化，使得每一行的均值为$0$。你考虑主成分分析（PCA; Principal Component Analysis）的两种方案：对样本协方差矩阵执行PCA，或对样本相关矩阵执行PCA。你还考虑是否在PCA之前对每个神经元进行$z$-score标准化。\n\n假设以下基本事实作为出发点：\n\n- 对于两个中心化的变量$x$和$y$，在$T$个样本上测得的样本协方差为 $\\operatorname{Cov}(x,y) = \\frac{1}{T} \\sum_{t=1}^{T} x_t y_t$。样本协方差矩阵为 $S = \\frac{1}{T} X X^{\\top}$。\n- 一个中心化变量$x$的样本方差为 $\\operatorname{Var}(x) = \\operatorname{Cov}(x,x)$。样本标准差为 $\\sigma = \\sqrt{\\operatorname{Var}(x)}$。\n- 两个中心化变量$x$和$y$的样本相关性为 $\\operatorname{Corr}(x,y) = \\operatorname{Cov}(x,y) / (\\sigma_x \\sigma_y)$。样本相关矩阵为 $R = D^{-1} S D^{-1}$，其中 $D = \\operatorname{diag}(\\sigma_1,\\dots,\\sigma_N)$。\n- PCA寻找方向$v \\in \\mathbb{R}^{N}$，该方向在约束$\\lVert v \\rVert_2 = 1$下最大化投影方差$v^{\\top} S v$。等价地，主成分是执行PCA所用对称矩阵的特征向量。\n\n考虑在许多脉冲计数数据集中，神经元$i$的变异性与其平均发放率大致成比例关系，即在类泊松统计下，$\\operatorname{Var}(X_{i\\cdot}) \\approx \\alpha_i \\, \\mathbb{E}[X_{i\\cdot}]$，且$\\alpha_i$接近$1$。因此，即使在相似条件下对时间窗进行均值中心化之后，平均发放率较高的神经元也往往具有较高的方差。你的目标是揭示低维度的共享群体结构（例如，神经元之间共有的潜在动态），而不是仅仅因为某些神经元的绝对发放率较高就优待它们。\n\n在这种情况下，以下哪个陈述是正确的？选择所有适用项。\n\n- A. 对相关矩阵执行PCA与对$z$-score标准化后的数据$Y = D^{-1} X$（其中$D$包含每个神经元的标准差）的协方差矩阵执行PCA完全等价，并且当神经元因类泊松缩放而导致发放率和方差差异很大时，这种方法是合适的，因为它强调与绝对发放率尺度无关的共享协变结构。\n- B. 对协方差矩阵执行PCA相对于每个神经元的重新缩放是尺度不变的；因此，即使在发放率高度异构的情况下，PCA之前对神经元进行$z$-score标准化也是不必要的。\n- C. 如果每个神经元方差的差异本身是有意义的目标信号（例如，一些神经元受到任务变量的强增益调制，而另一些则受到弱调制），那么使用未经$z$-score标准化的协方差矩阵可以保留此信息，而$z$-score标准化会通过将所有方差归一化为$1$来抑制它。\n- D. 在具有近似泊松统计的脉冲计数数据中，均值较高的神经元表现出较低的方差；因此，$z$-score标准化会错误地增加低发放率神经元的权重，应该避免使用。\n- E. 无论神经元尺度的异质性如何，对相关矩阵进行PCA产生的主成分总是与对协方差矩阵进行PCA得到的主成分在旋转变换内是相同的。",
            "solution": "### 解题推导\n问题的核心在于对样本协方差矩阵 $S$ 执行PCA与对样本相关矩阵 $R$ 执行PCA之间的区别。\n\n对协方差矩阵 $S$ 进行PCA是寻找 $S$ 的特征向量。目标是找到正交方向 $v$，以最大化投影方差 $v^\\top S v$。由于 $S$ 的对角元素是单个神经元的方差，$S_{ii} = \\sigma_i^2$，方差较大的神经元将对总方差贡献更大，因此往往在具有最大特征值（方差）的主成分中占主导地位。\n\n对相关矩阵 $R$ 进行PCA等同于在对每个变量（神经元）进行标准化后进行PCA。为了证明这一点，考虑通过对 $X$ 中的数据进行 $z$-score 标准化来创建一个新的数据矩阵 $Y$。回想一下，$X$ 已经是均值中心化的。一个完整的 $z$-score 变换将是 $Y_{it} = (X_{it} - \\mu_i) / \\sigma_i$。由于对于所有 $i$，$\\mu_i = 0$，这简化为将每个神经元的活动除以其标准差。以矩阵形式表示，即为 $Y = D^{-1} X$。\n\n这个新数据矩阵 $Y$ 的协方差矩阵是：\n$$ S_Y = \\frac{1}{T} Y Y^{\\top} = \\frac{1}{T} (D^{-1} X) (D^{-1} X)^{\\top} = \\frac{1}{T} D^{-1} X X^{\\top} (D^{-1})^{\\top} $$\n由于 $D$ 是一个对角矩阵，$D^{-1}$ 也是对角的，因此 $(D^{-1})^{\\top} = D^{-1}$。\n$$ S_Y = D^{-1} \\left( \\frac{1}{T} X X^{\\top} \\right) D^{-1} = D^{-1} S D^{-1} $$\n这正是问题陈述中给出的样本相关矩阵 $R$ 的定义。因此，对相关矩阵 $R$ 进行PCA在数学上与对 $z$-score 标准化数据的协方差矩阵进行PCA是相同的。\n\n当我们进行 $z$-score 标准化时，每个神经元的活动被重新缩放，使其样本方差为 $1$。这使得每个神经元在分析中具有相同的权重，防止高发放率（因此根据问题背景，方差也高）的神经元在分析中占主导地位。这个过程将焦点从依赖于尺度的协方差转移到尺度不变的相关性上。\n\n使用 $S$ 还是 $R$ 的选择取决于科学问题：\n- 如果神经元间方差的差异被认为是有意义的（例如，代表差异性调制或“增益”），那么使用 $S$ 是合适的，因为它保留了这一信息。\n- 如果方差的差异被认为是与问题核心无关的潜在属性的产物（例如，平均发放率的差异），并且目标是找到与单个神经元活动水平无关的协调模式，那么使用 $R$ 是合适的。问题陈述的目标是“揭示低维共享群体结构...而不是仅仅因为某些神经元的绝对发放率较大就优待它们”，这强烈支持使用相关矩阵。\n\n### 逐项分析\n\n- **A. 对相关矩阵执行PCA与对$z$-score标准化后的数据$Y = D^{-1} X$（其中$D$包含每个神经元的标准差）的协方差矩阵执行PCA完全等价，并且当神经元因类泊松缩放而导致发放率和方差差异很大时，这种方法是合适的，因为它强调与绝对发放率尺度无关的共享协变结构。**\n  - 如上推导，对相关矩阵 $R$ 执行PCA确实与对 $z$-score 标准化后的数据 $Y = D^{-1}X$ 的协方差矩阵执行PCA完全等价。陈述的第一部分是正确的。\n  - 问题陈述指出，均值较高的神经元具有较高的方差，且目标是找到共享结构而不优待具有高绝对发放率的神经元。使用相关矩阵将每个神经元的方差归一化为 $1$，有效消除了绝对发放率尺度的影响，并将分析重点放在相关结构上。因此，在此背景下其适用性的理由也是正确的。\n  - **结论：正确**\n\n- **B. 对协方差矩阵执行PCA相对于每个神经元的重新缩放是尺度不变的；因此，即使在发放率高度异构的情况下，PCA之前对神经元进行$z$-score标准化也是不必要的。**\n  - 对协方差矩阵进行PCA根本上是尺度依赖的，而不是尺度不变的。如果我们将神经元 $i$ 的活动重新缩放一个因子 $c$（即 $X'_{i\\cdot} = c X_{i\\cdot}$），其方差将被缩放 $c^2$ 倍，其与所有其他神经元 $j$ 的协方差将被缩放 $c$ 倍。这会非平凡地改变协方差矩阵 $S$，并且通常会改变其特征向量（主成分）。这个陈述的前提是错误的。因此，关于$z$-score标准化是不必要的结论在所述背景下也是错误的。\n  - **结论：不正确**\n\n- **C. 如果每个神经元方差的差异本身是有意义的目标信号（例如，一些神经元受到任务变量的强增益调制，而另一些则受到弱调制），那么使用未经$z$-score标准化的协方差矩阵可以保留此信息，而$z$-score标准化会通过将所有方差归一化为$1$来抑制它。**\n  - 直接使用协方差矩阵 $S$ 会将每个神经元的个体方差纳入PCA中。如果这些方差反映了像差异性增益调制这样的有意义的生物信号，那么基于协方差的PCA将产生反映此信号的成分。\n  - 根据定义，$Z$-score标准化强制每个神经元的方差为 $1$。这明确地移除或“抑制”了关于神经群体中差异性方差的信息。\n  - 该陈述准确地描述了这种权衡以及在何种情况下人们可能更倾向于基于协方- 差的PCA。\n  - **结论：正确**\n\n- **D. 在具有近似泊松统计的脉冲计数数据中，均值较高的神经元表现出较低的方差；因此，$z$-score标准化会错误地增加低发放率神经元的权重，应该避免使用。**\n  - 最初的断言在事实上是错误的。对于泊松过程，方差等于均值。问题陈述本身给出了 $\\operatorname{Var}(X_{i\\cdot}) \\approx \\alpha_i \\, \\mathbb{E}[X_{i\\cdot}]$，明确指出方差*随*均值缩放。因此，均值较高的神经元表现出较高而非较低的方差。该陈述的前提是错误的。\n  - **结论：不正确**\n\n- **E. 无论神经元尺度的异质性如何，对相关矩阵进行PCA产生的主成分总是与对协方差矩阵进行PCA得到的主成分在旋转变换内是相同的。**\n  - 来自协方差矩阵的主成分是 $S$ 的特征向量。来自相关矩阵的主成分是 $R = D^{-1} S D^{-1}$ 的特征向量。这两组特征向量仅在 $D$ 是单位矩阵的倍数（$D=cI$）的特殊情况下才相同，这意味着所有神经元具有相同的标准差。在方差异构的一般情况下， $S$ 和 $D^{-1} S D^{-1}$ 的特征向量是不同的，也并非通过简单的旋转相关联。旋转是一种正交变换，它会以某种方式保留数据点之间的几何关系，从而仅仅改变基底。从 $S$到$R$的变换是一种非均匀的重新缩放，这从根本上改变了这些几何关系。\n  - **结论：不正确**",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "成功执行PCA后，核心问题在于确定保留多少个主成分才能有效捕捉数据的内在结构。本练习审视了常用的“累计方差解释百分比”启发式方法，揭示了其在区分信号与噪声方面的局限性。通过这个练习，我们将探讨更具统计学原则的方法，以更准确地估计神经数据的真实潜在维度。",
            "id": "3979605",
            "problem": "考虑一个由零均值数据矩阵表示的群体神经记录，该矩阵包含 $p$ 个同步记录的特征（例如，神经元或局部场电位通道）。令样本协方差矩阵为 $S \\in \\mathbb{R}^{p \\times p}$，其特征值按 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p \\ge 0$ 的顺序排列。主成分分析 (Principal Component Analysis, PCA) 找到一个标准正交基，该基可将 $S$ 对角化，并按方差递减的顺序对各成分进行排序。定义前 $k$ 个主成分所解释的累计方差百分比如下：\n$$\nE_k \\triangleq \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}.\n$$\n在实践中，这个量被用来决定在构建神经群体活动的低维潜变量模型时要保留多少个成分 $k$。\n\n以下关于 $E_k$ 和 $k$ 的选择的陈述中，哪些是正确的？\n\nA. 当对协方差矩阵 $S$ 执行 PCA 时，函数 $E_k$ 关于 $k$ 是非递减的，并且满足 $E_p = 1$。\n\nB. 如果在对协方差矩阵计算 PCA 之前，每个特征都被一个任意的正常数（不同特征的常数不必相同）重缩放，那么对于所有的 $k$，$E_k$ 都保持不变。\n\nC. 对于一个固定的阈值 $\\tau \\in (0,1)$，选择满足 $E_k \\ge \\tau$ 的最小 $k$ 值，可以保证在附加的独立同分布高斯噪声下，样本外平方重构误差的最小化。\n\nD. 在一个生成模型下，观测到的协方差分解为 $S = S_{\\text{signal}} + \\sigma^2 I$，其中包含方差为 $\\sigma^2$ 的各向同性噪声和一个真实的 $d$ 维信号子空间 ($d \\ll p$)。对于 $k \\ge d$，有 $E_{k+1} - E_k \\approx \\sigma^2 / \\sum_{i=1}^p \\lambda_i$。并且当 $p$ 很大时，大量噪声的累积贡献可能导致仅使用 $E_k$ 会高估 $k$。\n\nE. 通过交叉验证的预测风险或通过将样本特征值与纯噪声数据的特征值进行比较的平行分析来选择 $k$，为 $E_k$ 阈值法提供了一种统计上更具原则性的替代方法，从而减轻了仅凭 $E_k$ 无法检测到的过拟合问题。\n\n选择所有适用项。",
            "solution": "### 对有效问题的分析\n\n我们将逐一分析每个陈述。数据中的总方差由协方差矩阵的迹给出，它也等于其特征值之和：$\\text{Tr}(S) = \\sum_{i=1}^p \\lambda_i$。样本协方差矩阵 $S$ 的一个关键性质是它是半正定的，这意味着它的特征值 $\\lambda_i$ 都是非负的（对于所有 $i$，$\\lambda_i \\ge 0$）。我们假设总方差不为零，即 $\\sum_{i=1}^p \\lambda_i > 0$。\n\n#### 选项 A\n**陈述：** 当对协方差矩阵 $S$ 执行 PCA 时，函数 $E_k$ 关于 $k$ 是非递减的，并且满足 $E_p = 1$。\n\n**分析：**\n函数 $E_k$ 定义为 $E_k = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^p \\lambda_j}$。\n让我们考察从 $k$ 到 $k+1$ 的变化，对于 $k  p$：\n$$\nE_{k+1} - E_k = \\frac{\\sum_{i=1}^{k+1} \\lambda_i}{\\sum_{j=1}^p \\lambda_j} - \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^p \\lambda_j} = \\frac{(\\sum_{i=1}^k \\lambda_i) + \\lambda_{k+1} - \\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^p \\lambda_j} = \\frac{\\lambda_{k+1}}{\\sum_{j=1}^p \\lambda_j}.\n$$\n由于 $S$ 是一个样本协方差矩阵，它是半正定的，其特征值是非负的：$\\lambda_{k+1} \\ge 0$。分母代表总方差，是正的（假设数据不是常数）。因此，$E_{k+1} - E_k \\ge 0$，这意味着 $E_k$ 是 $k$ 的一个非递减函数。\n\n现在，让我们在 $k=p$ 处计算 $E_k$：\n$$\nE_p = \\frac{\\sum_{i=1}^p \\lambda_i}{\\sum_{i=1}^p \\lambda_i} = 1.\n$$\n只要总方差不为零，这就成立。\n该陈述的两个部分都是正确的。\n\n**结论：** **正确**。\n\n#### 选项 B\n**陈述：** 如果在对协方差矩阵计算 PCA 之前，每个特征都被一个任意的正常数（不同特征的常数不必相同）重缩放，那么对于所有的 $k$，$E_k$ 都保持不变。\n\n**分析：**\n该陈述声称，对于量 $E_k$ 而言，对协方差矩阵进行 PCA 是尺度不变的。设原始的零均值数据矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中 $n$ 是样本数量。原始样本协方差矩阵为 $S \\propto X^T X$。\n将每个特征 $j$ 按一个正常数 $c_j$ 进行重缩放，等价于将数据矩阵变换为 $X' = XC$，其中 $C$ 是一个对角矩阵，其对角元素为 $C_{jj} = c_j$。\n新的协方差矩阵 $S'$ 与旧的 $S$ 之间的关系为：\n$$\nS' \\propto (XC)^T(XC) = C^T X^T X C = C S C,\n$$\n因为 $C$ 是对角矩阵，所以 $C^T = C$。一般来说，$S' = CSC$ 的特征值并不仅仅是 $S$ 的特征值的缩放版本，其特征向量也不同。因此，各成分解释的方差比例 $E_k$ 将会改变。\n让我们构造一个简单的反例。设 $p=2$，原始协方差矩阵为单位矩阵：\n$$\nS = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}.\n$$\n特征值为 $\\lambda_1=1$ 和 $\\lambda_2=1$。总方差为 $1+1=2$。第一个主成分解释的累计方差为 $E_1 = \\lambda_1 / (\\lambda_1+\\lambda_2) = 1/2$。\n现在，我们将第一个特征重缩放 $c_1=2$ 倍，第二个特征重缩放 $c_2=1$ 倍。缩放矩阵为 $C = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}$。新的协方差矩阵为：\n$$\nS' = CSC = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}.\n$$\n新的特征值为 $\\lambda'_1=4$ 和 $\\lambda'_2=1$。新的总方差为 $4+1=5$。第一个主成分的新累计方差为 $E'_1 = \\lambda'_1 / (\\lambda'_1+\\lambda'_2) = 4/5$。\n由于 $E'_1 = 4/5 \\neq E_1 = 1/2$，该陈述是错误的。这种缺乏尺度不变性的特性，是为什么在应用 PCA 之前通常需要对特征进行标准化（重缩放以具有单位方差）的主要原因，这等同于对相关矩阵执行 PCA。\n\n**结论：** **不正确**。\n\n#### 选项 C\n**陈述：** 对于一个固定的阈值 $\\tau \\in (0,1)$，选择满足 $E_k \\ge \\tau$ 的最小 $k$ 值，可以保证在附加的独立同分布高斯噪声下，样本外平方重构误差的最小化。\n\n**分析：**\n$E_k \\ge \\tau$ 这个标准完全基于样本内方差结构。它选择了需要用以解释训练数据中观测到的总方差的某个比例 $\\tau$ 的成分数量 $k$。这里的主要问题在于声称能“保证”样本外误差的最小化。\n1.  **样本内与样本外：** 在训练集（样本内）上的最优性并不意味着在未见过的数据（样本外）上的最优性。这是过拟合的根本问题。通过选择足够大的 $k$ 以捕获高百分比的方差（$\\tau$ 接近 $1$），人们可能会开始对特定于训练样本的噪声进行建模。这种噪声模型无法很好地泛化，并可能增加在新的、独立的测试集上的重构误差。\n2.  **没有保证：** 在统计学和机器学习中，像解释方差百分比阈值法这样的启发式方法，并不能为样本外性能提供理论保证。用于泛化的最优 $k$ 取决于真实的底层信号结构、噪声水平和数据量，而这些都不能被简单的 $E_k$ 启发式方法完全捕捉。“保证”这个词使这个陈述绝对是错误的。\n\n**结论：** **不正确**。\n\n#### 选项 D\n**陈述：** 在一个生成模型下，观测到的协方差分解为 $S = S_{\\text{signal}} + \\sigma^2 I$，其中包含方差为 $\\sigma^2$ 的各向同性噪声和一个真实的 $d$ 维信号子空间 ($d \\ll p$)。对于 $k \\ge d$，有 $E_{k+1} - E_k \\approx \\sigma^2 / \\sum_{i=1}^p \\lambda_i$。并且当 $p$ 很大时，大量噪声的累积贡献可能导致仅使用 $E_k$ 会高估 $k$。\n\n**分析：**\n让我们分析这个陈述的两个部分。\n1.  **增量解释方差：** 所指定的生成模型是一个标准的因子分析或概率 PCA 模型。在这个模型中，总体协方差矩阵 $\\Sigma$ 有 $d$ 个对应于信号加噪声的大特征值，以及 $p-d$ 个等于噪声方差 $\\sigma^2$ 的小特征值。对于有限样本协方差矩阵 $S$，其特征值是随机变量，但对于大样本，它们将反映这种结构。具体来说，对于索引 $i > d$，特征值 $\\lambda_i$ 将聚集在 $\\sigma^2$ 附近。\n    从选项 A 的分析中，我们知道 $E_{k+1} - E_k = \\lambda_{k+1} / \\sum_{i=1}^p \\lambda_i$。如果 $k \\ge d$，那么第 $(k+1)$-个成分是一个噪声成分，所以 $\\lambda_{k+1} \\approx \\sigma^2$。因此，在这个模型下，近似式 $E_{k+1} - E_k \\approx \\sigma^2 / \\sum_{i=1}^p \\lambda_i$ 是成立的。\n2.  **维数的高估：** 总方差是 $\\sum_{i=1}^p \\lambda_i = \\text{Tr}(S)$。在总体极限下，$\\text{Tr}(\\Sigma) = \\text{Tr}(\\Sigma_{\\text{signal}}) + \\text{Tr}(\\sigma^2 I) = \\sum_{i=1}^d \\mu_i + p\\sigma^2$，其中 $\\mu_i$ 是信号特征值。由 $p-d$ 个噪声维度解释的总方差是 $(p-d)\\sigma^2$。当 $p$ 很大时，这个“噪声主体”可能对总方差有显著贡献。\n    由真实信号维度解释的方差比例是 $E_d = \\frac{\\sum_{i=1}^d \\lambda_i}{\\sum_{i=1}^p \\lambda_i}$。如果分母中的项 $(p-d)\\sigma^2$ 相对于信号方差 $\\sum_{i=1}^d \\lambda_i - d\\sigma^2$ 较大，那么 $E_d$ 可能会很小。为了达到一个固定的阈值 $\\tau$（例如 $\\tau=0.9$），可能需要包含许多噪声成分，每个成分都贡献少量的方差。这将导致选择的维数 $k > d$，从而高估了真实的信号维数。这是高维 PCA 中的一个众所周知的现象。\n\n**结论：** **正确**。\n\n#### 选项 E\n**陈述：** 通过交叉验证的预测风险或通过将样本特征值与纯噪声数据的特征值进行比较的平行分析来选择 $k$，为 $E_k$ 阈值法提供了一种统计上更具原则性的替代方法，从而减轻了仅凭 $E_k$ 无法检测到的过拟合问题。\n\n**分析：**\n该陈述提出了两种方法，作为 $E_k$ 阈值规则的更优替代方案，并给出了其优越性的原因。\n1.  **替代方法：**\n    - **交叉验证 (CV)：** 该方法为不同的 $k$ 选择估计样本外性能（例如，在留出数据上的重构误差）。选择使该估计误差最小化的 $k$。这直接针对了良好泛化能力的目标。\n    - **平行分析 (PA)：** 该方法通过模拟与原始数据大小相同的随机数据（纯噪声），为特征值的大小创建一个基线。它建议仅保留那些特征值大于来自随机数据的相应特征值（例如，随机特征值分布的第95百分位数）的主成分。这为区分信号和噪声提供了一个统计标准。\n2.  **统计上具原则性：** CV 和 PA 都被认为是“统计上具原则性的”，因为它们基于合理的统计思想：CV 基于估计泛化误差，PA 基于假设检验（将观测到的特征值与零分布进行比较）。\n3.  **减轻过拟合：** 如选项 C 下所讨论的，简单的 $E_k$ 阈值规则容易导致过拟合，因为它纯粹是一个样本内度量。CV 和 PA 都是为解决这个问题而设计的。CV 通过评估在未见过数据上的性能来做到这一点，这会惩罚那些拟合了训练集特有噪声的模型。PA 则通过明确地尝试丢弃表现得像噪声的成分来做到这一点。因此，它们能有效减轻仅凭 $E_k$ 无法检测到的过拟合。\n\n**结论：** **正确**。",
            "answer": "$$\\boxed{ADE}$$"
        }
    ]
}