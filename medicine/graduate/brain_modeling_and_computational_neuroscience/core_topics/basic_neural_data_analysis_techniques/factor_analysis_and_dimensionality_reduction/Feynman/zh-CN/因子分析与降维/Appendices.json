{
    "hands_on_practices": [
        {
            "introduction": "这项练习探讨了一个至关重要的数据预处理步骤：均值中心化。我们将通过这个思想实验来理解，为什么对于通常具有显著基线放电率的神经数据，如果不进行中心化处理，主成分分析（PCA）可能会得出一个具有误导性的“主成分”，该成分仅仅反映了神经元的平均活动水平，而不是我们更感兴趣的神经协同的动态模式。",
            "id": "3979655",
            "problem": "一次神经元群体记录产生一个数据矩阵 $Y \\in \\mathbb{R}^{T \\times N}$，其中 $T$ 表示时间窗或试验的数量，$N$ 表示神经元的数量。设 $y_t \\in \\mathbb{R}^{N}$ 表示在时间或试验索引 $t$ 处的群体放电率向量，并设跨时间的样本均值为 $\\bar{y} = \\frac{1}{T} \\sum_{t=1}^{T} y_t$。考虑生成结构 $y_t = \\mu + z_t$，其中 $\\mu \\in \\mathbb{R}^{N}$ 是一个基线（强直性）放电率向量，$z_t \\in \\mathbb{R}^{N}$ 是零均值波动，即 $\\frac{1}{T} \\sum_{t=1}^{T} z_t = 0$，其协方差为半正定矩阵 $\\Sigma_z = \\frac{1}{T} \\sum_{t=1}^{T} z_t z_t^{\\top}$。假设 $\\|\\mu\\|^2$ 相对于 $\\Sigma_z$ 的最大特征值而言很大，这反映了与调制相比，基线值很大。\n\n主成分分析 (PCA) 寻找使投影方差最大化的方向。标准做法是根据均值中心化的数据计算样本协方差，而另一种（不正确的）做法是直接将 PCA 应用于未中心化的 $Y$。仅使用样本协方差、二阶矩和主成分分析 (PCA) 的方差最大化特性的定义，推理中心化如何改变优化过程和主方向的几何形状。\n\n在此设定下，以下哪些陈述是正确的？\n\nA. 对数据进行中心化是标准主成分分析 (PCA) 的必要步骤，因为它移除了均值的秩一贡献，确保主成分捕捉的是 $z_t$ 中的变异性；如果数据未中心化且 $\\|\\mu\\|^2$ 很大，第一个主成分会与 $\\mu / \\|\\mu\\|$ 紧密对齐，并可能掩盖与任务相关的变异性。\n\nB. 不对数据进行中心化只会影响主成分得分，使其增加一个加性常数，而主成分载荷（方向）保持不变。\n\nC. 直接对未中心化的数据矩阵 $Y$ 执行奇异值分解 (SVD)，当 $\\|\\mu\\|^2$ 占主导地位时，其顶部的右奇异向量会与均值方向对齐，通过 $\\|\\bar{y}\\|^2$ 的贡献夸大了可解释方差；因此，若不进行中心化，PCA 对大的基线值过于敏感。\n\nD. 在具有大强直性基线的神经数据中，省略中心化会产生一个虚假的全局共同激活成分，该成分在分解中占主导地位，可能导致下游动力学模型推断出集中在基线方向上的误导性低维结构；中心化通过移除直流偏移来缓解此问题。\n\nE. 基于从未中心化观测值计算出的样本协方差进行白化会自动移除均值，因此即使基线很大，也使中心化变得不必要。\n\n选择所有正确选项。",
            "solution": "本问题的核心是比较在有无数据中心化两种情况下，主成分分析（PCA）所作用的矩阵。\n\n**情况1：标准PCA（带中心化）**\n标准PCA在均值中心化的数据上执行。中心化后的数据点为 $\\tilde{y}_t = y_t - \\bar{y}$。\n根据问题定义，$y_t = \\mu + z_t$ 且 $\\bar{y} = \\mu$，因此：\n$$ \\tilde{y}_t = (\\mu + z_t) - \\mu = z_t $$\n样本协方差矩阵 $S_{\\text{centered}}$ 是根据这些中心化数据点计算的：\n$$ S_{\\text{centered}} = \\frac{1}{T} \\sum_{t=1}^{T} \\tilde{y}_t \\tilde{y}_t^{\\top} = \\frac{1}{T} \\sum_{t=1}^{T} z_t z_t^{\\top} = \\Sigma_z $$\n在这种情况下，PCA找到的是波动协方差矩阵 $\\Sigma_z$ 的特征向量。主成分捕捉的是数据中动态变化的结构。\n\n**情况2：对未中心化数据进行PCA**\n若直接对未中心化的数据 $y_t$ 应用PCA，算法实际上是找到关于原点的二阶矩矩阵 $S_{\\text{uncentered}}$ 的特征向量。\n$$ S_{\\text{uncentered}} = \\frac{1}{T} \\sum_{t=1}^{T} y_t y_t^{\\top} = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu + z_t)(\\mu + z_t)^{\\top} $$\n展开并利用 $\\frac{1}{T} \\sum z_t = 0$ 的性质，我们得到：\n$$ S_{\\text{uncentered}} = \\mu\\mu^{\\top} + \\Sigma_z $$\n矩阵 $\\mu\\mu^{\\top}$ 是一个秩一矩阵，其唯一的非零特征值为 $\\|\\mu\\|^2$，对应的特征向量为 $\\mu / \\|\\mu\\|$。根据题设，$\\|\\mu\\|^2$ 远大于 $\\Sigma_z$ 的最大特征值。根据矩阵扰动理论，$S_{\\text{uncentered}}$ 的最大特征向量将与均值向量的方向 $\\mu / \\|\\mu\\|$ 紧密对齐，其特征值约等于 $\\|\\mu\\|^2$。\n\n**逐项分析**\n\n*   **A**: 此陈述完全正确。中心化移除了均值的秩一贡献($\\mu\\mu^{\\top}$)，使PCA能专注于波动($z_t$)的变异性。若不中心化且基线很大，第一主成分将由静态均值主导，掩盖了动态信号。\n*   **B**: 这是错误的。如推导所示，主成分载荷（方向）会发生根本性改变，从 $\\Sigma_z$ 的特征向量变为 $\\mu\\mu^{\\top} + \\Sigma_z$ 的特征向量。\n*   **C**: 此陈述正确。对数据矩阵 $Y$ 进行奇异值分解(SVD)时，其右奇异向量是 $Y^{\\top}Y$ 的特征向量。$Y^{\\top}Y$ 与 $S_{\\text{uncentered}}$ 成正比。因此，当基线很大时，顶部的右奇异向量会与均值方向对齐。\n*   **D**: 此陈述正确，并从神经科学角度给出了恰当的解释。未中心化产生的由均值主导的第一主成分是一个“虚假”的共同激活模式，因为它反映的是静态基线而非协同波动，会误导后续的动力学分析。中心化通过移除此“直流偏移”来修正此问题。\n*   **E**: 这是错误的。白化变换（例如乘以 $S_{\\text{uncentered}}^{-1/2}$）会使数据的协方差变为单位矩阵，但不会使其均值变为零。白化后的均值是 $W\\mu$，通常不为零。中心化和白化是两个独立的操作。\n\n因此，正确选项为A、C、D。",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "在数据经过适当的预处理后，下一步是执行主成分分析（PCA）的核心计算。这项练习旨在揭示该算法的内在机制，我们将检验两种等效且基础的方法来提取主成分：对协方差矩阵进行特征值分解，以及对数据矩阵进行奇异值分解（SVD）。理解这些计算过程是将PCA从一个“黑箱”转变为一个透明的分析工具的关键。",
            "id": "3979639",
            "problem": "您同时记录了 $N$ 个皮层神经元，并获得了 $T$ 个连续的群体活动样本，每个时间窗口一个。将时间索引为 $t$ 时的神经群体向量表示为 $x_t \\in \\mathbb{R}^N$，并设 $\\bar{x}$ 为这 $T$ 个样本的经验均值。您打算仅使用这 $T$ 个观测值的有限样本二阶统计量，在没有额外分布假设的情况下，对神经元进行主成分分析 (PCA)。\n\n以下哪个程序正确地指定了神经元间的样本协方差，并由此推导出了主成分方向、按时间索引的成分得分和方差解释率的有效计算方法？选择所有适用项。\n\nA. 定义 $\\bar{x} = \\frac{1}{T}\\sum_{t=1}^T x_t$ 和样本协方差 $S = \\frac{1}{T-1}\\sum_{t=1}^T (x_t - \\bar{x})(x_t - \\bar{x})^\\top$。计算特征分解 $S = U\\Lambda U^\\top$，其中 $U^\\top U = I$，$\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_N)$，并对特征值进行排序，使得 $\\lambda_1 \\ge \\dots \\ge \\lambda_N \\ge 0$。主成分方向是 $U$ 的列向量 $u_i$。在时间 $t$ 时，第 $i$ 个成分的得分是 $z_{i,t} = u_i^\\top (x_t - \\bar{x})$。第 $i$ 个成分的方差解释率是 $\\lambda_i \\big/ \\mathrm{tr}(S) = \\lambda_i \\big/ \\sum_{j=1}^N \\lambda_j$。在存在特征值多重性的情况下，退化特征空间的任何标准正交基都是可接受的。\n\nB. 定义二阶矩矩阵 $S = \\frac{1}{T}\\sum_{t=1}^T x_t x_t^\\top$ (无中心化)。计算特征分解 $S = U\\Lambda U^\\top$ 并将列向量 $u_i$ 作为主方向。使用得分 $z_{i,t} = u_i^\\top x_t$ 和方差解释率 $\\lambda_i \\big/ \\sum_{j=1}^N \\lambda_j$。\n\nC. 首先，根据 $S$ 的对角元素（即方差）计算标准差，并形成相关矩阵 $R = D^{-1/2} S D^{-1/2}$，其中 $D = \\mathrm{diag}(S)$。计算特征分解 $R = Q\\Gamma Q^\\top$ 并将列向量 $q_i$ 作为原始神经元空间中的主方向。使用方差解释率 $\\gamma_i \\big/ \\sum_{j=1}^N \\gamma_j$，其中 $\\gamma_i$ 是 $R$ 的特征值。\n\nD. 构成中心化数据矩阵 $X_c = [x_1 - \\bar{x}, \\dots, x_T - \\bar{x}] \\in \\mathbb{R}^{N \\times T}$。计算奇异值分解 $X_c = U \\Sigma V^\\top$。则 $S = \\frac{1}{T-1} X_c X_c^\\top = U \\left(\\frac{\\Sigma^2}{T-1}\\right) U^\\top$。主成分方向是 $U$ 的列向量，$S$ 的特征值是 $\\lambda_i = \\sigma_i^2/(T-1)$，其中 $\\sigma_i$ 是 $\\Sigma$ 对角线上的奇异值，按时间索引的得分由 $U^\\top X_c = \\Sigma V^\\top$ 的行给出，方差解释率为 $\\lambda_i \\big/ \\sum_{j=1}^N \\lambda_j = \\sigma_i^2 \\big/ \\sum_{j=1}^{\\min(N,T)} \\sigma_j^2$。\n\nE. 构成未中心化的数据矩阵 $X = [x_1, \\dots, x_T] \\in \\mathbb{R}^{N \\times T}$ 并计算 $X^\\top X$。将其特征向量作为 $\\mathbb{R}^N$ 中的主方向，通过将 $x_t$ 投影到这些特征向量上来定义得分，并取与 $X$ 的奇异值 $\\sigma_i$ 成正比的方差解释率，即 $\\sigma_i \\big/ \\sum_j \\sigma_j$。",
            "solution": "主成分分析（PCA）旨在找到数据中方差最大的正交方向。标准的PCA程序包含以下步骤，我们可以通过两种等效的计算方法实现。\n\n1.  **数据中心化**: 首先计算样本均值 $\\bar{x}$，然后将每个数据点减去均值，得到中心化数据 $x_t^c = x_t - \\bar{x}$。PCA分析的是这些中心化数据中的方差。\n2.  **计算主成分**:\n    *   **方法一：协方差矩阵的特征分解**\n        首先计算 $N \\times N$ 的样本协方差矩阵 $S = \\frac{1}{T-1}\\sum_{t=1}^T (x_t - \\bar{x})(x_t - \\bar{x})^\\top$。然后对 $S$ 进行特征分解 $S = U\\Lambda U^\\top$。$U$ 的列向量 $u_i$ 是主成分方向（载荷），对应的特征值 $\\lambda_i$ 是数据在这些方向上的方差。\n    *   **方法二：数据矩阵的奇异值分解（SVD）**\n        构建一个 $N \\times T$ 的中心化数据矩阵 $X_c = [x_1 - \\bar{x}, \\dots, x_T - \\bar{x}]$。对 $X_c$ 进行SVD分解，$X_c = U \\Sigma V^\\top$。$U$ 的左奇异向量（即 $U$ 的列）就是主成分方向。协方差矩阵与SVD的关系为 $S = \\frac{1}{T-1} X_c X_c^\\top = \\frac{1}{T-1} U \\Sigma^2 U^\\top$。\n3.  **计算得分和解释方差**\n    *   **得分**: 第 $i$ 个主成分在时间 $t$ 的得分，是中心化数据点在第 $i$ 个主方向上的投影：$z_{i,t} = u_i^\\top (x_t - \\bar{x})$。整个得分矩阵为 $Z = U^\\top X_c$。\n    *   **解释方差率**: 第 $i$ 个主成分解释的方差占总方差的比例为 $\\frac{\\lambda_i}{\\sum_{j=1}^N \\lambda_j}$。总方差等于协方差矩阵的迹，也等于所有特征值的和。\n\n**逐项分析**\n\n*   **A**: 此选项完美地描述了基于协方差矩阵特征分解的标准PCA流程。所有定义，包括协方差矩阵（使用无偏估计的 $1/(T-1)$ 因子）、主方向、得分和方差解释率，都是正确的。\n*   **B**: 此选项是错误的，因为它没有对数据进行中心化。它计算的是关于原点的二阶矩矩阵，这会导致主成分被数据的均值所污染，而非纯粹反映方差结构。\n*   **C**: 此选项描述的是对*相关矩阵*而非协方差矩阵的PCA。这等同于在分析前对每个神经元的数据进行了标准化（使其方差为1）。虽然这是一种有用的技术，但它与标准的PCA不同，得出的主方向和解释方差也不同。\n*   **D**: 此选项正确地描述了使用SVD计算PCA的等效方法。它正确地关联了中心化数据矩阵 $X_c$ 的SVD与协方差矩阵 $S$ 的特征分解。主方向（$U$ 的列）、特征值与奇异值的关系（$\\lambda_i = \\sigma_i^2/(T-1)$）、得分矩阵（$\\Sigma V^\\top$ 的行）以及方差解释率的表达式都是正确的。\n*   **E**: 此选项包含多个错误。首先，它没有中心化数据。其次，它计算 $X^\\top X$，这是一个 $T \\times T$ 矩阵，其特征向量存在于时间空间 $\\mathbb{R}^T$ 中，而不是神经元空间 $\\mathbb{R}^N$ 中。最后，方差解释率应与奇异值的平方($\\sigma_i^2$)成正比，而不是奇异值本身($\\sigma_i$)。\n\n因此，正确选项为A和D。",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "主成分分析（PCA）的一个核心目标是降低维度，但我们应该保留多少个主成分呢？这项练习聚焦于模型选择这一关键问题，通过检验“累积解释方差”这一常用启发式方法。你将分析该方法的性质，以及更重要的——它的局限性，从而理解为什么我们常常需要更具统计学原则的方法来避免过拟合，并准确识别信号的真实维度。",
            "id": "3979605",
            "problem": "考虑一个群体神经记录，它由一个具有 $p$ 个同时记录的特征（例如，神经元或局部场电位通道）的零均值数据矩阵表示。设样本协方差矩阵为 $S \\in \\mathbb{R}^{p \\times p}$，其特征值排序为 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p \\ge 0$。主成分分析 (PCA; Principal Component Analysis) 寻找一个标准正交基，该基能将 $S$ 对角化，并按方差递减的顺序排列各成分。定义由前 $k$ 个主成分解释的累积方差百分比如下：\n$$\nE_k \\triangleq \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}.\n$$\n在实践中，当构建神经群体活动的低维潜变量模型时，这个量被用来决定保留多少个成分 $k$。\n\n关于 $E_k$ 和 $k$ 的选择，以下哪些陈述是正确的？\n\nA. 当对协方差矩阵 $S$ 执行 PCA 时，函数 $E_k$ 是关于 $k$ 的非递减函数，并且满足 $E_p = 1$。\n\nB. 如果在计算协方差矩阵的 PCA 之前，每个特征都被一个任意正常数（不同特征的常数不必相同）重新缩放，那么对于所有的 $k$，$E_k$ 都保持不变。\n\nC. 对于一个固定的阈值 $\\tau \\in (0,1)$，将 $k$ 选为满足 $E_k \\ge \\tau$ 的最小值，这保证了在附加的独立同分布高斯噪声下，样本外平方重构误差的最小化。\n\nD. 在一个生成模型下，观测到的协方差分解为 $S = S_{\\text{signal}} + \\sigma^2 I$，其中包含方差为 $\\sigma^2$ 的各向同性噪声和一个真实的 $d$ 维信号子空间 ($d \\ll p$)。对于 $k \\ge d$，我们有 $E_{k+1} - E_k \\approx \\sigma^2 / \\sum_{i=1}^p \\lambda_i$，并且当 $p$ 很大时，噪声体量的累积贡献会导致单独使用 $E_k$ 高估 $k$。\n\nE. 通过交叉验证的预测风险，或通过将样本特征值与仅含噪声数据的特征值进行比较的平行分析来选择 $k$，为对 $E_k$ 设置阈值的方法提供了一种统计上有原则的替代方案，从而减轻了仅凭 $E_k$ 无法检测到的过拟合问题。\n\n选择所有适用的选项。",
            "solution": "本问题探讨了使用累积解释方差百分比 $E_k$ 来选择主成分数量 $k$ 的性质和局限性。\n\n**逐项分析**\n\n*   **A**: 该陈述正确。$E_k$ 定义为 $E_k = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^p \\lambda_j}$。因为协方差矩阵是半正定的，其所有特征值 $\\lambda_i$ 均为非负数。所以，当 $k$ 增加时，分子增加或保持不变，因此 $E_k$ 是关于 $k$ 的非递减函数。当 $k=p$ 时，分子和分母相等，所以 $E_p=1$。\n*   **B**: 该陈述错误。PCA 对协方差矩阵的分析不是尺度不变的。如果将不同特征（神经元）的数据乘以不同的常数，协方差矩阵会相应改变，其特征值和特征向量也会改变，从而导致 $E_k$ 的值发生变化。例如，放大一个高方差特征的尺度会使其在总方差中的占比更高，从而影响所有 $E_k$ 的值。\n*   **C**: 该陈述错误。选择 $k$ 以满足某个样本内（in-sample）的解释方差阈值（如 $E_k \\ge 0.9$）是一种启发式方法。它并不能“保证”样本外（out-of-sample）重构误差的最小化。选择过高的阈值可能导致模型纳入了样本特有的噪声，即过拟合，这反而会增加在未见过的新数据上的重构误差。样本外性能的优化需要交叉验证等方法来估计。\n*   **D**: 该陈述正确。在一个信号维度为 $d$ 且混有各向同性噪声 $\\sigma^2 I$ 的模型中，协方差矩阵的特征值谱会有一个“拐点”。前 $d$ 个特征值较大（信号+噪声），而后 $p-d$ 个特征值都约等于 $\\sigma^2$（仅噪声）。\n    -   增量方差 $E_{k+1} - E_k = \\lambda_{k+1}/\\sum\\lambda_i$。对于 $k \\ge d$，$\\lambda_{k+1}$ 是一个噪声特征值，约等于 $\\sigma^2$，因此该近似成立。\n    -   当特征维度 $p$ 很大时，所有噪声维度贡献的总方差 $(p-d)\\sigma^2$ 可能非常可观，甚至会超过信号部分的总方差。这会“压低”信号部分解释的方差百分比 $E_d$。为了达到一个固定的高阈值（如90%），可能需要包含许多噪声维度，从而导致对真实信号维度 $d$ 的高估。\n*   **E**: 该陈述正确。简单的解释方差阈值法由于其纯粹的样本内性质，容易导致过拟合。\n    -   **交叉验证 (Cross-validation)** 通过在留出的数据上评估模型的预测/重构误差来直接估计泛化性能，从而选择最佳的 $k$。\n    -   **平行分析 (Parallel Analysis)** 通过与纯噪声数据的特征值谱进行比较，提供了一个统计标准来区分信号和噪声维度。\n    这两种方法都比简单的阈值法更具统计学原则，能有效缓解过拟合问题。\n\n因此，正确选项为A、D、E。",
            "answer": "$$\\boxed{ADE}$$"
        }
    ]
}