{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in analyzing neural spike counts is to characterize their variability. This practice introduces the Fano factor, $\\mathcal{F}$, a key metric for testing the Poisson model's assumption that the variance equals the mean ($\\mathcal{F}=1$). You will implement a bootstrap-based hypothesis test to determine if observed overdispersion ($\\mathcal{F} \\gt 1$) is statistically significant, a common feature in real neural data that motivates the use of more complex models.",
            "id": "4010449",
            "problem": "You are given spike count data across repeated trials for a single neuron under fixed stimulus conditions, recorded in one fixed-width time bin per trial. The spike counts are modeled as independent and identically distributed random variables. The goal is to compute empirical Fano factors and assess whether observed overdispersion supports a Negative Binomial count model relative to a Poisson count model.\n\nFundamental base and definitions to be used:\n- The Poisson distribution with rate parameter $\\lambda$ has mean $\\mathbb{E}[N] = \\lambda$ and variance $\\mathrm{Var}(N) = \\lambda$. The Fano factor is defined as $\\mathcal{F} = \\frac{\\mathrm{Var}(N)}{\\mathbb{E}[N]}$, and for the Poisson distribution $\\mathcal{F} = 1$.\n- The Negative Binomial distribution (NB) with mean $\\mu$ and positive dispersion parameter $k$ has variance $\\mathrm{Var}(N) = \\mu + \\frac{\\mu^{2}}{k}$. Its Fano factor is $\\mathcal{F} = 1 + \\frac{\\mu}{k}$. Overdispersion relative to Poisson is characterized by $\\mathrm{Var}(N)  \\mathbb{E}[N]$, equivalently $\\mathcal{F}  1$.\n- The unbiased sample mean for $n$ trials is $\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}$.\n- The unbiased sample variance is $s^{2} = \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_{i} - \\hat{\\mu})^{2}$.\n- The empirical Fano factor is $\\hat{\\mathcal{F}} = \\frac{s^{2}}{\\hat{\\mu}}$.\n- A method-of-moments estimator for the Negative Binomial dispersion parameter $k$ is obtained by equating the empirical variance and mean to the NB variance formula, yielding $\\hat{k} = \\frac{\\hat{\\mu}^{2}}{s^{2} - \\hat{\\mu}}$ when $s^{2}  \\hat{\\mu}$; otherwise, $\\hat{k}$ is undefined within the NB overdispersion parameterization.\n\nAssessment procedure:\n- Under the null hypothesis of equidispersion (Poisson), simulate $B$ bootstrap replicates of $n$ counts from $\\mathrm{Poisson}(\\hat{\\mu})$ using the fitted mean $\\hat{\\mu}$, compute bootstrap Fano factors $\\hat{\\mathcal{F}}^{(b)}$ for $b = 1,\\dots,B$, and estimate a one-sided $p$-value for overdispersion as $p = \\frac{1}{B} \\sum_{b=1}^{B} \\mathbf{1}\\left\\{ \\hat{\\mathcal{F}}^{(b)} \\ge \\hat{\\mathcal{F}} \\right\\}$.\n- Declare support for the Negative Binomial model if and only if $p  \\alpha$ and $s^{2}  \\hat{\\mu}$, where $\\alpha$ is the significance level.\n\nAlgorithmic requirements:\n- For each dataset, compute $\\hat{\\mu}$, $s^{2}$, $\\hat{\\mathcal{F}}$, the one-sided bootstrap $p$-value, and the method-of-moments $\\hat{k}$ if $s^{2}  \\hat{\\mu}$, otherwise set $\\hat{k}$ to $\\infty$ to indicate that the NB overdispersion parameterization does not apply.\n- Use $B = 10000$ bootstrap replicates and significance level $\\alpha = 0.05$.\n- All computations are dimensionless counts; no physical units are involved.\n\nTest suite:\n- Dataset $1$ (overdispersed, moderate mean): $[\\, $2$, $12$, $3$, $15$, $7$, $8$, $1$, $18$, $4$, $11$, $5$, $9$, $14$, $6$, $13$, $10$, $17$, $3$, $16$, $7$ \\,]$.\n- Dataset $2$ (approximately equidispersed, Poisson-like): $[\\, $3$, $5$, $4$, $6$, $5$, $5$, $4$, $7$, $6$, $5$, $4$, $5$, $6$, $5$, $5$, $4$, $6$, $5$, $5$, $5$, $6$, $4$, $5$, $6$, $5$, $5$, $4$, $6$, $5$, $5$ \\,]$.\n- Dataset $3$ (underdispersed): $[\\, $7$, $7$, $7$, $8$, $6$, $7$, $7$, $8$, $6$, $7$ \\,]$.\n- Dataset $4$ (small sample, low mean): $[\\, $0$, $0$, $1$, $2$, $0$ \\,]$.\n- Dataset $5$ (overdispersed, large mean): $[\\, $35$, $52$, $47$, $41$, $60$, $39$, $55$, $42$, $68$, $44$, $57$, $36$ \\,]$.\n\nFinal output specification:\n- For each dataset, produce a list $[\\, \\hat{\\mathcal{F}}, \\hat{k}, \\mathrm{support} \\,]$ where $\\hat{\\mathcal{F}}$ and $\\hat{k}$ are floats rounded to $6$ decimal places, and $\\mathrm{support}$ is a boolean indicating whether the Negative Binomial model is supported by the bootstrap test and overdispersion criterion.\n- Your program should produce a single line of output containing the results for all datasets as a comma-separated list enclosed in square brackets, with each per-dataset result itself a bracketed comma-separated list. For example, the output format must be of the form $[ [f_{1}, k_{1}, b_{1}], [f_{2}, k_{2}, b_{2}], \\dots ]$ where each $f_{i}$ and $k_{i}$ are floats with $6$ decimal places and each $b_{i}$ is a boolean.",
            "solution": "The problem has been validated as scientifically grounded, well-posed, and objective. It presents a standard task in computational neuroscience: comparing statistical models for neuronal spike counts. All definitions, data, and procedures are clear and consistent with established statistical and neuroscientific practices.\n\nThe objective is to analyze several datasets of neuronal spike counts. For each dataset, we must determine if a Negative Binomial ($NB$) model is supported over a simpler Poisson model. This assessment is based on two criteria: the presence of overdispersion (variance greater than the mean) and the result of a bootstrap hypothesis test for the Fano factor.\n\nThe Fano factor, defined as $\\mathcal{F} = \\frac{\\mathrm{Var}(N)}{\\mathbb{E}[N]}$, is a key measure of dispersion for count data. A Poisson process, characterized by a single parameter $\\lambda$, has a mean equal to its variance, so its Fano factor is precisely $\\mathcal{F} = 1$. This property is known as equidispersion. In contrast, neural spike trains often exhibit overdispersion, where the variance exceeds the mean, resulting in a Fano factor $\\mathcal{F}  1$. The Negative Binomial distribution, with parameters for mean $\\mu$ and dispersion $k$, can model such data. Its variance is $\\mathrm{Var}(N) = \\mu + \\frac{\\mu^{2}}{k}$, yielding a Fano factor $\\mathcal{F} = 1 + \\frac{\\mu}{k}$, which is greater than $1$ for a positive, finite dispersion parameter $k$.\n\nThe analytical procedure for each dataset involves the following steps:\n\n1.  **Compute Empirical Statistics**: Given a dataset of $n$ spike counts $\\{x_1, x_2, \\dots, x_n\\}$, we first compute the unbiased sample mean $\\hat{\\mu}$ and the unbiased sample variance $s^2$.\n    $$ \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} $$\n    $$ s^{2} = \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_{i} - \\hat{\\mu})^{2} $$\n    The empirical Fano factor, $\\hat{\\mathcal{F}}$, is then the ratio of these two quantities:\n    $$ \\hat{\\mathcal{F}} = \\frac{s^{2}}{\\hat{\\mu}} $$\n    A special case arises if $\\hat{\\mu} = 0$. This implies all $x_i = 0$, leading to $s^2 = 0$. The ratio is indeterminate, but the data perfectly fits a Poisson model with $\\lambda=0$. In this scenario, we can consider the process equidispersed, $\\hat{\\mathcal{F}}=1.0$.\n\n2.  **Estimate Negative Binomial Parameter**: The first condition for supporting the $NB$ model is observed overdispersion, i.e., $s^2  \\hat{\\mu}$. If this condition is met, we use the method of moments to estimate the dispersion parameter $k$. By setting the theoretical mean and variance of the $NB$ distribution equal to their sample estimates ($\\mathbb{E}[N] = \\hat{\\mu}$ and $\\mathrm{Var}(N) = s^2$), we solve for $k$:\n    $$ s^2 = \\hat{\\mu} + \\frac{\\hat{\\mu}^2}{\\hat{k}} \\implies \\hat{k} = \\frac{\\hat{\\mu}^2}{s^2 - \\hat{\\mu}} $$\n    If the data is not overdispersed ($s^2 \\le \\hat{\\mu}$), this estimator is not valid within the overdispersion parameterization of the $NB$ model. In this case, $\\hat{k}$ is set to $\\infty$, signifying a model that approaches the Poisson distribution (as $k \\to \\infty$, $\\frac{\\mu^2}{k} \\to 0$ and $\\mathrm{Var}(N) \\to \\mu$).\n\n3.  **Bootstrap Hypothesis Test**: The second condition is statistical significance. The observation of $\\hat{\\mathcal{F}}  1$ could be due to random sampling variability, even if the underlying process is Poisson. To assess this, we perform a bootstrap hypothesis test.\n    -   **Null Hypothesis ($H_0$)**: The data are generated by a Poisson process. The best estimate for the Poisson rate parameter under $H_0$ is the sample mean, $\\lambda = \\hat{\\mu}$.\n    -   **Procedure**: We simulate $B = 10000$ bootstrap datasets. Each dataset, indexed by $b$, consists of $n$ counts drawn independently from a $\\mathrm{Poisson}(\\hat{\\mu})$ distribution.\n    -   For each bootstrap dataset, we compute its sample mean $\\hat{\\mu}^{(b)}$ and sample variance $(s^2)^{(b)}$, and from them, the bootstrap Fano factor $\\hat{\\mathcal{F}}^{(b)} = (s^2)^{(b)} / \\hat{\\mu}^{(b)}$.\n    -   **P-value Calculation**: The one-sided $p$-value is the proportion of bootstrap Fano factors that are greater than or equal to the empirically observed Fano factor $\\hat{\\mathcal{F}}$.\n    $$ p = \\frac{1}{B} \\sum_{b=1}^{B} \\mathbf{1}\\left\\{ \\hat{\\mathcal{F}}^{(b)} \\ge \\hat{\\mathcal{F}} \\right\\} $$\n    where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. This $p$-value represents the probability of observing a Fano factor as large as or larger than $\\hat{\\mathcal{F}}$ under the null hypothesis that the data is Poisson-distributed.\n\n4.  **Final Decision**: Support for the Negative Binomial model is affirmed if and only if both of the following conditions are met:\n    -   The data is overdispersed: $s^2  \\hat{\\mu}$.\n    -   The bootstrap test result is statistically significant: $p  \\alpha$, where the significance level is given as $\\alpha = 0.05$.\n\nFor each of the five datasets provided, this full procedure is executed to compute a result tuple $[\\hat{\\mathcal{F}}, \\hat{k}, \\mathrm{support}]$. The floating-point values for $\\hat{\\mathcal{F}}$ and $\\hat{k}$ are rounded to $6$ decimal places. If $\\hat{k}$ is not estimable, it is represented as infinity. The $\\mathrm{support}$ result is a boolean value (`True` or `False`).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef analyze_spikes(data, B, alpha):\n    \"\"\"\n    Analyzes spike count data to assess support for a Negative Binomial model.\n\n    Args:\n        data (list): A list of integer spike counts.\n        B (int): The number of bootstrap replicates.\n        alpha (float): The significance level for the hypothesis test.\n\n    Returns:\n        list: A list containing [fano_hat, k_hat, support_nb], where\n              fano_hat is the empirical Fano factor, k_hat is the estimated NB\n              dispersion parameter (or inf), and support_nb is a boolean.\n    \"\"\"\n    counts = np.array(data)\n    n = len(counts)\n\n    # Step 1: Compute empirical statistics\n    if n  2:\n        # Unbiased variance is not defined for n  2.\n        # This case is not present in the test suite but is handled for robustness.\n        mu_hat = np.mean(counts) if n  0 else 0.0\n        s2 = np.nan\n        ff_hat = np.nan\n    else:\n        mu_hat = np.mean(counts)\n        s2 = np.var(counts, ddof=1)\n        # Handle the case where mu_hat is 0 to avoid division by zero.\n        # If mu_hat is 0, all counts are 0, s2 is 0. This is perfectly Poisson.\n        # The Fano factor is indeterminate but best represented as 1 (equidispersion).\n        if mu_hat == 0.0:\n            ff_hat = 1.0\n        else:\n            ff_hat = s2 / mu_hat\n\n    # Step 2: Estimate Negative Binomial Parameter\n    is_overdispersed = s2  mu_hat\n    if is_overdispersed:\n        k_hat = mu_hat**2 / (s2 - mu_hat)\n    else:\n        k_hat = float('inf')\n\n    # Step 3: Bootstrap Hypothesis Test\n    # If mu_hat is 0, the process is perfectly Poisson(0), p-value is 1.\n    if mu_hat == 0.0:\n        p_value = 1.0\n    else:\n        # Use a fixed seed for reproducibility.\n        rng = np.random.default_rng(seed=42)\n        bootstrap_samples = rng.poisson(lam=mu_hat, size=(B, n))\n\n        # Calculate Fano factors for all bootstrap samples\n        # axis=1 performs calculations across each row (each bootstrap replicate)\n        bootstrap_means = np.mean(bootstrap_samples, axis=1)\n        bootstrap_vars = np.var(bootstrap_samples, axis=1, ddof=1)\n\n        # Safely compute Fano factors, avoiding division by zero.\n        # If a bootstrap mean is 0, its variance is also 0. It's an equidispersed sample.\n        # We assign a Fano factor of 1.0 in this case.\n        bootstrap_ffs = np.divide(bootstrap_vars, bootstrap_means, \n                                  out=np.ones_like(bootstrap_means, dtype=float), \n                                  where=bootstrap_means != 0)\n\n        # Calculate the one-sided p-value\n        p_value = np.sum(bootstrap_ffs = ff_hat) / B\n    \n    # Step 4: Final Decision\n    support_nb = is_overdispersed and (p_value  alpha)\n\n    return [ff_hat, k_hat, support_nb]\n\n\ndef solve():\n    \"\"\"\n    Drives the analysis for all test cases and prints the formatted result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [2, 12, 3, 15, 7, 8, 1, 18, 4, 11, 5, 9, 14, 6, 13, 10, 17, 3, 16, 7],\n        [3, 5, 4, 6, 5, 5, 4, 7, 6, 5, 4, 5, 6, 5, 5, 4, 6, 5, 5, 5, 6, 4, 5, 6, 5, 5, 4, 6, 5, 5],\n        [7, 7, 7, 8, 6, 7, 7, 8, 6, 7],\n        [0, 0, 1, 2, 0],\n        [35, 52, 47, 41, 60, 39, 55, 42, 68, 44, 57, 36]\n    ]\n\n    B = 10000\n    alpha = 0.05\n    \n    results = []\n    for data in test_cases:\n        result = analyze_spikes(data, B, alpha)\n        results.append(result)\n\n    # Format the final output string exactly as specified.\n    formatted_results = []\n    for res in results:\n        ff, k, support = res\n        ff_str = f\"{ff:.6f}\"\n        k_str = \"inf\" if k == float('inf') else f\"{k:.6f}\"\n        support_str = str(support).lower()\n        formatted_results.append(f\"[{ff_str},{k_str},{support_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Estimating a neuron's firing rate, $\\lambda$, is a fundamental task, but a point estimate from a single observation is incomplete without quantifying its uncertainty. This exercise guides you through constructing confidence intervals for $\\lambda$, a crucial skill for rigorous data interpretation. You will implement and compare an exact interval, derived from the underlying Poisson statistics, with a more common Gaussian approximation, providing insight into the trade-offs between accuracy and simplicity.",
            "id": "4010384",
            "problem": "You are given independent spike counts from a homogeneous Poisson process, where the spike count $K$ observed in a fixed window of duration $T$ seconds follows a Poisson distribution with mean $\\mu = \\lambda T$, and $\\lambda$ is the unknown rate in spikes per second. Starting from the definition of a Poisson process and the Poisson distribution, and using only well-tested facts, implement two methods to construct a two-sided equal-tailed confidence interval for $\\lambda$ at level $1 - \\alpha$ for a single observed count $k$ in a window of duration $T$:\n- An exact interval obtained by inverting Poisson tail probabilities to find values of $\\mu$ whose cumulative probabilities equal the prescribed tails, then mapping back to $\\lambda$ via $\\lambda = \\mu / T$. You may use the well-established relationship between the Poisson distribution and the Chi-square distribution that arises in such inversions, but do not assume any ready-made shortcut formulas without derivation in your reasoning.\n- A Gaussian (normal) approximation interval justified by the Central Limit Theorem, applying an appropriate plug-in estimate for the variance at moderate count regimes. Enforce the non-negativity constraint for $\\lambda$ in your construction if needed.\n\nYour program must, for each test case, compute:\n- The exact lower bound for $\\lambda$,\n- The exact upper bound for $\\lambda$,\n- The Gaussian lower bound for $\\lambda$,\n- The Gaussian upper bound for $\\lambda$.\n\nAll bounds must be expressed in $s^{-1}$ and rounded to six decimal places. If any Gaussian lower bound is negative, truncate it to $0$ to respect $\\lambda \\ge 0$. If the Gaussian construction degenerates for $k = 0$, reflect that faithfully in your output after applying the non-negativity constraint.\n\nTest Suite (each test case is a triple $(k, T, \\alpha)$):\n- Case 1: $k = 12$, $T = 1.0$, $\\alpha = 0.05$.\n- Case 2 (edge case): $k = 0$, $T = 1.0$, $\\alpha = 0.05$.\n- Case 3: $k = 30$, $T = 0.5$, $\\alpha = 0.05$.\n- Case 4: $k = 5$, $T = 2.0$, $\\alpha = 0.10$.\n- Case 5: $k = 80$, $T = 4.0$, $\\alpha = 0.01$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The list should be flattened in the following order per test case: $[\\text{exact\\_lower}, \\text{exact\\_upper}, \\text{gaussian\\_lower}, \\text{gaussian\\_upper}]$ for Case $1$, followed by the same four numbers for Case $2$, and so on through Case $5$.\n- All numbers must be printed as decimal floats rounded to exactly six places.\nFor example, the output should look like $[\\dots]$ with exactly $20$ numbers for the five test cases in the specified order, each rounded to six decimals, and no additional text. Angles are not involved. No percentages should appear; $\\alpha$ must be treated as a decimal between $0$ and $1$.",
            "solution": "The user has provided a valid problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Process**: Homogeneous Poisson process.\n- **Observation**: A single spike count $K=k$ is observed in a fixed time window of duration $T$ seconds.\n- **Distribution**: $K$ follows a Poisson distribution with mean $\\mu = \\lambda T$, where $\\lambda$ is the unknown rate in spikes per second ($s^{-1}$).\n- **Objective**: Construct a two-sided equal-tailed confidence interval (CI) for $\\lambda$ at a confidence level of $1 - \\alpha$.\n- **Method 1 (Exact)**: Invert Poisson tail probabilities to find bounds for $\\mu$, then map to $\\lambda$. The relationship between the Poisson and Chi-square distributions can be used but must be derived.\n- **Method 2 (Gaussian)**: Use a normal approximation justified by the Central Limit Theorem, with a plug-in estimate for the variance. A non-negativity constraint must be enforced. Degeneracy at $k=0$ should be handled.\n- **Test Suite**: Five test cases are provided as tuples $(k, T, \\alpha)$:\n  1. $(12, 1.0, 0.05)$\n  2. $(0, 1.0, 0.05)$\n  3. $(30, 0.5, 0.05)$\n  4. $(5, 2.0, 0.10)$\n  5. $(80, 4.0, 0.01)$\n- **Output Requirements**: For each test case, compute four values: the exact lower bound, exact upper bound, Gaussian lower bound, and Gaussian upper bound for $\\lambda$. All values must be rounded to six decimal places and presented in a single flat list.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is well-grounded in statistics and computational neuroscience. The Poisson process is a standard model for spike trains, and constructing confidence intervals for its rate parameter is a fundamental task.\n- **Well-Posed**: The problem is clearly defined. It specifies the model, the data, the desired statistical construct (confidence interval), and the exact methods to be used. The inputs are sufficient to produce a unique solution.\n- **Objective**: The problem is stated using precise, unambiguous scientific and mathematical language.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically sound, well-posed, and objective. There are no contradictions, ambiguities, or missing information. A reasoned solution will be provided.\n\n### Principle-Based Solution\n\nThe objective is to find a $1-\\alpha$ confidence interval for the Poisson rate parameter $\\lambda$, based on an observed count $k$ in a time interval $T$. The count $K$ is a random variable from a Poisson distribution with mean $\\mu = \\lambda T$, denoted $K \\sim \\text{Poisson}(\\mu)$. We will first construct a confidence interval $[\\mu_L, \\mu_U]$ for the mean $\\mu$, and then obtain the interval for the rate $\\lambda$ by scaling, i.e., $[\\lambda_L, \\lambda_U] = [\\mu_L/T, \\mu_U/T]$.\n\n#### 1. Exact Confidence Interval (Clopper-Pearson Method for Poisson)\n\nThis method defines the interval by inverting the hypothesis tests for the parameter $\\mu$. For an observed count $k$, the two-sided equal-tailed $(1-\\alpha)$ confidence interval $[\\mu_L, \\mu_U]$ is defined by the following two equations:\n\n1.  The lower bound $\\mu_L$ is the value of $\\mu$ for which the probability of observing a count of $k$ or more is $\\alpha/2$:\n    $$P(K \\ge k | \\mu = \\mu_L) = \\sum_{j=k}^{\\infty} \\frac{e^{-\\mu_L} \\mu_L^j}{j!} = \\frac{\\alpha}{2}$$\n2.  The upper bound $\\mu_U$ is the value of $\\mu$ for which the probability of observing a count of $k$ or less is $\\alpha/2$:\n    $$P(K \\le k | \\mu = \\mu_U) = \\sum_{j=0}^{k} \\frac{e^{-\\mu_U} \\mu_U^j}{j!} = \\frac{\\alpha}{2}$$\n\nTo solve these equations, we use the relationship between the Poisson and Gamma distributions, and subsequently the Chi-square distribution.\n\n**Derivation of the Poisson-Gamma-Chi-square Relationship:**\nConsider a Poisson process with rate $1$. The number of events $K$ in an interval of length $\\mu$ follows $\\text{Poisson}(\\mu)$. The waiting time $W_n$ until the $n$-th event follows a Gamma distribution with shape parameter $n$ and rate parameter $1$, i.e., $W_n \\sim \\text{Gamma}(n, 1)$. The event that there are fewer than $k$ events in the interval $[0, \\mu]$ is equivalent to the event that the $k$-th event occurs after time $\\mu$.\n$$P(K  k | \\mu) = P(W_k  \\mu)$$\nTherefore, the survival function (SF) of the Poisson distribution is related to the cumulative distribution function (CDF) of the Gamma distribution:\n$$P(K \\ge k | \\mu) = 1 - P(K  k | \\mu) = 1 - P(W_k  \\mu) = P(W_k \\le \\mu) = F_{\\text{Gamma}(k,1)}(\\mu)$$\nNow, we relate the Gamma distribution to the Chi-square ($\\chi^2$) distribution. A $\\chi^2$ distribution with $\\nu$ degrees of freedom is a special case of the Gamma distribution: $\\chi^2_{\\nu} \\sim \\text{Gamma}(\\nu/2, 1/2)$. If a random variable $X \\sim \\text{Gamma}(a, b)$, then $2bX \\sim \\chi^2_{2a}$. For our waiting time $W_k \\sim \\text{Gamma}(k, 1)$, we have $a=k$ and $b=1$. Thus, $2W_k \\sim \\chi^2_{2k}$.\nThe CDF relationship is $F_{W_k}(\\mu) = F_{2W_k}(2\\mu) = F_{\\chi^2_{2k}}(2\\mu)$.\nCombining these facts, we have:\n$$P(K \\ge k | \\mu) = F_{\\chi^2_{2k}}(2\\mu)$$\n\n**Calculating the Bounds:**\n\n**Lower Bound ($\\mu_L$):**\nFor an observed count $k  0$, the defining equation is $P(K \\ge k | \\mu_L) = \\alpha/2$. Using our derived identity:\n$$F_{\\chi^2_{2k}}(2\\mu_L) = \\frac{\\alpha}{2}$$\nInverting this gives $2\\mu_L = F^{-1}_{\\chi^2_{2k}}(\\alpha/2)$, where $F^{-1}_{\\chi^2_{\\nu}}(p)$ is the quantile function (inverse CDF) of the $\\chi^2$ distribution with $\\nu$ degrees of freedom, evaluated at probability $p$.\n$$\\mu_L = \\frac{1}{2} F^{-1}_{\\chi^2_{2k}}\\left(\\frac{\\alpha}{2}\\right)$$\nFor the special case $k=0$, $P(K \\ge 0 | \\mu)=1$ for any $\\mu  0$. The defining equation cannot be satisfied unless $\\alpha=2$. By convention, we set $\\mu_L = 0$ when $k=0$, which corresponds to the physical limit of the parameter.\n\n**Upper Bound ($\\mu_U$):**\nThe defining equation is $P(K \\le k | \\mu_U) = \\alpha/2$. This can be rewritten using the survival function:\n$$1 - P(K \\ge k+1 | \\mu_U) = \\frac{\\alpha}{2} \\implies P(K \\ge k+1 | \\mu_U) = 1 - \\frac{\\alpha}{2}$$\nUsing our identity with $k+1$:\n$$F_{\\chi^2_{2(k+1)}}(2\\mu_U) = 1 - \\frac{\\alpha}{2}$$\nInverting this gives $2\\mu_U = F^{-1}_{\\chi^2_{2(k+1)}}(1 - \\alpha/2)$.\n$$\\mu_U = \\frac{1}{2} F^{-1}_{\\chi^2_{2(k+1)}}\\left(1 - \\frac{\\alpha}{2}\\right)$$\n\nFinally, the confidence interval for $\\lambda$ is $[\\lambda_L, \\lambda_U] = [\\mu_L/T, \\mu_U/T]$.\n\n#### 2. Gaussian (Normal) Approximation Interval\n\nFor large values of the mean $\\mu$, the Poisson distribution can be approximated by a normal distribution with the same mean and variance: $\\text{Poisson}(\\mu) \\approx N(\\mu, \\mu)$.\nWe consider the point estimator for the rate, $\\hat{\\lambda} = k/T$.\nThe expectation of this estimator is $E[\\hat{\\lambda}] = E[k/T] = E[K]/T = \\mu/T = (\\lambda T)/T = \\lambda$.\nThe variance is $\\text{Var}(\\hat{\\lambda}) = \\text{Var}(k/T) = \\frac{1}{T^2}\\text{Var}(K) = \\mu/T^2 = (\\lambda T)/T^2 = \\lambda/T$.\nFor large $k$, the sampling distribution of $\\hat{\\lambda}$ is approximately normal: $\\hat{\\lambda} \\stackrel{\\cdot}{\\sim} N(\\lambda, \\lambda/T)$.\nTo construct a confidence interval, we need the standard error of the estimator, $\\text{SE}(\\hat{\\lambda}) = \\sqrt{\\lambda/T}$. Since $\\lambda$ is unknown, we use a \"plug-in\" estimate by substituting $\\lambda$ with $\\hat{\\lambda}$:\n$$\\widehat{\\text{SE}}(\\hat{\\lambda}) = \\sqrt{\\frac{\\hat{\\lambda}}{T}} = \\sqrt{\\frac{k/T}{T}} = \\frac{\\sqrt{k}}{T}$$\nA $(1-\\alpha)$ confidence interval is then given by $\\hat{\\lambda} \\pm z_{1-\\alpha/2} \\widehat{\\text{SE}}(\\hat{\\lambda})$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution $N(0,1)$.\n\nThe bounds are:\n$$\\lambda_{L,G} = \\frac{k}{T} - z_{1-\\alpha/2} \\frac{\\sqrt{k}}{T}$$\n$$\\lambda_{U,G} = \\frac{k}{T} + z_{1-\\alpha/2} \\frac{\\sqrt{k}}{T}$$\n\nAs the rate $\\lambda$ cannot be negative, we enforce this constraint by truncating the lower bound at zero:\n$$\\lambda_{L,G}^{\\text{final}} = \\max\\left(0, \\lambda_{L,G}\\right)$$\n\nIn the special case where $k=0$, the estimator is $\\hat{\\lambda}=0$ and the estimated standard error is $\\widehat{\\text{SE}}(\\hat{\\lambda})=0$. This results in a degenerate interval $[0,0]$. This behavior is a known artifact of this particular normal approximation (the Wald interval) at the boundary of the parameter space.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2, norm\n\ndef solve():\n    \"\"\"\n    Computes exact and Gaussian confidence intervals for the rate parameter\n    of a Poisson process based on observed spike counts.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (k, T, alpha)\n        (12, 1.0, 0.05),\n        (0, 1.0, 0.05),\n        (30, 0.5, 0.05),\n        (5, 2.0, 0.10),\n        (80, 4.0, 0.01),\n    ]\n\n    flat_results = []\n    for k, T, alpha in test_cases:\n        # --- Method 1: Exact Interval (Clopper-Pearson for Poisson) ---\n        # The exact interval for the Poisson mean mu is given by Chi-square quantiles.\n        # We then scale by 1/T to get the interval for the rate lambda.\n\n        # Lower bound\n        if k == 0:\n            # For k=0, the lower bound for the rate is 0.\n            lambda_L_exact = 0.0\n        else:\n            # Degrees of freedom for lower bound calculation\n            df_L = 2 * k\n            # mu_L = 0.5 * chi2.ppf(alpha/2, 2*k)\n            # lambda_L = mu_L / T\n            lambda_L_exact = chi2.ppf(alpha / 2, df_L) / (2 * T)\n        \n        # Upper bound\n        # Degrees of freedom for upper bound calculation\n        df_U = 2 * (k + 1)\n        # mu_U = 0.5 * chi2.ppf(1 - alpha/2, 2*(k+1))\n        # lambda_U = mu_U / T\n        lambda_U_exact = chi2.ppf(1 - alpha / 2, df_U) / (2 * T)\n\n        # --- Method 2: Gaussian (Normal) Approximation Interval ---\n        # This is the standard Wald interval based on the normal approximation\n        # to the Poisson distribution (or to the sampling distribution of the estimator).\n        \n        # Point estimate of the rate\n        lambda_hat = k / T\n        \n        if k == 0:\n            # For k=0, lambda_hat=0 and the estimated standard error is 0.\n            # This leads to a degenerate interval [0, 0].\n            lambda_L_gauss = 0.0\n            lambda_U_gauss = 0.0\n        else:\n            # z-score for the (1-alpha) confidence level\n            z_score = norm.ppf(1 - alpha / 2)\n            # Plug-in estimate of the standard error of lambda_hat\n            se_hat = np.sqrt(k) / T\n            \n            # Lower bound with non-negativity constraint\n            lambda_L_gauss = max(0.0, lambda_hat - z_score * se_hat)\n            \n            # Upper bound\n            lambda_U_gauss = lambda_hat + z_score * se_hat\n        \n        flat_results.extend([lambda_L_exact, lambda_U_exact, lambda_L_gauss, lambda_U_gauss])\n\n    # Format all results to a string with exactly six decimal places\n    # and join them into a single comma-separated list.\n    formatted_results = [f\"{num:.6f}\" for num in flat_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "When spike count data exhibits significant overdispersion, the simple Poisson model is often inadequate, and the Negative Binomial (NB) distribution provides a better alternative. This practice delves into formal model selection by deriving a Likelihood-Ratio Test (LRT) to compare these two competing models. By working through the derivation, you will gain a deep understanding of how to statistically justify the choice of a more complex model to capture the true variability in neural data.",
            "id": "4010365",
            "problem": "A single cortical neuron is recorded across $n = 10$ repeated presentations of the same stimulus, yielding trial-by-trial spike counts $\\{x_{i}\\}_{i=1}^{n} = (0, 1, 0, 2, 3, 4, 1, 2, 5, 0)$. Two generative models are considered for the spike counts: a Poisson model with a common unknown rate $\\,\\mu\\,$ across trials, and a Negative Binomial (NB) model with common unknown mean $\\,\\mu\\,$ and a fixed, known size (dispersion) parameter $\\,k = 3\\,$. The size parameter $\\,k\\,$ was estimated from a large, independent calibration dataset and is treated here as externally supplied and fixed.\n\nStarting from the fundamental definition of the probability mass function for the Poisson and Negative Binomial distributions, derive a Likelihood-Ratio Test (LRT) comparing the Poisson and NB models for these spike counts, under the assumption of independent trials. Specifically:\n\n- Derive the maximum likelihood estimate (MLE) $\\,\\hat{\\mu}\\,$ for each model from first principles.\n- Derive the LRT statistic $\\,\\Lambda = 2\\left(\\ln L_{\\mathrm{NB}}(\\hat{\\mu}, k) - \\ln L_{\\mathrm{Pois}}(\\hat{\\mu})\\right)\\,$ in closed form for general spike counts $\\{x_{i}\\}$ and fixed $k$, expressing it using only the sufficient statistics and elementary functions as needed.\n- Compute $\\,\\Lambda\\,$ for the provided dataset and $\\,k = 3\\,$.\n\nIn your derivation, justify each step by explicit use of definitions and independence assumptions. Conclude by briefly explaining, based on the derived $\\,\\Lambda\\,$ and its functional dependence on $\\,k\\,$, how the dispersion parameter $\\,k\\,$ influences model selection between Poisson and NB for neural spike counts. Round the final numerical value of $\\,\\Lambda\\,$ to four significant figures. No physical units are required for the final numerical value.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It is a standard problem in statistical model comparison within computational neuroscience. All necessary data and definitions are provided. Therefore, I will proceed with a full derivation and solution.\n\nThe problem asks for a likelihood-ratio test to compare two models for neural spike count data $\\{x_i\\}_{i=1}^n$: a Poisson model and a Negative Binomial (NB) model. The trials are assumed to be independent.\n\nFirst, we define the probability mass functions (PMFs) for the two models.\nFor the Poisson distribution with rate (and mean) $\\mu$, the PMF is:\n$$P(X=x | \\mu) = \\frac{\\mu^x \\exp(-\\mu)}{x!}$$\nFor the Negative Binomial distribution, the problem specifies a parameterization with mean $\\mu$ and a fixed size parameter $k$. The standard PMF uses a size parameter $r$ and a success probability $p$. The relationship is $r=k$ and the mean $\\mu = \\frac{r(1-p)}{p} = \\frac{k(1-p)}{p}$. Solving for $p$ yields $p = \\frac{k}{k+\\mu}$, and thus $1-p = \\frac{\\mu}{k+\\mu}$. The PMF in terms of $\\mu$ and $k$ is:\n$$P(X=x | \\mu, k) = \\binom{x+k-1}{x} \\left(\\frac{k}{k+\\mu}\\right)^k \\left(\\frac{\\mu}{k+\\mu}\\right)^x$$\n\n**Step 1: Derive the Maximum Likelihood Estimate (MLE) for $\\mu$ in each model.**\n\n**Poisson Model:**\nGiven $n$ independent observations $\\{x_i\\}_{i=1}^n$, the likelihood function is the product of the individual probabilities:\n$$L_{\\mathrm{Pois}}(\\mu; \\{x_i\\}) = \\prod_{i=1}^{n} P(x_i | \\mu) = \\prod_{i=1}^{n} \\frac{\\mu^{x_i} \\exp(-\\mu)}{x_i!}$$\nIt is more convenient to work with the log-likelihood function, $\\ln L$:\n$$\\ln L_{\\mathrm{Pois}}(\\mu) = \\sum_{i=1}^{n} \\ln\\left(\\frac{\\mu^{x_i} \\exp(-\\mu)}{x_i!}\\right) = \\sum_{i=1}^{n} (x_i \\ln \\mu - \\mu - \\ln(x_i!))$$\n$$\\ln L_{\\mathrm{Pois}}(\\mu) = (\\ln \\mu) \\sum_{i=1}^{n} x_i - n\\mu - \\sum_{i=1}^{n} \\ln(x_i!)$$\nTo find the MLE $\\hat{\\mu}_{\\mathrm{Pois}}$, we differentiate $\\ln L_{\\mathrm{Pois}}(\\mu)$ with respect to $\\mu$ and set the result to zero:\n$$\\frac{d}{d\\mu} \\ln L_{\\mathrm{Pois}}(\\mu) = \\frac{1}{\\mu} \\sum_{i=1}^{n} x_i - n = 0$$\nSolving for $\\mu$ gives the MLE:\n$$\\hat{\\mu}_{\\mathrm{Pois}} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\bar{x}$$\nThe MLE for the Poisson rate is the sample mean of the data.\n\n**Negative Binomial Model:**\nThe log-likelihood function for the NB model with fixed $k$ is:\n$$\\ln L_{\\mathrm{NB}}(\\mu; \\{x_i\\}, k) = \\sum_{i=1}^{n} \\ln\\left( \\binom{x_i+k-1}{x_i} \\left(\\frac{k}{k+\\mu}\\right)^k \\left(\\frac{\\mu}{k+\\mu}\\right)^{x_i} \\right)$$\n$$\\ln L_{\\mathrm{NB}}(\\mu) = \\sum_{i=1}^{n} \\left[ \\ln\\binom{x_i+k-1}{x_i} + k\\ln\\left(\\frac{k}{k+\\mu}\\right) + x_i\\ln\\left(\\frac{\\mu}{k+\\mu}\\right) \\right]$$\n$$\\ln L_{\\mathrm{NB}}(\\mu) = C + \\sum_{i=1}^{n} \\left[ k(\\ln k - \\ln(k+\\mu)) + x_i(\\ln \\mu - \\ln(k+\\mu)) \\right]$$\nwhere $C = \\sum_{i=1}^n \\ln\\binom{x_i+k-1}{x_i}$ is a constant with respect to $\\mu$.\n$$\\ln L_{\\mathrm{NB}}(\\mu) = C + nk\\ln k - nk\\ln(k+\\mu) + (\\ln \\mu)\\sum_{i=1}^{n} x_i - (\\ln(k+\\mu))\\sum_{i=1}^{n} x_i$$\nTo find the MLE $\\hat{\\mu}_{\\mathrm{NB}}$, we differentiate with respect to $\\mu$ and set to zero:\n$$\\frac{d}{d\\mu} \\ln L_{\\mathrm{NB}}(\\mu) = -\\frac{nk}{k+\\mu} + \\frac{1}{\\mu}\\sum_{i=1}^{n} x_i - \\frac{1}{k+\\mu}\\sum_{i=1}^{n} x_i = 0$$\n$$\\frac{\\sum x_i}{\\mu} = \\frac{nk + \\sum x_i}{k+\\mu}$$\n$$(k+\\mu)\\sum x_i = \\mu(nk + \\sum x_i)$$\n$$k\\sum x_i + \\mu\\sum x_i = nk\\mu + \\mu\\sum x_i$$\n$$k\\sum x_i = nk\\mu$$\nSolving for $\\mu$ gives the MLE:\n$$\\hat{\\mu}_{\\mathrm{NB}} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\bar{x}$$\nThus, the MLE for the mean parameter $\\mu$ is the sample mean $\\bar{x}$ for both models. We will denote this common MLE as $\\hat{\\mu} = \\bar{x}$.\n\n**Step 2: Derive the Likelihood-Ratio Test statistic $\\Lambda$.**\nThe LRT statistic is defined as $\\Lambda = 2(\\ln L_{\\mathrm{NB}}(\\hat{\\mu}, k) - \\ln L_{\\mathrm{Pois}}(\\hat{\\mu}))$.\nWe can express this as a sum over the log-ratio of the probabilities for each data point, evaluated at $\\hat{\\mu}=\\bar{x}$:\n$$\\Lambda = 2 \\sum_{i=1}^{n} \\ln\\left(\\frac{P(x_i | \\hat{\\mu}, k)}{P(x_i | \\hat{\\mu})}\\right)$$\nLet's analyze the ratio for a single data point $x_i$:\n$$\\frac{P(x_i | \\bar{x}, k)}{P(x_i | \\bar{x})} = \\frac{\\binom{x_i+k-1}{x_i} (\\frac{k}{k+\\bar{x}})^k (\\frac{\\bar{x}}{k+\\bar{x}})^{x_i}}{\\frac{\\bar{x}^{x_i} \\exp(-\\bar{x})}{x_i!}}$$\nUsing $\\binom{x_i+k-1}{x_i} = \\frac{\\Gamma(x_i+k)}{x_i! \\Gamma(k)}$, where $\\Gamma(z)$ is the Gamma function:\n$$= \\frac{\\Gamma(x_i+k)}{x_i! \\Gamma(k)} \\frac{k^k \\bar{x}^{x_i}}{(k+\\bar{x})^{k+x_i}} \\frac{x_i!}{\\bar{x}^{x_i} \\exp(-\\bar{x})} = \\frac{\\Gamma(x_i+k)}{\\Gamma(k)} \\frac{k^k \\exp(\\bar{x})}{(k+\\bar{x})^{k+x_i}}$$\nTaking the natural logarithm of this ratio:\n$$\\ln\\left(\\frac{P(x_i | \\bar{x}, k)}{P(x_i | \\bar{x})}\\right) = \\ln\\Gamma(x_i+k) - \\ln\\Gamma(k) + k\\ln k + \\bar{x} - (k+x_i)\\ln(k+\\bar{x})$$\nSumming over all $n$ observations:\n$$\\sum_{i=1}^{n} \\ln\\left(\\frac{P(x_i | \\hat{\\mu}, k)}{P(x_i | \\hat{\\mu})}\\right) = \\sum_{i=1}^{n} \\left[ \\ln\\Gamma(x_i+k) - \\ln\\Gamma(k) + k\\ln k + \\bar{x} - (k+x_i)\\ln(k+\\bar{x}) \\right]$$\n$$= \\left(\\sum_{i=1}^{n} \\ln\\Gamma(x_i+k)\\right) - n\\ln\\Gamma(k) + nk\\ln k + n\\bar{x} - \\sum_{i=1}^{n}(k+x_i)\\ln(k+\\bar{x})$$\nThe last term can be expanded:\n$$\\sum_{i=1}^{n}(k+x_i)\\ln(k+\\bar{x}) = (nk + \\sum_{i=1}^{n} x_i)\\ln(k+\\bar{x}) = (nk + n\\bar{x})\\ln(k+\\bar{x})$$\nSubstituting back and multiplying by $2$ gives the final expression for $\\Lambda$:\n$$\\Lambda = 2 \\left[ \\sum_{i=1}^{n} \\ln\\Gamma(x_i+k) - n\\ln\\Gamma(k) + nk\\ln k - (nk+n\\bar{x})\\ln(k+\\bar{x}) + n\\bar{x} \\right]$$\nThis is the closed-form expression for the LRT statistic.\n\n**Step 3: Compute $\\Lambda$ for the given dataset.**\nThe data are $\\{x_i\\} = (0, 1, 0, 2, 3, 4, 1, 2, 5, 0)$.\nThe parameters are $n=10$ and $k=3$.\nFirst, calculate the sample mean $\\bar{x}$:\n$$\\sum_{i=1}^{10} x_i = 0+1+0+2+3+4+1+2+5+0 = 18$$\n$$\\bar{x} = \\frac{18}{10} = 1.8$$\nNow we compute each term in the expression for $\\Lambda$. We use $\\ln\\Gamma(z) = \\ln((z-1)!)$ for integer $z$.\nThe counts of each value are: $f(0)=3$, $f(1)=2$, $f(2)=2$, $f(3)=1$, $f(4)=1$, $f(5)=1$.\n$$\\sum_{i=1}^{10} \\ln\\Gamma(x_i+k) = \\sum_{i=1}^{10} \\ln\\Gamma(x_i+3)$$\n$$= 3\\ln\\Gamma(3) + 2\\ln\\Gamma(4) + 2\\ln\\Gamma(5) + 1\\ln\\Gamma(6) + 1\\ln\\Gamma(7) + 1\\ln\\Gamma(8)$$\n$$= 3\\ln(2!) + 2\\ln(3!) + 2\\ln(4!) + \\ln(5!) + \\ln(6!) + \\ln(7!)$$\n$$= 3\\ln(2) + 2\\ln(6) + 2\\ln(24) + \\ln(120) + \\ln(720) + \\ln(5040)$$\nUsing numerical values:\n$\\sum \\ln\\Gamma(x_i+3) \\approx 3(0.69315) + 2(1.79176) + 2(3.17805) + 4.78749 + 6.57925 + 8.52516 \\approx 31.9110$\nThe other terms are:\n$n\\ln\\Gamma(k) = 10\\ln\\Gamma(3) = 10\\ln(2) \\approx 6.9315$\n$nk\\ln(k) = 10 \\cdot 3 \\ln(3) = 30\\ln(3) \\approx 32.9584$\n$n\\bar{x} = 10 \\cdot 1.8 = 18$\n$(nk+n\\bar{x})\\ln(k+\\bar{x}) = (30+18)\\ln(3+1.8) = 48\\ln(4.8) \\approx 75.2936$\nSubstituting these into the formula for $\\Lambda$:\n$$\\Lambda = 2 [31.9110 - 6.9315 + 32.9584 - 75.2936 + 18]$$\n$$\\Lambda = 2 [82.8694 - 82.2251] = 2 [0.6443] = 1.2886$$\nRounding to four significant figures, $\\Lambda \\approx 1.289$.\n\n**Influence of the dispersion parameter $k$:**\nThe parameter $k$ in the Negative Binomial distribution controls its dispersion. The variance of the NB distribution is $\\mathrm{Var}(X) = \\mu + \\frac{\\mu^2}{k}$.\nThe Poisson distribution is a special case of the NB distribution in the limit as $k \\to \\infty$. In this limit, the variance of the NB becomes $\\mu$, which is the variance of the Poisson distribution. As a result, for very large $k$, the NB model becomes indistinguishable from the Poisson model. Consequently, the likelihoods become equal ($L_{\\mathrm{NB}} \\to L_{\\mathrm{Pois}}$), and the LRT statistic $\\Lambda \\to 0$.\nConversely, smaller values of $k$ correspond to greater overdispersion (i.e., variance is much larger than the mean). For a dataset that exhibits significant overdispersion (where the sample variance is substantially greater than the sample mean), a NB model with a small $k$ will provide a much better fit than a Poisson model. This will result in $L_{\\mathrm{NB}} \\gg L_{\\mathrm{Pois}}$ and a large, positive value for $\\Lambda$, providing strong evidence in favor of the NB model over the Poisson model.\nIn essence, $k$ parameterizes a family of models that bridge the gap between the highly overdispersed (small $k$) and the Poisson (large $k$). The LRT statistic $\\Lambda$ quantifies the evidence for the additional flexibility (overdispersion) offered by the NB model with a specific dispersion $k$, compared to the more restrictive Poisson model. A positive $\\Lambda$ indicates that the data are more likely under the NB model, suggesting the presence of overdispersion consistent with the value of $k$.",
            "answer": "$$\\boxed{1.289}$$"
        }
    ]
}