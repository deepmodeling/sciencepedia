{
    "hands_on_practices": [
        {
            "introduction": "在神经科学中，我们经常面临高维刺激（如图像或声音片段）但数据量有限的挑战。直接拟合模型会导致过拟合，因此正则化是必不可少的。本练习旨在引导您思考如何将抽象的统计工具（如岭回归、弹性网络和全变分惩罚）与关于神经元感受野结构的具体科学假设（例如平滑、稀疏或分片常数）联系起来。通过这个练习，您将学会如何为特定问题选择合适的模型先验，并使用嵌套交叉验证等稳健的方法来评估和选择最终模型。",
            "id": "3995053",
            "problem": "一个实验室正在使用线性-非线性 (LN) 级联模型来拟合由高维视觉刺激引发的脉冲响应。在时间 $t \\in \\{1,\\dots,n\\}$ 的刺激表示为向量 $x_t \\in \\mathbb{R}^p$（通过将二维像素网格向量化得到），脉冲计数 $y_t \\in \\{0,1,2,\\dots\\}$ 通过带有对数连接函数的泊松广义线性模型 (GLM) 进行建模，因此条件强度为 $\\lambda_t = \\exp\\{k^\\top x_t + b\\}$，其中 $k \\in \\mathbb{R}^p$ 是未知的感受野，$b \\in \\mathbb{R}$ 是一个偏置项。可用数据满足 $n \\ll p$（数据有限），并且刺激向量已被白化，因此 $\\{x_t\\}$ 的经验协方差近似为单位矩阵。该实验室正在考虑三种正则化惩罚来估计 $k$：(i) 岭回归 (ridge)，对 $k$ 使用 $\\ell_2$ 惩罚；(ii) 弹性网络 (elastic net)，对 $k$ 使用 $\\ell_1$ 和 $\\ell_2$ 惩罚的凸组合；(iii) 全变分 (Total Variation, TV)，惩罚 $k$ 在二维像素网格上的离散空间梯度的绝对值之和。该团队设想了三种可能的 $k$ 的真实结构：(S$1$) 形成平滑、振荡的类 Gabor 模式的密集且分布式的权重；(S$2$) 具有少量清晰边界的分段常数区域；(S$3$) 少量孤立、不连续的非零像素。他们还必须决定在数据有限的情况下如何选择惩罚族及其超参数。\n\n以下哪个选项正确地将惩罚项与它们最匹配的真实结构对应起来，并在这个 $n \\ll p$ 的泊松 LN 设置中提出了一个统计上合理的选择标准？\n\nA. 岭回归用于(S$2$)，弹性网络用于(S$1$)，TV 用于(S$3$)；通过在完整数据集上最小化惩罚化的训练负对数似然来选择惩罚项和超参数，以避免数据分割带来的方差。\n\nB. TV 用于(S$2$)，弹性网络用于(S$3$)，岭回归用于(S$1$)；通过嵌套 K 折交叉验证来选择惩罚族和超参数，该方法最大化完整 LN 模型的留出集预测对数似然（或等效地，最小化留出集泊松偏差）。\n\nC. TV 用于(S$3$)，因为它强制系数稀疏性；岭回归用于(S$2$)，以模糊边缘；弹性网络用于(S$1$)，以稳定密集模式；通过选择具有最小样本内赤池信息量准则 (AIC) 的模型来选择惩罚项，AIC 的计算对所有惩罚项均使用 $k$ 中非零元素的数量作为自由度。\n\nD. 在刺激白化下，岭回归和弹性网络是等效的，因此两者都足以用于 (S$1$) 和 (S$3$)；TV 不适用于 (S$2$)，因为它会过度平滑边缘；通过在训练数据上进行单次 K 折交叉验证来从三者中选择，该方法同时调整超参数并在相同的折上报告最终测试性能。",
            "solution": "用户希望我解决一个关于计算神经科学中模型选择的问题。我将首先验证问题陈述。\n\n### 步骤1：提取已知条件\n- **模型类型**: 线性-非线性 (LN) 级联模型。\n- **刺激表示**: 向量 $x_t \\in \\mathbb{R}^p$，时间 $t \\in \\{1,\\dots,n\\}$。向量由二维像素网格得到。\n- **响应表示**: 脉冲计数 $y_t \\in \\{0,1,2,\\dots\\}$。\n- **统计框架**: 泊松广义线性模型 (GLM)，使用对数连接函数。\n- **条件强度（放电率）**: $\\lambda_t = \\exp\\{k^\\top x_t + b\\}$。\n- **未知参数**: $k \\in \\mathbb{R}^p$ (感受野) 和 $b \\in \\mathbb{R}$ (偏置项)。\n- **数据状况**: $n \\ll p$ (样本数量远小于特征/像素数量)。\n- **刺激预处理**: 刺激向量 $\\{x_t\\}$ 已被白化，使其经验协方差近似为单位矩阵。\n- **考虑的正则化方法**:\n    - (i) 岭回归 (Ridge): 对 $k$ 进行 $\\ell_2$ 惩罚。\n    - (ii) 弹性网络 (Elastic Net): 对 $k$ 进行 $\\ell_1$ 和 $\\ell_2$ 惩罚的凸组合。\n    - (iii) 全变分 (TV): 惩罚 $k$ 的离散空间梯度的绝对值之和。\n- **k 的假设真实结构**:\n    - (S$1$): 形成平滑、振荡的类 Gabor 模式的密集且分布式的权重。\n    - (S$2$): 具有少量清晰边界的分段常数区域。\n    - (S$3$): 少量孤立、不连续的非零像素。\n- **目标**: 将惩罚项与它们最适合的结构进行匹配，并确定一个在 $n \\ll p$ 数据状况下选择惩罚族及其超参数的统计上合理的方法。\n\n### 步骤2：使用提取的已知条件进行验证\n对问题陈述进行有效性评估。\n\n- **科学基础扎实**: 该问题牢固地植根于计算神经科学和高维统计学。LN 模型是神经编码的经典模型。泊松 GLM 是建模脉冲计数的标准统计工具。岭回归、弹性网络和全变分都是用于处理高维问题 ($n \\ll p$) 的成熟且广泛使用的正则化技术。感受野 ($k$) 的假设结构在生物学上是合理的，并代表了信号处理和统计学中已充分研究的几类不同的先验。该问题在科学上是合理的。\n- **定义明确**: 问题要求找到统计工具（正则化器）和结构假设（关于 $k$ 的先验）之间的最佳匹配，以及一个有效的模型选择程序。这是统计学习理论中一个标准且定义明确的问题。可以基于既定原则得出唯一且有意义的结论。\n- **客观**: 问题使用统计学和神经科学中常见的精确技术语言陈述。对模型、数据和结构的描述是明确的。没有主观或基于意见的主张。\n\n### 步骤3：结论与行动\n问题陈述在科学上是合理的、定义明确且客观的。它不包含矛盾、模糊或事实错误。因此，该问题是**有效的**。我将继续推导解决方案。\n\n### 正则化惩罚与结构先验分析\n\n问题第一部分的核心是将每种正则化惩罚与它隐含支持的感受野 $k$ 的结构先验相匹配。正则化的工作原理是在拟合标准（在本例中为负对数似然）中添加一个惩罚项，对于符合特定结构的解 $k$，该惩罚项最小。\n\n1.  **岭回归 (Ridge Regression)**: 惩罚项与系数的平方 $\\ell_2$ 范数成正比，即 $\\|k\\|_2^2 = \\sum_i k_i^2$。此惩罚将所有系数向零收缩，但不会将它们精确地设置为零。当真实信号是“密集的”，即许多系数非零且幅度较小时，它表现最佳。对大系数的收缩也在一般意义上鼓励了更“平滑”的解。这与结构 **(S$1$)** 完美匹配，该结构被描述为密集、分布式和平滑的模式。\n\n2.  **弹性网络回归 (Elastic Net Regression)**: 惩罚是 $\\ell_1$ 范数和平方 $\\ell_2$ 范数的线性组合：$\\alpha \\|k\\|_1 + (1-\\alpha) \\|k\\|_2^2$。对于结构假设，关键成分是 $\\ell_1$ 范数 $\\|k\\|_1 = \\sum_i |k_i|$，这是 LASSO 回归的基础。众所周知，$\\ell_1$ 惩罚会诱导稀疏性，即它会驱动许多系数恰好为零，从而有效地进行变量选择。对于被认为是稀疏的信号，这是理想的先验。这与结构 **(S$3$)** 完美匹配，该结构被描述为少量孤立的非零像素。\n\n3.  **全变分 (TV) 正则化**: 惩罚与滤波器 $k$ 在其二维网格结构上的离散梯度的大小之和成正比。对于一个由像素 $(i, j)$ 索引的滤波器 $k$，惩罚为 $\\sum_{i,j} \\sqrt{(k_{i+1,j}-k_{i,j})^2 + (k_{i,j+1}-k_{i,j})^2}$（或其各向异性版本 $\\sum_{i,j} |k_{i+1,j}-k_{i,j}| + |k_{i,j+1}-k_{i,j}|$)。如果 $k$ 是分段常数的，则此惩罚很小，因为在常数区域内梯度为零。它允许区域之间存在剧烈跳变，因为惩罚是梯度的绝对值之和，而不是它们的平方，从而保留了边缘。这与结构 **(S$2$)** 完美匹配，该结构被明确描述为具有清晰边界的分段常数区域。\n\n综上所述，正确的配对是：\n- 岭回归 $\\to$ (S$1$) 密集、平滑\n- TV $\\to$ (S$2$) 分段常数\n- 弹性网络 $\\to$ (S$3$) 稀疏\n\n### 模型选择策略分析\n\n问题的第二部分涉及在数据有限的高维环境 ($n \\ll p$) 中，选择最佳的惩罚族（岭回归、弹性网络或 TV）及其相关超参数（例如，正则化强度 $\\lambda$）。\n\n- **$n \\ll p$ 的挑战**: 在这种情况下，模型极易对训练数据过拟合。因此，任何基于训练数据性能（样本内性能）的选择标准都不可靠。一个合理的方法必须估计样本外预测性能。\n\n- **交叉验证 (CV)**: K 折交叉验证是估计样本外误差的标准且最稳健的技术。它涉及重复地将一部分数据留作测试（验证）集，并在剩余数据上训练模型。\n\n- **模型选择与性能估计**: 一个关键问题是选择偏差。如果使用 K 折交叉验证来调整超参数并选择最佳模型族，则来自同一次交叉验证过程的性能指标是对最终所选模型的真实性能的乐观偏差估计。数据已被用于选择过程“耗尽”。\n\n- **嵌套交叉验证**: 这个问题的统计上严格的解决方案是嵌套交叉验证。\n    - 一个**外层循环**将数据分成 $K_{out}$ 折。每一折用作评估最终选定模型的、最终的未见测试集。\n    - 对于每个外层分割，在训练部分上执行一个 $K_{in}$ 折交叉验证的**内层循环**。这个内层循环用于为每个候选模型族（岭回归、EN、TV）找到最优超参数，然后选择性能最好的族。\n    - 在内层循环中选择并调整好的模型，将在外层循环的完整训练集上进行训练，并其性能在外层循环的留出测试集上进行评估。\n    - 在外层循环的所有测试集上的平均性能，为整个模型选择过程的泛化性能提供了一个无偏估计。\n\n- **性能度量**: 对于泊松 GLM，目标是准确预测潜在的放电率。原则上的性能度量是留出数据在模型下的对数似然。最大化预测对数似然等同于最小化泊松偏差，后者衡量泊松模型的拟合优度。\n\n### 逐项分析\n\n**A. 岭回归用于(S$2$)，弹性网络用于(S$1$)，TV 用于(S$3$)；通过在完整数据集上最小化惩罚化的训练负对数似然来选择惩罚项和超参数，以避免数据分割带来的方差。**\n\n- **匹配**: 匹配都是错误的。岭回归用于密集模式 (S$1$)，而非分段常数模式 (S$2$)。弹性网络用于稀疏模式 (S$3$)，而非密集模式 (S$1$)。TV 用于分段常数模式 (S$2$)，而非稀疏模式 (S$3$)。\n- **选择标准**: 最小化惩罚化的训练负对数似然是*拟合*模型的目标函数，而不是*选择*模型。使用它进行选择将总是偏爱最少的正则化，导致最大程度的过拟合。这是一个根本上有缺陷的程序。\n- **结论**: **错误**。\n\n**B. TV 用于(S$2$)，弹性网络用于(S$3$)，岭回归用于(S$1$)；通过嵌套 K 折交叉验证来选择惩罚族和超参数，该方法最大化完整 LN 模型的留出集预测对数似然（或等效地，最小化留出集泊松偏差）。**\n\n- **匹配**: 匹配都如上所述是正确的：TV $\\to$ (S$2$), 弹性网络 $\\to$ (S$3$), 岭回归 $\\to$ (S$1$)。\n- **选择标准**: 这提出了嵌套 K 折交叉验证，这是在获得无偏性能估计的同时，进行组合模型族和超参数选择的正确的、统计上严格的程序。度量标准，最大化留出集预测对数似然或最小化留出集泊松偏差，是泊松模型的正确度量。\n- **结论**: **正确**。\n\n**C. TV 用于(S$3$)，因为它强制系数稀疏性；岭回归用于(S$2$)，以模糊边缘；弹性网络用于(S$1$)，以稳定密集模式；通过选择具有最小样本内赤池信息量准则 (AIC) 的模型来选择惩罚项，AIC 的计算对所有惩罚项均使用 $k$ 中非零元素的数量作为自由度。**\n\n- **匹配**: 匹配及其理由是错误的。TV 用于分段常数结构 (S$2$)，而非像素级稀疏性 (S$3$)。岭回归用于密集/平滑模式 (S$1$)，将其应用于 S$2$ 会“模糊边缘”，这对于具有清晰边界的结构是不希望的。弹性网络用于稀疏模式 (S$3$)，而非密集模式 (S$1$)。\n- **选择标准**: 使用基于非零系数数量的 AIC 作为有效自由度 ($d_{eff}$) 的通用度量是一种粗略且不准确的近似。对于岭回归，即使没有零系数，$d_{eff} > 0$。对于 TV，这个概念更加复杂。在 $n \\ll p$ 的情况下，交叉验证通常比信息准则更可靠。\n- **结论**: **错误**。\n\n**D. 在刺激白化下，岭回归和弹性网络是等效的，因此两者都足以用于 (S$1$) 和 (S$3$)；TV 不适用于 (S$2$)，因为它会过度平滑边缘；通过在训练数据上进行单次 K 折交叉验证来从三者中选择，该方法同时调整超参数并在相同的折上报告最终测试性能。**\n\n- **匹配/属性**: 这个选项包含多个事实错误。\n    - “在刺激白化下，岭回归和弹性网络是等效的”：这是错误的。白化 ($X^\\top X \\approx I$) 简化了计算，但并不会改变惩罚的基本性质（$\\|k\\|_2^2$ vs. $\\alpha \\|k\\|_1 + (1-\\alpha) \\|k\\|_2^2$）。它们会导致不同的解。\n    - “TV 不适用于 (S$2$)，因为它会过度平滑边缘”：这是错误的。TV 专门设计用于在平滑平坦区域的同时*保留*清晰边缘。\n- **选择标准**: 提出单次 K 折交叉验证循环来同时调整参数并报告最终性能是一个有缺陷的程序，会导致乐观偏差的性能估计。这正是嵌套交叉验证旨在解决的问题。\n- **结论**: **错误**。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "随着我们试图用LN模型来描述神经元对日益复杂的时空刺激（如视频）的响应，模型的参数数量可能会爆炸式增长，使得模型拟合的计算成本成为一个主要障碍。本练习将带您深入分析模型拟合过程中的计算瓶颈，特别是梯度和Hessian矩阵的计算复杂度。更重要的是，它将引导您探索一种强大的优化技巧——低秩近似，来显著加速大规模模型的训练，这体现了在模型表达能力与计算可行性之间进行权衡的艺术。",
            "id": "3995099",
            "problem": "考虑一个神经编码的线性-非线性级联模型，其中离散时间索引 $t$ 处的时空刺激由矩阵 $X_t \\in \\mathbb{R}^{n_s \\times n_t}$ 表示，时空滤波器由矩阵 $W \\in \\mathbb{R}^{n_s \\times n_t}$ 表示，线性阶段计算 $y_t = \\mathrm{trace}(W^{\\top} X_t)$。非线性阶段是泊松广义线性模型 (GLM; Generalized Linear Model) 中的一个指数函数，用于脉冲计数，其条件强度为 $\\lambda_t = \\exp(y_t + b)$，其中 $b \\in \\mathbb{R}$ 是一个偏置。观测到的脉冲计数为 $\\{s_t\\}_{t=1}^{T}$，其中 $s_t \\in \\mathbb{N}$。泊松观测模型的对数似然为\n$$\n\\mathcal{L}(W,b) = \\sum_{t=1}^{T} \\left[ s_t \\ln \\lambda_t - \\lambda_t - \\ln(s_t!) \\right].\n$$\n你可以使用标准的矩阵微积分和链式法则，并在方便时通过 $\\mathrm{vec}(\\cdot)$ 对矩阵进行向量化。\n\n任务：\n1. 从上述对数似然和线性-非线性结构的定义出发，推导关于向量化滤波器 $w = \\mathrm{vec}(W) \\in \\mathbb{R}^{p}$（其中 $p = n_s n_t$）的梯度和关于 $w$ 的海森矩阵的表达式。然后，分析精确计算梯度和海森矩阵（每次评估）所需浮点运算的主阶计算复杂度，用 $T$、$n_s$ 和 $n_t$ 表示。假设一个朴素的密集实现，其中在 $\\mathbb{R}^{p \\times p}$ 中形成一个外积的成本约为 $p^2$ 次运算，并忽略常数因子。\n\n2. 提出一种形式为 $W = U V^{\\top}$ 的时空滤波器低秩近似，其中 $U \\in \\mathbb{R}^{n_s \\times r}$ 和 $V \\in \\mathbb{R}^{n_t \\times r}$，且 $r \\ll \\min(n_s,n_t)$。定义由 $U$ 和 $V$ 的元素堆叠而成的参数向量 $\\theta$，并确定其维度。在高斯-牛顿近似下（即，忽略线性预测器相对于参数的二阶导数，从而使海森矩阵近似为由强度缩放的雅可比行向量的外积之和），推导精确形成关于 $\\theta$ 的近似海森矩阵（每次评估）的主阶计算复杂度，用 $T$、$n_s$、$n_t$ 和 $r$ 表示。说明你对每个时间点 $t$ 组建雅可比向量的成本所做的任何假设。\n\n3. 使用任务1和2的结果，计算理论加速因子，该因子定义为在全参数化 $W$ 下形成精确海森矩阵与在低秩参数化 $W = U V^{\\top}$ 下形成高斯-牛顿近似海森矩阵的浮点运算次数之比。考虑 $n_s = 64$、$n_t = 100$、$T = 10^{5}$ 和 $r = 5$ 的情况。将加速因子以一个无量纲实数的形式给出，并将答案四舍五入到四位有效数字。不要包含任何单位。",
            "solution": "首先确认该问题在科学上是合理的、良定的、客观的和自洽的，没有明显的缺陷。因此，我们可以着手解决。\n\n该问题分为三个任务。我们将按顺序解决它们。\n\n**任务1：完整模型的梯度、海森矩阵和复杂度**\n\n泊松GLM的对数似然由下式给出：\n$$\n\\mathcal{L}(W,b) = \\sum_{t=1}^{T} \\left[ s_t \\ln \\lambda_t - \\lambda_t - \\ln(s_t!) \\right]\n$$\n条件强度为 $\\lambda_t = \\exp(y_t + b)$，其中线性预测器为 $y_t = \\mathrm{trace}(W^{\\top} X_t)$。我们感兴趣的参数是滤波器 $W$ 的元素。为了进行微积分计算，使用向量化的滤波器 $w = \\mathrm{vec}(W) \\in \\mathbb{R}^{p}$（其中 $p = n_s n_t$）会很方便。\n\n使用矩阵恒等式 $\\mathrm{trace}(A^{\\top} B) = \\mathrm{vec}(A)^{\\top} \\mathrm{vec}(B)$，我们可以将线性预测器重写为点积：\n$$\ny_t = \\mathrm{trace}(W^{\\top} X_t) = \\mathrm{vec}(W)^{\\top} \\mathrm{vec}(X_t) = w^{\\top} x_t\n$$\n其中 $x_t = \\mathrm{vec}(X_t) \\in \\mathbb{R}^{p}$。\n\n项 $\\ln(s_t!)$ 相对于参数 $w$ 和 $b$ 是常数，因此在求导时可以忽略它。设 $\\mathcal{L}_t = s_t \\ln \\lambda_t - \\lambda_t$。忽略常数项，完整的对数似然为 $\\mathcal{L} = \\sum_{t=1}^T \\mathcal{L}_t$。\n\n为了求梯度 $\\nabla_w \\mathcal{L}$，我们应用链式法则：\n$$\n\\nabla_w \\mathcal{L} = \\sum_{t=1}^{T} \\nabla_w \\mathcal{L}_t = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}_t}{\\partial \\lambda_t} \\frac{\\partial \\lambda_t}{\\partial y_t} \\nabla_w y_t\n$$\n各项导数分别为：\n- $\\frac{\\partial \\mathcal{L}_t}{\\partial \\lambda_t} = \\frac{s_t}{\\lambda_t} - 1$\n- $\\frac{\\partial \\lambda_t}{\\partial y_t} = \\frac{\\partial}{\\partial y_t} \\exp(y_t + b) = \\exp(y_t + b) = \\lambda_t$\n- $\\nabla_w y_t = \\nabla_w (w^{\\top} x_t) = x_t$\n\n将这些结合起来，得到单个时间点的梯度：\n$$\n\\nabla_w \\mathcal{L}_t = \\left(\\frac{s_t}{\\lambda_t} - 1\\right) \\lambda_t x_t = (s_t - \\lambda_t) x_t\n$$\n完整的梯度是所有时间点上的总和：\n$$\n\\nabla_w \\mathcal{L} = \\sum_{t=1}^{T} (s_t - \\lambda_t) x_t = \\sum_{t=1}^{T} (s_t - \\exp(w^{\\top}x_t + b)) \\mathrm{vec}(X_t)\n$$\n\n为了求海森矩阵 $\\nabla_w^2 \\mathcal{L}$，我们将梯度对 $w^{\\top}$ 求导：\n$$\n\\nabla_w^2 \\mathcal{L} = \\nabla_w \\left( \\sum_{t=1}^{T} (s_t - \\lambda_t) x_t \\right)^{\\top} = \\sum_{t=1}^{T} \\nabla_w \\left( (s_t - \\lambda_t) x_t^{\\top} \\right)\n$$\n使用乘法法则，并注意到 $s_t$ 和 $x_t$ 相对于 $w$ 是常数：\n$$\n\\nabla_w^2 \\mathcal{L}_t = \\nabla_w (s_t - \\lambda_t) x_t^{\\top} = (-\\nabla_w \\lambda_t) x_t^{\\top}\n$$\n我们使用链式法则求 $\\nabla_w \\lambda_t$：\n$$\n\\nabla_w \\lambda_t = \\frac{\\partial \\lambda_t}{\\partial y_t} \\nabla_w y_t = \\lambda_t x_t\n$$\n将其代回，我们得到单个时间点对海森矩阵的贡献：\n$$\n\\nabla_w^2 \\mathcal{L}_t = -(\\lambda_t x_t) x_t^{\\top} = -\\lambda_t x_t x_t^{\\top}\n$$\n完整的海森矩阵是所有时间点上的总和：\n$$\n\\nabla_w^2 \\mathcal{L} = -\\sum_{t=1}^{T} \\lambda_t x_t x_t^{\\top} = -\\sum_{t=1}^{T} \\exp(w^{\\top}x_t + b) \\mathrm{vec}(X_t) \\mathrm{vec}(X_t)^{\\top}\n$$\n\n现在进行计算复杂度分析。设 $p = n_s n_t$。\n- **梯度评估**：对于 $T$ 个时间点中的每一个，我们必须计算 $y_t = w^{\\top} x_t$（一个大小为 $p$ 的点积，成本为 $\\mathcal{O}(p)$ 次浮点运算），然后是 $\\lambda_t$（$\\mathcal{O}(1)$ 次浮点运算），然后是标量-向量乘积 $(s_t - \\lambda_t) x_t$（$\\mathcal{O}(p)$ 次浮点运算）。将这 $T$ 个大小为 $p$ 的向量相加的成本为 $\\mathcal{O}(Tp)$。因此，主阶复杂度为 $\\mathcal{O}(T p) = \\mathcal{O}(T n_s n_t)$。\n- **海森矩阵评估**：对于 $T$ 个时间点中的每一个，我们像之前一样计算 $\\lambda_t$（$\\mathcal{O}(p)$ 次浮点运算）。主要成本是形成外积 $x_t x_t^{\\top}$，这是一个 $p \\times p$ 矩阵，成本为 $\\mathcal{O}(p^2)$ 次浮点运算。将 $T$ 个这样的矩阵相加的成本为 $\\mathcal{O}(T p^2)$。因此，主阶复杂度为 $\\mathcal{O}(T p^2) = \\mathcal{O}(T (n_s n_t)^2) = \\mathcal{O}(T n_s^2 n_t^2)$。\n\n**任务2：低秩近似与高斯-牛顿复杂度**\n\n滤波器近似为 $W = U V^{\\top}$，其中 $U \\in \\mathbb{R}^{n_s \\times r}$ 和 $V \\in \\mathbb{R}^{n_t \\times r}$。新的参数向量 $\\theta$ 由 $U$ 和 $V$ 的堆叠元素组成：\n$$\n\\theta = \\begin{pmatrix} \\mathrm{vec}(U) \\\\ \\mathrm{vec}(V) \\end{pmatrix}\n$$\n$\\theta$ 的维度是 $U$ 和 $V$ 中元素数量的总和，即 $d = n_s r + n_t r = r(n_s + n_t)$。\n\n线性预测器变为 $y_t(\\theta) = \\mathrm{trace}((UV^{\\top})^{\\top} X_t) = \\mathrm{trace}(V U^{\\top} X_t)$。\n对数似然的海森矩阵的高斯-牛顿近似由下式给出：\n$$\nH_{GN} \\approx -\\sum_{t=1}^{T} \\lambda_t (\\nabla_{\\theta} y_t) (\\nabla_{\\theta} y_t)^{\\top}\n$$\n其中 $\\nabla_{\\theta} y_t$ 是时间点 $t$ 时线性预测器关于参数 $\\theta$ 的雅可比矩阵。我们使用矩阵微分法求关于 $U$ 和 $V$ 的导数。\n线性预测器 $y_t$ 的全微分为 $dy_t = \\mathrm{tr}(\\mathrm{d}(V U^{\\top} X_t)) = \\mathrm{tr}((\\mathrm{d}V) U^{\\top} X_t + V (\\mathrm{d}U)^{\\top} X_t)$。\n\n1.  **关于 U 的导数**: 保持 V 不变 ($\\mathrm{d}V=0$):\n    $dy_t = \\mathrm{tr}(V (\\mathrm{d}U)^{\\top} X_t) = \\mathrm{tr}((\\mathrm{d}U)^{\\top} X_t V) = \\langle X_t V, \\mathrm{d}U \\rangle$。\n    因此，$\\frac{\\partial y_t}{\\partial U} = X_t V$。\n\n2.  **关于 V 的导数**: 保持 U 不变 ($\\mathrm{d}U=0$):\n    $dy_t = \\mathrm{tr}((\\mathrm{d}V) U^{\\top} X_t) = \\langle (U^{\\top} X_t)^{\\top}, \\mathrm{d}V \\rangle = \\langle X_t^{\\top} U, \\mathrm{d}V \\rangle$。\n    因此，$\\frac{\\partial y_t}{\\partial V} = X_t^{\\top} U$。\n\n雅可比矩阵 $\\nabla_{\\theta} y_t$ 是将上述矩阵导数向量化后的拼接：\n$$\n\\nabla_{\\theta} y_t = J_t = \\begin{pmatrix} \\nabla_{\\mathrm{vec}(U)} y_t \\\\ \\nabla_{\\mathrm{vec}(V)} y_t \\end{pmatrix} = \\begin{pmatrix} \\mathrm{vec}(X_t V) \\\\ \\mathrm{vec}(X_t^{\\top} U) \\end{pmatrix}\n$$\n这个向量的维度是 $d = r(n_s+n_t)$。\n\n对于复杂度分析，我们假设成本主要由矩阵乘法中的浮点运算决定。\n为了形成高斯-牛顿海森矩阵 $H_{GN}$，对于每个时间步 $t=1, \\dots, T$：\n1. 计算雅可比矩阵 $J_t$：\n    - $X_t V$：$(n_s \\times n_t) \\times (n_t \\times r)$ 的成本为 $\\mathcal{O}(r n_s n_t)$ 次浮点运算。\n    - $X_t^{\\top} U$：$(n_t \\times n_s) \\times (n_s \\times r)$ 的成本为 $\\mathcal{O}(r n_s n_t)$ 次浮点运算。\n    - 形成 $J_t$ 的总成本为 $\\mathcal{O}(r n_s n_t)$。\n2. 形成外积 $J_t J_t^{\\top}$。这是一个 $d \\times d$ 矩阵。\n    - 成本为 $\\mathcal{O}(d^2) = \\mathcal{O}((r(n_s+n_t))^2) = \\mathcal{O}(r^2(n_s+n_t)^2)$。\n3. 将 $T$ 个这样的矩阵相加。\n\n由于 $r \\ll \\min(n_s, n_t)$，我们有 $d^2 = r^2(n_s+n_t)^2$ 通常大于 $r n_s n_t$。因此，主导成本是形成外积。\n形成完整高斯-牛顿海森矩阵的主阶复杂度是每个时间步的成本乘以 $T$：\n$$\n\\mathcal{O}(T d^2) = \\mathcal{O}(T r^2 (n_s+n_t)^2)\n$$\n\n**任务3：加速因子计算**\n\n理论加速因子是两种海森矩阵计算方法的浮点运算次数之比。我们使用上面推导出的主阶复杂度。\n$$\n\\text{加速因子} = \\frac{\\text{完整海森矩阵的成本}}{\\text{高斯-牛顿近似海森矩阵的成本}} = \\frac{\\mathcal{O}(T n_s^2 n_t^2)}{\\mathcal{O}(T r^2 (n_s+n_t)^2)}\n$$\n假设隐藏的常数在同一数量级，加速因子 $S$ 为：\n$$\nS = \\frac{T n_s^2 n_t^2}{T r^2 (n_s+n_t)^2} = \\frac{(n_s n_t)^2}{r^2 (n_s+n_t)^2} = \\left( \\frac{n_s n_t}{r(n_s+n_t)} \\right)^2\n$$\n给定值为：$n_s = 64$，$n_t = 100$，$r = 5$。$T=10^5$ 的值是无关紧要的，因为它被消掉了。\n\n将给定值代入 $S$ 的表达式：\n$$\nn_s n_t = 64 \\times 100 = 6400\n$$\n$$\nn_s + n_t = 64 + 100 = 164\n$$\n$$\nr(n_s+n_t) = 5 \\times 164 = 820\n$$\n$$\nS = \\left( \\frac{6400}{820} \\right)^2 = \\left( \\frac{640}{82} \\right)^2 = \\left( \\frac{320}{41} \\right)^2\n$$\n现在，我们计算数值：\n$$\nS = \\frac{320^2}{41^2} = \\frac{102400}{1681} \\approx 60.916121356...\n$$\n四舍五入到四位有效数字，加速因子为 $60.92$。",
            "answer": "$$\\boxed{60.92}$$"
        }
    ]
}