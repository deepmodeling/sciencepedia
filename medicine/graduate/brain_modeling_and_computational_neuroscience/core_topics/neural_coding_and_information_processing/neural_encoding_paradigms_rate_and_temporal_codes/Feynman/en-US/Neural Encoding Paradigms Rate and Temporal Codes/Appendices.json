{
    "hands_on_practices": [
        {
            "introduction": "We begin with the foundational rate-coding hypothesis, where information is encoded in the average firing rate of a neuron. This exercise challenges you to think like a modeler, investigating the conditions under which the parameters of a classic Gaussian tuning curve can be uniquely identified from spike count data. Understanding parameter identifiability is crucial for building and validating meaningful models of neural responses .",
            "id": "4003111",
            "problem": "Consider a single sensory neuron whose stimulus-dependent firing is modeled under a rate code as follows. For a scalar stimulus $x \\in \\mathbb{R}$ and observation window duration $T > 0$, the spike count $N$ in that window is generated conditionally independently across trials, with distribution $\\Pr(N=n \\mid x) = \\frac{(T r(x))^{n} e^{-T r(x)}}{n!}$ for $n \\in \\mathbb{N}$, where the mean firing rate (tuning curve) is parameterized by\n$$\nr(x) = r_{0} + \\alpha \\exp\\!\\Big(-\\frac{(x - \\mu)^{2}}{2 \\sigma^{2}}\\Big),\n$$\nwith parameters $(r_{0}, \\alpha, \\mu, \\sigma) \\in \\mathbb{R}^{4}$ subject to the constraints $r(x) > 0$ for all $x$, $\\sigma > 0$, and $T > 0$. You collect spike counts at stimulus values $\\{x_{i}\\}_{i=1}^{n}$ with $n \\geq 4$, each repeated many times, and use the conditional Poisson model to fit $(r_{0}, \\alpha, \\mu, \\sigma)$ by maximum likelihood.\n\nSelect all statements that correctly characterize identifiability conditions for $(r_{0}, \\alpha, \\mu, \\sigma)$ from such spike count data.\n\nA. If the observation window duration $T$ is unknown but constant across trials, then only the product $T r(x)$ is identifiable from spike counts; consequently, the absolute scale of $r_{0}$ and $\\alpha$ is not identifiable from counts alone.\n\nB. If the stimulus set $\\{x_{i}\\}_{i=1}^{n}$ contains at least one value $x_{\\text{far}}$ with $\\exp(-(x_{\\text{far}} - \\mu)^{2} / (2 \\sigma^{2})) \\approx 0$, at least one value $x_{\\text{near}}$ with $\\exp(-(x_{\\text{near}} - \\mu)^{2} / (2 \\sigma^{2})) \\approx 1$, and at least two additional values $x_{j}$ at distinct intermediate distances from $\\mu$ such that the corresponding $\\exp(-(x_{j} - \\mu)^{2} / (2 \\sigma^{2}))$ take distinct values in $(0, 1)$, then, under the constraints $\\sigma > 0$, $\\alpha \\neq 0$, $r_{0} > 0$, and $r_{0} + \\alpha > 0$, the parameter vector $(r_{0}, \\alpha, \\mu, \\sigma)$ is structurally identifiable from the mean rate function $r(x)$ and hence from spike counts under the Poisson model.\n\nC. Because $r(x)$ is symmetric in $x$ around $\\mu$, spike counts cannot identify $\\mu$ itself; only $| \\mu |$ can be recovered.\n\nD. If $\\alpha = 0$, then $\\mu$ and $\\sigma$ are not identifiable from spike counts regardless of stimulus design, but $r_{0}$ is identifiable provided $T$ is known.\n\nE. Identifiability of $\\sigma$ requires spike timing data (millisecond-resolution interspike intervals); spike counts are insufficient.\n\nF. If all presented stimuli satisfy $|x_{i} - \\mu| \\gg \\sigma$ so that $\\exp(-(x_{i} - \\mu)^{2} / (2 \\sigma^{2})) \\approx 0$ for all $i$, then $(\\alpha, \\mu, \\sigma)$ are not identifiable from the data and only $r_{0}$ is identifiable (assuming known $T$).",
            "solution": "The identifiability of the model's parameters $(r_{0}, \\alpha, \\mu, \\sigma)$ depends on whether the function $r(x)$ can be uniquely determined from experimental data, and whether the parameters in turn uniquely define the function $r(x)$. The data (spike counts) allow us to estimate the mean spike count $\\lambda(x) = T r(x)$. We analyze each option based on this principle.\n\n**A. Correct.** If the observation time $T$ is unknown, the experimental data can only identify the mean spike count function $\\lambda(x) = T r(x) = T r_0 + T\\alpha \\exp(\\dots)$. We can determine the products $T r_0$ and $T\\alpha$, but we cannot separate $T$ from $r_0$ and $\\alpha$. For example, a parameter set $(r_0, \\alpha)$ with time $T$ is indistinguishable from $(r_0/2, \\alpha/2)$ with time $2T$. Thus, the absolute scales of $r_0$ and $\\alpha$ are not identifiable.\n\n**B. Correct.** The model is structurally identifiable because the four parameters correspond to unique features of the Gaussian tuning curve's shape: baseline ($r_0$), peak amplitude ($\\alpha$), center ($\\mu$), and width ($\\sigma$). The described stimulus set is designed to probe these features by sampling the rate function far from the peak (to estimate $r_0$), near the peak (to estimate $r_0 + \\alpha$), and at intermediate points (to constrain $\\mu$ and $\\sigma$). With at least four such informative points, the four parameters can be uniquely recovered from the rate function $r(x)$.\n\n**C. Incorrect.** The tuning curve is symmetric around the stimulus value $x=\\mu$. Since the experimenter controls and knows the stimulus value $x$ presented, the peak of the response will be observed at a specific stimulus value. This value directly identifies $\\mu$. For instance, a peak at $x=10$ identifies $\\mu=10$, which is distinct from a peak at $x=-10$ which would identify $\\mu=-10$.\n\n**D. Correct.** If $\\alpha = 0$, the rate function simplifies to $r(x) = r_0$. The rate becomes constant and independent of the stimulus $x$. The parameters $\\mu$ and $\\sigma$, which describe the shape of the Gaussian component, no longer influence the firing rate and are therefore fundamentally non-identifiable. The constant rate $r_0$ can be identified from the mean spike count, provided the observation time $T$ is known ($r_0 = \\mathbb{E}[N]/T$).\n\n**E. Incorrect.** The parameter $\\sigma$ determines the width of the *rate* tuning curve $r(x)$. This width affects how the average spike count changes as a function of the stimulus $x$. By measuring spike counts over a range of stimuli, one can map out the shape of $r(x)$ and thereby estimate $\\sigma$. This is a rate code model, so by definition, all information is in the counts; spike timing within the window is irrelevant.\n\n**F. Correct.** If the experiment is designed such that all stimuli are in the \"tail\" of the tuning curve ($|x_i - \\mu| \\gg \\sigma$), the exponential term will be close to zero for all measurements. The observed firing rate will be approximately the constant baseline rate $r(x_i) \\approx r_0$. The data will contain no information about the peak of the tuning curve, rendering its shape parameters ($\\alpha$, $\\mu$, $\\sigma$) non-identifiable. Only the baseline rate $r_0$ can be estimated.",
            "answer": "$$\\boxed{ABDF}$$"
        },
        {
            "introduction": "We now shift our focus to temporal coding, where the precise timing of spikes carries information. This exercise explores the 'phase-of-firing' code, where a neuron's activity is synchronized to a background brain rhythm. You will construct a statistically principled decoder to read out the encoded information and evaluate its accuracy, linking the theoretical precision of the code to the practical performance of the decoder .",
            "id": "4003068",
            "problem": "A stimulus feature is encoded by the phase of spikes relative to a known ongoing oscillation, exemplifying a temporal code. Consider a single trial in which a decoder observes $N$ spike phases $\\{\\phi_{k}\\}_{k=1}^{N}$, each defined on the circle $[0,2\\pi)$ as the spikeâ€™s phase relative to the oscillation. Assume the conditional distribution of each phase given the unknown stimulus phase $\\theta$ is independent and identically distributed and follows a von Mises distribution with concentration parameter $\\kappa$ and mean direction $\\theta$, that is, for each spike,\n$$\np(\\phi \\mid \\theta) = \\frac{1}{2\\pi I_{0}(\\kappa)} \\exp\\!\\big(\\kappa \\cos(\\phi - \\theta)\\big),\n$$\nwhere $I_{0}(\\kappa)$ denotes the modified Bessel function of the first kind of order zero. The population-level circular variance of this phase distribution is defined as $\\mathcal{V} = 1 - \\rho$, where the mean resultant length $\\rho$ is $\\rho = I_{1}(\\kappa)/I_{0}(\\kappa)$, with $I_{1}(\\kappa)$ the modified Bessel function of the first kind of order one.\n\nStarting from these definitions and the independence assumption, construct a statistically principled decoder for $\\theta$ based on the observed phases $\\{\\phi_{k}\\}_{k=1}^{N}$. Working in the high-concentration regime where the von Mises distribution can be approximated by a small-noise model on the circle, derive an approximation for the expected mean squared error of the phase estimate as a function of the circular variance $\\mathcal{V}$ and the spike count $N$.\n\nEvaluate this approximation numerically for $N = 50$ spikes and circular variance $\\mathcal{V} = 0.15$. Express the final error in radians squared and round your answer to four significant figures.",
            "solution": "The problem poses a valid and well-defined question in computational neuroscience concerning the decoding of a stimulus feature from a temporal spike code. The provided model, based on the von Mises distribution, is a standard and scientifically grounded framework for analyzing phase-of-firing codes. The assumptions are clearly stated, and the task is to derive and evaluate the performance of a statistically principled decoder. There are no scientific or logical flaws, and all necessary information is provided.\n\nWe begin by constructing a statistically principled decoder for the unknown stimulus phase $\\theta$ from the observed spike phases $\\{\\phi_k\\}_{k=1}^{N}$. The Maximum Likelihood Estimator (MLE) is a canonical choice for such a task. The likelihood function $L(\\theta)$ for the set of $N$ independent and identically distributed spike phases is the product of their individual probability density functions:\n$$\nL(\\theta \\mid \\{\\phi_k\\}) = \\prod_{k=1}^{N} p(\\phi_k \\mid \\theta) = \\prod_{k=1}^{N} \\frac{1}{2\\pi I_0(\\kappa)} \\exp\\big(\\kappa \\cos(\\phi_k - \\theta)\\big)\n$$\nTo simplify the maximization, we work with the log-likelihood function, $\\ln L(\\theta)$:\n$$\n\\ln L(\\theta) = \\sum_{k=1}^{N} \\left[ -\\ln(2\\pi) - \\ln(I_0(\\kappa)) + \\kappa \\cos(\\phi_k - \\theta) \\right]\n$$\nThe MLE for $\\theta$, denoted $\\hat{\\theta}$, is the value of $\\theta$ that maximizes $\\ln L(\\theta)$. This is equivalent to maximizing the term that depends on $\\theta$:\n$$\n\\hat{\\theta} = \\arg\\max_{\\theta} \\sum_{k=1}^{N} \\kappa \\cos(\\phi_k - \\theta) = \\arg\\max_{\\theta} \\sum_{k=1}^{N} \\cos(\\phi_k - \\theta)\n$$\nUsing the trigonometric identity $\\cos(a - b) = \\cos a \\cos b + \\sin a \\sin b$, the summation becomes:\n$$\n\\sum_{k=1}^{N} \\cos(\\phi_k - \\theta) = \\left(\\sum_{k=1}^{N} \\cos\\phi_k\\right)\\cos\\theta + \\left(\\sum_{k=1}^{N} \\sin\\phi_k\\right)\\sin\\theta\n$$\nThis expression is of the form $C \\cos\\theta + S \\sin\\theta$, where $C = \\sum_{k=1}^{N} \\cos\\phi_k$ and $S = \\sum_{k=1}^{N} \\sin\\phi_k$. This can be rewritten in the form $R \\cos(\\theta - \\bar{\\phi})$, where the amplitude $R = \\sqrt{C^2 + S^2}$ and the phase angle $\\bar{\\phi} = \\operatorname{atan2}(S, C)$, the four-quadrant arctangent. This expression is maximized when its cosine term is equal to $1$, which occurs when $\\theta = \\bar{\\phi}$. Therefore, the MLE decoder is the circular mean of the observed phases:\n$$\n\\hat{\\theta} = \\operatorname{atan2}\\left(\\sum_{k=1}^{N} \\sin\\phi_k, \\sum_{k=1}^{N} \\cos\\phi_k\\right)\n$$\nNext, we derive an approximation for the expected mean squared error (MSE), defined as $E[(\\hat{\\theta} - \\theta)^2]$, under the high-concentration assumption ($\\kappa \\gg 1$). In this regime, the von Mises distribution is sharply peaked around its mean $\\theta$, and for small deviations $(\\phi - \\theta)$, we can approximate $\\cos(\\phi - \\theta) \\approx 1 - \\frac{(\\phi - \\theta)^2}{2}$. The von Mises PDF becomes:\n$$\np(\\phi \\mid \\theta) \\approx \\frac{1}{2\\pi I_0(\\kappa)} \\exp\\left(\\kappa \\left[1 - \\frac{(\\phi - \\theta)^2}{2}\\right]\\right) = \\frac{e^{\\kappa}}{2\\pi I_0(\\kappa)} \\exp\\left(-\\frac{(\\phi - \\theta)^2}{2(1/\\kappa)}\\right)\n$$\nUsing the asymptotic expansion for the modified Bessel function for large $\\kappa$, $I_0(\\kappa) \\approx \\frac{\\exp(\\kappa)}{\\sqrt{2\\pi\\kappa}}$, the normalization constant simplifies to $\\frac{1}{\\sqrt{2\\pi(1/\\kappa)}}$. Thus, the von Mises distribution is well-approximated by a Normal distribution on the line:\n$$\np(\\phi \\mid \\theta) \\approx \\mathcal{N}(\\theta, \\sigma^2 = 1/\\kappa)\n$$\nUnder this Gaussian approximation, the set of phases $\\{\\phi_k\\}$ can be treated as an i.i.d. sample from a Normal distribution with mean $\\theta$ and variance $1/\\kappa$. For a normal distribution, the MLE $\\hat{\\theta}$ is simply the sample arithmetic mean, which is consistent with the circular mean for tightly clustered data:\n$$\n\\hat{\\theta} \\approx \\frac{1}{N} \\sum_{k=1}^{N} \\phi_k\n$$\nThis estimator is unbiased, as $E[\\hat{\\theta}] = \\frac{1}{N} \\sum_{k=1}^{N} E[\\phi_k] = \\frac{1}{N} (N\\theta) = \\theta$. The variance of this estimator is:\n$$\n\\text{Var}(\\hat{\\theta}) = \\text{Var}\\left(\\frac{1}{N} \\sum_{k=1}^{N} \\phi_k\\right) = \\frac{1}{N^2} \\sum_{k=1}^{N} \\text{Var}(\\phi_k) = \\frac{1}{N^2} \\sum_{k=1}^{N} \\frac{1}{\\kappa} = \\frac{N}{N^2 \\kappa} = \\frac{1}{N\\kappa}\n$$\nSince the estimator is unbiased, its MSE is equal to its variance, so $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] \\approx \\frac{1}{N\\kappa}$.\n\nTo express the MSE in terms of the circular variance $\\mathcal{V}$, we use the provided definitions $\\mathcal{V} = 1 - \\rho$ and $\\rho = I_1(\\kappa)/I_0(\\kappa)$. We need the large-$\\kappa$ asymptotic behavior of $\\rho$. Using the expansions $I_1(\\kappa) \\approx \\frac{\\exp(\\kappa)}{\\sqrt{2\\pi\\kappa}}\\left(1 - \\frac{3}{8\\kappa}\\right)$ and $I_0(\\kappa) \\approx \\frac{\\exp(\\kappa)}{\\sqrt{2\\pi\\kappa}}\\left(1 + \\frac{1}{8\\kappa}\\right)$, their ratio is:\n$$\n\\rho \\approx \\frac{1 - 3/(8\\kappa)}{1 + 1/(8\\kappa)} \\approx \\left(1 - \\frac{3}{8\\kappa}\\right)\\left(1 - \\frac{1}{8\\kappa}\\right) \\approx 1 - \\frac{4}{8\\kappa} = 1 - \\frac{1}{2\\kappa}\n$$\nNow, we find the relationship between $\\mathcal{V}$ and $\\kappa$:\n$$\n\\mathcal{V} = 1 - \\rho \\approx 1 - \\left(1 - \\frac{1}{2\\kappa}\\right) = \\frac{1}{2\\kappa}\n$$\nThis implies that for large $\\kappa$, $\\kappa \\approx \\frac{1}{2\\mathcal{V}}$. Substituting this into our expression for the MSE gives the final approximation:\n$$\n\\text{MSE}(\\hat{\\theta}) \\approx \\frac{1}{N\\kappa} \\approx \\frac{1}{N (1/(2\\mathcal{V}))} = \\frac{2\\mathcal{V}}{N}\n$$\nFinally, we evaluate this expression numerically for the given values $N = 50$ and $\\mathcal{V} = 0.15$. The use of this approximation is justified because $\\mathcal{V} = 0.15$ implies $\\kappa \\approx \\frac{1}{2(0.15)} = \\frac{1}{0.3} \\approx 3.33$, which is moderately large.\n$$\n\\text{MSE}(\\hat{\\theta}) \\approx \\frac{2 \\times 0.15}{50} = \\frac{0.30}{50} = \\frac{3}{500} = 0.006\n$$\nThe problem asks for the answer to be rounded to four significant figures. Thus, the expected mean squared error is approximately $0.006000$ radians squared.",
            "answer": "$$\n\\boxed{0.006000}\n$$"
        },
        {
            "introduction": "Having examined rate and temporal codes individually, a key question for any neuroscientist is how to tell them apart with experimental data. This final exercise puts you in the role of an experimentalist, tasked with designing an analysis that can distinguish between a slow rate modulation and a fast temporal pattern. You will derive the necessary conditions for your primary analysis tool, the Peri-Stimulus Time Histogram (PSTH), to have sufficient temporal resolution and statistical power to reveal the signatures of a temporal code .",
            "id": "4003087",
            "problem": "A single neuron is recorded under repeated presentations of an identical stimulus. Each presentation defines a peristimulus window of duration $W = 100\\,\\mathrm{ms}$, and the total recording time is $T = N W$, where $N$ is the number of stimulus repetitions. The neuron's spike train is modeled as a conditionally inhomogeneous Poisson process with instantaneous rate $r(t)$ under both hypotheses considered. Under the rate-code hypothesis, $r(t)$ varies only slowly within $W$ so that spike count over coarse windows captures the information; under the temporal-code hypothesis, $r(t)$ contains narrow stimulus-locked features (peaks or troughs) with timing precision characterized by a standard deviation $\\sigma$ around the feature center. The expected average firing rate across the window is $r \\in [10,40]\\,\\mathrm{Hz}$, and the minimal fractional modulation of the narrow temporal feature relative to baseline is $m \\approx 0.3$.\n\nYou will analyze the Peri-Stimulus Time Histogram (PSTH), which is built by binning spike times with bin width $\\Delta t$ and aggregating counts across $N$ repetitions. Use the following fundamental bases: (i) Poisson counting statistics where the variance of counts equals the mean, (ii) the Central Limit Theorem for approximating count fluctuations by Gaussian noise at large counts, and (iii) the Nyquist-Shannon sampling theorem, which requires sampling at least twice the highest frequency to resolve a feature of duration scale. The target is to select $\\Delta t$ and $T$ so that the narrow temporal features are both resolvable and statistically detectable at a $z$-score threshold $z = 3$, while coarse-rate differences would also be detectable if present.\n\nWhich option provides a sufficient criterion on $\\Delta t$ and $T$ to reliably discriminate between a rate code and a temporal code in this setting?\n\nA. Choose $\\Delta t \\le \\sigma/2$ and ensure\n$$\nT \\ge \\frac{z^2}{m^2}\\,\\frac{W}{r\\,\\Delta t}\\,.\n$$\n\nB. Choose $\\Delta t = 1/r$ and ensure\n$$\nT \\ge \\frac{z^2}{m^2}\\,r\\,\\Delta t\\,W\\,.\n$$\n\nC. Choose $\\Delta t \\ge \\sigma$ and ensure\n$$\nT \\ge \\frac{z^2}{m^2}\\,\\frac{1}{r}\\,.\n$$\n\nD. Choose $\\Delta t \\le \\sigma/2$ and ensure\n$$\nT \\ge \\frac{z^2}{m^2}\\,\\frac{1}{r}\\,.\n$$",
            "solution": "The user requires a critical validation of the problem statement, followed by a rigorous derivation of the solution and evaluation of the given options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   Stimulus presentation window duration: $W = 100\\,\\mathrm{ms}$.\n*   Number of stimulus repetitions: $N$.\n*   Total recording time: $T = N W$.\n*   Spike train model: Conditionally inhomogeneous Poisson process with instantaneous rate $r(t)$.\n*   Rate-code hypothesis: $r(t)$ varies slowly within $W$.\n*   Temporal-code hypothesis: $r(t)$ contains narrow, stimulus-locked features with timing precision (standard deviation) $\\sigma$.\n*   Expected average firing rate: $r \\in [10, 40]\\,\\mathrm{Hz}$.\n*   Minimal fractional modulation of the temporal feature: $m \\approx 0.3$.\n*   Analysis method: Peri-Stimulus Time Histogram (PSTH) with bin width $\\Delta t$.\n*   Fundamental bases for analysis:\n    *   (i) Poisson counting statistics: variance equals mean.\n    *   (ii) Central Limit Theorem: Gaussian approximation for large counts.\n    *   (iii) Nyquist-Shannon sampling theorem analogy for resolving features.\n*   Detection threshold: $z$-score $z = 3$.\n*   Objective: Find a sufficient criterion on $\\Delta t$ and $T$ to discriminate between a rate code and a temporal code.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem is firmly rooted in computational neuroscience. The concepts of rate vs. temporal coding, modeling spike trains as Poisson processes, using PSTHs for analysis, and applying signal detection theory are all standard and fundamental to the field. The provided physical and statistical principles are appropriate for this context.\n2.  **Well-Posed:** The problem is well-posed. It asks for sufficient conditions on experimental parameters ($\\Delta t$, $T$) to achieve a clearly defined statistical goal (resolving and detecting a temporal feature with $z \\ge 3$). The provided information is sufficient to derive these conditions.\n3.  **Objective:** The problem is stated in precise, objective, and quantitative terms. All parameters ($W$, $r$, $m$, $z$) are defined numerically or as variables in a formal model.\n4.  **Incomplete or Contradictory Setup:** The setup is self-contained and consistent. The relationship $T = NW$ is defined. The statistical model (Poisson) and analysis framework are specified. There are no apparent contradictions.\n5.  **Unrealistic or Infeasible:** The numerical values provided ($W=100\\,\\mathrm{ms}$, $r \\in [10,40]\\,\\mathrm{Hz}$, $m \\approx 0.3$, $z=3$) are entirely realistic for neurophysiological experiments.\n6.  **Ill-Posed or Poorly Structured:** The problem structure is logical. It presents two hypotheses and asks for the conditions needed to distinguish them, which is a classic scientific task. The terms are well-defined within the context of neuroscience.\n7.  **Pseudo-Profound, Trivial, or Tautological:** The problem is non-trivial. It requires the synthesis of concepts from signal processing (Nyquist theorem), statistics (Poisson and Gaussian distributions, z-scores), and neuroscience (neural coding).\n8.  **Outside Scientific Verifiability:** The problem is entirely formalizable and solvable within the specified mathematical and scientific framework.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically sound, well-posed, objective, and contains sufficient information for a rigorous solution. We may proceed to the derivation.\n\n### Derivation of the Solution\n\nThe goal is to find sufficient conditions on the PSTH bin width $\\Delta t$ and the total recording time $T$ to reliably discriminate a temporal code from a rate code. This can be broken down into two requirements: (1) the temporal feature must be resolvable by the chosen bin width, and (2) the recording must be long enough for the feature to be statistically detectable.\n\n**1. Resolvability of the Temporal Feature**\n\nThe temporal code is characterized by narrow features with a timing precision of $\\sigma$. This $\\sigma$ can be interpreted as the characteristic time scale or duration of the feature (e.g., the standard deviation of a Gaussian-shaped peak in the firing rate). To resolve this feature in a histogram, the bin width $\\Delta t$ must be sufficiently small.\n\nThe problem invokes basis (iii), the Nyquist-Shannon sampling theorem. In this context, the binning process is analogous to sampling a continuous signal $r(t)$. A feature of duration/width $\\sigma$ contains significant frequency components up to approximately $f_{max} \\sim 1/\\sigma$. The Nyquist-Shannon theorem states that the sampling rate $f_s$ must be at least twice the maximum frequency, $f_s \\ge 2f_{max}$. Here, the sampling interval is $\\Delta t$, so the sampling rate is $f_s = 1/\\Delta t$.\n\nApplying the theorem:\n$$ \\frac{1}{\\Delta t} \\gtrsim 2 \\left( \\frac{1}{\\sigma} \\right) \\implies \\Delta t \\lesssim \\frac{\\sigma}{2} $$\nTherefore, a necessary condition for resolving the temporal feature is choosing a bin width no larger than half the feature's characteristic time scale.\n$$ \\Delta t \\le \\frac{\\sigma}{2} $$\n\n**2. Statistical Detectability of the Temporal Feature**\n\nWe need to determine the total recording time $T$ required to detect the feature with a z-score of at least $z=3$.\n\nLet the average baseline firing rate be $r$. A temporal feature is a modulation of this rate. A peak feature would have an instantaneous rate of $r_{feat}(t) > r$. The problem specifies a fractional modulation of $m$. We can model the rate at the center of the feature as $r_{peak} = r(1+m)$.\n\nThe PSTH accumulates spike counts across $N$ trials.\n*   **Expected count in a baseline bin:** The expected number of spikes in a bin of width $\\Delta t$ during one trial is $r \\Delta t$. Across $N$ trials, the expected count is $\\mu_{base} = N r \\Delta t$.\n*   **Expected count in a feature bin:** At the peak of the feature, the rate is $r(1+m)$. The expected count across $N$ trials in a bin centered on the feature is $\\mu_{feat} = N r(1+m) \\Delta t$.\n\nThe \"signal\" to be detected is the excess count in the feature bin compared to the baseline:\n$$ \\text{Signal} = S = \\mu_{feat} - \\mu_{base} = N r(1+m)\\Delta t - N r \\Delta t = N r m \\Delta t $$\n\nThe \"noise\" is the statistical fluctuation in the bin counts. According to basis (i), for a Poisson process, the variance of the count equals its mean. Using the baseline count for our noise estimate (as we are detecting a deviation from it), the variance is $\\text{Var}(\\text{count}) = \\mu_{base} = N r \\Delta t$. The standard deviation of the noise is therefore:\n$$ \\text{Noise} = \\sigma_{noise} = \\sqrt{\\mu_{base}} = \\sqrt{N r \\Delta t} $$\n(This is justified by basis (ii), the Central Limit Theorem, which states that for large $\\mu_{base}$, the Poisson distribution of counts is well-approximated by a Gaussian distribution with this mean and standard deviation).\n\nThe z-score is the signal-to-noise ratio:\n$$ z = \\frac{S}{\\sigma_{noise}} = \\frac{N r m \\Delta t}{\\sqrt{N r \\Delta t}} = m \\sqrt{N r \\Delta t} $$\n\nWe require this z-score to be greater than or equal to the given threshold, which we will call $z_{thresh}$ (the problem uses $z$ for this value, so we'll use $z$ as well).\n$$ m \\sqrt{N r \\Delta t} \\ge z $$\n\nTo find the required number of trials $N$, we square both sides and rearrange:\n$$ m^2 N r \\Delta t \\ge z^2 \\implies N \\ge \\frac{z^2}{m^2 r \\Delta t} $$\nThe problem asks for a condition on the total recording time $T$. Using the given relation $T = N W$, we substitute $N = T/W$:\n$$ \\frac{T}{W} \\ge \\frac{z^2}{m^2 r \\Delta t} $$\nFinally, solving for $T$ gives the condition for statistical detectability:\n$$ T \\ge \\frac{z^2}{m^2} \\frac{W}{r \\Delta t} $$\n\n**Combined Criterion**\n\nA sufficient criterion for discriminating the temporal code requires satisfying both the resolvability and detectability conditions:\n1.  $\\Delta t \\le \\sigma/2$\n2.  $T \\ge \\frac{z^2}{m^2} \\frac{W}{r \\Delta t}$\n\n### Option-by-Option Analysis\n\n**A. Choose $\\Delta t \\le \\sigma/2$ and ensure $T \\ge \\frac{z^2}{m^2}\\,\\frac{W}{r\\,\\Delta t}$.**\n*   The condition on $\\Delta t$ matches our derived resolvability criterion based on the Nyquist-Shannon principle.\n*   The condition on $T$ exactly matches our derived statistical detectability criterion for achieving the target z-score.\n*   This option provides a complete and sufficient set of conditions.\n*   **Verdict: Correct.**\n\n**B. Choose $\\Delta t = 1/r$ and ensure $T \\ge \\frac{z^2}{m^2}\\,r\\,\\Delta t\\,W$.**\n*   The condition $\\Delta t = 1/r$ relates the bin size to the average inter-spike interval. For $r \\in [10, 40]\\,\\mathrm{Hz}$, this yields $\\Delta t \\in [25, 100]\\,\\mathrm{ms}$. Fine temporal features in neural codes often have $\\sigma$ on the order of a few milliseconds. A bin width this large would average over and completely obscure such features, violating the resolvability requirement.\n*   The condition on $T$ is mathematically inconsistent with the required z-score under arbitrary valid parameters. As derived above, $T$ must be proportional to $1/(r \\Delta t)$, not $r \\Delta t W$.\n*   **Verdict: Incorrect.**\n\n**C. Choose $\\Delta t \\ge \\sigma$ and ensure $T \\ge \\frac{z^2}{m^2}\\,\\frac{1}{r}$.**\n*   The condition $\\Delta t \\ge \\sigma$ is the opposite of the resolvability requirement. Using a bin width larger than the feature itself makes it impossible to resolve the feature's shape and timing.\n*   The condition on $T$ is also incorrect. It lacks the dependence on $\\Delta t$ and $W$. As $\\Delta t$ is made smaller to better resolve a feature, the counts per bin decrease, which must be compensated by increasing the recording time $T$. This formula for $T$ does not account for this crucial trade-off.\n*   **Verdict: Incorrect.**\n\n**D. Choose $\\Delta t \\le \\sigma/2$ and ensure $T \\ge \\frac{z^2}{m^2}\\,\\frac{1}{r}$.**\n*   The condition on $\\Delta t$ is correct, satisfying the resolvability requirement.\n*   However, the condition on $T$ is the same incorrect formula as in option C. It does not provide sufficient statistical power. The required time $T$ must increase as $\\Delta t$ decreases to maintain a constant signal-to-noise ratio, a dependence captured by the $1/\\Delta t$ term in our derived formula. This option fails to ensure detectability for any valid choice of $\\Delta t$ where $\\Delta t \\ll W$.\n*   **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}