{
    "hands_on_practices": [
        {
            "introduction": "神经元或神经元群体在资源（如代谢能量或发放率）有限的情况下，应如何有效地编码多维刺激？这是一个核心问题。本练习将通过一个线性高斯通道模型，引导你推导出著名的“注水”原理（water-filling principle），这是一种最优的资源分配策略。通过这个练习，你将掌握将资源优先分配给信噪比较高通道的基本思想，为理解神经编码中的资源分配策略奠定数学基础。",
            "id": "3977210",
            "problem": "考虑一个 $n$ 维刺激 $S \\in \\mathbb{R}^{n}$，其被建模为一个零均值多变量高斯随机向量，协方差为 $\\Sigma_{S}$。一个线性编码器根据 $R = W S + N$ 产生神经响应 $R \\in \\mathbb{R}^{n}$，其中 $W$ 是一个对角编码矩阵，$N$ 是一个零均值多变量高斯噪声，其对角协方差为 $\\Sigma_{N}$，并且 $S$ 和 $N$ 是独立的。假设能量约束为 $\\mathbb{E}\\|R\\|_{2}^{2} \\leq C$。在大脑建模和计算神经科学的高等研究生学习层面，从互信息的基本定义 $I(S;R) = h(R) - h(R|S)$ 以及多变量高斯分布微分熵的成熟公式 $h(X) = \\frac{1}{2}\\ln\\!\\left((2\\pi e)^{n}\\det\\Sigma_{X}\\right)$ 出发，推导 $I(S;R)$ 的表达式，该表达式是编码器增益和协方差的函数。然后，使用拉格朗日乘数法和 Karush–Kuhn–Tucker (KKT) 条件，提出并求解在能量约束下最大化 $I(S;R)$ 的约束优化问题，证明最优分配遵循一种跨模式的注水原理。\n\n具体来说，令 $n=4$，令 $\\Sigma_{S} = \\mathrm{diag}(s_{1}^{2}, s_{2}^{2}, s_{3}^{2}, s_{4}^{2})$，其中 $(s_{1}^{2}, s_{2}^{2}, s_{3}^{2}, s_{4}^{2}) = (2, 1, 0.5, 0.25)$，令 $\\Sigma_{N} = \\mathrm{diag}(\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2}, \\sigma_{4}^{2})$，其中 $(\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2}, \\sigma_{4}^{2}) = (0.5, 1, 2, 3)$，并令 $W = \\mathrm{diag}(g_{1}, g_{2}, g_{3}, g_{4})$，其中增益为未知的非负数。能量预算为 $C = 9$，文中的对数均应取自然对数，因此信息量以奈特（nats）为单位。\n\n您的任务是：\n- 使用所述基本原理，推导 $I(S;R)$ 关于 $(g_{i}, s_{i}^{2}, \\sigma_{i}^{2})$ 的表达式。\n- 建立并求解在约束 $\\mathbb{E}\\|R\\|_{2}^{2} \\leq C$ 下最大化 $I(S;R)$ 的优化问题，使用拉格朗日乘数法和 Karush–Kuhn–Tucker (KKT) 条件，并写出跨模式最优分配的注水解（以注水水位参数表示）。\n- 使用给定的数值，计算注水分配下的最优互信息 $I^{\\star}$。\n\n将您最终报告的 $I^{\\star}$ 值四舍五入至四位有效数字，并以奈特（nats）为单位表示。最终答案必须是一个实数。",
            "solution": "用户提供了一个需要验证和解决的问题陈述。\n\n### 步骤 1：提取已知条件\n- 刺激：$S \\in \\mathbb{R}^{n}$，一个零均值多变量高斯随机向量，其协方差为 $\\Sigma_{S}$。\n- 编码器：$R = W S + N$，其中 $R \\in \\mathbb{R}^{n}$ 是神经响应。\n- 编码矩阵：$W$ 是一个对角矩阵，$W = \\mathrm{diag}(g_1, \\dots, g_n)$，其中 $g_i \\ge 0$。\n- 噪声：$N \\in \\mathbb{R}^{n}$，一个零均值多变量高斯随机向量，其对角协方差为 $\\Sigma_{N}$。\n- 独立性：$S$ 和 $N$ 是独立的。\n- 约束：响应的能量有界，$\\mathbb{E}\\|R\\|_{2}^{2} \\leq C$。\n- 互信息定义：$I(S;R) = h(R) - h(R|S)$。\n- 多变量高斯分布 $X$ 的微分熵：$h(X) = \\frac{1}{2}\\ln\\!\\left((2\\pi e)^{n}\\det\\Sigma_{X}\\right)$。\n- $n=4$ 时的数值：\n    - $\\Sigma_{S} = \\mathrm{diag}(s_{1}^{2}, s_{2}^{2}, s_{3}^{2}, s_{4}^{2})$，其中 $(s_{1}^{2}, s_{2}^{2}, s_{3}^{2}, s_{4}^{2}) = (2, 1, 0.5, 0.25)$。\n    - $\\Sigma_{N} = \\mathrm{diag}(\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2}, \\sigma_{4}^{2})$，其中 $(\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2}, \\sigma_{4}^{2}) = (0.5, 1, 2, 3)$。\n    - 能量预算：$C = 9$。\n- 对数：自然对数 ($\\ln$)，信息量以奈特（nats）为单位。\n- 任务：\n    1. 推导 $I(S;R)$ 关于 $(g_i, s_i^2, \\sigma_i^2)$ 的表达式。\n    2. 建立并求解在能量约束下最大化 $I(S;R)$ 的优化问题，使用拉格朗日乘数法和 KKT 条件找到注水解。\n    3. 对给定的数值计算最优互信息 $I^{\\star}$，并四舍五入到四位有效数字。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学依据**：该问题是理论神经科学和信息论中一个经典且基础的问题，旨在研究有效编码假说。线性高斯信道模型以及在功率约束下优化互信息是标准且成熟的方法。该问题在科学上是合理的。\n2.  **适定性**：该问题是适定的。它涉及在一个凸集上最大化一个严格凹函数，这保证了唯一解的存在。所有必要的信息和约束都已提供。\n3.  **客观性**：语言精确、数学化，并且没有主观论断。\n\n### 步骤 3：结论与行动\n问题有效。我将继续进行完整解答。\n\n### 互信息的推导\n刺激 $S$ 和响应 $R$ 之间的互信息由 $I(S;R) = h(R) - h(R|S)$ 给出。我们将分别计算每一项。\n\n首先，我们计算条件熵 $h(R|S)$。给定一个特定的刺激实现 $S=s$，响应为 $R = Ws + N$。由于 $Ws$ 是一个常数向量，而 $N$ 是一个协方差为 $\\Sigma_N$ 的零均值高斯随机向量，因此条件分布 $p(r|s)$ 是一个均值为 $Ws$、协方差为 $\\Sigma_N$ 的高斯分布。该条件分布的微分熵由高斯熵的公式给出：\n$$h(R|S=s) = \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det \\Sigma_N\\right)$$\n由于此表达式不依赖于 $s$ 的具体值，条件熵 $h(R|S)$ 是 $h(R|S=s)$ 对所有 $s$ 的期望，即表达式本身：\n$$h(R|S) = \\mathbb{E}_{S}[h(R|S=s)] = \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det \\Sigma_N\\right)$$\n\n接下来，我们计算响应的熵 $h(R)$。响应为 $R = WS + N$。由于 $S$ 和 $N$ 是独立的零均值高斯向量，它们的线性组合 $R$ 也是一个零均值高斯向量。我们需要求其协方差矩阵 $\\Sigma_R$。\n$$\\Sigma_R = \\mathbb{E}[RR^T] = \\mathbb{E}[(WS+N)(WS+N)^T] = \\mathbb{E}[WSS^TW^T + WSN^T + NS^TW^T + NN^T]$$\n由于 $S$ 和 $N$ 的独立性及其零均值特性，交叉项的期望为零：$\\mathbb{E}[WSN^T] = W\\mathbb{E}[S]\\mathbb{E}[N^T] = 0$ 且 $\\mathbb{E}[NS^TW^T] = \\mathbb{E}[N]\\mathbb{E}[S^T]W^T = 0$。\n因此，$R$ 的协方差是变换后的刺激协方差与噪声协方差之和：\n$$\\Sigma_R = W\\mathbb{E}[SS^T]W^T + \\mathbb{E}[NN^T] = W\\Sigma_S W^T + \\Sigma_N$$\n因此，$R$ 的熵为：\n$$h(R) = \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det \\Sigma_R\\right) = \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det(W\\Sigma_S W^T + \\Sigma_N)\\right)$$\n\n现在，我们合并各项来求互信息：\n$$I(S;R) = h(R) - h(R|S) = \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det(W\\Sigma_S W^T + \\Sigma_N)\\right) - \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det \\Sigma_N\\right)$$\n$$I(S;R) = \\frac{1}{2}\\left[\\ln(\\det(W\\Sigma_S W^T + \\Sigma_N)) - \\ln(\\det \\Sigma_N)\\right] = \\frac{1}{2}\\ln\\left(\\frac{\\det(W\\Sigma_S W^T + \\Sigma_N)}{\\det \\Sigma_N}\\right)$$\n由于矩阵 $W$、$\\Sigma_S$ 和 $\\Sigma_N$ 都是对角矩阵，其对角元素分别为 $g_i$、$s_i^2$ 和 $\\sigma_i^2$，因此它们的乘积和也是对角矩阵。对角矩阵的行列式是其对角元素的乘积。\n$$W\\Sigma_S W^T + \\Sigma_N = \\mathrm{diag}(g_1^2s_1^2 + \\sigma_1^2, \\dots, g_n^2s_n^2 + \\sigma_n^2)$$\n$$\\det(W\\Sigma_S W^T + \\Sigma_N) = \\prod_{i=1}^n (g_i^2s_i^2 + \\sigma_i^2)$$\n$$\\det \\Sigma_N = \\prod_{i=1}^n \\sigma_i^2$$\n将这些代入 $I(S;R)$ 的表达式中：\n$$I(S;R) = \\frac{1}{2}\\ln\\left(\\frac{\\prod_{i=1}^n (g_i^2s_i^2 + \\sigma_i^2)}{\\prod_{i=1}^n \\sigma_i^2}\\right) = \\frac{1}{2}\\sum_{i=1}^n \\ln\\left(\\frac{g_i^2s_i^2 + \\sigma_i^2}{\\sigma_i^2}\\right) = \\frac{1}{2}\\sum_{i=1}^n \\ln\\left(1 + \\frac{g_i^2s_i^2}{\\sigma_i^2}\\right)$$\n这就是所求的互信息表达式。\n\n### 优化问题与注水解\n我们的目标是在能量约束 $\\mathbb{E}\\|R\\|_{2}^{2} \\leq C$ 下最大化 $I(S;R)$。首先，我们用模型参数来表示这个约束。\n$$\\mathbb{E}\\|R\\|_2^2 = \\mathbb{E}\\left[\\sum_{i=1}^n R_i^2\\right] = \\sum_{i=1}^n \\mathbb{E}[R_i^2]$$\n对于每个分量，$R_i = g_i S_i + N_i$。$R_i$ 的方差是：\n$$\\mathbb{E}[R_i^2] = \\mathbb{E}[(g_i S_i + N_i)^2] = g_i^2 \\mathbb{E}[S_i^2] + 2g_i\\mathbb{E}[S_iN_i] + \\mathbb{E}[N_i^2]$$\n由于 $S_i$ 和 $N_i$ 是独立的零均值变量，$\\mathbb{E}[S_iN_i] = \\mathbb{E}[S_i]\\mathbb{E}[N_i]=0$。\n$$\\mathbb{E}[R_i^2] = g_i^2s_i^2 + \\sigma_i^2$$\n因此，总能量约束为 $\\sum_{i=1}^n (g_i^2s_i^2 + \\sigma_i^2) \\leq C$。\n\n令 $E_i = g_i^2s_i^2 + \\sigma_i^2$ 为第 $i$ 个响应分量的平均功率。约束为 $\\sum_{i=1}^n E_i \\leq C$。项 $g_i^2s_i^2$ 代表响应中的信号功率，必须为非负，因此 $E_i \\ge \\sigma_i^2$。\n互信息可以改写为 $E_i$ 的形式：\n$$I(S;R) = \\frac{1}{2}\\sum_{i=1}^n \\ln\\left(1 + \\frac{E_i - \\sigma_i^2}{\\sigma_i^2}\\right) = \\frac{1}{2}\\sum_{i=1}^n \\ln\\left(\\frac{E_i}{\\sigma_i^2}\\right) = \\frac{1}{2}\\left(\\sum_i \\ln(E_i) - \\sum_i \\ln(\\sigma_i^2)\\right)$$\n为了最大化 $I(S;R)$，我们可以等价地最大化 $\\sum_i \\ln(E_i)$，因为第二项是常数。优化问题是：\n$$ \\text{最大化} \\quad \\sum_{i=1}^n \\ln(E_i) $$\n$$ \\text{约束条件} \\quad \\sum_{i=1}^n E_i \\leq C \\quad \\text{且} \\quad E_i \\geq \\sigma_i^2 \\text{ 对所有 } i=1, \\dots, n. $$\n这是一个凸优化问题。我们通过最小化负目标函数来构造拉格朗日函数：\n$$\\mathcal{L}(\\{E_i\\}, \\lambda, \\{\\mu_i\\}) = -\\sum_{i=1}^n \\ln(E_i) + \\lambda\\left(\\sum_{i=1}^n E_i - C\\right) + \\sum_{i=1}^n \\mu_i(\\sigma_i^2 - E_i)$$\nKarush-Kuhn-Tucker (KKT) 条件是：\n1.  平稳性：对所有 $i$，$\\frac{\\partial \\mathcal{L}}{\\partial E_i} = -\\frac{1}{E_i} + \\lambda - \\mu_i = 0$。\n2.  原始可行性：$\\sum_i E_i \\leq C$ 且 $E_i \\ge \\sigma_i^2$。\n3.  对偶可行性：$\\lambda \\ge 0$ 且 $\\mu_i \\ge 0$。\n4.  互补松弛性：$\\lambda(\\sum_i E_i - C) = 0$ 且 $\\mu_i(\\sigma_i^2 - E_i) = 0$。\n\n根据关于 $\\mu_i$ 的互补松弛性，对每个信道 $i$ 有两种情况：\n- 情况1：$E_i > \\sigma_i^2$。这意味着 $\\mu_i = 0$。平稳性条件变为 $-\\frac{1}{E_i} + \\lambda = 0$，得出 $E_i = \\frac{1}{\\lambda}$。\n- 情况2：$E_i = \\sigma_i^2$。这意味着 $\\mu_i \\ge 0$。平稳性条件变为 $-\\frac{1}{\\sigma_i^2} + \\lambda = \\mu_i \\ge 0$，这意味着 $\\lambda \\ge \\frac{1}{\\sigma_i^2}$，或者 $\\frac{1}{\\lambda} \\le \\sigma_i^2$。\n\n我们定义一个“水位” $L = \\frac{1}{\\lambda}$。两种情况结合起来，给出每个响应分量的最优功率分配：\n$$E_i^\\star = \\max\\left(\\sigma_i^2, L\\right)$$\n预算约束 $\\sum_i E_i = C$ 是激活的（因为 $\\ln(E_i)$ 是单调递增的），所以 $\\lambda > 0$ 且 $L$ 是有限的。水位 $L$ 由总能量约束决定：\n$$\\sum_{i=1}^n \\max(\\sigma_i^2, L) = C$$\n这就是注水解。每个信道的总功率 $E_i$ 要么是其噪声基底 $\\sigma_i^2$（如果噪声高于水位），要么被填充到水位 $L$。\n\n### 数值计算\n给定 $n=4$, $C=9$, 以及噪声方差 $(\\sigma_1^2, \\sigma_2^2, \\sigma_3^2, \\sigma_4^2) = (0.5, 1, 2, 3)$。我们使用方程 $\\sum_{i=1}^4 \\max(\\sigma_i^2, L) = 9$ 来求解水位 $L$。我们通过猜测激活信道的数量（即 $L > \\sigma_i^2$ 的信道）来求解。\n我们将噪声方差排序：$\\sigma_1^2=0.5 < \\sigma_2^2=1 < \\sigma_3^2=2 < \\sigma_4^2=3$。\n\n1.  假设所有 4 个信道都激活（对所有 $i$，$E_i=L$）。那么 $4L = 9 \\implies L=2.25$。这个假设要求对所有 $i$，$L > \\sigma_i^2$。但 $L=2.25$ 并不大于 $\\sigma_4^2=3$。该假设错误。信道 4 未激活。\n\n2.  假设 3 个信道（1, 2, 3）激活，信道 4 未激活 ($E_4 = \\sigma_4^2$)。约束方程变为 $L+L+L+\\sigma_4^2 = 9$。\n    $$3L + 3 = 9 \\implies 3L = 6 \\implies L=2$$\n    我们检查一致性。对于激活信道 $i=1,2,3$，需要 $L \\ge \\sigma_i^2$。\n    - $i=1: L=2 \\ge \\sigma_1^2=0.5$ (真)\n    - $i=2: L=2 \\ge \\sigma_2^2=1$ (真)\n    - $i=3: L=2 \\ge \\sigma_3^2=2$ (真)\n    对于未激活信道 $i=4$，需要 $L \\le \\sigma_4^2$。\n    - $i=4: L=2 \\le \\sigma_4^2=3$ (真)\n    解是一致的。水位是 $L=2$。\n\n最优响应功率 $E_i^\\star$ 是：\n$E_1^\\star = \\max(0.5, 2) = 2$\n$E_2^\\star = \\max(1, 2) = 2$\n$E_3^\\star = \\max(2, 2) = 2$\n$E_4^\\star = \\max(3, 2) = 3$\n总和为 $2+2+2+3=9$，满足约束条件。\n\n最优信号功率 $P_i^\\star = g_i^2s_i^2 = E_i^\\star - \\sigma_i^2$ 是：\n$P_1^\\star = 2 - 0.5 = 1.5$\n$P_2^\\star = 2 - 1 = 1$\n$P_3^\\star = 2 - 2 = 0$\n$P_4^\\star = 3 - 3 = 0$\n\n最后，我们计算最大互信息 $I^\\star$：\n$$I^\\star = \\frac{1}{2}\\sum_{i=1}^4 \\ln\\left(1 + \\frac{P_i^\\star}{\\sigma_i^2}\\right)$$\n$$I^\\star = \\frac{1}{2}\\left[ \\ln\\left(1 + \\frac{1.5}{0.5}\\right) + \\ln\\left(1 + \\frac{1}{1}\\right) + \\ln\\left(1 + \\frac{0}{2}\\right) + \\ln\\left(1 + \\frac{0}{3}\\right) \\right]$$\n$$I^\\star = \\frac{1}{2}\\left[ \\ln(1 + 3) + \\ln(1 + 1) + \\ln(1) + \\ln(1) \\right]$$\n$$I^\\star = \\frac{1}{2}\\left[ \\ln(4) + \\ln(2) + 0 + 0 \\right]$$\n使用对数性质 $\\ln(a) + \\ln(b) = \\ln(ab)$：\n$$I^\\star = \\frac{1}{2}\\ln(4 \\times 2) = \\frac{1}{2}\\ln(8) = \\frac{1}{2}\\ln(2^3) = \\frac{3}{2}\\ln(2)$$\n现在我们计算数值：\n$$I^\\star = 1.5 \\times \\ln(2) \\approx 1.5 \\times 0.69314718 \\approx 1.03972077$$\n四舍五入到四位有效数字，我们得到 $1.040$。结果以奈特（nats）为单位。",
            "answer": "$$\\boxed{1.040}$$"
        },
        {
            "introduction": "在掌握了基本的资源分配原则之后，我们可以将其应用于更真实的场景：处理自然图像。这个练习将引导你将信息最大化原理应用于一个连续的二维感觉场，该场模拟了自然图像的统计特性。通过这个思想实验，你将看到初级视皮层（V1）中神经元典型的方向和空间频率调谐特性，是如何可以从自然场景的统计规律和噪声的存在中被预测出来的。",
            "id": "3977250",
            "problem": "考虑一个表示自然图像的二维、零均值、平稳高斯随机场。其在空间频率极坐标下的各向同性功率谱密度 (PSD) 由 $S(k) = A k^{-\\alpha}$ 给出，其中 $k \\geq 0$，常数 $A > 0$ 且 $\\alpha > 0$。编码过程由一个线性的、可分离的空间滤波器建模，其传递函数可分解为一个径向（空间频率）包络和一个方向包络的乘积，$H(k,\\theta) = h(k)\\,g(\\theta)$，其中 $h(k) \\geq 0$ 且 $g(\\theta)$ 是 $2\\pi$ 周期的。在滤波器输出端，编码被加性、方向无关的白高斯噪声所污染，其功率谱密度为 $N_{0} > 0$。\n\n假设输出功率预算受以下约束\n$$\n\\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\,|H(k,\\theta)|^{2} \\;=\\; P,\n$$\n其中 $P > 0$ 为固定值。对方向包络施加归一化\n$$\n\\int_{0}^{2\\pi} g(\\theta)^{2}\\,\\mathrm{d}\\theta \\;=\\; 1.\n$$\n\n以线性高斯信道的互信息作为在允许的 $H(k,\\theta)$ 上最大化的目标函数，从第一性原理出发，推导方向包络 $g^{\\star}(\\theta)$ 的最优选择，以及由最优径向包络 $h^{\\star}(k)$ 过渡到零的截止空间频率 $k_{c}$ 定义的最优空间频率调谐。将 $g^{\\star}(\\theta)$ 和 $k_{c}$ 表示为 $\\alpha$、$A$、$N_{0}$ 和 $P$ 的闭式函数。你的最终答案应以包含 $g^{\\star}(\\theta)$ 和 $k_{c}$ 的一个二元行矩阵形式给出。无需四舍五入，也无需单位。",
            "solution": "该问题要求推导最优滤波器传递函数 $H(k,\\theta)$，该函数在滤波器功率约束下，最大化输入信号与滤波器输出之间的互信息。传递函数是可分离的，即 $H(k,\\theta) = h(k)g(\\theta)$。\n\n输入是一个零均值平稳高斯随机场，其各向同性功率谱密度 (PSD) 为 $S(k) = A k^{-\\alpha}$。在滤波器输出端，编码被功率谱密度为常数 $N_{0}$ 的加性白高斯噪声所污染。因此，在给定空间频率 $(k, \\theta)$ 处的信噪比 (SNR) 为：\n$$\n\\text{SNR}(k, \\theta) = \\frac{|H(k,\\theta)|^{2} S(k)}{N_{0}} = \\frac{h(k)^{2} g(\\theta)^{2} A k^{-\\alpha}}{N_{0}}\n$$\n这里我们利用了 $|H(k,\\theta)|^2 = (h(k)g(\\theta))^2 = h(k)^2 g(\\theta)^2$ 这一事实，因为在不失一般性的前提下，可以选择 $h(k)$ 和 $g(\\theta)$ 为实数且非负。问题陈述中说明了 $h(k) \\ge 0$。我们也可以选择 $g(\\theta)$ 为实数。\n\n线性高斯信道的总互信息 $I$ 是信息速率密度在所有独立频率信道上的积分。在二维极坐标下，这由以下公式给出：\n$$\nI[h, g] = \\frac{1}{2} \\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\, \\ln \\left(1 + \\text{SNR}(k, \\theta) \\right)\n$$\n代入信噪比的表达式，我们得到：\n$$\nI[h, g] = \\frac{1}{2} \\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\, \\ln \\left(1 + \\frac{h(k)^{2} g(\\theta)^{2} S(k)}{N_{0}} \\right)\n$$\n因子 $\\frac{1}{2}$ 和自然对数 $\\ln$ 的使用意味着信息以奈特 (nats) 为单位。最大化此表达式等同于最大化没有 $\\frac{1}{2}$ 前置因子的表达式。\n\n该优化受两个约束条件限制：\n1.  输出功率预算（更准确地说，是对滤波器 L2 范数平方的约束）：\n    $$\n    \\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\,|H(k,\\theta)|^{2} = \\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\, h(k)^{2} g(\\theta)^{2} = P\n    $$\n2.  方向包络的归一化：\n    $$\n    \\int_{0}^{2\\pi} g(\\theta)^{2}\\,\\mathrm{d}\\theta = 1\n    $$\n\n我们可以利用 $g(\\theta)$ 的归一化来简化功率约束：\n$$\nP = \\int_{0}^{\\infty} k h(k)^{2} \\left( \\int_{0}^{2\\pi} g(\\theta)^{2} \\,\\mathrm{d}\\theta \\right) \\mathrm{d}k = \\int_{0}^{\\infty} k h(k)^{2} \\cdot 1 \\, \\mathrm{d}k = \\int_{0}^{\\infty} k h(k)^{2} \\,\\mathrm{d}k\n$$\n这个简化的约束只涉及径向包络 $h(k)$。\n\n这个优化问题可以分两步解决。首先，对于任意固定的 $h(k)$，我们对 $g(\\theta)$ 进行优化，然后我们再对 $h(k)$ 进行优化。\n\n第一步：优化方向包络 $g(\\theta)$。\n我们来考察互信息积分的结构。我们可以分离积分：\n$$\nI[h, g] = \\frac{1}{2} \\int_{0}^{\\infty} k \\left( \\int_{0}^{2\\pi} \\ln \\left(1 + \\frac{h(k)^{2} S(k)}{N_{0}} g(\\theta)^{2} \\right) \\mathrm{d}\\theta \\right) \\mathrm{d}k\n$$\n为了最大化 $I$，对于每一个 $k$ 值，我们必须最大化关于 $\\theta$ 的内层积分：\n$$\nJ[g] = \\int_{0}^{2\\pi} \\ln \\left(1 + C(k) g(\\theta)^{2} \\right) \\mathrm{d}\\theta\n$$\n其约束条件为 $\\int_{0}^{2\\pi} g(\\theta)^{2}\\,\\mathrm{d}\\theta = 1$。这里，$C(k) = \\frac{h(k)^{2} S(k)}{N_{0}}$ 是一个相对于 $\\theta$ 的正常数。\n\n我们使用变分法。我们构建拉格朗日泛函：\n$$\n\\mathcal{L}[g] = \\int_{0}^{2\\pi} \\left[ \\ln \\left(1 + C(k) g(\\theta)^{2} \\right) - \\mu g(\\theta)^{2} \\right] \\mathrm{d}\\theta\n$$\n其中 $\\mu$ 是一个拉格朗日乘子。欧拉-拉格朗日方程 $\\frac{\\delta \\mathcal{L}}{\\delta g} = 0$ 得到：\n$$\n\\frac{2 C(k) g(\\theta)}{1 + C(k) g(\\theta)^{2}} - 2 \\mu g(\\theta) = 0\n$$\n$$\ng(\\theta) \\left( \\frac{C(k)}{1 + C(k) g(\\theta)^{2}} - \\mu \\right) = 0\n$$\n这意味着要么 $g(\\theta) = 0$（一个违反约束条件的平凡解），要么 $\\frac{C(k)}{1 + C(k) g(\\theta)^{2}} = \\mu$。\n求解 $g(\\theta)^{2}$：\n$$\nC(k) = \\mu (1 + C(k) g(\\theta)^{2}) \\implies g(\\theta)^{2} = \\frac{1}{C(k)} \\left(\\frac{C(k)}{\\mu} - 1\\right) = \\frac{1}{\\mu} - \\frac{1}{C(k)}\n$$\n因为这对所有 $\\theta$ 都必须成立，所以 $g(\\theta)^{2}$ 必须是一个常数。这是一个直观的结果，因为信号功率谱密度 $S(k)$ 和噪声功率谱密度 $N_{0}$ 都是各向同性的（与 $\\theta$ 无关），所以没有理由偏好任何特定的方向。\n设 $g(\\theta)^{2} = G^{2}$，其中 $G$ 是一个常数。应用归一化约束：\n$$\n\\int_{0}^{2\\pi} G^{2}\\,\\mathrm{d}\\theta = 1 \\implies 2\\pi G^{2} = 1 \\implies G^{2} = \\frac{1}{2\\pi}\n$$\n因此，最优方向包络 $g^{\\star}(\\theta)$ 是一个常数。选择正实根，我们有：\n$$\ng^{\\star}(\\theta) = \\frac{1}{\\sqrt{2\\pi}}\n$$\n\n第二步：优化径向包络 $h(k)$。\n将 $g^{\\star}(\\theta)$ 代入互信息表达式：\n$$\nI[h] = \\frac{1}{2} \\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\, \\ln \\left(1 + \\frac{h(k)^{2} S(k)}{2\\pi N_{0}} \\right)\n$$\n被积函数现在与 $\\theta$ 无关，所以我们可以计算内层积分：\n$$\nI[h] = \\pi \\int_{0}^{\\infty} k \\ln \\left(1 + \\frac{h(k)^{2} S(k)}{2\\pi N_{0}} \\right) \\mathrm{d}k\n$$\n我们想在约束条件 $P = \\int_{0}^{\\infty} k h(k)^{2} \\,\\mathrm{d}k$ 下最大化这个泛函。\n让我们定义 $f(k) = h(k)^{2}$，因此 $f(k) \\geq 0$。问题是在 $\\int_0^\\infty k f(k) dk = P$ 的约束下最大化 $\\pi \\int_0^\\infty k \\ln\\left(1 + \\frac{f(k) S(k)}{2\\pi N_0}\\right) dk$。我们构建拉格朗日函数：\n$$\n\\mathcal{L}[f] = \\int_{0}^{\\infty} \\left[ \\pi k \\ln \\left(1 + \\frac{f(k) S(k)}{2\\pi N_{0}} \\right) - \\lambda k f(k) \\right] \\mathrm{d}k + \\lambda P\n$$\n其中 $\\lambda$ 是一个拉格朗日乘子。对 $f(k)$ 求泛函导数并令其为零，得到：\n$$\n\\frac{\\delta \\mathcal{L}}{\\delta f(k)} = \\pi k \\left( \\frac{1}{1 + \\frac{f(k) S(k)}{2\\pi N_{0}}} \\cdot \\frac{S(k)}{2\\pi N_{0}} \\right) - \\lambda k = 0\n$$\n对于 $k > 0$，我们可以除以 $k$：\n$$\n\\frac{S(k)}{2N_{0}} \\frac{1}{1 + \\frac{f(k) S(k)}{2\\pi N_{0}}} = \\lambda\n$$\n求解 $f(k) = h(k)^{2}$：\n$$\n\\frac{S(k)}{2N_{0}} = \\lambda \\left(1 + \\frac{f(k) S(k)}{2\\pi N_{0}}\\right) \\implies \\frac{f(k) S(k)}{2\\pi N_{0}} = \\frac{S(k)}{2\\lambda N_{0}} - 1\n$$\n$$\nf(k) = h(k)^{2} = \\frac{2\\pi N_{0}}{S(k)} \\left( \\frac{S(k)}{2\\lambda N_{0}} - 1 \\right) = \\frac{\\pi}{\\lambda} - \\frac{2\\pi N_{0}}{S(k)}\n$$\n由于 $h(k)^{2}$ 必须是非负的，最优滤波器增益由一个“注水”解给出：\n$$\nh^{\\star}(k)^{2} = \\max \\left(0, \\frac{\\pi}{\\lambda} - \\frac{2\\pi N_{0}}{S(k)} \\right)\n$$\n滤波器增益仅在频率 $k$ 满足 $\\frac{\\pi}{\\lambda} - \\frac{2\\pi N_{0}}{S(k)} > 0$ 时非零，这等价于 $S(k) > 2 \\lambda N_{0}$。\n设 $k_{c}$ 是增益过渡到零的截止空间频率。在此频率下，$S(k_{c}) = 2 \\lambda N_{0}$。\n代入 $S(k) = A k^{-\\alpha}$，我们得到 $A k_{c}^{-\\alpha} = 2 \\lambda N_{0}$。\n由于 $S(k)$ 是 $k$ 的递减函数（对于 $\\alpha > 0$），条件 $S(k) > S(k_c)$ 在 $k < k_{c}$ 时满足。因此，最优滤波器仅在 $k \\in [0, k_{c}]$ 时非零。\n我们可以将 $k \\leq k_{c}$ 时的 $h^{\\star}(k)^{2}$ 表示为：\n$$\nh^{\\star}(k)^{2} = \\frac{2\\pi N_{0}}{S(k_c)} - \\frac{2\\pi N_{0}}{S(k)} = 2\\pi N_{0} \\left( \\frac{1}{A k_{c}^{-\\alpha}} - \\frac{1}{A k^{-\\alpha}} \\right) = \\frac{2\\pi N_{0}}{A} \\left(k_{c}^{\\alpha} - k^{\\alpha}\\right)\n$$\n为了找到 $k_{c}$，我们使用功率约束：\n$$\nP = \\int_{0}^{\\infty} k h^{\\star}(k)^{2} \\mathrm{d}k = \\int_{0}^{k_{c}} k \\left[ \\frac{2\\pi N_{0}}{A} (k_{c}^{\\alpha} - k^{\\alpha}) \\right] \\mathrm{d}k\n$$\n$$\nP = \\frac{2\\pi N_{0}}{A} \\int_{0}^{k_{c}} (k_{c}^{\\alpha} k - k^{1+\\alpha}) \\mathrm{d}k\n$$\n计算积分：\n$$\nP = \\frac{2\\pi N_{0}}{A} \\left[ k_{c}^{\\alpha} \\frac{k^{2}}{2} - \\frac{k^{2+\\alpha}}{2+\\alpha} \\right]_{0}^{k_{c}} = \\frac{2\\pi N_{0}}{A} \\left( \\frac{k_{c}^{\\alpha+2}}{2} - \\frac{k_{c}^{\\alpha+2}}{2+\\alpha} \\right)\n$$\n$$\nP = \\frac{2\\pi N_{0}}{A} k_{c}^{2+\\alpha} \\left( \\frac{1}{2} - \\frac{1}{2+\\alpha} \\right) = \\frac{2\\pi N_{0}}{A} k_{c}^{2+\\alpha} \\left( \\frac{2+\\alpha-2}{2(2+\\alpha)} \\right)\n$$\n$$\nP = \\frac{2\\pi N_{0}}{A} k_{c}^{2+\\alpha} \\frac{\\alpha}{2(2+\\alpha)} = \\frac{\\pi N_{0} \\alpha}{A(2+\\alpha)} k_{c}^{2+\\alpha}\n$$\n最后，我们求解截止频率 $k_{c}$：\n$$\nk_{c}^{2+\\alpha} = \\frac{A P (2+\\alpha)}{\\pi N_{0} \\alpha}\n$$\n$$\nk_{c} = \\left( \\frac{A P (2+\\alpha)}{\\pi N_{0} \\alpha} \\right)^{\\frac{1}{2+\\alpha}}\n$$\n所需的两个量是最优方向包络 $g^{\\star}(\\theta)$ 和最优截止空间频率 $k_{c}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\sqrt{2\\pi}} & \\left( \\frac{A P (2+\\alpha)}{\\pi N_{0} \\alpha} \\right)^{\\frac{1}{2+\\alpha}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "除了对输入信号进行最优滤波之外，高效编码假说还引出了一种更精妙的神经计算策略：预测编码。该理论认为，大脑通过建立内部模型来预测即将到来的感觉输入，并主要对预测误差（即信号中出乎意料的部分）进行编码。本练习将指导你设计一个包含前馈和反馈通路的简单神经回路来实现这一原理，并量化其最终的信息吞吐量，从而深入理解大脑是如何通过动态预测来提升编码效率的。",
            "id": "3977263",
            "problem": "您必须设计一个包含前馈和反馈通路的、数学上明确定义的神经架构，该架构在高效编码假说（Efficient Coding Hypothesis）下实现预测误差编码。该架构必须是线性和高斯的，并且信息吞吐量必须使用第一性原理进行计算。您的程序必须为所提供的测试套件计算信息吞吐量，并完全按照指定格式输出结果。\n\n该架构作用于一个潜在刺激向量 $s \\in \\mathbb{R}^{n_s}$ 和一个观测到的感觉输入向量 $x \\in \\mathbb{R}^{n_x}$，其中 $n_x \\ge 1$ 且 $n_s \\ge 1$。环境根据一个多元正态分布和一个带有高斯感觉噪声的线性生成映射来生成刺激。该网络在前馈通路中使用预测误差单元，在反馈通路中使用预测单元，以实现对潜在刺激的最大后验（MAP）推断。向前传递的表征是在加性神经读出噪声下的预测误差。\n\n您必须使用以下核心假设和设计元素：\n\n- 潜在刺激 $s$ 从一个均值为零、协方差为对称正定矩阵 $C_s \\in \\mathbb{R}^{n_s \\times n_s}$ 的多元正态分布中抽取。\n- 感觉输入生成为 $x = W s + n_x$，其中 $W \\in \\mathbb{R}^{n_x \\times n_s}$ 是一个固定的生成矩阵，而 $n_x \\sim \\mathcal{N}(0, \\sigma_x^2 I_{n_x})$，其中 $\\sigma_x^2 > 0$ 且 $I_{n_x}$ 是维度为 $n_x$ 的单位矩阵。\n- 网络从内部表征 $\\hat{s}$ 计算出一个预测 $\\hat{x} = W \\hat{s}$，并在前馈通路中形成预测误差 $e = x - \\hat{x}$。反馈传递预测值 $W \\hat{s}$，从 $x$ 中减去该值以产生 $e$。\n- 内部表征 $\\hat{s}$ 是一个与高斯先验和高斯似然一致的二次能量函数的最小化子，并在稳态下计算。\n- 向前传递的编码是误差 $r = e + n_e$，其中 $n_e \\sim \\mathcal{N}(0, \\sigma_e^2 I_{n_x})$，$\\sigma_e^2 > 0$ 且与所有其他变量无关。\n\n您的任务是：\n\n1. 仅使用基础的概率原理和线性代数，从上述假设中推导出稳态估计量 $\\hat{s}$，不引入任何额外假设。\n2. 确定从 $x$ 到 $r$ 的线性映射，并用 $W$、$C_s$ 和 $\\sigma_x^2$ 来表示它。\n3. 对于一个线性高斯信道，计算定义为 $x$ 和 $r$ 之间互信息（MI）的信息吞吐量（单位：奈特），使用由环境引出的 $x$ 的真实分布以及误差编码中的独立高斯读出噪声。\n4. 实现一个程序，对于每个测试用例，计算吞吐量，结果为一个浮点数（单位：奈特），并四舍五入到六位小数。\n\n您的程序必须以数值稳定的方式为通用的 $n_x$ 和 $n_s$ 实现以下步骤：\n\n- 构建 $\\hat{s}$ 的稳态估计量。\n- 计算从 $x$ 到 $r$ 的线性响应映射。\n- 计算由环境引出的 $x$ 的协方差。\n- 计算指定信道下 $x$ 和 $r$ 之间的互信息。\n- 将所有结果汇总到所需的单行输出格式中。\n\n测试套件：\n- 情况1（理想路径，相关的潜在变量和非对角矩阵 $W$）：\n  - $n_x = 2$, $n_s = 2$\n  - $W = \\begin{bmatrix} 1.0 & 0.4 \\\\ 0.6 & 1.2 \\end{bmatrix}$\n  - $C_s = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.2 & 0.8 \\end{bmatrix}$\n  - $\\sigma_x^2 = 0.1$\n  - $\\sigma_e^2 = 0.05$\n\n- 情况2（边界情况，单位生成模型）：\n  - $n_x = 2$, $n_s = 2$\n  - $W = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$\n  - $C_s = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$\n  - $\\sigma_x^2 = 0.01$\n  - $\\sigma_e^2 = 0.1$\n\n- 情况3（边缘情况，极高的读出噪声）：\n  - $n_x = 2$, $n_s = 2$\n  - $W = \\begin{bmatrix} 1.0 & 0.4 \\\\ 0.6 & 1.2 \\end{bmatrix}$\n  - $C_s = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.2 & 0.8 \\end{bmatrix}$\n  - $\\sigma_x^2 = 0.1$\n  - $\\sigma_e^2 = 100.0$\n\n- 情况4（覆盖性测试，超完备的潜在维度）：\n  - $n_x = 2$, $n_s = 3$\n  - $W = \\begin{bmatrix} 1.0 & 0.0 & 0.5 \\\\ 0.0 & 1.0 & 0.5 \\end{bmatrix}$\n  - $C_s = \\begin{bmatrix} 1.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 \\\\ 0.0 & 0.0 & 10.0 \\end{bmatrix}$\n  - $\\sigma_x^2 = 0.5$\n  - $\\sigma_e^2 = 0.2$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，“[result1,result2,result3,result4]”）。\n- 每个结果必须是相应测试用例的互信息（单位：奈特），以浮点数形式四舍五入到六位小数。",
            "solution": "用户希望我解决一个基于高效编码假说的问题。我需要：\n1.  严谨地验证问题陈述。\n2.  如果问题有效，推导数学解法。\n3.  用 Python 实现该解法，以解决一系列测试用例。\n\n### 问题验证\n\n#### 步骤1：提取已知条件\n- **潜在刺激**：$s \\in \\mathbb{R}^{n_s}$，其中 $n_s \\ge 1$。分布为 $s \\sim \\mathcal{N}(0, C_s)$，其中 $C_s \\in \\mathbb{R}^{n_s \\times n_s}$ 是一个对称正定协方差矩阵。\n- **感觉输入**：$x \\in \\mathbb{R}^{n_x}$，其中 $n_x \\ge 1$。\n- **生成模型**：$x = W s + n_x$，其中 $W \\in \\mathbb{R}^{n_x \\times n_s}$ 是一个固定的生成矩阵。\n- **感觉噪声**：$n_x \\sim \\mathcal{N}(0, \\sigma_x^2 I_{n_x})$，其中 $\\sigma_x^2 > 0$ 且 $I_{n_x}$ 是 $n_x \\times n_x$ 的单位矩阵。\n- **预测**：$\\hat{x} = W \\hat{s}$。\n- **预测误差**：$e = x - \\hat{x}$。\n- **内部表征**：$\\hat{s}$ 是在给定 $x$ 的情况下对 $s$ 的最大后验（MAP）估计。这等价于最小化一个与指定的高斯先验和似然一致的二次能量函数。计算在稳态下进行。\n- **传递的编码**：$r = e + n_e$。\n- **读出噪声**：$n_e \\sim \\mathcal{N}(0, \\sigma_e^2 I_{n_x})$，其中 $\\sigma_e^2 > 0$ 且 $n_e$ 与所有其他变量无关。\n- **任务**：计算信息吞吐量，定义为互信息 $I(x; r)$，单位为奈特。\n- **测试套件**：提供了四组具体的参数（$n_x, n_s, W, C_s, \\sigma_x^2, \\sigma_e^2$）。\n\n#### 步骤2：使用提取的已知条件进行验证\n- **科学依据**：该问题牢固地植根于计算神经科学，特别是预测编码理论和高效编码假说。它对线性高斯系统使用了贝叶斯推断（MAP 估计）和信息论（互信息）的标准原理。该模型是一个成熟的形式化模型（例如，卡尔曼滤波器系列模型，稀疏编码模型）。该框架在科学上是合理的。\n- **适定性**：该问题是适定的。使用高斯先验和似然会得到唯一的 MAP 估计。协方差矩阵（$C_s$ 是正定的）和噪声方差（$\\sigma_x^2 > 0, \\sigma_e^2 > 0$）的条件确保了所有需要的矩阵逆和行列式都是良定义且为正的，从而为互信息提供了一个唯一、稳定且有意义的解。\n- **客观性**：问题以精确的数学语言陈述。所有术语都有正式定义，目标是定量的。没有主观或模棱两可的语言。\n- **完整性和一致性**：问题是自洽的。所有必要的变量、分布和关系都已定义。所提供的信息中没有矛盾之处。\n\n#### 步骤3：结论与行动\n问题是有效的。这是一个定义明确的统计信号处理理论练习，应用于神经编码模型。我将继续进行详细的推导和求解。\n\n### 解法推导\n\n求解需要分步推导信息吞吐量 $I(x; r)$。这通过首先找到最优的内部表征 $\\hat{s}$，然后描述从感觉输入 $x$ 到传递的编码 $r$ 的处理信道，最后应用线性高斯信道中互信息的公式来完成。\n\n#### 1. MAP 估计量 $\\hat{s}$ 的推导\n内部表征 $\\hat{s}$ 是在给定感觉输入 $x$ 的情况下，对潜在刺激 $s$ 的最大后验（MAP）估计。根据贝叶斯定理，后验概率为：\n$$p(s|x) \\propto p(x|s) p(s)$$\n问题指定了似然 $p(x|s)$ 和先验 $p(s)$ 的分布：\n- 先验分布为 $s \\sim \\mathcal{N}(0, C_s)$，因此其概率密度函数为 $p(s) = \\frac{1}{\\sqrt{(2\\pi)^{n_s}\\det(C_s)}} \\exp\\left(-\\frac{1}{2}s^T C_s^{-1} s\\right)$。\n- 生成模型 $x = Ws + n_x$ 且 $n_x \\sim \\mathcal{N}(0, \\sigma_x^2 I_{n_x})$ 意味着给定 $s$ 时 $x$ 的似然为 $x|s \\sim \\mathcal{N}(Ws, \\sigma_x^2 I_{n_x})$。其密度为 $p(x|s) = \\frac{1}{\\sqrt{(2\\pi\\sigma_x^2)^{n_x}}} \\exp\\left(-\\frac{1}{2\\sigma_x^2}(x-Ws)^T(x-Ws)\\right)$。\n\n为了找到 MAP 估计，我们最大化对数后验概率，这等价于最小化其负数（一个能量函数），忽略常数项：\n$$E(s) = -\\log p(s|x) = \\frac{1}{2\\sigma_x^2}(x-Ws)^T(x-Ws) + \\frac{1}{2}s^T C_s^{-1} s + \\text{const.}$$\n我们通过将 $E(s)$ 对 $s$ 的梯度设为零来找到最小值：\n$$\\nabla_s E(s) = \\frac{1}{2\\sigma_x^2} \\nabla_s(s^T W^T W s - 2x^T W s) + \\frac{1}{2} \\nabla_s(s^T C_s^{-1} s) = 0$$\n使用标准的矩阵微积分恒等式，我们得到：\n$$\\nabla_s E(s) = \\frac{1}{\\sigma_x^2}(W^T W s - W^T x) + C_s^{-1} s = 0$$\n求解 $s$（即我们的估计量 $\\hat{s}$）：\n$$\\left(\\frac{1}{\\sigma_x^2}W^T W + C_s^{-1}\\right) \\hat{s} = \\frac{1}{\\sigma_x^2} W^T x$$\n$$\\hat{s} = \\left(W^T W + \\sigma_x^2 C_s^{-1}\\right)^{-1} W^T x$$\n这表明 $\\hat{s}$ 是 $x$ 的一个线性函数。\n\n#### 2. 从感觉输入 $x$ 到表征 $r$ 的线性映射\n传递的编码 $r$ 定义为 $r = e + n_e$，其中 $e = x - \\hat{x} = x - W\\hat{s}$。通过代入 $\\hat{s}$ 的表达式，我们可以将 $e$ 表示为 $x$ 的线性变换：\n$$e = x - W \\left( (W^T W + \\sigma_x^2 C_s^{-1})^{-1} W^T \\right) x$$\n$$e = \\left( I_{n_x} - W (W^T W + \\sigma_x^2 C_s^{-1})^{-1} W^T \\right) x$$\n让我们将线性算子 $L_e \\in \\mathbb{R}^{n_x \\times n_x}$ 定义为：\n$$L_e = I_{n_x} - W (W^T W + \\sigma_x^2 C_s^{-1})^{-1} W^T$$\n从 $x$到 $r$ 的完整信道则由下式给出：\n$$r = L_e x + n_e$$\n由于 $x$ 是一个高斯随机变量（作为高斯变量 $s$ 和 $n_x$ 的线性变换），而 $n_e$ 是一个独立的高斯噪声，这描述了一个线性高斯通信信道。\n\n#### 3. 信息吞吐量计算\n信息吞吐量是互信息 $I(x; r)$。对于连续变量，它由熵的差值给出：\n$$I(x; r) = H(r) - H(r|x)$$\n多元高斯变量 $Z \\sim \\mathcal{N}(\\mu, \\Sigma)$ 的熵为 $H(Z) = \\frac{1}{2}\\log\\left(\\det(2\\pi e \\Sigma)\\right)$。\n\n首先，我们找到在给定 $x$ 的条件下 $r$ 的分布。由 $r = L_e x + n_e$ 和 $n_e \\sim \\mathcal{N}(0, \\sigma_e^2 I_{n_x})$，我们有：\n$$r|x \\sim \\mathcal{N}(L_e x, \\sigma_e^2 I_{n_x})$$\n这个条件分布的协方差 $\\Sigma_{r|x}$ 是 $\\sigma_e^2 I_{n_x}$。条件熵 $H(r|x)$ 对所有 $x$ 都是常数：\n$$H(r|x) = \\frac{1}{2}\\log\\left(\\det(2\\pi e \\sigma_e^2 I_{n_x})\\right)$$\n\n接下来，我们找到 $r$ 的边际分布。由于 $E[s]=0$ 和 $E[n_x]=0$，我们有 $E[x] = E[Ws+n_x] = 0$。这意味着 $E[r] = E[L_e x + n_e] = 0$。$r$ 的协方差 $\\Sigma_r$ 是：\n$$\\Sigma_r = \\text{Cov}(r) = E[rr^T] = E[(L_e x + n_e)(L_e x + n_e)^T]$$\n由于 $x$（依赖于 $s$ 和 $n_x$）和 $n_e$ 是独立的，交叉项消失：\n$$\\Sigma_r = L_e E[xx^T] L_e^T + E[n_e n_e^T] = L_e \\Sigma_x L_e^T + \\Sigma_{n_e}$$\n这里，$\\Sigma_x$ 是输入 $x$ 的协方差，$\\Sigma_{n_e} = \\sigma_e^2 I_{n_x}$。我们求 $\\Sigma_x$：\n$$\\Sigma_x = E[xx^T] = E[(Ws + n_x)(Ws + n_x)^T]$$\n由于 $s$ 和 $n_x$ 是独立的：\n$$\\Sigma_x = W E[ss^T] W^T + E[n_x n_x^T] = W C_s W^T + \\sigma_x^2 I_{n_x}$$\n因此， $r$ 的协方差是：\n$$\\Sigma_r = L_e (W C_s W^T + \\sigma_x^2 I_{n_x}) L_e^T + \\sigma_e^2 I_{n_x}$$\n$r$ 的熵是 $H(r) = \\frac{1}{2}\\log\\left(\\det(2\\pi e \\Sigma_r)\\right)$。\n\n现在，我们计算互信息：\n$$I(x; r) = H(r) - H(r|x) = \\frac{1}{2}\\log\\left(\\det(2\\pi e \\Sigma_r)\\right) - \\frac{1}{2}\\log\\left(\\det(2\\pi e \\sigma_e^2 I_{n_x})\\right)$$\n$$I(x; r) = \\frac{1}{2} \\left( \\log(\\det(\\Sigma_r)) - \\log(\\det(\\sigma_e^2 I_{n_x})) \\right) = \\frac{1}{2} \\log\\left(\\frac{\\det(\\Sigma_r)}{\\det(\\sigma_e^2 I_{n_x})}\\right)$$\n$$I(x; r) = \\frac{1}{2} \\log\\left(\\det\\left(\\Sigma_r (\\sigma_e^2 I_{n_x})^{-1}\\right)\\right) = \\frac{1}{2} \\log\\left(\\det\\left(\\frac{1}{\\sigma_e^2}\\Sigma_r\\right)\\right)$$\n代入 $\\Sigma_r$ 的表达式：\n$$I(x; r) = \\frac{1}{2} \\log\\left(\\det\\left(\\frac{1}{\\sigma_e^2} (L_e \\Sigma_x L_e^T + \\sigma_e^2 I_{n_x}) \\right)\\right)$$\n$$I(x; r) = \\frac{1}{2} \\log\\left(\\det\\left(I_{n_x} + \\frac{1}{\\sigma_e^2} L_e \\Sigma_x L_e^T \\right)\\right)$$\n这就是信息吞吐量的最终表达式。矩阵 $I_{n_x} + \\frac{1}{\\sigma_e^2} L_e \\Sigma_x L_e^T$ 是正定的，所以它的行列式是正的。此外，行列式大于或等于1，确保了互信息是非负的。\n\n#### 4. 计算步骤总结\n1.  给定 $W, C_s, \\sigma_x^2, \\sigma_e^2, n_x, n_s$。\n2.  计算 $C_s^{-1}$。\n3.  计算 $L_e = I_{n_x} - W (W^T W + \\sigma_x^2 C_s^{-1})^{-1} W^T$。\n4.  计算 $\\Sigma_x = W C_s W^T + \\sigma_x^2 I_{n_x}$。\n5.  构造矩阵 $B = I_{n_x} + \\frac{1}{\\sigma_e^2} L_e \\Sigma_x L_e^T$。\n6.  计算互信息为 $I(x; r) = \\frac{1}{2} \\log(\\det(B))$。为了数值稳定性，这通过 `0.5 * numpy.linalg.slogdet(B)[1]` 来计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    test_cases = [\n        {\n            \"nx\": 2, \"ns\": 2,\n            \"W\": [[1.0, 0.4], [0.6, 1.2]],\n            \"Cs\": [[1.0, 0.2], [0.2, 0.8]],\n            \"sigma_x_sq\": 0.1,\n            \"sigma_e_sq\": 0.05\n        },\n        {\n            \"nx\": 2, \"ns\": 2,\n            \"W\": [[1.0, 0.0], [0.0, 1.0]],\n            \"Cs\": [[1.0, 0.0], [0.0, 1.0]],\n            \"sigma_x_sq\": 0.01,\n            \"sigma_e_sq\": 0.1\n        },\n        {\n            \"nx\": 2, \"ns\": 2,\n            \"W\": [[1.0, 0.4], [0.6, 1.2]],\n            \"Cs\": [[1.0, 0.2], [0.2, 0.8]],\n            \"sigma_x_sq\": 0.1,\n            \"sigma_e_sq\": 100.0\n        },\n        {\n            \"nx\": 2, \"ns\": 3,\n            \"W\": [[1.0, 0.0, 0.5], [0.0, 1.0, 0.5]],\n            \"Cs\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 10.0]],\n            \"sigma_x_sq\": 0.5,\n            \"sigma_e_sq\": 0.2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_throughput(\n            case[\"nx\"], case[\"ns\"], case[\"W\"], case[\"Cs\"],\n            case[\"sigma_x_sq\"], case[\"sigma_e_sq\"]\n        )\n        results.append(result)\n\n    # Format the final output string.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef compute_throughput(nx, ns, W_list, Cs_list, sigma_x_sq, sigma_e_sq):\n    \"\"\"\n    Computes the information throughput for a single test case.\n\n    Args:\n        nx (int): Dimension of sensory input x.\n        ns (int): Dimension of latent stimulus s.\n        W_list (list): The generative matrix W.\n        Cs_list (list): The stimulus covariance matrix Cs.\n        sigma_x_sq (float): Variance of sensory noise.\n        sigma_e_sq (float): Variance of readout noise.\n\n    Returns:\n        float: The mutual information I(x; r) in nats.\n    \"\"\"\n    # Convert inputs to NumPy arrays for matrix operations.\n    W = np.array(W_list, dtype=np.float64)\n    Cs = np.array(Cs_list, dtype=np.float64)\n    I_nx = np.identity(nx, dtype=np.float64)\n    \n    # 1. Compute the inverse of the stimulus covariance matrix Cs.\n    # Cs is guaranteed to be symmetric positive definite.\n    try:\n        Cs_inv = np.linalg.inv(Cs)\n    except np.linalg.LinAlgError:\n        # Should not be reached given problem constraints.\n        return np.nan\n\n    # 2. Compute the linear operator Le that maps x to the prediction error e.\n    # Le = I_nx - W * (W.T * W + sigma_x_sq * Cs_inv)^-1 * W.T\n    # The term to be inverted is guaranteed to be positive definite.\n    precision_matrix_s = W.T @ W + sigma_x_sq * Cs_inv\n    try:\n        s_hat_mapper_from_Wtx = np.linalg.inv(precision_matrix_s)\n    except np.linalg.LinAlgError:\n        # Should not be reached.\n        return np.nan\n        \n    Le = I_nx - W @ s_hat_mapper_from_Wtx @ W.T\n    \n    # 3. Compute the covariance matrix of the sensory input x.\n    # Sigma_x = W * Cs * W.T + sigma_x_sq * I_nx\n    Sigma_x = W @ Cs @ W.T + sigma_x_sq * I_nx\n    \n    # 4. Compute the final mutual information I(x; r).\n    # I(x; r) = 0.5 * log(det(I_nx + (1/sigma_e_sq) * Le * Sigma_x * Le.T))\n    \n    # Form the matrix B for which we need the log-determinant.\n    matrix_B = I_nx + (1.0 / sigma_e_sq) * (Le @ Sigma_x @ Le.T)\n    \n    # Use slogdet for numerical stability. It returns (sign, log(abs(det))).\n    # For a positive definite matrix, sign must be 1.\n    sign, log_det_B = np.linalg.slogdet(matrix_B)\n    \n    # The theory guarantees det(B) >= 1, so log_det_B >= 0.\n    # Any negative result is due to numerical precision errors and should be clamped to 0.\n    if sign = 0 or log_det_B  0:\n        mi = 0.0\n    else:\n        mi = 0.5 * log_det_B\n        \n    return mi\n\n# Run the solver.\nsolve()\n```"
        }
    ]
}