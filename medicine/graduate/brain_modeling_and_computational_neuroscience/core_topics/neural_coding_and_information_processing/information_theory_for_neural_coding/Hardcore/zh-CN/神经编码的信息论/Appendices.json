{
    "hands_on_practices": [
        {
            "introduction": "我们从一个基本但极具启发性的场景开始，该场景构成了神经编码中信息论应用的核心。通过分析一个简化的线性神经元模型，其中高斯分布的信号受到独立高斯噪声的干扰，我们可以推导出刺激与响应之间互信息的精确解析表达式 。这个练习不仅巩固了微分熵的关键概念，还揭示了信噪比，即信号方差与噪声方差之比，如何从根本上决定一个连续通道的信息传输能力。",
            "id": "5037464",
            "problem": "在一个简化的单个感觉神经元的神经编码模型中，一个标量刺激 $S$ 表示围绕基线的偏差，并被建模为一个均值为零、方差为 $\\sigma_{S}^{2}$ 的高斯随机变量，即 $S \\sim \\mathcal{N}(0,\\sigma_{S}^{2})$。神经元的响应 $R$ 被假设是线性的，并受到独立加性噪声 $N \\sim \\mathcal{N}(0,\\sigma_{N}^{2})$ 的干扰，其中 $S$ 和 $N$ 独立，因此 $R = S + N$。以用微分熵表示的互信息定义和高斯随机变量的成熟性质为基本出发点，推导该模型中刺激与响应之间互信息 $I(S;R)$ 的一个闭式表达式。使用自然对数，以便信息以奈特（nats）为单位进行度量。用 $\\sigma_{S}^{2}$ 和 $\\sigma_{N}^{2}$ 符号化地表示您的最终答案，并且不要近似。最终答案必须是单个闭式解析表达式。",
            "solution": "该问题是有效的，因为它在信息论和统计学方面有科学依据，问题提法恰当，具有唯一且有意义的解，并使用客观、正式的语言进行陈述。所有必要的信息都已提供，并且没有矛盾或含糊之处。\n\n目标是推导刺激 $S$ 和神经响应 $R$ 之间互信息 $I(S;R)$ 的闭式表达式。该模型由以下给定条件定义：\n1.  刺激 $S$ 是一个均值为零、方差为 $\\sigma_{S}^{2}$ 的高斯随机变量，记为 $S \\sim \\mathcal{N}(0, \\sigma_{S}^{2})$。\n2.  响应 $R$ 由线性关系 $R = S + N$ 给出。\n3.  噪声 $N$ 是一个独立的、均值为零、方差为 $\\sigma_{N}^{2}$ 的高斯随机变量，记为 $N \\sim \\mathcal{N}(0, \\sigma_{N}^{2})$。\n4.  刺激 $S$ 和噪声 $N$ 是统计独立的。\n\n我们从用微分熵表示的互信息定义开始。对于连续随机变量 $S$ 和 $R$，互信息由下式给出：\n$$\nI(S;R) = H(R) - H(R|S)\n$$\n其中 $H(R)$ 是响应 $R$ 的微分熵，$H(R|S)$ 是在给定 $S$ 的条件下 $R$ 的条件微分熵。我们将分别计算这些项。\n\n首先，我们确定响应 $R$ 的分布。由于 $R$ 是两个独立高斯随机变量 $S$ 和 $N$ 的和，因此 $R$ 本身也是一个高斯随机变量。\n$R$ 的均值是 $S$ 和 $N$ 的均值之和：\n$$\nE[R] = E[S+N] = E[S] + E[N] = 0 + 0 = 0\n$$\n由于它们的独立性，$R$ 的方差是 $S$ 和 $N$ 的方差之和：\n$$\n\\text{Var}(R) = \\text{Var}(S+N) = \\text{Var}(S) + \\text{Var}(N) = \\sigma_{S}^{2} + \\sigma_{N}^{2}\n$$\n因此，响应的分布为 $R \\sim \\mathcal{N}(0, \\sigma_{S}^{2} + \\sigma_{N}^{2})$。\n\n对于高斯随机变量 $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$，当使用自然对数（以奈特为单位）时，其微分熵 $H(X)$ 由以下公式给出：\n$$\nH(X) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)\n$$\n将此公式应用于响应变量 $R$，我们得到其熵：\n$$\nH(R) = \\frac{1}{2} \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right)\n$$\n\n接下来，我们计算条件熵 $H(R|S)$。该项表示在已知 $S$ 的值时 $R$ 中剩余的不确定性。如果我们以一个特定的刺激值 $S=s$ 为条件，响应方程变为 $R = s + N$。在这种情况下，由于 $s$ 是一个固定值，$R$ 的随机性完全来自噪声 $N$。给定 $S=s$ 时 $R$ 的分布是一个高斯分布，其均值为 $E[R|S=s] = E[s+N] = s + E[N] = s$，方差为 $\\text{Var}(R|S=s) = \\text{Var}(s+N) = \\text{Var}(N) = \\sigma_{N}^{2}$。因此，$R|S=s \\sim \\mathcal{N}(s, \\sigma_{N}^{2})$。\n\n这个条件分布的熵是：\n$$\nH(R|S=s) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\n注意，这个表达式是一个常数，不依赖于 $s$ 的具体值。条件熵 $H(R|S)$ 是 $H(R|S=s)$ 在 $S$ 的所有可能值上的期望：\n$$\nH(R|S) = E_S[H(R|S=s)] = E_S\\left[\\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\\right]\n$$\n由于期望内的表达式相对于 $S$ 是一个常数，所以期望就是这个常数本身：\n$$\nH(R|S) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\n\n最后，我们将 $H(R)$ 和 $H(R|S)$ 的表达式代入互信息的定义中：\n$$\nI(S;R) = H(R) - H(R|S) = \\frac{1}{2} \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right) - \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\n使用对数性质 $\\ln(a) - \\ln(b) = \\ln(a/b)$，我们可以简化该表达式：\n$$\nI(S;R) = \\frac{1}{2} \\left[ \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right) - \\ln(2\\pi e \\sigma_{N}^{2}) \\right] = \\frac{1}{2} \\ln\\left(\\frac{2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})}{2\\pi e \\sigma_{N}^{2}}\\right)\n$$\n分子和分母中的项 $2\\pi e$ 可以消掉：\n$$\nI(S;R) = \\frac{1}{2} \\ln\\left(\\frac{\\sigma_{S}^{2} + \\sigma_{N}^{2}}{\\sigma_{N}^{2}}\\right)\n$$\n这可以通过分离分数写成更标准的形式：\n$$\nI(S;R) = \\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}\\right)\n$$\n项 $\\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}$ 是系统的信噪比（SNR）。这个结果是信息论中高斯信道的一个基本公式。",
            "answer": "$$\n\\boxed{\\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}\\right)}\n$$"
        },
        {
            "introduction": "在理解了噪声如何影响整体信息后，我们进一步探究神经变异性的统计特性如何塑造信息编码。本练习聚焦于一个核心的生物物理量——法诺因子（Fano factor），它量化了神经元发放计数的试次间变异程度 。通过在一个大发放率的近似框架下进行推导，您将亲身体会到，在平均发放率固定的情况下，响应的变异性（即噪声熵 $H(R|S)$）是如何直接依赖于其统计分布的，并理解为何亚泊松（sub-Poisson）发放等低变异性编码策略对提升信息保真度至关重要。",
            "id": "3990303",
            "problem": "考虑在单个刺激条件 $S$ 下，在一个持续时间为 $T$ 的固定计数窗口内观察到的一个神经元。设脉冲计数为一个离散随机变量 $R \\in \\{0, 1, 2, \\dots\\}$，其条件均值为 $\\mathbb{E}[R \\mid S] = m$，其中 $m$ 很大。\n\n使用自然对数，根据标准信息论定义来定义条件熵 $H(R \\mid S)$。\n\n假设在固定均值 $m$ 下有两种脉冲计数模型：\n- 泊松模型，其中试次间的变异性满足法诺因子（方差与均值之比）等于 $1$ 的性质。\n- 亚泊松模型，其中法诺因子 $F$ 满足 $0 < F < 1$，并且条件方差与均值成线性关系，$\\operatorname{Var}(R \\mid S) = F m$。\n\n从熵的核心定义和适用于大 $m$ 值的经过充分检验的渐近推理出发（例如，援引中心极限定理（CLT）来证明脉冲计数分布的平滑单峰近似是合理的，并在适当之处使用连续近似），推导在固定均值 $m$ 的情况下，当试次间的变异性从亚泊松情况增加到泊松情况时，引起的条件熵变化的领头阶解析表达式。具体来说，在上述大 $m$ 渐近条件下，推导出\n$$\n\\Delta H \\equiv H_{\\text{Poisson}}(R \\mid S) - H_{\\text{sub-Poisson},F}(R \\mid S)\n$$\n作为法诺因子 $F$ 的函数的显式闭式表达式。\n\n在整个计算过程中使用自然对数，并以奈特（nats）为单位报告 $\\Delta H$。你的最终答案必须是仅含 $F$ 的单个闭式解析表达式（在领头阶答案中不应再有对 $m$ 的依赖），且不进行四舍五入。",
            "solution": "问题要求计算在刺激 $S$ 下，神经元脉冲计数 $R$ 的条件熵变化量 $\\Delta H \\equiv H_{\\text{Poisson}}(R \\mid S) - H_{\\text{sub-Poisson},F}(R \\mid S)$。平均脉冲计数为 $\\mathbb{E}[R \\mid S] = m$，并假设 $m$ 很大。\n\n对于一个概率质量函数为 $p(k) = P(R=k \\mid S)$ 的离散随机变量 $R$，其条件熵定义为：\n$$\nH(R \\mid S) = -\\sum_{k=0}^{\\infty} p(k) \\ln(p(k))\n$$\n问题指出，对于一个大的平均计数值 $m$，我们可以使用渐近近似。由于脉冲计数 $R$ 可以看作是许多较小事件的总和，中心极限定理（CLT）证明了用一个连续的单峰分布，特别是高斯（正态）分布，来近似其离散概率质量函数 $p(k)$ 是合理的。\n\n设 $X$ 是一个近似离散随机变量 $R$ 的连续随机变量。这个高斯分布的参数将与 $R$ 的矩相匹配。均值为 $\\mu = \\mathbb{E}[R \\mid S] = m$，方差为 $\\sigma^2 = \\operatorname{Var}(R \\mid S)$。$X$ 的概率密度函数（PDF）由下式给出：\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-m)^2}{2\\sigma^2}\\right)\n$$\n对于一个均值很大且可以很好地由连续概率密度函数 $f(x)$ 近似的离散整值随机变量，其熵可以由该连续变量的微分熵来近似。这是因为离散熵定义中的求和可以用积分来近似：\n$$\nH(R \\mid S) = -\\sum_{k=0}^{\\infty} p(k) \\ln(p(k)) \\approx -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) dx\n$$\n右边的积分是连续随机变量 $X$ 的微分熵 $H(X)$ 的定义。在大 $m$ 极限下，该近似提供了熵的领头阶项。\n\n方差为 $\\sigma^2$ 的高斯分布的微分熵是信息论中的一个标准结果，以奈特（nats）为单位表示为：\n$$\nH_{\\text{Gaussian}}(X) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)\n$$\n我们将此公式应用于问题中描述的两种情况。\n\n情况 1：泊松模型\n在泊松模型中，方差等于均值。法诺因子为 $1$。\n$$\n\\operatorname{Var}_{\\text{Poisson}}(R \\mid S) = m\n$$\n因此，我们用一个均值为 $m$、方差为 $\\sigma_{\\text{Poisson}}^2 = m$ 的高斯分布来近似脉冲计数分布。其熵为：\n$$\nH_{\\text{Poisson}}(R \\mid S) \\approx \\frac{1}{2}\\ln(2\\pi e m)\n$$\n\n情况 2：亚泊松模型\n在此模型中，法诺因子为 $F$，其中 $0 < F < 1$。问题指明其方差为：\n$$\n\\operatorname{Var}_{\\text{sub-Poisson},F}(R \\mid S) = F m\n$$\n我们用一个均值为 $m$、方差为 $\\sigma_{\\text{sub-Poisson}}^2 = F m$ 的高斯分布来近似该分布。其熵为：\n$$\nH_{\\text{sub-Poisson},F}(R \\mid S) \\approx \\frac{1}{2}\\ln(2\\pi e F m)\n$$\n\n现在，我们通过计算这两个渐近表达式的差值来计算条件熵的变化量 $\\Delta H$。\n$$\n\\Delta H = H_{\\text{Poisson}}(R \\mid S) - H_{\\text{sub-Poisson},F}(R \\mid S)\n$$\n代入推导出的表达式：\n$$\n\\Delta H \\approx \\frac{1}{2}\\ln(2\\pi e m) - \\frac{1}{2}\\ln(2\\pi e F m)\n$$\n利用对数性质 $\\ln(a) - \\ln(b) = \\ln(a/b)$，我们可以简化表达式：\n$$\n\\Delta H \\approx \\frac{1}{2} \\left[ \\ln(2\\pi e m) - \\ln(2\\pi e F m) \\right] = \\frac{1}{2} \\ln\\left(\\frac{2\\pi e m}{2\\pi e F m}\\right)\n$$\n分数中的项 $2$、$\\pi$、$e$ 和 $m$ 相互抵消：\n$$\n\\Delta H \\approx \\frac{1}{2} \\ln\\left(\\frac{1}{F}\\right)\n$$\n利用另一个对数性质 $\\ln(1/F) = -\\ln(F)$，我们得到熵变化的领头阶最终表达式：\n$$\n\\Delta H = -\\frac{1}{2}\\ln(F)\n$$\n该表达式如要求一样，仅依赖于法诺因子 $F$。由于 $0 < F < 1$，$\\ln(F)$ 为负，因此 $\\Delta H$ 为正。这在科学上是一致的，因为将变异性从亚泊松水平（$F < 1$）增加到泊松水平（$F=1$）应该会增加脉冲计数的不确定性（熵）。",
            "answer": "$$\n\\boxed{-\\frac{1}{2}\\ln(F)}\n$$"
        },
        {
            "introduction": "理论推导为我们提供了深刻的见解，但在实际的神经科学研究中，我们面对的是有限且离散的实验数据。这个实践练习将带您从解析理论迈向计算实践，要求您为离散的神经响应数据编写程序来估计互信息 。您将实现一个“代入式”（plug-in）估计器，并运用非参数自助法（bootstrap）来量化估计的不确定性，从而为您的信息分析结果构建置信区间，这是任何严谨的数据分析中不可或缺的一步。",
            "id": "5037495",
            "problem": "给定一组刺激的离散化神经响应，其中响应已被分箱到整数类别中。设刺激为一个取 $K$ 个值的离散随机变量 $S$，分箱后的神经响应为一个取 $B$ 个值的离散随机变量 $R$。每个刺激 $s$ 呈现 $T$ 次试验，每次试验你观察到一个分箱响应值 $r \\in \\{0,1,\\dots,B-1\\}$。从经验联合概率分布、经验边缘分布、香农熵和条件熵的核心定义出发，从这些经验频率中推导出一个原则性的互信息 $I(S;R)$（单位为比特）的置换估计器（plug-in estimator）。实现一个程序，该程序：\n- 使用从经验数据中推导出的置换估计器，计算 $I(S;R)$ 的点估计值（单位为比特）。\n- 应用非参数自举（bootstrap）程序，该程序在每个刺激 $s$ 内独立地进行有放回的试验重采样（每个刺激保留 $T$ 次试验），以近似 $I(S;R)$ 的置换估计器的抽样分布。\n- 使用自举分布，基于百分位数截断点计算一个名义覆盖率为 $95$ 百分比的双侧置信区间。\n\n你的程序必须使用以 $2$ 为底的对数，以确保信息以比特为单位表示。自举过程必须使用恰好 $B_{\\text{boot}}=400$ 次重采样，并且必须使用种子 $12345$ 初始化一个确定性伪随机数生成器，以确保结果可复现。为了数值稳定性，在计算任何形式为 $p \\log_{2}\\!\\big(p/q\\big)$ 的表达式时，当 $p=0$ 时，其贡献定义为 $0$。\n\n你的程序必须解决以下测试套件。在每种情况下，响应取值于 $\\{0,1,2,3\\}$，即 $B=4$，并且在同一种情况下，每个刺激的试验次数 $T$ 相同。对于每种情况，你都会得到每个刺激的分箱响应列表：\n\n- 情况 A（一般性、信息丰富的响应）：$K=3$, $T=20$,\n  - 刺激 $s=0$：$[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2,2]$。\n  - 刺激 $s=1$：$[2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,1,1]$。\n  - 刺激 $s=2$：$[0,0,0,0,0,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3]$。\n- 情况 B（几乎无法区分的响应）：$K=2$, $T=30$,\n  - 刺激 $s=0$：$[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3]$。\n  - 刺激 $s=1$：$[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3]$。\n- 情况 C（小样本和边界计数）：$K=4$, $T=10$,\n  - 刺激 $s=0$：$[0,0,0,0,0,0,0,0,0,0]$。\n  - 刺激 $s=1$：$[1,1,1,1,1,1,1,1,1,1]$。\n  - 刺激 $s=2$：$[0,0,0,0,0,1,1,1,1,1]$。\n  - 刺激 $s=3$：$[2,2,2,2,2,3,3,3,3,3]$。\n\n你的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于每种情况，按此顺序报告三个浮点数：$I(S;R)$ 的自举百分位数下界、点估计值和自举百分位数上界（单位为比特）。所有数字必须四舍五入到六位小数。因此，输出格式必须为 $[L_{A},\\hat{I}_{A},U_{A},L_{B},\\hat{I}_{B},U_{B},L_{C},\\hat{I}_{C},U_{C}]$，其中 $L_{\\cdot}$ 和 $U_{\\cdot}$ 表示 $95$ 百分比置信区间的下界和上界，$\\hat{I}_{\\cdot}$ 表示置换点估计值，所有单位均为比特。",
            "solution": "问题陈述已经过严格验证，被认为是**有效的**。它在信息论和统计学方面有科学依据，问题提出得当，提供了所有必要的数据和约束条件，并以客观、精确的语言表述。它代表了计算神经科学中一个标准而有意义的任务。因此，我们可以着手解决。\n\n解决方案包括三个主要部分：\n1.  互信息置换估计器 $\\hat{I}(S;R)$ 的推导。\n2.  用于估计 $\\hat{I}(S;R)$ 抽样分布的非参数自举程序的描述。\n3.  从自举分布构建基于百分位数的置信区间。\n\n### 1. 互信息置换估计器的推导\n\n互信息 $I(S;R)$ 量化了两个随机变量——刺激 $S$ 和响应 $R$ 之间的统计依赖性。它衡量了通过观察一个变量而减少的关于另一个变量的不确定性。它是根据香农熵定义的：\n\n$$\nI(S;R) = H(R) - H(R|S)\n$$\n\n其中所有信息论量均以比特为单位度量，这意味着使用以 $2$ 为底的对数。\n\n-   **响应的香农熵**：$H(R)$ 是响应的边缘分布的熵，它衡量关于神经响应的总不确定性，而与刺激无关。对于一个可以取 $B$ 个值 $\\{r_0, ..., r_{B-1}\\}$ 的离散变量 $R$，其定义为：\n    $$\n    H(R) = - \\sum_{j=0}^{B-1} p(r_j) \\log_2 p(r_j)\n    $$\n-   **给定刺激下响应的条件熵**：$H(R|S)$ 是响应熵在所有刺激上的平均值。它衡量在已知刺激的情况下，关于响应的剩余不确定性。\n    $$\n    H(R|S) = \\sum_{i=0}^{K-1} p(s_i) H(R|S=s_i)\n    $$\n    其中 $p(s_i)$ 是刺激 $s_i$ 的概率，而 $H(R|S=s_i)$ 是该特定刺激的响应分布的熵：\n    $$\n    H(R|S=s_i) = - \\sum_{j=0}^{B-1} p(r_j|s_i) \\log_2 p(r_j|s_i)\n    $$\n\n为了从经验数据中计算 $I(S;R)$，我们使用**置换法**（plug-in method），即用基于频率的估计值替换真实的概率。\n\n设实验数据包含 $K$ 个刺激，每个刺激 $s_i$ ($i \\in \\{0, \\dots, K-1\\}$) 呈现 $T$ 次试验。神经响应被分箱到 $B$ 个类别之一，$r_j$ ($j \\in \\{0, \\dots, B-1\\}$)。总试验次数为 $N = K \\times T$。\n\n设 $N_{ij}$ 为呈现刺激 $s_i$ 时观察到响应 $r_j$ 的经验计数。\n经验概率估计如下：\n-   **刺激概率**：问题陈述每个刺激都呈现相同次数的试验。因此，任何给定刺激 $s_i$ 的概率是均匀的：\n    $$\n    \\hat{p}(s_i) = \\frac{T}{N} = \\frac{T}{KT} = \\frac{1}{K}\n    $$\n-   **条件响应概率**：给定刺激 $s_i$ 时响应 $r_j$ 的概率是该刺激下该响应的频率：\n    $$\n    \\hat{p}(r_j|s_i) = \\frac{N_{ij}}{T}\n    $$\n-   **边缘响应概率**：使用全概率定律，响应 $r_j$ 的边缘概率为：\n    $$\n    \\hat{p}(r_j) = \\sum_{i=0}^{K-1} \\hat{p}(s_i) \\hat{p}(r_j|s_i) = \\sum_{i=0}^{K-1} \\frac{1}{K} \\frac{N_{ij}}{T} = \\frac{\\sum_{i=0}^{K-1} N_{ij}}{KT}\n    $$\n\n将这些估计值代入熵公式，得到置换估计器。我们采用约定，如果 $p=0$，则 $p \\log_2 p = 0$。\n\n-   **$H(R)$ 的估计器**：\n    $$\n    \\hat{H}(R) = - \\sum_{j=0}^{B-1} \\hat{p}(r_j) \\log_2 \\hat{p}(r_j) = - \\sum_{j=0}^{B-1} \\left( \\frac{\\sum_{i=0}^{K-1} N_{ij}}{KT} \\right) \\log_2 \\left( \\frac{\\sum_{i=0}^{K-1} N_{ij}}{KT} \\right)\n    $$\n-   **$H(R|S)$ 的估计器**：\n    首先，我们估计每个刺激 $s_i$ 的条件熵：\n    $$\n    \\hat{H}(R|S=s_i) = - \\sum_{j=0}^{B-1} \\hat{p}(r_j|s_i) \\log_2 \\hat{p}(r_j|s_i) = - \\sum_{j=0}^{B-1} \\left( \\frac{N_{ij}}{T} \\right) \\log_2 \\left( \\frac{N_{ij}}{T} \\right)\n    $$\n    然后，我们对所有刺激的这些值进行平均，权重为 $\\hat{p}(s_i) = 1/K$：\n    $$\n    \\hat{H}(R|S) = \\sum_{i=0}^{K-1} \\frac{1}{K} \\hat{H}(R|S=s_i) = \\frac{1}{K} \\sum_{i=0}^{K-1} \\left[ - \\sum_{j=0}^{B-1} \\frac{N_{ij}}{T} \\log_2 \\left( \\frac{N_{ij}}{T} \\right) \\right]\n    $$\n结合这些，互信息的置换估计器为：\n$$\n\\hat{I}(S;R) = \\hat{H}(R) - \\hat{H}(R|S)\n$$\n\n### 2. 用于抽样分布的自举程序\n\n置换估计值 $\\hat{I}(S;R)$ 是从我们的特定数据集中计算出的单个值。作为一个统计量，它会受到抽样变异性的影响。为了量化此估计中的不确定性，我们使用非参数自举程序。该程序通过重复重采样观测数据来近似估计器 $\\hat{I}(S;R)$ 的抽样分布。\n\n程序如下：\n1.  设置自举重采样次数，$B_{\\text{boot}} = 400$。\n2.  使用固定种子（$12345$）初始化伪随机数生成器，以保证可复现性。\n3.  对于每次自举迭代 $b = 1, \\dots, B_{\\text{boot}}$：\n    a.  创建一个新的、重采样的数据集。问题指定在*每个刺激内部独立地*重采样试验。这意味着对于每个刺激 $s_i$，我们通过从该刺激的原始 $T$ 个观测响应中有放回地抽样，生成一组新的 $T$ 次试验。\n    b.  使用这个重采样的数据集，计算计数矩阵 $N_{ij}^*$，然后使用上面推导的相同置换估计器公式，计算互信息的自举副本 $\\hat{I}^*(S;R)_b$。\n4.  这 $B_{\\text{boot}}$ 个值的集合 $\\{\\hat{I}^*(S;R)_1, \\dots, \\hat{I}^*(S;R)_{B_{\\text{boot}}}\\}$ 构成了 $\\hat{I}(S;R)$ 抽样分布的经验近似。\n\n### 3. 置信区间构建\n\n使用自举分布，我们可以为真实的互信息构建一个置信区间。问题指定了百分位数法。对于一个名义覆盖率为 $95\\%$ 的双侧置信区间，我们找到包含自举分布中心 $95\\%$ 的值。\n\n1.  将自举副本按升序排序：$\\hat{I}^*_{(1)} \\leq \\hat{I}^*_{(2)} \\leq \\dots \\leq \\hat{I}^*_{(B_{\\text{boot}})}$。\n2.  置信区间的下界 $L$ 是此分布的第 $2.5$ 百分位数。\n3.  置信区间的上界 $U$ 是此分布的第 $97.5$ 百分位数。\n4.  最终得到的 $95\\%$ 置信区间是 $[L, U]$。\n\n最终的程序将为每个测试用例实现这些步骤，报告下界 $L$、来自原始数据的点估计值 $\\hat{I}(S;R)$ 和上界 $U$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_mi_plugin(data, B):\n    \"\"\"\n    Computes the plug-in estimate of mutual information I(S;R) in bits.\n\n    Args:\n        data (list of lists): A list where each inner list contains the\n                              binned responses for a single stimulus.\n        B (int): The number of response bins.\n\n    Returns:\n        float: The mutual information estimate in bits.\n    \"\"\"\n    K = len(data)\n    if K == 0:\n        return 0.0\n    T = len(data[0])\n    N = K * T\n    if N == 0:\n        return 0.0\n\n    # 1. Compute the count matrix N_sr of size (K, B)\n    # N_sr[s, r] = count of response r for stimulus s\n    N_sr = np.zeros((K, B), dtype=int)\n    for s_idx, stimulus_responses in enumerate(data):\n        for r_val in stimulus_responses:\n            N_sr[s_idx, r_val] += 1\n\n    # 2. Compute probability distributions\n    # p(r|s) = N_sr / T\n    # Note: Division by zero is not an issue here as T > 0 by problem structure.\n    p_r_given_s = N_sr / T\n\n    # p(r) = sum_s(N_sr) / N\n    N_r = np.sum(N_sr, axis=0)\n    p_r = N_r / N\n\n    # 3. Compute entropy H(R)\n    # H(R) = - sum_r p(r) log2(p(r))\n    # Using np.log2 with `where` handles the case p(r) = 0, where p*log(p) = 0.\n    H_R = -np.sum(p_r * np.log2(p_r, where=p_r > 0, out=np.zeros_like(p_r)))\n\n    # 4. Compute conditional entropy H(R|S)\n    # H(R|S) = sum_s p(s) H(R|S=s)\n    # Since p(s) = 1/K is uniform, H(R|S) is the mean of H(R|S=s).\n    # H(R|S=s) = - sum_r p(r|s) log2(p(r|s))\n    H_R_given_S_per_stimulus = -np.sum(p_r_given_s * np.log2(p_r_given_s, where=p_r_given_s > 0, out=np.zeros_like(p_r_given_s)), axis=1)\n    H_R_given_S = np.mean(H_R_given_S_per_stimulus)\n\n    # 5. Compute Mutual Information I(S;R) = H(R) - H(R|S)\n    mi = H_R - H_R_given_S\n    \n    # Due to floating point precision, very small negative numbers can occur\n    # when true MI is zero. By definition, MI is non-negative.\n    return max(0.0, mi)\n\ndef calculate_mi_and_ci(data, B, B_boot, seed):\n    \"\"\"\n    Calculates the point estimate and bootstrap confidence interval for mutual information.\n\n    Args:\n        data (list of lists): The raw response data for each stimulus.\n        B (int): The number of response bins.\n        B_boot (int): The number of bootstrap samples.\n        seed (int): The seed for the pseudorandom number generator.\n\n    Returns:\n        tuple: A tuple containing (lower_bound, point_estimate, upper_bound).\n    \"\"\"\n    # 1. Calculate the point estimate from the original data\n    point_estimate = compute_mi_plugin(data, B)\n\n    # 2. Perform nonparametric bootstrap\n    K = len(data)\n    if K == 0:\n        return 0.0, 0.0, 0.0\n    T = len(data[0])\n    \n    rng = np.random.default_rng(seed)\n    bootstrap_mi_values = np.zeros(B_boot)\n\n    original_data_arrays = [np.array(d) for d in data]\n\n    for i in range(B_boot):\n        resampled_data = []\n        for s_idx in range(K):\n            # Resample trials with replacement, within each stimulus\n            resampled_indices = rng.integers(0, T, size=T)\n            resampled_trials = original_data_arrays[s_idx][resampled_indices].tolist()\n            resampled_data.append(resampled_trials)\n        \n        bootstrap_mi_values[i] = compute_mi_plugin(resampled_data, B)\n\n    # 3. Compute the 95% confidence interval using the percentile method\n    alpha = 0.05\n    lower_bound = np.percentile(bootstrap_mi_values, 100 * alpha / 2.0)\n    upper_bound = np.percentile(bootstrap_mi_values, 100 * (1 - alpha / 2.0))\n\n    return lower_bound, point_estimate, upper_bound\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A (general, informative responses)\n        {\n            \"data\": [\n                [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2,2],\n                [2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,1,1],\n                [0,0,0,0,0,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3],\n            ],\n            \"B\": 4,\n        },\n        # Case B (near-indistinguishable responses)\n        {\n            \"data\": [\n                [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3],\n                [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3],\n            ],\n            \"B\": 4,\n        },\n        # Case C (small-sample and edge counts)\n        {\n            \"data\": [\n                [0,0,0,0,0,0,0,0,0,0],\n                [1,1,1,1,1,1,1,1,1,1],\n                [0,0,0,0,0,1,1,1,1,1],\n                [2,2,2,2,2,3,3,3,3,3],\n            ],\n            \"B\": 4,\n        },\n    ]\n\n    B_boot = 400\n    seed = 12345\n    \n    all_results = []\n    for case in test_cases:\n        data = case[\"data\"]\n        B = case[\"B\"]\n        \n        lower, point, upper = calculate_mi_and_ci(data, B, B_boot, seed)\n        \n        all_results.append(f\"{lower:.6f}\")\n        all_results.append(f\"{point:.6f}\")\n        all_results.append(f\"{upper:.6f}\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}