{
    "hands_on_practices": [
        {
            "introduction": "This first exercise explores one of the most fundamental models in information theory and its application to neural coding. By considering a simple linear neuron with additive Gaussian noise, we can derive a closed-form expression for the mutual information that directly relates to the signal-to-noise ratio ($SNR$). This foundational result  provides deep insight into the physical limits of information transmission in any noisy channel and serves as a crucial building block for more complex models.",
            "id": "5037464",
            "problem": "In a simplified neural coding model for a single sensory neuron, a scalar stimulus $S$ represents a deviation around baseline and is modeled as a zero-mean Gaussian random variable with variance $\\sigma_{S}^{2}$, that is $S \\sim \\mathcal{N}(0,\\sigma_{S}^{2})$. The neuron’s response $R$ is assumed to be linear and corrupted by independent additive noise $N \\sim \\mathcal{N}(0,\\sigma_{N}^{2})$, with $S$ and $N$ independent, so that $R = S + N$. Using as foundational starting points the definition of mutual information in terms of differential entropies and well-tested properties of Gaussian random variables, derive a closed-form expression for the mutual information $I(S;R)$ between the stimulus and the response in this model. Use natural logarithms so that the information is measured in nats. Express your final answer symbolically in terms of $\\sigma_{S}^{2}$ and $\\sigma_{N}^{2}$, and do not approximate. The final answer must be a single closed-form analytic expression.",
            "solution": "The problem is valid as it is scientifically grounded in information theory and statistics, well-posed with a unique and meaningful solution, and stated using objective, formal language. All necessary information is provided, and there are no contradictions or ambiguities.\n\nThe objective is to derive a closed-form expression for the mutual information $I(S;R)$ between a stimulus $S$ and a neural response $R$. The model is defined by the following givens:\n1.  The stimulus $S$ is a Gaussian random variable with zero mean and variance $\\sigma_{S}^{2}$, denoted as $S \\sim \\mathcal{N}(0, \\sigma_{S}^{2})$.\n2.  The response $R$ is given by a linear relationship $R = S + N$.\n3.  The noise $N$ is an independent Gaussian random variable with zero mean and variance $\\sigma_{N}^{2}$, denoted as $N \\sim \\mathcal{N}(0, \\sigma_{N}^{2})$.\n4.  The stimulus $S$ and the noise $N$ are statistically independent.\n\nWe begin with the definition of mutual information in terms of differential entropies. For continuous random variables $S$ and $R$, the mutual information is given by:\n$$\nI(S;R) = H(R) - H(R|S)\n$$\nwhere $H(R)$ is the differential entropy of the response $R$, and $H(R|S)$ is the conditional differential entropy of $R$ given $S$. We will calculate each of these terms separately.\n\nFirst, we determine the distribution of the response $R$. Since $R$ is the sum of two independent Gaussian random variables, $S$ and $N$, $R$ itself is a Gaussian random variable.\nThe mean of $R$ is the sum of the means of $S$ and $N$:\n$$\nE[R] = E[S+N] = E[S] + E[N] = 0 + 0 = 0\n$$\nThe variance of $R$ is the sum of the variances of $S$ and $N$, due to their independence:\n$$\n\\text{Var}(R) = \\text{Var}(S+N) = \\text{Var}(S) + \\text{Var}(N) = \\sigma_{S}^{2} + \\sigma_{N}^{2}\n$$\nThus, the distribution of the response is $R \\sim \\mathcal{N}(0, \\sigma_{S}^{2} + \\sigma_{N}^{2})$.\n\nThe differential entropy $H(X)$ of a Gaussian random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, when using the natural logarithm (measured in nats), is given by the formula:\n$$\nH(X) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)\n$$\nApplying this formula to the response variable $R$, we find its entropy:\n$$\nH(R) = \\frac{1}{2} \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right)\n$$\n\nNext, we calculate the conditional entropy $H(R|S)$. This term represents the uncertainty remaining in $R$ when the value of $S$ is known. If we condition on a specific stimulus value $S=s$, the response equation becomes $R = s + N$. Since $s$ is a fixed value in this context, the randomness in $R$ is due solely to the noise $N$. The distribution of $R$ given $S=s$ is a Gaussian distribution with mean $E[R|S=s] = E[s+N] = s + E[N] = s$ and variance $\\text{Var}(R|S=s) = \\text{Var}(s+N) = \\text{Var}(N) = \\sigma_{N}^{2}$. So, $R|S=s \\sim \\mathcal{N}(s, \\sigma_{N}^{2})$.\n\nThe entropy of this conditional distribution is:\n$$\nH(R|S=s) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\nNote that this expression is a constant and does not depend on the specific value of $s$. The conditional entropy $H(R|S)$ is the expectation of $H(R|S=s)$ over all possible values of $S$:\n$$\nH(R|S) = E_S[H(R|S=s)] = E_S\\left[\\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\\right]\n$$\nSince the expression inside the expectation is constant with respect to $S$, the expectation is simply the constant itself:\n$$\nH(R|S) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\n\nFinally, we substitute the expressions for $H(R)$ and $H(R|S)$ back into the definition of mutual information:\n$$\nI(S;R) = H(R) - H(R|S) = \\frac{1}{2} \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right) - \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\nUsing the logarithmic property $\\ln(a) - \\ln(b) = \\ln(a/b)$, we can simplify the expression:\n$$\nI(S;R) = \\frac{1}{2} \\left[ \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right) - \\ln(2\\pi e \\sigma_{N}^{2}) \\right] = \\frac{1}{2} \\ln\\left(\\frac{2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})}{2\\pi e \\sigma_{N}^{2}}\\right)\n$$\nThe term $2\\pi e$ cancels from the numerator and denominator:\n$$\nI(S;R) = \\frac{1}{2} \\ln\\left(\\frac{\\sigma_{S}^{2} + \\sigma_{N}^{2}}{\\sigma_{N}^{2}}\\right)\n$$\nThis can be written in a more standard form by separating the fraction:\n$$\nI(S;R) = \\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}\\right)\n$$\nThe term $\\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}$ is the signal-to-noise ratio (SNR) of the system. This result is a fundamental formula in information theory for a Gaussian channel.",
            "answer": "$$\n\\boxed{\\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}\\right)}\n$$"
        },
        {
            "introduction": "Moving from abstract models to measurable neural statistics, this practice examines how trial-to-trial variability impacts the information encoded by spike counts. We will use the Fano factor, the ratio of spike count variance to its mean, to characterize this variability and compare a 'noisy' Poisson neuron with a more reliable sub-Poisson one. This problem  demonstrates how to apply asymptotic approximations—a powerful tool in theoretical neuroscience—to quantify the information cost of neural noise and connect it to an experimentally observable quantity.",
            "id": "3990303",
            "problem": "Consider a neuron observed in a fixed counting window of duration $T$ under a single stimulus condition $S$. Let the spike count be a discrete random variable $R \\in \\{0, 1, 2, \\dots\\}$ with conditional mean $\\mathbb{E}[R \\mid S] = m$, where $m$ is large. Define the conditional entropy $H(R \\mid S)$ by the standard information-theoretic definition using the natural logarithm.\n\nAssume two spike-count models at fixed mean $m$:\n- A Poisson model, in which trial-to-trial variability satisfies the property that the Fano factor (variance-to-mean ratio) equals $1$.\n- A sub-Poisson model, in which the Fano factor $F$ satisfies $0 < F < 1$ and the conditional variance scales linearly with the mean, $\\operatorname{Var}(R \\mid S) = F m$.\n\nStarting from core definitions of entropy and well-tested asymptotic reasoning appropriate for large $m$ (for example, invoking the Central Limit Theorem (CLT) to justify a smooth unimodal approximation of the spike-count distribution and using continuous approximations where appropriate), derive the leading-order analytical expression for the change in conditional entropy induced by increasing trial-to-trial variability away from the sub-Poisson case to the Poisson case, at fixed mean $m$. Concretely, derive an explicit closed-form expression for\n$$\n\\Delta H \\equiv H_{\\text{Poisson}}(R \\mid S) - H_{\\text{sub-Poisson},F}(R \\mid S)\n$$\nas a function of the Fano factor $F$, under the large-$m$ asymptotic described above.\n\nUse the natural logarithm throughout and report $\\Delta H$ in nats. Your final answer must be a single closed-form analytic expression in terms of $F$ only (no dependence on $m$ should remain in the leading-order answer), with no rounding.",
            "solution": "The problem asks for the change in conditional entropy, $\\Delta H \\equiv H_{\\text{Poisson}}(R \\mid S) - H_{\\text{sub-Poisson},F}(R \\mid S)$, for a neuron's spike count $R$ under a stimulus $S$. The mean spike count is $\\mathbb{E}[R \\mid S] = m$, which is assumed to be large.\n\nThe conditional entropy of a discrete random variable $R$ with probability mass function $p(k) = P(R=k \\mid S)$ is defined as:\n$$\nH(R \\mid S) = -\\sum_{k=0}^{\\infty} p(k) \\ln(p(k))\n$$\nThe problem states that for a large mean count $m$, we can use an asymptotic approximation. Since the spike count $R$ can be thought of as a sum of many smaller events, the Central Limit Theorem (CLT) justifies approximating its discrete probability mass function $p(k)$ with a continuous, unimodal distribution, specifically a Gaussian (Normal) distribution.\n\nLet $X$ be a continuous random variable that approximates the discrete random variable $R$. The parameters of this Gaussian distribution will be matched to the moments of $R$. The mean is $\\mu = \\mathbb{E}[R \\mid S] = m$, and the variance is $\\sigma^2 = \\operatorname{Var}(R \\mid S)$. The probability density function (PDF) of $X$ is given by:\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-m)^2}{2\\sigma^2}\\right)\n$$\nFor a discrete integer-valued random variable with a large mean that is well-approximated by a continuous PDF $f(x)$, its entropy can be approximated by the differential entropy of the continuous variable. This is because the sum in the discrete entropy definition can be approximated by an integral:\n$$\nH(R \\mid S) = -\\sum_{k=0}^{\\infty} p(k) \\ln(p(k)) \\approx -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) dx\n$$\nThe integral on the right is the definition of the differential entropy, $H(X)$, of the continuous random variable $X$. This approximation provides the leading-order term for the entropy in the large-$m$ limit.\n\nThe differential entropy of a Gaussian distribution with variance $\\sigma^2$ is a standard result in information theory, given in nats by:\n$$\nH_{\\text{Gaussian}}(X) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)\n$$\nWe apply this formula to the two cases described in the problem.\n\nCase 1: The Poisson Model\nIn the Poisson model, the variance is equal to the mean. The Fano factor is $1$.\n$$\n\\operatorname{Var}_{\\text{Poisson}}(R \\mid S) = m\n$$\nThus, we approximate the spike count distribution with a Gaussian distribution having mean $m$ and variance $\\sigma_{\\text{Poisson}}^2 = m$. The entropy is:\n$$\nH_{\\text{Poisson}}(R \\mid S) \\approx \\frac{1}{2}\\ln(2\\pi e m)\n$$\n\nCase 2: The Sub-Poisson Model\nIn this model, the Fano factor is $F$, where $0 < F < 1$. The problem specifies that the variance is:\n$$\n\\operatorname{Var}_{\\text{sub-Poisson},F}(R \\mid S) = F m\n$$\nWe approximate this distribution with a Gaussian having mean $m$ and variance $\\sigma_{\\text{sub-Poisson}}^2 = F m$. The entropy is:\n$$\nH_{\\text{sub-Poisson},F}(R \\mid S) \\approx \\frac{1}{2}\\ln(2\\pi e F m)\n$$\n\nNow, we compute the change in conditional entropy, $\\Delta H$, by taking the difference between these two asymptotic expressions.\n$$\n\\Delta H = H_{\\text{Poisson}}(R \\mid S) - H_{\\text{sub-Poisson},F}(R \\mid S)\n$$\nSubstituting the derived expressions:\n$$\n\\Delta H \\approx \\frac{1}{2}\\ln(2\\pi e m) - \\frac{1}{2}\\ln(2\\pi e F m)\n$$\nUsing the property of logarithms, $\\ln(a) - \\ln(b) = \\ln(a/b)$, we can simplify the expression:\n$$\n\\Delta H \\approx \\frac{1}{2} \\left[ \\ln(2\\pi e m) - \\ln(2\\pi e F m) \\right] = \\frac{1}{2} \\ln\\left(\\frac{2\\pi e m}{2\\pi e F m}\\right)\n$$\nThe terms $2$, $\\pi$, $e$, and $m$ cancel out in the fraction:\n$$\n\\Delta H \\approx \\frac{1}{2} \\ln\\left(\\frac{1}{F}\\right)\n$$\nUsing another property of logarithms, $\\ln(1/F) = -\\ln(F)$, we arrive at the final expression for the leading-order change in entropy:\n$$\n\\Delta H = -\\frac{1}{2}\\ln(F)\n$$\nThis expression depends only on the Fano factor $F$, as required. Since $0 < F < 1$, $\\ln(F)$ is negative, and thus $\\Delta H$ is positive. This is scientifically consistent, as increasing variability from a sub-Poisson level ($F < 1$) to a Poisson level ($F=1$) should increase the uncertainty (entropy) of the spike count.",
            "answer": "$$\n\\boxed{-\\frac{1}{2}\\ln(F)}\n$$"
        },
        {
            "introduction": "Our final exercise tackles a more advanced and subtle topic in population coding: the role of noise correlations. While it is often assumed that correlated noise between neurons degrades the population's ability to encode information, this is not universally true. This problem  guides you through a carefully constructed scenario where shared noise fluctuations between two neurons actually increase the total information about a stimulus, a phenomenon known as synergy. Working through this example reveals how the structure of noise, not just its magnitude, is critical for understanding neural coding.",
            "id": "3990337",
            "problem": "Consider two neurons whose responses in a short time bin are binary random variables $R_1 \\in \\{0,1\\}$ and $R_2 \\in \\{0,1\\}$. A binary stimulus $S \\in \\{0,1\\}$ is presented with prior $P(S=0) = P(S=1) = \\frac{1}{2}$. The responses share a latent variable $Z \\in \\{a,b\\}$ that modulates a common gain; conditioned on $(S,Z)$, the responses are independent across neurons. Formally, for each $s \\in \\{0,1\\}$ and $z \\in \\{a,b\\}$, \n$$P(R_1=r_1,R_2=r_2 \\mid S=s, Z=z) = P(R_1=r_1 \\mid S=s, Z=z)\\, P(R_2=r_2 \\mid S=s, Z=z).$$\nThe latent variable satisfies $P(Z=a \\mid S=s) = P(Z=b \\mid S=s) = \\frac{1}{2}$ for both $s=0$ and $s=1$. The conditional response probabilities are:\n- For $S=0$, $P(R_i=1 \\mid S=0, Z=a) = \\frac{1}{2}$ and $P(R_i=1 \\mid S=0, Z=b) = \\frac{1}{2}$ for $i \\in \\{1,2\\}$.\n- For $S=1$, $P(R_i=1 \\mid S=1, Z=a) = 0.9$ and $P(R_i=1 \\mid S=1, Z=b) = 0.1$ for $i \\in \\{1,2\\}$.\n\nThis construction induces positive noise correlations when $S=1$ through the shared latent gain $Z$, while maintaining identical single-neuron marginals across stimuli. Let $R=(R_1,R_2)$ denote the joint response vector. Using the Shannon mutual information definition with natural logarithms, compute the mutual information $I(S;R)$ between the stimulus and the joint responses. Express your answer in nats and round your final numerical value to four significant figures. Additionally, you may assume that all unspecified probabilities are zero and that all expectations and probabilities are well-defined under the given model.",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   **Random Variables**: A binary stimulus $S \\in \\{0, 1\\}$, a latent variable $Z \\in \\{a, b\\}$, and two binary neural responses $R_1 \\in \\{0, 1\\}$ and $R_2 \\in \\{0, 1\\}$. The joint response is denoted by the vector $R = (R_1, R_2)$.\n-   **Prior Probabilities**:\n    -   $P(S=0) = P(S=1) = \\frac{1}{2}$.\n    -   $P(Z=a \\mid S=s) = P(Z=b \\mid S=s) = \\frac{1}{2}$ for $s \\in \\{0, 1\\}$.\n-   **Conditional Independence**: The responses $R_1$ and $R_2$ are conditionally independent given the stimulus $S$ and the latent variable $Z$.\n    $$P(R_1=r_1, R_2=r_2 \\mid S=s, Z=z) = P(R_1=r_1 \\mid S=s, Z=z) P(R_2=r_2 \\mid S=s, Z=z)$$\n-   **Conditional Response Probabilities**: For $i \\in \\{1, 2\\}$, the probabilities of firing ($R_i=1$) are given as:\n    -   $P(R_i=1 \\mid S=0, Z=a) = \\frac{1}{2}$.\n    -   $P(R_i=1 \\mid S=0, Z=b) = \\frac{1}{2}$.\n    -   $P(R_i=1 \\mid S=1, Z=a) = 0.9$.\n    -   $P(R_i=1 \\mid S=1, Z=b) = 0.1$.\n-   **Objective**: Compute the mutual information $I(S;R)$ using the natural logarithm, expressed in nats and rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is examined for validity:\n-   **Scientifically Grounded**: The problem describes a latent variable model, a standard theoretical construct in computational neuroscience used to model noise correlations in neural populations. The framework is based on established probability and information theory. It is scientifically sound.\n-   **Well-Posed**: All necessary probabilities and conditions are provided to fully specify the joint distribution $P(S, Z, R_1, R_2)$. The quantity to be calculated, mutual information, is a well-defined measure. A unique solution exists.\n-   **Objective**: The problem is stated in precise, formal mathematical language, free from ambiguity or subjective content.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\nThe mutual information between the stimulus $S$ and the joint response vector $R=(R_1, R_2)$ is given by the formula:\n$$I(S;R) = H(R) - H(R|S)$$\nwhere $H(R)$ is the entropy of the joint response distribution and $H(R|S)$ is the conditional entropy of the response given the stimulus. We will compute each term separately.\n\nFirst, we calculate the conditional entropy $H(R|S)$, which is defined as:\n$$H(R|S) = \\sum_{s \\in \\{0,1\\}} P(S=s) H(R|S=s) = \\frac{1}{2} H(R|S=0) + \\frac{1}{2} H(R|S=1)$$\nThis requires computing the conditional probability distributions $P(R|S=0)$ and $P(R|S=1)$. We obtain these by marginalizing over the latent variable $Z$:\n$$P(R=r \\mid S=s) = \\sum_{z \\in \\{a,b\\}} P(R=r \\mid S=s, Z=z) P(Z=z \\mid S=s)$$\nGiven $P(Z=a|S=s) = P(Z=b|S=s) = \\frac{1}{2}$, this becomes:\n$$P(R=r \\mid S=s) = \\frac{1}{2} P(R=r \\mid S=s, Z=a) + \\frac{1}{2} P(R=r \\mid S=s, Z=b)$$\nLet $p_{s,z} = P(R_i=1 \\mid S=s, Z=z)$. Due to conditional independence, $P(R=(r_1,r_2) \\mid S=s, Z=z) = p_{s,z}^{r_1+r_2}(1-p_{s,z})^{2-(r_1+r_2)}$.\n\n**Case 1: Stimulus $S=0$**\nThe given probabilities are $p_{0,a} = \\frac{1}{2}$ and $p_{0,b} = \\frac{1}{2}$. Since $p_{0,a} = p_{0,b}$, the latent variable has no effect on the response probabilities when $S=0$. Let $p_0 = \\frac{1}{2}$.\n$$P(R=r \\mid S=0) = P(R=r \\mid S=0, Z=a)$$\nThe responses $R_1$ and $R_2$ are independent Bernoulli trials with success probability $p_0 = \\frac{1}{2}$.\n$P(R=(0,0) \\mid S=0) = (1-p_0)^2 = (\\frac{1}{2})^2 = \\frac{1}{4}$.\n$P(R=(0,1) \\mid S=0) = (1-p_0)p_0 = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$.\n$P(R=(1,0) \\mid S=0) = p_0(1-p_0) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$.\n$P(R=(1,1) \\mid S=0) = p_0^2 = (\\frac{1}{2})^2 = \\frac{1}{4}$.\nThe distribution $P(R|S=0)$ is uniform over the four possible outcomes. The entropy is:\n$$H(R|S=0) = -\\sum_{r} P(r|S=0) \\ln P(r|S=0) = -4 \\left(\\frac{1}{4} \\ln \\frac{1}{4}\\right) = -\\ln\\left(\\frac{1}{4}\\right) = \\ln(4) = 2\\ln(2)$$\n\n**Case 2: Stimulus $S=1$**\nThe given probabilities are $p_{1,a} = 0.9$ and $p_{1,b} = 0.1$.\n$P(R=(0,0) \\mid S=1) = \\frac{1}{2}((1-p_{1,a})^2 + (1-p_{1,b})^2) = \\frac{1}{2}((0.1)^2 + (0.9)^2) = \\frac{1}{2}(0.01 + 0.81) = 0.41$.\n$P(R=(0,1) \\mid S=1) = \\frac{1}{2}((1-p_{1,a})p_{1,a} + (1-p_{1,b})p_{1,b}) = \\frac{1}{2}((0.1)(0.9) + (0.9)(0.1)) = \\frac{1}{2}(0.09 + 0.09) = 0.09$.\n$P(R=(1,0) \\mid S=1) = P(R=(0,1) \\mid S=1) = 0.09$ by symmetry.\n$P(R=(1,1) \\mid S=1) = \\frac{1}{2}(p_{1,a}^2 + p_{1,b}^2) = \\frac{1}{2}((0.9)^2 + (0.1)^2) = \\frac{1}{2}(0.81 + 0.01) = 0.41$.\nThe entropy is:\n$$H(R|S=1) = -[P((0,0)|1)\\ln P((0,0)|1) + P((0,1)|1)\\ln P((0,1)|1) + P((1,0)|1)\\ln P((1,0)|1) + P((1,1)|1)\\ln P((1,1)|1)]$$\n$$H(R|S=1) = -[2 \\times 0.41 \\ln(0.41) + 2 \\times 0.09 \\ln(0.09)]$$\nNow we compute the total conditional entropy $H(R|S)$:\n$$H(R|S) = \\frac{1}{2} H(R|S=0) + \\frac{1}{2} H(R|S=1) = \\frac{1}{2}(2\\ln 2) - \\frac{1}{2}[2 \\times 0.41 \\ln(0.41) + 2 \\times 0.09 \\ln(0.09)]$$\n$$H(R|S) = \\ln(2) - (0.41 \\ln(0.41) + 0.09 \\ln(0.09))$$\n\nNext, we calculate the marginal entropy $H(R)$. This requires the marginal response distribution $P(R)$.\n$$P(R=r) = P(R=r|S=0)P(S=0) + P(R=r|S=1)P(S=1) = \\frac{1}{2}(P(R=r|S=0) + P(R=r|S=1))$$\n$P(R=(0,0)) = \\frac{1}{2}(\\frac{1}{4} + 0.41) = \\frac{1}{2}(0.25 + 0.41) = \\frac{1}{2}(0.66) = 0.33$.\n$P(R=(0,1)) = \\frac{1}{2}(\\frac{1}{4} + 0.09) = \\frac{1}{2}(0.25 + 0.09) = \\frac{1}{2}(0.34) = 0.17$.\n$P(R=(1,0)) = P(R=(0,1)) = 0.17$.\n$P(R=(1,1)) = P(R=(0,0)) = 0.33$.\nThe marginal entropy $H(R)$ is:\n$$H(R) = -[2 \\times 0.33 \\ln(0.33) + 2 \\times 0.17 \\ln(0.17)]$$\n\nFinally, we compute the mutual information $I(S;R) = H(R) - H(R|S)$:\n$$I(S;R) = - (2 \\times 0.33 \\ln 0.33 + 2 \\times 0.17 \\ln 0.17) - [\\ln(2) - (0.41 \\ln 0.41 + 0.09 \\ln 0.09)]$$\nWe now substitute the numerical values for the logarithms:\n$\\ln(2) \\approx 0.693147$\n$\\ln(0.41) \\approx -0.891598$\n$\\ln(0.09) \\approx -2.407946$\n$\\ln(0.33) \\approx -1.108663$\n$\\ln(0.17) \\approx -1.771957$\n\nFirst, calculate $H(R)$:\n$$H(R) \\approx -[2 \\times 0.33 \\times (-1.108663) + 2 \\times 0.17 \\times (-1.771957)]$$\n$$H(R) \\approx -[-0.731718 - 0.602465] = 1.334183$$\nNext, calculate $H(R|S)$:\n$$H(R|S) \\approx 0.693147 - [0.41 \\times (-0.891598) + 0.09 \\times (-2.407946)]$$\n$$H(R|S) \\approx 0.693147 - [-0.365555 - 0.216715] = 0.693147 + 0.582270 = 1.275417$$\nNow, compute the difference:\n$$I(S;R) = H(R) - H(R|S) \\approx 1.334183 - 1.275417 = 0.058766$$\nRounding the result to four significant figures gives $0.05877$.\nThe information is approximately $0.05877$ nats.",
            "answer": "$$\n\\boxed{0.05877}\n$$"
        }
    ]
}