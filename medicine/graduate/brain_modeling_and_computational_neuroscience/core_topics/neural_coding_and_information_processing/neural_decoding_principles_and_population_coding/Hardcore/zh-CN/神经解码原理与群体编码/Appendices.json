{
    "hands_on_practices": [
        {
            "introduction": "神经解码的核心是从神经活动中推断外部或内部变量。本练习将引导您从第一性原理出发，为一个常见的泊松（Poisson）发放神经元群体构建解码器。通过推导最大似然（Maximum Likelihood, ML）和最大后验概率（Maximum A Posteriori, MAP）估计量，您将深入理解如何将神经编码模型转化为解码算法，并体会先验知识在解码过程中的作用。",
            "id": "4002619",
            "problem": "考虑一个标量感觉刺激 $s \\ge 0$，由一个包含 $N$ 个条件独立神经元的群体编码，神经元索引为 $i \\in \\{1,\\dots,N\\}$。在单个观察窗口内，神经元 $i$ 发出脉冲计数 $r_i \\in \\{0,1,2,\\dots\\}$，该计数被建模为一个泊松随机变量，其均值（率）为 $\\lambda_i(s) = c_i s$，其中 $c_i > 0$ 是一个已知的增益常数。令 $\\mathbf{r} = (r_1,\\dots,r_N)^{\\top}$ 表示观察到的计数。您可以假设在给定 $s$ 的条件下，脉冲计数是条件独立的。\n\n任务：\n- 仅使用核心定义，写出似然函数 $L(s) = p(\\mathbf{r}\\,|\\,s)$。\n- 使用最大似然（ML）的定义，推导最大似然估计量 $\\hat{s}_{\\mathrm{ML}}$，作为 $L(s)$ 在 $s \\ge 0$ 上的最大化者。\n- 假设 $s$ 上的先验密度 $p(s)$ 由一个形状参数为 $\\alpha > 0$ 且率参数为 $\\beta > 0$ 的伽马分布给出，即对于 $s \\ge 0$，有 $p(s) \\propto s^{\\alpha - 1} \\exp(-\\beta s)$。使用贝叶斯法则，推导最大后验（MAP）估计量 $\\hat{s}_{\\mathrm{MAP}}$，作为后验概率 $p(s\\,|\\,\\mathbf{r})$ 在 $s \\ge 0$ 上的最大化者。\n- 在重复独立观测下，比较 $\\hat{s}_{\\mathrm{ML}}$ 和 $\\hat{s}_{\\mathrm{MAP}}$ 的渐近性质。具体来说，假设实验对相同的真实 $s$ 重复进行 $T \\in \\mathbb{N}$ 次独立试验，为每个神经元 $i$ 产生计数 $\\{r_i^{(t)}\\}_{t=1}^T$。定义总计数 $R_i = \\sum_{t=1}^T r_i^{(t)}$。从费雪信息的定义出发，确定该模型的每次试验的费雪信息 $I_1(s)$，并描述大 $T$ 行为下 $\\hat{s}_{\\mathrm{ML}}$ 和 $\\hat{s}_{\\mathrm{MAP}}$ 的性质（一致性、渐近正态性、渐近方差以及 $\\hat{s}_{\\mathrm{MAP}}$ 的主阶偏差）。\n\n答案规格：\n- 以闭合形式的单行矩阵形式提供最终答案，即估计量对 $\\hat{s}_{\\mathrm{ML}}$ 和 $\\hat{s}_{\\mathrm{MAP}}$，用 $\\{r_i\\}_{i=1}^N$、$\\{c_i\\}_{i=1}^N$、$\\alpha$ 和 $\\beta$ 表示。\n- 无需进行数值四舍五入。\n- 不要在最终方框答案中包含单位。\n- 首次使用时定义所有缩略语（例如，最大似然（ML）和最大后验（MAP））。",
            "solution": "在进行求解之前，对问题陈述的有效性进行评估。\n\n**问题有效性验证**\n\n**步骤1：提取已知条件**\n-   标量刺激：$s \\ge 0$。\n-   神经元数量：$N$。\n-   神经元索引：$i \\in \\{1, \\dots, N\\}$。\n-   神经元 $i$ 的脉冲计数：$r_i \\in \\{0, 1, 2, \\dots\\}$。\n-   观测计数向量：$\\mathbf{r} = (r_1, \\dots, r_N)^{\\top}$。\n-   脉冲计数分布：泊松随机变量。\n-   神经元 $i$ 的泊松均值：$\\lambda_i(s) = c_i s$。\n-   神经元 $i$ 的增益常数：$c_i > 0$。\n-   条件独立性：给定 $s$ 时，脉冲计数 $\\{r_i\\}$ 条件独立。\n-   $s$ 的先验密度：$p(s) \\propto s^{\\alpha - 1} \\exp(-\\beta s)$（对于 $s \\ge 0$），这是一个形状参数为 $\\alpha > 0$、率参数为 $\\beta > 0$ 的伽马分布。\n-   重复试验分析：$T \\in \\mathbb{N}$ 次独立试验，使用相同的真实 $s$，为每个神经元 $i$ 产生计数 $\\{r_i^{(t)}\\}_{t=1}^T$。\n-   总计数：$R_i = \\sum_{t=1}^T r_i^{(t)}$。\n\n**步骤2：使用提取的已知条件进行验证**\n-   **科学依据**：该问题在计算神经科学中有充分的依据。使用带有线性调谐曲线的泊松脉冲计数是神经元群体编码的一个标准且基本的模型。应用最大似然（ML）和最大后验（MAP）估计是解码神经响应的核心技术。选择伽马先验在数学上很方便，因为它是泊松似然的共轭先验，这是贝叶斯建模中的一种常见构造。\n-   **适定性**：该问题是适定的。它提供了所有必要的信息——数据的统计模型、率的函数形式、先验分布以及所有参数约束——以唯一地推导出所要求的估计量并分析其性质。\n-   **客观性**：问题以精确、客观的数学语言陈述，没有歧义或主观内容。\n-   **完备性与一致性**：该问题是自洽且内部一致的。约束条件 $s \\ge 0$ 和 $c_i > 0$ 确保了泊松率 $\\lambda_i(s)$ 按要求是非负的。伽马先验的定义域与刺激 $s$ 的定义域相匹配。\n-   **其他标准**：该问题不违反任何其他有效性标准。这是一个标准的理论问题，既不平凡也不适定。\n\n**步骤3：结论与行动**\n-   **结论**：问题有效。\n-   **行动**：将提供完整的解答。\n\n**解题推导**\n\n**似然函数**\n问题陈述指出，来自神经元 $i$ 的脉冲计数 $r_i$ 服从均值为 $\\lambda_i(s) = c_i s$ 的泊松分布。单个神经元响应的概率质量函数（PMF）为：\n$$\np(r_i | s) = \\frac{(\\lambda_i(s))^{r_i} \\exp(-\\lambda_i(s))}{r_i!} = \\frac{(c_i s)^{r_i} \\exp(-c_i s)}{r_i!}\n$$\n鉴于神经元是条件独立的，观测到脉冲计数向量 $\\mathbf{r} = (r_1, \\dots, r_N)^{\\top}$ 的联合概率是各个概率的乘积。这个联合概率，当被看作是刺激 $s$ 的函数时，就是似然函数 $L(s)$：\n$$\nL(s) = p(\\mathbf{r} | s) = \\prod_{i=1}^N p(r_i | s) = \\prod_{i=1}^N \\frac{(c_i s)^{r_i} \\exp(-c_i s)}{r_i!}\n$$\n通过对依赖于 $s$ 的项进行分组，可以重写为：\n$$\nL(s) = \\left( \\prod_{i=1}^N \\frac{c_i^{r_i}}{r_i!} \\right) \\left( \\prod_{i=1}^N s^{r_i} \\right) \\left( \\prod_{i=1}^N \\exp(-c_i s) \\right) = \\left( \\prod_{i=1}^N \\frac{c_i^{r_i}}{r_i!} \\right) s^{\\sum_{i=1}^N r_i} \\exp\\left(-s \\sum_{i=1}^N c_i\\right)\n$$\n\n**最大似然（ML）估计量**\n最大似然（ML）估计量 $\\hat{s}_{\\mathrm{ML}}$ 是使 $L(s)$ 最大化的 $s$ 值。在计算上，最大化对数似然函数 $\\ell(s) = \\ln L(s)$ 更为方便，因为对数是单调函数。\n$$\n\\ell(s) = \\ln \\left( \\prod_{i=1}^N \\frac{(c_i s)^{r_i} \\exp(-c_i s)}{r_i!} \\right) = \\sum_{i=1}^N \\ln \\left( \\frac{(c_i s)^{r_i} \\exp(-c_i s)}{r_i!} \\right)\n$$\n$$\n\\ell(s) = \\sum_{i=1}^N \\left( r_i \\ln(c_i s) - c_i s - \\ln(r_i!) \\right) = \\sum_{i=1}^N \\left( r_i \\ln c_i + r_i \\ln s - c_i s - \\ln(r_i!) \\right)\n$$\n对涉及 $s$ 的项进行分组：\n$$\n\\ell(s) = \\left( \\sum_{i=1}^N r_i \\right) \\ln s - \\left( \\sum_{i=1}^N c_i \\right) s + \\mathrm{const}\n$$\n为了找到最大值，我们计算 $\\ell(s)$ 关于 $s$ 的导数并将其设为零：\n$$\n\\frac{d\\ell}{ds} = \\frac{\\sum_{i=1}^N r_i}{s} - \\sum_{i=1}^N c_i = 0\n$$\n解出 $s$ 即可得到 ML 估计量：\n$$\n\\hat{s}_{\\mathrm{ML}} = \\frac{\\sum_{i=1}^N r_i}{\\sum_{i=1}^N c_i}\n$$\n二阶导数 $\\frac{d^2\\ell}{ds^2} = -\\frac{\\sum_{i=1}^N r_i}{s^2}$ 是负的（因为 $r_i \\ge 0$ 且我们假设至少观测到一个脉冲），这证实了这是一个最大值。该估计量按要求是非负的。\n\n**最大后验（MAP）估计量**\n最大后验（MAP）估计量 $\\hat{s}_{\\mathrm{MAP}}$ 最大化后验概率密度 $p(s|\\mathbf{r})$。根据贝叶斯法则，后验概率正比于似然与先验的乘积：\n$$\np(s|\\mathbf{r}) \\propto p(\\mathbf{r}|s) p(s) = L(s) p(s)\n$$\n先验是伽马分布，$p(s) \\propto s^{\\alpha-1}\\exp(-\\beta s)$。我们最大化对数后验概率 $\\ln p(s|\\mathbf{r})$：\n$$\n\\ln p(s|\\mathbf{r}) = \\ln(L(s)) + \\ln(p(s)) + \\mathrm{const}\n$$\n使用我们之前关于 $\\ell(s)$ 和先验对数的表达式：\n$$\n\\ln p(s|\\mathbf{r}) \\propto \\left[ \\left(\\sum_{i=1}^N r_i\\right) \\ln s - \\left(\\sum_{i=1}^N c_i\\right) s \\right] + \\left[ (\\alpha-1)\\ln s - \\beta s \\right]\n$$\n$$\n\\ln p(s|\\mathbf{r}) \\propto \\left(\\sum_{i=1}^N r_i + \\alpha - 1\\right) \\ln s - \\left(\\sum_{i=1}^N c_i + \\beta\\right) s\n$$\n对 $s$ 求导并设为零：\n$$\n\\frac{d}{ds} \\ln p(s|\\mathbf{r}) = \\frac{\\sum_{i=1}^N r_i + \\alpha - 1}{s} - \\left(\\sum_{i=1}^N c_i + \\beta\\right) = 0\n$$\n解出 $s$ 即可得到 MAP 估计量：\n$$\n\\hat{s}_{\\mathrm{MAP}} = \\frac{\\sum_{i=1}^N r_i + \\alpha - 1}{\\sum_{i=1}^N c_i + \\beta}\n$$\n这对应于后验分布的众数，该后验分布是一个形状参数为 $\\tilde{\\alpha} = \\sum r_i + \\alpha$ 且率参数为 $\\tilde{\\beta} = \\sum c_i + \\beta$ 的伽马分布。\n\n**渐近性质**\n**费雪信息**：对于单次试验，费雪信息 $I_1(s)$ 定义为 $I_1(s) = -E\\left[\\frac{d^2\\ell(s)}{ds^2}\\right]$，其中期望是针对数据分布 $p(\\mathbf{r}|s)$ 计算的。\n我们得到 $\\frac{d^2\\ell}{ds^2} = -\\frac{\\sum_{i=1}^N r_i}{s^2}$。\n因此，\n$$\nI_1(s) = -E\\left[ -\\frac{\\sum_{i=1}^N r_i}{s^2} \\right] = \\frac{1}{s^2} E\\left[\\sum_{i=1}^N r_i\\right]\n$$\n根据期望的线性性质，$E\\left[\\sum_{i=1}^N r_i\\right] = \\sum_{i=1}^N E[r_i]$。由于 $r_i \\sim \\mathrm{Poisson}(c_i s)$，我们有 $E[r_i]=c_i s$。\n$$\nE\\left[\\sum_{i=1}^N r_i\\right] = \\sum_{i=1}^N c_i s = s \\sum_{i=1}^N c_i\n$$\n将此代入 $I_1(s)$ 的表达式中：\n$$\nI_1(s) = \\frac{1}{s^2} \\left(s \\sum_{i=1}^N c_i\\right) = \\frac{\\sum_{i=1}^N c_i}{s}\n$$\n**大-$T$ 行为**：对于 $T$ 次独立试验，总费雪信息为 $I_T(s) = T \\cdot I_1(s)$。\n-   **一致性**：$\\hat{s}_{\\mathrm{ML}}$ 和 $\\hat{s}_{\\mathrm{MAP}}$ 都是一致估计量。随着试验次数 $T \\to \\infty$，固定先验在 MAP 估计中的影响变得可以忽略不计，两个估计量都依概率收敛于真实值 $s$。\n-   **渐近正态性与方差**：对于大 $T$，两个估计量都是渐近正态的。它们的分布趋近于一个以 $s$ 为中心的高斯分布，其方差趋近于克拉默-拉奥下界（CRLB），由总费雪信息的倒数给出。\n$$\n\\text{Var}(\\hat{s}_{\\mathrm{ML}}), \\text{Var}(\\hat{s}_{\\mathrm{MAP}}) \\to \\frac{1}{I_T(s)} = \\frac{s}{T \\sum_{i=1}^N c_i} \\quad \\text{as } T \\to \\infty\n$$\n-   **$\\hat{s}_{\\mathrm{MAP}}$ 的偏差**：ML 估计量对于任何 $T$ 都是无偏的，因为 $E[\\hat{s}_{\\mathrm{ML}}] = E[\\frac{\\sum r_i}{\\sum c_i}] = \\frac{\\sum E[r_i]}{\\sum c_i} = \\frac{s \\sum c_i}{\\sum c_i} = s$。MAP 估计量对于有限的 $T$ 是有偏的。使用 $T$ 次试验的总计数的估计量，$\\hat{s}_{\\mathrm{MAP}}^{(T)} = \\frac{\\sum R_i + \\alpha - 1}{T \\sum c_i + \\beta}$：\n$$\n\\text{Bias}(\\hat{s}_{\\mathrm{MAP}}^{(T)}) = E[\\hat{s}_{\\mathrm{MAP}}^{(T)}] - s = \\frac{E[\\sum R_i] + \\alpha - 1}{T \\sum c_i + \\beta} - s\n$$\n由于 $E[\\sum R_i] = \\sum E[\\sum_{t=1}^T r_i^{(t)}] = \\sum T c_i s = T s \\sum c_i$：\n$$\n\\text{Bias}(\\hat{s}_{\\mathrm{MAP}}^{(T)}) = \\frac{T s \\sum c_i + \\alpha - 1}{T \\sum c_i + \\beta} - s = \\frac{\\alpha - 1 - s\\beta}{T \\sum c_i + \\beta}\n$$\n对于大 $T$，主阶偏差是 $O(1/T)$ 阶的：\n$$\n\\text{Bias}(\\hat{s}_{\\mathrm{MAP}}^{(T)}) \\approx \\frac{\\alpha - 1 - s\\beta}{T \\sum_{i=1}^N c_i}\n$$\n该偏差项反映了先验分布的“拉动”作用，随着收集到更多数据，这种作用会减弱。\n\n最终答案要求的是基于单次观测 $\\mathbf{r}$ 的估计量。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sum_{i=1}^N r_i}{\\sum_{i=1}^N c_i} & \\frac{\\sum_{i=1}^N r_i + \\alpha - 1}{\\sum_{i=1}^N c_i + \\beta}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在实际应用中，解码器通常建立在简化的假设之上，这些假设可能与真实的神经过程不完全匹配，例如忽略神经元之间的噪声相关性。本练习旨在探讨这种“模型失配”带来的后果，特别是它如何影响解码器的偏差（bias）。这项实践将磨练您分析解码算法鲁棒性的能力，而这对于设计实用的解码系统至关重要。",
            "id": "4002644",
            "problem": "考虑一个针对标量刺激 $s \\in \\mathbb{R}$ 的群体编码，该编码由 $n$ 个神经元组成。编码模型是加性的，并且是条件高斯的：给定 $s$，群体响应 $\\mathbf{r} \\in \\mathbb{R}^{n}$ 通过以下方式生成：\n$$\n\\mathbf{r} = \\mathbf{a}\\, s + \\boldsymbol{\\varepsilon},\n$$\n其中 $\\mathbf{a} \\in \\mathbb{R}^{n}$ 是一个已知的敏感度向量，$\\boldsymbol{\\varepsilon}$ 是从一个多元正态分布中抽取的零均值噪声，其真实协方差为 $\\boldsymbol{\\Sigma}_{\\text{true}} \\in \\mathbb{R}^{n \\times n}$。该协方差矩阵是正定的，但不一定是对角的（神经元可能具有相关的噪声）。解码器是在一个失配的假设下构建的，即假设神经元间的噪声是独立的，并使用一个正定对角矩阵 $\\boldsymbol{\\Sigma}_{\\text{assumed}} \\in \\mathbb{R}^{n \\times n}$ 来构建高斯似然。\n\n使用最大似然估计（MLE），失配的解码器通过最小化由 $\\boldsymbol{\\Sigma}_{\\text{assumed}}$ 和上述线性调谐模型构建的假定负对数似然（不含与 $s$ 无关的加性常数）来选择 $\\hat{s}$。\n\n从高斯似然的基本原理和统计偏差的定义 $\\mathbb{E}[\\hat{s}] - s$ 出发，推导在真实生成模型（使用 $\\boldsymbol{\\Sigma}_{\\text{true}}$）下，失配的最大似然估计量 $\\hat{s}$ 的偏差的闭式表达式。您的推导必须明确地追踪其对 $\\boldsymbol{\\Sigma}_{\\text{assumed}}$ 和 $\\boldsymbol{\\Sigma}_{\\text{true}}$ 的任何依赖关系。\n\n请将最终答案表示为单个闭式解析表达式。无需四舍五入。此问题不涉及物理单位。",
            "solution": "该问题要求推导刺激 $s$ 的最大似然估计量（MLE）的统计偏差，其中解码器使用了关于噪声协方差结构的失配假设。偏差定义为 $\\mathbb{E}[\\hat{s}] - s$，其中期望是基于真实的数据生成分布计算的。\n\n首先，我们确定获得估计量 $\\hat{s}$ 的过程。解码器在噪声是高斯的且在神经元之间独立的假设下运行。这对应于一个假定的噪声协方差矩阵 $\\boldsymbol{\\Sigma}_{\\text{assumed}}$，该矩阵是对角的且正定的。因此，假定的生成模型是 $p_{\\text{assumed}}(\\mathbf{r}|s) \\sim \\mathcal{N}(\\mathbf{a}s, \\boldsymbol{\\Sigma}_{\\text{assumed}})$。\n\n在此假设下的似然函数是多元正态分布的概率密度函数（PDF）：\n$$\nL_{\\text{assumed}}(s; \\mathbf{r}) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\boldsymbol{\\Sigma}_{\\text{assumed}})}} \\exp\\left(-\\frac{1}{2}(\\mathbf{r} - \\mathbf{a}s)^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} (\\mathbf{r} - \\mathbf{a}s)\\right)\n$$\n相应的对数似然为：\n$$\n\\ln L_{\\text{assumed}}(s; \\mathbf{r}) = -\\frac{1}{2}(\\mathbf{r} - \\mathbf{a}s)^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} (\\mathbf{r} - \\mathbf{a}s) - \\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(\\boldsymbol{\\Sigma}_{\\text{assumed}}))\n$$\n最大似然估计量 $\\hat{s}$ 是使该对数似然最大化的 $s$ 的值。这等价于最小化负对数似然。如问题所述，我们只需要考虑依赖于 $s$ 的项。因此，$\\hat{s}$ 通过最小化二次型 $J(s)$ 来找到：\n$$\nJ(s) = (\\mathbf{r} - \\mathbf{a}s)^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} (\\mathbf{r} - \\mathbf{a}s)\n$$\n为了找到最小值，我们将 $J(s)$ 对标量变量 $s$ 求导，并令导数为零。首先，我们展开二次型：\n$$\nJ(s) = \\mathbf{r}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} - s\\mathbf{r}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a} - s\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} + s^2 \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}\n$$\n由于这些项都是标量，所以 $\\mathbf{r}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a} = \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r}$。因此：\n$$\nJ(s) = \\mathbf{r}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} - 2s \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} + s^2 \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}\n$$\n对 $s$ 求导：\n$$\n\\frac{dJ(s)}{ds} = -2\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} + 2s \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}\n$$\n令导数为零以求得估计量 $\\hat{s}$：\n$$\n-2\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} + 2\\hat{s} \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a} = 0\n$$\n$$\n\\hat{s} (\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}) = \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r}\n$$\n项 $\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}$ 是一个标量。由于 $\\boldsymbol{\\Sigma}_{\\text{assumed}}$ 是正定的，其逆矩阵 $\\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1}$ 也是正定的。假设 $\\mathbf{a}$ 不是零向量（否则刺激将不可观测），这个标量项是严格为正的，这确保了我们可以用它来做除法。因此，失配的最大似然估计量为：\n$$\n\\hat{s} = \\frac{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r}}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}}\n$$\n接下来，我们在真实的生成模型下计算 $\\hat{s}$ 的期望。响应的真实模型是 $\\mathbf{r} = \\mathbf{a}s + \\boldsymbol{\\varepsilon}$，其中噪声 $\\boldsymbol{\\varepsilon}$ 来自 $\\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_{\\text{true}})$。关键在于，噪声的真实均值为 $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$。\n\n估计量 $\\hat{s}$ 的期望是：\n$$\n\\mathbb{E}[\\hat{s}] = \\mathbb{E}\\left[ \\frac{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r}}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}} \\right]\n$$\n分母相对于随机变量 $\\mathbf{r}$ 是一个常数，所以我们可以将其从期望中提出：\n$$\n\\mathbb{E}[\\hat{s}] = \\frac{1}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}} \\mathbb{E}[\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r}]\n$$\n期望是线性算子，所以我们可以将其移入内部：\n$$\n\\mathbb{E}[\\hat{s}] = \\frac{1}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}} \\left( \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbb{E}[\\mathbf{r}] \\right)\n$$\n现在，我们在真实模型下计算响应向量 $\\mathbf{r}$ 的期望：\n$$\n\\mathbb{E}[\\mathbf{r}] = \\mathbb{E}[\\mathbf{a}s + \\boldsymbol{\\varepsilon}] = \\mathbb{E}[\\mathbf{a}s] + \\mathbb{E}[\\boldsymbol{\\varepsilon}]\n$$\n由于 $s$ 是一个固定的非随机参数，$\\mathbf{a}$ 是一个已知的常数向量，因此 $\\mathbb{E}[\\mathbf{a}s] = \\mathbf{a}s$。真实的噪声是零均值的，所以 $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$。因此：\n$$\n\\mathbb{E}[\\mathbf{r}] = \\mathbf{a}s\n$$\n将此结果代回 $\\mathbb{E}[\\hat{s}]$ 的表达式中：\n$$\n\\mathbb{E}[\\hat{s}] = \\frac{1}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}} \\left( \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} (\\mathbf{a}s) \\right)\n$$\n由于 $s$ 是一个标量，它可以被提出来：\n$$\n\\mathbb{E}[\\hat{s}] = \\frac{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}} s\n$$\n标量项 $\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}$ 在分子和分母中被约去，得到：\n$$\n\\mathbb{E}[\\hat{s}] = s\n$$\n这个结果表明，失配的估计量是无偏的。偏差定义为 $\\text{Bias}(\\hat{s}) = \\mathbb{E}[\\hat{s}] - s$。\n$$\n\\text{Bias}(\\hat{s}) = s - s = 0\n$$\n偏差为零。这个结果之所以出现，是因为估计量对于观测值 $\\mathbf{r}$ 是线性的，并且数据均值的假定模型 $\\mathbb{E}[\\mathbf{r}] = \\mathbf{a}s$ 是正确的。协方差矩阵的失配（$\\boldsymbol{\\Sigma}_{\\text{assumed}}$ 与 $\\boldsymbol{\\Sigma}_{\\text{true}}$）会影响估计量的方差（从而影响其效率），但只要噪声是加性的且均值为零，就不会影响其偏差。偏差的最终结果与 $\\boldsymbol{\\Sigma}_{\\text{assumed}}$ 和 $\\boldsymbol{\\Sigma}_{\\text{true}}$ 均无关。",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "许多应用，如脑机接口（Brain-Computer Interfaces），需要根据神经脉冲实时追踪一个连续变化的状态（例如运动意图）。这个高级练习将指导您实现一个迭代扩展卡尔曼滤波器（Iterated Extended Kalman Filter, IEKF），这是一个功能强大的工具，用于从非线性和动态系统中进行解码。您将学习一种领域内的常用技巧——将泊松（Poisson）脉冲计数近似为高斯（Gaussian）观测，以便将其整合到卡尔曼滤波框架中。",
            "id": "4002636",
            "problem": "考虑一个隐状态向量 $x_t \\in \\mathbb{R}^n$，它代表一个待从 $m$ 个神经元群体中解码的低维刺激或运动变量。假设状态的先验为一个线性高斯动态模型，由 $x_t = A x_{t-1} + w_t$ 给出，其中 $w_t \\sim \\mathcal{N}(0, Q)$，并且单步预测均值 $x_{t|t-1}$ 和协方差 $P_{t|t-1}$ 已通过标准时间更新计算得出。时间 $t$ 的观测是一个 $m$ 维脉冲计数向量 $y_t \\in \\mathbb{R}^m$，由 $m$ 个条件独立的神经元在一个短时间窗内发放，并被建模为一个具有独立泊松分量的点过程，其均值通过调谐函数 $\\lambda(x_t) \\in \\mathbb{R}^m_{\\ge 0}$ 依赖于隐状态。采用以下广泛使用的近似方法：每个泊松观测由一个具有相同均值和方差的高斯分布来近似，得到 $y_t \\mid x_t \\approx \\mathcal{N}(\\lambda(x_t), R(x_t))$，其中 $R(x_t) = \\mathrm{diag}(\\lambda(x_t))$。\n\n从上述基本定义出发，在不假设后验分布存在闭式表达式的情况下，从第一性原理推导出一个迭代扩展卡尔曼滤波器（IEKF）的测量更新步骤，该步骤需包含观测模型中均值和协方差对状态的依赖性。具体而言，在当前迭代值附近对观测函数进行线性化，并将 $R(x_t)$ 在该迭代值处视为固定值，然后展示IEKF更新是如何通过最小化负对数后验的二次近似而产生的。\n\n对以下每个测试用例，实现由此得到的针对单个时间步的IEKF测量更新。在所有情况下，使用Joseph稳定协方差更新来保持半正定性。为保证数值稳定性，如果 $\\lambda(x)$ 的任何分量低于指定的正则化阈值 $\\varepsilon$，则在构成 $R(x)$ 时将其裁剪为 $\\varepsilon$，并将相同的 $\\varepsilon$ 加到新息协方差矩阵的对角线上。使用最大迭代次数 $N_{\\mathrm{max}}$ 和作用于状态迭代值相对变化的收敛容差 $\\tau$，该相对变化定义为 $\\|x^{(i+1)} - x^{(i)}\\|_2 / \\max(1, \\|x^{(i+1)}\\|_2)$。\n\n待实现的观测模型：\n- 线性调谐：$\\lambda(x) = C x$，其中对于相关的 $x$，$C \\in \\mathbb{R}^{m \\times n}$ 的输出为非负。\n- 指数调谐：$\\lambda(x) = \\exp(W x + b)$（按元素施加），其中 $W \\in \\mathbb{R}^{m \\times n}$ 且 $b \\in \\mathbb{R}^m$。\n\n对于每个测试用例，给定 $x_{t|t-1}$、$P_{t|t-1}$、观测参数、观测值 $y_t$、正则化参数 $\\varepsilon$、$N_{\\mathrm{max}}$ 和 $\\tau$。你的程序必须为单次测量更新计算 IEKF 后验均值 $x_{t|t}$。输出必须是无单位实数。\n\n测试套件：\n1. 线性调谐，中等计数。\n   - $n = 2$, $m = 3$\n   - $x_{t|t-1} = [1.0, 0.5]^T$\n   - $P_{t|t-1} = \\begin{bmatrix}0.2  0.05 \\\\ 0.05  0.3\\end{bmatrix}$\n   - $C = \\begin{bmatrix}1.0  0.0 \\\\ 0.0  1.0 \\\\ 0.5  0.5\\end{bmatrix}$\n   - $y_t = [2.0, 1.0, 1.0]^T$\n   - $\\varepsilon = 10^{-6}$, $N_{\\mathrm{max}} = 10$, $\\tau = 10^{-9}$\n\n2. 指数调谐，低计数。\n   - $n = 2$, $m = 3$\n   - $x_{t|t-1} = [0.2, -0.3]^T$\n   - $P_{t|t-1} = \\begin{bmatrix}0.1  0.0 \\\\ 0.0  0.1\\end{bmatrix}$\n   - $W = \\begin{bmatrix}1.0  0.0 \\\\ 0.5  -1.0 \\\\ -0.5  0.5\\end{bmatrix}$, $b = [0.0, -0.2, 0.1]^T$\n   - $y_t = [0.0, 1.0, 0.0]^T$\n   - $\\varepsilon = 10^{-6}$, $N_{\\mathrm{max}} = 25$, $\\tau = 10^{-8}$\n\n3. 指数调谐，近零率（奇点压力测试）。\n   - $n = 2$, $m = 2$\n   - $x_{t|t-1} = [-3.0, -3.0]^T$\n   - $P_{t|t-1} = \\begin{bmatrix}0.5  0.0 \\\\ 0.0  0.5\\end{bmatrix}$\n   - $W = \\begin{bmatrix}1.0  0.0 \\\\ 0.0  1.0\\end{bmatrix}$, $b = [-5.0, -5.0]^T$\n   - $y_t = [0.0, 0.0]^T$\n   - $\\varepsilon = 10^{-3}$, $N_{\\mathrm{max}} = 25$, $\\tau = 10^{-8}$\n\n4. 指数调谐，高计数。\n   - $n = 2$, $m = 3$\n   - $x_{t|t-1} = [2.0, 1.0]^T$\n   - $P_{t|t-1} = \\begin{bmatrix}1.0  0.2 \\\\ 0.2  1.5\\end{bmatrix}$\n   - $W = \\begin{bmatrix}0.8  0.3 \\\\ 1.2  -0.5 \\\\ -0.7  1.1\\end{bmatrix}$, $b = [0.5, 0.3, 0.2]^T$\n   - $y_t = [20.0, 30.0, 25.0]^T$\n   - $\\varepsilon = 10^{-6}$, $N_{\\mathrm{max}} = 25$, $\\tau = 10^{-8}$\n\n算法要求：\n- 在 $x^{(0)} = x_{t|t-1}$ 处初始化 IEKF 迭代值。\n- 在每次迭代 $i$ 中，计算 $\\lambda(x^{(i)})$ 及其雅可比矩阵 $H^{(i)} = \\frac{\\partial \\lambda}{\\partial x}\\big|_{x^{(i)}}$。\n- 构造 $R^{(i)} = \\mathrm{diag}(\\max(\\lambda(x^{(i)}), \\varepsilon))$ 和线性化测量值 $y_{\\mathrm{lin}}^{(i)} = y_t - \\lambda(x^{(i)}) + H^{(i)} x^{(i)}$。\n- 计算新息协方差 $S^{(i)} = H^{(i)} P_{t|t-1} (H^{(i)})^\\top + R^{(i)} + \\varepsilon I$ 和增益 $K^{(i)} = P_{t|t-1} (H^{(i)})^\\top (S^{(i)})^{-1}$。\n- 使用 $x^{(i+1)} = x_{t|t-1} + K^{(i)} \\left( y_{\\mathrm{lin}}^{(i)} - H^{(i)} x_{t|t-1} \\right)$ 更新迭代值。\n- 当相对变化低于 $\\tau$ 或 $i = N_{\\mathrm{max}}-1$ 时停止。\n- 收敛后，设置 $x_{t|t} = x^{(i+1)}$ 并使用 Joseph 形式更新协方差：\n  $$P_{t|t} = (I - K^{(i)} H^{(i)}) P_{t|t-1} (I - K^{(i)} H^{(i)})^\\top + K^{(i)} R^{(i)} (K^{(i)})^\\top.$$\n\n你的程序应生成单行输出，其中包含四个测试用例得到的后验均值 $x_{t|t}$，格式为逗号分隔的列表的列表，每个实数四舍五入到六位小数，格式如下：\n\"[[x_{1,1}, x_{1,2}], [x_{2,1}, x_{2,2}], [x_{3,1}, x_{3,2}], [x_{4,1}, x_{4,2}]]\"。",
            "solution": "该问题是有效的。它提出了计算神经科学领域一个明确定义的任务，具体是用于神经解码的迭代扩展卡尔曼滤波器（IEKF）的推导和实现。其底层的物理和数学模型，包括泊松发放模型、其高斯近似以及卡尔曼滤波框架，都是标准的且在科学上是合理的。所有参数和约束都已明确指定，构成了一个完整且自洽的问题。\n\n核心任务是在给定一直到当前脉冲计数向量 $y_t \\in \\mathbb{R}^m$ 的观测历史的情况下，求出隐状态 $x_t \\in \\mathbb{R}^n$ 的后验分布。IEKF 提供了一种迭代方法来近似后验均值和协方差。我们从第一性原理出发，将问题置于贝叶斯框架中进行分析。\n\n给定状态的高斯先验，即时间更新步骤的预测结果：\n$$\np(x_t|y_{1:t-1}) = \\mathcal{N}(x_t; x_{t|t-1}, P_{t|t-1})\n$$\n观测模型将隐状态与观测到的脉冲计数联系起来，它是一个泊松点过程，为便于处理，可近似为一个高斯分布：\n$$\np(y_t|x_t) \\approx \\mathcal{N}(y_t; \\lambda(x_t), R(x_t))\n$$\n其中 $\\lambda(x_t)$ 是平均发放率向量（即调谐函数），而 $R(x_t) = \\mathrm{diag}(\\lambda(x_t))$ 是观测噪声协方差，这是根据泊松分布的方差等于其均值的性质得出的。\n\n根据贝叶斯法则，后验分布正比于似然与先验的乘积：\n$$\np(x_t|y_t, y_{1:t-1}) \\propto p(y_t|x_t) p(x_t|y_{1:t-1})\n$$\nIEKF 寻求该后验分布的众数，这等价于寻找 $x_t$ 的最大后验（MAP）估计。这是通过最小化负对数后验 $J(x_t)$ 来实现的：\n$$\n\\hat{x}_{t|t} = \\arg\\max_{x_t} p(x_t|y_{1:t}) = \\arg\\min_{x_t} \\{-\\log p(y_t|x_t) - \\log p(x_t|y_{1:t-1})\\}\n$$\n将先验和似然近似的高斯形式代入，代价函数 $J(x_t)$ 变为（忽略常数项）：\n$$\nJ(x_t) = \\frac{1}{2} (x_t - x_{t|t-1})^\\top P_{t|t-1}^{-1} (x_t - x_{t|t-1}) + \\frac{1}{2} (y_t - \\lambda(x_t))^\\top R(x_t)^{-1} (y_t - \\lambda(x_t)) + \\frac{1}{2} \\log |R(x_t)|\n$$\n由于 $\\lambda(x_t)$ 的非线性和 $R(x_t)$ 的状态依赖性，该函数是非二次函数。IEKF 通过迭代方式最小化此函数。在每次迭代 $i$ 中，从初始猜测 $x^{(0)} = x_{t|t-1}$ 开始，我们在当前估计值 $x^{(i)}$ 周围构建 $J(x_t)$ 的二次近似。这是通过对观测函数 $\\lambda(x_t)$ 进行线性化，并将协方差矩阵 $R(x_t)$ 固定为其在 $x^{(i)}$ 处的值来完成的。\n\n$\\lambda(x_t)$ 在 $x^{(i)}$ 周围的一阶泰勒级数展开为：\n$$\n\\lambda(x_t) \\approx \\lambda(x^{(i)}) + H^{(i)} (x_t - x^{(i)})\n$$\n其中 $H^{(i)} = \\frac{\\partial \\lambda}{\\partial x_t}\\big|_{x_t=x^{(i)}}$ 是雅可比矩阵。我们还固定了局部噪声协方差 $R^{(i)} = R(x^{(i)})$。在迭代内部，$\\frac{1}{2}\\log|R(x_t)|$ 这一项相对于最小化变量 $x_t$ 也被视为常数，这是高斯-牛顿法中的一个标准简化。\n\n将这些代入 $J(x_t)$ 得到一个二次代价函数 $J_i(x_t)$：\n$$\nJ_i(x_t) \\approx \\frac{1}{2} (x_t - x_{t|t-1})^\\top P_{t|t-1}^{-1} (x_t - x_{t|t-1}) + \\frac{1}{2} (y_t - \\lambda(x^{(i)}) - H^{(i)}(x_t-x^{(i)}))^\\top (R^{(i)})^{-1} (y_t - \\lambda(x^{(i)}) - H^{(i)}(x_t-x^{(i)}))\n$$\n下一个迭代值 $x^{(i+1)}$ 是使 $J_i(x_t)$ 最小化的 $x_t$ 的值。我们通过将梯度 $\\nabla_{x_t} J_i(x_t)$ 设为零来找到它：\n$$\n\\nabla_{x_t} J_i(x_t) = P_{t|t-1}^{-1}(x_t - x_{t|t-1}) - (H^{(i)})^\\top(R^{(i)})^{-1}(y_t - \\lambda(x^{(i)}) - H^{(i)}(x_t-x^{(i)})) = 0\n$$\n对 $x_t$ 的项进行分组：\n$$\n\\left(P_{t|t-1}^{-1} + (H^{(i)})^\\top(R^{(i)})^{-1}H^{(i)}\\right) x_t = P_{t|t-1}^{-1}x_{t|t-1} + (H^{(i)})^\\top(R^{(i)})^{-1}(y_t - \\lambda(x^{(i)}) + H^{(i)}x^{(i)})\n$$\n这个方程是一个标准的线性高斯估计问题。其解可以用卡尔曼增益的形式来表示。左侧与 $x_t$ 相乘的项是该线性化子问题中后验协方差的逆，即 $(P^{(i+1)})^{-1}$。使用Woodbury矩阵恒等式，我们可以将 $P^{(i+1)}$ 表示为增益形式 $P^{(i+1)} = (I - K^{(i)}H^{(i)})P_{t|t-1}$，其中卡尔曼增益 $K^{(i)}$ 为：\n$$\nK^{(i)} = P_ {t|t-1}(H^{(i)})^\\top (S^{(i)})^{-1}\n$$\n新息协方差 $S^{(i)}$ 为：\n$$\nS^{(i)} = H^{(i)} P_{t|t-1} (H^{(i)})^\\top + R^{(i)}\n$$\n问题要求为了数值稳定性向 $S^{(i)}$ 添加一个正则化项 $\\varepsilon I$，因此 $S^{(i)} = H^{(i)} P_{t|t-1} (H^{(i)})^\\top + R^{(i)} + \\varepsilon I$。\n\n求解 $x^{(i+1)}$（即 $J_i(x_t)$ 的最小化器）得到更新方程：\n$$\nx^{(i+1)} = x_{t|t-1} + K^{(i)} \\left( y_t - \\lambda(x^{(i)}) - H^{(i)}(x_{t|t-1} - x^{(i)}) \\right)\n$$\n这可以改写为：\n$$\nx^{(i+1)} = x_{t|t-1} + K^{(i)} \\left( (y_t - \\lambda(x^{(i)}) + H^{(i)}x^{(i)}) - H^{(i)}x_{t|t-1} \\right)\n$$\n这与问题中规定的更新规则相匹配，其中 $y_{\\mathrm{lin}}^{(i)} = y_t - \\lambda(x^{(i)}) + H^{(i)}x^{(i)}$ 被识别为线性化测量值。迭代将持续进行，直到状态估计的相对变化 $\\|x^{(i+1)} - x^{(i)}\\|_2 / \\max(1, \\|x^{(i+1)}\\|_2)$ 低于容差 $\\tau$。\n\n在第 $i_{\\text{final}}$ 次迭代收敛后，后验均值为 $x_{t|t} = x^{(i_{\\text{final}})}$。后验协方差 $P_{t|t}$ 使用数值上稳定的 Joseph 形式，并代入最终迭代的增益 $K=K^{(i_{\\text{final}})}$、雅可比矩阵 $H=H^{(i_{\\text{final}})}$ 和噪声协方差 $R=R^{(i_{\\text{final}})}$ 进行计算：\n$$\nP_{t|t} = (I - K H) P_{t|t-1} (I - K H)^\\top + K R K^\\top\n$$\n实现将遵循这个推导出的过程。两个指定调谐函数的雅可比矩阵如下：\n1.  线性调谐，$\\lambda(x) = C x$：雅可比矩阵是常数，$H = C$。\n2.  指数调谐，$\\lambda(x) = \\exp(W x + b)$（按元素）：雅可比矩阵依赖于状态，$H(x) = \\mathrm{diag}(\\lambda(x)) W$。\n\n$R^{(i)}$ 的正则化是通过将 $\\lambda(x^{(i)})$ 的值裁剪到最小值 $\\varepsilon$ 来实现的：$R^{(i)} = \\mathrm{diag}(\\max(\\lambda(x^{(i)}), \\varepsilon))$。\n\n这样就完成了待实现算法的原理推导。",
            "answer": "[[1.205315, 0.725916], [0.111815, -0.063167], [-3.220199, -3.220199], [2.529897, 1.258957]]"
        }
    ]
}