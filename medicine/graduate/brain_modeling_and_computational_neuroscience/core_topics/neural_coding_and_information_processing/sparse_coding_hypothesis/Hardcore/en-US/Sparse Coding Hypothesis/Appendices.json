{
    "hands_on_practices": [
        {
            "introduction": "The sparse coding hypothesis is built on the idea that neural representations are \"sparse.\" To move from this qualitative statement to a testable scientific theory, we must first define sparsity quantitatively. This practice will guide you through deriving a formal sparsity index from a set of intuitive, first-principles requirements, allowing you to build a concrete mathematical understanding of this core concept .",
            "id": "4019306",
            "problem": "A central tenet of the sparse coding hypothesis in brain modeling and computational neuroscience is that natural stimuli are represented by neural activity patterns in which only a small subset of neurons are highly active while most are near silent. To operationalize this idea for population activity, consider a firing-rate vector $x \\in \\mathbb{R}_{\\ge 0}^{n}$, with $n$ neurons and nonnegative components, and define a population sparsity index $S(x)$ that quantifies how concentrated the activity is across neurons. Suppose we require, on first-principles grounds, that $S(x)$ satisfies the following properties: $S(x)$ is invariant under positive rescaling of $x$; $S(x) = 1$ for a maximally sparse pattern in which exactly one component of $x$ is nonzero; $S(x) = 0$ for a maximally dense uniform pattern in which all components of $x$ are equal and nonzero; and $S(x)$ depends monotonically (decreasingly) on the ratio of the $\\ell_{1}$ norm to the $\\ell_{2}$ norm of $x$, with an affine dependence on that ratio chosen to uniquely satisfy the endpoint constraints.\n\nUsing only the definitions of the $\\ell_{1}$ and $\\ell_{2}$ norms and well-tested inequalities from linear algebra (in particular, the Cauchy–Schwarz inequality), derive the unique affine functional form of $S(x)$ consistent with the above requirements. Then, for a population of $n = 9$ neurons whose trial-averaged firing rates (in spikes per second) in response to a single stimulus are measured as\n$$\nx = (0.3,\\ 0.0,\\ 11.2,\\ 0.0,\\ 0.5,\\ 1.1,\\ 0.0,\\ 0.0,\\ 0.0),\n$$\ncompute the value of the sparsity index $S(x)$. Express the final index as a unitless decimal and round your answer to $4$ significant figures.",
            "solution": "The problem requires the derivation of a specific functional form for a population sparsity index, $S(x)$, and its subsequent calculation for a given firing-rate vector $x$. I will first validate the problem statement according to the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   The variable is a firing-rate vector $x \\in \\mathbb{R}_{\\ge 0}^{n}$ for a population of $n$ neurons.\n-   The population sparsity index is denoted by $S(x)$.\n-   Property 1: $S(x)$ is invariant under positive rescaling of $x$, i.e., $S(kx) = S(x)$ for any scalar $k  0$.\n-   Property 2: $S(x) = 1$ for a maximally sparse pattern (exactly one non-zero component).\n-   Property 3: $S(x) = 0$ for a maximally dense uniform pattern (all components are equal and non-zero).\n-   Property 4: $S(x)$ depends monotonically and decreasingly on the ratio $R(x) = \\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}}$.\n-   Property 5: $S(x)$ has an affine dependence on $R(x)$, i.e., $S(x) = a \\cdot R(x) + b$ for some constants $a$ and $b$. The constants are uniquely determined by properties $2$ and $3$.\n-   Data for calculation: $n=9$ and $x = (0.3, 0.0, 11.2, 0.0, 0.5, 1.1, 0.0, 0.0, 0.0)$.\n-   Required output format: A unitless decimal rounded to $4$ significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded within computational neuroscience, specifically relating to the sparse coding hypothesis. The use of $\\ell_p$-norms to construct a sparsity index is a standard and rigorous method. The problem is well-posed. The five properties provide a self-contained and consistent set of constraints to uniquely determine the function $S(x)$. The dependence of $S(x)$ on the ratio $\\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}}$ automatically satisfies the scale-invariance property, as for any $k0$, $\\frac{\\|kx\\|_{\\ell_1}}{\\|kx\\|_{\\ell_2}} = \\frac{k\\|x\\|_{\\ell_1}}{k\\|x\\|_{\\ell_2}} = \\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}}$. The affine form $S(x) = a \\cdot \\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}} + b$ combined with the two endpoint constraints (properties $2$ and $3$) yields a system of two linear equations in the two unknowns $a$ and $b$, which admits a unique solution provided the conditions for the two endpoints are distinct. The problem is also objective and formalizable, with no ambiguity in its terms. It is not trivial and requires a valid derivation. All data are consistent. The problem is therefore deemed **valid**.\n\n### Derivation of the Sparsity Index $S(x)$\n\nThe problem states that $S(x)$ has an affine dependence on the ratio of the $\\ell_1$ norm to the $\\ell_2$ norm:\n$$\nS(x) = a \\left( \\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}} \\right) + b\n$$\nwhere $\\|x\\|_{\\ell_1} = \\sum_{i=1}^{n} |x_i|$ and $\\|x\\|_{\\ell_2} = \\sqrt{\\sum_{i=1}^{n} x_i^2}$. Since $x_i \\ge 0$, we have $\\|x\\|_{\\ell_1} = \\sum_{i=1}^{n} x_i$.\n\nThe constants $a$ and $b$ are determined by the endpoint constraints.\n\n1.  **Maximally Sparse Case**: Let $x_s$ be a vector with exactly one non-zero component, e.g., $x_s = (c, 0, \\dots, 0)$ for some $c0$.\n    The norms are:\n    $$\n    \\|x_s\\|_{\\ell_1} = c\n    $$\n    $$\n    \\|x_s\\|_{\\ell_2} = \\sqrt{c^2 + 0^2 + \\dots + 0^2} = c\n    $$\n    The ratio is $\\frac{\\|x_s\\|_{\\ell_1}}{\\|x_s\\|_{\\ell_2}} = \\frac{c}{c} = 1$.\n    According to Property 2, $S(x_s) = 1$. Substituting into the affine form gives our first equation:\n    $$\n    a \\cdot (1) + b = 1\n    $$\n\n2.  **Maximally Dense Case**: Let $x_d$ be a vector where all $n$ components are equal and non-zero, e.g., $x_d = (c, c, \\dots, c)$ for some $c0$.\n    The norms are:\n    $$\n    \\|x_d\\|_{\\ell_1} = \\sum_{i=1}^{n} c = nc\n    $$\n    $$\n    \\|x_d\\|_{\\ell_2} = \\sqrt{\\sum_{i=1}^{n} c^2} = \\sqrt{nc^2} = c\\sqrt{n}\n    $$\n    The ratio is $\\frac{\\|x_d\\|_{\\ell_1}}{\\|x_d\\|_{\\ell_2}} = \\frac{nc}{c\\sqrt{n}} = \\sqrt{n}$.\n    According to Property 3, $S(x_d) = 0$. This gives our second equation:\n    $$\n    a \\cdot \\sqrt{n} + b = 0\n    $$\n\nThe Cauchy-Schwarz inequality guarantees that the ratio $\\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}}$ lies in the interval $[1, \\sqrt{n}]$, confirming these two cases represent the extrema.\n\nWe now solve the system of linear equations for $a$ and $b$:\n1.  $a + b = 1$\n2.  $a\\sqrt{n} + b = 0$\n\nFrom equation (2), we have $b = -a\\sqrt{n}$. Substituting this into equation (1):\n$$\na - a\\sqrt{n} = 1 \\implies a(1 - \\sqrt{n}) = 1 \\implies a = \\frac{1}{1 - \\sqrt{n}}\n$$\nSince $n1$, $1-\\sqrt{n}  0$, so $a  0$, which correctly reflects the required decreasing monotonic dependence of $S(x)$ on the ratio.\n\nNow, we find $b$:\n$$\nb = 1 - a = 1 - \\frac{1}{1 - \\sqrt{n}} = \\frac{(1 - \\sqrt{n}) - 1}{1 - \\sqrt{n}} = \\frac{-\\sqrt{n}}{1 - \\sqrt{n}} = \\frac{\\sqrt{n}}{\\sqrt{n} - 1}\n$$\n\nSubstituting $a$ and $b$ back into the affine functional form:\n$$\nS(x) = \\left( \\frac{1}{1 - \\sqrt{n}} \\right) \\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}} + \\frac{\\sqrt{n}}{\\sqrt{n} - 1}\n$$\nRewriting with a common denominator of $\\sqrt{n}-1$:\n$$\nS(x) = \\left( \\frac{-1}{\\sqrt{n} - 1} \\right) \\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}} + \\frac{\\sqrt{n}}{\\sqrt{n} - 1}\n$$\n$$\nS(x) = \\frac{\\sqrt{n} - \\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}}}{\\sqrt{n} - 1}\n$$\nThis is the unique functional form for the sparsity index $S(x)$ that satisfies all the given requirements.\n\n### Calculation of $S(x)$ for the Given Data\n\nWe are given a population of $n=9$ neurons with the firing-rate vector:\n$$\nx = (0.3, 0.0, 11.2, 0.0, 0.5, 1.1, 0.0, 0.0, 0.0)\n$$\nFor $n=9$, we have $\\sqrt{n} = \\sqrt{9} = 3$. The formula for $S(x)$ becomes:\n$$\nS(x) = \\frac{3 - \\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}}}{3 - 1} = \\frac{1}{2} \\left( 3 - \\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}} \\right)\n$$\nFirst, we calculate the $\\ell_1$ norm of $x$:\n$$\n\\|x\\|_{\\ell_1} = 0.3 + 0.0 + 11.2 + 0.0 + 0.5 + 1.1 + 0.0 + 0.0 + 0.0 = 13.1\n$$\nNext, we calculate the $\\ell_2$ norm of $x$:\n$$\n\\|x\\|_{\\ell_2}^2 = (0.3)^2 + (0.0)^2 + (11.2)^2 + (0.0)^2 + (0.5)^2 + (1.1)^2 + (0.0)^2 + (0.0)^2 + (0.0)^2\n$$\n$$\n\\|x\\|_{\\ell_2}^2 = 0.09 + 125.44 + 0.25 + 1.21 = 126.99\n$$\n$$\n\\|x\\|_{\\ell_2} = \\sqrt{126.99}\n$$\nNow, we compute the ratio:\n$$\n\\frac{\\|x\\|_{\\ell_1}}{\\|x\\|_{\\ell_2}} = \\frac{13.1}{\\sqrt{126.99}} \\approx 1.16248\n$$\nFinally, we substitute this ratio into the expression for $S(x)$:\n$$\nS(x) = \\frac{1}{2} \\left( 3 - \\frac{13.1}{\\sqrt{126.99}} \\right) \\approx \\frac{1}{2} (3 - 1.16248) = \\frac{1}{2} (1.83752) \\approx 0.91876\n$$\nRounding the result to $4$ significant figures, we get $0.9188$.",
            "answer": "$$\\boxed{0.9188}$$"
        },
        {
            "introduction": "A powerful scientific hypothesis makes experimentally verifiable predictions. This practice puts you in the role of a computational neuroscientist testing the sparse coding hypothesis by simulating neural data and analyzing its statistical properties . You will implement several key tests, including measuring kurtosis and information-theoretic distances, to distinguish between sparse, Laplace-distributed activity and dense, Gaussian-distributed activity.",
            "id": "4019309",
            "problem": "Consider the following generative setting for testing the sparse coding hypothesis in brain modeling and computational neuroscience. Let a stimulus vector $x \\in \\mathbb{R}^{D}$ be generated by a linear generative model with an orthonormal dictionary $A \\in \\mathbb{R}^{D \\times D}$, a latent coefficient vector $s \\in \\mathbb{R}^{D}$, and additive noise $n \\in \\mathbb{R}^{D}$, according to\n$$\nx = A s + n,\n$$\nwhere $A^{\\top} A = I$. Across samples, assume that entries of $s$ are Independent and Identically Distributed (IID), zero-mean, and either drawn from a Gaussian distribution or from a Laplace (double-exponential) distribution, and the noise $n$ is IID zero-mean Gaussian. Define the transformed responses $y = A^{\\top} x = s + \\tilde{n}$ with $\\tilde{n} \\sim \\mathcal{N}(0, \\sigma_{n}^{2} I)$, which preserves orthonormality and transforms the observable data into a coordinate system aligned with the dictionary.\n\nTo operationalize experimental tests of the sparse coding hypothesis, implement three independent statistics on the responses $Y \\in \\mathbb{R}^{D \\times N}$ formed by stacking $N$ samples of $y$ as columns:\n\n(1) Excess kurtosis test. For each unit $i \\in \\{1,\\dots,D\\}$, compute the sample excess kurtosis\n$$\n\\kappa_{i} = \\frac{\\frac{1}{N} \\sum_{n=1}^{N} \\left(y_{i n} - \\bar{y}_{i}\\right)^{4}}{\\left(\\frac{1}{N} \\sum_{n=1}^{N} \\left(y_{i n} - \\bar{y}_{i}\\right)^{2}\\right)^{2}} - 3,\n$$\nwhere $\\bar{y}_{i} = \\frac{1}{N} \\sum_{n=1}^{N} y_{i n}$. Let $\\bar{\\kappa} = \\frac{1}{D} \\sum_{i=1}^{D} \\kappa_{i}$. The sparse coding prediction is that heavy-tailed latent activity yields $\\bar{\\kappa}$ positive and significantly above zero. Declare the kurtosis test positive if $\\bar{\\kappa}  \\tau_{\\kappa}$ with $\\tau_{\\kappa} = 0.5$.\n\n(2) Lifetime sparsity test via the Hoyer measure. For each unit $i$, define the nonnegative response vector $r^{(i)} \\in \\mathbb{R}^{N}$ with entries $r^{(i)}_{n} = |y_{i n}|$. The Hoyer sparsity of $r^{(i)}$ is\n$$\nS_{i} = \\frac{\\sqrt{N} - \\frac{\\|r^{(i)}\\|_{1}}{\\|r^{(i)}\\|_{2}}}{\\sqrt{N} - 1}.\n$$\nLet $\\bar{S} = \\frac{1}{D} \\sum_{i=1}^{D} S_{i}$. The sparse coding prediction is that $\\bar{S}$ is larger for Laplace-like activity than for Gaussian activity. Declare the lifetime sparsity test positive if $\\bar{S}  \\tau_{S}$ with $\\tau_{S} = 0.25$.\n\n(3) Distributional goodness-of-fit via Kullback-Leibler Divergence (KLD). Pool all scalar responses into a single set $\\{y_{i n}\\}$ of size $D N$. Estimate the empirical distribution $p$ by a symmetric histogram over the interval $\\left[-L, L\\right]$, where $L = 5 \\hat{\\sigma}$ and $\\hat{\\sigma}^{2} = \\frac{1}{D N} \\sum_{i,n} \\left(y_{i n} - \\bar{y}\\right)^{2}$ with $\\bar{y} = \\frac{1}{D N} \\sum_{i,n} y_{i n}$. Fit a zero-mean Gaussian model $q_{G}$ with variance parameter $\\sigma^{2}$ by maximum likelihood $\\hat{\\sigma}^{2} = \\frac{1}{D N} \\sum_{i,n} y_{i n}^{2}$ and a zero-mean Laplace model $q_{L}$ with scale parameter $b$ by maximum likelihood $\\hat{b} = \\frac{1}{D N} \\sum_{i,n} |y_{i n}|$. Approximate the KLD from the empirical distribution $p$ to each model by a finite bin sum\n$$\nD_{\\mathrm{KL}}(p \\Vert q) \\approx \\sum_{j=1}^{B} \\hat{p}_{j} \\log \\left(\\frac{\\hat{p}_{j}}{\\hat{q}_{j}}\\right),\n$$\nwhere $B$ is the number of bins, $\\hat{p}_{j}$ is the empirical probability mass in bin $j$, and $\\hat{q}_{j}$ is the model probability mass in bin $j$ obtained by approximating the bin integral via the model probability density function evaluated at the bin center times the bin width and renormalized across bins. Declare the KLD test positive if the Laplace model is closer than the Gaussian model, i.e., if $D_{\\mathrm{KL}}(p \\Vert q_{L}) - D_{\\mathrm{KL}}(p \\Vert q_{G})  0$.\n\nCombine the three tests via a majority rule: declare the sparse coding hypothesis to be supported for a dataset if at least two of the three tests are positive.\n\nProgram requirements:\n\nYou must write a complete program that simulates datasets under the generative model described above for the following test suite and applies the three tests to each dataset:\n\n- Case $1$: $D = 16$, $N = 2000$, Laplace latent coefficients with scale $b = 0.7$, Gaussian noise standard deviation $\\sigma_{n} = 0.1$, random seed $13$.\n- Case $2$: $D = 16$, $N = 2000$, Gaussian latent coefficients with standard deviation $\\sigma_{s} = 1.0$, Gaussian noise standard deviation $\\sigma_{n} = 0.1$, random seed $23$.\n- Case $3$: $D = 16$, $N = 500$, Laplace latent coefficients with scale $b = 0.7$, Gaussian noise standard deviation $\\sigma_{n} = 1.2$, random seed $37$.\n- Case $4$: $D = 16$, $N = 500$, Gaussian latent coefficients with standard deviation $\\sigma_{s} = 1.0$, Gaussian noise standard deviation $\\sigma_{n} = 0.5$, random seed $97$.\n\nIn all cases, draw $A$ as an orthonormal matrix by applying the $QR$ decomposition to a $D \\times D$ matrix with IID standard normal entries and setting $A$ to $Q$. Then simulate $N$ IID samples.\n\nYour program should output a single line containing a list of four boolean values, one per case, indicating whether the majority rule supports the sparse coding hypothesis for that case. The output must be a single line with the format \"[v1,v2,v3,v4]\" where each $v_{k}$ is either True or False. No additional text should be printed.",
            "solution": "The user-provided problem has been analyzed and is deemed valid. It is scientifically grounded in computational neuroscience, well-posed, and all necessary parameters for simulation are provided, with one minor ambiguity. The number of bins, $B$, for the histogram in the Kullback-Leibler Divergence (KLD) test is not specified. To provide a concrete and replicable solution, a reasonable choice must be made for this parameter. I will set $B=100$ for all test cases, a value sufficient to capture the shape of the distributions without being computationally prohibitive. I will proceed with a full solution.\n\nThe problem requires the implementation and application of three statistical tests to validate the sparse coding hypothesis on simulated datasets. The overall procedure involves: 1. Simulating the data according to the specified generative model for each case. 2. Applying the three tests (excess kurtosis, lifetime sparsity, and KLD goodness-of-fit). 3. Aggregating the test results using a majority vote to decide if the sparse coding hypothesis is supported.\n\n### 1. Data Simulation\n\nFor each of the four test cases, a dataset of neural responses is generated following the linear model $x = A s + n$. The components are generated as follows:\n\n*   **Dictionary $A$**: A $D \\times D$ orthonormal matrix is required. This is constructed by first generating a $D \\times D$ matrix of independent and identically distributed (IID) samples from a standard normal distribution, $\\mathcal{N}(0, 1)$. The QR decomposition is then applied to this matrix, and the resulting orthogonal matrix $Q$ is used as the dictionary $A$. This ensures $A^{\\top} A = I$.\n*   **Latent Coefficients $s$**: A set of $N$ latent vectors, each of dimension $D$, are drawn. The entries of these vectors are IID zero-mean samples from either a Laplace distribution with scale $b$ (the \"sparse\" case) or a Gaussian distribution with standard deviation $\\sigma_s$ (the \"non-sparse\" case), as specified for each test case.\n*   **Noise $n$**: A set of $N$ noise vectors, each of dimension $D$, are generated with IID entries drawn from a zero-mean Gaussian distribution with standard deviation $\\sigma_n$.\n*   **Transformed Responses $Y$**: The stimuli are first computed as $x_j = A s_j + n_j$ for each sample $j \\in \\{1, \\dots, N\\}$. The final responses, which are the inputs to our statistical tests, are obtained by transforming the stimuli into the dictionary's coordinate system: $y_j = A^{\\top} x_j$. Since $A$ is orthonormal, this simplifies to $y_j = A^{\\top}(A s_j + n_j) = A^{\\top}A s_j + A^{\\top}n_j = s_j + \\tilde{n}_j$, where $\\tilde{n}_j = A^{\\top}n_j$. Because rotations of IID Gaussian vectors remain IID Gaussian vectors with the same variance, the transformed noise $\\tilde{n}$ is distributed as $\\mathcal{N}(0, \\sigma_n^2 I)$. The resulting $N$ response vectors are collected as columns in a matrix $Y \\in \\mathbb{R}^{D \\times N}$.\n\n### 2. Statistical Tests\n\nThe data matrix $Y$ is subjected to three independent statistical tests.\n\n#### Test 1: Excess Kurtosis\nKurtosis measures the \"tailedness\" of a distribution. The sparse coding hypothesis predicts that neural activity follows a heavy-tailed distribution (like a Laplace distribution), which has a positive excess kurtosis. The excess kurtosis for each neural unit $i$ is calculated using the sample formula:\n$$\n\\kappa_{i} = \\frac{\\frac{1}{N} \\sum_{n=1}^{N} \\left(y_{i n} - \\bar{y}_{i}\\right)^{4}}{\\left(\\frac{1}{N} \\sum_{n=1}^{N} \\left(y_{i n} - \\bar{y}_{i}\\right)^{2}\\right)^{2}} - 3\n$$\nwhere $y_{in}$ is the response of unit $i$ to stimulus $n$, and $\\bar{y}_i$ is the sample mean for unit $i$. This is the sample form of the fourth standardized moment minus $3$ (the value for a Gaussian distribution).\nThe average excess kurtosis across all $D$ units, $\\bar{\\kappa} = \\frac{1}{D} \\sum_{i=1}^{D} \\kappa_{i}$, is then computed. The test is declared positive if $\\bar{\\kappa}$ exceeds a threshold $\\tau_{\\kappa} = 0.5$. This reflects a significant departure from Gaussianity towards a heavy-tailed distribution. To implement this, we use `scipy.stats.kurtosis` with `bias=True` to match the provided formula for the biased sample kurtosis.\n\n#### Test 2: Lifetime Sparsity (Hoyer Measure)\nSparsity implies that a neuron responds strongly to only a few stimuli. The Hoyer measure quantifies this property for the responses of a single unit over its lifetime (all $N$ samples). For each unit $i$, the measure is defined on the non-negative responses $r^{(i)}_n = |y_{in}|$ as:\n$$\nS_{i} = \\frac{\\sqrt{N} - \\frac{\\|r^{(i)}\\|_{1}}{\\|r^{(i)}\\|_{2}}}{\\sqrt{N} - 1}\n$$\nwhere $\\|\\cdot\\|_1$ and $\\|\\cdot\\|_2$ are the L1 and L2 norms, respectively. This measure ranges from $0$ (for a completely dense, uniform vector) to $1$ (for a maximally sparse vector with only one non-zero entry). The average sparsity $\\bar{S} = \\frac{1}{D} \\sum_{i=1}^{D} S_{i}$ is calculated. The test is positive if $\\bar{S}  \\tau_{S}$, with $\\tau_{S} = 0.25$.\n\n#### Test 3: Distributional Goodness-of-Fit via Kullback-Leibler Divergence (KLD)\nThis test directly compares how well the empirical distribution of responses fits a Laplace model versus a Gaussian model.\nFirst, all $D \\times N$ responses $\\{y_{in}\\}$ are pooled into a single dataset. An empirical probability distribution, $p$, is estimated using a histogram with $B=100$ bins over a symmetric range $[-L, L]$, where $L = 5\\hat{\\sigma}$ and $\\hat{\\sigma}$ is the standard deviation of the pooled data. The bin counts are normalized to yield empirical probabilities $\\hat{p}_j$.\nNext, two candidate models are fitted to the data using Maximum Likelihood Estimation (MLE):\n1.  A zero-mean Gaussian model, $q_G$, with variance $\\hat{\\sigma}_{G}^2 = \\frac{1}{DN} \\sum_{i,n} y_{in}^2$.\n2.  A zero-mean Laplace model, $q_L$, with scale parameter $\\hat{b}_{L} = \\frac{1}{DN} \\sum_{i,n} |y_{in}|$.\n\nFor each model, a discrete probability distribution $\\hat{q}$ is constructed over the same histogram bins. The probability for bin $j$, $\\hat{q}_j$, is calculated by evaluating the model's probability density function (PDF) at the bin's center, and then renormalizing these values across all bins to ensure they sum to $1$.\nThe KLD from the empirical distribution $p$ to each model distribution $q$ is then approximated by the sum:\n$$\nD_{\\mathrm{KL}}(p \\Vert q) \\approx \\sum_{j=1}^{B} \\hat{p}_{j} \\log \\left(\\frac{\\hat{p}_{j}}{\\hat{q}_{j}}\\right)\n$$\nThis quantity measures the \"distance\" from the model to the true data distribution. The KLD test is declared positive if the Laplace model is a better fit, i.e., has a lower KLD than the Gaussian model: $D_{\\mathrm{KL}}(p \\Vert q_L)  D_{\\mathrm{KL}}(p \\Vert q_G)$.\n\n### 3. Majority Rule\nThe final verdict for each dataset is determined by a majority vote. The sparse coding hypothesis is considered supported if at least two of the three tests (Kurtosis, Hoyer Sparsity, KLD) are positive. This provides a robust conclusion by not relying on a single measure, which might be sensitive to specific data characteristics.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kurtosis, entropy\nfrom scipy.linalg import qr\n\ndef run_simulation_case(D, N, latent_dist, latent_param, sigma_n, seed):\n    \"\"\"\n    Runs a single simulation case for testing the sparse coding hypothesis.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Data Generation\n    # Generate orthonormal dictionary A\n    A_rand = rng.standard_normal((D, D))\n    A, _ = qr(A_rand)\n\n    # Generate latent coefficients S\n    if latent_dist == 'laplace':\n        # For numpy.random.laplace, scale parameter is b\n        s = rng.laplace(loc=0.0, scale=latent_param, size=(D, N))\n    elif latent_dist == 'gaussian':\n        # For numpy.random.normal, scale parameter is standard deviation\n        s = rng.normal(loc=0.0, scale=latent_param, size=(D, N))\n    else:\n        raise ValueError(\"Invalid latent distribution specified.\")\n\n    # Generate noise n\n    n = rng.normal(loc=0.0, scale=sigma_n, size=(D, N))\n\n    # Compute transformed responses Y\n    # x = A @ s + n\n    # y = A.T @ x = A.T @ (A @ s + n) = s + A.T @ n\n    # We can directly compute y = s + A.T @ n\n    # However, to be fully faithful to the description x=As+n and y=A.T*x:\n    x = A @ s\n    # stimulus vector x = A s + n\n    x_samples = (A @ s) + n\n    # transformed responses y = A^T x\n    Y = A.T @ x_samples\n    \n    # 2. Apply Statistical Tests\n\n    # Test 1: Excess Kurtosis\n    kappa_i = kurtosis(Y, axis=1, fisher=True, bias=True)\n    kappa_bar = np.mean(kappa_i)\n    kurtosis_test_positive = kappa_bar  0.5\n\n    # Test 2: Lifetime Sparsity (Hoyer Measure)\n    R = np.abs(Y)\n    norm1 = np.sum(R, axis=1)\n    norm2 = np.linalg.norm(R, axis=1)\n    # Handle potential norm2=0 case to avoid division by zero\n    # Although unlikely with noise, it's good practice.\n    # The term norm1/norm2 is set to 0 if norm2 is 0.\n    ratio = np.divide(norm1, norm2, out=np.zeros_like(norm1), where=norm2!=0)\n    S_i = (np.sqrt(N) - ratio) / (np.sqrt(N) - 1)\n    S_bar = np.mean(S_i)\n    sparsity_test_positive = S_bar  0.25\n\n    # Test 3: KLD Goodness-of-Fit\n    y_flat = Y.flatten()\n    DN = y_flat.size\n    y_var_sample = np.var(y_flat) # Sample variance for range\n    L = 5.0 * np.sqrt(y_var_sample)\n    B = 100 # Number of bins\n    \n    # Empirical distribution p\n    hist_range = (-L, L)\n    p_counts, bin_edges = np.histogram(y_flat, bins=B, range=hist_range)\n    # Ensure no division by zero if DN=0, though not possible here.\n    p_hat = p_counts / DN if DN  0 else np.zeros(B)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n\n    # Gaussian model q_G\n    sigma2_mle = np.mean(y_flat**2)\n    sigma_mle = np.sqrt(sigma2_mle) if sigma2_mle  0 else 1e-9\n    # PDF values at bin centers for Gaussian\n    q_G_pdf_vals = (1.0 / (sigma_mle * np.sqrt(2 * np.pi))) * np.exp(-bin_centers**2 / (2 * sigma2_mle))\n    # Renormalize to get discrete probability distribution\n    q_G_hat = q_G_pdf_vals / np.sum(q_G_pdf_vals)\n\n    # Laplace model q_L\n    b_mle = np.mean(np.abs(y_flat))\n    if b_mle == 0: b_mle = 1e-9 # Avoid division by zero\n    # PDF values at bin centers for Laplace\n    q_L_pdf_vals = (1.0 / (2 * b_mle)) * np.exp(-np.abs(bin_centers) / b_mle)\n    # Renormalize\n    q_L_hat = q_L_pdf_vals / np.sum(q_L_pdf_vals)\n    \n    # Calculate KLD\n    # scipy.stats.entropy handles cases where p_hat has zeros.\n    # Add a small epsilon to q_hat to prevent inf if q_hat is 0 and p_hat isn't\n    # In this case, q_hat should never be zero due to model properties, but for robustness:\n    epsilon = 1e-12\n    kld_G = entropy(p_hat, q_G_hat + epsilon)\n    kld_L = entropy(p_hat, q_L_hat + epsilon)\n    kld_test_positive = kld_L  kld_G\n\n    # 3. Majority Rule\n    num_positive_tests = sum([kurtosis_test_positive, sparsity_test_positive, kld_test_positive])\n    \n    return num_positive_tests = 2\n\ndef solve():\n    test_cases = [\n        # (D, N, latent_dist, latent_param, sigma_n, seed)\n        (16, 2000, 'laplace', 0.7, 0.1, 13),\n        (16, 2000, 'gaussian', 1.0, 0.1, 23),\n        (16, 500, 'laplace', 0.7, 1.2, 37),\n        (16, 500, 'gaussian', 1.0, 0.5, 97)\n    ]\n\n    results = []\n    for D, N, latent_dist, latent_param, sigma_n, seed in test_cases:\n        result = run_simulation_case(D, N, latent_dist, latent_param, sigma_n, seed)\n        results.append(result)\n\n    print(f\"[{','.join(map(lambda x: str(x), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After defining and testing for sparsity, the next logical step is to build a model that can find sparse representations of data. This exercise will guide you through the implementation of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), a powerful and widely used solver for the sparse coding objective function . You will also confront a crucial practical challenge in machine learning: selecting the optimal regularization parameter $\\lambda$ by analyzing the trade-off between reconstruction accuracy and solution sparsity.",
            "id": "4019320",
            "problem": "You are given a formalization of the sparse coding hypothesis in the context of brain modeling and computational neuroscience. Consider a linear generative model in which a sensory input vector $x \\in \\mathbb{R}^{m}$ is approximated as a linear combination of columns of a dictionary matrix $D \\in \\mathbb{R}^{m \\times n}$ with coefficients $s \\in \\mathbb{R}^{n}$. The sparse coding hypothesis posits that neural activity $s$ is sparse while still reconstructing $x$ well. A widely accepted convex surrogate for sparsity uses the $\\ell_{1}$ norm. The reconstruction–sparsity tradeoff is captured by an energy that balances reconstruction fidelity against sparsity, controlled by a scalar parameter $\\lambda$.\n\nYour task is to design a program that, for each specified test case, selects a value of $\\lambda$ that balances reconstruction and sparsity by locating the corner of the L-curve via curvature maximization. You must compute the sparse code for a grid of $\\lambda$ values using an optimization method consistent with first principles, and then select $\\lambda$ by maximizing the curvature of the L-curve in log-log space.\n\nFundamental base and definitions:\n- The input $x$ is generated by a linear combination of a small number of dictionary atoms plus noise: $x \\approx D s_{\\mathrm{true}} + \\eta$, where $s_{\\mathrm{true}}$ is sparse and $\\eta$ is noise.\n- The reconstruction error is quantified by the squared $\\ell_{2}$ norm of the residual.\n- The sparse code $s^{\\ast}(\\lambda)$ for a given $\\lambda$ is defined as the minimizer of an energy functional balancing reconstruction and sparsity.\n\nOptimization target:\n- For each $\\lambda$, define the energy\n$$\n\\mathcal{E}(s;\\lambda) \\equiv \\frac{1}{2} \\left\\| x - D s \\right\\|_{2}^{2} + \\lambda \\left\\| s \\right\\|_{1}.\n$$\n- For each $\\lambda$ in a prescribed grid, compute an approximate minimizer $s^{\\ast}(\\lambda)$ of $\\mathcal{E}(s;\\lambda)$ using a convergent proximal gradient method justified by convex analysis (for example, Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), which is a specific instance of proximal gradient descent with Nesterov acceleration).\n\nTradeoff metrics and L-curve:\n- For each $\\lambda$, define the normalized reconstruction error\n$$\nE(\\lambda) \\equiv \\frac{\\left\\| x - D s^{\\ast}(\\lambda) \\right\\|_{2}}{\\left\\| x \\right\\|_{2}},\n$$\nand the regularizer magnitude\n$$\nR(\\lambda) \\equiv \\left\\| s^{\\ast}(\\lambda) \\right\\|_{1}.\n$$\n- Construct the L-curve as the parametric curve\n$$\n\\gamma(\\lambda) \\equiv \\left( \\log\\left( R(\\lambda) + \\varepsilon \\right), \\log\\left( E(\\lambda) + \\varepsilon \\right) \\right),\n$$\nwhere $\\varepsilon$ is a small positive constant to avoid singularities when $R(\\lambda)$ or $E(\\lambda)$ approach zero.\n- The curvature of a twice differentiable planar curve parameterized by $\\lambda$, $\\gamma(\\lambda) = \\left( x(\\lambda), y(\\lambda) \\right)$, is\n$$\n\\kappa(\\lambda) = \\frac{\\left| x'(\\lambda) y''(\\lambda) - y'(\\lambda) x''(\\lambda) \\right|}{\\left( \\left( x'(\\lambda) \\right)^{2} + \\left( y'(\\lambda) \\right)^{2} \\right)^{3/2}}.\n$$\n- Approximate derivatives using finite differences on the $\\lambda$ grid and select the interior grid point $\\lambda^{\\ast}$ that maximizes $\\kappa(\\lambda)$.\n\nGrid for $\\lambda$:\n- For each test case, compute\n$$\n\\lambda_{\\max} \\equiv \\left\\| D^{\\top} x \\right\\|_{\\infty},\n$$\nand set\n$$\n\\lambda_{\\min} \\equiv 10^{-4} \\cdot \\lambda_{\\max}.\n$$\n- Use a grid of $N$ values, uniformly spaced in $\\log$-space from $\\lambda_{\\min}$ to $\\lambda_{\\max}$, where $N = 40$.\n\nAlgorithmic requirements:\n- Use a proximal gradient method with a step size bounded by the inverse of the Lipschitz constant of the gradient of the smooth part of $\\mathcal{E}(s;\\lambda)$. The Lipschitz constant is given by the largest eigenvalue of $D^{\\top} D$, which equals the square of the largest singular value of $D$.\n- Terminate iterations when the relative change in $s$ is below a small tolerance or when a fixed maximum number of iterations is reached.\n- Set $\\varepsilon = 10^{-12}$ in the L-curve definition.\n\nTest suite:\nFor each test case, generate $D$ and $x$ deterministically as follows. Construct $D$ with columns normalized to unit $\\ell_{2}$ norm. Generate $s_{\\mathrm{true}}$ by choosing exactly $k$ indices uniformly at random and placing independent Gaussian coefficients with zero mean and unit variance at those indices; other entries are zero. Generate $x = D s_{\\mathrm{true}} + \\eta$, where $\\eta$ has entries drawn independently from a zero-mean Gaussian distribution with standard deviation $\\sigma$. Use independent pseudorandom generators with the given seeds.\n\n- Test case $1$: $m = 32$, $n = 64$, $k = 4$, $\\sigma = 0.05$, dictionary seed $= 1$, signal seed $= 11$, dictionary not correlated.\n- Test case $2$: $m = 32$, $n = 64$, $k = 8$, $\\sigma = 0.00$, dictionary seed $= 2$, signal seed $= 22$, dictionary not correlated.\n- Test case $3$: $m = 32$, $n = 64$, $k = 3$, $\\sigma = 0.50$, dictionary seed $= 3$, signal seed $= 33$, dictionary not correlated.\n- Test case $4$: $m = 64$, $n = 128$, $k = 6$, $\\sigma = 0.10$, dictionary seed $= 4$, signal seed $= 44$, dictionary weakly correlated by setting each column $d_{j}$ for $j \\geq 2$ to be the unit-norm normalization of $u_{j} + 0.3 \\cdot d_{j-1}$ where $u_{j}$ is independent Gaussian and $d_{1}$ is the normalized first column.\n\nFinal output specification:\n- For each test case, return the selected $\\lambda^{\\ast}$ as a float.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3},\\text{result}_{4}\\right]$).",
            "solution": "The user-provided problem is valid. It is scientifically grounded in the fields of computational neuroscience and convex optimization, presenting a well-posed and objective task. All necessary data, definitions, and constraints are provided, forming a complete and non-contradictory problem statement. The task involves implementing a standard and computationally feasible methodology for hyperparameter selection in sparse coding.\n\n### 1. Principle-Based Design\n\nThe solution is designed around three core principles: the mathematical formulation of sparse coding, the use of a convergent optimization algorithm, and a robust method for hyperparameter selection.\n\n#### 1.1 Sparse Coding as Convex Optimization\n\nThe problem models the generation of a sensory signal $x \\in \\mathbb{R}^{m}$ as a sparse linear combination of features (or \"atoms\") from a dictionary $D \\in \\mathbb{R}^{m \\times n}$. The goal is to recover the sparse coefficients $s \\in \\mathbb{R}^{n}$ given $x$ and $D$. This inverse problem is regularized by promoting sparsity in $s$. The trade-off between reconstructing $x$ accurately and ensuring $s$ is sparse is captured by the energy functional $\\mathcal{E}(s;\\lambda)$:\n$$\n\\mathcal{E}(s;\\lambda) = \\underbrace{\\frac{1}{2} \\left\\| x - D s \\right\\|_{2}^{2}}_{f(s)} + \\underbrace{\\lambda \\left\\| s \\right\\|_{1}}_{g(s)}\n$$\nHere, $f(s)$ is the data fidelity term, which is a smooth, convex, and differentiable function. The term $g(s)$ is the sparsity-promoting regularizer, which is convex but non-differentiable at points where components of $s$ are zero. The problem of finding the optimal sparse code $s^{\\ast}(\\lambda)$ is thus cast as a convex optimization problem:\n$$\ns^{\\ast}(\\lambda) = \\arg\\min_{s \\in \\mathbb{R}^{n}} \\mathcal{E}(s;\\lambda)\n$$\n\n#### 1.2 Proximal Gradient Method (FISTA)\n\nBecause the objective function is a sum of a smooth term $f(s)$ and a non-smooth term $g(s)$, this structure is ideally suited for a proximal gradient method. These methods work by iteratively performing a gradient descent step on the smooth part and then applying the proximal operator of the non-smooth part. The general update rule is:\n$$\ns_{k+1} = \\mathrm{prox}_{\\alpha g}(s_k - \\alpha \\nabla f(s_k))\n$$\nwhere $\\alpha$ is the step size. For our problem, the gradient is $\\nabla f(s) = D^{\\top}(Ds - x)$, and the proximal operator of $g(s) = \\lambda \\|s\\|_1$ is the soft-thresholding function, $S_{\\lambda\\alpha}(\\cdot)$:\n$$\n\\mathrm{prox}_{\\lambda\\alpha \\|\\cdot\\|_1}(z)_i = \\mathrm{sign}(z_i) \\max(|z_i| - \\lambda\\alpha, 0)\n$$\nConvergence is guaranteed if the step size $\\alpha$ satisfies $0  \\alpha \\leq 1/L$, where $L$ is the Lipschitz constant of $\\nabla f(s)$. The problem specifies that $L$ is the largest eigenvalue of $D^{\\top}D$, which is equivalent to the squared largest singular value of $D$.\n\nWe implement the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), an accelerated version of the basic proximal gradient method (often called ISTA). FISTA uses a Nesterov-style momentum term to achieve a faster convergence rate of $\\mathcal{O}(1/k^2)$ compared to ISTA's $\\mathcal{O}(1/k)$. The FISTA updates involve an auxiliary sequence $y_k$ and an update rule for the momentum term.\n\n#### 1.3 L-Curve and Curvature Maximization for Hyperparameter Selection\n\nThe regularization parameter $\\lambda$ controls the balance between reconstruction accuracy and sparsity. A small $\\lambda$ favors reconstruction, potentially leading to a dense solution, while a large $\\lambda$ enforces sparsity at the cost of higher reconstruction error. The L-curve is a standard technique for selecting a suitable $\\lambda$. It is a parametric plot of the regularizer magnitude $R(\\lambda) = \\|s^{\\ast}(\\lambda)\\|_1$ versus the reconstruction error $E(\\lambda) = \\|x - Ds^{\\ast}(\\lambda)\\|_2 / \\|x\\|_2$. For clarity and to spread out the points, this curve is plotted in log-log space:\n$$\n\\gamma(\\lambda) \\equiv (\\log(R(\\lambda) + \\varepsilon), \\log(E(\\lambda) + \\varepsilon))\n$$\nThis curve typically has an \"L\" shape. The corner of the \"L\" represents a point where both the error and the regularizer magnitude are reasonably small, indicating a well-balanced solution. We identify this corner by finding the point of maximum curvature.\n\nSince we compute the curve at a discrete set of $\\lambda$ values, we must approximate the curvature numerically. The curvature $\\kappa$ of a planar curve $(x(t), y(t))$ parameterized by $t$ is given by:\n$$\n\\kappa(t) = \\frac{|x'y'' - y'x''|}{(x'^2 + y'^2)^{3/2}}\n$$\nWe compute the curve points for a logarithmically spaced grid of $\\lambda$ values. Let $p = \\log(\\lambda)$. Since the grid of $p$ is uniform, we can use finite difference formulas to approximate the derivatives of the curve coordinates $X(p) = \\log(R+\\varepsilon)$ and $Y(p) = \\log(E+\\varepsilon)$ with respect to $p$. For an interior point $i$, we use central differences:\n$$\nX'_i \\approx \\frac{X_{i+1} - X_{i-1}}{2h}, \\quad X''_i \\approx \\frac{X_{i+1} - 2X_i + X_{i-1}}{h^2}\n$$\n(and similarly for $Y$), where $h$ is the step size in $p$. When these are substituted into the curvature formula, the step size $h$ cancels out, allowing for a simplified calculation based only on the coordinate values. The $\\lambda$ corresponding to the interior grid point with the highest computed curvature is selected as the optimal value $\\lambda^{\\ast}$.\n\n### 2. Algorithmic Implementation\n\nThe solution is implemented in a modular fashion:\n\n1.  **Data Generation**: A function `generate_data` deterministically creates the dictionary $D$ and signal $x$ for each test case, using the specified parameters and random seeds via `numpy.random.default_rng` for isolated pseudo-random number generation.\n2.  **FISTA Solver**: A function `fista_solver` implements the FISTA algorithm to find $s^{\\ast}(\\lambda)$ for a given $\\lambda$. It computes the Lipschitz constant $L$ and uses the prescribed step size $\\alpha = 1/L$.\n3.  **Main Loop**: For each test case, the main logic:\n    a. Generates the data $D$ and $x$.\n    b. Defines the search grid for $\\lambda$ from $\\lambda_{\\min}$ to $\\lambda_{\\max}$ as specified.\n    c. Iterates through the $\\lambda$ grid, calling the `fista_solver` for each value to obtain $s^{\\ast}(\\lambda)$.\n    d. Computes and stores the corresponding reconstruction error $E(\\lambda)$ and regularizer magnitude $R(\\lambda)$.\n    e. Calculates the curvature at each interior point of the resulting L-curve in log-log space.\n    f. Selects the $\\lambda^{\\ast}$ from the grid that maximizes this curvature.\n4.  **Output**: The final list of selected $\\lambda^{\\ast}$ values is formatted and printed as required.",
            "answer": "```python\nimport numpy as np\n\n# Global constants from the problem specification\nN_LAMBDAS = 40\nEPSILON = 1e-12\nFISTA_MAX_ITER = 1000\nFISTA_TOL = 1e-5\n\ndef soft_threshold(z, T):\n    \"\"\"\n    Soft-thresholding operator S_T(z).\n    \"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - T, 0)\n\ndef generate_data(m, n, k, sigma, dict_seed, sig_seed, correlated_dict):\n    \"\"\"\n    Generates a dictionary D and a signal x deterministically based on test case parameters.\n    \"\"\"\n    rng_dict = np.random.default_rng(dict_seed)\n    D = np.zeros((m, n))\n\n    if not correlated_dict:\n        D = rng_dict.standard_normal(size=(m, n))\n    else:  # Correlated case as specified in Test case 4\n        d1_raw = rng_dict.standard_normal(size=m)\n        D[:, 0] = d1_raw / np.linalg.norm(d1_raw)\n        for j in range(1, n):\n            u_j = rng_dict.standard_normal(size=m)\n            d_j_raw = u_j + 0.3 * D[:, j - 1]\n            D[:, j] = d_j_raw / np.linalg.norm(d_j_raw)\n\n    # All columns must be normalized to unit l2-norm.\n    if not correlated_dict:\n        norms = np.linalg.norm(D, axis=0)\n        D /= norms\n\n    rng_sig = np.random.default_rng(sig_seed)\n    s_true = np.zeros(n)\n    # Choose k random indices for non-zero elements\n    support = rng_sig.choice(n, k, replace=False)\n    # Fill with N(0,1) coefficients\n    s_true[support] = rng_sig.standard_normal(k)\n\n    # Generate signal x = Ds_true + noise\n    noise = rng_sig.normal(0, sigma, size=m)\n    x = D @ s_true + noise\n\n    return D, x\n\ndef fista_solver(D, x, lambda_val, L):\n    \"\"\"\n    Solves the sparse coding problem using the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA).\n    Objective: min_s 0.5 * ||x - Ds||_2^2 + lambda * ||s||_1\n    \"\"\"\n    m, n = D.shape\n    s = np.zeros(n)\n    s_prev = np.zeros(n)\n    y = np.zeros(n)\n    t = 1.0\n    \n    # Step size is bounded by the inverse of the Lipschitz constant\n    alpha = 1.0 / L\n    \n    # Precompute for efficiency\n    Dt = D.T\n    \n    for _ in range(FISTA_MAX_ITER):\n        s_prev = s.copy()\n        \n        grad_f_y = Dt @ (D @ y - x)\n        s = soft_threshold(y - alpha * grad_f_y, alpha * lambda_val)\n        \n        t_prev = t\n        t = (1.0 + np.sqrt(1.0 + 4.0 * t_prev**2)) / 2.0\n        \n        y = s + ((t_prev - 1.0) / t) * (s - s_prev)\n        \n        # Check for convergence based on relative change in s\n        rel_change = np.linalg.norm(s - s_prev) / (np.linalg.norm(s_prev) + 1e-9)\n        if rel_change  FISTA_TOL:\n            break\n            \n    return s\n\ndef find_optimal_lambda(m, n, k, sigma, dict_seed, sig_seed, correlated_dict):\n    \"\"\"\n    Finds the optimal lambda for a single test case by maximizing L-curve curvature.\n    \"\"\"\n    D, x = generate_data(m, n, k, sigma, dict_seed, sig_seed, correlated_dict)\n    \n    # The Lipschitz constant L is the squared largest singular value of D.\n    L = (np.linalg.svd(D, compute_uv=False)[0])**2\n    \n    # Define lambda grid as specified\n    lambda_max = np.linalg.norm(D.T @ x, ord=np.inf)\n    if lambda_max  1e-9: # handle edge case where x is orthogonal to all columns\n        lambda_max = 1.0\n\n    lambda_min = 1e-4 * lambda_max\n    lambda_grid = np.logspace(np.log10(lambda_min), np.log10(lambda_max), N_LAMBDAS)\n    \n    E_vals = []\n    R_vals = []\n    x_norm = np.linalg.norm(x)\n\n    for lambda_val in lambda_grid:\n        s_star = fista_solver(D, x, lambda_val, L)\n        \n        recon_error = np.linalg.norm(x - D @ s_star)\n        E = recon_error / x_norm if x_norm  0 else recon_error\n        R = np.linalg.norm(s_star, 1)\n        \n        E_vals.append(E)\n        R_vals.append(R)\n\n    # Construct the L-curve in log-log space\n    X_curve = np.log(np.array(R_vals) + EPSILON)\n    Y_curve = np.log(np.array(E_vals) + EPSILON)\n    \n    curvatures = []\n    # Calculate curvature for interior points using finite differences\n    for i in range(1, N_LAMBDAS - 1):\n        # First derivatives (unscaled central difference)\n        xp = X_curve[i+1] - X_curve[i-1]\n        yp = Y_curve[i+1] - Y_curve[i-1]\n        \n        # Second derivatives (unscaled central difference)\n        xpp = X_curve[i+1] - 2 * X_curve[i] + X_curve[i-1]\n        ypp = Y_curve[i+1] - 2 * Y_curve[i] + Y_curve[i-1]\n        \n        numerator = np.abs(xp * ypp - yp * xpp)\n        denominator = (xp**2 + yp**2)**1.5\n        \n        kappa = numerator / (denominator + 1e-12) # Add small const to avoid div by zero\n        curvatures.append(kappa)\n    \n    if not curvatures: # Should not happen with N_LAMBDAS=40\n        return lambda_grid[N_LAMBDAS // 2]\n\n    # Find the lambda corresponding to the maximum curvature.\n    # The j-th element of curvatures corresponds to the (j+1)-th point on the L-curve.\n    max_kappa_idx = np.argmax(curvatures)\n    optimal_lambda_idx = max_kappa_idx + 1\n    \n    return lambda_grid[optimal_lambda_idx]\n\ndef solve():\n    test_cases = [\n        # (m, n, k, sigma, dict_seed, sig_seed, correlated_dict)\n        (32, 64, 4, 0.05, 1, 11, False),\n        (32, 64, 8, 0.00, 2, 22, False),\n        (32, 64, 3, 0.50, 3, 33, False),\n        (64, 128, 6, 0.10, 4, 44, True),\n    ]\n\n    results = []\n    for case in test_cases:\n        m, n, k, sigma, dict_seed, sig_seed, correlated_dict = case\n        optimal_lambda = find_optimal_lambda(m, n, k, sigma, dict_seed, sig_seed, correlated_dict)\n        results.append(optimal_lambda)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}