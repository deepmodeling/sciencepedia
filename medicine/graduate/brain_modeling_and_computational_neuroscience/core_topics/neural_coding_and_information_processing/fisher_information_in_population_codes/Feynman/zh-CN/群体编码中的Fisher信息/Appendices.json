{
    "hands_on_practices": [
        {
            "introduction": "应用费雪信息理论的第一步是能够为一个已知的生成模型计算它。这个实践将使用一个神经科学中的标准模型——具有冯·米塞斯调谐曲线的独立泊松神经元群体——来探索群体参数如何塑造整个刺激空间内的信息分布 。通过这个练习，你将学会从一个理论模型出发，计算费雪信息并评估其均匀性，这是理解编码效率如何随刺激变化的基础。",
            "id": "3981055",
            "problem": "考虑一个环形刺激变量 $s \\in [-\\pi,\\pi)$（角度以弧度表示）。一个由 $N$ 个神经元组成的群体，其脉冲计数在给定 $s$ 的条件下是独立的，并遵循泊松分布，其速率函数为 $\\lambda_i(s)$。其中每个神经元 $i$ 都有一个偏好的刺激 $\\mu_i$ 和一个 von Mises 调谐曲线。假设一个同质群体模型，其速率函数为\n$$\n\\lambda_i(s) = b + a \\exp\\left(\\kappa \\cos\\left(s - \\mu_i\\right)\\right),\n$$\n其中 $b \\ge 0$ 是基线放电率（单位任意），$a \\ge 0$ 是调谐幅度，$\\kappa \\ge 0$ 是控制调谐宽度的集中参数。偏好的刺激 $\\mu_i$ 在圆周上均匀分布：\n$$\n\\mu_i = -\\pi + \\frac{2\\pi i}{N}, \\quad i = 0,1,\\ldots,N-1.\n$$\n对于独立的泊松生成模型，将给定 $s$ 时观察到脉冲计数 $\\{k_i\\}_{i=1}^N$ 的似然定义为\n$$\n\\mathcal{L}(s) = \\prod_{i=1}^N \\frac{\\lambda_i(s)^{k_i} e^{-\\lambda_i(s)}}{k_i!}.\n$$\n由该群体携带的关于 $s$ 的 Fisher 信息 $I(s)$ 定义为在真实刺激 $s$ 下，关于脉冲计数分布的期望值：\n$$\nI(s) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial s} \\log \\mathcal{L}(s)\\right)^2 \\middle| s \\right].\n$$\n您的任务是：\n- 仅使用上述基本定义和指定的调谐模型，推导出一个 $I(s)$ 的表达式，该表达式可以对任意给定的 $N$、$b$、$a$ 和 $\\kappa$ 进行数值计算。\n- 实现一个程序，对每个测试用例，在一个由 $M=2048$ 个在 $[-\\pi,\\pi)$（角度以弧度为单位）上均匀间隔的 $s$ 值组成的网格上计算 $I(s)$，在整个网格上汇总 $I(s)$，并通过计算变异系数来量化 $I(s)$ 在 $s$ 上的均匀程度\n$$\n\\mathrm{CV} = \\frac{\\sigma_{I}}{\\mu_{I}},\n$$\n其中 $\\sigma_I$ 是集合 $\\{I(s_m)\\}_{m=1}^M$ 的标准差，$\\mu_I$ 是其均值。如果 $\\mu_I = 0$，则定义 $\\mathrm{CV} = 0$。\n- 确保所有数值输出都是实数（浮点数）。输出中除了弧度之外，不需要其他物理单位。\n\n测试套件：\n- 案例 1：$N=50$, $\\kappa=3.0$, $b=5.0$, $a=20.0$。\n- 案例 2：$N=50$, $\\kappa=20.0$, $b=5.0$, $a=20.0$。\n- 案例 3：$N=4$, $\\kappa=3.0$, $b=5.0$, $a=20.0$。\n- 案例 4：$N=1$, $\\kappa=3.0$, $b=20.0$, $a=2.0$。\n- 案例 5：$N=50$, $\\kappa=3.0$, $b=10.0$, $a=0.0$。\n\n程序要求：\n- 对每个测试用例，按照上述描述为相应的 $I(s)$ 计算 $\\mathrm{CV}$。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个浮点数四舍五入到六位小数（例如，$[x_1,x_2,x_3,x_4,x_5]$）。",
            "solution": "该问题经评估有效。它在科学上基于标准的计算神经科学模型，在数学上是适定的、客观的，并包含唯一解所需的所有必要信息。该任务要求对一个指定的神经群体模型，推导并数值实现一个标准量，即 Fisher 信息。\n\nFisher 信息 $I(s)$ 的推导始于给定刺激 $s$ 时观察到脉冲计数 $\\{k_i\\}_{i=0}^{N-1}$ 的对数似然。一个由 $N$ 个具有独立泊松脉冲发放的神经元组成的群体的似然函数由下式给出：\n$$\n\\mathcal{L}(s) = \\prod_{i=0}^{N-1} \\frac{\\lambda_i(s)^{k_i} e^{-\\lambda_i(s)}}{k_i!}.\n$$\n因此，对数似然为\n$$\n\\log \\mathcal{L}(s) = \\sum_{i=0}^{N-1} \\left( k_i \\log \\lambda_i(s) - \\lambda_i(s) - \\log(k_i!) \\right).\n$$\nFisher 信息定义为得分（score）平方的期望值，得分是对数似然关于参数 $s$ 的导数：\n$$\nI(s) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial s} \\log \\mathcal{L}(s)\\right)^2 \\middle| s \\right].\n$$\n首先，我们计算得分：\n$$\n\\frac{\\partial}{\\partial s} \\log \\mathcal{L}(s) = \\frac{\\partial}{\\partial s} \\sum_{i=0}^{N-1} \\left( k_i \\log \\lambda_i(s) - \\lambda_i(s) - \\log(k_i!) \\right).\n$$\n逐项求导得出\n$$\n\\frac{\\partial}{\\partial s} \\log \\mathcal{L}(s) = \\sum_{i=0}^{N-1} \\left( k_i \\frac{1}{\\lambda_i(s)}\\frac{\\partial \\lambda_i(s)}{\\partial s} - \\frac{\\partial \\lambda_i(s)}{\\partial s} \\right) = \\sum_{i=0}^{N-1} \\left( \\frac{k_i - \\lambda_i(s)}{\\lambda_i(s)} \\right) \\lambda_i'(s),\n$$\n其中 $\\lambda_i'(s)$ 表示 $\\lambda_i(s)$ 关于 $s$ 的导数。\n\n接下来，我们将得分平方：\n$$\n\\left(\\frac{\\partial}{\\partial s} \\log \\mathcal{L}(s)\\right)^2 = \\left( \\sum_{i=0}^{N-1} \\frac{\\lambda_i'(s)}{\\lambda_i(s)} (k_i - \\lambda_i(s)) \\right)^2 = \\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1} \\frac{\\lambda_i'(s)\\lambda_j'(s)}{\\lambda_i(s)\\lambda_j(s)} (k_i - \\lambda_i(s))(k_j - \\lambda_j(s)).\n$$\n现在，我们求该量关于刺激 $s$ 下脉冲计数分布的期望。该期望以 $s$ 为条件，在此条件下，脉冲计数 $k_i$ 是独立的泊松随机变量，其均值和方差均等于 $\\lambda_i(s)$。因此，$\\mathbb{E}[k_i|s] = \\lambda_i(s)$ 且 $\\mathbb{E}[(k_i-\\lambda_i(s))^2|s] = \\mathrm{Var}(k_i|s) = \\lambda_i(s)$。由于独立性，对于 $i \\neq j$，交叉项的期望为零：\n$$\n\\mathbb{E}[(k_i - \\lambda_i(s))(k_j - \\lambda_j(s)) | s] = \\mathbb{E}[k_i - \\lambda_i(s) | s] \\mathbb{E}[k_j - \\lambda_j(s) | s] = (\\lambda_i(s) - \\lambda_i(s))(\\lambda_j(s) - \\lambda_j(s)) = 0.\n$$\n因此，在求期望后，双重求和中只有对角项（$i=j$）保留下来：\n$$\nI(s) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial s} \\log \\mathcal{L}(s)\\right)^2 \\middle| s \\right] = \\sum_{i=0}^{N-1} \\frac{(\\lambda_i'(s))^2}{(\\lambda_i(s))^2} \\mathbb{E}[(k_i - \\lambda_i(s))^2 | s].\n$$\n代入 $\\mathbb{E}[(k_i - \\lambda_i(s))^2 | s] = \\lambda_i(s)$，我们得到独立泊松群体的 Fisher 信息的一般公式：\n$$\nI(s) = \\sum_{i=0}^{N-1} \\frac{(\\lambda_i'(s))^2}{\\lambda_i(s)}.\n$$\n现在我们将此结果应用于指定的 von Mises 调谐模型。神经元 $i$ 的放电率为\n$$\n\\lambda_i(s) = b + a \\exp\\left(\\kappa \\cos\\left(s - \\mu_i\\right)\\right).\n$$\n其关于 $s$ 的导数使用链式法则求得：\n$$\n\\lambda_i'(s) = \\frac{\\partial}{\\partial s} \\left( b + a \\exp(\\kappa \\cos(s - \\mu_i)) \\right) = a \\exp(\\kappa \\cos(s - \\mu_i)) \\cdot (\\kappa (-\\sin(s - \\mu_i)))\n$$\n$$\n\\lambda_i'(s) = -a \\kappa \\sin(s - \\mu_i) \\exp(\\kappa \\cos(s - \\mu_i)).\n$$\n将 $\\lambda_i(s)$ 和 $\\lambda_i'(s)$ 代入 $I(s)$ 的公式，得到用于数值计算的最终表达式：\n$$\nI(s) = \\sum_{i=0}^{N-1} \\frac{\\left( -a \\kappa \\sin(s - \\mu_i) \\exp(\\kappa \\cos(s - \\mu_i)) \\right)^2}{b + a \\exp(\\kappa \\cos(s - \\mu_i))}\n$$\n$$\nI(s) = a^2 \\kappa^2 \\sum_{i=0}^{N-1} \\frac{\\sin^2(s - \\mu_i) \\exp(2\\kappa \\cos(s - \\mu_i))}{b + a \\exp(\\kappa \\cos(s - \\mu_i))}.\n$$\n为完成任务，此 $I(s)$ 表达式在一个由 $M = 2048$ 个在 $[-\\pi, \\pi)$ 上均匀间隔的刺激值 $s_m$ 组成的离散网格上进行评估。设这些计算值的集合为 $\\{I(s_m)\\}_{m=0}^{M-1}$。我们从此集合中计算均值 $\\mu_I$ 和标准差 $\\sigma_I$。然后变异系数计算为 $\\mathrm{CV} = \\sigma_I / \\mu_I$。如果 $\\mu_I = 0$，例如在 $a=0$ 的情况下，我们定义 $\\mathrm{CV} = 0$。算法实现将对每个提供的测试用例执行这些计算。这涉及对刺激网格和神经元群体上的 $I(s)$ 计算进行矢量化以提高效率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the coefficient of variation of the Fisher information for a population\n    of neurons with von Mises tuning curves and independent Poisson spiking.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple is (N, kappa, b, a)\n    test_cases = [\n        (50, 3.0, 5.0, 20.0),\n        (50, 20.0, 5.0, 20.0),\n        (4, 3.0, 5.0, 20.0),\n        (1, 3.0, 20.0, 2.0),\n        (50, 3.0, 10.0, 0.0),\n    ]\n\n    # Number of stimulus points for the grid\n    M = 2048\n    # Create the uniform grid of stimulus values s in [-pi, pi)\n    s_grid = np.linspace(-np.pi, np.pi, M, endpoint=False)\n    \n    results = []\n    \n    for case in test_cases:\n        N, kappa, b, a = case\n\n        # Handle the trivial case where tuning amplitude is zero.\n        # If a=0, the firing rates are constant (lambda_i(s) = b),\n        # so their derivatives are zero, making I(s) = 0 for all s.\n        # This results in mu_I = 0 and sigma_I = 0, so CV = 0 by definition.\n        if a == 0.0:\n            results.append(0.0)\n            continue\n\n        # Create the uniform grid of preferred stimuli mu_i in [-pi, pi)\n        # mu_i = -pi + (2*pi*i)/N for i=0,...,N-1\n        mu_grid = np.linspace(-np.pi, np.pi, int(N), endpoint=False)\n\n        # --- Vectorized Calculation ---\n        # Use numpy broadcasting to efficiently compute over the s and mu grids.\n        # s_grid shape: (M, 1), mu_grid shape: (1, N)\n        # The result of subtraction `args` will have shape (M, N).\n        args = s_grid[:, np.newaxis] - mu_grid[np.newaxis, :]\n        \n        cos_args = np.cos(args)\n        sin_args = np.sin(args)\n        \n        # Exponential term from the von Mises tuning curve. Shape: (M, N)\n        exp_term = np.exp(kappa * cos_args)\n        \n        # Firing rates lambda_i(s) for all neurons and stimuli. Shape: (M, N)\n        lambda_vals = b + a * exp_term\n        \n        # Derivatives lambda_i'(s). Shape: (M, N)\n        lambda_prime_vals = -a * kappa * sin_args * exp_term\n        \n        # Individual terms in the sum for Fisher information, (lambda_i'(s))^2 / lambda_i(s).\n        # Shape: (M, N)\n        # Since b, a >= 0, and exp() > 0, lambda_vals will be positive unless a=0 (handled)\n        # or b=0 and exp() underflows, which is unlikely with standard float64.\n        fisher_info_terms = (lambda_prime_vals**2) / lambda_vals\n        \n        # Sum over all neurons (axis=1) to get the total Fisher information I(s)\n        # for each stimulus s. Shape: (M,)\n        I_s_grid = np.sum(fisher_info_terms, axis=1)\n\n        # --- Coefficient of Variation Calculation ---\n        # Calculate the mean of the Fisher information over the stimulus grid\n        mu_I = np.mean(I_s_grid)\n        \n        # Per problem specification, if mean is 0, CV is 0.\n        if mu_I == 0.0:\n            cv = 0.0\n        else:\n            # Calculate the standard deviation\n            sigma_I = np.std(I_s_grid)\n            # Compute the coefficient of variation\n            cv = sigma_I / mu_I\n            \n        results.append(cv)\n\n    # Format the results to six decimal places as required.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Print the final output in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "真实的神经元群体活动常常表现出相关性，这会显著影响编码保真度。这个练习通过一个名为“试次重排”（trial shuffling）的假设性实验来探讨这一核心概念 。你的任务是分析特定的噪声相关性结构如何限制信息编码，这是解读真实神经数据的一个关键概念，其中噪声相关性无处不在。",
            "id": "4163132",
            "problem": "一个实验室记录了一个包含 $N$ 个感觉神经元的群体对一维刺激参数 $\\theta$ 的响应。在每次试验中，群体响应由一个随机向量 $\\mathbf{r} \\in \\mathbb{R}^N$ 表示，并且给定 $\\theta$ 时 $\\mathbf{r}$ 的条件分布被建模为均值为 $\\boldsymbol{\\mu}(\\theta)$、协方差与 $\\theta$ 无关的 $\\boldsymbol{\\Sigma}$ 的多元高斯分布，即 $p(\\mathbf{r} \\mid \\theta) = \\mathcal{N}(\\boldsymbol{\\mu}(\\theta), \\boldsymbol{\\Sigma})$。目标是使用 Fisher 信息 (FI) 来量化该群体编码的敏感性，其定义为 $J(\\theta) = \\mathbb{E}\\big[\\big(\\partial_{\\theta} \\log p(\\mathbf{r} \\mid \\theta)\\big)^2\\big]$，特别是分析在这种情况下常用的线性 Fisher 信息 $I_{\\text{lin}}(\\theta)$。\n\n为了探究噪声相关性在编码中的作用，该实验室应用了试次重排（trial shuffling）：在相同 $\\theta$ 的重复呈现中，对每个神经元的试验进行独立置换，以构建一个保留每个神经元单次试验统计特性但破坏神经元间相关性的伪群体。假设原始协方差具有秩-1 相关形式\n$$\n\\boldsymbol{\\Sigma} \\;=\\; \\sigma^2 \\mathbf{I} \\;+\\; \\lambda \\,\\mathbf{v}\\mathbf{v}^\\top,\n$$\n其中 $\\sigma^2 > 0$, $\\lambda > 0$, $\\|\\mathbf{v}\\|_2 = 1$，$\\mathbf{I}$ 是单位矩阵。试次重排产生一个有效协方差\n$$\n\\boldsymbol{\\Sigma}_{\\text{shuf}} \\;=\\; \\sigma^2 \\mathbf{I},\n$$\n同时保持 $\\boldsymbol{\\mu}(\\theta)$ 不变。令 $\\mathbf{g}(\\theta) = \\partial_{\\theta} \\boldsymbol{\\mu}(\\theta)$ 表示群体调谐斜率（“信号方向”）。\n\n使用 Fisher 信息 $J(\\theta)$ 的基本定义和所述的生成模型，分析试次重排如何影响 $I_{\\text{lin}}(\\theta)$ 的估计，以及这对神经编码中相关性的作用意味着什么。以下哪些陈述是正确的？\n\nA. 在所述的 $\\boldsymbol{\\Sigma}$ 下，只要 $\\mathbf{v}$ 在 $\\mathbf{g}(\\theta)$ 上有非零投影，试次重排就会增加 $I_{\\text{lin}}(\\theta)$。\n\nB. 如果 $\\mathbf{v} \\perp \\mathbf{g}(\\theta)$，试次重排不改变 $I_{\\text{lin}}(\\theta)$。\n\nC. 观察到试次重排后 $I_{\\text{lin}}(\\theta)$ 增加，是相关性对该编码起信息限制作用的证据。\n\nD. 对于任何群体和任何相关结构，试次重排总是会增加 $I_{\\text{lin}}(\\theta)$。",
            "solution": "首先对问题陈述进行验证。\n\n### 第 1 步：提取已知条件\n- 一个包含 $N$ 个感觉神经元的群体对刺激 $\\theta$ 作出响应。\n- 群体响应是一个随机向量 $\\mathbf{r} \\in \\mathbb{R}^N$。\n- 响应的条件概率分布是一个多元高斯分布：$p(\\mathbf{r} \\mid \\theta) = \\mathcal{N}(\\boldsymbol{\\mu}(\\theta), \\boldsymbol{\\Sigma})$。\n- 协方差矩阵 $\\boldsymbol{\\Sigma}$ 与刺激 $\\theta$ 无关。\n- Fisher 信息的定义为 $J(\\theta) = \\mathbb{E}\\big[\\big(\\partial_{\\theta} \\log p(\\mathbf{r} \\mid \\theta)\\big)^2\\big]$。分析涉及线性 Fisher 信息 $I_{\\text{lin}}(\\theta)$。\n- 原始协方差矩阵被指定为 $\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I} + \\lambda \\,\\mathbf{v}\\mathbf{v}^\\top$，其中 $\\sigma^2 > 0$，$\\lambda > 0$，$\\|\\mathbf{v}\\|_2 = 1$，且 $\\mathbf{I}$ 是 $N \\times N$ 单位矩阵。\n- 定义了一个“试次重排”过程，其产生一个有效的重排协方差矩阵 $\\boldsymbol{\\Sigma}_{\\text{shuf}} = \\sigma^2 \\mathbf{I}$。\n- 平均响应向量 $\\boldsymbol{\\mu}(\\theta)$ 不受此过程影响。\n- 群体调谐斜率定义为 $\\mathbf{g}(\\theta) = \\partial_{\\theta} \\boldsymbol{\\mu}(\\theta)$。\n\n### 第 2 步：使用提取的已知条件进行验证\n- **科学依据：** 该问题在理论神经科学中有充分的依据。使用多元高斯模型、Fisher 信息和试次重排来研究群体编码和噪声相关性是该领域的标准方法。\n- **良置性：** 问题是良置的。对于具有与刺激无关的协方差的高斯模型，一般的 Fisher 信息 $J(\\theta)$ 等价于线性 Fisher 信息 $I_{\\text{lin}}(\\theta) = (\\partial_{\\theta} \\boldsymbol{\\mu}(\\theta))^\\top \\boldsymbol{\\Sigma}^{-1} (\\partial_{\\theta} \\boldsymbol{\\mu}(\\theta))$。参数的定义使得协方差矩阵 $\\boldsymbol{\\Sigma}$ 是正定的：其特征值为 $\\sigma^2 + \\lambda$ 和 $\\sigma^2$（重数为 $N-1$），在 $\\sigma^2 > 0$ 和 $\\lambda > 0$ 的条件下，这两个特征值都是严格为正的。因此，$\\boldsymbol{\\Sigma}$ 是可逆的。\n- **客观性与一致性：** 问题以客观的数学术语陈述。“试次重排”结果 $\\boldsymbol{\\Sigma}_{\\text{shuf}} = \\sigma^2 \\mathbf{I}$ 的定义是一个公理化的简化。一个更符合物理实际的试次重排模型会保留原始协方差矩阵的对角元素，从而得到 $\\boldsymbol{\\Sigma}_{\\text{shuf}}' = \\text{diag}(\\sigma^2+\\lambda v_1^2, \\ldots, \\sigma^2+\\lambda v_N^2)$。然而，问题明确地*定义*了重排的结果为 $\\sigma^2 \\mathbf{I}$。这不是一个矛盾，而是该数学练习的一个具体前提，尽管是一个简化的前提。该问题是在由 $\\boldsymbol{\\Sigma}$ 和 $\\boldsymbol{\\Sigma}_{\\text{shuf}}$ 描述的两个系统之间进行有效比较。\n\n### 第 3 步：结论与行动\n问题陈述在理论建模的背景下是内部一致且有科学依据的。它被认为是**有效的**。我们可以开始求解。\n\n### 推导\n\n对于响应服从多元高斯分布 $p(\\mathbf{r} \\mid \\theta) = \\mathcal{N}(\\boldsymbol{\\mu}(\\theta), \\boldsymbol{\\Sigma})$（其中 $\\boldsymbol{\\Sigma}$ 与 $\\theta$ 无关）的神经元群体，其线性 Fisher 信息 (FI) 由以下公式给出：\n$$\nI_{\\text{lin}}(\\theta) = \\mathbf{g}(\\theta)^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{g}(\\theta)\n$$\n其中 $\\mathbf{g}(\\theta) = \\partial_{\\theta} \\boldsymbol{\\mu}(\\theta)$ 是平均响应对刺激参数的导数。为简洁起见，我们将省略对 $\\theta$ 的显式依赖，写作 $\\mathbf{g}$ 和 $I_{\\text{lin}}$。\n\n首先，我们计算重排情况下的 Fisher 信息 $I_{\\text{lin, shuf}}$。协方差矩阵为 $\\boldsymbol{\\Sigma}_{\\text{shuf}} = \\sigma^2 \\mathbf{I}$。其逆矩阵很简单：\n$$\n\\boldsymbol{\\Sigma}_{\\text{shuf}}^{-1} = (\\sigma^2 \\mathbf{I})^{-1} = \\frac{1}{\\sigma^2} \\mathbf{I}\n$$\n将其代入 FI 公式：\n$$\nI_{\\text{lin, shuf}} = \\mathbf{g}^\\top \\left(\\frac{1}{\\sigma^2} \\mathbf{I}\\right) \\mathbf{g} = \\frac{1}{\\sigma^2} (\\mathbf{g}^\\top \\mathbf{g}) = \\frac{\\|\\mathbf{g}\\|_2^2}{\\sigma^2}\n$$\n这表示一个编码中的信息，其中每个神经元只有方差为 $\\sigma^2$ 的独立噪声。\n\n接下来，我们计算原始相关情况下的 Fisher 信息 $I_{\\text{lin}}$。协方差矩阵为 $\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I} + \\lambda \\mathbf{v}\\mathbf{v}^\\top$。为了求其逆，我们使用 Sherman-Morrison 公式计算秩-1 更新的逆：$(\\mathbf{A} + \\mathbf{u}\\mathbf{w}^\\top)^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{w}^\\top\\mathbf{A}^{-1}}{1 + \\mathbf{w}^\\top\\mathbf{A}^{-1}\\mathbf{u}}$。\n这里，我们确定 $\\mathbf{A} = \\sigma^2 \\mathbf{I}$，$\\mathbf{u} = \\lambda\\mathbf{v}$，以及 $\\mathbf{w} = \\mathbf{v}$。\n$\\mathbf{A}$ 的逆是 $\\mathbf{A}^{-1} = \\frac{1}{\\sigma^2} \\mathbf{I}$。\n分母项是 $1 + \\mathbf{v}^\\top (\\frac{1}{\\sigma^2}\\mathbf{I})(\\lambda\\mathbf{v}) = 1 + \\frac{\\lambda}{\\sigma^2} \\mathbf{v}^\\top\\mathbf{v}$。由于 $\\|\\mathbf{v}\\|_2=1$，我们有 $\\mathbf{v}^\\top\\mathbf{v} = 1$，所以分母是 $1 + \\frac{\\lambda}{\\sigma^2} = \\frac{\\sigma^2+\\lambda}{\\sigma^2}$。\n分子矩阵是 $\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{w}^\\top\\mathbf{A}^{-1} = (\\frac{1}{\\sigma^2}\\mathbf{I})(\\lambda\\mathbf{v})\\mathbf{v}^\\top(\\frac{1}{\\sigma^2}\\mathbf{I}) = \\frac{\\lambda}{\\sigma^4}\\mathbf{v}\\mathbf{v}^\\top$。\n结合这些部分，逆协方差矩阵是：\n$$\n\\boldsymbol{\\Sigma}^{-1} = \\frac{1}{\\sigma^2}\\mathbf{I} - \\frac{\\frac{\\lambda}{\\sigma^4}\\mathbf{v}\\mathbf{v}^\\top}{\\frac{\\sigma^2+\\lambda}{\\sigma^2}} = \\frac{1}{\\sigma^2}\\mathbf{I} - \\frac{\\lambda}{\\sigma^2(\\sigma^2+\\lambda)}\\mathbf{v}\\mathbf{v}^\\top\n$$\n现在，我们计算 $I_{\\text{lin}}$：\n$$\nI_{\\text{lin}} = \\mathbf{g}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{g} = \\mathbf{g}^\\top \\left( \\frac{1}{\\sigma^2}\\mathbf{I} - \\frac{\\lambda}{\\sigma^2(\\sigma^2+\\lambda)}\\mathbf{v}\\mathbf{v}^\\top \\right) \\mathbf{g}\n$$\n$$\nI_{\\text{lin}} = \\frac{1}{\\sigma^2}\\mathbf{g}^\\top\\mathbf{g} - \\frac{\\lambda}{\\sigma^2(\\sigma^2+\\lambda)}\\mathbf{g}^\\top\\mathbf{v}\\mathbf{v}^\\top\\mathbf{g}\n$$\n认识到 $\\mathbf{g}^\\top\\mathbf{g} = \\|\\mathbf{g}\\|_2^2$ 和 $\\mathbf{g}^\\top\\mathbf{v}\\mathbf{v}^\\top\\mathbf{g} = (\\mathbf{g}^\\top\\mathbf{v})^2$，我们有：\n$$\nI_{\\text{lin}} = \\frac{\\|\\mathbf{g}\\|_2^2}{\\sigma^2} - \\frac{\\lambda}{\\sigma^2(\\sigma^2+\\lambda)}(\\mathbf{g}^\\top\\mathbf{v})^2\n$$\n第一项恰好是 $I_{\\text{lin, shuf}}$。因此，原始 FI 和重排 FI 之间的关系是：\n$$\nI_{\\text{lin}} = I_{\\text{lin, shuf}} - \\frac{\\lambda}{\\sigma^2(\\sigma^2+\\lambda)}(\\mathbf{g}^\\top\\mathbf{v})^2\n$$\n重排后 Fisher 信息的变化量为 $\\Delta I = I_{\\text{lin, shuf}} - I_{\\text{lin}}$：\n$$\n\\Delta I = \\frac{\\lambda}{\\sigma^2(\\sigma^2+\\lambda)}(\\mathbf{g}^\\top\\mathbf{v})^2\n$$\n由于 $\\lambda > 0$ 且 $\\sigma^2 > 0$，系数 $\\frac{\\lambda}{\\sigma^2(\\sigma^2+\\lambda)}$ 是严格为正的。项 $(\\mathbf{g}^\\top\\mathbf{v})^2$ 是非负的。\n因此，$\\Delta I \\ge 0$，这意味着 $I_{\\text{lin, shuf}} \\ge I_{\\text{lin}}$。在此模型下，试次重排永远不会减少 Fisher 信息。\n\n### 逐项分析\n\nA. **在所述的 $\\boldsymbol{\\Sigma}$ 下，只要 $\\mathbf{v}$ 在 $\\mathbf{g}(\\theta)$ 上有非零投影，试次重排就会增加 $I_{\\text{lin}}(\\theta)$。**\n条件“$\\mathbf{v}$ 在 $\\mathbf{g}(\\theta)$ 上有非零投影”等价于点积 $\\mathbf{g}^\\top\\mathbf{v}$ 不为零。如果 $\\mathbf{g}^\\top\\mathbf{v} \\neq 0$，则 $(\\mathbf{g}^\\top\\mathbf{v})^2 > 0$。根据我们推导的 FI 变化量表达式 $\\Delta I = \\frac{\\lambda}{\\sigma^2(\\sigma^2+\\lambda)}(\\mathbf{g}^\\top\\mathbf{v})^2$，非零投影意味着 $\\Delta I > 0$。这意味着 $I_{\\text{lin, shuf}} - I_{\\text{lin}} > 0$，或 $I_{\\text{lin, shuf}} > I_{\\text{lin}}$。因此，试次重排严格增加了 Fisher 信息。\n**结论：正确。**\n\nB. **如果 $\\mathbf{v} \\perp \\mathbf{g}(\\theta)$，试次重排不改变 $I_{\\text{lin}}(\\theta)$。**\n条件 $\\mathbf{v} \\perp \\mathbf{g}(\\theta)$ 意味着向量是正交的，所以它们的点积为零：$\\mathbf{g}^\\top\\mathbf{v} = 0$。将此代入我们 FI 变化量的表达式，得到 $\\Delta I = \\frac{\\lambda}{\\sigma^2(\\sigma^2+\\lambda)}(0)^2 = 0$。这意味着 $I_{\\text{lin, shuf}} - I_{\\text{lin}} = 0$，或 $I_{\\text{lin, shuf}} = I_{\\text{lin}}$。Fisher 信息保持不变。\n**结论：正确。**\n\nC. **观察到试次重排后 $I_{\\text{lin}}(\\theta)$ 增加，是相关性对该编码起信息限制作用的证据。**\n如果相关性的存在与基线情况（在此情境下，是移除了相关性的重排情况）相比减少了 Fisher 信息，那么这种相关性就被认为是“信息限制性的”。观察到的增加意味着 $I_{\\text{lin, shuf}} > I_{\\text{lin}}$。我们的推导表明，当噪声相关性（由 $\\lambda \\mathbf{v}\\mathbf{v}^\\top$ 表示）存在于与信号方向 $\\mathbf{g}$ 不正交的方向 $\\mathbf{v}$ 上时，就会发生这种情况。$I_{\\text{lin}}$ 之所以小于 $I_{\\text{lin, shuf}}$，正是因为 $I_{\\text{lin}}$ 表达式中的负项，该负项源于相关性。因此，观察到这种增加是相关性限制了该编码信息内容的直接证据。\n**结论：正确。**\n\nD. **对于任何群体和任何相关结构，试次重排总是会增加 $I_{\\text{lin}}(\\theta)$。**\n这个陈述对“任何相关结构”做出了一个普遍性的断言。问题中的特定结构 $\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I} + \\lambda \\mathbf{v}\\mathbf{v}^\\top$（其中 $\\lambda>0$）表示沿方向 $\\mathbf{v}$ 的正相关。这种类型的相关性确实是信息限制性或中性的。然而，还存在其他相关结构。考虑一个简单的双神经元系统，信号为 $\\mathbf{g} = [1, 1]^\\top$，协方差矩阵具有负相关：$\\boldsymbol{\\Sigma} = \\begin{pmatrix} 1  -0.8 \\\\ -0.8  1 \\end{pmatrix}$。其逆矩阵为 $\\boldsymbol{\\Sigma}^{-1} = \\frac{1}{1-0.64}\\begin{pmatrix} 1  0.8 \\\\ 0.8  1 \\end{pmatrix} = \\frac{1}{0.36}\\begin{pmatrix} 1  0.8 \\\\ 0.8  1 \\end{pmatrix}$。Fisher 信息为 $I_{\\text{lin}} = [1, 1] \\frac{1}{0.36}\\begin{pmatrix} 1  0.8 \\\\ 0.8  1 \\end{pmatrix} [1, 1]^\\top = \\frac{1}{0.36}(1+0.8+0.8+1) = \\frac{3.6}{0.36} = 10$。重排后，协方差变为 $\\boldsymbol{\\Sigma}_{\\text{shuf}}=\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$，且 $I_{\\text{lin, shuf}} = [1, 1] \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} [1, 1]^\\top = 1+1=2$。在这种情况下，$I_{\\text{lin}} > I_{\\text{lin, shuf}}$，意味着重排*减少*了信息。这种相关性被称为“信息增强性”。因此，该陈述是错误的。\n**结论：不正确。**",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "在真实的实验中，我们通常不知道神经元反应的精确数学模型。因此，我们必须从收集到的数据中估计费雪信息。这个最终的实践模拟了这一现实情境，指导你使用线性回归等统计工具来估计费雪信息公式中的关键组成部分，从而完成从原始数据到信息量化的完整分析流程 。",
            "id": "3981087",
            "problem": "考虑一个神经群体编码，其中标量刺激 $s \\in \\mathbb{R}$ 在 $N$ 个神经元上引起一个随机响应向量 $r \\in \\mathbb{R}^N$。假设一个局部模型，其中 $r$ 根据一个多元正态分布生成，该分布具有一个依赖于刺激的均值向量和一个不依赖于刺激的协方差矩阵。具体来说，假设 $p(r \\mid s)$ 是一个多元正态分布，其均值为 $f(s) \\in \\mathbb{R}^N$，协方差矩阵为 $C \\in \\mathbb{R}^{N \\times N}$，且该协方差矩阵不依赖于 $s$。该编码的局部敏感性由 Fisher 信息 $I(s)$ 来表征，其从第一性原理定义为\n$$\nI(s) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial s} \\log p(r \\mid s)\\right)^2\\right],\n$$\n其中期望是关于 $p(r \\mid s)$ 计算的。\n\n您需要推导并实现一个局部 Fisher 信息的估计量，该估计量使用基线刺激 $s_0$ 周围的微小刺激扰动，通过线性回归估计平均响应的局部导数 $f'(s_0)$，并利用残差的经验估计 $\\hat{C}$ 来估计试验间的协方差。该过程应基于以下与上下文相适应的基础：\n- 标量参数的 Fisher 信息的定义。\n- $p(r \\mid s)$ 的多元正态形式。\n- 使用微小扰动对 $f(s)$ 在 $s_0$ 周围进行局部线性近似。\n- 使用普通最小二乘法 (OLS) 线性回归来估计每个神经元的局部斜率。\n\n对于下文的每个测试用例，您必须：\n- 使用指定的模型和参数生成合成数据。\n- 使用基线 $s_0$ 周围的微小刺激扰动和 OLS 估计局部导数向量 $f'(s_0)$。\n- 在移除拟合的局部线性均值后，根据残差估计不依赖于刺激的协方差矩阵 $\\hat{C}$。\n- 根据这些量计算隐含的标量 Fisher 信息估计值。\n- 为每个测试用例生成一个单一的浮点数作为最终结果。\n\n每个测试用例中合成数据生成的模型设定：\n- 神经元 $i$ 的神经调谐函数为 $f_i(s) = a_i + b_i s + c_i s^2$，其中 $a_i$、$b_i$ 和 $c_i$ 是给定的系数。响应均值为 $f(s) = [f_1(s), \\dots, f_N(s)]^\\top$。\n- 协方差矩阵的元素为 $C_{ij} = \\sigma_i \\sigma_j \\rho^{|i - j|}$，其中标准差向量为 $\\sigma = [\\sigma_1, \\dots, \\sigma_N]^\\top$，相关参数为 $\\rho \\in (-1, 1)$。\n- 对于 $s_0$ 周围一小组扰动中的每个刺激值 $s_k$，通过对试验进行独立采样 $r^{(t)} \\sim \\mathcal{N}(f(s_k), C)$ 来生成指定数量的试验。\n\n估计量设计：\n- 对于每个神经元 $i$，通过拟合 OLS 模型 $r_i^{(t)} = \\alpha_i + \\beta_i s^{(t)} + \\varepsilon_i^{(t)}$ 来估计 $f'(s_0)$，这里使用所有微小扰动下的全部试验，然后取 $\\hat{f}'_i(s_0) = \\hat{\\beta}_i$。将这些堆叠起来形成 $\\hat{f}'(s_0) \\in \\mathbb{R}^N$。\n- 使用残差 $\\varepsilon^{(t)} = r^{(t)} - \\hat{\\alpha} - \\hat{\\beta} s^{(t)}$ 在扰动集中的所有试验中的样本协方差来估计 $\\hat{C}$。使用自由度为 $1$ 的无偏协方差估计量。\n- 计算这些局部量所隐含的标量 Fisher 信息估计值。\n\n数值稳定性考虑：\n- 对于某些测试用例，协方差矩阵 $\\hat{C}$ 可能是病态的。如果矩阵求逆失败或条件数过大，则在求逆之前应用一个小的对角正则化 $\\epsilon I$，其中 $\\epsilon$ 是一个小的正数，与 $\\hat{C}$ 的平均对角线大小成比例。\n\n角度单位不适用，且该问题中没有物理单位。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[r_1,r_2,\\dots]$），其中每个 $r_k$ 是测试用例 $k$ 的浮点 Fisher 信息估计值。\n\n测试套件：\n- 测试用例 1 (正常路径)：\n  - $N = 6$，基线刺激 $s_0 = 0.3$。\n  - 微小扰动 $\\Delta s = [-0.1, -0.05, 0.0, 0.05, 0.1]$，刺激值 $s = s_0 + \\Delta s$。\n  - 每次扰动的试验次数：$T = [200, 200, 200, 200, 200]$。\n  - 系数：$a = [0.5, 1.0, -0.3, 0.7, 0.2, -0.1]$，$b = [1.2, -0.8, 0.5, 1.0, -0.4, 0.9]$，$c = [0.3, 0.0, -0.2, 0.1, 0.05, -0.15]$。\n  - 噪声参数：$\\sigma = [0.2, 0.25, 0.22, 0.18, 0.3, 0.27]$，$\\rho = 0.2$。\n- 测试用例 2 (高相关性，近奇异协方差)：\n  - $N = 6$，基线刺激 $s_0 = 0.3$。\n  - 微小扰动 $\\Delta s = [-0.1, -0.05, 0.0, 0.05, 0.1]$，$s = s_0 + \\Delta s$。\n  - 每次扰动的试验次数：$T = [400, 400, 400, 400, 400]$。\n  - 系数：$a = [0.5, 1.0, -0.3, 0.7, 0.2, -0.1]$，$b = [1.2, -0.8, 0.5, 1.0, -0.4, 0.9]$，$c = [0.3, 0.0, -0.2, 0.1, 0.05, -0.15]$。\n  - 噪声参数：$\\sigma = [0.2, 0.25, 0.22, 0.18, 0.3, 0.27]$，$\\rho = 0.95$。\n- 测试用例 3 (部分神经元的弱导数)：\n  - $N = 6$，基线刺激 $s_0 = 0.4$。\n  - 微小扰动 $\\Delta s = [-0.1, 0.0, 0.1]$，$s = s_0 + \\Delta s$。\n  - 每次扰动的试验次数：$T = [300, 300, 300]$。\n  - 系数：$a = [0.1, -0.2, 0.3, 0.0, 0.4, -0.1]$，$c = [0.5, 0.3, -0.1, 0.2, 0.0, 0.25]$，以及明确给出的 $b = [-2 c_1 s_0, -2 c_2 s_0, -2 c_3 s_0, 0.6, -0.7, 0.8]$，即 $b = [-0.4, -0.24, 0.08, 0.6, -0.7, 0.8]$。\n  - 噪声参数：$\\sigma = [0.2, 0.2, 0.2, 0.25, 0.22, 0.3]$，$\\rho = 0.3$。\n- 测试用例 4 (试验次数少，扰动小)：\n  - $N = 6$，基线刺激 $s_0 = 0.5$。\n  - 微小扰动 $\\Delta s = [-0.02, 0.0, 0.02]$，$s = s_0 + \\Delta s$。\n  - 每次扰动的试验次数：$T = [25, 25, 25]$。\n  - 系数：$a = [0.3, 0.1, -0.2, 0.0, 0.5, 0.2]$，$b = [0.4, 0.7, -0.9, 0.2, 0.0, 0.3]$，$c = [0.0, 0.05, 0.1, -0.05, 0.02, 0.0]$。\n  - 噪声参数：$\\sigma = [0.3, 0.25, 0.35, 0.2, 0.22, 0.28]$，$\\rho = 0.5$。\n- 测试用例 5 (各向异性方差，较大群体)：\n  - $N = 8$，基线刺激 $s_0 = -0.2$。\n  - 微小扰动 $\\Delta s = [-0.05, 0.0, 0.05]$，$s = s_0 + \\Delta s$。\n  - 每次扰动的试验次数：$T = [100, 200, 100]$。\n  - 系数：$a = [0.0, 0.2, 0.1, -0.1, 0.3, -0.4, 0.5, 0.0]$，$b = [1.0, -0.5, 0.2, 0.6, -0.7, 0.3, 0.9, -1.1]$，$c = [0.1, -0.2, 0.0, 0.05, 0.02, -0.03, 0.01, 0.04]$。\n  - 噪声参数：$\\sigma = [0.15, 0.3, 0.25, 0.1, 0.45, 0.2, 0.35, 0.4]$，$\\rho = 0.7$。\n\n您的实现必须是确定性的。每个测试用例使用固定的随机种子（例如，测试用例 1 使用种子 0，测试用例 2 使用种子 1，依此类推）。最终输出必须是单行，包含浮点数结果的列表，格式为 $[r_1,r_2,r_3,r_4,r_5]$。",
            "solution": "目标是使用微小扰动和基于回归的估计量来估计神经群体编码在基线刺激周围的局部 Fisher 信息。推导从第一性原理开始，然后得出一个可实现的算法。\n\n基础设定：\n令 $r \\in \\mathbb{R}^N$ 表示群体响应，$s \\in \\mathbb{R}$ 表示刺激。假设 $p(r \\mid s)$ 是一个多元正态分布，其均值为 $f(s) \\in \\mathbb{R}^N$，协方差矩阵为 $C \\in \\mathbb{R}^{N \\times N}$，且该协方差矩阵与 $s$ 无关：\n$$\np(r \\mid s) = \\frac{1}{(2\\pi)^{N/2} |C|^{1/2}} \\exp\\left( -\\frac{1}{2} (r - f(s))^\\top C^{-1} (r - f(s)) \\right).\n$$\n标量参数的 Fisher 信息定义为\n$$\nI(s) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial s} \\log p(r \\mid s)\\right)^2\\right],\n$$\n其中期望是关于 $p(r \\mid s)$ 计算的。\n\n对于所述模型的 $I(s)$ 的推导：\n计算对数似然关于 $s$ 的导数：\n$$\n\\frac{\\partial}{\\partial s} \\log p(r \\mid s) = \\frac{\\partial}{\\partial s} \\left( -\\frac{1}{2} (r - f(s))^\\top C^{-1} (r - f(s)) \\right).\n$$\n令 $u(s) = r - f(s)$。则\n$$\n\\frac{\\partial}{\\partial s} \\log p(r \\mid s) = -\\frac{1}{2} \\frac{\\partial}{\\partial s} \\left( u(s)^\\top C^{-1} u(s) \\right) = -\\frac{1}{2} \\left( -f'(s)^\\top C^{-1} u(s) + u(s)^\\top C^{-1} (-f'(s)) \\right),\n$$\n其中 $f'(s) = \\frac{d}{ds} f(s)$ 是均值关于 $s$ 的导数向量，且 $\\frac{d}{ds} u(s) = -f'(s)$。利用恒等式 $x^\\top A y = y^\\top A^\\top x$ 和 $C^{-1}$ 的对称性，我们有\n$$\n\\frac{\\partial}{\\partial s} \\log p(r \\mid s) = f'(s)^\\top C^{-1} (r - f(s)).\n$$\n因此，Fisher 信息是\n$$\nI(s) = \\mathbb{E}\\left[ \\left( f'(s)^\\top C^{-1} (r - f(s)) \\right)^2 \\right] = f'(s)^\\top C^{-1} \\, \\mathbb{E}\\left[ (r - f(s)) (r - f(s))^\\top \\right] \\, C^{-1} f'(s).\n$$\n由于 $\\mathbb{E}\\left[ (r - f(s)) (r - f(s))^\\top \\right] = C$，我们得到\n$$\nI(s) = f'(s)^\\top C^{-1} f'(s).\n$$\n这就是在具有不依赖于刺激的协方差的多元正态噪声下的局部 Fisher 信息。\n\n从微小扰动进行估计：\n我们寻求在基线刺激 $s_0$ 附近的一个经验估计量 $\\hat{I}(s_0)$。使用局部线性近似，\n$$\nf(s) \\approx f(s_0) + f'(s_0) (s - s_0),\n$$\n对于 $s_0$ 小邻域内的 $s$。我们通过对每个神经元 $i$ 进行普通最小二乘法 (OLS) 回归来估计 $f'(s_0)$：\n$$\nr_i^{(t)} = \\alpha_i + \\beta_i s^{(t)} + \\varepsilon_i^{(t)},\n$$\n其中 $t$ 是试验的索引，$s^{(t)}$ 是在 $s_0$ 附近采样的刺激。在局部模型下，$\\beta_i$ 近似于 $f'_i(s_0)$，因此我们设 $\\hat{f}'_i(s_0) = \\hat{\\beta}_i$ 并构成 $\\hat{f}'(s_0) = [\\hat{\\beta}_1, \\dots, \\hat{\\beta}_N]^\\top$。截距 $\\hat{\\alpha}_i$ 局部地捕捉了 $f_i(s_0)$，但并非直接用于计算 Fisher 信息。\n\n接下来，我们使用残差来估计不依赖于刺激的协方差。构建残差\n$$\n\\varepsilon^{(t)} = r^{(t)} - \\hat{\\alpha} - \\hat{\\beta} s^{(t)},\n$$\n其中 $\\hat{\\alpha} = [\\hat{\\alpha}_1, \\dots, \\hat{\\alpha}_N]^\\top$ 且 $\\hat{\\beta} = [\\hat{\\beta}_1, \\dots, \\hat{\\beta}_N]^\\top$。使用无偏样本协方差\n$$\n\\hat{C} = \\frac{1}{T - 1} \\sum_{t=1}^{T} \\left( \\varepsilon^{(t)} - \\bar{\\varepsilon} \\right) \\left( \\varepsilon^{(t)} - \\bar{\\varepsilon} \\right)^\\top,\n$$\n其中 $T$ 是所有扰动下的总试验次数，$\\bar{\\varepsilon}$ 是残差的样本均值。在模型假设下，如果局部线性近似足够好且噪声是零均值的，那么 $\\bar{\\varepsilon}$ 应该近似为零，但无论如何，减去 $\\bar{\\varepsilon}$ 都会得到一个无偏估计量。\n\n最后，局部 Fisher 信息的估计量是\n$$\n\\hat{I}(s_0) = \\hat{f}'(s_0)^\\top \\, \\hat{C}^{-1} \\, \\hat{f}'(s_0).\n$$\n数值稳定性：\n如果 $\\hat{C}$ 是病态的或接近奇异的，直接求逆可能在数值上不稳定。一个常见的解决方法是 Tikhonov 正则化，即在求逆之前向 $\\hat{C}$ 添加一个小的 $\\epsilon I$：\n$$\n\\hat{I}_\\epsilon(s_0) = \\hat{f}'(s_0)^\\top \\, (\\hat{C} + \\epsilon I)^{-1} \\, \\hat{f}'(s_0),\n$$\n其中 $\\epsilon$ 的选择与 $\\hat{C}$ 的典型尺度成比例，例如，对于一个小的常数 $\\kappa$，$\\epsilon = \\kappa \\, \\frac{1}{N} \\mathrm{trace}(\\hat{C}) \\times 10^{-8}$。或者，可以检查条件数 $\\kappa(\\hat{C})$，仅当它超过阈值时才进行正则化。\n\n每个测试用例的算法步骤：\n1. 固定 $N$、$s_0$、扰动 $\\Delta s$、每次扰动的试验次数 $T_k$、系数 $a, b, c$ 和噪声参数 $\\sigma, \\rho$。\n2. 通过 $C_{ij} = \\sigma_i \\sigma_j \\rho^{|i - j|}$ 构建 $C$。\n3. 对于每个 $\\Delta s_k$，设 $s_k = s_0 + \\Delta s_k$，并生成 $T_k$ 个独立样本 $r^{(t)} \\sim \\mathcal{N}(f(s_k), C)$，其中 $f_i(s_k) = a_i + b_i s_k + c_i s_k^2$。\n4. 聚合所有试验，并通过 OLS 将每个神经元的响应 $r_i^{(t)}$ 对 $s^{(t)}$ 进行回归，以获得 $\\hat{\\alpha}_i$ 和 $\\hat{\\beta}_i$；设 $\\hat{f}'_i(s_0) = \\hat{\\beta}_i$。\n5. 计算残差 $\\varepsilon^{(t)} = r^{(t)} - \\hat{\\alpha} - \\hat{\\beta} s^{(t)}$，然后将 $\\hat{C}$ 计算为残差的无偏样本协方差。\n6. 对 $\\hat{C}$ 求逆，如有必要则应用小的对角正则化，并计算 $\\hat{I}(s_0)$。\n7. 输出该测试用例的标量 $\\hat{I}(s_0)$。\n\n在所述模型下，随着试验次数的增加且扰动保持在线性范围内，该估计量是一致的。测试套件中包含高相关性和少试验次数的场景，分别用于探究数值条件和样本变异性。\n\n实现必须是确定性的，即每个测试用例使用固定的随机种子。最终输出是包含五个估计的 Fisher 信息值的单行，格式为 $[r_1,r_2,r_3,r_4,r_5]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_covariance(sigma, rho):\n    \"\"\"\n    Construct covariance matrix C with entries C_ij = sigma_i * sigma_j * rho^{|i - j|}.\n    Ensures positive definiteness for |rho|  1 given positive sigma entries.\n    \"\"\"\n    sigma = np.asarray(sigma, dtype=float)\n    N = sigma.size\n    indices = np.arange(N)\n    # Toeplitz-like correlation structure\n    corr = rho ** np.abs(indices[:, None] - indices[None, :])\n    C = (sigma[:, None] * sigma[None, :]) * corr\n    return C\n\ndef generate_responses(N, a, b, c, s_values, trials_per_s, C, rng):\n    \"\"\"\n    Generate responses r ~ N(f(s), C) for each s_value, with specified number of trials.\n    Returns:\n        S: array of stimuli per trial (shape [T_total])\n        R: array of responses per trial (shape [T_total, N])\n    \"\"\"\n    a = np.asarray(a, dtype=float)\n    b = np.asarray(b, dtype=float)\n    c = np.asarray(c, dtype=float)\n    s_values = np.asarray(s_values, dtype=float)\n    trials_per_s = np.asarray(trials_per_s, dtype=int)\n    assert a.shape[0] == N and b.shape[0] == N and c.shape[0] == N\n    total_trials = int(np.sum(trials_per_s))\n    R = np.zeros((total_trials, N), dtype=float)\n    S = np.zeros(total_trials, dtype=float)\n    mean_template = np.zeros(N, dtype=float)\n\n    idx = 0\n    for s, T in zip(s_values, trials_per_s):\n        # compute mean vector f(s) = a + b*s + c*s^2\n        mean = a + b * s + c * (s ** 2)\n        # sample T trials from multivariate normal with mean and covariance C\n        samples = rng.multivariate_normal(mean, C, size=T)\n        R[idx:idx+T, :] = samples\n        S[idx:idx+T] = s\n        idx += T\n    return S, R\n\ndef estimate_derivative_and_cov(S, R):\n    \"\"\"\n    Estimate f'(s0) via OLS slope per neuron and residual covariance from residuals.\n    Returns:\n        fprime_hat: shape [N]\n        Chat: shape [N, N]\n    \"\"\"\n    T_total, N = R.shape\n    # Design matrix with intercept and stimulus\n    X = np.column_stack([np.ones(T_total), S])\n    # Solve for coefficients per neuron using least squares\n    # alpha_hat and beta_hat\n    coeffs, _, _, _ = np.linalg.lstsq(X, R, rcond=None)  # coeffs shape [2, N]\n    alpha_hat = coeffs[0, :]\n    beta_hat = coeffs[1, :]\n    # Residuals\n    R_hat = X @ coeffs  # shape [T_total, N]\n    residuals = R - R_hat\n    # Unbiased sample covariance across trials (rows are trials)\n    # np.cov expects variables in rows by default; set rowvar=False since variables are columns\n    Chat = np.cov(residuals, rowvar=False, ddof=1)\n    return beta_hat, Chat\n\ndef invert_covariance_with_regularization(C):\n    \"\"\"\n    Invert covariance matrix with a small diagonal regularization if needed.\n    Uses condition number to detect ill-conditioning.\n    \"\"\"\n    # Handle potential numerical issues\n    try:\n        cond = np.linalg.cond(C)\n    except np.linalg.LinAlgError:\n        cond = np.inf\n    if not np.isfinite(cond) or cond > 1e8:\n        # Regularize\n        avg_diag = float(np.mean(np.diag(C)))\n        eps = max(1e-12, 1e-8 * (avg_diag if avg_diag > 0 else 1.0))\n        C_reg = C + eps * np.eye(C.shape[0])\n        return np.linalg.inv(C_reg)\n    else:\n        return np.linalg.inv(C)\n\ndef fisher_information_estimate(fprime_hat, C_inv):\n    \"\"\"\n    Compute scalar Fisher information estimate: f'(s)^T C^{-1} f'(s).\n    \"\"\"\n    return float(fprime_hat.T @ (C_inv @ fprime_hat))\n\ndef run_test_case(case_index, N, s0, deltas, trials_per_delta, a, b, c, sigma, rho):\n    \"\"\"\n    Run a single test case:\n      - construct covariance\n      - generate synthetic data for stimuli s0 + deltas\n      - estimate derivative and covariance\n      - compute Fisher information\n    \"\"\"\n    rng = np.random.default_rng(case_index)  # deterministic seed per case\n    C = construct_covariance(sigma, rho)\n    s_values = s0 + np.asarray(deltas, dtype=float)\n    S, R = generate_responses(N, a, b, c, s_values, trials_per_delta, C, rng)\n    fprime_hat, Chat = estimate_derivative_and_cov(S, R)\n    C_inv = invert_covariance_with_regularization(Chat)\n    I_hat = fisher_information_estimate(fprime_hat, C_inv)\n    return I_hat\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"N\": 6,\n            \"s0\": 0.3,\n            \"deltas\": [-0.1, -0.05, 0.0, 0.05, 0.1],\n            \"trials\": [200, 200, 200, 200, 200],\n            \"a\": [0.5, 1.0, -0.3, 0.7, 0.2, -0.1],\n            \"b\": [1.2, -0.8, 0.5, 1.0, -0.4, 0.9],\n            \"c\": [0.3, 0.0, -0.2, 0.1, 0.05, -0.15],\n            \"sigma\": [0.2, 0.25, 0.22, 0.18, 0.3, 0.27],\n            \"rho\": 0.2\n        },\n        # Test case 2\n        {\n            \"N\": 6,\n            \"s0\": 0.3,\n            \"deltas\": [-0.1, -0.05, 0.0, 0.05, 0.1],\n            \"trials\": [400, 400, 400, 400, 400],\n            \"a\": [0.5, 1.0, -0.3, 0.7, 0.2, -0.1],\n            \"b\": [1.2, -0.8, 0.5, 1.0, -0.4, 0.9],\n            \"c\": [0.3, 0.0, -0.2, 0.1, 0.05, -0.15],\n            \"sigma\": [0.2, 0.25, 0.22, 0.18, 0.3, 0.27],\n            \"rho\": 0.95\n        },\n        # Test case 3\n        {\n            \"N\": 6,\n            \"s0\": 0.4,\n            \"deltas\": [-0.1, 0.0, 0.1],\n            \"trials\": [300, 300, 300],\n            \"a\": [0.1, -0.2, 0.3, 0.0, 0.4, -0.1],\n            \"b\": [-0.4, -0.24, 0.08, 0.6, -0.7, 0.8],  # derived from c and s0 for first three\n            \"c\": [0.5, 0.3, -0.1, 0.2, 0.0, 0.25],\n            \"sigma\": [0.2, 0.2, 0.2, 0.25, 0.22, 0.3],\n            \"rho\": 0.3\n        },\n        # Test case 4\n        {\n            \"N\": 6,\n            \"s0\": 0.5,\n            \"deltas\": [-0.02, 0.0, 0.02],\n            \"trials\": [25, 25, 25],\n            \"a\": [0.3, 0.1, -0.2, 0.0, 0.5, 0.2],\n            \"b\": [0.4, 0.7, -0.9, 0.2, 0.0, 0.3],\n            \"c\": [0.0, 0.05, 0.1, -0.05, 0.02, 0.0],\n            \"sigma\": [0.3, 0.25, 0.35, 0.2, 0.22, 0.28],\n            \"rho\": 0.5\n        },\n        # Test case 5\n        {\n            \"N\": 8,\n            \"s0\": -0.2,\n            \"deltas\": [-0.05, 0.0, 0.05],\n            \"trials\": [100, 200, 100],\n            \"a\": [0.0, 0.2, 0.1, -0.1, 0.3, -0.4, 0.5, 0.0],\n            \"b\": [1.0, -0.5, 0.2, 0.6, -0.7, 0.3, 0.9, -1.1],\n            \"c\": [0.1, -0.2, 0.0, 0.05, 0.02, -0.03, 0.01, 0.04],\n            \"sigma\": [0.15, 0.3, 0.25, 0.1, 0.45, 0.2, 0.35, 0.4],\n            \"rho\": 0.7\n        }\n    ]\n\n    results = []\n    for idx, case in enumerate(test_cases):\n        I_hat = run_test_case(\n            case_index=idx,\n            N=case[\"N\"],\n            s0=case[\"s0\"],\n            deltas=case[\"deltas\"],\n            trials_per_delta=case[\"trials\"],\n            a=case[\"a\"],\n            b=case[\"b\"],\n            c=case[\"c\"],\n            sigma=case[\"sigma\"],\n            rho=case[\"rho\"]\n        )\n        # Format result with a reasonable precision to ensure readable output\n        results.append(f\"{I_hat:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}