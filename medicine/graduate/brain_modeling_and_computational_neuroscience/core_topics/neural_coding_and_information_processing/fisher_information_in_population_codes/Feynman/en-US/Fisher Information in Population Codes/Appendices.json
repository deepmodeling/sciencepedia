{
    "hands_on_practices": [
        {
            "introduction": "We begin with a foundational exercise: deriving the Fisher Information for a population of independent neurons described by a Generalized Linear Model (GLM). This practice connects the abstract definition of Fisher Information to a concrete and widely used model of neural spiking, revealing how coding precision is shaped by the neuron's tuning properties and its intrinsic variability. ",
            "id": "3981091",
            "problem": "Consider a population of $N$ conditionally independent binary neurons recorded in a small time bin. Let $r_i \\in \\{0,1\\}$ denote the spike of neuron $i$ in that bin, and let $s \\in \\mathbb{R}$ be a one-dimensional stimulus parameter. Suppose that, conditioned on $s$, each neuron follows a Generalized Linear Model (GLM) with a logistic link: for each $i \\in \\{1,\\dots,N\\}$,\n$$\n\\Pr(r_i=1 \\mid s) \\equiv p_i(s) = \\sigma\\!\\big(a_i(s)\\big), \\quad \\text{with} \\quad \\sigma(z) = \\frac{1}{1+\\exp(-z)},\n$$\nwhere $a_i(s)$ is a twice continuously differentiable function of $s$, and $\\{r_i\\}_{i=1}^N$ are independent given $s$. Assume that all expectations with respect to the response distribution $\\Pr(\\mathbf{r}\\mid s)$ exist and are finite.\n\nStarting from the standard definition of Fisher information for a parametric family, derive a closed-form analytic expression for the Fisher information $I(s)$ of the population response $\\mathbf{r} = (r_1,\\dots,r_N)$ about the scalar parameter $s$, expressed only in terms of $\\{a_i(s)\\}$, their derivatives with respect to $s$, and $\\sigma$. Provide your final expression for $I(s)$ in its simplest form. Provide your answer as a single closed-form analytic expression with no units required.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. All conditions for a rigorous derivation are provided.\n\nThe Fisher information $I(s)$ for a parameter $s$ given an observation $\\mathbf{r}$ is defined by the expectation of the squared score, where the score is the derivative of the log-likelihood of the data with respect to the parameter.\n$$\nI(s) = \\mathbb{E}_{\\mathbf{r}|s} \\left[ \\left( \\frac{\\partial}{\\partial s} \\ln \\Pr(\\mathbf{r} \\mid s) \\right)^2 \\right]\n$$\nThe vector of neural responses is $\\mathbf{r} = (r_1, \\dots, r_N)$, where each $r_i$ is a binary random variable, $r_i \\in \\{0, 1\\}$. The neurons are conditionally independent given the stimulus $s$. Therefore, the joint probability mass function (the likelihood) is the product of the individual probabilities:\n$$\n\\Pr(\\mathbf{r} \\mid s) = \\prod_{i=1}^N \\Pr(r_i \\mid s)\n$$\nFor each neuron $i$, the response follows a Bernoulli distribution, as $r_i$ is binary. The probability of a spike ($r_i=1$) is given as $p_i(s) = \\Pr(r_i=1 \\mid s)$. Thus, the probability mass function for a single neuron can be written as:\n$$\n\\Pr(r_i \\mid s) = p_i(s)^{r_i} \\big(1 - p_i(s)\\big)^{1-r_i}\n$$\nThe full likelihood for the population is:\n$$\n\\Pr(\\mathbf{r} \\mid s) = \\prod_{i=1}^N p_i(s)^{r_i} \\big(1 - p_i(s)\\big)^{1-r_i}\n$$\nThe next step is to compute the natural logarithm of the likelihood:\n$$\n\\ln \\Pr(\\mathbf{r} \\mid s) = \\ln \\left( \\prod_{i=1}^N p_i(s)^{r_i} \\big(1 - p_i(s)\\big)^{1-r_i} \\right) = \\sum_{i=1}^N \\ln \\left( p_i(s)^{r_i} \\big(1 - p_i(s)\\big)^{1-r_i} \\right)\n$$\n$$\n\\ln \\Pr(\\mathbf{r} \\mid s) = \\sum_{i=1}^N \\left[ r_i \\ln p_i(s) + (1-r_i) \\ln\\big(1 - p_i(s)\\big) \\right]\n$$\nNow, we differentiate the log-likelihood with respect to $s$ to obtain the score function. Let $a_i'(s) = \\frac{da_i}{ds}$.\n$$\n\\frac{\\partial}{\\partial s} \\ln \\Pr(\\mathbf{r} \\mid s) = \\sum_{i=1}^N \\left[ r_i \\frac{1}{p_i(s)}\\frac{dp_i}{ds} + (1-r_i) \\frac{1}{1-p_i(s)} \\left(-\\frac{dp_i}{ds}\\right) \\right]\n$$\n$$\n= \\sum_{i=1}^N \\frac{dp_i}{ds} \\left[ \\frac{r_i}{p_i(s)} - \\frac{1-r_i}{1-p_i(s)} \\right] = \\sum_{i=1}^N \\frac{dp_i}{ds} \\left[ \\frac{r_i\\big(1-p_i(s)\\big) - \\big(1-r_i\\big)p_i(s)}{p_i(s)\\big(1-p_i(s)\\big)} \\right]\n$$\n$$\n= \\sum_{i=1}^N \\frac{dp_i}{ds} \\left[ \\frac{r_i - r_i p_i(s) - p_i(s) + r_i p_i(s)}{p_i(s)\\big(1-p_i(s)\\big)} \\right] = \\sum_{i=1}^N \\frac{dp_i}{ds} \\frac{r_i - p_i(s)}{p_i(s)\\big(1-p_i(s)\\big)}\n$$\nWe need to compute the derivative $\\frac{dp_i}{ds}$. We are given $p_i(s) = \\sigma(a_i(s))$, where $\\sigma(z) = (1+\\exp(-z))^{-1}$. The derivative of the sigmoid function $\\sigma(z)$ is a standard result:\n$$\n\\sigma'(z) = \\frac{d}{dz}(1+\\exp(-z))^{-1} = -1(1+\\exp(-z))^{-2}(-\\exp(-z)) = \\frac{\\exp(-z)}{(1+\\exp(-z))^2}\n$$\nThis can be written as $\\sigma'(z) = \\frac{1}{1+\\exp(-z)} \\frac{\\exp(-z)}{1+\\exp(-z)} = \\sigma(z)(1-\\sigma(z))$.\nUsing the chain rule for $\\frac{dp_i}{ds}$:\n$$\n\\frac{dp_i}{ds} = \\frac{d}{ds}\\sigma(a_i(s)) = \\sigma'(a_i(s)) \\cdot a_i'(s) = \\sigma(a_i(s))\\big(1-\\sigma(a_i(s))\\big) a_i'(s) = p_i(s)\\big(1-p_i(s)\\big) a_i'(s)\n$$\nSubstituting this result back into the expression for the score:\n$$\n\\frac{\\partial}{\\partial s} \\ln \\Pr(\\mathbf{r} \\mid s) = \\sum_{i=1}^N a_i'(s) p_i(s)\\big(1-p_i(s)\\big) \\frac{r_i - p_i(s)}{p_i(s)\\big(1-p_i(s)\\big)} = \\sum_{i=1}^N a_i'(s) \\big(r_i - p_i(s)\\big)\n$$\nThis is the score function. To find the Fisher information, we compute the expectation of its square:\n$$\nI(s) = \\mathbb{E} \\left[ \\left( \\sum_{i=1}^N a_i'(s) \\big(r_i - p_i(s)\\big) \\right)^2 \\right] = \\mathbb{E} \\left[ \\sum_{i=1}^N \\sum_{j=1}^N a_i'(s)a_j'(s)\\big(r_i - p_i(s)\\big)\\big(r_j - p_j(s)\\big) \\right]\n$$\nBy linearity of expectation, we can move the expectation inside the sums:\n$$\nI(s) = \\sum_{i=1}^N \\sum_{j=1}^N a_i'(s)a_j'(s) \\mathbb{E} \\left[ \\big(r_i - p_i(s)\\big)\\big(r_j - p_j(s)\\big) \\right]\n$$\nThe expectation term is the covariance between $r_i$ and $r_j$. Since the neurons are conditionally independent given $s$, their responses $r_i$ and $r_j$ are independent for $i \\neq j$. For $i \\neq j$:\n$$\n\\mathbb{E} \\left[ \\big(r_i - p_i(s)\\big)\\big(r_j - p_j(s)\\big) \\right] = \\mathbb{E} \\left[r_i - p_i(s)\\right] \\mathbb{E} \\left[r_j - p_j(s)\\right]\n$$\nBy definition, $\\mathbb{E}[r_k] = 1 \\cdot \\Pr(r_k=1) + 0 \\cdot \\Pr(r_k=0) = p_k(s)$. Thus, $\\mathbb{E}[r_k - p_k(s)] = p_k(s) - p_k(s) = 0$.\nSo, all cross-terms where $i \\neq j$ are zero. The sum reduces to the terms where $i=j$:\n$$\nI(s) = \\sum_{i=1}^N \\big(a_i'(s)\\big)^2 \\mathbb{E} \\left[ \\big(r_i - p_i(s)\\big)^2 \\right]\n$$\nThe expectation term is the variance of the Bernoulli variable $r_i$:\n$$\n\\mathbb{E} \\left[ \\big(r_i - p_i(s)\\big)^2 \\right] = \\text{Var}(r_i)\n$$\nThe variance of a Bernoulli variable with success probability $p$ is $p(1-p)$. Thus:\n$$\n\\text{Var}(r_i) = p_i(s)\\big(1 - p_i(s)\\big)\n$$\nSubstituting this into the expression for $I(s)$:\n$$\nI(s) = \\sum_{i=1}^N \\big(a_i'(s)\\big)^2 p_i(s)\\big(1 - p_i(s)\\big)\n$$\nFinally, we express this in terms of the specified functions. We substitute $p_i(s) = \\sigma(a_i(s))$:\n$$\nI(s) = \\sum_{i=1}^N \\big(a_i'(s)\\big)^2 \\sigma(a_i(s))\\big(1 - \\sigma(a_i(s))\\big)\n$$\nThis expression gives the total Fisher information as the sum of the Fisher information from each neuron, which is a direct consequence of the conditional independence assumption. The information from each neuron is proportional to the square of the derivative of its tuning curve input, $a_i(s)$, and the variance of its output, which is maximal when its firing probability $\\sigma(a_i(s))$ is $1/2$. Let's denote the derivative as $\\frac{da_i}{ds}$ for full clarity.\n$$\nI(s) = \\sum_{i=1}^N \\left(\\frac{da_i(s)}{ds}\\right)^2 \\sigma(a_i(s))\\big(1 - \\sigma(a_i(s))\\big)\n$$\nThis is the final closed-form expression.",
            "answer": "$$\n\\boxed{\\sum_{i=1}^N \\left(\\frac{da_i(s)}{ds}\\right)^2 \\sigma(a_i(s))\\big(1 - \\sigma(a_i(s))\\big)}\n$$"
        },
        {
            "introduction": "Next, we shift our focus to population codes with continuous, graded responses, which are often modeled with a multivariate Gaussian distribution. This practice guides you through the derivation of the Fisher Information Matrix ($J(s)$) for a multidimensional stimulus, demonstrating how to quantify the code's sensitivity along any arbitrary direction in the stimulus space. ",
            "id": "3981050",
            "problem": "Consider a neural population code in which the conditional response $r \\in \\mathbb{R}^{N}$ given a stimulus $s \\in \\mathbb{R}^{d}$ is modeled as a multivariate normal distribution with mean $A(s) \\in \\mathbb{R}^{N}$ and stimulus-independent covariance matrix $C \\in \\mathbb{R}^{N \\times N}$ that is symmetric positive definite. Let $\\mu(s) \\equiv A(s)$ denote the mean response. The Fisher information matrix $J(s) \\in \\mathbb{R}^{d \\times d}$ about $s$ is defined in terms of the score function by\n$$\nJ(s) \\equiv \\mathbb{E}\\!\\left[ \\left(\\nabla_{s} \\ln p(r \\mid s)\\right)\\left(\\nabla_{s} \\ln p(r \\mid s)\\right)^{\\top} \\right],\n$$\nwhere the expectation is with respect to $p(r \\mid s)$.\n\nYou will work in two parts:\n\n- Part (a): For any unit vector $u \\in \\mathbb{R}^{d}$ (i.e., $|u| = 1$), consider the scalarized parametrization $s(t) = s + t\\,u$ and the corresponding scalar Fisher information about the parameter $t$ at $t = 0$, defined by\n$$\nI(t)\\big|_{t=0} \\equiv \\mathbb{E}\\!\\left[ \\left( \\frac{\\partial}{\\partial t} \\ln p\\!\\left(r \\mid s(t)\\right) \\Big|_{t=0} \\right)^{2} \\right].\n$$\nStarting from the given definition of the Fisher information matrix and the Gaussian likelihood model specified above, derive an explicit expression for $I(t)\\big|_{t=0}$ in terms of $u$ and $J(s)$.\n\n- Part (b): Specialize to $d = 2$ and $N = 3$. Let the mean response be linear, $A(s) = M s$ with\n$$\nM \\equiv \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 2\n\\end{pmatrix},\n$$\nand let the covariance be diagonal,\n$$\nC \\equiv \\operatorname{diag}(1, 2, 4).\n$$\nTake the unit direction\n$$\nu \\equiv \\begin{pmatrix} 3/5 \\\\ 4/5 \\end{pmatrix}.\n$$\nCompute the scalar Fisher information along $u$ at any $s$, expressing your final result as an exact real number without approximation. The final answer must be a single real value. No rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of information in neural codes, is well-posed with all necessary information provided, and is stated in objective, formal language. We proceed with the solution in two parts.\n\nPart (a): Derivation of the scalar Fisher information.\n\nWe are given a neural population code where the response $r \\in \\mathbb{R}^{N}$ to a stimulus $s \\in \\mathbb{R}^{d}$ follows a multivariate normal distribution, $p(r \\mid s) \\sim \\mathcal{N}(\\mu(s), C)$, where $\\mu(s) = A(s)$ is the mean response and $C$ is a stimulus-independent, symmetric positive-definite covariance matrix.\n\nThe probability density function is\n$$\np(r \\mid s) = \\frac{1}{\\sqrt{(2\\pi)^{N} \\det(C)}} \\exp\\left(-\\frac{1}{2} (r - \\mu(s))^{\\top} C^{-1} (r - \\mu(s))\\right).\n$$\nThe log-likelihood, $\\ln p(r \\mid s)$, is\n$$\n\\ln p(r \\mid s) = -\\frac{N}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(C)) - \\frac{1}{2} (r - \\mu(s))^{\\top} C^{-1} (r - \\mu(s)).\n$$\nTo find the Fisher information matrix $J(s)$, we first compute the score function, which is the gradient of the log-likelihood with respect to the stimulus $s$. The terms that do not depend on $s$ have zero gradient.\n$$\n\\nabla_{s} \\ln p(r \\mid s) = \\nabla_{s} \\left(-\\frac{1}{2} (r - \\mu(s))^{\\top} C^{-1} (r - \\mu(s))\\right).\n$$\nUsing matrix calculus identities, specifically the derivative of a quadratic form, we get\n$$\n\\nabla_{s} \\ln p(r \\mid s) = (\\nabla_{s} \\mu(s))^{\\top} C^{-1} (r - \\mu(s)).\n$$\nHere, $\\nabla_{s} \\mu(s)$ is the Jacobian matrix of the mean response function, with dimensions $N \\times d$, which we denote as $\\mu'(s)$. Its transpose, $(\\mu'(s))^{\\top}$, has dimensions $d \\times N$. The score function is a $d \\times 1$ column vector.\n\nThe Fisher information matrix is defined as the covariance of the score function:\n$$\nJ(s) = \\mathbb{E}\\left[ (\\nabla_{s} \\ln p(r \\mid s)) (\\nabla_{s} \\ln p(r \\mid s))^{\\top} \\right].\n$$\nSubstituting the expression for the score function:\n$$\nJ(s) = \\mathbb{E}\\left[ \\left((\\mu'(s))^{\\top} C^{-1} (r - \\mu(s))\\right) \\left((\\mu'(s))^{\\top} C^{-1} (r - \\mu(s))\\right)^{\\top} \\right].\n$$\n$$\nJ(s) = \\mathbb{E}\\left[ (\\mu'(s))^{\\top} C^{-1} (r - \\mu(s)) (r - \\mu(s))^{\\top} (C^{-1})^{\\top} \\mu'(s) \\right].\n$$\nSince $C$ is symmetric, $C^{-1}$ is also symmetric, so $(C^{-1})^{\\top} = C^{-1}$. We can pull the terms not involving the random variable $r$ out of the expectation:\n$$\nJ(s) = (\\mu'(s))^{\\top} C^{-1} \\mathbb{E}\\left[ (r - \\mu(s)) (r - \\mu(s))^{\\top} \\right] C^{-1} \\mu'(s).\n$$\nThe expectation $\\mathbb{E}\\left[ (r - \\mu(s)) (r - \\mu(s))^{\\top} \\right]$ is the definition of the covariance matrix of $r$, which is $C$.\n$$\nJ(s) = (\\mu'(s))^{\\top} C^{-1} C C^{-1} \\mu'(s) = (\\mu'(s))^{\\top} C^{-1} \\mu'(s).\n$$\nThis is a general expression for the Fisher information matrix for a Gaussian population code with stimulus-independent covariance.\n\nNow, we consider the scalarized parametrization $s(t) = s + t u$ for a unit vector $u \\in \\mathbb{R}^{d}$. We want to compute the scalar Fisher information $I(t)$ at $t=0$. The score for the parameter $t$ is $\\frac{\\partial}{\\partial t} \\ln p(r \\mid s(t))$. By the chain rule:\n$$\n\\frac{\\partial}{\\partial t} \\ln p(r \\mid s(t)) = (\\nabla_{s} \\ln p(r \\mid s(t)))^{\\top} \\frac{d s(t)}{dt}.\n$$\nSince $\\frac{d s(t)}{dt} = u$, and at $t=0$, $s(t) = s$, we have:\n$$\n\\left. \\frac{\\partial}{\\partial t} \\ln p(r \\mid s(t)) \\right|_{t=0} = (\\nabla_{s} \\ln p(r \\mid s))^{\\top} u.\n$$\nThe scalar Fisher information about $t$ at $t=0$ is the variance of this score:\n$$\nI(t)\\big|_{t=0} = \\mathbb{E}\\left[ \\left( (\\nabla_{s} \\ln p(r \\mid s))^{\\top} u \\right)^{2} \\right].\n$$\nLet the score vector be $g_s = \\nabla_{s} \\ln p(r \\mid s)$. The expression inside the expectation is a scalar, so we can write its square as $(g_s^{\\top} u)^{2} = (u^{\\top} g_s)(g_s^{\\top} u) = u^{\\top} (g_s g_s^{\\top}) u$.\n$$\nI(t)\\big|_{t=0} = \\mathbb{E}\\left[ u^{\\top} (g_s g_s^{\\top}) u \\right].\n$$\nSince $u$ is a constant vector, we can move it outside the expectation:\n$$\nI(t)\\big|_{t=0} = u^{\\top} \\mathbb{E}\\left[ g_s g_s^{\\top} \\right] u.\n$$\nThe term $\\mathbb{E}\\left[ g_s g_s^{\\top} \\right]$ is precisely the definition of the Fisher information matrix $J(s)$.\nTherefore, the scalar Fisher information along direction $u$ is:\n$$\nI(t)\\big|_{t=0} = u^{\\top} J(s) u.\n$$\n\nPart (b): Calculation for the specific case.\n\nWe are given $d=2$, $N=3$, the mean response $\\mu(s) = M s$ with $M = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 2 \\end{pmatrix}$, the covariance $C = \\operatorname{diag}(1, 2, 4)$, and the direction $u = \\begin{pmatrix} 3/5 \\\\ 4/5 \\end{pmatrix}$.\n\nWe use the derived formula $u^{\\top} J(s) u$, where $J(s) = (\\mu'(s))^{\\top} C^{-1} \\mu'(s)$.\n\nFirst, we find the Jacobian of the mean response, $\\mu'(s)$. Since $\\mu(s) = M s$ is a linear function of $s$, its Jacobian is simply the matrix $M$:\n$$\n\\mu'(s) = \\frac{\\partial(Ms)}{\\partial s} = M = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 2 \\end{pmatrix}.\n$$\nAs the Jacobian is constant, the Fisher information matrix $J(s)$ will be independent of $s$, which we denote as $J$.\n\nNext, we find the inverse of the covariance matrix $C$:\n$$\nC = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} \\implies C^{-1} = \\begin{pmatrix} 1^{-1} & 0 & 0 \\\\ 0 & 2^{-1} & 0 \\\\ 0 & 0 & 4^{-1} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1/2 & 0 \\\\ 0 & 0 & 1/4 \\end{pmatrix}.\n$$\nNow we compute the Fisher information matrix $J = M^{\\top} C^{-1} M$:\n$$\nM^{\\top} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix}.\n$$\n$$\nJ = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1/2 & 0 \\\\ 0 & 0 & 1/4 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 2 \\end{pmatrix}\n$$\n$$\nJ = \\begin{pmatrix} 1 & 1/2 & 0 \\\\ 0 & 1/2 & 1/2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 2 \\end{pmatrix}\n$$\n$$\nJ = \\begin{pmatrix} 1(1) + (1/2)(1) + 0(0) & 1(0) + (1/2)(1) + 0(2) \\\\ 0(1) + (1/2)(1) + (1/2)(0) & 0(0) + (1/2)(1) + (1/2)(2) \\end{pmatrix}\n$$\n$$\nJ = \\begin{pmatrix} 1 + 1/2 & 1/2 \\\\ 1/2 & 1/2 + 1 \\end{pmatrix} = \\begin{pmatrix} 3/2 & 1/2 \\\\ 1/2 & 3/2 \\end{pmatrix}.\n$$\nFinally, we compute the scalar Fisher information $I = u^{\\top} J u$:\n$$\nu = \\begin{pmatrix} 3/5 \\\\ 4/5 \\end{pmatrix}.\n$$\n$$\nI = \\begin{pmatrix} 3/5 & 4/5 \\end{pmatrix} \\begin{pmatrix} 3/2 & 1/2 \\\\ 1/2 & 3/2 \\end{pmatrix} \\begin{pmatrix} 3/5 \\\\ 4/5 \\end{pmatrix}\n$$\n$$\nI = (3/5) \\left( (3/2)(3/5) + (1/2)(4/5) \\right) + (4/5) \\left( (1/2)(3/5) + (3/2)(4/5) \\right)\n$$\n$$\nI = (3/5) \\left( 9/10 + 4/10 \\right) + (4/5) \\left( 3/10 + 12/10 \\right)\n$$\n$$\nI = (3/5) (13/10) + (4/5) (15/10)\n$$\n$$\nI = \\frac{39}{50} + \\frac{60}{50} = \\frac{99}{50}.\n$$\nThe result is an exact real number.",
            "answer": "$$\\boxed{\\frac{99}{50}}$$"
        },
        {
            "introduction": "Theoretical formulas are powerful, but a crucial skill is learning how to apply them to experimental data. This hands-on coding exercise simulates a realistic data analysis scenario where you will estimate the Fisher Information by first using linear regression to find the tuning curve slopes and then computing the noise covariance from trial-to-trial variability. ",
            "id": "3981087",
            "problem": "Consider a neural population code in which a scalar stimulus $s \\in \\mathbb{R}$ elicits a stochastic response vector $r \\in \\mathbb{R}^N$ across $N$ neurons. Assume a local model in which $r$ is generated according to a multivariate normal distribution with a stimulus-dependent mean vector and a stimulus-independent covariance matrix. Specifically, assume $p(r \\mid s)$ is multivariate normal with mean $f(s) \\in \\mathbb{R}^N$ and covariance matrix $C \\in \\mathbb{R}^{N \\times N}$ that does not depend on $s$. The local sensitivity of the code is characterized by the Fisher information $I(s)$, which is defined from first principles by\n$$\nI(s) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial s} \\log p(r \\mid s)\\right)^2\\right],\n$$\nwhere the expectation is with respect to $p(r \\mid s)$.\n\nYou are to derive and implement an estimator for the local Fisher information that uses small stimulus perturbations around a baseline stimulus $s_0$, linear regression to estimate the local derivative of the mean response $f'(s_0)$, and an empirical estimate $\\hat{C}$ of the trial-to-trial covariance from residuals. The procedure should be based on the following context-appropriate foundations:\n- The definition of Fisher information for a scalar parameter.\n- The multivariate normal form of $p(r \\mid s)$.\n- A local linear approximation of $f(s)$ around $s_0$ using small perturbations.\n- Ordinary least squares (OLS) linear regression to estimate the local slope for each neuron.\n\nFor each test case below, you must:\n- Generate synthetic data using the specified model and parameters.\n- Use small stimulus perturbations around the baseline $s_0$ and OLS to estimate the local derivative vector $f'(s_0)$.\n- Estimate the stimulus-independent covariance matrix $\\hat{C}$ from residuals after removing the fitted local linear mean.\n- Compute the implied scalar Fisher information estimate from these quantities.\n- Produce the final result for each test case as a single floating-point number.\n\nModel specification for synthetic data generation in each test case:\n- The neural tuning function for neuron $i$ is $f_i(s) = a_i + b_i s + c_i s^2$, where $a_i$, $b_i$, and $c_i$ are given coefficients. The response mean is $f(s) = [f_1(s), \\dots, f_N(s)]^\\top$.\n- The covariance entries are $C_{ij} = \\sigma_i \\sigma_j \\rho^{|i - j|}$, where the vector of standard deviations is $\\sigma = [\\sigma_1, \\dots, \\sigma_N]^\\top$ and the correlation parameter is $\\rho \\in (-1, 1)$.\n- For each stimulus value $s_k$ in a small set of perturbations around $s_0$, generate the specified number of trials by sampling $r^{(t)} \\sim \\mathcal{N}(f(s_k), C)$ independently across trials.\n\nEstimator design:\n- Estimate $f'(s_0)$ by fitting, for each neuron $i$, the OLS model $r_i^{(t)} = \\alpha_i + \\beta_i s^{(t)} + \\varepsilon_i^{(t)}$ using all trials across the small perturbations, and then taking $\\hat{f}'_i(s_0) = \\hat{\\beta}_i$. Stack these to form $\\hat{f}'(s_0) \\in \\mathbb{R}^N$.\n- Estimate $\\hat{C}$ using the sample covariance of the residuals $\\varepsilon^{(t)} = r^{(t)} - \\hat{\\alpha} - \\hat{\\beta} s^{(t)}$ across all trials in the perturbation set. Use an unbiased covariance estimator with degrees of freedom equal to $1$.\n- Compute the scalar Fisher information estimate implied by these local quantities.\n\nNumerical stability considerations:\n- The covariance matrix $\\hat{C}$ may be ill-conditioned for some test cases. If matrix inversion fails or the condition number is large, apply a small diagonal regularization $\\epsilon I$ before inversion, where $\\epsilon$ is a small positive number proportional to the average diagonal magnitude of $\\hat{C}$.\n\nAngle units are not applicable and there are no physical units in this problem.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots]$), where each $r_k$ is the floating-point Fisher information estimate for test case $k$.\n\nTest suite:\n- Test case $1$ (happy path):\n  - $N = 6$, baseline stimulus $s_0 = 0.3$.\n  - Small perturbations $\\Delta s = [-0.1, -0.05, 0.0, 0.05, 0.1]$ and stimulus values $s = s_0 + \\Delta s$.\n  - Trials per perturbation: $T = [200, 200, 200, 200, 200]$.\n  - Coefficients: $a = [0.5, 1.0, -0.3, 0.7, 0.2, -0.1]$, $b = [1.2, -0.8, 0.5, 1.0, -0.4, 0.9]$, $c = [0.3, 0.0, -0.2, 0.1, 0.05, -0.15]$.\n  - Noise parameters: $\\sigma = [0.2, 0.25, 0.22, 0.18, 0.3, 0.27]$, $\\rho = 0.2$.\n- Test case $2$ (high correlation, near-singular covariance):\n  - $N = 6$, baseline stimulus $s_0 = 0.3$.\n  - Small perturbations $\\Delta s = [-0.1, -0.05, 0.0, 0.05, 0.1]$, $s = s_0 + \\Delta s$.\n  - Trials per perturbation: $T = [400, 400, 400, 400, 400]$.\n  - Coefficients: $a = [0.5, 1.0, -0.3, 0.7, 0.2, -0.1]$, $b = [1.2, -0.8, 0.5, 1.0, -0.4, 0.9]$, $c = [0.3, 0.0, -0.2, 0.1, 0.05, -0.15]$.\n  - Noise parameters: $\\sigma = [0.2, 0.25, 0.22, 0.18, 0.3, 0.27]$, $\\rho = 0.95$.\n- Test case $3$ (weak derivative for a subset of neurons):\n  - $N = 6$, baseline stimulus $s_0 = 0.4$.\n  - Small perturbations $\\Delta s = [-0.1, 0.0, 0.1]$, $s = s_0 + \\Delta s$.\n  - Trials per perturbation: $T = [300, 300, 300]$.\n  - Coefficients: $a = [0.1, -0.2, 0.3, 0.0, 0.4, -0.1]$, $c = [0.5, 0.3, -0.1, 0.2, 0.0, 0.25]$, and $b = [-2 c_1 s_0, -2 c_2 s_0, -2 c_3 s_0, 0.6, -0.7, 0.8]$ explicitly given as $b = [-0.4, -0.24, 0.08, 0.6, -0.7, 0.8]$.\n  - Noise parameters: $\\sigma = [0.2, 0.2, 0.2, 0.25, 0.22, 0.3]$, $\\rho = 0.3$.\n- Test case $4$ (few trials, small perturbations):\n  - $N = 6$, baseline stimulus $s_0 = 0.5$.\n  - Small perturbations $\\Delta s = [-0.02, 0.0, 0.02]$, $s = s_0 + \\Delta s$.\n  - Trials per perturbation: $T = [25, 25, 25]$.\n  - Coefficients: $a = [0.3, 0.1, -0.2, 0.0, 0.5, 0.2]$, $b = [0.4, 0.7, -0.9, 0.2, 0.0, 0.3]$, $c = [0.0, 0.05, 0.1, -0.05, 0.02, 0.0]$.\n  - Noise parameters: $\\sigma = [0.3, 0.25, 0.35, 0.2, 0.22, 0.28]$, $\\rho = 0.5$.\n- Test case $5$ (anisotropic variances, larger population):\n  - $N = 8$, baseline stimulus $s_0 = -0.2$.\n  - Small perturbations $\\Delta s = [-0.05, 0.0, 0.05]$, $s = s_0 + \\Delta s$.\n  - Trials per perturbation: $T = [100, 200, 100]$.\n  - Coefficients: $a = [0.0, 0.2, 0.1, -0.1, 0.3, -0.4, 0.5, 0.0]$, $b = [1.0, -0.5, 0.2, 0.6, -0.7, 0.3, 0.9, -1.1]$, $c = [0.1, -0.2, 0.0, 0.05, 0.02, -0.03, 0.01, 0.04]$.\n  - Noise parameters: $\\sigma = [0.15, 0.3, 0.25, 0.1, 0.45, 0.2, 0.35, 0.4]$, $\\rho = 0.7$.\n\nYour implementation must be deterministic. Use fixed random seeds per test case (for example, seed $0$ for test case $1$, seed $1$ for test case $2$, and so on). The final output must be a single line with the list of floating-point results in the exact format $[r_1,r_2,r_3,r_4,r_5]$.",
            "solution": "The goal is to estimate the local Fisher information of a neural population code around a baseline stimulus using small perturbations and regression-based estimators. The derivation begins from first principles and proceeds to an implementable algorithm.\n\nFoundational setup:\nLet $r \\in \\mathbb{R}^N$ denote the population response and $s \\in \\mathbb{R}$ the stimulus. Assume $p(r \\mid s)$ is multivariate normal with mean $f(s) \\in \\mathbb{R}^N$ and covariance matrix $C \\in \\mathbb{R}^{N \\times N}$, independent of $s$:\n$$\np(r \\mid s) = \\frac{1}{(2\\pi)^{N/2} |C|^{1/2}} \\exp\\left( -\\frac{1}{2} (r - f(s))^\\top C^{-1} (r - f(s)) \\right).\n$$\nThe Fisher information for a scalar parameter is defined by\n$$\nI(s) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial s} \\log p(r \\mid s)\\right)^2\\right],\n$$\nwhere the expectation is with respect to $p(r \\mid s)$.\n\nDerivation of $I(s)$ for the stated model:\nCompute the derivative of the log-likelihood with respect to $s$:\n$$\n\\frac{\\partial}{\\partial s} \\log p(r \\mid s) = \\frac{\\partial}{\\partial s} \\left( -\\frac{1}{2} (r - f(s))^\\top C^{-1} (r - f(s)) \\right).\n$$\nLet $u(s) = r - f(s)$. Then\n$$\n\\frac{\\partial}{\\partial s} \\log p(r \\mid s) = -\\frac{1}{2} \\frac{\\partial}{\\partial s} \\left( u(s)^\\top C^{-1} u(s) \\right) = -\\frac{1}{2} \\left( -f'(s)^\\top C^{-1} u(s) + u(s)^\\top C^{-1} (-f'(s)) \\right),\n$$\nwhere $f'(s) = \\frac{d}{ds} f(s)$ is the vector of derivatives of the mean with respect to $s$, and $\\frac{d}{ds} u(s) = -f'(s)$. Using the identity $x^\\top A y = y^\\top A^\\top x$ and the symmetry of $C^{-1}$, we have\n$$\n\\frac{\\partial}{\\partial s} \\log p(r \\mid s) = f'(s)^\\top C^{-1} (r - f(s)).\n$$\nTherefore, the Fisher information is\n$$\nI(s) = \\mathbb{E}\\left[ \\left( f'(s)^\\top C^{-1} (r - f(s)) \\right)^2 \\right] = f'(s)^\\top C^{-1} \\, \\mathbb{E}\\left[ (r - f(s)) (r - f(s))^\\top \\right] \\, C^{-1} f'(s).\n$$\nSince $\\mathbb{E}\\left[ (r - f(s)) (r - f(s))^\\top \\right] = C$, we obtain\n$$\nI(s) = f'(s)^\\top C^{-1} f'(s).\n$$\nThis is the local Fisher information under multivariate normal noise with stimulus-independent covariance.\n\nEstimation from small perturbations:\nWe seek an empirical estimator $\\hat{I}(s_0)$ near a baseline stimulus $s_0$. Using a local linear approximation,\n$$\nf(s) \\approx f(s_0) + f'(s_0) (s - s_0),\n$$\nfor $s$ in a small neighborhood around $s_0$. We estimate $f'(s_0)$ by ordinary least squares (OLS) regression for each neuron $i$:\n$$\nr_i^{(t)} = \\alpha_i + \\beta_i s^{(t)} + \\varepsilon_i^{(t)},\n$$\nwhere $t$ indexes trials and $s^{(t)}$ are stimuli sampled near $s_0$. Under the local model, $\\beta_i$ approximates $f'_i(s_0)$, so we set $\\hat{f}'_i(s_0) = \\hat{\\beta}_i$ and form $\\hat{f}'(s_0) = [\\hat{\\beta}_1, \\dots, \\hat{\\beta}_N]^\\top$. The intercepts $\\hat{\\alpha}_i$ capture $f_i(s_0)$ locally but are not needed for Fisher information directly.\n\nNext, we estimate the stimulus-independent covariance using residuals. Construct residuals\n$$\n\\varepsilon^{(t)} = r^{(t)} - \\hat{\\alpha} - \\hat{\\beta} s^{(t)},\n$$\nwhere $\\hat{\\alpha} = [\\hat{\\alpha}_1, \\dots, \\hat{\\alpha}_N]^\\top$ and $\\hat{\\beta} = [\\hat{\\beta}_1, \\dots, \\hat{\\beta}_N]^\\top$. Use the unbiased sample covariance\n$$\n\\hat{C} = \\frac{1}{T - 1} \\sum_{t=1}^{T} \\left( \\varepsilon^{(t)} - \\bar{\\varepsilon} \\right) \\left( \\varepsilon^{(t)} - \\bar{\\varepsilon} \\right)^\\top,\n$$\nwhere $T$ is the total number of trials across all perturbations and $\\bar{\\varepsilon}$ is the sample mean of residuals. Under the model assumptions, $\\bar{\\varepsilon}$ should be approximately zero if the local linear approximation is adequate and the noise is zero-mean, but subtracting $\\bar{\\varepsilon}$ yields an unbiased estimator regardless.\n\nFinally, the estimator of the local Fisher information is\n$$\n\\hat{I}(s_0) = \\hat{f}'(s_0)^\\top \\, \\hat{C}^{-1} \\, \\hat{f}'(s_0).\n$$\nNumerical stability:\nIf $\\hat{C}$ is ill-conditioned or nearly singular, direct inversion may be numerically unstable. A common remedy is Tikhonov regularization, adding a small $\\epsilon I$ to $\\hat{C}$ prior to inversion:\n$$\n\\hat{I}_\\epsilon(s_0) = \\hat{f}'(s_0)^\\top \\, (\\hat{C} + \\epsilon I)^{-1} \\, \\hat{f}'(s_0),\n$$\nwith $\\epsilon$ chosen proportional to the typical scale of $\\hat{C}$, for example $\\epsilon = \\kappa \\, \\frac{1}{N} \\mathrm{trace}(\\hat{C}) \\times 10^{-8}$ for a small constant $\\kappa$. Alternatively, one may check the condition number $\\kappa(\\hat{C})$ and regularize only if it exceeds a threshold.\n\nAlgorithmic steps per test case:\n1. Fix $N$, $s_0$, perturbations $\\Delta s$, trials per perturbation $T_k$, coefficients $a$, $b$, $c$, and noise parameters $\\sigma$, $\\rho$.\n2. Construct $C$ by $C_{ij} = \\sigma_i \\sigma_j \\rho^{|i - j|}$.\n3. For each $\\Delta s_k$, set $s_k = s_0 + \\Delta s_k$ and generate $T_k$ independent samples $r^{(t)} \\sim \\mathcal{N}(f(s_k), C)$, where $f_i(s_k) = a_i + b_i s_k + c_i s_k^2$.\n4. Aggregate all trials and regress each neuronâ€™s responses $r_i^{(t)}$ on $s^{(t)}$ via OLS to obtain $\\hat{\\alpha}_i$ and $\\hat{\\beta}_i$; set $\\hat{f}'_i(s_0) = \\hat{\\beta}_i$.\n5. Compute residuals $\\varepsilon^{(t)} = r^{(t)} - \\hat{\\alpha} - \\hat{\\beta} s^{(t)}$, then compute $\\hat{C}$ as the unbiased sample covariance of the residuals.\n6. Invert $\\hat{C}$, applying small diagonal regularization if necessary, and compute $\\hat{I}(s_0)$.\n7. Output the scalar $\\hat{I}(s_0)$ for the test case.\n\nThis estimator is consistent under the stated model as the number of trials grows and the perturbations remain within the linear regime. The inclusion of high correlation and few-trial scenarios in the test suite probes numerical conditioning and sample variability, respectively.\n\nThe implementation should be deterministic by fixing random seeds per test case. The final output is a single line with the five estimated Fisher information values in the format $[r_1,r_2,r_3,r_4,r_5]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_covariance(sigma, rho):\n    \"\"\"\n    Construct covariance matrix C with entries C_ij = sigma_i * sigma_j * rho^{|i - j|}.\n    Ensures positive definiteness for |rho| < 1 given positive sigma entries.\n    \"\"\"\n    sigma = np.asarray(sigma, dtype=float)\n    N = sigma.size\n    indices = np.arange(N)\n    # Toeplitz-like correlation structure\n    corr = rho ** np.abs(indices[:, None] - indices[None, :])\n    C = (sigma[:, None] * sigma[None, :]) * corr\n    return C\n\ndef generate_responses(N, a, b, c, s_values, trials_per_s, C, rng):\n    \"\"\"\n    Generate responses r ~ N(f(s), C) for each s_value, with specified number of trials.\n    Returns:\n        S: array of stimuli per trial (shape [T_total])\n        R: array of responses per trial (shape [T_total, N])\n    \"\"\"\n    a = np.asarray(a, dtype=float)\n    b = np.asarray(b, dtype=float)\n    c = np.asarray(c, dtype=float)\n    s_values = np.asarray(s_values, dtype=float)\n    trials_per_s = np.asarray(trials_per_s, dtype=int)\n    assert a.shape[0] == N and b.shape[0] == N and c.shape[0] == N\n    total_trials = int(np.sum(trials_per_s))\n    R = np.zeros((total_trials, N), dtype=float)\n    S = np.zeros(total_trials, dtype=float)\n    mean_template = np.zeros(N, dtype=float)\n\n    idx = 0\n    for s, T in zip(s_values, trials_per_s):\n        # compute mean vector f(s) = a + b*s + c*s^2\n        mean = a + b * s + c * (s ** 2)\n        # sample T trials from multivariate normal with mean and covariance C\n        samples = rng.multivariate_normal(mean, C, size=T)\n        R[idx:idx+T, :] = samples\n        S[idx:idx+T] = s\n        idx += T\n    return S, R\n\ndef estimate_derivative_and_cov(S, R):\n    \"\"\"\n    Estimate f'(s0) via OLS slope per neuron and residual covariance from residuals.\n    Returns:\n        fprime_hat: shape [N]\n        Chat: shape [N, N]\n    \"\"\"\n    T_total, N = R.shape\n    # Design matrix with intercept and stimulus\n    X = np.column_stack([np.ones(T_total), S])\n    # Solve for coefficients per neuron using least squares\n    # alpha_hat and beta_hat\n    coeffs, _, _, _ = np.linalg.lstsq(X, R, rcond=None)  # coeffs shape [2, N]\n    alpha_hat = coeffs[0, :]\n    beta_hat = coeffs[1, :]\n    # Residuals\n    R_hat = X @ coeffs  # shape [T_total, N]\n    residuals = R - R_hat\n    # Unbiased sample covariance across trials (rows are trials)\n    # np.cov expects variables in rows by default; set rowvar=False since variables are columns\n    Chat = np.cov(residuals, rowvar=False, ddof=1)\n    return beta_hat, Chat\n\ndef invert_covariance_with_regularization(C):\n    \"\"\"\n    Invert covariance matrix with a small diagonal regularization if needed.\n    Uses condition number to detect ill-conditioning.\n    \"\"\"\n    # Handle potential numerical issues\n    try:\n        cond = np.linalg.cond(C)\n    except np.linalg.LinAlgError:\n        cond = np.inf\n    if not np.isfinite(cond) or cond > 1e8:\n        # Regularize\n        avg_diag = float(np.mean(np.diag(C)))\n        eps = max(1e-12, 1e-8 * (avg_diag if avg_diag > 0 else 1.0))\n        C_reg = C + eps * np.eye(C.shape[0])\n        return np.linalg.inv(C_reg)\n    else:\n        return np.linalg.inv(C)\n\ndef fisher_information_estimate(fprime_hat, C_inv):\n    \"\"\"\n    Compute scalar Fisher information estimate: f'(s)^T C^{-1} f'(s).\n    \"\"\"\n    return float(fprime_hat.T @ (C_inv @ fprime_hat))\n\ndef run_test_case(case_index, N, s0, deltas, trials_per_delta, a, b, c, sigma, rho):\n    \"\"\"\n    Run a single test case:\n      - construct covariance\n      - generate synthetic data for stimuli s0 + deltas\n      - estimate derivative and covariance\n      - compute Fisher information\n    \"\"\"\n    rng = np.random.default_rng(case_index)  # deterministic seed per case\n    C = construct_covariance(sigma, rho)\n    s_values = s0 + np.asarray(deltas, dtype=float)\n    S, R = generate_responses(N, a, b, c, s_values, trials_per_delta, C, rng)\n    fprime_hat, Chat = estimate_derivative_and_cov(S, R)\n    C_inv = invert_covariance_with_regularization(Chat)\n    I_hat = fisher_information_estimate(fprime_hat, C_inv)\n    return I_hat\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"N\": 6,\n            \"s0\": 0.3,\n            \"deltas\": [-0.1, -0.05, 0.0, 0.05, 0.1],\n            \"trials\": [200, 200, 200, 200, 200],\n            \"a\": [0.5, 1.0, -0.3, 0.7, 0.2, -0.1],\n            \"b\": [1.2, -0.8, 0.5, 1.0, -0.4, 0.9],\n            \"c\": [0.3, 0.0, -0.2, 0.1, 0.05, -0.15],\n            \"sigma\": [0.2, 0.25, 0.22, 0.18, 0.3, 0.27],\n            \"rho\": 0.2\n        },\n        # Test case 2\n        {\n            \"N\": 6,\n            \"s0\": 0.3,\n            \"deltas\": [-0.1, -0.05, 0.0, 0.05, 0.1],\n            \"trials\": [400, 400, 400, 400, 400],\n            \"a\": [0.5, 1.0, -0.3, 0.7, 0.2, -0.1],\n            \"b\": [1.2, -0.8, 0.5, 1.0, -0.4, 0.9],\n            \"c\": [0.3, 0.0, -0.2, 0.1, 0.05, -0.15],\n            \"sigma\": [0.2, 0.25, 0.22, 0.18, 0.3, 0.27],\n            \"rho\": 0.95\n        },\n        # Test case 3\n        {\n            \"N\": 6,\n            \"s0\": 0.4,\n            \"deltas\": [-0.1, 0.0, 0.1],\n            \"trials\": [300, 300, 300],\n            \"a\": [0.1, -0.2, 0.3, 0.0, 0.4, -0.1],\n            \"b\": [-0.4, -0.24, 0.08, 0.6, -0.7, 0.8],  # derived from c and s0 for first three\n            \"c\": [0.5, 0.3, -0.1, 0.2, 0.0, 0.25],\n            \"sigma\": [0.2, 0.2, 0.2, 0.25, 0.22, 0.3],\n            \"rho\": 0.3\n        },\n        # Test case 4\n        {\n            \"N\": 6,\n            \"s0\": 0.5,\n            \"deltas\": [-0.02, 0.0, 0.02],\n            \"trials\": [25, 25, 25],\n            \"a\": [0.3, 0.1, -0.2, 0.0, 0.5, 0.2],\n            \"b\": [0.4, 0.7, -0.9, 0.2, 0.0, 0.3],\n            \"c\": [0.0, 0.05, 0.1, -0.05, 0.02, 0.0],\n            \"sigma\": [0.3, 0.25, 0.35, 0.2, 0.22, 0.28],\n            \"rho\": 0.5\n        },\n        # Test case 5\n        {\n            \"N\": 8,\n            \"s0\": -0.2,\n            \"deltas\": [-0.05, 0.0, 0.05],\n            \"trials\": [100, 200, 100],\n            \"a\": [0.0, 0.2, 0.1, -0.1, 0.3, -0.4, 0.5, 0.0],\n            \"b\": [1.0, -0.5, 0.2, 0.6, -0.7, 0.3, 0.9, -1.1],\n            \"c\": [0.1, -0.2, 0.0, 0.05, 0.02, -0.03, 0.01, 0.04],\n            \"sigma\": [0.15, 0.3, 0.25, 0.1, 0.45, 0.2, 0.35, 0.4],\n            \"rho\": 0.7\n        }\n    ]\n\n    results = []\n    for idx, case in enumerate(test_cases):\n        I_hat = run_test_case(\n            case_index=idx,\n            N=case[\"N\"],\n            s0=case[\"s0\"],\n            deltas=case[\"deltas\"],\n            trials_per_delta=case[\"trials\"],\n            a=case[\"a\"],\n            b=case[\"b\"],\n            c=case[\"c\"],\n            sigma=case[\"sigma\"],\n            rho=case[\"rho\"]\n        )\n        # Format result with a reasonable precision to ensure readable output\n        results.append(f\"{I_hat:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}