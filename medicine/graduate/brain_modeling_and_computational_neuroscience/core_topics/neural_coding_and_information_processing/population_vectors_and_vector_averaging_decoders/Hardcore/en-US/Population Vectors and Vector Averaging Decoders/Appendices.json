{
    "hands_on_practices": [
        {
            "introduction": "The most effective way to understand neural decoding is to build a decoder from the ground up. This practice guides you through the complete computational pipeline, from simulating a population of direction-tuned neurons to decoding novel stimuli. You will first generate synthetic spike data based on a realistic cosine tuning model with Poisson variability, and then, playing the role of the neuroscientist, you will estimate the unknown tuning parameters from this data using linear regression. This exercise  provides a foundational experience in implementing and testing a vector averaging decoder, bridging the gap between theoretical models and practical application.",
            "id": "4010760",
            "problem": "You are given a computational neuroscience task concerning neural population coding of a planar direction stimulus. Assume a population of neurons that each exhibits cosine tuning with respect to a stimulus direction, and that spiking is modeled as a Poisson point process. The objective is to estimate each neuron's tuning parameters and preferred direction from training data and then decode new stimuli using a vector averaging decoder. The entire task must be implemented as a single runnable program.\n\nFundamental basis and assumptions:\n- Each neuron $i$ has an unknown preferred direction $\\theta_i$ and produces spikes according to a cosine tuning law with baseline. The expected firing rate is a function of the stimulus direction. The neural response in trial $t$ is modeled as a Poisson random variable with mean equal to its rate under the stimulus shown in that trial.\n- The stimulus direction is represented on the circle, and angles must be handled in radians. A decoder based on population vectors and vector averaging produces a single direction estimate per trial.\n\nYour program must perform the following steps for each test case in the suite:\n1. Generate training data:\n   - Construct a population of $N$ neurons with specified preferred directions $\\{\\theta_i\\}_{i=1}^N$, baselines $\\{b_i\\}_{i=1}^N$ (in spikes per second), and gains $\\{g_i\\}_{i=1}^N$ (in spikes per second), where the expected rate of neuron $i$ under stimulus angle $\\phi$ is $b_i + g_i \\cos(\\phi - \\theta_i)$.\n   - Simulate $T$ training trials. In each trial $t$, draw a stimulus angle $\\phi_t$ uniformly from the interval $[-\\pi, \\pi)$, and for each neuron $i$, draw a spike count $k_{i,t}$ according to the Poisson distribution with mean $b_i + g_i \\cos(\\phi_t - \\theta_i)$.\n2. Estimate tuning parameters:\n   - Using the training data $\\{(\\phi_t, k_{i,t})\\}$, estimate for each neuron $i$ its baseline $\\hat{b}_i$, gain $\\hat{g}_i$, and preferred direction $\\hat{\\theta}_i$, relying solely on cosine-tuning structure and linear regression principles grounded in the trigonometric identity that converts a phase-shifted cosine into a linear combination of $\\cos(\\phi)$ and $\\sin(\\phi)$ terms.\n   - For numerical stability, ignore any neuron whose estimated gain falls below a small threshold $\\varepsilon$; use $\\varepsilon = 10^{-3}$ spikes per second.\n3. Generate test data and decode:\n   - Simulate $K$ test trials with new stimulus angles $\\phi'_u$ drawn uniformly from $[-\\pi, \\pi)$, and Poisson spike counts $k'_{i,u}$ generated under the true parameters $(b_i, g_i, \\theta_i)$.\n   - Decode each test stimulus angle using a vector averaging decoder derived from the population vector concept: treat each neuron's contribution as a unit vector oriented at its estimated preferred direction, weighted by a nonnegative activity proportional to its baseline-subtracted spike count. Specifically, for neuron $i$, use the weight $\\max(0, k'_{i,u} - \\hat{b}_i)$, and combine only neurons with $\\hat{g}_i \\ge \\varepsilon$.\n   - The decoder must produce a single angle estimate $\\hat{\\phi}'_u$ in radians for each test trial $u$.\n4. Evaluate performance:\n   - Compute the mean absolute circular error over test trials, where the absolute circular error between a decoded angle $\\hat{\\phi}$ and the true angle $\\phi$ is $d(\\hat{\\phi}, \\phi) = \\min_{k \\in \\mathbb{Z}} |\\hat{\\phi} - \\phi + 2\\pi k|$. Express this in radians.\n\nAngle unit requirement: All angles must be handled and reported in radians.\n\nPhysical units requirement: All firing rates must be treated in spikes per second.\n\nFinal output format: Your program should produce a single line of output containing the mean absolute circular error for each test case as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3]$), where each $result_j$ is a floating-point number in radians.\n\nTest suite:\n- Case $1$ (happy path, diverse gains, ample training):\n  - $N = 8$, $T = 200$, $K = 50$.\n  - Preferred directions: $\\theta_i$ evenly spaced on $[-\\pi, \\pi)$, i.e., $\\theta_i = -\\pi + \\frac{2\\pi (i-1)}{N}$ for $i = 1, \\dots, N$.\n  - Baselines: $b_i = 20$ (spikes per second) for all $i$.\n  - Gains: $(g_1, g_2, \\dots, g_8) = (14, 12, 16, 10, 18, 11, 15, 13)$ (spikes per second).\n  - Pseudo-random number generator seeds: training seed $= 101$, test seed $= 202$.\n- Case $2$ (boundary condition with zero-gain neurons):\n  - $N = 8$, $T = 60$, $K = 40$.\n  - Preferred directions: $\\theta_i$ evenly spaced on $[-\\pi, \\pi)$.\n  - Baselines: $b_i = 20$ (spikes per second) for all $i$.\n  - Gains: $(g_1, g_2, \\dots, g_8) = (0, 0, 12, 10, 8, 0, 6, 0)$ (spikes per second).\n  - Pseudo-random number generator seeds: training seed $= 303$, test seed $= 404$.\n- Case $3$ (small-sample, low-gain, noisier setting):\n  - $N = 6$, $T = 30$, $K = 30$.\n  - Preferred directions: $\\theta_i$ evenly spaced on $[-\\pi, \\pi)$.\n  - Baselines: $b_i = 15$ (spikes per second) for all $i$.\n  - Gains: $(g_1, g_2, \\dots, g_6) = (5, 4, 3, 2, 5, 4)$ (spikes per second).\n  - Pseudo-random number generator seeds: training seed $= 505$, test seed $= 606$.\n\nScientific realism constraints:\n- Use the Poisson distribution for spike counts with the specified mean rates to ensure nonnegative integer counts.\n- Ensure that all computations and decoding are performed in radians.\n- Avoid using any external data or user input; use the specified seeds for reproducible randomness.\n\nYour program must produce the single-line output summarizing the three mean absolute circular errors in radians as $[e_1,e_2,e_3]$.",
            "solution": "The problem is assessed to be scientifically grounded, well-posed, objective, and self-contained, and is therefore deemed valid. The following solution outlines the step-by-step procedure for modeling, estimation, and decoding as specified.\n\n### Step 1: Neural Response Model\nThe activity of each neuron in the population is governed by a cosine tuning model. The expected firing rate $r_i$ (in spikes per second, interpreted as spikes per trial) of neuron $i$ in response to a stimulus direction $\\phi$ (in radians) is given by:\n$$\nr_i(\\phi) = b_i + g_i \\cos(\\phi - \\theta_i)\n$$\nwhere $b_i$ is the neuron's baseline firing rate, $g_i$ is its gain (modulation depth), and $\\theta_i$ is its preferred direction. The number of spikes $k_{i,t}$ emitted by neuron $i$ in a given trial $t$ with stimulus $\\phi_t$ is modeled as a random variable drawn from a Poisson distribution with a mean equal to the expected rate:\n$$\nk_{i,t} \\sim \\text{Poisson}(r_i(\\phi_t))\n$$\nThe problem parameters for all test cases ensure that $b_i \\ge g_i$, guaranteeing a non-negative rate $r_i(\\phi) \\ge 0$.\n\n### Step 2: Training Data Generation\nFor each test case, we first synthesize a training dataset. This involves simulating $T$ trials. In each trial $t \\in \\{1, \\dots, T\\}$, a stimulus angle $\\phi_t$ is drawn from a uniform distribution over $[-\\pi, \\pi)$. Subsequently, for each of the $N$ neurons, a spike count $k_{i,t}$ is generated by drawing from the Poisson distribution $\\text{Poisson}(b_i + g_i \\cos(\\phi_t - \\theta_i))$, using the true, pre-specified parameters.\n\n### Step 3: Tuning Parameter Estimation\nThe central task in this step is to estimate the tuning parameters $(\\hat{b}_i, \\hat{g}_i, \\hat{\\theta}_i)$ for each neuron from the generated training data $\\{(\\phi_t, k_{i,t})\\}_{t=1}^T$. This is accomplished through linear regression, based on a trigonometric expansion of the tuning curve.\n\nThe cosine term is expanded as $\\cos(\\phi - \\theta_i) = \\cos\\phi \\cos\\theta_i + \\sin\\phi \\sin\\theta_i$. Substituting this into the rate equation gives:\n$$\nr_i(\\phi) = b_i + (g_i \\cos\\theta_i) \\cos\\phi + (g_i \\sin\\theta_i) \\sin\\phi\n$$\nThis equation has the form of a linear model, $r_i(\\phi) = \\beta_{i,0} \\cdot 1 + \\beta_{i,1} \\cos\\phi + \\beta_{i,2} \\sin\\phi$, with coefficients defined as:\n- $\\beta_{i,0} = b_i$\n- $\\beta_{i,1} = g_i \\cos\\theta_i$\n- $\\beta_{i,2} = g_i \\sin\\theta_i$\n\nWe estimate these coefficients by minimizing the sum of squared errors between the observed spike counts $k_{i,t}$ and the model's prediction. This is a multiple linear regression problem for each neuron, which can be expressed in matrix form for all trials:\n$$\n\\mathbf{k}_i \\approx \\mathbf{X} \\boldsymbol{\\beta}_i\n$$\nHere, $\\mathbf{k}_i = [k_{i,1}, \\dots, k_{i,T}]^T$ is the vector of observed spike counts for neuron $i$, $\\boldsymbol{\\beta}_i = [\\beta_{i,0}, \\beta_{i,1}, \\beta_{i,2}]^T$ is the vector of coefficients to be estimated, and $\\mathbf{X}$ is the $T \\times 3$ design matrix containing the predictor variables for all trials:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & \\cos(\\phi_1) & \\sin(\\phi_1) \\\\\n1 & \\cos(\\phi_2) & \\sin(\\phi_2) \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & \\cos(\\phi_T) & \\sin(\\phi_T)\n\\end{pmatrix}\n$$\nThe standard least-squares solution for the coefficient vector is $\\hat{\\boldsymbol{\\beta}}_i = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{k}_i$. We solve this system for all $N$ neurons to obtain the estimated coefficients $(\\hat{\\beta}_{i,0}, \\hat{\\beta}_{i,1}, \\hat{\\beta}_{i,2})$ for each neuron $i$.\n\nFrom these estimated coefficients, we recover the estimates for the original tuning parameters:\n- Estimated Baseline: $\\hat{b}_i = \\hat{\\beta}_{i,0}$\n- Estimated Gain: $\\hat{g}_i = \\sqrt{\\hat{\\beta}_{i,1}^2 + \\hat{\\beta}_{i,2}^2}$\n- Estimated Preferred Direction: $\\hat{\\theta}_i = \\text{atan2}(\\hat{\\beta}_{i,2}, \\hat{\\beta}_{i,1})$\n\n### Step 4: Decoding Test Stimuli\nAfter parameter estimation, we generate a new set of $K$ test trials. For each trial $u \\in \\{1, \\dots, K\\}$, a stimulus angle $\\phi'_u$ is drawn uniformly from $[-\\pi, \\pi)$, and spike counts $k'_{i,u}$ are generated using the true parameters.\n\nThe stimulus angle for each test trial is then decoded using a vector averaging decoder. This method computes a population vector, $\\vec{V}_u$, for each trial $u$ by summing vectorial contributions from all neurons deemed sufficiently tuned:\n$$\n\\vec{V}_u = \\sum_{i=1}^N w_{i,u} \\cdot \\vec{d}_i\n$$\n- A neuron $i$ contributes to the sum only if its estimated gain $\\hat{g}_i$ is above a threshold, $\\hat{g}_i \\ge \\varepsilon$, where $\\varepsilon=10^{-3}$.\n- The contribution of each valid neuron is a vector $\\vec{d}_i = [\\cos(\\hat{\\theta}_i), \\sin(\\hat{\\theta}_i)]^T$, which points in the direction of its estimated preferred angle $\\hat{\\theta}_i$.\n- This vector is weighted by the neuron's rectified, baseline-subtracted activity: $w_{i,u} = \\max(0, k'_{i,u} - \\hat{b}_i)$.\n\nThe decoded angle for the trial, $\\hat{\\phi}'_u$, is the angle of the resultant population vector $\\vec{V}_u = [V_{u,x}, V_{u,y}]^T$:\n$$\n\\hat{\\phi}'_u = \\text{atan2}(V_{u,y}, V_{u,x})\n$$\n\n### Step 5: Performance Evaluation\nThe decoder's accuracy is evaluated by computing the mean absolute circular error over all $K$ test trials. The absolute circular error for a single trial is the shortest angular distance between the true stimulus angle $\\phi'_u$ and the decoded angle $\\hat{\\phi}'_u$:\n$$\nd(\\hat{\\phi}'_u, \\phi'_u) = \\min_{k \\in \\mathbb{Z}} |\\hat{\\phi}'_u - \\phi'_u + 2\\pi k|\n$$\nThis is equivalent to wrapping the angle difference $\\hat{\\phi}'_u - \\phi'_u$ into the interval $[-\\pi, \\pi]$ and taking its absolute value. The final reported value for each test case is the average of these errors over the $K$ test trials. The entire procedure is encapsulated in a program that executes all three test cases and reports the results according to the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(N, T, K, thetas, bs, gs, train_seed, test_seed, epsilon):\n    \"\"\"\n    Solves a single test case for the neural decoding problem.\n\n    This function performs training data generation, parameter estimation,\n    test data generation, decoding, and performance evaluation.\n    \"\"\"\n    # === Step 1: Generate training data ===\n    rng_train = np.random.default_rng(train_seed)\n    train_stimuli = rng_train.uniform(-np.pi, np.pi, size=T)\n    \n    # train_spikes will have shape (N, T)\n    train_spikes = np.zeros((N, T))\n    for i in range(N):\n        # The problem statement's parameters ensure b_i >= g_i, so rates are non-negative.\n        rates = bs[i] + gs[i] * np.cos(train_stimuli - thetas[i])\n        train_spikes[i, :] = rng_train.poisson(rates)\n\n    # === Step 2: Estimate tuning parameters ===\n    # Design matrix X, shape (T, 3) for linear regression\n    X = np.vstack([np.ones(T), np.cos(train_stimuli), np.sin(train_stimuli)]).T\n    \n    # Solve the least squares problem for all neurons simultaneously.\n    # The model is k_counts.T (shape T,N) = X (shape T,3) @ beta_matrix.T (shape 3,N)\n    # np.linalg.lstsq solves ax=b for x. Here a=X, b=train_spikes.T, x=beta_matrix.\n    beta_matrix, _, _, _ = np.linalg.lstsq(X, train_spikes.T, rcond=None)\n    \n    # beta_matrix has shape (3, N). Rows correspond to b_hat, c_hat, s_hat.\n    b_hat = beta_matrix[0, :]\n    c_hat = beta_matrix[1, :]\n    s_hat = beta_matrix[2, :]\n    \n    g_hat = np.sqrt(c_hat**2 + s_hat**2)\n    theta_hat = np.arctan2(s_hat, c_hat)\n\n    # === Step 3: Generate test data and decode ===\n    rng_test = np.random.default_rng(test_seed)\n    test_stimuli = rng_test.uniform(-np.pi, np.pi, size=K)\n    \n    # test_spikes will have shape (N, K)\n    test_spikes = np.zeros((N, K))\n    for i in range(N):\n        rates = bs[i] + gs[i] * np.cos(test_stimuli - thetas[i])\n        test_spikes[i, :] = rng_test.poisson(rates)\n        \n    # Decoding using vector averaging\n    valid_neurons_mask = g_hat >= epsilon\n    \n    # Weights for population vector, shape (N, K)\n    weights = test_spikes - b_hat[:, np.newaxis]\n    weights[weights < 0] = 0.0\n    weights[~valid_neurons_mask, :] = 0.0  # Zero out weights for invalid neurons\n    \n    # Preferred direction vectors, one x and one y component per neuron\n    pref_dir_vectors_x = np.cos(theta_hat)\n    pref_dir_vectors_y = np.sin(theta_hat)\n    \n    # Population vector components, summed over neurons for each trial, shape (K,)\n    pop_vector_x = pref_dir_vectors_x @ weights\n    pop_vector_y = pref_dir_vectors_y @ weights\n    \n    decoded_stimuli = np.arctan2(pop_vector_y, pop_vector_x)\n\n    # === Step 4: Evaluate performance ===\n    # Calculate absolute circular error\n    error_diff = decoded_stimuli - test_stimuli\n    circular_errors = np.abs((error_diff + np.pi) % (2 * np.pi) - np.pi)\n    \n    mean_abs_error = np.mean(circular_errors)\n    \n    return mean_abs_error\n\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: happy path, diverse gains, ample training\n        {\n            \"N\": 8, \"T\": 200, \"K\": 50,\n            \"b_val\": 20.0, \"gs\": [14.0, 12.0, 16.0, 10.0, 18.0, 11.0, 15.0, 13.0],\n            \"train_seed\": 101, \"test_seed\": 202\n        },\n        # Case 2: boundary condition with zero-gain neurons\n        {\n            \"N\": 8, \"T\": 60, \"K\": 40,\n            \"b_val\": 20.0, \"gs\": [0.0, 0.0, 12.0, 10.0, 8.0, 0.0, 6.0, 0.0],\n            \"train_seed\": 303, \"test_seed\": 404\n        },\n        # Case 3: small-sample, low-gain, noisier setting\n        {\n            \"N\": 6, \"T\": 30, \"K\": 30,\n            \"b_val\": 15.0, \"gs\": [5.0, 4.0, 3.0, 2.0, 5.0, 4.0],\n            \"train_seed\": 505, \"test_seed\": 606\n        }\n    ]\n\n    results = []\n    epsilon = 1e-3\n\n    for case in test_cases:\n        N = case[\"N\"]\n        # True preferred directions are evenly spaced on [-pi, pi)\n        thetas = -np.pi + (2 * np.pi * np.arange(N)) / N\n        bs = np.full(N, case[\"b_val\"])\n        gs = np.array(case[\"gs\"])\n        \n        result = solve_case(\n            N=N, T=case[\"T\"], K=case[\"K\"],\n            thetas=thetas, bs=bs, gs=gs,\n            train_seed=case[\"train_seed\"], test_seed=case[\"test_seed\"],\n            epsilon=epsilon\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Neural decoding rarely occurs in a vacuum; the brain constantly merges incoming sensory evidence with prior expectations to form perceptions. This practice explores how to formally implement this process using Bayesian inference, where a prior belief about the stimulus is combined with the sensory likelihood represented by the population vector. This exercise  provides a powerful and intuitive demonstration of how Bayesian computation can be realized through the simple addition of vectors, yielding a Maximum A Posteriori (MAP) estimate that is more robust than one based on sensory evidence alone.",
            "id": "4010755",
            "problem": "A population of $N=8$ orientation-tuned neurons in primary visual cortex encodes a circular variable (orientation) $\\theta \\in [0,2\\pi)$. The neurons have evenly spaced preferred directions $\\phi_{k} \\in \\{0,\\frac{\\pi}{4},\\frac{\\pi}{2},\\frac{3\\pi}{4},\\pi,\\frac{5\\pi}{4},\\frac{3\\pi}{2},\\frac{7\\pi}{4}\\}$ for $k=1,\\dots,8$. Assume independent Poisson spiking with identical baseline rates and cosine-like tuning of identical shape and gain, and a dense, approximately uniform distribution of preferred directions. Under these conditions, the $\\theta$-dependent part of the log-likelihood is well-approximated by its first harmonic in the circular Fourier series and is compatible with a vector-averaging decoder that uses spike counts as weights over preferred directions. The recorded spike counts for a single observation window are\n$$\nn_{1}=14,\\quad n_{2}=23,\\quad n_{3}=21,\\quad n_{4}=10,\\quad n_{5}=7,\\quad n_{6}=5,\\quad n_{7}=8,\\quad n_{8}=12.\n$$\nIn addition, the observer has a non-uniform prior over $\\theta$ that is von Mises with mean direction $\\theta_{0}=\\frac{\\pi}{6}$ and concentration parameter $\\kappa_{0}=10$, i.e., $\\pi(\\theta)\\propto \\exp\\!\\big(\\kappa_{0}\\cos(\\theta-\\theta_{0})\\big)$. Using Bayesâ€™ rule and the modeling assumptions stated, determine the Maximum A Posteriori (MAP) estimate of $\\theta$ produced by a vector-averaging decoder that incorporates this prior. Express your final answer in radians and round your answer to four significant figures.",
            "solution": "The problem asks for the Maximum A Posteriori (MAP) estimate of a stimulus orientation $\\theta$, given a set of spike counts from a population of neurons and a prior distribution over $\\theta$. This is a canonical problem in Bayesian inference.\n\nAccording to Bayes' rule, the posterior probability of the stimulus $\\theta$ given the observed spike counts $\\mathbf{n} = (n_1, \\dots, n_N)$ is given by:\n$$\nP(\\theta|\\mathbf{n}) = \\frac{P(\\mathbf{n}|\\theta)\\pi(\\theta)}{P(\\mathbf{n})}\n$$\nwhere $P(\\mathbf{n}|\\theta)$ is the likelihood of observing the spike counts given the stimulus, $\\pi(\\theta)$ is the prior probability of the stimulus, and $P(\\mathbfn)$ is the marginal probability of the data, which serves as a normalization constant.\n\nThe MAP estimate, $\\hat{\\theta}_{MAP}$, is the value of $\\theta$ that maximizes the posterior probability $P(\\theta|\\mathbf{n})$. Since the denominator $P(\\mathbf{n})$ is independent of $\\theta$, this is equivalent to maximizing the product of the likelihood and the prior:\n$$\n\\hat{\\theta}_{MAP} = \\underset{\\theta}{\\arg\\max} \\, [P(\\mathbf{n}|\\theta)\\pi(\\theta)]\n$$\nThis is also equivalent to maximizing the log-posterior, $\\ln P(\\theta|\\mathbf{n}) = \\ln P(\\mathbf{n}|\\theta) + \\ln \\pi(\\theta) - \\ln P(\\mathbf{n})$.\n\nThe problem states that the decoding process is compatible with a vector-averaging decoder and that the $\\theta$-dependent part of the log-likelihood is well-approximated by its first harmonic. For independent Poisson neurons with cosine tuning, the log-likelihood can be shown to be approximately proportional to $\\sum_{k=1}^{N} n_k \\cos(\\theta - \\phi_k)$, where $n_k$ are the spike counts and $\\phi_k$ are the preferred directions. This expression can be rewritten as:\n$$\n\\sum_{k=1}^{N} n_k \\cos(\\theta - \\phi_k) = \\cos\\theta \\left(\\sum_{k=1}^{N} n_k \\cos\\phi_k\\right) + \\sin\\theta \\left(\\sum_{k=1}^{N} n_k \\sin\\phi_k\\right)\n$$\nThe terms in the parentheses are the components of the population vector, $V_{pop} = (V_{pop,x}, V_{pop,y})$, where $V_{pop,x} = \\sum_k n_k \\cos\\phi_k$ and $V_{pop,y} = \\sum_k n_k \\sin\\phi_k$. The expression is thus equivalent to $|V_{pop}|\\cos(\\theta - \\hat{\\theta}_{ML})$, where $|V_{pop}|$ is the magnitude of the population vector and $\\hat{\\theta}_{ML} = \\text{atan2}(V_{pop,y}, V_{pop,x})$ is its angle, which corresponds to the Maximum Likelihood (ML) estimate of $\\theta$.\n\nTherefore, the likelihood function can be modeled as a von Mises distribution, centered at $\\hat{\\theta}_{ML}$ with a concentration parameter proportional to $|V_{pop}|$:\n$$\nP(\\mathbf{n}|\\theta) \\propto \\exp(|V_{pop}|\\cos(\\theta - \\hat{\\theta}_{ML}))\n$$\nThis is a mathematical formalization of using a vector-averaging decoder. The \"evidence\" from the neural data is encapsulated by the population vector $V_{pop}$.\n\nThe prior distribution is given as a von Mises distribution with mean direction $\\theta_0 = \\frac{\\pi}{6}$ and concentration parameter $\\kappa_0=10$:\n$$\n\\pi(\\theta) \\propto \\exp(\\kappa_0 \\cos(\\theta - \\theta_0))\n$$\n\nThe log-posterior is therefore:\n$$\n\\ln P(\\theta|\\mathbf{n}) \\propto |V_{pop}|\\cos(\\theta - \\hat{\\theta}_{ML}) + \\kappa_0\\cos(\\theta - \\theta_0)\n$$\nMaximizing this expression is equivalent to finding the angle of the sum of two vectors. The first vector is the population vector $V_{pop}$ (representing the likelihood), and the second vector, $V_{prior}$, has a magnitude equal to the prior's concentration $\\kappa_0$ and a direction equal to the prior's mean $\\theta_0$.\n\nLet $V_{MAP} = V_{pop} + V_{prior}$. The MAP estimate $\\hat{\\theta}_{MAP}$ is the angle of this resultant vector.\n\nFirst, we compute the population vector $V_{pop} = (V_{pop,x}, V_{pop,y})$ using the given spike counts $n_k$ and preferred directions $\\phi_k$.\nThe preferred directions are $\\phi_k = (k-1)\\frac{\\pi}{4}$ for $k=1, \\dots, 8$:\n$\\{0, \\frac{\\pi}{4}, \\frac{\\pi}{2}, \\frac{3\\pi}{4}, \\pi, \\frac{5\\pi}{4}, \\frac{3\\pi}{2}, \\frac{7\\pi}{4}\\}$.\nThe spike counts are $n = \\{14, 23, 21, 10, 7, 5, 8, 12\\}$.\n\nThe x-component of the population vector is:\n$$\nV_{pop,x} = \\sum_{k=1}^{8} n_k \\cos(\\phi_k) = 14\\cos(0) + 23\\cos(\\frac{\\pi}{4}) + 21\\cos(\\frac{\\pi}{2}) + 10\\cos(\\frac{3\\pi}{4}) + 7\\cos(\\pi) + 5\\cos(\\frac{5\\pi}{4}) + 8\\cos(\\frac{3\\pi}{2}) + 12\\cos(\\frac{7\\pi}{4})\n$$\n$$\nV_{pop,x} = 14(1) + 23(\\frac{\\sqrt{2}}{2}) + 21(0) + 10(-\\frac{\\sqrt{2}}{2}) + 7(-1) + 5(-\\frac{\\sqrt{2}}{2}) + 8(0) + 12(\\frac{\\sqrt{2}}{2})\n$$\n$$\nV_{pop,x} = (14-7) + (23-10-5+12)\\frac{\\sqrt{2}}{2} = 7 + 20\\frac{\\sqrt{2}}{2} = 7 + 10\\sqrt{2}\n$$\n\nThe y-component of the population vector is:\n$$\nV_{pop,y} = \\sum_{k=1}^{8} n_k \\sin(\\phi_k) = 14\\sin(0) + 23\\sin(\\frac{\\pi}{4}) + 21\\sin(\\frac{\\pi}{2}) + 10\\sin(\\frac{3\\pi}{4}) + 7\\sin(\\pi) + 5\\sin(\\frac{5\\pi}{4}) + 8\\sin(\\frac{3\\pi}{2}) + 12\\sin(\\frac{7\\pi}{4})\n$$\n$$\nV_{pop,y} = 14(0) + 23(\\frac{\\sqrt{2}}{2}) + 21(1) + 10(\\frac{\\sqrt{2}}{2}) + 7(0) + 5(-\\frac{\\sqrt{2}}{2}) + 8(-1) + 12(-\\frac{\\sqrt{2}}{2})\n$$\n$$\nV_{pop,y} = (21-8) + (23+10-5-12)\\frac{\\sqrt{2}}{2} = 13 + 16\\frac{\\sqrt{2}}{2} = 13 + 8\\sqrt{2}\n$$\n\nNext, we define the prior vector $V_{prior} = (V_{prior,x}, V_{prior,y})$ with magnitude $\\kappa_0=10$ and angle $\\theta_0=\\frac{\\pi}{6}$.\n$$\nV_{prior,x} = \\kappa_0 \\cos(\\theta_0) = 10\\cos(\\frac{\\pi}{6}) = 10(\\frac{\\sqrt{3}}{2}) = 5\\sqrt{3}\n$$\n$$\nV_{prior,y} = \\kappa_0 \\sin(\\theta_0) = 10\\sin(\\frac{\\pi}{6}) = 10(\\frac{1}{2}) = 5\n$$\n\nThe resultant vector for the MAP estimate is $V_{MAP} = V_{pop} + V_{prior}$.\nIts components are:\n$$\nV_{MAP,x} = V_{pop,x} + V_{prior,x} = (7 + 10\\sqrt{2}) + 5\\sqrt{3} = 7 + 10\\sqrt{2} + 5\\sqrt{3}\n$$\n$$\nV_{MAP,y} = V_{pop,y} + V_{prior,y} = (13 + 8\\sqrt{2}) + 5 = 18 + 8\\sqrt{2}\n$$\n\nNow, we compute the numerical values of the components:\n$$\nV_{MAP,x} \\approx 7 + 10(1.41421) + 5(1.73205) = 7 + 14.1421 + 8.66025 = 29.80235\n$$\n$$\nV_{MAP,y} \\approx 18 + 8(1.41421) = 18 + 11.31368 = 29.31368\n$$\n\nThe MAP estimate is the angle of this resultant vector:\n$$\n\\hat{\\theta}_{MAP} = \\text{atan2}(V_{MAP,y}, V_{MAP,x}) = \\text{atan2}(29.31368, 29.80235)\n$$\nSince both components are positive, the angle is in the first quadrant.\n$$\n\\hat{\\theta}_{MAP} = \\arctan\\left(\\frac{29.31368}{29.80235}\\right) \\approx 0.777095 \\text{ radians}\n$$\n\nRounding the result to four significant figures, we get $0.7771$.",
            "answer": "$$\\boxed{0.7771}$$"
        },
        {
            "introduction": "Developing a functional decoder is the first step, but rigorously quantifying its performance is what transforms a model into a scientific tool. This practice introduces the gold-standard methodology for model evaluation: K-fold cross-validation. By systematically partitioning data into training and testing sets, you will learn to obtain an unbiased estimate of your decoder's ability to generalize to new, unseen data, a crucial step to avoid overfitting. This exercise  will solidify your skills in implementing robust evaluation protocols and calculating key performance metrics, such as mean absolute error and bias, that are essential in the field of brain-computer interfaces.",
            "id": "4010791",
            "problem": "You are tasked with implementing and evaluating a Vector Averaging Decoder (VAD) for directional stimuli using a realistic population of cosine-tuned neurons. The evaluation must use K-fold cross-validation (CV) and produce well-defined performance metrics. All angles must be expressed in radians. Every numerical result must be rounded to six decimal places.\n\nFundamental basis for the modeling:\n- Neuronal directional tuning is modeled by cosine tuning. For neuron $i$ with preferred direction $\\phi_i$, its mean firing rate in response to stimulus direction $\\theta$ is a rectified cosine function of the form $b + k \\cos(\\theta - \\phi_i)$, where $b \\ge 0$ is the baseline rate and $k \\ge 0$ is the modulation amplitude. To enforce non-negative rates, rectify by $\\max(0, b + k \\cos(\\theta - \\phi_i))$.\n- Spiking variability follows a Poisson process conditional on the mean rate. For trial $t$ and neuron $i$, the observed count $r_{i,t}$ is distributed as a Poisson random variable with mean equal to the rectified tuning rate.\n- The Vector Averaging Decoder (VAD) constructs a population vector from baseline-subtracted neural activities and neuron-specific preferred direction unit vectors, and decodes the stimulus direction as the angle of the resultant vector, computed via the two-argument arctangent function.\n- K-fold cross-validation (CV) partitions the $T$ trials into $K$ contiguous, equally sized folds. For each fold, the training set estimates per-neuron baselines by averaging their counts over the training trials, and the decoder is applied to the held-out test set. Aggregate all held-out trials across folds for computing metrics.\n\nRequired decoding and evaluation protocol:\n1. Generate a synthetic dataset for each test case as follows:\n   - Draw $N$ preferred directions $\\phi_i$ independently and uniformly from $[0, 2\\pi)$.\n   - Draw $T$ stimulus directions $\\theta_t$ independently and uniformly from $[0, 2\\pi)$.\n   - Compute mean rates $\\lambda_{i,t} = \\max(0, b + k \\cos(\\theta_t - \\phi_i))$.\n   - Sample counts $r_{i,t} \\sim \\mathrm{Poisson}(\\lambda_{i,t})$.\n2. Perform $K$-fold CV with contiguous folds. Let $L = T/K$ be the fold length (assume $K$ divides $T$). For fold $f \\in \\{0, 1, \\ldots, K-1\\}$, define the test indices $t \\in \\{fL, fL+1, \\ldots, (f+1)L - 1\\}$ and training indices as all others. For each fold:\n   - Estimate per-neuron baselines $\\hat{b}_i$ as the mean of $r_{i,t}$ over training trials.\n   - For each test trial $t$, compute the decoded angle $\\hat{\\theta}_t$ by forming the population vector using baseline-subtracted counts and unit vectors corresponding to $\\phi_i$, and taking its angle using the two-argument arctangent. If the resultant vector magnitude equals zero, set $\\hat{\\theta}_t = 0$ by convention.\n3. For all held-out trials (aggregated across folds), compute:\n   - Mean Absolute Circular Error (MACE): $\\frac{1}{T} \\sum_{t=1}^{T} \\left| \\mathrm{wrap}(\\hat{\\theta}_t - \\theta_t) \\right|$, where $\\mathrm{wrap}(\\alpha)$ maps $\\alpha$ into $(-\\pi, \\pi]$ using the identity $\\mathrm{wrap}(\\alpha) = \\arctan2(\\sin(\\alpha), \\cos(\\alpha))$.\n   - Circular Bias (BIAS): $\\frac{1}{T} \\sum_{t=1}^{T} \\mathrm{wrap}(\\hat{\\theta}_t - \\theta_t)$.\n   - Fraction within tolerance $\\tau$ (FRAC): the fraction of held-out trials with $|\\mathrm{wrap}(\\hat{\\theta}_t - \\theta_t)| \\le \\tau$.\n4. Angle unit is radians for both inputs and outputs. Baseline estimates, VAD decoding, and error computations must strictly follow the above definitions.\n\nYour program must implement the above protocol and produce results for the following test suite. In each case, use an independent random number generator with a fixed seed equal to $42 + s$, where $s$ is the test case index starting from $0$.\n\nTest suite (each tuple is $(N, T, K, b, k, \\tau)$):\n- Case $0$: $(50, 200, 5, 15, 10, 0.5)$\n- Case $1$: $(8, 120, 6, 10, 12, 0.5)$\n- Case $2$: $(4, 100, 4, 10, 10, 0.4)$\n- Case $3$: $(60, 240, 8, 20, 0, 0.5)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as an outer list of per-test-case results, where each per-test-case result is the list $[\\mathrm{MACE}, \\mathrm{BIAS}, \\mathrm{FRAC}]$ with all entries rounded to six decimal places. The line must be a comma-separated list enclosed in square brackets, with no spaces. For example: $[[0.123456,-0.000001,0.987654],[\\ldots],[\\ldots],[\\ldots]]$.",
            "solution": "The problem statement is assessed to be valid. It is scientifically sound, mathematically well-posed, and provides a complete and unambiguous specification for a computational neuroscience simulation. We may therefore proceed with a solution.\n\n**1. Foundational Models**\n\nFirst, we define the components of our neural encoding model.\n\n**Neuronal Tuning Model:**\nThe firing rate of each neuron is tuned to a specific direction. For a given neuron $i$ with a preferred direction $\\phi_i$, its mean firing rate $\\lambda_{i,t}$ in response to a stimulus presented at direction $\\theta_t$ is described by a rectified cosine tuning curve. This is a standard model for direction-selective neurons, for instance in the motor cortex. The formula is:\n$$\n\\lambda_{i,t} = \\max(0, b + k \\cos(\\theta_t - \\phi_i))\n$$\nHere, $b \\ge 0$ represents the baseline firing rate (the neuron's activity in the absence of directional modulation), and $k \\ge 0$ is the modulation amplitude, which determines how strongly the neuron's firing rate is affected by the stimulus direction. The $\\max(0, \\cdot)$ function ensures that the firing rate, being a physical quantity, cannot be negative.\n\n**Spiking Variability Model:**\nNeural spike trains exhibit inherent randomness. This variability is modeled using a Poisson process. The number of spikes (count) $r_{i,t}$ recorded from neuron $i$ during trial $t$ is a random variable drawn from a Poisson distribution, with the mean given by the tuning curve $\\lambda_{i,t}$:\n$$\nr_{i,t} \\sim \\mathrm{Poisson}(\\lambda_{i,t})\n$$\n\n**2. Data Generation Protocol**\n\nFor each test case, we generate a synthetic dataset according to the following specifications:\n- A population of $N$ neurons is created. Each neuron $i$ is assigned a preferred direction $\\phi_i$, drawn independently and uniformly from the interval $[0, 2\\pi)$.\n- A sequence of $T$ trials is simulated. For each trial $t$, a stimulus direction $\\theta_t$ is drawn independently and uniformly from $[0, 2\\pi)$.\n- For each neuron $i$ and trial $t$, the mean firing rate $\\lambda_{i,t}$ is calculated using the rectified cosine tuning model.\n- The corresponding spike count $r_{i,t}$ is then sampled from the Poisson distribution with mean $\\lambda_{i,t}$.\nThis process generates the matrix of spike counts $r_{i,t}$, along with the true stimulus directions $\\theta_t$ and neuron preferred directions $\\phi_i$.\n\n**3. Decoding and Cross-Validation**\n\nTo evaluate the decoder's performance robustly, a $K$-fold cross-validation procedure is employed. This method prevents overfitting by ensuring that the data used to train the decoder (i.e., estimate its parameters) is separate from the data used to test it.\n\n**Partitioning:** The $T$ trials are partitioned into $K$ non-overlapping, contiguous blocks (folds) of equal size $L = T/K$.\n\n**Iteration:** The process iterates $K$ times. In each iteration $f \\in \\{0, 1, \\ldots, K-1\\}$, one fold is designated as the test set, and the remaining $K-1$ folds constitute the training set.\n\n**Baseline Estimation:** A critical parameter for the VAD is the baseline firing rate of each neuron. The problem specifies an empirical estimation method. For each neuron $i$, its baseline rate $\\hat{b}_i$ is estimated by computing the average of its observed spike counts $r_{i,t}$ across all trials $t$ belonging to the current training set:\n$$\n\\hat{b}_i = \\frac{1}{T_{\\text{train}}} \\sum_{t \\in \\text{train}} r_{i,t}\n$$\nwhere $T_{\\text{train}}$ is the number of trials in the training set.\n\n**Vector Averaging Decoder (VAD):** For each trial $t$ in the test set, the stimulus direction is decoded. The VAD algorithm works as follows:\n- For each neuron $i$, its contribution to the population activity is its baseline-subtracted firing rate, $(r_{i,t} - \\hat{b}_i)$. This value represents how much the neuron's activity deviates from its average.\n- A population vector $\\vec{P}_t$ is formed by a weighted sum of unit vectors pointing in each neuron's preferred direction. The weight for each neuron is its baseline-subtracted rate.\n$$\n\\vec{P}_t = \\sum_{i=1}^{N} (r_{i,t} - \\hat{b}_i) \\vec{c}_i = \\sum_{i=1}^{N} (r_{i,t} - \\hat{b}_i) \\begin{bmatrix} \\cos(\\phi_i) \\\\ \\sin(\\phi_i) \\end{bmatrix} = \\begin{bmatrix} P_{t,x} \\\\ P_{t,y} \\end{bmatrix}\n$$\n- The decoded angle $\\hat{\\theta}_t$ is the direction of the resulting population vector $\\vec{P}_t$. This is computed using the two-argument arctangent function, $\\mathrm{atan2}$, which correctly resolves the angle in all four quadrants.\n$$\n\\hat{\\theta}_t = \\mathrm{atan2}(P_{t,y}, P_{t,x})\n$$\n- By convention, if the magnitude of the population vector is zero, $\\|\\vec{P}_t\\| = 0$, the decoded angle is set to $\\hat{\\theta}_t = 0$.\n\nThis procedure is repeated for all $K$ folds, yielding a decoded angle $\\hat{\\theta}_t$ for every trial $t=1, \\ldots, T$.\n\n**4. Performance Evaluation Metrics**\n\nAfter cross-validation, the performance of the decoder is quantified by comparing the decoded angles $\\hat{\\theta}_t$ with the true stimulus angles $\\theta_t$ for all $T$ trials. As we are dealing with circular data (angles), specialized error metrics are required.\n\n**Circular Error:** For each trial $t$, the circular error $\\Delta\\theta_t$ is the shortest angle between $\\hat{\\theta}_t$ and $\\theta_t$. It is calculated by wrapping their difference into the interval $(-\\pi, \\pi]$:\n$$\n\\Delta\\theta_t = \\mathrm{wrap}(\\hat{\\theta}_t - \\theta_t) = \\mathrm{atan2}(\\sin(\\hat{\\theta}_t - \\theta_t), \\cos(\\hat{\\theta}_t - \\theta_t))\n$$\nUsing these errors, we compute three aggregate metrics:\n\n- **Mean Absolute Circular Error (MACE):** The average absolute size of the decoding error. It measures the precision of the decoder.\n$$\n\\mathrm{MACE} = \\frac{1}{T} \\sum_{t=1}^{T} |\\Delta\\theta_t|\n$$\n\n- **Circular Bias (BIAS):** The average signed decoding error. It measures any systematic tendency of the decoder to overestimate or underestimate the angle.\n$$\n\\mathrm{BIAS} = \\frac{1}{T} \\sum_{t=1}^{T} \\Delta\\theta_t\n$$\n\n- **Fraction within Tolerance (FRAC):** The proportion of trials where the absolute circular error is within a specified tolerance $\\tau$.\n$$\n\\mathrm{FRAC} = \\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{I}(|\\Delta\\theta_t| \\le \\tau)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise.\n\nThe implementation will execute this entire protocol for each test case defined in the problem, using a distinct random seed for each case to ensure reproducibility. All numerical results will be rounded to six decimal places as required.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a Vector Averaging Decoder (VAD) for\n    directional stimuli using a population of cosine-tuned neurons.\n    The evaluation uses K-fold cross-validation and computes MACE, BIAS, and FRAC metrics.\n    \"\"\"\n\n    test_cases = [\n        # (N, T, K, b, k, tau)\n        (50, 200, 5, 15, 10, 0.5), # Case 0\n        (8, 120, 6, 10, 12, 0.5), # Case 1\n        (4, 100, 4, 10, 10, 0.4), # Case 2\n        (60, 240, 8, 20, 0, 0.5),  # Case 3\n    ]\n\n    all_results_formatted = []\n\n    for s, case in enumerate(test_cases):\n        N, T, K, b, k, tau = case\n        seed = 42 + s\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Generate synthetic dataset\n        # Preferred directions for N neurons, phi_i ~ U[0, 2*pi)\n        phi = rng.uniform(0, 2 * np.pi, size=N)  # Shape: (N,)\n        \n        # Stimulus directions for T trials, theta_t ~ U[0, 2*pi)\n        theta = rng.uniform(0, 2 * np.pi, size=T)  # Shape: (T,)\n\n        # Compute mean firing rates lambda_it = max(0, b + k*cos(theta_t - phi_i))\n        # Use broadcasting to compute difference for all (i, t) pairs\n        # theta[np.newaxis, :] -> shape (1, T)\n        # phi[:, np.newaxis]   -> shape (N, 1)\n        # Difference results in shape (N, T)\n        angle_diff = theta[np.newaxis, :] - phi[:, np.newaxis]\n        mean_rates = b + k * np.cos(angle_diff)\n        mean_rates[mean_rates < 0] = 0  # Rectification\n        \n        # Sample spike counts r_it ~ Poisson(lambda_it)\n        r = rng.poisson(mean_rates)  # Shape: (N, T)\n\n        # Arrays to store aggregated results from all test folds\n        all_theta_true = np.zeros(T)\n        all_theta_decoded = np.zeros(T)\n        \n        # Step 2: Perform K-fold cross-validation\n        fold_length = T // K\n        indices = np.arange(T)\n        \n        for f in range(K):\n            test_indices = indices[f * fold_length : (f + 1) * fold_length]\n            train_indices = np.setdiff1d(indices, test_indices, assume_unique=True)\n            \n            # Estimate per-neuron baselines b_hat_i from training data\n            # Average spike counts for each neuron over training trials\n            r_train = r[:, train_indices]\n            b_hat = np.mean(r_train, axis=1)  # Shape: (N,)\n            \n            # Decode each trial in the current test fold\n            r_test = r[:, test_indices]      # Shape: (N, fold_length)\n            theta_test = theta[test_indices] # Shape: (fold_length,)\n            \n            # Subtract estimated baselines from test counts\n            r_subtracted = r_test - b_hat[:, np.newaxis] # Broadcasting b_hat\n            \n            # Create unit vectors for preferred directions [c_x, c_y]\n            c_vectors = np.vstack([np.cos(phi), np.sin(phi)]) # Shape: (2, N)\n            \n            # Compute population vector P_t by summing weighted unit vectors\n            # P = c_vectors @ r_subtracted\n            # (2, N) @ (N, fold_length) results in (2, fold_length)\n            P = c_vectors @ r_subtracted\n            \n            # Decode angle as the angle of the population vector\n            theta_decoded_fold = np.arctan2(P[1, :], P[0, :])\n            \n            # Convention for zero-magnitude vectors\n            magnitudes = np.linalg.norm(P, axis=0)\n            theta_decoded_fold[magnitudes == 0] = 0.0\n\n            # Store true and decoded angles for this fold\n            all_theta_true[test_indices] = theta_test\n            all_theta_decoded[test_indices] = theta_decoded_fold\n\n        # Step 3: Compute performance metrics over all T trials\n        # Calculate circular error: wrap(decoded - true)\n        error = all_theta_decoded - all_theta_true\n        wrapped_error = np.arctan2(np.sin(error), np.cos(error))\n\n        # MACE: Mean Absolute Circular Error\n        mace = np.mean(np.abs(wrapped_error))\n        \n        # BIAS: Circular Bias\n        bias = np.mean(wrapped_error)\n        \n        # FRAC: Fraction of trials with absolute error <= tau\n        frac = np.mean(np.abs(wrapped_error) <= tau)\n        \n        # Format results for this test case\n        case_results_str = f\"[{mace:.6f},{bias:.6f},{frac:.6f}]\"\n        all_results_formatted.append(case_results_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results_formatted)}]\")\n\nsolve()\n```"
        }
    ]
}