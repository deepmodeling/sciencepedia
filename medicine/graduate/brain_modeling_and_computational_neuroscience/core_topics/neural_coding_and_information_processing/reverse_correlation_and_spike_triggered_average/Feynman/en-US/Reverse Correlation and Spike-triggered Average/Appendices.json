{
    "hands_on_practices": [
        {
            "introduction": "The Spike-Triggered Average (STA) is a cornerstone of receptive field analysis. This first exercise explores the ideal case of a Linear-Nonlinear (LN) model neuron driven by Gaussian white noise, demonstrating why the STA is such a powerful tool in this scenario. By working through this problem, you will prove a fundamental result: for a spherically symmetric stimulus, the STA faithfully recovers the direction of the neuron's linear filter, regardless of the shape of the subsequent monotonic nonlinearity ().",
            "id": "4016502",
            "problem": "Consider a neuron modeled by a Linear-Nonlinear (LN) Poisson spiking model. The stimulus at a single time bin is a $d$-dimensional random vector $\\mathbf{s} \\in \\mathbb{R}^{d}$ drawn independently across time from a zero-mean multivariate normal distribution with covariance $c^{2} \\mathbf{I}_{d}$, where $c > 0$ is the stimulus contrast and $\\mathbf{I}_{d}$ is the $d \\times d$ identity matrix. The neuron's linear filter is a nonzero vector $\\mathbf{k} \\in \\mathbb{R}^{d}$, and the instantaneous spiking intensity conditioned on the stimulus is given by a saturating sigmoid nonlinearity\n$$\n\\lambda(\\mathbf{s}) \\;=\\; \\frac{\\lambda_{\\max}}{1 + \\exp\\!\\left(-\\frac{\\mathbf{k}^{\\top}\\mathbf{s} - \\theta}{\\beta}\\right)},\n$$\nwhere $\\lambda_{\\max} > 0$ is the maximum firing rate, $\\theta \\in \\mathbb{R}$ is an offset, and $\\beta > 0$ sets the slope of the sigmoid.\n\nDefine the spike-triggered average (STA) vector over a long experiment as\n$$\n\\mathrm{STA} \\;=\\; \\mathbb{E}\\!\\left[\\,\\mathbf{s} \\,\\middle|\\, \\text{spike}\\,\\right] \\;=\\; \\frac{\\mathbb{E}\\!\\left[\\,\\mathbf{s}\\,\\lambda(\\mathbf{s})\\,\\right]}{\\mathbb{E}\\!\\left[\\,\\lambda(\\mathbf{s})\\,\\right]}.\n$$\nLet the bias in STA direction relative to the true filter direction be quantified by the angle\n$$\n\\Delta \\;=\\; \\arccos\\!\\left(\\frac{\\mathrm{STA}^{\\top}\\mathbf{k}}{\\|\\mathrm{STA}\\|\\,\\|\\mathbf{k}\\|}\\right),\n$$\nmeasured in radians.\n\nStarting from the LN Poisson model and the properties of zero-mean multivariate Gaussian stimuli, derive the STA and compute the bias angle $\\Delta$ as a function of the parameters $\\lambda_{\\max}$, $\\theta$, $\\beta$, $c$, $\\mathbf{k}$, and $d$. Express the final answer for $\\Delta$ in radians. No rounding is required.",
            "solution": "The problem requires the derivation of the spike-triggered average (STA) for a Linear-Nonlinear (LN) Poisson model neuron stimulated by zero-mean Gaussian white noise, and subsequently, the calculation of the bias angle $\\Delta$ between the STA and the neuron's true linear filter $\\mathbf{k}$.\n\nThe stimulus $\\mathbf{s}$ is a $d$-dimensional random vector drawn from a multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, c^2 \\mathbf{I}_d)$, where $\\mathbf{I}_d$ is the $d \\times d$ identity matrix. The linear filter is a vector $\\mathbf{k} \\in \\mathbb{R}^d$, with $\\mathbf{k} \\neq \\mathbf{0}$. The neuron's instantaneous firing rate is given by the nonlinear function $\\lambda(\\mathbf{s})$ applied to the filtered stimulus $\\mathbf{k}^\\top\\mathbf{s}$. Let us define a scalar function $f(x)$ representing the nonlinearity, where $x = \\mathbf{k}^\\top\\mathbf{s}$ is the projected stimulus:\n$$\nf(x) \\;=\\; \\frac{\\lambda_{\\max}}{1 + \\exp\\!\\left(-\\frac{x - \\theta}{\\beta}\\right)}\n$$\nThus, the firing rate can be written as $\\lambda(\\mathbf{s}) = f(\\mathbf{k}^\\top\\mathbf{s})$.\n\nThe spike-triggered average (STA) is defined as:\n$$\n\\mathrm{STA} \\;=\\; \\frac{\\mathbb{E}\\!\\left[\\,\\mathbf{s}\\,\\lambda(\\mathbf{s})\\,\\right]}{\\mathbb{E}\\!\\left[\\,\\lambda(\\mathbf{s})\\,\\right]} \\;=\\; \\frac{\\mathbb{E}\\!\\left[\\,\\mathbf{s}\\,f(\\mathbf{k}^\\top\\mathbf{s})\\,\\right]}{\\mathbb{E}\\!\\left[\\,f(\\mathbf{k}^\\top\\mathbf{s})\\,\\right]}\n$$\nWe begin by analyzing the numerator, which is the vector $\\mathbb{E}[\\mathbf{s}\\,f(\\mathbf{k}^\\top\\mathbf{s})]$. A key property of multivariate Gaussian distributions is given by Stein's Lemma (also known as Price's Theorem). For a random vector $\\mathbf{s} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})$ and a differentiable scalar function $g(\\mathbf{s})$, the following identity holds:\n$$\n\\mathbb{E}\\!\\left[\\,(\\mathbf{s}-\\boldsymbol{\\mu})\\,g(\\mathbf{s})\\,\\right] \\;=\\; \\mathbf{\\Sigma} \\, \\mathbb{E}\\!\\left[\\,\\nabla_{\\mathbf{s}} g(\\mathbf{s})\\,\\right]\n$$\nIn this problem, the stimulus distribution has zero mean ($\\boldsymbol{\\mu} = \\mathbf{0}$) and a spherical covariance matrix ($\\mathbf{\\Sigma} = c^2 \\mathbf{I}_d$). So, the identity simplifies to:\n$$\n\\mathbb{E}\\!\\left[\\,\\mathbf{s}\\,g(\\mathbf{s})\\,\\right] \\;=\\; c^2 \\mathbf{I}_d \\, \\mathbb{E}\\!\\left[\\,\\nabla_{\\mathbf{s}} g(\\mathbf{s})\\,\\right] \\;=\\; c^2 \\mathbb{E}\\!\\left[\\,\\nabla_{\\mathbf{s}} g(\\mathbf{s})\\,\\right]\n$$\nOur function is $g(\\mathbf{s}) = f(\\mathbf{k}^\\top\\mathbf{s})$. We compute its gradient with respect to $\\mathbf{s}$ using the chain rule. Let $x = \\mathbf{k}^\\top\\mathbf{s}$.\n$$\n\\nabla_{\\mathbf{s}} g(\\mathbf{s}) \\;=\\; \\nabla_{\\mathbf{s}} f(\\mathbf{k}^\\top\\mathbf{s}) \\;=\\; \\frac{df}{dx} \\frac{\\partial x}{\\partial \\mathbf{s}} \\;=\\; f'(\\mathbf{k}^\\top\\mathbf{s}) \\nabla_{\\mathbf{s}} (\\mathbf{k}^\\top \\mathbf{s})\n$$\nThe gradient of the linear projection $\\mathbf{k}^\\top\\mathbf{s}$ with respect to $\\mathbf{s}$ is simply the vector $\\mathbf{k}$. Therefore,\n$$\n\\nabla_{\\mathbf{s}} g(\\mathbf{s}) \\;=\\; f'(\\mathbf{k}^\\top\\mathbf{s}) \\, \\mathbf{k}\n$$\nNow, substitute this gradient back into the expectation from Stein's Lemma:\n$$\n\\mathbb{E}\\!\\left[\\,\\mathbf{s}\\,f(\\mathbf{k}^\\top\\mathbf{s})\\,\\right] \\;=\\; c^2 \\mathbb{E}\\!\\left[\\,f'(\\mathbf{k}^\\top\\mathbf{s}) \\, \\mathbf{k}\\,\\right]\n$$\nSince $\\mathbf{k}$ is a constant vector, it can be factored out of the expectation:\n$$\n\\mathbb{E}\\!\\left[\\,\\mathbf{s}\\,f(\\mathbf{k}^\\top\\mathbf{s})\\,\\right] \\;=\\; c^2 \\mathbf{k} \\, \\mathbb{E}\\!\\left[\\,f'(\\mathbf{k}^\\top\\mathbf{s})\\,\\right]\n$$\nThe term $\\mathbb{E}[f'(\\mathbf{k}^\\top\\mathbf{s})]$ is a scalar expectation. Let's denote it by $E_{f'} = \\mathbb{E}[f'(\\mathbf{k}^\\top\\mathbf{s})]$. The numerator of the STA is then $c^2 E_{f'} \\mathbf{k}$.\n\nThe denominator of the STA is the average firing rate, which is the scalar $E_f = \\mathbb{E}[f(\\mathbf{k}^\\top\\mathbf{s})]$.\nCombining the numerator and denominator, we find the expression for the STA:\n$$\n\\mathrm{STA} \\;=\\; \\frac{c^2 \\, \\mathbb{E}\\!\\left[\\,f'(\\mathbf{k}^\\top\\mathbf{s})\\,\\right]}{\\mathbb{E}\\!\\left[\\,f(\\mathbf{k}^\\top\\mathbf{s})\\,\\right]} \\, \\mathbf{k}\n$$\nThis result demonstrates that the STA is a scalar multiple of the true filter $\\mathbf{k}$. Let this scalar be $C$:\n$$\nC = \\frac{c^2 \\, \\mathbb{E}\\!\\left[\\,f'(\\mathbf{k}^\\top\\mathbf{s})\\,\\right]}{\\mathbb{E}\\!\\left[\\,f(\\mathbf{k}^\\top\\mathbf{s})\\,\\right]} \\quad \\implies \\quad \\mathrm{STA} = C \\mathbf{k}\n$$\nTo calculate the bias angle $\\Delta$, we must determine the sign of the scalar $C$.\nThe term $c^2$ is positive since $c > 0$.\nThe denominator is the average firing rate $\\mathbb{E}[f(\\mathbf{k}^\\top\\mathbf{s})]$. Since $\\lambda_{\\max} > 0$, the firing rate function $f(x) = \\frac{\\lambda_{\\max}}{1 + \\exp(- (x-\\theta)/\\beta)}$ is strictly positive for all inputs $x$. The expectation of a strictly positive random variable is also strictly positive. Thus, $\\mathbb{E}[f(\\mathbf{k}^\\top\\mathbf{s})] > 0$.\nThe numerator contains the expectation of the derivative of the nonlinearity, $f'(x)$. Let's compute this derivative:\n$$\nf'(x) \\;=\\; \\frac{d}{dx} \\left[ \\frac{\\lambda_{\\max}}{1 + \\exp\\!\\left(-\\frac{x - \\theta}{\\beta}\\right)} \\right] \\;=\\; \\lambda_{\\max} \\left[ -1 \\left(1 + \\exp\\!\\left(-\\frac{x - \\theta}{\\beta}\\right)\\right)^{-2} \\cdot \\exp\\!\\left(-\\frac{x - \\theta}{\\beta}\\right) \\cdot \\left(-\\frac{1}{\\beta}\\right) \\right]\n$$\n$$\nf'(x) \\;=\\; \\frac{\\lambda_{\\max}}{\\beta} \\frac{\\exp\\!\\left(-\\frac{x - \\theta}{\\beta}\\right)}{\\left(1 + \\exp\\!\\left(-\\frac{x - \\theta}{\\beta}\\right)\\right)^2}\n$$\nGiven that $\\lambda_{\\max} > 0$ and $\\beta > 0$, and noting that the exponential term is always positive and the denominator is a squared term (and thus positive), we can conclude that $f'(x) > 0$ for all $x \\in \\mathbb{R}$. Consequently, the expectation $\\mathbb{E}[f'(\\mathbf{k}^\\top\\mathbf{s})]$ must be positive.\nSince all three components of the scalar $C$ ($c^2$, $\\mathbb{E}[f'(\\mathbf{k}^\\top\\mathbf{s})]$, and the inverse of $\\mathbb{E}[f(\\mathbf{k}^\\top\\mathbf{s})]$) are positive, the scalar $C$ must be positive, i.e., $C > 0$.\n\nNow we compute the bias angle $\\Delta$:\n$$\n\\Delta \\;=\\; \\arccos\\!\\left(\\frac{\\mathrm{STA}^{\\top}\\mathbf{k}}{\\|\\mathrm{STA}\\|\\,\\|\\mathbf{k}\\|}\\right)\n$$\nSubstitute $\\mathrm{STA} = C \\mathbf{k}$:\n$$\n\\Delta \\;=\\; \\arccos\\!\\left(\\frac{(C\\mathbf{k})^{\\top}\\mathbf{k}}{\\|C\\mathbf{k}\\|\\,\\|\\mathbf{k}\\|}\\right)\n$$\nThe numerator of the argument is $(C\\mathbf{k})^{\\top}\\mathbf{k} = C(\\mathbf{k}^\\top\\mathbf{k}) = C\\|\\mathbf{k}\\|^2$.\nThe denominator of the argument is $\\|C\\mathbf{k}\\|\\|\\mathbf{k}\\| = |C|\\|\\mathbf{k}\\|\\|\\mathbf{k}\\| = |C|\\|\\mathbf{k}\\|^2$.\nThe expression inside the arccos becomes:\n$$\n\\frac{C\\|\\mathbf{k}\\|^2}{|C|\\|\\mathbf{k}\\|^2} \\;=\\; \\frac{C}{|C|}\n$$\nSince we established that $C > 0$, we have $|C| = C$. Therefore, the argument is:\n$$\n\\frac{C}{C} \\;=\\; 1\n$$\nFinally, we compute the angle $\\Delta$:\n$$\n\\Delta \\;=\\; \\arccos(1) \\;=\\; 0\n$$\nThe bias angle is $0$ radians. This demonstrates that for an LN model driven by spherically symmetric Gaussian noise, the STA is an unbiased estimator of the filter's direction, regardless of the specific form of the (monotonic) nonlinearity or the other model parameters.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Real-world sensory stimuli are rarely 'white'; they often possess their own rich statistical structure and correlations. This practice moves beyond the idealized case to explore how these stimulus correlations impact the computed STA. You will derive the analytical relationship showing how the stimulus covariance matrix, $\\Sigma$, systematically alters the STA, providing crucial insight into why the raw STA is not always a direct representation of the neuron's filter ().",
            "id": "4016503",
            "problem": "A neuron is modeled using a Linear-Nonlinear-Poisson (LNP) cascade receiving a two-channel stimulus. The stimulus at time $t$ is a vector $x(t) \\in \\mathbb{R}^{2}$, modeled as a stationary, zero-mean Gaussian process with covariance matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n1.5 & -0.8 \\\\\n-0.8 & 2.5\n\\end{pmatrix}.\n$$\nThe neuron’s instantaneous firing rate is\n$$\n\\lambda(t) \\;=\\; \\lambda_{0}\\,\\exp\\!\\big(\\beta\\,k^{\\top} x(t)\\big),\n$$\nwhere $\\lambda_{0} > 0$ is a constant baseline rate, $\\beta > 0$ is a scalar gain, and $k \\in \\mathbb{R}^{2}$ is the true linear filter. In this problem, the true filter is\n$$\nk_{\\text{true}} \\;=\\; \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix},\n$$\nand the gain is $\\,\\beta = 0.3\\,$.\n\nThe spike-triggered average (STA) is defined as the conditional mean of the stimulus given that a spike occurred, i.e.,\n$$\n\\text{STA} \\;=\\; \\mathbb{E}\\big[x(t) \\,\\big|\\, \\text{a spike occurs at time }t\\big].\n$$\n\nStarting only from the above definitions, the properties of Gaussian random vectors, and standard integral identities, derive the closed-form expression of the spike-triggered average $\\text{STA}$ in terms of $\\Sigma$, $\\beta$, and $k_{\\text{true}}$. Then, using the numerical values provided, compute the scalar projection of the STA onto the unit-norm direction of the true filter,\n$$\ns \\;=\\; \\left(\\frac{k_{\\text{true}}}{\\|k_{\\text{true}}\\|}\\right)^{\\top} \\text{STA},\n$$\nand report the value of $s$.\n\nRound your final numerical answer to four significant figures. Express the final answer without units.",
            "solution": "The problem requires the derivation of the spike-triggered average (STA) for a Linear-Nonlinear-Poisson (LNP) model neuron stimulated by a zero-mean Gaussian process, and the subsequent calculation of a specific scalar quantity.\n\nFirst, we establish the formal expression for the STA. The STA is defined as the conditional expectation of the stimulus $x(t)$ at the time of a spike. For a point process with an instantaneous rate $\\lambda(t)$ that depends on the stimulus $x(t)$, the STA can be calculated using the law of total expectation, which is sometimes referred to as Campbell's theorem or a form of Bayes' rule for rates. The probability of a spike occurring in a small time interval $dt$ is $\\lambda(t)dt$. The average stimulus, weighted by the probability of a spike, divided by the average probability of a spike, gives the STA.\n$$\n\\text{STA} \\;=\\; \\frac{\\mathbb{E}[x(t) \\lambda(t)]}{\\mathbb{E}[\\lambda(t)]}\n$$\nHere, the expectation $\\mathbb{E}[\\cdot]$ is taken over the distribution of the stimulus $x(t)$. The stimulus $x(t)$ is a $2$-dimensional Gaussian random vector, denoted as $x$, with mean $\\mathbb{E}[x] = 0$ and covariance matrix $\\Sigma$. Its probability density function is $p(x) = \\frac{1}{2\\pi |\\Sigma|^{1/2}} \\exp(-\\frac{1}{2} x^{\\top} \\Sigma^{-1} x)$.\n\nWe will now compute the denominator and numerator separately.\n\nThe denominator is the average firing rate, $\\mathbb{E}[\\lambda(t)]$.\n$$\n\\mathbb{E}[\\lambda(t)] \\;=\\; \\mathbb{E}\\left[\\lambda_{0}\\exp(\\beta k^{\\top} x)\\right] \\;=\\; \\lambda_{0} \\mathbb{E}\\left[\\exp(\\beta k^{\\top} x)\\right]\n$$\nLet the scalar random variable $y = \\beta k^{\\top} x$. Since $x$ is a zero-mean Gaussian vector, $y$ is a zero-mean Gaussian scalar. Its variance is:\n$$\n\\sigma_y^2 \\;=\\; \\text{Var}(y) \\;=\\; \\mathbb{E}[y^2] - (\\mathbb{E}[y])^2 \\;=\\; \\mathbb{E}[(\\beta k^{\\top} x)(\\beta k^{\\top} x)] \\;=\\; \\beta^2 \\mathbb{E}[k^{\\top} x x^{\\top} k] \\;=\\; \\beta^2 k^{\\top} \\mathbb{E}[x x^{\\top}] k \\;=\\; \\beta^2 k^{\\top} \\Sigma k\n$$\nThe expectation $\\mathbb{E}[\\exp(y)]$ is the moment-generating function of the Gaussian variable $y \\sim \\mathcal{N}(0, \\sigma_y^2)$ evaluated at $1$. For a general Gaussian $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the MGF is $M_z(s) = \\exp(\\mu s + \\frac{1}{2}\\sigma^2 s^2)$. For $y$, we have $\\mu=0$, so:\n$$\n\\mathbb{E}[\\exp(y)] \\;=\\; M_y(1) \\;=\\; \\exp\\left(\\frac{1}{2}\\sigma_y^2\\right) \\;=\\; \\exp\\left(\\frac{1}{2}\\beta^2 k^{\\top} \\Sigma k\\right)\n$$\nThus, the denominator is:\n$$\n\\mathbb{E}[\\lambda(t)] \\;=\\; \\lambda_{0} \\exp\\left(\\frac{1}{2}\\beta^2 k^{\\top} \\Sigma k\\right)\n$$\n\nNext, we compute the numerator, $\\mathbb{E}[x(t) \\lambda(t)]$.\n$$\n\\mathbb{E}[x \\lambda(x)] \\;=\\; \\mathbb{E}\\left[x \\lambda_{0}\\exp(\\beta k^{\\top} x)\\right] \\;=\\; \\lambda_{0} \\int_{\\mathbb{R}^2} x \\exp(\\beta k^{\\top} x) p(x) dx\n$$\nSubstituting the expression for $p(x)$:\n$$\n\\mathbb{E}[x \\lambda(x)] \\;=\\; \\frac{\\lambda_{0}}{(2\\pi) |\\Sigma|^{1/2}} \\int_{\\mathbb{R}^2} x \\exp\\left( \\beta k^{\\top} x - \\frac{1}{2} x^{\\top} \\Sigma^{-1} x \\right) dx\n$$\nThe term in the exponent is a quadratic form in $x$. We can complete the square.\n$$\n-\\frac{1}{2} x^{\\top} \\Sigma^{-1} x + \\beta k^{\\top} x = -\\frac{1}{2} \\left( x^{\\top} \\Sigma^{-1} x - 2\\beta k^{\\top} x \\right)\n$$\nWe seek to write this in the form $-\\frac{1}{2}(x - \\mu_*)^\\top \\Sigma^{-1} (x - \\mu_*)$ plus a constant. Expanding this form gives:\n$$\n-\\frac{1}{2}(x^\\top \\Sigma^{-1} x - 2x^\\top\\Sigma^{-1}\\mu_* + \\mu_*^\\top \\Sigma^{-1} \\mu_*)\n$$\nComparing the linear terms in $x$, we must have $x^\\top\\Sigma^{-1}\\mu_* = \\beta k^\\top x$, which implies $\\Sigma^{-1}\\mu_* = \\beta k$. Solving for $\\mu_*$ yields $\\mu_* = \\beta \\Sigma k$.\nThe completed square is then:\n$$\n-\\frac{1}{2} x^{\\top} \\Sigma^{-1} x + \\beta k^{\\top} x = -\\frac{1}{2}(x - \\beta\\Sigma k)^{\\top} \\Sigma^{-1} (x - \\beta\\Sigma k) + \\frac{1}{2}(\\beta\\Sigma k)^{\\top} \\Sigma^{-1} (\\beta\\Sigma k)\n$$\nThe term independent of $x$ simplifies to $\\frac{1}{2} \\beta^2 k^{\\top}\\Sigma^{\\top}\\Sigma^{-1}\\Sigma k = \\frac{1}{2} \\beta^2 k^{\\top}\\Sigma k$.\nSubstituting this back into the integral for the numerator:\n$$\n\\mathbb{E}[x \\lambda(x)] \\;=\\; \\lambda_{0} \\exp\\left(\\frac{1}{2}\\beta^2 k^{\\top} \\Sigma k\\right) \\int_{\\mathbb{R}^2} x \\frac{1}{2\\pi |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\beta\\Sigma k)^{\\top} \\Sigma^{-1} (x - \\beta\\Sigma k) \\right) dx\n$$\nThe integral is the expected value of $x$ under a modified Gaussian distribution with the same covariance $\\Sigma$ but with a non-zero mean $\\mu_* = \\beta \\Sigma k$. The value of this integral is therefore the mean of this distribution, $\\beta \\Sigma k$.\nSo, the numerator is:\n$$\n\\mathbb{E}[x \\lambda(x)] \\;=\\; \\lambda_{0} \\exp\\left(\\frac{1}{2}\\beta^2 k^{\\top} \\Sigma k\\right) (\\beta \\Sigma k)\n$$\n\nNow, we can find the STA by taking the ratio of the numerator and the denominator:\n$$\n\\text{STA} \\;=\\; \\frac{\\lambda_{0} \\exp\\left(\\frac{1}{2}\\beta^2 k^{\\top} \\Sigma k\\right) (\\beta \\Sigma k)}{\\lambda_{0} \\exp\\left(\\frac{1}{2}\\beta^2 k^{\\top} \\Sigma k\\right)} \\;=\\; \\beta \\Sigma k\n$$\nThis is the closed-form expression for the STA in terms of $\\beta$, $\\Sigma$, and the true filter $k$. In our problem, $k = k_{\\text{true}}$.\n\nWe now substitute the numerical values provided:\n$\\beta = 0.3$, $k_{\\text{true}} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$, and $\\Sigma = \\begin{pmatrix} 1.5 & -0.8 \\\\ -0.8 & 2.5 \\end{pmatrix}$.\nFirst, we compute the product $\\Sigma k_{\\text{true}}$:\n$$\n\\Sigma k_{\\text{true}} \\;=\\; \\begin{pmatrix} 1.5 & -0.8 \\\\ -0.8 & 2.5 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} (1.5)(2) + (-0.8)(1) \\\\ (-0.8)(2) + (2.5)(1) \\end{pmatrix} \\;=\\; \\begin{pmatrix} 3.0 - 0.8 \\\\ -1.6 + 2.5 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 2.2 \\\\ 0.9 \\end{pmatrix}\n$$\nNext, we multiply by $\\beta$:\n$$\n\\text{STA} \\;=\\; 0.3 \\begin{pmatrix} 2.2 \\\\ 0.9 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 0.66 \\\\ 0.27 \\end{pmatrix}\n$$\n\nThe final step is to compute the scalar projection $s$ of the STA onto the unit-norm direction of the true filter.\n$$\ns \\;=\\; \\left(\\frac{k_{\\text{true}}}{\\|k_{\\text{true}}\\|}\\right)^{\\top} \\text{STA} \\;=\\; \\frac{k_{\\text{true}}^{\\top} \\text{STA}}{\\|k_{\\text{true}}\\|}\n$$\nFirst, calculate the norm of $k_{\\text{true}}$:\n$$\n\\|k_{\\text{true}}\\| \\;=\\; \\sqrt{2^2 + 1^2} \\;=\\; \\sqrt{4 + 1} \\;=\\; \\sqrt{5}\n$$\nNext, calculate the dot product $k_{\\text{true}}^{\\top} \\text{STA}$:\n$$\nk_{\\text{true}}^{\\top} \\text{STA} \\;=\\; \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} 0.66 \\\\ 0.27 \\end{pmatrix} \\;=\\; (2)(0.66) + (1)(0.27) \\;=\\; 1.32 + 0.27 \\;=\\; 1.59\n$$\nFinally, compute the scalar projection $s$:\n$$\ns \\;=\\; \\frac{1.59}{\\sqrt{5}} \\;\\approx\\; \\frac{1.59}{2.2360679...} \\;\\approx\\; 0.7110702...\n$$\nRounding this value to four significant figures gives $0.7111$.",
            "answer": "$$\\boxed{0.7111}$$"
        },
        {
            "introduction": "A neuron's response may be driven by multiple stimulus features, some of which are invisible to the STA, which only captures the average. This exercise introduces Spike-Triggered Covariance (STC), a second-order method designed to find these additional relevant dimensions in the stimulus space. You will use the powerful framework of random matrix theory to distinguish true neural features from statistical noise, a critical skill for analyzing modern, high-dimensional neural data ().",
            "id": "4016504",
            "problem": "You are given a set of spike-triggered covariance (STC) eigenvalue lists alongside the stimulus dimensionality and the number of spike-triggered samples. The stimuli are modeled as independent Gaussian vectors of dimension $d$ with variance $\\sigma^2$ per coordinate and zero mean. The STC is computed from $n$ spike-triggered stimulus samples by subtracting the sample mean and forming the sample covariance matrix. Under the null hypothesis of no stimulus feature selectivity, the neuron’s spike-triggered ensemble has population covariance equal to $\\sigma^2 \\mathbf{I}_d$, and finite-sample fluctuations in the STC eigenvalues are governed by random matrix theory. Spike-Triggered Covariance (STC) is the covariance of stimuli conditioned on spikes, and Reverse Correlation refers to characterizing neural response properties by correlating stimuli with spikes.\n\nYour task is to compute, for each provided test case, the number of significant dimensions, defined as the count of STC eigenvalues that deviate beyond the random matrix null support expected under finite sampling. Use the following scientifically grounded base:\n\n- The sample covariance matrix of $n$ independent, zero-mean Gaussian observations in $d$ dimensions can be modeled as a properly scaled Wishart matrix, whose eigenvalue distribution under the null converges (in the large $d,n$ limit with fixed ratio $q = d/n$) to the Marchenko–Pastur distribution.\n- Under the null, deviations of the STC eigenvalues beyond the theoretical support interval implied by the random matrix model indicate dimensions with significantly altered variance (candidate stimulus features or suppressive directions).\n\nImplement the following rules in your computation:\n\n- Let $q = d/n$. The theoretical non-zero support interval under the null is determined by $q$ and $\\sigma^2$. An STC eigenvalue is significant if it lies strictly above the upper support edge or strictly below the lower support edge; equality at the edges is not significant.\n- When $q > 1$, the null model implies a point mass at zero: a fraction of eigenvalues are exactly zero due to rank deficiency. Such zero eigenvalues are consistent with the null and must not be counted as significant outliers below the lower support edge. Use a small non-negative tolerance to identify numerical zeros when $q > 1$.\n\nInput specification for each test case:\n- $d$: stimulus dimensionality as an integer (e.g., $d = 30$).\n- $n$: number of spike-triggered samples as an integer (e.g., $n = 300$).\n- $\\sigma^2$: stimulus variance per coordinate as a positive real number (e.g., $\\sigma^2 = 1.0$).\n- A list of $d$ real numbers representing the STC eigenvalues (e.g., $[1.0, 1.0, \\dots, 1.9]$ with exactly $d$ entries).\n\nTest suite:\n- Case $1$: $d = 30$, $n = 300$, $\\sigma^2 = 1.0$, eigenvalues $[1.9, 0.4, 1.8, \\underbrace{1.0, \\dots, 1.0}_{27 \\text{ entries}}]$.\n- Case $2$: $d = 100$, $n = 40$, $\\sigma^2 = 1.0$, eigenvalues $[\\underbrace{0.0, \\dots, 0.0}_{60 \\text{ entries}}, \\underbrace{1.0, \\dots, 1.0}_{39 \\text{ entries}}, 7.0]$.\n- Case $3$: $d = 50$, $n = 50$, $\\sigma^2 = 1.0$, eigenvalues $[\\underbrace{1.0, \\dots, 1.0}_{48 \\text{ entries}}, 4.0, 4.001]$.\n- Case $4$: $d = 10$, $n = 1000$, $\\sigma^2 = 1.0$, eigenvalues $[\\underbrace{1.0, \\dots, 1.0}_{8 \\text{ entries}}, 0.80, 1.22]$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the test suite above, the program must return a list of integers, each equal to the number of significant dimensions for the corresponding case, in the order of the cases. For example, the output format must be exactly like $[r_1,r_2,r_3,r_4]$ where each $r_i$ is an integer.\n\nThere are no physical units, angles, or percentages involved in this problem. The output consists of integers only. Your program must be self-contained and must not read any input; it should be runnable as provided and produce the specified output format.",
            "solution": "The problem requires the identification of significant eigenvalues from a Spike-Triggered Covariance (STC) analysis. The determination of significance is based on a null hypothesis model derived from random matrix theory, specifically the Marchenko-Pastur law. The solution implements the logic of the Marchenko-Pastur law to identify STC eigenvalues that are statistically significant.\n\n**1. Theoretical Framework**\nUnder the null hypothesis that a neuron's spiking is independent of the stimulus, the set of spike-triggered stimuli is a random sampling from the prior stimulus distribution. The stimuli are modeled as $d$-dimensional random vectors drawn from an isotropic Gaussian distribution with covariance $\\sigma^2 \\mathbf{I}_d$. The sample covariance matrix formed from $n$ such vectors has eigenvalues that, in the limit of large $n$ and $d$ with a fixed ratio $q = d/n$, are confined to a specific interval.\n\nThe continuous part of the eigenvalue spectrum of the sample covariance matrix is supported on the interval $[\\lambda_-, \\lambda_+]$, where the bounds are given by:\n$$ \\lambda_\\pm = \\sigma^2 (1 \\pm \\sqrt{q})^2 $$\nAny eigenvalue of the experimentally measured STC matrix that falls strictly outside this interval is considered statistically significant, suggesting it reflects a true feature of the neuron's stimulus selectivity rather than finite-sampling noise.\n\n**2. Handling of Rank Deficiency ($q > 1$)**\nWhen the dimensionality of the stimulus space $d$ is greater than the number of samples $n$ (i.e., $q > 1$), the sample covariance matrix is rank-deficient. Its rank is at most $n$. This results in at least $d-n$ zero eigenvalues. These structural zeros are a direct consequence of the high-dimensional geometry and are predicted by the Marchenko-Pastur law, which includes a delta function at eigenvalue $0$ with weight $1 - 1/q$. Therefore, eigenvalues that are zero (or numerically close to zero) are part of the null distribution for $q > 1$ and must not be counted as significant outliers.\n\n**3. Algorithm for Significance Testing**\nFor each test case, the number of significant dimensions is determined as follows:\n1.  Calculate the ratio $q = d/n$.\n2.  Compute the upper and lower bounds of the null support interval: $\\lambda_+ = \\sigma^2 (1 + \\sqrt{q})^2$ and $\\lambda_- = \\sigma^2 (1 - \\sqrt{q})^2$. Note that for $q>1$, this becomes $\\lambda_- = \\sigma^2 (\\sqrt{q} - 1)^2$.\n3.  Initialize a counter for significant eigenvalues to $0$.\n4.  Iterate through each provided eigenvalue $\\lambda_{obs}$.\n5.  Apply the significance test:\n    - If $\\lambda_{obs} > \\lambda_+$ or $\\lambda_{obs} < \\lambda_-$, the eigenvalue is a candidate for significance.\n    - If $q > 1$ and the eigenvalue is a candidate due to being below $\\lambda_-$, a further check is required. If $\\lambda_{obs}$ is numerically indistinguishable from zero (e.g., $|\\lambda_{obs}| < \\epsilon$ for a small tolerance $\\epsilon$), it is classified as a structural zero and is not counted as significant.\n    - If the eigenvalue passes these checks, increment the counter.\n6.  The final value of the counter represents the number of significant dimensions.\n\n**Case 1:** $d = 30$, $n = 300$, $\\sigma^2 = 1.0$.\n- $q = 30/300 = 0.1$.\n- $\\lambda_+ = 1.0 \\times (1 + \\sqrt{0.1})^2 \\approx 1.7324$.\n- $\\lambda_- = 1.0 \\times (1 - \\sqrt{0.1})^2 \\approx 0.4675$.\n- The null support is approximately $[0.4675, 1.7324]$.\n- Eigenvalues to check: $1.9, 0.4, 1.8$.\n  - $\\lambda = 1.9 > \\lambda_+$ (Significant).\n  - $\\lambda = 0.4 < \\lambda_-$ (Significant).\n  - $\\lambda = 1.8 > \\lambda_+$ (Significant).\n- Total significant dimensions: $3$.\n\n**Case 2:** $d = 100$, $n = 40$, $\\sigma^2 = 1.0$.\n- $q = 100/40 = 2.5 > 1$.\n- $\\lambda_+ = 1.0 \\times (1 + \\sqrt{2.5})^2 \\approx 6.6623$.\n- $\\lambda_- = 1.0 \\times (1 - \\sqrt{2.5})^2 = 1.0 \\times (\\sqrt{2.5} - 1)^2 \\approx 0.3377$.\n- The null support for non-zero eigenvalues is approximately $[0.3377, 6.6623]$.\n- There are $d-n = 60$ eigenvalues of $0.0$. These are structural zeros and not significant.\n- Eigenvalues to check: $1.0$ (repeated), $7.0$.\n  - $\\lambda = 1.0$: falls within the support interval (Not significant).\n  - $\\lambda = 7.0 > \\lambda_+$ (Significant).\n- Total significant dimensions: $1$.\n\n**Case 3:** $d = 50$, $n = 50$, $\\sigma^2 = 1.0$.\n- $q = 50/50 = 1.0$.\n- $\\lambda_+ = 1.0 \\times (1 + \\sqrt{1.0})^2 = 4.0$.\n- $\\lambda_- = 1.0 \\times (1 - \\sqrt{1.0})^2 = 0.0$.\n- The null support is $[0.0, 4.0]$. The significance criterion requires strict inequality.\n- Eigenvalues to check: $4.0$, $4.001$.\n  - $\\lambda = 4.0 = \\lambda_+$ (Not significant).\n  - $\\lambda = 4.001 > \\lambda_+$ (Significant).\n- Total significant dimensions: $1$.\n\n**Case 4:** $d = 10$, $n = 1000$, $\\sigma^2 = 1.0$.\n- $q = 10/1000 = 0.01$.\n- $\\lambda_+ = 1.0 \\times (1 + \\sqrt{0.01})^2 = (1.1)^2 = 1.21$.\n- $\\lambda_- = 1.0 \\times (1 - \\sqrt{0.01})^2 = (0.9)^2 = 0.81$.\n- The null support is $[0.81, 1.21]$.\n- Eigenvalues to check: $0.80$, $1.22$.\n  - $\\lambda = 0.80 < \\lambda_-$ (Significant).\n  - $\\lambda = 1.22 > \\lambda_+$ (Significant).\n- Total significant dimensions: $2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the number of significant STC eigenvalues for several test cases\n    based on the Marchenko-Pastur distribution.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (d, n, sigma_squared, eigenvalues)\n    test_cases = [\n        (30, 300, 1.0, [1.9, 0.4, 1.8] + [1.0] * 27),\n        (100, 40, 1.0, [0.0] * 60 + [1.0] * 39 + [7.0]),\n        (50, 50, 1.0, [1.0] * 48 + [4.0, 4.001]),\n        (10, 1000, 1.0, [1.0] * 8 + [0.80, 1.22])\n    ]\n\n    results = []\n    # Tolerance for identifying numerical zeros when q > 1.\n    zero_tolerance = 1e-9\n\n    for d, n, sigma_sq, eigenvalues in test_cases:\n        # Calculate the ratio q = d/n.\n        q = d / n\n\n        # Calculate the bounds of the Marchenko-Pastur support interval.\n        sqrt_q = np.sqrt(q)\n        lambda_plus = sigma_sq * (1 + sqrt_q)**2\n        lambda_minus = sigma_sq * (1 - sqrt_q)**2\n\n        significant_count = 0\n        for eig in eigenvalues:\n            # An eigenvalue is significant if it is strictly outside the support interval.\n            is_outlier = eig > lambda_plus or eig < lambda_minus\n            \n            if is_outlier:\n                # If q > 1, structural zeros are expected and not considered significant.\n                # These zeros can manifest as very small numbers due to numerics.\n                if q > 1 and abs(eig) < zero_tolerance:\n                    # This is a structural zero, not a significant deviation.\n                    continue\n                significant_count += 1\n\n        results.append(significant_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}