{
    "hands_on_practices": [
        {
            "introduction": "At the heart of learning an inverse model is the ability to incrementally correct motor commands based on performance error. This practice explores the fundamental mechanism of this process by asking you to derive the sensitivity of a task error to small changes in the motor command. By calculating the directional derivative for a nonlinear system, you will engage with the core mathematics of gradient-based learning, a cornerstone of computational motor control. ",
            "id": "3992041",
            "problem": "Consider a brain-inspired inverse model for motor control that selects a command vector $u \\in \\mathbb{R}^{3}$ to achieve a desired task output $y^{\\star} \\in \\mathbb{R}^{2}$ through a nonlinear plant $x = f(u) \\in \\mathbb{R}^{2}$ and a task mapping $y = h(x) \\in \\mathbb{R}^{2}$. The Central Nervous System (CNS) is hypothesized to learn an inverse model that maps $y^{\\star}$ to a command $u$ by reducing a scalar task error defined as $E(u) = \\frac{1}{2} \\|h(f(u)) - y^{\\star}\\|^{2}$. Assume small command variations are evaluated using the first-order linearization of the plant around a nominal command $u_{0}$, namely $x \\approx f(u_{0}) + J_{u}(u_{0})(u - u_{0})$, where $J_{u}(u_{0})$ is the Jacobian of $f$ with respect to $u$ at $u_{0}$. Starting from first principles (the chain rule and the definition of the gradient and directional derivative), derive the first-order sensitivity of the task error $E$ to small command changes, and explain how this local approximation supports gradient-based inverse-model updates.\n\nWork with the following scientifically plausible nonlinear mappings and parameters:\n- $f(u) = \\begin{pmatrix} \\tanh(u_{1}) + u_{2} u_{3} \\\\ u_{1}^{2} - \\cos(u_{2}) \\end{pmatrix}$,\n- $h(x) = \\begin{pmatrix} x_{1}^{2} + \\sin(x_{2}) \\\\ \\exp(x_{1}) + x_{2} \\end{pmatrix}$,\n- $u_{0} = \\begin{pmatrix} 0.3 \\\\ 0.2 \\\\ -0.5 \\end{pmatrix}$,\n- $y^{\\star} = \\begin{pmatrix} 0.2 \\\\ 0.4 \\end{pmatrix}$,\n- Direction of command change $\\Delta u = \\begin{pmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{pmatrix}$.\n\nUsing the linearization and the chain rule, compute the directional derivative of $E$ at $u_{0}$ along $\\Delta u$, that is, $\\left.\\frac{d}{d\\epsilon} E(u_{0} + \\epsilon \\Delta u)\\right|_{\\epsilon = 0}$, and round your numerical answer to four significant figures. Express the answer without any units.",
            "solution": "The core of the problem is to determine the first-order sensitivity of the task error, $E$, to a small change in the command vector, $u$. This sensitivity along a specific direction of change, $\\Delta u$, is given by the directional derivative.\n\nFirst, let us formalize the components. The task error is a scalar function of the command vector $u$:\n$$E(u) = \\frac{1}{2} \\|h(f(u)) - y^{\\star}\\|^{2}$$\nLet the error vector be $e(u) = h(f(u)) - y^{\\star}$. The task error can then be written as $E(u) = \\frac{1}{2} e(u)^T e(u)$.\nWe need to find the directional derivative of $E$ at a nominal command $u_0$ in the direction of a command change $\\Delta u$. This is defined as $\\nabla_{\\Delta u} E(u_0) = \\left.\\frac{d}{d\\epsilon} E(u_0 + \\epsilon \\Delta u)\\right|_{\\epsilon = 0}$. By the properties of directional derivatives, this is equivalent to the dot product of the gradient of $E$ at $u_0$ and the direction vector $\\Delta u$:\n$$\\nabla_{\\Delta u} E(u_0) = \\nabla E(u_0)^T \\Delta u$$\nUsing the chain rule, the gradient of the error function, $\\nabla E(u)$, is given by:\n$$\\nabla E(u) = J_f(u)^T J_h(f(u))^T e(u)$$\nwhere $J_f(u)$ is the Jacobian of the plant $f$ with respect to $u$, and $J_h(x)$ is the Jacobian of the task map $h$ with respect to its input $x$. This gradient vector $\\nabla E(u)$ points in the direction of the steepest ascent of the task error $E$. For gradient-based learning, the CNS would update the motor command in the opposite direction, $u_{k+1} = u_k - \\eta \\nabla E(u_k)$, where $\\eta$ is a small positive learning rate. This process iteratively adjusts the command to minimize the task error.\n\nThe directional derivative is then:\n$$\\nabla_{\\Delta u} E(u_0) = e(u_0)^T J_h(x_0) J_f(u_0) \\Delta u$$\nwhere $x_0 = f(u_0)$. We now proceed with the numerical computation with $u_0 = (0.3, 0.2, -0.5)^\\top$, $y^{\\star} = (0.2, 0.4)^\\top$, and $\\Delta u = (0.1, -0.2, 0.05)^\\top$.\n\nStep 1: Compute $x_0 = f(u_0)$.\n$x_0 = f(u_0) = \\begin{pmatrix} \\tanh(0.3) + (0.2)(-0.5) \\\\ (0.3)^{2} - \\cos(0.2) \\end{pmatrix} = \\begin{pmatrix} 0.29131 - 0.1 \\\\ 0.09 - 0.98007 \\end{pmatrix} = \\begin{pmatrix} 0.19131 \\\\ -0.89007 \\end{pmatrix}$.\n\nStep 2: Compute the Jacobians $J_f(u_0)$ and $J_h(x_0)$.\n$J_f(u) = \\begin{pmatrix} 1-\\tanh^2(u_1)  u_3  u_2 \\\\ 2u_1  \\sin(u_2)  0 \\end{pmatrix}$.\nAt $u_0$, $J_f(u_0) = \\begin{pmatrix} 1-\\tanh^2(0.3)  -0.5  0.2 \\\\ 2(0.3)  \\sin(0.2)  0 \\end{pmatrix} \\approx \\begin{pmatrix} 0.91515  -0.5  0.2 \\\\ 0.6  0.19867  0 \\end{pmatrix}$.\n\n$J_h(x) = \\begin{pmatrix} 2x_1  \\cos(x_2) \\\\ \\exp(x_1)  1 \\end{pmatrix}$.\nAt $x_0$, $J_h(x_0) = \\begin{pmatrix} 2(0.19131)  \\cos(-0.89007) \\\\ \\exp(0.19131)  1 \\end{pmatrix} \\approx \\begin{pmatrix} 0.38262  0.62933 \\\\ 1.21085  1 \\end{pmatrix}$.\n\nStep 3: Compute the error vector $e(u_0) = h(f(u_0)) - y^{\\star} = h(x_0) - y^{\\star}$.\n$y_0 = h(x_0) \\approx \\begin{pmatrix} (0.19131)^2 + \\sin(-0.89007) \\\\ \\exp(0.19131) + (-0.89007) \\end{pmatrix} = \\begin{pmatrix} 0.03660 - 0.77712 \\\\ 1.21085 - 0.89007 \\end{pmatrix} = \\begin{pmatrix} -0.74052 \\\\ 0.32078 \\end{pmatrix}$.\n$e(u_0) = y_0 - y^{\\star} \\approx \\begin{pmatrix} -0.74052 - 0.2 \\\\ 0.32078 - 0.4 \\end{pmatrix} = \\begin{pmatrix} -0.94052 \\\\ -0.07922 \\end{pmatrix}$.\n\nStep 4: Compute the full product $\\nabla_{\\Delta u} E(u_0) = e(u_0)^T J_h(x_0) J_f(u_0) \\Delta u$.\nWe can perform the matrix-vector products sequentially.\nFirst, $\\Delta x = J_f(u_0) \\Delta u$:\n$\\Delta x \\approx \\begin{pmatrix} 0.91515  -0.5  0.2 \\\\ 0.6  0.19867  0 \\end{pmatrix} \\begin{pmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{pmatrix} = \\begin{pmatrix} 0.091515 + 0.1 + 0.01 \\\\ 0.06 - 0.039734 + 0 \\end{pmatrix} = \\begin{pmatrix} 0.201515 \\\\ 0.020266 \\end{pmatrix}$.\n\nNext, $\\Delta y = J_h(x_0) \\Delta x$:\n$\\Delta y \\approx \\begin{pmatrix} 0.38262  0.62933 \\\\ 1.21085  1 \\end{pmatrix} \\begin{pmatrix} 0.201515 \\\\ 0.020266 \\end{pmatrix} = \\begin{pmatrix} 0.07711 + 0.01275 \\\\ 0.24401 + 0.02027 \\end{pmatrix} = \\begin{pmatrix} 0.08986 \\\\ 0.26428 \\end{pmatrix}$.\n\nFinally, the directional derivative is $e(u_0)^T \\Delta y$:\n$\\nabla_{\\Delta u} E(u_0) \\approx \\begin{pmatrix} -0.94052  -0.07922 \\end{pmatrix} \\begin{pmatrix} 0.08986 \\\\ 0.26428 \\end{pmatrix}$\n$= (-0.94052)(0.08986) + (-0.07922)(0.26428)$\n$= -0.08452 - 0.02094 = -0.10546$.\n\nUsing higher precision for all intermediate steps:\n$x_0 = \\begin{pmatrix} 0.19131260 \\\\ -0.89006658 \\end{pmatrix}$\n$e(u_0) = \\begin{pmatrix} -0.94051586 \\\\ -0.07921628 \\end{pmatrix}$\n$J_f(u_0) = \\begin{pmatrix} 0.9151523  -0.5  0.2 \\\\ 0.6  0.19866933  0 \\end{pmatrix}$\n$J_h(x_0) = \\begin{pmatrix} 0.3826252  0.6293315 \\\\ 1.2108503  1 \\end{pmatrix}$\n$J_y(u_0) \\Delta u = J_h(x_0) (J_f(u_0) \\Delta u) = \\begin{pmatrix} 0.08985895 \\\\ 0.2642747 \\end{pmatrix}$\n$\\nabla_{\\Delta u} E(u_0) = e(u_0)^T (J_y(u_0) \\Delta u) = (-0.94051586)(0.08985895) + (-0.07921628)(0.2642747) = -0.08451336 - 0.02093498 = -0.10544834$.\n\nRounding to four significant figures, the result is $-0.1054$.\nThe negative value indicates that moving the command vector from $u_0$ in the direction of $\\Delta u$ results in a decrease in the task error $E$. This is a desirable change from the perspective of learning.",
            "answer": "$$\\boxed{-0.1054}$$"
        },
        {
            "introduction": "While elegant, inverse models face significant challenges, particularly at kinematic singularities where the mapping from task goals to motor commands becomes ill-posed. This exercise delves into this critical issue by analyzing a two-link arm near its fully extended posture. You will compute the Jacobian's condition number to formally demonstrate how numerical stability breaks down, providing a mathematical basis for understanding why the nervous system must adopt robust control strategies that avoid or manage such instabilities. ",
            "id": "3992093",
            "problem": "Consider a planar, two-degree-of-freedom arm with link lengths $l_1$ and $l_2$, shoulder joint angle $\\theta_1$, and elbow joint angle $\\theta_2$. The end-effector position is given by the forward kinematics $x(\\theta_1,\\theta_2) = l_1 \\cos(\\theta_1) + l_2 \\cos(\\theta_1 + \\theta_2)$ and $y(\\theta_1,\\theta_2) = l_1 \\sin(\\theta_1) + l_2 \\sin(\\theta_1 + \\theta_2)$. The inverse model for task-space velocity control uses the Jacobian $J(\\theta_1,\\theta_2)$ mapping joint velocities $\\dot{\\theta} = (\\dot{\\theta}_1,\\dot{\\theta}_2)$ to end-effector velocities $\\dot{\\mathbf{r}} = (\\dot{x},\\dot{y})$ via $\\dot{\\mathbf{r}} = J \\dot{\\theta}$, and the pseudoinverse $J^+$ to solve $\\dot{\\theta} = J^+ \\dot{\\mathbf{r}}$. The numerical stability of this inverse mapping depends on the condition number $\\kappa(J)$, defined as the ratio of the largest to the smallest singular value of $J$.\n\nStarting strictly from the given forward kinematics and the standard definitions of Jacobian, singular values, and condition number, derive the Jacobian $J$, compute $J^{\\top}J$, and determine $\\kappa(J)$ near the elbow-straight configuration $\\theta_2 \\approx 0$. Using a leading-order asymptotic analysis in $|\\theta_2|$, obtain a closed-form expression for $\\kappa(J)$ that captures its divergence as $\\theta_2 \\to 0$, expressed in terms of $l_1$, $l_2$, and $\\theta_2$ only. Finally, explain based on this derivation how the behavior of $\\kappa(J)$ affects the numerical stability of inverse kinematics and inverse models for motor control near elbow-straight postures. The final answer must be a single analytic expression for the leading-order asymptotic of $\\kappa(J)$ in $|\\theta_2|$. No rounding is required, and no units are necessary.",
            "solution": "This problem requires deriving the Jacobian of a two-link arm and then performing an asymptotic analysis of its condition number near a singular configuration.\n\nThe forward kinematics are given by:\n$$x(\\theta_1, \\theta_2) = l_1 \\cos(\\theta_1) + l_2 \\cos(\\theta_1 + \\theta_2)$$\n$$y(\\theta_1, \\theta_2) = l_1 \\sin(\\theta_1) + l_2 \\sin(\\theta_1 + \\theta_2)$$\nThe Jacobian matrix $J$ relates joint velocities to end-effector velocities ($\\dot{\\mathbf{r}} = J \\dot{\\theta}$). It is the matrix of partial derivatives of the position vector with respect to the joint angles:\n$$J = \\begin{pmatrix} \\frac{\\partial x}{\\partial \\theta_1}  \\frac{\\partial x}{\\partial \\theta_2} \\\\ \\frac{\\partial y}{\\partial \\theta_1}  \\frac{\\partial y}{\\partial \\theta_2} \\end{pmatrix} = \\begin{pmatrix} -l_1 \\sin(\\theta_1) - l_2 \\sin(\\theta_1 + \\theta_2)  -l_2 \\sin(\\theta_1 + \\theta_2) \\\\ l_1 \\cos(\\theta_1) + l_2 \\cos(\\theta_1 + \\theta_2)  l_2 \\cos(\\theta_1 + \\theta_2) \\end{pmatrix}$$\nThe condition number $\\kappa(J) = \\sigma_{\\text{max}} / \\sigma_{\\text{min}}$ is found from the singular values of $J$, which are the square roots of the eigenvalues of $J^{\\top}J$. We first compute $J^{\\top}J$:\n$$J^{\\top}J = \\begin{pmatrix} l_1^2 + l_2^2 + 2l_1 l_2 \\cos(\\theta_2)  l_2^2 + l_1 l_2 \\cos(\\theta_2) \\\\ l_2^2 + l_1 l_2 \\cos(\\theta_2)  l_2^2 \\end{pmatrix}$$\nThe squared singular values, $\\sigma^2$, are the eigenvalues $\\lambda$ of $J^{\\top}J$, found from the characteristic equation $\\lambda^2 - \\text{Tr}(J^{\\top}J)\\lambda + \\det(J^{\\top}J) = 0$.\nThe trace and determinant are:\n$\\text{Tr}(J^{\\top}J) = l_1^2 + 2l_2^2 + 2l_1 l_2 \\cos(\\theta_2)$\n$\\det(J^{\\top}J) = (\\det J)^2 = (l_1 l_2 \\sin(\\theta_2))^2 = l_1^2 l_2^2 \\sin^2(\\theta_2)$\n\nWe are interested in the behavior near the elbow-straight configuration, $\\theta_2 \\approx 0$. We perform a leading-order asymptotic analysis using $\\cos(\\theta_2) \\approx 1 - \\frac{\\theta_2^2}{2}$ and $\\sin(\\theta_2) \\approx \\theta_2$.\nThe determinant becomes very small:\n$\\det(J^{\\top}J) \\approx l_1^2 l_2^2 \\theta_2^2$\nThe trace approaches a constant value:\n$\\lim_{\\theta_2 \\to 0} \\text{Tr}(J^{\\top}J) = l_1^2 + 2l_2^2 + 2l_1 l_2$\n\nFor a 2x2 matrix with a very small determinant, the eigenvalues can be approximated as $\\lambda_{\\text{max}} \\approx \\text{Tr}$ and $\\lambda_{\\text{min}} \\approx \\det/\\text{Tr}$.\n$\\sigma_{\\text{max}}^2 = \\lambda_{\\text{max}} \\approx l_1^2 + 2l_1 l_2 + 2l_2^2$\n$\\sigma_{\\text{min}}^2 = \\lambda_{\\text{min}} \\approx \\frac{l_1^2 l_2^2 \\theta_2^2}{l_1^2 + 2l_1 l_2 + 2l_2^2}$\n\nThe condition number is $\\kappa(J) = \\frac{\\sigma_{\\text{max}}}{\\sigma_{\\text{min}}} = \\sqrt{\\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}}}$.\nUsing our asymptotic expressions:\n$\\kappa(J)^2 \\approx \\frac{l_1^2 + 2l_1 l_2 + 2l_2^2}{\\frac{l_1^2 l_2^2 \\theta_2^2}{l_1^2 + 2l_1 l_2 + 2l_2^2}} = \\frac{(l_1^2 + 2l_1 l_2 + 2l_2^2)^2}{l_1^2 l_2^2 \\theta_2^2}$\n\nTaking the square root, we obtain the leading-order asymptotic expression for the condition number:\n$\\kappa(J) \\approx \\frac{l_1^2 + 2l_1 l_2 + 2l_2^2}{l_1 l_2 |\\theta_2|}$\n\nThis expression reveals that as $\\theta_2 \\to 0$, the condition number $\\kappa(J)$ diverges as $1/|\\theta_2|$. This has profound implications for motor control. A high condition number means the Jacobian matrix is ill-conditioned. For the inverse velocity problem $\\dot{\\theta} = J^{+} \\dot{\\mathbf{r}}$, this implies that small errors in the desired end-effector velocity $\\dot{\\mathbf{r}}$ can be magnified into very large errors in the computed joint velocities $\\dot{\\theta}$, leading to erratic movements. Furthermore, achieving even modest end-effector velocities may require unfeasibly large joint velocities. The divergence of $\\kappa(J)$ at this kinematic singularity demonstrates that a naive inverse model is not a viable neural control strategy near such postures, suggesting the brain must employ robust strategies like avoiding singular postures or using regularization.",
            "answer": "$$\n\\boxed{\\frac{l_1^2 + 2 l_1 l_2 + 2 l_2^2}{l_1 l_2 |\\theta_2|}}\n$$"
        },
        {
            "introduction": "Effective motor control extends beyond simply reaching a target; it involves navigating a world filled with obstacles and constraints. This practice formalizes this challenge as a constrained optimization problem, where the goal is to minimize a cost function subject to physical boundaries. By applying the Karush-Kuhn-Tucker (KKT) conditions, you will find an optimal motor command that respects workspace limits, illustrating a powerful framework for modeling goal-directed yet constrained behavior. ",
            "id": "3992122",
            "problem": "A planar single-joint limb of length $L$ rotates in the plane about the shoulder, producing an end-effector position $\\mathbf{x}(\\theta) = (x(\\theta), y(\\theta))$ given by the forward kinematics $x(\\theta) = L \\cos(\\theta)$ and $y(\\theta) = L \\sin(\\theta)$. An inverse model in motor control seeks a motor command $\\theta$ that achieves a desired end-effector target $\\mathbf{x}_{d} = (x_{d}, y_{d})$ while respecting workspace obstacles. Consider the cost function\n$$\nJ(\\theta) = \\frac{1}{2}\\left[(x(\\theta) - x_{d})^{2} + (y(\\theta) - y_{d})^{2}\\right] + \\frac{\\lambda}{2}(\\theta - \\theta_{0})^{2},\n$$\nwhich encodes accuracy of reaching and a regularization toward a nominal motor command $\\theta_{0}$. The workspace has a vertical obstacle boundary at $x_{w}$, and the feasible region requires the end-effector to remain to the right of the boundary, expressed by the inequality constraint\n$$\nh(\\theta) = x_{w} - x(\\theta) \\le 0.\n$$\nUse the following parameters: $L = 0.5$ $\\mathrm{m}$, $(x_{d}, y_{d}) = (0.3, 0.4)$ $\\mathrm{m}$, $x_{w} = 0.35$ $\\mathrm{m}$, $\\lambda = 0.1$, and $\\theta_{0} = \\arctan2(y_{d}, x_{d})$.\n\nStarting from fundamental definitions in forward kinematics and constrained optimization:\n\n- Formulate the constrained inverse kinematics problem that minimizes $J(\\theta)$ subject to $h(\\theta) \\le 0$.\n- Derive the Karush–Kuhn–Tucker (KKT) conditions for a constrained minimizer $\\theta^{\\star}$, introducing a nonnegative Lagrange multiplier $\\mu$.\n- Determine whether the obstacle constraint is active at the constrained optimum for the given numerical values, and compute the constrained optimal joint angle $\\theta^{\\star}$ in radians. Round your final numerical answer to four significant figures. Express the angle in radians.",
            "solution": "The problem is to find the motor command $\\theta$ that minimizes a cost function $J(\\theta)$ subject to a workspace constraint $h(\\theta) \\le 0$. The solution requires applying the Karush-Kuhn-Tucker (KKT) conditions for constrained optimization. The Lagrangian for this problem is $\\mathcal{L}(\\theta, \\mu) = J(\\theta) + \\mu h(\\theta)$, where $\\mu$ is a Lagrange multiplier.\n\nThe KKT conditions for an optimal solution $\\theta^\\star$ are:\n1.  **Stationarity**: $\\frac{d\\mathcal{L}}{d\\theta}|_{\\theta=\\theta^{\\star}} = \\frac{dJ}{d\\theta}|_{\\theta=\\theta^{\\star}} + \\mu \\frac{dh}{d\\theta}|_{\\theta=\\theta^{\\star}} = 0$\n2.  **Primal Feasibility**: $h(\\theta^{\\star}) = x_{w} - L \\cos(\\theta^{\\star}) \\le 0$\n3.  **Dual Feasibility**: $\\mu \\ge 0$\n4.  **Complementary Slackness**: $\\mu h(\\theta^{\\star}) = 0$\n\nFirst, we find the derivative of the cost function $J(\\theta)$. The derivative is:\n$$\n\\frac{dJ}{d\\theta} = x_{d} L \\sin(\\theta) - y_{d} L \\cos(\\theta) + \\lambda(\\theta - \\theta_{0})\n$$\nThe derivative of the constraint function is:\n$$\n\\frac{dh}{d\\theta} = \\frac{d}{d\\theta}(x_{w} - L \\cos(\\theta)) = L \\sin(\\theta)\n$$\nTo determine if the constraint is active, we first find the unconstrained optimum by setting $\\mu=0$ and solving $\\frac{dJ}{d\\theta}=0$. Given the target distance $\\sqrt{x_d^2 + y_d^2} = L$, we can write $x_d = L \\cos(\\theta_0)$ and $y_d = L \\sin(\\theta_0)$. Substituting this into $\\frac{dJ}{d\\theta}$ yields $L^2\\sin(\\theta-\\theta_0) + \\lambda(\\theta-\\theta_0) = 0$, which has the solution $\\theta_{unc} = \\theta_0$. We check if this unconstrained solution is feasible:\n$$\nh(\\theta_0) = x_w - L\\cos(\\theta_0) = x_w - x_d = 0.35 - 0.3 = 0.05\n$$\nSince $h(\\theta_0) = 0.05 > 0$, the unconstrained solution violates primal feasibility. Therefore, the constraint must be **active** at the constrained optimum, meaning $h(\\theta^{\\star}) = 0$.\n\nFor an active constraint, we have $h(\\theta^{\\star}) = x_w - L \\cos(\\theta^{\\star}) = 0$, which gives:\n$$\n\\cos(\\theta^{\\star}) = \\frac{x_w}{L} = \\frac{0.35}{0.5} = 0.7\n$$\nThis equation has two potential solutions, $\\theta^{\\star} = \\arccos(0.7)$ and $\\theta^{\\star} = -\\arccos(0.7)$. We must find which one satisfies the KKT conditions, specifically dual feasibility ($\\mu \\ge 0$). From the stationarity equation, we solve for $\\mu$:\n$$\n\\mu = - \\frac{x_{d} L \\sin(\\theta^{\\star}) - y_{d} L \\cos(\\theta^{\\star}) + \\lambda(\\theta^{\\star} - \\theta_{0})}{L \\sin(\\theta^{\\star})}\n$$\nWe calculate the nominal angle $\\theta_0 = \\arctan2(0.4, 0.3) \\approx 0.9273$ rad. Now we test the two candidates for $\\theta^{\\star}$.\n\n**Candidate 1: $\\theta^{\\star} = \\arccos(0.7) \\approx 0.7954$ rad**\nFor this angle, $\\sin(\\theta^{\\star}) = \\sqrt{1 - 0.7^2} = \\sqrt{0.51} > 0$. The denominator $L \\sin(\\theta^{\\star})$ is positive. The numerator evaluates to approximately $-0.0461$. Since the numerator is negative and the denominator is positive, $\\mu = -(\\text{negative}/\\text{positive}) > 0$. This candidate is a valid KKT point.\n\n**Candidate 2: $\\theta^{\\star} = -\\arccos(0.7) \\approx -0.7954$ rad**\nFor this angle, $\\sin(\\theta^{\\star}) = -\\sqrt{0.51}  0$. The denominator is negative. The numerator evaluates to approximately $-0.4194$. Since both numerator and denominator are negative, $\\mu = -(\\text{negative}/\\text{negative})  0$. This violates the dual feasibility condition $\\mu \\ge 0$.\n\nTherefore, the only valid solution is $\\theta^{\\star} = \\arccos(0.7)$.\nNumerically, $\\theta^{\\star} \\approx 0.795398...$ radians. Rounding to four significant figures gives the constrained optimal angle.",
            "answer": "$$\n\\boxed{0.7954}\n$$"
        }
    ]
}