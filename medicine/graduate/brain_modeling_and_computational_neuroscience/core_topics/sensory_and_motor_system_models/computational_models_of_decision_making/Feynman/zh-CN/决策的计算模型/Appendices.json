{
    "hands_on_practices": [
        {
            "introduction": "我们的实践练习将从经济决策的基本原理开始。本题深入探讨了冯·诺依曼-摩根斯坦期望效用理论 (von Neumann-Morgenstern Expected Utility Theory)，演示了如何使用效用函数来数学化地模拟决策者对风险选项的偏好。通过计算在何种风险规避水平上决策者会对不同选项持无差异态度 ，您将具体地理解风险态度这个概念是如何量化不确定性下的选择的。",
            "id": "3970893",
            "problem": "一个决策者在冯·诺依曼-摩根斯坦期望效用理论（EUT）的公理下，面临在两种彩票之间的二元选择。这些彩票具有货币收益，并且每个结果都具有相等的概率质量：彩票 $X$ 的收益为 $x \\in \\{0,10\\}$，其中 $p(x=10)=0.5$ 且 $p(x=0)=0.5$；彩票 $Y$ 的收益为 $y \\in \\{3,7\\}$，其中 $p(y=7)=0.5$ 且 $p(y=3)=0.5$。对货币收益的偏好由一个二阶连续可微的、具有恒定相对风险厌恶（CRRA）的伯努利效用函数表示，对于 $x \\ge 0$ 和 $\\rho \\in \\mathbb{R}$，$\\rho \\ne 1$，该函数为 $u(x) = x^{1-\\rho}/(1-\\rho)$，并通过在 $\\rho=1$ 处的连续性扩展为 $u(x) = \\ln(x)$（对于 $x>0$）。在 $x=0$ 处的定义域限制必须通过CRRA参数化下的适当极限来处理。\n\n从期望效用的定义和给定的CRRA函数形式出发，推导出每种彩票的期望效用作为风险厌恶参数 $\\rho$ 的函数，并仔细处理在 $x=0$ 处的行为。然后，计算唯一的实数值 $\\rho^{\\star}$，在该值下决策者对 $X$ 和 $Y$ 无差异，即 $X$ 和 $Y$ 的期望效用相等。将 $\\rho^{\\star}$ 报告为单个实数。如果需要数值计算，您将四舍五入到四位有效数字，但如果存在精确值，则提供精确值。",
            "solution": "问题要求解出恒定相对风险厌恶（CRRA）参数的特定值，记为 $\\rho^{\\star}$，在该值下，决策者对彩票 $X$ 和 $Y$ 无差异。当两种彩票的期望效用相等时，即 $E[u(X)] = E[u(Y)]$ 时，达到无差异状态。\n\n冯·诺依曼-摩根斯坦期望效用理论的公理指出，彩票的期望效用是其结果效用的概率加权和。对于具有结果 $\\{o_i\\}$ 和相应概率 $\\{p_i\\}$ 的离散彩票 $L$，其期望效用为 $E[u(L)] = \\sum_i p_i u(o_i)$。\n\n给定的彩票是：\n彩票 $X$：结果为 $\\{0, 10\\}$，概率为 $p(x=0) = 0.5$ 和 $p(x=10) = 0.5$。\n彩票 $Y$：结果为 $\\{3, 7\\}$，概率为 $p(y=3) = 0.5$ 和 $p(y=7) = 0.5$。\n\n伯努利效用函数是CRRA形式的：\n对于 $\\rho \\ne 1$，$u(z) = \\frac{z^{1-\\rho}}{1-\\rho}$。\n对于 $\\rho = 1$，$u(z) = \\ln(z)$。\n\n首先，我们推导每种彩票的期望效用作为 $\\rho$ 的函数。\n\n对于彩票 $Y$：\n其结果为 $3$ 和 $7$，均为严格为正的数。对于任何结果 $y>0$，其效用对所有 $\\rho$ 都有明确定义。\n对于 $\\rho \\ne 1$：\n$$E[u(Y)] = 0.5 \\cdot u(7) + 0.5 \\cdot u(3) = 0.5 \\left( \\frac{7^{1-\\rho}}{1-\\rho} + \\frac{3^{1-\\rho}}{1-\\rho} \\right) = \\frac{7^{1-\\rho} + 3^{1-\\rho}}{2(1-\\rho)}$$\n对于 $\\rho=1$：\n$$E[u(Y)] = 0.5 \\cdot \\ln(7) + 0.5 \\cdot \\ln(3) = 0.5 \\ln(21)$$\n在所有情况下，$E[u(Y)]$ 都是一个有限值。\n\n对于彩票 $X$：\n其结果为 $10$ 和 $0$。正如问题陈述中所指出的，结果 $0$ 的效用需要通过取极限进行仔细处理。\n$$E[u(X)] = 0.5 \\cdot u(10) + 0.5 \\cdot u(0)$$\n我们必须根据参数 $\\rho$ 来分析 $u(0)$ 的值。\n\n情况 1：$\\rho > 1$。\n在这种情况下，指数 $1-\\rho$ 是负数。令 $1-\\rho = -\\alpha$，其中 $\\alpha > 0$。\n结果 $z>0$ 的效用是 $u(z) = \\frac{z^{-\\alpha}}{-\\alpha}$。\n结果为零的效用由极限确定：\n$$u(0) = \\lim_{z \\to 0^+} \\frac{z^{-\\alpha}}{-\\alpha} = \\lim_{z \\to 0^+} -\\frac{1}{\\alpha z^{\\alpha}} = -\\infty$$\n因此，彩票 $X$ 的期望效用是：\n$$E[u(X)] = 0.5 \\cdot u(10) + 0.5 \\cdot (-\\infty) = -\\infty$$\n由于 $E[u(Y)]$ 是有限的，无差异条件 $E[u(X)] = E[u(Y)]$ 无法满足。因此，在 $\\rho > 1$ 的范围内，$\\rho^{\\star}$ 不存在解。\n\n情况 2：$\\rho = 1$。\n效用函数是 $u(z) = \\ln(z)$。结果为零的效用是：\n$$u(0) = \\lim_{z \\to 0^+} \\ln(z) = -\\infty$$\n彩票 $X$ 的期望效用是：\n$$E[u(X)] = 0.5 \\cdot \\ln(10) + 0.5 \\cdot (-\\infty) = -\\infty$$\n同样，$E[u(Y)]$ 是有限的，所以无差异条件无法满足。$\\rho^{\\star} = 1$ 不存在解。\n\n情况 3：$\\rho < 1$。\n在这种情况下，指数 $1-\\rho$ 是正数。\n结果为零的效用是：\n$$u(0) = \\lim_{z \\to 0^+} \\frac{z^{1-\\rho}}{1-\\rho} = \\frac{0}{1-\\rho} = 0$$\n彩票 $X$ 的期望效用是明确定义且有限的：\n$$E[u(X)] = 0.5 \\cdot u(10) + 0.5 \\cdot u(0) = 0.5 \\left( \\frac{10^{1-\\rho}}{1-\\rho} \\right) + 0.5 \\cdot 0 = \\frac{10^{1-\\rho}}{2(1-\\rho)}$$\n现在可以在此定义域内研究无差异条件 $E[u(X)] = E[u(Y)]$：\n$$\\frac{10^{1-\\rho}}{2(1-\\rho)} = \\frac{7^{1-\\rho} + 3^{1-\\rho}}{2(1-\\rho)}$$\n由于我们处在 $\\rho < 1$ 的定义域内，分母 $2(1-\\rho)$ 不为零，允许我们两边同乘以它：\n$$10^{1-\\rho} = 7^{1-\\rho} + 3^{1-\\rho}$$\n为了解出 $\\rho$，我们定义一个新变量 $k = 1-\\rho$。由于 $\\rho < 1$，因此 $k > 0$。方程变换为：\n$$10^k = 7^k + 3^k$$\n为了求 $k$，我们可以用 $10^k$（非零）除以方程两边：\n$$1 = \\frac{7^k}{10^k} + \\frac{3^k}{10^k}$$\n$$1 = \\left(\\frac{7}{10}\\right)^k + \\left(\\frac{3}{10}\\right)^k$$\n我们来分析函数 $f(k) = (0.7)^k + (0.3)^k$ 在 $k > 0$ 时的行为。我们寻求使 $f(k)=1$ 的 $k$ 值。\n函数 $f(k)$ 是两个正的、严格递减的指数函数之和，因为它们的底数（$0.7$ 和 $0.3$）介于 $0$ 和 $1$ 之间。因此，对于 $k>0$，$f(k)$ 是一个严格单调递减函数。这保证了如果存在解，那么解是唯一的。\n我们可以测试一个简单的 $k$ 值。令 $k=1$：\n$$f(1) = (0.7)^1 + (0.3)^1 = 0.7 + 0.3 = 1$$\n我们发现 $k=1$ 是唯一的解。\n最后，我们代回去求 $\\rho^{\\star}$ 的值：\n$$k = 1 - \\rho^{\\star}$$\n$$1 = 1 - \\rho^{\\star}$$\n$$\\rho^{\\star} = 0$$\n该值位于我们确定可能存在解的定义域 $\\rho < 1$ 内。对于 $\\rho=0$，效用函数为 $u(x) = x$，这对应于风险中性。风险中性的个体会比较期望货币价值。\n$E[X] = 0.5 \\cdot 10 + 0.5 \\cdot 0 = 5$。\n$E[Y] = 0.5 \\cdot 7 + 0.5 \\cdot 3 = 0.5 \\cdot 10 = 5$。\n由于 $E[X]=E[Y]$，风险中性的代理人确实是无差异的，这证实了我们的结果。唯一的实数值是 $\\rho^{\\star} = 0$。",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "在静态选择的基础上，我们现在转向序列决策问题，在其中智能体必须从经验中学习。本练习通过多臂老虎机 (Multi-Armed Bandit) 框架引入了经典的“探索-利用”困境 (exploration-exploitation dilemma)，这是强化学习的基石。您将应用上置信界 (Upper Confidence Bound, UCB1) 算法来决定下一步的最佳行动，从而获得关于智能体如何平衡利用已知回报与探索未知选项以优化长期收益的实践性见解 。",
            "id": "3970900",
            "problem": "一个主体参与一个三臂随机决策任务，该任务被建模为多臂老虎机（Multi-Armed Bandit, MAB），其中每个臂 $j \\in \\{1,2,3\\}$ 产生独立的奖励 $r_{j,i} \\in [0,1]$，这些奖励来自一个均值为 $\\mu_j$ 的未知平稳分布。在每个时间步 $t$，策略选择一个臂并观察到单个奖励。假设在给定臂的标识下，奖励是条件独立的，并且界于区间 $[0,1]$ 内。在时间 $t=20$ 时，历史交互记录包含每个臂的以下观测奖励序列：\n- 臂 $1$ 被拉动了 $8$ 次，奖励为：$0.9, 0.7, 0.6, 0.8, 0.7, 0.4, 0.9, 0.6$。\n- 臂 $2$ 被拉动了 $7$ 次，奖励为：$0.5, 0.8, 0.3, 0.6, 0.4, 0.7, 0.6$。\n- 臂 $3$ 被拉动了 $5$ 次，奖励为：$0.2, 0.4, 0.9, 0.6, 0.5$。\n\n使用上置信界1 (UCB1) 原则，该原则通过 Hoeffding 型置信区间和自然对数，从有界奖励的测度集中性推导得出，计算在时间 $t=20$ 时每个臂的经验均值和 UCB 指数，并选择 UCB1 在 $t=20$ 时会选择的动作。\n\n仅报告所选臂的索引，以单个整数形式。最终答案无需四舍五入。在您的推导和解题计算中，对任何对数量均使用自然对数。",
            "solution": "上置信界1 (UCB1) 算法旨在平衡多臂老虎机问题中的探索-利用权衡。它在每个时间步通过最大化一个 UCB 指数来选择一个臂，该指数是经验平均奖励（利用项）和一个置信度奖励（探索项）的总和。\n\n在时间步 $t$ 时，臂 $j$ 的 UCB1 指数由下式给出：\n$$\n\\text{UCB}_j(t) = Q_j(t) + \\sqrt{\\frac{2 \\ln t}{n_j(t)}}\n$$\n其中：\n- $Q_j(t)$ 是截至时间 $t$ 从臂 $j$ 获得的奖励的经验均值。\n- $n_j(t)$ 是截至时间 $t$ 臂 $j$ 被拉动的次数。\n- $t$ 是截至该时间点所有臂的总拉动次数。\n\n策略是选择具有最高 UCB 指数的臂：$a_{t+1} = \\underset{j}{\\operatorname{argmax}} \\, \\text{UCB}_j(t)$。\n\n在给定的时间 $t=20$，我们有以下信息：\n- 总试验次数：$t = 20$。\n- 对于臂 1：$n_1 = 8$。\n- 对于臂 2：$n_2 = 7$。\n- 对于臂 3：$n_3 = 5$。\n\n首先，我们计算每个臂的经验平均奖励 $Q_j$。\n\n**臂 1：**\n奖励总和为 $0.9 + 0.7 + 0.6 + 0.8 + 0.7 + 0.4 + 0.9 + 0.6 = 5.6$。\n经验均值为：\n$$\nQ_1 = \\frac{5.6}{8} = 0.7\n$$\n\n**臂 2：**\n奖励总和为 $0.5 + 0.8 + 0.3 + 0.6 + 0.4 + 0.7 + 0.6 = 3.9$。\n经验均值为：\n$$\nQ_2 = \\frac{3.9}{7} \\approx 0.5571\n$$\n\n**臂 3：**\n奖励总和为 $0.2 + 0.4 + 0.9 + 0.6 + 0.5 = 2.6$。\n经验均值为：\n$$\nQ_3 = \\frac{2.6}{5} = 0.52\n$$\n\n接下来，我们计算在 $t=20$ 时每个臂的 UCB1 指数。使用 $\\ln 20 \\approx 2.9957$：\n\n**臂 1 的 UCB1 指数：**\n$$\n\\text{UCB}_1 = Q_1 + \\sqrt{\\frac{2 \\ln t}{n_1}} = 0.7 + \\sqrt{\\frac{2 \\ln 20}{8}} = 0.7 + \\sqrt{\\frac{\\ln 20}{4}} \\approx 0.7 + \\sqrt{\\frac{2.9957}{4}} \\approx 0.7 + 0.8654 = 1.5654\n$$\n\n**臂 2 的 UCB1 指数：**\n$$\n\\text{UCB}_2 = Q_2 + \\sqrt{\\frac{2 \\ln t}{n_2}} = \\frac{3.9}{7} + \\sqrt{\\frac{2 \\ln 20}{7}} \\approx 0.5571 + \\sqrt{\\frac{2 \\times 2.9957}{7}} \\approx 0.5571 + 0.9252 = 1.4823\n$$\n\n**臂 3 的 UCB1 指数：**\n$$\n\\text{UCB}_3 = Q_3 + \\sqrt{\\frac{2 \\ln t}{n_3}} = 0.52 + \\sqrt{\\frac{2 \\ln 20}{5}} \\approx 0.52 + \\sqrt{\\frac{2 \\times 2.9957}{5}} \\approx 0.52 + 1.0947 = 1.6147\n$$\n\n比较计算出的 UCB 指数：\n$$\n1.6147 > 1.5654 > 1.4823\n$$\n这意味着 $\\text{UCB}_3 > \\text{UCB}_1 > \\text{UCB}_2$。\n\nUCB1 算法选择具有最大 UCB 指数的臂。在这种情况下，臂 3 的指数最高。这个决策主要是由臂 3 较大的探索奖励驱动的，因为它的拉动次数相对较少（$n_3=5$），这超过了它与臂 1 相比之下较低的经验平均奖励。\n\n因此，在时间 $t=20$ 时，UCB1 策略会选择臂 3 进行下一次拉动。",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "我们的最后一个实践问题将处理一个位于计算神经科学与人工智能前沿的复杂模型：行动者-评论家 (Actor-Critic) 架构。这些模型为强化学习提供了一个神经可信的 (neurally plausible) 框架，其中“行动者”学习一个策略，而“评论家”则对该策略进行评估。这个练习要求您为一个行动者-评论家智能体计算详细的参数更新，从而让您通过动手实践，深入理解智能体如何通过整合策略梯度和时间差分学习，在一个有状态的环境中学习并调整其行为 。",
            "id": "3970872",
            "problem": "考虑一个在有限马尔可夫决策过程（MDP）中的在策略强化学习智能体，该过程有两个状态 $s \\in \\{s_{1}, s_{2}\\}$ 和两个动作 $a \\in \\{a_{1}, a_{2}\\}$。该智能体使用 actor-critic 架构，其中评论家通过分解 $A(s,a) = Q(s,a) - V(s)$ 来估计优势 $A(s,a)$，其中 $Q(s,a)$ 是动作价值函数，$V(s)$ 是当前策略下的状态价值函数。策略是基于线性偏好的 softmax 函数。以下元素定义了模型和数据：\n- 状态特征映射 $\\phi(s) \\in \\mathbb{R}^{2}$ 由 $\\phi(s_{1}) = [1, 0]^{\\top}$ 和 $\\phi(s_{2}) = [0, 1]^{\\top}$ 给出。\n- Actor 的偏好是特征的线性函数：对于每个动作 $a \\in \\{a_{1}, a_{2}\\}$，偏好为 $h(a \\mid s) = \\theta_{a}^{\\top} \\phi(s)$，策略为 $\\pi(a \\mid s) = \\frac{\\exp(h(a \\mid s))}{\\sum_{b \\in \\{a_{1}, a_{2}\\}} \\exp(h(b \\mid s))}$。当前 actor 的参数为 $\\theta_{a_{1}} = [0.3, -0.1]^{\\top}$ 和 $\\theta_{a_{2}} = [-0.2, 0.25]^{\\top}$。\n- 评论家使用线性函数近似：\n  - $Q(s,a) \\approx w_{Q}^{\\top} \\xi(s,a)$，其中 $\\xi(s,a) = [\\phi_{1}(s), \\phi_{2}(s), \\mathbf{1}\\{a = a_{1}\\}, \\mathbf{1}\\{a = a_{2}\\}]^{\\top} \\in \\mathbb{R}^{4}$，当前参数为 $w_{Q} = [0.1, 0.2, 0.05, -0.1]^{\\top}$。\n  - $V(s) \\approx w_{V}^{\\top} \\phi(s)$，当前参数为 $w_{V} = [0.0, 0.3]^{\\top}$。\n- 折扣因子为 $\\gamma = 0.9$。\n- 终止状态的价值定义为 $V(\\text{terminal}) = 0$。\n- 学习率分别为：actor 的学习率 $\\alpha_{\\pi} = 0.05$，动作价值评论家的学习率 $\\alpha_{Q} = 0.1$，状态价值评论家的学习率 $\\alpha_{V} = 0.1$。\n- 观测到一个包含三个转移 $(s, a, r, s')$ 的批次：\n  1. $(s_{1}, a_{1}, 1.0, s_{2})$\n  2. $(s_{1}, a_{2}, 0.0, s_{1})$\n  3. $(s_{2}, a_{2}, 2.0, \\text{terminal})$\n\n从强化学习和大脑建模的基本原理出发，即期望回报的定义、策略下的价值函数、策略梯度定理和时序差分（TD）学习，构建 actor-critic 更新，其中：\n- Actor 使用优势 $A(s,a)$ 作为基线偏移信号，对期望回报执行梯度上升，即更新量与由 $A(s,a)$ 缩放的 $\\nabla_{\\theta} \\ln \\pi(a \\mid s)$ 成正比。\n- 评论家更新 $w_{Q}$ 以通过对平方误差进行梯度下降来减小 $Q(s,a)$ 的单步 TD 误差，使其趋向目标 $r + \\gamma V(s')$。\n- 评论家更新 $w_{V}$ 以通过对平方误差进行梯度下降来减小 $V(s)$ 的单步 TD 误差，使其趋向 $r + \\gamma V(s')$。\n\n将批量更新视为每个转移贡献的总和（不按批次大小进行归一化）。计算在当前参数下，此批次产生的单步参数更新向量 $\\Delta \\theta$、$\\Delta w_{Q}$ 和 $\\Delta w_{V}$。将连接后的更新向量按\n$[\\Delta \\theta_{a_{1},1}, \\Delta \\theta_{a_{1},2}, \\Delta \\theta_{a_{2},1}, \\Delta \\theta_{a_{2},2}, \\Delta w_{Q,1}, \\Delta w_{Q,2}, \\Delta w_{Q,3}, \\Delta w_{Q,4}, \\Delta w_{V,1}, \\Delta w_{V,2}]$ 的顺序排列，\n并按该顺序提供您的最终数值答案。将每个条目四舍五入到四位有效数字。答案不带单位。最终答案必须是指令中指定的单行矩阵。",
            "solution": "该任务要求基于一个包含三个转移的批次，计算一个行动者-评论家模型的单步参数更新。更新量被视为每个转移贡献的总和。\n\n**1. 参数更新规则**\n\n评论家参数 $w_Q$ 和 $w_V$ 通过对平方单步时序差分（TD）误差进行梯度下降来更新。对于一个转移 $(s, a, r, s')$：\n- $Q$函数更新：$\\Delta w_Q = \\alpha_Q (r + \\gamma V(s') - Q(s,a)) \\xi(s,a)$\n- $V$函数更新：$\\Delta w_V = \\alpha_V (r + \\gamma V(s') - V(s)) \\phi(s)$\n\n行动者参数 $\\theta$ 根据带有优势函数基线的策略梯度定理进行更新：\n- $\\Delta \\theta_b = \\alpha_{\\pi} A(s,a) (\\mathbf{1}\\{a=b\\} - \\pi(b \\mid s)) \\phi(s)$\n其中优势函数为 $A(s,a) = Q(s,a) - V(s)$。\n\n**2. 初始参数与特征**\n\n- $\\theta_{a_{1}} = [0.3, -0.1]^{\\top}$, $\\theta_{a_{2}} = [-0.2, 0.25]^{\\top}$\n- $w_{Q} = [0.1, 0.2, 0.05, -0.1]^{\\top}$, $w_{V} = [0.0, 0.3]^{\\top}$\n- $\\gamma = 0.9$, $\\alpha_{\\pi} = 0.05$, $\\alpha_Q = 0.1$, $\\alpha_V = 0.1$\n- $\\phi(s_1) = [1, 0]^{\\top}$, $\\phi(s_2) = [0, 1]^{\\top}$\n\n**3. 分步计算**\n\n我们将按顺序处理批次中的三个转移，并累加每个参数的更新量。\n\n### 转移 1：$(s, a, r, s') = (s_1, a_1, 1.0, s_2)$\n\n- **计算中间值 ($s=s_1$)**:\n  - 策略偏好: $h(a_1|s_1) = 0.3$, $h(a_2|s_1) = -0.2$\n  - 策略概率: $\\pi(a_1|s_1) \\approx 0.6225$, $\\pi(a_2|s_1) \\approx 0.3775$\n  - 价值估计: $V(s_1) = 0.0$, $Q(s_1, a_1) = 0.15$\n  - 下一状态价值: $V(s_2) = 0.3$\n- **计算误差与优势**:\n  - TD 目标: $y_1 = 1.0 + 0.9 \\cdot 0.3 = 1.27$\n  - TD 误差: $\\delta_{Q,1} = 1.27 - 0.15 = 1.12$, $\\delta_{V,1} = 1.27 - 0.0 = 1.27$\n  - 优势: $A(s_1, a_1) = 0.15 - 0.0 = 0.15$\n- **计算更新量**:\n  - $\\Delta w_{Q,1} = 0.1 \\cdot 1.12 \\cdot [1, 0, 1, 0]^{\\top} = [0.112, 0, 0.112, 0]^{\\top}$\n  - $\\Delta w_{V,1} = 0.1 \\cdot 1.27 \\cdot [1, 0]^{\\top} = [0.127, 0]^{\\top}$\n  - $\\Delta \\theta_{a_1,1} = 0.05 \\cdot 0.15 \\cdot (1 - 0.6225) \\cdot [1, 0]^{\\top} \\approx [0.002831, 0]^{\\top}$\n  - $\\Delta \\theta_{a_2,1} = 0.05 \\cdot 0.15 \\cdot (-0.3775) \\cdot [1, 0]^{\\top} \\approx [-0.002831, 0]^{\\top}$\n\n### 转移 2：$(s, a, r, s') = (s_1, a_2, 0.0, s_1)$\n\n- **计算中间值 ($s=s_1$)**:\n  - 价值估计: $Q(s_1, a_2) = 0.0$, $V(s_1) = 0.0$\n  - 下一状态价值: $V(s_1) = 0.0$\n- **计算误差与优势**:\n  - TD 目标: $y_2 = 0.0 + 0.9 \\cdot 0.0 = 0.0$\n  - TD 误差: $\\delta_{Q,2} = 0.0 - 0.0 = 0.0$, $\\delta_{V,2} = 0.0 - 0.0 = 0.0$\n  - 优势: $A(s_1, a_2) = 0.0 - 0.0 = 0.0$\n- **计算更新量**: 由于所有误差和优势都为零，该转移的所有参数更新量均为零。\n\n### 转移 3：$(s, a, r, s') = (s_2, a_2, 2.0, \\text{terminal})$\n\n- **计算中间值 ($s=s_2$)**:\n  - 策略偏好: $h(a_1|s_2) = -0.1$, $h(a_2|s_2) = 0.25$\n  - 策略概率: $\\pi(a_1|s_2) \\approx 0.4134$, $\\pi(a_2|s_2) \\approx 0.5866$\n  - 价值估计: $V(s_2) = 0.3$, $Q(s_2, a_2) = 0.1$\n  - 下一状态价值: $V(\\text{terminal}) = 0$\n- **计算误差与优势**:\n  - TD 目标: $y_3 = 2.0 + 0.9 \\cdot 0 = 2.0$\n  - TD 误差: $\\delta_{Q,3} = 2.0 - 0.1 = 1.9$, $\\delta_{V,3} = 2.0 - 0.3 = 1.7$\n  - 优势: $A(s_2, a_2) = 0.1 - 0.3 = -0.2$\n- **计算更新量**:\n  - $\\Delta w_{Q,3} = 0.1 \\cdot 1.9 \\cdot [0, 1, 0, 1]^{\\top} = [0, 0.19, 0, 0.19]^{\\top}$\n  - $\\Delta w_{V,3} = 0.1 \\cdot 1.7 \\cdot [0, 1]^{\\top} = [0, 0.17]^{\\top}$\n  - $\\Delta \\theta_{a_1,3} = 0.05 \\cdot (-0.2) \\cdot (-0.4134) \\cdot [0, 1]^{\\top} \\approx [0, 0.004134]^{\\top}$\n  - $\\Delta \\theta_{a_2,3} = 0.05 \\cdot (-0.2) \\cdot (1 - 0.5866) \\cdot [0, 1]^{\\top} \\approx [0, -0.004134]^{\\top}$\n\n### 4. 总批量更新\n\n总更新量是三个转移的更新量之和：\n- $\\Delta \\theta_{a_1} = [0.002831, 0]^{\\top} + [0, 0.004134]^{\\top} = [0.002831, 0.004134]^{\\top}$\n- $\\Delta \\theta_{a_2} = [-0.002831, 0]^{\\top} + [0, -0.004134]^{\\top} = [-0.002831, -0.004134]^{\\top}$\n- $\\Delta w_Q = [0.112, 0, 0.112, 0]^{\\top} + [0, 0.19, 0, 0.19]^{\\top} = [0.112, 0.19, 0.112, 0.19]^{\\top}$\n- $\\Delta w_V = [0.127, 0]^{\\top} + [0, 0.17]^{\\top} = [0.127, 0.17]^{\\top}$\n\n### 5. 最终组合向量\n\n将更新向量连接并四舍五入到四位有效数字：\n- $\\Delta \\theta_{a_1}$: $[0.002831, 0.004134] \\rightarrow [0.002832, 0.004134]$\n- $\\Delta \\theta_{a_2}$: $[-0.002831, -0.004134] \\rightarrow [-0.002832, -0.004134]$\n- $\\Delta w_Q$: $[0.112, 0.19, 0.112, 0.19] \\rightarrow [0.1120, 0.1900, 0.1120, 0.1900]$\n- $\\Delta w_V$: $[0.127, 0.17] \\rightarrow [0.1270, 0.1700]$\n\n连接后的向量为：\n$[0.002832, 0.004134, -0.002832, -0.004134, 0.1120, 0.1900, 0.1120, 0.1900, 0.1270, 0.1700]$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.002832 & 0.004134 & -0.002832 & -0.004134 & 0.1120 & 0.1900 & 0.1120 & 0.1900 & 0.1270 & 0.1700\n\\end{pmatrix}\n}\n$$"
        }
    ]
}