{
    "hands_on_practices": [
        {
            "introduction": "Expected Utility Theory (EUT) provides a foundational normative framework for understanding decision-making under risk. A central idea in EUT is that an individual's attitude toward risk can be mathematically described by the shape of their utility function. This exercise offers a concrete application of this principle by using the Constant Relative Risk Aversion (CRRA) utility function, a workhorse model in economics and neuroscience, to determine the precise risk preference that makes a person indifferent between two different gambles . Working through this problem illuminates the direct link between the abstract theory of utility and the tangible prediction of choice behavior.",
            "id": "3970893",
            "problem": "A decision-maker faces a binary choice between two lotteries under the axioms of von Neumann–Morgenstern Expected Utility Theory (EUT). The lotteries have monetary payoffs and equal probability mass on each outcome: lottery $X$ yields $x \\in \\{0,10\\}$ with $p(x=10)=0.5$ and $p(x=0)=0.5$, and lottery $Y$ yields $y \\in \\{3,7\\}$ with $p(y=7)=0.5$ and $p(y=3)=0.5$. Preferences over monetary outcomes are represented by a twice continuously differentiable Bernoulli utility with constant relative risk aversion (CRRA), given by $u(x) = x^{1-\\rho}/(1-\\rho)$ for $x \\ge 0$ and $\\rho \\in \\mathbb{R}$, $\\rho \\ne 1$, extended by continuity at $\\rho=1$ to $u(x) = \\ln(x)$ for $x>0$. The domain restriction at $x=0$ must be handled via the appropriate limit under the CRRA parameterization.\n\nStarting from the definition of expected utility and the given CRRA functional form, derive the expected utility of each lottery as a function of the risk aversion parameter $\\rho$, carefully addressing the behavior at $x=0$. Then, compute the unique real value $\\rho^{\\star}$ for which the decision-maker is indifferent between $X$ and $Y$, i.e., at which the expected utilities of $X$ and $Y$ are equal. Report $\\rho^{\\star}$ as a single real number. If numerical evaluation were required, you would round to four significant figures, but provide an exact value if one exists.",
            "solution": "The problem asks for the specific value of the constant relative risk aversion (CRRA) parameter, denoted $\\rho^{\\star}$, at which a decision-maker is indifferent between two lotteries, $X$ and $Y$. Indifference occurs when the expected utilities of the two lotteries are equal, i.e., $E[u(X)] = E[u(Y)]$.\n\nThe axioms of von Neumann–Morgenstern Expected Utility Theory state that the expected utility of a lottery is the probability-weighted sum of the utilities of its outcomes. For a discrete lottery $L$ with outcomes $\\{o_i\\}$ and corresponding probabilities $\\{p_i\\}$, the expected utility is $E[u(L)] = \\sum_i p_i u(o_i)$.\n\nThe given lotteries are:\nLottery $X$: outcomes $\\{0, 10\\}$ with probabilities $p(x=0) = 0.5$ and $p(x=10) = 0.5$.\nLottery $Y$: outcomes $\\{3, 7\\}$ with probabilities $p(y=3) = 0.5$ and $p(y=7) = 0.5$.\n\nThe Bernoulli utility function is of the CRRA form:\n$u(z) = \\frac{z^{1-\\rho}}{1-\\rho}$ for $\\rho \\ne 1$.\n$u(z) = \\ln(z)$ for $\\rho = 1$.\n\nFirst, we derive the expected utility for each lottery as a function of $\\rho$.\n\nFor lottery $Y$:\nThe outcomes are $3$ and $7$, which are strictly positive. The utility for any outcome $y>0$ is well-defined for all $\\rho$.\nFor $\\rho \\ne 1$:\n$$E[u(Y)] = 0.5 \\cdot u(7) + 0.5 \\cdot u(3) = 0.5 \\left( \\frac{7^{1-\\rho}}{1-\\rho} + \\frac{3^{1-\\rho}}{1-\\rho} \\right) = \\frac{7^{1-\\rho} + 3^{1-\\rho}}{2(1-\\rho)}$$\nFor $\\rho=1$:\n$$E[u(Y)] = 0.5 \\cdot \\ln(7) + 0.5 \\cdot \\ln(3) = 0.5 \\ln(21)$$\nIn all cases, $E[u(Y)]$ is a finite value.\n\nFor lottery $X$:\nThe outcomes are $10$ and $0$. The utility of the outcome $0$ requires careful treatment, as specified in the problem statement, by taking the limit.\n$$E[u(X)] = 0.5 \\cdot u(10) + 0.5 \\cdot u(0)$$\nWe must analyze the value of $u(0)$ based on the parameter $\\rho$.\n\nCase 1: $\\rho > 1$.\nIn this case, the exponent $1-\\rho$ is negative. Let $1-\\rho = -\\alpha$ where $\\alpha > 0$.\nThe utility of an outcome $z>0$ is $u(z) = \\frac{z^{-\\alpha}}{-\\alpha}$.\nThe utility of the zero outcome is determined by the limit:\n$$u(0) = \\lim_{z \\to 0^+} \\frac{z^{-\\alpha}}{-\\alpha} = \\lim_{z \\to 0^+} -\\frac{1}{\\alpha z^{\\alpha}} = -\\infty$$\nTherefore, the expected utility of lottery $X$ is:\n$$E[u(X)] = 0.5 \\cdot u(10) + 0.5 \\cdot (-\\infty) = -\\infty$$\nSince $E[u(Y)]$ is finite, the indifference condition $E[u(X)] = E[u(Y)]$ cannot be met. Thus, no solution for $\\rho^{\\star}$ exists in the range $\\rho > 1$.\n\nCase 2: $\\rho = 1$.\nThe utility function is $u(z) = \\ln(z)$. The utility of the zero outcome is:\n$$u(0) = \\lim_{z \\to 0^+} \\ln(z) = -\\infty$$\nThe expected utility of lottery $X$ is:\n$$E[u(X)] = 0.5 \\cdot \\ln(10) + 0.5 \\cdot (-\\infty) = -\\infty$$\nAgain, $E[u(Y)]$ is finite, so the indifference condition cannot be met. No solution exists for $\\rho^{\\star} = 1$.\n\nCase 3: $\\rho < 1$.\nIn this case, the exponent $1-\\rho$ is positive.\nThe utility of the zero outcome is:\n$$u(0) = \\lim_{z \\to 0^+} \\frac{z^{1-\\rho}}{1-\\rho} = \\frac{0}{1-\\rho} = 0$$\nThe expected utility of lottery $X$ is well-defined and finite:\n$$E[u(X)] = 0.5 \\cdot u(10) + 0.5 \\cdot u(0) = 0.5 \\left( \\frac{10^{1-\\rho}}{1-\\rho} \\right) + 0.5 \\cdot 0 = \\frac{10^{1-\\rho}}{2(1-\\rho)}$$\nThe indifference condition $E[u(X)] = E[u(Y)]$ can now be investigated in this domain:\n$$\\frac{10^{1-\\rho}}{2(1-\\rho)} = \\frac{7^{1-\\rho} + 3^{1-\\rho}}{2(1-\\rho)}$$\nSince we are in the domain $\\rho < 1$, the denominator $2(1-\\rho)$ is non-zero, allowing us to multiply both sides by it:\n$$10^{1-\\rho} = 7^{1-\\rho} + 3^{1-\\rho}$$\nTo solve this equation for $\\rho$, let's define a new variable $k = 1-\\rho$. Since $\\rho < 1$, it follows that $k > 0$. The equation transforms to:\n$$10^k = 7^k + 3^k$$\nTo find $k$, we can divide the equation by $10^k$ (which is non-zero):\n$$1 = \\frac{7^k}{10^k} + \\frac{3^k}{10^k}$$\n$$1 = \\left(\\frac{7}{10}\\right)^k + \\left(\\frac{3}{10}\\right)^k$$\nLet's analyze the function $f(k) = (0.7)^k + (0.3)^k$ for $k > 0$. We seek the value of $k$ for which $f(k)=1$.\nThe function $f(k)$ is a sum of two positive, strictly decreasing exponential functions, because their bases ($0.7$ and $0.3$) are between $0$ and $1$. Therefore, $f(k)$ is a strictly monotonic decreasing function for $k>0$. This guarantees that if a solution exists, it is unique.\nWe can test a simple value for $k$. Let $k=1$:\n$$f(1) = (0.7)^1 + (0.3)^1 = 0.7 + 0.3 = 1$$\nWe have found that $k=1$ is the unique solution.\nFinally, we substitute back to find the value of $\\rho^{\\star}$:\n$$k = 1 - \\rho^{\\star}$$\n$$1 = 1 - \\rho^{\\star}$$\n$$\\rho^{\\star} = 0$$\nThis value lies in the domain $\\rho < 1$ where we established a potential solution could exist. For $\\rho=0$, the utility function is $u(x) = x$, which corresponds to risk neutrality. A risk-neutral individual compares expected monetary values.\n$E[X] = 0.5 \\cdot 10 + 0.5 \\cdot 0 = 5$.\n$E[Y] = 0.5 \\cdot 7 + 0.5 \\cdot 3 = 0.5 \\cdot 10 = 5$.\nSince $E[X]=E[Y]$, a risk-neutral agent is indeed indifferent, confirming our result. The unique real value is $\\rho^{\\star} = 0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Many real-world decisions are not one-shot events but part of a sequence of choices within a dynamic environment. The Markov Decision Process (MDP) is the standard mathematical formalism for modeling such sequential decision problems. A critical component in solving MDPs is determining the long-term \"value\" of being in different states, and this exercise focuses on a fundamental computational step in this process: policy evaluation . By performing a single update of the state-value function, you will directly apply the Bellman expectation equation, a cornerstone algorithm in reinforcement learning that serves as a model for how the brain might compute and represent the future consequences of actions.",
            "id": "3970847",
            "problem": "Consider a Markov Decision Process (MDP) used to model a neural decision system with three states $s_{A}$, $s_{B}$, and $s_{C}$, and two actions $a_{1}$ and $a_{2}$. A fixed stochastic policy $\\pi$ governs action selection as follows: at $s_{A}$, $\\pi(a_{1}\\,|\\,s_{A}) = \\frac{7}{10}$ and $\\pi(a_{2}\\,|\\,s_{A}) = \\frac{3}{10}$; at $s_{B}$, $\\pi(a_{1}\\,|\\,s_{B}) = 1$; at $s_{C}$, $\\pi(a_{2}\\,|\\,s_{C}) = 1$. The one-step dynamics are Markovian: the probability of the next state depends only on the current state and action, and the immediate reward is a function of $(s,a,s')$.\n\nThe state-transition probabilities $P(s' \\mid s, a)$ and immediate rewards $r(s,a,s')$ are given by:\n- From $s_{A}$:\n  - Under $a_{1}$: $P(s_{B}\\mid s_{A},a_{1}) = \\frac{3}{5}$ with $r(s_{A},a_{1},s_{B}) = \\frac{6}{5}$, and $P(s_{C}\\mid s_{A},a_{1}) = \\frac{2}{5}$ with $r(s_{A},a_{1},s_{C}) = -\\frac{1}{2}$.\n  - Under $a_{2}$: $P(s_{B}\\mid s_{A},a_{2}) = \\frac{1}{5}$ with $r(s_{A},a_{2},s_{B}) = \\frac{7}{10}$, and $P(s_{C}\\mid s_{A},a_{2}) = \\frac{4}{5}$ with $r(s_{A},a_{2},s_{C}) = 0$.\n- From $s_{B}$:\n  - Under $a_{1}$: $P(s_{A}\\mid s_{B},a_{1}) = \\frac{1}{2}$ with $r(s_{B},a_{1},s_{A}) = \\frac{3}{10}$, and $P(s_{C}\\mid s_{B},a_{1}) = \\frac{1}{2}$ with $r(s_{B},a_{1},s_{C}) = 2$.\n  - Under $a_{2}$: $P(s_{C}\\mid s_{B},a_{2}) = 1$ with $r(s_{B},a_{2},s_{C}) = \\frac{3}{2}$.\n- From $s_{C}$:\n  - Under $a_{1}$: $P(s_{C}\\mid s_{C},a_{1}) = 1$ with $r(s_{C},a_{1},s_{C}) = -\\frac{1}{5}$.\n  - Under $a_{2}$: $P(s_{A}\\mid s_{C},a_{2}) = \\frac{3}{10}$ with $r(s_{C},a_{2},s_{A}) = \\frac{2}{5}$, and $P(s_{B}\\mid s_{C},a_{2}) = \\frac{7}{10}$ with $r(s_{C},a_{2},s_{B}) = \\frac{9}{10}$.\n\nAssume a discounted return with discount factor $\\gamma = \\frac{9}{10}$. The critic’s initial value estimate (prior to policy evaluation) is $V^{(0)}(s_{A}) = \\frac{1}{2}$, $V^{(0)}(s_{B}) = -\\frac{1}{5}$, and $V^{(0)}(s_{C}) = 1$.\n\nPerform one synchronous policy evaluation update under the fixed policy $\\pi$ to obtain the updated value vector $V^{(1)}$ from $V^{(0)}$, where each component is the expected one-step return plus the discounted expected next value under the Markov property and policy $\\pi$. Express your final answer as exact rational numbers with no rounding, in the order $(s_{A}, s_{B}, s_{C})$, as a single row vector using the $\\texttt{pmatrix}$ environment. No units are required.",
            "solution": "The synchronous policy evaluation update for the value of a state $s$ at iteration $k+1$ is given by the Bellman expectation equation for a fixed policy $\\pi$:\n$$V^{(k+1)}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} P(s'|s,a) [r(s,a,s') + \\gamma V^{(k)}(s')]$$\nWe apply this equation for $k=0$ with the initial value estimates $V^{(0)}(s_{A}) = \\frac{1}{2}$, $V^{(0)}(s_{B}) = -\\frac{1}{5}$, and $V^{(0)}(s_{C}) = 1$ to find $V^{(1)}$ for each state.\n\nFor state $s_{A}$:\n$$V^{(1)}(s_{A}) = \\pi(a_{1}|s_{A}) \\left( \\sum_{s'} P(s'|s_{A},a_{1}) [r(s_{A},a_{1},s') + \\gamma V^{(0)}(s')] \\right) + \\pi(a_{2}|s_{A}) \\left( \\sum_{s'} P(s'|s_{A},a_{2}) [r(s_{A},a_{2},s') + \\gamma V^{(0)}(s')] \\right)$$\n$$V^{(1)}(s_{A}) = \\frac{7}{10} \\left( \\frac{3}{5}\\left[\\frac{6}{5} + \\frac{9}{10}\\left(-\\frac{1}{5}\\right)\\right] + \\frac{2}{5}\\left[-\\frac{1}{2} + \\frac{9}{10}(1)\\right] \\right) + \\frac{3}{10} \\left( \\frac{1}{5}\\left[\\frac{7}{10} + \\frac{9}{10}\\left(-\\frac{1}{5}\\right)\\right] + \\frac{4}{5}\\left[0 + \\frac{9}{10}(1)\\right] \\right)$$\n$$V^{(1)}(s_{A}) = \\frac{7}{10} \\left( \\frac{3}{5}\\left[\\frac{51}{50}\\right] + \\frac{2}{5}\\left[\\frac{4}{10}\\right] \\right) + \\frac{3}{10} \\left( \\frac{1}{5}\\left[\\frac{26}{50}\\right] + \\frac{4}{5}\\left[\\frac{9}{10}\\right] \\right)$$\n$$V^{(1)}(s_{A}) = \\frac{7}{10} \\left( \\frac{153}{250} + \\frac{40}{250} \\right) + \\frac{3}{10} \\left( \\frac{26}{250} + \\frac{180}{250} \\right) = \\frac{7}{10} \\left( \\frac{193}{250} \\right) + \\frac{3}{10} \\left( \\frac{206}{250} \\right)$$\n$$V^{(1)}(s_{A}) = \\frac{1351}{2500} + \\frac{618}{2500} = \\frac{1969}{2500}$$\n\nFor state $s_{B}$:\nThe policy is deterministic, $\\pi(a_{1}|s_{B}) = 1$.\n$$V^{(1)}(s_{B}) = P(s_{A}|s_{B},a_{1})[r(s_{B},a_{1},s_{A}) + \\gamma V^{(0)}(s_{A})] + P(s_{C}|s_{B},a_{1})[r(s_{B},a_{1},s_{C}) + \\gamma V^{(0)}(s_{C})]$$\n$$V^{(1)}(s_{B}) = \\frac{1}{2} \\left[ \\frac{3}{10} + \\frac{9}{10}\\left(\\frac{1}{2}\\right) \\right] + \\frac{1}{2} \\left[ 2 + \\frac{9}{10}(1) \\right]$$\n$$V^{(1)}(s_{B}) = \\frac{1}{2} \\left[ \\frac{15}{20} \\right] + \\frac{1}{2} \\left[ \\frac{29}{10} \\right] = \\frac{15}{40} + \\frac{58}{40} = \\frac{73}{40}$$\n\nFor state $s_{C}$:\nThe policy is deterministic, $\\pi(a_{2}|s_{C}) = 1$.\n$$V^{(1)}(s_{C}) = P(s_{A}|s_{C},a_{2})[r(s_{C},a_{2},s_{A}) + \\gamma V^{(0)}(s_{A})] + P(s_{B}|s_{C},a_{2})[r(s_{C},a_{2},s_{B}) + \\gamma V^{(0)}(s_{B})]$$\n$$V^{(1)}(s_{C}) = \\frac{3}{10} \\left[ \\frac{2}{5} + \\frac{9}{10}\\left(\\frac{1}{2}\\right) \\right] + \\frac{7}{10} \\left[ \\frac{9}{10} + \\frac{9}{10}\\left(-\\frac{1}{5}\\right) \\right]$$\n$$V^{(1)}(s_{C}) = \\frac{3}{10} \\left[ \\frac{17}{20} \\right] + \\frac{7}{10} \\left[ \\frac{36}{50} \\right] = \\frac{51}{200} + \\frac{252}{500}$$\n$$V^{(1)}(s_{C}) = \\frac{255}{1000} + \\frac{504}{1000} = \\frac{759}{1000}$$\n\nThe updated value vector is $V^{(1)} = (V^{(1)}(s_{A}), V^{(1)}(s_{B}), V^{(1)}(s_{C})) = \\left(\\frac{1969}{2500}, \\frac{73}{40}, \\frac{759}{1000}\\right)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1969}{2500} & \\frac{73}{40} & \\frac{759}{1000}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A fundamental challenge in learning and adaptive behavior is the exploration-exploitation dilemma: should an agent exploit an option known to be good, or explore other, less-certain options that might be even better? The Multi-Armed Bandit (MAB) problem is a classic paradigm for studying this trade-off. This hands-on practice invites you to apply the Upper Confidence Bound 1 (UCB1) algorithm, an elegant and provably efficient solution to the MAB problem . By calculating the UCB indices from a history of past rewards, you will gain insight into how a decision-making system can formally balance an option's past performance (exploitation) against its associated uncertainty (exploration) to guide learning.",
            "id": "3970900",
            "problem": "A subject engages in a three-armed stochastic decision task modeled as a Multi-Armed Bandit (MAB), where each arm $j \\in \\{1,2,3\\}$ yields independent rewards $r_{j,i} \\in [0,1]$ drawn from an unknown stationary distribution with mean $\\mu_j$. At each time $t$, the policy selects one arm and observes a single reward. Assume rewards are conditionally independent given the arm identity and bounded in the interval $[0,1]$. At time $t=20$, the historical interaction record consists of the following observed reward sequences for each arm:\n- Arm $1$ was pulled $8$ times with rewards: $0.9, 0.7, 0.6, 0.8, 0.7, 0.4, 0.9, 0.6$.\n- Arm $2$ was pulled $7$ times with rewards: $0.5, 0.8, 0.3, 0.6, 0.4, 0.7, 0.6$.\n- Arm $3$ was pulled $5$ times with rewards: $0.2, 0.4, 0.9, 0.6, 0.5$.\n\nUsing the principle of Upper Confidence Bound 1 (UCB1), derived from concentration of measure for bounded rewards via Hoeffding-type confidence intervals and the natural logarithm, compute the empirical means and the UCB indices for each arm at time $t=20$, and select the action that UCB1 would choose at $t=20$.\n\nReport only the index of the selected arm as a single integer. No rounding is required for the final answer. Use the natural logarithm for any logarithmic quantity in your derivation and computations in the solution.",
            "solution": "The Upper Confidence Bound 1 (UCB1) algorithm selects an arm at each time step by maximizing a UCB index. This index is the sum of the empirical mean reward (exploitation term) and a confidence bonus (exploration term).\n\nThe UCB1 index for an arm $j$ at time step $t$ is given by:\n$$\n\\text{UCB}_j(t) = Q_j(t) + \\sqrt{\\frac{2 \\ln t}{n_j(t)}}\n$$\nwhere:\n- $Q_j(t)$ is the empirical mean of the rewards from arm $j$ up to time $t$.\n- $n_j(t)$ is the number of times arm $j$ has been pulled up to time $t$.\n- $t$ is the total number of pulls across all arms.\n\nThe policy is to select the arm with the highest UCB index: $a_{t+1} = \\underset{j}{\\operatorname{argmax}} \\, \\text{UCB}_j(t)$.\n\nAt the given time $t=20$, we have:\n- Total trials: $t = 20$.\n- For Arm $1$: $n_1 = 8$.\n- For Arm $2$: $n_2 = 7$.\n- For Arm $3$: $n_3 = 5$.\n\nFirst, we compute the empirical mean reward $Q_j$ for each arm.\n\n**Arm 1:**\nThe sum of rewards is $0.9 + 0.7 + 0.6 + 0.8 + 0.7 + 0.4 + 0.9 + 0.6 = 5.6$.\nThe empirical mean is $Q_1 = \\frac{5.6}{8} = 0.7$.\n\n**Arm 2:**\nThe sum of rewards is $0.5 + 0.8 + 0.3 + 0.6 + 0.4 + 0.7 + 0.6 = 3.9$.\nThe empirical mean is $Q_2 = \\frac{3.9}{7} \\approx 0.5571$.\n\n**Arm 3:**\nThe sum of rewards is $0.2 + 0.4 + 0.9 + 0.6 + 0.5 = 2.6$.\nThe empirical mean is $Q_3 = \\frac{2.6}{5} = 0.52$.\n\nNext, we calculate the UCB1 index for each arm at $t=20$. We will use approximations for comparison, with $\\ln 20 \\approx 2.9957$.\n\n**UCB1 Index for Arm 1:**\n$$\n\\text{UCB}_1 = Q_1 + \\sqrt{\\frac{2 \\ln t}{n_1}} = 0.7 + \\sqrt{\\frac{2 \\ln 20}{8}} = 0.7 + \\sqrt{\\frac{\\ln 20}{4}} \\approx 0.7 + 0.8654 = 1.5654\n$$\n\n**UCB1 Index for Arm 2:**\n$$\n\\text{UCB}_2 = Q_2 + \\sqrt{\\frac{2 \\ln t}{n_2}} = \\frac{3.9}{7} + \\sqrt{\\frac{2 \\ln 20}{7}} \\approx 0.5571 + \\sqrt{0.8559} \\approx 0.5571 + 0.9252 = 1.4823\n$$\n\n**UCB1 Index for Arm 3:**\n$$\n\\text{UCB}_3 = Q_3 + \\sqrt{\\frac{2 \\ln t}{n_3}} = 0.52 + \\sqrt{\\frac{2 \\ln 20}{5}} \\approx 0.52 + \\sqrt{1.1983} \\approx 0.52 + 1.0947 = 1.6147\n$$\n\nComparing the computed UCB indices:\n$\\text{UCB}_3 \\approx 1.6147$\n$\\text{UCB}_1 \\approx 1.5654$\n$\\text{UCB}_2 \\approx 1.4823$\n\nThe UCB1 algorithm selects the arm with the maximum UCB index. In this case, Arm $3$ has the highest index. The decision is driven by the large exploration bonus for Arm $3$ due to its lower number of pulls ($n_3=5$), which outweighs its lower empirical mean reward compared to Arm $1$.\n\nTherefore, the UCB1 policy would select arm $3$.",
            "answer": "$$\\boxed{3}$$"
        }
    ]
}