## 引言
在日常生活中，从选择早餐到制定职业规划，我们无时无刻不在做出决策。这些心智过程看似简单，实则涉及大脑复杂的计算。计算决策模型为理解这些复杂过程背后的神经与计算原理提供了强大的数学框架，使我们能够量化、预测和解释行为。然而，抽象的理论模型、其在大脑中的生物学实现，以及在现实世界中的应用之间往往存在鸿沟。本文旨在系统性地弥合这一差距，为读者呈现一幅关于现代决策科学的完整图景。

本文将带领读者踏上一段从理论到实践的旅程，通过三个核心章节来全面解析计算决策模型。首先，在“原理与机制”一章中，我们将深入剖析驱动决策的数学基础和神经机制，从贝叶斯最优性原则到强化学习，再到模拟认知过程的动态模型。接着，在“应用与跨学科连接”一章中，我们将探索这些模型如何走出实验室，在认知科学、神经科学、经济学乃至临床医学和工程学等领域解决实际问题。最后，“动手实践”部分将提供具体的计算练习，帮助读者将理论知识转化为可操作的技能，从而巩固对核心概念的理解。通过本章的学习，我们将为后续深入探索决策的复杂性奠定坚实的基础。

## 原理与机制

本章深入探讨了驱动决策的计算原则和神经机制。在前一章介绍决策的广泛背景之后，我们现在将系统地剖析构成现代计算神经科学决策模型核心的数学框架和生物学假设。我们的研究将从决策的规范性理论基础开始，然后扩展到复杂的序贯和分层情境，最后研究模拟决策过程本身实时动态的认知模型。

### 规范性决策的基石

在深入研究决策过程的动态机制之前，我们必须首先建立一个规范性的基础：在给定目标和可用信息的情况下，一个理想的决策者应该如何做出选择？本节探讨了两个基本理论，它们共同为理性决策提供了数学上的依据。

#### [贝叶斯决策理论](@entry_id:909090)

[计算模型](@entry_id:637456)中的一个核心目标是理解大脑如何在不确定的情况下做出最优决策。[贝叶斯决策理论](@entry_id:909090)为这一问题提供了基本的规范性框架。该理论假设决策者旨在最小化其选择导致的预期“损失”或“成本”。

设想一个典型的知觉决策任务：决策者观察一个由某个潜在环境特征 $\theta \in \Theta$ 生成的带有噪声的刺激，例如屏幕上运动点的真实方向。这个观察结果在大脑中产生了一个神经响应 $x \in \mathcal{X}$。决策者的任务是根据这个神经响应选择一个行动 $a \in \mathcal{A}$，比如按下两个按钮中的一个。

这个场景中的关键要素包括：
1.  **先验分布 $p(\theta)$**：决策者对潜在特征的初始信念，在观察任何证据之前。
2.  **[似然函数](@entry_id:921601) $p(x \mid \theta)$**：在给定真实特征 $\theta$ 的情况下，观察到特定神经响应 $x$ 的概率。这描述了感觉过程中的噪声。
3.  **[损失函数](@entry_id:634569) $L(\theta, a)$**：当真实特征为 $\theta$ 而决策者选择行动 $a$ 时所产生的成本。例如，在二选一任务中，一个简单的“[0-1损失](@entry_id:173640)”函数可以是：如果行动正确则损失为0，如果行动错误则损失为1。
4.  **决策规则 $\delta(x)$**：一个将每个可能的观察 $x$ 映射到一个行动 $a$ 的函数或策略。

一个理想的决策者的目标是选择一个能最小化在所有可能情况（即所有潜在特征 $\theta$ 和所有观察 $x$）下平均损失的决策规则 $\delta$。这个总的平均损失被称为**[贝叶斯风险](@entry_id:178425) (Bayes risk)**，定义为 $R(\delta) = \mathbb{E}_{\theta,x}[L(\theta, \delta(x))]$。我们的目标是找到最优决策规则 $\delta^*$，使得 $R(\delta^*)$ 最小。

为了找到这个最优规则，我们可以将[贝叶斯风险](@entry_id:178425)的定义展开：
$$
R(\delta) = \int_{\mathcal{X}} \int_{\Theta} L(\theta, \delta(x)) \, p(\theta, x) \, d\theta \, dx
$$
利用概率论的基本规则 $p(\theta, x) = p(\theta \mid x) p(x)$，我们可以重写这个表达式：
$$
R(\delta) = \int_{\mathcal{X}} \left( \int_{\Theta} L(\theta, \delta(x)) \, p(\theta \mid x) \, d\theta \right) p(x) \, dx
$$
括号内的项具有关键意义。它是在观察到特定证据 $x$ 后，对特定行动 $a = \delta(x)$ 的期望损失。这个期望是根据**后验分布 $p(\theta \mid x)$** 计算的，该分布通过贝叶斯定理 $p(\theta \mid x) \propto p(x \mid \theta)p(\theta)$ 结合了[先验信念](@entry_id:264565)和感官证据。我们将这个量称为**后验期望损失 (posterior expected loss)**：
$$
\rho(x, a) = \mathbb{E}_{p(\theta \mid x)}[L(\theta, a)] = \int_{\Theta} L(\theta, a) \, p(\theta \mid x) \, d\theta
$$
[贝叶斯风险](@entry_id:178425)因此可以被看作是后验期望损失在所有可能观察 $x$ 上的平均值。为了最小化整个积分 $R(\delta)$，由于 $p(x) \ge 0$ 恒成立，我们只需为每一个 $x$ 独立地最小化被积函数 $\rho(x, \delta(x))$。

这导出了[贝叶斯决策理论](@entry_id:909090)的核心原则：**贝叶斯最优决策规则 $\delta^*(x)$ 是在每次观察到 $x$ 后，选择能使后验期望损失最小化的行动 $a$**。形式上：
$$
\delta^*(x) = \arg\min_{a \in \mathcal{A}} \int_{\Theta} L(\theta, a) \, p(\theta \mid x) \, d\theta
$$
这个原则是普适的，适用于任何形式的损失函数，无论是用于[分类任务](@entry_id:635433)的[0-1损失](@entry_id:173640)，还是用于估计任务的二次损失（其最优解是后验均值）或绝对损失（其最优解是[后验中位数](@entry_id:174652)）。通过在每个决策点上局部地最小化期望损失，决策者能够实现全局最优，即最小化总的[贝叶斯风险](@entry_id:178425) 。

#### [期望效用理论](@entry_id:140626)

[贝叶斯决策理论](@entry_id:909090)假设存在一个量化结果好坏的[损失函数](@entry_id:634569)。但在许多现实决策中，结果的价值是主观的。例如，赢得100美元的价值不一定恰好是赢得50美元价值的两倍。[期望效用理论](@entry_id:140626)为在风险（即结果的概率已知）下进行决策提供了规范性基础，并解释了为什么我们可以将主观价值（**效用 (utility)**）分配给结果，并通过最大化[期望效用](@entry_id:147484)做出选择。

考虑一个决策者在一系列“**彩票 (lotteries)**”之间进行选择。一个彩票是对一组可能结果 $X$ 的概率分布。例如，一个彩票可能是 $L_1$：50%的概率得到100美元，50%的概率一无所获；另一个彩票可能是 $L_2$：100%的概率得到40美元。决策者在这些彩票上的偏好关系用 $\succeq$ 表示（$L_1 \succeq L_2$ 意为 $L_1$ 至少和 $L_2$ 一样好）。

冯·诺依曼-摩根斯坦 (von Neumann-Morgenstern, VNM) 效用定理指出，如果一个决策者的偏好满足四个“理性”公理，那么其行为就等同于在为每个基本结果 $x \in X$ 分配一个数值效用 $u(x)$，并选择能使[期望效用](@entry_id:147484) $\mathbb{E}[u] = \sum_i p_i u(x_i)$ 最大化的彩票。这些公理是：

1.  **完备性 (Completeness)**：对于任意两个彩票 $L$ 和 $M$，决策者要么偏好 $L$ 胜过 $M$ ($L \succeq M$)，要么偏好 $M$ 胜过 $L$ ($M \succeq L$)，或者两者无差异。这意味着决策者总能做出比较。

2.  **[传递性](@entry_id:141148) (Transitivity)**：对于任意三个彩票 $L, M, N$，如果 $L \succeq M$ 且 $M \succeq N$，那么必有 $L \succeq N$。这确保了偏好的一致性。

3.  **连续性 (Continuity)**：如果存在三个彩票使得 $L \succ M \succ N$（$\succ$ 表示严格偏好），那么一定存在一个概率 $\alpha \in (0,1)$，使得决策者对 $M$ 和一个由 $L$ 与 $N$ 混合而成的彩票 $\alpha L + (1-\alpha) N$ 无差异。这排除了“无限好”或“无限差”的结果。

4.  **独立性 (Independence)**：对于任意三个彩票 $L, M, N$ 和任意概率 $\alpha \in (0,1)$，当且仅当将它们与第三个彩票 $N$ 以相同的概率混合后，偏好关系保持不变，即 $L \succeq M \iff \alpha L + (1-\alpha) N \succeq \alpha M + (1-\alpha) N$。这是最关键的公理，它保证了[效用函数](@entry_id:137807)在概率上是线性的。

如果这些公理成立，那么不仅存在一个[效用函数](@entry_id:137807) $u$ 来表示偏好，而且这个函数在某种意义上是“[基数](@entry_id:754020)”的：它是**在正[仿射变换](@entry_id:144885)下唯一的**。这意味着如果 $u$ 是一个有效的效用函数，那么任何形式为 $u'(x) = a u(x) + b$（其中 $a>0$）的函数 $u'$ 也将代表完全相同的偏好。这种唯一[性比](@entry_id:172643)标准的[序数](@entry_id:150084)[效用函数](@entry_id:137807)（仅在严格递增变换下唯一）要强得多，它使我们能够有意义地比较效用差异 。这个理论为[强化学习](@entry_id:141144)等领域中使用的“奖励”或“价值”概念提供了坚实的理论基础。

### [序贯决策](@entry_id:145234)

现实世界中的决策很少是孤立的一次性事件。相反，它们通常是一系列相互关联的选择，其中当前的行为会影响未来的状态和机会。本节将介绍用于建模这类[序贯决策问题](@entry_id:136955)的核心框架。

#### 马尔可夫决策过程

当决策环境具有**马尔可夫特性**时，即未来状态的概率分布只依赖于当前状态和当前行动，而与之前的历史无关，我们可以使用**马尔可夫决策过程 (Markov Decision Process, MDP)** 来对其进行形式化。一个（有限）MDP由一个五元组 $(\mathcal{S}, \mathcal{A}, P, r, \gamma)$ 定义：

*   $\mathcal{S}$：一个有限的状态集合。
*   $\mathcal{A}$：一个有限的行动集合。
*   $P(s' \mid s, a) = \mathbb{P}(S_{t+1}=s' \mid S_t=s, A_t=a)$：**状态转移概率函数**，指定在状态 $s$ 采取行动 $a$ 后，转移到状态 $s'$ 的概率。
*   $r(s, a) = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a]$：**[奖励函数](@entry_id:138436)**，指定在状态 $s$ 采取行动 $a$ 后预期的即时奖励。
*   $\gamma \in [0, 1)$：**折扣因子**，用于衡量未来奖励相对于即时奖励的重要性。$\gamma$ 越小，决策者越“短视”。

决策者的目标是找到一个策略 $\pi(a \mid s)$（即从状态到行动的映射），以最大化**期望[折扣](@entry_id:139170)回报 (expected discounted return)**，即从当前时刻开始的所有未来奖励的折扣总和。

为了评估一个策略的好坏，我们定义**状态价值函数 (state-value function)** $V^\pi(s)$，它表示从状态 $s$ 开始，并遵循策略 $\pi$ 时所能获得的期望折扣回报：
$$
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+1+k} \mid S_t=s \right]
$$
由于马尔可夫特性，[价值函数](@entry_id:144750)满足一个重要的递归关系，即**[贝尔曼方程](@entry_id:1121499) (Bellman equation)**。我们可以将上式分解为即时奖励和未来状态价值的折扣期望：
$$
V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \left( r(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s,a) V^\pi(s') \right)
$$
所有决策问题的最终目标是找到一个**[最优策略](@entry_id:138495) $\pi^*$**，它能在所有状态下都产生最大的期望回报。对应的最优状态[价值函数](@entry_id:144750)记为 $V^*(s) = \max_{\pi} V^\pi(s)$。对于 $V^*(s)$，我们可以写出**贝尔曼最优方程 (Bellman optimality equation)**：
$$
V^*(s) = \max_{a \in \mathcal{A}} \left\{ r(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s,a) V^*(s') \right\}
$$
这个方程表达了一个深刻的原则：一个状态的最优价值等于在该状态下采取最优行动后所能获得的期望回报。这个回报由两部分组成：即时奖励 $r(s,a)$，以及所有可能的下一状态 $s'$ 的最优价值 $V^*(s')$ 的[折扣](@entry_id:139170)期望之和。$\max_a$ 算子表明，最优策略是在每个状态下都贪婪地选择能使这个表达式最大化的行动 。这个方程是许多[强化学习](@entry_id:141144)算法（如[价值迭代](@entry_id:146512)和策略迭代）的理论基础。

#### [时间差分学习](@entry_id:138242)与神经机制

[贝尔曼方程](@entry_id:1121499)为计算已知环境模型（即 $P$ 和 $r$ 已知）中的最优策略提供了理论依据。然而，在现实中，动物和人类通常必须在不完全了解环境动态的情况下学习。**时间差分 (Temporal Difference, TD) 学习**正是一类能够让智能体直接从经验中学习价值函数的算法，而无需环境模型。

TD学习的核心思想是利用[贝尔曼方程](@entry_id:1121499)的递归结构，但使用观察到的样本来更新价值估计。假设我们有一个当前的状态价值估计 $V(s_t)$。在执行一个行动后，我们观察到即时奖励 $r_t$ 和下一个状态 $s_{t+1}$。我们可以用这个单步转换的结果构造一个更准确的价值目标，即**TD目标**：$r_t + \gamma V(s_{t+1})$。这个目标是“自举 (bootstrapped)”的，因为它部分基于真实的奖励样本 $r_t$，部分基于当前的价值估计 $V(s_{t+1})$。

当前估计 $V(s_t)$ 与这个新目标之间的差异，被称为**TD误差 (TD error)** 或**奖励预测误差 (Reward Prediction Error, RPE)**，记为 $\delta_t$：
$$
\delta_t = \left( r_t + \gamma V(s_{t+1}) \right) - V(s_t)
$$
这个 $\delta_t$ 信号意义重大：
*   如果 $\delta_t > 0$，意味着结果（即时奖励加上未来期望）比预期的要好。
*   如果 $\delta_t  0$，意味着结果比预期的要差。
*   如果 $\delta_t = 0$，意味着结果与预期完全相符。

TD学习算法使用这个误差信号来更新价值函数，例如，通过一个简单的更新规则：$V(s_t) \leftarrow V(s_t) + \alpha \delta_t$，其中 $\alpha$ 是[学习率](@entry_id:140210)。

这一简洁的算法与大脑中一个关键的神经调节系统——中脑多巴胺系统——有着惊人的对应关系，形成了计算神经科学中最有影响力的理论之一。根据**奖励预测误差假说**，中脑多巴胺能神经元（位于[腹侧被盖区](@entry_id:201316)VTA和[黑质](@entry_id:150587)致密部SNc）的短暂、强烈的（**phasic**）放电活动并不编码奖励本身，而是编码奖励预测误差 $\delta_t$。

这个理论的生物学机制如下：
1.  **信号编码**：当一个事件比预期的好时（正RPE），多巴胺神经元会爆发式放电。当事件比预期的差时（负RPE），它们的放电会暂时被抑制到基线水平以下。当事件如预期发生时，放电率不变。
2.  **学习场所**：这些多巴胺神经元广泛投射到[纹状体](@entry_id:920761)，这是基底神经节的主要输入核。皮层（代表当前状态 $s_t$ 的信息）和[多巴胺](@entry_id:149480)（代表误差信号 $\delta_t$）的输入在[纹状体](@entry_id:920761)中的[中型多棘神经元](@entry_id:904814)（MSNs）上汇合。
3.  **三因子学习规则**：纹状体突触的可塑性（即连接强度的改变）遵循一个“三因子”规则：突触权重的改变需要（1）突触前活动（来自皮层的状态信号），（2）突触后活动（MSN的兴奋），以及（3）神经调节信号（多巴胺）。前两个因子共同产生一个“**资格痕迹 (eligibility trace)**”，标记最近活跃的突触有资格被修改。
4.  **通路特异性可塑性**：[多巴胺](@entry_id:149480)信号的到来“门控”着这个可塑性过程，其效果取决于突触后MSN表达的[多巴胺受体](@entry_id:173643)类型。
    *   表达**D1受体**的MSNs属于促进运动的“**直接通路**”。正RPE（多巴胺释放）会增强（[长时程增强](@entry_id:139004), LTP）活跃的皮层-纹状体突触，使得未来在相同状态下更可能选择该行动。
    *   表达**[D2受体](@entry_id:910633)**的MSNs属于抑制运动的“**[间接通路](@entry_id:199521)**”。正RPE会削弱（长时程抑制, LTD）这些突触。负RPE则产生相反的效果（在D1通路上LTD，在D2通路上LTP）。

这个精巧的机制使得大脑能够通过TD学习，在“行动者-评论者 (actor-critic)”架构中调整其行为策略：评论者（如腹侧纹状体）学习评估状态的价值并计算RPE，而行动者（如背侧[纹状体](@entry_id:920761)）利用这个RPE信号来调整其策略（即皮层-纹状体连接权重），从而最大化未来的奖励 。

### [序贯决策](@entry_id:145234)的高级主题

基于MDP和TD学习的基础，我们可以进一步探讨在更复杂和现实的决策场景中出现的挑战，例如信息搜集、任务分解和状态不确定性。

#### [探索与利用的权衡](@entry_id:1124777)

在未知环境中学习时，智能体面临一个经典的困境：是应该“**利用 (exploit)**”当前已知的最佳行动来最大化即时回报，还是应该“**探索 (explore)**”其他看似次优的行动以获取更多信息，从而可能在未来做出更好的决策？这就是**[探索-利用权衡](@entry_id:1124776) (exploration-exploitation trade-off)**。

这个问题可以用**K臂老虎机 (K-armed bandit)** 问题来形式化。想象一个有 $K$ 个臂的老虎机，每个臂 $i$ 以一个未知的平均回报 $\mu_i$ 产生奖励。在总共 $T$ 轮试验中，我们的目标是最大化总回报，这等价于最小化**累积悔恨 (cumulative regret)**。悔恨是在一次试验中，选择一个次优臂所损失的潜在回报，即 $\Delta_{a_t} = \mu^* - \mu_{a_t}$，其中 $\mu^* = \max_i \mu_i$ 是最优臂的平均回报，$a_t$ 是在第 $t$ 轮选择的臂。总期望累积悔恨为 $R_T = \sum_{t=1}^T \mathbb{E}[\Delta_{a_t}]$。

我们可以将总期望悔恨分解为对每个次优臂的拉动次数的贡献：
$$
\mathbb{E}[R_T] = \sum_{i=1}^K \Delta_i \mathbb{E}[N_i(T)]
$$
其中 $N_i(T)$ 是在 $T$ 轮中臂 $i$ 被拉动的次数。这个公式清楚地表明，最小化悔恨就是要尽量减少拉动次优臂的次数，尤其是那些回报差距 $\Delta_i$ 很大的臂。

一个纯粹的利用策略（例如，总是选择当前样本均值最高的臂）可能会因为早期的随机性而“锁定”在一个次优臂上，导致 $\mathbb{E}[N_i(T)]$ 随 $T$ 线性增长，从而产生线性的总悔恨。而一个有效的探索策略则旨在以一种受控的方式拉动次优臂，以确保它们的均值被足够精确地估计。

在奖励分布是**次高斯 (sub-Gaussian)**（即尾部衰减得像高斯分布一样快或更快）的常见假设下，可以设计出悔恨增长速度远慢于线性的算法。一类成功的算法基于“**面对不确定性时的乐观主义 (optimism in the face of uncertainty)**”原则，例如**上置信界 (Upper Confidence Bound, UCB)** 算法。UCB算法在每个时间步选择的臂是使下面这个量最大化的臂：
$$
a_t = \arg\max_i \left( \hat{\mu}_i(t-1) + C_i(t-1) \right)
$$
其中 $\hat{\mu}_i(t-1)$ 是臂 $i$ 的当前样本均值，而 $C_i(t-1)$ 是一个“置信半径”，它随着臂 $i$ 被拉动次数的增加而减小（例如，与 $\sqrt{\log t / N_i(t-1)}$ 成正比）。这个置信项鼓励算法探索那些被拉动次数较少、因而其真实均值更不确定的臂。可以证明，这类算法能够实现总期望悔恨的**对数增长**，即 $\mathbb{E}[R_T] = O(\log T)$，这在理论上被认为是可实现的最佳性能 。

#### 时间抽象：分层[强化学习](@entry_id:141144)

人类和动物能够毫不费力地执行跨越长时间尺度的复杂任务，例如“泡一杯茶”。这种行为似乎不是由一系列独立的[肌肉收缩](@entry_id:153054)构成的，而是由更高层次的、有意义的子任务（如“烧水”、“拿杯子”、“放茶包”）组成的。**分层强化学习 (Hierarchical Reinforcement Learning, HRL)** 旨在通过引入时间抽象来为智能体赋予这种能力。

**选项 (Options) 框架**是HRL中最具影响力的思想之一。一个选项可以被看作是一个时间上扩展的行动，它有自己的内部策略，并在满足某个终止条件时结束。形式上，一个选项 $o$ 是一个三元组 $(\mathcal{I}_o, \pi_o, \beta_o)$：

*   $\mathcal{I}_o \subseteq \mathcal{S}$：**起始集**，即可以启动该选项的状态集合。
*   $\pi_o(a \mid s)$：**选项内部策略**，在选项执行期间，它决定在每个状态 $s$ 采取哪个**基本行动 (primitive action)** $a$。
*   $\beta_o(s)$：**终止条件**，即选项在进入状态 $s$ 后终止的概率。

在一个带有选项的MDP中，决策在两个层次上进行。一个高层策略 $\mu(o \mid s)$ 在某个决策状态 $s$ 选择一个选项 $o$。然后，该选项的内部策略 $\pi_o$ 接管控制权，执行一系列基本行动，直到选项根据其终止条件 $\beta_o$ 终止。假设选项执行了 $\tau$ 步，并最终停在状态 $s_\tau$。控制权随后返回给高层策略，它在 $s_\tau$ 处选择下一个选项。

在这种“**调用-返回 (call-and-return)**”的半[马尔可夫决策过程](@entry_id:140981) (SMDP) 中，我们可以为高层策略 $\mu$ 定义选项价值函数 $Q_\mu(s, o)$，表示在状态 $s$ 启动选项 $o$ 并此后遵循策略 $\mu$ 的期望折扣回报。其[贝尔曼方程](@entry_id:1121499)为：
$$
Q_\mu(s,o) = \mathbb{E}\! \left[ \sum_{t=0}^{\tau-1} \gamma^{t} r_{t+1} + \gamma^{\tau} V_\mu(s_\tau) \mid s_0=s, o \right]
$$
其中 $V_\mu(s) = \sum_{o'} \mu(o' \mid s) Q_\mu(s, o')$ 是高层策略的状态价值函数。这个方程表明，一个选项的价值等于在其执行期间（随机时长为 $\tau$）累积的[折扣](@entry_id:139170)奖励，加上在选项终止时所达到的状态 $s_\tau$ 的价值（被恰当地折扣了 $\gamma^\tau$）的[期望值](@entry_id:150961)。选项框架通过将决策分层，使得学习和规划可以在更长的时间尺度上进行，从而显著提高了处理复杂问题的效率 。

#### 部分可观测性下的决策

标准的MDP假设智能体在任何时候都能完全地、无歧义地知道自己所处的状态。然而，在现实世界中，情况往往并非如此。由于[传感器噪声](@entry_id:1131486)、环境遮挡或内在的模糊性，智能体通常只能获得关于其真实状态的**部分可观测 (partially observable)** 的信息。

这类问题可以用**部分可观测马尔可夫决策过程 (Partially Observable Markov Decision Process, [POMDP](@entry_id:637181))** 来建模。一个[POMDP](@entry_id:637181)在MDP的基础上增加了一个观测集合 $\mathcal{O}$ 和一个观测概率函数 $Z(o \mid s', a)$，它指定了在行动 $a$ 导致转移到新状态 $s'$ 后，接收到观测 $o$ 的概率。

由于真实状态 $s$ 是未知的，智能体必须维持一个**[信念状态](@entry_id:195111) (belief state)** $b$，它是对真实状态的概率分布。这个[信念状态](@entry_id:195111) $b(s) = P(s \mid \text{history})$ 总结了到目前为止的所有历史信息。每当智能体执行一个行动 $a$ 并接收到一个新的观测 $o$ 时，它就使用贝叶斯法则来更新其信念状态。

在这种[信念状态](@entry_id:195111)空间中，[POMDP](@entry_id:637181)可以被转化为一个（连续状态的）MDP，其“状态”就是信念 $b$。一个最优策略 $\pi^*(b)$ 将每个信念状态映射到一个行动。最优[价值函数](@entry_id:144750) $V^*(b)$ 定义在信念单纯形（所有可能信念状态的集合）上，它具有一个非常特殊的结构：对于有限步数的[POMDP](@entry_id:637181)，**最优[价值函数](@entry_id:144750)是[分段线性](@entry_id:201467)和凸的 (piecewise-linear and convex, PWLC)**。

这个性质可以通过[动态规划](@entry_id:141107)和归纳法来证明。一个PWLC函数可以表示为一组有限的线性函数（在信念空间中是超平面）的上包络：
$$
V_t(b) = \max_{j} \sum_{s \in \mathcal{S}} b(s) \alpha_j^t(s) = \max_{j} b^\top \alpha_j^t
$$
其中，向量 $\alpha_j^t$ 被称为**$\alpha$-向量**。从$t-1$步的价值函数 $V_{t-1}$（由一组$\alpha$-向量 $\Gamma_{t-1}$表示）到$t$步的[价值函数](@entry_id:144750) $V_t$ 的更新过程被称为**贝尔曼备份 (Bellman backup)**。这个备份操作会为每个行动 $a$ 和每个可能的观测到$\alpha$-向量的映射，生成一个新的$\alpha$-向量。推导表明，由于[信念更新](@entry_id:266192)中的贝叶斯分母项 $\Pr(o \mid b,a)$ 最终会抵消，线性度得以保持。

具体来说，一个为行动 $a$ 和从观测到$\alpha$向量的特定映射 $i(\cdot)$ 生成的新$\alpha$向量 $\alpha^{a, i(\cdot)}$ 具有以下形式：
$$
\alpha^{a,i(\cdot)} = r_a + \gamma \sum_{o \in \mathcal{O}} T^a Z^{a,o} \alpha_{i(o)}
$$
其中 $r_a$ 是奖励向量，$T^a$ 是转移矩阵，$Z^{a,o}$ 是对角观测矩阵 。尽管在计算上极具挑战性（$\alpha$-向量的数量会爆炸式增长），但价值函数的PWLC结构是所有精确[POMDP](@entry_id:637181)求解算法的核心。

### 决策过程的认知模型

前面的章节主要关注规范性模型——即一个理想的智能体应该如何决策。现在，我们将转向描述性模型，这些模型旨在捕捉和解释人类和动物在进行决策时实际的认知和神经过程。这些“**过程模型 (process models)**”的核心在于它们明确地模拟了证据随时间累积的动态过程。

#### [漂移扩散模型](@entry_id:194261)

在二选一强制选择任务 (2AFC) 中，最成功和最具影响力的过程模型之一是**[漂移扩散模型](@entry_id:194261) (Drift-Diffusion Model, DDM)**。DDM假设决策者通过一个单一的决策变量 $X_t$ 来累积支持两个选项的证据差异。这个决策变量的演化由以下随机微分方程描述：
$$
dX_t = \mu \, dt + \sigma \, dW_t
$$
其中：
*   $\mu$ 是**漂移率 (drift rate)**，代表[证据累积](@entry_id:926289)的平均速度和方向。它的绝对值反映了任务的难度（例如，刺激的清晰度），符号表示哪个选项更受证据支持。
*   $\sigma$ 是**扩散系数 (diffusion coefficient)**，代表证据过程中噪声的强度。
*   $dW_t$ 是标准[维纳过程](@entry_id:137696)的增量，代表瞬时的随机波动。

这个过程从一个起始点（通常在两个边界之间，例如$X_0 = 0$）开始，直到它首次到达两个吸收边界中的一个为止。如果它到达上边界 $+B$，则选择第一个选项；如果到达下边界 $-B$，则选择第二个选项。决策所花费的时间，即**首次通过时间 (first passage time)**，构成了反应时间 (RT) 的主要部分。总的反应时间还包括一个**非决策时间 $T_{nd}$**，用于解释[感觉编码](@entry_id:1131479)和运动执行等与决策过程本身无关的固定延迟。

DDM之所以强大，不仅因为它能很好地拟合行为数据（包括反应时间的完整分布），还因为它与一个规范性模型——**[序贯概率比检验](@entry_id:176474) (Sequential Probability Ratio Test, SPRT)**——有着深刻的联系。SPRT是一种在两个假设之间进行区分的统计上最优的方法（在给定错误率下，平均决策时间最短）。可以证明，DDM是SPRT在连续时间和连续证据下的一个实现，其中决策变量 $X_t$ 对应于累积的[对数似然比](@entry_id:274622)。

对于一个具有对称边界 $\pm B$ 且从 $X_0 = 0$ 开始的DDM，其行为的关键预测具有解析解。选择上边界（假设为正确选择）的概率为：
$$
P_{\text{upper}} = \frac{1}{1 + \exp\left(-\frac{2\mu B}{\sigma^2}\right)} = \frac{1}{2}\left[1 + \tanh\left(\frac{\mu B}{\sigma^2}\right)\right]
$$
平均决策时间（不包括非决策时间）为：
$$
\mathbb{E}[T] = \frac{B}{\mu} \tanh\left(\frac{\mu B}{\sigma^2}\right) \quad (\text{对于 } \mu \neq 0)
$$
当漂移率趋近于零时（即任务极度困难），平均决策时间趋近于 $\mathbb{E}[T] \to B^2/\sigma^2$ 。DDM中的**速度-准确率权衡 (speed-accuracy trade-off)** 主要通过调整边界距离 $2B$ 来实现：更宽的边界需要更多的证据，导致更慢但更准确的决策；反之亦然。

#### 竞争性架构：[漂移扩散模型](@entry_id:194261)与竞争累积模型

尽管DDM非常成功，但它并非描述[证据累积](@entry_id:926289)过程的唯一架构。一个重要的替代理论是**竞争累积模型 (race models)**，有时也称为独立累积模型。与DDM中单一决策变量累积证据差异不同，竞争累积模型假设存在多个（在2AFC任务中为两个）独立的累积器，每个累积器只累积支持其对应选项的证据。

例如，一个双累积器竞争模型可以形式化为两个独立的[随机过程](@entry_id:268487) $X_1(t)$ 和 $X_2(t)$：
$$
dX_i(t) = \mu_i \, dt + \sigma_i \, dW^{(i)}_t, \quad \text{for } i \in \{1,2\}
$$
其中 $W^{(1)}_t$ 和 $W^{(2)}_t$ 是独立的[维纳过程](@entry_id:137696)。两个累积器都从0开始，向同一个阈值 $b$ “竞赛”。首先到达阈值的累积器决定最终的选择。

这两种模型架构在概念上不同，并导致了一些可以被经验验证的不同预测：

1.  **错误反应的速度**：这是一个经典的区分点。在DDM中，由于漂移率 $\mu$ 指向正确的边界，一个错误的选择意味着决策变量必须在噪声的驱动下“[逆流](@entry_id:201298)而上”到达错误的边界。这通常需要更长的时间。因此，**DDM预测错误反应平均比正确反应更慢**。相比之下，在[竞争模型](@entry_id:1122715)中，假设 $\mu_1  \mu_2$，一个错误（即累积器2获胜）的发生更有可能是在试验早期，由于一个较大的随机波动使较弱的累积器2意外地快速到达阈值。因此，**[竞争模型](@entry_id:1122715)预测错误反应平均比正确反应更快**。

2.  **反应时间分布的形状**：由于两个累积器是独立的，总决策时间 $T = \min\{T_1, T_2\}$ 的**[风险率](@entry_id:266388) (hazard function)** $h_T(t)$（即在时间 $t$ 尚未做出决策的情况下，在下一瞬间做出决策的瞬时概率）等于两个独立过程风险率的总和：$h_T(t) = h_{T_1}(t) + h_{T_2}(t)$。这种“统计促进”效应意味着[竞争模型](@entry_id:1122715)通常会产生比DDM更早出现峰值且[右偏](@entry_id:180351)程度更小的RT分布（即慢反应的尾部更薄）。

值得注意的是，尽管两个独立[漂移扩散](@entry_id:160427)过程的差值 $X_1(t) - X_2(t)$ 本身也是一个[漂移扩散](@entry_id:160427)过程，但这并不意味着[竞争模型](@entry_id:1122715)等同于DDM。其根本原因在于它们的决策规则（即边界条件）不同。[竞争模型](@entry_id:1122715)的决策边界是在 $(X_1, X_2)$ [状态空间](@entry_id:160914)中的两条正交线（$X_1=b$ 或 $X_2=b$），而DDM的边界是两条对角线（$X_1 - X_2 = \pm B$）。这种几何上的差异导致了它们不同的行为预测 。

#### 元认知建模：决策信心

除了做出选择本身，我们还对自己的决策有**信心 (confidence)** 的感觉。[计算模型](@entry_id:637456)为理解这种元认知判断的形成机制提供了可能。一个有影响力的理论将决策信心与选择正确的[后验概率](@entry_id:153467)联系起来。

我们可以在DDM的框架内推导出**贝叶斯决策信心**的表达式。假设两个假设 $H_+$ 和 $H_-$ 分别对应于漂移率 $+\mu$ 和 $-\mu$。决策者在做出选择 $c \in \{+1, -1\}$（对应于到达边界 $\pm A$）后，其信心可以定义为所选假设为真的后验概率 $P(H_c \mid \mathcal{D})$，其中 $\mathcal{D}$ 是观察到的决策变量的完整路径。

利用贝叶斯定理，这个[后验概率](@entry_id:153467)可以表示为对数后验优势的形式：
$$
\ln \frac{P(H_c \mid \mathcal{D})}{P(H_{-c} \mid \mathcal{D})} = \ln \frac{P(\mathcal{D} \mid H_c)}{P(\mathcal{D} \mid H_{-c})} + \ln \frac{P(H_c)}{P(H_{-c})}
$$
右边的第一项是**[对数似然比](@entry_id:274622)**，代表数据提供的证据。对于[漂移扩散](@entry_id:160427)过程，利用[吉尔萨诺夫定理](@entry_id:147068) (Girsanov's theorem) 可以证明，这个[对数似然比](@entry_id:274622)恰好等于 $\frac{2\mu X_\tau}{\sigma^2}$，其中 $X_\tau = cA$ 是决策变量在边界处的值。代入后得到 $\frac{2\mu A}{\sigma^2}$。值得注意的是，这个证据项不依赖于决策时间 $\tau$。第二项是**对数先验优势**，它捕捉了任何初始的偏好。如果先验概率为 $P(H_+) = \pi$，那么这项可以写为 $c \ln(\frac{\pi}{1-\pi})$。

将这两项结合，我们得到决策信心 $C = P(H_c \mid \mathcal{D})$ 的表达式：
$$
C = \frac{1}{1 + \exp\left( - \left( \frac{2\mu A}{\sigma^2} + c \ln\left(\frac{\pi}{1-\pi}\right) \right) \right)}
$$
这个公式揭示了几个关于信心的深刻见解：
*   信心随着证据质量（由 $\mu/\sigma^2$ 反映）和决策阈值 $A$ 的增加而增加。
*   信心受到先验偏见 $\pi$ 的影响。
*   一个惊人的预测是，在给定选择和所有模型参数的情况下，**贝叶斯信心不依赖于反应时间**。这与人们的直觉（更快的决策感觉上更自信）相悖，并激发了大量后续的理论和实验研究，探讨了直觉与这个规范性结果之间的差异 。这展示了[计算模型](@entry_id:637456)如何为我们理解心智的更高层次方面提供精确且可检验的假设。