{
    "hands_on_practices": [
        {
            "introduction": "在智能体学习如何改进其行为之前，我们首先需要一种方法来量化其当前策略的“优劣”。本练习介绍在马尔可夫决策过程（Markov Decision Process, MDP）中使用贝尔曼期望方程进行策略评估，这是计算在给定策略下某一状态长期价值的基础工具。这个计算过程构成了强化学习中“评价者”（critic）功能的核心。",
            "id": "3970847",
            "problem": "考虑一个用于模拟神经决策系统的马尔可夫决策过程 (MDP)，该系统具有三个状态 $s_{A}$、$s_{B}$ 和 $s_{C}$，以及两个动作 $a_{1}$ 和 $a_{2}$。一个固定的随机策略 $\\pi$ 控制动作选择如下：在 $s_{A}$ 处，$\\pi(a_{1}\\,|\\,s_{A}) = \\frac{7}{10}$ 且 $\\pi(a_{2}\\,|\\,s_{A}) = \\frac{3}{10}$；在 $s_{B}$ 处，$\\pi(a_{1}\\,|\\,s_{B}) = 1$；在 $s_{C}$ 处，$\\pi(a_{2}\\,|\\,s_{C}) = 1$。单步动态是马尔可夫的：下一个状态的概率仅取决于当前状态和动作，而即时奖励是 $(s,a,s')$ 的函数。\n\n状态转移概率 $P(s' \\mid s, a)$ 和即时奖励 $r(s,a,s')$ 由以下给出：\n- 从 $s_{A}$ 出发：\n  - 在动作 $a_{1}$ 下：$P(s_{B}\\mid s_{A},a_{1}) = \\frac{3}{5}$，奖励为 $r(s_{A},a_{1},s_{B}) = \\frac{6}{5}$；$P(s_{C}\\mid s_{A},a_{1}) = \\frac{2}{5}$，奖励为 $r(s_{A},a_{1},s_{C}) = -\\frac{1}{2}$。\n  - 在动作 $a_{2}$ 下：$P(s_{B}\\mid s_{A},a_{2}) = \\frac{1}{5}$，奖励为 $r(s_{A},a_{2},s_{B}) = \\frac{7}{10}$；$P(s_{C}\\mid s_{A},a_{2}) = \\frac{4}{5}$，奖励为 $r(s_{A},a_{2},s_{C}) = 0$。\n- 从 $s_{B}$ 出发：\n  - 在动作 $a_{1}$ 下：$P(s_{A}\\mid s_{B},a_{1}) = \\frac{1}{2}$，奖励为 $r(s_{B},a_{1},s_{A}) = \\frac{3}{10}$；$P(s_{C}\\mid s_{B},a_{1}) = \\frac{1}{2}$，奖励为 $r(s_{B},a_{1},s_{C}) = 2$。\n  - 在动作 $a_{2}$ 下：$P(s_{C}\\mid s_{B},a_{2}) = 1$，奖励为 $r(s_{B},a_{2},s_{C}) = \\frac{3}{2}$。\n- 从 $s_{C}$ 出发：\n  - 在动作 $a_{1}$ 下：$P(s_{C}\\mid s_{C},a_{1}) = 1$，奖励为 $r(s_{C},a_{1},s_{C}) = -\\frac{1}{5}$。\n  - 在动作 $a_{2}$ 下：$P(s_{A}\\mid s_{C},a_{2}) = \\frac{3}{10}$，奖励为 $r(s_{C},a_{2},s_{A}) = \\frac{2}{5}$；$P(s_{B}\\mid s_{C},a_{2}) = \\frac{7}{10}$，奖励为 $r(s_{C},a_{2},s_{B}) = \\frac{9}{10}$。\n\n假设折扣回报的折扣因子为 $\\gamma = \\frac{9}{10}$。评价器的初始价值估计（在策略评估之前）为 $V^{(0)}(s_{A}) = \\frac{1}{2}$、$V^{(0)}(s_{B}) = -\\frac{1}{5}$ 和 $V^{(0)}(s_{C}) = 1$。\n\n在固定策略 $\\pi$ 下执行一次同步策略评估更新，以从 $V^{(0)}$ 获得更新后的价值向量 $V^{(1)}$，其中每个分量是在马尔可夫性质和策略 $\\pi$ 下的期望单步回报加上折扣后的期望下一价值。请将您的最终答案表示为精确的有理数，不要四舍五入，并按 $(s_{A}, s_{B}, s_{C})$ 的顺序，使用 $\\texttt{pmatrix}$ 环境表示为单个行向量。无需单位。",
            "solution": "用户要求对给定的马尔可夫决策过程 (MDP) 执行一次同步策略评估更新。\n\n问题陈述的验证如下：\n- **已知条件提取**：\n  - 状态：$S = \\{s_{A}, s_{B}, s_{C}\\}$\n  - 动作：$A = \\{a_{1}, a_{2}\\}$\n  - 策略 $\\pi$：$\\pi(a_{1}\\,|\\,s_{A}) = \\frac{7}{10}$，$\\pi(a_{2}\\,|\\,s_{A}) = \\frac{3}{10}$；$\\pi(a_{1}\\,|\\,s_{B}) = 1$；$\\pi(a_{2}\\,|\\,s_{C}) = 1$。\n  - 转移概率 $P(s' \\mid s, a)$ 和奖励 $r(s,a,s')$：\n    - 从 $s_{A}$，$a_{1}$：$P(s_{B}\\mid s_{A},a_{1}) = \\frac{3}{5}$，$r(s_{A},a_{1},s_{B}) = \\frac{6}{5}$；$P(s_{C}\\mid s_{A},a_{1}) = \\frac{2}{5}$，$r(s_{A},a_{1},s_{C}) = -\\frac{1}{2}$。\n    - 从 $s_{A}$，$a_{2}$：$P(s_{B}\\mid s_{A},a_{2}) = \\frac{1}{5}$，$r(s_{A},a_{2},s_{B}) = \\frac{7}{10}$；$P(s_{C}\\mid s_{A},a_{2}) = \\frac{4}{5}$，$r(s_{A},a_{2},s_{C}) = 0$。\n    - 从 $s_{B}$，$a_{1}$：$P(s_{A}\\mid s_{B},a_{1}) = \\frac{1}{2}$，$r(s_{B},a_{1},s_{A}) = \\frac{3}{10}$；$P(s_{C}\\mid s_{B},a_{1}) = \\frac{1}{2}$，$r(s_{B},a_{1},s_{C}) = 2$。\n    - 从 $s_{B}$，$a_{2}$：$P(s_{C}\\mid s_{B},a_{2}) = 1$，$r(s_{B},a_{2},s_{C}) = \\frac{3}{2}$。\n    - 从 $s_{C}$，$a_{1}$：$P(s_{C}\\mid s_{C},a_{1}) = 1$，$r(s_{C},a_{1},s_{C}) = -\\frac{1}{5}$。\n    - 从 $s_{C}$，$a_{2}$：$P(s_{A}\\mid s_{C},a_{2}) = \\frac{3}{10}$，$r(s_{C},a_{2},s_{A}) = \\frac{2}{5}$；$P(s_{B}\\mid s_{C},a_{2}) = \\frac{7}{10}$，$r(s_{C},a_{2},s_{B}) = \\frac{9}{10}$。\n  - 折扣因子：$\\gamma = \\frac{9}{10}$。\n  - 初始价值函数：$V^{(0)}(s_{A}) = \\frac{1}{2}$，$V^{(0)}(s_{B}) = -\\frac{1}{5}$，$V^{(0)}(s_{C}) = 1$。\n\n- **验证检查**：\n  - 该问题具有科学依据，因为它使用了 MDP 的标准框架，这是计算神经科学和强化学习的基石。\n  - 该问题是适定的，提供了通过策略评估算法获得唯一解所需的所有信息。\n  - 语言是客观和定量的。\n  - 内部一致性检查确认，对于每个状态-动作对 $(s,a)$，转移概率之和 $\\sum_{s'} P(s'|s,a)$ 等于 $1$，并且对于每个状态 $s$，策略概率之和 $\\sum_{a} \\pi(a|s)$ 也等于 $1$。\n  - 根据验证清单，未发现任何缺陷。\n\n- **结论**：该问题有效。\n\n在迭代 $k+1$ 时，状态 $s$ 价值的同步策略评估更新由固定策略 $\\pi$ 的贝尔曼期望方程给出：\n$$V^{(k+1)}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} P(s'|s,a) [r(s,a,s') + \\gamma V^{(k)}(s')]$$\n我们将对 $k=0$ 应用此方程，以求得每个状态的 $V^{(1)}$。\n\n对于状态 $s_{A}$：\n$$V^{(1)}(s_{A}) = \\pi(a_{1}|s_{A}) \\left( \\sum_{s'} P(s'|s_{A},a_{1}) [r(s_{A},a_{1},s') + \\gamma V^{(0)}(s')] \\right) + \\pi(a_{2}|s_{A}) \\left( \\sum_{s'} P(s'|s_{A},a_{2}) [r(s_{A},a_{2},s') + \\gamma V^{(0)}(s')] \\right)$$\n$$V^{(1)}(s_{A}) = \\frac{7}{10} \\left( \\frac{3}{5}\\left[\\frac{6}{5} + \\frac{9}{10}\\left(-\\frac{1}{5}\\right)\\right] + \\frac{2}{5}\\left[-\\frac{1}{2} + \\frac{9}{10}(1)\\right] \\right) + \\frac{3}{10} \\left( \\frac{1}{5}\\left[\\frac{7}{10} + \\frac{9}{10}\\left(-\\frac{1}{5}\\right)\\right] + \\frac{4}{5}\\left[0 + \\frac{9}{10}(1)\\right] \\right)$$\n$$V^{(1)}(s_{A}) = \\frac{7}{10} \\left( \\frac{3}{5}\\left[\\frac{6}{5} - \\frac{9}{50}\\right] + \\frac{2}{5}\\left[-\\frac{5}{10} + \\frac{9}{10}\\right] \\right) + \\frac{3}{10} \\left( \\frac{1}{5}\\left[\\frac{7}{10} - \\frac{9}{50}\\right] + \\frac{4}{5}\\left[\\frac{9}{10}\\right] \\right)$$\n$$V^{(1)}(s_{A}) = \\frac{7}{10} \\left( \\frac{3}{5}\\left[\\frac{60-9}{50}\\right] + \\frac{2}{5}\\left[\\frac{4}{10}\\right] \\right) + \\frac{3}{10} \\left( \\frac{1}{5}\\left[\\frac{35-9}{50}\\right] + \\frac{36}{50} \\right)$$\n$$V^{(1)}(s_{A}) = \\frac{7}{10} \\left( \\frac{3}{5}\\left[\\frac{51}{50}\\right] + \\frac{8}{50} \\right) + \\frac{3}{10} \\left( \\frac{1}{5}\\left[\\frac{26}{50}\\right] + \\frac{36}{50} \\right)$$\n$$V^{(1)}(s_{A}) = \\frac{7}{10} \\left( \\frac{153}{250} + \\frac{40}{250} \\right) + \\frac{3}{10} \\left( \\frac{26}{250} + \\frac{180}{250} \\right)$$\n$$V^{(1)}(s_{A}) = \\frac{7}{10} \\left( \\frac{193}{250} \\right) + \\frac{3}{10} \\left( \\frac{206}{250} \\right) = \\frac{1351}{2500} + \\frac{618}{2500} = \\frac{1969}{2500}$$\n\n对于状态 $s_{B}$：\n该策略是确定性的，$\\pi(a_{1}|s_{B}) = 1$。\n$$V^{(1)}(s_{B}) = P(s_{A}|s_{B},a_{1})[r(s_{B},a_{1},s_{A}) + \\gamma V^{(0)}(s_{A})] + P(s_{C}|s_{B},a_{1})[r(s_{B},a_{1},s_{C}) + \\gamma V^{(0)}(s_{C})]$$\n$$V^{(1)}(s_{B}) = \\frac{1}{2} \\left[ \\frac{3}{10} + \\frac{9}{10}\\left(\\frac{1}{2}\\right) \\right] + \\frac{1}{2} \\left[ 2 + \\frac{9}{10}(1) \\right]$$\n$$V^{(1)}(s_{B}) = \\frac{1}{2} \\left[ \\frac{3}{10} + \\frac{9}{20} \\right] + \\frac{1}{2} \\left[ \\frac{20}{10} + \\frac{9}{10} \\right]$$\n$$V^{(1)}(s_{B}) = \\frac{1}{2} \\left[ \\frac{6}{20} + \\frac{9}{20} \\right] + \\frac{1}{2} \\left[ \\frac{29}{10} \\right] = \\frac{1}{2} \\left( \\frac{15}{20} \\right) + \\frac{29}{20}$$\n$$V^{(1)}(s_{B}) = \\frac{15}{40} + \\frac{58}{40} = \\frac{73}{40}$$\n\n对于状态 $s_{C}$：\n该策略是确定性的，$\\pi(a_{2}|s_{C}) = 1$。\n$$V^{(1)}(s_{C}) = P(s_{A}|s_{C},a_{2})[r(s_{C},a_{2},s_{A}) + \\gamma V^{(0)}(s_{A})] + P(s_{B}|s_{C},a_{2})[r(s_{C},a_{2},s_{B}) + \\gamma V^{(0)}(s_{B})]$$\n$$V^{(1)}(s_{C}) = \\frac{3}{10} \\left[ \\frac{2}{5} + \\frac{9}{10}\\left(\\frac{1}{2}\\right) \\right] + \\frac{7}{10} \\left[ \\frac{9}{10} + \\frac{9}{10}\\left(-\\frac{1}{5}\\right) \\right]$$\n$$V^{(1)}(s_{C}) = \\frac{3}{10} \\left[ \\frac{2}{5} + \\frac{9}{20} \\right] + \\frac{7}{10} \\left[ \\frac{9}{10} - \\frac{9}{50} \\right]$$\n$$V^{(1)}(s_{C}) = \\frac{3}{10} \\left[ \\frac{8}{20} + \\frac{9}{20} \\right] + \\frac{7}{10} \\left[ \\frac{45}{50} - \\frac{9}{50} \\right]$$\n$$V^{(1)}(s_{C}) = \\frac{3}{10} \\left( \\frac{17}{20} \\right) + \\frac{7}{10} \\left( \\frac{36}{50} \\right) = \\frac{51}{200} + \\frac{252}{500}$$\n$$V^{(1)}(s_{C}) = \\frac{51 \\times 5}{1000} + \\frac{252 \\times 2}{1000} = \\frac{255}{1000} + \\frac{504}{1000} = \\frac{759}{1000}$$\n\n更新后的价值向量为 $V^{(1)} = (V^{(1)}(s_{A}), V^{(1)}(s_{B}), V^{(1)}(s_{C}))$。\n其分量为 $V^{(1)}(s_{A}) = \\frac{1969}{2500}$，$V^{(1)}(s_{B}) = \\frac{73}{40}$ 和 $V^{(1)}(s_{C}) = \\frac{759}{1000}$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1969}{2500}  \\frac{73}{40}  \\frac{759}{1000}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "从已知环境（如前一个练习中的MDP）过渡到未知世界，决策者面临一个核心挑战：探索-利用困境（exploration-exploitation dilemma）。本练习将演示上置信界（Upper Confidence Bound, UCB1）算法，这是一种在多臂老虎机问题中管理这一权衡的有效方法。你将亲手计算智能体如何利用过去经验来选择下一个行动，从而在利用已知的高回报选项和探索知之甚少的选项之间取得平衡。",
            "id": "3970900",
            "problem": "一个主体参与一个三臂随机决策任务，该任务被建模为多臂老虎机（MAB），其中每个臂 $j \\in \\{1,2,3\\}$ 产生独立的奖励 $r_{j,i} \\in [0,1]$，这些奖励从均值为 $\\mu_j$ 的未知平稳分布中抽取。在每个时间点 $t$，策略选择一个臂并观察单个奖励。假设在给定臂的标识下，奖励是条件独立的，并且界于区间 $[0,1]$ 内。在时间 $t=20$ 时，历史交互记录包含每个臂的以下观测奖励序列：\n- 臂 $1$ 被拉动 $8$ 次，奖励为：$0.9, 0.7, 0.6, 0.8, 0.7, 0.4, 0.9, 0.6$。\n- 臂 $2$ 被拉动 $7$ 次，奖励为：$0.5, 0.8, 0.3, 0.6, 0.4, 0.7, 0.6$。\n- 臂 $3$ 被拉动 $5$ 次，奖励为：$0.2, 0.4, 0.9, 0.6, 0.5$。\n\n使用上置信界1（UCB1）原理，该原理通过霍夫丁型置信区间和自然对数从有界奖励的测度集中导出，计算在时间 $t=20$ 时每个臂的经验均值和UCB指数，并选择UCB1在 $t=20$ 时会选择的动作。\n\n仅报告所选臂的索引，以单个整数形式。最终答案无需四舍五入。在您的推导和解题计算中，对任何对数量都使用自然对数。",
            "solution": "问题陈述经过严格验证如下。\n\n**步骤1：提取已知信息**\n- 任务：三臂随机多臂老虎机（MAB）。\n- 臂：$j \\in \\{1, 2, 3\\}$。\n- 奖励：$r_{j,i} \\in [0, 1]$，从均值为 $\\mu_j$ 的未知平稳分布中抽取。\n- 时间步：决策将在时间 $t=20$ 进行，意味着总共已经进行了 $20$ 次拉动。\n- 历史数据：\n  - 臂 $1$：拉动 $n_1 = 8$ 次，奖励为：$0.9, 0.7, 0.6, 0.8, 0.7, 0.4, 0.9, 0.6$。\n  - 臂 $2$：拉动 $n_2 = 7$ 次，奖励为：$0.5, 0.8, 0.3, 0.6, 0.4, 0.7, 0.6$。\n  - 臂 $3$：拉动 $n_3 = 5$ 次，奖励为：$0.2, 0.4, 0.9, 0.6, 0.5$。\n- 算法：使用自然对数的上置信界1（UCB1）。\n- 目标：确定UCB1在时间 $t=20$ 时选择哪个臂。\n\n**步骤2：使用提取的已知信息进行验证**\n- **科学依据**：该问题是UCB1算法在多臂老虎机问题上的标准应用，这是强化学习和计算科学中一个成熟的课题。其前提是科学合理的。\n- **适定性**：所有必要信息均已提供。指定的总拉动次数 $t=20$ 与各臂拉动次数之和（$n_1+n_2+n_3 = 8+7+5 = 20$）相匹配。目标明确，可以得出唯一解。\n- **客观性**：问题以精确、客观的语言陈述。\n- **完整性与一致性**：问题是自洽的、一致的，并且没有歧义。数据和约束足以得到唯一解。\n\n**步骤3：结论与行动**\n- 问题有效。将提供解答。\n\n上置信界1（UCB1）算法旨在平衡MAB问题中的探索-利用权衡。它在每个时间步通过最大化一个UCB指数来选择一个臂，该指数是经验平均奖励（利用项）和一个置信奖励（探索项）之和。置信奖励源自Hoeffding不等式，并随着一个臂被拉动的频率增加而减少。\n\n在时间步 $t$ 时，臂 $j$ 的UCB1指数由下式给出：\n$$\n\\text{UCB}_j(t) = Q_j(t) + \\sqrt{\\frac{2 \\ln t}{n_j(t)}}\n$$\n其中：\n- $Q_j(t)$ 是截至时间 $t$ 从臂 $j$ 获得的奖励的经验均值。\n- $n_j(t)$ 是截至时间 $t$ 臂 $j$ 被拉动的次数。\n- $t$ 是截至该时间点所有臂的总拉动次数。\n\n策略是选择具有最高UCB指数的臂：$a_{t+1} = \\underset{j}{\\operatorname{argmax}} \\, \\text{UCB}_j(t)$。\n\n在给定的时间 $t=20$ 时，我们有以下信息：\n- 总试验次数：$t = 20$。\n- 对于臂 $1$：$n_1 = 8$。\n- 对于臂 $2$：$n_2 = 7$。\n- 对于臂 $3$：$n_3 = 5$。\n\n首先，我们计算每个臂的经验平均奖励 $Q_j$。\n\n**臂 1：**\n奖励总和为 $0.9 + 0.7 + 0.6 + 0.8 + 0.7 + 0.4 + 0.9 + 0.6 = 5.6$。\n经验均值为：\n$$\nQ_1 = \\frac{5.6}{8} = 0.7\n$$\n\n**臂 2：**\n奖励总和为 $0.5 + 0.8 + 0.3 + 0.6 + 0.4 + 0.7 + 0.6 = 3.9$。\n经验均值为：\n$$\nQ_2 = \\frac{3.9}{7}\n$$\n\n**臂 3：**\n奖励总和为 $0.2 + 0.4 + 0.9 + 0.6 + 0.5 = 2.6$。\n经验均值为：\n$$\nQ_3 = \\frac{2.6}{5} = 0.52\n$$\n\n接下来，我们计算在 $t=20$ 时每个臂的UCB1指数。\n\n**臂 1 的UCB1指数：**\n$$\n\\text{UCB}_1 = Q_1 + \\sqrt{\\frac{2 \\ln t}{n_1}} = 0.7 + \\sqrt{\\frac{2 \\ln 20}{8}} = 0.7 + \\sqrt{\\frac{\\ln 20}{4}}\n$$\n\n**臂 2 的UCB1指数：**\n$$\n\\text{UCB}_2 = Q_2 + \\sqrt{\\frac{2 \\ln t}{n_2}} = \\frac{3.9}{7} + \\sqrt{\\frac{2 \\ln 20}{7}}\n$$\n\n**臂 3 的UCB1指数：**\n$$\n\\text{UCB}_3 = Q_3 + \\sqrt{\\frac{2 \\ln t}{n_3}} = 0.52 + \\sqrt{\\frac{2 \\ln 20}{5}}\n$$\n\n为了确定选择哪个臂，我们必须比较这三个值。我们可以对这些值进行近似计算以进行比较。使用值 $\\ln 20 \\approx 2.9957$：\n\n- $\\text{UCB}_1 \\approx 0.7 + \\sqrt{\\frac{2.9957}{4}} \\approx 0.7 + \\sqrt{0.7489} \\approx 0.7 + 0.8654 = 1.5654$\n- $\\text{UCB}_2 \\approx \\frac{3.9}{7} + \\sqrt{\\frac{2 \\times 2.9957}{7}} \\approx 0.5571 + \\sqrt{0.8559} \\approx 0.5571 + 0.9252 = 1.4823$\n- $\\text{UCB}_3 \\approx 0.52 + \\sqrt{\\frac{2 \\times 2.9957}{5}} \\approx 0.52 + \\sqrt{1.1983} \\approx 0.52 + 1.0947 = 1.6147$\n\n比较计算出的UCB指数：\n$$\n1.6147 > 1.5654 > 1.4823\n$$\n这意味着 $\\text{UCB}_3 > \\text{UCB}_1 > \\text{UCB}_2$。\n\nUCB1算法选择具有最大UCB指数的臂。在这种情况下，臂 $3$ 的指数最高。这个决策主要是由臂 $3$ 较大的探索奖励驱动的，因为它的拉动次数相对较少（$n_3=5$），这超过了其与臂 $1$ 相比更低的经验平均奖励。\n\n因此，在时间 $t=20$ 时，UCB1策略会选择臂 $3$ 进行下一次拉动。",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "本练习综合了前面的概念，介绍了一种强大且具有神经现实性的强化学习模型——行动者-评价者（Actor-Critic）架构。该模型结合了一个策略学习器（“行动者”）和一个价值函数估计器（“评价者”）。通过这个实践，你将手动计算这两个组件的学习更新过程，体验评价者对行动结果的评估如何指导行动者做出更好的决策。",
            "id": "3970872",
            "problem": "考虑一个在线策略强化学习智能体，其处于一个有限马尔可夫决策过程（MDP）中，该过程有两个状态 $s \\in \\{s_{1}, s_{2}\\}$ 和两个动作 $a \\in \\{a_{1}, a_{2}\\}$。该智能体使用演员-评论家（actor-critic）架构，其中评论家通过分解 $A(s,a) = Q(s,a) - V(s)$ 来估计优势函数 $A(s,a)$，其中 $Q(s,a)$ 是动作-价值函数，$V(s)$ 是当前策略下的状态-价值函数。策略是基于线性偏好的 softmax 函数。以下元素定义了模型和数据：\n- 状态特征映射 $\\phi(s) \\in \\mathbb{R}^{2}$ 由 $\\phi(s_{1}) = [1, 0]^{\\top}$ 和 $\\phi(s_{2}) = [0, 1]^{\\top}$ 给出。\n- 演员的偏好是特征的线性函数：对于每个动作 $a \\in \\{a_{1}, a_{2}\\}$，偏好为 $h(a \\mid s) = \\theta_{a}^{\\top} \\phi(s)$，策略为 $\\pi(a \\mid s) = \\frac{\\exp(h(a \\mid s))}{\\sum_{b \\in \\{a_{1}, a_{2}\\}} \\exp(h(b \\mid s))}$。当前的演员参数为 $\\theta_{a_{1}} = [0.3, -0.1]^{\\top}$ 和 $\\theta_{a_{2}} = [-0.2, 0.25]^{\\top}$。\n- 评论家使用线性函数逼近：\n  - $Q(s,a) \\approx w_{Q}^{\\top} \\xi(s,a)$，其中 $\\xi(s,a) = [\\phi_{1}(s), \\phi_{2}(s), \\mathbf{1}\\{a = a_{1}\\}, \\mathbf{1}\\{a = a_{2}\\}]^{\\top} \\in \\mathbb{R}^{4}$，当前参数为 $w_{Q} = [0.1, 0.2, 0.05, -0.1]^{\\top}$。\n  - $V(s) \\approx w_{V}^{\\top} \\phi(s)$，当前参数为 $w_{V} = [0.0, 0.3]^{\\top}$。\n- 折扣因子为 $\\gamma = 0.9$。\n- 终止状态的价值定义为 $V(\\text{terminal}) = 0$。\n- 学习率分别为：演员 $\\alpha_{\\pi} = 0.05$，动作-价值评论家 $\\alpha_{Q} = 0.1$，状态-价值评论家 $\\alpha_{V} = 0.1$。\n- 观测到一个包含三个转移 $(s, a, r, s')$ 的批次：\n  1. $(s_{1}, a_{1}, 1.0, s_{2})$\n  2. $(s_{1}, a_{2}, 0.0, s_{1})$\n  3. $(s_{2}, a_{2}, 2.0, \\text{terminal})$\n\n从强化学习和大脑建模的基本原理出发，即期望回报的定义、策略下的价值函数、策略梯度定理和时间差分（TD）学习，构建演员-评论家更新，其中：\n- 演员对期望回报执行梯度上升，使用优势函数 $A(s,a)$ 作为基线偏移信号，即更新与由 $A(s,a)$ 缩放的 $\\nabla_{\\theta} \\ln \\pi(a \\mid s)$ 成正比。\n- 评论家更新 $w_{Q}$ 以通过对平方误差进行梯度下降，来减小 $Q(s,a)$ 的单步TD误差，使其趋向目标 $r + \\gamma V(s')$。\n- 评论家更新 $w_{V}$ 以通过对平方误差进行梯度下降，来减小 $V(s)$ 的单步TD误差，使其趋向目标 $r + \\gamma V(s')$。\n\n将批次更新视为每个转移贡献的总和（不按批次大小进行归一化）。计算当前参数下，此批次产生的单步参数更新向量 $\\Delta \\theta$、$\\Delta w_{Q}$ 和 $\\Delta w_{V}$。将连接后的更新向量按\n$[\\Delta \\theta_{a_{1},1}, \\Delta \\theta_{a_{1},2}, \\Delta \\theta_{a_{2},1}, \\Delta \\theta_{a_{2},2}, \\Delta w_{Q,1}, \\Delta w_{Q,2}, \\Delta w_{Q,3}, \\Delta w_{Q,4}, \\Delta w_{V,1}, \\Delta w_{V,2}]$\n的顺序排列，并按此顺序提供最终的数值答案。将每个条目四舍五入到四位有效数字。答案不带单位。最终答案必须是指令中指定的单行矩阵。",
            "solution": "用户在强化学习领域内提供了一个明确定义的计算问题。该问题具有科学依据，内容自洽且客观。因此，该问题被视为有效，可以构建完整的解决方案。\n\n任务是根据一批三个转移，计算一个演员-评论家模型的单步参数更新。更新被视为每个转移贡献的总和。模型参数是演员的策略参数 $\\theta_{a_1}$ 和 $\\theta_{a_2}$、动作-价值评论家的权重 $w_Q$ 和状态-价值评论家的权重 $w_V$。\n\n参数更新的一般形式源自梯度上升/下降原理。\n\n**1. 评论家更新**\n评论家参数 $w_Q$ 和 $w_V$ 通过对平方单步时间差分（TD）误差进行梯度下降来更新。\n\n对于动作-价值函数 $Q(s, a) \\approx w_Q^{\\top}\\xi(s,a)$，一个转移 $(s, a, r, s')$ 的损失是 $L_Q = \\frac{1}{2} \\delta_Q^2$，其中TD误差是相对于目标 $y_Q = r + \\gamma V(s')$ 定义的。\n$$ \\delta_Q = y_Q - Q(s,a) = r + \\gamma V(s') - w_Q^{\\top}\\xi(s,a) $$\n梯度下降更新为：\n$$ \\Delta w_Q = -\\alpha_Q \\nabla_{w_Q} L_Q = \\alpha_Q \\delta_Q \\nabla_{w_Q} Q(s,a) = \\alpha_Q \\delta_Q \\xi(s,a) $$\n\n对于状态-价值函数 $V(s) \\approx w_V^{\\top}\\phi(s)$，一个转移 $(s, a, r, s')$ 的损失是 $L_V = \\frac{1}{2} \\delta_V^2$，其中TD误差是相对于同一目标 $y_V = r + \\gamma V(s')$ 定义的。\n$$ \\delta_V = y_V - V(s) = r + \\gamma V(s') - w_V^{\\top}\\phi(s) $$\n梯度下降更新为：\n$$ \\Delta w_V = -\\alpha_V \\nabla_{w_V} L_V = \\alpha_V \\delta_V \\nabla_{w_V} V(s) = \\alpha_V \\delta_V \\phi(s) $$\n\n**2. 演员更新**\n演员参数 $\\theta$ 通过对期望回报目标 $J(\\theta)$ 进行梯度上升来更新。带优势函数基线的策略梯度定理给出了随机更新规则：\n$$ \\Delta \\theta = \\alpha_{\\pi} A(s,a) \\nabla_{\\theta} \\ln \\pi(a \\mid s) $$\n其中优势为 $A(s,a) = Q(s,a) - V(s)$。对于带有线性偏好 $h(a \\mid s) = \\theta_a^{\\top}\\phi(s)$ 的 softmax 策略 $\\pi(a \\mid s)$，对数策略相对于特定参数向量 $\\theta_b$ 的梯度为：\n$$ \\nabla_{\\theta_b} \\ln \\pi(a \\mid s) = (\\mathbf{1}\\{a=b\\} - \\pi(b \\mid s)) \\phi(s) $$\n因此，对于一个给定的转移 $(s, a, r, s')$，参数向量 $\\theta_b$ 的更新为：\n$$ \\Delta \\theta_b = \\alpha_{\\pi} A(s,a) (\\mathbf{1}\\{a=b\\} - \\pi(b \\mid s)) \\phi(s) $$\n\n我们现在将处理批次中的三个转移。每个参数向量的总更新将是来自每个转移的更新之和。\n\n初始参数：\n- $\\theta_{a_{1}} = [0.3, -0.1]^{\\top}$\n- $\\theta_{a_{2}} = [-0.2, 0.25]^{\\top}$\n- $w_{Q} = [0.1, 0.2, 0.05, -0.1]^{\\top}$\n- $w_{V} = [0.0, 0.3]^{\\top}$\n- $\\gamma = 0.9$, $\\alpha_{\\pi} = 0.05$, $\\alpha_Q = 0.1$, $\\alpha_V = 0.1$。\n状态特征：$\\phi(s_1) = [1, 0]^{\\top}$，$\\phi(s_2) = [0, 1]^{\\top}$。\n\n### 转移 1: $(s, a, r, s') = (s_1, a_1, 1.0, s_2)$\n\n首先，我们计算此转移所需的值。\n- 状态 $s=s_1$，动作 $a=a_1$。\n- 状态特征: $\\phi(s_1) = [1, 0]^{\\top}$。\n- $s_1$ 处的策略偏好：\n  $h(a_1 \\mid s_1) = \\theta_{a_1}^{\\top}\\phi(s_1) = 0.3 \\cdot 1 + (-0.1) \\cdot 0 = 0.3$。\n  $h(a_2 \\mid s_1) = \\theta_{a_2}^{\\top}\\phi(s_1) = -0.2 \\cdot 1 + 0.25 \\cdot 0 = -0.2$。\n- $s_1$ 处的策略概率：\n  $\\pi(a_1 \\mid s_1) = \\frac{\\exp(0.3)}{\\exp(0.3) + \\exp(-0.2)} \\approx \\frac{1.349859}{1.349859 + 0.818731} \\approx 0.622459$。\n  $\\pi(a_2 \\mid s_1) = 1 - \\pi(a_1 \\mid s_1) \\approx 0.377541$。\n- $s_1$ 处的价值函数：\n  $V(s_1) = w_V^{\\top}\\phi(s_1) = 0.0 \\cdot 1 + 0.3 \\cdot 0 = 0.0$。\n  $Q(s_1, a_1)$: 特征 $\\xi(s_1, a_1) = [1, 0, 1, 0]^{\\top}$。\n  $Q(s_1, a_1) = w_Q^{\\top}\\xi(s_1, a_1) = 0.1 \\cdot 1 + 0.2 \\cdot 0 + 0.05 \\cdot 1 + (-0.1) \\cdot 0 = 0.15$。\n- 下一状态 $s'=s_2$ 的价值：\n  $V(s_2) = w_V^{\\top}\\phi(s_2) = 0.0 \\cdot 0 + 0.3 \\cdot 1 = 0.3$。\n- TD 目标：$y_1 = r + \\gamma V(s_2) = 1.0 + 0.9 \\cdot 0.3 = 1.27$。\n- TD 误差和优势：\n  $\\delta_{Q,1} = y_1 - Q(s_1, a_1) = 1.27 - 0.15 = 1.12$。\n  $\\delta_{V,1} = y_1 - V(s_1) = 1.27 - 0.0 = 1.27$。\n  $A(s_1, a_1) = Q(s_1, a_1) - V(s_1) = 0.15 - 0.0 = 0.15$。\n\n现在，我们计算此转移的参数更新：\n- $\\Delta w_{Q,1} = \\alpha_Q \\delta_{Q,1} \\xi(s_1, a_1) = 0.1 \\cdot 1.12 \\cdot [1, 0, 1, 0]^{\\top} = [0.112, 0, 0.112, 0]^{\\top}$。\n- $\\Delta w_{V,1} = \\alpha_V \\delta_{V,1} \\phi(s_1) = 0.1 \\cdot 1.27 \\cdot [1, 0]^{\\top} = [0.127, 0]^{\\top}$。\n- $\\Delta \\theta_{a_1,1} = \\alpha_{\\pi} A(s_1,a_1) (1 - \\pi(a_1 \\mid s_1)) \\phi(s_1) = 0.05 \\cdot 0.15 \\cdot (1 - 0.622459) \\cdot [1, 0]^{\\top} \\approx [0.00283156, 0]^{\\top}$。\n- $\\Delta \\theta_{a_2,1} = \\alpha_{\\pi} A(s_1,a_1) (-\\pi(a_2 \\mid s_1)) \\phi(s_1) = 0.05 \\cdot 0.15 \\cdot (-0.377541) \\cdot [1, 0]^{\\top} \\approx [-0.00283156, 0]^{\\top}$。\n\n### 转移 2: $(s, a, r, s') = (s_1, a_2, 0.0, s_1)$\n\n- 状态 $s=s_1$，动作 $a=a_2$。$s_1$ 的相关值与转移 1 中相同（$V(s_1)=0.0$）。\n- 价值函数 $Q(s_1, a_2)$: 特征 $\\xi(s_1, a_2) = [1, 0, 0, 1]^{\\top}$。\n  $Q(s_1, a_2) = w_Q^{\\top}\\xi(s_1, a_2) = 0.1 \\cdot 1 + 0.2 \\cdot 0 + 0.05 \\cdot 0 + (-0.1) \\cdot 1 = 0.0$。\n- 下一状态 $s'=s_1$ 的价值: $V(s_1) = 0.0$。\n- TD 目标：$y_2 = r + \\gamma V(s_1) = 0.0 + 0.9 \\cdot 0.0 = 0.0$。\n- TD 误差和优势：\n  $\\delta_{Q,2} = y_2 - Q(s_1, a_2) = 0.0 - 0.0 = 0.0$。\n  $\\delta_{V,2} = y_2 - V(s_1) = 0.0 - 0.0 = 0.0$。\n  $A(s_1, a_2) = Q(s_1, a_2) - V(s_1) = 0.0 - 0.0 = 0.0$。\n- 由于所有误差和优势均为零，因此该转移的所有参数更新都是零向量。\n  $\\Delta w_{Q,2} = [0, 0, 0, 0]^{\\top}$, $\\Delta w_{V,2} = [0, 0]^{\\top}$, $\\Delta \\theta_{a_1,2} = [0, 0]^{\\top}$, $\\Delta \\theta_{a_2,2} = [0, 0]^{\\top}$。\n\n### 转移 3: $(s, a, r, s') = (s_2, a_2, 2.0, \\text{terminal})$\n\n- 状态 $s=s_2$，动作 $a=a_2$。\n- 状态特征: $\\phi(s_2) = [0, 1]^{\\top}$。\n- $s_2$ 处的策略偏好：\n  $h(a_1 \\mid s_2) = \\theta_{a_1}^{\\top}\\phi(s_2) = 0.3 \\cdot 0 + (-0.1) \\cdot 1 = -0.1$。\n  $h(a_2 \\mid s_2) = \\theta_{a_2}^{\\top}\\phi(s_2) = -0.2 \\cdot 0 + 0.25 \\cdot 1 = 0.25$。\n- $s_2$ 处的策略概率：\n  $\\pi(a_1 \\mid s_2) = \\frac{\\exp(-0.1)}{\\exp(-0.1) + \\exp(0.25)} \\approx \\frac{0.904837}{0.904837 + 1.284025} \\approx 0.413366$。\n  $\\pi(a_2 \\mid s_2) = 1 - \\pi(a_1 \\mid s_2) \\approx 0.586634$。\n- $s_2$ 处的价值函数：\n  $V(s_2) = w_V^{\\top}\\phi(s_2) = 0.0 \\cdot 0 + 0.3 \\cdot 1 = 0.3$。\n  $Q(s_2, a_2)$: 特征 $\\xi(s_2, a_2) = [0, 1, 0, 1]^{\\top}$。\n  $Q(s_2, a_2) = w_Q^{\\top}\\xi(s_2, a_2) = 0.1 \\cdot 0 + 0.2 \\cdot 1 + 0.05 \\cdot 0 + (-0.1) \\cdot 1 = 0.1$。\n- 下一状态 $s'=\\text{terminal}$ 的价值: $V(\\text{terminal}) = 0$。\n- TD 目标：$y_3 = r + \\gamma V(\\text{terminal}) = 2.0 + 0.9 \\cdot 0 = 2.0$。\n- TD 误差和优势：\n  $\\delta_{Q,3} = y_3 - Q(s_2, a_2) = 2.0 - 0.1 = 1.9$。\n  $\\delta_{V,3} = y_3 - V(s_2) = 2.0 - 0.3 = 1.7$。\n  $A(s_2, a_2) = Q(s_2, a_2) - V(s_2) = 0.1 - 0.3 = -0.2$。\n\n现在，我们计算此转移的参数更新：\n- $\\Delta w_{Q,3} = \\alpha_Q \\delta_{Q,3} \\xi(s_2, a_2) = 0.1 \\cdot 1.9 \\cdot [0, 1, 0, 1]^{\\top} = [0, 0.19, 0, 0.19]^{\\top}$。\n- $\\Delta w_{V,3} = \\alpha_V \\delta_{V,3} \\phi(s_2) = 0.1 \\cdot 1.7 \\cdot [0, 1]^{\\top} = [0, 0.17]^{\\top}$。\n- $\\Delta \\theta_{a_1,3} = \\alpha_{\\pi} A(s_2,a_2) (-\\pi(a_1 \\mid s_2)) \\phi(s_2) = 0.05 \\cdot (-0.2) \\cdot (-0.413366) \\cdot [0, 1]^{\\top} \\approx [0, 0.00413366]^{\\top}$。\n- $\\Delta \\theta_{a_2,3} = \\alpha_{\\pi} A(s_2,a_2) (1 - \\pi(a_2 \\mid s_2)) \\phi(s_2) = 0.05 \\cdot (-0.2) \\cdot (1 - 0.586634) \\cdot [0, 1]^{\\top} \\approx [0, -0.00413366]^{\\top}$。\n\n### 批次总更新\n\n总更新是三个转移的更新之和。\n- $\\Delta \\theta_{a_1} = \\Delta \\theta_{a_1,1} + \\Delta \\theta_{a_1,2} + \\Delta \\theta_{a_1,3} = [0.00283156, 0]^{\\top} + [0, 0]^{\\top} + [0, 0.00413366]^{\\top} = [0.00283156, 0.00413366]^{\\top}$。\n- $\\Delta \\theta_{a_2} = \\Delta \\theta_{a_2,1} + \\Delta \\theta_{a_2,2} + \\Delta \\theta_{a_2,3} = [-0.00283156, 0]^{\\top} + [0, 0]^{\\top} + [0, -0.00413366]^{\\top} = [-0.00283156, -0.00413366]^{\\top}$。\n- $\\Delta w_Q = \\Delta w_{Q,1} + \\Delta w_{Q,2} + \\Delta w_{Q,3} = [0.112, 0, 0.112, 0]^{\\top} + [0, 0, 0, 0]^{\\top} + [0, 0.19, 0, 0.19]^{\\top} = [0.112, 0.19, 0.112, 0.19]^{\\top}$。\n- $\\Delta w_V = \\Delta w_{V,1} + \\Delta w_{V,2} + \\Delta w_{V,3} = [0.127, 0]^{\\top} + [0, 0]^{\\top} + [0, 0.17]^{\\top} = [0.127, 0.17]^{\\top}$。\n\n### 最终组合向量\n\n最终结果是这些更新的连接向量，其分量四舍五入到四位有效数字。\n顺序为 $[\\Delta \\theta_{a_{1},1}, \\Delta \\theta_{a_{1},2}, \\Delta \\theta_{a_{2},1}, \\Delta \\theta_{a_{2},2}, \\Delta w_{Q,1}, \\Delta w_{Q,2}, \\Delta w_{Q,3}, \\Delta w_{Q,4}, \\Delta w_{V,1}, \\Delta w_{V,2}]$。\n\n- $\\Delta \\theta_{a_1,1} = 0.00283156 \\rightarrow 0.002832$\n- $\\Delta \\theta_{a_1,2} = 0.00413366 \\rightarrow 0.004134$\n- $\\Delta \\theta_{a_2,1} = -0.00283156 \\rightarrow -0.002832$\n- $\\Delta \\theta_{a_2,2} = -0.00413366 \\rightarrow -0.004134$\n- $\\Delta w_{Q,1} = 0.112 \\rightarrow 0.1120$\n- $\\Delta w_{Q,2} = 0.19 \\rightarrow 0.1900$\n- $\\Delta w_{Q,3} = 0.112 \\rightarrow 0.1120$\n- $\\Delta w_{Q,4} = 0.19 \\rightarrow 0.1900$\n- $\\Delta w_{V,1} = 0.127 \\rightarrow 0.1270$\n- $\\Delta w_{V,2} = 0.17 \\rightarrow 0.1700$\n\n连接后的向量是：\n$[0.002832, 0.004134, -0.002832, -0.004134, 0.1120, 0.1900, 0.1120, 0.1900, 0.1270, 0.1700]$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.002832  0.004134  -0.002832  -0.004134  0.1120  0.1900  0.1120  0.1900  0.1270  0.1700\n\\end{pmatrix}\n}\n$$"
        }
    ]
}