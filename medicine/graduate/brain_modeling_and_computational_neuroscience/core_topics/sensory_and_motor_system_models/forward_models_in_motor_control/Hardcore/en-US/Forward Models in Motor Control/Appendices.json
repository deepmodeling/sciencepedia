{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental function of a forward model is to predict the future state of a system based on its current state and a motor command. This first exercise provides a clear and direct application of this principle by asking you to construct a minimal forward model for a single joint. By calculating the predicted angle one time step into the future, you will practice the core operation of propagating a state forward using given dynamical equations, building a crucial foundation for understanding more complex predictive mechanisms .",
            "id": "3982443",
            "problem": "A single human elbow joint can be idealized as a single-degree-of-freedom rotational plant whose kinematics obey discrete-time forward Euler integration of the continuous-time relationships. Consider a minimal internal forward model deployed by the Central Nervous System (CNS) that uses an Efference Copy (EC) of the commanded torque-like signal to predict the immediate sensory consequence (joint angle) of the action one sampling step ahead. The plant is modeled by a state consisting of joint angle and angular velocity, where the discrete-time dynamics are given by $x_{t+1}=x_{t}+\\Delta t\\,\\omega_{t}$ and $\\omega_{t+1}=\\omega_{t}+\\Delta t\\,\\alpha\\,u_{t}$, with $x_{t}$ the joint angle at time step $t$, $\\omega_{t}$ the angular velocity at time step $t$, $\\Delta t$ the sampling interval, $\\alpha$ a known positive gain that maps motor command to angular acceleration, and $u_{t}$ the motor command at time $t$. Assume the noiseless sensory measurement is the angle itself, so the sensory consequence at time $t$ is $s_{t}=x_{t}$. Construct a minimal forward model that uses the current state $(x_{t},\\omega_{t})$ and the EC of the motor command $u_{t}$ to propagate one step ahead and predict the angle measurement at the next time step, $\\hat{s}_{t+1}$. Derive, from first principles and core definitions of state-space forward models in motor control, a closed-form analytic expression for the predicted sensory angle $\\hat{s}_{t+1}$ in terms of $x_{t}$, $\\omega_{t}$, $\\Delta t$, $\\alpha$, and $u_{t}$. Express the final answer as a single analytical expression. No rounding is required.",
            "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, objective, and internally consistent. It presents a standard, albeit simplified, state-space model of a single-joint system and asks for a one-step prediction based on explicitly defined dynamics. All necessary data and definitions are provided. The problem is therefore valid.\n\nThe core task is to derive an expression for the predicted sensory angle at the next time step, $\\hat{s}_{t+1}$. In the context of motor control, a forward model is an internal neural process that simulates the dynamics of the motor system (the \"plant\") to predict the sensory consequences of a motor command. It takes the current estimated state of the plant and an efference copy of the motor command as inputs and outputs a prediction of the next sensory state.\n\nThe problem defines the state of the system at a discrete time step $t$ by the joint angle $x_t$ and the angular velocity $\\omega_t$. The motor command is given as $u_t$. The sensory measurement at time $t$ is the angle itself, so $s_t = x_t$. Consequently, the predicted sensory angle at time $t+1$, denoted $\\hat{s}_{t+1}$, is equivalent to the predicted joint angle at time $t+1$, which we shall denote as $\\hat{x}_{t+1}$.\n\nThe problem explicitly provides the discrete-time dynamics of the plant, which are modeled using a forward Euler integration scheme. These dynamics constitute the \"internal model\" that the Central Nervous System (CNS) uses for its prediction. The equations are:\n$$x_{t+1} = x_{t} + \\Delta t\\,\\omega_{t}$$\n$$\\omega_{t+1} = \\omega_{t} + \\Delta t\\,\\alpha\\,u_{t}$$\nHere, $\\Delta t$ is the sampling interval and $\\alpha$ is a gain parameter.\n\nThe forward model's function is to compute the predicted state at time $t+1$, $(\\hat{x}_{t+1}, \\hat{\\omega}_{t+1})$, based on the known state at time $t$, $(x_t, \\omega_t)$, and the known motor command at time $t$, $u_t$. This is achieved by applying the system's dynamic equations one step forward in time.\n\nTo find the predicted angle $\\hat{x}_{t+1}$, we must apply the first dynamic equation. This equation defines how the angle evolves from one time step to the next:\n$$\\hat{x}_{t+1} = x_{t} + \\Delta t\\,\\omega_{t}$$\nThe prediction for the next angle, $\\hat{x}_{t+1}$, is calculated using the current angle, $x_t$, and the current angular velocity, $\\omega_t$.\n\nThe second dynamic equation describes the evolution of the angular velocity:\n$$\\hat{\\omega}_{t+1} = \\omega_{t} + \\Delta t\\,\\alpha\\,u_{t}$$\nThis part of the model uses the current velocity $\\omega_t$ and the motor command $u_t$ to predict the velocity at the next time step, $\\hat{\\omega}_{t+1}$.\n\nThe problem asks for the predicted sensory angle, $\\hat{s}_{t+1}$, which is $\\hat{x}_{t+1}$. Based on the first dynamic equation provided, the expression for $\\hat{x}_{t+1}$ is directly obtained.\n$$\\hat{s}_{t+1} = \\hat{x}_{t+1} = x_{t} + \\Delta t\\,\\omega_{t}$$\nIt is important to note that, according to the specific discrete-time dynamics provided (explicit forward Euler), the prediction of the angle at the next time step, $\\hat{x}_{t+1}$, depends only on the current state variables $x_t$ and $\\omega_t$. The motor command $u_t$ and the gain $\\alpha$ influence the predicted angular *velocity* at the next time step, $\\hat{\\omega}_{t+1}$, but they do not appear in the expression for the predicted *angle* at the next time step. The derivation must strictly adhere to the model as specified. While other numerical integration schemes (e.g., semi-implicit Euler) would yield a dependence on $u_t$ for the position update, the problem statement is unambiguous in its definition of the dynamics. The list of variables ($x_{t}$, $\\omega_{t}$, $\\Delta t$, $\\alpha$, and $u_{t}$) provided in the prompt represents the complete set of available parameters for the system, not a requirement that all must appear in the final expression for this specific prediction.\n\nTherefore, the closed-form analytic expression for the predicted sensory angle $\\hat{s}_{t+1}$ is derived directly from the provided state-update equation for the angle $x$.",
            "answer": "$$\\boxed{x_{t} + \\Delta t \\, \\omega_{t}}$$"
        },
        {
            "introduction": "A key challenge in motor control is dealing with noisy and delayed sensory feedback. A forward model's prediction acts as an independent source of information that can be integrated with actual sensory measurements to produce a more accurate and timely estimate of the body's state. This practice demonstrates this vital function by guiding you to optimally fuse a noisy proprioceptive measurement with a noisy forward model prediction, a process central to theories of Bayesian brain function and state estimation .",
            "id": "3982407",
            "problem": "In the context of motor control, the Central Nervous System (CNS) integrates a forward model prediction of sensory consequences with actual proprioceptive measurements to estimate the true joint angle. Consider a single-joint angle at time $t$, denoted by the scalar random variable $x_t$ in radians. The proprioceptive measurement is modeled as $y_t = x_t + \\varepsilon_t$, where $\\varepsilon_t$ is zero-mean Gaussian noise with variance $R$, and the forward-model predicted sensory consequence is modeled as $\\hat{y}_t = x_t + \\eta_t$, where $\\eta_t$ is zero-mean Gaussian noise with variance $P$. Assume $\\varepsilon_t$ and $\\eta_t$ are mutually independent and independent of $x_t$, and both measurement channels are unbiased.\n\nUsing only fundamental principles of unbiased linear estimation under additive independent Gaussian noise and the definition of mean-square error, derive the estimator that minimizes the expected squared error in estimating $x_t$ by fusing $y_t$ and $\\hat{y}_t$. Then compute the minimum achievable mean-square error (which equals the estimation-error variance at the optimum) for the parameters $R = 0.01$ and $P = 0.005$.\n\nExpress the final minimum mean-square error in radians squared (rad$^2$) and round your answer to four significant figures. The angle is in radians.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and complete. It presents a classic problem in optimal estimation theory, specifically the fusion of two noisy measurements of a single underlying state. All necessary components—the models for the measurements, the statistical properties of the noise, the independence assumptions, and the objective function (minimization of mean-square error)—are provided and are mutually consistent. Therefore, a valid solution can be derived.\n\nThe goal is to find an optimal estimate of the true joint angle, $x_t$, by linearly combining the proprioceptive measurement, $y_t$, and the forward-model prediction, $\\hat{y}_t$. Let the fused estimate be denoted by $\\hat{x}_t$. As we are seeking a linear estimator, it can be written as a weighted sum of the two available signals:\n$$\n\\hat{x}_t = w_1 y_t + w_2 \\hat{y}_t\n$$\nwhere $w_1$ and $w_2$ are the weights to be determined.\n\nThe problem requires the estimator to be unbiased. An estimator is unbiased if the expectation of the estimation error is zero, which implies that the expected value of the estimate is equal to the expected value of the true quantity being estimated.\n$$\nE[\\hat{x}_t] = E[x_t]\n$$\nLet us compute the expectation of our linear estimator:\n$$\nE[\\hat{x}_t] = E[w_1 y_t + w_2 \\hat{y}_t] = w_1 E[y_t] + w_2 E[\\hat{y}_t]\n$$\nThe problem states that $y_t = x_t + \\varepsilon_t$ and $\\hat{y}_t = x_t + \\eta_t$. The noise terms $\\varepsilon_t$ and $\\eta_t$ are both zero-mean, i.e., $E[\\varepsilon_t] = 0$ and $E[\\eta_t] = 0$. Therefore, the expectations of the measurements are:\n$$\nE[y_t] = E[x_t + \\varepsilon_t] = E[x_t] + E[\\varepsilon_t] = E[x_t]\n$$\n$$\nE[\\hat{y}_t] = E[x_t + \\eta_t] = E[x_t] + E[\\eta_t] = E[x_t]\n$$\nSubstituting these back into the expression for $E[\\hat{x}_t]$:\n$$\nE[\\hat{x}_t] = w_1 E[x_t] + w_2 E[x_t] = (w_1 + w_2) E[x_t]\n$$\nFor the unbiasedness condition $E[\\hat{x}_t] = E[x_t]$ to hold for any possible distribution of $x_t$, the sum of the weights must be unity:\n$$\nw_1 + w_2 = 1\n$$\nThis constraint allows us to express the estimator using a single unknown weight, say $w = w_1$, so that $w_2 = 1 - w$:\n$$\n\\hat{x}_t = w y_t + (1 - w) \\hat{y}_t\n$$\nThe next step is to minimize the mean-square error (MSE), defined as $\\text{MSE} = E[(\\hat{x}_t - x_t)^2]$. We first express the estimation error, $e_t = \\hat{x}_t - x_t$, in terms of the noise sources:\n$$\ne_t = \\left(w y_t + (1 - w) \\hat{y}_t\\right) - x_t\n$$\nSubstitute $y_t = x_t + \\varepsilon_t$ and $\\hat{y}_t = x_t + \\eta_t$:\n$$\ne_t = w(x_t + \\varepsilon_t) + (1 - w)(x_t + \\eta_t) - x_t\n$$\n$$\ne_t = w x_t + w \\varepsilon_t + x_t - w x_t + \\eta_t - w \\eta_t - x_t\n$$\n$$\ne_t = w \\varepsilon_t + (1 - w) \\eta_t\n$$\nNote that the expectation of the error is $E[e_t] = w E[\\varepsilon_t] + (1-w)E[\\eta_t] = w(0) + (1-w)(0) = 0$, confirming the estimator is unbiased. The MSE is the expectation of the square of this error, which is equivalent to the variance of the error since the error has zero mean.\n$$\n\\text{MSE} = E[e_t^2] = E\\left[(w \\varepsilon_t + (1 - w) \\eta_t)^2\\right]\n$$\n$$\n\\text{MSE} = E\\left[w^2 \\varepsilon_t^2 + (1 - w)^2 \\eta_t^2 + 2w(1 - w)\\varepsilon_t \\eta_t\\right]\n$$\nBy linearity of expectation:\n$$\n\\text{MSE} = w^2 E[\\varepsilon_t^2] + (1 - w)^2 E[\\eta_t^2] + 2w(1 - w)E[\\varepsilon_t \\eta_t]\n$$\nThe problem specifies the noise variances as $\\text{Var}(\\varepsilon_t) = E[\\varepsilon_t^2] - (E[\\varepsilon_t])^2 = E[\\varepsilon_t^2] = R$ and $\\text{Var}(\\eta_t) = E[\\eta_t^2] - (E[\\eta_t])^2 = E[\\eta_t^2] = P$. It also states that $\\varepsilon_t$ and $\\eta_t$ are mutually independent. Therefore, the expectation of their product is the product of their expectations: $E[\\varepsilon_t \\eta_t] = E[\\varepsilon_t] E[\\eta_t] = 0 \\cdot 0 = 0$.\nSubstituting these into the MSE expression gives the MSE as a function of the weight $w$:\n$$\n\\text{MSE}(w) = w^2 R + (1 - w)^2 P\n$$\nTo find the weight $w$ that minimizes the MSE, we differentiate $\\text{MSE}(w)$ with respect to $w$ and set the result to zero:\n$$\n\\frac{d}{dw}\\text{MSE}(w) = \\frac{d}{dw}\\left[w^2 R + (1 - w)^2 P\\right] = 2wR + 2(1 - w)(-1)P = 2wR - 2P + 2wP\n$$\nSetting the derivative to zero:\n$$\n2wR - 2P + 2wP = 0\n$$\n$$\nw(R+P) = P\n$$\nThe optimal weight $w^*$ is thus:\n$$\nw^* = w_1^* = \\frac{P}{R+P}\n$$\nThe corresponding optimal weight for the second measurement is $w_2^* = 1 - w^* = 1 - \\frac{P}{R+P} = \\frac{R}{R+P}$. To confirm this is a minimum, the second derivative, $\\frac{d^2}{dw^2}\\text{MSE}(w) = 2R + 2P$, is positive since variances $R$ and $P$ are positive.\n\nFinally, we compute the minimum achievable MSE, $\\text{MSE}_{\\min}$, by substituting the optimal weight $w^*$ back into the MSE formula:\n$$\n\\text{MSE}_{\\min} = (w^*)^2 R + (1 - w^*)^2 P\n$$\n$$\n\\text{MSE}_{\\min} = \\left(\\frac{P}{R+P}\\right)^2 R + \\left(\\frac{R}{R+P}\\right)^2 P\n$$\n$$\n\\text{MSE}_{\\min} = \\frac{P^2 R}{(R+P)^2} + \\frac{R^2 P}{(R+P)^2} = \\frac{P^2 R + R^2 P}{(R+P)^2}\n$$\nFactoring out the common term $RP$ from the numerator:\n$$\n\\text{MSE}_{\\min} = \\frac{RP(P+R)}{(R+P)^2} = \\frac{RP}{R+P}\n$$\nThis elegant result shows that the inverse of the minimum MSE (the fused precision) is the sum of the inverses of the individual variances (the individual precisions): $\\frac{1}{\\text{MSE}_{\\min}} = \\frac{1}{P} + \\frac{1}{R}$.\n\nNow, we substitute the given numerical values, $R = 0.01 \\, \\text{rad}^2$ and $P = 0.005 \\, \\text{rad}^2$:\n$$\n\\text{MSE}_{\\min} = \\frac{(0.01)(0.005)}{0.01 + 0.005} = \\frac{0.00005}{0.015}\n$$\n$$\n\\text{MSE}_{\\min} = \\frac{5 \\times 10^{-5}}{1.5 \\times 10^{-2}} = \\frac{5}{1.5} \\times 10^{-3} = \\frac{10}{3} \\times 10^{-3} = \\frac{1}{300}\n$$\nAs a decimal, this is $0.003333...$ $\\text{rad}^2$. The problem requires rounding to four significant figures. The first four significant figures are $3$, $3$, $3$, and $3$. The fifth digit is $3$, which is less than $5$, so we do not round up.\nThe minimum mean-square error is $0.003333 \\, \\text{rad}^2$.",
            "answer": "$$\\boxed{0.003333}$$"
        },
        {
            "introduction": "While linear models provide a powerful starting point, real biological systems are inherently nonlinear due to factors like muscle physiology and gravitational forces. This exercise moves our analysis into this more realistic domain by introducing a nonlinear model of an elbow-like limb. You will practice applying the Extended Kalman Filter (EKF) by deriving the Jacobian matrices needed to linearize the system's dynamics, a critical skill for implementing predictive models for complex, real-world motor control problems .",
            "id": "3982422",
            "problem": "Consider a single-degree-of-freedom elbow-like limb whose internal forward model predicts state evolution and sensory consequences. Let the state be $x_t = \\begin{pmatrix} q_t \\\\ \\omega_t \\end{pmatrix}$, where $q_t$ is the joint angle and $\\omega_t$ is the joint angular velocity. The control input $u_t$ is a neural command mapped to muscle torque by a saturating nonlinearity. The discrete-time forward dynamics are given by\n$$\nx_{t+1} = f(x_t,u_t) + w_t,\n$$\nwith\n$$\nf(x_t,u_t) \\equiv\n\\begin{pmatrix}\nq_t \\\\\n\\omega_t\n\\end{pmatrix}\n+\n\\Delta t\\,\n\\begin{pmatrix}\n\\omega_t \\\\\n\\frac{1}{I}\\big(\\beta\\,\\tanh(u_t) - b\\,\\omega_t - \\kappa\\,\\sin(q_t)\\big)\n\\end{pmatrix},\n$$\nwhere $I>0$ is the limb inertia, $b>0$ is viscous damping, $\\kappa>0$ is the gravity-related stiffness (for example $\\kappa = m g L$ for mass $m$, gravitational acceleration $g$, and lever arm $L$), $\\beta>0$ scales the control-to-torque mapping, and $\\Delta t>0$ is the sampling interval. The process noise $w_t$ is zero-mean Gaussian with covariance $Q_t$, written $w_t \\sim \\mathcal{N}(0,Q_t)$. The sensory measurement is\n$$\ny_t = h(x_t) + v_t,\n$$\nwith\n$$\nh(x_t) \\equiv\n\\begin{pmatrix}\nq_t \\\\\n\\arctan(\\omega_t)\n\\end{pmatrix},\n$$\nand measurement noise $v_t \\sim \\mathcal{N}(0,R_t)$.\n\nStarting from the definition of nonlinear state-space Bayesian filtering and the linearization principle underlying the Extended Kalman Filter (EKF; Extended Kalman Filter), derive the EKF prediction and update equations for the state mean and covariance using first-order Taylor expansions of $f$ and $h$ about the current operating point. Then, for the specific model above, compute explicitly the Jacobians $\\frac{\\partial f}{\\partial x}$, $\\frac{\\partial f}{\\partial u}$, and $\\frac{\\partial h}{\\partial x}$ needed for the prediction and update steps, evaluated at the general state $x_t = \\begin{pmatrix} q_t \\\\ \\omega_t \\end{pmatrix}$ and input $u_t$.\n\nAnswer specification: Provide your final answer as the single block-row concatenation $[A_t\\;\\;B_t\\;\\;H_t]$, where $A_t \\in \\mathbb{R}^{2\\times 2}$, $B_t \\in \\mathbb{R}^{2\\times 1}$, and $H_t \\in \\mathbb{R}^{2\\times 2}$. Express your final answer symbolically (no numerical substitution), and do not include any units. No rounding is required.",
            "solution": "The user-provided problem is first subjected to a validation process.\n\n### Step 1: Extract Givens\n- State vector: $x_t = \\begin{pmatrix} q_t \\\\ \\omega_t \\end{pmatrix}$, where $q_t$ is the joint angle and $\\omega_t$ is the joint angular velocity.\n- Control input: $u_t$.\n- Discrete-time dynamics model: $x_{t+1} = f(x_t,u_t) + w_t$.\n- Dynamics function: $f(x_t,u_t) \\equiv \\begin{pmatrix} q_t \\\\ \\omega_t \\end{pmatrix} + \\Delta t\\, \\begin{pmatrix} \\omega_t \\\\ \\frac{1}{I}\\big(\\beta\\,\\tanh(u_t) - b\\,\\omega_t - \\kappa\\,\\sin(q_t)\\big) \\end{pmatrix}$.\n- Model parameters: $I>0$, $b>0$, $\\kappa>0$, $\\beta>0$, $\\Delta t>0$.\n- Process noise: $w_t \\sim \\mathcal{N}(0,Q_t)$.\n- Sensory measurement model: $y_t = h(x_t) + v_t$.\n- Measurement function: $h(x_t) \\equiv \\begin{pmatrix} q_t \\\\ \\arctan(\\omega_t) \\end{pmatrix}$.\n- Measurement noise: $v_t \\sim \\mathcal{N}(0,R_t)$.\n- Task: Derive the general Extended Kalman Filter (EKF) prediction and update equations. Then, compute the Jacobians $\\frac{\\partial f}{\\partial x}$, $\\frac{\\partial f}{\\partial u}$, and $\\frac{\\partial h}{\\partial x}$ for the given model, evaluated at the general state $x_t$ and input $u_t$.\n- Final answer specification: Provide the result as a block-row matrix $[A_t \\;\\; B_t \\;\\; H_t]$, where $A_t = \\frac{\\partial f}{\\partial x}$, $B_t = \\frac{\\partial f}{\\partial u}$, and $H_t = \\frac{\\partial h}{\\partial x}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem describes a physically plausible biomechanical system (a single-joint limb under gravity, viscous damping, and controlled torque) using standard state-space representation. The nonlinearities ($\\sin$, $\\tanh$, $\\arctan$) are common and meaningful in such models. The use of an EKF for state estimation of this system is a canonical problem in computational neuroscience and control engineering. The formulation is scientifically sound.\n- **Well-Posed**: The problem is clearly defined with all necessary equations and parameters provided. It asks for a specific set of derivations (general EKF equations) and calculations (specific Jacobians), which admit a unique and stable mathematical solution.\n- **Objective**: The problem is stated in precise, objective mathematical language, free from any ambiguity or subjective content.\n\nNo flaws are found. The problem is self-contained, consistent, and scientifically valid.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe Extended Kalman Filter (EKF) is an algorithm for state estimation in nonlinear dynamical systems. It linearizes the system dynamics and measurement models around the current state estimate at each time step to propagate the mean and covariance of the state's probability distribution. Let the state estimate and its covariance at time $t$, given measurements up to time $t$, be denoted by $\\hat{x}_{t|t}$ and $P_{t|t}$, respectively. The EKF operates in two steps: prediction and update.\n\n**1. Prediction Step**\nGiven the estimate $\\hat{x}_{t|t}$ and covariance $P_{t|t}$ at time $t$, the goal is to predict the state and covariance at time $t+1$.\nThe predicted state mean, $\\hat{x}_{t+1|t}$, is obtained by propagating the previous estimate through the nonlinear dynamics function $f$:\n$$\n\\hat{x}_{t+1|t} = f(\\hat{x}_{t|t}, u_t)\n$$\nThe predicted state covariance, $P_{t+1|t}$, is obtained by linearizing the dynamics around $\\hat{x}_{t|t}$ and $u_t$. The linearized dynamics are determined by the Jacobian matrix $A_t = \\frac{\\partial f}{\\partial x}\\big|_{\\hat{x}_{t|t}, u_t}$. The covariance is then propagated as follows, including the addition of process noise with covariance $Q_t$:\n$$\nP_{t+1|t} = A_t P_{t|t} A_t^T + Q_t\n$$\n\n**2. Update Step**\nWhen a new measurement $y_{t+1}$ becomes available at time $t+1$, the predicted state $\\hat{x}_{t+1|t}$ and covariance $P_{t+1|t}$ are updated.\nFirst, the measurement model is linearized around the predicted state $\\hat{x}_{t+1|t}$. The relevant Jacobian is $H_{t+1} = \\frac{\\partial h}{\\partial x}\\big|_{\\hat{x}_{t+1|t}}$.\nThe innovation or measurement residual is the difference between the actual measurement $y_{t+1}$ and the predicted measurement $h(\\hat{x}_{t+1|t})$:\n$$\n\\tilde{y}_{t+1} = y_{t+1} - h(\\hat{x}_{t+1|t})\n$$\nThe innovation covariance $S_{t+1}$ is:\n$$\nS_{t+1} = H_{t+1} P_{t+1|t} H_{t+1}^T + R_{t+1}\n$$\nThe optimal Kalman gain $K_{t+1}$ is then computed:\n$$\nK_{t+1} = P_{t+1|t} H_{t+1}^T S_{t+1}^{-1}\n$$\nFinally, the updated state mean $\\hat{x}_{t+1|t+1}$ and covariance $P_{t+1|t+1}$ are calculated:\n$$\n\\hat{x}_{t+1|t+1} = \\hat{x}_{t+1|t} + K_{t+1} \\tilde{y}_{t+1}\n$$\n$$\nP_{t+1|t+1} = (I - K_{t+1} H_{t+1}) P_{t+1|t}\n$$\nwhere $I$ is the identity matrix. This completes the EKF cycle for one time step.\n\n**Jacobian Computations**\nThe problem requires computing the Jacobians $A_t = \\frac{\\partial f}{\\partial x}$, $B_t = \\frac{\\partial f}{\\partial u}$, and $H_t = \\frac{\\partial h}{\\partial x}$ evaluated at a general state $x_t = \\begin{pmatrix} q_t \\\\ \\omega_t \\end{pmatrix}$ and input $u_t$.\n\nThe state vector is $x_t = \\begin{pmatrix} q_t \\\\ \\omega_t \\end{pmatrix}$. The dynamics function $f(x_t, u_t)$ has components:\n$$\nf_1(q_t, \\omega_t, u_t) = q_t + \\Delta t \\, \\omega_t\n$$\n$$\nf_2(q_t, \\omega_t, u_t) = \\omega_t + \\frac{\\Delta t}{I}\\big(\\beta\\,\\tanh(u_t) - b\\,\\omega_t - \\kappa\\,\\sin(q_t)\\big)\n$$\n\n**Calculation of $A_t = \\frac{\\partial f}{\\partial x}$:**\n$A_t$ is a $2 \\times 2$ matrix with elements $A_{ij} = \\frac{\\partial f_i}{\\partial x_j}$, where $x_1 = q_t$ and $x_2 = \\omega_t$.\n$$\n\\frac{\\partial f_1}{\\partial q_t} = \\frac{\\partial}{\\partial q_t}(q_t + \\Delta t \\, \\omega_t) = 1\n$$\n$$\n\\frac{\\partial f_1}{\\partial \\omega_t} = \\frac{\\partial}{\\partial \\omega_t}(q_t + \\Delta t \\, \\omega_t) = \\Delta t\n$$\n$$\n\\frac{\\partial f_2}{\\partial q_t} = \\frac{\\partial}{\\partial q_t}\\left(\\omega_t + \\frac{\\Delta t}{I}\\big(\\beta\\,\\tanh(u_t) - b\\,\\omega_t - \\kappa\\,\\sin(q_t)\\big)\\right) = -\\frac{\\kappa \\Delta t}{I}\\cos(q_t)\n$$\n$$\n\\frac{\\partial f_2}{\\partial \\omega_t} = \\frac{\\partial}{\\partial \\omega_t}\\left(\\omega_t + \\frac{\\Delta t}{I}\\big(\\beta\\,\\tanh(u_t) - b\\,\\omega_t - \\kappa\\,\\sin(q_t)\\big)\\right) = 1 - \\frac{b \\Delta t}{I}\n$$\nThus, the Jacobian of the dynamics with respect to the state is:\n$$\nA_t = \\frac{\\partial f}{\\partial x} = \\begin{pmatrix} 1 & \\Delta t \\\\ -\\frac{\\kappa \\Delta t}{I}\\cos(q_t) & 1 - \\frac{b \\Delta t}{I} \\end{pmatrix}\n$$\n\n**Calculation of $B_t = \\frac{\\partial f}{\\partial u}$:**\n$B_t$ is a $2 \\times 1$ matrix with elements $B_i = \\frac{\\partial f_i}{\\partial u_t}$.\n$$\n\\frac{\\partial f_1}{\\partial u_t} = \\frac{\\partial}{\\partial u_t}(q_t + \\Delta t \\, \\omega_t) = 0\n$$\n$$\n\\frac{\\partial f_2}{\\partial u_t} = \\frac{\\partial}{\\partial u_t}\\left(\\omega_t + \\frac{\\Delta t}{I}\\big(\\beta\\,\\tanh(u_t) - b\\,\\omega_t - \\kappa\\,\\sin(q_t)\\big)\\right) = \\frac{\\beta \\Delta t}{I}\\frac{d}{du_t}(\\tanh(u_t)) = \\frac{\\beta \\Delta t}{I}\\sech^2(u_t)\n$$\nThus, the Jacobian of the dynamics with respect to the control input is:\n$$\nB_t = \\frac{\\partial f}{\\partial u} = \\begin{pmatrix} 0 \\\\ \\frac{\\beta \\Delta t}{I}\\sech^2(u_t) \\end{pmatrix}\n$$\n\n**Calculation of $H_t = \\frac{\\partial h}{\\partial x}$:**\nThe measurement function $h(x_t)$ has components:\n$$\nh_1(q_t, \\omega_t) = q_t\n$$\n$$\nh_2(q_t, \\omega_t) = \\arctan(\\omega_t)\n$$\n$H_t$ is a $2 \\times 2$ matrix with elements $H_{ij} = \\frac{\\partial h_i}{\\partial x_j}$.\n$$\n\\frac{\\partial h_1}{\\partial q_t} = \\frac{\\partial}{\\partial q_t}(q_t) = 1\n$$\n$$\n\\frac{\\partial h_1}{\\partial \\omega_t} = \\frac{\\partial}{\\partial \\omega_t}(q_t) = 0\n$$\n$$\n\\frac{\\partial h_2}{\\partial q_t} = \\frac{\\partial}{\\partial q_t}(\\arctan(\\omega_t)) = 0\n$$\n$$\n\\frac{\\partial h_2}{\\partial \\omega_t} = \\frac{\\partial}{\\partial \\omega_t}(\\arctan(\\omega_t)) = \\frac{1}{1 + \\omega_t^2}\n$$\nThus, the Jacobian of the measurement function with respect to the state is:\n$$\nH_t = \\frac{\\partial h}{\\partial x} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{1 + \\omega_t^2} \\end{pmatrix}\n$$\n\nThe final answer is the block-row concatenation $[A_t \\;\\; B_t \\;\\; H_t]$, which is a $2 \\times 5$ matrix.\n$$\n[A_t \\;\\; B_t \\;\\; H_t] = \\begin{pmatrix} 1 & \\Delta t & 0 & 1 & 0 \\\\ -\\frac{\\kappa \\Delta t}{I}\\cos(q_t) & 1 - \\frac{b \\Delta t}{I} & \\frac{\\beta \\Delta t}{I}\\sech^2(u_t) & 0 & \\frac{1}{1 + \\omega_t^2} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & \\Delta t & 0 & 1 & 0 \\\\\n-\\frac{\\kappa \\Delta t}{I}\\cos(q_t) & 1 - \\frac{b \\Delta t}{I} & \\frac{\\beta \\Delta t}{I}\\sech^2(u_t) & 0 & \\frac{1}{1 + \\omega_t^2}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}