{
    "hands_on_practices": [
        {
            "introduction": "The energy model was proposed to explain a key distinction between simple and complex cells in the primary visual cortex (V1): their differing sensitivity to the spatial phase of a visual stimulus. This exercise allows you to explore this foundational concept of phase invariance directly. By deriving the response of an ideal energy model to a counterphase grating, you will formulate a concrete, falsifiable prediction that cleanly separates the behavior of these two fundamental cell types, solidifying your understanding of the model's primary function .",
            "id": "3978675",
            "problem": "Consider a one-dimensional model neuron in primary visual cortex (V1) that is described by the classical energy model for a complex cell. The stimulus is a counterphase grating given by\n$$\ns(x,t) \\;=\\; A \\cos(k_0 x + \\phi)\\,\\cos(\\omega t),\n$$\nwhere $A$ is the contrast amplitude, $k_0$ is the spatial frequency, $\\phi$ is the spatial phase (in radians), and $\\omega$ is the temporal angular frequency (in radians per second). The neuron has two linear subunits with spatial receptive fields (RFs) $g_{e}(x)$ and $g_{o}(x)$ that form a spatial quadrature pair at spatial frequency $k_0$. The linear responses are\n$$\nr_{e}(t;\\phi)\\;=\\;\\int_{-\\infty}^{\\infty} g_{e}(x)\\,s(x,t)\\,dx,\\qquad\nr_{o}(t;\\phi)\\;=\\;\\int_{-\\infty}^{\\infty} g_{o}(x)\\,s(x,t)\\,dx,\n$$\nand the complex-cell output is the energy\n$$\nE(t;\\phi)\\;=\\;r_{e}(t;\\phi)^{2}\\;+\\;r_{o}(t;\\phi)^{2}.\n$$\nAssume both subunits are linear and time-invariant in space, that the quadrature property holds exactly at $k_0$ (equal gain and a spatial phase difference of $\\pi/2$), and that the temporal dependence of each linear response arises only from the stimulus $\\cos(\\omega t)$ factor. Define the time-averaged response over one temporal period $T = 2\\pi/\\omega$ by\n$$\n\\overline{E}(\\phi)\\;=\\;\\frac{1}{T}\\int_{0}^{T} E(t;\\phi)\\,dt,\n$$\nand define the phase invariance index\n$$\n\\mathrm{PI}\\;=\\;\\frac{R_{\\min}}{R_{\\max}},\n$$\nwhere $R_{\\min}$ and $R_{\\max}$ are, respectively, the minimum and maximum of $\\overline{E}(\\phi)$ over $\\phi \\in [0,2\\pi)$.\n\nUsing only first principles of linear systems and the quadrature property, do the following. First, formulate a falsifiable experimental prediction that distinguishes a simple cell from a complex cell under counterphase grating stimulation by specifying what would be different in the dependence of the time-averaged response on the spatial phase $\\phi$. Second, derive $\\overline{E}(\\phi)$ for the energy model neuron and compute the corresponding $\\mathrm{PI}$ for the ideal case described above.\n\nExpress your final answer as a dimensionless number. No rounding is required.",
            "solution": "The problem is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   Stimulus: $s(x,t) \\;=\\; A \\cos(k_0 x + \\phi)\\,\\cos(\\omega t)$\n-   Variables: contrast amplitude $A$, spatial frequency $k_0$, spatial phase $\\phi$, temporal angular frequency $\\omega$.\n-   Neuron Model: A complex cell described by the energy model.\n-   Subunits: Two linear subunits with spatial receptive fields (RFs) $g_{e}(x)$ and $g_{o}(x)$.\n-   Quadrature Property: At spatial frequency $k_0$, the subunits form a spatial quadrature pair, meaning they have equal gain and a spatial phase difference of $\\pi/2$.\n-   Linear Responses: $r_{e}(t;\\phi)\\;=\\;\\int_{-\\infty}^{\\infty} g_{e}(x)\\,s(x,t)\\,dx$ and $r_{o}(t;\\phi)\\;=\\;\\int_{-\\infty}^{\\infty} g_{o}(x)\\,s(x,t)\\,dx$.\n-   Complex-Cell Output (Energy): $E(t;\\phi)\\;=\\;r_{e}(t;\\phi)^{2}\\;+\\;r_{o}(t;\\phi)^{2}$.\n-   Assumptions:\n    1.  Subunits are linear and time-invariant in space.\n    2.  The temporal dependence of each linear response arises only from the stimulus factor $\\cos(\\omega t)$.\n-   Time-Averaged Response: $\\overline{E}(\\phi)\\;=\\;\\frac{1}{T}\\int_{0}^{T} E(t;\\phi)\\,dt$, with temporal period $T = 2\\pi/\\omega$.\n-   Phase Invariance Index: $\\mathrm{PI}\\;=\\;\\frac{R_{\\min}}{R_{\\max}}$, where $R_{\\min} = \\min_{\\phi} \\overline{E}(\\phi)$ and $R_{\\max} = \\max_{\\phi} \\overline{E}(\\phi)$ for $\\phi \\in [0,2\\pi)$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded:** The problem describes the canonical Adelson-Bergen energy model for a V1 complex cell, a cornerstone of computational neuroscience. The use of a counterphase grating as a stimulus is a standard technique in visual physiology. The problem is scientifically sound.\n-   **Well-Posed:** The problem is clearly defined, providing all necessary functions, definitions, and assumptions to derive the required quantities. The objectives are specific and lead to a unique solution.\n-   **Objective:** The problem is stated in precise, mathematical language, free from any subjective or ambiguous terminology.\n-   **Conclusion:** The problem statement is complete, consistent, and scientifically valid. It is well-posed and suitable for a rigorous solution.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n### Solution\n\nThe solution addresses the two parts of the problem: first, formulating a falsifiable prediction to distinguish simple and complex cells, and second, deriving the time-averaged response $\\overline{E}(\\phi)$ and computing the Phase Invariance Index $(\\mathrm{PI})$ for the ideal complex cell model.\n\n**Part 1: Falsifiable Prediction to Distinguish Simple and Complex Cells**\n\nA simple cell is typically modeled as a single linear filter followed by a static nonlinearity, for example, rectification or squaring. Its response is highly sensitive to the spatial phase of the stimulus. In contrast, the energy model for a complex cell pools the squared outputs of multiple linear filters with different phase preferences. This pooling is hypothesized to create a response that is invariant to the spatial phase. This difference leads to a clear experimental prediction.\n\nA simple cell's response could be modeled as $S(t;\\phi) = f(r_e(t;\\phi))$ for some single linear response $r_e(t;\\phi)$ and a nonlinear function $f$. As will be shown, $r_e(t;\\phi)$ is proportional to $\\cos(\\phi)$. Thus, its time-averaged response $\\overline{S}(\\phi)$ would be strongly modulated by $\\phi$, peaking at a preferred phase and being null at other phases. For instance, if $f(z)=z^2$, the time-averaged simple cell response would be proportional to $\\cos^2(\\phi)$, for which the ratio of minimum to maximum response is $0$.\n\nA complex cell's response is given by $E(t;\\phi)$. The derivation below will show that its time-averaged response $\\overline{E}(\\phi)$ is constant.\n\n**Falsifiable Prediction:** An experiment can be designed where a V1 neuron is presented with a counterphase grating stimulus $s(x,t) = A \\cos(k_0 x + \\phi)\\cos(\\omega t)$ at its preferred spatial frequency $k_0$. The neuron's time-averaged firing rate (serving as a measure of the response) is recorded as the spatial phase $\\phi$ is varied over the range $[0, 2\\pi)$.\n-   **If the cell is a simple cell**, its time-averaged response will show strong modulation with $\\phi$, exhibiting clear peaks and troughs. The phase invariance index $\\mathrm{PI}$ would be close to $0$.\n-   **If the cell is an ideal complex cell**, its time-averaged response will be constant, showing no modulation with $\\phi$. The phase invariance index $\\mathrm{PI}$ would be $1$.\nThis provides a direct, quantitative, and falsifiable method for classifying a neuron as simple or complex based on its response to this stimulus.\n\n**Part 2: Derivation of $\\overline{E}(\\phi)$ and the Phase Invariance Index $(\\mathrm{PI})$**\n\nWe begin by calculating the linear responses $r_{e}(t;\\phi)$ and $r_{o}(t;\\phi)$. Given the stimulus $s(x,t) = A \\cos(k_0 x + \\phi) \\cos(\\omega t)$ and the assumption that the temporal dynamics arise only from the stimulus, we can factor out the temporal component from the spatial integration.\n$$\nr_{j}(t;\\phi) = \\int_{-\\infty}^{\\infty} g_{j}(x) A \\cos(k_0 x + \\phi) \\cos(\\omega t) dx = A \\cos(\\omega t) \\int_{-\\infty}^{\\infty} g_j(x) \\cos(k_0 x + \\phi) dx\n$$\nfor $j \\in \\{e, o\\}$. Let's analyze the spatial integral term by expanding the cosine:\n$$\n\\int_{-\\infty}^{\\infty} g_j(x) [\\cos(k_0 x)\\cos(\\phi) - \\sin(k_0 x)\\sin(\\phi)] dx = \\cos(\\phi)\\int_{-\\infty}^{\\infty} g_j(x)\\cos(k_0 x)dx - \\sin(\\phi)\\int_{-\\infty}^{\\infty} g_j(x)\\sin(k_0 x)dx\n$$\nThe quadrature property implies that $g_e(x)$ is an even-symmetric filter (like a cosine Gabor) and $g_o(x)$ is an odd-symmetric filter (like a sine Gabor). For an even function $g_e(x)$, the integral of $g_e(x)\\sin(k_0 x)$ over $(-\\infty, \\infty)$ is zero because the integrand is odd. For an odd function $g_o(x)$, the integral of $g_o(x)\\cos(k_0 x)$ is zero because the integrand is odd.\nThis simplifies the spatial components of the responses:\n-   For $r_e(t;\\phi)$: The term with $\\sin(\\phi)$ vanishes. Let $C_e = \\int_{-\\infty}^{\\infty} g_e(x)\\cos(k_0 x)dx$. The response becomes $r_e(t;\\phi) = A C_e \\cos(\\phi) \\cos(\\omega t)$.\n-   For $r_o(t;\\phi)$: The term with $\\cos(\\phi)$ vanishes. Let $S_o = \\int_{-\\infty}^{\\infty} g_o(x)\\sin(k_0 x)dx$. The response becomes $r_o(t;\\phi) = -A S_o \\sin(\\phi) \\cos(\\omega t)$.\n\nThe quadrature property also states that the filters have \"equal gain\". This translates to the magnitude of their responses to their preferred inputs being equal. The constant $C_e$ represents the response of the even filter to a cosine grating, and $S_o$ represents the response of the odd filter to a sine grating. Equal gain implies $|C_e| = |S_o|$. Let us define this gain as $K  0$. By adjusting the signs of $g_e$ and $g_o$ if necessary, we can set $C_e = K$ and $S_o = K$.\nThe linear responses are thus:\n$$\nr_e(t;\\phi) = A K \\cos(\\phi) \\cos(\\omega t)\n$$\n$$\nr_o(t;\\phi) = -A K \\sin(\\phi) \\cos(\\omega t)\n$$\nNow, we compute the energy $E(t;\\phi) = r_e(t;\\phi)^2 + r_o(t;\\phi)^2$:\n$$\nE(t;\\phi) = [A K \\cos(\\phi) \\cos(\\omega t)]^2 + [-A K \\sin(\\phi) \\cos(\\omega t)]^2\n$$\n$$\nE(t;\\phi) = (A K)^2 \\cos^2(\\omega t) \\cos^2(\\phi) + (A K)^2 \\cos^2(\\omega t) \\sin^2(\\phi)\n$$\nFactoring out the common terms:\n$$\nE(t;\\phi) = (A K)^2 \\cos^2(\\omega t) [\\cos^2(\\phi) + \\sin^2(\\phi)]\n$$\nUsing the trigonometric identity $\\cos^2(\\phi) + \\sin^2(\\phi) = 1$:\n$$\nE(t;\\phi) = (A K)^2 \\cos^2(\\omega t)\n$$\nNotably, the instantaneous energy $E(t;\\phi)$ is independent of the spatial phase $\\phi$.\n\nNext, we compute the time-averaged response $\\overline{E}(\\phi)$ over one temporal period $T=2\\pi/\\omega$:\n$$\n\\overline{E}(\\phi) = \\frac{1}{T} \\int_0^T E(t;\\phi) dt = \\frac{1}{T} \\int_0^T (A K)^2 \\cos^2(\\omega t) dt\n$$\nThe average value of $\\cos^2(\\theta)$ over a full period is $1/2$. Formally:\n$$\n\\frac{1}{T} \\int_0^T \\cos^2(\\omega t) dt = \\frac{1}{T} \\int_0^T \\frac{1+\\cos(2\\omega t)}{2} dt = \\frac{1}{2T} \\left[t + \\frac{\\sin(2\\omega t)}{2\\omega}\\right]_0^T\n$$\n$$\n= \\frac{1}{2T} \\left[T + \\frac{\\sin(2\\omega (2\\pi/\\omega))}{2\\omega} - 0\\right] = \\frac{1}{2T} \\left[T + \\frac{\\sin(4\\pi)}{2\\omega}\\right] = \\frac{1}{2}\n$$\nTherefore, the time-averaged response is:\n$$\n\\overline{E}(\\phi) = (A K)^2 \\cdot \\frac{1}{2} = \\frac{(A K)^2}{2}\n$$\nThe time-averaged response $\\overline{E}(\\phi)$ is a constant, independent of the spatial phase $\\phi$.\n\nFinally, we compute the Phase Invariance Index, $\\mathrm{PI} = R_{\\min}/R_{\\max}$:\n$$\nR_{\\max} = \\max_{\\phi \\in [0,2\\pi)} \\overline{E}(\\phi) = \\max_{\\phi} \\frac{(A K)^2}{2} = \\frac{(A K)^2}{2}\n$$\n$$\nR_{\\min} = \\min_{\\phi \\in [0,2\\pi)} \\overline{E}(\\phi) = \\min_{\\phi} \\frac{(A K)^2}{2} = \\frac{(A K)^2}{2}\n$$\n$$\n\\mathrm{PI} = \\frac{R_{\\min}}{R_{\\max}} = \\frac{(A K)^2 / 2}{(A K)^2 / 2} = 1\n$$\nFor an ideal complex cell described by the energy model, the phase invariance index is exactly $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Beyond understanding the functional properties of the energy model, a crucial task in computational neuroscience is \"system identification\"—the process of fitting models to experimental data. This practice provides the mathematical foundation for learning the energy model's parameters using gradient-based optimization methods. By deriving the gradients of the energy function with respect to both the stimulus and the model's filter weights, you develop the essential toolkit for training the model to replicate the behavior of real neurons, a fundamental skill for any computational modeler .",
            "id": "3978680",
            "problem": "Consider a standard energy model of a primary visual cortex (V1) complex cell. Let the stimulus be a vector $x \\in \\mathbb{R}^{n}$ and let the two linear subunit filters be $h_{1} \\in \\mathbb{R}^{n}$ and $h_{2} \\in \\mathbb{R}^{n}$. The subunit responses are the inner products $y_{1} = h_{1}^{\\top} x$ and $y_{2} = h_{2}^{\\top} x$. The model’s energy output is defined as\n$$\nE(x) = y_{1}^{2} + y_{2}^{2} = \\left(h_{1}^{\\top} x\\right)^{2} + \\left(h_{2}^{\\top} x\\right)^{2}.\n$$\nStarting from the core definitions of linear filtering as an inner product and the chain rule for differentiation of composite functions, derive closed-form expressions for the gradients of $E(x)$ with respect to the stimulus $x$ and with respect to the filter parameters $h_{1}$ and $h_{2}$. Then, briefly explain how these gradients can be used to perform system identification of the filters from data consisting of measured complex-cell responses to known stimuli using gradient-based optimization.\n\nProvide your final answer for the three gradients in a single row matrix in the order $\\left(\\nabla_{x} E, \\nabla_{h_{1}} E, \\nabla_{h_{2}} E\\right)$. No numerical rounding is required, and no physical units apply. Express your final answer as a simplified, closed-form analytic expression.",
            "solution": "The problem is valid as it is scientifically grounded in the established energy model of V1 complex cells, is mathematically well-posed, objective, and self-contained. The task is to derive standard gradients for this model and explain their application in system identification, which is a core concept in computational neuroscience.\n\nThe energy output of the V1 complex cell model is given by:\n$$\nE(x) = \\left(h_{1}^{\\top} x\\right)^{2} + \\left(h_{2}^{\\top} x\\right)^{2}\n$$\nwhere $x \\in \\mathbb{R}^{n}$ is the stimulus vector and $h_{1}, h_{2} \\in \\mathbb{R}^{n}$ are the linear filter vectors. To derive the gradients, we will apply the chain rule of differentiation for vector-valued functions. For a composite function $f(g(z))$, the gradient with respect to $z$ is $\\nabla_z f = \\frac{\\partial f}{\\partial g} \\nabla_z g$. We also use two standard vector calculus identities: $\\nabla_z(a^\\top z) = a$ and $\\nabla_z(z^\\top a) = a$.\n\n1.  **Gradient with respect to the stimulus $x$:** $\\nabla_{x} E(x)$\n\nThe energy $E(x)$ is a sum of two terms. The gradient of a sum is the sum of the gradients. We compute the gradient of each term with respect to $x$ separately.\n\nFor the first term, let $y_{1}(x) = h_{1}^{\\top} x$. The term is $y_{1}^{2}$. Using the chain rule:\n$$\n\\nabla_{x} \\left(y_{1}^{2}\\right) = \\frac{d(y_{1}^{2})}{dy_{1}} \\nabla_{x} y_{1}\n$$\nThe derivative of $y_{1}^{2}$ with respect to $y_{1}$ is $2y_{1}$. The gradient of the linear function $y_{1}(x) = h_{1}^{\\top} x$ with respect to $x$ is:\n$$\n\\nabla_{x} \\left(h_{1}^{\\top} x\\right) = h_{1}\n$$\nCombining these, the gradient of the first term is:\n$$\n\\nabla_{x} \\left(\\left(h_{1}^{\\top} x\\right)^{2}\\right) = 2y_{1} h_{1} = 2\\left(h_{1}^{\\top} x\\right) h_{1}\n$$\nBy symmetry, the gradient of the second term, $\\left(h_{2}^{\\top} x\\right)^{2}$, is:\n$$\n\\nabla_{x} \\left(\\left(h_{2}^{\\top} x\\right)^{2}\\right) = 2\\left(h_{2}^{\\top} x\\right) h_{2}\n$$\nThe total gradient of $E(x)$ with respect to $x$ is the sum of these two results:\n$$\n\\nabla_{x} E(x) = 2\\left(h_{1}^{\\top} x\\right) h_{1} + 2\\left(h_{2}^{\\top} x\\right) h_{2}\n$$\n\n2.  **Gradient with respect to the filter $h_{1}$:** $\\nabla_{h_{1}} E$\n\nTo find the gradient with respect to $h_{1}$, we note that the second term, $\\left(h_{2}^{\\top} x\\right)^{2}$, does not depend on $h_{1}$, so its gradient is the zero vector. We only need to differentiate the first term, $\\left(h_{1}^{\\top} x\\right)^{2}$.\n\nLet $y_{1}(h_{1}) = h_{1}^{\\top} x$. Using the chain rule for the gradient with respect to $h_{1}$:\n$$\n\\nabla_{h_{1}} \\left(y_{1}^{2}\\right) = \\frac{d(y_{1}^{2})}{dy_{1}} \\nabla_{h_{1}} y_{1} = 2y_{1} \\nabla_{h_{1}} y_{1}\n$$\nTo find the gradient of $y_{1}(h_{1}) = h_{1}^{\\top} x$, we can rewrite the inner product as $x^{\\top}h_{1}$. The gradient of this linear function with respect to $h_{1}$ is:\n$$\n\\nabla_{h_{1}} \\left(x^{\\top} h_{1}\\right) = x\n$$\nSubstituting this back, we obtain the gradient of $E$ with respect to $h_{1}$:\n$$\n\\nabla_{h_{1}} E = 2y_{1} x = 2\\left(h_{1}^{\\top} x\\right) x\n$$\n\n3.  **Gradient with respect to the filter $h_{2}$:** $\\nabla_{h_{2}} E$\n\nThe derivation for $\\nabla_{h_{2}} E$ is symmetric to the one for $\\nabla_{h_{1}} E$. The first term, $\\left(h_{1}^{\\top} x\\right)^{2}$, does not depend on $h_{2}$. We differentiate the second term, $\\left(h_{2}^{\\top} x\\right)^{2}$, with respect to $h_{2}$.\n$$\n\\nabla_{h_{2}} E = \\nabla_{h_{2}} \\left(\\left(h_{2}^{\\top} x\\right)^{2}\\right)\n$$\nFollowing the same logic as for $h_{1}$:\n$$\n\\nabla_{h_{2}} \\left(\\left(h_{2}^{\\top} x\\right)^{2}\\right) = 2\\left(h_{2}^{\\top} x\\right) \\nabla_{h_{2}}\\left(h_{2}^{\\top} x\\right) = 2\\left(h_{2}^{\\top} x\\right) x\n$$\nThus, the gradient of $E$ with respect to $h_{2}$ is:\n$$\n\\nabla_{h_{2}} E = 2\\left(h_{2}^{\\top} x\\right) x\n$$\n\n**System Identification using Gradients**\n\nSystem identification is the process of building a mathematical model of a system from experimental data. In this context, it means finding the filter parameters $h_{1}$ and $h_{2}$ that best explain the observed responses of a V1 complex cell to a set of known stimuli.\n\nThe derived gradients, $\\nabla_{h_{1}} E$ and $\\nabla_{h_{2}} E$, are essential for this process when using gradient-based optimization methods. The procedure is as follows:\n\n1.  **Define a Loss Function:** First, a loss function $L(h_{1}, h_{2})$ is defined to quantify the discrepancy between the model's predicted energy output $E(x^{(i)})$ and the actual measured neural responses $r^{(i)}$ for a dataset of stimulus-response pairs $\\{ (x^{(i)}, r^{(i)}) \\}_{i=1}^{N}$. A common choice is the mean squared error (MSE):\n    $$\n    L(h_{1}, h_{2}) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(r^{(i)} - E(x^{(i)})\\right)^{2}\n    $$\n2.  **Gradient-Based Optimization:** The goal is to find the parameters $h_{1}$ and $h_{2}$ that minimize this loss function. Gradient descent is an iterative algorithm that accomplishes this by repeatedly updating the parameters in the direction opposite to the gradient of the loss function. The update rules for each filter are:\n    $$\n    h_{1} \\leftarrow h_{1} - \\alpha \\nabla_{h_{1}} L \\\\\n    h_{2} \\leftarrow h_{2} - \\alpha \\nabla_{h_{2}} L\n    $$\n    where $\\alpha$ is a small positive number called the learning rate.\n\n3.  **Calculate Loss Gradients:** The gradients of the loss function, $\\nabla_{h_{1}} L$ and $\\nabla_{h_{2}} L$, are computed using the chain rule, which requires the gradients of the energy model $E$ that we derived. For example, for $h_{1}$:\n    $$\n    \\nabla_{h_{1}} L = \\frac{\\partial L}{\\partial E} \\nabla_{h_{1}} E\n    $$\n    The gradient of the MSE loss with respect to $h_{1}$ for a single data point $(x, r)$ is:\n    $$\n    \\nabla_{h_{1}} L = \\frac{\\partial}{\\partial h_{1}} \\left(r - E(x)\\right)^{2} = 2(r - E(x))(-\\nabla_{h_{1}} E) = -2(r - E(x)) \\left(2(h_{1}^{\\top} x) x\\right)\n    $$\n    A similar expression holds for $\\nabla_{h_{2}} L$. By averaging these gradients over the dataset and applying the update rules iteratively, the algorithm converges to a set of filters $h_{1}$ and $h_{2}$ that optimally characterize the complex cell's response properties.",
            "answer": "$$\n\\boxed{\\pmatrix{ 2\\left(h_{1}^{\\top} x\\right) h_{1} + 2\\left(h_{2}^{\\top} x\\right) h_{2}  2\\left(h_{1}^{\\top} x\\right) x  2\\left(h_{2}^{\\top} x\\right) x }}\n$$"
        },
        {
            "introduction": "Computational principles discovered in the brain, like the energy model's mechanism for phase invariance, often provide powerful inspiration for modern machine learning architectures. This hands-on coding exercise reverses the typical analytical approach: instead of analyzing a given model, it challenges you to synthesize its structure from a functional objective. By designing and implementing a loss function that penalizes phase-dependent responses, you will train a pair of linear filters from a random initialization and observe their convergence to a quadrature-pair relationship, effectively \"discovering\" the energy model through optimization. This practice offers profound insight into how computational goals can shape the emergence of neural representations .",
            "id": "3978686",
            "problem": "You will model the phase-invariant energy mechanism of Primary Visual Cortex (V1) complex cells using a minimal two-filter energy model and a sinusoidal data augmentation scheme within Convolutional Neural Networks (ConvNets). The goal is to design an objective that penalizes phase modulation in the energy of feature responses and to demonstrate, through optimization on synthetic sinusoidal inputs, that the learned filters converge to a quadrature relationship. All angles must be in radians.\n\nStart from the following fundamental base and core definitions.\n\n1. Linear filtering: given an input vector $x \\in \\mathbb{R}^{T}$ and a filter $f \\in \\mathbb{R}^{T}$, the linear response is $r = f^{\\top} x$.\n2. Sinusoidal dataset with phase augmentation: for a fixed angular frequency $\\omega \\in (0,\\pi)$ and a set of phases $\\Phi = \\{\\phi_{p}\\}_{p=1}^{P}$ uniformly sampled on $[0,2\\pi)$, construct inputs $x_{\\phi} \\in \\mathbb{R}^{T}$ with components $x_{\\phi}[t] = \\sin(\\omega t + \\phi)$ for $t \\in \\{0,1,\\dots,T-1\\}$ and $\\phi \\in \\Phi$.\n3. Two-filter energy model: define two filters $f_{1}, f_{2} \\in \\mathbb{R}^{T}$ and their responses $r_{i}(\\phi) = f_{i}^{\\top} x_{\\phi}$ for $i \\in \\{1,2\\}$; define the energy $y(\\phi) = r_{1}(\\phi)^{2} + r_{2}(\\phi)^{2}$.\n4. Phase-invariance desideratum: the energy $y(\\phi)$ should be independent of the phase $\\phi$.\n\nYour tasks are:\n\nA. Propose and implement an objective that penalizes phase modulation of the energy across phases while keeping the filters non-degenerate. Your objective must be a function of the filters $f_{1}$ and $f_{2}$ and the dataset $\\{x_{\\phi}\\}$ that includes:\n- A term that penalizes modulation of $y(\\phi)$ over $\\phi$ by minimizing its across-phase variability.\n- A norm constraint encouraging a fixed total squared norm for $(f_{1}, f_{2})$.\n- A mild equalization term encouraging the two filters to contribute equally in mean-squared response.\n\nB. Optimize the filters using gradient-based optimization on the sinusoidal dataset. You must implement gradients explicitly in terms of $f_{1}$, $f_{2}$, and $\\{x_{\\phi}\\}$, starting from the core definitions above. No automatic differentiation is permitted.\n\nC. After optimization, evaluate whether the learned filters form an approximate quadrature pair at the training frequency. For each filter $f_{i}$, define its complex Fourier coefficient at angular frequency $\\omega$ by\n$$\n\\hat{c}_{i}(\\omega) = \\sum_{t=0}^{T-1} f_{i}[t] \\, e^{-j \\omega t}, \\quad i \\in \\{1,2\\}.\n$$\nLet the filter phases be $\\theta_{i} = \\arg\\big(\\hat{c}_{i}(\\omega)\\big)$. Define the phase deviation from quadrature as\n$$\n\\Delta_{\\text{phase}} = \\left| \\left( \\left( \\theta_{2} - \\theta_{1} \\right) \\bmod \\pi \\right) - \\frac{\\pi}{2} \\right|.\n$$\nDefine the amplitude ratio error as\n$$\n\\Delta_{\\text{mag}} = \\left| \\frac{|\\hat{c}_{1}(\\omega)|}{|\\hat{c}_{2}(\\omega)|} - 1 \\right|.\n$$\nDefine the residual modulation index as\n$$\n\\Delta_{\\text{mod}} = \\frac{\\operatorname{std}_{\\phi \\in \\Phi_{\\text{eval}}} \\left[ y(\\phi) \\right]}{\\operatorname{mean}_{\\phi \\in \\Phi_{\\text{eval}}} \\left[ y(\\phi) \\right]},\n$$\nwhere $\\Phi_{\\text{eval}}$ is a dense evaluation grid of $256$ phases uniformly sampled in $[0,2\\pi)$.\n\nProgram requirements:\n\n1. Implement a gradient-based optimizer (you may implement a variant of stochastic gradient descent with momentum or an adaptive method) to minimize your objective over $(f_{1}, f_{2})$ for each test case. Use deterministic initialization by setting a fixed random seed for reproducibility. Angles must be in radians.\n\n2. Compute and report the three scalar metrics $(\\Delta_{\\text{phase}}, \\Delta_{\\text{mag}}, \\Delta_{\\text{mod}})$ for each test case after training, in that order.\n\n3. Test suite. Run your program on the following test cases, each specified by $(T,\\ \\omega,\\ P,\\ \\text{steps},\\ \\text{lr},\\ \\alpha,\\ \\beta)$:\n- Case $1$: $(33,\\ 0.3\\pi,\\ 64,\\ 2000,\\ 0.02,\\ 1.0,\\ 0.1)$.\n- Case $2$: $(33,\\ 0.1\\pi,\\ 64,\\ 2000,\\ 0.02,\\ 1.0,\\ 0.1)$.\n- Case $3$: $(33,\\ 0.45\\pi,\\ 64,\\ 2500,\\ 0.015,\\ 1.0,\\ 0.1)$.\n- Case $4$: $(33,\\ 0.3\\pi,\\ 8,\\ 3000,\\ 0.015,\\ 1.0,\\ 0.2)$.\n\nIn the above, $T$ is the input length, $\\omega$ is the angular frequency in radians per sample, $P$ is the number of training phases, steps is the number of optimization steps, lr is the learning rate, $\\alpha$ is the coefficient of the norm constraint, and $\\beta$ is the coefficient of the equalization term.\n\n4. Final output format. Your program should produce a single line of output containing the results as a comma-separated list of all metrics across the four cases, in order, flattened into a single list:\n$$\n\\left[\\Delta_{\\text{phase}}^{(1)},\\ \\Delta_{\\text{mag}}^{(1)},\\ \\Delta_{\\text{mod}}^{(1)},\\ \\Delta_{\\text{phase}}^{(2)},\\ \\Delta_{\\text{mag}}^{(2)},\\ \\Delta_{\\text{mod}}^{(2)},\\ \\Delta_{\\text{phase}}^{(3)},\\ \\Delta_{\\text{mag}}^{(3)},\\ \\Delta_{\\text{mod}}^{(3)},\\ \\Delta_{\\text{phase}}^{(4)},\\ \\Delta_{\\text{mag}}^{(4)},\\ \\Delta_{\\text{mod}}^{(4)}\\right].\n$$\nAngles must be in radians, and all results must be reported as floating-point numbers without units. The program must not read any input and must not print anything else.",
            "solution": "The problem requires us to model the phase-invariant energy mechanism of V1 complex cells. This is achieved by defining an objective function that penalizes phase-dependent energy responses from a two-filter system, optimizing the filters using gradient descent on a synthetic sinusoidal dataset, and evaluating the resulting filters for quadrature structure.\n\n### A. Objective Function Design\n\nThe goal is to find two linear filters, $f_{1}, f_{2} \\in \\mathbb{R}^{T}$, such that the energy response $y(\\phi) = (f_{1}^{\\top} x_{\\phi})^{2} + (f_{2}^{\\top} x_{\\phi})^{2}$ is constant for input sinusoids $x_{\\phi}[t] = \\sin(\\omega t + \\phi)$ across all phases $\\phi$. We formulate a composite objective function $L(f_1, f_2)$ as a weighted sum of three terms, $L = L_{\\text{mod}} + \\alpha L_{\\text{norm}} + \\beta L_{\\text{eq}}$, designed to enforce this desideratum and ensure the filters are well-behaved.\n\n1.  **Phase Modulation Penalty ($L_{\\text{mod}}$):** To penalize the modulation of energy $y(\\phi)$ with phase $\\phi$, we minimize its variance across the set of training phases $\\Phi = \\{\\phi_{p}\\}_{p=1}^{P}$. Let $\\bar{y} = \\frac{1}{P} \\sum_{p=1}^{P} y(\\phi_p)$ be the mean energy. The variance is an effective measure of modulation.\n    $$\n    L_{\\text{mod}} = \\operatorname{Var}_{\\phi \\in \\Phi}[y(\\phi)] = \\frac{1}{P} \\sum_{p=1}^{P} \\left( y(\\phi_p) - \\bar{y} \\right)^2\n    $$\n\n2.  **Norm Constraint Penalty ($L_{\\text{norm}}$):** To keep the filters non-degenerate and at a constrained magnitude, we penalize the deviation of the total squared norm of the filter pair from a target constant. We select a target total squared norm of $C=1$ for simplicity.\n    $$\n    L_{\\text{norm}} = \\left( \\|f_1\\|^2 + \\|f_2\\|^2 - 1 \\right)^2 = \\left( \\sum_{t=0}^{T-1} f_1[t]^2 + \\sum_{t=0}^{T-1} f_2[t]^2 - 1 \\right)^2\n    $$\n    The coefficient $\\alpha$ controls the strength of this constraint.\n\n3.  **Response Equalization Penalty ($L_{\\text{eq}}$):** To encourage both filters to contribute equally to the energy, we penalize the difference between their mean-squared responses. Let $\\langle r_i^2 \\rangle_P = \\frac{1}{P} \\sum_{p=1}^{P} r_i(\\phi_p)^2$ be the mean-squared response for filter $f_i$ over the training phases.\n    $$\n    L_{\\text{eq}} = \\left( \\langle r_1^2 \\rangle_P - \\langle r_2^2 \\rangle_P \\right)^2\n    $$\n    The coefficient $\\beta$ controls the strength of this equalization.\n\nThe total objective function to be minimized is:\n$$\nL(f_1, f_2) = \\frac{1}{P} \\sum_{p=1}^{P} \\left( y(\\phi_p) - \\bar{y} \\right)^2 + \\alpha \\left( \\|f_1\\|^2 + \\|f_2\\|^2 - 1 \\right)^2 + \\beta \\left( \\langle r_1^2 \\rangle_P - \\langle r_2^2 \\rangle_P \\right)^2\n$$\n\n### B. Gradient-Based Optimization\n\nWe minimize the objective function $L$ with respect to the filters $f_1$ and $f_2$ using gradient descent with momentum. This requires computing the partial derivatives $\\frac{\\partial L}{\\partial f_1}$ and $\\frac{\\partial L}{\\partial f_2}$. We derive the gradients for each term separately using the chain rule. Below, $k \\in \\{1,2\\}$.\n\n**1. Gradient of $L_{\\text{mod}}$:**\n$$\n\\frac{\\partial L_{\\text{mod}}}{\\partial f_k} = \\frac{\\partial}{\\partial f_k} \\left[ \\frac{1}{P} \\sum_{p=1}^{P} \\left( y(\\phi_p) - \\bar{y} \\right)^2 \\right] = \\frac{2}{P} \\sum_{p=1}^{P} \\left( y(\\phi_p) - \\bar{y} \\right) \\left( \\frac{\\partial y(\\phi_p)}{\\partial f_k} - \\frac{\\partial \\bar{y}}{\\partial f_k} \\right)\n$$\nThe required derivatives are $\\frac{\\partial y(\\phi)}{\\partial f_k} = 2 r_k(\\phi) x_{\\phi}$ and $\\frac{\\partial \\bar{y}}{\\partial f_k} = \\frac{1}{P} \\sum_{p=1}^{P} \\frac{\\partial y(\\phi_p)}{\\partial f_k} = \\frac{2}{P} \\sum_{p=1}^P r_k(\\phi_p) x_{\\phi_p}$.\nSubstituting and using the property $\\sum_p (y(\\phi_p) - \\bar{y}) = 0$, the expression simplifies to:\n$$\n\\frac{\\partial L_{\\text{mod}}}{\\partial f_k} = \\frac{4}{P} \\sum_{p=1}^{P} \\left( y(\\phi_p) - \\bar{y} \\right) r_k(\\phi_p) x_{\\phi_p}\n$$\n\n**2. Gradient of $L_{\\text{norm}}$:**\n$$\n\\frac{\\partial L_{\\text{norm}}}{\\partial f_k} = \\frac{\\partial}{\\partial f_k} \\left[ \\left( \\|f_1\\|^2 + \\|f_2\\|^2 - 1 \\right)^2 \\right] = 2 \\left( \\|f_1\\|^2 + \\|f_2\\|^2 - 1 \\right) \\frac{\\partial}{\\partial f_k} \\left( \\|f_1\\|^2 + \\|f_2\\|^2 \\right)\n$$\nSince $\\frac{\\partial \\|f_j\\|^2}{\\partial f_k}$ is $2f_k$ for $j=k$ and $0$ otherwise, we get:\n$$\n\\frac{\\partial L_{\\text{norm}}}{\\partial f_k} = 4 \\left( \\|f_1\\|^2 + \\|f_2\\|^2 - 1 \\right) f_k\n$$\n\n**3. Gradient of $L_{\\text{eq}}$:**\nThe derivative with respect to $f_1$ is:\n$$\n\\frac{\\partial L_{\\text{eq}}}{\\partial f_1} = 2 \\left( \\langle r_1^2 \\rangle_P - \\langle r_2^2 \\rangle_P \\right) \\frac{\\partial \\langle r_1^2 \\rangle_P}{\\partial f_1} = 2 \\left( \\langle r_1^2 \\rangle_P - \\langle r_2^2 \\rangle_P \\right) \\left( \\frac{1}{P} \\sum_{p=1}^{P} 2 r_1(\\phi_p) x_{\\phi_p} \\right)\n$$\n$$\n\\frac{\\partial L_{\\text{eq}}}{\\partial f_1} = \\frac{4}{P} \\left( \\langle r_1^2 \\rangle_P - \\langle r_2^2 \\rangle_P \\right) \\sum_{p=1}^{P} r_1(\\phi_p) x_{\\phi_p}\n$$\nBy symmetry, the derivative with respect to $f_2$ is:\n$$\n\\frac{\\partial L_{\\text{eq}}}{\\partial f_2} = -\\frac{4}{P} \\left( \\langle r_1^2 \\rangle_P - \\langle r_2^2 \\rangle_P \\right) \\sum_{p=1}^{P} r_2(\\phi_p) x_{\\phi_p}\n$$\n\n**Total Gradient and Update Rule:**\nThe total gradients are the weighted sum of the component gradients:\n$$\n\\frac{\\partial L}{\\partial f_k} = \\frac{\\partial L_{\\text{mod}}}{\\partial f_k} + \\alpha \\frac{\\partial L_{\\text{norm}}}{\\partial f_k} + \\beta \\frac{\\partial L_{\\text{eq}}}{\\partial f_k}\n$$\nThe filters are updated iteratively using gradient descent with momentum. Let $g_k^{(s)}$ be the gradient $\\frac{\\partial L}{\\partial f_k}$ at step $s$. The velocity $v_k$ and filter $f_k$ are updated as:\n$$\nv_k^{(s)} = \\mu v_k^{(s-1)} + g_k^{(s)}\n$$\n$$\nf_k^{(s+1)} = f_k^{(s)} - \\eta v_k^{(s)}\n$$\nwhere $\\mu$ is the momentum coefficient (e.g., $0.9$) and $\\eta$ is the learning rate (`lr`).\n\n### C. Filter Evaluation\n\nAfter optimization, the learned filters $(f_1, f_2)$ are evaluated using three metrics to quantify their conformance to a quadrature pair and the phase-invariance of the resulting energy response.\n\n1.  **Phase Deviation from Quadrature ($\\Delta_{\\text{phase}}$):** An ideal quadrature pair has a $\\pi/2$ ($90^{\\circ}$) phase shift at the target frequency $\\omega$. We compute the complex Fourier coefficient $\\hat{c}_{i}(\\omega) = \\sum_{t=0}^{T-1} f_{i}[t] \\, e^{-j \\omega t}$ for each filter and extract their phases $\\theta_{i} = \\arg(\\hat{c}_{i}(\\omega))$. The deviation from quadrature is measured as the difference between their phase separation and $\\pi/2$, modulo $\\pi$.\n    $$\n    \\Delta_{\\text{phase}} = \\left| \\left( \\left( \\theta_{2} - \\theta_{1} \\right) \\bmod \\pi \\right) - \\frac{\\pi}{2} \\right|\n    $$\n\n2.  **Amplitude Ratio Error ($\\Delta_{\\text{mag}}$):** An ideal quadrature pair also has equal amplitude response at the target frequency. This is measured by the deviation of the ratio of their Fourier coefficient magnitudes from $1$.\n    $$\n    \\Delta_{\\text{mag}} = \\left| \\frac{|\\hat{c}_{1}(\\omega)|}{|\\hat{c}_{2}(\\omega)|} - 1 \\right|\n    $$\n\n3.  **Residual Modulation Index ($\\Delta_{\\text{mod}}$):** This metric directly measures the remaining phase-dependence of the energy function after optimization. It is the coefficient of variation (standard deviation divided by the mean) of the energy $y(\\phi)$ computed over a dense grid of evaluation phases $\\Phi_{\\text{eval}}$. A value close to $0$ indicates high phase-invariance.\n    $$\n    \\Delta_{\\text{mod}} = \\frac{\\operatorname{std}_{\\phi \\in \\Phi_{\\text{eval}}} \\left[ y(\\phi) \\right]}{\\operatorname{mean}_{\\phi \\in \\Phi_{\\text{eval}}} \\left[ y(\\phi) \\right]}\n    $$\n\nThe combination of these metrics provides a comprehensive assessment of how successfully the optimization procedure learned the desired phase-invariant representation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_optimization(T, omega_pi, P, steps, lr, alpha, beta, seed=42):\n    \"\"\"\n    Optimizes two filters to form a phase-invariant energy model and evaluates them.\n\n    Args:\n        T (int): Time dimension of inputs and filters.\n        omega_pi (float): Angular frequency in units of pi (e.g., 0.3 means 0.3*pi).\n        P (int): Number of phases in the training dataset.\n        steps (int): Number of optimization steps.\n        lr (float): Learning rate.\n        alpha (float): Weight for the norm constraint penalty.\n        beta (float): Weight for the response equalization penalty.\n        seed (int): Random seed for deterministic initialization.\n\n    Returns:\n        tuple: A tuple containing the three evaluation metrics\n               (delta_phase, delta_mag, delta_mod).\n    \"\"\"\n    omega = omega_pi * np.pi\n\n    # Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # A. Generate training dataset\n    t = np.arange(T)\n    train_phases = np.linspace(0, 2 * np.pi, P, endpoint=False)\n    X_train = np.sin(omega * t[None, :] + train_phases[:, None])  # Shape (P, T)\n\n    # B. Initialize filters and optimizer state\n    # Initialize with small random values. Scale ensures variance is ~1/T.\n    f1 = rng.normal(size=T, scale=1.0 / np.sqrt(T))\n    f2 = rng.normal(size=T, scale=1.0 / np.sqrt(T))\n    \n    mu = 0.9  # Momentum coefficient\n    v1 = np.zeros(T)\n    v2 = np.zeros(T)\n\n    # C. Optimization loop\n    for _ in range(steps):\n        # 1. Forward pass: compute responses and energy\n        r1 = X_train @ f1  # Shape (P,)\n        r2 = X_train @ f2  # Shape (P,)\n        y = r1**2 + r2**2   # Shape (P,)\n\n        # 2. Compute intermediates for loss and gradients\n        y_mean = np.mean(y)\n        r1_ms = np.mean(r1**2)\n        r2_ms = np.mean(r2**2)\n        f_norm_sq = np.sum(f1**2) + np.sum(f2**2)\n\n        # 3. Compute gradients for each loss component\n        # Gradient of L_mod\n        y_dev = y - y_mean\n        dL_mod_df1 = (4 / P) * (X_train.T @ (y_dev * r1))\n        dL_mod_df2 = (4 / P) * (X_train.T @ (y_dev * r2))\n\n        # Gradient of L_norm\n        dL_norm_df1 = 4 * (f_norm_sq - 1.0) * f1\n        dL_norm_df2 = 4 * (f_norm_sq - 1.0) * f2\n        \n        # Gradient of L_eq\n        eq_diff = r1_ms - r2_ms\n        dL_eq_df1 = (4 / P) * eq_diff * (X_train.T @ r1)\n        dL_eq_df2 = -(4 / P) * eq_diff * (X_train.T @ r2)\n\n        # Total gradient with coefficients alpha and beta\n        grad_f1 = dL_mod_df1 + alpha * dL_norm_df1 + beta * dL_eq_df1\n        grad_f2 = dL_mod_df2 + alpha * dL_norm_df2 + beta * dL_eq_df2\n        \n        # 4. Update filters with momentum\n        v1 = mu * v1 + grad_f1\n        v2 = mu * v2 + grad_f2\n        f1 -= lr * v1\n        f2 -= lr * v2\n\n    # D. Evaluation of the learned filters\n    eps = 1e-9 # Small constant for numerical stability\n\n    # 1. Fourier coefficients and derived metrics\n    exp_vec = np.exp(-1j * omega * t)\n    c1_hat = np.dot(f1, exp_vec)\n    c2_hat = np.dot(f2, exp_vec)\n    \n    # Delta_phase\n    theta1 = np.angle(c1_hat)\n    theta2 = np.angle(c2_hat)\n    delta_phase = np.abs(np.mod(theta2 - theta1, np.pi) - np.pi / 2)\n\n    # Delta_mag\n    c1_abs = np.abs(c1_hat)\n    c2_abs = np.abs(c2_hat)\n    delta_mag = np.abs(c1_abs / (c2_abs + eps) - 1)\n\n    # 2. Residual modulation\n    P_eval = 256\n    eval_phases = np.linspace(0, 2 * np.pi, P_eval, endpoint=False)\n    X_eval = np.sin(omega * t[None, :] + eval_phases[:, None])\n    \n    r1_eval = X_eval @ f1\n    r2_eval = X_eval @ f2\n    y_eval = r1_eval**2 + r2_eval**2\n    \n    y_eval_mean = np.mean(y_eval)\n    delta_mod = np.std(y_eval) / (y_eval_mean + eps)\n\n    return delta_phase, delta_mag, delta_mod\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, omega_as_multiple_of_pi, P, steps, lr, alpha, beta)\n        (33, 0.3, 64, 2000, 0.02, 1.0, 0.1),\n        (33, 0.1, 64, 2000, 0.02, 1.0, 0.1),\n        (33, 0.45, 64, 2500, 0.015, 1.0, 0.1),\n        (33, 0.3, 8, 3000, 0.015, 1.0, 0.2),\n    ]\n\n    results = []\n    # Use a fixed seed for all optimization runs for reproducibility.\n    # The problem asks for deterministic initialization. Using the same\n    # seed for each test case means they all start from the same f1, f2.\n    fixed_seed = 42 \n    for case in test_cases:\n        T, omega_pi, P, steps, lr, alpha, beta = case\n        metrics = run_optimization(T, omega_pi, P, steps, lr, alpha, beta, seed=fixed_seed)\n        results.extend(metrics)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}