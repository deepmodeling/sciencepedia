{
    "hands_on_practices": [
        {
            "introduction": "A key feature of V1 complex cells is that their response is largely insensitive to the precise spatial position of a stimulus within their receptive field, a property known as phase invariance. The energy model provides a canonical explanation for this phenomenon. This exercise  will guide you through a first-principles derivation to show how summing the squared outputs of a quadrature pair of linear filters results in a response that is independent of a stimulus's spatial phase, a foundational concept in sensory processing.",
            "id": "3978675",
            "problem": "Consider a one-dimensional model neuron in primary visual cortex (V1) that is described by the classical energy model for a complex cell. The stimulus is a counterphase grating given by\n$$\ns(x,t) \\;=\\; A \\cos(k_0 x + \\phi)\\,\\cos(\\omega t),\n$$\nwhere $A$ is the contrast amplitude, $k_0$ is the spatial frequency, $\\phi$ is the spatial phase (in radians), and $\\omega$ is the temporal angular frequency (in radians per second). The neuron has two linear subunits with spatial receptive fields (RFs) $g_{e}(x)$ and $g_{o}(x)$ that form a spatial quadrature pair at spatial frequency $k_0$. The linear responses are\n$$\nr_{e}(t;\\phi)\\;=\\;\\int_{-\\infty}^{\\infty} g_{e}(x)\\,s(x,t)\\,dx,\\qquad\nr_{o}(t;\\phi)\\;=\\;\\int_{-\\infty}^{\\infty} g_{o}(x)\\,s(x,t)\\,dx,\n$$\nand the complex-cell output is the energy\n$$\nE(t;\\phi)\\;=\\;r_{e}(t;\\phi)^{2}\\;+\\;r_{o}(t;\\phi)^{2}.\n$$\nAssume both subunits are linear and time-invariant in space, that the quadrature property holds exactly at $k_0$ (equal gain and a spatial phase difference of $\\pi/2$), and that the temporal dependence of each linear response arises only from the stimulus $\\cos(\\omega t)$ factor. Define the time-averaged response over one temporal period $T = 2\\pi/\\omega$ by\n$$\n\\overline{E}(\\phi)\\;=\\;\\frac{1}{T}\\int_{0}^{T} E(t;\\phi)\\,dt,\n$$\nand define the phase invariance index\n$$\n\\mathrm{PI}\\;=\\;\\frac{R_{\\min}}{R_{\\max}},\n$$\nwhere $R_{\\min}$ and $R_{\\max}$ are, respectively, the minimum and maximum of $\\overline{E}(\\phi)$ over $\\phi \\in [0,2\\pi)$.\n\nUsing only first principles of linear systems and the quadrature property, do the following. First, formulate a falsifiable experimental prediction that distinguishes a simple cell from a complex cell under counterphase grating stimulation by specifying what would be different in the dependence of the time-averaged response on the spatial phase $\\phi$. Second, derive $\\overline{E}(\\phi)$ for the energy model neuron and compute the corresponding $\\mathrm{PI}$ for the ideal case described above.\n\nExpress your final answer as a dimensionless number. No rounding is required.",
            "solution": "The problem is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   Stimulus: $s(x,t) \\;=\\; A \\cos(k_0 x + \\phi)\\,\\cos(\\omega t)$\n-   Variables: contrast amplitude $A$, spatial frequency $k_0$, spatial phase $\\phi$, temporal angular frequency $\\omega$.\n-   Neuron Model: A complex cell described by the energy model.\n-   Subunits: Two linear subunits with spatial receptive fields (RFs) $g_{e}(x)$ and $g_{o}(x)$.\n-   Quadrature Property: At spatial frequency $k_0$, the subunits form a spatial quadrature pair, meaning they have equal gain and a spatial phase difference of $\\pi/2$.\n-   Linear Responses: $r_{e}(t;\\phi)\\;=\\;\\int_{-\\infty}^{\\infty} g_{e}(x)\\,s(x,t)\\,dx$ and $r_{o}(t;\\phi)\\;=\\;\\int_{-\\infty}^{\\infty} g_{o}(x)\\,s(x,t)\\,dx$.\n-   Complex-Cell Output (Energy): $E(t;\\phi)\\;=\\;r_{e}(t;\\phi)^{2}\\;+\\;r_{o}(t;\\phi)^{2}$.\n-   Assumptions:\n    1.  Subunits are linear and time-invariant in space.\n    2.  The temporal dependence of each linear response arises only from the stimulus factor $\\cos(\\omega t)$.\n-   Time-Averaged Response: $\\overline{E}(\\phi)\\;=\\;\\frac{1}{T}\\int_{0}^{T} E(t;\\phi)\\,dt$, with temporal period $T = 2\\pi/\\omega$.\n-   Phase Invariance Index: $\\mathrm{PI}\\;=\\;\\frac{R_{\\min}}{R_{\\max}}$, where $R_{\\min} = \\min_{\\phi} \\overline{E}(\\phi)$ and $R_{\\max} = \\max_{\\phi} \\overline{E}(\\phi)$ for $\\phi \\in [0,2\\pi)$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded:** The problem describes the canonical Adelson-Bergen energy model for a V1 complex cell, a cornerstone of computational neuroscience. The use of a counterphase grating as a stimulus is a standard technique in visual physiology. The problem is scientifically sound.\n-   **Well-Posed:** The problem is clearly defined, providing all necessary functions, definitions, and assumptions to derive the required quantities. The objectives are specific and lead to a unique solution.\n-   **Objective:** The problem is stated in precise, mathematical language, free from any subjective or ambiguous terminology.\n-   **Conclusion:** The problem statement is complete, consistent, and scientifically valid. It is well-posed and suitable for a rigorous solution.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n### Solution\n\nThe solution addresses the two parts of the problem: first, formulating a falsifiable prediction to distinguish simple and complex cells, and second, deriving the time-averaged response $\\overline{E}(\\phi)$ and computing the Phase Invariance Index $(\\mathrm{PI})$ for the ideal complex cell model.\n\n**Part 1: Falsifiable Prediction to Distinguish Simple and Complex Cells**\n\nA simple cell is typically modeled as a single linear filter followed by a static nonlinearity, for example, rectification or squaring. Its response is highly sensitive to the spatial phase of the stimulus. In contrast, the energy model for a complex cell pools the squared outputs of multiple linear filters with different phase preferences. This pooling is hypothesized to create a response that is invariant to the spatial phase. This difference leads to a clear experimental prediction.\n\nA simple cell's response could be modeled as $S(t;\\phi) = f(r_e(t;\\phi))$ for some single linear response $r_e(t;\\phi)$ and a nonlinear function $f$. As will be shown, $r_e(t;\\phi)$ is proportional to $\\cos(\\phi)$. Thus, its time-averaged response $\\overline{S}(\\phi)$ would be strongly modulated by $\\phi$, peaking at a preferred phase and being null at other phases. For instance, if $f(z)=z^2$, the time-averaged simple cell response would be proportional to $\\cos^2(\\phi)$, for which the ratio of minimum to maximum response is $0$.\n\nA complex cell's response is given by $E(t;\\phi)$. The derivation below will show that its time-averaged response $\\overline{E}(\\phi)$ is constant.\n\n**Falsifiable Prediction:** An experiment can be designed where a V1 neuron is presented with a counterphase grating stimulus $s(x,t) = A \\cos(k_0 x + \\phi)\\cos(\\omega t)$ at its preferred spatial frequency $k_0$. The neuron's time-averaged firing rate (serving as a measure of the response) is recorded as the spatial phase $\\phi$ is varied over the range $[0, 2\\pi)$.\n-   **If the cell is a simple cell**, its time-averaged response will show strong modulation with $\\phi$, exhibiting clear peaks and troughs. The phase invariance index $\\mathrm{PI}$ would be close to $0$.\n-   **If the cell is an ideal complex cell**, its time-averaged response will be constant, showing no modulation with $\\phi$. The phase invariance index $\\mathrm{PI}$ would be $1$.\nThis provides a direct, quantitative, and falsifiable method for classifying a neuron as simple or complex based on its response to this stimulus.\n\n**Part 2: Derivation of $\\overline{E}(\\phi)$ and the Phase Invariance Index $(\\mathrm{PI})$**\n\nWe begin by calculating the linear responses $r_{e}(t;\\phi)$ and $r_{o}(t;\\phi)$. Given the stimulus $s(x,t) = A \\cos(k_0 x + \\phi) \\cos(\\omega t)$ and the assumption that the temporal dynamics arise only from the stimulus, we can factor out the temporal component from the spatial integration.\n$$\nr_{j}(t;\\phi) = \\int_{-\\infty}^{\\infty} g_{j}(x) A \\cos(k_0 x + \\phi) \\cos(\\omega t) dx = A \\cos(\\omega t) \\int_{-\\infty}^{\\infty} g_j(x) \\cos(k_0 x + \\phi) dx\n$$\nfor $j \\in \\{e, o\\}$. Let's analyze the spatial integral term by expanding the cosine:\n$$\n\\int_{-\\infty}^{\\infty} g_j(x) [\\cos(k_0 x)\\cos(\\phi) - \\sin(k_0 x)\\sin(\\phi)] dx = \\cos(\\phi)\\int_{-\\infty}^{\\infty} g_j(x)\\cos(k_0 x)dx - \\sin(\\phi)\\int_{-\\infty}^{\\infty} g_j(x)\\sin(k_0 x)dx\n$$\nThe quadrature property implies that $g_e(x)$ is an even-symmetric filter (like a cosine Gabor) and $g_o(x)$ is an odd-symmetric filter (like a sine Gabor). For an even function $g_e(x)$, the integral of $g_e(x)\\sin(k_0 x)$ over $(-\\infty, \\infty)$ is zero because the integrand is odd. For an odd function $g_o(x)$, the integral of $g_o(x)\\cos(k_0 x)$ is zero because the integrand is odd.\nThis simplifies the spatial components of the responses:\n-   For $r_e(t;\\phi)$: The term with $\\sin(\\phi)$ vanishes. Let $C_e = \\int_{-\\infty}^{\\infty} g_e(x)\\cos(k_0 x)dx$. The response becomes $r_e(t;\\phi) = A C_e \\cos(\\phi) \\cos(\\omega t)$.\n-   For $r_o(t;\\phi)$: The term with $\\cos(\\phi)$ vanishes. Let $S_o = \\int_{-\\infty}^{\\infty} g_o(x)\\sin(k_0 x)dx$. The response becomes $r_o(t;\\phi) = -A S_o \\sin(\\phi) \\cos(\\omega t)$.\n\nThe quadrature property also states that the filters have \"equal gain\". This translates to the magnitude of their responses to their preferred inputs being equal. The constant $C_e$ represents the response of the even filter to a cosine grating, and $S_o$ represents the response of the odd filter to a sine grating. Equal gain implies $|C_e| = |S_o|$. Let us define this gain as $K > 0$. By adjusting the signs of $g_e$ and $g_o$ if necessary, we can set $C_e = K$ and $S_o = K$.\nThe linear responses are thus:\n$$\nr_e(t;\\phi) = A K \\cos(\\phi) \\cos(\\omega t)\n$$\n$$\nr_o(t;\\phi) = -A K \\sin(\\phi) \\cos(\\omega t)\n$$\nNow, we compute the energy $E(t;\\phi) = r_e(t;\\phi)^2 + r_o(t;\\phi)^2$:\n$$\nE(t;\\phi) = [A K \\cos(\\phi) \\cos(\\omega t)]^2 + [-A K \\sin(\\phi) \\cos(\\omega t)]^2\n$$\n$$\nE(t;\\phi) = (A K)^2 \\cos^2(\\omega t) \\cos^2(\\phi) + (A K)^2 \\cos^2(\\omega t) \\sin^2(\\phi)\n$$\nFactoring out the common terms:\n$$\nE(t;\\phi) = (A K)^2 \\cos^2(\\omega t) [\\cos^2(\\phi) + \\sin^2(\\phi)]\n$$\nUsing the trigonometric identity $\\cos^2(\\phi) + \\sin^2(\\phi) = 1$:\n$$\nE(t;\\phi) = (A K)^2 \\cos^2(\\omega t)\n$$\nNotably, the instantaneous energy $E(t;\\phi)$ is independent of the spatial phase $\\phi$.\n\nNext, we compute the time-averaged response $\\overline{E}(\\phi)$ over one temporal period $T=2\\pi/\\omega$:\n$$\n\\overline{E}(\\phi) = \\frac{1}{T} \\int_0^T E(t;\\phi) dt = \\frac{1}{T} \\int_0^T (A K)^2 \\cos^2(\\omega t) dt\n$$\nThe average value of $\\cos^2(\\theta)$ over a full period is $1/2$. Formally:\n$$\n\\frac{1}{T} \\int_0^T \\cos^2(\\omega t) dt = \\frac{1}{T} \\int_0^T \\frac{1+\\cos(2\\omega t)}{2} dt = \\frac{1}{2T} \\left[t + \\frac{\\sin(2\\omega t)}{2\\omega}\\right]_0^T\n$$\n$$\n= \\frac{1}{2T} \\left[T + \\frac{\\sin(2\\omega (2\\pi/\\omega))}{2\\omega} - 0\\right] = \\frac{1}{2T} \\left[T + \\frac{\\sin(4\\pi)}{2\\omega}\\right] = \\frac{1}{2}\n$$\nTherefore, the time-averaged response is:\n$$\n\\overline{E}(\\phi) = (A K)^2 \\cdot \\frac{1}{2} = \\frac{(A K)^2}{2}\n$$\nThe time-averaged response $\\overline{E}(\\phi)$ is a constant, independent of the spatial phase $\\phi$.\n\nFinally, we compute the Phase Invariance Index, $\\mathrm{PI} = R_{\\min}/R_{\\max}$:\n$$\nR_{\\max} = \\max_{\\phi \\in [0,2\\pi)} \\overline{E}(\\phi) = \\max_{\\phi} \\frac{(A K)^2}{2} = \\frac{(A K)^2}{2}\n$$\n$$\nR_{\\min} = \\min_{\\phi \\in [0,2\\pi)} \\overline{E}(\\phi) = \\min_{\\phi} \\frac{(A K)^2}{2} = \\frac{(A K)^2}{2}\n$$\n$$\n\\mathrm{PI} = \\frac{R_{\\min}}{R_{\\max}} = \\frac{(A K)^2 / 2}{(A K)^2 / 2} = 1\n$$\nFor an ideal complex cell described by the energy model, the phase invariance index is exactly $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "While the energy model provides a powerful forward account of complex cell responses, a central task in computational neuroscience is the inverse problem: identifying a model's parameters from recorded neural data. This practice  focuses on the mathematical foundation for learning the energy model's filters using gradient-based optimization. By deriving the gradients of the energy function with respect to both the stimulus and the filter weights, you will uncover the core mechanism for data-driven system identification.",
            "id": "3978680",
            "problem": "Consider a standard energy model of a primary visual cortex (V1) complex cell. Let the stimulus be a vector $x \\in \\mathbb{R}^{n}$ and let the two linear subunit filters be $h_{1} \\in \\mathbb{R}^{n}$ and $h_{2} \\in \\mathbb{R}^{n}$. The subunit responses are the inner products $y_{1} = h_{1}^{\\top} x$ and $y_{2} = h_{2}^{\\top} x$. The model’s energy output is defined as\n$$\nE(x) = y_{1}^{2} + y_{2}^{2} = \\left(h_{1}^{\\top} x\\right)^{2} + \\left(h_{2}^{\\top} x\\right)^{2}.\n$$\nStarting from the core definitions of linear filtering as an inner product and the chain rule for differentiation of composite functions, derive closed-form expressions for the gradients of $E(x)$ with respect to the stimulus $x$ and with respect to the filter parameters $h_{1}$ and $h_{2}$. Then, briefly explain how these gradients can be used to perform system identification of the filters from data consisting of measured complex-cell responses to known stimuli using gradient-based optimization.\n\nProvide your final answer for the three gradients in a single row matrix in the order $\\left(\\nabla_{x} E, \\nabla_{h_{1}} E, \\nabla_{h_{2}} E\\right)$. No numerical rounding is required, and no physical units apply. Express your final answer as a simplified, closed-form analytic expression.",
            "solution": "The problem is valid as it is scientifically grounded in the established energy model of V1 complex cells, is mathematically well-posed, objective, and self-contained. The task is to derive standard gradients for this model and explain their application in system identification, which is a core concept in computational neuroscience.\n\nThe energy output of the V1 complex cell model is given by:\n$$\nE(x) = \\left(h_{1}^{\\top} x\\right)^{2} + \\left(h_{2}^{\\top} x\\right)^{2}\n$$\nwhere $x \\in \\mathbb{R}^{n}$ is the stimulus vector and $h_{1}, h_{2} \\in \\mathbb{R}^{n}$ are the linear filter vectors. To derive the gradients, we will apply the chain rule of differentiation for vector-valued functions. For a composite function $f(g(z))$, the gradient with respect to $z$ is $\\nabla_z f = \\frac{\\partial f}{\\partial g} \\nabla_z g$. We also use two standard vector calculus identities: $\\nabla_z(a^\\top z) = a$ and $\\nabla_z(z^\\top a) = a$.\n\n1.  **Gradient with respect to the stimulus $x$:** $\\nabla_{x} E(x)$\n\nThe energy $E(x)$ is a sum of two terms. The gradient of a sum is the sum of the gradients. We compute the gradient of each term with respect to $x$ separately.\n\nFor the first term, let $y_{1}(x) = h_{1}^{\\top} x$. The term is $y_{1}^{2}$. Using the chain rule:\n$$\n\\nabla_{x} \\left(y_{1}^{2}\\right) = \\frac{d(y_{1}^{2})}{dy_{1}} \\nabla_{x} y_{1}\n$$\nThe derivative of $y_{1}^{2}$ with respect to $y_{1}$ is $2y_{1}$. The gradient of the linear function $y_{1}(x) = h_{1}^{\\top} x$ with respect to $x$ is:\n$$\n\\nabla_{x} \\left(h_{1}^{\\top} x\\right) = h_{1}\n$$\nCombining these, the gradient of the first term is:\n$$\n\\nabla_{x} \\left(\\left(h_{1}^{\\top} x\\right)^{2}\\right) = 2y_{1} h_{1} = 2\\left(h_{1}^{\\top} x\\right) h_{1}\n$$\nBy symmetry, the gradient of the second term, $\\left(h_{2}^{\\top} x\\right)^{2}$, is:\n$$\n\\nabla_{x} \\left(\\left(h_{2}^{\\top} x\\right)^{2}\\right) = 2\\left(h_{2}^{\\top} x\\right) h_{2}\n$$\nThe total gradient of $E(x)$ with respect to $x$ is the sum of these two results:\n$$\n\\nabla_{x} E(x) = 2\\left(h_{1}^{\\top} x\\right) h_{1} + 2\\left(h_{2}^{\\top} x\\right) h_{2}\n$$\n\n2.  **Gradient with respect to the filter $h_{1}$:** $\\nabla_{h_{1}} E$\n\nTo find the gradient with respect to $h_{1}$, we note that the second term, $\\left(h_{2}^{\\top} x\\right)^{2}$, does not depend on $h_{1}$, so its gradient is the zero vector. We only need to differentiate the first term, $\\left(h_{1}^{\\top} x\\right)^{2}$.\n\nLet $y_{1}(h_{1}) = h_{1}^{\\top} x$. Using the chain rule for the gradient with respect to $h_{1}$:\n$$\n\\nabla_{h_{1}} \\left(y_{1}^{2}\\right) = \\frac{d(y_{1}^{2})}{dy_{1}} \\nabla_{h_{1}} y_{1} = 2y_{1} \\nabla_{h_{1}} y_{1}\n$$\nTo find the gradient of $y_{1}(h_{1}) = h_{1}^{\\top} x$, we can rewrite the inner product as $x^{\\top}h_{1}$. The gradient of this linear function with respect to $h_{1}$ is:\n$$\n\\nabla_{h_{1}} \\left(x^{\\top} h_{1}\\right) = x\n$$\nSubstituting this back, we obtain the gradient of $E$ with respect to $h_{1}$:\n$$\n\\nabla_{h_{1}} E = 2y_{1} x = 2\\left(h_{1}^{\\top} x\\right) x\n$$\n\n3.  **Gradient with respect to the filter $h_{2}$:** $\\nabla_{h_{2}} E$\n\nThe derivation for $\\nabla_{h_{2}} E$ is symmetric to the one for $\\nabla_{h_{1}} E$. The first term, $\\left(h_{1}^{\\top} x\\right)^{2}$, does not depend on $h_{2}$. We differentiate the second term, $\\left(h_{2}^{\\top} x\\right)^{2}$, with respect to $h_{2}$.\n$$\n\\nabla_{h_{2}} E = \\nabla_{h_{2}} \\left(\\left(h_{2}^{\\top} x\\right)^{2}\\right)\n$$\nFollowing the same logic as for $h_{1}$:\n$$\n\\nabla_{h_{2}} \\left(\\left(h_{2}^{\\top} x\\right)^{2}\\right) = 2\\left(h_{2}^{\\top} x\\right) \\nabla_{h_{2}}\\left(h_{2}^{\\top} x\\right) = 2\\left(h_{2}^{\\top} x\\right) x\n$$\nThus, the gradient of $E$ with respect to $h_{2}$ is:\n$$\n\\nabla_{h_{2}} E = 2\\left(h_{2}^{\\top} x\\right) x\n$$\n\n**System Identification using Gradients**\n\nSystem identification is the process of building a mathematical model of a system from experimental data. In this context, it means finding the filter parameters $h_{1}$ and $h_{2}$ that best explain the observed responses of a V1 complex cell to a set of known stimuli.\n\nThe derived gradients, $\\nabla_{h_{1}} E$ and $\\nabla_{h_{2}} E$, are essential for this process when using gradient-based optimization methods. The procedure is as follows:\n\n1.  **Define a Loss Function:** First, a loss function $L(h_{1}, h_{2})$ is defined to quantify the discrepancy between the model's predicted energy output $E(x^{(i)})$ and the actual measured neural responses $r^{(i)}$ for a dataset of stimulus-response pairs $\\{ (x^{(i)}, r^{(i)}) \\}_{i=1}^{N}$. A common choice is the mean squared error (MSE):\n    $$\n    L(h_{1}, h_{2}) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(r^{(i)} - E(x^{(i)})\\right)^{2}\n    $$\n2.  **Gradient-Based Optimization:** The goal is to find the parameters $h_{1}$ and $h_{2}$ that minimize this loss function. Gradient descent is an iterative algorithm that accomplishes this by repeatedly updating the parameters in the direction opposite to the gradient of the loss function. The update rules for each filter are:\n    $$\n    h_{1} \\leftarrow h_{1} - \\alpha \\nabla_{h_{1}} L \\\\\n    h_{2} \\leftarrow h_{2} - \\alpha \\nabla_{h_{2}} L\n    $$\n    where $\\alpha$ is a small positive number called the learning rate.\n\n3.  **Calculate Loss Gradients:** The gradients of the loss function, $\\nabla_{h_{1}} L$ and $\\nabla_{h_{2}} L$, are computed using the chain rule, which requires the gradients of the energy model $E$ that we derived. For example, for $h_{1}$:\n    $$\n    \\nabla_{h_{1}} L = \\frac{\\partial L}{\\partial E} \\nabla_{h_{1}} E\n    $$\n    The gradient of the MSE loss with respect to $h_{1}$ for a single data point $(x, r)$ is:\n    $$\n    \\nabla_{h_{1}} L = \\frac{\\partial}{\\partial h_{1}} \\left(r - E(x)\\right)^{2} = 2(r - E(x))(-\\nabla_{h_{1}} E) = -2(r - E(x)) \\left(2(h_{1}^{\\top} x) x\\right)\n    $$\n    A similar expression holds for $\\nabla_{h_{2}} L$. By averaging these gradients over the dataset and applying the update rules iteratively, the algorithm converges to a set of filters $h_{1}$ and $h_{2}$ that optimally characterize the complex cell's response properties.",
            "answer": "$$\n\\boxed{\\pmatrix{ 2\\left(h_{1}^{\\top} x\\right) h_{1} + 2\\left(h_{2}^{\\top} x\\right) h_{2} & 2\\left(h_{1}^{\\top} x\\right) x & 2\\left(h_{2}^{\\top} x\\right) x }}\n$$"
        },
        {
            "introduction": "Beyond gradient-based methods, other statistical techniques can reveal the structure of neural computations. This advanced exercise  explores Spike-Triggered Covariance (STC), a powerful method for identifying the subspace spanned by a neuron's relevant stimulus features. You will derive how the filters of an energy model neuron emerge as eigenvectors of the STC matrix and, crucially, quantify how this recovery is systematically affected by physiological noise.",
            "id": "3978719",
            "problem": "Consider a neuron in primary visual cortex (V1) modeled as an energy model complex cell with two linear subunits. Let the stimulus be a $d$-dimensional random vector $s \\in \\mathbb{R}^{d}$ drawn independently from a zero-mean multivariate Gaussian with identity covariance, that is $s \\sim \\mathcal{N}(0, I_{d})$. The two linear subunits are defined by filters $k_{1}, k_{2} \\in \\mathbb{R}^{d}$. The subunit outputs are contaminated by additive, independent, zero-mean Gaussian noise $\\epsilon_{1}, \\epsilon_{2} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$, independent of $s$ and of each other, so that the noisy subunit outputs are $x_{i} = k_{i}^{\\top} s + \\epsilon_{i}$ for $i \\in \\{1,2\\}$. The neuron’s instantaneous firing rate is given by the quadratic energy $r(s) = x_{1}^{2} + x_{2}^{2}$. Spiking is modeled as an inhomogeneous Poisson process with rate $r(s)$. Define the Spike-Triggered Covariance (STC) as $C_{\\mathrm{STC}} = \\mathbb{E}[s s^{\\top} \\mid \\text{spike}] - \\mathbb{E}[s s^{\\top}]$. Under the Poisson assumption in this Gaussian stimulus regime, $p(s \\mid \\text{spike}) \\propto r(s) p(s)$, so that $\\mathbb{E}[s s^{\\top} \\mid \\text{spike}] = \\mathbb{E}[r(s) s s^{\\top}] / \\mathbb{E}[r(s)]$.\n\nStarting from these definitions and using properties of Gaussian moments, derive $C_{\\mathrm{STC}}$ as a function of $k_{1}$, $k_{2}$, and $\\sigma_{\\epsilon}^{2}$. Under the additional assumption that $k_{1}$ and $k_{2}$ are orthogonal and have unit norm, identify the nonzero eigenvalues and associated eigenvectors of $C_{\\mathrm{STC}}$.\n\nFinally, quantify the degradation in subunit recovery due to the additive output noise by computing the multiplicative factor by which the nonzero STC eigenvalues are reduced relative to the noiseless case (i.e., $\\sigma_{\\epsilon}^{2} = 0$). Express this factor as a single closed-form analytic expression in terms of $\\|k_{1}\\|^{2}$, $\\|k_{2}\\|^{2}$, and $\\sigma_{\\epsilon}^{2}$. No numerical approximation is required, and no units are involved in the final expression.",
            "solution": "The problem asks for the derivation of the Spike-Triggered Covariance ($C_{\\mathrm{STC}}$) for an energy model complex cell, an analysis of its eigenspectrum under specific conditions, and a quantification of the effect of noise.\n\nThe Spike-Triggered Covariance is defined as $C_{\\mathrm{STC}} = \\mathbb{E}[s s^{\\top} \\mid \\text{spike}] - \\mathbb{E}[s s^{\\top}]$. The stimulus $s$ is drawn from a standard multivariate Gaussian distribution, $s \\sim \\mathcal{N}(0, I_{d})$, so its prior covariance is $\\mathbb{E}[s s^{\\top}] = I_{d}$. The problem provides the expression for the spike-triggered expectation under a Poisson spiking assumption:\n$$\n\\mathbb{E}[s s^{\\top} \\mid \\text{spike}] = \\frac{\\mathbb{E}[r(s) s s^{\\top}]}{\\mathbb{E}[r(s)]}\n$$\nThe firing rate is given by $r(s) = x_{1}^{2} + x_{2}^{2}$, where $x_{i} = k_{i}^{\\top} s + \\epsilon_{i}$ for $i \\in \\{1,2\\}$. The noise terms $\\epsilon_{1}, \\epsilon_{2}$ are independent and identically distributed as $\\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$, and are independent of the stimulus $s$.\n\nThe expression for $C_{\\mathrm{STC}}$ is therefore:\n$$\nC_{\\mathrm{STC}} = \\frac{\\mathbb{E}[r(s) s s^{\\top}]}{\\mathbb{E}[r(s)]} - I_d\n$$\nTo solve this, we compute the two expectation terms, $\\mathbb{E}[r(s)]$ and $\\mathbb{E}[r(s) s s^{\\top}]$, separately.\n\nFirst, we compute the expected firing rate, $\\mathbb{E}[r(s)]$. By linearity of expectation, $\\mathbb{E}[r(s)] = \\mathbb{E}[x_1^2] + \\mathbb{E}[x_2^2]$. We compute the second moment $\\mathbb{E}[x_i^2]$ for $i \\in \\{1, 2\\}$. The variable $x_i = k_i^\\top s + \\epsilon_i$ is a sum of two independent, zero-mean Gaussian variables, so $x_i$ itself is a zero-mean Gaussian variable, $\\mathbb{E}[x_i] = k_i^\\top \\mathbb{E}[s] + \\mathbb{E}[\\epsilon_i] = k_i^\\top 0 + 0 = 0$.\nThe expectation $\\mathbb{E}[x_i^2]$ is the variance of $x_i$:\n$$\n\\mathbb{E}[x_i^2] = \\mathrm{Var}(x_i) = \\mathrm{Var}(k_i^\\top s + \\epsilon_i)\n$$\nDue to the independence of $s$ and $\\epsilon_i$, the variance of the sum is the sum of variances:\n$$\n\\mathrm{Var}(k_i^\\top s + \\epsilon_i) = \\mathrm{Var}(k_i^\\top s) + \\mathrm{Var}(\\epsilon_i)\n$$\nThe variance of the noise is given as $\\mathrm{Var}(\\epsilon_i) = \\sigma_\\epsilon^2$. The variance of the filtered stimulus is computed as:\n$$\n\\mathrm{Var}(k_i^\\top s) = \\mathbb{E}[(k_i^\\top s)^2] - (\\mathbb{E}[k_i^\\top s])^2 = \\mathbb{E}[k_i^\\top s s^\\top k_i] - 0 = k_i^\\top \\mathbb{E}[s s^\\top] k_i\n$$\nSince $\\mathbb{E}[s s^\\top] = I_d$, we have $\\mathrm{Var}(k_i^\\top s) = k_i^\\top I_d k_i = k_i^\\top k_i = \\|k_i\\|^2$.\nTherefore, $\\mathbb{E}[x_i^2] = \\|k_i\\|^2 + \\sigma_\\epsilon^2$. Summing over the two subunits gives the mean firing rate:\n$$\n\\mathbb{E}[r(s)] = (\\|k_1\\|^2 + \\sigma_\\epsilon^2) + (\\|k_2\\|^2 + \\sigma_\\epsilon^2) = \\|k_1\\|^2 + \\|k_2\\|^2 + 2\\sigma_\\epsilon^2\n$$\n\nNext, we compute the numerator term $\\mathbb{E}[r(s) s s^{\\top}]$ in the expression for the spike-triggered covariance.\n$$\n\\mathbb{E}[r(s) s s^{\\top}] = \\mathbb{E}[(x_1^2 + x_2^2) s s^{\\top}] = \\mathbb{E}[x_1^2 s s^{\\top}] + \\mathbb{E}[x_2^2 s s^{\\top}]\n$$\nLet's analyze the first term, expanding $x_1^2$:\n$$\n\\mathbb{E}[x_1^2 s s^{\\top}] = \\mathbb{E}[(k_1^\\top s + \\epsilon_1)^2 s s^{\\top}] = \\mathbb{E}[((k_1^\\top s)^2 + 2\\epsilon_1(k_1^\\top s) + \\epsilon_1^2) s s^{\\top}]\n$$\nUsing the linearity of expectation, we evaluate each component:\n1. $\\mathbb{E}[\\epsilon_1^2 s s^{\\top}]$: Since $\\epsilon_1$ and $s$ are independent, this is $\\mathbb{E}[\\epsilon_1^2]\\mathbb{E}[s s^{\\top}] = \\sigma_\\epsilon^2 I_d$.\n2. $\\mathbb{E}[2\\epsilon_1(k_1^\\top s) s s^{\\top}]$: Since $\\epsilon_1$ is independent of $s$ and has zero mean, this term is $2\\mathbb{E}[\\epsilon_1]\\mathbb{E}[(k_1^\\top s) s s^{\\top}] = 0$.\n3. $\\mathbb{E}[(k_1^\\top s)^2 s s^{\\top}]$: This fourth-order moment of the standard Gaussian vector $s$ can be evaluated using Isserlis' theorem (or Wick's theorem), which for a zero-mean Gaussian vector $s$ and a vector $u$ yields the identity $\\mathbb{E}[(u^\\top s)^2 s s^\\top] = \\|u\\|^2 I_d + 2 u u^\\top$. Setting $u=k_1$, we have:\n$$\n\\mathbb{E}[(k_1^\\top s)^2 s s^{\\top}] = \\|k_1\\|^2 I_d + 2 k_1 k_1^\\top\n$$\nCombining these three parts gives:\n$$\n\\mathbb{E}[x_1^2 s s^{\\top}] = (\\|k_1\\|^2 I_d + 2 k_1 k_1^\\top) + 0 + \\sigma_\\epsilon^2 I_d = (\\|k_1\\|^2 + \\sigma_\\epsilon^2) I_d + 2 k_1 k_1^\\top\n$$\nBy symmetry, the expression for the second subunit is analogous:\n$$\n\\mathbb{E}[x_2^2 s s^{\\top}] = (\\|k_2\\|^2 + \\sigma_\\epsilon^2) I_d + 2 k_2 k_2^\\top\n$$\nSumming these results gives the full numerator:\n$$\n\\mathbb{E}[r(s) s s^{\\top}] = (\\|k_1\\|^2 + \\|k_2\\|^2 + 2\\sigma_\\epsilon^2) I_d + 2(k_1 k_1^\\top + k_2 k_2^\\top)\n$$\nThe scalar pre-factor of the identity matrix $I_d$ is identical to our previously calculated $\\mathbb{E}[r(s)]$. So, $\\mathbb{E}[r(s) s s^{\\top}] = \\mathbb{E}[r(s)] I_d + 2(k_1 k_1^\\top + k_2 k_2^\\top)$.\n\nNow we assemble the expression for $C_{\\mathrm{STC}}$:\n$$\nC_{\\mathrm{STC}} = \\frac{\\mathbb{E}[r(s)] I_d + 2(k_1 k_1^\\top + k_2 k_2^\\top)}{\\mathbb{E}[r(s)]} - I_d = I_d + \\frac{2(k_1 k_1^\\top + k_2 k_2^\\top)}{\\mathbb{E}[r(s)]} - I_d\n$$\n$$\nC_{\\mathrm{STC}} = \\frac{2(k_1 k_1^\\top + k_2 k_2^\\top)}{\\|k_1\\|^2 + \\|k_2\\|^2 + 2\\sigma_\\epsilon^2}\n$$\nThis is the general expression for $C_{\\mathrm{STC}}$.\n\nNext, we find the nonzero eigenvalues and eigenvectors under the assumptions that $k_1$ and $k_2$ are orthogonal ($k_1^\\top k_2 = 0$) and have unit norm ($\\|k_1\\|^2 = 1$, $\\|k_2\\|^2 = 1$). The denominator becomes $1 + 1 + 2\\sigma_\\epsilon^2 = 2(1 + \\sigma_\\epsilon^2)$, so:\n$$\nC_{\\mathrm{STC}} = \\frac{2(k_1 k_1^\\top + k_2 k_2^\\top)}{2(1 + \\sigma_\\epsilon^2)} = \\frac{1}{1 + \\sigma_\\epsilon^2}(k_1 k_1^\\top + k_2 k_2^\\top)\n$$\nTo find the eigenvectors, we test $k_1$:\n$$\nC_{\\mathrm{STC}} k_1 = \\frac{1}{1 + \\sigma_\\epsilon^2}(k_1 k_1^\\top k_1 + k_2 k_2^\\top k_1) = \\frac{1}{1 + \\sigma_\\epsilon^2}(k_1 \\|k_1\\|^2 + k_2 (0)) = \\frac{1}{1 + \\sigma_\\epsilon^2} k_1\n$$\nThus, $k_1$ is an eigenvector with eigenvalue $\\lambda_1 = \\frac{1}{1 + \\sigma_\\epsilon^2}$. By symmetry, testing $k_2$ yields:\n$$\nC_{\\mathrm{STC}} k_2 = \\frac{1}{1 + \\sigma_\\epsilon^2}(k_1 k_1^\\top k_2 + k_2 k_2^\\top k_2) = \\frac{1}{1 + \\sigma_\\epsilon^2}(k_1(0) + k_2 \\|k_2\\|^2) = \\frac{1}{1 + \\sigma_\\epsilon^2} k_2\n$$\nThus, $k_2$ is an eigenvector with the same eigenvalue, $\\lambda_2 = \\frac{1}{1 + \\sigma_\\epsilon^2}$. Any vector $v$ in the subspace orthogonal to both $k_1$ and $k_2$ is an eigenvector with eigenvalue $0$. The two nonzero eigenvalues are thus degenerate and equal to $\\frac{1}{1 + \\sigma_\\epsilon^2}$. The associated eigenvectors are any non-zero vectors in the subspace spanned by $\\{k_1, k_2\\}$.\n\nFinally, we find the multiplicative factor by which the nonzero eigenvalues are reduced due to noise. This factor is the ratio of the noisy eigenvalue to the noiseless eigenvalue, $\\frac{\\lambda_{\\text{noisy}}}{\\lambda_{\\text{noiseless}}}$. We use the general result for orthogonal filters $k_1, k_2$ without assuming unit norm, as the question asks for the result in terms of $\\|k_1\\|^2$ and $\\|k_2\\|^2$. The nonzero eigenvalues, associated with eigenvectors $k_1$ and $k_2$, are:\n$$\n\\lambda_{1, \\text{noisy}} = \\frac{2\\|k_1\\|^2}{\\|k_1\\|^2 + \\|k_2\\|^2 + 2\\sigma_\\epsilon^2} \\quad \\text{and} \\quad \\lambda_{2, \\text{noisy}} = \\frac{2\\|k_2\\|^2}{\\|k_1\\|^2 + \\|k_2\\|^2 + 2\\sigma_\\epsilon^2}\n$$\nIn the noiseless case ($\\sigma_\\epsilon^2=0$), these become:\n$$\n\\lambda_{1, \\text{noiseless}} = \\frac{2\\|k_1\\|^2}{\\|k_1\\|^2 + \\|k_2\\|^2} \\quad \\text{and} \\quad \\lambda_{2, \\text{noiseless}} = \\frac{2\\|k_2\\|^2}{\\|k_1\\|^2 + \\|k_2\\|^2}\n$$\nThe reduction factor for the first eigenvalue is the ratio:\n$$\n\\frac{\\lambda_{1, \\text{noisy}}}{\\lambda_{1, \\text{noiseless}}} = \\frac{\\frac{2\\|k_1\\|^2}{\\|k_1\\|^2 + \\|k_2\\|^2 + 2\\sigma_\\epsilon^2}}{\\frac{2\\|k_1\\|^2}{\\|k_1\\|^2 + \\|k_2\\|^2}} = \\frac{\\|k_1\\|^2 + \\|k_2\\|^2}{\\|k_1\\|^2 + \\|k_2\\|^2 + 2\\sigma_\\epsilon^2}\n$$\nThe factor is identical for the second eigenvalue. This single factor represents the degradation of the signal carried by the STC eigenvalues due to the presence of output noise.",
            "answer": "$$\\boxed{\\frac{\\|k_{1}\\|^{2} + \\|k_{2}\\|^{2}}{\\|k_{1}\\|^{2} + \\|k_{2}\\|^{2} + 2\\sigma_{\\epsilon}^{2}}}$$"
        }
    ]
}