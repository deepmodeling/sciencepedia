{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp Marr's framework, one must practice applying it to real-world neural computations. We begin with a foundational exercise focused on a classic problem in sensory neuroscience: how the brain determines the location of a sound source. This practice  challenges you to dissect this complex task, accurately mapping the distinct goals of explanation for the computational, algorithmic, and implementational levels. Successfully completing this exercise demonstrates a firm understanding of the core distinctions that define Marr's approach.",
            "id": "3995698",
            "problem": "A research team aims to model human horizontal sound localization by mapping binaural cues to azimuth. Let the inputs be $X$ representing binaural cues such as Interaural Time Difference (ITD) and Interaural Level Difference (ILD), and let the outputs be $Y$ representing the perceived azimuth. The task is to differentiate the targets of explanation at each of David Marr’s three levels of analysis (computational, algorithmic, implementational) for the mapping from $X$ to $Y$, and to state what constitutes a successful specification at each level. Choose the option that most accurately and completely characterizes (i) what each level is explaining for the task $X \\mapsto Y$ and (ii) what counts as success at that level, using the following context-appropriate base: the computational level characterizes the goal and constraints of the task, the algorithmic level characterizes the representation of $X$ and $Y$ and the procedures that compute the mapping, and the implementational level characterizes the physical substrate that realizes those procedures.\n\nA. Computational level: specify the functional goal of mapping $X$ to $Y$ and the environmental and sensory constraints that make this goal well-posed, including the criterion by which solutions are judged (for example, minimizing azimuth error under plausible sensory noise and scene statistics). Success: predictions from the specified goal correspond to psychophysical performance across conditions. Algorithmic level: specify representational choices for $X$ and $Y$ (for example, using phase and amplitude features of binaural signals) and a finite, well-defined procedure that transforms those representations to yield $Y$ (for example, temporal cross-correlation and a decision rule), together with resource trade-offs. Success: the procedure demonstrably and reproducibly computes the intended mapping from $X$ to $Y$ under the stated constraints. Implementational level: specify neural substrates and biophysical mechanisms that realize the procedure (for example, brainstem circuits including the Medial Superior Olive (MSO) and Lateral Superior Olive (LSO), their connectivity, and synaptic dynamics). Success: empirical evidence shows that these substrates causally implement the procedure and generate behavior consistent with the computational specification.\n\nB. Computational level: detail the specific neuron types, membrane currents, and synaptic connectivity in auditory brainstem nuclei involved in binaural processing. Success: accurate reproduction of known firing patterns. Algorithmic level: state that $Y$ is obtained by minimizing total synaptic energy in the network. Success: a low-energy state is found. Implementational level: explain that the purpose of the system is to aid survival by orienting toward sound sources. Success: the stated purpose is plausible.\n\nC. Computational level: define the mapping from $X$ to $Y$ as whatever function reproduces known physiological tuning curves in superior olive nuclei. Success: close match to measured spiking rates. Algorithmic level: list membrane time constants, conductances, and channel kinetics. Success: parameter fitting reproduces azimuth judgments. Implementational level: specify a normative estimator for $Y$ given $X$ without reference to neural hardware. Success: the estimator achieves optimality.\n\nD. Computational level: set the goal as maximizing the mutual information between $X$ and $Y$ to ensure informative cues. Success: mutual information is high. Algorithmic level: use any flexible machine learning mapping from $X$ to $Y$ (for example, a deep neural network) without specifying representations or internal procedures beyond training. Success: high test accuracy on localization tasks. Implementational level: regard physical realization as optional since behavior is already matched. Success: none required.\n\nE. Computational level: specify microcircuit parameters and dynamics that transform $X$ to $Y$ and argue why the chosen dynamics are optimal. Success: parameters match published values. Algorithmic level: justify that the result of computation is the most rational $Y$ for given $X$. Success: a proof of optimality is provided. Implementational level: show correlational neural recordings during the task. Success: statistical correlation with cues is strong.",
            "solution": "This problem requires applying the definitions of Marr's three levels of analysis to the specific task of sound localization. The correct option must accurately assign descriptions to each level and define appropriate success criteria.\n\n*   **Computational Level:** This level defines the *what* and *why*. It should describe the goal (mapping binaural cues $X$ to azimuth $Y$), the constraints (physics of sound, sensory noise), and the criterion for success (e.g., minimizing error, matching human psychophysical performance).\n*   **Algorithmic Level:** This level defines the *how*. It should specify the representations (e.g., encoding cues as signal features) and the procedure for the mapping (e.g., cross-correlation algorithm). Success means the algorithm can be shown to compute the desired mapping.\n*   **Implementational Level:** This level defines the *with what*. It should identify the physical hardware (e.g., specific neural circuits like the MSO and LSO) that realizes the algorithm. Success means there is causal evidence linking this hardware to the algorithm and behavior.\n\n**Analysis of Options:**\n*   **Option A:** Correctly aligns all three levels with their definitions and provides appropriate examples (minimizing error, cross-correlation, MSO/LSO circuits) and success criteria (psychophysical match, functional procedure, causal evidence).\n*   **Option B:** Incorrectly scrambles the levels, assigning implementational details (neuron types) to the computational level and a computational goal (energy minimization) to the algorithmic level.\n*   **Option C:** Incorrectly scrambles the levels, assigning an implementational description (physiological tuning curves) to the computational level and a computational description (normative estimator) to the implementational level.\n*   **Option D:** Incorrectly characterizes the algorithmic level as a \"black box\" and dismisses the implementational level, both of which contradict the explanatory goals of Marr's framework.\n*   **Option E:** Incorrectly scrambles the levels, assigning implementational details (microcircuit parameters) to the computational level and a computational goal (rationality) to the algorithmic level. It also proposes a weak success criterion (correlation) for the implementational level.\n\nTherefore, Option A is the only one that provides a completely accurate and coherent description across all three levels.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Beyond defining the levels, a key scientific skill is using them to interpret experimental findings. This next exercise presents a thought experiment common in neuroscience research: a targeted lesion to a neural circuit that, counter-intuitively, leaves behavior unchanged. This scenario  requires careful reasoning to determine what kind of evidence such a result provides, forcing a crucial distinction between an implementational-level discovery (e.g., neural degeneracy) and a computational-level claim about the brain's overall goals.",
            "id": "3995619",
            "problem": "In a lesion experiment within brain modeling and computational neuroscience, a focal inhibitory intervention is applied to a well-characterized cortico-striatal circuit $C$ during a decision-making task. The intervention reduces circuit-specific activity measures but leaves measured task performance unchanged. Let the following quantities be defined: the computational goal $G$ (what is computed in the abstract, e.g., a function $F$ mapping sensory inputs $S$ to optimal actions $R$), the algorithm $A$ (the representational scheme and stepwise procedure realizing $G$), and the implementation $I$ (the physical substrate and its dynamics). Let $B$ denote a scalar behavioral performance metric (e.g., accuracy) that is a function of $S$ and the internal states and outputs of $A$ and $I$. The intervention is modeled by a binary variable $L \\in \\{0,1\\}$ with $L=1$ indicating the lesion condition. Let $M$ denote a circuit-specific physiological measure (e.g., local field potential coherence or spike rate) derived from circuit $C$. Consider the following structural causal dependencies: the abstract goal $G$ constrains the set of algorithms $A$, which constrain possible implementations $I$; implementation $I$ produces behavior $B$ under inputs $S$; the lesion $L$ directly perturbs $I$ and hence $M$, and may or may not affect $B$ depending on compensatory pathways. Assume the observed data satisfy $p(M \\mid L=1) \\neq p(M \\mid L=0)$ and $p(B \\mid L=1) \\approx p(B \\mid L=0)$ for the tested task family. Let $H_C$ denote a computational-level hypothesis stating a particular function $F$ that the system computes across tasks, and let $H_I^{\\text{nec}}$ denote an implementational-level hypothesis stating that circuit $C$ is necessary for producing $B$ in these tasks, while $H_I^{\\text{deg}}$ denotes an implementational-level hypothesis stating that $C$ is one of multiple redundant substrates (degeneracy) capable of supporting $B$. Using the core definitions of David Marr’s levels of analysis and the basic rules of Bayesian evidence (likelihoods and Bayes factors), reason from first principles about what the data do and do not support. Which option(s) correctly specify what counts as computational-level evidence versus implementational-level evidence in this lesion study?\n\nA. Because $p(B \\mid L=1) \\approx p(B \\mid L=0)$, the data provide positive evidence for $H_C$; unchanged $B$ implies that the computational goal $G$ is preserved and thus supports the specific computational theory $H_C$ over alternatives.\n\nB. Because $p(M \\mid L=1) \\neq p(M \\mid L=0)$ while $p(B \\mid L=1) \\approx p(B \\mid L=0)$, the data provide implementational-level evidence consistent with $H_I^{\\text{deg}}$ (redundant realization), and they do not, by themselves, adjudicate among competing computational-level hypotheses $H_C$.\n\nC. Because $B$ is unchanged, the data falsify any claim that circuit $C$ contributes to the computation at any level; they are evidence against both $H_I^{\\text{nec}}$ and $H_C$.\n\nD. The data provide little to no Bayesian evidence for $H_C$ (the likelihood of $B$ under $L=1$ is approximately equal under different computational theories) but do provide evidence against $H_I^{\\text{nec}}$ (necessity) and in favor of $H_I^{\\text{deg}}$ (degeneracy), hence they should be interpreted primarily at the implementational level.\n\nE. To obtain computational-level evidence, one would need task manipulations that selectively engage the hypothesized function $F$: if for tasks $T_F$ that require $F$ one observes $p(B \\mid L=1, T_F) \\ll p(B \\mid L=0, T_F)$ while for control tasks $T_0$ one has $p(B \\mid L=1, T_0) \\approx p(B \\mid L=0, T_0)$, then the data would support $H_C$; in the present study, unchanged $B$ does not provide such evidence.",
            "solution": "This problem tests the interpretation of a lesion study's results through the lens of Marr's levels. The key finding is that a targeted perturbation of a neural circuit $C$ (confirmed by physiological measure $M$) does not affect overall behavioral performance $B$.\n\n**Reasoning from Marr's Levels:**\n*   **Implementational Level Evidence:** The experiment directly manipulates the physical implementation ($I$). The fact that performance $B$ is preserved despite the successful lesion of circuit $C$ ($p(M|L=1) \\neq p(M|L=0)$) provides strong evidence about the implementation. Specifically, it refutes the hypothesis that circuit $C$ is *necessary* ($H_I^{\\text{nec}}$) for the task. Instead, it supports the hypothesis of *degeneracy* or redundancy ($H_I^{\\text{deg}}$), where multiple, distinct neural substrates can produce the same functional output. The evidence is therefore primarily at the implementational level.\n\n*   **Computational Level Evidence:** The experiment does *not* provide evidence to distinguish between competing computational-level hypotheses (e.g., whether the brain is computing function $F_1$ or $F_2$). The system's overall input-output function, whatever it is, remains intact. To gather evidence for a specific computational hypothesis $H_C$, one must typically manipulate the *task itself* to see if behavior changes in a way that is uniquely predicted by that hypothesis. Since the task family was fixed, observing that the system is robust to a specific perturbation does not inform us about the abstract nature of the computation being performed.\n\n**Analysis of Options:**\n*   **A:** Incorrect. Preserved function demonstrates robustness of the system, not evidence for a specific computational goal over others.\n*   **B:** Correct. Accurately states that the evidence is implementational (supporting degeneracy/redundancy) and does not, on its own, resolve computational-level questions.\n*   **C:** Incorrect. This statement is too strong. The circuit may still contribute to the computation in a non-lesioned state; the result only shows it is not strictly necessary.\n*   **D:** Correct. This rephrases the points from option B using the language of Bayesian evidence. The data provide evidence against necessity ($H_I^{\\text{nec}}$) and for degeneracy ($H_I^{\\text{deg}}$), but the likelihood of the data is approximately equal under different computational theories ($H_C$), so it doesn't adjudicate between them.\n*   **E:** Correct. This option correctly explains *what would be required* for computational-level evidence (task manipulations that selectively target the hypothesized function $F$) and notes that the current study lacks this design, hence providing no such evidence.\n\nOptions B, D, and E all provide correct and complementary pieces of the full reasoning.",
            "answer": "$$\\boxed{BDE}$$"
        },
        {
            "introduction": "The ultimate power of Marr's framework lies in its ability to guide the synthesis of complete, multi-level models of brain function. This final practice provides a hands-on opportunity to construct such a model for a highly influential theory in modern neuroscience: Predictive Coding. Starting from a computational-level objective of minimizing prediction error , you will derive the corresponding algorithmic dynamics and analyze the implementational constraints on the system's stability. This exercise beautifully illustrates how the three levels are not just descriptive labels but are deeply intertwined, with constraints at one level shaping the possibilities at others.",
            "id": "3995618",
            "problem": "Consider a cortical microcircuit model of Predictive Coding (PC) analyzed through David Marr’s three levels of analysis. At the computational level, assume the brain’s goal is to infer a latent representation vector $r \\in \\mathbb{R}^{m}$ of a sensory input $x \\in \\mathbb{R}^{n}$ by minimizing a quadratic energy functional that balances prediction fidelity and a quadratic prior on the representation. Formally, the objective is to minimize the function\n$$\nE(r) = \\frac{1}{2}\\|x - W r\\|^{2} + \\frac{\\lambda}{2}\\|r\\|^{2},\n$$\nwhere $W \\in \\mathbb{R}^{n \\times m}$ is a fixed linear generative model and $\\lambda > 0$ is a prior strength.\n\nAt the algorithmic level, implement the inference as continuous-time gradient flow on $E(r)$ using two interacting neural populations: an error population $e \\in \\mathbb{R}^{n}$ representing residuals and a representation population $r \\in \\mathbb{R}^{m}$ encoding latent states. Assume both populations are leaky integrators with time constants $\\tau_{e} > 0$ and $\\tau_{r} > 0$. The error units encode residuals through a leaky computation, and the representation units descend the energy gradient by receiving feedback from the error units. At the implementation level, assume these computations are instantiated by local excitatory-inhibitory recurrent circuitry, with $W$ implemented by feedforward and feedback synapses between populations.\n\nUsing only the objective $E(r)$ and the leaky-integrator assumption, derive the coupled ordinary differential equations for $e(t)$ and $r(t)$. Then, analyze the linearized dynamics around the fixed point for a constant input $x$ by restricting the dynamics to a singular mode of $W$ with singular value $\\sigma \\ge 0$ (that is, along left-right singular vector pairs $(u, v)$ satisfying $W v = \\sigma u$ and $W^{\\top} u = \\sigma v$). In that $2 \\times 2$ subspace, determine the condition for critical damping (a repeated real eigenvalue) of the linearized dynamics. Finally, focusing on the worst-case mode corresponding to the largest singular value $\\sigma$, compute the smaller positive critical ratio\n$$\n\\rho^{\\star} = \\frac{\\tau_{r}}{\\tau_{e}}\n$$\nthat yields critical damping in that mode, expressed solely in terms of $\\lambda$ and $\\sigma$.\n\nYour final answer must be a single closed-form analytic expression for $\\rho^{\\star}$. No rounding is required, and no units are needed because $\\rho^{\\star}$ is dimensionless.",
            "solution": "The problem asks for the derivation of a critical parameter ratio in a Predictive Coding (PC) model. The analysis proceeds by first establishing the dynamics of the system, then linearizing it, and finally finding the condition for critical damping in a specific subspace.\n\nFirst, we establish the coupled ordinary differential equations (ODEs) governing the error population $e(t) \\in \\mathbb{R}^{n}$ and the representation population $r(t) \\in \\mathbb{R}^{m}$. The problem states these populations are leaky integrators.\n\nThe error units, $e$, are described as performing a leaky computation of the residual, which is defined as the difference between the sensory input $x$ and the prediction $Wr$. The dynamics of a leaky integrator with input $I$ and time constant $\\tau$ is generically $\\tau \\frac{dy}{dt} = -y + I$. Here, the state is $e$ and the input is the residual $x - Wr$. Thus, the dynamics for the error population are:\n$$\n\\tau_{e} \\frac{de}{dt} = -e + x - Wr\n$$\n\nThe representation units, $r$, are described as descending the gradient of the energy functional $E(r)$ by receiving feedback from the error units $e(t)$. The standard PC formulation, consistent with the problem description, models the dynamics of $r$ as being driven by feedback from the error population, $W^T e$, and regularized by a decay term, $-\\lambda r$, which arises from the gradient of the prior term $\\frac{\\lambda}{2}\\|r\\|^{2}$:\n$$\n\\tau_{r} \\frac{dr}{dt} = W^T e - \\lambda r\n$$\n\nThe complete set of coupled ODEs is:\n$$\n\\begin{cases}\n\\tau_{e} \\frac{de}{dt} = -e + x - Wr \\\\\n\\tau_{r} \\frac{dr}{dt} = W^T e - \\lambda r\n\\end{cases}\n$$\n\nNext, we find the fixed point $(\\bar{e}, \\bar{r})$ and linearize the dynamics around it. At the fixed point, the time derivatives are zero. The linearized system for the perturbations $(\\delta e, \\delta r)$ around the fixed point is:\n$$\n\\tau_{e} \\frac{d\\delta e}{dt} = -\\delta e - W \\delta r\n$$\n$$\n\\tau_{r} \\frac{d\\delta r}{dt} = W^T \\delta e - \\lambda \\delta r\n$$\nIn matrix form, where the matrix is the Jacobian of the system:\n$$\n\\frac{d}{dt} \\begin{pmatrix} \\delta e \\\\ \\delta r \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{\\tau_{e}}I & -\\frac{1}{\\tau_{e}}W \\\\ \\frac{1}{\\tau_{r}}W^T & -\\frac{\\lambda}{\\tau_{r}}I \\end{pmatrix} \\begin{pmatrix} \\delta e \\\\ \\delta r \\end{pmatrix}\n$$\n\nWe project the dynamics onto a singular mode of $W$ defined by singular value $\\sigma \\ge 0$ and vectors $(u, v)$ where $Wv = \\sigma u$ and $W^T u = \\sigma v$. By setting $\\delta e(t) = e_p(t) u$ and $\\delta r(t) = r_p(t) v$, the dynamics for the scalar coefficients become:\n$$\n\\frac{d}{dt} \\begin{pmatrix} e_p \\\\ r_p \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{\\tau_e} & -\\frac{\\sigma}{\\tau_e} \\\\ \\frac{\\sigma}{\\tau_r} & -\\frac{\\lambda}{\\tau_r} \\end{pmatrix} \\begin{pmatrix} e_p \\\\ r_p \\end{pmatrix}\n$$\nThe stability is determined by the eigenvalues $k$ of the $2 \\times 2$ matrix. The characteristic equation is $\\det(J_{uv} - kI) = 0$:\n$$\nk^2 + \\left(\\frac{1}{\\tau_e} + \\frac{\\lambda}{\\tau_r}\\right)k + \\frac{\\lambda + \\sigma^2}{\\tau_e\\tau_r} = 0\n$$\nCritical damping occurs when the discriminant of this quadratic equation is zero:\n$$\n\\Delta = \\left(\\frac{1}{\\tau_e} + \\frac{\\lambda}{\\tau_r}\\right)^2 - 4\\frac{\\lambda + \\sigma^2}{\\tau_e\\tau_r} = 0\n$$\nExpanding and simplifying gives:\n$$\n\\frac{1}{\\tau_e^2} + \\frac{2\\lambda}{\\tau_e\\tau_r} + \\frac{\\lambda^2}{\\tau_r^2} - \\frac{4\\lambda + 4\\sigma^2}{\\tau_e\\tau_r} = 0\n$$\n$$\n\\frac{1}{\\tau_e^2} - \\frac{2\\lambda + 4\\sigma^2}{\\tau_e\\tau_r} + \\frac{\\lambda^2}{\\tau_r^2} = 0\n$$\nLet $\\rho = \\frac{\\tau_r}{\\tau_e}$. We can rearrange the equation into a quadratic for $\\rho$ by multiplying by $\\tau_e^2$:\n$$\n1 - \\frac{2\\lambda + 4\\sigma^2}{\\rho} + \\frac{\\lambda^2}{\\rho^2} = 0\n$$\nMultiplying by $\\rho^2$ yields:\n$$\n\\rho^2 - (2\\lambda + 4\\sigma^2)\\rho + \\lambda^2 = 0\n$$\nWe solve for $\\rho$ using the quadratic formula:\n$$\n\\rho = \\frac{(2\\lambda + 4\\sigma^2) \\pm \\sqrt{(2\\lambda + 4\\sigma^2)^2 - 4\\lambda^2}}{2}\n$$\n$$\n\\rho = (\\lambda + 2\\sigma^2) \\pm \\sqrt{(\\lambda + 2\\sigma^2)^2 - \\lambda^2}\n$$\nThe term under the square root simplifies to $4\\sigma^2(\\lambda + \\sigma^2)$. Thus, the solutions are:\n$$\n\\rho = \\lambda + 2\\sigma^2 \\pm 2\\sigma\\sqrt{\\lambda + \\sigma^2}\n$$\nBoth roots are positive since $\\lambda > 0$ and $\\sigma \\ge 0$. The problem asks for the smaller positive critical ratio, $\\rho^\\star$. This is the solution with the minus sign:\n$$\n\\rho^\\star = \\lambda + 2\\sigma^2 - 2\\sigma\\sqrt{\\lambda + \\sigma^2}\n$$\nThis expression can be recognized as the expansion of a perfect square:\n$$\n\\rho^\\star = (\\lambda + \\sigma^2) - 2\\sigma\\sqrt{\\lambda+\\sigma^2} + \\sigma^2 = \\left(\\sqrt{\\lambda+\\sigma^2} - \\sigma\\right)^2\n$$\nThis is the final expression for the smaller positive critical ratio.",
            "answer": "$$\\boxed{\\left(\\sqrt{\\lambda+\\sigma^2} - \\sigma\\right)^2}$$"
        }
    ]
}