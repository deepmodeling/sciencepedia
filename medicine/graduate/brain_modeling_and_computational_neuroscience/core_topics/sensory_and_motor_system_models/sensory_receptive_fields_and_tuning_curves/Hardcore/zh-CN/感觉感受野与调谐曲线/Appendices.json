{
    "hands_on_practices": [
        {
            "introduction": "感受野是感觉神经元对特定刺激特征做出反应的区域。在早期视觉系统中，一个经典的感受野模型是高斯差分（DoG）模型，它有效地描述了具有“中央-周边”拮抗特性的神经元，例如视网膜神经节细胞。通过本练习，您将亲手实践如何通过将感受野核与特定刺激进行积分来计算神经元的理论响应，从而加深对感受野结构如何塑造神经元输出的理解。",
            "id": "4017975",
            "problem": "早期视觉通路中的一个中继神经元被建模为一个圆形对称的感受野，该感受野由高斯差分（DoG）给出，其中中心是兴奋性的，而周边是抑制性的。空间感受野核定义为\n$$\nh(\\mathbf{x}) \\equiv h(r) = k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) \\;-\\; k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right),\n$$\n其中 $r = \\|\\mathbf{x}\\|$，$k_{c}  0$，$k_{s}  0$，$ \\sigma_{c}  0$ 且 $\\sigma_{s}  0$。线性生成信号由标准线性滤波操作定义\n$$\nG \\;=\\; \\int_{\\mathbb{R}^{2}} h(\\mathbf{x})\\,s(\\mathbf{x})\\,\\mathrm{d}^{2}\\mathbf{x},\n$$\n其中 $s(\\mathbf{x})$ 是刺激对比度场（无量纲）。考虑一个刺激，它由一个半径为 $R_{c}$、对比度均匀为 $C_{c}$ 的中央圆盘，以及一个从半径 $R_{c}$ 延伸到 $R_{s}$、对比度均匀为 $C_{s}$ 的环形区域所包围，而在半径 $R_{s}$ 之外的对比度为零。即，\n$$\ns(r) \\;=\\; \\begin{cases}\nC_{c},  0 \\le r \\le R_{c},\\\\\nC_{s},  R_{c}  r \\le R_{s},\\\\\n0,  r  R_{s}.\n\\end{cases}\n$$\n令 $G_{\\mathrm{full}}$ 表示上述完整刺激的生成信号，令 $G_{\\mathrm{center}}$ 表示仅中央圆盘的生成信号（即，在 $0 \\le r \\le R_{c}$ 上有相同的 $C_{c}$，但在 $r  R_{c}$ 时 $C_{s} = 0$）。仅使用给定的定义和标准微积分，推导周边影响的一个精确、闭式的解析表达式\n$$\n\\Delta G \\equiv G_{\\mathrm{full}} - G_{\\mathrm{center}}\n$$\n用 $C_{c}$、$C_{s}$、$k_{c}$、$k_{s}$、$\\sigma_{c}$、$\\sigma_{s}$、$R_{c}$ 和 $R_{s}$ 来表示。\n\n将您的最终答案表示为单个简化的符号表达式。以任意单位 (a.u.) 报告生成信号。",
            "solution": "我们从线性生成信号的定义开始，即感受野与刺激的空间内积，\n$$\nG \\;=\\; \\int_{\\mathbb{R}^{2}} h(\\mathbf{x})\\,s(\\mathbf{x})\\,\\mathrm{d}^{2}\\mathbf{x}.\n$$\n感受野是径向对称的，由高斯差分 (DoG) 给出，\n$$\nh(r) \\;=\\; k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) \\;-\\; k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right),\n$$\n其中每个高斯分量在整个平面上的积分被归一化为1：\n$$\n\\int_{\\mathbb{R}^{2}} \\frac{1}{2\\pi \\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\mathrm{d}^{2}\\mathbf{x} \\;=\\; 1.\n$$\n刺激也是径向对称的，并且在半径上是分段常数。因为 $h$ 和 $s$ 都只依赖于 $r = \\|\\mathbf{x}\\|$，所以在极坐标下，二维积分简化为\n$$\nG \\;=\\; \\int_{0}^{\\infty} h(r)\\,s(r)\\,2\\pi r\\,\\mathrm{d}r.\n$$\n我们定义一个归一化高斯函数在半径为 $R$ 的圆盘上的累积积分：\n$$\nI(\\sigma; R) \\;\\equiv\\; \\int_{0}^{R} \\frac{1}{2\\pi \\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\,2\\pi r\\,\\mathrm{d}r \\;=\\; \\int_{0}^{R} \\frac{1}{\\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\,r\\,\\mathrm{d}r.\n$$\n进行换元，$u = \\frac{r^{2}}{2\\sigma^{2}}$，则 $\\mathrm{d}u = \\frac{r}{\\sigma^{2}}\\mathrm{d}r$ 且 $r\\,\\mathrm{d}r = \\sigma^{2}\\mathrm{d}u$。积分变为\n$$\nI(\\sigma; R) \\;=\\; \\int_{u=0}^{u=\\frac{R^{2}}{2\\sigma^{2}}} \\exp(-u)\\,\\mathrm{d}u \\;=\\; 1 - \\exp\\!\\left(-\\frac{R^{2}}{2\\sigma^{2}}\\right).\n$$\n因此，一个归一化高斯函数在从 $R_{1}$ 到 $R_{2}$ 的环形区域上的积分（其中 $0 \\le R_{1}  R_{2}$）是\n$$\nI(\\sigma; R_{2}) - I(\\sigma; R_{1}) \\;=\\; \\left[1 - \\exp\\!\\left(-\\frac{R_{2}^{2}}{2\\sigma^{2}}\\right)\\right] - \\left[1 - \\exp\\!\\left(-\\frac{R_{1}^{2}}{2\\sigma^{2}}\\right)\\right] \\;=\\; \\exp\\!\\left(-\\frac{R_{1}^{2}}{2\\sigma^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{2}^{2}}{2\\sigma^{2}}\\right).\n$$\n利用积分的线性和 $h(r)$ 的高斯差分分解，我们将生成信号表示为来自中央圆盘和周边环形区域贡献的总和：\n$$\nG_{\\mathrm{full}} \\;=\\; \\int_{0}^{R_{c}} \\!\\!\\left[k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) - k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right)\\right] C_{c}\\,2\\pi r\\,\\mathrm{d}r\n$$\n$$\n\\quad + \\int_{R_{c}}^{R_{s}} \\!\\!\\left[k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) - k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right)\\right] C_{s}\\,2\\pi r\\,\\mathrm{d}r.\n$$\n根据上述定义，这些可以计算为\n$$\nG_{\\mathrm{full}} \\;=\\; C_{c}\\left[k_{c}\\,I(\\sigma_{c}; R_{c}) - k_{s}\\,I(\\sigma_{s}; R_{c})\\right] \\;+\\; C_{s}\\left\\{k_{c}\\left[I(\\sigma_{c}; R_{s}) - I(\\sigma_{c}; R_{c})\\right] - k_{s}\\left[I(\\sigma_{s}; R_{s}) - I(\\sigma_{s}; R_{c})\\right]\\right\\}.\n$$\n类似地，仅中央刺激 ($C_{s}=0$) 的生成信号是\n$$\nG_{\\mathrm{center}} \\;=\\; C_{c}\\left[k_{c}\\,I(\\sigma_{c}; R_{c}) - k_{s}\\,I(\\sigma_{s}; R_{c})\\right].\n$$\n因此，周边影响是\n$$\n\\Delta G \\;\\equiv\\; G_{\\mathrm{full}} - G_{\\mathrm{center}} \\;=\\; C_{s}\\left\\{k_{c}\\left[I(\\sigma_{c}; R_{s}) - I(\\sigma_{c}; R_{c})\\right] - k_{s}\\left[I(\\sigma_{s}; R_{s}) - I(\\sigma_{s}; R_{c})\\right]\\right\\}.\n$$\n代入环形区域积分的闭式表达式，得到\n$$\nI(\\sigma; R_{s}) - I(\\sigma; R_{c}) \\;=\\; \\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma^{2}}\\right),\n$$\n因此\n$$\n\\Delta G \\;=\\; C_{s}\\left\\{k_{c}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{c}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{c}^{2}}\\right)\\right] \\;-\\; k_{s}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{s}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{s}^{2}}\\right)\\right]\\right\\}.\n$$\n最后，为了遵循所要求的指数表示法，我们使用指数函数来书写结果：\n$$\n\\Delta G \\;=\\; C_{s}\\left\\{k_{c}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{c}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{c}^{2}}\\right)\\right] - k_{s}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{s}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{s}^{2}}\\right)\\right]\\right\\}.\n$$\n这就是周边对生成信号影响的精确、闭式解析表达式，单位为任意单位 (a.u.)。",
            "answer": "$$\\boxed{C_{s}\\left\\{k_{c}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{c}^{2}}\\right)-\\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{c}^{2}}\\right)\\right]-k_{s}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{s}^{2}}\\right)-\\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{s}^{2}}\\right)\\right]\\right\\}}$$"
        },
        {
            "introduction": "理解了神经元如何根据其感受野产生响应后，下一步是量化其响应特性。调谐曲线是描述神经元选择性的基本工具，例如初级视皮层（V1）中神经元的方向选择性。本练习旨在建立调谐曲线的一个关键描述性特征——半峰全宽（HWHM）——与高斯函数等底层数学模型参数之间的直接联系，这是解读实验数据和评估神经元编码精度的核心技能。",
            "id": "4018030",
            "problem": "初级视皮层（V1）中的单个感觉神经元表现出方向调谐特性，在循环回绕效应可以忽略不计的范围内，该特性可以很好地用刺激方向 $\\theta$（以弧度为单位）的高斯函数来近似。神经元作为方向函数的发放率模型为 $R(\\theta) = R_{b} + A \\,\\exp\\!\\left(-\\frac{(\\theta - \\theta_{0})^{2}}{2\\,\\sigma^{2}}\\right)$，其中 $R_{b}$ 是基线发放率，$A$ 是高于基线的峰值振幅，$\\theta_{0}$ 是首选方向，$\\sigma$ 是控制调谐宽度的标准差参数。从高斯函数和半高半宽（HWHM）的定义出发，推导 HWHM 仅用 $\\sigma$ 表示的闭式解析表达式。HWHM 定义为正偏移量 $\\Delta \\theta$，使得 $R(\\theta_{0} \\pm \\Delta \\theta)$ 等于基线加上高于基线的峰值的一半。用弧度表示 HWHM。你的最终答案必须是单个解析表达式。",
            "solution": "问题要求推导高斯调谐曲线的半高半宽（HWHM）仅用其标准差参数 $\\sigma$ 表示的闭式解析表达式。\n\n神经元发放率 $R(\\theta)$ 作为刺激方向 $\\theta$ 函数的模型由下式给出：\n$$R(\\theta) = R_{b} + A \\exp\\left(-\\frac{(\\theta - \\theta_{0})^{2}}{2\\sigma^{2}}\\right)$$\n其中 $R_{b}$ 是基线发放率，$A$ 是高于基线的峰值振幅，$\\theta_{0}$ 是首选方向，$\\sigma$ 是标准差参数。\n\n首先，我们确定此模型所描述的最大和最小（基线）发放率。最大发放率 $R_{\\text{max}}$ 出现在指数为最小负值时，即 $\\theta = \\theta_{0}$ 时。\n$$R_{\\text{max}} = R(\\theta_{0}) = R_{b} + A \\exp\\left(-\\frac{(\\theta_{0} - \\theta_{0})^{2}}{2\\sigma^{2}}\\right) = R_{b} + A \\exp(0) = R_{b} + A$$\n基线率 $R_{b}$ 是当 $|\\theta - \\theta_{0}| \\to \\infty$ 时接近的速率。\n\n半高半宽（此处表示为 $\\Delta \\theta$）定义为与首选方向 $\\theta_{0}$ 的正偏移量，在该偏移量处，发放率恰好是基线率 $R_{b}$ 和最大率 $R_{\\text{max}}$ 之间的一半。设此半高发放率为 $R_{\\text{half}}$。\n$$R_{\\text{half}} = R_{b} + \\frac{R_{\\text{max}} - R_{b}}{2}$$\n代入 $R_{\\text{max}}$ 的表达式：\n$$R_{\\text{half}} = R_{b} + \\frac{(R_{b} + A) - R_{b}}{2} = R_{b} + \\frac{A}{2}$$\n这证实了问题将目标速率定义为“基线加上高于基线的峰值的一半”。\n\n根据 HWHM 的定义，我们必须找到值 $\\Delta \\theta  0$，使得 $R(\\theta_{0} \\pm \\Delta \\theta) = R_{\\text{half}}$。让我们计算 $R(\\theta_{0} + \\Delta \\theta)$：\n$$R(\\theta_{0} + \\Delta \\theta) = R_{b} + A \\exp\\left(-\\frac{((\\theta_{0} + \\Delta \\theta) - \\theta_{0})^{2}}{2\\sigma^{2}}\\right) = R_{b} + A \\exp\\left(-\\frac{(\\Delta \\theta)^{2}}{2\\sigma^{2}}\\right)$$\n注意，由于差值的平方，使用 $\\theta_{0} - \\Delta \\theta$ 会得到相同的结果。\n\n现在，我们将此表达式设为等于 $R_{\\text{half}}$ 并求解 $\\Delta \\theta$：\n$$R_{b} + A \\exp\\left(-\\frac{(\\Delta \\theta)^{2}}{2\\sigma^{2}}\\right) = R_{b} + \\frac{A}{2}$$\n两边都减去 $R_{b}$ 得到：\n$$A \\exp\\left(-\\frac{(\\Delta \\theta)^{2}}{2\\sigma^{2}}\\right) = \\frac{A}{2}$$\n因为 $A$ 代表峰值振幅，我们可以假设 $A  0$ 并将两边都除以 $A$：\n$$\\exp\\left(-\\frac{(\\Delta \\theta)^{2}}{2\\sigma^{2}}\\right) = \\frac{1}{2}$$\n为了求解 $\\Delta \\theta$，我们对两边取自然对数 ($\\ln$)：\n$$\\ln\\left(\\exp\\left(-\\frac{(\\Delta \\theta)^{2}}{2\\sigma^{2}}\\right)\\right) = \\ln\\left(\\frac{1}{2}\\right)$$\n$$-\\frac{(\\Delta \\theta)^{2}}{2\\sigma^{2}} = -\\ln(2)$$\n两边都乘以 $-1$：\n$$\\frac{(\\Delta \\theta)^{2}}{2\\sigma^{2}} = \\ln(2)$$\n现在，我们分离出 $(\\Delta \\theta)^{2}$：\n$$(\\Delta \\theta)^{2} = 2\\sigma^{2} \\ln(2)$$\n最后，对两边取平方根，并注意 HWHM ($\\Delta \\theta$) 是一个正宽度，我们得到 HWHM 的表达式：\n$$\\Delta \\theta = \\sqrt{2\\sigma^{2} \\ln(2)}$$\n$$\\Delta \\theta = \\sigma \\sqrt{2 \\ln(2)}$$\n这就是 HWHM 仅用 $\\sigma$ 表示的闭式解析表达式。因为 $\\sigma$ 是以弧度为单位指定的，所以 HWHM, $\\Delta \\theta$ 也将以弧度为单位。",
            "answer": "$$\\boxed{\\sigma \\sqrt{2 \\ln(2)}}$$"
        },
        {
            "introduction": "在计算神经科学中，一个核心挑战是解决“逆问题”：如何从神经元对刺激的响应中推断出其内在属性，即感受野。本练习将引导您运用最大后验（MAP）估计，这是一种超越简单最小二乘法的、基于贝叶斯原理的推断方法。您将推导出高斯先验如何自然地导向岭回归，这是一种在处理噪声数据时获得稳定、鲁棒的感受野估计的强大正则化技术，并理解其背后经典的偏倚-方差权衡。",
            "id": "4017986",
            "problem": "考虑一个用于感觉神经元的线性感受野模型，其中对一系列刺激的测量响应向量 $\\mathbf{r} \\in \\mathbb{R}^{T}$ 被建模为 $\\mathbf{r} = \\mathbf{X}\\mathbf{k} + \\boldsymbol{\\varepsilon}$。这里，$\\mathbf{X} \\in \\mathbb{R}^{T \\times d}$ 是刺激设计矩阵（每行是一个刺激，列对应于 $d$ 个特征），$\\mathbf{k} \\in \\mathbb{R}^{d}$ 是待估计的未知感受野（滤波器），$\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{T}$ 是加性测量噪声。假设噪声是独立同分布的高斯噪声，均值为零，协方差为 $\\sigma^{2}\\mathbf{I}_{T}$。又假设感受野的先验是具有各向同性精度的高斯分布，由 $p(\\mathbf{k}) \\propto \\exp\\!\\big(-\\lambda \\|\\mathbf{k}\\|_{2}^{2}\\big)$ 指定，其中 $\\lambda  0$ 是一个已知常数。以贝叶斯定理为基本原则，并假设线性高斯观测模型，推导最大后验（MAP）估计量 $\\hat{\\mathbf{k}}_{\\text{MAP}}$，该估计量最大化后验分布 $p(\\mathbf{k}\\mid \\mathbf{r}, \\mathbf{X})$。用 $\\mathbf{X}$、$\\mathbf{r}$、$\\sigma^{2}$ 和 $\\lambda$ 以闭式形式表示您的最终结果。然后，从高斯噪声下线性估计量的偏差和方差的定义出发，解释由高斯先验所蕴含的 $\\ell_{2}$ 惩罚项如何实现岭正则化，在 $\\lambda$ 变化时，在滤波器估计中权衡偏差与方差，过程中不得引用任何快捷公式。您的最终答案必须是 $\\hat{\\mathbf{k}}_{\\text{MAP}}$ 的单个闭式解析表达式。不需要进行数值近似。",
            "solution": "经评估，用户提供的问题是有效的。它在贝叶斯统计建模和计算神经科学方面有科学依据，问题陈述清晰，定义完整且一致，并且以客观、规范的语言表述。任务是为一个线性高斯感受野模型推导最大后验（MAP）估计量，并解释由先验施加的正则化所固有的偏差-方差权衡。\n\nMAP估计量 $\\hat{\\mathbf{k}}_{\\text{MAP}}$ 的推导始于贝叶斯定理。该定理指出，在给定测量响应 $\\mathbf{r}$ 和刺激矩阵 $\\mathbf{X}$ 的条件下，感受野 $\\mathbf{k}$ 的后验分布与数据似然和感受野先验概率的乘积成正比：\n$$\np(\\mathbf{k} \\mid \\mathbf{r}, \\mathbf{X}) \\propto p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) p(\\mathbf{k})\n$$\nMAP估计是使该后验概率最大化的 $\\mathbf{k}$ 的值。由于对数是单调函数，最大化后验概率的对数在计算上更为方便。对数后验由下式给出：\n$$\n\\ln p(\\mathbf{k} \\mid \\mathbf{r}, \\mathbf{X}) = \\ln p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) + \\ln p(\\mathbf{k}) + C\n$$\n其中 $C$ 是一个不依赖于 $\\mathbf{k}$ 的常数。\n\n首先，我们定义似然项 $p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X})$。模型为 $\\mathbf{r} = \\mathbf{X}\\mathbf{k} + \\boldsymbol{\\varepsilon}$，其中噪声 $\\boldsymbol{\\varepsilon}$ 从均值为零、协方差为 $\\sigma^{2}\\mathbf{I}_{T}$ 的高斯分布中抽取。这意味着响应 $\\mathbf{r}$ 也是高斯分布的，其均值为 $\\mathbf{X}\\mathbf{k}$，协方差为 $\\sigma^{2}\\mathbf{I}_{T}$。其概率密度函数为：\n$$\np(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) = \\frac{1}{(2\\pi\\sigma^{2})^{T/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} (\\mathbf{r} - \\mathbf{X}\\mathbf{k})^{T}(\\mathbf{r} - \\mathbf{X}\\mathbf{k})\\right) = \\frac{1}{(2\\pi\\sigma^{2})^{T/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|\\mathbf{r} - \\mathbf{X}\\mathbf{k}\\|_{2}^{2}\\right)\n$$\n因此，对数似然为：\n$$\n\\ln p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) = -\\frac{T}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}} \\|\\mathbf{r} - \\mathbf{X}\\mathbf{k}\\|_{2}^{2}\n$$\n\n接下来，我们考虑感受野的先验分布 $p(\\mathbf{k})$，给定为 $p(\\mathbf{k}) \\propto \\exp(-\\lambda \\|\\mathbf{k}\\|_{2}^{2})$。对数先验为：\n$$\n\\ln p(\\mathbf{k}) = -\\lambda \\|\\mathbf{k}\\|_{2}^{2} + C'\n$$\n其中 $C'$ 是另一个不依赖于 $\\mathbf{k}$ 的常数。\n\n将这些结合起来，需要对 $\\mathbf{k}$ 最大化的对数后验目标函数 $\\mathcal{L}(\\mathbf{k})$ 为：\n$$\n\\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} \\|\\mathbf{r} - \\mathbf{X}\\mathbf{k}\\|_{2}^{2} - \\lambda \\|\\mathbf{k}\\|_{2}^{2} + \\text{constants}\n$$\n为了找到最大值，我们计算 $\\mathcal{L}(\\mathbf{k})$ 相对于 $\\mathbf{k}$ 的梯度，并将其设为零向量。我们首先展开平方范数：\n$$\n\\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} (\\mathbf{r}^{T}\\mathbf{r} - 2\\mathbf{r}^{T}\\mathbf{X}\\mathbf{k} + \\mathbf{k}^{T}\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - \\lambda \\mathbf{k}^{T}\\mathbf{k} + \\text{constants}\n$$\n梯度为：\n$$\n\\nabla_{\\mathbf{k}} \\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} (-2\\mathbf{X}^{T}\\mathbf{r} + 2\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - 2\\lambda\\mathbf{k}\n$$\n将梯度设为 $\\mathbf{0}$：\n$$\n\\frac{1}{\\sigma^{2}}(\\mathbf{X}^{T}\\mathbf{r} - \\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - 2\\lambda\\mathbf{k} = \\mathbf{0}\n$$\n两边乘以 $\\sigma^{2}$ 并重新整理各项以求解 $\\mathbf{k}$：\n$$\n\\mathbf{X}^{T}\\mathbf{r} - \\mathbf{X}^{T}\\mathbf{X}\\mathbf{k} = 2\\sigma^{2}\\lambda\\mathbf{k} \\\\\n\\mathbf{X}^{T}\\mathbf{r} = \\mathbf{X}^{T}\\mathbf{X}\\mathbf{k} + 2\\sigma^{2}\\lambda\\mathbf{k} \\\\\n\\mathbf{X}^{T}\\mathbf{r} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})\\mathbf{k}\n$$\n其中 $\\mathbf{I}_{d}$ 是 $d \\times d$ 的单位矩阵。通过左乘矩阵项的逆，可以找到MAP估计量 $\\hat{\\mathbf{k}}_{\\text{MAP}}$。由于 $\\mathbf{X}^{T}\\mathbf{X}$ 是半正定的，且 $\\lambda  0$, $\\sigma^2  0$，矩阵 $(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})$ 是正定的，因此是可逆的。\n$$\n\\hat{\\mathbf{k}}_{\\text{MAP}} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{r}\n$$\n这是MAP估计量的闭式表达式，它等价于岭回归的解。\n\n现在，我们来解释 $\\ell_{2}$ 惩罚项如何实现偏差-方差权衡。设真实的、未知的感受野为 $\\mathbf{k}_{\\text{true}}$。观测数据根据 $\\mathbf{r} = \\mathbf{X}\\mathbf{k}_{\\text{true}} + \\boldsymbol{\\varepsilon}$ 生成。该估计量是数据 $\\mathbf{r}$ 的线性函数。\n\n估计量的偏差是其期望值与真值之差，其中期望是针对噪声分布计算的：\n$$\n\\text{Bias}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] - \\mathbf{k}_{\\text{true}}\n$$\n我们计算期望值：\n$$\n\\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] = \\mathbb{E}[(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{r}] \\\\\n= (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbb{E}[\\mathbf{r}]\n$$\n由于 $\\mathbb{E}[\\mathbf{r}] = \\mathbb{E}[\\mathbf{X}\\mathbf{k}_{\\text{true}} + \\boldsymbol{\\varepsilon}] = \\mathbf{X}\\mathbf{k}_{\\text{true}} + \\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{X}\\mathbf{k}_{\\text{true}}$，我们有：\n$$\n\\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}_{\\text{true}}\n$$\n因此，偏差为：\n$$\n\\text{Bias}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = [(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X} - \\mathbf{I}_{d}]\\mathbf{k}_{\\text{true}}\n$$\n如果 $\\lambda = 0$（最大似然情况），并假设 $\\mathbf{X}^{T}\\mathbf{X}$ 可逆，则偏差为 $[(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{X} - \\mathbf{I}_{d}]\\mathbf{k}_{\\text{true}} = [\\mathbf{I}_{d} - \\mathbf{I}_{d}]\\mathbf{k}_{\\text{true}} = \\mathbf{0}$。最大似然估计量 (MLE) 是无偏的。然而，对于任何 $\\lambda  0$，项 $(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X}$ 不是单位矩阵。因此，MAP估计量是有偏的。这个偏差项有效地将估计的滤波器系数与真实值相比向零收缩，并且该偏差的大小随 $\\lambda$ 的增加而增加。\n\n估计量的方差是其协方差矩阵：\n$$\n\\text{Var}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\mathbb{E}\\big[(\\hat{\\mathbf{k}}_{\\text{MAP}} - \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}])(\\hat{\\mathbf{k}}_{\\text{MAP}} - \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}])^{T}\\big]\n$$\n令 $\\mathbf{M} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}$。那么 $\\hat{\\mathbf{k}}_{\\text{MAP}} = \\mathbf{M}\\mathbf{r}$ 且 $\\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] = \\mathbf{M}\\mathbb{E}[\\mathbf{r}]$。与均值的偏差为：\n$$\n\\hat{\\mathbf{k}}_{\\text{MAP}} - \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] = \\mathbf{M}(\\mathbf{r} - \\mathbb{E}[\\mathbf{r}]) = \\mathbf{M}\\boldsymbol{\\varepsilon}\n$$\n于是方差为：\n$$\n\\text{Var}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\mathbb{E}[\\mathbf{M}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{T}\\mathbf{M}^{T}] = \\mathbf{M} \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{T}] \\mathbf{M}^{T}\n$$\n由于 $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{T}] = \\sigma^{2}\\mathbf{I}_{T}$，我们得到：\n$$\n\\text{Var}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\sigma^{2}\\mathbf{M}\\mathbf{M}^{T} = \\sigma^{2}(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\n$$\n对于无偏的MLE（$\\lambda = 0$），方差为 $\\sigma^{2}(\\mathbf{X}^{T}\\mathbf{X})^{-1}$。如果 $\\mathbf{X}^{T}\\mathbf{X}$ 是病态的（具有非常小的特征值），其逆矩阵将具有非常大的特征值，导致估计的方差极高。当 $\\lambda  0$ 时，在求逆之前将项 $2\\sigma^{2}\\lambda\\mathbf{I}_{d}$ 加到 $\\mathbf{X}^{T}\\mathbf{X}$ 上。这将所有特征值增加了 $2\\sigma^{2}\\lambda$，使矩阵变为良态的并稳定了求逆过程。因此，所得协方差矩阵中各项的大小减小。随着 $\\lambda$ 的增加，估计量的方差减小。\n\n总之，对 $\\mathbf{k}$ 的高斯先验在优化中转化为一个 $\\ell_{2}$ 惩罚项。这个惩罚项为MAP估计引入了偏差，系统地将其向先验均值（零）收缩。作为这种偏差的交换，估计的方差被大幅降低，特别是当刺激矩阵 $\\mathbf{X}$ 导致 $\\mathbf{X}^{T}\\mathbf{X}$ 矩阵是病态或奇异时。这种交换是经典的偏差-方差权衡。参数 $\\lambda$ 控制着这种权衡：增加 $\\lambda$ 会增加偏差但减少方差。可以选择一个最优的 $\\lambda$ 来最小化总均方误差，即偏差平方和方差之和。",
            "answer": "$$\n\\boxed{(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{r}}\n$$"
        }
    ]
}