{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of sensory neuroscience is the linear receptive field, which models how a neuron integrates features from a stimulus. This exercise provides foundational practice with the classic Difference-of-Gaussians (DoG) model, which elegantly captures the center-surround antagonism observed in the early visual system. By analytically calculating the neuron's response to a patterned stimulus, you will gain a concrete understanding of how spatial filtering shapes neural signals. ",
            "id": "4017975",
            "problem": "A relay neuron in the early visual pathway is modeled with a circularly symmetric receptive field given by a Difference of Gaussians (DoG), where the center is excitatory and the surround is inhibitory. The spatial receptive field kernel is defined as\n$$\nh(\\mathbf{x}) \\equiv h(r) = k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) \\;-\\; k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right),\n$$\nwith $r = \\|\\mathbf{x}\\|$, $k_{c} > 0$, $k_{s} > 0$, $\\sigma_{c} > 0$, and $\\sigma_{s} > 0$. The linear generator signal is defined by the standard linear filtering operation\n$$\nG \\;=\\; \\int_{\\mathbb{R}^{2}} h(\\mathbf{x})\\,s(\\mathbf{x})\\,\\mathrm{d}^{2}\\mathbf{x},\n$$\nwhere $s(\\mathbf{x})$ is the stimulus contrast field (dimensionless). Consider a stimulus that consists of a center disk of radius $R_{c}$ at uniform contrast $C_{c}$, surrounded by an annulus extending from radius $R_{c}$ to $R_{s}$ at uniform contrast $C_{s}$, and zero contrast outside radius $R_{s}$. That is,\n$$\ns(r) \\;=\\; \\begin{cases}\nC_{c}, & 0 \\le r \\le R_{c},\\\\\nC_{s}, & R_{c} < r \\le R_{s},\\\\\n0, & r > R_{s}.\n\\end{cases}\n$$\nLet $G_{\\mathrm{full}}$ denote the generator signal for the full stimulus described above, and let $G_{\\mathrm{center}}$ denote the generator signal for the center disk alone (that is, the same $C_{c}$ on $0 \\le r \\le R_{c}$ but with $C_{s} = 0$ for $r > R_{c}$). Using only the definitions given and standard calculus, derive an exact, closed-form analytical expression for the surround impact\n$$\n\\Delta G \\equiv G_{\\mathrm{full}} - G_{\\mathrm{center}}\n$$\nin terms of $C_{c}$, $C_{s}$, $k_{c}$, $k_{s}$, $\\sigma_{c}$, $\\sigma_{s}$, $R_{c}$, and $R_{s}$.\n\nExpress your final answer as a single simplified symbolic expression. Report the generator signal in arbitrary units (a.u.).",
            "solution": "We begin from the definition of the linear generator signal as the spatial inner product of the receptive field with the stimulus,\n$$\nG \\;=\\; \\int_{\\mathbb{R}^{2}} h(\\mathbf{x})\\,s(\\mathbf{x})\\,\\mathrm{d}^{2}\\mathbf{x}.\n$$\nThe receptive field is radially symmetric and is given by a Difference of Gaussians (DoG),\n$$\nh(r) \\;=\\; k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) \\;-\\; k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right),\n$$\nwhere each Gaussian component is normalized to unit integral over the plane:\n$$\n\\int_{\\mathbb{R}^{2}} \\frac{1}{2\\pi \\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\mathrm{d}^{2}\\mathbf{x} \\;=\\; 1.\n$$\nThe stimulus is also radially symmetric and piecewise constant in radius. Because both $h$ and $s$ depend only on $r = \\|\\mathbf{x}\\|$, the two-dimensional integral reduces, in polar coordinates, to\n$$\nG \\;=\\; \\int_{0}^{\\infty} h(r)\\,s(r)\\,2\\pi r\\,\\mathrm{d}r.\n$$\nLet us define the cumulative integral of a normalized Gaussian over a disk of radius $R$:\n$$\nI(\\sigma; R) \\;\\equiv\\; \\int_{0}^{R} \\frac{1}{2\\pi \\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\,2\\pi r\\,\\mathrm{d}r \\;=\\; \\int_{0}^{R} \\frac{1}{\\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\,r\\,\\mathrm{d}r.\n$$\nMake the substitution $u = \\frac{r^{2}}{2\\sigma^{2}}$, so that $\\mathrm{d}u = \\frac{r}{\\sigma^{2}}\\mathrm{d}r$ and $r\\,\\mathrm{d}r = \\sigma^{2}\\mathrm{d}u$. The integral becomes\n$$\nI(\\sigma; R) \\;=\\; \\int_{u=0}^{u=\\frac{R^{2}}{2\\sigma^{2}}} \\exp(-u)\\,\\mathrm{d}u \\;=\\; 1 - \\exp\\!\\left(-\\frac{R^{2}}{2\\sigma^{2}}\\right).\n$$\nTherefore, the integral of a normalized Gaussian over an annulus from $R_{1}$ to $R_{2}$, with $0 \\le R_{1} < R_{2}$, is\n$$\nI(\\sigma; R_{2}) - I(\\sigma; R_{1}) \\;=\\; \\left[1 - \\exp\\!\\left(-\\frac{R_{2}^{2}}{2\\sigma^{2}}\\right)\\right] - \\left[1 - \\exp\\!\\left(-\\frac{R_{1}^{2}}{2\\sigma^{2}}\\right)\\right] \\;=\\; \\exp\\!\\left(-\\frac{R_{1}^{2}}{2\\sigma^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{2}^{2}}{2\\sigma^{2}}\\right).\n$$\nUsing linearity of the integral and the DoG decomposition of $h(r)$, we express the generator signal as the sum of contributions from the center disk and the surround annulus:\n$$\nG_{\\mathrm{full}} \\;=\\; \\int_{0}^{R_{c}} \\!\\!\\left[k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) - k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right)\\right] C_{c}\\,2\\pi r\\,\\mathrm{d}r\n$$\n$$\n\\quad + \\int_{R_{c}}^{R_{s}} \\!\\!\\left[k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) - k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right)\\right] C_{s}\\,2\\pi r\\,\\mathrm{d}r.\n$$\nBy the definitions above, these evaluate to\n$$\nG_{\\mathrm{full}} \\;=\\; C_{c}\\left[k_{c}\\,I(\\sigma_{c}; R_{c}) - k_{s}\\,I(\\sigma_{s}; R_{c})\\right] \\;+\\; C_{s}\\left\\{k_{c}\\left[I(\\sigma_{c}; R_{s}) - I(\\sigma_{c}; R_{c})\\right] - k_{s}\\left[I(\\sigma_{s}; R_{s}) - I(\\sigma_{s}; R_{c})\\right]\\right\\}.\n$$\nSimilarly, the generator signal for the center-only stimulus ($C_{s}=0$) is\n$$\nG_{\\mathrm{center}} \\;=\\; C_{c}\\left[k_{c}\\,I(\\sigma_{c}; R_{c}) - k_{s}\\,I(\\sigma_{s}; R_{c})\\right].\n$$\nTherefore, the surround impact is\n$$\n\\Delta G \\;\\equiv\\; G_{\\mathrm{full}} - G_{\\mathrm{center}} \\;=\\; C_{s}\\left\\{k_{c}\\left[I(\\sigma_{c}; R_{s}) - I(\\sigma_{c}; R_{c})\\right] - k_{s}\\left[I(\\sigma_{s}; R_{s}) - I(\\sigma_{s}; R_{c})\\right]\\right\\}.\n$$\nSubstituting the closed-form expression for the annular integrals yields\n$$\nI(\\sigma; R_{s}) - I(\\sigma; R_{c}) \\;=\\; \\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma^{2}}\\right) - \\exp\\!\\left(-\\fracR_{s}^{2}}{2\\sigma^{2}}\\right),\n$$\nso that\n$$\n\\Delta G \\;=\\; C_{s}\\left\\{k_{c}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{c}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{c}^{2}}\\right)\\right] \\;-\\; k_{s}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{s}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{s}^{2}}\\right)\\right]\\right\\}.\n$$\nFinally, to adhere to the required exponential notation, we write the result using the exponential function:\n$$\n\\Delta G \\;=\\; C_{s}\\left\\{k_{c}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{c}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{c}^{2}}\\right)\\right] - k_{s}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{s}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{s}^{2}}\\right)\\right]\\right\\}.\n$$\nThis is the exact, closed-form analytical expression for the surround impact on the generator signal in arbitrary units (a.u.).",
            "answer": "$$\\boxed{C_{s}\\left\\{k_{c}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{c}^{2}}\\right)-\\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{c}^{2}}\\right)\\right]-k_{s}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{s}^{2}}\\right)-\\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{s}^{2}}\\right)\\right]\\right\\}}$$"
        },
        {
            "introduction": "Moving from theory to practice, a key challenge is the \"inverse problem\": estimating a neuron's unknown receptive field from its responses to a stimulus sequence. The success of this system identification heavily depends on the statistical properties of the stimulus itself. This problem explores how different stimulus designs—such as white noise and structured sequences—directly influence the precision of your receptive field estimate, providing critical insight for experimental design. ",
            "id": "4017993",
            "problem": "Consider a linear encoding model for a neuron's discrete-time response, with an unknown two-tap temporal receptive field (also called a linear time-invariant (LTI) filter) given by the vector $\\mathbf{k} = (k_{0}, k_{1})^{\\top}$. The scalar response at time $t$ is modeled as $r_{t} = k_{0} s_{t} + k_{1} s_{t-1} + \\epsilon_{t}$, where $s_{t}$ is the scalar stimulus at time $t$, and $\\epsilon_{t}$ is independent, identically distributed zero-mean Gaussian noise with variance $\\sigma_{\\epsilon}^{2}$. We will estimate $\\mathbf{k}$ by ordinary least squares using $N$ samples.\n\nYou will compare three stimulus protocols that are widely used for receptive field mapping and tuning curve estimation:\n\n- Zero-mean Gaussian white noise with variance $\\sigma_{s}^{2}$, i.e., $\\{s_{t}\\}$ are independent and identically distributed with $\\mathbb{E}[s_{t}] = 0$, $\\mathbb{E}[s_{t}^{2}] = \\sigma_{s}^{2}$.\n\n- A binary Maximum-Length Sequence (MLS), also called a pseudo-random binary sequence (PRBS), of period $L$ over $\\{+1,-1\\}$, repeated so that $N$ is an integer multiple of $L$; assume the sequence is balanced and uniformly cycles through all non-zero states of a linear feedback shift register. The MLS is treated as a stationary process over full periods.\n\n- A drifting sinusoidal grating stimulus $s_{t} = A \\cos(\\omega t + \\phi)$, where $A$ is the amplitude, $\\omega$ is the angular frequency in radians, and the initial phase $\\phi$ is independent and uniformly distributed over $[0, 2\\pi)$ and independent of $\\epsilon_{t}$.\n\nStarting from the linear model and first principles of least squares estimation under independent Gaussian noise, and using the Law of Large Numbers to replace empirical averages by expectations in the large-$N$ limit, derive for each protocol the $2 \\times 2$ stimulus covariance matrix $\\mathbf{C}$ whose $(i,j)$ entry is $\\mathbb{E}[s_{t-i} s_{t-j}]$ for $i,j \\in \\{0,1\\}$, then use it to obtain the asymptotic variance of the least squares estimator of $k_{0}$, denoted $\\mathrm{Var}(\\hat{k}_{0})$, in terms of $N$, $\\sigma_{\\epsilon}^{2}$, and the stimulus parameters. Work symbolically and do not substitute numerical values.\n\nReport your final expressions for $\\mathrm{Var}(\\hat{k}_{0})$ for the three protocols as a single row matrix in the order: white noise, MLS, drifting grating. No numerical rounding is required, and express angles in radians. No physical units are required in your answer.",
            "solution": "The problem asks for the asymptotic variance of the ordinary least squares (OLS) estimator for the first tap, $k_0$, of a two-tap linear filter. The neuron's response is modeled as $r_{t} = k_{0} s_{t} + k_{1} s_{t-1} + \\epsilon_{t}$. In vector form, this is $r_t = \\mathbf{s}_t^{\\top} \\mathbf{k} + \\epsilon_t$, where $\\mathbf{s}_t = (s_t, s_{t-1})^{\\top}$ and $\\mathbf{k} = (k_0, k_1)^{\\top}$.\n\nFor $N$ samples, the model is $\\mathbf{r} = \\mathbf{S} \\mathbf{k} + \\boldsymbol{\\epsilon}$, where $\\mathbf{r}$ is the $N \\times 1$ vector of responses, $\\mathbf{S}$ is the $N \\times 2$ stimulus design matrix whose $t$-th row is $\\mathbf{s}_t^{\\top}$, $\\mathbf{k}$ is the $2 \\times 1$ filter vector, and $\\boldsymbol{\\epsilon}$ is the $N \\times 1$ vector of noise samples. The noise $\\epsilon_t$ is i.i.d. Gaussian with $\\mathbb{E}[\\epsilon_t]=0$ and $\\mathbb{E}[\\epsilon_t^2]=\\sigma_{\\epsilon}^2$. This implies $\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}$ and $\\mathbb{E}[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{\\top}] = \\sigma_{\\epsilon}^2 \\mathbf{I}$, where $\\mathbf{I}$ is the $N \\times N$ identity matrix.\n\nThe OLS estimator for $\\mathbf{k}$ is given by $\\hat{\\mathbf{k}} = (\\mathbf{S}^{\\top}\\mathbf{S})^{-1} \\mathbf{S}^{\\top} \\mathbf{r}$. Substituting the model for $\\mathbf{r}$, we get:\n$$\n\\hat{\\mathbf{k}} = (\\mathbf{S}^{\\top}\\mathbf{S})^{-1} \\mathbf{S}^{\\top} (\\mathbf{S} \\mathbf{k} + \\boldsymbol{\\epsilon}) = (\\mathbf{S}^{\\top}\\mathbf{S})^{-1}(\\mathbf{S}^{\\top}\\mathbf{S})\\mathbf{k} + (\\mathbf{S}^{\\top}\\mathbf{S})^{-1}\\mathbf{S}^{\\top}\\boldsymbol{\\epsilon} = \\mathbf{k} + (\\mathbf{S}^{\\top}\\mathbf{S})^{-1}\\mathbf{S}^{\\top}\\boldsymbol{\\epsilon}\n$$\nThe estimation error is $\\hat{\\mathbf{k}} - \\mathbf{k} = (\\mathbf{S}^{\\top}\\mathbf{S})^{-1}\\mathbf{S}^{\\top}\\boldsymbol{\\epsilon}$. The covariance matrix of the estimator $\\hat{\\mathbf{k}}$ is defined as $\\mathrm{Cov}(\\hat{\\mathbf{k}}) = \\mathbb{E}[(\\hat{\\mathbf{k}} - \\mathbf{k})(\\hat{\\mathbf{k}} - \\mathbf{k})^{\\top}]$, where the expectation is over the noise distribution. Treating the stimulus matrix $\\mathbf{S}$ as fixed (or independent of the noise), we have:\n$$\n\\mathrm{Cov}(\\hat{\\mathbf{k}}) = \\mathbb{E}[((\\mathbf{S}^{\\top}\\mathbf{S})^{-1}\\mathbf{S}^{\\top}\\boldsymbol{\\epsilon})((\\mathbf{S}^{\\top}\\mathbf{S})^{-1}\\mathbf{S}^{\\top}\\boldsymbol{\\epsilon})^{\\top}] = (\\mathbf{S}^{\\top}\\mathbf{S})^{-1}\\mathbf{S}^{\\top} \\mathbb{E}[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{\\top}] \\mathbf{S} (\\mathbf{S}^{\\top}\\mathbf{S})^{-1}\n$$\nSubstituting $\\mathbb{E}[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{\\top}] = \\sigma_{\\epsilon}^2 \\mathbf{I}$:\n$$\n\\mathrm{Cov}(\\hat{\\mathbf{k}}) = (\\mathbf{S}^{\\top}\\mathbf{S})^{-1}\\mathbf{S}^{\\top} (\\sigma_{\\epsilon}^2 \\mathbf{I}) \\mathbf{S} (\\mathbf{S}^{\\top}\\mathbf{S})^{-1} = \\sigma_{\\epsilon}^2 (\\mathbf{S}^{\\top}\\mathbf{S})^{-1}\\mathbf{S}^{\\top}\\mathbf{S} (\\mathbf{S}^{\\top}\\mathbf{S})^{-1} = \\sigma_{\\epsilon}^2 (\\mathbf{S}^{\\top}\\mathbf{S})^{-1}\n$$\nThe problem asks for the asymptotic variance in the large-$N$ limit. We use the Law of Large Numbers to replace the empirical average $\\frac{1}{N}\\mathbf{S}^{\\top}\\mathbf{S}$ with its expectation, the stimulus covariance matrix $\\mathbf{C}$. The elements of $\\mathbf{S}^{\\top}\\mathbf{S}$ are $(\\mathbf{S}^{\\top}\\mathbf{S})_{ij} = \\sum_{t=1}^N s_{t-i} s_{t-j}$ for $i,j \\in \\{0,1\\}$. In the large-$N$ limit:\n$$\n\\frac{1}{N} \\mathbf{S}^{\\top}\\mathbf{S} \\to \\mathbf{C} = \\begin{pmatrix} \\mathbb{E}[s_t^2] & \\mathbb{E}[s_t s_{t-1}] \\\\ \\mathbb{E}[s_{t-1} s_t] & \\mathbb{E}[s_{t-1}^2] \\end{pmatrix}\n$$\nAssuming the stimulus process is stationary, $\\mathbb{E}[s_t^2] = \\mathbb{E}[s_{t-1}^2]$. Let's denote the autocorrelation function as $R(\\tau) = \\mathbb{E}[s_t s_{t-\\tau}]$. Then the covariance matrix is:\n$$\n\\mathbf{C} = \\begin{pmatrix} R(0) & R(1) \\\\ R(1) & R(0) \\end{pmatrix}\n$$\nThe asymptotic covariance of the estimator is $\\mathrm{Cov}(\\hat{\\mathbf{k}}) \\approx \\sigma_{\\epsilon}^2 (N\\mathbf{C})^{-1} = \\frac{\\sigma_{\\epsilon}^2}{N}\\mathbf{C}^{-1}$.\nThe inverse of $\\mathbf{C}$ is:\n$$\n\\mathbf{C}^{-1} = \\frac{1}{\\det(\\mathbf{C})} \\begin{pmatrix} R(0) & -R(1) \\\\ -R(1) & R(0) \\end{pmatrix} = \\frac{1}{R(0)^2 - R(1)^2} \\begin{pmatrix} R(0) & -R(1) \\\\ -R(1) & R(0) \\end{pmatrix}\n$$\nThe variance of the estimator $\\hat{k}_0$ is the first diagonal element of the covariance matrix:\n$$\n\\mathrm{Var}(\\hat{k}_0) = (\\mathrm{Cov}(\\hat{\\mathbf{k}}))_{00} = \\frac{\\sigma_{\\epsilon}^2}{N} (\\mathbf{C}^{-1})_{00} = \\frac{\\sigma_{\\epsilon}^2}{N} \\frac{R(0)}{R(0)^2 - R(1)^2}\n$$\nWe now apply this general formula to each of the three stimulus protocols.\n\n1.  **Gaussian White Noise (GWN)**\n    The stimulus $\\{s_t\\}$ is an i.i.d. process with $\\mathbb{E}[s_t] = 0$ and $\\mathbb{E}[s_t^2] = \\sigma_s^2$.\n    The autocorrelation at lag $\\tau=0$ is $R(0) = \\mathbb{E}[s_t^2] = \\sigma_s^2$.\n    For lag $\\tau=1$, since $s_t$ and $s_{t-1}$ are independent and zero-mean:\n    $R(1) = \\mathbb{E}[s_t s_{t-1}] = \\mathbb{E}[s_t]\\mathbb{E}[s_{t-1}] = 0 \\cdot 0 = 0$.\n    Substituting these into the variance formula:\n    $$\n    \\mathrm{Var}(\\hat{k}_0)_{\\mathrm{GWN}} = \\frac{\\sigma_{\\epsilon}^2}{N} \\frac{\\sigma_s^2}{(\\sigma_s^2)^2 - 0^2} = \\frac{\\sigma_{\\epsilon}^2}{N \\sigma_s^2}\n    $$\n\n2.  **Maximum-Length Sequence (MLS)**\n    The stimulus is a binary sequence over $\\{+1, -1\\}$ with period $L$. For such a sequence, $s_t^2 = 1$ for all $t$.\n    The autocorrelation at lag $\\tau=0$ is $R(0) = \\mathbb{E}[s_t^2] = 1$.\n    An ideal MLS has a two-valued periodic autocorrelation function. For a stationary process generated from this MLS, the expectation equals the time-average over one period. For any non-zero lag $\\tau$ (such that $1 \\le \\tau < L$):\n    $R(\\tau) = \\mathbb{E}[s_t s_{t-\\tau}] = -\\frac{1}{L}$.\n    So for $\\tau=1$, we have $R(1) = -1/L$.\n    Substituting $R(0)=1$ and $R(1)=-1/L$ into the variance formula:\n    $$\n    \\mathrm{Var}(\\hat{k}_0)_{\\mathrm{MLS}} = \\frac{\\sigma_{\\epsilon}^2}{N} \\frac{1}{1^2 - (-1/L)^2} = \\frac{\\sigma_{\\epsilon}^2}{N} \\frac{1}{1 - 1/L^2} = \\frac{\\sigma_{\\epsilon}^2}{N} \\frac{L^2}{L^2 - 1}\n    $$\n\n3.  **Drifting Sinusoidal Grating**\n    The stimulus is $s_t = A \\cos(\\omega t + \\phi)$, where $\\phi$ is a random variable uniformly distributed on $[0, 2\\pi)$. Expectations are taken with respect to $\\phi$.\n    The autocorrelation at lag $\\tau=0$ is the average power:\n    $R(0) = \\mathbb{E}[s_t^2] = \\mathbb{E}[A^2 \\cos^2(\\omega t + \\phi)] = A^2 \\mathbb{E}[\\frac{1}{2}(1 + \\cos(2\\omega t + 2\\phi))]$.\n    Since $\\mathbb{E}[\\cos(2\\omega t + 2\\phi)] = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\cos(2\\omega t + 2\\phi) d\\phi = 0$, we have:\n    $R(0) = \\frac{A^2}{2}$.\n    The autocorrelation at lag $\\tau=1$ is:\n    $R(1) = \\mathbb{E}[s_t s_{t-1}] = \\mathbb{E}[A \\cos(\\omega t + \\phi) \\cdot A \\cos(\\omega(t-1) + \\phi)]$.\n    Using the identity $\\cos(X)\\cos(Y) = \\frac{1}{2}(\\cos(X-Y) + \\cos(X+Y))$:\n    $R(1) = A^2 \\mathbb{E}[\\frac{1}{2}(\\cos(\\omega) + \\cos(2\\omega t - \\omega + 2\\phi))]$.\n    $R(1) = \\frac{A^2}{2}(\\cos(\\omega) + \\mathbb{E}[\\cos(2\\omega t - \\omega + 2\\phi)])$.\n    Again, the expectation of the cosine term with a $2\\phi$ argument is $0$. Thus:\n    $R(1) = \\frac{A^2}{2}\\cos(\\omega)$.\n    Substituting $R(0)$ and $R(1)$ into the variance formula:\n    $$\n    \\mathrm{Var}(\\hat{k}_0)_{\\mathrm{Sine}} = \\frac{\\sigma_{\\epsilon}^2}{N} \\frac{R(0)}{R(0)^2 - R(1)^2} = \\frac{\\sigma_{\\epsilon}^2}{N} \\frac{A^2/2}{(\\frac{A^2}{2})^2 - (\\frac{A^2}{2}\\cos(\\omega))^2}\n    $$\n    $$\n    \\mathrm{Var}(\\hat{k}_0)_{\\mathrm{Sine}} = \\frac{\\sigma_{\\epsilon}^2}{N} \\frac{A^2/2}{(\\frac{A^2}{2})^2(1 - \\cos^2(\\omega))} = \\frac{\\sigma_{\\epsilon}^2}{N} \\frac{1}{(\\frac{A^2}{2})\\sin^2(\\omega)} = \\frac{2\\sigma_{\\epsilon}^2}{N A^2 \\sin^2(\\omega)}\n    $$\n    This expression is valid for $\\omega \\neq n\\pi$ for any integer $n$, as otherwise the stimulus components $s_t$ and $s_{t-1}$ become linearly dependent and the covariance matrix $\\mathbf{C}$ is singular.\n\nThe final expressions for $\\mathrm{Var}(\\hat{k}_{0})$ for the three protocols are:\n- White noise: $\\frac{\\sigma_{\\epsilon}^2}{N \\sigma_s^2}$\n- MLS: $\\frac{\\sigma_{\\epsilon}^2 L^2}{N(L^2-1)}$\n- Drifting grating: $\\frac{2\\sigma_{\\epsilon}^2}{N A^2 \\sin^2(\\omega)}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma_{\\epsilon}^{2}}{N \\sigma_{s}^{2}} & \\frac{\\sigma_{\\epsilon}^{2} L^{2}}{N(L^{2}-1)} & \\frac{2\\sigma_{\\epsilon}^{2}}{N A^{2} \\sin^{2}(\\omega)}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In practice, estimating high-dimensional receptive fields from limited and noisy data can lead to unstable and unreliable results. This exercise introduces a powerful Bayesian framework to overcome this challenge through regularization, a concept at the heart of modern machine learning and statistics. By deriving the Maximum A Posteriori (MAP) estimator, you will see how incorporating a prior belief about the filter's structure—in this case, a preference for small coefficients—yields a more robust and plausible estimate. ",
            "id": "4017986",
            "problem": "Consider a linear receptive field model for a sensory neuron in which the measured response vector $\\mathbf{r} \\in \\mathbb{R}^{T}$ to a sequence of stimuli is modeled as $\\mathbf{r} = \\mathbf{X}\\mathbf{k} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{X} \\in \\mathbb{R}^{T \\times d}$ is the stimulus design matrix (each row a stimulus and columns corresponding to $d$ features), $\\mathbf{k} \\in \\mathbb{R}^{d}$ is the unknown receptive field (filter) to be estimated, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{T}$ is additive measurement noise. Assume the noise is independent and identically distributed Gaussian with zero mean and covariance $\\sigma^{2}\\mathbf{I}_{T}$, and that the prior over the receptive field is Gaussian with isotropic precision, specified by $p(\\mathbf{k}) \\propto \\exp\\!\\big(-\\lambda \\|\\mathbf{k}\\|_{2}^{2}\\big)$ for a known constant $\\lambda > 0$. Using Bayes’ theorem as the foundational principle and assuming the linear-Gaussian observation model, derive the Maximum A Posteriori (MAP) estimator $\\hat{\\mathbf{k}}_{\\text{MAP}}$ that maximizes the posterior distribution $p(\\mathbf{k}\\mid \\mathbf{r}, \\mathbf{X})$. Express your final result in closed form in terms of $\\mathbf{X}$, $\\mathbf{r}$, $\\sigma^{2}$, and $\\lambda$. Then, starting from the definitions of estimator bias and variance for linear estimators under Gaussian noise, explain how the $\\ell_{2}$ penalty implied by the Gaussian prior implements ridge regularization that trades bias for variance in the filter estimates as $\\lambda$ varies, without invoking any shortcut formulas. Your final answer must be a single closed-form analytic expression for $\\hat{\\mathbf{k}}_{\\text{MAP}}$. No numerical approximation is required.",
            "solution": "The derivation of the MAP estimator $\\hat{\\mathbf{k}}_{\\text{MAP}}$ begins with Bayes' theorem, which states that the posterior distribution of the receptive field $\\mathbf{k}$ given the measured response $\\mathbf{r}$ and stimulus matrix $\\mathbf{X}$ is proportional to the product of the likelihood of the data and the prior probability of the receptive field:\n$$\np(\\mathbf{k} \\mid \\mathbf{r}, \\mathbf{X}) \\propto p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) p(\\mathbf{k})\n$$\nThe MAP estimate is the value of $\\mathbf{k}$ that maximizes this posterior probability. It is computationally more convenient to maximize the logarithm of the posterior, as the logarithm is a monotonic function. The log-posterior is given by:\n$$\n\\ln p(\\mathbf{k} \\mid \\mathbf{r}, \\mathbf{X}) = \\ln p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) + \\ln p(\\mathbf{k}) + C\n$$\nwhere $C$ is a constant that does not depend on $\\mathbf{k}$.\n\nFirst, we define the likelihood term, $p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X})$. The model is $\\mathbf{r} = \\mathbf{X}\\mathbf{k} + \\boldsymbol{\\varepsilon}$, where the noise $\\boldsymbol{\\varepsilon}$ is drawn from a zero-mean Gaussian distribution with covariance $\\sigma^{2}\\mathbf{I}_{T}$. This implies that the response $\\mathbf{r}$ is also Gaussian, with mean $\\mathbf{X}\\mathbf{k}$ and covariance $\\sigma^{2}\\mathbf{I}_{T}$. The probability density function is:\n$$\np(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) = \\frac{1}{(2\\pi\\sigma^{2})^{T/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} (\\mathbf{r} - \\mathbf{X}\\mathbf{k})^{T}(\\mathbf{r} - \\mathbf{X}\\mathbf{k})\\right) = \\frac{1}{(2\\pi\\sigma^{2})^{T/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|\\mathbf{r} - \\mathbf{X}\\mathbf{k}\\|_{2}^{2}\\right)\n$$\nThe log-likelihood is therefore:\n$$\n\\ln p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) = -\\frac{T}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}} \\|\\mathbf{r} - \\mathbf{X}\\mathbf{k}\\|_{2}^{2}\n$$\n\nNext, we consider the prior distribution over the receptive field, $p(\\mathbf{k})$, which is given as $p(\\mathbf{k}) \\propto \\exp(-\\lambda \\|\\mathbf{k}\\|_{2}^{2})$. The log-prior is:\n$$\n\\ln p(\\mathbf{k}) = -\\lambda \\|\\mathbf{k}\\|_{2}^{2} + C'\n$$\nwhere $C'$ is another constant independent of $\\mathbf{k}$.\n\nCombining these, the log-posterior objective function $\\mathcal{L}(\\mathbf{k})$ to be maximized with respect to $\\mathbf{k}$ is:\n$$\n\\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} \\|\\mathbf{r} - \\mathbf{X}\\mathbf{k}\\|_{2}^{2} - \\lambda \\|\\mathbf{k}\\|_{2}^{2} + \\text{constants}\n$$\nTo find the maximum, we compute the gradient of $\\mathcal{L}(\\mathbf{k})$ with respect to $\\mathbf{k}$ and set it to the zero vector. We first expand the squared norms:\n$$\n\\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} (\\mathbf{r}^{T}\\mathbf{r} - 2\\mathbf{r}^{T}\\mathbf{X}\\mathbf{k} + \\mathbf{k}^{T}\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - \\lambda \\mathbf{k}^{T}\\mathbf{k} + \\text{constants}\n$$\nThe gradient is:\n$$\n\\nabla_{\\mathbf{k}} \\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} (-2\\mathbf{X}^{T}\\mathbf{r} + 2\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - 2\\lambda\\mathbf{k}\n$$\nSetting the gradient to $\\mathbf{0}$:\n$$\n\\frac{1}{\\sigma^{2}}(\\mathbf{X}^{T}\\mathbf{r} - \\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - 2\\lambda\\mathbf{k} = \\mathbf{0}\n$$\nMultiplying by $\\sigma^{2}$ and rearranging terms to solve for $\\mathbf{k}$:\n$$\n\\mathbf{X}^{T}\\mathbf{r} - \\mathbf{X}^{T}\\mathbf{X}\\mathbf{k} = 2\\sigma^{2}\\lambda\\mathbf{k} \\\\\n\\mathbf{X}^{T}\\mathbf{r} = \\mathbf{X}^{T}\\mathbf{X}\\mathbf{k} + 2\\sigma^{2}\\lambda\\mathbf{k} \\\\\n\\mathbf{X}^{T}\\mathbf{r} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})\\mathbf{k}\n$$\nwhere $\\mathbf{I}_{d}$ is the $d \\times d$ identity matrix. The MAP estimator $\\hat{\\mathbf{k}}_{\\text{MAP}}$ is found by pre-multiplying by the inverse of the matrix term. Since $\\mathbf{X}^{T}\\mathbf{X}$ is positive semi-definite and $\\lambda > 0$, $\\sigma^2 > 0$, the matrix $(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})$ is positive definite and thus invertible.\n$$\n\\hat{\\mathbf{k}}_{\\text{MAP}} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{r}\n$$\nThis is the closed-form expression for the MAP estimator, which is equivalent to the solution for ridge regression.\n\nNow, we explain how the $\\ell_{2}$ penalty implements a bias-variance tradeoff. Let the true, unknown receptive field be $\\mathbf{k}_{\\text{true}}$. The observed data are generated according to $\\mathbf{r} = \\mathbf{X}\\mathbf{k}_{\\text{true}} + \\boldsymbol{\\varepsilon}$. The estimator is a linear function of the data $\\mathbf{r}$.\n\nThe bias of an estimator is the difference between its expected value and the true value, where the expectation is taken over the noise distribution:\n$$\n\\text{Bias}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] - \\mathbf{k}_{\\text{true}}\n$$\nWe calculate the expected value:\n$$\n\\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] = \\mathbb{E}[(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{r}] \\\\\n= (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbb{E}[\\mathbf{r}]\n$$\nSince $\\mathbb{E}[\\mathbf{r}] = \\mathbb{E}[\\mathbf{X}\\mathbf{k}_{\\text{true}} + \\boldsymbol{\\varepsilon}] = \\mathbf{X}\\mathbf{k}_{\\text{true}} + \\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{X}\\mathbf{k}_{\\text{true}}$, we have:\n$$\n\\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}_{\\text{true}}\n$$\nThe bias is therefore:\n$$\n\\text{Bias}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = [(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X} - \\mathbf{I}_{d}]\\mathbf{k}_{\\text{true}}\n$$\nIf $\\lambda = 0$ (the Maximum Likelihood case), and assuming $\\mathbf{X}^{T}\\mathbf{X}$ is invertible, the bias is $[(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{X} - \\mathbf{I}_{d}]\\mathbf{k}_{\\text{true}} = [\\mathbf{I}_{d} - \\mathbf{I}_{d}]\\mathbf{k}_{\\text{true}} = \\mathbf{0}$. The Maximum Likelihood Estimator (MLE) is unbiased. However, for any $\\lambda > 0$, the term $(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X}$ is not the identity matrix. Thus, the MAP estimator is biased. This bias term effectively shrinks the estimated filter coefficients towards zero compared to the true values, and the magnitude of this bias increases as $\\lambda$ increases.\n\nThe variance of the estimator is its covariance matrix:\n$$\n\\text{Var}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\mathbb{E}\\big[(\\hat{\\mathbf{k}}_{\\text{MAP}} - \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}])(\\hat{\\mathbf{k}}_{\\text{MAP}} - \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}])^{T}\\big]\n$$\nLet $\\mathbf{M} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}$. Then $\\hat{\\mathbf{k}}_{\\text{MAP}} = \\mathbf{M}\\mathbf{r}$ and $\\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] = \\mathbf{M}\\mathbb{E}[\\mathbf{r}]$. The deviation from the mean is:\n$$\n\\hat{\\mathbf{k}}_{\\text{MAP}} - \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] = \\mathbf{M}(\\mathbf{r} - \\mathbb{E}[\\mathbf{r}]) = \\mathbf{M}\\boldsymbol{\\varepsilon}\n$$\nThe variance is then:\n$$\n\\text{Var}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\mathbb{E}[\\mathbf{M}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{T}\\mathbf{M}^{T}] = \\mathbf{M} \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{T}] \\mathbf{M}^{T}\n$$\nSince $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{T}] = \\sigma^{2}\\mathbf{I}_{T}$, we get:\n$$\n\\text{Var}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\sigma^{2}\\mathbf{M}\\mathbf{M}^{T} = \\sigma^{2}(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\n$$\nFor the unbiased MLE ($\\lambda = 0$), the variance is $\\sigma^{2}(\\mathbf{X}^{T}\\mathbf{X})^{-1}$. If $\\mathbf{X}^{T}\\mathbf{X}$ is ill-conditioned (has very small eigenvalues), its inverse will have very large eigenvalues, leading to extremely high variance in the estimate. When $\\lambda > 0$, the term $2\\sigma^{2}\\lambda\\mathbf{I}_{d}$ is added to $\\mathbf{X}^{T}\\mathbf{X}$ before inversion. This increases all eigenvalues by $2\\sigma^{2}\\lambda$, making the matrix well-conditioned and stabilizing the inversion. Consequently, the magnitude of the terms in the resulting covariance matrix is reduced. As $\\lambda$ increases, the variance of the estimator decreases.\n\nIn summary, the Gaussian prior on $\\mathbf{k}$ translates to an $\\ell_{2}$ penalty term in the optimization. This penalty introduces a bias into the MAP estimate, systematically shrinking it towards the prior mean (zero). In exchange for this bias, the variance of the estimate is substantially reduced, especially when the stimulus matrix $\\mathbf{X}$ leads to an ill-conditioned or singular $\\mathbf{X}^{T}\\mathbf{X}$ matrix. This exchange is the classic bias-variance tradeoff. The parameter $\\lambda$ controls this tradeoff: increasing $\\lambda$ increases the bias but decreases the variance. An optimal $\\lambda$ can be chosen to minimize the total mean squared error, which is the sum of the squared bias and the variance.",
            "answer": "$$\n\\boxed{(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{r}}\n$$"
        }
    ]
}