{
    "hands_on_practices": [
        {
            "introduction": "高斯差分（DoG）模型是描述早期视觉通路中，如视网膜神经节细胞，感受野中文献经典的“中心-周边”拮抗结构的有力数学工具。通过将感受野建模为一个兴奋性高斯函数与一个抑制性高斯函数的差，我们可以精确地分析神经元如何对不同空间模式的刺激产生反应。本练习旨在通过推导一个由中心盘和周边环组成的刺激所引发的响应，来加深对线性滤波概念的理解，并揭示周边区域如何调节对中心刺激的响应强度。",
            "id": "4017975",
            "problem": "早期视觉通路中的一个中继神经元被建模为一个具有圆对称感受野的神经元，该感受野由高斯差分 (DoG) 给出，其中心是兴奋性的，而周边是抑制性的。空间感受野核定义为\n$$\nh(\\mathbf{x}) \\equiv h(r) = k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) \\;-\\; k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right),\n$$\n其中 $r = \\|\\mathbf{x}\\|$，$k_{c} > 0$，$k_{s} > 0$，$ \\sigma_{c} > 0$，且 $\\sigma_{s} > 0$。线性生成信号由标准的线性滤波操作定义\n$$\nG \\;=\\; \\int_{\\mathbb{R}^{2}} h(\\mathbf{x})\\,s(\\mathbf{x})\\,\\mathrm{d}^{2}\\mathbf{x},\n$$\n其中 $s(\\mathbf{x})$ 是刺激对比度场（无量纲）。考虑一个刺激，它由一个半径为 $R_{c}$、对比度均匀为 $C_{c}$ 的中心圆盘，以及一个从半径 $R_{c}$ 延伸到 $R_{s}$、对比度均匀为 $C_{s}$ 的环状区域组成，在半径 $R_{s}$ 之外的对比度为零。也就是说，\n$$\ns(r) \\;=\\; \\begin{cases}\nC_{c}, & 0 \\le r \\le R_{c},\\\\\nC_{s}, & R_{c} < r \\le R_{s},\\\\\n0, & r > R_{s}.\n\\end{cases}\n$$\n令 $G_{\\mathrm{full}}$ 表示上述完整刺激的生成信号，令 $G_{\\mathrm{center}}$ 表示仅中心圆盘的生成信号（即，在 $0 \\le r \\le R_{c}$ 上有相同的 $C_{c}$，但在 $r > R_{c}$ 时 $C_{s} = 0$）。仅使用给定的定义和标准微积分，推导周边影响\n$$\n\\Delta G \\equiv G_{\\mathrm{full}} - G_{\\mathrm{center}}\n$$\n的一个精确的闭式解析表达式，用 $C_{c}$、$C_{s}$、$k_{c}$、$k_{s}$、$\\sigma_{c}$、$\\sigma_{s}$、$R_{c}$ 和 $R_{s}$ 表示。\n\n将您的最终答案表示为单个简化的符号表达式。以任意单位 (a.u.) 报告生成信号。",
            "solution": "我们从线性生成信号的定义开始，即感受野与刺激的空间内积：\n$$\nG \\;=\\; \\int_{\\mathbb{R}^{2}} h(\\mathbf{x})\\,s(\\mathbf{x})\\,\\mathrm{d}^{2}\\mathbf{x}.\n$$\n感受野是径向对称的，并由高斯差分 (DoG) 给出：\n$$\nh(r) \\;=\\; k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) \\;-\\; k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right),\n$$\n其中每个高斯分量在整个平面上的积分被归一化为1：\n$$\n\\int_{\\mathbb{R}^{2}} \\frac{1}{2\\pi \\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\mathrm{d}^{2}\\mathbf{x} \\;=\\; 1.\n$$\n刺激也是径向对称的，并且在半径上是分段常数。因为 $h$ 和 $s$ 都只依赖于 $r = \\|\\mathbf{x}\\|$，所以这个二维积分在极坐标下简化为：\n$$\nG \\;=\\; \\int_{0}^{\\infty} h(r)\\,s(r)\\,2\\pi r\\,\\mathrm{d}r.\n$$\n我们定义一个归一化高斯函数在半径为 $R$ 的圆盘上的累积积分：\n$$\nI(\\sigma; R) \\;\\equiv\\; \\int_{0}^{R} \\frac{1}{2\\pi \\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\,2\\pi r\\,\\mathrm{d}r \\;=\\; \\int_{0}^{R} \\frac{1}{\\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\,r\\,\\mathrm{d}r.\n$$\n进行换元 $u = \\frac{r^{2}}{2\\sigma^{2}}$，因此 $\\mathrm{d}u = \\frac{r}{\\sigma^{2}}\\mathrm{d}r$ 且 $r\\,\\mathrm{d}r = \\sigma^{2}\\mathrm{d}u$。积分变为：\n$$\nI(\\sigma; R) \\;=\\; \\int_{u=0}^{u=\\frac{R^{2}}{2\\sigma^{2}}} \\exp(-u)\\,\\mathrm{d}u \\;=\\; 1 - \\exp\\!\\left(-\\frac{R^{2}}{2\\sigma^{2}}\\right).\n$$\n因此，一个归一化高斯函数在从 $R_{1}$ 到 $R_{2}$ 的环状区域上的积分（其中 $0 \\le R_{1} \\le R_{2}$）是：\n$$\nI(\\sigma; R_{2}) - I(\\sigma; R_{1}) \\;=\\; \\left[1 - \\exp\\!\\left(-\\frac{R_{2}^{2}}{2\\sigma^{2}}\\right)\\right] - \\left[1 - \\exp\\!\\left(-\\frac{R_{1}^{2}}{2\\sigma^{2}}\\right)\\right] \\;=\\; \\exp\\!\\left(-\\frac{R_{1}^{2}}{2\\sigma^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{2}^{2}}{2\\sigma^{2}}\\right).\n$$\n利用积分的线性和 $h(r)$ 的 DoG 分解，我们将生成信号表示为中心圆盘和周边环状区域贡献的总和：\n$$\nG_{\\mathrm{full}} \\;=\\; \\int_{0}^{R_{c}} \\!\\!\\left[k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) - k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right)\\right] C_{c}\\,2\\pi r\\,\\mathrm{d}r\n$$\n$$\n\\quad + \\int_{R_{c}}^{R_{s}} \\!\\!\\left[k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) - k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right)\\right] C_{s}\\,2\\pi r\\,\\mathrm{d}r.\n$$\n根据上述定义，这些积分的计算结果为\n$$\nG_{\\mathrm{full}} \\;=\\; C_{c}\\left[k_{c}\\,I(\\sigma_{c}; R_{c}) - k_{s}\\,I(\\sigma_{s}; R_{c})\\right] \\;+\\; C_{s}\\left\\{k_{c}\\left[I(\\sigma_{c}; R_{s}) - I(\\sigma_{c}; R_{c})\\right] - k_{s}\\left[I(\\sigma_{s}; R_{s}) - I(\\sigma_{s}; R_{c})\\right]\\right\\}.\n$$\n类似地，仅有中心刺激（$C_{s}=0$）时的生成信号为\n$$\nG_{\\mathrm{center}} \\;=\\; C_{c}\\left[k_{c}\\,I(\\sigma_{c}; R_{c}) - k_{s}\\,I(\\sigma_{s}; R_{c})\\right].\n$$\n因此，周边影响为\n$$\n\\Delta G \\;\\equiv\\; G_{\\mathrm{full}} - G_{\\mathrm{center}} \\;=\\; C_{s}\\left\\{k_{c}\\left[I(\\sigma_{c}; R_{s}) - I(\\sigma_{c}; R_{c})\\right] - k_{s}\\left[I(\\sigma_{s}; R_{s}) - I(\\sigma_{s}; R_{c})\\right]\\right\\}.\n$$\n代入环形积分的闭式表达式，得到\n$$\nI(\\sigma; R_{s}) - I(\\sigma; R_{c}) \\;=\\; \\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma^{2}}\\right),\n$$\n因此\n$$\n\\Delta G \\;=\\; C_{s}\\left\\{k_{c}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{c}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{c}^{2}}\\right)\\right] \\;-\\; k_{s}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{s}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{s}^{2}}\\right)\\right]\\right\\}.\n$$\n这就是周边对生成信号影响的精确、闭式解析表达式，单位为任意单位 (a.u.)。",
            "answer": "$$\\boxed{C_{s}\\left\\{k_{c}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{c}^{2}}\\right)-\\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{c}^{2}}\\right)\\right]-k_{s}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{s}^{2}}\\right)-\\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{s}^{2}}\\right)\\right]\\right\\}}$$"
        },
        {
            "introduction": "神经元的响应并非一成不变，即使在相同刺激下，每次试验的脉冲计数也存在随机波动。理解并建模这种变异性对于探究神经编码的可靠性至关重要。本练习将对比三种经典的脉冲计数生成模型——泊松、伯努利-二项和负二项分布模型——来探讨如何量化神经响应的变异性（通过法诺因子），并进一步运用费雪信息（Fisher Information）来分析这种超离散（overdispersion）现象如何影响神经元对刺激的编码能力。",
            "id": "4017987",
            "problem": "用标量刺激 $s$ 探测单个感觉神经元，并在持续时间为 $T$ 的固定计数窗口内进行记录。令 $N$ 表示窗口内的脉冲计数。作为刺激函数，平均脉冲计数表示为 $\\mu(s) = \\mathbb{E}[N \\mid s]$。考虑以下三种备选生成模型，用于描述在给定 $s$ 条件下 $N$ 的试次间变异性：\n\n- 泊松脉冲发放：在给定 $s$ 的条件下，$N$ 服从均值为 $\\mu(s)$ 的泊松分布。\n\n- 伯努利-二项脉冲发放：将窗口划分为 $m$ 个相等的子区间，$m \\in \\mathbb{N}$ 足够大，以至于任何子区间内最多只能出现一个脉冲。在给定 $s$ 的条件下，每个子区间包含一个脉冲的概率为 $p(s) \\in [0,1]$，且各子区间之间相互独立。总计数 $N$ 是所有子区间的计数之和。\n\n- 通过泊松-伽马混合实现的负二项脉冲发放：在给定 $s$ 的条件下，由于缓慢的增益调制，瞬时速率 $\\lambda$ 在不同试次间波动，并服从伽马分布，其形状参数为 $k > 0$，尺度参数为 $\\theta(s)$，选择 $\\theta(s)$ 使得 $\\mathbb{E}[\\lambda T \\mid s] = \\mu(s)$。在给定 $\\lambda$ 和 $s$ 的条件下，计数 $N$ 服从均值为 $\\lambda T$ 的泊松分布。等价地，$N$ 在给定 $s$ 下的边缘分布是形状为 $k$、均值为 $\\mu(s)$ 的负二项分布，其概率质量函数为\n$$\np(N \\mid \\mu(s), k) = \\frac{\\Gamma(N + k)}{\\Gamma(k)\\,N!} \\left(\\frac{k}{k + \\mu(s)}\\right)^{k} \\left(\\frac{\\mu(s)}{k + \\mu(s)}\\right)^{N}.\n$$\n\n第 $1$ 部分：从上述每个模型的定义出发，推导法诺因子 $F(s) = \\mathrm{Var}[N \\mid s] / \\mathbb{E}[N \\mid s]$，并用 $\\mu(s)$、$p(s)$ 和 $k$ 表示。用最简符号形式表示你的答案。\n\n第 $2$ 部分：假设调谐曲线通过正标量增益 $g > 0$ 进行参数化，即 $\\mu(s; g) = g\\, f(s)$，其中 $f(s) > 0$ 是一个已知的确定性函数。将 $k$ 视为已知且固定的。对于在刺激 $s$ 下的单次观测，推导在以下模型中关于 $g$ 的期望费雪信息（Fisher information, FI）：\n\n- 泊松模型，以及\n\n- 具有上述概率质量函数的负二项模型。\n\n然后推导比率 $R(s; g) = I_{\\mathrm{NB}}(s; g) / I_{\\mathrm{Pois}}(s; g)$，将其表示为 $\\mu(s; g)$ 和 $k$ 的函数。\n\n你的最终答案必须是一个单行矩阵，按以下顺序包含第 $1$ 部分的三个法诺因子和第 $2$ 部分的信息比率 $R(s; g)$：\n$$\n\\left[F_{\\mathrm{Pois}}(s) ,\\, F_{\\mathrm{Bern}}(s) ,\\, F_{\\mathrm{NB}}(s) ,\\, R(s; g)\\right].\n$$\n所有内容均以符号表示。不包含任何单位。无需进行数值四舍五入。",
            "solution": "### 第1部分：法诺因子推导\n\n法诺因子定义为 $F(s) = \\frac{\\mathrm{Var}[N \\mid s]}{\\mathbb{E}[N \\mid s]}$。我们将为三个模型分别计算此值。\n\n**1. 泊松模型**\n对于一个均值为 $\\mu(s)$ 的泊松分布随机变量 $N$，其方差也等于其均值。\n-   均值：$\\mathbb{E}[N \\mid s] = \\mu(s)$\n-   方差：$\\mathrm{Var}[N \\mid s] = \\mu(s)$\n因此，法诺因子为：\n$$\nF_{\\mathrm{Pois}}(s) = \\frac{\\mathrm{Var}[N \\mid s]}{\\mathbb{E}[N \\mid s]} = \\frac{\\mu(s)}{\\mu(s)} = 1\n$$\n\n**2. 伯努利-二项模型**\n在此模型中，脉冲计数 $N$ 是 $m$ 次独立伯努利试验的总和，每次试验成功（即一次脉冲）的概率为 $p(s)$。因此，$N$ 服从二项分布，$N \\sim \\text{Binomial}(m, p(s))$。\n-   均值：$\\mathbb{E}[N \\mid s] = m p(s)$。问题中将其表示为 $\\mu(s)$。\n-   方差：$\\mathrm{Var}[N \\mid s] = m p(s) (1 - p(s))$\n法诺因子为：\n$$\nF_{\\mathrm{Bern}}(s) = \\frac{m p(s) (1 - p(s))}{m p(s)} = 1 - p(s)\n$$\n\n**3. 负二项模型**\n问题指出，$N$ 服从均值为 $\\mu(s)$、形状参数为 $k$ 的负二项分布。\n-   均值：$\\mathbb{E}[N \\mid s] = \\mu(s)$\n对于此参数化的负二项分布，方差为：\n$$\n\\mathrm{Var}[N \\mid s] = \\mu(s) \\left(1 + \\frac{\\mu(s)}{k}\\right)\n$$\n因此，法诺因子为：\n$$\nF_{\\mathrm{NB}}(s) = \\frac{\\mu(s) \\left(1 + \\frac{\\mu(s)}{k}\\right)}{\\mu(s)} = 1 + \\frac{\\mu(s)}{k}\n$$\n\n### 第2部分：费雪信息和比率\n\n参数 $\\theta$ 的费雪信息由 $I(\\theta) = \\mathbb{E}\\left[ \\left(\\frac{\\partial \\ln p(N \\mid \\theta)}{\\partial \\theta}\\right)^2 \\right]$ 给出。在这里，参数是增益 $g$，它通过平均脉冲计数 $\\mu(s; g) = g f(s)$ 影响概率分布。我们可以使用链式法则：\n$$\n\\frac{\\partial \\ln p}{\\partial g} = \\frac{\\partial \\ln p}{\\partial \\mu} \\frac{\\partial \\mu}{\\partial g} = \\frac{\\partial \\ln p}{\\partial \\mu} f(s)\n$$\n因此，$g$ 的费雪信息为：\n$$\nI(s; g) = \\mathbb{E}\\left[ \\left(\\frac{\\partial \\ln p}{\\partial \\mu} f(s)\\right)^2 \\right] = (f(s))^2 \\mathbb{E}\\left[ \\left(\\frac{\\partial \\ln p}{\\partial \\mu}\\right)^2 \\right] = (f(s))^2 I(\\mu)\n$$\n其中 $I(\\mu)$ 是均值参数 $\\mu$ 的费雪信息。\n\n**1. 泊松模型的费雪信息 ($I_{\\mathrm{Pois}}$)**\n泊松模型的对数似然为 $\\ln p(N \\mid \\mu) = N \\ln(\\mu) - \\mu - \\ln(N!)$。关于 $\\mu$ 的导数为：\n$$\n\\frac{\\partial \\ln p}{\\partial \\mu} = \\frac{N}{\\mu} - 1 = \\frac{N - \\mu}{\\mu}\n$$\n$I_{\\mathrm{Pois}}(\\mu)$ 是导数平方的期望值：\n$$\nI_{\\mathrm{Pois}}(\\mu) = \\mathbb{E}\\left[\\left(\\frac{N - \\mu}{\\mu}\\right)^2\\right] = \\frac{1}{\\mu^2} \\mathbb{E}[(N-\\mu)^2] = \\frac{\\mathrm{Var}[N]}{\\mu^2} = \\frac{\\mu}{\\mu^2} = \\frac{1}{\\mu}\n$$\n因此，$g$ 的费雪信息为：\n$$\nI_{\\mathrm{Pois}}(s; g) = \\frac{(f(s))^2}{\\mu(s; g)}\n$$\n\n**2. 负二项模型的费雪信息 ($I_{\\mathrm{NB}}$)**\n负二项模型的对数似然为 $\\ln p(N|\\mu,k) = \\ln(\\Gamma(N+k)) - \\ln(\\Gamma(k)) - \\ln(N!) + k\\ln(k) - (N+k)\\ln(k+\\mu) + N\\ln(\\mu)$。关于 $\\mu$ 的导数为：\n$$\n\\frac{\\partial \\ln p}{\\partial \\mu} = -\\frac{N+k}{k+\\mu} + \\frac{N}{\\mu} = \\frac{-N\\mu - k\\mu + Nk + N\\mu}{\\mu(k+\\mu)} = \\frac{k(N-\\mu)}{\\mu(k+\\mu)}\n$$\n$I_{\\mathrm{NB}}(\\mu)$ 是导数平方的期望值：\n$$\nI_{\\mathrm{NB}}(\\mu) = \\mathbb{E}\\left[\\left(\\frac{k(N-\\mu)}{\\mu(k+\\mu)}\\right)^2\\right] = \\frac{k^2}{(\\mu(k+\\mu))^2} \\mathbb{E}[(N-\\mu)^2] = \\frac{k^2 \\mathrm{Var}[N]}{(\\mu(k+\\mu))^2}\n$$\n代入方差 $\\mathrm{Var}[N] = \\mu(1+\\frac{\\mu}{k}) = \\mu\\frac{k+\\mu}{k}$：\n$$\nI_{\\mathrm{NB}}(\\mu) = \\frac{k^2}{(\\mu(k+\\mu))^2} \\left(\\mu\\frac{k+\\mu}{k}\\right) = \\frac{k}{\\mu(k+\\mu)}\n$$\n因此，$g$ 的费雪信息为：\n$$\nI_{\\mathrm{NB}}(s; g) = \\frac{k (f(s))^2}{\\mu(s; g)(k+\\mu(s; g))}\n$$\n\n**3. 比率 $R(s; g)$**\n最后，我们计算比率 $R(s; g) = I_{\\mathrm{NB}}(s; g) / I_{\\mathrm{Pois}}(s; g)$:\n$$\nR(s; g) = \\frac{\\frac{k (f(s))^2}{\\mu(s; g)(k+\\mu(s; g))}}{\\frac{(f(s))^2}{\\mu(s; g)}} = \\frac{k (f(s))^2}{\\mu(s; g)(k+\\mu(s; g))} \\cdot \\frac{\\mu(s; g)}{(f(s))^2} = \\frac{k}{k+\\mu(s; g)}\n$$\n这个比率表明，与泊松神经元相比，负二项神经元提供的信息减少了一个与其超离散（overdispersion）相关的因子。\n\n将所有结果汇总成一个单行矩阵。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 1 - p(s) & 1 + \\frac{\\mu(s)}{k} & \\frac{k}{k + \\mu(s; g)} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在理论模型与实验数据之间架起桥梁，是计算神经科学的核心任务之一。从充满噪声的神经记录中准确地估计感受野是一个典型的逆问题，当刺激维度高或数据量有限时，该问题通常是病态的（ill-posed）。本练习将介绍最大后验（MAP）估计方法，展示如何通过引入高斯先验（等价于岭回归）来正则化估计过程，从而在偏差与方差之间进行权衡，获得更稳定和鲁棒的感受野滤波器。",
            "id": "4017986",
            "problem": "考虑一个用于感觉神经元的线性感受野模型，其中对于一个刺激序列，测量到的响应向量 $\\mathbf{r} \\in \\mathbb{R}^{T}$ 被建模为 $\\mathbf{r} = \\mathbf{X}\\mathbf{k} + \\boldsymbol{\\varepsilon}$。这里，$\\mathbf{X} \\in \\mathbb{R}^{T \\times d}$ 是刺激设计矩阵（每行是一个刺激，列对应于 $d$ 个特征），$\\mathbf{k} \\in \\mathbb{R}^{d}$ 是待估计的未知感受野（滤波器），$\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{T}$ 是加性测量噪声。假设噪声是独立同分布的高斯噪声，其均值为零，协方差为 $\\sigma^{2}\\mathbf{I}_{T}$，并且感受野的先验是具有各向同性精度的高斯分布，具体表示为 $p(\\mathbf{k}) \\propto \\exp\\!\\big(-\\lambda \\|\\mathbf{k}\\|_{2}^{2}\\big)$，其中 $\\lambda > 0$ 是一个已知常数。以贝叶斯定理为基本原则，并假设为线性高斯观测模型，推导能够最大化后验分布 $p(\\mathbf{k}\\mid \\mathbf{r}, \\mathbf{X})$ 的最大后验（MAP）估计量 $\\hat{\\mathbf{k}}_{\\text{MAP}}$。请用 $\\mathbf{X}$、$\\mathbf{r}$、$\\sigma^{2}$ 和 $\\lambda$ 以闭合形式表示你的最终结果。然后，从估计量偏差和方差在线性估计量和高斯噪声下的定义出发，解释高斯先验所蕴含的 $\\ell_{2}$ 惩罚项如何实现岭正则化，即在 $\\lambda$ 变化时，通过权衡滤波器估计中的偏差与方差，且不使用任何简便公式。你的最终答案必须是单个闭合形式的解析表达式。不需要进行数值近似。",
            "solution": "最大后验（MAP）估计量 $\\hat{\\mathbf{k}}_{\\text{MAP}}$ 通过最大化后验分布 $p(\\mathbf{k} \\mid \\mathbf{r}, \\mathbf{X})$ 得到。根据贝叶斯定理，后验分布正比于似然与先验的乘积：\n$$\np(\\mathbf{k} \\mid \\mathbf{r}, \\mathbf{X}) \\propto p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) p(\\mathbf{k})\n$$\n最大化后验等价于最大化其对数。对数后验由下式给出：\n$$\n\\ln p(\\mathbf{k} \\mid \\mathbf{r}, \\mathbf{X}) = \\ln p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) + \\ln p(\\mathbf{k}) + C\n$$\n其中 $C$ 是一个不依赖于 $\\mathbf{k}$ 的常数。\n\n首先，我们写出似然项 $p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X})$。模型为 $\\mathbf{r} = \\mathbf{X}\\mathbf{k} + \\boldsymbol{\\varepsilon}$，其中噪声 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{T})$。这意味着响应 $\\mathbf{r}$ 服从均值为 $\\mathbf{X}\\mathbf{k}$，协方差为 $\\sigma^{2}\\mathbf{I}_{T}$ 的高斯分布。其对数似然为：\n$$\n\\ln p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) = -\\frac{T}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}} \\|\\mathbf{r} - \\mathbf{X}\\mathbf{k}\\|_{2}^{2}\n$$\n\n接下来，我们考虑感受野的先验分布 $p(\\mathbf{k}) \\propto \\exp(-\\lambda \\|\\mathbf{k}\\|_{2}^{2})$。其对数先验为：\n$$\n\\ln p(\\mathbf{k}) = -\\lambda \\|\\mathbf{k}\\|_{2}^{2} + C'\n$$\n其中 $C'$ 是一个不依赖于 $\\mathbf{k}$ 的常数。\n\n组合这些项，我们得到需要对 $\\mathbf{k}$ 最大化的对数后验目标函数 $\\mathcal{L}(\\mathbf{k})$：\n$$\n\\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} \\|\\mathbf{r} - \\mathbf{X}\\mathbf{k}\\|_{2}^{2} - \\lambda \\|\\mathbf{k}\\|_{2}^{2} + \\text{constants}\n$$\n为了找到最大值，我们计算 $\\mathcal{L}(\\mathbf{k})$ 相对于 $\\mathbf{k}$ 的梯度，并将其设为零。首先展开平方范数：\n$$\n\\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} (\\mathbf{r}^{T}\\mathbf{r} - 2\\mathbf{r}^{T}\\mathbf{X}\\mathbf{k} + \\mathbf{k}^{T}\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - \\lambda \\mathbf{k}^{T}\\mathbf{k} + \\text{constants}\n$$\n梯度为：\n$$\n\\nabla_{\\mathbf{k}} \\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} (-2\\mathbf{X}^{T}\\mathbf{r} + 2\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - 2\\lambda\\mathbf{k}\n$$\n将梯度设为零向量 $\\mathbf{0}$：\n$$\n\\frac{1}{\\sigma^{2}}(\\mathbf{X}^{T}\\mathbf{r} - \\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - 2\\lambda\\mathbf{k} = \\mathbf{0}\n$$\n整理各项以求解 $\\mathbf{k}$：\n$$\n\\mathbf{X}^{T}\\mathbf{r} - \\mathbf{X}^{T}\\mathbf{X}\\mathbf{k} = 2\\sigma^{2}\\lambda\\mathbf{k} \\\\\n\\mathbf{X}^{T}\\mathbf{r} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})\\mathbf{k}\n$$\n其中 $\\mathbf{I}_{d}$ 是 $d \\times d$ 的单位矩阵。由于 $\\mathbf{X}^{T}\\mathbf{X}$ 是半正定的，且 $\\lambda, \\sigma^2 > 0$，矩阵 $(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})$ 是正定的，因此可逆。通过左乘该矩阵的逆，我们得到MAP估计量：\n$$\n\\hat{\\mathbf{k}}_{\\text{MAP}} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{r}\n$$\n\n**偏差-方差权衡分析**\n设真实感受野为 $\\mathbf{k}_{\\text{true}}$，因此 $\\mathbf{r} = \\mathbf{X}\\mathbf{k}_{\\text{true}} + \\boldsymbol{\\varepsilon}$。\n估计量的**偏差**是其期望值与真实值之差：\n$$\n\\text{Bias}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] - \\mathbf{k}_{\\text{true}}\n$$\n由于 $\\mathbb{E}[\\mathbf{r}] = \\mathbf{X}\\mathbf{k}_{\\text{true}}$，我们有：\n$$\n\\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbb{E}[\\mathbf{r}] = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}_{\\text{true}}\n$$\n因此，偏差为：\n$$\n\\text{Bias}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = [(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X} - \\mathbf{I}_{d}]\\mathbf{k}_{\\text{true}}\n$$\n当 $\\lambda=0$ 时（对应最大似然估计），如果 $\\mathbf{X}^T\\mathbf{X}$ 可逆，则偏差为零。但对于任何 $\\lambda > 0$，该估计量是有偏的，因为它有效地将系数向零收缩。\n\n估计量的**方差**是其协方差矩阵。令 $\\mathbf{M} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}$。那么 $\\hat{\\mathbf{k}}_{\\text{MAP}} = \\mathbf{M}\\mathbf{r}$。方差为：\n$$\n\\text{Var}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\text{Var}(\\mathbf{M}\\mathbf{r}) = \\text{Var}(\\mathbf{M}(\\mathbf{X}\\mathbf{k}_{\\text{true}} + \\boldsymbol{\\varepsilon})) = \\text{Var}(\\mathbf{M}\\boldsymbol{\\varepsilon})\n$$\n$$\n\\text{Var}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\mathbf{M} \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{T}] \\mathbf{M}^{T} = \\mathbf{M}(\\sigma^{2}\\mathbf{I}_{T})\\mathbf{M}^{T} = \\sigma^{2}\\mathbf{M}\\mathbf{M}^{T}\n$$\n代入 $\\mathbf{M}$，我们得到：\n$$\n\\text{Var}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\sigma^{2}(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\n$$\n对于最大似然估计（$\\lambda=0$），方差为 $\\sigma^{2}(\\mathbf{X}^{T}\\mathbf{X})^{-1}$。如果 $\\mathbf{X}^T\\mathbf{X}$ 的特征值很小（即病态），其逆的特征值会很大，导致估计方差极高。通过加入正则化项 $2\\sigma^{2}\\lambda\\mathbf{I}_{d}$，我们增加了 $\\mathbf{X}^T\\mathbf{X}$ 的所有特征值，从而使矩阵求逆过程更稳定，并减小了估计的方差。\n\n总之，$\\ell_2$ 惩罚项（来自高斯先验）引入了偏差，但减少了方差。正则化参数 $\\lambda$ 控制了这种权衡：增加 $\\lambda$ 会增加偏差，但会减少方差，从而得到一个总均方误差可能更低的更鲁棒的估计。",
            "answer": "$$\n\\boxed{(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{r}}\n$$"
        }
    ]
}