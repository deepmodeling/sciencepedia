{
    "hands_on_practices": [
        {
            "introduction": "The theoretical storage capacity of a Hopfield network is fundamentally limited by the \"crosstalk\" noise that arises from the interference of other stored patterns during memory retrieval. This first practice is a foundational exercise where you will derive, from first principles, the exact variance of this crosstalk noise for a network of finite size $N$. By performing this derivation (), you will gain a deep understanding of how this interference scales with network size and memory load, and see how the classic infinite-size results are corrected in more realistic, finite systems.",
            "id": "4023731",
            "problem": "Consider a fully connected Hopfield network of size $N$ storing $P$ binary patterns $\\{\\boldsymbol{\\xi}^{\\mu}\\}_{\\mu=1}^{P}$ with independent and identically distributed components $\\xi_{i}^{\\mu} \\in \\{-1,+1\\}$ sampled uniformly and independently across both neuron index $i$ and pattern index $\\mu$. The synaptic matrix is constructed by the Hebbian prescription without self-couplings, that is,\n$$\nJ_{ij} \\equiv \\frac{1}{N}\\sum_{\\mu=1}^{P}\\xi_{i}^{\\mu}\\xi_{j}^{\\mu}, \\quad J_{ii}=0.\n$$\nAssume the network is initialized in the basin of attraction of a reference pattern $\\nu \\in \\{1,\\dots,P\\}$ and updated deterministically at zero temperature, so that the local field at neuron $i$ is\n$$\nh_{i}=\\sum_{j\\neq i}J_{ij}s_{j},\n$$\nand near perfect retrieval one can take $s_{j}=\\xi_{j}^{\\nu}$. Decompose $h_{i}$ into a signal term aligned with $\\xi_{i}^{\\nu}$ and a crosstalk term $\\eta_{i}$ arising from all non-condensed patterns $\\mu\\neq \\nu$. Under the Central Limit Theorem (CLT), the crosstalk $\\eta_{i}$ is approximated as Gaussian with mean zero and variance $\\sigma^{2}(N,P)$ that depends on $N$ and $P$.\n\nStarting from the definitions above and only using independence properties of the random patterns and the CLT as a qualitative justification for Gaussianity (not for computing moments), do the following:\n\n1) Derive the exact finite-$N$ expression for the crosstalk variance $\\sigma^{2}(N,P)$ at a site $i$ during retrieval of pattern $\\nu$, expressed in terms of $N$ and $P$, and then rewrite it in terms of the load $\\alpha\\equiv P/N$.\n\n2) In the infinite-size limit, the critical storage load is denoted $\\alpha_{c}^{\\infty}$ (a positive constant). A naive empirical procedure to estimate the critical load at finite $N$ is to set the measured crosstalk variance equal to the infinite-size critical variance, namely solve for $\\widehat{\\alpha}_{c}(N)$ in\n$$\n\\sigma^{2}\\bigl(N,\\widehat{\\alpha}_{c}(N)\\,N\\bigr)=\\alpha_{c}^{\\infty}.\n$$\nSolve this equation exactly for $\\widehat{\\alpha}_{c}(N)$ in closed form, simplifying the result as much as possible.\n\nYour final answer must be the single simplified symbolic expression for $\\widehat{\\alpha}_{c}(N)$ in terms of $N$ and $\\alpha_{c}^{\\infty}$. No numerical approximation is required.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the standard Hopfield model, well-posed, objective, and contains a complete and consistent set of definitions and constraints. The tasks are standard derivations in the field of computational neuroscience.\n\nThe solution is presented in two parts as requested by the problem statement.\n\nPart 1: Derivation of the crosstalk variance $\\sigma^{2}(N,P)$.\n\nThe local field $h_{i}$ at neuron $i$ is given by:\n$$\nh_{i}=\\sum_{j\\neq i}J_{ij}s_{j}\n$$\nThe synaptic weights $J_{ij}$ for $i \\neq j$ are defined by the Hebbian prescription:\n$$\nJ_{ij} = \\frac{1}{N}\\sum_{\\mu=1}^{P}\\xi_{i}^{\\mu}\\xi_{j}^{\\mu}\n$$\nWe assume the network state $s_j$ is perfectly aligned with a reference pattern $\\boldsymbol{\\xi}^{\\nu}$, so $s_{j}=\\xi_{j}^{\\nu}$. Substituting these into the expression for the local field gives:\n$$\nh_{i} = \\sum_{j\\neq i} \\left( \\frac{1}{N}\\sum_{\\mu=1}^{P}\\xi_{i}^{\\mu}\\xi_{j}^{\\mu} \\right) \\xi_{j}^{\\nu}\n$$\nBy rearranging the order of summation:\n$$\nh_{i} = \\frac{1}{N} \\sum_{\\mu=1}^{P} \\xi_{i}^{\\mu} \\sum_{j\\neq i} \\xi_{j}^{\\mu} \\xi_{j}^{\\nu}\n$$\nWe can decompose this sum into two parts: the term corresponding to the reference pattern $\\mu=\\nu$ and the terms for all other patterns $\\mu \\neq \\nu$.\n$$\nh_{i} = \\frac{1}{N} \\left( \\xi_{i}^{\\nu} \\sum_{j\\neq i} \\xi_{j}^{\\nu} \\xi_{j}^{\\nu} + \\sum_{\\mu \\neq \\nu} \\xi_{i}^{\\mu} \\sum_{j\\neq i} \\xi_{j}^{\\mu} \\xi_{j}^{\\nu} \\right)\n$$\nThe components of the patterns are $\\xi_{j}^{\\nu} \\in \\{-1, +1\\}$, so $(\\xi_{j}^{\\nu})^{2} = 1$. The first term is the signal term:\n$$\n\\text{Signal} = \\frac{1}{N} \\xi_{i}^{\\nu} \\sum_{j\\neq i} 1 = \\frac{1}{N} \\xi_{i}^{\\nu} (N-1) = \\left(1-\\frac{1}{N}\\right)\\xi_{i}^{\\nu}\n$$\nThis term is proportional to the desired state $\\xi_{i}^{\\nu}$. The second term is the crosstalk noise, $\\eta_{i}$:\n$$\n\\eta_{i} = \\frac{1}{N} \\sum_{\\mu \\neq \\nu} \\xi_{i}^{\\mu} \\left( \\sum_{j\\neq i} \\xi_{j}^{\\mu}\\xi_{j}^{\\nu} \\right)\n$$\nWe proceed to calculate the variance of $\\eta_{i}$, denoted $\\sigma^{2}$. The variance is $\\sigma^{2} = E[\\eta_{i}^{2}] - (E[\\eta_{i}])^{2}$. First, we compute the mean $E[\\eta_{i}]$. The expectation is taken over the distribution of the random patterns. The components $\\xi_{k}^{\\mu}$ are independent for all $k, \\mu$ and have zero mean, $E[\\xi_{k}^{\\mu}]=0$.\n$$\nE[\\eta_i] = \\frac{1}{N} \\sum_{\\mu \\neq \\nu} E\\left[\\xi_{i}^{\\mu} \\left( \\sum_{j\\neq i} \\xi_{j}^{\\mu}\\xi_{j}^{\\nu} \\right)\\right] = \\frac{1}{N} \\sum_{\\mu \\neq \\nu} E[\\xi_{i}^{\\mu}] E\\left[\\sum_{j\\neq i} \\xi_{j}^{\\mu}\\xi_{j}^{\\nu}\\right] = 0\n$$\nSince the mean is zero, the variance is $\\sigma^{2} = E[\\eta_{i}^{2}]$.\n$$\n\\sigma^{2} = E\\left[ \\left( \\frac{1}{N} \\sum_{\\mu \\neq \\nu} \\xi_{i}^{\\mu} \\sum_{j\\neq i} \\xi_{j}^{\\mu}\\xi_{j}^{\\nu} \\right)^{2} \\right] = \\frac{1}{N^{2}} E\\left[ \\left( \\sum_{\\mu \\neq \\nu} \\xi_{i}^{\\mu} \\sum_{j\\neq i} \\xi_{j}^{\\mu}\\xi_{j}^{\\nu} \\right) \\left( \\sum_{\\lambda \\neq \\nu} \\xi_{i}^{\\lambda} \\sum_{k\\neq i} \\xi_{k}^{\\lambda}\\xi_{k}^{\\nu} \\right) \\right]\n$$\n$$\n\\sigma^{2} = \\frac{1}{N^{2}} \\sum_{\\mu \\neq \\nu} \\sum_{\\lambda \\neq \\nu} E\\left[ \\xi_{i}^{\\mu}\\xi_{i}^{\\lambda} \\left(\\sum_{j\\neq i} \\xi_{j}^{\\mu}\\xi_{j}^{\\nu}\\right) \\left(\\sum_{k\\neq i} \\xi_{k}^{\\lambda}\\xi_{k}^{\\nu}\\right) \\right]\n$$\nDue to the independence of patterns, if $\\mu \\neq \\lambda$, the expectation of any product of variables from pattern $\\mu$ and pattern $\\lambda$ factorizes. For $\\mu \\neq \\lambda$, $E[\\xi_{i}^{\\mu}\\xi_{i}^{\\lambda}] = E[\\xi_{i}^{\\mu}]E[\\xi_{i}^{\\lambda}] = 0 \\times 0 = 0$. Thus, all cross-terms where $\\mu \\neq \\lambda$ vanish. We only need to consider terms where $\\mu = \\lambda$:\n$$\n\\sigma^{2} = \\frac{1}{N^{2}} \\sum_{\\mu \\neq \\nu} E\\left[ (\\xi_{i}^{\\mu})^{2} \\left(\\sum_{j\\neq i} \\xi_{j}^{\\mu}\\xi_{j}^{\\nu}\\right)^{2} \\right]\n$$\nSince $(\\xi_{i}^{\\mu})^{2}=1$, this simplifies to:\n$$\n\\sigma^{2} = \\frac{1}{N^{2}} \\sum_{\\mu \\neq \\nu} E\\left[ \\left(\\sum_{j\\neq i} \\xi_{j}^{\\mu}\\xi_{j}^{\\nu}\\right)^{2} \\right]\n$$\nLet's analyze the expectation. For a fixed $\\mu \\neq \\nu$, let $X_{j} = \\xi_{j}^{\\mu}\\xi_{j}^{\\nu}$. Since $\\xi_{j}^{\\mu}$ and $\\xi_{j}^{\\nu}$ are independent for different $j$, the variables $X_{j}$ are independent and identically distributed. Their mean is $E[X_{j}] = E[\\xi_{j}^{\\mu}]E[\\xi_{j}^{\\nu}]=0$. Their second moment is $E[X_{j}^{2}] = E[(\\xi_{j}^{\\mu})^{2}(\\xi_{j}^{\\nu})^{2}] = E[1 \\cdot 1] = 1$.\nThe expectation term is the variance of the sum of these variables:\n$$\nE\\left[ \\left(\\sum_{j\\neq i} X_{j}\\right)^{2} \\right] = E\\left[ \\sum_{j\\neq i} \\sum_{k\\neq i} X_{j}X_{k} \\right] = \\sum_{j\\neq i}\\sum_{k\\neq i} E[X_{j}X_{k}]\n$$\nSince $E[X_j]=0$ and $X_j, X_k$ are independent for $j \\neq k$, $E[X_{j}X_{k}] = E[X_{j}]E[X_{k}] = 0$ for $j \\neq k$. The sum is non-zero only for $j=k$:\n$$\nE\\left[ \\left(\\sum_{j\\neq i} X_{j}\\right)^{2} \\right] = \\sum_{j\\neq i} E[X_{j}^{2}] = \\sum_{j\\neq i} 1 = N-1\n$$\nThis value is the same for each of the patterns $\\mu \\neq \\nu$. The sum $\\sum_{\\mu \\neq \\nu}$ has $P-1$ terms. Substituting this back into the expression for $\\sigma^{2}$:\n$$\n\\sigma^{2} = \\frac{1}{N^{2}} \\sum_{\\mu \\neq \\nu} (N-1) = \\frac{1}{N^{2}} (P-1)(N-1)\n$$\nThis is the exact finite-$N$ expression for the crosstalk variance $\\sigma^{2}(N,P)$.\n\nTo express it in terms of the load $\\alpha \\equiv P/N$, we substitute $P = \\alpha N$:\n$$\n\\sigma^{2}(N,\\alpha) = \\frac{(\\alpha N - 1)(N-1)}{N^{2}}\n$$\n\nPart 2: Solving for the finite-size critical load $\\widehat{\\alpha}_{c}(N)$.\n\nWe are asked to solve the equation $\\sigma^{2}\\bigl(N,\\widehat{\\alpha}_{c}(N)\\,N\\bigr)=\\alpha_{c}^{\\infty}$ for $\\widehat{\\alpha}_{c}(N)$. Let $\\hat{\\alpha}_{c} \\equiv \\widehat{\\alpha}_{c}(N)$ for notational simplicity during the derivation. We substitute our derived expression for the variance:\n$$\n\\frac{(\\hat{\\alpha}_{c} N - 1)(N-1)}{N^{2}} = \\alpha_{c}^{\\infty}\n$$\nWe solve this algebraic equation for $\\hat{\\alpha}_{c}$:\n$$\n(\\hat{\\alpha}_{c} N - 1)(N-1) = N^{2} \\alpha_{c}^{\\infty}\n$$\n$$\n\\hat{\\alpha}_{c} N - 1 = \\frac{N^{2} \\alpha_{c}^{\\infty}}{N-1}\n$$\n$$\n\\hat{\\alpha}_{c} N = 1 + \\frac{N^{2} \\alpha_{c}^{\\infty}}{N-1}\n$$\n$$\n\\hat{\\alpha}_{c} = \\frac{1}{N} \\left( 1 + \\frac{N^{2} \\alpha_{c}^{\\infty}}{N-1} \\right)\n$$\nSimplifying this expression gives the final result for $\\widehat{\\alpha}_{c}(N)$:\n$$\n\\widehat{\\alpha}_{c}(N) = \\frac{1}{N} + \\frac{N \\alpha_{c}^{\\infty}}{N-1}\n$$\nThis expression can be expanded to show the leading term and finite-size corrections:\n$$\n\\widehat{\\alpha}_{c}(N) = \\frac{1}{N} + \\alpha_{c}^{\\infty} \\frac{N-1+1}{N-1} = \\frac{1}{N} + \\alpha_{c}^{\\infty} \\left(1 + \\frac{1}{N-1}\\right) = \\alpha_{c}^{\\infty} + \\frac{1}{N} + \\frac{\\alpha_{c}^{\\infty}}{N-1}\n$$\nThe form $\\frac{1}{N} + \\frac{N \\alpha_{c}^{\\infty}}{N-1}$ is already sufficiently simplified as requested.",
            "answer": "$$\n\\boxed{\\frac{1}{N} + \\frac{N \\alpha_{c}^{\\infty}}{N-1}}\n$$"
        },
        {
            "introduction": "The dynamics of a Hopfield network can be visualized as a descent on an energy landscape, where stable memories correspond to energy minima or \"attractors.\" However, this landscape also contains spurious attractorsâ€”unwanted stable states that are mixtures of stored patterns. This practice () challenges you to analyze the competition between a desired memory and a spurious mixture by comparing their respective energies. You will calculate the precise strength of an external field required to make the spurious state more energetically favorable, providing a quantitative grasp of how basins of attraction can be manipulated.",
            "id": "4023654",
            "problem": "Consider a fully connected Hopfield network of $N$ binary neurons $s_{i} \\in \\{-1, +1\\}$ storing $P$ independent, identically distributed random patterns $\\{\\xi_{i}^{\\mu}\\}_{\\mu=1}^{P}$ with $\\xi_{i}^{\\mu} \\in \\{-1, +1\\}$ and $\\mathbb{P}(\\xi_{i}^{\\mu} = +1) = \\mathbb{P}(\\xi_{i}^{\\mu} = -1) = \\tfrac{1}{2}$. Synaptic couplings are constructed by the Hebbian rule\n$$\nJ_{ij} \\;=\\; \\frac{1}{N} \\sum_{\\mu=1}^{P} \\xi_{i}^{\\mu} \\xi_{j}^{\\mu}, \\quad J_{ii} \\;=\\; 0,\n$$\nand the zero-temperature dynamics minimizes the standard Hopfield energy with an additive external field,\n$$\nE(\\boldsymbol{s}; \\boldsymbol{h}^{\\text{ext}}) \\;=\\; -\\frac{1}{2} \\sum_{i,j=1}^{N} J_{ij} s_{i} s_{j} \\;-\\; \\sum_{i=1}^{N} h_{i}^{\\text{ext}} s_{i}.\n$$\nAssume the thermodynamic limit $N \\to \\infty$ with fixed load $\\alpha = P/N$, and let the external field be aligned with pattern $\\mu = 3$, i.e.,\n$$\nh_{i}^{\\text{ext}} \\;=\\; h \\, \\xi_{i}^{3},\n$$\nwith a constant amplitude $h \\ge 0$.\n\nFocus on two candidate attractors:\n- the pure pattern state $s_{i} = \\xi_{i}^{1}$,\n- the odd spurious mixture state $s_{i} = \\operatorname{sign}\\!\\big(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3}\\big)$, which is well-defined because $\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3} \\in \\{-3, -1, +1, +3\\}$ is never zero.\n\nUsing foundational definitions above, the Law of Large Numbers and Central Limit Theorem (CLT) to evaluate pattern overlaps and the crosstalk contribution from non-condensed patterns in the large-$N$ limit, derive the energy per spin of each of these two states and determine the threshold external bias amplitude $h_{c}$ at which, as $h$ increases from zero, the spurious mixture state first becomes energetically favored over the pure state. Express your final answer as an exact fraction. No rounding is required. The answer is dimensionless.",
            "solution": "We start from the Hopfield energy with additive field\n$$\nE(\\boldsymbol{s}; \\boldsymbol{h}^{\\text{ext}}) \\;=\\; -\\frac{1}{2} \\sum_{i,j=1}^{N} J_{ij} s_{i} s_{j} \\;-\\; \\sum_{i=1}^{N} h_{i}^{\\text{ext}} s_{i}.\n$$\nWith the Hebbian $J_{ij}$ and $J_{ii}=0$, the coupling term can be rewritten in terms of pattern overlaps. Define the overlaps\n$$\nM_{\\mu} \\;=\\; \\sum_{i=1}^{N} \\xi_{i}^{\\mu} s_{i}, \\qquad m_{\\mu} \\;=\\; \\frac{M_{\\mu}}{N}.\n$$\nUsing $J_{ij} = \\frac{1}{N} \\sum_{\\mu} \\xi_{i}^{\\mu} \\xi_{j}^{\\mu}$, and separating diagonal terms via $J_{ii}=0$, we have the identity\n$$\n\\sum_{i,j=1}^{N} J_{ij} s_{i} s_{j}\n= \\frac{1}{N} \\sum_{\\mu=1}^{P} \\left( \\sum_{i=1}^{N} \\xi_{i}^{\\mu} s_{i} \\right)^{\\!2} - P\n= N \\sum_{\\mu=1}^{P} m_{\\mu}^{2} - P.\n$$\nTherefore, the energy per spin, $e = E/N$, is\n$$\ne(\\boldsymbol{s}; h) \\;=\\; -\\frac{1}{2} \\sum_{\\mu=1}^{P} m_{\\mu}^{2} \\;+\\; \\frac{1}{2}\\alpha \\;-\\; \\frac{1}{N} \\sum_{i=1}^{N} h_{i}^{\\text{ext}} s_{i}.\n$$\nThe last term evaluates to $-\\frac{1}{N} \\sum_{i} h \\xi_{i}^{3} s_{i} = - h m_{3}$.\n\nWe will compute $e$ for the two candidate states.\n\nPure pattern state $s_{i} = \\xi_{i}^{1}$. In the thermodynamic limit, the overlaps are\n$$\nm_{1} \\;=\\; \\frac{1}{N} \\sum_{i} \\xi_{i}^{1} \\xi_{i}^{1} \\;=\\; 1, \\qquad\nm_{\\mu} \\;=\\; \\frac{1}{N} \\sum_{i} \\xi_{i}^{\\mu} \\xi_{i}^{1} \\;\\to\\; 0 \\;\\text{for}\\; \\mu \\neq 1,\n$$\nby independence and the Law of Large Numbers. Thus,\n$$\n\\sum_{\\mu=1}^{P} m_{\\mu}^{2} \\;\\to\\; 1, \\qquad m_{3} \\;\\to\\; 0.\n$$\nHence the energy per spin is\n$$\ne_{\\text{pure}}(h) \\;=\\; -\\frac{1}{2} \\cdot 1 \\;+\\; \\frac{1}{2} \\alpha \\;-\\; h \\cdot 0\n\\;=\\; -\\frac{1}{2} + \\frac{1}{2} \\alpha.\n$$\n\nSpurious mixture state $s_{i} = \\operatorname{sign}\\!\\big(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3}\\big)$. By symmetry, the overlaps with the three participating patterns are equal, $m_{1} = m_{2} = m_{3}$. We compute $m_{1}$ exactly by enumerating the $2^{3}$ possibilities of $\\big(\\xi^{1},\\xi^{2},\\xi^{3}\\big) \\in \\{-1,+1\\}^{3}$. Define $x_{1} = \\xi^{1}$, $x_{2} = \\xi^{2}$, $x_{3} = \\xi^{3}$, uniformly independent. Then\n$$\nm_{1} \\;=\\; \\mathbb{E}\\left[ \\operatorname{sign}(x_{1}+x_{2}+x_{3}) \\, x_{1} \\right].\n$$\nThe sum $x_{1}+x_{2}+x_{3}$ takes values in $\\{-3,-1,+1,+3\\}$. Enumerating all eight cases:\n\n$$\n\\begin{array}{c|c|c}\n(x_{1},x_{2},x_{3})  \\operatorname{sign}(x_{1}+x_{2}+x_{3})  \\operatorname{sign}(x_{1}+x_{2}+x_{3}) \\, x_{1} \\\\\n\\hline\n(+,+,+)  +  + \\\\\n(+,+,-)  +  + \\\\\n(+,-,+)  +  + \\\\\n(+,-,-)  -  - \\\\\n(-,+,+)  +  - \\\\\n(-,+,-)  -  + \\\\\n(-,-,+)  -  + \\\\\n(-,-,-)  -  + \\\\\n\\end{array}\n$$\n\nThe product is $+1$ in six cases and $-1$ in two cases, so\n$$\nm_{1} \\;=\\; \\frac{6 - 2}{8} \\;=\\; \\frac{1}{2}.\n$$\nBy symmetry, $m_{2} = m_{3} = \\tfrac{1}{2}$ as well, and $m_{\\mu} \\to 0$ for $\\mu \\ge 4$ (non-condensed patterns). Therefore\n$$\n\\sum_{\\mu=1}^{P} m_{\\mu}^{2} \\;=\\; 3 \\left(\\frac{1}{2}\\right)^{\\!2} \\;=\\; \\frac{3}{4}, \\qquad m_{3} \\;=\\; \\frac{1}{2}.\n$$\nThe energy per spin is\n$$\ne_{\\text{mix3}}(h) \\;=\\; -\\frac{1}{2} \\cdot \\frac{3}{4} \\;+\\; \\frac{1}{2} \\alpha \\;-\\; h \\cdot \\frac{1}{2}\n\\;=\\; -\\frac{3}{8} + \\frac{1}{2} \\alpha - \\frac{h}{2}.\n$$\n\nThe spurious mixture becomes energetically favored over the pure state when $e_{\\text{mix3}}(h) \\le e_{\\text{pure}}(h)$. The threshold $h_{c}$ is defined by equality:\n$$\n-\\frac{3}{8} + \\frac{1}{2} \\alpha - \\frac{h_{c}}{2} \\;=\\; -\\frac{1}{2} + \\frac{1}{2} \\alpha.\n$$\nThe load term $\\frac{1}{2}\\alpha$ cancels on both sides, reflecting that non-condensed pattern crosstalk contributes equally to both states in this mean-field limit. Solving for $h_{c}$ gives\n$$\n-\\frac{3}{8} - \\frac{h_{c}}{2} \\;=\\; -\\frac{1}{2}\n\\;\\;\\Longrightarrow\\;\\;\n\\frac{1}{8} \\;=\\; \\frac{h_{c}}{2}\n\\;\\;\\Longrightarrow\\;\\;\nh_{c} \\;=\\; \\frac{1}{4}.\n$$\n\nThus, in the thermodynamic limit with zero temperature and an external field aligned to one constituent of the spurious mixture, the basin flips from the pure pattern to the three-pattern spurious mixture precisely when the external bias amplitude reaches $h_{c} = \\tfrac{1}{4}$.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "While analytical models often assume idealized, continuous-valued synaptic weights, biological and hardware implementations frequently involve constraints like weight quantization. This hands-on computational exercise () invites you to bridge theory and practice by simulating a network with \"clipped\" ternary synapses. By writing code to measure stability metrics and energy, you will directly observe how this non-ideality impacts the stability of both pure memories and spurious attractors in a finite-sized network, offering valuable insights into the robustness of associative memory models.",
            "id": "4023655",
            "problem": "Consider a fully connected binary associative memory network with $N$ neurons, modeled in the style of a Hopfield network. Each neuron state is $s_i \\in \\{-1,+1\\}$ for $i \\in \\{1,\\dots,N\\}$. A set of $P$ independent and identically distributed random patterns $\\{\\boldsymbol{\\xi}^\\mu\\}_{\\mu=1}^P$ is stored, where $\\xi_i^\\mu \\in \\{-1,+1\\}$ are independent and equiprobable. Synaptic couplings are constructed by the standard Hebbian rule, then quantized (clipped) to ternary values. Your task is to analyze the existence and stability of spurious mixture states under ternary clipping.\n\nFundamental base and definitions:\n\n- Hebbian couplings:\n$$\nJ_{ij} \\;=\\; \\begin{cases}\n\\dfrac{1}{N}\\sum_{\\mu=1}^{P} \\xi_i^\\mu \\,\\xi_j^\\mu,  i \\neq j,\\\\\n0,  i=j,\n\\end{cases}\n$$\nso $\\boldsymbol{J}$ is symmetric with zero diagonal.\n\n- Ternary clipping to the synaptic matrix $\\boldsymbol{W}$:\n$$\nw_{ij} \\;=\\; \\begin{cases}\nw_0 \\,\\mathrm{sign}(J_{ij}),  \\text{if } |J_{ij}| \\ge \\lambda \\text{ and } i \\neq j,\\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\nwith $w_{ii}=0$. Here $w_0  0$ is a fixed synaptic magnitude and $\\lambda \\ge 0$ is a clipping threshold.\n\n- Zero-temperature deterministic dynamics is defined by the local field:\n$$\nh_i(\\boldsymbol{s}) \\;=\\; \\sum_{j\\neq i} w_{ij} \\, s_j,\n$$\nand the stability condition for a fixed point $\\boldsymbol{s}$ is that every neuron is strictly aligned with its local field:\n$$\ns_i \\, h_i(\\boldsymbol{s}) \\;\\; 0 \\quad \\text{for all } i \\in \\{1,\\dots,N\\}.\n$$\nDefine the stability margin of $\\boldsymbol{s}$ as\n$$\n\\gamma(\\boldsymbol{s}) \\;=\\; \\min_{1\\le i \\le N} \\left[ s_i \\, h_i(\\boldsymbol{s}) \\right].\n$$\n\n- The Lyapunov energy function under symmetric $\\boldsymbol{W}$ is\n$$\nE(\\boldsymbol{s}) \\;=\\; -\\dfrac{1}{2} \\sum_{i\\neq j} w_{ij} \\, s_i \\, s_j,\n$$\nand the energy density is $e(\\boldsymbol{s}) \\;=\\; E(\\boldsymbol{s})/N^2$.\n\n- Define the $k$-pattern spurious mixture state using equal weights as\n$$\ns_i^{(\\mathrm{mix},k)} \\;=\\; \\mathrm{sign}\\!\\left( \\sum_{\\mu=1}^k \\xi_i^\\mu \\right),\n$$\nwith the tie-breaking convention $\\mathrm{sign}(0) = +1$. The pure retrieval state for pattern $\\mu=1$ is\n$$\ns_i^{(\\mathrm{pure})} \\;=\\; \\xi_i^1.\n$$\n\nYour program must, for each specified test case, do the following:\n\n1. Generate $P$ independent patterns $\\{\\boldsymbol{\\xi}^\\mu\\}_{\\mu=1}^P$ with entries in $\\{-1,+1\\}$, using the provided random seed for reproducibility.\n2. Construct $\\boldsymbol{J}$ by the Hebb rule and then clip to ternary $\\boldsymbol{W}$ using $w_0$ and $\\lambda$ as defined above.\n3. Construct the $k$-mixture spurious state $\\boldsymbol{s}^{(\\mathrm{mix},k)}$ from the first $k$ stored patterns.\n4. Compute, for both $\\boldsymbol{s}^{(\\mathrm{mix},k)}$ and $\\boldsymbol{s}^{(\\mathrm{pure})}$:\n   - The fraction of violated stability inequalities,\n     $$\n     \\phi(\\boldsymbol{s}) \\;=\\; \\dfrac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\left\\{ s_i \\, h_i(\\boldsymbol{s}) \\le 0 \\right\\}.\n     $$\n   - The minimum stability margin $\\gamma(\\boldsymbol{s})$.\n   - The energy density $e(\\boldsymbol{s})$.\n   - A boolean indicator of strict stability, defined as $\\mathrm{Stable}(\\boldsymbol{s}) = \\mathbf{1}\\{\\gamma(\\boldsymbol{s})0\\}$.\n\nOutput specification:\n\n- For each test case, your program must output a list of eight entries in the following order:\n  1. $\\mathrm{Stable}(\\boldsymbol{s}^{(\\mathrm{mix},k)})$ (boolean),\n  2. $\\phi(\\boldsymbol{s}^{(\\mathrm{mix},k)})$ (float),\n  3. $\\gamma(\\boldsymbol{s}^{(\\mathrm{mix},k)})$ (float),\n  4. $e(\\boldsymbol{s}^{(\\mathrm{mix},k)})$ (float),\n  5. $\\mathrm{Stable}(\\boldsymbol{s}^{(\\mathrm{pure})})$ (boolean),\n  6. $\\phi(\\boldsymbol{s}^{(\\mathrm{pure})})$ (float),\n  7. $\\gamma(\\boldsymbol{s}^{(\\mathrm{pure})})$ (float),\n  8. $e(\\boldsymbol{s}^{(\\mathrm{pure})})$ (float).\n- All floating-point outputs must be rounded to six decimal places.\n- Aggregate the results for all test cases into a single line printed as a comma-separated list of these per-test-case lists enclosed in square brackets, e.g., $[\\,[\\dots],\\,[\\dots],\\dots\\,]$.\n- There are no physical units involved in this problem. Angles are not used.\n\nTest suite (each tuple is $(N,P,k,w_0,\\lambda,\\mathrm{seed})$):\n\n- Case $1$: $(200,\\,20,\\,3,\\,1.0,\\,0.0,\\,1)$\n- Case $2$: $(200,\\,20,\\,3,\\,1.0,\\,0.01,\\,1)$\n- Case $3$: $(200,\\,40,\\,3,\\,1.0,\\,0.0,\\,2)$\n- Case $4$: $(200,\\,5,\\,3,\\,1.0,\\,0.0,\\,3)$\n- Case $5$: $(200,\\,20,\\,5,\\,1.0,\\,0.0,\\,4)$\n- Case $6$: $(200,\\,20,\\,2,\\,1.0,\\,0.0,\\,5)$\n- Case $7$: $(200,\\,20,\\,3,\\,1.0,\\,0.05,\\,6)$\n\nScientific requirements:\n\n- Base your reasoning on the definitions above, the law of large numbers, and the Central Limit Theorem (CLT) for approximations if needed, but your implementation must compute the exact quantities for the finite $N$ specified.\n- Ensure that your implementation respects the strict stability criterion $s_i h_i(\\boldsymbol{s})  0$ for all $i$ and the tie-breaking convention $\\mathrm{sign}(0)=+1$ when constructing the mixture.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order of the test cases listed above.",
            "solution": "The problem is well-posed and scientifically grounded, situated within the standard framework of computational neuroscience and the statistical mechanics of neural networks. It requires the numerical simulation of a Hopfield-type associative memory with a specific modification to its synaptic structure, namely ternary clipping. The task is to evaluate the stability of both a memorized pattern and a spurious mixture state.\n\nThe solution proceeds by first generating the necessary components of the network model for each test case, and then evaluating the specified quantities.\n\n1.  **Generation of Patterns and States:**\n    For each test case, defined by the parameters $(N, P, k, w_0, \\lambda, \\mathrm{seed})$, the simulation begins by initializing a pseudo-random number generator with the given seed for reproducibility.\n    A set of $P$ memory patterns, $\\{\\boldsymbol{\\xi}^\\mu\\}_{\\mu=1}^P$, is generated. Each pattern $\\boldsymbol{\\xi}^\\mu$ is a vector of size $N$, whose components $\\xi_i^\\mu$ are independent and identically distributed random variables taking values in $\\{-1, +1\\}$ with equal probability, i.e., $P(\\xi_i^\\mu = +1) = P(\\xi_i^\\mu = -1) = 1/2$.\n    From these patterns, two specific network states are constructed for analysis:\n    -   The pure retrieval state, $\\boldsymbol{s}^{(\\mathrm{pure})}$, is set to be the first stored pattern: $\\boldsymbol{s}^{(\\mathrm{pure})} = \\boldsymbol{\\xi}^1$.\n    -   The $k$-pattern spurious mixture state, $\\boldsymbol{s}^{(\\mathrm{mix},k)}$, is constructed from the first $k$ patterns. Its components are determined by the rule $s_i^{(\\mathrm{mix},k)} = \\mathrm{sign}(\\sum_{\\mu=1}^k \\xi_i^\\mu)$. The summation $\\sum_{\\mu=1}^k \\xi_i^\\mu$ results in an integer. If this sum is zero (which can only occur for even $k$), the tie-breaking convention $\\mathrm{sign}(0) = +1$ is applied.\n\n2.  **Synaptic Matrix Construction:**\n    The synaptic weights are determined in a two-step process. First, an intermediate Hebbian coupling matrix $\\boldsymbol{J}$ is computed. Following the Hebbian learning principle, the strength of the connection between neuron $i$ and neuron $j$ is proportional to the correlation of their activities across the stored patterns. The elements $J_{ij}$ are given by:\n    $$\n    J_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^{P} \\xi_i^\\mu \\xi_j^\\mu\n    $$\n    for $i \\neq j$, and $J_{ii} = 0$ to prevent self-interaction.\n    Second, this matrix $\\boldsymbol{J}$ is transformed into a ternary weight matrix $\\boldsymbol{W}$ through a clipping procedure. This is a model of synaptic simplification or quantization. For a given clipping threshold $\\lambda \\ge 0$ and synaptic magnitude $w_0  0$, the elements $w_{ij}$ are defined as:\n    $$\n    w_{ij} = \\begin{cases}\n    w_0 \\, \\mathrm{sign}(J_{ij}),  \\text{if } |J_{ij}| \\ge \\lambda \\text{ and } i \\neq j,\\\\\n    0,  \\text{otherwise}.\n    \\end{cases}\n    $$\n    The diagonal elements remain zero, $w_{ii}=0$. This clipping sets weak synapses to zero and assigns a uniform magnitude $\\pm w_0$ to all remaining synapses, preserving only the sign of the original Hebbian coupling.\n\n3.  **Stability and Energy Analysis:**\n    For a given network state $\\boldsymbol{s} \\in \\{-1, +1\\}^N$, its stability and energy are evaluated. The analysis is performed for both $\\boldsymbol{s}^{(\\mathrm{pure})}$ and $\\boldsymbol{s}^{(\\mathrm{mix},k)}$.\n    -   **Local Field and Stability:** The local field, or input, to neuron $i$ is the weighted sum of the states of all other neurons: $h_i(\\boldsymbol{s}) = \\sum_{j \\neq i} w_{ij} s_j$. This can be efficiently computed as a matrix-vector product, $\\boldsymbol{h}(\\boldsymbol{s}) = \\boldsymbol{W}\\boldsymbol{s}$.\n        A state $\\boldsymbol{s}$ is a stable fixed point of the zero-temperature dynamics if each neuron's state is aligned with its local field. The strict stability condition is $s_i h_i(\\boldsymbol{s})  0$ for all $i=1,\\dots,N$.\n        From this, we compute:\n        a. The minimum stability margin, $\\gamma(\\boldsymbol{s}) = \\min_{i} [s_i h_i(\\boldsymbol{s})]$.\n        b. The boolean indicator of stability, $\\mathrm{Stable}(\\boldsymbol{s}) = \\mathbf{1}\\{\\gamma(\\boldsymbol{s})  0\\}$, which is true if and only if all neurons satisfy the strict stability condition.\n        c. The fraction of neurons violating the stability condition, $\\phi(\\boldsymbol{s}) = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{s_i h_i(\\boldsymbol{s}) \\le 0\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Note that a neuron with $s_i h_i(\\boldsymbol{s}) = 0$ is considered unstable.\n    -   **Energy Function:** The network's Lyapunov energy function for a symmetric weight matrix $\\boldsymbol{W}$ is $E(\\boldsymbol{s}) = -\\frac{1}{2} \\sum_{i \\neq j} w_{ij} s_i s_j$. This can be computed more efficiently using the local fields: $E(\\boldsymbol{s}) = -\\frac{1}{2} \\sum_{i=1}^N s_i h_i(\\boldsymbol{s})$.\n        The energy density is then calculated as $e(\\boldsymbol{s}) = E(\\boldsymbol{s})/N^2$.\n\nThe implementation will loop through each test case, execute these steps to compute the eight required numerical values (four for the mixture state, four for the pure state), round the floating-point results to six decimal places, and format them into the specified list-of-lists structure for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, P, k, w_0, lambda, seed)\n        (200, 20, 3, 1.0, 0.0, 1),\n        (200, 20, 3, 1.0, 0.01, 1),\n        (200, 40, 3, 1.0, 0.0, 2),\n        (200, 5, 3, 1.0, 0.0, 3),\n        (200, 20, 5, 1.0, 0.0, 4),\n        (200, 20, 2, 1.0, 0.0, 5),\n        (200, 20, 3, 1.0, 0.05, 6),\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N, P, k, w0, lambda_val, seed = case\n        \n        # 1. Generate patterns and states\n        rng = np.random.default_rng(seed)\n        patterns = rng.choice([-1, 1], size=(P, N))\n        \n        # Pure retrieval state\n        s_pure = patterns[0, :].astype(np.float64)\n        \n        # K-mixture spurious state\n        mix_sum = np.sum(patterns[:k, :], axis=0)\n        s_mix = np.sign(mix_sum).astype(np.float64)\n        s_mix[s_mix == 0] = 1.0 # Tie-breaking rule sign(0) = +1\n        \n        # 2. Construct synaptic matrix\n        # Hebbian couplings J\n        J = (1.0 / N) * (patterns.T @ patterns)\n        np.fill_diagonal(J, 0)\n        \n        # Ternary clipped couplings W\n        W = np.zeros_like(J)\n        mask = np.abs(J) >= lambda_val\n        W[mask] = w0 * np.sign(J[mask])\n        np.fill_diagonal(W, 0) # Ensure no self-connections\n\n        # 3. Analyze states\n        mix_analysis = analyze_state(s_mix, W, N)\n        pure_analysis = analyze_state(s_pure, W, N)\n        \n        # 4. Collate results for the current case\n        case_results = list(mix_analysis) + list(pure_analysis)\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # We manually construct the string to match the required format: [[...],[...],...]\n    # without any spaces and with floating point numbers formatted to 6 decimal places.\n    outer_parts = []\n    for inner_list in all_results:\n        inner_parts = []\n        for item in inner_list:\n            if isinstance(item, (bool, np.bool_)):\n                inner_parts.append(str(item))\n            elif isinstance(item, (float, np.floating)):\n                inner_parts.append(f\"{item:.6f}\")\n            else:\n                inner_parts.append(str(item))\n        outer_parts.append(f\"[{','.join(inner_parts)}]\")\n    \n    final_string = f\"[{','.join(outer_parts)}]\"\n    print(final_string)\n\ndef analyze_state(s, W, N):\n    \"\"\"\n    Computes stability metrics and energy for a given state s.\n    \n    Args:\n        s (np.ndarray): The state vector of size N.\n        W (np.ndarray): The synaptic weight matrix of size (N, N).\n        N (int): The number of neurons.\n    \n    Returns:\n        tuple: A tuple containing (Stable, phi, gamma, energy_density).\n    \"\"\"\n    # Local fields h_i(s)\n    h = W @ s\n    \n    # Alignments s_i * h_i(s)\n    alignments = s * h\n    \n    # Minimum stability margin gamma(s)\n    gamma = np.min(alignments)\n    \n    # Boolean indicator of strict stability\n    is_stable = gamma > 0\n    \n    # Fraction of violated stability inequalities phi(s)\n    violated_count = np.sum(alignments = 0)\n    phi = violated_count / N\n    \n    # Energy E(s) and energy density e(s)\n    energy = -0.5 * np.sum(alignments)\n    energy_density = energy / (N**2)\n\n    return is_stable, round(phi, 6), round(gamma, 6), round(energy_density, 6)\n\nsolve()\n\n```"
        }
    ]
}