{
    "hands_on_practices": [
        {
            "introduction": "This practice focuses on the core mechanics of a Gated Recurrent Unit (GRU), a sophisticated RNN variant widely used in modeling. By performing a step-by-step calculation, you will gain a concrete understanding of how gating mechanisms dynamically control the flow of information and update the network's memory. This exercise highlights the crucial role of the update gate in creating a convex combination of the previous state and a new candidate state, a property fundamental to the GRU's ability to learn long-range dependencies .",
            "id": "4013919",
            "problem": "A local circuit model for cortical sequence generation is implemented as a three-dimensional Gated Recurrent Unit (GRU). At time step $t$, the previous hidden state is $\\boldsymbol{h}_{t-1} \\in \\mathbb{R}^{3}$, the update gate output is $\\boldsymbol{z}_{t} \\in (0,1)^{3}$, the reset gate output is $\\boldsymbol{r}_{t} \\in (0,1)^{3}$, and the precomputed candidate update (sometimes called the candidate hidden state) is $\\tilde{\\boldsymbol{h}}_{t} \\in (-1,1)^{3}$. You may assume the standard GRU forward equations from the literature and the usual properties of the logistic sigmoid and hyperbolic tangent.\n\nConsider the following numerically specified signals:\n- Previous hidden state $\\boldsymbol{h}_{t-1} = \\begin{pmatrix}0.35 \\\\ -0.70 \\\\ 0.20\\end{pmatrix}$.\n- Update gate $\\boldsymbol{z}_{t} = \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix}$.\n- Reset gate $\\boldsymbol{r}_{t} = \\begin{pmatrix}0.40 \\\\ 0.90 \\\\ 0.30\\end{pmatrix}$.\n- Candidate update $\\tilde{\\boldsymbol{h}}_{t} = \\begin{pmatrix}0.80 \\\\ -0.10 \\\\ -0.50\\end{pmatrix}$.\n\nTask:\n1. Using the GRU forward definition, compute the next hidden state $\\boldsymbol{h}_{t}$ from the given $\\boldsymbol{h}_{t-1}$, $\\boldsymbol{z}_{t}$, and $\\tilde{\\boldsymbol{h}}_{t}$.\n2. Using first principles of the GRU update rule and the range properties of the logistic sigmoid and hyperbolic tangent functions, verify that each coordinate of $\\boldsymbol{h}_{t}$ is a convex combination of the corresponding coordinates of $\\boldsymbol{h}_{t-1}$ and $\\tilde{\\boldsymbol{h}}_{t}$. Provide a brief justification that does not assume any property beyond the standard definitions of these functions and the GRU equations.\n3. Let $\\|\\boldsymbol{h}_{t}\\|_{2}$ denote the Euclidean norm of the new hidden state. Compute $\\|\\boldsymbol{h}_{t}\\|_{2}$ and express your final answer as a single real number, rounded to four significant figures. No units are required.",
            "solution": "The problem asks for three tasks related to a Gated Recurrent Unit (GRU) model: computation of the next hidden state, verification of a property of the update rule, and calculation of the Euclidean norm of the new hidden state. I will address these tasks in sequence.\n\nFirst, the givens are extracted:\nPrevious hidden state: $\\boldsymbol{h}_{t-1} = \\begin{pmatrix}0.35 \\\\ -0.70 \\\\ 0.20\\end{pmatrix}$.\nUpdate gate output: $\\boldsymbol{z}_{t} = \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix}$.\nReset gate output: $\\boldsymbol{r}_{t} = \\begin{pmatrix}0.40 \\\\ 0.90 \\\\ 0.30\\end{pmatrix}$.\nCandidate update: $\\tilde{\\boldsymbol{h}}_{t} = \\begin{pmatrix}0.80 \\\\ -0.10 \\\\ -0.50\\end{pmatrix}$.\n\nThe problem is validated as scientifically grounded, well-posed, and objective. It is based on the standard mathematical formulation of a GRU, a widely used component in machine learning and computational modeling. The provided data are consistent and sufficient for the required calculations. The value of the reset gate output $\\boldsymbol{r}_t$ is not required for the specified tasks, as the candidate update $\\tilde{\\boldsymbol{h}}_t$ is already provided. The problem is therefore valid.\n\n**Task 1: Compute the next hidden state $\\boldsymbol{h}_{t}$.**\n\nThe standard forward equation for updating the hidden state in a GRU is:\n$$\\boldsymbol{h}_{t} = (1 - \\boldsymbol{z}_{t}) \\odot \\boldsymbol{h}_{t-1} + \\boldsymbol{z}_{t} \\odot \\tilde{\\boldsymbol{h}}_{t}$$\nwhere $\\odot$ denotes the element-wise (Hadamard) product. The term $(1 - \\boldsymbol{z}_t)$ is computed by subtracting each element of $\\boldsymbol{z}_t$ from $1$.\n\nFirst, we compute the vector $(1 - \\boldsymbol{z}_t)$:\n$$1 - \\boldsymbol{z}_{t} = \\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix} = \\begin{pmatrix}1 - 0.25 \\\\ 1 - 0.60 \\\\ 1 - 0.05\\end{pmatrix} = \\begin{pmatrix}0.75 \\\\ 0.40 \\\\ 0.95\\end{pmatrix}$$\n\nNext, we compute the two terms of the update equation.\nThe first term is $(1 - \\boldsymbol{z}_{t}) \\odot \\boldsymbol{h}_{t-1}$:\n$$(1 - \\boldsymbol{z}_{t}) \\odot \\boldsymbol{h}_{t-1} = \\begin{pmatrix}0.75 \\\\ 0.40 \\\\ 0.95\\end{pmatrix} \\odot \\begin{pmatrix}0.35 \\\\ -0.70 \\\\ 0.20\\end{pmatrix} = \\begin{pmatrix}0.75 \\times 0.35 \\\\ 0.40 \\times (-0.70) \\\\ 0.95 \\times 0.20\\end{pmatrix} = \\begin{pmatrix}0.2625 \\\\ -0.28 \\\\ 0.19\\end{pmatrix}$$\n\nThe second term is $\\boldsymbol{z}_{t} \\odot \\tilde{\\boldsymbol{h}}_{t}$:\n$$\\boldsymbol{z}_{t} \\odot \\tilde{\\boldsymbol{h}}_{t} = \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix} \\odot \\begin{pmatrix}0.80 \\\\ -0.10 \\\\ -0.50\\end{pmatrix} = \\begin{pmatrix}0.25 \\times 0.80 \\\\ 0.60 \\times (-0.10) \\\\ 0.05 \\times (-0.50)\\end{pmatrix} = \\begin{pmatrix}0.20 \\\\ -0.06 \\\\ -0.025\\end{pmatrix}$$\n\nFinally, we sum these two resulting vectors to obtain $\\boldsymbol{h}_{t}$:\n$$\\boldsymbol{h}_{t} = \\begin{pmatrix}0.2625 \\\\ -0.28 \\\\ 0.19\\end{pmatrix} + \\begin{pmatrix}0.20 \\\\ -0.06 \\\\ -0.025\\end{pmatrix} = \\begin{pmatrix}0.2625 + 0.20 \\\\ -0.28 - 0.06 \\\\ 0.19 - 0.025\\end{pmatrix} = \\begin{pmatrix}0.4625 \\\\ -0.34 \\\\ 0.165\\end{pmatrix}$$\n\n**Task 2: Verify that each coordinate of $\\boldsymbol{h}_{t}$ is a convex combination of the corresponding coordinates of $\\boldsymbol{h}_{t-1}$ and $\\tilde{\\boldsymbol{h}}_{t}$.**\n\nA scalar $v$ is a convex combination of two scalars $u$ and $w$ if it can be written as $v = (1-\\alpha)u + \\alpha w$ for some coefficient $\\alpha \\in [0, 1]$.\n\nThe GRU update equation operates element-wise. For any coordinate $i \\in \\{1, 2, 3\\}$, the update rule is:\n$$(\\boldsymbol{h}_{t})_i = (1 - (\\boldsymbol{z}_{t})_i)(\\boldsymbol{h}_{t-1})_i + (\\boldsymbol{z}_{t})_i(\\tilde{\\boldsymbol{h}}_{t})_i$$\nLet's define the coefficient $\\alpha_i = (\\boldsymbol{z}_{t})_i$. The equation for the $i$-th coordinate becomes:\n$$(\\boldsymbol{h}_{t})_i = (1 - \\alpha_i)(\\boldsymbol{h}_{t-1})_i + \\alpha_i(\\tilde{\\boldsymbol{h}}_{t})_i$$\nThis is precisely the form of a convex combination of the scalars $(\\boldsymbol{h}_{t-1})_i$ and $(\\tilde{\\boldsymbol{h}}_{t})_i$.\n\nThe condition that must be met is $\\alpha_i \\in [0, 1]$. In the problem statement, the update gate output $\\boldsymbol{z}_t$ is given to be in $(0,1)^3$. This means that for each coordinate $i$, the component $(\\boldsymbol{z}_t)_i$ is strictly between $0$ and $1$. The interval $(0,1)$ is a subset of the interval $[0,1]$.\nTherefore, the coefficient $\\alpha_i = (\\boldsymbol{z}_t)_i$ satisfies the requirement for a convex combination. This confirms that each coordinate of the new hidden state $\\boldsymbol{h}_t$ is a convex combination of the corresponding coordinates of the previous hidden state $\\boldsymbol{h}_{t-1}$ and the candidate update $\\tilde{\\boldsymbol{h}}_t$. This property is fundamental to the stability of GRUs, as it ensures the new hidden state is an interpolation between the previous state and a candidate update, preventing runaway values.\n\n**Task 3: Compute the Euclidean norm $\\|\\boldsymbol{h}_{t}\\|_{2}$.**\n\nThe Euclidean norm of a vector $\\boldsymbol{v} = \\begin{pmatrix}v_1  v_2  v_3\\end{pmatrix}^T$ is given by the formula $\\|\\boldsymbol{v}\\|_2 = \\sqrt{v_1^2 + v_2^2 + v_3^2}$.\nUsing the vector $\\boldsymbol{h}_t$ computed in Task 1:\n$$\\boldsymbol{h}_{t} = \\begin{pmatrix}0.4625 \\\\ -0.34 \\\\ 0.165\\end{pmatrix}$$\nWe calculate its Euclidean norm:\n$$\\|\\boldsymbol{h}_{t}\\|_{2} = \\sqrt{(0.4625)^2 + (-0.34)^2 + (0.165)^2}$$\nFirst, compute the squares of the components:\n$$(0.4625)^2 = 0.21390625$$\n$$(-0.34)^2 = 0.1156$$\n$$(0.165)^2 = 0.027225$$\nNext, sum these squared values:\n$$\\|\\boldsymbol{h}_{t}\\|_{2}^2 = 0.21390625 + 0.1156 + 0.027225 = 0.35673125$$\nFinally, take the square root of the sum:\n$$\\|\\boldsymbol{h}_{t}\\|_{2} = \\sqrt{0.35673125} \\approx 0.59726983$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $5$, $9$, $7$, $2$. The fifth significant figure is $6$, so we round up the fourth figure.\n$$\\|\\boldsymbol{h}_{t}\\|_{2} \\approx 0.5973$$\nThis is the final numerical answer.",
            "answer": "$$\n\\boxed{0.5973}\n$$"
        },
        {
            "introduction": "Moving from a single unit to the network as a whole, this exercise explores the conditions required for an RNN to exhibit stable and predictable dynamics. You will investigate the Echo State Property (ESP), which ensures that the network's state becomes asymptotically independent of its initial conditions, a prerequisite for reliable sequence processing. This practice connects abstract mathematical concepts, such as the spectral radius of the recurrent weight matrix, to this vital functional property of recurrent networks .",
            "id": "4013925",
            "problem": "Consider a discrete-time Echo State Network (ESN) whose reservoir dynamics are given by $\\boldsymbol{x}_{t+1} = \\phi(\\boldsymbol{W} \\boldsymbol{x}_t + \\boldsymbol{U} \\boldsymbol{u}_t + \\boldsymbol{b})$, where $\\boldsymbol{x}_t \\in \\mathbb{R}^{n}$ is the reservoir state, $\\boldsymbol{u}_t \\in \\mathbb{R}^{m}$ is a bounded input ($\\sup_t \\|\\boldsymbol{u}_t\\|  \\infty$), $\\boldsymbol{W} \\in \\mathbb{R}^{n \\times n}$ is the recurrent weight matrix, $\\boldsymbol{U} \\in \\mathbb{R}^{n \\times m}$ is the input weight matrix, and $\\boldsymbol{b} \\in \\mathbb{R}^{n}$ is a bias. The activation function $\\phi$ acts elementwise and is $\\phi(z) = \\tanh(\\beta z)$ with $\\beta  0$. The Echo State Property (ESP) states that for any bounded input sequence, the reservoir state $\\boldsymbol{x}_t$ becomes asymptotically independent of the initial condition.\n\nYou are told that the eigenvalues of the matrix $\\boldsymbol{W}$ are\n$$\n\\lambda_1 = 0.82, \\quad \\lambda_2 = -0.60 + 0.75 i, \\quad \\lambda_3 = 1.04 \\exp\\!\\left(i \\frac{\\pi}{6}\\right), \\quad \\lambda_4 = -0.95,\n$$\nand that $\\beta = 0.96$. The spectral radius $\\rho(\\boldsymbol{W})$ is defined as the maximum modulus of the eigenvalues of $\\boldsymbol{W}$. The activation function $\\phi$ is globally Lipschitz with Lipschitz constant $L_{\\phi}$, defined by the smallest $L_{\\phi} \\ge 0$ such that $|\\phi(a) - \\phi(b)| \\le L_{\\phi} |a - b|$ for all $a,b \\in \\mathbb{R}$.\n\nStarting from fundamental definitions of a Lipschitz contraction, the spectral radius of a linear operator, and well-tested facts such as submultiplicativity of induced matrix norms and the Gelfand formula $\\lim_{k \\to \\infty} \\|\\boldsymbol{W}^{k}\\|^{1/k} = \\rho(\\boldsymbol{W})$ (for any consistent matrix norm), derive a sufficient, norm-independent contraction criterion for the ESN reservoir under elementwise $\\phi$. Then, compute the associated contraction coefficient $c$ for the given $\\boldsymbol{W}$ and $\\beta$.\n\nProvide only the numerical value of $c$, rounded to four significant figures. No units are required. Your derivation must be scientifically self-consistent and must not assume shortcut formulas beyond the stated base principles.",
            "solution": "The Echo State Property (ESP) is achieved when, for any bounded input sequence $\\{\\boldsymbol{u}_t\\}$, trajectories starting from different initial conditions converge to each other as $t \\to \\infty$. A standard route to sufficient conditions for ESP is to show that the driven state update map is a contraction with respect to the reservoir state for fixed input sequence. We formalize the argument using Lipschitz continuity, submultiplicativity of matrix norms, and spectral radius.\n\nLet the reservoir update be $\\boldsymbol{x}_{t+1} = \\phi(\\boldsymbol{W} \\boldsymbol{x}_t + \\boldsymbol{U} \\boldsymbol{u}_t + \\boldsymbol{b})$, acting elementwise. Consider two trajectories $\\boldsymbol{x}_t$ and $\\boldsymbol{y}_t$ driven by the same input sequence and bias, but initialized at different initial conditions $\\boldsymbol{x}_0$ and $\\boldsymbol{y}_0$. Define $\\boldsymbol{\\delta}_t = \\boldsymbol{x}_t - \\boldsymbol{y}_t$. Then\n\n$$\n\\boldsymbol{\\delta}_{t+1} = \\phi(\\boldsymbol{W} \\boldsymbol{x}_t + \\boldsymbol{U} \\boldsymbol{u}_t + \\boldsymbol{b}) - \\phi(\\boldsymbol{W} \\boldsymbol{y}_t + \\boldsymbol{U} \\boldsymbol{u}_t + \\boldsymbol{b}).\n$$\n\nBecause $\\phi$ acts elementwise and is globally Lipschitz with constant $L_{\\phi}$, for each coordinate $j$,\n\n$$\n|\\delta_{t+1}^{(j)}| \\le L_{\\phi} \\left| \\left(\\boldsymbol{W} \\boldsymbol{x}_t + \\boldsymbol{U} \\boldsymbol{u}_t + \\boldsymbol{b}\\right)^{(j)} - \\left(\\boldsymbol{W} \\boldsymbol{y}_t + \\boldsymbol{U} \\boldsymbol{u}_t + \\boldsymbol{b}\\right)^{(j)} \\right| = L_{\\phi} \\left| \\left(\\boldsymbol{W} \\boldsymbol{\\delta}_t\\right)^{(j)} \\right|.\n$$\n\nTaking a consistent induced norm $\\|\\cdot\\|$ on $\\mathbb{R}^n$, and using the property that elementwise Lipschitz continuity lifts to the vector norm with the same constant, we obtain\n\n$$\n\\|\\boldsymbol{\\delta}_{t+1}\\| \\le L_{\\phi} \\|\\boldsymbol{W} \\boldsymbol{\\delta}_t\\| \\le L_{\\phi} \\|\\boldsymbol{W}\\| \\, \\|\\boldsymbol{\\delta}_t\\|.\n$$\n\nThis inequality shows that the one-step Lipschitz constant (with respect to state differences) is bounded by $L_{\\phi} \\|\\boldsymbol{W}\\|$. If $L_{\\phi} \\|\\boldsymbol{W}\\|  1$ for some induced matrix norm $\\|\\cdot\\|$, the map is a contraction, ensuring that $\\|\\boldsymbol{\\delta}_t\\|$ decays exponentially, which implies ESP.\n\nTo remove dependence on the specific choice of matrix norm and obtain a norm-independent sufficient criterion, we consider the $k$-step map. Iterating the inequality yields\n\n$$\n\\|\\boldsymbol{\\delta}_{t+k}\\| \\le L_{\\phi}^{k} \\|\\boldsymbol{W}^{k}\\| \\, \\|\\boldsymbol{\\delta}_t\\|.\n$$\n\nBy the Gelfand formula, for any consistent matrix norm, $\\lim_{k \\to \\infty} \\|\\boldsymbol{W}^{k}\\|^{1/k} = \\rho(\\boldsymbol{W})$, where $\\rho(\\boldsymbol{W})$ is the spectral radius. Therefore,\n\n$$\n\\limsup_{k \\to \\infty} \\left(L_{\\phi}^{k} \\|\\boldsymbol{W}^{k}\\|\\right)^{1/k} = L_{\\phi} \\, \\rho(\\boldsymbol{W}).\n$$\n\nIf $L_{\\phi} \\rho(\\boldsymbol{W})  1$, the asymptotic $k$-step Lipschitz factor decays exponentially, guaranteeing contraction of trajectories and hence ESP for bounded inputs. Thus, a sufficient, norm-independent contraction criterion is\n\n$$\nL_{\\phi} \\, \\rho(\\boldsymbol{W})  1.\n$$\n\nWe now compute $L_{\\phi}$ and $\\rho(\\boldsymbol{W})$ for the given system. The activation is $\\phi(z) = \\tanh(\\beta z)$ elementwise. For scalar $z$, the derivative is\n\n$$\n\\frac{d}{dz} \\tanh(\\beta z) = \\beta \\, \\operatorname{sech}^{2}(\\beta z),\n$$\n\nwhose maximum over $z \\in \\mathbb{R}$ is $\\beta$ (attained at $z = 0$ since $\\operatorname{sech}^{2}(0) = 1$). Therefore, the global Lipschitz constant is\n\n$$\nL_{\\phi} = \\beta = 0.96.\n$$\n\nNext, compute the spectral radius $\\rho(\\boldsymbol{W})$ from the eigenvalues. The modulus of each eigenvalue is\n\n$$\n|\\lambda_1| = |0.82| = 0.82,\n$$\n\n\n$$\n|\\lambda_2| = \\sqrt{(-0.60)^{2} + (0.75)^{2}} = \\sqrt{0.36 + 0.5625} = \\sqrt{0.9225} = 0.96,\n$$\n\n\n$$\n|\\lambda_3| = \\left|1.04 \\exp\\!\\left(i \\frac{\\pi}{6}\\right)\\right| = 1.04,\n$$\n\n\n$$\n|\\lambda_4| = |-0.95| = 0.95.\n$$\n\nHence,\n\n$$\n\\rho(\\boldsymbol{W}) = \\max\\{0.82, 0.96, 1.04, 0.95\\} = 1.04.\n$$\n\nThe associated contraction coefficient computed from the norm-independent sufficient criterion is\n\n$$\nc = L_{\\phi} \\, \\rho(\\boldsymbol{W}) = 0.96 \\times 1.04 = 0.9984.\n$$\n\nRounded to four significant figures, this value remains $0.9984$. Since $c  1$, the sufficient criterion is met and the ESP holds under the given conditions. The requested final output is the numerical value of $c$.",
            "answer": "$$\\boxed{0.9984}$$"
        },
        {
            "introduction": "Our final practice addresses a critical challenge that arises when training RNNs for sequence generation: the discrepancy between the training and deployment phases. You will analytically derive the error accumulation that occurs when a model trained with \"teacher forcing\"—where the true previous state is provided as input at each step—is switched to a free-running, autoregressive mode. This exercise uses a simplified linear model to provide clear, quantitative insight into how model mismatch and unmodeled stochasticity can lead to compounding errors over time .",
            "id": "4013892",
            "problem": "Consider a simplified linear model of a cortical microcircuit’s latent activity sequence, represented as an Autoregressive order one ($AR(1)$) process. Let the ground-truth latent state be $\\{x_t\\}_{t \\geq 0}$ with dynamics\n$$\nx_t = a\\,x_{t-1} + \\epsilon_t,\n$$\nwhere $a \\in \\mathbb{R}$ is the true autoregressive coefficient, $\\epsilon_t$ are independent and identically distributed Gaussian innovations with $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$, and $x_0 \\sim \\mathcal{N}(0,s_0)$ is independent of $\\{\\epsilon_t\\}_{t \\geq 1}$. A Recurrent Neural Network (RNN) trained with teacher forcing learns a one-step linear predictor that, at training time, takes the true previous state $x_{t-1}$ as input and predicts $\\hat{x}_t = \\hat{a}\\,x_{t-1}$, where $\\hat{a} \\in \\mathbb{R}$ is the learned coefficient. Due to model mismatch, assume $\\hat{a} \\neq a$. At deployment for autonomous sequence generation, the RNN is run in free mode without teacher forcing, iterating\n$$\n\\hat{x}_t = \\hat{a}\\,\\hat{x}_{t-1}, \\quad \\text{with } \\hat{x}_0 = x_0.\n$$\nUsing only core definitions from probability and linear systems and the independence properties stated above, derive a closed-form analytic expression for the expected squared deviation between the free-running RNN prediction and the ground-truth latent state at a fixed horizon $T \\in \\mathbb{N}$,\n$$\n\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big],\n$$\nas a function of $a$, $\\hat{a}$, $\\sigma^2$, $s_0$, and $T$. Express your final answer as a single analytic expression. No rounding is required. Angles, if any appear, should be in radians; there are no physical units required for this quantity.",
            "solution": "To derive the expression for the expected squared deviation $\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big]$, we first need to find closed-form expressions for the ground-truth state $x_T$ and the predicted state $\\hat{x}_T$ at the time horizon $T$.\n\nFirst, consider the free-running RNN prediction $\\hat{x}_t$. The dynamics are given by the recurrence relation $\\hat{x}_t = \\hat{a}\\,\\hat{x}_{t-1}$ with the initial condition $\\hat{x}_0 = x_0$. We can unroll this recurrence:\n$$\n\\hat{x}_1 = \\hat{a}\\,\\hat{x}_0\n$$\n$$\n\\hat{x}_2 = \\hat{a}\\,\\hat{x}_1 = \\hat{a}(\\hat{a}\\,\\hat{x}_0) = \\hat{a}^2\\,\\hat{x}_0\n$$\nBy induction, the state at time $T$ is given by:\n$$\n\\hat{x}_T = \\hat{a}^T\\,\\hat{x}_0 = \\hat{a}^T\\,x_0\n$$\n\nNext, consider the ground-truth latent state $x_t$. The dynamics are given by the stochastic recurrence relation $x_t = a\\,x_{t-1} + \\epsilon_t$. We unroll this relation, starting from the initial state $x_0$:\n$$\nx_1 = a\\,x_0 + \\epsilon_1\n$$\n$$\nx_2 = a\\,x_1 + \\epsilon_2 = a(a\\,x_0 + \\epsilon_1) + \\epsilon_2 = a^2\\,x_0 + a\\,\\epsilon_1 + \\epsilon_2\n$$\nBy induction, we can express the state at time $T$ as a function of the initial state $x_0$ and the sequence of innovations $\\{\\epsilon_t\\}_{t=1}^T$:\n$$\nx_T = a^T\\,x_0 + \\sum_{k=1}^{T} a^{T-k} \\epsilon_k\n$$\n\nNow we can write the deviation term, $D_T = \\hat{x}_T - x_T$:\n$$\nD_T = \\hat{a}^T\\,x_0 - \\left( a^T\\,x_0 + \\sum_{k=1}^{T} a^{T-k} \\epsilon_k \\right)\n$$\n$$\nD_T = (\\hat{a}^T - a^T)\\,x_0 - \\sum_{k=1}^{T} a^{T-k} \\epsilon_k\n$$\nThe objective is to compute the expectation of the square of this deviation, $\\mathbb{E}[D_T^2]$. Let's expand the square:\n$$\nD_T^2 = \\left( (\\hat{a}^T - a^T)\\,x_0 - \\sum_{k=1}^{T} a^{T-k} \\epsilon_k \\right)^2\n$$\n$$\nD_T^2 = ((\\hat{a}^T - a^T)\\,x_0)^2 - 2 (\\hat{a}^T - a^T)\\,x_0 \\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right) + \\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\n$$\n$$\nD_T^2 = (\\hat{a}^T - a^T)^2\\,x_0^2 - 2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} x_0 \\epsilon_k + \\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\n$$\nBy linearity of expectation, we can compute the expectation of each term separately:\n$$\n\\mathbb{E}[D_T^2] = \\mathbb{E}\\left[(\\hat{a}^T - a^T)^2\\,x_0^2\\right] - \\mathbb{E}\\left[2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} x_0 \\epsilon_k\\right] + \\mathbb{E}\\left[\\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\\right]\n$$\n\nLet's analyze each of the three terms.\n\nTerm 1: $\\mathbb{E}\\left[(\\hat{a}^T - a^T)^2\\,x_0^2\\right]$\nThe factor $(\\hat{a}^T - a^T)^2$ is a constant. We can pull it out of the expectation:\n$$\n\\mathbb{E}\\left[(\\hat{a}^T - a^T)^2\\,x_0^2\\right] = (\\hat{a}^T - a^T)^2\\,\\mathbb{E}[x_0^2]\n$$\nWe are given that $x_0 \\sim \\mathcal{N}(0, s_0)$, which means $\\mathbb{E}[x_0] = 0$ and the variance is $\\text{Var}(x_0) = s_0$. The variance is defined as $\\text{Var}(x_0) = \\mathbb{E}[x_0^2] - (\\mathbb{E}[x_0])^2$. Since the mean is $0$, we have $\\mathbb{E}[x_0^2] = \\text{Var}(x_0) = s_0$. Therefore, the first term is:\n$$\n(\\hat{a}^T - a^T)^2\\,s_0\n$$\n\nTerm 2: $-\\mathbb{E}\\left[2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} x_0 \\epsilon_k\\right]$\nWe pull out the constants and use linearity of expectation:\n$$\n-2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} \\mathbb{E}[x_0 \\epsilon_k]\n$$\nThe problem states that $x_0$ is independent of $\\{\\epsilon_t\\}_{t \\geq 1}$. For any $k \\in \\{1, \\dots, T\\}$, $x_0$ and $\\epsilon_k$ are independent. For independent random variables, the expectation of their product is the product of their expectations: $\\mathbb{E}[x_0 \\epsilon_k] = \\mathbb{E}[x_0]\\mathbb{E}[\\epsilon_k]$.\nWe know $\\mathbb{E}[x_0] = 0$ and $\\mathbb{E}[\\epsilon_k] = 0$ for all $k$. Thus, $\\mathbb{E}[x_0 \\epsilon_k] = 0 \\times 0 = 0$.\nThe entire cross-term sum is therefore zero.\n\nTerm 3: $\\mathbb{E}\\left[\\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\\right]$\nLet $S_T = \\sum_{k=1}^{T} a^{T-k} \\epsilon_k$. Since $\\mathbb{E}[\\epsilon_k]=0$ for all $k$, the mean of $S_T$ is $\\mathbb{E}[S_T] = \\sum_{k=1}^{T} a^{T-k} \\mathbb{E}[\\epsilon_k] = 0$. The expression we want to compute is thus the variance of $S_T$, $\\text{Var}(S_T) = \\mathbb{E}[S_T^2] - (\\mathbb{E}[S_T])^2 = \\mathbb{E}[S_T^2]$.\nThe innovations $\\{\\epsilon_t\\}$ are i.i.d., which implies they are uncorrelated. For a sum of uncorrelated random variables, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(S_T) = \\text{Var}\\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right) = \\sum_{k=1}^{T} \\text{Var}(a^{T-k} \\epsilon_k)\n$$\nUsing the property $\\text{Var}(cZ) = c^2 \\text{Var}(Z)$, we get:\n$$\n\\sum_{k=1}^{T} (a^{T-k})^2 \\text{Var}(\\epsilon_k) = \\sum_{k=1}^{T} a^{2(T-k)} \\sigma^2 = \\sigma^2 \\sum_{k=1}^{T} a^{2(T-k)}\n$$\nLet's analyze the sum. We can change the index of summation by letting $j = T-k$. When $k=1$, $j=T-1$. When $k=T$, $j=0$. The sum becomes:\n$$\n\\sum_{j=0}^{T-1} a^{2j} = \\sum_{j=0}^{T-1} (a^2)^j\n$$\nThis is a finite geometric series with $T$ terms, first term $1$, and common ratio $r = a^2$. The sum is given by the formula $\\frac{1-r^n}{1-r}$, where $n=T$. This formula is valid for $r \\neq 1$, i.e., $a^2 \\neq 1$.\n$$\n\\sum_{j=0}^{T-1} (a^2)^j = \\frac{1 - (a^2)^T}{1 - a^2} = \\frac{1 - a^{2T}}{1 - a^2}\n$$\nIf $a^2 = 1$, the sum is simply the number of terms, which is $T$. The provided fractional form is standard for a single analytic expression, being the continuous extension for $a^2 \\to 1$. Thus, the third term is:\n$$\n\\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}\n$$\n\nFinally, we sum the three terms to get the final expression for the expected squared deviation:\n$$\n\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big] = (\\hat{a}^T - a^T)^2\\,s_0 + 0 + \\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}\n$$\n$$\n\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big] = (\\hat{a}^T - a^T)^2\\,s_0 + \\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}\n$$\nThis expression consists of two parts. The first term, $(\\hat{a}^T - a^T)^2\\,s_0$, represents the error propagation from the initial state due to the mismatch in dynamics between the model ($\\hat{a}$) and the true process ($a$). The second term, $\\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}$, represents the error accumulated from the unmodeled stochastic innovations $\\{\\epsilon_t\\}$ of the true process.",
            "answer": "$$\n\\boxed{(\\hat{a}^{T} - a^{T})^{2} s_{0} + \\sigma^{2} \\frac{1 - a^{2T}}{1 - a^{2}}}\n$$"
        }
    ]
}