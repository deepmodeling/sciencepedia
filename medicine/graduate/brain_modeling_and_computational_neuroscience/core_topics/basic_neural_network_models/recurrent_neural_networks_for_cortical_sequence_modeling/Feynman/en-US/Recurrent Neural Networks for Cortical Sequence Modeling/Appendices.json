{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how recurrent neural networks model complex sequences, we must first master their fundamental building block: the single-step update. This exercise provides a concrete, hands-on calculation of a forward pass in a Gated Recurrent Unit (GRU), one of the most effective and widely used RNN architectures. By working through the numerical update, you will gain an intuitive feel for how the update gate $z_t$ acts as a dynamic switch, allowing the network to either retain past information or integrate new candidate states, a core mechanism for capturing temporal dependencies ().",
            "id": "4013919",
            "problem": "A local circuit model for cortical sequence generation is implemented as a three-dimensional Gated Recurrent Unit (GRU). At time step $t$, the previous hidden state is $h_{t-1} \\in \\mathbb{R}^{3}$, the update gate output is $z_{t} \\in (0,1)^{3}$, the reset gate output is $r_{t} \\in (0,1)^{3}$, and the precomputed candidate update (sometimes called the candidate hidden state) is $\\tilde{h}_{t} \\in (-1,1)^{3}$. You may assume the standard GRU forward equations from the literature and the usual properties of the logistic sigmoid and hyperbolic tangent.\n\nConsider the following numerically specified signals:\n- Previous hidden state $h_{t-1} = \\begin{pmatrix}0.35 \\\\ -0.70 \\\\ 0.20\\end{pmatrix}$.\n- Update gate $z_{t} = \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix}$.\n- Reset gate $r_{t} = \\begin{pmatrix}0.40 \\\\ 0.90 \\\\ 0.30\\end{pmatrix}$.\n- Candidate update $\\tilde{h}_{t} = \\begin{pmatrix}0.80 \\\\ -0.10 \\\\ -0.50\\end{pmatrix}$.\n\nTask:\n1. Using the GRU forward definition, compute the next hidden state $h_{t}$ from the given $h_{t-1}$, $z_{t}$, and $\\tilde{h}_{t}$.\n2. Using first principles of the GRU update rule and the range properties of the logistic sigmoid and hyperbolic tangent functions, verify that each coordinate of $h_{t}$ is a convex combination of the corresponding coordinates of $h_{t-1}$ and $\\tilde{h}_{t}$. Provide a brief justification that does not assume any property beyond the standard definitions of these functions and the GRU equations.\n3. Let $\\|h_{t}\\|_{2}$ denote the Euclidean norm of the new hidden state. Compute $\\|h_{t}\\|_{2}$ and express your final answer as a single real number, rounded to four significant figures. No units are required.",
            "solution": "The problem asks for three tasks related to a Gated Recurrent Unit (GRU) model: computation of the next hidden state, verification of a property of the update rule, and calculation of the Euclidean norm of the new hidden state. I will address these tasks in sequence.\n\nFirst, the givens are extracted:\nPrevious hidden state: $h_{t-1} = \\begin{pmatrix}0.35 \\\\ -0.70 \\\\ 0.20\\end{pmatrix}$.\nUpdate gate output: $z_{t} = \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix}$.\nReset gate output: $r_{t} = \\begin{pmatrix}0.40 \\\\ 0.90 \\\\ 0.30\\end{pmatrix}$.\nCandidate update: $\\tilde{h}_{t} = \\begin{pmatrix}0.80 \\\\ -0.10 \\\\ -0.50\\end{pmatrix}$.\n\nThe problem is validated as scientifically grounded, well-posed, and objective. It is based on the standard mathematical formulation of a GRU, a widely used component in machine learning and computational modeling. The provided data are consistent and sufficient for the required calculations. The value of the reset gate output $r_t$ is not required for the specified tasks, as the candidate update $\\tilde{h}_t$ is already provided. The problem is therefore valid.\n\n**Task 1: Compute the next hidden state $h_{t}$.**\n\nThe standard forward equation for updating the hidden state in a GRU is:\n$$h_{t} = (1 - z_{t}) \\odot h_{t-1} + z_{t} \\odot \\tilde{h}_{t}$$\nwhere $\\odot$ denotes the element-wise (Hadamard) product. The term $(1 - z_t)$ is computed by subtracting each element of $z_t$ from $1$.\n\nFirst, we compute the vector $(1 - z_t)$:\n$$1 - z_{t} = \\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix} = \\begin{pmatrix}1 - 0.25 \\\\ 1 - 0.60 \\\\ 1 - 0.05\\end{pmatrix} = \\begin{pmatrix}0.75 \\\\ 0.40 \\\\ 0.95\\end{pmatrix}$$\n\nNext, we compute the two terms of the update equation.\nThe first term is $(1 - z_{t}) \\odot h_{t-1}$:\n$$(1 - z_{t}) \\odot h_{t-1} = \\begin{pmatrix}0.75 \\\\ 0.40 \\\\ 0.95\\end{pmatrix} \\odot \\begin{pmatrix}0.35 \\\\ -0.70 \\\\ 0.20\\end{pmatrix} = \\begin{pmatrix}0.75 \\times 0.35 \\\\ 0.40 \\times (-0.70) \\\\ 0.95 \\times 0.20\\end{pmatrix} = \\begin{pmatrix}0.2625 \\\\ -0.28 \\\\ 0.19\\end{pmatrix}$$\n\nThe second term is $z_{t} \\odot \\tilde{h}_{t}$:\n$$z_{t} \\odot \\tilde{h}_{t} = \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix} \\odot \\begin{pmatrix}0.80 \\\\ -0.10 \\\\ -0.50\\end{pmatrix} = \\begin{pmatrix}0.25 \\times 0.80 \\\\ 0.60 \\times (-0.10) \\\\ 0.05 \\times (-0.50)\\end{pmatrix} = \\begin{pmatrix}0.20 \\\\ -0.06 \\\\ -0.025\\end{pmatrix}$$\n\nFinally, we sum these two resulting vectors to obtain $h_{t}$:\n$$h_{t} = \\begin{pmatrix}0.2625 \\\\ -0.28 \\\\ 0.19\\end{pmatrix} + \\begin{pmatrix}0.20 \\\\ -0.06 \\\\ -0.025\\end{pmatrix} = \\begin{pmatrix}0.2625 + 0.20 \\\\ -0.28 - 0.06 \\\\ 0.19 - 0.025\\end{pmatrix} = \\begin{pmatrix}0.4625 \\\\ -0.34 \\\\ 0.165\\end{pmatrix}$$\n\n**Task 2: Verify that each coordinate of $h_{t}$ is a convex combination of the corresponding coordinates of $h_{t-1}$ and $\\tilde{h}_{t}$.**\n\nA scalar $v$ is a convex combination of two scalars $u$ and $w$ if it can be written as $v = (1-\\alpha)u + \\alpha w$ for some coefficient $\\alpha \\in [0, 1]$.\n\nThe GRU update equation operates element-wise. For any coordinate $i \\in \\{1, 2, 3\\}$, the update rule is:\n$$(h_{t})_i = (1 - (z_{t})_i)(h_{t-1})_i + (z_{t})_i(\\tilde{h}_{t})_i$$\nLet's define the coefficient $\\alpha_i = (z_{t})_i$. The equation for the $i$-th coordinate becomes:\n$$(h_{t})_i = (1 - \\alpha_i)(h_{t-1})_i + \\alpha_i(\\tilde{h}_{t})_i$$\nThis is precisely the form of a convex combination of the scalars $(h_{t-1})_i$ and $(\\tilde{h}_{t})_i$.\n\nThe condition that must be met is $\\alpha_i \\in [0, 1]$. In the problem statement, the update gate output $z_t$ is given to be in $(0,1)^3$. This means that for each coordinate $i$, the component $(z_t)_i$ is strictly between $0$ and $1$. The interval $(0,1)$ is a subset of the interval $[0,1]$.\nTherefore, the coefficient $\\alpha_i = (z_t)_i$ satisfies the requirement for a convex combination. This confirms that each coordinate of the new hidden state $h_t$ is a convex combination of the corresponding coordinates of the previous hidden state $h_{t-1}$ and the candidate update $\\tilde{h}_t$. This property is fundamental to the stability of GRUs, as it ensures the new hidden state is an interpolation between the previous state and a candidate update, preventing runaway values.\n\n**Task 3: Compute the Euclidean norm $\\|h_{t}\\|_{2}$.**\n\nThe Euclidean norm of a vector $v = \\begin{pmatrix}v_1  v_2  v_3\\end{pmatrix}^T$ is given by the formula $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2 + v_3^2}$.\nUsing the vector $h_t$ computed in Task 1:\n$$h_{t} = \\begin{pmatrix}0.4625 \\\\ -0.34 \\\\ 0.165\\end{pmatrix}$$\nWe calculate its Euclidean norm:\n$$\\|h_{t}\\|_{2} = \\sqrt{(0.4625)^2 + (-0.34)^2 + (0.165)^2}$$\nFirst, compute the squares of the components:\n$$(0.4625)^2 = 0.21390625$$\n$$(-0.34)^2 = 0.1156$$\n$$(0.165)^2 = 0.027225$$\nNext, sum these squared values:\n$$\\|h_{t}\\|_{2}^2 = 0.21390625 + 0.1156 + 0.027225 = 0.35673125$$\nFinally, take the square root of the sum:\n$$\\|h_{t}\\|_{2} = \\sqrt{0.35673125} \\approx 0.59726983$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $5$, $9$, $7$, $2$. The fifth significant figure is $6$, so we round up the fourth figure.\n$$\\|h_{t}\\|_{2} \\approx 0.5973$$\nThis is the final numerical answer.",
            "answer": "$$\n\\boxed{0.5973}\n$$"
        },
        {
            "introduction": "A central challenge in training generative sequence models is the discrepancy between training (with 'teacher forcing') and deployment (in 'free-running' mode). This practice delves into this issue by analyzing how prediction errors accumulate over time in a simplified autoregressive model (). By deriving an analytical expression for the expected squared error, you will uncover how model mismatch and unmodeled process noise contribute to the divergence of generated sequences from their ground-truth counterparts, offering a foundational understanding of the 'exposure bias' problem.",
            "id": "4013892",
            "problem": "Consider a simplified linear model of a cortical microcircuit’s latent activity sequence, represented as an Autoregressive order one ($AR(1)$) process. Let the ground-truth latent state be $\\{x_t\\}_{t \\geq 0}$ with dynamics\n$$\nx_t = a\\,x_{t-1} + \\epsilon_t,\n$$\nwhere $a \\in \\mathbb{R}$ is the true autoregressive coefficient, $\\epsilon_t$ are independent and identically distributed Gaussian innovations with $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$, and $x_0 \\sim \\mathcal{N}(0,s_0)$ is independent of $\\{\\epsilon_t\\}_{t \\geq 1}$. A Recurrent Neural Network (RNN) trained with teacher forcing learns a one-step linear predictor that, at training time, takes the true previous state $x_{t-1}$ as input and predicts $\\hat{x}_t = \\hat{a}\\,x_{t-1}$, where $\\hat{a} \\in \\mathbb{R}$ is the learned coefficient. Due to model mismatch, assume $\\hat{a} \\neq a$. At deployment for autonomous sequence generation, the RNN is run in free mode without teacher forcing, iterating\n$$\n\\hat{x}_t = \\hat{a}\\,\\hat{x}_{t-1}, \\quad \\text{with } \\hat{x}_0 = x_0.\n$$\nUsing only core definitions from probability and linear systems and the independence properties stated above, derive a closed-form analytic expression for the expected squared deviation between the free-running RNN prediction and the ground-truth latent state at a fixed horizon $T \\in \\mathbb{N}$,\n$$\n\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big],\n$$\nas a function of $a$, $\\hat{a}$, $\\sigma^2$, $s_0$, and $T$. Express your final answer as a single analytic expression. No rounding is required. Angles, if any appear, should be in radians; there are no physical units required for this quantity.",
            "solution": "The user wants me to solve this problem.\n\n### Step 1: Extract Givens\n- Ground-truth latent state dynamics: $x_t = a\\,x_{t-1} + \\epsilon_t$ for $t \\geq 1$.\n- True autoregressive coefficient: $a \\in \\mathbb{R}$.\n- Learned autoregressive coefficient: $\\hat{a} \\in \\mathbb{R}$, with $\\hat{a} \\neq a$.\n- Innovations: $\\epsilon_t$ are independent and identically distributed (i.i.d.) with $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n- Initial state distribution: $x_0 \\sim \\mathcal{N}(0, s_0)$.\n- Independence: $x_0$ is independent of the set of innovations $\\{\\epsilon_t\\}_{t \\geq 1}$.\n- Free-running RNN dynamics: $\\hat{x}_t = \\hat{a}\\,\\hat{x}_{t-1}$ for $t \\geq 1$.\n- Initial condition for the RNN: $\\hat{x}_0 = x_0$.\n- Fixed time horizon: $T \\in \\mathbb{N}$.\n- Objective: Derive a closed-form expression for $\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. It formulates a comparison between a true stochastic process (an Autoregressive model of order $1$, or $AR(1)$) and a deterministic linear model, which is a common and fundamental problem in time series analysis, control theory, and machine learning. The concepts of teacher forcing and free-running mode are standard in the study of recurrent neural networks.\n\nThe problem is well-posed. The givens are sufficient and consistent, allowing for a unique solution. The parameters ($a, \\hat{a}, \\sigma^2, s_0, T$) are clearly defined, and the objective is a standard statistical measure (mean squared error). All necessary distributional and independence assumptions are provided.\n\nThe problem is objective. The language is precise and mathematical, free from any subjective or speculative claims.\n\nThe problem requires a derivation based on first principles of probability and linear systems, which is a valid scientific task. It is not trivial or tautological, as it requires unrolling recurrences and applying properties of expectation and variance.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation.\n\nTo derive the expression for the expected squared deviation $\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big]$, we first need to find closed-form expressions for the ground-truth state $x_T$ and the predicted state $\\hat{x}_T$ at the time horizon $T$.\n\nFirst, consider the free-running RNN prediction $\\hat{x}_t$. The dynamics are given by the recurrence relation $\\hat{x}_t = \\hat{a}\\,\\hat{x}_{t-1}$ with the initial condition $\\hat{x}_0 = x_0$. We can unroll this recurrence:\n$$\n\\hat{x}_1 = \\hat{a}\\,\\hat{x}_0\n$$\n$$\n\\hat{x}_2 = \\hat{a}\\,\\hat{x}_1 = \\hat{a}(\\hat{a}\\,\\hat{x}_0) = \\hat{a}^2\\,\\hat{x}_0\n$$\nBy induction, the state at time $T$ is given by:\n$$\n\\hat{x}_T = \\hat{a}^T\\,\\hat{x}_0 = \\hat{a}^T\\,x_0\n$$\n\nNext, consider the ground-truth latent state $x_t$. The dynamics are given by the stochastic recurrence relation $x_t = a\\,x_{t-1} + \\epsilon_t$. We unroll this relation, starting from the initial state $x_0$:\n$$\nx_1 = a\\,x_0 + \\epsilon_1\n$$\n$$\nx_2 = a\\,x_1 + \\epsilon_2 = a(a\\,x_0 + \\epsilon_1) + \\epsilon_2 = a^2\\,x_0 + a\\,\\epsilon_1 + \\epsilon_2\n$$\nBy induction, we can express the state at time $T$ as a function of the initial state $x_0$ and the sequence of innovations $\\{\\epsilon_t\\}_{t=1}^T$:\n$$\nx_T = a^T\\,x_0 + \\sum_{k=1}^{T} a^{T-k} \\epsilon_k\n$$\n\nNow we can write the deviation term, $D_T = \\hat{x}_T - x_T$:\n$$\nD_T = \\hat{a}^T\\,x_0 - \\left( a^T\\,x_0 + \\sum_{k=1}^{T} a^{T-k} \\epsilon_k \\right)\n$$\n$$\nD_T = (\\hat{a}^T - a^T)\\,x_0 - \\sum_{k=1}^{T} a^{T-k} \\epsilon_k\n$$\nThe objective is to compute the expectation of the square of this deviation, $\\mathbb{E}[D_T^2]$. Let's expand the square:\n$$\nD_T^2 = \\left( (\\hat{a}^T - a^T)\\,x_0 - \\sum_{k=1}^{T} a^{T-k} \\epsilon_k \\right)^2\n$$\n$$\nD_T^2 = ((\\hat{a}^T - a^T)\\,x_0)^2 - 2 (\\hat{a}^T - a^T)\\,x_0 \\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right) + \\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\n$$\n$$\nD_T^2 = (\\hat{a}^T - a^T)^2\\,x_0^2 - 2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} x_0 \\epsilon_k + \\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\n$$\nBy linearity of expectation, we can compute the expectation of each term separately:\n$$\n\\mathbb{E}[D_T^2] = \\mathbb{E}\\left[(\\hat{a}^T - a^T)^2\\,x_0^2\\right] - \\mathbb{E}\\left[2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} x_0 \\epsilon_k\\right] + \\mathbb{E}\\left[\\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\\right]\n$$\n\nLet's analyze each of the three terms.\n\nTerm 1: $\\mathbb{E}\\left[(\\hat{a}^T - a^T)^2\\,x_0^2\\right]$\nThe factor $(\\hat{a}^T - a^T)^2$ is a constant. We can pull it out of the expectation:\n$$\n\\mathbb{E}\\left[(\\hat{a}^T - a^T)^2\\,x_0^2\\right] = (\\hat{a}^T - a^T)^2\\,\\mathbb{E}[x_0^2]\n$$\nWe are given that $x_0 \\sim \\mathcal{N}(0, s_0)$, which means $\\mathbb{E}[x_0] = 0$ and the variance is $\\text{Var}(x_0) = s_0$. The variance is defined as $\\text{Var}(x_0) = \\mathbb{E}[x_0^2] - (\\mathbb{E}[x_0])^2$. Since the mean is $0$, we have $\\mathbb{E}[x_0^2] = \\text{Var}(x_0) = s_0$. Therefore, the first term is:\n$$\n(\\hat{a}^T - a^T)^2\\,s_0\n$$\n\nTerm 2: $-\\mathbb{E}\\left[2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} x_0 \\epsilon_k\\right]$\nWe pull out the constants and use linearity of expectation:\n$$\n-2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} \\mathbb{E}[x_0 \\epsilon_k]\n$$\nThe problem states that $x_0$ is independent of $\\{\\epsilon_t\\}_{t \\geq 1}$. For any $k \\in \\{1, \\dots, T\\}$, $x_0$ and $\\epsilon_k$ are independent. For independent random variables, the expectation of their product is the product of their expectations: $\\mathbb{E}[x_0 \\epsilon_k] = \\mathbb{E}[x_0]\\mathbb{E}[\\epsilon_k]$.\nWe know $\\mathbb{E}[x_0] = 0$ and $\\mathbb{E}[\\epsilon_k] = 0$ for all $k$. Thus, $\\mathbb{E}[x_0 \\epsilon_k] = 0 \\times 0 = 0$.\nThe entire cross-term sum is therefore zero.\n\nTerm 3: $\\mathbb{E}\\left[\\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\\right]$\nLet $S_T = \\sum_{k=1}^{T} a^{T-k} \\epsilon_k$. Since $\\mathbb{E}[\\epsilon_k]=0$ for all $k$, the mean of $S_T$ is $\\mathbb{E}[S_T] = \\sum_{k=1}^{T} a^{T-k} \\mathbb{E}[\\epsilon_k] = 0$. The expression we want to compute is thus the variance of $S_T$, $\\text{Var}(S_T) = \\mathbb{E}[S_T^2] - (\\mathbb{E}[S_T])^2 = \\mathbb{E}[S_T^2]$.\nThe innovations $\\{\\epsilon_t\\}$ are i.i.d., which implies they are uncorrelated. For a sum of uncorrelated random variables, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(S_T) = \\text{Var}\\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right) = \\sum_{k=1}^{T} \\text{Var}(a^{T-k} \\epsilon_k)\n$$\nUsing the property $\\text{Var}(cZ) = c^2 \\text{Var}(Z)$, we get:\n$$\n\\sum_{k=1}^{T} (a^{T-k})^2 \\text{Var}(\\epsilon_k) = \\sum_{k=1}^{T} a^{2(T-k)} \\sigma^2 = \\sigma^2 \\sum_{k=1}^{T} a^{2(T-k)}\n$$\nLet's analyze the sum. We can change the index of summation by letting $j = T-k$. When $k=1$, $j=T-1$. When $k=T$, $j=0$. The sum becomes:\n$$\n\\sum_{j=0}^{T-1} a^{2j} = \\sum_{j=0}^{T-1} (a^2)^j\n$$\nThis is a finite geometric series with $T$ terms, first term $1$, and common ratio $r = a^2$. The sum is given by the formula $\\frac{1-r^n}{1-r}$, where $n=T$. This formula is valid for $r \\neq 1$, i.e., $a^2 \\neq 1$.\n$$\n\\sum_{j=0}^{T-1} (a^2)^j = \\frac{1 - (a^2)^T}{1 - a^2} = \\frac{1 - a^{2T}}{1 - a^2}\n$$\nIf $a^2 = 1$, the sum is simply the number of terms, which is $T$. However, the single analytic expression form is usually given by the fraction, which is continuous in the limit $a^2 \\to 1$. Thus, the third term is:\n$$\n\\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}\n$$\n\nFinally, we sum the three terms to get the final expression for the expected squared deviation:\n$$\n\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big] = (\\hat{a}^T - a^T)^2\\,s_0 + 0 + \\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}\n$$\n$$\n\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big] = (\\hat{a}^T - a^T)^2\\,s_0 + \\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}\n$$\nThis expression consists of two parts. The first term, $(\\hat{a}^T - a^T)^2\\,s_0$, represents the error propagation from the initial state due to the mismatch in dynamics between the model ($\\hat{a}$) and the true process ($a$). The second term, $\\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}$, represents the error accumulated from the unmodeled stochastic innovations $\\{\\epsilon_t\\}$ of the true process.",
            "answer": "$$\n\\boxed{(\\hat{a}^{T} - a^{T})^{2} s_{0} + \\sigma^{2} \\frac{1 - a^{2T}}{1 - a^{2}}}\n$$"
        },
        {
            "introduction": "Beyond analyzing pre-existing architectures, a key skill in computational neuroscience is designing networks to perform specific functions. This problem challenges you to engineer a continuous-time RNN (CT-RNN) to approximate a desired temporal filter, a computation fundamental to how cortical circuits process information over time. Through this coding-based exercise, you will implement a practical method for fitting the network's parameters to a target impulse response and validate its performance, bridging the gap between abstract RNN theory and its application as a functional model of neural processing ().",
            "id": "4013906",
            "problem": "Consider the design of a Continuous-Time Recurrent Neural Network (CT-RNN) to approximate a target temporal kernel in the context of cortical sequence modeling. The CT-RNN is modeled as a linear, time-invariant system composed of $m$ leaky integrator units with diagonal dynamics, driven by a scalar input. The goal is to learn an effective impulse response by matching the network’s output to a specified target temporal kernel using a basis of decaying exponentials and then validate the learned impulse response by driving the network with an approximate Dirac impulse.\n\nFundamental base for derivation and design:\n- The Linear Time-Invariant (LTI) system principle: For a causal system, the output $y(t)$ is the convolution of the input $u(t)$ and the impulse response $h(t)$, namely $y(t) = (h * u)(t) = \\int_{0}^{\\infty} h(\\tau) u(t-\\tau) \\, d\\tau$.\n- The impulse response $h(t)$ is defined as the output of the system when driven by a Dirac delta input $u(t) = \\delta(t)$.\n- A CT-RNN with linear dynamics and diagonal connectivity can implement a sum of decaying exponentials as its impulse response. Let the state vector be $x(t) \\in \\mathbb{R}^{m}$, the dynamics be $\\frac{dx_i(t)}{dt} = -\\frac{1}{\\tau_i} x_i(t) + u(t)$ with $\\tau_i  0$, and the output be $y(t) = \\sum_{i=1}^{m} w_i x_i(t)$, where $w_i \\in \\mathbb{R}$ are learned weights. The system’s impulse response is $h(t) = \\sum_{i=1}^{m} w_i e^{-t/\\tau_i}$ for $t \\ge 0$.\n\nDesign task:\n1. You are given target kernels $k(t)$ defined over $t \\in [0, T]$ with $T$ specified. You must approximate $k(t)$ by learning weights $w_i$ such that $h(t) \\approx k(t)$ in the least-squares sense over a uniform grid of time samples. The basis functions are $g_i(t) = e^{-t/\\tau_i}$ with fixed time constants $\\tau_i$ logarithmically spaced between a specified minimum and maximum.\n2. Fit weights $w_i$ by minimizing the regularized objective $\\sum_{j} \\left( \\sum_{i=1}^{m} w_i g_i(t_j) - k(t_j) \\right)^2 + \\lambda \\sum_{i=1}^{m} w_i^2$, yielding the closed-form solution via linear regression.\n3. Validate the learned impulse response by driving the CT-RNN with an approximate Dirac delta $u(t)$ modeled as a square pulse of width $\\delta$ and amplitude $1/\\delta$ (so that the area equals $1$) at time $t=0$. Compute the resulting output $y(t)$ analytically for this input and compare it to $k(t)$ on the same grid.\n4. Compute the relative $\\ell_2$ error $\\varepsilon = \\sqrt{\\frac{\\sum_{j} \\left( y(t_j) - k(t_j) \\right)^2}{\\sum_{j} k(t_j)^2}}$ for each test case.\n\nAll time quantities must be expressed in seconds. Angles inside trigonometric functions must be expressed in radians. The final numerical outputs must be floats. No percentage signs are permitted in any output.\n\nTest suite:\nFor all cases, use a uniform time grid $t_j = j \\cdot dt$ for $j = 0, 1, \\dots, N$ where $N = \\lfloor T/dt \\rfloor$.\n\n- Case 1 (happy path): Target kernel $k(t) = \\frac{t}{\\tau} e^{-t/\\tau}$ for $t \\ge 0$ with $\\tau = 0.05\\,\\text{s}$. Use $T = 1.0\\,\\text{s}$, $dt = 0.001\\,\\text{s}$, $\\delta = dt$, $m = 12$, $\\tau_i$ logarithmically spaced from $0.005\\,\\text{s}$ to $0.2\\,\\text{s}$, and regularization $\\lambda = 10^{-6}$.\n- Case 2 (double-exponential synapse): Target kernel $k(t) = e^{-t/\\tau_d} - e^{-t/\\tau_r}$ with $\\tau_r = 0.01\\,\\text{s}$ and $\\tau_d = 0.10\\,\\text{s}$. Use $T = 1.0\\,\\text{s}$, $dt = 0.001\\,\\text{s}$, $\\delta = dt$, $m = 12$, $\\tau_i$ logarithmically spaced from $0.003\\,\\text{s}$ to $0.3\\,\\text{s}$, and $\\lambda = 10^{-6}$.\n- Case 3 (edge case, oscillatory structure): Target kernel $k(t) = e^{-t/\\tau} \\cos(2 \\pi f t)$ with $\\tau = 0.10\\,\\text{s}$ and $f = 10.0\\,\\text{Hz}$, noting that $2 \\pi f t$ is in radians. Use $T = 1.0\\,\\text{s}$, $dt = 0.001\\,\\text{s}$, $\\delta = dt$, $m = 20$, $\\tau_i$ logarithmically spaced from $0.003\\,\\text{s}$ to $0.5\\,\\text{s}$, and $\\lambda = 10^{-6}$.\n- Case 4 (capacity boundary): Same functional form as Case 2 with $\\tau_r = 0.02\\,\\text{s}$ and $\\tau_d = 0.08\\,\\text{s}$ but with $m = 2$, $\\tau_i$ logarithmically spaced from $0.01\\,\\text{s}$ to $0.10\\,\\text{s}$, $T = 1.0\\,\\text{s}$, $dt = 0.001\\,\\text{s}$, $\\delta = dt$, and $\\lambda = 10^{-6}$.\n\nProgram requirements:\n- Implement the CT-RNN design, least-squares fitting, and impulse validation for each test case.\n- For the analytical validation with $u(t)$ being a square pulse of width $\\delta$ and amplitude $1/\\delta$, the state $x_i(t)$ is:\n  - For $0 \\le t \\le \\delta$, $x_i(t) = \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-t/\\tau_i} \\right)$.\n  - For $t  \\delta$, $x_i(t) = \\left( \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-\\delta/\\tau_i} \\right) \\right) e^{-(t-\\delta)/\\tau_i}$.\n  The output is $y(t) = \\sum_{i=1}^{m} w_i x_i(t)$.\n- Compute the relative $\\ell_2$ error $\\varepsilon$ for each case on the specified grid.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result must be the relative $\\ell_2$ error for the corresponding test case, expressed as a float.",
            "solution": "The user has provided a problem that requires the design and validation of a Continuous-Time Recurrent Neural Network (CT-RNN) for approximating a target temporal kernel. The validation will proceed through the formal steps outlined in the instructions.\n\n### Step 1: Extract Givens\n- **System Dynamics**: The state $x_i(t) \\in \\mathbb{R}$ of the $i$-th leaky integrator unit is governed by $\\frac{dx_i(t)}{dt} = -\\frac{1}{\\tau_i} x_i(t) + u(t)$, where $\\tau_i  0$ is the time constant and $u(t)$ is a scalar input. The system is composed of $m$ such units.\n- **System Output**: The network output is $y(t) = \\sum_{i=1}^{m} w_i x_i(t)$, where $w_i \\in \\mathbb{R}$ are learned weights.\n- **Impulse Response**: The system's impulse response is given as $h(t) = \\sum_{i=1}^{m} w_i e^{-t/\\tau_i}$ for $t \\ge 0$.\n- **Basis Functions**: The basis functions for approximation are $g_i(t) = e^{-t/\\tau_i}$. The time constants $\\tau_i$ are fixed and logarithmically spaced between a specified minimum and maximum.\n- **Objective Function for Weight Fitting**: The weights $w_i$ are found by minimizing the regularized least-squares objective: $\\sum_{j} \\left( \\sum_{i=1}^{m} w_i g_i(t_j) - k(t_j) \\right)^2 + \\lambda \\sum_{i=1}^{m} w_i^2$.\n- **Validation Input**: The network is driven by an approximate Dirac delta, $u(t)$, which is a square pulse of width $\\delta$ and amplitude $1/\\delta$ at $t=0$.\n- **Analytical State Response for Validation**:\n  - For $0 \\le t \\le \\delta$: $x_i(t) = \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-t/\\tau_i} \\right)$.\n  - For $t  \\delta$: $x_i(t) = \\left( \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-\\delta/\\tau_i} \\right) \\right) e^{-(t-\\delta)/\\tau_i}$.\n- **Error Metric**: The relative $\\ell_2$ error is $\\varepsilon = \\sqrt{\\frac{\\sum_{j} \\left( y(t_j) - k(t_j) \\right)^2}{\\sum_{j} k(t_j)^2}}$.\n- **Time Grid**: A uniform grid $t_j = j \\cdot dt$ for $j = 0, 1, \\dots, N$ where $N = \\lfloor T/dt \\rfloor$.\n- **Test Cases**:\n  - **Case 1**: $k(t) = \\frac{t}{\\tau} e^{-t/\\tau}$ with $\\tau = 0.05\\,\\text{s}$. $T = 1.0\\,\\text{s}$, $dt = 0.001\\,\\text{s}$, $\\delta = dt$, $m = 12$, $\\tau_i \\in [0.005, 0.2]\\,\\text{s}$, $\\lambda = 10^{-6}$.\n  - **Case 2**: $k(t) = e^{-t/\\tau_d} - e^{-t/\\tau_r}$ with $\\tau_r = 0.01\\,\\text{s}$, $\\tau_d = 0.10\\,\\text{s}$. $T = 1.0\\,\\text{s}$, $dt = 0.001\\,\\text{s}$, $\\delta = dt$, $m = 12$, $\\tau_i \\in [0.003, 0.3]\\,\\text{s}$, $\\lambda = 10^{-6}$.\n  - **Case 3**: $k(t) = e^{-t/\\tau} \\cos(2 \\pi f t)$ with $\\tau = 0.10\\,\\text{s}$, $f = 10.0\\,\\text{Hz}$. $T = 1.0\\,\\text{s}$, $dt = 0.001\\,\\text{s}$, $\\delta = dt$, $m = 20$, $\\tau_i \\in [0.003, 0.5]\\,\\text{s}$, $\\lambda = 10^{-6}$.\n  - **Case 4**: $k(t) = e^{-t/\\tau_d} - e^{-t/\\tau_r}$ with $\\tau_r = 0.02\\,\\text{s}$, $\\tau_d = 0.08\\,\\text{s}$. $m = 2$, $\\tau_i \\in [0.01, 0.10]\\,\\text{s}$, $T = 1.0\\,\\text{s}$, $dt = 0.001\\,\\text{s}$, $\\delta = dt$, $\\lambda = 10^{-6}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is scientifically sound. The modeling of a CT-RNN as a linear time-invariant system is a standard approach in computational neuroscience. The dynamics are described by first-order linear ordinary differential equations. The use of an exponential basis, least-squares fitting with Tikhonov regularization, and convolution principles are all standard and mathematically correct. The provided analytical solutions for the state response $x_i(t)$ to a square pulse input have been verified and are correct derivations from the governing differential equation.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable, presented with precise mathematical equations and algorithmic steps. It is directly relevant to the specified topic of recurrent neural networks for cortical sequence modeling.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained. All necessary parameters ($T, dt, \\delta, m, \\lambda$, and ranges for $\\tau_i$) and functional forms for the target kernels $k(t)$ are explicitly provided for each of the four test cases. There are no contradictions.\n4.  **Unrealistic or Infeasible**: The physical parameters (time constants, frequencies) are within realistic ranges for biophysical and computational models of neural dynamics. The computational task is feasible.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The least-squares objective function is convex, ensuring a unique solution for the weights $\\mathbf{w}$. The steps are logically structured from system definition, to parameter fitting, to validation.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires implementing a non-trivial numerical procedure involving linear algebra (matrix construction and solving a linear system), numerical evaluation of functions, and simulation of a dynamical system's response. The test cases are well-chosen to probe different aspects of the approximation method (e.g., fitting oscillatory patterns, effects of limited model capacity).\n7.  **Outside Scientific Verifiability**: The results are numerically computable and verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically grounded, well-posed, and complete. A full solution will be provided.\n\n---\n\nThe problem requires us to approximate a target temporal kernel, $k(t)$, using the impulse response of a specific Continuous-Time Recurrent Neural Network (CT-RNN). This involves two main phases: first, fitting the model's parameters to the target, and second, validating the resulting model.\n\n**1. Theoretical Framework**\n\nThe foundation of this problem rests on the principles of Linear Time-Invariant (LTI) systems and function approximation.\n\n- **System Dynamics and Impulse Response**: The state of each of the $m$ neuron units, $x_i(t)$, is governed by the linear differential equation:\n$$ \\frac{dx_i(t)}{dt} = -\\frac{1}{\\tau_i} x_i(t) + u(t) $$\nwhere $\\tau_i  0$ is the time constant of the $i$-th unit and $u(t)$ is a common scalar input. Assuming the system starts from rest, i.e., $x_i(0) = 0$, the solution for an input $u(t) = \\delta(t)$ (a Dirac delta function) is $x_{i, \\text{impulse}}(t) = e^{-t/\\tau_i}$ for $t \\ge 0$. The total network output $y(t)$ is a linear combination of the states, $y(t) = \\sum_{i=1}^{m} w_i x_i(t)$. Consequently, the impulse response of the entire network, $h(t)$, is the weighted sum of the individual unit impulse responses:\n$$ h(t) = \\sum_{i=1}^{m} w_i e^{-t/\\tau_i} $$\nThe functions $g_i(t) = e^{-t/\\tau_i}$ serve as the basis functions for our approximation of the target kernel $k(t)$.\n\n- **Kernel Approximation via Regularized Least-Squares**: To find the optimal weights $w_i$ that make $h(t)$ approximate $k(t)$, we discretize time into a grid of points $t_j$ and minimize a regularized least-squares objective function:\n$$ L(\\mathbf{w}) = \\sum_j \\left( h(t_j) - k(t_j) \\right)^2 + \\lambda \\sum_{i=1}^{m} w_i^2 = \\sum_j \\left( \\sum_{i=1}^{m} w_i g_i(t_j) - k(t_j) \\right)^2 + \\lambda \\sum_{i=1}^{m} w_i^2 $$\nIn matrix notation, this is $\\| \\mathbf{G}\\mathbf{w} - \\mathbf{k} \\|_2^2 + \\lambda \\| \\mathbf{w} \\|_2^2$, where $\\mathbf{G}$ is the design matrix with entries $G_{ji} = g_i(t_j)$, $\\mathbf{w}$ is the vector of weights, and $\\mathbf{k}$ is the vector of target kernel values at times $t_j$. The regularization term $\\lambda \\| \\mathbf{w} \\|_2^2$ stabilizes the solution and prevents overfitting. The closed-form solution that minimizes $L(\\mathbf{w})$ is given by the normal equations for ridge regression:\n$$ \\mathbf{w} = (\\mathbf{G}^T \\mathbf{G} + \\lambda \\mathbf{I})^{-1} \\mathbf{G}^T \\mathbf{k} $$\nwhere $\\mathbf{I}$ is the an identity matrix of size $m \\times m$.\n\n**2. Validation with an Approximate Dirac Impulse**\n\nThe validation step involves driving the CT-RNN, now equipped with the learned weights $w_i$, with an input pulse that approximates a Dirac delta function. This pulse has a duration of $\\delta$ and an amplitude of $1/\\delta$. The analytical response of each unit $x_i(t)$ to this pulse is given and can be derived by solving the state differential equation piece-wise:\n- For $0 \\le t \\le \\delta$, where $u(t) = 1/\\delta$, the solution to $\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + \\frac{1}{\\delta}$ with $x_i(0)=0$ is:\n$$ x_i(t) = \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-t/\\tau_i} \\right) $$\n- For $t  \\delta$, where $u(t) = 0$, the system evolves freely from its state at $t=\\delta$. The solution is:\n$$ x_i(t) = x_i(\\delta) e^{-(t-\\delta)/\\tau_i} = \\left( \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-\\delta/\\tau_i} \\right) \\right) e^{-(t-\\delta)/\\tau_i} $$\nThe total network output $y(t) = \\sum_{i=1}^m w_i x_i(t)$ is then computed using these expressions.\n\n**3. Error Calculation**\n\nThe accuracy of the resulting network is quantified by the relative $\\ell_2$ error, $\\varepsilon$, which compares the network's output $y(t_j)$ to the target kernel $k(t_j)$ over the discrete time grid:\n$$ \\varepsilon = \\sqrt{\\frac{\\sum_{j} (y(t_j) - k(t_j))^2}{\\sum_{j} k(t_j)^2}} $$\n\n**4. Implementation Algorithm**\n\nFor each specified test case, the following procedure is executed:\n1.  **Parameter Initialization**: Define all parameters: the function $k(t)$, time horizon $T$, time step $dt$, pulse width $\\delta$, number of basis functions $m$, the range for time constants $[\\tau_{\\text{min}}, \\tau_{\\text{max}}]$, and the regularization factor $\\lambda$.\n2.  **Grid and Basis Construction**: Create a uniform time grid $t_j$ from $0$ to $T$ and a logarithmically spaced set of time constants $\\tau_i$.\n3.  **Weight Calculation**:\n    a. Construct the design matrix $\\mathbf{G}$, where each column corresponds to a basis function $g_i(t)$ evaluated on the time grid.\n    b. Construct the target vector $\\mathbf{k}$ by evaluating $k(t)$ on the time grid.\n    c. Solve the linear system $(\\mathbf{G}^T \\mathbf{G} + \\lambda \\mathbf{I})\\mathbf{w} = \\mathbf{G}^T \\mathbf{k}$ to find the optimal weight vector $\\mathbf{w}$.\n4.  **Response Simulation and Validation**:\n    a. For each basis $i \\in \\{1, \\dots, m\\}$, compute the analytical state response vector $\\mathbf{x}_i$ to the square pulse input.\n    b. Calculate the total network output vector as the weighted sum $\\mathbf{y} = \\sum_{i=1}^m w_i \\mathbf{x}_i$.\n5.  **Error Computation**: Compute the relative $\\ell_2$ error $\\varepsilon$ between the output vector $\\mathbf{y}$ and the target vector $\\mathbf{k}$.\n\nThis sequence of steps is applied to each of the four test cases to derive the final numerical results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CT-RNN design and validation problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"k_func\": lambda t: (t / 0.05) * np.exp(-t / 0.05) if 0.05 > 0 else np.zeros_like(t),\n            \"T\": 1.0, \"dt\": 0.001, \"delta\": 0.001, \"m\": 12,\n            \"tau_min\": 0.005, \"tau_max\": 0.2, \"lambda_reg\": 1e-6\n        },\n        # Case 2 (double-exponential synapse)\n        {\n            \"k_func\": lambda t: np.exp(-t / 0.10) - np.exp(-t / 0.01),\n            \"T\": 1.0, \"dt\": 0.001, \"delta\": 0.001, \"m\": 12,\n            \"tau_min\": 0.003, \"tau_max\": 0.3, \"lambda_reg\": 1e-6\n        },\n        # Case 3 (edge case, oscillatory structure)\n        {\n            \"k_func\": lambda t: np.exp(-t / 0.10) * np.cos(2 * np.pi * 10.0 * t),\n            \"T\": 1.0, \"dt\": 0.001, \"delta\": 0.001, \"m\": 20,\n            \"tau_min\": 0.003, \"tau_max\": 0.5, \"lambda_reg\": 1e-6\n        },\n        # Case 4 (capacity boundary)\n        {\n            \"k_func\": lambda t: np.exp(-t / 0.08) - np.exp(-t / 0.02),\n            \"T\": 1.0, \"dt\": 0.001, \"delta\": 0.001, \"m\": 2,\n            \"tau_min\": 0.01, \"tau_max\": 0.10, \"lambda_reg\": 1e-6\n        }\n    ]\n\n    results = []\n    \n    def calculate_x_i(t, tau, delta):\n        \"\"\"\n        Calculates the analytical response x_i(t) of a single unit to a square pulse.\n        \"\"\"\n        res = np.zeros_like(t)\n        \n        # for 0 = t = delta\n        mask1 = (t >= 0)  (t = delta)\n        t_part1 = t[mask1]\n        res[mask1] = (tau / delta) * (1 - np.exp(-t_part1 / tau))\n\n        # for t > delta\n        mask2 = t > delta\n        t_part2 = t[mask2]\n        prefactor = (tau / delta) * (1 - np.exp(-delta / tau))\n        res[mask2] = prefactor * np.exp(-(t_part2 - delta) / tau)\n        \n        return res\n\n    for case in test_cases:\n        # 1. Setup\n        T = case[\"T\"]\n        dt = case[\"dt\"]\n        delta = case[\"delta\"]\n        m = case[\"m\"]\n        tau_min = case[\"tau_min\"]\n        tau_max = case[\"tau_max\"]\n        lambda_reg = case[\"lambda_reg\"]\n        k_func = case[\"k_func\"]\n\n        # 2. Discretization and Basis Construction\n        N = int(T / dt)\n        t_grid = np.linspace(0, T, N + 1)\n        tau_basis = np.logspace(np.log10(tau_min), np.log10(tau_max), m)\n\n        # 3. Weight Fitting\n        # Construct design matrix G\n        G = np.exp(-t_grid[:, np.newaxis] / tau_basis[np.newaxis, :])\n        \n        # Construct target vector k\n        k_target = k_func(t_grid)\n        \n        # Solve for weights w using regularized least-squares\n        # w = (G.T @ G + lambda * I)^-1 @ G.T @ k\n        A = G.T @ G + lambda_reg * np.eye(m)\n        b = G.T @ k_target\n        w = np.linalg.solve(A, b)\n\n        # 4. Validation\n        # Compute analytical response y(t)\n        y_response = np.zeros_like(t_grid)\n        for i in range(m):\n            tau_i = tau_basis[i]\n            x_i_response = calculate_x_i(t_grid, tau_i, delta)\n            y_response += w[i] * x_i_response\n\n        # 5. Error Computation\n        numerator = np.sum((y_response - k_target)**2)\n        denominator = np.sum(k_target**2)\n        \n        if denominator == 0:\n            # Handle the unlikely case of a zero target kernel\n            epsilon = 0.0 if numerator == 0.0 else np.inf\n        else:\n            epsilon = np.sqrt(numerator / denominator)\n            \n        results.append(epsilon)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}