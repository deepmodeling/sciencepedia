{
    "hands_on_practices": [
        {
            "introduction": "To begin, we must understand the fundamental mechanics of a Hopfield network. This exercise provides a direct, hands-on experience with the two core components: the Hebbian learning rule used to store patterns in the synaptic weights, and the asynchronous update rule that governs how individual neurons behave. By manually calculating the weight matrix and simulating a single neuron update, you will build a foundational intuition for how information is encoded and processed at the most basic level of the network. ",
            "id": "3962571",
            "problem": "Consider a binary associative memory modeled by a Hopfield network in which each neuron takes states $s_i \\in \\{-1,+1\\}$ and updates asynchronously according to the sign of its local field. Let there be $N=4$ neurons, two stored memory patterns $\\xi^{1}=(1,1,-1,-1)$ and $\\xi^{2}=(1,-1,1,-1)$, and thresholds $\\theta_i=0$ for all $i$. The synaptic weights use the standard Hebbian learning rule with no self-coupling, namely $w_{ij}=\\frac{1}{N}\\sum_{\\mu=1}^{2}\\xi_i^{\\mu}\\xi_j^{\\mu}$ for $i\\neq j$ and $w_{ii}=0$. The asynchronous deterministic update rule is: compute the local field $h_i(s)=\\sum_{j=1}^{N}w_{ij}s_j-\\theta_i$ and set $s_i \\leftarrow \\operatorname{sign}(h_i(s))$, with the convention that if $h_i(s)=0$ then $s_i$ remains unchanged.\n\nStarting from the network state $s=(1,-1,-1,-1)$, perform the following tasks:\n- Compute the full $4\\times 4$ weight matrix $W=(w_{ij})$.\n- Perform a single asynchronous update of neuron $3$ using the above rule.\n\nReport the updated value of neuron $3$ as your final numeric answer, expressed as either $-1$ or $+1$. No rounding is required and no physical units apply.",
            "solution": "The problem provides a complete and valid description of a standard Hopfield network. All parameters and rules are well-defined. We can proceed with the solution.\n\nThe problem asks for two tasks: first, to compute the weight matrix $W$, and second, to perform a single asynchronous update for neuron $3$ starting from a given initial state.\n\nThe network has $N=4$ neurons with states $s_i \\in \\{-1, +1\\}$.\nThe two stored memory patterns are:\n$$\n\\xi^{1} = (1, 1, -1, -1)\n$$\n$$\n\\xi^{2} = (1, -1, 1, -1)\n$$\nThe synaptic weights are determined by the Hebbian learning rule with no self-coupling ($w_{ii}=0$):\n$$\nw_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^{2} \\xi_i^{\\mu} \\xi_j^{\\mu} \\quad \\text{for } i \\neq j\n$$\n\n**Part 1: Computation of the Weight Matrix $W$**\n\nWith $N=4$, the formula for the weights is $w_{ij} = \\frac{1}{4} (\\xi_i^1 \\xi_j^1 + \\xi_i^2 \\xi_j^2)$ for $i \\neq j$. The diagonal elements $w_{ii}$ are all $0$. We compute the off-diagonal elements:\n\n$w_{12} = \\frac{1}{4} [(\\xi_1^1 \\xi_2^1) + (\\xi_1^2 \\xi_2^2)] = \\frac{1}{4} [(1)(1) + (1)(-1)] = \\frac{1}{4} [1-1] = 0$.\n$w_{13} = \\frac{1}{4} [(\\xi_1^1 \\xi_3^1) + (\\xi_1^2 \\xi_3^2)] = \\frac{1}{4} [(1)(-1) + (1)(1)] = \\frac{1}{4} [-1+1] = 0$.\n$w_{14} = \\frac{1}{4} [(\\xi_1^1 \\xi_4^1) + (\\xi_1^2 \\xi_4^2)] = \\frac{1}{4} [(1)(-1) + (1)(-1)] = \\frac{1}{4} [-1-1] = -\\frac{2}{4} = -\\frac{1}{2}$.\n$w_{23} = \\frac{1}{4} [(\\xi_2^1 \\xi_3^1) + (\\xi_2^2 \\xi_3^2)] = \\frac{1}{4} [(1)(-1) + (-1)(1)] = \\frac{1}{4} [-1-1] = -\\frac{2}{4} = -\\frac{1}{2}$.\n$w_{24} = \\frac{1}{4} [(\\xi_2^1 \\xi_4^1) + (\\xi_2^2 \\xi_4^2)] = \\frac{1}{4} [(1)(-1) + (-1)(-1)] = \\frac{1}{4} [-1+1] = 0$.\n$w_{34} = \\frac{1}{4} [(\\xi_3^1 \\xi_4^1) + (\\xi_3^2 \\xi_4^2)] = \\frac{1}{4} [(-1)(-1) + (1)(-1)] = \\frac{1}{4} [1-1] = 0$.\n\nSince the weight matrix must be symmetric ($w_{ij} = w_{ji}$), we have all the components. The full $4 \\times 4$ weight matrix $W$ is:\n$$\nW =\n\\begin{pmatrix}\nw_{11} & w_{12} & w_{13} & w_{14} \\\\\nw_{21} & w_{22} & w_{23} & w_{24} \\\\\nw_{31} & w_{32} & w_{33} & w_{34} \\\\\nw_{41} & w_{42} & w_{43} & w_{44}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & 0 & 0 & -\\frac{1}{2} \\\\\n0 & 0 & -\\frac{1}{2} & 0 \\\\\n0 & -\\frac{1}{2} & 0 & 0 \\\\\n-\\frac{1}{2} & 0 & 0 & 0\n\\end{pmatrix}\n$$\n\n**Part 2: Asynchronous Update of Neuron 3**\n\nThe initial state of the network is given as $s = (s_1, s_2, s_3, s_4) = (1, -1, -1, -1)$.\nWe need to update neuron $i=3$. The update rule is $s_3 \\leftarrow \\operatorname{sign}(h_3(s))$, where $h_3(s)$ is the local field of neuron $3$.\nThe thresholds are given as $\\theta_i = 0$ for all $i$. The local field is calculated as:\n$$\nh_i(s) = \\sum_{j=1}^{N} w_{ij}s_j - \\theta_i\n$$\nFor neuron $3$, this becomes:\n$$\nh_3(s) = \\sum_{j=1}^{4} w_{3j}s_j - \\theta_3 = w_{31}s_1 + w_{32}s_2 + w_{33}s_3 + w_{34}s_4 - 0\n$$\nSubstituting the values of the weights from the third row of matrix $W$ and the components of the initial state vector $s$:\n$w_{31} = 0$, $w_{32} = -\\frac{1}{2}$, $w_{33} = 0$, $w_{34} = 0$.\n$s_1 = 1$, $s_2 = -1$, $s_3 = -1$, $s_4 = -1$.\n\nThe local field is:\n$$\nh_3(s) = (0)(1) + \\left(-\\frac{1}{2}\\right)(-1) + (0)(-1) + (0)(-1) = 0 + \\frac{1}{2} + 0 + 0 = \\frac{1}{2}\n$$\nThe updated state for neuron $3$, let's call it $s_3'$, is given by the sign of the local field:\n$$\ns_3' = \\operatorname{sign}(h_3(s)) = \\operatorname{sign}\\left(\\frac{1}{2}\\right)\n$$\nSince $\\frac{1}{2}$ is a positive number, its sign is $+1$.\n$$\ns_3' = 1\n$$\nThe problem states that if $h_i(s)=0$, the neuron state remains unchanged. This is not the case here, as $h_3(s) = \\frac{1}{2} \\neq 0$.\nThus, the updated value of neuron $3$ is $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "With the basic update rule established, we now explore the principle that guides the network's collective behavior. The dynamics of a Hopfield network are not random; they are governed by a global energy function, where stored patterns act as attractors, or minima, in an 'energy landscape'. This exercise will allow you to explicitly demonstrate this principle by calculating the change in network energy following a single asynchronous update, confirming that the network spontaneously evolves to minimize its energy. ",
            "id": "3962539",
            "problem": "Consider a fully connected Hopfield network with three binary neurons having states $s_{i} \\in \\{-1,+1\\}$ and symmetric synaptic weights $w_{ij} = w_{ji}$ with zero self-couplings $w_{ii}=0$. The network energy is defined by the standard Hopfield energy function\n$$\nE(\\mathbf{s}) \\;=\\; -\\frac{1}{2}\\sum_{i=1}^{3}\\sum_{j=1}^{3} w_{ij}\\, s_i s_j \\;+\\; \\sum_{i=1}^{3} \\theta_i\\, s_i,\n$$\nwhere $\\theta_i$ are fixed thresholds. The asynchronous update rule is the deterministic sign rule\n$$\ns_i \\leftarrow \\operatorname{sgn}\\!\\Big(\\sum_{j=1}^{3} w_{ij}\\, s_j \\;-\\; \\theta_i\\Big),\n$$\nwith $\\operatorname{sgn}(x)=+1$ if $x>0$ and $\\operatorname{sgn}(x)=-1$ if $x<0$.\n\nLet $\\theta_i=0$ for all $i$, and let the nonzero weights be $w_{12}=w_{21}=1$, $w_{13}=w_{31}=-1$, $w_{23}=w_{32}=1$. Start from the state $\\mathbf{s}=(s_1,s_2,s_3)=(1,-1,1)$. Perform a single asynchronous update of neuron $1$ only, according to the deterministic sign rule, keeping neurons $2$ and $3$ fixed during this update.\n\nUsing only the definitions given above, explicitly demonstrate that this asynchronous update reduces the network energy and compute the resulting new state $\\mathbf{s}'$ and its energy $E(\\mathbf{s}')$. Provide the final answer as the four-entry row matrix $\\begin{pmatrix} s_1' & s_2' & s_3' & E(\\mathbf{s}') \\end{pmatrix}$. No rounding is required.",
            "solution": "The problem asks us to analyze a single asynchronous update in a 3-neuron Hopfield network. The validation process confirms that the problem is well-posed, scientifically grounded, and provides all necessary information. We may proceed with the solution.\n\nThe network consists of three neurons with states $s_i \\in \\{-1, +1\\}$. The energy function is given by:\n$$\nE(\\mathbf{s}) = -\\frac{1}{2}\\sum_{i=1}^{3}\\sum_{j=1}^{3} w_{ij}\\, s_i s_j \\;+\\; \\sum_{i=1}^{3} \\theta_i\\, s_i\n$$\nThe problem specifies that the thresholds are zero, $\\theta_i = 0$ for all $i$. The energy function simplifies to:\n$$\nE(\\mathbf{s}) = -\\frac{1}{2}\\sum_{i=1}^{3}\\sum_{j=1}^{3} w_{ij}\\, s_i s_j\n$$\nGiven the conditions of symmetric weights ($w_{ij} = w_{ji}$) and zero self-couplings ($w_{ii}=0$), we can expand the double summation for the 3-neuron case as:\n$$\nE(\\mathbf{s}) = -\\frac{1}{2} (w_{12}s_1s_2 + w_{13}s_1s_3 + w_{21}s_2s_1 + w_{23}s_2s_3 + w_{31}s_3s_1 + w_{32}s_3s_2)\n$$\n$$\nE(\\mathbf{s}) = -\\frac{1}{2} (2 w_{12}s_1s_2 + 2 w_{13}s_1s_3 + 2 w_{23}s_2s_3)\n$$\n$$\nE(\\mathbf{s}) = -(w_{12}s_1s_2 + w_{13}s_1s_3 + w_{23}s_2s_3)\n$$\nThe given nonzero weights are $w_{12}=1$, $w_{13}=-1$, and $w_{23}=1$. The initial state of the network is $\\mathbf{s} = (s_1, s_2, s_3) = (1, -1, 1)$.\n\nFirst, we compute the energy of the initial state, $E(\\mathbf{s})$:\n$$\nE(\\mathbf{s}) = -((1)(1)(-1) + (-1)(1)(1) + (1)(-1)(1))\n$$\n$$\nE(\\mathbf{s}) = -(-1 - 1 - 1) = -(-3) = 3\n$$\nSo, the initial energy is $E(\\mathbf{s}) = 3$.\n\nNext, we perform a single asynchronous update of neuron $1$. The update rule is given by:\n$$\ns_i \\leftarrow \\operatorname{sgn}\\!\\Big(\\sum_{j=1}^{3} w_{ij}\\, s_j \\;-\\; \\theta_i\\Big)\n$$\nFor neuron $1$ and $\\theta_1=0$, this becomes:\n$$\ns_1' = \\operatorname{sgn}\\!\\Big(\\sum_{j=1}^{3} w_{1j}\\, s_j\\Big) = \\operatorname{sgn}(w_{11}s_1 + w_{12}s_2 + w_{13}s_3)\n$$\nSince $w_{11}=0$, we calculate the local field $h_1$ acting on neuron $1$:\n$$\nh_1 = w_{12}s_2 + w_{13}s_3\n$$\nUsing the current state values $s_2 = -1$ and $s_3 = 1$, and the weights $w_{12}=1$, $w_{13}=-1$:\n$$\nh_1 = (1)(-1) + (-1)(1) = -1 - 1 = -2\n$$\nThe new state of neuron $1$, $s_1'$, is then:\n$$\ns_1' = \\operatorname{sgn}(-2) = -1\n$$\nThe other neurons remain unchanged in this asynchronous update step, so $s_2' = s_2 = -1$ and $s_3' = s_3 = 1$. The new state of the network is $\\mathbf{s}' = (s_1', s_2', s_3') = (-1, -1, 1)$.\n\nNow, we compute the energy of this new state, $E(\\mathbf{s}')$:\n$$\nE(\\mathbf{s}') = -(w_{12}s_1's_2' + w_{13}s_1's_3' + w_{23}s_2's_3')\n$$\nSubstituting the values for the new state $\\mathbf{s}' = (-1, -1, 1)$:\n$$\nE(\\mathbf{s}') = -((1)(-1)(-1) + (-1)(-1)(1) + (1)(-1)(1))\n$$\n$$\nE(\\mathbf{s}') = - (1 + 1 - 1) = -1\n$$\nThe energy of the new state is $E(\\mathbf{s}') = -1$.\n\nTo explicitly demonstrate that the update reduces the network energy, we compare the initial and final energies:\nInitial energy: $E(\\mathbf{s}) = 3$.\nFinal energy: $E(\\mathbf{s}') = -1$.\nSince $-1 < 3$, we have $E(\\mathbf{s}') < E(\\mathbf{s})$, which confirms that the deterministic asynchronous update has decreased the network energy, as expected for a Hopfield network. The change in energy is $\\Delta E = E(\\mathbf{s}') - E(\\mathbf{s}) = -1 - 3 = -4$.\n\nThe problem asks for the four-entry row matrix $\\begin{pmatrix} s_1' & s_2' & s_3' & E(\\mathbf{s}') \\end{pmatrix}$.\nThe components are:\n$s_1' = -1$\n$s_2' = -1$\n$s_3' = 1$\n$E(\\mathbf{s}') = -1$\nAssembling these into the final matrix form gives the answer.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-1 & -1 & 1 & -1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from microscopic rules to macroscopic performance, we now examine how the network functions as a whole during memory retrieval. This computational practice uses the concept of 'overlap'—a measure of how similar the network's current state is to a target memory—to simulate the full retrieval process over time. By iterating a map derived from statistical physics approximations, you will observe how the network can start from a noisy or corrupted cue and dynamically 'clean it up' to perfectly recall the stored pattern. ",
            "id": "3962552",
            "problem": "Consider a Hopfield Network (HN) with $N=1000$ binary neurons storing $p=50$ independent random binary patterns via Hebbian learning. Let the binary state of neuron $i$ at asynchronous sweep $t$ be $s_i(t) \\in \\{-1, +1\\}$, and let the target pattern be $\\boldsymbol{\\xi}^1 \\in \\{-1, +1\\}^N$. Define the macroscopic overlap with the target pattern at sweep $t$ as $m_t = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_i^1 s_i(t)$. Assume zero-temperature deterministic asynchronous dynamics with the standard sign update rule. Under the Central Limit Theorem (CLT) approximation for crosstalk noise in the large-$N$ limit, the evolution of the overlap is given by the iterative mapping\n$$\nm_{t+1} = \\operatorname{erf}\\!\\left(\\frac{m_t}{\\sqrt{2 \\alpha}}\\right),\n$$\nwhere $\\alpha = \\frac{p}{N}$ is the load and $\\operatorname{erf}(\\cdot)$ is the Gaussian error function.\n\nYou are given an initial corruption level $\\rho \\in [0,1]$, meaning a fraction $\\rho$ of the bits of $\\boldsymbol{\\xi}^1$ are flipped to initialize $s_i(0)$, so that the initial overlap is $m_0 = 1 - 2\\rho$. One full asynchronous sweep corresponds to one iteration of the mapping above.\n\nTask:\n- For each specified $\\rho$, starting from $m_0 = 1 - 2\\rho$, iterate the mapping $m_{t+1} = \\operatorname{erf}\\big(m_t / \\sqrt{2 \\alpha}\\big)$ with $\\alpha = p/N$ until the overlap exceeds the target threshold $m_t > 0.9$, and report the number of asynchronous sweeps $t$ required to reach this condition.\n- If $m_0 \\leq 0$, or if $m_t$ never exceeds $0.9$ within a maximum of $T_{\\max} = 1000$ sweeps, report $-1$ for that case.\n- Use $N=1000$ and $p=50$ to compute $\\alpha = p/N$.\n\nTest Suite:\n- Use the following set of initial corruption levels $\\rho$: $\\{0, 0.05, 0.1, 0.2, 0.3, 0.495, 0.51\\}$.\n\nAnswer Type:\n- For each $\\rho$, the answer must be an integer equal to the number of asynchronous sweeps needed to achieve $m_t > 0.9$, or $-1$ if the threshold is not reached within $T_{\\max} = 1000$ sweeps.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite for $\\rho$. For example, an output should look like $[a_1,a_2,a_3,a_4,a_5,a_6,a_7]$, where each $a_k$ is an integer result corresponding to the $k$-th $\\rho$ in the test suite.",
            "solution": "The problem requires us to determine the number of asynchronous sweeps a Hopfield Network (HN) needs to retrieve a target pattern, starting from a corrupted version of it. The dynamics are modeled by a well-established iterative map for the macroscopic overlap, derived from a mean-field approximation of the network's behavior.\n\n### Principle-Based Design\n\n1.  **System Definition**: The system is a Hopfield Network of $N=1000$ binary neurons, $s_i \\in \\{-1, +1\\}$, designed to store $p=50$ random binary patterns $\\boldsymbol{\\xi}^\\mu$.\n\n2.  **Macroscopic Order Parameter**: The state of the entire network at a given time is not tracked neuron by neuron. Instead, it is characterized by a single macroscopic variable called the overlap, $m_t$. The overlap measures the similarity between the current network state $\\boldsymbol{s}(t)$ and a specific target pattern, which we label $\\boldsymbol{\\xi}^1$. It is defined as the average of the product of corresponding neuron states:\n    $$\n    m_t = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_i^1 s_i(t)\n    $$\n    An overlap of $m_t=1$ means the network state is identical to the target pattern, $m_t=-1$ means it is the exact negative, and $m_t=0$ implies no correlation.\n\n3.  **Initial State and Corruption**: The network is initialized in a state that is a corrupted version of the target pattern $\\boldsymbol{\\xi}^1$. A fraction $\\rho$ of its neurons have their states flipped. If a neuron $i$ has its state $s_i$ flipped from $\\xi_i^1$ to $-\\xi_i^1$, its contribution $\\xi_i^1 s_i$ to the overlap sum changes from $+1$ to $-1$, a net change of $-2$. With a fraction $\\rho$ of $N$ neurons flipped, the total overlap sum decreases by $N \\rho \\times 2$. The average overlap, therefore, decreases from its perfect value of $1$ by $2\\rho$. This gives the initial overlap:\n    $$\n    m_0 = 1 - 2\\rho\n    $$\n\n4.  **Dynamics of Recall (Mean-Field Approximation)**: The evolution of the network state is governed by an asynchronous update rule, where each neuron's state is updated based on its local field. In the limit of a large number of neurons ($N \\to \\infty$), the collective effect of the other stored patterns ($\\mu = 2, \\dots, p$) on the local field of a neuron can be approximated as Gaussian noise (the \"crosstalk noise\"), a consequence of the Central Limit Theorem. The variance of this noise is proportional to the network load, $\\alpha = p/N$.\n\n    Under this approximation, the evolution of the macroscopic overlap from one full asynchronous sweep ($t$) to the next ($t+1$) is not random but follows a deterministic iterative map:\n    $$\n    m_{t+1} = \\operatorname{erf}\\!\\left(\\frac{m_t}{\\sqrt{2 \\alpha}}\\right)\n    $$\n    Here, $\\operatorname{erf}(\\cdot)$ is the Gaussian error function, which arises from integrating the Gaussian noise distribution to find the probability of a neuron aligning correctly with the target pattern. The argument of the `erf` function represents a signal-to-noise ratio, where $m_t$ is the signal strength and $\\sqrt{\\alpha}$ is proportional to the noise standard deviation.\n\n5.  **Algorithmic Solution**: The task is to find the number of sweeps $t$ required for the overlap $m_t$ to exceed a threshold of $0.9$. This can be solved by direct iteration of the map.\n\n    - First, we calculate the constant parameters: the load $\\alpha = p/N = 50/1000 = 0.05$. The term in the denominator of the `erf` argument is $\\sqrt{2\\alpha} = \\sqrt{2 \\times 0.05} = \\sqrt{0.1}$.\n\n    - For each given initial corruption level $\\rho$ from the test suite $\\{0, 0.05, 0.1, 0.2, 0.3, 0.495, 0.51\\}$, we compute the initial overlap $m_0 = 1 - 2\\rho$.\n\n    - We then apply the simulation logic:\n        a. If $m_0 \\leq 0$, retrieval is impossible as the initial state has no positive correlation (or is anti-correlated) with the target. The dynamics will drive the overlap towards $0$ or a negative fixed point. In this case, we report $-1$ as per the problem's rules.\n        b. If $m_0 > 0$, we start iterating. We initialize a sweep counter $t=0$.\n        c. We check the condition $m_t > 0.9$.\n        d. If the condition is met for the current $t$, we record $t$ as the result and stop. The check must be performed starting at $t=0$ for the initial state $m_0$.\n        e. If the condition is not met, and we have not exceeded the maximum number of sweeps ($t < T_{\\max}=1000$), we calculate the next overlap $m_{t+1} = \\operatorname{erf}(m_t / \\sqrt{2\\alpha})$ and increment the sweep counter $t \\to t+1$.\n        f. If the loop completes up to $T_{\\max}$ sweeps without the overlap exceeding $0.9$, we report $-1$. This means checking states $m_0, m_1, \\dots, m_{1000}$.\n\nThis procedure is implemented for each value of $\\rho$ in the test suite to generate the final list of results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Calculates the number of sweeps required for a Hopfield Network's overlap\n    to exceed a threshold, based on an iterative map derived from mean-field theory.\n    \"\"\"\n    \n    # Define the parameters from the problem statement.\n    N = 1000\n    p = 50\n    T_max = 1000\n    m_threshold = 0.9\n    \n    # Define the test suite of initial corruption levels rho.\n    test_cases = [0, 0.05, 0.1, 0.2, 0.3, 0.495, 0.51]\n    \n    # Calculate the network load alpha\n    alpha = p / N\n    \n    # Pre-calculate the constant denominator for the erf argument\n    # for improved efficiency inside the loop.\n    denominator = np.sqrt(2 * alpha)\n\n    results = []\n    for rho in test_cases:\n        # Calculate the initial overlap m_0 from the corruption level rho.\n        m_0 = 1 - 2 * rho\n        \n        # If the initial overlap is non-positive, retrieval fails immediately.\n        if m_0 <= 0:\n            results.append(-1)\n            continue\n            \n        m = m_0\n        converged = False\n        \n        # Iterate through sweeps from t=0 up to T_max.\n        # The loop runs T_max + 1 times to check states m_0, m_1, ..., m_T_max.\n        for t in range(T_max + 1):\n            # Check if the overlap exceeds the target threshold.\n            if m > m_threshold:\n                results.append(t)\n                converged = True\n                break\n            \n            # Update the overlap for the next sweep using the iterative map.\n            # This update is not performed on the last check (t=T_max).\n            if t < T_max:\n                m = erf(m / denominator)\n        \n        # If the loop completes without convergence, report failure.\n        if not converged:\n            results.append(-1)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}