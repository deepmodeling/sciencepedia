{
    "hands_on_practices": [
        {
            "introduction": "纯粹的赫布学习本质上是不稳定的，会导致突触权重无界增长。一个直观的稳定化方法是在每次更新后对权重向量进行归一化。这个练习将通过数学推导，揭示这个简单的“赫布增量后归一化”过程如何在连续时间极限下，等价于著名的Oja法则，从而为理解突触稳定性的基本机制提供坚实的基础。",
            "id": "3987622",
            "problem": "考虑一个单个线性速率神经元，它接收输入向量 $x \\in \\mathbb{R}^{n}$，其突触权重向量为 $w \\in \\mathbb{R}^{n}$。该神经元的发放率为 $y = w^{\\top} x$。在离散时间内，应用一个步长为 $\\eta > 0$ 的标准赫布增量，并紧接着进行乘性权重归一化至单位范数。也就是说，在一个学习步骤中：\n- 首先，计算赫布预更新 $w' = w + \\eta\\,y\\,x$。\n- 然后，强制单位范数 $w^{+} = \\dfrac{w'}{\\|w'\\|}$。\n假设在步骤开始时，权重向量已被归一化，即 $\\|w\\| = 1$。对于标量变量的光滑函数，你可以使用关于 $0$ 的一阶泰勒展开，包括 $\\sqrt{1+\\epsilon}$ 和 $(1+\\epsilon)^{-1}$。\n\n从欧几里得范数和一阶泰勒展开的基本定义出发，推导由“赫布更新后归一化”引起的权重变化 $w^{+} - w$ 的领先阶小步长近似（到 $\\eta$ 的一阶），并用 $\\eta$、$y$、$x$ 和 $w$ 表示，给定 $y = w^{\\top} x$。该领先阶表达式是在基于速率的公式中通常与 Oja 法则相关联的连续时间极限漂移。\n\n答案格式要求：\n- 仅提供关于 $\\eta$、$y$、$x$ 和 $w$ 的单个闭式解析表达式作为一阶项。\n- 最终答案中不应包含等号。\n- 无需四舍五入，也不涉及单位。",
            "solution": "本任务是为小的学习步长 $\\eta$ 推导权重变化 $w^{+} - w$ 的领先阶近似。更新规则是赫布增量后跟归一化。\n\n权重变化由 $\\Delta w = w^{+} - w$ 给出。更新后的权重向量 $w^{+}$ 定义为：\n$$ w^{+} = \\frac{w'}{\\|w'\\|} $$\n其中 $w'$ 是赫布预更新的结果：\n$$ w' = w + \\eta y x $$\n此处，$y = w^{\\top} x$ 是神经元的发放率，$x \\in \\mathbb{R}^{n}$ 是输入向量，$w \\in \\mathbb{R}^{n}$ 是权重向量，$\\eta > 0$ 是学习率。我们给定的初始条件是 $\\|w\\| = 1$。\n\n将 $w'$ 的表达式代入 $w^{+}$ 的方程中，得到：\n$$ w^{+} = \\frac{w + \\eta y x}{\\|w + \\eta y x\\|} $$\n为了推导关于 $\\eta$ 的一阶近似，我们必须分析分母中的项 $\\|w + \\eta y x\\|$。我们从展开它的平方开始：\n$$ \\|w + \\eta y x\\|^2 = (w + \\eta y x)^{\\top}(w + \\eta y x) $$\n使用转置和矩阵乘法的分配律：\n$$ \\|w + \\eta y x\\|^2 = (w^{\\top} + \\eta y x^{\\top})(w + \\eta y x) = w^{\\top}w + w^{\\top}(\\eta y x) + (\\eta y x^{\\top})w + (\\eta y x^{\\top})(\\eta y x) $$\n$$ \\|w + \\eta y x\\|^2 = \\|w\\|^2 + \\eta y (w^{\\top}x) + \\eta y (x^{\\top}w) + \\eta^2 y^2 (x^{\\top}x) $$\n由于向量点积是对称的，因此 $x^{\\top}w = w^{\\top}x$。给定 $\\|w\\| = 1$ 且 $y = w^{\\top}x$。将这些代入方程得到：\n$$ \\|w + \\eta y x\\|^2 = 1 + \\eta y (y) + \\eta y (y) + \\eta^2 y^2 \\|x\\|^2 $$\n$$ \\|w + \\eta y x\\|^2 = 1 + 2\\eta y^2 + \\eta^2 y^2 \\|x\\|^2 $$\n现在我们取平方根来求 $\\|w + \\eta y x\\|$：\n$$ \\|w + \\eta y x\\| = \\sqrt{1 + 2\\eta y^2 + \\eta^2 y^2 \\|x\\|^2} $$\n题目允许对小的 $\\eta$ 使用一阶泰勒展开。我们使用对于小的 $\\epsilon$ 的展开式 $\\sqrt{1+\\epsilon} \\approx 1 + \\frac{1}{2}\\epsilon$。在我们的例子中，项 $\\epsilon = 2\\eta y^2 + \\eta^2 y^2 \\|x\\|^2$。到 $\\eta$ 的一阶，我们只需要考虑与 $\\eta$ 成正比的项，所以我们可以近似 $\\epsilon \\approx 2\\eta y^2$。\n$$ \\|w + \\eta y x\\| \\approx \\sqrt{1 + 2\\eta y^2} \\approx 1 + \\frac{1}{2}(2\\eta y^2) = 1 + \\eta y^2 $$\n我们忽略了 $\\eta^2$ 阶及更高阶的项。\n\n接下来，我们需要求这个范数的倒数，它在 $w^{+}$ 的表达式中作为乘法因子出现。\n$$ \\frac{1}{\\|w + \\eta y x\\|} \\approx \\frac{1}{1 + \\eta y^2} $$\n使用对于小的 $\\delta$ 的泰勒展开式 $(1+\\delta)^{-1} \\approx 1 - \\delta$，其中 $\\delta = \\eta y^2$，我们得到：\n$$ \\frac{1}{\\|w + \\eta y x\\|} \\approx 1 - \\eta y^2 $$\n现在我们将这个近似值代回 $w^{+}$ 的方程中：\n$$ w^{+} \\approx (w + \\eta y x)(1 - \\eta y^2) $$\n展开这个乘积：\n$$ w^{+} \\approx w(1 - \\eta y^2) + \\eta y x(1 - \\eta y^2) $$\n$$ w^{+} \\approx w - \\eta y^2 w + \\eta y x - \\eta^2 y^3 x $$\n我们在寻找领先阶近似，这意味着我们保留到 $\\eta$ 一阶的项，并舍弃所有更高阶的项（即含有 $\\eta^2, \\eta^3, \\dots$ 的项）。\n$$ w^{+} \\approx w + \\eta y x - \\eta y^2 w $$\n权重变化为 $\\Delta w = w^{+} - w$。使用我们对 $w^{+}$ 的近似：\n$$ \\Delta w \\approx (w + \\eta y x - \\eta y^2 w) - w $$\n$$ \\Delta w \\approx \\eta y x - \\eta y^2 w $$\n这个表达式可以因式分解，得到一阶权重变化的最终形式：\n$$ \\Delta w \\approx \\eta(y x - y^2 w) $$\n这个结果也通常写作：\n$$ \\Delta w \\approx \\eta y(x - y w) $$\n其中 $y = w^{\\top}x$。这就是 Oja 法则的数学表达式。问题要求的是权重变化 $w^{+} - w$ 的一阶项，即这个表达式。",
            "answer": "$$\n\\boxed{\\eta y (x - y w)}\n$$"
        },
        {
            "introduction": "除了乘性归一化，神经元还可以通过其他机制实现权重稳定，例如线性衰减和输出饱和的组合。本练习探讨了这种稳定化方案，要求你应用平均场理论来分析一个具有非线性激活函数的神经元的动态。通过求解其非平凡不动点，你将精确地看到系统的最终权重是如何由输入信号的统计特性、学习率和衰减率共同决定的。",
            "id": "3987647",
            "problem": "考虑一个突触后速率神经元，其活动由 $y=\\phi(w^{\\top} x)$ 建模，其中 $w \\in \\mathbb{R}^{n}$ 是突触权重向量，$x \\in \\mathbb{R}^{n}$ 是一个零均值随机输入，$\\phi(\\cdot)$ 是一个光滑的奇饱和非线性函数。突触动力学遵循一个结合了赫布学习（Hebbian learning）和权重衰减的规则\n$$\n\\dot{w}=\\eta\\,y\\,x-\\gamma\\,w,\n$$\n其中 $\\eta>0$ 是学习率，$\\gamma>0$ 是衰减系数。假设输入是平稳的高斯分布，其协方差矩阵 $\\Sigma\\in\\mathbb{R}^{n\\times n}$ 是对称正定（SPD）的，即 $x\\sim\\mathcal{N}(0,\\Sigma)$，并且在工作区域内，该饱和非线性函数可以很好地由截断的奇次三次函数近似\n$$\n\\phi(u)=u-\\alpha u^{3},\n$$\n其中 $\\alpha>0$ 表征饱和强度。在平均场极限下进行分析，此时可以对 $w$ 的动力学方程关于输入分布进行平均是合理的。\n\n从赫布学习和速率模型的基本定义出发，推导关于 $w$ 和输入协方差矩阵 $\\Sigma$ 的预期突触更新的平均场动力学方程。然后，在假设 $w$ 与 $\\Sigma$ 的主特征向量 $v_{1}$（与最大特征值 $\\lambda_{1}>0$ 相关联）对齐的情况下，计算 $w^{\\ast}=a^{\\ast}v_{1}$ 中的非平凡不动点振幅 $a^{\\ast}>0$。明确展示衰减系数 $\\gamma$ 如何通过输入统计量（特别是 $\\lambda_{1}$）和饱和参数 $\\alpha$ 来设定稳态范数。你的最终答案应该是关于 $\\eta$、$\\gamma$、$\\alpha$ 和 $\\lambda_{1}$ 的 $a^{\\ast}$ 的闭式解析表达式。不需要进行数值评估，最终表达式中也不应包含任何单位。",
            "solution": "用户要求我解决一个关于速率神经元模型中赫布学习的计算神经科学问题。\n\n### 步骤1：问题验证 - 提取给定条件\n\n-   **神经元模型**：单个突触后速率神经元。\n-   **活动规则**：$y = \\phi(w^{\\top} x)$，其中 $y$ 是突触后活动。\n-   **突触权重向量**：$w \\in \\mathbb{R}^{n}$。\n-   **输入向量**：$x \\in \\mathbb{R}^{n}$，一个随机向量。\n-   **输入统计特性**：$x$ 是平稳的、零均值的高斯分布，$x \\sim \\mathcal{N}(0, \\Sigma)$。\n-   **输入协方差**：$\\Sigma \\in \\mathbb{R}^{n \\times n}$ 是对称正定（SPD）的。\n-   **激活函数**：$\\phi(u)$ 是一个光滑的奇饱和非线性函数，由截断的三次函数 $\\phi(u) = u - \\alpha u^{3}$ 近似，其中 $\\alpha > 0$。\n-   **突触可塑性规则**：$\\dot{w} = \\eta\\,y\\,x - \\gamma\\,w$。\n-   **学习率**：$\\eta > 0$。\n-   **衰减系数**：$\\gamma > 0$。\n-   **分析框架**：平均场极限，意味着对权重动力学方程关于输入分布进行平均是合理的。\n-   **假设**：权重向量与协方差矩阵的主特征向量对齐，$w=av_1$，其中 $\\Sigma v_1 = \\lambda_1 v_1$，$\\lambda_1$ 是最大特征值。\n-   **目标**：\n    1.  推导突触权重的平均场动力学方程。\n    2.  计算权重向量 $w^{\\ast} = a^{\\ast}v_{1}$ 的非平凡不动点振幅 $a^{\\ast} > 0$。\n    3.  提供 $a^{\\ast}$ 关于 $\\eta$、$\\gamma$、$\\alpha$ 和 $\\lambda_{1}$ 的解析表达式。\n\n### 步骤2：问题验证 - 评估有效性\n\n1.  **科学依据**：该问题在计算神经科学领域有着坚实的基础。它描述了一个结合了赫布学习和权重衰减的突触可塑性经典模型，常用于研究神经回路如何执行主成分分析（PCA）。该模型是一个基于速率的抽象，学习规则是 Oja 法则的一个变体，三次非线性函数是饱和激活函数（如 $\\tanh(u)$）的标准近似，而平均场近似是应用于神经网络的一种标准统计力学技术。该问题在科学上是合理的。\n2.  **适定性**：该问题是适定的。它提供了推导动力系统所需的所有必要参数和统计特性。权重与主特征向量对齐的假设将多维问题简化为权重振幅的一维问题，由于线性增长和三次饱和之间的平衡，在某些条件下，预计会存在一个唯一的、非平凡的、稳定的不动点。\n3.  **客观性**：该问题以精确、客观的数学语言陈述，没有歧义或主观陈述。\n\n该问题没有违反任何指定的无效标准。这是理论神经科学中一个标准的、可解的问题。\n\n### 步骤3：结论与行动\n\n问题有效。我将继续进行推导和求解。\n\n***\n\n### 求解推导\n\n该问题要求我们分析在一个特定学习规则下突触权重向量 $w$ 的动力学。该模型基于速率神经元建模和赫布可塑性的基本原理。\n\n**基本原理**\n\n-   **速率模型表述**：神经元的状态由连续变量 $x$ 和 $y$ 描述，它们代表其平均发放率，而不是离散的脉冲时间。\n-   **赫布学习**：其核心思想，通常概括为“一起发放的细胞连接在一起”，通过使突触权重的变化与突触前活动（$x$）和突触后活动（$y$）之间的相关性成正比来实现。学习规则中的 $\\eta y x$ 项代表了这一原理。\n-   **权重稳定化**：简单的赫布学习是不稳定的。给定的规则包含两种稳定化机制：一个线性权重衰减项（$-\\gamma w$）和一个饱和激活函数（$\\phi(u) = u - \\alpha u^3$）。衰减项防止所有权重的无界增长，而赫布项中的非线性提供了一种更具选择性的、依赖于活动的稳定化。\n\n**1. 平均场动力学方程的推导**\n\n起点是突触权重向量 $w$ 的随机微分方程：\n$$\n\\dot{w} = \\eta\\,y\\,x - \\gamma\\,w\n$$\n我们代入突触后活动度的表达式 $y = \\phi(w^\\top x)$:\n$$\n\\dot{w} = \\eta\\,\\phi(w^\\top x)\\,x - \\gamma\\,w\n$$\n问题指明在平均场极限下工作。这假设权重 $w$ 的演化时间尺度远慢于输入信号 $x$ 的波动。因此，我们可以将方程右侧对输入 $x$ 的平稳分布进行平均，同时将 $w$ 视为准静态的。因此，平均场方程为：\n$$\n\\dot{w} = \\langle \\eta\\,\\phi(w^\\top x)\\,x - \\gamma\\,w \\rangle_x\n$$\n利用期望算子的线性和 $w$ 相对于对 $x$ 的平均是常数这一事实：\n$$\n\\dot{w} = \\eta\\,\\langle \\phi(w^\\top x)\\,x \\rangle_x - \\gamma\\,w\n$$\n现在，我们必须计算期望项 $\\langle \\phi(w^\\top x)\\,x \\rangle_x$。我们使用给定的三次近似非线性函数 $\\phi(u) = u - \\alpha u^3$。令 $u = w^\\top x$。\n$$\n\\langle \\phi(w^\\top x)\\,x \\rangle_x = \\langle (w^\\top x - \\alpha (w^\\top x)^3)\\,x \\rangle_x = \\langle (w^\\top x)x \\rangle_x - \\alpha\\langle (w^\\top x)^3 x \\rangle_x\n$$\n我们分别计算每一项。输入 $x$ 是一个零均值多元高斯分布，$x \\sim \\mathcal{N}(0, \\Sigma)$。\n\n**第一项**: $\\langle (w^\\top x)x \\rangle_x$\n这可以重写为 $\\langle x(x^\\top w) \\rangle_x = \\langle xx^\\top \\rangle_x w$。根据定义，协方差矩阵为 $\\Sigma = \\langle xx^\\top \\rangle_x$。因此，\n$$\n\\langle (w^\\top x)x \\rangle_x = \\Sigma w\n$$\n\n**第二项**: $\\langle (w^\\top x)^3 x \\rangle_x$\n令 $u = w^\\top x$。我们需要计算 $\\langle u^3 x \\rangle_x$。由于 $x$ 是一个零均值高斯向量，其线性组合 $u$ 是一个零均值高斯标量随机变量。向量 $x$ 的分量和标量 $u$ 是联合高斯分布的。对于这类零均值变量，我们可以使用伊塞利斯定理（或威克定理）来计算高阶矩。四个零均值高斯变量 $Z_1, Z_2, Z_3, Z_4$ 的乘积的期望由 $\\langle Z_1 Z_2 Z_3 Z_4 \\rangle = \\langle Z_1 Z_2\\rangle\\langle Z_3 Z_4\\rangle + \\langle Z_1 Z_3\\rangle\\langle Z_2 Z_4\\rangle + \\langle Z_1 Z_4\\rangle\\langle Z_2 Z_3\\rangle$ 给出。\n\n我们计算向量 $\\langle u^3 x \\rangle_x$ 的第 $j$ 个分量，即 $\\langle u^3 x_j \\rangle = \\langle u \\cdot u \\cdot u \\cdot x_j \\rangle$。\n$$\n\\langle u u u x_j \\rangle = \\langle u u \\rangle\\langle u x_j \\rangle + \\langle u u \\rangle\\langle u x_j \\rangle + \\langle u x_j \\rangle\\langle u u \\rangle = 3\\langle u^2 \\rangle \\langle u x_j \\rangle\n$$\n我们需要计算 $\\langle u^2 \\rangle$ 和 $\\langle u x_j \\rangle$。\n-   $\\langle u^2 \\rangle = \\langle (w^\\top x)^2 \\rangle = \\langle w^\\top x x^\\top w \\rangle = w^\\top \\langle x x^\\top \\rangle w = w^\\top \\Sigma w$。\n-   项 $\\langle u x_j \\rangle$ 是向量 $\\langle u x \\rangle_x = \\langle (w^\\top x)x \\rangle_x$ 的第 $j$ 个分量，我们已经求得该向量为 $\\Sigma w$。\n\n结合这些结果，向量 $\\langle u^3 x \\rangle_x$ 是：\n$$\n\\langle u^3 x \\rangle_x = 3 \\langle u^2 \\rangle \\langle u x \\rangle_x = 3(w^\\top \\Sigma w)(\\Sigma w)\n$$\n现在，我们将这两项代回 $\\langle \\phi(w^\\top x)\\,x \\rangle_x$ 的表达式中：\n$$\n\\langle \\phi(w^\\top x)\\,x \\rangle_x = \\Sigma w - \\alpha [3(w^\\top \\Sigma w)(\\Sigma w)] = \\Sigma w - 3\\alpha(w^\\top \\Sigma w)\\Sigma w\n$$\n最后，我们将此结果代入 $\\dot{w}$ 的平均场方程中：\n$$\n\\dot{w} = \\eta \\left[ \\Sigma w - 3\\alpha(w^\\top \\Sigma w)\\Sigma w \\right] - \\gamma w\n$$\n这就是所求的权重向量 $w$ 的平均场动力学方程。\n\n**2. 非平凡不动点振幅的计算**\n\n我们被要求在假设不动点 $w^*$ 与协方差矩阵 $\\Sigma$ 的主特征向量 $v_1$ 对齐的情况下找到它。令 $\\lambda_1$ 为 $\\Sigma$ 的最大特征值，所以 $\\Sigma v_1 = \\lambda_1 v_1$。我们将特征向量归一化，使得 $v_1^\\top v_1 = 1$。假设 $w$ 的形式为 $w(t) = a(t) v_1$，其中 $a(t)$ 是随时间变化的振幅。不动点的形式将是 $w^* = a^* v_1$，其中 $a^*$ 是某个常数振幅。\n\n将 $w = a v_1$ 代入平均场方程：\n$$\n\\frac{d}{dt}(a v_1) = \\eta \\left[ \\Sigma (a v_1) - 3\\alpha((a v_1)^\\top \\Sigma (a v_1))\\Sigma (a v_1) \\right] - \\gamma (a v_1)\n$$\n由于 $v_1$ 是一个常数向量，左侧为 $\\dot{a}v_1$。我们利用特征向量的性质 $\\Sigma v_1 = \\lambda_1 v_1$ 来简化右侧的项：\n-   $\\Sigma (a v_1) = a (\\Sigma v_1) = a \\lambda_1 v_1$。\n-   $(a v_1)^\\top \\Sigma (a v_1) = a^2 v_1^\\top (\\Sigma v_1) = a^2 v_1^\\top (\\lambda_1 v_1) = a^2 \\lambda_1 (v_1^\\top v_1) = a^2 \\lambda_1$。\n-   整个三次项变为：$3\\alpha (a^2 \\lambda_1)(a \\lambda_1 v_1) = 3\\alpha a^3 \\lambda_1^2 v_1$。\n\n将这些简化后的表达式代回方程中：\n$$\n\\dot{a}v_1 = \\eta \\left[ a \\lambda_1 v_1 - 3\\alpha a^3 \\lambda_1^2 v_1 \\right] - \\gamma a v_1\n$$\n我们可以提取公因子向量 $v_1$：\n$$\n\\dot{a}v_1 = \\left( \\eta a \\lambda_1 - 3\\eta \\alpha a^3 \\lambda_1^2 - \\gamma a \\right) v_1\n$$\n这个向量方程可以简化为关于振幅 $a(t)$ 的标量常微分方程：\n$$\n\\dot{a} = (\\eta \\lambda_1 - \\gamma)a - 3\\eta \\alpha \\lambda_1^2 a^3\n$$\n通过设置 $\\dot{a} = 0$ 可以找到不动点 $a^*$：\n$$\n0 = (\\eta \\lambda_1 - \\gamma)a^* - 3\\eta \\alpha \\lambda_1^2 (a^*)^3\n$$\n提取公因子 $a^*$：\n$$\n0 = a^* \\left[ (\\eta \\lambda_1 - \\gamma) - 3\\eta \\alpha \\lambda_1^2 (a^*)^2 \\right]\n$$\n该方程有两类解。\n1.  平凡不动点：$a^* = 0$，对应于 $w^*=0$。\n2.  非平凡不动点，通过将方括号中的表达式设为零得到。这是我们寻求的解，因为问题要求 $a^*>0$。\n$$\n(\\eta \\lambda_1 - \\gamma) - 3\\eta \\alpha \\lambda_1^2 (a^*)^2 = 0\n$$\n解出 $(a^*)^2$：\n$$\n3\\eta \\alpha \\lambda_1^2 (a^*)^2 = \\eta \\lambda_1 - \\gamma\n$$\n$$\n(a^*)^2 = \\frac{\\eta \\lambda_1 - \\gamma}{3\\eta \\alpha \\lambda_1^2}\n$$\n为了存在一个实的、非平凡的解（$a^* \\ne 0$），分子必须为正，这意味着条件 $\\eta \\lambda_1 > \\gamma$ 必须成立。在此条件下，平凡不动点 $a=0$ 是不稳定的，并且会出现两个对称的非平凡不动点。我们感兴趣的是具有正振幅的那个，即 $a^* > 0$。取正平方根得到最终答案：\n$$\na^* = \\sqrt{\\frac{\\eta \\lambda_1 - \\gamma}{3\\eta \\alpha \\lambda_1^2}}\n$$\n这个表达式显示了稳态振幅 $a^*$（以及权重向量的范数 $\\|w^*\\| = a^*$）是如何由与输入协方差最大特征值（$\\lambda_1$）成比例的赫布驱动、学习率（$\\eta$）、衰减系数（$\\gamma$）和饱和参数（$\\alpha$）之间的平衡所决定的。具体来说，增加衰减 $\\gamma$ 会减小不动点振幅，从而对总突触强度起到稳态控制的作用。",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{\\eta \\lambda_{1} - \\gamma}{3\\eta \\alpha \\lambda_{1}^{2}}}}\n$$"
        },
        {
            "introduction": "前面的练习集中于单个神经元学习主成分。然而，大脑需要在网络层面提取多个有意义的特征。本练习将赫布原理扩展到一个神经元网络，并介绍Sanger法则，该算法能够有序地提取多个主成分。通过在白化输入这一简化条件下进行分析，我们可以清晰地揭示该法则如何通过级联的正交化过程，确保网络学习到一组正交的特征检测器。",
            "id": "3987575",
            "problem": "考虑一个线性速率网络，其输入为 $x \\in \\mathbb{R}^{n}$，具有零均值和白化统计特性 $\\mathbb{E}[x] = 0$ 和 $\\mathbb{E}[x x^{\\top}] = I_{n}$，其中 $I_{n}$ 表示 $n \\times n$ 单位矩阵。该网络有 $m$ 个输出单元，其瞬时速率汇集在 $y \\in \\mathbb{R}^{m}$ 中，由 $y = W^{\\top} x$ 给出，其中 $W \\in \\mathbb{R}^{n \\times m}$ 的列为 $w_{1},\\dots,w_{m} \\in \\mathbb{R}^{n}$。学习遵循 Sanger 规则，也称为广义赫布算法 (GHA)，是用于主成分分析 (PCA) 的一种赫布学习形式。Sanger 规则的列更新定义为\n$$\n\\Delta w_{i} = \\eta\\, y_{i}\\left(x - \\sum_{j=1}^{i} y_{j} w_{j}\\right), \\quad i=1,\\dots,m,\n$$\n其中 $\\eta > 0$ 是一个小的常数学习率。在平均场（小 $\\eta$）极限下处理学习，方法是将瞬时更新替换为其在输入分布上的期望，并转为连续时间，从而有 $\\frac{d w_{i}}{dt} = \\mathbb{E}[\\Delta w_{i}]$。\n\n任务：\n1) 仅使用白化输入属性 $\\mathbb{E}[x x^{\\top}] = I_{n}$ 和速率定义 $y = W^{\\top} x$，推导每一列 $w_{i}$ 的期望更新 $\\mathbb{E}[\\Delta w_{i}]$ 的显式简化表达式。\n2) 通过求解所有 $i$ 的 $\\mathbb{E}[\\Delta w_{i}] = 0$，确定平均场动力学的不动点集。\n3) 在具有正交归一列 $W^{\\star}$ (满足 ${W^{\\star}}^{\\top} W^{\\star} = I_{m}$) 的任意不动点周围线性化平均场动力学，并计算以下两种扰动模式的渐近指数收敛率（雅可比矩阵的特征值）：\n   (a) 一种归一化模式，它扰动单列 $w_{i}$ 的范数，同时在一阶上保持与其他列的正交性。\n   (b) 一种去相关模式，它在两个不同列之间引入一个小的互相关，即对于 $i \\neq k$，在 $w_{i}^{\\top} w_{k}$ 中引入一阶扰动。\n\n将你的最终答案以单行矩阵的形式给出，其中包含：通用列 $w_{i}$ 的简化期望更新、归一化模式收敛率和去相关模式收敛率。不需要数值近似；给出闭式解析表达式。最终答案不带单位。",
            "solution": "用户希望我分析在白化输入分布下 Sanger 规则的平均场动力学。我将首先验证问题陈述。\n\n### 步骤 1：提取已知条件\n-   输入向量：$x \\in \\mathbb{R}^{n}$\n-   输入统计：$\\mathbb{E}[x] = 0$（零均值）和 $\\mathbb{E}[x x^{\\top}] = I_{n}$（白化），其中 $I_{n}$ 是 $n \\times n$ 单位矩阵。\n-   输出向量：$y \\in \\mathbb{R}^{m}$\n-   网络输出函数：$y = W^{\\top} x$，其中 $W \\in \\mathbb{R}^{n \\times m}$ 是权重矩阵，其列为 $w_{1}, \\dots, w_{m} \\in \\mathbb{R}^{n}$。第 $i$ 个输出是 $y_i = w_i^\\top x$。\n-   学习规则（Sanger 规则 / 广义赫布算法）：$\\Delta w_{i} = \\eta\\, y_{i}\\left(x - \\sum_{j=1}^{i} y_{j} w_{j}\\right)$，对于 $i=1,\\dots,m$。\n-   学习率：$\\eta > 0$ 是一个小的常数。\n-   平均场动力学：$\\frac{d w_{i}}{dt} = \\mathbb{E}[\\Delta w_{i}]$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据，是适定的，并且是客观的。\n-   **科学依据**：该问题是计算神经科学和机器学习中的一个标准表述，涉及用于学习主成分的赫布可塑性（Sanger 规则）。在此理论框架内，假设（白化输入、速率模型、平均场极限）是常见且有效的。\n-   **适定性**：该问题提供了推导所要求量所需的所有定义和条件。任务规定明确，可得到唯一的解析解。\n-   **客观性**：该问题以精确的数学语言陈述，没有歧义或主观论断。\n\n### 步骤 3：结论与行动\n问题有效。我将继续进行求解。\n\n### 任务 1：期望更新的简化表达式\n权重向量 $w_i$ 的平均场动力学是通过对输入 $x$ 的分布取更新规则的期望来给出的。\n$$\n\\frac{d w_{i}}{dt} = \\mathbb{E}[\\Delta w_{i}] = \\mathbb{E}\\left[\\eta\\, y_{i}\\left(x - \\sum_{j=1}^{i} y_{j} w_{j}\\right)\\right]\n$$\n由于 $\\eta$ 和 $w_j$ 相对于输入分布不是随机变量，我们可以将期望算子移到内部。\n$$\n\\frac{d w_{i}}{dt} = \\eta \\left( \\mathbb{E}[y_{i}x] - \\sum_{j=1}^{i} \\mathbb{E}[y_{i}y_{j}] w_{j} \\right)\n$$\n现在我们利用输入 $x$ 的给定属性来计算这两个期望项。\n\n第一项：$\\mathbb{E}[y_{i}x]$。我们代入 $y_i = w_i^\\top x$：\n$$\n\\mathbb{E}[y_{i}x] = \\mathbb{E}[(w_{i}^{\\top}x)x] = \\mathbb{E}[x(x^{\\top}w_{i})] = \\mathbb{E}[xx^{\\top}]w_{i}\n$$\n使用白化输入统计量 $\\mathbb{E}[xx^{\\top}] = I_{n}$，我们得到：\n$$\n\\mathbb{E}[y_{i}x] = I_{n} w_{i} = w_{i}\n$$\n第二项：$\\mathbb{E}[y_{i}y_{j}]$。我们代入 $y_i = w_i^\\top x$ 和 $y_j = w_j^\\top x$：\n$$\n\\mathbb{E}[y_{i}y_{j}] = \\mathbb{E}[(w_{i}^{\\top}x)(w_{j}^{\\top}x)] = \\mathbb{E}[w_{i}^{\\top}x x^{\\top}w_{j}]\n$$\n由于 $w_i$ 和 $w_j$ 相对于 $x$ 的期望是常数，我们可以写出：\n$$\n\\mathbb{E}[y_{i}y_{j}] = w_{i}^{\\top}\\mathbb{E}[xx^{\\top}]w_{j} = w_{i}^{\\top}I_{n}w_{j} = w_{i}^{\\top}w_{j}\n$$\n将这些结果代回 $\\frac{d w_{i}}{dt}$ 的方程中：\n$$\n\\frac{d w_{i}}{dt} = \\eta \\left( w_{i} - \\sum_{j=1}^{i} (w_{i}^{\\top}w_{j}) w_{j} \\right)\n$$\n这就是期望更新的简化显式表达式。\n\n### 任务 2：平均场动力学的不动点\n动力学的不动点是使得对所有 $i = 1, \\dots, m$ 都有 $\\frac{d w_{i}}{dt} = 0$ 的权重矩阵 $W$。\n$$\n\\eta \\left( w_{i} - \\sum_{j=1}^{i} (w_{i}^{\\top}w_{j}) w_{j} \\right) = 0\n$$\n因为 $\\eta > 0$，这可以简化为：\n$$\nw_{i} - \\sum_{j=1}^{i} (w_{i}^{\\top}w_{j}) w_{j} = 0\n$$\n让我们对递增的 $i$ 分析这个条件：\n- 对于 $i=1$：$w_{1} - (w_{1}^{\\top}w_{1})w_{1} = 0$。一个非平凡解（$w_1 \\neq 0$）要求 $w_{1}^{\\top}w_{1} = \\|w_1\\|^2 = 1$。\n- 对于 $i=2$：$w_{2} - (w_{2}^{\\top}w_{1})w_{1} - (w_{2}^{\\top}w_{2})w_{2} = 0$。将此方程投影到 $w_1$ 上（假设 $\\|w_1\\|=1$）：\n$w_{1}^{\\top}w_{2} - (w_{2}^{\\top}w_{1})(w_{1}^{\\top}w_{1}) - (w_{2}^{\\top}w_{2})(w_{1}^{\\top}w_{2}) = 0$。这给出 $w_{1}^{\\top}w_{2} - w_{2}^{\\top}w_{1} - (w_{2}^{\\top}w_{2})(w_{1}^{\\top}w_{2}) = 0$，简化为 $-(w_{2}^{\\top}w_{2})(w_{1}^{\\top}w_{2}) = 0$。对于 $w_2$ 的一个非平凡解（$w_{2}^{\\top}w_{2} \\neq 0$）要求 $w_{1}^{\\top}w_{2} = 0$。那么 $w_2$ 的方程变为 $w_{2} - (w_{2}^{\\top}w_{2})w_{2} = 0$，这意味着 $w_{2}^{\\top}w_{2} = 1$。\n- 通过归纳法，我们可以证明对于每个 $i$，不动点条件意味着对所有 $j  i$ 有 $w_{i}^{\\top}w_{j} = 0$，并且 $w_{i}^{\\top}w_{i} = 1$。\n\n因此，不动点集是所有列向量 $\\{w_1, \\dots, w_m\\}$ 构成正交归一集的矩阵 $W$ 的集合，即对于 $i,j \\in \\{1,\\dots,m\\}$ 有 $w_i^\\top w_j = \\delta_{ij}$。这通常写作 $W^{\\top}W = I_m$。\n\n### 任务 3：线性化和收敛率\n令 $W^{\\star}$ 为一个不动点，因此其列 $w_i^{\\star}$ 满足 ${w_i^{\\star}}^{\\top}w_j^{\\star} = \\delta_{ij}$。我们在此不动点周围线性化动力学 $w_i = w_i^{\\star} + \\delta w_i$。令 $F_i(W) = \\eta \\left( w_{i} - \\sum_{j=1}^{i} (w_{i}^{\\top}w_{j}) w_{j} \\right)$。\n线性化动力学为 $\\frac{d (\\delta w_i)}{dt} \\approx \\delta F_i$。\n$$\n\\delta F_i = \\eta \\left( \\delta w_i - \\sum_{j=1}^{i} \\delta\\left[ (w_i^\\top w_j) w_j \\right] \\right)\n$$\n在不动点 $W^{\\star}$ 处：\n$$\n\\delta\\left[ (w_i^\\top w_j) w_j \\right] = (\\delta w_i^\\top w_j^\\star + w_i^{\\star\\top} \\delta w_j) w_j^\\star + (w_i^{\\star\\top} w_j^\\star) \\delta w_j = (\\delta w_i^\\top w_j^\\star + w_i^{\\star\\top} \\delta w_j) w_j^\\star + \\delta_{ij} \\delta w_j\n$$\n将此代入 $\\delta F_i$ 的表达式中：\n$$\n\\frac{1}{\\eta}\\delta F_i = \\delta w_i - \\sum_{j=1}^{i-1} \\left[ (\\delta w_i^\\top w_j^\\star + w_i^{\\star\\top} \\delta w_j) w_j^\\star \\right] - \\left[ (\\delta w_i^\\top w_i^\\star + w_i^{\\star\\top} \\delta w_i) w_i^\\star + \\delta w_i \\right]\n$$\n简化，并使用 $w_{j}^{\\star \\top} \\delta w_i = \\delta w_i^\\top w_j^\\star$：\n$$\n\\frac{d (\\delta w_i)}{dt} = \\delta F_i = -\\eta \\left[ (2 w_i^{\\star\\top} \\delta w_i) w_i^\\star + \\sum_{j=1}^{i-1} \\left( (w_j^{\\star\\top} \\delta w_i) w_j^\\star + (w_i^{\\star\\top} \\delta w_j) w_j^\\star \\right) \\right]\n$$\n\n**(a) 归一化模式：**\n此模式对应于单列 $w_i$ 长度的扰动，而其他列保持不变。令 $\\delta w_i = \\epsilon w_i^\\star$（其中 $\\epsilon$ 为某个小标量），且对于 $j \\neq i$ 有 $\\delta w_j = 0$。\n此扰动的动力学为：\n$$\n\\frac{d (\\delta w_i)}{dt} = \\frac{d (\\epsilon w_i^\\star)}{dt} = \\frac{d\\epsilon}{dt} w_i^\\star\n$$\n将扰动代入线性化方程：\n$$\n\\frac{d (\\delta w_i)}{dt} = -\\eta \\left[ (2 w_i^{\\star\\top} (\\epsilon w_i^\\star)) w_i^\\star + \\sum_{j=1}^{i-1} \\left( (w_j^{\\star\\top} (\\epsilon w_i^\\star)) w_j^\\star + (w_i^{\\star\\top} 0) w_j^\\star \\right) \\right]\n$$\n使用 $w_k^\\star$ 的正交归一性：$w_i^{\\star\\top} w_i^\\star = 1$ 且对于 $j  i$ 有 $w_j^{\\star\\top} w_i^\\star = 0$。\n$$\n\\frac{d (\\delta w_i)}{dt} = -\\eta \\left[ (2\\epsilon \\cdot 1) w_i^\\star + \\sum_{j=1}^{i-1} (\\epsilon \\cdot 0) w_j^\\star \\right] = -2\\eta\\epsilon w_i^\\star\n$$\n比较 $\\frac{d (\\delta w_i)}{dt}$ 的两个表达式：\n$$\n\\frac{d\\epsilon}{dt} w_i^\\star = -2\\eta\\epsilon w_i^\\star \\implies \\frac{d\\epsilon}{dt} = -2\\eta\\epsilon\n$$\n解为 $\\epsilon(t) = \\epsilon(0) \\exp(-2\\eta t)$。渐近指数收敛率（特征值）为 $-2\\eta$。\n\n**(b) 去相关模式：**\n此模式涉及两个不同列 $w_i$ 和 $w_k$ 之间的混合，其中 $i \\neq k$。我们考虑 $k  i$ 的情况。我们分析 $w_i$ 在 $w_k^\\star$ 方向上的扰动：$\\delta w_i = \\epsilon w_k^\\star$，而所有其他的 $\\delta w_j = 0$。\n$\\delta w_i$ 的动力学为：\n$$\n\\frac{d (\\delta w_i)}{dt} = \\frac{d (\\epsilon w_k^\\star)}{dt} = \\frac{d\\epsilon}{dt} w_k^\\star\n$$\n代入线性化方程中：\n$$\n\\frac{d (\\delta w_i)}{dt} = -\\eta \\left[ (2 w_i^{\\star\\top} (\\epsilon w_k^\\star)) w_i^\\star + \\sum_{j=1}^{i-1} \\left( (w_j^{\\star\\top} (\\epsilon w_k^\\star)) w_j^\\star + (w_i^{\\star\\top} 0) w_j^\\star \\right) \\right]\n$$\n使用正交归一性：对于 $k  i$ 有 $w_i^{\\star\\top} w_k^\\star = 0$。第一项消失。在和式中，$w_j^{\\star\\top} w_k^\\star = \\delta_{jk}$。只有当 $j=k$ 时和式非零。\n$$\n\\frac{d (\\delta w_i)}{dt} = -\\eta \\left[ 0 + (\\epsilon \\cdot 1) w_k^\\star \\right] = -\\eta\\epsilon w_k^\\star\n$$\n比较表达式：\n$$\n\\frac{d\\epsilon}{dt} w_k^\\star = -\\eta\\epsilon w_k^\\star \\implies \\frac{d\\epsilon}{dt} = -\\eta\\epsilon\n$$\n收敛率为 $-\\eta$。\n对 $k>i$ 进行类似分析会得到扰动 $\\delta w_i = \\alpha w_k^\\star$ 和 $\\delta w_k = \\beta w_i^\\star$ 的耦合系统，其特征值为 $0$ 和 $-\\eta$。与收敛对应的非零特征值为 $-\\eta$。因此，去相关率是统一的。\n\n最终答案是简化的期望更新、归一化率和去相关率。",
            "answer": "$$\n\\boxed{\\pmatrix{ \\eta \\left( w_{i} - \\sum_{j=1}^{i} (w_{i}^{\\top} w_{j}) w_{j} \\right)  -2\\eta  -\\eta }}\n$$"
        }
    ]
}