{
    "hands_on_practices": [
        {
            "introduction": "纯粹的赫布学习（$\\Delta w \\propto yx$）是无限制的，会导致权重无界增长。一种常见的稳定化方法是在每次更新后强制将权重向量的范数（norm）重置为1。这个练习将引导你通过数学推导，揭示这种看似简单的“先更新、后规范化”的离散过程，在学习率很小的情况下，其本质等价于一个著名的连续时间学习规则——奥哈法则（Oja's rule）。这不仅加深了对奥哈法则背后机制的理解，也展示了离散数值算法与连续动力学模型之间的深刻联系。",
            "id": "3987622",
            "problem": "考虑一个单一的线性速率神经元，它接收一个输入向量 $x \\in \\mathbb{R}^{n}$，其突触权重向量为 $w \\in \\mathbb{R}^{n}$。该神经元的放电速率为 $y = w^{\\top} x$。在离散时间内，施加一个步长为 $\\eta  0$ 的标准赫布增量，并紧接着进行乘性权重归一化至单位范数。也就是说，在一个学习步骤中：\n- 首先，计算赫布预更新 $w' = w + \\eta\\,y\\,x$。\n- 然后，强制单位范数 $w^{+} = \\dfrac{w'}{\\|w'\\|}$。\n假设在步骤开始时，权重向量已经被归一化，即 $\\|w\\| = 1$。对于标量变量的光滑函数，您可以使用关于 $0$ 的一阶泰勒展开，包括 $\\sqrt{1+\\epsilon}$ 和 $(1+\\epsilon)^{-1}$。\n\n从欧几里得范数和一阶泰勒展开的基本定义出发，推导由赫布更新后归一化（normalize-after-Hebbian）引起的权重变化 $w^{+} - w$ 的领先阶小步长近似（到 $\\eta$ 的一阶），用 $\\eta$、$y$、$x$ 和 $w$ 表示，其中 $y = w^{\\top} x$。这个领先阶表达式是在基于速率的公式中通常与 Oja 法则相关的连续时间极限漂移。\n\n答案格式要求：\n- 仅提供以 $\\eta$、$y$、$x$ 和 $w$ 表示的单解析闭式表达式的一阶项。\n- 最终答案中不要包含等号。\n- 不需要四舍五入，也不涉及单位。",
            "solution": "任务是推导在小学习步长 $\\eta$ 下权重变化 $w^{+} - w$ 的领先阶近似。更新规则是赫布增量后进行归一化。\n\n权重变化由 $\\Delta w = w^{+} - w$ 给出。更新后的权重向量 $w^{+}$ 定义为：\n$$ w^{+} = \\frac{w'}{\\|w'\\|} $$\n其中 $w'$ 是赫布预更新的结果：\n$$ w' = w + \\eta y x $$\n这里，$y = w^{\\top} x$ 是神经元的放电速率，$x \\in \\mathbb{R}^{n}$ 是输入向量，$w \\in \\mathbb{R}^{n}$ 是权重向量，$\\eta  0$ 是学习率。我们给定的初始条件是 $\\|w\\| = 1$。\n\n将 $w'$ 的表达式代入 $w^{+}$ 的方程中，得到：\n$$ w^{+} = \\frac{w + \\eta y x}{\\|w + \\eta y x\\|} $$\n为了推导 $\\eta$ 的一阶近似，我们必须分析分母中的项 $\\|w + \\eta y x\\|$。我们首先展开它的平方：\n$$ \\|w + \\eta y x\\|^2 = (w + \\eta y x)^{\\top}(w + \\eta y x) $$\n使用转置和矩阵乘法的分配律：\n$$ \\|w + \\eta y x\\|^2 = (w^{\\top} + \\eta y x^{\\top})(w + \\eta y x) = w^{\\top}w + w^{\\top}(\\eta y x) + (\\eta y x^{\\top})w + (\\eta y x^{\\top})(\\eta y x) $$\n$$ \\|w + \\eta y x\\|^2 = \\|w\\|^2 + \\eta y (w^{\\top}x) + \\eta y (x^{\\top}w) + \\eta^2 y^2 (x^{\\top}x) $$\n由于向量点积是对称的，$x^{\\top}w = w^{\\top}x$。我们已知 $\\|w\\| = 1$ 且 $y = w^{\\top}x$。将这些代入方程，得到：\n$$ \\|w + \\eta y x\\|^2 = 1 + \\eta y (y) + \\eta y (y) + \\eta^2 y^2 \\|x\\|^2 $$\n$$ \\|w + \\eta y x\\|^2 = 1 + 2\\eta y^2 + \\eta^2 y^2 \\|x\\|^2 $$\n现在我们取平方根来求 $\\|w + \\eta y x\\|$：\n$$ \\|w + \\eta y x\\| = \\sqrt{1 + 2\\eta y^2 + \\eta^2 y^2 \\|x\\|^2} $$\n题目允许对小的 $\\eta$ 使用一阶泰勒展开。我们对小的 $\\epsilon$ 使用展开式 $\\sqrt{1+\\epsilon} \\approx 1 + \\frac{1}{2}\\epsilon$。在我们的情况中，项 $\\epsilon = 2\\eta y^2 + \\eta^2 y^2 \\|x\\|^2$。对于 $\\eta$ 的一阶，我们只需要考虑与 $\\eta$ 成正比的项，因此我们可以近似 $\\epsilon \\approx 2\\eta y^2$。\n$$ \\|w + \\eta y x\\| \\approx \\sqrt{1 + 2\\eta y^2} \\approx 1 + \\frac{1}{2}(2\\eta y^2) = 1 + \\eta y^2 $$\n我们忽略了 $\\eta^2$ 阶及更高阶的项。\n\n接下来，我们需要求这个范数的倒数，它在 $w^{+}$ 的表达式中作为乘法因子出现。\n$$ \\frac{1}{\\|w + \\eta y x\\|} \\approx \\frac{1}{1 + \\eta y^2} $$\n使用泰勒展开 $(1+\\delta)^{-1} \\approx 1 - \\delta$（对于小的 $\\delta$），其中 $\\delta = \\eta y^2$，我们得到：\n$$ \\frac{1}{\\|w + \\eta y x\\|} \\approx 1 - \\eta y^2 $$\n现在我们将这个近似值代回 $w^{+}$ 的方程中：\n$$ w^{+} \\approx (w + \\eta y x)(1 - \\eta y^2) $$\n展开这个乘积：\n$$ w^{+} \\approx w(1 - \\eta y^2) + \\eta y x(1 - \\eta y^2) $$\n$$ w^{+} \\approx w - \\eta y^2 w + \\eta y x - \\eta^2 y^3 x $$\n我们正在寻找领先阶近似，这意味着我们保留到 $\\eta$ 一阶的项，并丢弃所有更高阶的项（即带有 $\\eta^2, \\eta^3, \\dots$ 的项）。\n$$ w^{+} \\approx w + \\eta y x - \\eta y^2 w $$\n权重变化为 $\\Delta w = w^{+} - w$。使用我们对 $w^{+}$ 的近似：\n$$ \\Delta w \\approx (w + \\eta y x - \\eta y^2 w) - w $$\n$$ \\Delta w \\approx \\eta y x - \\eta y^2 w $$\n这个表达式可以因式分解，得到一阶权重变化的最终形式：\n$$ \\Delta w \\approx \\eta(y x - y^2 w) $$\n这个结果也通常写为：\n$$ \\Delta w \\approx \\eta y(x - y w) $$\n其中 $y = w^{\\top}x$。这就是 Oja 法则的数学表达式。问题要求的是权重变化 $w^{+} - w$ 的一阶项，也就是这个表达式。",
            "answer": "$$\n\\boxed{\\eta y (x - y w)}\n$$"
        },
        {
            "introduction": "我们已经看到如何稳定赫布学习，但一个稳定的学习规则其计算目标是什么？在处理高维数据时，神经元如何学习提取有意义的特征？该练习通过分析两种不同的稳态机制——乘性稳态（如奥哈法则）和加性稳态（通过控制输出方差）——来回答这个问题。你将证明，尽管机制不同，它们都驱使神经元的感受野（receptive field）对齐到输入数据协方差矩阵的主特征向量上，从而实现了主成分分析（PCA）这一强大的计算功能。",
            "id": "3987601",
            "problem": "考虑一个单一线性速率神经元，其接收一个二维、零均值、平稳的高斯输入 $\\mathbf{x} \\in \\mathbb{R}^{2}$，协方差为\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n\\sigma_{1}^{2}  \\rho \\,\\sigma_{1}\\sigma_{2} \\\\\n\\rho \\,\\sigma_{1}\\sigma_{2}  \\sigma_{2}^{2}\n\\end{pmatrix},\n$$\n其中 $\\sigma_{1}  0$，$\\sigma_{2}  0$，且 $\\rho \\in (-1,1)$ 且 $\\rho \\neq 0$。该神经元的发放率为 $y = \\mathbf{w}^{\\top}\\mathbf{x}$，其中 $\\mathbf{w} \\in \\mathbb{R}^{2}$ 是突触权重。学习遵循一个与输入-输出相关性成正比的Hebb项，并结合以下两种不同的稳态机制之一：\n\n- 乘性稳态 (Oja类型): \n$$\n\\frac{d\\mathbf{w}}{dt} \\;=\\; \\eta\\big(\\mathbb{E}[y\\,\\mathbf{x}] \\;-\\; \\mathbb{E}[y^{2}]\\,\\mathbf{w}\\big),\n$$\n学习率为 $\\eta  0$。\n\n- 具有方差控制的加性稳态:\n$$\n\\frac{d\\mathbf{w}}{dt} \\;=\\; \\eta\\big(\\mathbb{E}[y\\,\\mathbf{x}] \\;-\\; \\lambda\\,\\mathbf{w}\\big), \\qquad \\frac{d\\lambda}{dt} \\;=\\; \\epsilon\\big(\\mathbb{E}[y^{2}] \\;-\\; q\\big),\n$$\n其中 $\\epsilon  0$ 且目标输出方差 $q  0$ 为固定值。\n\n从协方差和线性响应的定义出发，并且除了给定的 $\\Sigma$ 之外不假设任何特殊结构，分析两种动力学在收敛到稳定、非平凡感受野情况下的不动点。证明在这两种稳态机制下，学习到的感受野方向是相同的，并且对应于 $\\Sigma$ 的主特征向量。然后，令学习到的稳定权重向量的方向满足 $w_{2}  0$。计算 $\\Sigma$ 的主特征向量分量之比 $r = w_{1}/w_{2}$ 的精确闭式解析表达式，用 $\\sigma_{1}$、$\\sigma_{2}$ 和 $\\rho$ 表示。你的最终答案必须是 $r$ 的单一闭式表达式。不要进行近似或四舍五入。",
            "solution": "该问题要求计算在线性神经元的两种不同Hebb学习规则下，稳定感受野向量 $\\mathbf{w}$ 各分量的比率。我们必须首先证明，在这两种情况下，该向量的方向是相同的，并且对应于输入协方差矩阵 $\\Sigma$ 的主特征向量。\n\n首先，我们计算必要的期望值。输入 $\\mathbf{x}$ 是一个零均值高斯变量，$\\mathbb{E}[\\mathbf{x}] = \\mathbf{0}$。输出为 $y = \\mathbf{w}^{\\top}\\mathbf{x}$。输入的协方差矩阵由 $\\Sigma = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$ 给出。\n\n输入-输出相关性为：\n$$\n\\mathbb{E}[y\\,\\mathbf{x}] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})\\mathbf{x}] = \\mathbb{E}[\\mathbf{x}(\\mathbf{x}^{\\top}\\mathbf{w})] = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w} = \\Sigma\\mathbf{w}\n$$\n均方输出，即输出方差，为：\n$$\n\\mathbb{E}[y^2] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})^2] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})(\\mathbf{x}^{\\top}\\mathbf{w})] = \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w} = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w}\n$$\n\n现在，我们分析这两种学习规则的不动点。当 $\\frac{d\\mathbf{w}}{dt} = \\mathbf{0}$ 时，达到不动点 $\\mathbf{w}^*$。\n\n情况1：乘性稳态 (Oja类型规则)\n学习规则是 $\\frac{d\\mathbf{w}}{dt} = \\eta\\big(\\mathbb{E}[y\\,\\mathbf{x}] - \\mathbb{E}[y^{2}]\\,\\mathbf{w}\\big)$。\n代入期望值，我们得到：\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta\\big(\\Sigma\\mathbf{w} - (\\mathbf{w}^{\\top}\\Sigma\\mathbf{w})\\mathbf{w}\\big)\n$$\n在一个不动点，对于一个非平凡解 $\\mathbf{w} \\neq \\mathbf{0}$：\n$$\n\\Sigma\\mathbf{w} - (\\mathbf{w}^{\\top}\\Sigma\\mathbf{w})\\mathbf{w} = \\mathbf{0} \\implies \\Sigma\\mathbf{w} = (\\mathbf{w}^{\\top}\\Sigma\\mathbf{w})\\mathbf{w}\n$$\n这是一个特征向量方程 $\\Sigma\\mathbf{w} = \\mu\\mathbf{w}$，其中特征值为 $\\mu = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w}$。因此，任何不动点都必须是 $\\Sigma$ 的一个特征向量。对Oja规则的稳定性分析表明，动力学收敛到一个稳定的不动点，该不动点是 $\\Sigma$ 的主特征向量（即对应于最大特征值的特征向量）。\n\n情况2：具有方差控制的加性稳态\n动力学由一个耦合系统给出：\n$$\n\\text{(i)} \\quad \\frac{d\\mathbf{w}}{dt} = \\eta\\big(\\mathbb{E}[y\\,\\mathbf{x}] - \\lambda\\,\\mathbf{w}\\big) = \\eta(\\Sigma\\mathbf{w} - \\lambda\\mathbf{w})\n$$\n$$\n\\text{(ii)} \\quad \\frac{d\\lambda}{dt} = \\epsilon\\big(\\mathbb{E}[y^{2}] - q\\big) = \\epsilon(\\mathbf{w}^{\\top}\\Sigma\\mathbf{w} - q)\n$$\n在一个不动点 $(\\mathbf{w}^*, \\lambda^*)$，两个导数都为零。从(i)式，对于一个非平凡解 $\\mathbf{w}^* \\neq \\mathbf{0}$：\n$$\n\\Sigma\\mathbf{w}^* - \\lambda^*\\mathbf{w}^* = \\mathbf{0} \\implies \\Sigma\\mathbf{w}^* = \\lambda^*\\mathbf{w}^*\n$$\n这再次表明，不动点权重向量 $\\mathbf{w}^*$ 必须是 $\\Sigma$ 的一个特征向量，而稳态变量 $\\lambda^*$ 收敛到相应的特征值。从(ii)式：\n$$\n\\mathbf{w}^{*\\top}\\Sigma\\mathbf{w}^* - q = 0 \\implies \\mathbf{w}^{*\\top}\\Sigma\\mathbf{w}^* = q\n$$\n与第一条规则一样，稳定性分析证实系统收敛到一个状态，其中 $\\mathbf{w}^*$ 是 $\\Sigma$ 的主特征向量，而 $\\lambda^*$ 是最大特征值 $\\mu_{max}$。\n\n在这两种情况下，学习到的感受野 $\\mathbf{w}$ 的方向都是输入协方差矩阵 $\\Sigma$ 的主特征向量的方向。问题现在简化为找到这个主特征向量。\n\n协方差矩阵为：\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_{1}^{2}  \\rho \\,\\sigma_{1}\\sigma_{2} \\\\\n\\rho \\,\\sigma_{1}\\sigma_{2}  \\sigma_{2}^{2}\n\\end{pmatrix}\n$$\n特征值 $\\mu$ 是特征方程 $\\det(\\Sigma - \\mu I) = 0$ 的根：\n$$\n(\\sigma_1^2 - \\mu)(\\sigma_2^2 - \\mu) - (\\rho \\sigma_1 \\sigma_2)^2 = 0\n$$\n$$\n\\mu^2 - (\\sigma_1^2 + \\sigma_2^2)\\mu + \\sigma_1^2 \\sigma_2^2 (1 - \\rho^2) = 0\n$$\n使用二次公式，特征值为：\n$$\n\\mu = \\frac{(\\sigma_1^2 + \\sigma_2^2) \\pm \\sqrt{(\\sigma_1^2 + \\sigma_2^2)^2 - 4\\sigma_1^2\\sigma_2^2(1 - \\rho^2)}}{2}\n$$\n平方根下的项简化为 $(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2$。主特征向量对应于最大特征值 $\\mu_{max}$，通过取正号获得：\n$$\n\\mu_{max} = \\frac{1}{2}\\left( (\\sigma_1^2 + \\sigma_2^2) + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2} \\right)\n$$\n设主特征向量为 $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$。它必须满足方程 $(\\Sigma - \\mu_{max}I)\\mathbf{w} = \\mathbf{0}$。我们可以使用这个矩阵-向量方程的第二行：\n$$\n(\\rho \\sigma_1 \\sigma_2) w_1 + (\\sigma_2^2 - \\mu_{max}) w_2 = 0\n$$\n我们想求比率 $r = w_1/w_2$：\n$$\nr = \\frac{w_1}{w_2} = -\\frac{\\sigma_2^2 - \\mu_{max}}{\\rho \\sigma_1 \\sigma_2} = \\frac{\\mu_{max} - \\sigma_2^2}{\\rho \\sigma_1 \\sigma_2}\n$$\n现在我们计算分子 $\\mu_{max} - \\sigma_2^2$：\n$$\n\\mu_{max} - \\sigma_2^2 = \\frac{1}{2}\\left( (\\sigma_1^2 + \\sigma_2^2) + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2} \\right) - \\sigma_2^2\n$$\n$$\n= \\frac{1}{2}\\left( \\sigma_1^2 + \\sigma_2^2 - 2\\sigma_2^2 + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2} \\right)\n$$\n$$\n= \\frac{1}{2}\\left( \\sigma_1^2 - \\sigma_2^2 + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2} \\right)\n$$\n将此代回 $r$ 的表达式中：\n$$\nr = \\frac{\\frac{1}{2}\\left( \\sigma_1^2 - \\sigma_2^2 + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2} \\right)}{\\rho \\sigma_1 \\sigma_2}\n$$\n这简化为比率 $r = w_1 / w_2$ 的最终闭式表达式。条件 $w_2  0$ 只是确定了特征向量的方向，但不会改变这个比率。\n$$\nr = \\frac{\\sigma_1^2 - \\sigma_2^2 + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2}}{2 \\rho \\sigma_1 \\sigma_2}\n$$",
            "answer": "$$\n\\boxed{\\frac{\\sigma_1^2 - \\sigma_2^2 + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2}}{2 \\rho \\sigma_1 \\sigma_2}}\n$$"
        },
        {
            "introduction": "奥哈法则通过一个减法项强制维持权重范数，但生物神经元和人工神经网络中还存在其他稳定机制，例如权重的自然衰减和神经元发放率的饱和现象。本练习探讨了一个结合了线性权重衰减和非线性（三次）发放率饱和的赫布学习模型。通过求解该动力学系统的非平凡不动点，你将精确地看到，系统的稳态权重强度是如何由学习率、衰减率、饱和度以及输入数据统计特性（主特征值）共同决定的，这为理解神经网络中更复杂的自调节过程提供了洞见。",
            "id": "3987647",
            "problem": "考虑一个突触后基于速率的神经元，其活动由 $y=\\phi(w^{\\top} x)$ 建模，其中 $w \\in \\mathbb{R}^{n}$ 是突触权重向量，$x \\in \\mathbb{R}^{n}$ 是一个零均值随机输入，而 $\\phi(\\cdot)$ 是一个光滑的奇饱和非线性函数。突触动力学遵循一个结合了 Hebbian 学习和权重衰减的规则\n$$\n\\dot{w}=\\eta\\,y\\,x-\\gamma\\,w,\n$$\n其中 $\\eta0$ 是学习率，$\\gamma0$ 是衰减系数。假设输入是平稳且高斯的，其协方差矩阵 $\\Sigma\\in\\mathbb{R}^{n\\times n}$ 是对称正定 (SPD) 的，即 $x\\sim\\mathcal{N}(0,\\Sigma)$，并且饱和非线性函数在工作区间内可以被截断的奇三次函数\n$$\n\\phi(u)=u-\\alpha u^{3},\n$$\n很好地近似，其中 $\\alpha0$ 表征饱和强度。在平均场极限下进行研究，在该极限下，对 $w$ 的动力学方程关于输入分布进行平均是合理的。\n\n从 Hebbian 学习和基于速率的公式化的基本定义出发，推导关于 $w$ 和输入协方差矩阵 $\\Sigma$ 的期望突触更新的平均场动力学方程。然后，在 $w$ 与 $\\Sigma$ 的主特征向量 $v_{1}$ (与最大特征值 $\\lambda_{1}0$ 相关联) 对齐的假设下，计算 $w^{\\ast}=a^{\\ast}v_{1}$ 中的非平凡不动点振幅 $a^{\\ast}0$。明确地展示衰减系数 $\\gamma$ 如何通过输入统计量 (特别是 $\\lambda_{1}$) 和饱和参数 $\\alpha$ 来设定稳态范数。你的最终答案应该是关于 $\\eta$、$\\gamma$、$\\alpha$ 和 $\\lambda_{1}$ 的 $a^{\\ast}$ 的闭式解析表达式。不需要进行数值计算，最终表达式中也不应包含单位。",
            "solution": "我们的目标是推导权重向量 $w$ 的平均场动力学方程，并求解其非平凡不动点的振幅。\n\n**1. 平均场动力学方程的推导**\n\n权重向量 $w$ 的随机动力学方程为：\n$$ \\dot{w} = \\eta\\,y\\,x - \\gamma\\,w $$\n将突触后活动 $y = \\phi(w^\\top x)$ 代入，我们得到：\n$$ \\dot{w} = \\eta\\,\\phi(w^\\top x)\\,x - \\gamma\\,w $$\n在平均场近似下，我们对权重动力学关于输入 $x$ 的平稳分布取期望，并将 $w$ 视为准静态。\n$$ \\dot{w} = \\mathbb{E}_x[\\eta\\,\\phi(w^\\top x)\\,x - \\gamma\\,w] = \\eta\\,\\mathbb{E}_x[\\phi(w^\\top x)\\,x] - \\gamma\\,w $$\n我们使用给定的三次近似 $\\phi(u) = u - \\alpha u^3$，其中 $u = w^\\top x$。期望项变为：\n$$ \\mathbb{E}_x[\\phi(w^\\top x)\\,x] = \\mathbb{E}_x[(w^\\top x - \\alpha(w^\\top x)^3)x] = \\mathbb{E}_x[(w^\\top x)x] - \\alpha \\mathbb{E}_x[(w^\\top x)^3 x] $$\n我们分别计算这两项。输入 $x$ 是零均值高斯分布，$x \\sim \\mathcal{N}(0, \\Sigma)$。\n\n**第一项**：$\\mathbb{E}_x[(w^\\top x)x]$\n$$ \\mathbb{E}_x[x(x^\\top w)] = \\mathbb{E}_x[xx^\\top]w = \\Sigma w $$\n\n**第二项**：$\\mathbb{E}_x[(w^\\top x)^3 x]$\n令 $u = w^\\top x$。由于 $x$ 是零均值高斯向量，$u$ 是一个零均值高斯标量。我们需要计算 $\\mathbb{E}[u^3 x]$。对于零均值联合高斯变量，我们可以使用伊塞利斯定理（Isserlis' theorem）。$\\mathbb{E}[u^3 x]$ 的第 $j$ 个分量是 $\\mathbb{E}[u \\cdot u \\cdot u \\cdot x_j]$。\n$$ \\mathbb{E}[u \\cdot u \\cdot u \\cdot x_j] = \\mathbb{E}[u u]\\mathbb{E}[u x_j] + \\mathbb{E}[u u]\\mathbb{E}[u x_j] + \\mathbb{E}[u u]\\mathbb{E}[u x_j] = 3 \\mathbb{E}[u^2] \\mathbb{E}[u x_j] $$\n我们需要计算 $\\mathbb{E}[u^2]$ 和 $\\mathbb{E}[u x]$。\n- $\\mathbb{E}[u^2] = \\mathbb{E}[(w^\\top x)^2] = w^\\top \\mathbb{E}[xx^\\top] w = w^\\top \\Sigma w$。\n- $\\mathbb{E}[ux] = \\mathbb{E}[(w^\\top x)x] = \\Sigma w$。\n\n将这些结果组合，得到：\n$$ \\mathbb{E}_x[(w^\\top x)^3 x] = 3 \\mathbb{E}[u^2] \\mathbb{E}[ux] = 3(w^\\top \\Sigma w)(\\Sigma w) $$\n现在我们将这两项代回期望表达式中：\n$$ \\mathbb{E}_x[\\phi(w^\\top x)\\,x] = \\Sigma w - 3\\alpha(w^\\top \\Sigma w)\\Sigma w $$\n最后，将此结果代入平均场动力学方程，得到：\n$$ \\dot{w} = \\eta \\left[ \\Sigma w - 3\\alpha(w^\\top \\Sigma w)\\Sigma w \\right] - \\gamma w $$\n\n**2. 非平凡不动点振幅的计算**\n\n我们假设权重向量与 $\\Sigma$ 的主特征向量 $v_1$ 对齐，即 $w(t) = a(t) v_1$，其中 $\\Sigma v_1 = \\lambda_1 v_1$ 且 $v_1^\\top v_1 = 1$。不动点 $w^*$ 将具有 $w^* = a^* v_1$ 的形式。\n\n将 $w = a v_1$ 代入平均场方程：\n$$ \\frac{d}{dt}(a v_1) = \\eta \\left[ \\Sigma (a v_1) - 3\\alpha((a v_1)^\\top \\Sigma (a v_1))\\Sigma (a v_1) \\right] - \\gamma (a v_1) $$\n左侧为 $\\dot{a}v_1$。利用特征向量属性 $\\Sigma v_1 = \\lambda_1 v_1$ 来简化右侧：\n- $\\Sigma(a v_1) = a(\\Sigma v_1) = a \\lambda_1 v_1$\n- $(a v_1)^\\top \\Sigma (a v_1) = a^2 v_1^\\top (\\Sigma v_1) = a^2 v_1^\\top (\\lambda_1 v_1) = a^2 \\lambda_1 (v_1^\\top v_1) = a^2 \\lambda_1$\n- 三次项变为: $3\\alpha (a^2 \\lambda_1)(a \\lambda_1 v_1) = 3\\alpha a^3 \\lambda_1^2 v_1$\n\n代回方程：\n$$ \\dot{a}v_1 = \\eta \\left[ a \\lambda_1 v_1 - 3\\alpha a^3 \\lambda_1^2 v_1 \\right] - \\gamma a v_1 $$\n提出公因子 $v_1$：\n$$ \\dot{a}v_1 = \\left( \\eta a \\lambda_1 - 3\\eta \\alpha a^3 \\lambda_1^2 - \\gamma a \\right) v_1 $$\n这简化为关于振幅 $a(t)$ 的标量微分方程：\n$$ \\dot{a} = (\\eta \\lambda_1 - \\gamma)a - 3\\eta \\alpha \\lambda_1^2 a^3 $$\n我们通过设置 $\\dot{a} = 0$ 来寻找不动点 $a^*$：\n$$ 0 = (\\eta \\lambda_1 - \\gamma)a^* - 3\\eta \\alpha \\lambda_1^2 (a^*)^3 = a^* \\left[ (\\eta \\lambda_1 - \\gamma) - 3\\eta \\alpha \\lambda_1^2 (a^*)^2 \\right] $$\n此方程有一个平凡解 $a^* = 0$。非平凡解（$a^*  0$）要求括号内的项为零：\n$$ (\\eta \\lambda_1 - \\gamma) - 3\\eta \\alpha \\lambda_1^2 (a^*)^2 = 0 $$\n求解 $(a^*)^2$：\n$$ (a^*)^2 = \\frac{\\eta \\lambda_1 - \\gamma}{3\\eta \\alpha \\lambda_1^2} $$\n要使一个实的、非零解存在，分子必须为正，即 $\\eta \\lambda_1  \\gamma$。取正平方根，我们得到非平凡不动点振幅：\n$$ a^* = \\sqrt{\\frac{\\eta \\lambda_1 - \\gamma}{3\\eta \\alpha \\lambda_1^2}} $$\n这就是所求的 $a^*$ 的解析表达式。",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{\\eta \\lambda_{1} - \\gamma}{3\\eta \\alpha \\lambda_{1}^{2}}}}\n$$"
        }
    ]
}