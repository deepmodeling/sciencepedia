{
    "hands_on_practices": [
        {
            "introduction": "我们从神经计算的基本单元——突触开始。本练习旨在探索神经递质释放的随机性，这是大脑功能的关键过程。通过比较基于物理原理的二项分布模型及其常见的泊松近似，你将更深刻地理解建模假设如何影响我们对突触可变性的认识 。",
            "id": "2738674",
            "problem": "一个突触末梢包含 $n=5$ 个独立的、功能相同的释放位点。响应一个短暂的突触前动作电位，每个位点以概率 $p=0.2$ 释放最多一个突触小泡，且与其他位点和其他试验相互独立。令 $K$ 表示在单次试验（一个动作电位）中释放的小泡数量。假设突触后检测是完美的，因此 $K$ 可以直接作为每次试验的小泡计数被观测到。\n\n从细胞和分子神经科学以及概率论的第一性原理出发——即每个位点释放的独立伯努利试验、微观状态的组合计数，以及期望、方差和定义为 $\\mathrm{Var}(K)/\\mathbb{E}[K]$ 的法诺因子的定义——推导 $K$ 在 $k=0,1,2,3,4,5$ 时的精确计数分布，并计算其均值、方差和法诺因子。然后考虑一个具有相同均值 $\\lambda = n p$ 的神经递质释放的泊松过程模型。比较二项位点模型的均值、方差和法诺因子与泊松模型的相应值。\n\n报告二项位点模型的法诺因子与均值匹配的泊松模型的法诺因子之间的绝对差作为你的最终答案。将你的答案四舍五入到 $4$ 位有效数字，并以无单位的纯数字形式报告。",
            "solution": "对问题陈述进行验证。\n\n逐字提取的已知条件如下：\n- 独立的、功能相同的释放位点数量：$n=5$。\n- 每个位点每次试验的释放概率：$p=0.2$。\n- $K$ 是单次试验中释放的小泡数量。\n- 将考虑一个均值为 $\\lambda = n p$ 的泊松过程模型。\n- 法诺因子定义为 $\\mathrm{Var}(K)/\\mathbb{E}[K]$。\n- 最终答案是二项位点模型的法诺因子与均值匹配的泊松模型的法诺因子之间的绝对差，四舍五入到 $4$ 位有效数字。\n\n验证评估：\n- **科学依据充分：** 该问题使用了突触传递的二项模型，这是由 Katz 及其同事建立的细胞神经科学中的一个基本概念。与泊松分布的比较是该领域中一个标准且信息丰富的分析。参数 $n=5$ 和 $p=0.2$ 对于某些类型的突触处于现实范围内。该问题在科学上是合理的。\n- **问题明确：** 提供了所有必要的参数和定义。任务明确，能够导出一个唯一、稳定且有意义的解。\n- **客观性：** 语言正式、精确，没有主观或推测性内容。\n\n结论：该问题有效。它提出了一个在定量神经科学和概率论中标准的、定义明确的练习。可以开始求解过程。\n\n该问题描述了一个包含 $n=5$ 次独立伯努利试验的过程，其中每次试验（一个位点的潜在释放）的成功概率为 $p=0.2$。因此，总成功次数（释放的小泡数）$K$ 服从二项分布，$K \\sim \\mathrm{Binomial}(n, p)$。\n\n二项分布的概率质量函数 (PMF) 由下式给出：\n$$P(K=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n其中 $k$ 是成功次数，取值范围从 $0$ 到 $n$。\n\n首先，我们计算在给定 $n=5$ 和 $p=0.2$ 的情况下 $K$ 的精确计数分布。这需要计算当 $k \\in \\{0, 1, 2, 3, 4, 5\\}$ 时 的 $P(K=k)$。注意 $1-p = 1 - 0.2 = 0.8$。\n\n- 对于 $k=0$：$P(K=0) = \\binom{5}{0} (0.2)^0 (0.8)^5 = 1 \\times 1 \\times 0.32768 = 0.32768$\n- 对于 $k=1$：$P(K=1) = \\binom{5}{1} (0.2)^1 (0.8)^4 = 5 \\times 0.2 \\times 0.4096 = 0.4096$\n- 对于 $k=2$：$P(K=2) = \\binom{5}{2} (0.2)^2 (0.8)^3 = 10 \\times 0.04 \\times 0.512 = 0.2048$\n- 对于 $k=3$：$P(K=3) = \\binom{5}{3} (0.2)^3 (0.8)^2 = 10 \\times 0.008 \\times 0.64 = 0.0512$\n- 对于 $k=4$：$P(K=4) = \\binom{5}{4} (0.2)^4 (0.8)^1 = 5 \\times 0.0016 \\times 0.8 = 0.0064$\n- 对于 $k=5$：$P(K=5) = \\binom{5}{5} (0.2)^5 (0.8)^0 = 1 \\times 0.00032 \\times 1 = 0.00032$\n\n接下来，我们计算这个二项模型的均值、方差和法诺因子。\n二项分布的期望（均值）由下式给出：\n$$\\mathbb{E}[K] = np$$\n代入给定值：\n$$\\mathbb{E}[K] = 5 \\times 0.2 = 1$$\n\n二项分布的方差由下式给出：\n$$\\mathrm{Var}(K) = np(1-p)$$\n代入给定值：\n$$\\mathrm{Var}(K) = 5 \\times 0.2 \\times (1 - 0.2) = 1 \\times 0.8 = 0.8$$\n\n二项模型的法诺因子 $\\mathrm{FF}_{\\text{Binomial}}$ 是方差与均值的比值：\n$$\\mathrm{FF}_{\\text{Binomial}} = \\frac{\\mathrm{Var}(K)}{\\mathbb{E}[K]} = \\frac{np(1-p)}{np} = 1-p$$\n使用给定的 $p$ 值：\n$$\\mathrm{FF}_{\\text{Binomial}} = 1 - 0.2 = 0.8$$\n\n现在，我们考虑一个均值 $\\lambda$ 与二项模型均值相匹配的泊松过程模型。令 $K_{\\text{Poisson}}$ 为该模型的随机变量。\n匹配条件是：\n$$\\lambda = \\mathbb{E}[K] = np = 1$$\n因此，我们考虑一个参数为 $\\lambda=1$ 的泊松分布。\n\n泊松分布的基本性质是其均值和方差都等于其参数 $\\lambda$。\n泊松模型的均值为：\n$$\\mathbb{E}[K_{\\text{Poisson}}] = \\lambda = 1$$\n泊松模型的方差为：\n$$\\mathrm{Var}(K_{\\text{Poisson}}) = \\lambda = 1$$\n\n因此，泊松模型的法诺因子 $\\mathrm{FF}_{\\text{Poisson}}$ 为：\n$$\\mathrm{FF}_{\\text{Poisson}} = \\frac{\\mathrm{Var}(K_{\\text{Poisson}})}{\\mathbb{E}[K_{\\text{Poisson}}]} = \\frac{\\lambda}{\\lambda} = 1$$\n这个结果对于任何泊松过程都是定义性的；其方差总是等于其均值，从而得到单位为1的法诺因子。\n\n最后的任务是计算二项位点模型的法诺因子与均值匹配的泊松模型的法诺因子之间的绝对差。\n$$\\Delta \\mathrm{FF} = |\\mathrm{FF}_{\\text{Binomial}} - \\mathrm{FF}_{\\text{Poisson}}|$$\n代入计算出的值：\n$$\\Delta \\mathrm{FF} = |0.8 - 1| = |-0.2| = 0.2$$\n\n问题要求答案四舍五入到 $4$ 位有效数字。\n$$0.2 \\rightarrow 0.2000$$\n\n这个差异突显了二项释放模型的一个关键特征：因为释放位点的数量 $n$ 是有限的，所以方差相对于均值受到抑制。该释放过程是“亚泊松”的，其法诺因子严格小于 $1$（对于 $p0$）。与泊松模型的法诺因子 $1$ 相比，这种偏差的大小就是释放概率 $p$。",
            "answer": "$$\\boxed{0.2000}$$"
        },
        {
            "introduction": "从突触到神经元，我们常常需要在进一步分析之前对原始脉冲数据进行预处理。本实践将深入探讨对神经活动应用非线性变换的后果，这是方差稳定化等任务中的常见步骤。你将借助詹森不等式 (Jensen's inequality) 的视角，分析并量化这种变换如何系统性地改变神经元平均发放率与变换后信号统计量之间的关系 。",
            "id": "4160976",
            "problem": "一个单神经元的脉冲序列被离散化为宽度为 $\\Delta t$ 的小时间窗口，使得每个窗口内最多出现一个脉冲。令 $X$ 表示在给定时间窗口内的二元脉冲指示变量，其中 $X \\in \\{0,1\\}$ 且 $X \\sim \\mathrm{Bernoulli}(p)$，放电概率为 $p \\in [0,1]$。在点过程数据的常见预处理步骤中，会应用一个压缩非线性变换 $g$ 来稳定方差。考虑凹变换 $g(x) = \\sqrt{x}$，它在 $[0,1]$ 上有良好定义，并反映了神经数据分析中使用的一种压缩映射。\n\n仅使用期望和凹性的定义，构建一个明确的例子，证明关联 $g(\\mathbb{E}[X])$ 和 $\\mathbb{E}[g(X)]$ 的不等式相对于对凸变换成立的形式其方向是相反的。然后，对于上述伯努利输入，计算间隙\n$$\\Delta(p) \\equiv g(\\mathbb{E}[X]) - \\mathbb{E}[g(X)],$$\n的精确解析表达式，作为 $p \\in [0,1]$ 的函数。请以 $p$ 的闭式表达式形式给出最终答案。不需要进行数值近似或四舍五入。",
            "solution": "该问题陈述具有科学依据、适定且客观。它基于计算神经科学和概率论中的标准模型和概念，特别是用于脉冲序列的伯努利过程以及对随机变量应用变换。所有必要信息均已提供，问题没有矛盾或歧义。这是一个有效的问题。\n\n该问题要求我们分析凹变换 $g(x) = \\sqrt{x}$ 对服从伯努利分布的随机变量 $X$ 的影响。问题的核心在于应用期望的定义和理解琴生不等式（Jensen's inequality）。对于凹函数 $g$，琴生不等式表明 $\\mathbb{E}[g(X)] \\le g(\\mathbb{E}[X])$。我们被要求首先提供一个这方面的例子，然后计算精确的差值 $\\Delta(p) = g(\\mathbb{E}[X]) - \\mathbb{E}[g(X)]$。\n\n设随机变量 $X$ 表示脉冲指示变量，使得 $X \\sim \\mathrm{Bernoulli}(p)$。$X$ 的概率质量函数 (PMF) 由下式给出：\n$$\nP(X=k) = \\begin{cases}\np  \\text{ if } k=1 \\\\\n1-p  \\text{ if } k=0\n\\end{cases}\n$$\n$X$ 的支撑集是集合 $\\{0, 1\\}$。\n\n首先，我们计算 $X$ 的期望，记为 $\\mathbb{E}[X]$。根据定义，对于一个离散随机变量，期望是每个可能值乘以其概率的总和：\n$$\n\\mathbb{E}[X] = \\sum_{k \\in \\{0,1\\}} k \\cdot P(X=k) = (0 \\cdot P(X=0)) + (1 \\cdot P(X=1))\n$$\n从 PMF 中代入概率：\n$$\n\\mathbb{E}[X] = (0 \\cdot (1-p)) + (1 \\cdot p) = 0 + p = p\n$$\n\n接下来，我们计算项 $g(\\mathbb{E}[X])$。函数 $g$ 定义为 $g(x) = \\sqrt{x}$。将此函数应用于 $X$ 的期望：\n$$\ng(\\mathbb{E}[X]) = g(p) = \\sqrt{p}\n$$\n\n现在，我们计算变换后随机变量的期望 $\\mathbb{E}[g(X)]$。当 $X=0$ 时，随机变量 $g(X)$ 的取值为 $g(0) = \\sqrt{0} = 0$；当 $X=1$ 时，其取值为 $g(1) = \\sqrt{1} = 1$。这些结果的概率与 $X$ 的相同：$P(g(X)=0) = P(X=0) = 1-p$ 以及 $P(g(X)=1) = P(X=1) = p$。\n\n对随机变量 $g(X)$ 使用期望的定义：\n$$\n\\mathbb{E}[g(X)] = \\sum_{k \\in \\{0,1\\}} g(k) \\cdot P(X=k) = (g(0) \\cdot P(X=0)) + (g(1) \\cdot P(X=1))\n$$\n代入 $g(k)$ 的值和概率：\n$$\n\\mathbb{E}[g(X)] = (\\sqrt{0} \\cdot (1-p)) + (\\sqrt{1} \\cdot p) = (0 \\cdot (1-p)) + (1 \\cdot p) = 0 + p = p\n$$\n\n问题的第一部分要求给出一个明确的例子，用以说明凹函数的不等式。我们可以使用我们推导出的表达式。对于凹函数 $g$，琴生不等式为 $\\mathbb{E}[g(X)] \\le g(\\mathbb{E}[X])$。使用我们的结果，这可以转换为 $p \\le \\sqrt{p}$。这个不等式对所有 $p \\in [0, 1]$ 都成立。让我们在开区间 $(0, 1)$ 中选择一个特定的 $p$ 值来构建一个非平凡的例子，例如 $p = \\frac{1}{4}$。\n对于 $p = \\frac{1}{4}$：\n$\\mathbb{E}[X] = \\frac{1}{4}$。\n$g(\\mathbb{E}[X]) = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}$。\n$\\mathbb{E}[g(X)] = p = \\frac{1}{4}$。\n因此，我们有 $\\mathbb{E}[g(X)] = \\frac{1}{4}$ 和 $g(\\mathbb{E}[X]) = \\frac{1}{2}$，不等式 $\\frac{1}{4} \\le \\frac{1}{2}$ 成立，这证实了琴生不等式在此特定情况下的正确性。严格不等式成立，因为该随机变量不是一个常数，且函数在 $(0, \\infty)$ 上是严格凹的。\n\n第二部分是计算间隙 $\\Delta(p)$ 的解析表达式，其定义为：\n$$\n\\Delta(p) \\equiv g(\\mathbb{E}[X]) - \\mathbb{E}[g(X)]\n$$\n代入我们为 $g(\\mathbb{E}[X])$ 和 $\\mathbb{E}[g(X)]$ 推导出的表达式：\n$$\n\\Delta(p) = \\sqrt{p} - p\n$$\n这个表达式代表了变换后均值与变换变量均值之间的差值。如前所述，对于 $p \\in (0, 1)$，有 $\\sqrt{p}  p$，所以 $\\Delta(p)  0$。间隙仅在区间的边界处，即在 $p=0$ 和 $p=1$ 时为零，此时随机变量变为确定性的。\n间隙作为 $p$ 的函数的最终闭式表达式为 $\\sqrt{p} - p$。",
            "answer": "$$\\boxed{\\sqrt{p} - p}$$"
        },
        {
            "introduction": "系统神经科学的一个核心挑战是从神经元群体的集体活动中“解码”大脑状态。本实践将指导你从第一性原理出发，构建一个线性解码器，利用贝叶斯推断从群体发放率中估计一个连续变量（如手臂伸展方向）。你将推导其解析解并予以实现，从而架起概率论与实用神经数据分析工具之间的桥梁 。",
            "id": "4011627",
            "problem": "给定一个包含$N$个神经元的群体，其放电率在线性高斯观测框架下建模，用于解码一个一维的伸展方向变量。时间$t$的瞬时伸展方向用$x_t$表示，并被视为一个以弧度为单位的标量角度。时间$t$的群体放电率向量用$\\mathbf{r}_t \\in \\mathbb{R}^N$表示，单位为每秒脉冲数 (spk/s)。生成模型由以下几个部分组成：\n\n1. 关于潜伸展方向的高斯先验，\n$ p(x_t) = \\mathcal{N}(x_t \\mid \\mu_0, \\sigma_0^2), $\n其中$x_t$的单位是弧度，$\\mu_0$的单位是弧度，$\\sigma_0^2$的单位是弧度的平方。\n\n2. 在给定潜变量$x_t$的情况下，关于群体放电率的多元高斯似然（观测模型），\n$ p(\\mathbf{r}_t \\mid x_t) = \\mathcal{N}(\\mathbf{r}_t \\mid \\mathbf{C} x_t + \\mathbf{d}, \\boldsymbol{\\Sigma}), $\n其中$\\mathbf{C} \\in \\mathbb{R}^N$编码了每个神经元的线性调谐斜率，单位为spk/s每弧度；$\\mathbf{d} \\in \\mathbb{R}^N$编码了基线放电率，单位为spk/s；$\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{N \\times N}$是一个正定协方差矩阵，单位为$(\\text{spk/s})^2$。\n\n任务：\n- 从贝叶斯准则和多元正态分布的基本定义出发，推导后验分布$p(x_t \\mid \\mathbf{r}_t)$，并将其均值和方差表示为$\\mu_0$、$\\sigma_0^2$、$\\mathbf{C}$、$\\mathbf{d}$、$\\boldsymbol{\\Sigma}$以及观测值$\\mathbf{r}_t$的显式函数。\n- 从$\\mathbf{r}_t$构建一个用于$x_t$的线性解码器，该解码器在所述模型下等于后验均值。将此解码器表示为仿射映射$ \\hat{x}_t = b + \\mathbf{w}^\\top \\mathbf{r}_t $，并用模型参数提供偏置$b$（单位为弧度）和权重$\\mathbf{w} \\in \\mathbb{R}^N$（单位为弧度每spk/s）的显式表达式。\n\n全文中角度单位必须为弧度。放电率单位为spk/s。协方差单位为$(\\text{spk/s})^2$。\n\n使用以下$N = 4$的固定参数值：\n- 先验参数：$ \\mu_0 = 0.3 $（弧度），$ \\sigma_0^2 = 0.04 $（弧度的平方）。\n- 调谐斜率：$ \\mathbf{C} = [0.7,\\,-0.5,\\,0.3,\\,0.1] $（spk/s每弧度）。\n- 基线放电率：$ \\mathbf{d} = [15.0,\\,10.0,\\,12.0,\\,8.0] $（spk/s）。\n- 观测噪声协方差：\n$$\n\\boldsymbol{\\Sigma} =\n\\begin{bmatrix}\n4.0  0.6  0.2  0.1 \\\\\n0.6  3.0  0.4  0.2 \\\\\n0.2  0.4  2.5  0.3 \\\\\n0.1  0.2  0.3  3.5 \\\\\n\\end{bmatrix}\n\\quad \\text{(spk/s)}^2.\n$$\n\n测试套件（所有放电率单位均为spk/s）：\n- 情况1（基线）：$ \\mathbf{r}_1 = [15.0,\\,10.0,\\,12.0,\\,8.0] $。\n- 情况2（沿调谐方向的正向偏差）：$ \\mathbf{r}_2 = \\mathbf{d} + 2 \\mathbf{C} = [16.4,\\,9.0,\\,12.6,\\,8.2] $。\n- 情况3（沿调谐方向的负向偏差）：$ \\mathbf{r}_3 = \\mathbf{d} - 1.5 \\mathbf{C} = [13.95,\\,10.75,\\,11.55,\\,7.85] $。\n\n你的程序必须：\n1. 计算每个测试用例中$x_t$的后验均值和方差。\n2. 计算解码器的偏置$b$和权重$\\mathbf{w}$，使得后验均值等于$ b + \\mathbf{w}^\\top \\mathbf{r}_t $。\n3. 生成单行输出，其中包含用逗号分隔并用方括号括起来的结果列表，顺序必须严格如下：\n$ [\\mu_{\\text{post},1},\\,\\sigma^2_{\\text{post},1},\\,\\mu_{\\text{post},2},\\,\\sigma^2_{\\text{post},2},\\,\\mu_{\\text{post},3},\\,\\sigma^2_{\\text{post},3},\\,b,\\,w_1,\\,w_2,\\,w_3,\\,w_4] $。\n所有角度（$\\mu_{\\text{post},i}$和$b$）的单位必须是弧度，所有方差（$\\sigma^2_{\\text{post},i}$）的单位必须是弧度的平方。解码器权重$w_i$的单位必须是弧度每spk/s。\n\n不允许外部输入；在程序中硬编码给定的参数和测试套件。程序必须是自包含的，并且可以直接运行。",
            "solution": "该问题是有效的。这是一个计算神经科学领域中定义明确、有科学依据的问题，涉及使用线性高斯模型进行贝叶斯推断。所有必要的参数和条件都已提供，不存在内部矛盾或歧义。\n\n### 后验分布的推导\n\n我们的目标是在给定观测到的神经群体放电率向量$\\mathbf{r}_t$的情况下，确定潜伸展方向$x_t$的后验分布，记为$p(x_t \\mid \\mathbf{r}_t)$。根据贝叶斯准则，后验分布正比于似然和先验的乘积：\n$$ p(x_t \\mid \\mathbf{r}_t) \\propto p(\\mathbf{r}_t \\mid x_t) p(x_t) $$\n问题定义了一个高斯先验和一个高斯似然：\n1.  先验：$ p(x_t) = \\mathcal{N}(x_t \\mid \\mu_0, \\sigma_0^2) $\n2.  似然：$ p(\\mathbf{r}_t \\mid x_t) = \\mathcal{N}(\\mathbf{r}_t \\mid \\mathbf{C} x_t + \\mathbf{d}, \\boldsymbol{\\Sigma}) $\n\n两个高斯分布的乘积是一个未归一化的高斯分布。为了找到所得后验分布的参数，我们处理概率密度函数的指数部分。高斯分布$\\mathcal{N}(z \\mid \\mu, \\Sigma)$的概率密度函数（PDF）正比于$\\exp(-\\frac{1}{2}(z-\\mu)^\\top \\Sigma^{-1}(z-\\mu))$。\n\n先验分布$p(x_t)$的指数部分是：\n$$ \\mathcal{L}_{\\text{prior}} = -\\frac{1}{2\\sigma_0^2}(x_t - \\mu_0)^2 = -\\frac{1}{2}\\left( \\frac{1}{\\sigma_0^2}x_t^2 - \\frac{2\\mu_0}{\\sigma_0^2}x_t \\right) + \\text{const}_1 $$\n似然分布$p(\\mathbf{r}_t \\mid x_t)$的指数部分是：\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} (\\mathbf{r}_t - (\\mathbf{C} x_t + \\mathbf{d}))^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - (\\mathbf{C} x_t + \\mathbf{d})) $$\n我们展开似然指数中的二次型，重点关注包含$x_t$的项：\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} ((\\mathbf{r}_t - \\mathbf{d}) - \\mathbf{C}x_t)^\\top \\boldsymbol{\\Sigma}^{-1} ((\\mathbf{r}_t - \\mathbf{d}) - \\mathbf{C}x_t) $$\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} \\left( x_t \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} x_t - x_t \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}) - (\\mathbf{r}_t - \\mathbf{d})^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} x_t \\right) + \\text{const}_2 $$\n由于所有项都是标量，我们可以合并$x_t$的两个线性项：\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} \\left( (\\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C})x_t^2 - 2(\\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}))x_t \\right) + \\text{const}_2 $$\n\n后验分布的指数$\\mathcal{L}_{\\text{post}}$是先验和似然指数的和。我们按$x_t$的幂次收集项：\n$$ \\mathcal{L}_{\\text{post}} = \\mathcal{L}_{\\text{prior}} + \\mathcal{L}_{\\text{like}} $$\n$$ \\mathcal{L}_{\\text{post}} = -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C}\\right)x_t^2 - 2\\left(\\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d})\\right)x_t \\right] + \\text{const}_3 $$\n这是关于$x_t$的高斯分布的指数，我们将其表示为$p(x_t \\mid \\mathbf{r}_t) = \\mathcal{N}(x_t \\mid \\mu_{\\text{post}}, \\sigma^2_{\\text{post}})$。该后验分布的指数具有标准形式：\n$$ -\\frac{1}{2\\sigma^2_{\\text{post}}}(x_t - \\mu_{\\text{post}})^2 = -\\frac{1}{2}\\left( \\frac{1}{\\sigma^2_{\\text{post}}}x_t^2 - \\frac{2\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}}x_t \\right) + \\text{const}_4 $$\n通过匹配$x_t^2$和$x_t$项的系数，我们确定了后验精度$(\\sigma^2_{\\text{post}})^{-1}$和均值$\\mu_{\\text{post}}$。\n\n$x_t^2$的系数给出了后验精度：\n$$ \\frac{1}{\\sigma^2_{\\text{post}}} = \\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} $$\n因此，后验方差是：\n$$ \\sigma^2_{\\text{post}} = \\left( \\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} \\right)^{-1} $$\n注意，后验方差与观测值$\\mathbf{r}_t$无关。\n\n$x_t$的系数给出：\n$$ \\frac{2\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}} = 2\\left(\\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d})\\right) $$\n解出后验均值$\\mu_{\\text{post}}$：\n$$ \\mu_{\\text{post}} = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}) \\right) $$\n\n### 线性解码器的推导\n\n在平方误差损失下，后验均值$\\mu_{\\text{post}}$是$x_t$的最优贝叶斯估计量。我们将线性解码器$\\hat{x}_t$定义为该后验均值，即$\\hat{x}_t = \\mu_{\\text{post}}$。我们需要将其表示为仿射形式$\\hat{x}_t = b + \\mathbf{w}^\\top \\mathbf{r}_t$。\n我们展开$\\mu_{\\text{post}}$的表达式并重新整理：\n$$ \\mu_{\\text{post}} = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{r}_t - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right) $$\n$$ \\mu_{\\text{post}} = \\underbrace{\\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right)}_{b} + \\underbrace{\\left(\\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1}\\right)}_{\\mathbf{w}^\\top} \\mathbf{r}_t $$\n从这个重新整理的式子中，我们可以确定偏置$b$和权重向量$\\mathbf{w}$。\n\n偏置项$b$是：\n$$ b = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right) $$\n权重的行向量$\\mathbf{w}^\\top$是：\n$$ \\mathbf{w}^\\top = \\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} $$\n取转置，我们得到权重的列向量$\\mathbf{w}$：\n$$ \\mathbf{w} = (\\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1})^\\top = \\sigma^2_{\\text{post}} (\\boldsymbol{\\Sigma}^{-1})^\\top (\\mathbf{C}^\\top)^\\top = \\sigma^2_{\\text{post}} \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} $$\n最后一步利用了对称矩阵$\\boldsymbol{\\Sigma}$的逆矩阵也是对称的这一事实，即$(\\boldsymbol{\\Sigma}^{-1})^\\top = \\boldsymbol{\\Sigma}^{-1}$。这些关于$b$和$\\mathbf{w}$的公式允许从模型参数直接计算。\n\n### 数值计算\n现在将使用提供的数值来实现推导出的公式，以找出三个测试用例所需的量，以及恒定的解码器参数$b$和$\\mathbf{w}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the posterior distribution parameters and the linear decoder\n    parameters for a linear-Gaussian model of neural population decoding.\n    \"\"\"\n    # Define fixed parameters from the problem statement.\n    # N is the number of neurons.\n    N = 4\n    \n    # Prior parameters\n    mu0 = 0.3  # radians\n    sigma0_sq = 0.04  # radians^2\n\n    # Likelihood parameters\n    C = np.array([0.7, -0.5, 0.3, 0.1]).reshape(N, 1)  # spk/s per radian\n    d = np.array([15.0, 10.0, 12.0, 8.0]).reshape(N, 1)  # spk/s\n    Sigma = np.array([\n        [4.0, 0.6, 0.2, 0.1],\n        [0.6, 3.0, 0.4, 0.2],\n        [0.2, 0.4, 2.5, 0.3],\n        [0.1, 0.2, 0.3, 3.5]\n    ])  # (spk/s)^2\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (baseline): r1 = d\n        np.array([15.0, 10.0, 12.0, 8.0]).reshape(N, 1),\n        # Case 2 (positive deviation): r2 = d + 2*C\n        np.array([16.4, 9.0, 12.6, 8.2]).reshape(N, 1),\n        # Case 3 (negative deviation): r3 = d - 1.5*C\n        np.array([13.95, 10.75, 11.55, 7.85]).reshape(N, 1)\n    ]\n\n    # --- Step 1: Compute posterior variance ---\n    # This value is constant for all observations in a linear-Gaussian model.\n    # Pre-compute the inverse of the covariance matrix and the prior precision.\n    Sigma_inv = np.linalg.inv(Sigma)\n    prior_precision = 1.0 / sigma0_sq\n    \n    # Compute the term C^T * Sigma^-1 * C, which contributes to the likelihood's precision.\n    # .item() is used to extract the scalar value from the 1x1 matrix.\n    likelihood_precision_term = (C.T @ Sigma_inv @ C).item()\n    \n    # The posterior precision is the sum of the prior and likelihood precisions.\n    posterior_precision = prior_precision + likelihood_precision_term\n    \n    # The posterior variance is the reciprocal of the posterior precision.\n    sigma2_post = 1.0 / posterior_precision\n\n    # --- Step 2: Compute decoder bias (b) and weights (w) ---\n    # According to the derived formula: w = sigma^2_post * Sigma^-1 * C\n    w = sigma2_post * (Sigma_inv @ C)\n    \n    # According to the derived formula: b = sigma^2_post * (mu0/sigma0^2 - C^T * Sigma^-1 * d)\n    b = sigma2_post * (prior_precision * mu0 - (C.T @ Sigma_inv @ d).item())\n    \n    # --- Step 3: Compute posterior mean for each test case ---\n    # The posterior mean can be calculated using the decoder: mu_post = b + w^T * r\n    results = []\n    for r_t in test_cases:\n        mu_post = (b + w.T @ r_t).item()\n        results.extend([mu_post, sigma2_post])\n    \n    # --- Step 4: Assemble the final output list ---\n    # The final list includes results for the 3 cases, followed by the decoder parameters.\n    # w.flatten().tolist() converts the weight column vector to a list of scalars.\n    results.extend([b] + w.flatten().tolist())\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.12f}' for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}