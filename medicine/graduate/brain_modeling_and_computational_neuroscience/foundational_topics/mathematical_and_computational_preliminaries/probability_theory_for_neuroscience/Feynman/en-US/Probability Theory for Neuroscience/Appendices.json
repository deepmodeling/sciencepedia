{
    "hands_on_practices": [
        {
            "introduction": "This exercise explores the probabilistic nature of neurotransmitter release, a fundamental process in neural communication. By modeling release with a binomial distribution and comparing it to the simpler Poisson model, you will develop an intuition for how biophysical constraints shape neural variability. This practice  is crucial for understanding the origins of noise in the nervous system and for using key statistical measures like the Fano factor to characterize it.",
            "id": "2738674",
            "problem": "A single synaptic terminal contains $n=5$ independent, functionally identical release sites. In response to a brief presynaptic action potential, each site releases at most one synaptic vesicle with probability $p=0.2$, independently of other sites and trials. Let $K$ denote the number of vesicles released in a single trial (one action potential). Assume that the postsynaptic detection is perfect so that $K$ is directly observed as the vesicle count per trial.\n\nStarting from first principles appropriate to cellular and molecular neuroscience and probability theory—namely, independent Bernoulli trials for release at each site, combinatorial counting of microstates, and definitions of expectation, variance, and the Fano factor defined as $\\mathrm{Var}(K)/\\mathbb{E}[K]$—derive the exact count distribution of $K$ for $k=0,1,2,3,4,5$, and compute its mean, variance, and Fano factor. Then consider a Poisson process model of neurotransmitter release with the same mean $\\lambda = n p$. Compare the mean, variance, and Fano factor of the binomial site model to those of the Poisson model.\n\nReport as your final answer the absolute difference between the Fano factor of the binomial site model and that of the matched-mean Poisson model. Round your answer to $4$ significant figures and report it as a pure number without units.",
            "solution": "The problem is valid.\n\nGivens extracted verbatim are:\n- Number of independent, functionally identical release sites: $n=5$.\n- Probability of release at each site per trial: $p=0.2$.\n- $K$ is the number of vesicles released in a single trial.\n- A Poisson process model is to be considered with a mean $\\lambda = n p$.\n- The Fano factor is defined as $\\mathrm{Var}(K)/\\mathbb{E}[K]$.\n- The final answer is the absolute difference between the Fano factor of the binomial site model and that of the matched-mean Poisson model, rounded to $4$ significant figures.\n\nValidation assessment:\n- **Scientifically Grounded:** The problem uses the binomial model of synaptic transmission, a foundational concept in cellular neuroscience established by Katz and colleagues. The comparison with a Poisson distribution is a standard and informative analysis in this field. The parameters $n=5$ and $p=0.2$ are within a realistic range for certain types of synapses. The problem is scientifically sound.\n- **Well-Posed:** All necessary parameters and definitions are provided. The tasks are explicit and lead to a unique, stable, and meaningful solution.\n- **Objective:** The language is formal, precise, and devoid of subjective or speculative content.\n\nVerdict: The problem is valid. It presents a standard, well-defined exercise in quantitative neuroscience and probability theory.\n\nThe problem describes a process of $n=5$ independent Bernoulli trials, where each trial (the potential release from one site) has a success probability of $p=0.2$. The total number of successes (released vesicles), $K$, therefore follows a binomial distribution, $K \\sim \\mathrm{Binomial}(n, p)$.\n\nThe probability mass function (PMF) for a binomial distribution is given by:\n$$P(K=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\nwhere $k$ is the number of successes, which can range from $0$ to $n$.\n\nFirst, we will compute the exact count distribution for $K$ given $n=5$ and $p=0.2$. This requires calculating $P(K=k)$ for $k \\in \\{0, 1, 2, 3, 4, 5\\}$. Note that $1-p = 1 - 0.2 = 0.8$.\n\n- For $k=0$: $P(K=0) = \\binom{5}{0} (0.2)^0 (0.8)^5 = 1 \\times 1 \\times 0.32768 = 0.32768$\n- For $k=1$: $P(K=1) = \\binom{5}{1} (0.2)^1 (0.8)^4 = 5 \\times 0.2 \\times 0.4096 = 0.4096$\n- For $k=2$: $P(K=2) = \\binom{5}{2} (0.2)^2 (0.8)^3 = 10 \\times 0.04 \\times 0.512 = 0.2048$\n- For $k=3$: $P(K=3) = \\binom{5}{3} (0.2)^3 (0.8)^2 = 10 \\times 0.008 \\times 0.64 = 0.0512$\n- For $k=4$: $P(K=4) = \\binom{5}{4} (0.2)^4 (0.8)^1 = 5 \\times 0.0016 \\times 0.8 = 0.0064$\n- For $k=5$: $P(K=5) = \\binom{5}{5} (0.2)^5 (0.8)^0 = 1 \\times 0.00032 \\times 1 = 0.00032$\n\nNext, we compute the mean, variance, and Fano factor for this binomial model.\nThe expectation (mean) of a binomial distribution is given by:\n$$\\mathbb{E}[K] = np$$\nSubstituting the given values:\n$$\\mathbb{E}[K] = 5 \\times 0.2 = 1$$\n\nThe variance of a binomial distribution is given by:\n$$\\mathrm{Var}(K) = np(1-p)$$\nSubstituting the given values:\n$$\\mathrm{Var}(K) = 5 \\times 0.2 \\times (1 - 0.2) = 1 \\times 0.8 = 0.8$$\n\nThe Fano factor for the binomial model, $\\mathrm{FF}_{\\text{Binomial}}$, is the ratio of the variance to the mean:\n$$\\mathrm{FF}_{\\text{Binomial}} = \\frac{\\mathrm{Var}(K)}{\\mathbb{E}[K]} = \\frac{np(1-p)}{np} = 1-p$$\nUsing the given value of $p$:\n$$\\mathrm{FF}_{\\text{Binomial}} = 1 - 0.2 = 0.8$$\n\nNow, we consider a Poisson process model with a mean $\\lambda$ matched to the mean of the binomial model. Let $K_{\\text{Poisson}}$ be the random variable for this model.\nThe matching condition is:\n$$\\lambda = \\mathbb{E}[K] = np = 1$$\nSo, we consider a Poisson distribution with parameter $\\lambda=1$.\n\nThe fundamental properties of a Poisson distribution are that its mean and variance are both equal to its parameter $\\lambda$.\nThe mean of the Poisson model is:\n$$\\mathbb{E}[K_{\\text{Poisson}}] = \\lambda = 1$$\nThe variance of the Poisson model is:\n$$\\mathrm{Var}(K_{\\text{Poisson}}) = \\lambda = 1$$\n\nThe Fano factor for the Poisson model, $\\mathrm{FF}_{\\text{Poisson}}$, is therefore:\n$$\\mathrm{FF}_{\\text{Poisson}} = \\frac{\\mathrm{Var}(K_{\\text{Poisson}})}{\\mathbb{E}[K_{\\text{Poisson}}]} = \\frac{\\lambda}{\\lambda} = 1$$\nThis result is definitional for any Poisson process; its variance is always equal to its mean, yielding a Fano factor of unity.\n\nThe final task is to compute the absolute difference between the Fano factor of the binomial site model and that of the matched-mean Poisson model.\n$$\\Delta \\mathrm{FF} = |\\mathrm{FF}_{\\text{Binomial}} - \\mathrm{FF}_{\\text{Poisson}}|$$\nSubstituting the calculated values:\n$$\\Delta \\mathrm{FF} = |0.8 - 1| = |-0.2| = 0.2$$\n\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$0.2 \\rightarrow 0.2000$$\n\nThis difference highlights a key feature of the binomial release model: because the number of release sites $n$ is finite, the variance is suppressed relative to the mean. The release process is \"sub-Poissonian\", with a Fano factor strictly less than $1$ (for $p0$). The magnitude of this deviation from the Poissonian Fano factor of $1$ is simply the release probability $p$.",
            "answer": "$$\\boxed{0.2000}$$"
        },
        {
            "introduction": "We now scale up from a single synapse to decoding information from an entire population of neurons. This exercise  guides you through building a linear decoder using Bayesian inference, a cornerstone technique in both systems neuroscience and the design of brain-computer interfaces. You will derive the posterior distribution of a movement variable from simulated neural firing rates, connecting abstract theory to a tangible engineering application.",
            "id": "4011627",
            "problem": "You are given a population of $N$ neurons with firing rates modeled under a linear Gaussian observation framework for decoding a one-dimensional reaching direction variable. The instantaneous reaching direction at time $t$ is denoted by $x_t$ and is treated as a scalar angle in radians. The population firing rate vector at time $t$ is denoted by $\\mathbf{r}_t \\in \\mathbb{R}^N$ and is measured in spikes per second (spk/s). The generative model consists of the following components:\n\n1. A Gaussian prior over the latent reaching direction,\n$ p(x_t) = \\mathcal{N}(x_t \\mid \\mu_0, \\sigma_0^2), $\nwhere $x_t$ is in radians, $\\mu_0$ is in radians, and $\\sigma_0^2$ is in radians squared.\n\n2. A multivariate Gaussian likelihood (observation model) for the population firing rates given the latent $x_t$,\n$ p(\\mathbf{r}_t \\mid x_t) = \\mathcal{N}(\\mathbf{r}_t \\mid \\mathbf{C} x_t + \\mathbf{d}, \\boldsymbol{\\Sigma}), $\nwhere $\\mathbf{C} \\in \\mathbb{R}^N$ encodes the linear tuning slope of each neuron with units of spk/s per radian, $\\mathbf{d} \\in \\mathbb{R}^N$ encodes the baseline firing rates in spk/s, and $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{N \\times N}$ is a positive-definite covariance matrix with units of $(\\text{spk/s})^2$.\n\nTask:\n- Starting from the basic definitions of Bayes' rule and properties of the multivariate normal distribution, derive the posterior distribution $p(x_t \\mid \\mathbf{r}_t)$, specifying its mean and variance as explicit functions of $\\mu_0$, $\\sigma_0^2$, $\\mathbf{C}$, $\\mathbf{d}$, $\\boldsymbol{\\Sigma}$, and the observation $\\mathbf{r}_t$.\n- Construct a linear decoder for $x_t$ from $\\mathbf{r}_t$ that equals the posterior mean under the stated model. Express this decoder as an affine mapping $ \\hat{x}_t = b + \\mathbf{w}^\\top \\mathbf{r}_t $, and provide explicit expressions for the bias $b$ (in radians) and weights $\\mathbf{w} \\in \\mathbb{R}^N$ (in radians per spk/s) in terms of the model parameters.\n\nAngle units must be in radians throughout. Firing rate units are in spk/s. Covariance units are $(\\text{spk/s})^2$.\n\nUse the following fixed parameter values with $N = 4$:\n- Prior parameters: $ \\mu_0 = 0.3 $ (radians), $ \\sigma_0^2 = 0.04 $ (radians squared).\n- Tuning slopes: $ \\mathbf{C} = [0.7,\\,-0.5,\\,0.3,\\,0.1] $ (spk/s per radian).\n- Baseline rates: $ \\mathbf{d} = [15.0,\\,10.0,\\,12.0,\\,8.0] $ (spk/s).\n- Observation noise covariance:\n$$\n\\boldsymbol{\\Sigma} =\n\\begin{bmatrix}\n4.0  0.6  0.2  0.1 \\\\\n0.6  3.0  0.4  0.2 \\\\\n0.2  0.4  2.5  0.3 \\\\\n0.1  0.2  0.3  3.5 \\\\\n\\end{bmatrix}\n\\quad \\text{(spk/s)}^2.\n$$\n\nTest suite (all firing rates in spk/s):\n- Case 1 (baseline): $ \\mathbf{r}_1 = [15.0,\\,10.0,\\,12.0,\\,8.0] $.\n- Case 2 (positive deviation along tuning): $ \\mathbf{r}_2 = \\mathbf{d} + 2 \\mathbf{C} = [16.4,\\,9.0,\\,12.6,\\,8.2] $.\n- Case 3 (negative deviation along tuning): $ \\mathbf{r}_3 = \\mathbf{d} - 1.5 \\mathbf{C} = [13.95,\\,10.75,\\,11.55,\\,7.85] $.\n\nYour program must:\n1. Compute the posterior mean and variance for $x_t$ for each test case.\n2. Compute the decoder bias $b$ and weights $\\mathbf{w}$ such that the posterior mean equals $ b + \\mathbf{w}^\\top \\mathbf{r}_t $.\n3. Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n$ [\\mu_{\\text{post},1},\\,\\sigma^2_{\\text{post},1},\\,\\mu_{\\text{post},2},\\,\\sigma^2_{\\text{post},2},\\,\\mu_{\\text{post},3},\\,\\sigma^2_{\\text{post},3},\\,b,\\,w_1,\\,w_2,\\,w_3,\\,w_4] $.\nAll angles ($\\mu_{\\text{post},i}$ and $b$) must be in radians, and all variances ($\\sigma^2_{\\text{post},i}$) must be in radians squared. Decoder weights $w_i$ must be in radians per spk/s.\n\nNo external input is allowed; hard-code the given parameters and test suite in your program. The program must be self-contained and runnable as is.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in computational neuroscience involving Bayesian inference with a linear-Gaussian model. All necessary parameters and conditions are provided, and there are no internal contradictions or ambiguities.\n\n### Derivation of the Posterior Distribution\n\nOur objective is to determine the posterior distribution of the latent reaching direction $x_t$ given the observed neural population firing rate vector $\\mathbf{r}_t$, denoted as $p(x_t \\mid \\mathbf{r}_t)$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$ p(x_t \\mid \\mathbf{r}_t) \\propto p(\\mathbf{r}_t \\mid x_t) p(x_t) $$\nThe problem defines a Gaussian prior and a Gaussian likelihood:\n1.  Prior: $ p(x_t) = \\mathcal{N}(x_t \\mid \\mu_0, \\sigma_0^2) $\n2.  Likelihood: $ p(\\mathbf{r}_t \\mid x_t) = \\mathcal{N}(\\mathbf{r}_t \\mid \\mathbf{C} x_t + \\mathbf{d}, \\boldsymbol{\\Sigma}) $\n\nThe product of two Gaussian distributions is an unnormalized Gaussian. To find the parameters of the resulting posterior distribution, we work with the exponents of the probability density functions. The probability density function (PDF) of a Gaussian distribution $\\mathcal{N}(z \\mid \\mu, \\Sigma)$ is proportional to $\\exp(-\\frac{1}{2}(z-\\mu)^\\top \\Sigma^{-1}(z-\\mu))$.\n\nThe exponent of the prior distribution $p(x_t)$ is:\n$$ \\mathcal{L}_{\\text{prior}} = -\\frac{1}{2\\sigma_0^2}(x_t - \\mu_0)^2 = -\\frac{1}{2}\\left( \\frac{1}{\\sigma_0^2}x_t^2 - \\frac{2\\mu_0}{\\sigma_0^2}x_t \\right) + \\text{const}_1 $$\nThe exponent of the likelihood distribution $p(\\mathbf{r}_t \\mid x_t)$ is:\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} (\\mathbf{r}_t - (\\mathbf{C} x_t + \\mathbf{d}))^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - (\\mathbf{C} x_t + \\mathbf{d})) $$\nLet's expand the quadratic form in the likelihood's exponent, focusing on terms involving $x_t$:\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} ((\\mathbf{r}_t - \\mathbf{d}) - \\mathbf{C}x_t)^\\top \\boldsymbol{\\Sigma}^{-1} ((\\mathbf{r}_t - \\mathbf{d}) - \\mathbf{C}x_t) $$\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} \\left( x_t \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} x_t - x_t \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}) - (\\mathbf{r}_t - \\mathbf{d})^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} x_t \\right) + \\text{const}_2 $$\nSince all terms are scalars, we can combine the two linear terms in $x_t$:\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} \\left( (\\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C})x_t^2 - 2(\\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}))x_t \\right) + \\text{const}_2 $$\n\nThe exponent of the posterior, $\\mathcal{L}_{\\text{post}}$, is the sum of the prior and likelihood exponents. We collect terms in powers of $x_t$:\n$$ \\mathcal{L}_{\\text{post}} = \\mathcal{L}_{\\text{prior}} + \\mathcal{L}_{\\text{like}} $$\n$$ \\mathcal{L}_{\\text{post}} = -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C}\\right)x_t^2 - 2\\left(\\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d})\\right)x_t \\right] + \\text{const}_3 $$\nThis is the exponent of a Gaussian distribution for $x_t$, which we denote $p(x_t \\mid \\mathbf{r}_t) = \\mathcal{N}(x_t \\mid \\mu_{\\text{post}}, \\sigma^2_{\\text{post}})$. The exponent of this posterior distribution has the canonical form:\n$$ -\\frac{1}{2\\sigma^2_{\\text{post}}}(x_t - \\mu_{\\text{post}})^2 = -\\frac{1}{2}\\left( \\frac{1}{\\sigma^2_{\\text{post}}}x_t^2 - \\frac{2\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}}x_t \\right) + \\text{const}_4 $$\nBy matching the coefficients of the $x_t^2$ and $x_t$ terms, we identify the posterior precision $(\\sigma^2_{\\text{post}})^{-1}$ and mean $\\mu_{\\text{post}}$.\n\nThe coefficient of $x_t^2$ gives the posterior precision:\n$$ \\frac{1}{\\sigma^2_{\\text{post}}} = \\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} $$\nThus, the posterior variance is:\n$$ \\sigma^2_{\\text{post}} = \\left( \\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} \\right)^{-1} $$\nNotice that the posterior variance is independent of the observation $\\mathbf{r}_t$.\n\nThe coefficient of $x_t$ gives:\n$$ \\frac{2\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}} = 2\\left(\\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d})\\right) $$\nSolving for the posterior mean $\\mu_{\\text{post}}$:\n$$ \\mu_{\\text{post}} = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}) \\right) $$\n\n### Derivation of the Linear Decoder\n\nThe posterior mean $\\mu_{\\text{post}}$ is the optimal Bayesian estimator for $x_t$ under a squared error loss. We define our linear decoder $\\hat{x}_t$ to be this posterior mean, $\\hat{x}_t = \\mu_{\\text{post}}$. We need to express this in the affine form $\\hat{x}_t = b + \\mathbf{w}^\\top \\mathbf{r}_t$.\nLet's expand the expression for $\\mu_{\\text{post}}$ and rearrange it:\n$$ \\mu_{\\text{post}} = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{r}_t - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right) $$\n$$ \\mu_{\\text{post}} = \\underbrace{\\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right)}_{b} + \\underbrace{\\left(\\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1}\\right)}_{\\mathbf{w}^\\top} \\mathbf{r}_t $$\nFrom this rearrangement, we can identify the bias $b$ and the weight vector $\\mathbf{w}$.\n\nThe bias term $b$ is:\n$$ b = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right) $$\nThe row vector of weights $\\mathbf{w}^\\top$ is:\n$$ \\mathbf{w}^\\top = \\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} $$\nTaking the transpose, we obtain the column vector of weights $\\mathbf{w}$:\n$$ \\mathbf{w} = (\\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1})^\\top = \\sigma^2_{\\text{post}} (\\boldsymbol{\\Sigma}^{-1})^\\top (\\mathbf{C}^\\top)^\\top = \\sigma^2_{\\text{post}} \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} $$\nThe last step uses the fact that the inverse of a symmetric matrix $\\boldsymbol{\\Sigma}$ is also symmetric, i.e., $(\\boldsymbol{\\Sigma}^{-1})^\\top = \\boldsymbol{\\Sigma}^{-1}$. These formulas for $b$ and $\\mathbf{w}$ allow for direct computation from the model parameters.\n\n### Numerical Computation\nThe derived formulas will now be implemented with the provided numerical values to find the required quantities for the three test cases, along with the constant decoder parameters $b$ and $\\mathbf{w}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the posterior distribution parameters and the linear decoder\n    parameters for a linear-Gaussian model of neural population decoding.\n    \"\"\"\n    # Define fixed parameters from the problem statement.\n    # N is the number of neurons.\n    N = 4\n    \n    # Prior parameters\n    mu0 = 0.3  # radians\n    sigma0_sq = 0.04  # radians^2\n\n    # Likelihood parameters\n    C = np.array([0.7, -0.5, 0.3, 0.1]).reshape(N, 1)  # spk/s per radian\n    d = np.array([15.0, 10.0, 12.0, 8.0]).reshape(N, 1)  # spk/s\n    Sigma = np.array([\n        [4.0, 0.6, 0.2, 0.1],\n        [0.6, 3.0, 0.4, 0.2],\n        [0.2, 0.4, 2.5, 0.3],\n        [0.1, 0.2, 0.3, 3.5]\n    ])  # (spk/s)^2\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (baseline): r1 = d\n        np.array([15.0, 10.0, 12.0, 8.0]).reshape(N, 1),\n        # Case 2 (positive deviation): r2 = d + 2*C\n        np.array([16.4, 9.0, 12.6, 8.2]).reshape(N, 1),\n        # Case 3 (negative deviation): r3 = d - 1.5*C\n        np.array([13.95, 10.75, 11.55, 7.85]).reshape(N, 1)\n    ]\n\n    # --- Step 1: Compute posterior variance ---\n    # This value is constant for all observations in a linear-Gaussian model.\n    # Pre-compute the inverse of the covariance matrix and the prior precision.\n    Sigma_inv = np.linalg.inv(Sigma)\n    prior_precision = 1.0 / sigma0_sq\n    \n    # Compute the term C^T * Sigma^-1 * C, which contributes to the likelihood's precision.\n    # .item() is used to extract the scalar value from the 1x1 matrix.\n    likelihood_precision_term = (C.T @ Sigma_inv @ C).item()\n    \n    # The posterior precision is the sum of the prior and likelihood precisions.\n    posterior_precision = prior_precision + likelihood_precision_term\n    \n    # The posterior variance is the reciprocal of the posterior precision.\n    sigma2_post = 1.0 / posterior_precision\n\n    # --- Step 2: Compute decoder bias (b) and weights (w) ---\n    # According to the derived formula: w = sigma^2_post * Sigma^-1 * C\n    w = sigma2_post * (Sigma_inv @ C)\n    \n    # According to the derived formula: b = sigma^2_post * (mu0/sigma0^2 - C^T * Sigma^-1 * d)\n    b = sigma2_post * (prior_precision * mu0 - (C.T @ Sigma_inv @ d).item())\n    \n    # --- Step 3: Compute posterior mean for each test case ---\n    # The posterior mean can be calculated using the decoder: mu_post = b + w^T * r\n    results = []\n    for r_t in test_cases:\n        mu_post = (b + w.T @ r_t).item()\n        results.extend([mu_post, sigma2_post])\n    \n    # --- Step 4: Assemble the final output list ---\n    # The final list includes results for the 3 cases, followed by the decoder parameters.\n    # w.flatten().tolist() converts the weight column vector to a list of scalars.\n    results.extend([b] + w.flatten().tolist())\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.12f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice tackles a key challenge in modern neuroscience: uncovering hidden structure within large-scale neural recordings. This advanced exercise  introduces a powerful generative model, Poisson Non-negative Matrix Factorization, to identify latent neural assemblies from spike count data. You will derive the update rules for a sophisticated Bayesian inference algorithm, providing a hands-on look at the mathematical engine behind many state-of-the-art analysis methods.",
            "id": "4011649",
            "problem": "A population of $I$ cortical neurons is recorded over $T$ non-overlapping time bins, yielding spike count data $\\mathbf{n} = \\{n_{i t}\\}_{i=1,\\dots,I;\\,t=1,\\dots,T}$, where each $n_{i t} \\in \\{0,1,2,\\dots\\}$. Suppose the counts arise from $K$ latent assemblies such that the conditional distribution is modeled by a Poisson superposition,\n$$\nn_{i t} \\,\\big|\\, \\{w_{i k}\\}_{k=1}^{K}, \\{h_{k t}\\}_{k=1}^{K} \\sim \\mathrm{Poisson}\\!\\left(\\sum_{k=1}^{K} w_{i k} h_{k t}\\right),\n$$\nwith nonnegative factors $w_{i k}  0$ representing neuron-to-assembly loadings and $h_{k t}  0$ representing assembly-to-time activations. To enforce nonnegativity and induce shrinkage, adopt independent Gamma priors in shape–rate parameterization,\n$$\nw_{i k} \\sim \\mathrm{Gamma}(a_{w}, b_{w}),\\quad h_{k t} \\sim \\mathrm{Gamma}(a_{h}, b_{h}),\n$$\nwhere, for a generic variable $x$, $\\mathrm{Gamma}(a,b)$ denotes the density $p(x \\mid a,b) = \\frac{b^{a}}{\\Gamma(a)} x^{a-1} \\exp(-b x)$ supported on $x  0$. Introduce latent count variables $\\{z_{i t k}\\}$ to represent each assembly’s contribution to $n_{i t}$, satisfying $\\sum_{k=1}^{K} z_{i t k} = n_{i t}$ and, under the complete-data generative model,\n$$\nz_{i t k} \\sim \\mathrm{Poisson}(w_{i k} h_{k t}) \\quad \\text{independently across } (i,t,k),\n$$\nso that $n_{i t} = \\sum_{k} z_{i t k}$ holds by the Poisson superposition property. Consider the mean-field variational family\n$$\nq(\\mathbf{z}, \\mathbf{w}, \\mathbf{h}) = \\left[\\prod_{i=1}^{I} \\prod_{t=1}^{T} q(\\mathbf{z}_{i t \\cdot})\\right] \\left[\\prod_{i=1}^{I} \\prod_{k=1}^{K} q(w_{i k})\\right] \\left[\\prod_{k=1}^{K} \\prod_{t=1}^{T} q(h_{k t})\\right],\n$$\nwith $q(w_{i k})$ and $q(h_{k t})$ in the Gamma family and $q(\\mathbf{z}_{i t \\cdot})$ a multinomial distribution over the $K$ components constrained by $\\sum_{k} z_{i t k} = n_{i t}$, parameterized by responsibilities $\\boldsymbol{\\phi}_{i t} = (\\phi_{i t 1},\\dots,\\phi_{i t K})$ such that $\\sum_{k} \\phi_{i t k} = 1$ and $\\phi_{i t k} \\in (0,1)$.\n\nStarting from fundamental probabilistic definitions (Poisson likelihood, Gamma priors, and the Evidence Lower Bound (ELBO) for Kullback–Leibler divergence minimization), perform the following:\n\n- Derive the complete-data log joint distribution $\\ln p(\\mathbf{n}, \\mathbf{z}, \\mathbf{w}, \\mathbf{h})$ up to additive constants independent of $(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})$.\n- Using the mean-field assumption, derive the coordinate ascent updates that maximize the ELBO with respect to each factor in $q$. Specifically, obtain:\n  1. The multinomial responsibility update for $q(\\mathbf{z}_{i t \\cdot})$, expressed in terms of expectations under the current $q(w_{i k})$ and $q(h_{k t})$.\n  2. The Gamma shape–rate parameters for $q(w_{i k})$ and $q(h_{k t})$ in terms of expectations under the current variational distributions.\n- Express your final updates using the expectations $\\mathbb{E}_{q}[\\ln w_{i k}]$, $\\mathbb{E}_{q}[w_{i k}]$, $\\mathbb{E}_{q}[\\ln h_{k t}]$, and $\\mathbb{E}_{q}[h_{k t}]$ for Gamma distributions, recalling that for $x \\sim \\mathrm{Gamma}(a,b)$ in shape–rate parameterization, $\\mathbb{E}[x] = \\frac{a}{b}$ and $\\mathbb{E}[\\ln x] = \\psi(a) - \\ln(b)$, where $\\psi(\\cdot)$ is the digamma function.\n\nYour final answer must be a single closed-form analytic expression containing all coordinate ascent update formulas for the responsibilities and the Gamma variational parameters. No numerical evaluation is required. Do not include any units. If you introduce any acronyms, spell them out on first appearance. The final answer must be presented exactly as specified in the output structure and formatting rules, and must not be an inequality or an equation with unspecified free parameters beyond those defined above.",
            "solution": "The solution proceeds in two main parts as requested: first, the derivation of the complete-data log joint distribution, and second, the derivation of the coordinate ascent update equations for the variational parameters.\n\n**1. Complete-Data Log Joint Distribution**\n\nThe complete data in this model consists of the latent counts $\\mathbf{z}$, the neuron-to-assembly weights $\\mathbf{w}$, and the assembly activations $\\mathbf{h}$. The observed data $\\mathbf{n}$ is determined by $\\mathbf{z}$ via the constraint $n_{it} = \\sum_k z_{itk}$. The joint probability distribution over all variables is $p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h}) = p(\\mathbf{z}|\\mathbf{w}, \\mathbf{h})p(\\mathbf{w})p(\\mathbf{h})$.\n\nThe components are given by the model specification:\n-   $p(\\mathbf{z}|\\mathbf{w}, \\mathbf{h}) = \\prod_{i=1}^I \\prod_{t=1}^T \\prod_{k=1}^K p(z_{itk}|w_{ik}, h_{kt}) = \\prod_{i,t,k} \\frac{(w_{ik}h_{kt})^{z_{itk}} \\exp(-w_{ik}h_{kt})}{z_{itk}!}$\n-   $p(\\mathbf{w}) = \\prod_{i=1}^I \\prod_{k=1}^K p(w_{ik}|a_w, b_w) = \\prod_{i,k} \\frac{b_w^{a_w}}{\\Gamma(a_w)} w_{ik}^{a_w-1} \\exp(-b_w w_{ik})$\n-   $p(\\mathbf{h}) = \\prod_{k=1}^K \\prod_{t=1}^T p(h_{kt}|a_h, b_h) = \\prod_{k,t} \\frac{b_h^{a_h}}{\\Gamma(a_h)} h_{kt}^{a_h-1} \\exp(-b_h h_{kt})$\n\nThe log of the joint distribution, $\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})$, is the sum of the logs of these components. We drop additive terms that are constant with respect to $\\mathbf{z}$, $\\mathbf{w}$, and $\\mathbf{h}$.\n\n$\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h}) = \\sum_{i,t,k} \\ln p(z_{itk}|w_{ik}, h_{kt}) + \\sum_{i,k} \\ln p(w_{ik}) + \\sum_{k,t} \\ln p(h_{kt})$\n\nExpanding each term:\n-   $\\ln p(z_{itk}|w_{ik}, h_{kt}) = z_{itk}\\ln(w_{ik}h_{kt}) - w_{ik}h_{kt} - \\ln(z_{itk}!) = z_{itk}\\ln w_{ik} + z_{itk}\\ln h_{kt} - w_{ik}h_{kt} - \\ln(z_{itk}!)$\n-   $\\ln p(w_{ik}) = (a_w-1)\\ln w_{ik} - b_w w_{ik} + a_w \\ln b_w - \\ln\\Gamma(a_w)$\n-   $\\ln p(h_{kt}) = (a_h-1)\\ln h_{kt} - b_h h_{kt} + a_h \\ln b_h - \\ln\\Gamma(a_h)$\n\nSumming and dropping constants (terms involving only $a_w, b_w, a_h, b_h$), we get the complete-data log joint distribution:\n$$\n\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h}) \\stackrel{c}{=} \\sum_{i,t,k} \\left( z_{itk} (\\ln w_{ik} + \\ln h_{kt}) - w_{ik}h_{kt} - \\ln(z_{itk}!) \\right) + \\sum_{i,k} \\left( (a_w-1)\\ln w_{ik} - b_w w_{ik} \\right) + \\sum_{k,t} \\left( (a_h-1)\\ln h_{kt} - b_h h_{kt} \\right)\n$$\nThe term $\\ln(z_{itk}!)$ is kept as it depends on the latent variable $z_{itk}$.\n\n**2. Coordinate Ascent Variational Inference (CAVI) Updates**\n\nCAVI iteratively optimizes each factor of the variational distribution $q$ while holding the others fixed. The general update rule for a factor $q_j$ over variable(s) $\\theta_j$ is given by:\n$$ \\ln q_j^*(\\theta_j) = \\mathbb{E}_{q_{\\neg j}}[\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})] + \\text{const} $$\nwhere $\\mathbb{E}_{q_{\\neg j}}$ denotes the expectation with respect to all other factors in $q$.\n\n**Update for $q(\\mathbf{z}_{it\\cdot})$**\nWe seek the optimal form of $q(\\mathbf{z}_{it\\cdot})$ for a specific $(i,t)$. We collect all terms from $\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})$ that involve any $z_{itk}$ for $k=1,\\dots,K$.\n$$ \\ln q^*(\\mathbf{z}_{it\\cdot}) \\stackrel{c}{=} \\mathbb{E}_{q(\\mathbf{w},\\mathbf{h})} \\left[ \\sum_{k=1}^K \\left( z_{itk}(\\ln w_{ik} + \\ln h_{kt}) - \\ln(z_{itk}!) \\right) \\right] $$\nDue to the mean-field assumption, the expectation becomes:\n$$ \\ln q^*(\\mathbf{z}_{it\\cdot}) \\stackrel{c}{=} \\sum_{k=1}^K \\left( z_{itk}(\\mathbb{E}_q[\\ln w_{ik}] + \\mathbb{E}_q[\\ln h_{kt}]) - \\ln(z_{itk}!) \\right) = \\sum_{k=1}^K z_{itk} \\ln \\rho_{itk} - \\sum_{k=1}^K \\ln(z_{itk}!) $$\nwhere we define $\\ln \\rho_{itk} = \\mathbb{E}_q[\\ln w_{ik}] + \\mathbb{E}_q[\\ln h_{kt}]$. This is the log-kernel of a product of independent Poisson distributions, $\\prod_k \\mathrm{Poisson}(z_{itk} | \\rho_{itk})$, subject to the constraint $\\sum_k z_{itk} = n_{it}$. The conditional distribution of Poisson counts given their sum is a Multinomial distribution.\nThus, $q^*(\\mathbf{z}_{it\\cdot})$ is a Multinomial distribution with $n_{it}$ trials and probability parameters $\\phi_{itk}$ given by:\n$$ \\phi_{itk} = \\frac{\\rho_{itk}}{\\sum_{k'=1}^K \\rho_{itk'}} = \\frac{\\exp(\\mathbb{E}_q[\\ln w_{ik}] + \\mathbb{E}_q[\\ln h_{kt}])}{\\sum_{k'=1}^K \\exp(\\mathbb{E}_q[\\ln w_{ik'}] + \\mathbb{E}_q[\\ln h_{k't}])} $$\nThe expectation of $z_{itk}$ under this distribution is $\\mathbb{E}_q[z_{itk}] = n_{it} \\phi_{itk}$. This result is used in the other updates.\n\n**Update for $q(w_{ik})$**\nFor a specific $(i,k)$, we collect terms from $\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})$ involving $w_{ik}$:\n$$ \\ln q^*(w_{ik}) \\stackrel{c}{=} \\mathbb{E}_{q_{\\neg w_{ik}}} \\left[ \\sum_{t=1}^T (z_{itk} \\ln w_{ik} - w_{ik} h_{kt}) + (a_w-1)\\ln w_{ik} - b_w w_{ik} \\right] $$\n$$ \\ln q^*(w_{ik}) \\stackrel{c}{=} \\left( (a_w-1) + \\sum_{t=1}^T \\mathbb{E}_q[z_{itk}] \\right) \\ln w_{ik} - \\left( b_w + \\sum_{t=1}^T \\mathbb{E}_q[h_{kt}] \\right) w_{ik} $$\nThis is the log-kernel of a Gamma distribution, $\\ln p(x|a,b) \\stackrel{c}{=} (a-1)\\ln x - bx$. By matching terms, we identify the updated shape and rate parameters, which we denote by $\\hat{a}_{w_{ik}}$ and $\\hat{b}_{w_{ik}}$:\n$$ \\hat{a}_{w_{ik}} - 1 = a_w - 1 + \\sum_{t=1}^T \\mathbb{E}_q[z_{itk}] \\implies \\hat{a}_{w_{ik}} = a_w + \\sum_{t=1}^T n_{it} \\phi_{itk} $$\n$$ \\hat{b}_{w_{ik}} = b_w + \\sum_{t=1}^T \\mathbb{E}_q[h_{kt}] $$\nSo, $q^*(w_{ik}) = \\mathrm{Gamma}(w_{ik} | \\hat{a}_{w_{ik}}, \\hat{b}_{w_{ik}})$.\n\n**Update for $q(h_{kt})$**\nThe derivation is symmetric to that of $w_{ik}$. For a specific $(k,t)$, we collect terms from $\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})$ involving $h_{kt}$:\n$$ \\ln q^*(h_{kt}) \\stackrel{c}{=} \\mathbb{E}_{q_{\\neg h_{kt}}} \\left[ \\sum_{i=1}^I (z_{itk} \\ln h_{kt} - w_{ik} h_{kt}) + (a_h-1)\\ln h_{kt} - b_h h_{kt} \\right] $$\n$$ \\ln q^*(h_{kt}) \\stackrel{c}{=} \\left( (a_h-1) + \\sum_{i=1}^I \\mathbb{E}_q[z_{itk}] \\right) \\ln h_{kt} - \\left( b_h + \\sum_{i=1}^I \\mathbb{E}_q[w_{ik}] \\right) h_{kt} $$\nThis is again the log-kernel of a Gamma distribution. The updated shape and rate parameters, $\\hat{a}_{h_{kt}}$ and $\\hat{b}_{h_{kt}}$, are:\n$$ \\hat{a}_{h_{kt}} - 1 = a_h - 1 + \\sum_{i=1}^I \\mathbb{E}_q[z_{itk}] \\implies \\hat{a}_{h_{kt}} = a_h + \\sum_{i=1}^I n_{it} \\phi_{itk} $$\n$$ \\hat{b}_{h_{kt}} = b_h + \\sum_{i=1}^I \\mathbb{E}_q[w_{ik}] $$\nSo, $q^*(h_{kt}) = \\mathrm{Gamma}(h_{kt} | \\hat{a}_{h_{kt}}, \\hat{b}_{h_{kt}})$.\n\nThese equations define a full cycle of the CAVI algorithm. At each iteration, one updates the parameters of each variational distribution in turn, using the current expectations of the other variables. The expectations $\\mathbb{E}_q[\\cdot]$ are computed using the current variational parameters, e.g., $\\mathbb{E}_q[w_{ik}] = \\hat{a}_{w_{ik}}/\\hat{b}_{w_{ik}}$ and $\\mathbb{E}_q[\\ln w_{ik}] = \\psi(\\hat{a}_{w_{ik}}) - \\ln(\\hat{b}_{w_{ik}})$.",
            "answer": "$$\n\\boxed{\n\\begin{aligned}\n\\phi_{itk}  \\leftarrow \\frac{\\exp\\left(\\mathbb{E}_{q}[\\ln w_{ik}] + \\mathbb{E}_{q}[\\ln h_{kt}]\\right)}{\\sum_{k'=1}^{K} \\exp\\left(\\mathbb{E}_{q}[\\ln w_{ik'}] + \\mathbb{E}_{q}[\\ln h_{k't}]\\right)} \\\\\n\\hat{a}_{w_{ik}}  \\leftarrow a_w + \\sum_{t=1}^{T} n_{it} \\phi_{itk} \\\\\n\\hat{b}_{w_{ik}}  \\leftarrow b_w + \\sum_{t=1}^{T} \\mathbb{E}_{q}[h_{kt}] \\\\\n\\hat{a}_{h_{kt}}  \\leftarrow a_h + \\sum_{i=1}^{I} n_{it} \\phi_{itk} \\\\\n\\hat{b}_{h_{kt}}  \\leftarrow b_h + \\sum_{i=1}^{I} \\mathbb{E}_{q}[w_{ik}]\n\\end{aligned}\n}\n$$"
        }
    ]
}