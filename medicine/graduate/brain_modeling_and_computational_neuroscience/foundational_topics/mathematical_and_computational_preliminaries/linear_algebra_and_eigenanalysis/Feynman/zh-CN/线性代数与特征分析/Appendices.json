{
    "hands_on_practices": [
        {
            "introduction": "理解神经网络动态特性的第一步通常是分析其雅可比矩阵的特征值，因为它们定义了系统的基本活动模式。然而，对于大型网络，直接计算特征值可能非常困难。盖尔雄圆盘定理（Gershgorin circle theorem）提供了一个强大的工具，它允许我们仅通过检查矩阵的元素就能快速估计特征值在复平面上的位置。这个练习将指导你应用该定理到一个皮层微环路模型上，并揭示如何将特定的动态模式（特征值）与特定的神经元群体关联起来。",
            "id": "3994745",
            "problem": "考虑一个前馈皮层微电路的线性化四群体速率模型，其中不动点附近的动力学由雅可比矩阵 $J \\in \\mathbb{R}^{4 \\times 4}$ 近似。矩阵 $J$ 被设计成使得群体间的耦合是弱的，并以前馈方式单向作用，得到\n$$\nJ \\;=\\;\n\\begin{pmatrix}\n-0.5  0.10  0.04  0.02 \\\\\n0  -2.0  0.08  0.03 \\\\\n0  0  -4.0  0.06 \\\\\n0  0  0  -7.0\n\\end{pmatrix}.\n$$\n对于每一行 $i$，定义复平面中的盖尔圆盘（Gershgorin disk），其圆心为 $a_{ii}$，半径等于该行非对角线元素的绝对值之和。利用特征值的核心定义和谱的盖尔圆盘特征，完成以下任务：\n1. 计算 $J$ 每一行对应的盖尔圆盘的圆心和半径，并验证这些圆盘两两不相交。\n2. 提供一个严谨的论证，说明每个不相交的盖尔圆盘必须恰好包含 $J$ 的一个特征值，并简要讨论该性质如何帮助微电路的模式分配（即将特征模式与单个群体关联起来）。\n3. 最后，计算 $J$ 的行列式，结果为一个实数。最终数值答案无需四舍五入。",
            "solution": "首先验证问题的科学合理性、自洽性和清晰度。问题陈述提供了一个来自线性化动力系统的定义明确的雅可比矩阵 $J$，这是计算神经科学中的一种标准方法。任务涉及应用盖尔圆定理和计算行列式，这两者都是线性代数中的标准程序。前提具有科学依据和数学合理性。问题提法得当、无歧义，并提供了所有必要信息。因此，该问题被认为是有效的，并给出完整解答。\n\n雅可比矩阵由下式给出：\n$$\nJ \\;=\\;\n\\begin{pmatrix}\n-0.5  0.10  0.04  0.02 \\\\\n0  -2.0  0.08  0.03 \\\\\n0  0  -4.0  0.06 \\\\\n0  0  0  -7.0\n\\end{pmatrix}\n$$\n\n对于矩阵 $A = (a_{ij})$ 的第 $i$ 行，盖尔圆盘 $D_i$ 是复平面中的一个闭圆盘，其圆心为 $c_i = a_{ii}$，半径为 $r_i = \\sum_{j \\neq i} |a_{ij}|$。\n\n**1. 盖尔圆盘及其不相交性**\n\n我们计算与矩阵 $J$ 的每一行相对应的四个盖尔圆盘的圆心 $c_i$ 和半径 $r_i$。\n\n对于第 1 行：\n圆心是主对角线元素 $c_1 = j_{11} = -0.5$。\n半径是非对角线元素的绝对值之和：$r_1 = |0.10| + |0.04| + |0.02| = 0.10 + 0.04 + 0.02 = 0.16$。\n因此，第一个圆盘是 $D_1 = \\{z \\in \\mathbb{C} : |z - (-0.5)| \\le 0.16\\}$。\n\n对于第 2 行：\n圆心是 $c_2 = j_{22} = -2.0$。\n半径是 $r_2 = |0| + |0.08| + |0.03| = 0.08 + 0.03 = 0.11$。\n因此，第二个圆盘是 $D_2 = \\{z \\in \\mathbb{C} : |z - (-2.0)| \\le 0.11\\}$。\n\n对于第 3 行：\n圆心是 $c_3 = j_{33} = -4.0$。\n半径是 $r_3 = |0| + |0| + |0.06| = 0.06$。\n因此，第三个圆盘是 $D_3 = \\{z \\in \\mathbb{C} : |z - (-4.0)| \\le 0.06\\}$。\n\n对于第 4 行：\n圆心是 $c_4 = j_{44} = -7.0$。\n半径是 $r_4 = |0| + |0| + |0| = 0$。\n因此，第四个圆盘是 $D_4 = \\{z \\in \\mathbb{C} : |z - (-7.0)| \\le 0\\}$，这只是一个点 $z = -7.0$。\n\n接下来，我们验证这四个圆盘是两两不相交的。如果两个圆盘 $D_i$ 和 $D_j$ 的圆心距 $|c_i - c_j|$ 大于它们的半径之和 $r_i + r_j$，那么它们是不相交的。\n\n-   **$D_1$ 和 $D_2$**：$|c_1 - c_2| = |-0.5 - (-2.0)| = 1.5$。半径之和为 $r_1 + r_2 = 0.16 + 0.11 = 0.27$。因为 $1.5  0.27$，所以 $D_1$ 和 $D_2$ 不相交。\n-   **$D_1$ 和 $D_3$**：$|c_1 - c_3| = |-0.5 - (-4.0)| = 3.5$。半径之和为 $r_1 + r_3 = 0.16 + 0.06 = 0.22$。因为 $3.5  0.22$，所以 $D_1$ 和 $D_3$ 不相交。\n-   **$D_1$ 和 $D_4$**：$|c_1 - c_4| = |-0.5 - (-7.0)| = 6.5$。半径之和为 $r_1 + r_4 = 0.16 + 0 = 0.16$。因为 $6.5  0.16$，所以 $D_1$ 和 $D_4$ 不相交。\n-   **$D_2$ 和 $D_3$**：$|c_2 - c_3| = |-2.0 - (-4.0)| = 2.0$。半径之和为 $r_2 + r_3 = 0.11 + 0.06 = 0.17$。因为 $2.0  0.17$，所以 $D_2$ 和 $D_3$ 不相交。\n-   **$D_2$ 和 $D_4$**：$|c_2 - c_4| = |-2.0 - (-7.0)| = 5.0$。半径之和为 $r_2 + r_4 = 0.11 + 0 = 0.11$。因为 $5.0  0.11$，所以 $D_2$ 和 $D_4$ 不相交。\n-   **$D_3$ 和 $D_4$**：$|c_3 - c_4| = |-4.0 - (-7.0)| = 3.0$。半径之和为 $r_3 + r_4 = 0.06 + 0 = 0.06$。因为 $3.0  0.06$，所以 $D_3$ 和 $D_4$ 不相交。\n\n所有盖尔圆盘对都是不相交的。\n\n**2. 特征值定位与模式分配**\n\n盖尔圆定理的一个重要推广结果指出，如果 $k$ 个盖尔圆盘的集合与其余 $n-k$ 个圆盘不相交，那么前者的并集恰好包含该矩阵的 $k$ 个特征值。在我们的例子中，每个圆盘 $D_i$ 都与其他三个圆盘的并集不相交。对每个圆盘 $D_i$ ($i=1, 2, 3, 4$) 应用 $k=1$ 的定理，我们得出结论，每个圆盘必须恰好包含一个特征值。\n\n对该性质的严谨论证依赖于连续性。设 $D$ 是 $J$ 的对角部分， $O = J - D$ 是非对角部分。构建一个矩阵族 $J(t) = D + tO$，其中参数 $t \\in [0, 1]$。$J(t)$ 的特征值是 $t$ 的连续函数。\n当 $t=0$ 时，我们有 $J(0) = D$，这是一个对角矩阵。其特征值就是它的对角元素 $\\lambda_i(0) = j_{ii}$。每个特征值 $\\lambda_i(0)$ 位于其对应的盖尔圆盘 $D_i$ 的圆心。\n当 $t$ 从 $0$ 增加到 $1$ 时，特征值 $\\lambda_i(t)$ 在复平面中描绘出连续的路径。第一个盖尔圆定理保证了 $J=J(1)$ 的所有特征值都必须位于四个圆盘的并集 $\\bigcup_{i=1}^4 D_i$ 内。由于这些圆盘是不相交的，一条从 $D_i$ 开始的连续路径 $\\lambda_i(t)$ 不可能在不离开所有圆盘的并集的情况下到达另一个圆盘 $D_k$（其中 $k \\neq i$），因为这会与定理相矛盾。因此，每条特征值路径都保持在其原始圆盘内。由于在 $t=0$ 时每个圆盘中有一个特征值，所以在 $t=1$ 时每个圆盘中也必须恰好有一个特征值。\n\n在皮层微电路模型的背景下，这一性质对模式分配非常重要。系统在不动点附近的动力学由 $\\dot{\\mathbf{x}} = J\\mathbf{x}$ 给出，其中 $\\mathbf{x}$ 是群体活动的向量。解是形如 $\\mathbf{v}_k \\exp(\\lambda_k t)$ 的项的线性组合，其中 $(\\lambda_k, \\mathbf{v}_k)$ 是 $J$ 的一个特征对。每个这样的项都是一个动力学模式。\n每个特征值 $\\lambda_i$ 被限制在以 $j_{ii}$ 为中心的小圆盘 $D_i$ 内，这一事实意味着 $\\lambda_i \\approx j_{ii}$。对角元素 $j_{ii}$ 代表群体 $i$ 的内在衰减率。由于非对角耦合项很小，相应的特征向量 $\\mathbf{v}_i$ 的最大分量将在第 $i$ 维上。因此，与 $\\lambda_i$ 相关的动力学模式主要由群体 $i$ 的活动主导。这使得我们可以将每个特征模式与一个特定的群体直接关联起来，从而将网络复杂动力学的分析简化为一组近似独立的、特定于群体的动力学。\n\n**3. J 的行列式**\n\n矩阵 $J$ 是一个上三角矩阵。三角矩阵（包括上三角和下三角）的一个基本性质是其行列式等于其主对角线元素的乘积。\n因此，$J$ 的行列式计算如下：\n$$\n\\det(J) = j_{11} \\times j_{22} \\times j_{33} \\times j_{44}\n$$\n代入给定值：\n$$\n\\det(J) = (-0.5) \\times (-2.0) \\times (-4.0) \\times (-7.0)\n$$\n$$\n\\det(J) = (1.0) \\times (28.0)\n$$\n$$\n\\det(J) = 28.0\n$$\n另一种确认此结果的方法是，回想一下矩阵的行列式也是其特征值的乘积。由于 $J$ 是上三角矩阵，其特征值恰好是其对角线元素：$\\lambda_1 = -0.5$，$\\lambda_2 = -2.0$，$\\lambda_3 = -4.0$ 和 $\\lambda_4 = -7.0$。\n它们的乘积是 $\\lambda_1 \\lambda_2 \\lambda_3 \\lambda_4 = (-0.5)(-2.0)(-4.0)(-7.0) = 28.0$，这证实了结果。\n$J$ 的行列式是 $28$。",
            "answer": "$$\n\\boxed{28}\n$$"
        },
        {
            "introduction": "除了系统的内在动态模式，我们还关心它对外部输入的响应以及其对参数变化的鲁棒性。在计算神经科学中，许多网络模型，特别是那些处于“平衡”状态（即大的兴奋性和抑制性输入几乎相互抵消）的模型，虽然计算能力强大，但可能对参数变化非常敏感。这个练习将探讨这种敏感性，通过引入“病态”矩阵和“条件数”的概念，来量化突触权重的微小误差如何被放大，从而导致网络稳态活动的巨大变化。",
            "id": "3994759",
            "problem": "考虑一个接近平衡态的双群体皮层微电路的线性速率模型，其中稳态发放率 $\\mathbf{r}$ 满足 $(\\mathbf{I} - \\mathbf{W}) \\mathbf{r} = \\mathbf{u}$，$\\mathbf{u}$ 为外部输入。有效算子 $\\mathbf{A} = \\mathbf{I} - \\mathbf{W}$ 决定了敏感性 $\\mathbf{r} = \\mathbf{A}^{-1} \\mathbf{u}$。在这种状态下，测量 $\\mathbf{W}$ 的微小误差会转化为 $\\mathbf{A}$ 的误差，如果 $\\mathbf{A}$ 是病态的，则可能在 $\\mathbf{A}^{-1}$ 中产生巨大误差。\n\n构建以下明确示例，其中 $\\mathbf{A}$ 的微小误差导致 $\\mathbf{A}^{-1}$ 的巨大误差，并使用矩阵二范数（由欧几里得范数诱导）下的条件数 $\\kappa(\\mathbf{A})$ 来量化这种放大效应：\n\n- 设群体间耦合强且接近平衡，对称权重矩阵 $\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ 由下式给出\n$$\n\\mathbf{W} = \\begin{pmatrix}\n0  1 - \\varepsilon \\\\\n1 - \\varepsilon  0\n\\end{pmatrix},\n$$\n其中 $0  \\varepsilon \\ll 1$。则有效算子为\n$$\n\\mathbf{A} = \\mathbf{I} - \\mathbf{W} = \\begin{pmatrix}\n1  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1\n\\end{pmatrix}.\n$$\n\n- 假设实验测量误差将 $\\mathbf{A}$ 扰动为 $\\mathbf{A} + \\Delta \\mathbf{A}$，其微小的对称扰动形式如下\n$$\n\\Delta \\mathbf{A} = \\begin{pmatrix}\n0  \\delta \\\\\n\\delta  0\n\\end{pmatrix},\n$$\n其中 $|\\delta| \\ll 1$。\n\n仅使用核心定义和第一性原理——即矩阵逆的定义、应用于 $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}$ 的乘积法则以及诱导二范数的定义——执行以下操作：\n\n1. 推导 $\\Delta \\mathbf{A}$ 和 $\\Delta \\mathbf{A}^{-1}$ 之间的一阶关系，该关系解释了对于微小的 $|\\delta|$，$\\mathbf{A}$ 中的微小误差如何在 $\\mathbf{A}^{-1}$ 中被放大。\n\n2. 计算 $\\mathbf{A}$ 在二范数下的条件数 $\\kappa_{2}(\\mathbf{A})$，并将其表示为关于 $\\varepsilon$ 的闭式解。\n\n3. 对 $\\varepsilon = 0.01$ 的情况，数值计算 $\\kappa_{2}(\\mathbf{A})$。\n\n仅报告 $\\varepsilon = 0.01$ 时 $\\kappa_{2}(\\mathbf{A})$ 的数值作为最终答案。将您的答案四舍五入到四位有效数字。量 $\\kappa_{2}(\\mathbf{A})$ 是无量纲的。",
            "solution": "该问题要求完成三个与矩阵逆对扰动的敏感性相关的任务，使用的是一个从简化神经回路模型中派生出的特定 $2 \\times 2$ 矩阵 $\\mathbf{A}$。这些任务是：$1$) 推导逆矩阵变化量 $\\Delta \\mathbf{A}^{-1}$ 的一阶近似；$2$) 计算条件数 $\\kappa_2(\\mathbf{A})$ 作为参数 $\\varepsilon$ 的函数；$3$) 为 $\\varepsilon$ 的特定值计算此条件数。\n\n首先，我们处理扰动 $\\Delta \\mathbf{A}$ 与其导致的逆矩阵扰动 $\\Delta\\mathbf{A}^{-1}$ 之间的一阶关系的推导。我们从扰动矩阵 $(\\mathbf{A} + \\Delta\\mathbf{A})^{-1}$ 的逆矩阵定义开始。设其逆为 $\\mathbf{A}^{-1} + \\Delta\\mathbf{A}^{-1}$，其中 $\\Delta\\mathbf{A}^{-1}$ 表示逆矩阵的变化量。一个矩阵与其逆矩阵的乘积是单位矩阵 $\\mathbf{I}$：\n$$\n(\\mathbf{A} + \\Delta\\mathbf{A}) (\\mathbf{A}^{-1} + \\Delta\\mathbf{A}^{-1}) = \\mathbf{I}\n$$\n展开左侧可得：\n$$\n\\mathbf{A} \\mathbf{A}^{-1} + \\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{I}\n$$\n由于 $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}$，方程简化为：\n$$\n\\mathbf{I} + \\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{I}\n$$\n两边同时减去 $\\mathbf{I}$ 可得：\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{0}\n$$\n问题考虑的是微小扰动 $\\Delta\\mathbf{A}$（因此 $\\Delta\\mathbf{A}^{-1}$ 也很小）。因此，项 $\\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1}$ 是二阶小量，在一阶近似中可以忽略不计。这导出了线性近似：\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} \\approx \\mathbf{0}\n$$\n为了求解 $\\Delta\\mathbf{A}^{-1}$，我们重新整理这些项：\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} \\approx -\\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\n假设 $\\mathbf{A}$ 是可逆的，我们可以从左侧乘以 $\\mathbf{A}^{-1}$：\n$$\n\\mathbf{A}^{-1} (\\mathbf{A} \\Delta\\mathbf{A}^{-1}) \\approx \\mathbf{A}^{-1} (-\\Delta\\mathbf{A} \\mathbf{A}^{-1})\n$$\n$$\n(\\mathbf{A}^{-1} \\mathbf{A}) \\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\n$$\n\\mathbf{I} \\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\n这就得到了最终的一阶关系式，它解释了 $\\mathbf{A}$ 中的误差是如何传播到其逆矩阵的：\n$$\n\\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\n此关系表明，逆矩阵中的误差被因子 $\\mathbf{A}^{-1}$ 放大。如果 $\\mathbf{A}^{-1}$ 的范数很大，即使是很小的误差 $\\Delta\\mathbf{A}$ 也可能导致很大的误差 $\\Delta\\mathbf{A}^{-1}$。\n\n接下来，我们计算给定矩阵 $\\mathbf{A}$ 的条件数 $\\kappa_2(\\mathbf{A})$。关于矩阵二范数的条件数定义为 $\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2$。矩阵的二范数是其最大的奇异值 $\\sigma_{\\max}$。对于对称矩阵，奇异值是其特征值的绝对值。矩阵 $\\mathbf{A}$ 如下所示：\n$$\n\\mathbf{A} = \\begin{pmatrix}\n1  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1\n\\end{pmatrix}\n$$\n这是一个实对称矩阵。我们通过求解特征方程 $\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0$ 来找到其特征值 $\\lambda$。\n$$\n\\det\\begin{pmatrix}\n1 - \\lambda  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1 - \\lambda\n\\end{pmatrix} = 0\n$$\n$$\n(1 - \\lambda)^2 - (-1 + \\varepsilon)^2 = 0\n$$\n$$\n(1 - \\lambda)^2 = (1 - \\varepsilon)^2\n$$\n对两边取平方根，得到两种可能性：\n$$\n1 - \\lambda = \\pm(1 - \\varepsilon)\n$$\n情况1：$1 - \\lambda = 1 - \\varepsilon \\implies \\lambda_1 = \\varepsilon$。\n情况2：$1 - \\lambda = -(1 - \\varepsilon) \\implies 1 - \\lambda = -1 + \\varepsilon \\implies \\lambda_2 = 2 - \\varepsilon$。\n$\\mathbf{A}$ 的特征值是 $\\lambda_1 = \\varepsilon$ 和 $\\lambda_2 = 2 - \\varepsilon$。考虑到 $0  \\varepsilon \\ll 1$，两个特征值都是正的。$\\mathbf{A}$ 的奇异值是 $\\sigma_1 = |\\lambda_1| = \\varepsilon$ 和 $\\sigma_2 = |\\lambda_2| = 2 - \\varepsilon$。\n$\\mathbf{A}$ 的二范数是最大的奇异值：\n$$\n\\|\\mathbf{A}\\|_2 = \\sigma_{\\max}(\\mathbf{A}) = \\max(\\varepsilon, 2 - \\varepsilon)\n$$\n由于 $0  \\varepsilon  1$，我们有 $2 - \\varepsilon > 1$ 和 $\\varepsilon  1$，所以 $2 - \\varepsilon > \\varepsilon$。因此，$\\|\\mathbf{A}\\|_2 = 2 - \\varepsilon$。\n\n现在我们求 $\\|\\mathbf{A}^{-1}\\|_2$。矩阵 $\\mathbf{A}^{-1}$ 也是对称的，所以它的二范数是其特征值绝对值的最大值。$\\mathbf{A}^{-1}$ 的特征值是 $\\mathbf{A}$ 特征值的倒数，即 $1/\\lambda_1 = 1/\\varepsilon$ 和 $1/\\lambda_2 = 1/(2 - \\varepsilon)$。\n$\\mathbf{A}^{-1}$ 的二范数是：\n$$\n\\|\\mathbf{A}^{-1}\\|_2 = \\sigma_{\\max}(\\mathbf{A}^{-1}) = \\max\\left(\\left|\\frac{1}{\\varepsilon}\\right|, \\left|\\frac{1}{2 - \\varepsilon}\\right|\\right)\n$$\n由于 $0  \\varepsilon \\ll 1$，我们有 $0  \\varepsilon  2 - \\varepsilon$，这意味着 $1/\\varepsilon > 1/(2 - \\varepsilon)$。因此，$\\|\\mathbf{A}^{-1}\\|_2 = 1/\\varepsilon$。\n条件数 $\\kappa_2(\\mathbf{A})$ 是这两个范数的乘积：\n$$\n\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2 = (2 - \\varepsilon) \\left(\\frac{1}{\\varepsilon}\\right) = \\frac{2 - \\varepsilon}{\\varepsilon}\n$$\n这个表达式表明，当 $\\varepsilon \\to 0$ 时，条件数 $\\kappa_2(\\mathbf{A}) \\to \\infty$。这证实了对于微小的 $\\varepsilon$，矩阵 $\\mathbf{A}$ 是病态的。\n\n最后，我们计算 $\\varepsilon = 0.01$ 时的 $\\kappa_2(\\mathbf{A})$。将此值代入推导出的公式中：\n$$\n\\kappa_2(\\mathbf{A}) = \\frac{2 - 0.01}{0.01} = \\frac{1.99}{0.01} = 199\n$$\n问题要求答案四舍五入到四位有效数字。精确值是 $199$。表示为四位有效数字，即为 $199.0$。",
            "answer": "$$\n\\boxed{199.0}\n$$"
        },
        {
            "introduction": "在掌握了如何分析模型的理论特性之后，我们将注意力转向实际的神经数据分析。直接解释高维度的神经元群体活动记录可能非常困难。一种强大且常用的方法是将这些活动投影到一个与特定任务或计算相关的低维“任务子空间”上。这个练习将提供一个具体的操作指南，教你如何执行这种投影，并量化该子空间能解释的神经活动方差的比例，从而帮助你从复杂的数据中提取有意义的模式。",
            "id": "3994746",
            "problem": "考虑一个神经群体活动矩阵 $X \\in \\mathbb{R}^{N \\times T}$，其中 $N$ 是神经元数量，$T$ 是时间点或试验次数。设 $B \\in \\mathbb{R}^{N \\times K}$ 是一组基向量（作为列），它们定义了一个任务子空间。仅使用线性代数的核心定义，在程序中执行以下任务：\n\n1. 对活动矩阵 $X$ 按时间进行均值中心化，生成 $X_0 \\in \\mathbb{R}^{N \\times T}$。具体方法是从 $X$ 的每一行中减去该行在 $T$ 个时间点上的样本均值。\n2. 将任务子空间解释为 $B$ 的列在 $\\mathbb{R}^N$ 中张成的空间。仅使用正交投影的定义——即子空间中与给定向量欧几里得距离最小的唯一元素——来构造 $X_0$ 的每一列到该子空间的正交投影，从而生成一个投影矩阵 $\\hat{X} \\in \\mathbb{R}^{N \\times T}$。\n3. 将方差解释分数定义为投影捕获的总方差与 $X_0$ 中总方差之比。使用总方差的定义，即所有条目的偏差平方和。如果 $X_0$ 中的总方差为零，则将方差解释分数定义为 $0$。\n\n你的程序必须将上述步骤应用于下面的每个测试用例，并为每个用例生成方差解释分数，结果四舍五入到六位小数。\n\n测试套件（每个 $X_i$ 和 $B_i$ 如下指定）：\n\n- 情况 1（一般情况，非标准正交基，非平凡捕获）：\n  $$\n  X_1 = \\begin{bmatrix}\n  1  2  0  -1 \\\\\n  0  1  -1  -2 \\\\\n  2  0  1  -1\n  \\end{bmatrix}, \\quad\n  B_1 = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  1  0\n  \\end{bmatrix}.\n  $$\n\n- 情况 2（数据完全位于子空间内）：\n  $$\n  X_2 = \\begin{bmatrix}\n  2  3  4 \\\\\n  0  0  0\n  \\end{bmatrix}, \\quad\n  B_2 = \\begin{bmatrix}\n  1 \\\\\n  0\n  \\end{bmatrix}.\n  $$\n\n- 情况 3（数据与子空间正交）：\n  $$\n  X_3 = \\begin{bmatrix}\n  0  0  0 \\\\\n  5  6  7\n  \\end{bmatrix}, \\quad\n  B_3 = \\begin{bmatrix}\n  1 \\\\\n  0\n  \\end{bmatrix}.\n  $$\n\n- 情况 4（非标准正交、秩亏基）：\n  $$\n  X_4 = \\begin{bmatrix}\n  1  2  -1  0  3 \\\\\n  1  2  -1  0  3 \\\\\n  2  0  1  3  -1\n  \\end{bmatrix}, \\quad\n  B_4 = \\begin{bmatrix}\n  1  2 \\\\\n  1  2 \\\\\n  0  0\n  \\end{bmatrix}.\n  $$\n\n- 情况 5（零方差边界情况）：\n  $$\n  X_5 = \\begin{bmatrix}\n  1  1  1 \\\\\n  2  2  2 \\\\\n  3  3  3\n  \\end{bmatrix}, \\quad\n  B_5 = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  0  0\n  \\end{bmatrix}.\n  $$\n\n最终输出格式：你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5]$），其中每个 $r_i$ 是情况 $i$ 的方差解释分数，四舍五入到六位小数。不应打印任何额外文本。",
            "solution": "该问题要求计算神经群体活动矩阵 $X \\in \\mathbb{R}^{N \\times T}$ 中，能被其到由矩阵 $B \\in \\mathbb{R}^{N \\times K}$ 的列空间定义的任务子空间上的正交投影所解释的方差分数。该过程涉及三个主要步骤：对数据进行均值中心化，将均值中心化的数据投影到子空间上，以及计算方差之比。\n\n设 $X$ 是活动矩阵，其中 $N$ 是神经元数量，$T$ 是时间点数量。设 $B$ 是其列向量张成任务子空间的矩阵。\n\n**步骤 1：对活动矩阵进行均值中心化**\n\n第一步是将原始活动矩阵 $X$ 转换为均值中心化矩阵 $X_0$。这通过从每个神经元活动的时间序列中减去其时间均值来实现。对于 $X$ 的每一行 $i$（代表第 $i$ 个神经元），计算其在 $T$ 个时间点上的平均活动 $\\mu_i$：\n$$\n\\mu_i = \\frac{1}{T} \\sum_{j=1}^{T} X_{ij}\n$$\n均值中心化矩阵 $X_0$ 的条目则由下式给出：\n$$\n(X_0)_{ij} = X_{ij} - \\mu_i\n$$\n在矩阵表示法中，如果我们定义一个行均值组成的列向量 $\\boldsymbol{\\mu} \\in \\mathbb{R}^{N}$，其中 $(\\boldsymbol{\\mu})_i = \\mu_i$，以及一个全为1的行向量 $\\mathbf{1}_T^T \\in \\mathbb{R}^{1 \\times T}$，那么均值中心化矩阵为：\n$$\nX_0 = X - \\boldsymbol{\\mu} \\mathbf{1}_T^T\n$$\n此操作确保 $X_0$ 的每一行的均值为 $0$。\n\n**步骤 2：到任务子空间的正交投影**\n\n任务子空间 $\\mathcal{S}$ 是矩阵 $B$ 的列空间，记作 $\\mathcal{S} = \\text{span}(B)$。我们需要找到 $X_0$ 的每个列向量在 $\\mathcal{S}$ 上的正交投影。对于一个任意向量 $\\mathbf{x} \\in \\mathbb{R}^N$（代表 $X_0$ 的一列），其正交投影 $\\hat{\\mathbf{x}}$ 是 $\\mathcal{S}$ 中最小化欧几里得距离 $\\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|_2$ 的唯一向量。\n\n由于 $\\hat{\\mathbf{x}} \\in \\mathcal{S}$，它可以表示为 $B$ 的列的线性组合，即 $\\hat{\\mathbf{x}} = B\\mathbf{c}$，其中 $\\mathbf{c} \\in \\mathbb{R}^K$ 是某个系数向量。问题在于找到最小化 $\\|\\mathbf{x} - B\\mathbf{c}\\|_2^2$ 的 $\\mathbf{c}$。这是一个经典的线性最小二乘问题。解决方案通过求解正规方程得到：\n$$\n(B^T B) \\mathbf{c} = B^T \\mathbf{x}\n$$\n如果 $B$ 的列是线性无关的，那么 $B^T B$ 是可逆的，且 $\\mathbf{c} = (B^T B)^{-1} B^T \\mathbf{x}$。投影则为 $\\hat{\\mathbf{x}} = B(B^T B)^{-1} B^T \\mathbf{x}$。矩阵 $P = B(B^T B)^{-1} B^T$ 是投影矩阵。\n\n然而，$B$ 的列可能是线性相关的（即，$B$ 不是满列秩的），这使得 $B^T B$ 成为奇异矩阵。这个问题仍然可以使用 Moore-Penrose 伪逆来解决，用上标 ‘$+$’ 表示。到 $B$ 的列空间上的投影矩阵 $P$ 由通用公式给出：\n$$\nP = B B^+\n$$\n这个公式普遍适用，无论 $B$ 的秩是多少。整个均值中心化数据矩阵 $X_0$ 到子空间 $\\mathcal{S}$ 上的投影，是通过将投影矩阵 $P$ 应用于其每一列得到的：\n$$\n\\hat{X} = P X_0 = (B B^+) X_0\n$$\n一个重要的性质是，如果 $X_0$ 是均值中心化的（行均值为0），其投影 $\\hat{X}$ 也是均值中心化的。这是因为 $X_0$ 的列向量之和是零向量，而 $P(\\mathbf{0}) = \\mathbf{0}$。\n\n**步骤 3：方差解释分数**\n\n问题将均值中心化矩阵的总方差定义为其所有条目的偏差平方和。对于像 $X_0$ 这样的均值中心化矩阵，任何条目与其行均值的偏差就是该条目本身。因此，总方差是矩阵的弗罗贝尼乌斯范数的平方：\n$$\n\\text{Var}(X_0) = \\sum_{i=1}^{N} \\sum_{j=1}^{T} (X_0)_{ij}^2 = \\|X_0\\|_F^2\n$$\n类似地，由于 $\\hat{X}$ 也是均值中心化的，投影捕获的方差为：\n$$\n\\text{Var}(\\hat{X}) = \\sum_{i=1}^{N} \\sum_{j=1}^{T} (\\hat{X})_{ij}^2 = \\|\\hat{X}\\|_F^2\n$$\n方差解释分数（EVF）是投影数据的方差与原始均值中心化数据的总方差之比：\n$$\n\\text{EVF} = \\frac{\\text{Var}(\\hat{X})}{\\text{Var}(X_0)} = \\frac{\\|\\hat{X}\\|_F^2}{\\|X_0\\|_F^2}\n$$\n根据问题陈述，如果总方差 $\\text{Var}(X_0)$ 为 $0$，则 EVF 定义为 $0$。当原始矩阵 $X$ 的所有行随时间保持不变时，会发生这种情况。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_explained_variance(X: np.ndarray, B: np.ndarray) - float:\n    \"\"\"\n    Calculates the fraction of variance in X explained by projection onto the\n    subspace spanned by the columns of B.\n\n    Args:\n        X (np.ndarray): The neural activity matrix of shape (N, T).\n        B (np.ndarray): The basis matrix for the task subspace of shape (N, K).\n\n    Returns:\n        float: The explained variance fraction.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    X = np.asarray(X, dtype=float)\n    B = np.asarray(B, dtype=float)\n\n    # Step 1: Mean-center the activity matrix X across time.\n    # The shape of row_means will be (N, 1), which broadcasts correctly.\n    row_means = X.mean(axis=1, keepdims=True)\n    X0 = X - row_means\n\n    # Step 2: Calculate the total variance in the mean-centered data.\n    # Total variance is the squared Frobenius norm of the mean-centered matrix.\n    total_variance = np.sum(X0**2)\n\n    # Handle the edge case where total variance is zero.\n    if np.isclose(total_variance, 0):\n        return 0.0\n\n    # Step 3: Construct the orthogonal projection of X0 onto the subspace.\n    # Compute the projection matrix P = B * B_pseudoinverse.\n    B_pinv = np.linalg.pinv(B)\n    P = B @ B_pinv\n\n    # Project the mean-centered data matrix X0.\n    X_hat = P @ X0\n\n    # Step 4: Calculate the variance captured by the projection.\n    # This is the squared Frobenius norm of the projected matrix.\n    projected_variance = np.sum(X_hat**2)\n\n    # Step 5: Compute the explained variance fraction.\n    explained_variance_fraction = projected_variance / total_variance\n\n    return explained_variance_fraction\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general, non-orthonormal basis, nontrivial capture)\n        {\n            'X': [[1, 2, 0, -1], [0, 1, -1, -2], [2, 0, 1, -1]],\n            'B': [[1, 0], [0, 1], [1, 0]],\n        },\n        # Case 2 (data lie entirely in the subspace)\n        {\n            'X': [[2, 3, 4], [0, 0, 0]],\n            'B': [[1], [0]],\n        },\n        # Case 3 (data orthogonal to the subspace)\n        {\n            'X': [[0, 0, 0], [5, 6, 7]],\n            'B': [[1], [0]],\n        },\n        # Case 4 (non-orthonormal, rank-deficient basis)\n        {\n            'X': [[1, 2, -1, 0, 3], [1, 2, -1, 0, 3], [2, 0, 1, 3, -1]],\n            'B': [[1, 2], [1, 2], [0, 0]],\n        },\n        # Case 5 (zero variance edge case)\n        {\n            'X': [[1, 1, 1], [2, 2, 2], [3, 3, 3]],\n            'B': [[1, 0], [0, 1], [0, 0]],\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case['X']\n        B = case['B']\n        evf = calculate_explained_variance(X, B)\n        # Round the result to six decimal places\n        results.append(f\"{evf:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}