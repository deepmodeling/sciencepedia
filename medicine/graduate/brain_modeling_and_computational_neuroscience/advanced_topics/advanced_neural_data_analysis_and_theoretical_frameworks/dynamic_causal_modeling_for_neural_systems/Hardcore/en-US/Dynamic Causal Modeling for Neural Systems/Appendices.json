{
    "hands_on_practices": [
        {
            "introduction": "A foundational skill in any modeling discipline is the ability to implement the generative process, which involves simulating data directly from the model's equations. This practice moves beyond pure theory by tasking you with building a simulation of a basic two-region bilinear DCM. By translating the state-space equations into code and observing how neural activity evolves in response to a defined input, you will gain a concrete intuition for how DCMs link experimental design to dynamic brain activity ().",
            "id": "3976292",
            "problem": "Construct a minimal dynamic causal model for two interacting neural regions with a single exogenous input targeted at the first region, using a bilinear state equation and a linear observation model. The continuous-time latent neural state is a vector $x(t) \\in \\mathbb{R}^2$ with dynamics governed by the bilinear ordinary differential equation $\\dot{x}(t) = A x(t) + B u(t) x(t) + C u(t)$, and an observation model $y(t) = h(x(t)) + \\epsilon(t)$, where $y(t) \\in \\mathbb{R}^2$, $u(t) \\in \\mathbb{R}$ is a scalar exogenous input, $A \\in \\mathbb{R}^{2 \\times 2}$ is the intrinsic effective connectivity, $B \\in \\mathbb{R}^{2 \\times 2}$ is the input-dependent modulation of connectivity, $C \\in \\mathbb{R}^{2}$ is the direct input gain vector, and $h(x)$ is a linear mapping $h(x) = L x$ with $L \\in \\mathbb{R}^{2 \\times 2}$. Assume measurement noise $\\epsilon(t)$ is identically zero for simulation, i.e., $\\epsilon(t) = 0$.\n\nUse the following fixed parameters for all simulations:\n- Intrinsic connectivity $A = \\begin{bmatrix} -1.0 & 0.3 \\\\ 0.2 & -0.8 \\end{bmatrix}$.\n- Modulatory connectivity $B = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.5 & 0.0 \\end{bmatrix}$.\n- Direct input gain $C = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$.\n- Observation matrix $L = I_2 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- Initial state $x(0) = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n\nThe input $u(t)$ is a rectangular pulse defined by three parameters: amplitude $a$, onset time $t_{\\mathrm{on}}$, and offset time $t_{\\mathrm{off}}$. Specifically,\n$$\nu(t) = \\begin{cases}\na, & t_{\\mathrm{on}} \\le t < t_{\\mathrm{off}},\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n\nStarting from the definition of the time derivative and the forward Euler method for numerical integration of ordinary differential equations, derive a discrete-time update that approximates the solution of the bilinear system over a time grid with step size $\\Delta t$, and specify how to compute $y(t)$ at the final time $T$ given the above assumptions. Then implement a program that simulates the system for the following test suite of input parameters, each specified as a tuple $(a, t_{\\mathrm{on}}, t_{\\mathrm{off}}, T, \\Delta t)$:\n- Case $1$: $(1.0, 0.5, 1.5, 3.0, 0.001)$.\n- Case $2$: $(0.0, 0.5, 1.5, 3.0, 0.001)$.\n- Case $3$: $(2.0, 0.0, 0.01, 3.0, 0.001)$.\n- Case $4$: $(1.0, 0.5, 1.5, 3.0, 0.01)$.\n\nFor each case, simulate the trajectory from $t=0$ to $t=T$ using the Euler scheme you derive, and return the final observed state vector $y(T) \\in \\mathbb{R}^2$. Express each component of $y(T)$ as a floating-point number rounded to six decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the two-component list $[y_1(T),y_2(T)]$ for one case, with no spaces anywhere. For example, the overall output format must be exactly like $[[y_{11},y_{12}],[y_{21},y_{22}],[y_{31},y_{32}],[y_{41},y_{42}]]$.",
            "solution": "The user wants to construct a minimal dynamic causal model and simulate it.\n\n### Step 1: Extract Givens\n- **State Equation**: $\\dot{x}(t) = A x(t) + B u(t) x(t) + C u(t)$, where $x(t) \\in \\mathbb{R}^2$ is the latent neural state, and $u(t) \\in \\mathbb{R}$ is a scalar input.\n- **Observation Equation**: $y(t) = L x(t) + \\epsilon(t)$, where $y(t) \\in \\mathbb{R}^2$ is the observation, with the assumption that measurement noise $\\epsilon(t) = 0$.\n- **Intrinsic Connectivity**: $A = \\begin{bmatrix} -1.0 & 0.3 \\\\ 0.2 & -0.8 \\end{bmatrix}$.\n- **Modulatory Connectivity**: $B = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.5 & 0.0 \\end{bmatrix}$.\n- **Direct Input Gain**: $C = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$.\n- **Observation Matrix**: $L = I_2 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- **Initial State**: $x(0) = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n- **Input Function**: A rectangular pulse $u(t) = a$ for $t_{\\mathrm{on}} \\le t < t_{\\mathrm{off}}$ and $u(t)=0$ otherwise.\n- **Numerical Method**: Forward Euler method with a time step $\\Delta t$.\n- **Task**: For a given set of parameters $(a, t_{\\mathrm{on}}, t_{\\mathrm{off}}, T, \\Delta t)$, simulate the system from $t=0$ to $t=T$ and report the final observed state $y(T)$.\n- **Test Cases**:\n    - Case 1: $(1.0, 0.5, 1.5, 3.0, 0.001)$\n    - Case 2: $(0.0, 0.5, 1.5, 3.0, 0.001)$\n    - Case 3: $(2.0, 0.0, 0.01, 3.0, 0.001)$\n    - Case 4: $(1.0, 0.5, 1.5, 3.0, 0.01)$\n- **Output**: The components of the final state vector $y(T)$ for each case, rounded to six decimal places, formatted as a single-line string.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, describing a standard bilinear state-space model used in dynamic causal modeling (DCM), a well-established field in computational neuroscience. The system of ordinary differential equations (ODEs) is well-defined with a given initial condition, ensuring a unique solution exists. All parameters, variables, and constants are specified with mathematical precision and without ambiguity. The problem is self-contained, consistent, and computationally feasible. The numerical method requested (forward Euler) is a standard technique for solving ODEs. The task is a straightforward application of numerical methods to a defined physical model, making it a valid scientific problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Design\nThe problem requires the numerical integration of a system of first-order, non-linear ordinary differential equations (ODEs) that define the dynamics of the neural states. The specified model is a bilinear system, a class of non-linear systems where the non-linearity arises from the product of the state and the input.\n\nThe continuous-time dynamics of the latent neural state $x(t)$ are given by:\n$$\n\\dot{x}(t) = \\frac{dx(t)}{dt} = A x(t) + u(t) B x(t) + C u(t)\n$$\nHere, the term $u(t) B x(t)$ represents the modulatory effect of the input $u(t)$ on the intrinsic connectivity between the neural regions. The term $C u(t)$ represents the direct driving effect of the input on the regions.\n\nTo simulate the trajectory of $x(t)$ over a finite time interval $[0, T]$, we must discretize this continuous-time equation. The problem specifies the use of the forward Euler method, a first-order numerical procedure for solving ODEs with a given initial value.\n\nThe derivation of the forward Euler method begins with the definition of the time derivative:\n$$\n\\frac{dx(t)}{dt} = \\lim_{\\Delta t \\to 0} \\frac{x(t + \\Delta t) - x(t)}{\\Delta t}\n$$\nFor a small, finite time step $\\Delta t > 0$, we can approximate the derivative as:\n$$\n\\frac{dx(t)}{dt} \\approx \\frac{x(t + \\Delta t) - x(t)}{\\Delta t}\n$$\nSubstituting the system's differential equation into this approximation gives:\n$$\n\\frac{x(t + \\Delta t) - x(t)}{\\Delta t} \\approx A x(t) + u(t) B x(t) + C u(t)\n$$\nRearranging this equation to solve for the state at the next time step, $x(t + \\Delta t)$, yields the forward Euler update rule:\n$$\nx(t + \\Delta t) \\approx x(t) + \\Delta t \\left( A x(t) + u(t) B x(t) + C u(t) \\right)\n$$\nTo implement this numerically, we define a discrete time grid $t_k = k \\Delta t$ for $k = 0, 1, 2, \\dots, N$, where $N = T / \\Delta t$. Let $x_k = x(t_k)$ be the state at time $t_k$, and $u_k = u(t_k)$ be the input at that time. The discrete-time update equation is:\n$$\nx_{k+1} = x_k + \\Delta t \\left( A x_k + u_k B x_k + u_k C \\right)\n$$\nThe simulation algorithm proceeds as follows:\n1.  Initialize the state vector at time $t_0 = 0$ with the given initial condition: $x_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n2.  Determine the total number of integration steps, $N = \\text{integer}(T / \\Delta t)$.\n3.  Iterate for $k$ from $0$ to $N-1$:\n    a. Calculate the current time $t_k = k \\Delta t$.\n    b. Evaluate the input function $u_k = u(t_k)$ based on its definition: $u_k = a$ if $t_{\\mathrm{on}} \\le t_k < t_{\\mathrm{off}}$, and $u_k = 0$ otherwise.\n    c. Compute the state derivative approximation (the right-hand side of the ODE) at time $t_k$: $\\dot{x}_k = A x_k + u_k B x_k + u_k C$.\n    d. Apply the Euler update rule to find the state at the next time step: $x_{k+1} = x_k + \\Delta t \\cdot \\dot{x}_k$.\n4.  After the loop completes, the final state is $x_N$, which approximates $x(T)$.\n\nThe final step is to compute the observed output at time $T$. The observation model is given by $y(t) = L x(t) + \\epsilon(t)$. With the provided parameters $\\epsilon(t) = 0$ and $L = I_2$ (the $2 \\times 2$ identity matrix), the observation equation simplifies to:\n$$\ny(T) = I_2 x(T) = x(T)\n$$\nTherefore, the final observed state $y(T)$ is identical to the final latent state $x(T)$ obtained from the numerical integration. The program will execute this simulation for each test case, round the two components of the final vector $y(T)$ to six decimal places, and format the results as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the dynamic causal modeling problem by simulating a bilinear system\n    using the forward Euler method for a given set of test cases.\n    \"\"\"\n\n    # Define the fixed model parameters as specified in the problem.\n    # Intrinsic connectivity matrix A\n    A = np.array([[-1.0, 0.3], [0.2, -0.8]])\n    # Modulatory connectivity matrix B\n    B = np.array([[0.0, 0.0], [0.5, 0.0]])\n    # Direct input gain vector C\n    C = np.array([1.0, 0.0])\n    # Initial state vector x(0)\n    x0 = np.array([0.0, 0.0])\n\n    # The observation matrix L is the identity matrix, and noise is zero,\n    # so y(t) = x(t). We don't need L in the code.\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (a, t_on, t_off, T, dt)\n    test_cases = [\n        (1.0, 0.5, 1.5, 3.0, 0.001),  # Case 1\n        (0.0, 0.5, 1.5, 3.0, 0.001),  # Case 2\n        (2.0, 0.0, 0.01, 3.0, 0.001),  # Case 3\n        (1.0, 0.5, 1.5, 3.0, 0.01),   # Case 4\n    ]\n\n    results = []\n    # Process each test case.\n    for case in test_cases:\n        a, t_on, t_off, T, dt = case\n\n        # Calculate the number of integration steps.\n        num_steps = int(round(T / dt))\n        \n        # Initialize the state vector.\n        x = x0.copy()\n\n        # Perform the numerical integration using the forward Euler method.\n        for k in range(num_steps):\n            current_t = k * dt\n            \n            # Determine the value of the input u(t) at the current time step.\n            u_val = 0.0\n            if t_on <= current_t < t_off:\n                u_val = a\n            \n            # Calculate the derivative dx/dt using the bilinear state equation:\n            # dx/dt = A*x + u*(B*x) + u*C\n            dxdt = A @ x + u_val * (B @ x) + u_val * C\n            \n            # Update the state vector using the forward Euler step.\n            x = x + dt * dxdt\n\n        # The final observed state y(T) is equal to the final latent state x(T).\n        y_T = x\n        \n        # Round the components of the final state vector to six decimal places.\n        y_T_rounded = [round(y_T[0], 6), round(y_T[1], 6)]\n        results.append(y_T_rounded)\n\n    # Format the final list of results into the exact required single-line string,\n    # e.g., [[y11,y12],[y21,y22],...], with no spaces.\n    final_output_str = str(results).replace(' ', '')\n    \n    # Final print statement in the exact required format.\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While bilinear models are powerful, neural dynamics are inherently nonlinear. This exercise introduces a more complex nonlinear DCM and challenges you to analyze its local behavior using fundamental techniques from dynamical systems theory. You will derive the system's Jacobian matrix, $J(x)$, and use its eigenvalues to assess the stability of the system at a specific operating point, providing deep insight into how state-dependent modulations can shape the dynamic repertoire of a neural circuit ().",
            "id": "3976258",
            "problem": "Consider a simplified nonlinear Dynamic Causal Modeling (DCM) formulation for a pair of interacting neuronal populations, where the deterministic part of the generative model is given by the continuous-time dynamical system\n$$\n\\frac{d x}{d t} = f(x) = A x + \\sum_{k=1}^{2} x_{k} \\, D^{(k)} x,\n$$\nwith state $x \\in \\mathbb{R}^{2}$ representing population activities. Here, $A \\in \\mathbb{R}^{2 \\times 2}$ encodes baseline effective connectivity, and $D^{(k)} \\in \\mathbb{R}^{2 \\times 2}$ encodes nonlinear, state-dependent gating of connections by the activity $x_{k}$. Assume entries of $A$ and $D^{(k)}$ have physical units of $\\mathrm{s}^{-1}$ and the state $x$ is dimensionless. The operating point is specified as $x^{\\ast} = \\begin{pmatrix} 0.2 \\\\ 0.5 \\end{pmatrix}$.\n\nUsing first principles of linearization for continuous-time dynamical systems and the definition of the Jacobian $J(x) = \\frac{\\partial f}{\\partial x}(x)$, perform the following:\n\n1. Starting from the definition of $f(x)$ above, derive how the nonlinear term $\\sum_{k=1}^{2} x_{k} \\, D^{(k)} x$ contributes to the Jacobian $J(x)$ at a general state $x$. Your derivation must be explicit at the component level for $J_{i m}(x)$, where $i$ indexes output dimensions and $m$ indexes the partial derivative with respect to $x_{m}$.\n2. Using your general expression, compute the Jacobian $J(x^{\\ast})$ for the specific parameterization\n$$\nA = \\begin{pmatrix} -2.0 & 1.2 \\\\ 0.7 & -1.4 \\end{pmatrix}, \\quad\nD^{(1)} = \\begin{pmatrix} 0 & 0.5 \\\\ -0.3 & 0 \\end{pmatrix}, \\quad\nD^{(2)} = \\begin{pmatrix} 0 & -0.4 \\\\ 0.2 & 0 \\end{pmatrix}.\n$$\n3. Quantify the local stability at $x^{\\ast}$ by computing the largest real part of the eigenvalues of $J(x^{\\ast})$. Express your final stability metric in $\\mathrm{s}^{-1}$ and round your answer to four significant figures.\n\nYour final answer must be a single real number.",
            "solution": "The problem asks for the analysis of a simplified nonlinear Dynamic Causal Model (DCM) for a two-population system. This involves deriving the system's Jacobian, evaluating it at a specified operating point, and determining the local stability by finding the largest real part of its eigenvalues.\n\nThe state-space model is given by the differential equation:\n$$\n\\frac{d x}{d t} = f(x) = A x + \\sum_{k=1}^{2} x_{k} \\, D^{(k)} x\n$$\nwhere $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ is the state vector. The Jacobian matrix $J(x)$ is defined by its components $J_{im}(x) = \\frac{\\partial f_i(x)}{\\partial x_m}$, where $f_i(x)$ is the $i$-th component of the vector field $f(x)$.\n\nThe function $f(x)$ is a sum of a linear term, $A x$, and a nonlinear term, which we can denote as $g(x) = \\sum_{k=1}^{2} x_k D^{(k)} x$. By linearity of differentiation, the Jacobian is $J(x) = A + \\frac{\\partial g}{\\partial x}(x)$.\n\n**Part 1: Derivation of the Jacobian for the nonlinear term**\n\nWe first write the $i$-th component of the nonlinear term $g(x)$ explicitly.\n$$\ng_i(x) = \\left( \\sum_{k=1}^{2} x_k D^{(k)} x \\right)_i = \\sum_{k=1}^{2} x_k (D^{(k)} x)_i\n$$\nThe term $(D^{(k)} x)_i$ is the $i$-th component of the vector resulting from the matrix-vector product $D^{(k)} x$, which is $\\sum_{j=1}^{2} D^{(k)}_{ij} x_j$. Substituting this gives:\n$$\ng_i(x) = \\sum_{k=1}^{2} x_k \\left( \\sum_{j=1}^{2} D^{(k)}_{ij} x_j \\right) = \\sum_{k=1}^{2} \\sum_{j=1}^{2} D^{(k)}_{ij} x_k x_j\n$$\nNow, we find the contribution of this term to the Jacobian by taking the partial derivative with respect to $x_m$, where $m \\in \\{1, 2\\}$.\n$$\n\\frac{\\partial g_i(x)}{\\partial x_m} = \\frac{\\partial}{\\partial x_m} \\left( \\sum_{k=1}^{2} \\sum_{j=1}^{2} D^{(k)}_{ij} x_k x_j \\right)\n$$\nSince $D^{(k)}_{ij}$ are constants, we can move the derivative inside the sums:\n$$\n\\frac{\\partial g_i(x)}{\\partial x_m} = \\sum_{k=1}^{2} \\sum_{j=1}^{2} D^{(k)}_{ij} \\frac{\\partial (x_k x_j)}{\\partial x_m}\n$$\nUsing the product rule for differentiation and the fact that $\\frac{\\partial x_p}{\\partial x_q} = \\delta_{pq}$ (the Kronecker delta), we get:\n$$\n\\frac{\\partial (x_k x_j)}{\\partial x_m} = \\frac{\\partial x_k}{\\partial x_m} x_j + x_k \\frac{\\partial x_j}{\\partial x_m} = \\delta_{km} x_j + x_k \\delta_{jm}\n$$\nSubstituting this back into the expression for the partial derivative of $g_i(x)$:\n$$\n\\frac{\\partial g_i(x)}{\\partial x_m} = \\sum_{k=1}^{2} \\sum_{j=1}^{2} D^{(k)}_{ij} (\\delta_{km} x_j + x_k \\delta_{jm}) = \\sum_{k=1}^{2} \\sum_{j=1}^{2} D^{(k)}_{ij} \\delta_{km} x_j + \\sum_{k=1}^{2} \\sum_{j=1}^{2} D^{(k)}_{ij} x_k \\delta_{jm}\n$$\nIn the first term, the sum over $k$ collapses because $\\delta_{km}$ is non-zero only when $k=m$. In the second term, the sum over $j$ collapses because $\\delta_{jm}$ is non-zero only when $j=m$.\n$$\n\\frac{\\partial g_i(x)}{\\partial x_m} = \\sum_{j=1}^{2} D^{(m)}_{ij} x_j + \\sum_{k=1}^{2} D^{(k)}_{im} x_k\n$$\nThis is the general expression for the $(i, m)$-th component of the Jacobian of the nonlinear term. The full Jacobian is $J_{im}(x) = A_{im} + \\frac{\\partial g_i(x)}{\\partial x_m}$.\n\n**Part 2: Computation of the Jacobian at the operating point $x^{\\ast}$**\n\nWe are given the operating point $x^{\\ast} = \\begin{pmatrix} 0.2 \\\\ 0.5 \\end{pmatrix}$ and the matrices:\n$$\nA = \\begin{pmatrix} -2.0 & 1.2 \\\\ 0.7 & -1.4 \\end{pmatrix}, \\quad\nD^{(1)} = \\begin{pmatrix} 0 & 0.5 \\\\ -0.3 & 0 \\end{pmatrix}, \\quad\nD^{(2)} = \\begin{pmatrix} 0 & -0.4 \\\\ 0.2 & 0 \\end{pmatrix}\n$$\nLet's compute the Jacobian $J(x^{\\ast})$ by first calculating the Jacobian of the nonlinear term, $J_g(x^{\\ast})$, and then adding it to $A$. Let $J_g(x^{\\ast})$ have components $J_{g,im}$.\n\nFor $(i,m) = (1,1)$:\n$J_{g,11} = \\sum_{j=1}^{2} D^{(1)}_{1j} x_j^{\\ast} + \\sum_{k=1}^{2} D^{(k)}_{11} x_k^{\\ast} = (D^{(1)}_{11}x_1^{\\ast} + D^{(1)}_{12}x_2^{\\ast}) + (D^{(1)}_{11}x_1^{\\ast} + D^{(2)}_{11}x_2^{\\ast})$\n$J_{g,11} = (0 \\cdot 0.2 + 0.5 \\cdot 0.5) + (0 \\cdot 0.2 + 0 \\cdot 0.5) = 0.25 + 0 = 0.25$\n\nFor $(i,m) = (1,2)$:\n$J_{g,12} = \\sum_{j=1}^{2} D^{(2)}_{1j} x_j^{\\ast} + \\sum_{k=1}^{2} D^{(k)}_{12} x_k^{\\ast} = (D^{(2)}_{11}x_1^{\\ast} + D^{(2)}_{12}x_2^{\\ast}) + (D^{(1)}_{12}x_1^{\\ast} + D^{(2)}_{12}x_2^{\\ast})$\n$J_{g,12} = (0 \\cdot 0.2 + (-0.4) \\cdot 0.5) + (0.5 \\cdot 0.2 + (-0.4) \\cdot 0.5) = -0.2 + (0.1 - 0.2) = -0.3$\n\nFor $(i,m) = (2,1)$:\n$J_{g,21} = \\sum_{j=1}^{2} D^{(1)}_{2j} x_j^{\\ast} + \\sum_{k=1}^{2} D^{(k)}_{21} x_k^{\\ast} = (D^{(1)}_{21}x_1^{\\ast} + D^{(1)}_{22}x_2^{\\ast}) + (D^{(1)}_{21}x_1^{\\ast} + D^{(2)}_{21}x_2^{\\ast})$\n$J_{g,21} = (-0.3 \\cdot 0.2 + 0 \\cdot 0.5) + (-0.3 \\cdot 0.2 + 0.2 \\cdot 0.5) = -0.06 + (-0.06 + 0.1) = -0.02$\n\nFor $(i,m) = (2,2)$:\n$J_{g,22} = \\sum_{j=1}^{2} D^{(2)}_{2j} x_j^{\\ast} + \\sum_{k=1}^{2} D^{(k)}_{22} x_k^{\\ast} = (D^{(2)}_{21}x_1^{\\ast} + D^{(2)}_{22}x_2^{\\ast}) + (D^{(1)}_{22}x_1^{\\ast} + D^{(2)}_{22}x_2^{\\ast})$\n$J_{g,22} = (0.2 \\cdot 0.2 + 0 \\cdot 0.5) + (0 \\cdot 0.2 + 0 \\cdot 0.5) = 0.04 + 0 = 0.04$\n\nSo, the Jacobian of the nonlinear term at $x^{\\ast}$ is $J_g(x^{\\ast}) = \\begin{pmatrix} 0.25 & -0.3 \\\\ -0.02 & 0.04 \\end{pmatrix}$.\n\nThe full Jacobian is $J(x^{\\ast}) = A + J_g(x^{\\ast})$.\n$$\nJ(x^{\\ast}) = \\begin{pmatrix} -2.0 & 1.2 \\\\ 0.7 & -1.4 \\end{pmatrix} + \\begin{pmatrix} 0.25 & -0.3 \\\\ -0.02 & 0.04 \\end{pmatrix} = \\begin{pmatrix} -1.75 & 0.9 \\\\ 0.68 & -1.36 \\end{pmatrix}\n$$\n\n**Part 3: Stability analysis via eigenvalues**\n\nLocal stability at the operating point $x^{\\ast}$ is determined by the eigenvalues of the Jacobian matrix $J(x^{\\ast})$. The system is stable if the real parts of all eigenvalues are negative. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(J(x^{\\ast}) - \\lambda I) = 0$. For a $2 \\times 2$ matrix, this equation is $\\lambda^2 - \\text{Tr}(J) \\lambda + \\det(J) = 0$.\n\nLet $J' = J(x^{\\ast})$.\nThe trace of $J'$ is $\\text{Tr}(J') = -1.75 + (-1.36) = -3.11$.\nThe determinant of $J'$ is $\\det(J') = (-1.75)(-1.36) - (0.9)(0.68) = 2.38 - 0.612 = 1.768$.\n\nThe characteristic equation is:\n$$\n\\lambda^2 - (-3.11)\\lambda + 1.768 = 0 \\implies \\lambda^2 + 3.11\\lambda + 1.768 = 0\n$$\nThe roots are given by the quadratic formula:\n$$\n\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{-3.11 \\pm \\sqrt{(3.11)^2 - 4(1)(1.768)}}{2}\n$$\n$$\n\\lambda = \\frac{-3.11 \\pm \\sqrt{9.6721 - 7.072}}{2} = \\frac{-3.11 \\pm \\sqrt{2.6001}}{2}\n$$\nSince the discriminant $2.6001 > 0$, both eigenvalues are real. The stability metric is the largest real part of the eigenvalues, which is simply the larger of the two real eigenvalues.\n$$\n\\lambda_1 = \\frac{-3.11 + \\sqrt{2.6001}}{2} \\quad \\text{and} \\quad \\lambda_2 = \\frac{-3.11 - \\sqrt{2.6001}}{2}\n$$\nClearly, $\\lambda_1$ is the largest eigenvalue.\nCalculating its value:\n$\\sqrt{2.6001} \\approx 1.6124825$\n$$\n\\lambda_1 \\approx \\frac{-3.11 + 1.6124825}{2} = \\frac{-1.4975175}{2} \\approx -0.74875875\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n\\lambda_1 \\approx -0.7488\n$$\nThe units of the entries of $A$ and $D^{(k)}$ are $\\mathrm{s}^{-1}$, and $x$ is dimensionless. Therefore, the units of the Jacobian entries and its eigenvalues are $\\mathrm{s}^{-1}$. The largest real part of the eigenvalues is $-0.7488 \\, \\mathrm{s}^{-1}$. Since this is negative, the operating point $x^{\\ast}$ is locally stable.\nThe stability metric is the value itself.",
            "answer": "$$\\boxed{-0.7488}$$"
        },
        {
            "introduction": "Dynamic Causal Modeling is most powerful when used to test hypotheses about groups of subjects, connecting brain mechanisms to individual differences. This final practice bridges the gap between single-subject analysis and population-level inference using the framework of Parametric Empirical Bayes (PEB). You will implement a second-level hierarchical model to estimate how a group-level variable, such as symptom severity, relates to effective connectivity parameters, and learn to quantify the statistical evidence for such an effect (). This exercise is crucial for understanding how DCM is applied to draw scientifically meaningful conclusions from neuroimaging data.",
            "id": "3976280",
            "problem": "You are given subject-specific posterior summaries from Dynamic Causal Modeling (DCM) and a single group-level regressor representing symptom severity. The goal is to perform Parametric Empirical Bayes (PEB) at the second level to estimate the group effect and to compute its posterior probability that the severity effect is greater than zero. Use the following hierarchical setup grounded in Gaussian linear models and Bayes' theorem. The model assumptions are:\n\n- Dynamic Causal Modeling (DCM) provides, for each subject $i$, a posterior for a single scalar parameter (e.g., an effective connectivity) that is well-approximated by a Gaussian with mean $\\mu_i$ and variance $\\sigma_i^2$.\n- The second-level PEB model is a linear regression linking $\\mu_i$ to a group regressor with an intercept: $\\mu_i = x_{i0}\\,\\beta_0 + x_{i1}\\,\\beta_1 + \\epsilon_i$, where $x_{i0} = 1$ for all $i$, $x_{i1}$ is the symptom severity for subject $i$, and $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_i^2)$ reflects estimation uncertainty propagated from the first level.\n- The prior over the group parameters is Gaussian: $\\beta \\sim \\mathcal{N}(0,\\Sigma_0)$, where $\\beta = [\\beta_0,\\beta_1]^\\top$ and $\\Sigma_0$ is a known positive-definite covariance matrix.\n\nStarting from Bayes' theorem and the properties of the multivariate normal distribution, derive the posterior distribution $p(\\beta \\mid \\mu)$ under the above assumptions. Express the posterior mean and covariance in terms of the design matrix $X$, the subject-wise precisions (inverse variances), and the prior covariance $\\Sigma_0$. Then, for the severity effect $\\beta_1$, compute the posterior probability $p(\\beta_1 > 0 \\mid \\text{data})$.\n\nImplement a program that, for each test case below, constructs the hierarchical model, computes the posterior for $\\beta$, marginalizes to the component $\\beta_1$, and outputs the posterior probability $p(\\beta_1 > 0 \\mid \\text{data})$ as a decimal. No physical units are involved; probabilities must be expressed as decimals.\n\nUse the following test suite. In each case, the regressor vector of severities is $s$, the observed subject means are $y$ (representing $\\mu_i$), the observed subject variances are $v$ (representing $\\sigma_i^2$), and the prior covariance for $\\beta$ is diagonal with entries specified for the intercept and slope as $(\\sigma_{\\beta_0}^2,\\sigma_{\\beta_1}^2)$:\n\n- Test case $1$ (typical informative case):\n  - $s = [0.2,-0.1,0.3,0.0,0.5,-0.4]$\n  - $y = [0.19,-0.10,0.25,-0.01,0.42,-0.35]$\n  - $v = [0.04,0.05,0.03,0.06,0.02,0.07]$\n  - Prior diagonal variances $(\\sigma_{\\beta_0}^2,\\sigma_{\\beta_1}^2) = (1.0,1.0)$\n- Test case $2$ (one subject with large uncertainty, near-boundary informativeness):\n  - $s = [-0.5,-0.2,0.0,0.2,0.6]$\n  - $y = [-0.10,-0.04,-0.10,0.06,0.21]$\n  - $v = [0.02,0.02,2.0,0.02,0.02]$\n  - Prior diagonal variances $(\\sigma_{\\beta_0}^2,\\sigma_{\\beta_1}^2) = (2.0,2.0)$\n- Test case $3$ (weak evidence for severity effect, near-zero slope):\n  - $s = [-0.3,-0.1,0.0,0.1,0.3,0.5]$\n  - $y = [-0.02,0.01,0.00,-0.01,0.02,0.00]$\n  - $v = [0.05,0.05,0.05,0.05,0.05,0.05]$\n  - Prior diagonal variances $(\\sigma_{\\beta_0}^2,\\sigma_{\\beta_1}^2) = (1.0,1.0)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$), where $r_k$ is the posterior probability $p(\\beta_1>0 \\mid \\text{data})$ for test case $k$. The outputs must be decimals.",
            "solution": "The problem requires the derivation and computation of posterior quantities within a hierarchical Bayesian model, specifically a second-level Parametric Empirical Bayes (PEB) analysis of Dynamic Causal Modeling (DCM) posteriors. We will first derive the posterior distribution for the group-level parameters and then detail the method for computing the posterior probability of the effect of interest.\n\n### 1. Hierarchical Model Formulation\n\nThe problem describes a two-level hierarchical model. At the first level, a DCM analysis for each of $N$ subjects has yielded posterior distributions for a scalar parameter. These posteriors are approximated by Gaussian distributions. For subject $i$, the posterior parameter estimate is a random variable with mean $\\mu_i$ and variance $\\sigma_i^2$. These posterior means become the data for the second-level analysis.\n\nThe second-level model is a general linear model that seeks to explain the subject-to-subject variability in the parameter means $\\mu_i$ using a group-level regressor. The model is given as:\n$$\n\\mu_i = x_{i0}\\,\\beta_0 + x_{i1}\\,\\beta_1 + \\epsilon_i\n$$\nwhere $\\beta_0$ is the group-level intercept, $\\beta_1$ is the effect of symptom severity, $x_{i0}=1$ for all subjects, and $x_{i1}$ is the symptom severity for subject $i$. The term $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$ represents the uncertainty (estimation error) propagated from the first-level DCM, with $\\sigma_i^2$ being the posterior variance from subject $i$'s analysis.\n\nWe can express this model for all $N$ subjects in matrix form. Let $y$ be the $N \\times 1$ column vector of subject-specific means $[\\mu_1, \\mu_2, \\dots, \\mu_N]^\\top$. Let $X$ be the $N \\times 2$ design matrix, where the first column is a vector of ones and the second column is the vector of symptom severities $[x_{11}, x_{21}, \\dots, x_{N1}]^\\top$. Let $\\beta$ be the $2 \\times 1$ vector of group-level parameters $[\\beta_0, \\beta_1]^\\top$. The model is then:\n$$\ny = X\\beta + \\epsilon\n$$\nwhere $\\epsilon$ is the vector of noise terms $[\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_N]^\\top$. Since the $\\epsilon_i$ are independent, their covariance matrix, denoted $V$, is a diagonal matrix with the first-level variances $\\sigma_i^2$ on the diagonal: $V = \\text{diag}(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_N^2)$. The distribution of the data $y$ given the parameters $\\beta$ (the likelihood) is therefore a multivariate normal distribution:\n$$\np(y \\mid \\beta) = \\mathcal{N}(y; X\\beta, V)\n$$\n\nThe problem specifies a Gaussian prior on the group parameters $\\beta$:\n$$\np(\\beta) = \\mathcal{N}(\\beta; \\beta_p, \\Sigma_p)\n$$\nwhere the prior mean is the zero vector, $\\beta_p = [0, 0]^\\top$, and the prior covariance $\\Sigma_0$ is a known positive-definite matrix.\n\n### 2. Derivation of the Posterior Distribution\n\nOur goal is to find the posterior distribution of $\\beta$ given the data $y$, denoted $p(\\beta \\mid y)$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(\\beta \\mid y) \\propto p(y \\mid \\beta) p(\\beta)\n$$\nSince both the likelihood and the prior are Gaussian, their product will also be proportional to a Gaussian distribution. We can find the parameters of this posterior Gaussian by examining the log-posterior, focusing on terms that depend on $\\beta$.\n\nThe log-posterior is:\n$$\n\\ln p(\\beta \\mid y) = \\ln p(y \\mid \\beta) + \\ln p(\\beta) + \\text{const}\n$$\nThe log-likelihood is:\n$$\n\\ln p(y \\mid \\beta) = -\\frac{1}{2}(y - X\\beta)^\\top V^{-1} (y - X\\beta) + \\text{const}\n$$\nThe log-prior is:\n$$\n\\ln p(\\beta) = -\\frac{1}{2}(\\beta - \\beta_p)^\\top \\Sigma_0^{-1} (\\beta - \\beta_p) + \\text{const}\n$$\nSubstituting $\\beta_p=0$ and combining gives:\n$$\n\\ln p(\\beta \\mid y) = -\\frac{1}{2} \\left[ (y - X\\beta)^\\top V^{-1} (y - X\\beta) + \\beta^\\top \\Sigma_0^{-1} \\beta \\right] + \\text{const}\n$$\nExpanding the quadratic form:\n$$\n\\ln p(\\beta \\mid y) = -\\frac{1}{2} \\left[ y^\\top V^{-1} y - 2y^\\top V^{-1} X\\beta + \\beta^\\top X^\\top V^{-1} X\\beta + \\beta^\\top \\Sigma_0^{-1} \\beta \\right] + \\text{const}\n$$\nWe group terms involving $\\beta$:\n$$\n\\ln p(\\beta \\mid y) = -\\frac{1}{2} \\left[ \\beta^\\top (X^\\top V^{-1} X + \\Sigma_0^{-1}) \\beta - 2(X^\\top V^{-1} y)^\\top \\beta \\right] + \\text{const}\n$$\nThis expression is a quadratic function of $\\beta$. We can identify the parameters of the posterior distribution $p(\\beta \\mid y) = \\mathcal{N}(\\beta; \\mu_\\beta, \\Sigma_\\beta)$ by comparing this to the standard log-form of a multivariate Gaussian:\n$$\n\\ln \\mathcal{N}(\\beta; \\mu_\\beta, \\Sigma_\\beta) = -\\frac{1}{2} \\left[ \\beta^\\top \\Sigma_\\beta^{-1} \\beta - 2\\mu_\\beta^\\top \\Sigma_\\beta^{-1} \\beta + \\mu_\\beta^\\top \\Sigma_\\beta^{-1} \\mu_\\beta \\right] + \\text{const}\n$$\nBy matching the quadratic term in $\\beta$, we find the posterior precision (inverse covariance):\n$$\n\\Sigma_\\beta^{-1} = X^\\top V^{-1} X + \\Sigma_0^{-1}\n$$\nThe posterior covariance is therefore:\n$$\n\\Sigma_\\beta = (X^\\top V^{-1} X + \\Sigma_0^{-1})^{-1}\n$$\nLet's denote the subject-wise precisions by $\\Pi_y = V^{-1}$, which is a diagonal matrix with entries $1/\\sigma_i^2$. The expression becomes:\n$$\n\\Sigma_\\beta = (X^\\top \\Pi_y X + \\Sigma_0^{-1})^{-1}\n$$\nBy matching the linear term in $\\beta$, we have:\n$$\n\\mu_\\beta^\\top \\Sigma_\\beta^{-1} = (X^\\top V^{-1} y)^\\top = y^\\top V^{-1} X\n$$\nSolving for the posterior mean $\\mu_\\beta$:\n$$\n\\mu_\\beta = \\Sigma_\\beta (X^\\top V^{-1} y) = \\Sigma_\\beta (X^\\top \\Pi_y y)\n$$\nThese equations provide the full posterior distribution $p(\\beta \\mid y) = \\mathcal{N}(\\mu_\\beta, \\Sigma_\\beta)$.\n\n### 3. Computation of the Posterior Probability\n\nThe problem asks for the posterior probability that the severity effect is greater than zero, i.e., $p(\\beta_1 > 0 \\mid y)$. The posterior for $\\beta=[\\beta_0, \\beta_1]^\\top$ is a bivariate normal distribution. The marginal distribution for any single component of a multivariate normal is also normal.\n\nThe marginal posterior distribution for $\\beta_1$ is given by:\n$$\np(\\beta_1 \\mid y) = \\mathcal{N}(\\beta_1; \\mu_{\\beta_1}, \\sigma_{\\beta_1}^2)\n$$\nwhere the mean $\\mu_{\\beta_1}$ is the second element of the posterior mean vector $\\mu_\\beta$, and the variance $\\sigma_{\\beta_1}^2$ is the second diagonal element of the posterior covariance matrix $\\Sigma_\\beta$.\n$$\n\\mu_{\\beta_1} = (\\mu_\\beta)_1 \\quad (\\text{using 0-based indexing})\n$$\n$$\n\\sigma_{\\beta_1}^2 = (\\Sigma_\\beta)_{1,1} \\quad (\\text{using 0-based indexing})\n$$\nThe posterior standard deviation is $\\sigma_{\\beta_1} = \\sqrt{(\\Sigma_\\beta)_{1,1}}$.\n\nTo compute $p(\\beta_1 > 0 \\mid y)$, we integrate the probability density function of $p(\\beta_1 \\mid y)$ from $0$ to $\\infty$. This is equivalent to finding $1 - \\text{CDF}(0)$, where CDF is the cumulative distribution function of $\\mathcal{N}(\\mu_{\\beta_1}, \\sigma_{\\beta_1}^2)$.\nLet $Z = (\\beta_1 - \\mu_{\\beta_1}) / \\sigma_{\\beta_1}$, which follows a standard normal distribution $\\mathcal{N}(0, 1)$. The condition $\\beta_1 > 0$ is equivalent to $Z > -\\mu_{\\beta_1} / \\sigma_{\\beta_1}$. Thus:\n$$\np(\\beta_1 > 0 \\mid y) = P\\left(Z > -\\frac{\\mu_{\\beta_1}}{\\sigma_{\\beta_1}}\\right) = 1 - \\Phi\\left(-\\frac{\\mu_{\\beta_1}}{\\sigma_{\\beta_1}}\\right)\n$$\nwhere $\\Phi$ is the CDF of the standard normal distribution. Due to the symmetry of the standard normal distribution, $\\Phi(-z) = 1 - \\Phi(z)$, so we have:\n$$\np(\\beta_1 > 0 \\mid y) = 1 - \\left(1 - \\Phi\\left(\\frac{\\mu_{\\beta_1}}{\\sigma_{\\beta_1}}\\right)\\right) = \\Phi\\left(\\frac{\\mu_{\\beta_1}}{\\sigma_{\\beta_1}}\\right)\n$$\nThis quantity can be computed numerically using the standard normal CDF available in scientific computing libraries.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the Parametric Empirical Bayes (PEB) problem\n    for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Test case 1 (typical informative case)\n            's': np.array([0.2, -0.1, 0.3, 0.0, 0.5, -0.4]),\n            'y': np.array([0.19, -0.10, 0.25, -0.01, 0.42, -0.35]),\n            'v': np.array([0.04, 0.05, 0.03, 0.06, 0.02, 0.07]),\n            'prior_vars': np.array([1.0, 1.0])\n        },\n        {\n            # Test case 2 (one subject with large uncertainty)\n            's': np.array([-0.5, -0.2, 0.0, 0.2, 0.6]),\n            'y': np.array([-0.10, -0.04, -0.10, 0.06, 0.21]),\n            'v': np.array([0.02, 0.02, 2.0, 0.02, 0.02]),\n            'prior_vars': np.array([2.0, 2.0])\n        },\n        {\n            # Test case 3 (weak evidence for severity effect)\n            's': np.array([-0.3, -0.1, 0.0, 0.1, 0.3, 0.5]),\n            'y': np.array([-0.02, 0.01, 0.00, -0.01, 0.02, 0.00]),\n            'v': np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05]),\n            'prior_vars': np.array([1.0, 1.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        s = case['s']\n        y = case['y']\n        v = case['v']\n        prior_vars = case['prior_vars']\n\n        # Number of subjects\n        n_subjects = len(s)\n\n        # 1. Construct the model matrices\n        # Design matrix X (N x 2) with intercept and severity regressor\n        X = np.vstack([np.ones(n_subjects), s]).T\n\n        # Precision of first-level estimates (inverse of variance)\n        Pi_y = np.diag(1.0 / v)\n\n        # Prior precision matrix for beta\n        Pi_0 = np.diag(1.0 / prior_vars)\n\n        # 2. Compute posterior parameters for beta = [beta_0, beta_1]\n        # Posterior precision: Pi_beta = X' * Pi_y * X + Pi_0\n        Pi_beta = X.T @ Pi_y @ X + Pi_0\n\n        # Posterior covariance: Sigma_beta = inv(Pi_beta)\n        Sigma_beta = np.linalg.inv(Pi_beta)\n\n        # Posterior mean: mu_beta = Sigma_beta * X' * Pi_y * y\n        mu_beta = Sigma_beta @ X.T @ Pi_y @ y\n\n        # 3. Extract marginal posterior for the severity effect beta_1\n        # The mean of beta_1 is the second element of mu_beta\n        mu_beta1 = mu_beta[1]\n        \n        # The variance of beta_1 is the second diagonal element of Sigma_beta\n        var_beta1 = Sigma_beta[1, 1]\n        \n        # The standard deviation of beta_1\n        std_beta1 = np.sqrt(var_beta1)\n\n        # 4. Compute the posterior probability P(beta_1 > 0 | data)\n        # This is the CDF of the standard normal distribution evaluated at mu/std\n        # Z = (beta_1 - mu_beta1) / std_beta1 ~ N(0, 1)\n        # P(beta_1 > 0) = P(Z > -mu_beta1 / std_beta1) = 1 - Phi(-mu_beta1 / std_beta1)\n        # By symmetry, this is Phi(mu_beta1 / std_beta1).\n        prob_beta1_positive = norm.cdf(mu_beta1 / std_beta1)\n        \n        results.append(prob_beta1_positive)\n\n    # Format the output as a string and print\n    # Using f-string formatting to ensure decimal representation\n    formatted_results = [f\"{r:.10f}\".rstrip('0').rstrip('.') for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}