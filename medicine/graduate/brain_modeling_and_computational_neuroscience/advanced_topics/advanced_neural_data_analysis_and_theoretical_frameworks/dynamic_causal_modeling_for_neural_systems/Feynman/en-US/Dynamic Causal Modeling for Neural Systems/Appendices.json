{
    "hands_on_practices": [
        {
            "introduction": "The bilinear state equation is the engine of Dynamic Causal Modeling, describing how neural activity evolves and is influenced by external inputs. This first exercise provides a direct, hands-on calculation to clarify the role of the modulatory ($B$) parameters. By computing the instantaneous change in the brain's effective connectivity matrix in response to an experimental event, you will build a foundational intuition for how DCM represents task-induced changes in neural communication .",
            "id": "3976259",
            "problem": "Consider a three-region neuronal system modeled using Dynamic Causal Modeling (DCM) in its bilinear form. Dynamic Causal Modeling (DCM) posits that the deterministic neuronal dynamics are governed by a bilinear approximation in the neuronal states and experimental inputs, with an effective connectivity that depends on experimental conditions. Let the regions be indexed by $i \\in \\{1,2,3\\}$, and let the baseline effective connectivity be given by the $3 \\times 3$ matrix $A$, the condition-specific modulatory coupling for input $u_{1}(t)$ be given by the $3 \\times 3$ matrix $B^{(1)}$, and the exogenous input (driving) mapping be given by the $3 \\times 2$ matrix $C$. The matrices are:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n-0.5 & 0.2 & 0 \\\\\n0.1 & -0.4 & 0.3 \\\\\n0 & 0.05 & -0.3\n\\end{pmatrix},\n\\qquad\nB^{(1)} \\;=\\;\n\\begin{pmatrix}\n0 & 0.15 & 0 \\\\\n0 & 0 & 0.07 \\\\\n0.02 & 0 & 0\n\\end{pmatrix},\n\\qquad\nC \\;=\\;\n\\begin{pmatrix}\n0.25 & 0 \\\\\n0 & 0 \\\\\n0 & 0.2\n\\end{pmatrix}.\n$$\nAssume that at time $t_{0}$, the first input $u_{1}(t)$ switches instantaneously from $u_{1}(t_{0}^{-})=0$ to $u_{1}(t_{0}^{+})=1$, while the second input remains inactive, $u_{2}(t)=0$ for all $t$. Using the bilinear DCM framework and starting from first principles appropriate to DCM, determine the instantaneous modulation of the effective connectivity matrix at $t_{0}$ induced by this change in $u_{1}(t)$. That is, compute the matrix difference between the effective connectivity just after the switch and just before the switch. Give your final matrix entries exactly (no rounding). Report the matrix entries in units of $\\mathrm{s}^{-1}$, but do not include units in your final boxed answer.",
            "solution": "The problem asks for the instantaneous modulation of the effective connectivity matrix at time $t_0$ for a three-region system described by the bilinear form of Dynamic Causal Modeling (DCM). This modulation is defined as the difference between the effective connectivity just after an input switch ($t_0^+$) and just before the switch ($t_0^-$).\n\nFirst, we must establish the governing equation of the system from first principles appropriate to DCM. The state equation for the change in neuronal activity, $\\dot{x}(t) = \\frac{dx(t)}{dt}$, in a bilinear DCM with $m$ inputs is given by:\n$$\n\\dot{x}(t) = Ax(t) + \\sum_{j=1}^{m} u_j(t) B^{(j)} x(t) + Cu(t)\n$$\nwhere $x(t)$ is the vector of neuronal states, $u(t)$ is the vector of experimental inputs, $A$ is the matrix of intrinsic (baseline) effective connectivity, $B^{(j)}$ is the matrix representing the modulation of connectivity by the $j$-th input $u_j(t)$, and $C$ is the matrix that couples the inputs directly to the neuronal states (driving inputs).\n\nTo identify the effective connectivity matrix, we can factor the state vector $x(t)$ from the terms that depend on it:\n$$\n\\dot{x}(t) = \\left( A + \\sum_{j=1}^{m} u_j(t) B^{(j)} \\right) x(t) + Cu(t)\n$$\nThe term multiplying the state vector $x(t)$ represents the total effective connectivity of the system at time $t$. We denote this time-dependent effective connectivity matrix as $A_{\\text{eff}}(t)$:\n$$\nA_{\\text{eff}}(t) = A + \\sum_{j=1}^{m} u_j(t) B^{(j)}\n$$\nIn this specific problem, we have two inputs, $u_1(t)$ and $u_2(t)$, so $m=2$. The expression for the effective connectivity matrix becomes:\n$$\nA_{\\text{eff}}(t) = A + u_1(t) B^{(1)} + u_2(t) B^{(2)}\n$$\nThe problem statement provides the matrices $A$ and $B^{(1)}$, and specifies the behavior of the inputs. We are given that the second input is always inactive, i.e., $u_2(t) = 0$ for all $t$. Therefore, the term $u_2(t) B^{(2)}$ is always a zero matrix, and the effective connectivity simplifies to:\n$$\nA_{\\text{eff}}(t) = A + u_1(t) B^{(1)}\n$$\nThe problem asks for the instantaneous modulation at $t_0$, which is the change in $A_{\\text{eff}}$ as the input $u_1(t)$ switches. We need to calculate $A_{\\text{eff}}(t)$ just before the switch ($t = t_0^-$) and just after the switch ($t = t_0^+$).\n\nAt time $t_0^-$, just before the switch, the input is given as $u_1(t_0^-) = 0$. The effective connectivity matrix is:\n$$\nA_{\\text{eff}}(t_0^-) = A + (0) \\cdot B^{(1)} = A\n$$\nSo, just before the switch, the effective connectivity is simply the baseline connectivity matrix $A$.\n\nAt time $t_0^+$, just after the switch, the input is given as $u_1(t_0^+) = 1$. The effective connectivity matrix becomes:\n$$\nA_{\\text{eff}}(t_0^+) = A + (1) \\cdot B^{(1)} = A + B^{(1)}\n$$\nThe instantaneous modulation of the effective connectivity, which we denote as $\\Delta A_{\\text{eff}}(t_0)$, is the difference between these two matrices:\n$$\n\\Delta A_{\\text{eff}}(t_0) = A_{\\text{eff}}(t_0^+) - A_{\\text{eff}}(t_0^-)\n$$\nSubstituting the expressions we found:\n$$\n\\Delta A_{\\text{eff}}(t_0) = (A + B^{(1)}) - A = B^{(1)}\n$$\nThus, the instantaneous change in the effective connectivity induced by the switch in input $u_1(t)$ is precisely the modulatory coupling matrix $B^{(1)}$. The matrices $A$ and $C$, and the input $u_2(t)$, are not needed for this final calculation, as they do not contribute to the *change* in connectivity at $t_0$.\n\nThe value of this change is given by the matrix $B^{(1)}$ provided in the problem statement:\n$$\n\\Delta A_{\\text{eff}}(t_0) = B^{(1)} = \\begin{pmatrix} 0 & 0.15 & 0 \\\\ 0 & 0 & 0.07 \\\\ 0.02 & 0 & 0 \\end{pmatrix}\n$$\nThe entries of this matrix represent the change in effective connectivity in units of $\\mathrm{s}^{-1}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 0.15 & 0 \\\\\n0 & 0 & 0.07 \\\\\n0.02 & 0 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Understanding the DCM equations is one thing; seeing them in action is another. By translating the bilinear differential equation into a computer program, you will gain a concrete understanding of how inputs drive and modulate neural dynamics over time, generating the very data that DCM is designed to explain .",
            "id": "3976292",
            "problem": "Construct a minimal dynamic causal model for two interacting neural regions with a single exogenous input targeted at the first region, using a bilinear state equation and a linear observation model. The continuous-time latent neural state is a vector $x(t) \\in \\mathbb{R}^2$ with dynamics governed by the bilinear ordinary differential equation $\\dot{x}(t) = A x(t) + B u(t) x(t) + C u(t)$, and an observation model $y(t) = h(x(t)) + \\epsilon(t)$, where $y(t) \\in \\mathbb{R}^2$, $u(t) \\in \\mathbb{R}$ is a scalar exogenous input, $A \\in \\mathbb{R}^{2 \\times 2}$ is the intrinsic effective connectivity, $B \\in \\mathbb{R}^{2 \\times 2}$ is the input-dependent modulation of connectivity, $C \\in \\mathbb{R}^{2}$ is the direct input gain vector, and $h(x)$ is a linear mapping $h(x) = L x$ with $L \\in \\mathbb{R}^{2 \\times 2}$. Assume measurement noise $\\epsilon(t)$ is identically zero for simulation, i.e., $\\epsilon(t) = 0$.\n\nUse the following fixed parameters for all simulations:\n- Intrinsic connectivity $A = \\begin{bmatrix} -1.0 & 0.3 \\\\ 0.2 & -0.8 \\end{bmatrix}$.\n- Modulatory connectivity $B = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.5 & 0.0 \\end{bmatrix}$.\n- Direct input gain $C = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$.\n- Observation matrix $L = I_2 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- Initial state $x(0) = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n\nThe input $u(t)$ is a rectangular pulse defined by three parameters: amplitude $a$, onset time $t_{\\mathrm{on}}$, and offset time $t_{\\mathrm{off}}$. Specifically,\n$$\nu(t) = \\begin{cases}\na, & t_{\\mathrm{on}} \\le t < t_{\\mathrm{off}},\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n\nStarting from the definition of the time derivative and the forward Euler method for numerical integration of ordinary differential equations, derive a discrete-time update that approximates the solution of the bilinear system over a time grid with step size $\\Delta t$, and specify how to compute $y(t)$ at the final time $T$ given the above assumptions. Then implement a program that simulates the system for the following test suite of input parameters, each specified as a tuple $(a, t_{\\mathrm{on}}, t_{\\mathrm{off}}, T, \\Delta t)$:\n- Case $1$: $(1.0, 0.5, 1.5, 3.0, 0.001)$.\n- Case $2$: $(0.0, 0.5, 1.5, 3.0, 0.001)$.\n- Case $3$: $(2.0, 0.0, 0.01, 3.0, 0.001)$.\n- Case $4$: $(1.0, 0.5, 1.5, 3.0, 0.01)$.\n\nFor each case, simulate the trajectory from $t=0$ to $t=T$ using the Euler scheme you derive, and return the final observed state vector $y(T) \\in \\mathbb{R}^2$. Express each component of $y(T)$ as a floating-point number rounded to six decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the two-component list $[y_1(T),y_2(T)]$ for one case, with no spaces anywhere. For example, the overall output format must be exactly like $[[y_{11},y_{12}],[y_{21},y_{22}],[y_{31},y_{32}],[y_{41},y_{42}]]$.",
            "solution": "The user wants to construct a minimal dynamic causal model and simulate it.\n\n### Step 1: Extract Givens\n- **State Equation**: $\\dot{x}(t) = A x(t) + B u(t) x(t) + C u(t)$, where $x(t) \\in \\mathbb{R}^2$ is the latent neural state, and $u(t) \\in \\mathbb{R}$ is a scalar input.\n- **Observation Equation**: $y(t) = L x(t) + \\epsilon(t)$, where $y(t) \\in \\mathbb{R}^2$ is the observation, with the assumption that measurement noise $\\epsilon(t) = 0$.\n- **Intrinsic Connectivity**: $A = \\begin{bmatrix} -1.0 & 0.3 \\\\ 0.2 & -0.8 \\end{bmatrix}$.\n- **Modulatory Connectivity**: $B = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.5 & 0.0 \\end{bmatrix}$.\n- **Direct Input Gain**: $C = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$.\n- **Observation Matrix**: $L = I_2 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- **Initial State**: $x(0) = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n- **Input Function**: A rectangular pulse $u(t) = a$ for $t_{\\mathrm{on}} \\le t < t_{\\mathrm{off}}$ and $u(t)=0$ otherwise.\n- **Numerical Method**: Forward Euler method with a time step $\\Delta t$.\n- **Task**: For a given set of parameters $(a, t_{\\mathrm{on}}, t_{\\mathrm{off}}, T, \\Delta t)$, simulate the system from $t=0$ to $t=T$ and report the final observed state $y(T)$.\n- **Test Cases**:\n    - Case 1: $(1.0, 0.5, 1.5, 3.0, 0.001)$\n    - Case 2: $(0.0, 0.5, 1.5, 3.0, 0.001)$\n    - Case 3: $(2.0, 0.0, 0.01, 3.0, 0.001)$\n    - Case 4: $(1.0, 0.5, 1.5, 3.0, 0.01)$\n- **Output**: The components of the final state vector $y(T)$ for each case, rounded to six decimal places, formatted as a single-line string.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, describing a standard bilinear state-space model used in dynamic causal modeling (DCM), a well-established field in computational neuroscience. The system of ordinary differential equations (ODEs) is well-defined with a given initial condition, ensuring a unique solution exists. All parameters, variables, and constants are specified with mathematical precision and without ambiguity. The problem is self-contained, consistent, and computationally feasible. The numerical method requested (forward Euler) is a standard technique for solving ODEs. The task is a straightforward application of numerical methods to a defined physical model, making it a valid scientific problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Design\nThe problem requires the numerical integration of a system of first-order, non-linear ordinary differential equations (ODEs) that define the dynamics of the neural states. The specified model is a bilinear system, a class of non-linear systems where the non-linearity arises from the product of the state and the input.\n\nThe continuous-time dynamics of the latent neural state $x(t)$ are given by:\n$$\n\\dot{x}(t) = \\frac{dx(t)}{dt} = A x(t) + u(t) B x(t) + C u(t)\n$$\nHere, the term $u(t) B x(t)$ represents the modulatory effect of the input $u(t)$ on the intrinsic connectivity between the neural regions. The term $C u(t)$ represents the direct driving effect of the input on the regions.\n\nTo simulate the trajectory of $x(t)$ over a finite time interval $[0, T]$, we must discretize this continuous-time equation. The problem specifies the use of the forward Euler method, a first-order numerical procedure for solving ODEs with a given initial value.\n\nThe derivation of the forward Euler method begins with the definition of the time derivative:\n$$\n\\frac{dx(t)}{dt} = \\lim_{\\Delta t \\to 0} \\frac{x(t + \\Delta t) - x(t)}{\\Delta t}\n$$\nFor a small, finite time step $\\Delta t > 0$, we can approximate the derivative as:\n$$\n\\frac{dx(t)}{dt} \\approx \\frac{x(t + \\Delta t) - x(t)}{\\Delta t}\n$$\nSubstituting the system's differential equation into this approximation gives:\n$$\n\\frac{x(t + \\Delta t) - x(t)}{\\Delta t} \\approx A x(t) + u(t) B x(t) + C u(t)\n$$\nRearranging this equation to solve for the state at the next time step, $x(t + \\Delta t)$, yields the forward Euler update rule:\n$$\nx(t + \\Delta t) \\approx x(t) + \\Delta t \\left( A x(t) + u(t) B x(t) + C u(t) \\right)\n$$\nTo implement this numerically, we define a discrete time grid $t_k = k \\Delta t$ for $k = 0, 1, 2, \\dots, N$, where $N = T / \\Delta t$. Let $x_k = x(t_k)$ be the state at time $t_k$, and $u_k = u(t_k)$ be the input at that time. The discrete-time update equation is:\n$$\nx_{k+1} = x_k + \\Delta t \\left( A x_k + u_k B x_k + u_k C \\right)\n$$\nThe simulation algorithm proceeds as follows:\n1.  Initialize the state vector at time $t_0 = 0$ with the given initial condition: $x_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n2.  Determine the total number of integration steps, $N = \\text{integer}(T / \\Delta t)$.\n3.  Iterate for $k$ from $0$ to $N-1$:\n    a. Calculate the current time $t_k = k \\Delta t$.\n    b. Evaluate the input function $u_k = u(t_k)$ based on its definition: $u_k = a$ if $t_{\\mathrm{on}} \\le t_k < t_{\\mathrm{off}}$, and $u_k = 0$ otherwise.\n    c. Compute the state derivative approximation (the right-hand side of the ODE) at time $t_k$: $\\dot{x}_k = A x_k + u_k B x_k + u_k C$.\n    d. Apply the Euler update rule to find the state at the next time step: $x_{k+1} = x_k + \\Delta t \\cdot \\dot{x}_k$.\n4.  After the loop completes, the final state is $x_N$, which approximates $x(T)$.\n\nThe final step is to compute the observed output at time $T$. The observation model is given by $y(t) = L x(t) + \\epsilon(t)$. With the provided parameters $\\epsilon(t) = 0$ and $L = I_2$ (the $2 \\times 2$ identity matrix), the observation equation simplifies to:\n$$\ny(T) = I_2 x(T) = x(T)\n$$\nTherefore, the final observed state $y(T)$ is identical to the final latent state $x(T)$ obtained from the numerical integration. The program will execute this simulation for each test case, round the two components of the final vector $y(T)$ to six decimal places, and format the results as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the dynamic causal modeling problem by simulating a bilinear system\n    using the forward Euler method for a given set of test cases.\n    \"\"\"\n\n    # Define the fixed model parameters as specified in the problem.\n    # Intrinsic connectivity matrix A\n    A = np.array([[-1.0, 0.3], [0.2, -0.8]])\n    # Modulatory connectivity matrix B\n    B = np.array([[0.0, 0.0], [0.5, 0.0]])\n    # Direct input gain vector C\n    C = np.array([1.0, 0.0])\n    # Initial state vector x(0)\n    x0 = np.array([0.0, 0.0])\n\n    # The observation matrix L is the identity matrix, and noise is zero,\n    # so y(t) = x(t). We don't need L in the code.\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (a, t_on, t_off, T, dt)\n    test_cases = [\n        (1.0, 0.5, 1.5, 3.0, 0.001),  # Case 1\n        (0.0, 0.5, 1.5, 3.0, 0.001),  # Case 2\n        (2.0, 0.0, 0.01, 3.0, 0.001),  # Case 3\n        (1.0, 0.5, 1.5, 3.0, 0.01),   # Case 4\n    ]\n\n    results = []\n    # Process each test case.\n    for case in test_cases:\n        a, t_on, t_off, T, dt = case\n\n        # Calculate the number of integration steps.\n        num_steps = int(round(T / dt))\n        \n        # Initialize the state vector.\n        x = x0.copy()\n\n        # Perform the numerical integration using the forward Euler method.\n        for k in range(num_steps):\n            current_t = k * dt\n            \n            # Determine the value of the input u(t) at the current time step.\n            u_val = 0.0\n            if t_on <= current_t < t_off:\n                u_val = a\n            \n            # Calculate the derivative dx/dt using the bilinear state equation:\n            # dx/dt = A*x + u*(B*x) + u*C\n            dxdt = A @ x + u_val * (B @ x) + u_val * C\n            \n            # Update the state vector using the forward Euler step.\n            x = x + dt * dxdt\n\n        # The final observed state y(T) is equal to the final latent state x(T).\n        y_T = x\n        \n        # Round the components of the final state vector to six decimal places.\n        y_T_rounded = [round(y_T[0], 6), round(y_T[1], 6)]\n        results.append(y_T_rounded)\n\n    # Format the final list of results into the exact required single-line string,\n    # e.g., [[y11,y12],[y21,y22],...], with no spaces.\n    final_output_str = str(results).replace(' ', '')\n    \n    # Final print statement in the exact required format.\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate power of DCM lies in its ability to test neuroscientific hypotheses by inferring model parameters from observed data. This final exercise guides you through a crucial step in this process: performing group-level inference using Parametric Empirical Bayes (PEB). You will learn how to combine parameter estimates from multiple subjects to determine if a brain connectivity parameter is related to a group-level variable, such as symptom severity, moving from individual models to powerful group-level scientific conclusions .",
            "id": "3976280",
            "problem": "You are given subject-specific posterior summaries from Dynamic Causal Modeling (DCM) and a single group-level regressor representing symptom severity. The goal is to perform Parametric Empirical Bayes (PEB) at the second level to estimate the group effect and to compute its posterior probability that the severity effect is greater than zero. Use the following hierarchical setup grounded in Gaussian linear models and Bayes' theorem. The model assumptions are:\n\n- Dynamic Causal Modeling (DCM) provides, for each subject $i$, a posterior for a single scalar parameter (e.g., an effective connectivity) that is well-approximated by a Gaussian with mean $\\mu_i$ and variance $\\sigma_i^2$.\n- The second-level PEB model is a linear regression linking $\\mu_i$ to a group regressor with an intercept: $\\mu_i = x_{i0}\\,\\beta_0 + x_{i1}\\,\\beta_1 + \\epsilon_i$, where $x_{i0} = 1$ for all $i$, $x_{i1}$ is the symptom severity for subject $i$, and $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_i^2)$ reflects estimation uncertainty propagated from the first level.\n- The prior over the group parameters is Gaussian: $\\beta \\sim \\mathcal{N}(0,\\Sigma_0)$, where $\\beta = [\\beta_0,\\beta_1]^\\top$ and $\\Sigma_0$ is a known positive-definite covariance matrix.\n\nStarting from Bayes' theorem and the properties of the multivariate normal distribution, derive the posterior distribution $p(\\beta \\mid \\mu)$ under the above assumptions. Express the posterior mean and covariance in terms of the design matrix $X$, the subject-wise precisions (inverse variances), and the prior covariance $\\Sigma_0$. Then, for the severity effect $\\beta_1$, compute the posterior probability $p(\\beta_1 > 0 \\mid \\text{data})$.\n\nImplement a program that, for each test case below, constructs the hierarchical model, computes the posterior for $\\beta$, marginalizes to the component $\\beta_1$, and outputs the posterior probability $p(\\beta_1 > 0 \\mid \\text{data})$ as a decimal. No physical units are involved; probabilities must be expressed as decimals.\n\nUse the following test suite. In each case, the regressor vector of severities is $s$, the observed subject means are $y$ (representing $\\mu_i$), the observed subject variances are $v$ (representing $\\sigma_i^2$), and the prior covariance for $\\beta$ is diagonal with entries specified for the intercept and slope as $(\\sigma_{\\beta_0}^2, \\sigma_{\\beta_1}^2)$:\n\n- Test case $1$ (typical informative case):\n  - $s = [0.2,-0.1,0.3,0.0,0.5,-0.4]$\n  - $y = [0.19,-0.10,0.25,-0.01,0.42,-0.35]$\n  - $v = [0.04,0.05,0.03,0.06,0.02,0.07]$\n  - Prior diagonal variances $(\\sigma_{\\beta_0}^2, \\sigma_{\\beta_1}^2) = (1.0, 1.0)$\n- Test case $2$ (one subject with large uncertainty, near-boundary informativeness):\n  - $s = [-0.5,-0.2,0.0,0.2,0.6]$\n  - $y = [-0.10,-0.04,-0.10,0.06,0.21]$\n  - $v = [0.02,0.02,2.0,0.02,0.02]$\n  - Prior diagonal variances $(\\sigma_{\\beta_0}^2, \\sigma_{\\beta_1}^2) = (2.0, 2.0)$\n- Test case $3$ (weak evidence for severity effect, near-zero slope):\n  - $s = [-0.3,-0.1,0.0,0.1,0.3,0.5]$\n  - $y = [-0.02,0.01,0.00,-0.01,0.02,0.00]$\n  - $v = [0.05,0.05,0.05,0.05,0.05,0.05]$\n  - Prior diagonal variances $(\\sigma_{\\beta_0}^2, \\sigma_{\\beta_1}^2) = (1.0, 1.0)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$), where $r_k$ is the posterior probability $p(\\beta_1>0 \\mid \\text{data})$ for test case $k$. The outputs must be decimals.",
            "solution": "The problem requires the derivation and computation of posterior quantities within a hierarchical Bayesian model, specifically a second-level Parametric Empirical Bayes (PEB) analysis of Dynamic Causal Modeling (DCM) posteriors. We will first derive the posterior distribution for the group-level parameters and then detail the method for computing the posterior probability of the effect of interest.\n\n### 1. Hierarchical Model Formulation\n\nThe problem describes a two-level hierarchical model. At the first level, a DCM analysis for each of $N$ subjects has yielded posterior distributions for a scalar parameter. These posteriors are approximated by Gaussian distributions. For subject $i$, the posterior parameter estimate is a random variable with mean $\\mu_i$ and variance $\\sigma_i^2$. These posterior means become the data for the second-level analysis.\n\nThe second-level model is a general linear model that seeks to explain the subject-to-subject variability in the parameter means $\\mu_i$ using a group-level regressor. The model is given as:\n$$\n\\mu_i = x_{i0}\\,\\beta_0 + x_{i1}\\,\\beta_1 + \\epsilon_i\n$$\nwhere $\\beta_0$ is the group-level intercept, $\\beta_1$ is the effect of symptom severity, $x_{i0}=1$ for all subjects, and $x_{i1}$ is the symptom severity for subject $i$. The term $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$ represents the uncertainty (estimation error) propagated from the first-level DCM, with $\\sigma_i^2$ being the posterior variance from subject $i$'s analysis.\n\nWe can express this model for all $N$ subjects in matrix form. Let $y$ be the $N \\times 1$ column vector of subject-specific means $[\\mu_1, \\mu_2, \\dots, \\mu_N]^\\top$. Let $X$ be the $N \\times 2$ design matrix, where the first column is a vector of ones and the second column is the vector of symptom severities $[x_{11}, x_{21}, \\dots, x_{N1}]^\\top$. Let $\\beta$ be the $2 \\times 1$ vector of group-level parameters $[\\beta_0, \\beta_1]^\\top$. The model is then:\n$$\ny = X\\beta + \\epsilon\n$$\nwhere $\\epsilon$ is the vector of noise terms $[\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_N]^\\top$. Since the $\\epsilon_i$ are independent, their covariance matrix, denoted $V$, is a diagonal matrix with the first-level variances $\\sigma_i^2$ on the diagonal: $V = \\text{diag}(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_N^2)$. The distribution of the data $y$ given the parameters $\\beta$ (the likelihood) is therefore a multivariate normal distribution:\n$$\np(y \\mid \\beta) = \\mathcal{N}(y; X\\beta, V)\n$$\n\nThe problem specifies a Gaussian prior on the group parameters $\\beta$:\n$$\np(\\beta) = \\mathcal{N}(\\beta; \\beta_p, \\Sigma_p)\n$$\nwhere the prior mean is the zero vector, $\\beta_p = [0, 0]^\\top$, and the prior covariance $\\Sigma_0$ is a known positive-definite matrix.\n\n### 2. Derivation of the Posterior Distribution\n\nOur goal is to find the posterior distribution of $\\beta$ given the data $y$, denoted $p(\\beta \\mid y)$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(\\beta \\mid y) \\propto p(y \\mid \\beta) p(\\beta)\n$$\nSince both the likelihood and the prior are Gaussian, their product will also be proportional to a Gaussian distribution. We can find the parameters of this posterior Gaussian by examining the log-posterior, focusing on terms that depend on $\\beta$.\n\nThe log-posterior is:\n$$\n\\ln p(\\beta \\mid y) = \\ln p(y \\mid \\beta) + \\ln p(\\beta) + \\text{const}\n$$\nThe log-likelihood is:\n$$\n\\ln p(y \\mid \\beta) = -\\frac{1}{2}(y - X\\beta)^\\top V^{-1} (y - X\\beta) + \\text{const}\n$$\nThe log-prior is:\n$$\n\\ln p(\\beta) = -\\frac{1}{2}(\\beta - \\beta_p)^\\top \\Sigma_0^{-1} (\\beta - \\beta_p) + \\text{const}\n$$\nSubstituting $\\beta_p=0$ and combining gives:\n$$\n\\ln p(\\beta \\mid y) = -\\frac{1}{2} \\left[ (y - X\\beta)^\\top V^{-1} (y - X\\beta) + \\beta^\\top \\Sigma_0^{-1} \\beta \\right] + \\text{const}\n$$\nExpanding the quadratic form:\n$$\n\\ln p(\\beta \\mid y) = -\\frac{1}{2} \\left[ y^\\top V^{-1} y - 2y^\\top V^{-1} X\\beta + \\beta^\\top X^\\top V^{-1} X\\beta + \\beta^\\top \\Sigma_0^{-1} \\beta \\right] + \\text{const}\n$$\nWe group terms involving $\\beta$:\n$$\n\\ln p(\\beta \\mid y) = -\\frac{1}{2} \\left[ \\beta^\\top (X^\\top V^{-1} X + \\Sigma_0^{-1}) \\beta - 2(X^\\top V^{-1} y)^\\top \\beta \\right] + \\text{const}\n$$\nThis expression is a quadratic function of $\\beta$. We can identify the parameters of the posterior distribution $p(\\beta \\mid y) = \\mathcal{N}(\\beta; \\mu_\\beta, \\Sigma_\\beta)$ by comparing this to the standard log-form of a multivariate Gaussian:\n$$\n\\ln \\mathcal{N}(\\beta; \\mu_\\beta, \\Sigma_\\beta) = -\\frac{1}{2} \\left[ \\beta^\\top \\Sigma_\\beta^{-1} \\beta - 2\\mu_\\beta^\\top \\Sigma_\\beta^{-1} \\beta + \\mu_\\beta^\\top \\Sigma_\\beta^{-1} \\mu_\\beta \\right] + \\text{const}\n$$\nBy matching the quadratic term in $\\beta$, we find the posterior precision (inverse covariance):\n$$\n\\Sigma_\\beta^{-1} = X^\\top V^{-1} X + \\Sigma_0^{-1}\n$$\nThe posterior covariance is therefore:\n$$\n\\Sigma_\\beta = (X^\\top V^{-1} X + \\Sigma_0^{-1})^{-1}\n$$\nLet's denote the subject-wise precisions by $\\Pi_y = V^{-1}$, which is a diagonal matrix with entries $1/\\sigma_i^2$. The expression becomes:\n$$\n\\Sigma_\\beta = (X^\\top \\Pi_y X + \\Sigma_0^{-1})^{-1}\n$$\nBy matching the linear term in $\\beta$, we have:\n$$\n\\mu_\\beta^\\top \\Sigma_\\beta^{-1} = (X^\\top V^{-1} y)^\\top = y^\\top V^{-1} X\n$$\nSolving for the posterior mean $\\mu_\\beta$:\n$$\n\\mu_\\beta = \\Sigma_\\beta (X^\\top V^{-1} y) = \\Sigma_\\beta (X^\\top \\Pi_y y)\n$$\nThese equations provide the full posterior distribution $p(\\beta \\mid y) = \\mathcal{N}(\\mu_\\beta, \\Sigma_\\beta)$.\n\n### 3. Computation of the Posterior Probability\n\nThe problem asks for the posterior probability that the severity effect is greater than zero, i.e., $p(\\beta_1 > 0 \\mid y)$. The posterior for $\\beta=[\\beta_0, \\beta_1]^\\top$ is a bivariate normal distribution. The marginal distribution for any single component of a multivariate normal is also normal.\n\nThe marginal posterior distribution for $\\beta_1$ is given by:\n$$\np(\\beta_1 \\mid y) = \\mathcal{N}(\\beta_1; \\mu_{\\beta_1}, \\sigma_{\\beta_1}^2)\n$$\nwhere the mean $\\mu_{\\beta_1}$ is the second element of the posterior mean vector $\\mu_\\beta$, and the variance $\\sigma_{\\beta_1}^2$ is the second diagonal element of the posterior covariance matrix $\\Sigma_\\beta$.\n$$\n\\mu_{\\beta_1} = (\\mu_\\beta)_1 \\quad (\\text{using 0-based indexing})\n$$\n$$\n\\sigma_{\\beta_1}^2 = (\\Sigma_\\beta)_{1,1} \\quad (\\text{using 0-based indexing})\n$$\nThe posterior standard deviation is $\\sigma_{\\beta_1} = \\sqrt{(\\Sigma_\\beta)_{1,1}}$.\n\nTo compute $p(\\beta_1 > 0 \\mid y)$, we integrate the probability density function of $p(\\beta_1 \\mid y)$ from $0$ to $\\infty$. This is equivalent to finding $1 - \\text{CDF}(0)$, where CDF is the cumulative distribution function of $\\mathcal{N}(\\mu_{\\beta_1}, \\sigma_{\\beta_1}^2)$.\nLet $Z = (\\beta_1 - \\mu_{\\beta_1}) / \\sigma_{\\beta_1}$, which follows a standard normal distribution $\\mathcal{N}(0, 1)$. The condition $\\beta_1 > 0$ is equivalent to $Z > -\\mu_{\\beta_1} / \\sigma_{\\beta_1}$. Thus:\n$$\np(\\beta_1 > 0 \\mid y) = P\\left(Z > -\\frac{\\mu_{\\beta_1}}{\\sigma_{\\beta_1}}\\right) = 1 - \\Phi\\left(-\\frac{\\mu_{\\beta_1}}{\\sigma_{\\beta_1}}\\right)\n$$\nwhere $\\Phi$ is the CDF of the standard normal distribution. Due to the symmetry of the standard normal distribution, $\\Phi(-z) = 1 - \\Phi(z)$, so we have:\n$$\np(\\beta_1 > 0 \\mid y) = 1 - \\left(1 - \\Phi\\left(\\frac{\\mu_{\\beta_1}}{\\sigma_{\\beta_1}}\\right)\\right) = \\Phi\\left(\\frac{\\mu_{\\beta_1}}{\\sigma_{\\beta_1}}\\right)\n$$\nThis quantity can be computed numerically using the standard normal CDF available in scientific computing libraries.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the Parametric Empirical Bayes (PEB) problem\n    for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Test case 1 (typical informative case)\n            's': np.array([0.2, -0.1, 0.3, 0.0, 0.5, -0.4]),\n            'y': np.array([0.19, -0.10, 0.25, -0.01, 0.42, -0.35]),\n            'v': np.array([0.04, 0.05, 0.03, 0.06, 0.02, 0.07]),\n            'prior_vars': np.array([1.0, 1.0])\n        },\n        {\n            # Test case 2 (one subject with large uncertainty)\n            's': np.array([-0.5, -0.2, 0.0, 0.2, 0.6]),\n            'y': np.array([-0.10, -0.04, -0.10, 0.06, 0.21]),\n            'v': np.array([0.02, 0.02, 2.0, 0.02, 0.02]),\n            'prior_vars': np.array([2.0, 2.0])\n        },\n        {\n            # Test case 3 (weak evidence for severity effect)\n            's': np.array([-0.3, -0.1, 0.0, 0.1, 0.3, 0.5]),\n            'y': np.array([-0.02, 0.01, 0.00, -0.01, 0.02, 0.00]),\n            'v': np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05]),\n            'prior_vars': np.array([1.0, 1.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        s = case['s']\n        y = case['y']\n        v = case['v']\n        prior_vars = case['prior_vars']\n\n        # Number of subjects\n        n_subjects = len(s)\n\n        # 1. Construct the model matrices\n        # Design matrix X (N x 2) with intercept and severity regressor\n        X = np.vstack([np.ones(n_subjects), s]).T\n\n        # Precision of first-level estimates (inverse of variance)\n        Pi_y = np.diag(1.0 / v)\n\n        # Prior precision matrix for beta\n        Pi_0 = np.diag(1.0 / prior_vars)\n\n        # 2. Compute posterior parameters for beta = [beta_0, beta_1]\n        # Posterior precision: Pi_beta = X' * Pi_y * X + Pi_0\n        Pi_beta = X.T @ Pi_y @ X + Pi_0\n\n        # Posterior covariance: Sigma_beta = inv(Pi_beta)\n        Sigma_beta = np.linalg.inv(Pi_beta)\n\n        # Posterior mean: mu_beta = Sigma_beta * X' * Pi_y * y\n        mu_beta = Sigma_beta @ X.T @ Pi_y @ y\n\n        # 3. Extract marginal posterior for the severity effect beta_1\n        # The mean of beta_1 is the second element of mu_beta\n        mu_beta1 = mu_beta[1]\n        \n        # The variance of beta_1 is the second diagonal element of Sigma_beta\n        var_beta1 = Sigma_beta[1, 1]\n        \n        # The standard deviation of beta_1\n        std_beta1 = np.sqrt(var_beta1)\n\n        # 4. Compute the posterior probability P(beta_1 > 0 | data)\n        # This is the CDF of the standard normal distribution evaluated at mu/std\n        # Z = (beta_1 - mu_beta1) / std_beta1 ~ N(0, 1)\n        # P(beta_1 > 0) = P(Z > -mu_beta1 / std_beta1) = 1 - Phi(-mu_beta1 / std_beta1)\n        # By symmetry, this is Phi(mu_beta1 / std_beta1).\n        prob_beta1_positive = norm.cdf(mu_beta1 / std_beta1)\n        \n        results.append(prob_beta1_positive)\n\n    # Format the output as a string and print\n    # Using f-string formatting to ensure decimal representation\n    formatted_results = [f\"{r:.10f}\".rstrip('0').rstrip('.') for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}