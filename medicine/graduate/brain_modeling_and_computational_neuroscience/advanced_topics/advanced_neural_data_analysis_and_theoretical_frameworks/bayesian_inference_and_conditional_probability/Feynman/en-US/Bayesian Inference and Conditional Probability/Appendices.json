{
    "hands_on_practices": [
        {
            "introduction": "At the heart of Bayesian inference lies the elegant process of updating beliefs in light of new evidence. This first exercise provides a foundational look at this process in the context of a simple, yet powerful, scenario: estimating a hidden neural representation from a single noisy measurement . By working through the combination of a Gaussian prior with a Gaussian likelihood, you will derive not just the updated posterior distribution, but also uncover an intuitive principle where the final estimate is a precision-weighted average of your prior belief and the new data.",
            "id": "3964645",
            "problem": "In a neural encoding experiment, a single-trial sensory response $y$ is modeled as a noisy observation of an unknown latent stimulus representation $\\theta$. Assume additive Gaussian noise at the measurement stage due to synaptic and readout variability, and a Gaussian prior over the latent representation reflecting previously learned statistics of the environment. Concretely, suppose the likelihood and prior are specified by $p(y \\mid \\theta) = \\mathcal{N}(y; \\theta, \\sigma^{2})$ and $p(\\theta) = \\mathcal{N}(\\theta; \\mu_{0}, \\tau_{0}^{2})$, where $\\sigma^{2} > 0$ is the observation noise variance, $\\mu_{0} \\in \\mathbb{R}$ is the prior mean, and $\\tau_{0}^{2} > 0$ is the prior variance. Using the definition of Bayes’ theorem and properties of the Gaussian family, derive the posterior distribution $p(\\theta \\mid y)$ from first principles, compute its mean and variance in closed form, and express the posterior mean in a precision-weighted form, where precision is defined as the inverse of variance. Your final answer should be the two closed-form expressions for the posterior mean and posterior variance. No numerical approximation is required.",
            "solution": "The problem is valid. It describes a standard Bayesian inference scenario within computational neuroscience, involving a Gaussian likelihood and a Gaussian prior. All components are scientifically sound, well-defined, and the problem is mathematically well-posed, asking for a standard derivation with a unique solution.\n\nThe task is to derive the posterior distribution $p(\\theta \\mid y)$ for a latent variable $\\theta$ given an observation $y$. The likelihood and prior distributions are given as:\nLikelihood: $p(y \\mid \\theta) = \\mathcal{N}(y; \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right)$\nPrior: $p(\\theta) = \\mathcal{N}(\\theta; \\mu_0, \\tau_0^2) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right)$\n\nAccording to Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior:\n$$ p(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)} \\propto p(y \\mid \\theta) p(\\theta) $$\nThe term $p(y)$ is a normalization constant that does not depend on $\\theta$. We can find the functional form of the posterior by analyzing the product of the likelihood and prior.\n\nSubstituting the expressions for the Gaussian distributions:\n$$ p(\\theta \\mid y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right) \\right] $$\nIgnoring the constant multiplicative factors, we focus on the exponential terms:\n$$ p(\\theta \\mid y) \\propto \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2} - \\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right) $$\nLet's analyze the exponent, which we denote as $E$:\n$$ E = -\\frac{1}{2} \\left( \\frac{(y - \\theta)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right) $$\nWe expand the squared terms inside the parenthesis:\n$$ E = -\\frac{1}{2} \\left( \\frac{y^2 - 2y\\theta + \\theta^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\mu_0\\theta + \\mu_0^2}{\\tau_0^2} \\right) $$\nTo determine a new Gaussian distribution in $\\theta$, we collect terms with respect to powers of $\\theta$:\n$$ E = -\\frac{1}{2} \\left[ \\theta^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right) - 2\\theta \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right) + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right) \\right] $$\nThe posterior distribution for $\\theta$ is of the form $p(\\theta \\mid y) = \\mathcal{N}(\\theta; \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$, which has an exponent of the form $-\\frac{(\\theta - \\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2}$. Expanding this gives:\n$$ -\\frac{(\\theta - \\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2} = -\\frac{1}{2\\sigma_{\\text{post}}^2} (\\theta^2 - 2\\theta\\mu_{\\text{post}} + \\mu_{\\text{post}}^2) = -\\frac{1}{2} \\left( \\frac{1}{\\sigma_{\\text{post}}^2}\\theta^2 - \\frac{2\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2}\\theta + \\frac{\\mu_{\\text{post}}^2}{\\sigma_{\\text{post}}^2} \\right) $$\nBy comparing the coefficients of the powers of $\\theta$ in our derived exponent $E$ with this general form, we can identify the posterior mean $\\mu_{\\text{post}}$ and variance $\\sigma_{\\text{post}}^2$.\n\nFirst, comparing the coefficient of the $\\theta^2$ term:\n$$ \\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} $$\nThis immediately gives us the posterior variance $\\sigma_{\\text{post}}^2$:\n$$ \\sigma_{\\text{post}}^2 = \\left( \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} \\right)^{-1} = \\left( \\frac{\\tau_0^2 + \\sigma^2}{\\sigma^2 \\tau_0^2} \\right)^{-1} = \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2} $$\nNext, we compare the coefficient of the $\\theta$ term:\n$$ \\frac{2\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2} = 2 \\left( \\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) $$\n$$ \\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left( \\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) $$\nNow, substitute the expression we found for $\\sigma_{\\text{post}}^2$:\n$$ \\mu_{\\text{post}} = \\left( \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2} \\right) \\left( \\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) $$\n$$ \\mu_{\\text{post}} = \\left( \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2} \\right) \\left( \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 \\tau_0^2} \\right) $$\n$$ \\mu_{\\text{post}} = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2} $$\nThe posterior distribution is therefore a Gaussian, $p(\\theta \\mid y) = \\mathcal{N}(\\theta; \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$, with the derived mean and variance.\n\nThe problem also requires expressing the posterior mean in a precision-weighted form. Precision is defined as the inverse of variance. Let $\\pi_y = 1/\\sigma^2$ be the precision of the likelihood and $\\pi_0 = 1/\\tau_0^2$ be the precision of the prior. Let $\\pi_{\\text{post}} = 1/\\sigma_{\\text{post}}^2$ be the precision of the posterior.\nFrom our derivation for the posterior variance:\n$$ \\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} \\implies \\pi_{\\text{post}} = \\pi_y + \\pi_0 $$\nThis shows that the posterior precision is the sum of the likelihood precision and the prior precision.\nNow for the posterior mean. We can rewrite the expression for $\\mu_{\\text{post}}$ by dividing the numerator and denominator by $\\sigma^2 \\tau_0^2$:\n$$ \\mu_{\\text{post}} = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2} = \\frac{y \\frac{1}{\\sigma^2} + \\mu_0 \\frac{1}{\\tau_0^2}}{\\frac{1}{\\tau_0^2} + \\frac{1}{\\sigma^2}} $$\nIn terms of precisions, this becomes:\n$$ \\mu_{\\text{post}} = \\frac{y \\pi_y + \\mu_0 \\pi_0}{\\pi_y + \\pi_0} = \\frac{\\pi_y}{\\pi_y + \\pi_0} y + \\frac{\\pi_0}{\\pi_y + \\pi_0} \\mu_0 $$\nThis form beautifully illustrates that the posterior mean is a weighted average of the observation (measurement) $y$ and the prior mean $\\mu_0$, where the weights are determined by their respective precisions. The more precise (less uncertain) source contributes more to the final estimate.\n\nThe final requested answers are the closed-form expressions for the posterior mean and posterior variance.\nPosterior mean: $\\mu_{\\text{post}} = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2}$\nPosterior variance: $\\sigma_{\\text{post}}^2 = \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2} & \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from continuous measurements to the discrete world of neural spikes, this practice applies Bayesian principles to a core data type in neuroscience. Here, we model observed spike counts using a Poisson process, a standard for describing random point events, and infer the underlying firing rate $\\lambda$ . This exercise demonstrates the power of conjugate priors—in this case, the Gamma distribution for the Poisson likelihood—and extends the analysis to derive the marginal likelihood, or model evidence, a critical quantity for performing Bayesian model comparison.",
            "id": "3964667",
            "problem": "A single cortical neuron is observed under a stationary stimulus over $n$ non-overlapping time windows of known durations $t_1, t_2, \\dots, t_n$, with $t_i > 0$. Let the spike counts be $y_1, y_2, \\dots, y_n$, where each $y_i \\in \\{0,1,2,\\dots\\}$. Assume that, conditional on a constant firing rate $\\lambda$, the spike counts follow a Poisson process model, so that $y_i \\mid \\lambda \\sim \\operatorname{Poisson}(\\lambda t_i)$ independently across $i$. Prior belief about $\\lambda$ is encoded by a Gamma distribution with shape $\\alpha > 0$ and rate $\\beta > 0$, that is $p(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda)$ for $\\lambda > 0$.\n\nStarting from the definitions of the Poisson likelihood, the Gamma prior, and Bayes' theorem, derive the posterior distribution $p(\\lambda \\mid y_1, \\dots, y_n)$ and the marginal likelihood $p(y_1, \\dots, y_n)$ by integrating out $\\lambda$. Then, provide the closed-form expression of the log-marginal likelihood $\\log p(y_1, \\dots, y_n)$ as a function of $\\alpha$, $\\beta$, $\\{y_i\\}_{i=1}^n$, and $\\{t_i\\}_{i=1}^n$. Express your final answer for $\\log p(y_1, \\dots, y_n)$ in exact analytical form without approximation.",
            "solution": "The problem requires the derivation of the posterior distribution $p(\\lambda \\mid y_1, \\dots, y_n)$, the marginal likelihood $p(y_1, \\dots, y_n)$, and the log-marginal likelihood $\\log p(y_1, \\dots, y_n)$ for a Poisson model of neuronal spiking with a Gamma prior on the firing rate $\\lambda$.\n\nFirst, we validate the problem statement. The givens are:\n-   A set of $n$ spike counts $y_1, y_2, \\dots, y_n$ from non-overlapping time windows of durations $t_1, t_2, \\dots, t_n$.\n-   The likelihood model is a Poisson distribution for each count, $p(y_i \\mid \\lambda) = \\operatorname{Poisson}(y_i \\mid \\lambda t_i)$, where the observations are independent conditional on $\\lambda$.\n-   The prior distribution for the firing rate $\\lambda$ is a Gamma distribution, $p(\\lambda) = \\operatorname{Gamma}(\\lambda \\mid \\alpha, \\beta)$, with shape $\\alpha > 0$ and rate $\\beta > 0$.\n\nThe problem is scientifically grounded, as the Poisson process is a canonical model for spike counts and the Gamma distribution is its standard conjugate prior, forming a cornerstone of Bayesian analysis in computational neuroscience. The problem is well-posed, objective, and complete, providing all necessary information for a unique analytical solution. Thus, the problem is valid.\n\nWe proceed with the derivation.\n\nThe likelihood for a single observation $y_i$ is given by the Poisson probability mass function:\n$$p(y_i \\mid \\lambda) = \\frac{(\\lambda t_i)^{y_i} \\exp(-\\lambda t_i)}{y_i!}$$\nSince the observations are independent conditional on $\\lambda$, the total likelihood for the dataset $Y = (y_1, \\dots, y_n)$ is the product of the individual likelihoods:\n$$p(Y \\mid \\lambda) = \\prod_{i=1}^{n} p(y_i \\mid \\lambda) = \\prod_{i=1}^{n} \\frac{(\\lambda t_i)^{y_i} \\exp(-\\lambda t_i)}{y_i!}$$\nWe can group the terms that depend on $\\lambda$:\n$$p(Y \\mid \\lambda) = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\left( \\prod_{i=1}^{n} \\lambda^{y_i} \\right) \\left( \\prod_{i=1}^{n} \\exp(-\\lambda t_i) \\right)$$\n$$p(Y \\mid \\lambda) = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right)$$\n\nThe prior distribution for $\\lambda$ is a Gamma distribution:\n$$p(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda)$$\nwhere $\\Gamma(\\alpha)$ is the Gamma function.\n\nAccording to Bayes' theorem, the posterior distribution $p(\\lambda \\mid Y)$ is proportional to the product of the likelihood and the prior:\n$$p(\\lambda \\mid Y) \\propto p(Y \\mid \\lambda) p(\\lambda)$$\nSubstituting the expressions for the likelihood and prior:\n$$p(\\lambda \\mid Y) \\propto \\left[ \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\right] \\left[ \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda) \\right]$$\nWe collect all terms that do not depend on $\\lambda$ into the proportionality constant:\n$$p(\\lambda \\mid Y) \\propto \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\cdot \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda)$$\nCombining the powers of $\\lambda$ and the arguments of the exponential function:\n$$p(\\lambda \\mid Y) \\propto \\lambda^{\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right) - 1} \\exp\\left(-\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)\\lambda\\right)$$\nThis expression is the kernel of a Gamma distribution. We can identify the updated parameters for the posterior distribution. Let the posterior shape be $\\alpha'$ and the posterior rate be $\\beta'$:\n$$\\alpha' = \\alpha + \\sum_{i=1}^{n} y_i$$\n$$\\beta' = \\beta + \\sum_{i=1}^{n} t_i$$\nThus, the posterior distribution is a Gamma distribution:\n$$p(\\lambda \\mid Y) = \\operatorname{Gamma}(\\lambda \\mid \\alpha', \\beta') = \\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')} \\lambda^{\\alpha' - 1} \\exp(-\\beta' \\lambda)$$\n\nNext, we derive the marginal likelihood $p(Y)$. This is obtained by integrating the product of the likelihood and the prior over all possible values of $\\lambda$:\n$$p(Y) = \\int_{0}^{\\infty} p(Y \\mid \\lambda) p(\\lambda) \\,d\\lambda$$\n$$p(Y) = \\int_{0}^{\\infty} \\left[ \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\right] \\left[ \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda) \\right] \\,d\\lambda$$\nWe can pull the terms not dependent on $\\lambda$ out of the integral:\n$$p(Y) = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\int_{0}^{\\infty} \\lambda^{\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right) - 1} \\exp\\left(-\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)\\lambda\\right) \\,d\\lambda$$\nThe integral corresponds to the unnormalized Gamma distribution with parameters $\\alpha' = \\alpha + \\sum y_i$ and $\\beta' = \\beta + \\sum t_i$. The value of this integral is known to be $\\frac{\\Gamma(\\alpha')}{(\\beta')^{\\alpha'}}$. Substituting this result:\n$$p(Y) = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\frac{\\Gamma(\\alpha')}{(\\beta')^{\\alpha'}} = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\frac{\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)}{\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)^{\\alpha + \\sum_{i=1}^{n} y_i}}$$\n\nFinally, we compute the log-marginal likelihood, $\\log p(Y)$, by taking the natural logarithm of the expression for $p(Y)$:\n$$\\log p(Y) = \\log\\left[ \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\frac{\\beta^{\\alpha} \\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)}{\\Gamma(\\alpha) \\left(\\beta + \\sum_{i=1}^{n} t_i\\right)^{\\alpha + \\sum_{i=1}^{n} y_i}} \\right]$$\nUsing the properties of logarithms, we can expand this expression:\n$$\\log p(Y) = \\log\\left(\\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!}\\right) + \\log\\left(\\beta^{\\alpha}\\right) + \\log\\left(\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\right) - \\log(\\Gamma(\\alpha)) - \\log\\left(\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)^{\\alpha + \\sum_{i=1}^{n} y_i}\\right)$$\nThis simplifies to:\n$$\\log p(Y) = \\sum_{i=1}^{n} \\left( \\log(t_i^{y_i}) - \\log(y_i!) \\right) + \\alpha \\log(\\beta) + \\log\\left(\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\right) - \\log(\\Gamma(\\alpha)) - \\left(\\alpha + \\sum_{i=1}^{n} y_i\\right) \\log\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)$$\nUsing $\\ln$ for the natural logarithm and further simplifying the terms:\n$$\\log p(Y) = \\sum_{i=1}^{n} \\left(y_i \\ln(t_i) - \\ln(y_i!)\\right) + \\alpha \\ln(\\beta) + \\ln\\left(\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\right) - \\ln(\\Gamma(\\alpha)) - \\left(\\alpha + \\sum_{i=1}^{n} y_i\\right) \\ln\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)$$\nRearranging terms for clarity gives the final expression for the log-marginal likelihood:\n$$\\log p(Y) = \\ln\\left(\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\right) - \\ln(\\Gamma(\\alpha)) + \\alpha \\ln(\\beta) - \\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\ln\\left(\\beta + \\sum_{i=1}^{n} t_i\\right) + \\sum_{i=1}^{n} \\left(y_i \\ln(t_i) - \\ln(y_i!)\\right)$$\nThis is the closed-form analytical expression requested.",
            "answer": "$$\n\\boxed{\\ln\\left(\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\right) - \\ln(\\Gamma(\\alpha)) + \\alpha \\ln(\\beta) - \\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\ln\\left(\\beta + \\sum_{i=1}^{n} t_i\\right) + \\sum_{i=1}^{n} \\left(y_i \\ln(t_i) - \\ln(y_i!)\\right)}\n$$"
        },
        {
            "introduction": "Fitting a model to data is only half the battle; the other, equally important half is assessing whether the model is actually a good fit. This hands-on coding exercise introduces posterior predictive checking, a powerful simulation-based technique for model criticism . You will implement an algorithm to generate \"replicated\" datasets from your fitted model and compare their statistical properties to your real data, allowing you to diagnose specific ways in which your model might fail to capture the underlying neural dynamics.",
            "id": "3964656",
            "problem": "You are given a spike-train model in discrete time with equal-width bins of duration $\\Delta$ (in seconds). Let $y = (y_1,\\dots,y_B)$ be observed bin counts from a single neuron. Assume conditionally independent Poisson observations given an unknown constant firing intensity $\\lambda$ (in spikes per second) so that, given $\\lambda$, each bin count satisfies $y_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda \\Delta)$ independently across $i \\in \\{1,\\dots,B\\}$. A prior for $\\lambda$ is $\\lambda \\sim \\text{Gamma}(\\alpha_0,\\beta_0)$ using the shape–rate parameterization (that is, the density is proportional to $\\lambda^{\\alpha_0 - 1} \\exp(-\\beta_0 \\lambda)$). You will perform a posterior predictive check using the dispersion statistic\n$$\nT(y) \\equiv \\frac{1}{B} \\sum_{i=1}^B (y_i - \\bar{y})^2 - \\bar{y}, \\quad \\text{where} \\quad \\bar{y} \\equiv \\frac{1}{B}\\sum_{i=1}^B y_i.\n$$\nFor the posterior predictive check, define the posterior predictive $p$-value as\n$$\np \\equiv \\Pr\\big(T(\\tilde{y}) \\ge T(y) \\mid y\\big),\n$$\nwhere $\\tilde{y}$ is a replicated dataset drawn from the posterior predictive distribution. Compute $p$ by Monte Carlo posterior predictive simulation and report it as a decimal rounded to four places.\n\nFundamental base you may use:\n- Definitions of conditional probability, Bayes' theorem, and the law of total probability.\n- The definition of the Poisson likelihood for counts and the Gamma prior family.\n- The property that, conditional on $\\lambda$, the observations are independent.\n\nYour program must implement the following steps for each test case:\n1. Derive the posterior distribution of $\\lambda \\mid y$ from first principles using Bayes' theorem and the given prior–likelihood pairing.\n2. Generate posterior predictive replicates by first sampling $\\lambda$ from the posterior and then sampling $\\tilde{y}_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda \\Delta)$ independently for $i \\in \\{1,\\dots,B\\}$.\n3. Compute $T(y)$ for the observed data and $T(\\tilde{y})$ for each replicate.\n4. Estimate $p$ as the Monte Carlo fraction of replicates with $T(\\tilde{y}) \\ge T(y)$.\n\nUse the following fixed Monte Carlo settings for reproducibility:\n- Use exactly $120000$ posterior predictive replicates for each test case.\n- Use a fixed pseudo-random seed equal to $12345$.\n\nTest suite:\n- Case A (well-specified, approximately Poisson-like counts):\n  - Prior hyperparameters: $\\alpha_0 = 10^{-3}$, $\\beta_0 = 10^{-3}$.\n  - Number of bins: $B = 30$.\n  - Bin width: $\\Delta = 0.01$ seconds.\n  - Observed counts $y$ (length $30$): $[\\,0,1,1,0,1,1,1,0,2,0,0,1,0,1,0,1,1,0,1,0,2,1,0,1,0,0,1,1,0,0\\,]$.\n- Case B (overdispersed, bursty counts):\n  - Prior hyperparameters: $\\alpha_0 = 10^{-3}$, $\\beta_0 = 10^{-3}$.\n  - Number of bins: $B = 30$.\n  - Bin width: $\\Delta = 0.01$ seconds.\n  - Observed counts $y$ (length $30$): $[\\,0,2,0,0,1,3,0,0,0,2,1,0,0,1,0,4,0,0,2,0,1,0,0,3,0,0,1,0,2,0\\,]$.\n- Case C (underdispersed, near-binary counts):\n  - Prior hyperparameters: $\\alpha_0 = 10^{-3}$, $\\beta_0 = 10^{-3}$.\n  - Number of bins: $B = 30$.\n  - Bin width: $\\Delta = 0.01$ seconds.\n  - Observed counts $y$ (length $30$): $[\\,0,1,0,0,1,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0\\,]$.\n\nRequirements:\n- Your program should compute three posterior predictive $p$-values, one for each test case, and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, the output format must be exactly of the form $[p_A,p_B,p_C]$ with each value rounded to four decimal places and expressed as decimals (no percentage signs).\n- No external inputs are allowed; all data are provided above. The final output must be a single line with the exact required format.",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\nA list of all verbatim data, variables, and conditions is as follows:\n- **Model**: Spike-train model in discrete time.\n- **Bin duration**: $\\Delta$ (in seconds).\n- **Observed data**: $y = (y_1,\\dots,y_B)$ are observed bin counts from a single neuron.\n- **Likelihood**: Conditionally independent Poisson observations given an unknown constant firing intensity $\\lambda$ (in spikes per second). For each bin $i$, $y_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda \\Delta)$.\n- **Prior**: $\\lambda \\sim \\text{Gamma}(\\alpha_0,\\beta_0)$ using the shape–rate parameterization, with density proportional to $\\lambda^{\\alpha_0 - 1} \\exp(-\\beta_0 \\lambda)$.\n- **Task**: Perform a posterior predictive check using the dispersion statistic $T(y) \\equiv \\frac{1}{B} \\sum_{i=1}^B (y_i - \\bar{y})^2 - \\bar{y}$, where $\\bar{y} \\equiv \\frac{1}{B}\\sum_{i=1}^B y_i$.\n- **Target Quantity**: The posterior predictive $p$-value, defined as $p \\equiv \\Pr\\big(T(\\tilde{y}) \\ge T(y) \\mid y\\big)$, where $\\tilde{y}$ is a replicated dataset.\n- **Method**: Compute $p$ by Monte Carlo posterior predictive simulation.\n- **Monte Carlo Settings**: $120000$ posterior predictive replicates; fixed pseudo-random seed equal to $12345$.\n- **Test Case A**: $\\alpha_0 = 10^{-3}$, $\\beta_0 = 10^{-3}$, $B = 30$, $\\Delta = 0.01$, $y = [\\,0,1,1,0,1,1,1,0,2,0,0,1,0,1,0,1,1,0,1,0,2,1,0,1,0,0,1,1,0,0\\,]$.\n- **Test Case B**: $\\alpha_0 = 10^{-3}$, $\\beta_0 = 10^{-3}$, $B = 30$, $\\Delta = 0.01$, $y = [\\,0,2,0,0,1,3,0,0,0,2,1,0,0,1,0,4,0,0,2,0,1,0,0,3,0,0,1,0,2,0\\,]$.\n- **Test Case C**: $\\alpha_0 = 10^{-3}$, $\\beta_0 = 10^{-3}$, $B = 30$, $\\Delta = 0.01$, $y = [\\,0,1,0,0,1,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0\\,]$.\n- **Output Format**: A single line output `[p_A,p_B,p_C]` with each value rounded to four decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is grounded in standard Bayesian statistical methods applied to computational neuroscience. The Poisson-Gamma model is a canonical conjugate-pair model for count data, and its application to neural spike trains is a well-established technique. The dispersion statistic is a valid measure for checking the Poisson assumption (mean equals variance). The procedure is scientifically sound.\n- **Well-Posed**: The problem is well-posed. The derivation of the posterior is a standard textbook exercise. The simulation algorithm is precisely specified, including the number of replicates and the random seed, ensuring a unique and stable solution (up to the specifics of the pseudo-random number generator implementation).\n- **Objective**: The problem is stated in precise, mathematical language, free from any subjective or ambiguous terminology.\n- **Completeness**: The problem is self-contained. All necessary information, including the model, prior hyperparameters, observed data for three distinct cases, and simulation parameters, is explicitly provided.\n- **Consistency and Realism**: The provided data and parameters are consistent and plausible. The three datasets represent classic examples of neural firing patterns: approximately Poisson, overdispersed (bursty), and underdispersed (regular), which are commonly encountered in neuroscience research.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, well-posed, and complete. It is therefore deemed **valid**. A full solution will be provided.\n\nThe core of the problem is to assess a simple model of neuronal firing—one assuming a constant firing rate—against observed data. This is achieved via a posterior predictive check, a standard Bayesian model-checking technique. The process involves deriving the posterior distribution for the model parameter, simulating replicated data from the model, and comparing a chosen test statistic for the observed and replicated data.\n\nFirst, we derive the posterior distribution for the firing rate $\\lambda$ given the observed spike counts $y=(y_1, \\dots, y_B)$. The likelihood of observing the data $y$, given $\\lambda$, is the product of the probabilities of each independent count:\n$$\np(y \\mid \\lambda) = \\prod_{i=1}^B \\text{Poisson}(y_i \\mid \\lambda\\Delta) = \\prod_{i=1}^B \\frac{(\\lambda\\Delta)^{y_i} e^{-\\lambda\\Delta}}{y_i!}\n$$\nAs a function of $\\lambda$, this is proportional to:\n$$\np(y \\mid \\lambda) \\propto \\left( \\prod_{i=1}^B (\\lambda\\Delta)^{y_i} \\right) \\left( \\prod_{i=1}^B e^{-\\lambda\\Delta} \\right) = (\\lambda\\Delta)^{\\sum y_i} e^{-B\\lambda\\Delta} \\propto \\lambda^{\\sum y_i} e^{-(B\\Delta)\\lambda}\n$$\nLet $S = \\sum_{i=1}^B y_i$ be the total number of observed spikes. The likelihood is then proportional to $\\lambda^S e^{-(B\\Delta)\\lambda}$. The prior distribution for $\\lambda$ is given as a Gamma distribution with shape parameter $\\alpha_0$ and rate parameter $\\beta_0$:\n$$\np(\\lambda) \\propto \\lambda^{\\alpha_0 - 1} e^{-\\beta_0\\lambda}\n$$\nAccording to Bayes' theorem, the posterior distribution $p(\\lambda \\mid y)$ is proportional to the product of the likelihood and the prior:\n$$\np(\\lambda \\mid y) \\propto p(y \\mid \\lambda) p(\\lambda) \\propto (\\lambda^S e^{-(B\\Delta)\\lambda}) (\\lambda^{\\alpha_0 - 1} e^{-\\beta_0\\lambda})\n$$\nCombining terms involving $\\lambda$ gives:\n$$\np(\\lambda \\mid y) \\propto \\lambda^{S + \\alpha_0 - 1} e^{-(B\\Delta + \\beta_0)\\lambda}\n$$\nThis expression is the kernel of a Gamma distribution. Therefore, the posterior distribution is also a Gamma distribution, a property known as conjugacy. The posterior distribution for $\\lambda$ is:\n$$\n\\lambda \\mid y \\sim \\text{Gamma}(\\alpha_N, \\beta_N)\n$$\nwhere the posterior hyperparameters are updated as:\n- Posterior shape: $\\alpha_N = \\alpha_0 + S = \\alpha_0 + \\sum_{i=1}^B y_i$\n- Posterior rate: $\\beta_N = \\beta_0 + B\\Delta$\n\nNext, we perform the posterior predictive check. This involves comparing the observed data to data that the model would predict. We simulate replicated datasets $\\tilde{y}$ from the posterior predictive distribution, $p(\\tilde{y} \\mid y) = \\int p(\\tilde{y} \\mid \\lambda) p(\\lambda \\mid y) d\\lambda$. The simulation proceeds in two steps: first draw a parameter value from its posterior, then draw data conditional on that parameter.\n\nThe algorithm for the Monte Carlo estimation of the posterior predictive $p$-value, $p = \\Pr(T(\\tilde{y}) \\ge T(y) \\mid y)$, is as follows:\n1.  For the observed data $y$, calculate the observed value of the test statistic, $T(y)$. The statistic $T(y) = \\frac{1}{B} \\sum_{i=1}^B (y_i - \\bar{y})^2 - \\bar{y}$ measures the dispersion of the counts. For a Poisson distribution, the variance equals the mean, so we would expect $T(y)$ to be close to $0$ if the data were truly Poisson-distributed with a fixed rate.\n2.  Calculate the posterior hyperparameters $\\alpha_N$ and $\\beta_N$ using the observed data.\n3.  Generate a large number, $N_{mc} = 120000$, of posterior predictive replicates. For each replicate $j \\in \\{1, \\dots, N_{mc}\\}$:\n    a. Draw a sample of the firing rate from its posterior distribution: $\\lambda^{(j)} \\sim \\text{Gamma}(\\alpha_N, \\beta_N)$.\n    b. Generate a replicated dataset $\\tilde{y}^{(j)} = (\\tilde{y}_1^{(j)}, \\dots, \\tilde{y}_B^{(j)})$ by drawing each count from a Poisson distribution with the sampled rate: $\\tilde{y}_i^{(j)} \\sim \\text{Poisson}(\\lambda^{(j)}\\Delta)$ for $i \\in \\{1, \\dots, B\\}$.\n    c. Compute the test statistic for this replicate: $T(\\tilde{y}^{(j)})$.\n4.  The $p$-value is estimated as the fraction of replicates for which the test statistic is greater than or equal to the observed test statistic:\n$$\np \\approx \\frac{1}{N_{mc}} \\sum_{j=1}^{N_{mc}} \\mathbb{I}(T(\\tilde{y}^{(j)}) \\ge T(y))\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function.\n\nA $p$-value close to $0$ or $1$ indicates that the observed data is atypical under the model's predictions, suggesting model misfit. For this dispersion statistic, a $p$-value near $0$ implies the observed data is more dispersed than the model expects (e.g., bursty firing). A $p$-value near $1$ implies the observed data is less dispersed (more regular) than the model expects.\n\nThis procedure will now be implemented for the three test cases provided, using the specified fixed random seed for reproducibility. The final result for each case will be rounded to four decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef calculate_p_value(y, alpha_0, beta_0, B, delta, num_replicates, rng):\n    \"\"\"\n    Computes the posterior predictive p-value for a Poisson-Gamma model.\n\n    Args:\n        y (list or np.ndarray): Observed spike counts.\n        alpha_0 (float): Prior shape parameter for the Gamma distribution.\n        beta_0 (float): Prior rate parameter for the Gamma distribution.\n        B (int): Number of time bins.\n        delta (float): Duration of each time bin.\n        num_replicates (int): Number of Monte Carlo replicates.\n        rng (np.random.Generator): NumPy random number generator.\n\n    Returns:\n        float: The calculated posterior predictive p-value.\n    \"\"\"\n    # Ensure y is a numpy array for vectorized operations\n    y = np.array(y)\n\n    # 1. Calculate the observed test statistic T(y)\n    y_mean = np.mean(y)\n    y_var = np.var(y) # np.var uses ddof=0, which corresponds to the 1/B factor\n    T_obs = y_var - y_mean\n\n    # 2. Calculate posterior hyperparameters\n    S = np.sum(y)\n    alpha_n = alpha_0 + S\n    beta_n = beta_0 + B * delta\n\n    # 3. Generate posterior predictive replicates and their test statistics\n    \n    # 3a. Draw all lambda samples from the posterior Gamma distribution at once\n    # Note: numpy.random.gamma uses scale = 1/rate\n    lambda_samples = rng.gamma(alpha_n, scale=1.0/beta_n, size=num_replicates)\n\n    # 3b. Generate replicated datasets y_tilde\n    # The rate for the Poisson is lambda * delta. We can use broadcasting.\n    # poisson_rates shape: (num_replicates, 1)\n    # y_replicates shape: (num_replicates, B)\n    poisson_rates = lambda_samples[:, np.newaxis] * delta\n    y_replicates = rng.poisson(lam=poisson_rates, size=(num_replicates, B))\n\n    # 3c. Compute the test statistic T(y_tilde) for each replicate\n    rep_means = np.mean(y_replicates, axis=1)\n    rep_vars = np.var(y_replicates, axis=1)\n    T_reps = rep_vars - rep_means\n\n    # 4. Estimate the p-value\n    # This is the fraction of replicates where T(y_tilde) >= T(y)\n    p_value = np.mean(T_reps >= T_obs)\n\n    return p_value\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    NUM_REPLICATES = 120000\n    SEED = 12345\n\n    # Define the test cases\n    test_cases = [\n        {   # Case A: Well-specified\n            \"y\": [0,1,1,0,1,1,1,0,2,0,0,1,0,1,0,1,1,0,1,0,2,1,0,1,0,0,1,1,0,0],\n            \"alpha_0\": 1e-3, \"beta_0\": 1e-3, \"B\": 30, \"delta\": 0.01\n        },\n        {   # Case B: Overdispersed\n            \"y\": [0,2,0,0,1,3,0,0,0,2,1,0,0,1,0,4,0,0,2,0,1,0,0,3,0,0,1,0,2,0],\n            \"alpha_0\": 1e-3, \"beta_0\": 1e-3, \"B\": 30, \"delta\": 0.01\n        },\n        {   # Case C: Underdispersed\n            \"y\": [0,1,0,0,1,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0],\n            \"alpha_0\": 1e-3, \"beta_0\": 1e-3, \"B\": 30, \"delta\": 0.01\n        }\n    ]\n\n    # Initialize the random number generator with the fixed seed\n    rng = np.random.default_rng(SEED)\n\n    results = []\n    for case in test_cases:\n        p_val = calculate_p_value(\n            y=case[\"y\"],\n            alpha_0=case[\"alpha_0\"],\n            beta_0=case[\"beta_0\"],\n            B=case[\"B\"],\n            delta=case[\"delta\"],\n            num_replicates=NUM_REPLICATES,\n            rng=rng\n        )\n        # Format to four decimal places as required\n        results.append(f\"{p_val:.4f}\")\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solver\nsolve()\n```"
        }
    ]
}