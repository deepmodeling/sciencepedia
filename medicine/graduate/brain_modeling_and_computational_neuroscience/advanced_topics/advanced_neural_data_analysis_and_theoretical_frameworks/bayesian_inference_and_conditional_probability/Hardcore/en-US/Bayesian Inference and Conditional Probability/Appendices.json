{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides a foundational exercise in Bayesian inference for continuous variables. By assuming both the likelihood and prior are Gaussian, we can derive the posterior distribution in closed form, a scenario known as a conjugate model. This exercise is crucial for building intuition about how Bayesian updating works, revealing how the posterior mean becomes a weighted average of the prior mean and the observed data, with the weights determined by their respective precisions. ",
            "id": "3964645",
            "problem": "In a neural encoding experiment, a single-trial sensory response $y$ is modeled as a noisy observation of an unknown latent stimulus representation $\\theta$. Assume additive Gaussian noise at the measurement stage due to synaptic and readout variability, and a Gaussian prior over the latent representation reflecting previously learned statistics of the environment. Concretely, suppose the likelihood and prior are specified by $p(y \\mid \\theta) = \\mathcal{N}(y; \\theta, \\sigma^{2})$ and $p(\\theta) = \\mathcal{N}(\\theta; \\mu_{0}, \\tau_{0}^{2})$, where $\\sigma^{2}  0$ is the observation noise variance, $\\mu_{0} \\in \\mathbb{R}$ is the prior mean, and $\\tau_{0}^{2}  0$ is the prior variance. Using the definition of Bayes’ theorem and properties of the Gaussian family, derive the posterior distribution $p(\\theta \\mid y)$ from first principles, compute its mean and variance in closed form, and express the posterior mean in a precision-weighted form, where precision is defined as the inverse of variance. Your final answer should be the two closed-form expressions for the posterior mean and posterior variance. No numerical approximation is required.",
            "solution": "The problem is valid. It describes a standard Bayesian inference scenario within computational neuroscience, involving a Gaussian likelihood and a Gaussian prior. All components are scientifically sound, well-defined, and the problem is mathematically well-posed, asking for a standard derivation with a unique solution.\n\nThe task is to derive the posterior distribution $p(\\theta \\mid y)$ for a latent variable $\\theta$ given an observation $y$. The likelihood and prior distributions are given as:\nLikelihood: $p(y \\mid \\theta) = \\mathcal{N}(y; \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right)$\nPrior: $p(\\theta) = \\mathcal{N}(\\theta; \\mu_0, \\tau_0^2) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right)$\n\nAccording to Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior:\n$$ p(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)} \\propto p(y \\mid \\theta) p(\\theta) $$\nThe term $p(y)$ is a normalization constant that does not depend on $\\theta$. We can find the functional form of the posterior by analyzing the product of the likelihood and prior.\n\nSubstituting the expressions for the Gaussian distributions:\n$$ p(\\theta \\mid y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right) \\right] $$\nIgnoring the constant multiplicative factors, we focus on the exponential terms:\n$$ p(\\theta \\mid y) \\propto \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2} - \\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right) $$\nLet's analyze the exponent, which we denote as $E$:\n$$ E = -\\frac{1}{2} \\left( \\frac{(y - \\theta)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right) $$\nWe expand the squared terms inside the parenthesis:\n$$ E = -\\frac{1}{2} \\left( \\frac{y^2 - 2y\\theta + \\theta^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\mu_0\\theta + \\mu_0^2}{\\tau_0^2} \\right) $$\nTo determine a new Gaussian distribution in $\\theta$, we collect terms with respect to powers of $\\theta$:\n$$ E = -\\frac{1}{2} \\left[ \\theta^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right) - 2\\theta \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right) + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right) \\right] $$\nThe posterior distribution for $\\theta$ is of the form $p(\\theta \\mid y) = \\mathcal{N}(\\theta; \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$, which has an exponent of the form $-\\frac{(\\theta - \\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2}$. Expanding this gives:\n$$ -\\frac{(\\theta - \\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2} = -\\frac{1}{2\\sigma_{\\text{post}}^2} (\\theta^2 - 2\\theta\\mu_{\\text{post}} + \\mu_{\\text{post}}^2) = -\\frac{1}{2} \\left( \\frac{1}{\\sigma_{\\text{post}}^2}\\theta^2 - \\frac{2\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2}\\theta + \\frac{\\mu_{\\text{post}}^2}{\\sigma_{\\text{post}}^2} \\right) $$\nBy comparing the coefficients of the powers of $\\theta$ in our derived exponent $E$ with this general form, we can identify the posterior mean $\\mu_{\\text{post}}$ and variance $\\sigma_{\\text{post}}^2$.\n\nFirst, comparing the coefficient of the $\\theta^2$ term:\n$$ \\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} $$\nThis immediately gives us the posterior variance $\\sigma_{\\text{post}}^2$:\n$$ \\sigma_{\\text{post}}^2 = \\left( \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} \\right)^{-1} = \\left( \\frac{\\tau_0^2 + \\sigma^2}{\\sigma^2 \\tau_0^2} \\right)^{-1} = \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2} $$\nNext, we compare the coefficient of the $\\theta$ term:\n$$ \\frac{2\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2} = 2 \\left( \\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) $$\n$$ \\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left( \\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) $$\nNow, substitute the expression we found for $\\sigma_{\\text{post}}^2$:\n$$ \\mu_{\\text{post}} = \\left( \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2} \\right) \\left( \\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) $$\n$$ \\mu_{\\text{post}} = \\left( \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2} \\right) \\left( \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 \\tau_0^2} \\right) $$\n$$ \\mu_{\\text{post}} = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2} $$\nThe posterior distribution is therefore a Gaussian, $p(\\theta \\mid y) = \\mathcal{N}(\\theta; \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$, with the derived mean and variance.\n\nThe problem also requires expressing the posterior mean in a precision-weighted form. Precision is defined as the inverse of variance. Let $\\pi_y = 1/\\sigma^2$ be the precision of the likelihood and $\\pi_0 = 1/\\tau_0^2$ be the precision of the prior. Let $\\pi_{\\text{post}} = 1/\\sigma_{\\text{post}}^2$ be the precision of the posterior.\nFrom our derivation for the posterior variance:\n$$ \\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} \\implies \\pi_{\\text{post}} = \\pi_y + \\pi_0 $$\nThis shows that the posterior precision is the sum of the likelihood precision and the prior precision.\nNow for the posterior mean. We can rewrite the expression for $\\mu_{\\text{post}}$ by dividing the numerator and denominator by $\\sigma^2 \\tau_0^2$:\n$$ \\mu_{\\text{post}} = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2} = \\frac{y \\frac{1}{\\sigma^2} + \\mu_0 \\frac{1}{\\tau_0^2}}{\\frac{1}{\\tau_0^2} + \\frac{1}{\\sigma^2}} $$\nIn terms of precisions, this becomes:\n$$ \\mu_{\\text{post}} = \\frac{y \\pi_y + \\mu_0 \\pi_0}{\\pi_y + \\pi_0} = \\frac{\\pi_y}{\\pi_y + \\pi_0} y + \\frac{\\pi_0}{\\pi_y + \\pi_0} \\mu_0 $$\nThis form beautifully illustrates that the posterior mean is a weighted average of the observation (measurement) $y$ and the prior mean $\\mu_0$, where the weights are determined by their respective precisions. The more precise (less uncertain) source contributes more to the final estimate.\n\nThe final requested answers are the closed-form expressions for the posterior mean and posterior variance.\nPosterior mean: $\\mu_{\\text{post}} = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2}$\nPosterior variance: $\\sigma_{\\text{post}}^2 = \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2}$",
            "answer": "$$\n\\boxed{\n\\begin{aligned}\n\\mu_{\\text{post}} = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2} \\\\\n\\sigma_{\\text{post}}^2 = \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2}\n\\end{aligned}\n}\n$$"
        },
        {
            "introduction": "Moving from continuous variables to discrete events, this practice addresses the core task of modeling neuronal spike counts. We will use a Poisson likelihood, the standard model for spike counts under a constant firing rate, paired with its conjugate Gamma prior. This exercise not only demonstrates Bayesian updating for a key model in neuroscience but also guides you through deriving the marginal likelihood, or model evidence, which is a fundamental quantity for performing Bayesian model selection. ",
            "id": "3964667",
            "problem": "A single cortical neuron is observed under a stationary stimulus over $n$ non-overlapping time windows of known durations $t_1, t_2, \\dots, t_n$, with $t_i  0$. Let the spike counts be $y_1, y_2, \\dots, y_n$, where each $y_i \\in \\{0,1,2,\\dots\\}$. Assume that, conditional on a constant firing rate $\\lambda$, the spike counts follow a Poisson process model, so that $y_i \\mid \\lambda \\sim \\operatorname{Poisson}(\\lambda t_i)$ independently across $i$. Prior belief about $\\lambda$ is encoded by a Gamma distribution with shape $\\alpha  0$ and rate $\\beta  0$, that is $p(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda)$ for $\\lambda  0$.\n\nStarting from the definitions of the Poisson likelihood, the Gamma prior, and Bayes' theorem, derive the posterior distribution $p(\\lambda \\mid y_1, \\dots, y_n)$ and the marginal likelihood $p(y_1, \\dots, y_n)$ by integrating out $\\lambda$. Then, provide the closed-form expression of the log-marginal likelihood $\\log p(y_1, \\dots, y_n)$ as a function of $\\alpha$, $\\beta$, $\\{y_i\\}_{i=1}^n$, and $\\{t_i\\}_{i=1}^n$. Express your final answer for $\\log p(y_1, \\dots, y_n)$ in exact analytical form without approximation.",
            "solution": "The problem requires the derivation of the posterior distribution $p(\\lambda \\mid y_1, \\dots, y_n)$, the marginal likelihood $p(y_1, \\dots, y_n)$, and the log-marginal likelihood $\\log p(y_1, \\dots, y_n)$ for a Poisson model of neuronal spiking with a Gamma prior on the firing rate $\\lambda$.\n\nFirst, we validate the problem statement. The givens are:\n-   A set of $n$ spike counts $y_1, y_2, \\dots, y_n$ from non-overlapping time windows of durations $t_1, t_2, \\dots, t_n$.\n-   The likelihood model is a Poisson distribution for each count, $p(y_i \\mid \\lambda) = \\operatorname{Poisson}(y_i \\mid \\lambda t_i)$, where the observations are independent conditional on $\\lambda$.\n-   The prior distribution for the firing rate $\\lambda$ is a Gamma distribution, $p(\\lambda) = \\operatorname{Gamma}(\\lambda \\mid \\alpha, \\beta)$, with shape $\\alpha  0$ and rate $\\beta  0$.\n\nThe problem is scientifically grounded, as the Poisson process is a canonical model for spike counts and the Gamma distribution is its standard conjugate prior, forming a cornerstone of Bayesian analysis in computational neuroscience. The problem is well-posed, objective, and complete, providing all necessary information for a unique analytical solution. Thus, the problem is valid.\n\nWe proceed with the derivation.\n\nThe likelihood for a single observation $y_i$ is given by the Poisson probability mass function:\n$$p(y_i \\mid \\lambda) = \\frac{(\\lambda t_i)^{y_i} \\exp(-\\lambda t_i)}{y_i!}$$\nSince the observations are independent conditional on $\\lambda$, the total likelihood for the dataset $Y = (y_1, \\dots, y_n)$ is the product of the individual likelihoods:\n$$p(Y \\mid \\lambda) = \\prod_{i=1}^{n} p(y_i \\mid \\lambda) = \\prod_{i=1}^{n} \\frac{(\\lambda t_i)^{y_i} \\exp(-\\lambda t_i)}{y_i!}$$\nWe can group the terms that depend on $\\lambda$:\n$$p(Y \\mid \\lambda) = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\left( \\prod_{i=1}^{n} \\lambda^{y_i} \\right) \\left( \\prod_{i=1}^{n} \\exp(-\\lambda t_i) \\right)$$\n$$p(Y \\mid \\lambda) = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right)$$\n\nThe prior distribution for $\\lambda$ is a Gamma distribution:\n$$p(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda)$$\nwhere $\\Gamma(\\alpha)$ is the Gamma function.\n\nAccording to Bayes' theorem, the posterior distribution $p(\\lambda \\mid Y)$ is proportional to the product of the likelihood and the prior:\n$$p(\\lambda \\mid Y) \\propto p(Y \\mid \\lambda) p(\\lambda)$$\nSubstituting the expressions for the likelihood and prior:\n$$p(\\lambda \\mid Y) \\propto \\left[ \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\right] \\left[ \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda) \\right]$$\nWe collect all terms that do not depend on $\\lambda$ into the proportionality constant:\n$$p(\\lambda \\mid Y) \\propto \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\cdot \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda)$$\nCombining the powers of $\\lambda$ and the arguments of the exponential function:\n$$p(\\lambda \\mid Y) \\propto \\lambda^{\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right) - 1} \\exp\\left(-\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)\\lambda\\right)$$\nThis expression is the kernel of a Gamma distribution. We can identify the updated parameters for the posterior distribution. Let the posterior shape be $\\alpha'$ and the posterior rate be $\\beta'$:\n$$\\alpha' = \\alpha + \\sum_{i=1}^{n} y_i$$\n$$\\beta' = \\beta + \\sum_{i=1}^{n} t_i$$\nThus, the posterior distribution is a Gamma distribution:\n$$p(\\lambda \\mid Y) = \\operatorname{Gamma}(\\lambda \\mid \\alpha', \\beta') = \\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')} \\lambda^{\\alpha' - 1} \\exp(-\\beta' \\lambda)$$\n\nNext, we derive the marginal likelihood $p(Y)$. This is obtained by integrating the product of the likelihood and the prior over all possible values of $\\lambda$:\n$$p(Y) = \\int_{0}^{\\infty} p(Y \\mid \\lambda) p(\\lambda) \\,d\\lambda$$\n$$p(Y) = \\int_{0}^{\\infty} \\left[ \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\right] \\left[ \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda) \\right] \\,d\\lambda$$\nWe can pull the terms not dependent on $\\lambda$ out of the integral:\n$$p(Y) = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\int_{0}^{\\infty} \\lambda^{\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right) - 1} \\exp\\left(-\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)\\lambda\\right) \\,d\\lambda$$\nThe integral corresponds to the unnormalized Gamma distribution with parameters $\\alpha' = \\alpha + \\sum y_i$ and $\\beta' = \\beta + \\sum t_i$. The value of this integral is known to be $\\frac{\\Gamma(\\alpha')}{(\\beta')^{\\alpha'}}$. Substituting this result:\n$$p(Y) = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\frac{\\Gamma(\\alpha')}{(\\beta')^{\\alpha'}} = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\frac{\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)}{\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)^{\\alpha + \\sum_{i=1}^{n} y_i}}$$\n\nFinally, we compute the log-marginal likelihood, $\\log p(Y)$, by taking the natural logarithm of the expression for $p(Y)$:\n$$\\log p(Y) = \\log\\left[ \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\frac{\\beta^{\\alpha} \\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)}{\\Gamma(\\alpha) \\left(\\beta + \\sum_{i=1}^{n} t_i\\right)^{\\alpha + \\sum_{i=1}^{n} y_i}} \\right]$$\nUsing the properties of logarithms, we can expand this expression:\n$$\\log p(Y) = \\log\\left(\\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!}\\right) + \\log\\left(\\beta^{\\alpha}\\right) + \\log\\left(\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\right) - \\log(\\Gamma(\\alpha)) - \\log\\left(\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)^{\\alpha + \\sum_{i=1}^{n} y_i}\\right)$$\nThis simplifies to:\n$$\\log p(Y) = \\sum_{i=1}^{n} \\left( \\log(t_i^{y_i}) - \\log(y_i!) \\right) + \\alpha \\log(\\beta) + \\log\\left(\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\right) - \\log(\\Gamma(\\alpha)) - \\left(\\alpha + \\sum_{i=1}^{n} y_i\\right) \\log\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)$$\nUsing $\\ln$ for the natural logarithm and further simplifying the terms:\n$$\\log p(Y) = \\sum_{i=1}^{n} \\left(y_i \\ln(t_i) - \\ln(y_i!)\\right) + \\alpha \\ln(\\beta) + \\ln\\left(\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\right) - \\ln(\\Gamma(\\alpha)) - \\left(\\alpha + \\sum_{i=1}^{n} y_i\\right) \\ln\\left(\\beta + \\sum_{i=1}^{n} t_i\\right)$$\nRearranging terms for clarity gives the final expression for the log-marginal likelihood:\n$$\\log p(Y) = \\ln\\left(\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\right) - \\ln(\\Gamma(\\alpha)) + \\alpha \\ln(\\beta) - \\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\ln\\left(\\beta + \\sum_{i=1}^{n} t_i\\right) + \\sum_{i=1}^{n} \\left(y_i \\ln(t_i) - \\ln(y_i!)\\right)$$\nThis is the closed-form analytical expression requested.",
            "answer": "$$\n\\boxed{\\ln\\left(\\Gamma\\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\right) - \\ln(\\Gamma(\\alpha)) + \\alpha \\ln(\\beta) - \\left(\\alpha + \\sum_{i=1}^{n} y_i\\right)\\ln\\left(\\beta + \\sum_{i=1}^{n} t_i\\right) + \\sum_{i=1}^{n} \\left(y_i \\ln(t_i) - \\ln(y_i!)\\right)}\n$$"
        },
        {
            "introduction": "Real-world neural models are often too complex for exact posterior inference, necessitating the use of approximation methods. This practice introduces the Laplace approximation, a powerful technique for approximating the posterior distribution and the model evidence in the context of a Poisson Generalized Linear Model (GLM). You will derive the approximation and interpret the resulting \"Occam factor,\" a term that elegantly penalizes model complexity and embodies the principle of parsimony in a quantitative way. ",
            "id": "3964638",
            "problem": "A single cortical neuron is recorded across $n$ stimulus presentations, producing spike counts $\\{y_i\\}_{i=1}^{n}$ modeled by a Poisson Generalized Linear Model (GLM), where the conditional intensity is $\\lambda_i(\\theta) = \\exp(\\theta^{\\top} x_i)$ with feature vectors $x_i \\in \\mathbb{R}^{d}$ and parameter vector $\\theta \\in \\mathbb{R}^{d}$. The prior over $\\theta$ is Gaussian, $\\theta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)$. Let $\\hat{\\theta}$ denote the Maximum A Posteriori (MAP) estimate, and let $H(\\hat{\\theta})$ denote the Hessian of the negative log posterior evaluated at $\\hat{\\theta}$, i.e., $H(\\hat{\\theta}) = -\\nabla_{\\theta}^{2}\\left[\\ln p(y\\mid \\theta) + \\ln p(\\theta)\\right]\\big|_{\\theta=\\hat{\\theta}}$, assumed positive definite.\n\nStarting from Bayes’ rule and the definition of the evidence (marginal likelihood) $p(y)$, derive a leading-order approximation to $\\ln p(y)$ by applying a second-order Taylor expansion of the log posterior around $\\hat{\\theta}$ and evaluating the resulting Gaussian integral exactly. In your derivation, clearly identify the multiplicative term that emerges from the curvature of the posterior as the “Occam factor” and express it in terms of $d$ and $H(\\hat{\\theta})$. Provide a brief conceptual interpretation of how this term penalizes overly flexible models within this neural GLM context.\n\nFor a particular experiment with $d=3$, suppose that the Hessian at the MAP is diagonal,\n$$\nH(\\hat{\\theta}) = \\begin{pmatrix}\n120  0  0\\\\\n0  50  0\\\\\n0  0  10\n\\end{pmatrix}.\n$$\nCompute the Occam factor as a real-valued number. Round your answer to four significant figures.",
            "solution": "The primary goal is to approximate the model evidence (or marginal likelihood), $p(y)$, which is defined by the integral of the likelihood times the prior over all possible parameter values:\n$$\np(y) = \\int p(y|\\theta) p(\\theta) \\, d\\theta\n$$\nThe integrand, $p(y|\\theta) p(\\theta)$, is proportional to the posterior distribution $p(\\theta|y)$. Let's define the logarithm of the unnormalized posterior as $L(\\theta) = \\ln[p(y|\\theta) p(\\theta)] = \\ln p(y|\\theta) + \\ln p(\\theta)$. The evidence can be written as:\n$$\np(y) = \\int \\exp(L(\\theta)) \\, d\\theta\n$$\nThe Laplace approximation method involves performing a second-order Taylor series expansion of $L(\\theta)$ around its maximum, which is the Maximum A Posteriori (MAP) estimate, $\\hat{\\theta}$. The expansion is:\n$$\nL(\\theta) \\approx L(\\hat{\\theta}) + (\\theta - \\hat{\\theta})^{\\top} \\nabla_{\\theta} L(\\theta)\\big|_{\\theta=\\hat{\\theta}} + \\frac{1}{2} (\\theta - \\hat{\\theta})^{\\top} \\nabla_{\\theta}^{2} L(\\theta)\\big|_{\\theta=\\hat{\\theta}} (\\theta - \\hat{\\theta})\n$$\nBy definition of the MAP estimate $\\hat{\\theta}$, it is a mode of the posterior, so the gradient term is zero: $\\nabla_{\\theta} L(\\theta)\\big|_{\\theta=\\hat{\\theta}} = 0$. The problem defines the Hessian of the *negative* log posterior as $H(\\hat{\\theta}) = -\\nabla_{\\theta}^{2} L(\\theta)\\big|_{\\theta=\\hat{\\theta}}$. Substituting these into the expansion gives:\n$$\nL(\\theta) \\approx L(\\hat{\\theta}) - \\frac{1}{2} (\\theta - \\hat{\\theta})^{\\top} H(\\hat{\\theta}) (\\theta - \\hat{\\theta})\n$$\nNow, we substitute this approximation back into the integral for the evidence:\n$$\np(y) \\approx \\int \\exp\\left(L(\\hat{\\theta}) - \\frac{1}{2} (\\theta - \\hat{\\theta})^{\\top} H(\\hat{\\theta}) (\\theta - \\hat{\\theta})\\right) \\, d\\theta\n$$\nThe term $\\exp(L(\\hat{\\theta}))$ is constant with respect to $\\theta$ and can be factored out of the integral:\n$$\np(y) \\approx \\exp(L(\\hat{\\theta})) \\int \\exp\\left(-\\frac{1}{2} (\\theta - \\hat{\\theta})^{\\top} H(\\hat{\\theta}) (\\theta - \\hat{\\theta})\\right) \\, d\\theta\n$$\nThe remaining integral is the normalization constant of a multivariate Gaussian distribution in $d$ dimensions with mean $\\hat{\\theta}$ and precision matrix (inverse covariance) $H(\\hat{\\theta})$. The value of this integral is given by:\n$$\n\\int \\exp\\left(-\\frac{1}{2} (\\theta - \\hat{\\theta})^{\\top} H(\\hat{\\theta}) (\\theta - \\hat{\\theta})\\right) \\, d\\theta = (2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2}\n$$\nTherefore, the approximation for the evidence is:\n$$\np(y) \\approx \\exp(L(\\hat{\\theta})) (2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2}\n$$\nSubstituting back $L(\\hat{\\theta}) = \\ln p(y|\\hat{\\theta}) + \\ln p(\\hat{\\theta})$, we get:\n$$\np(y) \\approx p(y|\\hat{\\theta}) p(\\hat{\\theta}) (2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2}\n$$\nTaking the natural logarithm yields the desired approximation for the log evidence:\n$$\n\\ln p(y) \\approx \\ln p(y|\\hat{\\theta}) + \\ln p(\\hat{\\theta}) + \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(H(\\hat{\\theta})))\n$$\nThis expression shows that the log evidence is approximately the log posterior evaluated at its peak, plus a correction term related to the volume of the posterior.\n\n**Occam Factor and Interpretation**\n\nThe evidence approximation is a product of terms: $p(y) \\approx \\underbrace{p(y|\\hat{\\theta})}_\\text{Best-fit Likelihood} \\times \\underbrace{p(\\hat{\\theta})}_\\text{Prior at MAP} \\times \\underbrace{(2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2}}_\\text{Curvature Term}$.\n\nThe question asks to identify the multiplicative term that emerges from the curvature of the posterior as the \"Occam factor\". Based on our derivation, this term is:\n$$\n\\text{Occam Factor} = (2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2}\n$$\nThis term acts as a penalty for model complexity, an embodiment of Occam's razor. Its conceptual interpretation in the context of the neural GLM is as follows: The total evidence balances the model's goodness-of-fit (high likelihood $p(y|\\hat{\\theta})$) against its complexity. An overly flexible model (e.g., one with many parameters or weak prior constraints) can contort itself to fit the observed spike data very well, leading to a high likelihood value. However, this extreme fitting often results in a posterior distribution that is exceptionally sharp and narrow around the MAP estimate $\\hat{\\theta}$, as the model's flexibility is \"used up\" to explain the specific data instance. A very sharp posterior corresponds to a high curvature, meaning the Hessian $H(\\hat{\\theta})$ has a large determinant, $\\det(H(\\hat{\\theta}))$. Since the Occam factor is proportional to $\\det(H(\\hat{\\theta}))^{-1/2}$, a large determinant results in a very small Occam factor. This small multiplicative factor severely penalizes the high likelihood score, reducing the overall evidence for the overly flexible model. In essence, the Occam factor favors models that provide a good, robust explanation for the data (a reasonably high likelihood and a reasonably concentrated posterior) without being excessively tuned, thereby preventing overfitting.\n\n**Numerical Calculation**\n\nWe are given the dimension $d=3$ and the Hessian matrix:\n$$\nH(\\hat{\\theta}) = \\begin{pmatrix}\n120  0  0\\\\\n0  50  0\\\\\n0  0  10\n\\end{pmatrix}\n$$\nSince the matrix is diagonal, its determinant is the product of its diagonal entries:\n$$\n\\det(H(\\hat{\\theta})) = 120 \\times 50 \\times 10 = 60000\n$$\nNow we can compute the Occam factor:\n$$\n\\text{Occam Factor} = (2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2} = (2\\pi)^{3/2} (60000)^{-1/2}\n$$\nLet's evaluate this expression:\n$$\n (2\\pi)^{3/2} = (2\\pi)\\sqrt{2\\pi} \\approx 15.7496099\n$$\n$$\n(60000)^{-1/2} = \\frac{1}{\\sqrt{60000}} = \\frac{1}{100\\sqrt{6}} \\approx 0.0040824829\n$$\nMultiplying these two values:\n$$\n\\text{Occam Factor} \\approx 15.7496099 \\times 0.0040824829 \\approx 0.06429708\n$$\nRounding to four significant figures, we get $0.06430$.",
            "answer": "$$\n\\boxed{0.06430}\n$$"
        }
    ]
}