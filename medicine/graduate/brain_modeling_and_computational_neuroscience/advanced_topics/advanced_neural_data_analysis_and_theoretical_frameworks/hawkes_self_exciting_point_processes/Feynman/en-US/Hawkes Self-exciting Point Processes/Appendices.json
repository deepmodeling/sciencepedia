{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with the simplest case: a single, isolated Hawkes process. Understanding its long-term behavior is key to interpreting more complex models. This first practice challenges you to derive the fundamental quantities that define the process's stability and average activity level—the branching ratio and the stationary firing rate—from first principles .",
            "id": "3986738",
            "problem": "A single-neuron spike train in cortex is modeled as a linear Hawkes self-exciting point process on the real line with conditional intensity $ \\lambda(t) $ defined by the standard causal construction: $ \\lambda(t) $ depends on an exogenous baseline drive and the history of the process through a causal kernel. Specifically, suppose $ \\lambda(t) $ is given by a constant baseline $ \\mu $ (units $ \\mathrm{s}^{-1} $) plus contributions from past spikes via a nonnegative causal kernel $ \\phi(u) $ as $ t $ evolves, with $ u $ denoting elapsed time since a past spike. The biological interpretation is that each spike produces a transient increase in excitability that decays over time.\n\nAssume the following parameterization is scientifically valid for an excitatory synaptic effect: $ \\phi(u) = \\alpha \\exp(-\\beta u) $ for $ u > 0 $, and $ \\phi(u) = 0 $ for $ u \\leq 0 $, where $ \\alpha > 0 $ and $ \\beta > 0 $. The parameters satisfy dimensional consistency: $ \\mu $ has units $ \\mathrm{s}^{-1} $, $ \\phi $ has units $ \\mathrm{s}^{-1} $, and therefore $ \\alpha $ has units $ \\mathrm{s}^{-1} $ while $ \\beta $ has units $ \\mathrm{s}^{-1} $. The process is assumed to have achieved a statistically stationary regime in which the mean intensity exists and is finite.\n\nStarting only from the causal definition of the Hawkes process intensity and the interpretation that each spike contributes additional expected future spikes via the kernel, derive expressions for the following quantities in closed form in terms of $ \\mu $, $ \\alpha $, and $ \\beta $:\n\n- The branching ratio $ \\eta $, defined as the expected number of direct offspring generated by a single event through the self-excitation mechanism.\n- The stationary mean intensity $ \\bar{\\lambda} $, expressed in spikes per second (units $ \\mathrm{s}^{-1} $).\n- The stationary fraction of endogenous events (those generated by self-excitation) and the stationary fraction of exogenous events (immigrants generated by the baseline drive), each expressed as a dimensionless quantity in $ [0,1] $.\n\nAssume all necessary conditions for stationarity hold. If any condition on the parameters is required for the derivation to be valid, state it clearly in the solution. Provide your final expressions exactly; no rounding is required. In your final answer, present the four requested quantities in the order $ \\eta $, $ \\bar{\\lambda} $, endogenous fraction, exogenous fraction.",
            "solution": "The problem asks for the derivation of four key quantities related to a stationary linear Hawkes self-exciting point process: the branching ratio $\\eta$, the stationary mean intensity $\\bar{\\lambda}$, and the stationary fractions of endogenous and exogenous events. The process is defined by its conditional intensity function $\\lambda(t)$, which depends on a constant baseline rate $\\mu$ and the history of past events $\\{t_i\\}$ through a causal kernel $\\phi(u)$.\n\nThe conditional intensity at time $t$ is given by:\n$$\n\\lambda(t) = \\mu + \\sum_{t_i  t} \\phi(t - t_i)\n$$\nwhere the kernel is specified as $\\phi(u) = \\alpha \\exp(-\\beta u)$ for $u > 0$ and $\\phi(u) = 0$ for $u \\leq 0$. The parameters are given as $\\mu > 0$, $\\alpha > 0$, and $\\beta > 0$.\n\nFirst, we derive the branching ratio $\\eta$. The branching ratio is defined as the expected number of direct offspring events generated by a single parent event. A single event at time $t_j$ adds the term $\\phi(t - t_j)$ to the intensity for all future times $t > t_j$. The expected number of new events generated by this contribution to the intensity is the integral of this function over all future time.\nLetting $u = t - t_j$ be the time elapsed since the parent event, the expected number of offspring is:\n$$\n\\eta = \\int_{0}^{\\infty} \\phi(u) \\, du\n$$\nSubstituting the given exponential kernel:\n$$\n\\eta = \\int_{0}^{\\infty} \\alpha \\exp(-\\beta u) \\, du\n$$\nThis is a standard exponential integral. Since $\\alpha$ is a constant, we have:\n$$\n\\eta = \\alpha \\int_{0}^{\\infty} \\exp(-\\beta u) \\, du = \\alpha \\left[ -\\frac{1}{\\beta} \\exp(-\\beta u) \\right]_{0}^{\\infty}\n$$\nEvaluating the limits, and noting that $\\beta > 0$ ensures $\\lim_{u \\to \\infty} \\exp(-\\beta u) = 0$:\n$$\n\\eta = \\alpha \\left( 0 - \\left(-\\frac{1}{\\beta} \\exp(0)\\right) \\right) = \\alpha \\left(\\frac{1}{\\beta}\\right) = \\frac{\\alpha}{\\beta}\n$$\nThis expression for $\\eta$ is dimensionless, as both $\\alpha$ and $\\beta$ have units of $\\mathrm{s}^{-1}$.\n\nNext, we derive the stationary mean intensity $\\bar{\\lambda}$. For a stationary process, the mean intensity $\\bar{\\lambda}$ is constant, $\\bar{\\lambda} = E[\\lambda(t)]$. We take the expectation of the conditional intensity equation:\n$$\n\\bar{\\lambda} = E\\left[\\mu + \\sum_{t_i  t} \\phi(t - t_i)\\right]\n$$\nBy linearity of expectation:\n$$\n\\bar{\\lambda} = E[\\mu] + E\\left[\\sum_{t_i  t} \\phi(t - t_i)\\right] = \\mu + E\\left[\\int_{-\\infty}^{t} \\phi(t-s) \\, dN(s)\\right]\n$$\nwhere $dN(s)$ is the counting process increment at time $s$. Using Campbell's theorem for stationary point processes, the expectation of the integral is the integral of the kernel weighted by the mean rate of the process, $E[dN(s)] = \\bar{\\lambda} \\, ds$:\n$$\n\\bar{\\lambda} = \\mu + \\int_{-\\infty}^{t} \\phi(t-s) \\bar{\\lambda} \\, ds\n$$\nPerforming a change of variables with $u = t - s$, so $du = -ds$:\n$$\n\\bar{\\lambda} = \\mu + \\bar{\\lambda} \\int_{0}^{\\infty} \\phi(u) \\, du\n$$\nThe integral $\\int_{0}^{\\infty} \\phi(u) \\, du$ is precisely the branching ratio $\\eta$ we calculated earlier. This yields the fundamental self-consistency equation for the mean rate:\n$$\n\\bar{\\lambda} = \\mu + \\bar{\\lambda} \\eta\n$$\nWe can now solve for $\\bar{\\lambda}$:\n$$\n\\bar{\\lambda} (1 - \\eta) = \\mu\n$$\n$$\n\\bar{\\lambda} = \\frac{\\mu}{1 - \\eta}\n$$\nFor the process to be stationary and have a finite, positive mean intensity, we must have $\\bar{\\lambda}  0$. Since $\\mu > 0$ is given, this requires $1 - \\eta  0$, which implies $\\eta  1$. This is the critical condition for the stability of a linear Hawkes process. Substituting the expressions for $\\eta$, the condition is $\\frac{\\alpha}{\\beta}  1$, or $\\alpha  \\beta$.\nSubstituting $\\eta = \\frac{\\alpha}{\\beta}$ into the expression for $\\bar{\\lambda}$:\n$$\n\\bar{\\lambda} = \\frac{\\mu}{1 - \\frac{\\alpha}{\\beta}} = \\frac{\\mu}{\\frac{\\beta - \\alpha}{\\beta}} = \\frac{\\mu \\beta}{\\beta - \\alpha}\n$$\nThis expression for $\\bar{\\lambda}$ is valid under the condition that $\\alpha  \\beta$. The units are consistent, as $\\mu$ has units of $\\mathrm{s}^{-1}$ and the ratio $\\frac{\\beta}{\\beta - \\alpha}$ is dimensionless.\n\nFinally, we determine the stationary fractions of endogenous and exogenous events. The Hawkes process can be viewed through a \"thinning\" or \"clustering\" construction. Events are either \"exogenous\" immigrants, arriving according to a homogeneous Poisson process with rate $\\mu$, or \"endogenous\" offspring, generated by previous events.\nIn the stationary state, the total rate of events is $\\bar{\\lambda}$.\nThe rate of exogenous events is, by definition, $\\mu$.\nThe rate of endogenous events is the total rate minus the exogenous rate: $\\bar{\\lambda} - \\mu$.\nThe fraction of exogenous events is the ratio of the exogenous rate to the total rate:\n$$\n\\text{Fraction}_{\\text{exogenous}} = \\frac{\\mu}{\\bar{\\lambda}}\n$$\nUsing the relationship $\\bar{\\lambda} = \\frac{\\mu}{1 - \\eta}$, we find:\n$$\n\\text{Fraction}_{\\text{exogenous}} = \\frac{\\mu}{\\frac{\\mu}{1 - \\eta}} = 1 - \\eta = 1 - \\frac{\\alpha}{\\beta} = \\frac{\\beta - \\alpha}{\\beta}\n$$\nThe fraction of endogenous events is the ratio of the endogenous rate to the total rate:\n$$\n\\text{Fraction}_{\\text{endogenous}} = \\frac{\\bar{\\lambda} - \\mu}{\\bar{\\lambda}} = 1 - \\frac{\\mu}{\\bar{\\lambda}}\n$$\nThis is simply $1$ minus the exogenous fraction:\n$$\n\\text{Fraction}_{\\text{endogenous}} = 1 - (1 - \\eta) = \\eta = \\frac{\\alpha}{\\beta}\n$$\nThis result is intuitive: the branching ratio $\\eta$, which is the average number of offspring per event, is precisely the fraction of all events that are offspring in the stationary state. Both fractions are dimensionless and sum to $1$, as expected.\n\nThe four requested quantities in order are:\n1.  Branching ratio $\\eta$: $\\frac{\\alpha}{\\beta}$\n2.  Stationary mean intensity $\\bar{\\lambda}$: $\\frac{\\mu \\beta}{\\beta - \\alpha}$\n3.  Stationary fraction of endogenous events: $\\frac{\\alpha}{\\beta}$\n4.  Stationary fraction of exogenous events: $\\frac{\\beta - \\alpha}{\\beta}$\nAll expressions are derived under the necessary condition for stationarity: $\\eta  1$, which translates to $\\alpha  \\beta$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\alpha}{\\beta}  \\frac{\\mu \\beta}{\\beta - \\alpha}  \\frac{\\alpha}{\\beta}  \\frac{\\beta - \\alpha}{\\beta} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The true power of Hawkes processes is revealed when modeling networks of interacting elements, such as neurons in a circuit. This exercise transitions from the single-process case to a multivariate network, introducing the matrix-based formalism essential for this leap. You will practice determining the stability of an entire network and calculating the resulting stationary firing rates for each component .",
            "id": "3986832",
            "problem": "Consider a network of $d=3$ interacting neuronal spike trains modeled as a multivariate Hawkes self-exciting point process. The conditional intensity function for neuron $i \\in \\{1,2,3\\}$ at time $t$ is defined by\n$$\n\\lambda_i(t) \\;=\\; \\mu_i \\;+\\; \\sum_{j=1}^{3} \\int_{0}^{\\infty} \\phi_{ij}(s)\\, \\mathrm{d}N_j(t-s),\n$$\nwhere $\\mu_i \\ge 0$ is the baseline rate, $\\phi_{ij}(s) \\ge 0$ is the interaction kernel from neuron $j$ to neuron $i$, and $N_j(t)$ is the counting process of neuron $j$. Let the total interaction strengths be summarized by the matrix with entries\n$$\nA_{ij} \\;=\\; \\int_{0}^{\\infty} \\phi_{ij}(s)\\,\\mathrm{d}s,\n$$\nso that $A$ collects the integrated kernels (sometimes called the branching matrix). You are given the baseline rate vector (in spikes per second)\n$$\n\\mu \\;=\\; \\begin{pmatrix} \\frac{4}{5} \\\\ \\frac{1}{2} \\\\ \\frac{3}{10} \\end{pmatrix},\n$$\nand the integrated kernel matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n\\frac{1}{5}  \\frac{1}{10}  0 \\\\\n0  \\frac{3}{10}  \\frac{1}{20} \\\\\n0  0  \\frac{1}{10}\n\\end{pmatrix}.\n$$\nStarting only from the defining properties of the multivariate Hawkes process, and using standard results about expectations of point processes and matrices, do the following:\n\n- Determine whether the system is subcritical by computing the spectral radius of $A$ and comparing it to $1$, justifying your conclusion from first principles.\n- Derive and compute the stationary mean firing rate vector $r = \\begin{pmatrix} r_1  r_2  r_3 \\end{pmatrix}^{\\top}$ implied by stationarity and linearity of the model.\n\nProvide exact values; do not round. Express $r$ in spikes per second. For the subcriticality calculation, report the spectral radius of $A$ as a unitless exact value. For your final answer, report a single row matrix containing first the spectral radius of $A$, followed by $r_1$, $r_2$, and $r_3$, in that order. No rounding is required.",
            "solution": "The problem requires us to analyze a $d=3$ dimensional multivariate Hawkes process, first by determining its stability and second by computing its stationary mean firing rates.\n\nThe first task is to determine if the system is subcritical. A multivariate Hawkes process is stable, or subcritical, if and only if the spectral radius of its integrated kernel matrix $A$ is strictly less than $1$. The spectral radius, denoted $\\rho(A)$, is defined as the maximum of the moduli of the eigenvalues of $A$. The given integrated kernel matrix is:\n$$\nA \\;=\\; \\begin{pmatrix}\n\\frac{1}{5}  \\frac{1}{10}  0 \\\\\n0  \\frac{3}{10}  \\frac{1}{20} \\\\\n0  0  \\frac{1}{10}\n\\end{pmatrix}\n$$\nThis matrix is upper triangular. A fundamental property of triangular matrices is that their eigenvalues are the entries on their main diagonal. Therefore, the eigenvalues of $A$ are $\\lambda_1 = \\frac{1}{5}$, $\\lambda_2 = \\frac{3}{10}$, and $\\lambda_3 = \\frac{1}{10}$.\n\nThe spectral radius is the maximum of the absolute values of these eigenvalues:\n$$\n\\rho(A) \\;=\\; \\max\\left\\{ \\left|\\frac{1}{5}\\right|, \\left|\\frac{3}{10}\\right|, \\left|\\frac{1}{10}\\right| \\right\\}\n$$\nSince all eigenvalues are positive, this simplifies to:\n$$\n\\rho(A) \\;=\\; \\max\\left\\{ \\frac{1}{5}, \\frac{3}{10}, \\frac{1}{10} \\right\\} \\;=\\; \\max\\left\\{ 0.2, 0.3, 0.1 \\right\\} \\;=\\; 0.3 \\;=\\; \\frac{3}{10}\n$$\nThe condition for subcriticality is $\\rho(A)  1$. In our case, $\\rho(A) = \\frac{3}{10}$, which satisfies the condition. Thus, the system is subcritical. This ensures that the process is non-explosive and converges to a stationary state with a well-defined mean firing rate. From a branching process perspective, $\\rho(A)$ represents the average number of direct offspring events triggered by a single event, so a value less than $1$ implies that event cascades will eventually die out.\n\nThe second task is to derive and compute the stationary mean firing rate vector $r = \\begin{pmatrix} r_1  r_2  r_3 \\end{pmatrix}^{\\top}$. For a stationary process, the mean firing rate $r_i$ of neuron $i$ is constant over time and is equal to the expectation of its conditional intensity, $r_i = \\mathbb{E}[\\lambda_i(t)]$.\n\nThe conditional intensity for neuron $i$ is given by:\n$$\n\\lambda_i(t) \\;=\\; \\mu_i \\;+\\; \\sum_{j=1}^{3} \\int_{0}^{\\infty} \\phi_{ij}(s)\\,\\mathrm{d}N_j(t-s)\n$$\nTaking the expectation of both sides, and using the linearity of the expectation operator:\n$$\nr_i \\;=\\; \\mathbb{E}[\\lambda_i(t)] \\;=\\; \\mathbb{E}[\\mu_i] \\;+\\; \\sum_{j=1}^{3} \\mathbb{E}\\left[\\int_{0}^{\\infty} \\phi_{ij}(s)\\,\\mathrm{d}N_j(t-s)\\right]\n$$\nSince $\\mu_i$ is a constant baseline rate, $\\mathbb{E}[\\mu_i] = \\mu_i$. By Campbell's theorem for stationary point processes, the expectation of the stochastic integral is:\n$$\n\\mathbb{E}\\left[\\int_{0}^{\\infty} \\phi_{ij}(s)\\,\\mathrm{d}N_j(t-s)\\right] \\;=\\; \\int_{0}^{\\infty} \\phi_{ij}(s) \\mathbb{E}[\\mathrm{d}N_j(t-s)]\n$$\nThe expected number of events for a stationary process with rate $r_j$ in an infinitesimal interval $\\mathrm{d}s$ is $r_j \\mathrm{d}s$. Therefore:\n$$\n\\int_{0}^{\\infty} \\phi_{ij}(s) r_j \\,\\mathrm{d}s \\;=\\; r_j \\int_{0}^{\\infty} \\phi_{ij}(s)\\,\\mathrm{d}s \\;=\\; r_j A_{ij}\n$$\nSubstituting this back into the equation for $r_i$:\n$$\nr_i \\;=\\; \\mu_i \\;+\\; \\sum_{j=1}^{3} A_{ij} r_j\n$$\nThis system of equations for $i = 1, 2, 3$ can be expressed in matrix form as:\n$$\nr \\;=\\; \\mu \\;+\\; A r\n$$\nwhere $r = \\begin{pmatrix} r_1  r_2  r_3 \\end{pmatrix}^{\\top}$ and $\\mu = \\begin{pmatrix} \\mu_1  \\mu_2  \\mu_3 \\end{pmatrix}^{\\top}$. We can solve for $r$:\n$$\nI r - A r \\;=\\; \\mu \\implies (I-A)r \\;=\\; \\mu\n$$\nThe existence of a unique, non-negative solution for $r$ is guaranteed by the subcriticality condition $\\rho(A)1$, which ensures that $(I-A)$ is invertible. The system of equations is:\n$$\n\\begin{pmatrix} 1 - \\frac{1}{5}  -\\frac{1}{10}  0 \\\\ 0  1 - \\frac{3}{10}  -\\frac{1}{20} \\\\ 0  0  1 - \\frac{1}{10} \\end{pmatrix}\n\\begin{pmatrix} r_1 \\\\ r_2 \\\\ r_3 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} \\frac{4}{5} \\\\ \\frac{1}{2} \\\\ \\frac{3}{10} \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} \\frac{4}{5}  -\\frac{1}{10}  0 \\\\ 0  \\frac{7}{10}  -\\frac{1}{20} \\\\ 0  0  \\frac{9}{10} \\end{pmatrix}\n\\begin{pmatrix} r_1 \\\\ r_2 \\\\ r_3 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} \\frac{4}{5} \\\\ \\frac{1}{2} \\\\ \\frac{3}{10} \\end{pmatrix}\n$$\nThis is an upper triangular system, which can be solved efficiently by back-substitution, starting from the last equation for $r_3$:\n$$\n\\frac{9}{10} r_3 = \\frac{3}{10} \\quad\\implies\\quad r_3 = \\frac{3}{10} \\cdot \\frac{10}{9} = \\frac{3}{9} = \\frac{1}{3}\n$$\nNext, we solve for $r_2$ using the second equation and the value of $r_3$:\n$$\n\\frac{7}{10} r_2 - \\frac{1}{20} r_3 = \\frac{1}{2} \\quad\\implies\\quad \\frac{7}{10} r_2 - \\frac{1}{20}\\left(\\frac{1}{3}\\right) = \\frac{1}{2}\n$$\n$$\n\\frac{7}{10} r_2 - \\frac{1}{60} = \\frac{1}{2} \\quad\\implies\\quad \\frac{7}{10} r_2 = \\frac{1}{2} + \\frac{1}{60} = \\frac{30}{60} + \\frac{1}{60} = \\frac{31}{60}\n$$\n$$\nr_2 = \\frac{31}{60} \\cdot \\frac{10}{7} = \\frac{31}{42}\n$$\nFinally, we solve for $r_1$ using the first equation and the value of $r_2$:\n$$\n\\frac{4}{5} r_1 - \\frac{1}{10} r_2 = \\frac{4}{5} \\quad\\implies\\quad \\frac{4}{5} r_1 - \\frac{1}{10}\\left(\\frac{31}{42}\\right) = \\frac{4}{5}\n$$\nDividing the entire equation by $\\frac{4}{5}$ gives:\n$$\nr_1 - \\frac{1}{10}\\left(\\frac{31}{42}\\right)\\left(\\frac{5}{4}\\right) = 1 \\quad\\implies\\quad r_1 - \\frac{31}{8 \\cdot 42} = 1\n$$\n$$\nr_1 - \\frac{31}{336} = 1 \\quad\\implies\\quad r_1 = 1 + \\frac{31}{336} = \\frac{336+31}{336} = \\frac{367}{336}\n$$\nThe spectral radius is $\\frac{3}{10}$. The stationary mean firing rate vector is $r = \\begin{pmatrix} \\frac{367}{336}  \\frac{31}{42}  \\frac{1}{3} \\end{pmatrix}^{\\top}$ in units of spikes per second.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{3}{10}  \\frac{367}{336}  \\frac{31}{42}  \\frac{1}{3} \\end{pmatrix}}$$"
        },
        {
            "introduction": "A theoretical model is only as useful as our ability to connect it to real-world observations. This final practice delves into the crucial task of parameter estimation from data, the cornerstone of statistical inference with Hawkes processes. You will derive the gradient of the log-likelihood function, a critical component required to find the model parameters that best explain an observed set of event times using numerical optimization techniques .",
            "id": "3986888",
            "problem": "Consider a $p$-dimensional Hawkes self-exciting point process modeling spiking activity of $p$ neurons over a finite observation window $[0,T]$, with counting processes $\\{N_i(t)\\}_{i=1}^p$ and event times $\\{t_k^{(i)}\\}_{k=1}^{n_i}$ for neuron $i$. The conditional intensity of neuron $i$ is given by\n$$\n\\lambda_i(t) \\;=\\; \\mu_i \\;+\\; \\sum_{j=1}^p \\int_{0}^{t} \\phi_{ij}(t-s)\\,\\mathrm{d}N_j(s),\n$$\nwhere $\\mu_i \\ge 0$ is the baseline rate and the interaction kernel is exponential,\n$$\n\\phi_{ij}(u) \\;=\\; \\alpha_{ij}\\,\\exp(-\\beta u)\\,\\mathbf{1}_{\\{u0\\}},\n$$\nwith fixed $\\beta0$ and parameters $\\alpha_{ij}\\ge 0$. Assume the standard log-likelihood for simple point processes over $[0,T]$,\n$$\n\\ell \\;=\\; \\sum_{i=1}^p \\left( \\int_{0}^{T} \\ln \\lambda_i(t)\\,\\mathrm{d}N_i(t) \\;-\\; \\int_{0}^{T} \\lambda_i(t)\\,\\mathrm{d}t \\right),\n$$\nand that $\\lambda_i(t)0$ for all $t\\in[0,T]$ almost surely.\n\nDerive the gradient components of the log-likelihood with respect to $\\mu_i$ and $\\alpha_{ij}$, expressed explicitly in terms of the observed event times, the parameters, and $\\beta$. Your final answer must be two symbolic expressions representing the generic components $\\partial \\ell/\\partial \\mu_i$ and $\\partial \\ell/\\partial \\alpha_{ij}$, in closed form. Express the final result as a single row matrix containing these two expressions. No numerical approximation is required, and no units are involved.",
            "solution": "We are tasked with deriving the gradient of the log-likelihood function $\\ell$ with respect to the parameters $\\mu_i$ and $\\alpha_{ij}$. The log-likelihood for a $p$-dimensional point process over the interval $[0,T]$ is given by:\n$$ \\ell = \\sum_{r=1}^p \\left( \\int_{0}^{T} \\ln \\lambda_r(t)\\,\\mathrm{d}N_r(t) - \\int_{0}^{T} \\lambda_r(t)\\,\\mathrm{d}t \\right) $$\nThe integral with respect to the counting measure $\\mathrm{d}N_r(t)$ can be expressed as a sum over the event times $\\{t_k^{(r)}\\}_{k=1}^{n_r}$ of neuron $r$. Thus, the log-likelihood can be written as:\n$$ \\ell = \\sum_{r=1}^p \\left( \\sum_{k=1}^{n_r} \\ln \\lambda_r(t_k^{(r)}) - \\int_{0}^{T} \\lambda_r(t)\\,\\mathrm{d}t \\right) $$\nThe conditional intensity for neuron $r$ at time $t$ is:\n$$ \\lambda_r(t) = \\mu_r + \\sum_{s=1}^p \\int_{0}^{t} \\phi_{rs}(t-u)\\,\\mathrm{d}N_s(u) $$\nWith the exponential kernel $\\phi_{rs}(u) = \\alpha_{rs}\\exp(-\\beta u)$, this becomes:\n$$ \\lambda_r(t) = \\mu_r + \\sum_{s=1}^p \\sum_{t_m^{(s)}t} \\alpha_{rs} \\exp(-\\beta(t-t_m^{(s)})) $$\nWe can compute the gradient of $\\ell$ with respect to a generic parameter $\\theta$ using the chain rule. Assuming sufficient regularity to interchange differentiation and integration (which holds for this model), we have:\n$$ \\frac{\\partial \\ell}{\\partial \\theta} = \\sum_{r=1}^p \\left( \\sum_{k=1}^{n_r} \\frac{1}{\\lambda_r(t_k^{(r)})} \\frac{\\partial \\lambda_r(t_k^{(r)})}{\\partial \\theta} - \\int_{0}^{T} \\frac{\\partial \\lambda_r(t)}{\\partial \\theta}\\,\\mathrm{d}t \\right) $$\n\nFirst, we derive the partial derivative with respect to $\\mu_i$.\nLet the parameter $\\theta$ be $\\mu_i$. The parameter $\\mu_i$ only appears in the expression for $\\lambda_i(t)$. It does not affect $\\lambda_r(t)$ for $r \\neq i$. Therefore, in the sum over $r$, only the term for $r=i$ is non-zero.\nThe derivative of $\\lambda_i(t)$ with respect to $\\mu_i$ is:\n$$ \\frac{\\partial \\lambda_i(t)}{\\partial \\mu_i} = \\frac{\\partial}{\\partial \\mu_i} \\left( \\mu_i + \\sum_{j=1}^p \\sum_{t_m^{(j)}t} \\alpha_{ij} \\exp(-\\beta(t-t_m^{(j)})) \\right) = 1 $$\nSubstituting this into the general formula for the gradient:\n$$ \\frac{\\partial \\ell}{\\partial \\mu_i} = \\left( \\sum_{k=1}^{n_i} \\frac{1}{\\lambda_i(t_k^{(i)})} \\cdot 1 \\right) - \\int_{0}^{T} 1\\,\\mathrm{d}t $$\nThe integral evaluates to $T$. This yields:\n$$ \\frac{\\partial \\ell}{\\partial \\mu_i} = \\sum_{k=1}^{n_i} \\frac{1}{\\lambda_i(t_k^{(i)})} - T $$\nTo express this explicitly in terms of the given parameters and event times, we substitute the expression for $\\lambda_i(t_k^{(i)})$:\n$$ \\frac{\\partial \\ell}{\\partial \\mu_i} = \\sum_{k=1}^{n_i} \\left( \\mu_i + \\sum_{j=1}^p \\sum_{t_m^{(j)}  t_k^{(i)}} \\alpha_{ij} \\exp(-\\beta(t_k^{(i)} - t_m^{(j)})) \\right)^{-1} - T $$\n\nNext, we derive the partial derivative with respect to $\\alpha_{ij}$.\nLet the parameter $\\theta$ be $\\alpha_{ij}$. This parameter only appears in the expression for $\\lambda_i(t)$. Thus, only the $r=i$ term in the sum for $\\frac{\\partial \\ell}{\\partial \\theta}$ survives. We need the derivative of $\\lambda_i(t)$ with respect to $\\alpha_{ij}$:\n$$ \\frac{\\partial \\lambda_i(t)}{\\partial \\alpha_{ij}} = \\frac{\\partial}{\\partial \\alpha_{ij}} \\left( \\mu_i + \\sum_{l=1}^p \\sum_{t_q^{(l)}t} \\alpha_{il} \\exp(-\\beta(t-t_q^{(l)})) \\right) $$\nThe derivative is non-zero only for the term where $l=j$:\n$$ \\frac{\\partial \\lambda_i(t)}{\\partial \\alpha_{ij}} = \\sum_{t_m^{(j)}t} \\exp(-\\beta(t-t_m^{(j)})) $$\nLet's denote this derivative by $D_{ij}(t)$. Substituting this into the general gradient formula:\n$$ \\frac{\\partial \\ell}{\\partial \\alpha_{ij}} = \\sum_{k=1}^{n_i} \\frac{D_{ij}(t_k^{(i)})}{\\lambda_i(t_k^{(i)})} - \\int_{0}^{T} D_{ij}(t)\\,\\mathrm{d}t $$\nThe first term is a sum over the spike times of neuron $i$:\n$$ \\sum_{k=1}^{n_i} \\frac{1}{\\lambda_i(t_k^{(i)})} \\left( \\sum_{t_m^{(j)}  t_k^{(i)}} \\exp(-\\beta(t_k^{(i)} - t_m^{(j)})) \\right) $$\nThe second term is an integral we need to evaluate:\n$$ \\int_{0}^{T} D_{ij}(t)\\,\\mathrm{d}t = \\int_{0}^{T} \\left( \\sum_{t_m^{(j)}t} \\exp(-\\beta(t-t_m^{(j)})) \\right) \\mathrm{d}t $$\nWe can swap the summation (which is over the $n_j$ spikes of neuron $j$) and the integration:\n$$ \\sum_{m=1}^{n_j} \\int_{0}^{T} \\mathbf{1}_{\\{t > t_m^{(j)}\\}} \\exp(-\\beta(t-t_m^{(j)}))\\,\\mathrm{d}t = \\sum_{m=1}^{n_j} \\int_{t_m^{(j)}}^{T} \\exp(-\\beta(t-t_m^{(j)}))\\,\\mathrm{d}t $$\nLet $u = t - t_m^{(j)}$, so $\\mathrm{d}u = \\mathrm{d}t$. The integration limits become $0$ and $T - t_m^{(j)}$:\n$$ \\sum_{m=1}^{n_j} \\int_{0}^{T-t_m^{(j)}} \\exp(-\\beta u)\\,\\mathrm{d}u = \\sum_{m=1}^{n_j} \\left[ -\\frac{1}{\\beta}\\exp(-\\beta u) \\right]_{0}^{T-t_m^{(j)}} $$\n$$ = \\sum_{m=1}^{n_j} \\left( -\\frac{1}{\\beta}\\exp(-\\beta(T-t_m^{(j)})) - \\left(-\\frac{1}{\\beta}\\exp(0)\\right) \\right) = \\frac{1}{\\beta} \\sum_{m=1}^{n_j} (1 - \\exp(-\\beta(T-t_m^{(j)}))) $$\nCombining the two terms, we get the final expression for the gradient component with respect to $\\alpha_{ij}$. We must carefully substitute the full expression for $\\lambda_i(t_k^{(i)})$ in the denominator, using distinct indices to avoid ambiguity.\n$$ \\frac{\\partial \\ell}{\\partial \\alpha_{ij}} = \\sum_{k=1}^{n_i} \\frac{\\sum_{t_m^{(j)}t_k^{(i)}} \\exp(-\\beta(t_k^{(i)}-t_m^{(j)}))}{\\mu_i + \\sum_{l=1}^p \\sum_{t_q^{(l)}t_k^{(i)}} \\alpha_{il} \\exp(-\\beta(t_k^{(i)}-t_q^{(l)}))} - \\frac{1}{\\beta} \\sum_{m=1}^{n_j} \\left(1 - \\exp(-\\beta(T-t_m^{(j)}))\\right) $$\nThese two derived expressions are the components of the gradient of the log-likelihood function.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\sum_{k=1}^{n_i} \\left( \\mu_i + \\sum_{j=1}^p \\sum_{t_m^{(j)}t_k^{(i)}} \\alpha_{ij} \\exp(-\\beta(t_k^{(i)}-t_m^{(j)})) \\right)^{-1} - T  \\sum_{k=1}^{n_i} \\frac{\\sum_{t_m^{(j)}t_k^{(i)}} \\exp(-\\beta(t_k^{(i)}-t_m^{(j)}))}{\\mu_i + \\sum_{l=1}^p \\sum_{t_q^{(l)}t_k^{(i)}} \\alpha_{il} \\exp(-\\beta(t_k^{(i)}-t_q^{(l)}))} - \\frac{1}{\\beta} \\sum_{m=1}^{n_j} \\left(1 - \\exp(-\\beta(T-t_m^{(j)}))\\right) \\end{pmatrix} } $$"
        }
    ]
}