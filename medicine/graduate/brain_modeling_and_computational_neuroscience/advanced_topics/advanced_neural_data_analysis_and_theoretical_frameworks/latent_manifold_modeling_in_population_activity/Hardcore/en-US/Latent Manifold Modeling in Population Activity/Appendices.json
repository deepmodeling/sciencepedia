{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in modeling neural activity is to select an appropriate statistical distribution for spike counts. While the Poisson model is a common starting point, real neural data often exhibits variance that is greater than its mean—a phenomenon known as overdispersion. This practice  guides you through the derivation of the Negative Binomial model, a powerful tool for capturing this extra variability by constructing it from a Poisson-Gamma mixture. This exercise reinforces fundamental probability concepts, such as the law of total variance, and demonstrates how to build more flexible and realistic observation models for population activity.",
            "id": "3993296",
            "problem": "Consider a latent manifold model of neural population activity in which a low-dimensional latent state $z$ drives the time-varying mean firing rate of a single neuron through a link function, producing an observation $y \\in \\{0,1,2,\\dots\\}$ of spike counts in a fixed bin. To capture overdispersion relative to a Poisson model, suppose the observation model is defined by the following hierarchical construction: conditional on an instantaneous rate $\\lambda$, the spike count is distributed as a Poisson random variable $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$, and the rate itself fluctuates due to unobserved latent variability, modeled as a Gamma random variable $\\lambda \\sim \\mathrm{Gamma}(k,\\beta)$ with shape parameter $k>0$ and rate parameter $\\beta>0$. Assume that the latent state $z$ determines the mean of $\\lambda$ through the link $\\mu = \\mathbb{E}[\\lambda]$, where $\\mu$ is any positive function of $z$, such as the exponential mapping $\\mu = \\exp\\!\\big(C^{\\top} z + b\\big)$ commonly used in a Generalized Linear Model (GLM), with $C$ a loading vector and $b$ a bias.\n\nUsing only the definitions of the Poisson and Gamma distributions and the laws of total expectation and total variance, first pick the Gamma rate parameter $\\beta$ in terms of $k$ and $\\mu$ so that $\\mathbb{E}[\\lambda] = \\mu$, and thereby define a negative binomial observation model for $y$ with mean $\\mu$ and dispersion $k$. Then derive the unconditional variance $\\mathrm{Var}[y]$ as a closed-form expression in terms of $\\mu$ and $k$, which quantifies overdispersion relative to the Poisson case. Provide your final answer as the expression for $\\mathrm{Var}[y]$ in closed form in terms of $\\mu$ and $k$. No numerical approximation is required, and no units apply.",
            "solution": "The user wants me to solve the following problem:\nThe problem statement is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- The observation is a spike count $y \\in \\{0, 1, 2, \\dots\\}$.\n- The conditional distribution of the spike count is Poisson: $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$.\n- The instantaneous rate $\\lambda$ is a random variable.\n- The distribution of the rate is Gamma: $\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$.\n- The shape parameter is $k > 0$.\n- The rate parameter is $\\beta > 0$.\n- The mean of the rate $\\lambda$ is denoted by $\\mu$: $\\mathbb{E}[\\lambda] = \\mu$.\n- $\\mu$ is a positive function of a latent state $z$.\n- The tools to be used are the definitions of the Poisson and Gamma distributions and the laws of total expectation and total variance.\n- The first objective is to determine the Gamma rate parameter $\\beta$ in terms of $k$ and $\\mu$.\n- The second objective is to derive the unconditional variance $\\mathrm{Var}[y]$ in terms of $\\mu$ and $k$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in statistics and computational neuroscience. The Poisson-Gamma mixture model is a standard and well-established method for modeling overdispersed count data, such as neural spike counts.\n- **Well-Posed**: The problem is clearly stated and has a unique, derivable solution based on the provided information and standard probability theory.\n- **Objective**: The language is formal, precise, and objective, using standard mathematical terminology.\n- **Completeness and Consistency**: The problem is self-contained. It provides all necessary definitions and constraints to perform the required derivations. It is not underspecified or overconstrained. The instruction to find $\\beta$ is part of the problem's task, not a missing piece of information.\n- **Other Flaws**: The problem does not exhibit any other flaws such as being unrealistic, ill-posed, trivial, or unverifiable. It represents a standard derivation in statistical modeling.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe problem requires a two-part derivation based on a hierarchical model where the spike count $y$ is conditionally Poisson and its rate parameter $\\lambda$ is Gamma-distributed.\n\nFirst, we establish the relationship between the parameters of the Gamma distribution and the mean rate $\\mu$. The Gamma distribution for a random variable $\\lambda$ with shape parameter $k > 0$ and rate parameter $\\beta > 0$, denoted $\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$, has the probability density function $f(\\lambda; k, \\beta) = \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} \\exp(-\\beta \\lambda)$ for $\\lambda > 0$. The mean of this distribution is given by:\n$$\n\\mathbb{E}[\\lambda] = \\frac{k}{\\beta}\n$$\nThe problem specifies that the mean of $\\lambda$ is $\\mu$, i.e., $\\mathbb{E}[\\lambda] = \\mu$. By equating the two expressions for the mean, we can solve for the rate parameter $\\beta$:\n$$\n\\mu = \\frac{k}{\\beta} \\implies \\beta = \\frac{k}{\\mu}\n$$\nThis fulfills the first part of the problem. This reparameterization defines the distribution of the rate $\\lambda$ in terms of its mean $\\mu$ and a shape/dispersion parameter $k$.\n\nNext, we derive the unconditional variance of the spike count, $\\mathrm{Var}[y]$. We use the law of total variance, which states that for any two random variables $y$ and $\\lambda$:\n$$\n\\mathrm{Var}[y] = \\mathbb{E}[\\mathrm{Var}[y \\mid \\lambda]] + \\mathrm{Var}[\\mathbb{E}[y \\mid \\lambda]]\n$$\nTo apply this law, we need the conditional expectation $\\mathbb{E}[y \\mid \\lambda]$ and the conditional variance $\\mathrm{Var}[y \\mid \\lambda]$. The problem states that the conditional distribution of $y$ given $\\lambda$ is a Poisson distribution, $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$. For a Poisson distribution with rate parameter $\\lambda$, the mean and variance are both equal to the rate parameter:\n$$\n\\mathbb{E}[y \\mid \\lambda] = \\lambda\n$$\n$$\n\\mathrm{Var}[y \\mid \\lambda] = \\lambda\n$$\nSubstituting these expressions into the law of total variance formula, we get:\n$$\n\\mathrm{Var}[y] = \\mathbb{E}[\\lambda] + \\mathrm{Var}[\\lambda]\n$$\nWe are given that $\\mathbb{E}[\\lambda] = \\mu$. The remaining term to find is $\\mathrm{Var}[\\lambda]$. The variance of the Gamma distribution $\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$ is given by:\n$$\n\\mathrm{Var}[\\lambda] = \\frac{k}{\\beta^2}\n$$\nNow, we substitute the expression for $\\beta$ that we derived earlier, $\\beta = \\frac{k}{\\mu}$, into the formula for the variance of $\\lambda$:\n$$\n\\mathrm{Var}[\\lambda] = \\frac{k}{\\left(\\frac{k}{\\mu}\\right)^2} = \\frac{k}{\\frac{k^2}{\\mu^2}} = \\frac{k \\mu^2}{k^2} = \\frac{\\mu^2}{k}\n$$\nFinally, we substitute the expressions for $\\mathbb{E}[\\lambda]$ and $\\mathrm{Var}[\\lambda]$ back into the equation for $\\mathrm{Var}[y]$:\n$$\n\\mathrm{Var}[y] = \\mu + \\frac{\\mu^2}{k}\n$$\nThis is the closed-form expression for the unconditional variance of the spike count $y$. This result is characteristic of the Negative Binomial distribution, which is the marginal distribution of $y$ in a Poisson-Gamma mixture model. The variance is composed of a term equal to the mean, $\\mu$, which is the variance of a simple Poisson process, and an additional positive term, $\\frac{\\mu^2}{k}$, which accounts for the overdispersion due to the variability of the underlying rate $\\lambda$. The parameter $k$ is often called the dispersion parameter, where smaller values of $k$ correspond to greater overdispersion. As $k \\to \\infty$, this extra variance term vanishes, and $\\mathrm{Var}[y] \\to \\mu$, recovering the Poisson case.",
            "answer": "$$\n\\boxed{\\mu + \\frac{\\mu^2}{k}}\n$$"
        },
        {
            "introduction": "A major challenge in analyzing population recordings is the inherent variability in the timing of neural events from one trial to the next. This practice  addresses this issue directly by incorporating trial-wise time warping into a Gaussian Process Factor Analysis (GPFA) framework. You will derive a closed-form update rule for an affine time warp parameter, providing a concrete example of how to construct an EM-like algorithm that can jointly infer latent dynamics and align their temporal structure, a crucial step for uncovering stereotyped neural trajectories hidden within variable data.",
            "id": "3993242",
            "problem": "Consider a single-trial instance of Gaussian Process Factor Analysis (GPFA; Gaussian Process Factor Analysis) with trial-wise time warping. A single one-dimensional latent trajectory $x(s)$ is modeled as a zero-mean Gaussian process with covariance function $k(s,s^{\\prime})$ that does not depend on the warp. The observed population activity at discrete acquisition times $\\{t_{i}\\}_{i=1}^{T}$ is an $N$-dimensional vector $y_{i} \\in \\mathbb{R}^{N}$ generated by\n$$\ny_{i} \\;=\\; c\\, x\\!\\left(\\tau(t_{i})\\right) \\;+\\; b \\;+\\; \\varepsilon_{i},\n$$\nwhere $c \\in \\mathbb{R}^{N}$ is a fixed loading vector, $b \\in \\mathbb{R}^{N}$ is a fixed offset vector, $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2} I_{N})$ with known $\\sigma^{2} > 0$, and the trial-specific time warp is the scalar affine scaling $\\tau(t) = a\\, t$ with unknown $a>0$. Let $T \\geq 2$, $N \\geq 1$, and assume that $c \\neq 0$ and the acquisition times satisfy $\\sum_{i=1}^{T} t_{i}^{2} > 0$.\n\nYou will construct an Expectation–Maximization (EM; Expectation–Maximization) like alignment step for the warp. In the E-step, given a current warp $a_{\\text{old}}$, the posterior over $x(s)$ under the GP prior and Gaussian likelihood is Gaussian. For a single generalized EM iteration, adopt the common plug-in approximation in which the expected complete-data log-likelihood is approximated by replacing $x$ with its posterior mean while ignoring the posterior variance contribution’s dependence on $a$. Further assume that, over the time window of the trial, the posterior mean of the latent is well-approximated by a linear function,\n$$\n\\mu(s) \\;\\approx\\; m_{0} \\;+\\; m_{1}\\, s,\n$$\nwith known scalars $m_{0}, m_{1} \\in \\mathbb{R}$ obtained from the E-step at $a_{\\text{old}}$, and $m_{1} \\neq 0$.\n\nUnder these assumptions, the generalized M-step for the warp $a$ maximizes the approximate expected complete-data log-likelihood, which is equivalent to minimizing the surrogate squared-error objective\n$$\n\\mathcal{L}(a) \\;=\\; \\sum_{i=1}^{T} \\left\\|\\, y_{i} \\;-\\; b \\;-\\; c\\, \\mu\\!\\left(a\\, t_{i}\\right) \\,\\right\\|_{2}^{2}.\n$$\n\nDerive a closed-form expression for the unconstrained minimizer $a_{\\text{new}}$ of $\\mathcal{L}(a)$ in terms of $\\{y_{i}\\}_{i=1}^{T}$, $\\{t_{i}\\}_{i=1}^{T}$, $c$, $b$, $m_{0}$, and $m_{1}$. Your final answer must be a single analytic expression. State no intermediate equations in the final answer. Express the final answer without units. If you introduce any additional intermediate symbols in your derivation, eliminate them from the final expression. Assume $\\sum_{i=1}^{T} t_{i}^{2} \\neq 0$, $m_{1} \\neq 0$, and $\\|c\\|_{2} \\neq 0$ so that the expression is well-defined.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It presents a standard optimization task within the recognized framework of Gaussian Process Factor Analysis (GPFA) for neural data analysis. All required data and assumptions are provided, and no contradictions or ambiguities are present. Therefore, the problem is deemed valid.\n\nThe objective is to find the unconstrained minimizer $a_{\\text{new}}$ of the surrogate squared-error objective function $\\mathcal{L}(a)$. The function is given by:\n$$\n\\mathcal{L}(a) = \\sum_{i=1}^{T} \\left\\|\\, y_{i} - b - c\\, \\mu(a\\, t_{i}) \\,\\right\\|_{2}^{2}\n$$\nWe are given the linear approximation for the posterior mean of the latent trajectory:\n$$\n\\mu(s) \\approx m_{0} + m_{1}\\, s\n$$\nSubstituting this into the argument of the norm in $\\mathcal{L}(a)$, we have:\n$$\n\\mu(a\\, t_{i}) = m_{0} + m_{1} (a\\, t_{i}) = m_{0} + m_{1} a t_{i}\n$$\nThe objective function can thus be written as:\n$$\n\\mathcal{L}(a) = \\sum_{i=1}^{T} \\left\\|\\, y_{i} - b - c (m_{0} + m_{1} a t_{i}) \\,\\right\\|_{2}^{2}\n$$\nTo find the value of $a$ that minimizes this function, we can take the derivative of $\\mathcal{L}(a)$ with respect to $a$ and set it to zero. First, let us expand the squared norm. The squared Euclidean norm of a vector $v$ is given by $\\|v\\|_{2}^{2} = v^{T} v$.\nLet the vector inside the norm be $v_i(a)$:\n$$\nv_i(a) = y_{i} - b - c\\,m_{0} - (c\\,m_{1}t_{i}) a\n$$\nThis is an affine function of $a$. Let us define two vectors that do not depend on $a$:\n$$\nu_i = y_{i} - b - c\\,m_{0}\n$$\n$$\nw_i = c\\,m_{1}t_{i}\n$$\nThen $v_i(a) = u_i - a\\,w_i$. The objective function becomes:\n$$\n\\mathcal{L}(a) = \\sum_{i=1}^{T} \\left( u_i - a\\,w_i \\right)^{T} \\left( u_i - a\\,w_i \\right)\n$$\nExpanding the product, we get:\n$$\n\\mathcal{L}(a) = \\sum_{i=1}^{T} \\left( u_i^{T}u_i - a\\,w_i^{T}u_i - a\\,u_i^{T}w_i + a^2 w_i^{T}w_i \\right)\n$$\nSince $u_i^{T}w_i$ is a scalar, it is equal to its transpose, $(u_i^{T}w_i)^{T} = w_i^{T}u_i$. Therefore, the two middle terms are identical.\n$$\n\\mathcal{L}(a) = \\sum_{i=1}^{T} \\left( u_i^{T}u_i - 2a\\,u_i^{T}w_i + a^2 w_i^{T}w_i \\right)\n$$\nThis is a quadratic function of $a$. We can rearrange the summation to make the structure clear:\n$$\n\\mathcal{L}(a) = a^2 \\left( \\sum_{i=1}^{T} w_i^{T}w_i \\right) - 2a \\left( \\sum_{i=1}^{T} u_i^{T}w_i \\right) + \\left( \\sum_{i=1}^{T} u_i^{T}u_i \\right)\n$$\nTo find the minimum, we compute the derivative with respect to $a$ and set it to zero:\n$$\n\\frac{d\\mathcal{L}}{da} = 2a \\left( \\sum_{i=1}^{T} w_i^{T}w_i \\right) - 2 \\left( \\sum_{i=1}^{T} u_i^{T}w_i \\right) = 0\n$$\nSolving for $a$, which we denote $a_{\\text{new}}$:\n$$\na_{\\text{new}} = \\frac{\\sum_{i=1}^{T} u_i^{T}w_i}{\\sum_{i=1}^{T} w_i^{T}w_i}\n$$\nThe second derivative, $\\frac{d^2\\mathcal{L}}{da^2} = 2 \\sum_{i=1}^{T} w_i^{T}w_i = 2 \\sum_{i=1}^{T} \\|w_i\\|_2^2$, is positive, because the problem states $c \\neq 0$, $m_1 \\neq 0$, and $\\sum_{i=1}^{T} t_i^2 > 0$, which ensures that at least one $w_i$ is non-zero. Thus, this value of $a$ corresponds to a minimum.\n\nNow, we substitute the original expressions for $u_i$ and $w_i$ back into the formula for $a_{\\text{new}}$.\n\nThe denominator is:\n$$\n\\sum_{i=1}^{T} w_i^{T}w_i = \\sum_{i=1}^{T} (c\\,m_{1}t_{i})^{T} (c\\,m_{1}t_{i}) = \\sum_{i=1}^{T} m_1^2 t_i^2 c^{T}c = m_1^2 \\|c\\|_{2}^{2} \\sum_{i=1}^{T} t_i^2\n$$\nThe numerator is:\n$$\n\\sum_{i=1}^{T} u_i^{T}w_i = \\sum_{i=1}^{T} (y_{i} - b - c\\,m_{0})^{T} (c\\,m_{1}t_{i})\n$$\nThe terms in the sum are scalars, so we can factor out $m_1$ and $t_i$:\n$$\n\\sum_{i=1}^{T} m_1 t_i (y_{i} - b - c\\,m_{0})^{T} c = m_1 \\sum_{i=1}^{T} t_i \\left( y_{i}^{T}c - b^{T}c - m_{0}c^{T}c \\right)\n$$\nUsing the property that $v^T w = w^T v$ for vectors $v, w$ and $\\|c\\|_2^2 = c^T c$:\n$$\n\\sum_{i=1}^{T} u_i^{T}w_i = m_1 \\sum_{i=1}^{T} t_i \\left( c^{T}y_{i} - c^{T}b - m_{0}\\|c\\|_{2}^{2} \\right)\n$$\nNow we assemble the expression for $a_{\\text{new}}$:\n$$\na_{\\text{new}} = \\frac{m_1 \\sum_{i=1}^{T} t_i \\left( c^{T}y_{i} - c^{T}b - m_{0}\\|c\\|_{2}^{2} \\right)}{m_1^2 \\|c\\|_{2}^{2} \\sum_{i=1}^{T} t_i^2}\n$$\nSince $m_1 \\neq 0$, we can cancel one factor of $m_1$:\n$$\na_{\\text{new}} = \\frac{\\sum_{i=1}^{T} t_i \\left( c^{T}y_{i} - c^{T}b - m_{0}\\|c\\|_{2}^{2} \\right)}{m_1 \\|c\\|_{2}^{2} \\sum_{i=1}^{T} t_i^2}\n$$\nWe can distribute the summation in the numerator to arrive at the final expression. Specifically, we group terms involving $y_i$ and $b$, and separate the term involving $m_0$:\n$$\n\\sum_{i=1}^{T} t_i \\left( c^{T}y_{i} - c^{T}b \\right) - \\sum_{i=1}^{T} t_i m_{0}\\|c\\|_{2}^{2} = c^{T}\\left(\\sum_{i=1}^{T} t_i y_i - b\\sum_{i=1}^{T} t_i\\right) - m_0 \\|c\\|_2^2 \\sum_{i=1}^{T} t_i\n$$\nThus, the final expression for $a_{\\text{new}}$ is:\n$$\na_{\\text{new}} = \\frac{c^{T}\\left(\\sum_{i=1}^{T} t_i y_i - b\\sum_{i=1}^{T} t_i\\right) - m_0 \\|c\\|_2^2 \\sum_{i=1}^{T} t_i}{m_1 \\|c\\|_2^2 \\sum_{i=1}^{T} t_i^2}\n$$",
            "answer": "$$\n\\boxed{\\frac{c^{T}\\left(\\sum_{i=1}^{T} t_i y_i - b\\sum_{i=1}^{T} t_i\\right) - m_0 \\|c\\|_2^2 \\sum_{i=1}^{T} t_i}{m_1 \\|c\\|_2^2 \\sum_{i=1}^{T} t_i^2}}\n$$"
        },
        {
            "introduction": "After developing and fitting several candidate models, a fundamental scientific task is to decide which one provides the most parsimonious and generalizable account of the data. This practice  delves into the theoretical foundations of two of the most widely used tools for this purpose: the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). By deriving these criteria from first principles, you will gain a deeper understanding of how they formalize the trade-off between model fit and complexity, and the distinct statistical philosophies that motivate their respective forms.",
            "id": "3993305",
            "problem": "You are modeling neural population activity from $N$ simultaneously recorded neurons across $n$ statistically independent trials. On each trial $i \\in \\{1,\\dots,n\\}$ you observe a vector $\\mathbf{y}_{i} \\in \\mathbb{R}^{N}$ generated by a latent manifold model of the form\n$$\n\\mathbf{y}_{i} \\sim p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}),\n$$\nwhere $\\mathcal{M}$ denotes a model family defined by continuous latent variables $\\mathbf{x}_{i} \\in \\mathbb{R}^{d}$ that are integrated out,\n$$\np(\\mathbf{y}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}) \\;=\\; \\int p(\\mathbf{y}_{i}, \\mathbf{x}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}) \\, d\\mathbf{x}_{i},\n$$\nand $\\boldsymbol{\\theta} \\in \\mathbb{R}^{k}$ collects all identifiable free parameters after imposing any necessary constraints to remove non-identifiabilities (e.g., rotational symmetries), so that the model is regular in a neighborhood of the maximum likelihood estimator. Assume the trials are independent and identically distributed conditional on $\\boldsymbol{\\theta}$, and let the observed-data log-likelihood be\n$$\n\\ell_{n}(\\boldsymbol{\\theta}) \\;=\\; \\sum_{i=1}^{n} \\ln p(\\mathbf{y}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}).\n$$\nLet $\\widehat{\\boldsymbol{\\theta}}$ denote the maximum likelihood estimator that maximizes $\\ell_{n}(\\boldsymbol{\\theta})$. Suppose the model family $\\mathcal{M}$ is equipped with a prior density $\\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$ that is positive and twice continuously differentiable in a neighborhood of $\\widehat{\\boldsymbol{\\theta}}$, and define the marginal likelihood (model evidence)\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\;=\\; \\int \\bigg[\\prod_{i=1}^{n} p(\\mathbf{y}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M})\\bigg] \\, \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta}.\n$$\nStarting from the following foundational bases:\n- The Laplace method for asymptotically approximating integrals of the form $\\int \\exp\\{n f(\\boldsymbol{\\theta})\\} g(\\boldsymbol{\\theta}) \\, d\\boldsymbol{\\theta}$ around a nondegenerate maximizer of $f$.\n- Asymptotic normality and consistency of maximum likelihood estimators under standard regularity conditions for independent observations.\n- The definition of Kullback-Leibler (KL) divergence and the interpretation of the expected out-of-sample log-likelihood as a proxy for minimizing the expected KL divergence to the data-generating distribution.\n\nDerive, to leading order in $n$, two model selection criteria expressed in terms of the maximized observed-data log-likelihood $\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$ and the parameter dimension $k$:\n1. A criterion obtained by asymptotically approximating $-2 \\ln p(\\mathcal{D}\\,|\\,\\mathcal{M})$ via the Laplace method, discarding additive constants that do not depend on the model $\\mathcal{M}$.\n2. A criterion obtained by asymptotically approximating $-2$ times the expected out-of-sample log-likelihood on an independent replicate of size $n$, using a second-order Taylor expansion and the asymptotic covariance of $\\widehat{\\boldsymbol{\\theta}}$.\n\nInterpret these two criteria as the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC), respectively, specialized to latent variable models using the observed-data likelihood that integrates out the latent variables. Express both criteria in closed form in terms of $\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$, $k$, and $n$, as values to be minimized. Provide your final answer as a two-entry row matrix containing, in order, the analytical expressions for $\\mathrm{AIC}(\\mathcal{M})$ and $\\mathrm{BIC}(\\mathcal{M})$. No numerical evaluation is required; give exact symbolic forms. Do not include any units. If any constants arise that are shared across models and do not affect model comparisons, omit them from your final expressions.",
            "solution": "The problem requires the derivation of two model selection criteria, the Bayesian Information Criterion (BIC) and the Akaike Information Criterion (AIC), for a latent variable model of neural population activity. The derivation will be based on the provided foundational principles and will focus on the observed-data log-likelihood, which marginalizes out the latent variables.\n\n### 1. Derivation of the Bayesian Information Criterion (BIC)\n\nThe first criterion is derived from an asymptotic approximation of the marginal likelihood, or model evidence, $p(\\mathcal{D}\\,|\\,\\mathcal{M})$. The marginal likelihood is defined as:\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) = \\int p(\\mathcal{D}\\,|\\,\\boldsymbol{\\theta},\\mathcal{M}) \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta} = \\int \\exp\\{\\ell_{n}(\\boldsymbol{\\theta})\\} \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta}\n$$\nwhere $\\ell_{n}(\\boldsymbol{\\theta})$ is the observed-data log-likelihood and $\\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$ is the prior over the $k$-dimensional parameter vector $\\boldsymbol{\\theta}$.\n\nWe will use the Laplace method to approximate this integral for large $n$. The method approximates the integral by concentrating the probability mass around the mode of the posterior distribution. For large $n$, the likelihood term $\\ell_{n}(\\boldsymbol{\\theta})$ dominates the prior term $\\ln \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$, so the posterior mode is asymptotically close to the maximum likelihood estimator (MLE) $\\widehat{\\boldsymbol{\\theta}}$, which maximizes $\\ell_{n}(\\boldsymbol{\\theta})$.\n\nLet's define $h(\\boldsymbol{\\theta}) = \\ell_{n}(\\boldsymbol{\\theta}) + \\ln \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$. We perform a second-order Taylor expansion of $h(\\boldsymbol{\\theta})$ around the MLE $\\widehat{\\boldsymbol{\\theta}}$. A more precise expansion is around the posterior mode $\\tilde{\\boldsymbol{\\theta}} = \\arg\\max_{\\boldsymbol{\\theta}} h(\\boldsymbol{\\theta})$, but for large $n$, $\\tilde{\\boldsymbol{\\theta}} \\approx \\widehat{\\boldsymbol{\\theta}}$ and the leading-order terms are identical. Let's expand $\\ell_n(\\boldsymbol{\\theta})$ around $\\widehat{\\boldsymbol{\\theta}}$:\n$$\n\\ell_{n}(\\boldsymbol{\\theta}) \\approx \\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} \\nabla\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} \\nabla^2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})\n$$\nBy definition of the MLE, $\\nabla\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) = \\mathbf{0}$. So, the expansion simplifies to:\n$$\n\\ell_{n}(\\boldsymbol{\\theta}) \\approx \\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) - \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} J_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})\n$$\nwhere $J_n(\\widehat{\\boldsymbol{\\theta}}) = -\\nabla^2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$ is the observed Fisher information matrix, which is positive definite at the maximum.\n\nSubstituting this into the integral for $p(\\mathcal{D}\\,|\\,\\mathcal{M})$ and approximating the smooth prior term $\\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$ by its value at the peak, $\\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M})$, we get:\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\int \\exp\\left\\{ \\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) - \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} J_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}}) \\right\\} \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta}\n$$\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\exp\\{\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})\\} \\, \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\int \\exp\\left\\{ - \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} J_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}}) \\right\\} d\\boldsymbol{\\theta}\n$$\nThe integral is of a multivariate Gaussian probability density function (unnormalized) with mean $\\widehat{\\boldsymbol{\\theta}}$ and covariance matrix $J_{n}(\\widehat{\\boldsymbol{\\theta}})^{-1}$. The value of this integral is $(2\\pi)^{k/2} \\det(J_{n}(\\widehat{\\boldsymbol{\\theta}}))^{-1/2}$.\nTherefore,\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\exp\\{\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})\\} \\, \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\, (2\\pi)^{k/2} \\det(J_{n}(\\widehat{\\boldsymbol{\\theta}}))^{-1/2}\n$$\nBy the law of large numbers, for i.i.d. data, the observed information $J_n(\\widehat{\\boldsymbol{\\theta}})$ scales linearly with $n$. That is, $J_n(\\widehat{\\boldsymbol{\\theta}}) = O(n)$. We can write $J_n(\\widehat{\\boldsymbol{\\theta}}) \\approx n I(\\boldsymbol{\\theta}_0)$, where $I(\\boldsymbol{\\theta}_0)$ is the Fisher information for a single observation evaluated at the true parameter value $\\boldsymbol{\\theta}_0$.\nThus, $\\det(J_n(\\widehat{\\boldsymbol{\\theta}})) \\approx \\det(nI(\\boldsymbol{\\theta}_0)) = n^k \\det(I(\\boldsymbol{\\theta}_0))$. Substituting this into the approximation:\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\exp\\{\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})\\} \\, \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\, (2\\pi)^{k/2} (n^k \\det(I(\\boldsymbol{\\theta}_0)))^{-1/2}\n$$\nNow, we compute $-2 \\ln p(\\mathcal{D}\\,|\\,\\mathcal{M})$:\n$$\n-2 \\ln p(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) - 2\\ln \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) - k\\ln(2\\pi) + k\\ln n + \\ln \\det(I(\\boldsymbol{\\theta}_0))\n$$\nThe BIC is obtained by retaining only the terms that grow with the sample size $n$. The term $\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$ is $O(n)$, and the term $k\\ln n$ is $O(\\ln n)$. All other terms, such as those involving the prior and the Fisher information, are $O(1)$ and are asymptotically negligible. Discarding these $O(1)$ terms yields the BIC:\n$$\n\\mathrm{BIC}(\\mathcal{M}) = -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + k \\ln n\n$$\nThis criterion is to be minimized for model selection. It represents an asymptotic approximation to minus twice the log marginal evidence. It penalizes the model's lack of fit (first term) and its complexity (second term).\n\n### 2. Derivation of the Akaike Information Criterion (AIC)\n\nThe second criterion is an approximation of $-2$ times the expected out-of-sample log-likelihood. The goal is to estimate how well the fitted model will predict new, unseen data from the same generating process. Let $p_0(\\mathbf{y})$ be the true but unknown data-generating distribution. Let $\\mathcal{D}' = \\{\\mathbf{y}'_1, \\dots, \\mathbf{y}'_n\\}$ be an independent replicate dataset. The expected out-of-sample log-likelihood for a model fitted on data $\\mathcal{D}$ is:\n$$\nC = E_{\\mathcal{D}'}[\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}))] = E_{\\mathcal{D}'}\\left[ \\sum_{i=1}^{n} \\ln p(\\mathbf{y}'_i \\,|\\, \\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}), \\mathcal{M}) \\right] = n \\int p_0(\\mathbf{y}) \\ln p(\\mathbf{y} \\,|\\, \\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}), \\mathcal{M}) d\\mathbf{y}\n$$\nThis quantity depends on the specific training dataset $\\mathcal{D}$ through $\\widehat{\\boldsymbol{\\theta}}(\\mathcal{D})$. We desire an estimate of its expectation over all possible training sets $\\mathcal{D}$:\n$$\nE_{\\mathcal{D}}[C] = E_{\\mathcal{D}} \\left[ n \\int p_0(\\mathbf{y}) \\ln p(\\mathbf{y} \\,|\\, \\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}), \\mathcal{M}) d\\mathbf{y} \\right]\n$$\nLet $\\boldsymbol{\\theta}_0$ be the parameter value that minimizes the KL divergence between the true distribution $p_0(\\mathbf{y})$ and the model family $p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta},\\mathcal{M})$. Under the assumption that the model family is correctly specified, $p_0(\\mathbf{y}) = p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta}_0, \\mathcal{M})$.\nLet's define $L(\\boldsymbol{\\theta}) = E_{\\mathbf{y}\\sim p_0}[\\ln p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta},\\mathcal{M})]$. The quantity we want to estimate is $E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})]$.\nUsing a second-order Taylor expansion of $L(\\widehat{\\boldsymbol{\\theta}})$ around $\\boldsymbol{\\theta}_0$:\n$$\nL(\\widehat{\\boldsymbol{\\theta}}) \\approx L(\\boldsymbol{\\theta}_0) + (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\nabla L(\\boldsymbol{\\theta}_0) + \\frac{1}{2}(\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\nabla^2 L(\\boldsymbol{\\theta}_0) (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)\n$$\nBy definition, $\\boldsymbol{\\theta}_0$ maximizes $L(\\boldsymbol{\\theta})$, so $\\nabla L(\\boldsymbol{\\theta}_0) = \\mathbf{0}$. Also, $\\nabla^2 L(\\boldsymbol{\\theta}_0) = E_{\\mathbf{y}\\sim p_0} [\\nabla^2 \\ln p(\\mathbf{y} | \\boldsymbol{\\theta}_0)] = -I(\\boldsymbol{\\theta}_0)$, where $I(\\boldsymbol{\\theta}_0)$ is the Fisher information matrix for one observation.\nTaking the expectation over $\\mathcal{D}$:\n$$\nE_{\\mathcal{D}}[L(\\widehat{\\boldsymbol{\\theta}})] \\approx L(\\boldsymbol{\\theta}_0) + \\frac{1}{2} E_{\\mathcal{D}}\\left[ (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T (-I(\\boldsymbol{\\theta}_0)) (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0) \\right]\n$$\nUsing the trace identity and linearity of expectation, this becomes:\n$$\nE_{\\mathcal{D}}[L(\\widehat{\\boldsymbol{\\theta}})] \\approx L(\\boldsymbol{\\theta}_0) - \\frac{1}{2} \\mathrm{Tr}\\left( I(\\boldsymbol{\\theta}_0) E_{\\mathcal{D}}\\left[ (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)(\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\right] \\right)\n$$\nUnder the regularity conditions stated, the MLE $\\widehat{\\boldsymbol{\\theta}}$ is asymptotically normal, and its asymptotic covariance is $E_{\\mathcal{D}}\\left[ (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)(\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\right] \\approx \\frac{1}{n} I(\\boldsymbol{\\theta}_0)^{-1}$.\n$$\nE_{\\mathcal{D}}[L(\\widehat{\\boldsymbol{\\theta}})] \\approx L(\\boldsymbol{\\theta}_0) - \\frac{1}{2} \\mathrm{Tr}\\left( I(\\boldsymbol{\\theta}_0) \\frac{1}{n} I(\\boldsymbol{\\theta}_0)^{-1} \\right) = L(\\boldsymbol{\\theta}_0) - \\frac{1}{2n} \\mathrm{Tr}(I_k) = L(\\boldsymbol{\\theta}_0) - \\frac{k}{2n}\n$$\nSo, the expected out-of-sample log-likelihood is $E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})] \\approx n L(\\boldsymbol{\\theta}_0) - \\frac{k}{2}$.\n\nNow we relate this to the in-sample maximized log-likelihood, $\\ell_n(\\widehat{\\boldsymbol{\\theta}})$. Let's find its expectation. A similar expansion for $\\frac{1}{n}\\ell_n(\\widehat{\\boldsymbol{\\theta}})$ shows that $E_{\\mathcal{D}}[\\ell_n(\\widehat{\\boldsymbol{\\theta}})]$ has an upward bias:\n$$\nE_{\\mathcal{D}}[\\ell_n(\\widehat{\\boldsymbol{\\theta}})] \\approx n L(\\boldsymbol{\\theta}_0) + \\frac{k}{2}\n$$\nThe difference $E_{\\mathcal{D}}[\\ell_n(\\widehat{\\boldsymbol{\\theta}})] - E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})] \\approx k$ is the \"optimism\" of the in-sample log-likelihood. This suggests that $\\ell_n(\\widehat{\\boldsymbol{\\theta}}) - k$ is an approximately unbiased estimator of the expected out-of-sample log-likelihood $E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})]$.\nThe Akaike Information Criterion is defined as $-2$ times this corrected log-likelihood:\n$$\n\\mathrm{AIC}(\\mathcal{M}) = -2(\\ell_n(\\widehat{\\boldsymbol{\\theta}}) - k) = -2\\ell_n(\\widehat{\\boldsymbol{\\theta}}) + 2k\n$$\nThis criterion is also to be minimized for model selection. It provides an estimate of the expected prediction error on new data, penalizing the in-sample fit by a term proportional to the number of parameters.\n\nThe key insight is that both derivations, while originating from different philosophical standpoints (Bayesian vs. frequentist), depend only on the asymptotic properties of the observed-data log-likelihood $\\ell_n(\\boldsymbol{\\theta})$ and its maximizer $\\widehat{\\boldsymbol{\\theta}}$. The fact that $\\ell_n(\\boldsymbol{\\theta})$ is derived from a latent variable model by marginalization is handled by the problem's assumption that $\\ell_n(\\boldsymbol{\\theta})$ still satisfies the necessary regularity conditions for the MLE asymptotics to hold, with $k$ being the number of identifiable parameters.\n\nIn summary, the two criteria are:\n1. $\\mathrm{BIC}(\\mathcal{M}) = -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + k \\ln n$\n2. $\\mathrm{AIC}(\\mathcal{M}) = -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + 2k$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + 2k & -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + k \\ln n \\end{pmatrix}}\n$$"
        }
    ]
}