{
    "hands_on_practices": [
        {
            "introduction": "在为神经群体活动建立潜变量模型时，一个基础步骤是选择一个合适的观测模型来描述单个神经元的发放计数。虽然泊松分布是一个常见的起点，但真实的神经数据往往表现出比泊松模型所能解释的更大的变异性，即“超离散”现象。这个练习将指导您从第一性原理出发，通过构建一个分层的泊松-伽马混合模型来推导出负二项分布，这是一种能够捕捉超离散的更灵活的工具。通过这个实践，您将掌握量化神经发放变异性的关键方法，这是构建更真实、更准确的神经编码模型的重要一步。",
            "id": "3993296",
            "problem": "考虑一个神经元群体活动的潜流形模型，其中低维潜状态 $z$ 通过一个连接函数驱动单个神经元随时间变化的平均发放率，在固定的时间窗内产生脉冲计数观测值 $y \\in \\{0,1,2,\\dots\\}$。为了捕捉相对于泊松模型的过离散现象，假设观测模型由以下分层结构定义：在给定瞬时率 $\\lambda$ 的条件下，脉冲计数服从泊松随机变量 $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$，而率本身由于未观测到的潜在变异性而波动，该波动被建模为具有形状参数 $k>0$ 和率参数 $\\beta>0$ 的伽马随机变量 $\\lambda \\sim \\mathrm{Gamma}(k,\\beta)$。假设潜状态 $z$ 通过连接 $\\mu = \\mathbb{E}[\\lambda]$ 决定 $\\lambda$ 的均值，其中 $\\mu$ 是 $z$ 的任意正函数，例如广义线性模型 (GLM) 中常用的指数映射 $\\mu = \\exp\\!\\big(C^{\\top} z + b\\big)$，其中 $C$ 是载荷向量，$b$ 是偏置。\n\n仅使用泊松分布和伽马分布的定义以及全期望定律和全方差定律，首先选择伽马分布的率参数 $\\beta$（用 $k$ 和 $\\mu$ 表示），使得 $\\mathbb{E}[\\lambda] = \\mu$，从而为 $y$ 定义一个均值为 $\\mu$、离散度为 $k$ 的负二项观测模型。然后，推导无条件方差 $\\mathrm{Var}[y]$ 的闭式表达式（用 $\\mu$ 和 $k$ 表示），该表达式量化了相对于泊松情况的过离散程度。以 $\\mathrm{Var}[y]$ 关于 $\\mu$ 和 $k$ 的闭式表达式的形式提供您的最终答案。不需要数值近似，也没有单位。",
            "solution": "用户希望我解决以下问题：\n首先根据指定标准验证问题陈述。\n\n### 第1步：提取已知条件\n- 观测值是脉冲计数 $y \\in \\{0, 1, 2, \\dots\\}$。\n- 脉冲计数的条件分布是泊松分布：$y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$。\n- 瞬时率 $\\lambda$ 是一个随机变量。\n- 率的分布是伽马分布：$\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$。\n- 形状参数为 $k > 0$。\n- 率参数为 $\\beta > 0$。\n- 率 $\\lambda$ 的均值用 $\\mu$ 表示：$\\mathbb{E}[\\lambda] = \\mu$。\n- $\\mu$ 是潜状态 $z$ 的一个正函数。\n- 使用的工具是泊松分布和伽马分布的定义，以及全期望定律和全方差定律。\n- 第一个目标是确定伽马分布的率参数 $\\beta$（用 $k$ 和 $\\mu$ 表示）。\n- 第二个目标是推导无条件方差 $\\mathrm{Var}[y]$（用 $\\mu$ 和 $k$ 表示）。\n\n### 第2步：使用提取的已知条件进行验证\n- **科学依据**：该问题在统计学和计算神经科学中有充分的依据。泊松-伽马混合模型是为过离散计数数据（如神经脉冲计数）建模的一种标准且成熟的方法。\n- **适定性**：问题陈述清晰，并且基于所提供的信息和标准概率论，有一个唯一的、可推导的解。\n- **客观性**：语言正式、精确、客观，使用了标准的数学术语。\n- **完整性与一致性**：问题是自洽的。它提供了执行所需推导的所有必要定义和约束。它没有欠定或过约束。找出 $\\beta$ 的指令是问题任务的一部分，而不是缺失信息。\n- **其他缺陷**：该问题没有表现出任何其他缺陷，例如不切实际、不适定、过于简单或无法验证。它代表了统计建模中的一个标准推导。\n\n### 第3步：结论与行动\n问题是有效的。将提供完整的解答。\n\n### 解题推导\n\n该问题要求基于一个分层模型进行两部分推导，其中脉冲计数 $y$ 条件性地服从泊松分布，其率参数 $\\lambda$ 服从伽马分布。\n\n首先，我们建立伽马分布参数与平均率 $\\mu$ 之间的关系。对于一个形状参数为 $k > 0$、率参数为 $\\beta > 0$ 的随机变量 $\\lambda$，其伽马分布记为 $\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$，概率密度函数为 $f(\\lambda; k, \\beta) = \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} \\exp(-\\beta \\lambda)$（对于 $\\lambda > 0$）。该分布的均值为：\n$$\n\\mathbb{E}[\\lambda] = \\frac{k}{\\beta}\n$$\n问题指明 $\\lambda$ 的均值为 $\\mu$，即 $\\mathbb{E}[\\lambda] = \\mu$。通过令两个均值表达式相等，我们可以解出率参数 $\\beta$：\n$$\n\\mu = \\frac{k}{\\beta} \\implies \\beta = \\frac{k}{\\mu}\n$$\n这完成了问题的第一部分。这种重新参数化的方法根据率 $\\lambda$ 的均值 $\\mu$ 和一个形状/离散参数 $k$ 来定义其分布。\n\n接下来，我们推导脉冲计数的无条件方差 $\\mathrm{Var}[y]$。我们使用全方差定律，该定律指出对于任意两个随机变量 $y$ 和 $\\lambda$：\n$$\n\\mathrm{Var}[y] = \\mathbb{E}[\\mathrm{Var}[y \\mid \\lambda]] + \\mathrm{Var}[\\mathbb{E}[y \\mid \\lambda]]\n$$\n要应用此定律，我们需要条件期望 $\\mathbb{E}[y \\mid \\lambda]$ 和条件方差 $\\mathrm{Var}[y \\mid \\lambda]$。问题陈述，给定 $\\lambda$ 时 $y$ 的条件分布是泊松分布，$y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$。对于率参数为 $\\lambda$ 的泊松分布，其均值和方差都等于率参数：\n$$\n\\mathbb{E}[y \\mid \\lambda] = \\lambda\n$$\n$$\n\\mathrm{Var}[y \\mid \\lambda] = \\lambda\n$$\n将这些表达式代入全方差定律公式，我们得到：\n$$\n\\mathrm{Var}[y] = \\mathbb{E}[\\lambda] + \\mathrm{Var}[\\lambda]\n$$\n我们已知 $\\mathbb{E}[\\lambda] = \\mu$。剩下需要求解的项是 $\\mathrm{Var}[\\lambda]$。伽马分布 $\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$ 的方差由下式给出：\n$$\n\\mathrm{Var}[\\lambda] = \\frac{k}{\\beta^2}\n$$\n现在，我们将先前推导出的 $\\beta$ 的表达式 $\\beta = \\frac{k}{\\mu}$ 代入 $\\lambda$ 的方差公式中：\n$$\n\\mathrm{Var}[\\lambda] = \\frac{k}{\\left(\\frac{k}{\\mu}\\right)^2} = \\frac{k}{\\frac{k^2}{\\mu^2}} = \\frac{k \\mu^2}{k^2} = \\frac{\\mu^2}{k}\n$$\n最后，我们将 $\\mathbb{E}[\\lambda]$ 和 $\\mathrm{Var}[\\lambda]$ 的表达式代回 $\\mathrm{Var}[y]$ 的方程中：\n$$\n\\mathrm{Var}[y] = \\mu + \\frac{\\mu^2}{k}\n$$\n这就是脉冲计数 $y$ 的无条件方差的闭式表达式。这个结果是负二项分布的特征，而负二项分布是泊松-伽马混合模型中 $y$ 的边际分布。该方差由一个等于均值 $\\mu$ 的项（即简单泊松过程的方差）和一个额外的正项 $\\frac{\\mu^2}{k}$ 组成，后者解释了由于基础率 $\\lambda$ 的变异性而导致的过离散。参数 $k$ 通常被称为离散参数，其中较小的 $k$ 值对应于较大的过离散。当 $k \\to \\infty$ 时，这个额外的方差项消失，$\\mathrm{Var}[y] \\to \\mu$，恢复为泊松情况。",
            "answer": "$$\n\\boxed{\\mu + \\frac{\\mu^2}{k}}\n$$"
        },
        {
            "introduction": "将低维潜变量映射到神经元发放率需要一个非负的连接函数。这个函数的选择不仅仅是技术细节，它对模型的数学特性和优化过程有着深远的影响。本练习将深入比较两种常用的连接函数：指数函数和softplus函数。通过分析它们在潜变量后验最大化（MAP）估计中对似然函数曲率的影响，您将理解这些选择如何决定优化问题的凹性，从而影响算法的稳定性和收敛性。",
            "id": "3993252",
            "problem": "考虑一个用于神经元脉冲计数的群体泊松观测模型，其中在时间 $t$ 的潜低维状态为 $x_t \\in \\mathbb{R}^k$，通过 $z_t = C x_t + d$（其中 $C \\in \\mathbb{R}^{n \\times k}$ 且 $d \\in \\mathbb{R}^n$）线性映射到每个神经元的驱动 $z_t \\in \\mathbb{R}^n$。观测到的脉冲计数为 $y_t \\in \\mathbb{N}^n$，模型将其视为条件独立的泊松随机变量，其平均速率 $\\lambda_t \\in \\mathbb{R}_+^n$ 由一个单调连接函数 $g$ 逐元素地应用于 $z_t$ 给出，即 $\\lambda_t = g(z_t)$。为确保速率为正，我们感兴趣的两个连接函数是指数连接 $g_{\\exp}(z) = \\exp(z)$ 和 softplus 连接 $g_{\\mathrm{sp}}(z) = \\log\\!\\left(1 + \\exp(z)\\right)$。\n\n假设一个高斯潜轨迹先验，它是关于 $x_{1:T}$ 的二次函数（例如，线性动力系统先验），因此先验对数密度在最大后验（MAP）目标函数中贡献了一个负定二次惩罚项。在期望最大化（EM）算法中，潜变量 MAP E-步（E-step）最大化了关于 $x_{1:T}$ 的后验对数密度，该密度（在忽略加法常数的情况下）可以写成泊松对数似然项 $y_{t,i} \\log \\lambda_{t,i} - \\lambda_{t,i}$ 对所有时间和神经元的总和，再加上先验二次项。\n\n仅使用上述基本定义，比较指数连接和 softplus 连接在潜变量 MAP E-步中的曲率和优化行为。下列哪些陈述是正确的？\n\nA. 对于指数连接 $g_{\\exp}$，观测对数似然作为线性驱动 $z$ 的函数是严格凹的，其负二阶导数与 $\\exp(z)$ 成比例。当与线性函数 $z(x)$ 复合，并与二次高斯先验相加时，关于 $x$ 的完整 MAP 目标函数是严格凹的，保证了唯一的全局最大值点。\n\nB. 对于 softplus 连接 $g_{\\mathrm{sp}}$，观测对数似然作为线性驱动 $z$ 的函数对于所有非负计数和所有实数 $z$ 都是凹的。因此，当与线性函数 $z(x)$ 复合，并与二次高斯先验相加时，关于 $x$ 的完整 MAP 目标函数是凹的，确保了唯一的全局最大值点。\n\nC. 相对于指数连接，softplus 连接缓和了曲率：对于固定的观测计数 $y$，在 $g_{\\mathrm{sp}}$ 下，观测对数似然关于 $z$ 的二阶导数的绝对值在 $z$ 上一致有界，并且关于 $z$ 的梯度对于所有 $z$ 都有下界 $-1$。相反，在 $g_{\\exp}$ 下，二阶导数的绝对值随着 $z \\to +\\infty$ 无界增长。\n\nD. 在具有线性 $z(x)$ 的潜变量 MAP E-步中，由于观测模型是泊松模型，两种连接在 $x$ 中产生相同的费雪信息（Fisher information）；因此，在 $g_{\\exp}$ 和 $g_{\\mathrm{sp}}$ 下，观测对数似然关于 $x$ 的海森矩阵（Hessian）是相同的。\n\nE. 从指数连接切换到 softplus 连接必然会破坏后验分布的对数凹性，从而使围绕潜变量 MAP 估计的拉普拉斯近似（Laplace approximation）无效。\n\n选择所有适用项。",
            "solution": "用户希望我验证问题陈述，如果有效，则通过分析所提供的每个选项来解决问题。\n\n### 步骤 1：提取已知条件\n问题描述了一个具有潜变量结构的泊松广义线性模型（GLM）。\n-   时间 $t$ 的潜状态：$x_t \\in \\mathbb{R}^k$。\n-   $n$ 个神经元的线性驱动：$z_t = C x_t + d$，其中 $C \\in \\mathbb{R}^{n \\times k}$ 且 $d \\in \\mathbb{R}^n$。\n-   观测到的脉冲计数：$y_t \\in \\mathbb{N}^n$。\n-   观测模型：$y_{t,i}$ 是在给定速率 $\\lambda_{t,i}$ 的情况下的条件独立泊松变量。\n-   连接函数：速率 $\\lambda_t$ 通过一个单调连接函数 $g$ 逐元素地与驱动 $z_t$ 相关联，即 $\\lambda_t = g(z_t)$。\n-   给出了两个具体的连接函数：\n    1.  指数连接：$g_{\\exp}(z) = \\exp(z)$。\n    2.  Softplus 连接：$g_{\\mathrm{sp}}(z) = \\log(1 + \\exp(z))$。\n-   潜先验：潜轨迹 $x_{1:T}$ 的先验是高斯分布，其对数密度在 MAP 目标函数中提供了一个负定二次惩罚项。这意味着对数先验是关于 $x_{1:T}$ 的严格凹函数。\n-   MAP 目标函数：在 E-步中要最大化的目标是潜变量的对数后验，它（在忽略常数的情况下）是对数似然和对数先验之和。\n-   泊松对数似然：对于单个观测 $y_{t,i}$，速率为 $\\lambda_{t,i}$，其对数似然为 $\\ell_{t,i} = y_{t,i} \\log \\lambda_{t,i} - \\lambda_{t,i}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学依据**：所描述的模型是计算神经科学中使用的标准潜变量模型，通常称为具有潜在动力学的泊松 GLM 或状态空间 GLM。指数函数和 softplus 函数是确保正速率的常用连接函数。数学表述是标准且合理的。\n2.  **良态性**：该问题是一个定义明确的数学问题，涉及从给定模型导出的目标函数的性质（特别是凹性和导数）。它要求在两个定义明确的连接函数之间进行比较。可以推导出唯一的答案。\n3.  **客观性**：语言精确，术语有数学定义。没有主观性。\n4.  **不完整或矛盾的设置**：问题提供了分析对数似然和 MAP 目标函数性质所需的所有定义。先验的负定二次惩罚假设是提供的关键信息，并且与高斯先验一致。\n5.  **不切实际或不可行**：该设置是一个标准的统计模型，在计算上和科学上都是可行的。\n\n### 步骤 3：结论和行动\n问题陈述是有效的。它在科学上是合理的、良态的、客观的并且是自洽的。我将继续推导解决方案并评估各个选项。\n\n### 解题推导\n\n分析的核心是单个神经元的对数似然 $\\ell(z) = y \\log g(z) - g(z)$ 作为驱动 $z$ 的函数的性质。总 MAP 目标函数（对数后验）涉及将这些项对所有神经元和时间点求和，与线性映射 $z_t(x_t) = C x_t + d$ 复合，并加上严格凹的对数先验。\n\nMAP 目标函数的凹性是关键。凹函数的和是凹函数。凹函数与严格凹函数的和是严格凹函数。严格凹函数有唯一的全局最大值。\n\n我们需要 $\\ell(z)$ 关于 $z$ 的一阶和二阶导数：\n-   $\\frac{d\\ell}{dz} = \\frac{y}{g(z)} g'(z) - g'(z) = g'(z) \\left( \\frac{y}{g(z)} - 1 \\right)$\n-   $\\frac{d^2\\ell}{dz^2} = y \\left( \\frac{g''(z)g(z) - (g'(z))^2}{g(z)^2} \\right) - g''(z)$\n\n现在，我们为每个连接函数分析这一点。\n\n**情况 1：指数连接, $g_{\\exp}(z) = \\exp(z)$**\n-   $g'(z) = \\exp(z)$\n-   $g''(z) = \\exp(z)$\n-   对数似然的二阶导数为：\n    $$ \\frac{d^2\\ell}{dz^2} = y \\left( \\frac{\\exp(z)\\exp(z) - (\\exp(z))^2}{(\\exp(z))^2} \\right) - \\exp(z) = y \\cdot 0 - \\exp(z) = -\\exp(z) $$\n-   由于对所有 $z \\in \\mathbb{R}$ 都有 $\\exp(z) > 0$，所以 $\\frac{d^2\\ell}{dz^2}$ 总是严格为负。因此，$\\ell(z)$ 是 $z$ 的严格凹函数。\n\n**情况 2：Softplus 连接, $g_{\\mathrm{sp}}(z) = \\log(1 + \\exp(z))$**\n-   $g'(z) = \\frac{\\exp(z)}{1 + \\exp(z)} = \\sigma(z)$（逻辑斯谛 S 型函数）。\n-   $g''(z) = \\frac{\\exp(z)(1+\\exp(z)) - \\exp(z)\\exp(z)}{(1+\\exp(z))^2} = \\frac{\\exp(z)}{(1+\\exp(z))^2} = \\sigma(z)(1-\\sigma(z))$。\n-   注意，对所有 $z \\in \\mathbb{R}$ 都有 $g'(z) > 0$ 和 $g''(z) > 0$。\n-   让我们分析项 $g''(z)g(z) - (g'(z))^2$ 的符号：\n    $$ g''(z)g(z) - (g'(z))^2 = \\frac{e^z}{(1+e^z)^2} \\log(1+e^z) - \\left(\\frac{e^z}{1+e^z}\\right)^2 = \\frac{e^z}{(1+e^z)^2} \\left[ \\log(1+e^z) - \\frac{e^z}{1+e^z} \\right] $$\n    对于任何 $u>0$，我们知道 $\\log(1+u) > \\frac{u}{1+u}$。令 $u = e^z$，我们有 $\\log(1+e^z) > \\frac{e^z}{1+e^z}$。因此，方括号中的项为正。由于 $\\frac{e^z}{(1+e^z)^2} > 0$，我们有 $g''(z)g(z) - (g'(z))^2 > 0$。\n-   现在考虑对数似然的完整二阶导数：\n    $$ \\frac{d^2\\ell}{dz^2} = \\underbrace{y}_{\\ge 0} \\underbrace{\\left( \\frac{g''(z)g(z) - (g'(z))^2}{g(z)^2} \\right)}_{>0} - \\underbrace{g''(z)}_{>0} $$\n    当 $y=0$ 时，$\\frac{d^2\\ell}{dz^2} = -g''(z)  0$。当 $y>0$ 时，该表达式的符号并不立即明确是负的。然而，泊松GLM的对数似然对于任何典则连接函数（其逆函数是累积量函数的导数）都是凹的。对于泊松分布，典则连接是对数函数，其逆是指数函数。Softplus 不是典则连接。但可以证明泊松-softplus GLM的对数似然是凹的。\n    一个更直接的证明：$\\frac{d^2\\ell}{dz^2} = y \\frac{g''}{g} - y(\\frac{g'}{g})^2 - g'' = (y/g-1)g'' - y(g'/g)^2$。这也不明显。\n    让我们回到 $\\frac{d^2\\ell}{dz^2} = y \\left( \\frac{g''g - (g')^2}{g^2} \\right) - g''(z)$。虽然 $g''g - (g')^2 > 0$，但整个表达式是否为负取决于 $y$。一个更稳健的证明是，泊松分布属于指数族，其对数似然对于其自然参数是凹的。$z$ 是自然参数的仿射变换，如果连接函数是典则连接（对数），则对数似然对 $z$ 是凹的。Softplus不是典则连接，但GLM文献中的一个标准结果是，只要 $g'' \\ge 0$ (log-concave $g$)，对数似然就是凹的。Softplus的二阶导数大于0，所以它不是log-concave，这个简单论证不适用。\n    让我们重新检查解决方案中的推导。它声称 $\\log(1+u)  u$ (这是正确的)，所以 $g''g-(g')^2  0$ (这是错误的，因为它是 $\\log(1+e^z) - \\frac{e^z}{1+e^z}$，不是 $\\log(1+e^z) - e^z$)。\n    实际上，对于 $u>0$, $\\log(1+u) > \\frac{u}{1+u}$。所以 $g''g-(g')^2 > 0$ 是正确的。\n    $\\frac{d^2\\ell}{dz^2} = y \\frac{g''g - (g')^2}{g^2} - g'' = \\frac{y(g''g - (g')^2) - g^2 g''}{g^2}$。\n    这个函数的凹性是一个标准但非平凡的结果。为了保持简约，我们接受这个标准结果，即它确实是凹的。对于任何 $y\\ge 0$ 和 $z \\in \\mathbb{R}$，$\\ell(z)$ 是凹的。事实上，它是严格凹的。\n\n**MAP 目标函数的凹性**\n总对数似然为 $\\mathcal{L}(x_{1:T}) = \\sum_t \\sum_i \\ell_{t,i}(z_{t,i}(x_t))$。每一项 $\\ell_{t,i}$ 是一个 $z$ 的严格凹函数与一个从 $x_t$ 到 $z_{t,i}$ 的仿射映射的复合。这种复合产生一个关于 $x_t$ 的凹函数。这些凹函数的和 $\\mathcal{L}(x_{1:T})$ 是关于 $x_{1:T}$ 的凹函数。\nMAP 目标函数是 $J(x_{1:T}) = \\mathcal{L}(x_{1:T}) + \\log p(x_{1:T})$。\n已知对数先验 $\\log p(x_{1:T})$ 是一个严格凹函数。一个凹函数（$\\mathcal{L}$）和一个严格凹函数（$\\log p$）的和是严格凹的。\n因此，对于两种连接函数，MAP 目标函数都是严格凹的，这保证了它有一个唯一的全局最大值点。\n\n### 逐项分析\n\n**A. 对于指数连接 $g_{\\exp}$，观测对数似然作为线性驱动 $z$ 的函数是严格凹的，其负二阶导数与 $\\exp(z)$ 成比例。当与线性函数 $z(x)$ 复合，并与二次高斯先验相加时，关于 $x$ 的完整 MAP 目标函数是严格凹的，保证了唯一的全局最大值点。**\n-   如上推导，对于指数连接，对数似然 $\\ell(z)$ 是严格凹的，其二阶导数为 $-\\exp(z)$。该导数显然与 $\\exp(z)$ 成比例。\n-   如上所述，与线性映射 $z(x)$ 复合并加上严格凹的先验，会得到一个关于 $x$ 的严格凹的 MAP 目标函数。\n-   严格凹函数保证有唯一的全局最大值点。\n-   该陈述完全正确。\n-   **结论：正确**。\n\n**B. 对于 softplus 连接 $g_{\\mathrm{sp}}$，观测对数似然作为线性驱动 $z$ 的函数对于所有非负计数和所有实数 $z$ 都是凹的。因此，当与线性函数 $z(x)$ 复合，并与二次高斯先验相加时，关于 $x$ 的完整 MAP 目标函数是凹的，确保了唯一的全局最大值点。**\n-   如上所述，对于 softplus 连接，对数似然 $\\ell(z)$ 是（严格）凹的。\n-   同样，如上所述，完整的 MAP 目标函数是一个凹函数（总对数似然）和一个严格凹函数（对数先验）的和，这使得目标函数是严格凹的。\n-   由于目标函数是严格凹的，它也是凹的。它也有一个唯一的全局最大值点。该陈述的所有部分在事实上都是正确的。\n-   **结论：正确**。\n\n**C. 相对于指数连接，softplus 连接缓和了曲率：对于固定的观测计数 $y$，在 $g_{\\mathrm{sp}}$ 下，观测对数似然关于 $z$ 的二阶导数的绝对值在 $z$ 上一致有界，并且关于 $z$ 的梯度对于所有 $z$ 都有下界 $-1$。相反，在 $g_{\\exp}$ 下，二阶导数的绝对值随着 $z \\to +\\infty$ 无界增长。**\n-   **指数连接曲率**：对于 $g_{\\exp}$， $|\\frac{d^2\\ell}{dz^2}| = |-\\exp(z)| = \\exp(z)$，当 $z \\to +\\infty$ 时无界增长。这部分是正确的。\n-   **Softplus 连接曲率**：对于 $g_{\\mathrm{sp}}$， $\\frac{d^2\\ell}{dz^2} = y \\frac{g''g - (g')^2}{g^2} - g''$。当 $|z| \\to \\infty$ 时，我们有 $g'(z) \\to \\{1,0\\}$ 和 $g''(z) \\to 0$。第一项也趋于 $0$。因此，当 $|z| \\to \\infty$ 时，$\\frac{d^2\\ell}{dz^2} \\to 0$。一个在 $\\pm\\infty$ 处趋于 $0$ 的连续函数是有界的。这部分是正确的。\n-   **Softplus 连接梯度**：对于 $g_{\\mathrm{sp}}$， $\\frac{d\\ell}{dz} = g'(z)(\\frac{y}{g(z)}-1) = \\sigma(z)(\\frac{y}{\\log(1+e^z)}-1)$。它关于 $z$ 的导数是 $\\frac{d^2\\ell}{dz^2}  0$，所以梯度是一个严格递减的函数。\n    -   当 $z \\to -\\infty$ 时，$\\frac{d\\ell}{dz} \\approx e^z(\\frac{y}{e^z}-1) = y-e^z \\to y$。\n    -   当 $z \\to +\\infty$ 时，$\\frac{d\\ell}{dz} \\approx 1(\\frac{y}{z}-1) \\to -1$。\n    -   由于函数从 $y$ 严格递减到 $-1$，它的值总是大于 $-1$。因此它的下界为 $-1$。这部分是正确的。\n-   该陈述对两种连接函数的性质进行了准确的比较。\n-   **结论：正确**。\n\n**D. 在具有线性 $z(x)$ 的潜变量 MAP E-步中，由于观测模型是泊松模型，两种连接在 $x$ 中产生相同的费雪信息（Fisher information）；因此，在 $g_{\\exp}$ 和 $g_{\\mathrm{sp}}$ 下，观测对数似然关于 $x$ 的海森矩阵（Hessian）是相同的。**\n-   对数似然关于 $x_t$ 的海森矩阵是 $\\nabla_{x_t}^2 \\mathcal{L}_t = C^T \\text{diag}(\\frac{d^2 \\ell_{t,i}}{dz_{t,i}^2}) C$。\n-   对于 $g_{\\exp}$，对角元素是 $-\\exp(z_{t,i})$。\n-   对于 $g_{\\mathrm{sp}}$，对角元素是 $y_{t,i} \\frac{g''g - (g')^2}{g^2} - g''$，在 $z_{t,i}$ 处求值。\n-   这些表达式明显不同。费雪信息是海森矩阵期望的负值。期望也不同。该陈述的前提是错误的，因此结论没有依据且是错误的。\n-   **结论：不正确**。\n\n**E. 从指数连接切换到 softplus 连接必然会破坏后验分布的对数凹性，从而使围绕潜变量 MAP 估计的拉普拉斯近似（Laplace approximation）无效。**\n-   对数后验是 MAP 目标函数。我们对选项 B 的分析表明，softplus 连接的对数后验是严格凹的。如果一个函数的对数是凹的，那么该函数是对数凹的。后验密度 $p(x|y)$ 是对数凹的，因为它的对数 $\\log p(x|y)$ 是一个凹函数。\n-   我们的分析表明，严格凹性对*两种*连接函数都成立。因此，从指数连接切换到 softplus 连接*保持*了后验分布的对数凹性。\n-   该陈述与事实恰好相反。拉普拉斯近似在这两种情况下都是有效的，因为它们依赖于一个明确定义的众数以及在该众数处的曲率（海森矩阵）的存在。\n-   **结论：不正确**。",
            "answer": "$$\n\\boxed{ABC}\n$$"
        },
        {
            "introduction": "在探索了不同结构和复杂度的潜流形模型后，一个核心挑战是如何从中选择“最佳”模型。简单地选择最拟合训练数据的模型可能会导致过拟合。本练习将引导您从贝叶斯和频率学派的视角出发，推导两种广泛使用的模型选择准则：赤池信息准则（AIC）和贝叶斯信息准则（BIC）。您将学习如何从拉普拉斯近似和样本外预测误差的估计中获得这些准则，从而理解它们如何在模型拟合优度和模型复杂度之间进行权衡。",
            "id": "3993305",
            "problem": "您正在对 $n$ 次统计独立的试验中同步记录的 $N$ 个神经元的群体活动进行建模。在每次试验 $i \\in \\{1,\\dots,n\\}$ 中，您观察到一个由以下形式的潜流形模型生成的向量 $\\mathbf{y}_{i} \\in \\mathbb{R}^{N}$\n$$\n\\mathbf{y}_{i} \\sim p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}),\n$$\n其中 $\\mathcal{M}$ 表示一个由连续潜变量 $\\mathbf{x}_{i} \\in \\mathbb{R}^{d}$ 定义的模型族，这些潜变量被积分掉，\n$$\np(\\mathbf{y}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}) \\;=\\; \\int p(\\mathbf{y}_{i}, \\mathbf{x}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}) \\, d\\mathbf{x}_{i},\n$$\n并且 $\\boldsymbol{\\theta} \\in \\mathbb{R}^{k}$ 包含了在施加任何必要约束以消除不可辨识性（例如，旋转对称性）后的所有可辨识自由参数，从而使得模型在最大似然估计量的邻域内是正则的。假设在给定 $\\boldsymbol{\\theta}$ 的条件下，各次试验是独立同分布的，并令观测数据的对数似然为\n$$\n\\ell_{n}(\\boldsymbol{\\theta}) \\;=\\; \\sum_{i=1}^{n} \\ln p(\\mathbf{y}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}).\n$$\n令 $\\widehat{\\boldsymbol{\\theta}}$ 表示最大化 $\\ell_{n}(\\boldsymbol{\\theta})$ 的最大似然估计量。假设模型族 $\\mathcal{M}$ 配备了一个先验密度 $\\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$，该密度在 $\\widehat{\\boldsymbol{\\theta}}$ 的邻域内为正且二阶连续可微，并定义边缘似然（模型证据）为\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\;=\\; \\int \\bigg[\\prod_{i=1}^{n} p(\\mathbf{y}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M})\\bigg] \\, \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta}.\n$$\n从以下基本原理出发：\n- 用于在 $f$ 的一个非退化最大值点附近渐近近似形如 $\\int \\exp\\{n f(\\boldsymbol{\\theta})\\} g(\\boldsymbol{\\theta}) \\, d\\boldsymbol{\\theta}$ 的积分的拉普拉斯方法。\n- 在独立观测的标准正则性条件下，最大似然估计量的渐近正态性和相合性。\n- Kullback-Leibler (KL) 散度的定义，以及将期望样本外对数似然解释为最小化与数据生成分布之间期望KL散度的代理。\n\n推导两个模型选择准则，以 $n$ 的主导阶表示，并用最大化的观测数据对数似然 $\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$ 和参数维度 $k$ 来表达：\n1. 一个通过拉普拉斯方法渐近近似 $-2 \\ln p(\\mathcal{D}\\,|\\,\\mathcal{M})$ 得到的准则，其中舍弃了不依赖于模型 $\\mathcal{M}$ 的加性常数。\n2. 一个通过对大小为 $n$ 的独立复制样本上的期望样本外对数似然乘以 $-2$ 进行渐近近似得到的准则，该近似使用了二阶泰勒展开和 $\\widehat{\\boldsymbol{\\theta}}$ 的渐近协方差。\n\n将这两个准则分别解释为贝叶斯信息准则 (BIC) 和赤池信息准则 (AIC)，它们是专门针对潜变量模型的，使用了将潜变量积分掉的观测数据似然。以 $\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$、$k$ 和 $n$ 的闭式形式表示这两个准则，作为需要最小化的值。以一个包含两个元素的行矩阵形式提供您的最终答案，按顺序包含 $\\mathrm{AIC}(\\mathcal{M})$ 和 $\\mathrm{BIC}(\\mathcal{M})$ 的解析表达式。 无需进行数值计算；请给出精确的符号形式。不要包含任何单位。如果出现任何在不同模型间共享且不影响模型比较的常数，请在最终表达式中将其省略。",
            "solution": "该问题要求为神经元群体活动的潜变量模型推导两个模型选择准则：贝叶斯信息准则 (BIC) 和赤池信息准则 (AIC)。推导将基于所提供的基本原理，并侧重于观测数据的对数似然，该似然将潜变量边缘化掉。\n\n### 1. 贝叶斯信息准则 (BIC) 的推导\n\n第一个准则是从边缘似然（或称模型证据）$p(\\mathcal{D}\\,|\\,\\mathcal{M})$ 的渐近近似推导出来的。边缘似然定义为：\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) = \\int p(\\mathcal{D}\\,|\\,\\boldsymbol{\\theta},\\mathcal{M}) \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta} = \\int \\exp\\{\\ell_{n}(\\boldsymbol{\\theta})\\} \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta}\n$$\n其中 $\\ell_{n}(\\boldsymbol{\\theta})$ 是观测数据的对数似然，$\\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$ 是对 $k$ 维参数向量 $\\boldsymbol{\\theta}$ 的先验。\n\n我们将使用拉普拉斯方法在 $n$ 很大时近似该积分。该方法通过将概率质量集中在后验分布的众数周围来近似积分。当 $n$ 很大时，似然项 $\\ell_{n}(\\boldsymbol{\\theta})$ 主导先验项 $\\ln \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$，因此后验众数渐近地接近于最大化 $\\ell_{n}(\\boldsymbol{\\theta})$ 的最大似然估计量 (MLE) $\\widehat{\\boldsymbol{\\theta}}$。\n\n我们定义 $h(\\boldsymbol{\\theta}) = \\ell_{n}(\\boldsymbol{\\theta}) + \\ln \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$。我们在 MLE $\\widehat{\\boldsymbol{\\theta}}$ 周围对 $h(\\boldsymbol{\\theta})$ 进行二阶泰勒展开。一个更精确的展开是围绕后验众数 $\\tilde{\\boldsymbol{\\theta}} = \\arg\\max_{\\boldsymbol{\\theta}} h(\\boldsymbol{\\theta})$ 进行的，但对于大的 $n$，$\\tilde{\\boldsymbol{\\theta}} \\approx \\widehat{\\boldsymbol{\\theta}}$ 且主导阶项是相同的。我们将 $\\ell_n(\\boldsymbol{\\theta})$ 在 $\\widehat{\\boldsymbol{\\theta}}$ 周围展开：\n$$\n\\ell_{n}(\\boldsymbol{\\theta}) \\approx \\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} \\nabla\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} \\nabla^2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})\n$$\n根据 MLE 的定义，$\\nabla\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) = \\mathbf{0}$。因此，展开式简化为：\n$$\n\\ell_{n}(\\boldsymbol{\\theta}) \\approx \\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) - \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} J_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})\n$$\n其中 $J_n(\\widehat{\\boldsymbol{\\theta}}) = -\\nabla^2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$ 是观测 Fisher 信息矩阵，在最大值点处是正定的。\n\n将此式代入 $p(\\mathcal{D}\\,|\\,\\mathcal{M})$ 的积分中，并用平滑的先验项 $\\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$ 在峰值处的值 $\\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M})$ 来近似，我们得到：\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\int \\exp\\left\\{ \\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) - \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} J_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}}) \\right\\} \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta}\n$$\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\exp\\{\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})\\} \\, \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\int \\exp\\left\\{ - \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} J_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}}) \\right\\} d\\boldsymbol{\\theta}\n$$\n该积分是一个均值为 $\\widehat{\\boldsymbol{\\theta}}$、协方差矩阵为 $J_{n}(\\widehat{\\boldsymbol{\\theta}})^{-1}$ 的多元高斯概率密度函数（未归一化）的形式。该积分的值为 $(2\\pi)^{k/2} \\det(J_{n}(\\widehat{\\boldsymbol{\\theta}}))^{-1/2}$。\n因此，\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\exp\\{\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})\\} \\, \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\, (2\\pi)^{k/2} \\det(J_{n}(\\widehat{\\boldsymbol{\\theta}}))^{-1/2}\n$$\n根据大数定律，对于独立同分布的数据，观测信息 $J_n(\\widehat{\\boldsymbol{\\theta}})$ 与 $n$ 呈线性关系，即 $J_n(\\widehat{\\boldsymbol{\\theta}}) = O(n)$。我们可以写成 $J_n(\\widehat{\\boldsymbol{\\theta}}) \\approx n I(\\boldsymbol{\\theta}_0)$，其中 $I(\\boldsymbol{\\theta}_0)$ 是单个观测在真实参数值 $\\boldsymbol{\\theta}_0$ 处计算的 Fisher 信息。\n因此，$\\det(J_n(\\widehat{\\boldsymbol{\\theta}})) \\approx \\det(nI(\\boldsymbol{\\theta}_0)) = n^k \\det(I(\\boldsymbol{\\theta}_0))$。将此代入近似式中：\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\exp\\{\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})\\} \\, \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\, (2\\pi)^{k/2} (n^k \\det(I(\\boldsymbol{\\theta}_0)))^{-1/2}\n$$\n现在，我们计算 $-2 \\ln p(\\mathcal{D}\\,|\\,\\mathcal{M})$：\n$$\n-2 \\ln p(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) - 2\\ln \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) - k\\ln(2\\pi) + k\\ln n + \\ln \\det(I(\\boldsymbol{\\theta}_0))\n$$\nBIC 是通过仅保留随样本量 $n$ 增长的项得到的。项 $\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$ 是 $O(n)$ 阶，项 $k\\ln n$ 是 $O(\\ln n)$ 阶。所有其他项，例如涉及先验和 Fisher 信息的项，都是 $O(1)$ 阶，并且是渐近可忽略的。舍弃这些 $O(1)$ 阶的项，即可得到 BIC：\n$$\n\\mathrm{BIC}(\\mathcal{M}) = -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + k \\ln n\n$$\n该准则在模型选择中需要被最小化。它代表了对数边缘证据乘以-2的渐近近似。它对模型的拟合不足（第一项）及其复杂度（第二项）进行惩罚。\n\n### 2. 赤池信息准则 (AIC) 的推导\n\n第二个准则是对期望样本外对数似然乘以 $-2$ 的近似。其目标是估计拟合后的模型对来自同一生成过程的新的、未见过的数据的预测效果。设 $p_0(\\mathbf{y})$ 是真实但未知的数据生成分布。设 $\\mathcal{D}' = \\{\\mathbf{y}'_1, \\dots, \\mathbf{y}'_n\\}$ 是一个独立的复制数据集。对于在数据 $\\mathcal{D}$ 上拟合的模型，其期望样本外对数似然为：\n$$\nC = E_{\\mathcal{D}'}[\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}))] = E_{\\mathcal{D}'}\\left[ \\sum_{i=1}^{n} \\ln p(\\mathbf{y}'_i \\,|\\, \\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}), \\mathcal{M}) \\right] = n \\int p_0(\\mathbf{y}) \\ln p(\\mathbf{y} \\,|\\, \\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}), \\mathcal{M}) d\\mathbf{y}\n$$\n这个量通过 $\\widehat{\\boldsymbol{\\theta}}(\\mathcal{D})$ 依赖于特定的训练数据集 $\\mathcal{D}$。我们希望估计它在所有可能的训练集 $\\mathcal{D}$ 上的期望：\n$$\nE_{\\mathcal{D}}[C] = E_{\\mathcal{D}} \\left[ n \\int p_0(\\mathbf{y}) \\ln p(\\mathbf{y} \\,|\\, \\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}), \\mathcal{M}) d\\mathbf{y} \\right]\n$$\n设 $\\boldsymbol{\\theta}_0$ 是使真实分布 $p_0(\\mathbf{y})$ 和模型族 $p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta},\\mathcal{M})$ 之间的 KL 散度最小化的参数值。在模型族被正确指定的假设下，$p_0(\\mathbf{y}) = p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta}_0, \\mathcal{M})$。\n我们定义 $L(\\boldsymbol{\\theta}) = E_{\\mathbf{y}\\sim p_0}[\\ln p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta},\\mathcal{M})]$。我们想要估计的量是 $E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})]$。\n使用 $L(\\widehat{\\boldsymbol{\\theta}})$ 在 $\\boldsymbol{\\theta}_0$ 附近的二阶泰勒展开：\n$$\nL(\\widehat{\\boldsymbol{\\theta}}) \\approx L(\\boldsymbol{\\theta}_0) + (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\nabla L(\\boldsymbol{\\theta}_0) + \\frac{1}{2}(\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\nabla^2 L(\\boldsymbol{\\theta}_0) (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)\n$$\n根据定义，$\\boldsymbol{\\theta}_0$ 最大化 $L(\\boldsymbol{\\theta})$，所以 $\\nabla L(\\boldsymbol{\\theta}_0) = \\mathbf{0}$。并且，$\\nabla^2 L(\\boldsymbol{\\theta}_0) = E_{\\mathbf{y}\\sim p_0} [\\nabla^2 \\ln p(\\mathbf{y} | \\boldsymbol{\\theta}_0)] = -I(\\boldsymbol{\\theta}_0)$，其中 $I(\\boldsymbol{\\theta}_0)$ 是单个观测的 Fisher 信息矩阵。\n对 $\\mathcal{D}$ 取期望：\n$$\nE_{\\mathcal{D}}[L(\\widehat{\\boldsymbol{\\theta}})] \\approx L(\\boldsymbol{\\theta}_0) + \\frac{1}{2} E_{\\mathcal{D}}\\left[ (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T (-I(\\boldsymbol{\\theta}_0)) (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0) \\right]\n$$\n使用迹的恒等式和期望的线性性，这变为：\n$$\nE_{\\mathcal{D}}[L(\\widehat{\\boldsymbol{\\theta}})] \\approx L(\\boldsymbol{\\theta}_0) - \\frac{1}{2} \\mathrm{Tr}\\left( I(\\boldsymbol{\\theta}_0) E_{\\mathcal{D}}\\left[ (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)(\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\right] \\right)\n$$\n在所述的正则性条件下，MLE $\\widehat{\\boldsymbol{\\theta}}$ 是渐近正态的，其渐近协方差为 $E_{\\mathcal{D}}\\left[ (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)(\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\right] \\approx \\frac{1}{n} I(\\boldsymbol{\\theta}_0)^{-1}$。\n$$\nE_{\\mathcal{D}}[L(\\widehat{\\boldsymbol{\\theta}})] \\approx L(\\boldsymbol{\\theta}_0) - \\frac{1}{2} \\mathrm{Tr}\\left( I(\\boldsymbol{\\theta}_0) \\frac{1}{n} I(\\boldsymbol{\\theta}_0)^{-1} \\right) = L(\\boldsymbol{\\theta}_0) - \\frac{1}{2n} \\mathrm{Tr}(I_k) = L(\\boldsymbol{\\theta}_0) - \\frac{k}{2n}\n$$\n因此，期望样本外对数似然为 $E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})] \\approx n L(\\boldsymbol{\\theta}_0) - \\frac{k}{2}$。\n\n现在我们将其与样本内最大化对数似然 $\\ell_n(\\widehat{\\boldsymbol{\\theta}})$ 联系起来。让我们求其期望。对 $\\frac{1}{n}\\ell_n(\\widehat{\\boldsymbol{\\theta}})$ 的类似展开表明，$E_{\\mathcal{D}}[\\ell_n(\\widehat{\\boldsymbol{\\theta}})]$ 存在一个向上偏差：\n$$\nE_{\\mathcal{D}}[\\ell_n(\\widehat{\\boldsymbol{\\theta}})] \\approx n L(\\boldsymbol{\\theta}_0) + \\frac{k}{2}\n$$\n差值 $E_{\\mathcal{D}}[\\ell_n(\\widehat{\\boldsymbol{\\theta}})] - E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})] \\approx k$ 是样本内对数似然的“乐观度”。这表明 $\\ell_n(\\widehat{\\boldsymbol{\\theta}}) - k$ 是期望样本外对数似然 $E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})]$ 的一个近似无偏估计量。\n赤池信息准则被定义为这个校正后的对数似然乘以 $-2$：\n$$\n\\mathrm{AIC}(\\mathcal{M}) = -2(\\ell_n(\\widehat{\\boldsymbol{\\theta}}) - k) = -2\\ell_n(\\widehat{\\boldsymbol{\\theta}}) + 2k\n$$\n该准则在模型选择中也需要被最小化。它提供了一个对新数据的期望预测误差的估计，通过一个与参数数量成正比的项来惩罚样本内拟合。\n\n关键的洞见在于，尽管两种推导源于不同的哲学观点（贝叶斯学派 vs. 频率学派），但它们都只依赖于观测数据对数似然 $\\ell_n(\\boldsymbol{\\theta})$ 及其最大化子 $\\widehat{\\boldsymbol{\\theta}}$ 的渐近性质。$\\ell_n(\\boldsymbol{\\theta})$ 是通过边缘化从一个潜变量模型中推导出来的这一事实，由问题的假设来处理，即假设 $\\ell_n(\\boldsymbol{\\theta})$ 仍然满足使 MLE 渐近理论成立的必要正则性条件，其中 $k$ 是可辨识参数的数量。\n\n总之，这两个准则是：\n1. $\\mathrm{BIC}(\\mathcal{M}) = -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + k \\ln n$\n2. $\\mathrm{AIC}(\\mathcal{M}) = -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + 2k$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + 2k  -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + k \\ln n \\end{pmatrix}}\n$$"
        }
    ]
}