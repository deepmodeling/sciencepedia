{
    "hands_on_practices": [
        {
            "introduction": "Modern neural recordings often involve high-dimensional stimuli, making computational feasibility a primary concern. This exercise  provides a practical foundation by guiding you through a \"back-of-the-envelope\" calculation of the memory required for Spike-Triggered Covariance (STC) analysis. By confronting the immense scale of the full covariance matrix, you will understand the critical need for the low-rank subspace approximations that are central to these methods.",
            "id": "4021279",
            "problem": "A single-neuron encoding model is probed with a high-dimensional spatiotemporal stimulus, where each stimulus frame consists of a two-dimensional pixel grid of size $128 \\times 128$, and the neuron is sensitive to the past $20$ frames. Each spatiotemporal stimulus sample is vectorized into a column vector of dimension $D$, where $D$ equals the total number of spatiotemporal pixels. Over the experiment, $S = 100{,}000$ spikes are recorded, and the spike-triggered ensemble is collected as a matrix of spike-triggered samples with one row per spike. All arrays are stored in double precision with $64$ bits per floating-point number. Use $1 \\ \\text{gigabyte (GB)} = 10^{9}$ bytes. \n\nYou will analyze memory requirements for Spike-Triggered Average (STA) and Spike-Triggered Covariance (STC) estimation, and then propose a low-rank subspace compression.\n\nBase your derivations on the following standard definitions:\n- The STA is the empirical mean of the spike-triggered samples, a vector in $\\mathbb{R}^{D}$.\n- The STC is the empirical covariance of the spike-triggered samples, a symmetric matrix in $\\mathbb{R}^{D \\times D}$.\n- To store a matrix of spike-triggered samples explicitly, one needs to store $S$ samples, each of dimension $D$.\n\nFirst, compute the stimulus dimensionality $D$, then compute the memory required to store:\n1. The spike-triggered samples matrix.\n2. The full STC matrix.\n\nNext, propose a low-rank STC representation that stores only the STA, the top $r$ eigenvectors of the STC assembled as columns of a matrix in $\\mathbb{R}^{D \\times r}$, and their corresponding $r$ eigenvalues in a vector. Assume that the STC eigenvalue spectrum decays geometrically as $\\lambda_{i} = \\lambda_{1} \\alpha^{i-1}$ with common ratio $\\alpha = 0.9$ for $i = 1, 2, \\dots, D$, and that $D$ is large enough for geometric-series approximations to be valid. Determine the smallest integer $r$ such that the fraction of variance explained by the top $r$ eigenvalues is at least $0.95$. Then, compute the total memory required by this compressed representation (STA, top-$r$ eigenvectors, and top-$r$ eigenvalues).\n\nExpress the final memory requirement of the compressed representation in GB. Round your final answer to four significant figures.",
            "solution": "The user has provided a valid problem statement. The task is to analyze the memory requirements for Spike-Triggered Average (STA) and Spike-Triggered Covariance (STC) analysis and to determine the memory required for a compressed, low-rank representation of the STC.\n\nFirst, we determine the dimensionality $D$ of the spatiotemporal stimulus vector. Each stimulus sample covers a spatial grid of $128 \\times 128$ pixels over a temporal window of $20$ frames. The total number of spatiotemporal pixels, which defines the dimension $D$, is the product of these values.\n$$D = (\\text{spatial pixels}) \\times (\\text{temporal frames}) = (128 \\times 128) \\times 20 = 16{,}384 \\times 20 = 327{,}680$$\n\nNext, we calculate the memory required to store the full spike-triggered dataset and the full STC matrix. The problem states that all numbers are stored in double precision, which corresponds to $64$ bits or $8$ bytes per number.\n\n1.  **Memory for the spike-triggered samples matrix:**\n    The matrix of spike-triggered samples has one row for each of the $S = 100{,}000$ spikes and one column for each of the $D = 327{,}680$ stimulus dimensions. The total number of elements in this matrix is $S \\times D$.\n    The memory required, $M_{\\text{samples}}$, is:\n    $$M_{\\text{samples}} = S \\times D \\times (\\text{bytes per number})$$\n    $$M_{\\text{samples}} = 100{,}000 \\times 327{,}680 \\times 8 \\ \\text{bytes} = 262{,}144{,}000{,}000 \\ \\text{bytes}$$\n    Using the conversion $1 \\ \\text{GB} = 10^9 \\ \\text{bytes}$, this is equivalent to $262.144 \\ \\text{GB}$.\n\n2.  **Memory for the full STC matrix:**\n    The STC matrix is a symmetric matrix in $\\mathbb{R}^{D \\times D}$. Storing the full matrix without leveraging its symmetry requires storing $D^2$ elements.\n    The memory required, $M_{\\text{STC}}$, is:\n    $$M_{\\text{STC}} = D^2 \\times (\\text{bytes per number})$$\n    $$M_{\\text{STC}} = (327{,}680)^2 \\times 8 \\ \\text{bytes} = 107{,}374{,}182{,}400 \\times 8 \\ \\text{bytes} = 858{,}993{,}459{,}200 \\ \\text{bytes}$$\n    This is equivalent to approximately $859 \\ \\text{GB}$. The large memory footprint for both the raw data and the full covariance matrix motivates the use of a compressed representation.\n\nNext, we determine the rank $r$ of the low-rank approximation. The fraction of variance, $F(r)$, explained by the top $r$ eigenvectors is the ratio of the sum of the top $r$ eigenvalues to the sum of all eigenvalues.\n$$F(r) = \\frac{\\sum_{i=1}^{r} \\lambda_i}{\\sum_{i=1}^{D} \\lambda_i}$$\nThe eigenvalues are modeled by a geometric progression $\\lambda_i = \\lambda_1 \\alpha^{i-1}$ with common ratio $\\alpha = 0.9$. The sum of the top $r$ eigenvalues is a partial geometric series:\n$$\\sum_{i=1}^{r} \\lambda_i = \\sum_{i=1}^{r} \\lambda_1 \\alpha^{i-1} = \\lambda_1 \\sum_{j=0}^{r-1} \\alpha^j = \\lambda_1 \\frac{1-\\alpha^r}{1-\\alpha}$$\nThe problem states that $D$ is large enough to approximate the total sum as an infinite geometric series:\n$$\\sum_{i=1}^{D} \\lambda_i \\approx \\sum_{i=1}^{\\infty} \\lambda_i = \\lambda_1 \\sum_{j=0}^{\\infty} \\alpha^j = \\frac{\\lambda_1}{1-\\alpha}$$\nThe fraction of variance explained is therefore:\n$$F(r) \\approx \\frac{\\lambda_1 (1-\\alpha^r)/(1-\\alpha)}{\\lambda_1/(1-\\alpha)} = 1 - \\alpha^r$$\nWe need to find the smallest integer $r$ such that the fraction of variance explained is at least $0.95$:\n$$1 - \\alpha^r \\ge 0.95$$\nSubstituting $\\alpha = 0.9$:\n$$1 - (0.9)^r \\ge 0.95$$\n$$(0.9)^r \\le 0.05$$\nTo solve for $r$, we take the natural logarithm of both sides. Since $\\ln(0.9)$ is negative, we must reverse the inequality sign:\n$$r \\ln(0.9) \\le \\ln(0.05)$$\n$$r \\ge \\frac{\\ln(0.05)}{\\ln(0.9)}$$\nUsing numerical values:\n$$r \\ge \\frac{-2.99573...}{-0.10536...} \\approx 28.433$$\nSince $r$ must be an integer, the smallest value that satisfies this condition is $r = 29$.\n\nFinally, we compute the total memory required by the compressed representation. This representation stores:\n1.  The STA vector, which has $D$ elements.\n2.  The top $r$ eigenvectors, stored as a $D \\times r$ matrix, which has $D \\times r$ elements.\n3.  The top $r$ eigenvalues, which has $r$ elements.\n\nThe total number of floating-point values to store, $N_{\\text{comp}}$, is:\n$$N_{\\text{comp}} = D + (D \\times r) + r = D(1+r) + r$$\nSubstituting $D = 327{,}680$ and $r = 29$:\n$$N_{\\text{comp}} = 327{,}680 \\times (1+29) + 29 = 327{,}680 \\times 30 + 29$$\n$$N_{\\text{comp}} = 9{,}830{,}400 + 29 = 9{,}830{,}429$$\nThe total memory required for the compressed representation, $M_{\\text{comp}}$, is the number of values multiplied by the size of each value ($8$ bytes):\n$$M_{\\text{comp}} = 9{,}830{,}429 \\times 8 \\ \\text{bytes} = 78{,}643{,}432 \\ \\text{bytes}$$\nTo express this in gigabytes (GB), we divide by $10^9$:\n$$M_{\\text{comp}} = \\frac{78{,}643{,}432}{10^9} \\ \\text{GB} = 0.078643432 \\ \\text{GB}$$\nRounding to four significant figures, we get $0.07864 \\ \\text{GB}$.",
            "answer": "$$\\boxed{0.07864}$$"
        },
        {
            "introduction": "Identifying dimensions with high variance in the spike-triggered ensemble does not guarantee they are neurally relevant; they could arise from sampling noise. This practice  provides a complete guide to implementing a rigorous permutation test to establish the statistical significance of STC results. You will learn to construct a null distribution and use it to control the family-wise error rate, a critical skill for publishing robust scientific findings.",
            "id": "4021308",
            "problem": "You are to implement, analyze, and test a permutation-based inferential procedure for spike-triggered covariance using synthetic data with a well-defined generative model. The objective is to estimate the null distribution of eigenvalues of the spike-triggered covariance difference and control the Family-Wise Error Rate (FWER) across multiple eigenvalues.\n\nConsider a stimulus time series comprised of $T$ independent draws $\\{\\mathbf{x}_t\\}_{t=1}^T$ where each $\\mathbf{x}_t \\in \\mathbb{R}^d$ is drawn from a $d$-dimensional standard Gaussian with zero mean and identity covariance. A binary spike train $\\{s_t\\}_{t=1}^T$, with $s_t \\in \\{0,1\\}$, is generated by a neuron whose spiking probability per time-bin depends on quadratic projections of the stimulus onto two latent axes. Specifically, let $\\mathbf{v}_1, \\mathbf{v}_2 \\in \\mathbb{R}^d$ be orthonormal unit vectors. The spike probability is given by the logistic model\n$$\np_t \\equiv \\mathbb{P}(s_t = 1 \\mid \\mathbf{x}_t) = \\sigma\\!\\left(c_0 + \\theta_1\\, (\\mathbf{v}_1^\\top \\mathbf{x}_t)^2 + \\theta_2\\, (\\mathbf{v}_2^\\top \\mathbf{x}_t)^2\\right),\n$$\nwhere $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $c_0 \\in \\mathbb{R}$ is a baseline logit, and $\\theta_1, \\theta_2 \\in \\mathbb{R}$ are quadratic weights controlling sensitivity along $\\mathbf{v}_1$ and $\\mathbf{v}_2$. Spikes $s_t$ are sampled as independent Bernoulli random variables with success probability $p_t$.\n\nDefine the empirical stimulus covariance across all time bins as\n$$\n\\mathbf{C} \\equiv \\frac{1}{T} \\sum_{t=1}^{T} \\left(\\mathbf{x}_t - \\overline{\\mathbf{x}}\\right)\\left(\\mathbf{x}_t - \\overline{\\mathbf{x}}\\right)^\\top,\n$$\nwhere $\\overline{\\mathbf{x}} = \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{x}_t$ is the empirical mean. Define the spike-triggered covariance as\n$$\n\\mathbf{C}_{\\text{spike}} \\equiv \\frac{1}{N_s} \\sum_{t: s_t = 1} \\left(\\mathbf{x}_t - \\overline{\\mathbf{x}}_{\\text{spike}}\\right)\\left(\\mathbf{x}_t - \\overline{\\mathbf{x}}_{\\text{spike}}\\right)^\\top,\n$$\nwhere $N_s = \\sum_{t=1}^T s_t$ is the total number of spikes and $\\overline{\\mathbf{x}}_{\\text{spike}} = \\frac{1}{N_s} \\sum_{t: s_t = 1} \\mathbf{x}_t$ is the mean stimulus during spike times. The spike-triggered covariance difference is\n$$\n\\Delta \\mathbf{C} \\equiv \\mathbf{C}_{\\text{spike}} - \\mathbf{C}.\n$$\nLet $\\lambda_1(\\Delta \\mathbf{C}), \\ldots, \\lambda_d(\\Delta \\mathbf{C})$ denote the eigenvalues of $\\Delta \\mathbf{C}$ sorted by decreasing absolute magnitude, i.e., $|\\lambda_1| \\geq |\\lambda_2| \\geq \\cdots \\geq |\\lambda_d|$.\n\nUnder the null hypothesis that spike times are independent of stimuli, the distribution of $\\Delta \\mathbf{C}$ is centered around zero, and thus the eigenvalues reflect random fluctuations. To estimate the null distribution, apply a permutation test that shuffles spike times while preserving the total spike count and the temporal arrangement in aggregate. For each permutation, compute the permuted $\\Delta \\mathbf{C}$, extract its eigenvalues, and record the maximum absolute eigenvalue statistic across the top $k$ eigenvalues. Use this empirical null distribution of the maximum-type statistic to control the Family-Wise Error Rate (FWER) at level $\\alpha_{\\mathrm{FWER}}$ across the $k$ tested eigenvalues.\n\nYour program must:\n\n- Simulate the stimuli and spike trains for each test case using the specified parameters.\n- Compute $\\Delta \\mathbf{C}$ and its eigenvalues for the observed spike train.\n- Perform $B$ permutations by randomly permuting the spike train $\\{s_t\\}$ (i.e., random reordering of the spike indicators), recomputing $\\Delta \\mathbf{C}$ and the maximum absolute eigenvalue over the top $k$ eigenvalues for each permutation.\n- Construct the empirical null distribution of the maximum-type statistic and determine the FWER-controlling threshold at level $\\alpha_{\\mathrm{FWER}}$ using the $(1-\\alpha_{\\mathrm{FWER}})$-quantile of the permutation distribution.\n- Declare each of the top $k$ observed eigenvalues significant if its absolute value exceeds the threshold. Return the total count of significant eigenvalues among the top $k$.\n\nImplementation details and scientific base:\n\n- The mathematical base includes independence and exchangeability under the null, properties of covariance and eigen-decomposition, and permutation testing rationale.\n- The Family-Wise Error Rate (FWER) is controlled by a maximum-type test: comparing each test statistic to the $(1-\\alpha_{\\mathrm{FWER}})$-quantile of the null distribution of the maximum statistic ensures strong control for any of the $k$ hypotheses.\n\nTest Suite:\n\nProvide the following four test cases, each specified by the tuple $(r, d, T, c_0, \\theta_1, \\theta_2, B, k, \\alpha_{\\mathrm{FWER}})$ where $r$ is the random seed used to ensure reproducibility.\n\n1. Baseline happy path with two informative quadratic axes:\n   - $(2025, 12, 40000, -5.5, 1.5, 1.0, 100, 4, 0.05)$\n\n2. Null dependence case (no stimulus sensitivity):\n   - $(77, 12, 40000, -4.0, 0.0, 0.0, 100, 4, 0.05)$\n\n3. Low spike count edge case (rare spiking):\n   - $(310, 8, 30000, -7.0, 1.5, 1.0, 100, 4, 0.05)$\n\n4. Higher dimensional moderate sensitivity:\n   - $(88, 20, 50000, -5.0, 0.8, 0.6, 100, 6, 0.05)$\n\nFinal Output Specification:\n\n- For each test case, your program must output a single integer equal to the number of significant eigenvalues among the top $k$ after FWER control using the maximum-type permutation threshold.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[2,0,0,1]\").\n\nThere are no physical units involved in this problem. Angles do not appear. Any fractions or probabilities in the output must be presented implicitly via integer counts; no percentage signs are permitted.",
            "solution": "The problem presents a valid and well-posed challenge in computational neuroscience, requiring the implementation of a spike-triggered covariance (STC) analysis coupled with a permutation-based hypothesis test to control the Family-Wise Error Rate (FWER). The problem is scientifically grounded, employing standard models (Generalized Linear Model with a quadratic term) and statistical techniques (permutation testing, maximum-type test for FWER control). All parameters and procedures are clearly defined, allowing for a unique and verifiable solution.\n\nThe core task is to identify stimulus dimensions that significantly influence a neuron's spiking activity. The neuron's response is modeled as a function of the stimulus variance along two specific, albeit hidden, dimensions. STC analysis is designed to recover these dimensions by analyzing the difference between the stimulus covariance conditioned on a spike and the overall stimulus covariance.\n\n### Mathematical and Procedural Framework\n\n**1. Generative Model**\n\nA synthetic dataset is generated to simulate a realistic neuroscientific experiment. The stimulus $\\mathbf{x}_t \\in \\mathbb{R}^d$ at each time $t \\in \\{1, \\dots, T\\}$ is an independent draw from a $d$-dimensional isotropic Gaussian distribution, $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. The neuron's spiking probability $p_t$ is modulated by the stimulus projections onto two orthonormal vectors, $\\mathbf{v}_1$ and $\\mathbf{v}_2$, via a logistic model:\n\n$$\np_t = \\sigma\\!(c_0 + \\theta_1 (\\mathbf{v}_1^\\top \\mathbf{x}_t)^2 + \\theta_2 (\\mathbf{v}_2^\\top \\mathbf{x}_t)^2)\n$$\n\nwhere $\\sigma(z) = (1 + e^{-z})^{-1}$ is the logistic function, $c_0$ is a baseline firing parameter, and $\\theta_1, \\theta_2$ control the sensitivity to stimulus variance along $\\mathbf{v}_1$ and $\\mathbf{v}_2$, respectively. A spike $s_t=1$ is generated as a Bernoulli trial with probability $p_t$. Since the stimulus distribution is isotropic, the specific choice of the orthonormal pair $(\\mathbf{v}_1, \\mathbf{v}_2)$ does not alter the statistical properties of the problem. We can, without loss of generality, align them with the first two standard basis vectors, $\\mathbf{e}_1$ and $\\mathbf{e}_2$.\n\n**2. Spike-Triggered Covariance (STC) Analysis**\n\nSTC aims to find stimulus dimensions where the variance of spike-eliciting stimuli differs from the background stimulus variance. We compute two covariance matrices:\n\n- The overall stimulus covariance, $\\mathbf{C}$:\n$$\n\\mathbf{C} = \\frac{1}{T} \\sum_{t=1}^{T} (\\mathbf{x}_t - \\overline{\\mathbf{x}})(\\mathbf{x}_t - \\overline{\\mathbf{x}})^\\top\n$$\n- The spike-triggered covariance, $\\mathbf{C}_{\\text{spike}}$:\n$$\n\\mathbf{C}_{\\text{spike}} = \\frac{1}{N_s} \\sum_{t: s_t = 1} (\\mathbf{x}_t - \\overline{\\mathbf{x}}_{\\text{spike}})(\\mathbf{x}_t - \\overline{\\mathbf{x}}_{\\text{spike}})^\\top\n$$\nwhere $N_s$ is the total spike count, $\\overline{\\mathbf{x}}$ is the mean stimulus, and $\\overline{\\mathbf{x}}_{\\text{spike}}$ is the mean spike-triggered stimulus.\n\nThe difference, $\\Delta \\mathbf{C} = \\mathbf{C}_{\\text{spike}} - \\mathbf{C}$, isolates the changes in covariance. The eigenvectors of $\\Delta \\mathbf{C}$ corresponding to eigenvalues of large magnitude identify the stimulus dimensions of interest. In our model, positive $\\theta_1$ and $\\theta_2$ imply that stimuli with large projections on $\\mathbf{v}_1$ and $\\mathbf{v}_2$ are more likely to cause spikes. This increases the variance of the spike-triggered ensemble along these directions, leading to eigenvectors of $\\Delta \\mathbf{C}$ that align with $\\mathbf{v}_1, \\mathbf{v}_2$ and have positive eigenvalues.\n\n**3. Permutation Test for Statistical Significance**\n\nTo assess whether the observed eigenvalues of $\\Delta \\mathbf{C}$ are statistically significant or merely due to random sampling, we employ a permutation test.\n\n- **Null Hypothesis ($H_0$)**: The spike times are independent of the stimulus. Under $H_0$, the expectation of $\\Delta \\mathbf C$ is the zero matrix, and any observed non-zero eigenvalues are noise.\n- **Permutation Rationale**: By randomly permuting the spike train $\\{s_t\\}$, we break the temporal association between stimuli $\\mathbf{x}_t$ and spikes $s_t$, effectively creating surrogate data that honor the null hypothesis. This procedure preserves the total number of spikes and the marginal statistics of the stimuli, providing a powerful non-parametric method to estimate the null distribution of our test statistics.\n\n**4. Family-Wise Error Rate (FWER) Control**\n\nWe are testing $k$ hypotheses simultaneously, one for each of the top $k$ eigenvalues (sorted by absolute magnitude). To avoid an inflated rate of false positives, we must control the FWERâ€”the probability of making at least one Type I error. We use a single-step maximum-type procedure:\n\n- **Test Statistics**: The observed test statistics are the absolute values of the top $k$ eigenvalues of the observed $\\Delta \\mathbf{C}$, denoted $\\{|\\lambda_i|\\}_{i=1}^k$.\n- **Null Distribution of the Maximum**: For each of the $B$ permutations, we compute a permuted covariance difference $\\Delta \\mathbf{C}^*$ and its eigenvalues. We then record the maximum absolute eigenvalue, $m^* = \\max_{j=1,\\ldots,d} |\\lambda_j^*|$. This is equivalent to finding $|\\lambda_1^*|$ from the permuted eigenvalues sorted by absolute magnitude. The set of these $B$ values, $\\{m^*_1, \\dots, m^*_B\\}$, forms an empirical null distribution for the most extreme possible eigenvalue.\n- **Thresholding**: We set a single significance threshold, $c_{\\text{crit}}$, as the $(1-\\alpha_{\\mathrm{FWER}})$-quantile of this empirical distribution of maximums.\n- **Decision Rule**: We declare an observed eigenvalue $\\lambda_i$ significant if its absolute value $|\\lambda_i|$ exceeds this common, stringent threshold $c_{\\text{crit}}$. This procedure provides strong control of the FWER at level $\\alpha_{\\mathrm{FWER}}$.\n\n### Algorithmic Implementation Steps\n\n1.  **Initialize**: For each test case, set the random seed $r$ and parameters $(d, T, c_0, \\theta_1, \\theta_2, B, k, \\alpha_{\\mathrm{FWER}})$.\n2.  **Generate Data**:\n    -   Generate the $T \\times d$ stimulus matrix $\\mathbf{X}$ from $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$.\n    -   Choose $\\mathbf{v}_1 = \\mathbf{e}_1$ and $\\mathbf{v}_2 = \\mathbf{e}_2$.\n    -   Compute spike probabilities $p_t$ according to the generative model.\n    -   Generate the binary spike train $s_t$ of length $T$.\n3.  **Compute Observed Statistics**:\n    -   Calculate the observed $\\Delta \\mathbf{C} = \\mathbf{C}_{\\text{spike}} - \\mathbf{C}$.\n    -   Compute the eigenvalues of $\\Delta \\mathbf{C}$ and sort them by decreasing absolute magnitude.\n    -   Extract the top $k$ absolute eigenvalues, $\\{|\\lambda_i|\\}_{i=1}^k$.\n4.  **Generate Null Distribution**:\n    -   Initialize an empty list for the null distribution statistics.\n    -   Loop $B$ times:\n        -   Create a permuted spike train $s_t^*$ by shuffling $s_t$.\n        -   Compute the permuted covariance difference $\\Delta \\mathbf{C}^*$.\n        -   Find the maximum absolute eigenvalue of $\\Delta \\mathbf{C}^*$ and append it to the list.\n5.  **Determine Significance**:\n    -   Calculate the threshold $c_{\\text{crit}}$ as the $(1-\\alpha_{\\mathrm{FWER}})$ percentile of the generated null distribution.\n    -   Count how many of the $k$ observed absolute eigenvalues, $|\\lambda_i|$, are greater than $c_{\\text{crit}}$.\n    -   Return this count.\nThis procedure is repeated for each test case provided in the problem statement.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import expit\n\ndef run_stc_permutation_test(r, d, T, c0, theta1, theta2, B, k, alpha_fwer):\n    \"\"\"\n    Simulates a neuron's spike train, performs spike-triggered covariance (STC)\n    analysis, and uses a permutation test to find significant eigenvalues\n    while controlling the Family-Wise Error Rate (FWER).\n\n    Args:\n        r (int): Random seed for reproducibility.\n        d (int): Dimensionality of the stimulus space.\n        T (int): Number of time bins.\n        c0 (float): Baseline logit for spiking.\n        theta1 (float): Weight for the first quadratic feature.\n        theta2 (float): Weight for the second quadratic feature.\n        B (int): Number of permutations for the null distribution.\n        k (int): Number of top eigenvalues to test for significance.\n        alpha_fwer (float): Target Family-Wise Error Rate.\n\n    Returns:\n        int: The number of significant eigenvalues among the top k.\n    \"\"\"\n    rng = np.random.default_rng(r)\n\n    # Step 1: Simulate stimuli and spike train\n    # Due to the isotropic stimulus distribution N(0, I), we can choose v1 and v2\n    # to be the first two standard basis vectors without loss of generality.\n    v1 = np.zeros(d)\n    v1[0] = 1.0\n    v2 = np.zeros(d)\n    v2[1] = 1.0\n\n    X = rng.standard_normal(size=(T, d))\n    \n    proj1_sq = (X @ v1)**2\n    proj2_sq = (X @ v2)**2\n\n    logits = c0 + theta1 * proj1_sq + theta2 * proj2_sq\n    p_spike = expit(logits)  # Numerically stable logistic function\n    s = rng.binomial(1, p_spike)\n\n    spike_indices = np.where(s == 1)[0]\n    Ns = len(spike_indices)\n\n    # If Ns < 2, covariance is undefined. No significant dimensions can be found.\n    # This is extremely unlikely with the given test cases.\n    if Ns < 2:\n        return 0\n\n    # Step 2: Compute observed Delta_C and its eigenvalues\n    X_spike = X[spike_indices]\n    \n    # Use np.cov with ddof=0 to divide by N, as per the problem's formulas.\n    # rowvar=False because our variables are columns of X.\n    C = np.cov(X, rowvar=False, ddof=0)\n    C_spike = np.cov(X_spike, rowvar=False, ddof=0)\n    delta_C = C_spike - C\n\n    # eigh is for symmetric matrices and returns eigenvalues in ascending order.\n    eigvals_obs = np.linalg.eigh(delta_C)[0]\n    \n    # Sort eigenvalues by decreasing absolute magnitude\n    sorted_indices_obs = np.argsort(np.abs(eigvals_obs))[::-1]\n    sorted_eigvals_obs = eigvals_obs[sorted_indices_obs]\n    \n    # These are the k test statistics we will evaluate\n    abs_eigvals_obs_k = np.abs(sorted_eigvals_obs[:k])\n\n    # Step 3: Build null distribution via permutations\n    max_perm_eigvals = np.zeros(B)\n    \n    for i in range(B):\n        # Permute the spike train to break stimulus-spike relationship\n        s_perm = rng.permutation(s)\n        spike_indices_perm = np.where(s_perm == 1)[0]\n        \n        X_spike_perm = X[spike_indices_perm]\n        \n        C_spike_perm = np.cov(X_spike_perm, rowvar=False, ddof=0)\n        delta_C_perm = C_spike_perm - C\n\n        eigvals_perm = np.linalg.eigh(delta_C_perm)[0]\n        \n        # For the max-T FWER control, we need the maximum of the test statistics\n        # under permutation. This is the largest absolute eigenvalue.\n        max_perm_eigvals[i] = np.max(np.abs(eigvals_perm))\n        \n    # Step 4: Determine threshold and count significant eigenvalues\n    # The threshold is the (1 - alpha) quantile of the null distribution of the max statistic.\n    threshold = np.quantile(max_perm_eigvals, 1 - alpha_fwer)\n    \n    # An observed eigenvalue is significant if its absolute value exceeds the FWER-controlling threshold.\n    significant_count = np.sum(abs_eigvals_obs_k > threshold)\n    \n    return int(significant_count)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (r, d, T, c0, theta1, theta2, B, k, alpha_fwer)\n        (2025, 12, 40000, -5.5, 1.5, 1.0, 100, 4, 0.05),\n        (77, 12, 40000, -4.0, 0.0, 0.0, 100, 4, 0.05),\n        (310, 8, 30000, -7.0, 1.5, 1.0, 100, 4, 0.05),\n        (88, 20, 50000, -5.0, 0.8, 0.6, 100, 6, 0.05)\n    ]\n\n    results = []\n    for case in test_cases:\n        count = run_stc_permutation_test(*case)\n        results.append(count)\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In simulation studies or model comparisons, it is essential to quantify how well an estimated subspace matches a ground-truth or alternative model. This exercise  introduces a principled method for measuring the distance between two subspaces using principal angles. By connecting this geometric concept to the singular value decomposition (SVD), you will develop a quantitative tool for evaluating the accuracy of subspace estimation methods like STC.",
            "id": "4021267",
            "problem": "Consider a neuron driven by high-dimensional, Gaussian-whitened stimuli whose relevant feature space is low-dimensional, as commonly modeled in spike-triggered covariance (STC). After whitening, the ground-truth relevant subspace is spanned by an orthonormal basis matrix $U \\in \\mathbb{R}^{4 \\times 2}$, and the STC estimate yields an orthonormal basis matrix $\\hat U \\in \\mathbb{R}^{4 \\times 2}$. Suppose\n$$\nU = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix}, \\quad\n\\hat U = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{3}} \\\\\n\\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & \\sqrt{\\frac{2}{3}}\n\\end{pmatrix}.\n$$\nStarting from the definition of principal angles between subspaces as the sequence $\\{\\theta_i\\}$ obtained by recursively maximizing the cosine of the angle between unit vectors in each subspace under mutual orthogonality constraints, and using well-tested properties of orthonormal bases and the Singular Value Decomposition (SVD) of the cross-basis matrix $U^{\\top}\\hat U$, derive the relationship needed to compute the principal angles $\\{\\theta_i\\}$ from $U$ and $\\hat U$. Then, define a quantitative subspace discrepancy $d$ as the square root of the sum of the squared sines of these principal angles and compute its value for the matrices above.\n\nExpress your final answer for $d$ as a real number, and round your answer to four significant figures. No units are required.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective, permitting a formal solution. It asks for the derivation of a method to compute principal angles between two subspaces from their orthonormal bases and then to calculate a specific discrepancy metric.\n\nLet the two subspaces be denoted by $S_U = \\text{span}(U)$ and $S_{\\hat{U}} = \\text{span}(\\hat{U})$, where $U \\in \\mathbb{R}^{n \\times k}$ and $\\hat{U} \\in \\mathbb{R}^{n \\times k}$ are matrices with orthonormal columns that form bases for their respective $k$-dimensional subspaces. Here, $n=4$ and $k=2$.\n\nThe principal angles $\\{\\theta_i\\}_{i=1}^k$ between these two subspaces are defined recursively. The first principal angle, $\\theta_1$, is the smallest angle between any pair of unit vectors, one from each subspace. Its cosine is given by the maximization:\n$$ \\cos(\\theta_1) = \\max_{\\substack{\\mathbf{u} \\in S_U, \\, ||\\mathbf{u}||=1 \\\\ \\mathbf{\\hat{u}} \\in S_{\\hat{U}}, \\, ||\\mathbf{\\hat{u}}||=1}} \\mathbf{u}^\\top \\mathbf{\\hat{u}} $$\nAny vector $\\mathbf{u} \\in S_U$ can be written as a linear combination of the basis vectors in $U$, i.e., $\\mathbf{u} = U\\mathbf{x}$ for some coefficient vector $\\mathbf{x} \\in \\mathbb{R}^k$. Since the columns of $U$ are orthonormal, $||\\mathbf{u}||^2 = (U\\mathbf{x})^\\top(U\\mathbf{x}) = \\mathbf{x}^\\top U^\\top U \\mathbf{x} = \\mathbf{x}^\\top I_k \\mathbf{x} = ||\\mathbf{x}||^2$. Thus, the condition $||\\mathbf{u}||=1$ implies $||\\mathbf{x}||=1$. Similarly, for $\\mathbf{\\hat{u}} \\in S_{\\hat{U}}$, we can write $\\mathbf{\\hat{u}} = \\hat{U}\\mathbf{y}$ with $||\\mathbf{y}||=1$.\n\nSubstituting these into the maximization problem, we get:\n$$ \\cos(\\theta_1) = \\max_{||\\mathbf{x}||=1, ||\\mathbf{y}||=1} (U\\mathbf{x})^\\top (\\hat{U}\\mathbf{y}) = \\max_{||\\mathbf{x}||=1, ||\\mathbf{y}||=1} \\mathbf{x}^\\top (U^\\top \\hat{U}) \\mathbf{y} $$\nLet $M = U^\\top \\hat{U}$. The expression $\\mathbf{x}^\\top M \\mathbf{y}$ is a bilinear form. The maximum value of this form for unit vectors $\\mathbf{x}$ and $\\mathbf{y}$ is, by definition, the largest singular value of the matrix $M$, denoted $\\sigma_1(M)$.\n\nThe subsequent principal angles are found by repeating this process on the orthogonal complements of the previously found principal vectors. The general relationship, which is a fundamental result of linear algebra, states that the cosines of the principal angles $\\{\\theta_i\\}$ are the singular values of the cross-basis matrix $M = U^\\top \\hat{U}$.\n$$ \\cos(\\theta_i) = \\sigma_i(U^\\top \\hat{U}) \\quad \\text{for } i=1, \\dots, k $$\nwhere the singular values are ordered $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$.\n\nThe problem defines a subspace discrepancy metric $d$ as the square root of the sum of the squared sines of these principal angles:\n$$ d = \\sqrt{\\sum_{i=1}^k \\sin^2(\\theta_i)} $$\nUsing the identity $\\sin^2(\\theta_i) + \\cos^2(\\theta_i) = 1$ and the relationship $\\cos(\\theta_i) = \\sigma_i$, we can express $d$ in terms of the singular values:\n$$ d = \\sqrt{\\sum_{i=1}^k (1 - \\cos^2(\\theta_i))} = \\sqrt{\\sum_{i=1}^k (1 - \\sigma_i^2)} $$\nNow, we compute this value for the given matrices:\n$$ U = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\quad \\hat{U} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & \\sqrt{\\frac{2}{3}} \\end{pmatrix} $$\nFirst, we compute the matrix $M = U^\\top \\hat{U}$. The transpose of $U$ is:\n$$ U^\\top = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\end{pmatrix} $$\nThe product is:\n$$ M = U^\\top \\hat{U} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & \\sqrt{\\frac{2}{3}} \\end{pmatrix} = \\begin{pmatrix} (1)(\\frac{1}{\\sqrt{2}}) + (0)(0) & (1)(0) + (0)(\\frac{1}{\\sqrt{3}}) \\\\ (0)(\\frac{1}{\\sqrt{2}}) + (1)(0) & (0)(0) + (1)(\\frac{1}{\\sqrt{3}}) \\end{pmatrix} $$\n$$ M = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & \\frac{1}{\\sqrt{3}} \\end{pmatrix} $$\nThe matrix $M$ is a diagonal matrix. The singular values of a diagonal matrix are the absolute values of its diagonal entries. Therefore, the singular values are:\n$$ \\sigma_1 = \\frac{1}{\\sqrt{2}}, \\quad \\sigma_2 = \\frac{1}{\\sqrt{3}} $$\nNote that $\\frac{1}{\\sqrt{2}} > \\frac{1}{\\sqrt{3}}$, so this ordering is correct.\n\nNow we can compute the discrepancy $d^2$:\n$$ d^2 = \\sum_{i=1}^{2} (1 - \\sigma_i^2) = (1 - \\sigma_1^2) + (1 - \\sigma_2^2) $$\n$$ d^2 = \\left(1 - \\left(\\frac{1}{\\sqrt{2}}\\right)^2\\right) + \\left(1 - \\left(\\frac{1}{\\sqrt{3}}\\right)^2\\right) $$\n$$ d^2 = \\left(1 - \\frac{1}{2}\\right) + \\left(1 - \\frac{1}{3}\\right) = \\frac{1}{2} + \\frac{2}{3} $$\n$$ d^2 = \\frac{3}{6} + \\frac{4}{6} = \\frac{7}{6} $$\nThe discrepancy $d$ is the square root of this value:\n$$ d = \\sqrt{\\frac{7}{6}} $$\nTo provide the numerical answer, we compute the value and round to four significant figures:\n$$ d = \\sqrt{\\frac{7}{6}} \\approx \\sqrt{1.166666...} \\approx 1.080123... $$\nRounding to four significant figures gives $1.080$.",
            "answer": "$$\\boxed{1.080}$$"
        }
    ]
}