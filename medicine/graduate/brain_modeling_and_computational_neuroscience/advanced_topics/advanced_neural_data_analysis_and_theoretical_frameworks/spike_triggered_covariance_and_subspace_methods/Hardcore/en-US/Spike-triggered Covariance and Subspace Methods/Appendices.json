{
    "hands_on_practices": [
        {
            "introduction": "The primary goal of Spike-Triggered Covariance (STC) analysis is to identify stimulus dimensions along which stimulus variance is significantly different for spike-eliciting stimuli compared to all stimuli. The eigenvectors of the STC matrix with the largest absolute eigenvalues are candidates for these dimensions, but how do we know if they reflect genuine neural tuning or simply sampling noise? This practice guides you through implementing a permutation test, a powerful non-parametric method to establish statistical significance, allowing you to build a null distribution for the eigenvalues and control the family-wise error rate when testing multiple dimensions simultaneously .",
            "id": "4021308",
            "problem": "You are to implement, analyze, and test a permutation-based inferential procedure for spike-triggered covariance using synthetic data with a well-defined generative model. The objective is to estimate the null distribution of eigenvalues of the spike-triggered covariance difference and control the Family-Wise Error Rate (FWER) across multiple eigenvalues.\n\nConsider a stimulus time series comprised of $T$ independent draws $\\{\\mathbf{x}_t\\}_{t=1}^T$ where each $\\mathbf{x}_t \\in \\mathbb{R}^d$ is drawn from a $d$-dimensional standard Gaussian with zero mean and identity covariance. A binary spike train $\\{s_t\\}_{t=1}^T$, with $s_t \\in \\{0,1\\}$, is generated by a neuron whose spiking probability per time-bin depends on quadratic projections of the stimulus onto two latent axes. Specifically, let $\\mathbf{v}_1, \\mathbf{v}_2 \\in \\mathbb{R}^d$ be orthonormal unit vectors. The spike probability is given by the logistic model\n$$\np_t \\equiv \\mathbb{P}(s_t = 1 \\mid \\mathbf{x}_t) = \\sigma\\!\\left(c_0 + \\theta_1\\, (\\mathbf{v}_1^\\top \\mathbf{x}_t)^2 + \\theta_2\\, (\\mathbf{v}_2^\\top \\mathbf{x}_t)^2\\right),\n$$\nwhere $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $c_0 \\in \\mathbb{R}$ is a baseline logit, and $\\theta_1, \\theta_2 \\in \\mathbb{R}$ are quadratic weights controlling sensitivity along $\\mathbf{v}_1$ and $\\mathbf{v}_2$. Spikes $s_t$ are sampled as independent Bernoulli random variables with success probability $p_t$.\n\nDefine the empirical stimulus covariance across all time bins as\n$$\n\\mathbf{C} \\equiv \\frac{1}{T} \\sum_{t=1}^{T} \\left(\\mathbf{x}_t - \\overline{\\mathbf{x}}\\right)\\left(\\mathbf{x}_t - \\overline{\\mathbf{x}}\\right)^\\top,\n$$\nwhere $\\overline{\\mathbf{x}} = \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{x}_t$ is the empirical mean. Define the spike-triggered covariance as\n$$\n\\mathbf{C}_{\\text{spike}} \\equiv \\frac{1}{N_s} \\sum_{t: s_t = 1} \\left(\\mathbf{x}_t - \\overline{\\mathbf{x}}_{\\text{spike}}\\right)\\left(\\mathbf{x}_t - \\overline{\\mathbf{x}}_{\\text{spike}}\\right)^\\top,\n$$\nwhere $N_s = \\sum_{t=1}^T s_t$ is the total number of spikes and $\\overline{\\mathbf{x}}_{\\text{spike}} = \\frac{1}{N_s} \\sum_{t: s_t = 1} \\mathbf{x}_t$ is the mean stimulus during spike times. The spike-triggered covariance difference is\n$$\n\\Delta \\mathbf{C} \\equiv \\mathbf{C}_{\\text{spike}} - \\mathbf{C}.\n$$\nLet $\\lambda_1(\\Delta \\mathbf{C}), \\ldots, \\lambda_d(\\Delta \\mathbf{C})$ denote the eigenvalues of $\\Delta \\mathbf{C}$ sorted by decreasing absolute magnitude, i.e., $|\\lambda_1| \\geq |\\lambda_2| \\geq \\cdots \\geq |\\lambda_d|$.\n\nUnder the null hypothesis that spike times are independent of stimuli, the distribution of $\\Delta \\mathbf{C}$ is centered around zero, and thus the eigenvalues reflect random fluctuations. To estimate the null distribution, apply a permutation test that shuffles spike times while preserving the total spike count and the temporal arrangement in aggregate. For each permutation, compute the permuted $\\Delta \\mathbf{C}$, extract its eigenvalues, and record the maximum absolute eigenvalue statistic across the top $k$ eigenvalues. Use this empirical null distribution of the maximum-type statistic to control the Family-Wise Error Rate (FWER) at level $\\alpha_{\\mathrm{FWER}}$ across the $k$ tested eigenvalues.\n\nYour program must:\n\n- Simulate the stimuli and spike trains for each test case using the specified parameters.\n- Compute $\\Delta \\mathbf{C}$ and its eigenvalues for the observed spike train.\n- Perform $B$ permutations by randomly permuting the spike train $\\{s_t\\}$ (i.e., random reordering of the spike indicators), recomputing $\\Delta \\mathbf{C}$ and the maximum absolute eigenvalue over the top $k$ eigenvalues for each permutation.\n- Construct the empirical null distribution of the maximum-type statistic and determine the FWER-controlling threshold at level $\\alpha_{\\mathrm{FWER}}$ using the $(1-\\alpha_{\\mathrm{FWER}})$-quantile of the permutation distribution.\n- Declare each of the top $k$ observed eigenvalues significant if its absolute value exceeds the threshold. Return the total count of significant eigenvalues among the top $k$.\n\nImplementation details and scientific base:\n\n- The mathematical base includes independence and exchangeability under the null, properties of covariance and eigen-decomposition, and permutation testing rationale.\n- The Family-Wise Error Rate (FWER) is controlled by a maximum-type test: comparing each test statistic to the $(1-\\alpha_{\\mathrm{FWER}})$-quantile of the null distribution of the maximum statistic ensures strong control for any of the $k$ hypotheses.\n\nTest Suite:\n\nProvide the following four test cases, each specified by the tuple $(r, d, T, c_0, \\theta_1, \\theta_2, B, k, \\alpha_{\\mathrm{FWER}})$ where $r$ is the random seed used to ensure reproducibility.\n\n1. Baseline happy path with two informative quadratic axes:\n   - $(2025, 12, 40000, -5.5, 1.5, 1.0, 100, 4, 0.05)$\n\n2. Null dependence case (no stimulus sensitivity):\n   - $(77, 12, 40000, -4.0, 0.0, 0.0, 100, 4, 0.05)$\n\n3. Low spike count edge case (rare spiking):\n   - $(310, 8, 30000, -7.0, 1.5, 1.0, 100, 4, 0.05)$\n\n4. Higher dimensional moderate sensitivity:\n   - $(88, 20, 50000, -5.0, 0.8, 0.6, 100, 6, 0.05)$\n\nFinal Output Specification:\n\n- For each test case, your program must output a single integer equal to the number of significant eigenvalues among the top $k$ after FWER control using the maximum-type permutation threshold.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[2,0,0,1]\").\n\nThere are no physical units involved in this problem. Angles do not appear. Any fractions or probabilities in the output must be presented implicitly via integer counts; no percentage signs are permitted.",
            "solution": "The problem presents a valid and well-posed challenge in computational neuroscience, requiring the implementation of a spike-triggered covariance (STC) analysis coupled with a permutation-based hypothesis test to control the Family-Wise Error Rate (FWER). The problem is scientifically grounded, employing standard models (Generalized Linear Model with a quadratic term) and statistical techniques (permutation testing, maximum-type test for FWER control). All parameters and procedures are clearly defined, allowing for a unique and verifiable solution.\n\nThe core task is to identify stimulus dimensions that significantly influence a neuron's spiking activity. The neuron's response is modeled as a function of the stimulus variance along two specific, albeit hidden, dimensions. STC analysis is designed to recover these dimensions by analyzing the difference between the stimulus covariance conditioned on a spike and the overall stimulus covariance.\n\n### Mathematical and Procedural Framework\n\n**1. Generative Model**\n\nA synthetic dataset is generated to simulate a realistic neuroscientific experiment. The stimulus $\\mathbf{x}_t \\in \\mathbb{R}^d$ at each time $t \\in \\{1, \\dots, T\\}$ is an independent draw from a $d$-dimensional isotropic Gaussian distribution, $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. The neuron's spiking probability $p_t$ is modulated by the stimulus projections onto two orthonormal vectors, $\\mathbf{v}_1$ and $\\mathbf{v}_2$, via a logistic model:\n\n$$\np_t = \\sigma\\!(c_0 + \\theta_1 (\\mathbf{v}_1^\\top \\mathbf{x}_t)^2 + \\theta_2 (\\mathbf{v}_2^\\top \\mathbf{x}_t)^2)\n$$\n\nwhere $\\sigma(z) = (1 + e^{-z})^{-1}$ is the logistic function, $c_0$ is a baseline firing parameter, and $\\theta_1, \\theta_2$ control the sensitivity to stimulus variance along $\\mathbf{v}_1$ and $\\mathbf{v}_2$, respectively. A spike $s_t=1$ is generated as a Bernoulli trial with probability $p_t$. Since the stimulus distribution is isotropic, the specific choice of the orthonormal pair $(\\mathbf{v}_1, \\mathbf{v}_2)$ does not alter the statistical properties of the problem. We can, without loss of generality, align them with the first two standard basis vectors, $\\mathbf{e}_1$ and $\\mathbf{e}_2$.\n\n**2. Spike-Triggered Covariance (STC) Analysis**\n\nSTC aims to find stimulus dimensions where the variance of spike-eliciting stimuli differs from the background stimulus variance. We compute two covariance matrices:\n\n- The overall stimulus covariance, $\\mathbf{C}$:\n$$\n\\mathbf{C} = \\frac{1}{T} \\sum_{t=1}^{T} (\\mathbf{x}_t - \\overline{\\mathbf{x}})(\\mathbf{x}_t - \\overline{\\mathbf{x}})^\\top\n$$\n- The spike-triggered covariance, $\\mathbf{C}_{\\text{spike}}$:\n$$\n\\mathbf{C}_{\\text{spike}} = \\frac{1}{N_s} \\sum_{t: s_t = 1} (\\mathbf{x}_t - \\overline{\\mathbf{x}}_{\\text{spike}})(\\mathbf{x}_t - \\overline{\\mathbf{x}}_{\\text{spike}})^\\top\n$$\nwhere $N_s$ is the total spike count, $\\overline{\\mathbf{x}}$ is the mean stimulus, and $\\overline{\\mathbf{x}}_{\\text{spike}}$ is the mean spike-triggered stimulus.\n\nThe difference, $\\Delta \\mathbf{C} = \\mathbf{C}_{\\text{spike}} - \\mathbf{C}$, isolates the changes in covariance. The eigenvectors of $\\Delta \\mathbf{C}$ corresponding to eigenvalues of large magnitude identify the stimulus dimensions of interest. In our model, positive $\\theta_1$ and $\\theta_2$ imply that stimuli with large projections on $\\mathbf{v}_1$ and $\\mathbf{v}_2$ are more likely to cause spikes. This increases the variance of the spike-triggered ensemble along these directions, leading to eigenvectors of $\\Delta \\mathbf{C}$ that align with $\\mathbf{v}_1, \\mathbf{v}_2$ and have positive eigenvalues.\n\n**3. Permutation Test for Statistical Significance**\n\nTo assess whether the observed eigenvalues of $\\Delta \\mathbf{C}$ are statistically significant or merely due to random sampling, we employ a permutation test.\n\n- **Null Hypothesis ($H_0$)**: The spike times are independent of the stimulus. Under $H_0$, the expectation of $\\Delta \\mathbf C$ is the zero matrix, and any observed non-zero eigenvalues are noise.\n- **Permutation Rationale**: By randomly permuting the spike train $\\{s_t\\}$, we break the temporal association between stimuli $\\mathbf{x}_t$ and spikes $s_t$, effectively creating surrogate data that honor the null hypothesis. This procedure preserves the total number of spikes and the marginal statistics of the stimuli, providing a powerful non-parametric method to estimate the null distribution of our test statistics.\n\n**4. Family-Wise Error Rate (FWER) Control**\n\nWe are testing $k$ hypotheses simultaneously, one for each of the top $k$ eigenvalues (sorted by absolute magnitude). To avoid an inflated rate of false positives, we must control the FWER—the probability of making at least one Type I error. We use a single-step maximum-type procedure:\n\n- **Test Statistics**: The observed test statistics are the absolute values of the top $k$ eigenvalues of the observed $\\Delta \\mathbf{C}$, denoted $\\{|\\lambda_i|\\}_{i=1}^k$.\n- **Null Distribution of the Maximum**: For each of the $B$ permutations, we compute a permuted covariance difference $\\Delta \\mathbf{C}^*$ and its eigenvalues. We then record the maximum absolute eigenvalue, $m^* = \\max_{j=1,\\ldots,d} |\\lambda_j^*|$. This is equivalent to finding $|\\lambda_1^*|$ from the permuted eigenvalues sorted by absolute magnitude. The set of these $B$ values, $\\{m^*_1, \\dots, m^*_B\\}$, forms an empirical null distribution for the most extreme possible eigenvalue.\n- **Thresholding**: We set a single significance threshold, $c_{\\text{crit}}$, as the $(1-\\alpha_{\\mathrm{FWER}})$-quantile of this empirical distribution of maximums.\n- **Decision Rule**: We declare an observed eigenvalue $\\lambda_i$ significant if its absolute value $|\\lambda_i|$ exceeds this common, stringent threshold $c_{\\text{crit}}$. This procedure provides strong control of the FWER at level $\\alpha_{\\mathrm{FWER}}$.\n\n### Algorithmic Implementation Steps\n\n1.  **Initialize**: For each test case, set the random seed $r$ and parameters $(d, T, c_0, \\theta_1, \\theta_2, B, k, \\alpha_{\\mathrm{FWER}})$.\n2.  **Generate Data**:\n    -   Generate the $T \\times d$ stimulus matrix $\\mathbf{X}$ from $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$.\n    -   Choose $\\mathbf{v}_1 = \\mathbf{e}_1$ and $\\mathbf{v}_2 = \\mathbf{e}_2$.\n    -   Compute spike probabilities $p_t$ according to the generative model.\n    -   Generate the binary spike train $s_t$ of length $T$.\n3.  **Compute Observed Statistics**:\n    -   Calculate the observed $\\Delta \\mathbf{C} = \\mathbf{C}_{\\text{spike}} - \\mathbf{C}$.\n    -   Compute the eigenvalues of $\\Delta \\mathbf{C}$ and sort them by decreasing absolute magnitude.\n    -   Extract the top $k$ absolute eigenvalues, $\\{|\\lambda_i|\\}_{i=1}^k$.\n4.  **Generate Null Distribution**:\n    -   Initialize an empty list for the null distribution statistics.\n    -   Loop $B$ times:\n        -   Create a permuted spike train $s_t^*$ by shuffling $s_t$.\n        -   Compute the permuted covariance difference $\\Delta \\mathbf{C}^*$.\n        -   Find the maximum absolute eigenvalue of $\\Delta \\mathbf{C}^*$ and append it to the list.\n5.  **Determine Significance**:\n    -   Calculate the threshold $c_{\\text{crit}}$ as the $(1-\\alpha_{\\mathrm{FWER}})$ percentile of the generated null distribution.\n    -   Count how many of the $k$ observed absolute eigenvalues, $|\\lambda_i|$, are greater than $c_{\\text{crit}}$.\n    -   Return this count.\nThis procedure is repeated for each test case provided in the problem statement.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import expit\n\ndef run_stc_permutation_test(r, d, T, c0, theta1, theta2, B, k, alpha_fwer):\n    \"\"\"\n    Simulates a neuron's spike train, performs spike-triggered covariance (STC)\n    analysis, and uses a permutation test to find significant eigenvalues\n    while controlling the Family-Wise Error Rate (FWER).\n\n    Args:\n        r (int): Random seed for reproducibility.\n        d (int): Dimensionality of the stimulus space.\n        T (int): Number of time bins.\n        c0 (float): Baseline logit for spiking.\n        theta1 (float): Weight for the first quadratic feature.\n        theta2 (float): Weight for the second quadratic feature.\n        B (int): Number of permutations for the null distribution.\n        k (int): Number of top eigenvalues to test for significance.\n        alpha_fwer (float): Target Family-Wise Error Rate.\n\n    Returns:\n        int: The number of significant eigenvalues among the top k.\n    \"\"\"\n    rng = np.random.default_rng(r)\n\n    # Step 1: Simulate stimuli and spike train\n    # Due to the isotropic stimulus distribution N(0, I), we can choose v1 and v2\n    # to be the first two standard basis vectors without loss of generality.\n    v1 = np.zeros(d)\n    v1[0] = 1.0\n    v2 = np.zeros(d)\n    v2[1] = 1.0\n\n    X = rng.standard_normal(size=(T, d))\n    \n    proj1_sq = (X @ v1)**2\n    proj2_sq = (X @ v2)**2\n\n    logits = c0 + theta1 * proj1_sq + theta2 * proj2_sq\n    p_spike = expit(logits)  # Numerically stable logistic function\n    s = rng.binomial(1, p_spike)\n\n    spike_indices = np.where(s == 1)[0]\n    Ns = len(spike_indices)\n\n    # If Ns  2, covariance is undefined. No significant dimensions can be found.\n    # This is extremely unlikely with the given test cases.\n    if Ns  2:\n        return 0\n\n    # Step 2: Compute observed Delta_C and its eigenvalues\n    X_spike = X[spike_indices]\n    \n    # Use np.cov with ddof=0 to divide by N, as per the problem's formulas.\n    # rowvar=False because our variables are columns of X.\n    C = np.cov(X, rowvar=False, ddof=0)\n    C_spike = np.cov(X_spike, rowvar=False, ddof=0)\n    delta_C = C_spike - C\n\n    # eigh is for symmetric matrices and returns eigenvalues in ascending order.\n    eigvals_obs = np.linalg.eigh(delta_C)[0]\n    \n    # Sort eigenvalues by decreasing absolute magnitude\n    sorted_indices_obs = np.argsort(np.abs(eigvals_obs))[::-1]\n    sorted_eigvals_obs = eigvals_obs[sorted_indices_obs]\n    \n    # These are the k test statistics we will evaluate\n    abs_eigvals_obs_k = np.abs(sorted_eigvals_obs[:k])\n\n    # Step 3: Build null distribution via permutations\n    max_perm_eigvals = np.zeros(B)\n    \n    for i in range(B):\n        # Permute the spike train to break stimulus-spike relationship\n        s_perm = rng.permutation(s)\n        spike_indices_perm = np.where(s_perm == 1)[0]\n        \n        X_spike_perm = X[spike_indices_perm]\n        \n        C_spike_perm = np.cov(X_spike_perm, rowvar=False, ddof=0)\n        delta_C_perm = C_spike_perm - C\n\n        eigvals_perm = np.linalg.eigh(delta_C_perm)[0]\n        \n        # For the max-T FWER control, we need the maximum of the test statistics\n        # under permutation. This is the largest absolute eigenvalue.\n        max_perm_eigvals[i] = np.max(np.abs(eigvals_perm))\n        \n    # Step 4: Determine threshold and count significant eigenvalues\n    # The threshold is the (1 - alpha) quantile of the null distribution of the max statistic.\n    threshold = np.quantile(max_perm_eigvals, 1 - alpha_fwer)\n    \n    # An observed eigenvalue is significant if its absolute value exceeds the FWER-controlling threshold.\n    significant_count = np.sum(abs_eigvals_obs_k  threshold)\n    \n    return int(significant_count)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (r, d, T, c0, theta1, theta2, B, k, alpha_fwer)\n        (2025, 12, 40000, -5.5, 1.5, 1.0, 100, 4, 0.05),\n        (77, 12, 40000, -4.0, 0.0, 0.0, 100, 4, 0.05),\n        (310, 8, 30000, -7.0, 1.5, 1.0, 100, 4, 0.05),\n        (88, 20, 50000, -5.0, 0.8, 0.6, 100, 6, 0.05)\n    ]\n\n    results = []\n    for case in test_cases:\n        count = run_stc_permutation_test(*case)\n        results.append(count)\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In realistic scenarios, a neuron's firing is influenced not only by external stimuli but also by its own recent activity, a phenomenon known as spike-history dependence. These history effects can become confounded with stimulus features, potentially corrupting STC analysis and leading to misinterpretation of the relevant subspace. This exercise explores a crucial technique for mitigating such confounds: residualizing the stimulus by regressing out the linearly predictable components from spike-history predictors, thereby isolating the stimulus information more effectively .",
            "id": "4021328",
            "problem": "Consider an experiment in which a neuron is driven by a high-dimensional stimulus and exhibits spike-history effects. The stimulus at time index $t$ is a $3$-dimensional vector $s_t \\in \\mathbb{R}^3$, and the spike-history feature vector $h_t \\in \\mathbb{R}^2$ collects two predictors $h_{1,t}$ and $h_{2,t}$ encoding recent spiking. Assume $(s_t, h_t)$ is jointly zero-mean and second-order stationary with well-defined covariances.\n\nLet $C_{ss} \\in \\mathbb{R}^{3 \\times 3}$, $C_{hh} \\in \\mathbb{R}^{2 \\times 2}$, and $C_{sh} \\in \\mathbb{R}^{3 \\times 2}$ denote the unconditional stimulus covariance, history covariance, and stimulus-history cross-covariance, respectively. The Spike-Triggered Covariance (STC) of the stimulus is defined as $C_{ss|\\text{spike}} - C_{ss}$, the difference between the covariance of stimuli preceding spikes and the unconditional stimulus covariance.\n\nTo mitigate confounding from spike-history, one can regress out the component of $s_t$ that is linearly predictable from history by projecting stimuli onto the orthogonal complement of history predictors prior to computing STC. In the population (infinite-sample) limit under second-order stationarity, the orthogonal projection that removes linear predictability from history is equivalent to residualizing $s_t$ by subtracting its best linear prediction from $h_t$. Denote the residualized stimulus by $s_t^{\\perp} = s_t - B h_t$, where $B \\in \\mathbb{R}^{3 \\times 2}$ is chosen to minimize the expected squared prediction error.\n\nSuppose the true covariances are\n$$\nC_{ss} = \\begin{pmatrix}\n2.0  0.5  0.3 \\\\\n0.5  1.5  0.2 \\\\\n0.3  0.2  1.0\n\\end{pmatrix}, \\quad\nC_{hh} = \\begin{pmatrix}\n1.0  0.3 \\\\\n0.3  0.8\n\\end{pmatrix}, \\quad\nC_{sh} = \\begin{pmatrix}\n0.4  0.2 \\\\\n0.1  0.3 \\\\\n0.2  0.1\n\\end{pmatrix}.\n$$\n\nNow consider a mis-specified regression in which only the first history predictor $h_{1,t}$ is included when residualizing. Let $u_t = h_{1,t}$ and $v_t = h_{2,t}$. In this case, the residualizing transformation uses only $u_t$, yielding $s_t^{\\perp u} = s_t - B_u u_t$, where $B_u \\in \\mathbb{R}^{3 \\times 1}$ minimizes the expected squared error using $u_t$ alone. The residual confounding with respect to the omitted predictor $v_t$ can be quantified by the ratio of the norm of the residual stimulus-history cross-covariance with $v_t$ to the original norm with $v_t$:\n$$\n\\gamma \\equiv \\frac{\\|C_{s^{\\perp u} v}\\|_{2}}{\\|C_{s v}\\|_{2}},\n$$\nwhere $C_{s v} \\in \\mathbb{R}^{3 \\times 1}$ is the second column of $C_{sh}$ and $C_{s^{\\perp u} v}$ is the cross-covariance between $s_t^{\\perp u}$ and $v_t$.\n\nStarting from core definitions of covariance, orthogonal projections, and least-squares regression, derive the population-optimal residualizing transformation $s_t^{\\perp} = s_t - B h_t$ as the projection onto the orthogonal complement of the history predictors, explain how this relates to computing STC on residualized stimuli, and then, for the mis-specified single-predictor residualization using $u_t$, compute the scalar residual confounding index $\\gamma$ using the given covariances.\n\nExpress the final scalar as a dimensionless number and round your answer to four significant figures.",
            "solution": "We begin with the foundational definitions. The stimulus $s_t \\in \\mathbb{R}^3$ and history $h_t \\in \\mathbb{R}^2$ are jointly zero-mean with unconditional second moments represented by $C_{ss}$, $C_{hh}$, and $C_{sh}$. The Spike-Triggered Covariance (STC) is defined as the difference $C_{ss|\\text{spike}} - C_{ss}$, capturing second-order stimulus features associated with spiking. Spike-history confounding arises when $s_t$ and $h_t$ are correlated, causing STC to reflect variance structure attributable to history rather than stimulus selectivity.\n\nTo regress out history, we use the orthogonal projection principle: for a given predictor matrix (here, the history features), the least-squares residual is orthogonal to the predictor subspace. In the population limit, this equivalence can be stated without reference to finite-sample design matrices by using second moments.\n\nConsider the linear predictor of $s_t$ from $h_t$ of the form $B h_t$ with $B \\in \\mathbb{R}^{3 \\times 2}$. The best linear predictor in the mean-square sense minimizes the expected squared error\n$$\n\\mathbb{E}\\big[ \\| s_t - B h_t \\|_2^2 \\big].\n$$\nDifferentiating with respect to $B$ and setting the gradient to zero gives the normal equations\n$$\n\\frac{\\partial}{\\partial B} \\mathbb{E}\\big[ \\| s_t - B h_t \\|_2^2 \\big] = -2 \\, \\mathbb{E}\\big[ s_t h_t^{\\top} \\big] + 2 \\, B \\, \\mathbb{E}\\big[ h_t h_t^{\\top} \\big] = -2 C_{sh} + 2 B C_{hh} = 0,\n$$\nwhich implies\n$$\nB C_{hh} = C_{sh} \\quad \\Longrightarrow \\quad B = C_{sh} C_{hh}^{-1}.\n$$\nTherefore, the residualized stimulus that is orthogonal to the history predictor subspace in the least-squares sense is\n$$\ns_t^{\\perp} = s_t - C_{sh} C_{hh}^{-1} h_t.\n$$\nBy construction, this residual satisfies\n$$\n\\mathbb{E}\\big[ s_t^{\\perp} h_t^{\\top} \\big] = \\mathbb{E}\\big[ s_t h_t^{\\top} \\big] - C_{sh} C_{hh}^{-1} \\mathbb{E}\\big[ h_t h_t^{\\top} \\big] = C_{sh} - C_{sh} C_{hh}^{-1} C_{hh} = 0,\n$$\nso $s_t^{\\perp}$ is orthogonal to $h_t$ in the second-order sense. Computing STC on $s_t^{\\perp}$ therefore removes variance components in $s_t$ that are linearly predictable from $h_t$, aligning with subspace methods that isolate stimulus-informative dimensions uncontaminated by history.\n\nWe now analyze a mis-specified residualization that uses only the first history component $u_t = h_{1,t}$. The one-predictor least-squares solution is\n$$\nB_u = \\arg\\min_{b \\in \\mathbb{R}^{3 \\times 1}} \\mathbb{E}\\big[ \\| s_t - b \\, u_t \\|_2^2 \\big].\n$$\nThe normal equations reduce to\n$$\nb \\, \\mathbb{E}\\big[ u_t^2 \\big] = \\mathbb{E}\\big[ s_t u_t \\big] \\quad \\Longrightarrow \\quad B_u = C_{s u} C_{u u}^{-1},\n$$\nwhere $C_{s u} \\in \\mathbb{R}^{3 \\times 1}$ is the first column of $C_{sh}$ and $C_{u u} \\in \\mathbb{R}$ is the $(1,1)$ entry of $C_{hh}$. The residualized stimulus is\n$$\ns_t^{\\perp u} = s_t - C_{s u} C_{u u}^{-1} u_t.\n$$\nThe cross-covariance between $s_t^{\\perp u}$ and the omitted predictor $v_t = h_{2,t}$ is then\n$$\nC_{s^{\\perp u} v} = \\mathbb{E}\\big[ s_t^{\\perp u} v_t \\big] = \\mathbb{E}\\big[ s_t v_t \\big] - C_{s u} C_{u u}^{-1} \\mathbb{E}\\big[ u_t v_t \\big] = C_{s v} - C_{s u} C_{u u}^{-1} C_{u v},\n$$\nwhere $C_{s v} \\in \\mathbb{R}^{3 \\times 1}$ is the second column of $C_{sh}$ and $C_{u v}$ is the $(1,2)$ entry of $C_{hh}$. This expression quantifies the residual confounding due to omitted history predictors.\n\nWe define the residual confounding index\n$$\n\\gamma \\equiv \\frac{\\|C_{s^{\\perp u} v}\\|_{2}}{\\|C_{s v}\\|_{2}},\n$$\nwhich is dimensionless and lies in $[0,1]$ when residualization reduces the correlation with $v_t$.\n\nUsing the provided covariances,\n$$\nC_{s u} = \\begin{pmatrix} 0.4 \\\\ 0.1 \\\\ 0.2 \\end{pmatrix}, \\quad\nC_{s v} = \\begin{pmatrix} 0.2 \\\\ 0.3 \\\\ 0.1 \\end{pmatrix}, \\quad\nC_{u u} = 1.0, \\quad\nC_{u v} = 0.3.\n$$\nThus\n$$\nC_{s^{\\perp u} v} = C_{s v} - C_{s u} C_{u u}^{-1} C_{u v} = \\begin{pmatrix} 0.2 \\\\ 0.3 \\\\ 0.1 \\end{pmatrix} - 0.3 \\begin{pmatrix} 0.4 \\\\ 0.1 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} 0.08 \\\\ 0.27 \\\\ 0.04 \\end{pmatrix}.\n$$\nCompute the squared norms:\n$$\n\\|C_{s v}\\|_2^2 = 0.2^2 + 0.3^2 + 0.1^2 = 0.14, \\quad\n\\|C_{s^{\\perp u} v}\\|_2^2 = 0.08^2 + 0.27^2 + 0.04^2 = 0.0809.\n$$\nTherefore,\n$$\n\\gamma = \\frac{\\|C_{s^{\\perp u} v}\\|_{2}}{\\|C_{s v}\\|_{2}} = \\sqrt{\\frac{0.0809}{0.14}} = \\sqrt{\\frac{809}{1400}} \\approx 0.760169733\\ldots\n$$\nRounded to four significant figures, the residual confounding index is $0.7602$.",
            "answer": "$$\\boxed{0.7602}$$"
        },
        {
            "introduction": "After estimating a relevant subspace using STC, a critical step—especially in simulation studies with known ground truth—is to quantify the accuracy of the estimate. How can we measure the \"distance\" or \"discrepancy\" between the estimated subspace and the true one? This practice introduces the concept of principal angles, a rigorous and geometrically intuitive method for comparing subspaces, which are derived from the singular value decomposition of the cross-basis matrix .",
            "id": "4021267",
            "problem": "Consider a neuron driven by high-dimensional, Gaussian-whitened stimuli whose relevant feature space is low-dimensional, as commonly modeled in spike-triggered covariance (STC). After whitening, the ground-truth relevant subspace is spanned by an orthonormal basis matrix $U \\in \\mathbb{R}^{4 \\times 2}$, and the STC estimate yields an orthonormal basis matrix $\\hat U \\in \\mathbb{R}^{4 \\times 2}$. Suppose\n$$\nU = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0 \\\\\n0  0\n\\end{pmatrix}, \\quad\n\\hat U = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{3}} \\\\\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\sqrt{\\frac{2}{3}}\n\\end{pmatrix}.\n$$\nStarting from the definition of principal angles between subspaces as the sequence $\\{\\theta_i\\}$ obtained by recursively maximizing the cosine of the angle between unit vectors in each subspace under mutual orthogonality constraints, and using well-tested properties of orthonormal bases and the Singular Value Decomposition (SVD) of the cross-basis matrix $U^{\\top}\\hat U$, derive the relationship needed to compute the principal angles $\\{\\theta_i\\}$ from $U$ and $\\hat U$. Then, define a quantitative subspace discrepancy $d$ as the square root of the sum of the squared sines of these principal angles and compute its value for the matrices above.\n\nExpress your final answer for $d$ as a real number, and round your answer to four significant figures. No units are required.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective, permitting a formal solution. It asks for the derivation of a method to compute principal angles between two subspaces from their orthonormal bases and then to calculate a specific discrepancy metric.\n\nLet the two subspaces be denoted by $S_U = \\text{span}(U)$ and $S_{\\hat{U}} = \\text{span}(\\hat{U})$, where $U \\in \\mathbb{R}^{n \\times k}$ and $\\hat{U} \\in \\mathbb{R}^{n \\times k}$ are matrices with orthonormal columns that form bases for their respective $k$-dimensional subspaces. Here, $n=4$ and $k=2$.\n\nThe principal angles $\\{\\theta_i\\}_{i=1}^k$ between these two subspaces are defined recursively. The first principal angle, $\\theta_1$, is the smallest angle between any pair of unit vectors, one from each subspace. Its cosine is given by the maximization:\n$$ \\cos(\\theta_1) = \\max_{\\substack{\\mathbf{u} \\in S_U, \\, ||\\mathbf{u}||=1 \\\\ \\mathbf{\\hat{u}} \\in S_{\\hat{U}}, \\, ||\\mathbf{\\hat{u}}||=1}} \\mathbf{u}^\\top \\mathbf{\\hat{u}} $$\nAny vector $\\mathbf{u} \\in S_U$ can be written as a linear combination of the basis vectors in $U$, i.e., $\\mathbf{u} = U\\mathbf{x}$ for some coefficient vector $\\mathbf{x} \\in \\mathbb{R}^k$. Since the columns of $U$ are orthonormal, $||\\mathbf{u}||^2 = (U\\mathbf{x})^\\top(U\\mathbf{x}) = \\mathbf{x}^\\top U^\\top U \\mathbf{x} = \\mathbf{x}^\\top I_k \\mathbf{x} = ||\\mathbf{x}||^2$. Thus, the condition $||\\mathbf{u}||=1$ implies $||\\mathbf{x}||=1$. Similarly, for $\\mathbf{\\hat{u}} \\in S_{\\hat{U}}$, we can write $\\mathbf{\\hat{u}} = \\hat{U}\\mathbf{y}$ with $||\\mathbf{y}||=1$.\n\nSubstituting these into the maximization problem, we get:\n$$ \\cos(\\theta_1) = \\max_{||\\mathbf{x}||=1, ||\\mathbf{y}||=1} (U\\mathbf{x})^\\top (\\hat{U}\\mathbf{y}) = \\max_{||\\mathbf{x}||=1, ||\\mathbf{y}||=1} \\mathbf{x}^\\top (U^\\top \\hat{U}) \\mathbf{y} $$\nLet $M = U^\\top \\hat{U}$. The expression $\\mathbf{x}^\\top M \\mathbf{y}$ is a bilinear form. The maximum value of this form for unit vectors $\\mathbf{x}$ and $\\mathbf{y}$ is, by definition, the largest singular value of the matrix $M$, denoted $\\sigma_1(M)$.\n\nThe subsequent principal angles are found by repeating this process on the orthogonal complements of the previously found principal vectors. The general relationship, which is a fundamental result of linear algebra, states that the cosines of the principal angles $\\{\\theta_i\\}$ are the singular values of the cross-basis matrix $M = U^\\top \\hat{U}$.\n$$ \\cos(\\theta_i) = \\sigma_i(U^\\top \\hat{U}) \\quad \\text{for } i=1, \\dots, k $$\nwhere the singular values are ordered $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$.\n\nThe problem defines a subspace discrepancy metric $d$ as the square root of the sum of the squared sines of these principal angles:\n$$ d = \\sqrt{\\sum_{i=1}^k \\sin^2(\\theta_i)} $$\nUsing the identity $\\sin^2(\\theta_i) + \\cos^2(\\theta_i) = 1$ and the relationship $\\cos(\\theta_i) = \\sigma_i$, we can express $d$ in terms of the singular values:\n$$ d = \\sqrt{\\sum_{i=1}^k (1 - \\cos^2(\\theta_i))} = \\sqrt{\\sum_{i=1}^k (1 - \\sigma_i^2)} $$\nNow, we compute this value for the given matrices:\n$$ U = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}, \\quad \\hat{U} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  0 \\\\ 0  \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{2}}  0 \\\\ 0  \\sqrt{\\frac{2}{3}} \\end{pmatrix} $$\nFirst, we compute the matrix $M = U^\\top \\hat{U}$. The transpose of $U$ is:\n$$ U^\\top = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\end{pmatrix} $$\nThe product is:\n$$ M = U^\\top \\hat{U} = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  0 \\\\ 0  \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{2}}  0 \\\\ 0  \\sqrt{\\frac{2}{3}} \\end{pmatrix} = \\begin{pmatrix} (1)(\\frac{1}{\\sqrt{2}}) + (0)(0)  (1)(0) + (0)(\\frac{1}{\\sqrt{3}}) \\\\ (0)(\\frac{1}{\\sqrt{2}}) + (1)(0)  (0)(0) + (1)(\\frac{1}{\\sqrt{3}}) \\end{pmatrix} $$\n$$ M = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  0 \\\\ 0  \\frac{1}{\\sqrt{3}} \\end{pmatrix} $$\nThe matrix $M$ is a diagonal matrix. The singular values of a diagonal matrix are the absolute values of its diagonal entries. Therefore, the singular values are:\n$$ \\sigma_1 = \\frac{1}{\\sqrt{2}}, \\quad \\sigma_2 = \\frac{1}{\\sqrt{3}} $$\nNote that $\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{3}}$, so this ordering is correct.\n\nNow we can compute the discrepancy $d^2$:\n$$ d^2 = \\sum_{i=1}^{2} (1 - \\sigma_i^2) = (1 - \\sigma_1^2) + (1 - \\sigma_2^2) $$\n$$ d^2 = \\left(1 - \\left(\\frac{1}{\\sqrt{2}}\\right)^2\\right) + \\left(1 - \\left(\\frac{1}{\\sqrt{3}}\\right)^2\\right) $$\n$$ d^2 = \\left(1 - \\frac{1}{2}\\right) + \\left(1 - \\frac{1}{3}\\right) = \\frac{1}{2} + \\frac{2}{3} $$\n$$ d^2 = \\frac{3}{6} + \\frac{4}{6} = \\frac{7}{6} $$\nThe discrepancy $d$ is the square root of this value:\n$$ d = \\sqrt{\\frac{7}{6}} $$\nTo provide the numerical answer, we compute the value and round to four significant figures:\n$$ d = \\sqrt{\\frac{7}{6}} \\approx \\sqrt{1.166666...} \\approx 1.080123... $$\nRounding to four significant figures gives $1.080$.",
            "answer": "$$\\boxed{1.080}$$"
        }
    ]
}