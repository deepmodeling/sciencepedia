{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of modeling spike trains is connecting theoretical models to experimental data. The simplest renewal model is the Poisson process, characterized by memoryless, exponentially distributed interspike intervals (ISIs). This first exercise provides a foundational skill: using the powerful method of maximum likelihood estimation to infer the model's single parameter, the firing rate $\\lambda$, directly from a sequence of observed ISIs. Mastering this derivation is the first step toward statistically fitting any renewal process model to neural data .",
            "id": "4014954",
            "problem": "A single neuron is modeled as a stationary renewal process in which each interspike interval (ISI) is an independent and identically distributed positive random variable with an exponential probability density function parameterized by a rate parameter $\\lambda$. You observe $n$ ISIs, denoted by $\\{x_{i}\\}_{i=1}^{n}$, under a constant stimulus drive. Assume that the ISIs are independent samples from the exponential distribution with unknown rate $\\lambda$. Using only the following foundational elements: the definition of a renewal process, the exponential probability density function, the product rule for independent likelihoods, the definition of the Maximum Likelihood Estimator (MLE), and the large-sample theory for MLEs based on Fisher information, derive the MLE for the exponential rate parameter and obtain its asymptotic variance. Express your final answer symbolically, without numerical approximation, and do not include units.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded, well-posed, objective, and provides a complete and consistent set of givens to derive the requested quantities. The problem is a standard exercise in statistical inference applied to a canonical model in computational neuroscience.\n\nThe task is to derive the Maximum Likelihood Estimator (MLE) for the rate parameter $\\lambda$ of an exponential distribution and its asymptotic variance, given a set of $n$ independent and identically distributed (i.i.d.) interspike intervals (ISIs), $\\{x_i\\}_{i=1}^n$.\n\nFirst, we establish the probability density function (PDF) for a single ISI, $x$. Given that the ISIs follow an exponential distribution with rate parameter $\\lambda$, the PDF is:\n$$p(x|\\lambda) = \\lambda \\exp(-\\lambda x) \\quad \\text{for} \\quad x > 0$$\nThe problem states that the neuron is modeled as a stationary renewal process and the ISIs are independent samples. Therefore, the joint probability of observing the entire sequence of $n$ ISIs, which is the likelihood function $L(\\lambda)$, is the product of the individual PDFs:\n$$L(\\lambda | \\{x_i\\}_{i=1}^n) = \\prod_{i=1}^{n} p(x_i|\\lambda) = \\prod_{i=1}^{n} \\lambda \\exp(-\\lambda x_i)$$\nThis can be simplified as:\n$$L(\\lambda) = \\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^{n} x_i\\right)$$\nTo find the MLE, we must find the value of $\\lambda$ that maximizes this likelihood function. It is mathematically more convenient to maximize the natural logarithm of the likelihood function, known as the log-likelihood function, $\\mathcal{L}(\\lambda)$. Since the logarithm is a monotonically increasing function, maximizing $\\mathcal{L}(\\lambda)$ is equivalent to maximizing $L(\\lambda)$.\n$$\\mathcal{L}(\\lambda) = \\ln(L(\\lambda)) = \\ln\\left(\\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^{n} x_i\\right)\\right)$$\nUsing the properties of logarithms, we get:\n$$\\mathcal{L}(\\lambda) = \\ln(\\lambda^n) + \\ln\\left(\\exp\\left(-\\lambda \\sum_{i=1}^{n} x_i\\right)\\right) = n \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} x_i$$\nTo find the maximum, we compute the first derivative of the log-likelihood with respect to $\\lambda$ and set it to zero.\n$$\\frac{d\\mathcal{L}}{d\\lambda} = \\frac{d}{d\\lambda}\\left(n \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} x_i\\right) = \\frac{n}{\\lambda} - \\sum_{i=1}^{n} x_i$$\nSetting this derivative to zero gives the equation for the MLE, denoted as $\\hat{\\lambda}_{\\text{MLE}}$:\n$$\\frac{n}{\\hat{\\lambda}_{\\text{MLE}}} - \\sum_{i=1}^{n} x_i = 0$$\nSolving for $\\hat{\\lambda}_{\\text{MLE}}$ yields:\n$$\\hat{\\lambda}_{\\text{MLE}} = \\frac{n}{\\sum_{i=1}^{n} x_i} = \\left(\\frac{1}{n}\\sum_{i=1}^{n} x_i\\right)^{-1}$$\nThis is the first part of the answer: the MLE for the rate parameter $\\lambda$ is the reciprocal of the sample mean of the ISIs.\n\nNext, we derive the asymptotic variance of this estimator. According to the large-sample theory for MLEs, the variance of $\\hat{\\lambda}_{\\text{MLE}}$ for large $n$ approaches the CramÃ©r-Rao lower bound, which is the inverse of the Fisher information, $I(\\lambda)$. The Fisher information for $n$ i.i.d. samples, $I_n(\\lambda)$, is $n$ times the Fisher information for a single sample, $I_1(\\lambda)$.\n$$\\mathrm{Var}(\\hat{\\lambda}_{\\text{MLE}}) \\approx \\frac{1}{I_n(\\lambda)} = \\frac{1}{n I_1(\\lambda)}$$\nThe Fisher information for a single observation, $I_1(\\lambda)$, is defined as the negative expected value of the second derivative of the log-PDF.\n$$I_1(\\lambda) = -E\\left[\\frac{d^2}{d\\lambda^2} \\ln(p(x|\\lambda))\\right]$$\nFirst, we find the log-PDF for a single observation $x$:\n$$\\ln(p(x|\\lambda)) = \\ln(\\lambda \\exp(-\\lambda x)) = \\ln(\\lambda) - \\lambda x$$\nThe first derivative with respect to $\\lambda$ is:\n$$\\frac{d}{d\\lambda} \\ln(p(x|\\lambda)) = \\frac{1}{\\lambda} - x$$\nThe second derivative is:\n$$\\frac{d^2}{d\\lambda^2} \\ln(p(x|\\lambda)) = \\frac{d}{d\\lambda}\\left(\\frac{1}{\\lambda} - x\\right) = -\\frac{1}{\\lambda^2}$$\nNow, we compute the Fisher information by taking the negative expectation of this quantity. Since $-\\frac{1}{\\lambda^2}$ is a constant with respect to the random variable $x$, its expectation is itself.\n$$I_1(\\lambda) = -E\\left[-\\frac{1}{\\lambda^2}\\right] = - \\left(-\\frac{1}{\\lambda^2}\\right) = \\frac{1}{\\lambda^2}$$\nThe Fisher information for $n$ samples is:\n$$I_n(\\lambda) = n I_1(\\lambda) = \\frac{n}{\\lambda^2}$$\nFinally, the asymptotic variance of the MLE is the inverse of $I_n(\\lambda)$:\n$$\\mathrm{Var}(\\hat{\\lambda}_{\\text{MLE}}) \\approx [I_n(\\lambda)]^{-1} = \\left(\\frac{n}{\\lambda^2}\\right)^{-1} = \\frac{\\lambda^2}{n}$$\nThis is the second part of the answer. The asymptotic variance of the MLE depends on the true value of the parameter $\\lambda$ and decreases with the number of samples $n$.\n\nThe two derived quantities are the MLE for $\\lambda$ and its asymptotic variance.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\left(\\frac{1}{n} \\sum_{i=1}^{n} x_i\\right)^{-1}  \\frac{\\lambda^2}{n} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While the Poisson process is a crucial baseline, it lacks key biophysical features of real neurons. One of the most fundamental is the refractory period, a brief time after a spike during which a neuron cannot fire again. This practice moves beyond the simple constant-hazard assumption of the Poisson model to incorporate an absolute refractory period. By constructing the ISI distribution from a piecewise-constant hazard function, you will see how biophysical constraints directly shape the statistical properties of a spike train .",
            "id": "4014967",
            "problem": "A neuron is modeled as a renewal process in which the conditional intensity (hazard) function for the interspike interval is piecewise-constant with an absolute refractory period. Specifically, the hazard is given by $h(t)=0$ for $td$ and $h(t)=\\alpha$ for $t \\ge d$, where $d0$ represents a refractory duration and $\\alpha0$ is a constant rate parameter. Starting from the definition of the hazard for a renewal process and the standard relationships among the hazard, survival function, and probability density function, derive the interspike interval probability density function $f_{T}(t)$ for all $t \\ge 0$ and compute the mean interspike interval $\\mathbb{E}[T]$. Provide your final answer as two expressions: first the closed-form $f_{T}(t)$, and second the scalar $\\mathbb{E}[T]$. Do not provide intermediate steps in the final answer.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded, well-posed, objective, and contains sufficient information to derive unique and meaningful solutions for the requested quantities. The model described is a standard representation of a neuron with an absolute refractory period, often referred to as a shifted Poisson process or a Poisson process with dead time, a fundamental concept in computational neuroscience and renewal theory.\n\nThe solution proceeds by first deriving the interspike interval (ISI) probability density function $f_{T}(t)$ from the given hazard function $h(t)$, and then calculating the mean ISI, $\\mathbb{E}[T]$.\n\nThe fundamental relationship between the hazard function $h(t)$, the survival function $S(t) = P(T > t)$, and the probability density function $f_T(t)$ for a continuous random variable $T \\ge 0$ is given by:\n$$S(t) = \\exp\\left(-\\int_0^t h(\\tau) \\, d\\tau\\right)$$\nand\n$$f_T(t) = h(t) S(t)$$\n\nThe hazard function is given as a piecewise function:\n$$h(t) = \\begin{cases} 0  \\text{for } t  d \\\\ \\alpha  \\text{for } t \\ge d \\end{cases}$$\nwhere $d > 0$ and $\\alpha > 0$.\n\nFirst, we calculate the cumulative hazard, $H(t) = \\int_0^t h(\\tau) \\, d\\tau$. We must evaluate this integral piecewise.\n\nCase 1: $0 \\le t  d$\nIn this interval, $h(\\tau) = 0$ for all $\\tau \\in [0, t)$.\n$$H(t) = \\int_0^t h(\\tau) \\, d\\tau = \\int_0^t 0 \\, d\\tau = 0$$\n\nCase 2: $t \\ge d$\nThe integral must be split at the point of discontinuity, $d$.\n$$H(t) = \\int_0^t h(\\tau) \\, d\\tau = \\int_0^d h(\\tau) \\, d\\tau + \\int_d^t h(\\tau) \\, d\\tau$$\n$$H(t) = \\int_0^d 0 \\, d\\tau + \\int_d^t \\alpha \\, d\\tau = 0 + \\alpha[\\tau]_d^t = \\alpha(t-d)$$\n\nNow, we can find the survival function $S(t) = \\exp(-H(t))$.\n\nFor $0 \\le t  d$:\n$$S(t) = \\exp(-0) = 1$$\nThis result is logical: the probability of the neuron not spiking before the end of the absolute refractory period $d$ is $1$.\n\nFor $t \\ge d$:\n$$S(t) = \\exp(-\\alpha(t-d))$$\n\nCombining these, the survival function is:\n$$S(t) = \\begin{cases} 1  \\text{for } 0 \\le t  d \\\\ \\exp(-\\alpha(t-d))  \\text{for } t \\ge d \\end{cases}$$\n\nNext, we derive the probability density function $f_T(t)$ using the relation $f_T(t) = h(t)S(t)$.\n\nFor $0 \\le t  d$:\n$$f_T(t) = h(t)S(t) = 0 \\cdot 1 = 0$$\nThis is also logical, as the probability density of a spike occurring during the absolute refractory period is zero.\n\nFor $t \\ge d$:\n$$f_T(t) = h(t)S(t) = \\alpha \\exp(-\\alpha(t-d))$$\n\nTherefore, the complete ISI probability density function is:\n$$f_T(t) = \\begin{cases} 0  \\text{for } t  d \\\\ \\alpha \\exp(-\\alpha(t-d))  \\text{for } t \\ge d \\end{cases}$$\nThis is the probability density function of a shifted exponential distribution.\n\nFinally, we compute the mean interspike interval, $\\mathbb{E}[T]$. The mean can be calculated by integrating the survival function over its domain:\n$$\\mathbb{E}[T] = \\int_0^\\infty S(t) \\, dt$$\nWe split the integral at $t=d$:\n$$\\mathbb{E}[T] = \\int_0^d S(t) \\, dt + \\int_d^\\infty S(t) \\, dt$$\nSubstituting the expressions for $S(t)$:\n$$\\mathbb{E}[T] = \\int_0^d 1 \\, dt + \\int_d^\\infty \\exp(-\\alpha(t-d)) \\, dt$$\nThe first integral is straightforward:\n$$\\int_0^d 1 \\, dt = [t]_0^d = d$$\nFor the second integral, we can use the substitution $u = t-d$, which implies $du = dt$. The limits of integration change from $t=d \\rightarrow u=0$ and $t \\to \\infty \\rightarrow u \\to \\infty$.\n$$\\int_d^\\infty \\exp(-\\alpha(t-d)) \\, dt = \\int_0^\\infty \\exp(-\\alpha u) \\, du$$\nThis is the integral of the kernel of an exponential distribution.\n$$\\int_0^\\infty \\exp(-\\alpha u) \\, du = \\left[-\\frac{1}{\\alpha}\\exp(-\\alpha u)\\right]_0^\\infty = \\left( \\lim_{u\\to\\infty} -\\frac{1}{\\alpha}\\exp(-\\alpha u) \\right) - \\left(-\\frac{1}{\\alpha}\\exp(0)\\right) = 0 - \\left(-\\frac{1}{\\alpha}\\right) = \\frac{1}{\\alpha}$$\nCombining the results of the two integrals gives the mean ISI:\n$$\\mathbb{E}[T] = d + \\frac{1}{\\alpha}$$\nThis result is intuitive: the mean ISI is the sum of the fixed refractory duration $d$ and the mean interval of the subsequent Poisson process, which is $1/\\alpha$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\begin{cases} 0  t  d \\\\ \\alpha \\exp(-\\alpha (t-d))  t \\ge d \\end{cases}  d + \\frac{1}{\\alpha} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The spiking patterns of many neurons are more regular than predicted by a Poisson process. To capture this regularity, we can generalize the exponential ISI distribution to the more flexible Gamma distribution. This advanced practice builds on the principles of maximum likelihood estimation to fit a two-parameter model ($k$ and $\\theta$) to data, which can describe a wide range of firing statistics from bursting to regular spiking. This exercise will introduce you to the mathematical tools, such as the digamma function, that are essential for working with more sophisticated and realistic renewal process models .",
            "id": "4014931",
            "problem": "Consider a renewal process model for a single neuron's spike train in which the interspike intervals (ISIs) are independent and identically distributed according to a Gamma distribution with shape parameter $k0$ and scale parameter $\\theta0$. You observe $n$ complete interspike intervals $\\{x_{i}\\}_{i=1}^{n}$ from a long recording during which stationarity holds. Assume no censoring and ignore edge effects. Let $\\Gamma(\\cdot)$ denote the Gamma function, and let $\\psi(\\cdot)$ denote the digamma function, defined as $\\psi(k) = \\frac{d}{dk}\\ln\\Gamma(k)$.\n\nTasks:\n- Starting from the independence of interspike intervals and the Gamma probability density function $f(x \\mid k,\\theta)$, derive the log-likelihood $\\ell(k,\\theta;\\mathbf{x})$ for the observed interspike intervals $\\mathbf{x}=(x_{1},\\dots,x_{n})$.\n- Compute the components of the score vector, $\\frac{\\partial \\ell}{\\partial k}$ and $\\frac{\\partial \\ell}{\\partial \\theta}$, and set them equal to zero to obtain the maximum likelihood estimation (MLE) equations for $k$ and $\\theta$. Express $\\theta$ explicitly as a function of $k$ and the data, and reduce the $k$-equation to an implicit scalar equation in $k$ that highlights the role of the digamma function.\n- Define the sample mean $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$ and the sample mean of logarithms $m_{\\ln}=\\frac{1}{n}\\sum_{i=1}^{n}\\ln x_{i}$, and show that the $k$-equation can be written in terms of $\\psi(k)$, $\\ln k$, $\\bar{x}$, and $m_{\\ln}$.\n\nAnswer specification:\n- Your final boxed answer must be a single row matrix with three entries, in this order: the log-likelihood $\\ell(k,\\theta;\\mathbf{x})$ as a single analytic expression; the score component with respect to $k$ written as a single analytic expression (do not include an equality); and the score component with respect to $\\theta$ written as a single analytic expression (do not include an equality).\n- Do not include any equations or inequalities in the final boxed answer; write only the expressions. No numerical approximation or units are required.",
            "solution": "The problem is valid and well-posed, providing a standard exercise in statistical inference for a key model in computational neuroscience.\n\nThe probability density function (PDF) of a Gamma distribution with shape parameter $k$ and scale parameter $\\theta$ is given by:\n$$f(x \\mid k, \\theta) = \\frac{x^{k-1} \\exp(-x/\\theta)}{\\Gamma(k) \\theta^k}$$\nfor $x > 0$, $k > 0$, and $\\theta > 0$.\n\n**Task 1: Derive the log-likelihood $\\ell(k, \\theta; \\mathbf{x})$**\n\nSince the interspike intervals $\\{x_i\\}_{i=1}^n$ are independent and identically distributed, the likelihood function $L(k, \\theta; \\mathbf{x})$ is the product of the individual PDFs:\n$$L(k, \\theta; \\mathbf{x}) = \\prod_{i=1}^{n} f(x_i \\mid k, \\theta) = \\prod_{i=1}^{n} \\left( \\frac{x_i^{k-1} \\exp(-x_i/\\theta)}{\\Gamma(k) \\theta^k} \\right)$$\nThe log-likelihood, $\\ell(k, \\theta; \\mathbf{x})$, is the natural logarithm of the likelihood function:\n$$\\ell(k, \\theta; \\mathbf{x}) = \\ln(L(k, \\theta; \\mathbf{x})) = \\ln \\left( \\prod_{i=1}^{n} \\frac{x_i^{k-1} \\exp(-x_i/\\theta)}{\\Gamma(k) \\theta^k} \\right)$$\nUsing the properties of logarithms, we can write this as a sum:\n$$\\ell(k, \\theta; \\mathbf{x}) = \\sum_{i=1}^{n} \\ln \\left( \\frac{x_i^{k-1} \\exp(-x_i/\\theta)}{\\Gamma(k) \\theta^k} \\right)$$\n$$\\ell(k, \\theta; \\mathbf{x}) = \\sum_{i=1}^{n} \\left[ \\ln(x_i^{k-1}) + \\ln(\\exp(-x_i/\\theta)) - \\ln(\\Gamma(k) \\theta^k) \\right]$$\n$$\\ell(k, \\theta; \\mathbf{x}) = \\sum_{i=1}^{n} \\left[ (k-1)\\ln x_i - \\frac{x_i}{\\theta} - \\ln\\Gamma(k) - k\\ln\\theta \\right]$$\nWe can separate the summation over the terms:\n$$\\ell(k, \\theta; \\mathbf{x}) = (k-1)\\sum_{i=1}^{n}\\ln x_i - \\frac{1}{\\theta}\\sum_{i=1}^{n}x_i - n\\ln\\Gamma(k) - nk\\ln\\theta$$\nThis is the log-likelihood function.\n\n**Task 2: Compute the score components and derive the MLE equations**\n\nThe score vector consists of the partial derivatives of the log-likelihood with respect to the parameters $k$ and $\\theta$.\n\n- **Component with respect to $k$**:\n$$\\frac{\\partial \\ell}{\\partial k} = \\frac{\\partial}{\\partial k} \\left( (k-1)\\sum_{i=1}^{n}\\ln x_i - \\frac{1}{\\theta}\\sum_{i=1}^{n}x_i - n\\ln\\Gamma(k) - nk\\ln\\theta \\right)$$\n$$\\frac{\\partial \\ell}{\\partial k} = \\sum_{i=1}^{n}\\ln x_i - 0 - n\\frac{d}{dk}\\ln\\Gamma(k) - n\\ln\\theta$$\nUsing the definition of the digamma function, $\\psi(k) = \\frac{d}{dk}\\ln\\Gamma(k)$:\n$$\\frac{\\partial \\ell}{\\partial k} = \\sum_{i=1}^{n}\\ln x_i - n\\psi(k) - n\\ln\\theta$$\n\n- **Component with respect to $\\theta$**:\n$$\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( (k-1)\\sum_{i=1}^{n}\\ln x_i - \\frac{1}{\\theta}\\sum_{i=1}^{n}x_i - n\\ln\\Gamma(k) - nk\\ln\\theta \\right)$$\n$$\\frac{\\partial \\ell}{\\partial \\theta} = 0 - \\left(\\sum_{i=1}^{n}x_i\\right)\\left(-\\frac{1}{\\theta^2}\\right) - 0 - nk\\left(\\frac{1}{\\theta}\\right)$$\n$$\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{1}{\\theta^2}\\sum_{i=1}^{n}x_i - \\frac{nk}{\\theta}$$\n\nTo find the MLE estimates, we set these partial derivatives to zero. Let $\\hat{k}$ and $\\hat{\\theta}$ be the MLEs.\n1.  $\\frac{\\partial \\ell}{\\partial \\theta} = 0 \\implies \\frac{1}{\\hat{\\theta}^2}\\sum_{i=1}^{n}x_i - \\frac{n\\hat{k}}{\\hat{\\theta}} = 0$\n2.  $\\frac{\\partial \\ell}{\\partial k} = 0 \\implies \\sum_{i=1}^{n}\\ln x_i - n\\psi(\\hat{k}) - n\\ln\\hat{\\theta} = 0$\n\nFrom equation (1), we can solve for $\\hat{\\theta}$:\n$$\\frac{1}{\\hat{\\theta}^2}\\sum_{i=1}^{n}x_i = \\frac{n\\hat{k}}{\\hat{\\theta}}$$\n$$\\sum_{i=1}^{n}x_i = n\\hat{k}\\hat{\\theta}$$\n$$\\hat{\\theta} = \\frac{\\sum_{i=1}^{n}x_i}{n\\hat{k}} = \\frac{\\bar{x}}{\\hat{k}}$$\nwhere $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$ is the sample mean.\n\nNext, we substitute this expression for $\\hat{\\theta}$ into equation (2):\n$$\\sum_{i=1}^{n}\\ln x_i - n\\psi(\\hat{k}) - n\\ln\\left(\\frac{\\bar{x}}{\\hat{k}}\\right) = 0$$\nDividing by $n$:\n$$\\frac{1}{n}\\sum_{i=1}^{n}\\ln x_i - \\psi(\\hat{k}) - \\ln\\left(\\frac{\\bar{x}}{\\hat{k}}\\right) = 0$$\n$$\\frac{1}{n}\\sum_{i=1}^{n}\\ln x_i - \\psi(\\hat{k}) - (\\ln\\bar{x} - \\ln\\hat{k}) = 0$$\nRearranging the terms gives the implicit equation for $\\hat{k}$:\n$$\\ln\\hat{k} - \\psi(\\hat{k}) = \\ln\\bar{x} - \\frac{1}{n}\\sum_{i=1}^{n}\\ln x_i$$\n\n**Task 3: Rewrite the k-equation**\n\nUsing the definitions for the sample mean $\\bar{x}$ and the sample mean of logarithms $m_{\\ln}$:\n$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$$\n$$m_{\\ln} = \\frac{1}{n}\\sum_{i=1}^{n}\\ln x_i$$\nThe implicit equation for $\\hat{k}$ becomes:\n$$\\ln\\hat{k} - \\psi(\\hat{k}) = \\ln(\\bar{x}) - m_{\\ln}$$\nThis successfully expresses the equation for $\\hat{k}$ in the required terms.\n\nThe final answer requires the three expressions: $\\ell(k, \\theta; \\mathbf{x})$, $\\frac{\\partial \\ell}{\\partial k}$, and $\\frac{\\partial \\ell}{\\partial \\theta}$, which are provided in the boxed answer below.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(k-1) \\sum_{i=1}^{n} \\ln x_{i} - \\frac{1}{\\theta} \\sum_{i=1}^{n} x_{i} - n \\ln \\Gamma(k) - n k \\ln \\theta  \\sum_{i=1}^{n} \\ln x_{i} - n \\psi(k) - n \\ln \\theta  \\frac{1}{\\theta^{2}} \\sum_{i=1}^{n} x_{i} - \\frac{nk}{\\theta}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}