{
    "hands_on_practices": [
        {
            "introduction": "At the heart of predictive coding lies the Bayesian brain hypothesis, which posits that the brain optimally integrates prior knowledge with incoming sensory data. This foundational exercise guides you through the core mathematical operation of this process: Bayesian belief updating for a simple linear-Gaussian model. By deriving the posterior distribution, you will see precisely how a new belief state emerges as a weighted average of the prior belief and the sensory evidence, where the weights are determined by their respective reliability or precision . This principle of precision-weighting is a cornerstone of predictive coding frameworks.",
            "id": "5052192",
            "problem": "Consider a single-latent-variable generative model commonly used to formalize Predictive Coding and Bayesian brain frameworks in neurobiology. A latent cause $x$ represents the brain’s prediction about a sensory input, and a measurement $y$ is the observed sensory signal. Assume a Gaussian prior on the latent cause, $x \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$, and a Gaussian likelihood for the observation given the cause, $y \\mid x \\sim \\mathcal{N}(x, \\sigma_{y}^{2})$, where $\\sigma_{0}^{2} > 0$ and $\\sigma_{y}^{2} > 0$ are known constants. Using Bayes’ rule and the well-tested fact that products of Gaussian densities yield another Gaussian density in the variable of interest, derive the posterior density $p(x \\mid y)$ in closed form and show that it is Gaussian. Then express the posterior mean and posterior variance using precision-weighted combinations of the prior mean and the observation. Define precision as the inverse of variance, so that $\\Pi_{0} = 1 / \\sigma_{0}^{2}$ and $\\Pi_{y} = 1 / \\sigma_{y}^{2}$. Provide the final expressions for the posterior mean and the posterior variance in terms of $\\mu_{0}$, $y$, $\\Pi_{0}$, and $\\Pi_{y}$. No numerical approximation is required; present exact symbolic expressions. The final answer must be a calculation and should be expressed as a single row matrix containing the posterior mean and posterior variance.",
            "solution": "The problem statement is evaluated and found to be valid. It is a well-posed problem in Bayesian statistics, grounded in established mathematical principles and directly applicable to the specified neurobiological framework of predictive coding. All necessary information is provided, and the terms are unambiguous.\n\nThe task is to derive the posterior probability density function (PDF) $p(x \\mid y)$ for a latent cause $x$ given an observation $y$. We are given the prior distribution for $x$ and the likelihood function for $y$ given $x$.\n\nThe prior is a Gaussian distribution:\n$$p(x) = \\mathcal{N}(x \\mid \\mu_{0}, \\sigma_{0}^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}}\\right)$$\nThe likelihood is also a Gaussian distribution:\n$$p(y \\mid x) = \\mathcal{N}(y \\mid x, \\sigma_{y}^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y}^{2}}} \\exp\\left(-\\frac{(y - x)^2}{2\\sigma_{y}^{2}}\\right)$$\nAccording to Bayes' rule, the posterior distribution is proportional to the product of the likelihood and the prior:\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\nSubstituting the given distributions, we have:\n$$p(x \\mid y) \\propto \\exp\\left(-\\frac{(y - x)^2}{2\\sigma_{y}^{2}}\\right) \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}}\\right)$$\nWe can combine the arguments of the exponential functions:\n$$p(x \\mid y) \\propto \\exp\\left( -\\frac{(y - x)^2}{2\\sigma_{y}^{2}} - \\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}} \\right)$$\nTo find the form of the posterior, we analyze the exponent, focusing on the terms that depend on $x$. Let $E(x)$ be the argument of the exponential:\n$$E(x) = -\\frac{1}{2} \\left( \\frac{(y - x)^2}{\\sigma_{y}^{2}} + \\frac{(x - \\mu_{0})^2}{\\sigma_{0}^{2}} \\right)$$\nExpanding the squared terms:\n$$E(x) = -\\frac{1}{2} \\left( \\frac{y^2 - 2yx + x^2}{\\sigma_{y}^{2}} + \\frac{x^2 - 2x\\mu_{0} + \\mu_{0}^2}{\\sigma_{0}^{2}} \\right)$$\nWe now group the terms by powers of $x$:\n$$E(x) = -\\frac{1}{2} \\left[ x^2 \\left(\\frac{1}{\\sigma_{0}^{2}} + \\frac{1}{\\sigma_{y}^{2}}\\right) - 2x \\left(\\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\frac{y}{\\sigma_{y}^{2}}\\right) + \\left(\\frac{\\mu_{0}^2}{\\sigma_{0}^{2}} + \\frac{y^2}{\\sigma_{y}^{2}}\\right) \\right]$$\nThe problem defines precision as the inverse of variance: $\\Pi_{0} = 1/\\sigma_{0}^{2}$ and $\\Pi_{y} = 1/\\sigma_{y}^{2}$. Substituting these into the expression:\n$$E(x) = -\\frac{1}{2} \\left[ x^2 (\\Pi_{0} + \\Pi_{y}) - 2x (\\Pi_{0}\\mu_{0} + \\Pi_{y}y) + (\\Pi_{0}\\mu_{0}^2 + \\Pi_{y}y^2) \\right]$$\nThis expression is a quadratic function of $x$. This implies that the posterior distribution $p(x \\mid y)$ is a Gaussian distribution, as stated in the problem. A generic Gaussian PDF for a variable $x$ with mean $\\mu_{\\text{post}}$ and variance $\\sigma_{\\text{post}}^2$ has an exponent of the form:\n$$-\\frac{(x - \\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2} = -\\frac{1}{2\\sigma_{\\text{post}}^2} (x^2 - 2x\\mu_{\\text{post}} + \\mu_{\\text{post}}^2) = -\\frac{1}{2} (\\Pi_{\\text{post}}x^2 - 2\\Pi_{\\text{post}}\\mu_{\\text{post}}x + \\text{const})$$\nwhere $\\Pi_{\\text{post}} = 1/\\sigma_{\\text{post}}^2$ is the posterior precision.\n\nBy comparing the coefficients of the powers of $x$ in our derived exponent $E(x)$ with the general form, we can identify the parameters of the posterior distribution.\nComparing the coefficients of the $x^2$ term:\n$$\\Pi_{\\text{post}} = \\Pi_{0} + \\Pi_{y}$$\nThis shows that the posterior precision is the sum of the prior precision and the likelihood precision. The posterior variance, $\\sigma_{\\text{post}}^2$, is the inverse of the posterior precision:\n$$\\sigma_{\\text{post}}^2 = \\frac{1}{\\Pi_{\\text{post}}} = \\frac{1}{\\Pi_{0} + \\Pi_{y}}$$\nNow, comparing the coefficients of the $x$ term:\n$$2\\Pi_{\\text{post}}\\mu_{\\text{post}} = 2(\\Pi_{0}\\mu_{0} + \\Pi_{y}y)$$\n$$\\mu_{\\text{post}} = \\frac{\\Pi_{0}\\mu_{0} + \\Pi_{y}y}{\\Pi_{\\text{post}}}$$\nSubstituting the expression for $\\Pi_{\\text{post}}$:\n$$\\mu_{\\text{post}} = \\frac{\\Pi_{0}\\mu_{0} + \\Pi_{y}y}{\\Pi_{0} + \\Pi_{y}}$$\nThe posterior mean is a precision-weighted average of the prior mean $\\mu_{0}$ and the observed data $y$. The terms in $E(x)$ that do not depend on $x$ are absorbed into the normalization constant of the posterior Gaussian PDF.\n\nThus, the posterior distribution $p(x \\mid y)$ is a Gaussian $\\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$ with the derived mean and variance.\n\nThe posterior mean is:\n$$\\mu_{\\text{post}} = \\frac{\\Pi_{0}\\mu_{0} + \\Pi_{y}y}{\\Pi_{0} + \\Pi_{y}}$$\nThe posterior variance is:\n$$\\sigma_{\\text{post}}^2 = \\frac{1}{\\Pi_{0} + \\Pi_{y}}$$\nThese are the final symbolic expressions required.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\Pi_{0} \\mu_{0} + \\Pi_{y} y}{\\Pi_{0} + \\Pi_{y}} & \\frac{1}{\\Pi_{0} + \\Pi_{y}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While linear models provide a crucial foundation, the brain must often infer causes from complex, nonlinear relationships. This practice moves from a static, one-shot update to the dynamic process of inference that defines predictive coding. You will work with a generative model where the sensory observation is a nonlinear function of the hidden cause, and derive the update rule that minimizes prediction error through gradient descent . This exercise makes tangible how neural activity could continuously adjust internal representations to better predict the sensory world.",
            "id": "4011082",
            "problem": "Consider a single-level nonlinear generative model used in predictive coding frameworks for brain modeling and computational neuroscience. An observation $y$ is generated from a latent state $x$ via a differentiable nonlinear observation function $g(x)$ with additive Gaussian noise. Assume the observation function is the logistic sigmoid $g(x) = \\frac{1}{1 + \\exp(-x)}$. The observation model is $y \\mid x \\sim \\mathcal{N}(g(x), \\Pi_{s}^{-1})$, where $\\Pi_{s} > 0$ is the sensory precision (inverse variance). The latent state has a Gaussian prior $x \\sim \\mathcal{N}(\\mu_{0}, R^{-1})$, where $R > 0$ is the prior precision. \n\nUsing the principle of Variational Free Energy (VFE) minimization within Predictive Coding (PC), one can update an estimate of $x$ by performing gradient ascent on the log joint density $\\ln p(y, x)$ with a small positive step size $\\gamma$, that is, $x \\leftarrow x + \\gamma \\nabla_{x} \\ln p(y, x)$.\n\nStarting from first principles of Gaussian likelihoods and priors, derive the gradient $\\nabla_{x} \\ln p(y, x)$ for this model in terms of $y$, $x$, $g(x)$, $g'(x)$, $\\Pi_{s}$, and $R$. Then, for the specific values $x = 0$, $y = 0.8$, $\\mu_{0} = 0.1$, $R = 5$, $\\Pi_{s} = 20$, and $\\gamma = 0.05$, compute the one-step updated state $x_{\\text{new}}$ explicitly. You must compute and use the exact derivative $g'(x)$ at the given $x$. Express your final answer as a single real number. No rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We can therefore proceed with the derivation and calculation.\n\nThe core of the problem is to compute the gradient of the log joint probability density, $\\nabla_{x} \\ln p(y, x)$, and then use it to perform a one-step update of the latent state estimate $x$. The joint probability $p(y, x)$ is given by the product of the likelihood $p(y|x)$ and the prior $p(x)$, based on the chain rule of probability:\n$$p(y, x) = p(y | x) p(x)$$\nTaking the natural logarithm of this expression, we get:\n$$\\ln p(y, x) = \\ln p(y | x) + \\ln p(x)$$\nWe will now define the explicit forms for the log-likelihood and log-prior.\n\nFirst, we consider the likelihood $p(y|x)$. The problem states that the observation $y$ is drawn from a normal distribution with mean $g(x)$ and precision $\\Pi_{s}$ (which corresponds to a variance of $\\Pi_{s}^{-1}$). The probability density function (PDF) is:\n$$p(y | x) = \\mathcal{N}(y; g(x), \\Pi_{s}^{-1}) = \\frac{1}{\\sqrt{2\\pi (\\Pi_{s}^{-1})}} \\exp\\left(-\\frac{(y - g(x))^2}{2 (\\Pi_{s}^{-1})}\\right)$$\nSimplifying the expression:\n$$p(y | x) = \\sqrt{\\frac{\\Pi_{s}}{2\\pi}} \\exp\\left(-\\frac{\\Pi_{s}}{2} (y - g(x))^2\\right)$$\nThe log-likelihood is therefore:\n$$\\ln p(y | x) = \\ln\\left(\\sqrt{\\frac{\\Pi_{s}}{2\\pi}}\\right) - \\frac{\\Pi_{s}}{2} (y - g(x))^2$$\nThe first term is a constant with respect to $x$.\n\nSecond, we consider the prior $p(x)$. The problem states that the latent state $x$ is drawn from a normal distribution with mean $\\mu_{0}$ and precision $R$ (variance $R^{-1}$). The PDF is:\n$$p(x) = \\mathcal{N}(x; \\mu_{0}, R^{-1}) = \\frac{1}{\\sqrt{2\\pi (R^{-1})}} \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2 (R^{-1})}\\right)$$\nSimplifying the expression:\n$$p(x) = \\sqrt{\\frac{R}{2\\pi}} \\exp\\left(-\\frac{R}{2} (x - \\mu_{0})^2\\right)$$\nThe log-prior is:\n$$\\ln p(x) = \\ln\\left(\\sqrt{\\frac{R}{2\\pi}}\\right) - \\frac{R}{2} (x - \\mu_{0})^2$$\nAgain, the first term is a constant with respect to $x$.\n\nCombining the log-likelihood and log-prior, the log joint probability density is:\n$$\\ln p(y, x) = -\\frac{\\Pi_{s}}{2} (y - g(x))^2 - \\frac{R}{2} (x - \\mu_{0})^2 + C$$\nwhere $C = \\ln\\left(\\sqrt{\\frac{\\Pi_{s}}{2\\pi}}\\right) + \\ln\\left(\\sqrt{\\frac{R}{2\\pi}}\\right)$ is a constant that does not depend on $x$.\n\nTo find the gradient $\\nabla_{x} \\ln p(y, x)$, we differentiate this expression with respect to $x$.\n$$\\nabla_{x} \\ln p(y, x) = \\frac{d}{dx} \\left( -\\frac{\\Pi_{s}}{2} (y - g(x))^2 - \\frac{R}{2} (x - \\mu_{0})^2 \\right)$$\nUsing the chain rule for each term:\n\\begin{align*} \\nabla_{x} \\ln p(y, x) &= -\\frac{\\Pi_{s}}{2} \\cdot 2(y - g(x)) \\cdot \\frac{d}{dx}(-g(x)) - \\frac{R}{2} \\cdot 2(x - \\mu_{0}) \\cdot \\frac{d}{dx}(x) \\\\ &= -\\Pi_{s}(y - g(x))(-g'(x)) - R(x - \\mu_{0}) \\\\ &= \\Pi_{s}(y - g(x))g'(x) - R(x - \\mu_{0}) \\end{align*}\nThis is the general expression for the gradient. In the context of predictive coding, the term $\\Pi_{s}(y - g(x))$ represents the precision-weighted sensory prediction error, and the term $-R(x - \\mu_{0})$ represents the precision-weighted prior prediction error.\n\nNow, we must compute the numerical value of the updated state $x_{\\text{new}}$. The update rule is given by:\n$$x_{\\text{new}} = x + \\gamma \\nabla_{x} \\ln p(y, x)$$\nWe are given the initial state $x=0$, and the parameters $y=0.8$, $\\mu_{0}=0.1$, $R=5$, $\\Pi_{s}=20$, and $\\gamma=0.05$.\n\nFirst, we evaluate $g(x)$ and its derivative $g'(x)$ at $x=0$.\nThe observation function is the logistic sigmoid: $g(x) = \\frac{1}{1 + \\exp(-x)}$.\nAt $x=0$:\n$$g(0) = \\frac{1}{1 + \\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2} = 0.5$$\nThe derivative of the sigmoid function is $g'(x) = g(x)(1-g(x))$.\nAt $x=0$:\n$$g'(0) = g(0)(1-g(0)) = \\frac{1}{2}\\left(1-\\frac{1}{2}\\right) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4} = 0.25$$\n\nNext, we substitute these values and the given parameters into the gradient expression:\n\\begin{align*} \\nabla_{x} \\ln p(y, x) \\Big|_{x=0} &= \\Pi_{s}(y - g(0))g'(0) - R(0 - \\mu_{0}) \\\\ &= 20(0.8 - 0.5)(0.25) - 5(0 - 0.1) \\\\ &= 20(0.3)(0.25) - 5(-0.1) \\\\ &= 6(0.25) + 0.5 \\\\ &= 1.5 + 0.5 \\\\ &= 2 \\end{align*}\nThe value of the gradient at $x=0$ is $2$.\n\nFinally, we apply the update rule to find $x_{\\text{new}}$:\n$$x_{\\text{new}} = x + \\gamma \\cdot (\\text{gradient})$$\n$$x_{\\text{new}} = 0 + 0.05 \\cdot 2$$\n$$x_{\\text{new}} = 0.1$$\nThe updated state after one step is $0.1$.",
            "answer": "$$\\boxed{0.1}$$"
        },
        {
            "introduction": "Predictive coding offers a powerful explanation for how the brain resolves perceptual ambiguity, such as when viewing a Necker cube. This hands-on coding challenge explores this phenomenon by modeling inference as gradient descent on a non-convex \"free energy landscape\" with multiple valleys, each representing a stable perceptual interpretation . By implementing the algorithm, you will gain direct insight into how the balance between prior expectations and sensory data—along with the initial state of the system—determines which perception ultimately wins out, demonstrating the dynamic and context-sensitive nature of inference in the brain.",
            "id": "4011074",
            "problem": "Consider a one-dimensional predictive coding system that performs approximate Bayesian inference by minimizing a Variational Free Energy (VFE). The latent variable is denoted by $x \\in \\mathbb{R}$ and the observed data by $y \\in \\mathbb{R}$. The generative model comprises a Gaussian likelihood with precision $\\Pi > 0$ and a non-Gaussian prior whose negative log-density induces a double-well potential. The system seeks the Maximum A Posteriori (MAP) estimate by minimizing the VFE over $x$ using gradient descent.\n\nFundamental base:\n- The posterior over $x$ given $y$ is proportional to the product of the likelihood and the prior according to Bayes' rule.\n- Under Gaussian observation noise with precision $\\Pi$, the negative log-likelihood contributes a precision-weighted squared prediction error term to the VFE.\n- Predictive coding realizes inference as gradient descent on the VFE, where the descent direction is the negative gradient of the objective with respect to the latent variable $x$.\n\nConstruct a double-well free energy function using the following components:\n- A prior energy term that creates two stable modes at $\\pm m$, with $m > 0$, given by a quartic double-well potential $U(x) = \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2$, with $\\alpha > 0$.\n- A data-fitting term from a Gaussian likelihood with precision $\\Pi$: $\\dfrac{\\Pi}{2} \\left( y - x \\right)^2$.\n\nThe Variational Free Energy to be minimized is therefore\n$$\nF(x; \\alpha, m, \\Pi, y) = \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2 + \\dfrac{\\Pi}{2} \\left( y - x \\right)^2.\n$$\n\nTasks:\n1. Starting from the definitions above, derive the gradient descent update rule for $x$ that minimizes $F(x; \\alpha, m, \\Pi, y)$.\n2. Implement a deterministic algorithm that, given $(\\alpha, m, \\Pi, y)$ and an initialization $x_0$, performs gradient descent on $F$ until convergence. Convergence must be declared when the absolute gradient magnitude is less than $10^{-10}$ or when the iteration count exceeds the specified maximum. Use backtracking to ensure that each accepted update does not increase the free energy. The backtracking procedure should halve the current step size whenever a proposed update increases $F$, with at most $20$ halvings per iteration.\n3. After convergence, assign a mode index to the solution $x^\\star$ as follows. Let $d_+ = |x^\\star - m|$ and $d_- = |x^\\star + m|$. Output $+1$ if $d_+ < d_-$ (closer to the right well at $+m$), $-1$ if $d_- < d_+$ (closer to the left well at $-m$), and $0$ if $d_+ = d_-$ (equidistant, which occurs at $x^\\star = 0$).\n4. Analyze how nonconvexity affects convergence by showing, through the test suite below, how initialization determines the basin of attraction and the final mode.\n\nTest suite:\nFor each case, use the tuple $(\\alpha, m, \\Pi, y, x_0, \\eta, \\text{max\\_iter})$ where $\\eta$ is the initial step size for gradient descent and $\\text{max\\_iter}$ is the maximum number of iterations. Use these exact values:\n- Case $1$: $(4.0, 1.0, 2.0, 0.2, 0.9, 0.02, 20000)$, a general case where the observation slightly favors the right well.\n- Case $2$: $(4.0, 1.0, 2.0, -0.2, -0.9, 0.02, 20000)$, a symmetric counterpart favoring the left well.\n- Case $3$: $(4.0, 1.0, 2.0, 0.0, 0.0, 0.02, 20000)$, an unstable critical point exactly at the saddle.\n- Case $4$: $(10.0, 1.0, 0.5, 0.0, -0.1, 0.02, 20000)$, a strong prior with weak data pulling initialization into the left basin.\n- Case $5$: $(1.0, 1.0, 50.0, 0.3, -0.7, 0.02, 20000)$, data-dominated regime that pulls the solution to the right well.\n- Case $6$: $(4.0, 1.5, 2.0, -0.1, -0.4, 0.02, 20000)$, altered well positions with $m = 1.5$.\n\nFinal output format:\nYour program should produce a single line of output containing the mode indices for the six test cases as a comma-separated list enclosed in square brackets (for example, $[+1,-1,0,-1,+1,-1]$). Each entry must be an integer from $\\{-1, 0, +1\\}$ representing the final mode index determined by the convergence of gradient descent from the specified initialization.",
            "solution": "The problem is assessed as valid. It is scientifically grounded in the principles of computational neuroscience and Bayesian inference, specifically the predictive coding framework. The problem is well-posed, providing a clear objective function, a standard numerical optimization task, and all necessary parameters. The definitions and goals are objective and mathematically precise. All givens are self-contained and consistent.\n\nThe provided givens are:\n- Latent variable: $x \\in \\mathbb{R}$\n- Observed data: $y \\in \\mathbb{R}$\n- Prior energy (quartic double-well potential): $U(x) = \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2$, with parameters $\\alpha > 0$ and $m > 0$.\n- Data-fitting energy (negative log-likelihood of a Gaussian): $\\dfrac{\\Pi}{2} \\left( y - x \\right)^2$, with precision $\\Pi > 0$.\n- Variational Free Energy (VFE): $F(x; \\alpha, m, \\Pi, y) = \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2 + \\dfrac{\\Pi}{2} \\left( y - x \\right)^2$.\n- Optimization method: Gradient descent with backtracking to find $x^\\star = \\arg\\min_x F(x)$.\n- Convergence criteria: Absolute gradient magnitude $|\\nabla_x F| < 10^{-10}$ or iteration count exceeds `max_iter`.\n- Backtracking rule: If an update increases $F$, the step size is halved. This is repeated at most $20$ times per iteration.\n- Mode classification: The converged state $x^\\star$ is assigned a mode index based on its proximity to the prior's potential wells at $+m$ and $-m$.\n- Test cases: A suite of six parameter sets $(\\alpha, m, \\Pi, y, x_0, \\eta, \\text{max_iter})$ to test the implementation.\n\n**1. Derivation of the Gradient Descent Update Rule**\n\nThe core of the predictive coding process described here is the minimization of the Variational Free Energy, $F(x)$, via gradient descent. The update rule for the latent variable $x$ is given by moving $x$ in the direction opposite to the gradient of $F(x)$. First, we must compute the gradient, $\\dfrac{dF}{dx}$.\n\nThe VFE is the sum of two terms:\n$$\nF(x) = F_{prior}(x) + F_{data}(x) = \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2 + \\dfrac{\\Pi}{2} \\left( y - x \\right)^2\n$$\n\nWe differentiate each term with respect to $x$ using the chain rule:\n\nFor the prior energy term, $F_{prior}(x)$:\n$$\n\\dfrac{dF_{prior}}{dx} = \\dfrac{d}{dx} \\left[ \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2 \\right] = \\dfrac{\\alpha}{4} \\cdot 2 \\left( x^2 - m^2 \\right) \\cdot \\dfrac{d}{dx}(x^2 - m^2) = \\dfrac{\\alpha}{2} \\left( x^2 - m^2 \\right) (2x) = \\alpha x (x^2 - m^2)\n$$\nThis simplifies to $\\alpha (x^3 - m^2 x)$.\n\nFor the data-fitting energy term, $F_{data}(x)$:\n$$\n\\dfrac{dF_{data}}{dx} = \\dfrac{d}{dx} \\left[ \\dfrac{\\Pi}{2} \\left( y - x \\right)^2 \\right] = \\dfrac{\\Pi}{2} \\cdot 2 \\left( y - x \\right) \\cdot \\dfrac{d}{dx}(y - x) = \\Pi (y - x) (-1) = \\Pi (x - y)\n$$\n\nThe total gradient of the VFE is the sum of these two components:\n$$\n\\dfrac{dF}{dx} = \\alpha (x^3 - m^2 x) + \\Pi(x - y)\n$$\n\nThe gradient descent update rule for $x$ at iteration $k$, with a step size (learning rate) $\\eta_k$, is:\n$$\nx_{k+1} = x_k - \\eta_k \\left. \\dfrac{dF}{dx} \\right|_{x=x_k}\n$$\nSubstituting the derived gradient, we get the explicit update rule:\n$$\nx_{k+1} = x_k - \\eta_k \\left( \\alpha (x_k^3 - m^2 x_k) + \\Pi(x_k - y) \\right)\n$$\n\n**2. Algorithmic Design and Implementation**\n\nThe problem requires a deterministic algorithm to perform this gradient descent until convergence. A non-convex function like $F(x)$ can have multiple local minima, and the standard gradient descent algorithm only guarantees convergence to one of these, depending on the initialization $x_0$. To ensure stable convergence, a backtracking line search is employed to determine the step size $\\eta_k$ at each iteration.\n\nThe algorithm proceeds as follows:\n1.  Initialize $x$ with the given starting value $x_0$.\n2.  Begin an iterative loop, up to a maximum of `max_iter` iterations.\n3.  In each iteration $k$, calculate the gradient $g_k = \\dfrac{dF}{dx}|_{x=x_k}$.\n4.  Check for convergence: if the absolute magnitude of the gradient, $|g_k|$, is less than the tolerance $10^{-10}$, the process has converged to a stationary point. The loop is terminated.\n5.  If not converged, perform a backtracking line search to find an appropriate step size $\\eta_k$.\n    a. Start with the initial step size $\\eta_k = \\eta$. Let the current energy be $F_k = F(x_k)$.\n    b. Propose a new state $x_{prop} = x_k - \\eta_k g_k$.\n    c. If $F(x_{prop}) < F_k$, the step size is accepted.\n    d. If $F(x_{prop}) \\geq F_k$, the step size is too large. It is halved ($\\eta_k \\leftarrow \\eta_k / 2$), and step 5b is repeated. This halving is done at most $20$ times.\n    e. If a suitable step size is not found after all allowed halvings, it implies that no further progress can be made from $x_k$. The algorithm is considered converged at $x_k$.\n6.  Update the state using the accepted step size: $x_{k+1} = x_k - \\eta_k g_k$.\n7.  The loop continues until a termination condition (gradient tolerance, max iterations, or failed backtracking) is met. The final value of $x$ is the solution $x^\\star$.\n\n**3. Mode Classification**\n\nAfter convergence to a stable point $x^\\star$, the solution must be classified according to which of the prior's two potential wells it landed in. The wells are centered at $+m$ and $-m$. The classification is based on the Euclidean distance from $x^\\star$ to these two points:\n-   Let $d_+ = |x^\\star - m|$ be the distance to the right well.\n-   Let $d_- = |x^\\star + m|$ be the distance to the left well.\n\nThe mode index is assigned as:\n-   $+1$ if $d_+ < d_-$, meaning $x^\\star$ is closer to the right well. This is true for any $x^\\star > 0$.\n-   $-1$ if $d_- < d_+$, meaning $x^\\star$ is closer to the left well. This is true for any $x^\\star < 0$.\n-   $0$ if $d_+ = d_-$, meaning $x^\\star$ is equidistant from both wells. This occurs only if $x^\\star = 0$, which is a saddle point (local maximum) of the VFE when $y=0$.\n\nThis procedure correctly partitions the real line into three regions corresponding to the basins of attraction of the two minima and the unstable fixed point between them. The test suite demonstrates how the interplay of the prior ($\\alpha, m$), the data ($\\Pi, y$), and the initialization ($x_0$) determines which of these regions the system converges to. For instance, a strong data term (large $\\Pi$) can pull the system out of the basin suggested by its initialization, whereas a strong prior (large $\\alpha$) will make the system more resilient to conflicting data.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the predictive coding problem for a suite of test cases.\n    \"\"\"\n\n    class PredictiveCodingSystem:\n        \"\"\"\n        Implements the minimization of a double-well Variational Free Energy function.\n        \"\"\"\n        def __init__(self, alpha, m, precision, y, x0, eta, max_iter):\n            self.alpha = float(alpha)\n            self.m = float(m)\n            self.precision = float(precision)\n            self.y = float(y)\n            self.x = float(x0)\n            self.eta = float(eta)\n            self.max_iter = int(max_iter)\n            self.grad_tol = 1e-10\n            self.max_backtrack = 20\n\n        def free_energy(self, x):\n            \"\"\"Calculates the Variational Free Energy F(x).\"\"\"\n            prior_energy = (self.alpha / 4.0) * ((x**2 - self.m**2)**2)\n            data_term = (self.precision / 2.0) * ((self.y - x)**2)\n            return prior_energy + data_term\n\n        def gradient(self, x):\n            \"\"\"Calculates the gradient of F(x) with respect to x.\"\"\"\n            prior_grad = self.alpha * x * (x**2 - self.m**2)\n            data_grad = self.precision * (x - self.y)\n            return prior_grad + data_grad\n\n        def minimize(self):\n            \"\"\"\n            Performs gradient descent with backtracking to find the minimum of F(x).\n            \"\"\"\n            for _ in range(self.max_iter):\n                grad = self.gradient(self.x)\n\n                if abs(grad) < self.grad_tol:\n                    # Convergence based on small gradient\n                    break\n\n                current_energy = self.free_energy(self.x)\n                step_eta = self.eta\n                found_descent_step = False\n                \n                # Backtracking line search to find a step that decreases energy\n                for i in range(self.max_backtrack + 1):\n                    x_proposed = self.x - step_eta * grad\n                    \n                    if self.free_energy(x_proposed) < current_energy:\n                        self.x = x_proposed\n                        found_descent_step = True\n                        break # Step accepted, exit backtracking\n                    \n                    step_eta /= 2.0\n                \n                if not found_descent_step:\n                    # Could not find a descent step even after max halvings.\n                    # This implies we are at a minimum (to machine precision).\n                    break\n            \n            return self.x\n\n        def get_mode_index(self, x_star):\n            \"\"\"\n            Classifies the converged state x_star into a mode index {-1, 0, +1}.\n            \"\"\"\n            d_plus = abs(x_star - self.m)\n            d_minus = abs(x_star + self.m)\n\n            if d_plus < d_minus:\n                return 1\n            elif d_minus < d_plus:\n                return -1\n            else:\n                return 0\n\n    def solve_case(params):\n        \"\"\"\n        Initializes and runs the minimization for a single test case.\n        \"\"\"\n        alpha, m, precision, y, x0, eta, max_iter = params\n        system = PredictiveCodingSystem(alpha, m, precision, y, x0, eta, max_iter)\n        x_star = system.minimize()\n        return system.get_mode_index(x_star)\n\n    # Test suite as defined in the problem statement\n    test_cases = [\n        # (alpha, m, precision, y, x0, eta, max_iter)\n        (4.0, 1.0, 2.0, 0.2, 0.9, 0.02, 20000),   # Case 1\n        (4.0, 1.0, 2.0, -0.2, -0.9, 0.02, 20000),  # Case 2\n        (4.0, 1.0, 2.0, 0.0, 0.0, 0.02, 20000),   # Case 3\n        (10.0, 1.0, 0.5, 0.0, -0.1, 0.02, 20000),  # Case 4\n        (1.0, 1.0, 50.0, 0.3, -0.7, 0.02, 20000),  # Case 5\n        (4.0, 1.5, 2.0, -0.1, -0.4, 0.02, 20000),  # Case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        mode_index = solve_case(case)\n        results.append(mode_index)\n\n    # Format the final output string exactly as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}