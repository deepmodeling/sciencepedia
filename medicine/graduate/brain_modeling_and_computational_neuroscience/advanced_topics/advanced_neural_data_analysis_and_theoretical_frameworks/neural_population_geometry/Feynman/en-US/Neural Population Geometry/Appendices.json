{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is a foundational step in studying neural population dynamics. You will simulate neural data from a specified low-dimensional latent process—a common approach for modeling brain activity—and then construct the corresponding state-space trajectory from trial-averaged spike counts . By calculating the trajectory's arc length, you will practice quantifying a basic geometric feature that reflects the total change in population activity over time.",
            "id": "4003652",
            "problem": "You are given trial-averaged spike counts recorded from a population of neurons and asked to construct a neural population state-space trajectory and compute its arc length. The neural population at each time bin is represented by a state vector $x(t) \\in \\mathbb{R}^N$ whose components are the trial-averaged spike counts of $N$ neurons at time bin $t$, where $t \\in \\{1,2,\\dots,T\\}$. The state-space trajectory arc length is defined by the Euclidean metric as\n$$\nL \\;=\\; \\sum_{t=1}^{T-1} \\left\\| x(t+1) - x(t) \\right\\|_2 \\, .\n$$\nStart from the following foundations:\n\n- Euclidean space $\\mathbb{R}^N$ and the Euclidean norm $\\|y\\|_2 = \\sqrt{\\sum_{i=1}^N y_i^2}$ for $y \\in \\mathbb{R}^N$.\n- A standard point-process model for spiking in which observed spike counts $s_i^{(k)}(t)$ for neuron $i$ in trial $k$ and time bin $t$ are independent Poisson random variables with mean equal to a nonnegative firing rate $\\lambda_i(t)$ times the bin width $\\Delta$ (in seconds), namely $s_i^{(k)}(t) \\sim \\mathrm{Poisson}(\\lambda_i(t)\\Delta)$.\n- A low-dimensional latent dynamical process $z(t) \\in \\mathbb{R}^d$ that drives firing rates through a linear map plus offset followed by an exponential nonlinearity to ensure positivity: $\\lambda(t) = \\exp\\!\\big(W z(t) + b\\big)$, where $W \\in \\mathbb{R}^{N \\times d}$ and $b \\in \\mathbb{R}^{N}$, with the exponential applied component-wise.\n\nFor each test case below, construct $z(t)$ deterministically according to the specified dynamics, generate $W$ and $b$ using the given random seed, compute $\\lambda(t)$, sample $K$ independent trials to obtain spike counts $s_i^{(k)}(t)$, form the trial-averaged spike-count state vector\n$$\nx(t) \\;=\\; \\frac{1}{K} \\sum_{k=1}^K s^{(k)}(t) \\in \\mathbb{R}^N \\, ,\n$$\nand compute the arc length $L$. Express each arc length in spikes (counts) as a real number rounded to six decimal places.\n\nYour program must implement the above steps and produce the outputs for the following test suite. Use the exact parameter values specified, with all random number generation controlled by the provided seeds for reproducibility. Time indexing should use integers $t \\in \\{0,1,\\dots,T-1\\}$ in implementation, corresponding to $t \\in \\{1,2,\\dots,T\\}$ in the formulas above.\n\nTest suite of parameterized cases:\n\n- Case A (happy path, rotational plus ramp latent):\n  - $N = 100$, $T = 1000$, $K = 50$, $d = 3$, $\\Delta = 0.01$ s.\n  - Latent dynamics: $z_1(t) = A_1 \\cos(\\omega t)$ with $A_1 = 1.0$, $z_2(t) = A_2 \\sin(\\omega t)$ with $A_2 = 0.8$, $z_3(t) = \\alpha \\left(t - \\frac{T}{2}\\right)$ with $\\alpha = 0.001$, $\\omega = 0.02$ radians per bin.\n  - Weight and offset: $W$ and $b$ built with seed $\\mathrm{seed}_W = 1$. Construct $W$ with independent $\\mathcal{N}(0,\\sigma_W^2)$ entries with $\\sigma_W = 0.2$, and $b$ as $b_i = \\log(5.0) + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_b^2)$, $\\sigma_b = 0.3$, independently for $i = 1,\\dots,N$.\n  - Spike sampling seed: $\\mathrm{seed}_{\\mathrm{spk}} = 11$.\n\n- Case B (boundary jump, minimal $T$):\n  - $N = 100$, $T = 2$, $K = 50$, $d = 1$, $\\Delta = 0.01$ s.\n  - Latent dynamics: $z_1(0) = 0$, $z_1(1) = A_{\\mathrm{step}}$ with $A_{\\mathrm{step}} = 2.0$.\n  - Weight and offset: $W$, $b$ with $\\mathrm{seed}_W = 2$, using the same distributions as Case A.\n  - Spike sampling seed: $\\mathrm{seed}_{\\mathrm{spk}} = 22$.\n\n- Case C (constant latent, tests noise-only trajectory):\n  - $N = 100$, $T = 1000$, $K = 100$, $d = 1$, $\\Delta = 0.01$ s.\n  - Latent dynamics: $z_1(t) = 0$ for all $t$.\n  - Weight and offset: $W$, $b$ with $\\mathrm{seed}_W = 3$, same distributions as Case A.\n  - Spike sampling seed: $\\mathrm{seed}_{\\mathrm{spk}} = 33$.\n\n- Case D (single neuron sinusoid, small $T$):\n  - $N = 1$, $T = 10$, $K = 1000$, $d = 1$, $\\Delta = 0.02$ s.\n  - Latent dynamics: $z_1(t) = A \\sin(\\omega t)$ with $A = 3.0$, $\\omega = 0.5$ radians per bin.\n  - Weight and offset: $W$, $b$ with $\\mathrm{seed}_W = 4$, same distributions as Case A.\n  - Spike sampling seed: $\\mathrm{seed}_{\\mathrm{spk}} = 44$.\n\n- Case E (edge case, $T=1$ yields zero arc length):\n  - $N = 100$, $T = 1$, $K = 20$, $d = 1$, $\\Delta = 0.01$ s.\n  - Latent dynamics: $z_1(0) = 0$.\n  - Weight and offset: $W$, $b$ with $\\mathrm{seed}_W = 5$, same distributions as Case A.\n  - Spike sampling seed: $\\mathrm{seed}_{\\mathrm{spk}} = 55$.\n\nImplementation details:\n\n- Use independent random-number generators for constructing $W$, $b$, and for spike sampling, seeded as specified for each case.\n- Construct $\\lambda(t) = \\exp(W z(t) + b)$ with the exponential applied component-wise to ensure positivity.\n- Sample spike counts $s_i^{(k)}(t) \\sim \\mathrm{Poisson}(\\lambda_i(t)\\Delta)$ independently across neurons, time bins, and trials; then compute $x(t)$ by averaging across trials.\n- Compute $L$ exactly as given by the Euclidean arc length formula above.\n- Express each $L$ in spikes as a floating-point number rounded to six decimal places.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of cases A through E, for example, \"[result_A,result_B,result_C,result_D,result_E]\". Each entry must be rounded to six decimal places.",
            "solution": "The problem is found to be valid after a thorough validation process. It is scientifically grounded in the principles of computational neuroscience, specifically concerning the modeling of neural population activity. All parameters and procedures are explicitly defined, making the problem well-posed and objective. It is free from contradictions, ambiguities, and factual unsoundness. The problem requires the implementation of a standard generative model for neural spike trains and the calculation of a geometric property—the arc length—of the resulting state-space trajectory.\n\nThe solution proceeds by first constructing a low-dimensional latent trajectory, then mapping it to high-dimensional neural firing rates, sampling spike counts from a stochastic process, and finally computing the geometry of the trial-averaged activity. Each step is detailed below.\n\nA state-space representation of neural population activity provides a geometric framework for understanding how information is processed. In this model, the state of a neural population of $N$ neurons at a given time $t$ is a vector $x(t) \\in \\mathbb{R}^N$. The sequence of these vectors over time, $\\{x(0), x(1), \\dots, x(T-1)\\}$, forms a trajectory through this $N$-dimensional state space. The geometric properties of this trajectory, such as its length, can reveal computational desiderata.\n\nThe objective is to compute the arc length $L$ of this trajectory, which is defined as the sum of Euclidean distances between consecutive state vectors:\n$$\nL = \\sum_{t=0}^{T-2} \\| x(t+1) - x(t) \\|_2\n$$\nwhere $\\| \\cdot \\|_2$ denotes the standard Euclidean norm in $\\mathbb{R}^N$. Note that the summation is adjusted for zero-based time indexing $t \\in \\{0, 1, \\dots, T-1\\}$ as specified for implementation.\n\nThe generation of the state vector $x(t)$ follows a multi-step generative process typical in brain modeling:\n\n1.  **Latent Dynamics**: A low-dimensional time series $z(t) \\in \\mathbb{R}^d$, where $d \\ll N$, is first defined. This latent variable is assumed to capture the essential, underlying computations of the neural circuit. The dynamics of $z(t)$ are given deterministically for each test case. For instance, in Case A, the dynamics are a combination of sinusoidal and linear functions, creating a helical trajectory in the $d=3$ dimensional latent space:\n    $$\n    z(t) = \\begin{pmatrix} A_1 \\cos(\\omega t) \\\\ A_2 \\sin(\\omega t) \\\\ \\alpha(t - T/2) \\end{pmatrix}\n    $$\n\n2.  **Firing Rate Generation**: The latent dynamics $z(t)$ drive the firing rates $\\lambda(t) \\in \\mathbb{R}^N$ of the $N$ neurons. This mapping is modeled as a linear transformation followed by a non-linear function to ensure firing rates are non-negative.\n    $$\n    \\lambda(t) = \\exp(W z(t) + b)\n    $$\n    Here, $W \\in \\mathbb{R}^{N \\times d}$ is a weight matrix and $b \\in \\mathbb{R}^N$ is a bias vector. The exponential function is applied component-wise. $W$ and $b$ are drawn from specified normal distributions, with a dedicated random number generator seeded by $\\mathrm{seed}_W$ for reproducibility.\n\n3.  **Spike Count Sampling**: The activity of individual neurons is stochastic. We model the spike count of neuron $i$ in trial $k$ at time $t$, denoted $s_i^{(k)}(t)$, as an independent draw from a Poisson distribution. The mean of this distribution is the firing rate $\\lambda_i(t)$ multiplied by the time bin width $\\Delta$.\n    $$\n    s_i^{(k)}(t) \\sim \\mathrm{Poisson}(\\lambda_i(t) \\Delta)\n    $$\n    This step is performed for $K$ independent trials, for each of the $N$ neurons and $T$ time bins. A second, independent random number generator, seeded by $\\mathrm{seed}_{\\mathrm{spk}}$, is used for this stochastic sampling.\n\n4.  **State Vector Construction**: To obtain a less noisy representation of the population state, the spike counts are averaged across the $K$ trials. This yields the state vector $x(t)$:\n    $$\n    x(t) = \\frac{1}{K} \\sum_{k=1}^K s^{(k)}(t)\n    $$\n    where $s^{(k)}(t)$ is the vector of spike counts for all $N$ neurons in trial $k$ at time $t$. The resulting set of vectors $\\{x(0), \\dots, x(T-1)\\}$ constitutes the neural trajectory.\n\n5.  **Arc Length Calculation**: Finally, the arc length $L$ is computed by summing the Euclidean distances between consecutive points on the trajectory, as defined by the initial formula. For the edge case where $T \\le 1$, the trajectory consists of at most one point, and the sum over an empty set of segments is defined to be $L=0$.\n\nThe implementation will systematically execute these steps for each test case. Vectorized operations using the `numpy` library will be employed for efficiency. Separate random number generators will be initialized for model parameter generation ($W, b$) and for spike count sampling to ensure precise adherence to the specified seeds and a reproducible result. The final arc length for each case will be rounded to six decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the arc length of neural population state-space trajectories\n    for a suite of test cases based on a generative model.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"params\": {\n                \"N\": 100, \"T\": 1000, \"K\": 50, \"d\": 3, \"delta\": 0.01,\n                \"sigma_W\": 0.2, \"sigma_b\": 0.3, \"log_base_rate\": np.log(5.0)\n            },\n            \"latent_dynamics\": {\n                \"type\": \"rotational_ramp\",\n                \"A1\": 1.0, \"A2\": 0.8, \"omega\": 0.02, \"alpha\": 0.001\n            },\n            \"seeds\": {\"seed_W\": 1, \"seed_spk\": 11}\n        },\n        {\n            \"name\": \"Case B\",\n            \"params\": {\n                \"N\": 100, \"T\": 2, \"K\": 50, \"d\": 1, \"delta\": 0.01,\n                \"sigma_W\": 0.2, \"sigma_b\": 0.3, \"log_base_rate\": np.log(5.0)\n            },\n            \"latent_dynamics\": {\n                \"type\": \"step\",\n                \"A_step\": 2.0\n            },\n            \"seeds\": {\"seed_W\": 2, \"seed_spk\": 22}\n        },\n        {\n            \"name\": \"Case C\",\n            \"params\": {\n                \"N\": 100, \"T\": 1000, \"K\": 100, \"d\": 1, \"delta\": 0.01,\n                \"sigma_W\": 0.2, \"sigma_b\": 0.3, \"log_base_rate\": np.log(5.0)\n            },\n            \"latent_dynamics\": {\n                \"type\": \"constant_zero\"\n            },\n            \"seeds\": {\"seed_W\": 3, \"seed_spk\": 33}\n        },\n        {\n            \"name\": \"Case D\",\n            \"params\": {\n                \"N\": 1, \"T\": 10, \"K\": 1000, \"d\": 1, \"delta\": 0.02,\n                \"sigma_W\": 0.2, \"sigma_b\": 0.3, \"log_base_rate\": np.log(5.0)\n            },\n            \"latent_dynamics\": {\n                \"type\": \"sinusoid\",\n                \"A\": 3.0, \"omega\": 0.5\n            },\n            \"seeds\": {\"seed_W\": 4, \"seed_spk\": 44}\n        },\n        {\n            \"name\": \"Case E\",\n            \"params\": {\n                \"N\": 100, \"T\": 1, \"K\": 20, \"d\": 1, \"delta\": 0.01,\n                \"sigma_W\": 0.2, \"sigma_b\": 0.3, \"log_base_rate\": np.log(5.0)\n            },\n            \"latent_dynamics\": {\n                \"type\": \"constant_zero\"\n            },\n            \"seeds\": {\"seed_W\": 5, \"seed_spk\": 55}\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        p = case[\"params\"]\n        ld = case[\"latent_dynamics\"]\n        s = case[\"seeds\"]\n\n        # Unpack parameters for clarity\n        N, T, K, d, delta = p[\"N\"], p[\"T\"], p[\"K\"], p[\"d\"], p[\"delta\"]\n        sigma_W, sigma_b, log_base_rate = p[\"sigma_W\"], p[\"sigma_b\"], p[\"log_base_rate\"]\n        seed_W, seed_spk = s[\"seed_W\"], s[\"seed_spk\"]\n        \n        # 1. Initialize random number generators with specified seeds\n        rng_W = np.random.default_rng(seed_W)\n        rng_spk = np.random.default_rng(seed_spk)\n\n        # 2. Generate weight matrix W and offset vector b\n        W = rng_W.normal(loc=0.0, scale=sigma_W, size=(N, d))\n        epsilon = rng_W.normal(loc=0.0, scale=sigma_b, size=N)\n        b = log_base_rate + epsilon\n\n        # 3. Generate the latent trajectory z(t)\n        t_steps = np.arange(T)\n        z = np.zeros((T, d))\n        \n        if ld[\"type\"] == \"rotational_ramp\":\n            z[:, 0] = ld[\"A1\"] * np.cos(ld[\"omega\"] * t_steps)\n            z[:, 1] = ld[\"A2\"] * np.sin(ld[\"omega\"] * t_steps)\n            z[:, 2] = ld[\"alpha\"] * (t_steps - T / 2.0)\n        elif ld[\"type\"] == \"step\":\n            if T > 1:\n                z[1, 0] = ld[\"A_step\"]\n        elif ld[\"type\"] == \"constant_zero\":\n            # z is already initialized to zeros\n            pass\n        elif ld[\"type\"] == \"sinusoid\":\n            z[:, 0] = ld[\"A\"] * np.sin(ld[\"omega\"] * t_steps)\n\n        # 4. Compute firing rates lambda(t)\n        # z is (T, d), W is (N, d). We need z @ W.T which is (T, N)\n        # b is (N,). Broadcasting adds b to each row.\n        lambda_t = np.exp(z @ W.T + b)\n\n        # 5. Compute Poisson means\n        poisson_means = lambda_t * delta\n\n        # 6. Generate spike counts s_i^(k)(t)\n        # `poisson_means` is (T, N). `size=(K, T, N)` broadcasts correctly.\n        s_counts = rng_spk.poisson(lam=poisson_means, size=(K, T, N))\n\n        # 7. Compute trial-averaged state vector x(t)\n        # Average over the trials axis (axis 0)\n        x = s_counts.mean(axis=0)\n\n        # 8. Compute the arc length L\n        if T = 1:\n            L = 0.0\n        else:\n            # x is (T, N). Differences between consecutive time points.\n            diffs = x[1:] - x[:-1]  # Shape (T-1, N)\n            # Calculate Euclidean norm for each time step difference along neuron axis\n            segment_lengths = np.linalg.norm(diffs, axis=1) # Shape (T-1,)\n            L = np.sum(segment_lengths)\n            \n        results.append(f\"{L:.6f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A central goal of analyzing neural geometry is to understand how it supports computation. This practice provides a direct link between geometry and function by exploring how the distance between neural representations impacts decoding performance . You will derive the relationship between the geometric margin, as defined by a Support Vector Machine (SVM), and a linear classifier's robustness to noise, revealing why greater separation in state-space is computationally advantageous.",
            "id": "4003672",
            "problem": "Consider two experimental conditions in a neural population, each represented by a mean response vector in a real $n$-dimensional space. Let the mean response vectors be denoted by $\\mathbf{m}_{+} \\in \\mathbb{R}^{n}$ and $\\mathbf{m}_{-} \\in \\mathbb{R}^{n}$ for the positive and negative condition, respectively. Assume that classification is performed by a linear Support Vector Machine (SVM), which finds a separating hyperplane with maximal geometric margin for linearly separable data. The Support Vector Machine (SVM) primal optimization problem for linearly separable labeled data $\\{ (\\mathbf{x}_{i}, y_{i}) \\}_{i=1}^{N}$ with labels $y_{i} \\in \\{+1, -1\\}$ is to minimize $\\frac{1}{2} \\lVert \\mathbf{w} \\rVert_{2}^{2}$ subject to $y_{i} \\left( \\mathbf{w}^{\\top} \\mathbf{x}_{i} + b \\right) \\geq 1$, where $\\mathbf{w} \\in \\mathbb{R}^{n}$ is the normal vector to the separating hyperplane and $b \\in \\mathbb{R}$ is the offset. The geometric margin of a hyperplane defined by $(\\mathbf{w}, b)$ is the minimal distance from the data to the hyperplane measured along the normal direction and equals $\\frac{1}{\\lVert \\mathbf{w} \\rVert_{2}}$ in the canonical scaling.\n\nModel robustness against noise is assessed by additive isotropic Gaussian noise on the neural population response, i.e., the observed response is $\\mathbf{x}' = \\mathbf{x} + \\boldsymbol{\\eta}$ with $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{n})$, where $\\sigma > 0$ is the per-coordinate standard deviation and $\\mathbf{I}_{n}$ is the $n \\times n$ identity matrix. The decision variable for a linear classifier is $d(\\mathbf{x}) = \\mathbf{w}^{\\top} \\mathbf{x} + b$. Under the additive noise model, the fluctuation of the decision variable is determined by the distribution of $\\mathbf{w}^{\\top} \\boldsymbol{\\eta}$ and standard properties of the normal distribution.\n\nStarting only from the definitions above and well-tested facts about the normal distribution and linear classification, derive an algorithm to:\n1. Compute the SVM geometric margin $m$ for the pair of condition-mean responses $\\mathbf{m}_{+}$ and $\\mathbf{m}_{-}$ in the linearly separable case.\n2. Relate the margin $m$ to robustness against additive isotropic Gaussian noise by determining the largest $\\sigma$ (denoted $\\sigma_{\\text{max}}$) such that, when the classifier is applied to the condition-mean responses, the misclassification probability at either mean does not exceed a specified error tolerance $\\tau \\in (0, 0.5)$ expressed as a decimal fraction.\n\nYour program must implement the above derivation and compute $(m, \\sigma_{\\text{max}})$ for each test case below. If the data are not linearly separable at the level of condition means (for example, if $\\mathbf{m}_{+} = \\mathbf{m}_{-}$), define the geometric margin as $m = 0$ and report $\\sigma_{\\text{max}} = 0$ for any $\\tau \\in (0, 0.5)$.\n\nTest suite (each case is specified by $(\\mathbf{m}_{+}, \\mathbf{m}_{-}, \\tau)$):\n- Case $1$: $\\mathbf{m}_{+} = (1, 0, 0, 0, 0)$, $\\mathbf{m}_{-} = (-1, 0, 0, 0, 0)$, $\\tau = 0.1$.\n- Case $2$: $\\mathbf{m}_{+} = (0.1, 0, 0)$, $\\mathbf{m}_{-} = (0, 0, 0)$, $\\tau = 0.01$.\n- Case $3$: $\\mathbf{m}_{+} = (0.5, 0.5, \\dots, 0.5) \\in \\mathbb{R}^{50}$, $\\mathbf{m}_{-} = (-0.5, -0.5, \\dots, -0.5) \\in \\mathbb{R}^{50}$, $\\tau = 0.001$.\n- Case $4$: $\\mathbf{m}_{+} = (0, 0, 0)$, $\\mathbf{m}_{-} = (0, 0, 0)$, $\\tau = 0.1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list $[m, \\sigma_{\\text{max}}]$ corresponding to a test case. All floating-point numbers must be rounded to six decimal places. For example, an output for two hypothetical cases would look like $[[0.500000,0.300000],[1.250000,0.800000]]$.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of linear algebra, statistical decision theory, and the geometry of support vector machines. The problem is well-posed, objective, and contains all necessary information to derive a unique solution.\n\nThe derivation proceeds in two parts as requested: first, computing the geometric margin $m$ for the two condition-mean responses, and second, relating this margin to the maximum tolerable noise $\\sigma_{\\text{max}}$.\n\n### Part 1: Derivation of the SVM Geometric Margin ($m$)\n\nWe are given two condition-mean response vectors, $\\mathbf{m}_{+} \\in \\mathbb{R}^{n}$ and $\\mathbf{m}_{-} \\in \\mathbb{R}^{n}$. These can be treated as a dataset of two points, $(\\mathbf{m}_{+}, +1)$ and $(\\mathbf{m}_{-}, -1)$. The goal of a linear Support Vector Machine (SVM) is to find a hyperplane defined by a normal vector $\\mathbf{w} \\in \\mathbb{R}^{n}$ and an offset $b \\in \\mathbb{R}$ that maximizes the geometric margin. The margin maximization problem is formulated as:\n$$\n\\min_{\\mathbf{w}, b} \\frac{1}{2} \\lVert \\mathbf{w} \\rVert_{2}^{2}\n$$\nsubject to the constraints:\n$$\n\\begin{cases}\n(+1) (\\mathbf{w}^{\\top} \\mathbf{m}_{+} + b) \\geq 1 \\\\\n(-1) (\\mathbf{w}^{\\top} \\mathbf{m}_{-} + b) \\geq 1\n\\end{cases}\n$$\nIn the maximal margin case for two linearly separable points, the points themselves become the support vectors, meaning the inequality constraints become active equalities:\n$$\n\\mathbf{w}^{\\top} \\mathbf{m}_{+} + b = 1 \\\\\n\\mathbf{w}^{\\top} \\mathbf{m}_{-} + b = -1\n$$\nThis is a system of two linear equations. Subtracting the second equation from the first yields:\n$$\n(\\mathbf{w}^{\\top} \\mathbf{m}_{+} + b) - (\\mathbf{w}^{\\top} \\mathbf{m}_{-} + b) = 1 - (-1)\n$$\n$$\n\\mathbf{w}^{\\top} (\\mathbf{m}_{+} - \\mathbf{m}_{-}) = 2\n$$\nTo minimize $\\lVert \\mathbf{w} \\rVert_{2}^{2}$ subject to this single linear constraint on $\\mathbf{w}$, the vector $\\mathbf{w}$ must be parallel to the vector defining the constraint, which is $(\\mathbf{m}_{+} - \\mathbf{m}_{-})$. This is a standard result from optimization using Lagrange multipliers or an application of the Cauchy-Schwarz inequality. We can therefore write $\\mathbf{w} = k (\\mathbf{m}_{+} - \\mathbf{m}_{-})$ for some scalar $k \\in \\mathbb{R}$. Substituting this form into the constraint equation:\n$$\n(k (\\mathbf{m}_{+} - \\mathbf{m}_{-}))^{\\top} (\\mathbf{m}_{+} - \\mathbf{m}_{-}) = 2\n$$\n$$\nk \\lVert \\mathbf{m}_{+} - \\mathbf{m}_{-} \\rVert_{2}^{2} = 2\n$$\nIf $\\mathbf{m}_{+} \\neq \\mathbf{m}_{-}$, we can solve for $k$:\n$$\nk = \\frac{2}{\\lVert \\mathbf{m}_{+} - \\mathbf{m}_{-} \\rVert_{2}^{2}}\n$$\nThis gives the optimal weight vector:\n$$\n\\mathbf{w} = \\frac{2}{\\lVert \\mathbf{m}_{+} - \\mathbf{m}_{-} \\rVert_{2}^{2}} (\\mathbf{m}_{+} - \\mathbf{m}_{-})\n$$\nThe geometric margin $m$ is defined as $m = \\frac{1}{\\lVert \\mathbf{w} \\rVert_{2}}$. We compute the norm of $\\mathbf{w}$:\n$$\n\\lVert \\mathbf{w} \\rVert_{2} = \\left\\lVert \\frac{2}{\\lVert \\mathbf{m}_{+} - \\mathbf{m}_{-} \\rVert_{2}^{2}} (\\mathbf{m}_{+} - \\mathbf{m}_{-}) \\right\\rVert_{2} = \\frac{2}{\\lVert \\mathbf{m}_{+} - \\mathbf{m}_{-} \\rVert_{2}^{2}} \\lVert \\mathbf{m}_{+} - \\mathbf{m}_{-} \\rVert_{2} = \\frac{2}{\\lVert \\mathbf{m}_{+} - \\mathbf{m}_{-} \\rVert_{2}}\n$$\nTherefore, the geometric margin is:\n$$\nm = \\frac{1}{\\lVert \\mathbf{w} \\rVert_{2}} = \\frac{\\lVert \\mathbf{m}_{+} - \\mathbf{m}_{-} \\rVert_{2}}{2}\n$$\nThis result is intuitive: the maximal margin for two points is half the Euclidean distance between them. If the points are identical ($\\mathbf{m}_{+} = \\mathbf{m}_{-}$), they are not linearly separable in a non-trivial sense. In this case, $\\lVert \\mathbf{m}_{+} - \\mathbf{m}_{-} \\rVert_{2} = 0$, leading to $m=0$, which is consistent with the problem's specified handling of this edge case.\n\n### Part 2: Derivation of the Maximum Noise Standard Deviation ($\\sigma_{\\text{max}}$)\n\nWe now assess the classifier's robustness to additive isotropic Gaussian noise. An observed response is $\\mathbf{x}' = \\mathbf{x} + \\boldsymbol{\\eta}$, where $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{n})$. Classification is performed using the decision function $d(\\mathbf{x}') = \\mathbf{w}^{\\top}\\mathbf{x}' + b$.\n\nConsider a noisy observation around the positive mean, $\\mathbf{x}'_{+} = \\mathbf{m}_{+} + \\boldsymbol{\\eta}$. The decision variable is:\n$$\nd(\\mathbf{x}'_{+}) = \\mathbf{w}^{\\top} (\\mathbf{m}_{+} + \\boldsymbol{\\eta}) + b = (\\mathbf{w}^{\\top} \\mathbf{m}_{+} + b) + \\mathbf{w}^{\\top} \\boldsymbol{\\eta}\n$$\nFrom the active SVM constraint, we know $\\mathbf{w}^{\\top} \\mathbf{m}_{+} + b = 1$. Thus,\n$$\nd(\\mathbf{x}'_{+}) = 1 + \\mathbf{w}^{\\top} \\boldsymbol{\\eta}\n$$\nA misclassification occurs if $d(\\mathbf{x}'_{+})  0$ (since the true label is $+1$). The misclassification probability is $P(1 + \\mathbf{w}^{\\top} \\boldsymbol{\\eta}  0) = P(\\mathbf{w}^{\\top} \\boldsymbol{\\eta}  -1)$.\n\nLet's characterize the random variable $Z = \\mathbf{w}^{\\top} \\boldsymbol{\\eta}$. Since it is a linear combination of independent Gaussian random variables, $Z$ is also Gaussian. Its mean is $\\mathbb{E}[Z] = \\mathbf{w}^{\\top} \\mathbb{E}[\\boldsymbol{\\eta}] = \\mathbf{w}^{\\top} \\mathbf{0} = 0$. Its variance is $\\text{Var}(Z) = \\mathbf{w}^{\\top} \\text{Cov}(\\boldsymbol{\\eta}) \\mathbf{w} = \\mathbf{w}^{\\top} (\\sigma^{2} \\mathbf{I}_{n}) \\mathbf{w} = \\sigma^{2} \\lVert \\mathbf{w} \\rVert_{2}^{2}$.\nSo, $Z \\sim \\mathcal{N}(0, \\sigma^{2} \\lVert \\mathbf{w} \\rVert_{2}^{2})$.\n\nThe misclassification probability, $P_{\\text{error}}$, is:\n$$\nP_{\\text{error}} = P(Z  -1) = \\Phi\\left(\\frac{-1 - 0}{\\sqrt{\\sigma^{2} \\lVert \\mathbf{w} \\rVert_{2}^{2}}}\\right) = \\Phi\\left(\\frac{-1}{\\sigma \\lVert \\mathbf{w} \\rVert_{2}}\\right)\n$$\nwhere $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution, $\\mathcal{N}(0, 1)$. Substituting $\\lVert \\mathbf{w} \\rVert_{2} = 1/m$, we get:\n$$\nP_{\\text{error}} = \\Phi\\left(\\frac{-m}{\\sigma}\\right)\n$$\nA symmetric analysis for the negative mean, $\\mathbf{m}_{-}$, gives a misclassification probability $P(\\mathbf{w}^{\\top} \\boldsymbol{\\eta} > 1) = 1 - \\Phi(m/\\sigma) = \\Phi(-m/\\sigma)$, which is identical.\n\nWe require this probability not to exceed a tolerance $\\tau \\in (0, 0.5)$:\n$$\n\\Phi\\left(\\frac{-m}{\\sigma}\\right) \\leq \\tau\n$$\nSince $\\Phi$ is monotonically increasing, we can apply its inverse, $\\Phi^{-1}$ (the probit or quantile function), to both sides:\n$$\n\\frac{-m}{\\sigma} \\leq \\Phi^{-1}(\\tau)\n$$\nWe want to find the largest $\\sigma$ that satisfies this. Let $z_{\\tau} = \\Phi^{-1}(\\tau)$. Since $\\tau \\in (0, 0.5)$, $z_{\\tau}$ is negative. Rearranging for $\\sigma > 0$ requires flipping the inequality sign:\n$$\n\\sigma \\leq \\frac{-m}{z_{\\tau}} \\implies \\sigma \\leq \\frac{-m}{\\Phi^{-1}(\\tau)}\n$$\nThe maximum allowed value for $\\sigma$ is therefore:\n$$\n\\sigma_{\\text{max}} = \\frac{-m}{\\Phi^{-1}(\\tau)}\n$$\nIf $m=0$, then clearly $\\sigma_{\\text{max}}=0$, which aligns with the problem statement.\n\n### Algorithm Summary\nFor each test case $(\\mathbf{m}_{+}, \\mathbf{m}_{-}, \\tau)$:\n1.  Compute the Euclidean distance $d = \\lVert \\mathbf{m}_{+} - \\mathbf{m}_{-} \\rVert_{2}$.\n2.  Calculate the geometric margin $m = d/2$.\n3.  If $m = 0$, the result is $(m, \\sigma_{\\text{max}}) = (0, 0)$.\n4.  If $m > 0$, calculate the standard normal quantile $z_{\\tau} = \\Phi^{-1}(\\tau)$ and then compute $\\sigma_{\\text{max}} = -m/z_{\\tau}$.\n5.  Return the pair $(m, \\sigma_{\\text{max}})$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the SVM geometric margin and maximum noise robustness for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([1.0, 0.0, 0.0, 0.0, 0.0]), np.array([-1.0, 0.0, 0.0, 0.0, 0.0]), 0.1),\n        (np.array([0.1, 0.0, 0.0]), np.array([0.0, 0.0, 0.0]), 0.01),\n        (np.full(50, 0.5), np.full(50, -0.5), 0.001),\n        (np.array([0.0, 0.0, 0.0]), np.array([0.0, 0.0, 0.0]), 0.1),\n    ]\n\n    results_str = []\n    for m_plus, m_minus, tau in test_cases:\n        # Step 1  2: Calculate the geometric margin m.\n        # The margin is half the Euclidean distance between the two mean vectors.\n        distance = np.linalg.norm(m_plus - m_minus)\n        margin = distance / 2.0\n\n        sigma_max = 0.0\n        # Step 3  4: Calculate sigma_max based on the margin.\n        if margin > 0:\n            # For a margin > 0, calculate the maximum tolerable noise standard deviation.\n            # Find the z-score corresponding to the cumulative probability tau.\n            # This is the inverse of the standard normal CDF, also known as the percent point function (ppf).\n            z_tau = norm.ppf(tau)\n            \n            # The derivation yields sigma_max = -m / z_tau.\n            # Since tau is in (0, 0.5), z_tau is negative, making sigma_max positive.\n            sigma_max = -margin / z_tau\n        # If margin is 0, sigma_max remains 0 as per the problem statement.\n\n        # Append the formatted result string for the current case.\n        results_str.append(f\"[{margin:.6f},{sigma_max:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "How can we determine if two neural populations implement a similar representational scheme, especially if the specific neurons recorded are different? This advanced practice introduces Procrustes analysis, a powerful method for aligning two geometric configurations through optimal rotation to assess their underlying similarity . Mastering this technique is essential for comparing neural codes across different brain areas, learning stages, or even different individuals.",
            "id": "4003561",
            "problem": "Consider two neural population geometries represented by condition-mean response matrices. Let $C$ denote the number of conditions and let $D$ denote the number of recorded units. For each session, define a matrix $A \\in \\mathbb{R}^{C \\times D}$ whose $i$-th row encodes the mean response vector for condition $i$ across $D$ units. Similarly define $B \\in \\mathbb{R}^{C \\times D}$ for a second session under the same $C$ conditions but potentially different recording axes. The task is to align $A$ to $B$ using only an orthogonal transformation in the special orthogonal group and to quantify the residual mismatch after optimal alignment.\n\nStarting from fundamental definitions of Euclidean geometry and least-squares, you must implement the following procedure for each test case:\n- Remove translation by centering each geometry across conditions: compute column means of $A$ and $B$, subtract them to obtain $A_{c}$ and $B_{c}$ with $\\sum_{i=1}^{C} A_{c}(i, j) = 0$ and $\\sum_{i=1}^{C} B_{c}(i, j) = 0$ for all $j \\in \\{1,\\dots,D\\}$.\n- Compute the optimal orthogonal matrix $Q^{\\star} \\in \\mathbb{R}^{D \\times D}$ with $Q^{\\star \\top} Q^{\\star} = I$ and $\\det(Q^{\\star}) = +1$ that minimizes the squared Frobenius norm $\\|A_{c} Q - B_{c}\\|_{F}^{2}$ over all $Q$ satisfying the constraints. This restricts $Q$ to be a proper rotation without reflection.\n- Quantify the residual alignment error as the ratio\n$$\n\\mathrm{err} = \\frac{\\|A_{c} Q^{\\star} - B_{c}\\|_{F}^{2}}{\\|B_{c}\\|_{F}^{2}},\n$$\nwhich is unitless and lies in $[0, +\\infty)$, with $\\mathrm{err} = 0$ indicating perfect alignment.\n\nImplement a complete program that computes $\\mathrm{err}$ for each of the following test cases. All matrices are specified explicitly, with $C = 4$ and $D = 3$ for every case.\n\nTest case $1$ (exact rotation):\n$$\nA_{1} =\n\\begin{bmatrix}\n0  1  2 \\\\\n1  0  1 \\\\\n2  1  0 \\\\\n1  2  1\n\\end{bmatrix},\n\\quad\nR_{z,90} =\n\\begin{bmatrix}\n0  -1  0 \\\\\n1  \\phantom{-}0  0 \\\\\n0  \\phantom{-}0  1\n\\end{bmatrix},\n\\quad\nB_{1} = A_{1} R_{z,90}.\n$$\n\nTest case $2$ (rotation plus small noise):\n$$\nA_{2} = A_{1}, \\quad\nN_{2} =\n\\begin{bmatrix}\n0.05  -0.03  0.02 \\\\\n-0.01  0.04  -0.02 \\\\\n0.00  -0.02  0.03 \\\\\n0.02  0.01  -0.01\n\\end{bmatrix},\n\\quad\nB_{2} = A_{2} R_{z,90} + N_{2}.\n$$\n\nTest case $3$ (rank-deficient geometry, rotation plus small noise):\n$$\nA_{3} =\n\\begin{bmatrix}\n1  2  3 \\\\\n2  4  6 \\\\\n3  6  9 \\\\\n4  8  12\n\\end{bmatrix},\n\\quad\nN_{3} =\n\\begin{bmatrix}\n0.10  -0.10  0.00 \\\\\n0.00  \\phantom{-}0.00  0.10 \\\\\n-0.05  \\phantom{-}0.02  -0.02 \\\\\n0.03  -0.04  0.01\n\\end{bmatrix},\n\\quad\nB_{3} = A_{3} R_{z,90} + N_{3}.\n$$\n\nTest case $4$ (pure reflection mismatch with orientation-preserving constraint):\n$$\nA_{4} = A_{1}, \\quad\nF_{x} =\n\\begin{bmatrix}\n-1  0  0 \\\\\n\\phantom{-}0  1  0 \\\\\n\\phantom{-}0  0  1\n\\end{bmatrix},\n\\quad\nB_{4} = A_{4} F_{x}.\n$$\n\nYour program must compute $\\mathrm{err}$ for each test case using the steps above and produce a single line of output containing the four floating-point results as a comma-separated list enclosed in square brackets, for example $[\\mathrm{err}_{1},\\mathrm{err}_{2},\\mathrm{err}_{3},\\mathrm{err}_{4}]$. There are no physical units involved. The answer must be in the exact format described.\n\nYour solution must be derived from first principles of Euclidean geometry, least-squares, and orthogonal transformations, without relying on shortcut formulas presented directly in the problem statement.",
            "solution": "The problem requires us to find the optimal alignment between two geometric configurations of neural population responses and to quantify the residual error. The configurations are represented by matrices $A$ and $B$ in $\\mathbb{R}^{C \\times D}$, where $C$ is the number of experimental conditions and $D$ is the number of neurons. The alignment is restricted to a proper rotation, an element of the special orthogonal group $SO(D)$.\n\nThe prescribed procedure is as follows:\n1.  Remove any translational offset by centering the data. For a data matrix $X \\in \\mathbb{R}^{C \\times D}$, the centered matrix $X_c$ is obtained by subtracting the mean response across conditions for each neuron. This is equivalent to subtracting the column-wise mean from each column. The resulting centered matrices $A_c$ and $B_c$ have the property that the sum of each column is zero.\n2.  Find the optimal proper rotation $Q^{\\star} \\in SO(D)$ that minimizes the alignment cost, defined as the squared Frobenius norm of the difference between the rotated geometry $A_c Q$ and the target geometry $B_c$.\n3.  Calculate the normalized residual error.\n\nWe will now derive the solution from first principles.\n\nLet the centered matrices be $A_c \\in \\mathbb{R}^{C \\times D}$ and $B_c \\in \\mathbb{R}^{C \\times D}$. The optimization problem is to find a matrix $Q^{\\star}$ that solves:\n$$\n\\min_{Q} \\|A_{c} Q - B_{c}\\|_{F}^{2} \\quad \\text{subject to} \\quad Q^{\\top}Q = I \\text{ and } \\det(Q) = 1\n$$\nHere, $I$ is the $D \\times D$ identity matrix, and $\\| \\cdot \\|_F$ denotes the Frobenius norm.\n\nFirst, we expand the objective function. The squared Frobenius norm of a matrix $M$ is given by $\\mathrm{Tr}(M^{\\top}M)$, where $\\mathrm{Tr}(\\cdot)$ is the trace operator.\n$$\n\\|A_{c} Q - B_{c}\\|_{F}^{2} = \\mathrm{Tr}\\left( (A_{c} Q - B_{c})^{\\top} (A_{c} Q - B_{c}) \\right)\n$$\nExpanding the product:\n$$\n= \\mathrm{Tr}\\left( (Q^{\\top} A_{c}^{\\top} - B_{c}^{\\top}) (A_{c} Q - B_{c}) \\right)\n$$\n$$\n= \\mathrm{Tr}\\left( Q^{\\top} A_{c}^{\\top} A_{c} Q - Q^{\\top} A_{c}^{\\top} B_{c} - B_{c}^{\\top} A_{c} Q + B_{c}^{\\top} B_{c} \\right)\n$$\nUsing the linearity of the trace, we can separate the terms:\n$$\n= \\mathrm{Tr}(Q^{\\top} A_{c}^{\\top} A_{c} Q) - \\mathrm{Tr}(Q^{\\top} A_{c}^{\\top} B_{c}) - \\mathrm{Tr}(B_{c}^{\\top} A_{c} Q) + \\mathrm{Tr}(B_{c}^{\\top} B_{c})\n$$\nWe simplify each term. Using the cyclic property of the trace, $\\mathrm{Tr}(XYZ) = \\mathrm{Tr}(ZXY)$, and the orthogonality of $Q$ ($Q^{\\top}Q = I$):\n$$\n\\mathrm{Tr}(Q^{\\top} A_{c}^{\\top} A_{c} Q) = \\mathrm{Tr}(A_{c}^{\\top} A_{c} Q Q^{\\top}) = \\mathrm{Tr}(A_{c}^{\\top} A_{c}) = \\|A_{c}\\|_{F}^{2}\n$$\nThe last term is simply $\\|B_{c}\\|_{F}^{2}$. For the middle terms, we note that the trace of a matrix is equal to the trace of its transpose, so $\\mathrm{Tr}(B_{c}^{\\top} A_{c} Q) = \\mathrm{Tr}((B_{c}^{\\top} A_{c} Q)^{\\top}) = \\mathrm{Tr}(Q^{\\top} A_{c}^{\\top} B_{c})$.\nThus, the objective function simplifies to:\n$$\n\\|A_{c} Q - B_{c}\\|_{F}^{2} = \\|A_{c}\\|_{F}^{2} + \\|B_{c}\\|_{F}^{2} - 2 \\mathrm{Tr}(Q^{\\top} A_{c}^{\\top} B_{c})\n$$\nThe terms $\\|A_{c}\\|_{F}^{2}$ and $\\|B_{c}\\|_{F}^{2}$ are constant with respect to $Q$. Therefore, minimizing the objective function is equivalent to maximizing the trace term:\n$$\n\\max_{Q \\in SO(D)} \\mathrm{Tr}(Q^{\\top} M), \\quad \\text{where} \\quad M = A_{c}^{\\top} B_{c} \\in \\mathbb{R}^{D \\times D}\n$$\nThis is a classic problem known as the Orthogonal Procrustes problem, with the additional constraint that the transformation must be a proper rotation.\n\nTo solve this maximization, we use the Singular Value Decomposition (SVD) of the matrix $M$. Let the SVD of $M$ be $M = U \\Sigma V^{\\top}$, where $U, V \\in \\mathbb{R}^{D \\times D}$ are orthogonal matrices ($U^{\\top}U = I$, $V^{\\top}V = I$) and $\\Sigma$ is a diagonal matrix with non-negative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_D \\ge 0$ on its diagonal.\nSubstituting the SVD into the trace:\n$$\n\\mathrm{Tr}(Q^{\\top} M) = \\mathrm{Tr}(Q^{\\top} U \\Sigma V^{\\top})\n$$\nAgain using the cyclic property of the trace:\n$$\n\\mathrm{Tr}(Q^{\\top} U \\Sigma V^{\\top}) = \\mathrm{Tr}(\\Sigma V^{\\top} Q^{\\top} U)\n$$\nLet's define a new matrix $R = V^{\\top} Q^{\\top} U$. Since $U$, $V$, and $Q$ are orthogonal, $R$ is also an orthogonal matrix. The problem is transformed into maximizing $\\mathrm{Tr}(\\Sigma R)$ over all orthogonal matrices $R$ that satisfy the determinant constraint.\n$$\n\\mathrm{Tr}(\\Sigma R) = \\sum_{i=1}^{D} (\\Sigma R)_{ii} = \\sum_{i=1}^{D} \\sigma_i R_{ii}\n$$\nSince $\\sigma_i \\ge 0$ and $R$ is orthogonal (implying $|R_{ii}| \\le 1$), this sum is maximized when $R_{ii}$ is as large as possible for each $i$, which is $R_{ii}=1$. This is achieved if $R$ is the identity matrix, $R=I$.\nIf $R=I$, then $V^{\\top}Q^{\\top}U = I$, which implies $Q^{\\top} = V U^{\\top}$, and thus $Q = U V^{\\top}$. This is the solution for the unconstrained orthogonal Procrustes problem.\n\nNow, we must enforce the constraint $\\det(Q) = +1$.\nThe determinant of our proposed solution is $\\det(Q) = \\det(U V^{\\top}) = \\det(U) \\det(V^{\\top}) = \\det(U) \\det(V)$. The determinants of $U$ and $V$ from SVD can be either $+1$ or $-1$.\nCase 1: $\\det(U)\\det(V) = +1$. In this case, $\\det(UV^{\\top}) = +1$, so our solution $Q^{\\star} = UV^{\\top}$ is a proper rotation and is the optimal solution.\nCase 2: $\\det(U)\\det(V) = -1$. In this case, $\\det(UV^{\\top}) = -1$, meaning the optimal orthogonal transformation is a reflection. We are constrained to find the best *proper rotation*. We must find an orthogonal matrix $Q$ with $\\det(Q)=+1$ that maximizes $\\mathrm{Tr}(Q^\\top M)$. This is equivalent to finding an orthogonal matrix $R=V^{\\top} Q^{\\top} U$ with $\\det(R) = \\det(V)\\det(Q)\\det(U) = \\det(V)(+1)\\det(U) = \\det(U)\\det(V) = -1$ that maximizes $\\mathrm{Tr}(\\Sigma R) = \\sum_{i} \\sigma_i R_{ii}$.\nWith $\\sigma_1 \\ge \\dots \\ge \\sigma_D \\ge 0$, the maximum is achieved by setting $R_{ii}=1$ for $i=1, \\dots, D-1$ and $R_{DD}=-1$ to satisfy $\\det(R) = -1$. This choice minimizes the penalty incurred by the negative sign, as it is applied to the smallest singular value $\\sigma_D$.\nSo, the optimal $R$ in this case is $R = \\mathrm{diag}(1, 1, \\dots, -1)$.\nThe optimal rotation $Q^{\\star}$ is then found by solving $V^{\\top} (Q^{\\star})^{\\top} U = \\mathrm{diag}(1, \\dots, -1)$ for $Q^{\\star}$, which gives $Q^{\\star} = U \\mathrm{diag}(1, \\dots, -1) V^{\\top}$.\n\nWe can unify both cases with a single expression. Let $S = \\mathrm{diag}(1, \\dots, 1, \\det(UV^{\\top}))$.\nThe optimal proper rotation is $Q^{\\star} = U S V^{\\top}$.\nIf $\\det(UV^{\\top}) = 1$, $S=I$ and $Q^{\\star} = UV^{\\top}$.\nIf $\\det(UV^{\\top}) = -1$, $S=\\mathrm{diag}(1, \\dots, -1)$ and $Q^{\\star} = U \\mathrm{diag}(1, \\dots, -1) V^{\\top}$.\n\nThe overall algorithm is:\n1.  For input matrices $A$ and $B$, compute the centered matrices $A_c$ and $B_c$.\n2.  Compute the matrix $M = A_c^{\\top} B_c$.\n3.  Perform the SVD of $M = U \\Sigma V^{\\top}$ to find $U$ and $V$.\n4.  Construct the optimal rotation $Q^{\\star} = U S V^{\\top}$, where $S = \\mathrm{diag}(1, \\dots, 1, \\det(UV^{\\top}))$.\n5.  Calculate the final error metric:\n$$\n\\mathrm{err} = \\frac{\\|A_{c} Q^{\\star} - B_{c}\\|_{F}^{2}}{\\|B_{c}\\|_{F}^{2}}\n$$\nThis procedure provides a unique and stable solution for all test cases, including those with rank-deficient geometries or reflection mismatches.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the neural population geometry alignment problem for all test cases.\n    \"\"\"\n\n    # Define the base matrix A1 and transformation matrices\n    A1 = np.array([\n        [0., 1., 2.],\n        [1., 0., 1.],\n        [2., 1., 0.],\n        [1., 2., 1.]\n    ])\n\n    Rz_90 = np.array([\n        [0., -1., 0.],\n        [1.,  0., 0.],\n        [0.,  0., 1.]\n    ])\n\n    # Test Case 1: Exact Rotation\n    A1_case = A1\n    B1_case = A1_case @ Rz_90\n\n    # Test Case 2: Rotation plus small noise\n    A2_case = A1\n    N2 = np.array([\n        [ 0.05, -0.03,  0.02],\n        [-0.01,  0.04, -0.02],\n        [ 0.00, -0.02,  0.03],\n        [ 0.02,  0.01, -0.01]\n    ])\n    B2_case = A2_case @ Rz_90 + N2\n\n    # Test Case 3: Rank-deficient geometry, rotation plus small noise\n    A3_case = np.array([\n        [1., 2., 3.],\n        [2., 4., 6.],\n        [3., 6., 9.],\n        [4., 8., 12.]\n    ])\n    N3 = np.array([\n        [ 0.10, -0.10,  0.00],\n        [ 0.00,  0.00,  0.10],\n        [-0.05,  0.02, -0.02],\n        [ 0.03, -0.04,  0.01]\n    ])\n    B3_case = A3_case @ Rz_90 + N3\n\n    # Test Case 4: Pure reflection mismatch\n    A4_case = A1\n    Fx = np.array([\n        [-1., 0., 0.],\n        [ 0., 1., 0.],\n        [ 0., 0., 1.]\n    ])\n    B4_case = A4_case @ Fx\n\n    test_cases = [\n        (A1_case, B1_case),\n        (A2_case, B2_case),\n        (A3_case, B3_case),\n        (A4_case, B4_case)\n    ]\n\n    results = []\n    for A, B in test_cases:\n        err = compute_alignment_error(A, B)\n        # Format the result to a reasonable precision for display\n        results.append(f\"{err:.8f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef compute_alignment_error(A, B):\n    \"\"\"\n    Computes the alignment error between two neural population geometries A and B.\n    \n    Args:\n        A (np.ndarray): The first C x D condition-mean response matrix.\n        B (np.ndarray): The second C x D condition-mean response matrix.\n\n    Returns:\n        float: The normalized residual alignment error.\n    \"\"\"\n    # Step 1: Remove translation by centering each geometry\n    # A_c = A - A.mean(axis=0) creates a new array for the mean that is broadcasted.\n    A_c = A - A.mean(axis=0, keepdims=True)\n    B_c = B - B.mean(axis=0, keepdims=True)\n\n    # Step 2: Compute the optimal orthogonal matrix Q*\n    # This involves solving the Orthogonal Procrustes problem for a proper rotation.\n    \n    # Construct the matrix M = A_c^T * B_c\n    D = A.shape[1]\n    M = A_c.T @ B_c\n    \n    # Perform Singular Value Decomposition (SVD) on M\n    # M = U * Sigma * V^T\n    try:\n        U, s, Vt = np.linalg.svd(M)\n    except np.linalg.LinAlgError:\n        # If SVD fails, this indicates a serious numerical issue with the input.\n        # For the given test cases, this is not expected.\n        return np.nan\n\n    # The optimal orthogonal matrix (could be rotation or reflection) is Q_ortho = U @ Vt.\n    # We need to ensure Q is a proper rotation, i.e., det(Q) = +1.\n    \n    # Check the determinant of U @ Vt\n    det_check = np.linalg.det(U @ Vt)\n    \n    # Create the correction matrix S.\n    # S is the identity, but if det(U @ Vt) is -1, the last diagonal\n    # element of S is flipped to -1. This corrects the 'reflection' case.\n    S = np.identity(D)\n    if det_check  0:\n        S[-1, -1] = -1\n        \n    # The optimal proper rotation matrix Q_star\n    Q_star = U @ S @ Vt\n\n    # Step 3: Quantify the residual alignment error\n    \n    # Calculate the squared Frobenius norm of the residual\n    residual_norm_sq = np.linalg.norm(A_c @ Q_star - B_c, 'fro')**2\n    \n    # Calculate the squared Frobenius norm of the target geometry\n    target_norm_sq = np.linalg.norm(B_c, 'fro')**2\n    \n    # Avoid division by zero, though not expected for these test cases.\n    # If target_norm_sq is zero, it means all points in B_c are the origin.\n    # If A_c is also all-zero, they are perfectly aligned (error 0).\n    # If A_c is not, they are unalignable (error inf). We'll assume the former.\n    if target_norm_sq  1e-15:\n        return 0.0 if np.linalg.norm(A_c, 'fro')**2  1e-15 else np.inf\n\n    err = residual_norm_sq / target_norm_sq\n    \n    return err\n\nsolve()\n\n```"
        }
    ]
}