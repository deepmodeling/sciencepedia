{
    "hands_on_practices": [
        {
            "introduction": "The Ornstein-Uhlenbeck (OU) process is a cornerstone for modeling neuronal dynamics, elegantly capturing the balance between a neuron's passive \"leak\" current pulling the membrane potential towards a resting state and the constant bombardment of stochastic synaptic inputs. This exercise guides you through the fundamental derivation of the process's solution and its transition probability density from first principles. Mastering this derivation provides a deep understanding of how a neuron's voltage evolves probabilistically over time and is a core skill for analyzing linear stochastic systems in neuroscience .",
            "id": "4023191",
            "problem": "Consider the subthreshold membrane potential of a single-compartment neuron modeled as an Ornstein–Uhlenbeck (OU) process. The dynamics are given by the stochastic differential equation (SDE)\n$$\n\\mathrm{d}V_t \\;=\\; -\\frac{1}{\\tau_m}\\big(V_t - V_{\\infty}\\big)\\,\\mathrm{d}t \\;+\\; \\sqrt{2D}\\,\\mathrm{d}W_t,\n$$\nwhere $V_t$ is the membrane potential at time $t$, $\\tau_m > 0$ is the membrane time constant, $V_{\\infty} \\in \\mathbb{R}$ is the asymptotic (drift) potential, $D > 0$ is the synaptic noise intensity, and $W_t$ is a standard Wiener process (also called Brownian motion). Assume the initial condition $V_0 = x_0$ is given and deterministic. Using only the fundamental definition of a stochastic differential equation, properties of the Wiener process, and the variation-of-constants (integrating factor) method, derive the explicit solution for $V_t$ and then compute the transition probability density $p(x,t \\mid x_0)$ for $t > 0$.\n\nYour derivation must proceed from first principles, including the computation of the time-dependent mean and variance of $V_t$. Express your final answer as a single closed-form analytic expression for $p(x,t \\mid x_0)$ in terms of $x$, $t$, $x_0$, $\\tau_m$, $V_{\\infty}$, and $D$. No numerical rounding is required, and no physical units should be included in the expression.",
            "solution": "The problem is well-posed and asks for the transition probability density $p(x,t \\mid x_0)$ of a process $V_t$ governed by the Ornstein-Uhlenbeck (OU) stochastic differential equation (SDE):\n$$\n\\mathrm{d}V_t = -\\frac{1}{\\tau_m}(V_t - V_{\\infty})\\,\\mathrm{d}t + \\sqrt{2D}\\,\\mathrm{d}W_t\n$$\nwith initial condition $V_0 = x_0$. Here, $V_t$ is the membrane potential, $\\tau_m$ is the membrane time constant, $V_{\\infty}$ is the asymptotic potential, $D$ is the noise intensity, and $W_t$ is a standard Wiener process.\n\nOur strategy is to first solve the SDE for $V_t$ using the integrating factor method. Then, we will determine the statistical properties of the solution, specifically its mean and variance. Since the SDE is linear in $V_t$ and driven by Gaussian white noise (the formal derivative of $W_t$), the resulting process $V_t$ for a deterministic initial condition $x_0$ is a Gaussian process. A Gaussian process is fully characterized by its mean and covariance function. For a fixed time $t$, the distribution of $V_t$ is a normal distribution, which is defined by its mean $\\mu(t)$ and variance $\\sigma^2(t)$. The transition probability density $p(x,t \\mid x_0)$ is the probability density function (PDF) of this normal distribution.\n\nFirst, we rearrange the SDE into a standard linear form:\n$$\n\\mathrm{d}V_t + \\frac{1}{\\tau_m}V_t\\,\\mathrm{d}t = \\frac{V_{\\infty}}{\\tau_m}\\,\\mathrm{d}t + \\sqrt{2D}\\,\\mathrm{d}W_t\n$$\nWe introduce the integrating factor $I(t) = \\exp\\left(\\int \\frac{1}{\\tau_m} \\mathrm{d}t\\right) = \\exp(t/\\tau_m)$. Multiplying the SDE by $I(t)$ yields:\n$$\n\\exp(t/\\tau_m)\\,\\mathrm{d}V_t + \\frac{1}{\\tau_m}\\exp(t/\\tau_m)V_t\\,\\mathrm{d}t = \\frac{V_{\\infty}}{\\tau_m}\\exp(t/\\tau_m)\\,\\mathrm{d}t + \\sqrt{2D}\\exp(t/\\tau_m)\\,\\mathrm{d}W_t\n$$\nThe left-hand side is the exact differential of the product $\\exp(t/\\tau_m)V_t$. This can be confirmed using Itô's product rule for a function $f(t, v) = g(t)v$. In our case, $g(t) = \\exp(t/\\tau_m)$ and Itô's rule simplifies to the standard product rule since the second derivative with respect to $v$ is zero: $\\mathrm{d}(\\exp(t/\\tau_m)V_t) = (\\frac{1}{\\tau_m}\\exp(t/\\tau_m)V_t)\\mathrm{d}t + \\exp(t/\\tau_m)\\mathrm{d}V_t$.\nThus, the equation becomes:\n$$\n\\mathrm{d}(\\exp(t/\\tau_m)V_t) = \\frac{V_{\\infty}}{\\tau_m}\\exp(t/\\tau_m)\\,\\mathrm{d}t + \\sqrt{2D}\\exp(t/\\tau_m)\\,\\mathrm{d}W_t\n$$\nWe now integrate both sides from time $s=0$ to $s=t$:\n$$\n\\int_0^t \\mathrm{d}(\\exp(s/\\tau_m)V_s) = \\int_0^t \\frac{V_{\\infty}}{\\tau_m}\\exp(s/\\tau_m)\\,\\mathrm{d}s + \\int_0^t \\sqrt{2D}\\exp(s/\\tau_m)\\,\\mathrm{d}W_s\n$$\nEvaluating the integrals gives:\n$$\n[\\exp(s/\\tau_m)V_s]_{s=0}^{s=t} = \\frac{V_{\\infty}}{\\tau_m}\\left[\\tau_m \\exp(s/\\tau_m)\\right]_{s=0}^{s=t} + \\sqrt{2D}\\int_0^t \\exp(s/\\tau_m)\\,\\mathrm{d}W_s\n$$\n$$\n\\exp(t/\\tau_m)V_t - \\exp(0)V_0 = V_{\\infty}(\\exp(t/\\tau_m) - \\exp(0)) + \\sqrt{2D}\\int_0^t \\exp(s/\\tau_m)\\,\\mathrm{d}W_s\n$$\nSubstituting the initial condition $V_0 = x_0$:\n$$\n\\exp(t/\\tau_m)V_t - x_0 = V_{\\infty}(\\exp(t/\\tau_m) - 1) + \\sqrt{2D}\\int_0^t \\exp(s/\\tau_m)\\,\\mathrm{d}W_s\n$$\nFinally, we solve for $V_t$ by multiplying by $\\exp(-t/\\tau_m)$:\n$$\nV_t = x_0\\exp(-t/\\tau_m) + V_{\\infty}(1 - \\exp(-t/\\tau_m)) + \\sqrt{2D}\\exp(-t/\\tau_m)\\int_0^t \\exp(s/\\tau_m)\\,\\mathrm{d}W_s\n$$\nThis expression can be written more compactly by bringing the factor $\\exp(-t/\\tau_m)$ inside the stochastic integral:\n$$\nV_t = x_0\\exp(-t/\\tau_m) + V_{\\infty}(1 - \\exp(-t/\\tau_m)) + \\sqrt{2D}\\int_0^t \\exp\\left(-\\frac{t-s}{\\tau_m}\\right)\\,\\mathrm{d}W_s\n$$\nThis is the explicit solution for $V_t$. Now we compute its mean and variance.\n\nThe mean of $V_t$, denoted $\\mu(t) = \\mathbb{E}[V_t]$, is found by taking the expectation of the solution. The first two terms are deterministic. The expectation of the Itô integral is zero, a fundamental property of the Wiener process.\n$$\n\\mu(t) = \\mathbb{E}\\left[x_0\\exp(-t/\\tau_m) + V_{\\infty}(1 - \\exp(-t/\\tau_m))\\right] + \\mathbb{E}\\left[\\sqrt{2D}\\int_0^t \\exp\\left(-\\frac{t-s}{\\tau_m}\\right)\\,\\mathrm{d}W_s\\right]\n$$\n$$\n\\mu(t) = x_0\\exp(-t/\\tau_m) + V_{\\infty}(1 - \\exp(-t/\\tau_m)) + 0\n$$\nSo, the mean is $\\mu(t) = V_{\\infty} + (x_0 - V_{\\infty})\\exp(-t/\\tau_m)$.\n\nThe variance of $V_t$, denoted $\\sigma^2(t) = \\text{Var}[V_t]$, is the variance of the stochastic part of the solution, as the other terms are deterministic.\n$$\n\\sigma^2(t) = \\text{Var}\\left[\\sqrt{2D}\\int_0^t \\exp\\left(-\\frac{t-s}{\\tau_m}\\right)\\,\\mathrm{d}W_s\\right] = \\mathbb{E}\\left[\\left(\\sqrt{2D}\\int_0^t \\exp\\left(-\\frac{t-s}{\\tau_m}\\right)\\,\\mathrm{d}W_s\\right)^2\\right]\n$$\nUsing the Itô isometry property, which states that $\\mathbb{E}\\left[(\\int_0^t f(s)\\,\\mathrm{d}W_s)^2\\right] = \\int_0^t \\mathbb{E}[f(s)^2]\\,\\mathrm{d}s$, and since our integrand $f(s) = \\exp(-(t-s)/\\tau_m)$ is deterministic, this simplifies to $\\int_0^t f(s)^2\\,\\mathrm{d}s$:\n$$\n\\sigma^2(t) = 2D \\int_0^t \\left(\\exp\\left(-\\frac{t-s}{\\tau_m}\\right)\\right)^2\\,\\mathrm{d}s = 2D \\int_0^t \\exp\\left(-\\frac{2(t-s)}{\\tau_m}\\right)\\,\\mathrm{d}s\n$$\n$$\n\\sigma^2(t) = 2D \\exp(-2t/\\tau_m) \\int_0^t \\exp(2s/\\tau_m)\\,\\mathrm{d}s\n$$\nEvaluating the integral:\n$$\n\\int_0^t \\exp(2s/\\tau_m)\\,\\mathrm{d}s = \\left[\\frac{\\tau_m}{2}\\exp(2s/\\tau_m)\\right]_0^t = \\frac{\\tau_m}{2}(\\exp(2t/\\tau_m) - 1)\n$$\nSubstituting this back into the expression for the variance:\n$$\n\\sigma^2(t) = 2D \\exp(-2t/\\tau_m) \\left(\\frac{\\tau_m}{2}(\\exp(2t/\\tau_m) - 1)\\right) = D\\tau_m (1 - \\exp(-2t/\\tau_m))\n$$\nSince $V_t$ is a Gaussian random variable with mean $\\mu(t)$ and variance $\\sigma^2(t)$, its probability density function is given by the normal distribution formula:\n$$\np(x,t \\mid x_0) = \\mathcal{N}(x; \\mu(t), \\sigma^2(t)) = \\frac{1}{\\sqrt{2\\pi\\sigma^2(t)}} \\exp\\left( -\\frac{(x-\\mu(t))^2}{2\\sigma^2(t)} \\right)\n$$\nSubstituting the derived expressions for $\\mu(t)$ and $\\sigma^2(t)$:\n$$\np(x,t \\mid x_0) = \\frac{1}{\\sqrt{2\\pi D\\tau_m(1 - \\exp(-2t/\\tau_m))}} \\exp\\left( -\\frac{\\left(x - \\left(V_{\\infty} + (x_0 - V_{\\infty})\\exp(-t/\\tau_m)\\right)\\right)^2}{2D\\tau_m(1 - \\exp(-2t/\\tau_m))} \\right)\n$$\nThis is the final expression for the transition probability density of the Ornstein-Uhlenbeck process.",
            "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{2\\pi D\\tau_m(1 - \\exp(-2t/\\tau_m))}} \\exp\\left(-\\frac{\\left(x - V_{\\infty} - (x_0 - V_{\\infty})\\exp(-t/\\tau_m)\\right)^2}{2D\\tau_m(1 - \\exp(-2t/\\tau_m))}\\right)}\n$$"
        },
        {
            "introduction": "Building on the concept of a fluctuating membrane potential, a critical question is: when does the neuron fire? This practice simplifies the neuron's dynamics to a drifted Brownian motion and asks for the statistics of the first passage time to a firing threshold, a model known as the \"perfect integrate-and-fire\" neuron. By using the powerful method of the infinitesimal generator, you will calculate the mean and variance of the interspike interval, connecting the underlying parameters of neural input directly to the observable firing statistics .",
            "id": "4023143",
            "problem": "A single-compartment neuron model can be approximated near rest by a one-dimensional Stochastic Differential Equation (SDE) for its membrane potential, driven by constant input and fast synaptic noise. Let the membrane potential be modeled by a drifted Brownian motion satisfying the SDE $dX_t = \\mu\\, dt + \\sigma\\, dW_t$, where $X_0 = x_0$ with $x_0 < a$, $\\mu > 0$ is a constant drift reflecting net input current, $\\sigma > 0$ quantifies synaptic noise amplitude, and $W_t$ is a standard Brownian motion. Define the first passage time $\\tau$ to a firing threshold $a$ by $\\tau := \\inf\\{t \\ge 0 : X_t = a\\}$. Use a derivation grounded in the infinitesimal generator of the process and the Laplace transform of the stopping time (without invoking any pre-stated closed-form hitting-time distributions) to obtain closed-form analytic expressions for the mean $\\mathbb{E}[\\tau]$ and the variance $\\operatorname{Var}(\\tau)$ in terms of $\\mu$, $\\sigma$, $x_0$, and $a$. Express the mean in seconds and the variance in seconds squared. Your final answer must be a single closed-form analytic expression collecting both quantities as entries of a row matrix. No numerical approximation is required.",
            "solution": "Let $X_t$ be the process governed by the SDE $dX_t = \\mu\\, dt + \\sigma\\, dW_t$, with initial state $X_0 = x_0$. We are interested in the moments of the first passage time to a threshold $a > x_0$, defined as $\\tau = \\inf\\{t \\geq 0 : X_t = a\\}$.\n\nOur derivation proceeds by first finding the Laplace transform of the stopping time $\\tau$, and then using its moment-generating properties to find the mean $\\mathbb{E}[\\tau]$ and variance $\\operatorname{Var}(\\tau)$. The infinitesimal generator $\\mathcal{L}$ for the Itô diffusion $X_t$ is given by:\n$$\n\\mathcal{L}f(x) = \\mu \\frac{df}{dx} + \\frac{1}{2}\\sigma^2 \\frac{d^2f}{dx^2}\n$$\n\nLet $u(x_0; \\lambda) = \\mathbb{E}_{x_0}[\\exp(-\\lambda \\tau)]$ be the Laplace transform of $\\tau$ for a given starting position $x_0$ and a parameter $\\lambda > 0$. The notation $\\mathbb{E}_{x_0}[\\cdot]$ denotes the expectation conditional on $X_0=x_0$. It is a standard result from the theory of stochastic processes that $u(x; \\lambda)$ satisfies the ordinary differential equation (ODE):\n$$\n\\mathcal{L}u(x; \\lambda) - \\lambda u(x; \\lambda) = 0\n$$\nSubstituting the expression for the generator $\\mathcal{L}$, we get:\n$$\n\\frac{1}{2}\\sigma^2 u''(x; \\lambda) + \\mu u'(x; \\lambda) - \\lambda u(x; \\lambda) = 0\n$$\nwhere primes denote differentiation with respect to $x$.\n\nWe need to solve this second-order linear homogeneous ODE for $x < a$. This requires two boundary conditions.\n1.  If the process starts at the threshold, $x_0 = a$, the time to reach it is $\\tau = 0$. Thus, $u(a; \\lambda) = \\mathbb{E}_a[\\exp(-\\lambda \\cdot 0)] = 1$.\n2.  For the second condition, we consider the behavior as $x_0 \\to -\\infty$. Since $\\mu > 0$, the drift pushes the process towards $a$. For the Laplace transform to be a well-behaved probability (i.e., bounded), we must discard any solution that grows unboundedly as $x_0 \\to -\\infty$.\n\nThe characteristic equation for the ODE is:\n$$\n\\frac{1}{2}\\sigma^2 r^2 + \\mu r - \\lambda = 0\n$$\nThe roots are given by the quadratic formula:\n$$\nr = \\frac{-\\mu \\pm \\sqrt{\\mu^2 - 4(\\frac{1}{2}\\sigma^2)(-\\lambda)}}{\\sigma^2} = \\frac{-\\mu \\pm \\sqrt{\\mu^2 + 2\\sigma^2\\lambda}}{\\sigma^2}\n$$\nLet's denote the two roots as $r_1$ and $r_2$:\n$$\nr_1 = \\frac{-\\mu + \\sqrt{\\mu^2 + 2\\sigma^2\\lambda}}{\\sigma^2} > 0\n$$\n$$\nr_2 = \\frac{-\\mu - \\sqrt{\\mu^2 + 2\\sigma^2\\lambda}}{\\sigma^2} < 0\n$$\nThe general solution to the ODE is:\n$$\nu(x; \\lambda) = C_1 \\exp(r_1 x) + C_2 \\exp(r_2 x)\n$$\nApplying the boundary condition at $x \\to -\\infty$: for the solution to remain bounded, the coefficient of the term that grows as $x \\to -\\infty$ must be zero. Since $r_2 < 0$, $\\exp(r_2 x)$ diverges as $x \\to -\\infty$. Therefore, we must set $C_2 = 0$.\n\nThe solution simplifies to $u(x; \\lambda) = C_1 \\exp(r_1 x)$. Now, we apply the boundary condition at $x=a$:\n$$\nu(a; \\lambda) = C_1 \\exp(r_1 a) = 1 \\implies C_1 = \\exp(-r_1 a)\n$$\nThus, the Laplace transform of $\\tau$ for a starting point $x_0$ is:\n$$\nu(x_0; \\lambda) = \\exp(-r_1 a) \\exp(r_1 x_0) = \\exp(r_1(x_0-a))\n$$\nSubstituting the expression for $r_1$:\n$$\nu(x_0; \\lambda) = \\mathbb{E}_{x_0}[\\exp(-\\lambda \\tau)] = \\exp\\left( \\left(\\frac{-\\mu + \\sqrt{\\mu^2 + 2\\sigma^2\\lambda}}{\\sigma^2}\\right)(x_0 - a) \\right)\n$$\nThe moments of $\\tau$ can be found by taking derivatives of $u(x_0; \\lambda)$ with respect to $\\lambda$ and evaluating at $\\lambda = 0$. Specifically:\n$$\n\\mathbb{E}_{x_0}[\\tau^k] = (-1)^k \\frac{d^k}{d\\lambda^k} u(x_0; \\lambda) \\bigg|_{\\lambda=0}\n$$\n\n**Mean First Passage Time, $\\mathbb{E}[\\tau]$**\nThe mean is the negative of the first derivative at $\\lambda=0$:\n$$\n\\mathbb{E}[\\tau] = -\\frac{d}{d\\lambda} u(x_0; \\lambda) \\bigg|_{\\lambda=0}\n$$\n(From here on, we drop the subscript on $\\mathbb{E}$ for brevity, as the starting point $x_0$ is fixed). Let $g(\\lambda) = r_1(x_0-a)$. Then $u(x_0; \\lambda) = \\exp(g(\\lambda))$.\n$$\n\\frac{du}{d\\lambda} = u(x_0; \\lambda) \\cdot \\frac{dg}{d\\lambda}\n$$\nFirst, we find $\\frac{dg}{d\\lambda}$:\n$$\n\\frac{dg}{d\\lambda} = \\frac{d}{d\\lambda} \\left[ \\left(\\frac{x_0 - a}{\\sigma^2}\\right) \\left(-\\mu + \\sqrt{\\mu^2 + 2\\sigma^2\\lambda}\\right) \\right] = \\left(\\frac{x_0 - a}{\\sigma^2}\\right) \\left( \\frac{1}{2\\sqrt{\\mu^2 + 2\\sigma^2\\lambda}} \\cdot 2\\sigma^2 \\right) = \\frac{x_0 - a}{\\sqrt{\\mu^2 + 2\\sigma^2\\lambda}}\n$$\nEvaluating at $\\lambda=0$:\n$$\n\\frac{dg}{d\\lambda}\\bigg|_{\\lambda=0} = \\frac{x_0 - a}{\\sqrt{\\mu^2}} = \\frac{x_0 - a}{\\mu}\n$$\nAlso, we note that $u(x_0; 0) = \\mathbb{E}[\\exp(0)] = 1$. This corresponds to the probability of hitting the threshold, which is $1$ for $\\mu > 0$.\nSo,\n$$\n\\frac{du}{d\\lambda}\\bigg|_{\\lambda=0} = u(x_0; 0) \\cdot \\frac{dg}{d\\lambda}\\bigg|_{\\lambda=0} = 1 \\cdot \\frac{x_0 - a}{\\mu} = \\frac{x_0 - a}{\\mu}\n$$\nThe mean first passage time is:\n$$\n\\mathbb{E}[\\tau] = - \\left( \\frac{x_0 - a}{\\mu} \\right) = \\frac{a - x_0}{\\mu}\n$$\n\n**Variance of the First Passage Time, $\\operatorname{Var}(\\tau)$**\nFirst, we find the second moment $\\mathbb{E}[\\tau^2] = \\frac{d^2}{d\\lambda^2} u(x_0; \\lambda) \\bigg|_{\\lambda=0}$.\n$$\n\\frac{d^2u}{d\\lambda^2} = \\frac{d}{d\\lambda}\\left( u \\cdot \\frac{dg}{d\\lambda} \\right) = \\left(\\frac{du}{d\\lambda}\\right) \\left(\\frac{dg}{d\\lambda}\\right) + u \\left(\\frac{d^2g}{d\\lambda^2}\\right) = u \\left(\\frac{dg}{d\\lambda}\\right)^2 + u \\left(\\frac{d^2g}{d\\lambda^2}\\right)\n$$\nWe need to calculate the second derivative of $g(\\lambda)$:\n$$\n\\frac{d^2g}{d\\lambda^2} = \\frac{d}{d\\lambda} \\left[ (x_0-a)(\\mu^2 + 2\\sigma^2\\lambda)^{-1/2} \\right] = (x_0-a) \\left(-\\frac{1}{2}\\right) (\\mu^2 + 2\\sigma^2\\lambda)^{-3/2} (2\\sigma^2) = -(x_0-a)\\sigma^2 (\\mu^2 + 2\\sigma^2\\lambda)^{-3/2}\n$$\nEvaluating at $\\lambda=0$:\n$$\n\\frac{d^2g}{d\\lambda^2}\\bigg|_{\\lambda=0} = -(x_0-a)\\sigma^2 (\\mu^2)^{-3/2} = -(x_0-a)\\frac{\\sigma^2}{\\mu^3} = (a-x_0)\\frac{\\sigma^2}{\\mu^3}\n$$\nNow we can compute the second moment:\n$$\n\\mathbb{E}[\\tau^2] = \\frac{d^2u}{d\\lambda^2}\\bigg|_{\\lambda=0} = u(x_0; 0) \\left( \\frac{dg}{d\\lambda}\\bigg|_{\\lambda=0} \\right)^2 + u(x_0; 0) \\left( \\frac{d^2g}{d\\lambda^2}\\bigg|_{\\lambda=0} \\right)\n$$\n$$\n\\mathbb{E}[\\tau^2] = 1 \\cdot \\left(\\frac{x_0 - a}{\\mu}\\right)^2 + 1 \\cdot \\left((a-x_0)\\frac{\\sigma^2}{\\mu^3}\\right) = \\frac{(a-x_0)^2}{\\mu^2} + \\frac{(a-x_0)\\sigma^2}{\\mu^3}\n$$\nThe variance is $\\operatorname{Var}(\\tau) = \\mathbb{E}[\\tau^2] - (\\mathbb{E}[\\tau])^2$.\n$$\n\\operatorname{Var}(\\tau) = \\left( \\frac{(a-x_0)^2}{\\mu^2} + \\frac{(a-x_0)\\sigma^2}{\\mu^3} \\right) - \\left( \\frac{a - x_0}{\\mu} \\right)^2\n$$\n$$\n\\operatorname{Var}(\\tau) = \\left( \\frac{(a-x_0)^2}{\\mu^2} + \\frac{(a-x_0)\\sigma^2}{\\mu^3} \\right) - \\frac{(a - x_0)^2}{\\mu^2} = \\frac{(a-x_0)\\sigma^2}{\\mu^3}\n$$\n\nThe final expressions are:\nMean: $\\mathbb{E}[\\tau] = \\frac{a-x_0}{\\mu}$\nVariance: $\\operatorname{Var}(\\tau) = \\frac{(a-x_0)\\sigma^2}{\\mu^3}$\n\nThe units are consistent. For membrane potential in volts ($V$) and time in seconds ($s$), $[\\mu]$ is $V/s$ and $[\\sigma^2]$ is $V^2/s$.\n$[\\mathbb{E}[\\tau]] = [V] / [V/s] = s$.\n$[\\operatorname{Var}(\\tau)] = [V] \\cdot [V^2/s] / [V^3/s^3] = [V^3/s] / [V^3/s^3] = s^2$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{a - x_0}{\\mu} & \\frac{(a - x_0)\\sigma^2}{\\mu^3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "After modeling spike generation, we must turn to the analysis of spike train data, where subtle statistical traps await. This problem confronts the famous \"inspection paradox,\" a counter-intuitive phenomenon that arises when sampling sequential events like neural spikes at random moments in time. By deriving the distribution of the waiting time to the next spike, you will uncover why random sampling is inherently biased and learn to correctly interpret the statistics of renewal processes, a critical skill for any experimental or theoretical neuroscientist .",
            "id": "4023175",
            "problem": "A stationary spike train from a single neuron is modeled as a renewal process with independent and identically distributed interspike intervals (ISIs) $\\{X_{n}\\}_{n \\ge 1}$ with common probability density function $f_{X}(x)$ supported on $x \\in (0,\\infty)$, finite mean $\\mu = \\mathbb{E}[X] \\in (0,\\infty)$, and finite second moment $\\mathbb{E}[X^{2}] < \\infty$. You observe the spike train at a time chosen uniformly at random over a very long recording window, independent of the spikes. Let $R$ denote the forward recurrence time, that is, the time from the observation to the next spike. Let $B$ denote the backward recurrence time to the previous spike, and $C = B + R$ the length of the ISI that contains the observation time.\n\nStarting from the defining properties of a stationary renewal process, the definition of size-biased sampling for interval lengths, and basic conditioning rules of probability, derive the probability density function of the forward recurrence time $R$ in terms of the interspike interval distribution. Then, express the mean of $R$ in terms of the first two moments of $X$ and interpret this mean relative to the mean ISI, using the coefficient of variation (CV) $\\mathrm{CV} = \\sqrt{\\operatorname{Var}(X)}/\\mu$. Use these derivations to explain the inspection paradox in spike train sampling by comparing the mean of the length of the containing interval $C$ to the mean ISI and by relating $R$ to $C$.\n\nYour derivation must proceed from first principles appropriate to renewal processes and basic probability, without invoking prepackaged formulas for equilibrium or forward-recurrence distributions.\n\nProvide, as your final answer, a single closed-form analytic expression for the probability density function $f_{R}(t)$ of the forward recurrence time $R$ in terms of the interspike interval cumulative distribution function $F_{X}(t)$ and the mean $\\mu$. No numerical approximations are required, and no units are needed.",
            "solution": "The problem is well-posed and asks for a derivation concerning the inspection paradox in renewal theory, applied to neural spike trains. A full, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe derivation proceeds from the principle of size-biased sampling, which arises from observing a process at a random point in time.\n\n#### 1. The Distribution of the Containing Interval $C$\nConsider a very long time duration $T$. The number of ISIs, $N$, occurring in this duration is approximately $T/\\mu$. The collection of intervals $\\{X_i\\}_{i=1}^N$ can be binned by their length. The number of intervals with length in a small range $[x, x+dx)$ is approximately $N f_X(x) dx$. The total time occupied by all such intervals is their number multiplied by their length, which is $(N f_X(x) dx) \\cdot x$.\n\nThe probability of a randomly chosen observation time falling within an interval of length in $[x, x+dx)$ is proportional to the total time these intervals occupy. Let $f_C(x)$ be the PDF of the length $C$ of the containing interval.\n$$\nP(x \\leq C \\leq x+dx) = f_C(x)dx \\propto (N f_X(x) dx) \\cdot x\n$$\nThis implies that the density $f_C(x)$ is proportional to $x f_X(x)$. To make this a valid PDF, we must normalize it by dividing by the integral over all possible lengths:\n$$\n\\int_0^\\infty y f_X(y) dy = \\mathbb{E}[X] = \\mu\n$$\nTherefore, the PDF of the length of the containing interval $C$ is\n$$\nf_C(x) = \\frac{x f_X(x)}{\\mu}\n$$\nThis is known as the size-biased distribution corresponding to $f_X(x)$. It formalizes the intuition that longer intervals are more likely to be sampled.\n\n#### 2. The Distribution of the Forward Recurrence Time $R$\nWe aim to find the PDF of the forward recurrence time, $f_R(t)$. It is often easier to first derive the survivor function, $S_R(t) = P(R > t)$, and then obtain the PDF by differentiation: $f_R(t) = -\\frac{d}{dt}S_R(t)$.\n\nWe can find $P(R > t)$ by conditioning on the length of the containing interval, $C$.\n$$\nP(R > t) = \\int_0^\\infty P(R > t | C=x) f_C(x) dx\n$$\nGiven that the observation time falls into an interval of a specific length $C=x$, the \"stationary\" nature of the observation implies that the observation time is uniformly distributed over this interval $[0, x]$. Let the time of the previous spike be $0$ and the time of the next spike be $x$. The observation time $T_{obs}$ is a random variable Uniform$(0, x)$. The backward recurrence time is $B = T_{obs}$ and the forward recurrence time is $R = x - T_{obs}$.\n\nThe conditional probability $P(R > t | C=x)$ is the probability that $x - T_{obs} > t$, or $T_{obs} < x-t$.\nIf $t \\ge x$, this is impossible, so the probability is $0$.\nIf $t < x$, since $T_{obs} \\sim \\text{Uniform}(0,x)$, the probability is $P(T_{obs} < x-t) = \\frac{x-t}{x}$.\nSo, $P(R > t | C=x) = \\frac{(x-t)^+}{x}$, where $(\\cdot)^+$ denotes the positive part.\n\nSubstituting this and the expression for $f_C(x)$ into the integral for $P(R > t)$:\n$$\nP(R > t) = \\int_t^\\infty \\left(\\frac{x-t}{x}\\right) \\left(\\frac{x f_X(x)}{\\mu}\\right) dx\n$$\nThe integral's lower limit is $t$ because for $x < t$, the term $(x-t)$ is negative and the probability is zero.\n$$\nP(R > t) = \\frac{1}{\\mu} \\int_t^\\infty (x-t) f_X(x) dx\n$$\nThis expression can be simplified. Recall that the survivor function of the original ISI distribution is $S_X(x) = P(X > x) = \\int_x^\\infty f_X(u)du = 1 - F_X(x)$. We can show that $\\int_t^\\infty (x-t)f_X(x)dx = \\int_t^\\infty S_X(u)du$. To prove this, we use integration by parts on $\\int_t^\\infty (x-t)f_X(x)dx$. Let $u = x-t$ and $dv = f_X(x)dx$. Then $du=dx$ and $v = -\\int_x^\\infty f_X(y)dy = -S_X(x)$.\n\\begin{align*}\n\\int_t^\\infty (x-t)f_X(x)dx &= \\left[-(x-t)S_X(x)\\right]_t^\\infty - \\int_t^\\infty (-S_X(x))dx \\\\\n&= \\left( \\lim_{x\\to\\infty} -(x-t)S_X(x) \\right) - (-(t-t)S_X(t)) + \\int_t^\\infty S_X(x)dx \\\\\n&= 0 + 0 + \\int_t^\\infty S_X(x)dx\n\\end{align*}\nThe limit term $\\lim_{x\\to\\infty} xS_X(x) = 0$ is a consequence of the finite second moment $\\mathbb{E}[X^2] < \\infty$. Specifically, $\\mathbb{E}[X^2] = 2\\int_0^\\infty x S_X(x) dx$, so for this integral to converge, the integrand must tend to $0$.\n\nThus, the survivor function for the forward recurrence time is:\n$$\nS_R(t) = P(R > t) = \\frac{1}{\\mu} \\int_t^\\infty S_X(x) dx = \\frac{1}{\\mu} \\int_t^\\infty (1 - F_X(x)) dx\n$$\nTo find the PDF $f_R(t)$, we differentiate $S_R(t)$ with respect to $t$, using the Fundamental Theorem of Calculus:\n$$\nf_R(t) = -\\frac{d}{dt} S_R(t) = -\\frac{d}{dt} \\left[ \\frac{1}{\\mu} \\int_t^\\infty (1 - F_X(x)) dx \\right] = -\\frac{1}{\\mu} \\left( -(1 - F_X(t)) \\right)\n$$\nThis gives the desired PDF for the forward recurrence time:\n$$\nf_R(t) = \\frac{1 - F_X(t)}{\\mu}\n$$\n\n#### 3. The Inspection Paradox\nThe inspection paradox is revealed by comparing the mean values of the sampled interval $C$ and the waiting time $R$ to the intrinsic properties of the process, namely the mean ISI $\\mu$.\n\nFirst, let's find the mean forward recurrence time, $\\mathbb{E}[R]$. For any non-negative random variable, the mean is the integral of its survivor function:\n$$\n\\mathbb{E}[R] = \\int_0^\\infty S_R(t) dt = \\int_0^\\infty \\left( \\frac{1}{\\mu} \\int_t^\\infty S_X(x) dx \\right) dt = \\frac{1}{\\mu} \\int_0^\\infty \\int_t^\\infty S_X(x) dx dt\n$$\nWe change the order of integration. The domain is $0 \\le t \\le x < \\infty$.\n$$\n\\mathbb{E}[R] = \\frac{1}{\\mu} \\int_0^\\infty S_X(x) \\left( \\int_0^x dt \\right) dx = \\frac{1}{\\mu} \\int_0^\\infty x S_X(x) dx\n$$\nThe integral $\\int_0^\\infty x S_X(x) dx$ is related to the second moment of $X$. For a non-negative random variable, $\\mathbb{E}[X^2] = 2\\int_0^\\infty x S_X(x) dx$. Therefore, $\\int_0^\\infty x S_X(x) dx = \\frac{\\mathbb{E}[X^2]}{2}$.\nSubstituting this in, we get:\n$$\n\\mathbb{E}[R] = \\frac{1}{\\mu} \\left( \\frac{\\mathbb{E}[X^2]}{2} \\right) = \\frac{\\mathbb{E}[X^2]}{2\\mu}\n$$\nNow, we express this using the coefficient of variation, $\\mathrm{CV} = \\frac{\\sqrt{\\operatorname{Var}(X)}}{\\mu}$. We know $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\mathbb{E}[X^2] - \\mu^2$. This gives $\\mu^2 \\mathrm{CV}^2 = \\mathbb{E}[X^2] - \\mu^2$, or $\\mathbb{E}[X^2] = \\mu^2 + \\mu^2 \\mathrm{CV}^2 = \\mu^2(1+\\mathrm{CV}^2)$.\nSubstituting this into the expression for $\\mathbb{E}[R]$:\n$$\n\\mathbb{E}[R] = \\frac{\\mu^2(1+\\mathrm{CV}^2)}{2\\mu} = \\frac{\\mu}{2}(1+\\mathrm{CV}^2)\n$$\nA naive guess for the average waiting time might be $\\mu/2$. However, our result shows that $\\mathbb{E}[R] \\ge \\mu/2$, with equality only if $\\mathrm{CV}=0$ (i.e., the ISIs are constant). For any variability in spike timing, the average wait is longer than $\\mu/2$. For a Poisson process, ISIs are exponential, $\\mathrm{CV}=1$, and $\\mathbb{E}[R]=\\mu$.\n\nNow, consider the mean length of the containing interval, $\\mathbb{E}[C]$. Since the process is stationary, the statistics of the backward recurrence time $B$ are identical to those of the forward recurrence time $R$. Thus, $\\mathbb{E}[B]=\\mathbb{E}[R]$.\n$$\n\\mathbb{E}[C] = \\mathbb{E}[B+R] = \\mathbb{E}[B] + \\mathbb{E}[R] = 2\\mathbb{E}[R] = 2 \\left( \\frac{\\mathbb{E}[X^2]}{2\\mu} \\right) = \\frac{\\mathbb{E}[X^2]}{\\mu}\n$$\nUsing the CV expression again:\n$$\n\\mathbb{E}[C] = \\frac{\\mu^2(1+\\mathrm{CV}^2)}{\\mu} = \\mu(1+\\mathrm{CV}^2)\n$$\n**Explanation of the Paradox**:\n1.  **Biased Sampling**: The mean length of the interval containing our random observation, $\\mathbb{E}[C] = \\mu(1+\\mathrm{CV}^2)$, is greater than or equal to the true mean ISI, $\\mu = \\mathbb{E}[X]$. This is because our random observation is more likely to fall into a longer interval than a shorter one (size-biasing). The difference is $\\mathbb{E}[C]-\\mu = \\mu\\mathrm{CV}^2 = \\operatorname{Var}(X)/\\mu$, which is directly proportional to the variance of the ISIs.\n2.  **Waiting Time**: When we land in an interval of length $C$, our position is uniform within it. The expected time to the next spike from that position is $\\mathbb{E}[R|C] = C/2$. The overall expected waiting time is thus $\\mathbb{E}[R] = \\mathbb{E}[C/2] = \\mathbb{E}[C]/2$. crucially, this is half the mean of the *biased* containing interval, not half the mean of a typical interval. Therefore, $\\mathbb{E}[R] = \\frac{\\mu(1+\\mathrm{CV}^2)}{2}$.\n\nThe paradox is that randomly inspecting the process leads to observing intervals that are, on average, longer than the true average interval, and consequently, the waiting time to the next event is longer than one might naively expect.",
            "answer": "$$\n\\boxed{\\frac{1 - F_{X}(t)}{\\mu}}\n$$"
        }
    ]
}