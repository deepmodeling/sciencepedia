{
    "hands_on_practices": [
        {
            "introduction": "在应用最优线性估计时，一个核心问题是理解其基础假设。虽然普通最小二乘法（OLS）等简单估计器很常见，但它们的“最优性”依赖于对噪声的严格假设。本练习将通过一个具体的反例，量化当噪声的零均值和同方差假设被违背时，估计器会产生怎样的偏差和方差膨胀，从而揭示考虑真实噪声结构（即使用广义最小二乘法）的必要性。",
            "id": "4006252",
            "problem": "在大脑建模和计算神经科学中，考虑一个线性神经群体解码情景。一个标量刺激 $s \\in \\mathbb{R}$ 驱动一个由两个神经元组成的群体，其调谐向量为 $f \\in \\mathbb{R}^{2}$。在单次试验中观测到的放电率向量 $r \\in \\mathbb{R}^{2}$ 被建模为\n$$\nr = f\\,s + \\varepsilon,\n$$\n其中 $\\varepsilon \\in \\mathbb{R}^{2}$ 是随机噪声。线性解码器寻求一个估计值 $\\hat{s} = w^{\\top} r$，其中 $w \\in \\mathbb{R}^{2}$ 是权重向量。\n\n在经典的高斯-马尔可夫设定中，最佳线性无偏估计量 (BLUE) 的定义基于噪声是零均值和同方差的假设，其协方差与单位矩阵成正比。在神经解码中，这些假设经常不被满足。您的目标是提供一个具体的反例，说明违背零均值或同方差性假设如何分别通过引入偏差或增大方差来破坏 BLUE 的最优性，并量化这两种效应。\n\n使用以下具体的、科学上合理的参数：\n- 调谐向量 $f = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$。\n- 噪声均值 $m = \\mathbb{E}[\\varepsilon] = \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix}$。\n- 噪声协方差 $\\Sigma = \\operatorname{Cov}[\\varepsilon] = \\begin{pmatrix}1  0 \\\\ 0  9\\end{pmatrix}$，这在神经元之间是异方差的。\n\n假设解码器是在经典的普通最小二乘法 (OLS) 假设下构建的——即，它忽略了异方差性和非零均值的违背情况——并选择在噪声是零均值和同方差的特殊情况下最优的权重，即在该特殊情况下，在无偏性约束下最小化估计量的方差。\n\n从无偏性（$\\mathbb{E}[\\hat{s}]=s$）、方差（$\\operatorname{Var}[\\hat{s}]$）的基本定义，以及用于约束二次最小化的拉格朗日乘数法出发，完成以下任务：\n1. 在同方差零均值假设下，推导出 OLS 权重 $w_{\\mathrm{OLS}}$，并计算当真实噪声均值为上述给定的 $m \\neq 0$ 时产生的偏差 $b = \\mathbb{E}[\\hat{s}]-s$。\n2. 当真实协方差为上述给定的 $\\Sigma$ 时，在无偏性约束下推导出最小化 $\\operatorname{Var}[\\hat{s}]$ 的广义最小二乘法 (GLS) 权重 $w_{\\mathrm{GLS}}$，并计算估计量方差之比 $\\rho = \\frac{\\operatorname{Var}_{\\mathrm{OLS}}(\\hat{s})}{\\operatorname{Var}_{\\mathrm{GLS}}(\\hat{s})}$。\n\n将您的最终答案表示为精确的有理数，并将这两个量 $(b, \\rho)$ 一起以单个行向量的形式报告。不需要单位。不要四舍五入。最终答案必须是包含偏差和方差膨胀比的单行矩阵，并按此顺序排列。",
            "solution": "我们从线性观测模型开始\n$$\nr = f\\,s + \\varepsilon,\n$$\n其调谐向量为 $f \\in \\mathbb{R}^{2}$，噪声均值为 $m = \\mathbb{E}[\\varepsilon]$，协方差为 $\\Sigma = \\operatorname{Cov}[\\varepsilon]$。线性解码器使用 $\\hat{s} = w^{\\top} r$。\n\n无偏性定义为对所有 $s$ 都有 $\\mathbb{E}[\\hat{s}] = s$。利用期望的线性性质，\n$$\n\\mathbb{E}[\\hat{s}] = \\mathbb{E}[w^{\\top} r] = w^{\\top} \\mathbb{E}[r] = w^{\\top} (f\\,s + m) = s\\, w^{\\top} f + w^{\\top} m.\n$$\n为了对所有 $s$ 都无偏，必须满足\n$$\nw^{\\top} f = 1, \\quad \\text{和} \\quad w^{\\top} m = 0.\n$$\n在实践中，通常的普通最小二乘法 (OLS) 构建假定 $m = 0$ 和同方差噪声，在这种情况下，无偏性约束简化为 $w^{\\top} f = 1$。估计量的方差为\n$$\n\\operatorname{Var}[\\hat{s}] = \\operatorname{Var}[w^{\\top} r] = \\operatorname{Var}[w^{\\top} \\varepsilon] = w^{\\top} \\Sigma\\, w.\n$$\n\n第1部分：非零均值噪声下的 OLS 权重和偏差。\n\n在同方差假设 $\\Sigma = \\sigma^{2} I$ 下，最小化 $\\operatorname{Var}[\\hat{s}] = \\sigma^{2} w^{\\top} w$（约束条件为 $w^{\\top} f = 1$）等价于在相同约束下最小化 $w^{\\top} w$。使用拉格朗日乘数法，最小化\n$$\n\\mathcal{L}(w, \\lambda) = w^{\\top} w + \\lambda (w^{\\top} f - 1).\n$$\n将梯度设为零，\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = 2 w + \\lambda f = 0 \\quad \\Rightarrow \\quad w = -\\frac{\\lambda}{2} f.\n$$\n施加约束 $w^{\\top} f = 1$：\n$$\n\\left( -\\frac{\\lambda}{2} f \\right)^{\\top} f = 1 \\quad \\Rightarrow \\quad -\\frac{\\lambda}{2} f^{\\top} f = 1 \\quad \\Rightarrow \\quad \\lambda = -\\frac{2}{f^{\\top} f}.\n$$\n因此，OLS权重向量为\n$$\nw_{\\mathrm{OLS}} = \\frac{f}{f^{\\top} f}.\n$$\n当 $f = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$ 时，我们有 $f^{\\top} f = 1^{2} + 2^{2} = 5$，因此\n$$\nw_{\\mathrm{OLS}} = \\begin{pmatrix}1/5 \\\\ 2/5\\end{pmatrix}.\n$$\n当真实噪声均值为 $m = \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix} \\neq 0$ 时，偏差为\n$$\nb = \\mathbb{E}[\\hat{s}] - s = w_{\\mathrm{OLS}}^{\\top} m = \\begin{pmatrix}\\frac{1}{5}  \\frac{2}{5}\\end{pmatrix} \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix} = \\frac{1}{5} \\cdot \\frac{1}{2} + \\frac{2}{5} \\cdot 0 = \\frac{1}{10}.\n$$\n\n第2部分：异方差协方差下的 GLS 权重和方差膨胀。\n\n当真实协方差为 $\\Sigma$ 且解码器通过 $w^{\\top} f = 1$ 来强制无偏性时，方差最小的线性无偏估计量是通过在约束 $w^{\\top} f = 1$ 下最小化二次型 $w^{\\top} \\Sigma w$ 得到的。使用拉格朗日乘数法，\n$$\n\\mathcal{L}(w, \\lambda) = w^{\\top} \\Sigma w + \\lambda (w^{\\top} f - 1).\n$$\n平稳性条件给出\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = 2 \\Sigma w + \\lambda f = 0 \\quad \\Rightarrow \\quad \\Sigma w = -\\frac{\\lambda}{2} f \\quad \\Rightarrow \\quad w = -\\frac{\\lambda}{2} \\Sigma^{-1} f.\n$$\n施加约束 $w^{\\top} f = 1$ 得到\n$$\n\\left( -\\frac{\\lambda}{2} \\Sigma^{-1} f \\right)^{\\top} f = 1 \\quad \\Rightarrow \\quad -\\frac{\\lambda}{2} f^{\\top} \\Sigma^{-1} f = 1 \\quad \\Rightarrow \\quad \\lambda = -\\frac{2}{f^{\\top} \\Sigma^{-1} f}.\n$$\n因此，广义最小二乘法 (GLS) 权重为\n$$\nw_{\\mathrm{GLS}} = \\frac{\\Sigma^{-1} f}{f^{\\top} \\Sigma^{-1} f}.\n$$\n相应的最小方差为\n$$\n\\operatorname{Var}_{\\mathrm{GLS}}(\\hat{s}) = w_{\\mathrm{GLS}}^{\\top} \\Sigma w_{\\mathrm{GLS}} = \\frac{1}{f^{\\top} \\Sigma^{-1} f}.\n$$\n对于给定的 $\\Sigma = \\begin{pmatrix}1  0 \\\\ 0  9\\end{pmatrix}$，我们有 $\\Sigma^{-1} = \\begin{pmatrix}1  0 \\\\ 0  \\frac{1}{9}\\end{pmatrix}$。计算\n$$\nf^{\\top} \\Sigma^{-1} f = \\begin{pmatrix}1  2\\end{pmatrix} \\begin{pmatrix}1  0 \\\\ 0  \\frac{1}{9}\\end{pmatrix} \\begin{pmatrix}1 \\\\ 2\\end{pmatrix} = 1 \\cdot 1 \\cdot 1 + 2 \\cdot \\frac{1}{9} \\cdot 2 = 1 + \\frac{4}{9} = \\frac{13}{9}.\n$$\n因此\n$$\nw_{\\mathrm{GLS}} = \\frac{\\Sigma^{-1} f}{f^{\\top} \\Sigma^{-1} f} = \\frac{1}{\\frac{13}{9}} \\begin{pmatrix}1 \\\\ \\frac{2}{9}\\end{pmatrix} = \\frac{9}{13} \\begin{pmatrix}1 \\\\ \\frac{2}{9}\\end{pmatrix} = \\begin{pmatrix}\\frac{9}{13} \\\\ \\frac{2}{13}\\end{pmatrix},\n$$\n且\n$$\n\\operatorname{Var}_{\\mathrm{GLS}}(\\hat{s}) = \\frac{1}{f^{\\top} \\Sigma^{-1} f} = \\frac{1}{\\frac{13}{9}} = \\frac{9}{13}.\n$$\n\n对于应用于真实异方差 $\\Sigma$ 的 OLS 权重，其方差为\n$$\n\\operatorname{Var}_{\\mathrm{OLS}}(\\hat{s}) = w_{\\mathrm{OLS}}^{\\top} \\Sigma w_{\\mathrm{OLS}} = \\begin{pmatrix}\\frac{1}{5}  \\frac{2}{5}\\end{pmatrix} \\begin{pmatrix}1  0 \\\\ 0  9\\end{pmatrix} \\begin{pmatrix}\\frac{1}{5} \\\\ \\frac{2}{5}\\end{pmatrix} = \\left( \\frac{1}{5} \\right)^{2} \\cdot 1 + \\left( \\frac{2}{5} \\right)^{2} \\cdot 9 = \\frac{1}{25} + \\frac{36}{25} = \\frac{37}{25}.\n$$\n因此，方差膨胀比为\n$$\n\\rho = \\frac{\\operatorname{Var}_{\\mathrm{OLS}}(\\hat{s})}{\\operatorname{Var}_{\\mathrm{GLS}}(\\hat{s})} = \\frac{\\frac{37}{25}}{\\frac{9}{13}} = \\frac{37}{25} \\cdot \\frac{13}{9} = \\frac{481}{225}.\n$$\n\n总而言之，对于指定的反例，当使用 OLS 权重时，因违背零均值噪声假设而导致的偏差为 $b = \\frac{1}{10}$，而在异方差性下相对于 GLS 的方差膨胀比为 $\\rho = \\frac{481}{225}$。将这两个量按此顺序以单行矩阵的形式报告。",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{10}  \\frac{481}{225}\\end{pmatrix}}$$"
        },
        {
            "introduction": "理解了选择正确估计器的重要性后，我们进一步探究神经元群本身的特性如何影响解码性能。本练习聚焦于两个关键因素：神经元调谐曲线的相似性（由调谐向量之间的夹角表示）和神经噪声的相关性。通过推导估计方差，我们将定量地理解这些编码属性如何决定从神经元到解码器的信息流的效率。",
            "id": "4006208",
            "problem": "考虑在一个线性高斯模型下，从 $N=2$ 个神经响应中解码一个 $d=2$ 维的潜变量 $\\mathbf{x}\\in\\mathbb{R}^{2}$。在该模型中，响应满足 $\\mathbf{r}=H\\mathbf{x}+\\boldsymbol{\\epsilon}$，其中 $\\boldsymbol{\\epsilon}$ 是零均值高斯噪声，其协方差为 $\\Sigma_{\\epsilon}\\in\\mathbb{R}^{2\\times 2}$。调谐矩阵 $H\\in\\mathbb{R}^{2\\times 2}$ 的列向量为 $\\mathbf{h}_{1}$ 和 $\\mathbf{h}_{2}$，它们编码了每个潜分量如何对响应做出贡献。假设以下符合科学现实的设定：\n- 调谐向量为单位长度，它们之间的夹角为 $\\theta\\in(0,\\pi)$（以弧度为单位），其中 $\\mathbf{h}_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$ 且 $\\mathbf{h}_{2}=\\begin{pmatrix}\\cos\\theta\\\\\\sin\\theta\\end{pmatrix}$。\n- 噪声协方差为 $\\Sigma_{\\epsilon}=\\sigma^{2}\\begin{pmatrix}1  \\rho \\\\ \\rho  1\\end{pmatrix}$，其中 $\\sigma^{2}>0$ 且 $|\\rho|1$。\n\n从最优线性估计器（OLE）是通过最小化高斯噪声下的加权最小二乘目标函数得到的这一原理出发，推导估计器协方差，用调谐矩阵 $H$ 和噪声协方差 $\\Sigma_{\\epsilon}$ 表示。使用此结果分析调谐向量的对齐程度（即 $\\theta$ 很小）如何影响信息矩阵 $H^{\\top}\\Sigma_{\\epsilon}^{-1}H$ 的条件数。然后，计算总估计方差，其定义为 $\\operatorname{tr}\\!\\left(\\operatorname{Cov}(\\hat{\\mathbf{x}})\\right)$，并将其表示为 $\\theta$、$\\sigma^{2}$ 和 $\\rho$ 的显式函数。\n\n将你的最终答案报告为关于 $\\theta$（以弧度为单位）、$\\sigma^{2}$ 和 $\\rho$ 的 $\\operatorname{tr}\\!\\left(\\operatorname{Cov}(\\hat{\\mathbf{x}})\\right)$ 的单个简化解析表达式。无需四舍五入。",
            "solution": "用户提供的问题是有效的，因为它具有科学依据、问题设定良好且客观。这是一个计算神经科学领域中关于最优线性估计的标准问题。所有必要的信息都已提供，并且参数的约束在物理上和数学上都是合理的。\n\n问题要求在线性高斯模型下，从神经响应 $\\mathbf{r} \\in \\mathbb{R}^{2}$ 推导潜变量 $\\mathbf{x} \\in \\mathbb{R}^{2}$ 的总估计方差：\n$$\n\\mathbf{r} = H\\mathbf{x} + \\boldsymbol{\\epsilon}\n$$\n其中 $\\boldsymbol{\\epsilon}$ 是一个零均值高斯噪声向量，其协方差矩阵为 $\\Sigma_{\\epsilon}$，即 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_{\\epsilon})$。\n\n最优线性估计器（OLE），也称为最佳线性无偏估计器（BLUE）或 Gauss-Markov 估计器，是通过最小化加权最小二乘目标函数得到的，对于高斯噪声，这等同于最大化对数似然。目标函数 $J(\\mathbf{x})$ 由下式给出：\n$$\nJ(\\mathbf{x}) = (\\mathbf{r} - H\\mathbf{x})^{\\top} \\Sigma_{\\epsilon}^{-1} (\\mathbf{r} - H\\mathbf{x})\n$$\n为了找到最小化此目标的估计器 $\\hat{\\mathbf{x}}$，我们计算 $J(\\mathbf{x})$ 关于 $\\mathbf{x}$ 的梯度并将其设为零：\n$$\n\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = -2 H^{\\top} \\Sigma_{\\epsilon}^{-1} \\mathbf{r} + 2 H^{\\top} \\Sigma_{\\epsilon}^{-1} H \\mathbf{x} = \\mathbf{0}\n$$\n对 $\\mathbf{x}$ 求解，得到最优线性估计器 $\\hat{\\mathbf{x}}$：\n$$\n\\hat{\\mathbf{x}} = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1} \\mathbf{r}\n$$\n该估计器的协方差 $\\operatorname{Cov}(\\hat{\\mathbf{x}})$ 通过分析估计误差 $\\hat{\\mathbf{x}} - \\mathbf{x}$ 来找到。代入 $\\mathbf{r} = H\\mathbf{x} + \\boldsymbol{\\epsilon}$：\n$$\n\\hat{\\mathbf{x}} - \\mathbf{x} = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1} (H\\mathbf{x} + \\boldsymbol{\\epsilon}) - \\mathbf{x}\n$$\n$$\n\\hat{\\mathbf{x}} - \\mathbf{x} = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)\\mathbf{x} + (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1} \\boldsymbol{\\epsilon} - \\mathbf{x}\n$$\n$$\n\\hat{\\mathbf{x}} - \\mathbf{x} = \\mathbf{x} + (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1} \\boldsymbol{\\epsilon} - \\mathbf{x} = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1} \\boldsymbol{\\epsilon}\n$$\n协方差为 $\\operatorname{Cov}(\\hat{\\mathbf{x}}) = \\mathbb{E}[(\\hat{\\mathbf{x}} - \\mathbf{x})(\\hat{\\mathbf{x}} - \\mathbf{x})^{\\top}]$。由于 $\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0}$，该估计器是无偏的，即 $\\mathbb{E}[\\hat{\\mathbf{x}}] = \\mathbf{x}$。\n令 $A = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1}$。协方差为：\n$$\n\\operatorname{Cov}(\\hat{\\mathbf{x}}) = \\mathbb{E}[A \\boldsymbol{\\epsilon} (A \\boldsymbol{\\epsilon})^{\\top}] = A \\mathbb{E}[\\boldsymbol{\\epsilon} \\boldsymbol{\\epsilon}^{\\top}] A^{\\top} = A \\Sigma_{\\epsilon} A^{\\top}\n$$\n代入 $A$ 的表达式：\n$$\n\\operatorname{Cov}(\\hat{\\mathbf{x}}) = [(H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1}] \\Sigma_{\\epsilon} [\\Sigma_{\\epsilon}^{-1} H (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1}]\n$$\n这可以简化为：\n$$\n\\operatorname{Cov}(\\hat{\\mathbf{x}}) = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1}\n$$\n矩阵 $Q = H^{\\top} \\Sigma_{\\epsilon}^{-1} H$ 是此估计问题的 Fisher 信息矩阵。估计器协方差是 Fisher 信息的逆。\n\n接下来，我们使用给定的规格计算矩阵 $Q$。调谐矩阵 $H$ 由列向量 $\\mathbf{h}_{1}$ 和 $\\mathbf{h}_{2}$ 构成：\n$$\nH = \\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix}\n$$\n噪声协方差矩阵 $\\Sigma_{\\epsilon}$ 为：\n$$\n\\Sigma_{\\epsilon} = \\sigma^{2}\\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}\n$$\n其逆矩阵 $\\Sigma_{\\epsilon}^{-1}$ 为：\n$$\n\\Sigma_{\\epsilon}^{-1} = \\frac{1}{\\det(\\Sigma_{\\epsilon})} \\sigma^{2}\\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} = \\frac{1}{\\sigma^{4}(1-\\rho^{2})} \\sigma^{2}\\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}\n$$\n现在，我们计算 $Q = H^{\\top}\\Sigma_{\\epsilon}^{-1}H$：\n$$\nQ = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  0 \\\\ \\cos\\theta  \\sin\\theta \\end{pmatrix} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} \\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix}\n$$\n首先，我们计算乘积 $\\Sigma_{\\epsilon}^{-1}H$：\n$$\n\\Sigma_{\\epsilon}^{-1}H = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} \\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix} = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  \\cos\\theta - \\rho\\sin\\theta \\\\ -\\rho  -\\rho\\cos\\theta + \\sin\\theta \\end{pmatrix}\n$$\n然后，我们左乘 $H^{\\top}$：\n$$\nQ = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  0 \\\\ \\cos\\theta  \\sin\\theta \\end{pmatrix} \\begin{pmatrix} 1  \\cos\\theta - \\rho\\sin\\theta \\\\ -\\rho  \\sin\\theta - \\rho\\cos\\theta \\end{pmatrix}\n$$\n$$\nQ = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  \\cos\\theta - \\rho\\sin\\theta \\\\ \\cos\\theta - \\rho\\sin\\theta  \\cos\\theta(\\cos\\theta - \\rho\\sin\\theta) + \\sin\\theta(\\sin\\theta - \\rho\\cos\\theta) \\end{pmatrix}\n$$\n元素 $Q_{22}$ 中的表达式简化为：\n$$\nQ_{22, \\text{inner}} = \\cos^{2}\\theta - \\rho\\sin\\theta\\cos\\theta + \\sin^{2}\\theta - \\rho\\sin\\theta\\cos\\theta = 1 - 2\\rho\\sin\\theta\\cos\\theta\n$$\n因此，Fisher 信息矩阵为：\n$$\nQ = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  \\cos\\theta - \\rho\\sin\\theta \\\\ \\cos\\theta - \\rho\\sin\\theta  1 - 2\\rho\\sin\\theta\\cos\\theta \\end{pmatrix}\n$$\n问题要求分析当 $\\theta$ 很小时 $Q$ 的条件数。矩阵的条件数与其行列式有关。当矩阵的行列式趋于零时，该矩阵变为病态的（ill-conditioned）。\n$$\n\\det(Q) = \\det(H^{\\top}\\Sigma_{\\epsilon}^{-1}H) = (\\det(H))^{2} \\det(\\Sigma_{\\epsilon}^{-1}) = \\frac{(\\det(H))^{2}}{\\det(\\Sigma_{\\epsilon})}\n$$\n由于 $\\det(H) = \\sin\\theta$ 且 $\\det(\\Sigma_{\\epsilon}) = \\sigma^{4}(1-\\rho^{2})$，我们有：\n$$\n\\det(Q) = \\frac{\\sin^{2}\\theta}{\\sigma^{4}(1-\\rho^{2})}\n$$\n当 $\\theta \\to 0$ 时，$\\sin\\theta \\to 0$，因此 $\\det(Q) \\to 0$。这表明 $Q$ 变得奇异，因此是病态的。发生这种情况是因为当 $\\theta$ 很小时，调谐向量 $\\mathbf{h}_1$ 和 $\\mathbf{h}_2$ 变得近乎共线，提供了相似的信息，从而难以区分 $x_1$ 和 $x_2$ 的贡献。\n\n最后，我们计算总估计方差，定义为 $\\operatorname{tr}(\\operatorname{Cov}(\\hat{\\mathbf{x}})) = \\operatorname{tr}(Q^{-1})$。对于一个通用的 $2\\times2$ 矩阵 $A$，有 $\\operatorname{tr}(A^{-1}) = \\frac{\\operatorname{tr}(A)}{\\det(A)}$。我们将此公式应用于 $Q$。\n$Q$ 的迹为：\n$$\n\\operatorname{tr}(Q) = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} (1 + 1 - 2\\rho\\sin\\theta\\cos\\theta) = \\frac{2(1 - \\rho\\sin\\theta\\cos\\theta)}{\\sigma^{2}(1-\\rho^{2})}\n$$\n现在我们计算 $\\operatorname{tr}(Q^{-1})$：\n$$\n\\operatorname{tr}(Q^{-1}) = \\frac{\\operatorname{tr}(Q)}{\\det(Q)} = \\frac{\\frac{2(1 - \\rho\\sin\\theta\\cos\\theta)}{\\sigma^{2}(1-\\rho^{2})}}{\\frac{\\sin^{2}\\theta}{\\sigma^{4}(1-\\rho^{2})}}\n$$\n$$\n\\operatorname{tr}(\\operatorname{Cov}(\\hat{\\mathbf{x}})) = \\frac{2(1 - \\rho\\sin\\theta\\cos\\theta)}{\\sigma^{2}(1-\\rho^{2})} \\cdot \\frac{\\sigma^{4}(1-\\rho^{2})}{\\sin^{2}\\theta}\n$$\n简化该表达式可得到总估计方差：\n$$\n\\operatorname{tr}(\\operatorname{Cov}(\\hat{\\mathbf{x}})) = \\frac{2\\sigma^{2}(1 - \\rho\\sin\\theta\\cos\\theta)}{\\sin^{2}\\theta}\n$$\n此表达式给出了总方差，其是调谐角 $\\theta$、噪声方差 $\\sigma^{2}$ 和噪声相关性 $\\rho$ 的函数。",
            "answer": "$$\n\\boxed{\\frac{2\\sigma^{2}(1 - \\rho\\sin\\theta\\cos\\theta)}{\\sin^{2}\\theta}}\n$$"
        },
        {
            "introduction": "本章的最后一个练习将探讨神经科学中的一个经典问题：神经冗余是好是坏？直觉上，增加调谐特性完全相同的神经元似乎是一种资源浪费。本练习将挑战这一直觉，通过分析增加冗余神经元对信噪比（SNR）和互信息（MI）的影响，并结合推导与编程实践，揭示噪声相关性在决定冗余是有益还是有害时所扮演的关键且出人意料的角色。",
            "id": "4006217",
            "problem": "考虑一个标量刺激的线性高斯神经编码和解码场景。设刺激为 $x \\sim \\mathcal{N}(0,\\sigma_x^2)$，群体响应为 $\\mathbf{r} \\in \\mathbb{R}^N$，由 $\\mathbf{r} = \\mathbf{f}\\,x + \\mathbf{n}$ 给出，其中 $\\mathbf{f} \\in \\mathbb{R}^N$ 是群体调谐向量，$\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0},\\boldsymbol{\\Sigma}_n)$ 是独立于 $x$ 的零均值加性噪声。冗余调谐意味着每个神经元具有相同的调谐，因此 $\\mathbf{f} = g\\,\\mathbf{1}_N$，其中 $g \\in \\mathbb{R}$，$\\mathbf{1}_N$ 表示 $N$ 维全一向量。神经元间的噪声是等相关的：$\\boldsymbol{\\Sigma}_n = \\sigma_n^2\\left((1-\\rho)\\,\\mathbf{I}_N + \\rho\\,\\mathbf{1}_N\\mathbf{1}_N^\\top\\right)$，其中 $\\sigma_n^2 > 0$，$\\rho \\in \\mathbb{R}$ 满足 $-1/(N-1)  \\rho  1$ 以确保半正定性，$\\mathbf{I}_N$ 是 $N \\times N$ 单位矩阵。\n\n线性解码器产生一个估计值 $\\hat{x} = \\mathbf{w}^\\top \\mathbf{r}$，其中权重向量为 $\\mathbf{w} \\in \\mathbb{R}^N$。目标是分析在最优线性解码下，增加具有冗余调谐的神经元如何影响信噪比 (SNR) 和互信息 (MI)。估计值的信噪比 (SNR) 定义为在上述模型下计算的 $\\hat{x}$ 中信号贡献的方差与噪声贡献的方差之比。$x$ 和 $\\mathbf{r}$ 之间的互信息 (MI) 定义为 $I(x;\\mathbf{r}) = H(x) - H(x|\\mathbf{r})$，对于联合高斯变量，必须以奈特 (nats) 表示；使用自然对数。\n\n您必须从高斯模型中线性估计的核心定义、多元高斯分布的性质以及信噪比 (SNR) 和互信息 (MI) 的定义出发。推导最大化 SNR 的最优线性解码器，并计算由此产生的 SNR 和 MI。然后，在两种资源机制下比较增加神经元的效果：\n- 固定单神经元增益：$g$ 不随 $N$ 变化。\n- 固定总增益预算：总平方增益 $\\|\\mathbf{f}\\|_2^2$ 保持不变，因此 $g$ 按 $g = g_0/\\sqrt{N}$ 进行缩放，其中 $g_0$ 为某个常数。\n\n使用这些推导，实现一个程序来为每个测试用例计算：\n- 最优线性解码器的 SNR。\n- 使用自然对数的 MI（以奈特为单位）。\n- 一个布尔值，指示在相同参数下增加一个冗余神经元（从 $N$ 到 $N+1$）是否会增加 MI，从而将冗余对该增量添加的线性解码分类为有益（布尔值为真）或有害（布尔值为假）。\n\n假设所有测试用例中的常数如下：$\\sigma_x^2 = 1.0$，$\\sigma_n^2 = 1.0$，以及 $g_0 = 1.0$。当机制为“固定单神经元增益”时，使用 $g = g_0$；当机制为“固定总增益预算”时，使用 $g = g_0/\\sqrt{N}$。\n\n测试套件：\n- 用例 1：$N = 1$，$\\rho = 0.0$，机制 = 固定单神经元增益。\n- 用例 2：$N = 10$，$\\rho = 0.0$，机制 = 固定单神经元增益。\n- 用例 3：$N = 10$，$\\rho = 0.9$，机制 = 固定单神经元增益。\n- 用例 4：$N = 10$，$\\rho = 0.9$，机制 = 固定总增益预算。\n- 用例 5：$N = 10$，$\\rho = -0.09$，机制 = 固定单神经元增益。\n- 用例 6：$N = 50$，$\\rho = 0.99$，机制 = 固定单神经元增益。\n- 用例 7：$N = 50$，$\\rho = -0.019$，机制 = 固定总增益预算。\n\n您的程序应生成单行输出，其中包含结果，格式为逗号分隔的内部列表，每个内部列表对应上述顺序的一个测试用例，并包含三个元素：$[\\text{SNR}, \\text{MI}, \\text{helps}]$。例如，输出格式必须类似于 $[[s_1,i_1,h_1],[s_2,i_2,h_2],\\dots]$，其中每个 $s_k$ 和 $i_k$ 是浮点数，每个 $h_k$ 是布尔值。MI 必须以奈特 (nats) 表示。除了声明 MI 以奈特为单位外，SNR 或 MI 均不关联任何单位。此问题中不出现角度。此问题中不出现百分比。打印的单行必须与此格式完全匹配，不得有额外的空格或文本。",
            "solution": "该问题已经过验证，被认为是合理的。这是一个计算神经科学中定义明确的问题，基于线性高斯模型、信息论和最优估计的既定原理。所有给定条件都是自洽、一致且科学上严谨的。\n\n解答过程分为三个主要部分。首先，我们推导了使刺激估计的信噪比 (SNR) 最大化的最优线性解码器权重 $\\mathbf{w}$。其次，我们推导了刺激与神经响应之间的互信息 (MI) 公式，并展示了其与最大 SNR 的直接关系。第三，我们分析了在两种指定的资源机制下，神经元数量 $N$ 如何影响 SNR 和 MI。\n\n刺激是一个随机变量 $x \\sim \\mathcal{N}(0, \\sigma_x^2)$，而 $N$ 维神经群体响应由线性高斯模型 $\\mathbf{r} = \\mathbf{f}x + \\mathbf{n}$ 给出。噪声 $\\mathbf{n}$ 来自 $\\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_n)$ 分布，且独立于 $x$。线性解码器将刺激估计为 $\\hat{x} = \\mathbf{w}^\\top \\mathbf{r}$。\n\n代入 $\\mathbf{r}$ 的模型，估计值为 $\\hat{x} = \\mathbf{w}^\\top(\\mathbf{f}x + \\mathbf{n}) = (\\mathbf{w}^\\top\\mathbf{f})x + \\mathbf{w}^\\top\\mathbf{n}$。此表达式将估计值分为信号分量 $(\\mathbf{w}^\\top\\mathbf{f})x$ 和噪声分量 $\\mathbf{w}^\\top\\mathbf{n}$。\n信号分量的方差为 $\\text{Var}((\\mathbf{w}^\\top\\mathbf{f})x) = (\\mathbf{w}^\\top\\mathbf{f})^2 \\text{Var}(x) = (\\mathbf{w}^\\top\\mathbf{f})^2 \\sigma_x^2$。\n噪声分量的方差为 $\\text{Var}(\\mathbf{w}^\\top\\mathbf{n}) = \\mathbf{w}^\\top \\text{Cov}(\\mathbf{n}) \\mathbf{w} = \\mathbf{w}^\\top \\boldsymbol{\\Sigma}_n \\mathbf{w}$。\n\n估计的信噪比 (SNR) 定义为这两个方差之比：\n$$ \\text{SNR}(\\mathbf{w}) = \\frac{(\\mathbf{w}^\\top\\mathbf{f})^2 \\sigma_x^2}{\\mathbf{w}^\\top \\boldsymbol{\\Sigma}_n \\mathbf{w}} $$\n这是一个广义瑞利商。相对于 $\\mathbf{w}$ 最大化此量是一个标准的优化问题。当 $\\mathbf{w}$ 与 $\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f}$ 成比例时达到最大值。我们选择 $\\mathbf{w}_{opt} = \\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f}$（任意的缩放常数会抵消掉）。将此最优权重向量代入 SNR 表达式，可得到任何线性解码器的最大可能 SNR：\n$$ \\text{SNR}_{opt} = \\frac{((\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f})^\\top\\mathbf{f})^2 \\sigma_x^2}{(\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f})^\\top \\boldsymbol{\\Sigma}_n (\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f})} = \\frac{(\\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f})^2 \\sigma_x^2}{\\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f}} = \\sigma_x^2 (\\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f}) $$\n这个量 $\\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f}$ 是群体响应关于刺激 $x$ 的费雪信息。\n\n为了计算这个表达式，我们必须计算 $\\boldsymbol{\\Sigma}_n^{-1}$。噪声协方差给出为 $\\boldsymbol{\\Sigma}_n = \\sigma_n^2\\left((1-\\rho)\\,\\mathbf{I}_N + \\rho\\,\\mathbf{1}_N\\mathbf{1}_N^\\top\\right)$。我们可以使用 Sherman-Morrison-Woodbury 公式来求此矩阵的逆，该公式表述为 $(\\mathbf{A}+\\mathbf{UV}^\\top)^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{I}+\\mathbf{V}^\\top\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^\\top\\mathbf{A}^{-1}$。\n设 $\\mathbf{A} = (1-\\rho)\\mathbf{I}_N$, $\\mathbf{U} = \\rho\\mathbf{1}_N$, 且 $\\mathbf{V} = \\mathbf{1}_N$。其逆为：\n$$ \\boldsymbol{\\Sigma}_n^{-1} = \\frac{1}{\\sigma_n^2} \\left[ \\frac{1}{1-\\rho}\\mathbf{I}_N - \\frac{\\rho}{(1-\\rho)(1+(N-1)\\rho)} \\mathbf{1}_N \\mathbf{1}_N^\\top \\right] $$\n问题指定了冗余调谐 $\\mathbf{f} = g\\mathbf{1}_N$。我们现在计算二次型 $\\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f}$：\n$$ \\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f} = g^2 \\mathbf{1}_N^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{1}_N = \\frac{g^2}{\\sigma_n^2} \\mathbf{1}_N^\\top \\left[ \\frac{1}{1-\\rho}\\mathbf{I}_N - \\frac{\\rho}{(1-\\rho)(1+(N-1)\\rho)} \\mathbf{1}_N \\mathbf{1}_N^\\top \\right] \\mathbf{1}_N $$\n使用 $\\mathbf{1}_N^\\top \\mathbf{I}_N \\mathbf{1}_N = N$ 和 $\\mathbf{1}_N^\\top (\\mathbf{1}_N \\mathbf{1}_N^\\top) \\mathbf{1}_N = (\\mathbf{1}_N^\\top \\mathbf{1}_N)^2 = N^2$：\n$$ \\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f} = \\frac{g^2}{\\sigma_n^2} \\left[ \\frac{N}{1-\\rho} - \\frac{\\rho N^2}{(1-\\rho)(1+(N-1)\\rho)} \\right] = \\frac{g^2 N}{\\sigma_n^2(1-\\rho)} \\left[ 1 - \\frac{N\\rho}{1+(N-1)\\rho} \\right] $$\n$$ = \\frac{g^2 N}{\\sigma_n^2(1-\\rho)} \\left[ \\frac{1+(N-1)\\rho - N\\rho}{1+(N-1)\\rho} \\right] = \\frac{g^2 N}{\\sigma_n^2(1-\\rho)} \\left[ \\frac{1-\\rho}{1+(N-1)\\rho} \\right] = \\frac{g^2 N}{\\sigma_n^2(1+(N-1)\\rho)} $$\n因此，最优 SNR 为：\n$$ \\text{SNR}_{opt} = \\sigma_x^2 \\frac{g^2 N}{\\sigma_n^2(1+(N-1)\\rho)} $$\n我们现在将此公式应用于两种机制，使用给定的常数 $\\sigma_x^2=1.0$，$\\sigma_n^2=1.0$ 和 $g_0=1.0$。\n1.  **固定单神经元增益**：$g=g_0=1.0$。\n    $$ \\text{SNR}_{opt}(N) = \\frac{N}{1+(N-1)\\rho} $$\n2.  **固定总增益预算**：$g = g_0/\\sqrt{N} = 1.0/\\sqrt{N}$。\n    $$ \\text{SNR}_{opt}(N) = \\left(\\frac{1}{\\sqrt{N}}\\right)^2 \\frac{N}{1+(N-1)\\rho} = \\frac{1}{1+(N-1)\\rho} $$\n\n接下来，我们推导互信息 $I(x;\\mathbf{r})$。对于联合高斯系统，$I(x;\\mathbf{r}) = \\frac{1}{2} \\ln \\det(\\mathbf{I} + \\text{Cov}(x)^{-1} \\text{Cov}(x, \\mathbf{r}) \\text{Cov}(\\mathbf{r}|x)^{-1} \\text{Cov}(\\mathbf{r}, x))$。一种更简单的方法是使用协方差矩阵行列式之间的关系：$I(x;\\mathbf{r}) = \\frac{1}{2}\\ln\\left( \\frac{\\det(\\text{Cov}(x))\\det(\\text{Cov}(\\mathbf{r}))}{\\det(\\text{Cov}(x,\\mathbf{r}))} \\right)$，或者最直接地从 MI 和后验方差之间的关系得出。在线性高斯模型中，$I(x;\\mathbf{r}) = \\frac{1}{2}\\ln\\left(1 + \\sigma_x^2 \\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f}\\right)$。识别对数内的项，我们发现 MI 与最优 SNR 之间存在直接关系：\n$$ I(x;\\mathbf{r}) = \\frac{1}{2}\\ln(1 + \\text{SNR}_{opt}) $$\n按照要求，MI 以奈特 (nats) 为单位测量。\n\n最后，我们确定增加一个神经元（从 $N$ 到 $N+1$）是否有助于解码，如果 $I(N+1) > I(N)$，则为真。由于 $\\ln(1+z)$ 对于 $z > -1$ 是一个严格递增的函数，这个条件等价于 $\\text{SNR}_{opt}(N+1) > \\text{SNR}_{opt}(N)$。\n\n1.  **固定单神经元增益**：我们检验是否 $\\frac{N+1}{1+N\\rho} > \\frac{N}{1+(N-1)\\rho}$。这个不等式化简为 $(N+1)(1+(N-1)\\rho) > N(1+N\\rho)$，进一步化简为 $1-\\rho > 0$，即 $\\rho  1$。由于问题指定了 $\\rho  1$ 以使 $\\boldsymbol{\\Sigma}_n$ 为正定，因此在此机制下增加神经元总是会增加 MI。因此，冗余总是**有益的**。\n\n2.  **固定总增益预算**：我们检验是否 $\\frac{1}{1+N\\rho} > \\frac{1}{1+(N-1)\\rho}$。这当且仅当 $1+(N-1)\\rho > 1+N\\rho$ 时成立，化简为 $-\\rho > 0$，即 $\\rho  0$。在此机制下，仅当噪声相关性为负 ($\\rho  0$) 时，增加神经元才**有益**，而当噪声相关性为正 ($\\rho > 0$) 时则**有害**。如果 $\\rho=0$，则没有变化。\n\n这些推导出的公式和条件被实现用来计算每个测试用例的结果。",
            "answer": "[[1.0,0.34657359027997264,true],[10.0,1.198673516525732,true],[1.0989010989010988,0.3707303356788322,true],[0.10989010989010989,0.05210340572621111,false],[52.63157894736842,1.992433446950285,true],[1.00989701029897,0.3492025349918231,true],[14.492753623188406,1.3705096536551144,true]]"
        }
    ]
}