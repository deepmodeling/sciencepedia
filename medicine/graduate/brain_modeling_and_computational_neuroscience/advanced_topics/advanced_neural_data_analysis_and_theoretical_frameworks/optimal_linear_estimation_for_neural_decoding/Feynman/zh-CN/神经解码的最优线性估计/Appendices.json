{
    "hands_on_practices": [
        {
            "introduction": "最佳线性无偏估计器（BLUE）虽然能提供最精确的估计，但其最优性依赖于关于噪声结构的关键假设。本练习提供了一个动手实践的机会，旨在探索当这些假设被违背时会发生什么。通过一个具体的反例 ，你将量化非零均值噪声所引入的偏差，以及因忽略异方差性而导致的精度损失（方差膨胀），从而突显在最优解码中使用正确噪声模型（如广义最小二乘法）的重要性。",
            "id": "4006252",
            "problem": "考虑大脑建模和计算神经科学中的一个线性神经群体解码场景。一个标量刺激 $s \\in \\mathbb{R}$ 驱动一个具有调谐向量 $f \\in \\mathbb{R}^{2}$ 的双神经元群体，在单次试验中观察到的放电率向量 $r \\in \\mathbb{R}^{2}$ 被建模为\n$$\nr = f\\,s + \\varepsilon,\n$$\n其中 $\\varepsilon \\in \\mathbb{R}^{2}$ 是随机噪声。线性解码器寻求一个估计 $\\hat{s} = w^{\\top} r$，其中权重向量为 $w \\in \\mathbb{R}^{2}$。\n\n在经典的高斯-马尔可夫设定中，最佳线性无偏估计量 (BLUE) 的定义基于噪声是零均值和同方差的假设，其协方差与单位矩阵成正比。在神经解码中，这些假设经常被违反。您的目标是提供一个具体的反例，说明违反零均值或同方差性假设如何分别通过引入偏差或增大方差来破坏 BLUE 的最优性，并量化这两种效应。\n\n使用以下具体的、科学上合理的参数：\n- 调谐向量 $f = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$。\n- 噪声均值 $m = \\mathbb{E}[\\varepsilon] = \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix}$。\n- 噪声协方差 $\\Sigma = \\operatorname{Cov}[\\varepsilon] = \\begin{pmatrix}1  0 \\\\ 0  9\\end{pmatrix}$，这在神经元之间是异方差的。\n\n假设解码器是根据经典的普通最小二乘 (OLS) 假设构建的——即，它忽略了异方差性和零均值违规——并选择在噪声为零均值和同方差情况下最优的权重，即在该特殊情况下，在无偏性约束下最小化估计量方差。\n\n从无偏性（$\\mathbb{E}[\\hat{s}]=s$）、方差（$\\operatorname{Var}[\\hat{s}]$）的基本定义以及用于约束二次最小化的拉格朗日乘子法出发，完成以下任务：\n1. 在同方差零均值假设下推导 OLS 权重 $w_{\\mathrm{OLS}}$，并计算当真实噪声均值为上述给定的 $m \\neq 0$ 时产生的偏差 $b = \\mathbb{E}[\\hat{s}]-s$。\n2. 当真实协方差为上述给定的 $\\Sigma$ 时，在无偏性约束下推导最小化 $\\operatorname{Var}[\\hat{s}]$ 的广义最小二乘 (GLS) 权重 $w_{\\mathrm{GLS}}$，并计算估计量方差之比 $\\rho = \\frac{\\operatorname{Var}_{\\mathrm{OLS}}(\\hat{s})}{\\operatorname{Var}_{\\mathrm{GLS}}(\\hat{s})}$。\n\n请用精确的有理数表示您的最终答案，并将这两个量 $(b, \\rho)$ 合并为一个行向量报告。不需要单位。请勿四舍五入。最终答案必须是包含偏差和方差膨胀比的单行矩阵，并按此顺序排列。",
            "solution": "我们从线性观测模型开始\n$$\nr = f\\,s + \\varepsilon,\n$$\n其中调谐向量为 $f \\in \\mathbb{R}^{2}$，噪声均值为 $m = \\mathbb{E}[\\varepsilon]$，协方差为 $\\Sigma = \\operatorname{Cov}[\\varepsilon]$。线性解码器使用 $\\hat{s} = w^{\\top} r$。\n\n无偏性定义为对于所有 $s$，$\\mathbb{E}[\\hat{s}] = s$。利用期望的线性性质，\n$$\n\\mathbb{E}[\\hat{s}] = \\mathbb{E}[w^{\\top} r] = w^{\\top} \\mathbb{E}[r] = w^{\\top} (f\\,s + m) = s\\, w^{\\top} f + w^{\\top} m.\n$$\n为了对所有 $s$ 都无偏，必须满足\n$$\nw^{\\top} f = 1, \\quad \\text{和} \\quad w^{\\top} m = 0.\n$$\n在实践中，通常的普通最小二乘 (OLS) 构造假设 $m=0$ 和同方差噪声，在这种情况下，无偏性约束简化为 $w^{\\top} f = 1$。估计量的方差为\n$$\n\\operatorname{Var}[\\hat{s}] = \\operatorname{Var}[w^{\\top} r] = \\operatorname{Var}[w^{\\top} \\varepsilon] = w^{\\top} \\Sigma\\, w.\n$$\n\n第 1 部分：非零均值噪声下的 OLS 权重和偏差。\n\n在同方差假设 $\\Sigma = \\sigma^{2} I$ 下，最小化 $\\operatorname{Var}[\\hat{s}] = \\sigma^{2} w^{\\top} w$（受约束于 $w^{\\top} f = 1$）等价于在相同约束下最小化 $w^{\\top} w$。使用拉格朗日乘子法，最小化\n$$\n\\mathcal{L}(w, \\lambda) = w^{\\top} w + \\lambda (w^{\\top} f - 1).\n$$\n令梯度为零，\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = 2 w + \\lambda f = 0 \\quad \\Rightarrow \\quad w = -\\frac{\\lambda}{2} f.\n$$\n施加约束 $w^{\\top} f = 1$：\n$$\n\\left( -\\frac{\\lambda}{2} f \\right)^{\\top} f = 1 \\quad \\Rightarrow \\quad -\\frac{\\lambda}{2} f^{\\top} f = 1 \\quad \\Rightarrow \\quad \\lambda = -\\frac{2}{f^{\\top} f}.\n$$\n因此 OLS 权重向量为\n$$\nw_{\\mathrm{OLS}} = \\frac{f}{f^{\\top} f}.\n$$\n当 $f = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$ 时，我们有 $f^{\\top} f = 1^{2} + 2^{2} = 5$，因此\n$$\nw_{\\mathrm{OLS}} = \\begin{pmatrix}1/5 \\\\ 2/5\\end{pmatrix}.\n$$\n当真实噪声均值为 $m = \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix} \\neq 0$ 时，偏差为\n$$\nb = \\mathbb{E}[\\hat{s}] - s = w_{\\mathrm{OLS}}^{\\top} m = \\begin{pmatrix}\\frac{1}{5}  \\frac{2}{5}\\end{pmatrix} \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix} = \\frac{1}{5} \\cdot \\frac{1}{2} + \\frac{2}{5} \\cdot 0 = \\frac{1}{10}.\n$$\n\n第 2 部分：异方差协方差下的 GLS 权重和方差膨胀。\n\n当真实协方差为 $\\Sigma$ 且解码器通过 $w^{\\top} f = 1$ 强制无偏性时，最小化方差的线性无偏估计量是通过最小化二次型 $w^{\\top} \\Sigma w$（受约束于 $w^{\\top} f = 1$）得到的。使用拉格朗日乘子法，\n$$\n\\mathcal{L}(w, \\lambda) = w^{\\top} \\Sigma w + \\lambda (w^{\\top} f - 1).\n$$\n平稳性条件给出\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = 2 \\Sigma w + \\lambda f = 0 \\quad \\Rightarrow \\quad \\Sigma w = -\\frac{\\lambda}{2} f \\quad \\Rightarrow \\quad w = -\\frac{\\lambda}{2} \\Sigma^{-1} f.\n$$\n施加约束 $w^{\\top} f = 1$ 得到\n$$\n\\left( -\\frac{\\lambda}{2} \\Sigma^{-1} f \\right)^{\\top} f = 1 \\quad \\Rightarrow \\quad -\\frac{\\lambda}{2} f^{\\top} \\Sigma^{-1} f = 1 \\quad \\Rightarrow \\quad \\lambda = -\\frac{2}{f^{\\top} \\Sigma^{-1} f}.\n$$\n因此，广义最小二乘 (GLS) 权重为\n$$\nw_{\\mathrm{GLS}} = \\frac{\\Sigma^{-1} f}{f^{\\top} \\Sigma^{-1} f}.\n$$\n相应的最小方差是\n$$\n\\operatorname{Var}_{\\mathrm{GLS}}(\\hat{s}) = w_{\\mathrm{GLS}}^{\\top} \\Sigma w_{\\mathrm{GLS}} = \\frac{1}{f^{\\top} \\Sigma^{-1} f}.\n$$\n对于给定的 $\\Sigma = \\begin{pmatrix}1  0 \\\\ 0  9\\end{pmatrix}$，我们有 $\\Sigma^{-1} = \\begin{pmatrix}1  0 \\\\ 0  \\frac{1}{9}\\end{pmatrix}$。计算\n$$\nf^{\\top} \\Sigma^{-1} f = \\begin{pmatrix}1  2\\end{pmatrix} \\begin{pmatrix}1  0 \\\\ 0  \\frac{1}{9}\\end{pmatrix} \\begin{pmatrix}1 \\\\ 2\\end{pmatrix} = 1 \\cdot 1 \\cdot 1 + 2 \\cdot \\frac{1}{9} \\cdot 2 = 1 + \\frac{4}{9} = \\frac{13}{9}.\n$$\n因此\n$$\nw_{\\mathrm{GLS}} = \\frac{\\Sigma^{-1} f}{f^{\\top} \\Sigma^{-1} f} = \\frac{1}{\\frac{13}{9}} \\begin{pmatrix}1 \\\\ \\frac{2}{9}\\end{pmatrix} = \\frac{9}{13} \\begin{pmatrix}1 \\\\ \\frac{2}{9}\\end{pmatrix} = \\begin{pmatrix}\\frac{9}{13} \\\\ \\frac{2}{13}\\end{pmatrix},\n$$\n并且\n$$\n\\operatorname{Var}_{\\mathrm{GLS}}(\\hat{s}) = \\frac{1}{f^{\\top} \\Sigma^{-1} f} = \\frac{1}{\\frac{13}{9}} = \\frac{9}{13}.\n$$\n\n对于应用于真实异方差 $\\Sigma$ 的 OLS 权重，方差为\n$$\n\\operatorname{Var}_{\\mathrm{OLS}}(\\hat{s}) = w_{\\mathrm{OLS}}^{\\top} \\Sigma w_{\\mathrm{OLS}} = \\begin{pmatrix}\\frac{1}{5}  \\frac{2}{5}\\end{pmatrix} \\begin{pmatrix}1  0 \\\\ 0  9\\end{pmatrix} \\begin{pmatrix}\\frac{1}{5} \\\\ \\frac{2}{5}\\end{pmatrix} = \\left( \\frac{1}{5} \\right)^{2} \\cdot 1 + \\left( \\frac{2}{5} \\right)^{2} \\cdot 9 = \\frac{1}{25} + \\frac{36}{25} = \\frac{37}{25}.\n$$\n因此，方差膨胀比为\n$$\n\\rho = \\frac{\\operatorname{Var}_{\\mathrm{OLS}}(\\hat{s})}{\\operatorname{Var}_{\\mathrm{GLS}}(\\hat{s})} = \\frac{\\frac{37}{25}}{\\frac{9}{13}} = \\frac{37}{25} \\cdot \\frac{13}{9} = \\frac{481}{225}.\n$$\n\n总结来说，对于指定的反例，在使用 OLS 权重时违反零均值噪声假设所导致的偏差为 $b = \\frac{1}{10}$，而异方差性下相对于 GLS 的方差膨胀为 $\\rho = \\frac{481}{225}$。按此顺序将两者报告为单个行矩阵。",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{10}  \\frac{481}{225}\\end{pmatrix}}$$"
        },
        {
            "introduction": "一旦我们掌握了最优估计器的正确形式，就可以探索神经元群体的特性如何影响解码的准确性。这个问题  深入探讨了神经元调谐特性（以调谐向量之间的夹角表示）与它们的噪声相关性之间的相互作用。你将推导这些因素如何共同决定整体的估计方差，从而直观地理解为什么多样化的调谐和较低的噪声相关性通常对群体编码有利。",
            "id": "4006208",
            "problem": "考虑在线性高斯模型下从 $N=2$ 个神经响应中解码一个 $d=2$ 维的潜变量 $\\mathbf{x}\\in\\mathbb{R}^{2}$，其中响应满足 $\\mathbf{r}=H\\mathbf{x}+\\boldsymbol{\\epsilon}$，$\\boldsymbol{\\epsilon}$ 是均值为零、协方差为 $\\Sigma_{\\epsilon}\\in\\mathbb{R}^{2\\times 2}$ 的高斯噪声。调谐矩阵 $H\\in\\mathbb{R}^{2\\times 2}$ 的列向量为 $\\mathbf{h}_{1}$ 和 $\\mathbf{h}_{2}$，它们编码了每个潜分量对响应的贡献方式。假设有以下科学上真实的设定：\n- 调谐向量是单位长度，并形成一个角度 $\\theta\\in(0,\\pi)$（以弧度为单位），其中 $\\mathbf{h}_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$ 且 $\\mathbf{h}_{2}=\\begin{pmatrix}\\cos\\theta\\\\\\sin\\theta\\end{pmatrix}$。\n- 噪声协方差为 $\\Sigma_{\\epsilon}=\\sigma^{2}\\begin{pmatrix}1  \\rho \\\\ \\rho  1\\end{pmatrix}$，其中 $\\sigma^{2}>0$ 且 $|\\rho|  1$。\n\n从最优线性估计器 (OLE) 是通过最小化高斯噪声的加权最小二乘目标函数得到的这一原理出发，推导估计器协方差，用调谐矩阵 $H$ 和噪声协方差 $\\Sigma_{\\epsilon}$ 表示。利用此结果分析调谐向量的对齐程度（小 $\\theta$）如何影响信息矩阵 $H^{\\top}\\Sigma_{\\epsilon}^{-1}H$ 的条件数。然后计算总估计方差，定义为 $\\operatorname{tr}\\!\\left(\\operatorname{Cov}(\\hat{\\mathbf{x}})\\right)$，将其表示为 $\\theta$、$\\sigma^{2}$ 和 $\\rho$ 的显式函数。\n\n将您的最终答案报告为 $\\operatorname{tr}\\!\\left(\\operatorname{Cov}(\\hat{\\mathbf{x}})\\right)$ 关于 $\\theta$（以弧度为单位）、$\\sigma^{2}$ 和 $\\rho$ 的单个简化解析表达式。无需四舍五入。",
            "solution": "该问题要求在线性高斯模型下，根据神经响应 $\\mathbf{r} \\in \\mathbb{R}^{2}$ 推导潜变量 $\\mathbf{x} \\in \\mathbb{R}^{2}$ 的总估计方差：\n$$\n\\mathbf{r} = H\\mathbf{x} + \\boldsymbol{\\epsilon}\n$$\n其中 $\\boldsymbol{\\epsilon}$ 是一个零均值高斯噪声向量，其协方差矩阵为 $\\Sigma_{\\epsilon}$，即 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_{\\epsilon})$。\n\n最优线性估计器 (OLE)，也称为最佳线性无偏估计器 (BLUE) 或 Gauss-Markov 估计器，是通过最小化加权最小二乘目标函数得到的，对于高斯噪声，这相当于最大化对数似然。目标函数 $J(\\mathbf{x})$ 由下式给出：\n$$\nJ(\\mathbf{x}) = (\\mathbf{r} - H\\mathbf{x})^{\\top} \\Sigma_{\\epsilon}^{-1} (\\mathbf{r} - H\\mathbf{x})\n$$\n为了找到最小化此目标函数的估计器 $\\hat{\\mathbf{x}}$，我们计算 $J(\\mathbf{x})$ 关于 $\\mathbf{x}$ 的梯度并将其设为零：\n$$\n\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = -2 H^{\\top} \\Sigma_{\\epsilon}^{-1} \\mathbf{r} + 2 H^{\\top} \\Sigma_{\\epsilon}^{-1} H \\mathbf{x} = \\mathbf{0}\n$$\n求解 $\\mathbf{x}$ 得到最优线性估计器 $\\hat{\\mathbf{x}}$：\n$$\n\\hat{\\mathbf{x}} = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1} \\mathbf{r}\n$$\n该估计器的协方差 $\\operatorname{Cov}(\\hat{\\mathbf{x}})$ 可通过分析估计误差 $\\hat{\\mathbf{x}} - \\mathbf{x}$ 得到。代入 $\\mathbf{r} = H\\mathbf{x} + \\boldsymbol{\\epsilon}$：\n$$\n\\hat{\\mathbf{x}} - \\mathbf{x} = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1} (H\\mathbf{x} + \\boldsymbol{\\epsilon}) - \\mathbf{x}\n$$\n$$\n\\hat{\\mathbf{x}} - \\mathbf{x} = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)\\mathbf{x} + (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1} \\boldsymbol{\\epsilon} - \\mathbf{x}\n$$\n$$\n\\hat{\\mathbf{x}} - \\mathbf{x} = \\mathbf{x} + (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1} \\boldsymbol{\\epsilon} - \\mathbf{x} = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1} \\boldsymbol{\\epsilon}\n$$\n协方差为 $\\operatorname{Cov}(\\hat{\\mathbf{x}}) = \\mathbb{E}[(\\hat{\\mathbf{x}} - \\mathbf{x})(\\hat{\\mathbf{x}} - \\mathbf{x})^{\\top}]$。由于 $\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0}$，该估计器是无偏的，即 $\\mathbb{E}[\\hat{\\mathbf{x}}] = \\mathbf{x}$。\n令 $A = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1}$。协方差为：\n$$\n\\operatorname{Cov}(\\hat{\\mathbf{x}}) = \\mathbb{E}[A \\boldsymbol{\\epsilon} (A \\boldsymbol{\\epsilon})^{\\top}] = A \\mathbb{E}[\\boldsymbol{\\epsilon} \\boldsymbol{\\epsilon}^{\\top}] A^{\\top} = A \\Sigma_{\\epsilon} A^{\\top}\n$$\n代入 $A$ 的表达式：\n$$\n\\operatorname{Cov}(\\hat{\\mathbf{x}}) = [(H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1} H^{\\top} \\Sigma_{\\epsilon}^{-1}] \\Sigma_{\\epsilon} [\\Sigma_{\\epsilon}^{-1} H (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1}]\n$$\n这可以简化为：\n$$\n\\operatorname{Cov}(\\hat{\\mathbf{x}}) = (H^{\\top} \\Sigma_{\\epsilon}^{-1} H)^{-1}\n$$\n矩阵 $Q = H^{\\top} \\Sigma_{\\epsilon}^{-1} H$ 是此估计问题的 Fisher 信息矩阵。估计器的协方差是 Fisher 信息的逆。\n\n接下来，我们使用给定的设定计算矩阵 $Q$。调谐矩阵 $H$ 由列向量 $\\mathbf{h}_{1}$ 和 $\\mathbf{h}_{2}$ 构成：\n$$\nH = \\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix}\n$$\n噪声协方差矩阵 $\\Sigma_{\\epsilon}$ 为：\n$$\n\\Sigma_{\\epsilon} = \\sigma^{2}\\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}\n$$\n其逆矩阵 $\\Sigma_{\\epsilon}^{-1}$ 为：\n$$\n\\Sigma_{\\epsilon}^{-1} = \\frac{1}{\\det(\\Sigma_{\\epsilon})} \\sigma^{2}\\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} = \\frac{1}{\\sigma^{4}(1-\\rho^{2})} \\sigma^{2}\\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}\n$$\n现在，我们计算 $Q = H^{\\top}\\Sigma_{\\epsilon}^{-1}H$：\n$$\nQ = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  0 \\\\ \\cos\\theta  \\sin\\theta \\end{pmatrix} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} \\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix}\n$$\n首先，我们计算乘积 $\\Sigma_{\\epsilon}^{-1}H$：\n$$\n\\Sigma_{\\epsilon}^{-1}H = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} \\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix} = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  \\cos\\theta - \\rho\\sin\\theta \\\\ -\\rho  -\\rho\\cos\\theta + \\sin\\theta \\end{pmatrix}\n$$\n然后，我们左乘 $H^{\\top}$：\n$$\nQ = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  0 \\\\ \\cos\\theta  \\sin\\theta \\end{pmatrix} \\begin{pmatrix} 1  \\cos\\theta - \\rho\\sin\\theta \\\\ -\\rho  \\sin\\theta - \\rho\\cos\\theta \\end{pmatrix}\n$$\n$$\nQ = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  \\cos\\theta - \\rho\\sin\\theta \\\\ \\cos\\theta - \\rho\\sin\\theta  \\cos\\theta(\\cos\\theta - \\rho\\sin\\theta) + \\sin\\theta(\\sin\\theta - \\rho\\cos\\theta) \\end{pmatrix}\n$$\n元素 $Q_{22}$ 的内部项简化为：\n$$\nQ_{22, \\text{inner}} = \\cos^{2}\\theta - \\rho\\sin\\theta\\cos\\theta + \\sin^{2}\\theta - \\rho\\sin\\theta\\cos\\theta = 1 - 2\\rho\\sin\\theta\\cos\\theta\n$$\n因此，Fisher 信息矩阵为：\n$$\nQ = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} \\begin{pmatrix} 1  \\cos\\theta - \\rho\\sin\\theta \\\\ \\cos\\theta - \\rho\\sin\\theta  1 - 2\\rho\\sin\\theta\\cos\\theta \\end{pmatrix}\n$$\n问题要求分析当 $\\theta$ 很小时 $Q$ 的条件数。矩阵的条件数与其行列式有关。当矩阵的行列式趋于零时，该矩阵变为病态的。\n$$\n\\det(Q) = \\det(H^{\\top}\\Sigma_{\\epsilon}^{-1}H) = (\\det(H))^{2} \\det(\\Sigma_{\\epsilon}^{-1}) = \\frac{(\\det(H))^{2}}{\\det(\\Sigma_{\\epsilon})}\n$$\n由于 $\\det(H) = \\sin\\theta$ 且 $\\det(\\Sigma_{\\epsilon}) = \\sigma^{4}(1-\\rho^{2})$，我们有：\n$$\n\\det(Q) = \\frac{\\sin^{2}\\theta}{\\sigma^{4}(1-\\rho^{2})}\n$$\n当 $\\theta \\to 0$ 时，$\\sin\\theta \\to 0$，因此 $\\det(Q) \\to 0$。这表明 $Q$ 变为奇异矩阵，因此是病态的。这是因为当 $\\theta$ 很小时，调谐向量 $\\mathbf{h}_1$ 和 $\\mathbf{h}_2$ 变得几乎共线，提供相似的信息，从而难以区分 $x_1$ 和 $x_2$ 的贡献。\n\n最后，我们计算总估计方差，定义为 $\\operatorname{tr}(\\operatorname{Cov}(\\hat{\\mathbf{x}})) = \\operatorname{tr}(Q^{-1})$。对于一个通用的 $2\\times2$ 矩阵 $A$，有 $\\operatorname{tr}(A^{-1}) = \\frac{\\operatorname{tr}(A)}{\\det(A)}$。我们将此公式应用于 $Q$。\n$Q$ 的迹为：\n$$\n\\operatorname{tr}(Q) = \\frac{1}{\\sigma^{2}(1-\\rho^{2})} (1 + 1 - 2\\rho\\sin\\theta\\cos\\theta) = \\frac{2(1 - \\rho\\sin\\theta\\cos\\theta)}{\\sigma^{2}(1-\\rho^{2})}\n$$\n现在我们计算 $\\operatorname{tr}(Q^{-1})$：\n$$\n\\operatorname{tr}(Q^{-1}) = \\frac{\\operatorname{tr}(Q)}{\\det(Q)} = \\frac{\\frac{2(1 - \\rho\\sin\\theta\\cos\\theta)}{\\sigma^{2}(1-\\rho^{2})}}{\\frac{\\sin^{2}\\theta}{\\sigma^{4}(1-\\rho^{2})}}\n$$\n$$\n\\operatorname{tr}(\\operatorname{Cov}(\\hat{\\mathbf{x}})) = \\frac{2(1 - \\rho\\sin\\theta\\cos\\theta)}{\\sigma^{2}(1-\\rho^{2})} \\cdot \\frac{\\sigma^{4}(1-\\rho^{2})}{\\sin^{2}\\theta}\n$$\n简化表达式得到总估计方差：\n$$\n\\operatorname{tr}(\\operatorname{Cov}(\\hat{\\mathbf{x}})) = \\frac{2\\sigma^{2}(1 - \\rho\\sin\\theta\\cos\\theta)}{\\sin^{2}\\theta}\n$$\n此表达式给出了总方差，是调谐角 $\\theta$、噪声方差 $\\sigma^{2}$ 和噪声相关性 $\\rho$ 的函数。",
            "answer": "$$\n\\boxed{\\frac{2\\sigma^{2}(1 - \\rho\\sin\\theta\\cos\\theta)}{\\sin^{2}\\theta}}\n$$"
        },
        {
            "introduction": "神经科学的一个核心问题是信息如何在大规模神经元群体中进行表征。本次实践  旨在研究随着群体规模的增长，冗余和噪声相关性所扮演的复杂角色。通过分析信噪比和互信息，你将发现一些出人意料的条件，在这些条件下增加“冗余”的神经元既可能增强解码能力，也可能削弱解码能力，从而揭示高效神经编码的深层原理。",
            "id": "4006217",
            "problem": "考虑一个标量刺激的线性高斯神经编码和解码场景。设刺激为 $x \\sim \\mathcal{N}(0,\\sigma_x^2)$，群体响应为 $\\mathbf{r} \\in \\mathbb{R}^N$，由 $\\mathbf{r} = \\mathbf{f}\\,x + \\mathbf{n}$ 给出，其中 $\\mathbf{f} \\in \\mathbb{R}^N$ 是群体调谐向量，$\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0},\\boldsymbol{\\Sigma}_n)$ 是与 $x$ 无关的零均值加性噪声。冗余调谐意味着每个神经元具有相同的调谐，因此 $\\mathbf{f} = g\\,\\mathbf{1}_N$，其中 $g \\in \\mathbb{R}$，$\\mathbf{1}_N$ 表示 $N$ 维全一向量。噪声在神经元之间是等相关的：$\\boldsymbol{\\Sigma}_n = \\sigma_n^2\\left((1-\\rho)\\,\\mathbf{I}_N + \\rho\\,\\mathbf{1}_N\\mathbf{1}_N^\\top\\right)$，其中 $\\sigma_n^2  0$，$\\rho \\in \\mathbb{R}$ 满足 $-\\frac{1}{N-1}  \\rho  1$ 以确保半正定性，$\\mathbf{I}_N$ 是 $N \\times N$ 单位矩阵。\n\n线性解码器产生一个估计值 $\\hat{x} = \\mathbf{w}^\\top \\mathbf{r}$，其中 $\\mathbf{w} \\in \\mathbb{R}^N$ 是某个权重向量。目标是分析在最优线性解码下，增加具有冗余调谐的神经元如何影响信噪比（SNR）和互信息（MI）。估计的信噪比（SNR）定义为 $\\hat{x}$ 中信号贡献的方差与 $\\hat{x}$ 中噪声贡献的方差之比，根据上述模型计算。$x$ 和 $\\mathbf{r}$ 之间的互信息（MI）定义为 $I(x;\\mathbf{r}) = H(x) - H(x|\\mathbf{r})$，对于联合高斯变量，必须以奈特（nats）为单位表示；使用自然对数。\n\n您必须从高斯模型中线性估计的核心定义、多元高斯分布的性质以及信噪比（SNR）和互信息（MI）的定义出发。推导最大化 SNR 的最优线性解码器，并计算由此产生的 SNR 和 MI。然后，在两种资源机制下比较增加神经元的效果：\n- 固定单神经元增益：$g$ 不随 $N$ 变化。\n- 固定总增益预算：总平方增益 $\\|\\mathbf{f}\\|_2^2$ 保持不变，因此 $g$ 按 $g = g_0/\\sqrt{N}$ 的比例缩放，其中 $g_0$ 是某个常数。\n\n利用这些推导，实现一个程序来为每个测试用例计算：\n- 最优线性解码器的 SNR。\n- 使用自然对数的 MI（以奈特为单位）。\n- 一个布尔值，指示在相同参数下增加一个冗余神经元（从 $N$ 到 $N+1$）是否会增加 MI，从而将冗余分类为对该增量添加的线性解码有帮助（布尔值为真）还是有害（布尔值为假）。\n\n假设所有测试用例中都使用以下常数：$\\sigma_x^2 = 1.0$，$\\sigma_n^2 = 1.0$，以及 $g_0 = 1.0$。当机制为“固定单神经元增益”时，使用 $g = g_0$；当机制为“固定总增益预算”时，使用 $g = g_0/\\sqrt{N}$。\n\n测试套件：\n- 案例 1：$N = 1$，$\\rho = 0.0$，机制 = 固定单神经元增益。\n- 案例 2：$N = 10$，$\\rho = 0.0$，机制 = 固定单神经元增益。\n- 案例 3：$N = 10$，$\\rho = 0.9$，机制 = 固定单神经元增益。\n- 案例 4：$N = 10$，$\\rho = 0.9$，机制 = 固定总增益预算。\n- 案例 5：$N = 10$，$\\rho = -0.09$，机制 = 固定单神经元增益。\n- 案例 6：$N = 50$，$\\rho = 0.99$，机制 = 固定单神经元增益。\n- 案例 7：$N = 50$，$\\rho = -0.019$，机制 = 固定总增益预算。\n\n您的程序应生成单行输出，其中包含一个由内部列表组成的逗号分隔列表形式的结果，每个内部列表对应上述顺序的一个测试用例，并包含三个元素：$[\\text{SNR}, \\text{MI}, \\text{helps}]$。例如，输出格式必须类似于 $[[s_1,i_1,h_1],[s_2,i_2,h_2],\\dots]$，其中每个 $s_k$ 和 $i_k$ 是浮点数，每个 $h_k$ 是布尔值。MI 必须以奈特为单位表示。除了说明 MI 以奈特为单位外，SNR 或 MI 没有其他单位。此问题不涉及角度。此问题不涉及百分比。打印的单行必须与此格式完全匹配，不得有额外的空格或文本。",
            "solution": "解决方案主要分三部分进行。首先，我们推导最大化刺激估计的信噪比（SNR）的最优线性解码器权重 $\\mathbf{w}$。其次，我们推导刺激和神经响应之间互信息（MI）的公式，并展示其与最大 SNR 的直接关系。第三，我们分析在两种指定的资源机制下，神经元数量 $N$ 如何影响 SNR 和 MI。\n\n刺激是一个随机变量 $x \\sim \\mathcal{N}(0, \\sigma_x^2)$，$N$ 维神经群体响应由线性高斯模型 $\\mathbf{r} = \\mathbf{f}x + \\mathbf{n}$ 给出。噪声 $\\mathbf{n}$ 来自 $\\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_n)$ 且与 $x$ 无关。线性解码器将刺激估计为 $\\hat{x} = \\mathbf{w}^\\top \\mathbf{r}$。\n\n代入 $\\mathbf{r}$ 的模型，估计值为 $\\hat{x} = \\mathbf{w}^\\top(\\mathbf{f}x + \\mathbf{n}) = (\\mathbf{w}^\\top\\mathbf{f})x + \\mathbf{w}^\\top\\mathbf{n}$。该表达式将估计值分为信号分量 $(\\mathbf{w}^\\top\\mathbf{f})x$ 和噪声分量 $\\mathbf{w}^\\top\\mathbf{n}$。\n信号分量的方差为 $\\text{Var}((\\mathbf{w}^\\top\\mathbf{f})x) = (\\mathbf{w}^\\top\\mathbf{f})^2 \\text{Var}(x) = (\\mathbf{w}^\\top\\mathbf{f})^2 \\sigma_x^2$。\n噪声分量的方差为 $\\text{Var}(\\mathbf{w}^\\top\\mathbf{n}) = \\mathbf{w}^\\top \\text{Cov}(\\mathbf{n}) \\mathbf{w} = \\mathbf{w}^\\top \\boldsymbol{\\Sigma}_n \\mathbf{w}$。\n\n估计的信噪比（SNR）定义为这两个方差之比：\n$$ \\text{SNR}(\\mathbf{w}) = \\frac{(\\mathbf{w}^\\top\\mathbf{f})^2 \\sigma_x^2}{\\mathbf{w}^\\top \\boldsymbol{\\Sigma}_n \\mathbf{w}} $$\n这是一个广义瑞利商。关于 $\\mathbf{w}$ 最大化此量是一个标准的优化问题。当 $\\mathbf{w}$ 与 $\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f}$ 成正比时，达到最大值。我们选择 $\\mathbf{w}_{opt} = \\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f}$（任意的缩放常数会抵消掉）。将这个最优权重向量代入 SNR 表达式，得到任何线性解码器可能的最大 SNR：\n$$ \\text{SNR}_{opt} = \\frac{((\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f})^\\top\\mathbf{f})^2 \\sigma_x^2}{(\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f})^\\top \\boldsymbol{\\Sigma}_n (\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f})} = \\frac{(\\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1}\\mathbf{f})^2 \\sigma_x^2}{\\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f}} = \\sigma_x^2 (\\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f}) $$\n这个量 $\\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f}$ 是群体响应关于刺激 $x$ 的费雪信息。\n\n为了计算这个表达式，我们必须计算 $\\boldsymbol{\\Sigma}_n^{-1}$。噪声协方差由 $\\boldsymbol{\\Sigma}_n = \\sigma_n^2\\left((1-\\rho)\\,\\mathbf{I}_N + \\rho\\,\\mathbf{1}_N\\mathbf{1}_N^\\top\\right)$ 给出。我们可以使用 Sherman-Morrison-Woodbury 公式来求该矩阵的逆，该公式为 $(\\mathbf{A}+\\mathbf{UV}^\\top)^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{I}+\\mathbf{V}^\\top\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^\\top\\mathbf{A}^{-1}$。\n令 $\\mathbf{A} = (1-\\rho)\\mathbf{I}_N$，$\\mathbf{U} = \\rho\\mathbf{1}_N$，以及 $\\mathbf{V} = \\mathbf{1}_N$。其逆为：\n$$ \\boldsymbol{\\Sigma}_n^{-1} = \\frac{1}{\\sigma_n^2} \\left[ \\frac{1}{1-\\rho}\\mathbf{I}_N - \\frac{\\rho}{(1-\\rho)(1+(N-1)\\rho)} \\mathbf{1}_N \\mathbf{1}_N^\\top \\right] $$\n问题指定了冗余调谐，$\\mathbf{f} = g\\mathbf{1}_N$。我们现在计算二次型 $\\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f}$：\n$$ \\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f} = g^2 \\mathbf{1}_N^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{1}_N = \\frac{g^2}{\\sigma_n^2} \\mathbf{1}_N^\\top \\left[ \\frac{1}{1-\\rho}\\mathbf{I}_N - \\frac{\\rho}{(1-\\rho)(1+(N-1)\\rho)} \\mathbf{1}_N \\mathbf{1}_N^\\top \\right] \\mathbf{1}_N $$\n使用 $\\mathbf{1}_N^\\top \\mathbf{I}_N \\mathbf{1}_N = N$ 和 $\\mathbf{1}_N^\\top (\\mathbf{1}_N \\mathbf{1}_N^\\top) \\mathbf{1}_N = (\\mathbf{1}_N^\\top \\mathbf{1}_N)^2 = N^2$：\n$$ \\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f} = \\frac{g^2}{\\sigma_n^2} \\left[ \\frac{N}{1-\\rho} - \\frac{\\rho N^2}{(1-\\rho)(1+(N-1)\\rho)} \\right] = \\frac{g^2 N}{\\sigma_n^2(1-\\rho)} \\left[ 1 - \\frac{N\\rho}{1+(N-1)\\rho} \\right] $$\n$$ = \\frac{g^2 N}{\\sigma_n^2(1-\\rho)} \\left[ \\frac{1+(N-1)\\rho - N\\rho}{1+(N-1)\\rho} \\right] = \\frac{g^2 N}{\\sigma_n^2(1-\\rho)} \\left[ \\frac{1-\\rho}{1+(N-1)\\rho} \\right] = \\frac{g^2 N}{\\sigma_n^2(1+(N-1)\\rho)} $$\n因此，最优 SNR 为：\n$$ \\text{SNR}_{opt} = \\sigma_x^2 \\frac{g^2 N}{\\sigma_n^2(1+(N-1)\\rho)} $$\n我们现在针对两种机制，使用给定的常数 $\\sigma_x^2=1.0$，$\\sigma_n^2=1.0$ 和 $g_0=1.0$ 来具体化这个公式。\n1.  **固定单神经元增益**：$g=g_0=1.0$。\n    $$ \\text{SNR}_{opt}(N) = \\frac{N}{1+(N-1)\\rho} $$\n2.  **固定总增益预算**：$g = g_0/\\sqrt{N} = 1.0/\\sqrt{N}$。\n    $$ \\text{SNR}_{opt}(N) = \\left(\\frac{1}{\\sqrt{N}}\\right)^2 \\frac{N}{1+(N-1)\\rho} = \\frac{1}{1+(N-1)\\rho} $$\n\n接下来，我们推导互信息 $I(x;\\mathbf{r})$。对于一个联合高斯系统，$I(x;\\mathbf{r}) = \\frac{1}{2} \\ln \\det(\\mathbf{I} + \\text{Cov}(x)^{-1} \\text{Cov}(x, \\mathbf{r}) \\text{Cov}(\\mathbf{r}|x)^{-1} \\text{Cov}(\\mathbf{r}, x))$。一个更简单的方法是使用协方差矩阵行列式之间的关系：$I(x;\\mathbf{r}) = \\frac{1}{2}\\ln\\left( \\frac{\\det(\\text{Cov}(x))\\det(\\text{Cov}(\\mathbf{r}))}{\\det(\\text{Cov}(x,\\mathbf{r}))} \\right)$，或者最直接地，利用 MI 和后验方差之间的关系。在线性高斯模型中，$I(x;\\mathbf{r}) = \\frac{1}{2}\\ln\\left(1 + \\sigma_x^2 \\mathbf{f}^\\top \\boldsymbol{\\Sigma}_n^{-1} \\mathbf{f}\\right)$。识别出对数内的项，我们发现 MI 和最优 SNR 之间存在直接关系：\n$$ I(x;\\mathbf{r}) = \\frac{1}{2}\\ln(1 + \\text{SNR}_{opt}) $$\n根据要求，MI以奈特（nats）为单位度量。\n\n最后，我们确定增加一个神经元（从 $N$ 到 $N+1$）是否有助于解码，如果 $I(N+1)  I(N)$ 则为真。由于 $\\ln(1+z)$ 对于 $z  -1$ 是一个严格递增函数，这个条件等价于 $\\text{SNR}_{opt}(N+1)  \\text{SNR}_{opt}(N)$。\n\n1.  **固定单神经元增益**：我们测试是否 $\\frac{N+1}{1+N\\rho}  \\frac{N}{1+(N-1)\\rho}$。这个不等式简化为 $(N+1)(1+(N-1)\\rho)  N(1+N\\rho)$，进一步简化为 $1-\\rho  0$，即 $\\rho  1$。由于问题指定 $\\rho  1$ 以使 $\\boldsymbol{\\Sigma}_n$ 为正定，增加一个神经元在这种机制下总是会增加 MI。因此，冗余总是**有帮助的**。\n\n2.  **固定总增益预算**：我们测试是否 $\\frac{1}{1+N\\rho}  \\frac{1}{1+(N-1)\\rho}$。这当且仅当 $1+(N-1)\\rho  1+N\\rho$ 时为真，简化为 $-\\rho  0$，即 $\\rho  0$。在这种机制下，只有当噪声相关为负（$\\rho  0$）时，增加一个神经元才**有帮助**，如果为正（$\\rho  0$），则**有害**。如果 $\\rho=0$，则没有变化。\n\n这些推导出的公式和条件被实现用来计算每个测试用例的结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the neural decoding problem for a suite of test cases.\n\n    This function calculates the Signal-to-Noise Ratio (SNR) of an optimal\n    linear decoder, the Mutual Information (MI) between stimulus and response,\n    and a boolean indicating if adding one neuron improves the MI. The\n    calculations are performed for several parameter sets defined in the\n    problem statement.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Note: Using dicts for clarity, but the order is preserved.\n    test_cases = [\n        {'N': 1, 'rho': 0.0, 'regime': 'fixed per-neuron gain'},\n        {'N': 10, 'rho': 0.0, 'regime': 'fixed per-neuron gain'},\n        {'N': 10, 'rho': 0.9, 'regime': 'fixed per-neuron gain'},\n        {'N': 10, 'rho': 0.9, 'regime': 'fixed total gain budget'},\n        {'N': 10, 'rho': -0.09, 'regime': 'fixed per-neuron gain'},\n        {'N': 50, 'rho': 0.99, 'regime': 'fixed per-neuron gain'},\n        {'N': 50, 'rho': -0.019, 'regime': 'fixed total gain budget'},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        N = case['N']\n        rho = case['rho']\n        regime = case['regime']\n\n        snr = 0.0\n        helps = False\n\n        if N  1:\n            # The model is not defined for N1. Skip or handle error.\n            # For this problem, all N >= 1.\n            pass\n\n        # Check for positive definiteness condition, although all test cases are valid.\n        # This check is for robustness. For N > 1, rho must be > -1/(N-1).\n        if N > 1 and rho = -1 / (N - 1):\n             raise ValueError(f\"Invalid rho={rho} for N={N}. Condition is rho > -1/(N-1).\")\n\n        if regime == 'fixed per-neuron gain':\n            # SNR = N / (1 + (N-1)*rho)\n            # The constants sigma_x^2, g_0, sigma_n^2 are all 1.0.\n            denominator_snr = 1.0 + (N - 1) * rho\n            if denominator_snr > 0:\n                snr = N / denominator_snr\n            else:\n                # This case should not be reached with valid rho\n                snr = float('inf')\n            \n            # As derived, with fixed per-neuron gain and rho  1,\n            # adding a neuron always increases SNR and MI.\n            helps = True\n\n        elif regime == 'fixed total gain budget':\n            # SNR = 1 / (1 + (N-1)*rho)\n            # The constants sigma_x^2, g_0, sigma_n^2 are all 1.0.\n            denominator_snr = 1.0 + (N - 1) * rho\n            if denominator_snr > 0:\n                snr = 1.0 / denominator_snr\n            else:\n                # This case should not be reached with valid rho\n                snr = float('inf')\n\n            # As derived, with fixed total gain, adding a neuron helps\n            # only if noise correlations are negative.\n            helps = rho  0.0\n        \n        # Mutual Information is I = 0.5 * log(1 + SNR)\n        # Using np.log for the natural logarithm (for MI in nats).\n        mi = 0.5 * np.log(1.0 + snr)\n\n        all_results.append([snr, mi, helps])\n\n    # Format the final output string as per problem specification.\n    # e.g., [[s1,i1,h1],[s2,i2,h2],...]\n    # Booleans are converted to lowercase 'true'/'false'.\n    result_strings = []\n    for s, m, h in all_results:\n        result_strings.append(f\"[{s},{m},{str(h).lower()}]\")\n\n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}