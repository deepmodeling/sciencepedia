{
    "hands_on_practices": [
        {
            "introduction": "At its core, analysis-by-synthesis involves reconciling internal generative models with incoming sensory data. This foundational exercise provides a concrete mathematical look at this process. By deriving the posterior distribution from a Gaussian prior and likelihood, you will quantify how prior expectations can systematically bias perception, offering a precise definition of a perceptual illusion as the discrepancy between the most likely sensory value and the final, integrated percept .",
            "id": "3984161",
            "problem": "Consider an instance of analysis-by-synthesis in sensory inference where a latent cause $z$ in a generative model represents a perceived physical property and an observation $x$ is a noisy measurement of $z$. The generative model is specified by a Gaussian prior $z \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$ and a Gaussian likelihood $x \\mid z \\sim \\mathcal{N}(z,\\sigma_x^2)$. Here, $\\mu_0$ encodes a biased prior expectation and $\\sigma_0^2$ encodes prior uncertainty, while $\\sigma_x^2$ encodes sensory uncertainty. The observer performs Bayesian inference to obtain the posterior $p(z \\mid x)$ and uses the posterior mean $\\mathbb{E}[z \\mid x]$ as the estimate of $z$.\n\nStarting only from the definition of Bayes’ rule and the standard form of the Gaussian probability density function, derive the posterior distribution $p(z \\mid x)$ and obtain its mean $\\mathbb{E}[z \\mid x]$. Define the illusion strength $S(x)$ as the deviation of the Bayesian posterior mean from the maximum likelihood estimate, that is, $S(x) = \\mathbb{E}[z \\mid x] - x$. Provide a single closed-form analytic expression for $S(x)$ in terms of $\\mu_0$, $\\sigma_0^2$, $\\sigma_x^2$, and $x$. No numerical approximation is required, and no units are to be reported. Your final answer must be a single analytic expression.",
            "solution": "The problem is scientifically grounded in Bayesian statistics, a core component of computational neuroscience. It is well-posed, with all necessary information provided, allowing for the derivation of a unique, meaningful solution.\n\nWe begin by applying Bayes' rule to find the posterior distribution $p(z \\mid x)$. Bayes' rule states:\n$$\np(z \\mid x) = \\frac{p(x \\mid z) p(z)}{p(x)}\n$$\nThe term $p(x)$ in the denominator is the marginal likelihood, which acts as a normalization constant and does not depend on the latent cause $z$. Therefore, we can express the posterior as being proportional to the product of the likelihood and the prior:\n$$\np(z \\mid x) \\propto p(x \\mid z) p(z)\n$$\nThe problem specifies a Gaussian prior, $z \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$, and a Gaussian likelihood, $x \\mid z \\sim \\mathcal{N}(z, \\sigma_x^2)$. Their respective probability density functions (PDFs) are:\n$$\np(z) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} \\right)\n$$\n$$\np(x \\mid z) = \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} \\exp\\left( -\\frac{(x - z)^2}{2\\sigma_x^2} \\right)\n$$\nMultiplying these two PDFs, and ignoring the constant coefficients (since we are working with proportionality), we get:\n$$\np(z \\mid x) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} \\right) \\exp\\left( -\\frac{(x - z)^2}{2\\sigma_x^2} \\right) = \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} - \\frac{(x - z)^2}{2\\sigma_x^2} \\right)\n$$\nTo identify the form of the posterior distribution, we analyze the argument of the exponential. We expand the quadratic terms in the exponent and collect terms with respect to powers of $z$:\n$$\n-\\frac{1}{2} \\left[ \\frac{z^2 - 2z\\mu_0 + \\mu_0^2}{\\sigma_0^2} + \\frac{z^2 - 2zx + x^2}{\\sigma_x^2} \\right] = -\\frac{1}{2} \\left[ z^2 \\left( \\frac{1}{\\sigma_0^2} + \\frac{1}{\\sigma_x^2} \\right) - 2z \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{x}{\\sigma_x^2} \\right) + \\text{const} \\right]\n$$\nThis expression is a quadratic function of $z$, which means the posterior $p(z \\mid x)$ is a Gaussian distribution. To find its parameters, we complete the square for $z$. The posterior precision (inverse variance) $\\sigma_{post}^{-2}$ is the coefficient of the $z^2$ term (multiplied by -2), and the posterior mean $\\mu_{post}$ can be derived from the linear term.\n\nBy comparing the coefficients with the standard form of a Gaussian log-PDF, $\\log p(z) \\propto -\\frac{1}{2\\sigma^2}(z-\\mu)^2$, we can identify the posterior precision $\\sigma_{post}^{-2} = \\frac{1}{\\sigma_0^2} + \\frac{1}{\\sigma_x^2}$. The posterior mean $\\mu_{post} = \\mathbb{E}[z \\mid x]$ is given by a precision-weighted average of the prior mean and the data:\n$$\n\\mathbb{E}[z \\mid x] = \\frac{\\mu_0/\\sigma_0^2 + x/\\sigma_x^2}{1/\\sigma_0^2 + 1/\\sigma_x^2} = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_x^2 + \\sigma_0^2}\n$$\nThe problem defines the illusion strength $S(x)$ as the deviation of the Bayesian posterior mean from the maximum likelihood estimate (MLE). The likelihood function is $p(x|z) = \\mathcal{N}(x; z, \\sigma_x^2)$. The MLE for $z$ is the value that maximizes this function, which is $z_{MLE} = x$. The problem correctly identifies this by defining $S(x) = \\mathbb{E}[z \\mid x] - x$.\nWe can now derive the expression for $S(x)$:\n$$\nS(x) = \\mathbb{E}[z \\mid x] - x = \\left( \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 + \\sigma_x^2} \\right) - x\n$$\nTo simplify, we put everything over a common denominator:\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2 - x(\\sigma_0^2 + \\sigma_x^2)}{\\sigma_0^2 + \\sigma_x^2}\n$$\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2 - x \\sigma_0^2 - x \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\nThe terms involving $x \\sigma_0^2$ cancel out:\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 - x \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\nFactoring out the common term $\\sigma_x^2$ in the numerator gives the final closed-form expression:\n$$\nS(x) = \\frac{(\\mu_0 - x) \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$",
            "answer": "$$\n\\boxed{S(x) = \\frac{(\\mu_0 - x) \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}}\n$$"
        },
        {
            "introduction": "Generative models of cognition and neural processing are often organized hierarchically, with abstract causes generating progressively more concrete representations. This exercise moves beyond a single latent variable to explore inference in a three-level deep generative model. By working with precisions (the inverse of variance), you will derive how a single observation influences beliefs at every level of the hierarchy, revealing the characteristic structure of information flow in these powerful models .",
            "id": "3984111",
            "problem": "Consider an analysis-by-synthesis linear-Gaussian generative model of a three-level latent hierarchy with a single scalar observation. Let the latent variables be $z_3$, $z_2$, and $z_1$ with the following generative structure:\n- The top-level latent variable has a zero-mean Gaussian prior: $z_3 \\sim \\mathcal{N}(0, \\tau_3^{-1})$, where $\\tau_3 > 0$ is the prior precision.\n- The middle-level conditional is Gaussian: $z_2 \\mid z_3 \\sim \\mathcal{N}(z_3, \\tau_2^{-1})$, where $\\tau_2 > 0$ is the conditional precision.\n- The bottom-level conditional is Gaussian: $z_1 \\mid z_2 \\sim \\mathcal{N}(z_2, \\tau_1^{-1})$, where $\\tau_1 > 0$ is the conditional precision.\n- The observation model is Gaussian: $x \\mid z_1 \\sim \\mathcal{N}(z_1, \\gamma^{-1})$, where $\\gamma > 0$ is the observation precision and $x \\in \\mathbb{R}$ is observed.\n\nUsing only the foundational facts that Bayes’ rule turns products of Gaussian factors into a Gaussian posterior and that a multivariate Gaussian can be written in canonical (information) form with a precision matrix and a shift vector, derive the joint posterior over $(z_1, z_2, z_3)$ given $x$, and from it compute the marginal posterior means $\\mu_\\ell = \\mathbb{E}[z_\\ell \\mid x]$ and marginal posterior precisions $\\tau_\\ell^{\\text{post}} = \\left(\\operatorname{Var}[z_\\ell \\mid x]\\right)^{-1}$ for $\\ell \\in \\{1, 2, 3\\}$. Express your answers as exact symbolic expressions in terms of $x$, $\\tau_1$, $\\tau_2$, $\\tau_3$, and $\\gamma$.\n\nYour final answer must be a single row matrix, in the order $\\mu_1, \\mu_2, \\mu_3, \\tau_1^{\\text{post}}, \\tau_2^{\\text{post}}, \\tau_3^{\\text{post}}$. No numerical approximation or rounding is required, and no units are involved.",
            "solution": "The problem requires deriving the marginal posterior means and precisions for a three-level hierarchical linear-Gaussian model. Since the model is a product of Gaussians, the joint posterior over the latent variables $\\mathbf{z} = (z_1, z_2, z_3)^T$ is also a multivariate Gaussian. We will work with the log-posterior, which is a quadratic function of $\\mathbf{z}$.\n\nThe log of a Gaussian density $\\mathcal{N}(y \\mid \\mu, \\tau^{-1})$ is, up to a constant, $-\\frac{\\tau}{2}(y-\\mu)^2$. The log-posterior is proportional to the sum of the logs of the individual probability densities defined in the model:\n$$ \\ln p(\\mathbf{z} \\mid x) \\propto -\\frac{\\gamma}{2}(x - z_1)^2 - \\frac{\\tau_1}{2}(z_1 - z_2)^2 - \\frac{\\tau_2}{2}(z_2 - z_3)^2 - \\frac{\\tau_3}{2}(z_3 - 0)^2 $$\nA multivariate Gaussian posterior $p(\\mathbf{z} \\mid x) = \\mathcal{N}(\\mathbf{z} \\mid \\boldsymbol{\\mu}_{\\text{post}}, \\mathbf{\\Lambda}_{\\text{post}}^{-1})$ has a log-density in canonical form:\n$$ \\ln p(\\mathbf{z} \\mid x) \\propto -\\frac{1}{2} \\mathbf{z}^T \\mathbf{\\Lambda}_{\\text{post}} \\mathbf{z} + \\mathbf{h}_{\\text{post}}^T \\mathbf{z} $$\nwhere $\\mathbf{\\Lambda}_{\\text{post}}$ is the posterior precision matrix and $\\mathbf{h}_{\\text{post}}$ is the shift vector. We can find these by expanding our log-posterior expression and collecting quadratic and linear terms in $\\mathbf{z}$.\n$$ \\ln p(\\mathbf{z} \\mid x) \\propto -\\frac{1}{2} \\left[ (\\gamma + \\tau_1)z_1^2 + (\\tau_1 + \\tau_2)z_2^2 + (\\tau_2 + \\tau_3)z_3^2 - 2\\tau_1z_1z_2 - 2\\tau_2z_2z_3 \\right] + \\gamma x z_1 $$\nFrom this, we can identify the symmetric posterior precision matrix $\\mathbf{\\Lambda}_{\\text{post}}$ and the shift vector $\\mathbf{h}_{\\text{post}}$:\n$$ \\mathbf{\\Lambda}_{\\text{post}} = \\begin{pmatrix} \\gamma + \\tau_1 & -\\tau_1 & 0 \\\\ -\\tau_1 & \\tau_1 + \\tau_2 & -\\tau_2 \\\\ 0 & -\\tau_2 & \\tau_2 + \\tau_3 \\end{pmatrix}, \\quad \\mathbf{h}_{\\text{post}} = \\begin{pmatrix} \\gamma x \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe posterior mean vector is $\\boldsymbol{\\mu}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1} \\mathbf{h}_{\\text{post}}$, and the posterior covariance matrix is $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1}$.\n\nTo compute the inverse $\\mathbf{\\Lambda}_{\\text{post}}^{-1}$, we first find its determinant, $D = \\det(\\mathbf{\\Lambda}_{\\text{post}})$:\n$$ D = (\\gamma + \\tau_1)[(\\tau_1 + \\tau_2)(\\tau_2 + \\tau_3) - \\tau_2^2] - (-\\tau_1)[(-\\tau_1)(\\tau_2+\\tau_3)] $$\n$$ D = (\\gamma + \\tau_1)(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) - \\tau_1^2(\\tau_2 + \\tau_3) $$\n$$ D = \\gamma(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) + \\tau_1(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) - \\tau_1^2\\tau_2 - \\tau_1^2\\tau_3 $$\n$$ D = \\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3 $$\nThe inverse matrix is $\\frac{1}{D}\\text{adj}(\\mathbf{\\Lambda}_{\\text{post}})$. We need the first column of the adjugate matrix to find the posterior means $\\boldsymbol{\\mu}_{\\text{post}}$:\n$C_{11} = \\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3$\n$C_{21} = \\tau_1(\\tau_2 + \\tau_3)$\n$C_{31} = \\tau_1\\tau_2$\nSo, $\\boldsymbol{\\mu}_{\\text{post}} = \\frac{1}{D} \\text{adj}(\\mathbf{\\Lambda}_{\\text{post}}) \\mathbf{h}_{\\text{post}} = \\frac{\\gamma x}{D} (C_{11}, C_{21}, C_{31})^T$.\n$$ \\mu_1 = \\frac{\\gamma x (\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3)}{D}, \\quad \\mu_2 = \\frac{\\gamma x \\tau_1(\\tau_2 + \\tau_3)}{D}, \\quad \\mu_3 = \\frac{\\gamma x \\tau_1\\tau_2}{D} $$\nFor the marginal posterior precisions $\\tau_\\ell^{\\text{post}}$, we need the diagonal elements of the covariance matrix $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1}$, which are $(\\mathbf{\\Sigma}_{\\text{post}})_{\\ell\\ell} = C_{\\ell\\ell}/D$.\n$$ \\operatorname{Var}[z_1 \\mid x] = \\frac{C_{11}}{D} = \\frac{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3}{D} $$\n$$ \\operatorname{Var}[z_2 \\mid x] = \\frac{C_{22}}{D} = \\frac{(\\gamma+\\tau_1)(\\tau_2+\\tau_3)}{D} $$\n$$ \\operatorname{Var}[z_3 \\mid x] = \\frac{C_{33}}{D} = \\frac{(\\gamma+\\tau_1)(\\tau_1+\\tau_2)-\\tau_1^2}{D} = \\frac{\\gamma\\tau_1 + \\gamma\\tau_2 + \\tau_1\\tau_2}{D} $$\nThe marginal posterior precisions are the reciprocals of these variances, $\\tau_\\ell^{\\text{post}} = 1/\\operatorname{Var}[z_\\ell \\mid x]$.\n$$ \\tau_1^{\\text{post}} = \\frac{D}{C_{11}} = \\frac{\\gamma(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) + \\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3} = \\gamma + \\frac{\\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3} $$\n$$ \\tau_2^{\\text{post}} = \\frac{D}{C_{22}} = \\frac{D}{(\\gamma+\\tau_1)(\\tau_2+\\tau_3)} $$\n$$ \\tau_3^{\\text{post}} = \\frac{D}{C_{33}} = \\frac{D}{\\gamma\\tau_1 + \\gamma\\tau_2 + \\tau_1\\tau_2} $$\nSubstituting the expression for $D$ gives the final forms presented in the answer box. Note that for consistency in the final answer, we reorder terms in the denominator $D$ to be $\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3$.",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\frac{\\gamma \\tau_1 (\\tau_2 + \\tau_3) x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\frac{\\gamma \\tau_1 \\tau_2 x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\gamma + \\frac{\\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3} & \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)} & \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}{\\gamma(\\tau_1 + \\tau_2) + \\tau_1\\tau_2} \\end{pmatrix}} $$"
        },
        {
            "introduction": "While previous exercises focused on the final result of inference, this practice explores the dynamic process of getting there. Here, you will implement a core component of the predictive coding framework, where inference is achieved by dynamically minimizing a \"free energy\" objective function via gradient descent. This computational experiment will allow you to directly observe how the precision of sensory information acts as a gain on prediction errors, governing the speed at which the model converges on the most likely cause of its sensations .",
            "id": "3984145",
            "problem": "You are tasked with designing and executing a computational experiment, in the setting of generative models and analysis-by-synthesis, to demonstrate how increasing the precision of sensory noise, denoted by $\\Pi_x$, modulates the gain of error units and the speed of convergence in predictive coding. Consider a linear Gaussian generative model where a latent state vector $z \\in \\mathbb{R}^2$ gives rise to an observation vector $x \\in \\mathbb{R}^3$ via a synthesis mapping $C \\in \\mathbb{R}^{3 \\times 2}$ and additive sensory noise. The generative model is defined by\n$$\nx = C z + \\varepsilon_x,\n$$\nwhere the sensory noise is drawn from a Gaussian distribution with zero mean and covariance matrix $\\Sigma_x \\in \\mathbb{R}^{3 \\times 3}$ and precision $\\Pi_x = \\Sigma_x^{-1}$. The prior over $z$ is Gaussian with mean $\\mu_z$ and precision $\\Pi_z$.\n\nIn predictive coding, the latent $z$ is inferred by minimizing a synthesis-analysis consistency objective that penalizes prediction error and deviation from the prior. A suitable objective for a single observation $x$ is\n$$\nJ(z) = \\frac{1}{2}\\,\\Pi_x\\,\\|x - C z\\|_2^2 + \\frac{1}{2}\\,\\Pi_z\\,\\|z - \\mu_z\\|_2^2.\n$$\nYou must derive the gradient descent update rule for $z$ that minimizes $J(z)$ and implement the resultant iterative predictive coding dynamics. Let the error unit be defined by $e_x(t) = x - C z(t)$ for iteration index $t \\in \\{0,1,2,\\ldots\\}$. The gain applied to the error unit is embodied by the precision-weighted error $\\tilde{e}_x(t) = \\Pi_x\\, e_x(t)$. Quantify the instantaneous gain at initialization by the ratio\n$$\nG(\\Pi_x) = \\frac{\\|\\tilde{e}_x(0)\\|_2}{\\|e_x(0)\\|_2}.\n$$\nQuantify the speed of convergence by the number of iterations required for the Euclidean norm of the gradient $\\|\\nabla J(z(t))\\|_2$ to fall below a specified tolerance $\\varepsilon$, subject to a maximum iteration budget $T_{\\max}$.\n\nImplement the following experiment parameters, which are scientifically plausible and self-consistent, and use them identically across all test cases:\n- Synthesis matrix $C$:\n$$ C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} $$\n- Prior mean $\\mu_z$ and prior precision $\\Pi_z$:\n$$ \\mu_z = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad \\Pi_z = 1 $$\n- Ground-truth latent $z_{\\text{true}}$, deterministic sensory noise vector $\\varepsilon_x$, and observation $x$:\n$$ z_{\\text{true}} = \\begin{bmatrix} 2.0 \\\\ -1.0 \\end{bmatrix}, \\quad \\varepsilon_x = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}, \\quad x = C z_{\\text{true}} + \\varepsilon_x $$\n- Initial latent $z(0)$:\n$$ z(0) = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix} $$\n- Fixed learning rate $\\eta = 0.005$, tolerance $\\varepsilon = 10^{-6}$, and maximum iterations $T_{\\max} = 10000$.\n\nStarting from the definitions above, derive the gradient descent update rule for $z$ that decreases $J(z)$, implement the dynamics, and run the experiment over the following test suite of sensory precisions $\\Pi_x$:\n1. $\\Pi_x = 0.01$ (very low precision, near-uninformative sensory likelihood).\n2. $\\Pi_x = 0.1$ (low precision).\n3. $\\Pi_x = 1.0$ (moderate precision).\n4. $\\Pi_x = 10.0$ (high precision).\n5. $\\Pi_x = 100.0$ (very high precision).\n\nFor each test case, compute and return the following three quantities:\n- The gain $G(\\Pi_x)$ as defined above, a float.\n- The number of iterations $N(\\Pi_x)$ required until $\\|\\nabla J(z(t))\\|_2 \\le \\varepsilon$, an integer; if convergence does not occur within $T_{\\max}$ iterations, set $N(\\Pi_x) = T_{\\max}$.\n- A boolean $B(\\Pi_x)$ indicating whether convergence was achieved, where $B(\\Pi_x) = \\text{True}$ if $\\|\\nabla J(z(t))\\|_2 \\le \\varepsilon$ at termination and $\\text{False}$ otherwise.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a comma-separated list in square brackets of the form `[G(Pi_x),N(Pi_x),B(Pi_x)]`. For example, for three cases your output would look like `[[G_1,N_1,B_1],[G_2,N_2,B_2],[G_3,N_3,B_3]]`, but with the values corresponding to the five specified test precisions. No physical units or angles are involved; all quantities are dimensionless.",
            "solution": "The problem requires implementing an iterative inference scheme (predictive coding) based on gradient descent. The core task is to derive the update rule for the latent state $z$ by minimizing the objective function $J(z)$.\n\nThe objective function is:\n$$\nJ(z) = \\frac{1}{2}\\,\\Pi_x\\,\\|x - C z\\|_2^2 + \\frac{1}{2}\\,\\Pi_z\\,\\|z - \\mu_z\\|_2^2\n$$\nThis function represents the negative log-posterior of $z$ given $x$, under isotropic Gaussian assumptions for both the likelihood and the prior. Minimizing $J(z)$ is equivalent to maximizing the posterior probability, i.e., finding the MAP estimate of $z$.\n\nWe use gradient descent to find the minimum. The update rule is $z(t+1) = z(t) - \\eta \\nabla J(z(t))$, where $\\eta$ is the learning rate. We first derive the gradient of $J(z)$ with respect to $z$.\n\nUsing vector calculus, the gradient of the first term (likelihood) is:\n$$\n\\nabla_z \\left(\\frac{1}{2}\\Pi_x (x - Cz)^T(x - Cz)\\right) = \\frac{1}{2}\\Pi_x \\nabla_z \\left((x - Cz)^T(x - Cz)\\right) = \\frac{1}{2}\\Pi_x \\left( 2 C^T(Cz - x) \\right) = \\Pi_x C^T(Cz - x)\n$$\nThe gradient of the second term (prior) is:\n$$\n\\nabla_z \\left(\\frac{1}{2}\\Pi_z(z - \\mu_z)^T(z - \\mu_z)\\right) = \\frac{1}{2}\\Pi_z \\left( 2 (z - \\mu_z) \\right) = \\Pi_z(z - \\mu_z)\n$$\nCombining them, the full gradient is:\n$$\n\\nabla J(z) = \\Pi_x C^T(Cz - x) + \\Pi_z(z - \\mu_z)\n$$\nThis gradient represents the amount by which the objective function changes for an infinitesimal change in $z$. The predictive coding dynamics will iteratively update $z$ in the direction opposite to the gradient. The update rule is:\n$$\nz(t+1) = z(t) - \\eta \\left( \\Pi_x C^T(C z(t) - x) + \\Pi_z(z(t) - \\mu_z) \\right)\n$$\nNext, we calculate the instantaneous gain at initialization, $G(\\Pi_x)$. It is defined as the ratio of the norms of the precision-weighted error to the raw error at $t=0$:\n$$\nG(\\Pi_x) = \\frac{\\|\\tilde{e}_x(0)\\|_2}{\\|e_x(0)\\|_2} = \\frac{\\|\\Pi_x\\, e_x(0)\\|_2}{\\|e_x(0)\\|_2}\n$$\nSince $\\Pi_x$ is a scalar in this problem, we can factor it out of the norm:\n$$\nG(\\Pi_x) = \\frac{|\\Pi_x| \\|e_x(0)\\|_2}{\\|e_x(0)\\|_2} = |\\Pi_x|\n$$\nGiven that all specified test values for $\\Pi_x$ are positive, the gain is simply $G(\\Pi_x) = \\Pi_x$. This shows that sensory precision acts as a linear gain on the prediction error signals that drive inference.\n\nThe computational experiment proceeds by first computing the fixed observation vector $x$. Then, for each value of $\\Pi_x$ in the test suite, we initialize $z(0)$ and iterate the gradient descent update rule. In each iteration, we calculate the norm of the gradient, $\\|\\nabla J(z(t))\\|_2$. The loop terminates when this norm falls below the tolerance $\\varepsilon$, or when the maximum number of iterations $T_{\\max}$ is reached. The number of iterations and a boolean flag for convergence are recorded for each $\\Pi_x$. The results are expected to show that convergence is faster (requires fewer iterations) for higher values of $\\Pi_x$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and executes a computational experiment on a predictive coding model.\n    \"\"\"\n    # 1. Define fixed parameters for the experiment\n    C = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n    mu_z = np.array([0.0, 0.0])\n    Pi_z = 1.0\n    z_true = np.array([2.0, -1.0])\n    eps_x_vec = np.array([0.1, -0.2, 0.05])\n    z_initial = np.array([0.0, 0.0])\n    \n    eta = 0.005  # Learning rate\n    tolerance = 1e-6\n    T_max = 10000\n\n    # Test suite for sensory precision\n    test_precisions = [0.01, 0.1, 1.0, 10.0, 100.0]\n\n    # Pre-compute constant values\n    x = C @ z_true + eps_x_vec\n    C_T = C.T\n\n    # Store results for all test cases\n    all_results = []\n\n    # 2. Iterate through each test case\n    for Pi_x in test_precisions:\n        # Calculate gain G(Pi_x)\n        # G = ||Pi_x * e_x(0)|| / ||e_x(0)|| = |Pi_x|\n        gain = float(Pi_x)\n\n        # Initialize variables for the iterative process\n        z = z_initial.copy()\n        converged = False\n        num_iterations = 0\n\n        # 3. Run predictive coding dynamics (gradient descent)\n        for i in range(T_max + 1):\n            # Calculate gradient: grad_J = Pi_x * C^T * (C*z - x) + Pi_z * (z - mu_z)\n            gradient = Pi_x * (C_T @ (C @ z - x)) + Pi_z * (z - mu_z)\n            \n            grad_norm = np.linalg.norm(gradient)\n\n            # Check for convergence\n            if grad_norm = tolerance:\n                num_iterations = i\n                converged = True\n                break\n            \n            # If we are at the last possible iteration and haven't converged\n            if i == T_max:\n                num_iterations = T_max\n                converged = (grad_norm = tolerance) # Final check\n                break\n\n            # Update latent state z\n            z -= eta * gradient\n        \n        # Store results for this test case\n        all_results.append([gain, num_iterations, converged])\n\n    # 4. Format and print the final output\n    result_str_parts = []\n    for res in all_results:\n        res_str = f\"[{res[0]},{res[1]},{res[2]}]\"\n        result_str_parts.append(res_str)\n    \n    final_output = f\"[{','.join(result_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}