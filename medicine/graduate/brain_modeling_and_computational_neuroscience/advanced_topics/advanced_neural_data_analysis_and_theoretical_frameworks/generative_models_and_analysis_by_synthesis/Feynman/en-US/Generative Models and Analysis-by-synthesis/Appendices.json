{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds our understanding of analysis-by-synthesis in the foundational principles of Bayesian inference. You will derive the posterior distribution that results from combining a Gaussian prior belief with a noisy Gaussian sensory measurement, a scenario that forms the bedrock of the \"Bayesian Brain\" hypothesis. By deriving the illusion strength—the discrepancy between the Bayesian percept and the raw sensory data—you will gain a quantitative understanding of how prior expectations can systematically and rationally bias perception .",
            "id": "3984161",
            "problem": "Consider an instance of analysis-by-synthesis in sensory inference where a latent cause $z$ in a generative model represents a perceived physical property and an observation $x$ is a noisy measurement of $z$. The generative model is specified by a Gaussian prior $z \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$ and a Gaussian likelihood $x \\mid z \\sim \\mathcal{N}(z,\\sigma_x^2)$. Here, $\\mu_0$ encodes a biased prior expectation and $\\sigma_0^2$ encodes prior uncertainty, while $\\sigma_x^2$ encodes sensory uncertainty. The observer performs Bayesian inference to obtain the posterior $p(z \\mid x)$ and uses the posterior mean $\\mathbb{E}[z \\mid x]$ as the estimate of $z$.\n\nStarting only from the definition of Bayes’ rule and the standard form of the Gaussian probability density function, derive the posterior distribution $p(z \\mid x)$ and obtain its mean $\\mathbb{E}[z \\mid x]$. Define the illusion strength $S(x)$ as the deviation of the Bayesian posterior mean from the maximum likelihood estimate, that is, $S(x) = \\mathbb{E}[z \\mid x] - x$. Provide a single closed-form analytic expression for $S(x)$ in terms of $\\mu_0$, $\\sigma_0^2$, $\\sigma_x^2$, and $x$. No numerical approximation is required, and no units are to be reported. Your final answer must be a single analytic expression.",
            "solution": "We begin by applying Bayes' rule to find the posterior distribution $p(z \\mid x)$. Bayes' rule states:\n$$\np(z \\mid x) = \\frac{p(x \\mid z) p(z)}{p(x)}\n$$\nThe term $p(x)$ in the denominator is the marginal likelihood, which acts as a normalization constant and does not depend on the latent cause $z$. Therefore, we can express the posterior as being proportional to the product of the likelihood and the prior:\n$$\np(z \\mid x) \\propto p(x \\mid z) p(z)\n$$\nThe problem specifies a Gaussian prior, $z \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$, and a Gaussian likelihood, $x \\mid z \\sim \\mathcal{N}(z, \\sigma_x^2)$. Their respective probability density functions (PDFs) are:\n$$\np(z) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} \\right)\n$$\n$$\np(x \\mid z) = \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} \\exp\\left( -\\frac{(x - z)^2}{2\\sigma_x^2} \\right)\n$$\nMultiplying these two PDFs, and ignoring the constant coefficients (since we are working with proportionality), we get:\n$$\np(z \\mid x) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} \\right) \\exp\\left( -\\frac{(x - z)^2}{2\\sigma_x^2} \\right)\n$$\n$$\np(z \\mid x) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} - \\frac{(x - z)^2}{2\\sigma_x^2} \\right)\n$$\nTo identify the form of the posterior distribution, we analyze the argument of the exponential. Let this argument be denoted by $E$.\n$$\nE = -\\frac{1}{2} \\left[ \\frac{(z - \\mu_0)^2}{\\sigma_0^2} + \\frac{(z - x)^2}{\\sigma_x^2} \\right]\n$$\nWe expand the squared terms:\n$$\nE = -\\frac{1}{2} \\left[ \\frac{z^2 - 2z\\mu_0 + \\mu_0^2}{\\sigma_0^2} + \\frac{z^2 - 2zx + x^2}{\\sigma_x^2} \\right]\n$$\nNow, we group terms with respect to powers of $z$:\n$$\nE = -\\frac{1}{2} \\left[ z^2 \\left( \\frac{1}{\\sigma_0^2} + \\frac{1}{\\sigma_x^2} \\right) - 2z \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{x}{\\sigma_x^2} \\right) + \\left( \\frac{\\mu_0^2}{\\sigma_0^2} + \\frac{x^2}{\\sigma_x^2} \\right) \\right]\n$$\nThe expression for $p(z \\mid x)$ is an exponential of a quadratic function of $z$. This is the form of a Gaussian distribution. A generic Gaussian PDF for a variable $z$ with mean $\\mu_{post}$ and variance $\\sigma_{post}^2$ has the form:\n$$\np(z) \\propto \\exp\\left( -\\frac{(z - \\mu_{post})^2}{2\\sigma_{post}^2} \\right) = \\exp\\left( -\\frac{z^2 - 2z\\mu_{post} + \\mu_{post}^2}{2\\sigma_{post}^2} \\right)\n$$\n$$\np(z) \\propto \\exp\\left( -\\frac{1}{2} \\left[ z^2 \\left( \\frac{1}{\\sigma_{post}^2} \\right) - 2z \\left( \\frac{\\mu_{post}}{\\sigma_{post}^2} \\right) + \\frac{\\mu_{post}^2}{\\sigma_{post}^2} \\right] \\right)\n$$\nBy comparing the coefficients of the $z^2$ and $z$ terms between our derived expression for $E$ and the standard form, we can identify the posterior variance $\\sigma_{post}^2$ and mean $\\mu_{post}$.\nEquating the coefficients of $z^2$:\n$$\n\\frac{1}{\\sigma_{post}^2} = \\frac{1}{\\sigma_0^2} + \\frac{1}{\\sigma_x^2} = \\frac{\\sigma_x^2 + \\sigma_0^2}{\\sigma_0^2 \\sigma_x^2} \\implies \\sigma_{post}^2 = \\frac{\\sigma_0^2 \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\nEquating the coefficients of the linear $z$ term:\n$$\n\\frac{\\mu_{post}}{\\sigma_{post}^2} = \\frac{\\mu_0}{\\sigma_0^2} + \\frac{x}{\\sigma_x^2}\n$$\nNow we solve for the posterior mean, $\\mu_{post}$:\n$$\n\\mu_{post} = \\sigma_{post}^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{x}{\\sigma_x^2} \\right) = \\left( \\frac{\\sigma_0^2 \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2} \\right) \\left( \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 \\sigma_x^2} \\right)\n$$\n$$\n\\mu_{post} = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\nThus, the posterior distribution $p(z \\mid x)$ is a Gaussian with mean $\\mu_{post}$ and variance $\\sigma_{post}^2$. The observer's estimate for $z$ is the posterior mean, $\\mathbb{E}[z \\mid x]$, which is $\\mu_{post}$.\n$$\n\\mathbb{E}[z \\mid x] = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\nThe problem defines the illusion strength $S(x)$ as the deviation of the Bayesian posterior mean from the maximum likelihood estimate (MLE). The likelihood function is $p(x|z) = \\mathcal{N}(x; z, \\sigma_x^2)$. The MLE for $z$ is the value that maximizes this function, which is $z_{MLE} = x$. The problem statement correctly identifies this by defining $S(x) = \\mathbb{E}[z \\mid x] - x$.\nWe can now derive the expression for $S(x)$:\n$$\nS(x) = \\mathbb{E}[z \\mid x] - x = \\left( \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 + \\sigma_x^2} \\right) - x\n$$\nTo simplify, we put everything over a common denominator:\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2 - x(\\sigma_0^2 + \\sigma_x^2)}{\\sigma_0^2 + \\sigma_x^2}\n$$\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2 - x \\sigma_0^2 - x \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\nThe terms involving $x \\sigma_0^2$ cancel out:\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 - x \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\nFactoring out the common term $\\sigma_x^2$ in the numerator gives the final closed-form expression:\n$$\nS(x) = \\frac{(\\mu_0 - x) \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$",
            "answer": "$$\n\\boxed{\\frac{(\\mu_0 - x) \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}}\n$$"
        },
        {
            "introduction": "Building upon the single-layer model, this exercise introduces the crucial concept of hierarchy, a key organizing principle of the cerebral cortex. You will analyze a three-level generative model, which provides a simplified yet powerful analogue for hierarchical processing streams in the brain, such as the visual pathway. By deriving the posterior beliefs at each level, this practice reveals how sensory evidence at the lowest level propagates upward to refine high-level abstract representations, and how prior beliefs flow downward to shape lower-level estimates .",
            "id": "3984111",
            "problem": "Consider an analysis-by-synthesis linear-Gaussian generative model of a three-level latent hierarchy with a single scalar observation. Let the latent variables be $z_3$, $z_2$, and $z_1$ with the following generative structure:\n- The top-level latent variable has a zero-mean Gaussian prior: $z_3 \\sim \\mathcal{N}(0, \\tau_3^{-1})$, where $\\tau_3 > 0$ is the prior precision.\n- The middle-level conditional is Gaussian: $z_2 \\mid z_3 \\sim \\mathcal{N}(z_3, \\tau_2^{-1})$, where $\\tau_2 > 0$ is the conditional precision.\n- The bottom-level conditional is Gaussian: $z_1 \\mid z_2 \\sim \\mathcal{N}(z_2, \\tau_1^{-1})$, where $\\tau_1 > 0$ is the conditional precision.\n- The observation model is Gaussian: $x \\mid z_1 \\sim \\mathcal{N}(z_1, \\gamma^{-1})$, where $\\gamma > 0$ is the observation precision and $x \\in \\mathbb{R}$ is observed.\n\nUsing only the foundational facts that Bayes’ rule turns products of Gaussian factors into a Gaussian posterior and that a multivariate Gaussian can be written in canonical (information) form with a precision matrix and a shift vector, derive the joint posterior over $(z_1, z_2, z_3)$ given $x$, and from it compute the marginal posterior means $\\mu_\\ell = \\mathbb{E}[z_\\ell \\mid x]$ and marginal posterior precisions $\\tau_\\ell^{\\text{post}} = \\left(\\operatorname{Var}[z_\\ell \\mid x]\\right)^{-1}$ for $\\ell \\in \\{1, 2, 3\\}$. Express your answers as exact symbolic expressions in terms of $x$, $\\tau_1$, $\\tau_2$, $\\tau_3$, and $\\gamma$.\n\nYour final answer must be a single row matrix, in the order $\\mu_1, \\mu_2, \\mu_3, \\tau_1^{\\text{post}}, \\tau_2^{\\text{post}}, \\tau_3^{\\text{post}}$. No numerical approximation or rounding is required, and no units are involved.",
            "solution": "The problem asks for the marginal posterior means and precisions for the latent variables $z_1$, $z_2$, and $z_3$ in a three-level linear-Gaussian generative model. The model is defined by the following distributions:\n$p(z_3) = \\mathcal{N}(z_3 \\mid 0, \\tau_3^{-1})$\n$p(z_2 \\mid z_3) = \\mathcal{N}(z_2 \\mid z_3, \\tau_2^{-1})$\n$p(z_1 \\mid z_2) = \\mathcal{N}(z_1 \\mid z_2, \\tau_1^{-1})$\n$p(x \\mid z_1) = \\mathcal{N}(x \\mid z_1, \\gamma^{-1})$\n\nThe joint distribution over all variables is given by the product $p(x, z_1, z_2, z_3) = p(x \\mid z_1) p(z_1 \\mid z_2) p(z_2 \\mid z_3) p(z_3)$.\nBy Bayes' rule, the joint posterior distribution over the latent variables $\\mathbf{z} = (z_1, z_2, z_3)^T$ given the observation $x$ is proportional to the joint distribution:\n$p(\\mathbf{z} \\mid x) \\propto p(x, \\mathbf{z})$\n\nSince each factor is a Gaussian, their product is also a Gaussian (or, more precisely, its density is proportional to a Gaussian function). We will work with the log-posterior, which will be a quadratic function of the variables $z_1, z_2, z_3$. The log of a Gaussian density $\\mathcal{N}(y \\mid \\mu, \\tau^{-1})$ is, up to an additive constant, $-\\frac{\\tau}{2}(y-\\mu)^2$.\n\nThe log-posterior is proportional to the sum of the logs of the individual probability densities:\n$$ \\ln p(z_1, z_2, z_3 \\mid x) = -\\frac{\\gamma}{2}(z_1 - x)^2 - \\frac{\\tau_1}{2}(z_1 - z_2)^2 - \\frac{\\tau_2}{2}(z_2 - z_3)^2 - \\frac{\\tau_3}{2}(z_3 - 0)^2 + C $$\nwhere $C$ is a constant that does not depend on $z_1, z_2, z_3$.\n\nA multivariate Gaussian posterior $p(\\mathbf{z} \\mid x) = \\mathcal{N}(\\mathbf{z} \\mid \\boldsymbol{\\mu}_{\\text{post}}, \\mathbf{\\Lambda}_{\\text{post}}^{-1})$ can be written in its canonical or information form, where its log-density is:\n$$ \\ln p(\\mathbf{z} \\mid x) = -\\frac{1}{2} \\mathbf{z}^T \\mathbf{\\Lambda}_{\\text{post}} \\mathbf{z} + \\mathbf{h}_{\\text{post}}^T \\mathbf{z} + C' $$\nHere, $\\mathbf{\\Lambda}_{\\text{post}}$ is the posterior precision matrix, and $\\mathbf{h}_{\\text{post}}$ is the potential or shift vector. The posterior mean and covariance are given by $\\boldsymbol{\\mu}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1} \\mathbf{h}_{\\text{post}}$ and $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1}$, respectively.\n\nTo find $\\mathbf{\\Lambda}_{\\text{post}}$ and $\\mathbf{h}_{\\text{post}}$, we expand the expression for the log-posterior and collect terms that are quadratic and linear in $\\mathbf{z}$:\n$$ \\ln p(\\mathbf{z} \\mid x) \\propto -\\frac{1}{2} \\left[ \\gamma(z_1^2 - 2z_1x + x^2) + \\tau_1(z_1^2 - 2z_1z_2 + z_2^2) + \\tau_2(z_2^2 - 2z_2z_3 + z_3^2) + \\tau_3z_3^2 \\right] $$\nGrouping terms by $z_i z_j$ and $z_i$:\n$$ \\ln p(\\mathbf{z} \\mid x) \\propto -\\frac{1}{2} \\left[ (\\gamma + \\tau_1)z_1^2 + (\\tau_1 + \\tau_2)z_2^2 + (\\tau_2 + \\tau_3)z_3^2 - 2\\tau_1z_1z_2 - 2\\tau_2z_2z_3 \\right] + \\gamma x z_1 $$\nComparing this to the canonical form $-\\frac{1}{2}\\mathbf{z}^T \\mathbf{\\Lambda}_{\\text{post}} \\mathbf{z} + \\mathbf{h}_{\\text{post}}^T\\mathbf{z}$, we can identify the posterior precision matrix $\\mathbf{\\Lambda}_{\\text{post}}$ and the shift vector $\\mathbf{h}_{\\text{post}}$:\n$$ \\mathbf{\\Lambda}_{\\text{post}} = \\begin{pmatrix} \\gamma + \\tau_1 & -\\tau_1 & 0 \\\\ -\\tau_1 & \\tau_1 + \\tau_2 & -\\tau_2 \\\\ 0 & -\\tau_2 & \\tau_2 + \\tau_3 \\end{pmatrix} $$\n$$ \\mathbf{h}_{\\text{post}} = \\begin{pmatrix} \\gamma x \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe posterior mean vector is $\\boldsymbol{\\mu}_{\\text{post}} = (\\mu_1, \\mu_2, \\mu_3)^T = \\mathbf{\\Lambda}_{\\text{post}}^{-1} \\mathbf{h}_{\\text{post}}$. The posterior covariance matrix is $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1}$. The marginal posterior variances are the diagonal elements of this matrix, $\\operatorname{Var}[z_\\ell \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{\\ell\\ell}$. The marginal posterior precisions are $\\tau_\\ell^{\\text{post}} = \\left((\\mathbf{\\Sigma}_{\\text{post}})_{\\ell\\ell}\\right)^{-1}$.\n\nTo compute the inverse of $\\mathbf{\\Lambda}_{\\text{post}}$, we first find its determinant, which we denote as $D$:\n$$ D = \\det(\\mathbf{\\Lambda}_{\\text{post}}) = (\\gamma + \\tau_1) \\left( (\\tau_1 + \\tau_2)(\\tau_2 + \\tau_3) - (-\\tau_2)^2 \\right) - (-\\tau_1) \\left( (-\\tau_1)(\\tau_2 + \\tau_3) - 0 \\right) $$\n$$ D = (\\gamma + \\tau_1)(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) - \\tau_1^2(\\tau_2 + \\tau_3) $$\n$$ D = \\gamma(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) + \\tau_1(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) - \\tau_1^2\\tau_2 - \\tau_1^2\\tau_3 $$\n$$ D = \\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3 $$\n\nThe inverse is $\\mathbf{\\Lambda}_{\\text{post}}^{-1} = \\frac{1}{D} \\operatorname{adj}(\\mathbf{\\Lambda}_{\\text{post}})$, where $\\operatorname{adj}(\\mathbf{\\Lambda}_{\\text{post}})$ is the adjugate matrix. Since $\\mathbf{\\Lambda}_{\\text{post}}$ is symmetric, its adjugate is the matrix of cofactors. Let's compute the necessary cofactors $C_{ij}$:\n$C_{11} = (\\tau_1+\\tau_2)(\\tau_2+\\tau_3) - \\tau_2^2 = \\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3$\n$C_{21} = -(-\\tau_1(\\tau_2+\\tau_3) - 0) = \\tau_1(\\tau_2+\\tau_3)$\n$C_{31} = (-\\tau_1)(-\\tau_2) - 0 = \\tau_1\\tau_2$\n$C_{22} = (\\gamma+\\tau_1)(\\tau_2+\\tau_3)$\n$C_{33} = (\\gamma+\\tau_1)(\\tau_1+\\tau_2) - (-\\tau_1)^2 = \\gamma\\tau_1 + \\gamma\\tau_2 + \\tau_1\\tau_2$\n\nNow we compute the posterior means $\\boldsymbol{\\mu}_{\\text{post}} = \\frac{1}{D} \\operatorname{adj}(\\mathbf{\\Lambda}_{\\text{post}}) \\mathbf{h}_{\\text{post}}$:\n$$ \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{pmatrix} = \\frac{1}{D} \\begin{pmatrix} C_{11} & C_{21} & C_{31} \\\\ C_{12} & C_{22} & C_{32} \\\\ C_{13} & C_{23} & C_{33} \\end{pmatrix} \\begin{pmatrix} \\gamma x \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{\\gamma x}{D} \\begin{pmatrix} C_{11} \\\\ C_{21} \\\\ C_{31} \\end{pmatrix} $$\nSubstituting the cofactors and $D$:\n$$ \\mu_1 = \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) x}{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3} $$\n$$ \\mu_2 = \\frac{\\gamma \\tau_1 (\\tau_2 + \\tau_3) x}{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3} $$\n$$ \\mu_3 = \\frac{\\gamma \\tau_1 \\tau_2 x}{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3} $$\nFor consistency in the final answer, we can reorder the terms in the numerator of $\\mu_1$ to match the order in the denominator: $\\mu_1 = \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}$.\n\nNext, we find the marginal posterior precisions. The marginal variances are the diagonal elements of $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1}$:\n$$ \\operatorname{Var}[z_1 \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{11} = \\frac{C_{11}}{D} = \\frac{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3}{D} $$\n$$ \\operatorname{Var}[z_2 \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{22} = \\frac{C_{22}}{D} = \\frac{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)}{D} $$\n$$ \\operatorname{Var}[z_3 \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{33} = \\frac{C_{33}}{D} = \\frac{\\gamma\\tau_1 + \\gamma\\tau_2 + \\tau_1\\tau_2}{D} $$\n\nThe marginal posterior precisions are the reciprocals of these variances:\n$$ \\tau_1^{\\text{post}} = \\frac{D}{C_{11}} = \\frac{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3} = \\gamma + \\frac{\\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3} $$\nThe latter simplified form is more elegant and revealing, confirming that the posterior precision is the sum of the likelihood precision ($\\gamma$) and the marginal prior precision of $z_1$.\n$$ \\tau_2^{\\text{post}} = \\frac{D}{C_{22}} = \\frac{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3}{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)} $$\n$$ \\tau_3^{\\text{post}} = \\frac{D}{C_{33}} = \\frac{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3}{\\gamma\\tau_1 + \\gamma\\tau_2 + \\tau_1\\tau_2} $$\nThese expressions cannot be simplified as neatly as $\\tau_1^{\\text{post}}$ and so will be presented in their fractional form. We substitute the expression for $D$ and reorder as before for consistency. For example, $\\tau_2^{\\text{post}} = \\frac{\\gamma(\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)}$.\n\nThe final results are collated into a single row matrix as requested.",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\frac{\\gamma \\tau_1 (\\tau_2 + \\tau_3) x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\frac{\\gamma \\tau_1 \\tau_2 x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\gamma + \\frac{\\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3} & \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)} & \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}{\\gamma(\\tau_1 + \\tau_2) + \\tau_1\\tau_2} \\end{pmatrix}} $$"
        },
        {
            "introduction": "While the previous exercises focused on deriving the static, optimal solution to an inference problem, this final practice shifts our focus to the *process* of inference itself. You will implement a simple form of predictive coding, a highly influential neuro-computational theory suggesting the brain performs analysis-by-synthesis via iterative error correction. This computational experiment will allow you to explore the dynamics of perception and see firsthand how the brain might leverage the precision of sensory signals to modulate the speed and gain of its inferential process .",
            "id": "3984145",
            "problem": "You are tasked with designing and executing a computational experiment, in the setting of generative models and analysis-by-synthesis, to demonstrate how increasing the precision of sensory noise, denoted by $\\Pi_x$, modulates the gain of error units and the speed of convergence in predictive coding. Consider a linear Gaussian generative model where a latent state vector $z \\in \\mathbb{R}^2$ gives rise to an observation vector $x \\in \\mathbb{R}^3$ via a synthesis mapping $C \\in \\mathbb{R}^{3 \\times 2}$ and additive sensory noise. The generative model is defined by\n$$\nx = C z + \\varepsilon_x,\n$$\nwhere the sensory noise is drawn from a Gaussian distribution with zero mean and covariance matrix $\\Sigma_x \\in \\mathbb{R}^{3 \\times 3}$ and precision $\\Pi_x = \\Sigma_x^{-1}$. The prior over $z$ is Gaussian with mean $\\mu_z$ and precision $\\Pi_z$.\n\nIn predictive coding, the latent $z$ is inferred by minimizing a synthesis-analysis consistency objective that penalizes prediction error and deviation from the prior. A suitable objective for a single observation $x$ is\n$$\nJ(z) = \\frac{1}{2}\\,\\Pi_x\\,\\|x - C z\\|_2^2 + \\frac{1}{2}\\,\\Pi_z\\,\\|z - \\mu_z\\|_2^2.\n$$\nYou must derive the gradient descent update rule for $z$ that minimizes $J(z)$ and implement the resultant iterative predictive coding dynamics. Let the error unit be defined by $e_x(t) = x - C z(t)$ for iteration index $t \\in \\{0,1,2,\\ldots\\}$. The gain applied to the error unit is embodied by the precision-weighted error $\\tilde{e}_x(t) = \\Pi_x\\, e_x(t)$. Quantify the instantaneous gain at initialization by the ratio\n$$\nG(\\Pi_x) = \\frac{\\|\\tilde{e}_x(0)\\|_2}{\\|e_x(0)\\|_2}.\n$$\nQuantify the speed of convergence by the number of iterations required for the Euclidean norm of the gradient $\\|\\nabla J(z(t))\\|_2$ to fall below a specified tolerance $\\varepsilon$, subject to a maximum iteration budget $T_{\\max}$.\n\nImplement the following experiment parameters, which are scientifically plausible and self-consistent, and use them identically across all test cases:\n- Synthesis matrix $C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$.\n- Prior mean $\\mu_z = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ and prior precision $\\Pi_z = 1$.\n- Ground-truth latent $z_{\\text{true}} = \\begin{bmatrix} 2.0 \\\\ -1.0 \\end{bmatrix}$, deterministic sensory noise vector $\\varepsilon_x = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$, and observation $x = C z_{\\text{true}} + \\varepsilon_x$.\n- Initial latent $z(0) = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n- Fixed learning rate $\\eta = 0.005$, tolerance $\\varepsilon = 10^{-6}$, and maximum iterations $T_{\\max} = 10000$.\n\nStarting from the definitions above, derive the gradient descent update rule for $z$ that decreases $J(z)$, implement the dynamics, and run the experiment over the following test suite of sensory precisions $\\Pi_x$:\n1. $\\Pi_x = 0.01$ (very low precision, near-uninformative sensory likelihood).\n2. $\\Pi_x = 0.1$ (low precision).\n3. $\\Pi_x = 1.0$ (moderate precision).\n4. $\\Pi_x = 10.0$ (high precision).\n5. $\\Pi_x = 100.0$ (very high precision).\n\nFor each test case, compute and return the following three quantities:\n- The gain $G(\\Pi_x)$ as defined above, a float.\n- The number of iterations $N(\\Pi_x)$ required until $\\|\\nabla J(z(t))\\|_2 \\le \\varepsilon$, an integer; if convergence does not occur within $T_{\\max}$ iterations, set $N(\\Pi_x) = T_{\\max}$.\n- A boolean $B(\\Pi_x)$ indicating whether convergence was achieved, where $B(\\Pi_x) = \\text{True}$ if $\\|\\nabla J(z(t))\\|_2 \\le \\varepsilon$ at termination and $\\text{False}$ otherwise.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a comma-separated list in square brackets of the form $[G(\\Pi_x),N(\\Pi_x),B(\\Pi_x)]$. For example, for three cases your output would look like\n$$\n\\big[ [G_1,N_1,B_1],[G_2,N_2,B_2],[G_3,N_3,B_3] \\big],\n$$\nbut with the values corresponding to the five specified test precisions. No physical units or angles are involved; all quantities are dimensionless.",
            "solution": "The core of the problem is to infer a latent state vector $z \\in \\mathbb{R}^2$ from an observation $x \\in \\mathbb{R}^3$ by minimizing an objective function $J(z)$. This function represents the negative log-posterior probability of $z$ given $x$, also known as a free energy functional in the context of predictive coding. The objective function is given by:\n$$\nJ(z) = \\frac{1}{2}\\,\\Pi_x\\,\\|x - C z\\|_2^2 + \\frac{1}{2}\\,\\Pi_z\\,\\|z - \\mu_z\\|_2^2\n$$\nHere, the first term is the negative log-likelihood, penalizing the prediction error $x - C z$ weighted by the scalar sensory precision $\\Pi_x$. The second term is the negative log-prior, penalizing the deviation of the latent state $z$ from its prior mean $\\mu_z$, weighted by the scalar prior precision $\\Pi_z$. The model assumes isotropic Gaussian noise, hence the use of scalar precisions.\n\nTo implement the predictive coding dynamics, we use gradient descent to find the value of $z$ that minimizes $J(z)$. The update rule is $z(t+1) = z(t) - \\eta \\nabla J(z(t))$, where $\\eta$ is the learning rate. We must first derive the gradient of $J(z)$ with respect to $z$.\n\nLet's expand the terms in $J(z)$ using vector transpose notation:\n$$\nJ(z) = \\frac{1}{2}\\Pi_x (x - Cz)^T(x - Cz) + \\frac{1}{2}\\Pi_z(z - \\mu_z)^T(z - \\mu_z)\n$$\nWe find the gradient of each term separately. For the first term, we use the chain rule for vector calculus, noting that $\\nabla_v(v^T v) = 2v$.\n$$\n\\nabla_z \\left(\\frac{1}{2}\\Pi_x (x - Cz)^T(x - Cz)\\right) = \\frac{1}{2}\\Pi_x \\left( 2 C^T(Cz - x) \\right) = \\Pi_x C^T(Cz - x)\n$$\nFor the second term:\n$$\n\\nabla_z \\left(\\frac{1}{2}\\Pi_z(z - \\mu_z)^T(z - \\mu_z)\\right) = \\frac{1}{2}\\Pi_z \\left( 2 (z - \\mu_z) \\right) = \\Pi_z(z - \\mu_z)\n$$\nCombining these, the total gradient is:\n$$\n\\nabla J(z) = \\Pi_x C^T(Cz - x) + \\Pi_z(z - \\mu_z)\n$$\nThe problem defines the prediction error as $e_x(t) = x - C z(t)$. Substituting this into the gradient expression, we get:\n$$\n\\nabla J(z(t)) = -\\Pi_x C^T e_x(t) + \\Pi_z(z(t) - \\mu_z)\n$$\nThe gradient descent update rule for the latent state $z$ is therefore:\n$$\nz(t+1) = z(t) - \\eta \\left( \\Pi_x C^T(C z(t) - x) + \\Pi_z(z(t) - \\mu_z) \\right)\n$$\nThis iterative rule describes the dynamics of the inference process, where the latent state is updated to reduce the objective function $J(z)$.\n\nNext, we calculate the instantaneous gain at initialization, $G(\\Pi_x)$. It is defined as the ratio of the norms of the precision-weighted error to the raw error at time $t=0$:\n$$\nG(\\Pi_x) = \\frac{\\|\\tilde{e}_x(0)\\|_2}{\\|e_x(0)\\|_2} = \\frac{\\|\\Pi_x\\, e_x(0)\\|_2}{\\|e_x(0)\\|_2}\n$$\nSince $\\Pi_x$ is a scalar, we can factor it out of the norm:\n$$\nG(\\Pi_x) = \\frac{|\\Pi_x| \\|e_x(0)\\|_2}{\\|e_x(0)\\|_2} = |\\Pi_x|\n$$\nGiven that all specified test values for $\\Pi_x$ are positive, the gain is simply $G(\\Pi_x) = \\Pi_x$. This demonstrates that the sensory precision directly scales the contribution of the sensory prediction error to the system's dynamics.\n\nThe computational experiment proceeds as follows. First, we compute the constant observation vector $x = C z_{\\text{true}} + \\varepsilon_x$ using the provided parameters. Then, for each value of $\\Pi_x$ in the test suite, we perform the following steps:\n1. Calculate the gain $G(\\Pi_x) = \\Pi_x$.\n2. Initialize the latent state $z(0) = \\begin{bmatrix} 0.0, 0.0 \\end{bmatrix}^T$.\n3. Iterate for a number of steps $N$ from $0$ to $T_{\\max}$:\n    a. Compute the gradient $\\nabla J(z(N))$ using the current state $z(N)$.\n    b. Compute the Euclidean norm of the gradient, $\\|\\nabla J(z(N))\\|_2$.\n    c. If the norm is less than or equal to the tolerance $\\varepsilon=10^{-6}$, the process has converged. We record the number of iterations $N(\\Pi_x) = N$ and a convergence flag $B(\\Pi_x) = \\text{True}$, then terminate the loop for the current $\\Pi_x$.\n    d. If the convergence criterion is not met and $N < T_{\\max}$, update the state to $z(N+1)$ using the gradient descent rule.\n4. If the loop completes without meeting the tolerance (i.e., $N=T_{\\max}$), we set $N(\\Pi_x) = T_{\\max}$ and $B(\\Pi_x) = \\text{False}$.\n\nThis procedure is repeated for all specified values of $\\Pi_x$, and the results—$[G(\\Pi_x), N(\\Pi_x), B(\\Pi_x)]$-—are collected for each case. The results are expected to show that as $\\Pi_x$ increases, the number of iterations $N(\\Pi_x)$ required for convergence decreases, illustrating a fundamental principle of predictive coding: higher sensory precision accelerates inference by assigning greater weight to prediction errors, leading to more rapid correction of the internal model.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and executes a computational experiment on a predictive coding model.\n    \"\"\"\n    # 1. Define fixed parameters for the experiment\n    C = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n    mu_z = np.array([0.0, 0.0])\n    Pi_z = 1.0\n    z_true = np.array([2.0, -1.0])\n    eps_x_vec = np.array([0.1, -0.2, 0.05])\n    z_initial = np.array([0.0, 0.0])\n    \n    eta = 0.005  # Learning rate\n    tolerance = 1e-6\n    T_max = 10000\n\n    # Test suite for sensory precision\n    test_precisions = [0.01, 0.1, 1.0, 10.0, 100.0]\n\n    # Pre-compute constant values\n    x = C @ z_true + eps_x_vec\n    C_T = C.T\n\n    # Store results for all test cases\n    all_results = []\n\n    # 2. Iterate through each test case\n    for Pi_x in test_precisions:\n        # Calculate gain G(Pi_x)\n        # G = ||Pi_x * e_x(0)|| / ||e_x(0)|| = |Pi_x| * ||e_x(0)|| / ||e_x(0)||\n        # Since Pi_x is positive, G = Pi_x.\n        gain = float(Pi_x)\n\n        # Initialize variables for the iterative process\n        z = z_initial.copy()\n        converged = False\n        num_iterations = 0\n\n        # 3. Run predictive coding dynamics (gradient descent)\n        for i in range(T_max + 1):\n            # Calculate gradient: grad_J = Pi_x * C^T * (C*z - x) + Pi_z * (z - mu_z)\n            # With mu_z = 0, this is Pi_x * C^T * (C*z - x) + Pi_z * z\n            pred_error = C @ z - x\n            grad_likelihood = Pi_x * (C_T @ pred_error)\n            grad_prior = Pi_z * z\n            gradient = grad_likelihood + grad_prior\n            \n            grad_norm = np.linalg.norm(gradient)\n\n            # Check for convergence\n            if grad_norm = tolerance:\n                num_iterations = i\n                converged = True\n                break\n            \n            # If we are at the last possible iteration and haven't converged\n            if i == T_max:\n                num_iterations = T_max\n                converged = (grad_norm = tolerance) # Final check for rare case\n                break\n\n            # Update latent state z\n            z -= eta * gradient\n        \n        # Store results for this test case\n        all_results.append([gain, num_iterations, converged])\n\n    # 4. Format and print the final output\n    result_str_parts = []\n    for res in all_results:\n        # Ensure True/False are capitalized as per Python bool standard\n        res_str = f\"[{res[0]},{res[1]},{res[2]}]\"\n        result_str_parts.append(res_str)\n    \n    final_output = f\"[{','.join(result_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}