{
    "hands_on_practices": [
        {
            "introduction": "要掌握隐马尔可夫模型（HMM），我们首先要从其核心定义入手。给定一组观测序列，其边际似然是所有可能产生该序列的隐藏状态路径的概率之和。通过这个练习 ()，您将为一个简单模型手动计算这个值，从而巩固对 HMM 联合概率和边际化过程的基本理解，并体会为何对于真实长度的序列，我们需要更高效的算法。",
            "id": "3987961",
            "problem": "考虑一个用于从标量神经特征时间序列中识别潜在神经状态的两状态隐马尔可夫模型 (HMM)。设时间 $t$ 的潜在状态为 $z_t \\in \\{1,2\\}$，初始分布为 $\\pi = [0.5, 0.5]$，转移矩阵为\n$$\nA=\\begin{pmatrix}\n0.9  0.1 \\\\\n0.2  0.8\n\\end{pmatrix}.\n$$\n观测模型是高斯分布：对于每个时间 $t$，给定潜在状态，观测到的标量 $x_t$ 是条件独立的，其分布为\n$$\nx_t \\mid z_t=k \\sim \\mathcal{N}(\\mu_k, \\sigma^2),\n$$\n其中 $\\mu_1 = 0$，$\\mu_2 = 3$，且 $\\sigma = 1$。仅使用隐马尔可夫模型 (HMM) 的核心定义——即潜在链的马尔可夫性质、给定潜在状态时观测值的条件独立性以及高斯发射密度——从第一性原理为长度为 3 的观测序列\n$$\nx_{1:3} = [0,\\, 3,\\, 3],\n$$\n推导边际对数似然 $\\log p(x_{1:3})$，方法是在以下定义中对所有潜在序列 $z_{1:3} \\in \\{1,2\\}^3$ 进行显式求和\n$$\np(x_{1:3}) \\;=\\; \\sum_{z_{1:3} \\in \\{1,2\\}^3} p(z_1)\\, p(x_1 \\mid z_1)\\, p(z_2 \\mid z_1)\\, p(x_2 \\mid z_2)\\, p(z_3 \\mid z_2)\\, p(x_3 \\mid z_3).\n$$\n将最终答案表示为使用 $\\ln(\\cdot)$ 和 $\\exp(\\cdot)$ 的单个闭式解析表达式，并明确所有常数。请勿近似或四舍五入；提供精确的解析表达式。最终答案应是以奈特 (nats) 为单位的纯数。",
            "solution": "### 步骤 1：提取已知条件\n- **潜在状态**：$z_t \\in \\{1, 2\\}$。\n- **初始分布**：$\\pi = [\\pi_1, \\pi_2] = [0.5, 0.5]$。这意味着 $p(z_1=1) = 0.5$ 且 $p(z_1=2) = 0.5$。\n- **转移矩阵**：从状态 $i$ 转移到状态 $j$ 的概率由 $A_{ij} = p(z_t=j \\mid z_{t-1}=i)$ 给出。\n$$\nA=\\begin{pmatrix}\nA_{11}  A_{12} \\\\\nA_{21}  A_{22}\n\\end{pmatrix} = \\begin{pmatrix}\n0.9  0.1 \\\\\n0.2  0.8\n\\end{pmatrix}\n$$\n- **观测模型**：给定潜在状态 $z_t=k$ 时观测到 $x_t$ 的概率是一个高斯分布，$p(x_t \\mid z_t=k) = \\mathcal{N}(x_t; \\mu_k, \\sigma^2)$。\n- **发射参数**：状态 1 的均值为 $\\mu_1 = 0$。状态 2 的均值为 $\\mu_2 = 3$。两个状态的标准差均为 $\\sigma = 1$。\n- **观测序列**：长度为 3 的序列：$x_{1:3} = [x_1, x_2, x_3] = [0, 3, 3]$。\n- **任务**：通过对所有 $2^3=8$ 种可能的潜在序列 $z_{1:3} \\in \\{1,2\\}^3$ 进行显式求和，计算边际对数似然 $\\ln p(x_{1:3})$。使用的公式是：\n$$\np(x_{1:3}) = \\sum_{z_{1:3} \\in \\{1,2\\}^3} p(z_1) p(x_1 \\mid z_1) p(z_2 \\mid z_1) p(x_2 \\mid z_2) p(z_3 \\mid z_2) p(x_3 \\mid z_3)\n$$\n\n### 边际对数似然的推导\n\n边际似然 $p(x_{1:3})$ 是对所有可能的潜在状态序列 $z_{1:3}$ 的联合概率 $p(x_{1:3}, z_{1:3})$ 的总和。特定序列 $z_{1:3} = (z_1, z_2, z_3)$ 的联合概率由初始概率、转移概率和发射概率的乘积给出：\n$$\np(x_{1:3}, z_{1:3}) = p(z_1) \\cdot A_{z_1 z_2} \\cdot A_{z_2 z_3} \\cdot p(x_1 \\mid z_1) \\cdot p(x_2 \\mid z_2) \\cdot p(x_3 \\mid z_3)\n$$\n\n首先，我们为每个状态定义高斯发射概率密度函数：\n对于状态 $z_t=1$：\n$$\np(x_t \\mid z_t=1) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_t - \\mu_1)^2}{2\\sigma^2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_t^2}{2}\\right)\n$$\n对于状态 $z_t=2$：\n$$\np(x_t \\mid z_t=2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_t - \\mu_2)^2}{2\\sigma^2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_t - 3)^2}{2}\\right)\n$$\n\n接下来，我们为给定的观测序列 $x_{1:3} = [0, 3, 3]$ 计算这些发射概率：\n- 对于 $x_1=0$：\n  - $p(x_1=0 \\mid z_1=1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{0^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}}$\n  - $p(x_1=0 \\mid z_1=2) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(0-3)^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{9}{2}\\right)$\n- 对于 $x_2=3$：\n  - $p(x_2=3 \\mid z_2=1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{3^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{9}{2}\\right)$\n  - $p(x_2=3 \\mid z_2=2) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(3-3)^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}}$\n- 对于 $x_3=3$：\n  - $p(x_3=3 \\mid z_3=1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{3^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{9}{2}\\right)$\n  - $p(x_3=3 \\mid z_3=2) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(3-3)^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}}$\n\n求和的每一项都会出现一个公因式 $(\\frac{1}{\\sqrt{2\\pi}})^3 = (2\\pi)^{-3/2}$。\n\n现在我们为 8 个可能的潜在序列 $z_{1:3}$ 中的每一个计算联合概率 $p(x_{1:3}, z_{1:3})$：\n\n1.  $z_{1:3} = (1, 1, 1)$:\n    $p_{111} = \\pi_1 A_{11} A_{11} \\cdot p(x_1|1) p(x_2|1) p(x_3|1) = (0.5)(0.9)(0.9) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) = (2\\pi)^{-3/2} (0.405) \\exp(-9)$\n2.  $z_{1:3} = (1, 1, 2)$:\n    $p_{112} = \\pi_1 A_{11} A_{12} \\cdot p(x_1|1) p(x_2|1) p(x_3|2) = (0.5)(0.9)(0.1) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}} = (2\\pi)^{-3/2} (0.045) \\exp(-\\frac{9}{2})$\n3.  $z_{1:3} = (1, 2, 1)$:\n    $p_{121} = \\pi_1 A_{12} A_{21} \\cdot p(x_1|1) p(x_2|2) p(x_3|1) = (0.5)(0.1)(0.2) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) = (2\\pi)^{-3/2} (0.01) \\exp(-\\frac{9}{2})$\n4.  $z_{1:3} = (1, 2, 2)$:\n    $p_{122} = \\pi_1 A_{12} A_{22} \\cdot p(x_1|1) p(x_2|2) p(x_3|2) = (0.5)(0.1)(0.8) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}} = (2\\pi)^{-3/2} (0.04)$\n5.  $z_{1:3} = (2, 1, 1)$:\n    $p_{211} = \\pi_2 A_{21} A_{11} \\cdot p(x_1|2) p(x_2|1) p(x_3|1) = (0.5)(0.2)(0.9) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) = (2\\pi)^{-3/2} (0.09) \\exp(-\\frac{27}{2})$\n6.  $z_{1:3} = (2, 1, 2)$:\n    $p_{212} = \\pi_2 A_{21} A_{12} \\cdot p(x_1|2) p(x_2|1) p(x_3|2) = (0.5)(0.2)(0.1) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}} = (2\\pi)^{-3/2} (0.01) \\exp(-9)$\n7.  $z_{1:3} = (2, 2, 1)$:\n    $p_{221} = \\pi_2 A_{22} A_{21} \\cdot p(x_1|2) p(x_2|2) p(x_3|1) = (0.5)(0.8)(0.2) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) = (2\\pi)^{-3/2} (0.08) \\exp(-9)$\n8.  $z_{1:3} = (2, 2, 2)$:\n    $p_{222} = \\pi_2 A_{22} A_{22} \\cdot p(x_1|2) p(x_2|2) p(x_3|2) = (0.5)(0.8)(0.8) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}} = (2\\pi)^{-3/2} (0.32) \\exp(-\\frac{9}{2})$\n\n边际似然 $p(x_{1:3})$ 是这 8 个概率的总和：\n$$\np(x_{1:3}) = \\sum_{z_1,z_2,z_3} p_{z_1 z_2 z_3}\n$$\n提取公因式 $(2\\pi)^{-3/2}$：\n$$\np(x_{1:3}) = (2\\pi)^{-3/2} \\left[ (0.405)\\exp(-9) + (0.045)\\exp(-\\frac{9}{2}) + (0.01)\\exp(-\\frac{9}{2}) + 0.04 + (0.09)\\exp(-\\frac{27}{2}) + (0.01)\\exp(-9) + (0.08)\\exp(-9) + (0.32)\\exp(-\\frac{9}{2}) \\right]\n$$\n按指数因子对各项进行分组：\n$$\np(x_{1:3}) = (2\\pi)^{-3/2} \\left[ 0.04 + (0.045+0.01+0.32)\\exp(-\\frac{9}{2}) + (0.405+0.01+0.08)\\exp(-9) + (0.09)\\exp(-\\frac{27}{2}) \\right]\n$$\n$$\np(x_{1:3}) = (2\\pi)^{-3/2} \\left[ 0.04 + 0.375\\exp(-\\frac{9}{2}) + 0.495\\exp(-9) + 0.09\\exp(-\\frac{27}{2}) \\right]\n$$\n最后，我们通过取 $p(x_{1:3})$ 的自然对数来计算边际对数似然：\n$$\n\\ln p(x_{1:3}) = \\ln\\left( (2\\pi)^{-3/2} \\left[ 0.04 + 0.375\\exp(-\\frac{9}{2}) + 0.495\\exp(-9) + 0.09\\exp(-\\frac{27}{2}) \\right] \\right)\n$$\n使用对数性质 $\\ln(ab) = \\ln(a) + \\ln(b)$：\n$$\n\\ln p(x_{1:3}) = \\ln((2\\pi)^{-3/2}) + \\ln\\left( 0.04 + 0.375\\exp(-\\frac{9}{2}) + 0.495\\exp(-9) + 0.09\\exp(-\\frac{27}{2}) \\right)\n$$\n$$\n\\ln p(x_{1:3}) = -\\frac{3}{2}\\ln(2\\pi) + \\ln\\left( 0.04 + 0.375\\exp(-\\frac{9}{2}) + 0.495\\exp(-9) + 0.09\\exp(-\\frac{27}{2}) \\right)\n$$\n这就是边际对数似然的最终闭式解析表达式。",
            "answer": "$$\n\\boxed{-\\frac{3}{2}\\ln(2\\pi) + \\ln\\left(0.04 + 0.375\\exp\\left(-\\frac{9}{2}\\right) + 0.495\\exp(-9) + 0.09\\exp\\left(-\\frac{27}{2}\\right)\\right)}\n$$"
        },
        {
            "introduction": "在理解了 HMM 的基本概率结构后，下一步是从数据中学习模型的参数（例如 $\\pi$, $A$ 和发射率 $\\lambda$）。期望最大化（EM）算法（在 HMM 的情境下也称为 Baum-Welch 算法）为此提供了一个强大的迭代框架。这个编码练习 () 将指导您实现 EM 算法的一个完整循环，包括使用前向-后向算法执行 E-步（计算后验概率）和 M-步（更新参数），并验证 EM 算法保证数据似然单调递增的核心特性。",
            "id": "3987936",
            "problem": "您的任务是实现一次针对具有泊松发射（Poisson emissions）的双状态隐马尔可夫模型（HMM）的期望最大化（EM）迭代，以便从脉冲计数数据中识别潜在的神经状态。核心目标是通过前向-后向算法计算后验状态边缘概率 $\\gamma_t(k)$ 和成对后验概率 $\\xi_t(i,j)$，更新转移矩阵 $A$ 和泊松率 $\\lambda_k$，并验证观测数据的对数似然 $\\log p(x_{1:T} \\mid \\theta)$ 在 EM 更新后严格增加。\n\n从基本概率定义开始。一个隐马尔可夫模型（HMM）包含：\n- 一个覆盖 $K$ 个离散状态的初始状态分布 $\\pi$。\n- 一个状态转移矩阵 $A$，其元素为 $A_{ij} = p(z_{t+1}=j \\mid z_t=i)$，其中各行之和为 $1$。\n- 一个发射模型 $p(x_t \\mid z_t)$，描述了在潜在状态 $z_t$ 条件下观测数据 $x_t$ 的概率。\n\n对于本问题，发射模型是泊松分布：对于脉冲计数 $x_t \\in \\{0,1,2,\\dots\\}$ 和状态 $k \\in \\{1,2\\}$，似然函数为\n$$\np(x_t \\mid z_t = k, \\lambda_k) = \\frac{\\lambda_k^{x_t} e^{-\\lambda_k}}{x_t!}.\n$$\n令 $x_{1:T} \\equiv (x_1, x_2, \\dots, x_T)$ 和 $z_{1:T} \\equiv (z_1, z_2, \\dots, z_T)$ 分别表示观测到的计数和潜在状态。在参数 $\\theta \\equiv (\\pi, A, \\lambda)$ 下的联合分布为\n$$\np(x_{1:T}, z_{1:T} \\mid \\theta) = \\pi_{z_1} \\left( \\prod_{t=1}^{T-1} A_{z_t, z_{t+1}} \\right) \\left( \\prod_{t=1}^{T} p(x_t \\mid z_t, \\lambda_{z_t}) \\right).\n$$\n\n需要实现和验证的任务：\n1. 对于每个测试用例，根据给定的真实参数 $(\\pi^{\\text{true}}, A^{\\text{true}}, \\lambda^{\\text{true}})$ 从一个双状态 HMM 中生成一个合成的脉冲计数序列 $x_{1:T}$，使用指定的随机种子以确保可复现性。使用上述联合分布所隐含的生成过程（采样 $z_1 \\sim \\pi^{\\text{true}}$，然后递归采样 $z_{t+1} \\sim A^{\\text{true}}_{z_t, \\cdot}$，并对每个 $t$ 采样 $x_t \\sim \\text{Poisson}(\\lambda^{\\text{true}}_{z_t})$）。\n2. 给定初始参数 $\\theta^{(0)} \\equiv (\\pi^{(0)}, A^{(0)}, \\lambda^{(0)})$，从第一性原理推导前向-后向算法，用于计算后验状态边缘概率 $\\gamma_t(k) \\equiv p(z_t=k \\mid x_{1:T}, \\theta^{(0)})$ 和成对后验概率 $\\xi_t(i,j) \\equiv p(z_t=i, z_{t+1}=j \\mid x_{1:T}, \\theta^{(0)})$，其中 $t \\in \\{1,\\dots,T-1\\}$。\n3. 从 $\\theta^{(0)}$ 下的期望完全数据对数似然出发，推导最大化步骤（M-step）中用于更新转移矩阵 $A$ 和泊松率 $\\lambda_k$ 的更新规则，其中使用从 $\\gamma_t(k)$ 和 $\\xi_t(i,j)$ 获得的充分统计量，同时保持初始分布 $\\pi$ 固定为 $\\pi^{(0)}$。\n4. 在 EM 更新前后，使用数值稳定的缩放前向算法计算观测数据的对数似然 $\\log p(x_{1:T} \\mid \\theta)$。使用自然对数（以 $e$ 为底）。\n5. 对于每个测试用例，验证对数似然在 EM 更新后是否严格增加，即检查是否 $\\log p(x_{1:T} \\mid \\theta^{(1)}) > \\log p(x_{1:T} \\mid \\theta^{(0)})$，其中 $\\theta^{(1)}$ 是更新后的参数。\n\n测试套件规范：\n- 所有测试用例均使用 $K = 2$ 个状态。以下四组参数集涵盖了不同的情况：\n  - 情况 $1$（一般混合，中等速率，长度 $T=50$）：\n    - 真实参数：$\\pi^{\\text{true}} = [\\,0.6,\\,0.4\\,]$，\n      $A^{\\text{true}} = \\begin{bmatrix} 0.95  0.05 \\\\ 0.05  0.95 \\end{bmatrix}$，\n      $\\lambda^{\\text{true}} = [\\,3.0,\\,10.0\\,]$，\n      随机种子 $1234$。\n    - 初始参数：$\\pi^{(0)} = [\\,0.5,\\,0.5\\,]$，\n      $A^{(0)} = \\begin{bmatrix} 0.8  0.2 \\\\ 0.2  0.8 \\end{bmatrix}$，\n      $\\lambda^{(0)} = [\\,5.0,\\,5.0\\,]$。\n  - 情况 $2$（强速率对比，长度 $T=60$）：\n    - 真实参数：$\\pi^{\\text{true}} = [\\,0.5,\\,0.5\\,]$，\n      $A^{\\text{true}} = \\begin{bmatrix} 0.9  0.1 \\\\ 0.1  0.9 \\end{bmatrix}$，\n      $\\lambda^{\\text{true}} = [\\,0.5,\\,20.0\\,]$，\n      随机种子 $5678$。\n    - 初始参数：$\\pi^{(0)} = [\\,0.5,\\,0.5\\,]$，\n      $A^{(0)} = \\begin{bmatrix} 0.7  0.3 \\\\ 0.3  0.7 \\end{bmatrix}$，\n      $\\lambda^{(0)} = [\\,1.0,\\,15.0\\,]$。\n  - 情况 $3$（近确定性转移，长度 $T=40$）：\n    - 真实参数：$\\pi^{\\text{true}} = [\\,0.7,\\,0.3\\,]$，\n      $A^{\\text{true}} = \\begin{bmatrix} 0.99  0.01 \\\\ 0.01  0.99 \\end{bmatrix}$，\n      $\\lambda^{\\text{true}} = [\\,2.0,\\,12.0\\,]$，\n      随机种子 $1357$。\n    - 初始参数：$\\pi^{(0)} = [\\,0.5,\\,0.5\\,]$，\n      $A^{(0)} = \\begin{bmatrix} 0.85  0.15 \\\\ 0.15  0.85 \\end{bmatrix}$，\n      $\\lambda^{(0)} = [\\,4.0,\\,9.0\\,]$。\n  - 情况 $4$（短序列边界情况，长度 $T=5$）：\n    - 真实参数：$\\pi^{\\text{true}} = [\\,0.5,\\,0.5\\,]$，\n      $A^{\\text{true}} = \\begin{bmatrix} 0.9  0.1 \\\\ 0.1  0.9 \\end{bmatrix}$，\n      $\\lambda^{\\text{true}} = [\\,1.0,\\,8.0\\,]$，\n      随机种子 $2468$。\n    - 初始参数：$\\pi^{(0)} = [\\,0.5,\\,0.5\\,]$，\n      $A^{(0)} = \\begin{bmatrix} 0.6  0.4 \\\\ 0.4  0.6 \\end{bmatrix}$，\n      $\\lambda^{(0)} = [\\,2.0,\\,6.0\\,]$。\n\n答案规范：\n- 对于每个测试用例，计算一个布尔值，指示对数似然在 EM 更新后是否严格增加。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$\\texttt{[result1,result2,result3,result4]}$）。每个条目必须是一个布尔值 $\\texttt{True}$ 或 $\\texttt{False}$，并按测试用例的顺序（$1$ 到 $4$）排列。\n- 本问题不涉及物理单位或角度单位。如果在实现中需要小数，请使用浮点数。所有对数必须是自然对数。",
            "solution": "本问题要求实现并验证一次针对具有泊松发射的双状态隐马尔可夫模型（HMM）的期望最大化（EM）迭代。该过程涉及生成合成数据，通过前向-后向算法执行 E-step，执行 M-step 来更新参数，并验证观测数据对数似然的单调递增性。\n\n设 HMM 由参数 $\\theta = (\\pi, A, \\lambda)$ 定义，其中 $\\pi$ 是初始状态分布，$A$ 是状态转移矩阵，$\\lambda$ 是泊松发射率向量。该模型有 $K=2$ 个潜在状态 $z_t \\in \\{1, 2\\}$，并产生观测到的脉冲计数 $x_t \\in \\{0, 1, 2, \\dots\\}$。\n\n首先，对于每个测试用例，根据真实参数 $(\\pi^{\\text{true}}, A^{\\text{true}}, \\lambda^{\\text{true}})$ 和指定的随机种子，生成一个长度为 $T$ 的观测序列 $x_{1:T} = (x_1, \\dots, x_T)$。这包括采样一个初始状态 $z_1 \\sim \\text{Categorical}(\\pi^{\\text{true}})$，然后递归地采样状态 $z_{t+1} \\sim \\text{Categorical}(A_{z_t, \\cdot}^{\\text{true}})$ 和观测值 $x_t \\sim \\text{Poisson}(\\lambda_{z_t}^{\\text{true}})$。\n\n任务的核心是使用一次 EM 迭代将一组初始参数 $\\theta^{(0)} = (\\pi^{(0)}, A^{(0)}, \\lambda^{(0)})$ 更新为新参数 $\\theta^{(1)}$。EM 算法通过迭代最大化期望完全数据对数似然来最大化观测数据的对数似然 $\\log p(x_{1:T} \\mid \\theta)$。完全数据的对数似然由下式给出：\n$$\n\\log p(x_{1:T}, z_{1:T} \\mid \\theta) = \\log \\pi_{z_1} + \\sum_{t=1}^{T-1} \\log A_{z_t, z_{t+1}} + \\sum_{t=1}^{T} \\log p(x_t \\mid z_t, \\lambda_{z_t})\n$$\nEM 算法在两个步骤之间交替进行：\n\n**1. 期望（E）步骤**\n在 E-step 中，我们计算关于在给定观测数据和当前参数 $\\theta^{(0)}$ 下的潜在状态后验分布的期望完全数据对数似然。这就是 $Q$ 函数：\n$$\nQ(\\theta \\mid \\theta^{(0)}) = E_{z_{1:T} \\mid x_{1:T}, \\theta^{(0)}}[\\log p(x_{1:T}, z_{1:T} \\mid \\theta)]\n$$\n计算这个期望需要计算后验状态边缘概率 $\\gamma_t(k) = p(z_t=k \\mid x_{1:T}, \\theta^{(0)})$ 和后验成对边缘概率 $\\xi_t(i,j) = p(z_t=i, z_{t+1}=j \\mid x_{1:T}, \\theta^{(0)})$。这些是通过前向-后向算法找到的。为了防止数值下溢，使用了该算法的缩放版本。\n\n**前向算法（缩放版）**\n前向消息 $\\alpha_t(k) = p(x_{1:t}, z_t=k \\mid \\theta^{(0)})$ 是递归计算的。为保持数值稳定性，我们定义缩放后的消息 $\\hat{\\alpha}_t(k) = p(z_t=k \\mid x_{1:t}, \\theta^{(0)})$。\n在 $t=1$ 处的初始化为：\n$$\n\\alpha_1(k) = \\pi^{(0)}_k p(x_1 \\mid z_1=k, \\lambda^{(0)}_k)\n$$\n缩放因子为 $c_1 = \\sum_{k=1}^K \\alpha_1(k)$，缩放后的消息为 $\\hat{\\alpha}_1(k) = \\alpha_1(k) / c_1$。\n对于 $t=2, \\dots, T$，递归公式为：\n$$\n\\alpha_t(k) = p(x_t \\mid z_t=k, \\lambda^{(0)}_k) \\sum_{j=1}^K \\hat{\\alpha}_{t-1}(j) A^{(0)}_{jk}\n$$\n缩放因子为 $c_t = \\sum_{k=1}^K \\alpha_t(k)$，缩放后的消息为 $\\hat{\\alpha}_t(k) = \\alpha_t(k) / c_t$。\n观测数据的对数似然可以直接通过这些缩放因子计算得出：\n$$\n\\log p(x_{1:T} \\mid \\theta^{(0)}) = \\sum_{t=1}^T \\log c_t\n$$\n\n**后向算法（缩放版）**\n后向消息 $\\beta_t(k) = p(x_{t+1:T} \\mid z_t=k, \\theta^{(0)})$ 也是递归计算的。我们定义与前向缩放兼容的缩放后消息 $\\hat{\\beta}_t(k)$。\n在 $t=T$ 处的初始化为 $\\hat{\\beta}_T(k) = 1$，对所有 $k \\in \\{1, \\dots, K\\}$ 成立。\n对于 $t=T-1, \\dots, 1$ 的递归公式为：\n$$\n\\hat{\\beta}_t(i) = \\frac{1}{c_{t+1}} \\sum_{j=1}^K A^{(0)}_{ij} p(x_{t+1} \\mid z_{t+1}=j, \\lambda^{(0)}_j) \\hat{\\beta}_{t+1}(j)\n$$\n\n**后验概率**\n利用缩放后的前向和后向消息，所需的后验概率计算如下：\n状态边缘后验概率 $\\gamma_t(k)$：\n$$\n\\gamma_t(k) = p(z_t=k \\mid x_{1:T}, \\theta^{(0)}) = \\frac{\\alpha_t(k)\\beta_t(k)}{p(x_{1:T}|\\theta^{(0)})} = \\hat{\\alpha}_t(k) \\hat{\\beta}_t(k)\n$$\n成对边缘后验概率 $\\xi_t(i,j)$：\n$$\n\\xi_t(i,j) = p(z_t=i, z_{t+1}=j \\mid x_{1:T}, \\theta^{(0)}) = \\frac{\\alpha_t(i) A^{(0)}_{ij} p(x_{t+1} \\mid z_{t+1}=j, \\lambda_j^{(0)}) \\beta_{t+1}(j)}{p(x_{1:T}|\\theta^{(0)})}\n$$\n$$\n= \\frac{\\hat{\\alpha}_t(i) A^{(0)}_{ij} p(x_{t+1} \\mid z_{t+1}=j, \\lambda_j^{(0)}) \\hat{\\beta}_{t+1}(j)}{c_{t+1}}\n$$\n\n**2. 最大化（M）步骤**\n在 M-step 中，我们找到最大化 $Q$ 函数 $Q(\\theta \\mid \\theta^{(0)})$ 的参数 $\\theta^{(1)}$。初始分布 $\\pi$ 保持固定为 $\\pi^{(0)}$。\n\n**转移矩阵更新**\n$Q(\\theta \\mid \\theta^{(0)})$ 中依赖于 $A$ 的部分是 $\\sum_{t=1}^{T-1} \\sum_{i=1}^K \\sum_{j=1}^K \\xi_t(i,j) \\log A_{ij}$。在约束条件 $\\sum_j A_{ij} = 1$（对每一行 $i$）下最大化此式，得到更新公式：\n$$\nA^{(1)}_{ij} = \\frac{\\text{从 } i \\text{ 转移到 } j \\text{ 的期望次数}}{\\text{从 } i \\text{ 开始转移的期望次数}} = \\frac{\\sum_{t=1}^{T-1} \\xi_t(i,j)}{\\sum_{t=1}^{T-1} \\gamma_t(i)}\n$$\n\n**发射率更新**\n$Q(\\theta \\mid \\theta^{(0)})$ 中依赖于 $\\lambda$ 的部分是 $\\sum_{t=1}^T \\sum_{k=1}^K \\gamma_t(k) \\log p(x_t \\mid z_t=k, \\lambda_k)$。对于泊松发射模型，$\\log p(x_t \\mid z_t=k, \\lambda_k) = x_t \\log \\lambda_k - \\lambda_k - \\log(x_t!)$。对 $\\lambda_k$ 进行最大化，得到：\n$$\n\\lambda^{(1)}_k = \\frac{\\text{状态 } k \\text{ 中的期望总计数}}{\\text{状态 } k \\text{ 中的期望持续时间}} = \\frac{\\sum_{t=1}^T \\gamma_t(k) x_t}{\\sum_{t=1}^T \\gamma_t(k)}\n$$\n初始分布是固定的，因此 $\\pi^{(1)} = \\pi^{(0)}$。这就定义了新的参数集 $\\theta^{(1)} = (\\pi^{(0)}, A^{(1)}, \\lambda^{(1)})$。\n\n**3. 验证**\n最后，我们通过使用 $\\theta^{(1)}$ 运行缩放前向算法，来计算使用更新后参数的对数似然 $\\log p(x_{1:T} \\mid \\theta^{(1)})$。EM 算法的基本性质保证了 $\\log p(x_{1:T} \\mid \\theta^{(1)}) \\ge \\log p(x_{1:T} \\mid \\theta^{(0)})$。本问题要求验证严格递增，这是预料之中的，因为初始参数 $\\theta^{(0)}$ 不在似然函数的局部最大值点。\n\n实现将对每个测试用例执行这些步骤，并报告严格不等式 $\\log p(x_{1:T} \\mid \\theta^{(1)})  \\log p(x_{1:T} \\mid \\theta^{(0)})$ 是否成立。所有对数均为自然对数（以 $e$ 为底）。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp, gammaln\n\ndef solve():\n    \"\"\"\n    Main function to run the EM HMM validation for all test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"T\": 50,\n            \"pi_true\": np.array([0.6, 0.4]),\n            \"A_true\": np.array([[0.95, 0.05], [0.05, 0.95]]),\n            \"lambda_true\": np.array([3.0, 10.0]),\n            \"seed\": 1234,\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.8, 0.2], [0.2, 0.8]]),\n            \"lambda0\": np.array([5.0, 5.0])\n        },\n        {\n            \"T\": 60,\n            \"pi_true\": np.array([0.5, 0.5]),\n            \"A_true\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"lambda_true\": np.array([0.5, 20.0]),\n            \"seed\": 5678,\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.7, 0.3], [0.3, 0.7]]),\n            \"lambda0\": np.array([1.0, 15.0])\n        },\n        {\n            \"T\": 40,\n            \"pi_true\": np.array([0.7, 0.3]),\n            \"A_true\": np.array([[0.99, 0.01], [0.01, 0.99]]),\n            \"lambda_true\": np.array([2.0, 12.0]),\n            \"seed\": 1357,\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.85, 0.15], [0.15, 0.85]]),\n            \"lambda0\": np.array([4.0, 9.0])\n        },\n        {\n            \"T\": 5,\n            \"pi_true\": np.array([0.5, 0.5]),\n            \"A_true\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"lambda_true\": np.array([1.0, 8.0]),\n            \"seed\": 2468,\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.6, 0.4], [0.4, 0.6]]),\n            \"lambda0\": np.array([2.0, 6.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x_obs = generate_data(\n            case[\"T\"], case[\"pi_true\"], case[\"A_true\"], case[\"lambda_true\"], case[\"seed\"]\n        )\n\n        pi0, A0, lambda0 = case[\"pi0\"], case[\"A0\"], case[\"lambda0\"]\n\n        # E-step with initial parameters theta_0\n        gamma, xi, log_L0 = forward_backward(x_obs, pi0, A0, lambda0)\n        \n        # M-step to get updated parameters theta_1\n        pi1, A1, lambda1 = m_step(x_obs, gamma, xi, pi0)\n\n        # Calculate log-likelihood with updated parameters theta_1\n        _, _, log_L1 = forward_backward(x_obs, pi1, A1, lambda1)\n\n        # Verify strict increase in log-likelihood\n        results.append(log_L1 > log_L0)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef generate_data(T, pi_true, A_true, lambda_true, seed):\n    \"\"\"\n    Generates a sequence of spike counts from an HMM with Poisson emissions.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    K = len(pi_true)\n    states = np.arange(K)\n    \n    z_obs = np.zeros(T, dtype=int)\n    x_obs = np.zeros(T, dtype=int)\n\n    # Sample initial state z_1\n    z_obs[0] = rng.choice(states, p=pi_true)\n    x_obs[0] = rng.poisson(lambda_true[z_obs[0]])\n\n    # Sample remaining states and observations\n    for t in range(T - 1):\n        z_obs[t+1] = rng.choice(states, p=A_true[z_obs[t], :])\n        x_obs[t+1] = rng.poisson(lambda_true[z_obs[t+1]])\n    \n    return x_obs\n\n\ndef poisson_log_pmf(x, rate):\n    \"\"\"\n    Computes the log of the Poisson probability mass function.\n    \"\"\"\n    return x * np.log(rate) - rate - gammaln(x + 1)\n\n\ndef forward_backward(x_obs, pi, A, lmbda):\n    \"\"\"\n    Performs the forward-backward algorithm to compute posteriors and log-likelihood.\n    Uses log-space calculations for numerical stability.\n    \"\"\"\n    T = len(x_obs)\n    K = len(pi)\n\n    # Compute emission log-probabilities\n    log_em_probs = np.zeros((T, K))\n    for k in range(K):\n        log_em_probs[:, k] = poisson_log_pmf(x_obs, lmbda[k])\n    \n    # --- Forward pass (scaled alphas) ---\n    log_alpha_hat = np.zeros((T, K))\n    log_c = np.zeros(T)\n\n    # Initialization t=0\n    log_alpha_0_unscaled = np.log(pi) + log_em_probs[0, :]\n    log_c[0] = logsumexp(log_alpha_0_unscaled)\n    log_alpha_hat[0, :] = log_alpha_0_unscaled - log_c[0]\n\n    # Recursion t=1 to T-1\n    for t in range(T - 1):\n        log_alpha_t_plus_1_unscaled = np.zeros(K)\n        for k in range(K):\n            # Sum over previous state j\n            log_alpha_t_plus_1_unscaled[k] = log_em_probs[t+1, k] + \\\n                logsumexp(log_alpha_hat[t, :] + np.log(A[:, k]))\n        \n        log_c[t+1] = logsumexp(log_alpha_t_plus_1_unscaled)\n        log_alpha_hat[t+1, :] = log_alpha_t_plus_1_unscaled - log_c[t+1]\n\n    log_likelihood = np.sum(log_c)\n\n    # --- Backward pass (scaled betas) ---\n    log_beta_hat = np.zeros((T, K))\n    # Initialization t=T-1 is log(1) = 0\n    \n    # Recursion t=T-2 down to 0\n    for t in range(T - 2, -1, -1):\n        for i in range(K):\n            # Sum over next state j\n            terms = np.log(A[i, :]) + log_em_probs[t+1, :] + log_beta_hat[t+1, :]\n            log_beta_hat[t, i] = logsumexp(terms) - log_c[t+1]\n\n    # --- Compute posteriors ---\n    # Gamma: p(z_t=k | x_{1:T})\n    log_gamma = log_alpha_hat + log_beta_hat\n    gamma = np.exp(log_gamma)\n    \n    # Xi: p(z_t=i, z_{t+1}=j | x_{1:T})\n    log_xi = np.zeros((T - 1, K, K))\n    for t in range(T - 1):\n        for i in range(K):\n            for j in range(K):\n                log_xi[t, i, j] = log_alpha_hat[t, i] + np.log(A[i, j]) + \\\n                                  log_em_probs[t+1, j] + log_beta_hat[t+1, j] - \\\n                                  log_c[t+1]\n    xi = np.exp(log_xi)\n\n    return gamma, xi, log_likelihood\n\n\ndef m_step(x_obs, gamma, xi, pi0):\n    \"\"\"\n    Performs the M-step to update the HMM parameters.\n    \"\"\"\n    K = gamma.shape[1]\n    T = gamma.shape[0]\n\n    # Update transition matrix A\n    # Denominator: sum of gammas over t=0 to T-2\n    sum_gamma_T_minus_1 = np.sum(gamma[:-1, :], axis=0)\n    # Numerator: sum of xi over t=0 to T-2\n    sum_xi = np.sum(xi, axis=0)\n    A1 = sum_xi / sum_gamma_T_minus_1[:, np.newaxis]\n    \n    # Update Poisson rates lambda\n    sum_gamma_T = np.sum(gamma, axis=0)\n    # Numerator: sum of gamma_t * x_t over t=0 to T-1\n    weighted_x = np.sum(gamma * x_obs[:, np.newaxis], axis=0)\n    lambda1 = weighted_x / sum_gamma_T\n\n    # Initial distribution pi is fixed\n    pi1 = pi0\n\n    return pi1, A1, lambda1\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "训练好一个 HMM 模型后，关键任务是解释其参数以获得神经科学的洞见。模型转换矩阵中的自转换概率（$A_{kk}$）直接关系到每个隐藏状态的“粘性”或持续性。这个练习 () 揭示了该参数与一个直观且有意义的物理量——神经状态的期望“停留时间”——之间的直接数学关系，这对于理解所识别出的大脑状态的时间动态至关重要。",
            "id": "3987987",
            "problem": "考虑一个离散时间隐马尔可夫模型（HMM），用于描述在固定宽度时间窗口内记录的皮层神经群体中的潜在神经状态。设潜在过程是一个具有三个状态 $k \\in \\{1,2,3\\}$ 和转移矩阵 $A$ 的时间齐次马尔可夫链，其中 $A_{kk}$ 表示在下一个时间窗口中保持在状态 $k$ 的自转移概率。假设对于所有 $k$，有 $0  A_{kk}  1$，并且所有非自转移概率都严格为正，且与 $A_{kk}$ 的和为 $1$。您观察到同一潜在状态的连续运行，并将状态 $k$ 的驻留时间 $d_{k}$ 定义为从进入状态 $k$ 开始到首次退出到不同状态为止，在该状态下连续花费的时间窗口数。\n\n仅使用马尔可夫性质和链的时间齐次性，推导 $d_{k}$ 的概率质量函数（以 $A_{kk}$ 表示）。然后，对于一个自转移概率为 $A_{11}=0.95$，$A_{22}=0.90$ 和 $A_{33}=0.85$ 的模型，计算每个状态 $k \\in \\{1,2,3\\}$ 的期望驻留时间 $E[d_{k}]$。以时间窗口为单位表示期望持续时间。以无舍入的精确值形式提供最终数值答案，并将其表示为单行向量。无需关于发射或观测噪声的额外假设。最终答案必须是单一的计算结果，并且不得在最终的方框表达式内包含单位。",
            "solution": "该问题要求两个结果：首先，推导潜在神经状态 $k$ 中驻留时间 $d_k$ 的概率质量函数（PMF）；其次，计算三个特定状态的期望驻留时间 $E[d_k]$。\n\n设潜在过程是一个时间齐次马尔可夫链，在时间 $t$ 的状态为 $S_t \\in \\{1, 2, 3\\}$。从状态 $i$到状态 $j$ 的转移概率是 $A_{ij} = P(S_{t+1}=j|S_t=i)$。状态 $k$ 的自转移概率是 $A_{kk}$。\n\n状态 $k$ 的驻留时间 $d_k$ 定义为从进入该状态开始，在该状态下连续花费的时间窗口数。让我们假设系统在时间 $t_0$ 进入状态 $k$。对于 $m \\in \\{1, 2, 3, \\dots\\}$，事件 $\\{d_k = m\\}$ 对应于系统在状态 $k$ 中停留 $m$ 个时间窗口然后转移到不同状态的状态序列。这意味着 $S_{t_0} = k, S_{t_0+1} = k, \\dots, S_{t_0+m-1} = k$ 且 $S_{t_0+m} \\neq k$。\n\n由于时间齐次的马尔可夫性质，这一事件序列的概率是各次独立转移概率的乘积：\n$$ P(d_k = m) = P(S_{t_0+1}=k|S_{t_0}=k) \\times \\dots \\times P(S_{t_0+m-1}=k|S_{t_0+m-2}=k) \\times P(S_{t_0+m} \\neq k|S_{t_0+m-1}=k) $$\n保持在状态 $k$ 的概率是 $A_{kk}$。退出状态 $k$ 的概率是 $1 - A_{kk}$，因为从任何状态出发的概率之和必须为 $1$。事件 $\\{d_k = m\\}$ 由 $m-1$ 次成功的自转移和一次到不同状态的转移组成。\n因此，概率为：\n$$ P(d_k = m) = (A_{kk})^{m-1} (1 - A_{kk}) $$\n这是 $d_k$ 的概率质量函数。这是一个在集合 $\\{1, 2, 3, \\dots\\}$ 上的几何分布，其成功参数为 $p = 1 - A_{kk}$，其中“成功”定义为退出该状态。\n\n接下来，我们推导期望驻留时间 $E[d_k]$。根据定义，离散随机变量的期望值是每个值乘以其概率的总和：\n$$ E[d_k] = \\sum_{m=1}^{\\infty} m \\cdot P(d_k = m) = \\sum_{m=1}^{\\infty} m (A_{kk})^{m-1} (1 - A_{kk}) $$\n我们可以提出常数项 $(1 - A_{kk})$：\n$$ E[d_k] = (1 - A_{kk}) \\sum_{m=1}^{\\infty} m (A_{kk})^{m-1} $$\n该求和是几何级数分析中的一个标准结果。对于任何满足 $|x|  1$ 的变量 $x$，几何级数公式为 $\\sum_{m=0}^{\\infty} x^m = \\frac{1}{1-x}$。对两边关于 $x$求导得出：\n$$ \\frac{d}{dx} \\sum_{m=0}^{\\infty} x^m = \\sum_{m=1}^{\\infty} m x^{m-1} = \\frac{d}{dx} \\left( \\frac{1}{1-x} \\right) = \\frac{1}{(1-x)^2} $$\n由于问题指定 $0  A_{kk}  1$，我们可以将 $x = A_{kk}$ 代入此结果：\n$$ \\sum_{m=1}^{\\infty} m (A_{kk})^{m-1} = \\frac{1}{(1 - A_{kk})^2} $$\n将此代回 $E[d_k]$ 的表达式中：\n$$ E[d_k] = (1 - A_{kk}) \\left( \\frac{1}{(1 - A_{kk})^2} \\right) = \\frac{1}{1 - A_{kk}} $$\n这是状态 $k$ 中期望驻留时间的一般表达式。\n\n最后，我们应用此公式计算每个状态的期望驻留时间，给定自转移概率 $A_{11}=0.95$，$A_{22}=0.90$ 和 $A_{33}=0.85$。\n\n对于状态 $k=1$：\n$$ E[d_1] = \\frac{1}{1 - A_{11}} = \\frac{1}{1 - 0.95} = \\frac{1}{0.05} = 20 $$\n\n对于状态 $k=2$：\n$$ E[d_2] = \\frac{1}{1 - A_{22}} = \\frac{1}{1 - 0.90} = \\frac{1}{0.10} = 10 $$\n\n对于状态 $k=3$：\n$$ E[d_3] = \\frac{1}{1 - A_{33}} = \\frac{1}{1 - 0.85} = \\frac{1}{0.15} = \\frac{1}{\\frac{15}{100}} = \\frac{100}{15} = \\frac{20}{3} $$\n\n状态 $1$、$2$ 和 $3$ 的期望驻留时间分别为 $20$、$10$ 和 $\\frac{20}{3}$ 个时间窗口。这些将以单行向量的形式呈现。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 20  10  \\frac{20}{3} \\end{pmatrix}}\n$$"
        }
    ]
}