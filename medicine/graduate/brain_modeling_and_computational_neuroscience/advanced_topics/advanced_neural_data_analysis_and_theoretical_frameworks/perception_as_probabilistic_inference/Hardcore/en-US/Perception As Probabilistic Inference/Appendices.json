{
    "hands_on_practices": [
        {
            "introduction": "The first step in modeling perception as probabilistic inference is to formalize our assumptions about how sensory data is generated. This is known as creating a \"generative model,\" which mathematically describes the causal process from latent variables in the world to sensory observations. This practice  guides you through constructing a generative model for a foundational task in psychophysics—signal detection—helping you translate a conceptual scenario into a precise joint probability distribution.",
            "id": "4008976",
            "problem": "Consider a single-trial visual detection task in which a latent binary variable $z \\in \\{0,1\\}$ encodes whether a target is absent ($z=0$) or present ($z=1$). The measurement $x \\in \\mathbb{R}$ is a scalar sensory observation. Assume that the measurement noise is Gaussian and additive, and that the baseline measurement when the target is absent has mean $0$, whereas the measurement when the target is present has mean $\\mu_1 \\in \\mathbb{R}$. Let the prior probability of target presence be $\\pi \\in (0,1)$, and let the noise variance be $\\sigma^2 \\in (0,\\infty)$. Define the parameter vector as $\\theta = (\\pi, \\mu_1, \\sigma^2)$. Which option correctly specifies a scientifically sound joint generative model $p(x,z \\mid \\theta)$ such that the latent cause $z$ influences the observation $x$ through Gaussian noise, and give a clear interpretation of each term in the specification?\n\nA. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian with mean $z\\,\\mu_1$ and variance $\\sigma^2$, and $\\theta=(\\pi,\\mu_1,\\sigma^2)$ parameterizes the prior and the likelihood.\n\nB. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{x^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian with mean $0$ and variance $\\sigma^2$ for both $z \\in \\{0,1\\}$, and $\\theta=(\\pi,\\sigma^2)$ parameterizes the prior and the likelihood.\n\nC. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{2b} \\exp\\!\\left(-\\frac{|x - z\\,\\mu_1|}{b}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Laplace with mean $z\\,\\mu_1$ and scale $b>0$, and $\\theta=(\\pi,\\mu_1,b)$ parameterizes the prior and the likelihood.\n\nD. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian up to proportionality, and $\\theta=(\\pi,\\mu_1,\\sigma^2)$ parameterizes the prior and the likelihood.\n\nE. $p(x,z \\mid \\theta) = p(x \\mid \\theta)\\,p(z \\mid x,\\theta)$ with $p(x \\mid \\theta) = \\pi\\,\\mathcal{N}(x; \\mu_1, \\sigma^2) + (1-\\pi)\\,\\mathcal{N}(x; 0, \\sigma^2)$ and $p(z=1 \\mid x,\\theta) = \\frac{1}{1+\\exp(-\\alpha x)}$, where $\\alpha \\in \\mathbb{R}$ controls the slope, and $\\theta=(\\pi,\\mu_1,\\sigma^2,\\alpha)$ parameterizes the mixture and the posterior classifier.\n\nSelect the correct option(s).",
            "solution": "The user requires a critical validation of the problem statement, followed by a full derivation and evaluation of the options provided for a question on generative models in computational neuroscience.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\nThe problem statement provides the following information:\n- A latent binary variable $z \\in \\{0,1\\}$.\n- $z=0$ represents target absence.\n- $z=1$ represents target presence.\n- A scalar sensory observation $x \\in \\mathbb{R}$.\n- The measurement noise is Gaussian and additive.\n- The mean of the measurement is $0$ when the target is absent ($z=0$).\n- The mean of the measurement is $\\mu_1 \\in \\mathbb{R}$ when the target is present ($z=1$).\n- The prior probability of target presence is $P(z=1) = \\pi$, with $\\pi \\in (0,1)$.\n- The noise variance is $\\sigma^2$, with $\\sigma^2 \\in (0,\\infty)$.\n- The parameter vector is defined as $\\theta = (\\pi, \\mu_1, \\sigma^2)$.\n- The task is to identify the option that correctly specifies the joint generative model $p(x,z \\mid \\theta)$ and its interpretation, where the latent cause $z$ influences the observation $x$ through Gaussian noise.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a canonical model from signal detection theory (SDT), which is a fundamental framework in psychophysics, perception, and computational neuroscience for modeling decisions under uncertainty. It is a classic example of perception as probabilistic inference. The model is scientifically sound and widely used.\n- **Well-Posed:** The problem is well-posed. It provides clear definitions for the variables ($z, x$), parameters ($\\pi, \\mu_1, \\sigma^2$), and the structure of the probabilistic relationship (prior on a binary variable, Gaussian conditional distributions). The goal—to construct the joint probability distribution $p(x,z \\mid \\theta)$—is a standard and solvable task in probability theory.\n- **Objective:** The language is formal, mathematical, and unambiguous. There are no subjective or opinion-based claims.\n\nThe problem statement does not violate any of the invalidity criteria. It is scientifically sound, well-posed, objective, complete, and formalizable.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. The solution process will now proceed.\n\n### Derivation of the Joint Generative Model\n\nThe goal is to construct the joint probability distribution $p(x,z \\mid \\theta)$. A generative model specifies how data are generated from latent causes. This is naturally expressed using the chain rule of probability as:\n$$p(x,z \\mid \\theta) = p(x \\mid z, \\theta) \\, p(z \\mid \\theta)$$\nThis factorization represents the generative sequence: first, a latent cause $z$ is drawn from its prior distribution $p(z \\mid \\theta)$, and then an observation $x$ is generated from the likelihood (or conditional distribution) $p(x \\mid z, \\theta)$.\n\n1.  **The Prior Distribution $p(z \\mid \\theta)$:**\n    The latent variable $z$ is binary, $z \\in \\{0, 1\\}$. We are given the prior probability of target presence, $P(z=1) = \\pi$. Since there are only two outcomes, the probability of target absence is $P(z=0) = 1-\\pi$. This describes a Bernoulli distribution. A compact mathematical expression for the probability mass function of a Bernoulli variable is:\n    $$p(z \\mid \\theta) = \\pi^z (1-\\pi)^{1-z}$$\n    This expression correctly yields $\\pi$ for $z=1$ and $1-\\pi$ for $z=0$. The parameter $\\theta$ relevant for the prior is $\\pi$.\n\n2.  **The Likelihood Function $p(x \\mid z, \\theta)$:**\n    The problem states that the observation $x$ is generated from a process involving additive Gaussian noise. The mean of this process depends on the state of the latent variable $z$.\n    -   When $z=0$ (target absent), the mean of $x$ is $0$. The distribution of $x$ is Gaussian with mean $0$ and variance $\\sigma^2$. The probability density function (PDF) is:\n        $$p(x \\mid z=0, \\theta) = \\mathcal{N}(x; 0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - 0)^2}{2\\sigma^2}\\right)$$\n    -   When $z=1$ (target present), the mean of $x$ is $\\mu_1$. The distribution of $x$ is Gaussian with mean $\\mu_1$ and variance $\\sigma^2$. The PDF is:\n        $$p(x \\mid z=1, \\theta) = \\mathcal{N}(x; \\mu_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu_1)^2}{2\\sigma^2}\\right)$$\n    We can write a single expression for the likelihood that covers both cases. The mean of the Gaussian distribution is $0$ if $z=0$ and $\\mu_1$ if $z=1$. This can be written compactly as $z\\,\\mu_1$. Therefore, the general form of the likelihood is:\n    $$p(x \\mid z, \\theta) = \\mathcal{N}(x; z\\,\\mu_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$$\n    The parameters from $\\theta$ relevant for the likelihood are $\\mu_1$ and $\\sigma^2$.\n\n3.  **The Joint Distribution $p(x,z \\mid \\theta)$:**\n    Combining the prior and the likelihood, we get the joint distribution:\n    $$p(x,z \\mid \\theta) = p(z \\mid \\theta) \\, p(x \\mid z, \\theta) = \\left[ \\pi^z (1-\\pi)^{1-z} \\right] \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right) \\right]$$\n\n### Evaluation of Options\n\n**A. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian with mean $z\\,\\mu_1$ and variance $\\sigma^2$, and $\\theta=(\\pi,\\mu_1,\\sigma^2)$ parameterizes the prior and the likelihood.**\n-   **Mathematical Expression:** This expression exactly matches the one derived from first principles. The term $\\pi^z(1-\\pi)^{1-z}$ correctly represents the Bernoulli prior $p(z \\mid \\theta)$. The term $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$ correctly represents the Gaussian likelihood $p(x \\mid z, \\theta)$ with a mean of $z\\,\\mu_1$ (which is $0$ for $z=0$ and $\\mu_1$ for $z=1$) and variance $\\sigma^2$.\n-   **Interpretation:** The interpretation provided is fully consistent with the problem statement and the mathematical derivation. $\\pi$ is the prior for $z=1$, the conditional distribution of $x$ given $z$ is indeed Gaussian with mean $z\\mu_1$ and variance $\\sigma^2$, and the parameters $\\theta$ are correctly partitioned between the prior and the likelihood.\n-   **Verdict:** **Correct**.\n\n**B. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{x^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian with mean $0$ and variance $\\sigma^2$ for both $z \\in \\{0,1\\}$, and $\\theta=(\\pi,\\sigma^2)$ parameterizes the prior and the likelihood.**\n-   **Mathematical Expression:** The likelihood part of this expression, $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{x^2}{2\\sigma^2}\\right)$, corresponds to a Gaussian distribution with a mean of $0$ for all values of $z$. This means that the observation $x$ is independent of the latent cause $z$, i.e., $p(x \\mid z, \\theta) = p(x \\mid \\theta)$.\n-   **Interpretation:** This contradicts the problem statement that \"the measurement when the target is present has mean $\\mu_1$.\" The model specified in this option implies the latent cause $z$ has no influence on the mean of the observation $x$.\n-   **Verdict:** **Incorrect**.\n\n**C. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{2b} \\exp\\!\\left(-\\frac{|x - z\\,\\mu_1|}{b}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Laplace with mean $z\\,\\mu_1$ and scale $b>0$, and $\\theta=(\\pi,\\mu_1,b)$ parameterizes the prior and the likelihood.**\n-   **Mathematical Expression:** The likelihood part of this expression, $\\frac{1}{2b} \\exp\\!\\left(-\\frac{|x - z\\,\\mu_1|}{b}\\right)$, is the PDF of a Laplace distribution, not a Gaussian distribution.\n-   **Interpretation:** This contradicts the explicit problem requirement that \"the measurement noise is Gaussian and additive.\"\n-   **Verdict:** **Incorrect**.\n\n**D. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian up to proportionality, and $\\theta=(\\pi,\\mu_1,\\sigma^2)$ parameterizes the prior and the likelihood.**\n-   **Mathematical Expression:** The expression for the likelihood, $\\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$, is missing the normalization constant $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$. A joint probability density function for variables including a continuous variable (like $x$) must be normalized such that its integral over the continuous variables and sum over the discrete variables equals $1$. Without this constant, the function is not a valid joint PDF. The question asks for a correct specification of the generative model, which implies a properly defined probability distribution.\n-   **Interpretation:** While the statement \"is Gaussian up to proportionality\" acknowledges the missing term, it describes an incomplete and thus incorrect specification for a probability density function.\n-   **Verdict:** **Incorrect**.\n\n**E. $p(x,z \\mid \\theta) = p(x \\mid \\theta)\\,p(z \\mid x,\\theta)$ with $p(x \\mid \\theta) = \\pi\\,\\mathcal{N}(x; \\mu_1, \\sigma^2) + (1-\\pi)\\,\\mathcal{N}(x; 0, \\sigma^2)$ and $p(z=1 \\mid x,\\theta) = \\frac{1}{1+\\exp(-\\alpha x)}$, where $\\alpha \\in \\mathbb{R}$ controls the slope, and $\\theta=(\\pi,\\mu_1,\\sigma^2,\\alpha)$ parameterizes the mixture and the posterior classifier.**\n-   **Mathematical Expression:** This option uses the factorization $p(x,z) = p(x)p(z|x)$. This is mathematically valid, but it represents the \"diagnostic\" or \"discriminative\" direction ($x \\to z$), not the \"generative\" direction ($z \\to x$) where the latent cause influences the observation. The problem explicitly asks for the generative model. Furthermore, the posterior probability $p(z=1 \\mid x,\\theta)$ is specified as a simple logistic function of $x$, which is an approximation. The true posterior derived from the generative model in option A is a logistic function of a linear function of $x$, specifically involving an offset term. The introduction of an independent parameter $\\alpha$ is also inconsistent with the original parameter set $\\theta$.\n-   **Interpretation:** This formulation describes a model structure that is conceptually different from the generative process requested. It mixes a marginal distribution (evidence) with a separately parameterized posterior (classifier). This is not the standard specification of the joint generative model itself.\n-   **Verdict:** **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once a generative model is established, the core of perception is inference: inverting the model to deduce the probable causes of our sensations. This exercise  demonstrates this process through Bayesian updating, where a prior belief about a latent parameter is refined by incoming data to form a posterior belief. By working through the calculus of a conjugate prior-likelihood pair, you will see exactly how a rational observer learns from evidence.",
            "id": "4008982",
            "problem": "In a binary detection task in perceptual decision-making, an observer models the probability of detecting a stimulus on any given trial as a latent parameter $z \\in (0,1)$. The observer assumes a Beta prior over $z$ with shape parameters $\\alpha > 0$ and $\\beta > 0$, so that the prior density is $p(z) = \\mathrm{Beta}(z;\\alpha,\\beta)$, and observes $n$ independent Bernoulli trials $x_{1},\\dots,x_{n} \\in \\{0,1\\}$ generated conditionally on $z$. \n\nStarting from the fundamental laws of probability, specifically Bayes’ rule and the definition of independence, and using only well-tested facts about the Bernoulli likelihood and the Beta distribution family, perform the following:\n\n1. Derive the exact, normalized posterior density $p(z \\mid x_{1:n})$ in closed form, as a function of $\\alpha$, $\\beta$, and the data. Your derivation should explicitly identify the posterior’s normalizing constant in terms of the Beta function and reduce the posterior density to a member of a known family.\n\n2. Under quadratic loss for estimating $z$ (that is, loss function $\\ell(\\hat{z},z) = (\\hat{z}-z)^{2}$), compute the Bayes estimator as an explicit formula in terms of $\\alpha$, $\\beta$, and the data. Then evaluate this estimator numerically under the following concrete parameters and data summary: prior parameters $\\alpha = 0.8$, $\\beta = 1.2$, number of trials $n = 20$, and number of detections (successes) $s = \\sum_{i=1}^{n} x_{i} = 13$.\n\nRound your numerical answer to $4$ significant figures. Express the final answer as a dimensionless quantity.",
            "solution": "We begin from Bayes’ rule. Let $x_{1:n} = (x_{1},\\dots,x_{n})$ denote the observed binary data. The posterior density of $z$ given the data is\n$$\np(z \\mid x_{1:n}) \\propto p(x_{1:n} \\mid z)\\, p(z).\n$$\nBy conditional independence of the Bernoulli trials given $z$,\n$$\np(x_{1:n} \\mid z) = \\prod_{i=1}^{n} p(x_{i} \\mid z) = \\prod_{i=1}^{n} z^{x_{i}} (1-z)^{1-x_{i}} = z^{\\sum_{i=1}^{n} x_{i}} (1-z)^{n - \\sum_{i=1}^{n} x_{i}}.\n$$\nWrite $s = \\sum_{i=1}^{n} x_{i}$ for the number of detections (successes). The prior density for $z$ is Beta with parameters $\\alpha,\\beta$:\n$$\np(z) = \\mathrm{Beta}(z;\\alpha,\\beta) = \\frac{1}{B(\\alpha,\\beta)} z^{\\alpha - 1} (1-z)^{\\beta - 1},\n$$\nwhere $B(\\alpha,\\beta)$ is the Beta function, which can be expressed in terms of the Gamma function as $B(\\alpha,\\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$.\n\nMultiplying likelihood and prior,\n$$\np(z \\mid x_{1:n}) \\propto \\left[z^{s} (1-z)^{n-s}\\right] \\left[\\frac{1}{B(\\alpha,\\beta)} z^{\\alpha - 1} (1-z)^{\\beta - 1}\\right]\n= \\frac{1}{B(\\alpha,\\beta)} z^{\\alpha + s - 1} (1-z)^{\\beta + n - s - 1}.\n$$\nTo normalize, we use the integral identity defining the Beta function:\n$$\n\\int_{0}^{1} z^{a-1} (1-z)^{b-1} \\, dz = B(a,b),\n$$\nwith $a = \\alpha + s$ and $b = \\beta + n - s$. Therefore, the normalized posterior density is\n$$\np(z \\mid x_{1:n}) = \\frac{1}{B(\\alpha + s,\\; \\beta + n - s)} z^{\\alpha + s - 1} (1-z)^{\\beta + n - s - 1}.\n$$\nThis is a Beta distribution with updated parameters:\n$$\nz \\mid x_{1:n} \\sim \\mathrm{Beta}(\\alpha + s,\\; \\beta + n - s).\n$$\n\nNext, we compute the Bayes estimator under quadratic loss. For squared-error loss $\\ell(\\hat{z},z) = (\\hat{z}-z)^{2}$, the Bayes estimator is the posterior mean:\n$$\n\\hat{z}_{\\text{Bayes}} = \\mathbb{E}[z \\mid x_{1:n}].\n$$\nFor a Beta distribution with parameters $a$ and $b$, the mean is $\\frac{a}{a+b}$. In our case $a = \\alpha + s$ and $b = \\beta + n - s$, hence\n$$\n\\hat{z}_{\\text{Bayes}} = \\frac{\\alpha + s}{\\alpha + \\beta + n}.\n$$\n\nWe now evaluate this expression for the given numerical values $\\alpha = 0.8$, $\\beta = 1.2$, $n = 20$, and $s = 13$:\n$$\n\\hat{z}_{\\text{Bayes}} = \\frac{0.8 + 13}{0.8 + 1.2 + 20} = \\frac{13.8}{22}.\n$$\nCompute the decimal value:\n$$\n\\frac{13.8}{22} = 0.6272727\\ldots\n$$\nRounded to $4$ significant figures, this yields\n$$\n0.6273.\n$$\nThus, the Bayes estimator under quadratic loss for the specified prior and data is $0.6273$ (dimensionless).",
            "answer": "$$\\boxed{0.6273}$$"
        },
        {
            "introduction": "Probabilistic inference provides a posterior distribution, which quantifies our full belief state about a latent variable. However, perception often culminates in a single, discrete choice or estimate. This practice  delves into Bayesian decision theory to show how an optimal estimate is selected by minimizing an expected loss, revealing that the \"best\" answer depends critically on the costs associated with different errors. It contrasts common estimators and highlights how asymmetric goals can, and should, bias our perception.",
            "id": "4008950",
            "problem": "Consider a latent stimulus variable $z \\in \\mathbb{R}$ (for example, a visual orientation) and a sensory observation $x \\in \\mathbb{R}$. The perceptual system combines a likelihood $p(x \\mid z)$ with a prior $p(z)$ to form a posterior $p(z \\mid x)$ via Bayes' rule. In Bayesian decision theory, the optimal estimator $\\hat{z}(x)$ minimizes posterior expected loss, defined by the risk functional $\\mathcal{R}(\\hat{z};x) = \\int L(\\hat{z},z) \\, p(z \\mid x) \\, dz$, where $L(\\hat{z},z)$ is a task-dependent loss. Two canonical estimators are the Maximum A Posteriori (MAP) estimator $\\hat{z}_{\\mathrm{MAP}}(x) = \\arg\\max_{z} p(z \\mid x)$ and the Minimum Mean Squared Error (MMSE) estimator $\\hat{z}_{\\mathrm{MMSE}}(x) = \\mathbb{E}[z \\mid x]$, which arise under different loss specifications. In advanced perceptual settings, loss may be asymmetric: for instance, underestimation of $z$ may be more costly than overestimation due to motor or safety constraints. Consider the asymmetric linear loss\n$$\nL(\\hat{z},z) = c_{+} \\, (z - \\hat{z})_{+} + c_{-} \\, (\\hat{z} - z)_{+},\n$$\nwhere $(a)_{+} = \\max\\{a,0\\}$, $c_{+} > 0$ is the cost weight when the true $z$ exceeds the estimate $\\hat{z}$ (underestimation), and $c_{-} > 0$ is the cost weight when $\\hat{z}$ exceeds $z$ (overestimation). Assume $p(z \\mid x)$ is continuous with cumulative distribution function $F_{Z \\mid X}(z \\mid x)$. Select all statements that are correct about how $\\hat{z}_{\\mathrm{MAP}}$ and $\\hat{z}_{\\mathrm{MMSE}}$ contrast, and when they differ under asymmetric loss.\n\nA. The Bayes estimator that minimizes posterior expected loss is $\\hat{z}(x) = \\arg\\min_{\\hat{z}} \\int L(\\hat{z},z) \\, p(z \\mid x) \\, dz$; under squared-error loss $L(\\hat{z},z) = (z - \\hat{z})^{2}$ it is $\\hat{z}_{\\mathrm{MMSE}}(x) = \\mathbb{E}[z \\mid x]$; under a limiting neighborhood version of $0$-$1$ loss in the continuous case it is the posterior mode $\\hat{z}_{\\mathrm{MAP}}(x)$; and under the asymmetric linear loss $L(\\hat{z},z) = c_{+} (z - \\hat{z})_{+} + c_{-} (\\hat{z} - z)_{+}$ it is the posterior quantile solving $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{+}}{c_{+} + c_{-}}$, which coincides with both $\\hat{z}_{\\mathrm{MMSE}}$ and $\\hat{z}_{\\mathrm{MAP}}$ only when $p(z \\mid x)$ is symmetric unimodal and $c_{+} = c_{-}$.\n\nB. The MAP estimator $\\hat{z}_{\\mathrm{MAP}}(x)$ minimizes expected loss for any loss function $L(\\hat{z},z)$ because it maximizes $p(z \\mid x)$.\n\nC. For any convex loss function $L(\\hat{z},z)$, the Bayes-optimal estimator is always $\\hat{z}_{\\mathrm{MMSE}}(x) = \\mathbb{E}[z \\mid x]$, so asymmetry in loss does not alter the optimal estimator.\n\nD. For the asymmetric linear loss $L(\\hat{z},z) = c_{+} (z - \\hat{z})_{+} + c_{-} (\\hat{z} - z)_{+}$ with $c_{+} > c_{-}$, the Bayes-optimal estimator satisfies $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{-}}{c_{+} + c_{-}}$, implying a downward shift of the estimator relative to $\\mathbb{E}[z \\mid x]$.\n\nE. If $p(z \\mid x)$ is Gaussian and the loss is squared-error, then $\\hat{z}_{\\mathrm{MAP}}(x) = \\hat{z}_{\\mathrm{MMSE}}(x)$; under the asymmetric linear loss with $c_{+} \\neq c_{-}$, the Bayes-optimal estimator is a posterior quantile rather than the posterior mean, so $\\hat{z}_{\\mathrm{MMSE}}(x)$ is not optimal for that loss.",
            "solution": "The problem statement is a valid and well-posed question in Bayesian decision theory, a standard framework in computational neuroscience and statistics. The definitions and concepts provided are scientifically grounded, internally consistent, and sufficient for a rigorous analysis. We will proceed to derive the optimal estimators for the specified loss functions and then evaluate each statement.\n\n### Derivation of Bayes Estimators\n\nThe Bayes-optimal estimator $\\hat{z}(x)$ is the value that minimizes the posterior expected loss, or risk, $\\mathcal{R}(\\hat{z};x)$:\n$$\n\\hat{z}(x) = \\arg\\min_{\\hat{z}} \\mathcal{R}(\\hat{z};x) = \\arg\\min_{\\hat{z}} \\int_{-\\infty}^{\\infty} L(\\hat{z},z) \\, p(z \\mid x) \\, dz\n$$\nWe analyze the specific estimators resulting from different choices of the loss function $L(\\hat{z},z)$.\n\n**1. Squared-Error Loss:** $L(\\hat{z},z) = (z - \\hat{z})^{2}$\nThe risk is $\\mathcal{R}(\\hat{z};x) = \\int (z - \\hat{z})^{2} p(z \\mid x) \\, dz$. To find the minimum, we set the derivative with respect to $\\hat{z}$ to zero:\n$$\n\\frac{\\partial \\mathcal{R}}{\\partial \\hat{z}} = \\frac{\\partial}{\\partial \\hat{z}} \\int (z^2 - 2z\\hat{z} + \\hat{z}^2) p(z \\mid x) \\, dz = \\int (-2z + 2\\hat{z}) p(z \\mid x) \\, dz\n$$\n$$\n\\frac{\\partial \\mathcal{R}}{\\partial \\hat{z}} = 2\\hat{z} \\int p(z \\mid x) \\, dz - 2 \\int z p(z \\mid x) \\, dz = 2\\hat{z} - 2\\mathbb{E}[z \\mid x]\n$$\nSetting the derivative to zero gives $\\hat{z} = \\mathbb{E}[z \\mid x]$. The second derivative is $\\frac{\\partial^2 \\mathcal{R}}{\\partial \\hat{z}^2} = 2 > 0$, confirming a minimum. Thus, the optimal estimator for squared-error loss is the posterior mean, $\\hat{z}_{\\mathrm{MMSE}}(x)$.\n\n**2. 0-1 Loss (Limiting Case):** $L(\\hat{z},z) = \\begin{cases} 0 & \\text{if } |z - \\hat{z}| \\le \\epsilon \\\\ 1 & \\text{if } |z - \\hat{z}| > \\epsilon \\end{cases}$ for a small $\\epsilon > 0$.\nThe risk is $\\mathcal{R}(\\hat{z};x) = \\int_{|z - \\hat{z}| > \\epsilon} 1 \\cdot p(z \\mid x) \\, dz = 1 - \\int_{\\hat{z}-\\epsilon}^{\\hat{z}+\\epsilon} p(z \\mid x) \\, dz$.\nMinimizing the risk is equivalent to maximizing the term $\\int_{\\hat{z}-\\epsilon}^{\\hat{z}+\\epsilon} p(z \\mid x) \\, dz$. As $\\epsilon \\to 0$, this integral is maximized by choosing $\\hat{z}$ to be the value where the density $p(z \\mid x)$ is highest. This is, by definition, the posterior mode. Thus, the optimal estimator for this loss is the Maximum A Posteriori (MAP) estimator, $\\hat{z}_{\\mathrm{MAP}}(x) = \\arg\\max_z p(z \\mid x)$.\n\n**3. Asymmetric Linear Loss:** $L(\\hat{z},z) = c_{+} \\, (z - \\hat{z})_{+} + c_{-} \\, (\\hat{z} - z)_{+}$\nThe risk can be written as:\n$$\n\\mathcal{R}(\\hat{z};x) = \\int_{-\\infty}^{\\hat{z}} c_{-} (\\hat{z} - z) p(z \\mid x) \\, dz + \\int_{\\hat{z}}^{\\infty} c_{+} (z - \\hat{z}) p(z \\mid x) \\, dz\n$$\nWe differentiate with respect to $\\hat{z}$ using the Leibniz integral rule:\n$$\n\\frac{\\partial \\mathcal{R}}{\\partial \\hat{z}} = \\int_{-\\infty}^{\\hat{z}} c_{-} p(z \\mid x) \\, dz - \\int_{\\hat{z}}^{\\infty} c_{+} p(z \\mid x) \\, dz\n$$\nLet $F_{Z \\mid X}(z \\mid x)$ be the cumulative distribution function (CDF) of the posterior $p(z \\mid x)$. The derivative is:\n$$\n\\frac{\\partial \\mathcal{R}}{\\partial \\hat{z}} = c_{-} F_{Z \\mid X}(\\hat{z} \\mid x) - c_{+} [1 - F_{Z \\mid X}(\\hat{z} \\mid x)]\n$$\nSetting this to zero to find the optimal $\\hat{z}$:\n$$\nc_{-} F_{Z \\mid X}(\\hat{z} \\mid x) = c_{+} - c_{+} F_{Z \\mid X}(\\hat{z} \\mid x)\n$$\n$$\n(c_{+} + c_{-}) F_{Z \\mid X}(\\hat{z} \\mid x) = c_{+}\n$$\n$$\nF_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{+}}{c_{+} + c_{-}}\n$$\nThe second derivative is $(c_{+} + c_{-})p(\\hat{z} \\mid x) > 0$, confirming a minimum. The optimal estimator is the quantile of the posterior distribution corresponding to the cumulative probability $\\frac{c_{+}}{c_{+} + c_{-}}$.\n\n### Evaluation of Options\n\n**A. The Bayes estimator that minimizes posterior expected loss is $\\hat{z}(x) = \\arg\\min_{\\hat{z}} \\int L(\\hat{z},z) \\, p(z \\mid x) \\, dz$; under squared-error loss $L(\\hat{z},z) = (z - \\hat{z})^{2}$ it is $\\hat{z}_{\\mathrm{MMSE}}(x) = \\mathbb{E}[z \\mid x]$; under a limiting neighborhood version of $0$-$1$ loss in the continuous case it is the posterior mode $\\hat{z}_{\\mathrm{MAP}}(x)$; and under the asymmetric linear loss $L(\\hat{z},z) = c_{+} (z - \\hat{z})_{+} + c_{-} (\\hat{z} - z)_{+}$ it is the posterior quantile solving $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{+}}{c_{+} + c_{-}}$, which coincides with both $\\hat{z}_{\\mathrm{MMSE}}$ and $\\hat{z}_{\\mathrm{MAP}}$ only when $p(z \\mid x)$ is symmetric unimodal and $c_{+} = c_{-}$.**\nThis statement is a comprehensive summary.\n- The initial definition of the Bayes estimator is correct.\n- The identification of the MMSE estimator for squared-error loss is correct, as shown in our derivation.\n- The identification of the MAP estimator for 0-1 loss is correct, as shown in our derivation.\n- The identification of the quantile estimator for asymmetric linear loss, with the correct formula $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{+}}{c_{+} + c_{-}}$, is correct, as shown in our derivation.\n- The final condition for coincidence: If $p(z \\mid x)$ is symmetric and unimodal, its mean ($\\hat{z}_{\\mathrm{MMSE}}$), median, and mode ($\\hat{z}_{\\mathrm{MAP}}$) are all equal. If $c_{+} = c_{-}$, the optimal quantile is at probability $\\frac{c_{+}}{c_{+} + c_{-}} = \\frac{1}{2}$, which is the median. Therefore, under these conditions, all three estimators coincide. The \"only when\" part holds for a broad class of well-behaved unimodal distributions typically used in such models. For the estimators to coincide, we need mean = mode = $q$-quantile. For a unimodal distribution, mean=mode implies symmetry. Symmetry implies mean=median. So the $q$-quantile must be the median, which requires $q=1/2$, and thus $c_+=c_-$. The statement is accurate in the standard context.\nVerdict: **Correct**.\n\n**B. The MAP estimator $\\hat{z}_{\\mathrm{MAP}}(x)$ minimizes expected loss for any loss function $L(\\hat{z},z)$ because it maximizes $p(z \\mid x)$.**\nThis statement is false. The optimal Bayes estimator is dependent on the specific loss function. As derived above, squared-error loss leads to the posterior mean ($\\hat{z}_{\\mathrm{MMSE}}$), not the posterior mode ($\\hat{z}_{\\mathrm{MAP}}$), unless the posterior is symmetric and unimodal. Maximizing the posterior density is a distinct optimization criterion from minimizing the posterior expected loss for a general loss function.\nVerdict: **Incorrect**.\n\n**C. For any convex loss function $L(\\hat{z},z)$, the Bayes-optimal estimator is always $\\hat{z}_{\\mathrm{MMSE}}(x) = \\mathbb{E}[z \\mid x]$, so asymmetry in loss does not alter the optimal estimator.**\nThis statement is false. The asymmetric linear loss function is convex in $\\hat{z}$, but as we derived, its optimal estimator is a posterior quantile, not the posterior mean (unless the quantile happens to equal the mean). Another counterexample is the absolute error loss $L(\\hat{z},z) = |z-\\hat{z}|$, which is also convex and yields the posterior median as the optimal estimator. The assertion that asymmetry does not alter the estimator is also false; if $c_{+} \\neq c_{-}$, the quantile is not the median, demonstrating the effect of asymmetry.\nVerdict: **Incorrect**.\n\n**D. For the asymmetric linear loss $L(\\hat{z},z) = c_{+} (z - \\hat{z})_{+} + c_{-} (\\hat{z} - z)_{+}$ with $c_{+} > c_{-}$, the Bayes-optimal estimator satisfies $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{-}}{c_{+} + c_{-}}$, implying a downward shift of the estimator relative to $\\mathbb{E}[z \\mid x]$.**\nThis statement contains two errors. First, our derivation shows the correct formula is $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{+}}{c_{+} + c_{-}}$. The statement incorrectly has $c_{-}$ in the numerator. Second, a high cost for underestimation ($c_{+} > c_{-}$) should intuitively lead to an upward bias in the estimate to avoid costly underestimates. The correct formula confirms this: if $c_{+} > c_{-}$, then $\\frac{c_{+}}{c_{+} + c_{-}} > \\frac{1}{2}$, so the optimal estimate is a quantile above the median, representing an upward shift for symmetric posteriors. The incorrect formula in the statement would imply a quantile below the median, leading to the incorrect conclusion of a downward shift.\nVerdict: **Incorrect**.\n\n**E. If $p(z \\mid x)$ is Gaussian and the loss is squared-error, then $\\hat{z}_{\\mathrm{MAP}}(x) = \\hat{z}_{\\mathrm{MMSE}}(x)$; under the asymmetric linear loss with $c_{+} \\neq c_{-}$, the Bayes-optimal estimator is a posterior quantile rather than the posterior mean, so $\\hat{z}_{\\mathrm{MMSE}}(x)$ is not optimal for that loss.**\n- First part: A Gaussian posterior is a symmetric and unimodal distribution. Its mean, median, and mode are all located at the center of the distribution. Therefore, for a Gaussian posterior, $\\hat{z}_{\\mathrm{MAP}}(x)$ (the mode) is equal to $\\hat{z}_{\\mathrm{MMSE}}(x)$ (the mean). This is correct.\n- Second part: For the asymmetric linear loss with $c_{+} \\neq c_{-}$, the optimal estimator is the quantile at $q = \\frac{c_{+}}{c_{+} + c_{-}} \\neq \\frac{1}{2}$. For a Gaussian distribution (or any symmetric one), the mean is the median (the $1/2$-quantile). Since the optimal estimator is a quantile different from the median, it is not the posterior mean. This is correct.\n- Conclusion: Consequently, the MMSE estimator is not optimal for this asymmetric loss. This is the correct logical conclusion.\nThe entire statement is factually correct.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{AE}$$"
        }
    ]
}