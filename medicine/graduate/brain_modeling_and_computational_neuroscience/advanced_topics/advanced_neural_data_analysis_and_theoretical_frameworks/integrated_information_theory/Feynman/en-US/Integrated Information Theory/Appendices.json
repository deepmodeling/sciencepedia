{
    "hands_on_practices": [
        {
            "introduction": "Integrated Information Theory is built upon the concept of irreducible cause-effect power. This practice provides a foundational exercise in quantifying this irreducibility by calculating the integrated information ($\\varphi$) for a single, canonical mechanism: the XOR gate . By following the formal steps of defining repertoires and finding the minimum partition, you will see precisely how information is lost when an integrated mechanism is conceptually split into non-interacting parts.",
            "id": "3990919",
            "problem": "Consider a discrete-time, binary, Markovian system composed of three elements $\\{X, Y, Z\\}$ with states in $\\{0,1\\}$. The system evolves according to the deterministic update rule $Z_{t+1} = X_{t} \\oplus Y_{t}$, where $\\oplus$ denotes exclusive-or. Assume a maximum-entropy prior over the present state of the inputs, so that $p(X_{t} = x, Y_{t} = y) = \\frac{1}{4}$ for all $(x,y) \\in \\{0,1\\}^{2}$. Focus on the mechanism $M = \\{X_{t}, Y_{t}\\}$ in a specific current state $(x,y)$ and its effect repertoire over the purview $Q = \\{Z_{t+1}\\}$.\n\nUsing the definitions of Integrated Information Theory (IIT) and the Earth Mover’s Distance (EMD), where EMD is computed with the Hamming ground metric on the single-bit state space of $Z_{t+1}$ (so the distance between the two atomic states $0$ and $1$ is $1$), determine the integrated information $\\varphi$ of the mechanism $M$ in state $(x,y)$ at time $t$ with respect to its effect on $Z_{t+1}$. Explicitly evaluate the unpartitioned effect repertoire, construct the partitioned effect repertoires under all nontrivial bipartitions of $M$, and compute the minimal EMD between the unpartitioned and partitioned repertoires. Then, state the value of $\\varphi$ and explain why the mechanism’s effect is irreducible under all bipartitions.\n\nExpress the final answer as a single real number. No rounding is required. Do not include any units in your final answer.\n\nIntegrated Information Theory (IIT) and Earth Mover’s Distance (EMD) should be defined at first use. All mathematical derivations must be shown from core definitions, and any claim of irreducibility must be supported by explicit evaluation over all bipartitions of $M$.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained. It presents a standard exercise in applying the formal definitions of Integrated Information Theory (IIT) to a simple, deterministic system. Therefore, the problem is valid, and a solution can be derived.\n\nThe core task is to compute the integrated information, denoted as $\\varphi$, of a mechanism $M$ in a given state. According to Integrated Information Theory, $\\varphi$ quantifies the irreducibility of a system's causal structure. For the effect-side of causality, it is defined as the distance between the effect repertoire of the whole mechanism and the effect repertoire of its minimum information partition (MIP). This distance measures how much information is lost when the mechanism is assumed to be composed of independent parts.\n\nFormally, the integrated information of a mechanism $M$ in state $m$ over a purview $Q$ is:\n$$\n\\varphi(M, m, Q) = D(p_{\\text{unpart}}(Q | M=m) \\ || \\ p_{\\text{part}}(Q | M=m))\n$$\nwhere $p_{\\text{unpart}}$ is the unpartitioned effect repertoire, $p_{\\text{part}}$ is the partitioned effect repertoire corresponding to the minimum information partition, and $D$ is a specified distance measure between probability distributions. The problem specifies this distance $D$ to be the Earth Mover’s Distance (EMD).\n\nThe Earth Mover's Distance (EMD), also known as the Wasserstein-$1$ distance, between two probability distributions $P$ and $Q$ over a metric space $(S, d_{\\text{ground}})$ is the minimum cost to transform one distribution into the other, where the cost of moving a unit of probability mass from state $s_i$ to $s_j$ is given by the ground distance $d_{\\text{ground}}(s_i, s_j)$. For the specific case of a binary state space $S = \\{0, 1\\}$ with ground distance $d_{\\text{ground}}(0,1) = 1$, and distributions $P = (p_0, p_1)$ and $Q = (q_0, q_1)$, the EMD simplifies to:\n$$\n\\text{EMD}(P, Q) = |p_0 - q_0| \\cdot d_{\\text{ground}}(0,1) = |p_0 - q_0|\n$$\n\nLet us now proceed with the calculation for the given system.\n\n1.  **System Specification**\n    -   Mechanism: $M = \\{X_{t}, Y_{t}\\}$\n    -   Purview: $Q = \\{Z_{t+1}\\}$\n    -   System dynamics: $Z_{t+1} = X_{t} \\oplus Y_{t}$ (exclusive-OR).\n    -   Prior distribution (unconstrained): $p(X_{t} = x, Y_{t} = y) = \\frac{1}{4}$. This implies $X_t$ and $Y_t$ are independent and uniformly distributed, i.e., $p(X_t=x) = \\frac{1}{2}$ and $p(Y_t=y) = \\frac{1}{2}$ for $x, y \\in \\{0, 1\\}$.\n    -   The calculation is for a specific current state of the mechanism, $(x,y)$. The result will be shown to be independent of the specific state chosen. Let us select the state $(x,y) = (0,1)$ for the main derivation.\n\n2.  **Unpartitioned Effect Repertoire**\n    The unpartitioned effect repertoire, $p_{\\text{unpart}}$, is the probability distribution over the purview $Q=\\{Z_{t+1}\\}$ conditioned on the mechanism $M$ being in its specified state $(X_t=0, Y_t=1)$.\n    $$\n    p_{\\text{unpart}}(Z_{t+1}=z) = p(Z_{t+1}=z | X_t=0, Y_t=1)\n    $$\n    Since the update rule $Z_{t+1} = X_t \\oplus Y_t$ is deterministic and the mechanism includes all causal inputs to $Z_{t+1}$, the outcome is fully determined.\n    For the state $(X_t=0, Y_t=1)$, we have $Z_{t+1} = 0 \\oplus 1 = 1$.\n    Therefore, the probability distribution is a delta function: $p(Z_{t+1}=1 | X_t=0, Y_t=1) = 1$ and $p(Z_{t+1}=0 | X_t=0, Y_t=1) = 0$.\n    The unpartitioned effect repertoire is the distribution $(p(Z_{t+1}=0), p(Z_{t+1}=1)) = (0, 1)$.\n\n3.  **Partitioned Effect Repertoires**\n    We must consider all nontrivial bipartitions of the mechanism $M = \\{X_{t}, Y_{t}\\}$. For a set of two elements, there is only one such partition:\n    $$\n    \\pi = \\{\\{X_t\\}, \\{Y_t\\}\\} = \\{M_1, M_2\\}\n    $$\n    The partitioned effect repertoire, $p_{\\text{part}}$, is constructed by assuming statistical independence between the parts of the partition. We first compute the effect repertoire for each part individually and then combine them.\n\n    -   **Effect Repertoire of Part $M_1 = \\{X_t\\}$ in state $x=0$:**\n        This is the probability distribution on $Z_{t+1}$ given $X_t=0$, averaging over all possible states of $Y_t$ according to the unconstrained prior probability $p(Y_t=y' | X_t=0)$. Due to independence, $p(Y_t=y'|X_t=0) = p(Y_t=y') = \\frac{1}{2}$.\n        $$\n        p_1(Z_{t+1}=z) = p(Z_{t+1}=z | X_t=0) = \\sum_{y' \\in \\{0,1\\}} p(Z_{t+1}=z | X_t=0, Y_t=y') p(Y_t=y')\n        $$\n        For $z=0$: $Z_{t+1}=0 \\oplus Y_t = Y_t$.\n        $p_1(Z_{t+1}=0) = p(Y_t=0) = \\frac{1}{2}$.\n        For $z=1$: $Z_{t+1}=0 \\oplus Y_t = Y_t$.\n        $p_1(Z_{t+1}=1) = p(Y_t=1) = \\frac{1}{2}$.\n        The effect repertoire of $M_1$ is $p_1 = (\\frac{1}{2}, \\frac{1}{2})$.\n\n    -   **Effect Repertoire of Part $M_2 = \\{Y_t\\}$ in state $y=1$:**\n        Similarly, we compute the distribution on $Z_{t+1}$ given $Y_t=1$, averaging over $X_t$ with prior $p(X_t=x') = \\frac{1}{2}$.\n        $$\n        p_2(Z_{t+1}=z) = p(Z_{t+1}=z | Y_t=1) = \\sum_{x' \\in \\{0,1\\}} p(Z_{t+1}=z | X_t=x', Y_t=1) p(X_t=x')\n        $$\n        For $z=0$: $Z_{t+1}=X_t \\oplus 1 = \\neg X_t$.\n        $p_2(Z_{t+1}=0) = p(\\neg X_t=0) = p(X_t=1) = \\frac{1}{2}$.\n        For $z=1$: $Z_{t+1}=X_t \\oplus 1 = \\neg X_t$.\n        $p_2(Z_{t+1}=1) = p(\\neg X_t=1) = p(X_t=0) = \\frac{1}{2}$.\n        The effect repertoire of $M_2$ is $p_2 = (\\frac{1}{2}, \\frac{1}{2})$.\n\n    -   **Combined Partitioned Repertoire:**\n        The partitioned repertoire $p_{\\text{part}}$ is obtained by taking the product of the individual part-repertoires and normalizing.\n        Product distribution: $(p_1(0)p_2(0), p_1(1)p_2(1)) = (\\frac{1}{2} \\cdot \\frac{1}{2}, \\frac{1}{2} \\cdot \\frac{1}{2}) = (\\frac{1}{4}, \\frac{1}{4})$.\n        Normalization constant: $K = \\sum_z p_1(z)p_2(z) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n        Normalized partitioned repertoire:\n        $$\n        p_{\\text{part}} = (\\frac{1/4}{1/2}, \\frac{1/4}{1/2}) = (\\frac{1}{2}, \\frac{1}{2})\n        $$\n        This distribution represents the effect of the partitioned mechanism, which is maximally uncertain about the future state of $Z_{t+1}$.\n\n4.  **Compute Integrated Information $\\varphi$**\n    Since there is only one nontrivial bipartition, the minimum information partition is this partition. The integrated information $\\varphi$ is the EMD between the unpartitioned and partitioned repertoires.\n    $$\n    \\varphi = \\text{EMD}(p_{\\text{unpart}}, p_{\\text{part}}) = \\text{EMD}((0, 1), (\\frac{1}{2}, \\frac{1}{2}))\n    $$\n    Using the simplified EMD formula:\n    $$\n    \\varphi = |p_{\\text{unpart}}(Z_{t+1}=0) - p_{\\text{part}}(Z_{t+1}=0)| = |0 - \\frac{1}{2}| = \\frac{1}{2}\n    $$\n    This result is independent of the specific state chosen. For example, if we chose state $(x,y)=(1,1)$, the unpartitioned repertoire would be $(1,0)$ since $1 \\oplus 1 = 0$. The individual part repertoires are still $(\\frac{1}{2}, \\frac{1}{2})$, leading to a partitioned repertoire of $(\\frac{1}{2}, \\frac{1}{2})$. The EMD would be $|1 - \\frac{1}{2}| = \\frac{1}{2}$. The value of $\\varphi$ is consistently $\\frac{1}{2}$ for any of the four possible states of the mechanism.\n\n5.  **Irreducibility of the Mechanism's Effect**\n    The mechanism's effect is considered irreducible if its integrated information $\\varphi$ is greater than zero. In this case, we found $\\varphi = \\frac{1}{2} > 0$. The non-zero value demonstrates that there is a difference between the causal properties of the whole mechanism and its parts considered independently.\n    \n    The irreducibility arises because the whole mechanism $\\{X_t, Y_t\\}$ acts as a single, integrated cause that completely determines the effect $Z_{t+1}$. This is reflected in the unpartitioned effect repertoire being a delta distribution (maximum certainty). In contrast, the partition $\\pi = \\{\\{X_t\\}, \\{Y_t\\}\\}$ tears the mechanism apart, creating two independent sub-mechanisms. Each sub-mechanism, considered in isolation, has only partial information about the cause of $Z_{t+1}$ and must average over the unknown state of the other part. This necessary averaging introduces uncertainty, leading to a partitioned effect repertoire that is maximally uncertain ($(\\frac{1}{2}, \\frac{1}{2})$).\n    \n    The integrated information $\\varphi = \\frac{1}{2}$ precisely quantifies the information lost by this partitioning—it is the distance between the certain prediction of the whole and the uncertain prediction of the independent parts. Because this distance is non-zero for the only possible nontrivial bipartition, the effect of the mechanism is irreducible.",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "A frequent and critical misunderstanding is to equate the causal 'integration' measured by $\\Phi$ with statistical 'correlation' (multi-information). This exercise is designed to sever that link by demonstrating a clear counterexample: a system with perfect correlation but zero causal integration . This calculation will solidify your understanding of what IIT measures—the irreducibility of a system's internal causal mechanisms, not merely the statistical dependencies of its current state.",
            "id": "3990925",
            "problem": "Consider a discrete-time, two-element system in the sense of Integrated Information Theory (IIT). Let the system be composed of two binary mechanisms $A$ and $B$, with system state $X_{t} = (A_{t}, B_{t}) \\in \\{0,1\\}^{2}$. The system evolves according to independent identity dynamics:\n$$\n\\Pr\\big(A_{t+1} = a' \\mid A_{t} = a, B_{t} = b\\big) = \\delta_{a',a}, \\quad \\Pr\\big(B_{t+1} = b' \\mid A_{t} = a, B_{t} = b\\big) = \\delta_{b',b},\n$$\nfor all $a,a',b,b' \\in \\{0,1\\}$, where $\\delta$ denotes the Kronecker delta. Assume that at time $t$ the state distribution is correlated due to prior factors: $X_{t}$ takes values $(0,0)$ and $(1,1)$ each with probability $1/2$, and takes values $(0,1)$ and $(1,0)$ with probability $0$.\n\nYou will compare two quantities defined on the system.\n\n1. The multi-information (also known as total correlation) of a discrete random vector $X = (X_{1},\\dots,X_{n})$ with joint distribution $p(x)$ and marginals $p_{i}(x_{i})$ is defined by\n$$\nC(X) = \\sum_{i=1}^{n} H(X_{i}) - H(X),\n$$\nwhere $H(\\cdot)$ denotes the Shannon entropy computed with logarithms base $2$ so that all information quantities are in bits.\n\n2. The one-step system integrated information $\\Phi$ for a bipartition is quantified as the Kullback–Leibler divergence (computed with logarithms base $2$) between the actual joint transition kernel and the partitioned kernel, averaged over a uniform intervention distribution on the current system state. Specifically, for the bipartition $(A \\mid B)$, define the actual kernel\n$$\nP\\big(x' \\mid x\\big) = \\Pr\\big(A_{t+1} = a', B_{t+1} = b' \\mid A_{t} = a, B_{t} = b\\big),\n$$\nand the partitioned kernel\n$$\nP^{\\mathrm{cut}}\\big(x' \\mid x\\big) = \\Pr\\big(A_{t+1} = a' \\mid A_{t} = a\\big)\\,\\Pr\\big(B_{t+1} = b' \\mid B_{t} = b\\big),\n$$\nwith $x = (a,b)$ and $x' = (a',b')$. Let $u(x) = 1/4$ for all $x \\in \\{0,1\\}^{2}$ denote the uniform intervention distribution on $X_{t}$. The one-step integrated information across the bipartition is then\n$$\n\\Phi = \\sum_{x \\in \\{0,1\\}^{2}} u(x)\\, D_{\\mathrm{KL}}\\!\\left(P(\\cdot \\mid x)\\,\\big\\|\\, P^{\\mathrm{cut}}(\\cdot \\mid x)\\right),\n$$\nwhere $D_{\\mathrm{KL}}(p\\|q) = \\sum_{x'} p(x')\\,\\log_{2}\\!\\left(\\frac{p(x')}{q(x')}\\right)$.\n\nTask: Using only these definitions and the specified system, compute $C(X_{t})$ in bits for the correlated state distribution at time $t$, and compute the one-step integrated information $\\Phi$ in bits for the bipartition $(A \\mid B)$ under the uniform intervention distribution. Express your final answers as a row matrix of the form $\\begin{pmatrix} C(X_{t}) & \\Phi \\end{pmatrix}$, in bits. No rounding is required. Conclude, via your calculation, that integrated information cannot be reduced to multi-information by exhibiting this case with high total correlation but zero integrated information.",
            "solution": "The problem asks for the computation of two quantities for a given two-element system: the multi-information (or total correlation) $C(X_{t})$ of the system's state at time $t$, and the one-step integrated information $\\Phi$ across the bipartition $(A|B)$.\n\nFirst, we compute the multi-information $C(X_t)$. The definition provided is $C(X_t) = H(A_t) + H(B_t) - H(X_t)$, where $H(\\cdot)$ is the Shannon entropy with base-$2$ logarithm, and $X_t = (A_t, B_t)$.\n\nThe joint probability distribution of the state $X_t$ is given as:\n$\\Pr(X_t=(0,0)) = 1/2$\n$\\Pr(X_t=(1,1)) = 1/2$\n$\\Pr(X_t=(0,1)) = 0$\n$\\Pr(X_t=(1,0)) = 0$\n\nWe first compute the joint entropy, $H(X_t) = H(A_t, B_t)$:\n$$\nH(X_t) = - \\sum_{a,b \\in \\{0,1\\}} \\Pr(A_t=a, B_t=b) \\log_{2}(\\Pr(A_t=a, B_t=b))\n$$\nSince only two states have non-zero probability, the sum becomes:\n$$\nH(X_t) = - \\left( \\Pr(0,0)\\log_{2}(\\Pr(0,0)) + \\Pr(1,1)\\log_{2}(\\Pr(1,1)) \\right)\n$$\n$$\nH(X_t) = - \\left( \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) \\right) = - \\left( \\frac{1}{2}(-1) + \\frac{1}{2}(-1) \\right) = -(-1) = 1 \\, \\text{bit}\n$$\n\nNext, we compute the marginal probability distributions for $A_t$ and $B_t$.\nFor $A_t$:\n$\\Pr(A_t=0) = \\Pr(A_t=0, B_t=0) + \\Pr(A_t=0, B_t=1) = 1/2 + 0 = 1/2$\n$\\Pr(A_t=1) = \\Pr(A_t=1, B_t=0) + \\Pr(A_t=1, B_t=1) = 0 + 1/2 = 1/2$\n\nFor $B_t$:\n$\\Pr(B_t=0) = \\Pr(A_t=0, B_t=0) + \\Pr(A_t=1, B_t=0) = 1/2 + 0 = 1/2$\n$\\Pr(B_t=1) = \\Pr(A_t=0, B_t=1) + \\Pr(A_t=1, B_t=1) = 0 + 1/2 = 1/2$\n\nNow we compute the marginal entropies, $H(A_t)$ and $H(B_t)$. Since both $A_t$ and $B_t$ follow an identical Bernoulli distribution with $p=1/2$, their entropies are equal:\n$$\nH(A_t) = - \\left( \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) \\right) = 1 \\, \\text{bit}\n$$\n$$\nH(B_t) = - \\left( \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) \\right) = 1 \\, \\text{bit}\n$$\n\nFinally, we compute the multi-information $C(X_t)$:\n$$\nC(X_t) = H(A_t) + H(B_t) - H(X_t) = 1 + 1 - 1 = 1 \\, \\text{bit}\n$$\nThis result indicates perfect correlation between $A_t$ and $B_t$, which is consistent with the given state distribution where $A_t=B_t$.\n\nSecond, we compute the one-step integrated information $\\Phi$. The formula is:\n$$\n\\Phi = \\sum_{x \\in \\{0,1\\}^{2}} u(x)\\, D_{\\mathrm{KL}}\\!\\left(P(\\cdot \\mid x)\\,\\big\\|\\, P^{\\mathrm{cut}}(\\cdot \\mid x)\\right)\n$$\nwhere $x=(a,b)$ is the current state, $x'=(a',b')$ is the next state, $u(x)=1/4$ is a uniform intervention distribution, and $D_{\\mathrm{KL}}$ is the Kullback-Leibler divergence. We need to determine the actual kernel $P(x'|x)$ and the partitioned kernel $P^{\\mathrm{cut}}(x'|x)$.\n\nThe actual kernel $P(x'|x)$ is the joint transition probability $\\Pr(A_{t+1}=a', B_{t+1}=b' \\mid A_t=a, B_t=b)$. The problem specifies the system dynamics as \"independent identity dynamics\":\n$\\Pr(A_{t+1} = a' \\mid A_t = a, B_t = b) = \\delta_{a',a}$\n$\\Pr(B_{t+1} = b' \\mid A_t = a, B_t = b) = \\delta_{b',b}$\nThe term \"independent dynamics\" implies that the evolution of $A$ and $B$ are causally separate processes. Formally, $A_{t+1}$ is conditionally independent of $B_t$ given $A_t$, and $B_{t+1}$ is conditionally independent of $A_t$ given $B_t$. Therefore, the joint transition probability is the product of the individual ones:\n$$\nP(a',b' \\mid a,b) = \\Pr(A_{t+1}=a' \\mid A_t=a, B_t=b) \\times \\Pr(B_{t+1}=b' \\mid A_t=a, B_t=b)\n$$\n$$\nP(a',b' \\mid a,b) = \\delta_{a',a} \\, \\delta_{b',b}\n$$\nThis means the system is deterministic: state $(a,b)$ at time $t$ transitions to state $(a,b)$ at time $t+1$ with probability $1$.\n\nThe partitioned kernel is defined as $P^{\\mathrm{cut}}(x' \\mid x) = \\Pr(A_{t+1} = a' \\mid A_t = a)\\,\\Pr(B_{t+1} = b' \\mid B_t = b)$. We must compute these marginal transition probabilities. From the given dynamics, we can see that the evolution of $A$ does not depend on $b$, and the evolution of $B$ does not depend on $a$.\n$$\n\\Pr(A_{t+1} = a' \\mid A_t = a) = \\Pr(A_{t+1} = a' \\mid A_t = a, B_t = b) = \\delta_{a',a}\n$$\n$$\n\\Pr(B_{t+1} = b' \\mid B_t = b) = \\Pr(B_{t+1} = b' \\mid A_t = a, B_t = b) = \\delta_{b',b}\n$$\nTherefore, the partitioned kernel is:\n$$\nP^{\\mathrm{cut}}(a',b' \\mid a,b) = \\delta_{a',a} \\, \\delta_{b',b}\n$$\n\nWe observe that for any current state $x=(a,b)$, the actual kernel $P(x'|x)$ and the partitioned kernel $P^{\\mathrm{cut}}(x'|x)$ are identical.\n$$\nP(x' \\mid x) = P^{\\mathrm{cut}}(x' \\mid x) \\quad \\forall x, x' \\in \\{0,1\\}^2\n$$\nThe Kullback-Leibler divergence $D_{\\mathrm{KL}}(p\\|q)$ is zero if and only if the probability distributions $p$ and $q$ are identical. Thus, for each $x$:\n$$\nD_{\\mathrm{KL}}\\!\\left(P(\\cdot \\mid x)\\,\\big\\|\\, P^{\\mathrm{cut}}(\\cdot \\mid x)\\right) = \\sum_{x' \\in \\{0,1\\}^2} P(x' \\mid x) \\log_2 \\left( \\frac{P(x' \\mid x)}{P^{\\mathrm{cut}}(x' \\mid x)} \\right) = \\sum_{x' \\in \\{0,1\\}^2} P(x' \\mid x) \\log_2(1) = 0\n$$\nSince the KL divergence is zero for every possible initial state $x$, the integrated information $\\Phi$ is also zero:\n$$\n\\Phi = \\sum_{x \\in \\{0,1\\}^{2}} u(x) \\times 0 = \\sum_{x \\in \\{0,1\\}^{2}} \\frac{1}{4} \\times 0 = 0 \\, \\text{bits}\n$$\n\nIn conclusion, the multi-information of the state distribution is $C(X_t)=1$ bit, while the one-step integrated information of the system's dynamics is $\\Phi = 0$ bits. This calculation demonstrates that integrated information ($\\Phi$) is not reducible to multi-information ($C$). The system's state variables $A_t$ and $B_t$ are maximally correlated ($C=1$ bit), but this correlation arises from external factors that prepared the initial state, not from any causal interaction between the system's parts. The system's dynamics are functionally equivalent to two completely independent, non-interacting parts, which is captured by $\\Phi=0$. $\\Phi$ measures the causal integration of the system's mechanisms, whereas $C$ measures the statistical correlation of its state variables.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While manual calculations are essential for understanding the fundamentals, applying IIT to any non-trivial system requires a computational approach. This practice challenges you to translate the theory's formalism into a working algorithm . You will implement the search for the Minimum Information Partition (MIP), which involves constructing effect repertoires under noise and using optimization techniques to find the partition that minimizes information loss, thereby quantifying a mechanism's irreducibility in a practical setting.",
            "id": "3991004",
            "problem": "Consider a discrete-time mechanism with three binary elements labeled $A$, $B$, and $C$, forming a mechanism in the sense of Integrated Information Theory (IIT). Let the joint state at time $t$ be denoted by $s_t \\in \\{0,1\\}^3$, ordered as $s_t = (A_t,B_t,C_t)$. The system evolves according to a deterministic update rule $f:\\{0,1\\}^3 \\to \\{0,1\\}^3$ followed by independent bit-flip noise. Specifically, given $s_t$, the deterministic next state is $d = f(s_t)$, and each bit of $d$ flips independently with probability $\\epsilon \\in [0,1]$ to produce a distribution over $s_{t+1}$. Denote by $p(s_{t+1}\\mid s_t)$ the resulting conditional probability distribution on $\\{0,1\\}^3$.\n\nFundamental base definitions:\n- The effect repertoire for a fixed current state $s_t$ is the conditional distribution $p_{\\text{full}}(s_{t+1}) = p(s_{t+1}\\mid s_t)$.\n- A bipartition of $\\{A,B,C\\}$ is a split of the set into two non-empty, disjoint blocks $X$ and $Y$ such that $X \\cup Y = \\{A,B,C\\}$. For three elements, enumerate all bipartitions into two blocks.\n- Under a bipartition $(X \\mid Y)$, define the partitioned effect distribution by independently composing blockwise effect repertoires where out-of-block inputs are replaced by independent maximum-entropy noise. Concretely, for block $X$, let $p_{t+1}^X(x')$ denote the marginal distribution on the future states of $X$ obtained by averaging $p(s_{t+1}\\mid s_t)$ over all assignments of the out-of-block current state $s_t^Y$ with a uniform distribution over $\\{0,1\\}^{|Y|}$, while holding $s_t^X$ fixed at its observed value. Analogously define $p_{t+1}^Y(y')$. The partitioned joint effect distribution is then $p_{\\text{part}}(x',y') = p_{t+1}^X(x') \\cdot p_{t+1}^Y(y')$ for all $(x',y') \\in \\{0,1\\}^{|X|} \\times \\{0,1\\}^{|Y|}$, mapped to $\\{0,1\\}^3$ by the block coordinate order.\n- The Earth Mover's Distance (EMD), also known as the $1$-Wasserstein distance, between two distributions $p$ and $q$ on a finite metric space $(\\Omega, d)$ is the optimal transport cost $$D_{\\mathrm{EMD}}(p,q) = \\min_{F \\in \\mathcal{F}(p,q)} \\sum_{i \\in \\Omega}\\sum_{j \\in \\Omega} d(i,j) F_{ij},$$ where $\\mathcal{F}(p,q)$ is the set of nonnegative flows such that $\\sum_{j} F_{ij} = p(i)$ and $\\sum_{i} F_{ij} = q(j)$. In this problem, the ground metric $d$ on $\\{0,1\\}^3$ is the Hamming distance on bit-strings. The Minimum Information Partition (MIP) is the bipartition that minimizes $D_{\\mathrm{EMD}}(p_{\\text{full}}, p_{\\text{part}})$.\n\nStarting from the fundamental laws of probability and the above definitions, implement a program that, for each test case below, computes the effect repertoire $p_{\\text{full}}(s_{t+1})$ for the specified deterministic update rule $f$, noise parameter $\\epsilon$, and fixed current state $s_t$, enumerates all bipartitions of $\\{A,B,C\\}$, constructs $p_{\\text{part}}$ using uniform out-of-block noising, computes $D_{\\mathrm{EMD}}$ for each bipartition using the Hamming ground distance via optimal transport, and returns the minimum value across bipartitions (the MIP value). Express all final numerical answers as floats rounded to six decimals. No physical units apply. Angles are not involved. Percentages must not be used.\n\nTest suite specification with three cases:\n- Case $1$ (\"integrated coupling\"): $f(s_t) = (\\mathrm{XOR}(B_t,C_t),\\, A_t \\wedge C_t,\\, \\mathrm{Majority}(A_t,B_t,C_t))$, noise parameter $\\epsilon = 0.1$, current state $s_t = (1,1,0)$.\n- Case $2$ (\"nearly independent identity\"): $f(s_t) = (A_t,\\, B_t,\\, C_t)$, noise parameter $\\epsilon = 0.05$, current state $s_t = (0,1,1)$.\n- Case $3$ (\"ring copy coupling\"): $f(s_t) = (B_t,\\, C_t,\\, A_t)$, noise parameter $\\epsilon = 0.2$, current state $s_t = (1,0,1)$.\n\nYour program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3]$, where each $r_i$ is the minimum $D_{\\mathrm{EMD}}$ over all bipartitions for the corresponding case, rounded to six decimals.",
            "solution": "The problem requires the computation of a quantity related to integrated information in a small, discrete system. The core task is to find the Minimum Information Partition (MIP) for a given mechanism, which involves calculating the minimum Earth Mover's Distance ($D_{\\mathrm{EMD}}$) between a full effect repertoire and various partitioned effect repertoires. The solution is derived by algorithmically implementing the definitions provided.\n\nLet the set of elements be $\\mathcal{S} = \\{A, B, C\\}$. The state space is $\\Omega = \\{0,1\\}^3$, where each state $s \\in \\Omega$ is an ordered tuple $s = (s_A, s_B, s_C)$. We can represent these $2^3=8$ states by integers from $0$ to $7$ corresponding to their binary value, e.g., $(0,0,0) \\to 0$, $(0,0,1) \\to 1$, ..., $(1,1,1) \\to 7$.\n\nThe algorithm proceeds in four main steps for each test case, which specifies a deterministic update rule $f: \\Omega \\to \\Omega$, a noise parameter $\\epsilon \\in [0,1]$, and a fixed current state $s_t \\in \\Omega$.\n\nStep 1: Computation of the Full Effect Repertoire ($p_{\\text{full}}$)\nThe full effect repertoire is the conditional probability distribution $p_{\\text{full}}(s_{t+1}) = p(s_{t+1} \\mid s_t)$ over the future state $s_{t+1} \\in \\Omega$. Given the current state $s_t$, the system first deterministically transitions to an intermediate state $d = f(s_t)$. Subsequently, each of the $3$ bits of $d$ flips independently with probability $\\epsilon$.\nThe probability of a specific future state $s_{t+1}$ is therefore determined by the number of bit flips required to transform $d$ into $s_{t+1}$. This number is the Hamming distance, $H(s_{t+1}, d)$. The probability is given by the binomial formula:\n$$p_{\\text{full}}(s_{t+1}) = \\epsilon^{H(s_{t+1}, d)} (1-\\epsilon)^{3 - H(s_{t+1}, d)}$$\nThis produces a probability distribution vector of length $8$ over the state space $\\Omega$.\n\nStep 2: Enumeration of Bipartitions and Construction of Partitioned Repertoires ($p_{\\text{part}}$)\nA bipartition of $\\mathcal{S}$ is a split into two non-empty, disjoint subsets $X$ and $Y$ such that $X \\cup Y = \\mathcal{S}$. For a set of $3$ elements, there are $S_2(3) = 3$ such bipartitions, where $S_2(n)$ is a Stirling number of the second kind:\n1.  $(\\{A\\} \\mid \\{B,C\\})$\n2.  $(\\{B\\} \\mid \\{A,C\\})$\n3.  $(\\{C\\} \\mid \\{A,B\\})$\n\nFor each bipartition $(X \\mid Y)$, we construct a partitioned effect repertoire $p_{\\text{part}}$ by assuming independence between the future states of blocks $X$ and $Y$. This distribution is defined as $p_{\\text{part}}(s'_{t+1}) = p^{X}_{t+1}((s'_{t+1})^X) \\cdot p^{Y}_{t+1}((s'_{t+1})^Y)$, where $(s'_{t+1})^X$ denotes the components of $s'_{t+1}$ corresponding to the elements in block $X$, and $p^{X}_{t+1}$ is the marginal effect repertoire for block $X$.\n\nThe marginal repertoire $p^{X}_{t+1}(x')$ is calculated by a specific \"noising\" procedure. We hold the current state of the elements in $X$, $s_t^X$, fixed at their observed values. We then consider all possible states for the out-of-block elements $Y$, which are drawn from a uniform distribution over $\\{0,1\\}^{|Y|}$. For each of these $2^{|Y|}$ \"noised\" current states, $\\tilde{s}_t = (\\tilde{s}_t^X, \\tilde{s}_t^Y)$ where $\\tilde{s}_t^X=s_t^X$, we compute the corresponding full effect repertoire $p(s_{t+1} \\mid \\tilde{s}_t)$ as described in Step 1. These repertoires are then averaged:\n$$\\bar{p}_X(s_{t+1}) = \\frac{1}{2^{|Y|}} \\sum_{\\tilde{s}_t^Y \\in \\{0,1\\}^{|Y|}} p(s_{t+1} \\mid (s_t^X, \\tilde{s}_t^Y))$$\nThe marginal distribution for the future states of $X$ is then obtained by summing over all possible future states of $Y$:\n$$p^{X}_{t+1}(x') = \\sum_{y' \\in \\{0,1\\}^{|Y|}} \\bar{p}_X((x', y'))$$\nAn analogous procedure yields the marginal repertoire $p^{Y}_{t+1}(y')$. These marginals are then multiplied to form the full $8$-element $p_{\\text{part}}$ distribution for the given partition.\n\nStep 3: Computation of the Earth Mover's Distance ($D_{\\mathrm{EMD}}$)\nThe Earth Mover's Distance between the full repertoire $p_{\\text{full}}$ and a partitioned repertoire $p_{\\text{part}}$ is calculated. The ground metric $d(s_i, s_j)$ on the state space $\\Omega = \\{0,1\\}^3$ is defined as the Hamming distance between the bit-string representations of states $s_i$ and $s_j$. The EMD is the solution to the optimal transport problem:\n$$D_{\\mathrm{EMD}}(p_{\\text{full}}, p_{\\text{part}}) = \\min_{F \\in \\mathcal{F}(p_{\\text{full}}, p_{\\text{part}})} \\sum_{s_i \\in \\Omega}\\sum_{s_j \\in \\Omega} d(s_i, s_j) F_{ij}$$\nwhere $F$ is a flow matrix whose entries $F_{ij}$ represent the amount of probability mass moved from state $s_i$ in $p_{\\text{full}}$ to state $s_j$ in $p_{\\text{part}}$. The set $\\mathcal{F}$ contains all non-negative matrices $F$ satisfying the marginal constraints:\n$$ \\sum_{j} F_{ij} = p_{\\text{full}}(s_i) \\quad \\forall i $$\n$$ \\sum_{i} F_{ij} = p_{\\text{part}}(s_j) \\quad \\forall j $$\nThis is a linear programming problem. Letting the states $s_i$ be indexed from $i=0, \\dots, 7$, the variables are the $64$ entries of the flow matrix $F_{ij}$. The objective function to minimize is $\\sum_{i,j} d_{ij} F_{ij}$, subject to $16$ equality constraints (the $8$ row sums and $8$ column sums) and $64$ non-negativity constraints ($F_{ij} \\geq 0$). This problem is solved using `scipy.optimize.linprog`.\n\nStep 4: Finding the Minimum Information Partition (MIP)\nThe procedure from Step 2 and Step 3 is repeated for each of the $3$ possible bipartitions. The MIP value for the given test case is the minimum $D_{\\mathrm{EMD}}$ found among all partitions:\n$$ \\Phi_{\\text{effect}} = \\min_{(X \\mid Y)} D_{\\mathrm{EMD}}(p_{\\text{full}}, p_{\\text{part}}) $$\nThis final value is calculated for each test case and rounded to six decimal places.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    def majority(a, b, c):\n        return int((a + b + c) >= 2)\n\n    def xor(a, b):\n        return int(a != b)\n\n    def f1(s):\n        a, b, c = s\n        return (xor(b, c), int(a and c), majority(a, b, c))\n\n    def f2(s):\n        return s\n\n    def f3(s):\n        a, b, c = s\n        return (b, c, a)\n\n    test_cases = [\n        {'f': f1, 'epsilon': 0.1, 's_t': (1, 1, 0)},\n        {'f': f2, 'epsilon': 0.05, 's_t': (0, 1, 1)},\n        {'f': f3, 'epsilon': 0.2, 's_t': (1, 0, 1)},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_mip_value(case['f'], case['epsilon'], case['s_t'])\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef get_p_dist(d, epsilon, n=3):\n    \"\"\"\n    Calculates the probability distribution over next states given a deterministic\n    next state 'd' and noise 'epsilon'.\n    \"\"\"\n    num_states = 2**n\n    p_dist = np.zeros(num_states)\n    states = [tuple(map(int, bin(i)[2:].zfill(n))) for i in range(num_states)]\n    d_arr = np.array(d)\n    for i, state in enumerate(states):\n        state_arr = np.array(state)\n        h_dist = np.sum(state_arr != d_arr)\n        p_dist[i] = (epsilon**h_dist) * ((1 - epsilon)**(n - h_dist))\n    return p_dist\n\ndef calculate_mip_value(f, epsilon, s_t):\n    \"\"\"\n    Calculates the MIP value for a single test case.\n    \"\"\"\n    n = 3\n    num_states = 2**n\n\n    states = [tuple(map(int, bin(i)[2:].zfill(n))) for i in range(num_states)]\n    \n    # Pre-compute Hamming distance matrix for EMD calculation\n    dist_matrix = np.zeros((num_states, num_states))\n    for i in range(num_states):\n        for j in range(num_states):\n            dist_matrix[i, j] = np.sum(np.array(states[i]) != np.array(states[j]))\n\n    # Step 1: Compute the full effect repertoire p_full\n    d = f(s_t)\n    p_full = get_p_dist(d, epsilon, n)\n\n    # Step 2: Enumerate bipartitions\n    partitions = [\n        ([0], [1, 2]),  # {A} | {B,C}\n        ([1], [0, 2]),  # {B} | {A,C}\n        ([2], [0, 1]),  # {C} | {A,B}\n    ]\n\n    min_emd = np.inf\n    s_t_arr = np.array(s_t)\n\n    # Iterate through each partition to find the minimum EMD\n    for part_X_indices, part_Y_indices in partitions:\n        \n        # --- Construct p_part for the current partition ---\n\n        # 1. Compute marginal distribution for part X\n        num_Y_states_to_avg = 2**len(part_Y_indices)\n        avg_dist_for_X = np.zeros(num_states)\n        y_states = [tuple(map(int, bin(i)[2:].zfill(len(part_Y_indices)))) for i in range(num_Y_states_to_avg)]\n\n        for y_state in y_states:\n            current_s_t = np.zeros(n, dtype=int)\n            current_s_t[part_X_indices] = s_t_arr[part_X_indices]\n            current_s_t[part_Y_indices] = y_state\n            \n            d_noised = f(tuple(current_s_t))\n            p_noised = get_p_dist(d_noised, epsilon, n)\n            avg_dist_for_X += p_noised\n        avg_dist_for_X /= num_Y_states_to_avg\n\n        marginal_p_X = {}\n        for i, state in enumerate(states):\n            x_state_key = tuple(np.array(state)[part_X_indices])\n            marginal_p_X[x_state_key] = marginal_p_X.get(x_state_key, 0) + avg_dist_for_X[i]\n\n        # 2. Compute marginal distribution for part Y\n        num_X_states_to_avg = 2**len(part_X_indices)\n        avg_dist_for_Y = np.zeros(num_states)\n        x_states = [tuple(map(int, bin(i)[2:].zfill(len(part_X_indices)))) for i in range(num_X_states_to_avg)]\n        \n        for x_state in x_states:\n            current_s_t = np.zeros(n, dtype=int)\n            current_s_t[part_Y_indices] = s_t_arr[part_Y_indices]\n            current_s_t[part_X_indices] = x_state\n            \n            d_noised = f(tuple(current_s_t))\n            p_noised = get_p_dist(d_noised, epsilon, n)\n            avg_dist_for_Y += p_noised\n        avg_dist_for_Y /= num_X_states_to_avg\n\n        marginal_p_Y = {}\n        for i, state in enumerate(states):\n            y_state_key = tuple(np.array(state)[part_Y_indices])\n            marginal_p_Y[y_state_key] = marginal_p_Y.get(y_state_key, 0) + avg_dist_for_Y[i]\n\n        # 3. Construct p_part by multiplying marginals\n        p_part = np.zeros(num_states)\n        for i, state in enumerate(states):\n            x_state_key = tuple(np.array(state)[part_X_indices])\n            y_state_key = tuple(np.array(state)[part_Y_indices])\n            p_part[i] = marginal_p_X[x_state_key] * marginal_p_Y[y_state_key]\n\n        # --- Step 3: Compute EMD using Linear Programming ---\n        c = dist_matrix.flatten()\n        \n        A_eq = np.zeros((num_states * 2, num_states * num_states))\n        b_eq = np.zeros(num_states * 2)\n\n        for i in range(num_states):\n            A_eq[i, i * num_states:(i + 1) * num_states] = 1\n            b_eq[i] = p_full[i]\n        \n        for j in range(num_states):\n            A_eq[num_states + j, j::num_states] = 1\n            b_eq[num_states + j] = p_part[j]\n            \n        bounds = (0, None)\n        \n        # Suppress potential minor precision issues in linprog\n        # that might be flagged as problems\n        b_eq = np.clip(b_eq, 0, None)\n        p_full_sum = np.sum(p_full)\n        p_part_sum = np.sum(p_part)\n        b_eq[:num_states] /= p_full_sum\n        b_eq[num_states:] /= p_part_sum\n\n        res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n        \n        emd_val = res.fun\n        min_emd = min(min_emd, emd_val)\n            \n    return min_emd\n\nsolve()\n```"
        }
    ]
}