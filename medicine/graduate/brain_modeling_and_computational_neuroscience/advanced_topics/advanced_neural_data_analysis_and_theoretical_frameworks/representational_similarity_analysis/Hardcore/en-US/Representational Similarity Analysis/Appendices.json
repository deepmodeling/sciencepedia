{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of Representational Similarity Analysis (RSA) is the Representational Dissimilarity Matrix (RDM), which provides a snapshot of the geometry of neural representations. This first exercise guides you through the fundamental process of constructing an RDM from a set of multivoxel activation patterns using the familiar Euclidean distance. By calculating pairwise distances and organizing them into a matrix, you will develop a concrete understanding of how abstract experimental categories, like visual objects or semantic concepts, manifest as a distinct block-like structure within the RDM . This practice is essential for building intuition about how neural data can be transformed to reveal underlying cognitive structure.",
            "id": "4015383",
            "problem": "A core tool in Representational Similarity Analysis (RSA) is the representational dissimilarity matrix (RDM), which summarizes pairwise dissimilarities between condition-evoked multivoxel patterns. Consider the condition-by-voxel response matrix $X \\in \\mathbb{R}^{4 \\times 3}$ for $4$ experimental conditions measured over $3$ voxels:\n$$\nX \\;=\\;\n\\begin{pmatrix}\n5 & 1 & 0 \\\\\n4 & 2 & 0 \\\\\n0 & 4 & 5 \\\\\n1 & 5 & 4\n\\end{pmatrix}.\n$$\nAssume conditions $1$ and $2$ belong to category $\\mathcal{A}$ and conditions $3$ and $4$ belong to category $\\mathcal{B}$. Using the Euclidean distance across voxel responses, compute the $4 \\times 4$ RDM whose $(i,j)$ entry is the distance between the multivoxel patterns of conditions $i$ and $j$. Then, quantify category separability by first computing the within-category average dissimilarity as the mean of the distances for the pairs $\\{(1,2),(3,4)\\}$, and the between-category average dissimilarity as the mean of the distances for the pairs $\\{(1,3),(1,4),(2,3),(2,4)\\}$. Define the separability index\n$$\nS \\;=\\; \\frac{d_{\\mathrm{between}} - d_{\\mathrm{within}}}{d_{\\mathrm{within}}}.\n$$\nDiscuss how the structure of the RDM reflects category separability in this dataset, based on these distances. Report the single numerical value of S as your final answer, rounded to $4$ significant figures. The answer is a dimensionless ratio.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- The condition-by-voxel response matrix $X \\in \\mathbb{R}^{4 \\times 3}$ is given:\n$$\nX \\;=\\;\n\\begin{pmatrix}\n5 & 1 & 0 \\\\\n4 & 2 & 0 \\\\\n0 & 4 & 5 \\\\\n1 & 5 & 4\n\\end{pmatrix}\n$$\n- The problem involves $4$ conditions and $3$ voxels.\n- Category memberships are defined: conditions $1$ and $2$ belong to category $\\mathcal{A}$, and conditions $3$ and $4$ belong to category $\\mathcal{B}$.\n- The dissimilarity measure is the Euclidean distance.\n- The task is to compute the $4 \\times 4$ representational dissimilarity matrix (RDM).\n- The within-category average dissimilarity, $d_{\\mathrm{within}}$, is the mean of distances for the pairs $\\{(1,2),(3,4)\\}$.\n- The between-category average dissimilarity, $d_{\\mathrm{between}}$, is the mean of distances for the pairs $\\{(1,3),(1,4),(2,3),(2,4)\\}$.\n- The separability index is defined as $S = \\frac{d_{\\mathrm{between}} - d_{\\mathrm{within}}}{d_{\\mathrm{within}}}$.\n- The final answer required is the numerical value of $S$, rounded to $4$ significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as Representational Similarity Analysis (RSA), RDMs, and category separability are standard concepts in computational neuroscience and brain modeling. The problem is well-posed, providing all necessary data (the matrix $X$), definitions (category assignments), and formulas (Euclidean distance, $d_{\\mathrm{within}}$, $d_{\\mathrm{between}}$, and $S$) to arrive at a unique solution. The language is objective and precise. The problem is self-contained, consistent, and does not violate any fundamental principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution\nThe multivoxel response patterns for the $4$ conditions are the row vectors of the matrix $X$. Let these vectors be denoted by $\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4 \\in \\mathbb{R}^3$:\n$$\n\\mathbf{x}_1 = (5, 1, 0)\n$$\n$$\n\\mathbf{x}_2 = (4, 2, 0)\n$$\n$$\n\\mathbf{x}_3 = (0, 4, 5)\n$$\n$$\n\\mathbf{x}_4 = (1, 5, 4)\n$$\nThe $(i,j)$ entry of the RDM is the Euclidean distance $d_{ij}$ between the response vectors $\\mathbf{x}_i$ and $\\mathbf{x}_j$. The formula for the Euclidean distance between two vectors $\\mathbf{a}=(a_1, a_2, a_3)$ and $\\mathbf{b}=(b_1, b_2, b_3)$ is $d(\\mathbf{a}, \\mathbf{b}) = \\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + (a_3-b_3)^2}$.\n\nFirst, we compute the pairwise distances required for the analysis. These distances form the off-diagonal elements of the $4 \\times 4$ RDM. The RDM is symmetric ($d_{ij} = d_{ji}$) with zeros on the diagonal ($d_{ii}=0$).\n\nThe within-category distances are $d_{12}$ (for category $\\mathcal{A}$) and $d_{34}$ (for category $\\mathcal{B}$):\n$$\nd_{12} = \\sqrt{(5-4)^2 + (1-2)^2 + (0-0)^2} = \\sqrt{1^2 + (-1)^2 + 0^2} = \\sqrt{1+1} = \\sqrt{2}\n$$\n$$\nd_{34} = \\sqrt{(0-1)^2 + (4-5)^2 + (5-4)^2} = \\sqrt{(-1)^2 + (-1)^2 + 1^2} = \\sqrt{1+1+1} = \\sqrt{3}\n$$\n\nThe between-category distances are $d_{13}$, $d_{14}$, $d_{23}$, and $d_{24}$:\n$$\nd_{13} = \\sqrt{(5-0)^2 + (1-4)^2 + (0-5)^2} = \\sqrt{5^2 + (-3)^2 + (-5)^2} = \\sqrt{25+9+25} = \\sqrt{59}\n$$\n$$\nd_{14} = \\sqrt{(5-1)^2 + (1-5)^2 + (0-4)^2} = \\sqrt{4^2 + (-4)^2 + (-4)^2} = \\sqrt{16+16+16} = \\sqrt{48}\n$$\n$$\nd_{23} = \\sqrt{(4-0)^2 + (2-4)^2 + (0-5)^2} = \\sqrt{4^2 + (-2)^2 + (-5)^2} = \\sqrt{16+4+25} = \\sqrt{45}\n$$\n$$\nd_{24} = \\sqrt{(4-1)^2 + (2-5)^2 + (0-4)^2} = \\sqrt{3^2 + (-3)^2 + (-4)^2} = \\sqrt{9+9+16} = \\sqrt{34}\n$$\n\nThe full RDM is:\n$$\n\\mathrm{RDM} =\n\\begin{pmatrix}\n0 & \\sqrt{2} & \\sqrt{59} & \\sqrt{48} \\\\\n\\sqrt{2} & 0 & \\sqrt{45} & \\sqrt{34} \\\\\n\\sqrt{59} & \\sqrt{45} & 0 & \\sqrt{3} \\\\\n\\sqrt{48} & \\sqrt{34} & \\sqrt{3} & 0\n\\end{pmatrix}\n$$\n\nNext, we compute the within-category and between-category average dissimilarities.\nThe within-category average dissimilarity, $d_{\\mathrm{within}}$, is the mean of the dissimilarities for pairs within the same category, which are $(1,2)$ and $(3,4)$.\n$$\nd_{\\mathrm{within}} = \\frac{d_{12} + d_{34}}{2} = \\frac{\\sqrt{2} + \\sqrt{3}}{2}\n$$\nThe between-category average dissimilarity, $d_{\\mathrm{between}}$, is the mean of dissimilarities for pairs in different categories, which are $(1,3)$, $(1,4)$, $(2,3)$, and $(2,4)$.\n$$\nd_{\\mathrm{between}} = \\frac{d_{13} + d_{14} + d_{23} + d_{24}}{4} = \\frac{\\sqrt{59} + \\sqrt{48} + \\sqrt{45} + \\sqrt{34}}{4}\n$$\n\nThe structure of the RDM reflects category separability. If the conditions are ordered by category, the RDM takes on a block structure. The on-diagonal blocks contain within-category dissimilarities (e.g., the top-left $2 \\times 2$ block for category $\\mathcal{A}$ and the bottom-right $2 \\times 2$ block for category $\\mathcal{B}$). The off-diagonal blocks contain between-category dissimilarities. For this dataset, the within-category dissimilarities ($\\sqrt{2} \\approx 1.414$ and $\\sqrt{3} \\approx 1.732$) are considerably smaller than the between-category dissimilarities ($\\sqrt{59} \\approx 7.68$, $\\sqrt{48} \\approx 6.93$, $\\sqrt{45} \\approx 6.71$, $\\sqrt{34} \\approx 5.83$). This visual pattern in the RDM, with low values on the diagonal blocks and high values on the off-diagonal blocks, indicates that the neural representation patterns for items within the same category are more similar to each other than to patterns for items in a different category. This implies a strong categorical structure is present in the data.\n\nFinally, we compute the separability index $S$:\n$$\nS = \\frac{d_{\\mathrm{between}} - d_{\\mathrm{within}}}{d_{\\mathrm{within}}}\n$$\nWe substitute the expressions for $d_{\\mathrm{within}}$ and $d_{\\mathrm{between}}$:\n$$\nS = \\frac{\\frac{\\sqrt{59} + \\sqrt{48} + \\sqrt{45} + \\sqrt{34}}{4} - \\frac{\\sqrt{2} + \\sqrt{3}}{2}}{\\frac{\\sqrt{2} + \\sqrt{3}}{2}}\n$$\nTo obtain the final numerical answer, we evaluate these expressions:\n$$\nd_{\\mathrm{within}} \\approx \\frac{1.41421 + 1.73205}{2} = \\frac{3.14626}{2} \\approx 1.57313\n$$\n$$\nd_{\\mathrm{between}} \\approx \\frac{7.68115 + 6.92820 + 6.70820 + 5.83095}{4} = \\frac{27.14850}{4} \\approx 6.78713\n$$\nNow, we compute $S$:\n$$\nS \\approx \\frac{6.78713 - 1.57313}{1.57313} = \\frac{5.21400}{1.57313} \\approx 3.31441\n$$\nRounding to $4$ significant figures, we get $S \\approx 3.314$. The large positive value of $S$ quantitatively confirms the strong category separability observed qualitatively from the RDM structure.",
            "answer": "$$\\boxed{3.314}$$"
        },
        {
            "introduction": "While Euclidean distance offers an intuitive starting point, it operates on the simplifying assumption that all measurement channels (e.g., voxels) are independent and have equal noise. In biological reality, this is rarely the case. This practice introduces a more statistically robust metric, the Mahalanobis distance, which accounts for the estimated noise covariance across voxels. You will work through the core computation of \"whitening\" the neural patterns, which involves calculating the inverse square root of the noise covariance matrix. This exercise provides critical hands-on experience with the linear algebra that allows RSA to discount noisy channels and reveal a clearer picture of the representational geometry .",
            "id": "4015346",
            "problem": "In Representational Similarity Analysis (RSA), a common approach to quantify dissimilarity between multivariate neural patterns is to use the Euclidean distance in a whitened space, which corresponds to the Mahalanobis distance under an empirically estimated noise covariance. Consider a toy dataset of voxel patterns across $2$ voxels for $3$ experimental conditions, with condition-specific response vectors given by $x_{1} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$, $x_{2} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$, and $x_{3} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$. Suppose the empirical noise covariance across voxels estimated from baseline measurements is \n$$\n\\Sigma = \\begin{pmatrix} 2 & 0.5 \\\\ 0.5 & 2 \\end{pmatrix}.\n$$\nStarting from the definitions of covariance and the spectral properties of symmetric positive-definite matrices, construct the inverse square root $\\,\\Sigma^{-1/2}\\,$ via orthogonal diagonalization, use it to whiten the patterns $y_{i} = \\Sigma^{-1/2} x_{i}$, compute the pairwise Euclidean distances $d_{12}$, $d_{13}$, and $d_{23}$ between the whitened patterns, and then compute the arithmetic mean of these three distances. Round your final numerical answer to four significant figures. Express the final quantity as a dimensionless number.",
            "solution": "Representational Similarity Analysis (RSA) compares condition-specific multivariate patterns via a dissimilarity measure. When an empirical noise covariance $\\,\\Sigma\\,$ is available, the Mahalanobis distance between two patterns $\\,x\\,$ and $\\,x'\\,$ is \n$$\nd_{M}(x,x') \\equiv \\left( (x - x')^{\\top} \\Sigma^{-1} (x - x') \\right)^{1/2},\n$$\nwhich is equal to the Euclidean distance between whitened patterns $\\,y = \\Sigma^{-1/2} x\\,$ and $\\,y' = \\Sigma^{-1/2} x'\\,$:\n$$\n\\|y - y'\\|_{2} = \\left\\| \\Sigma^{-1/2} (x - x') \\right\\|_{2} = d_{M}(x,x').\n$$\nThus, to compute whitened distances, we require $\\,\\Sigma^{-1/2}\\,$.\n\nFundamental base: The empirical covariance matrix $\\,\\Sigma\\,$ is symmetric positive-definite. By the spectral theorem, any symmetric positive-definite matrix admits an orthogonal diagonalization $\\,\\Sigma = U \\Lambda U^{\\top}\\,$, where $\\,U\\,$ is orthonormal and $\\,\\Lambda\\,$ is diagonal with positive entries. It follows that\n$$\n\\Sigma^{-1/2} = U \\Lambda^{-1/2} U^{\\top},\n$$\nwhere $\\,\\Lambda^{-1/2}\\,$ is the diagonal matrix formed by taking the inverse square roots of the eigenvalues.\n\nStep 1: Eigen-decomposition of $\\,\\Sigma\\,$. The matrix \n$$\n\\Sigma = \\begin{pmatrix} 2 & 0.5 \\\\ 0.5 & 2 \\end{pmatrix}\n$$\nis of the form $\\,\\begin{pmatrix} a & b \\\\ b & a \\end{pmatrix}\\,$ with $\\,a = 2\\,$ and $\\,b = 0.5\\,$. Its eigenvectors are $\\,u_{1} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\,$ and $\\,u_{2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\,$, with corresponding eigenvalues \n$$\n\\lambda_{1} = a + b = 2.5, \\quad \\lambda_{2} = a - b = 1.5.\n$$\nThus,\n$$\nU = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}, \\quad \\Lambda = \\begin{pmatrix} 2.5 & 0 \\\\ 0 & 1.5 \\end{pmatrix}.\n$$\nDefine $\\,\\alpha = \\lambda_{1}^{-1/2} = \\frac{1}{\\sqrt{2.5}} = \\frac{\\sqrt{2}}{\\sqrt{5}}\\,$ and $\\,\\beta = \\lambda_{2}^{-1/2} = \\frac{1}{\\sqrt{1.5}} = \\frac{\\sqrt{2}}{\\sqrt{3}}\\,$. Then\n$$\n\\Lambda^{-1/2} = \\begin{pmatrix} \\alpha & 0 \\\\ 0 & \\beta \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\Sigma^{-1/2} = U \\Lambda^{-1/2} U^{\\top} = \\frac{1}{2} \n\\begin{pmatrix}\n\\alpha + \\beta & \\alpha - \\beta \\\\\n\\alpha - \\beta & \\alpha + \\beta\n\\end{pmatrix}\n= \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{5}} + \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{5}} - \\frac{1}{\\sqrt{3}} \\\\\n\\frac{1}{\\sqrt{5}} - \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{5}} + \\frac{1}{\\sqrt{3}}\n\\end{pmatrix}.\n$$\n\nStep 2: Whiten the patterns $\\,y_{i} = \\Sigma^{-1/2} x_{i}\\,$. Denote $\\,a \\equiv \\frac{1}{\\sqrt{5}} + \\frac{1}{\\sqrt{3}}\\,$ and $\\,b \\equiv \\frac{1}{\\sqrt{5}} - \\frac{1}{\\sqrt{3}}\\,$ so that\n$$\n\\Sigma^{-1/2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} a & b \\\\ b & a \\end{pmatrix}.\n$$\nCompute each $\\,y_{i}$:\n$$\ny_{1} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} a & b \\\\ b & a \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n= \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3a + b \\\\ 3b + a \\end{pmatrix}\n= \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{4}{\\sqrt{5}} + \\frac{2}{\\sqrt{3}} \\\\ \\frac{4}{\\sqrt{5}} - \\frac{2}{\\sqrt{3}} \\end{pmatrix},\n$$\n$$\ny_{2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} a & b \\\\ b & a \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n= \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2(a + b) \\\\ 2(a + b) \\end{pmatrix}\n= \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{4}{\\sqrt{5}} \\\\ \\frac{4}{\\sqrt{5}} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{\\sqrt{10}} \\\\ \\frac{4}{\\sqrt{10}} \\end{pmatrix},\n$$\n$$\ny_{3} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} a & b \\\\ b & a \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\n= \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -b \\\\ -a \\end{pmatrix}\n= \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{\\sqrt{3}} - \\frac{1}{\\sqrt{5}} \\\\ -\\left( \\frac{1}{\\sqrt{5}} + \\frac{1}{\\sqrt{3}} \\right) \\end{pmatrix}.\n$$\n\nStep 3: Compute pairwise Euclidean distances in whitened space. The Euclidean distance between $\\,y_{i}\\,$ and $\\,y_{j}\\,$ is $\\,d_{ij} = \\|y_{i} - y_{j}\\|_{2}\\,$.\n\nFirst,\n$$\ny_{1} - y_{2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{4}{\\sqrt{5}} + \\frac{2}{\\sqrt{3}} \\\\ \\frac{4}{\\sqrt{5}} - \\frac{2}{\\sqrt{3}} \\end{pmatrix}\n- \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{4}{\\sqrt{5}} \\\\ \\frac{4}{\\sqrt{5}} \\end{pmatrix}\n= \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{2}{\\sqrt{3}} \\\\ -\\frac{2}{\\sqrt{3}} \\end{pmatrix}.\n$$\nThus,\n$$\nd_{12}^{2} = \\left\\| \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{2}{\\sqrt{3}} \\\\ -\\frac{2}{\\sqrt{3}} \\end{pmatrix} \\right\\|_{2}^{2}\n= \\frac{1}{2} \\left( \\frac{4}{3} + \\frac{4}{3} \\right) = \\frac{4}{3}, \\quad d_{12} = \\frac{2}{\\sqrt{3}}.\n$$\n\nNext,\n$$\ny_{1} - y_{3} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{4}{\\sqrt{5}} + \\frac{2}{\\sqrt{3}} + b \\\\ \\frac{4}{\\sqrt{5}} - \\frac{2}{\\sqrt{3}} + a \\end{pmatrix}\n= \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\sqrt{5} + \\frac{1}{\\sqrt{3}} \\\\ \\sqrt{5} - \\frac{1}{\\sqrt{3}} \\end{pmatrix},\n$$\nso\n$$\nd_{13}^{2} = \\frac{1}{2} \\left[ \\left( \\sqrt{5} + \\frac{1}{\\sqrt{3}} \\right)^{2} + \\left( \\sqrt{5} - \\frac{1}{\\sqrt{3}} \\right)^{2} \\right]\n= \\frac{1}{2} \\cdot 2 \\left( 5 + \\frac{1}{3} \\right) = \\frac{16}{3},\n\\quad d_{13} = \\frac{4}{\\sqrt{3}}.\n$$\n\nSimilarly,\n$$\ny_{2} - y_{3} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{4}{\\sqrt{5}} + b \\\\ \\frac{4}{\\sqrt{5}} + a \\end{pmatrix}\n= \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\sqrt{5} - \\frac{1}{\\sqrt{3}} \\\\ \\sqrt{5} + \\frac{1}{\\sqrt{3}} \\end{pmatrix},\n$$\nyielding\n$$\nd_{23}^{2} = \\frac{1}{2} \\left[ \\left( \\sqrt{5} - \\frac{1}{\\sqrt{3}} \\right)^{2} + \\left( \\sqrt{5} + \\frac{1}{\\sqrt{3}} \\right)^{2} \\right] = \\frac{16}{3},\n\\quad d_{23} = \\frac{4}{\\sqrt{3}}.\n$$\n\nStep 4: Compute the arithmetic mean of the three distances,\n$$\n\\bar{d} = \\frac{d_{12} + d_{13} + d_{23}}{3} = \\frac{ \\frac{2}{\\sqrt{3}} + \\frac{4}{\\sqrt{3}} + \\frac{4}{\\sqrt{3}} }{3} = \\frac{10}{3 \\sqrt{3}}.\n$$\nNumerically, $\\,\\sqrt{3} \\approx 1.7320508075688772\\,$, so\n$$\n\\bar{d} \\approx \\frac{10}{3 \\cdot 1.7320508075688772} \\approx \\frac{10}{5.196152422706632} \\approx 1.924500897 \\ldots\n$$\nRounded to four significant figures,\n$$\n\\bar{d} \\approx 1.925.\n$$\nThis final quantity is dimensionless.",
            "answer": "$$\\boxed{1.925}$$"
        },
        {
            "introduction": "After learning to compute representational distances, a deeper question emerges: what does the magnitude of a dissimilarity value actually tell us about the information encoded in a brain region? This final practice moves from computation to theoretical insight by forging a direct link between RSA and decoding (or Multivariate Pattern Analysis, MVPA). You will derive the analytical relationship between the squared Mahalanobis distance, $d^2$, and the maximum achievable accuracy of a linear classifier. This powerful result  demonstrates that the geometric distances used in RSA are not arbitrary; they are quantitatively tied to how effectively information can be read out from neural population activity, solidifying the role of RSA as a tool for understanding neural information processing.",
            "id": "4190802",
            "problem": "In a two-condition neuroimaging experiment, assume each condition elicits a multivariate pattern of responses across $p$ voxels or channels, modeled as multivariate Gaussian with shared covariance. Specifically, let condition $1$ produce responses $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma})$ and condition $2$ produce responses $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{2}, \\boldsymbol{\\Sigma})$, with equal priors and $\\boldsymbol{\\Sigma}$ positive definite. Consider the Bayes-optimal linear decoder under these assumptions (equivalent to Linear Discriminant Analysis (LDA)), which chooses a linear decision boundary to minimize classification error. Let the squared Mahalanobis distance between the two condition means be defined as $d^{2} = (\\boldsymbol{\\mu}_{2} - \\boldsymbol{\\mu}_{1})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_{2} - \\boldsymbol{\\mu}_{1})$. Using only the core definitions of multivariate Gaussian distributions, the Bayes-optimal linear decision rule under equal covariance, and the definition of $d^{2}$, derive from first principles a closed-form analytic expression for the expected decoding accuracy of the optimal linear classifier as a function of $d^{2}$. Define any special functions you use. Finally, explain how this functional dependence formally links decoding accuracy to the dissimilarities used in Representational Similarity Analysis (RSA), which commonly employs the squared Mahalanobis distance between condition-specific patterns. Provide the final answer as a single closed-form analytic expression in terms of $d^{2}$ only; no numerical rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded in standard statistical pattern recognition theory, well-posed with a clear objective, and provides a complete and consistent set of premises.\n\nWe begin by establishing the theoretical framework for the derivation. We are given two conditions, which we label $C_1$ and $C_2$. The neural response patterns $\\mathbf{x} \\in \\mathbb{R}^p$ for these conditions are modeled by multivariate Gaussian distributions with a shared covariance matrix $\\boldsymbol{\\Sigma}$:\n- Condition $C_1$: $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma})$\n- Condition $C_2$: $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{2}, \\boldsymbol{\\Sigma})$\n\nThe problem states that the prior probabilities are equal, i.e., $P(C_1) = P(C_2) = \\frac{1}{2}$. The Bayes-optimal classifier minimizes the probability of misclassification. Under equal priors, this is achieved by assigning a given observation $\\mathbf{x}$ to the class with the higher posterior probability, which simplifies to assigning it to the class with the higher likelihood. The decision rule is:\nAssign $\\mathbf{x}$ to $C_1$ if $p(\\mathbf{x}|C_1) > p(\\mathbf{x}|C_2)$, and to $C_2$ otherwise.\n\nTo make the comparison tractable, we work with the log-likelihood ratio. The probability density function for a multivariate normal distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ is\n$$p(\\mathbf{x}|\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{p/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)$$\nThe log-likelihood is\n$$\\ln p(\\mathbf{x}|\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = -\\frac{p}{2}\\ln(2\\pi) -\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| -\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})$$\nThe decision rule is based on the sign of the log-likelihood ratio:\n$$\\ln\\left(\\frac{p(\\mathbf{x}|C_1)}{p(\\mathbf{x}|C_2)}\\right) = \\ln p(\\mathbf{x}|C_1) - \\ln p(\\mathbf{x}|C_2) > 0$$\nSubstituting the expressions for the log-likelihoods, the constant terms cancel out, leaving:\n$$\\frac{1}{2} \\left[ (\\mathbf{x}-\\boldsymbol{\\mu}_2)^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_2) - (\\mathbf{x}-\\boldsymbol{\\mu}_1)^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_1) \\right] > 0$$\nExpanding the quadratic forms:\n$$(\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - 2\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_2 + \\boldsymbol{\\mu}_2^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_2) - (\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - 2\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_1^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1) > 0$$\nThe term $\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}$ cancels, resulting in a linear decision boundary. The inequality simplifies to:\n$$2\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) - (\\boldsymbol{\\mu}_1^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_2) > 0$$\nThis defines a linear discriminant function. Let us define a projection vector $\\mathbf{w} = \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)$. The decision rule is to classify as $C_1$ if $\\mathbf{w}^{\\top}\\mathbf{x} > b$, where $b = \\frac{1}{2}(\\boldsymbol{\\mu}_1^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_2) = \\frac{1}{2}(\\boldsymbol{\\mu}_1+\\boldsymbol{\\mu}_2)^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2) = \\frac{1}{2}(\\boldsymbol{\\mu}_1+\\boldsymbol{\\mu}_2)^{\\top}\\mathbf{w}$.\n\nThe core of the analysis is to examine the distribution of the scalar projection $y = \\mathbf{w}^{\\top}\\mathbf{x}$. Since $\\mathbf{x}$ is a Gaussian random vector and $y$ is a linear transformation of $\\mathbf{x}$, $y$ is a scalar Gaussian random variable. We need to find its mean and variance under each condition.\n\nIf $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma})$, then $y \\sim \\mathcal{N}(E[y|C_k], Var(y|C_k))$.\nThe conditional means are:\n$$E[y|C_1] = m_1 = \\mathbf{w}^{\\top}\\boldsymbol{\\mu}_1 = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1$$\n$$E[y|C_2] = m_2 = \\mathbf{w}^{\\top}\\boldsymbol{\\mu}_2 = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_2$$\nThe conditional variance is the same for both classes because the covariance $\\boldsymbol{\\Sigma}$ is shared:\n$$Var(y) = \\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w} = ((\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^{\\top}\\boldsymbol{\\Sigma}^{-1})\\boldsymbol{\\Sigma}(\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2))$$\nSince $\\boldsymbol{\\Sigma}$ is symmetric, $\\boldsymbol{\\Sigma}^{-1}$ is also symmetric, thus $(\\boldsymbol{\\Sigma}^{-1})^{\\top} = \\boldsymbol{\\Sigma}^{-1}$.\n$$Var(y) = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)$$\nThis quantity is precisely the squared Mahalanobis distance between the means, $d^2$, noting that $(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^{\\top} = -(\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)^{\\top}$ and the overall expression is a quadratic form giving $(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^{\\top}\\mathbf{A}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) = (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)^{\\top}\\mathbf{A}(\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)$.\nSo, $Var(y) = d^2$, where $d^2 = (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)$.\nThe standard deviation of $y$ is $\\sqrt{d^2}$. We may assume $d \\ge 0$.\n\nThe difference between the conditional means of the projected data is:\n$$m_1 - m_2 = \\mathbf{w}^{\\top}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) = d^2$$\nThe decision threshold for $y$ is at $b = \\frac{1}{2}(\\mathbf{w}^{\\top}\\boldsymbol{\\mu}_1 + \\mathbf{w}^{\\top}\\boldsymbol{\\mu}_2) = \\frac{1}{2}(m_1 + m_2)$. This is the midpoint between the two projected means, as expected for an optimal classifier with equal priors.\n\nThe expected decoding accuracy is the average of the correct classification probabilities for each class, weighted by their priors:\n$$Accuracy = P(\\text{correct}) = P(C_1)P(\\text{correct}|C_1) + P(C_2)P(\\text{correct}|C_2)$$\n$$Accuracy = \\frac{1}{2} P(y > b | C_1) + \\frac{1}{2} P(y < b | C_2)$$\nWe standardize the variables to compute these probabilities. Let $Z \\sim \\mathcal{N}(0, 1)$ be a standard normal random variable.\nFor condition $C_1$, we have $y \\sim \\mathcal{N}(m_1, d^2)$. The probability of correct classification is:\n$$P(y > b | C_1) = P\\left(\\frac{y-m_1}{\\sqrt{d^2}} > \\frac{b-m_1}{\\sqrt{d^2}}\\right) = P\\left(Z > \\frac{\\frac{1}{2}(m_1+m_2) - m_1}{\\sqrt{d^2}}\\right)$$\n$$= P\\left(Z > \\frac{\\frac{1}{2}(m_2-m_1)}{\\sqrt{d^2}}\\right) = P\\left(Z > \\frac{-\\frac{1}{2}d^2}{\\sqrt{d^2}}\\right) = P\\left(Z > -\\frac{\\sqrt{d^2}}{2}\\right)$$\nBy the symmetry of the standard normal distribution, $P(Z > -a) = P(Z < a)$, which is given by the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(a)$.\nThus, $P(\\text{correct}|C_1) = \\Phi\\left(\\frac{\\sqrt{d^2}}{2}\\right)$.\n\nFor condition $C_2$, we have $y \\sim \\mathcal{N}(m_2, d^2)$. The probability of correct classification is:\n$$P(y < b | C_2) = P\\left(\\frac{y-m_2}{\\sqrt{d^2}} < \\frac{b-m_2}{\\sqrt{d^2}}\\right) = P\\left(Z < \\frac{\\frac{1}{2}(m_1+m_2) - m_2}{\\sqrt{d^2}}\\right)$$\n$$= P\\left(Z < \\frac{\\frac{1}{2}(m_1-m_2)}{\\sqrt{d^2}}\\right) = P\\left(Z < \\frac{\\frac{1}{2}d^2}{\\sqrt{d^2}}\\right) = P\\left(Z < \\frac{\\sqrt{d^2}}{2}\\right) = \\Phi\\left(\\frac{\\sqrt{d^2}}{2}\\right)$$\nThe special function used is $\\Phi(z)$, the CDF of the standard normal distribution, defined as:\n$$\\Phi(z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt$$\nThe total expected accuracy is:\n$$Accuracy = \\frac{1}{2} \\Phi\\left(\\frac{\\sqrt{d^2}}{2}\\right) + \\frac{1}{2} \\Phi\\left(\\frac{\\sqrt{d^2}}{2}\\right) = \\Phi\\left(\\frac{\\sqrt{d^2}}{2}\\right)$$\nThis expression gives the expected decoding accuracy of the Bayes-optimal linear classifier solely as a function of the squared Mahalanobis distance $d^2$.\n\nThe formal link to Representational Similarity Analysis (RSA) is now evident. In RSA, a central object is the Representational Dissimilarity Matrix (RDM), which quantifies the dissimilarity between neural patterns elicited by different experimental conditions. The squared Mahalanobis distance, $d^2$, is a principled and widely used measure of dissimilarity for constructing RDMs, particularly because it is sensitive to multivariate information and invariant to affine transformations of the response space (when the covariance is estimated and appropriately transformed).\n\nOur derivation establishes a direct, quantitative relationship between this dissimilarity measure and a key information-theoretic quantity: the discriminability of the neural representations. The resulting equation, Accuracy $= \\Phi\\left(\\frac{\\sqrt{d^2}}{2}\\right)$, shows that decoding accuracy is a monotonically increasing function of $\\sqrt{d^2}$ (and thus of $d^2$). This means that a larger dissimilarity between two conditions' neural patterns, as measured by $d^2$, directly implies that they are more separable in the neural state space and can be distinguished with higher accuracy by an optimal linear decoder. Therefore, the Mahalanobis distance in RSA is not merely a geometric descriptor; it is functionally coupled to the information content of the neural code, specifically the information that supports linear decoding.",
            "answer": "$$\\boxed{\\Phi\\left(\\frac{\\sqrt{d^2}}{2}\\right)}$$"
        }
    ]
}