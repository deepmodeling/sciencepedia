## Introduction
How does the brain transform sensory input into meaningful, abstract concepts? Answering this question requires methods that can characterize and compare the complex, high-dimensional patterns of neural activity that form the basis of cognition. Representational Similarity Analysis (RSA) has emerged as a powerful and versatile framework designed to do just that. It addresses the fundamental challenge of comparing representations across different brain regions, individuals, species, and even between biological brains and artificial intelligence models, by abstracting them into a common language: the geometry of their relationships.

This article provides a comprehensive guide to the RSA framework. In the first chapter, **Principles and Mechanisms**, we will dissect the core components of RSA, from constructing Representational Dissimilarity Matrices (RDMs) to the critical choices in measuring dissimilarity and performing statistical inference. The second chapter, **Applications and Interdisciplinary Connections**, will showcase how RSA is used to map information in the brain, track cognitive dynamics, and bridge the gap between neuroscience and AI. Finally, **Hands-On Practices** will offer a chance to engage with the concepts directly through guided computational exercises. By moving from theory to application, you will gain a deep understanding of how RSA provides a unique window into the structure of neural information.

## Principles and Mechanisms

### The Representational Dissimilarity Matrix: A Signature of Neural Geometry

The fundamental data structure at the heart of Representational Similarity Analysis (RSA) is the **Representational Dissimilarity Matrix (RDM)**. The RDM provides a concise, quantitative summary of the relationships between activity patterns elicited by a set of experimental conditions or stimuli. This summary forms a "signature" of the **[representational geometry](@entry_id:1130876)** within a given neural system, be it a brain region, a single neuron, or a layer in a computational model.

Formally, consider a study with $n$ distinct experimental conditions, $\mathcal{S} = \{s_1, s_2, \ldots, s_n\}$. Each condition $s_i$ evokes a multivariate response pattern, $\mathbf{r}_i \in \mathbb{R}^p$, measured across $p$ channels (e.g., voxels in fMRI, neurons in an [electrophysiological recording](@entry_id:198351), or units in a neural network). The process of RSA begins by computing a pairwise dissimilarity for every possible pair of conditions. This is achieved using a dissimilarity function, $\delta: \mathbb{R}^p \times \mathbb{R}^p \to \mathbb{R}_{\ge 0}$, which takes two activity patterns and returns a single non-negative number quantifying how different they are.

The RDM, denoted as $D$, is an $n \times n$ matrix where each entry $D_{ij}$ is the dissimilarity between the response patterns for condition $i$ and condition $j$:

$D_{ij} = \delta(\mathbf{r}_i, \mathbf{r}_j)$

By definition, any valid dissimilarity measure must be symmetric, such that $\delta(\mathbf{r}_i, \mathbf{r}_j) = \delta(\mathbf{r}_j, \mathbf{r}_i)$, and the dissimilarity of a pattern to itself must be zero, $\delta(\mathbf{r}_i, \mathbf{r}_i) = 0$. These properties directly confer two fundamental characteristics upon any RDM :

1.  **Symmetry**: The RDM is symmetric about its main diagonal, i.e., $D_{ij} = D_{ji}$. The dissimilarity between condition $i$ and $j$ is the same as that between $j$ and $i$.
2.  **Zero Diagonal**: All entries on the main diagonal are zero, i.e., $D_{ii} = 0$. A condition's representation is perfectly similar to itself.

Due to this structure, all unique dissimilarity values are contained in the upper (or lower) triangle of the matrix. For analysis, it is therefore sufficient to extract these $n(n-1)/2$ unique off-diagonal entries, typically by vectorizing the upper triangle . The diagonal and the redundant lower triangle are disregarded.

The power of the RDM lies in its abstraction. It transforms the complex, high-dimensional set of activity patterns $\{\mathbf{r}_i\}_{i=1}^n$ into a simple, two-dimensional matrix that captures their relational structure. This process intentionally discards certain information, such as the absolute activity levels of individual channels and the specific coordinate system of the measurement space, to focus on the geometry of the representations themselves .

### Measuring Dissimilarity: Choices and Consequences

The choice of dissimilarity function $\delta$ is a critical step that determines which properties of the raw activity patterns are preserved in the RDM and which are discarded. Different metrics possess different **invariance properties**, and understanding them is key to constructing RDMs that are robust to irrelevant sources of variability, such as measurement units or noise characteristics. Let us examine the properties of several common [distance measures](@entry_id:145286) .

A straightforward choice is the **Euclidean distance**, $d_{\mathrm{E}}(\mathbf{r}_i, \mathbf{r}_j) = \lVert \mathbf{r}_i - \mathbf{r}_j \rVert_2$. This metric is intuitive, corresponding to the straight-line distance between the two pattern vectors in the $p$-dimensional measurement space. The RDM constructed from Euclidean distances is invariant to [rigid motions](@entry_id:170523) of the entire [point cloud](@entry_id:1129856) of activity patterns:
-   **Translation Invariance**: Adding a constant vector $\mathbf{c}$ to all patterns ($\mathbf{r}_k' = \mathbf{r}_k + \mathbf{c}$) does not change the RDM.
-   **Rotation Invariance**: Applying an orthogonal rotation $Q$ to all patterns ($\mathbf{r}_k' = Q\mathbf{r}_k$) does not change the RDM.

However, Euclidean distance is **not invariant to scaling**. If we rescale the units of our measurement channels by a factor $a$ (i.e., $\mathbf{r}_k' = a\mathbf{r}_k$), the distances will also scale by $a$. This sensitivity to the absolute scale of the measurement units can be problematic when comparing representations across different modalities or systems where units are arbitrary.

To address this, the **[correlation distance](@entry_id:634939)** is often preferred. Defined as $d_{\mathrm{COR}}(\mathbf{r}_i, \mathbf{r}_j) = 1 - \rho(\mathbf{r}_i, \mathbf{r}_j)$, where $\rho$ is the Pearson correlation coefficient, this measure is inherently robust to certain transformations. Pearson correlation first mean-centers each vector before comparing their angle. Consequently, [correlation distance](@entry_id:634939) is invariant to:
-   **Pattern-specific Offset**: Adding a different constant to all channels within each pattern ($\mathbf{r}_i' = \mathbf{r}_i + c_i\mathbf{1}$) does not change the correlation. This makes it robust to shifts in the overall baseline activation for each stimulus.
-   **Pattern-specific Positive Scaling**: Multiplying each pattern by a different positive scalar ($\mathbf{r}_i' = a_i\mathbf{r}_i$ with $a_i > 0$) does not change the correlation. This makes it robust to fluctuations in response gain or contrast for each stimulus.

By using [correlation distance](@entry_id:634939), the resulting RDM preserves the "shape" of the activity patterns while discarding information about their mean activation and overall amplitude . This is a powerful way to achieve independence from arbitrary measurement units . A related measure, the **[cosine distance](@entry_id:635585)**, is invariant to uniform scaling but not to offsets.

A still more powerful approach for handling measurement-space properties is the **Mahalanobis distance**. This metric is particularly useful when measurement channels have heterogeneous noise levels or are correlated. The squared Mahalanobis distance is defined as $d_{\mathrm{M}}^2(\mathbf{r}_i, \mathbf{r}_j) = (\mathbf{r}_i - \mathbf{r}_j)^\top \Sigma^{-1} (\mathbf{r}_i - \mathbf{r}_j)$, where $\Sigma$ is the covariance matrix of the noise. This metric effectively "whitens" the space by down-weighting noisy or redundant channels. Its key property is its invariance to any [invertible linear transformation](@entry_id:149915) of the measurement space, provided the covariance matrix is transformed accordingly. This endows the resulting RDM with a geometry that is independent of the specific basis chosen for the measurement channels, creating a truly unitless scale  . However, it requires a reliable estimate of the [noise covariance](@entry_id:1128754) $\Sigma$, which can be challenging.

### Comparing Geometries: The Core of RSA

Once RDMs are constructed, RSA proceeds by comparing them. The central idea is to test whether two representational systems (e.g., a brain region and a computational model) organize the stimuli in a similar way. This is done by measuring the correspondence between their respective RDMs, $D^{(A)}$ and $D^{(B)}$. The standard procedure is to vectorize the unique, off-diagonal elements of each RDM and compute a correlation between the resulting vectors .

A crucial distinction arises between metric and rank-based comparisons. Using a **Pearson correlation** coefficient measures the strength of a linear relationship between the two sets of dissimilarities. A high Pearson correlation indicates that the dissimilarities in one RDM are a good linear predictor of the dissimilarities in the other.

However, a more common and arguably more powerful approach in RSA is to use a rank-based measure, such as **Spearman's [rank correlation](@entry_id:175511)** or Kendall's tau. Spearman correlation is simply the Pearson correlation computed on the rank-transformed data. The profound implication of this choice is that the comparison becomes invariant to any strictly increasing monotonic transformation of the dissimilarity values .

This invariance provides a robust epistemic foundation for RSA, especially when comparing disparate modalities like brain activity and computational models . We often do not know the precise function that maps a latent "true" dissimilarity in a neural representation to the dissimilarity we measure (e.g., from the BOLD signal). This measurement process could introduce arbitrary, non-linear distortions. As long as we can assume this distortion is monotonic (i.e., it preserves the ordering, so that larger true dissimilarities lead to larger measured dissimilarities), using Spearman correlation allows us to bypass this unknown function. It tests for a shared **ordinal structure**â€”a correspondence in the rank-ordering of stimulus-pair dissimilarities. Two models that are non-linearly but monotonically related to a true underlying geometry will both achieve a perfect Spearman correlation with it, whereas their Pearson correlations might differ substantially. This focus on abstract, scale-free relational structure is a defining feature of the RSA framework.

### Statistical Inference and Interpretation

A high correlation between two RDMs suggests a similarity in [representational geometry](@entry_id:1130876), but to draw a valid conclusion, we must assess its [statistical significance](@entry_id:147554). This presents a unique challenge because the entries of an RDM are not independent observations. Any dissimilarity $D_{ij}$ shares its underlying data (the patterns $\mathbf{r}_i$ and $\mathbf{r}_j$) with all other dissimilarities involving either condition $i$ or condition $j$. For example, $D_{12}$ and $D_{13}$ are not independent because both depend on the pattern $\mathbf{r}_1$. This violation of independence invalidates standard parametric tests for the significance of a correlation, which would lead to dramatically inflated estimates of significance .

The correct approach is to use a **non-parametric [permutation test](@entry_id:163935)** that respects the dependency structure of the RDM . The null hypothesis is that there is no systematic correspondence between the stimulus labels of the two RDMs. To simulate this null hypothesis, we can perform a **condition-label [permutation test](@entry_id:163935)**:
1.  Compute the observed correlation (e.g., Spearman $\rho$) between the two RDMs, $D^{(A)}$ and $D^{(B)}$.
2.  Repeatedly shuffle the condition labels of one RDM, say $D^{(B)}$. This is achieved by simultaneously permuting its rows and columns according to a [random permutation](@entry_id:270972) of the indices $\{1, \ldots, n\}$.
3.  For each permutation, recompute the correlation between $D^{(A)}$ and the shuffled $D^{(B)}$.
4.  The collection of these correlations forms a null distribution. The $p$-value is the proportion of correlations in the null distribution that are greater than or equal to the originally observed correlation.

This procedure preserves the internal structure of each RDM while breaking the stimulus-specific relationship between them, providing a valid basis for statistical inference.

A second critical aspect of interpretation is contextualizing the magnitude of an observed correlation. Is a correlation of $\rho = 0.4$ good or bad? The answer depends on the quality of the data. This is where the concept of the **[noise ceiling](@entry_id:1128751)** becomes indispensable . The [noise ceiling](@entry_id:1128751) is an estimate of the highest possible correlation any model can be expected to achieve, given the level of noise and inter-subject variability in the data. It is estimated from the data alone, independent of any specific model.

The [noise ceiling](@entry_id:1128751) is typically presented as a range defined by a lower and an upper bound, computed using the consistency across a group of subjects:
-   The **lower bound** is estimated by correlating each subject's RDM with the average RDM of all *other* subjects (a leave-one-out procedure). Because the noise in the individual subject's data is independent of the noise in the group average, this correlation is attenuated and provides a conservative estimate.
-   The **upper bound** is estimated by correlating each subject's RDM with the average RDM of the *full* group (including that subject). Here, the shared noise between the individual and the group average artificially inflates the correlation, providing an optimistic estimate.

The [noise ceiling](@entry_id:1128751) provides an essential benchmark. If a model's performance approaches or falls within the noise ceiling, it is considered to be explaining the data as well as the data's own internal consistency allows. A low [noise ceiling](@entry_id:1128751) indicates that high correlations are impossible to achieve, not because models are poor, but because the data itself is noisy or highly variable across subjects.

### RSA in the Context of Other Multivariate Methods

RSA is one of several powerful multivariate methods for analyzing neural data. Understanding its unique strengths requires comparing it to related approaches, primarily multivariate decoding (MVPA) and voxel-wise [encoding models](@entry_id:1124422). The key distinctions lie in the level of abstraction and the types of questions each method addresses .

#### RSA versus Decoding

Decoding analysis asks whether information about a specific task variable (e.g., stimulus category) is present in a brain region's activity patterns, typically by training a classifier to predict the variable from the patterns. High decoding accuracy indicates that the classes are separable. RSA, in contrast, provides a more comprehensive characterization of the entire representational space, examining the relationships between *all* pairs of conditions, not just those relevant to a particular classification boundary.

This distinction gives rise to a crucial insight: two brain regions can exhibit identical decoding accuracy for a task, yet possess fundamentally different representational geometries . Consider a scenario where one region, A, tightly clusters all stimuli belonging to the same category, while another region, B, systematically separates those same stimuli based on a secondary, "nuisance" variable (e.g., viewpoint). A linear decoder for the category label could perform equally well in both regions, as the categories remain linearly separable. However, their RDMs would be markedly different: the within-category distances in region B's RDM would be large, reflecting the coding of the nuisance variable, while they would be near zero in region A's RDM. Thus, while decoding reveals that certain information is *present*, RSA reveals *how* that information, and more, is structured.

#### RSA versus Encoding Models

Voxel-wise [encoding models](@entry_id:1124422) attempt to predict the response of each individual measurement channel (e.g., each voxel) from a set of features describing the stimuli. An encoding model tests a hypothesis about the specific feature tuning implemented by the underlying neural populations. As such, it is highly sensitive to the coordinate system, or basis, of the measurement space.

RSA operates at a higher level of abstraction. By focusing on the geometry defined by pairwise dissimilarities, it is less committed to the specific implementation in individual channels. This is most evident when using [distance metrics](@entry_id:636073) and comparison statistics with strong invariances. For instance, an RSA comparison using Mahalanobis distance is invariant to any invertible linear remapping of the voxel activities. A rank-based comparison is even more abstract, testing only for shared ordinal relationships. This means a computational model can have a similar RDM to a brain region without having a [one-to-one mapping](@entry_id:183792) between its units and the brain's voxels. RSA tests for equivalence of relational structure, whereas [encoding models](@entry_id:1124422) test for equivalence of functional tuning at the level of individual measurement units.

In summary, the three frameworks test for different [equivalence classes](@entry_id:156032) of representations. Encoding models are the most constrained, testing for specific representational axes. Decoding is the most liberal, testing only for [linear separability](@entry_id:265661), which is invariant to any [invertible linear transformation](@entry_id:149915). RSA occupies a principled middle ground, testing for the equivalence of geometric structure, which is invariant to [rigid motions](@entry_id:170523) (with Euclidean distance) and, at its most abstract, to any monotonic warping of the dissimilarity space (with rank-based correlation).