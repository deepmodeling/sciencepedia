## Applications and Interdisciplinary Connections

Having established the core principles and mechanisms of Representational Similarity Analysis (RSA), we now turn our attention to its extensive applications and interdisciplinary reach. The true power of RSA lies not merely in its capacity to characterize the structure of neural codes, but in its function as an abstract framework—a *lingua franca*—for comparing representational geometries across different contexts, measurement modalities, and even systems entirely. By focusing on the dissimilarity structure among a set of items, RSA abstracts away from the specific, high-dimensional implementation of a representation, such as the idiosyncratic firing of individual neurons or the activation of specific voxels. This abstraction allows us to pose and answer profound questions that bridge cognitive neuroscience, artificial intelligence, evolutionary biology, and engineering. This chapter will explore a range of these applications, demonstrating how the foundational principles of RSA are leveraged to yield insights in diverse scientific domains.

### Core Applications in Cognitive Neuroscience

The most direct and widespread use of RSA is within cognitive neuroscience, where it serves as a primary tool for testing hypotheses about how the brain represents and processes information. Its applications range from mapping the spatial layout of information in the brain to tracking its temporal evolution and dissecting the neural substrates of complex cognitive phenomena.

#### Mapping Representational Content in the Brain

A fundamental goal in neuroscience is to determine *what* information is represented in a given brain region and *where* in the brain specific representations are housed. RSA provides a powerful framework for this endeavor. The standard approach involves comparing an empirically derived neural Representational Dissimilarity Matrix (RDM) from a region of interest (ROI) with one or more theoretical model RDMs, each embodying a specific hypothesis about the organization of information.

The construction of a neural RDM from functional Magnetic Resonance Imaging (fMRI) data, for example, requires careful methodological considerations to ensure scientific rigor. To obtain an unbiased estimate of the dissimilarity between the neural patterns for two conditions, it is essential to use data from independent measurements, a technique known as cross-validation. For instance, by splitting fMRI runs into odd and even sets, one can estimate the neural patterns for each condition from each split and then compute the dissimilarity between patterns drawn from different splits. This procedure prevents the measurement noise, which is shared within a split, from artificially inflating the apparent similarity of the patterns. Furthermore, to account for the complex [spatial correlation](@entry_id:203497) of noise and differing reliability across voxels, a noise-normalized distance metric such as the cross-validated Mahalanobis distance is often employed. The resulting neural RDM captures the [representational geometry](@entry_id:1130876) of the ROI, which can then be compared to a model RDM (e.g., derived from stimulus features or behavioral ratings) using [rank correlation](@entry_id:175511). Statistical significance is typically assessed via a non-parametric [permutation test](@entry_id:163935) where condition labels are randomly shuffled to generate a null distribution, correctly preserving the complex spatiotemporal structure of the neural data while breaking the hypothesized link between conditions and representations .

While ROI-based RSA is powerful for testing hypotheses in specific, pre-defined regions, neuroscientists are often interested in exploring the entire brain to discover where a particular representational structure exists. The "searchlight" RSA technique was developed for this purpose. This method involves moving a small, spherical (or otherwise shaped) ROI—the searchlight—voxel by voxel throughout the brain. At each location, a local neural RDM is computed from the multivariate patterns of the voxels within the searchlight. This local RDM is then compared to a fixed model RDM, and the resulting similarity score (e.g., a Spearman correlation) is assigned to the center voxel of the searchlight. By iterating this procedure across all brain voxels, a continuous, whole-brain map is generated. This map reveals the anatomical distribution of the representational geometry specified by the model, providing a powerful, data-driven tool for localizing cognitive function .

#### Tracking Representational Dynamics

Many cognitive processes unfold over hundreds of milliseconds, a timescale inaccessible to fMRI. To study the temporal dynamics of neural representations, RSA can be applied to data from high-temporal-resolution modalities like magnetoencephalography (MEG) and electroencephalography (EEG). This "time-resolved RSA" approach generates a "movie" of the evolving [representational geometry](@entry_id:1130876).

The pipeline involves computing a separate neural RDM at each time point (or in short time windows) following stimulus presentation. As with fMRI, [robust estimation](@entry_id:261282) requires cross-validation, typically performed by splitting trials into independent partitions. For each time point $t$, a neural RDM, $D_{neural}(t)$, is constructed. The representational dynamics are then revealed by correlating this sequence of neural RDMs with a fixed model RDM, $D_{model}$. This produces a time course of model-brain correspondence, showing when the information captured by the model emerges, peaks, and fades in the neural signal. This method allows researchers to track distinct stages of processing, such as the transition from low-level sensory representations to more abstract, categorical ones .

#### Dissecting Complex Cognitive Processes

Beyond mapping and tracking, RSA is instrumental in testing nuanced predictions from cognitive theories. By designing competing model RDMs, researchers can adjudicate between different hypotheses about high-level cognition.

One profound application is in the study of consciousness. A central question is whether certain types of information, such as abstract category membership, are represented only when a stimulus is consciously perceived. RSA can address this by comparing neural RDMs from "conscious" versus "unconscious" trials, as defined by convergent behavioral evidence (e.g., visibility ratings and task performance). A rigorous analysis must, however, control for critical confounds. For instance, consciously perceived stimuli typically evoke stronger, higher signal-to-noise ratio (SNR) responses. A difference in RDM-model correlation could therefore reflect a difference in data quality rather than a true difference in representational content. This confound is addressed by estimating the "[noise ceiling](@entry_id:1128751)" for each condition, which quantifies the maximum possible model performance given the data's reliability, and then normalizing the model fits accordingly. Furthermore, to isolate abstract categorical representation from simpler stimulus features, a model RDM based on low-level visual properties can be included as a [statistical control](@entry_id:636808) using [partial correlation](@entry_id:144470). By combining these controls, RSA can provide robust evidence for or against the consciousness-dependence of specific neural codes .

Another powerful example comes from the field of memory research. The Complementary Learning Systems (CLS) theory posits a division of labor between the hippocampus and the neocortex: the hippocampus rapidly learns the specific details of individual episodes ([pattern separation](@entry_id:199607)), while the neocortex slowly learns the statistical regularities across many episodes to form generalized knowledge. RSA can test this theory directly. According to CLS, immediately after encoding, hippocampal representations should be highly item-specific, showing high dissimilarity even between items of the same semantic category. Its RDM should therefore correlate poorly with a semantic category model but well with an "episodic" model where each item is unique. In contrast, after a period of offline consolidation, the neocortex should extract the shared category structure. Its RDM should therefore show an increased correlation with the semantic category model over time. RSA provides the precise quantitative tools to test this predicted temporal and regional double [dissociation](@entry_id:144265), offering a direct window into the representational transformations underlying memory consolidation .

### Advanced Methodological Extensions: Comparing Multiple Models

A frequent scenario in science involves not one, but several competing hypotheses. RSA can be extended with more advanced statistical methods to formally compare and adjudicate between multiple model RDMs.

A common approach is to use [partial correlation](@entry_id:144470). Suppose we want to test a target model, $D^{(T)}$, while suspecting that the neural data, $D^{(N)}$, might also be explained by a simpler, confounding model, $D^{(C)}$ (e.g., a model of low-level visual features). The [partial correlation](@entry_id:144470) between $D^{(N)}$ and $D^{(T)}$ while controlling for $D^{(C)}$ quantifies the unique linear association between the neural data and the target model, after the [variance explained](@entry_id:634306) by the confound model has been statistically removed from both. Mathematically, it is the correlation between the residuals of $D^{(N)}$ and $D^{(T)}$ after each has been regressed on $D^{(C)}$. This provides a powerful tool for isolating the unique contribution of a specific hypothesis .

When several models are hypothesized to contribute to the neural representation, a [multiple regression](@entry_id:144007) framework can be employed. Here, the vectorized neural RDM, $\mathbf{d}$, is modeled as a linear combination of several vectorized model RDMs, $\{\mathbf{m}_k\}$. The problem can be formulated as finding the weights, $\mathbf{w}$, that best predict the neural data: $\hat{\mathbf{d}} = \sum_k w_k \mathbf{m}_k$. Often, a non-negativity constraint is imposed ($w_k \ge 0$), reflecting the assumption that models make additive contributions. This turns the problem into a Nonnegative Least Squares (NNLS) estimation. While this approach is powerful, it faces a challenge when model RDMs are correlated with each other (collinearity). In such cases, while the overall predicted RDM, $\hat{\mathbf{d}}$, is uniquely determined, the individual weights, $w_k$, may not be uniquely identifiable. This highlights a fundamental limit: when two models make similar predictions, it becomes difficult to definitively disentangle their respective contributions based on the data .

### Bridging Brains and Artificial Intelligence

One of the most exciting interdisciplinary applications of RSA is in bridging neuroscience and artificial intelligence (AI), particularly in the study of Deep Neural Networks (DNNs). DNNs trained on tasks like [object recognition](@entry_id:1129025) have shown a remarkable ability to predict neural responses in the primate [visual pathway](@entry_id:895544). RSA provides a "common currency" to directly compare the internal representations of these artificial systems with those of the brain.

The procedure involves presenting the same set of stimuli to both a biological subject (e.g., a human undergoing fMRI) and a trained DNN. For the brain, a neural RDM is computed for a given cortical region. For the DNN, an RDM is computed for each layer by treating the layer's unit activations for each stimulus as a pattern vector. By then correlating the layer-wise DNN RDMs with the neural RDM, researchers can establish a correspondence between the stages of processing in the artificial and biological systems. For instance, studies have consistently found that earlier layers of a DNN have representational geometries similar to those in early visual cortex (e.g., V1, V2), while deeper layers correspond better to higher-level visual areas (e.g., inferior temporal cortex). This approach not only provides a powerful way to test and understand the function of brain regions but also offers a neuroscientific benchmark for evaluating and improving AI models .

### Inter-Subject and Cross-Species Comparisons

Comparing representations across different individuals, or even different species, presents a significant challenge. The precise implementation of neural codes can vary substantially from one brain to another due to anatomical and functional idiosyncrasies. RSA, when combined with advanced alignment techniques, provides a path forward.

#### Inter-Subject Comparison and Functional Alignment

If we compute RDMs for the same brain region from two different subjects, the correlation between them is often surprisingly low. This is because even if both subjects share a common latent representational geometry, this geometry is instantiated in unique, high-dimensional voxel patterns in each person's brain. A simple rotation of the voxel space, for example, can be enough to drastically alter an RDM computed with a metric like [correlation distance](@entry_id:634939). This "functional misalignment" problem obscures shared representations and weakens group-level [statistical power](@entry_id:197129) .

To overcome this, [functional alignment](@entry_id:1125376) methods have been developed. One prominent example is "[hyperalignment](@entry_id:1126288)," a data-driven approach that aligns individual subjects' representational spaces without reference to their anatomical location. Hyperalignment uses the shared neural responses to a rich, time-varying stimulus (like a movie) to learn a set of transformations (e.g., rotations) that map each subject's individual voxel space into a common, high-dimensional [model space](@entry_id:637948). In this common space, the functional topographies are aligned. When RDMs are computed from these aligned data, the [inter-subject correlation](@entry_id:1126568) increases dramatically, revealing the shared representational structure that was previously hidden by idiosyncratic implementations. This allows for more sensitive and powerful analyses of shared cognitive processes across a population .

#### Comparative Neuroscience and the Search for Homology

RSA's abstract nature makes it uniquely suited for comparative neuroscience, where the goal is to understand the evolution of brain and cognition. A central question in this field is whether a similar cognitive trait observed in two different species (e.g., numerosity in primates and birds) is the result of a shared evolutionary origin (homology) or independent evolution (analogy or convergence).

Simply finding similar neural responses is insufficient to claim homology. RSA provides a more rigorous test at the level of the computational algorithm. By comparing the [representational geometry](@entry_id:1130876) of the underlying neural codes, we can ask if the *structure* of information is conserved. A high correlation between the RDMs from a macaque prefrontal cortex and a crow nidopallium caudolaterale during a numerosity task provides stronger evidence for a shared computational mechanism than similar single-neuron tuning curves. However, interpreting such a finding requires extreme care. A high RDM correlation could be driven by confounds like shared sensitivity to low-level stimulus features rather than a shared high-level algorithm. Furthermore, differences in task demands and stimulus sampling across studies can profoundly alter the resulting RDMs. A truly rigorous claim of functional homology requires a multi-level program: demonstrating that the RDM similarity is robust across different stimulus sets, holds after controlling for low-level features, is observed under matched task conditions, and is consistent with the phylogenetic evidence (e.g., by checking for the trait in a relevant outgroup species)  . In this context, RSA serves as a critical tool for operationalizing the mechanistic criterion of homology.

### Beyond Neuroscience: RSA in Machine Learning and Engineering

The core logic of RSA—comparing abstract similarity structures—is domain-general and finds powerful applications outside of neuroscience. In machine learning and materials science, for instance, RSA can be used as a tool for [model interpretability](@entry_id:171372) and diagnostics.

Consider the task of predicting material properties using Graph Neural Networks (GNNs). A GNN might be pre-trained on a large dataset to predict one property (e.g., formation energy) and then adapted to predict a new, related property (e.g., ion diffusion barrier) via transfer learning. A key question is which layers of the pre-trained network should be fine-tuned and which should be kept frozen. RSA can guide this decision. One can create a "target" RDM from the ground-[truth values](@entry_id:636547) of the new property (the [diffusion barrier](@entry_id:148409)). Then, for each layer of the GNN, one can compute two representation RDMs: one from the "frozen" pre-trained network and one after "[fine-tuning](@entry_id:159910)." By correlating these layer-wise RDMs with the target RDM, one can quantify which layers benefit from fine-tuning. If a layer's RDM alignment with the target property *improves* after [fine-tuning](@entry_id:159910), it indicates that the layer's features are being usefully adapted. If the alignment *decreases*, it suggests the pre-trained features were more suitable, and freezing that layer may be a better strategy. This application demonstrates how RSA can be repurposed as a general-purpose analytical tool to probe the internal workings of complex AI models and optimize their performance in engineering domains .

### Conclusion

As this chapter has illustrated, Representational Similarity Analysis is far more than a single technique; it is a versatile conceptual framework with profound interdisciplinary implications. From mapping information in the human brain and tracking its millisecond-by-millisecond dynamics to testing grand theories of memory and consciousness, RSA has become an indispensable tool in the cognitive neuroscientist's arsenal. Its utility extends further, providing a quantitative bridge to compare biological intelligence with artificial intelligence, to trace the evolutionary history of cognitive functions across species, and even to diagnose and interpret machine learning models in fields like materials science. The common thread across these diverse applications is RSA's elegant ability to capture and compare the abstract geometry of information, providing a shared language to explore the structure of knowledge in any system, biological or artificial.