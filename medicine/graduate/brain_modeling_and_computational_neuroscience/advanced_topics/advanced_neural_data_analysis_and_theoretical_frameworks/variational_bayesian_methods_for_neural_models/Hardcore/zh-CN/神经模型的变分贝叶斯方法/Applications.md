## 应用与跨学科联系

在前面的章节中，我们已经探讨了[变分贝叶斯方法](@entry_id:1133718)的核心原理与机制，尤其是[证据下界](@entry_id:634110)（ELBO）的推导及其在推断问题中的核心作用。现在，我们将视野从理论基础转向实践应用，展示这些原理如何在神经科学及相关领域的多元化、真实世界和跨学科背景下发挥作用。本章的目的不是重复讲授核心概念，而是通过一系列应用导向的范例，揭示[变分贝叶斯方法](@entry_id:1133718)的实用性、[延展性](@entry_id:160108)及其在整合不同学科思想时的强大能力。我们将看到，[变分贝叶斯](@entry_id:756437)不仅是一种计算工具，更是一种统一的数学语言，能够连接统计建模、机器学习、信息论和[理论神经科学](@entry_id:1132971)。

### 神经数据的[生成建模](@entry_id:165487)

[变分贝叶斯方法](@entry_id:1133718)的一个核心应用领域是[生成建模](@entry_id:165487)，即构建能够解释观测数据（如神经活动）如何由不可见的潜在变量（latent variables）生成的概率模型。通过[变分推断](@entry_id:634275)，我们可以反向工程这一过程，从观测数据中推断出这些潜在变量的特性。

#### 利用[点过程](@entry_id:1129862)广义线性模型（GLM）建模[脉冲序列](@entry_id:1132157)

在计算神经科学中，一个基本任务是理解神经元如何响应外部刺激和其自身的放电历史。[点过程](@entry_id:1129862)广义线性模型（Point-Process GLM）是完成此任务的标准统计工具。在此模型中，神经元的瞬时放电率被建模为一系列协变量（如刺激[特征和](@entry_id:189446)历史放电活动）的线性组合，再通过一个[非线性](@entry_id:637147)函数（通常是指数函数）进行转换。贝叶斯方法允许我们不仅估计这些协变量的权重，还能量化我们对这些估计的不确定性。然而，由于[点过程似然](@entry_id:1129855)函数的[非高斯性](@entry_id:158327)质，权重的后验分布通常是难以解析求解的。

[变分贝叶斯](@entry_id:756437)为解决这一问题提供了优雅的方案。通过设定一个[参数化](@entry_id:265163)的高斯分布（例如，一个均值场高斯分布）作为变分后验，我们可以为GLM推导出[证据下界](@entry_id:634110)（ELBO）。优化这个ELBO可以得到权重后验分布的近似。此过程的推导本身就是一个经典的练习，它展示了如何在非[共轭模型](@entry_id:905086)中处理[似然函数](@entry_id:921601)项的期望（例如，[高斯变量](@entry_id:276673)的指数函数的期望），并将其与[高斯先验](@entry_id:749752)下的[KL散度](@entry_id:140001)项结合起来，从而形成一个可处理的优化目标。这种方法使得研究人员能够以一种有原则的方式，从神经[脉冲序列](@entry_id:1132157)数据中推断出连接权重及其不确定性。

#### 揭示神经群体中的[潜在流形](@entry_id:1127095)

当处理大规模神经群体记录时，一个核心假设是高维的[群体活动](@entry_id:1129935)实际上受一个低维的潜在动态系统（或称“流形”）所驱动。[变分自编码器](@entry_id:177996)（Variational Autoencoder, VAE）作为[变分贝叶斯方法](@entry_id:1133718)在深度学习中的一个杰出应用，为发现这种[非线性](@entry_id:637147)低维结构提供了强大工具。VAE由一个“编码器”神经网络（推断网络）和一个“解码器”神经网络（生成网络）组成。编码器将高维的神经活动模式映射到一个潜在空间的概率分布（通常是高斯分布），而解码器则从该[潜在空间](@entry_id:171820)中采样，并尝试重构原始的神经活动。

训练VAE的目标是联合优化编码器和解码器的参数，以最大化ELBO。ELBO此时由两部分组成：一部分是重构误差（期望对数似然），衡量解码器重构数据的能力；另一部分是[KL散度](@entry_id:140001)，它正则化编码器产生的潜在分布，使其接近一个预设的先验（通常是[标准正态分布](@entry_id:184509)）。例如，在一个线性的高斯解码器设定下，我们可以解析地推导出ELBO的完整表达式，该表达式依赖于生成模型的参数（如权重矩阵$W$和偏置$b$）以及推断网络输出的变分参数（均值$\mu_{\phi}(x)$和方差$s_{\phi}^{2}(x)$）。通过最大化这个目标，VAE能够学习到一个压缩的、有意义的神经活动表示。

#### 学习[解耦](@entry_id:160890)表征

在神经科学中，理想的潜在表征应是“[解耦](@entry_id:160890)的”（disentangled），即潜在空间的每个维度分别对应现实世界中一个独立的生成因子（例如，一个视觉刺激的位置、朝向或颜色）。标准的VAE并不保证能学到这种[解耦](@entry_id:160890)表征。然而，通过对ELBO进行简单的修改，如$\beta$-VAE框架所做的那样，我们可以更强地鼓励模型学习这种结构。

$\beta$-VAE的[目标函数](@entry_id:267263)是$\mathbb{E}_q[\log p(D\mid z)]-\beta \mathrm{KL}(q(z)\parallel p(z))$。当$\beta > 1$时，它加大了对[KL散度](@entry_id:140001)项的惩罚。从信息论的角度看，[KL散度](@entry_id:140001)项可以被分解为数据与潜在变量之间的互信息$I_q(D;z)$以及聚合后验与先验之间的[KL散度](@entry_id:140001)。增加$\beta$[实质](@entry_id:149406)上是加强了对[互信息](@entry_id:138718)$I_q(D;z)$的惩罚，迫使模型在潜在空间中创建一个更窄的“[信息瓶颈](@entry_id:263638)”，只允许与重构最相关的信息通过。这种压力，结合因子化的先验（如标准[高斯先验](@entry_id:749752)），鼓励模型学习一个尽可能高效且因子化的表征，这通常对应于[解耦](@entry_id:160890)的表征。然而，过大的$\beta$值可能导致“后验坍塌”（posterior collapse），即潜在变量完全忽略输入数据，导致重构质量严重下降。因此，$\beta$值的选择是在重构保真度与表征[解耦](@entry_id:160890)度之间的一种权衡。

### 建模神经动力学与[序列数据](@entry_id:636380)

大脑的功能本质上是动态的，神经活动在时间上不断演化。[变分贝叶斯方法](@entry_id:1133718)同样可以被扩展，以处理这类[时间序列数据](@entry_id:262935)，从简单的[马尔可夫模型](@entry_id:899700)到复杂的[循环神经网络](@entry_id:634803)。

#### 利用结构化[变分贝叶斯](@entry_id:756437)捕捉时间依赖性

最简单的[变分推断](@entry_id:634275)形式，即均值场（mean-field）近似，假设后验分布中的所有变量都是[相互独立](@entry_id:273670)的。这一假设虽然简化了计算，但也带来了一个显著的局限性：它无法捕捉变量之间的后验相关性。在一个神经模型中，这可能非常关键。例如，在一个包含刺激驱动和 refractory（不应期）效应的GLM中，估计的刺激权重和历史权重在真实后验中几乎肯定是相关的。均值场近似会强制将它们的后验协方差设为零，从而错误地表征了参数的不确定性。

为了克服这一局限，我们可以使用更丰富的“结构化”变分族。当为神经[动力学建模](@entry_id:204326)时，一个常见的模型是状态空间模型（state-space model），其中潜在的神经状态随时间演化，并产生观测到的活动。这类模型（如线性高斯动态系统）的先验具有马尔可夫结构，即每个时刻的状态只依赖于前一时刻的状态。这种时间依赖性在后验中依然存在。我们可以设计一个能够反映这种结构的变分后验，例如，一个具有块三对角（block-tridiagonal）[精度矩阵](@entry_id:264481)（协方差矩阵的逆）的多元高斯分布。这种结构精确地编码了一个[高斯马尔可夫随机场](@entry_id:749746)（GMRF）的[条件独立性](@entry_id:262650)，即每个状态给定其邻居后与其他所有状态独立。与均值场（对应于块对角精度）相比，这种[结构化近似](@entry_id:755572)能够捕捉相邻时间步之间的后验相关性，从而提供了一个更紧密的ELBO。更重要的是，利用[带状矩阵](@entry_id:746657)的线性代数算法，我们可以在$O(T)$的时间内完成推断，其中$T$是时间序列的长度，这远比处理一个稠密的$O(T^3)$协方差矩阵要高效得多。

#### [变分贝叶斯](@entry_id:756437)[滤波与平滑](@entry_id:188825)

[变分贝叶斯](@entry_id:756437)卡尔曼滤波（Variational Bayesian Kalman filtering）是结构化[变分推断](@entry_id:634275)在实践中的一个经典例子。考虑一个标准的线性高斯[状态空间模型](@entry_id:137993)，但假设过程噪声和观测噪声的精度（方差的倒数）是未知的。我们可以为这些精度参数赋予共轭的Gamma先验。然后，使用一个因子化的变分后验$q(\text{states})q(\text{precisions})$，我们可以推导出坐标上升更新规则。

有趣的是，对潜在状态序列的更[新形式](@entry_id:199611)与标准的[卡尔曼平滑](@entry_id:750983)算法（Kalman smoother）完全相同，只不过其中未知的噪声方差被其在变分后验下的[期望值](@entry_id:150961)所取代。同时，对噪声精度的更新则依赖于从状态后验中计算出的期望[误差平方和](@entry_id:149299)。这个过程在状态更新（E-step）和超参数更新（M-step）之间迭代，类似于[EM算法](@entry_id:274778)，但它是一个完全贝叶斯的方法，能够联合推断状态和模型参数的完整后验分布。例如，观测噪声精度$\beta$的后验期望，可以解析地表示为先验参数和数据相关的充分统计量（如期望的观测[残差平方和](@entry_id:174395)$S_v$）的函数：$\mathbb{E}_{q(\beta)}[\beta] = (c_{0} + \frac{1}{2} T n_{y}) / (d_{0} + \frac{1}{2} S_{v})$，其中$(c_0, d_0)$是先验Gamma分布的参数。这展示了VB在处理具有未知参数的动态系统时的强大能力。

#### 贝叶斯循环神经网络

对于更复杂的[非线性](@entry_id:637147)[序列数据](@entry_id:636380)，如长期的电子健康记录或复杂的神经动态，[循环神经网络](@entry_id:634803)（RNNs），特别是长短期记忆网络（LSTM），已成为标准工具。将[贝叶斯方法](@entry_id:914731)应用于RNN，可以为[时间序列预测](@entry_id:1133170)提供关键的[不确定性估计](@entry_id:191096)。在贝叶斯LSTM中，我们不对权重使用单一的[点估计](@entry_id:174544)，而是为它们赋予先验分布，并使用[变分推断](@entry_id:634275)来近似其后验。

训练这样一个网络需要在整个展开的时间序列上计算ELBO的梯度，这个过程称为“时间反向传播”（Backpropagation Through Time, [BPTT](@entry_id:633900)）。在变分设置中，这涉及到从变分后验$q(\mathbf{W})$中采样一组权重（通常使用[重参数化技巧](@entry_id:636986)以获得低方差梯度），然后用这组固定的权重对整个序列进行前向传播，计算[似然](@entry_id:167119)项，最后[反向传播](@entry_id:199535)梯度以更新变分参数。这个过程带来了巨大的计算挑战，包括存储所有时间步激活所需的大量内存、由于长序列的[雅可比矩阵](@entry_id:178326)连乘导致的梯度消失或爆炸问题，以及由于在长随机[计算图](@entry_id:636350)上传播而增加的梯度方差。尽[管存](@entry_id:1127299)在这些挑战，贝叶斯RNNs为在临床决策等高风险领域进行不确定性感知的序列建模提供了一个有原则的框架。

### 不确定性量化与模型正则化

[贝叶斯方法](@entry_id:914731)的核心优势之一是它能够对不确定性进行有原则的量化。[变分贝叶斯](@entry_id:756437)作为一种[近似推断](@entry_id:746496)技术，保留了这一关键特性，并揭示了其与标准机器学习中[正则化技术](@entry_id:261393)的深刻联系。

#### 分解与量化预测不确定性

在预测任务中，总的不确定性可以分解为两种不同性质的成分：认知不确定性（epistemic uncertainty）和[偶然不确定性](@entry_id:634772)（aleatoric uncertainty）。
*   **[偶然不确定性](@entry_id:634772)**源于数据生成过程中固有的、不可约减的随机性或噪声。即使我们拥有无限的数据并知道了模型的“真实”参数，这种不确定性依然存在。
*   **认知不确定性**源于我们对模型参数本身的不确定性，这是由有限的训练数据造成的。通过收集更多的数据，认知不确定性可以被降低。

在[贝叶斯神经网络](@entry_id:746725)（BNN）中，这两种不确定性可以通过[全变差](@entry_id:140383)定律（law of total variance）进行优雅地分解。对于一个回归问题，总的预测方差可以分解为[偶然不确定性](@entry_id:634772)（likelihood方差在参数后验上的期望）和认知不确定性（模型预测均值在参数后验上的方差）。对于[分类问题](@entry_id:637153)，类似地，总的预测熵可以分解为[偶然不确定性](@entry_id:634772)（给定参数下的期望熵）和认知不确定性（预测和参数之间的[互信息](@entry_id:138718)）。

[变分推断](@entry_id:634275)的ELBO目标函数自然地反映了这两种不确定性。期望[对数似然](@entry_id:273783)项$\mathbb{E}_{q(\theta)}[\log p(\mathcal{D} \mid \theta)]$驱动模型去拟[合数](@entry_id:263553)据，并学习数据中的[偶然不确定性](@entry_id:634772)（例如，通过一个能够预测[数据依赖](@entry_id:748197)噪声的异方差似然）。而[KL散度](@entry_id:140001)项$\mathrm{KL}(q(\theta) \,\|\, p(\theta))$则正则化[模型复杂度](@entry_id:145563)，控制认知不确定性的大小。在医学等安全攸关的应用中，区分这两种不确定性至关重要。高的[偶然不确定性](@entry_id:634772)可能意味着一个病人的状况本身就难以预测，而高的认知不确定性则是一个明确的信号，表明模型在其输入的这个区域“知识不足”，应当将决策转交给人类专家。

#### 变分Dropout作为[贝叶斯正则化](@entry_id:635494)

许多在[深度学习](@entry_id:142022)中凭经验发展起来的[正则化技术](@entry_id:261393)，事后被发现可以被解释为某种形式的近似贝叶斯推断。Dropout是一个典型的例子。变分Dropout将权重上的乘性[高斯噪声](@entry_id:260752)（dropout的一种连续近似）重新诠释为对一个具有特定形式的变分[后验分布](@entry_id:145605)的推断。具体来说，一个权重$w$的后验可以被建模为$q(w) = \mathcal{N}(m, \alpha m^{2})$，其中$m$是均值权重，$\alpha$是与dropout率相关的噪声参数。

通过最小化KL散度项，我们可以推导出最优的dropout率。例如，在一个权重上，如果其先验是零均值高斯分布，那么最小化[KL散度](@entry_id:140001)的最优dropout率$p^*$，与该权重的[后验均值](@entry_id:173826)$m$和先验方差$\tau^2$之间存在一个简单的解析关系：$p^{\ast} = \frac{\tau^{2}}{m^{2} + \tau^{2}}$ 。这个结果非常直观：对于一个“确信”且远离先验的权重（即$|m|$大），最优的dropout率很低；而对于一个接近先验（$|m|$小）的权重，最优的dropout率接近1，模型倾向于将其“丢弃”。这为dropout的稀疏化效应提供了一个贝叶斯视角的解释。

#### 理论保证：[PAC-贝叶斯](@entry_id:634219)连接

[变分贝叶斯方法](@entry_id:1133718)的正则化效应不仅仅是[启发式](@entry_id:261307)的，它还与[学习理论](@entry_id:634752)中的[泛化界](@entry_id:637175)（generalization bounds）有深刻的联系。[PAC-贝叶斯理论](@entry_id:753065)（Probably Approximately Correct Bayes）为在[后验分布](@entry_id:145605)上进行预测的模型的[泛化误差](@entry_id:637724)（即期望测试风险）提供了一个[上界](@entry_id:274738)。这个上界通常由两部分组成：一部分是在训练集上评估的[经验风险](@entry_id:633993)，另一部分是一个复杂度项，它依赖于[后验分布](@entry_id:145605)$q$与[先验分布](@entry_id:141376)$p$之间的[KL散度](@entry_id:140001)，并随数据量$n$的增加而减小。

这与我们最小化的变分目标（负ELBO）的形式惊人地相似：负ELBO也是一个经验损失项（期望[负对数似然](@entry_id:637801)）和一个[KL散度](@entry_id:140001)复杂度项的和。因此，最小化变分目标可以被看作是在隐式地最小化一个[泛化误差](@entry_id:637724)的[上界](@entry_id:274738)。[KL散度](@entry_id:140001)项$\mathrm{KL}(q(w) \,\|\, p(w))$直接出现在这两个框架中，扮演着控制模型复杂度的角色。减小这个KL散度，就意味着收紧了[PAC-贝叶斯](@entry_id:634219)[泛化界](@entry_id:637175)，从而为模型更好的泛化性能提供了理论支持。这种联系为[贝叶斯深度学习](@entry_id:633961)的成功提供了强有力的理论基础。

### 大脑作为[贝叶斯推断](@entry_id:146958)机器：更广阔的理论框架

[变分贝叶斯方法](@entry_id:1133718)不仅是分析神经数据的工具，其核心概念——[变分自由能](@entry_id:1133721)——本身也已成为构建大脑功能理论的基石。在这个视角下，大脑自身就被认为是一个在执行[变分推断](@entry_id:634275)的器官。

#### [自由能原理](@entry_id:1125309)与[最小描述长度](@entry_id:261078)

信息论中的[最小描述长度](@entry_id:261078)（Minimum Description Length, MDL）原理指出，最好的模型是能为数据提供最短编码（description length）的模型。对于一个概率模型，数据$y$的理想编码长度是它的“惊奇”（surprisal），即$-\ln p(y)$，其中$p(y)$是模型的证据。因此，模型选择可以被看作是寻找证据$p(y)$最大的模型。

正如我们在推导ELBO时所见，$\ln p(y) \ge \mathcal{L}(q)$，这意味着$-\ln p(y) \le -\mathcal{L}(q)$。这个$-\mathcal{L}(q)$就是[变分自由能](@entry_id:1133721)$F(q,y)$。因此，[变分自由能](@entry_id:1133721)是数据在模型下真实编码长度的一个上界。最大化ELBO等价于最小化[变分自由能](@entry_id:1133721)，这可以被解释为通过调整我们的信念（由$q$表示）来寻找对数据更短、更有效的解释。这个过程完美地契合了MDL原理。

#### [预测编码](@entry_id:150716)作为[自由能最小化](@entry_id:183270)的神经算法

如果大脑确实在最小化自由能，那么它是通过何种神经算法实现的呢？[预测编码](@entry_id:150716)（Predictive Coding, PC）理论提供了一个引人注目的候选答案。PC理论假设，大脑皮层形成了一个层次化的[生成模型](@entry_id:177561)。在这个层次结构中，高层区域向低层区域发送“预测”信号，而低层区域则将这些预测与其接收到的输入（来自更低层或[感觉器官](@entry_id:269741)）进行比较，并将差值——即“预测误差”——向上传递。

这个看似简单的方案与变分[自由能最小化](@entry_id:183270)有着深刻的数学联系。可以证明，在一个线性的高斯生成模型下，如果我们将神经元的活动解释为对潜在变量[后验均值](@entry_id:173826)的表征（$\boldsymbol{\mu}$），并将[预测误差](@entry_id:753692)神经元的活动解释为自由能梯度，那么预测编码的更新规则就等价于在自由能$\mathcal{F}(\boldsymbol{\mu})$上进行梯度下降。具体来说，表征单元$\boldsymbol{\mu}$的动态变化$\dot{\boldsymbol{\mu}}$正比于$-\nabla_{\boldsymbol{\mu}} \mathcal{F}(\boldsymbol{\mu})$，这个梯度本身可以被分解为来自上层（先验）和下层（[似然](@entry_id:167119)）的、由[精度加权](@entry_id:914249)的预测误差项的组合。因此，预测编码为[自由能最小化](@entry_id:183270)这一抽象计算原理提供了一个具体的、神经上可信的实现机制。

#### 辨析核心假说

在[理论神经科学](@entry_id:1132971)的语境中，区分几个密切相关但又截然不同的概念至关重要：[贝叶斯大脑假说](@entry_id:917738)（Bayesian Brain Hypothesis, BBH）、[自由能原理](@entry_id:1125309)（Free Energy Principle, FEP）和预测编码（Predictive Coding, PC）。
*   **[贝叶斯大脑假说](@entry_id:917738)**是一个**规范性**（normative）理论，它描述了大脑在计算层面上的**目标**：执行贝叶斯推断以理解世界。它不规定实现这一目标的具体算法。
*   **[自由能原理](@entry_id:1125309)**是一个**过程理论**（process theory），它提出了一个更普适的原理：任何自组织、自适应的系统（包括大脑）都必须最小化其[变分自由能](@entry_id:1133721)以维持其存在。最小化自由能恰好等价于执行（近似）[贝叶斯推断](@entry_id:146958)，因此FEP为BBH提供了一个“为何”的解释和一个数学上统一的框架。
*   **[预测编码](@entry_id:150716)**是一个**算法性**（algorithmic）理论，它描述了大脑**如何**可能实现[自由能最小化](@entry_id:183270)。它是一个具体的神经计算方案，是FEP和BBH的一个可能实现。

因此，这三者处于不同的解释层次。FEP为BBH提供了一个第一性原理的推导，而PC则是FEP的一个具体实现算法。它们彼此兼容，但逻辑上并不相互蕴含。例如，大脑可以是贝叶斯的，但并非通过PC来实现；或者，大脑可以最小化自由能，但使用不同于PC的算法。

### 科学应用的实践考量

最后，将这些强大的模型应用于科学研究时，我们必须面对一些关键的实践挑战，尤其是在确保研究的严谨性和可重复性方面。

#### 复杂贝叶斯模型的可复现性

像动态因果模型（Dynamic Causal Modeling, DCM）这样复杂的、基于[变分贝叶斯](@entry_id:756437)的全贝叶斯模型，其推断结果对许多细节高度敏感。DCM常用于从fMRI数据中推断脑区之间的有效连接。要使一个DCM分析能够被独立复现，仅仅报告模型结构和最终结果是远远不够的。由于[变分推断](@entry_id:634275)是一个[非凸优化](@entry_id:634396)过程，最终结果取决于初始值、优化器设置和[收敛准则](@entry_id:158093)。此外，结果也取决于[先验分布](@entry_id:141376)的精确设定（包括均值和协方差）、噪声模型的超参数以及所有[数据预处理](@entry_id:197920)步骤。因此，为了确保计算上的[可复现性](@entry_id:151299)，研究人员必须提供一份详尽的清单，包括：完整的生成模型和先验参数；所有数据变换的细节；以及推断算法的所有设置，如初始化、数值积分器类型和步长、[收敛容差](@entry_id:635614)，乃至所使用的软件版本和随机数种子。

#### 选择你的近似：VI vs. MCMC

在实际应用中，我们常常需要在计算速度和推断精度之间做出权衡。[变分推断](@entry_id:634275)（VI）和[马尔可夫链蒙特卡洛](@entry_id:138779)（MCMC）是两种主要的近似贝叶斯推断方法，它们代表了这种权衡的两个不同端点。
*   **MCMC** 方法（如[哈密顿蒙特卡洛](@entry_id:144208)）在理论上能保证在足够长的时间后产生来自真实后验分布的样本，因此被认为是[近似推断](@entry_id:746496)的“黄金标准”。然而，它通常计算成本高昂，难以用于需要快速预测的实时系统。
*   **VI** 方法将推断问题转化为一个优化问题，通常速度快得多，使其适用于大规模数据和低延迟部署。然而，它的精度受限于所选变分族的[表达能力](@entry_id:149863)，并且众所周知（特别是均值场VI）会系统性地低估后验方差。

在临床等高要求环境中，一种务实的策略是将两者结合起来：在模型部署时，使用经过校准的、快速的VI近似来进行实时预测；而在离线的模型开发和验证阶段，则使用计算密集的MCMC作为基准，来审计VI近似的质量、评估其[不确定性估计](@entry_id:191096)的准确性，并为设置安全的决策阈值提供更可靠的依据。

### 结论

本章的旅程展示了[变分贝叶斯方法](@entry_id:1133718)的广度和深度。从为单个神经元的放电模式建模，到为整个大脑的功能提供理论框架，VB提供了一套连贯而强大的工具。它不仅使我们能够在复杂的概率模型中进行高效推断，还为理解正则化、不确定性乃至心智本身的计算原理提供了深刻的见解。掌握这些应用，意味着研究者不仅能将VB作为一种技术来使用，更能将其作为一种思维方式，用以构建、测试和理解关于神经系统如何处理信息和应对不确定世界的理论。