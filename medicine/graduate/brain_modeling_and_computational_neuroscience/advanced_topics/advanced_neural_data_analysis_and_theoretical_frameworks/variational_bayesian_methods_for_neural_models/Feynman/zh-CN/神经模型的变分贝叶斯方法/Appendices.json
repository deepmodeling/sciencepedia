{
    "hands_on_practices": [
        {
            "introduction": "变分推断的核心思想之一是坐标上升法 (coordinate ascent variational inference, CAVI)，它在共轭指数族模型中尤为有效，构成了许多经典变分算法的基础。本练习  将指导你使用变分消息传递 (Variational Message Passing, VMP) 的框架，为一个在神经尖峰计数分析中常见的层级模型推导出其变分更新法则。这个实践旨在为你理解变分推断的基本运作机制打下坚实的基础。",
            "id": "4032028",
            "problem": "考虑一个在固定时间窗口内观察到的包含 $N$ 个神经元的群体。令 $y_{i} \\in \\{0,1,2,\\dots\\}$ 表示神经元 $i$ 在持续时间为 $s_{i} > 0$ 的窗口内的脉冲发放计数，其中 $i = 1,\\dots,N$。假设一个条件独立的生成模型，其中每个计数 $y_{i}$ 从一个泊松分布中抽取，该分布具有神经元特定的速率 $\\lambda_{i} > 0$（由窗口持续时间缩放），并且这些速率从一个分层的伽马先验中抽取，该先验具有一个在所有神经元间共享的全局速率参数 $b > 0$。该模型为\n$$\np(y,\\lambda,b) \\;=\\; \\prod_{i=1}^{N} p(y_{i} \\mid \\lambda_{i}) \\, p(\\lambda_{i} \\mid a,b) \\; p(b \\mid c,d),\n$$\n其中\n$$\np(y_{i} \\mid \\lambda_{i}) \\;=\\; \\text{Poisson}(y_{i} \\mid s_{i} \\lambda_{i}), \\quad p(\\lambda_{i} \\mid a,b) \\;=\\; \\text{Gamma}(\\lambda_{i} \\mid a,b), \\quad p(b \\mid c,d) \\;=\\; \\text{Gamma}(b \\mid c,d).\n$$\n这里，$\\text{Poisson}(y \\mid \\mu)$ 表示泊松质量函数 $p(y \\mid \\mu) = \\exp(-\\mu)\\,\\mu^{y}/y!$，而 $\\text{Gamma}(x \\mid \\alpha,\\beta)$ 表示形状为 $\\alpha$、速率为 $\\beta$ 的伽马密度函数，$p(x \\mid \\alpha,\\beta) = \\beta^{\\alpha}/\\Gamma(\\alpha)\\,x^{\\alpha - 1}\\exp(-\\beta x)$，其中 $\\Gamma(\\cdot)$ 是伽马函数。所有超参数 $a,c,d$ 都是已知的正常数，$s_{i}$ 是已知的正持续时间。\n\n将此模型表示在一个因子图上，其变量节点为 $\\{\\lambda_{i}\\}_{i=1}^{N}$ 和 $b$，因子节点由似然 $\\{p(y_{i} \\mid \\lambda_{i})\\}_{i=1}^{N}$、先验 $\\{p(\\lambda_{i} \\mid a,b)\\}_{i=1}^{N}$ 和超先验 $p(b \\mid c,d)$ 给出。使用变分消息传递（VMP）的框架，在平均场族 $q(\\lambda,b) = \\prod_{i=1}^{N} q(\\lambda_{i})\\, q(b)$（限制于与真实模型一致的共轭指数族因子）下，推导出从该因子图产生的 $q(\\lambda_{i})$ 和 $q(b)$ 的闭式坐标更新。\n\n推导必须从适用于计算神经科学中变分贝叶斯方法的基本原理开始：\n- 用于变分推断的证据下界（ELBO）定义及其坐标上升特性。\n- 平均场分解和通用更新规则 $q(x) \\propto \\exp\\big(\\mathbb{E}_{-x}[\\ln p(x,\\text{others},\\text{data})]\\big)$。\n- 泊松-伽马层级结构的共轭指数族结构及其对自然参数相加的影响。\n\n你的最终答案必须是变量节点上VMP更新的一对闭式解析表达式，用邻近变量的期望表示，并且只使用模型量 $(y_{i}, s_{i}, a, c, d)$ 和当前变分因子下的期望来表达。不要提供不等式或需要数值求解的方程。本问题不需要数值取整或单位。将最终答案表示为包含两个更新的单个解析表达式，使用上面指定的规范伽马族参数化。",
            "solution": "该问题要求为神经脉冲计数的层次化泊松-伽马模型，推导其变分后验分布 $q(\\lambda_i)$ 和 $q(b)$ 的变分消息传递（VMP）坐标上升更新。推导将基于共轭指数模型的变分推断原理。\n\n首先，我们确定模型的结构。观测数据 $y = \\{y_i\\}_{i=1}^{N}$、潜速率 $\\lambda = \\{\\lambda_i\\}_{i=1}^{N}$ 和全局速率参数 $b$ 的联合概率分布由下式给出：\n$$\np(y, \\lambda, b) = p(b \\mid c, d) \\prod_{i=1}^{N} p(y_i \\mid \\lambda_i) p(\\lambda_i \\mid a, b)\n$$\n各分量分布为：\n- $p(y_i \\mid \\lambda_i) = \\text{Poisson}(y_i \\mid s_i \\lambda_i) = \\frac{(s_i \\lambda_i)^{y_i} \\exp(-s_i \\lambda_i)}{y_i!}$\n- $p(\\lambda_i \\mid a, b) = \\text{Gamma}(\\lambda_i \\mid a, b) = \\frac{b^a}{\\Gamma(a)} \\lambda_i^{a-1} \\exp(-b \\lambda_i)$\n- $p(b \\mid c, d) = \\text{Gamma}(b \\mid c, d) = \\frac{d^c}{\\Gamma(c)} b^{c-1} \\exp(-d b)$\n\n该模型可以表示为因子图。变量 $\\lambda_1, \\dots, \\lambda_N$ 和 $b$ 是变量节点。因子是 $f_i(\\lambda_i) = p(y_i \\mid \\lambda_i)$（$i=1, \\dots, N$），每个因子连接到单个变量节点 $\\lambda_i$；$g_i(\\lambda_i, b) = p(\\lambda_i \\mid a, b)$（$i=1, \\dots, N$），每个因子连接到一对节点 $(\\lambda_i, b)$；以及 $h(b) = p(b \\mid c, d)$，它连接到节点 $b$。这种图结构明确了条件依赖关系：$\\lambda_i$ 的更新将依赖于其邻居，即数据 $y_i$ 和全局参数 $b$。$b$ 的更新将依赖于所有速率 $\\lambda_i$ 及其自身的先验参数。\n\n变分推断旨在通过最大化证据下界（ELBO）来用一个分解的分布 $q(\\lambda, b)$ 逼近真实的后验 $p(\\lambda, b \\mid y)$。对于平均场近似 $q(\\lambda, b) = q(b) \\prod_{i=1}^{N} q(\\lambda_i)$，在保持其他因子固定的情况下，单个因子 $q_j$（这里是 $q(\\lambda_i)$ 或 $q(b)$）的最优分布由坐标上升更新规则给出：\n$$\n\\ln q^*(x_j) = \\mathbb{E}_{q_{-j}}[\\ln p(y, \\lambda, b)] + \\text{constant}\n$$\n其中 $\\mathbb{E}_{q_{-j}}$ 表示在当前变分分布下对除 $x_j$ 之外的所有变量求期望。由于该模型由共轭指数族构建，更新后的变分分布 $q^*(\\lambda_i)$ 和 $q^*(b)$ 将与其先验保持在同一族中（伽马分布）。\n\n为了应用此规则，我们首先写出完整的对数联合概率：\n$$\n\\ln p(y, \\lambda, b) = \\sum_{i=1}^N \\ln p(y_i \\mid \\lambda_i) + \\sum_{i=1}^N \\ln p(\\lambda_i \\mid a, b) + \\ln p(b \\mid c, d)\n$$\n展开此式，我们得到：\n$$\n\\ln p(y, \\lambda, b) = \\sum_{i=1}^N \\left( y_i \\ln(s_i \\lambda_i) - s_i \\lambda_i - \\ln(y_i!) \\right) + \\sum_{i=1}^N \\left( a \\ln b - \\ln\\Gamma(a) + (a-1)\\ln\\lambda_i - b\\lambda_i \\right) + \\left( c \\ln d - \\ln\\Gamma(c) + (c-1)\\ln b - db \\right)\n$$\n我们可以省略相对于所有潜变量 $\\lambda$ 和 $b$ 均为常数的项：\n$$\n\\ln p(y, \\lambda, b) = \\sum_{i=1}^N \\left( y_i \\ln\\lambda_i - s_i \\lambda_i + (a-1)\\ln\\lambda_i - b\\lambda_i \\right) + Na \\ln b + (c-1)\\ln b - db + \\text{constant}\n$$\n为每个变量收集项：\n$$\n\\ln p(y, \\lambda, b) = \\sum_{i=1}^N \\left[ (y_i + a - 1)\\ln\\lambda_i - (s_i + b)\\lambda_i \\right] + (Na + c - 1)\\ln b - db + \\text{constant}\n$$\n\n**$q(\\lambda_i)$ 更新的推导**\n最优变分分布 $q^*(\\lambda_i)$ 是通过对对数联合概率关于所有其他变量（在本例中仅为 $b$）在其当前变分分布 $q(b)$ 下求期望得到的。\n$$\n\\ln q^*(\\lambda_i) = \\mathbb{E}_{q(b)}[\\ln p(y, \\lambda, b)] + \\text{constant}_{\\lambda_i}\n$$\n我们只需要 $\\ln p(y, \\lambda, b)$ 中依赖于 $\\lambda_i$ 的项：\n$$\n\\ln q^*(\\lambda_i) \\propto \\mathbb{E}_{q(b)}\\left[ (y_i + a - 1)\\ln\\lambda_i - (s_i + b)\\lambda_i \\right]\n$$\n$$\n\\ln q^*(\\lambda_i) \\propto (y_i + a - 1)\\ln\\lambda_i - (s_i + \\mathbb{E}_{q}[b])\\lambda_i\n$$\n这个表达式是伽马分布核的对数。一个 $\\text{Gamma}(\\lambda_i \\mid \\tilde{a}_i, \\tilde{b}_i)$ 分布的对数密度是 $(\\tilde{a}_i - 1)\\ln\\lambda_i - \\tilde{b}_i\\lambda_i + \\text{constant}$。通过比较，我们确定了 $\\lambda_i$ 的变分后验的更新参数：\n形状：$\\tilde{a}_i = y_i + a$\n速率：$\\tilde{b}_i = s_i + \\mathbb{E}_{q}[b]$\n因此，$\\lambda_i$ 的更新后变分分布为 $q^*(\\lambda_i) = \\text{Gamma}(\\lambda_i \\mid y_i + a, s_i + \\mathbb{E}_{q}[b])$。\n\n**$q(b)$ 更新的推导**\n类似地，最优变分分布 $q^*(b)$ 是通过关于所有 $\\lambda_i$ 在其当前变分分布 $q(\\lambda_i)$ 下求期望得到的。\n$$\n\\ln q^*(b) = \\mathbb{E}_{\\prod_j q(\\lambda_j)}[\\ln p(y, \\lambda, b)] + \\text{constant}_{b}\n$$\n我们分离出 $\\ln p(y, \\lambda, b)$ 中依赖于 $b$ 的项：\n$$\n\\ln q^*(b) \\propto \\mathbb{E}_{\\prod_j q(\\lambda_j)}\\left[ -\\sum_{i=1}^N b\\lambda_i + (Na + c - 1)\\ln b - db \\right]\n$$\n$$\n\\ln q^*(b) \\propto \\mathbb{E}_{\\prod_j q(\\lambda_j)}\\left[ (Na + c - 1)\\ln b - b \\left( d + \\sum_{i=1}^N \\lambda_i \\right) \\right]\n$$\n关于 $q(\\lambda_i)$ 分布求期望得到：\n$$\n\\ln q^*(b) \\propto (Na + c - 1)\\ln b - b \\left( d + \\sum_{i=1}^N \\mathbb{E}_{q}[\\lambda_i] \\right)\n$$\n这是关于 $b$ 的伽马分布的对数核。通过与通用形式 $(\\tilde{c}-1)\\ln b - \\tilde{d}b + \\text{constant}$ 进行比较，我们确定更新后的参数：\n形状：$\\tilde{c} = Na + c$\n速率：$\\tilde{d} = d + \\sum_{i=1}^N \\mathbb{E}_{q}[\\lambda_i]$\n因此，$b$ 的更新后变分分布为 $q^*(b) = \\text{Gamma}(b \\mid Na + c, d + \\sum_{i=1}^N \\mathbb{E}_{q}[\\lambda_i])$。\n\n期望 $\\mathbb{E}_{q}[b]$ 和 $\\mathbb{E}_{q}[\\lambda_i]$ 使用变分分布的当前参数计算。对于一个分布 $q(x) = \\text{Gamma}(x \\mid \\alpha, \\beta)$，其期望为 $\\mathbb{E}_{q}[x] = \\alpha/\\beta$。VMP算法通过对每个变量迭代应用这些更新，直到ELBO收敛。\n\n最终得到的变量节点处变分分布的VMP更新对，即为 $q(\\lambda_i)$ 和 $q(b)$ 的新函数形式。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nq^*(\\lambda_i) = \\text{Gamma}(\\lambda_i \\mid y_i + a, s_i + \\mathbb{E}_{q}[b])\n\nq^*(b) = \\text{Gamma}(b \\mid Na + c, d + \\sum_{i=1}^{N} \\mathbb{E}_{q}[\\lambda_i])\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "对于大规模数据集和复杂的非共轭模型（例如使用神经网络的模型），经典的坐标上升变分推断方法在计算上变得不可行，现代变分推断因此转向了随机变分推断 (Stochastic Variational Inference, SVI)。本练习  的核心是使用“重参数化技巧”来推导证据下界 (ELBO) 的随机梯度估计器，这是训练变分自编码器 (VAE) 等现代神经生成模型的关键技术。",
            "id": "4032003",
            "problem": "考虑一个潜变量神经尖峰模型，该模型具有 $N$ 个独立观测值 $\\{x_{i}\\}_{i=1}^{N}$，其中每个 $x_{i} \\in \\mathbb{N}^{K}$ 是在固定时间窗口内收集的 $K$ 个神经元的尖峰计数向量。该生成模型引入了潜变量 $z_{i} \\in \\mathbb{R}^{D}$，其先验分布为标准正态分布 $p(z_{i}) = \\mathcal{N}(0, I)$，以及一个泊松似然，其率由一个线性-非线性映射参数化：对于每个神经元 $k \\in \\{1,\\dots,K\\}$，其条件分布为\n$$\np_{\\theta}(x_{ik} \\mid z_{i}) = \\mathrm{Poisson}(\\lambda_{ik}), \\quad \\text{其中} \\quad \\ln \\lambda_{ik} = (W z_{i} + b)_{k},\n$$\n其中 $\\theta = (W, b)$，$W \\in \\mathbb{R}^{K \\times D}$ 且 $b \\in \\mathbb{R}^{K}$。令变分族为摊销对角高斯分布，$q_{\\phi}(z_{i} \\mid x_{i}) = \\mathcal{N}\\!\\big(\\mu_{\\phi}(x_{i}), \\mathrm{diag}(\\sigma_{\\phi}(x_{i})^{2})\\big)$，其中 $\\mu_{\\phi} : \\mathbb{R}^{K} \\to \\mathbb{R}^{D}$ 和 $\\sigma_{\\phi} : \\mathbb{R}^{K} \\to \\mathbb{R}^{D}$ 是依赖于参数 $\\phi$ 的可微映射。使用标准重参数化 $z_{i} = \\mu_{\\phi}(x_{i}) + \\sigma_{\\phi}(x_{i}) \\odot \\epsilon_{i}$，其中 $\\epsilon_{i} \\sim \\mathcal{N}(0, I)$，$\\odot$ 表示逐元素乘法。\n\n从证据下界 (ELBO) 的定义出发，\n$$\n\\mathcal{L}(\\theta, \\phi) = \\sum_{i=1}^{N} \\mathbb{E}_{q_{\\phi}(z_{i} \\mid x_{i})}\\!\\big[ \\ln p_{\\theta}(x_{i} \\mid z_{i}) + \\ln p(z_{i}) - \\ln q_{\\phi}(z_{i} \\mid x_{i}) \\big],\n$$\n在小批量子采样下，推导 $\\nabla_{W} \\mathcal{L}$、$\\nabla_{b} \\mathcal{L}$ 和 $\\nabla_{\\phi} \\mathcal{L}$ 的随机梯度估计量。假设从 $\\{1,\\dots,N\\}$ 中无放回地均匀随机抽取一个大小为 $m$ 的小批量 $\\mathcal{B}$，并且对于每个 $i \\in \\mathcal{B}$，使用 $L$ 个独立的标准正态样本 $\\{\\epsilon_{i}^{(\\ell)}\\}_{\\ell=1}^{L}$ 来构成蒙特卡洛样本 $z_{i}^{(\\ell)} = \\mu_{\\phi}(x_{i}) + \\sigma_{\\phi}(x_{i}) \\odot \\epsilon_{i}^{(\\ell)}$。将随机梯度估计量用 $N$、$m$、$L$、$W$、$b$、$\\mu_{\\phi}(x_{i})$、$\\sigma_{\\phi}(x_{i})$、$x_{i}$ 和 $\\epsilon_{i}^{(\\ell)}$ 明确表示出来，并给出为确保相对于全数据 ELBO 的无偏性所必需的缩放规则。\n\n您必须从第一性原理出发推导您的表达式，从 ELBO 定义和模型的独立性结构开始，不调用预先封装好的梯度恒等式。阐明重参数化如何进入关于 $\\phi$ 的梯度计算中，并使用对角高斯分布 $q_{\\phi}(z_{i} \\mid x_{i})$ 相对于标准正态分布 $p(z_{i})$ 的 Kullback-Leibler 散度的解析形式，来获得不需要蒙特卡洛采样的那部分梯度。最后，以紧凑的形式给出 $\\nabla_{W} \\mathcal{L}$、$\\nabla_{b} \\mathcal{L}$ 和 $\\nabla_{\\phi} \\mathcal{L}$ 的无偏随机梯度的单一闭式解析表达式。不需要数值近似或四舍五入，也不涉及单位。使用 $\\,\\ln\\,$ 表示自然对数，并在推导中明确定义您使用的任何雅可比矩阵。",
            "solution": "整个数据集 $\\{x_i\\}_{i=1}^N$ 的证据下界 (ELBO) 由下式给出：\n$$\n\\mathcal{L}(\\theta, \\phi) = \\sum_{i=1}^{N} \\mathcal{L}_i(\\theta, \\phi) = \\sum_{i=1}^{N} \\mathbb{E}_{q_{\\phi}(z_{i} \\mid x_{i})}\\!\\big[ \\ln p_{\\theta}(x_{i} \\mid z_{i}) + \\ln p(z_{i}) - \\ln q_{\\phi}(z_{i} \\mid x_{i}) \\big]\n$$\n其中 $\\theta = (W, b)$。期望内的项可以拆分。单个数据点 $x_i$ 的 ELBO 可以写成期望对数似然项和 Kullback-Leibler (KL) 散度项的和：\n$$\n\\mathcal{L}_i(\\theta, \\phi) = \\mathbb{E}_{q_{\\phi}(z_{i} \\mid x_{i})}\\!\\big[ \\ln p_{\\theta}(x_{i} \\mid z_{i}) \\big] - D_{KL}\\big( q_{\\phi}(z_{i} \\mid x_{i}) \\,\\|\\, p(z_{i}) \\big)\n$$\n总 ELBO 的梯度是每个数据点梯度的总和：$\\nabla \\mathcal{L} = \\sum_{i=1}^{N} \\nabla \\mathcal{L}_i$。为了使用大小为 $m$ 的小批量 $\\mathcal{B} \\subset \\{1,\\dots,N\\}$ 构建该梯度的无偏随机估计量，我们对小批量中数据点的梯度进行平均，并将结果乘以 $\\frac{N}{m}$ 进行缩放。即 $\\widehat{\\nabla \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\nabla \\mathcal{L}_i$。我们的任务是推导 $\\nabla_{W} \\mathcal{L}_i$、$\\nabla_{b} \\mathcal{L}_i$ 和 $\\nabla_{\\phi} \\mathcal{L}_i$ 的表达式。\n\n首先，我们求出 KL 散度项的解析形式。先验分布是 $p(z_i) = \\mathcal{N}(0, I)$，变分后验是 $q_{\\phi}(z_i \\mid x_i) = \\mathcal{N}(z_i \\mid \\mu_i, \\mathrm{diag}(\\sigma_i^2))$，为简洁起见，我们记 $\\mu_i = \\mu_{\\phi}(x_i)$ 和 $\\sigma_i = \\sigma_{\\phi}(x_i)$。两个多元高斯分布 $\\mathcal{N}(\\mu_1, \\Sigma_1)$ 和 $\\mathcal{N}(\\mu_2, \\Sigma_2)$ 之间的 KL 散度是 $D_{KL}(\\mathcal{N}_1 \\| \\mathcal{N}_2) = \\frac{1}{2} \\left[ \\mathrm{tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\mu_2-\\mu_1)^T\\Sigma_2^{-1}(\\mu_2-\\mu_1) - D + \\ln\\left(\\frac{\\det\\Sigma_2}{\\det\\Sigma_1}\\right) \\right]$。\n对于我们的特定情况，这可以简化为：\n$$\nD_{KL}\\big( q_{\\phi}(z_{i} \\mid x_{i}) \\,\\|\\, p(z_{i}) \\big) = \\frac{1}{2} \\sum_{j=1}^{D} \\left( \\mu_{ij}^{2} + \\sigma_{ij}^{2} - 2\\ln(\\sigma_{ij}) - 1 \\right)\n$$\n其中 $\\mu_{ij}$ 和 $\\sigma_{ij}$ 分别是 $\\mu_i$ 和 $\\sigma_i$ 的第 $j$ 个分量。注意，这一项依赖于 $\\phi$ (通过 $\\mu_i$ 和 $\\sigma_i$)，但不依赖于 $\\theta=(W, b)$。\n\n接下来，我们分析期望对数似然项。单个尖峰计数 $x_{ik}$ 的对数似然是 $\\ln p_{\\theta}(x_{ik} \\mid z_i) = x_{ik} \\ln(\\lambda_{ik}) - \\lambda_{ik} - \\ln(x_{ik}!)$。已知 $\\ln \\lambda_{ik} = (W z_i + b)_k$，所以 $\\lambda_{ik} = \\exp((W z_i + b)_k)$。观测值 $x_i$ 的完整对数似然是：\n$$\n\\ln p_{\\theta}(x_i \\mid z_i) = \\sum_{k=1}^{K} \\left( x_{ik} (W z_i + b)_k - \\exp((W z_i + b)_k) \\right) - \\sum_{k=1}^{K} \\ln(x_{ik}!)\n$$\n期望 $\\mathbb{E}_{q_{\\phi}(z_{i} \\mid x_{i})}[\\cdot]$ 难以计算。我们使用包含 $L$ 个样本的蒙特卡洛估计来近似它，并采用重参数化技巧：$z_i^{(\\ell)} = \\mu_i + \\sigma_i \\odot \\epsilon_i^{(\\ell)}$，其中 $\\epsilon_i^{(\\ell)} \\sim \\mathcal{N}(0, I)$，$\\ell=1, \\dots, L$。\n$$\n\\mathbb{E}_{q_{\\phi}}[\\ln p_{\\theta}(x_i \\mid z_i)] \\approx \\frac{1}{L} \\sum_{\\ell=1}^{L} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)})\n$$\n因此，$\\mathcal{L}_i$ 的蒙特卡洛估计量为：\n$$\n\\hat{\\mathcal{L}}_i(\\theta, \\phi) = \\frac{1}{L} \\sum_{\\ell=1}^{L} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}) - \\frac{1}{2} \\sum_{j=1}^{D} \\left( \\mu_{ij}^{2} + \\sigma_{ij}^{2} - 2\\ln(\\sigma_{ij}) - 1 \\right)\n$$\n\n我们现在来推导梯度。\n\n**关于模型参数 $\\theta = (W, b)$ 的梯度：**\n参数 $W$ 和 $b$ 仅出现在对数似然项中。梯度算子可以穿过求和与蒙特卡洛平均。\n令 $\\eta_i^{(\\ell)} = W z_i^{(\\ell)} + b$，则 $\\lambda_i^{(\\ell)} = \\exp(\\eta_i^{(\\ell)})$。神经元 $k$ 的残差是 $x_{ik} - \\lambda_{ik}^{(\\ell)}$。\n对于偏置向量 $b$：\n$$\n\\nabla_b \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}) = x_i - \\lambda_i^{(\\ell)}\n$$\n对于权重矩阵 $W$：关于矩阵元素 $W_{kj}$ 的导数是 $(x_{ik} - \\lambda_{ik}^{(\\ell)})z_{ij}^{(\\ell)}$。以矩阵形式表示，这是一个外积：\n$$\n\\nabla_W \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}) = (x_i - \\lambda_i^{(\\ell)}) (z_i^{(\\ell)})^T\n$$\n对 $L$ 个蒙特卡洛样本求平均，得到 $\\mathcal{L}_i$ 的梯度估计量。然后，完整的无偏小批量随机梯度估计量为：\n$$\n\\widehat{\\nabla_W \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\left[ \\frac{1}{L} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)}) (z_i^{(\\ell)})^T \\right]\n$$\n$$\n\\widehat{\\nabla_b \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\left[ \\frac{1}{L} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)}) \\right]\n$$\n其中 $\\lambda_i^{(\\ell)} = \\exp(W(\\mu_i + \\sigma_i \\odot \\epsilon_i^{(\\ell)}) + b)$。\n\n**关于变分参数 $\\phi$ 的梯度：**\n参数向量 $\\phi$ 通过对数似然项（通过 $z_i$ 的分布）和 KL 散度项（通过 $\\mu_i$ 和 $\\sigma_i$）影响 $\\mathcal{L}_i$。我们使用链式法则，对推理网络的输出 $\\mu_i$ 和 $\\sigma_i$ 求梯度。\n\n对数似然项的关键步骤是重参数化技巧，它将关于 $q_\\phi$ 的期望重新表示为关于一个不依赖于 $\\phi$ 的固定分布 $p(\\epsilon)$ 的期望。这允许交换梯度和期望算子：\n$$\n\\nabla_{\\phi} \\mathbb{E}_{q_{\\phi}(z_i \\mid x_i)}[\\ln p_{\\theta}(x_i \\mid z_i)] = \\nabla_{\\phi} \\mathbb{E}_{p(\\epsilon_i)}[\\ln p_{\\theta}(x_i \\mid \\mu_i + \\sigma_i \\odot \\epsilon_i)] = \\mathbb{E}_{p(\\epsilon_i)}[\\nabla_{\\phi} \\ln p_{\\theta}(x_i \\mid \\mu_i + \\sigma_i \\odot \\epsilon_i)]\n$$\n然后用蒙特卡洛采样来估计梯度。我们计算 $\\hat{\\mathcal{L}}_i$ 关于 $\\mu_i$ 和 $\\sigma_i$ 的梯度，然后通过链式法则链接到 $\\phi$。\n$$\n\\nabla_{\\phi} \\hat{\\mathcal{L}}_i = (\\nabla_{\\mu_i}\\hat{\\mathcal{L}}_i)^T \\frac{\\partial \\mu_i}{\\partial \\phi} + (\\nabla_{\\sigma_i}\\hat{\\mathcal{L}}_i)^T \\frac{\\partial \\sigma_i}{\\partial \\phi}\n$$\n项 $\\frac{\\partial \\mu_i}{\\partial \\phi}$ 和 $\\frac{\\partial \\sigma_i}{\\partial \\phi}$ 是推理网络的雅可比矩阵。我们将其定义为 $J_{\\mu_i} \\triangleq \\nabla_{\\phi} \\mu_{\\phi}(x_i)$ 和 $J_{\\sigma_i} \\triangleq \\nabla_{\\phi} \\sigma_{\\phi}(x_i)$。\n\n对数似然项关于 $\\mu_i$ 的梯度是：\n$$\n\\nabla_{\\mu_i} \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}) \\right) = \\frac{1}{L} \\sum_{\\ell=1}^{L} (\\nabla_{z_i^{(\\ell)}} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}))^T \\frac{\\partial z_i^{(\\ell)}}{\\partial \\mu_i}\n$$\n我们有 $\\nabla_z \\ln p_\\theta(x_i|z) = W^T (x_i - \\lambda_i)$ 且 $\\frac{\\partial z_i^{(\\ell)}}{\\partial \\mu_i} = I$。所以梯度是 $\\frac{1}{L} \\sum_{\\ell=1}^{L} W^T(x_i - \\lambda_i^{(\\ell)})$。\n对数似然项关于 $\\sigma_i$ 的梯度是：\n$$\n\\nabla_{\\sigma_i} \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}) \\right) = \\frac{1}{L} \\sum_{\\ell=1}^{L} (\\nabla_{z_i^{(\\ell)}} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}))^T \\frac{\\partial z_i^{(\\ell)}}{\\partial \\sigma_i}\n$$\n这里，$\\frac{\\partial z_i^{(\\ell)}}{\\partial \\sigma_i} = \\mathrm{diag}(\\epsilon_i^{(\\ell)})$，导致逐元素乘法。梯度是 $\\frac{1}{L} \\sum_{\\ell=1}^{L} (W^T(x_i - \\lambda_i^{(\\ell)})) \\odot \\epsilon_i^{(\\ell)}$。\n\n负 KL 散度项 $-\\frac{1}{2} \\sum_{j=1}^{D} (\\mu_{ij}^2 + \\sigma_{ij}^2 - 2\\ln \\sigma_{ij} - 1)$ 的梯度是：\n$$\n\\nabla_{\\mu_i} (-D_{KL}) = -\\mu_i\n$$\n$$\n\\nabla_{\\sigma_i} (-D_{KL}) = -(\\sigma_i - \\sigma_i^{\\odot-1}) = \\sigma_i^{\\odot-1} - \\sigma_i\n$$\n其中 $\\sigma_i^{\\odot-1}$ 是逐元素倒数的向量 $(1/\\sigma_{i1}, \\dots, 1/\\sigma_{iD})^T$。\n\n结合这些部分，$\\hat{\\mathcal{L}}_i$ 关于 $\\phi$ 的梯度是：\n$$\n\\nabla_{\\phi} \\hat{\\mathcal{L}}_i = J_{\\mu_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} W^T (x_i - \\lambda_i^{(\\ell)}) \\right) - \\mu_i \\right] + J_{\\sigma_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} (W^T (x_i - \\lambda_i^{(\\ell)})) \\odot \\epsilon_i^{(\\ell)} \\right) + (\\sigma_i^{\\odot-1} - \\sigma_i) \\right]\n$$\n完整的无偏小批量随机梯度估计量是：\n$$\n\\widehat{\\nabla_{\\phi} \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\nabla_{\\phi} \\hat{\\mathcal{L}}_i\n$$\n为保证无偏性所必需的缩放规则是对小批量 $\\mathcal{B}$ 的总和应用因子 $\\frac{N}{m}$。\n\n为了提供梯度的单一紧凑表达式，我们为每个 $i \\in \\mathcal{B}$ 定义以下量：\n令 $\\mu_i = \\mu_{\\phi}(x_i)$，$\\sigma_i=\\sigma_{\\phi}(x_i)$。对于 $\\ell=1, \\dots, L$，令 $\\epsilon_i^{(\\ell)} \\sim \\mathcal{N}(0, I)$，$z_i^{(\\ell)} = \\mu_i+\\sigma_i \\odot \\epsilon_i^{(\\ell)}$，以及 $\\lambda_i^{(\\ell)} = \\exp(W z_i^{(\\ell)} + b)$。\n并令 $J_{\\mu_i} \\triangleq \\nabla_{\\phi} \\mu_{\\phi}(x_i)$ 和 $J_{\\sigma_i} \\triangleq \\nabla_{\\phi} \\sigma_{\\phi}(x_i)$。\n\n无偏随机梯度估计量是：\n$$\n\\widehat{\\nabla_W \\mathcal{L}} = \\frac{N}{mL} \\sum_{i \\in \\mathcal{B}} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)}) (z_i^{(\\ell)})^T\n$$\n$$\n\\widehat{\\nabla_b \\mathcal{L}} = \\frac{N}{mL} \\sum_{i \\in \\mathcal{B}} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)})\n$$\n$$\n\\widehat{\\nabla_{\\phi} \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\left( J_{\\mu_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} W^T (x_i - \\lambda_i^{(\\ell)}) \\right) - \\mu_i \\right] + J_{\\sigma_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} (W^T (x_i - \\lambda_i^{(\\ell)})) \\odot \\epsilon_i^{(\\ell)} \\right) + \\frac{1}{\\sigma_i} - \\sigma_i \\right] \\right)\n$$\n其中向量的除法和指数运算是逐元素的。为了更紧凑的表示，可以将 $W$ 和 $b$ 的求和与平均移到内部。最终表达式报告如下。",
            "answer": "$$\n\\boxed{\\begin{aligned}\n\\widehat{\\nabla_W \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\frac{1}{L} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)}) (z_i^{(\\ell)})^T \\\\\n\\widehat{\\nabla_b \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\frac{1}{L} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)}) \\\\\n\\widehat{\\nabla_{\\phi} \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\left( J_{\\mu_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} W^T (x_i - \\lambda_i^{(\\ell)}) \\right) - \\mu_i \\right] + J_{\\sigma_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} (W^T (x_i - \\lambda_i^{(\\ell)})) \\odot \\epsilon_i^{(\\ell)} \\right) + \\sigma_i^{\\odot-1} - \\sigma_i \\right] \\right)\n\\end{aligned}}\n$$"
        },
        {
            "introduction": "在掌握了如何实现变分推断之后，理解其内在属性变得至关重要。证据下界 (ELBO) 在数据拟合项和一个将后验分布正则化到先验分布的 KL 散度项之间取得了平衡。本练习  提供了一个计算实验，让你亲手探究不准确的先验设定如何影响学习到的后验分布乃至 ELBO 本身，从而让你对先验在贝叶斯推断中的关键作用获得具体的认识。",
            "id": "4032007",
            "problem": "考虑一个线性高斯神经编码模型，其具有潜变量 $\\mathbf{z} \\in \\mathbb{R}^{M}$、刺激 $\\mathbf{s} \\in \\mathbb{R}^{D}$ 和神经响应 $\\mathbf{y} \\in \\mathbb{R}^{K}$。其生成模型由下式指定\n$$\n\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s} \\sim \\mathcal{N}(\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{s}, \\sigma^{2}\\mathbf{I}_{K}),\n$$\n其中 $\\mathbf{W} \\in \\mathbb{R}^{K \\times M}$，$\\mathbf{U} \\in \\mathbb{R}^{K \\times D}$，且 $\\sigma^{2} \\in \\mathbb{R}_{+}$。用于生成数据的真实潜变量先验是\n$$\np_{\\text{true}}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{M}).\n$$\n然而，在推断过程中，你将假设一个（可能不准确的）先验\n$$\np(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_{p}, \\boldsymbol{\\Sigma}_{p}),\n$$\n并使用一个高斯变分族 $q(\\mathbf{z}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$。对于线性高斯模型，在假设模型下，高斯 $q$ 上的证据下界 (ELBO) 的最大化器等于假设模型的精确后验，\n$$\nq^{\\star}(\\mathbf{z}) = p(\\mathbf{z} \\mid \\mathbf{y}, \\mathbf{s}).\n$$\n\n你的任务是，对于几个假设的先验 $p(\\mathbf{z})$，评估不准确的先验设定如何通过 KL 散度 (Kullback–Leibler divergence) 正则化项 $\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z}))$ 影响 ELBO 优化，并量化其对所学到的后验均值相对于正确先验 $p_{\\text{true}}(\\mathbf{z})$ 下的后验的影响。\n\n使用以下固定的模型参数和数据集：\n- 维数：$K = 3$, $M = 2$, $D = 2$。\n- 观测噪声：$\\sigma^{2} = 0.25$。\n- 编码矩阵\n$$\n\\mathbf{W} = \\begin{bmatrix}\n0.9  -0.3 \\\\\n0.2  0.8 \\\\\n-0.5  0.4\n\\end{bmatrix},\n\\quad\n\\mathbf{U} = \\begin{bmatrix}\n0.5  -0.2 \\\\\n-0.1  0.3 \\\\\n0.0  0.4\n\\end{bmatrix}.\n$$\n- 由 $\\mathbf{y}_{i} = \\mathbf{W}\\mathbf{z}_{i} + \\mathbf{U}\\mathbf{s}_{i} + \\boldsymbol{\\epsilon}_{i}$ 生成的 $N = 4$ 个刺激-响应对的数据集，其中 $\\boldsymbol{\\epsilon}_{i}$ 与 $\\sigma^{2}$ 一致（但为确定性而固定）：\n  - 刺激 $\\{\\mathbf{s}_{i}\\}_{i=1}^{4}$：\n    - $\\mathbf{s}_{1} = [1.0, 0.5]^{\\top}$，\n    - $\\mathbf{s}_{2} = [-0.5, 1.2]^{\\top}$，\n    - $\\mathbf{s}_{3} = [0.0, -1.0]^{\\top}$，\n    - $\\mathbf{s}_{4} = [2.0, 0.0]^{\\top}$。\n  - 潜变量 $\\{\\mathbf{z}_{i}\\}_{i=1}^{4}$：\n    - $\\mathbf{z}_{1} = [0.3, -0.7]^{\\top}$，\n    - $\\mathbf{z}_{2} = [-1.0, 0.5]^{\\top}$，\n    - $\\mathbf{z}_{3} = [0.2, 0.2]^{\\top}$，\n    - $\\mathbf{z}_{4} = [1.5, -0.3]^{\\top}$。\n  - 固定噪声向量 $\\{\\boldsymbol{\\epsilon}_{i}\\}_{i=1}^{4}$：\n    - $\\boldsymbol{\\epsilon}_{1} = [0.05, -0.02, 0.10]^{\\top}$，\n    - $\\boldsymbol{\\epsilon}_{2} = [-0.10, 0.20, -0.05]^{\\top}$，\n    - $\\boldsymbol{\\epsilon}_{3} = [0.00, -0.08, 0.03]^{\\top}$，\n    - $\\boldsymbol{\\epsilon}_{4} = [0.12, -0.15, 0.00]^{\\top}$。\n  - 由上述参数所隐含的相应响应 $\\{\\mathbf{y}_{i}\\}_{i=1}^{4}$ 是：\n    - $\\mathbf{y}_{1} = [0.93, -0.47, -0.13]^{\\top}$，\n    - $\\mathbf{y}_{2} = [-1.64, 0.81, 1.13]^{\\top}$，\n    - $\\mathbf{y}_{3} = [0.32, -0.18, -0.39]^{\\top}$，\n    - $\\mathbf{y}_{4} = [2.56, -0.29, -0.87]^{\\top}$。\n\n假设先验的测试套件 $p(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_{p}, \\operatorname{diag}(\\mathbf{v}_{p}))$：\n- 情况 A（正确先验）：$\\boldsymbol{\\mu}_{p} = [0.0, 0.0]^{\\top}$，$\\mathbf{v}_{p} = [1.0, 1.0]^{\\top}$。\n- 情况 B（无信息宽先验）：$\\boldsymbol{\\mu}_{p} = [0.0, 0.0]^{\\top}$，$\\mathbf{v}_{p} = [10.0, 10.0]^{\\top}$。\n- 情况 C（轻度不准确先验）：$\\boldsymbol{\\mu}_{p} = [0.5, -0.2]^{\\top}$，$\\mathbf{v}_{p} = [1.5, 0.7]^{\\top}$。\n- 情况 D（过度自信的错误先验）：$\\boldsymbol{\\mu}_{p} = [1.0, 1.0]^{\\top}$，$\\mathbf{v}_{p} = [0.3, 0.3]^{\\top}$。\n\n对每个先验情况，为每个数据点 $(\\mathbf{s}_{i}, \\mathbf{y}_{i})$ 执行以下操作：\n1) 计算假设模型下的最优变分后验 $q^{\\star}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$，其中\n$$\n\\mathbf{S} = \\left(\\boldsymbol{\\Sigma}_{p}^{-1} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\mathbf{W} \\right)^{-1}, \\quad\n\\mathbf{m} = \\mathbf{S}\\left(\\boldsymbol{\\Sigma}_{p}^{-1}\\boldsymbol{\\mu}_{p} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\left(\\mathbf{y}_{i} - \\mathbf{U}\\mathbf{s}_{i}\\right)\\right).\n$$\n2) 计算期望对数似然项\n$$\n\\mathbb{E}_{q}[\\log p(\\mathbf{y}_{i} \\mid \\mathbf{z}, \\mathbf{s}_{i})] = -\\frac{K}{2}\\log(2\\pi \\sigma^{2}) - \\frac{1}{2\\sigma^{2}}\\left(\\left\\|\\mathbf{y}_{i} - \\mathbf{U}\\mathbf{s}_{i} - \\mathbf{W}\\mathbf{m}\\right\\|_{2}^{2} + \\operatorname{tr}(\\mathbf{W}\\mathbf{S}\\mathbf{W}^{\\top})\\right).\n$$\n3) 计算 KL 散度 (Kullback–Leibler divergence)\n$$\n\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z})) = \\frac{1}{2}\\left(\\log\\frac{\\det \\boldsymbol{\\Sigma}_{p}}{\\det \\mathbf{S}} - M + \\operatorname{tr}(\\boldsymbol{\\Sigma}_{p}^{-1}\\mathbf{S}) + (\\mathbf{m}-\\boldsymbol{\\mu}_{p})^{\\top}\\boldsymbol{\\Sigma}_{p}^{-1}(\\mathbf{m}-\\boldsymbol{\\mu}_{p})\\right).\n$$\n4) 计算数据点的 ELBO\n$$\n\\text{ELBO}_{i} = \\mathbb{E}_{q}[\\log p(\\mathbf{y}_{i} \\mid \\mathbf{z}, \\mathbf{s}_{i})] - \\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z})).\n$$\n5) 对于相同的 $(\\mathbf{s}_{i}, \\mathbf{y}_{i})$，计算在正确先验 $p_{\\text{true}}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{M})$ 下的后验均值：\n$$\n\\mathbf{S}_{\\text{true}} = \\left(\\mathbf{I}_{M} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\mathbf{W}\\right)^{-1}, \\quad\n\\mathbf{m}_{\\text{true}} = \\mathbf{S}_{\\text{true}}\\left(\\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\left(\\mathbf{y}_{i} - \\mathbf{U}\\mathbf{s}_{i}\\right)\\right).\n$$\n通过下式量化所学后验均值的偏差\n$$\n\\Delta_{i} = \\left\\|\\mathbf{m} - \\mathbf{m}_{\\text{true}}\\right\\|_{2}.\n$$\n6) 使用下式量化负 ELBO 目标 $-\\text{ELBO}_{i} = -\\mathbb{E}_{q}[\\log p(\\mathbf{y}_{i} \\mid \\mathbf{z}, \\mathbf{s}_{i})] + \\mathrm{KL}$ 中正则化项的相对影响\n$$\n\\rho_{i} = \\frac{\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z}))}{\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z})) + \\left(-\\mathbb{E}_{q}[\\log p(\\mathbf{y}_{i} \\mid \\mathbf{z}, \\mathbf{s}_{i})]\\right)}.\n$$\n\n对每个先验情况，报告在 $N = 4$ 个数据点上的平均值：\n- $\\overline{\\text{ELBO}} = \\frac{1}{N}\\sum_{i=1}^{N} \\text{ELBO}_{i}$，\n- $\\overline{\\mathrm{KL}} = \\frac{1}{N}\\sum_{i=1}^{N} \\mathrm{KL}_{i}$，\n- $\\overline{\\Delta} = \\frac{1}{N}\\sum_{i=1}^{N} \\Delta_{i}$，\n- $\\overline{\\rho} = \\frac{1}{N}\\sum_{i=1}^{N} \\rho_{i}$。\n\n程序要求：\n- 对四种先验情况 A、B、C 和 D 精确实现上述计算。\n- 最终程序输出必须是单行，包含一个用方括号括起来的逗号分隔的浮点数列表。序列必须是\n$$\n[\\overline{\\text{ELBO}}_{A}, \\overline{\\mathrm{KL}}_{A}, \\overline{\\Delta}_{A}, \\overline{\\rho}_{A}, \\overline{\\text{ELBO}}_{B}, \\overline{\\mathrm{KL}}_{B}, \\overline{\\Delta}_{B}, \\overline{\\rho}_{B}, \\overline{\\text{ELBO}}_{C}, \\overline{\\mathrm{KL}}_{C}, \\overline{\\Delta}_{C}, \\overline{\\rho}_{C}, \\overline{\\text{ELBO}}_{D}, \\overline{\\mathrm{KL}}_{D}, \\overline{\\Delta}_{D}, \\overline{\\rho}_{D}],\n$$\n每个值四舍五入到 $6$ 位小数。\n- 不应打印任何其他文本。\n\n此问题不涉及物理单位或角度单位；所有量均为无量纲。",
            "solution": "该问题要求我们在一个用于线性高斯神经编码模型的变分贝叶斯框架中，分析先验设定不当（prior misspecification）的影响。我们将为四种不同的假设先验——一个正确的、一个无信息的、一个轻度不准确的和一个过度自信的错误先验——计算若干度量指标，以量化它们对所学后验分布和证据下界（ELBO）的影响。\n\n生成模型定义为：\n$$\np(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s}) = \\mathcal{N}(\\mathbf{y} \\mid \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{s}, \\sigma^{2}\\mathbf{I}_{K})\n$$\n完整的生成过程，包括潜变量 $\\mathbf{z}$ 上的先验，是 $p(\\mathbf{y}, \\mathbf{z} \\mid \\mathbf{s}) = p(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s}) p(\\mathbf{z})$。\n\n变分推断旨在通过最大化 ELBO 来使用一个来自易处理族（tractable family）的分布 $q(\\mathbf{z})$ 来近似真实后验 $p(\\mathbf{z} \\mid \\mathbf{y}, \\mathbf{s})$。ELBO 是模型对数证据 $\\log p(\\mathbf{y} \\mid \\mathbf{s})$ 的一个下界。ELBO 由下式给出：\n$$\n\\text{ELBO}(q) = \\mathbb{E}_{q(\\mathbf{z})}[\\log p(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s})] - \\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z}))\n$$\n这个表达式将目标分解为两项：\n1.  期望对数似然，$\\mathbb{E}_{q(\\mathbf{z})}[\\log p(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s})]$，它促使后验 $q(\\mathbf{z})$ 能够准确地重构观测数据 $\\mathbf{y}$。这是一个数据拟合或重构项。\n2.  KL 散度（Kullback-Leibler divergence），$\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z}))$，它惩罚近似后验 $q(\\mathbf{z})$ 相对于假设先验 $p(\\mathbf{z})$ 的偏离。这起到了正则化项的作用。\n\n对于一个具有高斯先验 $p(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_{p}, \\boldsymbol{\\Sigma}_{p})$ 和高斯似然的线性高斯模型，其真实后验 $p(\\mathbf{z} \\mid \\mathbf{y}, \\mathbf{s})$ 也是高斯的。因此，当我们使用高斯变分族 $q(\\mathbf{z}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$ 时，最大化 ELBO 的最优变分后验 $q^{\\star}(\\mathbf{z})$ 正是*假设*模型的真实后验，$p(\\mathbf{z} \\mid \\mathbf{y}, \\mathbf{s})$。该后验的参数是通过对对数后验 $\\log p(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s}) + \\log p(\\mathbf{z})$ 的指数部分进行配方推导出来的。这给出了后验协方差 $\\mathbf{S}$ 和均值 $\\mathbf{m}$ 的解析表达式：\n$$\n\\mathbf{S}^{-1} = \\boldsymbol{\\Sigma}_{p}^{-1} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\mathbf{W} \\implies \\mathbf{S} = \\left(\\boldsymbol{\\Sigma}_{p}^{-1} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\mathbf{W} \\right)^{-1}\n$$\n$$\n\\mathbf{S}^{-1}\\mathbf{m} = \\boldsymbol{\\Sigma}_{p}^{-1}\\boldsymbol{\\mu}_{p} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}(\\mathbf{y} - \\mathbf{U}\\mathbf{s}) \\implies \\mathbf{m} = \\mathbf{S}\\left(\\boldsymbol{\\Sigma}_{p}^{-1}\\boldsymbol{\\mu}_{p} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}(\\mathbf{y} - \\mathbf{U}\\mathbf{s})\\right)\n$$\n这些公式描述了先验信息（均值 $\\boldsymbol{\\mu}_p$，精度 $\\boldsymbol{\\Sigma}_p^{-1}$）如何与来自数据的信息（由似然派生的项概括）结合以形成后验。\n\n分析过程是通过为每个假设的先验和每个数据点 $(\\mathbf{s}_i, \\mathbf{y}_i)$ 计算以下度量指标来进行的：\n-   $\\mathbf{m}_{\\text{true}}$：如果我们使用了正确的生成先验 $p_{\\text{true}}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{M})$，所得到的后验均值。对于每个数据点，这作为我们对潜变量估计的基准参考。\n-   $\\Delta_{i} = \\left\\|\\mathbf{m} - \\mathbf{m}_{\\text{true}}\\right\\|_{2}$：使用（可能不正确的）假设先验计算出的后验均值与参考均值之间的欧几里得距离。这直接量化了由先验设定不当引起的潜变量估计误差。\n-   $\\text{ELBO}_{i}$：证据下界的值。对于 $q=q^\\star$，这等于在假设模型下数据的对数边缘似然，$\\log p(\\mathbf{y}_i | \\mathbf{s}_i)$。它衡量了假设模型（先验+似然）对数据的解释程度。\n-   $\\mathrm{KL}_{i}$：从先验到后验的 KL 散度。它衡量从观测数据中获得的信息增益，或者等价地，偏离先验的“代价”。\n-   $\\rho_{i}$：KL 散度与负 ELBO 的比率。负 ELBO 可以解释为总成本或“自由能”。因此，$\\rho_i$ 代表了该总成本中可归因于正则化项的部分，显示了先验在塑造最终后验中的相对影响。\n\n计算过程如下：\n1.  按规定初始化所有模型参数（$\\mathbf{W}$、$\\mathbf{U}$、$\\sigma^2$）和数据集（$\\{\\mathbf{s}_i, \\mathbf{y}_i\\}$）。\n2.  对 $N=4$ 个数据点中的每一个，使用真实先验 $p_{\\text{true}}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{M})$ 预先计算参考后验均值 $\\mathbf{m}_{\\text{true}, i}$。\n3.  遍历四种指定的先验情况（A、B、C、D）。\n4.  对于每种先验情况，遍历四个数据点。\n5.  在内层循环中，对于给定的先验设定 $(\\boldsymbol{\\mu}_{p}, \\boldsymbol{\\Sigma}_{p})$ 和数据点 $(\\mathbf{s}_i, \\mathbf{y}_i)$：\n    a. 使用提供的解析公式计算后验参数 $\\mathbf{S}$ 和 $\\mathbf{m}$。\n    b. 计算后验均值的误差，$\\Delta_{i} = \\left\\|\\mathbf{m} - \\mathbf{m}_{\\text{true}, i}\\right\\|_{2}$。\n    c. 计算期望对数似然，$\\mathbb{E}_{q}[\\log p(\\mathbf{y}_{i} \\mid \\mathbf{z}, \\mathbf{s}_{i})]$。\n    d. 计算 KL 散度，$\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z}))$。\n    e. 计算 ELBO，$\\text{ELBO}_{i} = \\mathbb{E}_{q}[\\dots] - \\mathrm{KL}$。\n    f. 计算相对 KL 影响，$\\rho_i$。\n6.  对于每种先验情况，将计算出的度量指标（$\\text{ELBO}_{i}$、$\\mathrm{KL}_{i}$、$\\Delta_{i}$、$\\rho_{i}$）在 $N=4$ 个数据点上取平均。\n7.  最后，按照最终输出的要求收集并格式化 16 个平均值。这涉及直接实现问题陈述中定义的矩阵和向量运算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the variational Bayesian inference problem for a linear-Gaussian model\n    under different prior specifications.\n    \"\"\"\n    # Fixed model parameters and dataset\n    K, M, D = 3, 2, 2\n    sigma2 = 0.25\n    inv_sigma2 = 1.0 / sigma2\n\n    W = np.array([\n        [0.9, -0.3],\n        [0.2, 0.8],\n        [-0.5, 0.4]\n    ])\n\n    U = np.array([\n        [0.5, -0.2],\n        [-0.1, 0.3],\n        [0.0, 0.4]\n    ])\n\n    stimuli_data = [\n        np.array([1.0, 0.5]),\n        np.array([-0.5, 1.2]),\n        np.array([0.0, -1.0]),\n        np.array([2.0, 0.0])\n    ]\n\n    responses_data = [\n        np.array([0.93, -0.47, -0.13]),\n        np.array([-1.64, 0.81, 1.13]),\n        np.array([0.32, -0.18, -0.39]),\n        np.array([2.56, -0.29, -0.87])\n    ]\n    \n    N = len(stimuli_data)\n\n    # Test suite of assumed priors\n    test_cases = [\n        # Case A: Correct prior\n        {'mu_p': np.array([0.0, 0.0]), 'v_p': np.array([1.0, 1.0])},\n        # Case B: Uninformative broad prior\n        {'mu_p': np.array([0.0, 0.0]), 'v_p': np.array([10.0, 10.0])},\n        # Case C: Mildly inaccurate prior\n        {'mu_p': np.array([0.5, -0.2]), 'v_p': np.array([1.5, 0.7])},\n        # Case D: Overconfident wrong prior\n        {'mu_p': np.array([1.0, 1.0]), 'v_p': np.array([0.3, 0.3])}\n    ]\n\n    # Pre-compute constant matrices\n    WTW = W.T @ W\n    WT = W.T\n    I_M = np.eye(M)\n\n    # --- Compute posterior means under the correct prior (m_true) for all data points ---\n    Sigma_p_true_inv = I_M  # Since Sigma_p_true is Identity\n    S_true_inv = Sigma_p_true_inv + inv_sigma2 * WTW\n    S_true = np.linalg.inv(S_true_inv)\n\n    m_true_list = []\n    for i in range(N):\n        s_i = stimuli_data[i]\n        y_i = responses_data[i]\n        \n        # This part of the calculation for m_true uses mu_p_true = 0\n        m_true = S_true @ (inv_sigma2 * WT @ (y_i - U @ s_i))\n        m_true_list.append(m_true)\n\n    final_results = []\n    # --- Loop through each test case ---\n    for case_params in test_cases:\n        mu_p = case_params['mu_p']\n        v_p = case_params['v_p']\n        \n        Sigma_p = np.diag(v_p)\n        Sigma_p_inv = np.diag(1.0 / v_p)\n        log_det_Sigma_p = np.sum(np.log(v_p))\n\n        # Store metrics for this case to be averaged later\n        elbo_list, kl_list, delta_list, rho_list = [], [], [], []\n\n        # --- Loop through each data point ---\n        for i in range(N):\n            s_i = stimuli_data[i]\n            y_i = responses_data[i]\n            m_true = m_true_list[i]\n\n            # 1) Compute optimal variational posterior q*(z) = N(m, S)\n            S_inv = Sigma_p_inv + inv_sigma2 * WTW\n            S = np.linalg.inv(S_inv)\n            log_det_S = np.linalg.slogdet(S)[1]\n            \n            m = S @ (Sigma_p_inv @ mu_p + inv_sigma2 * WT @ (y_i - U @ s_i))\n\n            # 2) Compute expected log-likelihood\n            term_const = -0.5 * K * np.log(2 * np.pi * sigma2)\n            y_hat = y_i - U @ s_i\n            reconstruction_term = np.sum((y_hat - W @ m)**2)\n            trace_term = np.trace(WTW @ S)\n            \n            E_log_lik = term_const - 0.5 * inv_sigma2 * (reconstruction_term + trace_term)\n\n            # 3) Compute KL divergence KL(q || p)\n            trace_Sigma_inv_S = np.trace(Sigma_p_inv @ S)\n            m_minus_mu = m - mu_p\n            quad_term = m_minus_mu.T @ Sigma_p_inv @ m_minus_mu\n            \n            kl_div = 0.5 * (log_det_Sigma_p - log_det_S - M + trace_Sigma_inv_S + quad_term)\n            \n            # 4) Compute ELBO\n            elbo = E_log_lik - kl_div\n            \n            # 5) Compute deviation Delta\n            delta = np.linalg.norm(m - m_true)\n\n            # 6) Compute relative influence rho\n            neg_E_log_lik = -E_log_lik\n            # The denominator is -ELBO, which is guaranteed to be positive\n            rho = kl_div / (kl_div + neg_E_log_lik)\n\n            # Append to lists\n            elbo_list.append(elbo)\n            kl_list.append(kl_div)\n            delta_list.append(delta)\n            rho_list.append(rho)\n\n        # Average the metrics for the current case\n        avg_elbo = np.mean(elbo_list)\n        avg_kl = np.mean(kl_list)\n        avg_delta = np.mean(delta_list)\n        avg_rho = np.mean(rho_list)\n        \n        final_results.extend([avg_elbo, avg_kl, avg_delta, avg_rho])\n\n    # Final print statement in the exact required format\n    output_str = ','.join(f'{x:.6f}' for x in final_results)\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}