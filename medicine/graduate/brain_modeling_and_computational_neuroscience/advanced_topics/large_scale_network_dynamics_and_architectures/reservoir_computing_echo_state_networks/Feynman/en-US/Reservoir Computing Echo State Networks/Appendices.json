{
    "hands_on_practices": [
        {
            "introduction": "To grasp the complex dynamics of an Echo State Network, it is instructive to first examine a simplified linear model. This exercise strips away the nonlinearity to reveal the core principle governing the network's memory: the Echo State Property (ESP). By working through a simple two-dimensional system from first principles , you will connect the abstract concept of fading memory to the concrete mathematical condition that the spectral radius $\\rho(W)$ of the reservoir matrix must satisfy $\\rho(W) \\lt 1$.",
            "id": "4015639",
            "problem": "Consider a discrete-time reservoir in an Echo State Network (ESN) with linear activation function $\\phi(x)=x$. The reservoir state $\\mathbf{x}_{t} \\in \\mathbb{R}^{2}$ evolves according to\n$$\n\\mathbf{x}_{t+1}=\\phi\\!\\left(W \\mathbf{x}_{t}+W_{\\mathrm{in}} \\mathbf{u}_{t+1}\\right)=W \\mathbf{x}_{t}+W_{\\mathrm{in}} \\mathbf{u}_{t+1},\n$$\nwhere $W \\in \\mathbb{R}^{2 \\times 2}$ is the recurrent weight matrix, $W_{\\mathrm{in}} \\in \\mathbb{R}^{2 \\times m}$ is an input weight matrix, and $\\{\\mathbf{u}_{t}\\}_{t \\ge 0}$ is a bounded input sequence in $\\mathbb{R}^{m}$. Suppose\n$$\nW=\\begin{bmatrix}\n0.5  0\\\\\n0  0.8\n\\end{bmatrix}.\n$$\nStarting from standard linear algebra definitions of eigenvalues and the spectral radius, compute the spectral radius $\\rho(W)$. Then, using only the definition of the Echo State Property (ESP) for input-driven systems and basic properties of linear time-invariant dynamics, determine whether the ESP holds under the given linear activation, and justify your determination from first principles.\n\nReport only the computed value of $\\rho(W)$ as your final answer. No rounding is required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**1. Extraction of Givens**\n- System: A discrete-time reservoir in an Echo State Network (ESN).\n- Reservoir state: $\\mathbf{x}_{t} \\in \\mathbb{R}^{2}$.\n- Activation function: $\\phi(x)=x$ (linear).\n- State evolution equation: $\\mathbf{x}_{t+1}=\\phi\\!\\left(W \\mathbf{x}_{t}+W_{\\mathrm{in}} \\mathbf{u}_{t+1}\\right)=W \\mathbf{x}_{t}+W_{\\mathrm{in}} \\mathbf{u}_{t+1}$.\n- Recurrent weight matrix: $W = \\begin{bmatrix} 0.5  0 \\\\ 0  0.8 \\end{bmatrix}$, where $W \\in \\mathbb{R}^{2 \\times 2}$.\n- Input weight matrix: $W_{\\mathrm{in}} \\in \\mathbb{R}^{2 \\times m}$.\n- Input sequence: $\\{\\mathbf{u}_{t}\\}_{t \\ge 0}$ is a bounded sequence in $\\mathbb{R}^{m}$.\n- Task: Compute the spectral radius $\\rho(W)$ and determine if the Echo State Property (ESP) holds, justifying the determination from first principles.\n\n**2. Validation**\n- The problem is scientifically grounded in the theory of reservoir computing and linear dynamical systems. All definitions and equations are standard for this field.\n- The problem is well-posed. The matrix $W$ is explicitly given, allowing for a unique calculation of its eigenvalues and spectral radius. The conditions are sufficient to analyze the Echo State Property.\n- The problem is objective, stated with precise mathematical language and definitions.\n- The problem is complete and internally consistent.\n\n**Verdict:** The problem is valid.\n\n**Solution**\n\nThe first task is to compute the spectral radius, $\\rho(W)$, of the given matrix $W$.\n\nBy definition, the eigenvalues $\\lambda$ of a square matrix $W$ are the scalars that satisfy the characteristic equation $\\det(W - \\lambda I) = 0$, where $I$ is the identity matrix of the same dimension as $W$. The spectral radius $\\rho(W)$ is defined as the maximum of the absolute values of the eigenvalues of $W$. Mathematically, $\\rho(W) = \\max_i |\\lambda_i|$.\n\nThe given recurrent weight matrix is\n$$\nW=\\begin{bmatrix}\n0.5  0\\\\\n0  0.8\n\\end{bmatrix}\n$$\nThis is a $2 \\times 2$ diagonal matrix. For a diagonal (or triangular) matrix, the eigenvalues are simply the entries on the main diagonal.\nTherefore, the eigenvalues of $W$ are $\\lambda_1 = 0.5$ and $\\lambda_2 = 0.8$.\n\nNow, we compute the spectral radius by taking the maximum of the absolute values of these eigenvalues:\n$$\n\\rho(W) = \\max(|\\lambda_1|, |\\lambda_2|) = \\max(|0.5|, |0.8|) = \\max(0.5, 0.8) = 0.8\n$$\n\nThe second task is to determine if the Echo State Property (ESP) holds. The ESP dictates that for any bounded input sequence, the state of the reservoir must asymptotically become independent of the initial state $\\mathbf{x}_0$. This ensures that the network's state is driven solely by the recent history of the input, making it a true \"echo\" of the input signal.\n\nTo analyze this from first principles, let us consider two distinct initial states, $\\mathbf{x}_0^{(1)}$ and $\\mathbf{x}_0^{(2)}$, subjected to the same bounded input sequence $\\{\\mathbf{u}_t\\}_{t \\ge 0}$. The resulting state trajectories are given by:\n$$\n\\mathbf{x}_{t+1}^{(1)} = W \\mathbf{x}_{t}^{(1)} + W_{\\mathrm{in}} \\mathbf{u}_{t+1}\n$$\n$$\n\\mathbf{x}_{t+1}^{(2)} = W \\mathbf{x}_{t}^{(2)} + W_{\\mathrm{in}} \\mathbf{u}_{t+1}\n$$\nThe ESP holds if the difference between these two state trajectories converges to zero as time goes to infinity, i.e., $\\lim_{t \\to \\infty} (\\mathbf{x}_{t}^{(1)} - \\mathbf{x}_{t}^{(2)}) = \\mathbf{0}$.\n\nLet us define the difference vector as $\\Delta\\mathbf{x}_t = \\mathbf{x}_{t}^{(1)} - \\mathbf{x}_{t}^{(2)}$. Subtracting the second equation from the first, we obtain the dynamics of this difference:\n$$\n\\Delta\\mathbf{x}_{t+1} = \\mathbf{x}_{t+1}^{(1)} - \\mathbf{x}_{t+1}^{(2)} = (W \\mathbf{x}_{t}^{(1)} + W_{\\mathrm{in}} \\mathbf{u}_{t+1}) - (W \\mathbf{x}_{t}^{(2)} + W_{\\mathrm{in}} \\mathbf{u}_{t+1})\n$$\n$$\n\\Delta\\mathbf{x}_{t+1} = W \\mathbf{x}_{t}^{(1)} - W \\mathbf{x}_{t}^{(2)} = W (\\mathbf{x}_{t}^{(1)} - \\mathbf{x}_{t}^{(2)}) = W \\Delta\\mathbf{x}_{t}\n$$\nThis is a linear time-invariant discrete dynamical system for the difference vector $\\Delta\\mathbf{x}_t$. The solution to this recurrence relation can be found by repeatedly applying the relation:\n$$\n\\Delta\\mathbf{x}_{t} = W^t \\Delta\\mathbf{x}_{0}\n$$\nwhere $\\Delta\\mathbf{x}_{0} = \\mathbf{x}_{0}^{(1)} - \\mathbf{x}_{0}^{(2)}$ is the initial difference.\n\nThe ESP condition, $\\lim_{t \\to \\infty} \\Delta\\mathbf{x}_t = \\mathbf{0}$, is satisfied for any arbitrary initial difference $\\Delta\\mathbf{x}_0$ if and only if $\\lim_{t \\to \\infty} W^t = \\mathbf{0}$ (the zero matrix). A fundamental theorem of linear algebra states that for any square matrix $W$, the limit $\\lim_{t \\to \\infty} W^t$ converges to the zero matrix if and only if its spectral radius is strictly less than $1$, i.e., $\\rho(W)  1$.\n\nWe have already computed the spectral radius of the given matrix $W$ to be $\\rho(W) = 0.8$. Since $\\rho(W) = 0.8  1$, the condition is satisfied. Consequently, $\\lim_{t \\to \\infty} W^t = \\mathbf{0}$, which in turn guarantees that $\\lim_{t \\to \\infty} \\Delta\\mathbf{x}_t = \\mathbf{0}$ for any initial states.\n\nTherefore, the effect of the initial state $\\mathbf{x}_0$ decays to zero over time, and the system's state becomes a function only of the input history. The Echo State Property holds for this linear reservoir.\n\nThe problem asks to report only the computed value of $\\rho(W)$.",
            "answer": "$$\n\\boxed{0.8}\n$$"
        },
        {
            "introduction": "Now that the importance of the spectral radius for ensuring the Echo State Property is clear, a practical question arises: how do we control it? This practice  addresses this directly by having you derive the fundamental relationship between scaling the reservoir weight matrix $W$ by a scalar $\\gamma$ and its resulting spectral radius. This derivation demonstrates that a global scaling factor provides a simple yet powerful lever to tune the reservoir's dynamics, a crucial technique for network design and optimization.",
            "id": "4015597",
            "problem": "Consider an Echo State Network (ESN) defined by the discrete-time reservoir dynamics $x_{t+1}=\\phi\\!\\left(\\gamma W x_t + U u_t\\right)$, where $x_t \\in \\mathbb{R}^{N}$ is the reservoir state, $u_t \\in \\mathbb{R}^{M}$ is the input, $W \\in \\mathbb{R}^{N \\times N}$ is the reservoir weight matrix, $U \\in \\mathbb{R}^{N \\times M}$ is the input weight matrix, $\\gamma \\in \\mathbb{R}$ is a scalar global scaling of the reservoir, and $\\phi$ is a pointwise activation function with Lipschitz constant $L$ satisfying $L \\leq 1$. The Echo State Property (ESP) refers to the asymptotic fading of the influence of initial conditions on the reservoir state for bounded input sequences.\n\nLet the spectral radius of a square matrix be defined by $\\rho(W)=\\max\\{|\\lambda|:\\lambda \\in \\Lambda(W)\\}$, where $\\Lambda(W)$ denotes the multiset of eigenvalues of $W$ in $\\mathbb{C}$. Using only the core definitions of eigenvalues and determinants (including the characteristic polynomial), derive the spectral radius of the scaled reservoir matrix $\\gamma W$ explicitly in terms of $\\gamma$ and $\\rho(W)$. Provide your final result as a single closed-form analytic expression. No numerical approximation or rounding is required. Express the final answer without units.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the fields of linear algebra and reservoir computing, well-posed, objective, and contains sufficient information for a rigorous derivation. The problem asks for the derivation of the spectral radius of a scaled matrix, $\\rho(\\gamma W)$, in terms of the scalar $\\gamma$ and the spectral radius of the original matrix, $\\rho(W)$. The derivation will proceed from the fundamental definition of eigenvalues via the characteristic polynomial, as specified.\n\nLet $W$ be a square matrix of size $N \\times N$, i.e., $W \\in \\mathbb{R}^{N \\times N}$. Let $\\gamma \\in \\mathbb{R}$ be a scalar. We are tasked with finding the spectral radius of the matrix $\\gamma W$.\n\nThe eigenvalues of an $N \\times N$ matrix $A$ are the roots of its characteristic polynomial, given by the equation $\\det(A - \\lambda I) = 0$, where $I$ is the $N \\times N$ identity matrix and $\\lambda \\in \\mathbb{C}$ are the eigenvalues.\n\nLet $\\lambda_W$ be an eigenvalue of the matrix $W$. By definition, $\\lambda_W$ is a root of the characteristic polynomial of $W$:\n$$\n\\det(W - \\lambda_W I) = 0\n$$\n\nNow, let us find the eigenvalues of the scaled matrix $\\gamma W$. Let $\\lambda_{\\gamma W}$ denote an eigenvalue of $\\gamma W$. According to the definition, $\\lambda_{\\gamma W}$ must satisfy the characteristic equation for the matrix $\\gamma W$:\n$$\n\\det(\\gamma W - \\lambda_{\\gamma W} I) = 0\n$$\n\nWe can analyze this equation by considering two cases for the scalar $\\gamma$.\n\nCase 1: $\\gamma \\neq 0$.\nWe can factor $\\gamma$ out of the expression inside the determinant:\n$$\n\\det\\left(\\gamma \\left(W - \\frac{\\lambda_{\\gamma W}}{\\gamma} I\\right)\\right) = 0\n$$\nUsing the property of determinants that for any scalar $c$ and any $N \\times N$ matrix $A$, $\\det(cA) = c^N \\det(A)$, we have:\n$$\n\\gamma^N \\det\\left(W - \\frac{\\lambda_{\\gamma W}}{\\gamma} I\\right) = 0\n$$\nSince we have assumed $\\gamma \\neq 0$, it follows that $\\gamma^N \\neq 0$. Therefore, for the product to be zero, the determinant must be zero:\n$$\n\\det\\left(W - \\frac{\\lambda_{\\gamma W}}{\\gamma} I\\right) = 0\n$$\nThis is precisely the characteristic equation for the matrix $W$, with the placeholder for the eigenvalue being the term $\\frac{\\lambda_{\\gamma W}}{\\gamma}$. This implies that $\\frac{\\lambda_{\\gamma W}}{\\gamma}$ must be an eigenvalue of $W$. Let us denote this eigenvalue as $\\lambda_W$.\n$$\n\\frac{\\lambda_{\\gamma W}}{\\gamma} = \\lambda_W\n$$\nSolving for $\\lambda_{\\gamma W}$, we find:\n$$\n\\lambda_{\\gamma W} = \\gamma \\lambda_W\n$$\nThis demonstrates that for any eigenvalue $\\lambda_W$ of $W$, the value $\\gamma \\lambda_W$ is an eigenvalue of $\\gamma W$. Conversely, any eigenvalue of $\\gamma W$ must be of the form $\\gamma \\lambda_W$. Thus, the multiset of eigenvalues of $\\gamma W$, denoted $\\Lambda(\\gamma W)$, is related to the multiset of eigenvalues of $W$, denoted $\\Lambda(W)$, by the following relation:\n$$\n\\Lambda(\\gamma W) = \\{\\gamma \\lambda : \\lambda \\in \\Lambda(W)\\}\n$$\n\nCase 2: $\\gamma = 0$.\nIf $\\gamma = 0$, the scaled matrix is $\\gamma W = 0 \\cdot W = \\mathbf{0}$, where $\\mathbf{0}$ is the $N \\times N$ zero matrix. The characteristic equation for the zero matrix is $\\det(\\mathbf{0} - \\lambda I) = 0$, which simplifies to $\\det(-\\lambda I) = (-\\lambda)^N = 0$. The only root of this equation is $\\lambda = 0$. Therefore, the only eigenvalue of the zero matrix is $0$. The multiset of eigenvalues $\\Lambda(\\mathbf{0})$ contains $N$ zeros.\nThe spectral radius is $\\rho(\\mathbf{0}) = \\max\\{|0|\\} = 0$.\nOur derived formula from Case 1 gives $\\rho(\\gamma W) = |\\gamma| \\rho(W) = |0| \\rho(W) = 0 \\cdot \\rho(W) = 0$. The result is consistent for $\\gamma = 0$.\n\nNow we can proceed to derive the spectral radius. The spectral radius of a matrix $A$, denoted $\\rho(A)$, is defined as the maximum of the magnitudes (moduli) of its eigenvalues:\n$$\n\\rho(A) = \\max\\{|\\lambda| : \\lambda \\in \\Lambda(A)\\}\n$$\nApplying this definition to the matrix $\\gamma W$:\n$$\n\\rho(\\gamma W) = \\max\\{|\\lambda_{\\gamma W}| : \\lambda_{\\gamma W} \\in \\Lambda(\\gamma W)\\}\n$$\nUsing the relationship $\\lambda_{\\gamma W} = \\gamma \\lambda_W$ derived above, we can substitute into the expression for $\\rho(\\gamma W)$:\n$$\n\\rho(\\gamma W) = \\max\\{|\\gamma \\lambda_W| : \\lambda_W \\in \\Lambda(W)\\}\n$$\nUsing the property of the modulus that $|ab| = |a||b|$ for any $a, b \\in \\mathbb{C}$ (and noting that $\\gamma \\in \\mathbb{R}$ is a special case):\n$$\n\\rho(\\gamma W) = \\max\\{|\\gamma| |\\lambda_W| : \\lambda_W \\in \\Lambda(W)\\}\n$$\nSince $|\\gamma|$ is a non-negative real number, it is constant with respect to the maximization over the eigenvalues $\\lambda_W$. Therefore, we can factor it out of the maximization operator:\n$$\n\\rho(\\gamma W) = |\\gamma| \\max\\{|\\lambda_W| : \\lambda_W \\in \\Lambda(W)\\}\n$$\nThe term $\\max\\{|\\lambda_W| : \\lambda_W \\in \\Lambda(W)\\}$ is, by definition, the spectral radius of the matrix $W$, i.e., $\\rho(W)$.\nTherefore, we arrive at the final expression:\n$$\n\\rho(\\gamma W) = |\\gamma| \\rho(W)\n$$\nThis completes the derivation from the specified first principles.",
            "answer": "$$\n\\boxed{|\\gamma| \\rho(W)}\n$$"
        },
        {
            "introduction": "Real-world Echo State Networks leverage nonlinear activation functions, which introduces important subtleties not present in linear systems. This exercise  challenges you to distinguish between two critical system properties: the Echo State Property (ESP) and Bounded-Input Bounded-State (BIBS) stability. Analyzing a network with a $\\tanh$ activation reveals how a bounded nonlinearity can guarantee that the state remains stable, while the ESP—the network's capacity to forget initial conditions—still depends critically on the strength of the recurrent weights.",
            "id": "4015609",
            "problem": "Consider a discrete-time Echo State Network (ESN) with reservoir state in $\\mathbb{R}^n$ governed by the update\n$$\n\\mathbf{x}_{t+1} = \\tanh\\!\\big( a \\mathbf{W}\\,\\mathbf{x}_t + \\mathbf{U}\\,\\mathbf{u}_t \\big),\n$$\nwhere $\\tanh(\\cdot)$ is applied elementwise, $a \\in \\mathbb{R}$ is a scalar gain, $\\mathbf{W} \\in \\mathbb{R}^{n \\times n}$ is the recurrent weight matrix, $\\mathbf{U} \\in \\mathbb{R}^{n \\times m}$ is the input weight matrix, and $\\{\\mathbf{u}_t\\}_{t \\ge 0}$ is a bounded input sequence, meaning there exists $B \\ge 0$ such that $\\sup_{t \\ge 0} \\|\\mathbf{u}_t\\|_2 \\le B$. Two fundamental properties for driven dynamical systems are:\n- the Echo State Property (ESP), which informally requires that for any fixed input sequence, the reservoir state becomes asymptotically independent of the initial condition; and\n- the bounded-input bounded-state property (BIBS), which requires that for any bounded input sequence, the reservoir state remains bounded for all time.\n\nAssume the induced spectral norm $\\|\\cdot\\|_2$ is used on $\\mathbb{R}^n$. Evaluate the following statements about this ESN and select the single correct option.\n\nA. If $\\|a \\mathbf{W}\\|_2  1$, then for any bounded input sequence, the ESN satisfies both the Echo State Property (ESP) and the bounded-input bounded-state property (BIBS). If $\\|a \\mathbf{W}\\|_2 \\ge 1$, then for some bounded input sequences, BIBS still holds but ESP can fail.\n\nB. For any bounded input sequence, the ESN satisfies both ESP and BIBS regardless of the value of $\\|a \\mathbf{W}\\|_2$, because the hyperbolic tangent nonlinearity output is bounded.\n\nC. The ESN satisfies ESP whenever the spectral radius $\\rho(a \\mathbf{W}) \\le 1$ (even if $\\|a \\mathbf{W}\\|_2  1$), and BIBS fails whenever the input sequence is nonzero.\n\nD. If $\\|a \\mathbf{W}\\|_2  1$, then BIBS fails unless the input sequence is identically zero, because contraction drives the state toward zero only in the absence of input.\n\nChoose the option that correctly distinguishes the Echo State Property (ESP) from bounded-input bounded-state (BIBS) in this setting and properly characterizes when each holds.",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n\n-   **System**: A discrete-time Echo State Network (ESN).\n-   **Reservoir State**: $\\mathbf{x}_t \\in \\mathbb{R}^n$.\n-   **State Update Equation**: $\\mathbf{x}_{t+1} = \\tanh\\!\\big( a \\mathbf{W}\\,\\mathbf{x}_t + \\mathbf{U}\\,\\mathbf{u}_t \\big)$.\n-   **Activation Function**: The hyperbolic tangent $\\tanh(\\cdot)$ is applied elementwise.\n-   **Parameters**: $a \\in \\mathbb{R}$ (scalar gain), $\\mathbf{W} \\in \\mathbb{R}^{n \\times n}$ (recurrent weight matrix), $\\mathbf{U} \\in \\mathbb{R}^{n \\times m}$ (input weight matrix).\n-   **Input**: A bounded sequence $\\{\\mathbf{u}_t\\}_{t \\ge 0}$, meaning there exists a constant $B \\ge 0$ such that $\\sup_{t \\ge 0} \\|\\mathbf{u}_t\\|_2 \\le B$.\n-   **Norm**: The induced spectral norm, $\\|\\cdot\\|_2$, is used on $\\mathbb{R}^n$.\n-   **Definitions**:\n    -   **Echo State Property (ESP)**: Informally, for any fixed input sequence, the reservoir state becomes asymptotically independent of the initial condition.\n    -   **Bounded-Input Bounded-State Property (BIBS)**: For any bounded input sequence, the reservoir state remains bounded for all time.\n-   **Objective**: Evaluate the provided statements and select the single correct option.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is based on the standard mathematical model of an Echo State Network, a well-established concept in the field of reservoir computing and computational neuroscience. The notions of the Echo State Property (ESP) and Bounded-Input Bounded-State (BIBS) stability are central to the theory of such networks. The model and its properties are scientifically and mathematically sound.\n-   **Well-Posed**: The problem is clearly defined. It provides a specific dynamical system and asks for an evaluation of its properties under certain conditions. The properties themselves (ESP and BIBS) have standard formal definitions that allow for a unique and meaningful analysis.\n-   **Objective**: The problem statement and the properties to be analyzed are expressed in precise mathematical language, free of subjective or ambiguous terminology.\n-   **Completeness**: The problem provides all necessary information to analyze the dynamics of the ESN. The \"informal\" definition of ESP is the standard one in the literature, which directly corresponds to a formal mathematical condition ($\\|\\mathbf{x}_t - \\mathbf{x}'_t\\| \\to 0$ for two trajectories with different initial states).\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and complete. I will proceed with the derivation and evaluation of the options.\n\n## Solution Derivation\n\nThe analysis will address the two properties, BIBS and ESP, separately.\n\n### Bounded-Input Bounded-State (BIBS) Property\n\nThe state update equation is given by $\\mathbf{x}_{t+1} = \\tanh\\!\\big( a \\mathbf{W}\\,\\mathbf{x}_t + \\mathbf{U}\\,\\mathbf{u}_t \\big)$. The hyperbolic tangent function, $\\tanh(z)$, has a co-domain of $(-1, 1)$ for any real input $z$. This function is applied elementwise to the vector argument.\n\nLet $\\mathbf{v}_t = a \\mathbf{W}\\,\\mathbf{x}_t + \\mathbf{U}\\,\\mathbf{u}_t$. The $i$-th component of the next state is $x_{t+1,i} = \\tanh(v_{t,i})$.\nBy the definition of the $\\tanh$ function, we have:\n$$ -1  x_{t+1,i}  1 \\quad \\forall i \\in \\{1, \\dots, n\\} $$\nThis implies that $x_{t+1,i}^2  1$.\n\nWe can now compute the squared $L_2$ norm (spectral norm for vectors) of the state vector $\\mathbf{x}_{t+1}$:\n$$ \\|\\mathbf{x}_{t+1}\\|_2^2 = \\sum_{i=1}^{n} x_{t+1,i}^2 $$\nSubstituting the inequality for each component:\n$$ \\|\\mathbf{x}_{t+1}\\|_2^2  \\sum_{i=1}^{n} 1^2 = n $$\nTaking the square root, we find that the norm of the state is bounded:\n$$ \\|\\mathbf{x}_{t+1}\\|_2  \\sqrt{n} $$\nThis bound holds for all $t$ for which the state is computed via the update rule (i.e., for $t \\ge 0$, leading to a bound on $\\mathbf{x}_{t}$ for $t \\ge 1$). Since the state vector $\\mathbf{x}_t$ is always confined to a hypersphere of radius $\\sqrt{n}$, the state is bounded. The input $\\{\\mathbf{u}_t\\}$ is given as bounded. Therefore, for any bounded input, the state is bounded.\n\nThis conclusion holds irrespective of the values of $a$, $\\mathbf{W}$, or $\\mathbf{U}$. The BIBS property is guaranteed by the nature of the $\\tanh$ activation function, which \"squashes\" the state into a bounded region.\n\n### Echo State Property (ESP)\n\nThe ESP requires that for any two trajectories, $\\mathbf{x}_t$ and $\\mathbf{x}'_t$, starting from different initial conditions, $\\mathbf{x}_0$ and $\\mathbf{x}'_0$, but driven by the same input sequence $\\{\\mathbf{u}_t\\}$, the difference between their states must vanish as time goes to infinity.\nFormally, we must show that $\\lim_{t \\to \\infty} \\|\\mathbf{x}_t - \\mathbf{x}'_t\\|_2 = 0$.\n\nLet $\\Delta\\mathbf{x}_t = \\mathbf{x}_t - \\mathbf{x}'_t$. The difference at step $t+1$ is:\n$$ \\Delta\\mathbf{x}_{t+1} = \\mathbf{x}_{t+1} - \\mathbf{x}'_{t+1} = \\tanh(a\\mathbf{W}\\mathbf{x}_t + \\mathbf{U}\\mathbf{u}_t) - \\tanh(a\\mathbf{W}\\mathbf{x}'_t + \\mathbf{U}\\mathbf{u}_t) $$\nThe vector-valued Mean Value Theorem is not as straightforward as the scalar version, but we can leverage the property of the $\\tanh$ function. For any two vectors $\\mathbf{p}, \\mathbf{q} \\in \\mathbb{R}^n$, it can be shown that $\\|\\tanh(\\mathbf{p}) - \\tanh(\\mathbf{q})\\|_2 \\le \\|\\mathbf{p} - \\mathbf{q}\\|_2$. This is because the derivative of $\\tanh(z)$ is $\\operatorname{sech}^2(z)$, which has a maximum value of $1$. This property makes the elementwise $\\tanh$ function a non-expansion.\n\nApplying this property, we get:\n$$ \\|\\Delta\\mathbf{x}_{t+1}\\|_2 \\le \\| (a\\mathbf{W}\\mathbf{x}_t + \\mathbf{U}\\mathbf{u}_t) - (a\\mathbf{W}\\mathbf{x}'_t + \\mathbf{U}\\mathbf{u}_t) \\|_2 $$\n$$ \\|\\Delta\\mathbf{x}_{t+1}\\|_2 \\le \\| a\\mathbf{W}(\\mathbf{x}_t - \\mathbf{x}'_t) \\|_2 = \\| a\\mathbf{W}\\Delta\\mathbf{x}_t \\|_2 $$\nUsing the definition of the induced matrix norm (in this case, the spectral norm):\n$$ \\| a\\mathbf{W}\\Delta\\mathbf{x}_t \\|_2 \\le \\|a\\mathbf{W}\\|_2 \\|\\Delta\\mathbf{x}_t\\|_2 $$\nCombining these inequalities, we obtain a recursive relation for the norm of the state difference:\n$$ \\|\\Delta\\mathbf{x}_{t+1}\\|_2 \\le \\|a\\mathbf{W}\\|_2 \\|\\Delta\\mathbf{x}_t\\|_2 $$\nBy iterating this inequality back to $t=0$:\n$$ \\|\\Delta\\mathbf{x}_{t+1}\\|_2 \\le (\\|a\\mathbf{W}\\|_2)^{t+1} \\|\\Delta\\mathbf{x}_0\\|_2 $$\nFor the limit $\\lim_{t \\to \\infty} \\|\\Delta\\mathbf{x}_t\\|_2 = 0$ to be guaranteed for any initial difference $\\|\\Delta\\mathbf{x}_0\\|_2$, we need the term $(\\|a\\mathbf{W}\\|_2)^{t+1}$ to converge to zero. This occurs if and only if:\n$$ \\|a\\mathbf{W}\\|_2  1 $$\nThis is a sufficient condition for the ESP to hold. If this condition is not met (i.e., $\\|a\\mathbf{W}\\|_2 \\ge 1$), the state map is not guaranteed to be a contraction, and the ESP *can* fail. For instance, if the recurrent dynamics are strong enough (e.g., if the spectral radius $\\rho(a\\mathbf{W})  1$), the system could have multiple attractors even for a fixed input (such as $\\mathbf{u}_t = \\mathbf{0}$), causing the final state to depend on the initial condition.\n\n## Option-by-Option Analysis\n\n**A. If $\\|a \\mathbf{W}\\|_2  1$, then for any bounded input sequence, the ESN satisfies both the Echo State Property (ESP) and the bounded-input bounded-state property (BIBS). If $\\|a \\mathbf{W}\\|_2 \\ge 1$, then for some bounded input sequences, BIBS still holds but ESP can fail.**\n\n-   **Part 1**: \"If $\\|a \\mathbf{W}\\|_2  1$, then... satisfies both ESP and BIBS.\"\n    -   As shown above, BIBS holds for any value of $\\|a \\mathbf{W}\\|_2$.\n    -   As shown above, $\\|a \\mathbf{W}\\|_2  1$ is a sufficient condition for ESP.\n    -   This part of the statement is correct.\n-   **Part 2**: \"If $\\|a \\mathbf{W}\\|_2 \\ge 1$, then... BIBS still holds but ESP can fail.\"\n    -   \"BIBS still holds\": This is correct. BIBS is independent of $\\|a\\mathbf{W}\\|_2$.\n    -   \"ESP can fail\": This is also correct. When $\\|a \\mathbf{W}\\|_2 \\ge 1$, the contraction mapping argument is not valid, and it is known that ESP can be lost, for example, if the system develops multiple stable attractors.\n    -   This part of the statement is also correct.\n-   **Verdict**: **Correct**. This option accurately describes the conditions for both properties.\n\n**B. For any bounded input sequence, the ESN satisfies both ESP and BIBS regardless of the value of $\\|a \\mathbf{W}\\|_2$, because the hyperbolic tangent nonlinearity output is bounded.**\n\n-   This statement claims that both ESP and BIBS always hold.\n-   While it is true that BIBS always holds due to the bounded nonlinearity, it is false that ESP always holds. ESP depends critically on the magnitude of the recurrent weights, encapsulated by $\\|a\\mathbf{W}\\|_2$ (or more generally, $\\rho(a\\mathbf{W})$). If the recurrent connections are too strong, the network will have its own complex internal dynamics that are not washed out by the input, violating ESP.\n-   **Verdict**: **Incorrect**.\n\n**C. The ESN satisfies ESP whenever the spectral radius $\\rho(a \\mathbf{W}) \\le 1$ (even if $\\|a \\mathbf{W}\\|_2  1$), and BIBS fails whenever the input sequence is nonzero.**\n\n-   **Part 1**: \"The ESN satisfies ESP whenever the spectral radius $\\rho(a \\mathbf{W}) \\le 1$\". The typical sufficient condition is strict inequality, $\\rho(a \\mathbf{W})  1$. The boundary case $\\rho(a \\mathbf{W}) = 1$ is problematic and does not guarantee convergence of state differences to zero.\n-   **Part 2**: \"...and BIBS fails whenever the input sequence is nonzero.\" This is demonstrably false. As proven earlier, the BIBS property holds for this system for any input sequence, due to the bounded nature of the $\\tanh$ function.\n-   **Verdict**: **Incorrect**. The claim about BIBS is fundamentally wrong.\n\n**D. If $\\|a \\mathbf{W}\\|_2  1$, then BIBS fails unless the input sequence is identically zero, because contraction drives the state toward zero only in the absence of input.**\n\n-   This statement claims that BIBS fails if the input is non-zero. This is incorrect. BIBS holding means the state remains bounded, which it does ($ \\|\\mathbf{x}_t\\|_2  \\sqrt{n}$ for $t \\ge 1$). The statement confuses \"boundedness\" with \"convergence to zero\".\n-   The reasoning given is also flawed in its implication. While it's true that for $\\mathbf{u}_t=\\mathbf{0}$, the state converges to $\\mathbf{x}=\\mathbf{0}$, this does not mean the state becomes unbounded when $\\mathbf{u}_t \\neq \\mathbf{0}$. The state will simply follow a bounded trajectory dictated by the input.\n-   **Verdict**: **Incorrect**. It fundamentally misunderstands the definition of BIBS.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}