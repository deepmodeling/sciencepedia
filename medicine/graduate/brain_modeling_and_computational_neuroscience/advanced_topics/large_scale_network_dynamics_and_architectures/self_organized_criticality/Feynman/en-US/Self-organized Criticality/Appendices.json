{
    "hands_on_practices": [
        {
            "introduction": "The Abelian Sandpile Model (ASM) is a paradigmatic example of a system exhibiting self-organized criticality. To truly grasp how complex, system-spanning avalanches can emerge from simple, local rules, there is no substitute for simulating the process yourself. This exercise  provides a foundational, hands-on experience by having you manually trace an avalanche on a small grid, building a concrete intuition for the propagation of activity that underpins more complex theories.",
            "id": "4142578",
            "problem": "Consider the Abelian sandpile model (ASM) on a two-dimensional square lattice with open boundaries. Let the lattice be the $3 \\times 3$ grid with sites indexed by coordinates $(i,j) \\in \\{-1,0,1\\} \\times \\{-1,0,1\\}$, and let the origin be $(0,0)$. Each site $(i,j)$ carries an integer height $h(i,j) \\in \\mathbb{Z}_{\\ge 0}$. A site is unstable if its height is at least $4$. A toppling at site $(i,j)$ reduces $h(i,j)$ by $4$ and increases the height of each of its nearest neighbors (up to four of them in the interior, fewer on the boundary) by $1$. With open boundary conditions, any grains that would be sent to a neighbor outside the $3 \\times 3$ grid are dissipated and leave the system permanently.\n\nStart from the maximally stable initial configuration $h(i,j) = 3$ at all nine sites. Add a single grain at the origin $(0,0)$ and stabilize the configuration by iteratively toppling unstable sites until all sites are stable. Demonstrate the stabilization by explicitly enumerating the sequence of topplings you apply and reporting the final stabilized heights at all sites.\n\nWhat is the total number of topplings that occur over all sites during this stabilization process? Provide your answer as a single integer. No rounding is required and no units are involved.",
            "solution": "The problem statement has been validated and is deemed a well-posed, scientifically grounded problem within the domain of complex adaptive systems modeling, specifically the Abelian sandpile model (ASM).\n\nThe system is a $3 \\times 3$ grid of sites, indexed by coordinates $(i,j)$ where $i, j \\in \\{-1, 0, 1\\}$. Each site $(i,j)$ has an integer height $h(i,j)$. A site is unstable if its height $h(i,j) \\ge 4$. A toppling at site $(i,j)$ results in the height changes:\n$h(i,j) \\to h(i,j) - 4$\n$h(\\text{neighbors}) \\to h(\\text{neighbors}) + 1$\nThe system has open boundaries, so any grains that would go to a site outside the grid are lost.\n\nThe initial state is the maximally stable configuration, where all sites have a height of $3$. We can represent the height configuration as a matrix $H$, where the matrix indices $(r,c)$ with $r,c \\in \\{1,2,3\\}$ correspond to the grid coordinates $(j-2, 2-r)$.\nThe initial configuration $H_{init}$ is:\n$$ H_{init} = \\begin{pmatrix} 3  3  3 \\\\ 3  3  3 \\\\ 3  3  3 \\end{pmatrix} $$\n\nThe process begins by adding a single grain to the origin, site $(0,0)$. This corresponds to the center of the grid. The height at $(0,0)$ becomes $3+1=4$. The configuration is now:\n$$ H_0 = \\begin{pmatrix} 3  3  3 \\\\ 3  4  3 \\\\ 3  3  3 \\end{pmatrix} $$\nThe site $(0,0)$ is now unstable. The stabilization process proceeds as a sequence of topplings. A key property of the ASM is that the final configuration and the total number of topplings are independent of the order in which unstable sites are toppled.\n\n**Step 1: First Toppling**\nThe only unstable site is $(0,0)$ with $h(0,0)=4$. We topple this site.\n- The height at $(0,0)$ decreases by $4$: $h(0,0) \\to 4-4=0$.\n- The heights of its four nearest neighbors, $(0,-1)$, $(0,1)$, $(-1,0)$, and $(1,0)$, each increase by $1$. Their heights become $3+1=4$.\nThe resulting configuration, $H_1$, is:\n$$ H_1 = \\begin{pmatrix} 3  4  3 \\\\ 4  0  4 \\\\ 3  4  3 \\end{pmatrix} $$\nThe number of topplings thus far is $1$.\n\n**Step 2: Second Round of Topplings**\nThe configuration $H_1$ has four unstable sites: the four edge sites $(0,-1)$, $(0,1)$, $(-1,0)$, and $(1,0)$, all with height $4$. We topple these four sites. The order does not matter. Let's analyze the collective effect of these four topplings.\n- Each of the four sites topples once. Their heights decrease by $4$: $h \\to 4-4=0$.\n- Site $(0,0)$: It is a neighbor to all four toppling sites. Its height increases by $4$: $h(0,0) \\to 0+4=4$.\n- The corner sites, $(\\pm 1, \\pm 1)$: Each corner site is a neighbor to two of the toppling edge sites. For instance, site $(-1,-1)$ is a neighbor to $(-1,0)$ and $(0,-1)$. Therefore, the height of each corner site increases by $2$: $h \\to 3+2=5$.\nThe resulting configuration, $H_2$, is:\n$$ H_2 = \\begin{pmatrix} 5  0  5 \\\\ 0  4  0 \\\\ 5  0  5 \\end{pmatrix} $$\nThe number of topplings in this round is $4$. The total number of topplings is now $1+4=5$.\n\n**Step 3: Final Round of Topplings**\nThe configuration $H_2$ has five unstable sites: the four corners with height $5$ and the center with height $4$. We proceed to topple these sites.\n1.  First, let's topple the center site $(0,0)$, which has $h(0,0)=4$.\n    - Its height becomes $h(0,0) \\to 4-4=0$.\n    - Its four neighbors, the edge sites, currently have height $0$. Their heights increase by $1$: $h \\to 0+1=1$.\n    This toppling is the $6^{th}$ total toppling. The configuration becomes:\n    $$ H_{2a} = \\begin{pmatrix} 5  1  5 \\\\ 1  0  1 \\\\ 5  1  5 \\end{pmatrix} $$\n2.  Now, the four corner sites $(\\pm 1, \\pm 1)$ are unstable with height $5$. We topple these four sites.\n    - Their heights decrease by $4$: $h \\to 5-4=1$.\n    - Each corner site has two neighbors on the grid. For example, toppling $(-1,-1)$ adds one grain to $(-1,0)$ and one to $(0,-1)$.\n    - Let's look at the effect on an edge site, say $(-1,0)$. Its current height is $1$. It receives one grain from the toppling of $(-1,-1)$ and one from $(-1,1)$. Its height therefore increases by 2, becoming 3.\n    - By symmetry, all four edge sites will have their heights change from $1$ to $3$.\nThis round involves $4$ more topplings, one for each corner. The final stabilized configuration, $H_f$, is:\n$$ H_f = \\begin{pmatrix} 1  3  1 \\\\ 3  0  3 \\\\ 1  3  1 \\end{pmatrix} $$\nAll heights are now less than $4$, so the configuration is stable.\n\n**Summary of Topplings**\nTo find the total number of topplings, we sum the topplings from each stage:\n- Step 1: $1$ toppling at site $(0,0)$.\n- Step 2: $4$ topplings, one at each of the four edge sites.\n- Step 3: $1$ toppling at site $(0,0)$ and $4$ topplings, one at each of the four corner sites. This is a total of $5$ topplings.\n\nTotal number of topplings = $1 + 4 + 5 = 10$.\n\nThe final stabilized heights at all sites $(i,j)$ are:\n- $h(0,0) = 0$\n- $h(\\pm 1, 0) = h(0, \\pm 1) = 3$\n- $h(\\pm 1, \\pm 1) = 1$\n\nThe total number of topplings that occurred during the stabilization process is $10$.",
            "answer": "$$\\boxed{10}$$"
        },
        {
            "introduction": "The principles of self-organized criticality extend far beyond abstract models, offering a powerful framework for understanding neural dynamics. In real neural networks, mechanisms like refractory periods and synaptic transmission failures introduce forms of dissipation that are not present in idealized models. This problem  challenges you to apply your conceptual understanding of criticality to a neurobiological context, predicting how these biophysical details alter the statistics of \"neural avalanches\" and shift the system away from a perfectly critical state.",
            "id": "4017619",
            "problem": "Consider a large, sparse, directed network of neurons operating under slow external drive and fast relaxation, a regime typical of Self-Organized Criticality (SOC). An avalanche is defined as a contiguous cascade of spikes triggered by a single perturbation, with no external input during the cascade due to separation of timescales. In an idealized conservative case at criticality, activity is locally conserved and the effective branching ratio $\\sigma_{\\mathrm{eff}} \\approx 1$, yielding scale-free avalanche statistics. Empirically, avalanche size $s$ and avalanche duration $T$ in such a critical regime are observed to follow power-law distributions of the form $P(s)\\sim s^{-\\tau_s}$ and $P(T)\\sim T^{-\\tau_T}$ over broad ranges, consistent with a critical branching process.\n\nNow consider two biophysically grounded sources of dissipation introduced into the network dynamics:\n\n(i) A refractory period of length $r$ discrete time steps, during which any neuron that spikes becomes temporarily inexcitable and cannot spike again. This dynamically removes potential propagators from the cascade and constrains local activity flow.\n\n(ii) A spike failure mechanism with failure probability $p_f$, such that each attempted synaptic transmission fails independently with probability $p_f$, reducing the probability of successful postsynaptic activation.\n\nAssume that the out-degree distribution is bounded, synaptic strengths do not grow during an avalanche, and there is no compensatory mechanism during the avalanche that adds energy or activity to the cascade; that is, activity is not conserved once refractory and failures are present. Use well-tested principles about branching processes and SOC: criticality corresponds to $\\sigma_{\\mathrm{eff}}=1$ with conservation-like propagation, while $\\sigma_{\\mathrm{eff}}1$ (subcritical) yields finite mean avalanche sizes and durations and exponentially decaying tails; in near-critical regimes, scaling with cutoffs is observed via $P(s)\\sim s^{-\\tau_s} f(s/s_c)$ and $P(T)\\sim T^{-\\tau_T} g(T/T_c)$ where $f$ and $g$ encode cutoffs at scales $s_c$ and $T_c$ set by distance from criticality and system size.\n\nQualitatively compare the avalanche size and duration distributions in the presence of refractory periods and spike failures to the ideal conservative critical case. Which option best captures the impact of these forms of dissipation on $P(s)$ and $P(T)$?\n\nA. Both $P(s)$ and $P(T)$ remain pure power laws with unchanged exponents and broad ranges; dissipation only rescales prefactors without altering tails.\n\nB. Dissipation via refractory periods and spike failures lowers $\\sigma_{\\mathrm{eff}}$ below $1$, making dynamics subcritical; both $P(s)$ and $P(T)$ become truncated relative to the conservative case, suppressing large avalanches and long durations, with cutoffs $s_c$ and $T_c$ shifting to smaller values as $p_f$ and $r$ increase.\n\nC. Refractory periods and spike failures increase $\\sigma_{\\mathrm{eff}}$ above $1$, promoting supercritical cascades that flatten both distributions and enhance the frequency of very large and long avalanches.\n\nD. Refractory periods temporally space spikes and therefore increase avalanche durations even in the presence of spike failures; $P(T)$ becomes heavier-tailed while $P(s)$ is reduced, producing longer but smaller avalanches overall.\n\nE. Dissipation leaves the critical exponents $\\tau_s$ and $\\tau_T$ exactly unchanged but induces stretched-exponential tails that increase the relative probability of very large avalanches compared to the conservative critical case.\n\nAnswer by selecting the single option that is most consistent with the stated principles and mechanisms.",
            "solution": "The user requires a critical validation of the problem statement, followed by a detailed solution and evaluation of the provided options.\n\n### Step 1: Extract Givens\n\n- **System Context:** A large, sparse, directed network of neurons operating in a Self-Organized Criticality (SOC) regime, characterized by slow external drive and fast relaxation.\n- **Avalanche Definition:** A contiguous spike cascade from a single perturbation without intermediate external input.\n- **Ideal Critical Case:** At criticality, the effective branching ratio is $\\sigma_{\\mathrm{eff}} \\approx 1$. Activity is locally conserved. Avalanche size ($s$) and duration ($T$) distributions follow pure power laws: $P(s) \\sim s^{-\\tau_s}$ and $P(T) \\sim T^{-\\tau_T}$. This is modeled as a critical branching process.\n- **Introduced Dissipative Mechanisms:**\n    1.  **Refractory period:** A neuron that spikes becomes inexcitable for $r$ discrete time steps.\n    2.  **Spike failure:** Each synaptic transmission fails with an independent probability $p_f$.\n- **Constraints and Principles:**\n    - The system is dissipative; activity is not conserved with the new mechanisms.\n    - Out-degree is bounded, and synaptic strengths do not increase during an avalanche.\n    - Criticality implies $\\sigma_{\\mathrm{eff}} = 1$.\n    - Subcriticality implies $\\sigma_{\\mathrm{eff}}  1$, leading to finite mean avalanche sizes/durations and exponentially decaying tails.\n    - Near-critical systems exhibit distributions with cutoffs, expressed as $P(s) \\sim s^{-\\tau_s} f(s/s_c)$ and $P(T) \\sim T^{-\\tau_T} g(T/T_c)$, where $s_c$ and $T_c$ are cutoff scales.\n- **Question:** Determine the qualitative effect of the refractory period and spike failure on the avalanche size and duration distributions, compared to the ideal critical case.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is subjected to rigorous validation.\n\n- **Scientific Grounding:** The problem is firmly grounded in the established theoretical framework of self-organized criticality, specifically its application to neural dynamics and brain modeling. The concepts of branching processes, the branching ratio $\\sigma_{\\mathrm{eff}}$, power-law distributions for neural avalanches, and the distinction between critical ($\\sigma_{\\mathrm{eff}} = 1$), subcritical ($\\sigma_{\\mathrm{eff}}  1$), and supercritical ($\\sigma_{\\mathrm{eff}}  1$) regimes are standard in computational neuroscience and statistical physics. The specified dissipative mechanisms—refractory periods and synaptic unreliability—are fundamental biophysical properties of real neural systems. The description is consonant with seminal and ongoing research in the field. The problem is scientifically sound.\n- **Well-Posedness:** The question asks for a qualitative comparison based on a set of well-defined principles and mechanisms. The initial state is defined (criticality, $\\sigma_{\\mathrm{eff}} \\approx 1$), the perturbations are specified (dissipative mechanisms), and the theoretical consequences of changing $\\sigma_{\\mathrm{eff}}$ are provided. This structure allows for a unique, logical deduction about the resulting dynamics and statistics. The problem is well-posed.\n- **Objectivity:** The problem is phrased using precise, objective, and technical language (e.g., \"effective branching ratio,\" \"power-law distributions,\" \"dissipation\"). It is free of subjective claims or ambiguity.\n- **Invalidity Checklist:**\n    1.  **Scientific/Factual Unsoundness:** None. The premises are consistent with established scientific theory.\n    2.  **Non-Formalizable/Irrelevant:** None. The problem is a standard conceptual model in its field.\n    3.  **Incomplete/Contradictory Setup:** None. The problem provides the necessary principles to connect the introduced mechanisms to their statistical consequences.\n    4.  **Unrealistic/Infeasible:** None. The mechanisms are biophysically motivated abstractions used in standard theoretical models.\n    5.  **Ill-Posed/Poorly Structured:** None. A unique qualitative answer can be derived.\n    6.  **Pseudo-Profound/Trivial:** None. The problem requires a correct application of the core principles of criticality to a non-trivial scenario.\n    7.  **Outside Scientific Verifiability:** None. The predictions are testable via computational simulation.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. Proceeding to solution derivation and option analysis.\n\n### Solution Derivation\n\nThe central concept governing the avalanche statistics is the **effective branching ratio**, $\\sigma_{\\mathrm{eff}}$, which represents the average number of subsequent spikes caused by a single spike within the cascade. The problem establishes that the initial, idealized state is critical, meaning $\\sigma_{\\mathrm{eff}} \\approx 1$. In this state, activity on average neither grows nor decays, allowing for cascades of all sizes and durations, resulting in scale-free, power-law distributions.\n\nWe must now analyze the impact of the two introduced dissipative mechanisms on $\\sigma_{\\mathrm{eff}}$.\n\n1.  **Refractory Period ($r$):** When a neuron fires, it becomes inexcitable for a duration $r$. If another spike arrives at this neuron during its refractory period, that transmission event fails to produce a new spike. This dynamically removes potential nodes from the network that could otherwise propagate activity. This process constitutes a loss of activity within the cascade. Consequently, the average number of new spikes generated per existing spike will decrease. This mechanism inherently reduces $\\sigma_{\\mathrm{eff}}$.\n\n2.  **Spike Failure ($p_f$):** Each synaptic transmission has a probability $p_f  0$ of failing. This means that even if a signal is sent to a perfectly excitable neuron, it may not be received, thus preventing a potential spike. This mechanism directly reduces the probability of successful propagation along every branch of the cascade. If a neuron projects to $k$ other neurons, the expected number of successful transmissions is scaled down by a factor of $(1 - p_f)$. This is a direct dissipative effect that reduces $\\sigma_{\\mathrm{eff}}$.\n\nBoth mechanisms, being sources of dissipation, act to reduce the propagation of activity. When introduced into a system poised at criticality ($\\sigma_{\\mathrm{eff}} \\approx 1$), their combined effect is to push the branching ratio to a value strictly less than $1$. Therefore, the new effective branching ratio, $\\sigma_{\\mathrm{eff-new}}$, will be less than $1$.\n\nAccording to the principles provided in the problem statement, a system where $\\sigma_{\\mathrm{eff}}  1$ is defined as **subcritical**. A key feature of subcritical dynamics is that activity cascades tend to die out after a characteristic number of steps. This prevents the formation of system-spanning avalanches. Statistically, this is reflected in the distributions of avalanche size ($s$) and duration ($T$). Instead of being pure power laws, the distributions follow a power law for small values but are then \"cut off\" or \"truncated\" for large values. This is mathematically expressed by the forms $P(s) \\sim s^{-\\tau_s} f(s/s_c)$ and $P(T) \\sim T^{-\\tau_T} g(T/T_c)$, where $f$ and $g$ are rapidly decaying cutoff functions (e.g., exponential, $f(x) = e^{-x}$).\n\nThe cutoff scales, $s_c$ and $T_c$, determine the point at which the distributions deviate from the power law. These scales are inversely related to the distance from criticality, i.e., $|1 - \\sigma_{\\mathrm{eff}}|$. As the dissipative parameters $p_f$ and $r$ increase, the system becomes more dissipative, $\\sigma_{\\mathrm{eff-new}}$ decreases further, and the system moves farther away from the critical point. This causes the cutoff scales $s_c$ and $T_c$ to shift to smaller values, meaning that large and long avalanches become even more suppressed.\n\nIn summary, dissipation drives the system from critical to subcritical, which truncates the power-law distributions of avalanche size and duration, suppressing large events.\n\n### Option-by-Option Analysis\n\n**A. Both $P(s)$ and $P(T)$ remain pure power laws with unchanged exponents and broad ranges; dissipation only rescales prefactors without altering tails.**\n- **Analysis:** This is incorrect. A change in the branching ratio away from $\\sigma_{\\mathrm{eff}}=1$ fundamentally alters the nature of the distributions from scale-free (pure power-law) to having a characteristic scale (a cutoff). The tails of the distributions are significantly affected, specifically by being suppressed.\n- **Verdict:** Incorrect.\n\n**B. Dissipation via refractory periods and spike failures lowers $\\sigma_{\\mathrm{eff}}$ below $1$, making dynamics subcritical; both $P(s)$ and $P(T)$ become truncated relative to the conservative case, suppressing large avalanches and long durations, with cutoffs $s_c$ and $T_c$ shifting to smaller values as $p_f$ and $r$ increase.**\n- **Analysis:** This option accurately summarizes the derived consequences. It correctly identifies that dissipation lowers $\\sigma_{\\mathrm{eff}}$, which leads to subcritical dynamics. It correctly describes the effect on the distributions as truncation, which suppresses large and long avalanches. Finally, it correctly relates the strength of dissipation (increasing $p_f$ and $r$) to a further reduction in the cutoff scales ($s_c$ and $T_c$).\n- **Verdict:** Correct.\n\n**C. Refractory periods and spike failures increase $\\sigma_{\\mathrm{eff}}$ above $1$, promoting supercritical cascades that flatten both distributions and enhance the frequency of very large and long avalanches.**\n- **Analysis:** This is incorrect. The described mechanisms are dissipative by nature; they remove or block activity propagation. They cannot increase the branching ratio to a supercritical state ($\\sigma_{\\mathrm{eff}}  1$). This contradicts the physical role of refractoriness and transmission failure.\n- **Verdict:** Incorrect.\n\n**D. Refractory periods temporally space spikes and therefore increase avalanche durations even in the presence of spike failures; $P(T)$ becomes heavier-tailed while $P(s)$ is reduced, producing longer but smaller avalanches overall.**\n- **Analysis:** This presents a flawed intuition. While a refractory period affects the timing of individual neuron firing, its primary network-level effect in a cascade is to terminate branches of activity. This leads to shorter, not longer, avalanches on average. Pushing the system into a subcritical state truncates *both* the size and duration distributions. The prediction of a heavier-tailed duration distribution is contrary to the expected outcome of subcritical dynamics.\n- **Verdict:** Incorrect.\n\n**E. Dissipation leaves the critical exponents $\\tau_s$ and $\\tau_T$ exactly unchanged but induces stretched-exponential tails that increase the relative probability of very large avalanches compared to the conservative critical case.**\n- **Analysis:** This is incorrect. The main flaw is the claim that dissipation *increases* the probability of very large avalanches. A subcritical system is defined by the suppression of large events compared to the critical case. Whether the cutoff is exponential or stretched-exponential, its function is to cause a rapid decay in probability for large events, not an enhancement.\n- **Verdict:** Incorrect.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "A hallmark of self-organized criticality is the emergence of power-law distributions in event sizes, which appear as straight lines on a log-log plot. However, in rigorous scientific practice, visual inspection is insufficient; robust statistical validation is essential to distinguish true power laws from other heavy-tailed distributions. This advanced practice  guides you through the process of implementing a modern statistical pipeline to test the power-law hypothesis, equipping you with the critical skills needed to analyze empirical or simulated data and avoid common pitfalls in identifying SOC.",
            "id": "4301973",
            "problem": "Design and implement a complete program that, for a given set of synthetic datasets representing event sizes in complex systems, evaluates whether the data support a pure power law as the most parsimonious model for the tail, in a way that avoids overinterpreting noise as Self-Organized Criticality (SOC). Your solution must be grounded in first principles from probability theory and statistical inference, and use likelihood ratios to compare a pure power law with alternatives. The final output must be a single line containing a list of booleans that summarize the decision for each dataset.\n\nFoundational definitions and assumptions:\n- Self-Organized Criticality (SOC) is empirically associated with scale-free, heavy-tailed event-size distributions. A canonical model is the continuous, lower-truncated power law for event sizes $x \\ge x_{\\min}$ with probability density function (pdf)\n$$\nf_{\\mathrm{PL}}(x \\mid \\alpha, x_{\\min}) = (\\alpha - 1) x_{\\min}^{\\alpha - 1} x^{-\\alpha}, \\quad \\alpha  1, \\; x \\ge x_{\\min}.\n$$\n- Competing models include:\n  1. The lower-truncated exponential with rate parameter $\\lambda$,\n  $$\n  f_{\\mathrm{EXP}}(x \\mid \\lambda, x_{\\min}) = \\lambda \\exp\\big(-\\lambda (x - x_{\\min})\\big), \\quad \\lambda  0, \\; x \\ge x_{\\min}.\n  $$\n  2. The lower-truncated lognormal with parameters $\\mu$ and $\\sigma$,\n  $$\n  f_{\\mathrm{LN}}(x \\mid \\mu, \\sigma, x_{\\min}) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} \\exp\\!\\left( -\\frac{(\\ln x - \\mu)^2}{2\\sigma^2} \\right) \\bigg/ \\Big(1 - \\Phi\\!\\Big( \\frac{\\ln x_{\\min} - \\mu}{\\sigma} \\Big) \\Big), \\quad \\sigma  0, \\; x \\ge x_{\\min},\n  $$\n  where $\\Phi(\\cdot)$ is the standard normal cumulative distribution function.\n  3. The doubly-truncated power law on $x \\in [x_{\\min}, x_{\\max}]$ with exponent $\\alpha$,\n  $$\n  f_{\\mathrm{TPL}}(x \\mid \\alpha, x_{\\min}, x_{\\max}) = \\frac{x^{-\\alpha}}{Z(\\alpha;x_{\\min},x_{\\max})}, \\quad Z(\\alpha;x_{\\min},x_{\\max}) = \n  \\begin{cases}\n    \\dfrac{x_{\\min}^{1-\\alpha} - x_{\\max}^{1-\\alpha}}{1 - \\alpha},  \\alpha \\ne 1, \\\\\n    \\ln\\!\\left(\\dfrac{x_{\\max}}{x_{\\min}}\\right),  \\alpha = 1,\n  \\end{cases}\n  $$\n  with $x_{\\max}$ taken to be the sample maximum to avoid free-boundary pathologies.\n\n- Maximum Likelihood Estimation (MLE) is used to fit parameters within each tail model for observations $x_i \\ge x_{\\min}$. For the lower-truncated power law, the MLE for the exponent is\n$$\n\\hat{\\alpha} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)},\n$$\nwhere $n$ is the number of tail observations. For the lower-truncated exponential,\n$$\n\\hat{\\lambda} = \\frac{1}{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - x_{\\min})}.\n$$\nThe lower-truncated lognormal $(\\mu, \\sigma)$ requires numerical maximization of the log-likelihood. The doubly-truncated power law exponent $\\alpha$ is estimated by numerical maximization of the log-likelihood given $x_{\\max}$ fixed to the sample maximum.\n\n- The Kolmogorov–Smirnov (KS) method is used to select the lower cutoff $x_{\\min}$ for the pure power law tail: over a candidate set of cutoffs $x_{\\min}$ that leave at least $n_{\\text{tail}}$ observations, compute $\\hat{\\alpha}(x_{\\min})$ and the one-sample KS distance between the empirical tail cumulative distribution function and $F_{\\mathrm{PL}}(x \\mid \\hat{\\alpha}(x_{\\min}), x_{\\min}) = 1 - (x/x_{\\min})^{1-\\hat{\\alpha}}$. Choose the $x_{\\min}$ that minimizes the KS distance, and retain the corresponding tail.\n\n- Model comparison uses the log-likelihood ratio and the Vuong test for non-nested models with Bayesian Information Criterion (BIC) correction. Given two fitted models $M_1$ and $M_2$, define per-observation log-likelihood contributions $\\ell_{1,i}$ and $\\ell_{2,i}$. For $n$ tail observations, define\n$$\n\\delta_i = \\ell_{1,i} - \\ell_{2,i}, \\quad \\bar{\\delta} = \\frac{1}{n} \\sum_{i=1}^{n} \\delta_i, \\quad s = \\sqrt{ \\frac{1}{n-1} \\sum_{i=1}^{n} (\\delta_i - \\bar{\\delta})^2 }.\n$$\nLet $k_1$ and $k_2$ be the parameter counts of $M_1$ and $M_2$. The BIC-corrected mean is\n$$\n\\bar{\\delta}_{\\mathrm{BIC}} = \\bar{\\delta} - \\frac{(k_1 - k_2) \\ln n}{2n}.\n$$\nThe Vuong $z$-statistic is\n$$\nz = \\frac{\\sqrt{n} \\, \\bar{\\delta}_{\\mathrm{BIC}}}{s},\n$$\nwith two-sided $p$-value computed under the standard normal distribution. The total log-likelihood ratio is $L = \\sum_{i=1}^{n} \\delta_i$.\n\n- Decision criteria to avoid overinterpreting noise as SOC:\n  1. The tail sample size must satisfy $n_{\\text{tail}} \\ge 100$.\n  2. The estimated pure power law exponent must fall within a physically plausible heavy-tail range $1.5 \\le \\hat{\\alpha} \\le 3.5$.\n  3. Using Bonferroni correction for three pairwise comparisons, require $p  0.05/3$ and a positive log-likelihood ratio in favor of the pure power law when comparing against the lower-truncated exponential and the lower-truncated lognormal.\n  4. The pure power law should not be significantly worse than the doubly-truncated power law; specifically, either the log-likelihood ratio is non-negative or the $p$-value is not significant at the Bonferroni-adjusted level $0.05/3$.\n  5. All inference is performed on the tail defined by the KS-selected $x_{\\min}$.\n\nYour program must:\n- Generate the following five synthetic datasets, each with a fixed random seed, then apply the above tail selection, fitting, and model comparison procedure to produce a boolean per dataset indicating whether the data meet the SOC-supporting criteria.\n  1. Dataset A (happy path): Continuous pure power law with $n = 5000$, $\\alpha = 2.5$, $x_{\\min} = 1$, seed $= 12345$.\n  2. Dataset B (alternative heavy-tail): Continuous lognormal with $n = 5000$, $\\mu = 0$, $\\sigma = 1$, seed $= 23456$.\n  3. Dataset C (thin-tail): Lower-truncated exponential with $n = 5000$, $\\lambda = 1$, $x_{\\min} = 1$, seed $= 34567$.\n  4. Dataset D (finite-support heavy-tail): Doubly-truncated power law with $n = 5000$, $\\alpha = 2.2$, $x_{\\min} = 1$, $x_{\\max} = 100$, seed $= 45678$.\n  5. Dataset E (edge case): Continuous pure power law with small sample $n = 50$, $\\alpha = 2.5$, $x_{\\min} = 1$, seed $= 56789$.\n- For each dataset, estimate $x_{\\min}$ by KS minimization over a candidate set that leaves at least $50$ tail observations for consideration, fit all models on the selected tail, compute BIC-corrected Vuong statistics for the pairs $(\\mathrm{PL}, \\mathrm{EXP})$, $(\\mathrm{PL}, \\mathrm{LN})$, and $(\\mathrm{PL}, \\mathrm{TPL})$, and apply the decision criteria above to produce a single boolean result.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is a boolean corresponding to datasets A through E in order.\n\nNo physical units are involved in this problem. Angles are not applicable. All numerical results must be reported as booleans, with no percentage signs in any context. The program must be fully deterministic, using the specified seeds, and must not require any user input or external files.",
            "solution": "The problem requires the design and implementation of a rigorous statistical pipeline to evaluate whether synthetic datasets exhibit behavior consistent with Self-Organized Criticality (SOC), which is empirically characterized by a pure power-law distribution of event sizes. The solution avoids spurious identification of SOC by systematically comparing the power-law model against plausible alternatives—exponential, lognormal, and truncated power-law distributions—using a principled framework based on maximum likelihood estimation and likelihood ratio tests.\n\n### 1. Overall Methodological Framework\n\nThe core of the solution is a multi-stage validation process applied to each dataset. This process is designed to be conservative, accepting a power law as the best model only when there is strong statistical evidence. The stages are:\n1.  **Data Generation**: Synthetic datasets are created according to specified distributions to serve as test cases.\n2.  **Tail Identification**: The start of the heavy tail, $x_{\\min}$, is objectively determined from the data.\n3.  **Model Fitting**: The pure power-law model and three alternative models are fit to the identified tail data using Maximum Likelihood Estimation (MLE).\n4.  **Model Comparison**: The power-law model is compared pair-wise against each alternative using the Bayesian Information Criterion (BIC)-corrected Vuong's test for non-nested models.\n5.  **Decision Synthesis**: A set of predefined criteria, incorporating statistical significance, physical plausibility of parameters, and sample size, is used to make a final boolean decision.\n\n### 2. Data Generation\n\nTo test the methodology, five distinct datasets are generated. The generation process for a random variable $X$ with a continuous cumulative distribution function (CDF) $F(x)$ relies on the principle of inverse transform sampling. A uniformly distributed random number $U \\in [0, 1)$ is drawn, and the sample is computed as $x = F^{-1}(U)$.\n\n-   **Continuous Power Law (PL)**: The CDF is $F(x) = 1 - (x/x_{\\min})^{1-\\alpha}$. Inverting this gives the generator function $x = x_{\\min}(1-U)^{-1/(\\alpha-1)}$, where $U$ is a uniform random variate.\n-   **Lower-Truncated Exponential (EXP)**: The CDF is $F(x) = 1 - \\exp(-\\lambda(x-x_{\\min}))$. The generator is $x = x_{\\min} - \\frac{1}{\\lambda}\\ln(1-U)$.\n-   **Continuous Lognormal (LN)**: Generated by drawing a sample $y$ from a normal distribution $N(\\mu, \\sigma)$ and computing $x = \\exp(y)$.\n-   **Doubly-Truncated Power Law (TPL)**: For $\\alpha \\ne 1$, the CDF is $F(x) = \\frac{x^{1-\\alpha} - x_{\\min}^{1-\\alpha}}{x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha}}$. The generator is $x = \\left[x_{\\min}^{1-\\alpha} + U(x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha})\\right]^{1/(1-\\alpha)}$.\n\nFixed random seeds ensure the entire analysis is deterministic and reproducible.\n\n### 3. Tail Identification: The Kolmogorov-Smirnov Approach\n\nA critical step in power-law analysis is identifying the lower bound of the tail region, $x_{\\min}$, above which the power law is hypothesized to hold. We adopt the method proposed by Clauset, Shalizi, and Newman, which selects the $x_{\\min}$ that makes the data best fit a power-law distribution.\n\nThe algorithm proceeds as follows:\n1.  A set of candidate values for $x_{\\min}$ is chosen from the unique values in the dataset. To ensure sufficient data for reliable fitting, only candidates that result in a tail containing at least $n_{\\text{tail-search}} = 50$ data points are considered.\n2.  For each candidate $x_{\\min}$, the data points $x_i \\ge x_{\\min}$ form the tail.\n3.  The power-law exponent $\\hat{\\alpha}$ is estimated for this tail using its analytical MLE formula.\n4.  The Kolmogorov-Smirnov (KS) statistic, $D$, is calculated. This is the maximum absolute difference between the empirical CDF of the tail data and the theoretical CDF of the fitted power law, $F_{\\mathrm{PL}}(x | \\hat{\\alpha}, x_{\\min})$.\n5.  The optimal $x_{\\min}$ is the candidate value that minimizes the KS statistic $D$. This $x_{\\min}$ and its corresponding tail data are used for all subsequent analysis.\n\n### 4. Model Fitting: Maximum Likelihood Estimation (MLE)\n\nFor a given model with parameters $\\theta$ and a set of $n$ tail data points $\\{x_i\\}$, MLE finds the parameter values $\\hat{\\theta}$ that maximize the likelihood function $L(\\theta | \\{x_i\\}) = \\prod_{i=1}^n f(x_i | \\theta)$, or equivalently, the log-likelihood $\\mathcal{L}(\\theta | \\{x_i\\}) = \\sum_{i=1}^n \\ln f(x_i | \\theta)$. MLE is chosen for its desirable properties of asymptotic unbiasedness, efficiency, and consistency.\n\n-   **Power Law (PL, $k=1$)**: The MLE for the exponent $\\hat{\\alpha}$ is given by the analytical formula: $\\hat{\\alpha} = 1 + n / \\sum_{i=1}^{n} \\ln(x_i/x_{\\min})$.\n-   **Exponential (EXP, $k=1$)**: The MLE for the rate $\\hat{\\lambda}$ is also analytical: $\\hat{\\lambda} = \\left(\\frac{1}{n} \\sum_{i=1}^{n} x_i - x_{\\min}\\right)^{-1}$.\n-   **Lognormal (LN, $k=2$)**: The log-likelihood for the truncated lognormal distribution, involving parameters $\\mu$ and $\\sigma$, does not have a closed-form solution for its maximum. Therefore, $\\hat{\\mu}$ and $\\hat{\\sigma}$ are found by numerically minimizing the negative log-likelihood using a quasi-Newton optimization algorithm (`scipy.optimize.minimize`).\n-   **Doubly-Truncated Power Law (TPL, $k=1$)**: Similar to the lognormal case, the MLE for the exponent $\\hat{\\alpha}$ (with $x_{\\min}$ and $x_{\\max}$ fixed) must be found by numerically maximizing its log-likelihood function. This is a one-dimensional optimization problem, handled by `scipy.optimize.minimize_scalar`. Special care is taken for the normalization constant $Z(\\alpha)$ near $\\alpha=1$ to ensure numerical stability.\n\n### 5. Model Comparison: The Vuong Test with BIC Correction\n\nTo decide which model best describes the data, we employ a pairwise comparison strategy. The Vuong test is a likelihood-ratio test for model selection that can compare non-nested models, which is our case (e.g., power law vs. lognormal).\n\nGiven two models, $M_1$ and $M_2$, with fitted parameters and per-observation log-likelihoods $\\ell_{1,i}$ and $\\ell_{2,i}$:\n1.  The point-wise log-likelihood ratio is computed: $\\delta_i = \\ell_{1,i} - \\ell_{2,i}$.\n2.  The total log-likelihood ratio is $L = \\sum_{i} \\delta_i$. A positive $L$ favors $M_1$.\n3.  To account for model parsimony, the Bayesian Information Criterion (BIC) correction is applied. This penalizes models with more parameters. The BIC-corrected log-likelihood ratio is $L_{\\mathrm{BIC}} = L - \\frac{(k_1 - k_2) \\ln n}{2}$, where $k_1$ and $k_2$ are the number of parameters.\n4.  The Vuong $z$-statistic is calculated as $z = \\frac{L_{\\mathrm{BIC}}}{\\sqrt{n} s}$, where $s$ is the sample standard deviation of the $\\delta_i$ values.\n5.  Under the null hypothesis that the models are indistinguishable, $z$ follows a standard normal distribution. A two-sided $p$-value is computed from $z$. A small $p$-value indicates that one model is significantly better than the other, with the sign of $z$ determining which one.\n\n### 6. Decision Criteria for SOC\n\nA final boolean decision is made by applying a strict, sequential set of criteria designed to prevent the misidentification of SOC from statistical noise or alternative processes:\n1.  **Sufficient Tail Data**: The number of data points in the tail selected by the KS method, $n_{\\text{tail}}$, must be at least $100$. This ensures that all subsequent statistical tests are well-powered.\n2.  **Plausible Exponent**: The estimated power-law exponent $\\hat{\\alpha}$ must lie in the range $[1.5, 3.5]$. This range is consistent with many theoretical models of SOC and empirical observations.\n3.  **Superiority to Thin-Tailed and Lognormal Alternatives**: The power-law model must be statistically superior to both the exponential and lognormal models. This is assessed using the Vuong test. For both comparisons (PL vs. EXP and PL vs. LN), the total log-likelihood ratio $L$ must be positive (favoring PL), and the superiority must be significant at a Bonferroni-corrected level of $p  0.05/3$.\n4.  **Non-Inferiority to Truncated Power Law**: The pure power law must not be significantly worse than a doubly-truncated power law. This checks for strong finite-size effects. This condition is met if either the likelihood ratio favors the pure power-law ($L \\ge 0$) or the difference is not statistically significant ($p \\ge 0.05/3$).\n\nA dataset is considered to support the SOC hypothesis only if it passes all these criteria. This structured approach provides a robust and objective framework for the problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize, minimize_scalar\nfrom scipy.special import erf, log_ndtr\n\n# ==============================================================================\n# 1. Data Generation\n# ==============================================================================\n\ndef generate_power_law(n, alpha, x_min, seed):\n    \"\"\"Generates n samples from a continuous power-law distribution.\"\"\"\n    rng = np.random.default_rng(seed)\n    u = rng.uniform(0, 1, n)\n    return x_min * (1 - u)**(-1.0 / (alpha - 1.0))\n\ndef generate_lognormal(n, mu, sigma, seed):\n    \"\"\"Generates n samples from a lognormal distribution.\"\"\"\n    rng = np.random.default_rng(seed)\n    return rng.lognormal(mean=mu, sigma=sigma, size=n)\n\ndef generate_truncated_exponential(n, lam, x_min, seed):\n    \"\"\"Generates n samples from a lower-truncated exponential distribution.\"\"\"\n    rng = np.random.default_rng(seed)\n    u = rng.uniform(0, 1, n)\n    return x_min - (1.0 / lam) * np.log(1 - u)\n\ndef generate_doubly_truncated_power_law(n, alpha, x_min, x_max, seed):\n    \"\"\"Generates n samples from a doubly-truncated power-law distribution.\"\"\"\n    rng = np.random.default_rng(seed)\n    u = rng.uniform(0, 1, n)\n    if alpha == 1.0:\n        return x_min * np.exp(u * np.log(x_max / x_min))\n    else:\n        v = 1.0 - alpha\n        return (x_min**v + u * (x_max**v - x_min**v))**(1.0 / v)\n\n# ==============================================================================\n# 2. Model Fitting and Likelihoods\n# ==============================================================================\n\nclass Model:\n    def log_likelihood_per_point(self, params, data, x_min):\n        raise NotImplementedError\n\n    def fit(self, data, x_min):\n        raise NotImplementedError\n        \nclass PowerLaw(Model):\n    k = 1\n    \n    def fit(self, data, x_min):\n        n = len(data)\n        alpha = 1.0 + n / np.sum(np.log(data / x_min))\n        return (alpha,)\n\n    def log_likelihood_per_point(self, params, data, x_min):\n        alpha = params[0]\n        if alpha = 1: return np.full_like(data, -np.inf)\n        return np.log(alpha - 1) + (alpha - 1) * np.log(x_min) - alpha * np.log(data)\n\nclass Exponential(Model):\n    k = 1\n\n    def fit(self, data, x_min):\n        lam = 1.0 / (np.mean(data) - x_min)\n        return (lam,)\n\n    def log_likelihood_per_point(self, params, data, x_min):\n        lam = params[0]\n        if lam = 0: return np.full_like(data, -np.inf)\n        return np.log(lam) - lam * (data - x_min)\n\nclass Lognormal(Model):\n    k = 2\n\n    def fit(self, data, x_min):\n        def neg_log_likelihood(params, data, x_min):\n            return -np.sum(self.log_likelihood_per_point(params, data, x_min))\n        \n        # Initial guess from untruncated data properties\n        log_data = np.log(data)\n        mu_guess = np.mean(log_data)\n        sigma_guess = np.std(log_data)\n        \n        result = minimize(\n            neg_log_likelihood, \n            x0=[mu_guess, sigma_guess], \n            args=(data, x_min), \n            method='L-BFGS-B', \n            bounds=[(None, None), (1e-6, None)]\n        )\n        return tuple(result.x)\n\n    def log_likelihood_per_point(self, params, data, x_min):\n        mu, sigma = params\n        if sigma = 0: return np.full_like(data, -np.inf)\n        \n        log_data = np.log(data)\n        norm_const = -log_ndtr((np.log(x_min) - mu) / sigma)\n\n        log_pdf_untruncated = -log_data - np.log(sigma) - 0.5 * np.log(2 * np.pi) - ((log_data - mu)**2) / (2 * sigma**2)\n        return log_pdf_untruncated - norm_const\n\nclass TruncatedPowerLaw(Model):\n    k = 1\n\n    def fit(self, data, x_min):\n        x_max = np.max(data)\n        \n        def neg_log_likelihood(alpha, data, x_min, x_max_val):\n            return -np.sum(self.log_likelihood_per_point((alpha,), data, x_min, x_max_val))\n\n        result = minimize_scalar(\n            neg_log_likelihood, \n            args=(data, x_min, x_max),\n            bounds=(0.1, 10), # Reasonable search range for alpha\n            method='bounded'\n        )\n        return (result.x,)\n\n    def _z_alpha(self, alpha, x_min, x_max):\n        # Numerically stable calculation of the normalization constant Z\n        if np.abs(1.0 - alpha)  1e-8:\n            return np.log(x_max / x_min)\n        else:\n            v = 1.0 - alpha\n            # Use log-power to avoid overflow with large exponents\n            log_x_min_v = v * np.log(x_min)\n            log_x_max_v = v * np.log(x_max)\n            return (np.exp(log_x_max_v) - np.exp(log_x_min_v)) / v\n\n    def log_likelihood_per_point(self, params, data, x_min, x_max_val=None):\n        alpha = params[0]\n        if x_max_val is None: x_max_val = np.max(data)\n        \n        z = self._z_alpha(alpha, x_min, x_max_val)\n        if z = 0: return np.full_like(data, -np.inf)\n        \n        return -alpha * np.log(data) - np.log(z)\n\n# ==============================================================================\n# 3. Tail and Model Comparison\n# ==============================================================================\ndef find_xmin(data, min_tail_size):\n    unique_data = np.unique(data)\n    possible_xmins = unique_data[unique_data  0]\n    \n    best_d = np.inf\n    best_xmin = -1\n    \n    for x_min_candidate in possible_xmins:\n        tail = data[data = x_min_candidate]\n        n_tail = len(tail)\n        \n        if n_tail  min_tail_size:\n            break # Since data is sorted, subsequent tails will be smaller\n            \n        # Fit power law\n        try:\n            alpha = 1.0 + n_tail / np.sum(np.log(tail / x_min_candidate))\n        except (ValueError, ZeroDivisionError):\n            continue\n\n        if alpha = 1.0: # Not a valid power law\n            continue\n        \n        # Calculate theoretical CDF\n        empirical_cdf = np.arange(1, n_tail + 1) / n_tail\n        theoretical_cdf = 1.0 - (tail / x_min_candidate)**(1.0 - alpha)\n        \n        # KS statistic\n        d = np.max(np.abs(empirical_cdf - theoretical_cdf))\n        \n        if d  best_d:\n            best_d = d\n            best_xmin = x_min_candidate\n            \n    if best_xmin == -1:\n        return np.min(data), data[data = np.min(data)]\n    \n    final_tail = data[data = best_xmin]\n    return best_xmin, final_tail\n\ndef vuong_test(data, x_min, model1, params1, model2, params2):\n    n = len(data)\n    \n    is_tpl = isinstance(model2, TruncatedPowerLaw)\n    x_max = np.max(data) if is_tpl else None\n    \n    l1 = model1.log_likelihood_per_point(params1, data, x_min)\n    l2 = model2.log_likelihood_per_point(params2, data, x_min, x_max_val=x_max)\n\n    delta = l1 - l2\n    \n    # Handle -inf from bad fits\n    if np.any(np.isinf(l1)) or np.any(np.isinf(l2)):\n        return -np.inf, 1.0\n\n    L = np.sum(delta)\n    s = np.std(delta, ddof=1)\n    if s == 0: # Models are identical or one is a scaled version of other\n        return L, 1.0\n\n    k1, k2 = model1.k, model2.k\n    bic_correction = (k1 - k2) * np.log(n) / 2.0\n    L_bic = L - bic_correction\n    \n    z = L_bic / (s * np.sqrt(n))\n    p_value = 2 * norm.sf(np.abs(z))\n    \n    return L, p_value\n\n# ==============================================================================\n# 4. Main Solver\n# ==============================================================================\ndef analyze_dataset(data):\n    \"\"\"Applies the full statistical pipeline to one dataset.\"\"\"\n    p_alpha = 0.05 / 3.0\n    \n    # 1. Tail identification\n    best_xmin, tail_data = find_xmin(data, min_tail_size=50)\n    tail_data.sort()\n    n_tail = len(tail_data)\n\n    # 2. Criterion 1: Sufficient tail size\n    if n_tail  100:\n        return False\n        \n    pl_model = PowerLaw()\n\n    # 3. Fit PL and check Criterion 2\n    try:\n        alpha_hat = pl_model.fit(tail_data, best_xmin)[0]\n    except (ValueError, ZeroDivisionError):\n        return False\n\n    if not (1.5 = alpha_hat = 3.5):\n        return False\n\n    params_pl = (alpha_hat,)\n    \n    # 4. Fit alternative models\n    exp_model, ln_model, tpl_model = Exponential(), Lognormal(), TruncatedPowerLaw()\n    try:\n        params_exp = exp_model.fit(tail_data, best_xmin)\n        params_ln = ln_model.fit(tail_data, best_xmin)\n        params_tpl = tpl_model.fit(tail_data, best_xmin)\n    except (RuntimeError, ValueError, ZeroDivisionError):\n        # Catch potential optimization or numerical errors\n        return False\n        \n    # 5. Vuong tests and Criteria 3  4\n    # PL vs EXP\n    L_pl_exp, p_pl_exp = vuong_test(tail_data, best_xmin, pl_model, params_pl, exp_model, params_exp)\n    if not (L_pl_exp  0 and p_pl_exp  p_alpha):\n        return False\n        \n    # PL vs LN\n    L_pl_ln, p_pl_ln = vuong_test(tail_data, best_xmin, pl_model, params_pl, ln_model, params_ln)\n    if not (L_pl_ln  0 and p_pl_ln  p_alpha):\n        return False\n        \n    # PL vs TPL\n    L_pl_tpl, p_pl_tpl = vuong_test(tail_data, best_xmin, pl_model, params_pl, tpl_model, params_tpl)\n    if not (L_pl_tpl = 0 or p_pl_tpl = p_alpha):\n        return False\n\n    return True\n\ndef solve():\n    test_cases = [\n        ('A', 'pl', {'n': 5000, 'alpha': 2.5, 'x_min': 1, 'seed': 12345}),\n        ('B', 'ln', {'n': 5000, 'mu': 0, 'sigma': 1, 'seed': 23456}),\n        ('C', 'exp', {'n': 5000, 'lam': 1, 'x_min': 1, 'seed': 34567}),\n        ('D', 'tpl', {'n': 5000, 'alpha': 2.2, 'x_min': 1, 'x_max': 100, 'seed': 45678}),\n        ('E', 'pl', {'n': 50, 'alpha': 2.5, 'x_min': 1, 'seed': 56789}),\n    ]\n    \n    results = []\n    \n    for _, type, params in test_cases:\n        if type == 'pl':\n            data = generate_power_law(**params)\n        elif type == 'ln':\n            data = generate_lognormal(**params)\n        elif type == 'exp':\n            data = generate_truncated_exponential(**params)\n        elif type == 'tpl':\n            data = generate_doubly_truncated_power_law(**params)\n        \n        result = analyze_dataset(data)\n        results.append(result)\n\n    # Format output as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    # To conform to platform requirements, run solve() directly.\n    solve()\n\n```"
        }
    ]
}