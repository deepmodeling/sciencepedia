{
    "hands_on_practices": [
        {
            "introduction": "Bridging scales in the brain often begins with an accurate model of its fundamental components: the neurons. This first practice provides hands-on experience in numerically solving the conductance-based cable equation, a partial differential equation describing voltage dynamics along a dendrite. By implementing an operator-splitting scheme and verifying your code using the Method of Manufactured Solutions , you will build foundational computational skills essential for simulating biophysically detailed models and develop a rigorous approach to code verification and error analysis.",
            "id": "4000922",
            "problem": "Consider a one-dimensional, nondimensionalized conductance-based cable equation on a dendritic segment with sealed ends. The membrane potential is denoted by $V(x,t)$ for spatial coordinate $x \\in [0,L]$ and time $t \\in [0,T]$. The governing equation is\n$$\nC \\,\\frac{\\partial V}{\\partial t} \\;=\\; D\\,\\frac{\\partial^2 V}{\\partial x^2} \\;-\\; g_L\\,(V - E_L) \\;-\\; g_s\\, s(x,t)\\,(V - E_s) \\;+\\; I(x,t),\n$$\nsubject to homogeneous Neumann boundary conditions at $x=0$ and $x=L$,\n$$\n\\frac{\\partial V}{\\partial x}(0,t) = 0, \\qquad \\frac{\\partial V}{\\partial x}(L,t) = 0,\n$$\nand initial condition\n$$\nV(x,0) = V^\\star(x,0),\n$$\nwhere all variables and parameters are nondimensional. The leak conductance term $g_L$ and the synaptic conductance strength $g_s$ are constants, $C$ is the membrane capacitance, $D$ is the axial diffusion coefficient, $E_L$ and $E_s$ are reversal potentials for leak and synaptic currents, respectively, and $s(x,t)$ is a prescribed synaptic activation. You are given the manufactured exact fields\n$$\nV^\\star(x,t) \\;=\\; E_L \\;+\\; A \\cos(k x)\\,e^{-\\omega t} \\;+\\; B \\cos(2 k x)\\,e^{-2\\omega t},\n$$\n$$\ns^\\star(x,t) \\;=\\; s_0 \\;+\\; s_A \\cos(k x)\\,e^{-\\nu t},\n$$\nwith $k = \\pi$ to satisfy the sealed-end boundary condition. Define the source $I(x,t)$ by substitution so that $V^\\star$ exactly satisfies the partial differential equation with $s=s^\\star$:\n$$\nI(x,t) \\;=\\; C\\,\\frac{\\partial V^\\star}{\\partial t}(x,t) \\;-\\; D\\,\\frac{\\partial^2 V^\\star}{\\partial x^2}(x,t) \\;+\\; g_L\\,(V^\\star(x,t) - E_L) \\;+\\; g_s\\, s^\\star(x,t)\\,(V^\\star(x,t) - E_s).\n$$\n\nImplement a Lie–Trotter operator-splitting integrator over the interval $t \\in [0,T]$ that alternates between a reaction step and a diffusion step of equal duration $\\Delta t$ at each time level $t^n = n\\,\\Delta t$:\n\n- Reaction substep: At each spatial grid point, evolve $V$ over one time increment $\\Delta t$ by solving the spatially local ordinary differential equation\n$$\nC\\,\\frac{dV}{dt} \\;=\\; -\\,g_L\\,(V - E_L) \\;-\\; g_s\\, s(x,t)\\,(V - E_s) \\;+\\; I(x,t),\n$$\nwith $s(x,t)$ and $I(x,t)$ both treated as frozen at their values at time $t^n$, i.e., as time-constant coefficients over the substep.\n\n- Diffusion substep: Solve the diffusion-only problem over one time increment $\\Delta t$,\n$$\nC\\,\\frac{\\partial V}{\\partial t} \\;=\\; D\\,\\frac{\\partial^2 V}{\\partial x^2},\n$$\ndiscretized in space by the standard second-order central difference scheme with homogeneous Neumann boundary conditions and in time by the backward Euler method.\n\nUse a uniform grid with $N_x$ points on $[0,L]$, with the usual discrete step size $\\Delta x = L/(N_x - 1)$. Use the initial condition $V(x,0) = V^\\star(x,0)$. At the final time $T$, compute the discrete $L^2$ error over the spatial grid,\n$$\n\\|e\\|_{L^2} \\;\\approx\\; \\left( \\sum_{j=0}^{N_x - 1} \\left[V_j(T) - V^\\star(x_j,T)\\right]^2 \\Delta x \\right)^{1/2}.\n$$\n\nYour tasks:\n\n1. Derive the expression for $I(x,t)$ from the given $V^\\star$ and $s^\\star$ by direct substitution of the governing equation and compute it pointwise as needed by the numerical scheme.\n\n2. Implement the Lie–Trotter operator-splitting scheme exactly as specified, with the reaction substep using the exact solution of the local linear ordinary differential equation under frozen coefficients over $\\Delta t$, and with the diffusion substep using backward Euler in time and second-order central differences in space that enforce homogeneous Neumann boundary conditions.\n\n3. Using the following fixed parameter values,\n$$\nL = 1, \\quad T = 0.2, \\quad C = 1, \\quad D = 0.1, \\quad g_L = 0.2, \\quad g_s = 0.5, \\quad E_L = -0.65, \\quad E_s = 0,\n$$\nand manufactured-solution parameters\n$$\nA = 0.05, \\quad B = -0.03, \\quad k = \\pi, \\quad \\omega = 3.0, \\quad s_0 = 0.1, \\quad s_A = 0.05, \\quad \\nu = 5.0,\n$$\nperform two refinement studies to estimate the global-order scaling of the numerical error with respect to the time step $\\Delta t$ and the spatial grid spacing $\\Delta x$:\n\n- Time refinement (with spatial grid fine enough to make spatial error negligible): Fix $N_x = 200$ and use time steps $\\Delta t \\in \\{T/50,\\, T/100,\\, T/200,\\, T/400\\}$. For each $\\Delta t$, compute the $L^2$ error at $t=T$. Estimate the convergence order $p_t$ by a least-squares linear regression of $\\log(\\|e\\|_{L^2})$ versus $\\log(\\Delta t)$.\n\n- Space refinement (with time step small enough to make temporal error negligible): Fix $\\Delta t = T/4000$ and use $N_x \\in \\{25,\\, 50,\\, 100,\\, 200\\}$, i.e., $\\Delta x = L/(N_x-1)$. For each $N_x$, compute the $L^2$ error at $t=T$. Estimate the convergence order $p_x$ by a least-squares linear regression of $\\log(\\|e\\|_{L^2})$ versus $\\log(\\Delta x)$.\n\n4. Additionally, verify monotonic decrease of the error in each refinement sequence by reporting whether the errors strictly decrease as $\\Delta t$ decreases and as $\\Delta x$ decreases.\n\nTest suite and required final output:\n\n- Use exactly the parameter values and refinement sets specified above.\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[p_t, p_x, m_t, m_x]$, where $p_t$ and $p_x$ are floating-point estimates of the orders of accuracy from the regressions, rounded to three decimal places, and $m_t$ and $m_x$ are integers that are $1$ if the corresponding sequence of errors is strictly monotonically decreasing and $0$ otherwise. All quantities are dimensionless and require no physical units.",
            "solution": "The user-provided problem is assessed as **valid** following a rigorous validation process. It is a well-posed, scientifically grounded problem from computational neuroscience that uses the standard Method of Manufactured Solutions (MMS) to verify a numerical scheme. All parameters and procedures are clearly defined, constituting a complete and solvable problem.\n\nThe solution proceeds in three main steps: derivation of the analytical source term, formulation of the numerical operator-splitting scheme, and implementation of the verification studies.\n\n### 1. Derivation of the Source Term $I(x,t)$\n\nThe problem requires that the manufactured solution $V^\\star(x,t)$ exactly satisfies the governing PDE when a specific source term $I(x,t)$ is used. The expression for $I(x,t)$ is obtained by rearranging the PDE:\n$$\nI(x,t) = C\\,\\frac{\\partial V^\\star}{\\partial t} - D\\,\\frac{\\partial^2 V^\\star}{\\partial x^2} + g_L\\,(V^\\star - E_L) + g_s\\, s^\\star\\,(V^\\star - E_s)\n$$\nWe are given the manufactured solution for the membrane potential $V^\\star(x,t)$ and the synaptic activation $s^\\star(x,t)$:\n$$\nV^\\star(x,t) = E_L + A \\cos(k x)\\,e^{-\\omega t} + B \\cos(2 k x)\\,e^{-2\\omega t}\n$$\n$$\ns^\\star(x,t) = s_0 + s_A \\cos(k x)\\,e^{-\\nu t}\n$$\nFirst, we compute the necessary partial derivatives of $V^\\star(x,t)$:\nThe partial derivative with respect to time $t$ is:\n$$\n\\frac{\\partial V^\\star}{\\partial t}(x,t) = -A\\omega \\cos(k x)\\,e^{-\\omega t} - 2B\\omega \\cos(2 k x)\\,e^{-2\\omega t}\n$$\nThe second partial derivative with respect to space $x$ is:\n$$\n\\frac{\\partial V^\\star}{\\partial x}(x,t) = -Ak \\sin(k x)\\,e^{-\\omega t} - 2Bk \\sin(2 k x)\\,e^{-2\\omega t}\n$$\n$$\n\\frac{\\partial^2 V^\\star}{\\partial x^2}(x,t) = -Ak^2 \\cos(k x)\\,e^{-\\omega t} - 4Bk^2 \\cos(2 k x)\\,e^{-2\\omega t}\n$$\nThe choice $k=\\pi$ and $L=1$ ensures that the homogeneous Neumann boundary conditions, $\\frac{\\partial V^\\star}{\\partial x}(0,t) = 0$ and $\\frac{\\partial V^\\star}{\\partial x}(L,t) = 0$, are satisfied, since $\\sin(0) = 0$, $\\sin(kL) = \\sin(\\pi) = 0$, and $\\sin(2kL) = \\sin(2\\pi) = 0$.\n\nSubstituting these derivatives and the expressions for $V^\\star$ and $s^\\star$ into the formula for $I(x,t)$ provides the complete analytical form of the source term, which can be computed pointwise as required by the numerical scheme.\n\n### 2. Lie–Trotter Operator-Splitting Scheme\n\nThe time evolution from $t^n$ to $t^{n+1} = t^n + \\Delta t$ is split into two substeps, reaction and diffusion, each applied sequentially over the full time step $\\Delta t$. Let $V^n$ be the numerical solution at time $t^n$.\n\n#### 2.1. Reaction Substep\nThe first substep involves solving the spatially local ordinary differential equation (ODE) at each grid point $x_j$:\n$$\nC\\,\\frac{dV}{dt} = -g_L\\,(V - E_L) - g_s\\, s(x_j,t^n)\\,(V - E_s) + I(x_j,t^n)\n$$\nThe terms $s(x,t)$ and $I(x,t)$ are frozen at their values at $t=t^n$. This is a linear first-order ODE of the form $C \\frac{dV}{dt} = -\\gamma V + \\delta$, where:\n$$\n\\gamma_j = g_L + g_s s^\\star(x_j, t^n)\n$$\n$$\n\\delta_j = g_L E_L + g_s s^\\star(x_j, t^n) E_s + I(x_j, t^n)\n$$\nThe exact solution to this ODE over the interval $[t^n, t^n + \\Delta t]$ with initial condition $V(t^n) = V^n_j$ is:\n$$\nV^{int}_j = \\frac{\\delta_j}{\\gamma_j} + \\left(V^n_j - \\frac{\\delta_j}{\\gamma_j}\\right) e^{-\\frac{\\gamma_j}{C}\\Delta t}\n$$\nHere, $V^{int}$ represents the intermediate solution after the reaction step.\n\n#### 2.2. Diffusion Substep\nThe second substep solves the diffusion equation using the intermediate solution $V^{int}$ as the initial condition:\n$$\nC\\,\\frac{\\partial V}{\\partial t} = D\\,\\frac{\\partial^2 V}{\\partial x^2}\n$$\nThis PDE is discretized using the backward Euler method in time and a second-order central difference scheme in space. For a uniform grid with spacing $\\Delta x$, the scheme is:\n$$\nC \\frac{V_j^{n+1} - V^{int}_j}{\\Delta t} = D \\frac{V_{j+1}^{n+1} - 2V_j^{n+1} + V_{j-1}^{n+1}}{(\\Delta x)^2}\n$$\nwhere $V^{n+1}$ is the final solution at time $t^{n+1}$. Rearranging the terms, we get a linear system of equations for the vector $V^{n+1}$:\n$$\n-\\alpha V_{j-1}^{n+1} + (1+2\\alpha)V_j^{n+1} - \\alpha V_{j+1}^{n+1} = V^{int}_j, \\quad \\text{for } j=1, \\dots, N_x-2\n$$\nwhere $\\alpha = \\frac{D \\Delta t}{C (\\Delta x)^2}$. The homogeneous Neumann boundary conditions $\\frac{\\partial V}{\\partial x}=0$ are discretized using centered differences on ghost points, which yields modified equations at the boundaries ($j=0$ and $j=N_x-1$):\n$$\n\\text{At } j=0: \\qquad (1+2\\alpha)V_0^{n+1} - 2\\alpha V_1^{n+1} = V^{int}_0\n$$\n$$\n\\text{At } j=N_x-1: \\quad -2\\alpha V_{N_x-2}^{n+1} + (1+2\\alpha)V_{N_x-1}^{n+1} = V^{int}_{N_x-1}\n$$\nThis results in a tridiagonal linear system $M V^{n+1} = V^{int}$, where the matrix $M$ is constant for a fixed $\\Delta t$ and $\\Delta x$. For efficiency, this matrix is constructed and factorized (e.g., using LU decomposition) once before the time-stepping loop begins.\n\n### 3. Convergence Analysis\nThe Lie-Trotter splitting scheme is formally first-order accurate in time ($p_t \\approx 1$). The spatial discretization using central differences is second-order accurate ($p_x \\approx 2$). To verify these theoretical orders, two numerical refinement studies are conducted:\n1.  **Time Refinement:** The spatial grid is made sufficiently fine ($N_x=200$) so that spatial errors are negligible. The time step $\\Delta t$ is systematically reduced, and the resulting $L^2$ error is computed at the final time $T$. The convergence order $p_t$ is estimated as the slope of a least-squares linear fit to $\\log(\\|e\\|_{L^2})$ versus $\\log(\\Delta t)$.\n2.  **Space Refinement:** Similarly, the time step is made sufficiently small ($\\Delta t=T/4000$) to isolate spatial errors. The grid spacing $\\Delta x$ is reduced by increasing $N_x$. The order $p_x$ is estimated as the slope of a fit to $\\log(\\|e\\|_{L^2})$ versus $\\log(\\Delta x)$.\n\nFinally, the sequence of errors for each study is checked for strict monotonic decrease, a typical indicator of convergence for a well-behaved numerical method.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import lu_factor, lu_solve\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the numerical simulation, convergence studies,\n    and final result calculation as per the problem description.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    L = 1.0\n    T = 0.2\n    C = 1.0\n    D = 0.1\n    g_L = 0.2\n    g_s = 0.5\n    E_L = -0.65\n    E_s = 0.0\n\n    # --- Manufactured Solution Parameters ---\n    A = 0.05\n    B = -0.03\n    k = math.pi\n    omega = 3.0\n    s_0 = 0.1\n    s_A = 0.05\n    nu = 5.0\n\n    # --- Helper Functions for Manufactured Solution and Source Term ---\n\n    def V_star(x, t):\n        \"\"\"Computes the manufactured exact solution V*(x, t).\"\"\"\n        term1 = A * np.cos(k * x) * np.exp(-omega * t)\n        term2 = B * np.cos(2 * k * x) * np.exp(-2 * omega * t)\n        return E_L + term1 + term2\n\n    def s_star(x, t):\n        \"\"\"Computes the prescribed synaptic activation s*(x, t).\"\"\"\n        return s_0 + s_A * np.cos(k * x) * np.exp(-nu * t)\n\n    def I_fun(x, t):\n        \"\"\"Computes the source term I(x, t) to satisfy the PDE with V*.\"\"\"\n        v_star_val = V_star(x, t)\n        \n        # ∂V*/∂t\n        dv_dt = -A * omega * np.cos(k * x) * np.exp(-omega * t) \\\n                - 2 * B * omega * np.cos(2 * k * x) * np.exp(-2 * omega * t)\n                \n        # ∂²V*/∂x²\n        d2v_dx2 = -A * k**2 * np.cos(k * x) * np.exp(-omega * t) \\\n                  - 4 * B * k**2 * np.cos(2 * k * x) * np.exp(-2 * omega * t)\n        \n        s_star_val = s_star(x, t)\n        \n        I_val = C * dv_dt - D * d2v_dx2 + g_L * (v_star_val - E_L) + g_s * s_star_val * (v_star_val - E_s)\n        return I_val\n\n    # --- Simulation Core ---\n    \n    def run_simulation(Nx, Nt):\n        \"\"\"\n        Runs the simulation for a given spatial and temporal discretization.\n        Returns the L2 error at the final time T.\n        \"\"\"\n        dx = L / (Nx - 1)\n        dt = T / Nt\n        x = np.linspace(0, L, Nx)\n\n        V = V_star(x, 0)\n\n        # Setup the diffusion matrix M for the backward Euler step\n        alpha = D * dt / (C * dx**2)\n        M = np.zeros((Nx, Nx))\n        \n        # Interior points (j = 1, ..., Nx-2)\n        diag_indices = np.arange(1, Nx - 1)\n        M[diag_indices, diag_indices - 1] = -alpha\n        M[diag_indices, diag_indices] = 1 + 2 * alpha\n        M[diag_indices, diag_indices + 1] = -alpha\n        \n        # Boundary points (j=0 and j=Nx-1) for Neumann BCs\n        M[0, 0] = 1 + 2 * alpha\n        M[0, 1] = -2 * alpha\n        M[Nx - 1, Nx - 1] = 1 + 2 * alpha\n        M[Nx - 1, Nx - 2] = -2 * alpha\n\n        M_lu, M_piv = lu_factor(M, check_finite=False)\n        \n        for n in range(Nt):\n            t_n = n * dt\n            \n            # --- 1. Reaction Substep ---\n            s_frozen = s_star(x, t_n)\n            I_frozen = I_fun(x, t_n)\n            \n            gamma = g_L + g_s * s_frozen\n            delta = g_L * E_L + g_s * s_frozen * E_s + I_frozen\n            \n            V_ss = delta / gamma\n            V_intermediate = V_ss + (V - V_ss) * np.exp(-gamma * dt / C)\n\n            # --- 2. Diffusion Substep ---\n            V = lu_solve((M_lu, M_piv), V_intermediate, check_finite=False)\n\n        V_exact_T = V_star(x, T)\n        error_L2 = np.sqrt(np.sum((V - V_exact_T)**2) * dx)\n        \n        return error_L2\n\n    # --- Time Refinement Study ---\n    Nx_t = 200\n    Nt_t_vals = [50, 100, 200, 400]\n    dt_vals = np.array([T / nt for nt in Nt_t_vals])\n    \n    errors_t = np.array([run_simulation(Nx=Nx_t, Nt=nt) for nt in Nt_t_vals])\n\n    p_t = np.polyfit(np.log(dt_vals), np.log(errors_t), 1)[0]\n    m_t = 1 if np.all(np.diff(errors_t)  0) else 0\n\n    # --- Space Refinement Study ---\n    Nt_x = 4000\n    Nx_x_vals = [25, 50, 100, 200]\n    dx_vals = np.array([L / (nx - 1) for nx in Nx_x_vals])\n    \n    errors_x = np.array([run_simulation(Nx=nx, Nt=Nt_x) for nx in Nx_x_vals])\n        \n    p_x = np.polyfit(np.log(dx_vals), np.log(errors_x), 1)[0]\n    m_x = 1 if np.all(np.diff(errors_x)  0) else 0\n    \n    # --- Final Output ---\n    p_t_rounded = round(p_t, 3)\n    p_x_rounded = round(p_x, 3)\n    \n    results = [p_t_rounded, p_x_rounded, m_t, m_x]\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While detailed spatial models are powerful, much insight can be gained from analytically tractable 'point-neuron' models. This exercise focuses on a core process of neural computation: synaptic integration. By treating the synapse as a linear time-invariant system, you will derive the postsynaptic current resulting from a train of incoming spikes . This practice hones your ability to apply convolution and LTI system theory to bridge the gap between discrete spike events and the continuous electrical dynamics of a neuron.",
            "id": "4000876",
            "problem": "A single-compartment neuron is voltage clamped at a constant membrane potential $V_{\\text{hold}}$. It receives a set of presynaptic spikes at times $\\{t_k\\}_{k=1}^{N}$ impinging on an excitatory conductance-based synapse with reversal potential $E_{\\text{syn}}$. At the microscopic level, an isolated presynaptic spike at time $t=0$ evokes a causal alpha-function conductance transient\n$$\ng_{\\alpha}(t) \\;=\\; g_0 \\,\\frac{t}{\\tau}\\,\\exp\\!\\big(1 - \\frac{t}{\\tau}\\big)\\,H(t),\n$$\nwhere $g_0$ is a peak conductance, $\\tau$ is a synaptic time constant, and $H(t)$ is the Heaviside step function. Assume linearity and time-invariance of the synaptic input-output relationship and negligible interaction between conductances beyond linear superposition. Represent the presynaptic spike train as a sum of Dirac delta impulses and use the definition of convolution to derive from first principles the effective synaptic filter produced by the entire spike train and, under the voltage clamp, the resulting postsynaptic current $I(t)$.\n\nIn particular:\n- Start from a representation of the spike train $s(t)$ in terms of Dirac delta distributions and the convolution integral for a linear time-invariant system.\n- Derive a closed-form expression for the total synaptic conductance $g(t)$ evoked by the spike train $\\{t_k\\}$.\n- Using Ohm’s law for synaptic current under voltage clamp, $I(t) = g(t)\\,(V_{\\text{hold}} - E_{\\text{syn}})$, compute a single closed-form analytic expression for $I(t)$ in terms of $t$, $\\{t_k\\}$, $g_0$, and $\\tau$ that enforces causality.\n\nProvide your final answer as a single closed-form analytic expression for $I(t)$ (no units inside the final expression). Do not round.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, objective, and internally consistent. It represents a standard exercise in computational neuroscience. We may therefore proceed with the derivation.\n\nThe objective is to derive the postsynaptic current $I(t)$ in a voltage-clamped neuron that receives a train of presynaptic spikes. The derivation will proceed from first principles, utilizing the concepts of linear time-invariant (LTI) systems and convolution.\n\nFirst, we represent the presynaptic spike train, arriving at discrete times $\\{t_k\\}_{k=1}^{N}$, as a signal $s(t)$. As stipulated, we model this train as a sum of Dirac delta impulses:\n$$\ns(t) = \\sum_{k=1}^{N} \\delta(t - t_k)\n$$\nwhere $\\delta(t)$ is the Dirac delta function. Each term $\\delta(t - t_k)$ represents a single spike occurring precisely at time $t_k$.\n\nSecond, we identify the system's impulse response. The problem states that a single presynaptic spike at time $t=0$, which is represented by $\\delta(t)$, evokes a causal alpha-function conductance transient $g_{\\alpha}(t)$. Therefore, $g_{\\alpha}(t)$ is, by definition, the impulse response function (or kernel) of the synapse.\n$$\ng_{\\alpha}(t) = g_0 \\frac{t}{\\tau} \\exp\\left(1 - \\frac{t}{\\tau}\\right) H(t)\n$$\nHere, $g_0$ is the peak conductance, $\\tau$ is the time constant, and $H(t)$ is the Heaviside step function, which ensures causality (i.e., the response is zero for $t  0$).\n\nThird, we compute the total synaptic conductance $g(t)$ resulting from the entire spike train $s(t)$. The problem states we should assume linearity and time-invariance. For a linear time-invariant (LTI) system, the output signal is the convolution of the input signal with the system's impulse response. Thus, the total conductance $g(t)$ is given by the convolution of $s(t)$ and $g_{\\alpha}(t)$:\n$$\ng(t) = (s * g_{\\alpha})(t) = \\int_{-\\infty}^{\\infty} s(t') g_{\\alpha}(t - t') dt'\n$$\nSubstituting the expression for the spike train $s(t')$:\n$$\ng(t) = \\int_{-\\infty}^{\\infty} \\left( \\sum_{k=1}^{N} \\delta(t' - t_k) \\right) g_{\\alpha}(t - t') dt'\n$$\nSince the summation is over a finite number of terms $N$, we can interchange the order of summation and integration:\n$$\ng(t) = \\sum_{k=1}^{N} \\int_{-\\infty}^{\\infty} \\delta(t' - t_k) g_{\\alpha}(t - t') dt'\n$$\nWe now apply the sifting property of the Dirac delta function, which states that $\\int_{-\\infty}^{\\infty} \\delta(x - a) f(x) dx = f(a)$. In our case, the variable of integration is $t'$, the delta function is centered at $t_k$, and the function being sifted is $g_{\\alpha}(t - t')$. Evaluating the integral for each term in the sum gives:\n$$\ng(t) = \\sum_{k=1}^{N} g_{\\alpha}(t - t_k)\n$$\nThis result embodies the principle of linear superposition: the total conductance is the sum of the individual conductance transients generated by each spike. Now, we substitute the functional form of $g_{\\alpha}$ with its argument replaced by $(t-t_k)$:\n$$\ng(t) = \\sum_{k=1}^{N} g_0 \\frac{t - t_k}{\\tau} \\exp\\left(1 - \\frac{t - t_k}{\\tau}\\right) H(t - t_k)\n$$\nThe Heaviside function $H(t - t_k)$ correctly enforces causality for each spike-evoked transient, ensuring it only contributes to the total conductance for times $t \\ge t_k$.\n\nFinally, we compute the postsynaptic current $I(t)$ under voltage clamp. The problem provides the relevant form of Ohm's law:\n$$\nI(t) = g(t) (V_{\\text{hold}} - E_{\\text{syn}})\n$$\nThe term $(V_{\\text{hold}} - E_{\\text{syn}})$ is the driving force, which is constant because the membrane potential is clamped at $V_{\\text{hold}}$ and the synaptic reversal potential $E_{\\text{syn}}$ is a fixed parameter. Substituting our derived expression for the total conductance $g(t)$ yields the final expression for the postsynaptic current:\n$$\nI(t) = \\left( \\sum_{k=1}^{N} g_0 \\frac{t - t_k}{\\tau} \\exp\\left(1 - \\frac{t - t_k}{\\tau}\\right) H(t - t_k) \\right) (V_{\\text{hold}} - E_{\\text{syn}})\n$$\nThis can be written more compactly by factoring out the constant terms:\n$$\nI(t) = g_0 (V_{\\text{hold}} - E_{\\text{syn}}) \\sum_{k=1}^{N} \\frac{t - t_k}{\\tau} \\exp\\left(1 - \\frac{t - t_k}{\\tau}\\right) H(t - t_k)\n$$\nThis is the single closed-form analytic expression for the total postsynaptic current $I(t)$ that was requested.",
            "answer": "$$\\boxed{g_0 (V_{\\text{hold}} - E_{\\text{syn}}) \\sum_{k=1}^{N} \\frac{t - t_k}{\\tau} \\exp\\left(1 - \\frac{t - t_k}{\\tau}\\right) H(t - t_k)}$$"
        },
        {
            "introduction": "The brain's computational power emerges from the collective activity of vast neural populations. In this practice, we scale up our focus from the single cell to a mesoscopic model of interacting excitatory and inhibitory neuron pools, the classic Wilson-Cowan model. You will use the tools of dynamical systems theory to find the model's fixed points and analyze their stability, uncovering how network interactions give rise to emergent phenomena like bistability and oscillations . This exercise provides a foundation for understanding how macroscopic brain states and rhythms can emerge from the underlying circuit structure.",
            "id": "4000907",
            "problem": "Consider a mesoscopic excitatory–inhibitory population model of cortical tissue given by the Wilson–Cowan system of ordinary differential equations (ODEs), obtained by coarse-graining spiking neuron dynamics into population activity fractions. Let $E(t)$ and $I(t)$ denote the fractions of active excitatory and inhibitory neurons, respectively, taking values in $[0,1]$. The dynamics are\n$$\n\\frac{dE}{dt} \\;=\\; -E \\;+\\; S_E\\!\\left(w_{EE} E \\;-\\; w_{EI} I \\;+\\; P_E\\right),\n\\qquad\n\\frac{dI}{dt} \\;=\\; -I \\;+\\; S_I\\!\\left(w_{IE} E \\;-\\; w_{II} I \\;+\\; P_I\\right),\n$$\nwhere $w_{EE}, w_{EI}, w_{IE}, w_{II}  0$ are effective coupling strengths between populations, $P_E$ and $P_I$ are constant external drives, and $S_E(\\cdot)$ and $S_I(\\cdot)$ are sigmoidal activation functions modeling the population response to net input. Assume the sigmoids are logistic functions\n$$\nS_E(x) \\;=\\; \\frac{1}{1 + \\exp\\!\\big(-a_E(x - \\theta_E)\\big)},\n\\qquad\nS_I(x) \\;=\\; \\frac{1}{1 + \\exp\\!\\big(-a_I(x - \\theta_I)\\big)},\n$$\nwith slopes $a_E0$, $a_I0$ and thresholds $\\theta_E$, $\\theta_I$. The time scale has been nondimensionalized so that coefficients in front of $E$ and $I$ are unity.\n\nTasks:\n- Starting from the definitions of fixed points and linearization for nonlinear ODEs, derive the fixed point conditions for $(E^{*}, I^{*})$ and the Jacobian matrix $J(E^{*}, I^{*})$ governing linear stability. Express the linear stability criteria in terms of the trace and determinant of $J$.\n- Using the general Hopf bifurcation conditions for a two-dimensional smooth dynamical system, express the condition for a Hopf bifurcation in terms of $(E^{*}, I^{*})$, the parameters $(w_{EE}, w_{EI}, w_{IE}, w_{II})$, and the slopes $a_E$ and $a_I$ evaluated at the fixed point via $S_E'(x)$ and $S_I'(x)$.\n- Explain, in terms of nullcline geometry and gain slope, how parameter regimes with sufficiently strong recurrent excitation and appropriately tuned inhibition can produce bistability (two distinct stable fixed points separated by an unstable one).\n- Now specialize to the parameter set\n$$\na_E \\;=\\; 1,\\quad a_I \\;=\\; 1,\\quad \\theta_E \\;=\\; 3,\\quad \\theta_I \\;=\\; 3,\\\\\nw_{EE} \\;=\\; 14,\\quad w_{EI} \\;=\\; 12,\\quad w_{IE} \\;=\\; 13,\\quad w_{II} \\;=\\; 6,\\\\\nP_I \\;=\\; -0.5,\n$$\nand consider $P_E$ as the bifurcation parameter. Determine the critical value $P_E^{H}$ at which a Hopf bifurcation occurs. Your derivation must proceed from the fixed point equations and the Jacobian-based conditions. Express $P_E^{H}$ as a single real number. Round your final numeric answer to four significant figures. No physical units are required since all quantities are nondimensional.",
            "solution": "The problem asks for an analysis of the Wilson-Cowan model, a system of two coupled ordinary differential equations (ODEs) describing the activity of excitatory ($E$) and inhibitory ($I$) neural populations.\n\nThe system is given by:\n$$\n\\frac{dE}{dt} = -E + S_E(w_{EE} E - w_{EI} I + P_E)\n$$\n$$\n\\frac{dI}{dt} = -I + S_I(w_{IE} E - w_{II} I + P_I)\n$$\nwhere $E, I \\in [0, 1]$. The activation functions $S_E$ and $S_I$ are sigmoidal logistic functions.\n\nFirst, the problem statement is validated.\n**Step 1: Extract Givens**\n- ODEs for $E(t)$ and $I(t)$:\n  $\\frac{dE}{dt} = -E + S_E(w_{EE} E - w_{EI} I + P_E)$\n  $\\frac{dI}{dt} = -I + S_I(w_{IE} E - w_{II} I + P_I)$\n- Activation functions:\n  $S_E(x) = \\frac{1}{1 + \\exp(-a_E(x - \\theta_E))}$\n  $S_I(x) = \\frac{1}{1 + \\exp(-a_I(x - \\theta_I))}$\n- Parameter constraints: $w_{EE}, w_{EI}, w_{IE}, w_{II}  0$; $a_E  0$, $a_I  0$.\n- Specific parameter values for the final task:\n  $a_E = 1$, $a_I = 1$, $\\theta_E = 3$, $\\theta_I = 3$\n  $w_{EE} = 14$, $w_{EI} = 12$, $w_{IE} = 13$, $w_{II} = 6$\n  $P_I = -0.5$\n- Bifurcation parameter: $P_E$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The Wilson-Cowan model is a foundational and widely studied model in theoretical and computational neuroscience for describing cortical population dynamics. The problem is scientifically sound.\n- **Well-Posed**: The problem asks for standard dynamical systems analysis: finding fixed points, linearization, stability criteria, bifurcation conditions, and calculating a specific bifurcation point for a given set of parameters. The tasks are clearly defined and lead to a unique answer.\n- **Objective**: The problem is stated in precise mathematical and scientific language, free of subjective claims.\n- The problem is self-contained and consistent, with no apparent flaws. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n**Task 1: Fixed Points and Linear Stability**\n\nA fixed point, denoted by $(E^*, I^*)$, is a state of the system where the dynamics cease, i.e., $\\frac{dE}{dt} = 0$ and $\\frac{dI}{dt} = 0$. Setting the time derivatives to zero yields the fixed point conditions:\n$$\nE^* = S_E(w_{EE} E^* - w_{EI} I^* + P_E)\n$$\n$$\nI^* = S_I(w_{IE} E^* - w_{II} I^* + P_I)\n$$\nThese are a pair of coupled transcendental equations. The solutions $(E^*, I^*)$ correspond to the intersections of the system's nullclines.\n\nTo analyze the linear stability of a fixed point, we linearize the system around $(E^*, I^*)$. Let the right-hand sides of the ODEs be $F_E(E, I)$ and $F_I(E, I)$. The dynamics of small perturbations $(\\delta E, \\delta I)$ around the fixed point are governed by the Jacobian matrix $J$ evaluated at $(E^*, I^*)$:\n$$\nJ(E^*, I^*) = \\begin{pmatrix} \\frac{\\partial F_E}{\\partial E}  \\frac{\\partial F_E}{\\partial I} \\\\ \\frac{\\partial F_I}{\\partial E}  \\frac{\\partial F_I}{\\partial I} \\end{pmatrix}_{(E^*, I^*)}\n$$\nThe partial derivatives are:\n$\\frac{\\partial F_E}{\\partial E} = -1 + w_{EE} S_E'(x_E)$, where $x_E = w_{EE} E - w_{EI} I + P_E$.\n$\\frac{\\partial F_E}{\\partial I} = -w_{EI} S_E'(x_E)$.\n$\\frac{\\partial F_I}{\\partial E} = w_{IE} S_I'(x_I)$, where $x_I = w_{IE} E - w_{II} I + P_I$.\n$\\frac{\\partial F_I}{\\partial I} = -1 - w_{II} S_I'(x_I)$.\n\nLet $S_{E,*}' = S_E'(w_{EE} E^* - w_{EI} I^* + P_E)$ and $S_{I,*}' = S_I'(w_{IE} E^* - w_{II} I^* + P_I)$. The Jacobian at the fixed point is:\n$$\nJ = \\begin{pmatrix} -1 + w_{EE} S_{E,*}'  -w_{EI} S_{E,*}' \\\\ w_{IE} S_{I,*}'  -1 - w_{II} S_{I,*}' \\end{pmatrix}\n$$\nThe stability of the fixed point is determined by the eigenvalues of $J$. For a two-dimensional system, a fixed point is stable if and only if both eigenvalues have negative real parts. According to the Routh-Hurwitz stability criterion, this is equivalent to two conditions on the trace ($\\text{Tr}(J)$) and determinant ($\\text{Det}(J)$) of the Jacobian:\n$1.$ $\\text{Tr}(J) = \\lambda_1 + \\lambda_2  0$\n$2.$ $\\text{Det}(J) = \\lambda_1 \\lambda_2  0$\n\nThe trace and determinant for this system are:\n$$\n\\text{Tr}(J) = (-1 + w_{EE} S_{E,*}') + (-1 - w_{II} S_{I,*}') = w_{EE} S_{E,*}' - w_{II} S_{I,*}' - 2\n$$\n$$\n\\text{Det}(J) = (-1 + w_{EE} S_{E,*}')(-1 - w_{II} S_{I,*}') - (-w_{EI} S_{E,*}')(w_{IE} S_{I,*}') = 1 - (w_{EE} S_{E,*}' - w_{II} S_{I,*}') + (w_{EI} w_{IE} - w_{EE} w_{II}) S_{E,*}' S_{I,*}'\n$$\n\n**Task 2: Hopf Bifurcation Condition**\n\nA Hopf bifurcation occurs when a fixed point loses stability as a pair of complex conjugate eigenvalues crosses the imaginary axis. For a two-dimensional system, this happens when:\n$1.$ $\\text{Tr}(J) = 0$\n$2.$ $\\text{Det}(J)  0$\n\nThe condition $\\text{Tr}(J) = 0$ gives:\n$$\nw_{EE} S_{E,*}' - w_{II} S_{I,*}' - 2 = 0 \\quad \\implies \\quad w_{EE} S_{E,*}' - w_{II} S_{I,*}' = 2\n$$\nThe derivative of the logistic function $S(x) = (1 + \\exp(-a(x-\\theta)))^{-1}$ is $S'(x) = a S(x) (1 - S(x))$. At a fixed point, $S_E(\\dots) = E^*$ and $S_I(\\dots) = I^*$, so we can write $S_{E,*}' = a_E E^*(1-E^*)$ and $S_{I,*}' = a_I I^*(1-I^*)$.\nSubstituting these into the trace condition, the Hopf bifurcation condition becomes an algebraic relation between the fixed-point coordinates $(E^*, I^*)$:\n$$\nw_{EE} a_E E^*(1-E^*) - w_{II} a_I I^*(1-I^*) = 2\n$$\n\n**Task 3: Explanation of Bistability**\n\nBistability refers to the co-existence of two stable fixed points for the same set of parameters. In the Wilson-Cowan model, this arises from the interplay of recurrent excitation and inhibition, which shapes the geometry of the system's nullclines.\nThe nullclines are the curves where $\\frac{dE}{dt}=0$ (E-nullcline) and $\\frac{dI}{dt}=0$ (I-nullcline). Fixed points are their intersections.\nThe E-nullcline is defined by $E = S_E(w_{EE} E - w_{EI} I + P_E)$. Due to the recurrent excitatory coupling $w_{EE}E$, this nullcline can have a non-monotonic, 'N'-shape when plotted as $I$ versus $E$. This shape emerges when the self-excitation is sufficiently strong, specifically when the \"gain\" of the recurrent loop, $w_{EE} S_E'$, exceeds $1$ for some value of $E$. High $w_{EE}$ and a steep sigmoid slope $a_E$ promote this condition.\nThe I-nullcline is given by $I = S_I(w_{IE} E - w_{II} I + P_I)$. Due to self-inhibition ($-w_{II}I$), this nullcline is typically a monotonic, increasing function of $E$.\nBistability occurs when the monotonic I-nullcline intersects the N-shaped E-nullcline at three points. The stability of these fixed points alternates along the E-nullcline. The two intersections on the outer, positive-slope branches of the 'N' are stable, while the middle intersection on the negative-slope branch is unstable (a saddle point). Thus, \"appropriately tuned inhibition\" means that the position and slope of the I-nullcline are such that it cuts through all three branches of the E-nullcline's 'N' shape.\n\n**Task 4: Calculation of the Hopf Bifurcation Point $P_E^H$**\n\nWe must find the value of the bifurcation parameter $P_E$, denoted $P_E^H$, for which the system undergoes a Hopf bifurcation. This requires simultaneously solving the two fixed-point equations and the Hopf condition $\\text{Tr}(J)=0$.\nThe given parameters are:\n$a_E = 1, a_I = 1, \\theta_E = 3, \\theta_I = 3$\n$w_{EE} = 14, w_{EI} = 12, w_{IE} = 13, w_{II} = 6$\n$P_I = -0.5$\n\nThe system of equations for the bifurcation point $(E^*, I^*, P_E^H)$ is:\n$1.$ (E-nullcline) $E^* = \\frac{1}{1 + \\exp(-(14E^* - 12I^* + P_E^H - 3))}$\n$2.$ (I-nullcline) $I^* = \\frac{1}{1 + \\exp(-(13E^* - 6I^* - 0.5 - 3))} = \\frac{1}{1 + \\exp(-(13E^* - 6I^* - 3.5))}$\n$3.$ (Hopf Condition) $14 \\cdot 1 \\cdot E^*(1-E^*) - 6 \\cdot 1 \\cdot I^*(1-I^*) = 2$\n\nWe first find the fixed point $(E^*, I^*)$ at the bifurcation by solving the I-nullcline and Hopf condition equations. The symmetry of the parameters ($a_E=a_I=1$, $\\theta_E=\\theta_I=3$) and the structure of the equations suggest testing a symmetric fixed point, such as $(E^*, I^*) = (0.5, 0.5)$.\n\nLet's check if $(E^*, I^*) = (0.5, 0.5)$ satisfies the equations:\n- **Hopf Condition**: $14 \\cdot 0.5(1-0.5) - 6 \\cdot 0.5(1-0.5) = 14(0.25) - 6(0.25) = 3.5 - 1.5 = 2$. The condition is satisfied.\n- **I-nullcline**: We check if $I^*=0.5$ for $E^*=0.5$. The input to the sigmoid is $x_I = 13(0.5) - 6(0.5) - 3.5 = 6.5 - 3.0 - 3.5 = 0$. The value of the sigmoid is $S_I(0) = \\frac{1}{1 + \\exp(-1(0 - 3))} = \\frac{1}{1+e^3} \\neq 0.5$.\nThere must be a mistake in the reasoning above. Let's re-invert the sigmoid to find the condition on its argument. For $I^*=0.5$, we need $S_I(x_I) = 0.5$. This occurs when the argument of the exponential is zero, i.e., $-a_I(x_I - \\theta_I) = 0$. Given $a_I > 0$, this means $x_I = \\theta_I$.\nSo, from the I-nullcline, we need $13E^* - 6I^* - 3.5 = 0$. No, the argument of the sigmoid is $x_I = w_{IE}E^* - w_{II}I^* + P_I$, and we need this to equal $\\theta_I$.\n$x_I = 13E^* - 6I^* - 0.5$. We need $x_I = \\theta_I = 3$.\nSo, $13E^* - 6I^* - 0.5 = 3 \\implies 13E^* - 6I^* = 3.5$.\n\nSimilarly, for the E-nullcline, if we assume $E^*=0.5$, then its input argument must be $\\theta_E=3$.\n$x_E = 14E^* - 12I^* + P_E^H = 3$.\n\nLet's retry solving for the fixed point at bifurcation. We have a system of two algebraic equations for $(E^*, I^*)$:\n(a) $14E^*(1-E^*) - 6I^*(1-I^*) = 2$ (from Hopf)\n(b) $13E^* - 6I^* = 3.5$ (from I-nullcline if $I^*=0.5$)\nThis line of reasoning is incorrect. The fixed point does not have to be at the center of the sigmoid. The previous derivation in the scratchpad was correct.\n\nLet's re-verify the previous correct derivation.\nThe system to solve is:\n1. $E^* = S_E(14E^* - 12I^* + P_E^H - 3)$\n2. $I^* = S_I(13E^* - 6I^* - 3.5)$\n3. $14E^*(1-E^*) - 6I^*(1-I^*) = 2$\n\nBy inspection, we test if $(E^*, I^*) = (0.5, 0.5)$ satisfies equations 2 and 3.\nFor eq 3: $14(0.5)(0.5) - 6(0.5)(0.5) = 3.5 - 1.5 = 2$. It satisfies eq 3.\nFor eq 2: The argument of the sigmoid is $13(0.5) - 6(0.5) - 3.5 = 6.5 - 3 - 3.5 = 0$.\nThe sigmoid becomes $S_I(0) = 1/(1 + \\exp(-1(0))) = 1/(1+1) = 0.5$. So $I^* = 0.5$ is satisfied.\nTherefore, the fixed point at the Hopf bifurcation is indeed $(E^*, I^*) = (0.5, 0.5)$.\n\nWe also check that $\\text{Det}(J)  0$.\n$S_{E,*}' = a_E E^*(1-E^*) = 1 \\cdot 0.5 \\cdot 0.5 = 0.25$.\n$S_{I,*}' = a_I I^*(1-I^*) = 1 \\cdot 0.5 \\cdot 0.5 = 0.25$.\n$\\text{Tr}(J) = 14(0.25) - 6(0.25) - 2 = 3.5 - 1.5 - 2 = 0$. (This is the Hopf condition we enforced).\n$\\text{Det}(J) = 1 - \\text{Tr}(J) - 2 + (w_{EI} w_{IE} - w_{EE} w_{II}) S_{E,*}' S_{I,*}'$\n$\\text{Det}(J) = 1 - (w_{EE} S'_{E,*} - w_{II} S'_{I,*}) + (w_{EI}w_{IE} - w_{EE}w_{II})S'_{E,*}S'_{I,*}$\n$\\text{Det}(J) = 1 - 2 + (12 \\cdot 13 - 14 \\cdot 6)(0.25)(0.25) = -1 + (156 - 84)(0.0625) = -1 + 72 \\cdot 0.0625 = -1 + 4.5 = 3.5$.\nSince $\\text{Det}(J) = 3.5 > 0$, the condition is fully met.\n\nFinally, we find $P_E^H$ from equation 1 using the fixed point $(0.5, 0.5)$:\n$0.5 = S_E(14(0.5) - 12(0.5) + P_E^H - 3)$\nFor the sigmoid $S_E$ to be $0.5$, its argument must be $0$.\n$14(0.5) - 12(0.5) + P_E^H - 3 = 0$\n$7 - 6 + P_E^H - 3 = 0$\n$1 + P_E^H - 3 = 0$\n$P_E^H - 2 = 0$\n$P_E^H = 2$.\n\nThe critical value is exactly 2. To four significant figures, this is 2.000.\nThe solution logic is sound and the calculations are correct.",
            "answer": "$$\n\\boxed{2.000}\n$$"
        }
    ]
}