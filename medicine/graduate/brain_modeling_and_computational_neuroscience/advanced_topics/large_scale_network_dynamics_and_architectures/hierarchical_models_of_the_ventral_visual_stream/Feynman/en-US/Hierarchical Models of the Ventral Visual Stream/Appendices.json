{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of hierarchical processing in the ventral visual stream is the systematic expansion of receptive field ($RF$) size at successively higher stages. This anatomical and functional organization allows the brain to build representations of complex objects from simple features. This exercise provides a hands-on opportunity to quantify this growth by calculating the receptive field size for a unit in a multi-layered model, helping to build a concrete intuition for how deeper layers integrate contextual information from progressively larger regions of the input space .",
            "id": "3988326",
            "problem": "A hierarchical model of the ventral visual stream can be idealized as a feedforward stack of locally connected stages, analogous to a Convolutional Neural Network (CNN), where each layer applies spatially localized linear filters followed by a downsampling specified by a stride. In such models, the concept of a receptive field formalizes how many input units (pixels) can influence a single unit at a deeper layer. Assume a three-layer feedforward model whose layers are strictly spatially local and translation equivariant, with square, isotropic filters of sizes $7 \\times 7$, $5 \\times 5$, and $3 \\times 3$ at layers $1$, $2$, and $3$ respectively, and strides $1$, $2$, and $2$ for layers $1$, $2$, and $3$ respectively. Assume unit dilation, no skip connections, and that padding (if present) does not change the count of distinct input pixels that can influence a unit at a deeper layer. Define the receptive field for a unit in layer $l$ as the number of distinct input pixels along one spatial axis (i.e., the linear extent in pixels) of the smallest contiguous input interval such that changes in those input pixels can affect the unit’s output, consistent with the standard convolutional support geometry.\n\nStarting from first principles—namely, the definitions of local connectivity and stride in hierarchical models—derive the recursive relationship that governs how receptive field extent grows through layers and use it to compute the linear receptive field size (in pixels along one axis) for a unit in layer $3$ of this model. Then provide a brief interpretation of what this receptive field size implies for the integration of contextual information in the ventral stream hierarchy, focusing on the trade-off between spatial resolution and context aggregation. Express the final numerical answer as the linear receptive field size along one axis, in pixels. No rounding is required.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in the principles of computational neuroscience and deep learning, specifically concerning hierarchical models of the visual cortex. The problem is well-posed, providing all necessary parameters and a clear objective. The language is precise and objective, and the setup is internally consistent and complete.\n\nThe primary task is to determine the linear receptive field size for a unit in the third layer of a simplified hierarchical model. To do this, we must first derive the general recursive relationship governing the growth of the receptive field size across layers.\n\nLet $RF_l$ denote the linear receptive field size (the extent along one axis) of a unit in layer $l$, measured in terms of pixels in the input layer (layer $0$).\nLet $k_l$ be the linear size of the filter (kernel) at layer $l$.\nLet $s_l$ be the stride of the convolution at layer $l$.\n\nThe receptive field of a unit in a given layer is the region of the input space that can influence its activation. We can establish a recursive formula for $RF_l$.\n\nFor the first layer ($l=1$), a unit's output is computed by convolving a filter of size $k_1$ directly with the input image. Therefore, the number of input pixels along one axis that can affect this unit is simply the filter size.\n$$RF_1 = k_1$$\n\nNow, consider a unit in layer $l$ (for $l > 1$). Its output is computed by applying a filter of size $k_l$ to the output feature map of layer $l-1$. This means the unit's activation depends on a contiguous block of $k_l$ units along one axis from layer $l-1$. The total receptive field of the layer-$l$ unit is the union of the receptive fields of these $k_l$ units from layer $l-1$.\n\nTo find the size of this union, we need to know how the receptive fields of adjacent units in the layer $l-1$ feature map are spaced in the original input space. This spacing, which we call the cumulative stride or jump, $J_{l-1}$, is the product of the strides of all preceding layers.\nThe jump $J_i$ is the distance in input pixels between the centers of the receptive fields of two adjacent units in the output of layer $i$.\nFor layer $1$, a stride of $s_1$ means adjacent units' receptive fields are centered $s_1$ pixels apart on the input. So, $J_1 = s_1$.\nFor layer $2$, a stride of $s_2$ operates on the feature map of layer $1$. A single step in the layer $2$ map corresponds to a step of size $s_1$ in the layer $1$ map. Thus, a single step in the layer $2$ output corresponds to a jump of $s_2 \\times J_1 = s_2 s_1$ pixels on the input.\nIn general, the cumulative stride up to layer $l-1$ is:\n$$J_{l-1} = \\prod_{i=1}^{l-1} s_i$$\n\nA unit in layer $l$ sees $k_l$ adjacent units from layer $l-1$. The receptive fields of these $k_l$ units each have a size of $RF_{l-1}$ on the input. The centers of these receptive fields are separated by $J_{l-1}$ pixels. The total extent is the size of the receptive field of the first unit in the block, plus the additional distance covered by the remaining $k_l-1$ units. This additional distance is $(k_l-1) \\times J_{l-1}$.\nTherefore, the recursive relationship is:\n$$RF_l = RF_{l-1} + (k_l - 1) \\times J_{l-1} = RF_{l-1} + (k_l - 1) \\prod_{i=1}^{l-1} s_i$$\nThis is the recursive relationship that governs how receptive field extent grows through the layers.\n\nNow, we apply this formula to the specific problem. The given parameters are:\nFilter sizes: $k_1 = 7$, $k_2 = 5$, $k_3 = 3$.\nStrides: $s_1 = 1$, $s_2 = 2$, $s_3 = 2$. Note that $s_3$ is not needed for calculating $RF_3$ but would be needed for $RF_4$.\n\nStep 1: Compute the receptive field size for a unit in layer $1$.\nUsing the base case:\n$$RF_1 = k_1 = 7$$\nA unit in layer $1$ has a receptive field size of $7$ pixels.\n\nStep 2: Compute the receptive field size for a unit in layer $2$.\nUsing the recursive formula for $l=2$:\n$$RF_2 = RF_1 + (k_2 - 1) \\prod_{i=1}^{1} s_i = RF_1 + (k_2 - 1) s_1$$\nSubstituting the values:\n$$RF_2 = 7 + (5 - 1) \\times 1 = 7 + 4 = 11$$\nA unit in layer $2$ has a receptive field size of $11$ pixels.\n\nStep 3: Compute the receptive field size for a unit in layer $3$.\nUsing the recursive formula for $l=3$:\n$$RF_3 = RF_2 + (k_3 - 1) \\prod_{i=1}^{2} s_i = RF_2 + (k_3 - 1) s_1 s_2$$\nSubstituting the values:\n$$RF_3 = 11 + (3 - 1) \\times (1 \\times 2) = 11 + 2 \\times 2 = 11 + 4 = 15$$\nThe linear receptive field size for a unit in layer $3$ is $15$ pixels.\n\nInterpretation:\nThe calculated receptive field size of $15 \\times 15$ pixels for a unit in layer $3$ encapsulates a core principle of hierarchical processing in the ventral visual stream. There is a fundamental trade-off between the aggregation of contextual information and the preservation of fine-grained spatial resolution.\n\nContext Aggregation: The receptive field size increases with each layer, from $7 \\times 7$ at layer $1$ to $11 \\times 11$ at layer $2$, and finally to $15 \\times 15$ at layer $3$. This growth demonstrates that higher-level units integrate information over progressively larger areas of the input. This allows the model to build representations of increasingly complex and abstract features. A layer-$3$ unit can respond to a large-scale pattern or object part by combining simpler features (e.g., edges, textures) detected by lower-level units across its $15 \\times 15$ input window.\n\nLoss of Spatial Resolution: This aggregation of context is enabled by downsampling, implemented here by strides greater than one ($s_2=2, s_3=2$). While a layer-$3$ unit is sensitive to stimuli within a large $15 \\times 15$ area, it has poor acuity regarding the precise location of features within that area. A feature could shift several pixels within the $15 \\times 15$ field without changing the identity of which layer-$3$ unit is activated. This creates positional invariance.\n\nThis trade-off is central to the function of the ventral (\"what\") pathway, which is primarily concerned with object recognition. The system systematically sacrifices spatial precision (\"where\") to build representations that are selective for object identity (\"what\") and tolerant to variations in position, scale, and pose. The hierarchical increase in receptive field size is the key mechanism for achieving this context-dependent, yet spatially-invariant, representation.",
            "answer": "$$\\boxed{15}$$"
        },
        {
            "introduction": "While receptive fields grow larger through the hierarchy, the computations within them are designed to build tolerance to nuisance variability, such as changes in an object's precise position. The classic simple-to-complex cell transformation, first described by Hubel and Wiesel, provides the canonical model for this process. In this practice, you will simulate this fundamental operation by computing the responses of model simple ($S1$) and complex ($C1$) cells to an edge stimulus, thereby demonstrating how local pooling creates the crucial property of translation invariance .",
            "id": "3988315",
            "problem": "Consider a canonical feedforward hierarchical model of the ventral visual stream in which the Simple layer (S1) implements linear filtering akin to classical simple cells and the Complex layer (C1) implements local pooling akin to classical complex cells. Assume the following foundational operations: linear filtering by convolution for S1 and local maximum pooling for C1.\n\nYou are given a grayscale image stimulus $I \\in \\mathbb{R}^{4 \\times 4}$ containing a sharp vertical step edge, with pixel intensities\n$$\nI \\;=\\;\n\\begin{pmatrix}\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 1\n\\end{pmatrix}.\n$$\nThe S1 stage is equipped with a single orientation-selective filter approximating a vertically oriented edge detector,\n$$\nF^{(v)} \\;=\\;\n\\begin{pmatrix}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1\n\\end{pmatrix}.\n$$\nDefine the S1 response at a valid location $(i,j)$ (where the $3 \\times 3$ filter fits inside the image) by rectified linear filtering\n$$\ns_{i,j} \\;=\\; \\max\\!\\left(0,\\;\\sum_{m=1}^{3}\\sum_{n=1}^{3} F^{(v)}_{m,n}\\,P_{i,j}(m,n)\\right),\n$$\nwhere $P_{i,j} \\in \\mathbb{R}^{3 \\times 3}$ is the image patch centered at $(i,j)$ (i.e., $P_{i,j}(m,n) = I(i+m-2,\\, j+n-2)$ using $1$-based indexing). Let the valid S1 centers be the set $\\{(2,2),(2,3),(3,2),(3,3)\\}$.\n\nDefine a single C1 unit that pools locally by the maximum over the window\n$$\nW \\;=\\; \\{(i,j) \\;\\big|\\; i \\in \\{2,3\\},\\; j \\in \\{2,3\\}\\}.\n$$\nThus, the C1 response is\n$$\nC1 \\;=\\; \\max_{(i,j)\\in W} s_{i,j}.\n$$\n\nTask: Compute the numerical value of $C1$ for the given stimulus and filter, and then interpret, in your derivation, how local maximum pooling in $W$ supports invariance to small stimulus translations relative to the filter. Express the final answer as a single real number. No rounding is required. No units are required.",
            "solution": "The problem requires the computation of a Complex layer (C1) response in a simple hierarchical model of the ventral visual stream. This involves two stages: first, computing the responses of Simple layer (S1) units via rectified linear filtering, and second, computing the C1 response by taking the maximum of the S1 responses over a specified pooling window.\n\nThe input is a $4 \\times 4$ grayscale image $I$ and a $3 \\times 3$ vertically oriented filter $F^{(v)}$:\n$$\nI \\;=\\;\n\\begin{pmatrix}\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 1\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nF^{(v)} \\;=\\;\n\\begin{pmatrix}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1\n\\end{pmatrix}.\n$$\nThe S1 response at a location $(i,j)$ is given by $s_{i,j} = \\max(0, \\text{linear response})$, where the linear response is the sum of element-wise products of the filter $F^{(v)}$ and the image patch $P_{i,j}$ centered at $(i,j)$. We must compute $s_{i,j}$ for all valid centers $(i,j) \\in \\{(2,2), (2,3), (3,2), (3,3)\\}$.\n\n1.  **Compute S1 response at $(i,j) = (2,2)$:**\n    The image patch $P_{2,2}$ is the $3 \\times 3$ region of $I$ centered at $(2,2)$, which spans rows $1$ to $3$ and columns $1$ to $3$.\n    $$\n    P_{2,2} \\;=\\;\n    \\begin{pmatrix}\n    0 & 0 & 1 \\\\\n    0 & 0 & 1 \\\\\n    0 & 0 & 1\n    \\end{pmatrix}.\n    $$\n    The linear response is the dot product of the flattened filter and patch, or $\\sum_{m,n} F^{(v)}_{m,n} P_{2,2}(m,n)$.\n    $$\n    \\text{Linear Response}_{(2,2)} = (-1)(0) + (0)(0) + (1)(1) + (-1)(0) + (0)(0) + (1)(1) + (-1)(0) + (0)(0) + (1)(1) = 1+1+1 = 3.\n    $$\n    The rectified S1 response is:\n    $$\n    s_{2,2} = \\max(0, 3) = 3.\n    $$\n\n2.  **Compute S1 response at $(i,j) = (2,3)$:**\n    The image patch $P_{2,3}$ is centered at $(2,3)$, spanning rows $1$ to $3$ and columns $2$ to $4$.\n    $$\n    P_{2,3} \\;=\\;\n    \\begin{pmatrix}\n    0 & 1 & 1 \\\\\n    0 & 1 & 1 \\\\\n    0 & 1 & 1\n    \\end{pmatrix}.\n    $$\n    The linear response is:\n    $$\n    \\text{Linear Response}_{(2,3)} = (-1)(0) + (0)(1) + (1)(1) + (-1)(0) + (0)(1) + (1)(1) + (-1)(0) + (0)(1) + (1)(1) = 1+1+1 = 3.\n    $$\n    The rectified S1 response is:\n    $$\n    s_{2,3} = \\max(0, 3) = 3.\n    $$\n\n3.  **Compute S1 response at $(i,j) = (3,2)$:**\n    The image patch $P_{3,2}$ is centered at $(3,2)$, spanning rows $2$ to $4$ and columns $1$ to $3$. Due to the vertical uniformity of the input image $I$, this patch is identical to $P_{2,2}$.\n    $$\n    P_{3,2} \\;=\\; P_{2,2} \\;=\\;\n    \\begin{pmatrix}\n    0 & 0 & 1 \\\\\n    0 & 0 & 1 \\\\\n    0 & 0 & 1\n    \\end{pmatrix}.\n    $$\n    Therefore, the response is the same as for $(2,2)$.\n    $$\n    s_{3,2} = s_{2,2} = 3.\n    $$\n\n4.  **Compute S1 response at $(i,j) = (3,3)$:**\n    The image patch $P_{3,3}$ is centered at $(3,3)$, spanning rows $2$ to $4$ and columns $2$ to $4$. Due to the vertical uniformity of the input image $I$, this patch is identical to $P_{2,3}$.\n    $$\n    P_{3,3} \\;=\\; P_{2,3} \\;=\\;\n    \\begin{pmatrix}\n    0 & 1 & 1 \\\\\n    0 & 1 & 1 \\\\\n    0 & 1 & 1\n    \\end{pmatrix}.\n    $$\n    Therefore, the response is the same as for $(2,3)$.\n    $$\n    s_{3,3} = s_{2,3} = 3.\n    $$\n\nThe set of S1 responses within the pooling window $W = \\{(2,2), (2,3), (3,2), (3,3)\\}$ is $\\{s_{2,2}, s_{2,3}, s_{3,2}, s_{3,3}\\} = \\{3, 3, 3, 3\\}$. The filter $F^{(v)}$ robustly detects the vertical edge feature at all four positions within the $2 \\times 2$ grid of S1 units.\n\nNow, we compute the C1 response by taking the maximum over these S1 responses:\n$$\nC1 \\;=\\; \\max_{(i,j)\\in W} s_{i,j} \\;=\\; \\max(s_{2,2}, s_{2,3}, s_{3,2}, s_{3,3}) = \\max(3, 3, 3, 3) = 3.\n$$\n\nThe final numerical value of the C1 unit's response is $3$.\n\nThe derivation illustrates how local maximum pooling supports translation invariance. The S1 layer acts as a layer of feature detectors; in this case, a high $s_{i,j}$ value signals the presence of a vertical edge at location $(i,j)$. The C1 layer pools the outputs of these detectors. By computing the maximum, the C1 unit's response is determined by the strongest feature signal within its pooling region, effectively ignoring which specific S1 detector produced that signal. This makes the C1 response robust to the precise location of the feature.\n\nTo make this concrete, consider what would happen if the stimulus edge shifted one pixel to the left, yielding a new image $I'$:\n$$\nI' \\;=\\;\n\\begin{pmatrix}\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 1 & 0\n\\end{pmatrix}.\n$$\nRepeating the S1 calculation for this shifted stimulus $I'$ over the same window $W$ would yield a different set of S1 responses: $\\{s'_{2,2}, s'_{2,3}, s'_{3,2}, s'_{3,3}\\} = \\{3, 0, 3, 0\\}$. The pattern of activity in the S1 layer has shifted. However, the C1 response would be:\n$$\nC1' \\;=\\; \\max(3, 0, 3, 0) = 3.\n$$\nThe output of the C1 unit remains unchanged despite the translation of the stimulus. This demonstrates that the maximum pooling operation provides tolerance to small shifts in feature position, a foundational property for robust object recognition known as translation invariance.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "Ultimately, the goal of the ventral visual stream is not just to represent features, but to support decisions and guide behavior. A powerful way to understand this function is through normative frameworks like Bayesian Decision Theory, which describes how an ideal observer should make choices to maximize rewards. This problem challenges you to apply these principles to a model of ventral representation, deriving how an organism should adjust its decisions when the statistical regularities of its environment change, and exploring the consequences of a neural representation that has learned a specific prior bias .",
            "id": "3988304",
            "problem": "Consider a hierarchical model of the ventral visual stream in which a binary category variable $C \\in \\{0,1\\}$ generates low-level features that are summarized by a scalar ventral representation. The model has two levels: a generative level and a representational level. At the generative level, the low-level feature $x \\in \\mathbb{R}$ is drawn from class-conditional Gaussian distributions $x \\mid C=c \\sim \\mathcal{N}(\\mu_c,\\sigma^2)$ with known parameters $\\mu_0,\\mu_1 \\in \\mathbb{R}$ and $\\sigma > 0$. At the representational level, the ventral representation encodes a decision variable that is an affine function of the log-likelihood ratio, potentially including a bias learned from the dataset prior.\n\nUse Bayesian Decision Theory (BDT) and Signal Detection Theory (SDT) as the fundamental base. Specifically, define the Log-Likelihood Ratio (LLR) as $\\mathrm{LLR}(x) = \\log p(x \\mid C=1) - \\log p(x \\mid C=0)$, and the training dataset prior as $\\pi_{\\mathrm{train}} = p_{\\mathrm{train}}(C=1)$, with prior odds encoded as $\\beta_{\\mathrm{train}} = \\log\\left(\\frac{\\pi_{\\mathrm{train}}}{1-\\pi_{\\mathrm{train}}}\\right)$. Assume the ventral representation implements $r(x) = \\mathrm{LLR}(x) + \\beta_{\\mathrm{train}}$, reflecting a learned prior bias from the dataset statistics. At test time, the environment prior may shift to $\\pi_{\\mathrm{test}} = p_{\\mathrm{test}}(C=1)$ with corresponding odds $\\beta_{\\mathrm{test}} = \\log\\left(\\frac{\\pi_{\\mathrm{test}}}{1-\\pi_{\\mathrm{test}}}\\right)$. Under a minimum Bayes risk rule with equal misclassification costs, there exists an optimal decision threshold for the test environment that depends on these quantities.\n\nYour task is to:\n- Derive from first principles the optimal test-time threshold on the ventral representation that implements the Bayes classifier under a shifted prior, starting from definitions of $\\mathrm{LLR}(x)$ and the Bayes decision rule.\n- Quantify the threshold shift $\\Delta \\tau$ induced by the prior change, defined as the difference between the optimal test-time threshold and the optimal training-time threshold.\n- Quantify the amplitude of the learned prior bias $B$ in the ventral representation, defined as the additive constant contributed by the training prior.\n\nYour program must implement these computations using the definitions above, without any user input, and produce numerical answers for the following test suite of parameter values:\n- Case $1$: $\\pi_{\\mathrm{train}} = 0.5$, $\\pi_{\\mathrm{test}} = 0.5$, $\\mu_0 = 0$, $\\mu_1 = 1$, $\\sigma = 1$.\n- Case $2$: $\\pi_{\\mathrm{train}} = 0.8$, $\\pi_{\\mathrm{test}} = 0.5$, $\\mu_0 = 0$, $\\mu_1 = 1$, $\\sigma = 1$.\n- Case $3$: $\\pi_{\\mathrm{train}} = 0.7$, $\\pi_{\\mathrm{test}} = 0.99$, $\\mu_0 = 0$, $\\mu_1 = 1$, $\\sigma = 1$.\n- Case $4$: $\\pi_{\\mathrm{train}} = 0.5$, $\\pi_{\\mathrm{test}} = 0.1$, $\\mu_0 = 0$, $\\mu_1 = 0.2$, $\\sigma = 1$.\n- Case $5$: $\\pi_{\\mathrm{train}} = 0.99$, $\\pi_{\\mathrm{test}} = 0.01$, $\\mu_0 = 0$, $\\mu_1 = 2$, $\\sigma = 1$.\n\nAll logarithms must be natural logarithms. No physical units are involved. Angles are not used. For each case $i$, compute two floats:\n- The threshold shift $\\Delta \\tau_i$ induced by the prior change.\n- The learned bias amplitude $B_i$ in the representation.\n\nFinal output format: Your program should produce a single line of output containing a flat list of results, ordered as $[\\Delta \\tau_1,B_1,\\Delta \\tau_2,B_2,\\Delta \\tau_3,B_3,\\Delta \\tau_4,B_4,\\Delta \\tau_5,B_5]$, printed as a comma-separated list enclosed in square brackets.",
            "solution": "The problem requires the derivation and computation of two quantities related to a Bayesian decision model of the ventral visual stream: the shift in the optimal decision threshold, $\\Delta\\tau$, due to a change in prior probabilities from a training to a test environment, and the amplitude of the learned prior bias, $B$, embedded in the neural representation. The derivation is based on first principles of Bayesian Decision Theory (BDT) and Signal Detection Theory (SDT).\n\n**1. The Bayes Optimal Decision Rule**\n\nAccording to BDT, for a binary classification problem with categories $C \\in \\{0, 1\\}$ and equal misclassification costs, the decision rule that minimizes the Bayes risk is to choose the category with the higher posterior probability. Given an observation $x$, we decide $C=1$ if $p(C=1 \\mid x) > p(C=0 \\mid x)$.\n\nUsing Bayes' theorem, $p(C=c \\mid x) = \\frac{p(x \\mid C=c)p(C=c)}{p(x)}$, this rule can be expressed in terms of likelihoods $p(x \\mid C=c)$ and priors $p(C=c)$:\n$$p(x \\mid C=1)p(C=1) > p(x \\mid C=0)p(C=0)$$\nDividing both sides by $p(x \\mid C=0)p(C=1)$ (assuming they are non-zero) yields the likelihood ratio test:\n$$\\frac{p(x \\mid C=1)}{p(x \\mid C=0)} > \\frac{p(C=0)}{p(C=1)}$$\nTaking the natural logarithm of both sides gives the rule in terms of the Log-Likelihood Ratio (LLR):\n$$\\log\\left(\\frac{p(x \\mid C=1)}{p(x \\mid C=0)}\\right) > \\log\\left(\\frac{p(C=0)}{p(C=1)}\\right)$$\nLet the prior probability of class $1$ be $\\pi = p(C=1)$, so $p(C=0) = 1-\\pi$. The LLR is defined as $\\mathrm{LLR}(x) = \\log p(x \\mid C=1) - \\log p(x \\mid C=0)$. The decision rule becomes:\n$$\\mathrm{LLR}(x) > \\log\\left(\\frac{1-\\pi}{\\pi}\\right) = -\\log\\left(\\frac{\\pi}{1-\\pi}\\right)$$\nWe define the log-prior-odds as $\\beta = \\log\\left(\\frac{\\pi}{1-\\pi}\\right)$. The Bayes optimal decision rule is to choose $C=1$ if and only if:\n$$\\mathrm{LLR}(x) > -\\beta$$\n\n**2. Optimal Threshold on the Ventral Representation**\n\nThe problem states that the ventral representation is $r(x) = \\mathrm{LLR}(x) + \\beta_{\\mathrm{train}}$, where $\\beta_{\\mathrm{train}} = \\log\\left(\\frac{\\pi_{\\mathrm{train}}}{1-\\pi_{\\mathrm{train}}}\\right)$ is the log-odds corresponding to the training set prior $\\pi_{\\mathrm{train}}$. This representation incorporates the statistics of the training environment.\n\nWe need to find the optimal decision threshold on this representation, $r(x)$, when operating in a new test environment with a potentially different prior, $\\pi_{\\mathrm{test}}$. The corresponding log-odds for the test environment is $\\beta_{\\mathrm{test}} = \\log\\left(\\frac{\\pi_{\\mathrm{test}}}{1-\\pi_{\\mathrm{test}}}\\right)$.\n\nThe optimal decision rule for the test environment is $\\mathrm{LLR}(x) > -\\beta_{\\mathrm{test}}$. To apply this rule using the available representation $r(x)$, we must express $\\mathrm{LLR}(x)$ in terms of $r(x)$. From the definition of $r(x)$, we have $\\mathrm{LLR}(x) = r(x) - \\beta_{\\mathrm{train}}$.\n\nSubstituting this into the optimal test-time rule:\n$$r(x) - \\beta_{\\mathrm{train}} > -\\beta_{\\mathrm{test}}$$\n$$r(x) > \\beta_{\\mathrm{train}} - \\beta_{\\mathrm{test}}$$\nTherefore, the optimal decision threshold on the ventral representation $r(x)$ for the test environment is:\n$$\\tau_{\\mathrm{test}} = \\beta_{\\mathrm{train}} - \\beta_{\\mathrm{test}}$$\n\n**3. Quantifying the Threshold Shift, $\\Delta\\tau$**\n\nThe threshold shift is defined as $\\Delta\\tau = \\tau_{\\mathrm{test}} - \\tau_{\\mathrm{train}}$. We have derived $\\tau_{\\mathrm{test}}$. We now derive the optimal threshold for the training environment, $\\tau_{\\mathrm{train}}$.\n\nFor the training environment, the prior is $\\pi_{\\mathrm{train}}$ and the log-odds are $\\beta_{\\mathrm{train}}$. The optimal decision rule is $\\mathrm{LLR}(x) > -\\beta_{\\mathrm{train}}$. Expressing this in terms of $r(x)$:\n$$r(x) - \\beta_{\\mathrm{train}} > -\\beta_{\\mathrm{train}}$$\n$$r(x) > 0$$\nThus, the optimal threshold on $r(x)$ for the training environment is $\\tau_{\\mathrm{train}} = 0$. This is intuitive, as the representation $r(x) = \\mathrm{LLR}(x) + \\beta_{\\mathrm{train}}$ represents the log-posterior-odds under the training prior, for which the natural decision boundary is $0$.\n\nThe threshold shift is then:\n$$\\Delta\\tau = \\tau_{\\mathrm{test}} - \\tau_{\\mathrm{train}} = (\\beta_{\\mathrm{train}} - \\beta_{\\mathrm{test}}) - 0$$\n$$\\Delta\\tau = \\beta_{\\mathrm{train}} - \\beta_{\\mathrm{test}}$$\nSubstituting the definitions of the log-odds:\n$$\\Delta\\tau = \\log\\left(\\frac{\\pi_{\\mathrm{train}}}{1-\\pi_{\\mathrm{train}}}\\right) - \\log\\left(\\frac{\\pi_{\\mathrm{test}}}{1-\\pi_{\\mathrm{test}}}\\right)$$\nNote that this expression depends only on the training and test priors, not on the parameters of the class-conditional distributions ($\\mu_0$, $\\mu_1$, $\\sigma$).\n\n**4. Quantifying the Learned Prior Bias, $B$**\n\nThe problem defines the learned prior bias $B$ as \"the additive constant contributed by the training prior\" in the ventral representation $r(x) = \\mathrm{LLR}(x) + \\beta_{\\mathrm{train}}$. The term $\\mathrm{LLR}(x)$ contains the sensory evidence, while $\\beta_{\\mathrm{train}}$ is the additive constant reflecting the training prior.\n\nBy this definition, the learned bias amplitude is simply:\n$$B = \\beta_{\\mathrm{train}} = \\log\\left(\\frac{\\pi_{\\mathrm{train}}}{1-\\pi_{\\mathrm{train}}}\\right)$$\n\n**5. Computational Formulas**\n\nFor each test case $i$ characterized by $(\\pi_{\\mathrm{train}, i}, \\pi_{\\mathrm{test}, i})$, we compute:\n- The learned bias amplitude: $B_i = \\log\\left(\\frac{\\pi_{\\mathrm{train}, i}}{1-\\pi_{\\mathrm{train}, i}}\\right)$\n- The threshold shift: $\\Delta\\tau_i = B_i - \\log\\left(\\frac{\\pi_{\\mathrm{test}, i}}{1-\\pi_{\\mathrm{test}, i}}\\right)$\n\nThese formulas will be implemented to solve for the given parameter sets.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the threshold shift (Delta_tau) and learned bias amplitude (B)\n    for a hierarchical model of the ventral visual stream under changing priors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (pi_train, pi_test, mu_0, mu_1, sigma).\n    # The parameters mu_0, mu_1, and sigma are not needed for computing\n    # Delta_tau and B, as these quantities only depend on the priors.\n    test_cases = [\n        (0.5, 0.5, 0, 1, 1),    # Case 1\n        (0.8, 0.5, 0, 1, 1),    # Case 2\n        (0.7, 0.99, 0, 1, 1),   # Case 3\n        (0.5, 0.1, 0, 0.2, 1),  # Case 4\n        (0.99, 0.01, 0, 2, 1),  # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        pi_train, pi_test, _, _, _ = case\n\n        # The log-prior-odds is defined as log(pi / (1 - pi)).\n        # It is well-defined for all given priors as they are in (0, 1).\n\n        # Calculate the log-prior-odds for the training environment.\n        beta_train = np.log(pi_train / (1.0 - pi_train))\n\n        # Calculate the log-prior-odds for the test environment.\n        beta_test = np.log(pi_test / (1.0 - pi_test))\n\n        # 1. Quantify the learned prior bias amplitude, B.\n        # B is the additive constant in the ventral representation r(x)\n        # contributed by the training prior, which is beta_train.\n        B = beta_train\n        \n        # 2. Quantify the threshold shift, Delta_tau.\n        # Delta_tau is the difference between the optimal test-time threshold\n        # and the optimal training-time threshold.\n        # tau_test = beta_train - beta_test\n        # tau_train = 0\n        # Delta_tau = tau_test - tau_train = beta_train - beta_test.\n        delta_tau = beta_train - beta_test\n\n        results.extend([delta_tau, B])\n\n    # Final print statement in the exact required format.\n    # The format is a flat list: [Δτ₁,B₁,Δτ₂,B₂,Δτ₃,B₃,Δτ₄,B₄,Δτ₅,B₅]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}