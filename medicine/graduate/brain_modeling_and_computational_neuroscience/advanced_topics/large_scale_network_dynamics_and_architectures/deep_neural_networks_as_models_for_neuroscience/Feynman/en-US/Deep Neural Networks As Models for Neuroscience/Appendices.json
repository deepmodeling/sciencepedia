{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of systems neuroscience is the observation that neurons in the visual cortex have receptive fields that grow in size and complexity along the ventral stream. Deep Convolutional Neural Networks (CNNs) offer a powerful computational framework for understanding this hierarchical processing. This practice  provides a first-principles exercise in calculating the effective receptive field of a neuron in a multi-layered network, allowing you to directly connect architectural parameters to a fundamental neurophysiological measure and critically evaluate the model's biological plausibility.",
            "id": "3974364",
            "problem": "A Convolutional Neural Network (CNN) is used as a model of hierarchical visual processing. Consider a feedforward stack of three two-dimensional convolutional layers with isotropic square kernels. The kernel sizes and strides are $(k_1=7,s_1=2)$ for the first layer, $(k_2=3,s_2=2)$ for the second layer, and $(k_3=3,s_3=2)$ for the third layer. Assume no dilation and that zero-padding, if present, does not alter the interior receptive field extent (i.e., padding does not increase the number of real input pixels that can influence a unit).\n\nThe input image spans a retinotopic visual field of $P=256$ pixels corresponding to $20$ degrees of visual angle, uniformly sampled, so each input pixel subtends the same visual angle. Treat the receptive field extent along a single spatial dimension (e.g., the horizontal axis) as the quantity of interest; under isotropy, the two-dimensional receptive field has equal extents along both axes.\n\nStarting from the definition of discrete convolution as a local linear operator that, at each output location, linearly combines $k$ adjacent inputs with stride $s$, use first principles to derive the effective receptive field size, in input pixels, of a single unit in the third layer. Then convert this receptive field size from pixels to degrees of visual angle using the given mapping.\n\nExpress the final answer as the visual angle covered by a unit in layer $3$, in degrees. Provide the exact value without rounding.\n\nFinally, discuss in your reasoning whether the computed visual angle is plausibly aligned with receptive field diameters reported in visual cortical area V4 (V4), which are often on the order of $5$ degrees at mid-eccentricities, and justify your conclusion from the computation and assumptions.",
            "solution": "The problem requires the calculation of the effective receptive field size of a unit in the third layer of a three-layer Convolutional Neural Network (CNN), followed by a comparison to neurophysiological data from visual area V4.\n\nFirst, we must establish the mathematical principle for the growth of the receptive field (RF) size through a hierarchy of convolutional layers. Let $R_i$ be the effective receptive field size (along one dimension) for a unit in layer $i$, measured in pixels of the input layer (layer $0$). Let $k_i$ and $s_i$ be the kernel size and stride of layer $i$, respectively. The input itself can be considered layer $0$, where each unit (pixel) has a receptive field of size $R_0 = 1$.\n\nA unit in layer $1$ is computed via a convolution with kernel size $k_1$ over the input layer $0$. Therefore, its receptive field is simply the size of the kernel:\n$$R_1 = k_1$$\n\nNow, consider a unit in layer $2$. It is computed from a neighborhood of $k_2$ units in the output of layer $1$. These $k_2$ units in layer $1$ are adjacent in their own grid. Due to the stride $s_1$ of the first layer, any two adjacent units in layer $1$ have receptive fields centered $s_1$ pixels apart in the input layer $0$.\nThe total spatial extent covered by these $k_2$ units from layer $1$ is determined by the span of their centers plus the size of the individual receptive fields. The centers of the $k_2$ receptive fields from layer $1$ span a distance of $(k_2-1)s_1$ pixels in the input layer. To find the total receptive field size for the layer $2$ unit, we add the receptive field size of a single layer $1$ unit, $R_1$, to this span. This accounts for the full extent from the beginning of the first unit's RF to the end of the last unit's RF. Thus, the recursive relationship for the receptive field size is:\n$$R_2 = R_1 + (k_2 - 1)s_1$$\nGeneralizing this, the effective receptive field size $R_i$ for a unit in layer $i$ is given by its size relative to layer $i-1$, which is $R_{i-1}$, plus the additional extent covered by the $k_i$ kernels whose centers are separated by the cumulative stride from all previous layers. The stride between adjacent units of layer $i-1$ projected back to the input layer is the product of all preceding strides, $\\prod_{j=1}^{i-1} s_j$. The recursive formula is:\n$$R_i = R_{i-1} + (k_i - 1) \\prod_{j=1}^{i-1} s_j$$\n\nWe are given the following parameters for a three-layer network:\n- Layer 1: $k_1 = 7$, $s_1 = 2$\n- Layer 2: $k_2 = 3$, $s_2 = 2$\n- Layer 3: $k_3 = 3$, $s_3 = 2$\n\nWe can now compute the receptive field size step-by-step.\nThe base case is a single pixel in the input layer:\n$$R_0 = 1$$\nFor a unit in layer $1$:\n$$R_1 = R_0 + (k_1 - 1) \\prod_{j=1}^{0} s_j = R_0 + (k_1 - 1) \\times 1 = 1 + (7 - 1) = 7 \\text{ pixels}$$\nThis is consistent, as the first layer's kernel directly views $7$ input pixels.\n\nFor a unit in layer $2$:\n$$R_2 = R_1 + (k_2 - 1) \\prod_{j=1}^{1} s_j = R_1 + (k_2 - 1)s_1$$\n$$R_2 = 7 + (3 - 1) \\times 2 = 7 + 2 \\times 2 = 11 \\text{ pixels}$$\n\nFor a unit in layer $3$:\n$$R_3 = R_2 + (k_3 - 1) \\prod_{j=1}^{2} s_j = R_2 + (k_3 - 1)s_1 s_2$$\n$$R_3 = 11 + (3 - 1) \\times (2 \\times 2) = 11 + 2 \\times 4 = 11 + 8 = 19 \\text{ pixels}$$\nSo, the effective receptive field of a single unit in the third layer spans $19$ pixels of the input image.\n\nNext, we convert this size from pixels to degrees of visual angle. The input image has a size of $P = 256$ pixels and corresponds to a visual field of $20$ degrees. The sampling is uniform, so the conversion factor is constant across the visual field.\nThe visual angle per pixel is:\n$$\\frac{20 \\text{ degrees}}{256 \\text{ pixels}}$$\nThe receptive field size in degrees, which we denote $\\theta_{RF}$, is the size in pixels multiplied by this conversion factor:\n$$\\theta_{RF} = R_3 \\times \\frac{20}{256} = 19 \\times \\frac{20}{256} = \\frac{380}{256}$$\nTo express this as an exact fraction in simplest form, we find the greatest common divisor of the numerator and denominator. Both are divisible by $4$:\n$$380 \\div 4 = 95$$\n$$256 \\div 4 = 64$$\nSo, the exact visual angle is:\n$$\\theta_{RF} = \\frac{95}{64} \\text{ degrees}$$\n\nFinally, the problem asks for a discussion on whether this value is plausibly aligned with receptive field diameters in visual cortical area V4, which are cited to be on the order of $5$ degrees.\nThe calculated value is $\\frac{95}{64} \\approx 1.48$ degrees. This value is substantially smaller than the typical $5$ degrees reported for V4 receptive fields at mid-eccentricities.\n\nBased on this computation, the specified three-layer CNN is not a quantitatively accurate model for achieving V4-like receptive field sizes. The discrepancy suggests that the model is too simple or its parameters are too conservative to capture the full spatial integration properties of the ventral visual stream up to area V4. Several factors could account for this difference:\n1.  **Network Depth**: The biological visual hierarchy (e.g., Retina -> LGN -> V1 -> V2 -> V4) involves more processing stages than the three layers in this model. A deeper network would compound receptive fields to a greater extent, leading to a larger final RF size.\n2.  **Pooling Layers**: The model exclusively uses strided convolutions. Architectures modeling the visual system often include pooling layers (e.g., max-pooling), which also increase the effective receptive field size at each step, often more aggressively than a stride of $2$ within a convolutional layer.\n3.  **Dilation**: The model assumes no dilation. Dilated (or atrous) convolutions are a mechanism for increasing the receptive field size without increasing the number of parameters. This is a plausible biological mechanism and a common feature in modern CNNs designed for large-scale spatial understanding.\n4.  **Model Parameters**: The kernel sizes and strides might be too small. Larger values, especially for strides in early layers, would cause the receptive field to grow much faster.\n5.  **Eccentricity Dependence**: The comparison value of $5$ degrees is for \"mid-eccentricities.\" V4 receptive fields are smaller near the fovea. Our calculation for a generic unit in the CNN might be more representative of a parafoveal V4 neuron rather than one at mid-eccentricity. However, even for foveal V4, $1.48$ degrees is on the small side.\n6.  **Non-Feedforward Mechanisms**: The biological cortex contains extensive lateral and feedback connections, which can dynamically modulate receptive field properties and are not captured by this simple, strictly feedforward model.\n\nIn conclusion, while the model correctly implements the principle of hierarchical feature and receptive field construction, it is too shallow to produce RF sizes comparable to those in cortical area V4.",
            "answer": "$$\\boxed{\\frac{95}{64}}$$"
        },
        {
            "introduction": "While feedforward networks excel at modeling sensory processing, the brain's ability to maintain information over time relies on recurrent connections and persistent activity. Ring attractor networks are a canonical model for how neural circuits can sustain a \"bump\" of activity to represent a continuous variable, such as head direction. In this exercise , you will linearize the dynamics of a ring attractor to discover how the pattern of synaptic weights determines whether the network can support a stable memory trace, providing a foundational understanding of attractor-based computation.",
            "id": "3974360",
            "problem": "Consider a ring-attractor recurrent neural network that models a one-dimensional circular variable (for example, head direction) with $N \\geq 5$ neurons indexed by preferred angles $\\theta_i \\in \\{0, \\frac{2\\pi}{N}, \\ldots, \\frac{2\\pi(N-1)}{N}\\}$ on the unit circle. The recurrent weight matrix is translation-invariant on the ring and defined by\n$$\nW_{ij} \\;=\\; w_0 \\;+\\; w_1 \\cos\\!\\big(\\theta_i - \\theta_j\\big),\n$$\nwith constants $w_0 \\in \\mathbb{R}$ and $w_1 \\in \\mathbb{R}$. Neuronal firing rates $r_i(t)$ follow a standard rate model\n$$\n\\tau \\frac{d r_i}{dt} \\;=\\; -\\,r_i \\;+\\; \\phi\\!\\Big(g \\sum_{j=1}^{N} W_{ij}\\, r_j \\;+\\; I\\Big),\n$$\nwhere $\\tau > 0$ is the membrane time constant, $g > 0$ is a global synaptic gain, $I \\in \\mathbb{R}$ is a constant external drive, and $\\phi(\\cdot)$ is a smooth, monotonically increasing activation function. Assume there exists a homogeneous fixed point $r_i(t) = \\bar{r}$ for all $i$ satisfying $\\bar{r} = \\phi\\!\\big(g \\sum_j W_{ij}\\, \\bar{r} + I\\big)$, and define the local slope at the operating point by $\\alpha := \\phi'(u)$, where $u := g \\sum_j W_{ij}\\, \\bar{r} + I$.\n\nStarting from first principles of linearization and symmetry on the circle, analyze infinitesimal perturbations $\\delta r_i(t)$ around the homogeneous fixed point and use the discrete Fourier basis to diagonalize the linearized dynamics. Then:\n\n1. Derive the eigenmodes and eigenvalues of the weight matrix $W$ in the discrete Fourier basis on the ring.\n2. Using the linearized dynamics, determine which Fourier mode becomes unstable first as the gain $g$ is increased, and identify the condition on $w_0$ and $w_1$ under which the first nontrivial spatial (bump-forming) mode destabilizes before the uniform (zero wavenumber) mode.\n3. Under that condition, compute the critical gain $g^{\\star}$ at which the first spatial Fourier mode loses stability, expressed in closed form in terms of $N$, $w_1$, and $\\alpha$. Provide this $g^{\\star}$ as your final answer.\n\nExpress your final answer as a single closed-form analytic expression. Do not include units and do not provide inequalities or additional conditions in the final answer box.",
            "solution": "The problem is well-posed and scientifically grounded within the standard theoretical framework of computational neuroscience. We can proceed with a formal solution.\n\nThe solution is structured in three parts as requested: first, we determine the eigenvalues of the weight matrix $W$; second, we analyze the stability of the homogeneous state and find the condition for a spatial instability; third, we compute the critical gain $g^{\\star}$ at the onset of this instability.\n\n**1. Eigenmodes and Eigenvalues of the Weight Matrix $W$**\n\nThe recurrent weight matrix is given by $W_{ij} = w_0 + w_1 \\cos(\\theta_i - \\theta_j)$. The neuron indices are $i,j \\in \\{1, \\dots, N\\}$, and the preferred angles are $\\theta_i = \\frac{2\\pi(i-1)}{N}$. The term $\\theta_i - \\theta_j$ can be written as $\\frac{2\\pi(i-j)}{N}$. Since $W_{ij}$ depends only on the difference $(i-j) \\pmod N$, the matrix $W$ is a circulant matrix.\n\nThe eigenvectors of any $N \\times N$ circulant matrix are the discrete Fourier modes. The $k$-th unnormalized eigenvector, for wavenumber $k \\in \\{0, 1, \\ldots, N-1\\}$, is a vector $\\mathbf{v}^{(k)}$ with components $v_j^{(k)} = \\exp(i \\theta_j k) = \\exp\\left(i \\frac{2\\pi(j-1)k}{N}\\right)$.\n\nThe corresponding eigenvalue $\\lambda_k$ is found by applying the matrix $W$ to its eigenvector $\\mathbf{v}^{(k)}$:\n$$ (\\mathbf{W}\\mathbf{v}^{(k)})_i = \\sum_{j=1}^{N} W_{ij} v_j^{(k)} = \\sum_{j=1}^{N} \\left(w_0 + w_1 \\cos(\\theta_i - \\theta_j)\\right) \\exp(i \\theta_j k) $$\nWe can factor out $\\exp(i\\theta_i k)$ and let the summation index be $m=i-j$:\n$$ (\\mathbf{W}\\mathbf{v}^{(k)})_i = \\exp(i\\theta_i k) \\sum_{j=1}^{N} \\left(w_0 + w_1 \\cos(\\theta_i - \\theta_j)\\right) \\exp(i(\\theta_j - \\theta_i)k) $$\n$$ = v_i^{(k)} \\sum_{m=i-N}^{i-1} \\left(w_0 + w_1 \\cos\\left(\\frac{2\\pi m}{N}\\right)\\right) \\exp\\left(-i\\frac{2\\pi m k}{N}\\right) $$\nSince the terms in the sum are periodic in $m$ with period $N$, we can sum over any $N$ consecutive integers, for instance $m \\in \\{0, 1, \\ldots, N-1\\}$. Thus, the eigenvalue $\\lambda_k$ is given by:\n$$ \\lambda_k = \\sum_{m=0}^{N-1} \\left(w_0 + w_1 \\cos\\left(\\frac{2\\pi m}{N}\\right)\\right) \\exp\\left(-i\\frac{2\\pi m k}{N}\\right) $$\nWe separate the sum into two parts:\n$$ \\lambda_k = w_0 \\sum_{m=0}^{N-1} \\exp\\left(-i\\frac{2\\pi m k}{N}\\right) + w_1 \\sum_{m=0}^{N-1} \\cos\\left(\\frac{2\\pi m}{N}\\right) \\exp\\left(-i\\frac{2\\pi m k}{N}\\right) $$\nThe first sum is a geometric series which equals $N$ if $k=0 \\pmod N$ and $0$ otherwise. Using the Kronecker delta, this term is $N w_0 \\delta_{k,0}$.\n\nFor the second sum, we use Euler's formula for cosine, $\\cos(x) = \\frac{1}{2}(\\exp(ix) + \\exp(-ix))$:\n$$ w_1 \\sum_{m=0}^{N-1} \\frac{1}{2}\\left(\\exp\\left(i\\frac{2\\pi m}{N}\\right) + \\exp\\left(-i\\frac{2\\pi m}{N}\\right)\\right) \\exp\\left(-i\\frac{2\\pi m k}{N}\\right) $$\n$$ = \\frac{w_1}{2} \\sum_{m=0}^{N-1} \\left(\\exp\\left(-i\\frac{2\\pi m (k-1)}{N}\\right) + \\exp\\left(-i\\frac{2\\pi m (k+1)}{N}\\right)\\right) $$\nThis sum is non-zero only if $k-1=0 \\pmod N$ (i.e., $k=1$) or $k+1=0 \\pmod N$ (i.e., $k=N-1$).\nFor $k=1$, the first term in the parenthesis sums to $N$ and the second to $0$ (since $N \\ge 5$, $k+1=2 \\not\\equiv 0 \\pmod N$).\nFor $k=N-1$, the second term sums to $N$ and the first to $0$ (since $k-1=N-2 \\not\\equiv 0 \\pmod N$).\nCombining the results, the eigenvalues of $W$ are:\n- For $k=0$: $\\lambda_0 = N w_0$.\n- For $k=1$: $\\lambda_1 = \\frac{N w_1}{2}$.\n- For $k=N-1$: $\\lambda_{N-1} = \\frac{N w_1}{2}$.\n- For $k \\in \\{2, 3, \\ldots, N-2\\}$: $\\lambda_k = 0$.\n\nThe eigenmodes are the discrete Fourier basis vectors $\\mathbf{v}^{(k)}$.\n\n**2. Linearization and Stability Analysis**\n\nWe linearize the dynamical equation $\\tau \\frac{dr_i}{dt} = -r_i + \\phi(g \\sum_{j} W_{ij} r_j + I)$ around the homogeneous fixed point $r_i(t) = \\bar{r}$. Let $r_i(t) = \\bar{r} + \\delta r_i(t)$, where $\\delta r_i(t)$ is an infinitesimal perturbation.\nThe argument of the activation function is $u_i = g \\sum_j W_{ij} r_j + I$. At the fixed point, this argument is $u = g \\sum_j W_{ij} \\bar{r} + I = g \\bar{r} (N w_0) + I$, which is independent of $i$.\nThe linearized dynamics for the perturbation are:\n$$ \\tau \\frac{d\\delta r_i}{dt} = -\\delta r_i + \\left. \\frac{\\partial \\phi}{\\partial u_i} \\right|_{u} \\left(g \\sum_{j} W_{ij} \\delta r_j\\right) $$\nUsing the given definition $\\alpha = \\phi'(u)$, this becomes:\n$$ \\tau \\frac{d\\delta r_i}{dt} = -\\delta r_i + \\alpha g \\sum_{j=1}^{N} W_{ij} \\delta r_j $$\nIn vector form, letting $\\delta\\mathbf{r} = (\\delta r_1, \\ldots, \\delta r_N)^T$, the equation is:\n$$ \\tau \\frac{d\\delta\\mathbf{r}}{dt} = (-I_N + \\alpha g W) \\delta\\mathbf{r} $$\nwhere $I_N$ is the identity matrix. The stability of the fixed point is determined by the eigenvalues of the Jacobian matrix $J = \\frac{1}{\\tau}(-I_N + \\alpha g W)$. The eigenvalues of $J$, denoted $\\mu_k$, are related to the eigenvalues of $W$, $\\lambda_k$, by:\n$$ \\mu_k = \\frac{1}{\\tau}(-1 + \\alpha g \\lambda_k) $$\nThe homogeneous fixed point becomes unstable when the real part of any eigenvalue $\\mu_k$ becomes positive. As $\\alpha > 0$ and $g>0$, instability occurs when $-1 + \\alpha g \\lambda_k > 0$ for some $k$, which implies $\\alpha g \\lambda_k > 1$. This requires $\\lambda_k > 0$. As $g$ is increased from $0$, the mode $k$ with the largest positive eigenvalue $\\lambda_k$ will be the first to destabilize.\n\nThe nontrivial spatial (bump-forming) mode corresponds to $k=1$ (and $k=N-1$). The uniform mode corresponds to $k=0$. The other modes ($k \\in \\{2, ..., N-2\\}$) have $\\lambda_k=0$, so they are always stable with $\\mu_k = -1/\\tau$.\nFor the spatial mode to destabilize before the uniform mode, its corresponding eigenvalue must be positive and larger than the eigenvalue of the uniform mode. That is, we require $\\lambda_1 > 0$ and $\\lambda_1 > \\lambda_0$.\n1.  $\\lambda_1 > 0 \\implies \\frac{N w_1}{2} > 0 \\implies w_1 > 0$.\n2.  $\\lambda_1 > \\lambda_0 \\implies \\frac{N w_1}{2} > N w_0 \\implies w_1 > 2 w_0$.\n\nTherefore, the condition for the first spatial mode to destabilize first is $w_1 > 0$ and $w_1 > 2w_0$.\n\n**3. Critical Gain $g^{\\star}$**\n\nUnder the condition derived above, the first instability to occur as $g$ increases is the Turing-type bifurcation corresponding to the spatial mode $k=1$. The critical gain $g^{\\star}$ is the value of $g$ at which the real part of the eigenvalue $\\mu_1$ crosses zero.\n$$ \\text{Re}(\\mu_1) = \\frac{1}{\\tau}(-1 + \\alpha g^{\\star} \\lambda_1) = 0 $$\n$$ \\implies 1 = \\alpha g^{\\star} \\lambda_1 $$\nSolving for $g^{\\star}$, we get:\n$$ g^{\\star} = \\frac{1}{\\alpha \\lambda_1} $$\nSubstituting the expression for $\\lambda_1 = \\frac{N w_1}{2}$:\n$$ g^{\\star} = \\frac{1}{\\alpha \\left(\\frac{N w_1}{2}\\right)} = \\frac{2}{N \\alpha w_1} $$\nThis is the critical gain at which the homogeneous state loses stability to a spatially patterned state (a \"bump\"). The expression is in terms of the specified parameters $N$, $w_1$, and $\\alpha$.",
            "answer": "$$\n\\boxed{\\frac{2}{N \\alpha w_1}}\n$$"
        },
        {
            "introduction": "The functional architecture of neural circuits is shaped by experience through synaptic plasticity. A fundamental question is how local learning rules can give rise to globally coherent computations. This practice  explores Oja's rule, a classic, biologically plausible model of Hebbian learning, and asks you to demonstrate through a mean-field analysis that it enables a single neuron to find the principal component of its input data. This exercise provides a crucial link between synaptic-level mechanisms and the ability of neural networks to learn meaningful statistical structure from the environment.",
            "id": "3974366",
            "problem": "A single linear neuron with synaptic weight vector $\\mathbf{w} \\in \\mathbb{R}^{d}$ receives inputs $\\mathbf{x}_{t} \\in \\mathbb{R}^{d}$ that are independent and identically distributed (IID), zero-mean, with covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ that is symmetric and positive semidefinite. The neuron's activity is $y_{t} = \\mathbf{w}_{t}^{\\top} \\mathbf{x}_{t}$. The synaptic plasticity follows Oja’s rule, which is a normalized Hebbian update: \n$$\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\eta\\, y_{t}\\big(\\mathbf{x}_{t} - y_{t}\\,\\mathbf{w}_{t}\\big),\n$$\nwhere $\\eta > 0$ is a small learning rate. Principal Component Analysis (PCA) is defined by the maximization of the Rayleigh quotient $R(\\mathbf{w}) = \\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w}$ subject to the unit-norm constraint $\\|\\mathbf{w}\\|_{2} = 1$, whose maximizer is any principal eigenvector $\\mathbf{v}_{1}$ of $\\boldsymbol{\\Sigma}$ corresponding to the largest eigenvalue $\\lambda_{1}$.\n\nStarting only from the definitions above and the fundamental facts that (i) $\\mathbb{E}[\\mathbf{x}_{t}] = \\mathbf{0}$, (ii) $\\mathbb{E}[\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top}] = \\boldsymbol{\\Sigma}$, and (iii) the continuous-time limit of a small-step stochastic update can be approximated by an ordinary differential equation (ODE) for the expected dynamics, do the following:\n\n- Derive the mean-field ODE for the expected evolution of $\\mathbf{w}(t)$ implied by Oja’s rule in the limit $\\eta \\to 0$.\n- Using the unit-norm manifold $\\|\\mathbf{w}\\|_{2} = 1$ and the Rayleigh quotient $R(\\mathbf{w})$, argue from first principles why the stable fixed points of the mean-field dynamics correspond to the principal eigenvector(s) of $\\boldsymbol{\\Sigma}$ and hence implement online PCA.\n- Linearize the mean-field ODE around the unit-norm principal eigenvector $\\mathbf{v}_{1}$ with eigenvalue $\\lambda_{1}$ to obtain the asymptotic rate at which perturbations in directions orthogonal to $\\mathbf{v}_{1}$ decay. Assume the covariance spectrum has eigenvalues $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{d} \\ge 0$ and that the initial condition satisfies $\\|\\mathbf{w}(0)\\|_{2} = 1$.\n\nYour final answer must be the single symbolic expression for the asymptotic exponential convergence rate (the slowest decay rate among orthogonal modes) of the mean-field continuous-time dynamics, expressed in terms of the leading eigenvalue $\\lambda_{1}$ and the second-largest eigenvalue $\\lambda_{2}$. Do not include any units. No numerical rounding is required; provide the exact analytic expression.",
            "solution": "The problem requires a three-part analysis of Oja's rule for synaptic plasticity. We must first validate the problem statement, which has been found to be valid, well-posed, and scientifically sound. It is a canonical problem in theoretical neuroscience. We proceed with the solution.\n\nThe problem provides the following components: a linear neuron with output $y_{t} = \\mathbf{w}_{t}^{\\top} \\mathbf{x}_{t}$, where $\\mathbf{w}_{t} \\in \\mathbb{R}^{d}$ is the weight vector and $\\mathbf{x}_{t} \\in \\mathbb{R}^{d}$ is the input. The inputs are IID with $\\mathbb{E}[\\mathbf{x}_{t}] = \\mathbf{0}$ and covariance $\\mathbb{E}[\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top}] = \\boldsymbol{\\Sigma}$. The weights evolve according to Oja's rule:\n$$\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\eta\\, y_{t}\\big(\\mathbf{x}_{t} - y_{t}\\,\\mathbf{w}_{t}\\big)\n$$\nwhere $\\eta > 0$ is a small learning rate.\n\n**1. Derivation of the Mean-Field ODE**\n\nTo derive the continuous-time dynamics in the limit of a small learning rate $\\eta \\to 0$, we first rewrite the discrete update rule. Let $\\Delta \\mathbf{w}_{t} = \\mathbf{w}_{t+1} - \\mathbf{w}_{t}$. Then,\n$$\n\\frac{\\Delta \\mathbf{w}_{t}}{\\eta} = y_{t}\\mathbf{x}_{t} - y_{t}^{2}\\mathbf{w}_{t}\n$$\nIn the continuous-time limit, we let $\\eta$ be an infinitesimal time step $dt$, so the left-hand side becomes the time derivative $\\frac{d\\mathbf{w}}{dt}$. The right-hand side is a stochastic quantity dependent on the input $\\mathbf{x}_{t}$. For a small learning rate, the weight vector $\\mathbf{w}(t)$ changes much more slowly than the input $\\mathbf{x}(t)$. We can therefore perform a mean-field approximation by averaging the update term over the distribution of $\\mathbf{x}$, treating $\\mathbf{w}$ as quasistatic.\n$$\n\\frac{d\\mathbf{w}}{dt} = \\mathbb{E}_{\\mathbf{x}}\\left[ y\\mathbf{x} - y^{2}\\mathbf{w} \\right]\n$$\nwhere $y = \\mathbf{w}^{\\top}\\mathbf{x}$. We compute the expectation of each term separately.\n\nFor the first term, we use the definition of $y$ and the linearity of expectation:\n$$\n\\mathbb{E}[y\\mathbf{x}] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})\\mathbf{x}] = \\mathbb{E}[\\mathbf{x}(\\mathbf{x}^{\\top}\\mathbf{w})]\n$$\nSince $\\mathbf{w}$ is treated as constant with respect to the expectation over $\\mathbf{x}$, we can write:\n$$\n\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{w}] = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w} = \\boldsymbol{\\Sigma}\\mathbf{w}\n$$\nFor the second term, we first compute the expectation of $y^{2}$:\n$$\n\\mathbb{E}[y^{2}] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})^{2}] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})(\\mathbf{x}^{\\top}\\mathbf{w})] = \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w} = \\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w}\n$$\nThe expectation of the second term in the update is then:\n$$\n\\mathbb{E}[y^{2}\\mathbf{w}] = \\mathbb{E}[y^{2}]\\mathbf{w} = (\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w})\\mathbf{w}\n$$\nCombining these results, we obtain the mean-field ordinary differential equation (ODE) for the evolution of the weight vector $\\mathbf{w}(t)$:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\boldsymbol{\\Sigma}\\mathbf{w} - (\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w})\\mathbf{w}\n$$\n\n**2. Fixed Points and Connection to PCA**\n\nThe problem states that PCA is equivalent to maximizing the Rayleigh quotient $R(\\mathbf{w}) = \\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w}$ subject to $\\|\\mathbf{w}\\|_{2} = 1$. The maximizer is the principal eigenvector $\\mathbf{v}_{1}$ of $\\boldsymbol{\\Sigma}$. We will now show that the stable fixed points of the derived ODE correspond to these principal eigenvectors.\n\nFirst, let's analyze the norm of $\\mathbf{w}(t)$. The initial condition is $\\|\\mathbf{w}(0)\\|_{2} = 1$. The evolution of the squared norm $\\|\\mathbf{w}\\|_{2}^{2} = \\mathbf{w}^{\\top}\\mathbf{w}$ is given by:\n$$\n\\frac{d}{dt}(\\mathbf{w}^{\\top}\\mathbf{w}) = 2\\mathbf{w}^{\\top}\\frac{d\\mathbf{w}}{dt} = 2\\mathbf{w}^{\\top}\\left[ \\boldsymbol{\\Sigma}\\mathbf{w} - (\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w})\\mathbf{w} \\right]\n$$\n$$\n\\frac{d}{dt}\\|\\mathbf{w}\\|_{2}^{2} = 2\\left[ \\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w} - (\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w})(\\mathbf{w}^{\\top}\\mathbf{w}) \\right] = 2(\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w})(1 - \\|\\mathbf{w}\\|_{2}^{2})\n$$\nIf $\\|\\mathbf{w}(0)\\|_{2}^{2} = 1$, then $\\frac{d}{dt}\\|\\mathbf{w}\\|_{2}^{2}|_{t=0} = 0$. This implies that $\\|\\mathbf{w}(t)\\|_{2} = 1$ for all $t \\ge 0$. The unit sphere is thus an invariant manifold of the dynamics.\n\nThe fixed points $\\mathbf{w}^{*}$ of the ODE are found by setting $\\frac{d\\mathbf{w}}{dt} = \\mathbf{0}$:\n$$\n\\boldsymbol{\\Sigma}\\mathbf{w}^{*} - (\\mathbf{w}^{*\\top}\\boldsymbol{\\Sigma}\\mathbf{w}^{*})\\mathbf{w}^{*} = \\mathbf{0} \\quad \\implies \\quad \\boldsymbol{\\Sigma}\\mathbf{w}^{*} = (\\mathbf{w}^{*\\top}\\boldsymbol{\\Sigma}\\mathbf{w}^{*})\\mathbf{w}^{*}\n$$\nThis is an eigenvector equation. Any non-zero fixed point $\\mathbf{w}^{*}$ must be an eigenvector of the covariance matrix $\\boldsymbol{\\Sigma}$, with the corresponding eigenvalue being $\\lambda = \\mathbf{w}^{*\\top}\\boldsymbol{\\Sigma}\\mathbf{w}^{*} = R(\\mathbf{w}^{*})$.\n\nTo determine stability, we consider the Rayleigh quotient $R(\\mathbf{w}) = \\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w}$ as a Lyapunov function for the dynamics on the unit sphere. Its time derivative is:\n$$\n\\frac{dR}{dt} = \\frac{d}{dt}(\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w}) = 2(\\boldsymbol{\\Sigma}\\mathbf{w})^{\\top}\\frac{d\\mathbf{w}}{dt} = 2\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\frac{d\\mathbf{w}}{dt}\n$$\nSubstituting the ODE for $\\frac{d\\mathbf{w}}{dt}$:\n$$\n\\frac{dR}{dt} = 2\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\left[\\boldsymbol{\\Sigma}\\mathbf{w} - (\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w})\\mathbf{w}\\right] = 2\\left[\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}^{2}\\mathbf{w} - (\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w})^{2}\\right]\n$$\nLet the orthonormal eigenvectors of $\\boldsymbol{\\Sigma}$ be $\\{\\mathbf{v}_{i}\\}$ with eigenvalues $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{d} \\ge 0$. We can express $\\mathbf{w}$ in this basis: $\\mathbf{w} = \\sum_{i=1}^{d} c_{i}\\mathbf{v}_{i}$, where $\\sum_{i=1}^{d} c_{i}^{2} = \\|\\mathbf{w}\\|_{2}^{2} = 1$.\nThen $\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}^{2}\\mathbf{w} = \\sum_{i=1}^{d} c_{i}^{2}\\lambda_{i}^{2}$ and $(\\mathbf{w}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{w})^{2} = (\\sum_{i=1}^{d} c_{i}^{2}\\lambda_{i})^{2}$.\nThe term $\\sum_{i} c_{i}^{2}\\lambda_{i}$ is the expected value of a random variable that takes value $\\lambda_{i}$ with probability $c_{i}^{2}$. By Jensen's inequality, $\\mathbb{E}[X^2] \\ge (\\mathbb{E}[X])^2$, so $\\sum c_{i}^{2}\\lambda_{i}^{2} \\ge (\\sum c_{i}^{2}\\lambda_{i})^{2}$.\nThus, $\\frac{dR}{dt} \\ge 0$. The Rayleigh quotient is non-decreasing. The dynamics drive $\\mathbf{w}(t)$ \"uphill\" on the landscape defined by $R(\\mathbf{w})$ on the unit sphere. Equality $\\frac{dR}{dt}=0$ holds if and only if $\\mathbf{w}$ is an eigenvector of $\\boldsymbol{\\Sigma}$.\nThe stable fixed points correspond to local maxima of $R(\\mathbf{w})$. The global maximum of $R(\\mathbf{w})$ on the unit sphere is $\\lambda_{1}$, achieved when $\\mathbf{w}$ is a principal eigenvector $\\mathbf{v}_{1}$. Therefore, the dynamics converge to the principal eigenvector(s) of $\\boldsymbol{\\Sigma}$, effectively performing PCA.\n\n**3. Linearization and Convergence Rate**\n\nWe now linearize the ODE around a principal eigenvector fixed point $\\mathbf{w}^{*} = \\mathbf{v}_{1}$ (assuming $\\lambda_{1} > \\lambda_{2}$ for uniqueness). Let $\\mathbf{w}(t) = \\mathbf{v}_{1} + \\boldsymbol{\\epsilon}(t)$, where $\\boldsymbol{\\epsilon}(t)$ is a small perturbation. Since $\\mathbf{w}(t)$ remains on the unit sphere, we have to first order:\n$$\n\\|\\mathbf{v}_{1} + \\boldsymbol{\\epsilon}\\|_{2}^{2} = \\mathbf{v}_{1}^{\\top}\\mathbf{v}_{1} + 2\\mathbf{v}_{1}^{\\top}\\boldsymbol{\\epsilon} + O(\\|\\boldsymbol{\\epsilon}\\|_{2}^{2}) = 1 + 2\\mathbf{v}_{1}^{\\top}\\boldsymbol{\\epsilon} \\approx 1\n$$\nThis implies $\\mathbf{v}_{1}^{\\top}\\boldsymbol{\\epsilon} = 0$, so the perturbation $\\boldsymbol{\\epsilon}(t)$ must be orthogonal to $\\mathbf{v}_{1}$.\nThe time derivative of the perturbation is $\\frac{d\\boldsymbol{\\epsilon}}{dt}$. Substituting $\\mathbf{w} = \\mathbf{v}_{1} + \\boldsymbol{\\epsilon}$ into the ODE:\n$$\n\\frac{d\\boldsymbol{\\epsilon}}{dt} = \\boldsymbol{\\Sigma}(\\mathbf{v}_{1} + \\boldsymbol{\\epsilon}) - \\big((\\mathbf{v}_{1} + \\boldsymbol{\\epsilon})^{\\top}\\boldsymbol{\\Sigma}(\\mathbf{v}_{1} + \\boldsymbol{\\epsilon})\\big)(\\mathbf{v}_{1} + \\boldsymbol{\\epsilon})\n$$\nExpanding and keeping terms up to first order in $\\boldsymbol{\\epsilon}$:\n- $\\boldsymbol{\\Sigma}(\\mathbf{v}_{1} + \\boldsymbol{\\epsilon}) = \\boldsymbol{\\Sigma}\\mathbf{v}_{1} + \\boldsymbol{\\Sigma}\\boldsymbol{\\epsilon} = \\lambda_{1}\\mathbf{v}_{1} + \\boldsymbol{\\Sigma}\\boldsymbol{\\epsilon}$.\n- $(\\mathbf{v}_{1} + \\boldsymbol{\\epsilon})^{\\top}\\boldsymbol{\\Sigma}(\\mathbf{v}_{1} + \\boldsymbol{\\epsilon}) = \\mathbf{v}_{1}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{v}_{1} + 2\\mathbf{v}_{1}^{\\top}\\boldsymbol{\\Sigma}\\boldsymbol{\\epsilon} + O(\\|\\boldsymbol{\\epsilon}\\|_{2}^{2})$.\n- Since $\\boldsymbol{\\Sigma}$ is symmetric, $\\mathbf{v}_{1}^{\\top}\\boldsymbol{\\Sigma} = (\\boldsymbol{\\Sigma}\\mathbf{v}_{1})^{\\top} = \\lambda_{1}\\mathbf{v}_{1}^{\\top}$. The term becomes $\\lambda_{1} + 2\\lambda_{1}\\mathbf{v}_{1}^{\\top}\\boldsymbol{\\epsilon} + \\dots = \\lambda_{1}$, because $\\mathbf{v}_{1}^{\\top}\\boldsymbol{\\epsilon}=0$.\nThe ODE becomes:\n$$\n\\frac{d\\boldsymbol{\\epsilon}}{dt} \\approx (\\lambda_{1}\\mathbf{v}_{1} + \\boldsymbol{\\Sigma}\\boldsymbol{\\epsilon}) - \\lambda_{1}(\\mathbf{v}_{1} + \\boldsymbol{\\epsilon}) = \\boldsymbol{\\Sigma}\\boldsymbol{\\epsilon} - \\lambda_{1}\\boldsymbol{\\epsilon} = (\\boldsymbol{\\Sigma} - \\lambda_{1}\\mathbf{I})\\boldsymbol{\\epsilon}\n$$\nThis is the linearized equation for the perturbation. Since $\\boldsymbol{\\epsilon} \\perp \\mathbf{v}_{1}$, we can express it in the basis of the other eigenvectors: $\\boldsymbol{\\epsilon}(t) = \\sum_{i=2}^{d} c_{i}(t)\\mathbf{v}_{i}$.\nSubstituting into the linearized ODE:\n$$\n\\sum_{i=2}^{d} \\frac{dc_{i}}{dt}\\mathbf{v}_{i} = (\\boldsymbol{\\Sigma} - \\lambda_{1}\\mathbf{I})\\sum_{i=2}^{d} c_{i}(t)\\mathbf{v}_{i} = \\sum_{i=2}^{d} c_{i}(t)(\\boldsymbol{\\Sigma}\\mathbf{v}_{i} - \\lambda_{1}\\mathbf{v}_{i}) = \\sum_{i=2}^{d} c_{i}(t)(\\lambda_{i} - \\lambda_{1})\\mathbf{v}_{i}\n$$\nBy comparing coefficients for each eigenvector (due to orthogonality), we get a set of decoupled first-order ODEs for the components $c_{i}(t)$:\n$$\n\\frac{dc_{i}}{dt} = (\\lambda_{i} - \\lambda_{1})c_{i}(t) \\quad \\text{for } i = 2, 3, \\dots, d\n$$\nThe solution for each component is $c_{i}(t) = c_{i}(0)\\exp((\\lambda_{i} - \\lambda_{1})t)$. The decay rate for mode $i$ is the positive constant in the exponent, which is $-(\\lambda_{i} - \\lambda_{1}) = \\lambda_{1} - \\lambda_{i}$.\nThe asymptotic convergence rate is determined by the slowest decay, which corresponds to the smallest decay rate constant. The rates are $\\{\\lambda_{1} - \\lambda_{2}, \\lambda_{1} - \\lambda_{3}, \\dots, \\lambda_{1} - \\lambda_{d}\\}$.\nGiven the ordering $\\lambda_{2} \\ge \\lambda_{3} \\ge \\cdots$, we have $-\\lambda_{2} \\le -\\lambda_{3} \\le \\cdots$. Therefore, $\\lambda_{1} - \\lambda_{2} \\le \\lambda_{1} - \\lambda_{3} \\le \\cdots$.\nThe slowest (smallest) rate of decay for perturbations orthogonal to $\\mathbf{v}_{1}$ is determined by the spectral gap between the first and second eigenvalues. This rate is $\\lambda_{1} - \\lambda_{2}$.",
            "answer": "$$\\boxed{\\lambda_{1} - \\lambda_{2}}$$"
        }
    ]
}