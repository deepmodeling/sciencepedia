{
    "hands_on_practices": [
        {
            "introduction": "A core structural parallel between Deep Convolutional Networks (DCNs) and the primate ventral visual stream is their hierarchical organization, where neurons in successively higher areas respond to stimuli in ever-larger regions of the visual field. This exercise solidifies this concept by tasking you with deriving the growth of receptive fields from first principles. By calculating how a unit's receptive field expands across layers as a function of kernel size and stride, you will gain a fundamental skill for analyzing any DCN architecture and appreciating its correspondence to biological vision .",
            "id": "3988311",
            "problem": "Consider a hierarchical feedforward model of the primate ventral visual stream, implemented as a stack of convolutional layers as in a Convolutional Neural Network (CNN). In visual neuroscience, the receptive field of a neuron is the subset of sensory input space whose stimulation modulates the neuron's activity. In computational models with discrete inputs, the receptive field of a unit at a layer is the set of input samples whose values can influence that unit's activation through the sequence of operations below it. Assume one-dimensional spatial indexing along an image row; for two-dimensional isotropic kernels and strides, the one-dimensional result applies identically along both axes.\n\nStart from the following fundamental base: (i) the definition above of receptive field as the subset of inputs influencing a unit's activation, and (ii) the well-tested fact that a convolutional layer with kernel size $k_i$ and stride $s_i$ samples $k_i$ adjacent units from the previous layer per unit in the current layer, and positions current-layer units spaced $s_i$ indices apart relative to the previous layer. Let $R_i$ denote the receptive field size of a unit at layer $i$ with respect to the original input samples, and let $S_i$ denote the effective sampling stride of layer $i$ units with respect to the input.\n\nDerive, from first principles, a closed-form expression for $R_L$ in terms of $\\{k_i\\}_{i=1}^{L}$ and $\\{s_i\\}_{i=1}^{L}$. Then, apply your derivation to the following ventral-stream-inspired architecture with $L=4$ layers:\n- Layer $1$: kernel size $k_1=7$, stride $s_1=2$ (V1-like simple-complex integration and subsampling),\n- Layer $2$: kernel size $k_2=5$, stride $s_2=2$ (V2-like integration and subsampling),\n- Layer $3$: kernel size $k_3=9$, stride $s_3=2$ (V4-like integration and subsampling),\n- Layer $4$: kernel size $k_4=9$, stride $s_4=1$ (inferotemporal-like integration without subsampling).\n\nProvide your final answer as the single integer equal to $R_4$, the top-layer receptive field size measured in number of input samples. No rounding is necessary. Do not include any units in your final answer.",
            "solution": "The receptive field is defined as the set of inputs whose values can influence the activation of a particular unit. In a hierarchical feedforward stack, this set grows with depth because each unit aggregates information from multiple units below it, each of which is itself influenced by an expanding subset of the original input.\n\nWe formalize this in a one-dimensional discrete model. Let $R_i$ be the receptive field size at layer $i$ with respect to the original input (layer $0$). The base case is $R_0 = 1$, since one input sample influences itself. Define $S_i$ as the effective sampling stride of layer $i$ with respect to the input. Because strides compound multiplicatively, we have\n$$\nS_i \\;=\\; \\prod_{j=1}^{i} s_j, \\quad \\text{with} \\quad S_0 = 1.\n$$\nConsider the transition from layer $i-1$ to layer $i$. A unit at layer $i$ samples $k_i$ adjacent units from layer $i-1$. The leftmost and rightmost sampled units are separated by $k_i - 1$ steps at layer $i-1$, and each step at layer $i-1$ corresponds to $S_{i-1}$ steps in the input. Therefore, the incremental expansion of the receptive field contributed by the $i$-th layer is $(k_i - 1) \\, S_{i-1}$. The total receptive field at layer $i$ is the receptive field at layer $i-1$ plus this increment:\n$$\nR_i \\;=\\; R_{i-1} \\;+\\; (k_i - 1) \\, S_{i-1}.\n$$\nUnrolling this recurrence from $R_0 = 1$ yields a closed form:\n$$\nR_L \\;=\\; 1 \\;+\\; \\sum_{i=1}^{L} (k_i - 1) \\, \\prod_{j=1}^{i-1} s_j.\n$$\nWe now compute $R_4$ for the specified architecture:\n- Given $k_1 = 7$, $s_1 = 2$, $k_2 = 5$, $s_2 = 2$, $k_3 = 9$, $s_3 = 2$, $k_4 = 9$, $s_4 = 1$.\n- Compute the products $\\prod_{j=1}^{i-1} s_j$ for $i = 1,2,3,4$:\n  - For $i=1$, the empty product is $1$, so $\\prod_{j=1}^{0} s_j = 1$.\n  - For $i=2$, $\\prod_{j=1}^{1} s_j = s_1 = 2$.\n  - For $i=3$, $\\prod_{j=1}^{2} s_j = s_1 s_2 = 2 \\cdot 2 = 4$.\n  - For $i=4$, $\\prod_{j=1}^{3} s_j = s_1 s_2 s_3 = 2 \\cdot 2 \\cdot 2 = 8$.\n- Compute each term $(k_i - 1) \\prod_{j=1}^{i-1} s_j$:\n  - For $i=1$: $(k_1 - 1) \\cdot 1 = (7 - 1) \\cdot 1 = 6$.\n  - For $i=2$: $(k_2 - 1) \\cdot 2 = (5 - 1) \\cdot 2 = 4 \\cdot 2 = 8$.\n  - For $i=3$: $(k_3 - 1) \\cdot 4 = (9 - 1) \\cdot 4 = 8 \\cdot 4 = 32$.\n  - For $i=4$: $(k_4 - 1) \\cdot 8 = (9 - 1) \\cdot 8 = 8 \\cdot 8 = 64$.\n- Sum and add the base $1$:\n$$\nR_4 \\;=\\; 1 + (6 + 8 + 32 + 64) \\;=\\; 1 + 110 \\;=\\; 111.\n$$\n\nThus, the top-layer receptive field spans $111$ input samples along one dimension. For isotropic two-dimensional kernels and strides, the receptive field extent is the same along each axis, and the receptive field area would be $111 \\times 111$, but the requested quantity is the one-dimensional span $R_4$.",
            "answer": "$$\\boxed{111}$$"
        },
        {
            "introduction": "While DCNs excel at object recognition, they often exhibit a strong \"texture bias,\" relying on local surface properties rather than the global object shape that is central to human vision. This analytical practice explores how such non-human-like strategies can be a direct consequence of the statistical regularities within a training dataset. By modeling this phenomenon and deriving a principled reweighting scheme to counteract it, you will develop a deeper understanding of the crucial interplay between data, learning algorithms, and the resulting representational biases in computational models of the brain .",
            "id": "3974084",
            "problem": "Consider a simplified theoretical analysis of how a Deep Convolutional Neural Network (DCNN) trained by empirical risk minimization can acquire a texture bias when used as a model of the primate ventral visual stream. Suppose a binary classification task with labels $y \\in \\{-1, +1\\}$ and two latent cues per image: a shape cue $s$ and a texture cue $t$. Assume a linear readout from a fixed feature representation $(s,t)$ is trained by least squares to predict $y$, which is a standard analytical proxy for gradient-based training dynamics in the large-sample limit. Each training example is generated by one of three augmentation regimes with mixture fractions $f_A$, $f_B$, and $f_C$ that sum to $1$:\n- Regime $A$ (fraction $f_A$): both shape and texture align with the label, modeled as $s = y + \\eta_{s,A}$ and $t = y + \\eta_{t,A}$.\n- Regime $B$ (fraction $f_B$): shape aligns but texture is randomized (destroyed), modeled as $s = y + \\eta_{s,B}$ and $t = \\eta_{t,B}$, independent of $y$.\n- Regime $C$ (fraction $f_C$): a cue-conflict manipulation where texture anti-aligns with the label, modeled as $s = y + \\eta_{s,C}$ and $t = -y + \\eta_{t,C}$.\n\nAssume across all regimes: $y$ is equally likely to be $+1$ or $-1$, all noises $\\eta$ are zero-mean, mutually independent of $y$ and of each other across cues and regimes, and have the following variances: $\\sigma_{s,A}^{2} = \\sigma_{s,B}^{2} = \\sigma_{s,C}^{2} = 2.25$, $\\sigma_{t,A}^{2} = \\sigma_{t,C}^{2} = 0.0025$, and $\\sigma_{t,B}^{2} = 1.0$. Suppose the dataset composition is $f_A = 0.9$, $f_B = 0.05$, and $f_C = 0.05$.\n\nDefine the least-squares linear readout $w = (w_s, w_t)^\\top$ that minimizes the expected squared error $E[(y - w_s s - w_t t)^2]$. In the large-sample limit, $w$ satisfies the normal equations $w = \\Sigma_{xx}^{-1} \\Sigma_{xy}$, where $\\Sigma_{xx} = \\mathrm{Cov}([s,t]^\\top)$ and $\\Sigma_{xy} = \\mathrm{Cov}([s,t]^\\top, y)$ under the mixture distribution.\n\nTasks:\n1. Starting from the above generative assumptions and the normal equations, derive the analytic expressions for $\\Sigma_{xx}$ and $\\Sigma_{xy}$ as functions of $f_A$, $f_B$, $f_C$ and the specified noise variances. Then compute $w$ and demonstrate, for the given numerical values, that $|w_t| > |w_s|$ (a texture bias).\n2. Propose a principled importance reweighting of the training examples across regimes by nonnegative factors $\\alpha_A$, $\\alpha_B$, $\\alpha_C$ (not necessarily summing to $1$) that is designed to eliminate the net correlation between texture and label in the weighted population, i.e., to enforce $\\mathrm{Cov}_{\\alpha}(t,y) = 0$ in the large-sample limit. Impose the convention $\\alpha_A = 1$ and $\\alpha_B = 1$ and solve for the single free parameter $\\alpha_C$ that achieves $\\mathrm{Cov}_{\\alpha}(t,y) = 0$. \n\nGive your final answer as the value of $\\alpha_C$ that achieves this objective for the specified $f_A$, $f_B$, and $f_C$. Express the final answer as an exact real number (no rounding).",
            "solution": "The solution is divided into two parts as per the problem statement. First, we derive the weights $w_s$ and $w_t$ of the linear readout and demonstrate the texture bias. Second, we determine the importance weight $\\alpha_C$ required to eliminate the texture-label correlation.\n\nTo begin, we compute the necessary covariance matrices $\\Sigma_{xx}$ and $\\Sigma_{xy}$. The population is a mixture of three regimes, so we use the law of total expectation. For any random variable $Z$, its expectation is $E[Z] = f_A E[Z|A] + f_B E[Z|B] + f_C E[Z|C]$, where $A$, $B$, and $C$ denote the three data generation regimes.\n\nFirst, we establish the means of the variables. The label $y$ is balanced, so $E[y] = \\frac{1}{2}(+1) + \\frac{1}{2}(-1) = 0$. Consequently, $E[y^2] = \\frac{1}{2}(+1)^2 + \\frac{1}{2}(-1)^2 = 1$. The noises $\\eta$ are zero-mean, $E[\\eta]=0$, and independent of $y$ and of each other.\n\nThe means of the cues $s$ and $t$ are:\n$E[s] = E[f_A(y+\\eta_{s,A}) + f_B(y+\\eta_{s,B}) + f_C(y+\\eta_{s,C})] = (f_A+f_B+f_C)E[y] = E[y] = 0$.\n$E[t] = E[f_A(y+\\eta_{t,A}) + f_B(\\eta_{t,B}) + f_C(-y+\\eta_{t,C})] = f_A E[y] + f_B E[\\eta_{t,B}] - f_C E[y] + f_C E[\\eta_{t,C}] = 0$.\nSince all variables $y, s, t$ are zero-mean, the covariances simplify to expectations of products, e.g., $\\mathrm{Cov}(s,y) = E[sy]$.\n\nWe calculate the conditional expectations for products needed for the covariance matrices.\nFor regime $A$ ($s = y + \\eta_{s,A}$, $t = y + \\eta_{t,A}$):\n$E[sy|A] = E[(y+\\eta_{s,A})y] = E[y^2] = 1$.\n$E[ty|A] = E[(y+\\eta_{t,A})y] = E[y^2] = 1$.\n$E[s^2|A] = E[(y+\\eta_{s,A})^2] = E[y^2] + E[\\eta_{s,A}^2] = 1 + \\sigma_{s,A}^2$.\n$E[t^2|A] = E[(y+\\eta_{t,A})^2] = E[y^2] + E[\\eta_{t,A}^2] = 1 + \\sigma_{t,A}^2$.\n$E[st|A] = E[(y+\\eta_{s,A})(y+\\eta_{t,A})] = E[y^2] = 1$.\n\nFor regime $B$ ($s = y + \\eta_{s,B}$, $t = \\eta_{t,B}$):\n$E[sy|B] = E[(y+\\eta_{s,B})y] = E[y^2] = 1$.\n$E[ty|B] = E[\\eta_{t,B}y] = E[\\eta_{t,B}]E[y] = 0$.\n$E[s^2|B] = E[(y+\\eta_{s,B})^2] = 1 + \\sigma_{s,B}^2$.\n$E[t^2|B] = E[\\eta_{t,B}^2] = \\sigma_{t,B}^2$.\n$E[st|B] = E[(y+\\eta_{s,B})\\eta_{t,B}] = 0$.\n\nFor regime $C$ ($s = y + \\eta_{s,C}$, $t = -y + \\eta_{t,C}$):\n$E[sy|C] = E[(y+\\eta_{s,C})y] = E[y^2] = 1$.\n$E[ty|C] = E[(-y+\\eta_{t,C})y] = -E[y^2] = -1$.\n$E[s^2|C] = E[(y+\\eta_{s,C})^2] = 1 + \\sigma_{s,C}^2$.\n$E[t^2|C] = E[(-y+\\eta_{t,C})^2] = E[y^2] + E[\\eta_{t,C}^2] = 1 + \\sigma_{t,C}^2$.\n$E[st|C] = E[(y+\\eta_{s,C})(-y+\\eta_{t,C})] = -E[y^2] = -1$.\n\nNow we combine these using the mixture fractions $f_A, f_B, f_C$.\nThe vector $\\Sigma_{xy}$:\n$\\mathrm{Cov}(s,y) = E[sy] = f_A(1) + f_B(1) + f_C(1) = 1$.\n$\\mathrm{Cov}(t,y) = E[ty] = f_A(1) + f_B(0) + f_C(-1) = f_A - f_C$.\nSo, $\\Sigma_{xy} = \\begin{pmatrix} 1 \\\\ f_A - f_C \\end{pmatrix}$.\n\nThe matrix $\\Sigma_{xx}$:\n$\\mathrm{Var}(s) = E[s^2] = f_A(1+\\sigma_{s,A}^2) + f_B(1+\\sigma_{s,B}^2) + f_C(1+\\sigma_{s,C}^2)$.\nGiven $\\sigma_{s,A}^2 = \\sigma_{s,B}^2 = \\sigma_{s,C}^2 = \\sigma_s^2$, this is $(f_A+f_B+f_C)(1+\\sigma_s^2) = 1 + \\sigma_s^2$.\n$\\mathrm{Var}(t) = E[t^2] = f_A(1+\\sigma_{t,A}^2) + f_B\\sigma_{t,B}^2 + f_C(1+\\sigma_{t,C}^2)$.\n$\\mathrm{Cov}(s,t) = E[st] = f_A(1) + f_B(0) + f_C(-1) = f_A-f_C$.\nSo, $\\Sigma_{xx} = \\begin{pmatrix} 1+\\sigma_s^2 & f_A-f_C \\\\ f_A-f_C & f_A(1+\\sigma_{t,A}^2) + f_B\\sigma_{t,B}^2 + f_C(1+\\sigma_{t,C}^2) \\end{pmatrix}$.\n\n**Part 1: Compute weights and demonstrate texture bias.**\nWe substitute the given numerical values: $f_A = 0.9$, $f_B = 0.05$, $f_C = 0.05$.\n$\\sigma_s^2 = 2.25$, $\\sigma_{t,A}^2 = 0.0025$, $\\sigma_{t,B}^2 = 1.0$, $\\sigma_{t,C}^2 = 0.0025$.\n\n$f_A - f_C = 0.9 - 0.05 = 0.85$.\n$\\Sigma_{xy} = \\begin{pmatrix} 1 \\\\ 0.85 \\end{pmatrix}$.\n\n$\\mathrm{Var}(s) = 1 + 2.25 = 3.25$.\n$\\mathrm{Var}(t) = 0.9(1+0.0025) + 0.05(1.0) + 0.05(1+0.0025) = 0.9(1.0025) + 0.05 + 0.05(1.0025) = 0.95(1.0025) + 0.05 = 0.952375 + 0.05 = 1.002375$.\n$\\mathrm{Cov}(s,t) = 0.85$.\n$\\Sigma_{xx} = \\begin{pmatrix} 3.25 & 0.85 \\\\ 0.85 & 1.002375 \\end{pmatrix}$.\n\nThe weights are $w = \\Sigma_{xx}^{-1} \\Sigma_{xy}$.\nThe inverse of $\\Sigma_{xx}$ is $\\frac{1}{\\det(\\Sigma_{xx})} \\begin{pmatrix} \\mathrm{Var}(t) & -\\mathrm{Cov}(s,t) \\\\ -\\mathrm{Cov}(s,t) & \\mathrm{Var}(s) \\end{pmatrix}$.\n$\\det(\\Sigma_{xx}) = \\mathrm{Var}(s)\\mathrm{Var}(t) - (\\mathrm{Cov}(s,t))^2 = (3.25)(1.002375) - (0.85)^2 = 3.25771875 - 0.7225 = 2.53521875$.\n$w = \\begin{pmatrix} w_s \\\\ w_t \\end{pmatrix} = \\frac{1}{2.53521875} \\begin{pmatrix} 1.002375 & -0.85 \\\\ -0.85 & 3.25 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0.85 \\end{pmatrix}$.\n\n$w_s = \\frac{1 \\times 1.002375 - 0.85 \\times 0.85}{2.53521875} = \\frac{1.002375 - 0.7225}{2.53521875} = \\frac{0.279875}{2.53521875}$.\n$w_t = \\frac{-0.85 \\times 1 + 3.25 \\times 0.85}{2.53521875} = \\frac{0.85(3.25 - 1)}{2.53521875} = \\frac{0.85 \\times 2.25}{2.53521875} = \\frac{1.9125}{2.53521875}$.\n\nTo demonstrate texture bias ($|w_t| > |w_s|$), we compare the numerators, as the denominator is positive and common to both.\nNumerator of $w_s$: $0.279875$.\nNumerator of $w_t$: $1.9125$.\nSince $1.9125 > 0.279875$, and both are positive, we have $|w_t| > |w_s|$. This demonstrates a texture bias, meaning the model relies more on the texture cue than the shape cue.\n\n**Part 2: Importance reweighting to eliminate texture correlation.**\nWe introduce non-negative importance weights $\\alpha_A, \\alpha_B, \\alpha_C$ for each regime. The linear readout is now trained by minimizing the weighted expected squared error $E[\\alpha(y - w \\cdot x)^2]$. The large-sample solution satisfies the weighted normal equations $w_\\alpha = (E[\\alpha x x^\\top])^{-1} E[\\alpha x y]$.\nThe problem requires eliminating the net correlation between texture and label, which we interpret as setting the texture-label component of the cross-covariance term $E[\\alpha x y]$ to zero. This component is $E[\\alpha t y]$.\n$E[\\alpha t y] = \\sum_{R \\in \\{A,B,C\\}} f_R \\alpha_R E[ty|R]$.\nUsing our previously computed conditional expectations:\n$E[\\alpha t y] = f_A \\alpha_A E[ty|A] + f_B \\alpha_B E[ty|B] + f_C \\alpha_C E[ty|C]$\n$E[\\alpha t y] = f_A \\alpha_A (1) + f_B \\alpha_B (0) + f_C \\alpha_C (-1) = f_A \\alpha_A - f_C \\alpha_C$.\n\nWe set this to zero to fulfill the condition:\n$f_A \\alpha_A - f_C \\alpha_C = 0$.\nThis gives $\\alpha_C = \\frac{f_A \\alpha_A}{f_C}$.\n\nUsing the specified conventions $\\alpha_A = 1$ and $\\alpha_B = 1$, and the dataset composition $f_A = 0.9$ and $f_C = 0.05$, we can solve for $\\alpha_C$:\n$\\alpha_C = \\frac{0.9 \\times 1}{0.05} = \\frac{90}{5} = 18$.\nThe factor $\\alpha_C = 18$ is non-negative as required. This means we must upweight each example from the cue-conflict regime $C$ by a factor of $18$ relative to examples from regimes $A$ and $B$ (which have weights of $1$) to debias the learned association between texture and the label.",
            "answer": "$$\\boxed{18}$$"
        },
        {
            "introduction": "One of the most powerful uses of DCNs as ventral stream models is their role as platforms for *in silico* experimentation, allowing for targeted manipulations that would be infeasible in biological systems. This hands-on coding exercise simulates a classic neuroscience \"lesion study\" to causally probe the function of different feature maps in a network. You will implement a full pipeline to identify critical units using a saliency measure, computationally \"remove\" them, and quantify the resulting category-specific performance deficits, thereby gaining practical experience in using DCNs to test causal hypotheses about neural function .",
            "id": "3973984",
            "problem": "You are given a synthetic representation of Deep Convolutional Network (DCN) feature maps modeling the Ventral Visual Stream (VVS) in the brain. Consider a multiclass scenario with three categories. Each example is represented by a feature vector corresponding to the responses of $D$ feature maps (channels) from a single layer of a DCN. The goal is to quantify how linear decodability of categories changes when removing a set of feature maps identified as critical by a saliency measure, and to relate this change to category-specific deficits.\n\nStart from the following definitions and principles:\n\n- Let there be $C$ categories, indexed by $c \\in \\{0,1,2\\}$. Each example has a feature vector $\\mathbf{x} \\in \\mathbb{R}^D$.\n- Linear decoders are trained in a one-versus-rest manner. For category $c$, define target labels $y_{i}^{(c)} \\in \\{+1,-1\\}$, with $y_{i}^{(c)} = +1$ if example $i$ is of category $c$, and $y_{i}^{(c)} = -1$ otherwise.\n- To include an intercept (bias) without penalizing it, augment the feature vector to $\\tilde{\\mathbf{x}} = [\\mathbf{x}^\\top, 1]^\\top \\in \\mathbb{R}^{D+1}$ and correspondingly define a diagonal penalty matrix $\\mathbf{P} \\in \\mathbb{R}^{(D+1)\\times(D+1)}$ with $\\mathbf{P}_{00} = 0$ for the bias term and $\\mathbf{P}_{kk} = 1$ for all feature dimensions $k \\in \\{1,\\dots,D\\}$.\n- For category $c$, train a ridge-regression decoder by solving\n$$\n\\mathbf{w}^{(c)} = \\arg\\min_{\\mathbf{w} \\in \\mathbb{R}^{D+1}} \\sum_{i} \\left( y_{i}^{(c)} - \\tilde{\\mathbf{x}}_{i}^\\top \\mathbf{w} \\right)^2 + \\lambda \\, \\mathbf{w}^\\top \\mathbf{P} \\mathbf{w},\n$$\nwhich yields the closed-form solution\n$$\n\\mathbf{w}^{(c)} = \\left( \\tilde{\\mathbf{X}}^\\top \\tilde{\\mathbf{X}} + \\lambda \\mathbf{P} \\right)^{-1} \\tilde{\\mathbf{X}}^\\top \\mathbf{y}^{(c)},\n$$\nwhere $\\tilde{\\mathbf{X}}$ is the matrix of augmented training features and $\\mathbf{y}^{(c)}$ is the vector of one-versus-rest labels for category $c$.\n- For a test example with augmented feature $\\tilde{\\mathbf{x}}$, the linear score for category $c$ is $s^{(c)}(\\tilde{\\mathbf{x}}) = \\tilde{\\mathbf{x}}^\\top \\mathbf{w}^{(c)}$. Multiclass prediction is performed by the winner-take-all rule, selecting $\\hat{c} = \\arg\\max_{c} s^{(c)}(\\tilde{\\mathbf{x}})$.\n- Decodability for category $c$ is measured as the fraction of test examples from category $c$ that are correctly predicted by the multiclass decoder. Denote this accuracy by $A_c \\in [0,1]$.\n\nTo identify critical feature maps for category $c$, use a gradient-based saliency measure adapted to linear decoders. The gradient of $s^{(c)}(\\tilde{\\mathbf{x}})$ with respect to the non-bias feature dimensions is the vector of weights $\\mathbf{w}^{(c)}_{\\text{feat}} \\in \\mathbb{R}^D$ excluding the bias term. Define the category-specific saliency for feature $k$ by\n$$\nS_c(k) = \\left| w^{(c)}_{\\text{feat},k} \\right| \\cdot \\mathbb{E}_{\\mathbf{x} \\sim \\text{test}, \\, y=c}\\left[ \\left| x_k \\right| \\right],\n$$\nwhich scales the absolute weight by the average absolute activation of that feature within category $c$ on held-out test data. Define a global saliency by\n$$\nS_{\\text{global}}(k) = \\sum_{c=0}^{C-1} S_c(k).\n$$\n\nFeature removal is defined by selecting a set $\\mathcal{M} \\subset \\{1,\\dots,D\\}$ of size $K$ of the most salient feature indices by either selecting the top-$K$ according to $S_c(k)$ for a specified category $c$ (per-class selection) or according to $S_{\\text{global}}(k)$ (global selection), and then zeroing those feature dimensions in both training and test representations:\n$$\nx_k \\leftarrow 0 \\quad \\text{for all } k \\in \\mathcal{M}.\n$$\n\nFor a given regularization parameter $\\lambda$ and selection mode, compute:\n1. Baseline decodability $A_c^{\\text{base}}$ by training decoders on the standardized original features and evaluating on standardized test features.\n2. Modified decodability $A_c^{\\text{mod}}$ by training decoders on standardized features after zeroing the selected feature dimensions and evaluating on standardized test features after the same zeroing.\n3. The category-specific deficit\n$$\n\\Delta_c = A_c^{\\text{base}} - A_c^{\\text{mod}},\n$$\nexpressed as a decimal in $[ -1, 1 ]$.\n\nDataset generation protocol (fixed and reproducible) for $C=3$ categories and $D=30$ feature maps:\n- Partition the feature indices into three disjoint groups $G_0 = \\{1,\\dots,10\\}$, $G_1 = \\{11,\\dots,20\\}$, and $G_2 = \\{21,\\dots,30\\}$.\n- For each category $c$, draw training examples: for features in $G_c$, sample from a Gaussian distribution with mean $\\mu_{\\text{high}} = 2.0$ and standard deviation $\\sigma = 0.5$; for all other features, sample from a Gaussian distribution with mean $0$ and standard deviation $\\sigma = 0.5$. Do the same for test examples.\n- Use $N_{\\text{train}} = 300$ training examples (equally distributed across categories) and $N_{\\text{test}} = 180$ test examples (equally distributed), and a fixed random seed for reproducibility.\n- Standardize features by z-scoring each feature dimension with statistics computed on the training data only: for each feature $k$, compute the training mean $m_k$ and standard deviation $s_k$; then transform all inputs by $x_k \\leftarrow (x_k - m_k) / s_k$ with the convention that if $s_k = 0$, set $s_k = 1$.\n\nTest suite:\n- Each test case is a tuple $(K, \\lambda, \\text{mode}, c^\\star)$ described as follows:\n    - $K$: number of salient features to remove (an integer).\n    - $\\lambda$: ridge regularization coefficient (a positive float).\n    - $\\text{mode}$: either \"per\\_class\" or \"global\", indicating whether saliency is computed for a specific category $c^\\star$ or across all categories.\n    - $c^\\star$: the category index used for per-class selection (an integer in $\\{0,1,2\\}$); if $\\text{mode} = \\text{global}$, set $c^\\star = -1$ and ignore it.\n- Use the following test cases:\n    1. $(5, 0.01, \\text{\"per\\_class\"}, 0)$: happy-path removal of a small number of category $0$-critical features.\n    2. $(0, 0.01, \\text{\"per\\_class\"}, 0)$: boundary condition with no removal.\n    3. $(10, 0.50, \\text{\"per\\_class\"}, 1)$: edge case removing an entire block of category $1$-diagnostic features under stronger regularization.\n    4. $(12, 0.01, \\text{\"global\"}, -1)$: global removal of cross-category salient features.\n\nYour program should implement the above protocol, compute the deficits $\\Delta_c$ for $c \\in \\{0,1,2\\}$ for each test case, and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The final output should be the flattened list of deficits ordered by test case, then by category index within each test case, i.e., $[\\Delta^{(1)}_0, \\Delta^{(1)}_1, \\Delta^{(1)}_2, \\Delta^{(2)}_0, \\dots, \\Delta^{(4)}_2]$, where superscripts indicate the test case index in the list above.",
            "solution": "The solution proceeds by implementing the specified protocol for each test case. This involves data generation, standardization, training baseline and modified decoders, and finally calculating the resulting performance deficits. For reproducibility, a random seed of $42$ is used.\n\n**Step 1: Data Generation and Preprocessing**\nFirst, we generate the raw training and testing datasets. For $C=3$ categories and $D=30$ features, we have $N_{\\text{train}}=300$ and $N_{\\text{test}}=180$ samples. The feature indices are partitioned into three groups: $G_0$ (indices $0-9$), $G_1$ (indices $10-19$), and $G_2$ (indices $20-29$). For each sample of category $c$, features in group $G_c$ are drawn from a Gaussian distribution $\\mathcal{N}(\\mu=2.0, \\sigma=0.5)$, and all other features are drawn from $\\mathcal{N}(\\mu=0, \\sigma=0.5)$.\n\nNext, we standardize the data. We compute the mean $m_k$ and standard deviation $s_k$ for each feature dimension $k$ from the training set $\\mathbf{X}_{\\text{train}}$. If $s_k=0$, we set $s_k=1$. We then transform both the training and test sets using $x_k \\leftarrow (x_k - m_k)/s_k$. Let these be $\\mathbf{X}_{\\text{train, std}}$ and $\\mathbf{X}_{\\text{test, std}}$.\n\nFinally, we augment the feature vectors by prepending a constant $1$ for the bias term. For any feature vector $\\mathbf{x}_{\\text{std}}$, the augmented vector is $\\tilde{\\mathbf{x}}_{\\text{std}} = [1, \\mathbf{x}_{\\text{std}}^\\top]^\\top \\in \\mathbb{R}^{31}$. This gives us matrices $\\tilde{\\mathbf{X}}_{\\text{train, std}}$ and $\\tilde{\\mathbf{X}}_{\\text{test, std}}$.\n\n**Step 2: Baseline Performance Evaluation ($A_c^{\\text{base}}$)**\nFor each of the four test cases, we first compute the baseline performance. This involves training three one-vs-rest decoders, one for each category $c \\in \\{0, 1, 2\\}$, using the regularization parameter $\\lambda$ specified in the test case. The weight vector for each decoder is calculated using the closed-form solution:\n$$\n\\mathbf{w}^{(c)}_{\\text{base}} = \\left( \\tilde{\\mathbf{X}}_{\\text{train, std}}^\\top \\tilde{\\mathbf{X}}_{\\text{train, std}} + \\lambda \\mathbf{P} \\right)^{-1} \\tilde{\\mathbf{X}}_{\\text{train, std}}^\\top \\mathbf{y}^{(c)}_{\\text{train}}\n$$\nwhere $\\mathbf{y}^{(c)}_{\\text{train}}$ is the vector of one-vs-rest labels ($+1/-1$) for category $c$, and $\\mathbf{P}$ is the $31 \\times 31$ diagonal penalty matrix with $\\mathbf{P}_{00}=0$ and $\\mathbf{P}_{kk}=1$ for $k \\ge 1$.\n\nWith the trained weights, we predict the class for each test sample. For a test sample $\\tilde{\\mathbf{x}}_{\\text{test, std}, i}$, we compute the three scores $s^{(c)}_i = \\tilde{\\mathbf{x}}_{\\text{test, std}, i}^\\top \\mathbf{w}^{(c)}_{\\text{base}}$. The predicted class is $\\hat{c}_i = \\arg\\max_{c} s^{(c)}_i$. The baseline accuracy for category $c$, $A_c^{\\text{base}}$, is the fraction of test samples with true label $c$ that are correctly classified.\n\n**Step 3: Saliency Calculation and Feature Selection**\nUsing the baseline weights $\\mathbf{w}^{(c)}_{\\text{base}}$, we compute the feature saliencies. The feature-specific weights are $\\mathbf{w}^{(c)}_{\\text{feat, base}}$, which are the components of $\\mathbf{w}^{(c)}_{\\text{base}}$ from index $1$ to $30$. For each feature $k \\in \\{1, \\dots, 30\\}$ and category $c$, the saliency is:\n$$\nS_c(k) = \\left| w^{(c)}_{\\text{feat, base},k} \\right| \\cdot \\mathbb{E}_{\\mathbf{x} \\sim \\text{test, std}, \\, y=c}\\left[ \\left| x_k \\right| \\right]\n$$\nThe expectation is estimated as the sample mean of absolute activations over test examples belonging to category $c$.\n\nBased on the test case's `mode` parameter:\n- If `mode` is `\"per_class\"`, we identify the set $\\mathcal{M}$ of $K$ features with the highest saliency scores $S_{c^\\star}(k)$ for the specified category $c^\\star$.\n- If `mode` is `\"global\"`, we compute the global saliency $S_{\\text{global}}(k) = \\sum_{c=0}^{2} S_c(k)$ and select the set $\\mathcal{M}$ of $K$ features with the highest global scores.\n\nFor the special case $K=0$, the set $\\mathcal{M}$ is empty.\n\n**Step 4: Modified Performance Evaluation ($A_c^{\\text{mod}}$)**\nWe create modified datasets, $\\tilde{\\mathbf{X}}_{\\text{train, mod}}$ and $\\tilde{\\mathbf{X}}_{\\text{test, mod}}$, by taking the standardized and augmented datasets and setting to zero all values in the columns corresponding to the feature indices in $\\mathcal{M}$. Note that the bias column (index $0$) is never removed.\n\nWe then train a new set of decoders on the modified training data, $\\tilde{\\mathbf{X}}_{\\text{train, mod}}$, using the same procedure as in Step 2:\n$$\n\\mathbf{w}^{(c)}_{\\text{mod}} = \\left( \\tilde{\\mathbf{X}}_{\\text{train, mod}}^\\top \\tilde{\\mathbf{X}}_{\\text{train, mod}} + \\lambda \\mathbf{P} \\right)^{-1} \\tilde{\\mathbf{X}}_{\\text{train, mod}}^\\top \\mathbf{y}^{(c)}_{\\text{train}}\n$$\nThese new decoders are evaluated on the modified test data, $\\tilde{\\mathbf{X}}_{\\text{test, mod}}$, to compute the modified accuracies $A_c^{\\text{mod}}$.\n\n**Step 5: Deficit Calculation ($\\Delta_c$)**\nFinally, for each category $c$, the deficit is calculated as the difference between the baseline and modified accuracies:\n$$\n\\Delta_c = A_c^{\\text{base}} - A_c^{\\text{mod}}\n$$\nThis entire process is repeated for all four test cases provided, and the resulting twelve deficit values are collected and formatted. For the case $K=0$, $A_c^{\\text{mod}} = A_c^{\\text{base}}$, yielding $\\Delta_c = 0$ for all $c$.\n\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and compute deficits.\n    \"\"\"\n    \n    # Define problem constants\n    C = 3  # Number of categories\n    D = 30  # Number of features\n    N_TRAIN_TOTAL = 300\n    N_TEST_TOTAL = 180\n    N_TRAIN_PER_CLASS = N_TRAIN_TOTAL // C\n    N_TEST_PER_CLASS = N_TEST_TOTAL // C\n    MU_HIGH = 2.0\n    SIGMA = 0.5\n    SEED = 42\n\n    def generate_data(rng):\n        \"\"\"Generates raw training and test data according to the protocol.\"\"\"\n        X_train_raw = np.zeros((N_TRAIN_TOTAL, D))\n        y_train = np.zeros(N_TRAIN_TOTAL, dtype=int)\n        X_test_raw = np.zeros((N_TEST_TOTAL, D))\n        y_test = np.zeros(N_TEST_TOTAL, dtype=int)\n        \n        feature_groups = [range(0, 10), range(10, 20), range(20, 30)]\n\n        # Generate training data\n        for c in range(C):\n            start_idx = c * N_TRAIN_PER_CLASS\n            end_idx = (c + 1) * N_TRAIN_PER_CLASS\n            y_train[start_idx:end_idx] = c\n            \n            # Draw from N(0, 0.5) for all features\n            X_train_raw[start_idx:end_idx, :] = rng.normal(loc=0.0, scale=SIGMA, size=(N_TRAIN_PER_CLASS, D))\n            # For diagnostic features, redraw from N(2.0, 0.5)\n            diagnostic_features = feature_groups[c]\n            X_train_raw[start_idx:end_idx, diagnostic_features] = rng.normal(loc=MU_HIGH, scale=SIGMA, size=(N_TRAIN_PER_CLASS, len(diagnostic_features)))\n\n        # Generate test data\n        for c in range(C):\n            start_idx = c * N_TEST_PER_CLASS\n            end_idx = (c + 1) * N_TEST_PER_CLASS\n            y_test[start_idx:end_idx] = c\n            \n            X_test_raw[start_idx:end_idx, :] = rng.normal(loc=0.0, scale=SIGMA, size=(N_TEST_PER_CLASS, D))\n            diagnostic_features = feature_groups[c]\n            X_test_raw[start_idx:end_idx, diagnostic_features] = rng.normal(loc=MU_HIGH, scale=SIGMA, size=(N_TEST_PER_CLASS, len(diagnostic_features)))\n            \n        return X_train_raw, y_train, X_test_raw, y_test\n\n    def run_single_case(case_params, data):\n        \"\"\"\n        Runs the full analysis for a single test case.\n        \"\"\"\n        K, lam, mode, c_star = case_params\n        X_train_raw, y_train, X_test_raw, y_test = data\n\n        # 1. Standardize data\n        train_mean = np.mean(X_train_raw, axis=0)\n        train_std = np.std(X_train_raw, axis=0)\n        train_std[train_std == 0] = 1.0  # Convention for zero std dev\n\n        X_train_std = (X_train_raw - train_mean) / train_std\n        X_test_std = (X_test_raw - train_mean) / train_std\n\n        # Augment features with bias term\n        X_train_aug = np.hstack([np.ones((N_TRAIN_TOTAL, 1)), X_train_std])\n        X_test_aug = np.hstack([np.ones((N_TEST_TOTAL, 1)), X_test_std])\n        \n        # Penalty matrix P\n        P = np.diag([0.] + [1.] * D)\n\n        def train_and_eval(X_train, X_test):\n            weights = np.zeros((D + 1, C))\n            for c in range(C):\n                y_ovr = np.where(y_train == c, 1, -1)\n                term1 = X_train.T @ X_train + lam * P\n                term2 = X_train.T @ y_ovr\n                weights[:, c] = np.linalg.solve(term1, term2)\n            \n            scores = X_test @ weights\n            predictions = np.argmax(scores, axis=1)\n            \n            accuracies = []\n            for c in range(C):\n                class_mask = (y_test == c)\n                correct_predictions = np.sum(predictions[class_mask] == y_test[class_mask])\n                accuracies.append(correct_predictions / np.sum(class_mask))\n            return np.array(accuracies), weights\n\n        # 2. Baseline performance\n        A_base, W_base = train_and_eval(X_train_aug, X_test_aug)\n\n        if K == 0:\n            return [0.0, 0.0, 0.0]\n\n        # 3. Saliency calculation\n        W_feat_base = W_base[1:, :] # Exclude bias weight\n        saliencies = np.zeros((D, C))\n        for c in range(C):\n            class_mask = (y_test == c)\n            # E[|x_k|] for class c\n            mean_abs_activation = np.mean(np.abs(X_test_std[class_mask, :]), axis=0)\n            saliencies[:, c] = np.abs(W_feat_base[:, c]) * mean_abs_activation\n        \n        if mode == 'per_class':\n            target_saliency = saliencies[:, c_star]\n        else: # 'global'\n            target_saliency = np.sum(saliencies, axis=1)\n\n        # Identify features to remove (indices are 0-based for features 0..D-1)\n        features_to_remove_indices = np.argsort(target_saliency)[-K:]\n\n        # 4. Modified performance\n        X_train_mod = X_train_aug.copy()\n        X_test_mod = X_test_aug.copy()\n        \n        # Zero out feature columns (+1 offset for bias column)\n        X_train_mod[:, features_to_remove_indices + 1] = 0.0\n        X_test_mod[:, features_to_remove_indices + 1] = 0.0\n        \n        A_mod, _ = train_and_eval(X_train_mod, X_test_mod)\n        \n        # 5. Deficit calculation\n        deficits = A_base - A_mod\n        return deficits.tolist()\n\n    # Define test cases\n    test_cases = [\n        (5, 0.01, \"per_class\", 0),\n        (0, 0.01, \"per_class\", 0),\n        (10, 0.50, \"per_class\", 1),\n        (12, 0.01, \"global\", -1),\n    ]\n\n    # Initialize RNG and generate data once\n    rng = np.random.default_rng(SEED)\n    data = generate_data(rng)\n\n    # Run all test cases and collect results\n    all_deficits = []\n    for case in test_cases:\n        deficits = run_single_case(case, data)\n        all_deficits.extend(deficits)\n\n    # Format and print the final output\n    formatted_results = [f\"{d:.8f}\" for d in all_deficits]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```",
            "answer": "[0.46666667,0.01666667,0.00000000,0.00000000,0.00000000,0.00000000,0.00000000,1.00000000,0.00000000,0.51666667,0.48333333,0.55000000]"
        }
    ]
}