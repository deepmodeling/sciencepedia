{
    "hands_on_practices": [
        {
            "introduction": "A core challenge in designing any Brain-Computer Interface (BCI) is faithfully converting continuous analog brain signals into a discrete digital format that a computer can process. This practice focuses on the foundational principles of this conversion, guided by the Nyquist-Shannon sampling theorem. By working through this exercise , you will determine the minimum sampling rate required for an EEG-based BCI, accounting for practical hardware limitations like anti-aliasing filter transition bands, and learn to quantify the risk of aliasing—a critical error where high-frequency noise masquerades as a low-frequency signal.",
            "id": "3966663",
            "problem": "A brain-computer interface (BCI) system aims to capture human electroencephalography rhythms, specifically the mu band spanning $8$–$12$ $\\text{Hz}$ and the beta band spanning $13$–$30$ $\\text{Hz}$, for motor imagery decoding. The analog front-end includes an anti-aliasing low-pass filter whose passband must fully contain the union of these rhythms. The filter exhibits a transition band of width up to $\\Delta f = 5$ $\\text{Hz}$, meaning that starting at a passband edge $f_{p}$, the filter response transitions over a width $\\Delta f$ before entering the stopband. The sampling subsystem is modeled as uniform sampling at frequency $f_{s}$.\n\nUsing the Nyquist–Shannon sampling theorem and the requirement that the continuous-time signal be strictly bandlimited below the Nyquist frequency $f_{s}/2$ prior to sampling, determine the minimum sampling rate $f_{s}^{\\min}$ that guarantees the entire desired passband and the full transition band lie strictly below $f_{s}/2$. Express $f_{s}^{\\min}$ in $\\text{Hz}$.\n\nNext, for a sampler operating at $f_{s} = 100$ $\\text{Hz}$, define the aliasing mapping for an analog frequency $f_{a} \\in (f_{s}/2, f_{s})$ by $f_{\\text{alias}}(f_{a}) = f_{s} - f_{a}$, which gives the frequency to which $f_{a}$ folds in the discrete-time spectrum over $[0, f_{s}/2]$. Define the aliasing risk measure $R$ as the Lebesgue measure (in $\\text{Hz}$) of the set\n$$\n\\mathcal{A} = \\left\\{ f_{a} \\in \\left(\\frac{f_{s}}{2}, f_{s}\\right) \\,\\middle|\\, f_{\\text{alias}}(f_{a}) \\in [8, 30] \\right\\}.\n$$\nCompute $R$ for $f_{s} = 100$ $\\text{Hz}$ and express $R$ in $\\text{Hz}$.\n\nProvide your final numerical results in $\\text{Hz}$. No rounding is required.",
            "solution": "The user has provided a two-part problem concerning the determination of a minimum sampling rate for a Brain-Computer Interface (BCI) system and the calculation of an aliasing risk measure. The problem is scientifically grounded, well-posed, and contains all necessary information for a unique solution.\n\nFirst, we address the determination of the minimum sampling rate, $f_{s}^{\\min}$.\nThe signal of interest for the BCI consists of the mu rhythm in the frequency range $[8, 12]$ $\\text{Hz}$ and the beta rhythm in the range $[13, 30]$ $\\text{Hz}$. The union of these two frequency bands represents the total frequency content that must be preserved. The highest frequency in this union is $f_{\\text{signal,max}} = 30$ $\\text{Hz}$.\n\nThe problem states that the passband of the anti-aliasing low-pass filter must fully contain this union. To determine the minimum required sampling rate, we must consider the most efficient filter design that satisfies this constraint. This corresponds to setting the upper edge of the filter's passband, $f_{p}$, equal to the maximum frequency of the desired signal. Thus, we set $f_{p} = 30$ $\\text{Hz}$.\n\nThe filter has a transition band that starts at the passband edge $f_{p}$ and has a width of up to $\\Delta f = 5$ $\\text{Hz}$. To ensure that the signal is properly bandlimited under the worst-case filter performance, we must assume the maximum possible transition band width. Therefore, the transition band extends from $f_p = 30$ $\\text{Hz}$ to $f_p + \\Delta f = 30 + 5 = 35$ $\\text{Hz}$. The highest frequency component that is not fully attenuated by the filter is at the upper edge of this transition band, $f_{\\text{upper}} = 35$ $\\text{Hz}$.\n\nThe Nyquist-Shannon sampling theorem requires that the sampling frequency $f_{s}$ must be greater than twice the maximum frequency in the signal to avoid aliasing. The problem imposes a stricter condition: \"the entire desired passband and the full transition band lie strictly below the Nyquist frequency $f_{s}/2$\". This means that the highest frequency we must protect, $f_{\\text{upper}}$, must be strictly less than $f_s/2$.\nThis condition is expressed by the inequality:\n$$f_{\\text{upper}} < \\frac{f_{s}}{2}$$\nSubstituting the value for $f_{\\text{upper}}$:\n$$35 < \\frac{f_{s}}{2}$$\nSolving for $f_{s}$, we find:\n$$f_{s} > 70 \\text{ Hz}$$\nThe problem asks for the minimum sampling rate, $f_{s}^{\\min}$. This corresponds to the infimum of the set of allowed sampling rates that satisfy the strict inequality. Therefore, the minimum sampling rate is the boundary value.\n$$f_{s}^{\\min} = 70 \\text{ Hz}$$\n\nNext, we address the second part of the problem: the computation of the aliasing risk measure $R$.\nWe are given a sampling rate of $f_s = 100$ $\\text{Hz}$. The corresponding Nyquist frequency is $f_s/2 = 50$ $\\text{Hz}$.\nThe aliasing mapping for an analog frequency $f_a$ in the range $(f_s/2, f_s) = (50, 100)$ $\\text{Hz}$ is given by:\n$$f_{\\text{alias}}(f_a) = f_s - f_a = 100 - f_a$$\nThe aliasing risk is associated with analog frequencies $f_a$ that alias into the BCI's band of interest. The problem defines this band as $[8, 30]$ $\\text{Hz}$. The set of offending analog frequencies is:\n$$\\mathcal{A} = \\left\\{ f_{a} \\in \\left(50, 100\\right) \\,\\middle|\\, f_{\\text{alias}}(f_{a}) \\in [8, 30] \\right\\}$$\nTo find the set $\\mathcal{A}$, we solve the inequality for $f_a$:\n$$8 \\le f_{\\text{alias}}(f_a) \\le 30$$\nSubstituting the expression for $f_{\\text{alias}}(f_a)$:\n$$8 \\le 100 - f_a \\le 30$$\nThis compound inequality can be split into two parts:\n1. $8 \\le 100 - f_a \\implies f_a \\le 100 - 8 \\implies f_a \\le 92$\n2. $100 - f_a \\le 30 \\implies 100 - 30 \\le f_a \\implies 70 \\le f_a$\n\nCombining these two results, we find that the condition on $f_{\\text{alias}}(f_a)$ restricts $f_a$ to the closed interval $[70, 92]$. We must also satisfy the condition that $f_a \\in (50, 100)$. Since the interval $[70, 92]$ is fully contained within $(50, 100)$, the set $\\mathcal{A}$ is:\n$$\\mathcal{A} = [70, 92]$$\nThe aliasing risk measure $R$ is defined as the Lebesgue measure of the set $\\mathcal{A}$. For an interval, the Lebesgue measure is its length.\n$$R = \\text{measure}(\\mathcal{A}) = 92 - 70 = 22$$\nThe units of $R$ are $\\text{Hz}$.\n\nThe two final numerical results are $f_s^{\\min} = 70$ $\\text{Hz}$ and $R = 22$ $\\text{Hz}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 70 & 22 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Once a neural signal is digitized, the next step is to decode the user's intent, a task often complicated by noise and artifacts. This exercise introduces a powerful method for robust decoding using a state-space model and a recursive Bayesian estimator, exemplified by the Kalman filter. You will derive how the estimator's \"Kalman gain\"—which dictates how strongly the system trusts new measurements—should be optimally adjusted when the signal is contaminated by artifacts . This provides deep insight into the essential trade-off between a decoder's responsiveness to true intent and its stability against spurious noise.",
            "id": "3966609",
            "problem": "Consider an Electroencephalography (EEG)-based Brain-Computer Interface (BCI) decoder modeled by a scalar linear Gaussian state-space process for a single latent control variable. Let the latent state be $x_k \\in \\mathbb{R}$ and the observation be $y_k \\in \\mathbb{R}$. The dynamics are given by the linear model $x_{k+1} = a x_k + w_k$, where $w_k$ is zero-mean Gaussian process noise with variance $q$, and the observation model is $y_k = h x_k + v_k$, where $v_k$ is zero-mean Gaussian measurement noise with variance $r$. Due to transient electromyographic artifact contamination, the observation becomes $y_k = h x_k + v_k + a_k$, where $a_k$ is zero-mean Gaussian artifact noise independent of $v_k$ and $w_k$, with variance $\\sigma_a^2$. Assume a standard recursive Bayesian estimator that minimizes mean-squared error under the linear Gaussian assumptions, and suppose at time $k$ the prior distribution of $x_k$ conditioned on past observations has mean $\\hat{x}_{k|k-1}$ and variance $p = \\operatorname{Var}(x_k \\mid y_{1:k-1})$.\n\nStarting from the definitions of the linear Gaussian model and Bayes’ rule, derive how the inflation of the measurement noise variance by $\\sigma_a^2$ alters the measurement update and, specifically, the gain factor that weights the innovation $y_k - h \\hat{x}_{k|k-1}$. Compute a closed-form analytic expression for the ratio between the artifact-inflated measurement gain and the baseline measurement gain, expressed purely in terms of $h$, $p$, $r$, and $\\sigma_a^2$. Express your final answer as a single simplified analytic expression. No numerical approximation or rounding is required.\n\nAdditionally, qualitatively discuss, based on your derivation, the implications of this variance inflation for decoder responsiveness (how quickly estimates track changes in $y_k$) and estimator stability (resistance to erratic fluctuations), but do not include any qualitative discussion in your final answer.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of Bayesian state estimation, specifically the Kalman filter, which is a standard method in signal processing and computational neuroscience for tasks like BCI decoding. The problem is well-posed, providing all necessary definitions, variables ($a$, $h$, $p$, $r$, $\\sigma_a^2$), and a clear objective. It is free from ambiguity, contradiction, and subjective content.\n\nThe core of the problem lies in the application of the measurement update step of a recursive Bayesian estimator for a linear Gaussian system. For such a system, the estimator that minimizes the mean-squared error is the Kalman filter. The problem is scalar, concerning a single latent state $x_k \\in \\mathbb{R}$ and a single observation $y_k \\in \\mathbb{R}$.\n\nThe filter operates in two steps: a prediction step and a measurement update step. The problem provides the prior distribution of the state at time $k$ conditioned on all past observations up to time $k-1$, denoted $y_{1:k-1}$. This prior is given as a Gaussian distribution with mean $\\hat{x}_{k|k-1}$ and variance $p$:\n$$p(x_k | y_{1:k-1}) = \\mathcal{N}(x_k; \\hat{x}_{k|k-1}, p)$$\n\nThe measurement update step incorporates the new observation $y_k$ to refine this prior into a posterior distribution $p(x_k | y_{1:k})$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$p(x_k | y_{1:k}) \\propto p(y_k | x_k) p(x_k | y_{1:k-1})$$\n\nFor the given linear Gaussian model, the likelihood $p(y_k | x_k)$ is also Gaussian. The observation model is $y_k = h x_k + \\epsilon_k$, where $\\epsilon_k$ is the total measurement noise. Given $x_k$, $y_k$ is distributed as $\\mathcal{N}(y_k; h x_k, R_{\\text{total}})$, where $R_{\\text{total}}$ is the variance of the total measurement noise $\\epsilon_k$. Since the product of two Gaussian distributions is another (unnormalized) Gaussian, the posterior $p(x_k | y_{1:k})$ is also Gaussian.\n\nThe updated (posterior) state estimate $\\hat{x}_{k|k}$ and its variance $p_{k|k}$ are given by the standard Kalman filter measurement update equations. For the scalar case, the posterior mean is:\n$$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (y_k - h \\hat{x}_{k|k-1})$$\nThe term $(y_k - h \\hat{x}_{k|k-1})$ is the innovation or prediction error. The factor $K_k$ is the Kalman gain, which optimally weights this innovation. The gain is calculated to minimize the posterior variance $p_{k|k}$. The formula for the scalar Kalman gain is:\n$$K_k = \\frac{p h}{h^2 p + R_{\\text{total}}}$$\nwhere $p$ is the prior variance $\\operatorname{Var}(x_k \\mid y_{1:k-1})$, and $R_{\\text{total}}$ is the variance of the measurement noise process.\n\nWe will now compute the gain for two cases as specified by the problem.\n\nCase 1: Baseline (no artifact)\nIn the baseline scenario, the observation model is $y_k = h x_k + v_k$. The measurement noise is solely $v_k$, which is a zero-mean Gaussian process with variance $r$. Therefore, the total measurement noise variance is:\n$$R_{\\text{base}} = \\operatorname{Var}(v_k) = r$$\nThe baseline measurement gain, which we denote $K_{\\text{base}}$, is found by substituting $R_{\\text{base}}$ into the gain equation:\n$$K_{\\text{base}} = \\frac{p h}{h^2 p + r}$$\n\nCase 2: Artifact-Inflated\nIn the presence of artifact contamination, the observation model becomes $y_k = h x_k + v_k + a_k$. The total measurement noise is now the sum of the intrinsic measurement noise and the artifact noise: $\\epsilon_k = v_k + a_k$.\nThe problem states that $v_k$ and $a_k$ are independent, zero-mean Gaussian random variables with variances $r$ and $\\sigma_a^2$, respectively. The sum of two independent random variables has a variance equal to the sum of their individual variances. Thus, the variance of the total measurement noise is:\n$$R_{\\text{artifact}} = \\operatorname{Var}(v_k + a_k) = \\operatorname{Var}(v_k) + \\operatorname{Var}(a_k) = r + \\sigma_a^2$$\nThe artifact-inflated measurement gain, denoted $K_{\\text{artifact}}$, is found by substituting this new total variance into the gain equation:\n$$K_{\\text{artifact}} = \\frac{p h}{h^2 p + (r + \\sigma_a^2)}$$\n\nFinally, we compute the required ratio of the artifact-inflated gain to the baseline gain:\n$$\\frac{K_{\\text{artifact}}}{K_{\\text{base}}} = \\frac{\\frac{p h}{h^2 p + r + \\sigma_a^2}}{\\frac{p h}{h^2 p + r}}$$\nThe term $p h$ in the numerator of both expressions cancels out, yielding:\n$$\\frac{K_{\\text{artifact}}}{K_{\\text{base}}} = \\frac{h^2 p + r}{h^2 p + r + \\sigma_a^2}$$\nThis is the final closed-form analytic expression for the ratio, expressed purely in terms of the given variables $h$, $p$, $r$, and $\\sigma_a^2$.\n\nBased on this derivation, we can discuss the qualitative implications. Since the artifact variance $\\sigma_a^2$ is a non-negative quantity (a variance must be $\\ge 0$; for a non-trivial artifact, $\\sigma_a^2 > 0$), the denominator of the ratio, $h^2 p + r + \\sigma_a^2$, is strictly greater than the numerator, $h^2 p + r$. Consequently, the ratio is always less than $1$, meaning $K_{\\text{artifact}} < K_{\\text{base}}$. The presence of the artifact correctly forces the optimal estimator to reduce its gain.\n\nRegarding decoder responsiveness, the Kalman gain $K_k$ determines how much the state estimate is adjusted based on the new measurement. A smaller gain means the estimator places less weight on the incoming (and now less reliable) observation $y_k$ and more weight on its own prediction $\\hat{x}_{k|k-1}$. As a result, the state estimate becomes less sensitive to immediate changes in the measurement. This makes the BCI decoder less responsive, or more \"sluggish,\" in tracking the user's intent as reflected in the signal $y_k$.\n\nRegarding estimator stability, the reduced reliance on the noisy measurement has the benefit of making the estimate smoother and more robust against erratic fluctuations. The artifact noise $a_k$ introduces high-variance, spurious changes in $y_k$. By lowering the gain, the filter effectively filters out more of this noise, preventing it from corrupting the state estimate $x_k$. This increases the stability of the estimator. In essence, the Kalman filter automatically and optimally trades off responsiveness for stability when faced with increased measurement uncertainty. The inflation of the measurement noise variance by $\\sigma_a^2$ is the mechanism by which the filter is informed of this uncertainty.",
            "answer": "$$\\boxed{\\frac{h^2 p + r}{h^2 p + r + \\sigma_a^2}}$$"
        },
        {
            "introduction": "After developing and testing a BCI, we must rigorously assess its performance to ensure the results are not due to chance. This is particularly challenging in BCI research, where analyzing many EEG channels and frequency bands simultaneously—a multiple comparisons problem—greatly increases the risk of false positives. This problem guides you through a state-of-the-art statistical validation technique: a nonparametric permutation test using the maximum statistic to control the family-wise error rate (FWER) . Mastering this practice is essential for producing statistically sound and publishable research in the field of BCI and computational neuroscience.",
            "id": "3966629",
            "problem": "A researcher is evaluating a motor-imagery Brain-Computer Interface (BCI) using multichannel electroencephalography (EEG) recordings. For each EEG channel-band pair, the classification pipeline applies feature extraction followed by a linear classifier and reports balanced accuracy. Let the balanced accuracy be denoted by $A_{\\text{bal}}$, and let the test statistic for a given channel $c$ and band $b$ be defined as $T_{c,b} = A_{\\text{bal}} - 0.5$, where $0.5$ corresponds to chance-level performance for a binary classification task. The null hypothesis is that labels are exchangeable, implying $T_{c,b}$ is centered at $0$ when the labels are permuted.\n\nTo control for multiple comparisons across channels and bands, the researcher uses a nonparametric permutation test with the maximum statistic across all tests: in each permutation $p$, labels are randomly permuted while preserving class counts, the pipeline is re-run identically, and the maximum test statistic across all channel-band pairs is computed, $\\max_{c,b} T_{c,b}^{(p)}$. The experiment includes $C = 32$ channels and $B = 5$ frequency bands, so there are $K = C \\times B$ simultaneous comparisons. The researcher performs $P = 10000$ independent label permutations and records the empirical distribution of the maximum statistic. The one-sided alternative hypothesis is that a specific pair $(c^{\\ast}, b^{\\ast})$ has $T_{c^{\\ast},b^{\\ast}} > 0$.\n\nFor the particular pair $(c^{\\ast}, b^{\\ast})$, the observed test statistic is $T_{c^{\\ast},b^{\\ast}}^{\\text{obs}} = 0.136$. Across the $P = 10000$ permutations, the number of permutation maxima satisfying $\\max_{c,b} T_{c,b}^{(p)} \\geq T_{c^{\\ast},b^{\\ast}}^{\\text{obs}}$ is $m = 214$. Assume exchangeability of labels under the null, and use a finite-sample conservative adjustment so that zero $p$-values do not occur.\n\nCompute the family-wise error rate (FWER) controlled $p$-value for the one-sided test using the empirical maximum-statistic permutation distribution. Round your answer to four significant figures. Express the final value as a decimal without a percentage sign.",
            "solution": "The problem statement is evaluated for validity and is found to be scientifically grounded, well-posed, and objective. It describes a standard and statistically sound procedure for multiple comparisons correction in neuroimaging data analysis, specifically the use of a maximum statistic permutation test to control the family-wise error rate (FWER). All necessary data for the calculation are provided.\n\nThe objective is to compute the FWER-controlled $p$-value for a specific observed test statistic using an empirical distribution derived from permutations. The family-wise error rate is the probability of making at least one Type I error (a false positive) among a family of simultaneous hypothesis tests, under the assumption that the global null hypothesis (i.e., all individual null hypotheses are true) is correct. The maximum statistic permutation testing procedure, as described, is designed to strongly control the FWER.\n\nThe test statistic for a given channel-band pair $(c, b)$ is defined as $T_{c,b} = A_{\\text{bal}} - 0.5$, where $A_{\\text{bal}}$ is the balanced accuracy and $0.5$ represents chance-level performance. The researcher is testing $K = C \\times B = 32 \\times 5 = 160$ hypotheses simultaneously. The specific observed statistic for the pair of interest, $(c^{\\ast}, b^{\\ast})$, is $T_{c^{\\ast},b^{\\ast}}^{\\text{obs}} = 0.136$.\n\nThe core of the method is to compare this observed statistic against the distribution of the *maximum* statistic obtained under the null hypothesis. The null hypothesis of exchangeability implies that permuting the class labels creates datasets consistent with the absence of a true effect. For each of $P = 10000$ permutations, the maximum test statistic across all $K$ pairs is calculated, yielding a set of values $\\{\\max_{c,b} T_{c,b}^{(p)}\\}_{p=1}^{P}$. This set forms the empirical null distribution for the maximum statistic.\n\nThe FWER-controlled $p$-value for the observed statistic $T_{c^{\\ast},b^{\\ast}}^{\\text{obs}}$ is the probability of observing a maximum permuted statistic at least as large as $T_{c^{\\ast},b^{\\ast}}^{\\text{obs}}$. This can be expressed as:\n$$\np_{\\text{FWER}} = \\text{Pr}(\\max_{c,b} T_{c,b}^{(p)} \\geq T_{c^{\\ast},b^{\\ast}}^{\\text{obs}} | H_0)\n$$\nwhere $H_0$ is the global null hypothesis.\n\nAn empirical estimate of this probability is the proportion of permutation maxima that meet this criterion. The problem states that $m = 214$ out of the $P = 10000$ permutations resulted in a maximum statistic greater than or equal to the observed statistic, i.e., instances where $\\max_{c,b} T_{c,b}^{(p)} \\geq T_{c^{\\ast},b^{\\ast}}^{\\text{obs}}$.\n\nThe problem specifies the use of a \"finite-sample conservative adjustment so that zero $p$-values do not occur.\" This refers to the standard practice of including the observed data's outcome as part of the permutation distribution. This adjustment prevents $p$-values of zero, which are nonsensical in a finite permutation sample, and provides a more accurate estimate of the true $p$-value. The formula for this adjusted $p$-value is:\n$$\np_{\\text{FWER}} = \\frac{m + 1}{P + 1}\n$$\nHere, $m$ is the count of permutations where the maximum statistic is greater than or equal to the observed statistic, and $P$ is the total number of permutations. The numerator is incremented to account for the observed statistic itself (which, under the null, is just another draw from the same distribution), and the denominator is incremented to reflect the total number of samples being considered ($P$ permutations plus the original data).\n\nSubstituting the given values:\n$m = 214$\n$P = 10000$\n\nWe compute the $p$-value:\n$$\np_{\\text{FWER}} = \\frac{214 + 1}{10000 + 1} = \\frac{215}{10001}\n$$\n\nNow, we perform the division to obtain the decimal value:\n$$\np_{\\text{FWER}} = \\frac{215}{10001} \\approx 0.02149785021...\n$$\n\nThe problem requires rounding the answer to four significant figures. The first four significant figures are $2$, $1$, $4$, and $9$. The fifth significant figure is $7$. Since $7 \\geq 5$, we round up the fourth significant figure, $9$. This results in a carry-over, changing the last part from $49$ to $50$.\n\nTherefore, the rounded $p$-value is $0.02150$. The trailing zero is significant and must be included.",
            "answer": "$$\\boxed{0.02150}$$"
        }
    ]
}