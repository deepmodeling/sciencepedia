## 引言
记忆是认知能力的核心，但其背后复杂的神经机制长期以来是科学界面临的一大挑战。大脑如何在瞬息万变的环境中快速学习新知识，同时又稳定地保存长达数十年的记忆？[计算模型](@entry_id:637456)为我们提供了一套严谨的数学语言和理论框架，用以揭示这些看似矛盾的功能背后的统一原理。本文旨在系统性地梳理记忆的[计算模型](@entry_id:637456)，帮助读者构建一个从微观突触到宏观脑系统的完整认知图景。在接下来的内容中，我们将首先在“原理与机制”一章中，深入探讨神经表征、联想记忆和突触可塑性的核心理论；随后，在“应用与跨学科连接”一章中，我们将展示这些理论如何应用于解释[空间导航](@entry_id:173666)、临床疾病乃至人工智能的最新进展；最后，通过“动手实践”环节，读者将有机会亲手实现并验证这些关键模型。

## 原理与机制

本章旨在深入探讨记忆[计算模型](@entry_id:637456)的基础原理和底层机制。继导论之后，我们将从记忆的计算定义与[神经表征](@entry_id:1128614)的基本单元出发，逐步构建起关联记忆的动态机制，并阐述突触可塑性的核心法则。最终，我们将把这些构件整合到系统层面，探讨学习系统所面临的根本挑战及其精巧的解决方案。

### 神经表征与记忆的基本概念

在对记忆进行建模之前，我们必须首先建立一个精确的计算框架来描述记忆是什么以及它如何由神经活动来承载。

#### [记忆系统](@entry_id:273054)的计算定义

在[计算神经科学](@entry_id:274500)中，一个[记忆系统](@entry_id:273054)可以被形式化地描述为一个动态系统，其核心在于内部状态的维持与演化。我们可以将一个记忆模型抽象为一个元组 $(\mathcal{X}, \mathcal{S}, \mathcal{Y}, f_\theta, g_\theta)$，其中：
- $\mathcal{X}$ 是输入空间，代表进入系统的刺激或信息。
- $\mathcal{S}$ 是系统的内部[状态空间](@entry_id:160914)，是记忆的载体。
- $\mathcal{Y}$ 是输出空间，代表系统的行为或回忆出的信息。
- $f_\theta : \mathcal{S} \times \mathcal{X} \to \mathcal{S}$ 是一个由参数 $\theta$（通常代表突触权重）决定的状态更新规则。
- $g_\theta : \mathcal{S} \to \mathcal{Y}$ 是一个从内部状态到输出的读出函数。

基于这个框架，我们可以对不同类型的记忆进行计算层面的区分 。

- **[工作记忆](@entry_id:894267) (Working Memory)** 负责临时维持与当前任务相关的信息。在计算上，这要求状态 $\mathbf{s}_t \in \mathcal{S}$ 能够在没有持续输入的情况下维持数秒。这通常通过[循环神经网络](@entry_id:634803)（Recurrent Neural Networks, RNNs）的动态特性来实现，其中状态[更新函数](@entry_id:275392) $f_\theta$ 支持**[持续性活动](@entry_id:908229) (persistent activity)**，例如通过具有接近于1的特征值的动力学模式（近中性本征模）来维持状态 $\mathbf{s}_t$，而不需要改变突触参数 $\theta$。由于工作记忆常需表征连续变量（如空间位置），它倾向于使用**分布式群体编码 (distributed population codes)**。

- **[情景记忆](@entry_id:173757) (Episodic Memory)** 负责快速编码和检索特定的、自传式的事件。这要求突触参数 $\theta$ 能够进行快速、甚至一次性的改变。为了存储大量独特的情景并最小化它们之间的干扰，模型（如[海马体](@entry_id:152369)模型）通常假定[状态空间](@entry_id:160914) $\mathcal{S}$ 是一个高维空间，其中每个情景被编码为**稀疏分布式编码 (sparse distributed codes)**。[稀疏性](@entry_id:136793)确保了不同记忆表征之间的重叠度极低，从而提高了存储容量并减少了干扰。检索过程则常被建模为**[吸引子动力学](@entry_id:1121240) (attractor dynamics)**，通过[模式补全](@entry_id:1129444)机制，一个部分的线索可以恢复整个记忆。

- **语义记忆 (Semantic Memory)** 存储的是关于世界的[一般性](@entry_id:161765)知识、事实和概念，这些知识是从大量经验中抽象出来的。这对应于突触参数 $\theta$ 的**缓慢、渐进式学习**过程。通过在众多情景中提取统计规律，系统学习到概念的**分布式特征嵌入 (distributed feature embeddings)**。相似的概念在[状态空间](@entry_id:160914) $\mathcal{S}$ 中拥有相似的活动模式，这种共享的子结构是实现**泛化 (generalization)** 的基础，即能够对新实例进行推理。

- **[程序性记忆](@entry_id:153564) (Procedural Memory)** 涉及技能、习惯和策略的学习（“如何做”的知识）。在计算上，这属于强化学习（Reinforcement Learning, RL）的范畴。系统通过与环境交互学习一个策略 $\pi_\theta$ 或一个动作[价值函数](@entry_id:144750) $Q_\theta$，以最大化未来的奖励。状态更新和读出函数 $f_\theta$ 和 $g_\theta$ 共同实现了这个策略或价值函数。为了在相似但新颖的情境中泛化已学到的技能，[程序性记忆](@entry_id:153564)同样依赖于状态和动作的**分布式特征表征**。

#### [神经编码](@entry_id:263658)的本质：稀疏分布式编码

[神经编码](@entry_id:263658)的选择——即信息如何在神经元活动模式中表示——对[记忆系统](@entry_id:273054)的性能有深远影响。其中，**稀疏分布式编码 (sparse distributed coding)** 是一种尤为高效的策略，它指的是每个记忆或概念由一小部分（即稀疏的，$a \ll 1$）但数量众多（即分布式的）的神经元激活来表示 。

稀疏编码的核心优势在于它显著降低了不同记忆模式之间的**表征重叠 (representational overlap)**。在一个拥有 $N$ 个神经元的网络中，两个独立的、随机生成的稀疏模式（激活概率为 $a$）的预期重叠（即同时被激活的神经元数量）大约为 $a^2N$。当 $a$ 很小时，这个值非常低，使得不同记忆的神经表征近乎正交。

这种低重叠特性带来了两个关键好处。首先，它极大地减小了**[串扰噪声](@entry_id:1123244) (crosstalk noise)**。在联想记忆模型中，当试图回忆一个目标模式时，存储在同一组突触权重中的其他模式会产生干扰。[稀疏编码](@entry_id:180626)使得这种干扰的方差（其标准差大约与 $a\sqrt{MN}$ 成正比，其中 $M$ 是存储的模式数量）相对于信号（与 $aN$ 成正比）来说非常小。

其次，也是更重要的，稀疏性显著提高了网络的**存储容量**。通过对信号和噪声的细致统计分析可以推导出，在一个采用均值减去型赫布规则的自联想网络中，其最大存储容量 $M_{\max}$ 的缩放关系近似为：
$$
M_{\max} \propto \frac{N}{a|\ln a|}
$$
当稀疏度增加（即 $a \to 0$）时，分母 $a|\ln a|$ 趋向于零，导致存储容量 $M_{\max}$ 大幅增加。因此，通过采用[稀疏编码](@entry_id:180626)，神经网络可以在不牺牲保真度的前提下存储远超密集编码的记忆数量。这一原理被认为是海马体等脑区能够高效存储大量[情景记忆](@entry_id:173757)的计算基础。

### 联想记忆的核心机制

联想记忆的核心思想是通过神经元之间的连接权重来存储信息模式之间的关联。当呈现一个线索时，网络能够通过其内部动力学自动地“联想”到完整的、相关的模式。

#### 联想式回忆：内容可寻址存储器

大脑的记忆检索方式与计算机有着本质的不同。计算机内存是**基于地址的存储器 (Address-Based Memory, ABM)**，信息通过一个唯一的、与其内容无关的地址（索引）来精确访问。而大脑的[记忆系统](@entry_id:273054)，尤其是联想记忆，则被认为是**内容可寻址存储器 (Content-Addressable Memory, CAM)** 。

在CAM中，记忆不是通过地址来索引，而是通过其自身的内容。你可以提供一个记忆的片段或一个与之相关联的线索（即部分内容），网络便能够自动地、动态地演化到与该线索最匹配的完整记忆状态。这一过程被称为**[模式补全](@entry_id:1129444) (pattern completion)**。例如，闻到一种熟悉的气味可能会唤起一段完整的童年记忆。这种从部分线索中恢复完整信息的能力是联想记忆的标志性特征，也是其鲁棒性的关键来源。

#### 吸引子网络假说

实现内容可寻址记忆的[经典计算](@entry_id:136968)模型是**吸引子网络 (attractor network)**。这个假说认为，记忆被编码为神经网络[状态空间](@entry_id:160914)中的稳定状态，即**[吸引子](@entry_id:270989)**。

想象一个由 $N$ 个神经元组成的网络，其状态可以用一个 $N$ 维向量 $\mathbf{s}$ 来表示，该向量位于一个巨大的[状态空间](@entry_id:160914)中。学习过程通过调整神经元之间的突触权重，在这个[状态空间](@entry_id:160914)中“雕刻”出一些特定的“凹地”或“山谷”。每一个这样的“山谷”底部就是一个[吸引子](@entry_id:270989)，对应一个被存储的记忆模式。

当一个外部线索（一个初始的神经活动模式）呈现给网络时，它将网络的状态置于[状态空间](@entry_id:160914)的某个点。如果这个点落入某个“山谷”的斜坡上，即**吸引盆 (basin of attraction)**，网络的内部动力学（神经元之间相互作用）就会像一个滚下山坡的小球一样，驱动网络状态向“谷底”演化，最终收敛到那个稳定的[吸引子](@entry_id:270989)状态。这个收敛过程，就是[模式补全](@entry_id:1129444)和记忆检索的动态体现。

#### 记忆的“能量学”：[霍普菲尔德网络](@entry_id:1126163)与[李雅普诺夫函数](@entry_id:273986)

[吸引子网络](@entry_id:1121242)的动态收敛行为可以通过**[霍普菲尔德网络](@entry_id:1126163) (Hopfield network)** 模型进行严格的数学描述 。John Hopfield 在1982年引入了一个“能量函数”的概念，为理解这类网络的稳定性提供了深刻的见解。

对于一个由 $N$ 个二值（例如，$s_i \in \{-1, +1\}$）神经元组成的网络，其能量函数（或称[李雅普诺夫函数](@entry_id:273986)）可以定义为：
$$
E(\mathbf{s}) = -\frac{1}{2} \sum_{i \neq j} W_{ij} s_i s_j - \sum_{i=1}^{N} b_i s_i
$$
其中，$s_i$ 是神经元 $i$ 的状态，$W_{ij}$ 是神经元 $j$ 到 $i$ 的突触权重，$b_i$ 是神经元 $i$ 的偏置（或阈值）。

Hopfield证明，如果突触权重矩阵是对称的（即 $W_{ij} = W_{ji}$）且对角线为零（$W_{ii} = 0$），并且神经元采用[异步更新](@entry_id:266256)规则（即一次只更新一个神经元的状态），那么网络的每一次状态更新都会使能量函数 $E(\mathbf{s})$ 的值非增。具体来说，当一个神经元的状态发生改变（翻转）时，能量值**必然会减小**；如果状态不改变，能量值则保持不变。

这个性质至关重要，因为它保证了网络动力学不会陷入无限循环。由于网络的状态总数是有限的（$2^N$），而能量值在每次状态变化时都单调递减且有下界，所以网络最终必须收敛到一个能量的局部最小值。这些能量的局部最小值，正是在吸引子网络假说中我们所说的**[吸引子](@entry_id:270989)**或**固定点 (fixed points)**，它们代表了稳定存储的记忆模式。因此，能量函数为吸引子网络的稳定检索和[模式补全](@entry_id:1129444)能力提供了坚实的数学基础。

#### 竞争性功能：[模式分离](@entry_id:199607)与[模式补全](@entry_id:1129444)的权衡

[记忆系统](@entry_id:273054)，特别是像海马体这样的结构，被认为执行两种既关键又相互竞争的功能：**[模式补全](@entry_id:1129444) (pattern completion)** 和 **[模式分离](@entry_id:199607) (pattern separation)** 。

- **[模式补全](@entry_id:1129444)**正如前述，是通过[吸引子动力学](@entry_id:1121240)从不完整或带噪声的线索中恢复完整、准确的记忆。
- **[模式分离](@entry_id:199607)**则是将相似的输入映射到非常不同的、不重叠的神经表征上。这个功能对于区分相似但不相同的事件或情景至关重要（例如，你今天停车的位置和昨天停车的位置）。

这两种功能之间存在一个固有的权衡，而这个权衡与[神经编码](@entry_id:263658)的稀疏度 $a$ 密切相关。

- **[模式分离](@entry_id:199607)的实现**：当两个不同的记忆模式被存储时，它们之间的重叠度越小，就越容易被区分。正如我们之前分析的，两个独立随机模式的预期重叠度为 $a^2N$。因此，**增加稀疏度（减小 $a$）** 会使不同记忆的表征变得更加正交，从而增强了[模式分离](@entry_id:199607)的能力。随机出现高重叠（即“假匹配”）的概率会随着 $a$ 的减小而呈指数级下降。

- **对[模式补全](@entry_id:1129444)的影响**：[模式补全](@entry_id:1129444)依赖于输入线索与目标记忆之间有足够的“信号”重叠来将网络状态拉入正确的吸引盆。假设一个部分线索包含了目标记忆中 $\rho$ 比例的激活神经元，那么这个线索与目标记忆的信号重叠度大约为 $\rho a N$。如果网络的检索阈值为 $\theta$，那么成功补全需要 $\rho a N > \theta$。对于固定的线索完整度 $\rho$ 和阈值 $\theta$，**增加稀疏度（减小 $a$）** 会降低信号重叠的绝对大小，可能使其降到阈值以下，从而导致[模式补全](@entry_id:1129444)失败。

因此，一个根本性的权衡出现了：过于稀疏的编码虽然极大地有利于[模式分离](@entry_id:199607)，但也可能损害系统从部分线索中进行[模式补全](@entry_id:1129444)的能力。[记忆系统](@entry_id:273054)必须在这个权衡中找到一个最佳的平衡点，以同时支持这两种重要的功能。

### 突触可塑性原理：记忆如何形成

记忆的物理基础被认为是神经元之间连接强度的改变，即**[突触可塑性](@entry_id:137631) (synaptic plasticity)**。学习规则规定了这些改变如何根据神经活动发生。

#### 赫布学习：“一起放电的神经元连接在一起”

1949年，[Donald Hebb](@entry_id:1123912) 提出了一个影响深远的假说，通常被概括为“**一起放电的神经元连接在一起 (neurons that fire together, wire together)**”。这个假说为学习的细胞机制提供了第一个理论框架。在计算上，一个简单的**[赫布学习](@entry_id:156080)规则 (Hebbian learning)** 可以表示为，突触权重 $\mathbf{w}$ 的改变与突触前活动 $\mathbf{x}$ 和突触后活动 $y$ 的相关性成正比：
$$
\Delta \mathbf{w} = \eta \, y \, \mathbf{x}
$$
其中 $\eta$ 是一个小的正学习率。这个规则直观地实现了关联学习：如果输入 $\mathbf{x}$ 的活动成功地驱动了输出神经元 $y$ 的活动，那么它们之间的连接就会被加强，使得未来同样的输入更有可能再次激活该输出。

#### [赫布学习](@entry_id:156080)的不稳定性与[稳态](@entry_id:139253)需求

然而，纯粹的[赫布学习](@entry_id:156080)有一个致命的缺陷：它是**不稳定**的。对于任何具有非零方差的输入，赫布规则平均上会导致突触权重的范数 $\|\mathbf{w}\|$ 无限增长 。这是因为权重更新项在期望上是正定的。这种失控的增长在生物学上是不现实的，并且会导致[神经元活动](@entry_id:174309)饱和，从而失去处理信息的能力。因此，任何一个可行的学习系统都必须包含某种形式的**[稳态机制](@entry_id:141716) (homeostatic mechanism)** 来约束或归一化突触权重的增长。

#### 奥哈规则：一种生物学上合理的归一化

为了解决[赫布学习](@entry_id:156080)的不稳定性，Erkki Oja 在1982年提出了一个优雅的修正，现在被称为**奥哈规则 (Oja's rule)**。这个规则在赫布项的基础上增加了一个与突触后活动平方成比例的“遗忘”或“衰减”项：
$$
\Delta \mathbf{w} = \eta (y \mathbf{x} - y^2 \mathbf{w})
$$
这个规则的关键优势在于它的**局部性 (locality)**：计算更新只需要突触前活动 $\mathbf{x}$、突触后活动 $y$ 和局部的突触权重值 $\mathbf{w}$，这使其在生物学上是可行的。

奥哈规则通过一个负反馈机制巧妙地实现了权重的归一化。可以证明，在期望上，这个规则会驱动权重[向量的范数](@entry_id:154882) $\|\mathbf{w}\|$ 稳定在1附近。当 $\|\mathbf{w}\| > 1$ 时，更新项的期望为负，使权重减小；当 $\|\mathbf{w}\|  1$ 时，期望为正，使权重增大。

更有趣的是，奥哈规则不仅仅是稳定了权重。当应用于一个线性神经元时，它会使权重向量 $\mathbf{w}$ 收敛到输入[数据协方差](@entry_id:748192)矩阵 $\mathbf{C} = \mathbb{E}[\mathbf{x}\mathbf{x}^\top]$ 的**[主特征向量](@entry_id:264358)**上，也就是数据中方差最大的方向。因此，奥哈规则将赫布式的关联学习与一种基本的无监督[特征提取](@entry_id:164394)方法——**[主成分分析](@entry_id:145395) (Principal Component Analysis, PCA)** ——联系了起来 。

#### 引入时间精度：[脉冲时间依赖可塑性 (STDP)](@entry_id:148242)

赫布学习和奥哈规则通常是在**发放率模型 (rate-based models)** 的框架下描述的，其中神经活动被表示为平均发放频率。然而，大脑中的信息通常也编码在单个脉冲的**精确时间**上。**脉冲时间依赖可塑性 (Spike-Timing-Dependent Plasticity, STDP)** 是一个描述突触权重如何根据突触前和突触后脉冲的相对时间进行调整的学习规则 。

典型的STDP表现为一种**双相学习窗口 (biphasic learning window)**：
- 如果一个突触前脉冲在突触后脉冲**之前**到达（$\Delta t = t_{\mathrm{post}} - t_{\mathrm{pre}} > 0$），并且时间间隔很短（通常在几十毫秒内），那么突触权重会得到增强，这个过程称为**长时程增强 (Long-Term Potentiation, LTP)**。这符合赫布的因果关系直觉：突触前神经元对突触后神经元的放电有贡献。
- 相反，如果突触前脉冲在突触后脉冲**之后**到达（$\Delta t  0$），那么突触权重会被削弱，这个过程称为**长时程抑制 (Long-Term Depression, LTD)**。

这个学习窗口 $W(\Delta t)$ 通常可以用一个双指数函数来建模：
$$
W(\Delta t) = \begin{cases} A_+ e^{-\Delta t/\tau_+},  \Delta t > 0 \\ -A_- e^{\Delta t/\tau_-},  \Delta t  0 \end{cases}
$$
其中 $A_+, A_-, \tau_+, \tau_-$ 都是正常数，分别决定了LTP和LTD的幅度和时间尺度。

与速率模型中的赫布学习一样，STDP也需要一个稳定性条件。在不相关的泊松脉冲活动下，突触权重的平均漂移与学习窗口的积分 $\int_{-\infty}^{\infty} W(\Delta t) d\Delta t$ 成正比。为了防止失控的权重增长，这个积分必须是非正的。在许多典型的STDP模型中，LTD的总面积超过LTP的总面积（即 $A_-\tau_- > A_+\tau_+$），这确保了在随机活动下，突触权重有一个朝向减弱的净漂移，从而实现了稳定性。

### 系统层面的挑战与解决方案

将上述原理组合成一个完整的、能够在复杂环境中[持续学习](@entry_id:634283)的[记忆系统](@entry_id:273054)，会遇到一些根本性的挑战。

#### 稳定-可塑性困境

所有自适应系统都面临一个核心的权衡，即**稳定-可塑性困境 (stability-plasticity dilemma)** 。系统必须具有**可塑性**，以便能够快速有效地学习新的信息；同时，它也必须具有**稳定性**，以防止新学习的内容破坏或覆盖已经存储的旧记忆。一个[学习率](@entry_id:140210)过高的系统会遭受所谓的**[灾难性遗忘](@entry_id:636297) (catastrophic forgetting)**，而一个[学习率](@entry_id:140210)过低的系统则无法适应变化。

#### 区分干扰与[灾难性遗忘](@entry_id:636297)

为了精确地解决这个问题，区分两个相关但不同的概念至关重要：**干扰 (interference)** 和 **[灾难性遗忘](@entry_id:636297) (catastrophic forgetting)** 。

- **干扰** 是一个**检索时**的问题，源于**表征重叠**。当多个记忆被叠加存储在同一组突触权重上时，如果它们的[神经表征](@entry_id:1128614)不是完全正交的，那么在回忆一个目标记忆时，其他记忆的痕迹就会产生[串扰噪声](@entry_id:1123244)，导致回忆出错。即使突触权重是固定的，这种干扰也存在。

- **[灾难性遗忘](@entry_id:636297)** 是一个**学习时**的问题，源于**更新引发的覆盖 (update-induced overwriting)**。当系统学习一个新任务（如任务B）时，用于优化任务B性能的[梯度下降](@entry_id:145942)更新，可能会修改对旧任务（任务A）至关重要的突触权重。从数学上讲，如果任务A和任务B的[损失函数](@entry_id:634569)梯度在[参数空间](@entry_id:178581)中指向相反的方向（即它们的[内积](@entry_id:750660)为负），那么为任务B进行的一步学习将必然增加任务A的损失，从而“遗忘”了任务A。

#### 解决困境的机制

为了在不牺牲可塑性的前提下保证稳定性，大脑和[计算模型](@entry_id:637456)采用了多种精巧的策略。

- **门控学习 (Gated Learning)**：该机制通过一个“门控”信号来动态调节学习率，从而在时间上控制可塑性。这个门控信号通常与新颖性、注意力或强化信号（如多巴胺）相关。只有当系统判定当前信息是重要的、值得学习的时候，学习的“大门”才会打开（例如，在我们的通用更新规则中，门控因子 $g(t)$ 变为1）。在其他时间，学习被关闭或抑制，从而保护现有记忆免受无关信息的持续冲刷 。

- **元可塑性 (Metaplasticity)**：这个概念指的是“可塑性的可塑性”，即突触改变自身规则的能力。它通过使学习率或可塑性阈值依赖于突触自身的活动历史来调节可塑性。例如，一个被持续、稳定地加强的突触（可能是一个巩固良好的旧记忆的一部分）可以进入一个“深层”状态，其有效学习率会降低，从而变得更加稳定，不易被改变。相反，一个不活跃或经历不一致活动的突触则保持在“浅层”状态，具有较高的[学习率](@entry_id:140210)，随时准备编码新信息。[BCM理论](@entry_id:177448)中的**滑动阈值**是另一个例子，它根据突触后活动的慢速平均值来调整LTP/LTD的诱导阈值，从而实现[稳态](@entry_id:139253)。这些机制允许突触根据其“重要性”或“年龄”来差异化地调节其可塑性 。

#### [互补学习系统](@entry_id:926487)（CLS）框架

**[互补学习系统](@entry_id:926487) (Complementary Learning Systems, CLS)** 理论提出了一个宏大的系统级架构方案，以解决稳定-可塑性困境 。该理论主张，大脑拥有两个功能和特性互补的[记忆系统](@entry_id:273054)，主要对应于**[海马体](@entry_id:152369)**和**新皮层**。

- **海马体系统：快速、稀疏的学习者**。[海马体](@entry_id:152369)被认为是快速学习系统，能够一次性地编码特定的[情景记忆](@entry_id:173757)。这之所以可能，是因为它使用了高度稀疏和[模式分离](@entry_id:199607)的神经表征。如前所述，[稀疏编码](@entry_id:180626)（用小的 $\epsilon$ 表示其低重叠性）使得干扰极小。因此，[海马体](@entry_id:152369)可以采用一个非常高的[学习率](@entry_id:140210) $\alpha_{\mathrm{H}}$ 来快速形成新的记忆痕迹，而不会立即破坏旧的记忆。

- **新皮层系统：缓慢、结构的整合者**。新皮层则被认为是慢速学习系统，其任务是从大量经验中提取统计规律，形成结构化的语义知识。新皮层中的表征是高度分布式的，并且有很大的重叠（用大的 $\rho$ 表示其高重叠性），这种重叠对于泛化至关重要。然而，也正是这种高重叠性使得新皮层极易受到[灾难性遗忘](@entry_id:636297)的影响。为了维持其知识库的稳定性，新皮层必须采用一个非常低的[学习率](@entry_id:140210) $\alpha_{\mathrm{C}}$。

- **系统级巩固与回放**：这两个系统通过一个称为**系统级巩固 (systems consolidation)** 的过程协同工作。海马体在快速编码一个新情景后，会在“离线”状态（如睡眠期间）反复**回放 (replay)** 该记忆的神经活动模式。每一次回放都为新皮层提供了一个学习样本。通过将新旧记忆的样本交错回放，新皮层能够以其缓慢的学习率，逐渐地、非破坏性地将新信息整合到其庞大的知识结构中。

CLS框架优雅地统一了本章讨论的多个核心概念——稀疏与分布式编码、干扰与[灾难性遗忘](@entry_id:636297)、快慢两种[学习率](@entry_id:140210)——形成了一个关于大脑如何同时实现快速学习和稳定记忆的、具有强大解释力的理论。