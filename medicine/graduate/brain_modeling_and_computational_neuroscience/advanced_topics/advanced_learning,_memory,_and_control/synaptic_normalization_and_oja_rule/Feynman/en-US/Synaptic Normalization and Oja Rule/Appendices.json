{
    "hands_on_practices": [
        {
            "introduction": "Before delving into the specifics of Oja's rule, it is crucial to understand the fundamental problem it was designed to solve. Simple mathematical formulations of Hebbian plasticity, while elegant, often suffer from runaway synaptic growth, leading to instability. This practice  invites you to analyze this classic issue by comparing the dynamics of the standard correlation-based Hebbian rule with the covariance rule. By deriving the expected weight dynamics for each, you will discover why normalization is not merely an optional feature but a necessary mechanism for stable feature extraction, especially when inputs have a non-zero mean.",
            "id": "4025537",
            "problem": "A single linear neuron receives an input vector $\\mathbf{x} \\in \\mathbb{R}^n$ drawn independently over time from a stationary distribution with mean $\\boldsymbol{\\mu} = \\mathbb{E}[\\mathbf{x}] \\neq \\mathbf{0}$ and covariance matrix $\\mathbf{C} = \\mathbb{E}\\left[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})^\\top\\right]$, where $\\mathbf{C}$ is symmetric positive semidefinite. The neuron produces output $y = \\mathbf{w}^\\top \\mathbf{x}$, where $\\mathbf{w} \\in \\mathbb{R}^n$ are the synaptic weights. Consider the following synaptic plasticity rules with small learning rate $\\eta > 0$ in the continuous-time limit, where $\\Delta \\mathbf{w}$ denotes the increment over a small time step:\n\n- Correlation Hebbian rule: $\\Delta \\mathbf{w} = \\eta\\, y\\, \\mathbf{x}$.\n- Covariance Hebbian rule: $\\Delta \\mathbf{w} = \\eta\\, y\\, (\\mathbf{x} - \\boldsymbol{\\mu})$.\n- Oja's rule (a local synaptic normalization rule): $\\Delta \\mathbf{w} = \\eta\\, y\\, (\\mathbf{x} - y\\, \\mathbf{w})$.\n\nStarting from the definitions of mean and covariance and the linear neuron model, analyze the expected weight dynamics and norm behavior for the correlation Hebbian rule and the covariance Hebbian rule when $\\boldsymbol{\\mu} \\neq \\mathbf{0}$, and interpret how Oja's rule changes stability and asymptotic behavior with and without mean-centering of inputs. Assume standard regularity conditions ensuring interchange of expectation and differentiation in the small-$\\eta$ limit, and that the covariance matrix $\\mathbf{C}$ has at least one strictly positive eigenvalue.\n\nWhich of the following statements are correct?\n\nA. Under the correlation Hebbian rule, the expected weight dynamics satisfy $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\left(\\mathbf{C}\\,\\mathbf{w} + (\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}\\right)$, which implies an unbounded drift along $\\boldsymbol{\\mu}$ when $\\boldsymbol{\\mu} \\neq \\mathbf{0}$; there are no finite-norm stable fixed points without additional normalization.\n\nB. Under the covariance Hebbian rule, the expected weight dynamics satisfy $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\, \\mathbf{C}\\,\\mathbf{w}$, so the direction of $\\mathbf{w}$ aligns with the eigenvector of $\\mathbf{C}$ associated with its largest eigenvalue, but the norm $\\lVert \\mathbf{w} \\rVert$ grows without bound in the absence of normalization.\n\nC. The covariance Hebbian rule alone guarantees globally stable convergence of $\\mathbf{w}$ to a bounded fixed point for any positive semidefinite $\\mathbf{C}$, regardless of the value of $\\boldsymbol{\\mu}$.\n\nD. Replacing either Hebbian rule with Oja's rule, the expected dynamics become $\\dfrac{d\\mathbf{w}}{dt} = \\mathbf{C}\\,\\mathbf{w} - (\\mathbf{w}^\\top \\mathbf{C}\\,\\mathbf{w})\\, \\mathbf{w}$ when inputs are mean-centered ($\\boldsymbol{\\mu} = \\mathbf{0}$), yielding stable convergence of $\\mathbf{w}$ to a unit-norm eigenvector of $\\mathbf{C}$ corresponding to the largest eigenvalue; with $\\boldsymbol{\\mu} \\neq \\mathbf{0}$ and raw inputs, an extra term $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ appears in the drift that biases the solution toward $\\boldsymbol{\\mu}$ unless inputs are centered.\n\nE. The correlation Hebbian rule is stable when $\\boldsymbol{\\mu} \\neq \\mathbf{0}$, because the term $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ acts as a weight decay that reduces $\\lVert \\mathbf{w} \\rVert$.",
            "solution": "The problem statement presents a standard analysis of several Hebbian-type synaptic plasticity rules in computational neuroscience. The definitions, assumptions, and physical context are consistent with established literature in the field. The setup is mathematically well-defined and self-contained. All terms are standard and their relationships are correctly specified. The problem is scientifically grounded, well-posed, and objective.\n\nThe analysis proceeds by deriving the expected dynamics for each learning rule in the continuous-time limit. For a small learning rate $\\eta$, the evolution of the weight vector $\\mathbf{w}$ can be approximated by the ordinary differential equation (ODE) $\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}\\left[\\frac{\\Delta \\mathbf{w}}{\\eta}\\right]$, where the expectation is taken over the distribution of the input vector $\\mathbf{x}$.\n\nFirst, we establish a key identity relating the second moment matrix $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top]$ to the covariance matrix $\\mathbf{C}$ and the mean vector $\\boldsymbol{\\mu}$. By definition, $\\mathbf{C} = \\mathbb{E}[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})^\\top]$. Expanding this expression yields:\n$$\n\\mathbf{C} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top - \\mathbf{x}\\boldsymbol{\\mu}^\\top - \\boldsymbol{\\mu}\\mathbf{x}^\\top + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top]\n$$\nUsing the linearity of expectation and that $\\mathbb{E}[\\mathbf{x}] = \\boldsymbol{\\mu}$:\n$$\n\\mathbf{C} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] - \\mathbb{E}[\\mathbf{x}]\\boldsymbol{\\mu}^\\top - \\boldsymbol{\\mu}\\mathbb{E}[\\mathbf{x}^\\top] + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top\n$$\n$$\n\\mathbf{C} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top\n$$\nTherefore, the second moment matrix is given by:\n$$\n\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top\n$$\n\nWith this, we analyze each rule.\n\n**Correlation Hebbian Rule**\nThe update rule is $\\Delta \\mathbf{w} = \\eta\\, y\\, \\mathbf{x}$. Substituting $y = \\mathbf{w}^\\top \\mathbf{x}$, we get $\\Delta \\mathbf{w} = \\eta\\, (\\mathbf{w}^\\top \\mathbf{x})\\, \\mathbf{x}$. The expected change is:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[(\\mathbf{w}^\\top \\mathbf{x})\\, \\mathbf{x}] = \\eta \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top \\mathbf{w}]\n$$\nSince $\\mathbf{w}$ is constant with respect to the expectation over $\\mathbf{x}$:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] \\mathbf{w}\n$$\nSubstituting $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top$:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top) \\mathbf{w} = \\eta (\\mathbf{C}\\mathbf{w} + \\boldsymbol{\\mu}(\\boldsymbol{\\mu}^\\top\\mathbf{w})) = \\eta (\\mathbf{C}\\mathbf{w} + (\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu})\n$$\nThe dynamics have two driving terms. The first, $\\eta\\mathbf{C}\\mathbf{w}$, drives $\\mathbf{w}$ towards the principal eigenvectors of $\\mathbf{C}$. The second, $\\eta(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$, provides positive feedback for the component of $\\mathbf{w}$ along the mean vector $\\boldsymbol{\\mu}$. If, for instance, $\\mathbf{w}$ has an initial component along $\\boldsymbol{\\mu}$ such that $\\mathbf{w}^\\top\\boldsymbol{\\mu} > 0$, this component will grow exponentially. This indicates an instability and leads to unbounded growth of the weight vector norm, $\\lVert \\mathbf{w} \\rVert$. There are no finite-norm stable fixed points.\n\n**Covariance Hebbian Rule**\nThe update rule is $\\Delta \\mathbf{w} = \\eta\\, y\\, (\\mathbf{x} - \\boldsymbol{\\mu})$. Substituting $y = \\mathbf{w}^\\top \\mathbf{x}$:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[(\\mathbf{w}^\\top \\mathbf{x})(\\mathbf{x} - \\boldsymbol{\\mu})] = \\eta \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top \\mathbf{w} - \\mathbf{x}\\boldsymbol{\\mu}^\\top \\mathbf{w}]\n$$\nUsing linearity of expectation and treating $\\mathbf{w}$ as constant:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta (\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top]\\mathbf{w} - \\mathbb{E}[\\mathbf{x}]\\boldsymbol{\\mu}^\\top \\mathbf{w})\n$$\nSubstituting $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top$ and $\\mathbb{E}[\\mathbf{x}] = \\boldsymbol{\\mu}$:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta ((\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w} - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top \\mathbf{w}) = \\eta (\\mathbf{C}\\mathbf{w} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top \\mathbf{w} - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top \\mathbf{w}) = \\eta \\mathbf{C}\\mathbf{w}\n$$\nThis is a linear ODE. Its solution represents the power iteration method for finding the eigenvector corresponding to the largest eigenvalue of $\\mathbf{C}$. If $\\mathbf{w}(t) = \\sum_i c_i(t) \\mathbf{v}_i$ where $\\mathbf{v}_i$ are the eigenvectors of $\\mathbf{C}$ with eigenvalues $\\lambda_i$, the dynamics for the coefficients are $\\frac{dc_i}{dt} = \\eta \\lambda_i c_i$, leading to $c_i(t) = c_i(0)e^{\\eta\\lambda_i t}$. As $t\\to\\infty$, the term corresponding to the largest eigenvalue, $\\lambda_{\\text{max}}$, will dominate. The direction of $\\mathbf{w}$ will align with the corresponding eigenvector $\\mathbf{v}_{\\text{max}}$. Since the problem states that $\\mathbf{C}$ has at least one strictly positive eigenvalue, $\\lambda_{\\text{max}} > 0$. Consequently, the norm $\\lVert \\mathbf{w} \\rVert$ will grow exponentially without bound.\n\n**Oja's Rule**\nThe update rule is $\\Delta \\mathbf{w} = \\eta\\, y (\\mathbf{x} - y \\mathbf{w})$. The expected dynamics are:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[y\\mathbf{x} - y^2\\mathbf{w}] = \\eta (\\mathbb{E}[y\\mathbf{x}] - \\mathbb{E}[y^2]\\mathbf{w})\n$$\nThe first term is the same as in the correlation rule: $\\mathbb{E}[y\\mathbf{x}] = (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w}$.\nThe second term is the expected squared output: $\\mathbb{E}[y^2] = \\mathbb{E}[(\\mathbf{w}^\\top\\mathbf{x})^2] = \\mathbb{E}[\\mathbf{w}^\\top\\mathbf{x}\\mathbf{x}^\\top\\mathbf{w}] = \\mathbf{w}^\\top \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] \\mathbf{w} = \\mathbf{w}^\\top(\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w}$.\nThe full dynamics are:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\left( (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w} - (\\mathbf{w}^\\top(\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w})\\mathbf{w} \\right)\n$$\nCase 1: Mean-centered inputs, $\\boldsymbol{\\mu} = \\mathbf{0}$.\nThe dynamics simplify to $\\frac{d\\mathbf{w}}{dt} = \\eta (\\mathbf{C}\\mathbf{w} - (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})\\mathbf{w})$.\nTo analyze the norm, we compute $\\frac{d}{dt}\\lVert\\mathbf{w}\\rVert^2 = 2\\mathbf{w}^\\top \\frac{d\\mathbf{w}}{dt}$:\n$$\n\\frac{d}{dt}\\lVert\\mathbf{w}\\rVert^2 = 2\\mathbf{w}^\\top \\eta (\\mathbf{C}\\mathbf{w} - (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})\\mathbf{w}) = 2\\eta (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w} - (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})\\lVert\\mathbf{w}\\rVert^2) = 2\\eta (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})(1 - \\lVert\\mathbf{w}\\rVert^2)\n$$\nSince $\\mathbf{C}$ is positive semidefinite with a strictly positive eigenvalue, $\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w} > 0$ for any $\\mathbf{w}$ not in the null space. The dynamics drive $\\lVert\\mathbf{w}\\rVert^2 \\to 1$. The rule stabilizes the norm of the weight vector to unity. A more detailed stability analysis shows that $\\mathbf{w}$ converges to a unit-norm eigenvector corresponding to the largest eigenvalue of $\\mathbf{C}$, $\\mathbf{v}_{\\text{max}}$.\nCase 2: Non-zero mean, $\\boldsymbol{\\mu} \\neq \\mathbf{0}$.\nThe dynamics are driven by the correlation term $\\mathbb{E}[y\\mathbf{x}] = (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w} = \\mathbf{C}\\mathbf{w} + (\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$. Compared to the mean-centered case, the driving or \"drift\" term in the dynamics is modified from $\\mathbf{C}\\mathbf{w}$ to $(\\mathbf{C}+\\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w}$, which includes the extra term $(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$. This term correctly describes the bias towards the input mean.\n\nNow we evaluate the given options.\n\n**A. Under the correlation Hebbian rule, the expected weight dynamics satisfy $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\left(\\mathbf{C}\\,\\mathbf{w} + (\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}\\right)$, which implies an unbounded drift along $\\boldsymbol{\\mu}$ when $\\boldsymbol{\\mu} \\neq \\mathbf{0}$; there are no finite-norm stable fixed points without additional normalization.**\nOur derivation matches the provided equation for the dynamics. The term $\\eta(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$ creates positive feedback along the direction of $\\boldsymbol{\\mu}$, leading to unbounded growth. Both this term and the $\\eta\\mathbf{C}\\mathbf{w}$ term (due to $\\lambda_{\\text{max}} > 0$) cause the norm to grow, hence the system is unstable and has no finite-norm stable fixed points.\n**Verdict: Correct.**\n\n**B. Under the covariance Hebbian rule, the expected weight dynamics satisfy $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\, \\mathbf{C}\\,\\mathbf{w}$, so the direction of $\\mathbf{w}$ aligns with the eigenvector of $\\mathbf{C}$ associated with its largest eigenvalue, but the norm $\\lVert \\mathbf{w} \\rVert$ grows without bound in the absence of normalization.**\nOur derivation confirms the dynamics are $\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbf{C}\\mathbf{w}$. This is the power method, which causes the direction of $\\mathbf{w}$ to converge to the principal eigenvector of $\\mathbf{C}$. Since $\\lambda_{\\text{max}} > 0$, the norm of $\\mathbf{w}$ grows exponentially like $e^{\\eta\\lambda_{\\text{max}}t}$.\n**Verdict: Correct.**\n\n**C. The covariance Hebbian rule alone guarantees globally stable convergence of $\\mathbf{w}$ to a bounded fixed point for any positive semidefinite $\\mathbf{C}$, regardless of the value of $\\boldsymbol{\\mu}$.**\nThis is false. As established in the analysis for B, the covariance rule leads to unbounded growth of the weight norm whenever $\\mathbf{C}$ has a positive eigenvalue. It does not converge to a bounded fixed point (except for the unstable fixed point at $\\mathbf{w}=\\mathbf{0}$).\n**Verdict: Incorrect.**\n\n**D. Replacing either Hebbian rule with Oja's rule, the expected dynamics become $\\dfrac{d\\mathbf{w}}{dt} = \\mathbf{C}\\,\\mathbf{w} - (\\mathbf{w}^\\top \\mathbf{C}\\,\\mathbf{w})\\, \\mathbf{w}$ when inputs are mean-centered ($\\boldsymbol{\\mu} = \\mathbf{0}$), yielding stable convergence of $\\mathbf{w}$ to a unit-norm eigenvector of $\\mathbf{C}$ corresponding to the largest eigenvalue; with $\\boldsymbol{\\mu} \\neq \\mathbf{0}$ and raw inputs, an extra term $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ appears in the drift that biases the solution toward $\\boldsymbol{\\mu}$ unless inputs are centered.**\nThe analysis of Oja's rule confirms both parts of this statement. For $\\boldsymbol{\\mu}=\\mathbf{0}$, the dynamics (up to a scaling factor $\\eta$) and the stability properties are correctly described. For $\\boldsymbol{\\mu}\\neq\\mathbf{0}$, the effective \"drift\" term in the dynamics is modified from $\\mathbf{C}\\mathbf{w}$ to $(\\mathbf{C}+\\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w}$, which includes the extra term $(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$. This term correctly describes the bias towards the input mean.\n**Verdict: Correct.**\n\n**E. The correlation Hebbian rule is stable when $\\boldsymbol{\\mu} \\neq \\mathbf{0}$, because the term $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ acts as a weight decay that reduces $\\lVert \\mathbf{w} \\rVert$.**\nThis statement makes a fundamentally incorrect claim. The term $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ is a positive feedback term, not a decay term. A decay term would have a negative sign (e.g., $-\\gamma\\mathbf{w}$). Our analysis of the norm derivative, $\\frac{d}{dt}\\lVert\\mathbf{w}\\rVert^2 = 2\\eta (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w} + (\\mathbf{w}^\\top\\boldsymbol{\\mu})^2)$, shows that both terms contribute to norm growth, guaranteeing instability, not stability.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "Neural learning rules operate on a sample-by-sample basis, which makes them inherently stochastic processes. While mean-field equations describe the average behavior of the system, a complete understanding requires us to characterize the fluctuations around this mean. This practice  provides a hands-on opportunity to analyze the stochastic nature of Oja's rule by deriving the covariance matrix of the instantaneous weight update. This rigorous calculation will give you a concrete appreciation for how input statistics, such as the covariance $\\mathbf{C}$, shape the noise and variability inherent in the learning process.",
            "id": "4025522",
            "problem": "Consider a single linear neuron with synaptic weight vector $\\mathbf{w} \\in \\mathbb{R}^{d}$ receiving an input $\\mathbf{x} \\in \\mathbb{R}^{d}$. The neuron’s instantaneous output is $y = \\mathbf{w}^{\\top}\\mathbf{x}$. The synaptic update at a single time step follows Oja’s rule with instantaneous normalization:\n$$\n\\Delta \\mathbf{w} \\;=\\; \\eta\\big( y\\,\\mathbf{x} - y^{2}\\,\\mathbf{w} \\big),\n$$\nwhere $\\eta > 0$ is a constant learning rate. Assume the input is zero-mean multivariate Gaussian, $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{C})$, with covariance matrix $\\mathbf{C} \\in \\mathbb{R}^{d \\times d}$ that is symmetric and positive semidefinite. Treat $\\mathbf{w}$ as fixed over the single update. Let $\\lambda = \\mathbf{w}^{\\top}\\mathbf{C}\\mathbf{w}$.\n\nUsing only fundamental definitions of expectation, covariance, and the properties of zero-mean multivariate Gaussian random vectors, derive the covariance matrix of the instantaneous Oja update,\n$$\n\\mathrm{Var}[\\Delta \\mathbf{w}] \\;=\\; \\mathbb{E}\\big[(\\Delta \\mathbf{w} - \\mathbb{E}[\\Delta \\mathbf{w}])(\\Delta \\mathbf{w} - \\mathbb{E}[\\Delta \\mathbf{w}])^{\\top}\\big],\n$$\nand express your final result in closed form in terms of $\\mathbf{C}$, $\\mathbf{w}$, $\\lambda$, and $\\eta$. Provide a single closed-form analytic expression for $\\mathrm{Var}[\\Delta \\mathbf{w}]$. No numerical approximation or rounding is required.",
            "solution": "The problem requires the derivation of the covariance matrix of the instantaneous Oja update vector, $\\Delta \\mathbf{w}$. The covariance matrix of a random vector $\\mathbf{z}$ is defined as $\\mathrm{Var}[\\mathbf{z}] = \\mathbb{E}[(\\mathbf{z} - \\mathbb{E}[\\mathbf{z}])(\\mathbf{z} - \\mathbb{E}[\\mathbf{z}])^{\\top}]$. This can be expanded to $\\mathrm{Var}[\\mathbf{z}] = \\mathbb{E}[\\mathbf{z}\\mathbf{z}^{\\top}] - \\mathbb{E}[\\mathbf{z}]\\mathbb{E}[\\mathbf{z}]^{\\top}$. We will apply this formula to $\\Delta \\mathbf{w}$.\n\nThe instantaneous Oja update is given by:\n$$\n\\Delta \\mathbf{w} \\;=\\; \\eta\\big( y\\,\\mathbf{x} - y^{2}\\,\\mathbf{w} \\big)\n$$\nwhere $y = \\mathbf{w}^{\\top}\\mathbf{x}$ and the input $\\mathbf{x}$ is a zero-mean multivariate Gaussian random vector, $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{C})$. The learning rate $\\eta > 0$ is a constant, and the weight vector $\\mathbf{w}$ is treated as fixed for this single-step analysis.\n\nThe derivation proceeds in two main steps: first, we compute the expected update $\\mathbb{E}[\\Delta \\mathbf{w}]$, and second, we compute the second moment matrix $\\mathbb{E}[\\Delta \\mathbf{w} (\\Delta \\mathbf{w})^{\\top}]$.\n\n**Step 1: Compute the expected update $\\mathbb{E}[\\Delta \\mathbf{w}]$**\n\nUsing the linearity of the expectation operator, we have:\n$$\n\\mathbb{E}[\\Delta \\mathbf{w}] = \\mathbb{E}\\big[\\eta\\big( y\\,\\mathbf{x} - y^{2}\\,\\mathbf{w} \\big)\\big] = \\eta \\big( \\mathbb{E}[y\\,\\mathbf{x}] - \\mathbb{E}[y^2]\\,\\mathbf{w} \\big)\n$$\nWe need to evaluate the two expectations, $\\mathbb{E}[y\\,\\mathbf{x}]$ and $\\mathbb{E}[y^2]$.\nThe neuron's output $y = \\mathbf{w}^{\\top}\\mathbf{x}$ is a linear combination of the components of the zero-mean Gaussian vector $\\mathbf{x}$, so $y$ is a zero-mean scalar Gaussian random variable. Its mean is $\\mathbb{E}[y] = \\mathbb{E}[\\mathbf{w}^{\\top}\\mathbf{x}] = \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}] = \\mathbf{w}^{\\top}\\mathbf{0} = 0$.\n\nThe first expectation is:\n$$\n\\mathbb{E}[y\\,\\mathbf{x}] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})\\mathbf{x}] = \\mathbb{E}[\\mathbf{x}(\\mathbf{w}^{\\top}\\mathbf{x})] = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{w}] = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w}\n$$\nFor a zero-mean random vector, the covariance matrix is $\\mathbf{C} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}] - \\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{x}]^{\\top}$. Since $\\mathbb{E}[\\mathbf{x}] = \\mathbf{0}$, we have $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}] = \\mathbf{C}$. Therefore:\n$$\n\\mathbb{E}[y\\,\\mathbf{x}] = \\mathbf{C}\\mathbf{w}\n$$\n\nThe second expectation is the variance of $y$:\n$$\n\\mathbb{E}[y^2] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})^2] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})(\\mathbf{x}^{\\top}\\mathbf{w})] = \\mathbb{E}[\\mathbf{w}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{w}] = \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w} = \\mathbf{w}^{\\top}\\mathbf{C}\\mathbf{w}\n$$\nThe problem statement defines this quantity as $\\lambda$. So, $\\mathbb{E}[y^2] = \\lambda$. This also means that $y \\sim \\mathcal{N}(0, \\lambda)$.\n\nSubstituting these results back into the expression for the expected update:\n$$\n\\mathbb{E}[\\Delta \\mathbf{w}] = \\eta(\\mathbf{C}\\mathbf{w} - \\lambda\\mathbf{w})\n$$\n\n**Step 2: Compute the second moment matrix $\\mathbb{E}[\\Delta \\mathbf{w} (\\Delta \\mathbf{w})^{\\top}]$**\n\nWe expand the outer product of the update vector with itself:\n$$\n\\Delta \\mathbf{w} (\\Delta \\mathbf{w})^{\\top} = \\eta^2 \\big( y\\,\\mathbf{x} - y^{2}\\,\\mathbf{w} \\big) \\big( y\\,\\mathbf{x} - y^{2}\\,\\mathbf{w} \\big)^{\\top} = \\eta^2 \\big( y^2 \\mathbf{x}\\mathbf{x}^{\\top} - y^3 \\mathbf{x}\\mathbf{w}^{\\top} - y^3 \\mathbf{w}\\mathbf{x}^{\\top} + y^4 \\mathbf{w}\\mathbf{w}^{\\top} \\big)\n$$\nTaking the expectation gives:\n$$\n\\mathbb{E}[\\Delta \\mathbf{w} (\\Delta \\mathbf{w})^{\\top}] = \\eta^2 \\big( \\mathbb{E}[y^2 \\mathbf{x}\\mathbf{x}^{\\top}] - \\mathbb{E}[y^3 \\mathbf{x}\\mathbf{w}^{\\top}] - \\mathbb{E}[y^3 \\mathbf{w}\\mathbf{x}^{\\top}] + \\mathbb{E}[y^4 \\mathbf{w}\\mathbf{w}^{\\top}] \\big)\n$$\nThis requires calculating third and fourth-order moments of the Gaussian random variables involved. We will use Isserlis' theorem (also known as Wick's theorem for Gaussian distributions). For any set of four zero-mean Gaussian random variables $z_1, z_2, z_3, z_4$, the theorem states:\n$$\n\\mathbb{E}[z_1 z_2 z_3 z_4] = \\mathbb{E}[z_1 z_2]\\mathbb{E}[z_3 z_4] + \\mathbb{E}[z_1 z_3]\\mathbb{E}[z_2 z_4] + \\mathbb{E}[z_1 z_4]\\mathbb{E}[z_2 z_3]\n$$\nAlso, for any odd number of zero-mean Gaussian variables, their expected product is zero, e.g., $\\mathbb{E}[z_1 z_2 z_3] = 0$.\n\nLet's evaluate each term:\n\n- **Term 1: $\\mathbb{E}[y^4 \\mathbf{w}\\mathbf{w}^{\\top}]$**\nSince $\\mathbf{w}$ is constant, this is $\\mathbb{E}[y^4]\\mathbf{w}\\mathbf{w}^{\\top}$. As $y \\sim \\mathcal{N}(0, \\lambda)$, its fourth moment is $\\mathbb{E}[y^4] = 3(\\mathbb{E}[y^2])^2 = 3\\lambda^2$.\nSo, $\\mathbb{E}[y^4 \\mathbf{w}\\mathbf{w}^{\\top}] = 3\\lambda^2 \\mathbf{w}\\mathbf{w}^{\\top}$.\n\n- **Term 2: $\\mathbb{E}[y^3 \\mathbf{x}\\mathbf{w}^{\\top}]$ and $\\mathbb{E}[y^3 \\mathbf{w}\\mathbf{x}^{\\top}]$**\nWe need to find $\\mathbb{E}[y^3 \\mathbf{x}]$. Let's consider its $i$-th component, $\\mathbb{E}[y^3 x_i]$. Applying Isserlis' theorem with $z_1=z_2=z_3=y$ and $z_4=x_i$:\n$$\n\\mathbb{E}[y^3 x_i] = \\mathbb{E}[y y y x_i] = \\mathbb{E}[y y]\\mathbb{E}[y x_i] + \\mathbb{E}[y y]\\mathbb{E}[y x_i] + \\mathbb{E}[y y]\\mathbb{E}[y x_i] = 3 \\mathbb{E}[y^2] \\mathbb{E}[y x_i]\n$$\nWe have $\\mathbb{E}[y^2] = \\lambda$. The term $\\mathbb{E}[y x_i]$ is the $i$-th component of the vector $\\mathbb{E}[y \\mathbf{x}] = \\mathbf{C}\\mathbf{w}$. Thus $\\mathbb{E}[y x_i] = (\\mathbf{C}\\mathbf{w})_i$.\nSo, $\\mathbb{E}[y^3 x_i] = 3\\lambda (\\mathbf{C}\\mathbf{w})_i$. In vector form, $\\mathbb{E}[y^3 \\mathbf{x}] = 3\\lambda \\mathbf{C}\\mathbf{w}$.\nThis gives $\\mathbb{E}[y^3 \\mathbf{x}\\mathbf{w}^{\\top}] = (3\\lambda \\mathbf{C}\\mathbf{w}) \\mathbf{w}^{\\top} = 3\\lambda \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}$.\nThe other term is its transpose: $\\mathbb{E}[y^3 \\mathbf{w}\\mathbf{x}^{\\top}] = \\mathbf{w}\\mathbb{E}[y^3\\mathbf{x}]^{\\top} = \\mathbf{w}(3\\lambda \\mathbf{C}\\mathbf{w})^{\\top} = 3\\lambda \\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C}^{\\top}$. Since $\\mathbf{C}$ is symmetric, $\\mathbf{C}^{\\top}=\\mathbf{C}$, so this is $3\\lambda \\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C}$.\n\n- **Term 3: $\\mathbb{E}[y^2 \\mathbf{x}\\mathbf{x}^{\\top}]$**\nWe compute the $(i, j)$-th element of this matrix, $\\mathbb{E}[y^2 x_i x_j]$. Applying Isserlis' theorem with $z_1=z_2=y$, $z_3=x_i$, $z_4=x_j$:\n$$\n\\mathbb{E}[y^2 x_i x_j] = \\mathbb{E}[y y x_i x_j] = \\mathbb{E}[y y]\\mathbb{E}[x_i x_j] + \\mathbb{E}[y x_i]\\mathbb{E}[y x_j] + \\mathbb{E}[y x_j]\\mathbb{E}[y x_i]\n$$\nUsing our previously calculated quantities: $\\mathbb{E}[y^2]=\\lambda$, $\\mathbb{E}[x_i x_j]=C_{ij}$, $\\mathbb{E}[y x_i]=(\\mathbf{C}\\mathbf{w})_i$, and $\\mathbb{E}[y x_j]=(\\mathbf{C}\\mathbf{w})_j$.\n$$\n\\mathbb{E}[y^2 x_i x_j] = \\lambda C_{ij} + 2 (\\mathbf{C}\\mathbf{w})_i (\\mathbf{C}\\mathbf{w})_j\n$$\nIn matrix form, this is:\n$$\n\\mathbb{E}[y^2 \\mathbf{x}\\mathbf{x}^{\\top}] = \\lambda \\mathbf{C} + 2 (\\mathbf{C}\\mathbf{w})(\\mathbf{C}\\mathbf{w})^{\\top} = \\lambda \\mathbf{C} + 2 \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C}\n$$\n(using $(\\mathbf{C}\\mathbf{w})^{\\top} = \\mathbf{w}^{\\top}\\mathbf{C}^{\\top} = \\mathbf{w}^{\\top}\\mathbf{C}$).\n\nNow, we assemble the second moment matrix:\n$$\n\\frac{1}{\\eta^2}\\mathbb{E}[\\Delta \\mathbf{w} (\\Delta \\mathbf{w})^{\\top}] = (\\lambda \\mathbf{C} + 2 \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C}) - (3\\lambda \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}) - (3\\lambda \\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C}) + (3\\lambda^2 \\mathbf{w}\\mathbf{w}^{\\top})\n$$\n\n**Step 3: Compute the covariance matrix $\\mathrm{Var}[\\Delta \\mathbf{w}]$**\n\nWe now use the formula $\\mathrm{Var}[\\Delta \\mathbf{w}] = \\mathbb{E}[\\Delta \\mathbf{w} (\\Delta \\mathbf{w})^{\\top}] - \\mathbb{E}[\\Delta \\mathbf{w}]\\mathbb{E}[\\Delta \\mathbf{w}]^{\\top}$.\nLet's compute the second term:\n$$\n\\mathbb{E}[\\Delta \\mathbf{w}]\\mathbb{E}[\\Delta \\mathbf{w}]^{\\top} = \\eta(\\mathbf{C}\\mathbf{w} - \\lambda\\mathbf{w}) \\left( \\eta(\\mathbf{C}\\mathbf{w} - \\lambda\\mathbf{w}) \\right)^{\\top}\n$$\n$$\n\\frac{1}{\\eta^2}\\mathbb{E}[\\Delta \\mathbf{w}]\\mathbb{E}[\\Delta \\mathbf{w}]^{\\top} = (\\mathbf{C}\\mathbf{w} - \\lambda\\mathbf{w}) (\\mathbf{w}^{\\top}\\mathbf{C} - \\lambda\\mathbf{w}^{\\top})\n$$\n$$\n= \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} - \\lambda\\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top} - \\lambda\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} + \\lambda^2\\mathbf{w}\\mathbf{w}^{\\top}\n$$\nFinally, we subtract this from the scaled second moment matrix:\n$$\n\\frac{1}{\\eta^2}\\mathrm{Var}[\\Delta \\mathbf{w}] = (\\lambda \\mathbf{C} + 2 \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} - 3\\lambda \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top} - 3\\lambda \\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} + 3\\lambda^2 \\mathbf{w}\\mathbf{w}^{\\top}) - (\\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} - \\lambda\\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top} - \\lambda\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} + \\lambda^2\\mathbf{w}\\mathbf{w}^{\\top})\n$$\nCollecting like terms:\n- $\\mathbf{C}$ term: $\\lambda \\mathbf{C}$\n- $\\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C}$ term: $2 - 1 = 1 \\implies \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C}$\n- $\\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}$ term: $-3\\lambda - (-\\lambda) = -2\\lambda \\implies -2\\lambda \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}$\n- $\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C}$ term: $-3\\lambda - (-\\lambda) = -2\\lambda \\implies -2\\lambda \\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C}$\n- $\\mathbf{w}\\mathbf{w}^{\\top}$ term: $3\\lambda^2 - \\lambda^2 = 2\\lambda^2 \\implies 2\\lambda^2 \\mathbf{w}\\mathbf{w}^{\\top}$\n\nCombining these results, we get:\n$$\n\\frac{1}{\\eta^2}\\mathrm{Var}[\\Delta \\mathbf{w}] = \\lambda \\mathbf{C} + \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} - 2\\lambda \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top} - 2\\lambda \\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} + 2\\lambda^2 \\mathbf{w}\\mathbf{w}^{\\top}\n$$\nMultiplying by $\\eta^2$ yields the final expression for the covariance matrix:\n$$\n\\mathrm{Var}[\\Delta \\mathbf{w}] = \\eta^2 (\\lambda \\mathbf{C} + \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} - 2\\lambda \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top} - 2\\lambda \\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} + 2\\lambda^2 \\mathbf{w}\\mathbf{w}^{\\top})\n$$",
            "answer": "$$\\boxed{\\eta^{2} \\big(\\lambda \\mathbf{C} + \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} - 2\\lambda \\mathbf{C}\\mathbf{w}\\mathbf{w}^{\\top} - 2\\lambda \\mathbf{w}\\mathbf{w}^{\\top}\\mathbf{C} + 2\\lambda^{2} \\mathbf{w}\\mathbf{w}^{\\top}\\big)}$$"
        },
        {
            "introduction": "Biological systems must adapt to environments where the statistical properties of sensory inputs are constantly changing. This exercise  explores the performance of Oja's rule in such a non-stationary setting, challenging it to track a principal component that is continuously rotating over time. By deriving the steady-state angular lag $\\delta$ between the neuron's weight vector and the true eigenvector, you will quantitatively explore the trade-off between plasticity and stability. This reveals the fundamental relationship between the learning rate $\\eta$ and the system's ability to track a dynamic feature, a core concept in adaptive filter theory and computational neuroscience.",
            "id": "4025497",
            "problem": "Consider a single linear neuron receiving a two-dimensional zero-mean input $x(t) \\in \\mathbb{R}^{2}$ with time-varying covariance matrix $C(t)$. The synaptic weight vector is $w(t) \\in \\mathbb{R}^{2}$. The synapse obeys Oja’s rule, a normalized Hebbian plasticity law, given in its deterministic mean-field form by\n$$\n\\dot{w}(t) \\;=\\; \\eta \\left( C(t)\\, w(t) \\;-\\; \\big(w(t)^{\\top} C(t)\\, w(t)\\big)\\, w(t) \\right),\n$$\nwhere $\\eta>0$ is the learning rate. Assume the input covariance is rank-one with a rotating principal axis,\n$$\nC(t) \\;=\\; e_{1}(t)\\, e_{1}(t)^{\\top}, \\qquad e_{1}(t) \\;=\\; \\begin{pmatrix}\\cos\\big(\\Omega t\\big) \\\\ \\sin\\big(\\Omega t\\big)\\end{pmatrix},\n$$\nwhere $\\Omega>0$ is a constant drift angular frequency and angles are measured in radians. Define the signed angular lag $\\delta$ as the steady-state constant difference between the weight direction and the instantaneous top eigenvector direction, that is,\n$$\n\\delta \\;=\\; \\psi(t) \\;-\\; \\theta(t),\n$$\nin the phase-locked regime where $\\delta$ is time-independent, with $w(t) = \\big(\\cos\\psi(t),\\, \\sin\\psi(t)\\big)^{\\top}$ and $e_{1}(t) = \\big(\\cos\\theta(t),\\, \\sin\\theta(t)\\big)^{\\top}$, and $\\theta(t) = \\Omega t$. Starting from these definitions and the given dynamics, derive a closed-form analytic expression for the steady-state signed lag $\\delta$ as a function of $\\eta$ and $\\Omega$, under the assumption that the system achieves phase-locking. Express your final answer as a single analytic expression in radians. No numerical evaluation is required.",
            "solution": "The problem asks for the steady-state angular lag $\\delta$ between the weight vector $w(t)$ and the rotating principal eigenvector $e_1(t)$ under Oja's rule. We can solve this by deriving two expressions for the time derivative of the weight vector, $\\dot{w}(t)$, and equating them. One expression comes from the kinematics of a rotating vector, and the other from the Oja dynamics.\n\n**1. Kinematic Expression for $\\dot{w}(t)$**\nThe weight vector is parametrized by its angle $\\psi(t)$ as $w(t) = (\\cos\\psi(t), \\sin\\psi(t))^\\top$. Its time derivative is found using the chain rule:\n$$ \\dot{w}(t) = \\frac{d}{dt} \\begin{pmatrix}\\cos\\psi(t) \\\\ \\sin\\psi(t)\\end{pmatrix} = \\begin{pmatrix}-\\sin\\psi(t) \\\\ \\cos\\psi(t)\\end{pmatrix} \\dot{\\psi}(t) = \\dot{\\psi}(t) w_{\\perp}(t) $$\nwhere $w_{\\perp}(t)$ is the unit vector orthogonal to $w(t)$.\nThe problem assumes a phase-locked state where the lag $\\delta = \\psi(t) - \\theta(t)$ is constant. This implies that the angular velocities are equal: $\\dot{\\psi}(t) = \\dot{\\theta}(t)$. Since $\\theta(t) = \\Omega t$, we have $\\dot{\\theta}(t) = \\Omega$. Therefore, $\\dot{\\psi}(t) = \\Omega$.\nSubstituting this into the kinematic expression gives:\n$$ \\dot{w}(t) = \\Omega w_{\\perp}(t) $$\n\n**2. Dynamic Expression for $\\dot{w}(t)$ from Oja's Rule**\nFirst, we simplify the terms in the Oja's rule equation:\n$$ \\dot{w}(t) = \\eta \\left( C(t)w(t) - (w(t)^{\\top} C(t) w(t)) w(t) \\right) $$\nThe term $C(t)w(t)$ becomes:\n$$ C(t)w(t) = (e_1(t)e_1(t)^\\top)w(t) = e_1(t)(e_1(t)^\\top w(t)) $$\nThe inner product $e_1(t)^\\top w(t)$ is the cosine of the angle between the two vectors, which is $\\delta$:\n$$ e_1(t)^\\top w(t) = \\cos\\theta(t)\\cos\\psi(t) + \\sin\\theta(t)\\sin\\psi(t) = \\cos(\\psi(t) - \\theta(t)) = \\cos(\\delta) $$\nSo, $C(t)w(t) = e_1(t)\\cos(\\delta)$.\nThe quadratic term $w(t)^{\\top} C(t) w(t)$ is:\n$$ w(t)^{\\top} C(t) w(t) = w(t)^{\\top} (e_1(t)\\cos(\\delta)) = (w(t)^\\top e_1(t))\\cos(\\delta) = \\cos(\\delta)\\cos(\\delta) = \\cos^2(\\delta) $$\nSubstituting these back into the Oja's rule equation gives the dynamic expression for $\\dot{w}(t)$:\n$$ \\dot{w}(t) = \\eta \\left( e_1(t)\\cos(\\delta) - w(t)\\cos^2(\\delta) \\right) $$\n\n**3. Equating and Solving for $\\delta$**\nNow, we equate the kinematic and dynamic expressions for $\\dot{w}(t)$:\n$$ \\Omega w_{\\perp}(t) = \\eta \\left( e_1(t)\\cos(\\delta) - w(t)\\cos^2(\\delta) \\right) $$\nTo isolate $\\delta$, we project this vector equation onto $w_{\\perp}(t)$ by taking the dot product with $w_{\\perp}(t)^\\top$:\n$$ w_{\\perp}(t)^\\top (\\Omega w_{\\perp}(t)) = w_{\\perp}(t)^\\top \\left( \\eta e_1(t)\\cos(\\delta) \\right) - w_{\\perp}(t)^\\top \\left( \\eta w(t)\\cos^2(\\delta) \\right) $$\nThe terms simplify as follows:\n- Left side: $\\Omega (w_{\\perp}(t)^\\top w_{\\perp}(t)) = \\Omega \\|w_{\\perp}(t)\\|^2 = \\Omega$.\n- First term on right side: We need the projection $w_{\\perp}(t)^\\top e_1(t)$.\n    $$ w_{\\perp}(t)^\\top e_1(t) = (-\\sin\\psi(t), \\cos\\psi(t)) \\cdot (\\cos\\theta(t), \\sin\\theta(t)) = \\cos\\psi(t)\\sin\\theta(t) - \\sin\\psi(t)\\cos\\theta(t) = \\sin(\\theta(t) - \\psi(t)) = \\sin(-\\delta) = -\\sin(\\delta) $$\n- Second term on right side: This is zero because $w_{\\perp}(t)$ and $w(t)$ are orthogonal, so $w_{\\perp}(t)^\\top w(t) = 0$.\n\nThe equation becomes:\n$$ \\Omega = \\eta \\cos(\\delta) (-\\sin(\\delta)) $$\n$$ \\Omega = -\\eta \\sin(\\delta)\\cos(\\delta) $$\nUsing the double-angle identity $\\sin(2x) = 2\\sin(x)\\cos(x)$, we get:\n$$ \\Omega = -\\frac{\\eta}{2} \\sin(2\\delta) $$\nSolving for $\\sin(2\\delta)$:\n$$ \\sin(2\\delta) = -\\frac{2\\Omega}{\\eta} $$\nFinally, we solve for the lag $\\delta$ by taking the inverse sine. The stable solution corresponds to the principal value of the arcsin function.\n$$ 2\\delta = \\arcsin\\left(-\\frac{2\\Omega}{\\eta}\\right) = -\\arcsin\\left(\\frac{2\\Omega}{\\eta}\\right) $$\n$$ \\delta = -\\frac{1}{2}\\arcsin\\left(\\frac{2\\Omega}{\\eta}\\right) $$\nThis result is valid when the argument of the arcsin is between -1 and 1, which implies $\\eta \\ge 2\\Omega$. This condition means the learning rate must be fast enough to track the rotating signal. The negative sign indicates that the weight vector lags behind the target eigenvector, as expected.",
            "answer": "$$\n\\boxed{-\\frac{1}{2}\\arcsin\\left(\\frac{2\\Omega}{\\eta}\\right)}\n$$"
        }
    ]
}