{
    "hands_on_practices": [
        {
            "introduction": "A core challenge in Hebbian learning is the inherent instability that leads to unbounded synaptic weight growth. This exercise provides a foundational analysis of this problem by contrasting the dynamics of a simple correlation-based Hebbian rule with those of a covariance-based rule and Oja's rule . By deriving and comparing the expected weight dynamics for each, you will gain a clear understanding of why normalization is essential for creating stable feature detectors and how different rules achieve this.",
            "id": "4025537",
            "problem": "A single linear neuron receives an input vector $\\mathbf{x} \\in \\mathbb{R}^n$ drawn independently over time from a stationary distribution with mean $\\boldsymbol{\\mu} = \\mathbb{E}[\\mathbf{x}] \\neq \\mathbf{0}$ and covariance matrix $\\mathbf{C} = \\mathbb{E}\\left[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})^\\top\\right]$, where $\\mathbf{C}$ is symmetric positive semidefinite. The neuron produces output $y = \\mathbf{w}^\\top \\mathbf{x}$, where $\\mathbf{w} \\in \\mathbb{R}^n$ are the synaptic weights. Consider the following synaptic plasticity rules with small learning rate $\\eta > 0$ in the continuous-time limit, where $\\Delta \\mathbf{w}$ denotes the increment over a small time step:\n\n- Correlation Hebbian rule: $\\Delta \\mathbf{w} = \\eta\\, y\\, \\mathbf{x}$.\n- Covariance Hebbian rule: $\\Delta \\mathbf{w} = \\eta\\, y\\, (\\mathbf{x} - \\boldsymbol{\\mu})$.\n- Oja's rule (a local synaptic normalization rule): $\\Delta \\mathbf{w} = \\eta\\, y\\, (\\mathbf{x} - y\\, \\mathbf{w})$.\n\nStarting from the definitions of mean and covariance and the linear neuron model, analyze the expected weight dynamics and norm behavior for the correlation Hebbian rule and the covariance Hebbian rule when $\\boldsymbol{\\mu} \\neq \\mathbf{0}$, and interpret how Oja's rule changes stability and asymptotic behavior with and without mean-centering of inputs. Assume standard regularity conditions ensuring interchange of expectation and differentiation in the small-$\\eta$ limit, and that the covariance matrix $\\mathbf{C}$ has at least one strictly positive eigenvalue.\n\nWhich of the following statements are correct?\n\nA. Under the correlation Hebbian rule, the expected weight dynamics satisfy $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\left(\\mathbf{C}\\,\\mathbf{w} + (\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}\\right)$, which implies an unbounded drift along $\\boldsymbol{\\mu}$ when $\\boldsymbol{\\mu} \\neq \\mathbf{0}$; there are no finite-norm stable fixed points without additional normalization.\n\nB. Under the covariance Hebbian rule, the expected weight dynamics satisfy $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\, \\mathbf{C}\\,\\mathbf{w}$, so the direction of $\\mathbf{w}$ aligns with the eigenvector of $\\mathbf{C}$ associated with its largest eigenvalue, but the norm $\\lVert \\mathbf{w} \\rVert$ grows without bound in the absence of normalization.\n\nC. The covariance Hebbian rule alone guarantees globally stable convergence of $\\mathbf{w}$ to a bounded fixed point for any positive semidefinite $\\mathbf{C}$, regardless of the value of $\\boldsymbol{\\mu}$.\n\nD. Replacing either Hebbian rule with Oja's rule, the expected dynamics become $\\dfrac{d\\mathbf{w}}{dt} = \\mathbf{C}\\,\\mathbf{w} - (\\mathbf{w}^\\top \\mathbf{C}\\,\\mathbf{w})\\, \\mathbf{w}$ when inputs are mean-centered ($\\boldsymbol{\\mu} = \\mathbf{0}$), yielding stable convergence of $\\mathbf{w}$ to a unit-norm eigenvector of $\\mathbf{C}$ corresponding to the largest eigenvalue; with $\\boldsymbol{\\mu} \\neq \\mathbf{0}$ and raw inputs, an extra term $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ appears in the drift that biases the solution toward $\\boldsymbol{\\mu}$ unless inputs are centered.\n\nE. The correlation Hebbian rule is stable when $\\boldsymbol{\\mu} \\neq \\mathbf{0}$, because the term $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ acts as a weight decay that reduces $\\lVert \\mathbf{w} \\rVert$.",
            "solution": "The problem statement presents a standard analysis of several Hebbian-type synaptic plasticity rules in computational neuroscience. The definitions, assumptions, and physical context are consistent with established literature in the field. The setup is mathematically well-defined and self-contained. All terms are standard and their relationships are correctly specified. The problem is scientifically grounded, well-posed, and objective.\n\nThe analysis proceeds by deriving the expected dynamics for each learning rule in the continuous-time limit. For a small learning rate $\\eta$, the evolution of the weight vector $\\mathbf{w}$ can be approximated by the ordinary differential equation (ODE) $\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}\\left[\\frac{\\Delta \\mathbf{w}}{\\eta}\\right]$, where the expectation is taken over the distribution of the input vector $\\mathbf{x}$.\n\nFirst, we establish a key identity relating the second moment matrix $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top]$ to the covariance matrix $\\mathbf{C}$ and the mean vector $\\boldsymbol{\\mu}$. By definition, $\\mathbf{C} = \\mathbb{E}[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})^\\top]$. Expanding this expression yields:\n$$\n\\mathbf{C} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top - \\mathbf{x}\\boldsymbol{\\mu}^\\top - \\boldsymbol{\\mu}\\mathbf{x}^\\top + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top]\n$$\nUsing the linearity of expectation and that $\\mathbb{E}[\\mathbf{x}] = \\boldsymbol{\\mu}$:\n$$\n\\mathbf{C} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] - \\mathbb{E}[\\mathbf{x}]\\boldsymbol{\\mu}^\\top - \\boldsymbol{\\mu}\\mathbb{E}[\\mathbf{x}^\\top] + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top\n$$\n$$\n\\mathbf{C} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top\n$$\nTherefore, the second moment matrix is given by:\n$$\n\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top\n$$\n\nWith this, we analyze each rule.\n\n**Correlation Hebbian Rule**\nThe update rule is $\\Delta \\mathbf{w} = \\eta\\, y\\, \\mathbf{x}$. Substituting $y = \\mathbf{w}^\\top \\mathbf{x}$, we get $\\Delta \\mathbf{w} = \\eta\\, (\\mathbf{w}^\\top \\mathbf{x})\\, \\mathbf{x}$. The expected change is:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[(\\mathbf{w}^\\top \\mathbf{x})\\, \\mathbf{x}] = \\eta \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top \\mathbf{w}]\n$$\nSince $\\mathbf{w}$ is constant with respect to the expectation over $\\mathbf{x}$:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] \\mathbf{w}\n$$\nSubstituting $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top$:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top) \\mathbf{w} = \\eta (\\mathbf{C}\\mathbf{w} + \\boldsymbol{\\mu}(\\boldsymbol{\\mu}^\\top\\mathbf{w})) = \\eta (\\mathbf{C}\\mathbf{w} + (\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu})\n$$\nThe dynamics have two driving terms. The first, $\\eta\\mathbf{C}\\mathbf{w}$, drives $\\mathbf{w}$ towards the principal eigenvectors of $\\mathbf{C}$. The second, $\\eta(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$, provides positive feedback for the component of $\\mathbf{w}$ along the mean vector $\\boldsymbol{\\mu}$. If, for instance, $\\mathbf{w}$ has an initial component along $\\boldsymbol{\\mu}$ such that $\\mathbf{w}^\\top\\boldsymbol{\\mu}  0$, this component will grow exponentially. This indicates an instability and leads to unbounded growth of the weight vector norm, $\\lVert \\mathbf{w} \\rVert$. There are no finite-norm stable fixed points.\n\n**Covariance Hebbian Rule**\nThe update rule is $\\Delta \\mathbf{w} = \\eta\\, y\\, (\\mathbf{x} - \\boldsymbol{\\mu})$. Substituting $y = \\mathbf{w}^\\top \\mathbf{x}$:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[(\\mathbf{w}^\\top \\mathbf{x})(\\mathbf{x} - \\boldsymbol{\\mu})] = \\eta \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top \\mathbf{w} - \\mathbf{x}\\boldsymbol{\\mu}^\\top \\mathbf{w}]\n$$\nUsing linearity of expectation and treating $\\mathbf{w}$ as constant:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta (\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top]\\mathbf{w} - \\mathbb{E}[\\mathbf{x}]\\boldsymbol{\\mu}^\\top \\mathbf{w})\n$$\nSubstituting $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top$ and $\\mathbb{E}[\\mathbf{x}] = \\boldsymbol{\\mu}$:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta ((\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w} - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top \\mathbf{w}) = \\eta (\\mathbf{C}\\mathbf{w} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top \\mathbf{w} - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top \\mathbf{w}) = \\eta \\mathbf{C}\\mathbf{w}\n$$\nThis is a linear ODE. Its solution represents the power iteration method for finding the eigenvector corresponding to the largest eigenvalue of $\\mathbf{C}$. If $\\mathbf{w}(t) = \\sum_i c_i(t) \\mathbf{v}_i$ where $\\mathbf{v}_i$ are the eigenvectors of $\\mathbf{C}$ with eigenvalues $\\lambda_i$, the dynamics for the coefficients are $\\frac{dc_i}{dt} = \\eta \\lambda_i c_i$, leading to $c_i(t) = c_i(0)e^{\\eta\\lambda_i t}$. As $t\\to\\infty$, the term corresponding to the largest eigenvalue, $\\lambda_{\\text{max}}$, will dominate. The direction of $\\mathbf{w}$ will align with the corresponding eigenvector $\\mathbf{v}_{\\text{max}}$. Since the problem states that $\\mathbf{C}$ has at least one strictly positive eigenvalue, $\\lambda_{\\text{max}}  0$. Consequently, the norm $\\lVert \\mathbf{w} \\rVert$ will grow exponentially without bound.\n\n**Oja's Rule**\nThe update rule is $\\Delta \\mathbf{w} = \\eta\\, y (\\mathbf{x} - y \\mathbf{w})$. The expected dynamics are:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[y\\mathbf{x} - y^2\\mathbf{w}] = \\eta (\\mathbb{E}[y\\mathbf{x}] - \\mathbb{E}[y^2]\\mathbf{w})\n$$\nThe first term is the same as in the correlation rule: $\\mathbb{E}[y\\mathbf{x}] = (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w}$.\nThe second term is the expected squared output: $\\mathbb{E}[y^2] = \\mathbb{E}[(\\mathbf{w}^\\top\\mathbf{x})^2] = \\mathbb{E}[\\mathbf{w}^\\top\\mathbf{x}\\mathbf{x}^\\top\\mathbf{w}] = \\mathbf{w}^\\top \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] \\mathbf{w} = \\mathbf{w}^\\top(\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w}$.\nThe full dynamics are:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\left( (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w} - (\\mathbf{w}^\\top(\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w})\\mathbf{w} \\right)\n$$\nCase 1: Mean-centered inputs, $\\boldsymbol{\\mu} = \\mathbf{0}$.\nThe dynamics simplify to $\\frac{d\\mathbf{w}}{dt} = \\eta (\\mathbf{C}\\mathbf{w} - (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})\\mathbf{w})$.\nTo analyze the norm, we compute $\\frac{d}{dt}\\lVert\\mathbf{w}\\rVert^2 = 2\\mathbf{w}^\\top \\frac{d\\mathbf{w}}{dt}$:\n$$\n\\frac{d}{dt}\\lVert\\mathbf{w}\\rVert^2 = 2\\mathbf{w}^\\top \\eta (\\mathbf{C}\\mathbf{w} - (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})\\mathbf{w}) = 2\\eta (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w} - (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})\\lVert\\mathbf{w}\\rVert^2) = 2\\eta (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})(1 - \\lVert\\mathbf{w}\\rVert^2)\n$$\nSince $\\mathbf{C}$ is positive semidefinite with a strictly positive eigenvalue, $\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w}  0$ for any $\\mathbf{w}$ not in the null space. The dynamics drive $\\lVert\\mathbf{w}\\rVert^2 \\to 1$. The rule stabilizes the norm of the weight vector to unity. A more detailed stability analysis shows that $\\mathbf{w}$ converges to a unit-norm eigenvector corresponding to the largest eigenvalue of $\\mathbf{C}$, $\\mathbf{v}_{\\text{max}}$.\nCase 2: Non-zero mean, $\\boldsymbol{\\mu} \\neq \\mathbf{0}$.\nThe dynamics are driven by the correlation term $\\mathbb{E}[y\\mathbf{x}] = (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w} = \\mathbf{C}\\mathbf{w} + (\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$. Compared to the mean-centered case, the driving or \"drift\" term in the dynamics is modified from $\\mathbf{C}\\mathbf{w}$ to $(\\mathbf{C}+\\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w}$, which includes the extra term $(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$. This term correctly describes the bias towards the input mean.\n\nNow we evaluate the given options.\n\n**A. Under the correlation Hebbian rule, the expected weight dynamics satisfy $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\left(\\mathbf{C}\\,\\mathbf{w} + (\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}\\right)$, which implies an unbounded drift along $\\boldsymbol{\\mu}$ when $\\boldsymbol{\\mu} \\neq \\mathbf{0}$; there are no finite-norm stable fixed points without additional normalization.**\nOur derivation matches the provided equation for the dynamics. The term $\\eta(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$ creates positive feedback along the direction of $\\boldsymbol{\\mu}$, leading to unbounded growth. Both this term and the $\\eta\\mathbf{C}\\mathbf{w}$ term (due to $\\lambda_{\\text{max}}  0$) cause the norm to grow, hence the system is unstable and has no finite-norm stable fixed points.\n**Verdict: Correct.**\n\n**B. Under the covariance Hebbian rule, the expected weight dynamics satisfy $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\, \\mathbf{C}\\,\\mathbf{w}$, so the direction of $\\mathbf{w}$ aligns with the eigenvector of $\\mathbf{C}$ associated with its largest eigenvalue, but the norm $\\lVert \\mathbf{w} \\rVert$ grows without bound in the absence of normalization.**\nOur derivation confirms the dynamics are $\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbf{C}\\mathbf{w}$. This is the power method, which causes the direction of $\\mathbf{w}$ to converge to the principal eigenvector of $\\mathbf{C}$. Since $\\lambda_{\\text{max}}  0$, the norm of $\\mathbf{w}$ grows exponentially like $e^{\\eta\\lambda_{\\text{max}}t}$.\n**Verdict: Correct.**\n\n**C. The covariance Hebbian rule alone guarantees globally stable convergence of $\\mathbf{w}$ to a bounded fixed point for any positive semidefinite $\\mathbf{C}$, regardless of the value of $\\boldsymbol{\\mu}$.**\nThis is false. As established in the analysis for B, the covariance rule leads to unbounded growth of the weight norm whenever $\\mathbf{C}$ has a positive eigenvalue. It does not converge to a bounded fixed point (except for the unstable fixed point at $\\mathbf{w}=\\mathbf{0}$).\n**Verdict: Incorrect.**\n\n**D. Replacing either Hebbian rule with Oja's rule, the expected dynamics become $\\dfrac{d\\mathbf{w}}{dt} = \\mathbf{C}\\,\\mathbf{w} - (\\mathbf{w}^\\top \\mathbf{C}\\,\\mathbf{w})\\, \\mathbf{w}$ when inputs are mean-centered ($\\boldsymbol{\\mu} = \\mathbf{0}$), yielding stable convergence of $\\mathbf{w}$ to a unit-norm eigenvector of $\\mathbf{C}$ corresponding to the largest eigenvalue; with $\\boldsymbol{\\mu} \\neq \\mathbf{0}$ and raw inputs, an extra term $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ appears in the drift that biases the solution toward $\\boldsymbol{\\mu}$ unless inputs are centered.**\nThe analysis of Oja's rule confirms both parts of this statement. For $\\boldsymbol{\\mu}=\\mathbf{0}$, the dynamics (up to a scaling factor $\\eta$) and the stability properties are correctly described. For $\\boldsymbol{\\mu}\\neq\\mathbf{0}$, the effective \"drift\" term in the dynamics is modified from $\\mathbf{C}\\mathbf{w}$ to $(\\mathbf{C}+\\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w}$, which includes the extra term $(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$. This term correctly describes the bias towards the input mean.\n**Verdict: Correct.**\n\n**E. The correlation Hebbian rule is stable when $\\boldsymbol{\\mu} \\neq \\mathbf{0}$, because the term $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ acts as a weight decay that reduces $\\lVert \\mathbf{w} \\rVert$.**\nThis statement makes a fundamentally incorrect claim. The term $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ is a positive feedback term, not a decay term. A decay term would have a negative sign (e.g., $-\\gamma\\mathbf{w}$). Our analysis of the norm derivative, $\\frac{d}{dt}\\lVert\\mathbf{w}\\rVert^2 = 2\\eta (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w} + (\\mathbf{w}^\\top\\boldsymbol{\\mu})^2)$, shows that both terms contribute to norm growth, guaranteeing instability, not stability.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "While many theoretical models assume a static environment, neural systems must constantly adapt to changing input statistics. This practice  explores the performance of Oja's rule in a non-stationary setting, where the principal feature of the input is continuously rotating. By deriving the phase lag between the neuron's weight vector and the true eigenvector, you will quantify the system's ability to track dynamic signals and uncover the fundamental relationship between learning rate and tracking accuracy.",
            "id": "4025497",
            "problem": "Consider a single linear neuron receiving a two-dimensional zero-mean input $x(t) \\in \\mathbb{R}^{2}$ with time-varying covariance matrix $C(t)$. The synaptic weight vector is $w(t) \\in \\mathbb{R}^{2}$. The synapse obeys Oja’s rule, a normalized Hebbian plasticity law, given in its deterministic mean-field form by\n$$\n\\dot{w}(t) \\;=\\; \\eta \\left( C(t)\\, w(t) \\;-\\; \\big(w(t)^{\\top} C(t)\\, w(t)\\big)\\, w(t) \\right),\n$$\nwhere $\\eta0$ is the learning rate. Assume the input covariance is rank-one with a rotating principal axis,\n$$\nC(t) \\;=\\; e_{1}(t)\\, e_{1}(t)^{\\top}, \\qquad e_{1}(t) \\;=\\; \\begin{pmatrix}\\cos\\big(\\Omega t\\big) \\\\ \\sin\\big(\\Omega t\\big)\\end{pmatrix},\n$$\nwhere $\\Omega0$ is a constant drift angular frequency and angles are measured in radians. Define the signed angular lag $\\delta$ as the steady-state constant difference between the weight direction and the instantaneous top eigenvector direction, that is,\n$$\n\\delta \\;=\\; \\psi(t) \\;-\\; \\theta(t),\n$$\nin the phase-locked regime where $\\delta$ is time-independent, with $w(t) = \\big(\\cos\\psi(t),\\, \\sin\\psi(t)\\big)^{\\top}$ and $e_{1}(t) = \\big(\\cos\\theta(t),\\, \\sin\\theta(t)\\big)^{\\top}$, and $\\theta(t) = \\Omega t$. Starting from these definitions and the given dynamics, derive a closed-form analytic expression for the steady-state signed lag $\\delta$ as a function of $\\eta$ and $\\Omega$, under the assumption that the system achieves phase-locking. Express your final answer as a single analytic expression in radians. No numerical evaluation is required.",
            "solution": "The problem asks for the steady-state angular lag $\\delta$ of a weight vector $w(t)$ that tracks a rotating principal eigenvector $e_1(t)$ via Oja's rule. We are given the deterministic mean-field dynamics and assume a phase-locked solution exists.\n\nFirst, we express the components of the Oja update rule using the provided angular parameterizations. The weight vector is $w(t)$ with angle $\\psi(t)$, and the eigenvector is $e_1(t)$ with angle $\\theta(t) = \\Omega t$. The angular lag is $\\delta = \\psi(t) - \\theta(t)$, which is constant by assumption.\n\nThe scalar product $w(t)^\\top e_1(t)$ is:\n$$\nw(t)^\\top e_1(t) = \\cos(\\psi(t))\\cos(\\theta(t)) + \\sin(\\psi(t))\\sin(\\theta(t)) = \\cos(\\psi(t) - \\theta(t)) = \\cos(\\delta)\n$$\nUsing this, we can simplify the terms in the Oja equation:\n1.  $C(t)w(t) = (e_1(t)e_1(t)^\\top)w(t) = e_1(t)(e_1(t)^\\top w(t)) = e_1(t)\\cos(\\delta)$\n2.  $w(t)^\\top C(t) w(t) = w(t)^\\top (e_1(t)\\cos(\\delta)) = (w(t)^\\top e_1(t))\\cos(\\delta) = \\cos^2(\\delta)$\n\nSubstituting these back into the Oja's rule dynamics gives:\n$$\n\\dot{w}(t) = \\eta \\left( e_1(t)\\cos(\\delta) - w(t)\\cos^2(\\delta) \\right) \\quad (*_1)\n$$\nThis is the first expression for $\\dot{w}(t)$, derived from the learning rule.\n\nNext, we derive a kinematic expression for $\\dot{w}(t)$. Since $w(t) = (\\cos\\psi(t), \\sin\\psi(t))^\\top$, its time derivative is:\n$$\n\\dot{w}(t) = \\begin{pmatrix} -\\sin(\\psi(t))\\dot{\\psi}(t) \\\\ \\cos(\\psi(t))\\dot{\\psi}(t) \\end{pmatrix} = \\dot{\\psi}(t) w_\\perp(t)\n$$\nwhere $w_\\perp(t) = (-\\sin\\psi(t), \\cos\\psi(t))^\\top$ is the unit vector orthogonal to $w(t)$. In the phase-locked regime, $\\delta = \\psi(t) - \\theta(t) = \\text{const}$, which implies $\\dot{\\psi}(t) = \\dot{\\theta}(t)$. Since $\\theta(t) = \\Omega t$, we have $\\dot{\\theta}(t) = \\Omega$. Therefore, $\\dot{\\psi}(t) = \\Omega$.\nThe kinematic expression for $\\dot{w}(t)$ is thus:\n$$\n\\dot{w}(t) = \\Omega w_\\perp(t) \\quad (*_2)\n$$\nNow, we equate the two expressions for $\\dot{w}(t)$:\n$$\n\\Omega w_\\perp(t) = \\eta \\left( e_1(t)\\cos(\\delta) - w(t)\\cos^2(\\delta) \\right)\n$$\nTo solve for $\\delta$, we can project this vector equation onto $w_\\perp(t)$ by taking the dot product with $w_\\perp(t)^\\top$:\n$$\n\\Omega w_\\perp(t)^\\top w_\\perp(t) = \\eta \\cos(\\delta) (w_\\perp(t)^\\top e_1(t)) - \\eta \\cos^2(\\delta) (w_\\perp(t)^\\top w(t))\n$$\nWe evaluate the scalar products:\n- $w_\\perp(t)^\\top w_\\perp(t) = \\|w_\\perp(t)\\|^2 = 1$\n- $w_\\perp(t)^\\top w(t) = 0$ (by definition of orthogonality)\n- $w_\\perp(t)^\\top e_1(t) = (-\\sin\\psi(t))\\cos\\theta(t) + \\cos\\psi(t)\\sin\\theta(t) = \\sin(\\theta(t) - \\psi(t)) = \\sin(-\\delta) = -\\sin(\\delta)$\n\nSubstituting these back into the projected equation gives:\n$$\n\\Omega \\cdot 1 = \\eta \\cos(\\delta)(-\\sin(\\delta)) - \\eta \\cos^2(\\delta) \\cdot 0\n$$\n$$\n\\Omega = -\\eta \\sin(\\delta)\\cos(\\delta)\n$$\nUsing the double angle identity $\\sin(2\\delta) = 2\\sin(\\delta)\\cos(\\delta)$, we get:\n$$\n\\Omega = -\\frac{\\eta}{2} \\sin(2\\delta)\n$$\nSolving for $\\sin(2\\delta)$:\n$$\n\\sin(2\\delta) = -\\frac{2\\Omega}{\\eta}\n$$\nFinally, we solve for the lag $\\delta$ by taking the inverse sine. The stable solution corresponds to the principal value of the arcsin function.\n$$\n2\\delta = \\arcsin\\left(-\\frac{2\\Omega}{\\eta}\\right)\n$$\nUsing the property that $\\arcsin(-x) = -\\arcsin(x)$:\n$$\n2\\delta = -\\arcsin\\left(\\frac{2\\Omega}{\\eta}\\right)\n$$\n$$\n\\delta = -\\frac{1}{2}\\arcsin\\left(\\frac{2\\Omega}{\\eta}\\right)\n$$\nThis is the desired closed-form expression for the angular lag. A real solution exists if $|\\frac{2\\Omega}{\\eta}| \\le 1$, meaning the learning rate $\\eta$ must be at least twice the angular frequency $\\Omega$ for tracking to be possible. The negative sign indicates that the weight vector lags behind the target eigenvector, as expected.",
            "answer": "$$\n\\boxed{-\\frac{1}{2}\\arcsin\\left(\\frac{2\\Omega}{\\eta}\\right)}\n$$"
        },
        {
            "introduction": "Synaptic normalization can be viewed as a form of constrained optimization, but the choice of constraint has profound consequences. This problem  extends the concept of normalization beyond the standard $L_2$ norm implicit in Oja's rule to explore the effects of an $L_1$ norm constraint. Analyzing the stability and structure of the resulting synaptic weights will reveal the crucial link between the mathematical form of the constraint and the emergence of sparse versus distributed neural representations.",
            "id": "4025498",
            "problem": "A single linear neuron receives zero-mean inputs $x \\in \\mathbb{R}^d$ with covariance matrix $C \\in \\mathbb{R}^{d \\times d}$ that is symmetric and positive semidefinite. Its output is $y = w^\\top x$ for a synaptic weight vector $w \\in \\mathbb{R}^d$. Consider synaptic plasticity rules that ascend the expected output power under a fixed-norm synaptic normalization constraint, so that in the continuous-time limit the dynamics implement constrained ascent of the objective $J(w) = \\mathbb{E}[y^2] = w^\\top C w$ subject to either an $L_2$ constraint $\\|w\\|_2 = 1$ or an $L_1$ constraint $\\|w\\|_1 = 1$. This setting is a standard abstraction of Hebbian learning with synaptic normalization, of which Oja’s rule is a canonical $L_2$-normalized instance. Assume a data regime with a small set of $k$ dominant features: there exists an index set $S \\subset \\{1,\\dots,d\\}$ with $|S| = k$ and $k \\ll d$, such that the eigenvalues of the principal submatrix $C_{SS}$ are much larger than those of $C$ restricted to $S^c$, and such that within $S$ the off-diagonal entries of $C$ are not all zero so that the top eigenvector of $C$ has nonzero loadings (of the same sign) on all coordinates in $S$. In this regime, answer the following by selecting all correct statements about fixed points, their stability, and the sparsity of the converged $w$ under the two different norm constraints.\n\nA. Under the $L_2$ constraint, the asymptotically stable fixed points coincide with the unit-norm eigenvectors associated with the top eigenvalue of $C$ (up to a global sign). In the stated regime with mutually correlated dominant features, the converged $w$ typically has nonzero components across all coordinates in $S$ (and need not be sparse in the ambient basis).\n\nB. Under the $L_1$ constraint, if $C$ is diagonal with a unique strictly largest diagonal element $C_{ii}$, then every asymptotically stable fixed point of the constrained ascent has exactly one nonzero entry at coordinate $i$ (with either sign), i.e., $w = \\pm e_i$, and all other coordinates are exactly zero.\n\nC. Under the $L_2$ constraint with a simple spectral gap between the top two eigenvalues of $C$, there exists a continuous one-parameter family (a circle) of asymptotically stable fixed points consisting of all unit vectors in the span of the top two eigenvectors.\n\nD. Under the $L_1$ constraint with two equal dominant variances and negligible others, there is a continuum of asymptotically stable fixed points forming a nontrivial line segment on the $L_1$ sphere with nonzero weights on both dominant coordinates.\n\nE. Under the $L_2$ constraint, non-principal eigenvectors are unstable and stability is governed by the spectral gap; under the $L_1$ constraint, when multiple dominant features are exactly tied (e.g., equal diagonal entries in a diagonal $C$), there are multiple discrete sparse attractors corresponding to the tied features (sign-symmetric), but not a continuum of stable fixed points.",
            "solution": "This problem analyzes the properties of stable fixed points for a learning rule that maximizes output variance ($J(w) = w^\\top C w$) subject to either an $L_2$ or an $L_1$ norm constraint on the weight vector $w$.\n\n**Analysis of the $L_2$ constraint:**\nThis is the standard Principal Component Analysis (PCA) problem. The objective is to maximize the Rayleigh quotient $w^\\top C w / w^\\top w$. The stationary points are the eigenvectors of $C$. Gradient ascent dynamics on the sphere (as implemented by Oja's rule) converge to the eigenvector(s) corresponding to the largest eigenvalue, $\\lambda_{\\max}$.\n- If $\\lambda_{\\max}$ is a unique (simple) eigenvalue, the only asymptotically stable fixed points are the corresponding pair of eigenvectors $\\pm v_{\\max}$. Any other eigenvector is an unstable fixed point (a saddle point).\n- If $\\lambda_{\\max}$ is degenerate (e.g., $\\lambda_1 = \\lambda_2 > \\lambda_3$), then any unit vector in the subspace spanned by the corresponding eigenvectors is a stable fixed point, forming a continuous manifold of stable solutions.\n\n**Analysis of the $L_1$ constraint:**\nThis is a formulation of Sparse PCA. The constraint surface, $\\|w\\|_1=1$, is a cross-polytope. The corners of this polytope are the sparse vectors $\\pm e_i$ (where $e_i$ is a standard basis vector). Gradient ascent dynamics on this surface tend to drive solutions towards these corners, promoting sparsity.\n- A stable fixed point must be a local maximum of $w^\\top C w$ on the surface $\\|w\\|_1=1$.\n- Due to the geometry of the $L_1$ ball, solutions often have many zero components.\n\nNow, let's evaluate each statement:\n\n**A. Under the $L_2$ constraint, the asymptotically stable fixed points coincide with the unit-norm eigenvectors associated with the top eigenvalue of $C$ (up to a global sign). In the stated regime with mutually correlated dominant features, the converged $w$ typically has nonzero components across all coordinates in $S$ (and need not be sparse in the ambient basis).**\nThis is correct. The first part is the fundamental result of PCA and Oja-type learning. The second part follows from the problem's specific assumption that the top eigenvector of $C$ (the solution) has non-zero loadings on all coordinates in the dominant set $S$. PCA generally produces dense eigenvectors, so the converged weight vector $w$ will be dense, not sparse.\n\n**B. Under the $L_1$ constraint, if $C$ is diagonal with a unique strictly largest diagonal element $C_{ii}$, then every asymptotically stable fixed point of the constrained ascent has exactly one nonzero entry at coordinate $i$ (with either sign), i.e., $w = \\pm e_i$, and all other coordinates are exactly zero.**\nThis is correct. If $C$ is diagonal, $J(w) = \\sum_j C_{jj} w_j^2$. To maximize this subject to $\\sum_j |w_j|=1$, we should put all the \"mass\" of the $L_1$ norm onto the component $w_i$ for which the coefficient $C_{ii}$ is largest. This happens when $w = \\pm e_i$. These \"corner\" solutions are the only stable maxima. Any solution with weight on other components could increase its objective value by shifting that weight to the $i$-th component.\n\n**C. Under the $L_2$ constraint with a simple spectral gap between the top two eigenvalues of $C$, there exists a continuous one-parameter family (a circle) of asymptotically stable fixed points consisting of all unit vectors in the span of the top two eigenvectors.**\nThis is incorrect. A simple spectral gap means $\\lambda_1 > \\lambda_2$, so the top eigenvalue is unique. In this case, there are only two asymptotically stable fixed points, $\\pm v_1$. A continuous family of stable fixed points (a circle) only arises in the case of a degenerate top eigenvalue, i.e., $\\lambda_1 = \\lambda_2$.\n\n**D. Under the $L_1$ constraint with two equal dominant variances and negligible others, there is a continuum of asymptotically stable fixed points forming a nontrivial line segment on the $L_1$ sphere with nonzero weights on both dominant coordinates.**\nThis is incorrect. Let's assume a diagonal $C$ with $C_{11} = C_{22} > C_{jj}$ for $j>2$. The objective to maximize on the $L_1$ sphere is $J(w) = C_{11}(w_1^2 + w_2^2)$ subject to $|w_1|+|w_2|=1$. Let $|w_1|=\\alpha$ and $|w_2|=1-\\alpha$ for $\\alpha \\in [0,1]$. The objective is $C_{11}(\\alpha^2 + (1-\\alpha)^2)$. This is a U-shaped parabola in $\\alpha$ whose minima is at $\\alpha=1/2$ and whose maxima are at the endpoints $\\alpha=0$ and $\\alpha=1$. Therefore, the stable fixed points are the sparse solutions $(\\pm 1, 0)$ and $(0, \\pm 1)$. The continuum of solutions where both weights are non-zero are unstable minima with respect to perturbations that make the solution sparser.\n\n**E. Under the $L_2$ constraint, non-principal eigenvectors are unstable and stability is governed by the spectral gap; under the $L_1$ constraint, when multiple dominant features are exactly tied (e.g., equal diagonal entries in a diagonal $C$), there are multiple discrete sparse attractors corresponding to the tied features (sign-symmetric), but not a continuum of stable fixed points.**\nThis is correct. The first clause accurately summarizes stability for the $L_2$ case. The second clause accurately summarizes the outcome for the $L_1$ case with tied dominant features, as analyzed for option D. The stable fixed points are the discrete, sparse solutions corresponding to the tied features (e.g., $\\pm e_i, \\pm e_j$ if $C_{ii}=C_{jj}$), not a continuous manifold of solutions.\n\nTherefore, the correct statements are A, B, and E.",
            "answer": "$$\\boxed{ABE}$$"
        }
    ]
}