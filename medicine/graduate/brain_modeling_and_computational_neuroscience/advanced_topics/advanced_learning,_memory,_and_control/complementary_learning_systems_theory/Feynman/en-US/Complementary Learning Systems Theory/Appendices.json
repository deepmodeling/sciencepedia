{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone prediction of Complementary Learning Systems (CLS) theory is its elegant explanation for temporally graded retrograde amnesia, where brain lesions impair recent memories more than remote ones. This exercise guides you through building a simple but powerful model of the fast (hippocampal) and slow (cortical) memory traces to observe this principle directly. By simulating a lesion of the fast system, you will quantify how memory impairment varies with age, providing a tangible demonstration of the consolidation process at the heart of CLS theory .",
            "id": "3970414",
            "problem": "You are asked to implement a simulation and analysis grounded in Complementary Learning Systems (CLS) theory to demonstrate how lesioning the fast learning system produces a deficit in recent memory while sparing remote memory, and to quantify a gradient that captures how this effect depends on the consolidation rate. The simulation must be specified purely in mathematical terms and solved algorithmically. The base assumptions and definitions are as follows. Complementary Learning Systems (CLS) posit two interacting learning systems: a fast system and a slow system. Consider a single scalar target item with true value $y=1$ presented at time $t=0$. Let the fast system trace be $h(t)$ and the slow system trace be $c(t)$. The encoding event at $t=0$ updates the fast trace immediately according to $h(0^{+})=\\alpha_h$ and the slow trace according to $c(0^{+})=\\alpha_c$, where $0<\\alpha_c\\ll\\alpha_h\\leq 1$ reflects rapid initial learning in the fast system and weak initial learning in the slow system. After encoding, the fast trace decays exponentially, modeled as the solution of the linear differential equation $dh/dt=-\\lambda_h h$ with solution $h(t)=\\alpha_h e^{-\\lambda_h t}$ for $t\\geq 0$, where $\\lambda_h>0$ is the decay rate of the fast system. The slow system accumulates consolidation via replay from the fast system without intrinsic decay over the simulated horizon, modeled as $dc/dt=\\beta h(t)$ with $\\lambda_c=0$, giving $c(t)=\\alpha_c+\\beta\\alpha_h\\int_0^t e^{-\\lambda_h s}\\,ds=\\alpha_c+\\beta\\alpha_h\\frac{1-e^{-\\lambda_h t}}{\\lambda_h}$ for $t\\geq 0$ and $\\beta\\geq 0$ the consolidation rate. Assume the intact prediction at time $t$ is the sum of system outputs, $\\hat{y}_{\\text{intact}}(t)=h(t)+c(t)$, and the lesioned prediction (fast system removed) is $\\hat{y}_{\\text{lesion}}(t)=c(t)$. Define the impairment fraction $I(t;\\beta)=1-\\frac{\\hat{y}_{\\text{lesion}}(t)}{\\hat{y}_{\\text{intact}}(t)}=1-\\frac{c(t)}{h(t)+c(t)}$, which quantifies how much performance is lost due to the lesion at retention interval $t$. Define the remote-sparing gradient as $G(\\beta)=I(t_{\\text{recent}};\\beta)-I(t_{\\text{remote}};\\beta)$ for fixed retention intervals $t_{\\text{recent}}>0$ and $t_{\\text{remote}}\\gg t_{\\text{recent}}$. Your task is to compute $G(\\beta)$ for a small test suite of $\\beta$ values, using fixed, scientifically plausible parameters. Use $\\alpha_h=1.0$, $\\alpha_c=0.05$, $\\lambda_h=1.0$, $t_{\\text{recent}}=0.2$, and $t_{\\text{remote}}=10.0$. These values are dimensionless and there are no physical units involved. The test suite consists of four consolidation rates $\\beta\\in\\{0.0,0.1,0.5,1.0\\}$ that cover boundary and typical cases: $\\beta=0.0$ (no consolidation), $\\beta=0.1$ (weak consolidation), $\\beta=0.5$ (moderate consolidation), and $\\beta=1.0$ (strong consolidation). For each $\\beta$, compute $G(\\beta)$ as a float rounded to six decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\") in the exact order of the provided test suite $\\beta$ values.",
            "solution": "The problem is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- **Theory**: Complementary Learning Systems (CLS) with a fast system and a slow system.\n- **Target Item**: A single scalar target item with true value $y=1$ presented at time $t=0$.\n- **System Traces**: Fast system trace $h(t)$, slow system trace $c(t)$.\n- **Initial Conditions ($t=0^+$)**: $h(0^{+})=\\alpha_h$ and $c(0^{+})=\\alpha_c$.\n- **Parameter Constraints**: $0<\\alpha_c\\ll\\alpha_h\\leq 1$.\n- **Fast System Dynamics ($t \\geq 0$)**: Differential equation $dh/dt=-\\lambda_h h$, with solution $h(t)=\\alpha_h e^{-\\lambda_h t}$. The decay rate is $\\lambda_h>0$.\n- **Slow System Dynamics ($t \\geq 0$)**: Differential equation $dc/dt=\\beta h(t)$, with intrinsic decay $\\lambda_c=0$. The solution is given as $c(t)=\\alpha_c+\\beta\\alpha_h\\int_0^t e^{-\\lambda_h s}\\,ds=\\alpha_c+\\beta\\alpha_h\\frac{1-e^{-\\lambda_h t}}{\\lambda_h}$. The consolidation rate is $\\beta\\geq 0$.\n- **Prediction Models**: \n    - Intact system: $\\hat{y}_{\\text{intact}}(t)=h(t)+c(t)$.\n    - Lesioned system (fast system removed): $\\hat{y}_{\\text{lesion}}(t)=c(t)$.\n- **Impairment Fraction Definition**: $I(t;\\beta)=1-\\frac{\\hat{y}_{\\text{lesion}}(t)}{\\hat{y}_{\\text{intact}}(t)}=1-\\frac{c(t)}{h(t)+c(t)}$.\n- **Remote-Sparing Gradient Definition**: $G(\\beta)=I(t_{\\text{recent}};\\beta)-I(t_{\\text{remote}};\\beta)$.\n- **Fixed Parameters**:\n    - $\\alpha_h=1.0$\n    - $\\alpha_c=0.05$\n    - $\\lambda_h=1.0$\n    - $t_{\\text{recent}}=0.2$\n    - $t_{\\text{remote}}=10.0$\n- **Task**: Compute $G(\\beta)$ for a test suite of consolidation rates $\\beta\\in\\{0.0,0.1,0.5,1.0\\}$.\n- **Output Format**: A comma-separated list of float values rounded to six decimal places, enclosed in square brackets.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is a mathematical formalization of the Complementary Learning Systems (CLS) theory, a cornerstone of computational neuroscience for explaining memory consolidation. The model equations for exponential decay and integrated consolidation are standard and plausible simplifications used in the field. The phenomenon it aims to model—greater impairment of recent versus remote memories after hippocampal lesion (Ribot's Law)—is a well-documented empirical finding that CLS theory addresses. The problem is scientifically sound.\n- **Well-Posed**: All functions, variables, and parameters are explicitly defined. The task is a direct computation based on these definitions. The denominator in the expression for $I(t;\\beta)$, which is $h(t)+c(t)$, is always positive for the given parameter ranges ($h(t)=\\alpha_h e^{-\\lambda_h t} > 0$ for finite $t$, and $c(t)=\\alpha_c+\\dots \\geq \\alpha_c > 0$), so the function is always well-defined. A unique, stable solution exists for each $\\beta$.\n- **Objective**: The problem is stated in precise, quantitative, and unbiased mathematical language.\n- **Complete and Consistent**: The problem provides all necessary data and relationships to perform the calculation. There are no contradictions in the definitions or constraints.\n- **No Other Flaws**: The problem is not metaphorical, is directly relevant to its stated topic, is not ill-posed, trivial, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be developed.\n\nThe objective is to compute the remote-sparing gradient, $G(\\beta)$, for several values of the consolidation rate $\\beta$. This gradient quantifies the differential impairment between recent and remote memory recall, a key prediction of CLS theory.\n\nThe analysis begins from the provided definitions. The impairment fraction $I(t;\\beta)$ is defined as:\n$$\nI(t;\\beta) = 1 - \\frac{c(t)}{h(t)+c(t)}\n$$\nThis can be algebraically simplified to express the impairment as the ratio of the fast system's contribution to the total system output:\n$$\nI(t;\\beta) = \\frac{(h(t)+c(t)) - c(t)}{h(t)+c(t)} = \\frac{h(t)}{h(t)+c(t)}\n$$\nThe expressions for the fast trace, $h(t)$, and slow trace, $c(t)$, are given by:\n$$\nh(t) = \\alpha_h e^{-\\lambda_h t}\n$$\n$$\nc(t) = \\alpha_c + \\beta\\alpha_h\\frac{1-e^{-\\lambda_h t}}{\\lambda_h}\n$$\nWe substitute the given constant parameter values: $\\alpha_h=1.0$, $\\alpha_c=0.05$, and $\\lambda_h=1.0$.\n$$\nh(t) = 1.0 \\cdot e^{-1.0 \\cdot t} = e^{-t}\n$$\n$$\nc(t) = 0.05 + \\beta \\cdot 1.0 \\cdot \\frac{1-e^{-1.0 \\cdot t}}{1.0} = 0.05 + \\beta(1-e^{-t})\n$$\nSubstituting these simplified forms into the expression for $I(t;\\beta)$ yields:\n$$\nI(t;\\beta) = \\frac{e^{-t}}{e^{-t} + (0.05 + \\beta(1-e^{-t}))} = \\frac{e^{-t}}{e^{-t} + 0.05 + \\beta - \\beta e^{-t}}\n$$\n$$\nI(t;\\beta) = \\frac{e^{-t}}{(1-\\beta)e^{-t} + 0.05 + \\beta}\n$$\nThe remote-sparing gradient, $G(\\beta)$, is defined as the difference in impairment between a recent and a remote time point:\n$$\nG(\\beta) = I(t_{\\text{recent}};\\beta) - I(t_{\\text{remote}};\\beta)\n$$\nUsing the specified time points $t_{\\text{recent}}=0.2$ and $t_{\\text{remote}}=10.0$:\n$$\nG(\\beta) = I(0.2;\\beta) - I(10.0;\\beta)\n$$\nThis gives the final computable expression for the gradient:\n$$\nG(\\beta) = \\left( \\frac{e^{-0.2}}{(1-\\beta)e^{-0.2} + 0.05 + \\beta} \\right) - \\left( \\frac{e^{-10.0}}{(1-\\beta)e^{-10.0} + 0.05 + \\beta} \\right)\n$$\nThis function will be evaluated for each value of $\\beta$ in the test suite $\\{0.0, 0.1, 0.5, 1.0\\}$. The algorithm is as follows:\n1.  For each given $\\beta$:\n2.  Calculate the value of $I(0.2;\\beta)$ using the formula above.\n3.  Calculate the value of $I(10.0;\\beta)$ using the formula above.\n4.  Compute their difference, $G(\\beta) = I(0.2;\\beta) - I(10.0;\\beta)$.\n5.  Round the result to six decimal places.\n\nThis procedure demonstrates a core prediction of CLS theory: for any non-negative consolidation rate $\\beta$, the impairment at a recent time ($t_{\\text{recent}}$) is substantially larger than at a remote time ($t_{\\text{remote}}$), where the fast trace $h(t)$ has decayed to nearly zero. The gradient $G(\\beta)$ captures this effect. As $\\beta$ increases, consolidation is more effective, so the slow system becomes a reliable source of memory more quickly, which is expected to decrease the overall magnitude of the impairment $I(t;\\beta)$ at all times, and thus may alter the gradient $G(\\beta)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the remote-sparing gradient G(beta) for a set of consolidation rates\n    based on a simplified model of Complementary Learning Systems (CLS) theory.\n    \"\"\"\n\n    # Define the fixed parameters from the problem statement.\n    # These values are dimensionless.\n    alpha_h = 1.0  # Initial strength of the fast system trace\n    alpha_c = 0.05 # Initial strength of the slow system trace\n    lambda_h = 1.0 # Decay rate of the fast system trace\n    t_recent = 0.2 # Retention interval for recent memory\n    t_remote = 10.0 # Retention interval for remote memory\n\n    # Define the test suite of consolidation rates (beta).\n    test_cases = [0.0, 0.1, 0.5, 1.0]\n\n    def impairment_fraction(t: float, beta: float) -> float:\n        \"\"\"\n        Calculates the impairment fraction I(t; beta) for a given retention\n        interval t and consolidation rate beta.\n\n        The impairment fraction is defined as I = h(t) / (h(t) + c(t)).\n        \"\"\"\n        # Fast system trace h(t)\n        # h(t) = alpha_h * exp(-lambda_h * t)\n        h_t = alpha_h * np.exp(-lambda_h * t)\n\n        # Slow system trace c(t)\n        # c(t) = alpha_c + beta * alpha_h * (1 - exp(-lambda_h * t)) / lambda_h\n        c_t = alpha_c + beta * alpha_h * (1 - np.exp(-lambda_h * t)) / lambda_h\n        \n        # Total intact system output\n        y_intact = h_t + c_t\n        \n        # The problem validation confirmed that y_intact > 0 for all t>=0.\n        return h_t / y_intact\n\n    results = []\n    for beta in test_cases:\n        # Calculate the impairment for the recent memory time point.\n        I_recent = impairment_fraction(t_recent, beta)\n\n        # Calculate the impairment for the remote memory time point.\n        I_remote = impairment_fraction(t_remote, beta)\n        \n        # The remote-sparing gradient is the difference between the two impairments.\n        G_beta = I_recent - I_remote\n        \n        # Round the result to six decimal places as required.\n        results.append(round(G_beta, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond behavioral outcomes, CLS theory posits that the hippocampus and neocortex employ fundamentally different strategies for representing information. This practice delves into the geometry of these neural representations, tasking you with simulating how the hippocampus enacts pattern separation to create distinct, orthogonal memory traces, while the cortex gradually integrates information to form structured, category-based knowledge. This exercise offers a concrete implementation of the distinct coding principles that enable the two systems to fulfill their complementary roles .",
            "id": "3970454",
            "problem": "You are asked to implement, from first principles, an analysis pipeline grounded in the Complementary Learning Systems (CLS) theory to test whether hippocampal representations become more orthogonal while cortical representations become more clustered by semantic category across learning. Represent each item by an input feature vector and simulate two learning systems: a hippocampal system that performs rapid pattern separation and a cortical system that performs gradual category-based integration. The pipeline must quantify and test hypothesized changes in representational geometry over learning epochs using mathematically defined metrics.\n\nFundamental definitions:\n- Let there be $N$ items, each associated with a semantic category label $\\kappa(i) \\in \\{1,\\dots,K\\}$, and an input feature vector $\\mathbf{x}_i \\in \\mathbb{R}^d$.\n- Cosine similarity between two non-zero vectors $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^d$ is defined as $\\operatorname{cos}(\\mathbf{a}, \\mathbf{b}) = \\dfrac{\\mathbf{a}^\\top \\mathbf{b}}{\\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2}$, where $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n- Hippocampal orthogonality is operationalized as the inverse of mean absolute pairwise cosine similarity across items. Define the hippocampal orthogonality cost at epoch $t$ as $$J_h^{(t)} = \\frac{2}{N(N-1)} \\sum_{1 \\le i < j \\le N} \\left|\\operatorname{cos}\\left(\\mathbf{h}_i^{(t)}, \\mathbf{h}_j^{(t)}\\right)\\right|,$$ where $\\mathbf{h}_i^{(t)}$ is the hippocampal representation of item $i$ at epoch $t$. Lower $J_h^{(t)}$ indicates greater orthogonality.\n- Cortical clustering by category is operationalized as the difference between mean within-category similarity and mean between-category similarity. Define the cortical clustering index at epoch $t$ as $$I_c^{(t)} = \\overline{\\operatorname{cos}}_{\\text{within}}^{(t)} - \\overline{\\operatorname{cos}}_{\\text{between}}^{(t)},$$ where $\\overline{\\operatorname{cos}}_{\\text{within}}^{(t)}$ averages $\\operatorname{cos}\\left(\\mathbf{c}_i^{(t)}, \\mathbf{c}_j^{(t)}\\right)$ over all pairs with $\\kappa(i)=\\kappa(j)$ and $\\overline{\\operatorname{cos}}_{\\text{between}}^{(t)}$ averages over all pairs with $\\kappa(i)\\ne\\kappa(j)$; $\\mathbf{c}_i^{(t)}$ is the cortical representation at epoch $t$. Higher $I_c^{(t)}$ indicates stronger category clustering.\n\nLearning dynamics to implement:\n- Hippocampus (pattern separation): Starting with $\\mathbf{h}_i^{(0)} = \\mathbf{x}_i$, update $\\mathbf{h}_i^{(t)}$ by iteratively reducing pairwise cosine similarities. The algorithmic step should be derived from minimizing the sum of squared cosine similarities with respect to $\\mathbf{h}_i^{(t)}$, yielding an update that subtracts projections onto directions of other items weighted by their current cosine similarity. Use a learning rate $\\eta_h > 0$ and implement a stable, local update that uses current normalized representations.\n- Cortex (category integration): Starting with $\\mathbf{c}_i^{(0)} = \\mathbf{x}_i$, update $\\mathbf{c}_i^{(t)}$ by moving each item representation toward its category mean prototype, using a learning rate $\\eta_c > 0$. The category mean prototype should be computed from the input features $\\{\\mathbf{x}_i\\}$ for each category.\n\nDecision criteria:\n- Hippocampus improvement boolean is defined as true if two conditions hold: (i) net decrease $J_h^{(T)} < J_h^{(0)} - \\delta_h$, and (ii) at least a fraction $q_h$ of epoch-to-epoch differences are non-positive, i.e., $J_h^{(t+1)} - J_h^{(t)} \\le 0$.\n- Cortex improvement boolean is defined as true if two conditions hold: (i) net increase $I_c^{(T)} > I_c^{(0)} + \\delta_c$, and (ii) at least a fraction $q_c$ of epoch-to-epoch differences are non-negative, i.e., $I_c^{(t+1)} - I_c^{(t)} \\ge 0$.\nUse thresholds $\\delta_h = 0.05$, $\\delta_c = 0.05$, and fractions $q_h = 0.7$, $q_c = 0.7$.\n\nData generation:\n- For each category $k \\in \\{1,\\dots,K\\}$, generate a prototype $\\boldsymbol{\\mu}_k \\in \\mathbb{R}^d$ as a sample from a standard normal distribution and normalize it to unit norm. For each item $i$ in category $k$, generate $\\mathbf{x}_i = \\boldsymbol{\\mu}_k + \\boldsymbol{\\epsilon}_i$, where $\\boldsymbol{\\epsilon}_i \\sim \\mathcal{N}\\left(\\mathbf{0}, \\sigma^2 \\mathbf{I}_d\\right)$ and $\\mathbf{I}_d$ is the $d \\times d$ identity matrix. Ensure $\\mathbf{x}_i$ is not the zero vector by the properties of the Gaussian noise and prototype sampling. Use the specified random seed in each test case to make the simulation deterministic.\n\nTest suite:\nImplement the pipeline for the following test cases. Each test case specifies $(d, K, n, \\sigma, T, \\eta_h, \\eta_c, \\text{seed})$, where $n$ is the number of items per category and $T$ is the number of epochs.\n1. Happy path case: $(d = 32, K = 3, n = 10, \\sigma = 0.3, T = 20, \\eta_h = 0.2, \\eta_c = 0.05, \\text{seed} = 123)$.\n2. No-learning boundary case: $(d = 32, K = 3, n = 10, \\sigma = 0.3, T = 20, \\eta_h = 0.0, \\eta_c = 0.0, \\text{seed} = 123)$.\n3. High-noise edge case: $(d = 32, K = 3, n = 10, \\sigma = 2.0, T = 20, \\eta_h = 0.2, \\eta_c = 0.01, \\text{seed} = 234)$.\n4. Slow-learning long-run case: $(d = 32, K = 3, n = 10, \\sigma = 0.5, T = 100, \\eta_h = 0.05, \\eta_c = 0.02, \\text{seed} = 345)$.\n\nRequired output:\n- For each test case, produce a two-element list $[\\text{hippocampus\\_improved}, \\text{cortex\\_improved}]$ where each element is a boolean.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the form $$[[b_{h,1}, b_{c,1}],[b_{h,2}, b_{c,2}],[b_{h,3}, b_{c,3}],[b_{h,4}, b_{c,4}]],$$ where $b_{h,i}$ and $b_{c,i}$ are the booleans for test case $i$.\n\nAngle units are not applicable. No physical units are involved.",
            "solution": "The problem requires the implementation and analysis of a computational model inspired by the Complementary Learning Systems (CLS) theory. The model simulates two learning systems, a hippocampus-like system and a cortex-like system, to test the hypothesis that their representational geometries diverge over learning epochs. The hippocampal system is expected to orthogonalize representations (pattern separation), while the cortical system is expected to cluster representations based on semantic categories (integration). The analysis will be performed on simulated data, and the outcomes will be evaluated against specific quantitative criteria.\n\nFirst, we establish the procedure for generating the input data. We are given $K$ semantic categories, with $n$ items per category, for a total of $N=nK$ items. Each item $i$ is represented by a $d$-dimensional feature vector $\\mathbf{x}_i \\in \\mathbb{R}^d$ and is assigned a category label $\\kappa(i) \\in \\{1,\\dots,K\\}$. The data generation process is as follows:\n1. For each category $k$, a prototype vector $\\boldsymbol{\\mu}_k \\in \\mathbb{R}^d$ is sampled from a multivariate standard normal distribution, $\\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$. This prototype is then normalized to have a unit Euclidean norm, i.e., $\\|\\boldsymbol{\\mu}_k\\|_2 = 1$.\n2. For each item $i$ belonging to category $k$, its feature vector $\\mathbf{x}_i$ is generated by adding Gaussian noise to the category prototype: $\\mathbf{x}_i = \\boldsymbol{\\mu}_k + \\boldsymbol{\\epsilon}_i$, where the noise term $\\boldsymbol{\\epsilon}_i$ is drawn from an isotropic Gaussian distribution $\\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_d)$ with a given standard deviation $\\sigma$. This process creates a dataset where items from the same category are clustered around their respective prototypes in the feature space. The use of a specified random seed ensures the deterministic reproducibility of the generated data.\n\nNext, we define the learning dynamics for the two systems over $T$ epochs. Both systems start with representations identical to the input features: $\\mathbf{h}_i^{(0)} = \\mathbf{x}_i$ and $\\mathbf{c}_i^{(0)} = \\mathbf{x}_i$ for all items $i$.\n\nThe hippocampal system is designed to perform pattern separation by making representations more orthogonal. The update rule is described as an iterative process of reducing pairwise cosine similarities by subtracting projections. A rigorous and stable implementation of this concept is a synchronous, partial Gram-Schmidt-like orthogonalization procedure. At each epoch $t$, we update the representation $\\mathbf{h}_i^{(t)}$ for each item $i$ as follows:\n$$ \\mathbf{h}_i^{(t+1)} = \\mathbf{h}_i^{(t)} - \\eta_h \\sum_{j \\neq i} \\frac{\\left(\\mathbf{h}_i^{(t)}\\right)^\\top \\mathbf{h}_j^{(t)}}{\\left\\|\\mathbf{h}_j^{(t)}\\right\\|_2^2} \\mathbf{h}_j^{(t)} $$\nHere, $\\eta_h > 0$ is the hippocampal learning rate. The term $\\frac{(\\mathbf{h}_i^{(t)})^\\top \\mathbf{h}_j^{(t)}}{\\|\\mathbf{h}_j^{(t)}\\|_2^2} \\mathbf{h}_j^{(t)}$ is the projection of vector $\\mathbf{h}_i^{(t)}$ onto vector $\\mathbf{h}_j^{(t)}$. By subtracting a sum of these projections, the update rule moves $\\mathbf{h}_i^{(t+1)}$ into a direction that is more orthogonal to the subspace spanned by the other representations $\\{\\mathbf{h}_j^{(t)}\\}_{j \\neq i}$. The updates are synchronous, meaning all new representations $\\mathbf{h}^{(t+1)}$ are computed based on the set of representations from epoch $t$.\n\nThe cortical system is designed to perform category-based integration. It updates representations by moving them closer to their respective category's prototypical representation. First, for each category $k$, a fixed category mean prototype $\\mathbf{m}_k$ is computed from the initial input features:\n$$ \\mathbf{m}_k = \\frac{1}{n} \\sum_{i | \\kappa(i)=k} \\mathbf{x}_i $$\nThe update rule for a cortical representation $\\mathbf{c}_i^{(t)}$ of an item $i$ in category $k$ is a linear interpolation towards this prototype:\n$$ \\mathbf{c}_i^{(t+1)} = (1 - \\eta_c)\\mathbf{c}_i^{(t)} + \\eta_c \\mathbf{m}_k $$\nwhere $\\eta_c > 0$ is the cortical learning rate. This rule gradually pulls all representations of items within the same category towards a common point, thereby increasing their similarity and forming a cluster.\n\nTo quantify the changes in representational geometry, we calculate two metrics at each epoch $t \\in \\{0, \\dots, T\\}$.\n\nThe hippocampal orthogonality cost, $J_h^{(t)}$, measures the average pairwise similarity of hippocampal representations. A lower value indicates greater orthogonality. It is defined as:\n$$ J_h^{(t)} = \\frac{2}{N(N-1)} \\sum_{1 \\le i < j \\le N} \\left|\\operatorname{cos}\\left(\\mathbf{h}_i^{(t)}, \\mathbf{h}_j^{(t)}\\right)\\right| $$\nwhere $\\operatorname{cos}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a}^\\top \\mathbf{b}}{\\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2}$. This is computed by finding the mean of the absolute values of the elements in the upper triangle of the pairwise cosine-similarity matrix of all hippocampal representations.\n\nThe cortical clustering index, $I_c^{(t)}$, measures the degree to which cortical representations are structured by category. A higher value indicates stronger clustering. It is defined as a-la-Silhouette-score:\n$$ I_c^{(t)} = \\overline{\\operatorname{cos}}_{\\text{within}}^{(t)} - \\overline{\\operatorname{cos}}_{\\text{between}}^{(t)} $$\nHere, $\\overline{\\operatorname{cos}}_{\\text{within}}^{(t)}$ is the mean cosine similarity between pairs of items from the same category, and $\\overline{\\operatorname{cos}}_{\\text{between}}^{(t)}$ is the mean cosine similarity between pairs of items from different categories.\n\nFinally, we apply decision criteria to determine if the learning objectives were met. A boolean value is computed for each system.\nFor the hippocampus, improvement is confirmed (`True`) if both conditions are met:\n1. A net decrease in cost: $J_h^{(T)} < J_h^{(0)} - \\delta_h$, with $\\delta_h=0.05$.\n2. A consistent decrease: at least a fraction $q_h=0.7$ of the epoch-to-epoch changes are non-positive, i.e., $|\\{t \\in \\{0,..,T-1\\} | J_h^{(t+1)} - J_h^{(t)} \\le 0\\}| / T \\ge 0.7$.\n\nFor the cortex, improvement is confirmed (`True`) if both conditions are met:\n1. A net increase in the index: $I_c^{(T)} > I_c^{(0)} + \\delta_c$, with $\\delta_c=0.05$.\n2. A consistent increase: at least a fraction $q_c=0.7$ of the epoch-to-epoch changes are non-negative, i.e., $|\\{t \\in \\{0,..,T-1\\} | I_c^{(t+1)} - I_c^{(t)} \\ge 0\\}| / T \\ge 0.7$.\n\nThe implementation will simulate this entire process for each test case, starting from data generation, running the learning dynamics for $T$ epochs while recording the metrics, and finally applying the decision criteria to produce the boolean outcomes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the CLS simulation for all specified test cases.\n    \"\"\"\n    test_cases = [\n        # (d, K, n, sigma, T, eta_h, eta_c, seed)\n        (32, 3, 10, 0.3, 20, 0.2, 0.05, 123),\n        (32, 3, 10, 0.3, 20, 0.0, 0.0, 123),\n        (32, 3, 10, 2.0, 20, 0.2, 0.01, 234),\n        (32, 3, 10, 0.5, 100, 0.05, 0.02, 345),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = run_simulation(*params)\n        all_results.append(result)\n\n    # Format the final output string\n    result_str = '[' + ','.join([str(r) for r in all_results]) + ']'\n    print(result_str)\n\ndef run_simulation(d, K, n, sigma, T, eta_h, eta_c, seed):\n    \"\"\"\n    Runs the simulation for a single test case.\n    \"\"\"\n    # Set seed for reproducibility\n    np.random.seed(seed)\n\n    # --- Data Generation ---\n    N = K * n\n    labels = np.repeat(np.arange(K), n)\n    \n    # Generate and normalize prototypes\n    prototypes = np.random.randn(K, d)\n    prototypes /= np.linalg.norm(prototypes, axis=1, keepdims=True)\n    \n    # Generate input vectors\n    X = np.zeros((N, d))\n    for k in range(K):\n        # Noise for all items in category k\n        noise = np.random.randn(n, d) * sigma\n        # Items for category k\n        category_items = prototypes[k, :] + noise\n        X[labels == k] = category_items\n    \n    # --- Initialization ---\n    H = X.copy()\n    C = X.copy()\n    \n    # --- Metrics Calculation ---\n    def compute_J_h(representations):\n        N_items = representations.shape[0]\n        norms = np.linalg.norm(representations, axis=1, keepdims=True)\n        # Avoid division by zero for zero vectors, though unlikely\n        norms[norms == 0] = 1.0\n        normalized_reps = representations / norms\n        \n        sim_matrix = normalized_reps @ normalized_reps.T\n        # Get upper triangle indices, excluding the diagonal\n        triu_indices = np.triu_indices(N_items, k=1)\n        \n        # Mean of absolute similarities for unique pairs\n        mean_abs_sim = np.mean(np.abs(sim_matrix[triu_indices]))\n        return mean_abs_sim\n\n    def compute_I_c(representations, labels_vec):\n        N_items = representations.shape[0]\n        norms = np.linalg.norm(representations, axis=1, keepdims=True)\n        norms[norms == 0] = 1.0\n        normalized_reps = representations / norms\n        \n        sim_matrix = normalized_reps @ normalized_reps.T\n\n        # Create masks for within- and between-category pairs\n        labels_col = labels_vec[:, np.newaxis]\n        labels_row = labels_vec[np.newaxis, :]\n        \n        within_mask = (labels_col == labels_row)\n        # Exclude diagonal\n        np.fill_diagonal(within_mask, False)\n        \n        between_mask = (labels_col != labels_row)\n\n        # Handle cases with no pairs (e.g., if K=1)\n        mean_within = np.mean(sim_matrix[within_mask]) if np.any(within_mask) else 0.0\n        mean_between = np.mean(sim_matrix[between_mask]) if np.any(between_mask) else 0.0\n        \n        return mean_within - mean_between\n\n    # --- Learning Simulation ---\n    J_h_history = [compute_J_h(H)]\n    I_c_history = [compute_I_c(C, labels)]\n    \n    # Pre-compute cortical prototypes\n    cortical_prototypes = np.zeros((K, d))\n    for k in range(K):\n        cortical_prototypes[k] = np.mean(X[labels == k], axis=0)\n    expanded_cortical_prototypes = cortical_prototypes[labels]\n\n    for t in range(T):\n        # Hippocampal update\n        if eta_h > 0:\n            h_norms_sq = np.sum(H**2, axis=1)\n            # Avoid division by zero\n            h_norms_sq[h_norms_sq == 0] = 1.0\n            # Matrix of projection coefficients\n            proj_coeffs = (H @ H.T) / h_norms_sq[np.newaxis, :]\n            np.fill_diagonal(proj_coeffs, 0)\n            \n            # Synchronous update\n            H = H - eta_h * (proj_coeffs @ H)\n\n        # Cortical update\n        if eta_c > 0:\n            C = (1 - eta_c) * C + eta_c * expanded_cortical_prototypes\n\n        # Record metrics\n        J_h_history.append(compute_J_h(H))\n        I_c_history.append(compute_I_c(C, labels))\n\n    # --- Decision Criteria ---\n    delta_h, delta_c = 0.05, 0.05\n    q_h, q_c = 0.7, 0.7\n\n    # Hippocampus\n    J_h_history = np.array(J_h_history)\n    h_improved_net = J_h_history[-1] < J_h_history[0] - delta_h\n    if T > 0:\n        J_h_diffs = J_h_history[1:] - J_h_history[:-1]\n        h_improved_consistent = np.mean(J_h_diffs <= 0) >= q_h\n    else:\n        h_improved_consistent = True # No change is consistent\n    hippocampus_improved = h_improved_net and h_improved_consistent\n\n    # Cortex\n    I_c_history = np.array(I_c_history)\n    c_improved_net = I_c_history[-1] > I_c_history[0] + delta_c\n    if T > 0:\n        I_c_diffs = I_c_history[1:] - I_c_history[:-1]\n        c_improved_consistent = np.mean(I_c_diffs >= 0) >= q_c\n    else:\n        c_improved_consistent = True\n    cortex_improved = c_improved_net and c_improved_consistent\n\n    return [hippocampus_improved, cortex_improved]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "At a more abstract level, the CLS framework offers a solution to the \"stability-plasticity dilemma\": how to learn new things quickly without catastrophically forgetting old ones. This final practice formalizes this computational trade-off using the powerful analogy of a state-space model, akin to a Kalman filter. By deriving how two systems with different learning rates optimally weigh new evidence, you will gain a deeper, information-theoretic appreciation for why two complementary learning systems can be more effective than a single one .",
            "id": "3970398",
            "problem": "Consider the Complementary Learning Systems (CLS) theory in brain modeling, where a hippocampal system rapidly acquires information and a cortical system slowly consolidates it. Model these two systems as latent states in a linear-Gaussian state-space model. Let the hippocampal state be $x_{h,t}$ and the cortical state be $x_{c,t}$. Assume independent random-walk dynamics:\n$$x_{h,t} = x_{h,t-1} + w_{h,t}, \\quad w_{h,t} \\sim \\mathcal{N}(0, q_h),$$\n$$x_{c,t} = x_{c,t-1} + w_{c,t}, \\quad w_{c,t} \\sim \\mathcal{N}(0, q_c),$$\nwith $w_{h,t}$ and $w_{c,t}$ mutually independent and independent of all other variables. At time $t$, a scalar observation $y_t$ reports the sum of the two states corrupted by observation noise:\n$$y_t = x_{h,t} + x_{c,t} + v_t, \\quad v_t \\sim \\mathcal{N}(0, r),$$\nwith $v_t$ independent of $w_{h,t}$ and $w_{c,t}$. Assume that at time $t-1$ the joint posterior over $(x_{h,t-1}, x_{c,t-1})$ is degenerate at a point (i.e., zero covariance), so that the one-step prediction to time $t$ generates a prior covariance equal to the process noise covariance. That is, before observing $y_t$, the prior covariance of $(x_{h,t}, x_{c,t})$ is\n$$P_{t|t-1} = \\begin{pmatrix} q_h & 0 \\\\ 0 & q_c \\end{pmatrix}.$$\n\nStarting from the definitions of linear-Gaussian models and Bayesian inference (i.e., prior, likelihood, and posterior for multivariate normal variables), derive how the posterior mean at time $t$ integrates the observation $y_t$ into the hippocampal and cortical estimates via an evidence-weighted update (Kalman gain), and then compute the posterior variance of each state at time $t$ as a function of the observation noise variance $r$ and the process noise variances $q_h$ and $q_c$. Express your final answer as closed-form analytic expressions for the hippocampal posterior variance and the cortical posterior variance, respectively. Do not use numerical approximations; no rounding is required. The final answer must be a single row matrix containing the two expressions.",
            "solution": "The problem asks for the derivation of the posterior variances of hippocampal and cortical states in a simplified linear-Gaussian state-space model representing the Complementary Learning Systems (CLS) theory. The derivation must proceed from the fundamental principles of Bayesian inference for such models.\n\nFirst, we formalize the problem in the standard notation of state-space models. The state vector at time $t$ is composed of the hippocampal and cortical states:\n$$\nx_t = \\begin{pmatrix} x_{h,t} \\\\ x_{c,t} \\end{pmatrix}\n$$\nThe state dynamics are given by $x_t = x_{t-1} + w_t$, which can be written as $x_t = A x_{t-1} + w_t$ with the identity matrix $A = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$. The process noise vector $w_t = \\begin{pmatrix} w_{h,t} \\\\ w_{c,t} \\end{pmatrix}$ has a covariance matrix $Q$:\n$$\nQ = \\text{Cov}(w_t) = \\begin{pmatrix} q_h & 0 \\\\ 0 & q_c \\end{pmatrix}\n$$\nThe observation $y_t$ is a scalar given by $y_t = x_{h,t} + x_{c,t} + v_t$. This can be written as $y_t = H x_t + v_t$, where the observation matrix $H$ is:\n$$\nH = \\begin{pmatrix} 1 & 1 \\end{pmatrix}\n$$\nThe observation noise $v_t$ has variance $R = r$.\n\nThe problem states that the prior distribution over the state $p(x_t)$ at time $t$ (before observing $y_t$) is a Gaussian distribution with a mean $\\hat{x}_{t|t-1}$ and a covariance matrix $P_{t|t-1}$. The covariance is explicitly given as:\n$$\nP_{t|t-1} = \\begin{pmatrix} q_h & 0 \\\\ 0 & q_c \\end{pmatrix}\n$$\nThe likelihood of the observation $y_t$ given the state $x_t$ is also Gaussian, $p(y_t|x_t) = \\mathcal{N}(y_t; Hx_t, R)$.\n\nOur goal is to compute the posterior distribution $p(x_t|y_t)$. For a linear-Gaussian system, the posterior is also Gaussian, $p(x_t|y_t) = \\mathcal{N}(x_t; \\hat{x}_{t|t}, P_{t|t})$. The posterior mean $\\hat{x}_{t|t}$ and covariance $P_{t|t}$ are given by the Kalman filter update equations, which are a direct consequence of Bayes' rule for conditioning on a jointly Gaussian variable.\n\nThe update equations are:\n$$\n\\hat{x}_{t|t} = \\hat{x}_{t|t-1} + K_t (y_t - H\\hat{x}_{t|t-1})\n$$\n$$\nP_{t|t} = (I - K_t H) P_{t|t-1}\n$$\nwhere $K_t$ is the Kalman gain, defined as:\n$$\nK_t = P_{t|t-1} H^T S_t^{-1}\n$$\nand $S_t$ is the innovation covariance (the variance of the predicted observation):\n$$\nS_t = H P_{t|t-1} H^T + R\n$$\nWe proceed by calculating these quantities step-by-step.\n\n1.  **Calculate the innovation covariance $S_t$**:\n    $S_t$ is a scalar in this case.\n    $$\n    S_t = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} q_h & 0 \\\\ 0 & q_c \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + r\n    $$\n    $$\n    S_t = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} q_h \\\\ q_c \\end{pmatrix} + r = (q_h + q_c) + r\n    $$\n    So, $S_t = q_h + q_c + r$. Its inverse is $S_t^{-1} = \\frac{1}{q_h + q_c + r}$.\n\n2.  **Calculate the Kalman gain $K_t$**:\n    $K_t$ is a $2 \\times 1$ vector.\n    $$\n    K_t = P_{t|t-1} H^T S_t^{-1} = \\begin{pmatrix} q_h & 0 \\\\ 0 & q_c \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\frac{1}{q_h + q_c + r}\n    $$\n    $$\n    K_t = \\begin{pmatrix} q_h \\\\ q_c \\end{pmatrix} \\frac{1}{q_h + q_c + r} = \\begin{pmatrix} \\frac{q_h}{q_h + q_c + r} \\\\ \\frac{q_c}{q_h + q_c + r} \\end{pmatrix}\n    $$\n    The Kalman gain vector $K_t$ determines how the prediction error, $(y_t - H\\hat{x}_{t|t-1})$, is distributed between the updates for the hippocampal and cortical states. The update for each state's mean is proportional to its prior uncertainty ($q_h$ or $q_c$). This demonstrates the evidence-weighted update mechanism.\n\n3.  **Calculate the posterior covariance $P_{t|t}$**:\n    The posterior covariance matrix contains the posterior variances on its diagonal. We use the formula $P_{t|t} = (I - K_t H) P_{t|t-1}$.\n    First, we compute the term $K_t H$:\n    $$\n    K_t H = \\begin{pmatrix} \\frac{q_h}{q_h + q_c + r} \\\\ \\frac{q_c}{q_h + q_c + r} \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{q_h + q_c + r} \\begin{pmatrix} q_h & q_h \\\\ q_c & q_c \\end{pmatrix}\n    $$\n    Next, we compute the term $(I - K_t H)$:\n    $$\n    I - K_t H = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\frac{1}{q_h + q_c + r} \\begin{pmatrix} q_h & q_h \\\\ q_c & q_c \\end{pmatrix}\n    $$\n    $$\n    I - K_t H = \\frac{1}{q_h + q_c + r} \\left[ \\begin{pmatrix} q_h + q_c + r & 0 \\\\ 0 & q_h + q_c + r \\end{pmatrix} - \\begin{pmatrix} q_h & q_h \\\\ q_c & q_c \\end{pmatrix} \\right]\n    $$\n    $$\n    I - K_t H = \\frac{1}{q_h + q_c + r} \\begin{pmatrix} q_c + r & -q_h \\\\ -q_c & q_h + r \\end{pmatrix}\n    $$\n    Finally, we multiply this by the prior covariance $P_{t|t-1}$:\n    $$\n    P_{t|t} = (I - K_t H) P_{t|t-1} = \\frac{1}{q_h + q_c + r} \\begin{pmatrix} q_c + r & -q_h \\\\ -q_c & q_h + r \\end{pmatrix} \\begin{pmatrix} q_h & 0 \\\\ 0 & q_c \\end{pmatrix}\n    $$\n    $$\n    P_{t|t} = \\frac{1}{q_h + q_c + r} \\begin{pmatrix} (q_c + r)q_h & (-q_h)q_c \\\\ (-q_c)q_h & (q_h + r)q_c \\end{pmatrix}\n    $$\n    $$\n    P_{t|t} = \\frac{1}{q_h + q_c + r} \\begin{pmatrix} q_h(q_c + r) & -q_h q_c \\\\ -q_h q_c & q_c(q_h + r) \\end{pmatrix}\n    $$\n    The posterior covariance matrix is $P_{t|t} = \\begin{pmatrix} \\text{Var}(x_{h,t}|y_t) & \\text{Cov}(x_{h,t}, x_{c,t}|y_t) \\\\ \\text{Cov}(x_{c,t}, x_{h,t}|y_t) & \\text{Var}(x_{c,t}|y_t) \\end{pmatrix}$. The diagonal elements are the requested posterior variances.\n\nThe posterior variance of the hippocampal state $x_{h,t}$ is the top-left element:\n$$\n\\text{Var}(x_{h,t}|y_t) = \\frac{q_h(q_c + r)}{q_h + q_c + r}\n$$\nThe posterior variance of the cortical state $x_{c,t}$ is the bottom-right element:\n$$\n\\text{Var}(x_{c,t}|y_t) = \\frac{q_c(q_h + r)}{q_h + q_c + r}\n$$\nThese expressions represent the reduction in uncertainty about each state after incorporating the observation $y_t$. The uncertainty reduction depends on the initial uncertainties ($q_h$, $q_c$) and the observation noise variance ($r$).",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{q_h(q_c + r)}{q_h + q_c + r} & \\frac{q_c(q_h + r)}{q_h + q_c + r}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}