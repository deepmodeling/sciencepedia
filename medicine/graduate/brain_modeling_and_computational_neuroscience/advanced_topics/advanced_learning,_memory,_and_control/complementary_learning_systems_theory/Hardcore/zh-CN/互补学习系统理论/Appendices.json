{
    "hands_on_practices": [
        {
            "introduction": "互补学习系统理论的一个核心预测是，它能够解释暂时性遗忘症（即所谓的“Ribot定律”），即海马体损伤后近期记忆比远期记忆受损更严重。本练习将通过建立一个计算模型来模拟这一现象，量化快速学习系统（模拟海马体）的损伤如何导致近期记忆的缺失，而已经巩固到慢速学习系统（模拟新皮层）的远期记忆则得以保留。通过这个实践，你将亲手验证记忆巩固的关键作用 。",
            "id": "3970414",
            "problem": "您需要实现一个基于互补学习系统（CLS）理论的模拟与分析，以证明损伤快速学习系统会导致近期记忆的缺陷，同时保留远期记忆，并量化一个梯度，该梯度能够捕捉此效应如何依赖于巩固率。该模拟必须纯粹用数学术语来指定，并通过算法求解。基本假设和定义如下。互补学习系统（CLS）理论假设存在两个相互作用的学习系统：一个快速系统和一个慢速系统。考虑在时间 $t=0$ 时呈现一个真实值为 $y=1$ 的单一标量目标项。设快速系统痕迹为 $h(t)$，慢速系统痕迹为 $c(t)$。在 $t=0$ 时的编码事件会根据 $h(0^{+})=\\alpha_h$ 和 $c(0^{+})=\\alpha_c$ 立即更新快速痕迹和慢速痕迹，其中 $0  \\alpha_c \\ll \\alpha_h \\leq 1$ 反映了快速系统中迅速的初始学习和慢速系统中微弱的初始学习。编码后，快速痕迹呈指数衰减，模型为线性微分方程 $dh/dt=-\\lambda_h h$ 的解，其解为 $h(t)=\\alpha_h e^{-\\lambda_h t}$（对于 $t\\geq 0$），其中 $\\lambda_h>0$ 是快速系统的衰减率。在模拟的时间范围内，慢速系统通过来自快速系统的重放累积巩固，自身没有内在衰减，模型为 $dc/dt=\\beta h(t)$ 且 $\\lambda_c=0$，得出 $c(t)=\\alpha_c+\\beta\\alpha_h\\int_0^t e^{-\\lambda_h s}\\,ds=\\alpha_c+\\beta\\alpha_h\\frac{1-e^{-\\lambda_h t}}{\\lambda_h}$（对于 $t\\geq 0$），其中 $\\beta\\geq 0$ 是巩固率。假设在时间 $t$ 的完整系统预测是两个系统输出之和，即 $\\hat{y}_{\\text{intact}}(t)=h(t)+c(t)$，而损伤系统（移除了快速系统）的预测是 $\\hat{y}_{\\text{lesion}}(t)=c(t)$。定义损伤分数 $I(t;\\beta)=1-\\frac{\\hat{y}_{\\text{lesion}}(t)}{\\hat{y}_{\\text{intact}}(t)}=1-\\frac{c(t)}{h(t)+c(t)}$，它量化了在记忆保留间隔 $t$ 时因损伤而损失的性能。定义远期记忆保留梯度为 $G(\\beta)=I(t_{\\text{recent}};\\beta)-I(t_{\\text{remote}};\\beta)$，其中 $t_{\\text{recent}}>0$ 和 $t_{\\text{remote}}\\gg t_{\\text{recent}}$ 是固定的记忆保留间隔。您的任务是使用固定的、科学上合理的参数，为一小组 $\\beta$ 值的测试套件计算 $G(\\beta)$。使用 $\\alpha_h=1.0$，$\\alpha_c=0.05$，$\\lambda_h=1.0$， $t_{\\text{recent}}=0.2$ 和 $t_{\\text{remote}}=10.0$。这些值是无量纲的，不涉及物理单位。该测试套件包含四个巩固率 $\\beta\\in\\{0.0,0.1,0.5,1.0\\}$，涵盖了边界情况和典型情况：$\\beta=0.0$（无巩固），$\\beta=0.1$（弱巩固），$\\beta=0.5$（中等巩固）和 $\\beta=1.0$（强巩固）。对于每个 $\\beta$，计算 $G(\\beta)$ 并将结果四舍五入到六位小数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[result1,result2,result3,result4]”），结果的顺序与所提供测试套件中 $\\beta$ 值的顺序完全一致。",
            "solution": "在尝试任何解决方案之前，都将对问题进行验证。\n\n### 步骤 1：提取已知条件\n- **理论**：互补学习系统（CLS）理论，包含一个快速系统和一个慢速系统。\n- **目标项**：在时间 $t=0$ 时呈现的一个真实值为 $y=1$ 的单一标量目标项。\n- **系统痕迹**：快速系统痕迹 $h(t)$，慢速系统痕迹 $c(t)$。\n- **初始条件 ($t=0^+$)**：$h(0^{+})=\\alpha_h$ 和 $c(0^{+})=\\alpha_c$。\n- **参数约束**：$0  \\alpha_c \\ll \\alpha_h \\leq 1$。\n- **快速系统动力学 ($t \\geq 0$)**：微分方程 $dh/dt=-\\lambda_h h$，其解为 $h(t)=\\alpha_h e^{-\\lambda_h t}$。衰减率为 $\\lambda_h>0$。\n- **慢速系统动力学 ($t \\geq 0$)**：微分方程 $dc/dt=\\beta h(t)$，其内在衰减为 $\\lambda_c=0$。其解给出为 $c(t)=\\alpha_c+\\beta\\alpha_h\\int_0^t e^{-\\lambda_h s}\\,ds=\\alpha_c+\\beta\\alpha_h\\frac{1-e^{-\\lambda_h t}}{\\lambda_h}$。巩固率为 $\\beta\\geq 0$。\n- **预测模型**：\n    - 完整系统：$\\hat{y}_{\\text{intact}}(t)=h(t)+c(t)$。\n    - 损伤系统（快速系统被移除）：$\\hat{y}_{\\text{lesion}}(t)=c(t)$。\n- **损伤分数定义**：$I(t;\\beta)=1-\\frac{\\hat{y}_{\\text{lesion}}(t)}{\\hat{y}_{\\text{intact}}(t)}=1-\\frac{c(t)}{h(t)+c(t)}$。\n- **远期记忆保留梯度定义**：$G(\\beta)=I(t_{\\text{recent}};\\beta)-I(t_{\\text{remote}};\\beta)$。\n- **固定参数**：\n    - $\\alpha_h=1.0$\n    - $\\alpha_c=0.05$\n    - $\\lambda_h=1.0$\n    - $t_{\\text{recent}}=0.2$\n    - $t_{\\text{remote}}=10.0$\n- **任务**：为巩固率测试套件 $\\beta\\in\\{0.0,0.1,0.5,1.0\\}$ 计算 $G(\\beta)$。\n- **输出格式**：一个用方括号括起来的、逗号分隔的浮点值列表，四舍五入到六位小数。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **具有科学依据**：该问题是互补学习系统（CLS）理论的数学形式化，该理论是计算神经科学中解释记忆巩固的基石。用于指数衰减和积分巩固的模型方程是该领域使用的标准且合理的简化。它旨在模拟的现象——海马损伤后近期记忆比远期记忆受损更严重（Ribot 定律）——是一个有充分文献记载的实证发现，而 CLS 理论正是为了解释这一现象。该问题在科学上是合理的。\n- **适定的**：所有函数、变量和参数都已明确定义。任务是基于这些定义的直接计算。$I(t;\\beta)$ 表达式中的分母 $h(t)+c(t)$，在给定的参数范围内始终为正（对于有限的 $t$，$h(t)=\\alpha_h e^{-\\lambda_h t} > 0$，且 $c(t)=\\alpha_c+\\dots \\ge \\alpha_c > 0$），因此该函数始终是良定义的。对于每个 $\\beta$，都存在唯一、稳定的解。\n- **客观的**：问题以精确、量化且无偏见的数学语言陈述。\n- **完整且一致的**：问题提供了执行计算所需的所有数据和关系。定义或约束中没有矛盾。\n- **无其他缺陷**：问题不是隐喻性的，与其陈述的主题直接相关，不是病态的、琐碎的或不可验证的。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将制定解决方案。\n\n目标是为巩固率 $\\beta$ 的几个值计算远期记忆保留梯度 $G(\\beta)$。该梯度量化了近期和远期记忆提取之间的损伤差异，这是 CLS 理论的一个关键预测。\n\n分析从所提供的定义开始。损伤分数 $I(t;\\beta)$ 定义为：\n$$\nI(t;\\beta) = 1 - \\frac{c(t)}{h(t)+c(t)}\n$$\n这可以通过代数简化，将损伤表示为快速系统贡献与总系统输出的比率：\n$$\nI(t;\\beta) = \\frac{(h(t)+c(t)) - c(t)}{h(t)+c(t)} = \\frac{h(t)}{h(t)+c(t)}\n$$\n快速痕迹 $h(t)$ 和慢速痕迹 $c(t)$ 的表达式由以下公式给出：\n$$\nh(t) = \\alpha_h e^{-\\lambda_h t}\n$$\n$$\nc(t) = \\alpha_c + \\beta\\alpha_h\\frac{1-e^{-\\lambda_h t}}{\\lambda_h}\n$$\n我们代入给定的常数参数值：$\\alpha_h=1.0$，$\\alpha_c=0.05$ 和 $\\lambda_h=1.0$。\n$$\nh(t) = 1.0 \\cdot e^{-1.0 \\cdot t} = e^{-t}\n$$\n$$\nc(t) = 0.05 + \\beta \\cdot 1.0 \\cdot \\frac{1-e^{-1.0 \\cdot t}}{1.0} = 0.05 + \\beta(1-e^{-t})\n$$\n将这些简化形式代入 $I(t;\\beta)$ 的表达式中，得到：\n$$\nI(t;\\beta) = \\frac{e^{-t}}{e^{-t} + (0.05 + \\beta(1-e^{-t}))} = \\frac{e^{-t}}{e^{-t} + 0.05 + \\beta - \\beta e^{-t}}\n$$\n$$\nI(t;\\beta) = \\frac{e^{-t}}{(1-\\beta)e^{-t} + 0.05 + \\beta}\n$$\n远期记忆保留梯度 $G(\\beta)$ 定义为近期时间点和远期时间点损伤的差值：\n$$\nG(\\beta) = I(t_{\\text{recent}};\\beta) - I(t_{\\text{remote}};\\beta)\n$$\n使用指定的时间点 $t_{\\text{recent}}=0.2$ 和 $t_{\\text{remote}}=10.0$：\n$$\nG(\\beta) = I(0.2;\\beta) - I(10.0;\\beta)\n$$\n这给出了梯度的最终可计算表达式：\n$$\nG(\\beta) = \\left( \\frac{e^{-0.2}}{(1-\\beta)e^{-0.2} + 0.05 + \\beta} \\right) - \\left( \\frac{e^{-10.0}}{(1-\\beta)e^{-10.0} + 0.05 + \\beta} \\right)\n$$\n将对测试套件 $\\{0.0, 0.1, 0.5, 1.0\\}$ 中的每个 $\\beta$ 值评估此函数。算法如下：\n1. 对于每个给定的 $\\beta$：\n2. 使用上述公式计算 $I(0.2;\\beta)$ 的值。\n3. 使用上述公式计算 $I(10.0;\\beta)$ 的值。\n4. 计算它们的差值，$G(\\beta) = I(0.2;\\beta) - I(10.0;\\beta)$。\n5. 将结果四舍五入到六位小数。\n\n此过程展示了 CLS 理论的一个核心预测：对于任何非负巩固率 $\\beta$，近期时间（$t_{\\text{recent}}$）的损伤远大于远期时间（$t_{\\text{remote}}$）的损伤，因为在远期时间，快速痕迹 $h(t)$ 已衰减至几乎为零。梯度 $G(\\beta)$ 捕捉了这种效应。随着 $\\beta$ 的增加，巩固变得更有效，因此慢速系统能更快地成为可靠的记忆来源，这预计会降低所有时间点上损伤 $I(t;\\beta)$ 的总体幅度，因此可能会改变梯度 $G(\\beta)$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the remote-sparing gradient G(beta) for a set of consolidation rates\n    based on a simplified model of Complementary Learning Systems (CLS) theory.\n    \"\"\"\n\n    # Define the fixed parameters from the problem statement.\n    # These values are dimensionless.\n    alpha_h = 1.0  # Initial strength of the fast system trace\n    alpha_c = 0.05 # Initial strength of the slow system trace\n    lambda_h = 1.0 # Decay rate of the fast system trace\n    t_recent = 0.2 # Retention interval for recent memory\n    t_remote = 10.0 # Retention interval for remote memory\n\n    # Define the test suite of consolidation rates (beta).\n    test_cases = [0.0, 0.1, 0.5, 1.0]\n\n    def impairment_fraction(t: float, beta: float) - float:\n        \"\"\"\n        Calculates the impairment fraction I(t; beta) for a given retention\n        interval t and consolidation rate beta.\n\n        The impairment fraction is defined as I = h(t) / (h(t) + c(t)).\n        \"\"\"\n        # Fast system trace h(t)\n        # h(t) = alpha_h * exp(-lambda_h * t)\n        h_t = alpha_h * np.exp(-lambda_h * t)\n\n        # Slow system trace c(t)\n        # c(t) = alpha_c + beta * alpha_h * (1 - exp(-lambda_h * t)) / lambda_h\n        c_t = alpha_c + beta * alpha_h * (1 - np.exp(-lambda_h * t)) / lambda_h\n        \n        # Total intact system output\n        y_intact = h_t + c_t\n        \n        # The problem validation confirmed that y_intact  0 for all t=0.\n        return h_t / y_intact\n\n    results = []\n    for beta in test_cases:\n        # Calculate the impairment for the recent memory time point.\n        I_recent = impairment_fraction(t_recent, beta)\n\n        # Calculate the impairment for the remote memory time point.\n        I_remote = impairment_fraction(t_remote, beta)\n        \n        # The remote-sparing gradient is the difference between the two impairments.\n        G_beta = I_recent - I_remote\n        \n        # Round the result to six decimal places as required.\n        results.append(round(G_beta, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "超越行为层面的预测，CLS理论对海马体和新皮层在神经表征层面上的计算功能做出了不同假设：前者进行模式分离，后者进行知识整合。本练习要求你设计一个分析流程，来模拟和检验这两个系统在学习过程中表征几何学的动态变化。你将观察到海马体的表征如何变得更加正交，而皮层的表征如何根据语义类别进行聚类，从而深入理解CLS理论的神经计算机制 。",
            "id": "3970454",
            "problem": "要求您从基本原理出发，实现一个基于互补学习系统（CLS）理论的分析流程，用以检验在学习过程中，海马表征是否变得更加正交，而皮层表征是否按照语义类别变得更加聚类。通过一个输入特征向量表示每个项目，并模拟两个学习系统：一个执行快速模式分离的海马系统，以及一个执行渐进式、基于类别的整合的皮层系统。该流程必须使用数学上定义的度量标准，量化并检验在多个学习周期中表征几何发生的假设性变化。\n\n基本定义：\n- 设有 $N$ 个项目，每个项目关联一个语义类别标签 $\\kappa(i) \\in \\{1,\\dots,K\\}$ 和一个输入特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^d$。\n- 两个非零向量 $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^d$ 之间的余弦相似度定义为 $\\operatorname{cos}(\\mathbf{a}, \\mathbf{b}) = \\dfrac{\\mathbf{a}^\\top \\mathbf{b}}{\\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2}$，其中 $\\|\\cdot\\|_2$ 表示欧几里得范数。\n- 海马正交性被操作化为与项目间平均绝对成对余弦相似度成反比。我们将周期 $t$ 的海马正交性成本定义为 $$J_h^{(t)} = \\frac{2}{N(N-1)} \\sum_{1 \\le i  j \\le N} \\left|\\operatorname{cos}\\left(\\mathbf{h}_i^{(t)}, \\mathbf{h}_j^{(t)}\\right)\\right|,$$ 其中 $\\mathbf{h}_i^{(t)}$ 是项目 $i$ 在周期 $t$ 的海马表征。$J_h^{(t)}$ 值越低，表示正交性越高。\n- 皮层按类别的聚类被操作化为类别内平均相似度与类别间平均相似度之差。我们将周期 $t$ 的皮层聚类指数定义为 $$I_c^{(t)} = \\overline{\\operatorname{cos}}_{\\text{within}}^{(t)} - \\overline{\\operatorname{cos}}_{\\text{between}}^{(t)},$$ 其中 $\\overline{\\operatorname{cos}}_{\\text{within}}^{(t)}$ 是对所有满足 $\\kappa(i)=\\kappa(j)$ 的配对的 $\\operatorname{cos}\\left(\\mathbf{c}_i^{(t)}, \\mathbf{c}_j^{(t)}\\right)$ 进行平均，而 $\\overline{\\operatorname{cos}}_{\\text{between}}^{(t)}$ 是对所有满足 $\\kappa(i)\\ne\\kappa(j)$ 的配对进行平均；$\\mathbf{c}_i^{(t)}$ 是周期 $t$ 的皮层表征。$I_c^{(t)}$ 值越高，表示类别聚类越强。\n\n需实现的学习动态：\n- 海马（模式分离）：从 $\\mathbf{h}_i^{(0)} = \\mathbf{x}_i$ 开始，通过迭代地减小成对余弦相似度来更新 $\\mathbf{h}_i^{(t)}$。该算法步骤应通过最小化关于 $\\mathbf{h}_i^{(t)}$ 的余弦相似度平方和推导得出，从而产生一个更新规则，该规则减去投影到其他项目方向上的分量，这些分量由它们当前的余弦相似度加权。使用一个学习率 $\\eta_h  0$，并实现一个使用当前归一化表征的稳定、局部更新。\n- 皮层（类别整合）：从 $\\mathbf{c}_i^{(0)} = \\mathbf{x}_i$ 开始，使用学习率 $\\eta_c  0$ 将每个项目的表征向其类别均值原型移动，从而更新 $\\mathbf{c}_i^{(t)}$。类别均值原型应根据每个类别的输入特征 $\\{\\mathbf{x}_i\\}$ 计算得出。\n\n判定标准：\n- 海马改进布尔值定义为真（true），需满足两个条件：(i) 净减少量满足 $J_h^{(T)}  J_h^{(0)} - \\delta_h$，以及 (ii) 至少有比例为 $q_h$ 的逐周期差异为非正值，即 $J_h^{(t+1)} - J_h^{(t)} \\le 0$。\n- 皮层改进布尔值定义为真（true），需满足两个条件：(i) 净增加量满足 $I_c^{(T)}  I_c^{(0)} + \\delta_c$，以及 (ii) 至少有比例为 $q_c$ 的逐周期差异为非负值，即 $I_c^{(t+1)} - I_c^{(t)} \\ge 0$。\n使用阈值 $\\delta_h = 0.05$、$\\delta_c = 0.05$ 和比例 $q_h = 0.7$、 $q_c = 0.7$。\n\n数据生成：\n- 对于每个类别 $k \\in \\{1,\\dots,K\\}$，从标准正态分布中采样生成一个原型 $\\boldsymbol{\\mu}_k \\in \\mathbb{R}^d$，并将其归一化为单位范数。对于类别 $k$ 中的每个项目 $i$，生成 $\\mathbf{x}_i = \\boldsymbol{\\mu}_k + \\boldsymbol{\\epsilon}_i$，其中 $\\boldsymbol{\\epsilon}_i \\sim \\mathcal{N}\\left(\\mathbf{0}, \\sigma^2 \\mathbf{I}_d\\right)$ 且 $\\mathbf{I}_d$ 是 $d \\times d$ 的单位矩阵。通过高斯噪声和原型采样的性质，确保 $\\mathbf{x}_i$ 不是零向量。在每个测试用例中使用指定的随机种子，以使模拟具有确定性。\n\n测试套件：\n为以下测试用例实现该流程。每个测试用例指定 $(d, K, n, \\sigma, T, \\eta_h, \\eta_c, \\text{seed})$，其中 $n$ 是每个类别的项目数，$T$ 是周期数。\n1. 正常路径用例：$(d = 32, K = 3, n = 10, \\sigma = 0.3, T = 20, \\eta_h = 0.2, \\eta_c = 0.05, \\text{seed} = 123)$。\n2. 无学习边界用例：$(d = 32, K = 3, n = 10, \\sigma = 0.3, T = 20, \\eta_h = 0.0, \\eta_c = 0.0, \\text{seed} = 123)$。\n3. 高噪声边缘用例：$(d = 32, K = 3, n = 10, \\sigma = 2.0, T = 20, \\eta_h = 0.2, \\eta_c = 0.01, \\text{seed} = 234)$。\n4. 慢学习长时运行用例：$(d = 32, K = 3, n = 10, \\sigma = 0.5, T = 100, \\eta_h = 0.05, \\eta_c = 0.02, \\text{seed} = 345)$。\n\n要求输出：\n- 对于每个测试用例，生成一个包含两个元素的列表 $[\\text{hippocampus\\_improved}, \\text{cortex\\_improved}]$，其中每个元素都是一个布尔值。\n- 您的程序应生成单行输出，其中包含一个以逗号分隔的列表形式的结果，并用方括号括起来，格式如下：$$[[b_{h,1}, b_{c,1}],[b_{h,2}, b_{c,2}],[b_{h,3}, b_{c,3}],[b_{h,4}, b_{c,4}]],$$ 其中 $b_{h,i}$ 和 $b_{c,i}$ 是测试用例 $i$ 的布尔值。\n\n不适用角度单位。不涉及物理单位。",
            "solution": "该问题要求实现并分析一个受互补学习系统（CLS）理论启发的计算模型。该模型模拟了两个学习系统，一个类海马系统和一个类皮层系统，用以检验它们的表征几何在学习周期中发生分化的假设。海马系统预期将正交化表征（模式分离），而皮层系统预期将根据语义类别对表征进行聚类（整合）。分析将在模拟数据上进行，其结果将根据特定的量化标准进行评估。\n\n首先，我们建立生成输入数据的程序。给定 $K$ 个语义类别，每个类别有 $n$ 个项目，总共有 $N=nK$ 个项目。每个项目 $i$ 由一个 $d$ 维特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 表示，并被分配一个类别标签 $\\kappa(i) \\in \\{1,\\dots,K\\}$。数据生成过程如下：\n1. 对于每个类别 $k$，从多元标准正态分布 $\\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$ 中采样一个原型向量 $\\boldsymbol{\\mu}_k \\in \\mathbb{R}^d$。然后将该原型归一化，使其欧几里得范数为1，即 $\\|\\boldsymbol{\\mu}_k\\|_2 = 1$。\n2. 对于属于类别 $k$ 的每个项目 $i$，其特征向量 $\\mathbf{x}_i$ 是通过向类别原型添加高斯噪声生成的：$\\mathbf{x}_i = \\boldsymbol{\\mu}_k + \\boldsymbol{\\epsilon}_i$，其中噪声项 $\\boldsymbol{\\epsilon}_i$ 从具有给定标准差 $\\sigma$ 的各向同性高斯分布 $\\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_d)$ 中抽取。此过程创建了一个数据集，其中来自同一类别的项目在特征空间中围绕其各自的原型聚类。使用指定的随机种子可确保生成数据的确定性可复现性。\n\n接下来，我们定义这两个系统在 $T$ 个周期内的学习动态。两个系统都以与输入特征相同的表征开始：对于所有项目 $i$，$\\mathbf{h}_i^{(0)} = \\mathbf{x}_i$ 且 $\\mathbf{c}_i^{(0)} = \\mathbf{x}_i$。\n\n海马系统旨在通过使表征更加正交来执行模式分离。更新规则被描述为一个通过减去投影来迭代降低成对余弦相似度的过程。这个概念的一个严谨且稳定的实现是一个同步的、局部的类格拉姆-施密特（Gram-Schmidt-like）正交化过程。在每个周期 $t$，我们对每个项目 $i$ 的表征 $\\mathbf{h}_i^{(t)}$ 进行如下更新：\n$$ \\mathbf{h}_i^{(t+1)} = \\mathbf{h}_i^{(t)} - \\eta_h \\sum_{j \\neq i} \\frac{\\left(\\mathbf{h}_i^{(t)}\\right)^\\top \\mathbf{h}_j^{(t)}}{\\left\\|\\mathbf{h}_j^{(t)}\\right\\|_2^2} \\mathbf{h}_j^{(t)} $$\n这里，$\\eta_h  0$ 是海马学习率。项 $\\frac{(\\mathbf{h}_i^{(t)})^\\top \\mathbf{h}_j^{(t)}}{\\|\\mathbf{h}_j^{(t)}\\|_2^2} \\mathbf{h}_j^{(t)}$ 是向量 $\\mathbf{h}_i^{(t)}$ 在向量 $\\mathbf{h}_j^{(t)}$ 上的投影。通过减去这些投影的总和，更新规则将 $\\mathbf{h}_i^{(t+1)}$ 移动到一个与由其他表征 $\\{\\mathbf{h}_j^{(t)}\\}_{j \\neq i}$ 张成的子空间更具正交性的方向。更新是同步的，意味着所有新的表征 $\\mathbf{h}^{(t+1)}$ 都是基于周期 $t$ 的表征集合计算的。\n\n皮层系统旨在执行基于类别的整合。它通过将表征移近其各自类别的原型表征来进行更新。首先，对于每个类别 $k$，从初始输入特征计算一个固定的类别均值原型 $\\mathbf{m}_k$：\n$$ \\mathbf{m}_k = \\frac{1}{n} \\sum_{i | \\kappa(i)=k} \\mathbf{x}_i $$\n类别 $k$ 中项目 $i$ 的皮层表征 $\\mathbf{c}_i^{(t)}$ 的更新规则是朝向该原型的线性插值：\n$$ \\mathbf{c}_i^{(t+1)} = (1 - \\eta_c)\\mathbf{c}_i^{(t)} + \\eta_c \\mathbf{m}_k $$\n其中 $\\eta_c  0$ 是皮层学习率。此规则逐渐将同一类别内所有项目的表征拉向一个共同点，从而增加它们的相似性并形成一个聚类。\n\n为了量化表征几何的变化，我们在每个周期 $t \\in \\{0, \\dots, T\\}$ 计算两个度量指标。\n\n海马正交性成本 $J_h^{(t)}$ 衡量海马表征的平均成对相似度。值越低表示正交性越高。其定义如下：\n$$ J_h^{(t)} = \\frac{2}{N(N-1)} \\sum_{1 \\le i  j \\le N} \\left|\\operatorname{cos}\\left(\\mathbf{h}_i^{(t)}, \\mathbf{h}_j^{(t)}\\right)\\right| $$\n其中 $\\operatorname{cos}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a}^\\top \\mathbf{b}}{\\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2}$。该值通过计算所有海马表征的成对余弦相似度矩阵上三角部分元素的绝对值的平均值来得出。\n\n皮层聚类指数 $I_c^{(t)}$ 衡量皮层表征按类别结构化的程度。值越高表示聚类越强。它的定义类似于轮廓系数（Silhouette score）：\n$$ I_c^{(t)} = \\overline{\\operatorname{cos}}_{\\text{within}}^{(t)} - \\overline{\\operatorname{cos}}_{\\text{between}}^{(t)} $$\n此处，$\\overline{\\operatorname{cos}}_{\\text{within}}^{(t)}$ 是来自同一类别的项目对之间的平均余弦相似度，而 $\\overline{\\operatorname{cos}}_{\\text{between}}^{(t)}$ 是来自不同类别的项目对之间的平均余弦相似度。\n\n最后，我们应用判定标准来确定是否达到了学习目标。为每个系统计算一个布尔值。\n对于海马系统，如果同时满足以下两个条件，则确认为有改进 (`True`)：\n1. 成本净减少：$J_h^{(T)}  J_h^{(0)} - \\delta_h$，其中 $\\delta_h=0.05$。\n2. 持续减少：至少有比例为 $q_h=0.7$ 的逐周期变化为非正值，即 $|\\{t \\in \\{0,..,T-1\\} | J_h^{(t+1)} - J_h^{(t)} \\le 0\\}| / T \\ge 0.7$。\n\n对于皮层系统，如果同时满足以下两个条件，则确认为有改进 (`True`)：\n1. 指数净增加：$I_c^{(T)}  I_c^{(0)} + \\delta_c$，其中 $\\delta_c=0.05$。\n2. 持续增加：至少有比例为 $q_c=0.7$ 的逐周期变化为非负值，即 $|\\{t \\in \\{0,..,T-1\\} | I_c^{(t+1)} - I_c^{(t)} \\ge 0\\}| / T \\ge 0.7$。\n\n该实现将为每个测试用例模拟整个过程，从数据生成开始，运行 $T$ 个周期的学习动态并记录各项指标，最后应用判定标准生成布尔结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the CLS simulation for all specified test cases.\n    \"\"\"\n    test_cases = [\n        # (d, K, n, sigma, T, eta_h, eta_c, seed)\n        (32, 3, 10, 0.3, 20, 0.2, 0.05, 123),\n        (32, 3, 10, 0.3, 20, 0.0, 0.0, 123),\n        (32, 3, 10, 2.0, 20, 0.2, 0.01, 234),\n        (32, 3, 10, 0.5, 100, 0.05, 0.02, 345),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = run_simulation(*params)\n        all_results.append(result)\n\n    # Format the final output string\n    result_str = '[' + ','.join([str(r) for r in all_results]) + ']'\n    print(result_str)\n\ndef run_simulation(d, K, n, sigma, T, eta_h, eta_c, seed):\n    \"\"\"\n    Runs the simulation for a single test case.\n    \"\"\"\n    # Set seed for reproducibility\n    np.random.seed(seed)\n\n    # --- Data Generation ---\n    N = K * n\n    labels = np.repeat(np.arange(K), n)\n    \n    # Generate and normalize prototypes\n    prototypes = np.random.randn(K, d)\n    prototypes /= np.linalg.norm(prototypes, axis=1, keepdims=True)\n    \n    # Generate input vectors\n    X = np.zeros((N, d))\n    for k in range(K):\n        # Noise for all items in category k\n        noise = np.random.randn(n, d) * sigma\n        # Items for category k\n        category_items = prototypes[k, :] + noise\n        X[labels == k] = category_items\n    \n    # --- Initialization ---\n    H = X.copy()\n    C = X.copy()\n    \n    # --- Metrics Calculation ---\n    def compute_J_h(representations):\n        N_items = representations.shape[0]\n        norms = np.linalg.norm(representations, axis=1, keepdims=True)\n        # Avoid division by zero for zero vectors, though unlikely\n        norms[norms == 0] = 1.0\n        normalized_reps = representations / norms\n        \n        sim_matrix = normalized_reps @ normalized_reps.T\n        # Get upper triangle indices, excluding the diagonal\n        triu_indices = np.triu_indices(N_items, k=1)\n        \n        # Mean of absolute similarities for unique pairs\n        mean_abs_sim = np.mean(np.abs(sim_matrix[triu_indices]))\n        return mean_abs_sim\n\n    def compute_I_c(representations, labels_vec):\n        N_items = representations.shape[0]\n        norms = np.linalg.norm(representations, axis=1, keepdims=True)\n        norms[norms == 0] = 1.0\n        normalized_reps = representations / norms\n        \n        sim_matrix = normalized_reps @ normalized_reps.T\n\n        # Create masks for within- and between-category pairs\n        labels_col = labels_vec[:, np.newaxis]\n        labels_row = labels_vec[np.newaxis, :]\n        \n        within_mask = (labels_col == labels_row)\n        # Exclude diagonal\n        np.fill_diagonal(within_mask, False)\n        \n        between_mask = (labels_col != labels_row)\n\n        # Handle cases with no pairs (e.g., if K=1)\n        mean_within = np.mean(sim_matrix[within_mask]) if np.any(within_mask) else 0.0\n        mean_between = np.mean(sim_matrix[between_mask]) if np.any(between_mask) else 0.0\n        \n        return mean_within - mean_between\n\n    # --- Learning Simulation ---\n    J_h_history = [compute_J_h(H)]\n    I_c_history = [compute_I_c(C, labels)]\n    \n    # Pre-compute cortical prototypes\n    cortical_prototypes = np.zeros((K, d))\n    for k in range(K):\n        cortical_prototypes[k] = np.mean(X[labels == k], axis=0)\n    expanded_cortical_prototypes = cortical_prototypes[labels]\n\n    for t in range(T):\n        # Hippocampal update\n        if eta_h > 0:\n            h_norms_sq = np.sum(H**2, axis=1)\n            # Avoid division by zero\n            h_norms_sq[h_norms_sq == 0] = 1.0\n            # Matrix of projection coefficients\n            proj_coeffs = (H @ H.T) / h_norms_sq[np.newaxis, :]\n            np.fill_diagonal(proj_coeffs, 0)\n            \n            # Synchronous update\n            H = H - eta_h * (proj_coeffs @ H)\n\n        # Cortical update\n        if eta_c > 0:\n            C = (1 - eta_c) * C + eta_c * expanded_cortical_prototypes\n\n        # Record metrics\n        J_h_history.append(compute_J_h(H))\n        I_c_history.append(compute_I_c(C, labels))\n\n    # --- Decision Criteria ---\n    delta_h, delta_c = 0.05, 0.05\n    q_h, q_c = 0.7, 0.7\n\n    # Hippocampus\n    J_h_history = np.array(J_h_history)\n    h_improved_net = J_h_history[-1]  J_h_history[0] - delta_h\n    if T > 0:\n        J_h_diffs = J_h_history[1:] - J_h_history[:-1]\n        h_improved_consistent = np.mean(J_h_diffs = 0) >= q_h\n    else:\n        h_improved_consistent = True # No change is consistent\n    hippocampus_improved = h_improved_net and h_improved_consistent\n\n    # Cortex\n    I_c_history = np.array(I_c_history)\n    c_improved_net = I_c_history[-1] > I_c_history[0] + delta_c\n    if T > 0:\n        I_c_diffs = I_c_history[1:] - I_c_history[:-1]\n        c_improved_consistent = np.mean(I_c_diffs >= 0) >= q_c\n    else:\n        c_improved_consistent = True\n    cortex_improved = c_improved_net and c_improved_consistent\n\n    return [hippocampus_improved, cortex_improved]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "为了从更形式化的角度理解互补学习系统，我们可以将其视为一个最优信息处理模型。本练习将双系统类比为一个线性高斯状态空间模型，运用卡尔曼滤波的原理来推导快速（海马）和慢速（皮层）状态如何整合新的观测数据。这个优雅的数学框架揭示了CLS架构如何以贝叶斯最优的方式结合先验知识和新证据，从而高效地进行学习和推断 。",
            "id": "3970398",
            "problem": "考虑大脑建模中的互补学习系统（Complementary Learning Systems, CLS）理论，其中海马系统快速获取信息，而皮层系统则缓慢地巩固信息。将这两个系统建模为线性高斯状态空间模型中的潜变量。设海马状态为 $x_{h,t}$，皮层状态为 $x_{c,t}$。假设其动态为独立的随机游走：\n$$x_{h,t} = x_{h,t-1} + w_{h,t}, \\quad w_{h,t} \\sim \\mathcal{N}(0, q_h),$$\n$$x_{c,t} = x_{c,t-1} + w_{c,t}, \\quad w_{c,t} \\sim \\mathcal{N}(0, q_c),$$\n其中 $w_{h,t}$ 和 $w_{c,t}$ 相互独立，并且与其他所有变量独立。在时间 $t$，一个标量观测值 $y_t$ 是两个状态之和加上观测噪声：\n$$y_t = x_{h,t} + x_{c,t} + v_t, \\quad v_t \\sim \\mathcal{N}(0, r),$$\n其中 $v_t$ 独立于 $w_{h,t}$ 和 $w_{c,t}$。假设在时间 $t-1$ 时，$(x_{h,t-1}, x_{c,t-1})$ 的联合后验分布退化为一个点（即零协方差），因此到时间 $t$ 的单步预测生成的先验协方差等于过程噪声协方差。也就是说，在观测到 $y_t$ 之前，$(x_{h,t}, x_{c,t})$ 的先验协方差为\n$$P_{t|t-1} = \\begin{pmatrix} q_h  0 \\\\ 0  q_c \\end{pmatrix}。$$\n\n从线性高斯模型和贝叶斯推断（即多元正态变量的先验、似然和后验）的定义出发，推导时间 $t$ 的后验均值如何通过证据加权更新（卡尔曼增益）将观测值 $y_t$ 整合到海马和皮层的估计中，然后计算时间 $t$ 时每个状态的后验方差，使其表示为观测噪声方差 $r$ 和过程噪声方差 $q_h$、$q_c$ 的函数。请将您的最终答案表示为海马后验方差和皮层后验方差的封闭形式解析表达式。不要使用数值近似；无需四舍五入。最终答案必须是包含这两个表达式的单行矩阵。",
            "solution": "问题要求推导在一个简化的代表互补学习系统（CLS）理论的线性高斯状态空间模型中，海马状态和皮层状态的后验方差。推导必须从这类模型的贝叶斯推断基本原理出发。\n\n首先，我们使用状态空间模型的标准表示法来形式化该问题。时间 $t$ 的状态向量由海马状态和皮层状态组成：\n$$\nx_t = \\begin{pmatrix} x_{h,t} \\\\ x_{c,t} \\end{pmatrix}\n$$\n状态动态由 $x_t = x_{t-1} + w_t$ 给出，可以写作 $x_t = A x_{t-1} + w_t$，其中 $A$ 为单位矩阵 $A = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$。过程噪声向量 $w_t = \\begin{pmatrix} w_{h,t} \\\\ w_{c,t} \\end{pmatrix}$ 的协方差矩阵为 $Q$：\n$$\nQ = \\text{Cov}(w_t) = \\begin{pmatrix} q_h  0 \\\\ 0  q_c \\end{pmatrix}\n$$\n观测值 $y_t$ 是一个标量，由 $y_t = x_{h,t} + x_{c,t} + v_t$ 给出。这可以写作 $y_t = H x_t + v_t$，其中观测矩阵 $H$ 为：\n$$\nH = \\begin{pmatrix} 1  1 \\end{pmatrix}\n$$\n观测噪声 $v_t$ 的方差为 $R = r$。\n\n问题陈述，在时间 $t$（观测到 $y_t$ 之前），状态 $x_t$ 的先验分布 $p(x_t)$ 是一个高斯分布，其均值为 $\\hat{x}_{t|t-1}$，协方差矩阵为 $P_{t|t-1}$。该协方差明确给出为：\n$$\nP_{t|t-1} = \\begin{pmatrix} q_h  0 \\\\ 0  q_c \\end{pmatrix}\n$$\n给定状态 $x_t$，观测值 $y_t$ 的似然也是高斯的，$p(y_t|x_t) = \\mathcal{N}(y_t; Hx_t, R)$。\n\n我们的目标是计算后验分布 $p(x_t|y_t)$。对于线性高斯系统，后验分布也是高斯的，$p(x_t|y_t) = \\mathcal{N}(x_t; \\hat{x}_{t|t}, P_{t|t})$。后验均值 $\\hat{x}_{t|t}$ 和协方差 $P_{t|t}$ 由卡尔曼滤波器更新方程给出，这些方程是对联合高斯变量进行条件化的贝叶斯法则的直接推论。\n\n更新方程为：\n$$\n\\hat{x}_{t|t} = \\hat{x}_{t|t-1} + K_t (y_t - H\\hat{x}_{t|t-1})\n$$\n$$\nP_{t|t} = (I - K_t H) P_{t|t-1}\n$$\n其中 $K_t$ 是卡尔曼增益，定义为：\n$$\nK_t = P_{t|t-1} H^T S_t^{-1}\n$$\n而 $S_t$ 是新息协方差（预测观测值的方差）：\n$$\nS_t = H P_{t|t-1} H^T + R\n$$\n我们按步骤计算这些量。\n\n1.  **计算新息协方差 $S_t$**：\n    在这种情况下，$S_t$ 是一个标量。\n    $$\n    S_t = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} q_h  0 \\\\ 0  q_c \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + r\n    $$\n    $$\n    S_t = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} q_h \\\\ q_c \\end{pmatrix} + r = (q_h + q_c) + r\n    $$\n    因此，$S_t = q_h + q_c + r$。其逆为 $S_t^{-1} = \\frac{1}{q_h + q_c + r}$。\n\n2.  **计算卡尔曼增益 $K_t$**：\n    $K_t$ 是一个 $2 \\times 1$ 的向量。\n    $$\n    K_t = P_{t|t-1} H^T S_t^{-1} = \\begin{pmatrix} q_h  0 \\\\ 0  q_c \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\frac{1}{q_h + q_c + r}\n    $$\n    $$\n    K_t = \\begin{pmatrix} q_h \\\\ q_c \\end{pmatrix} \\frac{1}{q_h + q_c + r} = \\begin{pmatrix} \\frac{q_h}{q_h + q_c + r} \\\\ \\frac{q_c}{q_h + q_c + r} \\end{pmatrix}\n    $$\n    卡尔曼增益向量 $K_t$ 决定了预测误差 $(y_t - H\\hat{x}_{t|t-1})$ 如何在海马状态和皮层状态的更新之间分配。每个状态均值的更新与其先验不确定性（$q_h$ 或 $q_c$）成正比。这展示了证据加权更新机制。\n\n3.  **计算后验协方差 $P_{t|t}$**：\n    后验协方差矩阵的对角线上包含后验方差。我们使用公式 $P_{t|t} = (I - K_t H) P_{t|t-1}$。\n    首先，我们计算项 $K_t H$：\n    $$\n    K_t H = \\begin{pmatrix} \\frac{q_h}{q_h + q_c + r} \\\\ \\frac{q_c}{q_h + q_c + r} \\end{pmatrix} \\begin{pmatrix} 1  1 \\end{pmatrix} = \\frac{1}{q_h + q_c + r} \\begin{pmatrix} q_h  q_h \\\\ q_c  q_c \\end{pmatrix}\n    $$\n    接下来，我们计算项 $(I - K_t H)$：\n    $$\n    I - K_t H = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\frac{1}{q_h + q_c + r} \\begin{pmatrix} q_h  q_h \\\\ q_c  q_c \\end{pmatrix}\n    $$\n    $$\n    I - K_t H = \\frac{1}{q_h + q_c + r} \\left[ \\begin{pmatrix} q_h + q_c + r  0 \\\\ 0  q_h + q_c + r \\end{pmatrix} - \\begin{pmatrix} q_h  q_h \\\\ q_c  q_c \\end{pmatrix} \\right]\n    $$\n    $$\n    I - K_t H = \\frac{1}{q_h + q_c + r} \\begin{pmatrix} q_c + r  -q_h \\\\ -q_c  q_h + r \\end{pmatrix}\n    $$\n    最后，我们将其与先验协方差 $P_{t|t-1}$ 相乘：\n    $$\n    P_{t|t} = (I - K_t H) P_{t|t-1} = \\frac{1}{q_h + q_c + r} \\begin{pmatrix} q_c + r  -q_h \\\\ -q_c  q_h + r \\end{pmatrix} \\begin{pmatrix} q_h  0 \\\\ 0  q_c \\end{pmatrix}\n    $$\n    $$\n    P_{t|t} = \\frac{1}{q_h + q_c + r} \\begin{pmatrix} (q_c + r)q_h  (-q_h)q_c \\\\ (-q_c)q_h  (q_h + r)q_c \\end{pmatrix}\n    $$\n    $$\n    P_{t|t} = \\frac{1}{q_h + q_c + r} \\begin{pmatrix} q_h(q_c + r)  -q_h q_c \\\\ -q_h q_c  q_c(q_h + r) \\end{pmatrix}\n    $$\n    后验协方差矩阵为 $P_{t|t} = \\begin{pmatrix} \\text{Var}(x_{h,t}|y_t)  \\text{Cov}(x_{h,t}, x_{c,t}|y_t) \\\\ \\text{Cov}(x_{c,t}, x_{h,t}|y_t)  \\text{Var}(x_{c,t}|y_t) \\end{pmatrix}$。对角线元素即为所求的后验方差。\n\n海马状态 $x_{h,t}$ 的后验方差是左上角的元素：\n$$\n\\text{Var}(x_{h,t}|y_t) = \\frac{q_h(q_c + r)}{q_h + q_c + r}\n$$\n皮层状态 $x_{c,t}$ 的后验方差是右下角的元素：\n$$\n\\text{Var}(x_{c,t}|y_t) = \\frac{q_c(q_h + r)}{q_h + q_c + r}\n$$\n这些表达式表示在整合了观测值 $y_t$ 之后，每个状态不确定性的减少。不确定性的减少取决于初始不确定性（$q_h$，$q_c$）和观测噪声方差（$r$）。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{q_h(q_c + r)}{q_h + q_c + r}  \\frac{q_c(q_h + r)}{q_h + q_c + r}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}