{
    "hands_on_practices": [
        {
            "introduction": "Understanding the long-term behavior of a learning rule is crucial for predicting its function. This exercise guides you through a foundational stability analysis of the Bienenstock–Cooper–Munro (BCM) rule in a simplified, single-synapse setting. By deriving the mean-field dynamics and determining the stability of its equilibrium points, you will see how the rule leverages input statistics to drive synaptic self-organization.",
            "id": "3965026",
            "problem": "Consider a single linear neuron receiving a scalar presynaptic input $x$ and producing a scalar postsynaptic output $y$ given by $y=wx$, where $w$ is a scalar synaptic efficacy. Assume $x$ is a stationary random variable with finite moments up to third order and with zero mean. Let $\\sigma^{2}=\\mathbb{E}[x^{2}]>0$ and $\\mu_{3}=\\mathbb{E}[x^{3}] \\neq 0$. The synapse adapts according to the Bienenstock–Cooper–Munro (BCM) learning rule with time-scale separation (fast activity, slow synaptic change), namely that the expected synaptic change is governed by\n$$\n\\frac{dw}{dt}=\\eta\\,\\mathbb{E}\\!\\left[y\\,(y-\\theta)\\,x\\right],\n$$\nwhere $\\eta>0$ is a learning rate and the sliding threshold is $\\theta=\\beta\\,\\mathbb{E}[y^{2}]$, with $\\beta>0$ a constant. Using only the definitions above and the properties of expectations, derive the deterministic mean-field weight dynamics, compute all equilibrium values of $w$, and determine the local stability of each equilibrium by analyzing the sign of the linearization of the mean-field dynamics around the equilibria. Express your final answer as the set of equilibrium values of $w$ in closed form. No numerical rounding is required.",
            "solution": "To find the equilibrium values of the synaptic weight $w$, we first derive the mean-field differential equation for $w$. The given learning rule is:\n$$\n\\frac{dw}{dt}=\\eta\\,\\mathbb{E}\\!\\left[y\\,(y-\\theta)\\,x\\right]\n$$\nWe substitute the definitions for the postsynaptic output $y=wx$ and the threshold $\\theta=\\beta\\,\\mathbb{E}[y^{2}]$. First, express the threshold in terms of $w$:\n$$\n\\theta = \\beta\\,\\mathbb{E}[(wx)^2] = \\beta w^2 \\mathbb{E}[x^2] = \\beta w^2 \\sigma^2\n$$\nNow, substitute both $y$ and $\\theta$ into the expectation in the learning rule:\n$$\n\\mathbb{E}\\!\\left[y\\,(y-\\theta)\\,x\\right] = \\mathbb{E}\\!\\left[wx(wx - \\beta w^2 \\sigma^2)x\\right]\n$$\nUsing the linearity of expectation, we expand the terms:\n$$\n\\mathbb{E}\\!\\left[w^2 x^3 - \\beta w^3 \\sigma^2 x^2\\right] = w^2 \\mathbb{E}[x^3] - \\beta w^3 \\sigma^2 \\mathbb{E}[x^2]\n$$\nUsing the given input statistics $\\mathbb{E}[x^2] = \\sigma^2$ and $\\mathbb{E}[x^3] = \\mu_3$, the expression becomes:\n$$\nw^2 \\mu_3 - \\beta w^3 \\sigma^2 (\\sigma^2) = w^2 \\mu_3 - \\beta w^3 \\sigma^4\n$$\nPlugging this back into the differential equation for $w$, we get the mean-field dynamics:\n$$\n\\frac{dw}{dt} = \\eta \\left( w^2 \\mu_3 - \\beta w^3 \\sigma^4 \\right) = \\eta w^2 (\\mu_3 - \\beta \\sigma^4 w)\n$$\nEquilibrium values $w^*$ are found by setting $\\frac{dw}{dt} = 0$:\n$$\n\\eta w^{*2} (\\mu_3 - \\beta \\sigma^4 w^*) = 0\n$$\nSince $\\eta > 0$, this equation yields two possible solutions for the equilibrium points:\n1.  $w^{*2} = 0 \\implies w_1^* = 0$\n2.  $\\mu_3 - \\beta \\sigma^4 w^* = 0 \\implies w_2^* = \\frac{\\mu_3}{\\beta \\sigma^4}$\n\nThe problem asks for the set of equilibrium values, which are $0$ and $\\frac{\\mu_3}{\\beta \\sigma^4}$. The stability analysis (which is not required for the final answer but confirms the behavior) shows that $w_1^* = 0$ is an unstable equilibrium (specifically, semi-stable) and $w_2^*$ is a stable equilibrium, as long as $\\mu_3 \\neq 0$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & \\frac{\\mu_{3}}{\\beta \\sigma^{4}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Simple Hebbian plasticity, based on correlations, can be sensitive to the mean level of activity, not just the informative patterns within it. Covariance-based rules improve upon this by centering the variables, making learning sensitive to structured co-variation rather than baseline activity. This practice provides a concrete, quantitative comparison, demonstrating how the update directions for correlation- and covariance-based rules can differ, a key principle for developing feature selectivity in neurons.",
            "id": "3965027",
            "problem": "Consider a single linear neuron with output $y = \\mathbf{w}^{\\top}\\mathbf{x}$. The input is mean-shifted as $\\mathbf{x} = \\tilde{\\mathbf{x}} + \\boldsymbol{\\mu}$, where $\\tilde{\\mathbf{x}}$ is a zero-mean random vector with covariance $\\boldsymbol{\\Sigma}$. Assume $\\tilde{\\mathbf{x}}$ is Gaussian with $\\tilde{\\mathbf{x}} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$, and let the parameters be $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}$, $\\boldsymbol{\\mu} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, and $\\mathbf{w} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. \n\nCompare the correlation-based Hebbian update direction, which is proportional to the expectation $\\mathbb{E}[\\mathbf{x} y]$, with the covariance-based update direction, which is proportional to the expectation $\\mathbb{E}\\big[(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}])\\,(y - \\mathbb{E}[y])\\big]$. These two learning directions arise respectively in classical Hebbian learning and in centered (covariance-based) formulations related to Bienenstock–Cooper–Munro (BCM) theory.\n\nCompute the angle $\\theta$ between the two expected update direction vectors. Give your answer as a single exact analytic expression in radians. Do not approximate, and do not include units in your final boxed answer.",
            "solution": "The problem asks for the angle $\\theta$ between the correlation-based Hebbian update direction, $\\mathbf{v}_H$, and the covariance-based update direction, $\\mathbf{v}_C$. These vectors are defined as:\n$$ \\mathbf{v}_H = \\mathbb{E}[\\mathbf{x} y] $$\n$$ \\mathbf{v}_C = \\mathbb{E}\\big[(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}])(y - \\mathbb{E}[y])\\big] $$\nThe angle $\\theta$ can be found using the dot product formula: $\\cos(\\theta) = \\frac{\\mathbf{v}_H^{\\top}\\mathbf{v}_C}{\\|\\mathbf{v}_H\\| \\|\\mathbf{v}_C\\|}$.\n\nFirst, we calculate the necessary expectations. The expectation of the input $\\mathbf{x}$ is:\n$$ \\mathbb{E}[\\mathbf{x}] = \\mathbb{E}[\\tilde{\\mathbf{x}} + \\boldsymbol{\\mu}] = \\mathbb{E}[\\tilde{\\mathbf{x}}] + \\boldsymbol{\\mu} = \\mathbf{0} + \\boldsymbol{\\mu} = \\boldsymbol{\\mu} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} $$\nThe expectation of the output $y = \\mathbf{w}^{\\top}\\mathbf{x}$ is:\n$$ \\mathbb{E}[y] = \\mathbb{E}[\\mathbf{w}^{\\top}\\mathbf{x}] = \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}] = \\mathbf{w}^{\\top}\\boldsymbol{\\mu} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = 1 - 2 = -1 $$\n\nNext, we compute the covariance-based update vector, $\\mathbf{v}_C$. This vector is the covariance between the input $\\mathbf{x}$ and the output $y$, which simplifies to:\n$$ \\mathbf{v}_C = \\text{Cov}(\\mathbf{x}, y) = \\text{Cov}(\\mathbf{x}, \\mathbf{w}^{\\top}\\mathbf{x}) = \\text{Cov}(\\mathbf{x})\\mathbf{w} = \\boldsymbol{\\Sigma}\\mathbf{w} $$\nUsing the given values:\n$$ \\mathbf{v}_C = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2(1) + 1(-1) \\\\ 1(1) + 3(-1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} $$\n\nNow, we compute the Hebbian (correlation-based) update vector, $\\mathbf{v}_H$:\n$$ \\mathbf{v}_H = \\mathbb{E}[\\mathbf{x} y] = \\mathbb{E}[\\mathbf{x} (\\mathbf{w}^{\\top}\\mathbf{x})] = \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}] \\mathbf{w} $$\nThe correlation matrix $\\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}]$ is related to the covariance matrix by $\\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}] = \\boldsymbol{\\Sigma} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top}$.\nFirst, calculate $\\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top}$:\n$$ \\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix} $$\nThen, the correlation matrix is:\n$$ \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}] = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} + \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix} = \\begin{pmatrix} 3 & 3 \\\\ 3 & 7 \\end{pmatrix} $$\nFinally, compute $\\mathbf{v}_H$:\n$$ \\mathbf{v}_H = \\begin{pmatrix} 3 & 3 \\\\ 3 & 7 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 3(1) + 3(-1) \\\\ 3(1) + 7(-1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -4 \\end{pmatrix} $$\n\nWith the vectors $\\mathbf{v}_H = \\begin{pmatrix} 0 \\\\ -4 \\end{pmatrix}$ and $\\mathbf{v}_C = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$, we compute the components for the angle formula:\n- Dot product: $\\mathbf{v}_H^{\\top}\\mathbf{v}_C = (0)(1) + (-4)(-2) = 8$\n- Magnitudes:\n  - $\\|\\mathbf{v}_H\\| = \\sqrt{0^2 + (-4)^2} = 4$\n  - $\\|\\mathbf{v}_C\\| = \\sqrt{1^2 + (-2)^2} = \\sqrt{1 + 4} = \\sqrt{5}$\n\nThe cosine of the angle is:\n$$ \\cos(\\theta) = \\frac{8}{4\\sqrt{5}} = \\frac{2}{\\sqrt{5}} $$\nThe angle $\\theta$ is the arccosine of this value:\n$$ \\theta = \\arccos\\left(\\frac{2}{\\sqrt{5}}\\right) $$",
            "answer": "$$\n\\boxed{\\arccos\\left(\\frac{2}{\\sqrt{5}}\\right)}\n$$"
        },
        {
            "introduction": "Translating a continuous-time theoretical model into a discrete-time computer simulation introduces practical challenges, most notably the risk of numerical instability. This exercise bridges that gap by having you discretize the BCM differential equation using the explicit Euler method. You will then derive the critical constraint on the learning rate, $\\eta$, which is essential for ensuring your simulation remains stable and produces meaningful results.",
            "id": "3965044",
            "problem": "Consider a single postsynaptic neuron driven by a single presynaptic input in the Bienenstock–Cooper–Munro (BCM) framework. Let the synaptic weight be $w(t) \\in \\mathbb{R}$, the presynaptic input be $x(t) \\in \\mathbb{R}$, and the postsynaptic response be $y(t) = w(t)\\,x(t)$. The continuous-time BCM synaptic modification rule is\n$$\n\\frac{d w(t)}{d t} \\;=\\; \\eta\\, x(t)\\, y(t)\\,\\big(y(t) - \\theta(t)\\big),\n$$\nwhere $\\eta > 0$ is a learning-rate constant and $\\theta(t)$ is a sliding threshold that depends on the activity history. Assume a standard time-scale separation in which $\\theta(t)$ evolves on a much slower time scale than $w(t)$, so that near the equilibrium of interest $\\theta(t)$ can be regarded as approximately constant, $\\theta(t) \\approx \\theta_0$ with $\\theta_0 > 0$. Assume further that the input sequence is bounded in magnitude in discrete time, i.e., for samples $x_t = x(t_t)$ taken at uniform sampling times $t_t$, there exists $X > 0$ such that $|x_t| \\leq X$ for all $t$.\n\nTasks:\n- Starting from the continuous-time rule above, derive the first-order explicit Euler discretization for $w_{t+1}$ in terms of $w_t$, $x_t$, $y_t$, and $\\theta_t$, using an effective step size denoted by $\\eta$ (i.e., absorb the sampling interval into $\\eta$).\n- Under the bounded-input and time-scale-separation assumptions stated above, compute the supremum value $\\eta_{\\max}$ of the effective step size $\\eta$ such that the discrete-time zero fixed point $w^\\star = 0$ is locally asymptotically stable uniformly for any input sequence $\\{x_t\\}$ with $|x_t| \\leq X$. Express $\\eta_{\\max}$ in closed form in terms of $\\theta_0$ and $X$.\n\nYour final answer must be a single closed-form analytic expression for $\\eta_{\\max}$.",
            "solution": "This solution proceeds in two parts. First, we derive the discrete-time update rule for the synaptic weight $w$ using the explicit Euler method. Second, we perform a local stability analysis of the zero fixed point ($w^\\star = 0$) for this discrete-time system to find the maximum effective step size $\\eta_{\\max}$ that ensures stability.\n\n**Part 1: Discretization of the BCM Rule**\n\nThe continuous-time BCM rule is given as:\n$$\n\\frac{d w(t)}{d t} \\;=\\; \\eta_{c}\\, x(t)\\, y(t)\\,\\big(y(t) - \\theta(t)\\big)\n$$\nApplying the first-order explicit Euler method with a time step $\\Delta t$, we get:\n$$\nw(t+\\Delta t) \\approx w(t) + \\Delta t \\cdot \\frac{d w(t)}{d t}\n$$\nUsing discrete-time notation ($w_t \\equiv w(t_t)$) and defining an effective step size $\\eta = \\eta_c \\Delta t$, the update rule becomes:\n$$\nw_{t+1} = w_t + \\eta\\, x_t\\, y_t\\,(y_t - \\theta_t)\n$$\n\n**Part 2: Stability Analysis for $\\eta_{\\max}$**\n\nWe analyze the stability of the fixed point $w^\\star = 0$. We substitute $y_t = w_t x_t$ into the update rule and assume a constant threshold $\\theta_t \\approx \\theta_0 > 0$:\n$$\nw_{t+1} = w_t + \\eta\\, x_t\\, (w_t x_t)\\,\\big(w_t x_t - \\theta_0\\big) = w_t + \\eta w_t x_t^2 (w_t x_t - \\theta_0)\n$$\nLet's define the update map as $w_{t+1} = F(w_t, x_t)$. It is clear that $w^\\star = 0$ is a fixed point, since $F(0, x_t) = 0$.\n\nFor local asymptotic stability of a discrete map, the magnitude of the derivative of the map with respect to the state variable, evaluated at the fixed point, must be less than 1. The derivative of $F(w_t, x_t)$ with respect to $w_t$ is:\n$$\n\\frac{\\partial F}{\\partial w_t} = \\frac{\\partial}{\\partial w_t} \\left[ w_t + \\eta x_t^3 w_t^2 - \\eta \\theta_0 x_t^2 w_t \\right] = 1 + 2 \\eta x_t^3 w_t - \\eta \\theta_0 x_t^2\n$$\nEvaluating this derivative at the fixed point $w_t = w^\\star = 0$, we get the multiplier $\\lambda_t$:\n$$\n\\lambda_t = \\left. \\frac{\\partial F}{\\partial w_t} \\right|_{w_t=0} = 1 - \\eta \\theta_0 x_t^2\n$$\nFor uniform local asymptotic stability, we require $|\\lambda_t|  1$ for all possible inputs $x_t$ such that $|x_t| \\leq X$. This is equivalent to the condition:\n$$\n-1  1 - \\eta \\theta_0 x_t^2  1\n$$\nThe right side of the inequality, $1 - \\eta \\theta_0 x_t^2  1$, simplifies to $-\\eta \\theta_0 x_t^2  0$. Since $\\eta, \\theta_0 > 0$, this is always satisfied for any $x_t \\neq 0$.\n\nThe left side of the inequality, $-1  1 - \\eta \\theta_0 x_t^2$, provides the critical constraint:\n$$\n\\eta \\theta_0 x_t^2  2\n$$\nTo ensure this holds for any input sequence where $|x_t| \\leq X$, we must satisfy the condition for the worst-case scenario, which is when $x_t^2$ is maximized. The maximum value of $x_t^2$ is $X^2$. Therefore, we must have:\n$$\n\\eta \\theta_0 X^2  2\n$$\nSolving for $\\eta$ gives the upper bound on the effective step size:\n$$\n\\eta  \\frac{2}{\\theta_0 X^2}\n$$\nThe supremum of the set of all valid $\\eta$ values is the value of this upper bound.\n$$\n\\eta_{\\max} = \\frac{2}{\\theta_0 X^2}\n$$",
            "answer": "$$\\boxed{\\frac{2}{\\theta_0 X^2}}$$"
        }
    ]
}