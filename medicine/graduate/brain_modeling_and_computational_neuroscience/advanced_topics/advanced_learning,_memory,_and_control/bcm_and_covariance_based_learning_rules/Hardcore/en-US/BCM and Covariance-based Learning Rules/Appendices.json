{
    "hands_on_practices": [
        {
            "introduction": "The BCM learning rule describes a dynamic process where synaptic weights evolve to stable states. This exercise guides you through a mean-field analysis to uncover how these stable points are determined by the statistical moments of the input signal . Understanding this derivation is fundamental to grasping how BCM synapses self-organize based on their activity history.",
            "id": "3965026",
            "problem": "Consider a single linear neuron receiving a scalar presynaptic input $x$ and producing a scalar postsynaptic output $y$ given by $y=wx$, where $w$ is a scalar synaptic efficacy. Assume $x$ is a stationary random variable with finite moments up to third order and with zero mean. Let $\\sigma^{2}=\\mathbb{E}[x^{2}]0$ and $\\mu_{3}=\\mathbb{E}[x^{3}] \\neq 0$. The synapse adapts according to the Bienenstock–Cooper–Munro (BCM) learning rule with time-scale separation (fast activity, slow synaptic change), namely that the expected synaptic change is governed by\n$$\n\\frac{dw}{dt}=\\eta\\,\\mathbb{E}\\!\\left[y\\,(y-\\theta)\\,x\\right],\n$$\nwhere $\\eta0$ is a learning rate and the sliding threshold is $\\theta=\\beta\\,\\mathbb{E}[y^{2}]$, with $\\beta0$ a constant. Using only the definitions above and the properties of expectations, derive the deterministic mean-field weight dynamics, compute all equilibrium values of $w$, and determine the local stability of each equilibrium by analyzing the sign of the linearization of the mean-field dynamics around the equilibria. Express your final answer as the set of equilibrium values of $w$ in closed form. No numerical rounding is required.",
            "solution": "The user wants me to solve a problem related to the BCM learning rule in computational neuroscience.\n\nHere's my plan:\n1.  **Validate the problem:**\n    *   **Step 1: Extract Givens:** I will list all the definitions and equations provided in the problem statement.\n        *   Neuron model: $y = wx$\n        *   Input properties: $x$ is a stationary random variable, $\\mathbb{E}[x] = 0$, $\\sigma^2 = \\mathbb{E}[x^2]  0$, $\\mu_3 = \\mathbb{E}[x^3] \\neq 0$.\n        *   BCM learning rule (mean-field): $\\frac{dw}{dt} = \\eta \\mathbb{E}[y(y-\\theta)x]$\n        *   Learning rate: $\\eta  0$\n        *   Sliding threshold: $\\theta = \\beta \\mathbb{E}[y^2]$\n        *   Threshold constant: $\\beta  0$\n    *   **Step 2: Validate:** I'll check the givens for scientific soundness, completeness, and consistency.\n        *   **Scientific Grounding**: The BCM learning rule is a well-established model in computational neuroscience. The neuron model ($y=wx$) is a standard linear neuron. The assumptions about the input signal ($x$) are common in this type of analysis. The problem is scientifically grounded.\n        *   **Well-Posed**: The problem asks for the derivation of the dynamics, the equilibria, and their stability. This is a standard dynamical systems analysis. The setup appears to allow for a unique solution for the dynamics and a discrete set of equilibria. The problem is well-posed.\n        *   **Objective**: The problem uses precise mathematical language and definitions. It is objective.\n        *   **Flaws**:\n            1.  **Scientific/Factual Unsoundness**: None.\n            2.  **Non-Formalizable/Irrelevant**: The problem is formalizable and directly relevant to the topic of BCM learning rules.\n            3.  **Incomplete/Contradictory**: The problem seems self-contained. All necessary variables and constants are defined. There are no apparent contradictions.\n            4.  **Unrealistic/Infeasible**: The setup is a simplified model, but not unrealistic within the context of theoretical neuroscience.\n            5.  **Ill-Posed/Poorly Structured**: The structure is clear. It's a standard stability analysis problem.\n            6.  **Pseudo-Profound/Trivial**: The problem requires a multi-step derivation and analysis, not trivial.\n            7.  **Outside Verifiability**: The mathematical derivation is verifiable.\n    *   **Step 3: Verdict:** The problem is valid. I will proceed with the solution.\n\n2.  **Solve the problem:**\n    *   **Part 1: Derive the mean-field weight dynamics.**\n        *   The goal is to express $\\frac{dw}{dt}$ as a function of $w$ and the constants ($\\eta, \\beta, \\sigma^2, \\mu_3$).\n        *   The starting point is $\\frac{dw}{dt} = \\eta \\mathbb{E}[y(y-\\theta)x]$.\n        *   First, I need to express the terms inside the expectation in terms of $x$ and $w$.\n            *   $y = wx$\n            *   $\\theta = \\beta \\mathbb{E}[y^2] = \\beta \\mathbb{E}[(wx)^2] = \\beta w^2 \\mathbb{E}[x^2] = \\beta w^2 \\sigma^2$.\n        *   Now substitute these into the expectation:\n            *   $\\mathbb{E}[y(y-\\theta)x] = \\mathbb{E}[wx(wx - \\beta w^2 \\sigma^2)x]$\n        *   Distribute the terms inside the expectation:\n            *   $= \\mathbb{E}[wx(w x^2 - \\beta w^2 \\sigma^2 x)]$\n            *   $= \\mathbb{E}[w^2 x^3 - \\beta w^3 \\sigma^2 x^2]$\n        *   Use the linearity of expectation:\n            *   $= w^2 \\mathbb{E}[x^3] - \\beta w^3 \\sigma^2 \\mathbb{E}[x^2]$\n        *   Substitute the given moments: $\\mathbb{E}[x^3] = \\mu_3$ and $\\mathbb{E}[x^2] = \\sigma^2$.\n            *   $= w^2 \\mu_3 - \\beta w^3 \\sigma^2 \\sigma^2 = w^2 \\mu_3 - \\beta w^3 \\sigma^4$\n        *   Now, substitute this back into the expression for $\\frac{dw}{dt}$:\n            *   $\\frac{dw}{dt} = \\eta (w^2 \\mu_3 - \\beta w^3 \\sigma^4)$\n            *   $\\frac{dw}{dt} = \\eta w^2 (\\mu_3 - \\beta \\sigma^4 w)$\n        *   This is the deterministic mean-field weight dynamics.\n\n    *   **Part 2: Compute all equilibrium values of $w$.**\n        *   Equilibria $w^*$ are found by setting $\\frac{dw}{dt} = 0$.\n        *   $\\eta w^{*2} (\\mu_3 - \\beta \\sigma^4 w^*) = 0$\n        *   Since $\\eta  0$, we have two possibilities:\n            1.  $w^{*2} = 0 \\implies w_1^* = 0$. This is a double root, which might have implications for stability analysis, but for finding equilibria, it's just one point.\n            2.  $\\mu_3 - \\beta \\sigma^4 w^* = 0 \\implies w_2^* = \\frac{\\mu_3}{\\beta \\sigma^4}$.\n        *   So the equilibrium values are $w_1^* = 0$ and $w_2^* = \\frac{\\mu_3}{\\beta \\sigma^4}$.\n        *   The problem states $\\mu_3 \\neq 0$, $\\beta  0$, and $\\sigma^2  0$ (so $\\sigma^4  0$). This means $w_2^* \\neq 0$, so the two equilibria are distinct.\n\n    *   **Part 3: Determine the local stability of each equilibrium.**\n        *   Let $f(w) = \\frac{dw}{dt} = \\eta w^2 (\\mu_3 - \\beta \\sigma^4 w) = \\eta \\mu_3 w^2 - \\eta \\beta \\sigma^4 w^3$.\n        *   Local stability is determined by the sign of the derivative $f'(w)$ evaluated at the equilibrium point $w^*$.\n            *   If $f'(w^*)  0$, the equilibrium is stable.\n            *   If $f'(w^*)  0$, the equilibrium is unstable.\n            *   If $f'(w^*) = 0$, the linear stability analysis is inconclusive, and higher-order derivatives are needed.\n        *   First, compute the derivative $f'(w)$:\n            *   $f'(w) = \\frac{d}{dw} (\\eta \\mu_3 w^2 - \\eta \\beta \\sigma^4 w^3)$\n            *   $f'(w) = 2 \\eta \\mu_3 w - 3 \\eta \\beta \\sigma^4 w^2$\n            *   $f'(w) = \\eta w (2 \\mu_3 - 3 \\beta \\sigma^4 w)$\n\n        *   **Stability of $w_1^* = 0$:**\n            *   $f'(0) = \\eta (0) (2 \\mu_3 - 3 \\beta \\sigma^4 (0)) = 0$.\n            *   The linear stability analysis is inconclusive. We must analyze the function $f(w)$ directly in the neighborhood of $w=0$.\n            *   $f(w) = \\eta w^2 (\\mu_3 - \\beta \\sigma^4 w)$.\n            *   Near $w=0$, the term $(\\mu_3 - \\beta \\sigma^4 w)$ is approximately $\\mu_3$. So, $f(w) \\approx \\eta \\mu_3 w^2$.\n            *   The term $w^2$ is always non-negative for real $w$. Since $\\eta  0$ and $\\mu_3 \\neq 0$ are constants, the sign of $f(w)$ for $w$ close to $0$ is determined by the sign of $\\mu_3$.\n            *   If $\\mu_3  0$, then $f(w)  0$ for $w$ in a neighborhood of $0$ (except at $w=0$). This means that for any $w$ slightly less than $0$, $\\frac{dw}{dt}  0$ (moving toward $0$), and for any $w$ slightly greater than $0$, $\\frac{dw}{dt}  0$ (moving away from $0$). This is a semi-stable equilibrium.\n            *   If $\\mu_3  0$, then $f(w)  0$ for $w$ in a neighborhood of $0$. This means that for any $w$ slightly greater than $0$, $\\frac{dw}{dt}  0$ (moving toward $0$), and for any $w$ slightly less than $0$, $\\frac{dw}{dt}  0$ (moving away from $0$). This is also a semi-stable equilibrium.\n            *   Since trajectories do not universally converge to $w=0$ from both sides, this equilibrium is not locally stable.\n\n        *   **Stability of $w_2^* = \\frac{\\mu_3}{\\beta \\sigma^4}$:**\n            *   We evaluate $f'(w)$ at this point:\n            *   $f'(w_2^*) = \\eta w_2^* (2 \\mu_3 - 3 \\beta \\sigma^4 w_2^*)$\n            *   Substitute the expression for $w_2^*$:\n            *   $f'(w_2^*) = \\eta \\left( \\frac{\\mu_3}{\\beta \\sigma^4} \\right) \\left( 2 \\mu_3 - 3 \\beta \\sigma^4 \\left( \\frac{\\mu_3}{\\beta \\sigma^4} \\right) \\right)$\n            *   $f'(w_2^*) = \\eta \\left( \\frac{\\mu_3}{\\beta \\sigma^4} \\right) (2 \\mu_3 - 3 \\mu_3)$\n            *   $f'(w_2^*) = \\eta \\left( \\frac{\\mu_3}{\\beta \\sigma^4} \\right) (- \\mu_3)$\n            *   $f'(w_2^*) = - \\frac{\\eta \\mu_3^2}{\\beta \\sigma^4}$\n            *   We are given that $\\eta0$, $\\beta0$, and $\\sigma^20$, which implies $\\sigma^40$.\n            *   We are also given $\\mu_3 \\neq 0$, which implies $\\mu_3^2  0$.\n            *   Therefore, every constant in the expression is positive, and the overall sign is negative.\n            *   $f'(w_2^*)  0$.\n            *   This indicates that the equilibrium point $w_2^* = \\frac{\\mu_3}{\\beta \\sigma^4}$ is locally stable.\n\nThe problem asks for the set of equilibrium values of $w$. Based on our analysis, these are $w_1^* = 0$ and $w_2^* = \\frac{\\mu_3}{\\beta \\sigma^4}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  \\frac{\\mu_{3}}{\\beta \\sigma^{4}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A key innovation of BCM over classical Hebbian learning is its reliance on covariance rather than simple correlation. This practice provides a concrete scenario to explore the profound difference this makes, especially when inputs have a non-zero mean . By calculating the distinct update directions proposed by each rule, you will see precisely how covariance-based learning filters out the influence of mean firing rates, a critical feature for stable learning.",
            "id": "3965027",
            "problem": "Consider a single linear neuron with output $y = \\mathbf{w}^{\\top}\\mathbf{x}$. The input is mean-shifted as $\\mathbf{x} = \\tilde{\\mathbf{x}} + \\boldsymbol{\\mu}$, where $\\tilde{\\mathbf{x}}$ is a zero-mean random vector with covariance $\\boldsymbol{\\Sigma}$. Assume $\\tilde{\\mathbf{x}}$ is Gaussian with $\\tilde{\\mathbf{x}} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$, and let the parameters be $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$, $\\boldsymbol{\\mu} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, and $\\mathbf{w} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. \n\nCompare the correlation-based Hebbian update direction, which is proportional to the expectation $\\mathbb{E}[\\mathbf{x} y]$, with the covariance-based update direction, which is proportional to the expectation $\\mathbb{E}\\big[(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}])\\,(y - \\mathbb{E}[y])\\big]$. These two learning directions arise respectively in classical Hebbian learning and in centered (covariance-based) formulations related to Bienenstock–Cooper–Munro (BCM) theory.\n\nCompute the angle $\\theta$ between the two expected update direction vectors. Give your answer as a single exact analytic expression in radians. Do not approximate, and do not include units in your final boxed answer.",
            "solution": "The user wants to find the angle between two update direction vectors derived from a linear neuron model. The first is the correlation-based Hebbian update direction, and the second is the covariance-based update direction.\n\nFirst, we must validate the problem statement. The problem provides a well-defined linear neuron model $y = \\mathbf{w}^{\\top}\\mathbf{x}$ with a Gaussian input $\\mathbf{x}$ having a non-zero mean. All parameters ($\\boldsymbol{\\Sigma}$, $\\boldsymbol{\\mu}$, $\\mathbf{w}$) are given and are mathematically consistent. The covariance matrix $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$ is symmetric and positive definite since its determinant is $(2)(3) - (1)(1) = 5  0$ and its trace is $2+3=50$. The concepts of correlation-based (Hebbian) and covariance-based learning rules are central to computational neuroscience. The problem is scientifically grounded, well-posed, and objective, with sufficient information for a unique solution. Therefore, the problem is valid.\n\nWe are asked to compute the angle $\\theta$ between two vectors. Let the correlation-based Hebbian update direction be $\\mathbf{v}_H$ and the covariance-based update direction be $\\mathbf{v}_C$. These are defined as:\n$$ \\mathbf{v}_H = \\mathbb{E}[\\mathbf{x} y] $$\n$$ \\mathbf{v}_C = \\mathbb{E}\\big[(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}])(y - \\mathbb{E}[y])\\big] $$\nThe angle $\\theta$ between these two vectors is given by the formula:\n$$ \\theta = \\arccos\\left(\\frac{\\mathbf{v}_H^{\\top}\\mathbf{v}_C}{\\|\\mathbf{v}_H\\| \\|\\mathbf{v}_C\\|}\\right) $$\nTo proceed, we must compute the vectors $\\mathbf{v}_H$ and $\\mathbf{v}_C$. This requires calculating the expectations of the input $\\mathbf{x}$ and the output $y$.\n\nThe input vector is given by $\\mathbf{x} = \\tilde{\\mathbf{x}} + \\boldsymbol{\\mu}$, where $\\tilde{\\mathbf{x}}$ is a zero-mean random vector, i.e., $\\mathbb{E}[\\tilde{\\mathbf{x}}] = \\mathbf{0}$. The expectation of the input $\\mathbf{x}$ is:\n$$ \\mathbb{E}[\\mathbf{x}] = \\mathbb{E}[\\tilde{\\mathbf{x}} + \\boldsymbol{\\mu}] = \\mathbb{E}[\\tilde{\\mathbf{x}}] + \\boldsymbol{\\mu} = \\mathbf{0} + \\boldsymbol{\\mu} = \\boldsymbol{\\mu} $$\nGiven $\\boldsymbol{\\mu} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, we have $\\mathbb{E}[\\mathbf{x}] = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\nThe output is $y = \\mathbf{w}^{\\top}\\mathbf{x}$. Its expectation is:\n$$ \\mathbb{E}[y] = \\mathbb{E}[\\mathbf{w}^{\\top}\\mathbf{x}] = \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}] = \\mathbf{w}^{\\top}\\boldsymbol{\\mu} $$\nUsing the given values $\\mathbf{w} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ and $\\boldsymbol{\\mu} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$:\n$$ \\mathbb{E}[y] = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = (1)(1) + (-1)(2) = 1 - 2 = -1 $$\n\nNow we compute the covariance-based update vector, $\\mathbf{v}_C$. Let $\\mathbf{x}' = \\mathbf{x} - \\mathbb{E}[\\mathbf{x}]$ and $y' = y - \\mathbb{E}[y]$.\nThe term $y'$ can be rewritten as $y - \\mathbb{E}[y] = \\mathbf{w}^{\\top}\\mathbf{x} - \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}] = \\mathbf{w}^{\\top}(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}]) = \\mathbf{w}^{\\top}\\mathbf{x}'$.\nSo, $\\mathbf{v}_C$ becomes:\n$$ \\mathbf{v}_C = \\mathbb{E}[\\mathbf{x}' y'] = \\mathbb{E}[\\mathbf{x}' (\\mathbf{x}')^{\\top}\\mathbf{w}] = \\mathbb{E}[(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}])(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}])^{\\top}]\\mathbf{w} $$\nThe expectation term is the definition of the covariance matrix of $\\mathbf{x}$, denoted $\\text{Cov}(\\mathbf{x})$. The covariance of $\\mathbf{x} = \\tilde{\\mathbf{x}} + \\boldsymbol{\\mu}$ is $\\text{Cov}(\\mathbf{x}) = \\text{Cov}(\\tilde{\\mathbf{x}}) = \\boldsymbol{\\Sigma}$.\nTherefore,\n$$ \\mathbf{v}_C = \\boldsymbol{\\Sigma} \\mathbf{w} $$\nSubstituting the given matrices:\n$$ \\mathbf{v}_C = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2(1) + 1(-1) \\\\ 1(1) + 3(-1) \\end{pmatrix} = \\begin{pmatrix} 2-1 \\\\ 1-3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} $$\n\nNext, we compute the Hebbian update vector, $\\mathbf{v}_H$:\n$$ \\mathbf{v}_H = \\mathbb{E}[\\mathbf{x} y] = \\mathbb{E}[\\mathbf{x} (\\mathbf{w}^{\\top}\\mathbf{x})] = \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top} \\mathbf{w}] = \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}] \\mathbf{w} $$\nThe term $\\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}]$ is the correlation matrix of $\\mathbf{x}$. It relates to the covariance matrix via the formula $\\text{Cov}(\\mathbf{x}) = \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}] - \\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{x}]^{\\top}$.\nThus, $\\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}] = \\text{Cov}(\\mathbf{x}) + \\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{x}]^{\\top} = \\boldsymbol{\\Sigma} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top}$.\nFirst, calculate $\\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top}$:\n$$ \\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\begin{pmatrix} 1  2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1  1 \\cdot 2 \\\\ 2 \\cdot 1  2 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 1  2 \\\\ 2  4 \\end{pmatrix} $$\nNow compute the correlation matrix:\n$$ \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}] = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} + \\begin{pmatrix} 1  2 \\\\ 2  4 \\end{pmatrix} = \\begin{pmatrix} 3  3 \\\\ 3  7 \\end{pmatrix} $$\nFinally, compute $\\mathbf{v}_H$:\n$$ \\mathbf{v}_H = \\begin{pmatrix} 3  3 \\\\ 3  7 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 3(1) + 3(-1) \\\\ 3(1) + 7(-1) \\end{pmatrix} = \\begin{pmatrix} 3-3 \\\\ 3-7 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -4 \\end{pmatrix} $$\n\nWith the two vectors $\\mathbf{v}_H = \\begin{pmatrix} 0 \\\\ -4 \\end{pmatrix}$ and $\\mathbf{v}_C = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$, we can compute the angle $\\theta$.\nFirst, the dot product $\\mathbf{v}_H^{\\top}\\mathbf{v}_C$:\n$$ \\mathbf{v}_H^{\\top}\\mathbf{v}_C = \\begin{pmatrix} 0  -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = (0)(1) + (-4)(-2) = 8 $$\nNext, the magnitudes of the vectors:\n$$ \\|\\mathbf{v}_H\\| = \\sqrt{0^2 + (-4)^2} = \\sqrt{16} = 4 $$\n$$ \\|\\mathbf{v}_C\\| = \\sqrt{1^2 + (-2)^2} = \\sqrt{1 + 4} = \\sqrt{5} $$\nNow, we find the cosine of the angle:\n$$ \\cos(\\theta) = \\frac{\\mathbf{v}_H^{\\top}\\mathbf{v}_C}{\\|\\mathbf{v}_H\\| \\|\\mathbf{v}_C\\|} = \\frac{8}{4\\sqrt{5}} = \\frac{2}{\\sqrt{5}} $$\nThe angle $\\theta$ is the arccosine of this value:\n$$ \\theta = \\arccos\\left(\\frac{2}{\\sqrt{5}}\\right) $$\nThis is the required exact analytic expression for the angle in radians.",
            "answer": "$$\n\\boxed{\\arccos\\left(\\frac{2}{\\sqrt{5}}\\right)}\n$$"
        },
        {
            "introduction": "Translating a continuous-time model like BCM into a discrete-time computer simulation requires careful consideration of numerical stability. This exercise focuses on this practical and crucial step, asking you to derive the explicit Euler discretization of the learning rule and analyze its behavior . Determining the maximum stable learning rate is a foundational skill in computational science, ensuring that your simulations are reliable and not artifacts of the chosen time step.",
            "id": "3965044",
            "problem": "Consider a single postsynaptic neuron driven by a single presynaptic input in the Bienenstock–Cooper–Munro (BCM) framework. Let the synaptic weight be $w(t) \\in \\mathbb{R}$, the presynaptic input be $x(t) \\in \\mathbb{R}$, and the postsynaptic response be $y(t) = w(t)\\,x(t)$. The continuous-time BCM synaptic modification rule is\n$$\n\\frac{d w(t)}{d t} \\;=\\; \\eta\\, x(t)\\, y(t)\\,\\big(y(t) - \\theta(t)\\big),\n$$\nwhere $\\eta  0$ is a learning-rate constant and $\\theta(t)$ is a sliding threshold that depends on the activity history. Assume a standard time-scale separation in which $\\theta(t)$ evolves on a much slower time scale than $w(t)$, so that near the equilibrium of interest $\\theta(t)$ can be regarded as approximately constant, $\\theta(t) \\approx \\theta_0$ with $\\theta_0  0$. Assume further that the input sequence is bounded in magnitude in discrete time, i.e., for samples $x_t = x(t_t)$ taken at uniform sampling times $t_t$, there exists $X  0$ such that $|x_t| \\leq X$ for all $t$.\n\nTasks:\n- Starting from the continuous-time rule above, derive the first-order explicit Euler discretization for $w_{t+1}$ in terms of $w_t$, $x_t$, $y_t$, and $\\theta_t$, using an effective step size denoted by $\\eta$ (i.e., absorb the sampling interval into $\\eta$).\n- Under the bounded-input and time-scale-separation assumptions stated above, compute the supremum value $\\eta_{\\max}$ of the effective step size $\\eta$ such that the discrete-time zero fixed point $w^\\star = 0$ is locally asymptotically stable uniformly for any input sequence $\\{x_t\\}$ with $|x_t| \\leq X$. Express $\\eta_{\\max}$ in closed form in terms of $\\theta_0$ and $X$.\n\nYour final answer must be a single closed-form analytic expression for $\\eta_{\\max}$.",
            "solution": "This problem requires a two-part solution. First, we will derive the discrete-time update rule for the synaptic weight $w$ using the explicit Euler method. Second, we will perform a local stability analysis of the zero fixed point $w^\\star = 0$ for this discrete-time system to determine the maximum permissible learning rate $\\eta_{\\max}$.\n\nThe problem statement has been validated and is deemed scientifically sound, well-posed, and objective. All necessary information is provided, and the assumptions are standard within the field of theoretical neuroscience.\n\nPart 1: Discretization of the BCM Rule\nThe continuous-time dynamics of the synaptic weight $w(t)$ are given by the Bienenstock–Cooper–Munro (BCM) rule:\n$$\n\\frac{d w(t)}{d t} \\;=\\; \\eta_{c}\\, x(t)\\, y(t)\\,\\big(y(t) - \\theta(t)\\big)\n$$\nwhere $y(t) = w(t)x(t)$ is the postsynaptic response, $x(t)$ is the presynaptic input, $\\theta(t)$ is the sliding threshold, and $\\eta_c  0$ is the continuous-time learning rate.\n\nTo obtain a discrete-time representation, we apply the first-order explicit Euler method. Let $\\Delta t$ be the time step duration. The weight at the next time step, $w(t+\\Delta t)$, is approximated as:\n$$\nw(t+\\Delta t) \\approx w(t) + \\Delta t \\cdot \\frac{d w(t)}{d t}\n$$\nSubstituting the BCM differential equation, we get:\n$$\nw(t+\\Delta t) \\approx w(t) + \\Delta t \\cdot \\eta_{c}\\, x(t)\\, y(t)\\,\\big(y(t) - \\theta(t)\\big)\n$$\nWe now adopt a discrete-time notation, where $w_t \\equiv w(t_t)$ for discrete time points $t_t = t \\cdot \\Delta t$. The problem specifies using an effective step size $\\eta$ that absorbs the sampling interval $\\Delta t$, i.e., $\\eta = \\eta_c \\Delta t$. The discretized update rule is thus:\n$$\nw_{t+1} = w_t + \\eta\\, x_t\\, y_t\\,(y_t - \\theta_t)\n$$\nThis is the first-order explicit Euler discretization for $w_{t+1}$, as required.\n\nPart 2: Stability Analysis for $\\eta_{\\max}$\nTo analyze the stability of the zero fixed point, we first substitute the expression for the postsynaptic response, $y_t = w_t x_t$, into the discrete-time rule. We also apply the time-scale separation assumption, treating the threshold as a constant, $\\theta_t \\approx \\theta_0  0$.\nThe update rule becomes a non-autonomous map $w_{t+1} = F(w_t, x_t)$:\n$$\nw_{t+1} = w_t + \\eta\\, x_t\\, (w_t x_t)\\,\\big(w_t x_t - \\theta_0\\big)\n$$\n$$\nw_{t+1} = w_t + \\eta\\, w_t x_t^2 (w_t x_t - \\theta_0)\n$$\n$$\nw_{t+1} = w_t - \\eta \\theta_0 x_t^2 w_t + \\eta x_t^3 w_t^2\n$$\nWe must verify that $w^\\star = 0$ is a fixed point of this map. Substituting $w_t = 0$:\n$$\nw_{t+1} = 0 - \\eta \\theta_0 x_t^2 (0) + \\eta x_t^3 (0)^2 = 0\n$$\nIndeed, $w^\\star = 0$ is a fixed point for any input $x_t$.\n\nFor local asymptotic stability of a fixed point $w^\\star$ of a scalar discrete map $w_{t+1} = F(w_t)$, the magnitude of the derivative of the map with respect to $w_t$, evaluated at $w^\\star$, must be strictly less than $1$. In this case, the map is non-autonomous, depending on the input $x_t$. The condition for local asymptotic stability, to hold uniformly for any input sequence, must be satisfied for all possible values of $x_t$.\nThe derivative of $F(w_t, x_t)$ with respect to $w_t$ is:\n$$\n\\frac{\\partial F}{\\partial w_t} = 1 - \\eta \\theta_0 x_t^2 + 2 \\eta x_t^3 w_t\n$$\nWe evaluate this derivative at the fixed point $w_t = w^\\star = 0$. This gives the time-dependent multiplier $\\lambda_t$, which governs the dynamics near the origin:\n$$\n\\lambda_t = \\left. \\frac{\\partial F}{\\partial w_t} \\right|_{w_t=0} = 1 - \\eta \\theta_0 x_t^2\n$$\nLocal asymptotic stability requires that the system contracts towards the fixed point. A sufficient condition, which prevents divergence and oscillatory instability for any valid input, is that $|\\lambda_t|  1$ for any input $x_t$ that can cause instability. If $\\lambda_t  1$ or $\\lambda_t  -1$, the magnitude of the weight will grow, leading to instability. If $\\lambda_t = -1$, the system will oscillate, which is not asymptotically stable.\n\nThe condition $|\\lambda_t|  1$ is equivalent to $-1  \\lambda_t  1$.\n$$\n-1  1 - \\eta \\theta_0 x_t^2  1\n$$\nLet us analyze the two inequalities separately:\n1.  $1 - \\eta \\theta_0 x_t^2  1 \\implies -\\eta \\theta_0 x_t^2  0$. Since $\\eta  0$, $\\theta_0  0$, and $x_t^2 \\geq 0$, this inequality holds for any non-zero input $x_t \\neq 0$. If $x_t=0$, we have $\\lambda_t=1$, which corresponds to neutral stability for that step, but does not cause divergence.\n2.  $-1  1 - \\eta \\theta_0 x_t^2 \\implies \\eta \\theta_0 x_t^2  2$.\n\nThis second inequality places a constraint on the learning rate $\\eta$. The problem states that this stability must hold uniformly for any input sequence $\\{x_t\\}$ satisfying the bound $|x_t| \\leq X$ for some given $X  0$. To ensure this, the stability condition must be met for the \"worst-case\" input, which is the one that makes satisfying the inequality most difficult. The term $\\eta \\theta_0 x_t^2$ is non-negative and is maximized when $|x_t|$ is maximal. Thus, the worst-case scenario corresponds to $x_t^2 = X^2$.\n\nWe must therefore enforce the condition for $x_t^2 = X^2$:\n$$\n\\eta \\theta_0 X^2  2\n$$\nSolving for the effective step size $\\eta$, we obtain the condition:\n$$\n\\eta  \\frac{2}{\\theta_0 X^2}\n$$\nThis strict inequality defines the set of all values of $\\eta$ for which the zero fixed point is locally asymptotically stable for any input sequence (excluding the pathological case where $x_t=0$ for all time, for which no learning can occur).\nThe problem asks for the supremum value, $\\eta_{\\max}$, of this set of permissible step sizes. The set of valid $\\eta$ is the open interval $(0, \\frac{2}{\\theta_0 X^2})$. The supremum (least upper bound) of this set is its endpoint.\n$$\n\\eta_{\\max} = \\sup \\left\\{ \\eta \\in \\mathbb{R} \\mid 0  \\eta  \\frac{2}{\\theta_0 X^2} \\right\\} = \\frac{2}{\\theta_0 X^2}\n$$\nAt the boundary value $\\eta = \\eta_{\\max}$, an input of $|x_t| = X$ would yield $\\lambda_t = -1$, leading to oscillations, not asymptotic stability. Therefore, any $\\eta$ must be strictly less than this value for uniform local asymptotic stability, and $\\eta_{\\max}$ is the supremum of such values.",
            "answer": "$$\\boxed{\\frac{2}{\\theta_0 X^2}}$$"
        }
    ]
}