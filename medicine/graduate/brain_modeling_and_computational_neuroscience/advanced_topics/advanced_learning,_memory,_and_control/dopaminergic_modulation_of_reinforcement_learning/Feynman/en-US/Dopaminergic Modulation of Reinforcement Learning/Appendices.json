{
    "hands_on_practices": [
        {
            "introduction": "The core idea linking dopamine to reinforcement learning is that phasic dopamine bursts encode a reward prediction error (RPE). This exercise provides a foundational, step-by-step walkthrough of the Temporal Difference (TD) learning algorithm, which formalizes this concept. By manually calculating the RPE, denoted as $\\delta_t$, and updating state values through simulated episodes, you will gain a concrete understanding of how an agent learns to predict future rewards and how prediction errors drive this learning process from scratch .",
            "id": "3975958",
            "problem": "Consider a Markov Decision Process (MDP) with three nonterminal states $s_{0}$, $s_{1}$, and $s_{2}$ that deterministically transition in a chain $s_{0} \\rightarrow s_{1} \\rightarrow s_{2} \\rightarrow \\text{terminal}$. Rewards are zero on all transitions except the final transition into the terminal state, which yields reward $r=1$. Use the convention that the value of the terminal state is zero. Let the discount factor be $\\gamma = 0.9$ and the step size (learning rate) be $\\alpha = 0.1$. Initialize the value function to $V(s) = 0$ for all nonterminal states.\n\nUnder Temporal Difference learning with zero-step bootstrapping (TD(0)), the reward prediction error (RPE), identified with phasic dopamine activity in computational neuroscience, is defined at time $t$ as $\\delta_{t} = r_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})$, and the value update is $V(s_{t}) \\leftarrow V(s_{t}) + \\alpha \\delta_{t}$.\n\nSimulate two complete episodes starting from $s_{0}$ and proceeding to the terminal state. Compute the sequence of $\\delta_{t}$ values and corresponding $V$ updates at each time step in each episode. Finally, report the value estimate $V(s_{2})$ after the second episode. Provide your final answer as a single real number with no units. No rounding is required.",
            "solution": "The user wants to find the value of state $s_2$ after two episodes of Temporal Difference (TD) learning.\n\n### Problem Validation\nThe problem statement provides a well-defined Markov Decision Process (MDP) with a deterministic state-transition chain ($s_{0} \\rightarrow s_{1} \\rightarrow s_{2} \\rightarrow \\text{terminal}$), specified rewards, a discount factor $\\gamma = 0.9$, and a learning rate $\\alpha = 0.1$. The initial value function is given as $V(s) = 0$ for all nonterminal states, and $V(\\text{terminal}) = 0$. The update rules for the value function $V(s)$ and the reward prediction error $\\delta_t$ are explicitly defined according to the TD(0) algorithm. The problem is self-contained, scientifically sound within the framework of reinforcement learning, and mathematically well-posed. There are no contradictions, ambiguities, or missing information. The problem is valid.\n\n### Solution Derivation\nWe will simulate the learning process for two full episodes, starting from state $s_0$ in each episode. An episode is a sequence of transitions from $s_0$ to the terminal state. The value function is updated at each time step.\n\nThe TD(0) update rules are:\nReward Prediction Error (RPE): $\\delta_{t} = r_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})$\nValue update: $V(s_{t}) \\leftarrow V(s_{t}) + \\alpha \\delta_{t}$\n\nThe given parameters are:\nDiscount factor: $\\gamma = 0.9$\nLearning rate: $\\alpha = 0.1$\n\nInitial state of the value function:\n$V_0(s_0) = 0$\n$V_0(s_1) = 0$\n$V_0(s_2) = 0$\n$V_0(\\text{terminal}) = 0$\n\n**Episode 1**\nThe episode trajectory is $s_0 \\rightarrow s_1 \\rightarrow s_2 \\rightarrow \\text{terminal}$.\n\n**Step 1: Transition $s_0 \\rightarrow s_1$**\n- Time step $t=0$: state $s_t = s_0$, next state $s_{t+1} = s_1$.\n- Reward $r_{t+1} = 0$.\n- The RPE $\\delta_0$ is calculated using the initial value function $V_0$:\n$$\n\\delta_0 = r_{1} + \\gamma V_0(s_1) - V_0(s_0) = 0 + 0.9 \\times 0 - 0 = 0\n$$\n- The value of state $s_0$ is updated:\n$$\nV(s_0) \\leftarrow V_0(s_0) + \\alpha \\delta_0 = 0 + 0.1 \\times 0 = 0\n$$\n- The value function is now: $V(s_0)=0, V(s_1)=0, V(s_2)=0$.\n\n**Step 2: Transition $s_1 \\rightarrow s_2$**\n- Time step $t=1$: state $s_t = s_1$, next state $s_{t+1} = s_2$.\n- Reward $r_{t+1} = 0$.\n- The RPE $\\delta_1$ is calculated using the current value function:\n$$\n\\delta_1 = r_{2} + \\gamma V(s_2) - V(s_1) = 0 + 0.9 \\times 0 - 0 = 0\n$$\n- The value of state $s_1$ is updated:\n$$\nV(s_1) \\leftarrow V(s_1) + \\alpha \\delta_1 = 0 + 0.1 \\times 0 = 0\n$$\n- The value function is now: $V(s_0)=0, V(s_1)=0, V(s_2)=0$.\n\n**Step 3: Transition $s_2 \\rightarrow \\text{terminal}$**\n- Time step $t=2$: state $s_t = s_2$, next state $s_{t+1} = \\text{terminal}$.\n- Reward $r_{t+1} = 1$.\n- The RPE $\\delta_2$ is calculated using the current value of $V(s_2)$ and the given value of $V(\\text{terminal})$:\n$$\n\\delta_2 = r_{3} + \\gamma V(\\text{terminal}) - V(s_2) = 1 + 0.9 \\times 0 - 0 = 1\n$$\n- The value of state $s_2$ is updated:\n$$\nV(s_2) \\leftarrow V(s_2) + \\alpha \\delta_2 = 0 + 0.1 \\times 1 = 0.1\n$$\n\nAt the end of Episode 1, the value function, denoted $V_1$, is:\n$V_1(s_0) = 0$\n$V_1(s_1) = 0$\n$V_1(s_2) = 0.1$\n\n**Episode 2**\nThe second episode starts with the value function $V_1$. The trajectory is again $s_0 \\rightarrow s_1 \\rightarrow s_2 \\rightarrow \\text{terminal}$.\n\n**Step 1: Transition $s_0 \\rightarrow s_1$**\n- Time step $t=0$: state $s_t = s_0$, next state $s_{t+1} = s_1$.\n- Reward $r_{t+1} = 0$.\n- The RPE $\\delta_0$ is calculated using the value function $V_1$:\n$$\n\\delta_0 = r_{1} + \\gamma V_1(s_1) - V_1(s_0) = 0 + 0.9 \\times 0 - 0 = 0\n$$\n- The value of state $s_0$ is updated:\n$$\nV(s_0) \\leftarrow V_1(s_0) + \\alpha \\delta_0 = 0 + 0.1 \\times 0 = 0\n$$\n- The value function is now: $V(s_0)=0, V(s_1)=0, V(s_2)=0.1$.\n\n**Step 2: Transition $s_1 \\rightarrow s_2$**\n- Time step $t=1$: state $s_t = s_1$, next state $s_{t+1} = s_2$.\n- Reward $r_{t+1} = 0$.\n- The RPE $\\delta_1$ is calculated using the current value function (which still has $V(s_1)=0$ and $V(s_2)=0.1$ from the end of the last episode):\n$$\n\\delta_1 = r_{2} + \\gamma V(s_2) - V(s_1) = 0 + 0.9 \\times 0.1 - 0 = 0.09\n$$\n- The value of state $s_1$ is updated:\n$$\nV(s_1) \\leftarrow V(s_1) + \\alpha \\delta_1 = 0 + 0.1 \\times 0.09 = 0.009\n$$\n- The value function is now: $V(s_0)=0, V(s_1)=0.009, V(s_2)=0.1$.\n\n**Step 3: Transition $s_2 \\rightarrow \\text{terminal}$**\n- Time step $t=2$: state $s_t = s_2$, next state $s_{t+1} = \\text{terminal}$.\n- Reward $r_{t+1} = 1$.\n- The RPE $\\delta_2$ is calculated using the current value of $V(s_2)$, which is still $0.1$:\n$$\n\\delta_2 = r_{3} + \\gamma V(\\text{terminal}) - V(s_2) = 1 + 0.9 \\times 0 - 0.1 = 0.9\n$$\n- The value of state $s_2$ is updated:\n$$\nV(s_2) \\leftarrow V(s_2) + \\alpha \\delta_2 = 0.1 + 0.1 \\times 0.9 = 0.1 + 0.09 = 0.19\n$$\n\nAfter the second complete episode, the final value of $V(s_2)$ is $0.19$.\nThe final value function is:\n$V_2(s_0) = 0$\n$V_2(s_1) = 0.009$\n$V_2(s_2) = 0.19$\n\nThe problem asks for the value estimate $V(s_2)$ after the second episode.",
            "answer": "$$\\boxed{0.19}$$"
        },
        {
            "introduction": "How can an abstract signal like the reward prediction error be implemented at the level of a single synapse? This problem explores the leading hypothesis: a three-factor learning rule where synaptic plasticity requires the conjunction of pre-synaptic activity, post-synaptic activity, and a neuromodulatory signal. Here, you will model this by calculating a synaptic eligibility trace, $e(t)$, and seeing how its interaction with a delayed dopamine-like pulse, $R(t)$, determines the final change in synaptic weight, $\\Delta w$ . This practice provides a crucial bridge from algorithmic reinforcement learning to biophysically plausible synaptic mechanisms.",
            "id": "3976000",
            "problem": "A core ingredient of three-factor synaptic learning rules in computational reinforcement learning is an eligibility trace that accumulates at pre-synaptic spike times and decays in between, which is then gated by a neuromodulatory signal such as dopamine. Consider a single synapse receiving a pre-synaptic spike train defined by the Dirac delta function $\\delta(t)$ as $s(t) = \\delta(t - t_{1}) + \\delta(t - t_{2})$, with spike times $t_{1} = 0.012$ s and $t_{2} = 0.058$ s. The eligibility trace $e(t)$ is modeled as a linear decaying memory variable obeying the ordinary differential equation $\\frac{d e(t)}{d t} = -\\frac{1}{\\tau_{e}} e(t) + \\alpha\\, s(t)$ with initial condition $e(0) = 0$, where the decay constant is $\\tau_{e} = 0.040$ s and the increment amplitude is $\\alpha = 1$. A dopamine concentration pulse $R(t)$ is triggered at time $t_{d} = 0.030$ s and modeled as a causal exponential $R(t) = R_{0}\\, \\exp\\!\\big( -\\frac{t - t_{d}}{\\tau_{R}} \\big)\\, u(t - t_{d})$, where $u(t)$ is the Heaviside unit step function, the amplitude is $R_{0} = 0.8$, and the decay constant is $\\tau_{R} = 0.050$ s. The synaptic weight update follows the three-factor rule $\\Delta w = \\eta \\int_{0}^{\\infty} R(t)\\, e(t)\\, dt$, where the learning rate is $\\eta = 0.25$.\n\nStarting from the stated dynamical definition of the eligibility trace and the given form of the dopamine pulse, first derive $e(t)$ explicitly for all $t \\ge 0$, and then compute the synaptic update $\\Delta w$ in closed form. Express your final answer for $\\Delta w$ as a single exact analytic expression using the natural exponential function, without any numerical approximation. No units are required in your final answer.",
            "solution": "The problem statement is scientifically sound, well-posed, and provides a complete and consistent set of definitions and parameters. It represents a standard modeling problem in computational neuroscience, requiring the solution of a linear ordinary differential equation and the computation of a definite integral. Therefore, the problem is valid, and a solution will be provided.\n\nThe solution process consists of two primary parts: first, deriving the explicit time-dependent expression for the eligibility trace $e(t)$, and second, calculating the total synaptic weight update $\\Delta w$ by integrating the product of the eligibility trace and the dopamine signal.\n\n**Part 1: Derivation of the Eligibility Trace $e(t)$**\n\nThe eligibility trace $e(t)$ is governed by the first-order linear ordinary differential equation:\n$$ \\frac{d e(t)}{d t} + \\frac{1}{\\tau_{e}} e(t) = \\alpha\\, s(t) $$\nwith the initial condition $e(0) = 0$. The input forcing term is the pre-synaptic spike train $s(t) = \\delta(t - t_{1}) + \\delta(t - t_{2})$, where $\\delta(t)$ is the Dirac delta function.\n\nThe solution to this type of differential equation can be found using the method of variation of parameters or by convolving the input with the system's impulse response. The solution for a general forcing term $f(t)$ and initial condition $e(0)=0$ is:\n$$ e(t) = \\int_{0}^{t} \\exp\\left(-\\frac{t-t'}{\\tau_e}\\right) f(t') dt' $$\nIn our case, $f(t) = \\alpha s(t)$. Substituting $s(t)$ yields:\n$$ e(t) = \\alpha \\int_{0}^{t} \\exp\\left(-\\frac{t-t'}{\\tau_e}\\right) \\left[ \\delta(t' - t_{1}) + \\delta(t' - t_{2}) \\right] dt' $$\nBy the sifting property of the Dirac delta function, which states that $\\int g(x) \\delta(x-c) dx = g(c)$, we can evaluate the integral for each delta function term. The result is a superposition of exponential decay functions, each initiated at a spike time. Using the Heaviside step function $u(t)$ to denote causality, the expression for $e(t)$ is:\n$$ e(t) = \\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) u(t - t_1) + \\alpha \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right) u(t - t_2) $$\nGiven the spike times $t_{1} = 0.012$ and $t_{2} = 0.058$, we can write $e(t)$ in a piecewise form for $t \\ge 0$:\n$$ e(t) = \\begin{cases} 0 & \\text{for } 0 \\le t < t_1 \\\\ \\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) & \\text{for } t_1 \\le t < t_2 \\\\ \\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) + \\alpha \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right) & \\text{for } t \\ge t_2 \\end{cases} $$\n\n**Part 2: Computation of the Synaptic Weight Update $\\Delta w$**\n\nThe synaptic weight update $\\Delta w$ is defined by the three-factor learning rule:\n$$ \\Delta w = \\eta \\int_{0}^{\\infty} R(t)\\, e(t)\\, dt $$\nThe dopamine signal is given as $R(t) = R_{0}\\, \\exp\\left( -\\frac{t - t_{d}}{\\tau_{R}} \\right)\\, u(t - t_{d})$. Since $R(t)=0$ for $t < t_d$, the effective lower limit of integration is $t_d$. The given time parameters are $t_1 = 0.012$, $t_d = 0.030$, and $t_2 = 0.058$, which are ordered as $t_1 < t_d < t_2$.\n\nWe must split the integral at $t_2$ because the functional form of $e(t)$ changes at this point:\n$$ \\Delta w = \\eta \\int_{t_d}^{\\infty} R(t) e(t) dt = \\eta \\left[ \\int_{t_d}^{t_2} R(t) e(t) dt + \\int_{t_2}^{\\infty} R(t) e(t) dt \\right] $$\nSubstituting the respective piecewise expressions for $e(t)$:\n$$ \\Delta w = \\eta \\left[ \\int_{t_d}^{t_2} R(t) \\left(\\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right)\\right) dt + \\int_{t_2}^{\\infty} R(t) \\left(\\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) + \\alpha \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right)\\right) dt \\right] $$\nThis expression can be simplified by regrouping the terms corresponding to each spike's contribution to the eligibility trace:\n$$ \\Delta w = \\eta \\alpha \\left[ \\int_{t_d}^{\\infty} R(t) \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) dt + \\int_{t_2}^{\\infty} R(t) \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right) dt \\right] $$\nLet's define these two integrals as $I_1$ and $I_2$:\n$I_1 = \\int_{t_d}^{\\infty} R_0 \\exp\\left(-\\frac{t-t_d}{\\tau_R}\\right) \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) dt$\n$I_2 = \\int_{t_2}^{\\infty} R_0 \\exp\\left(-\\frac{t-t_d}{\\tau_R}\\right) \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right) dt$\n\nFor $I_1$:\n$$ I_1 = R_0 \\exp\\left(\\frac{t_d}{\\tau_R} + \\frac{t_1}{\\tau_e}\\right) \\int_{t_d}^{\\infty} \\exp\\left(-t\\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right)\\right) dt $$\nLet the effective decay time constant be $\\tau_{eff} = \\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right)^{-1} = \\frac{\\tau_R \\tau_e}{\\tau_R + \\tau_e}$. The integral is:\n$$ \\int_{t_d}^{\\infty} \\exp\\left(-\\frac{t}{\\tau_{eff}}\\right) dt = \\left[-\\tau_{eff} \\exp\\left(-\\frac{t}{\\tau_{eff}}\\right)\\right]_{t_d}^{\\infty} = \\tau_{eff} \\exp\\left(-\\frac{t_d}{\\tau_{eff}}\\right) $$\nSubstituting this back, the exponent of the final expression for $I_1$ becomes $\\frac{t_d}{\\tau_R} + \\frac{t_1}{\\tau_e} - \\frac{t_d}{\\tau_{eff}} = \\frac{t_d}{\\tau_R} + \\frac{t_1}{\\tau_e} - t_d\\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right) = \\frac{t_1 - t_d}{\\tau_e}$.\nSo, $I_1 = R_0 \\tau_{eff} \\exp\\left(\\frac{t_1 - t_d}{\\tau_e}\\right)$.\n\nFor $I_2$:\n$$ I_2 = R_0 \\exp\\left(\\frac{t_d}{\\tau_R} + \\frac{t_2}{\\tau_e}\\right) \\int_{t_2}^{\\infty} \\exp\\left(-\\frac{t}{\\tau_{eff}}\\right) dt $$\nThe integral evaluates to $\\tau_{eff} \\exp\\left(-\\frac{t_2}{\\tau_{eff}}\\right)$. The exponent for $I_2$ becomes $\\frac{t_d}{\\tau_R} + \\frac{t_2}{\\tau_e} - \\frac{t_2}{\\tau_{eff}} = \\frac{t_d}{\\tau_R} + \\frac{t_2}{\\tau_e} - t_2\\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right) = \\frac{t_d - t_2}{\\tau_R}$.\nSo, $I_2 = R_0 \\tau_{eff} \\exp\\left(\\frac{t_d - t_2}{\\tau_R}\\right)$.\n\nThe total weight update is:\n$$ \\Delta w = \\eta \\alpha (I_1 + I_2) = \\eta \\alpha R_0 \\frac{\\tau_R \\tau_e}{\\tau_R + \\tau_e} \\left[ \\exp\\left(\\frac{t_1 - t_d}{\\tau_e}\\right) + \\exp\\left(\\frac{t_d - t_2}{\\tau_R}\\right) \\right] $$\nThis is the closed-form analytical result. Now, we substitute the provided numerical values:\n$\\eta = 0.25$, $\\alpha = 1$, $R_0 = 0.8$, $\\tau_e = 0.040$, $\\tau_R = 0.050$, $t_1 = 0.012$, $t_d = 0.030$, $t_2 = 0.058$.\n\nThe prefactor is:\n$$ \\eta \\alpha R_0 \\frac{\\tau_R \\tau_e}{\\tau_R + \\tau_e} = (0.25)(1)(0.8) \\frac{(0.050)(0.040)}{0.050 + 0.040} = (0.2) \\frac{0.002}{0.09} = \\frac{1}{5} \\cdot \\frac{2}{90} = \\frac{1}{5} \\cdot \\frac{1}{45} = \\frac{1}{225} $$\nThe arguments of the exponential functions are:\n$$ \\frac{t_1 - t_d}{\\tau_e} = \\frac{0.012 - 0.030}{0.040} = \\frac{-0.018}{0.040} = -\\frac{18}{40} = -\\frac{9}{20} $$\n$$ \\frac{t_d - t_2}{\\tau_R} = \\frac{0.030 - 0.058}{0.050} = \\frac{-0.028}{0.050} = -\\frac{28}{50} = -\\frac{14}{25} $$\nSubstituting these values gives the final exact expression for $\\Delta w$.\n$$ \\Delta w = \\frac{1}{225} \\left( \\exp\\left(-\\frac{9}{20}\\right) + \\exp\\left(-\\frac{14}{25}\\right) \\right) $$",
            "answer": "$$\\boxed{\\frac{1}{225} \\left( \\exp\\left(-\\frac{9}{20}\\right) + \\exp\\left(-\\frac{14}{25}\\right) \\right)}$$"
        },
        {
            "introduction": "Beyond simply learning the value of states, an agent must learn how to act. The actor-critic architecture provides a powerful framework for this, separating the problem into learning a policy (the 'actor') and learning state values (the 'critic'). This advanced exercise demonstrates how a single dopaminergic RPE signal, $\\delta_t$, can simultaneously guide the learning of both components. You will derive and compute the concurrent updates for both the actor and critic parameters, illustrating how a dopamine-like signal can effectively orchestrate both value and policy improvements in a unified system .",
            "id": "3975960",
            "problem": "Consider an agent modeled within Reinforcement Learning (RL) that uses an actor-critic architecture. The actor implements a softmax policy over actions with inverse temperature $\\,\\beta\\,$ and linear action preferences, and the critic implements a linear value function over state features. Phasic dopamine is modeled as a scalar burst that gates plasticity multiplicatively. Use only the following foundational definitions to derive the concurrent one-step updates for the actor and critic parameters on a single transition:\n- The softmax policy with linear preferences: $\\pi(a \\mid s) = \\dfrac{\\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_a(s)\\big)}{\\sum_{a'} \\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_{a'}(s)\\big)}$, where $\\boldsymbol{\\theta}$ are the policy parameters and $\\boldsymbol{\\phi}_a(s)$ are the action-specific features at state $s$.\n- The value function approximation: $V_{\\boldsymbol{w}}(s) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s)$, where $\\boldsymbol{w}$ are the value parameters and $\\boldsymbol{x}(s)$ are the state features.\n- The one-step Temporal Difference (TD) reward prediction error: $\\delta_t = r_t + \\gamma\\, V_{\\boldsymbol{w}}(s_{t+1}) - V_{\\boldsymbol{w}}(s_t)$.\n- Dopamine gating function: $g(D) = \\dfrac{1}{1 + \\exp\\!\\big(-k(D - D_0)\\big)}$, which multiplicatively gates plasticity in both actor and critic updates.\n\nA single transition $(s_t, a_t, r_t, s_{t+1})$ is observed with the following quantities specified:\n- Inverse temperature $\\,\\beta = 1\\,$.\n- Policy parameters $\\,\\boldsymbol{\\theta} = \\begin{pmatrix} \\tfrac{1}{2} \\\\[4pt] -\\tfrac{1}{2} \\end{pmatrix}\\,$.\n- Action feature vectors at $\\,s_t\\,$: $\\boldsymbol{\\phi}_1(s_t) = \\begin{pmatrix} 1 \\\\[2pt] 0 \\end{pmatrix}$ and $\\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} 0 \\\\[2pt] 1 \\end{pmatrix}$.\n- Value parameters $\\,\\boldsymbol{w} = \\begin{pmatrix} \\tfrac{1}{10} \\\\[4pt] \\tfrac{2}{10} \\end{pmatrix}\\,$.\n- State feature vectors: $\\boldsymbol{x}(s_t) = \\begin{pmatrix} 1 \\\\[2pt] 1 \\end{pmatrix}$ and $\\boldsymbol{x}(s_{t+1}) = \\begin{pmatrix} 0 \\\\[2pt] 2 \\end{pmatrix}$.\n- Reward $\\,r_t = \\tfrac{1}{2}\\,$ and discount factor $\\,\\gamma = \\tfrac{9}{10}\\,$.\n- The action sampled at $\\,s_t\\,$ is $\\,a_t = 1\\,$.\n- Dopamine burst amplitude $\\,D = \\tfrac{6}{5}\\,$ with gating parameters $\\,k = 5\\,$ and $\\,D_0 = \\tfrac{4}{5}\\,$, so that $\\,g(D) = \\dfrac{1}{1 + \\exp\\!\\big(-k(D - D_0)\\big)}\\,$.\n- Actor learning rate $\\,\\alpha_a = \\tfrac{1}{10}\\,$ and critic learning rate $\\,\\alpha_c = \\tfrac{1}{5}\\,$.\n\nStarting only from these definitions, derive the concurrent one-step parameter updates $\\,\\Delta \\boldsymbol{\\theta}\\,$ and $\\,\\Delta \\boldsymbol{w}\\,$ for this single transition, under the assumption that phasic dopamine gates both updates multiplicatively via $\\,g(D)\\,$ and that both updates are scaled by the TD error $\\,\\delta_t\\,$. Express your final answer as a single row matrix listing the four components $\\,\\Delta \\theta_1, \\Delta \\theta_2, \\Delta w_1, \\Delta w_2\\,$ in exact symbolic form using $\\,\\exp(\\cdot)\\,$. No rounding is required.",
            "solution": "The problem will be validated by first extracting the given information and then assessing its scientific grounding, consistency, and formal structure.\n\n### Step 1: Extract Givens\n-   **Softmax policy with linear preferences**: $\\pi(a \\mid s) = \\dfrac{\\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_a(s)\\big)}{\\sum_{a'} \\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_{a'}(s)\\big)}$\n-   **Value function approximation**: $V_{\\boldsymbol{w}}(s) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s)$\n-   **One-step Temporal Difference (TD) error**: $\\delta_t = r_t + \\gamma\\, V_{\\boldsymbol{w}}(s_{t+1}) - V_{\\boldsymbol{w}}(s_t)$\n-   **Dopamine gating function**: $g(D) = \\dfrac{1}{1 + \\exp\\!\\big(-k(D - D_0)\\big)}$\n-   **Assumption**: Phasic dopamine gates both actor and critic updates multiplicatively via $g(D)$, and both updates are scaled by the TD error $\\delta_t$.\n-   **Inverse temperature**: $\\beta = 1$\n-   **Policy parameters**: $\\boldsymbol{\\theta} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\end{pmatrix}$\n-   **Action feature vectors at $s_t$**: $\\boldsymbol{\\phi}_1(s_t) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $\\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n-   **Value parameters**: $\\boldsymbol{w} = \\begin{pmatrix} \\tfrac{1}{10} \\\\ \\tfrac{2}{10} \\end{pmatrix}$\n-   **State feature vectors**: $\\boldsymbol{x}(s_t) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $\\boldsymbol{x}(s_{t+1}) = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$\n-   **Reward**: $r_t = \\tfrac{1}{2}$\n-   **Discount factor**: $\\gamma = \\tfrac{9}{10}$\n-   **Action taken**: $a_t = 1$\n-   **Dopamine parameters**: $D = \\tfrac{6}{5}$, $k = 5$, $D_0 = \\tfrac{4}{5}$\n-   **Learning rates**: $\\alpha_a = \\tfrac{1}{10}$ (actor), $\\alpha_c = \\tfrac{1}{5}$ (critic)\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded within the fields of computational neuroscience and machine learning, specifically reinforcement learning (RL). All definitions provided (actor-critic architecture, softmax policy, linear function approximation, TD error) are standard concepts in RL. The modeling of dopamine as a multiplicative gate for synaptic plasticity is a well-established hypothesis in computational neuroscience. The problem is well-posed, providing all necessary parameters and functional forms to compute a unique solution. The language is objective and precise. The data are numerically and dimensionally consistent. The problem is not trivial, as it requires the systematic application of several theoretical concepts. It is not overly metaphorical and is directly related to the specified topic. All conditions are met.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Solution\n\nThe goal is to find the one-step parameter updates $\\Delta \\boldsymbol{\\theta}$ and $\\Delta \\boldsymbol{w}$. The problem specifies that both updates are scaled by the TD error $\\delta_t$ and the dopamine gating function $g(D)$. The updates follow the principle of stochastic gradient ascent/descent.\n\n**1. Critic Update Rule**\nThe critic's parameters $\\boldsymbol{w}$ are updated to minimize the TD error. This is typically done by gradient descent on the squared TD error, which for a single sample leads to an update proportional to the TD error and the gradient of the value function. For a linear approximator $V_{\\boldsymbol{w}}(s) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s)$, the gradient with respect to $\\boldsymbol{w}$ is $\\nabla_{\\boldsymbol{w}} V_{\\boldsymbol{w}}(s_t) = \\boldsymbol{x}(s_t)$. The update rule is:\n$$ \\Delta \\boldsymbol{w} = \\alpha_c \\, g(D) \\, \\delta_t \\, \\nabla_{\\boldsymbol{w}} V_{\\boldsymbol{w}}(s_t) = \\alpha_c \\, g(D) \\, \\delta_t \\, \\boldsymbol{x}(s_t) $$\n\n**2. Actor Update Rule**\nThe actor's parameters $\\boldsymbol{\\theta}$ are updated using a policy gradient method to increase the probability of actions that lead to positive TD error. The update is proportional to the gradient of the log-policy, scaled by the TD error. This is known as an actor-critic algorithm. The update rule is:\n$$ \\Delta \\boldsymbol{\\theta} = \\alpha_a \\, g(D) \\, \\delta_t \\, \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a_t \\mid s_t) $$\nThe gradient of the log-policy for a softmax function is:\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a \\mid s) = \\beta \\left( \\boldsymbol{\\phi}_a(s) - \\sum_{a'} \\pi(a' \\mid s) \\boldsymbol{\\phi}_{a'}(s) \\right) = \\beta \\left( \\boldsymbol{\\phi}_a(s) - \\mathbb{E}_{a' \\sim \\pi(\\cdot \\mid s)}[\\boldsymbol{\\phi}_{a'}(s)] \\right) $$\n\nNow, we compute the required quantities step-by-step.\n\n**A. Compute the TD Error $\\delta_t$**\nFirst, we calculate the value estimates for states $s_t$ and $s_{t+1}$.\n$$ V_{\\boldsymbol{w}}(s_t) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s_t) = \\begin{pmatrix} \\frac{1}{10} & \\frac{2}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{10} \\cdot 1 + \\frac{2}{10} \\cdot 1 = \\frac{3}{10} $$\n$$ V_{\\boldsymbol{w}}(s_{t+1}) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s_{t+1}) = \\begin{pmatrix} \\frac{1}{10} & \\frac{2}{10} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\frac{1}{10} \\cdot 0 + \\frac{2}{10} \\cdot 2 = \\frac{4}{10} $$\nNow, we compute the TD error $\\delta_t = r_t + \\gamma V_{\\boldsymbol{w}}(s_{t+1}) - V_{\\boldsymbol{w}}(s_t)$.\n$$ \\delta_t = \\frac{1}{2} + \\frac{9}{10} \\cdot \\frac{4}{10} - \\frac{3}{10} = \\frac{5}{10} + \\frac{36}{100} - \\frac{30}{100} = \\frac{50+36-30}{100} = \\frac{56}{100} = \\frac{14}{25} $$\n\n**B. Compute the Dopamine Gating Term $g(D)$**\nThe gating term is $g(D) = \\frac{1}{1 + \\exp(-k(D - D_0))}$.\nThe argument of the exponential is:\n$$ k(D - D_0) = 5 \\left( \\frac{6}{5} - \\frac{4}{5} \\right) = 5 \\left( \\frac{2}{5} \\right) = 2 $$\nThus, the gating term is:\n$$ g(D) = \\frac{1}{1 + \\exp(-2)} $$\nFor algebraic cleanliness, we can write this as $g(D) = \\frac{\\exp(2)}{\\exp(2)+1}$.\n\n**C. Compute the Critic Parameter Update $\\Delta \\boldsymbol{w}$**\nUsing the derived formula $\\Delta \\boldsymbol{w} = \\alpha_c \\, g(D) \\, \\delta_t \\, \\boldsymbol{x}(s_t)$:\n$$ \\Delta \\boldsymbol{w} = \\frac{1}{5} \\cdot \\frac{1}{1 + \\exp(-2)} \\cdot \\frac{14}{25} \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n$$ \\Delta \\boldsymbol{w} = \\frac{14}{125(1 + \\exp(-2))} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{14 \\exp(2)}{125(\\exp(2)+1)} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nThe components are:\n$$ \\Delta w_1 = \\Delta w_2 = \\frac{14 \\exp(2)}{125(\\exp(2) + 1)} $$\n\n**D. Compute the Actor Parameter Update $\\Delta \\boldsymbol{\\theta}$**\nFirst, we compute the policy gradient term $\\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a_t \\mid s_t)$. The action taken was $a_t=1$.\nWe need the action probabilities $\\pi(a' \\mid s_t)$. These depend on the action preferences $\\boldsymbol{\\theta}^{\\top}\\boldsymbol{\\phi}_{a'}(s_t)$.\nGiven $\\beta=1$:\n$$ \\text{Preference for } a=1: \\quad p_1 = \\boldsymbol{\\theta}^{\\top}\\boldsymbol{\\phi}_1(s_t) = \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{2} $$\n$$ \\text{Preference for } a=2: \\quad p_2 = \\boldsymbol{\\theta}^{\\top}\\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = -\\frac{1}{2} $$\nThe probabilities are:\n$$ \\pi(1 \\mid s_t) = \\frac{\\exp(p_1)}{\\exp(p_1) + \\exp(p_2)} = \\frac{\\exp(1/2)}{\\exp(1/2) + \\exp(-1/2)} $$\n$$ \\pi(2 \\mid s_t) = \\frac{\\exp(p_2)}{\\exp(p_1) + \\exp(p_2)} = \\frac{\\exp(-1/2)}{\\exp(1/2) + \\exp(-1/2)} $$\nLet's simplify $\\pi(2 \\mid s_t)$ by multiplying the numerator and denominator by $\\exp(1/2)$:\n$$ \\pi(2 \\mid s_t) = \\frac{\\exp(-1/2)\\exp(1/2)}{(\\exp(1/2) + \\exp(-1/2))\\exp(1/2)} = \\frac{\\exp(0)}{\\exp(1) + \\exp(0)} = \\frac{1}{\\exp(1) + 1} $$\nThe policy gradient for $a_t=1$ is $\\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = \\beta (\\boldsymbol{\\phi}_1(s_t) - [\\pi(1|s_t)\\boldsymbol{\\phi}_1(s_t) + \\pi(2|s_t)\\boldsymbol{\\phi}_2(s_t)])$. With $\\beta=1$:\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = (1 - \\pi(1 \\mid s_t))\\boldsymbol{\\phi}_1(s_t) - \\pi(2 \\mid s_t)\\boldsymbol{\\phi}_2(s_t) $$\nSince $1 - \\pi(1 \\mid s_t) = \\pi(2 \\mid s_t)$, this simplifies to:\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = \\pi(2 \\mid s_t) (\\boldsymbol{\\phi}_1(s_t) - \\boldsymbol{\\phi}_2(s_t)) $$\n$$ \\boldsymbol{\\phi}_1(s_t) - \\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nSo the policy gradient term is:\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = \\frac{1}{\\exp(1) + 1} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nNow we assemble the actor update $\\Delta \\boldsymbol{\\theta} = \\alpha_a \\, g(D) \\, \\delta_t \\, \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a_t \\mid s_t)$:\n$$ \\Delta \\boldsymbol{\\theta} = \\frac{1}{10} \\cdot \\frac{1}{1 + \\exp(-2)} \\cdot \\frac{14}{25} \\cdot \\frac{1}{\\exp(1) + 1} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n$$ \\Delta \\boldsymbol{\\theta} = \\frac{14}{250(1 + \\exp(-2))(\\exp(1) + 1)} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{7}{125(1 + \\exp(-2))(\\exp(1) + 1)} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nUsing the identity $1/(1+\\exp(-2)) = \\exp(2)/(\\exp(2)+1)$:\n$$ \\Delta \\boldsymbol{\\theta} = \\frac{7 \\exp(2)}{125(\\exp(2)+1)(\\exp(1) + 1)} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nThe components are:\n$$ \\Delta \\theta_1 = \\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n$$ \\Delta \\theta_2 = -\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n\n**Final Answer Assembly**\nThe final answer is a row matrix containing the four components: $\\Delta\\theta_1$, $\\Delta\\theta_2$, $\\Delta w_1$, $\\Delta w_2$.\n$$ \\Delta \\theta_1 = \\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n$$ \\Delta \\theta_2 = -\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n$$ \\Delta w_1 = \\frac{14 \\exp(2)}{125(\\exp(2) + 1)} $$\n$$ \\Delta w_2 = \\frac{14 \\exp(2)}{125(\\exp(2) + 1)} $$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} & -\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} & \\frac{14 \\exp(2)}{125(\\exp(2) + 1)} & \\frac{14 \\exp(2)}{125(\\exp(2) + 1)}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}