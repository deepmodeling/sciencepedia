{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握时序差分学习，必须熟练其核心计算步骤。本练习将引导你对一个给定的经验序列，一步步地应用 TD(0) 更新规则 。通过亲手计算时序差分误差 $\\delta_t$ 并更新状态价值，你将对智能体如何根据其预测与实际结果之间的差异进行学习，建立起具体而深入的直观理解。",
            "id": "4026748",
            "problem": "考虑一个确定性马尔可夫奖励过程，其状态为 $s_A$、$s_B$、$s_C$ 和一个吸收终止状态 $s_T$。价值函数 $V(s)$ 初始化为 $V(s_A)=0.2$，$V(s_B)=0.1$，$V(s_C)=-0.1$ 和 $V(s_T)=0$。单个回合生成了带有立即奖励的转移轨迹：$s_A \\rightarrow s_B$ 奖励为 $0$，$s_B \\rightarrow s_C$ 奖励为 $1$，$s_C \\rightarrow s_B$ 奖励为 $-0.5$，$s_B \\rightarrow s_T$ 奖励为 $2$。假设折扣因子 $\\gamma=0.8$，固定学习率 $\\alpha=0.5$。终止状态值 $V(s_T)$ 保持固定为 $0$，且从不更新。\n\n使用时间差分（TD）学习框架，采用单步自举（通常表示为 TD($0$)），并利用定义为所获单步回报与当前预测之间差异的奖励预测误差概念，沿着观测到的轨迹顺序执行原地更新，仅更新刚刚访问过的状态的价值估计。报告第四次更新后 $V(s_B)$ 的数值。请将最终答案表示为一个不带单位的纯数字。本题没有四舍五入要求。",
            "solution": "用户想要使用时间差分（TD）学习算法，具体来说是 TD(0)，找到经过一系列更新后 $V(s_B)$ 的值。\n\n首先，对问题进行验证。\n**步骤 1：提取已知条件**\n- 状态：$s_A$、$s_B$、$s_C$ 和一个终止状态 $s_T$。\n- 初始价值函数：$V(s_A)=0.2$，$V(s_B)=0.1$，$V(s_C)=-0.1$，$V(s_T)=0$。\n- 带奖励的轨迹：\n  1. $s_A \\rightarrow s_B$ 奖励为 $R_1=0$。\n  2. $s_B \\rightarrow s_C$ 奖励为 $R_2=1$。\n  3. $s_C \\rightarrow s_B$ 奖励为 $R_3=-0.5$。\n  4. $s_B \\rightarrow s_T$ 奖励为 $R_4=2$。\n- 折扣因子：$\\gamma=0.8$。\n- 学习率：$\\alpha=0.5$。\n- 约束条件：终止状态值 $V(s_T)$ 固定为 $0$，且从不更新。\n- 更新方法：原地 TD(0) 更新，更新刚刚访问过的状态的价值。\n\n**步骤 2：使用提取的已知条件进行验证**\n- 该问题具有科学依据，在强化学习和马尔可夫奖励过程的标准框架内运作。时间差分学习是该领域的基石。\n- 该问题是良定义的，提供了所有必要的参数（$\\gamma$、$\\alpha$）、初始条件（$V(s)$）以及一个具体的状态转移和奖励序列。目标明确。\n- 语言客观、精确。\n- 问题没有矛盾、科学缺陷和歧义。循环（$s_B \\rightarrow s_C \\rightarrow s_B$）的存在是马尔可夫过程的一个有效特征。\n\n**步骤 3：结论与行动**\n问题有效。将推导解答。\n\n在转移到状态 $s_{t+1}$ 并获得奖励 $R_{t+1}$ 后，状态 $s_t$ 的价值的 TD(0) 更新规则如下：\n$$V(s_t) \\leftarrow V(s_t) + \\alpha [R_{t+1} + \\gamma V(s_{t+1}) - V(s_t)]$$\n项 $\\delta_t = R_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$ 是 TD 误差，也称为奖励预测误差。项 $R_{t+1} + \\gamma V(s_{t+1})$ 是 TD 目标。更新是“原地”执行的，意味着一次更新的结果会立即用于后续计算。\n\n初始值为：\n$V(s_A) = 0.2$\n$V(s_B) = 0.1$\n$V(s_C) = -0.1$\n$V(s_T) = 0$\n常量为 $\\alpha = 0.5$ 和 $\\gamma = 0.8$。\n\n我们现在将沿着轨迹顺序执行更新。\n\n**更新 1：转移 $s_A \\rightarrow s_B$，奖励 $R_1 = 0$。**\n状态 $s_A$ 的价值被更新。我们使用当前值 $V(s_A) = 0.2$ 和 $V(s_B) = 0.1$。\nTD 目标是 $R_1 + \\gamma V(s_B) = 0 + (0.8)(0.1) = 0.08$。\nTD 误差是 $\\delta_0 = 0.08 - V(s_A) = 0.08 - 0.2 = -0.12$。\n$V(s_A)$ 的新值为：\n$V(s_A) \\leftarrow V(s_A) + \\alpha \\delta_0 = 0.2 + (0.5)(-0.12) = 0.2 - 0.06 = 0.14$。\n本次更新后，各价值为：$V(s_A) = 0.14$，$V(s_B) = 0.1$，$V(s_C) = -0.1$。\n\n**更新 2：转移 $s_B \\rightarrow s_C$，奖励 $R_2 = 1$。**\n状态 $s_B$ 的价值被更新。我们使用当前值 $V(s_B) = 0.1$ 和 $V(s_C) = -0.1$。\nTD 目标是 $R_2 + \\gamma V(s_C) = 1 + (0.8)(-0.1) = 1 - 0.08 = 0.92$。\nTD 误差是 $\\delta_1 = 0.92 - V(s_B) = 0.92 - 0.1 = 0.82$。\n$V(s_B)$ 的新值为：\n$V(s_B) \\leftarrow V(s_B) + \\alpha \\delta_1 = 0.1 + (0.5)(0.82) = 0.1 + 0.41 = 0.51$。\n本次更新后，各价值为：$V(s_A) = 0.14$，$V(s_B) = 0.51$，$V(s_C) = -0.1$。\n\n**更新 3：转移 $s_C \\rightarrow s_B$，奖励 $R_3 = -0.5$。**\n状态 $s_C$ 的价值被更新。我们使用当前值 $V(s_C) = -0.1$ 和新更新的价值 $V(s_B) = 0.51$。\nTD 目标是 $R_3 + \\gamma V(s_B) = -0.5 + (0.8)(0.51) = -0.5 + 0.408 = -0.092$。\nTD 误差是 $\\delta_2 = -0.092 - V(s_C) = -0.092 - (-0.1) = 0.008$。\n$V(s_C)$ 的新值为：\n$V(s_C) \\leftarrow V(s_C) + \\alpha \\delta_2 = -0.1 + (0.5)(0.008) = -0.1 + 0.004 = -0.096$。\n本次更新后，各价值为：$V(s_A) = 0.14$，$V(s_B) = 0.51$，$V(s_C) = -0.096$。\n\n**更新 4：转移 $s_B \\rightarrow s_T$，奖励 $R_4 = 2$。**\n状态 $s_B$ 的价值被第二次更新。我们使用当前值 $V(s_B) = 0.51$ 和固定的终止状态值 $V(s_T) = 0$。\nTD 目标是 $R_4 + \\gamma V(s_T) = 2 + (0.8)(0) = 2$。\nTD 误差是 $\\delta_3 = 2 - V(s_B) = 2 - 0.51 = 1.49$。\n$V(s_B)$ 的新值为：\n$V(s_B) \\leftarrow V(s_B) + \\alpha \\delta_3 = 0.51 + (0.5)(1.49) = 0.51 + 0.745 = 1.255$。\n\n第四次更新后 $V(s_B)$ 的值为 $1.255$。",
            "answer": "$$\\boxed{1.255}$$"
        },
        {
            "introduction": "时序差分学习不仅是一种算法，更是一个强大的框架，可用于模拟生物系统如何从经验中学习。本练习要求你模拟经典的条件反射现象——“消退”(extinction)，即一个已建立的关联被逐渐消除的过程 。你将推导当预期奖励突然改变时，状态价值如何随时间演化，从而为持续的负向预测误差如何驱动新学习这一神经科学核心概念提供清晰的数学基础。",
            "id": "4026721",
            "problem": "给定一个最小马尔可夫奖励过程，该过程用于模拟大脑建模和计算神经科学中使用的巴甫洛夫条件反射和消退学习。存在两种状态：提示状态 $C$ 和终止状态 $T$。每次试验恰好包含 $2$ 个时间步：在时间步 $t$，智能体处于状态 $C$，在时间步 $t+1$，智能体转移到状态 $T$ 并获得一个标量奖励 $r_t$。终止状态 $T$ 是吸收状态，其价值为 $V(T) = 0$。在变化点之前，期望奖励是一个常数 $\\mu_{\\text{pre}}$。在变化点之后，期望奖励是一个常数 $\\mu_{\\text{post}}$，且 $\\mu_{\\text{post}} \\le \\mu_{\\text{pre}}$。预训练已收敛，使得在变化点时刻，状态 $C$ 的价值估计为 $V_0 = \\mu_{\\text{pre}}$。\n\n对于学习，请使用以下时间差分学习的核心定义。在时间步 $t$，状态 $s_t$ 的时间差分误差 $\\delta_t$ 定义为 $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$，其中 $\\gamma \\in [0,1)$ 是折扣因子。价值更新公式为 $V(s_t) \\leftarrow V(s_t) + \\alpha \\, \\delta_t$，其中 $\\alpha \\in [0,1]$ 是一个常数学习率。假设只有一个非终止状态 $C$，因此唯一被更新的价值是 $V(C)$，我们将其记为经过 $k$ 次变化后试验后的 $V_k$。在变化点之后，每次试验的奖励都是确定性的，即 $r_t = \\mu_{\\text{post}}$。\n\n任务：\n1. 仅使用上述定义和所描述的试验结构，推导在固定学习率 $\\alpha$ 和变化后常数奖励 $\\mu_{\\text{post}}$ 条件下，序列 $\\{V_k\\}_{k \\ge 0}$ 的闭式表达式。根据时间差分误差在各次试验中的符号和大小来解释消退学习，并确定这些误差为负的条件。\n2. 实现一个算法，给定 $V_0$、$\\mu_{\\text{post}}$、$\\alpha$ 和一个整数 $n \\ge 0$，计算 $V_n$，即经过 $n$ 次变化后试验后的价值估计。\n3. 您的实现必须使用推导出的闭式表达式或您的推导所隐含的逻辑等价计算，并且必须生成四舍五入到 $10$ 位小数的输出。\n\n测试套件：\n为以下参数集提供输出，每个参数集定义为一个元组 $(V_0, \\mu_{\\text{post}}, \\alpha, n)$：\n- 情况 A (理想路径，完全消退)：$(1.0, 0.0, 0.2, 10)$。\n- 情况 B (部分消退至较低的渐近线)：$(1.0, 0.3, 0.1, 20)$。\n- 情况 C (边界学习率，立即更新)：$(1.0, 0.0, 1.0, 3)$。\n- 情况 D (无学习的边界情况)：$(0.75, 0.0, 0.0, 50)$。\n- 情况 E (多次试验中的缓慢消退)：$(0.8, 0.0, 0.05, 100)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[result\\_A,result\\_B,result\\_C,result\\_D,result\\_E]$）。每个 $result\\_X$ 必须是相应情况下 $V_n$ 的浮点数值，四舍五入到 $10$ 位小数。不应打印其他任何文本。",
            "solution": "该问题要求推导在一个由时间差分 (TD) 学习建模的消退学习范式中，提示状态 $V(C)$ 价值的闭式表达式，并实现该表达式。\n\n首先，我们将学习过程形式化为一个递推关系。设 $V_k$ 表示变化点后经过 $k$ 次试验后提示状态 $C$ 的价值估计，其中 $k \\ge 0$。通过预训练建立的初始价值为 $V_0 = \\mu_{\\text{pre}}$。\n\n对于任何试验 $k \\ge 1$，智能体从状态 $s_t = C$ 开始，此时的当前价值估计为 $V_{k-1}$。然后，智能体转移到终止状态 $s_{t+1} = T$ 并获得奖励 $r_t = \\mu_{\\text{post}}$。终止状态的价值给定为 $V(T) = 0$。\n\n根据所提供的定义，第 $k$ 次试验的时间差分误差（我们记为 $\\delta_{k-1}$）为：\n$$\n\\delta_{k-1} = r_t + \\gamma V(s_{t+1}) - V(s_t)\n$$\n代入试验结构的给定值：\n$$\n\\delta_{k-1} = \\mu_{\\text{post}} + \\gamma(0) - V_{k-1} = \\mu_{\\text{post}} - V_{k-1}\n$$\n然后使用学习率 $\\alpha \\in [0, 1]$ 更新状态 $C$ 的价值：\n$$\nV_k \\leftarrow V_{k-1} + \\alpha \\delta_{k-1}\n$$\n这为我们提供了序列 $\\{V_k\\}$ 的以下一阶线性递推关系：\n$$\nV_k = V_{k-1} + \\alpha (\\mu_{\\text{post}} - V_{k-1}) = (1-\\alpha)V_{k-1} + \\alpha \\mu_{\\text{post}}\n$$\n此关系描述了提示状态的价值估计如何逐次试验演变。折扣因子 $\\gamma$ 没有出现在最终的递推关系中，因为后继状态是价值为 $0$ 的终止状态。\n\n为推导 $V_k$ 的闭式表达式，我们求解此递推关系。一种标准方法是分析其与系统不动点 $V_{\\infty}$ 的偏差。不动点是价值不再发生变化时的值，即 $V_k = V_{k-1} = V_{\\infty}$。\n$$\nV_{\\infty} = (1-\\alpha)V_{\\infty} + \\alpha \\mu_{\\text{post}} \\implies \\alpha V_{\\infty} = \\alpha \\mu_{\\text{post}}\n$$\n对于非零学习率 $\\alpha  0$，价值收敛到 $V_{\\infty} = \\mu_{\\text{post}}$，即变化点后的真实期望奖励。如果 $\\alpha=0$，价值将保持在 $V_0$ 不变。\n\n让我们将与渐近值的差定义为 $\\Delta_k = V_k - \\mu_{\\text{post}}$。我们可以写出 $\\Delta_k$ 的递推关系：\n$$\n\\Delta_k = V_k - \\mu_{\\text{post}} = [(1-\\alpha)V_{k-1} + \\alpha \\mu_{\\text{post}}] - \\mu_{\\text{post}}\n$$\n$$\n\\Delta_k = (1-\\alpha)V_{k-1} - (1-\\alpha)\\mu_{\\text{post}} = (1-\\alpha)(V_{k-1} - \\mu_{\\text{post}})\n$$\n$$\n\\Delta_k = (1-\\alpha)\\Delta_{k-1}\n$$\n这是一个简单的等比数列。通过归纳法，我们可以用初始差值 $\\Delta_0$ 来表示 $\\Delta_k$：\n$$\n\\Delta_k = (1-\\alpha)^k \\Delta_0\n$$\n初始差值为 $\\Delta_0 = V_0 - \\mu_{\\text{post}}$。将其代回，我们得到：\n$$\nV_k - \\mu_{\\text{post}} = (1-\\alpha)^k (V_0 - \\mu_{\\text{post}})\n$$\n这就得出了经过 $k$ 次试验后价值估计 $V_k$ 的闭式解：\n$$\nV_k = \\mu_{\\text{post}} + (1-\\alpha)^k (V_0 - \\mu_{\\text{post}})\n$$\n问题要求的是经过 $n$ 次变化后试验后的价值 $V_n$，这可直接将 $k=n$ 代入此公式得到。\n\n当一个已习得的正向关联因奖励的移除或减少而减弱时，就会发生消退学习。在此模型中，这对应于 $\\mu_{\\text{post}}  \\mu_{\\text{pre}} = V_0$。第 $k$ 次试验的 TD 误差 $\\delta_{k-1} = \\mu_{\\text{post}} - V_{k-1}$ 代表了预测误差。我们可以使用闭式解来表示这个误差：\n$$\n\\delta_{k-1} = \\mu_{\\text{post}} - \\left( \\mu_{\\text{post}} + (1-\\alpha)^{k-1} (V_0 - \\mu_{\\text{post}}) \\right) = -(1-\\alpha)^{k-1} (V_0 - \\mu_{\\text{post}})\n$$\n在消退学习的条件下，即 $V_0  \\mu_{\\text{post}}$，项 $(V_0 - \\mu_{\\text{post}})$ 是正的。对于学习率 $\\alpha \\in (0, 1]$，当 $k \\ge 1$ 时，项 $(1-\\alpha)^{k-1}$ 是非负的且小于 $1$。因此（假设 $\\alpha  0$），在所有试验中 TD 误差 $\\delta_{k-1}$ 都是负的。负的 TD 误差表明结果比预期的要差，从而驱使价值估计向新的、较低的奖励期望 $\\mu_{\\text{post}}$ 下降。这个误差的大小 $|\\delta_{k-1}|$ 随着每次试验呈指数级下降，表明系统的预测正在改善并收敛到新的现实情况。TD 误差为负的条件是 $V_{k-1}  \\mu_{\\text{post}}$，只要价值是从上方收敛的（$V_0  \\mu_{\\text{post}}$），并且学习率没有大到足以引起超调（此处不可能，因为 $\\alpha \\le 1$），这个条件就成立。\n\n实现将对每个测试用例直接计算 $V_n = \\mu_{\\text{post}} + (1-\\alpha)^n (V_0 - \\mu_{\\text{post}})$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the value estimate V_n after n post-change trials for a set of test cases\n    based on the derived closed-form solution for a temporal difference learning model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (V0, mu_post, alpha, n).\n    test_cases = [\n        (1.0, 0.0, 0.2, 10),    # Case A (happy path, full extinction)\n        (1.0, 0.3, 0.1, 20),    # Case B (partial extinction to a lower asymptote)\n        (1.0, 0.0, 1.0, 3),     # Case C (boundary learning rate, immediate update)\n        (0.75, 0.0, 0.0, 50),   # Case D (no learning edge case)\n        (0.8, 0.0, 0.05, 100),  # Case E (slow extinction over many trials)\n    ]\n\n    results = []\n    for V0, mu_post, alpha, n in test_cases:\n        # The closed-form expression derived for the value estimate V_n after n trials is:\n        # V_n = mu_post + (1 - alpha)^n * (V0 - mu_post)\n        \n        # This formula is computationally robust for the given constraints.\n        # If alpha = 1.0, (1 - alpha) = 0. V_n becomes mu_post for n = 1.\n        # If alpha = 0.0, (1 - alpha) = 1. V_n remains V0.\n        \n        # Using floating point arithmetic\n        base = 1.0 - alpha\n        \n        # Calculate the exponential term\n        decay_factor = base ** n\n        \n        # Calculate the final value estimate\n        Vn = mu_post + decay_factor * (V0 - mu_post)\n        \n        # Round the result to 10 decimal places as required.\n        results.append(round(Vn, 10))\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of float values enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "强化学习的一个核心目标不仅是学会预测未来（“期望”什么），更是学会如何行动（“做什么”）。本练习将向你介绍“行动者-评论家”（Actor-Critic）框架，这是一个具有生物学合理性的模型，其中时序差分误差信号扮演着双重角色 。你将看到这个常被比作大脑中多巴胺信号的单一误差信号，如何能同时更新“评论家”的价值估计，并指导“行动者”的策略朝着更有利可图的行动方向调整。",
            "id": "4026719",
            "problem": "考虑一个双状态马尔可夫决策过程（MDP），其状态为 $s_A$ 和 $s_B$，两个动作为 $a_1$ 和 $a_2$，以及由以下观测轨迹定义的确定性转移：在时间 $t=0$ 时，智能体从 $s_A$ 开始，采取动作 $a_1$，获得奖励 $r_0 = 0$，并转移到 $s_B$；在时间 $t=1$ 时，在 $s_B$ 中，智能体采取动作 $a_2$，获得奖励 $r_1 = 1$，并转移到一个终止状态。折扣因子为 $\\gamma = 0.9$。评价器（critic）通过线性函数 $V(s; \\mathbf{w}) = \\mathbf{w}^{\\top} \\boldsymbol{\\phi}(s)$ 来表示状态值函数 $V(s)$，其中固定的特征向量为 $\\boldsymbol{\\phi}(s_A) = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ 和 $\\boldsymbol{\\phi}(s_B) = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$。初始评价器参数为 $\\mathbf{w}_0 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix}$。行动器（actor）在每个状态 $s$ 使用伯努利策略，该策略带有一个特定于状态的 logit 参数 $\\theta_s$，其中 $\\pi(a_1 \\mid s; \\theta_s)$ 是在状态 $s$ 中选择 $a_1$ 的概率，而 $\\pi(a_2 \\mid s; \\theta_s) = 1 - \\pi(a_1 \\mid s; \\theta_s)$。从 $\\theta_s$ 到动作概率的映射由逻辑函数 $\\sigma(\\theta_s)$ 给出，因此 $\\pi(a_1 \\mid s; \\theta_s) = \\sigma(\\theta_s)$ 且 $\\pi(a_2 \\mid s; \\theta_s) = 1 - \\sigma(\\theta_s)$，其中 $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$。初始行动器参数为 $\\theta_{A,0} = 0$ 和 $\\theta_{B,0} = 0$。评价器每次更新使用恒定的步长 $\\alpha_w = 0.5$，行动器每次更新使用恒定的步长 $\\alpha_{\\theta} = 0.1$。\n\n利用计算神经科学中时间差分学习和多巴胺能奖励预测误差的基础，将时间差分误差 $\\delta_t$ 视为驱动评价器的半梯度值更新和行动器的策略参数更新的奖励预测误差信号，以处理观测到的轨迹。顺序处理轨迹，在每个时间步根据当前参数计算出的 $\\delta_t$ 在线更新评价器和行动器。推导每个时间差分误差，在每个步骤中应用评价器的更新，并使用在所选动作被选择的状态下其对数概率的梯度来应用行动器的参数更新。完成两个时间步后，确定行动器参数 $\\theta_{B}$ 的最终值。\n\n将最终答案表示为一个实数。无需四舍五入。",
            "solution": "### 步骤1：提取已知条件\n-   状态：$s_A$、$s_B$ 和一个终止状态 $s_T$。\n-   动作：$a_1$、$a_2$。\n-   轨迹：\n    -   在 $t=0$ 时：状态 $s_0=s_A$，动作 $a_0=a_1$，奖励 $r_0=0$，下一状态 $s_1=s_B$。\n    -   在 $t=1$ 时：状态 $s_1=s_B$，动作 $a_1=a_2$，奖励 $r_1=1$，下一状态 $s_2=s_T$。\n-   折扣因子：$\\gamma = 0.9$。\n-   评价器值函数近似：$V(s; \\mathbf{w}) = \\mathbf{w}^{\\top} \\boldsymbol{\\phi}(s)$。\n-   特征向量：$\\boldsymbol{\\phi}(s_A) = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，$\\boldsymbol{\\phi}(s_B) = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$。\n-   初始评价器参数：$\\mathbf{w}_0 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix}$。\n-   行动器策略：$\\pi(a_1 \\mid s; \\theta_s) = \\sigma(\\theta_s) = \\frac{1}{1 + \\exp(-\\theta_s)}$，且 $\\pi(a_2 \\mid s; \\theta_s) = 1 - \\sigma(\\theta_s)$。\n-   初始行动器参数：$\\theta_{A,0} = 0$，$\\theta_{B,0} = 0$。\n-   步长：$\\alpha_w = 0.5$ (评价器)，$\\alpha_{\\theta} = 0.1$ (行动器)。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题是计算强化学习中的一个标准练习，具体是使用在线、单步行动器-评价器（Actor-Critic）方法，其中评价器使用线性函数近似，行动器使用参数化策略。\n-   **科学依据**：该模型基于时间差分学习的成熟原理，这是现代强化学习理论的基石，并与大脑功能模型有紧密联系。所有组件（MDP、TD误差、半梯度更新、策略梯度）都是标准的且在数学上是合理的。\n-   **适定性**：所有必要的参数、初始条件和更新规则都已明确给出。轨迹是确定性的且有限的。问题是自包含的，并且可以唯一地、一步步地计算出最终的参数值。\n-   **客观性**：问题以精确的数学和算法术语陈述，没有歧义或主观论断。\n\n### 步骤3：结论与行动\n问题有效。将推导完整的解。\n\n### 推导\n求解过程需要按顺序处理这个两步轨迹，在每个步骤中对评价器的权重 $\\mathbf{w}$ 和行动器的参数 $\\theta$ 进行在线更新。时间差分（TD）误差 $\\delta_t$ 作为共同的误差信号。\n\n**初始状态 ($t=0$):**\n初始参数为 $\\mathbf{w}_0 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix}$，$\\theta_{A,0}=0$ 和 $\\theta_{B,0}=0$。\n特征表示意味着 $V(s_A; \\mathbf{w}) = w_A$ 和 $V(s_B; \\mathbf{w}) = w_B$。\n因此，初始值估计为：\n$V(s_A; \\mathbf{w}_0) = 0.4$\n$V(s_B; \\mathbf{w}_0) = -0.2$\n\n**处理时间步 $t=0$:**\n智能体处于状态 $s_0=s_A$，采取动作 $a_0=a_1$，获得奖励 $r_0=0$，并转移到状态 $s_1=s_B$。\n\n1.  **计算TD误差 $\\delta_0$**：\n    TD误差定义为 $\\delta_t = r_t + \\gamma V(s_{t+1}; \\mathbf{w}_t) - V(s_t; \\mathbf{w}_t)$。\n    对于 $t=0$，我们使用初始权重 $\\mathbf{w}_0$：\n    $$ \\delta_0 = r_0 + \\gamma V(s_1; \\mathbf{w}_0) - V(s_0; \\mathbf{w}_0) $$\n    $$ \\delta_0 = 0 + 0.9 \\times V(s_B; \\mathbf{w}_0) - V(s_A; \\mathbf{w}_0) $$\n    $$ \\delta_0 = 0.9 \\times (-0.2) - 0.4 = -0.18 - 0.4 = -0.58 $$\n\n2.  **将评价器的权重更新为 $\\mathbf{w}_1$**：\n    评价器的半梯度更新规则是 $\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha_w \\delta_t \\nabla_{\\mathbf{w}} V(s_t; \\mathbf{w}_t)$。对于线性近似，$\\nabla_{\\mathbf{w}} V(s; \\mathbf{w}) = \\boldsymbol{\\phi}(s)$。\n    $$ \\mathbf{w}_1 = \\mathbf{w}_0 + \\alpha_w \\delta_0 \\boldsymbol{\\phi}(s_0) = \\mathbf{w}_0 + \\alpha_w \\delta_0 \\boldsymbol{\\phi}(s_A) $$\n    $$ \\mathbf{w}_1 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix} + 0.5 \\times (-0.58) \\times \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix} + \\begin{pmatrix}-0.29 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix} $$\n    新的权重为 $\\mathbf{w}_1 = \\begin{pmatrix}w_{A,1} \\\\ w_{B,1}\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix}$。\n\n3.  **更新状态 $s_A$ 的行动器参数**：\n    参数 $\\theta_s$ 的行动器更新规则是 $\\theta_{s,t+1} = \\theta_{s,t} + \\alpha_{\\theta} \\delta_t \\nabla_{\\theta_s} \\ln \\pi(a_t \\mid s_t; \\theta_{s,t})$。\n    在 $t=0$ 时，我们更新 $\\theta_A$，因为动作是在 $s_A$ 中采取的。参数 $\\theta_B$ 不更新。\n    动作 $a_1$ 的对数概率的梯度为：\n    $$ \\nabla_{\\theta_s} \\ln \\pi(a_1 \\mid s; \\theta_s) = \\frac{\\partial}{\\partial \\theta_s} \\ln(\\sigma(\\theta_s)) = 1 - \\sigma(\\theta_s) = 1 - \\pi(a_1 \\mid s; \\theta_s) $$\n    在 $t=0$ 时，相关参数为 $\\theta_{A,0}=0$。所采取的动作 $a_1$ 的概率是 $\\pi(a_1 \\mid s_A; \\theta_{A,0}) = \\sigma(0) = \\frac{1}{1+\\exp(0)} = 0.5$。\n    梯度值为 $1 - 0.5 = 0.5$。\n    $\\theta_A$ 的更新为：\n    $$ \\theta_{A,1} = \\theta_{A,0} + \\alpha_{\\theta} \\delta_0 \\nabla_{\\theta_{A,0}} \\ln \\pi(a_1 \\mid s_A; \\theta_{A,0}) $$\n    $$ \\theta_{A,1} = 0 + 0.1 \\times (-0.58) \\times 0.5 = -0.029 $$\n    状态 $s_B$ 的参数保持不变：$\\theta_{B,1} = \\theta_{B,0} = 0$。\n\n**处理时间步 $t=1$:**\n智能体处于状态 $s_1=s_B$，采取动作 $a_1=a_2$，获得奖励 $r_1=1$，并转移到终止状态 $s_2=s_T$。根据定义，终止状态的值为 $V(s_T) = 0$。\n当前参数为 $\\mathbf{w}_1 = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix}$ 和 $\\theta_{B,1} = 0$。\n\n1.  **计算TD误差 $\\delta_1$**：\n    我们使用更新后的权重 $\\mathbf{w}_1$。\n    $$ \\delta_1 = r_1 + \\gamma V(s_2; \\mathbf{w}_1) - V(s_1; \\mathbf{w}_1) $$\n    $$ \\delta_1 = 1 + \\gamma V(s_T; \\mathbf{w}_1) - V(s_B; \\mathbf{w}_1) $$\n    $$ \\delta_1 = 1 + 0.9 \\times 0 - (\\mathbf{w}_1^\\top \\boldsymbol{\\phi}(s_B)) = 1 - (-0.2) = 1.2 $$\n\n2.  **将评价器的权重更新为 $\\mathbf{w}_2$**：\n    $$ \\mathbf{w}_2 = \\mathbf{w}_1 + \\alpha_w \\delta_1 \\boldsymbol{\\phi}(s_1) = \\mathbf{w}_1 + \\alpha_w \\delta_1 \\boldsymbol{\\phi}(s_B) $$\n    $$ \\mathbf{w}_2 = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix} + 0.5 \\times 1.2 \\times \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix} + \\begin{pmatrix}0 \\\\ 0.6\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ 0.4\\end{pmatrix} $$\n\n3.  **更新状态 $s_B$ 的行动器参数**：\n    在 $t=1$ 时，我们更新 $\\theta_B$，因为动作是在 $s_B$ 中采取的。参数 $\\theta_A$ 不更新。问题要求 $\\theta_B$ 的最终值。\n    动作 $a_2$ 的对数概率的梯度为：\n    $$ \\nabla_{\\theta_s} \\ln \\pi(a_2 \\mid s; \\theta_s) = \\frac{\\partial}{\\partial \\theta_s} \\ln(1 - \\sigma(\\theta_s)) = -\\sigma(\\theta_s) = -\\pi(a_1 \\mid s; \\theta_s) $$\n    在 $t=1$ 时，相关参数为 $\\theta_{B,1}=0$。在状态 $s_B$ 中动作 $a_1$ 的概率是 $\\pi(a_1 \\mid s_B; \\theta_{B,1}) = \\sigma(0) = 0.5$。\n    梯度值为 $-0.5$。\n    $\\theta_B$ 的更新为：\n    $$ \\theta_{B,2} = \\theta_{B,1} + \\alpha_{\\theta} \\delta_1 \\nabla_{\\theta_{B,1}} \\ln \\pi(a_2 \\mid s_B; \\theta_{B,1}) $$\n    $$ \\theta_{B,2} = 0 + 0.1 \\times 1.2 \\times (-0.5) $$\n    $$ \\theta_{B,2} = 0.1 \\times (-0.6) = -0.06 $$\n\n轨迹处理完毕。行动器参数 $\\theta_B$ 的最终值为 $\\theta_{B,2}$。",
            "answer": "$$ \\boxed{-0.06} $$"
        }
    ]
}