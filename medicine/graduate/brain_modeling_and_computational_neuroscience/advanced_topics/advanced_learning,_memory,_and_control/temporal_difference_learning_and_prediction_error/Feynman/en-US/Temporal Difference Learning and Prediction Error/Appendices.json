{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Temporal Difference (TD) learning, we must first master its core computational step. This exercise provides a concrete trajectory through a state space, allowing you to manually calculate the value updates at each step using the TD(0) algorithm. By working through these calculations, you will build an intuition for how value estimates are iteratively refined based on moment-to-moment experience and the crucial concept of bootstrapping from one estimate to the next.",
            "id": "4026748",
            "problem": "Consider a deterministic Markov reward process with states $s_A$, $s_B$, $s_C$, and an absorbing terminal state $s_T$. The value function $V(s)$ is initialized as $V(s_A)=0.2$, $V(s_B)=0.1$, $V(s_C)=-0.1$, and $V(s_T)=0$. A single episode generates the trajectory of transitions with immediate rewards: $s_A \\rightarrow s_B$ with reward $0$, $s_B \\rightarrow s_C$ with reward $1$, $s_C \\rightarrow s_B$ with reward $-0.5$, and $s_B \\rightarrow s_T$ with reward $2$. Assume a discount factor $\\gamma=0.8$ and a constant learning rate $\\alpha=0.5$. The terminal value $V(s_T)$ remains fixed at $0$ and is never updated.\n\nUsing the framework of Temporal Difference (TD) learning with one-step bootstrapping (often denoted TD($0$)) and the concept of reward prediction error defined as the discrepancy between the obtained one-step return and the current prediction, perform in-place updates along the trajectory in the order observed, updating only the value estimate of the state that was just visited. Report the numerical value of $V(s_B)$ after the fourth update. Express your final answer as a pure number with no units. No rounding instruction is required for this problem.",
            "solution": "The user wants to find the value of $V(s_B)$ after a sequence of updates using the Temporal Difference (TD) learning algorithm, specifically TD($0$).\n\nFirst, the problem is validated.\n**Step 1: Extract Givens**\n- States: $s_A$, $s_B$, $s_C$, and a terminal state $s_T$.\n- Initial value function: $V(s_A)=0.2$, $V(s_B)=0.1$, $V(s_C)=-0.1$, $V(s_T)=0$.\n- Trajectory with rewards:\n  1. $s_A \\rightarrow s_B$ with reward $R_1=0$.\n  2. $s_B \\rightarrow s_C$ with reward $R_2=1$.\n  3. $s_C \\rightarrow s_B$ with reward $R_3=-0.5$.\n  4. $s_B \\rightarrow s_T$ with reward $R_4=2$.\n- Discount factor: $\\gamma=0.8$.\n- Learning rate: $\\alpha=0.5$.\n- Constraint: The terminal value $V(s_T)$ is fixed at $0$ and is never updated.\n- Update method: In-place TD($0$) updates, updating the value of the state that was just visited.\n\n**Step 2: Validate Using Extracted Givens**\n- The problem is scientifically grounded, operating within the standard framework of reinforcement learning and Markov reward processes. Temporal Difference learning is a cornerstone of this field.\n- The problem is well-posed, providing all necessary parameters ($\\gamma$, $\\alpha$), initial conditions ($V(s)$), and a specific sequence of state transitions and rewards. The goal is clearly defined.\n- The language is objective and precise.\n- The problem is free of contradictions, scientific flaws, and ambiguities. The presence of a cycle ($s_B \\rightarrow s_C \\rightarrow s_B$) is a valid feature of a Markov process.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be derived.\n\nThe TD($0$) update rule for the value of a state $s_t$ after transitioning to state $s_{t+1}$ with reward $R_{t+1}$ is given by:\n$$V(s_t) \\leftarrow V(s_t) + \\alpha [R_{t+1} + \\gamma V(s_{t+1}) - V(s_t)]$$\nThe term $\\delta_t = R_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$ is the TD error, also known as the reward prediction error. The term $R_{t+1} + \\gamma V(s_{t+1})$ is the TD target. Updates are performed \"in-place,\" meaning the result of an update is used immediately in subsequent calculations.\n\nThe initial values are:\n$V(s_A) = 0.2$\n$V(s_B) = 0.1$\n$V(s_C) = -0.1$\n$V(s_T) = 0$\nThe constants are $\\alpha = 0.5$ and $\\gamma = 0.8$.\n\nWe will now perform the updates sequentially along the trajectory.\n\n**Update 1: Transition $s_A \\rightarrow s_B$ with $R_1 = 0$.**\nThe value of state $s_A$ is updated. We use the current values $V(s_A) = 0.2$ and $V(s_B) = 0.1$.\nThe TD target is $R_1 + \\gamma V(s_B) = 0 + (0.8)(0.1) = 0.08$.\nThe TD error is $\\delta_0 = 0.08 - V(s_A) = 0.08 - 0.2 = -0.12$.\nThe new value of $V(s_A)$ is:\n$V(s_A) \\leftarrow V(s_A) + \\alpha \\delta_0 = 0.2 + (0.5)(-0.12) = 0.2 - 0.06 = 0.14$.\nAfter this update, the values are: $V(s_A) = 0.14$, $V(s_B) = 0.1$, $V(s_C) = -0.1$.\n\n**Update 2: Transition $s_B \\rightarrow s_C$ with $R_2 = 1$.**\nThe value of state $s_B$ is updated. We use the current values $V(s_B) = 0.1$ and $V(s_C) = -0.1$.\nThe TD target is $R_2 + \\gamma V(s_C) = 1 + (0.8)(-0.1) = 1 - 0.08 = 0.92$.\nThe TD error is $\\delta_1 = 0.92 - V(s_B) = 0.92 - 0.1 = 0.82$.\nThe new value of $V(s_B)$ is:\n$V(s_B) \\leftarrow V(s_B) + \\alpha \\delta_1 = 0.1 + (0.5)(0.82) = 0.1 + 0.41 = 0.51$.\nAfter this update, the values are: $V(s_A) = 0.14$, $V(s_B) = 0.51$, $V(s_C) = -0.1$.\n\n**Update 3: Transition $s_C \\rightarrow s_B$ with $R_3 = -0.5$.**\nThe value of state $s_C$ is updated. We use the current value $V(s_C) = -0.1$ and the newly updated value $V(s_B) = 0.51$.\nThe TD target is $R_3 + \\gamma V(s_B) = -0.5 + (0.8)(0.51) = -0.5 + 0.408 = -0.092$.\nThe TD error is $\\delta_2 = -0.092 - V(s_C) = -0.092 - (-0.1) = 0.008$.\nThe new value of $V(s_C)$ is:\n$V(s_C) \\leftarrow V(s_C) + \\alpha \\delta_2 = -0.1 + (0.5)(0.008) = -0.1 + 0.004 = -0.096$.\nAfter this update, the values are: $V(s_A) = 0.14$, $V(s_B) = 0.51$, $V(s_C) = -0.096$.\n\n**Update 4: Transition $s_B \\rightarrow s_T$ with $R_4 = 2$.**\nThe value of state $s_B$ is updated for a second time. We use the current value $V(s_B) = 0.51$ and the fixed terminal value $V(s_T) = 0$.\nThe TD target is $R_4 + \\gamma V(s_T) = 2 + (0.8)(0) = 2$.\nThe TD error is $\\delta_3 = 2 - V(s_B) = 2 - 0.51 = 1.49$.\nThe new value of $V(s_B)$ is:\n$V(s_B) \\leftarrow V(s_B) + \\alpha \\delta_3 = 0.51 + (0.5)(1.49) = 0.51 + 0.745 = 1.255$.\n\nThe value of $V(s_B)$ after the fourth update is $1.255$.",
            "answer": "$$\\boxed{1.255}$$"
        },
        {
            "introduction": "Having practiced the mechanics of TD updates, we now apply this framework to a classic phenomenon in neuroscience: extinction learning. This problem models how an agent adjusts its expectations when a previously rewarded cue no longer predicts a reward, generating a sequence of negative prediction errors. By deriving and implementing the learning curve, you will see how a simple, local update rule, $V_k = (1-\\alpha)V_{k-1} + \\alpha \\mu_{\\text{post}}$, can produce the characteristic decay of a learned association.",
            "id": "4026721",
            "problem": "You are given a minimal Markov reward process that models Pavlovian conditioning and extinction learning as used in brain modeling and computational neuroscience. There are two states: a cue state $C$ and a terminal state $T$. Each trial consists of exactly $2$ time steps: at time step $t$, the agent is in $C$, and at time step $t+1$, the agent transitions to $T$ and receives a scalar reward $r_t$. The terminal state $T$ is absorbing with value $V(T) = 0$. Before a change point, the expected reward is a constant $\\mu_{\\text{pre}}$. After the change point, the expected reward is a constant $\\mu_{\\text{post}}$ with $\\mu_{\\text{post}} \\le \\mu_{\\text{pre}}$. Pre-training has converged such that at the moment of the change point, the value estimate at $C$ is $V_0 = \\mu_{\\text{pre}}$.\n\nFor learning, use the following core definitions of temporal difference learning. At time step $t$, define the temporal difference error $\\delta_t$ for state $s_t$ as $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$, where $\\gamma \\in [0,1)$ is the discount factor. The value update is $V(s_t) \\leftarrow V(s_t) + \\alpha \\, \\delta_t$, where $\\alpha \\in [0,1]$ is a constant learning rate. Assume there is only one non-terminal state $C$, so the only updated value is $V(C)$, which we denote by $V_k$ after $k$ post-change trials. After the change point, the reward is deterministic with $r_t = \\mu_{\\text{post}}$ on every trial.\n\nTasks:\n1. Using only the definitions above and the described trial structure, derive a closed-form expression for the sequence $\\{V_k\\}_{k \\ge 0}$ under the fixed learning rate $\\alpha$ and the post-change constant reward $\\mu_{\\text{post}}$. Interpret extinction learning in terms of the sign and magnitude of the temporal difference errors over trials, and identify conditions under which these errors are negative.\n2. Implement an algorithm that, given $V_0$, $\\mu_{\\text{post}}$, $\\alpha$, and an integer $n \\ge 0$, computes $V_n$, the value estimate after $n$ post-change trials.\n3. Your implementation must use the derived closed-form expression or a logically equivalent computation implied by your derivation, and must produce outputs rounded to $10$ decimal places.\n\nTest Suite:\nProvide outputs for the following parameter sets, each defined as the tuple $(V_0, \\mu_{\\text{post}}, \\alpha, n)$:\n- Case A (happy path, full extinction): $(1.0, 0.0, 0.2, 10)$.\n- Case B (partial extinction to a lower asymptote): $(1.0, 0.3, 0.1, 20)$.\n- Case C (boundary learning rate, immediate update): $(1.0, 0.0, 1.0, 3)$.\n- Case D (no learning edge case): $(0.75, 0.0, 0.0, 50)$.\n- Case E (slow extinction over many trials): $(0.8, 0.0, 0.05, 100)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result\\_A,result\\_B,result\\_C,result\\_D,result\\_E]$). Each $result\\_X$ must be the float value of $V_n$ for the corresponding case, rounded to $10$ decimal places. No other text should be printed.",
            "solution": "The problem requires the derivation of a closed-form expression for the value of a cue state, $V(C)$, during an extinction learning paradigm modeled by temporal difference (TD) learning, and to implement this expression.\n\nFirst, we formalize the learning process as a recurrence relation. Let $V_k$ denote the value estimate of the cue state $C$ after $k$ post-change-point trials, for $k \\ge 0$. The initial value, established by pre-training, is $V_0 = \\mu_{\\text{pre}}$.\n\nFor any trial $k \\ge 1$, the agent starts in state $s_t = C$ with the current value estimate $V_{k-1}$. The agent then transitions to the terminal state $s_{t+1} = T$ and receives a reward $r_t = \\mu_{\\text{post}}$. The value of the terminal state is given as $V(T) = 0$.\n\nAccording to the provided definition, the temporal difference error, which we denote $\\delta_{k-1}$ for the $k$-th trial, is:\n$$\n\\delta_{k-1} = r_t + \\gamma V(s_{t+1}) - V(s_t)\n$$\nSubstituting the given values for the trial structure:\n$$\n\\delta_{k-1} = \\mu_{\\text{post}} + \\gamma(0) - V_{k-1} = \\mu_{\\text{post}} - V_{k-1}\n$$\nThe value of state $C$ is then updated using the learning rate $\\alpha \\in [0, 1]$:\n$$\nV_k \\leftarrow V_{k-1} + \\alpha \\delta_{k-1}\n$$\nThis gives us the following first-order linear recurrence relation for the sequence $\\{V_k\\}$:\n$$\nV_k = V_{k-1} + \\alpha (\\mu_{\\text{post}} - V_{k-1}) = (1-\\alpha)V_{k-1} + \\alpha \\mu_{\\text{post}}\n$$\nThis relation describes how the value estimate of the cue state evolves from trial to trial. The discount factor $\\gamma$ is not present in the final recurrence because the successor state is terminal with a value of $0$.\n\nTo derive the closed-form expression for $V_k$, we solve this recurrence relation. A standard method is to analyze the deviation from the system's fixed point, $V_{\\infty}$. The fixed point is the value at which no further changes occur, i.e., $V_k = V_{k-1} = V_{\\infty}$.\n$$\nV_{\\infty} = (1-\\alpha)V_{\\infty} + \\alpha \\mu_{\\text{post}} \\implies \\alpha V_{\\infty} = \\alpha \\mu_{\\text{post}}\n$$\nFor a non-zero learning rate $\\alpha > 0$, the value converges to $V_{\\infty} = \\mu_{\\text{post}}$, the true expected reward after the change point. If $\\alpha=0$, the value remains fixed at $V_0$.\n\nLet's define the difference from the asymptotic value as $\\Delta_k = V_k - \\mu_{\\text{post}}$. We can write a recurrence for $\\Delta_k$:\n$$\n\\Delta_k = V_k - \\mu_{\\text{post}} = [(1-\\alpha)V_{k-1} + \\alpha \\mu_{\\text{post}}] - \\mu_{\\text{post}}\n$$\n$$\n\\Delta_k = (1-\\alpha)V_{k-1} - (1-\\alpha)\\mu_{\\text{post}} = (1-\\alpha)(V_{k-1} - \\mu_{\\text{post}})\n$$\n$$\n\\Delta_k = (1-\\alpha)\\Delta_{k-1}\n$$\nThis is a simple geometric progression. By induction, we can express $\\Delta_k$ in terms of the initial difference $\\Delta_0$:\n$$\n\\Delta_k = (1-\\alpha)^k \\Delta_0\n$$\nThe initial difference is $\\Delta_0 = V_0 - \\mu_{\\text{post}}$. Substituting back, we get:\n$$\nV_k - \\mu_{\\text{post}} = (1-\\alpha)^k (V_0 - \\mu_{\\text{post}})\n$$\nThis yields the closed-form solution for the value estimate $V_k$ after $k$ trials:\n$$\nV_k = \\mu_{\\text{post}} + (1-\\alpha)^k (V_0 - \\mu_{\\text{post}})\n$$\nThe problem asks for the value $V_n$ after $n$ post-change trials, which is directly given by this formula with $k=n$.\n\nExtinction learning occurs when a learned positive association is weakened by the removal or reduction of reward. In this model, this corresponds to $\\mu_{\\text{post}}  \\mu_{\\text{pre}} = V_0$. The TD error on the $k$-th trial, $\\delta_{k-1} = \\mu_{\\text{post}} - V_{k-1}$, represents the prediction error. We can express this error using the closed-form solution:\n$$\n\\delta_{k-1} = \\mu_{\\text{post}} - \\left( \\mu_{\\text{post}} + (1-\\alpha)^{k-1} (V_0 - \\mu_{\\text{post}}) \\right) = -(1-\\alpha)^{k-1} (V_0 - \\mu_{\\text{post}})\n$$\nGiven the condition for extinction learning, $V_0  \\mu_{\\text{post}}$, the term $(V_0 - \\mu_{\\text{post}})$ is positive. For a learning rate $\\alpha \\in (0, 1]$, the term $(1-\\alpha)^{k-1}$ is non-negative and less than $1$ for $k > 1$. Therefore, the TD error $\\delta_{k-1}$ is negative for all trials (assuming $\\alpha > 0$). A negative TD error signals that the outcome was worse than expected, driving the value estimate down towards the new, lower reward expectation $\\mu_{\\text{post}}$. The magnitude of this error, $|\\delta_{k-1}|$, decreases exponentially with each trial, indicating that the system's prediction is improving and converging to the new reality. The condition for the TD errors to be negative is $V_{k-1}  \\mu_{\\text{post}}$, which holds as long as the value is converging from above ($V_0  \\mu_{\\text{post}}$) and the learning rate is not so large as to cause overshooting (which is not possible here as $\\alpha \\le 1$).\n\nThe implementation will directly compute $V_n = \\mu_{\\text{post}} + (1-\\alpha)^n (V_0 - \\mu_{\\text{post}})$ for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the value estimate V_n after n post-change trials for a set of test cases\n    based on the derived closed-form solution for a temporal difference learning model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (V0, mu_post, alpha, n).\n    test_cases = [\n        (1.0, 0.0, 0.2, 10),    # Case A (happy path, full extinction)\n        (1.0, 0.3, 0.1, 20),    # Case B (partial extinction to a lower asymptote)\n        (1.0, 0.0, 1.0, 3),     # Case C (boundary learning rate, immediate update)\n        (0.75, 0.0, 0.0, 50),   # Case D (no learning edge case)\n        (0.8, 0.0, 0.05, 100),  # Case E (slow extinction over many trials)\n    ]\n\n    results = []\n    for V0, mu_post, alpha, n in test_cases:\n        # The closed-form expression derived for the value estimate V_n after n trials is:\n        # V_n = mu_post + (1 - alpha)^n * (V0 - mu_post)\n        \n        # This formula is computationally robust for the given constraints.\n        # If alpha = 1.0, (1 - alpha) = 0. V_n becomes mu_post for n = 1.\n        # If alpha = 0.0, (1 - alpha) = 1. V_n remains V0.\n        \n        # Using floating point a\n        base = 1.0 - alpha\n        \n        # Calculate the exponential term\n        decay_factor = base ** n\n        \n        # Calculate the final value estimate\n        Vn = mu_post + decay_factor * (V0 - mu_post)\n        \n        # Round the result to 10 decimal places as required.\n        results.append(round(Vn, 10))\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of float values enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The Temporal Difference prediction error is more than just a signal for updating value estimates; it is a powerful teaching signal that can directly guide action selection. This practice introduces the actor-critic architecture, where the TD error $\\delta_t$ generated by the 'critic' (the value function) is used to update the policy of the 'actor' (the decision-maker). Working through this example will demonstrate how prediction and control are elegantly integrated in a single learning loop, a key idea in both artificial intelligence and computational models of the brain.",
            "id": "4026719",
            "problem": "Consider a two-state Markov Decision Process (MDP) with states $s_A$ and $s_B$, two actions $a_1$ and $a_2$, and deterministic transitions defined by the following observed trajectory: at time $t=0$, the agent starts in $s_A$, takes action $a_1$, receives reward $r_0 = 0$, and transitions to $s_B$; at time $t=1$, in $s_B$, the agent takes action $a_2$, receives reward $r_1 = 1$, and transitions to a terminal state. The discount factor is $\\gamma = 0.9$. The critic represents the state-value function $V(s)$ by a linear function $V(s; \\mathbf{w}) = \\mathbf{w}^{\\top} \\boldsymbol{\\phi}(s)$, where the fixed feature vectors are $\\boldsymbol{\\phi}(s_A) = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ and $\\boldsymbol{\\phi}(s_B) = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$. The initial critic parameters are $\\mathbf{w}_0 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix}$. The actor uses a Bernoulli policy at each state $s$ with a single state-specific logit parameter $\\theta_s$, where $\\pi(a_1 \\mid s; \\theta_s)$ is the probability of selecting $a_1$ in state $s$ and $\\pi(a_2 \\mid s; \\theta_s) = 1 - \\pi(a_1 \\mid s; \\theta_s)$. The mapping from $\\theta_s$ to action probability is given by the logistic function $\\sigma(\\theta_s)$, so that $\\pi(a_1 \\mid s; \\theta_s) = \\sigma(\\theta_s)$ and $\\pi(a_2 \\mid s; \\theta_s) = 1 - \\sigma(\\theta_s)$, with $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$. The initial actor parameters are $\\theta_{A,0} = 0$ and $\\theta_{B,0} = 0$. The critic uses a constant step size $\\alpha_w = 0.5$ for each update, and the actor uses a constant step size $\\alpha_{\\theta} = 0.1$ for each update.\n\nUsing the foundations of temporal difference learning and dopaminergic reward prediction error in computational neuroscience, treat the temporal difference error $\\delta_t$ as the reward prediction error signal driving both the critic’s semi-gradient value update and the actor’s policy parameter update for the observed trajectory. Process the trajectory sequentially, updating the critic and actor online at each time step based on $\\delta_t$ computed from the current parameters. Derive each temporal difference error, apply the critic’s update at each step, and apply the actor’s parameter update at each step using the gradient of the log-probability of the selected action in the state where it was selected. After completing both time steps, determine the final value of the actor parameter $\\theta_{B}$.\n\nExpress your final answer as a real number. No rounding is required.",
            "solution": "### Step 1: Extract Givens\n-   States: $s_A$, $s_B$, and a terminal state $s_T$.\n-   Actions: $a_1$, $a_2$.\n-   Trajectory:\n    -   At $t=0$: state $s_0=s_A$, action $a_0=a_1$, reward $r_0=0$, next state $s_1=s_B$.\n    -   At $t=1$: state $s_1=s_B$, action $a_1=a_2$, reward $r_1=1$, next state $s_2=s_T$.\n-   Discount factor: $\\gamma = 0.9$.\n-   Critic value function approximation: $V(s; \\mathbf{w}) = \\mathbf{w}^{\\top} \\boldsymbol{\\phi}(s)$.\n-   Feature vectors: $\\boldsymbol{\\phi}(s_A) = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $\\boldsymbol{\\phi}(s_B) = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$.\n-   Initial critic parameters: $\\mathbf{w}_0 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix}$.\n-   Actor policy: $\\pi(a_1 \\mid s; \\theta_s) = \\sigma(\\theta_s) = \\frac{1}{1 + \\exp(-\\theta_s)}$, and $\\pi(a_2 \\mid s; \\theta_s) = 1 - \\sigma(\\theta_s)$.\n-   Initial actor parameters: $\\theta_{A,0} = 0$, $\\theta_{B,0} = 0$.\n-   Step sizes: $\\alpha_w = 0.5$ (critic), $\\alpha_{\\theta} = 0.1$ (actor).\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in computational reinforcement learning, specifically using an online, one-step Actor-Critic method with linear function approximation for the critic and a parameterized policy for the actor.\n-   **Scientifically Grounded**: The model is based on well-established principles of temporal difference learning, which is a cornerstone of modern reinforcement learning theory and has strong ties to models of brain function. All components (MDP, TD error, semi-gradient updates, policy gradient) are standard and mathematically sound.\n-   **Well-Posed**: All necessary parameters, initial conditions, and update rules are explicitly provided. The trajectory is deterministic and finite. The problem is self-contained and allows for a unique, step-by-step calculation of the final parameter value.\n-   **Objective**: The problem is stated in precise mathematical and algorithmic terms, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be derived.\n\n### Derivation\nThe solution requires processing the two-step trajectory sequentially, performing online updates to both the critic's weights $\\mathbf{w}$ and the actor's parameters $\\theta$ at each step. The temporal difference (TD) error $\\delta_t$ serves as the common error signal.\n\n**Initial State ($t=0$):**\nThe initial parameters are $\\mathbf{w}_0 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix}$, $\\theta_{A,0}=0$, and $\\theta_{B,0}=0$.\nThe feature representation implies $V(s_A; \\mathbf{w}) = w_A$ and $V(s_B; \\mathbf{w}) = w_B$.\nThus, the initial value estimates are:\n$V(s_A; \\mathbf{w}_0) = 0.4$\n$V(s_B; \\mathbf{w}_0) = -0.2$\n\n**Processing Time Step $t=0$:**\nThe agent is in state $s_0=s_A$, takes action $a_0=a_1$, receives reward $r_0=0$, and transitions to state $s_1=s_B$.\n\n1.  **Calculate the TD error $\\delta_0$**:\n    The TD error is defined as $\\delta_t = r_t + \\gamma V(s_{t+1}; \\mathbf{w}_t) - V(s_t; \\mathbf{w}_t)$.\n    For $t=0$, we use the initial weights $\\mathbf{w}_0$:\n    $$ \\delta_0 = r_0 + \\gamma V(s_1; \\mathbf{w}_0) - V(s_0; \\mathbf{w}_0) $$\n    $$ \\delta_0 = 0 + 0.9 \\times V(s_B; \\mathbf{w}_0) - V(s_A; \\mathbf{w}_0) $$\n    $$ \\delta_0 = 0.9 \\times (-0.2) - 0.4 = -0.18 - 0.4 = -0.58 $$\n\n2.  **Update the critic's weights to $\\mathbf{w}_1$**:\n    The critic's semi-gradient update rule is $\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha_w \\delta_t \\nabla_{\\mathbf{w}} V(s_t; \\mathbf{w}_t)$. With linear approximation, $\\nabla_{\\mathbf{w}} V(s; \\mathbf{w}) = \\boldsymbol{\\phi}(s)$.\n    $$ \\mathbf{w}_1 = \\mathbf{w}_0 + \\alpha_w \\delta_0 \\boldsymbol{\\phi}(s_0) = \\mathbf{w}_0 + \\alpha_w \\delta_0 \\boldsymbol{\\phi}(s_A) $$\n    $$ \\mathbf{w}_1 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix} + 0.5 \\times (-0.58) \\times \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix} + \\begin{pmatrix}-0.29 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix} $$\n    The new weights are $\\mathbf{w}_1 = \\begin{pmatrix}w_{A,1} \\\\ w_{B,1}\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix}$.\n\n3.  **Update the actor's parameter for state $s_A$**:\n    The actor update rule for the parameter $\\theta_s$ is $\\theta_{s,t+1} = \\theta_{s,t} + \\alpha_{\\theta} \\delta_t \\nabla_{\\theta_s} \\ln \\pi(a_t \\mid s_t; \\theta_{s,t})$.\n    At $t=0$, we update $\\theta_A$ since the action was taken in $s_A$. The parameter $\\theta_B$ is not updated.\n    The gradient of the log-probability for action $a_1$ is:\n    $$ \\nabla_{\\theta_s} \\ln \\pi(a_1 \\mid s; \\theta_s) = \\frac{\\partial}{\\partial \\theta_s} \\ln(\\sigma(\\theta_s)) = 1 - \\sigma(\\theta_s) = 1 - \\pi(a_1 \\mid s; \\theta_s) $$\n    At $t=0$, the relevant parameter is $\\theta_{A,0}=0$. The probability of the taken action $a_1$ is $\\pi(a_1 \\mid s_A; \\theta_{A,0}) = \\sigma(0) = \\frac{1}{1+\\exp(0)} = 0.5$.\n    The gradient value is $1 - 0.5 = 0.5$.\n    The update for $\\theta_A$ is:\n    $$ \\theta_{A,1} = \\theta_{A,0} + \\alpha_{\\theta} \\delta_0 \\nabla_{\\theta_{A,0}} \\ln \\pi(a_1 \\mid s_A; \\theta_{A,0}) $$\n    $$ \\theta_{A,1} = 0 + 0.1 \\times (-0.58) \\times 0.5 = -0.029 $$\n    The parameter for state $s_B$ remains unchanged: $\\theta_{B,1} = \\theta_{B,0} = 0$.\n\n**Processing Time Step $t=1$:**\nThe agent is in state $s_1=s_B$, takes action $a_1=a_2$, receives reward $r_1=1$, and transitions to the terminal state $s_2=s_T$. The value of the terminal state is $V(s_T) = 0$ by definition.\nThe current parameters are $\\mathbf{w}_1 = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix}$ and $\\theta_{B,1} = 0$.\n\n1.  **Calculate the TD error $\\delta_1$**:\n    We use the updated weights $\\mathbf{w}_1$.\n    $$ \\delta_1 = r_1 + \\gamma V(s_2; \\mathbf{w}_1) - V(s_1; \\mathbf{w}_1) $$\n    $$ \\delta_1 = 1 + \\gamma V(s_T; \\mathbf{w}_1) - V(s_B; \\mathbf{w}_1) $$\n    $$ \\delta_1 = 1 + 0.9 \\times 0 - (\\mathbf{w}_1^\\top \\boldsymbol{\\phi}(s_B)) = 1 - (-0.2) = 1.2 $$\n\n2.  **Update the critic's weights to $\\mathbf{w}_2$**:\n    $$ \\mathbf{w}_2 = \\mathbf{w}_1 + \\alpha_w \\delta_1 \\boldsymbol{\\phi}(s_1) = \\mathbf{w}_1 + \\alpha_w \\delta_1 \\boldsymbol{\\phi}(s_B) $$\n    $$ \\mathbf{w}_2 = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix} + 0.5 \\times 1.2 \\times \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix} + \\begin{pmatrix}0 \\\\ 0.6\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ 0.4\\end{pmatrix} $$\n\n3.  **Update the actor's parameter for state $s_B$**:\n    At $t=1$, we update $\\theta_B$ as the action was taken in $s_B$. The parameter $\\theta_A$ is not updated. The question asks for the final value of $\\theta_B$.\n    The gradient of the log-probability for action $a_2$ is:\n    $$ \\nabla_{\\theta_s} \\ln \\pi(a_2 \\mid s; \\theta_s) = \\frac{\\partial}{\\partial \\theta_s} \\ln(1 - \\sigma(\\theta_s)) = -\\sigma(\\theta_s) = -\\pi(a_1 \\mid s; \\theta_s) $$\n    At $t=1$, the relevant parameter is $\\theta_{B,1}=0$. The probability of action $a_1$ in state $s_B$ is $\\pi(a_1 \\mid s_B; \\theta_{B,1}) = \\sigma(0) = 0.5$.\n    The gradient value is $-0.5$.\n    The update for $\\theta_B$ is:\n    $$ \\theta_{B,2} = \\theta_{B,1} + \\alpha_{\\theta} \\delta_1 \\nabla_{\\theta_{B,1}} \\ln \\pi(a_2 \\mid s_B; \\theta_{B,1}) $$\n    $$ \\theta_{B,2} = 0 + 0.1 \\times 1.2 \\times (-0.5) $$\n    $$ \\theta_{B,2} = 0.1 \\times (-0.6) = -0.06 $$\n\nThe trajectory is complete. The final value of the actor parameter $\\theta_B$ is $\\theta_{B,2}$.",
            "answer": "$$ \\boxed{-0.06} $$"
        }
    ]
}