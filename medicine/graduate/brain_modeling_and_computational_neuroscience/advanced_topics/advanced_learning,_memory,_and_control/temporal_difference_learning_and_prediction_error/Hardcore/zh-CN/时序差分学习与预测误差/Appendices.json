{
    "hands_on_practices": [
        {
            "introduction": "要掌握任何算法，最有效的方法之一就是亲手执行它。本练习将带领你逐一完成时序差分（TD）学习的核心计算过程。通过追踪一个智能体在特定轨迹上的经历，你将实践如何运用 TD(0) 更新规则，根据即时奖励和对未来价值的“自举”估计来计算预测误差，并用它来修正当前状态的价值。这个基础练习是理解所有更复杂 TD 模型的第一步 。",
            "id": "4026748",
            "problem": "考虑一个确定性马尔可夫奖励过程，其状态包括 $s_A$、$s_B$、$s_C$ 以及一个吸收终止状态 $s_T$。价值函数 $V(s)$ 初始化为 $V(s_A)=0.2$，$V(s_B)=0.1$，$V(s_C)=-0.1$ 以及 $V(s_T)=0$。单个回合生成了带有立即奖励的转移轨迹：$s_A \\rightarrow s_B$ 奖励为 $0$，$s_B \\rightarrow s_C$ 奖励为 $1$，$s_C \\rightarrow s_B$ 奖励为 $-0.5$，以及 $s_B \\rightarrow s_T$ 奖励为 $2$。假设折扣因子 $\\gamma=0.8$ 且恒定学习率 $\\alpha=0.5$。终止状态的价值 $V(s_T)$ 保持固定为 $0$ 且永不更新。\n\n使用一步自举的时间差分 (TD) 学习（通常表示为 TD($0$)）框架，以及定义为所获得的单步回报与当前预测之间差异的奖励预测误差概念，沿着观察到的轨迹顺序执行原地更新，每次仅更新刚刚访问过的状态的价值估计。报告第四次更新后 $V(s_B)$ 的数值。最终答案以纯数字形式表示，不带单位。本题没有四舍五入的要求。",
            "solution": "用户希望在使用时间差分 (TD) 学习算法（具体为 TD($0$)）进行一系列更新后，求出 $V(s_B)$ 的值。\n\n首先，对问题进行验证。\n**步骤 1：提取已知条件**\n- 状态：$s_A$、$s_B$、$s_C$ 和一个终止状态 $s_T$。\n- 初始价值函数：$V(s_A)=0.2$，$V(s_B)=0.1$，$V(s_C)=-0.1$，$V(s_T)=0$。\n- 带奖励的轨迹：\n  1. $s_A \\rightarrow s_B$ 奖励 $R_1=0$。\n  2. $s_B \\rightarrow s_C$ 奖励 $R_2=1$。\n  3. $s_C \\rightarrow s_B$ 奖励 $R_3=-0.5$。\n  4. $s_B \\rightarrow s_T$ 奖励 $R_4=2$。\n- 折扣因子：$\\gamma=0.8$。\n- 学习率：$\\alpha=0.5$。\n- 约束条件：终止状态的价值 $V(s_T)$ 固定为 $0$ 且永不更新。\n- 更新方法：原地 TD($0$) 更新，更新刚刚访问过的状态的价值。\n\n**步骤 2：使用提取的已知条件进行验证**\n- 该问题具有科学依据，在强化学习和马尔可夫奖励过程的标准框架内运作。时间差分学习是该领域的一个基石。\n- 该问题是适定的，提供了所有必要的参数（$\\gamma$、$\\alpha$）、初始条件（$V(s)$）以及一个特定的状态转移和奖励序列。目标被明确定义。\n- 语言客观而精确。\n- 该问题没有矛盾、科学缺陷和歧义。循环（$s_B \\rightarrow s_C \\rightarrow s_B$）的存在是马尔可夫过程的一个有效特征。\n\n**步骤 3：结论和行动**\n问题是有效的。将推导解答。\n\n在以奖励 $R_{t+1}$ 转移到状态 $s_{t+1}$ 后，状态 $s_t$ 的价值的 TD($0$) 更新规则如下：\n$$V(s_t) \\leftarrow V(s_t) + \\alpha [R_{t+1} + \\gamma V(s_{t+1}) - V(s_t)]$$\n项 $\\delta_t = R_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$ 是 TD 误差，也称为奖励预测误差。项 $R_{t+1} + \\gamma V(s_{t+1})$ 是 TD 目标。更新是“原地”执行的，意味着一次更新的结果会立即用于后续的计算中。\n\n初始值为：\n$V(s_A) = 0.2$\n$V(s_B) = 0.1$\n$V(s_C) = -0.1$\n$V(s_T) = 0$\n常数为 $\\alpha = 0.5$ 和 $\\gamma = 0.8$。\n\n现在我们沿着轨迹依次执行更新。\n\n**更新 1：转移 $s_A \\rightarrow s_B$，奖励 $R_1 = 0$。**\n更新状态 $s_A$ 的价值。我们使用当前值 $V(s_A) = 0.2$ 和 $V(s_B) = 0.1$。\nTD 目标是 $R_1 + \\gamma V(s_B) = 0 + (0.8)(0.1) = 0.08$。\nTD 误差是 $\\delta_0 = 0.08 - V(s_A) = 0.08 - 0.2 = -0.12$。\n$V(s_A)$ 的新值为：\n$V(s_A) \\leftarrow V(s_A) + \\alpha \\delta_0 = 0.2 + (0.5)(-0.12) = 0.2 - 0.06 = 0.14$。\n本次更新后，各价值为：$V(s_A) = 0.14$，$V(s_B) = 0.1$，$V(s_C) = -0.1$。\n\n**更新 2：转移 $s_B \\rightarrow s_C$，奖励 $R_2 = 1$。**\n更新状态 $s_B$ 的价值。我们使用当前值 $V(s_B) = 0.1$ 和 $V(s_C) = -0.1$。\nTD 目标是 $R_2 + \\gamma V(s_C) = 1 + (0.8)(-0.1) = 1 - 0.08 = 0.92$。\nTD 误差是 $\\delta_1 = 0.92 - V(s_B) = 0.92 - 0.1 = 0.82$。\n$V(s_B)$ 的新值为：\n$V(s_B) \\leftarrow V(s_B) + \\alpha \\delta_1 = 0.1 + (0.5)(0.82) = 0.1 + 0.41 = 0.51$。\n本次更新后，各价值为：$V(s_A) = 0.14$，$V(s_B) = 0.51$，$V(s_C) = -0.1$。\n\n**更新 3：转移 $s_C \\rightarrow s_B$，奖励 $R_3 = -0.5$。**\n更新状态 $s_C$ 的价值。我们使用当前值 $V(s_C) = -0.1$ 和最新更新的价值 $V(s_B) = 0.51$。\nTD 目标是 $R_3 + \\gamma V(s_B) = -0.5 + (0.8)(0.51) = -0.5 + 0.408 = -0.092$。\nTD 误差是 $\\delta_2 = -0.092 - V(s_C) = -0.092 - (-0.1) = 0.008$。\n$V(s_C)$ 的新值为：\n$V(s_C) \\leftarrow V(s_C) + \\alpha \\delta_2 = -0.1 + (0.5)(0.008) = -0.1 + 0.004 = -0.096$。\n本次更新后，各价值为：$V(s_A) = 0.14$，$V(s_B) = 0.51$，$V(s_C) = -0.096$。\n\n**更新 4：转移 $s_B \\rightarrow s_T$，奖励 $R_4 = 2$。**\n状态 $s_B$ 的价值被第二次更新。我们使用当前值 $V(s_B) = 0.51$ 和固定的终止状态价值 $V(s_T) = 0$。\nTD 目标是 $R_4 + \\gamma V(s_T) = 2 + (0.8)(0) = 2$。\nTD 误差是 $\\delta_3 = 2 - V(s_B) = 2 - 0.51 = 1.49$。\n$V(s_B)$ 的新值为：\n$V(s_B) \\leftarrow V(s_B) + \\alpha \\delta_3 = 0.51 + (0.5)(1.49) = 0.51 + 0.745 = 1.255$。\n\n第四次更新后，$V(s_B)$ 的值为 $1.255$。",
            "answer": "$$\\boxed{1.255}$$"
        },
        {
            "introduction": "时序差分学习的真正力量在于它能够为大脑中的学习机制提供一个计算模型。本练习将 TD 学习从纯粹的算法领域带入计算神经科学的应用中，用以模拟经典的消退学习（extinction learning）现象 。你将看到，当一个先前被预测会产生的奖励不再出现时，持续的负向预测误差（即结果比预期的要差）会如何驱动价值估计逐渐下降。通过推导价值随时间变化的封闭形式表达式并将其付诸实践，你将深刻理解 TD 误差作为学习信号的动态作用。",
            "id": "4026721",
            "problem": "给定一个用于大脑建模和计算神经科学的最小马尔可夫奖励过程，该过程模拟了巴甫洛夫式条件反射和消退学习。存在两种状态：一个线索状态 $C$ 和一个终止状态 $T$。每次试验恰好包含 $2$ 个时间步：在时间步 $t$，智能体处于状态 $C$；在时间步 $t+1$，智能体转移到状态 $T$ 并接收一个标量奖励 $r_t$。终止状态 $T$ 是吸收性的，其价值为 $V(T) = 0$。在变化点之前，期望奖励是一个常数 $\\mu_{\\text{pre}}$。在变化点之后，期望奖励是一个常数 $\\mu_{\\text{post}}$，且满足 $\\mu_{\\text{post}} \\le \\mu_{\\text{pre}}$。预训练已收敛，因此在变化点时刻，状态 $C$ 的价值估计为 $V_0 = \\mu_{\\text{pre}}$。\n\n对于学习过程，请使用以下时序差分学习的核心定义。在时间步 $t$，状态 $s_t$ 的时序差分误差 $\\delta_t$ 定义为 $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$，其中 $\\gamma \\in [0,1)$ 是折扣因子。价值更新规则为 $V(s_t) \\leftarrow V(s_t) + \\alpha \\, \\delta_t$，其中 $\\alpha \\in [0,1]$ 是一个恒定的学习率。假设只有一个非终止状态 $C$，因此唯一被更新的价值是 $V(C)$，我们将其记为 $k$ 次变化后试验后的 $V_k$。在变化点之后，每次试验的奖励都是确定性的，为 $r_t = \\mu_{\\text{post}}$。\n\n任务：\n1. 仅使用上述定义和所描述的试验结构，推导在固定学习率 $\\alpha$ 和变化后恒定奖励 $\\mu_{\\text{post}}$ 条件下，序列 $\\{V_k\\}_{k \\ge 0}$ 的闭式表达式。根据时序差分误差随试验变化的符号和大小来解释消退学习，并确定这些误差为负的条件。\n2. 实现一个算法，给定 $V_0$、$\\mu_{\\text{post}}$、$\\alpha$ 和一个整数 $n \\ge 0$，计算 $V_n$，即 $n$ 次变化后试验后的价值估计。\n3. 你的实现必须使用推导出的闭式表达式或你的推导所隐含的逻辑等价计算，并且必须生成四舍五入到 $10$ 位小数的输出。\n\n测试套件：\n为以下参数集提供输出，每个参数集定义为一个元组 $(V_0, \\mu_{\\text{post}}, \\alpha, n)$：\n- 案例 A（正常路径，完全消退）：$(1.0, 0.0, 0.2, 10)$。\n- 案例 B（部分消退至较低的渐近线）：$(1.0, 0.3, 0.1, 20)$。\n- 案例 C（边界学习率，立即更新）：$(1.0, 0.0, 1.0, 3)$。\n- 案例 D（不学习的边缘情况）：$(0.75, 0.0, 0.0, 50)$。\n- 案例 E（多次试验后的缓慢消退）：$(0.8, 0.0, 0.05, 100)$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[result\\_A,result\\_B,result\\_C,result\\_D,result\\_E]$）。每个 $result\\_X$ 必须是相应案例的 $V_n$ 的浮点值，四舍五入到 $10$ 位小数。不应打印任何其他文本。",
            "solution": "该问题要求推导在线索状态 $V(C)$ 在一个由时序差分（TD）学习建模的消退学习范式中的价值的闭式表达式，并实现这个表达式。\n\n首先，我们将学习过程形式化为一个递推关系。设 $V_k$ 表示在变化点后的第 $k$ 次试验后线索状态 $C$ 的价值估计，其中 $k \\ge 0$。通过预训练建立的初始价值为 $V_0 = \\mu_{\\text{pre}}$。\n\n对于任何试验 $k \\ge 1$，智能体从状态 $s_t = C$ 开始，其当前价值估计为 $V_{k-1}$。然后，智能体转移到终止状态 $s_{t+1} = T$ 并接收奖励 $r_t = \\mu_{\\text{post}}$。终止状态的价值给定为 $V(T) = 0$。\n\n根据所提供的定义，第 $k$ 次试验的时序差分误差（我们记为 $\\delta_{k-1}$）是：\n$$\n\\delta_{k-1} = r_t + \\gamma V(s_{t+1}) - V(s_t)\n$$\n代入试验结构的给定值：\n$$\n\\delta_{k-1} = \\mu_{\\text{post}} + \\gamma(0) - V_{k-1} = \\mu_{\\text{post}} - V_{k-1}\n$$\n然后使用学习率 $\\alpha \\in [0, 1]$ 更新状态 $C$ 的价值：\n$$\nV_k \\leftarrow V_{k-1} + \\alpha \\delta_{k-1}\n$$\n这为我们提供了序列 $\\{V_k\\}$ 的以下一阶线性递推关系：\n$$\nV_k = V_{k-1} + \\alpha (\\mu_{\\text{post}} - V_{k-1}) = (1-\\alpha)V_{k-1} + \\alpha \\mu_{\\text{post}}\n$$\n这个关系描述了线索状态的价值估计如何逐次试验地演变。折扣因子 $\\gamma$ 没有出现在最终的递推关系中，因为后继状态是价值为 $0$ 的终止状态。\n\n为了推导 $V_k$ 的闭式表达式，我们求解这个递推关系。一种标准方法是分析系统与不动点 $V_{\\infty}$ 的偏差。不动点是指价值不再发生变化的那个值，即 $V_k = V_{k-1} = V_{\\infty}$。\n$$\nV_{\\infty} = (1-\\alpha)V_{\\infty} + \\alpha \\mu_{\\text{post}} \\implies \\alpha V_{\\infty} = \\alpha \\mu_{\\text{post}}\n$$\n对于非零学习率 $\\alpha  0$，价值收敛到 $V_{\\infty} = \\mu_{\\text{post}}$，即变化点后的真实期望奖励。如果 $\\alpha=0$，价值将保持固定在 $V_0$。\n\n让我们将与渐近值的差定义为 $\\Delta_k = V_k - \\mu_{\\text{post}}$。我们可以为 $\\Delta_k$ 写出一个递推关系：\n$$\n\\Delta_k = V_k - \\mu_{\\text{post}} = [(1-\\alpha)V_{k-1} + \\alpha \\mu_{\\text{post}}] - \\mu_{\\text{post}}\n$$\n$$\n\\Delta_k = (1-\\alpha)V_{k-1} - (1-\\alpha)\\mu_{\\text{post}} = (1-\\alpha)(V_{k-1} - \\mu_{\\text{post}})\n$$\n$$\n\\Delta_k = (1-\\alpha)\\Delta_{k-1}\n$$\n这是一个简单的等比数列。通过归纳法，我们可以用初始差值 $\\Delta_0$ 来表示 $\\Delta_k$：\n$$\n\\Delta_k = (1-\\alpha)^k \\Delta_0\n$$\n初始差值为 $\\Delta_0 = V_0 - \\mu_{\\text{post}}$。代回后，我们得到：\n$$\nV_k - \\mu_{\\text{post}} = (1-\\alpha)^k (V_0 - \\mu_{\\text{post}})\n$$\n这就得出了 $k$ 次试验后价值估计 $V_k$ 的闭式解：\n$$\nV_k = \\mu_{\\text{post}} + (1-\\alpha)^k (V_0 - \\mu_{\\text{post}})\n$$\n问题要求的是 $n$ 次变化后试验后的价值 $V_n$，这可由该公式在 $k=n$ 时直接给出。\n\n当一个已习得的正向关联因奖励的移除或减少而被削弱时，就会发生消退学习。在这个模型中，这对应于 $\\mu_{\\text{post}}  \\mu_{\\text{pre}} = V_0$。第 $k$ 次试验的 TD 误差 $\\delta_{k-1} = \\mu_{\\text{post}} - V_{k-1}$ 代表了预测误差。我们可以使用闭式解来表示这个误差：\n$$\n\\delta_{k-1} = \\mu_{\\text{post}} - \\left( \\mu_{\\text{post}} + (1-\\alpha)^{k-1} (V_0 - \\mu_{\\text{post}}) \\right) = -(1-\\alpha)^{k-1} (V_0 - \\mu_{\\text{post}})\n$$\n在消退学习的条件下，$V_0  \\mu_{\\text{post}}$，因此项 $(V_0 - \\mu_{\\text{post}})$ 是正的。对于学习率 $\\alpha \\in (0, 1]$，当 $k  1$ 时，项 $(1-\\alpha)^{k-1}$ 是非负且小于 $1$ 的。因此，对于所有试验，TD 误差 $\\delta_{k-1}$ 都是负的（假设 $\\alpha  0$）。一个负的 TD 误差表明结果比预期的要差，从而将价值估计向下驱动，趋向于新的、更低的奖励期望 $\\mu_{\\text{post}}$。这个误差的大小 $|\\delta_{k-1}|$ 随着每次试验呈指数级下降，表明系统的预测正在改善并向新的现实收敛。TD 误差为负的条件是 $V_{k-1}  \\mu_{\\text{post}}$，只要价值是从上方收敛（$V_0  \\mu_{\\text{post}}$）且学习率不大到足以引起过冲（在此处不可能，因为 $\\alpha \\le 1$），这个条件就成立。\n\n该实现将对每个测试案例直接计算 $V_n = \\mu_{\\text{post}} + (1-\\alpha)^n (V_0 - \\mu_{\\text{post}})$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the value estimate V_n after n post-change trials for a set of test cases\n    based on the derived closed-form solution for a temporal difference learning model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (V0, mu_post, alpha, n).\n    test_cases = [\n        (1.0, 0.0, 0.2, 10),    # Case A (happy path, full extinction)\n        (1.0, 0.3, 0.1, 20),    # Case B (partial extinction to a lower asymptote)\n        (1.0, 0.0, 1.0, 3),     # Case C (boundary learning rate, immediate update)\n        (0.75, 0.0, 0.0, 50),   # Case D (no learning edge case)\n        (0.8, 0.0, 0.05, 100),  # Case E (slow extinction over many trials)\n    ]\n\n    results = []\n    for V0, mu_post, alpha, n in test_cases:\n        # The closed-form expression derived for the value estimate V_n after n trials is:\n        # V_n = mu_post + (1 - alpha)^n * (V0 - mu_post)\n        \n        # This formula is computationally robust for the given constraints.\n        # If alpha = 1.0, (1 - alpha) = 0. V_n becomes mu_post for n >= 1.\n        # If alpha = 0.0, (1 - alpha) = 1. V_n remains V0.\n        \n        # Using floating point arithmetic\n        base = 1.0 - alpha\n        \n        # Calculate the exponential term\n        decay_factor = base ** n\n        \n        # Calculate the final value estimate\n        Vn = mu_post + decay_factor * (V0 - mu_post)\n        \n        # Round the result to 10 decimal places as required.\n        results.append(round(Vn, 10))\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of float values enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了如何使用预测误差来评估状态（“这个状态有多好？”）之后，一个自然而然的进阶是利用这个信号来改进未来的决策（“在这种状态下我应该做什么？”）。本练习将你引入行动者-评论家（Actor-Critic）框架，这是强化学习中的一个核心思想，也被广泛认为是理解基底节功能的一个关键模型 。通过一个具体的计算实例，你将学习到同一个时序差分预测误差信号如何兵分两路：一路用于更新价值函数（评论家），另一路则用于调整策略参数，使未来更有可能选择那些带来积极意外的行动（行动者）。",
            "id": "4026719",
            "problem": "考虑一个具有两个状态 $s_A$ 和 $s_B$、两个动作 $a_1$ 和 $a_2$ 的马尔可夫决策过程（MDP），其转移是确定性的，由以下观测到的轨迹定义：在时间 $t=0$ 时，智能体从 $s_A$ 开始，采取动作 $a_1$，获得奖励 $r_0 = 0$，并转移到 $s_B$；在时间 $t=1$ 时，在 $s_B$ 中，智能体采取动作 $a_2$，获得奖励 $r_1 = 1$，并转移到一个终止状态。折扣因子为 $\\gamma = 0.9$。评论家（Critic）通过线性函数 $V(s; \\mathbf{w}) = \\mathbf{w}^{\\top} \\boldsymbol{\\phi}(s)$ 来表示状态价值函数 $V(s)$，其中固定的特征向量为 $\\boldsymbol{\\phi}(s_A) = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ 和 $\\boldsymbol{\\phi}(s_B) = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$。初始评论家参数为 $\\mathbf{w}_0 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix}$。行动家（Actor）在每个状态 $s$ 使用伯努利策略，该策略带有一个特定于状态的 logit 参数 $\\theta_s$，其中 $\\pi(a_1 \\mid s; \\theta_s)$ 是在状态 $s$ 中选择 $a_1$ 的概率，且 $\\pi(a_2 \\mid s; \\theta_s) = 1 - \\pi(a_1 \\mid s; \\theta_s)$。从 $\\theta_s$ 到动作概率的映射由逻辑斯谛函数 $\\sigma(\\theta_s)$ 给出，因此 $\\pi(a_1 \\mid s; \\theta_s) = \\sigma(\\theta_s)$ 且 $\\pi(a_2 \\mid s; \\theta_s) = 1 - \\sigma(\\theta_s)$，其中 $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$。初始行动家参数为 $\\theta_{A,0} = 0$ 和 $\\theta_{B,0} = 0$。评论家每次更新使用恒定的步长 $\\alpha_w = 0.5$，行动家每次更新使用恒定的步长 $\\alpha_{\\theta} = 0.1$。\n\n基于计算神经科学中的时序差分学习和多巴胺能奖励预测误差的基础，将时序差分误差 $\\delta_t$ 视为奖励预测误差信号，该信号驱动评论家的半梯度价值更新和行动家的策略参数更新，以处理观测到的轨迹。顺序处理轨迹，在每个时间步根据当前参数计算出的 $\\delta_t$ 在线更新评论家和行动家。推导出每个时序差分误差，在每个步骤中应用评论家的更新，并使用在选定动作的状态下该动作的对数概率的梯度来应用行动家的参数更新。完成两个时间步后，确定行动家参数 $\\theta_B$ 的最终值。\n\n将你的最终答案表示为一个实数。不需要四舍五入。",
            "solution": "### 第 1 步：提取已知条件\n-   状态：$s_A$，$s_B$ 和一个终止状态 $s_T$。\n-   动作：$a_1$，$a_2$。\n-   轨迹：\n    -   在 $t=0$ 时：状态 $s_0=s_A$，动作 $a_0=a_1$，奖励 $r_0=0$，下一状态 $s_1=s_B$。\n    -   在 $t=1$ 时：状态 $s_1=s_B$，动作 $a_1=a_2$，奖励 $r_1=1$，下一状态 $s_2=s_T$。\n-   折扣因子：$\\gamma = 0.9$。\n-   评论家价值函数近似：$V(s; \\mathbf{w}) = \\mathbf{w}^{\\top} \\boldsymbol{\\phi}(s)$。\n-   特征向量：$\\boldsymbol{\\phi}(s_A) = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，$\\boldsymbol{\\phi}(s_B) = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$。\n-   初始评论家参数：$\\mathbf{w}_0 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix}$。\n-   行动家策略：$\\pi(a_1 \\mid s; \\theta_s) = \\sigma(\\theta_s) = \\frac{1}{1 + \\exp(-\\theta_s)}$，且 $\\pi(a_2 \\mid s; \\theta_s) = 1 - \\sigma(\\theta_s)$。\n-   初始行动家参数：$\\theta_{A,0} = 0$，$\\theta_{B,0} = 0$。\n-   步长：$\\alpha_w = 0.5$ (评论家)，$\\alpha_{\\theta} = 0.1$ (行动家)。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题是计算强化学习中的一个标准练习，具体是使用在线、单步行动家-评论家（Actor-Critic）方法，其中评论家使用线性函数近似，行动家使用参数化策略。\n-   **科学依据**：该模型基于时序差分学习的公认原理，这是现代强化学习理论的基石，并与大脑功能模型有紧密联系。所有组件（MDP、TD 误差、半梯度更新、策略梯度）都是标准的且在数学上是合理的。\n-   **适定性**：所有必要的参数、初始条件和更新规则都已明确给出。轨迹是确定性的且有限的。该问题是自包含的，并允许对最终参数值进行唯一的、逐步的计算。\n-   **客观性**：该问题以精确的数学和算法术语陈述，没有歧义或主观论断。\n\n### 第 3 步：结论与行动\n问题有效。将推导出一个完整的解。\n\n### 推导\n求解过程需要顺序处理这个两步轨迹，在每个步骤中对评论家的权重 $\\mathbf{w}$ 和行动家的参数 $\\theta$ 进行在线更新。时序差分（TD）误差 $\\delta_t$ 作为共同的误差信号。\n\n**初始状态 ($t=0$):**\n初始参数为 $\\mathbf{w}_0 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix}$，$\\theta_{A,0}=0$ 和 $\\theta_{B,0}=0$。\n特征表示意味着 $V(s_A; \\mathbf{w}) = w_A$ 和 $V(s_B; \\mathbf{w}) = w_B$。\n因此，初始价值估计为：\n$V(s_A; \\mathbf{w}_0) = 0.4$\n$V(s_B; \\mathbf{w}_0) = -0.2$\n\n**处理时间步 $t=0$:**\n智能体处于状态 $s_0=s_A$，采取动作 $a_0=a_1$，获得奖励 $r_0=0$，并转移到状态 $s_1=s_B$。\n\n1.  **计算 TD 误差 $\\delta_0$**：\n    TD 误差定义为 $\\delta_t = r_t + \\gamma V(s_{t+1}; \\mathbf{w}_t) - V(s_t; \\mathbf{w}_t)$。\n    对于 $t=0$，我们使用初始权重 $\\mathbf{w}_0$：\n    $$ \\delta_0 = r_0 + \\gamma V(s_1; \\mathbf{w}_0) - V(s_0; \\mathbf{w}_0) $$\n    $$ \\delta_0 = 0 + 0.9 \\times V(s_B; \\mathbf{w}_0) - V(s_A; \\mathbf{w}_0) $$\n    $$ \\delta_0 = 0.9 \\times (-0.2) - 0.4 = -0.18 - 0.4 = -0.58 $$\n\n2.  **将评论家的权重更新为 $\\mathbf{w}_1$**：\n    评论家的半梯度更新规则是 $\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha_w \\delta_t \\nabla_{\\mathbf{w}} V(s_t; \\mathbf{w}_t)$。对于线性近似，$\\nabla_{\\mathbf{w}} V(s; \\mathbf{w}) = \\boldsymbol{\\phi}(s)$。\n    $$ \\mathbf{w}_1 = \\mathbf{w}_0 + \\alpha_w \\delta_0 \\boldsymbol{\\phi}(s_0) = \\mathbf{w}_0 + \\alpha_w \\delta_0 \\boldsymbol{\\phi}(s_A) $$\n    $$ \\mathbf{w}_1 = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix} + 0.5 \\times (-0.58) \\times \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0.4 \\\\ -0.2\\end{pmatrix} + \\begin{pmatrix}-0.29 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix} $$\n    新的权重为 $\\mathbf{w}_1 = \\begin{pmatrix}w_{A,1} \\\\ w_{B,1}\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix}$。\n\n3.  **更新状态 $s_A$ 的行动家参数**：\n    参数 $\\theta_s$ 的行动家更新规则是 $\\theta_{s,t+1} = \\theta_{s,t} + \\alpha_{\\theta} \\delta_t \\nabla_{\\theta_s} \\ln \\pi(a_t \\mid s_t; \\theta_{s,t})$。\n    在 $t=0$ 时，我们更新 $\\theta_A$，因为动作是在 $s_A$ 中采取的。参数 $\\theta_B$ 不更新。\n    动作 $a_1$ 的对数概率的梯度为：\n    $$ \\nabla_{\\theta_s} \\ln \\pi(a_1 \\mid s; \\theta_s) = \\frac{\\partial}{\\partial \\theta_s} \\ln(\\sigma(\\theta_s)) = 1 - \\sigma(\\theta_s) = 1 - \\pi(a_1 \\mid s; \\theta_s) $$\n    在 $t=0$ 时，相关参数为 $\\theta_{A,0}=0$。所采取动作 $a_1$ 的概率为 $\\pi(a_1 \\mid s_A; \\theta_{A,0}) = \\sigma(0) = \\frac{1}{1+\\exp(0)} = 0.5$。\n    梯度值为 $1 - 0.5 = 0.5$。\n    $\\theta_A$ 的更新为：\n    $$ \\theta_{A,1} = \\theta_{A,0} + \\alpha_{\\theta} \\delta_0 \\nabla_{\\theta_{A,0}} \\ln \\pi(a_1 \\mid s_A; \\theta_{A,0}) $$\n    $$ \\theta_{A,1} = 0 + 0.1 \\times (-0.58) \\times 0.5 = -0.029 $$\n    状态 $s_B$ 的参数保持不变：$\\theta_{B,1} = \\theta_{B,0} = 0$。\n\n**处理时间步 $t=1$:**\n智能体处于状态 $s_1=s_B$，采取动作 $a_1=a_2$，获得奖励 $r_1=1$，并转移到终止状态 $s_2=s_T$。根据定义，终止状态的价值为 $V(s_T)=0$。\n当前参数为 $\\mathbf{w}_1 = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix}$ 和 $\\theta_{B,1} = 0$。\n\n1.  **计算 TD 误差 $\\delta_1$**：\n    我们使用更新后的权重 $\\mathbf{w}_1$。\n    $$ \\delta_1 = r_1 + \\gamma V(s_2; \\mathbf{w}_1) - V(s_1; \\mathbf{w}_1) $$\n    $$ \\delta_1 = 1 + \\gamma V(s_T; \\mathbf{w}_1) - V(s_B; \\mathbf{w}_1) $$\n    $$ \\delta_1 = 1 + 0.9 \\times 0 - (\\mathbf{w}_1^\\top \\boldsymbol{\\phi}(s_B)) = 1 - (-0.2) = 1.2 $$\n\n2.  **将评论家的权重更新为 $\\mathbf{w}_2$**：\n    $$ \\mathbf{w}_2 = \\mathbf{w}_1 + \\alpha_w \\delta_1 \\boldsymbol{\\phi}(s_1) = \\mathbf{w}_1 + \\alpha_w \\delta_1 \\boldsymbol{\\phi}(s_B) $$\n    $$ \\mathbf{w}_2 = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix} + 0.5 \\times 1.2 \\times \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ -0.2\\end{pmatrix} + \\begin{pmatrix}0 \\\\ 0.6\\end{pmatrix} = \\begin{pmatrix}0.11 \\\\ 0.4\\end{pmatrix} $$\n\n3.  **更新状态 $s_B$ 的行动家参数**：\n    在 $t=1$ 时，我们更新 $\\theta_B$，因为动作是在 $s_B$ 中采取的。参数 $\\theta_A$ 不更新。问题要求的是 $\\theta_B$ 的最终值。\n    动作 $a_2$ 的对数概率的梯度为：\n    $$ \\nabla_{\\theta_s} \\ln \\pi(a_2 \\mid s; \\theta_s) = \\frac{\\partial}{\\partial \\theta_s} \\ln(1 - \\sigma(\\theta_s)) = -\\sigma(\\theta_s) = -\\pi(a_1 \\mid s; \\theta_s) $$\n    在 $t=1$ 时，相关参数为 $\\theta_{B,1}=0$。在状态 $s_B$ 中采取动作 $a_1$ 的概率为 $\\pi(a_1 \\mid s_B; \\theta_{B,1}) = \\sigma(0) = 0.5$。\n    梯度值为 $-0.5$。\n    $\\theta_B$ 的更新为：\n    $$ \\theta_{B,2} = \\theta_{B,1} + \\alpha_{\\theta} \\delta_1 \\nabla_{\\theta_{B,1}} \\ln \\pi(a_2 \\mid s_B; \\theta_{B,1}) $$\n    $$ \\theta_{B,2} = 0 + 0.1 \\times 1.2 \\times (-0.5) $$\n    $$ \\theta_{B,2} = 0.1 \\times (-0.6) = -0.06 $$\n\n轨迹处理完毕。行动家参数 $\\theta_B$ 的最终值为 $\\theta_{B,2}$。",
            "answer": "$$ \\boxed{-0.06} $$"
        }
    ]
}