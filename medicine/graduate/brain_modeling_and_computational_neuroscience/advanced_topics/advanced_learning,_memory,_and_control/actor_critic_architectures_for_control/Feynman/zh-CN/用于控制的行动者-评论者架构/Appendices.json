{
    "hands_on_practices": [
        {
            "introduction": "在行动家-评论家（actor-critic）方法中，确保评论家为行动家提供有用的学习信号至关重要。本练习旨在探讨“兼容函数近似”定理，该定理为评论家提供了确保其更新是真实策略梯度的无偏估计的条件。通过构建一个兼容的评论家并计算最终的策略梯度，您将对如何从理论上协调行动家和评论家以实现稳定学习获得具体的理解。",
            "id": "3962001",
            "problem": "一个智能体在固定状态下执行单步控制。行动者（actor）是一个关于连续动作的随机策略，而评论者（critic）使用线性函数逼近器。该策略是高斯分布，其方差固定，均值由状态依赖特征的线性形式给出。控制成本是动作与状态的目标仿射函数之间偏差的二次函数。施加行动者和评论者之间的兼容性条件，以确保使用评论者计算的行动者更新与真实策略梯度一致。\n\n考虑单个状态 $s \\in \\mathbb{R}$，其状态特征向量 $x(s) \\in \\mathbb{R}^{2}$ 定义为 $x(s) = \\begin{pmatrix} s \\\\ s^{3} \\end{pmatrix}$。行动者的策略是一个高斯分布 $\\pi_{\\theta}(a \\mid s) = \\mathcal{N}(\\mu_{\\theta}(s), \\sigma^{2})$，其方差固定为 $\\sigma^{2} = 1$，均值为 $\\mu_{\\theta}(s) = \\theta^{\\top} x(s)$，其中 $\\theta \\in \\mathbb{R}^{2}$ 是参数向量。在状态 $s$ 和动作 $a$ 下的即时控制成本（负奖励）为\n$$\n\\ell(s,a) = \\frac{1}{2}\\,q\\,(a - \\kappa s)^{2} + \\frac{1}{2}\\,c\\,s^{2},\n$$\n其中 $q > 0$，$\\kappa \\in \\mathbb{R}$，$c > 0$。对于此单步问题，动作-价值函数为 $Q(s,a) = -\\ell(s,a)$。评论者使用线性函数类 $Q_{w}(s,a) = \\phi(s,a)^{\\top} w$，$w \\in \\mathbb{R}^{2}$，其中特征映射 $\\phi(s,a)$ 必须满足行动者和评论者之间的兼容性要求。\n\n任务：\n1. 使用行动者-评论者架构中函数逼近的兼容性定义，构建与上述高斯行动者兼容的评论者特征向量 $\\phi(s,a)$。\n2. 对于固定状态 $s = 1.5$，参数 $\\theta = \\begin{pmatrix} 0.4 \\\\ -0.1 \\end{pmatrix}$，以及控制成本系数 $q = 2$，$\\kappa = 0.8$, $c = 0.4$，显式计算行动者更新向量\n$$\ng(\\theta) \\equiv \\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot \\mid s)}\\!\\left[\\,\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s)\\,Q_{w}(s,a)\\,\\right]\n$$\n，其中评论者参数 $w$ 的选择是为了在给定状态 $s$ 下，根据策略诱导的动作分布，求解 $Q(s,a)$ 到 $\\phi(s,a)$ 所张成空间上的最小二乘投影。将 $g(\\theta)$ 的最终数值答案表示为使用 LaTeX $\\mathrm{pmatrix}$ 环境的单个行向量。不要四舍五入；以有限小数或分数形式提供精确值。",
            "solution": "用户提供了一个来自强化学习和控制理论领域的有效问题陈述。该问题提法恰当，有科学依据，并包含推导出唯一解所需的所有必要信息。\n\n问题分为两个任务。第一，为评论者构建一个兼容的特征向量。第二，为一组特定参数计算行动者更新向量。\n\n**任务1：构建评论者的特征向量 $\\phi(s,a)$**\n\n带函数逼近的行动者-评论者方法的理论建立了一个行动者和评论者之间的兼容性条件，以确保策略梯度是无偏的。对于形式为 $Q_{w}(s,a) = \\phi(s,a)^{\\top} w$ 的线性评论者，如果评论者的特征向量 $\\phi(s,a)$ 被选为行动者策略的得分函数（score function），即对数策略关于其参数 $\\theta$ 的梯度，则可实现兼容性。\n$$\n\\phi(s,a) = \\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s)\n$$\n行动者的策略是一个高斯分布 $\\pi_{\\theta}(a \\mid s) = \\mathcal{N}(\\mu_{\\theta}(s), \\sigma^{2})$，由概率密度函数给出：\n$$\n\\pi_{\\theta}(a \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(a - \\mu_{\\theta}(s))^2}{2\\sigma^2}\\right)\n$$\n策略的自然对数为：\n$$\n\\ln \\pi_{\\theta}(a \\mid s) = -\\frac{1}{2} \\ln(2\\pi\\sigma^2) - \\frac{(a - \\mu_{\\theta}(s))^2}{2\\sigma^2}\n$$\n为了求得关于 $\\theta$ 的梯度，我们对此表达式求导。第一项相对于 $\\theta$ 是一个常数。\n$$\n\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) = \\nabla_{\\theta} \\left( -\\frac{(a - \\mu_{\\theta}(s))^2}{2\\sigma^2} \\right)\n$$\n使用链式法则，我们得到：\n$$\n\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) = -\\frac{1}{2\\sigma^2} \\cdot 2(a - \\mu_{\\theta}(s)) \\cdot (-\\nabla_{\\theta} \\mu_{\\theta}(s)) = \\frac{a - \\mu_{\\theta}(s)}{\\sigma^2} \\nabla_{\\theta} \\mu_{\\theta}(s)\n$$\n策略均值定义为 $\\mu_{\\theta}(s) = \\theta^{\\top} x(s)$。它关于参数向量 $\\theta$ 的梯度是：\n$$\n\\nabla_{\\theta} \\mu_{\\theta}(s) = \\nabla_{\\theta} (\\theta^{\\top} x(s)) = x(s)\n$$\n将此代入得分函数的表达式，我们得到兼容的特征向量：\n$$\n\\phi(s,a) = \\frac{a - \\theta^{\\top} x(s)}{\\sigma^2} x(s)\n$$\n给定固定方差 $\\sigma^2 = 1$ 和状态特征向量 $x(s) = \\begin{pmatrix} s \\\\ s^{3} \\end{pmatrix}$，评论者的特征向量是：\n$$\n\\phi(s,a) = (a - \\theta^{\\top} x(s)) x(s) = (a - (\\theta_1 s + \\theta_2 s^3)) \\begin{pmatrix} s \\\\ s^{3} \\end{pmatrix}\n$$\n\n**任务2：计算行动者更新向量 $g(\\theta)$**\n\n行动者更新向量定义为：\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot \\mid s)}\\!\\left[\\,\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s)\\,Q_{w}(s,a)\\,\\right]\n$$\n问题陈述指出，评论者的权重向量 $w$ 是通过在策略诱导的动作分布下，将真实动作-价值函数 $Q(s,a)$ 投影到由 $\\phi(s,a)$ 张成的特征空间上的最小二乘法确定的。这意味着 $w$ 最小化 $\\mathbb{E}_{a \\sim \\pi_{\\theta}}[(Q_w(s,a) - Q(s,a))^2]$。解由正规方程给出：\n$$\n\\left( \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) \\phi(s,a)^{\\top}] \\right) w = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) Q(s,a)]\n$$\n将 $Q_w(s,a) = \\phi(s,a)^{\\top} w$ 和 $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) = \\phi(s,a)$ 代入 $g(\\theta)$ 的定义：\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) \\phi(s,a)^{\\top} w] = \\left( \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) \\phi(s,a)^{\\top}] \\right) w\n$$\n从正规方程可知，这意味行动者更新 $g(\\theta)$ 正是正规方程的右侧：\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) Q(s,a)] = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) Q(s,a)]\n$$\n这就是此单步问题的真实策略梯度。现在我们计算这个期望。真实的动作-价值函数是 $Q(s,a) = -\\ell(s,a) = -\\frac{1}{2}q(a-\\kappa s)^2 - \\frac{1}{2}c s^2$。\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}} \\left[ \\left(\\frac{a - \\mu_{\\theta}(s)}{\\sigma^2} x(s)\\right) \\left(-\\frac{1}{2}q(a-\\kappa s)^2 - \\frac{1}{2}c s^2\\right) \\right]\n$$\n我们可以提出不依赖于随机变量 $a$ 的项：\n$$\ng(\\theta) = \\frac{x(s)}{\\sigma^2} \\mathbb{E}_{a \\sim \\pi_{\\theta}} \\left[ (a - \\mu_{\\theta}(s)) \\left(-\\frac{q}{2}(a-\\kappa s)^2 - \\frac{c}{2}s^2\\right) \\right]\n$$\n让我们定义一个零均值随机变量 $\\epsilon = a - \\mu_{\\theta}(s)$。因为 $a \\sim \\mathcal{N}(\\mu_{\\theta}(s), \\sigma^2)$，所以 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$。动作为 $a = \\mu_{\\theta}(s) + \\epsilon$。期望变为：\n$$\n\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)} \\left[ \\epsilon \\left( -\\frac{q}{2}(\\mu_{\\theta}(s) + \\epsilon - \\kappa s)^2 - \\frac{c}{2}s^2 \\right) \\right]\n$$\n让我们展开期望内的项：\n$$\n\\epsilon \\left( -\\frac{q}{2} [(\\mu_{\\theta}(s) - \\kappa s) + \\epsilon]^2 - \\frac{c}{2}s^2 \\right) = \\epsilon \\left( -\\frac{q}{2} [(\\mu_{\\theta}(s) - \\kappa s)^2 + 2\\epsilon(\\mu_{\\theta}(s) - \\kappa s) + \\epsilon^2] - \\frac{c}{2}s^2 \\right)\n$$\n分配 $\\epsilon$：\n$$\n-\\frac{q}{2}(\\mu_{\\theta}(s) - \\kappa s)^2 \\epsilon - q(\\mu_{\\theta}(s) - \\kappa s) \\epsilon^2 - \\frac{q}{2}\\epsilon^3 - \\frac{c}{2}s^2 \\epsilon\n$$\n现在，我们对这个表达式求期望。我们使用零均值正态分布的矩：$\\mathbb{E}[\\epsilon] = 0$，$\\mathbb{E}[\\epsilon^2] = \\sigma^2$，以及 $\\mathbb{E}[\\epsilon^3] = 0$。\n$$\n\\mathbb{E}[\\dots] = -\\frac{q}{2}(\\mu_{\\theta}(s) - \\kappa s)^2 \\mathbb{E}[\\epsilon] - q(\\mu_{\\theta}(s) - \\kappa s) \\mathbb{E}[\\epsilon^2] - \\frac{q}{2}\\mathbb{E}[\\epsilon^3] - \\frac{c s^2}{2} \\mathbb{E}[\\epsilon]\n$$\n$$\n\\mathbb{E}[\\dots] = 0 - q(\\mu_{\\theta}(s) - \\kappa s) \\sigma^2 - 0 - 0 = -q \\sigma^2 (\\mu_{\\theta}(s) - \\kappa s)\n$$\n将此结果代回 $g(\\theta)$ 的表达式中：\n$$\ng(\\theta) = \\frac{x(s)}{\\sigma^2} \\left( -q \\sigma^2 (\\mu_{\\theta}(s) - \\kappa s) \\right) = -q x(s) (\\mu_{\\theta}(s) - \\kappa s)\n$$\n现在我们代入数值：$s = 1.5$，$\\theta = \\begin{pmatrix} 0.4 \\\\ -0.1 \\end{pmatrix}$，$q=2$，$\\kappa = 0.8$。\n首先，计算所需的分量：\n状态特征向量为 $x(s) = x(1.5) = \\begin{pmatrix} 1.5 \\\\ (1.5)^3 \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix}$。\n策略均值为 $\\mu_{\\theta}(s) = \\theta^{\\top} x(s) = \\begin{pmatrix} 0.4 & -0.1 \\end{pmatrix} \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix} = 0.4 \\times 1.5 - 0.1 \\times 3.375 = 0.6 - 0.3375 = 0.2625$。\n目标动作函数值为 $\\kappa s = 0.8 \\times 1.5 = 1.2$。\n差值为 $\\mu_{\\theta}(s) - \\kappa s = 0.2625 - 1.2 = -0.9375$。\n现在，组装最终的梯度向量 $g(\\theta)$：\n$$\ng(\\theta) = -2 \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix} (-0.9375)\n$$\n$$\ng(\\theta) = (2 \\times 0.9375) \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix} = 1.875 \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix}\n$$\n对每个分量进行最后的乘法运算：\n$$\ng_1(\\theta) = 1.875 \\times 1.5 = 2.8125\n$$\n$$\ng_2(\\theta) = 1.875 \\times 3.375 = 6.328125\n$$\n因此，行动者更新向量为 $g(\\theta) = \\begin{pmatrix} 2.8125 \\\\ 6.328125 \\end{pmatrix}$。问题要求答案为单个行向量。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.8125 & 6.328125\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "行动家-评论家算法的收敛性取决于对行动家和评论家学习率的精心设计。本练习将深入探讨双时间尺度随机近似理论，并将其应用于经典的线性二次调节器（LQR）问题。您还将从第一性原理出发，推导最优控制律，从而将强化学习与经典控制理论联系起来。通过分析步长方案和求解LQR问题，您将明晰算法收敛所需的理论条件，并为现代强化学习与成熟的最优控制解之间架起一座桥梁。",
            "id": "3962047",
            "problem": "考虑一个标量线性二次控制系统，其由离散时间动态 $x_{t+1} = a x_{t} + b u_{t}$ 和一个静态线性状态反馈 actor $u_{t} = -k_{t} x_{t}$ 定义。性能准则为无穷时域折扣成本 $J(k) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\left(q x_{t}^{2} + r u_{t}^{2}\\right)\\right]$，其中 $0 < \\gamma < 1$，$q > 0$ 且 $r > 0$。actor 参数 $k_{t}$ 使用双时间尺度 actor-critic 算法进行更新，该算法包含一个 critic 参数 $v_{t}$，通过时间差分 (TD) 学习来近似价值函数 $V(x) \\approx v x^{2}$；以及一个 actor 更新，该更新对期望折扣成本执行随机梯度下降。假设对于每个固定的 $k$，actor 的梯度估计是 $\\nabla J(k)$ 的无偏估计量，并且在线性二次结构下，对于固定的 $k$，critic 的 TD 误差在均值上定义了一个收缩。\n\n任务：\n1. 提出 critic 的步长序列 $\\{\\alpha_{t}\\}$ 和 actor 的步长序列 $\\{\\beta_{t}\\}$，使之满足 Robbins–Monro 随机逼近条件，并实现适当的双时间尺度分离（critic 比 actor 快）。明确验证所需的可和性。\n2. 从折扣线性二次控制问题的基本原理和策略 $u_{t} = -k x_{t}$ 的定义出发，推导出最小化 $J(k)$ 的最优反馈增益 $k^{\\star}$ 的精确表达式，用 $a$、$b$、$q$、$r$ 和 $\\gamma$ 表示你的结果。你的推导必须是自洽的，并且只使用给定的系统和成本定义。\n3. 使用任务2的结果，对参数值 $a = 0.9$，$b = 1$，$q = 1$，$r = 0.25$ 和 $\\gamma = 0.95$ 进行数值计算 $k^{\\star}$。将你的最终数值答案四舍五入到四位有效数字。只提供四舍五入后的 $k^{\\star}$ 值作为最终答案。\n\n你可以假设初始状态 $x_{0}$ 具有有限二阶矩，actor 参数序列被限制在一个紧集内，以确保在整个迭代过程中 $\\gamma (a - b k)^{2} < 1$，并且 TD 和梯度估计器中的噪声过程满足与随机逼近理论一致的标准鞅差和有界方差条件。",
            "solution": "用户提供了一个分为三部分的问题，涉及用于标量离散时间线性二次控制系统的双时间尺度 actor-critic 算法。该问题要求提出有效的步长序列，解析推导最优控制增益，并对该增益进行数值评估。\n\n### 问题验证\n\n首先，我将验证问题陈述。\n\n**第1步：提取给定条件**\n\n*   系统动态：$x_{t+1} = a x_{t} + b u_{t}$\n*   Actor 策略：$u_{t} = -k_{t} x_{t}$\n*   成本函数：$J(k) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\left(q x_{t}^{2} + r u_{t}^{2}\\right)\\right]$\n*   折扣因子：$0 < \\gamma < 1$\n*   成本权重：$q > 0$，$r > 0$\n*   Critic 近似：$V(x) \\approx v x^{2}$\n*   Critic 参数：$v_t$\n*   Actor 参数：$k_t$\n*   学习率：$\\{\\alpha_{t}\\}$ (critic)，$\\{\\beta_{t}\\}$ (actor)\n*   算法属性：双时间尺度 actor-critic，actor 使用随机梯度下降，critic 使用时间差分 (TD) 学习。\n*   假设：\n    *   Actor 的梯度估计是 $\\nabla J(k)$ 的无偏估计量。\n    *   对于固定的 $k$，critic 的 TD 误差在均值上定义了一个收缩。\n    *   步长必须满足 Robbins–Monro 条件。\n    *   Critic 的时间尺度比 Actor 快。\n    *   初始状态 $x_{0}$ 具有有限二阶矩。\n    *   Actor 参数序列 $\\{k_t\\}$ 被限制在一个紧集内，其中 $\\gamma (a - b k)^{2} < 1$。\n    *   噪声过程满足标准随机逼近条件。\n*   任务3的数值：$a = 0.9$，$b = 1$，$q = 1$，$r = 0.25$，$\\gamma = 0.95$。\n\n**第2步：使用提取的给定条件进行验证**\n\n*   **科学依据：** 该问题牢固地植根于最优控制理论（线性二次调节器）和强化学习（actor-critic 方法，随机逼近）的既定原则。该模型是分析此类算法的标准基准。所提供的假设在双时间尺度随机逼近算法收敛的理论分析中是标准的。该问题在科学上是合理的。\n*   **适定性：** 问题的各个部分都是适定的。任务1要求给出一个有效的步长序列示例，这样的示例有很多。任务2要求推导唯一的最优增益 $k^{\\star}$，已知在给定条件下（$q > 0, r > 0$），折扣 LQR 问题存在这样的增益。任务3是一个直接的数值计算。问题是自洽的，并提供了所有必要的信息。\n*   **客观性：** 问题以精确、形式化的数学语言陈述，没有任何主观性或歧义。\n\n**第3步：结论与行动**\n\n这个问题是有效的。它是一个适定的、有科学依据的问题，处于控制理论和机器学习的交叉领域。我将继续提供一个完整的解决方案。\n\n### 解决方案\n\n**任务1：步长序列**\n\n双时间尺度随机逼近算法的收敛要求两个步长序列，即快速（critic）过程的 $\\{\\alpha_{t}\\}$ 和慢速（actor）过程的 $\\{\\beta_{t}\\}$，都满足标准的 Robbins–Monro (RM) 条件。此外，还必须满足一个时间尺度分离条件。\n\n对于一个通用的步长序列 $\\{\\eta_t\\}$，RM 条件是：\n$$\n\\sum_{t=0}^{\\infty} \\eta_t = \\infty\n\\quad \\text{和} \\quad\n\\sum_{t=0}^{\\infty} \\eta_t^2 < \\infty\n$$\n第一个条件确保算法可以克服任何初始误差，而第二个条件确保每一步注入的噪声最终消失。\n\n使 critic 更新比 actor 更新“更快”的时间尺度分离条件是：\n$$\n\\lim_{t \\to \\infty} \\frac{\\beta_{t}}{\\alpha_{t}} = 0\n$$\n\n我们提出形式为 $\\alpha_t = (t+1)^{-p}$ 和 $\\beta_t = (t+1)^{-q}$ 的序列。\n为了使这些序列满足 RM 条件，指数 $p$ 和 $q$ 必须在区间 $(0.5, 1]$ 内。这是因为级数 $\\sum_{t=0}^{\\infty} (t+1)^{-c}$ 当 $c \\le 1$ 时发散，当 $c > 1$ 时收敛（根据积分判别法）。\n*   对于 $\\sum \\alpha_t = \\sum (t+1)^{-p} = \\infty$，我们需要 $p \\le 1$。\n*   对于 $\\sum \\alpha_t^2 = \\sum (t+1)^{-2p} < \\infty$，我们需要 $2p > 1$，或 $p > 0.5$。\n同样的逻辑适用于 $q$：$0.5 < q \\le 1$。\n\n对于时间尺度分离条件 $\\lim_{t \\to \\infty} \\beta_t / \\alpha_t = 0$，我们需要：\n$$\n\\lim_{t \\to \\infty} \\frac{(t+1)^{-q}}{(t+1)^{-p}} = \\lim_{t \\to \\infty} (t+1)^{p-q} = 0\n$$\n这个极限为零当且仅当指数 $p-q$ 为负，即 $p < q$。\n\n综合所有条件，我们必须选择 $p$ 和 $q$ 使得 $0.5 < p < q \\le 1$。\n一个有效的选择是设置 $p = \\frac{2}{3}$ 和 $q = 1$。让我们定义序列并进行验证。\n\n*   Critic (快速) 步长: $\\alpha_{t} = \\frac{1}{(t+1)^{2/3}}$\n*   Actor (慢速) 步长: $\\beta_{t} = \\frac{1}{t+1}$\n\n验证：\n1.  对于 $\\{\\alpha_t\\}$: $\\sum_{t=0}^{\\infty} \\alpha_t = \\sum_{t=0}^{\\infty} (t+1)^{-2/3}$ 是一个发散的p级数 ($p=2/3 \\le 1$)。$\\sum_{t=0}^{\\infty} \\alpha_t^2 = \\sum_{t=0}^{\\infty} (t+1)^{-4/3}$ 是一个收敛的p级数 ($p=4/3 > 1$)。RM 条件成立。\n2.  对于 $\\{\\beta_t\\}$: $\\sum_{t=0}^{\\infty} \\beta_t = \\sum_{t=0}^{\\infty} (t+1)^{-1}$ 是调和级数，它是一个发散的p级数 ($p=1 \\le 1$)。$\\sum_{t=0}^{\\infty} \\beta_t^2 = \\sum_{t=0}^{\\infty} (t+1)^{-2}$ 是一个收敛的p级数 ($p=2 > 1$)。RM 条件成立。\n3.  时间尺度分离: $\\lim_{t \\to \\infty} \\frac{\\beta_t}{\\alpha_t} = \\lim_{t \\to \\infty} \\frac{(t+1)^{-1}}{(t+1)^{-2/3}} = \\lim_{t \\to \\infty} (t+1)^{-1/3} = 0$。该条件得到满足。\n\n**任务2：最优增益 $k^{\\star}$ 的推导**\n\n最优控制增益 $k^{\\star}$ 可以从该折扣线性二次问题的贝尔曼最优性方程中推导出来。已知最优价值函数是二次的，即 $V^{\\star}(x) = P x^{2}$，其中 $P > 0$ 为某个常数。\n\n贝尔曼最优性方程是：\n$$\nV^{\\star}(x) = \\min_{u} \\left\\{ q x^{2} + r u^{2} + \\gamma V^{\\star}(x_{t+1}) \\right\\}\n$$\n代入 $V^{\\star}(x) = P x^{2}$ 和 $x_{t+1} = ax + bu$：\n$$\nP x^{2} = \\min_{u} \\left\\{ q x^{2} + r u^{2} + \\gamma P (ax+bu)^{2} \\right\\}\n$$\n为了找到最小化的控制输入 $u^{\\star}$，我们将最小值内的表达式对 $u$ 求导，并令结果为零：\n$$\n\\frac{\\partial}{\\partial u} \\left[ q x^{2} + r u^{2} + \\gamma P (ax+bu)^{2} \\right] = 2ru + \\gamma P \\cdot 2(ax+bu) \\cdot b = 0\n$$\n$$\nru + \\gamma P b (ax+bu) = 0\n$$\n$$\nu(r + \\gamma P b^{2}) = - \\gamma P a b x\n$$\n因此，最优控制律是一个线性状态反馈：\n$$\nu^{\\star} = - \\left( \\frac{\\gamma a b P}{r + \\gamma b^{2} P} \\right) x\n$$\n其形式为 $u^{\\star} = -k^{\\star} x$，其中最优增益 $k^{\\star}$ 是：\n$$\nk^{\\star} = \\frac{\\gamma a b P}{r + \\gamma b^{2} P}\n$$\n为了求出 $P$ 的值，我们将 $u^{\\star}$ 代回贝尔曼方程。该方程对于最小化的 $u^{\\star}$ 必须成立。\n$$\nP x^{2} = q x^{2} + r (u^{\\star})^{2} + \\gamma P (ax+bu^{\\star})^{2}\n$$\n除以 $x^2$（因为该方程必须对所有 $x$ 成立）并代入 $u^{\\star} = -k^{\\star}x$：\n$$\nP = q + r (k^{\\star})^{2} + \\gamma P (a-bk^{\\star})^{2}\n$$\n我们将 $k^{\\star}$ 的表达式代入此方程。先表达 $(a-bk^{\\star})$ 会更容易：\n$$\na - bk^{\\star} = a - b \\left(\\frac{\\gamma a b P}{r + \\gamma b^{2} P}\\right) = \\frac{a(r + \\gamma b^{2} P) - \\gamma a b^{2} P}{r + \\gamma b^{2} P} = \\frac{ar}{r + \\gamma b^{2} P}\n$$\n现在将 $k^{\\star}$ 和 $(a-bk^{\\star})$ 代入关于 $P$ 的方程：\n$$\nP = q + r \\left( \\frac{\\gamma a b P}{r + \\gamma b^{2} P} \\right)^{2} + \\gamma P \\left( \\frac{ar}{r + \\gamma b^{2} P} \\right)^{2}\n$$\n$$\nP = q + \\frac{r \\gamma^{2} a^{2} b^{2} P^{2} + \\gamma P a^{2} r^{2}}{(r + \\gamma b^{2} P)^{2}}\n$$\n$$\nP = q + \\frac{\\gamma a^{2} r P (r \\gamma b^2 P / (\\gamma a^2 r P) + r)}{ (r + \\gamma b^{2} P)^{2}} = q + \\frac{\\gamma a^{2} r P (r + \\gamma b^{2} P)}{(r + \\gamma b^{2} P)^{2}}\n$$\n$$\nP = q + \\frac{\\gamma a^{2} r P}{r + \\gamma b^{2} P}\n$$\n这个关于 $P$ 的方程是该折扣问题的标量离散时间代数黎卡提方程 (DARE)。我们将其重新整理为关于 $P$ 的二次方程：\n$$\nP(r + \\gamma b^{2} P) = q(r + \\gamma b^{2} P) + \\gamma a^{2} r P\n$$\n$$\nPr + \\gamma b^{2} P^{2} = qr + q\\gamma b^{2} P + \\gamma a^{2} r P\n$$\n$$\n(\\gamma b^{2}) P^{2} + (r - \\gamma q b^{2} - \\gamma a^{2} r) P - qr = 0\n$$\n这是一个形如 $A P^2 + B P + C = 0$ 的二次方程。因为 $q>0$ 且 $r>0$，与成本相关的 $P$ 值必须为正。系数 $A = \\gamma b^2 > 0$ 且系数 $C = -qr < 0$。根的乘积 $C/A$ 为负，因此有一个正实根和一个负实根。我们必须取正根：\n$$\nP = \\frac{-B + \\sqrt{B^{2} - 4AC}}{2A}\n$$\n其中 $A = \\gamma b^2$，$B = r - \\gamma(q b^2 + a^2 r)$，以及 $C = -qr$。\n然后通过将这个 $P$ 值代入其表达式来找到最优增益 $k^{\\star}$：$k^{\\star} = \\frac{\\gamma a b P}{r + \\gamma b^{2} P}$。\n\n**任务3：$k^{\\star}$ 的数值评估**\n\n给定参数：$a = 0.9$，$b = 1$，$q = 1$，$r = 0.25$，$\\gamma = 0.95$。\n\n首先，我们计算 $P$ 的黎卡提方程的系数：\n$A = \\gamma b^{2} = 0.95 \\times 1^{2} = 0.95$\n$B = r - \\gamma(q b^{2} + a^{2} r) = 0.25 - 0.95(1 \\times 1^{2} + 0.9^{2} \\times 0.25)$\n$B = 0.25 - 0.95(1 + 0.81 \\times 0.25) = 0.25 - 0.95(1 + 0.2025)$\n$B = 0.25 - 0.95(1.2025) = 0.25 - 1.142375 = -0.892375$\n$C = -qr = -1 \\times 0.25 = -0.25$\n\n$P$ 的方程是 $0.95 P^{2} - 0.892375 P - 0.25 = 0$。\n我们求解正根 $P$：\n$$\nP = \\frac{-(-0.892375) + \\sqrt{(-0.892375)^{2} - 4(0.95)(-0.25)}}{2(0.95)}\n$$\n$$\nP = \\frac{0.892375 + \\sqrt{0.7963320625 + 0.95}}{1.9}\n$$\n$$\nP = \\frac{0.892375 + \\sqrt{1.7463320625}}{1.9} \\approx \\frac{0.892375 + 1.32148858}{1.9}\n$$\n$$\nP \\approx \\frac{2.21386364}{1.9} \\approx 1.16519139\n$$\n现在，我们使用这个 $P$ 值来计算 $k^{\\star}$：\n$$\nk^{\\star} = \\frac{\\gamma a b P}{r + \\gamma b^{2} P} = \\frac{0.95 \\times 0.9 \\times 1 \\times P}{0.25 + 0.95 \\times 1^{2} \\times P} = \\frac{0.855 P}{0.25 + 0.95 P}\n$$\n代入 $P$ 的数值：\n$$\nk^{\\star} \\approx \\frac{0.855 \\times 1.16519139}{0.25 + 0.95 \\times 1.16519139}\n$$\n$$\nk^{\\star} \\approx \\frac{0.99623864}{0.25 + 1.10693182} = \\frac{0.99623864}{1.35693182} \\approx 0.7341829\n$$\n四舍五入到四位有效数字，我们得到 $k^{\\star} \\approx 0.7342$。",
            "answer": "$$\n\\boxed{0.7342}\n$$"
        },
        {
            "introduction": "现代的行动家-评论家方法常常需要从其他策略（即异策略学习）收集的数据中学习，这引入了高方差的挑战。本问题介绍了V-trace算法，这是一种强大的异策略校正技术，它通过截断重要性采样来平衡偏差和方差。通过推导和应用V-trace的目标值及其相应的校正优势函数，您将掌握一项关键机制，该机制使得在大型数据驱动应用中实现稳定高效的学习成为可能。",
            "id": "3962015",
            "problem": "考虑一个用于在马尔可夫决策过程 (MDP) 中进行控制的离策略 (off-policy) Actor-Critic 智能体。该 MDP 具有状态 $x_t$、动作 $a_t$、奖励 $r_t$ 和折扣因子 $\\gamma \\in (0,1)$。目标策略是由 $\\theta$ 参数化的 $\\pi_{\\theta}(a \\mid x)$，而数据是在另一个行为策略 $\\mu(a \\mid x)$ 下收集的。为了校正离策略采样，重要性采样 (IS) 使用每个时间步的比率 $r_t = \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}$。为了减少方差并稳定学习，我们考虑使用截断的重要性采样，其截断水平为 $\\bar{\\rho} > 0$ 和 $\\bar{c} > 0$，定义如下\n$$\n\\rho_t = \\min\\left(\\bar{\\rho}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right), \\quad c_t = \\min\\left(\\bar{c}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right).\n$$\nCritic 寻求一个多步目标，该目标使用截断的重要性采样来校正时序差分误差；而 Actor 寻求一个校正后的优势函数，该优势函数以一种与离策略学习保持一致的方式来缩放策略梯度 $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid x_t)$。\n\n仅从上述定义和强化学习 (RL) 的基本原则（状态价值 $V^{\\pi}(x)$、动作价值 $Q^{\\pi}(x,a)$ 以及 $Q^{\\pi}(x,a) = \\mathbb{E}\\left[r + \\gamma V^{\\pi}(x') \\mid x,a\\right]$）出发，推导 Critic 的截断重要性采样多步价值目标 $v_s$ 以及 Actor 在时间 $s$ 使用的相应校正优势函数，并用 $\\rho_t$、$c_t$、即时奖励、自举值和 $\\gamma$ 表示。然后，对于一个特定的3步轨迹片段 $t \\in \\{0,1,2\\}$（在 $t=3$ 时进行终端自举），使用以下数据计算在时间 $t=0$ 时的校正优势函数：\n- $\\gamma = 0.9$,\n- $\\bar{\\rho} = 1.3$, $\\bar{c} = 1.0$,\n- $\\mu(a_0 \\mid x_0) = 0.4$, $\\pi_{\\theta}(a_0 \\mid x_0) = 0.7$,\n- $\\mu(a_1 \\mid x_1) = 0.6$, $\\pi_{\\theta}(a_1 \\mid x_1) = 0.5$,\n- $\\mu(a_2 \\mid x_2) = 0.2$, $\\pi_{\\theta}(a_2 \\mid x_2) = 0.4$,\n- $r_0 = 1.2$, $r_1 = -0.3$, $r_2 = 0.5$,\n- $V(x_0) = 2.0$, $V(x_1) = 1.5$, $V(x_2) = 1.0$, $V(x_3) = 0$.\n\n将时间 $t=0$ 时的最终标量校正优势函数表示为一个实数，四舍五入到四位有效数字。无需单位。",
            "solution": "在进行解答之前，首先评估问题陈述的有效性。\n\n### 步骤 1：提取已知条件\n- **框架**: 马尔可夫决策过程 (MDP)，包含状态 $x_t$、动作 $a_t$、奖励 $r_t$。\n- **策略**: 目标策略 $\\pi_{\\theta}(a \\mid x)$，行为策略 $\\mu(a \\mid x)$。\n- **折扣因子**: $\\gamma \\in (0,1)$。\n- **重要性采样 (IS) 比率**:\n  - 问题首先将重要性采样比率定义为 $r_t = \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}$，但随后在数值计算部分，$r_t$ 被用来表示奖励。这是一种常见的符号重载，但为了清晰起见，我们将始终使用 $r_t$ 表示奖励，并直接使用 $\\frac{\\pi_{\\theta}}{\\mu}$ 来表示比率。\n  - 用于价值估计的截断比率：$\\rho_t = \\min\\left(\\bar{\\rho}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right)$。\n  - 用于递归更新的截断比率：$c_t = \\min\\left(\\bar{c}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right)$。\n  - 截断水平：$\\bar{\\rho} > 0$, $\\bar{c} > 0$。\n- **目标**:\n  1. 推导 Critic 的截断 IS 多步价值目标 $v_s$。\n  2. 推导 Actor 在时间 $s$ 使用的相应校正优势函数。\n  3. 对于一个给定的3步轨迹片段 $t \\in \\{0,1,2\\}$（在 $t=3$ 时进行终端自举），计算在时间 $t=0$ 时的校正优势函数。\n- **基本原则**: 状态价值 $V^{\\pi}(x)$、动作价值 $Q^{\\pi}(x,a)$ 以及贝尔曼期望方程 $Q^{\\pi}(x,a) = \\mathbb{E}\\left[r + \\gamma V^{\\pi}(x') \\mid x,a\\right]$。\n- **数值数据**:\n  - $\\gamma = 0.9$。\n  - $\\bar{\\rho} = 1.3$, $\\bar{c} = 1.0$。\n  - 策略概率：$\\mu(a_0 \\mid x_0) = 0.4$, $\\pi_{\\theta}(a_0 \\mid x_0) = 0.7$；$\\mu(a_1 \\mid x_1) = 0.6$, $\\pi_{\\theta}(a_1 \\mid x_1) = 0.5$；$\\mu(a_2 \\mid x_2) = 0.2$, $\\pi_{\\theta}(a_2 \\mid x_2) = 0.4$。\n  - 奖励：$r_0 = 1.2$, $r_1 = -0.3$, $r_2 = 0.5$。\n  - 价值函数估计：$V(x_0) = 2.0$, $V(x_1) = 1.5$, $V(x_2) = 1.0$, $V(x_3) = 0$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据。截断重要性采样比率 $\\rho_t$ 和 $c_t$ 的公式，以及推导多步离策略价值目标和校正优势函数的任务，是 V-trace 算法（Espeholt et al., 2018）的典型特征。V-trace 是现代强化学习中一种成熟且广泛用于大规模离策略 Actor-Critic 架构的方法。该问题定义明确，为获得唯一解提供了所有必要的定义和数据。语言客观且正式。问题陈述中对符号 $r_t$ 的双重使用是一个小瑕疵，但上下文使其含义清晰，不会妨碍求解。该问题不违反任何无效性标准。\n\n### 步骤 3：结论与行动\n问题有效。将提供解答。\n\n### 价值目标与校正优势函数的推导\n\n该问题描述了一个基于 V-trace 的学习过程。我们将根据其原理推导多步价值目标和校正优势函数的表达式。\n\n设 $V(x_t)$ 是价值函数的当前估计值。在时间 $t$ 的单步时序差分 (TD) 残差定义为：\n$$\n\\delta_t = r_t + \\gamma V(x_{t+1}) - V(x_t)\n$$\n该残差衡量了基于观测到的转移 $(x_t, a_t, r_t, x_{t+1})$ 的价值估计 $V(x_t)$ 中的误差。\n\n对于离策略学习，TD 残差通过重要性采样进行校正。V-trace 算法基于从时间 $s$ 到 $s+n-1$ 的轨迹片段，为状态 $x_s$ 定义了一个多步价值目标 $v_s$。该目标是通过累积折扣和重要性加权的 TD 残差来构建的。在时间 $t \\ge s$ 的价值目标 $v_t$ 的递归定义是：\n$$\nv_t = V(x_t) + \\rho_t (r_t + \\gamma V(x_{t+1}) - V(x_t)) + \\gamma c_t (v_{t+1} - V(x_{t+1}))\n$$\n在 $n$ 步处有一个终止条件，例如 $v_{s+n} = V(x_{s+n})$。\n\n将这个递归从 $t=s$ 展开到 $s+n-1$ 可以得到 Critic 的显式多步价值目标 $v_s$：\n$$\nv_s = V(x_s) + \\sum_{t=s}^{s+n-1} \\gamma^{t-s} \\left( \\prod_{i=s}^{t-1} c_i \\right) \\rho_t \\delta_t\n$$\n其中，当 $t=s$ 时，乘积项 $\\prod_{i=s}^{t-1} c_i$ 定义为 $1$。\n\nActor 使用策略梯度方法更新策略 $\\pi_{\\theta}$。V-trace 算法在时间 $s$ 使用一个校正后的优势估计值来缩放得分函数 $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_s \\mid x_s)$。我们将此优势函数表示为 $\\hat{A}_s$，其表达式为：\n$$\n\\hat{A}_s = \\rho_s (r_s + \\gamma v_{s+1} - V(x_s))\n$$\n在这里，$v_{s+1}$ 是后续状态 $x_{s+1}$ 的 V-trace 目标。这种形式与标准优势函数 $A(x,a) = Q(x,a) - V(x)$ 保持着密切的联系，但其中 $Q(x_s, a_s)$ 由 $r_s + \\gamma v_{s+1}$ 估计，而 $v_{s+1}$ 作为下一个状态价值 $V(x_{s+1})$ 的一个离策略校正估计。\n\n### 在 $t=0$ 时校正优势函数的数值计算\n\n我们需要计算在时间 $t=0$ 时的校正优势函数，即 $\\hat{A}_0 = \\rho_0 (r_0 + \\gamma v_1 - V(x_0))$。这需要我们首先计算中间量 $\\rho_t$、$c_t$、$\\delta_t$，然后计算价值目标 $v_1$。轨迹片段为 $t \\in \\{0,1,2\\}$ ($n=3$)，自举值为 $V(x_3)=0$。\n\n**1. 计算重要性采样比率及其截断版本**\n未截断的比率为 $\\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}$。给定 $\\bar{\\rho} = 1.3$ 和 $\\bar{c} = 1.0$：\n- 对于 $t=0$：比率 $= \\frac{0.7}{0.4} = 1.75$。\n  - $\\rho_0 = \\min(1.3, 1.75) = 1.3$。\n  - $c_0 = \\min(1.0, 1.75) = 1.0$。\n- 对于 $t=1$：比率 $= \\frac{0.5}{0.6} = \\frac{5}{6} \\approx 0.8333$。\n  - $\\rho_1 = \\min(1.3, 5/6) = 5/6$。\n  - $c_1 = \\min(1.0, 5/6) = 5/6$。\n- 对于 $t=2$：比率 $= \\frac{0.4}{0.2} = 2.0$。\n  - $\\rho_2 = \\min(1.3, 2.0) = 1.3$。\n  - $c_2 = \\min(1.0, 2.0) = 1.0$。\n\n**2. 计算 TD 残差 $\\delta_t$**\n给定 $\\gamma = 0.9$：\n- $\\delta_0 = r_0 + \\gamma V(x_1) - V(x_0) = 1.2 + 0.9(1.5) - 2.0 = 1.2 + 1.35 - 2.0 = 0.55$。\n- $\\delta_1 = r_1 + \\gamma V(x_2) - V(x_1) = -0.3 + 0.9(1.0) - 1.5 = -0.3 + 0.9 - 1.5 = -0.9$。\n- $\\delta_2 = r_2 + \\gamma V(x_3) - V(x_2) = 0.5 + 0.9(0) - 1.0 = 0.5 - 1.0 = -0.5$。\n\n**3. 计算价值目标 $v_1$**\n价值目标 $v_1$ 是在从 $t=1$ 到 $t=2$ 的剩余轨迹上计算的，并使用 $V(x_3)$ 进行自举。\n$$\nv_1 = V(x_1) + \\sum_{t=1}^{2} \\gamma^{t-1} \\left( \\prod_{i=1}^{t-1} c_i \\right) \\rho_t \\delta_t\n$$\n展开求和：\n$$\nv_1 = V(x_1) + \\rho_1 \\delta_1 + \\gamma c_1 \\rho_2 \\delta_2\n$$\n代入计算出的值：\n$$\nv_1 = 1.5 + \\left(\\frac{5}{6} \\cdot (-0.9)\\right) + \\left(0.9 \\cdot \\frac{5}{6} \\cdot 1.3 \\cdot (-0.5)\\right)\n$$\n$$\nv_1 = 1.5 - 0.75 - 0.4875\n$$\n$$\nv_1 = 0.2625\n$$\n\n**4. 计算最终的校正优势函数 $\\hat{A}_0$**\n现在我们计算在 $t=0$ 时的校正优势函数：\n$$\n\\hat{A}_0 = \\rho_0 (r_0 + \\gamma v_1 - V(x_0))\n$$\n代入所有值：\n$$\n\\hat{A}_0 = 1.3 \\left( 1.2 + 0.9 \\cdot (0.2625) - 2.0 \\right)\n$$\n$$\n\\hat{A}_0 = 1.3 \\left( 1.2 + 0.23625 - 2.0 \\right)\n$$\n$$\n\\hat{A}_0 = 1.3 \\left( 1.43625 - 2.0 \\right)\n$$\n$$\n\\hat{A}_0 = 1.3 \\left( -0.56375 \\right)\n$$\n$$\n\\hat{A}_0 = -0.732875\n$$\n四舍五入到四位有效数字得到 $-0.7329$。",
            "answer": "$$\\boxed{-0.7329}$$"
        }
    ]
}