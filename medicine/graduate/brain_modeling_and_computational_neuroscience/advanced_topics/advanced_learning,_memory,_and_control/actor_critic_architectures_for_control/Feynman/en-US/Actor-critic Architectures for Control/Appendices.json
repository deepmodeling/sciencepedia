{
    "hands_on_practices": [
        {
            "introduction": "At the heart of many actor-critic methods lies the challenge of ensuring the critic provides a useful learning signal to the actor. The principle of compatible function approximation provides a rigorous way to guarantee that the policy gradient estimate is unbiased. This practice  offers a concrete, hands-on calculation to construct a compatible critic from first principles and then use it to compute the exact policy gradient vector, solidifying your understanding of this core actor-critic mechanism.",
            "id": "3962001",
            "problem": "An agent performs a one-step control at a fixed state. The actor is a stochastic policy over a continuous action, and the critic uses a linear function approximator. The policy is Gaussian with fixed variance and mean given by a linear form in state-dependent features. The control cost is quadratic in the deviation of the action from a target affine function of the state. The conditions for compatibility between actor and critic are imposed so that the actor update computed using the critic aligns with the true policy gradient.\n\nConsider a single state $s \\in \\mathbb{R}$ with the state-feature vector $x(s) \\in \\mathbb{R}^{2}$ defined by $x(s) = \\begin{pmatrix} s \\\\ s^{3} \\end{pmatrix}$. The actor’s policy is a Gaussian $\\pi_{\\theta}(a \\mid s) = \\mathcal{N}(\\mu_{\\theta}(s), \\sigma^{2})$ with fixed variance $\\sigma^{2} = 1$ and mean $\\mu_{\\theta}(s) = \\theta^{\\top} x(s)$ where $\\theta \\in \\mathbb{R}^{2}$ is the parameter vector. The immediate control cost (negative reward) at state $s$ and action $a$ is\n$$\n\\ell(s,a) = \\frac{1}{2}\\,q\\,(a - \\kappa s)^{2} + \\frac{1}{2}\\,c\\,s^{2},\n$$\nwith $q  0$, $\\kappa \\in \\mathbb{R}$, $c  0$. The action-value function for this one-step problem is $Q(s,a) = -\\ell(s,a)$. The critic uses a linear function class $Q_{w}(s,a) = \\phi(s,a)^{\\top} w$ for $w \\in \\mathbb{R}^{2}$, where the feature map $\\phi(s,a)$ must satisfy the compatibility requirement between actor and critic.\n\nTasks:\n1. Using the definition of compatibility for function approximation in actor-critic architectures, construct the critic’s feature vector $\\phi(s,a)$ that satisfies compatibility with the Gaussian actor described above.\n2. For the fixed state $s = 1.5$, parameters $\\theta = \\begin{pmatrix} 0.4 \\\\ -0.1 \\end{pmatrix}$, and control cost coefficients $q = 2$, $\\kappa = 0.8$, $c = 0.4$, compute the actor update vector\n$$\ng(\\theta) \\equiv \\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot \\mid s)}\\!\\left[\\,\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s)\\,Q_{w}(s,a)\\,\\right]\n$$\nexplicitly, where the critic parameter $w$ is chosen to solve the least-squares projection of $Q(s,a)$ onto the span of $\\phi(s,a)$ under the policy-induced action distribution at the given $s$. Express your final numerical answer for $g(\\theta)$ as a single row vector using the LaTeX $\\mathrm{pmatrix}$ environment. Do not round; provide the exact values as terminating decimals or fractions.",
            "solution": "The user has provided a valid problem statement from the field of reinforcement learning and control theory. The problem is well-posed, scientifically grounded, and contains all necessary information to derive a unique solution.\n\nThe problem is divided into two tasks. First, to construct a compatible feature vector for the critic. Second, to compute the actor update vector for a specific set of parameters.\n\n**Task 1: Construct the critic’s feature vector $\\phi(s,a)$**\n\nThe theory of actor-critic methods with function approximation establishes a compatibility condition between the actor and critic to ensure that the policy gradient is unbiased. For a linear critic of the form $Q_{w}(s,a) = \\phi(s,a)^{\\top} w$, compatibility is achieved if the critic's feature vector $\\phi(s,a)$ is chosen to be the score function of the actor's policy, i.e., the gradient of the log-policy with respect to its parameters $\\theta$.\n$$\n\\phi(s,a) = \\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s)\n$$\nThe actor's policy is a Gaussian distribution $\\pi_{\\theta}(a \\mid s) = \\mathcal{N}(\\mu_{\\theta}(s), \\sigma^{2})$, given by the probability density function:\n$$\n\\pi_{\\theta}(a \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(a - \\mu_{\\theta}(s))^2}{2\\sigma^2}\\right)\n$$\nThe natural logarithm of the policy is:\n$$\n\\ln \\pi_{\\theta}(a \\mid s) = -\\frac{1}{2} \\ln(2\\pi\\sigma^2) - \\frac{(a - \\mu_{\\theta}(s))^2}{2\\sigma^2}\n$$\nTo find the gradient with respect to $\\theta$, we differentiate this expression. The first term is a constant with respect to $\\theta$.\n$$\n\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) = \\nabla_{\\theta} \\left( -\\frac{(a - \\mu_{\\theta}(s))^2}{2\\sigma^2} \\right)\n$$\nUsing the chain rule, we get:\n$$\n\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) = -\\frac{1}{2\\sigma^2} \\cdot 2(a - \\mu_{\\theta}(s)) \\cdot (-\\nabla_{\\theta} \\mu_{\\theta}(s)) = \\frac{a - \\mu_{\\theta}(s)}{\\sigma^2} \\nabla_{\\theta} \\mu_{\\theta}(s)\n$$\nThe policy mean is defined as $\\mu_{\\theta}(s) = \\theta^{\\top} x(s)$. Its gradient with respect to the parameter vector $\\theta$ is:\n$$\n\\nabla_{\\theta} \\mu_{\\theta}(s) = \\nabla_{\\theta} (\\theta^{\\top} x(s)) = x(s)\n$$\nSubstituting this back into the expression for the score function, we obtain the compatible feature vector:\n$$\n\\phi(s,a) = \\frac{a - \\theta^{\\top} x(s)}{\\sigma^2} x(s)\n$$\nGiven the fixed variance $\\sigma^2 = 1$ and the state-feature vector $x(s) = \\begin{pmatrix} s \\\\ s^{3} \\end{pmatrix}$, the critic's feature vector is:\n$$\n\\phi(s,a) = (a - \\theta^{\\top} x(s)) x(s) = (a - (\\theta_1 s + \\theta_2 s^3)) \\begin{pmatrix} s \\\\ s^{3} \\end{pmatrix}\n$$\n\n**Task 2: Compute the actor update vector $g(\\theta)$**\n\nThe actor update vector is defined as:\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot \\mid s)}\\!\\left[\\,\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s)\\,Q_{w}(s,a)\\,\\right]\n$$\nThe problem states that the critic's weight vector $w$ is determined by a least-squares projection of the true action-value function $Q(s,a)$ onto the feature space spanned by $\\phi(s,a)$, under the policy-induced distribution over actions. This means $w$ minimizes $\\mathbb{E}_{a \\sim \\pi_{\\theta}}[(Q_w(s,a) - Q(s,a))^2]$. The solution is given by the normal equations:\n$$\n\\left( \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) \\phi(s,a)^{\\top}] \\right) w = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) Q(s,a)]\n$$\nSubstituting $Q_w(s,a) = \\phi(s,a)^{\\top} w$ and $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) = \\phi(s,a)$ into the definition of $g(\\theta)$:\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) \\phi(s,a)^{\\top} w] = \\left( \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) \\phi(s,a)^{\\top}] \\right) w\n$$\nFrom the normal equations, this implies that the actor update $g(\\theta)$ is precisely the right-hand side of the normal equations:\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) Q(s,a)] = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) Q(s,a)]\n$$\nThis is the true policy gradient for this one-step problem. Now we compute this expectation. The true action-value function is $Q(s,a) = -\\ell(s,a) = -\\frac{1}{2}q(a-\\kappa s)^2 - \\frac{1}{2}c s^2$.\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}} \\left[ \\left(\\frac{a - \\mu_{\\theta}(s)}{\\sigma^2} x(s)\\right) \\left(-\\frac{1}{2}q(a-\\kappa s)^2 - \\frac{1}{2}c s^2\\right) \\right]\n$$\nWe can factor out terms that do not depend on the random variable $a$:\n$$\ng(\\theta) = \\frac{x(s)}{\\sigma^2} \\mathbb{E}_{a \\sim \\pi_{\\theta}} \\left[ (a - \\mu_{\\theta}(s)) \\left(-\\frac{q}{2}(a-\\kappa s)^2 - \\frac{c}{2}s^2\\right) \\right]\n$$\nLet's define a zero-mean random variable $\\epsilon = a - \\mu_{\\theta}(s)$. Since $a \\sim \\mathcal{N}(\\mu_{\\theta}(s), \\sigma^2)$, then $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. The action is $a = \\mu_{\\theta}(s) + \\epsilon$. The expectation becomes:\n$$\n\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)} \\left[ \\epsilon \\left( -\\frac{q}{2}(\\mu_{\\theta}(s) + \\epsilon - \\kappa s)^2 - \\frac{c}{2}s^2 \\right) \\right]\n$$\nLet's expand the term inside the expectation:\n$$\n\\epsilon \\left( -\\frac{q}{2} [(\\mu_{\\theta}(s) - \\kappa s) + \\epsilon]^2 - \\frac{c}{2}s^2 \\right) = \\epsilon \\left( -\\frac{q}{2} [(\\mu_{\\theta}(s) - \\kappa s)^2 + 2\\epsilon(\\mu_{\\theta}(s) - \\kappa s) + \\epsilon^2] - \\frac{c}{2}s^2 \\right)\n$$\nDistributing $\\epsilon$:\n$$\n-\\frac{q}{2}(\\mu_{\\theta}(s) - \\kappa s)^2 \\epsilon - q(\\mu_{\\theta}(s) - \\kappa s) \\epsilon^2 - \\frac{q}{2}\\epsilon^3 - \\frac{c}{2}s^2 \\epsilon\n$$\nNow, we take the expectation of this expression. We use the moments of a zero-mean normal distribution: $\\mathbb{E}[\\epsilon] = 0$, $\\mathbb{E}[\\epsilon^2] = \\sigma^2$, and $\\mathbb{E}[\\epsilon^3] = 0$.\n$$\n\\mathbb{E}[\\dots] = -\\frac{q}{2}(\\mu_{\\theta}(s) - \\kappa s)^2 \\mathbb{E}[\\epsilon] - q(\\mu_{\\theta}(s) - \\kappa s) \\mathbb{E}[\\epsilon^2] - \\frac{q}{2}\\mathbb{E}[\\epsilon^3] - \\frac{c s^2}{2} \\mathbb{E}[\\epsilon]\n$$\n$$\n\\mathbb{E}[\\dots] = 0 - q(\\mu_{\\theta}(s) - \\kappa s) \\sigma^2 - 0 - 0 = -q \\sigma^2 (\\mu_{\\theta}(s) - \\kappa s)\n$$\nSubstituting this result back into the expression for $g(\\theta)$:\n$$\ng(\\theta) = \\frac{x(s)}{\\sigma^2} \\left( -q \\sigma^2 (\\mu_{\\theta}(s) - \\kappa s) \\right) = -q x(s) (\\mu_{\\theta}(s) - \\kappa s)\n$$\nWe now substitute the numerical values: $s = 1.5$, $\\theta = \\begin{pmatrix} 0.4 \\\\ -0.1 \\end{pmatrix}$, $q=2$, $\\kappa = 0.8$.\nFirst, calculate the required components:\nThe state-feature vector is $x(s) = x(1.5) = \\begin{pmatrix} 1.5 \\\\ (1.5)^3 \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix}$.\nThe policy mean is $\\mu_{\\theta}(s) = \\theta^{\\top} x(s) = \\begin{pmatrix} 0.4  -0.1 \\end{pmatrix} \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix} = 0.4 \\times 1.5 - 0.1 \\times 3.375 = 0.6 - 0.3375 = 0.2625$.\nThe target action function value is $\\kappa s = 0.8 \\times 1.5 = 1.2$.\nThe difference is $\\mu_{\\theta}(s) - \\kappa s = 0.2625 - 1.2 = -0.9375$.\nNow, assemble the final gradient vector $g(\\theta)$:\n$$\ng(\\theta) = -2 \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix} (-0.9375)\n$$\n$$\ng(\\theta) = (2 \\times 0.9375) \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix} = 1.875 \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix}\n$$\nPerforming the final multiplications for each component:\n$$\ng_1(\\theta) = 1.875 \\times 1.5 = 2.8125\n$$\n$$\ng_2(\\theta) = 1.875 \\times 3.375 = 6.328125\n$$\nSo, the actor update vector is $g(\\theta) = \\begin{pmatrix} 2.8125 \\\\ 6.328125 \\end{pmatrix}$. The problem requires the answer as a single row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.8125  6.328125\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Actor-critic algorithms are iterative, stochastic processes, and understanding the conditions for their convergence is crucial for both theory and practice. This practice  bridges reinforcement learning theory with classical optimal control, grounding the algorithm's behavior in a well-understood framework. You will first establish valid learning rate schedules based on two-timescale stochastic approximation theory, and then derive the exact optimal policy for a benchmark Linear-Quadratic control problem, providing a clear analytical target for what the actor-critic algorithm aims to find.",
            "id": "3962047",
            "problem": "Consider a scalar linear-quadratic control system defined by the discrete-time dynamics $x_{t+1} = a x_{t} + b u_{t}$ and a stationary linear state-feedback actor $u_{t} = -k_{t} x_{t}$. The performance criterion is the infinite-horizon discounted cost $J(k) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\left(q x_{t}^{2} + r u_{t}^{2}\\right)\\right]$, where $0  \\gamma  1$, $q  0$, and $r  0$. The actor parameter $k_{t}$ is updated using a two-timescale actor-critic algorithm with a critic parameter $v_{t}$ approximating the value function $V(x) \\approx v x^{2}$ via Temporal Difference (TD) learning, and an actor update that performs stochastic gradient descent on the expected discounted cost. Assume the gradient estimate for the actor is an unbiased estimator of $\\nabla J(k)$ for each fixed $k$ and that the critic TD error defines a contraction in the mean for fixed $k$ under the linear-quadratic structure.\n\nTasks:\n1. Propose step-size schedules $\\{\\alpha_{t}\\}$ for the critic and $\\{\\beta_{t}\\}$ for the actor that satisfy the Robbins–Monro stochastic approximation conditions and achieve appropriate two-timescale separation (critic faster than actor). Explicitly verify the required summability properties.\n2. Starting from first principles of the discounted linear-quadratic control problem and the definition of the policy $u_{t} = -k x_{t}$, derive the exact expression for the optimal feedback gain $k^{\\star}$ that minimizes $J(k)$, expressing your result in terms of $a$, $b$, $q$, $r$, and $\\gamma$. Your derivation must be self-contained and use only the provided system and cost definitions.\n3. Using the result from Task 2, evaluate $k^{\\star}$ numerically for the parameter values $a = 0.9$, $b = 1$, $q = 1$, $r = 0.25$, and $\\gamma = 0.95$. Round your final numerical answer to four significant figures. Provide only the rounded value of $k^{\\star}$ as your final answer.\n\nYou may assume the initial state $x_{0}$ has finite second moment, the actor parameter sequence is constrained to a compact set that ensures $\\gamma (a - b k)^{2}  1$ throughout the iterations, and the noise processes in the TD and gradient estimators satisfy standard martingale-difference and bounded-variance conditions consistent with stochastic approximation theory.",
            "solution": "The user has provided a problem in three parts concerning a two-timescale actor-critic algorithm for a scalar discrete-time linear-quadratic control system. The problem asks for the proposal of valid step-size schedules, the analytical derivation of the optimal control gain, and a numerical evaluation of this gain.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n\n*   System dynamics: $x_{t+1} = a x_{t} + b u_{t}$\n*   Actor policy: $u_{t} = -k_{t} x_{t}$\n*   Cost function: $J(k) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\left(q x_{t}^{2} + r u_{t}^{2}\\right)\\right]$\n*   Discount factor: $0  \\gamma  1$\n*   Cost weights: $q  0$, $r  0$\n*   Critic approximation: $V(x) \\approx v x^{2}$\n*   Critic parameter: $v_t$\n*   Actor parameter: $k_t$\n*   Learning rates: $\\{\\alpha_{t}\\}$ (critic), $\\{\\beta_{t}\\}$ (actor)\n*   Algorithm properties: Two-timescale actor-critic, stochastic gradient descent for the actor, Temporal Difference (TD) learning for the critic.\n*   Assumptions:\n    *   The actor's gradient estimate is an unbiased estimator of $\\nabla J(k)$.\n    *   The critic's TD error defines a contraction in the mean for fixed $k$.\n    *   Step-sizes must satisfy Robbins–Monro conditions.\n    *   Critic is on a faster timescale than the actor.\n    *   Initial state $x_{0}$ has a finite second moment.\n    *   The actor parameter sequence $\\{k_t\\}$ is constrained to a compact set where $\\gamma (a - b k)^{2}  1$.\n    *   Noise processes satisfy standard stochastic approximation conditions.\n*   Numerical values for Task 3: $a = 0.9$, $b = 1$, $q = 1$, $r = 0.25$, $\\gamma = 0.95$.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded:** The problem is firmly rooted in established principles of optimal control theory (Linear-Quadratic Regulator) and reinforcement learning (actor-critic methods, stochastic approximation). The model is a standard benchmark for analyzing such algorithms. The assumptions provided are standard in the theoretical analysis of the convergence of two-timescale stochastic approximation algorithms. The problem is scientifically sound.\n*   **Well-Posed:** All parts of the problem are well-posed. Task 1 asks for a valid example of step-size schedules, for which many exist. Task 2 requires the derivation of a unique optimal gain $k^{\\star}$, which is known to exist for the discounted LQR problem under the given conditions ($q  0$, $r  0$). Task 3 is a direct numerical computation. The problem is self-contained and provides all necessary information.\n*   **Objective:** The problem is stated in precise, formal mathematical language, free from any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. It is a well-posed, scientifically grounded problem from the intersection of control theory and machine learning. I will proceed to provide a complete solution.\n\n### Solution\n\n**Task 1: Step-Size Schedules**\n\nThe convergence of two-timescale stochastic approximation algorithms requires that both step-size schedules, $\\{\\alpha_{t}\\}$ for the fast (critic) process and $\\{\\beta_{t}\\}$ for the slow (actor) process, satisfy the standard Robbins–Monro (RM) conditions. Additionally, a timescale separation condition must be met.\n\nThe RM conditions for a generic step-size schedule $\\{\\eta_t\\}$ are:\n$$\n\\sum_{t=0}^{\\infty} \\eta_t = \\infty\n\\quad \\text{and} \\quad\n\\sum_{t=0}^{\\infty} \\eta_t^2  \\infty\n$$\nThe first condition ensures that the algorithm can overcome any initial error, while the second condition ensures that the noise injected at each step eventually vanishes.\n\nThe timescale separation condition, which makes the critic's updates \"faster\" than the actor's, is:\n$$\n\\lim_{t \\to \\infty} \\frac{\\beta_{t}}{\\alpha_{t}} = 0\n$$\n\nWe propose schedules of the form $\\alpha_t = (t+1)^{-p}$ and $\\beta_t = (t+1)^{-q}$.\nFor these schedules to satisfy the RM conditions, the exponents $p$ and $q$ must be in the interval $(0.5, 1]$. This is because the series $\\sum_{t=0}^{\\infty} (t+1)^{-c}$ diverges for $c \\le 1$ (by the integral test) and converges for $c  1$.\n*   For $\\sum \\alpha_t = \\sum (t+1)^{-p} = \\infty$, we need $p \\le 1$.\n*   For $\\sum \\alpha_t^2 = \\sum (t+1)^{-2p}  \\infty$, we need $2p  1$, or $p  0.5$.\nThe same logic applies to $q$: $0.5  q \\le 1$.\n\nFor the timescale separation condition $\\lim_{t \\to \\infty} \\beta_t / \\alpha_t = 0$, we require:\n$$\n\\lim_{t \\to \\infty} \\frac{(t+1)^{-q}}{(t+1)^{-p}} = \\lim_{t \\to \\infty} (t+1)^{p-q} = 0\n$$\nThis limit is zero if and only if the exponent $p-q$ is negative, i.e., $p  q$.\n\nCombining all conditions, we must choose $p$ and $q$ such that $0.5  p  q \\le 1$.\nA valid choice is to set $p = \\frac{2}{3}$ and $q = 1$. Let us define the schedules and verify them.\n\n*   Critic (fast) step-size: $\\alpha_{t} = \\frac{1}{(t+1)^{2/3}}$\n*   Actor (slow) step-size: $\\beta_{t} = \\frac{1}{t+1}$\n\nVerification:\n1.  For $\\{\\alpha_t\\}$: $\\sum_{t=0}^{\\infty} \\alpha_t = \\sum_{t=0}^{\\infty} (t+1)^{-2/3}$ is a divergent p-series ($p=2/3 \\le 1$). $\\sum_{t=0}^{\\infty} \\alpha_t^2 = \\sum_{t=0}^{\\infty} (t+1)^{-4/3}$ is a convergent p-series ($p=4/3  1$). The RM conditions hold.\n2.  For $\\{\\beta_t\\}$: $\\sum_{t=0}^{\\infty} \\beta_t = \\sum_{t=0}^{\\infty} (t+1)^{-1}$ is the harmonic series, which is a divergent p-series ($p=1 \\le 1$). $\\sum_{t=0}^{\\infty} \\beta_t^2 = \\sum_{t=0}^{\\infty} (t+1)^{-2}$ is a convergent p-series ($p=2  1$). The RM conditions hold.\n3.  Timescale separation: $\\lim_{t \\to \\infty} \\frac{\\beta_t}{\\alpha_t} = \\lim_{t \\to \\infty} \\frac{(t+1)^{-1}}{(t+1)^{-2/3}} = \\lim_{t \\to \\infty} (t+1)^{-1/3} = 0$. The condition is satisfied.\n\n**Task 2: Derivation of the Optimal Gain $k^{\\star}$**\n\nThe optimal control gain $k^{\\star}$ can be derived from the Bellman optimality equation for this discounted Linear-Quadratic problem. The optimal value function is known to be quadratic, i.e., $V^{\\star}(x) = P x^{2}$ for some constant $P  0$.\n\nThe Bellman optimality equation is:\n$$\nV^{\\star}(x) = \\min_{u} \\left\\{ q x^{2} + r u^{2} + \\gamma V^{\\star}(x_{t+1}) \\right\\}\n$$\nSubstituting $V^{\\star}(x) = P x^{2}$ and $x_{t+1} = ax + bu$:\n$$\nP x^{2} = \\min_{u} \\left\\{ q x^{2} + r u^{2} + \\gamma P (ax+bu)^{2} \\right\\}\n$$\nTo find the minimizing control input $u^{\\star}$, we differentiate the expression inside the minimum with respect to $u$ and set the result to zero:\n$$\n\\frac{\\partial}{\\partial u} \\left[ q x^{2} + r u^{2} + \\gamma P (ax+bu)^{2} \\right] = 2ru + \\gamma P \\cdot 2(ax+bu) \\cdot b = 0\n$$\n$$\nru + \\gamma P b (ax+bu) = 0\n$$\n$$\nu(r + \\gamma P b^{2}) = - \\gamma P a b x\n$$\nThe optimal control law is thus a linear state feedback:\n$$\nu^{\\star} = - \\left( \\frac{\\gamma a b P}{r + \\gamma b^{2} P} \\right) x\n$$\nThis is of the form $u^{\\star} = -k^{\\star} x$, where the optimal gain $k^{\\star}$ is:\n$$\nk^{\\star} = \\frac{\\gamma a b P}{r + \\gamma b^{2} P}\n$$\nTo find the value of $P$, we substitute $u^{\\star}$ back into the Bellman equation. The equation must hold for the minimizing $u^{\\star}$.\n$$\nP x^{2} = q x^{2} + r (u^{\\star})^{2} + \\gamma P (ax+bu^{\\star})^{2}\n$$\nDividing by $x^2$ (since the equation must hold for all $x$) and substituting $u^{\\star} = -k^{\\star}x$:\n$$\nP = q + r (k^{\\star})^{2} + \\gamma P (a-bk^{\\star})^{2}\n$$\nWe substitute the expression for $k^{\\star}$ into this equation. It is easier to first express $(a-bk^{\\star})$:\n$$\na - bk^{\\star} = a - b \\left(\\frac{\\gamma a b P}{r + \\gamma b^{2} P}\\right) = \\frac{a(r + \\gamma b^{2} P) - \\gamma a b^{2} P}{r + \\gamma b^{2} P} = \\frac{ar}{r + \\gamma b^{2} P}\n$$\nNow substitute $k^{\\star}$ and $(a-bk^{\\star})$ into the equation for $P$:\n$$\nP = q + r \\left( \\frac{\\gamma a b P}{r + \\gamma b^{2} P} \\right)^{2} + \\gamma P \\left( \\frac{ar}{r + \\gamma b^{2} P} \\right)^{2}\n$$\n$$\nP = q + \\frac{r \\gamma^{2} a^{2} b^{2} P^{2} + \\gamma P a^{2} r^{2}}{(r + \\gamma b^{2} P)^{2}}\n$$\n$$\nP = q + \\frac{\\gamma a^{2} r P (r \\gamma b^2 P / (\\gamma a^2 r P) + r)}{ (r + \\gamma b^{2} P)^{2}} = q + \\frac{\\gamma a^{2} r P (r + \\gamma b^{2} P)}{(r + \\gamma b^{2} P)^{2}}\n$$\n$$\nP = q + \\frac{\\gamma a^{2} r P}{r + \\gamma b^{2} P}\n$$\nThis equation for $P$ is the scalar discrete-time Algebraic Riccati Equation (DARE) for the discounted problem. We rearrange it into a quadratic equation for $P$:\n$$\nP(r + \\gamma b^{2} P) = q(r + \\gamma b^{2} P) + \\gamma a^{2} r P\n$$\n$$\nPr + \\gamma b^{2} P^{2} = qr + q\\gamma b^{2} P + \\gamma a^{2} r P\n$$\n$$\n(\\gamma b^{2}) P^{2} + (r - \\gamma q b^{2} - \\gamma a^{2} r) P - qr = 0\n$$\nThis is a quadratic equation of the form $A P^2 + B P + C = 0$. Since $q0$ and $r0$, the value $P$ associated with the cost must be positive. The coefficient $A = \\gamma b^2  0$ and the coefficient $C = -qr  0$. The product of the roots $C/A$ is negative, so there is one positive and one negative real root. We must take the positive root:\n$$\nP = \\frac{-B + \\sqrt{B^{2} - 4AC}}{2A}\n$$\nwhere $A = \\gamma b^2$, $B = r - \\gamma(q b^2 + a^2 r)$, and $C = -qr$.\nThe optimal gain $k^{\\star}$ is then found by substituting this value of $P$ into its expression: $k^{\\star} = \\frac{\\gamma a b P}{r + \\gamma b^{2} P}$.\n\n**Task 3: Numerical Evaluation of $k^{\\star}$**\n\nGiven the parameters: $a = 0.9$, $b = 1$, $q = 1$, $r = 0.25$, $\\gamma = 0.95$.\n\nFirst, we calculate the coefficients of the Riccati equation for $P$:\n$A = \\gamma b^{2} = 0.95 \\times 1^{2} = 0.95$\n$B = r - \\gamma(q b^{2} + a^{2} r) = 0.25 - 0.95(1 \\times 1^{2} + 0.9^{2} \\times 0.25)$\n$B = 0.25 - 0.95(1 + 0.81 \\times 0.25) = 0.25 - 0.95(1 + 0.2025)$\n$B = 0.25 - 0.95(1.2025) = 0.25 - 1.142375 = -0.892375$\n$C = -qr = -1 \\times 0.25 = -0.25$\n\nThe equation for $P$ is $0.95 P^{2} - 0.892375 P - 0.25 = 0$.\nWe solve for the positive root $P$:\n$$\nP = \\frac{-(-0.892375) + \\sqrt{(-0.892375)^{2} - 4(0.95)(-0.25)}}{2(0.95)}\n$$\n$$\nP = \\frac{0.892375 + \\sqrt{0.7963320625 + 0.95}}{1.9}\n$$\n$$\nP = \\frac{0.892375 + \\sqrt{1.7463320625}}{1.9} \\approx \\frac{0.892375 + 1.32148858}{1.9}\n$$\n$$\nP \\approx \\frac{2.21386364}{1.9} \\approx 1.16519139\n$$\nNow, we use this value of $P$ to compute $k^{\\star}$:\n$$\nk^{\\star} = \\frac{\\gamma a b P}{r + \\gamma b^{2} P} = \\frac{0.95 \\times 0.9 \\times 1 \\times P}{0.25 + 0.95 \\times 1^{2} \\times P} = \\frac{0.855 P}{0.25 + 0.95 P}\n$$\nSubstituting the numerical value of $P$:\n$$\nk^{\\star} \\approx \\frac{0.855 \\times 1.16519139}{0.25 + 0.95 \\times 1.16519139}\n$$\n$$\nk^{\\star} \\approx \\frac{0.99623864}{0.25 + 1.10693182} = \\frac{0.99623864}{1.35693182} \\approx 0.7341829\n$$\nRounding to four significant figures, we get $k^{\\star} \\approx 0.7342$.",
            "answer": "$$\n\\boxed{0.7342}\n$$"
        },
        {
            "introduction": "To be sample-efficient, modern actor-critic methods must often learn from off-policy data, which introduces a statistical mismatch that must be corrected without destabilizing learning. The V-trace algorithm offers an elegant solution by constructing a multi-step value target using truncated importance sampling to carefully balance bias and variance. This exercise  demystifies this advanced technique by guiding you through a step-by-step calculation of a V-trace target and its corresponding corrected advantage, providing a practical look into the mechanics of a state-of-the-art algorithm.",
            "id": "3962015",
            "problem": "Consider an off-policy actor-critic agent for control in a Markov Decision Process (MDP) with states $x_t$, actions $a_t$, rewards $r_t$, and discount factor $\\gamma \\in (0,1)$. The target policy is $\\pi_{\\theta}(a \\mid x)$ parameterized by $\\theta$, and data are collected under a different behavior policy $\\mu(a \\mid x)$. To correct for off-policy sampling, Importance Sampling (IS) uses the per-time-step ratio $r_t = \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}$. In order to reduce variance and stabilize learning, consider truncated IS with truncation levels $\\bar{\\rho}  0$ and $\\bar{c}  0$, defining\n$$\n\\rho_t = \\min\\left(\\bar{\\rho}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right), \\quad c_t = \\min\\left(\\bar{c}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right).\n$$\nThe critic seeks a multi-step target that corrects temporal-difference errors using truncated IS, and the actor seeks a corrected advantage that scales the policy gradient $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid x_t)$ in a way that remains consistent with off-policy learning.\n\nStarting only from the definitions above and basic Reinforcement Learning (RL) principles (state-value $V^{\\pi}(x)$, action-value $Q^{\\pi}(x,a)$, and that $Q^{\\pi}(x,a) = \\mathbb{E}\\left[r + \\gamma V^{\\pi}(x') \\mid x,a\\right]$), derive the truncated IS multi-step value target $v_s$ for the critic and the corresponding corrected advantage used by the actor at time $s$, in terms of $\\rho_t$, $c_t$, immediate rewards, bootstrapped values, and $\\gamma$. Then, for the specific $3$-step trajectory segment $t \\in \\{0,1,2\\}$ with terminal bootstrap at $t=3$, compute the corrected advantage at time $t=0$ using the following data:\n- $\\gamma = 0.9$,\n- $\\bar{\\rho} = 1.3$, $\\bar{c} = 1.0$,\n- $\\mu(a_0 \\mid x_0) = 0.4$, $\\pi_{\\theta}(a_0 \\mid x_0) = 0.7$,\n- $\\mu(a_1 \\mid x_1) = 0.6$, $\\pi_{\\theta}(a_1 \\mid x_1) = 0.5$,\n- $\\mu(a_2 \\mid x_2) = 0.2$, $\\pi_{\\theta}(a_2 \\mid x_2) = 0.4$,\n- $r_0 = 1.2$, $r_1 = -0.3$, $r_2 = 0.5$,\n- $V(x_0) = 2.0$, $V(x_1) = 1.5$, $V(x_2) = 1.0$, $V(x_3) = 0$.\n\nExpress the final scalar corrected advantage at time $t=0$ as a real number, rounded to four significant figures. No units are required.",
            "solution": "The problem statement is evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Framework**: Markov Decision Process (MDP) with states $x_t$, actions $a_t$, rewards $r_t$.\n- **Policies**: Target policy $\\pi_{\\theta}(a \\mid x)$, behavior policy $\\mu(a \\mid x)$.\n- **Discount Factor**: $\\gamma \\in (0,1)$.\n- **Importance Sampling (IS) Ratios**:\n  - The problem first defines a per-time-step ratio $r_t = \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}$, but this notation for the ratio is immediately superseded by the definitions of $\\rho_t$ and $c_t$, and $r_t$ is used for rewards in the numerical section. The derivation and calculation will use $r_t$ exclusively for rewards as is customary.\n  - Truncated ratio for value estimation: $\\rho_t = \\min\\left(\\bar{\\rho}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right)$.\n  - Truncated ratio for recursive updates: $c_t = \\min\\left(\\bar{c}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right)$.\n  - Truncation levels: $\\bar{\\rho}  0$, $\\bar{c}  0$.\n- **Objectives**:\n  1. Derive the truncated IS multi-step value target $v_s$ for the critic.\n  2. Derive the corresponding corrected advantage used by the actor at time $s$.\n  3. Compute the corrected advantage at time $t=0$ for a given $3$-step trajectory segment $t \\in \\{0,1,2\\}$ with a terminal bootstrap at $t=3$.\n- **Fundamental Principles**: State-value $V^{\\pi}(x)$, action-value $Q^{\\pi}(x,a)$, and the Bellman expectation equation $Q^{\\pi}(x,a) = \\mathbb{E}\\left[r + \\gamma V^{\\pi}(x') \\mid x,a\\right]$.\n- **Numerical Data**:\n  - $\\gamma = 0.9$.\n  - $\\bar{\\rho} = 1.3$, $\\bar{c} = 1.0$.\n  - Policy probabilities: $\\mu(a_0 \\mid x_0) = 0.4$, $\\pi_{\\theta}(a_0 \\mid x_0) = 0.7$; $\\mu(a_1 \\mid x_1) = 0.6$, $\\pi_{\\theta}(a_1 \\mid x_1) = 0.5$; $\\mu(a_2 \\mid x_2) = 0.2$, $\\pi_{\\theta}(a_2 \\mid x_2) = 0.4$.\n  - Rewards: $r_0 = 1.2$, $r_1 = -0.3$, $r_2 = 0.5$.\n  - Value function estimates: $V(x_0) = 2.0$, $V(x_1) = 1.5$, $V(x_2) = 1.0$, $V(x_3) = 0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. The formulation of truncated importance sampling ratios $\\rho_t$ and $c_t$ and the task of deriving a multi-step off-policy value target and corrected advantage are characteristic of the V-trace algorithm (Espeholt et al., 2018), a well-established and widely used method in modern reinforcement learning for large-scale, off-policy actor-critic architectures. The problem is well-posed, providing all necessary definitions and data for a unique solution. The language is objective and formal. While there is a minor notational ambiguity where the IS ratio is initially labeled $r_t$, which is also the symbol for reward, the subsequent, more specific definitions of $\\rho_t$ and $c_t$ resolve any confusion. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Derivation of the Value Target and Corrected Advantage\n\nThe problem describes a learning procedure based on V-trace. We will derive the expressions for the multi-step value target and the corrected advantage based on its principles.\n\nLet $V(x_t)$ be the current estimate of the value function. The one-step temporal difference (TD) residual at time $t$ is defined as:\n$$\n\\delta_t = r_t + \\gamma V(x_{t+1}) - V(x_t)\n$$\nThis residual measures the error in the value estimate $V(x_t)$ based on the observed transition $(x_t, a_t, r_t, x_{t+1})$.\n\nFor off-policy learning, TD-residuals are corrected by importance sampling. The V-trace algorithm defines a multi-step value target $v_s$ for state $x_s$ based on a trajectory segment from time $s$ to $s+n-1$. This target is built by accumulating discounted and importance-weighted TD-residuals. The recursive definition of the value target $v_t$ at time $t \\ge s$ is:\n$$\nv_t = V(x_t) + \\rho_t (r_t + \\gamma V(x_{t+1}) - V(x_t)) + \\gamma c_t (v_{t+1} - V(x_{t+1}))\n$$\nwith a terminal condition an $n$-steps, such as $v_{s+n} = V(x_{s+n})$.\n\nUnrolling this recursion from $t=s$ to $s+n-1$ yields the explicit multi-step value target $v_s$ for the critic:\n$$\nv_s = V(x_s) + \\sum_{t=s}^{s+n-1} \\gamma^{t-s} \\left( \\prod_{i=s}^{t-1} c_i \\right) \\rho_t \\delta_t\n$$\nwhere the product term $\\prod_{i=s}^{t-1} c_i$ is defined as $1$ for $t=s$.\n\nThe actor updates the policy $\\pi_{\\theta}$ using a policy gradient method. The V-trace algorithm uses a corrected advantage estimate at time $s$ to scale the score function $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_s \\mid x_s)$. This advantage, which we denote $\\hat{A}_s$, is given by:\n$$\n\\hat{A}_s = \\rho_s (r_s + \\gamma v_{s+1} - V(x_s))\n$$\nHere, $v_{s+1}$ is the V-trace target for the subsequent state $x_{s+1}$. This form retains a close connection to the standard advantage function $A(x,a) = Q(x,a) - V(x)$, but with $Q(x_s, a_s)$ estimated by $r_s + \\gamma v_{s+1}$, where $v_{s+1}$ serves as an off-policy corrected estimate of the value of the next state $V(x_{s+1})$.\n\n### Numerical Computation of the Corrected Advantage at $t=0$\n\nWe need to compute the corrected advantage at time $t=0$, which is $\\hat{A}_0 = \\rho_0 (r_0 + \\gamma v_1 - V(x_0))$. This requires us to first compute the intermediate quantities $\\rho_t$, $c_t$, $\\delta_t$, and then the value target $v_1$. The trajectory segment is for $t \\in \\{0,1,2\\}$ ($n=3$), with a bootstrap value $V(x_3)=0$.\n\n**1. Calculate Importance Sampling Ratios and Truncated Versions**\nThe untruncated ratio is $\\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}$. Given $\\bar{\\rho} = 1.3$ and $\\bar{c} = 1.0$:\n- For $t=0$: Ratio $= \\frac{0.7}{0.4} = 1.75$.\n  - $\\rho_0 = \\min(1.3, 1.75) = 1.3$.\n  - $c_0 = \\min(1.0, 1.75) = 1.0$.\n- For $t=1$: Ratio $= \\frac{0.5}{0.6} = \\frac{5}{6} \\approx 0.8333$.\n  - $\\rho_1 = \\min(1.3, 5/6) = 5/6$.\n  - $c_1 = \\min(1.0, 5/6) = 5/6$.\n- For $t=2$: Ratio $= \\frac{0.4}{0.2} = 2.0$.\n  - $\\rho_2 = \\min(1.3, 2.0) = 1.3$.\n  - $c_2 = \\min(1.0, 2.0) = 1.0$.\n\n**2. Calculate TD-Residuals $\\delta_t$**\nGiven $\\gamma = 0.9$:\n- $\\delta_0 = r_0 + \\gamma V(x_1) - V(x_0) = 1.2 + 0.9(1.5) - 2.0 = 1.2 + 1.35 - 2.0 = 0.55$.\n- $\\delta_1 = r_1 + \\gamma V(x_2) - V(x_1) = -0.3 + 0.9(1.0) - 1.5 = -0.3 + 0.9 - 1.5 = -0.9$.\n- $\\delta_2 = r_2 + \\gamma V(x_3) - V(x_2) = 0.5 + 0.9(0) - 1.0 = 0.5 - 1.0 = -0.5$.\n\n**3. Calculate the Value Target $v_1$**\nThe value target $v_1$ is computed over the remainder of the trajectory from $t=1$ to $t=2$, bootstrapping with $V(x_3)$.\n$$\nv_1 = V(x_1) + \\sum_{t=1}^{2} \\gamma^{t-1} \\left( \\prod_{i=1}^{t-1} c_i \\right) \\rho_t \\delta_t\n$$\nExpanding the sum:\n$$\nv_1 = V(x_1) + (\\gamma^0 \\rho_1 \\delta_1) + (\\gamma^1 c_1 \\rho_2 \\delta_2)\n$$\nSubstituting the calculated values:\n$$\nv_1 = 1.5 + \\left(1 \\cdot \\frac{5}{6} \\cdot (-0.9)\\right) + \\left(0.9 \\cdot \\frac{5}{6} \\cdot 1.3 \\cdot (-0.5)\\right)\n$$\n$$\nv_1 = 1.5 - \\frac{4.5}{6} - \\left(0.9 \\cdot \\frac{5}{6} \\cdot 0.65\\right)\n$$\n$$\nv_1 = 1.5 - 0.75 - (0.75 \\times 1.3 \\times 0.5)\n$$\n$$\nv_1 = 1.5 - 0.75 - 0.4875\n$$\n$$\nv_1 = 0.75 - 0.4875 = 0.2625\n$$\n\n**4. Calculate the Final Corrected Advantage $\\hat{A}_0$**\nNow we compute the corrected advantage at $t=0$:\n$$\n\\hat{A}_0 = \\rho_0 (r_0 + \\gamma v_1 - V(x_0))\n$$\nPlugging in all values:\n$$\n\\hat{A}_0 = 1.3 \\left( 1.2 + 0.9 \\cdot (0.2625) - 2.0 \\right)\n$$\n$$\n\\hat{A}_0 = 1.3 \\left( 1.2 + 0.23625 - 2.0 \\right)\n$$\n$$\n\\hat{A}_0 = 1.3 \\left( 1.43625 - 2.0 \\right)\n$$\n$$\n\\hat{A}_0 = 1.3 \\left( -0.56375 \\right)\n$$\n$$\n\\hat{A}_0 = -0.732875\n$$\nRounding to four significant figures gives $-0.7329$.",
            "answer": "$$\\boxed{-0.7329}$$"
        }
    ]
}