{
    "hands_on_practices": [
        {
            "introduction": "To begin, we explore the fundamental calculation at the heart of synaptic scaling. This practice uses a simplified linear model of a neuron to isolate the core principle: how to proportionally adjust all synaptic weights to guide the neuron's average firing rate toward a desired homeostatic setpoint. By deriving the necessary multiplicative factor, you will gain a concrete understanding of the basic mechanism that allows neurons to regulate their own activity levels .",
            "id": "4047498",
            "problem": "Consider a single neuron in a neuromorphic system implementing homeostatic plasticity through synaptic scaling. The neuron receives inputs from $N$ presynaptic channels with average input rates $\\bar{x}_{i}$ and synaptic weights $w_{i}$, and its average firing rate over a slow homeostatic timescale is modeled by the linear rate equation $r=\\sum_{i=1}^{N}w_{i}\\bar{x}_{i}$. A homeostatic controller seeks to achieve a target average firing rate $r^{*}$ by multiplicatively scaling all synaptic weights by a common positive factor $\\alpha$, while preserving the ratios of synaptic strengths. Assume that over the timescale on which the controller acts, the presynaptic averages $\\bar{x}_{i}$ remain fixed, and that the neuron's transfer function is well approximated by a linear mapping in this operating regime.\n\nDerive, from these premises alone, an analytical expression for the scaling factor $\\alpha$ that achieves the target rate $r^{*}$. Then, evaluate $\\alpha$ for the specific case with $N=3$, presynaptic averages $\\bar{x}_{1}=5\\,\\text{Hz}$, $\\bar{x}_{2}=7\\,\\text{Hz}$, $\\bar{x}_{3}=3\\,\\text{Hz}$, synaptic weights $w_{1}=0.8$, $w_{2}=0.5$, $w_{3}=-0.2$, and target average rate $r^{*}=10\\,\\text{Hz}$. Round your final numerical answer to four significant figures. The final answer should be reported as the single value of $\\alpha$.",
            "solution": "The problem is deemed valid as it is scientifically grounded in a standard model of homeostatic plasticity, well-posed with a clear objective and sufficient data, and expressed in objective, formalizable language.\n\nThe initial average firing rate of the neuron, which we can denote as $r_{\\text{initial}}$, is given by the linear rate equation:\n$$r_{\\text{initial}} = \\sum_{i=1}^{N} w_{i} \\bar{x}_{i}$$\nHere, $w_{i}$ are the initial synaptic weights and $\\bar{x}_{i}$ are the average presynaptic input rates.\n\nHomeostatic plasticity adjusts the synaptic weights to achieve a target average firing rate, $r^{*}$. The mechanism specified is a multiplicative scaling of all synaptic weights by a common positive factor, $\\alpha$. The new weights, $w'_{i}$, are therefore given by:\n$$w'_{i} = \\alpha w_{i}$$\nThe problem states that this scaling should result in the neuron's average firing rate becoming $r^{*}$. Using the same rate equation with the new weights, we have:\n$$r^{*} = \\sum_{i=1}^{N} w'_{i} \\bar{x}_{i}$$\nSubstituting the expression for the new weights, $w'_{i}$, into this equation:\n$$r^{*} = \\sum_{i=1}^{N} (\\alpha w_{i}) \\bar{x}_{i}$$\nSince the scaling factor $\\alpha$ is a constant common to all terms in the summation, it can be factored out:\n$$r^{*} = \\alpha \\left( \\sum_{i=1}^{N} w_{i} \\bar{x}_{i} \\right)$$\nThe term in the parentheses is the initial average firing rate, $r_{\\text{initial}}$. Thus, the relationship is:\n$$r^{*} = \\alpha \\cdot r_{\\text{initial}}$$\nTo find the required scaling factor $\\alpha$, we can rearrange this equation, assuming $r_{\\text{initial}} \\neq 0$:\n$$\\alpha = \\frac{r^{*}}{r_{\\text{initial}}} = \\frac{r^{*}}{\\sum_{i=1}^{N} w_{i} \\bar{x}_{i}}$$\nThis is the general analytical expression for the scaling factor $\\alpha$.\n\nNow, we evaluate this expression for the specific case provided. The givens are:\nNumber of inputs, $N=3$.\nPresynaptic average rates: $\\bar{x}_{1}=5\\,\\text{Hz}$, $\\bar{x}_{2}=7\\,\\text{Hz}$, $\\bar{x}_{3}=3\\,\\text{Hz}$.\nInitial synaptic weights: $w_{1}=0.8$, $w_{2}=0.5$, $w_{3}=-0.2$.\nTarget average rate: $r^{*}=10\\,\\text{Hz}$.\n\nFirst, we calculate the initial firing rate, $r_{\\text{initial}}$, which is the denominator in our expression for $\\alpha$:\n$$r_{\\text{initial}} = \\sum_{i=1}^{3} w_{i} \\bar{x}_{i} = w_{1}\\bar{x}_{1} + w_{2}\\bar{x}_{2} + w_{3}\\bar{x}_{3}$$\nSubstituting the numerical values:\n$$r_{\\text{initial}} = (0.8)(5) + (0.5)(7) + (-0.2)(3)$$\n$$r_{\\text{initial}} = 4.0 + 3.5 - 0.6 = 6.9\\,\\text{Hz}$$\nSince $r_{\\text{initial}} = 6.9 \\neq 0$, the scaling factor $\\alpha$ is well-defined.\n\nNow, we can calculate $\\alpha$ using the target rate $r^{*}=10\\,\\text{Hz}$:\n$$\\alpha = \\frac{r^{*}}{r_{\\text{initial}}} = \\frac{10}{6.9}$$\n$$\\alpha \\approx 1.44927536...$$\nThe problem requires rounding the final numerical answer to four significant figures.\nThe first four significant figures are $1$, $4$, $4$, and $9$. The fifth digit is $2$, which is less than $5$, so we round down (i.e., we do not change the fourth digit).\nTherefore, the value of $\\alpha$ rounded to four significant figures is $1.449$.",
            "answer": "$$\\boxed{1.449}$$"
        },
        {
            "introduction": "While multiplicative scaling is the canonical model for this form of homeostasis, it's crucial to understand why. This exercise challenges you to compare multiplicative scaling with a plausible alternative—additive scaling—and evaluate them against key desiderata for synaptic plasticity, such as preserving non-negativity and the relative information encoded in synaptic weight ratios. Through this rigorous comparison, you will uncover the fundamental reasons why multiplicative updates are thought to be essential for maintaining stable synaptic circuits .",
            "id": "3989724",
            "problem": "Consider a single conductance-based neuron receiving inputs from $N$ excitatory synapses indexed by $i \\in \\{1,\\dots,N\\}$ with baseline synaptic weights $w_i \\ge 0$ and fixed presynaptic firing rates $r_i \\ge 0$. Assume linear summation of synaptic drive so that the mean excitatory drive is $I = \\sum_{i=1}^{N} w_i r_i$. A homeostatic mechanism aims to adjust all synaptic weights globally to match a strictly positive target mean drive $I^{\\star}  0$ while minimizing disruption of relative synaptic efficacies.\n\nTwo global update classes are considered: multiplicative scaling with a common factor $\\alpha \\in \\mathbb{R}$ applied as $w_i' = \\alpha w_i$ for all $i$, and additive scaling with a common offset $\\beta \\in \\mathbb{R}$ applied as $w_i' = w_i + \\beta$ for all $i$. For both updates, impose the following desiderata:\n- Non-negativity: all adjusted weights must satisfy $w_i' \\ge 0$ for all $i$.\n- Proportionality preservation: the relative efficacy ratios must be preserved, that is $w_i'/w_j' = w_i/w_j$ for all $i,j$ with $w_j  0$.\n\nStarting from the definitions above, and using only fundamental modeling assumptions for synaptic integration and homeostatic control, do the following:\n1. Derive necessary and sufficient conditions on $\\alpha$ for multiplicative scaling to satisfy non-negativity and proportionality preservation simultaneously for arbitrary nonnegative $\\{w_i\\}$. Repeat the derivation for additive scaling to obtain necessary and sufficient conditions on $\\beta$ that ensure non-negativity and proportionality preservation simultaneously for arbitrary nonnegative $\\{w_i\\}$. Explicitly state any degenerate cases you identify.\n2. Enforce the homeostatic target $\\sum_{i=1}^{N} w_i' r_i = I^{\\star}$ under each update class and solve symbolically for the parameter ($\\alpha$ in the multiplicative case and $\\beta$ in the additive case) in terms of $\\{w_i\\}$, $\\{r_i\\}$, and $I^{\\star}$. State clearly any regularity conditions required for existence and uniqueness of the solutions you derive.\n3. By analyzing the effect of each update on the ratios $w_i'/w_j'$, show rigorously that additive scaling distorts these ratios for generic weight configurations except when $\\beta = 0$; characterize any non-generic, degenerate sets of $\\{w_i\\}$ for which additive scaling does not distort ratios.\n\nAssume $\\sum_{i=1}^{N} r_i  0$ and $\\sum_{i=1}^{N} w_i r_i  0$ so that both the baseline drive and the total presynaptic drive are strictly positive. Provide your reasoning and intermediate steps. Finally, report the closed-form analytic expression for the unique multiplicative scaling factor $\\alpha$ that achieves the target $I^{\\star}$ while satisfying the desiderata above. The final answer must be a single closed-form expression with no units.",
            "solution": "The problem statement is a valid exercise in computational neuroscience, presenting a well-posed and scientifically grounded scenario for analyzing homeostatic synaptic plasticity. It is self-contained, with all necessary definitions and assumptions provided to derive a unique and meaningful solution. The problem is free of scientific inaccuracies, contradictions, and ambiguities. We may therefore proceed with a formal solution.\n\nThe analysis is structured into three parts as requested by the problem statement, followed by the determination of the final expression for the multiplicative scaling factor.\n\n**Part 1: Derivation of Necessary and Sufficient Conditions for Desiderata**\n\nWe analyze the two proposed update classes, multiplicative and additive scaling, against the two desiderata: non-negativity ($w_i' \\ge 0$) and proportionality preservation ($w_i'/w_j' = w_i/w_j$ for $w_j  0$). These conditions must hold for an arbitrary set of initial non-negative weights $\\{w_i\\}$.\n\n**Multiplicative Scaling:**\nThe update rule is $w_i' = \\alpha w_i$ for $\\alpha \\in \\mathbb{R}$.\n\n1.  **Non-negativity:** The condition is $w_i' = \\alpha w_i \\ge 0$. Since this must hold for any arbitrary initial weight set $\\{w_i\\}$ where $w_i \\ge 0$, we can consider a non-degenerate case where at least one weight $w_k  0$. For this weight, the condition becomes $\\alpha w_k \\ge 0$, which implies $\\alpha \\ge 0$. This is a necessary condition. It is also sufficient, because if $\\alpha \\ge 0$ and $w_i \\ge 0$ for all $i \\in \\{1, \\dots, N\\}$, then their product $\\alpha w_i$ is guaranteed to be non-negative.\n    Thus, the necessary and sufficient condition for non-negativity is $\\alpha \\ge 0$.\n\n2.  **Proportionality Preservation:** The condition is $w_i'/w_j' = w_i/w_j$ for any pair $i, j$ where $w_j  0$. Substituting the update rule, we have:\n    $$ \\frac{\\alpha w_i}{\\alpha w_j} = \\frac{w_i}{w_j} $$\n    For this equation to be meaningful and for the fraction on the left to be well-defined, the denominator $\\alpha w_j$ cannot be zero. Since we consider cases where $w_j  0$, we must have $\\alpha \\ne 0$. If $\\alpha \\ne 0$, we can cancel $\\alpha$ from the numerator and denominator, yielding the identity $w_i/w_j = w_i/w_j$, which is always true. Therefore, proportionality is preserved if and only if $\\alpha \\ne 0$. A degenerate case occurs if all $w_i=0$, in which case the condition is vacuously satisfied for any $\\alpha$, but the problem assumes a non-trivial baseline drive $\\sum w_i r_i  0$, so not all $w_i$ can be zero if the corresponding $r_i$ are positive.\n\n3.  **Simultaneous Conditions:** To satisfy both non-negativity ($\\alpha \\ge 0$) and proportionality preservation ($\\alpha \\ne 0$) simultaneously, the scaling factor must satisfy $\\alpha  0$.\n\n**Additive Scaling:**\nThe update rule is $w_i' = w_i + \\beta$ for $\\beta \\in \\mathbb{R}$.\n\n1.  **Non-negativity:** The condition is $w_i' = w_i + \\beta \\ge 0$. This must hold for all $w_i \\ge 0$. The most restrictive case is for the smallest possible initial weight, which is $w_i = 0$. For non-negativity to hold, we must have $0 + \\beta \\ge 0$, which implies $\\beta \\ge 0$. This is a necessary condition. It is also sufficient, as if $\\beta \\ge 0$ and $w_i \\ge 0$, their sum $w_i + \\beta$ is always non-negative.\n    Thus, the necessary and sufficient condition for non-negativity is $\\beta \\ge 0$.\n\n2.  **Proportionality Preservation:** The condition is $w_i'/w_j' = w_i/w_j$ for $w_j  0$.\n    $$ \\frac{w_i + \\beta}{w_j + \\beta} = \\frac{w_i}{w_j} $$\n    Cross-multiplying gives:\n    $$ w_j (w_i + \\beta) = w_i (w_j + \\beta) $$\n    $$ w_i w_j + \\beta w_j = w_i w_j + \\beta w_i $$\n    $$ \\beta w_j = \\beta w_i $$\n    $$ \\beta (w_j - w_i) = 0 $$\n    This equation must hold for an arbitrary set of non-negative weights $\\{w_i\\}$. In a generic configuration, we can choose weights $w_i$ and $w_j$ such that $w_i \\ne w_j$. For the equation to hold in this general case, we must have $\\beta = 0$.\n\n3.  **Simultaneous Conditions:** To satisfy both non-negativity ($\\beta \\ge 0$) and proportionality preservation ($\\beta = 0$), the only possible value for the additive offset is $\\beta = 0$. This corresponds to a trivial update where the weights are not changed. A degenerate case where proportionality is preserved for any $\\beta \\ge 0$ occurs if all initial weights are identical, $w_i = c$ for all $i$. In this specific scenario, $w_j - w_i = 0$, and the condition $\\beta(w_j-w_i)=0$ is satisfied for any $\\beta$.\n\n**Part 2: Solving for Scaling Parameters**\n\nWe now enforce the homeostatic target $I' = \\sum_{i=1}^{N} w_i' r_i = I^{\\star}$.\n\n**Multiplicative Scaling:**\nThe target equation is $\\sum_{i=1}^{N} (\\alpha w_i) r_i = I^{\\star}$. Factoring out the constant $\\alpha$:\n$$ \\alpha \\sum_{i=1}^{N} w_i r_i = I^{\\star} $$\nThe sum term is the baseline drive, $I = \\sum_{i=1}^{N} w_i r_i$. So, $\\alpha I = I^{\\star}$.\nSolving for $\\alpha$:\n$$ \\alpha = \\frac{I^{\\star}}{I} = \\frac{I^{\\star}}{\\sum_{i=1}^{N} w_i r_i} $$\nFor this solution to exist and be unique, the denominator must be non-zero. The problem explicitly assumes that the baseline drive is strictly positive, i.e., $\\sum_{i=1}^{N} w_i r_i  0$. Since the target drive $I^{\\star}$ is also strictly positive, the resulting $\\alpha$ will be strictly positive ($\\alpha  0$). This is fully consistent with the necessary and sufficient condition derived in Part 1 for satisfying both desiderata.\n\n**Additive Scaling:**\nThe target equation is $\\sum_{i=1}^{N} (w_i + \\beta) r_i = I^{\\star}$. Distributing the sum:\n$$ \\sum_{i=1}^{N} w_i r_i + \\sum_{i=1}^{N} \\beta r_i = I^{\\star} $$\n$$ I + \\beta \\sum_{i=1}^{N} r_i = I^{\\star} $$\nSolving for $\\beta$:\n$$ \\beta = \\frac{I^{\\star} - I}{\\sum_{i=1}^{N} r_i} = \\frac{I^{\\star} - \\sum_{i=1}^{N} w_i r_i}{\\sum_{i=1}^{N} r_i} $$\nFor this solution to exist and be unique, the denominator must be non-zero. The problem explicitly assumes $\\sum_{i=1}^{N} r_i  0$. Therefore, a unique solution for $\\beta$ always exists. However, as shown in Part 1, additive scaling only preserves proportionality for generic weights if $\\beta = 0$, which implies $I^{\\star} = I$. Thus, additive scaling cannot, in general, reach an arbitrary target $I^{\\star} \\ne I$ without distorting the relative weight structure. Furthermore, for non-negativity we need $\\beta \\ge 0$, which requires $I^{\\star} \\ge I$. If the target drive is lower than the current drive, additive scaling would require making some weights negative, violating the non-negativity constraint for synapses with initially small weights.\n\n**Part 3: Analysis of Ratio Distortion by Additive Scaling**\n\nAs derived in Part 1, the condition for additive scaling to preserve the ratio $w_i/w_j$ is $\\beta(w_j - w_i) = 0$.\nFor a generic set of weights $\\{w_k\\}$, not all weights are equal. This means there exists at least one pair of indices $(i, j)$ for which $w_i \\ne w_j$. For such a pair, the term $(w_j - w_i)$ is non-zero. Consequently, for the equation $\\beta(w_j - w_i) = 0$ to hold, it must be that $\\beta = 0$.\nA non-zero additive shift ($\\beta \\ne 0$) will therefore distort the ratio for any pair of synapses with initially different weights. Let $R_{ij} = w_i/w_j$. The new ratio is $R'_{ij} = (w_i+\\beta)/(w_j+\\beta)$. If we assume $\\beta  0$ and $w_i  w_j  0$, then we can show $R'_{ij}  R_{ij}$. The transformation compresses the ratios towards $1$.\n\nThe only non-generic, degenerate set of weights $\\{w_i\\}$ for which additive scaling preserves all ratios for any $\\beta$ is the set where all weights are equal.\nLet $w_i = c$ for all $i \\in \\{1, \\dots, N\\}$ for some constant $c \\ge 0$.\nThen for any pair $(i, j)$ with $c0$, the initial ratio is $w_i/w_j = c/c = 1$.\nThe updated weights are $w_i' = c+\\beta$. The new ratio is $w_i'/w_j' = (c+\\beta)/(c+\\beta) = 1$.\nThe ratio is preserved. If $c=0$, all weights are zero and the concept of ratio is ill-defined. In summary, additive scaling is structure-preserving only for the highly non-generic case of a completely uniform weight distribution.\n\n**Conclusion and Final Answer Formulation:**\nThe analysis rigorously demonstrates that only multiplicative scaling with a factor $\\alpha  0$ can achieve the homeostatic target $I^\\star$ while strictly preserving the non-negativity of weights and the relative efficacy of all synapses for an arbitrary initial weight distribution. The unique scaling factor that accomplishes this is derived in Part 2. The final answer required is the closed-form expression for this factor.",
            "answer": "$$\n\\boxed{\\frac{I^{\\star}}{\\sum_{i=1}^{N} w_i r_i}}\n$$"
        },
        {
            "introduction": "Homeostatic regulation is a dynamic feedback process, and the interplay of timescales is critical for its stability. This advanced practice moves from static rules to the dynamics of the control loop, investigating how delays in the system can lead to unintended consequences like oscillations. By analyzing a linearized model of homeostatic control, you will derive the conditions required for smooth, stable convergence and develop an intuition for why a separation of timescales is a key principle in neural regulation .",
            "id": "3989748",
            "problem": "Consider a single-population firing rate model with homeostatic synaptic scaling. Let the population firing rate be $r(t)$ and the synaptic scaling controller be $c(t)$, which multiplicatively modulates the effective synaptic drive. Around a desired setpoint $r^{*}$, the controller attempts to reduce the error in firing rate via negative feedback driven by the deviation $r(t) - r^{*}$. Assume that, near the operating point, the dynamics are well approximated by a linear system in the small deviations $x(t) \\equiv r(t) - r^{*}$ and $y(t) \\equiv c(t) - c^{*}$ of the form\n$$\n\\tau_{r} \\frac{d x(t)}{dt} = - x(t) + G\\, y(t), \\quad \\frac{d y(t)}{dt} = - \\frac{1}{\\tau_{c}}\\, y(t) - \\eta\\, x(t),\n$$\nwhere $\\tau_{r}  0$ is the intrinsic rate relaxation time constant of the population, $\\tau_{c}  0$ is the controller time constant that sets the speed of synaptic scaling, $G  0$ is the local linear gain mapping controller perturbations to rate perturbations, and $\\eta  0$ is the learning rate that sets the strength of rate-error feedback onto the controller. These parameters are all constant. The above model is a standard linearization of homeostatic synaptic scaling: the rate relaxes to its baseline on the timescale $\\tau_{r}$, the controller slowly decays on the timescale $\\tau_{c}$, and the controller uses the error $x(t)$ to correct $y(t)$ proportionally with sensitivity $\\eta$.\n\nExplain, using dynamical systems reasoning, how mismatched timescales (for example, $\\tau_{c}$ comparable to $\\tau_{r}$ while $\\eta$ is large) can cause oscillations in $r(t)$ during the homeostatic process. Then, starting from the linearized system above and first principles of linear stability for two-dimensional systems, derive the exact analytic expression for the largest learning rate, as a function of $\\tau_{r}$, $\\tau_{c}$, and $G$, that guarantees overdamped convergence of $r(t)$ to $r^{*}$ (that is, ensures the system’s eigenvalues are real and negative). Report your final answer as a single closed-form analytic expression depending on $\\tau_{r}$, $\\tau_{c}$, and $G$. No numerical values are provided or required, and no rounding is requested.",
            "solution": "The problem asks for two things: first, a qualitative explanation of how mismatched timescales in the provided homeostatic plasticity model can lead to oscillations; and second, the derivation of the maximum learning rate $\\eta$ that ensures overdamped (non-oscillatory) convergence to the fixed point.\n\nFirst, we will address the qualitative explanation. The system of equations describes a negative feedback control loop. Let's analyze the causal chain:\n$1$. A deviation of the firing rate from its setpoint, $x(t) = r(t) - r^*$, is detected.\n$2$. According to the controller dynamics, $\\frac{d y(t)}{dt} = - \\eta\\, x(t) - \\frac{1}{\\tau_{c}}\\, y(t)$, a positive error $x(t)  0$ drives the controller value $y(t)$ to decrease. The strength of this corrective drive is proportional to the learning rate $\\eta$.\n$3$. According to the rate dynamics, $\\tau_{r} \\frac{d x(t)}{dt} = - x(t) + G\\, y(t)$, a decrease in the controller value $y(t)$ (i.e., $y(t)  0$) drives the firing rate deviation $x(t)$ to decrease.\nThis completes the negative feedback loop: an increase in firing rate ultimately causes a corrective action that decreases the firing rate.\n\nOscillations in such a feedback system typically arise from a combination of strong feedback gain and significant phase lag (delays) in the loop. In this model:\n- The learning rate $\\eta$ controls the gain of the feedback. A large $\\eta$ means a strong corrective response to any given error $x(t)$.\n- The time constants $\\tau_r$ and $\\tau_c$ introduce delays. $\\tau_r$ is the intrinsic timescale for the firing rate to respond to its inputs, and $\\tau_c$ is the timescale for the controller to integrate the error signal and adjust itself.\n\nThe problem describes the case where $\\tau_c$ is comparable to $\\tau_r$ and $\\eta$ is large. A large $\\eta$ implies a very strong \"kick\" from the controller in response to an error. If the controller's timescale $\\tau_c$ is not significantly longer than the rate's timescale $\\tau_r$, the controller is not acting on a slowly averaged error but rather on its more instantaneous value. The combination of a strong, fast-acting controller leads to overcorrection. For example, suppose a perturbation causes $x(t)$ to become positive. The large $\\eta$ causes a strong and rapid decrease in $y(t)$. This sharp drop in $y(t)$, via the gain $G$, pushes $x(t)$ down. However, because of the system's intrinsic delays and the strength of the correction, $x(t)$ will not smoothly return to $0$ but will overshoot, becoming negative. This new negative error $x(t)  0$ then triggers a strong and rapid increase in $y(t)$, which in turn pushes $x(t)$ back up, causing it to overshoot $0$ in the positive direction. This cycle of overcorrections manifests as damped oscillations around the setpoint $r^*$.\n\nIn summary, oscillations arise because the corrective action, amplified by a large learning rate $\\eta$, is applied too forcefully and quickly relative to the system's ability to settle, leading to a series of overshoots. A very slow controller (large $\\tau_c$) would average out fast fluctuations in $x(t)$ and apply a smoother, more gradual correction, which is less likely to produce oscillations.\n\nNext, we derive the condition for overdamped convergence. We start by writing the system of linear differential equations in matrix form. First, we rewrite the equations as:\n$$\n\\frac{dx}{dt} = -\\frac{1}{\\tau_r} x(t) + \\frac{G}{\\tau_r} y(t)\n$$\n$$\n\\frac{dy}{dt} = -\\eta x(t) - \\frac{1}{\\tau_c} y(t)\n$$\nThis can be expressed as $\\frac{d\\mathbf{v}}{dt} = A \\mathbf{v}$, where $\\mathbf{v}(t) = \\begin{pmatrix} x(t) \\\\ y(t) \\end{pmatrix}$ and the Jacobian matrix $A$ is:\n$$\nA = \\begin{pmatrix} -\\frac{1}{\\tau_r}  \\frac{G}{\\tau_r} \\\\ -\\eta  -\\frac{1}{\\tau_c} \\end{pmatrix}\n$$\nThe dynamics of the system are determined by the eigenvalues $\\lambda$ of the matrix $A$. The eigenvalues are the roots of the characteristic equation $\\det(A - \\lambda I) = 0$, where $I$ is the identity matrix.\n$$\n\\det \\begin{pmatrix} -\\frac{1}{\\tau_r} - \\lambda  \\frac{G}{\\tau_r} \\\\ -\\eta  -\\frac{1}{\\tau_c} - \\lambda \\end{pmatrix} = 0\n$$\n$$\n\\left(-\\frac{1}{\\tau_r} - \\lambda\\right) \\left(-\\frac{1}{\\tau_c} - \\lambda\\right) - \\left(\\frac{G}{\\tau_r}\\right)(-\\eta) = 0\n$$\n$$\n\\left(\\lambda + \\frac{1}{\\tau_r}\\right) \\left(\\lambda + \\frac{1}{\\tau_c}\\right) + \\frac{\\eta G}{\\tau_r} = 0\n$$\nExpanding this equation gives the characteristic polynomial:\n$$\n\\lambda^2 + \\left(\\frac{1}{\\tau_r} + \\frac{1}{\\tau_c}\\right)\\lambda + \\left(\\frac{1}{\\tau_r \\tau_c} + \\frac{\\eta G}{\\tau_r}\\right) = 0\n$$\nThis is a quadratic equation for $\\lambda$. The fixed point at $(x, y) = (0, 0)$ is stable if the real parts of both eigenvalues are negative. The trace of the matrix is $\\text{Tr}(A) = -\\frac{1}{\\tau_r} - \\frac{1}{\\tau_c}$. Since $\\tau_r  0$ and $\\tau_c  0$, we have $\\text{Tr}(A)  0$. The determinant is $\\det(A) = \\frac{1}{\\tau_r \\tau_c} + \\frac{\\eta G}{\\tau_r}$. Since all parameters $\\tau_r, \\tau_c, G, \\eta$ are positive, we have $\\det(A)  0$. Because $\\text{Tr}(A) = \\lambda_1 + \\lambda_2  0$ and $\\det(A) = \\lambda_1 \\lambda_2  0$, the real parts of the eigenvalues are always negative, ensuring the system is stable.\n\nThe convergence to the fixed point is overdamped (non-oscillatory) if the eigenvalues are real and negative. This occurs when the discriminant of the quadratic characteristic equation, $\\Delta$, is non-negative ($\\Delta \\ge 0$). The roots of $a\\lambda^2 + b\\lambda + c = 0$ are given by $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$. The discriminant is $\\Delta = b^2 - 4ac$.\nIn our case, $a=1$, $b = \\frac{1}{\\tau_r} + \\frac{1}{\\tau_c}$, and $c = \\frac{1}{\\tau_r \\tau_c} + \\frac{\\eta G}{\\tau_r}$.\nThe condition for real eigenvalues is:\n$$\n\\Delta = \\left(\\frac{1}{\\tau_r} + \\frac{1}{\\tau_c}\\right)^2 - 4\\left(1\\right)\\left(\\frac{1}{\\tau_r \\tau_c} + \\frac{\\eta G}{\\tau_r}\\right) \\ge 0\n$$\nLet's expand and simplify the expression:\n$$\n\\left(\\frac{1}{\\tau_r^2} + \\frac{2}{\\tau_r \\tau_c} + \\frac{1}{\\tau_c^2}\\right) - \\frac{4}{\\tau_r \\tau_c} - \\frac{4\\eta G}{\\tau_r} \\ge 0\n$$\n$$\n\\frac{1}{\\tau_r^2} - \\frac{2}{\\tau_r \\tau_c} + \\frac{1}{\\tau_c^2} - \\frac{4\\eta G}{\\tau_r} \\ge 0\n$$\nThe first three terms form a perfect square:\n$$\n\\left(\\frac{1}{\\tau_r} - \\frac{1}{\\tau_c}\\right)^2 - \\frac{4\\eta G}{\\tau_r} \\ge 0\n$$\nNow, we solve for the learning rate $\\eta$.\n$$\n\\left(\\frac{1}{\\tau_r} - \\frac{1}{\\tau_c}\\right)^2 \\ge \\frac{4\\eta G}{\\tau_r}\n$$\nSince $\\tau_r  0$, $G  0$, and $4  0$, we can rearrange to isolate $\\eta$ without changing the inequality direction:\n$$\n\\eta \\le \\frac{\\tau_r}{4G} \\left(\\frac{1}{\\tau_r} - \\frac{1}{\\tau_c}\\right)^2\n$$\nThis inequality defines the range of $\\eta$ for which the system is overdamped. The question asks for the largest learning rate that guarantees overdamped convergence. This corresponds to the boundary case where the discriminant is zero ($\\Delta = 0$), which signifies critically damped behavior. This maximum value, let's call it $\\eta_{max}$, is:\n$$\n\\eta_{max} = \\frac{\\tau_r}{4G} \\left(\\frac{1}{\\tau_r} - \\frac{1}{\\tau_c}\\right)^2\n$$\nWe can simplify this expression further:\n$$\n\\eta_{max} = \\frac{\\tau_r}{4G} \\left(\\frac{\\tau_c - \\tau_r}{\\tau_r \\tau_c}\\right)^2 = \\frac{\\tau_r}{4G} \\frac{(\\tau_c - \\tau_r)^2}{\\tau_r^2 \\tau_c^2}\n$$\nCanceling one factor of $\\tau_r$ yields the final expression:\n$$\n\\eta_{max} = \\frac{(\\tau_c - \\tau_r)^2}{4G \\tau_r \\tau_c^2}\n$$\nSince the numerator is squared, this can also be written as $\\frac{(\\tau_r - \\tau_c)^2}{4G \\tau_r \\tau_c^2}$. For any $\\eta \\le \\eta_{max}$, the system eigenvalues will be real and negative, ensuring non-oscillatory, stable convergence to the setpoint.",
            "answer": "$$\n\\boxed{\\frac{(\\tau_r - \\tau_c)^2}{4 G \\tau_r \\tau_c^2}}\n$$"
        }
    ]
}