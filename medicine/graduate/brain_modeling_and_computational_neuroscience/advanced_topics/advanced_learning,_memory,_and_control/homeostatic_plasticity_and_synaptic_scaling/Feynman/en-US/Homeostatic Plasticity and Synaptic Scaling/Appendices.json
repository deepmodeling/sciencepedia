{
    "hands_on_practices": [
        {
            "introduction": "Synaptic scaling is a fundamental form of homeostatic plasticity that allows neurons to maintain a stable average firing rate. This first exercise models this process at its most basic level, treating it as a multiplicative gain control problem. By working through this idealized scenario , you will derive the precise scaling factor needed to adjust all synaptic weights to restore a neuron's activity to a desired setpoint, providing a quantitative foundation for how this negative feedback system operates.",
            "id": "4047498",
            "problem": "Consider a single neuron in a neuromorphic system implementing homeostatic plasticity through synaptic scaling. The neuron receives inputs from $N$ presynaptic channels with average input rates $\\bar{x}_{i}$ and synaptic weights $w_{i}$, and its average firing rate over a slow homeostatic timescale is modeled by the linear rate equation $r=\\sum_{i=1}^{N}w_{i}\\bar{x}_{i}$. A homeostatic controller seeks to achieve a target average firing rate $r^{*}$ by multiplicatively scaling all synaptic weights by a common positive factor $\\alpha$, while preserving the ratios of synaptic strengths. Assume that over the timescale on which the controller acts, the presynaptic averages $\\bar{x}_{i}$ remain fixed, and that the neuron's transfer function is well approximated by a linear mapping in this operating regime.\n\nDerive, from these premises alone, an analytical expression for the scaling factor $\\alpha$ that achieves the target rate $r^{*}$. Then, evaluate $\\alpha$ for the specific case with $N=3$, presynaptic averages $\\bar{x}_{1}=5\\,\\text{Hz}$, $\\bar{x}_{2}=7\\,\\text{Hz}$, $\\bar{x}_{3}=3\\,\\text{Hz}$, synaptic weights $w_{1}=0.8$, $w_{2}=0.5$, $w_{3}=-0.2$, and target average rate $r^{*}=10\\,\\text{Hz}$. Round your final numerical answer to four significant figures. The final answer should be reported as the single value of $\\alpha$.",
            "solution": "The problem is deemed valid as it is scientifically grounded in a standard model of homeostatic plasticity, well-posed with a clear objective and sufficient data, and expressed in objective, formalizable language.\n\nThe initial average firing rate of the neuron, which we can denote as $r_{\\text{initial}}$, is given by the linear rate equation:\n$$r_{\\text{initial}} = \\sum_{i=1}^{N} w_{i} \\bar{x}_{i}$$\nHere, $w_{i}$ are the initial synaptic weights and $\\bar{x}_{i}$ are the average presynaptic input rates.\n\nHomeostatic plasticity adjusts the synaptic weights to achieve a target average firing rate, $r^{*}$. The mechanism specified is a multiplicative scaling of all synaptic weights by a common positive factor, $\\alpha$. The new weights, $w'_{i}$, are therefore given by:\n$$w'_{i} = \\alpha w_{i}$$\nThe problem states that this scaling should result in the neuron's average firing rate becoming $r^{*}$. Using the same rate equation with the new weights, we have:\n$$r^{*} = \\sum_{i=1}^{N} w'_{i} \\bar{x}_{i}$$\nSubstituting the expression for the new weights, $w'_{i}$, into this equation:\n$$r^{*} = \\sum_{i=1}^{N} (\\alpha w_{i}) \\bar{x}_{i}$$\nSince the scaling factor $\\alpha$ is a constant common to all terms in the summation, it can be factored out:\n$$r^{*} = \\alpha \\left( \\sum_{i=1}^{N} w_{i} \\bar{x}_{i} \\right)$$\nThe term in the parentheses is the initial average firing rate, $r_{\\text{initial}}$. Thus, the relationship is:\n$$r^{*} = \\alpha \\cdot r_{\\text{initial}}$$\nTo find the required scaling factor $\\alpha$, we can rearrange this equation, assuming $r_{\\text{initial}} \\neq 0$:\n$$\\alpha = \\frac{r^{*}}{r_{\\text{initial}}} = \\frac{r^{*}}{\\sum_{i=1}^{N} w_{i} \\bar{x}_{i}}$$\nThis is the general analytical expression for the scaling factor $\\alpha$.\n\nNow, we evaluate this expression for the specific case provided. The givens are:\nNumber of inputs, $N=3$.\nPresynaptic average rates: $\\bar{x}_{1}=5\\,\\text{Hz}$, $\\bar{x}_{2}=7\\,\\text{Hz}$, $\\bar{x}_{3}=3\\,\\text{Hz}$.\nInitial synaptic weights: $w_{1}=0.8$, $w_{2}=0.5$, $w_{3}=-0.2$.\nTarget average rate: $r^{*}=10\\,\\text{Hz}$.\n\nFirst, we calculate the initial firing rate, $r_{\\text{initial}}$, which is the denominator in our expression for $\\alpha$:\n$$r_{\\text{initial}} = \\sum_{i=1}^{3} w_{i} \\bar{x}_{i} = w_{1}\\bar{x}_{1} + w_{2}\\bar{x}_{2} + w_{3}\\bar{x}_{3}$$\nSubstituting the numerical values:\n$$r_{\\text{initial}} = (0.8)(5) + (0.5)(7) + (-0.2)(3)$$\n$$r_{\\text{initial}} = 4.0 + 3.5 - 0.6 = 6.9\\,\\text{Hz}$$\nSince $r_{\\text{initial}} = 6.9 \\neq 0$, the scaling factor $\\alpha$ is well-defined.\n\nNow, we can calculate $\\alpha$ using the target rate $r^{*}=10\\,\\text{Hz}$:\n$$\\alpha = \\frac{r^{*}}{r_{\\text{initial}}} = \\frac{10}{6.9}$$\n$$\\alpha \\approx 1.44927536...$$\nThe problem requires rounding the final numerical answer to four significant figures.\nThe first four significant figures are $1$, $4$, $4$, and $9$. The fifth digit is $2$, which is less than $5$, so we round down (i.e., we do not change the fourth digit).\nTherefore, the value of $\\alpha$ rounded to four significant figures is $1.449$.",
            "answer": "$$\\boxed{1.449}$$"
        },
        {
            "introduction": "While multiplicative scaling is a canonical model, it is crucial to understand *why* it is preferred over other plausible mechanisms. This practice  invites you to critically compare multiplicative scaling with an additive alternative, evaluating each against the biologically important desiderata of preserving non-negative weights and, most importantly, the relative ratios of synaptic efficacies. This analysis reveals the unique ability of multiplicative scaling to globally regulate activity without overwriting the specific patterns of synaptic strengths that may encode memories.",
            "id": "3989724",
            "problem": "Consider a single conductance-based neuron receiving inputs from $N$ excitatory synapses indexed by $i \\in \\{1,\\dots,N\\}$ with baseline synaptic weights $w_i \\ge 0$ and fixed presynaptic firing rates $r_i \\ge 0$. Assume linear summation of synaptic drive so that the mean excitatory drive is $I = \\sum_{i=1}^{N} w_i r_i$. A homeostatic mechanism aims to adjust all synaptic weights globally to match a strictly positive target mean drive $I^{\\star}  0$ while minimizing disruption of relative synaptic efficacies.\n\nTwo global update classes are considered: multiplicative scaling with a common factor $\\alpha \\in \\mathbb{R}$ applied as $w_i' = \\alpha w_i$ for all $i$, and additive scaling with a common offset $\\beta \\in \\mathbb{R}$ applied as $w_i' = w_i + \\beta$ for all $i$. For both updates, impose the following desiderata:\n- Non-negativity: all adjusted weights must satisfy $w_i' \\ge 0$ for all $i$.\n- Proportionality preservation: the relative efficacy ratios must be preserved, that is $w_i'/w_j' = w_i/w_j$ for all $i,j$ with $w_j  0$.\n\nStarting from the definitions above, and using only fundamental modeling assumptions for synaptic integration and homeostatic control, do the following:\n1. Derive necessary and sufficient conditions on $\\alpha$ for multiplicative scaling to satisfy non-negativity and proportionality preservation simultaneously for arbitrary nonnegative $\\{w_i\\}$. Repeat the derivation for additive scaling to obtain necessary and sufficient conditions on $\\beta$ that ensure non-negativity and proportionality preservation simultaneously for arbitrary nonnegative $\\{w_i\\}$. Explicitly state any degenerate cases you identify.\n2. Enforce the homeostatic target $\\sum_{i=1}^{N} w_i' r_i = I^{\\star}$ under each update class and solve symbolically for the parameter ($\\alpha$ in the multiplicative case and $\\beta$ in the additive case) in terms of $\\{w_i\\}$, $\\{r_i\\}$, and $I^{\\star}$. State clearly any regularity conditions required for existence and uniqueness of the solutions you derive.\n3. By analyzing the effect of each update on the ratios $w_i'/w_j'$, show rigorously that additive scaling distorts these ratios for generic weight configurations except when $\\beta = 0$; characterize any non-generic, degenerate sets of $\\{w_i\\}$ for which additive scaling does not distort ratios.\n\nAssume $\\sum_{i=1}^{N} r_i  0$ and $\\sum_{i=1}^{N} w_i r_i  0$ so that both the baseline drive and the total presynaptic drive are strictly positive. Provide your reasoning and intermediate steps. Finally, report the closed-form analytic expression for the unique multiplicative scaling factor $\\alpha$ that achieves the target $I^{\\star}$ while satisfying the desiderata above. The final answer must be a single closed-form expression with no units.",
            "solution": "The problem statement is a valid exercise in computational neuroscience, presenting a well-posed and scientifically grounded scenario for analyzing homeostatic synaptic plasticity. It is self-contained, with all necessary definitions and assumptions provided to derive a unique and meaningful solution. The problem is free of scientific inaccuracies, contradictions, and ambiguities. We may therefore proceed with a formal solution.\n\nThe analysis is structured into three parts as requested by the problem statement, followed by the determination of the final expression for the multiplicative scaling factor.\n\n**Part 1: Derivation of Necessary and Sufficient Conditions for Desiderata**\n\nWe analyze the two proposed update classes, multiplicative and additive scaling, against the two desiderata: non-negativity ($w_i' \\ge 0$) and proportionality preservation ($w_i'/w_j' = w_i/w_j$ for $w_j  0$). These conditions must hold for an arbitrary set of initial non-negative weights $\\{w_i\\}$.\n\n**Multiplicative Scaling:**\nThe update rule is $w_i' = \\alpha w_i$ for $\\alpha \\in \\mathbb{R}$.\n\n1.  **Non-negativity:** The condition is $w_i' = \\alpha w_i \\ge 0$. Since this must hold for any arbitrary initial weight set $\\{w_i\\}$ where $w_i \\ge 0$, we can consider a non-degenerate case where at least one weight $w_k  0$. For this weight, the condition becomes $\\alpha w_k \\ge 0$, which implies $\\alpha \\ge 0$. This is a necessary condition. It is also sufficient, because if $\\alpha \\ge 0$ and $w_i \\ge 0$ for all $i \\in \\{1, \\dots, N\\}$, then their product $\\alpha w_i$ is guaranteed to be non-negative.\n    Thus, the necessary and sufficient condition for non-negativity is $\\alpha \\ge 0$.\n\n2.  **Proportionality Preservation:** The condition is $w_i'/w_j' = w_i/w_j$ for any pair $i, j$ where $w_j  0$. Substituting the update rule, we have:\n    $$ \\frac{\\alpha w_i}{\\alpha w_j} = \\frac{w_i}{w_j} $$\n    For this equation to be meaningful and for the fraction on the left to be well-defined, the denominator $\\alpha w_j$ cannot be zero. Since we consider cases where $w_j  0$, we must have $\\alpha \\ne 0$. If $\\alpha \\ne 0$, we can cancel $\\alpha$ from the numerator and denominator, yielding the identity $w_i/w_j = w_i/w_j$, which is always true. Therefore, proportionality is preserved if and only if $\\alpha \\ne 0$. A degenerate case occurs if all $w_i=0$, in which case the condition is vacuously satisfied for any $\\alpha$, but the problem assumes a non-trivial baseline drive $\\sum w_i r_i  0$, so not all $w_i$ can be zero if the corresponding $r_i$ are positive.\n\n3.  **Simultaneous Conditions:** To satisfy both non-negativity ($\\alpha \\ge 0$) and proportionality preservation ($\\alpha \\ne 0$) simultaneously, the scaling factor must satisfy $\\alpha  0$.\n\n**Additive Scaling:**\nThe update rule is $w_i' = w_i + \\beta$ for $\\beta \\in \\mathbb{R}$.\n\n1.  **Non-negativity:** The condition is $w_i' = w_i + \\beta \\ge 0$. This must hold for all $w_i \\ge 0$. The most restrictive case is for the smallest possible initial weight, which is $w_i = 0$. For non-negativity to hold, we must have $0 + \\beta \\ge 0$, which implies $\\beta \\ge 0$. This is a necessary condition. It is also sufficient, as if $\\beta \\ge 0$ and $w_i \\ge 0$, their sum $w_i + \\beta$ is always non-negative.\n    Thus, the necessary and sufficient condition for non-negativity is $\\beta \\ge 0$.\n\n2.  **Proportionality Preservation:** The condition is $w_i'/w_j' = w_i/w_j$ for $w_j  0$.\n    $$ \\frac{w_i + \\beta}{w_j + \\beta} = \\frac{w_i}{w_j} $$\n    Cross-multiplying gives:\n    $$ w_j (w_i + \\beta) = w_i (w_j + \\beta) $$\n    $$ w_i w_j + \\beta w_j = w_i w_j + \\beta w_i $$\n    $$ \\beta w_j = \\beta w_i $$\n    $$ \\beta (w_j - w_i) = 0 $$\n    This equation must hold for an arbitrary set of non-negative weights $\\{w_i\\}$. In a generic configuration, we can choose weights $w_i$ and $w_j$ such that $w_i \\ne w_j$. For the equation to hold in this general case, we must have $\\beta = 0$.\n\n3.  **Simultaneous Conditions:** To satisfy both non-negativity ($\\beta \\ge 0$) and proportionality preservation ($\\beta = 0$), the only possible value for the additive offset is $\\beta = 0$. This corresponds to a trivial update where the weights are not changed. A degenerate case where proportionality is preserved for any $\\beta \\ge 0$ occurs if all initial weights are identical, $w_i = c$ for all $i$. In this specific scenario, $w_j - w_i = 0$, and the condition $\\beta(w_j-w_i)=0$ is satisfied for any $\\beta$.\n\n**Part 2: Solving for Scaling Parameters**\n\nWe now enforce the homeostatic target $I' = \\sum_{i=1}^{N} w_i' r_i = I^{\\star}$.\n\n**Multiplicative Scaling:**\nThe target equation is $\\sum_{i=1}^{N} (\\alpha w_i) r_i = I^{\\star}$. Factoring out the constant $\\alpha$:\n$$ \\alpha \\sum_{i=1}^{N} w_i r_i = I^{\\star} $$\nThe sum term is the baseline drive, $I = \\sum_{i=1}^{N} w_i r_i$. So, $\\alpha I = I^{\\star}$.\nSolving for $\\alpha$:\n$$ \\alpha = \\frac{I^{\\star}}{I} = \\frac{I^{\\star}}{\\sum_{i=1}^{N} w_i r_i} $$\nFor this solution to exist and be unique, the denominator must be non-zero. The problem explicitly assumes that the baseline drive is strictly positive, i.e., $\\sum_{i=1}^{N} w_i r_i  0$. Since the target drive $I^{\\star}$ is also strictly positive, the resulting $\\alpha$ will be strictly positive ($\\alpha  0$). This is fully consistent with the necessary and sufficient condition derived in Part 1 for satisfying both desiderata.\n\n**Additive Scaling:**\nThe target equation is $\\sum_{i=1}^{N} (w_i + \\beta) r_i = I^{\\star}$. Distributing the sum:\n$$ \\sum_{i=1}^{N} w_i r_i + \\sum_{i=1}^{N} \\beta r_i = I^{\\star} $$\n$$ I + \\beta \\sum_{i=1}^{N} r_i = I^{\\star} $$\nSolving for $\\beta$:\n$$ \\beta = \\frac{I^{\\star} - I}{\\sum_{i=1}^{N} r_i} = \\frac{I^{\\star} - \\sum_{i=1}^{N} w_i r_i}{\\sum_{i=1}^{N} r_i} $$\nFor this solution to exist and be unique, the denominator must be non-zero. The problem explicitly assumes $\\sum_{i=1}^{N} r_i  0$. Therefore, a unique solution for $\\beta$ always exists. However, as shown in Part 1, additive scaling only preserves proportionality for generic weights if $\\beta = 0$, which implies $I^{\\star} = I$. Thus, additive scaling cannot, in general, reach an arbitrary target $I^{\\star} \\ne I$ without distorting the relative weight structure. Furthermore, for non-negativity we need $\\beta \\ge 0$, which requires $I^{\\star} \\ge I$. If the target drive is lower than the current drive, additive scaling would require making some weights negative, violating the non-negativity constraint for synapses with initially small weights.\n\n**Part 3: Analysis of Ratio Distortion by Additive Scaling**\n\nAs derived in Part 1, the condition for additive scaling to preserve the ratio $w_i/w_j$ is $\\beta(w_j - w_i) = 0$.\nFor a generic set of weights $\\{w_k\\}$, not all weights are equal. This means there exists at least one pair of indices $(i, j)$ for which $w_i \\ne w_j$. For such a pair, the term $(w_j - w_i)$ is non-zero. Consequently, for the equation $\\beta(w_j - w_i) = 0$ to hold, it must be that $\\beta = 0$.\nA non-zero additive shift ($\\beta \\ne 0$) will therefore distort the ratio for any pair of synapses with initially different weights. Let $R_{ij} = w_i/w_j$. The new ratio is $R'_{ij} = (w_i+\\beta)/(w_j+\\beta)$. If we assume $\\beta  0$ and $w_i  w_j  0$, then we can show $R'_{ij}  R_{ij}$. The transformation compresses the ratios towards $1$.\n\nThe only non-generic, degenerate set of weights $\\{w_i\\}$ for which additive scaling preserves all ratios for any $\\beta$ is the set where all weights are equal.\nLet $w_i = c$ for all $i \\in \\{1, \\dots, N\\}$ for some constant $c \\ge 0$.\nThen for any pair $(i, j)$ with $c0$, the initial ratio is $w_i/w_j = c/c = 1$.\nThe updated weights are $w_i' = c+\\beta$. The new ratio is $w_i'/w_j' = (c+\\beta)/(c+\\beta) = 1$.\nThe ratio is preserved. If $c=0$, all weights are zero and the concept of ratio is ill-defined. In summary, additive scaling is structure-preserving only for the highly non-generic case of a completely uniform weight distribution.\n\n**Conclusion and Final Answer Formulation:**\nThe analysis rigorously demonstrates that only multiplicative scaling with a factor $\\alpha  0$ can achieve the homeostatic target $I^\\star$ while strictly preserving the non-negativity of weights and the relative efficacy of all synapses for an arbitrary initial weight distribution. The unique scaling factor that accomplishes this is derived in Part 2. The final answer required is the closed-form expression for this factor.",
            "answer": "$$\n\\boxed{\\frac{I^{\\star}}{\\sum_{i=1}^{N} w_i r_i}}\n$$"
        },
        {
            "introduction": "Homeostatic regulation is not an instantaneous event but a dynamic process that unfolds over time. This exercise  moves beyond the static calculation of a scaling factor to model its continuous, error-driven adjustment. By deriving the time constant of convergence for a linear rate neuron, you will gain insight into the temporal characteristics of homeostatic plasticity and how its feedback loop brings a neuron's firing rate to its target exponentially.",
            "id": "3989736",
            "problem": "Consider a single postsynaptic unit modeled as a linear rate neuron, whose instantaneous firing rate $r(t)$ equals the weighted sum of its inputs. Let there be $N$ presynaptic inputs with activities $x_i$ that are constant in time and a synaptic weight vector whose elements are $w_i(t)$. Assume a synaptic scaling architecture in which all synapses share a global multiplicative factor $\\alpha(t)$ applied to fixed base weights $v_i$, so that $w_i(t) = \\alpha(t) v_i$. The postsynaptic target firing rate is $r^{*}$, and homeostatic plasticity is implemented by an error-driven scaling rule that adjusts the global factor according to the difference between the target and actual firing rates, with a learning rate $\\eta  0$. Specifically, the rate dynamics of $\\alpha(t)$ are defined by the ordinary differential equation (ODE) $\\dot{\\alpha}(t) = \\eta \\left(r^{*} - r(t)\\right)$. Assume all $x_i$ and $v_i$ are time-invariant constants, and that the total effective input $S = \\sum_{i=1}^{N} v_i x_i$ is strictly positive.\n\nStarting from the foundational definitions of a linear rate neuron with $r(t) = \\sum_{i=1}^{N} w_i(t) x_i$ and the given error-driven scaling ODE, derive the closed-form time constant of convergence $\\tau$ that characterizes the exponential relaxation of $r(t)$ toward $r^{*}$. Express your final answer as a single exact symbolic expression in terms of $\\eta$, $v_i$, and $x_i$. No numerical evaluation or rounding is required. Do not introduce any additional assumptions beyond those stated above. If you use any abbreviations, define them at first appearance (e.g., Ordinary Differential Equation (ODE)).",
            "solution": "The problem statement is first subjected to a validation process to ensure its scientific and logical integrity.\n\n**Problem Validation**\n\n**Step 1: Extracted Givens**\n-   **Neuron Model**: A single postsynaptic unit is modeled as a linear rate neuron.\n-   **Firing Rate Definition**: The instantaneous firing rate $r(t)$ is given by $r(t) = \\sum_{i=1}^{N} w_i(t) x_i$.\n-   **Inputs**: There are $N$ presynaptic inputs with activities $x_i$, which are constant in time.\n-   **Synaptic Weights**: The synaptic weights $w_i(t)$ are time-dependent.\n-   **Synaptic Scaling Architecture**: The weights are defined by $w_i(t) = \\alpha(t) v_i$, where $\\alpha(t)$ is a global multiplicative factor and $v_i$ are fixed, time-invariant base weights.\n-   **Target Rate**: The postsynaptic target firing rate is a constant, $r^{*}$.\n-   **Plasticity Rule**: Homeostatic plasticity is governed by the Ordinary Differential Equation (ODE), $\\dot{\\alpha}(t) = \\eta \\left(r^{*} - r(t)\\right)$.\n-   **Learning Rate**: The learning rate $\\eta$ is a positive constant, $\\eta  0$.\n-   **Total Effective Input**: A quantity $S$ is defined as $S = \\sum_{i=1}^{N} v_i x_i$.\n-   **Constraint**: The total effective input is strictly positive, $S  0$.\n-   **Objective**: To derive the closed-form time constant of convergence, $\\tau$, for the exponential relaxation of $r(t)$ towards $r^{*}$. The result must be expressed in terms of $\\eta$, $v_i$, and $x_i$.\n\n**Step 2: Validation Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is well-grounded in computational neuroscience. The linear rate neuron, synaptic scaling, and error-driven homeostatic plasticity are standard, albeit simplified, models used to study neural dynamics and learning. The formulation is a canonical representation of this process.\n-   **Well-Posed**: The problem is well-posed. It provides a system of equations describing the dynamics of the neuron's firing rate and asks for a specific, derivable characteristic of that systemâ€”the time constant. The presence of a first-order linear ODE guarantees that a unique solution exists for a given initial condition, and its exponential decay is characterized by a single time constant.\n-   **Objective**: The problem is stated using precise mathematical language, free from ambiguity or subjective content.\n-   **Completeness and Consistency**: The problem is self-contained. All necessary definitions and constraints (e.g., $r(t)$, $w_i(t)$, $\\dot{\\alpha}(t)$, $S  0$, $\\eta  0$) are provided. The condition $S  0$ is crucial for ensuring a stable, non-degenerate solution, demonstrating a well-thought-out problem structure. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, objective, and internally consistent. The derivation of the solution may proceed.\n\n**Solution Derivation**\nThe goal is to determine the time constant $\\tau$ that governs the convergence of the firing rate $r(t)$ to its target value $r^{*}$. This requires us to formulate an Ordinary Differential Equation (ODE) for $r(t)$ and identify its characteristic time scale.\n\n1.  We begin with the definition of the instantaneous firing rate of the linear neuron:\n    $$r(t) = \\sum_{i=1}^{N} w_i(t) x_i$$\n\n2.  The synaptic weights $w_i(t)$ are subject to multiplicative scaling, as given by the relation:\n    $$w_i(t) = \\alpha(t) v_i$$\n    where $\\alpha(t)$ is a global scaling factor and $v_i$ are fixed base weights.\n\n3.  Substituting the expression for $w_i(t)$ into the equation for $r(t)$:\n    $$r(t) = \\sum_{i=1}^{N} \\left(\\alpha(t) v_i\\right) x_i$$\n    Since $\\alpha(t)$ is a global factor, it is independent of the summation index $i$ and can be factored out:\n    $$r(t) = \\alpha(t) \\sum_{i=1}^{N} v_i x_i$$\n\n4.  The problem defines the total effective input as $S = \\sum_{i=1}^{N} v_i x_i$. Using this definition, the expression for $r(t)$ simplifies to a direct proportionality with $\\alpha(t)$:\n    $$r(t) = \\alpha(t) S$$\n\n5.  The dynamics of the system are described by the plasticity rule for the scaling factor $\\alpha(t)$:\n    $$\\frac{d\\alpha(t)}{dt} = \\eta \\left(r^{*} - r(t)\\right)$$\n\n6.  To find the dynamics of $r(t)$, we can differentiate our expression $r(t) = \\alpha(t) S$ with respect to time $t$. Since $S$ is a constant (as both $v_i$ and $x_i$ are time-invariant), we have:\n    $$\\frac{dr(t)}{dt} = \\frac{d}{dt} \\left(\\alpha(t) S\\right) = S \\frac{d\\alpha(t)}{dt}$$\n\n7.  Now, we substitute the given ODE for $\\frac{d\\alpha(t)}{dt}$ into this equation:\n    $$\\frac{dr(t)}{dt} = S \\left[ \\eta \\left(r^{*} - r(t)\\right) \\right]$$\n    This gives us a first-order linear ODE for $r(t)$:\n    $$\\frac{dr(t)}{dt} = \\eta S r^{*} - \\eta S r(t)$$\n\n8.  To identify the time constant, we rearrange this ODE into the standard form $\\frac{dy}{dt} + \\frac{1}{\\tau} y = C$:\n    $$\\frac{dr(t)}{dt} + (\\eta S) r(t) = \\eta S r^{*}$$\n\n9.  This equation describes the exponential relaxation of the variable $r(t)$ towards a steady-state value. The general solution to an equation of the form $\\frac{dy}{dt} + a y = b$ is $y(t) = \\frac{b}{a} + (y(0) - \\frac{b}{a}) \\exp(-at)$. The time constant of this process is $\\tau = \\frac{1}{a}$.\n\n10. By comparing our ODE for $r(t)$ with the standard form, we can identify the coefficient of the $r(t)$ term as the inverse of the time constant $\\tau$:\n    $$\\frac{1}{\\tau} = \\eta S$$\n\n11. Therefore, the time constant of convergence for $r(t)$ is:\n    $$\\tau = \\frac{1}{\\eta S}$$\n\n12. Finally, we express $\\tau$ in terms of the fundamental parameters given in the problem statement by substituting the definition of $S$:\n    $$\\tau = \\frac{1}{\\eta \\left( \\sum_{i=1}^{N} v_i x_i \\right)}$$\nThe conditions $\\eta  0$ and $S = \\sum v_i x_i  0$ ensure that the time constant $\\tau$ is a finite, positive real number, which is physically a required property for a stable convergence time. This completes the derivation.",
            "answer": "$$\n\\boxed{\\frac{1}{\\eta \\sum_{i=1}^{N} v_i x_i}}\n$$"
        }
    ]
}