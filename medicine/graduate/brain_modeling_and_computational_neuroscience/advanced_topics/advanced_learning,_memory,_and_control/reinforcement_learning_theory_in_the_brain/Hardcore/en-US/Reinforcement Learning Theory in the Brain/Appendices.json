{
    "hands_on_practices": [
        {
            "introduction": "Temporal Difference (TD) learning is a cornerstone of modern reinforcement learning and a leading model for how dopaminergic neurons compute reward prediction errors. To truly grasp why this algorithm is so powerful and biologically plausible, it is essential to understand its statistical properties. This exercise  provides a direct, quantitative comparison between the TD(0) algorithm and the conceptually simpler Monte Carlo method, guiding you through a derivation of their respective bias and variance to reveal the fundamental tradeoff that makes TD learning so efficient.",
            "id": "4014671",
            "problem": "A dopaminergic prediction-error model in the basal ganglia can be formalized as Temporal Difference learning with zero eligibility traces (Temporal Difference (TD)(0)). Consider an episodic, deterministic, rightward-moving chain of four states $S_{1} \\to S_{2} \\to S_{3} \\to S_{4}$, where $S_{4}$ is terminal. Starting from $S_{1}$, the episode lasts exactly $3$ transitions. At transition $t \\in \\{0,1,2\\}$, the reward is $R_{t} = \\bar{r}_{t} + \\varepsilon_{t}$, where $\\bar{r}_{t} = 1$ for all $t$, and the noise $\\varepsilon_{t}$ are independent, zero-mean, with variance $\\sigma^{2} = 1$. The discount factor is $\\gamma = 0.9$. All initial value estimates are $V_{0}(S) = 0$ for all nonterminal states, and the TD learning rate is $\\alpha = \\frac{1}{2}$. Use the conventional definition of the TD(0) update with online, in-episode updates at each visited nonterminal state. Use the conventional definition of the Monte Carlo return for a single episode.\n\nTasks:\n- Process one episode starting from $S_{1}$ and write the value estimates after each of the three TD(0) updates, expressed as functions of the realized rewards $R_{0}, R_{1}, R_{2}$.\n- Let $V^{\\ast}(S_{1})$ denote the true state value under the given dynamics and reward statistics. Using the law of total expectation and the law of total variance applied to the randomness in $R_{0}, R_{1}, R_{2}$, derive the bias and variance of:\n  1) the TD(0) estimate of $V(S_{1})$ after this one episode, and\n  2) the single-episode Monte Carlo estimate of $V(S_{1})$.\n- From these, obtain the mean-squared error for each estimator and compute the ratio\n$$\n\\frac{\\text{MSE}_{\\text{TD}}}{\\text{MSE}_{\\text{MC}}}.\n$$\n\nReport only the numerical value of this ratio, rounded to four significant figures.",
            "solution": "The user-provided problem has been validated and is determined to be sound. It is a well-posed problem in reinforcement learning theory, free of scientific, mathematical, or logical flaws.\n\nThe problem asks for a comparison of the Mean-Squared Error (MSE) of the Temporal Difference (TD)(0) and Monte Carlo (MC) estimators for the value of state $S_1$ after a single episode. We will proceed by first calculating the true state values, then analyzing each estimator's bias and variance.\n\n**1. True State Value Function**\n\nThe environment is a deterministic chain $S_{1} \\to S_{2} \\to S_{3} \\to S_{4}$, where $S_4$ is a terminal state with value $V(S_{4})=0$. The expected reward for each transition is $E[R_{t}] = \\bar{r}_{t} = 1$. The true state values, denoted $V^{\\ast}(S)$, satisfy the Bellman equation $V^{\\ast}(S) = E[R] + \\gamma V^{\\ast}(S')$. We can compute them by working backward from the terminal state:\n$$V^{\\ast}(S_{4}) = 0$$\n$$V^{\\ast}(S_{3}) = E[R_{2}] + \\gamma V^{\\ast}(S_{4}) = 1 + \\gamma(0) = 1$$\n$$V^{\\ast}(S_{2}) = E[R_{1}] + \\gamma V^{\\ast}(S_{3}) = 1 + \\gamma(1) = 1 + \\gamma$$\n$$V^{\\ast}(S_{1}) = E[R_{0}] + \\gamma V^{\\ast}(S_{2}) = 1 + \\gamma(1+\\gamma) = 1 + \\gamma + \\gamma^2$$\n\n**2. TD(0) Learning Analysis**\n\nThe TD(0) update rule for a state $S_t$ visited at time $t$ is:\n$$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha \\left( R_{t} + \\gamma V(S_{t+1}) - V(S_{t}) \\right)$$\nThe initial values are $V_{0}(S) = 0$ for all non-terminal states $S \\in \\{S_1, S_2, S_3\\}$. The episode consists of three transitions, leading to three updates.\n\n- **Update 1 (transition $S_{1} \\to S_{2}$ at $t=0$):**\n  The TD error for $S_1$ is $\\delta_0 = R_{0} + \\gamma V_{0}(S_{2}) - V_{0}(S_{1}) = R_{0} + \\gamma(0) - 0 = R_{0}$.\n  The value of $V(S_1)$ is updated: $V_{1}(S_1) = V_{0}(S_1) + \\alpha \\delta_0 = 0 + \\alpha R_{0} = \\alpha R_{0}$.\n  After this update, the value estimates are: $V(S_1) = \\alpha R_0, V(S_2) = 0, V(S_3) = 0$.\n\n- **Update 2 (transition $S_{2} \\to S_{3}$ at $t=1$):**\n  The TD error for $S_2$ is $\\delta_1 = R_{1} + \\gamma V_{1}(S_{3}) - V_{1}(S_{2}) = R_{1} + \\gamma(0) - 0 = R_1$.\n  The value of $V(S_2)$ is updated: $V_{2}(S_2) = V_{1}(S_2) + \\alpha \\delta_1 = 0 + \\alpha R_1 = \\alpha R_1$.\n  After this update, the value estimates are: $V(S_1) = \\alpha R_0, V(S_2) = \\alpha R_1, V(S_3) = 0$.\n\n- **Update 3 (transition $S_{3} \\to S_{4}$ at $t=2$):**\n  The TD error for $S_3$ is $\\delta_2 = R_{2} + \\gamma V_{2}(S_{4}) - V_{2}(S_{3}) = R_{2} + \\gamma(0) - 0 = R_2$.\n  The value of $V(S_3)$ is updated: $V_{3}(S_3) = V_{2}(S_3) + \\alpha \\delta_2 = 0 + \\alpha R_2 = \\alpha R_2$.\n  After this final update, the value estimates are: $V(S_1) = \\alpha R_0, V(S_2) = \\alpha R_1, V(S_3) = \\alpha R_2$.\n\nThe TD(0) estimate for $V(S_1)$ after one episode, denoted $\\hat{V}_{\\text{TD}}(S_1)$, is the final value of $V(S_1)$, which is only updated once.\n$$\\hat{V}_{\\text{TD}}(S_1) = \\alpha R_0$$\n\nNow, we compute the bias and variance of this estimator.\nThe expectation is:\n$$E[\\hat{V}_{\\text{TD}}(S_1)] = E[\\alpha R_0] = \\alpha E[R_0] = \\alpha \\cdot 1 = \\alpha$$\nThe bias is the difference between the expected estimate and the true value:\n$$\\text{Bias}_{\\text{TD}} = E[\\hat{V}_{\\text{TD}}(S_1)] - V^{\\ast}(S_1) = \\alpha - (1 + \\gamma + \\gamma^2)$$\nThe variance is:\n$$\\text{Var}(\\hat{V}_{\\text{TD}}(S_1)) = \\text{Var}(\\alpha R_0) = \\alpha^2 \\text{Var}(R_0)$$\nGiven $R_t = 1 + \\varepsilon_t$, $\\text{Var}(R_t) = \\text{Var}(\\varepsilon_t) = \\sigma^2 = 1$.\n$$\\text{Var}_{\\text{TD}} = \\alpha^2 \\sigma^2 = \\alpha^2$$\nThe Mean-Squared Error is the sum of the squared bias and the variance:\n$$\\text{MSE}_{\\text{TD}} = (\\text{Bias}_{\\text{TD}})^2 + \\text{Var}_{\\text{TD}} = \\left( \\alpha - (1 + \\gamma + \\gamma^2) \\right)^2 + \\alpha^2$$\n\n**3. Monte Carlo (MC) Estimation Analysis**\n\nThe single-episode MC estimate of a state's value is the total discounted return from that state. For $S_1$ at time $t=0$, the return $G_0$ is:\n$$G_0 = R_0 + \\gamma R_1 + \\gamma^2 R_2$$\nThe MC estimate, $\\hat{V}_{\\text{MC}}(S_1)$, is therefore:\n$$\\hat{V}_{\\text{MC}}(S_1) = R_0 + \\gamma R_1 + \\gamma^2 R_2$$\n\nNow, we compute the bias and variance of this estimator.\nThe expectation is:\n$$E[\\hat{V}_{\\text{MC}}(S_1)] = E[R_0 + \\gamma R_1 + \\gamma^2 R_2] = E[R_0] + \\gamma E[R_1] + \\gamma^2 E[R_2]$$\n$$E[\\hat{V}_{\\text{MC}}(S_1)] = 1 + \\gamma(1) + \\gamma^2(1) = 1 + \\gamma + \\gamma^2 = V^{\\ast}(S_1)$$\nThe bias is zero, as the expected estimate equals the true value:\n$$\\text{Bias}_{\\text{MC}} = E[\\hat{V}_{\\text{MC}}(S_1)] - V^{\\ast}(S_1) = 0$$\nThe variance is computed using the independence of the noise terms $\\varepsilon_t$, which makes the rewards $R_t$ independent:\n$$\\text{Var}(\\hat{V}_{\\text{MC}}(S_1)) = \\text{Var}(R_0 + \\gamma R_1 + \\gamma^2 R_2) = \\text{Var}(R_0) + \\gamma^2 \\text{Var}(R_1) + (\\gamma^2)^2 \\text{Var}(R_2)$$\n$$\\text{Var}_{\\text{MC}} = \\sigma^2 + \\gamma^2 \\sigma^2 + \\gamma^4 \\sigma^2 = (1 + \\gamma^2 + \\gamma^4)\\sigma^2$$\nWith $\\sigma^2=1$:\n$$\\text{Var}_{\\text{MC}} = 1 + \\gamma^2 + \\gamma^4$$\nThe Mean-Squared Error for the MC estimator is:\n$$\\text{MSE}_{\\text{MC}} = (\\text{Bias}_{\\text{MC}})^2 + \\text{Var}_{\\text{MC}} = 0^2 + (1 + \\gamma^2 + \\gamma^4) = 1 + \\gamma^2 + \\gamma^4$$\n\n**4. Numerical Calculation and Ratio**\n\nWe are given $\\alpha = \\frac{1}{2} = 0.5$ and $\\gamma = 0.9$.\nFirst, calculate the common term $1 + \\gamma + \\gamma^2$:\n$$1 + 0.9 + (0.9)^2 = 1 + 0.9 + 0.81 = 2.71$$\nNow, compute $\\text{MSE}_{\\text{TD}}$:\n$$\\text{MSE}_{\\text{TD}} = (0.5 - 2.71)^2 + (0.5)^2 = (-2.21)^2 + 0.25 = 4.8841 + 0.25 = 5.1341$$\nNext, compute $\\text{MSE}_{\\text{MC}}$:\n$$\\text{MSE}_{\\text{MC}} = 1 + (0.9)^2 + (0.9)^4 = 1 + 0.81 + 0.6561 = 2.4661$$\nFinally, we compute the ratio:\n$$\\frac{\\text{MSE}_{\\text{TD}}}{\\text{MSE}_{\\text{MC}}} = \\frac{5.1341}{2.4661} \\approx 2.0818295$$\nRounding to four significant figures, the ratio is $2.082$.",
            "answer": "$$\\boxed{2.082}$$"
        },
        {
            "introduction": "How does the brain build a \"cognitive map\" of the world to enable flexible, goal-directed behavior? The Successor Representation (SR) offers a compelling answer, proposing a mechanism that separates the representation of environmental dynamics from the representation of rewards. This practice  delves into the mechanics of the SR, asking you to derive it for a simple environment and use it to predict how the value of a state changes when rewards are reconfigured, a process analogous to rapid behavioral adaptation observed experimentally.",
            "id": "4014662",
            "problem": "Consider a four-state ring gridworld modeling spatial navigation under a policy consistent with known dopaminergic reward prediction mechanisms in the brain. The environment is a Markov Decision Process (MDP), defined by a state set $\\{1,2,3,4\\}$ arranged on a ring so that from state $1$ one can move to states $2$ or $4$, from state $2$ to states $1$ or $3$, from state $3$ to states $2$ or $4$, and from state $4$ to states $3$ or $1$. At each discrete time step, the agent chooses an action uniformly at random among the two available moves (counterclockwise or clockwise), so the one-step transition probabilities are $1/2$ to each neighbor. The discount factor is $\\gamma = \\frac{4}{5}$.\n\nThe Successor Representation (SR) is defined for this ring under the random policy, as follows: for states $i,j \\in \\{1,2,3,4\\}$,\n$$\nM_{ij} \\equiv \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\,\\mathbf{1}\\{s_{t} = j\\} \\,\\big|\\, s_{0} = i \\right],\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. This definition reflects the expected discounted occupancies of states under the policy, and is widely used in computational neuroscience to model hippocampal representations and their interaction with dopaminergic prediction error signals. The state-value function under a scalar reward $r(j)$ at state $j$ is defined by $V(i) = \\sum_{j} M_{ij} \\, r(j)$.\n\nStarting only from the above definitions and the Markov property, derive the linear system that determines $M$ for this ring under the random policy, solve for $M$, and then use $V(i) = \\sum_{j} M_{ij} r(j)$ to predict the change in value at state $1$ when a unit-magnitude scalar reward is moved from state $2$ to state $3$. Explicitly:\n\n- Compute the entries of $M$ required to determine $V(1)$ when $r(2) = 1$ and $r(j) = 0$ for $j \\neq 2$, and when $r(3) = 1$ and $r(j) = 0$ for $j \\neq 3$.\n- Compute the predicted change $\\Delta V(1)$ when the reward is moved from state $2$ to state $3$, i.e., $\\Delta V(1) = V_{\\text{new}}(1) - V_{\\text{old}}(1)$, where “old” refers to the reward at state $2$ and “new” refers to the reward at state $3$.\n\nExpress your final answer for $\\Delta V(1)$ as an exact rational number. No numerical rounding is permitted.",
            "solution": "The problem is well-posed, scientifically grounded in the theory of reinforcement learning, and its formulation is mathematically precise and self-contained. The problem is valid.\n\nThe task is to compute the change in the state-value function at state $1$, $\\Delta V(1)$, when a unit reward is moved from state $2$ to state $3$. The solution proceeds in three main steps:\n1.  Derive the linear system governing the Successor Representation (SR) matrix $M$.\n2.  Solve for the necessary entries of $M$ by exploiting the symmetries of the problem.\n3.  Compute the old and new values of $V(1)$ and their difference.\n\n**Step 1: Deriving the Linear System for the Successor Representation**\n\nThe Successor Representation $M$ is defined as:\n$$\nM_{ij} = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\,\\mathbf{1}\\{s_{t} = j\\} \\,\\big|\\, s_{0} = i \\right]\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, $s_t$ is the state at time $t$, and $\\gamma$ is the discount factor. We can expand the summation:\n$$\nM_{ij} = \\mathbb{E}\\left[\\mathbf{1}\\{s_{0} = j\\} + \\sum_{t=1}^{\\infty} \\gamma^{t} \\,\\mathbf{1}\\{s_{t} = j\\} \\,\\big|\\, s_{0} = i \\right]\n$$\nGiven $s_0=i$, the first term $\\mathbf{1}\\{s_{0} = j\\}$ is simply the Kronecker delta $\\delta_{ij}$. We can factor out $\\gamma$ from the remaining sum:\n$$\nM_{ij} = \\delta_{ij} + \\mathbb{E}\\left[\\gamma \\sum_{t'=0}^{\\infty} \\gamma^{t'} \\,\\mathbf{1}\\{s_{t'+1} = j\\} \\,\\big|\\, s_{0} = i \\right]\n$$\nwhere $t' = t-1$. By the law of total expectation, we can condition on the next state $s_1$:\n$$\nM_{ij} = \\delta_{ij} + \\gamma \\, \\mathbb{E}_{s_1 \\sim P(s_1|s_0=i)}\\left[ \\mathbb{E}\\left[\\sum_{t'=0}^{\\infty} \\gamma^{t'} \\,\\mathbf{1}\\{s_{t'+1} = j\\} \\,\\big|\\, s_1 \\right] \\right]\n$$\nThe inner expectation is the definition of $M_{s_1, j}$. Summing over all possible next states $k$ according to the transition probabilities $P_{ik} = P(s_1=k|s_0=i)$, we get:\n$$\nM_{ij} = \\delta_{ij} + \\gamma \\sum_{k=1}^{4} P_{ik} M_{kj}\n$$\nThis is a system of linear equations for the entries of $M$. In matrix form, this is written as:\n$$\nM = I + \\gamma P M\n$$\nwhere $I$ is the identity matrix and $P$ is the state transition matrix. Rearranging, we obtain the linear system that determines $M$:\n$$\n(I - \\gamma P) M = I\n$$\nThus, $M$ is the inverse of the matrix $(I - \\gamma P)$.\n$$\nM = (I - \\gamma P)^{-1}\n$$\n\n**Step 2: Solving for the Entries of M**\n\nFirst, we construct the transition matrix $P$ from the problem description. The states are $\\{1,2,3,4\\}$. The policy is to choose one of two available moves uniformly at random (probability $1/2$). The discount factor is $\\gamma = 4/5$.\n-   From state $1$, moves to $2$ or $4$: $P_{12}=1/2$, $P_{14}=1/2$.\n-   From state $2$, moves to $1$ or $3$: $P_{21}=1/2$, $P_{23}=1/2$.\n-   From state $3$, moves to $2$ or $4$: $P_{32}=1/2$, $P_{34}=1/2$.\n-   From state $4$, moves to $1$ or $3$: $P_{41}=1/2$, $P_{43}=1/2$.\nAll other transition probabilities are $0$. The transition matrix is:\n$$\nP = \\begin{pmatrix}\n0  1/2  0  1/2 \\\\\n1/2  0  1/2  0 \\\\\n0  1/2  0  1/2 \\\\\n1/2  0  1/2  0\n\\end{pmatrix}\n$$\nNext, we compute $\\gamma P$:\n$$\n\\gamma P = \\frac{4}{5} \\begin{pmatrix}\n0  1/2  0  1/2 \\\\\n1/2  0  1/2  0 \\\\\n0  1/2  0  1/2 \\\\\n1/2  0  1/2  0\n\\end{pmatrix} = \\begin{pmatrix}\n0  2/5  0  2/5 \\\\\n2/5  0  2/5  0 \\\\\n0  2/5  0  2/5 \\\\\n2/5  0  2/5  0\n\\end{pmatrix}\n$$\nThe problem describes a bipartite graph where states $\\{1, 3\\}$ form one partition and $\\{2, 4\\}$ form the other. $P$ is symmetric, so $M = (I - \\gamma P)^{-1}$ must also be symmetric. The symmetries of the graph and policy imply certain equalities among the entries of $M$. Let:\n-   $M_{11} = M_{33} = a$\n-   $M_{22} = M_{44} = b$\n-   $M_{12} = M_{21} = M_{14} = M_{41} = M_{32} = M_{23} = M_{34} = M_{43} = c$\n-   $M_{13} = M_{31} = e$\n-   $M_{24} = M_{42} = f$\n\nThe matrix $M$ has the structure:\n$$\nM = \\begin{pmatrix} a  c  e  c \\\\ c  b  c  f \\\\ e  c  a  c \\\\ c  f  c  b \\end{pmatrix}\n$$\nWe use the element-wise equation $M_{ij} = \\delta_{ij} + \\gamma \\sum_k P_{ik} M_{kj}$ to find the values of $a, b, c, e, f$.\n-   For $M_{11}$: $a = 1 + \\gamma (P_{12}M_{21} + P_{14}M_{41}) = 1 + \\frac{2}{5}(c+c) = 1 + \\frac{4}{5}c$.\n-   For $M_{22}$: $b = 1 + \\gamma (P_{21}M_{12} + P_{23}M_{32}) = 1 + \\frac{2}{5}(c+c) = 1 + \\frac{4}{5}c$.\nThis confirms $a=b$.\n-   For $M_{12}$: $c = 0 + \\gamma (P_{12}M_{22} + P_{14}M_{42}) = \\frac{2}{5}(b+f) = \\frac{2}{5}(a+f)$.\n-   For $M_{13}$: $e = 0 + \\gamma (P_{12}M_{23} + P_{14}M_{43}) = \\frac{2}{5}(c+c) = \\frac{4}{5}c$.\n-   For $M_{24}$: $f = 0 + \\gamma (P_{21}M_{14} + P_{23}M_{34}) = \\frac{2}{5}(c+c) = \\frac{4}{5}c$.\nThis confirms $e=f$.\n\nWe now have a system of three equations for $a, c, f$:\n1.  $a = 1 + \\frac{4}{5}c$\n2.  $c = \\frac{2}{5}(a+f)$\n3.  $f = \\frac{4}{5}c$\n\nSubstitute (3) into (2):\n$$\nc = \\frac{2}{5}\\left(a + \\frac{4}{5}c\\right) = \\frac{2}{5}a + \\frac{8}{25}c\n$$\n$$\nc\\left(1 - \\frac{8}{25}\\right) = \\frac{2}{5}a \\implies \\frac{17}{25}c = \\frac{2}{5}a \\implies c = \\frac{25}{17} \\cdot \\frac{2}{5}a = \\frac{10}{17}a\n$$\nNow substitute this expression for $c$ into (1):\n$$\na = 1 + \\frac{4}{5}\\left(\\frac{10}{17}a\\right) = 1 + \\frac{8}{17}a\n$$\n$$\na\\left(1 - \\frac{8}{17}\\right) = 1 \\implies \\frac{9}{17}a = 1 \\implies a = \\frac{17}{9}\n$$\nWith $a$ known, we can find $c$ and $f$:\n$$\nc = \\frac{10}{17}a = \\frac{10}{17} \\cdot \\frac{17}{9} = \\frac{10}{9}\n$$\n$$\nf = \\frac{4}{5}c = \\frac{4}{5} \\cdot \\frac{10}{9} = \\frac{8}{9}\n$$\nSince $e=f$, we have $e = 8/9$. The required matrix entries are $M_{12}=c=10/9$ and $M_{13}=e=8/9$.\n\n**Step 3: Calculating the Change in Value**\n\nThe state-value function is $V(i) = \\sum_j M_{ij} r(j)$. We need to calculate $V(1)$ for two different reward functions.\n\nCase 1 (old): A unit reward at state $2$. The reward vector is $\\mathbf{r}_{\\text{old}}$, where $r(2)=1$ and $r(j)=0$ for $j\\neq 2$.\n$$\nV_{\\text{old}}(1) = M_{11}r(1) + M_{12}r(2) + M_{13}r(3) + M_{14}r(4) = M_{12} \\cdot 1 = M_{12}\n$$\n$$\nV_{\\text{old}}(1) = \\frac{10}{9}\n$$\n\nCase 2 (new): A unit reward at state $3$. The reward vector is $\\mathbf{r}_{\\text{new}}$, where $r(3)=1$ and $r(j)=0$ for $j\\neq 3$.\n$$\nV_{\\text{new}}(1) = M_{11}r(1) + M_{12}r(2) + M_{13}r(3) + M_{14}r(4) = M_{13} \\cdot 1 = M_{13}\n$$\n$$\nV_{\\text{new}}(1) = \\frac{8}{9}\n$$\n\nThe predicted change in value at state $1$ is $\\Delta V(1)$:\n$$\n\\Delta V(1) = V_{\\text{new}}(1) - V_{\\text{old}}(1) = M_{13} - M_{12} = \\frac{8}{9} - \\frac{10}{9} = -\\frac{2}{9}\n$$\nThe value at state $1$ decreases, which is expected as the reward has moved from a neighboring state (distance $1$) to an opposite state (distance $2$).",
            "answer": "$$\\boxed{-\\frac{2}{9}}$$"
        },
        {
            "introduction": "The classic model of a reward prediction error is a single scalar value, representing the difference between expected and actual reward. However, recent neurophysiological evidence reveals a striking diversity in dopamine neuron responses, suggesting the brain may encode more than just the expected value. This exercise  explores this frontier by introducing a distributional reinforcement learning critic, which learns the entire probability distribution of future returns. By contrasting the update of this distributional critic with a standard TD update, you will gain insight into how this richer representation can capture the full spectrum of possible outcomes and account for the observed heterogeneity in neural signals.",
            "id": "4014631",
            "problem": "Consider a two-state Markov Decision Process (MDP) with states $s_A$ and $s_B$, one available action per state, and deterministic transition $s_A \\to s_B$. State $s_B$ is terminal. The discount factor is $\\gamma = 0.9$. Assume a distributional critic that represents the return distribution $Z(s)$ at any state $s$ by a categorical distribution with $N=3$ equally weighted atoms located at quantile estimates $\\theta_i(s)$ corresponding to quantile levels $\\tau_i = \\frac{i - 0.5}{N}$ for $i \\in \\{1,2,3\\}$. This representation is consistent with quantile-based distributional reinforcement learning, wherein heterogeneous dopaminergic prediction error signals across neurons can be interpreted as encoding different quantiles of the return distribution rather than a single expected value.\n\nInitially, the critic’s atoms are $\\theta_1(s_A) = 0.5$, $\\theta_2(s_A) = 1.5$, $\\theta_3(s_A) = 2.5$, and $\\theta_1(s_B) = 0$, $\\theta_2(s_B) = 1$, $\\theta_3(s_B) = 3$. A single observed transition from $s_A \\to s_B$ yields an immediate reward sample $r = 1$. Using quantile regression temporal-difference with the absolute quantile loss (Huber parameter set to $0$), define the per-quantile loss for state $s_A$ as\n$$\nL_i(s_A) = \\frac{1}{N} \\sum_{j=1}^{N} \\rho_{\\tau_i}\\!\\left(y_j - \\theta_i(s_A)\\right),\n$$\nwhere the target samples are $y_j = r + \\gamma\\,\\theta_j(s_B)$ and the quantile loss is $\\rho_{\\tau}(u) = u\\left(\\tau - \\mathbb{I}\\{u  0\\}\\right)$. Perform one stochastic gradient descent step with learning rate $\\alpha = 0.3$ on each $\\theta_i(s_A)$ to obtain updated atoms $\\theta_i^{+}(s_A)$.\n\nIn parallel, consider an expected-value scalar critic $V(s)$, initialized by the means of the corresponding categorical distributions: $V(s_A) = \\frac{1}{3}\\sum_{i=1}^{3}\\theta_i(s_A)$ and $V(s_B) = \\frac{1}{3}\\sum_{i=1}^{3}\\theta_i(s_B)$. Perform one standard temporal-difference (TD) update at $s_A$ with the same learning rate $\\alpha$,\n$$\nV^{+}(s_A) = V(s_A) + \\alpha\\left[r + \\gamma V(s_B) - V(s_A)\\right].\n$$\n\nLet $\\bar{Z}^{+}(s_A) = \\frac{1}{3}\\sum_{i=1}^{3}\\theta_i^{+}(s_A)$ denote the mean of the updated categorical distribution at $s_A$. Compute the scalar difference\n$$\n\\Delta = V^{+}(s_A) - \\bar{Z}^{+}(s_A).\n$$\nProvide $\\Delta$ as a single real number. No rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded in reinforcement learning theory, and contains all necessary information to proceed with a unique solution. The definitions and algorithms provided are standard in the field of computational neuroscience and machine learning. Therefore, the problem is valid.\n\nThe solution proceeds in two parallel parts: first, updating the scalar value function $V(s_A)$ using the standard temporal-difference (TD) learning rule; second, updating the distributional value function's atoms $\\theta_i(s_A)$ using quantile regression. Finally, we compute the difference between the updated scalar value and the mean of the updated distribution.\n\nThe givens are:\nStates: $s_A, s_B$ (terminal).\nTransition: $s_A \\to s_B$.\nReward sample: $r=1$.\nDiscount factor: $\\gamma = 0.9$.\nLearning rate: $\\alpha = 0.3$.\nDistributional critic: $N=3$ atoms $\\theta_i$, with quantile levels $\\tau_i = \\frac{i-0.5}{N}$.\nInitial atoms at $s_A$: $\\theta_1(s_A) = 0.5$, $\\theta_2(s_A) = 1.5$, $\\theta_3(s_A) = 2.5$.\nInitial atoms at $s_B$: $\\theta_1(s_B) = 0$, $\\theta_2(s_B) = 1$, $\\theta_3(s_B) = 3$.\n\nFirst, we calculate the initial values for the scalar critic $V(s)$ by taking the mean of the initial atoms.\nFor state $s_A$:\n$$\nV(s_A) = \\frac{1}{N} \\sum_{i=1}^{N} \\theta_i(s_A) = \\frac{1}{3} (0.5 + 1.5 + 2.5) = \\frac{4.5}{3} = 1.5\n$$\nFor state $s_B$:\n$$\nV(s_B) = \\frac{1}{N} \\sum_{i=1}^{N} \\theta_i(s_B) = \\frac{1}{3} (0 + 1 + 3) = \\frac{4}{3}\n$$\n\nNext, we perform one standard TD update for $V(s_A)$. The TD error, $\\delta_{TD}$, is calculated as:\n$$\n\\delta_{TD} = r + \\gamma V(s_B) - V(s_A) = 1 + 0.9 \\left( \\frac{4}{3} \\right) - 1.5\n$$\n$$\n\\delta_{TD} = 1 + \\frac{9}{10} \\cdot \\frac{4}{3} - 1.5 = 1 + 1.2 - 1.5 = 0.7\n$$\nThe updated value, $V^{+}(s_A)$, is:\n$$\nV^{+}(s_A) = V(s_A) + \\alpha \\delta_{TD} = 1.5 + 0.3(0.7) = 1.5 + 0.21 = 1.71\n$$\n\nNow, we proceed with the update for the distributional critic. First, we determine the quantile levels $\\tau_i$ for $i \\in \\{1, 2, 3\\}$ with $N=3$:\n$$\n\\tau_1 = \\frac{1 - 0.5}{3} = \\frac{0.5}{3} = \\frac{1}{6}\n$$\n$$\n\\tau_2 = \\frac{2 - 0.5}{3} = \\frac{1.5}{3} = \\frac{1}{2}\n$$\n$$\n\\tau_3 = \\frac{3 - 0.5}{3} = \\frac{2.5}{3} = \\frac{5}{6}\n$$\n\nNext, we calculate the target samples $y_j = r + \\gamma \\theta_j(s_B)$ for $j \\in \\{1, 2, 3\\}$:\n$$\ny_1 = 1 + 0.9 \\cdot \\theta_1(s_B) = 1 + 0.9(0) = 1\n$$\n$$\ny_2 = 1 + 0.9 \\cdot \\theta_2(s_B) = 1 + 0.9(1) = 1.9\n$$\n$$\ny_3 = 1 + 0.9 \\cdot \\theta_3(s_B) = 1 + 0.9(3) = 1 + 2.7 = 3.7\n$$\n\nThe update for each atom $\\theta_i(s_A)$ is a stochastic gradient descent step:\n$$\n\\theta_i^{+}(s_A) = \\theta_i(s_A) - \\alpha \\frac{\\partial L_i(s_A)}{\\partial \\theta_i(s_A)}\n$$\nThe gradient of the loss $L_i(s_A) = \\frac{1}{N} \\sum_{j=1}^{N} \\rho_{\\tau_i}\\!\\left(y_j - \\theta_i(s_A)\\right)$ with respect to $\\theta_i(s_A)$ is:\n$$\n\\frac{\\partial L_i(s_A)}{\\partial \\theta_i(s_A)} = \\frac{1}{N} \\sum_{j=1}^{N} \\frac{\\partial}{\\partial \\theta_i(s_A)} \\rho_{\\tau_i}(y_j - \\theta_i(s_A))\n$$\nUsing the chain rule and the derivative of the quantile loss $\\rho_{\\tau}(u)$, which is $\\frac{d\\rho_\\tau}{du} = \\tau - \\mathbb{I}\\{u  0\\}$, we get:\n$$\n\\frac{\\partial}{\\partial \\theta_i(s_A)} \\rho_{\\tau_i}(y_j - \\theta_i(s_A)) = -(\\tau_i - \\mathbb{I}\\{y_j - \\theta_i(s_A)  0\\}) = \\mathbb{I}\\{y_j  \\theta_i(s_A)\\} - \\tau_i\n$$\nSo the gradient is:\n$$\n\\nabla_{\\theta_i(s_A)} L_i = \\frac{1}{N} \\sum_{j=1}^{N} \\left(\\mathbb{I}\\{y_j  \\theta_i(s_A)\\} - \\tau_i\\right)\n$$\nWe compute this for each $i \\in \\{1, 2, 3\\}$:\nFor $\\theta_1(s_A) = 0.5$ and $\\tau_1 = 1/6$:\nThe conditions $y_j  0.5$ are false for all $j \\in \\{1, 2, 3\\}$ as $y=\\{1, 1.9, 3.7\\}$.\n$$\n\\nabla_1 = \\frac{1}{3} \\left[ (0 - \\frac{1}{6}) + (0 - \\frac{1}{6}) + (0 - \\frac{1}{6}) \\right] = \\frac{1}{3} \\left(-\\frac{3}{6}\\right) = -\\frac{1}{6}\n$$\nThe update for $\\theta_1(s_A)$ is:\n$$\n\\theta_1^{+}(s_A) = 0.5 - 0.3 \\left(-\\frac{1}{6}\\right) = 0.5 + 0.05 = 0.55\n$$\n\nFor $\\theta_2(s_A) = 1.5$ and $\\tau_2 = 1/2$:\nThe condition $y_j  1.5$ is true only for $j=1$ ($y_1=1$).\n$$\n\\nabla_2 = \\frac{1}{3} \\left[ (1 - \\frac{1}{2}) + (0 - \\frac{1}{2}) + (0 - \\frac{1}{2}) \\right] = \\frac{1}{3} \\left(\\frac{1}{2} - \\frac{1}{2} - \\frac{1}{2}\\right) = \\frac{1}{3} \\left(-\\frac{1}{2}\\right) = -\\frac{1}{6}\n$$\nThe update for $\\theta_2(s_A)$ is:\n$$\n\\theta_2^{+}(s_A) = 1.5 - 0.3 \\left(-\\frac{1}{6}\\right) = 1.5 + 0.05 = 1.55\n$$\n\nFor $\\theta_3(s_A) = 2.5$ and $\\tau_3 = 5/6$:\nThe condition $y_j  2.5$ is true for $j=1$ ($y_1=1$) and $j=2$ ($y_2=1.9$).\n$$\n\\nabla_3 = \\frac{1}{3} \\left[ (1 - \\frac{5}{6}) + (1 - \\frac{5}{6}) + (0 - \\frac{5}{6}) \\right] = \\frac{1}{3} \\left(\\frac{1}{6} + \\frac{1}{6} - \\frac{5}{6}\\right) = \\frac{1}{3} \\left(-\\frac{3}{6}\\right) = -\\frac{1}{6}\n$$\nThe update for $\\theta_3(s_A)$ is:\n$$\n\\theta_3^{+}(s_A) = 2.5 - 0.3 \\left(-\\frac{1}{6}\\right) = 2.5 + 0.05 = 2.55\n$$\nThe updated atoms are $\\theta_1^{+}(s_A) = 0.55$, $\\theta_2^{+}(s_A) = 1.55$, and $\\theta_3^{+}(s_A) = 2.55$.\n\nNow, we compute the mean of the updated categorical distribution, $\\bar{Z}^{+}(s_A)$:\n$$\n\\bar{Z}^{+}(s_A) = \\frac{1}{3} \\sum_{i=1}^{3} \\theta_i^{+}(s_A) = \\frac{1}{3} (0.55 + 1.55 + 2.55) = \\frac{4.65}{3} = 1.55\n$$\n\nFinally, we compute the scalar difference $\\Delta$:\n$$\n\\Delta = V^{+}(s_A) - \\bar{Z}^{+}(s_A) = 1.71 - 1.55 = 0.16\n$$",
            "answer": "$$\n\\boxed{0.16}\n$$"
        }
    ]
}