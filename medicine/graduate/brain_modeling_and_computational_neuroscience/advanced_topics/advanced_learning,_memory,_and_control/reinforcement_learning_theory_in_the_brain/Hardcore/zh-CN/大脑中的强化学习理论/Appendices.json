{
    "hands_on_practices": [
        {
            "introduction": "本练习旨在探讨时间差分（TD）学习与蒙特卡洛（MC）方法这两种强化学习基石算法的根本区别。通过在一个简单的、可控的场景下分别计算两种估计量的偏差和方差，你将具体理解偏差-方差权衡这一核心概念。这个练习对于深入解读作为奖赏预测误差信号的多巴胺神经活动具有重要意义。",
            "id": "4014671",
            "problem": "基底节中的多巴胺能预测误差模型可以形式化为资格迹为零的时间差分学习（Temporal Difference (TD)(0)）。考虑一个由四个状态组成的、分幕式的、确定性的、向右移动的状态链 $S_{1} \\to S_{2} \\to S_{3} \\to S_{4}$，其中 $S_{4}$ 是终止状态。从 $S_{1}$ 开始，该幕恰好持续 $3$ 次转移。在转移 $t \\in \\{0,1,2\\}$ 时，奖励为 $R_{t} = \\bar{r}_{t} + \\varepsilon_{t}$，其中对所有 $t$ 都有 $\\bar{r}_{t} = 1$，噪声 $\\varepsilon_{t}$ 是独立的、零均值的，方差为 $\\sigma^{2} = 1$。折扣因子为 $\\gamma = 0.9$。所有非终止状态的初始价值估计均为 $V_{0}(S) = 0$，TD 学习率为 $\\alpha = \\frac{1}{2}$。在每个访问到的非终止状态处，使用 TD(0) 更新的传统定义进行在线、幕内更新。对于单个幕，使用蒙特卡洛回报的传统定义。\n\n任务：\n- 处理一个从 $S_{1}$ 开始的幕，并写出三次 TD(0) 更新后每次的价值估计，表示为已实现奖励 $R_{0}, R_{1}, R_{2}$ 的函数。\n- 设 $V^{\\ast}(S_{1})$ 表示在给定动力学和奖励统计下的真实状态价值。利用应用于 $R_{0}, R_{1}, R_{2}$ 随机性的全期望定律和全方差定律，推导以下两种估计的偏差和方差：\n  1) 经过这一个幕后，$V(S_{1})$ 的 TD(0) 估计，以及\n  2) $V(S_{1})$ 的单幕蒙特卡洛估计。\n- 由此，求出每个估计器的均方误差，并计算比率\n$$\n\\frac{\\text{MSE}_{\\text{TD}}}{\\text{MSE}_{\\text{MC}}}.\n$$\n\n仅报告此比率的数值，四舍五入至四位有效数字。",
            "solution": "用户提供的问题已经过验证，并被确定为是合理的。这是一个强化学习理论中定义明确的问题，没有科学、数学或逻辑上的缺陷。\n\n问题要求在单个幕之后，比较时间差分 (TD)(0) 估计器和蒙特卡洛 (MC) 估计器对状态 $S_1$ 价值的均方误差 (MSE)。我们将首先计算真实的状态价值，然后分析每个估计器的偏差和方差。\n\n**1. 真实状态价值函数**\n\n环境是一个确定性链 $S_{1} \\to S_{2} \\to S_{3} \\to S_{4}$，其中 $S_4$ 是终止状态，其价值为 $V(S_{4})=0$。每次转移的期望奖励是 $E[R_{t}] = \\bar{r}_{t} = 1$。真实的状态价值，记为 $V^{\\ast}(S)$，满足贝尔曼方程 $V^{\\ast}(S) = E[R] + \\gamma V^{\\ast}(S')$。我们可以从终止状态向后计算它们：\n$$V^{\\ast}(S_{4}) = 0$$\n$$V^{\\ast}(S_{3}) = E[R_{2}] + \\gamma V^{\\ast}(S_{4}) = 1 + \\gamma(0) = 1$$\n$$V^{\\ast}(S_{2}) = E[R_{1}] + \\gamma V^{\\ast}(S_{3}) = 1 + \\gamma(1) = 1 + \\gamma$$\n$$V^{\\ast}(S_{1}) = E[R_{0}] + \\gamma V^{\\ast}(S_{2}) = 1 + \\gamma(1+\\gamma) = 1 + \\gamma + \\gamma^2$$\n\n**2. TD(0) 学习分析**\n\n在时间 $t$ 访问的状态 $S_t$ 的 TD(0) 更新规则是：\n$$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha \\left( R_{t} + \\gamma V(S_{t+1}) - V(S_{t}) \\right)$$\n所有非终止状态 $S \\in \\{S_1, S_2, S_3\\}$ 的初始价值均为 $V_{0}(S) = 0$。该幕包含三次转移，从而导致三次更新。\n\n- **更新 1（在 $t=0$ 时从 $S_{1} \\to S_{2}$ 的转移）：**\n  $S_1$ 的 TD 误差是 $\\delta_0 = R_{0} + \\gamma V_{0}(S_{2}) - V_{0}(S_{1}) = R_{0} + \\gamma(0) - 0 = R_{0}$。\n  $V(S_1)$ 的价值被更新：$V_{1}(S_1) = V_{0}(S_1) + \\alpha \\delta_0 = 0 + \\alpha R_{0} = \\alpha R_{0}$。\n  这次更新后，价值估计为：$V(S_1) = \\alpha R_0, V(S_2) = 0, V(S_3) = 0$。\n\n- **更新 2（在 $t=1$ 时从 $S_{2} \\to S_{3}$ 的转移）：**\n  $S_2$ 的 TD 误差是 $\\delta_1 = R_{1} + \\gamma V_{1}(S_{3}) - V_{1}(S_{2}) = R_{1} + \\gamma(0) - 0 = R_1$。\n  $V(S_2)$ 的价值被更新：$V_{2}(S_2) = V_{1}(S_2) + \\alpha \\delta_1 = 0 + \\alpha R_1 = \\alpha R_1$。\n  这次更新后，价值估计为：$V(S_1) = \\alpha R_0, V(S_2) = \\alpha R_1, V(S_3) = 0$。\n\n- **更新 3（在 $t=2$ 时从 $S_{3} \\to S_{4}$ 的转移）：**\n  $S_3$ 的 TD 误差是 $\\delta_2 = R_{2} + \\gamma V_{2}(S_{4}) - V_{2}(S_{3}) = R_{2} + \\gamma(0) - 0 = R_2$。\n  $V(S_3)$ 的价值被更新：$V_{3}(S_3) = V_{2}(S_3) + \\alpha \\delta_2 = 0 + \\alpha R_2 = \\alpha R_2$。\n  这次最终更新后，价值估计为：$V(S_1) = \\alpha R_0, V(S_2) = \\alpha R_1, V(S_3) = \\alpha R_2$。\n\n经过一个幕后，$V(S_1)$ 的 TD(0) 估计，记为 $\\hat{V}_{\\text{TD}}(S_1)$，是 $V(S_1)$ 的最终值，它只被更新了一次。\n$$\\hat{V}_{\\text{TD}}(S_1) = \\alpha R_0$$\n\n现在，我们计算这个估计器的偏差和方差。\n期望为：\n$$E[\\hat{V}_{\\text{TD}}(S_1)] = E[\\alpha R_0] = \\alpha E[R_0] = \\alpha \\cdot 1 = \\alpha$$\n偏差是期望估计值与真实值之间的差：\n$$\\text{Bias}_{\\text{TD}} = E[\\hat{V}_{\\text{TD}}(S_1)] - V^{\\ast}(S_1) = \\alpha - (1 + \\gamma + \\gamma^2)$$\n方差为：\n$$\\text{Var}(\\hat{V}_{\\text{TD}}(S_1)) = \\text{Var}(\\alpha R_0) = \\alpha^2 \\text{Var}(R_0)$$\n给定 $R_t = 1 + \\varepsilon_t$，$\\text{Var}(R_t) = \\text{Var}(\\varepsilon_t) = \\sigma^2 = 1$。\n$$\\text{Var}_{\\text{TD}} = \\alpha^2 \\sigma^2 = \\alpha^2$$\n均方误差是偏差的平方与方差之和：\n$$\\text{MSE}_{\\text{TD}} = (\\text{Bias}_{\\text{TD}})^2 + \\text{Var}_{\\text{TD}} = \\left( \\alpha - (1 + \\gamma + \\gamma^2) \\right)^2 + \\alpha^2$$\n\n**3. 蒙特卡洛 (MC) 估计分析**\n\n一个状态价值的单幕 MC 估计是从该状态开始的总折扣回报。对于 $t=0$ 时的 $S_1$，回报 $G_0$ 是：\n$$G_0 = R_0 + \\gamma R_1 + \\gamma^2 R_2$$\n因此，MC 估计 $\\hat{V}_{\\text{MC}}(S_1)$ 是：\n$$\\hat{V}_{\\text{MC}}(S_1) = R_0 + \\gamma R_1 + \\gamma^2 R_2$$\n\n现在，我们计算这个估计器的偏差和方差。\n期望为：\n$$E[\\hat{V}_{\\text{MC}}(S_1)] = E[R_0 + \\gamma R_1 + \\gamma^2 R_2] = E[R_0] + \\gamma E[R_1] + \\gamma^2 E[R_2]$$\n$$E[\\hat{V}_{\\text{MC}}(S_1)] = 1 + \\gamma(1) + \\gamma^2(1) = 1 + \\gamma + \\gamma^2 = V^{\\ast}(S_1)$$\n偏差为零，因为期望估计值等于真实值：\n$$\\text{Bias}_{\\text{MC}} = E[\\hat{V}_{\\text{MC}}(S_1)] - V^{\\ast}(S_1) = 0$$\n方差是利用噪声项 $\\varepsilon_t$ 的独立性来计算的，这使得奖励 $R_t$ 也是独立的：\n$$\\text{Var}(\\hat{V}_{\\text{MC}}(S_1)) = \\text{Var}(R_0 + \\gamma R_1 + \\gamma^2 R_2) = \\text{Var}(R_0) + \\gamma^2 \\text{Var}(R_1) + (\\gamma^2)^2 \\text{Var}(R_2)$$\n$$\\text{Var}_{\\text{MC}} = \\sigma^2 + \\gamma^2 \\sigma^2 + \\gamma^4 \\sigma^2 = (1 + \\gamma^2 + \\gamma^4)\\sigma^2$$\n当 $\\sigma^2=1$ 时：\n$$\\text{Var}_{\\text{MC}} = 1 + \\gamma^2 + \\gamma^4$$\nMC 估计器的均方误差是：\n$$\\text{MSE}_{\\text{MC}} = (\\text{Bias}_{\\text{MC}})^2 + \\text{Var}_{\\text{MC}} = 0^2 + (1 + \\gamma^2 + \\gamma^4) = 1 + \\gamma^2 + \\gamma^4$$\n\n**4. 数值计算与比率**\n\n我们已知 $\\alpha = \\frac{1}{2} = 0.5$ 以及 $\\gamma = 0.9$。\n首先，计算公共项 $1 + \\gamma + \\gamma^2$：\n$$1 + 0.9 + (0.9)^2 = 1 + 0.9 + 0.81 = 2.71$$\n现在，计算 $\\text{MSE}_{\\text{TD}}$：\n$$\\text{MSE}_{\\text{TD}} = (0.5 - 2.71)^2 + (0.5)^2 = (-2.21)^2 + 0.25 = 4.8841 + 0.25 = 5.1341$$\n接下来，计算 $\\text{MSE}_{\\text{MC}}$：\n$$\\text{MSE}_{\\text{MC}} = 1 + (0.9)^2 + (0.9)^4 = 1 + 0.81 + 0.6561 = 2.4661$$\n最后，我们计算比率：\n$$\\frac{\\text{MSE}_{\\text{TD}}}{\\text{MSE}_{\\text{MC}}} = \\frac{5.1341}{2.4661} \\approx 2.0818295$$\n四舍五入到四位有效数字，该比率为 $2.082$。",
            "answer": "$$\\boxed{2.082}$$"
        },
        {
            "introduction": "在简单的价值估计之上，本练习将引入后继表示（Successor Representation, SR），这是一个强大的模型，用于解释大脑（特别是海马体）如何表征环境结构。你将为一个环形网格世界推导出后继表示矩阵，并用它来预测当奖赏位置改变时价值信号的变化。这个过程清晰地展示了这种表征的灵活性，使其能够快速适应环境中的奖赏变化。",
            "id": "4014662",
            "problem": "考虑一个四状态环形网格世界，它模拟了在大脑中已知的多巴胺能奖励预测机制下，符合某种策略的空间导航。该环境是一个马尔可夫决策过程 (MDP)，由一个状态集 $\\{1,2,3,4\\}$ 定义，这些状态排列在一个环上，因此从状态 $1$ 可以移动到状态 $2$ 或 $4$，从状态 $2$ 可以移动到状态 $1$ 或 $3$，从状态 $3$ 可以移动到状态 $2$ 或 $4$，从状态 $4$ 可以移动到状态 $3$ 或 $1$。在每个离散时间步，智能体在两个可用动作（逆时针或顺时针）中均匀随机地选择一个，因此到每个邻居的一步转移概率都是 $1/2$。折扣因子为 $\\gamma = \\frac{4}{5}$。\n\n在此随机策略下，该环形世界的后继表示 (SR) 定义如下：对于状态 $i,j \\in \\{1,2,3,4\\}$，\n$$\nM_{ij} \\equiv \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\,\\mathbf{1}\\{s_{t} = j\\} \\,\\big|\\, s_{0} = i \\right],\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。此定义反映了在该策略下状态的期望折扣占用率，并广泛用于计算神经科学中，以模拟海马体表征及其与多巴胺能预测误差信号的相互作用。在状态 $j$ 处有一个标量奖励 $r(j)$ 时，状态价值函数定义为 $V(i) = \\sum_{j} M_{ij} \\, r(j)$。\n\n仅从上述定义和马尔可夫性质出发，推导决定在此随机策略下该环形世界的 $M$ 的线性系统，求解 $M$，然后使用 $V(i) = \\sum_{j} M_{ij} r(j)$ 预测当一个单位大小的标量奖励从状态 $2$ 移动到状态 $3$ 时，状态 $1$ 的价值变化。具体来说：\n\n- 当 $r(2) = 1$ 且对于 $j \\neq 2$ 时 $r(j) = 0$，以及当 $r(3) = 1$ 且对于 $j \\neq 3$ 时 $r(j) = 0$ 的情况下，计算确定 $V(1)$ 所需的 $M$ 的条目。\n- 计算当奖励从状态 $2$ 移动到状态 $3$ 时，预测的变化量 $\\Delta V(1)$，即 $\\Delta V(1) = V_{\\text{new}}(1) - V_{\\text{old}}(1)$，其中“old”指奖励在状态 $2$ 时的情景，“new”指奖励在状态 $3$ 时的情景。\n\n将您关于 $\\Delta V(1)$ 的最终答案表示为一个精确的有理数。不允许进行数值四舍五入。",
            "solution": "该问题陈述清晰，具有强化学习理论的科学依据，其表述在数学上是精确且自洽的。该问题是有效的。\n\n任务是计算当一个单位奖励从状态 2 移动到状态 3 时，状态 1 的状态价值函数的变化量 $\\Delta V(1)$。解决方案分为三个主要步骤：\n1.  推导控制后继表示 (SR) 矩阵 M 的线性系统。\n2.  利用问题的对称性求解 M 的必要条目。\n3.  计算 V(1) 的旧值和新值以及它们的差值。\n\n**步骤 1：推导后继表示的线性系统**\n\n后继表示 M 定义为：\n$$\nM_{ij} = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\,\\mathbf{1}\\{s_{t} = j\\} \\,\\big|\\, s_{0} = i \\right]\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数，$s_t$ 是在时间 $t$ 的状态，$\\gamma$ 是折扣因子。我们可以展开求和式：\n$$\nM_{ij} = \\mathbb{E}\\left[\\mathbf{1}\\{s_{0} = j\\} + \\sum_{t=1}^{\\infty} \\gamma^{t} \\,\\mathbf{1}\\{s_{t} = j\\} \\,\\big|\\, s_{0} = i \\right]\n$$\n给定 $s_0=i$，第一项 $\\mathbf{1}\\{s_{0} = j\\}$ 就是克罗内克 $\\delta$ 函数 $\\delta_{ij}$。我们可以从剩余的和式中提出因子 $\\gamma$：\n$$\nM_{ij} = \\delta_{ij} + \\mathbb{E}\\left[\\gamma \\sum_{t'=0}^{\\infty} \\gamma^{t'} \\,\\mathbf{1}\\{s_{t'+1} = j\\} \\,\\big|\\, s_{0} = i \\right]\n$$\n其中 $t' = t-1$。根据全期望定律，我们可以对下一个状态 $s_1$ 取条件：\n$$\nM_{ij} = \\delta_{ij} + \\gamma \\, \\mathbb{E}_{s_1 \\sim P(s_1|s_0=i)}\\left[ \\mathbb{E}\\left[\\sum_{t'=0}^{\\infty} \\gamma^{t'} \\,\\mathbf{1}\\{s_{t'+1} = j\\} \\,\\big|\\, s_1 \\right] \\right]\n$$\n内部的期望是 $M_{s_1, j}$ 的定义。根据转移概率 $P_{ik} = P(s_1=k|s_0=i)$ 对所有可能的下一个状态 $k$ 求和，我们得到：\n$$\nM_{ij} = \\delta_{ij} + \\gamma \\sum_{k=1}^{4} P_{ik} M_{kj}\n$$\n这是一个关于 M 条目的线性方程组。用矩阵形式表示为：\n$$\nM = I + \\gamma P M\n$$\n其中 $I$ 是单位矩阵，$P$ 是状态转移矩阵。重新整理，我们得到决定 M 的线性系统：\n$$\n(I - \\gamma P) M = I\n$$\n因此，$M$ 是矩阵 $(I - \\gamma P)$ 的逆。\n$$\nM = (I - \\gamma P)^{-1}\n$$\n\n**步骤 2：求解 M 的条目**\n\n首先，我们根据问题描述构建转移矩阵 $P$。状态为 $\\{1,2,3,4\\}$。策略是在两个可用动作中以均匀随机的方式选择一个（概率为 $1/2$）。折扣因子为 $\\gamma = 4/5$。\n-   从状态 $1$，移动到 $2$ 或 $4$：$P_{12}=1/2$, $P_{14}=1/2$。\n-   从状态 $2$，移动到 $1$ 或 $3$：$P_{21}=1/2$, $P_{23}=1/2$。\n-   从状态 $3$，移动到 $2$ 或 $4$：$P_{32}=1/2$, $P_{34}=1/2$。\n-   从状态 $4$，移动到 $1$ 或 $3$：$P_{41}=1/2$, $P_{43}=1/2$。\n所有其他转移概率均为 0。转移矩阵为：\n$$\nP = \\begin{pmatrix}\n0   1/2  0  1/2 \\\\\n1/2  0  1/2  0 \\\\\n0  1/2  0  1/2 \\\\\n1/2  0  1/2  0\n\\end{pmatrix}\n$$\n接下来，我们计算 $\\gamma P$：\n$$\n\\gamma P = \\frac{4}{5} \\begin{pmatrix}\n0  1/2  0  1/2 \\\\\n1/2  0  1/2  0 \\\\\n0  1/2  0  1/2 \\\\\n1/2  0  1/2  0\n\\end{pmatrix} = \\begin{pmatrix}\n0  2/5  0  2/5 \\\\\n2/5  0  2/5  0 \\\\\n0  2/5  0  2/5 \\\\\n2/5  0  2/5  0\n\\end{pmatrix}\n$$\n该问题描述了一个二分图，其中状态 $\\{1, 3\\}$ 构成一个部分，状态 $\\{2, 4\\}$ 构成另一个部分。$P$ 是对称的，所以 $M = (I - \\gamma P)^{-1}$ 也必须是对称的。图和策略的对称性意味着 M 的条目之间存在某些相等关系。令：\n-   $M_{11} = M_{33} = a$\n-   $M_{22} = M_{44} = b$\n-   $M_{12} = M_{21} = M_{14} = M_{41} = M_{32} = M_{23} = M_{34} = M_{43} = c$\n-   $M_{13} = M_{31} = e$\n-   $M_{24} = M_{42} = f$\n\n矩阵 $M$ 的结构为：\n$$\nM = \\begin{pmatrix} a  c  e  c \\\\ c  b  c  f \\\\ e  c  a  c \\\\ c  f  c  b \\end{pmatrix}\n$$\n我们使用逐元素的方程 $M_{ij} = \\delta_{ij} + \\gamma \\sum_k P_{ik} M_{kj}$ 来求解 $a, b, c, e, f$ 的值。\n-   对于 $M_{11}$：$a = 1 + \\gamma (P_{12}M_{21} + P_{14}M_{41}) = 1 + \\frac{4}{5}(\\frac{1}{2}c+\\frac{1}{2}c) = 1 + \\frac{4}{5}c$。\n-   对于 $M_{22}$：$b = 1 + \\gamma (P_{21}M_{12} + P_{23}M_{32}) = 1 + \\frac{4}{5}(\\frac{1}{2}c+\\frac{1}{2}c) = 1 + \\frac{4}{5}c$。\n这证实了 $a=b$。\n-   对于 $M_{12}$：$c = 0 + \\gamma (P_{12}M_{22} + P_{14}M_{42}) = \\frac{4}{5}(\\frac{1}{2}b+\\frac{1}{2}f) = \\frac{2}{5}(a+f)$。\n-   对于 $M_{13}$：$e = 0 + \\gamma (P_{12}M_{23} + P_{14}M_{43}) = \\frac{4}{5}(\\frac{1}{2}c+\\frac{1}{2}c) = \\frac{4}{5}c$。\n-   对于 $M_{24}$：$f = 0 + \\gamma (P_{21}M_{14} + P_{23}M_{34}) = \\frac{4}{5}(\\frac{1}{2}c+\\frac{1}{2}c) = \\frac{4}{5}c$。\n这证实了 $e=f$。\n\n我们现在得到一个关于 $a, c, f$ 的三元方程组：\n1.  $a = 1 + \\frac{4}{5}c$\n2.  $c = \\frac{2}{5}(a+f)$\n3.  $f = \\frac{4}{5}c$\n\n将 (3) 代入 (2)：\n$$\nc = \\frac{2}{5}\\left(a + \\frac{4}{5}c\\right) = \\frac{2}{5}a + \\frac{8}{25}c\n$$\n$$\nc\\left(1 - \\frac{8}{25}\\right) = \\frac{2}{5}a \\implies \\frac{17}{25}c = \\frac{2}{5}a \\implies c = \\frac{25}{17} \\cdot \\frac{2}{5}a = \\frac{10}{17}a\n$$\n现在将这个关于 $c$ 的表达式代入 (1)：\n$$\na = 1 + \\frac{4}{5}\\left(\\frac{10}{17}a\\right) = 1 + \\frac{8}{17}a\n$$\n$$\na\\left(1 - \\frac{8}{17}\\right) = 1 \\implies \\frac{9}{17}a = 1 \\implies a = \\frac{17}{9}\n$$\n已知 $a$ 后，我们可以求出 $c$ 和 $f$：\n$$\nc = \\frac{10}{17}a = \\frac{10}{17} \\cdot \\frac{17}{9} = \\frac{10}{9}\n$$\n$$\nf = \\frac{4}{5}c = \\frac{4}{5} \\cdot \\frac{10}{9} = \\frac{8}{9}\n$$\n因为 $e=f$，我们有 $e = 8/9$。所需的矩阵条目是 $M_{12}=c=10/9$ 和 $M_{13}=e=8/9$。\n\n**步骤 3：计算价值的变化**\n\n状态价值函数为 $V(i) = \\sum_j M_{ij} r(j)$。我们需要为两种不同的奖励函数计算 $V(1)$。\n\n情况 1 (旧): 在状态 2 有一个单位奖励。奖励向量为 $\\mathbf{r}_{\\text{old}}$，其中 $r(2)=1$ 且对于 $j\\neq 2$，$r(j)=0$。\n$$\nV_{\\text{old}}(1) = M_{11}r(1) + M_{12}r(2) + M_{13}r(3) + M_{14}r(4) = M_{12} \\cdot 1 = M_{12}\n$$\n$$\nV_{\\text{old}}(1) = \\frac{10}{9}\n$$\n\n情况 2 (新): 在状态 3 有一个单位奖励。奖励向量为 $\\mathbf{r}_{\\text{new}}$，其中 $r(3)=1$ 且对于 $j\\neq 3$，$r(j)=0$。\n$$\nV_{\\text{new}}(1) = M_{11}r(1) + M_{12}r(2) + M_{13}r(3) + M_{14}r(4) = M_{13} \\cdot 1 = M_{13}\n$$\n$$\nV_{\\text{new}}(1) = \\frac{8}{9}\n$$\n\n状态 1 的预测价值变化为 $\\Delta V(1)$：\n$$\n\\Delta V(1) = V_{\\text{new}}(1) - V_{\\text{old}}(1) = M_{13} - M_{12} = \\frac{8}{9} - \\frac{10}{9} = -\\frac{2}{9}\n$$\n状态 1 的价值下降，这是符合预期的，因为奖励从一个相邻状态（距离 1）移动到了一个相对的状态（距离 2）。",
            "answer": "$$\\boxed{-\\frac{2}{9}}$$"
        },
        {
            "introduction": "本项高级练习挑战了将价值视为单个标量的经典观点，旨在探索前沿的分布强化学习理论。你将通过更新一组代表未来回报完整分布的分位数，来体验该框架如何为观察到的多巴胺神经元反应的多样性提供更丰富的解释。这一实践将帮助你理解学习信号如何从单一的预测误差演变为一个多维的误差向量。",
            "id": "4014631",
            "problem": "考虑一个具有两个状态 $s_A$ 和 $s_B$ 的马尔可夫决策过程 (MDP)，每个状态有一个可用动作，以及一个确定性转移 $s_A \\to s_B$。状态 $s_B$ 是一个终止状态。折扣因子为 $\\gamma = 0.9$。假设一个分布式评价器，它通过一个具有 $N=3$ 个等权重原子的分类分布来表示任意状态 $s$ 的回报分布 $Z(s)$。这些原子位于分位数估计 $\\theta_i(s)$ 处，对应于分位数水平 $\\tau_i = \\frac{i - 0.5}{N}$，其中 $i \\in \\{1,2,3\\}$。这种表示方法与基于分位数的分布式强化学习一致，其中跨神经元的异构多巴胺能预测误差信号可以被解释为编码回报分布的不同分位数，而不是一个单一的期望值。\n\n初始时，评价器的原子为 $\\theta_1(s_A) = 0.5$，$\\theta_2(s_A) = 1.5$，$\\theta_3(s_A) = 2.5$，以及 $\\theta_1(s_B) = 0$，$\\theta_2(s_B) = 1$，$\\theta_3(s_B) = 3$。从 $s_A \\to s_B$ 观测到的单次转移产生一个即时奖励样本 $r = 1$。使用分位数回归时序差分方法和绝对分位数损失（Huber 参数设为 $0$），将状态 $s_A$ 的每个分位数的损失定义为\n$$\nL_i(s_A) = \\frac{1}{N} \\sum_{j=1}^{N} \\rho_{\\tau_i}\\!\\left(y_j - \\theta_i(s_A)\\right),\n$$\n其中目标样本为 $y_j = r + \\gamma\\,\\theta_j(s_B)$，分位数损失为 $\\rho_{\\tau}(u) = u\\left(\\tau - \\mathbb{I}\\{u  0\\}\\right)$。对每个 $\\theta_i(s_A)$ 执行一步学习率为 $\\alpha = 0.3$ 的随机梯度下降，以获得更新后的原子 $\\theta_i^{+}(s_A)$。\n\n与此同时，考虑一个期望值标量评价器 $V(s)$，其通过相应分类分布的均值进行初始化：$V(s_A) = \\frac{1}{3}\\sum_{i=1}^{3}\\theta_i(s_A)$ 和 $V(s_B) = \\frac{1}{3}\\sum_{i=1}^{3}\\theta_i(s_B)$。在 $s_A$ 处使用相同的学习率 $\\alpha$ 执行一次标准时序差分 (TD) 更新，\n$$\nV^{+}(s_A) = V(s_A) + \\alpha\\left[r + \\gamma V(s_B) - V(s_A)\\right].\n$$\n\n令 $\\bar{Z}^{+}(s_A) = \\frac{1}{3}\\sum_{i=1}^{3}\\theta_i^{+}(s_A)$ 表示 $s_A$ 处更新后的分类分布的均值。计算标量差\n$$\n\\Delta = V^{+}(s_A) - \\bar{Z}^{+}(s_A).\n$$\n以单个实数形式提供 $\\Delta$。无需四舍五入。",
            "solution": "该问题是良定的，具有强化学习理论的科学基础，并包含了得出唯一解所需的所有信息。所提供的定义和算法在计算神经科学和机器学习领域是标准的。因此，该问题是有效的。\n\n解题过程分为并行的两部分：首先，使用标准时序差分 (TD) 学习规则更新标量值函数 $V(s_A)$；其次，使用分位数回归更新分布式值函数的原子 $\\theta_i(s_A)$。最后，我们计算更新后的标量值与更新后的分布均值之间的差。\n\n已知条件如下：\n状态：$s_A, s_B$（终止）。\n转移：$s_A \\to s_B$。\n奖励样本：$r=1$。\n折扣因子：$\\gamma = 0.9$。\n学习率：$\\alpha = 0.3$。\n分布式评价器：$N=3$ 个原子 $\\theta_i$，分位数水平为 $\\tau_i = \\frac{i-0.5}{N}$。\n$s_A$ 处的初始原子：$\\theta_1(s_A) = 0.5$, $\\theta_2(s_A) = 1.5$, $\\theta_3(s_A) = 2.5$。\n$s_B$ 处的初始原子：$\\theta_1(s_B) = 0$, $\\theta_2(s_B) = 1$, $\\theta_3(s_B) = 3$。\n\n首先，我们通过对初始原子取均值来计算标量评价器 $V(s)$ 的初始值。\n对于状态 $s_A$：\n$$\nV(s_A) = \\frac{1}{N} \\sum_{i=1}^{N} \\theta_i(s_A) = \\frac{1}{3} (0.5 + 1.5 + 2.5) = \\frac{4.5}{3} = 1.5\n$$\n对于状态 $s_B$：\n$$\nV(s_B) = \\frac{1}{N} \\sum_{i=1}^{N} \\theta_i(s_B) = \\frac{1}{3} (0 + 1 + 3) = \\frac{4}{3}\n$$\n\n接下来，我们对 $V(s_A)$ 执行一次标准 TD 更新。TD 误差 $\\delta_{TD}$ 计算如下：\n$$\n\\delta_{TD} = r + \\gamma V(s_B) - V(s_A) = 1 + 0.9 \\left( \\frac{4}{3} \\right) - 1.5\n$$\n$$\n\\delta_{TD} = 1 + \\frac{9}{10} \\cdot \\frac{4}{3} - 1.5 = 1 + \\frac{12}{10} - 1.5 = 1 + 1.2 - 1.5 = 0.7\n$$\n更新后的值 $V^{+}(s_A)$ 为：\n$$\nV^{+}(s_A) = V(s_A) + \\alpha \\delta_{TD} = 1.5 + 0.3(0.7) = 1.5 + 0.21 = 1.71\n$$\n\n现在，我们进行分布式评价器的更新。首先，我们确定当 $N=3$ 时，对于 $i \\in \\{1, 2, 3\\}$ 的分位数水平 $\\tau_i$：\n$$\n\\tau_1 = \\frac{1 - 0.5}{3} = \\frac{0.5}{3} = \\frac{1}{6}\n$$\n$$\n\\tau_2 = \\frac{2 - 0.5}{3} = \\frac{1.5}{3} = \\frac{1}{2}\n$$\n$$\n\\tau_3 = \\frac{3 - 0.5}{3} = \\frac{2.5}{3} = \\frac{5}{6}\n$$\n\n接下来，我们计算对于 $j \\in \\{1, 2, 3\\}$ 的目标样本 $y_j = r + \\gamma \\theta_j(s_B)$：\n$$\ny_1 = 1 + 0.9 \\cdot \\theta_1(s_B) = 1 + 0.9(0) = 1\n$$\n$$\ny_2 = 1 + 0.9 \\cdot \\theta_2(s_B) = 1 + 0.9(1) = 1.9\n$$\n$$\ny_3 = 1 + 0.9 \\cdot \\theta_3(s_B) = 1 + 0.9(3) = 1 + 2.7 = 3.7\n$$\n\n对每个原子 $\\theta_i(s_A)$ 的更新是一个随机梯度下降步骤：\n$$\n\\theta_i^{+}(s_A) = \\theta_i(s_A) - \\alpha \\frac{\\partial L_i(s_A)}{\\partial \\theta_i(s_A)}\n$$\n损失 $L_i(s_A) = \\frac{1}{N} \\sum_{j=1}^{N} \\rho_{\\tau_i}\\!\\left(y_j - \\theta_i(s_A)\\right)$ 关于 $\\theta_i(s_A)$ 的梯度是：\n$$\n\\frac{\\partial L_i(s_A)}{\\partial \\theta_i(s_A)} = \\frac{1}{N} \\sum_{j=1}^{N} \\frac{\\partial}{\\partial \\theta_i(s_A)} \\rho_{\\tau_i}(y_j - \\theta_i(s_A))\n$$\n使用链式法则和分位数损失 $\\rho_{\\tau}(u)$ 的导数 $\\frac{d\\rho_\\tau}{du} = \\tau - \\mathbb{I}\\{u  0\\}$，我们得到：\n$$\n\\frac{\\partial}{\\partial \\theta_i(s_A)} \\rho_{\\tau_i}(y_j - \\theta_i(s_A)) = -(\\tau_i - \\mathbb{I}\\{y_j - \\theta_i(s_A)  0\\}) = \\mathbb{I}\\{y_j  \\theta_i(s_A)\\} - \\tau_i\n$$\n所以梯度是：\n$$\n\\nabla_{\\theta_i(s_A)} L_i = \\frac{1}{N} \\sum_{j=1}^{N} \\left(\\mathbb{I}\\{y_j  \\theta_i(s_A)\\} - \\tau_i\\right)\n$$\n我们为每个 $i \\in \\{1, 2, 3\\}$ 计算这个值：\n对于 $\\theta_1(s_A) = 0.5$ 和 $\\tau_1 = 1/6$：\n因为 $y=\\{1, 1.9, 3.7\\}$，所以条件 $y_j  0.5$ 对所有 $j \\in \\{1, 2, 3\\}$ 都不成立。\n$$\n\\nabla_1 = \\frac{1}{3} \\left[ (0 - \\frac{1}{6}) + (0 - \\frac{1}{6}) + (0 - \\frac{1}{6}) \\right] = \\frac{1}{3} \\left(-\\frac{3}{6}\\right) = -\\frac{1}{6}\n$$\n$\\theta_1(s_A)$ 的更新为：\n$$\n\\theta_1^{+}(s_A) = 0.5 - 0.3 \\left(-\\frac{1}{6}\\right) = 0.5 + 0.05 = 0.55\n$$\n\n对于 $\\theta_2(s_A) = 1.5$ 和 $\\tau_2 = 1/2$：\n条件 $y_j  1.5$ 仅对 $j=1$ ($y_1=1$) 成立。\n$$\n\\nabla_2 = \\frac{1}{3} \\left[ (1 - \\frac{1}{2}) + (0 - \\frac{1}{2}) + (0 - \\frac{1}{2}) \\right] = \\frac{1}{3} \\left(\\frac{1}{2} - \\frac{1}{2} - \\frac{1}{2}\\right) = \\frac{1}{3} \\left(-\\frac{1}{2}\\right) = -\\frac{1}{6}\n$$\n$\\theta_2(s_A)$ 的更新为：\n$$\n\\theta_2^{+}(s_A) = 1.5 - 0.3 \\left(-\\frac{1}{6}\\right) = 1.5 + 0.05 = 1.55\n$$\n\n对于 $\\theta_3(s_A) = 2.5$ 和 $\\tau_3 = 5/6$：\n条件 $y_j  2.5$ 对 $j=1$ ($y_1=1$) 和 $j=2$ ($y_2=1.9$) 成立。\n$$\n\\nabla_3 = \\frac{1}{3} \\left[ (1 - \\frac{5}{6}) + (1 - \\frac{5}{6}) + (0 - \\frac{5}{6}) \\right] = \\frac{1}{3} \\left(\\frac{1}{6} + \\frac{1}{6} - \\frac{5}{6}\\right) = \\frac{1}{3} \\left(-\\frac{3}{6}\\right) = -\\frac{1}{6}\n$$\n$\\theta_3(s_A)$ 的更新为：\n$$\n\\theta_3^{+}(s_A) = 2.5 - 0.3 \\left(-\\frac{1}{6}\\right) = 2.5 + 0.05 = 2.55\n$$\n更新后的原子为 $\\theta_1^{+}(s_A) = 0.55$，$\\theta_2^{+}(s_A) = 1.55$ 和 $\\theta_3^{+}(s_A) = 2.55$。\n\n现在，我们计算更新后的分类分布的均值 $\\bar{Z}^{+}(s_A)$：\n$$\n\\bar{Z}^{+}(s_A) = \\frac{1}{3} \\sum_{i=1}^{3} \\theta_i^{+}(s_A) = \\frac{1}{3} (0.55 + 1.55 + 2.55) = \\frac{4.65}{3} = 1.55\n$$\n\n最后，我们计算标量差 $\\Delta$：\n$$\n\\Delta = V^{+}(s_A) - \\bar{Z}^{+}(s_A) = 1.71 - 1.55 = 0.16\n$$",
            "answer": "$$\n\\boxed{0.16}\n$$"
        }
    ]
}