{
    "hands_on_practices": [
        {
            "introduction": "最优反馈控制理论的核心在于通过动态规划寻找最优策略。本练习将通过一个简化的离散时间线性二次调节器（LQR）问题，带你亲手实践贝尔曼最优性原理。通过从最终时刻反向递推，你将推导出每一步的最优控制律，从而深刻理解最优决策是如何在“当前成本”与“未来成本”之间进行权衡的。",
            "id": "4006058",
            "problem": "考虑一个简化的神经控制情境，其中单个标量皮层活动变量$x_t$编码一个运动相关状态，该状态需要通过运动指令$u_t$在一个短的规划时域内进行调节。该神经对象根据离散时间线性动力学$x_{t+1} = a x_t + b u_t$演化，其中$a \\in \\mathbb{R}$模拟内在状态传播，$b \\in \\mathbb{R}$模拟控制有效性。在两步时域$t \\in \\{0,1\\}$内的目标是选择控制量$u_0$和$u_1$以最小化累积二次代价\n$$J = q x_0^{2} + r u_0^{2} + q x_1^{2} + r u_1^{2} + q x_2^{2},$$\n其中$q \\ge 0$惩罚状态偏差（可解释为任务误差或与神经目标的偏差），$r > 0$惩罚控制努力（可解释为运动指令的能量成本）。在时间$t=2$时的最终惩罚$q x_2^{2}$反映了终端偏差成本。假设给定初始状态$x_0 \\in \\mathbb{R}$且$b \\ne 0$。应用有限时域问题的 Bellman 最优性原理（动态规划），推导出最优初始控制$u_0^{\\star}$作为$a$、$b$、$q$、$r$和$x_0$的显式函数。将最终答案表示为单个闭式解析表达式。无需四舍五入，也无需为答案附加物理单位。",
            "solution": "我们使用动态规划（贝尔曼最优性原理）从最终时刻$t=2$开始反向求解。价值函数$V_t(x)$表示从时刻$t$、状态$x$开始的最小未来累积成本。\n\n**第1步：时刻 t=2**\n在终点，未来成本为零，价值函数等于终端成本。\n$$V_2(x_2) = q x_2^2$$\n这可以写成二次型$V_2(x_2) = P_2 x_2^2$，其中$P_2 = q$。\n\n**第2步：时刻 t=1**\n贝尔曼方程为$V_1(x_1) = \\min_{u_1} \\{q x_1^2 + r u_1^2 + V_2(x_2)\\}$。\n代入$x_2 = a x_1 + b u_1$和$V_2(x_2)$：\n$$V_1(x_1) = \\min_{u_1} \\{q x_1^2 + r u_1^2 + P_2 (a x_1 + b u_1)^2\\}$$\n为了找到使括号内表达式最小化的$u_1$，我们对其求导并令其为零：\n$$\\frac{\\partial}{\\partial u_1} \\left( r u_1^2 + P_2 (a x_1 + b u_1)^2 \\right) = 2 r u_1 + 2 P_2(a x_1 + b u_1) b = 0$$\n$$u_1 (r + P_2 b^2) = -P_2 a b x_1 \\implies u_1^\\star = -\\frac{P_2 a b}{r + P_2 b^2} x_1$$\n将$P_2=q$代入，得到最优控制$u_1^\\star = -\\frac{q a b}{r + q b^2} x_1$。\n价值函数$V_1(x_1)$是$x_1$的二次函数，$V_1(x_1) = P_1 x_1^2$。根据离散时间里卡提方程，$P_1$为：\n$$P_1 = q + a P_2 a - (a P_2 b)(r + b P_2 b)^{-1}(b P_2 a)$$\n$$P_1 = q + a^2 q - \\frac{a^2 b^2 q^2}{r + q b^2} = \\frac{q(r+qb^2) + a^2q(r+qb^2) - a^2b^2q^2}{r+qb^2} = \\frac{qr + q^2b^2 + a^2qr + a^2q^2b^2 - a^2b^2q^2}{r+qb^2} = \\frac{q(r(1+a^2) + qb^2)}{r+qb^2}$$\n\n**第3步：时刻 t=0**\n根据问题定义，在$t=0$时，我们需要选择$u_0$来最小化从该点开始的成本，即$q x_0^2 + r u_0^2 + V_1(x_1)$。由于$q x_0^2$是常数，我们只需最小化$r u_0^2 + V_1(x_1)$。\n$$\\min_{u_0} \\{r u_0^2 + V_1(x_1)\\} = \\min_{u_0} \\{r u_0^2 + P_1 x_1^2\\} = \\min_{u_0} \\{r u_0^2 + P_1 (a x_0 + b u_0)^2\\}$$\n同样，对$u_0$求导并令其为零：\n$$\\frac{\\partial}{\\partial u_0} \\left( r u_0^2 + P_1 (a x_0 + b u_0)^2 \\right) = 2 r u_0 + 2 P_1(a x_0 + b u_0) b = 0$$\n$$u_0 (r + P_1 b^2) = -P_1 a b x_0 \\implies u_0^\\star = -\\frac{P_1 a b}{r + P_1 b^2} x_0$$\n\n**第4步：代入 P_1**\n现在我们将$P_1$的表达式代入$u_0^\\star$的公式中。\n分子：$P_1 a b = \\frac{abq(r(1+a^2) + qb^2)}{r+qb^2}$。\n分母：$r + P_1 b^2 = r + \\frac{qb^2(r(1+a^2) + qb^2)}{r+qb^2} = \\frac{r(r+qb^2) + qb^2(r(1+a^2) + qb^2)}{r+qb^2} = \\frac{r^2+rqb^2 + rqb^2(1+a^2) + q^2b^4}{r+qb^2} = \\frac{r^2 + 2rqb^2 + a^2rqb^2 + q^2b^4}{r+qb^2} = \\frac{(r+qb^2)^2 + a^2rqb^2}{r+qb^2}$。\n将分子和分母合并：\n$$u_0^\\star = -\\frac{\\frac{abq(r(1+a^2) + qb^2)}{r+qb^2}}{\\frac{(r+qb^2)^2 + a^2rqb^2}{r+qb^2}} x_0 = -\\frac{a b q (r(1+a^{2}) + q b^{2})}{(r+q b^{2})^{2} + a^{2} r q b^{2}} x_0$$\n这就是最优初始控制$u_0^\\star$的最终表达式。",
            "answer": "$$\\boxed{-\\frac{a b q (r(1+a^{2}) + q b^{2}) x_0}{(r+q b^{2})^{2} + a^{2} r q b^{2}}}$$"
        },
        {
            "introduction": "在连续时间系统中，我们如何量化控制一个系统的难易程度？本练习引入了可控性格拉姆矩阵（Controllability Gramian）这一关键工具，它揭示了状态空间中哪些方向“更容易”被控制。通过计算该矩阵并将其与最小控制能量联系起来，你将学会如何从系统动力学的内在结构出发，来预测和计算实现某一控制目标所需的最小代价。",
            "id": "4006062",
            "problem": "考虑一个连续时间线性时不变神经群体模型，其状态$\\mathbf{x}(t) \\in \\mathbb{R}^{2}$遵循动力学方程$\\frac{d\\mathbf{x}}{dt} = A \\mathbf{x} + B u(t)$，其中$A \\in \\mathbb{R}^{2 \\times 2}$模拟突触泄漏和区域间耦合，而$B \\in \\mathbb{R}^{2 \\times 1}$指定了单个刺激通道。设\n$$\nA = \\begin{pmatrix}\n-1  2 \\\\\n-3  -2\n\\end{pmatrix}, \n\\quad\nB = \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}.\n$$\n假设$A$是稳定的（所有特征值的实部都严格为负），并定义无穷时域可控性格拉姆矩阵为\n$$\nW_c = \\int_{0}^{\\infty} \\exp(A t)\\, B B^{\\top} \\exp(A^{\\top} t)\\, dt.\n$$\n从给定的积分定义以及矩阵指数和线性时不变系统的基本性质出发，精确计算$W_c$。然后，利用连续时间最优反馈控制理论的基础，解释为什么$W_c$的特征向量可以识别出状态空间中更容易或更难控制的方向，以及这与最小控制能量有何关联。\n\n最后，设控制目标是将系统从静息状态$\\mathbf{x}(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$驱动到目标状态$\\mathbf{x}_f = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$，终端时间足够大，以至于无穷时域可控性格拉姆矩阵适用。在所有能够使$\\mathbf{x}(T) = \\mathbf{x}_f$（对于足够大的$T$）成立的容许输入$u(t)$中，您必须计算的是最小平方$L^{2}$控制能量。请将您的最终答案表示为一个实数（无量纲）。无需四舍五入。",
            "solution": "该问题的解答分为三部分：计算可控性格拉姆矩阵$W_c$，解释其特征向量的物理意义，以及计算到达目标状态所需的最小控制能量。\n\n**第1部分：计算可控性格拉姆矩阵 $W_c$**\n由于系统矩阵$A$是稳定的，无穷时域可控性格拉姆矩阵$W_c$是以下连续时间李雅普诺夫方程的唯一对称正定解：\n$$ A W_c + W_c A^{\\top} = -BB^{\\top} $$\n我们有$A = \\begin{pmatrix} -1  2 \\\\ -3  -2 \\end{pmatrix}$和$B = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。因此：\n$$ -BB^{\\top} = -\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\end{pmatrix} = \\begin{pmatrix} -1  0 \\\\ 0  0 \\end{pmatrix} $$\n设$W_c = \\begin{pmatrix} w_{11}  w_{12} \\\\ w_{12}  w_{22} \\end{pmatrix}$。代入李雅普诺夫方程：\n$$ \\begin{pmatrix} -1  2 \\\\ -3  -2 \\end{pmatrix} \\begin{pmatrix} w_{11}  w_{12} \\\\ w_{12}  w_{22} \\end{pmatrix} + \\begin{pmatrix} w_{11}  w_{12} \\\\ w_{12}  w_{22} \\end{pmatrix} \\begin{pmatrix} -1  -3 \\\\ 2  -2 \\end{pmatrix} = \\begin{pmatrix} -1  0 \\\\ 0  0 \\end{pmatrix} $$\n执行矩阵乘法并相加，得到：\n$$ \\begin{pmatrix} -2w_{11} + 4w_{12}  -3w_{11} - 3w_{12} + 2w_{22} \\\\ -3w_{11} - 3w_{12} + 2w_{22}  -6w_{12} - 4w_{22} \\end{pmatrix} = \\begin{pmatrix} -1  0 \\\\ 0  0 \\end{pmatrix} $$\n这给出了一个线性方程组：\n1.  $-2w_{11} + 4w_{12} = -1$\n2.  $-3w_{11} - 3w_{12} + 2w_{22} = 0$\n3.  $-6w_{12} - 4w_{22} = 0$\n从(3)解得$w_{22} = -\\frac{3}{2}w_{12}$。代入(2)得到$w_{11} = -2w_{12}$。最后将此代入(1)得到$-2(-2w_{12}) + 4w_{12} = -1$，即$8w_{12} = -1$，所以$w_{12} = -1/8$。\n由此可得$w_{11} = 1/4$和$w_{22} = 3/16$。因此，可控性格拉姆矩阵为：\n$$ W_c = \\begin{pmatrix} 1/4  -1/8 \\\\ -1/8  3/16 \\end{pmatrix} $$\n\n**第2部分：$W_c$ 特征向量的解释**\n$W_c$的特征向量指出了状态空间中控制起来“容易”或“困难”的方向。这是因为达到某个目标状态$\\mathbf{x}_f$所需的最小控制能量由$E_{\\min} = \\mathbf{x}_f^{\\top} W_c^{-1} \\mathbf{x}_f$给出。\n若将$\\mathbf{x}_f$选为$W_c$的归一化特征向量$\\mathbf{v}_i$，其对应特征值为$\\lambda_i$，则$W_c \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$，进而$W_c^{-1} \\mathbf{v}_i = (1/\\lambda_i) \\mathbf{v}_i$。此时，最小能量为$E_{\\min} = \\mathbf{v}_i^{\\top} (1/\\lambda_i) \\mathbf{v}_i = 1/\\lambda_i$。\n-   **大特征值** $\\lambda_i$ 意味着小的$1/\\lambda_i$，即沿对应特征向量$\\mathbf{v}_i$方向移动状态所需的能量很少。这些是**容易控制**的方向。\n-   **小特征值** $\\lambda_i$ 意味着大的$1/\\lambda_i$，即沿对应特征向量$\\mathbf{v}_i$方向移动状态需要很大的能量。这些是**难以控制**的方向。\n从几何上看，所有能用单位控制能量达到的状态集合构成一个由$\\mathbf{x}^{\\top} W_c^{-1} \\mathbf{x} \\le 1$定义的椭球。该椭球的主轴与$W_c$的特征向量对齐，半轴长度为$\\sqrt{\\lambda_i}$。长轴（大$\\lambda_i$）方向是容易控制的，短轴（小$\\lambda_i$）方向是难以控制的。\n\n**第3部分：最小控制能量的计算**\n要将系统从原点驱动到$\\mathbf{x}_f = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$，所需的最小$L^2$控制能量为$E_{\\min} = \\mathbf{x}_f^{\\top} W_c^{-1} \\mathbf{x}_f$。\n首先计算$W_c^{-1}$。$W_c$的行列式为$\\det(W_c) = (1/4)(3/16) - (-1/8)^2 = 3/64 - 1/64 = 2/64 = 1/32$。\n$$ W_c^{-1} = \\frac{1}{1/32} \\begin{pmatrix} 3/16  1/8 \\\\ 1/8  1/4 \\end{pmatrix} = 32 \\begin{pmatrix} 3/16  1/8 \\\\ 1/8  1/4 \\end{pmatrix} = \\begin{pmatrix} 6  4 \\\\ 4  8 \\end{pmatrix} $$\n现在计算能量：\n$$ E_{\\min} = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 6  4 \\\\ 4  8 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n$$ E_{\\min} = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 6 - 4 \\\\ 4 - 8 \\end{pmatrix} = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -4 \\end{pmatrix} = 1(2) + (-1)(-4) = 2 + 4 = 6 $$\n因此，最小控制能量为6。",
            "answer": "$$ \\boxed{6} $$"
        },
        {
            "introduction": "真实世界的运动控制，例如大脑对肢体的调控，本质上是非线性的。本练习将向你展示如何将线性二次调节器（LQR）的思想扩展到非线性系统，通过学习迭代LQR（iLQR）算法的核心步骤。你将通过一个具体的反向扫描计算，理解iLQR是如何在一条名义轨迹周围进行局部二次近似，并迭代地求解出最优的前馈和反馈控制律的。",
            "id": "4006007",
            "problem": "一个运动皮层控制器被建模为在时间步长$t \\in \\{0,\\dots,T-1\\}$上解决一个有限时域最优控制问题，该问题具有平滑非线性动力学$x_{t+1} = f(x_t,u_t)$、阶段成本$\\ell(x_t,u_t)$和终端成本$\\phi(x_T)$。根据Bellman最优性原理，未来成本（cost-to-go）$V_t(x_t)$满足$V_t(x_t) = \\min_{u_t} \\big\\{ \\ell(x_t,u_t) + V_{t+1}(f(x_t,u_t)) \\big\\}$。迭代线性二次调节器（iterative Linear Quadratic Regulator, iLQR）算法通过围绕标称轨迹$\\{\\bar{x}_t,\\bar{u}_t\\}_{t=0}^{T-1}$对动力学和未来成本进行一阶和二阶泰勒展开，然后对得到的二次近似应用动态规划，以在每个时间步获得线性反馈律，从而构建局部反馈改进。考虑在时间$t$的一个反向传播步骤，其中$f$、$\\ell$和$V_{t+1}$的平滑性确保它们的一阶和二阶导数存在且有界。设在$(\\bar{x}_t,\\bar{u}_t)$处的动力学线性化为$A_t = \\partial f/\\partial x|_{(\\bar{x}_t,\\bar{u}_t)}$和$B_t = \\partial f/\\partial u|_{(\\bar{x}_t,\\bar{u}_t)}$。假设在$(\\bar{x}_t,\\bar{u}_t)$处阶段成本的二次近似系数为$\\ell_x, \\ell_u, \\ell_{xx}, \\ell_{ux}, \\ell_{uu}$，并且在$\\bar{x}_{t+1}$处$V_{t+1}$的局部二次模型的导数为$V_{x}^{+}$和$V_{xx}^{+}$。假设使用Gauss-Newton近似，即在$V_{t+1}(f(x_t,u_t))$的二阶展开中忽略$f$的二阶导数。\n\n对于一个具体的一维示例（$x_t \\in \\mathbb{R}$, $u_t \\in \\mathbb{R}$），假设在时间$t$时，标称轨迹上的以下数值导数成立：$A_t = 0.9$，$B_t = 0.5$，$\\ell_x = 0.2$，$\\ell_u = -0.4$，$\\ell_{xx} = 1.0$，$\\ell_{ux} = 0.1$，$\\ell_{uu} = 0.5$，$V_{x}^{+} = 1.2$，$V_{xx}^{+} = 2.0$。在iLQR局部二次近似和时间$t$的反向传播最小化下，最优的局部控制更新是状态偏差$\\delta x_t$的一个仿射函数，即$\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$，其中$k_t$是前馈项，$K_t$是反馈增益。\n\n哪个选项正确地描述了在Gauss-Newton假设下的iLQR反向增益计算，并为上述数据得出了正确的$k_t$和$K_t$数值？\n\nA. 使用Bellman目标的二次展开，该展开通过线性化动力学（Gauss-Newton）结合了阶段成本曲率和传播的价值曲率，使用$A_t$、$B_t$、$V_{x}^{+}$和$V_{xx}^{+}$定义局部$Q$函数曲率项，并选择使该二次型最小化的仿射律$\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$。增益由局部控制曲率的逆和交叉曲率给出，对于所提供的数值，得出$k_t = -0.2$和$K_t = -1.0$。\n\nB. 忽略未来成本的传播曲率，仅最小化时间$t$的阶段成本二次型，取$\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$且曲率仅来自$\\ell$。对于所提供的数值，这会产生$k_t = 0.8$和$K_t = -0.2$。\n\nC. 传播价值曲率，但选择仿射律对局部二次型进行梯度上升以避免欠驱动，因此对于所提供的数值，$k_t = +0.2$和$K_t = +1.0$。\n\nD. 始终应用一个固定阻尼$\\lambda = 0.1$的Levenberg–Marquardt (LM) 正则化，即使在未正则化的曲率已经是正定的情况下，也通过加上$\\lambda I$来修改控制曲率，对于所提供的数值，得出$k_t \\approx -0.1818$和$K_t \\approx -0.9091$。",
            "solution": "iLQR算法的反向传播步骤旨在找到一个局部最优的仿射控制律$\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$。这通过对动作-价值函数$Q_t(x_t, u_t) = \\ell(x_t, u_t) + V_{t+1}(f(x_t, u_t))$进行局部二次近似，并最小化该近似来实现。\n\n**1. 局部二次近似**\n我们围绕标称轨迹$(\\bar{x}_t, \\bar{u}_t)$展开$Q_t$，得到关于偏差$\\delta x_t = x_t - \\bar{x}_t$和$\\delta u_t = u_t - \\bar{u}_t$的二次模型。其梯度和Hessian矩阵（所谓的$Q$因子）如下：\n-   $Q_u = \\ell_u + B_t^T V_x^+$\n-   $Q_{uu} = \\ell_{uu} + B_t^T V_{xx}^+ B_t$\n-   $Q_{ux} = \\ell_{ux} + B_t^T V_{xx}^+ A_t$\n\n这里我们应用了链式法则，并根据问题要求使用了Gauss-Newton近似（忽略$f$的二阶导数）。\n\n**2. 计算 Q 因子**\n将给定的数值（均为标量）代入：\n$A_t = 0.9, B_t = 0.5, \\ell_u = -0.4, \\ell_{uu} = 0.5, \\ell_{ux} = 0.1, V_x^+ = 1.2, V_{xx}^+ = 2.0$。\n\n-   **梯度 $Q_u$**:\n    $Q_u = -0.4 + (0.5) \\cdot (1.2) = -0.4 + 0.6 = 0.2$\n\n-   **Hessian $Q_{uu}$ (控制成本曲率)**:\n    $Q_{uu} = 0.5 + (0.5) \\cdot (2.0) \\cdot (0.5) = 0.5 + 0.5 = 1.0$\n\n-   **Hessian $Q_{ux}$ (交叉成本曲率)**:\n    $Q_{ux} = 0.1 + (0.5) \\cdot (2.0) \\cdot (0.9) = 0.1 + 0.9 = 1.0$\n\n**3. 计算最优控制更新**\n最优控制更新$\\delta u_t^\\star$使$Q_t$的二次近似相对于$\\delta u_t$最小化。我们对近似式求导并令其为零：\n$$ \\frac{\\partial (\\delta Q_t)}{\\partial \\delta u_t} = Q_{uu} \\delta u_t + Q_{ux} \\delta x_t + Q_u = 0 $$\n解出$\\delta u_t^\\star$：\n$$ \\delta u_t^\\star = -Q_{uu}^{-1} Q_u - Q_{uu}^{-1} Q_{ux} \\delta x_t $$\n这与仿射形式$\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$相匹配，其中：\n-   前馈项：$k_t = -Q_{uu}^{-1} Q_u$\n-   反馈增益：$K_t = -Q_{uu}^{-1} Q_{ux}$\n\n**4. 计算增益数值**\n-   $k_t = -(1.0)^{-1} \\cdot (0.2) = -0.2$\n-   $K_t = -(1.0)^{-1} \\cdot (1.0) = -1.0$\n\n计算结果为$k_t = -0.2$和$K_t = -1.0$。\n\n**5. 选项分析**\n-   **选项 A** 正确描述了iLQR的计算过程（结合阶段成本和传播价值的曲率）并得出了与我们计算相符的数值。\n-   **选项 B** 错误地忽略了来自未来价值函数$V_{t+1}$的贡献，这违背了动态规划的基本原理。\n-   **选项 C** 错误地建议进行成本最大化（梯度上升），这与最优控制的目标相反。\n-   **选项 D** 引入了正则化，这并非标准iLQR算法的必要步骤，尤其是在$Q_{uu}$已经正定的情况下。\n\n因此，选项A是唯一正确描述了所要求解的过程并给出正确答案的选项。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}