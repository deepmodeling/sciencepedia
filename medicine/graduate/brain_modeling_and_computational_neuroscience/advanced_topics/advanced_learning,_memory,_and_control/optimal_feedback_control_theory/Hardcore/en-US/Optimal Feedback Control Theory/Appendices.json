{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration of optimal feedback control, we will start with its foundational building block: solving a finite-horizon Linear Quadratic Regulator (LQR) problem using dynamic programming. This exercise  requires you to work backward in time, applying the Bellman optimality principle to derive the optimal control law step-by-step. By solving this simple scalar problem, you will gain a concrete understanding of the Riccati recursion, the core computational method for finding optimal feedback gains in a wide range of control problems.",
            "id": "4006058",
            "problem": "Consider a simplified neural control setting where a single scalar cortical activity variable $x_t$ encodes a motor-relevant state to be regulated over a short planning horizon by a motor command $u_t$. The neural plant evolves according to the discrete-time linear dynamics $x_{t+1} = a x_t + b u_t$, where $a \\in \\mathbb{R}$ models intrinsic state propagation and $b \\in \\mathbb{R}$ models control effectiveness. The objective over a two-step horizon $t \\in \\{0,1\\}$ is to select controls $u_0$ and $u_1$ to minimize the cumulative quadratic cost \n$$J = q x_0^{2} + r u_0^{2} + q x_1^{2} + r u_1^{2} + q x_2^{2},$$\nwhere $q \\ge 0$ penalizes state deviation (interpretable as task error or deviation from a neural target) and $r  0$ penalizes control effort (interpretable as motor command energetic cost). The final penalty $q x_2^{2}$ at time $t=2$ reflects terminal deviation cost. Assume an initial state $x_0 \\in \\mathbb{R}$ is given and $b \\ne 0$. Using the Bellman optimality principle (dynamic programming) applied to this finite-horizon problem, derive the optimal initial control $u_0^{\\star}$ explicitly as a function of $a$, $b$, $q$, $r$, and $x_0$. Express your final answer as a single closed-form analytical expression. No rounding is required, and no physical units are to be attached to the answer.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n**Step 1: Extract Givens**\n- **System Dynamics:** The state $x_t \\in \\mathbb{R}$ evolves according to the discrete-time linear equation $x_{t+1} = a x_t + b u_t$.\n- **Control Input:** $u_t \\in \\mathbb{R}$.\n- **System Parameters:** $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$, with the constraint $b \\ne 0$.\n- **Time Horizon:** The problem is defined over a two-step horizon, for $t \\in \\{0,1\\}$, with states up to $x_2$.\n- **Cost Function:** The objective is to minimize the cumulative cost $J = q x_0^{2} + r u_0^{2} + q x_1^{2} + r u_1^{2} + q x_2^{2}$.\n- **Cost Parameters:** $q \\ge 0$ and $r  0$.\n- **Initial Condition:** The initial state $x_0 \\in \\mathbb{R}$ is given.\n- **Objective:** Derive the optimal initial control $u_0^{\\star}$ as a function of $a$, $b$, $q$, $r$, and $x_0$.\n- **Methodology:** Use the Bellman optimality principle (dynamic programming).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a classic finite-horizon, discrete-time Linear Quadratic Regulator (LQR) problem. This is a fundamental topic in optimal control theory and is a well-established, scientifically valid framework for modeling control systems, including simplified models in computational neuroscience.\n- **Well-Posed:** The problem is well-posed. The cost function is quadratic and convex since the state penalty coefficient $q \\ge 0$ and the control penalty coefficient $r  0$. The system dynamics are linear. The condition $r  0$ ensures a unique minimum exists for the control inputs. The condition $b \\ne 0$ ensures the system is controllable. Thus, a unique, stable, and meaningful solution for the optimal control sequence exists.\n- **Objective:** The problem is stated using precise mathematical language, free from ambiguity or subjective claims.\n- **Flaw Checklist:** The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, feasible, and well-posed. It is a standard problem that requires substantive reasoning.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be derived.\n\n### Derivation of the Optimal Control\nThe problem is to find the control sequence $u_0, u_1$ that minimizes the cost function $J$. We will use dynamic programming, working backwards from the final time step $t=2$.\n\nThe cost function is $J = q x_0^{2} + r u_0^{2} + q x_1^{2} + r u_1^{2} + q x_2^{2}$. This can be structured for dynamic programming by defining a stage cost $L(x_t, u_t) = q x_t^2 + r u_t^2$ for $t \\in \\{0, 1\\}$ and a terminal cost $M(x_2) = q x_2^2$. The total cost is then $J = L(x_0, u_0) + L(x_1, u_1) + M(x_2)$.\n\nWe define the optimal cost-to-go, or value function, $V_t(x_t)$ as the minimum cost achievable from state $x_t$ at time $t$ to the end of the horizon.\n\n**Step T=2 (Final Step):**\nThe value function at the terminal time $t=2$ is simply the terminal cost:\n$$V_2(x_2) = M(x_2) = q x_2^2$$\nThis is a quadratic function of the state. We can write this as $V_2(x_2) = P_2 x_2^2$, where $P_2 = q$.\n\n**Step T=1:**\nAccording to the Bellman principle, the value function at time $t=1$ is given by:\n$$V_1(x_1) = \\min_{u_1} \\left[ L(x_1, u_1) + V_2(x_2) \\right]$$\nSubstituting the expressions for the stage cost, value function $V_2$, and the system dynamics $x_2 = a x_1 + b u_1$:\n$$V_1(x_1) = \\min_{u_1} \\left[ q x_1^2 + r u_1^2 + P_2(a x_1 + b u_1)^2 \\right]$$\nTo find the optimal control $u_1^{\\star}$ that minimizes this expression, we take the partial derivative with respect to $u_1$ and set it to zero. Since $r0$, the quadratic function in $u_1$ is strictly convex, and this will yield the unique minimum.\n$$\\frac{\\partial}{\\partial u_1} \\left[ q x_1^2 + r u_1^2 + P_2(a x_1 + b u_1)^2 \\right] = 2 r u_1 + 2 P_2 (a x_1 + b u_1) b = 0$$\n$$r u_1 + P_2 a b x_1 + P_2 b^2 u_1 = 0$$\n$$(r + P_2 b^2) u_1 = -P_2 a b x_1$$\n$$u_1^{\\star}(x_1) = -\\frac{P_2 a b}{r + P_2 b^2} x_1$$\nThis is the optimal feedback control law at $t=1$. Now we find the value function $V_1(x_1)$ by substituting $u_1^{\\star}$ back into its definition. The value function will also be a quadratic function of the state, $V_1(x_1) = P_1 x_1^2$.\n$$V_1(x_1) = q x_1^2 + r (u_1^{\\star})^2 + P_2(a x_1 + b u_1^{\\star})^2$$\nSubstituting $u_1^{\\star} = -K_1 x_1$ where $K_1 = \\frac{P_2 a b}{r + P_2 b^2}$:\n$$V_1(x_1) = q x_1^2 + r K_1^2 x_1^2 + P_2(a - b K_1)^2 x_1^2 = \\left( q + r K_1^2 + P_2(a - b K_1)^2 \\right) x_1^2$$\nThus, $P_1 = q + r K_1^2 + P_2(a - b K_1)^2$. A more direct form of this update, known as the discrete-time Riccati equation, is $P_1 = q + a^2 P_2 - \\frac{(a b P_2)^2}{r + b^2 P_2} = q + \\frac{a^2 P_2 r}{r + b^2 P_2}$.\nSubstituting $P_2 = q$:\n$$P_1 = q + \\frac{a^2 q r}{r + q b^2}$$\n\n**Step T=0:**\nWe repeat the process for $t=0$ to find the desired optimal control $u_0^{\\star}$. The value function at $t=0$ is:\n$$V_0(x_0) = \\min_{u_0} \\left[ L(x_0, u_0) + V_1(x_1) \\right]$$\nSubstituting stage cost, $V_1(x_1) = P_1 x_1^2$, and dynamics $x_1 = a x_0 + b u_0$:\n$$V_0(x_0) = \\min_{u_0} \\left[ q x_0^2 + r u_0^2 + P_1(a x_0 + b u_0)^2 \\right]$$\nThe term $q x_0^2$ does not depend on $u_0$, so it does not affect the optimization. To find $u_0^{\\star}$, we differentiate the expression inside the minimum with respect to $u_0$ and set it to zero:\n$$\\frac{\\partial}{\\partial u_0} \\left[ q x_0^2 + r u_0^2 + P_1(a x_0 + b u_0)^2 \\right] = 2 r u_0 + 2 P_1(a x_0 + b u_0) b = 0$$\n$$r u_0 + P_1 a b x_0 + P_1 b^2 u_0 = 0$$\n$$(r + P_1 b^2) u_0 = -P_1 a b x_0$$\n$$u_0^{\\star}(x_0) = -\\frac{P_1 a b}{r + P_1 b^2} x_0$$\nThe final step is to substitute the expression for $P_1$ into this equation to get $u_0^{\\star}$ as a function of the given parameters.\nWe have $P_1 = q + \\frac{a^2 q r}{r + q b^2} = \\frac{q(r + q b^2) + a^2 q r}{r + q b^2} = \\frac{q r + q^2 b^2 + a^2 q r}{r + q b^2} = \\frac{q(r(1+a^2) + q b^2)}{r + q b^2}$.\n\nNow, let's compute the components of the expression for $u_0^{\\star}$:\nThe numerator term is $P_1 a b x_0$:\n$$P_1 a b x_0 = a b x_0 \\frac{q(r(1+a^2) + q b^2)}{r + q b^2}$$\nThe denominator term is $r + P_1 b^2$:\n$$r + P_1 b^2 = r + b^2 \\left( q + \\frac{a^2 q r}{r + q b^2} \\right) = r + q b^2 + \\frac{a^2 q r b^2}{r + q b^2}$$\n$$= \\frac{(r+q b^2)^2 + a^2 q r b^2}{r + q b^2}$$\nNow, form the fraction for $u_0^{\\star}$:\n$$u_0^{\\star} = - \\frac{P_1 a b}{r + P_1 b^2} x_0 = -abx_0 \\frac{P_1}{r+P_1b^2} = -abx_0 \\frac{\\frac{q(r(1+a^2) + q b^2)}{r + q b^2}}{\\frac{(r+q b^2)^2 + a^2 q r b^2}{r + q b^2}}$$\nSimplifying by canceling the common term $(r + q b^2)$ in the denominator of the numerator and denominator fractions:\n$$u_0^{\\star} = -abx_0 \\frac{q(r(1+a^2) + q b^2)}{(r+q b^2)^2 + a^2 q r b^2}$$\nThis can be written as a single fraction:\n$$u_0^{\\star} = - \\frac{a b q (r(1+a^2) + q b^2) x_0}{(r+q b^2)^2 + a^2 r q b^2}$$\nThis final expression gives the optimal initial control $u_0^{\\star}$ as a function of the system parameters $a, b$, cost parameters $q, r$, and the initial state $x_0$, as required.",
            "answer": "$$\\boxed{-\\frac{a b q (r(1+a^{2}) + q b^{2}) x_0}{(r+q b^{2})^{2} + a^{2} r q b^{2}}}$$"
        },
        {
            "introduction": "Having established the mechanics of LQR, we now turn to a deeper analysis of the controlled system's intrinsic properties. This exercise  introduces the controllability Gramian, a fundamental concept that quantifies the 'reachability' of a system's states. You will compute the Gramian and use it to determine the minimum control energy required to reach a target state, providing a direct link between abstract control theory and the tangible cost of movement.",
            "id": "4006062",
            "problem": "Consider a continuous-time, linear time-invariant neural population model with state $\\mathbf{x}(t) \\in \\mathbb{R}^{2}$ obeying the dynamics $\\frac{d\\mathbf{x}}{dt} = A \\mathbf{x} + B u(t)$, where $A \\in \\mathbb{R}^{2 \\times 2}$ models synaptic leak and inter-areal coupling, and $B \\in \\mathbb{R}^{2 \\times 1}$ specifies a single channel of stimulation. Let\n$$\nA = \\begin{pmatrix}\n-1  2 \\\\\n-3  -2\n\\end{pmatrix}, \n\\quad\nB = \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}.\n$$\nAssume $A$ is stable (all eigenvalues have strictly negative real parts), and define the infinite-horizon controllability Gramian by\n$$\nW_c = \\int_{0}^{\\infty} \\exp(A t)\\, B B^{\\top} \\exp(A^{\\top} t)\\, dt.\n$$\nStarting from the given integral definition and fundamental properties of matrix exponentials and linear time-invariant systems, compute $W_c$ exactly. Then, using the foundations of optimal feedback control theory in continuous time, explain why the eigenvectors of $W_c$ identify directions in state space that are easier or harder to control, and how this relates to minimal control effort.\n\nFinally, let the control objective be to drive the system from the resting state $\\mathbf{x}(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ to the target state $\\mathbf{x}_f = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ at some large enough terminal time so that the infinite-horizon controllability Gramian is applicable. Over all admissible inputs $u(t)$ achieving $\\mathbf{x}(T) = \\mathbf{x}_f$ for sufficiently large $T$, the minimal squared $L^{2}$ control effort is the quantity you must compute. Express your final answer as a single real number (dimensionless). No rounding is required.",
            "solution": "The user wants to solve a problem based on optimal feedback control theory.\n\n### Step 1: Extract Givens\n-   System dynamics: $\\frac{d\\mathbf{x}}{dt} = A \\mathbf{x} + B u(t)$, with $\\mathbf{x}(t) \\in \\mathbb{R}^{2}$.\n-   System matrix: $A = \\begin{pmatrix} -1  2 \\\\ -3  -2 \\end{pmatrix}$.\n-   Input matrix: $B = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n-   Assumption: The matrix $A$ is stable.\n-   Definition of the infinite-horizon controllability Gramian: $W_c = \\int_{0}^{\\infty} \\exp(A t)\\, B B^{\\top} \\exp(A^{\\top} t)\\, dt$.\n-   Initial state: $\\mathbf{x}(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n-   Target state: $\\mathbf{x}_f = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n-   Objective:\n    1.  Compute $W_c$ exactly.\n    2.  Explain why the eigenvectors of $W_c$ identify directions of easier/harder control.\n    3.  Compute the minimal squared $L^2$ control effort to drive the system from $\\mathbf{x}(0)$ to $\\mathbf{x}_f$ in the infinite-horizon limit.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is rooted in standard linear time-invariant (LTI) control theory.\n\n1.  **Scientific or Factual Unsoundness**: The model is a standard LTI state-space representation, a cornerstone of control theory and a common simplification in computational neuroscience. The concepts of the controllability Gramian and minimal control effort are fundamental and mathematically rigorous.\n2.  **Well-Posedness**: The problem requires checking the stability of $A$ and the controllability of the pair $(A, B)$.\n    -   **Stability of $A$**: The characteristic equation is $\\det(A - \\lambda I) = 0$.\n        $$ (\\!-\\!1-\\lambda)(-2-\\lambda) - (2)(-3) = \\lambda^2+3\\lambda+2+6 = \\lambda^2+3\\lambda+8 = 0 $$\n        The eigenvalues are $\\lambda = \\frac{-3 \\pm \\sqrt{3^2 - 4(1)(8)}}{2} = \\frac{-3 \\pm i\\sqrt{23}}{2}$. The real part of both eigenvalues is $-\\frac{3}{2}$, which is strictly negative. Thus, the system matrix $A$ is stable (Hurwitz), and the assumption holds. The integral for $W_c$ converges.\n    -   **Controllability**: The controllability matrix is $\\mathcal{C} = \\begin{pmatrix} B  AB \\end{pmatrix}$.\n        $$ AB = \\begin{pmatrix} -1  2 \\\\ -3  -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -3 \\end{pmatrix} $$\n        $$ \\mathcal{C} = \\begin{pmatrix} 1  -1 \\\\ 0  -3 \\end{pmatrix} $$\n        The determinant is $\\det(\\mathcal{C}) = (1)(-3) - (-1)(0) = -3 \\neq 0$. The system is controllable, so any finite state $\\mathbf{x}_f$ can be reached from the origin.\n3.  **Completeness**: All necessary matrices, definitions, and boundary conditions are provided.\n\nThe problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid and can be solved.\n\nThe solution proceeds in three parts as requested.\n\n**Part 1: Computation of the Controllability Gramian $W_c$**\n\nFor a stable LTI system, the infinite-horizon controllability Gramian $W_c = \\int_{0}^{\\infty} \\exp(A t)\\, B B^{\\top} \\exp(A^{\\top} t)\\, dt$ is the unique, symmetric, positive definite solution to the continuous-time Lyapunov equation:\n$$ A W_c + W_c A^{\\top} = -BB^{\\top} $$\nThis equation is a fundamental property derived by integrating the derivative of the integrand $\\exp(A t)\\, B B^{\\top} \\exp(A^{\\top} t)$ from $t=0$ to $t=\\infty$ and using the stability of $A$.\n\nLet $W_c = \\begin{pmatrix} w_{11}  w_{12} \\\\ w_{21}  w_{22} \\end{pmatrix}$. Due to symmetry, $w_{12} = w_{21}$.\nThe matrices are:\n$$ A = \\begin{pmatrix} -1  2 \\\\ -3  -2 \\end{pmatrix}, \\quad A^{\\top} = \\begin{pmatrix} -1  -3 \\\\ 2  -2 \\end{pmatrix} $$\n$$ -BB^{\\top} = -\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\end{pmatrix} = -\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} -1  0 \\\\ 0  0 \\end{pmatrix} $$\nSubstituting these into the Lyapunov equation:\n$$ \\begin{pmatrix} -1  2 \\\\ -3  -2 \\end{pmatrix} \\begin{pmatrix} w_{11}  w_{12} \\\\ w_{12}  w_{22} \\end{pmatrix} + \\begin{pmatrix} w_{11}  w_{12} \\\\ w_{12}  w_{22} \\end{pmatrix} \\begin{pmatrix} -1  -3 \\\\ 2  -2 \\end{pmatrix} = \\begin{pmatrix} -1  0 \\\\ 0  0 \\end{pmatrix} $$\nMultiplying the matrices on the left-hand side yields:\n$$ \\begin{pmatrix} -w_{11} + 2w_{12}  -w_{12} + 2w_{22} \\\\ -3w_{11} - 2w_{12}  -3w_{12} - 2w_{22} \\end{pmatrix} + \\begin{pmatrix} -w_{11} + 2w_{12}  -3w_{11} - 2w_{12} \\\\ -w_{12} + 2w_{22}  -3w_{12} - 2w_{22} \\end{pmatrix} = \\begin{pmatrix} -1  0 \\\\ 0  0 \\end{pmatrix} $$\nSumming these matrices gives:\n$$ \\begin{pmatrix} -2w_{11} + 4w_{12}  -3w_{11} - 3w_{12} + 2w_{22} \\\\ -3w_{11} - 3w_{12} + 2w_{22}  -6w_{12} - 4w_{22} \\end{pmatrix} = \\begin{pmatrix} -1  0 \\\\ 0  0 \\end{pmatrix} $$\nThis yields a system of three linear equations for the three unknown elements of $W_c$:\n1.  $-2w_{11} + 4w_{12} = -1$\n2.  $-3w_{11} - 3w_{12} + 2w_{22} = 0$\n3.  $-6w_{12} - 4w_{22} = 0$\n\nFrom equation (3), $w_{22} = -\\frac{6}{4}w_{12} = -\\frac{3}{2}w_{12}$.\nSubstitute this into equation (2):\n$$ -3w_{11} - 3w_{12} + 2\\left(-\\frac{3}{2}w_{12}\\right) = 0 \\implies -3w_{11} - 3w_{12} - 3w_{12} = 0 \\implies -3w_{11} = 6w_{12} \\implies w_{11} = -2w_{12} $$\nSubstitute this result for $w_{11}$ into equation (1):\n$$ -2(-2w_{12}) + 4w_{12} = -1 \\implies 4w_{12} + 4w_{12} = -1 \\implies 8w_{12} = -1 \\implies w_{12} = -\\frac{1}{8} $$\nNow, we find the other components:\n$$ w_{11} = -2\\left(-\\frac{1}{8}\\right) = \\frac{1}{4} $$\n$$ w_{22} = -\\frac{3}{2}\\left(-\\frac{1}{8}\\right) = \\frac{3}{16} $$\nThus, the controllability Gramian is:\n$$ W_c = \\begin{pmatrix} \\frac{1}{4}  -\\frac{1}{8} \\\\ -\\frac{1}{8}  \\frac{3}{16} \\end{pmatrix} $$\n\n**Part 2: Interpretation of the Eigenvectors of $W_c$**\n\nThe controllability Gramian $W_c$ characterizes the reachable states from the origin. The set of all states $\\mathbf{x}$ that can be reached from $\\mathbf{x}(0) = \\mathbf{0}$ using control inputs $u(t)$ with a total squared $L^2$ norm (energy) of unity, i.e., $\\int_0^\\infty u(t)^2 dt \\leq 1$, forms an ellipsoid in the state space defined by the quadratic form $\\mathbf{x}^{\\top} W_c^{-1} \\mathbf{x} \\leq 1$.\n\nThe eigenvectors of $W_c$ define the principal axes of this reachable-set ellipsoid. Let the eigendecomposition of the symmetric matrix $W_c$ be $W_c = V \\Lambda V^{\\top}$, where $V$ is an orthogonal matrix whose columns are the eigenvectors $\\mathbf{v}_i$, and $\\Lambda$ is a diagonal matrix of the corresponding eigenvalues $\\lambda_i$. The inverse is $W_c^{-1} = V \\Lambda^{-1} V^{\\top}$.\n\nThe ellipsoid equation becomes $\\mathbf{x}^{\\top} (V \\Lambda^{-1} V^{\\top}) \\mathbf{x} \\leq 1$. By changing coordinates to the basis of eigenvectors, $\\mathbf{y} = V^{\\top}\\mathbf{x}$, the equation simplifies to $\\mathbf{y}^{\\top} \\Lambda^{-1} \\mathbf{y} \\leq 1$, or $\\sum_i \\frac{y_i^2}{\\lambda_i} \\leq 1$. The semi-axis length along the $i$-th eigenvector direction is $\\sqrt{\\lambda_i}$.\n\nA large eigenvalue $\\lambda_i$ corresponds to a long semi-axis of the ellipsoid. This means that a unit-energy input can drive the state far in the direction of the corresponding eigenvector $\\mathbf{v}_i$. Therefore, directions in state space aligned with eigenvectors having large eigenvalues are \"easy\" to control; they require little control effort to achieve a given displacement.\n\nConversely, a small eigenvalue $\\lambda_j$ corresponds to a short semi-axis. This means that even a unit-energy input can only move the state a small distance in the direction of $\\mathbf{v}_j$. These directions are \"hard\" to control, requiring large control effort for significant displacement.\n\nThis relationship is explicitly captured by the formula for the minimal control effort required to reach a target state $\\mathbf{x}_f$, which is $E_{\\min} = \\mathbf{x}_f^{\\top} W_c^{-1} \\mathbf{x}_f$. If $\\mathbf{x}_f$ is an eigenvector $\\mathbf{v}_i$, then $E_{\\min} = \\mathbf{v}_i^{\\top} (V \\Lambda^{-1} V^{\\top}) \\mathbf{v}_i = \\mathbf{e}_i^{\\top} \\Lambda^{-1} \\mathbf{e}_i = 1/\\lambda_i$, where $\\mathbf{e}_i$ is the standard basis vector. The minimal effort is inversely proportional to the eigenvalue, formally linking larger eigenvalues to easier control.\n\n**Part 3: Computation of Minimal Control Effort**\n\nThe problem asks for the minimal squared $L^2$ control effort, $E_{\\min} = \\int_0^T u(t)^2 dt$, to steer the system from $\\mathbf{x}(0) = \\mathbf{0}$ to $\\mathbf{x}(T) = \\mathbf{x}_f$ for a sufficiently large time $T$. For $T \\to \\infty$, this minimal effort is given by the quadratic form:\n$$ E_{\\min} = \\mathbf{x}_f^{\\top} W_c^{-1} \\mathbf{x}_f $$\nwhere $W_c$ is the infinite-horizon controllability Gramian computed in Part 1. First, we must calculate the inverse of $W_c$:\n$$ W_c = \\begin{pmatrix} \\frac{1}{4}  -\\frac{1}{8} \\\\ -\\frac{1}{8}  \\frac{3}{16} \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} 4  -2 \\\\ -2  3 \\end{pmatrix} $$\nThe determinant is:\n$$ \\det(W_c) = \\left(\\frac{1}{4}\\right)\\left(\\frac{3}{16}\\right) - \\left(-\\frac{1}{8}\\right)^2 = \\frac{3}{64} - \\frac{1}{64} = \\frac{2}{64} = \\frac{1}{32} $$\nThe inverse $W_c^{-1}$ is:\n$$ W_c^{-1} = \\frac{1}{\\det(W_c)} \\begin{pmatrix} \\frac{3}{16}  \\frac{1}{8} \\\\ \\frac{1}{8}  \\frac{1}{4} \\end{pmatrix} = 32 \\begin{pmatrix} \\frac{3}{16}  \\frac{1}{8} \\\\ \\frac{1}{8}  \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 32 \\cdot \\frac{3}{16}  32 \\cdot \\frac{1}{8} \\\\ 32 \\cdot \\frac{1}{8}  32 \\cdot \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 6  4 \\\\ 4  8 \\end{pmatrix} $$\nNow, we compute the minimal effort $E_{\\min}$ with the target state $\\mathbf{x}_f = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$:\n$$ E_{\\min} = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 6  4 \\\\ 4  8 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nFirst, perform the matrix-vector multiplication:\n$$ \\begin{pmatrix} 6  4 \\\\ 4  8 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 6(1) + 4(-1) \\\\ 4(1) + 8(-1) \\end{pmatrix} = \\begin{pmatrix} 6 - 4 \\\\ 4 - 8 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -4 \\end{pmatrix} $$\nThen, perform the final vector-vector multiplication:\n$$ E_{\\min} = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -4 \\end{pmatrix} = 1(2) + (-1)(-4) = 2 + 4 = 6 $$\nThe minimal squared $L^2$ control effort is $6$.",
            "answer": "$$ \\boxed{6} $$"
        },
        {
            "introduction": "The real power of optimal control becomes apparent when we move beyond linear systems to model the complex, nonlinear dynamics inherent in biological motor control. This final practice  introduces the iterative Linear Quadratic Regulator (iLQR), a cornerstone algorithm for nonlinear trajectory optimization. You will perform a single 'backward pass' of the iLQR algorithm, learning how to construct a local linear-quadratic approximation of a nonlinear problem to iteratively improve a control policy.",
            "id": "4006007",
            "problem": "A motor cortical controller is modeled as solving a finite-horizon optimal control problem over time steps $t \\in \\{0,\\dots,T-1\\}$ with smooth nonlinear dynamics $x_{t+1} = f(x_t,u_t)$, stage cost $\\ell(x_t,u_t)$, and terminal cost $\\phi(x_T)$. According to the Bellman optimality principle, the cost-to-go $V_t(x_t)$ satisfies $V_t(x_t) = \\min_{u_t} \\big\\{ \\ell(x_t,u_t) + V_{t+1}(f(x_t,u_t)) \\big\\}$. The iterative Linear Quadratic Regulator (iLQR) algorithm constructs a local feedback improvement by expanding the dynamics and cost-to-go locally around a nominal trajectory $\\{\\bar{x}_t,\\bar{u}_t\\}_{t=0}^{T-1}$ using first- and second-order Taylor expansions, and then applying dynamic programming on the resulting quadratic approximation to obtain a linear feedback law at each time step. Consider one backward-sweep step at time $t$, where the smoothness of $f$, $\\ell$, and $V_{t+1}$ ensures that their first and second derivatives exist and are bounded. Let the linearization of the dynamics at $(\\bar{x}_t,\\bar{u}_t)$ be $A_t = \\partial f/\\partial x|_{(\\bar{x}_t,\\bar{u}_t)}$ and $B_t = \\partial f/\\partial u|_{(\\bar{x}_t,\\bar{u}_t)}$. Suppose the quadratic approximation coefficients of the stage cost at $(\\bar{x}_t,\\bar{u}_t)$ are $\\ell_x, \\ell_u, \\ell_{xx}, \\ell_{ux}, \\ell_{uu}$, and the local quadratic model of $V_{t+1}$ at $\\bar{x}_{t+1}$ has derivatives $V_{x}^{+}, V_{xx}^{+}$. Assume the Gauss-Newton approximation that ignores second derivatives of $f$ in the second-order expansion of $V_{t+1}(f(x_t,u_t))$. \n\nFor a specific one-dimensional example ($x_t \\in \\mathbb{R}$, $u_t \\in \\mathbb{R}$), suppose at time $t$ the following numerical derivatives hold at the nominal trajectory: $A_t = 0.9$, $B_t = 0.5$, $\\ell_x = 0.2$, $\\ell_u = -0.4$, $\\ell_{xx} = 1.0$, $\\ell_{ux} = 0.1$, $\\ell_{uu} = 0.5$, $V_{x}^{+} = 1.2$, $V_{xx}^{+} = 2.0$. Under the iLQR local quadratic approximation and backward-sweep minimization at time $t$, the optimal local control update is an affine function of the state deviation $\\delta x_t$, namely $\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$, where $k_t$ is the feedforward term and $K_t$ is the feedback gain.\n\nWhich option correctly characterizes the iLQR backward gain computation under the Gauss-Newton assumption and yields the correct numerical values of $k_t$ and $K_t$ for the data above?\n\nA. Use a quadratic expansion of the Bellman objective that combines the stage cost curvature and the propagated value curvature through the linearized dynamics (Gauss-Newton), define local $Q$-function curvature terms using $A_t$, $B_t$, $V_{x}^{+}$, and $V_{xx}^{+}$, and choose the affine law $\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$ that minimizes this quadratic. The gains are given by the inverse of the local control curvature and the cross-curvature, yielding $k_t = -0.2$ and $K_t = -1.0$ for the provided numbers.\n\nB. Ignore the propagated curvature of the future cost-to-go and minimize only the stage cost quadratic at time $t$, taking $\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$ with curvatures from $\\ell$ alone. This produces $k_t = 0.8$ and $K_t = -0.2$ for the provided numbers.\n\nC. Propagate value curvature but choose the affine law to ascend the local quadratic (gradient ascent) to avoid underactuation, so that $k_t = +0.2$ and $K_t = +1.0$ for the provided numbers.\n\nD. Always apply Levenberg–Marquardt (LM) regularization with a fixed damping $\\lambda = 0.1$ that modifies the control curvature by adding $\\lambda I$ even when the unregularized curvature is already positive definite, giving $k_t \\approx -0.1818$ and $K_t \\approx -0.9091$ for the provided numbers.",
            "solution": "The user wants me to validate the problem statement and then solve for the feedforward and feedback gains in an iLQR backward pass.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\nThe problem statement provides the following information:\n-   A finite-horizon optimal control problem with time steps $t \\in \\{0,\\dots,T-1\\}$.\n-   Smooth nonlinear dynamics: $x_{t+1} = f(x_t,u_t)$.\n-   Stage cost $\\ell(x_t,u_t)$ and terminal cost $\\phi(x_T)$.\n-   Bellman equation: $V_t(x_t) = \\min_{u_t} \\big\\{ \\ell(x_t,u_t) + V_{t+1}(f(x_t,u_t)) \\big\\}$.\n-   The algorithm is the iterative Linear Quadratic Regulator (iLQR).\n-   A local feedback improvement is found by expanding around a nominal trajectory $\\{\\bar{x}_t,\\bar{u}_t\\}_{t=0}^{T-1}$.\n-   The Gauss-Newton approximation is used, which ignores second derivatives of the dynamics $f$.\n-   At a specific time step $t$, the linearization of the dynamics at $(\\bar{x}_t,\\bar{u}_t)$ is given by $A_t = \\partial f/\\partial x$ and $B_t = \\partial f/\\partial u$.\n-   The quadratic approximation coefficients of the stage cost $\\ell$ at $(\\bar{x}_t,\\bar{u}_t)$ are $\\ell_x, \\ell_u, \\ell_{xx}, \\ell_{ux}, \\ell_{uu}$.\n-   The local quadratic model of the future cost-to-go $V_{t+1}$ at $\\bar{x}_{t+1}$ is characterized by its gradient $V_{x}^{+}$ and Hessian $V_{xx}^{+}$.\n-   The optimal local control update is an affine function of the state deviation: $\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$.\n-   A one-dimensional numerical example is provided with the following values: $A_t = 0.9$, $B_t = 0.5$, $\\ell_x = 0.2$, $\\ell_u = -0.4$, $\\ell_{xx} = 1.0$, $\\ell_{ux} = 0.1$, $\\ell_{uu} = 0.5$, $V_{x}^{+} = 1.2$, $V_{xx}^{+} = 2.0$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is firmly rooted in optimal feedback control theory. The iLQR algorithm, Bellman's principle of optimality, and the Gauss-Newton approximation are all standard and fundamental concepts in this domain. The application to a motor cortical controller is a well-established area of research in computational neuroscience.\n-   **Well-Posed:** The problem is well-posed. It requires the calculation of specific parameters ($k_t$ and $K_t$) for a single step of a clearly defined algorithm (iLQR backward pass), and it provides all the necessary numerical inputs to perform this calculation. A unique solution for the gains exists.\n-   **Objective:** The problem is stated using precise, objective mathematical language and standard terminology from the field of control theory. There are no subjective or ambiguous statements.\n-   The problem is free of any of the invalidity flaws. It is mathematically sound, complete for the task, and computationally verifiable.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will now proceed to derive the solution.\n\n### Derivation of the iLQR Backward Pass\n\nThe core of the iLQR backward pass is to form a local quadratic model of the action-value function, or $Q$-function, at time $t$ and then minimize it to find the optimal control update. The action-value function is the argument of the minimization in the Bellman equation:\n$$\nQ_t(x_t, u_t) = \\ell(x_t, u_t) + V_{t+1}(f(x_t, u_t))\n$$\nWe perform a second-order Taylor expansion of $Q_t$ around the nominal point $(\\bar{x}_t, \\bar{u}_t)$ in terms of deviations $\\delta x_t = x_t - \\bar{x}_t$ and $\\delta u_t = u_t - \\bar{u}_t$. The change in $Q_t$ is approximately:\n$$\n\\delta Q_t \\approx \\frac{1}{2} \\begin{pmatrix} \\delta x_t \\\\ \\delta u_t \\end{pmatrix}^T \\begin{pmatrix} Q_{xx}  Q_{xu} \\\\ Q_{ux}  Q_{uu} \\end{pmatrix} \\begin{pmatrix} \\delta x_t \\\\ \\delta u_t \\end{pmatrix} + \\begin{pmatrix} Q_x \\\\ Q_u \\end{pmatrix}^T \\begin{pmatrix} \\delta x_t \\\\ \\delta u_t \\end{pmatrix}\n$$\nThe terms $Q_x$, $Q_u$, $Q_{xx}$, $Q_{ux}$, and $Q_{uu}$ are the first and second-order derivatives of $Q_t$ with respect to $x_t$ and $u_t$, evaluated at the nominal trajectory $(\\bar{x}_t, \\bar{u}_t)$. These are known as the $Q$-factors.\n\nUsing the chain rule, we can express these derivatives in terms of the derivatives of $\\ell$ and $V_{t+1}$. The future value function $V_{t+1}$ is evaluated at the nominal next state $\\bar{x}_{t+1} = f(\\bar{x}_t, \\bar{u}_t)$. Derivatives of $V_{t+1}$ are denoted with a superscript $'+'$, e.g., $V_x^+ = \\nabla V_{t+1}(\\bar{x}_{t+1})$.\n\nThe first derivatives are:\n$$\nQ_x = \\ell_x + (\\nabla_x f)^T \\nabla V_{t+1} = \\ell_x + A_t^T V_x^+\n$$\n$$\nQ_u = \\ell_u + (\\nabla_u f)^T \\nabla V_{t+1} = \\ell_u + B_t^T V_x^+\n$$\n\nThe second derivatives (Hessians) require care. The full Hessian of $V_{t+1}(f(x_t, u_t))$ involves second derivatives of $f$. The Gauss-Newton approximation specified in the problem consists of dropping these terms. Therefore, we have:\n$$\nQ_{xx} \\approx \\ell_{xx} + A_t^T V_{xx}^+ A_t\n$$\n$$\nQ_{uu} \\approx \\ell_{uu} + B_t^T V_{xx}^+ B_t\n$$\n$$\nQ_{ux} \\approx \\ell_{ux} + B_t^T V_{xx}^+ A_t\n$$\n\nNow, we substitute the provided numerical values into these formulas. Since the problem is one-dimensional, all matrices and vectors are scalars.\n$$\nQ_u = \\ell_u + B_t V_x^+ = -0.4 + (0.5)(1.2) = -0.4 + 0.6 = 0.2\n$$\n$$\nQ_{uu} = \\ell_{uu} + B_t V_{xx}^+ B_t = 0.5 + (0.5)(2.0)(0.5) = 0.5 + 0.5 = 1.0\n$$\n$$\nQ_{ux} = \\ell_{ux} + B_t V_{xx}^+ A_t = 0.1 + (0.5)(2.0)(0.9) = 0.1 + 0.9 = 1.0\n$$\n\nTo find the optimal control update $\\delta u_t^\\star$, we minimize the quadratic approximation of $\\delta Q_t$ with respect to $\\delta u_t$. We take the derivative and set it to zero:\n$$\n\\frac{\\partial (\\delta Q_t)}{\\partial \\delta u_t} = Q_{uu} \\delta u_t + Q_{ux} \\delta x_t + Q_u = 0\n$$\nSolving for $\\delta u_t$ gives the optimal update $\\delta u_t^\\star$:\n$$\nQ_{uu} \\delta u_t^\\star = -Q_u - Q_{ux} \\delta x_t\n$$\n$$\n\\delta u_t^\\star = -Q_{uu}^{-1} Q_u - Q_{uu}^{-1} Q_{ux} \\delta x_t\n$$\nThis expression is of the form $\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$, where $k_t$ is the open-loop or feedforward term and $K_t$ is the feedback gain.\n$$\nk_t = -Q_{uu}^{-1} Q_u\n$$\n$$\nK_t = -Q_{uu}^{-1} Q_{ux}\n$$\n\nUsing our calculated $Q$-factors:\n$$\nk_t = -(1.0)^{-1} (0.2) = -0.2\n$$\n$$\nK_t = -(1.0)^{-1} (1.0) = -1.0\n$$\nThus, the optimal local control update is $\\delta u_t^\\star = -0.2 - 1.0 \\delta x_t$.\n\n### Option-by-Option Analysis\n\nA. **Use a quadratic expansion of the Bellman objective that combines the stage cost curvature and the propagated value curvature through the linearized dynamics (Gauss-Newton), define local $Q$-function curvature terms using $A_t$, $B_t$, $V_{x}^{+}$, and $V_{xx}^{+}$, and choose the affine law $\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$ that minimizes this quadratic. The gains are given by the inverse of the local control curvature and the cross-curvature, yielding $k_t = -0.2$ and $K_t = -1.0$ for the provided numbers.**\nThis option accurately describes the iLQR backward pass with the Gauss-Newton approximation. The methodology of combining stage cost and propagated value function curvature to form the $Q$-factors, and then minimizing the resulting quadratic form, is correct. The resulting numerical values, $k_t = -0.2$ and $K_t = -1.0$, match our derivation.\n**Verdict: Correct**\n\nB. **Ignore the propagated curvature of the future cost-to-go and minimize only the stage cost quadratic at time $t$, taking $\\delta u_t^{\\star} = k_t + K_t \\,\\delta x_t$ with curvatures from $\\ell$ alone. This produces $k_t = 0.8$ and $K_t = -0.2$ for the provided numbers.**\nThis option proposes an incorrect algorithm. Ignoring the propagated curvature ($V_{xx}^+$) and gradient ($V_x^+$) from the future cost-to-go is fundamentally contrary to the principle of dynamic programming on which iLQR is based. If we were to follow this myopic approach, we would use $Q_u \\approx \\ell_u = -0.4$, $Q_{uu} \\approx \\ell_{uu} = 0.5$, and $Q_{ux} \\approx \\ell_{ux} = 0.1$. The gains would be $k_t = -(\\ell_{uu})^{-1} \\ell_u = -(0.5)^{-1}(-0.4) = 2 \\times 0.4 = 0.8$ and $K_t = -(\\ell_{uu})^{-1} \\ell_{ux} = -(0.5)^{-1}(0.1) = -2 \\times 0.1 = -0.2$. While the numbers match the calculation under this flawed premise, the method itself is incorrect.\n**Verdict: Incorrect**\n\nC. **Propagate value curvature but choose the affine law to ascend the local quadratic (gradient ascent) to avoid underactuation, so that $k_t = +0.2$ and $K_t = +1.0$ for the provided numbers.**\nThis option proposes to maximize the cost function (gradient ascent) instead of minimizing it. The goal of optimal control is cost minimization. Maximizing the cost is the antithesis of the objective. The numerical values $k_t = +0.2$ and $K_t = +1.0$ are simply the negated values of the correct gains, which would arise from moving in the direction of the positive gradient instead of the negative. The justification of \"avoiding underactuation\" is nonsensical in this context.\n**Verdict: Incorrect**\n\nD. **Always apply Levenberg–Marquardt (LM) regularization with a fixed damping $\\lambda = 0.1$ that modifies the control curvature by adding $\\lambda I$ even when the unregularized curvature is already positive definite, giving $k_t \\approx -0.1818$ and $K_t \\approx -0.9091$ for the provided numbers.**\nThis option describes a regularized version of iLQR. Regularization (e.g., Levenberg-Marquardt) is typically applied to ensure that the Hessian of the $Q$-function with respect to control, $Q_{uu}$, is positive definite, which guarantees a unique minimum and a stable feedback law. In our case, $Q_{uu} = 1.0$, which is already positive definite. While some implementations add a small amount of regularization unconditionally for numerical stability, it is not a part of the fundamental iLQR algorithm described in the problem. The problem asks for the standard gain computation, not a specific regularized variant. The calculation with regularization $Q_{uu}^{\\text{reg}} = Q_{uu} + \\lambda = 1.0 + 0.1 = 1.1$ yields $k_t = -(1.1)^{-1}(0.2) \\approx -0.1818$ and $K_t = -(1.1)^{-1}(1.0) \\approx -0.9091$. The numbers are correct for this modified algorithm, but the algorithm itself is not what the problem asks for.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}