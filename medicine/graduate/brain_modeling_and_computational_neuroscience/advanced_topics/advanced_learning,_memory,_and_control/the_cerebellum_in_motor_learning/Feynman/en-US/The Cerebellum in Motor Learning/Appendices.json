{
    "hands_on_practices": [
        {
            "introduction": "To understand how the cerebellum learns, we must first grasp the fundamental wiring of its canonical microcircuit. This exercise simplifies the complex biology into a linear rate-based model, a powerful tool for analyzing signal flow. By constructing an adjacency matrix , you will trace the path from mossy fiber input to the deep cerebellar nuclei and calculate the net computational effect, revealing the core transformation performed by this critical pathway.",
            "id": "4027513",
            "problem": "Consider a canonical cerebellar microcircuit comprised of the following four node types: mossy fibers ($M$), granule cells ($G$), Purkinje cells ($P$), and deep cerebellar nuclei ($D$). In biological terms, the mossy fiber to granule cell synapse is glutamatergic and therefore excitatory, the parallel fiber (granule cell axon) to Purkinje cell synapse is excitatory, and the Purkinje cell to deep cerebellar nuclei synapse is gamma-aminobutyric acid (GABA)-mediated and therefore inhibitory. In a linear rate-based model at the population level, let the state vector be $x \\in \\mathbb{R}^{4}$ ordered as $(M, G, P, D)$, and let the synaptic graph be represented by an adjacency matrix $A \\in \\mathbb{R}^{4 \\times 4}$, where $A_{j i}$ is the signed gain from node $i$ to node $j$ per unit time.\n\nThe nonzero synaptic gains are given as follows: from mossy fibers to granule cells $w_{G M} = 0.8$, from granule cells to Purkinje cells $w_{P G} = 0.9$, and from Purkinje cells to deep cerebellar nuclei $w_{D P} = 1.2$. Because the Purkinje cell to deep cerebellar nuclei synapse is inhibitory, it contributes a negative sign. All other direct connections among these four nodes are set to zero for this construction.\n\n1. Using these rules and the node order $(M, G, P, D)$, construct the directed adjacency matrix $A$.\n2. Starting from the linear time-invariant population dynamics $x_{t+1} = A x_{t}$ and first principles of linear composition of synaptic actions, derive the net signed gain transmitted exclusively along the Purkinje cell branch $M \\to G \\to P \\to D$. Express the final result as a single dimensionless real number.\n\nYour final answer must be a single real number. No rounding is required. Express the final answer as a dimensionless quantity.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in a simplified but standard model of cerebellar circuitry, well-posed with sufficient and consistent information, and objectively stated. The task is to construct an adjacency matrix for a linear rate-based model and then calculate the net gain along a specific polysynaptic pathway.\n\nFirst, we address the construction of the directed adjacency matrix $A$. The state vector $x$ is ordered as $(M, G, P, D)$, which corresponds to indices $1, 2, 3, 4$ respectively. The element $A_{ji}$ of the adjacency matrix represents the signed gain from node $i$ (the source) to node $j$ (the target).\n\nThe given connections and their corresponding non-zero matrix elements are:\n1.  The connection from mossy fibers ($M$, index $1$) to granule cells ($G$, index $2$) is excitatory with a gain of $w_{GM} = 0.8$. Therefore, the matrix element $A_{21}$ is positive:\n    $$A_{21} = +w_{GM} = 0.8$$\n2.  The connection from granule cells ($G$, index $2$) to Purkinje cells ($P$, index $3$) is excitatory with a gain of $w_{PG} = 0.9$. Therefore, the matrix element $A_{32}$ is positive:\n    $$A_{32} = +w_{PG} = 0.9$$\n3.  The connection from Purkinje cells ($P$, index $3$) to deep cerebellar nuclei ($D$, index $4$) is inhibitory with a gain of $w_{DP} = 1.2$. Therefore, the matrix element $A_{43}$ is negative:\n    $$A_{43} = -w_{DP} = -1.2$$\n\nThe problem states that all other direct connections among these four nodes are set to zero. This implies that all other elements of the matrix $A$ are $0$. With the ordering $(M, G, P, D)$, the resulting $4 \\times 4$ adjacency matrix $A$ is:\n$$\nA = \n\\begin{pmatrix}\nA_{11}  A_{12}  A_{13}  A_{14} \\\\\nA_{21}  A_{22}  A_{23}  A_{24} \\\\\nA_{31}  A_{32}  A_{33}  A_{34} \\\\\nA_{41}  A_{42}  A_{43}  A_{44}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0  0  0  0 \\\\\n0.8  0  0  0 \\\\\n0  0.9  0  0 \\\\\n0  0  -1.2  0\n\\end{pmatrix}\n$$\n\nNext, we derive the net signed gain transmitted exclusively along the Purkinje cell branch $M \\to G \\to P \\to D$. The system dynamics are given by the linear time-invariant equation $x_{t+1} = A x_{t}$. The effect of a signal propagating along a multi-step path is found by composing the actions of the adjacency matrix.\n\nA signal starting at the mossy fibers ($M$) at time $t=0$ propagates through the network.\n- At time $t=1$, the activity from $M$ influences $G$. The gain is given by $A_{21}$.\n- At time $t=2$, the activity from $G$ (which originated from $M$) influences $P$. The gain of this step is $A_{32}$. The cumulative gain from $M$ to $P$ over these two steps is the product $A_{32} \\times A_{21}$.\n- At time $t=3$, the activity from $P$ (which originated from $G$, which originated from $M$) influences $D$. The gain of this final step is $A_{43}$.\n\nThe net signed gain along the entire trisynaptic path $M \\to G \\to P \\to D$ is the product of the individual signed gains for each synaptic step in the sequence. This corresponds to the influence of an initial impulse at node $M$ on node $D$ after three time steps, along this specific path. Mathematically, this is the product of the corresponding matrix elements:\n$$ \\text{Net Gain} = A_{43} \\times A_{32} \\times A_{21} $$\n\nThis can also be seen by computing the matrix power $A^3$. The element $(A^3)_{ji}$ gives the total gain from node $i$ to node $j$ over all paths of length $3$. In this specific circuit, the only path of length $3$ from $M$ (node $1$) to $D$ (node $4$) is $M \\to G \\to P \\to D$. Thus, the net gain is precisely the element $(A^3)_{41}$.\n\nWe calculate the product of the gains:\n$$ \\text{Net Gain} = (-w_{DP}) \\times (+w_{PG}) \\times (+w_{GM}) $$\nSubstituting the given numerical values:\n$$ \\text{Net Gain} = (-1.2) \\times (0.9) \\times (0.8) $$\nFirst, we compute the product of the first two terms:\n$$ (-1.2) \\times (0.9) = -1.08 $$\nThen, we multiply this result by the third term:\n$$ -1.08 \\times 0.8 = -0.864 $$\n\nThe net signed gain along the path $M \\to G \\to P \\to D$ is a dimensionless quantity representing the overall amplification or attenuation and inversion of the signal. The negative sign indicates that the overall effect of this pathway is inhibitory, which is expected due to the final inhibitory synapse from the Purkinje cell.",
            "answer": "$$\n\\boxed{-0.864}\n$$"
        },
        {
            "introduction": "The cerebellum's remarkable ability to refine motor skills stems from synaptic plasticity, particularly at the parallel fiber-to-Purkinje cell synapse. This practice delves into the theoretical core of this process, the Marr-Albus model of motor learning. Using a three-factor learning rule , you will derive the steady-state expected change in synaptic weight, providing a mathematical foundation for how climbing fiber error signals sculpt motor commands over time.",
            "id": "4027517",
            "problem": "A canonical computational model for cerebellar motor learning uses a linear Purkinje cell readout driven by parallel fiber inputs and a three-factor synaptic plasticity rule modulated by the climbing fiber complex spike. Consider a single Purkinje cell with linear output $\\hat{y}_t = \\sum_{i=1}^{N} w_i x_{i,t}$ driven by parallel fiber inputs $x_{i,t}$. The climbing fiber emits a binary complex spike $c_t \\in \\{0,1\\}$ that signals the presence of a motor error on time step $t$. Assume the following:\n\n- The weight update obeys a three-factor rule $\\Delta w_i(t) = - \\eta\\, c_t\\, e_{i,t}$, where $\\eta  0$ is the learning rate and $e_{i,t}$ is an eligibility trace.\n- The eligibility trace is a linear leaky accumulator of presynaptic activity, $e_{i,t} = \\gamma\\, e_{i,t-1} + x_{i,t}$, with $0 \\leq \\gamma  1$.\n- The presynaptic input $x_{i,t}$ is weakly stationary with finite first moment and time-invariant mean $\\mathbb{E}[x_{i,t}] = \\mu_i$ (no further distributional assumptions are required for the first-moment analysis).\n- The complex spike $c_t$ is Bernoulli with parameter $p \\in (0,1)$, independent across time and independent of $x_{i,t}$ and $e_{i,t}$.\n- Steady state exists for the eligibility trace first moment because $|\\gamma|  1$.\n\nUsing only these assumptions and standard definitions of expectation and stationarity, derive the steady-state expected weight change per time step, $\\mathbb{E}[\\Delta w_i(t)]$, as a closed-form analytic expression in terms of $\\eta$, $p$, $\\gamma$, and $\\mu_i$. Your final answer must be a single closed-form analytic expression. No rounding is required, and no physical units apply.",
            "solution": "The problem has been validated and is deemed scientifically grounded, well-posed, objective, and internally consistent. It is a standard derivation in theoretical neuroscience. Therefore, I will proceed with the solution.\n\nThe objective is to derive the steady-state expected weight change per time step, which we denote as $\\mathbb{E}[\\Delta w_i]_{\\text{ss}}$. The analysis begins with the provided synaptic weight update rule.\n\nThe change in the synaptic weight $w_i$ at time step $t$ is given by the three-factor rule:\n$$\n\\Delta w_i(t) = - \\eta\\, c_t\\, e_{i,t}\n$$\nwhere $\\eta  0$ is the learning rate, $c_t \\in \\{0, 1\\}$ is the climbing fiber signal, and $e_{i,t}$ is the eligibility trace for the $i$-th input.\n\nTo find the expected weight change, we take the expectation of both sides of this equation:\n$$\n\\mathbb{E}[\\Delta w_i(t)] = \\mathbb{E}[- \\eta\\, c_t\\, e_{i,t}]\n$$\nBy the linearity of the expectation operator, the constant $-\\eta$ can be factored out:\n$$\n\\mathbb{E}[\\Delta w_i(t)] = - \\eta\\, \\mathbb{E}[c_t\\, e_{i,t}]\n$$\nThe problem states that the climbing fiber signal $c_t$ is independent of the eligibility trace $e_{i,t}$. This independence allows us to separate the expectation of the product into the product of the expectations:\n$$\n\\mathbb{E}[c_t\\, e_{i,t}] = \\mathbb{E}[c_t]\\, \\mathbb{E}[e_{i,t}]\n$$\nThe climbing fiber signal $c_t$ is a Bernoulli random variable with parameter $p \\in (0,1)$. The expectation of a Bernoulli variable is equal to its parameter, so:\n$$\n\\mathbb{E}[c_t] = p\n$$\nSubstituting this back into the expression for the expected weight change, we get:\n$$\n\\mathbb{E}[\\Delta w_i(t)] = - \\eta\\, p\\, \\mathbb{E}[e_{i,t}]\n$$\nThis expression gives the expected weight change at any time step $t$. To find the *steady-state* expected weight change, we must first determine the steady-state expectation of the eligibility trace, $\\mathbb{E}[e_{i,t}]$.\n\nThe dynamics of the eligibility trace are given by the linear leaky accumulator model:\n$$\ne_{i,t} = \\gamma\\, e_{i,t-1} + x_{i,t}\n$$\nwhere $0 \\le \\gamma  1$ is the decay factor and $x_{i,t}$ is the presynaptic input.\n\nTaking the expectation of this equation yields:\n$$\n\\mathbb{E}[e_{i,t}] = \\mathbb{E}[\\gamma\\, e_{i,t-1} + x_{i,t}]\n$$\nUsing the linearity of expectation again:\n$$\n\\mathbb{E}[e_{i,t}] = \\gamma\\, \\mathbb{E}[e_{i,t-1}] + \\mathbb{E}[x_{i,t}]\n$$\nWe are given that the presynaptic input $x_{i,t}$ is weakly stationary with a time-invariant mean $\\mathbb{E}[x_{i,t}] = \\mu_i$. Substituting this into the equation gives a recurrence relation for the mean of the eligibility trace:\n$$\n\\mathbb{E}[e_{i,t}] = \\gamma\\, \\mathbb{E}[e_{i,t-1}] + \\mu_i\n$$\nThe problem states that a steady state for the first moment of the eligibility trace exists. At steady state, the statistical properties of the process are time-invariant. Therefore, the expectation of the eligibility trace must be constant over time. We can define the steady-state expectation as $\\mathbb{E}[e_i]_{\\text{ss}}$, such that:\n$$\n\\mathbb{E}[e_i]_{\\text{ss}} = \\mathbb{E}[e_{i,t}] = \\mathbb{E}[e_{i,t-1}]\n$$\nSubstituting this into the recurrence relation gives:\n$$\n\\mathbb{E}[e_i]_{\\text{ss}} = \\gamma\\, \\mathbb{E}[e_i]_{\\text{ss}} + \\mu_i\n$$\nWe now solve this algebraic equation for $\\mathbb{E}[e_i]_{\\text{ss}}$:\n$$\n\\mathbb{E}[e_i]_{\\text{ss}} - \\gamma\\, \\mathbb{E}[e_i]_{\\text{ss}} = \\mu_i\n$$\n$$\n(1 - \\gamma)\\, \\mathbb{E}[e_i]_{\\text{ss}} = \\mu_i\n$$\nSince it is given that $0 \\le \\gamma  1$, the term $(1 - \\gamma)$ is strictly positive and non-zero. We can therefore divide by it to find the unique solution:\n$$\n\\mathbb{E}[e_i]_{\\text{ss}} = \\frac{\\mu_i}{1 - \\gamma}\n$$\nThis expression represents the steady-state mean of the eligibility trace. It is the time-averaged presynaptic input $\\mu_i$ scaled by the factor $1/(1-\\gamma)$, which corresponds to the DC gain of the leaky integrator.\n\nFinally, we substitute this steady-state value back into our expression for the expected weight change. The steady-state expected weight change, $\\mathbb{E}[\\Delta w_i]_{\\text{ss}}$, is therefore:\n$$\n\\mathbb{E}[\\Delta w_i]_{\\text{ss}} = - \\eta\\, p\\, \\mathbb{E}[e_i]_{\\text{ss}} = - \\eta\\, p\\, \\left(\\frac{\\mu_i}{1 - \\gamma}\\right)\n$$\nThis yields the final closed-form analytic expression in terms of the given parameters $\\eta$, $p$, $\\gamma$, and $\\mu_i$.",
            "answer": "$$\n\\boxed{-\\frac{\\eta p \\mu_i}{1 - \\gamma}}\n$$"
        },
        {
            "introduction": "Having explored the circuit's structure and its learning rule, we now scale up to the systems level to see how these elements enable motor learning. This exercise frames the cerebellum as an adaptive function approximator, a device that learns to generate desired motor torques by combining a vast array of internal basis functions represented by parallel fiber activity. By solving for the optimal synaptic weights with and without biological constraints , you will gain insight into how the cerebellum implements internal models to achieve fluid, coordinated movement.",
            "id": "4027515",
            "problem": "You are modeling the transformation performed by the cerebellum during motor learning as a supervised function approximation from kinematic basis functions to a desired torque trajectory. Let there be $m$ basis functions $\\{ \\phi_i(t) \\}_{i=1}^m$ sampled at $n$ time points $\\{ t_k \\}_{k=1}^n$ over an interval of duration $1$ second. Define the design matrix $X \\in \\mathbb{R}^{n \\times m}$ by $X_{k,i} = \\phi_i(t_k)$ and the target torque vector $y \\in \\mathbb{R}^n$ by $y_k = y(t_k)$, where $y(t)$ is the desired torque profile. The torque has physical units Newton meter (N·m), and all angles used inside trigonometric functions are in radians. The modeled cerebellar output is $\\hat{y}(t) = \\sum_{i=1}^m w_i \\phi_i(t)$, where $w \\in \\mathbb{R}^m$ represents synaptic weights.\n\nStarting from first principles of least squares estimation, the unconstrained optimal weights $w^{*}$ minimize the empirical mean squared error\n$$\nJ(w) = \\frac{1}{n} \\sum_{k=1}^n \\left( \\sum_{i=1}^m w_i \\phi_i(t_k) - y(t_k) \\right)^2,\n$$\nsubject to no constraints on $w$. Biological synaptic constraints such as non-negativity require $w_i \\ge 0$ for all $i \\in \\{1,\\dots,m\\}$, which alters the optimization problem to a Non-Negative Least Squares problem.\n\nYour task is to:\n- Compute the unconstrained optimal weights $w^{*}$ by minimizing $J(w)$.\n- Compute the constrained optimal weights $\\tilde{w}$ under $w_i \\ge 0$ for all $i$.\n- Quantify the effect of the non-negativity constraint on the fit quality and the weights.\n\nUse the following test suite. In all tests, use $n = 100$ samples at times $t_k = \\frac{k-1}{n-1}$ seconds for $k \\in \\{1,\\dots,n\\}$, and Gaussian basis functions\n$$\n\\phi_i(t) = \\exp\\left( - \\frac{(t - c_i)^2}{2\\sigma^2} \\right),\n$$\nwhich are dimensionless. For each case, $X_{k,i} = \\phi_i(t_k)$ and $y_k = y(t_k)$ with $y(t)$ specified per case.\n\n- Test Case $1$ (happy path, well-conditioned, positive target):\n  - $m = 3$\n  - Centers $c = [0.2, 0.5, 0.8]$ (seconds)\n  - $\\sigma = 0.08$ (seconds)\n  - Desired torque $y(t) = 1.5 \\sin^2(2\\pi t)$ in N·m.\n- Test Case $2$ (boundary case, nearly collinear basis, narrow target):\n  - $m = 3$\n  - Centers $c = [0.45, 0.5, 0.55]$ (seconds)\n  - $\\sigma = 0.03$ (seconds)\n  - Desired torque $y(t) = \\exp\\left( - \\frac{(t - 0.5)^2}{2 \\cdot 0.02^2} \\right)$ in N·m.\n- Test Case $3$ (edge case, sign-changing target with positive basis):\n  - $m = 3$\n  - Centers $c = [0.25, 0.5, 0.75]$ (seconds)\n  - $\\sigma = 0.06$ (seconds)\n  - Desired torque $y(t) = 0.8 \\sin(2\\pi t)$ in N·m.\n\nFor each test case, compute and return:\n- The root mean square error of the unconstrained solution $RMSE_{\\text{uncon}} = \\sqrt{ \\frac{1}{n} \\sum_{k=1}^n ( (Xw^{*})_k - y_k )^2 }$ in N·m.\n- The root mean square error of the non-negative solution $RMSE_{\\text{nn}} = \\sqrt{ \\frac{1}{n} \\sum_{k=1}^n ( (X\\tilde{w})_k - y_k )^2 }$ in N·m.\n- A boolean indicating whether the unconstrained solution has any negative weights, i.e., whether $\\exists i$ such that $w^{*}_i  0$.\n- The Euclidean norm difference $\\| w^{*} - \\tilde{w} \\|_2$ (dimensionless).\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-case results enclosed in square brackets, with each per-case result itself being a list in the order $[RMSE_{\\text{uncon}}, RMSE_{\\text{nn}}, \\text{has\\_neg}, \\| w^{*} - \\tilde{w} \\|_2]$. For example, the output format is $[[r_1,s_1,b_1,d_1],[r_2,s_2,b_2,d_2],[r_3,s_3,b_3,d_3]]$, where $r_i$, $s_i$, and $d_i$ are floats in N·m (for $r_i$ and $s_i$) and dimensionless (for $d_i$), and $b_i$ is a boolean. No rounding is required; report the raw numeric values produced by standard floating-point computation.",
            "solution": "The problem requires us to model a cerebellar motor learning task as a function approximation problem. We are to find optimal weights $w$ for a linear combination of basis functions $\\hat{y}(t) = \\sum_{i=1}^m w_i \\phi_i(t)$ that approximates a target torque trajectory $y(t)$. We will solve this problem under two conditions: first, with no constraints on the weights $w_i$, and second, with the biological constraint of non-negativity, $w_i \\ge 0$.\n\nLet's begin by formalizing the problem. The model is evaluated at $n$ discrete time points $t_k$. The relationship between the weights $w$, the basis functions $\\phi_i$, and the predicted torque $\\hat{y}$ can be expressed in matrix form as $\\hat{y} = Xw$. Here, $\\hat{y} \\in \\mathbb{R}^n$ is the vector of predicted torques, $w \\in \\mathbb{R}^m$ is the vector of synaptic weights, and $X \\in \\mathbb{R}^{n \\times m}$ is the design matrix, where its elements are $X_{k,i} = \\phi_i(t_k)$. The basis functions are given as Gaussians:\n$$\n\\phi_i(t) = \\exp\\left( - \\frac{(t - c_i)^2}{2\\sigma^2} \\right)\n$$\nwhere $c_i$ is the center and $\\sigma$ is the width. The time points are uniformly sampled over the interval $[0, 1]$ as $t_k = \\frac{k-1}{n-1}$ for $k \\in \\{1,\\dots,n\\}$ with $n=100$.\n\nThe objective is to find weights $w$ that minimize the discrepancy between the predicted torque $\\hat{y}$ and a target torque vector $y \\in \\mathbb{R}^n$. The error is quantified by the Mean Squared Error (MSE), $J(w) = \\frac{1}{n} \\|Xw - y\\|_2^2$.\n\n**1. Unconstrained Optimal Weights ($w^{*}$)**\n\nThe first task is to find the unconstrained weights $w^*$ that minimize $J(w)$. This is a standard Ordinary Least Squares (OLS) problem. To find the minimum, we set the gradient of the cost function with respect to $w$ to zero:\n$$\n\\nabla_w J(w) = \\nabla_w \\left( \\frac{1}{n} (Xw - y)^T(Xw - y) \\right) = \\frac{2}{n} X^T(Xw - y) = 0\n$$\nThis leads to the normal equations:\n$$\nX^T X w = X^T y\n$$\nAssuming that $X^T X$ is invertible, which is true for the given test cases as the basis functions are linearly independent, the unique solution $w^*$ is:\n$$\nw^* = (X^T X)^{-1} X^T y\n$$\nThe matrix $(X^T X)^{-1} X^T$ is the Moore-Penrose pseudoinverse of $X$, denoted $X^+$. Numerically, it is more stable to solve this system using methods based on singular value decomposition (SVD) or QR decomposition, rather than by direct inversion of $X^T X$. We will use a standard numerical linear algebra library function for this, such as `numpy.linalg.lstsq`.\n\n**2. Constrained Optimal Weights ($\\tilde{w}$)**\n\nThe second task introduces the biological constraint that synaptic weights cannot be negative, i.e., $w_i \\ge 0$ for all $i \\in \\{1, \\dots, m\\}$. The problem becomes a Non-Negative Least Squares (NNLS) optimization problem:\n$$\n\\text{minimize} \\quad \\|Xw - y\\|_2^2 \\\\\n\\text{subject to} \\quad w \\ge 0\n$$\nThis is a convex optimization problem, which guarantees a unique global minimum. It cannot be solved by the simple closed-form normal equations. Instead, iterative algorithms, such as active-set methods, are employed. We will use `scipy.optimize.nnls`, which implements such an algorithm.\n\n**3. Quantification of a Constraint's Effect**\n\nTo evaluate the impact of the non-negativity constraint, we will compute four metrics for each test case:\n\n- **Unconstrained Root Mean Square Error ($RMSE_{\\text{uncon}}$)**: This measures the goodness of fit for the unconstrained solution.\n  $$\n  RMSE_{\\text{uncon}} = \\sqrt{J(w^*)} = \\sqrt{\\frac{1}{n} \\|Xw^* - y\\|_2^2}\n  $$\n- **Non-Negative Root Mean Square Error ($RMSE_{\\text{nn}}$)**: This measures the goodness of fit for the constrained solution.\n  $$\n  RMSE_{\\text{nn}} = \\sqrt{J(\\tilde{w})} = \\sqrt{\\frac{1}{n} \\|X\\tilde{w} - y\\|_2^2}\n  $$\n  By definition, since the unconstrained solution minimizes the error over a larger set (all of $\\mathbb{R}^m$), we must have $RMSE_{\\text{uncon}} \\le RMSE_{\\text{nn}}$.\n\n- **Presence of Negative Weights**: A boolean flag, $\\exists i : w_i^*  0$, indicating whether the unconstrained solution $w^*$ would have violated the non-negativity constraint. If this is false, then $w^*$ is already a valid solution for the NNLS problem, which implies $w^* = \\tilde{w}$ and $RMSE_{\\text{uncon}} = RMSE_{\\text{nn}}$.\n\n- **Weight Vector Difference**: The Euclidean norm of the difference between the two weight vectors, $\\| w^{*} - \\tilde{w} \\|_2$. This quantifies how much the weight vector had to change to satisfy the constraint. The problem statement says this is dimensionless. From dimensional analysis, if $y$ is in N·m and $\\phi_i(t)$ is dimensionless, then the weights $w_i$ must have units of N·m. Thus, the norm of the difference vector should also be in N·m. Acknowledging this inconsistency in the problem statement, we will compute the numerical value as requested and report it as a dimensionless quantity per the instructions.\n\n**Computational Strategy**\n\nFor each test case, we will perform the following steps:\n1.  Define the parameters $m$, $c_i$, $\\sigma$, and the target function $y(t)$.\n2.  Generate the time vector $t$ of $n=100$ points from $0$ to $1$.\n3.  Construct the $n \\times m$ design matrix $X$ where $X_{k,i} = \\phi_i(t_k)$.\n4.  Construct the $n \\times 1$ target vector $y$ where $y_k = y(t_k)$.\n5.  Solve for the unconstrained weights $w^*$ using `numpy.linalg.lstsq(X, y)`.\n6.  Solve for the non-negative weights $\\tilde{w}$ using `scipy.optimize.nnls(X, y)`.\n7.  Calculate the four required metrics using the computed weights $w^*$ and $\\tilde{w}$ and the data $X$ and $y$.\n8.  Store the results for final formatted output.\n\nThis procedure will be applied to all three test cases to generate the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef solve():\n    \"\"\"\n    Main function to solve the cerebellar learning problem for all test cases.\n    \"\"\"\n\n    def compute_for_case(m, c_centers, sigma, y_func_str):\n        \"\"\"\n        Computes the required metrics for a single test case.\n\n        Args:\n            m (int): Number of basis functions.\n            c_centers (list): List of centers for the Gaussian basis functions.\n            sigma (float): Width of the Gaussian basis functions.\n            y_func_str (str): A string lambda expression for the target torque y(t).\n\n        Returns:\n            list: A list containing [RMSE_uncon, RMSE_nn, has_neg, norm_diff].\n        \"\"\"\n        n = 100\n        t = np.linspace(0, 1, n)\n\n        # Create design matrix X\n        # X_{k,i} = phi_i(t_k)\n        # Use broadcasting to efficiently compute the matrix\n        # t is (n, 1), c_centers is (1, m)\n        # The result of (t[:, np.newaxis] - c_centers) is an (n, m) matrix\n        c_centers_np = np.array(c_centers)\n        X = np.exp(-(t[:, np.newaxis] - c_centers_np[np.newaxis, :])**2 / (2 * sigma**2))\n\n        # Create target vector y\n        # The lambda function is created using eval for dynamic function definition.\n        # This is safe in this context as the strings are defined internally.\n        y_func = eval(y_func_str)\n        y = y_func(t)\n\n        # 1. Unconstrained solution (Ordinary Least Squares)\n        w_star, residuals_star, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        \n        # 2. Constrained solution (Non-Negative Least Squares)\n        w_tilde, _ = nnls(X, y)\n\n        # 3. Compute metrics\n        # RMSE for unconstrained solution\n        y_hat_star = X @ w_star\n        rmse_uncon = np.sqrt(np.mean((y_hat_star - y)**2))\n\n        # RMSE for non-negative solution\n        y_hat_tilde = X @ w_tilde\n        rmse_nn = np.sqrt(np.mean((y_hat_tilde - y)**2))\n\n        # Check if unconstrained solution has negative weights\n        has_neg = np.any(w_star  0)\n\n        # Euclidean norm difference between weight vectors\n        norm_diff = np.linalg.norm(w_star - w_tilde)\n\n        return [rmse_uncon, rmse_nn, bool(has_neg), norm_diff]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: m, centers, sigma, y(t)\n        (3, [0.2, 0.5, 0.8], 0.08, 'lambda t: 1.5 * np.sin(2 * np.pi * t)**2'),\n        # Case 2: m, centers, sigma, y(t)\n        (3, [0.45, 0.5, 0.55], 0.03, 'lambda t: np.exp(-(t - 0.5)**2 / (2 * 0.02**2))'),\n        # Case 3: m, centers, sigma, y(t)\n        (3, [0.25, 0.5, 0.75], 0.06, 'lambda t: 0.8 * np.sin(2 * np.pi * t)')\n    ]\n\n    results = []\n    for case in test_cases:\n        m_val, c_val, sigma_val, y_str = case\n        result = compute_for_case(m_val, c_val, sigma_val, y_str)\n        results.append(result)\n\n    # Format the final output as a string representation of a list of lists.\n    # Using a more robust method to format the output string to match the required format.\n    output_str = \"[\" + \",\".join([f\"[{r[0]}, {r[1]}, {str(r[2]).lower()}, {r[3]}]\" for r in results]) + \"]\"\n    print(output_str)\n\n# Using a try-except block to ensure the script runs within a secure context.\ntry:\n    solve()\nexcept Exception as e:\n    print(e)\n\n```"
        }
    ]
}