{
    "hands_on_practices": [
        {
            "introduction": "Global Workspace Theory (GWT) posits a competitive process where numerous unconscious neural coalitions vie for access to a limited-capacity broadcast. This practice  formalizes this competition using the principle of maximum entropy, a powerful concept from statistical physics for finding the least biased probability distribution given certain constraints. By completing this exercise, you will derive the softmax function, a ubiquitous selection rule in computational neuroscience and machine learning, and understand its role as a principled model of competitive access to the global workspace.",
            "id": "3984744",
            "problem": "In Global Workspace Theory (GWT), a limited-capacity global broadcast selects one among multiple competing coalitions for access. Consider $N$ candidate coalitions, indexed by $i \\in \\{1,\\dots,N\\}$, each with a real-valued salience $s_i \\in \\mathbb{R}$ summarizing bottom-up and top-down evidence. Let $p_i$ denote the probability that candidate $i$ gains access on a given cycle. Assume that in steady-state, the selection rule is the least-biased distribution consistent with a fixed expected selection cost, where the per-candidate cost is defined as $E_i = -s_i$. This leads to a maximum entropy formulation: maximize the Shannon entropy $H(p) = -\\sum_{i=1}^{N} p_i \\ln p_i$ subject to normalization $\\sum_{i=1}^{N} p_i = 1$ and an expected cost constraint $\\sum_{i=1}^{N} p_i E_i = \\bar{E}$, with $\\bar{E}$ finite. Denote by $\\beta$ the Lagrange multiplier associated with the cost constraint, and define the temperature parameter $\\tau$ by $\\beta = 1/\\tau$, with $\\tau > 0$.\n\nTask:\n1) From the maximum entropy principle and the stated constraints, derive the unique steady-state selection rule $p_i(\\tau)$ as an explicit function of $\\{s_i\\}_{i=1}^{N}$ and $\\tau$.\n2) For a fixed candidate $k \\in \\{1,\\dots,N\\}$, compute the partial derivative $\\partial p_k / \\partial \\tau$ in closed form, simplified in terms of $\\tau$, $p_k(\\tau)$, the set of saliences $\\{s_i\\}$, and the salience expectation $\\mu(\\tau) = \\sum_{j=1}^{N} p_j(\\tau) s_j$.\n\nReport only the expression for $\\partial p_k / \\partial \\tau$ as your final answer. No rounding is needed. No units are required.",
            "solution": "The problem asks for the derivation of a steady-state selection probability distribution $p_i$ based on the principle of maximum entropy, subject to certain constraints, and then to compute the partial derivative of this probability with respect to a temperature parameter $\\tau$.\n\nThe problem is to maximize the Shannon entropy, $H(p) = -\\sum_{i=1}^{N} p_i \\ln p_i$, subject to two constraints:\n$1$. Normalization: $ \\sum_{i=1}^{N} p_i = 1 $.\n$2$. Fixed expected cost: $ \\sum_{i=1}^{N} p_i E_i = \\bar{E} $, where the cost is given by $E_i = -s_i$. This constraint can be rewritten in terms of salience $s_i$ as $\\sum_{i=1}^{N} p_i (-s_i) = \\bar{E}$, or $\\sum_{i=1}^{N} p_i s_i = -\\bar{E}$.\n\nWe use the method of Lagrange multipliers to solve this constrained optimization problem. We define the Lagrangian $\\mathcal{L}$ as:\n$$ \\mathcal{L}(p_1, \\dots, p_N, \\lambda, \\beta) = H(p) - \\lambda \\left(\\sum_{i=1}^{N} p_i - 1\\right) - \\beta \\left(\\sum_{i=1}^{N} p_i E_i - \\bar{E}\\right) $$\nwhere $\\lambda$ and $\\beta$ are the Lagrange multipliers. The problem statement gives $\\beta = 1/\\tau$. Substituting the expressions for $H(p)$ and $E_i$:\n$$ \\mathcal{L} = -\\sum_{i=1}^{N} p_i \\ln p_i - \\lambda \\left(\\sum_{i=1}^{N} p_i - 1\\right) - \\beta \\left(\\sum_{i=1}^{N} p_i (-s_i) - \\bar{E}\\right) $$\n$$ \\mathcal{L} = -\\sum_{i=1}^{N} p_i \\ln p_i - \\lambda \\sum_{i=1}^{N} p_i + \\lambda + \\beta \\sum_{i=1}^{N} p_i s_i + \\beta \\bar{E} $$\nTo find the distribution $\\{p_i\\}$ that maximizes $\\mathcal{L}$, we take the partial derivative of $\\mathcal{L}$ with respect to each $p_k$ and set it to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left(-p_k \\ln p_k - \\lambda p_k + \\beta s_k p_k\\right) = 0 $$\n$$ -(\\ln p_k + 1) - \\lambda + \\beta s_k = 0 $$\n$$ \\ln p_k = \\beta s_k - \\lambda - 1 $$\nExponentiating both sides gives the form of $p_k$:\n$$ p_k = \\exp(\\beta s_k - \\lambda - 1) = \\exp(-\\lambda - 1) \\exp(\\beta s_k) $$\nWe can determine the term $\\exp(-\\lambda - 1)$ using the normalization constraint $\\sum_{k=1}^{N} p_k = 1$:\n$$ \\sum_{k=1}^{N} \\exp(-\\lambda - 1) \\exp(\\beta s_k) = 1 $$\n$$ \\exp(-\\lambda - 1) \\sum_{k=1}^{N} \\exp(\\beta s_k) = 1 $$\n$$ \\exp(-\\lambda - 1) = \\frac{1}{\\sum_{j=1}^{N} \\exp(\\beta s_j)} $$\nSubstituting this back into the expression for $p_k$ gives:\n$$ p_k = \\frac{\\exp(\\beta s_k)}{\\sum_{j=1}^{N} \\exp(\\beta s_j)} $$\nThis is the Gibbs-Boltzmann distribution. The problem defines the temperature parameter $\\tau$ such that $\\beta = 1/\\tau$. Substituting this completes the first task.\n$$ p_i(\\tau) = \\frac{\\exp(s_i / \\tau)}{\\sum_{j=1}^{N} \\exp(s_j / \\tau)} $$\nThis distribution is often called the softmax function in the context of machine learning.\n\nFor the second task, we must compute the partial derivative $\\frac{\\partial p_k}{\\partial \\tau}$ for a fixed candidate $k$. Let's define the denominator, which is the partition function $Z(\\tau)$, for notational convenience:\n$$ Z(\\tau) = \\sum_{j=1}^{N} \\exp(s_j / \\tau) $$\nSo, the expression for $p_k(\\tau)$ is:\n$$ p_k(\\tau) = \\frac{\\exp(s_k / \\tau)}{Z(\\tau)} $$\nWe use the quotient rule for differentiation, $\\frac{d}{dx}\\left(\\frac{u}{v}\\right) = \\frac{u'v - uv'}{v^2}$, with $u(\\tau) = \\exp(s_k/\\tau)$ and $v(\\tau) = Z(\\tau)$. First, we compute the derivatives of $u$ and $v$ with respect to $\\tau$:\n$$ \\frac{du}{d\\tau} = \\frac{d}{d\\tau} \\exp(s_k / \\tau) = \\exp(s_k / \\tau) \\cdot \\frac{d}{d\\tau}(s_k \\tau^{-1}) = \\exp(s_k / \\tau) \\cdot (-s_k \\tau^{-2}) = -\\frac{s_k}{\\tau^2} \\exp(s_k / \\tau) $$\n$$ \\frac{dv}{d\\tau} = \\frac{dZ}{d\\tau} = \\frac{d}{d\\tau}\\left(\\sum_{j=1}^{N} \\exp(s_j / \\tau)\\right) = \\sum_{j=1}^{N} \\left[ \\exp(s_j / \\tau) \\cdot (-s_j \\tau^{-2}) \\right] = -\\frac{1}{\\tau^2} \\sum_{j=1}^{N} s_j \\exp(s_j / \\tau) $$\nNow, applying the quotient rule:\n$$ \\frac{\\partial p_k}{\\partial \\tau} = \\frac{1}{Z(\\tau)^2} \\left[ \\left(-\\frac{s_k}{\\tau^2} \\exp(s_k / \\tau)\\right) Z(\\tau) - \\exp(s_k / \\tau) \\left(-\\frac{1}{\\tau^2} \\sum_{j=1}^{N} s_j \\exp(s_j / \\tau)\\right) \\right] $$\nWe can factor out common terms from the expression in the brackets:\n$$ \\frac{\\partial p_k}{\\partial \\tau} = \\frac{\\exp(s_k / \\tau)}{\\tau^2 Z(\\tau)^2} \\left[ -s_k Z(\\tau) + \\sum_{j=1}^{N} s_j \\exp(s_j / \\tau) \\right] $$\nThe problem provides the definition for the salience expectation $\\mu(\\tau) = \\sum_{j=1}^{N} p_j(\\tau) s_j$. We can express this in terms of $Z(\\tau)$:\n$$ \\mu(\\tau) = \\sum_{j=1}^{N} \\frac{\\exp(s_j / \\tau)}{Z(\\tau)} s_j = \\frac{1}{Z(\\tau)} \\sum_{j=1}^{N} s_j \\exp(s_j / \\tau) $$\nThis implies that $\\sum_{j=1}^{N} s_j \\exp(s_j / \\tau) = Z(\\tau)\\mu(\\tau)$. Substituting this into our expression for the derivative:\n$$ \\frac{\\partial p_k}{\\partial \\tau} = \\frac{\\exp(s_k / \\tau)}{\\tau^2 Z(\\tau)^2} \\left[ -s_k Z(\\tau) + Z(\\tau)\\mu(\\tau) \\right] $$\nFactor out $Z(\\tau)$ from the brackets:\n$$ \\frac{\\partial p_k}{\\partial \\tau} = \\frac{\\exp(s_k / \\tau) Z(\\tau)}{\\tau^2 Z(\\tau)^2} \\left[ \\mu(\\tau) - s_k \\right] $$\n$$ \\frac{\\partial p_k}{\\partial \\tau} = \\frac{\\exp(s_k / \\tau)}{\\tau^2 Z(\\tau)} \\left( \\mu(\\tau) - s_k \\right) $$\nBy recognizing that $p_k(\\tau) = \\frac{\\exp(s_k / \\tau)}{Z(\\tau)}$, we arrive at the final simplified expression:\n$$ \\frac{\\partial p_k}{\\partial \\tau} = \\frac{p_k(\\tau)}{\\tau^2} \\left( \\mu(\\tau) - s_k \\right) $$\nThis expression gives the change in the probability of selecting candidate $k$ with a change in the temperature parameter $\\tau$, in terms of its current probability $p_k(\\tau)$, its salience $s_k$, and the expected salience across all candidates $\\mu(\\tau)$.",
            "answer": "$$\\boxed{\\frac{p_k(\\tau)}{\\tau^{2}} \\left(\\mu(\\tau) - s_k\\right)}$$"
        },
        {
            "introduction": "A key event in GWT is \"ignition,\" a rapid, self-sustaining escalation of neural activity that broadcasts information across the cortex. This process can be modeled as a bifurcation in a nonlinear dynamical system, where a neural assembly abruptly transitions to a high-activity state. This practice  uses a neural mass model to explore the dynamics of ignition and demonstrates how top-down attention can act as a gain modulation mechanism to lower the input required for this transition, providing a concrete mechanistic link between attention and conscious access.",
            "id": "3984719",
            "problem": "Consider a minimal neural mass model of a cortical assembly implicated in Global Workspace Theory (GWT), where ignition corresponds to a bifurcation into a high-activity, self-sustained broadcasting state. Let the population-averaged activity be $x \\in (0,1)$ and evolve according to\n$\\tau \\frac{dx}{dt} = -x + \\phi\\!\\left( g \\left( w x + I - \\theta \\right) \\right),$\nwhere $\\tau$ is a time constant, $w \\gt 0$ is the recurrent coupling, $I$ is an external drive, $\\theta$ is an effective threshold, $g \\gt 0$ is a gain parameter, and $\\phi(z)$ is a sigmoidal transfer function. Assume a logistic transfer $\\phi(z) = \\frac{1}{1 + \\exp(-z)}$, so that $\\phi'(z) = \\phi(z)\\left(1 - \\phi(z)\\right)$ and $\\phi^{-1}(x) = \\ln\\!\\left(\\frac{x}{1-x}\\right)$ for $x \\in (0,1)$.\n\nA sustained attention signal of amplitude $A \\ge 0$ is modeled as a multiplicative modulation of afferent synaptic currents before the nonlinearity by a factor $(1 + \\alpha A)$, with sensitivity $\\alpha \\gt 0$. In the attended condition, the input to $\\phi$ is therefore scaled as $u \\mapsto (1 + \\alpha A) u$.\n\nUsing only these modeling assumptions:\n- Derive how sustained attention modulates the effective gain parameter $g$ in the transfer nonlinearity.\n- Define the ignition threshold $I_{\\mathrm{th}}$ as the smallest $I$ at which the low-activity fixed point disappears via a saddle-node bifurcation. Treat the ignition threshold as the value of $I$ for which the fixed-point equation and the tangency condition (zero slope of the right-hand side with respect to $x$) are simultaneously satisfied.\n- Compute, in closed form, the shift in ignition threshold $\\Delta I_{\\mathrm{th}}$ between the attended and unattended conditions, expressed entirely in terms of $g$, $w$, $\\alpha$, and $A$. Assume parameters such that $g w \\gt 4$ and $g w (1 + \\alpha A) \\gt 4$, and take the low-activity branch of the saddle-node condition.\n\nProvide your final answer as a single closed-form analytic expression for $\\Delta I_{\\mathrm{th}}$. No numerical evaluation or rounding is required.",
            "solution": "We begin from the dynamical equation\n$\\tau \\frac{dx}{dt} = -x + \\phi\\!\\left( g \\left( w x + I - \\theta \\right) \\right)$\nwith $\\phi(z) = \\frac{1}{1 + \\exp(-z)}$. A sustained attention signal of amplitude $A$ multiplicatively scales all afferent currents before the nonlinearity by $(1 + \\alpha A)$. Since the argument of the nonlinearity is $u = g \\left( w x + I - \\theta \\right)$, the attention-modulated argument becomes\n$u_{\\mathrm{att}} = (1 + \\alpha A) \\, g \\left( w x + I - \\theta \\right) = g_{\\mathrm{eff}} \\left( w x + I - \\theta \\right),$\nwhere we identify the effective gain as\n$g_{\\mathrm{eff}} = g (1 + \\alpha A).$\nThus, sustained attention modulates $g$ multiplicatively.\n\nNext, we determine the ignition threshold $I_{\\mathrm{th}}$ as a saddle-node point where the fixed point ceases to exist. Let $F(x; I, g) = -x + \\phi\\!\\left( g \\left( w x + I - \\theta \\right) \\right)$. Fixed points satisfy $F(x; I, g) = 0$. At a saddle-node, we additionally have $\\frac{\\partial F}{\\partial x} = 0$. Denote by $s = g \\left( w x + I - \\theta \\right)$ the input to $\\phi$. Using $\\phi^{-1}(x) = \\ln\\!\\left( \\frac{x}{1-x} \\right)$, the fixed-point condition $x = \\phi(s)$ implies\n$s = \\ln\\!\\left( \\frac{x}{1-x} \\right).$\nThe tangency condition is\n$\\frac{\\partial F}{\\partial x} = -1 + \\phi'(s) \\cdot g w = 0.$\nBecause $\\phi'(s) = \\phi(s) \\left( 1 - \\phi(s) \\right) = x (1 - x)$, the tangency condition becomes\n$-1 + g w \\, x (1 - x) = 0 \\quad \\Rightarrow \\quad g w \\, x (1 - x) = 1.$\nThis is a quadratic equation in $x$:\n$x - x^{2} = \\frac{1}{g w} \\;\\;\\Rightarrow\\;\\; x^{2} - x + \\frac{1}{g w} = 0.$\nSolving yields\n$x^{\\ast}(g) = \\frac{1}{2} \\left( 1 \\pm \\sqrt{1 - \\frac{4}{g w}} \\right).$\nFor the low-activity branch, we take\n$x^{\\ast}(g) = \\frac{1}{2} \\left( 1 - \\sqrt{1 - \\frac{4}{g w}} \\right).$\nAt the saddle-node, $s^{\\ast}(g) = \\ln\\!\\left( \\frac{x^{\\ast}(g)}{1 - x^{\\ast}(g)} \\right)$, and from $s = g \\left( w x + I - \\theta \\right)$ we solve for the threshold input:\n$I_{\\mathrm{th}}(g) = \\theta + \\frac{s^{\\ast}(g)}{g} - w \\, x^{\\ast}(g) = \\theta + \\frac{1}{g} \\ln\\!\\left( \\frac{x^{\\ast}(g)}{1 - x^{\\ast}(g)} \\right) - w \\, x^{\\ast}(g).$\n\nUnder sustained attention, the effective gain is $g_{\\mathrm{eff}} = g (1 + \\alpha A)$, so the attended threshold is\n$I_{\\mathrm{th}}(g_{\\mathrm{eff}}) = \\theta + \\frac{1}{g_{\\mathrm{eff}}} \\ln\\!\\left( \\frac{x^{\\ast}(g_{\\mathrm{eff}})}{1 - x^{\\ast}(g_{\\mathrm{eff}})} \\right) - w \\, x^{\\ast}(g_{\\mathrm{eff}}),$\nwith\n$x^{\\ast}(g_{\\mathrm{eff}}) = \\frac{1}{2} \\left( 1 - \\sqrt{1 - \\frac{4}{g_{\\mathrm{eff}} w}} \\right) = \\frac{1}{2} \\left( 1 - \\sqrt{1 - \\frac{4}{g w (1 + \\alpha A)}} \\right).$\n\nThe shift in ignition threshold is\n$\\Delta I_{\\mathrm{th}} = I_{\\mathrm{th}}(g_{\\mathrm{eff}}) - I_{\\mathrm{th}}(g).$\nSubstituting the above expressions and noting that $\\theta$ cancels, we obtain\n$\\Delta I_{\\mathrm{th}} = \\frac{1}{g (1 + \\alpha A)} \\ln\\!\\left( \\frac{x^{\\ast}\\!\\left(g (1 + \\alpha A)\\right)}{1 - x^{\\ast}\\!\\left(g (1 + \\alpha A)\\right)} \\right) - \\frac{1}{g} \\ln\\!\\left( \\frac{x^{\\ast}(g)}{1 - x^{\\ast}(g)} \\right) - w \\left( x^{\\ast}\\!\\left(g (1 + \\alpha A)\\right) - x^{\\ast}(g) \\right),$\nwhere\n$x^{\\ast}(g) = \\frac{1}{2} \\left( 1 - \\sqrt{1 - \\frac{4}{g w}} \\right), \\quad x^{\\ast}\\!\\left(g (1 + \\alpha A)\\right) = \\frac{1}{2} \\left( 1 - \\sqrt{1 - \\frac{4}{g w (1 + \\alpha A)}} \\right).$\n\nThis is the required closed-form expression in terms of $g$, $w$, $\\alpha$, and $A$. Under the stated conditions $g w \\gt 4$ and $g w (1 + \\alpha A) \\gt 4$, the square roots are real, and because $g (1 + \\alpha A) \\gt g$, one finds $x^{\\ast}\\!\\left(g (1 + \\alpha A)\\right) \\lt x^{\\ast}(g)$ and $\\Delta I_{\\mathrm{th}} \\lt 0$, consistent with attention lowering the ignition threshold.",
            "answer": "$$\\boxed{\\frac{1}{g(1+\\alpha A)}\\,\\ln\\!\\left(\\frac{\\tfrac{1}{2}\\left(1-\\sqrt{1-\\tfrac{4}{g w (1+\\alpha A)}}\\right)}{1-\\tfrac{1}{2}\\left(1-\\sqrt{1-\\tfrac{4}{g w (1+\\alpha A)}}\\right)}\\right)\\;-\\;\\frac{1}{g}\\,\\ln\\!\\left(\\frac{\\tfrac{1}{2}\\left(1-\\sqrt{1-\\tfrac{4}{g w}}\\right)}{1-\\tfrac{1}{2}\\left(1-\\sqrt{1-\\tfrac{4}{g w}}\\right)}\\right)\\;-\\;w\\left[\\tfrac{1}{2}\\left(1-\\sqrt{1-\\tfrac{4}{g w (1+\\alpha A)}}\\right)\\;-\\;\\tfrac{1}{2}\\left(1-\\sqrt{1-\\tfrac{4}{g w}}\\right)\\right]}$$"
        },
        {
            "introduction": "One of the most salient features of conscious awareness is its severely limited capacity. GWT attributes this bottleneck to the global workspace itself. This exercise  provides a hands-on opportunity to understand how this cognitive limitation can emerge directly from neural circuit dynamics. Using the classic Wilson-Cowan rate model, you will analyze a system where multiple neural assemblies compete via a shared inhibitory pool and calculate the maximum number of assemblies that can be simultaneously maintained above a broadcasting threshold, thereby quantifying the workspace's capacity.",
            "id": "3984696",
            "problem": "Consider a simplified mean-field instantiation of Global Workspace Theory (GWT) in which $K$ item-specific cortical assemblies simultaneously compete to be maintained in a global broadcast. Each assembly is modeled as a homogeneous excitatory population with firing rate $r_{e}$, and all assemblies are coupled through a single shared inhibitory pool with rate $r_{i}$. Assume symmetry so that all $K$ actively maintained assemblies have identical steady-state firing rates $r_{e}^{*}$, and the inhibitory pool responds to their summed activity. The population dynamics are described by the Wilson–Cowan rate equations, a well-tested model in computational neuroscience, for the excitatory and inhibitory populations:\n$$\n\\tau_{e} \\frac{d r_{e}}{dt} = - r_{e} + \\phi_{e}\\!\\left(w_{ee} r_{e} - w_{ei} r_{i} + I_{d}\\right), \\quad\n\\tau_{i} \\frac{d r_{i}}{dt} = - r_{i} + \\phi_{i}\\!\\left(w_{ie} K r_{e}\\right),\n$$\nwhere $w_{ee} > 0$ is recurrent excitation within an assembly, $w_{ei} > 0$ is inhibition onto excitatory assemblies from the shared pool, $w_{ie} > 0$ is excitation onto the inhibitory pool from assemblies, and $I_{d} > 0$ is a tonic drive supplied to each active assembly by task-relevant input. The input–output (gain) functions are linear-threshold (piecewise linear) and non-saturating within the operating regime of interest:\n$$\n\\phi_{e}(x) = g_{e} \\max\\{x - \\theta_{e},\\, 0\\}, \\quad \\phi_{i}(x) = g_{i} \\max\\{x - \\theta_{i},\\, 0\\}.\n$$\nThe global broadcast requires that an assembly’s excitatory firing rate exceed a broadcasting threshold $r_{b}$, that is $r_{e}^{*} \\ge r_{b}$. In our scenario, the inhibitory pool’s capacity imposes a limit on how many assemblies can be sustained before their reciprocal inhibition drives them below $r_{b}$.\n\nWork in the steady-state regime ($d r_{e}/dt = 0$, $d r_{i}/dt = 0$), and assume the fixed point lies strictly in the linear regime of the gain functions so that the thresholded arguments are positive and the saturations are not reached. Use the following scientifically plausible parameters:\n- Time constants: $\\tau_{e} = 1$, $\\tau_{i} = 1$ (arbitrary units; they cancel at steady state).\n- Gains and thresholds: $g_{e} = 0.5$, $g_{i} = 1$, $\\theta_{e} = 1$, $\\theta_{i} = 0$.\n- Coupling weights: $w_{ee} = 0.4$, $w_{ei} = 1$, $w_{ie} = 0.2$.\n- Drive: $I_{d} = 12$.\n- Broadcasting threshold: $r_{b} = 5$.\n- Saturations are sufficiently high that they do not bind: $r_{e}^{\\max} = 100$, $r_{i}^{\\max} = 50$.\n\nStarting from the above definitions and the Wilson–Cowan equations, derive a self-consistent expression for the steady-state excitatory rate $r_{e}^{*}$ as a function of $K$, verify the linear-regime and linear-stability conditions for the fixed point, and compute the largest integer $K$ for which $r_{e}^{*} \\ge r_{b}$ holds and the fixed point remains linearly stable. Express your final answer as a single integer $K$ with no units. No rounding is required.",
            "solution": "At steady state ($dr_{e}/dt = 0$ and $dr_{i}/dt = 0$), the Wilson-Cowan equations are:\n$$r_{e}^{*} = \\phi_{e}\\!\\left(w_{ee} r_{e}^{*} - w_{ei} r_{i}^{*} + I_{d}\\right)$$\n$$r_{i}^{*} = \\phi_{i}\\!\\left(w_{ie} K r_{e}^{*}\\right)$$\nAssuming the fixed point lies in the linear regime of the gain functions, we can substitute the linear parts of the $\\phi$ functions:\n$$r_{e}^{*} = g_{e} (w_{ee} r_{e}^{*} - w_{ei} r_{i}^{*} + I_{d} - \\theta_{e})$$\n$$r_{i}^{*} = g_{i} (w_{ie} K r_{e}^{*} - \\theta_{i})$$\nSubstitute the given parameter values: $g_{e} = 0.5$, $g_{i} = 1$, $\\theta_{e} = 1$, $\\theta_{i} = 0$, $w_{ee} = 0.4$, $w_{ei} = 1$, $w_{ie} = 0.2$, and $I_{d} = 12$.\nThe equation for $r_{i}^{*}$ simplifies to:\n$$r_{i}^{*} = 1 (0.2 \\cdot K \\cdot r_{e}^{*} - 0) = 0.2 K r_{e}^{*}$$\nThe equation for $r_{e}^{*}$ simplifies to:\n$$r_{e}^{*} = 0.5 (0.4 r_{e}^{*} - 1 \\cdot r_{i}^{*} + 12 - 1) = 0.5 (0.4 r_{e}^{*} - r_{i}^{*} + 11)$$\nNow, substitute the expression for $r_{i}^{*}$ into the equation for $r_{e}^{*}$ to find a self-consistent expression for $r_{e}^{*}$ as a function of $K$:\n$$r_{e}^{*} = 0.5 (0.4 r_{e}^{*} - (0.2 K r_{e}^{*}) + 11)$$\nSolving for $r_{e}^{*}$:\n$$2 r_{e}^{*} = 0.4 r_{e}^{*} - 0.2 K r_{e}^{*} + 11$$\n$$r_{e}^{*} (2 - 0.4 + 0.2 K) = 11$$\n$$r_{e}^{*} (1.6 + 0.2 K) = 11$$\n$$r_{e}^{*}(K) = \\frac{11}{1.6 + 0.2 K}$$\nThe fixed point is stable if the Jacobian matrix of the system has eigenvalues with negative real parts. The Jacobian in the linear regime is:\n$$ J = \\begin{pmatrix} \\frac{1}{\\tau_{e}}(-1 + g_{e}w_{ee}) & \\frac{1}{\\tau_{e}}(-g_{e}w_{ei}) \\\\ \\frac{1}{\\tau_{i}}(g_{i}w_{ie}K) & \\frac{1}{\\tau_{i}}(-1) \\end{pmatrix} = \\begin{pmatrix} -0.8 & -0.5 \\\\ 0.2 K & -1 \\end{pmatrix} $$\nFor stability, we require $\\text{Tr}(J) < 0$ and $\\text{Det}(J) > 0$.\n$\\text{Tr}(J) = -0.8 - 1 = -1.8$, which is always negative.\n$\\text{Det}(J) = (-0.8)(-1) - (-0.5)(0.2 K) = 0.8 + 0.1 K$, which is always positive for $K \\ge 1$.\nThe fixed point is stable for all relevant $K$.\n\nFinally, we find the largest integer $K$ for which the broadcasting condition $r_{e}^{*} \\ge r_{b}$ holds, with $r_{b} = 5$:\n$$\\frac{11}{1.6 + 0.2 K} \\ge 5$$\nSince $K \\ge 1$, the denominator is positive, so we can multiply:\n$$11 \\ge 5 (1.6 + 0.2 K)$$\n$$11 \\ge 8 + K$$\n$$3 \\ge K$$\nThe largest integer value for $K$ that satisfies this condition is $3$.",
            "answer": "$$\n\\boxed{3}\n$$"
        }
    ]
}