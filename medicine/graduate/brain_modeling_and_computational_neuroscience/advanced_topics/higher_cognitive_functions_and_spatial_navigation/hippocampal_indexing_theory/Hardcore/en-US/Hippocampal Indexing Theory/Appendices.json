{
    "hands_on_practices": [
        {
            "introduction": "The core postulate of Hippocampal Indexing Theory is that a sparse code in the hippocampus acts as a pointer to a distributed pattern of activity in the neocortex. This binding is thought to be instantiated by Hebbian plasticity at the synapses connecting the two regions. This first exercise provides a foundational analysis of this mechanism, asking you to derive how the total synaptic strength across the network grows as new memories are encoded . By calculating the expected magnitude of the connection matrix, you will gain a quantitative understanding of the resource costs associated with storing an ever-increasing number of episodic memories.",
            "id": "3988866",
            "problem": "Consider a simplified hippocampal indexing theory model in which a population of $N_{H}$ hippocampal indexing units binds to a distributed cortical population of $N_{C}$ feature units. Each episode $p \\in \\{1,\\dots,P\\}$ is represented by a hippocampal index vector $h^{(p)} \\in \\{0,1\\}^{N_{H}}$ and a cortical pattern vector $x^{(p)} \\in \\{0,1\\}^{N_{C}}$. Assume the following foundational modeling assumptions, based on the Hebbian postulate and sparse coding observations in cortex and hippocampus:\n\n- For each episode $p$, each component $h_{i}^{(p)}$ and $x_{j}^{(p)}$ is an independent Bernoulli random variable equal to $1$ with probability $a \\in (0,1)$ and $0$ otherwise, independently across indices $i$, $j$, and episodes $p$.\n- Synaptic weights are initialized at $0$ and updated once per episode by a Hebbian outer-product rule that binds the active hippocampal index to the concurrently active cortical features:\n$$\n\\Delta W_{CH} \\;=\\; \\eta \\, h^{(p)} \\big(x^{(p)}\\big)^{\\top} ,\n$$\nwhere $W_{CH} \\in \\mathbb{R}^{N_{H} \\times N_{C}}$ and $\\eta > 0$ is a learning rate. Hippocampal-to-cortical reinstatement at recall may be produced by $x_{\\mathrm{reinst}} \\,=\\, W_{CH}^{\\top} h$, which is consistent with $W_{CH}$ storing hippocampal-to-cortical bindings via its transpose.\n\nStarting from these assumptions alone, derive an analytic expression for the expected squared Frobenius norm of the weight matrix after storing $P$ statistically independent episodes:\n$$\n\\mathbb{E}\\!\\left[\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2}\\right] ,\n$$\nas a function of $N_{H}$, $N_{C}$, $\\eta$, $P$, and $a$. Express your final answer in closed form. No numerical approximation or rounding is required.",
            "solution": "The objective is to compute the expected squared Frobenius norm of the weight matrix $W_{CH}^{(P)}$ after $P$ episodes. The weight matrix is given by the accumulation of updates from an initial state of $W_{CH}^{(0)} = 0$:\n$$\nW_{CH}^{(P)} = \\sum_{p=1}^{P} \\eta \\, h^{(p)} (x^{(p)})^{\\top}\n$$\nwhere $\\eta$ is the learning rate, $h^{(p)} \\in \\{0,1\\}^{N_H}$ is the hippocampal index vector, and $x^{(p)} \\in \\{0,1\\}^{N_C}$ is the cortical pattern vector for episode $p$. The components of the weight matrix are:\n$$\nW_{CH, ij}^{(P)} = \\eta \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)}\n$$\nfor $i \\in \\{1,\\dots,N_H\\}$ and $j \\in \\{1,\\dots,N_C\\}$.\n\nThe squared Frobenius norm of $W_{CH}^{(P)}$ is the sum of the squares of its elements:\n$$\n\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2} = \\sum_{i=1}^{N_H} \\sum_{j=1}^{N_C} \\left( W_{CH, ij}^{(P)} \\right)^2\n$$\nWe are asked to find the expectation $\\mathbb{E}\\!\\left[\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2}\\right]$. Using the linearity of the expectation operator, we can write:\n$$\n\\mathbb{E}\\!\\left[\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2}\\right] = \\mathbb{E}\\!\\left[ \\sum_{i=1}^{N_H} \\sum_{j=1}^{N_C} \\left( W_{CH, ij}^{(P)} \\right)^2 \\right] = \\sum_{i=1}^{N_H} \\sum_{j=1}^{N_C} \\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right]\n$$\nSince the vector components $h_i^{(p)}$ and $x_j^{(p)}$ are independent and identically distributed (i.i.d.) Bernoulli random variables for all $i, j, p$, the expectation $\\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right]$ is constant for all $i, j$. We can therefore simplify the sum:\n$$\n\\mathbb{E}\\!\\left[\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2}\\right] = N_H N_C \\, \\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right]\n$$\nNow, let's compute the expectation of a single squared weight element:\n$$\n\\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right] = \\mathbb{E}\\!\\left[ \\left( \\eta \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right)^2 \\right] = \\eta^2 \\, \\mathbb{E}\\!\\left[ \\left( \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right)^2 \\right]\n$$\nExpanding the square of the sum:\n$$\n\\left( \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right)^2 = \\left( \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right) \\left( \\sum_{q=1}^{P} h_i^{(q)} x_j^{(q)} \\right) = \\sum_{p=1}^{P} \\sum_{q=1}^{P} h_i^{(p)} x_j^{(p)} h_i^{(q)} x_j^{(q)}\n$$\nTaking the expectation gives:\n$$\n\\mathbb{E}\\!\\left[ \\left( \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right)^2 \\right] = \\sum_{p=1}^{P} \\sum_{q=1}^{P} \\mathbb{E}\\!\\left[ h_i^{(p)} h_i^{(q)} x_j^{(p)} x_j^{(q)} \\right]\n$$\nWe can split the double summation into two cases: diagonal terms where $p=q$, and off-diagonal terms where $p \\neq q$.\n\nCase 1: $p=q$.\nThe terms are of the form $\\mathbb{E}\\!\\left[ h_i^{(p)} h_i^{(p)} x_j^{(p)} x_j^{(p)} \\right] = \\mathbb{E}\\!\\left[ (h_i^{(p)})^2 (x_j^{(p)})^2 \\right]$.\nThe variables $h_i^{(p)}$ and $x_j^{(p)}$ are Bernoulli random variables, taking values in $\\{0,1\\}$. Therefore, $(h_i^{(p)})^2 = h_i^{(p)}$ and $(x_j^{(p)})^2 = x_j^{(p)}$. The expectation becomes $\\mathbb{E}\\!\\left[ h_i^{(p)} x_j^{(p)} \\right]$.\nSince $h_i^{(p)}$ and $x_j^{(p)}$ are independent, $\\mathbb{E}\\!\\left[ h_i^{(p)} x_j^{(p)} \\right] = \\mathbb{E}\\!\\left[ h_i^{(p)} \\right] \\mathbb{E}\\!\\left[ x_j^{(p)} \\right]$.\nGiven that $P(h_i^{(p)}=1) = a$ and $P(x_j^{(p)}=1) = a$, their expectations are both $a$.\nSo, for $p=q$, the expectation is $a \\cdot a = a^2$. There are $P$ such diagonal terms.\n\nCase 2: $p \\neq q$.\nThe terms are of the form $\\mathbb{E}\\!\\left[ h_i^{(p)} h_i^{(q)} x_j^{(p)} x_j^{(q)} \\right]$.\nThe problem states that all random variables are independent across indices $i, j,$ and episodes $p$. Therefore, all four random variables in the expectation are mutually independent.\n$$\n\\mathbb{E}\\!\\left[ h_i^{(p)} h_i^{(q)} x_j^{(p)} x_j^{(q)} \\right] = \\mathbb{E}\\!\\left[ h_i^{(p)} \\right] \\mathbb{E}\\!\\left[ h_i^{(q)} \\right] \\mathbb{E}\\!\\left[ x_j^{(p)} \\right] \\mathbb{E}\\!\\left[ x_j^{(q)} \\right]\n$$\nEach expectation is equal to $a$. So, the total expectation is $a \\cdot a \\cdot a \\cdot a = a^4$.\nThere are $P^2 - P = P(P-1)$ off-diagonal terms where $p \\neq q$.\n\nCombining these two cases, we get:\n$$\n\\mathbb{E}\\!\\left[ \\left( \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right)^2 \\right] = \\sum_{p=q} a^2 + \\sum_{p \\neq q} a^4 = P \\cdot a^2 + P(P-1) \\cdot a^4\n$$\nSubstituting this back into the expression for $\\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right]$:\n$$\n\\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right] = \\eta^2 \\left[ P a^2 + P(P-1) a^4 \\right]\n$$\nFinally, we multiply by $N_H N_C$ to obtain the expected squared Frobenius norm:\n$$\n\\mathbb{E}\\!\\left[\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2}\\right] = N_H N_C \\eta^2 \\left[ P a^2 + P(P-1) a^4 \\right]\n$$",
            "answer": "$$\n\\boxed{N_{H} N_{C} \\eta^{2} \\left[ P a^{2} + P(P-1) a^{4} \\right]}\n$$"
        },
        {
            "introduction": "To be effective, an index must be unique to the episode it points to, which includes the spatial and situational context. This practice connects the abstract concept of an index to its biological substrate in the medial entorhinal cortex, modeling it as a readout of grid cell populations. You will investigate how the brain can generate distinct indices for different contexts through a process called 'global remapping'—a coordinated shift in grid cell firing phases—by deriving the optimal phase shift that minimizes interference between contextual representations . This exercise beautifully illustrates the principle of orthogonalization for creating separable memory codes.",
            "id": "3988822",
            "problem": "Consider a formalization of hippocampal indexing theory in which the hippocampus stores an index code for episodic bindings by linearly reading out a spatial basis constructed from medial entorhinal cortex (MEC) grid cells. Let the one-dimensional environment coordinate be $s \\in [0,L]$ and assume $M$ independent grid modules indexed by $j \\in \\{1,\\dots,M\\}$. The grid response of module $j$ at position $s$ is given by $g_{j}(s;\\phi_{j}) = \\cos(k_{j} s + \\phi_{j})$, where $k_{j} = \\frac{2\\pi}{\\lambda_{j}}$ and $\\lambda_{j} > 0$ is the grid period of module $j$, and $\\phi_{j} \\in [0,2\\pi)$ is its phase. Stack the module responses into the $M$-dimensional grid code $g(s;\\phi) = \\big(g_{1}(s;\\phi_{1}),\\dots,g_{M}(s;\\phi_{M})\\big)^{\\top}$. The hippocampal index code for context $c$ is $h(s;c) = W g(s;\\phi^{(c)})$, where $W \\in \\mathbb{R}^{N \\times M}$ is a fixed readout with full column rank and $\\phi^{(c)}$ is the phase vector for context $c$.\n\nDefine global remapping between two contexts $c$ and $c'$ as a uniform phase offset applied to all modules: there exists a single $\\delta \\in [0,2\\pi)$ such that $\\phi^{(c')}_{j} = \\phi^{(c)}_{j} + \\delta$ for all $j$. To quantify cross-context interference in the index code while abstracting away the readout $W$, consider the interference functional\n$$\nJ(\\delta) = \\frac{1}{M} \\sum_{j=1}^{M} \\left( \\frac{1}{L} \\int_{0}^{L} g_{j}\\big(s;\\phi^{(c)}_{j}\\big)\\, g_{j}\\big(s;\\phi^{(c)}_{j}+\\delta\\big)\\, \\mathrm{d}s \\right)^{2}.\n$$\nAssume the following foundational conditions:\n- $L$ is an integer multiple of every $\\lambda_{j}$, or $s$ is uniformly sampled over $[0,L]$ with $L$ sufficiently large to average over many fundamental periods.\n- The grid periods $\\{\\lambda_{j}\\}_{j=1}^{M}$ are pairwise incommensurate.\n- The readout $W$ has full column rank and does not depend on context.\n\nStarting from these assumptions and the definition of $J(\\delta)$, derive a closed-form expression for $J(\\delta)$ and determine the value of the global phase offset $\\delta^{\\star}$ that minimizes $J(\\delta)$ under the constraint that distinct episodic bindings within each context are preserved in the sense that the map $s \\mapsto h(s;c)$ remains injective almost everywhere. Provide the final answer as the value of $\\delta^{\\star}$ in radians. No rounding is required. Express the angle in radians.",
            "solution": "First, we derive the closed-form expression for $J(\\delta)$. The functional is defined as:\n$$\nJ(\\delta) = \\frac{1}{M} \\sum_{j=1}^{M} \\left( \\frac{1}{L} \\int_{0}^{L} g_{j}\\big(s;\\phi^{(c)}_{j}\\big)\\, g_{j}\\big(s;\\phi^{(c)}_{j}+\\delta\\big)\\, \\mathrm{d}s \\right)^{2}\n$$\nLet's analyze the integral term for a single module $j$, which we denote by $I_j(\\delta)$:\n$$\nI_j(\\delta) = \\frac{1}{L} \\int_{0}^{L} g_{j}\\big(s;\\phi^{(c)}_{j}\\big)\\, g_{j}\\big(s;\\phi^{(c)}_{j}+\\delta\\big)\\, \\mathrm{d}s\n$$\nSubstituting the definition of $g_j(s;\\phi)$, we have:\n$$\nI_j(\\delta) = \\frac{1}{L} \\int_{0}^{L} \\cos(k_{j} s + \\phi^{(c)}_{j}) \\cos(k_{j} s + \\phi^{(c)}_{j} + \\delta) \\, \\mathrm{d}s\n$$\nWe use the product-to-sum trigonometric identity $\\cos(A)\\cos(B) = \\frac{1}{2}(\\cos(A-B) + \\cos(A+B))$.\nLet $A = k_{j} s + \\phi^{(c)}_{j} + \\delta$ and $B = k_{j} s + \\phi^{(c)}_{j}$. Then $A-B = \\delta$ and $A+B = 2k_{j} s + 2\\phi^{(c)}_{j} + \\delta$.\nThe integral becomes:\n$$\nI_j(\\delta) = \\frac{1}{L} \\int_{0}^{L} \\frac{1}{2} \\left[ \\cos(\\delta) + \\cos(2k_{j} s + 2\\phi^{(c)}_{j} + \\delta) \\right] \\mathrm{d}s\n$$\nWe can split the integral into two parts:\n$$\nI_j(\\delta) = \\frac{1}{2L} \\left[ \\int_{0}^{L} \\cos(\\delta) \\, \\mathrm{d}s + \\int_{0}^{L} \\cos(2k_{j} s + 2\\phi^{(c)}_{j} + \\delta) \\, \\mathrm{d}s \\right]\n$$\nFor the first integral, $\\cos(\\delta)$ is a constant with respect to $s$, so $\\int_{0}^{L} \\cos(\\delta) \\, \\mathrm{d}s = L \\cos(\\delta)$.\nFor the second integral, we have:\n$$\n\\int_{0}^{L} \\cos(2k_{j} s + 2\\phi^{(c)}_{j} + \\delta) \\, \\mathrm{d}s = \\left[ \\frac{\\sin(2k_{j} s + 2\\phi^{(c)}_{j} + \\delta)}{2k_{j}} \\right]_{0}^{L}\n$$\nThis evaluates to:\n$$\n\\frac{1}{2k_{j}} \\left[ \\sin(2k_{j}L + 2\\phi^{(c)}_{j} + \\delta) - \\sin(2\\phi^{(c)}_{j} + \\delta) \\right]\n$$\nUsing the foundational condition that $L$ is an integer multiple of every period $\\lambda_j$ (i.e., $L = n_j \\lambda_j$), we have $k_j L = (2\\pi/\\lambda_j) (n_j \\lambda_j) = 2\\pi n_j$. Thus, $2k_j L$ is a multiple of $2\\pi$, and since the sine function has a period of $2\\pi$, the integral evaluates to $0$. The same result holds under the alternative assumption of a sufficiently large $L$, as the integral of an oscillating function over many periods averages to zero.\nThus, the second integral vanishes, and we are left with:\n$$\nI_j(\\delta) = \\frac{1}{2L} [L \\cos(\\delta) + 0] = \\frac{1}{2} \\cos(\\delta)\n$$\nThis result is independent of the module index $j$. Now we substitute this back into the expression for $J(\\delta)$:\n$$\nJ(\\delta) = \\frac{1}{M} \\sum_{j=1}^{M} \\left( \\frac{1}{2} \\cos(\\delta) \\right)^{2} = \\frac{1}{M} \\sum_{j=1}^{M} \\frac{1}{4} \\cos^2(\\delta)\n$$\nSince the term in the sum is constant, the sum is $M$ times the term:\n$$\nJ(\\delta) = \\frac{1}{M} \\cdot M \\cdot \\frac{1}{4} \\cos^2(\\delta) = \\frac{1}{4} \\cos^2(\\delta)\n$$\nNext, we must find the value $\\delta^{\\star}$ that minimizes $J(\\delta)$. This is equivalent to minimizing $\\cos^2(\\delta)$ for $\\delta \\in [0, 2\\pi)$. The function $\\cos^2(\\delta)$ is non-negative, and its minimum value is $0$. This minimum occurs when $\\cos(\\delta) = 0$. The values of $\\delta$ in the interval $[0, 2\\pi)$ that satisfy this condition are $\\delta = \\frac{\\pi}{2}$ and $\\delta = \\frac{3\\pi}{2}$.\n\nFinally, we consider the constraint that the map $s \\mapsto h(s;c)$ must remain injective almost everywhere. The injectivity of this map is guaranteed by the foundational condition that the grid periods $\\{\\lambda_j\\}$ are pairwise incommensurate. A global phase offset $\\delta$ shifts the phase of all modules equally but does not alter their periods or incommensurability. Therefore, the geometric properties of the grid code trajectory that ensure injectivity are preserved for any value of $\\delta$. The constraint is satisfied by all possible values of $\\delta$, including those that minimize $J(\\delta)$.\n\nChoosing the smallest positive angle that minimizes the functional, we select $\\delta^{\\star} = \\frac{\\pi}{2}$.",
            "answer": "$$\n\\boxed{\\frac{\\pi}{2}}\n$$"
        },
        {
            "introduction": "Memory retrieval is rarely a perfect process; real-world cues are often noisy, incomplete, or ambiguous, potentially activating multiple memory traces. This final practice moves from encoding to retrieval, challenging you to build a computational model that can handle such uncertainty. You will implement a Bayesian probabilistic data association framework to determine which memory is the most likely target given a set of cues that may originate from multiple episodes or from background 'clutter' . This hands-on coding exercise provides a powerful demonstration of how the brain might use probabilistic inference to robustly navigate the vast landscape of stored memories.",
            "id": "3988799",
            "problem": "You are modeling hippocampal index retrieval under the hippocampal indexing theory using a Bayesian probabilistic data association approach. In this abstraction, a set of observed cues is assumed to be generated from a mixture of latent sources: a finite set of episodic indices and a clutter source. The goal is to compute, from first principles, the probabilistic association of each cue to each episode (and to clutter), and to compute the posterior probability over which single episode is the retrieval target given all cues when each cue may arise from multiple episodes with known mixture weights.\n\nAssume the following generative base, grounded in Bayes' rule, the law of total probability, and conditional independence assumptions:\n\n- There are $K$ episodes indexed by $k \\in \\{1,\\dots,K\\}$ and a clutter source indexed by $k=0$.\n- The prior mixture weight of clutter is $w_0 \\in [0,1]$, and the prior mixture weights over episodes are $\\{w_k\\}_{k=1}^K$ with $w_k \\in [0,1]$ and $w_0 + \\sum_{k=1}^K w_k = 1$. Define $\\pi_0 = w_0$ and $\\pi_k = w_k$ for $k \\in \\{1,\\dots,K\\}$.\n- There are $M$ cues, indexed by $i \\in \\{1,\\dots,M\\}$. For each cue $i$, the likelihood of its value given episode $k$ is $\\ell_{i,k} = p(\\text{cue}_i \\mid k)$ for $k \\in \\{1,\\dots,K\\}$, and the likelihood under clutter is $u_i = p(\\text{cue}_i \\mid k=0)$.\n- For per-cue association, assume that for each cue $i$, the latent source assignment is drawn independently from the mixture with weights $\\{\\pi_k\\}_{k=0}^K$, and then the cue value is drawn from the corresponding likelihood.\n- For posterior over a single target episode given all cues, assume that there exists one latent target episode $\\kappa \\in \\{1,\\dots,K\\}$ drawn from the prior $\\{w_k\\}_{k=1}^K$. For each cue $i$, with probability $P_{D,i} \\in [0,1]$ (the detection probability), the cue is generated by the target episode $\\kappa$ via $\\ell_{i,\\kappa}$, and with probability $(1-P_{D,i})$ it is generated by the global mixture $\\{\\pi_k\\}_{k=0}^K$ independent of $\\kappa$.\n\nTask specifications:\n\n- Input is implicit and embedded in the program as constants; do not read from standard input. Your program must compute:\n  1. For each cue $i$, the per-cue association probabilities $\\gamma_{i,k} = p(z_i = k \\mid \\text{cue}_i)$ for $k \\in \\{0,1,\\dots,K\\}$, where $k=0$ denotes clutter. These probabilities must be normalized for each cue $i$ so that $\\sum_{k=0}^K \\gamma_{i,k} = 1$.\n  2. The posterior distribution over target episodes $p(\\kappa = k \\mid \\text{all cues})$ for $k \\in \\{1,\\dots,K\\}$ under the detection model defined above.\n\n- All cues are conditionally independent given the relevant latent variables. All quantities are purely dimensionless probabilities; no physical units are involved.\n\n- Your program must implement the above using Bayes' rule and the law of total probability from first principles, without relying on any specialized external packages beyond those permitted.\n\nTest suite. The following cases must be embedded and evaluated by your program:\n\n- Case $1$:\n  - $K = 2$, $M = 2$.\n  - $w_0 = 0.1$, $w = [0.6, 0.3]$.\n  - Likelihood matrix $\\ell$ (rows are cues, columns are episodes $1$ through $K$):\n    - Cue $1$: $\\ell_{1,\\cdot} = [0.5, 0.2]$.\n    - Cue $2$: $\\ell_{2,\\cdot} = [0.1, 0.4]$.\n  - Clutter likelihoods: $u = [0.05, 0.05]$.\n  - Detection probabilities: $P_D = [0.8, 0.9]$.\n\n- Case $2$ (equal likelihood symmetry):\n  - $K = 3$, $M = 1$.\n  - $w_0 = 0.2$, $w = [0.5, 0.2, 0.1]$.\n  - Likelihood matrix $\\ell$:\n    - Cue $1$: $\\ell_{1,\\cdot} = [0.2, 0.2, 0.2]$.\n  - Clutter likelihoods: $u = [0.2]$.\n  - Detection probabilities: $P_D = [0.5]$.\n\n- Case $3$ (near-zero mixture weight episode and ambiguous cues):\n  - $K = 3$, $M = 3$.\n  - $w_0 = 0.05$, $w = [0.94, 0.01, 0.0]$.\n  - Likelihood matrix $\\ell$:\n    - Cue $1$: $\\ell_{1,\\cdot} = [0.9, 0.4, 0.1]$.\n    - Cue $2$: $\\ell_{2,\\cdot} = [0.3, 0.6, 0.5]$.\n    - Cue $3$: $\\ell_{3,\\cdot} = [0.2, 0.2, 0.2]$.\n  - Clutter likelihoods: $u = [0.1, 0.2, 0.2]$.\n  - Detection probabilities: $P_D = [0.7, 0.6, 0.1]$.\n\n- Case $4$ (high clutter prior, fully diagnostic cues):\n  - $K = 2$, $M = 2$.\n  - $w_0 = 0.5$, $w = [0.25, 0.25]$.\n  - Likelihood matrix $\\ell$:\n    - Cue $1$: $\\ell_{1,\\cdot} = [0.9, 0.1]$.\n    - Cue $2$: $\\ell_{2,\\cdot} = [0.8, 0.2]$.\n  - Clutter likelihoods: $u = [10^{-6}, 10^{-6}]$.\n  - Detection probabilities: $P_D = [1.0, 1.0]$.\n\nOutput specification:\n\n- For each case, output a list with two elements: \n  - The first element is the per-cue association probability matrix $\\Gamma$ as a list of $M$ rows, where each row is a list of $K+1$ floats corresponding to $[k=0 \\text{ (clutter)}, k=1, \\dots, k=K]$.\n  - The second element is the posterior over episodes as a list of $K$ floats corresponding to $[p(\\kappa=1 \\mid \\text{cues}), \\dots, p(\\kappa=K \\mid \\text{cues})]$.\n- All floats must be rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, where each case contributes a list as described above. For example: \"[[Gamma_case1,Posterior_case1],[Gamma_case2,Posterior_case2],...]\" with each \"Gamma_caseX\" instantiated as a list of lists as specified.",
            "solution": "The problem asks for the computation of two sets of posterior probabilities within a Bayesian framework designed to model memory retrieval, as conceptualized by hippocampal indexing theory. The model involves a set of observed cues generated from a mixture of latent sources: several \"episode\" sources and a \"clutter\" source. We are to determine (1) the probability that each cue originated from each source, and (2) the overall posterior probability of each episode being the single retrieval target given all cues. The solution is derived from first principles of probability theory, namely Bayes' rule and the law of total probability.\n\nThe fundamental principles are as follows:\n\n1.  **Bayes' Rule**: For a hypothesis $H$ and evidence $E$, the posterior probability of the hypothesis given the evidence is related to the prior probability of the hypothesis and the likelihood of the evidence given the hypothesis:\n    $$p(H \\mid E) = \\frac{p(E \\mid H) p(H)}{p(E)}$$\n2.  **Law of Total Probability**: The probability of an event $E$ can be found by summing over a set of mutually exclusive and exhaustive hypotheses $\\{H_j\\}$:\n    $$p(E) = \\sum_j p(E \\mid H_j) p(H_j)$$\nThe denominator $p(E)$ in Bayes' rule is often computed using this law, serving as a normalization constant.\n\nThe problem defines two distinct but related generative models, and we will apply these principles to each. Let there be $M$ cues, indexed by $i \\in \\{1,\\dots,M\\}$, and $K$ episodes, indexed by $k \\in \\{1,\\dots,K\\}$, plus a clutter source indexed by $k=0$. The prior mixture weights are given by $\\pi_k$ for $k \\in \\{0, \\dots, K\\}$, where $\\pi_0 = w_0$ and $\\pi_k = w_k$ for $k>0$. The likelihood of observing cue $i$ given it originated from episode $k$ is $\\ell_{i,k} = p(\\text{cue}_i \\mid \\text{source}=k)$, and from clutter is $u_i = p(\\text{cue}_i \\mid \\text{source}=0)$.\n\n### Task 1: Per-Cue Association Probabilities\n\nFor the first task, we compute $\\gamma_{i,k} = p(z_i=k \\mid \\text{cue}_i)$, the posterior probability that the latent source $z_i$ for cue $i$ is source $k$ (where $k=0$ is clutter), given the observation of cue $i$. The generative model for this task assumes each cue's source $z_i$ is drawn independently from the prior mixture distribution $\\{\\pi_k\\}_{k=0}^K$.\n\nApplying Bayes' rule to find $\\gamma_{i,k}$:\n$$\\gamma_{i,k} = p(z_i = k \\mid \\text{cue}_i) = \\frac{p(\\text{cue}_i \\mid z_i = k) p(z_i = k)}{p(\\text{cue}_i)}$$\n\nThe terms in this equation are defined by the problem statement:\n-   $p(z_i = k)$, the prior probability of source $k$, is simply the mixture weight $\\pi_k$.\n-   $p(\\text{cue}_i \\mid z_i = k)$, the likelihood, is $\\ell_{i,k}$ for an episode $k \\in \\{1, \\dots, K\\}$ and $u_i$ for clutter $k=0$.\n\nThe denominator, $p(\\text{cue}_i)$, is the marginal likelihood or \"evidence\" for cue $i$. We compute it using the law of total probability, marginalizing over all possible sources:\n$$p(\\text{cue}_i) = \\sum_{j=0}^{K} p(\\text{cue}_i \\mid z_i = j) p(z_i = j) = p(\\text{cue}_i \\mid z_i=0)\\pi_0 + \\sum_{j=1}^{K} p(\\text{cue}_i \\mid z_i=j)\\pi_j$$\nSubstituting the given likelihoods and priors:\n$$p(\\text{cue}_i) = u_i \\pi_0 + \\sum_{j=1}^{K} \\ell_{i,j} \\pi_j$$\nThis term represents the likelihood of observing cue $i$ under the global mixture model, and we will denote it $p_{\\text{mix}}(\\text{cue}_i)$.\n\nThe final expression for the association probability $\\gamma_{i,k}$ is thus the ratio of the unnormalized posterior for source $k$ to the sum of all unnormalized posteriors. For clutter ($k=0$):\n$$\\gamma_{i,0} = \\frac{u_i \\pi_0}{p_{\\text{mix}}(\\text{cue}_i)}$$\nAnd for an episode $k \\in \\{1,\\dots,K\\}$:\n$$\\gamma_{i,k} = \\frac{\\ell_{i,k} \\pi_k}{p_{\\text{mix}}(\\text{cue}_i)}$$\nThese probabilities are calculated for each cue $i$ and for each source $k \\in \\{0, \\dots, K\\}$, forming an $M \\times (K+1)$ matrix $\\Gamma = [\\gamma_{i,k}]$.\n\n### Task 2: Posterior Over Target Episodes\n\nFor the second task, we compute $p(\\kappa=k \\mid \\text{cues})$, the posterior probability that episode $k$ is the single latent retrieval target, $\\kappa$, given the full set of cues $\\{ \\text{cue}_i \\}_{i=1}^M$.\n\nThe generative model for this task posits a single target episode $\\kappa \\in \\{1, \\dots, K\\}$ drawn from a prior distribution over episodes. The problem states the weights are $\\{w_k\\}_{k=1}^K$. As these must sum to $1$ to form a valid prior, we normalize them:\n$$p(\\kappa=k) = \\frac{w_k}{\\sum_{j=1}^K w_j} = \\frac{w_k}{1 - w_0}$$\nWe denote this normalized prior as $p_k^*$.\n\nEach cue $i$ is then generated either from this target $\\kappa$ with probability $P_{D,i}$ (detection), or from the global background mixture (as defined in Task 1) with probability $1-P_{D,i}$.\n\nApplying Bayes' rule to find the posterior over targets:\n$$p(\\kappa=k \\mid \\text{cues}) = \\frac{p(\\text{cues} \\mid \\kappa=k) p(\\kappa=k)}{p(\\text{cues})}$$\n\nThe terms are:\n-   $p(\\kappa=k)$ is the normalized prior $p_k^*$.\n-   $p(\\text{cues} \\mid \\kappa=k)$ is the likelihood of the set of all cues given target $k$. Due to the conditional independence of cues given the target, this is the product of individual cue likelihoods:\n    $$p(\\text{cues} \\mid \\kappa=k) = \\prod_{i=1}^M p(\\text{cue}_i \\mid \\kappa=k)$$\n-   $p(\\text{cues})$ is the total evidence, found by marginalizing over all possible targets $j \\in \\{1, \\dots, K\\}$:\n    $$p(\\text{cues}) = \\sum_{j=1}^K p(\\text{cues} \\mid \\kappa=j) p(\\kappa=j)$$\n\nThe critical step is deriving the single-cue likelihood, $p(\\text{cue}_i \\mid \\kappa=k)$. Using the law of total probability over the two possibilities for the cue's origin (target or mixture):\n$$p(\\text{cue}_i \\mid \\kappa=k) = p(\\text{cue}_i \\mid \\text{from target } k)P_{D,i} + p(\\text{cue}_i \\mid \\text{from mixture})(1 - P_{D,i})$$\nThe likelihood given the cue is from the target is $\\ell_{i,k}$. The likelihood given it's from the mixture is independent of the target's identity and is precisely the marginal likelihood $p_{\\text{mix}}(\\text{cue}_i)$ we calculated in Task 1. Therefore:\n$$p(\\text{cue}_i \\mid \\kappa=k) = \\ell_{i,k} P_{D,i} + p_{\\text{mix}}(\\text{cue}_i) (1 - P_{D,i})$$\n\nLet $L_k = p(\\text{cues} \\mid \\kappa=k) = \\prod_{i=1}^M [ \\ell_{i,k} P_{D,i} + p_{\\text{mix}}(\\text{cue}_i) (1 - P_{D,i}) ]$. The unnormalized posterior for target $k$ is $\\tilde{P}_k = L_k p_k^*$.\nThe final posterior is obtained by normalizing:\n$$p(\\kappa=k \\mid \\text{cues}) = \\frac{\\tilde{P}_k}{\\sum_{j=1}^K \\tilde{P}_j} = \\frac{L_k p_k^*}{\\sum_{j=1}^K L_j p_j^*}$$\nThis calculation is performed for each episode $k \\in \\{1, \\dots, K\\}$.\n\n### Computational Algorithm\n\nThe implementation will proceed as follows:\n1.  For each test case, define the given parameters $K, M, w_0, w, \\ell, u, P_D$.\n2.  For each cue $i \\in \\{1, \\dots, M\\}$, compute the marginal likelihood $p_{\\text{mix}}(\\text{cue}_i) = u_i w_0 + \\sum_{j=1}^K \\ell_{i,j} w_j$.\n3.  **Task 1**: Compute the $M \\times (K+1)$ matrix $\\Gamma$. For each $(i,k)$, compute $\\gamma_{i,k}$ using the formulas derived above and the pre-computed $p_{\\text{mix}}(\\text{cue}_i)$.\n4.  **Task 2**: Compute the posterior vector over $K$ episodes.\n    a. Calculate the normalized prior over targets, $p_k^* = w_k / \\sum_{j=1}^K w_j$.\n    b. For each candidate target $k \\in \\{1, \\dots, K\\}$, compute the total likelihood $L_k$ by taking the product over all cues of $p(\\text{cue}_i \\mid \\kappa=k)$.\n    c. Compute the unnormalized posterior $\\tilde{P}_k = L_k p_k^*$.\n    d. Normalize these values to get the final posterior $p(\\kappa=k \\mid \\text{cues})$.\n5.  Collect the results for all test cases and format them as specified, rounding all floating-point numbers to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve all test cases for the Bayesian hippocampal index retrieval model.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"K\": 2, \"M\": 2,\n            \"w0\": 0.1, \"w\": [0.6, 0.3],\n            \"L_mat\": [[0.5, 0.2], [0.1, 0.4]],\n            \"u\": [0.05, 0.05],\n            \"PD\": [0.8, 0.9]\n        },\n        {\n            \"K\": 3, \"M\": 1,\n            \"w0\": 0.2, \"w\": [0.5, 0.2, 0.1],\n            \"L_mat\": [[0.2, 0.2, 0.2]],\n            \"u\": [0.2],\n            \"PD\": [0.5]\n        },\n        {\n            \"K\": 3, \"M\": 3,\n            \"w0\": 0.05, \"w\": [0.94, 0.01, 0.0],\n            \"L_mat\": [[0.9, 0.4, 0.1], [0.3, 0.6, 0.5], [0.2, 0.2, 0.2]],\n            \"u\": [0.1, 0.2, 0.2],\n            \"PD\": [0.7, 0.6, 0.1]\n        },\n        {\n            \"K\": 2, \"M\": 2,\n            \"w0\": 0.5, \"w\": [0.25, 0.25],\n            \"L_mat\": [[0.9, 0.1], [0.8, 0.2]],\n            \"u\": [1e-6, 1e-6],\n            \"PD\": [1.0, 1.0]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = solve_case(**case)\n        all_results.append(result)\n    \n    # Custom string formatting to avoid extra spaces and match exact output format\n    def format_list(l):\n        if not isinstance(l, list):\n            return str(l)\n        return '[' + ','.join(format_list(item) for item in l) + ']'\n    \n    # Produces \"[[...],[...]]\" format\n    output_str = '[' + ','.join(format_list(res) for res in all_results) + ']'\n    print(output_str)\n\ndef solve_case(K, M, w0, w, L_mat, u, PD):\n    \"\"\"\n    Solves a single case of the problem.\n    \"\"\"\n    w = np.array(w)\n    L_mat = np.array(L_mat)\n    u = np.array(u)\n    PD = np.array(PD)\n\n    # --- Task 1: Per-cue association probabilities (Gamma matrix) ---\n    \n    # Calculate p_mix(cue_i) for each cue i\n    # p_mix(cue_i) = u_i * pi_0 + sum_j(l_{i,j} * pi_j)\n    p_mix_cues = u * w0 + np.dot(L_mat, w)\n    \n    gamma_matrix = np.zeros((M, K + 1))\n    \n    # Association with clutter (k=0)\n    # The division by zero is avoided by checking p_mix_cues.\n    # In valid probability models, p_mix_cues will be zero iff all likelihoods and priors are zero.\n    gamma_matrix[:, 0] = np.divide(u * w0, p_mix_cues, out=np.zeros_like(p_mix_cues), where=p_mix_cues!=0)\n    \n    # Association with episodes (k=1..K)\n    gamma_matrix[:, 1:] = np.divide((L_mat * w), p_mix_cues[:, np.newaxis], out=np.zeros_like(L_mat), where=p_mix_cues[:, np.newaxis]!=0)\n\n\n    # --- Task 2: Posterior over target episodes p(kappa=k | cues) ---\n    \n    # Prior on target episodes, p*(k) = w_k / sum(w)\n    sum_w = np.sum(w)\n    if sum_w == 0:\n        # If no episodes have non-zero prior weight, no episode can be a target.\n        # The posterior is ill-defined, but a uniform distribution over K options\n        # or a zero vector are plausible interpretations. A zero vector is chosen\n        # as p(k)=0 for all k.\n        p_star_k = np.zeros(K)\n    else:\n        p_star_k = w / sum_w\n\n    unnormalized_posterior = np.zeros(K)\n    \n    # For each potential target episode k\n    for k_idx in range(K):\n        # p(cue_i | kappa=k) = l_{i,k}*P_{D,i} + p_mix(cue_i)*(1-P_{D,i})\n        p_cues_given_k = L_mat[:, k_idx] * PD + p_mix_cues * (1 - PD)\n        \n        # L_k = product over i of p(cue_i | kappa=k)\n        L_k_val = np.prod(p_cues_given_k)\n        \n        unnormalized_posterior[k_idx] = L_k_val * p_star_k[k_idx]\n\n    # Normalize the posterior\n    norm_const = np.sum(unnormalized_posterior)\n    if norm_const == 0:\n        # This can happen if all priors w_k are zero, or if a likelihood L_k_val is zero\n        # for all k with non-zero prior. We assign a uniform probability over the K\n        # episodes as a default for this indeterminate case.\n        posterior = np.full(K, 1.0 / K)\n    else:\n        posterior = unnormalized_posterior / norm_const\n        \n    # Format output as lists of lists and lists, with rounding\n    gamma_list = np.round(gamma_matrix, 6).tolist()\n    posterior_list = np.round(posterior, 6).tolist()\n    \n    return [gamma_list, posterior_list]\n\nsolve()\n```"
        }
    ]
}