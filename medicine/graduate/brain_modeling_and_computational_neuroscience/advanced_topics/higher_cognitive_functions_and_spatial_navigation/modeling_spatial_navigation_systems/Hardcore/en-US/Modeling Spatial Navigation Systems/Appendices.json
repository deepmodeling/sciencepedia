{
    "hands_on_practices": [
        {
            "introduction": "A robust sense of direction is the bedrock of any navigation system. In the brain, this is provided by head-direction cells, which act as a neural compass. This first exercise  delves into how the brain might read out a precise directional heading from the noisy, collective activity of a large population of these cells. You will implement a population vector decoder, a foundational model in computational neuroscience, and explore how biological realities like non-uniform tuning properties can introduce systematic biases and affect the decoder's precision, providing a concrete link between neural properties and perceptual accuracy.",
            "id": "3998140",
            "problem": "You are tasked with constructing and analyzing a linear population-vector decoder for head direction in a neural population of head direction cells. Assume a population of $N$ neurons indexed by preferred directions $\\phi \\in [-\\pi,\\pi)$ drawn from a probability density $p(\\phi)$ on the unit circle. Each neuron fires spikes as an independent inhomogeneous Poisson process over an observation window of duration $T$ seconds. Conditional on a fixed head direction stimulus $\\theta_0 \\in [-\\pi,\\pi)$, the expected firing rate (in spikes per second) of a neuron with preferred direction $\\phi$ is given by a von Mises tuning curve with baseline and multiplicative heterogeneity:\n- The mean firing rate is $\\lambda(\\phi \\mid \\theta_0) = b + m(\\phi)\\, \\exp\\!\\big(\\kappa \\cos(\\theta_0 - \\phi)\\big)$, where $b \\ge 0$ is the baseline rate, $m(\\phi) \\ge 0$ is the peak modulation, and $\\kappa \\ge 0$ is the tuning concentration.\n- The modulation is $m(\\phi) = m \\big[1 + \\varepsilon \\cos(\\phi - \\mu_g)\\big]$, where $m \\ge 0$ is a global modulation, $\\varepsilon \\in [0,1)$ controls heterogeneity strength, and $\\mu_g \\in [-\\pi,\\pi)$ is the heterogeneity axis.\n\nThe linear population-vector decoder forms the complex sum\n$$\nS \\;=\\; \\sum_{i=1}^{N} r_i \\, e^{\\mathrm{i}\\,\\phi_i},\n$$\nwhere $r_i$ is the spike count of neuron $i$ in duration $T$ seconds and $\\phi_i$ is its preferred direction. The decoded head direction estimate is\n$$\n\\hat{\\theta} \\;=\\; \\mathrm{arg}\\, S \\;\\in\\; [-\\pi,\\pi),\n$$\nwhere $\\mathrm{arg}$ returns the principal argument of a complex number and the wraparound convention maps angles into $[-\\pi,\\pi)$.\n\nFundamentals to be used and not to be exceeded: \n- Independent Poisson spiking with mean equal to variance for spike counts, $r_i \\sim \\mathrm{Poisson}\\big(\\Lambda(\\phi_i \\mid \\theta_0)\\big)$, where $\\Lambda(\\phi \\mid \\theta_0) = T\\, \\lambda(\\phi \\mid \\theta_0)$.\n- Law of large numbers for sums over large populations to replace empirical averages by expectations under $p(\\phi)$.\n- First-order perturbation (delta method) for the angle of a random vector perturbed around its mean, using the small-angle approximation for $\\hat{\\theta}$ fluctuations around the mean resultant direction.\n\nYour tasks:\n1. From these fundamentals, derive the large-$N$ expressions needed to characterize bias and variance of the decoder:\n   - Define the mean complex population vector\n     $$\n     \\mu \\;=\\; \\mathbb{E}[S] \\;=\\; N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0)\\, e^{\\mathrm{i}\\,\\phi}\\, p(\\phi)\\, d\\phi.\n     $$\n     Define the asymptotic mean decoded angle $\\theta_\\star = \\mathrm{arg}\\,\\mu$, and the signed bias $b_{\\mathrm{asym}} = \\mathrm{wrap}(\\theta_\\star - \\theta_0)$ in radians, wrapped to $[-\\pi,\\pi)$.\n   - Using a first-order small-angle approximation around $\\theta_\\star$, derive that the asymptotic variance of the decoder can be expressed as\n     $$\n     \\mathrm{Var}(\\hat{\\theta}) \\;\\approx\\; \\frac{N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0)\\, \\sin^2(\\phi - \\theta_\\star)\\, p(\\phi)\\, d\\phi}{\\lvert \\mu \\rvert^2}.\n     $$\n     The numerator arises from independent Poisson variability projected onto the direction orthogonal to $\\mu$, and the denominator is the squared magnitude of the mean vector.\n   - You must not use any closed-form special-function shortcut for these integrals; instead, state them as integrals to be evaluated numerically.\n\n2. Implement a program that, for each parameter set in the test suite below, numerically computes the asymptotic signed bias $b_{\\mathrm{asym}}$ and the asymptotic variance $\\mathrm{Var}(\\hat{\\theta})$ by evaluating the above integrals with sufficiently fine numerical quadrature over $\\phi \\in [-\\pi,\\pi)$. Use double-precision arithmetic and a uniform grid of at least $4096$ points over $[-\\pi,\\pi)$.\n\n3. Angle units: all angles, including $\\theta_0$, $\\mu_d$, and $\\mu_g$, must be interpreted and returned in radians. Express the output bias in radians. The variance is unitless as it pertains to squared radians.\n\n4. Population density of preferred directions:\n   - Uniform: $p(\\phi) = \\frac{1}{2\\pi}$.\n   - Nonuniform von Mises: $p(\\phi) = \\dfrac{\\exp\\!\\big(\\kappa_d \\cos(\\phi - \\mu_d)\\big)}{2\\pi I_0(\\kappa_d)}$, where $I_0$ is the modified Bessel function of the first kind of order zero. You must numerically normalize $p(\\phi)$ over the grid; do not use closed-form $I_0$.\n\n5. Test suite. For each of the following cases, compute and return the pair $[b_{\\mathrm{asym}}, \\mathrm{Var}(\\hat{\\theta})]$:\n   - Case A (happy path, uniform preferred directions):\n     - $N = 60$, $\\theta_0 = 0.7$, $\\kappa = 2.0$, $b = 5.0$, $m = 15.0$, $\\varepsilon = 0.0$, $\\mu_g = 0.0$, $T = 0.25$, $p(\\phi)$ uniform.\n   - Case B (nonuniform preferred directions, clustered around $0$):\n     - $N = 100$, $\\theta_0 = 1.2$, $\\kappa = 2.0$, $b = 5.0$, $m = 10.0$, $\\varepsilon = 0.0$, $\\mu_g = 0.0$, $T = 0.5$, $p(\\phi)$ von Mises with $\\kappa_d = 1.5$, $\\mu_d = 0.0$.\n   - Case C (uniform preferred directions but heterogeneous gains):\n     - $N = 80$, $\\theta_0 = -1.4$, $\\kappa = 1.2$, $b = 8.0$, $m = 12.0$, $\\varepsilon = 0.3$, $\\mu_g = 0.5$, $T = 0.4$, $p(\\phi)$ uniform.\n   - Case D (boundary case near wrap-around, clustered near $\\pi$):\n     - $N = 120$, $\\theta_0 = 3.05$, $\\kappa = 3.0$, $b = 3.0$, $m = 9.0$, $\\varepsilon = 0.2$, $\\mu_g = 3.0$, $T = 0.3$, $p(\\phi)$ von Mises with $\\kappa_d = 2.0$, $\\mu_d = 3.0$.\n\n6. Final output format. Your program should produce a single line of output containing a single comma-separated list with the $8$ floating-point results for the above four cases, flattened in the following order and rounded to six decimal places:\n$$\n\\big[ b_A,\\; v_A,\\; b_B,\\; v_B,\\; b_C,\\; v_C,\\; b_D,\\; v_D \\big],\n$$\nwhere $b_\\cdot$ denotes the signed bias $b_{\\mathrm{asym}}$ for the indicated case, and $v_\\cdot$ denotes $\\mathrm{Var}(\\hat{\\theta})$ for the indicated case, all in radians for bias and in squared radians for variance. The printed line must match exactly the bracketed format with values separated by commas and no additional text.",
            "solution": "The problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It describes a canonical model in computational neuroscience for analyzing the performance of a population code. The tasks involve a standard, albeit detailed, derivation from first principles followed by a numerical implementation. The problem is therefore deemed valid. We proceed with the solution.\n\nThe solution is divided into two parts. First, we provide the theoretical derivation for the asymptotic bias and variance of the population-vector decoder, as required. Second, we implement the resulting formulas numerically to compute these quantities for the specified test cases.\n\n### Theoretical Derivation\n\n**1. Model Specification**\nWe consider a population of $N$ head-direction neurons. The preferred direction of the $i$-th neuron is $\\phi_i \\in [-\\pi, \\pi)$, drawn from a probability density $p(\\phi)$. For a true head direction $\\theta_0$, neuron $i$ emits spikes as an independent Poisson process with a rate (in spikes/sec) given by the tuning curve:\n$$\n\\lambda(\\phi_i \\mid \\theta_0) = b + m(\\phi_i)\\, \\exp\\!\\big(\\kappa \\cos(\\theta_0 - \\phi_i)\\big)\n$$\nThe peak modulation $m(\\phi_i)$ may be heterogeneous:\n$$\nm(\\phi_i) = m \\big[1 + \\varepsilon \\cos(\\phi_i - \\mu_g)\\big]\n$$\nOver an observation interval of duration $T$, the spike count $r_i$ for neuron $i$ follows a Poisson distribution, $r_i \\sim \\mathrm{Poisson}(\\Lambda(\\phi_i \\mid \\theta_0))$, where the mean spike count is $\\Lambda(\\phi \\mid \\theta_0) = T\\,\\lambda(\\phi \\mid \\theta_0)$.\n\nThe population vector decoder estimates the head direction $\\hat{\\theta}$ from the angle of the complex vector $S$:\n$$\nS = \\sum_{i=1}^{N} r_i \\, e^{\\mathrm{i}\\,\\phi_i}, \\quad \\hat{\\theta} = \\mathrm{arg}\\,S\n$$\n\n**2. Asymptotic Bias**\nThe bias of the estimator is the difference between its expected value and the true value $\\theta_0$. For a nonlinear estimator like $\\hat{\\theta} = \\mathrm{arg}\\,S$, it is standard to approximate the expectation of the estimate, $\\mathbb{E}[\\hat{\\theta}]$, by the angle of the expected vector, $\\mathrm{arg}\\,\\mathbb{E}[S]$. This is the large-$N$ limit where the fluctuations of $S$ around its mean become small.\n\nWe define the mean complex population vector $\\mu = \\mathbb{E}[S]$. Using the linearity of expectation and the independence of neurons:\n$$\n\\mu = \\mathbb{E}\\left[\\sum_{i=1}^{N} r_i \\, e^{\\mathrm{i}\\,\\phi_i}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[r_i] \\, e^{\\mathrm{i}\\,\\phi_i}\n$$\nThe expectation of the spike count for neuron $i$ is $\\mathbb{E}[r_i] = \\Lambda(\\phi_i \\mid \\theta_0)$.\n$$\n\\mu = \\sum_{i=1}^{N} \\Lambda(\\phi_i \\mid \\theta_0) \\, e^{\\mathrm{i}\\,\\phi_i}\n$$\nIn the limit of a large population $N$, the law of large numbers allows us to replace the sum over the empirical sample of neurons with an expectation over the distribution of preferred directions $p(\\phi)$. This converts the sum into an integral:\n$$\n\\mu = N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0)\\, e^{\\mathrm{i}\\,\\phi}\\, p(\\phi)\\, d\\phi\n$$\nThis is the integral to be evaluated numerically.\nThe asymptotic decoded angle is defined as the angle of this mean vector:\n$$\n\\theta_\\star = \\mathrm{arg}\\,\\mu\n$$\nThe asymptotic signed bias, $b_{\\mathrm{asym}}$, is the wrapped difference between this asymptotic angle and the true stimulus angle, ensuring the result is in $[-\\pi, \\pi)$:\n$$\nb_{\\mathrm{asym}} = \\mathrm{wrap}(\\theta_\\star - \\theta_0) \\equiv (\\theta_\\star - \\theta_0 + \\pi) \\pmod{2\\pi} - \\pi\n$$\n\n**3. Asymptotic Variance**\nThe variance of the estimator, $\\mathrm{Var}(\\hat{\\theta})$, is analyzed using a first-order perturbation approximation, also known as the delta method. We consider small fluctuations, $\\delta S = S - \\mu$, of the population vector around its mean. The decoded angle is $\\hat{\\theta} = \\arg(\\mu + \\delta S)$.\n\nFor small $\\delta S$, the change in angle, $\\delta\\hat{\\theta} = \\hat{\\theta} - \\theta_\\star$, is primarily caused by the component of $\\delta S$ that is orthogonal to the mean vector $\\mu$. Writing $\\mu = |\\mu| e^{\\mathrm{i}\\theta_\\star}$, we can approximate the angle fluctuation as the projection of the fluctuation vector $\\delta S$ onto the direction orthogonal to $\\mu$, divided by the magnitude of $\\mu$. Let $\\delta S = \\delta S_x + \\mathrm{i}\\delta S_y$. The direction vector for $\\mu$ is $(\\cos\\theta_\\star, \\sin\\theta_\\star)$ and the orthogonal direction is $(-\\sin\\theta_\\star, \\cos\\theta_\\star)$. The fluctuation in angle is:\n$$\n\\delta\\hat{\\theta} \\approx \\frac{\\delta S_y \\cos\\theta_\\star - \\delta S_x \\sin\\theta_\\star}{|\\mu|}\n$$\nThe variance is $\\mathrm{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\delta\\hat{\\theta})^2]$ (since $\\mathbb{E}[\\delta\\hat{\\theta}] \\approx 0$ to first order).\n$$\n\\mathrm{Var}(\\hat{\\theta}) \\approx \\frac{1}{|\\mu|^2} \\mathrm{Var}\\left( S_y \\cos\\theta_\\star - S_x \\sin\\theta_\\star \\right)\n$$\nwhere $S_x = \\sum_i r_i \\cos\\phi_i$ and $S_y = \\sum_i r_i \\sin\\phi_i$. The variance of the linear combination is:\n$$\n\\mathrm{Var}\\left( S_y \\cos\\theta_\\star - S_x \\sin\\theta_\\star \\right) = \\cos^2\\theta_\\star \\mathrm{Var}(S_y) + \\sin^2\\theta_\\star \\mathrm{Var}(S_x) - 2\\sin\\theta_\\star\\cos\\theta_\\star \\mathrm{Cov}(S_x, S_y)\n$$\nSince the neurons are independent, the variances and covariances of $S_x$ and $S_y$ are sums of the contributions from individual neurons:\n$$\n\\mathrm{Var}(S_x) = \\sum_{i=1}^N \\mathrm{Var}(r_i \\cos\\phi_i) = \\sum_{i=1}^N \\cos^2\\phi_i \\mathrm{Var}(r_i)\n$$\n$$\n\\mathrm{Var}(S_y) = \\sum_{i=1}^N \\mathrm{Var}(r_i \\sin\\phi_i) = \\sum_{i=1}^N \\sin^2\\phi_i \\mathrm{Var}(r_i)\n$$\n$$\n\\mathrm{Cov}(S_x, S_y) = \\sum_{i=1}^N \\mathrm{Cov}(r_i \\cos\\phi_i, r_i \\sin\\phi_i) = \\sum_{i=1}^N \\cos\\phi_i \\sin\\phi_i \\mathrm{Var}(r_i)\n$$\nA key property of the Poisson distribution is that the variance equals the mean: $\\mathrm{Var}(r_i) = \\mathbb{E}[r_i] = \\Lambda(\\phi_i \\mid \\theta_0)$. Substituting this and taking the large-$N$ limit by converting sums to integrals:\n$$\n\\mathrm{Var}(S_x) \\approx N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) \\cos^2\\phi \\, p(\\phi) \\, d\\phi\n$$\n$$\n\\mathrm{Var}(S_y) \\approx N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) \\sin^2\\phi \\, p(\\phi) \\, d\\phi\n$$\n$$\n\\mathrm{Cov}(S_x, S_y) \\approx N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) \\cos\\phi \\sin\\phi \\, p(\\phi) \\, d\\phi\n$$\nSubstituting these back into the expression for $\\mathrm{Var}(S_y \\cos\\theta_\\star - S_x \\sin\\theta_\\star)$:\n\\begin{align*}\n\\text{Numerator} &= N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) p(\\phi) \\left[ \\cos^2\\theta_\\star \\sin^2\\phi + \\sin^2\\theta_\\star \\cos^2\\phi - 2\\sin\\theta_\\star\\cos\\theta_\\star\\cos\\phi\\sin\\phi \\right] d\\phi \\\\\n&= N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) p(\\phi) \\left( \\sin\\phi\\cos\\theta_\\star - \\cos\\phi\\sin\\theta_\\star \\right)^2 d\\phi \\\\\n&= N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) p(\\phi) \\sin^2(\\phi - \\theta_\\star) d\\phi\n\\end{align*}\nFinally, the asymptotic variance is the ratio of this numerator to the squared magnitude of the mean vector, $|\\mu|^2$:\n$$\n\\mathrm{Var}(\\hat{\\theta}) \\approx \\frac{N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0)\\, \\sin^2(\\phi - \\theta_\\star)\\, p(\\phi)\\, d\\phi}{\\lvert \\mu \\rvert^2}\n$$\nThis completes the derivation of the required expressions. They are now ready for numerical implementation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the asymptotic bias and variance of a population vector decoder\n    for head direction cells based on a set of test cases.\n    \"\"\"\n\n    def compute_decoder_performance(params, num_points=4097):\n        \"\"\"\n        Calculates the bias and variance for a single parameter set.\n\n        Args:\n            params (dict): A dictionary containing all model parameters.\n            num_points (int): The number of points for the numerical integration grid.\n\n        Returns:\n            tuple: A tuple containing the asymptotic bias (radians) and variance.\n        \"\"\"\n        # Unpack parameters\n        N = params['N']\n        theta_0 = params['theta_0']\n        kappa = params['kappa']\n        b = params['b']\n        m = params['m']\n        epsilon = params['epsilon']\n        mu_g = params['mu_g']\n        T = params['T']\n        p_phi_type = params['p_phi_type']\n        kappa_d = params['kappa_d']\n        mu_d = params['mu_d']\n        \n        # Set up the integration grid for phi\n        phi = np.linspace(-np.pi, np.pi, num_points)\n\n        # 1. Define the probability density of preferred directions, p(phi)\n        if p_phi_type == 'uniform':\n            p_phi = np.full_like(phi, 1.0 / (2 * np.pi))\n        else:  # 'von_mises'\n            # Numerically normalize p(phi) as required by the problem statement\n            unnormalized_p_phi = np.exp(kappa_d * np.cos(phi - mu_d))\n            norm_const = np.trapz(unnormalized_p_phi, phi)\n            p_phi = unnormalized_p_phi / norm_const\n\n        # 2. Define the heterogeneous modulation term, m(phi)\n        m_phi = m * (1 + epsilon * np.cos(phi - mu_g))\n\n        # 3. Define the mean total spike count, Lambda(phi|theta_0)\n        Lambda_phi = T * (b + m_phi * np.exp(kappa * np.cos(theta_0 - phi)))\n\n        # 4. Compute the mean complex population vector, mu\n        integrand_mu = Lambda_phi * np.exp(1j * phi) * p_phi\n        mu_per_neuron = np.trapz(integrand_mu, phi)\n        mu = N * mu_per_neuron\n\n        # 5. Compute the asymptotic bias\n        theta_star = np.angle(mu)\n        bias = theta_star - theta_0\n        # Wrap the bias to the interval [-pi, pi)\n        bias = (bias + np.pi) % (2 * np.pi) - np.pi\n\n        # 6. Compute the asymptotic variance\n        mu_magnitude_sq = np.abs(mu)**2\n        \n        # Numerator of the variance expression\n        integrand_var = Lambda_phi * (np.sin(phi - theta_star))**2 * p_phi\n        numerator_integral = np.trapz(integrand_var, phi)\n        numerator = N * numerator_integral\n\n        if mu_magnitude_sq == 0:\n            # This case is unlikely with the given parameters\n            variance = np.inf\n        else:\n            variance = numerator / mu_magnitude_sq\n            \n        return bias, variance\n\n    # Test cases as defined in the problem description\n    test_cases = [\n        # Case A: Uniform p(phi), no heterogeneity\n        {\n            'N': 60, 'theta_0': 0.7, 'kappa': 2.0, 'b': 5.0, 'm': 15.0, \n            'epsilon': 0.0, 'mu_g': 0.0, 'T': 0.25, 'p_phi_type': 'uniform', \n            'kappa_d': None, 'mu_d': None\n        },\n        # Case B: Non-uniform p(phi)\n        {\n            'N': 100, 'theta_0': 1.2, 'kappa': 2.0, 'b': 5.0, 'm': 10.0, \n            'epsilon': 0.0, 'mu_g': 0.0, 'T': 0.5, 'p_phi_type': 'von_mises', \n            'kappa_d': 1.5, 'mu_d': 0.0\n        },\n        # Case C: Uniform p(phi), heterogeneous gain\n        {\n            'N': 80, 'theta_0': -1.4, 'kappa': 1.2, 'b': 8.0, 'm': 12.0, \n            'epsilon': 0.3, 'mu_g': 0.5, 'T': 0.4, 'p_phi_type': 'uniform', \n            'kappa_d': None, 'mu_d': None\n        },\n        # Case D: Non-uniform p(phi) and heterogeneous gain\n        {\n            'N': 120, 'theta_0': 3.05, 'kappa': 3.0, 'b': 3.0, 'm': 9.0, \n            'epsilon': 0.2, 'mu_g': 3.0, 'T': 0.3, 'p_phi_type': 'von_mises', \n            'kappa_d': 2.0, 'mu_d': 3.0\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        bias, variance = compute_decoder_performance(params)\n        results.extend([bias, variance])\n\n    # Print the final output in the required format\n    print(f\"[{','.join(f'{val:.6f}' for val in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a stable heading reference, an agent can navigate by integrating its own motion, a process known as path integration or dead reckoning. This practice  formalizes this dynamic process and confronts its fundamental limitation: the inevitable accumulation of error. By deriving the growth of position uncertainty from noisy velocity and yaw-rate signals, you will uncover a critical principle: error from heading uncertainty grows cubically with time ($T^3$), while error from speed uncertainty grows only linearly ($T$). This powerful scaling law reveals the inherent fragility of pure path integration and motivates the need for corrective mechanisms in any robust navigation system.",
            "id": "3998099",
            "problem": "Consider a two-dimensional path integration model of an agent navigating in the plane. The agentâ€™s true motion is at constant forward speed $v_{0}$ along the $x$-axis, with true heading angle $\\theta_{\\mathrm{true}}(t) = 0$ for all $t \\in [0, T]$. The agent estimates its position by integrating noisy measurements of forward speed and yaw-rate. Let the measured forward speed be $v_{m}(t) = v_{0} + n_{v}(t)$, where $n_{v}(t)$ is zero-mean, stationary white Gaussian noise with autocorrelation $\\mathbb{E}[n_{v}(t)n_{v}(s)] = S_{v}\\,\\delta(t-s)$, with $S_{v} > 0$ denoting the noise intensity (equivalently, the two-sided Power Spectral Density (PSD) level), and $\\delta(\\cdot)$ denoting the Dirac delta distribution. Let the measured yaw-rate be $\\omega_{m}(t) = n_{\\omega}(t)$, where $n_{\\omega}(t)$ is zero-mean, stationary white Gaussian noise with autocorrelation $\\mathbb{E}[n_{\\omega}(t)n_{\\omega}(s)] = S_{\\omega}\\,\\delta(t-s)$ and $S_{\\omega} > 0$ is its noise intensity. Assume $n_{v}$ and $n_{\\omega}$ are independent.\n\nThe agent forms its heading estimate by integrating yaw-rate: $\\theta_{m}(t) = \\int_{0}^{t} \\omega_{m}(\\tau)\\,d\\tau$, with initial condition $\\theta_{m}(0) = 0$. The position estimate is given by\n$$\n\\mathbf{r}_{\\mathrm{est}}(T) = \\int_{0}^{T} v_{m}(t)\\,\\begin{pmatrix}\\cos\\theta_{m}(t) \\\\ \\sin\\theta_{m}(t) \\end{pmatrix}dt,\n$$\nand the true position is $\\mathbf{r}_{\\mathrm{true}}(T) = \\begin{pmatrix}v_{0}T \\\\ 0\\end{pmatrix}$. Define the position error vector $\\Delta\\mathbf{r}(T) = \\mathbf{r}_{\\mathrm{est}}(T) - \\mathbf{r}_{\\mathrm{true}}(T)$ and the radial mean-squared position error\n$$\n\\mathcal{E}(T) = \\mathbb{E}\\left[\\|\\Delta\\mathbf{r}(T)\\|^{2}\\right] = \\mathbb{E}\\left[\\Delta x(T)^{2} + \\Delta y(T)^{2}\\right].\n$$\n\nUnder the small-angle approximation, retain only first-order terms in $\\theta_{m}(t)$ (i.e., use $\\cos\\theta_{m}(t) \\approx 1$ and $\\sin\\theta_{m}(t) \\approx \\theta_{m}(t)$, and neglect all products of $\\theta_{m}(t)$ with $n_{v}(t)$ and any powers $\\theta_{m}(t)^{k}$ for $k \\geq 2$). Starting from these assumptions and the definitions above, derive the leading-order time dependence of $\\mathbb{E}\\left[\\Delta x(T)^{2}\\right]$ and $\\mathbb{E}\\left[\\Delta y(T)^{2}\\right]$, and thereby obtain a closed-form expression for $\\mathcal{E}(T)$ that identifies the dominant scaling terms in $T$ arising from velocity noise and heading noise.\n\nExpress the final mean-squared position error $\\mathcal{E}(T)$ in meters squared. The final answer must be a single closed-form analytic expression in terms of $T$, $v_{0}$, $S_{v}$, and $S_{\\omega}$.",
            "solution": "The problem statement is evaluated as valid. It is a well-posed problem in the field of computational neuroscience and stochastic process theory, grounded in established modeling principles for path integration. The givens are self-contained, consistent, and objective, allowing for a rigorous mathematical derivation.\n\nThe goal is to compute the radial mean-squared position error $\\mathcal{E}(T) = \\mathbb{E}\\left[\\|\\Delta\\mathbf{r}(T)\\|^{2}\\right]$, where $\\Delta\\mathbf{r}(T) = \\mathbf{r}_{\\mathrm{est}}(T) - \\mathbf{r}_{\\mathrm{true}}(T)$. The components of the error vector are $\\Delta x(T)$ and $\\Delta y(T)$. The total mean-squared error is the sum of the mean-squared errors of the components: $\\mathcal{E}(T) = \\mathbb{E}[\\Delta x(T)^2] + \\mathbb{E}[\\Delta y(T)^2]$, since the cross-term $\\mathbb{E}[\\Delta x(T) \\Delta y(T)]$ will be shown to be zero.\n\nFirst, we express the estimated position vector $\\mathbf{r}_{\\mathrm{est}}(T)$ using the provided definitions and approximations.\nThe estimated velocity vector is $v_{m}(t)\\,\\begin{pmatrix}\\cos\\theta_{m}(t) \\\\ \\sin\\theta_{m}(t) \\end{pmatrix}$.\nUsing the small-angle approximations $\\cos\\theta_{m}(t) \\approx 1$ and $\\sin\\theta_{m}(t) \\approx \\theta_{m}(t)$, this becomes:\n$$\n(v_0 + n_v(t)) \\begin{pmatrix} 1 \\\\ \\theta_m(t) \\end{pmatrix} = \\begin{pmatrix} v_0 + n_v(t) \\\\ v_0 \\theta_m(t) + n_v(t)\\theta_m(t) \\end{pmatrix}\n$$\nThe problem specifies that we neglect products of $\\theta_m(t)$ with $n_v(t)$. Thus, the approximated velocity vector is $\\begin{pmatrix} v_0 + n_v(t) \\\\ v_0 \\theta_m(t) \\end{pmatrix}$.\n\nThe estimated position is the integral of this velocity vector:\n$$\n\\mathbf{r}_{\\mathrm{est}}(T) = \\int_{0}^{T} \\begin{pmatrix} v_0 + n_v(t) \\\\ v_0 \\theta_m(t) \\end{pmatrix} dt = \\begin{pmatrix} v_0 T + \\int_0^T n_v(t) dt \\\\ v_0 \\int_0^T \\theta_m(t) dt \\end{pmatrix}\n$$\nThe true position is given as $\\mathbf{r}_{\\mathrm{true}}(T) = \\begin{pmatrix}v_{0}T \\\\ 0\\end{pmatrix}$.\nThe position error vector $\\Delta \\mathbf{r}(T) = \\begin{pmatrix}\\Delta x(T) \\\\ \\Delta y(T) \\end{pmatrix}$ is therefore:\n$$\n\\Delta x(T) = \\left(v_0 T + \\int_0^T n_v(t) dt\\right) - v_0 T = \\int_0^T n_v(t) dt\n$$\n$$\n\\Delta y(T) = \\left(v_0 \\int_0^T \\theta_m(t) dt\\right) - 0 = v_0 \\int_0^T \\theta_m(t) dt\n$$\nNow, we compute the mean-squared error for each component.\n\nFor the $x$-component error, $\\Delta x(T)$ depends only on the velocity noise $n_v(t)$.\n$$\n\\mathbb{E}\\left[\\Delta x(T)^2\\right] = \\mathbb{E}\\left[\\left(\\int_0^T n_v(t) dt\\right) \\left(\\int_0^T n_v(s) ds\\right)\\right] = \\mathbb{E}\\left[\\int_0^T \\int_0^T n_v(t) n_v(s) dt ds\\right]\n$$\nBy linearity of expectation, we can move the expectation operator inside the integrals:\n$$\n\\mathbb{E}\\left[\\Delta x(T)^2\\right] = \\int_0^T \\int_0^T \\mathbb{E}\\left[n_v(t) n_v(s)\\right] dt ds\n$$\nUsing the given autocorrelation for the white noise $n_v$, $\\mathbb{E}[n_v(t)n_v(s)] = S_v \\delta(t-s)$:\n$$\n\\mathbb{E}\\left[\\Delta x(T)^2\\right] = \\int_0^T \\int_0^T S_v \\delta(t-s) dt ds = S_v \\int_0^T \\left(\\int_0^T \\delta(t-s) dt\\right) ds\n$$\nThe inner integral over $t$ is equal to $1$ for any $s \\in [0, T]$.\n$$\n\\mathbb{E}\\left[\\Delta x(T)^2\\right] = S_v \\int_0^T 1 \\, ds = S_v T\n$$\nThis term represents the error accumulation due to noise in the forward velocity measurement, which follows a random walk (Wiener process), and its variance grows linearly with time.\n\nFor the $y$-component error, $\\Delta y(T)$ depends on the estimated heading $\\theta_m(t)$, which in turn depends on the yaw-rate noise $n_\\omega(t)$.\n$$\n\\mathbb{E}\\left[\\Delta y(T)^2\\right] = \\mathbb{E}\\left[\\left(v_0 \\int_0^T \\theta_m(t) dt\\right)^2\\right] = v_0^2 \\mathbb{E}\\left[\\int_0^T \\int_0^T \\theta_m(t) \\theta_m(s) dt ds\\right]\n$$\n$$\n\\mathbb{E}\\left[\\Delta y(T)^2\\right] = v_0^2 \\int_0^T \\int_0^T \\mathbb{E}\\left[\\theta_m(t) \\theta_m(s)\\right] dt ds\n$$\nWe must first find the covariance function of the estimated heading angle, $\\mathbb{E}[\\theta_m(t)\\theta_m(s)]$. The heading $\\theta_m(t)$ is the time integral of the yaw-rate noise $n_\\omega(t)$:\n$$\n\\theta_m(t) = \\int_0^t n_\\omega(\\tau) d\\tau\n$$\nThe covariance is:\n$$\n\\mathbb{E}[\\theta_m(t)\\theta_m(s)] = \\mathbb{E}\\left[\\left(\\int_0^t n_\\omega(\\tau_1) d\\tau_1\\right) \\left(\\int_0^s n_\\omega(\\tau_2) d\\tau_2\\right)\\right] = \\int_0^t \\int_0^s \\mathbb{E}[n_\\omega(\\tau_1) n_\\omega(\\tau_2)] d\\tau_1 d\\tau_2\n$$\nUsing the autocorrelation $\\mathbb{E}[n_\\omega(\\tau_1)n_\\omega(\\tau_2)] = S_\\omega \\delta(\\tau_1-\\tau_2)$:\n$$\n\\mathbb{E}[\\theta_m(t)\\theta_m(s)] = \\int_0^s \\left(\\int_0^t S_\\omega \\delta(\\tau_1-\\tau_2) d\\tau_1\\right) d\\tau_2\n$$\nAssuming without loss of generality that $s \\le t$, the inner integral evaluates to $S_\\omega$ for $\\tau_2 \\in [0,s]$ and $0$ otherwise.\n$$\n\\mathbb{E}[\\theta_m(t)\\theta_m(s)] = \\int_0^s S_\\omega d\\tau_2 = S_\\omega s\n$$\nIn general, $\\mathbb{E}[\\theta_m(t)\\theta_m(s)] = S_\\omega \\min(t,s)$. This shows $\\theta_m(t)$ is a Wiener process. Substituting this back into the expression for $\\mathbb{E}[\\Delta y(T)^2]$:\n$$\n\\mathbb{E}\\left[\\Delta y(T)^2\\right] = v_0^2 S_\\omega \\int_0^T \\int_0^T \\min(t,s) dt ds\n$$\nThe double integral can be evaluated by splitting the domain of integration:\n$$\n\\int_0^T \\int_0^T \\min(t,s) dt ds = \\int_0^T \\left( \\int_0^t \\min(t,s) ds + \\int_t^T \\min(t,s) ds \\right) dt\n$$\nIn the first inner integral, $s \\le t$, so $\\min(t,s) = s$. In the second, $s \\ge t$, so $\\min(t,s) = t$.\n$$\n\\int_0^T \\left( \\int_0^t s\\,ds + \\int_t^T t\\,ds \\right) dt = \\int_0^T \\left( \\left[\\frac{s^2}{2}\\right]_0^t + \\left[ts\\right]_t^T \\right) dt\n$$\n$$\n= \\int_0^T \\left( \\frac{t^2}{2} + t(T-t) \\right) dt = \\int_0^T \\left( \\frac{t^2}{2} + tT - t^2 \\right) dt = \\int_0^T \\left( tT - \\frac{t^2}{2} \\right) dt\n$$\n$$\n= \\left[ \\frac{t^2T}{2} - \\frac{t^3}{6} \\right]_0^T = \\frac{T^3}{2} - \\frac{T^3}{6} = \\frac{3T^3 - T^3}{6} = \\frac{2T^3}{6} = \\frac{T^3}{3}\n$$\nTherefore, the mean-squared error in the $y$-component is:\n$$\n\\mathbb{E}\\left[\\Delta y(T)^2\\right] = \\frac{1}{3} v_0^2 S_\\omega T^3\n$$\nThis term represents error from the integration of heading error, which itself grows as a random walk. The resulting position error variance grows with the third power of time.\n\nSince $n_v$ and $n_\\omega$ are independent, $\\Delta x(T)$ (a functional of $n_v$) and $\\Delta y(T)$ (a functional of $n_\\omega$) are also independent. Both have zero mean because the noises are zero-mean. Thus, the covariance term $\\mathbb{E}[\\Delta x(T) \\Delta y(T)] = \\mathbb{E}[\\Delta x(T)]\\mathbb{E}[\\Delta y(T)] = 0 \\cdot 0 = 0$.\n\nThe total radial mean-squared position error $\\mathcal{E}(T)$ is the sum of the variances:\n$$\n\\mathcal{E}(T) = \\mathbb{E}\\left[\\Delta x(T)^2\\right] + \\mathbb{E}\\left[\\Delta y(T)^2\\right] = S_v T + \\frac{1}{3} v_0^2 S_\\omega T^3\n$$\nThis expression reveals the distinct time-scaling contributions from the two noise sources. The velocity noise leads to an error variance that grows linearly with time, while the yaw-rate (heading) noise leads to a much faster cubic growth in error variance. For long navigation times $T$, the error will be dominated by the heading integration noise.",
            "answer": "$$\n\\boxed{S_v T + \\frac{1}{3} v_0^2 S_\\omega T^3}\n$$"
        },
        {
            "introduction": "The previous exercise demonstrated that pure path integration is doomed to fail over long journeys. Biological navigation systems solve this by periodically correcting their internal position estimate using external landmarks. This final practice  models such a hybrid system, allowing you to quantify the dramatic stabilizing effect of intermittent landmark-based corrections. You will derive and compare the error dynamics of a system with and without these external resets, observing firsthand how combining internal (idiothetic) and external (allothetic) cues transforms an unstable estimator with unbounded error growth into a robust one with statistically stationary, bounded error.",
            "id": "3998165",
            "problem": "Consider a two-dimensional planar path-integration estimator for spatial navigation, as is standard in models of hippocampal and entorhinal systems. At discrete time step $k \\in \\{1,\\dots,T\\}$, the estimator updates its position by integrating speed and heading according to the rule\n$$\n\\widehat{\\boldsymbol{x}}_{k} = \\widehat{\\boldsymbol{x}}_{k-1} + v_k \\, \\Delta t \\, \\begin{bmatrix}\\cos \\theta_k \\\\ \\sin \\theta_k \\end{bmatrix},\n$$\nwhere $\\Delta t$ is the fixed step duration in seconds, $v_k$ is the measured speed in meters per second, and $\\theta_k$ is the measured heading angle in radians. The true motion is a constant-speed, constant-heading trajectory with speed $v_0$ and heading $\\theta_0$, so the true displacement per step is\n$$\n\\boldsymbol{d}_0 = v_0 \\, \\Delta t \\, \\begin{bmatrix}\\cos \\theta_0 \\\\ \\sin \\theta_0 \\end{bmatrix}.\n$$\nMeasurements are corrupted by zero-mean Gaussian noise that is independent across time and between speed and heading:\n$$\nv_k = v_0 + \\varepsilon_{v,k}, \\quad \\varepsilon_{v,k} \\sim \\mathcal{N}(0, \\sigma_v^2),\n$$\n$$\n\\theta_k = \\theta_0 + \\varepsilon_{\\theta,k}, \\quad \\varepsilon_{\\theta,k} \\sim \\mathcal{N}(0, \\sigma_\\theta^2),\n$$\nwith $(\\varepsilon_{v,k})$ and $(\\varepsilon_{\\theta,k})$ independent sequences and independent of each other. The estimation error at step $T$ without any landmark corrections is\n$$\n\\boldsymbol{e}_T^{\\mathrm{nc}} = \\widehat{\\boldsymbol{x}}_{T} - \\boldsymbol{x}_{T},\n$$\nwhere $\\boldsymbol{x}_{T} = \\boldsymbol{x}_{0} + T \\boldsymbol{d}_0$ is the true position.\n\nNow consider intermittent landmark corrections every $M$ steps. At correction times $k \\in \\{M, 2M, 3M, \\dots\\}$, the estimator receives a landmark-based position measurement that resets the position estimate to\n$$\n\\widehat{\\boldsymbol{x}}_{k} \\leftarrow \\boldsymbol{x}_{k} + \\boldsymbol{\\eta}_{k},\n$$\nwhere $\\boldsymbol{\\eta}_{k} \\sim \\mathcal{N}\\!\\left(\\boldsymbol{0}, \\sigma_L^2 \\boldsymbol{I}_2\\right)$ is an isotropic zero-mean Gaussian correction noise in meters, independent of all other noise and independent across corrections, and $\\boldsymbol{I}_2$ is the $2 \\times 2$ identity matrix. Between corrections, the estimator continues to integrate $v_k$ and $\\theta_k$ as above. The estimation error at step $T$ with corrections is denoted\n$$\n\\boldsymbol{e}_T^{\\mathrm{wc}} = \\widehat{\\boldsymbol{x}}_{T} - \\boldsymbol{x}_{T}.\n$$\n\nYour tasks:\n1. Starting from the discrete-time path-integration rule, independence of Gaussian noise, and standard expectations for trigonometric functions of Gaussian variables, derive exact expressions for the bias vector $\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]$ and the covariance matrix $\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}})$ as functions of $T$, $\\Delta t$, $v_0$, $\\theta_0$, $\\sigma_v$, and $\\sigma_\\theta$. Then, derive the corresponding bias vector and covariance matrix for $\\boldsymbol{e}_T^{\\mathrm{wc}}$ under intermittent landmark corrections with period $M$ and correction noise variance $\\sigma_L^2$.\n2. Based on your derivations, implement a program that computes, for each test case, the Euclidean norm of the bias vector $\\|\\mathbb{E}[\\boldsymbol{e}_T]\\|$ in meters and the trace of the covariance matrix $\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T))$ in square meters, both without corrections and with corrections. All angles must be in radians. Express speeds in meters per second, time in seconds, positions in meters, and covariance traces in square meters. Do not round; output raw floating-point numbers.\n3. Compare scaling laws: use your formulas to characterize how the bias norm and covariance trace scale with $T$ without corrections and with intermittent corrections (perfect and noisy), and ensure the program numerically reflects these laws for the given test suite.\n\nTest suite (each case is a tuple $(v_0, \\theta_0, \\Delta t, T, \\sigma_v, \\sigma_\\theta, M, \\sigma_L)$ with $v_0$ in meters per second, $\\theta_0$ in radians, $\\Delta t$ in seconds, $T$ in steps, $\\sigma_v$ in meters per second, $\\sigma_\\theta$ in radians, $M$ in steps, and $\\sigma_L$ in meters):\n- Case 1 (general case): $(1.2, 0.5, 0.1, 100, 0.15, 0.05, 20, 0.0)$.\n- Case 2 (boundary case with zero heading noise): $(0.8, 1.0, 0.05, 200, 0.2, 0.0, 25, 0.0)$.\n- Case 3 (stronger heading noise with noisy corrections): $\\left(1.0, \\frac{\\pi}{3}, 0.2, 300, 0.05, 0.2, 30, 0.02\\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order for each case: $[\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]\\|,\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}})),\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]\\|,\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}}))]$. Concatenate all three cases in order, resulting in a list of $12$ floats. For example, the output format must be:\n$$\n[\\text{case1\\_bias\\_nc},\\text{case1\\_trace\\_nc},\\text{case1\\_bias\\_wc},\\text{case1\\_trace\\_wc},\\text{case2\\_bias\\_nc},\\dots,\\text{case3\\_trace\\_wc}].\n$$",
            "solution": "### Problem Validation\n\n**Step 1: Extract Givens**\n- Path integration update rule: $\\widehat{\\boldsymbol{x}}_{k} = \\widehat{\\boldsymbol{x}}_{k-1} + v_k \\, \\Delta t \\, \\begin{bmatrix}\\cos \\theta_k \\\\ \\sin \\theta_k \\end{bmatrix}$\n- Time step: $k \\in \\{1,\\dots,T\\}$, duration $\\Delta t$ (seconds)\n- True motion: speed $v_0$, heading $\\theta_0$ (both constant)\n- True displacement per step: $\\boldsymbol{d}_0 = v_0 \\, \\Delta t \\, \\begin{bmatrix}\\cos \\theta_0 \\\\ \\sin \\theta_0 \\end{bmatrix}$\n- Measured speed: $v_k = v_0 + \\varepsilon_{v,k}$, with noise $\\varepsilon_{v,k} \\sim \\mathcal{N}(0, \\sigma_v^2)$\n- Measured heading: $\\theta_k = \\theta_0 + \\varepsilon_{\\theta,k}$, with noise $\\varepsilon_{\\theta,k} \\sim \\mathcal{N}(0, \\sigma_\\theta^2)$\n- Noise properties: $(\\varepsilon_{v,k})$ and $(\\varepsilon_{\\theta,k})$ are independent sequences and independent of each other.\n- Error without corrections (nc): $\\boldsymbol{e}_T^{\\mathrm{nc}} = \\widehat{\\boldsymbol{x}}_{T} - \\boldsymbol{x}_{T}$, where $\\boldsymbol{x}_{T} = \\boldsymbol{x}_{0} + T \\boldsymbol{d}_0$. Assumed initial condition $\\widehat{\\boldsymbol{x}}_{0} = \\boldsymbol{x}_0$.\n- Landmark corrections: Occur every $M$ steps, at $k \\in \\{M, 2M, 3M, \\dots\\}$.\n- Correction rule: $\\widehat{\\boldsymbol{x}}_{k} \\leftarrow \\boldsymbol{x}_{k} + \\boldsymbol{\\eta}_{k}$\n- Correction noise: $\\boldsymbol{\\eta}_{k} \\sim \\mathcal{N}\\!\\left(\\boldsymbol{0}, \\sigma_L^2 \\boldsymbol{I}_2\\right)$, independent of all other noise and across time.\n- Error with corrections (wc): $\\boldsymbol{e}_T^{\\mathrm{wc}} = \\widehat{\\boldsymbol{x}}_{T} - \\boldsymbol{x}_{T}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem describes a standard model of path integration in computational neuroscience, a process thought to be implemented by head-direction cells, grid cells, and place cells. The use of Gaussian noise to model sensory uncertainty is a ubiquitous and valid assumption in this field. The model is a simplification but is firmly rooted in established theoretical frameworks.\n- **Well-Posed**: The problem asks for the first two moments (mean and covariance) of the estimation error, which is a well-defined statistical quantity. The inputs are clearly specified, and the model dynamics are unambiguous, leading to a unique solution.\n- **Objective**: The problem is stated in precise mathematical and statistical language, free of subjective or ambiguous terminology.\n- **Completeness**: All necessary parameters ($v_0, \\theta_0, \\Delta t, T, \\sigma_v, \\sigma_\\theta, M, \\sigma_L$) and model equations are provided.\n- **Consistency**: There are no contradictions in the problem statement.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-posed, scientifically grounded problem in theoretical neuroscience. A full derivation and solution are warranted.\n\n### Derivation of Bias and Covariance\n\nWe are tasked with finding the bias vector $\\mathbb{E}[\\boldsymbol{e}_T]$ and the covariance matrix $\\mathrm{Cov}(\\boldsymbol{e}_T)$ for the estimation error, both without and with landmark corrections.\n\n#### 1. Analysis Without Corrections (nc)\n\nAssuming the initial estimate is perfect, $\\widehat{\\boldsymbol{x}}_0 = \\boldsymbol{x}_0$, the error at step $T$ is the sum of single-step errors:\n$$\n\\boldsymbol{e}_T^{\\mathrm{nc}} = \\widehat{\\boldsymbol{x}}_T - \\boldsymbol{x}_T = \\sum_{k=1}^T \\left( \\widehat{\\boldsymbol{d}}_k - \\boldsymbol{d}_0 \\right)\n$$\nwhere $\\widehat{\\boldsymbol{d}}_k = v_k \\Delta t \\begin{bmatrix} \\cos\\theta_k \\\\ \\sin\\theta_k \\end{bmatrix}$ is the estimated displacement and $\\boldsymbol{d}_0 = v_0 \\Delta t \\begin{bmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 \\end{bmatrix}$ is the true displacement. Let $\\boldsymbol{\\delta}_k = \\widehat{\\boldsymbol{d}}_k - \\boldsymbol{d}_0$ be the single-step error. Since the noise terms $(\\varepsilon_{v,k}, \\varepsilon_{\\theta,k})$ are independent and identically distributed (i.i.d.) over time, the single-step errors $\\boldsymbol{\\delta}_k$ are also i.i.d.\n\n**Bias Vector $\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]$**\nBy linearity of expectation, $\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}] = \\sum_{k=1}^T \\mathbb{E}[\\boldsymbol{\\delta}_k] = T \\mathbb{E}[\\boldsymbol{\\delta}_1]$. We compute $\\mathbb{E}[\\boldsymbol{\\delta}_1] = \\mathbb{E}[\\widehat{\\boldsymbol{d}}_1] - \\boldsymbol{d}_0$.\n$$\n\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1] = \\Delta t \\, \\mathbb{E}\\left[ v_1 \\begin{bmatrix} \\cos\\theta_1 \\\\ \\sin\\theta_1 \\end{bmatrix} \\right] = \\Delta t \\begin{bmatrix} \\mathbb{E}[v_1 \\cos\\theta_1] \\\\ \\mathbb{E}[v_1 \\sin\\theta_1] \\end{bmatrix}\n$$\nSince $v_1$ and $\\theta_1$ are independent, $\\mathbb{E}[v_1 \\cos\\theta_1] = \\mathbb{E}[v_1]\\mathbb{E}[\\cos\\theta_1]$. We have $\\mathbb{E}[v_1] = \\mathbb{E}[v_0 + \\varepsilon_{v,1}] = v_0$.\nFor a Gaussian variable $\\theta_1 \\sim \\mathcal{N}(\\theta_0, \\sigma_\\theta^2)$, we use the standard results: $\\mathbb{E}[\\cos\\theta_1] = \\cos(\\theta_0) e^{-\\sigma_\\theta^2/2}$ and $\\mathbb{E}[\\sin\\theta_1] = \\sin(\\theta_0) e^{-\\sigma_\\theta^2/2}$.\n$$\n\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1] = \\Delta t \\, v_0 e^{-\\sigma_\\theta^2/2} \\begin{bmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 \\end{bmatrix} = e^{-\\sigma_\\theta^2/2} \\boldsymbol{d}_0\n$$\nThe bias per step is $\\mathbb{E}[\\boldsymbol{\\delta}_1] = (e^{-\\sigma_\\theta^2/2} - 1)\\boldsymbol{d}_0$. The total bias is:\n$$\n\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}] = T(e^{-\\sigma_\\theta^2/2} - 1)\\boldsymbol{d}_0 = T v_0 \\Delta t (e^{-\\sigma_\\theta^2/2} - 1) \\begin{bmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 \\end{bmatrix}\n$$\nThe Euclidean norm of the bias vector is:\n$$\n\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]\\| = |T v_0 \\Delta t (e^{-\\sigma_\\theta^2/2} - 1)| = T v_0 \\Delta t (1 - e^{-\\sigma_\\theta^2/2})\n$$\n\n**Covariance Matrix $\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}})$**\nSince the step errors $\\boldsymbol{\\delta}_k$ are i.i.d., $\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}}) = \\sum_{k=1}^T \\mathrm{Cov}(\\boldsymbol{\\delta}_k) = T \\mathrm{Cov}(\\boldsymbol{\\delta}_1)$. Also $\\mathrm{Cov}(\\boldsymbol{\\delta}_1) = \\mathrm{Cov}(\\widehat{\\boldsymbol{d}}_1)$. The trace is rotation-invariant, which simplifies the calculation.\n$$\n\\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\boldsymbol{d}}_1)) = \\mathrm{tr}(\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1 \\widehat{\\boldsymbol{d}}_1^T] - \\mathbb{E}[\\widehat{\\boldsymbol{d}}_1]\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1]^T) = \\mathbb{E}[\\|\\widehat{\\boldsymbol{d}}_1\\|^2] - \\|\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1]\\|^2\n$$\nWe have $\\|\\widehat{\\boldsymbol{d}}_1\\|^2 = (v_1 \\Delta t)^2$. So, $\\mathbb{E}[\\|\\widehat{\\boldsymbol{d}}_1\\|^2] = (\\Delta t)^2 \\mathbb{E}[v_1^2] = (\\Delta t)^2(v_0^2 + \\sigma_v^2)$.\nWe also have $\\|\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1]\\|^2 = \\|e^{-\\sigma_\\theta^2/2} \\boldsymbol{d}_0\\|^2 = (e^{-\\sigma_\\theta^2/2})^2 \\|v_0 \\Delta t \\begin{bmatrix}\\cos\\theta_0 \\\\ \\sin\\theta_0\\end{bmatrix}\\|^2 = e^{-\\sigma_\\theta^2} (v_0 \\Delta t)^2$.\nThus, the trace of the single-step covariance is:\n$$\n\\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\boldsymbol{d}}_1)) = (\\Delta t)^2(v_0^2 + \\sigma_v^2) - e^{-\\sigma_\\theta^2}(v_0\\Delta t)^2 = (\\Delta t)^2 [v_0^2(1 - e^{-\\sigma_\\theta^2}) + \\sigma_v^2]\n$$\nThe trace of the total error covariance is:\n$$\n\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}})) = T (\\Delta t)^2 [v_0^2(1 - e^{-\\sigma_\\theta^2}) + \\sigma_v^2]\n$$\n\n#### 2. Analysis With Corrections (wc)\n\nLandmark corrections reset the estimate every $M$ steps. The error at time $T$ depends on the number of steps since the last correction.\nLet $k_{last} = M \\lfloor T/M \\rfloor$ be the time of the last correction at or before $T$. If $T < M$, no correction has occurred, and $k_{last}=0$. The number of steps since the last reset is $T_{rem} = T - k_{last}$.\n\nCase 1: $T < M$. No corrections have happened. The error is identical to the no-correction case.\n$$\n\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]\\| = \\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]\\| \\quad \\text{and} \\quad \\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}})) = \\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}}))\n$$\n\nCase 2: $T \\ge M$. The error at time $T$ originates from the error at the last correction time $k_{last}$ and the path integration from $k_{last}+1$ to $T$.\n$$\n\\boldsymbol{e}_T^{\\mathrm{wc}} = (\\widehat{\\boldsymbol{x}}_{k_{last}} - \\boldsymbol{x}_{k_{last}}) + \\sum_{j=1}^{T_{rem}} (\\widehat{\\boldsymbol{d}}_{k_{last}+j} - \\boldsymbol{d}_0)\n$$\nThe first term is the error immediately after correction. If $k_{last} > 0$, this is $\\boldsymbol{\\eta}_{k_{last}}$. If $k_{last}=0$ (i.e. $T<M$), this is $\\boldsymbol{0}$. The second term is an independent path integration error accumulated over $T_{rem}$ steps, which we can denote $\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}}$.\n$$\n\\boldsymbol{e}_T^{\\mathrm{wc}} = \\boldsymbol{\\eta}_{k_{last}} + \\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}} \\quad (\\text{for } T \\ge M \\text{, where } \\boldsymbol{\\eta}_0 \\equiv \\boldsymbol{0})\n$$\n\n**Bias Vector $\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]$**\nSince $\\mathbb{E}[\\boldsymbol{\\eta}_{k_{last}}] = \\boldsymbol{0}$, we have $\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}] = \\mathbb{E}[\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}}]$.\n$$\n\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]\\| = \\|\\mathbb{E}[\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}}]\\| = T_{rem} v_0 \\Delta t (1 - e^{-\\sigma_\\theta^2/2})\n$$\nwhere $T_{rem} = T \\pmod M$. If $T$ is a multiple of $M$, then $T_{rem}=0$ and the bias norm is $0$.\n\n**Covariance Matrix $\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}})$**\nSince $\\boldsymbol{\\eta}_{k_{last}}$ and $\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}}$ are independent:\n$$\n\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}}) = \\mathrm{Cov}(\\boldsymbol{\\eta}_{k_{last}}) + \\mathrm{Cov}(\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}})\n$$\nFor $T \\ge M$, $k_{last} > 0$, so $\\mathrm{Cov}(\\boldsymbol{\\eta}_{k_{last}}) = \\sigma_L^2 \\boldsymbol{I}_2$.\n$$\n\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}})) = \\mathrm{tr}(\\sigma_L^2 \\boldsymbol{I}_2) + \\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}})) = 2\\sigma_L^2 + T_{rem} (\\Delta t)^2 [v_0^2(1 - e^{-\\sigma_\\theta^2}) + \\sigma_v^2]\n$$\nwhere again $T_{rem} = T \\pmod M$. If $T_{rem}=0$, the second term vanishes, and the trace is simply $2\\sigma_L^2$.\n\n#### 3. Scaling Laws\n\n- **Without Corrections**: The bias norm grows linearly with time, $\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]\\| \\propto T$, and the error variance (trace of covariance) also grows linearly, $\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}})) \\propto T$. This is characteristic of a random walk, where the uncertainty grows without bound.\n- **With Corrections**: The error accumulation is reset every $M$ steps. Both the bias norm and the covariance trace are periodic functions of time with period $M$. Their values are bounded, and do not grow with $T$ over long periods, i.e., $\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]\\| = O(1)$ and $\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}})) = O(1)$ for large $T$. Intermittent corrections effectively tame the error growth.\n\n### Summary of Formulas for Implementation\n\nLet `path_integ_bias_norm(k)` and `path_integ_trace(k)` be the formulas for the no-correction case over $k$ steps.\n1.  $\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]\\|$: `path_integ_bias_norm(T)`\n2.  $\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}}))`: `path_integ_trace(T)`\n3.  $\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]\\|$:\n    - If $T < M$: `path_integ_bias_norm(T)`\n    - If $T \\ge M$: `path_integ_bias_norm(T % M)`\n4.  $\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}}))`:\n    - If $T < M$: `path_integ_trace(T)`\n    - If $T \\ge M$: $2\\sigma_L^2 + \\text{path\\_integ\\_trace}(T \\pmod M)$\n\nwhere\n`path_integ_bias_norm(k)` $= k \\cdot v_0 \\Delta t (1 - e^{-\\sigma_\\theta^2/2})$\n`path_integ_trace(k)` $= k \\cdot (\\Delta t)^2 [v_0^2(1 - e^{-\\sigma_\\theta^2}) + \\sigma_v^2]$\n\nThe program will implement this logic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the spatial navigation estimator problem for the given test suite.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1 (general case)\n        (1.2, 0.5, 0.1, 100, 0.15, 0.05, 20, 0.0),\n        # Case 2 (boundary case with zero heading noise)\n        (0.8, 1.0, 0.05, 200, 0.2, 0.0, 25, 0.0),\n        # Case 3 (stronger heading noise with noisy corrections)\n        (1.0, np.pi/3, 0.2, 300, 0.05, 0.2, 30, 0.02),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        v0, theta0, delta_t, T, sigma_v, sigma_theta, M, sigma_L = case\n\n        # Helper functions for base path integration error (no corrections)\n        def path_integ_bias_norm(k, v0, delta_t, sigma_theta):\n            if k == 0:\n                return 0.0\n            sigma_theta_sq = sigma_theta**2\n            # Formula: k * v0 * delta_t * (1 - exp(-sigma_theta^2 / 2))\n            return k * v0 * delta_t * (1.0 - np.exp(-sigma_theta_sq / 2.0))\n\n        def path_integ_trace(k, v0, delta_t, sigma_v, sigma_theta):\n            if k == 0:\n                return 0.0\n            sigma_v_sq = sigma_v**2\n            sigma_theta_sq = sigma_theta**2\n            v0_sq = v0**2\n            delta_t_sq = delta_t**2\n            # Formula: k * delta_t^2 * [v0^2 * (1 - exp(-sigma_theta^2)) + sigma_v^2]\n            return k * delta_t_sq * (v0_sq * (1.0 - np.exp(-sigma_theta_sq)) + sigma_v_sq)\n\n        # --- NO CORRECTIONS (nc) ---\n        bias_nc_norm = path_integ_bias_norm(T, v0, delta_t, sigma_theta)\n        trace_nc = path_integ_trace(T, v0, delta_t, sigma_v, sigma_theta)\n        \n        # --- WITH CORRECTIONS (wc) ---\n        bias_wc_norm = 0.0\n        trace_wc = 0.0\n        \n        if T < M:\n            # No corrections have occurred yet, so error is the same as the 'nc' case.\n            bias_wc_norm = bias_nc_norm\n            trace_wc = trace_nc\n        else:\n            # At least one correction period has passed.\n            t_rem = T % M\n            \n            # The landmark error contribution to variance is always present\n            # after the first correction at step M.\n            landmark_var_contrib = 2.0 * sigma_L**2\n            \n            # Bias and variance from path integration accumulate since the last reset.\n            # If t_rem is 0, the process is exactly at a correction step, so no\n            # path integration error has accumulated since the reset.\n            pi_bias_contrib = path_integ_bias_norm(t_rem, v0, delta_t, sigma_theta)\n            pi_trace_contrib = path_integ_trace(t_rem, v0, delta_t, sigma_v, sigma_theta)\n            \n            bias_wc_norm = pi_bias_contrib\n            trace_wc = landmark_var_contrib + pi_trace_contrib\n            \n        results.extend([bias_nc_norm, trace_nc, bias_wc_norm, trace_wc])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}