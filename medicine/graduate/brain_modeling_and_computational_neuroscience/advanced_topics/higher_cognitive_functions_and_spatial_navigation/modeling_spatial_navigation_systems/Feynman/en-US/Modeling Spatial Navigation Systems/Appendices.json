{
    "hands_on_practices": [
        {
            "introduction": "The brain's sense of direction relies on head-direction (HD) cells, which collectively function as an internal compass. To make this information useful, downstream brain areas must decode the activity of the entire HD cell population. This exercise guides you through the construction of a population vector decoder, a fundamental tool in computational neuroscience, to understand how factors like non-uniform cell preferences can introduce biases and affect the precision of the decoded heading .",
            "id": "3998140",
            "problem": "You are tasked with constructing and analyzing a linear population-vector decoder for head direction in a neural population of head direction cells. Assume a population of $N$ neurons indexed by preferred directions $\\phi \\in [-\\pi,\\pi)$ drawn from a probability density $p(\\phi)$ on the unit circle. Each neuron fires spikes as an independent inhomogeneous Poisson process over an observation window of duration $T$ seconds. Conditional on a fixed head direction stimulus $\\theta_0 \\in [-\\pi,\\pi)$, the expected firing rate (in spikes per second) of a neuron with preferred direction $\\phi$ is given by a von Mises tuning curve with baseline and multiplicative heterogeneity:\n- The mean firing rate is $\\lambda(\\phi \\mid \\theta_0) = b + m(\\phi)\\, \\exp\\!\\big(\\kappa \\cos(\\theta_0 - \\phi)\\big)$, where $b \\ge 0$ is the baseline rate, $m(\\phi) \\ge 0$ is the peak modulation, and $\\kappa \\ge 0$ is the tuning concentration.\n- The modulation is $m(\\phi) = m \\big[1 + \\varepsilon \\cos(\\phi - \\mu_g)\\big]$, where $m \\ge 0$ is a global modulation, $\\varepsilon \\in [0,1)$ controls heterogeneity strength, and $\\mu_g \\in [-\\pi,\\pi)$ is the heterogeneity axis.\n\nThe linear population-vector decoder forms the complex sum\n$$\nS \\;=\\; \\sum_{i=1}^{N} r_i \\, e^{\\mathrm{i}\\,\\phi_i},\n$$\nwhere $r_i$ is the spike count of neuron $i$ in duration $T$ seconds and $\\phi_i$ is its preferred direction. The decoded head direction estimate is\n$$\n\\hat{\\theta} \\;=\\; \\mathrm{arg}\\, S \\;\\in\\; [-\\pi,\\pi),\n$$\nwhere $\\mathrm{arg}$ returns the principal argument of a complex number and the wraparound convention maps angles into $[-\\pi,\\pi)$.\n\nFundamentals to be used and not to be exceeded: \n- Independent Poisson spiking with mean equal to variance for spike counts, $r_i \\sim \\mathrm{Poisson}\\big(\\Lambda(\\phi_i \\mid \\theta_0)\\big)$, where $\\Lambda(\\phi \\mid \\theta_0) = T\\, \\lambda(\\phi \\mid \\theta_0)$.\n- Law of large numbers for sums over large populations to replace empirical averages by expectations under $p(\\phi)$.\n- First-order perturbation (delta method) for the angle of a random vector perturbed around its mean, using the small-angle approximation for $\\hat{\\theta}$ fluctuations around the mean resultant direction.\n\nYour tasks:\n1. From these fundamentals, derive the large-$N$ expressions needed to characterize bias and variance of the decoder:\n   - Define the mean complex population vector\n     $$\n     \\mu \\;=\\; \\mathbb{E}[S] \\;=\\; N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0)\\, e^{\\mathrm{i}\\,\\phi}\\, p(\\phi)\\, d\\phi.\n     $$\n     Define the asymptotic mean decoded angle $\\theta_\\star = \\mathrm{arg}\\,\\mu$, and the signed bias $b_{\\mathrm{asym}} = \\mathrm{wrap}(\\theta_\\star - \\theta_0)$ in radians, wrapped to $[-\\pi,\\pi)$.\n   - Using a first-order small-angle approximation around $\\theta_\\star$, derive that the asymptotic variance of the decoder can be expressed as\n     $$\n     \\mathrm{Var}(\\hat{\\theta}) \\;\\approx\\; \\frac{N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0)\\, \\sin^2(\\phi - \\theta_\\star)\\, p(\\phi)\\, d\\phi}{\\lvert \\mu \\rvert^2}.\n     $$\n     The numerator arises from independent Poisson variability projected onto the direction orthogonal to $\\mu$, and the denominator is the squared magnitude of the mean vector.\n   - You must not use any closed-form special-function shortcut for these integrals; instead, state them as integrals to be evaluated numerically.\n\n2. Implement a program that, for each parameter set in the test suite below, numerically computes the asymptotic signed bias $b_{\\mathrm{asym}}$ and the asymptotic variance $\\mathrm{Var}(\\hat{\\theta})$ by evaluating the above integrals with sufficiently fine numerical quadrature over $\\phi \\in [-\\pi,\\pi)$. Use double-precision arithmetic and a uniform grid of at least $4096$ points over $[-\\pi,\\pi)$.\n\n3. Angle units: all angles, including $\\theta_0$, $\\mu_d$, and $\\mu_g$, must be interpreted and returned in radians. Express the output bias in radians. The variance is unitless as it pertains to squared radians.\n\n4. Population density of preferred directions:\n   - Uniform: $p(\\phi) = \\frac{1}{2\\pi}$.\n   - Nonuniform von Mises: $p(\\phi) = \\dfrac{\\exp\\!\\big(\\kappa_d \\cos(\\phi - \\mu_d)\\big)}{2\\pi I_0(\\kappa_d)}$, where $I_0$ is the modified Bessel function of the first kind of order zero. You must numerically normalize $p(\\phi)$ over the grid; do not use closed-form $I_0$.\n\n5. Test suite. For each of the following cases, compute and return the pair $[b_{\\mathrm{asym}}, \\mathrm{Var}(\\hat{\\theta})]$:\n   - Case A (happy path, uniform preferred directions):\n     - $N = 60$, $\\theta_0 = 0.7$, $\\kappa = 2.0$, $b = 5.0$, $m = 15.0$, $\\varepsilon = 0.0$, $\\mu_g = 0.0$, $T = 0.25$, $p(\\phi)$ uniform.\n   - Case B (nonuniform preferred directions, clustered around $0$):\n     - $N = 100$, $\\theta_0 = 1.2$, $\\kappa = 2.0$, $b = 5.0$, $m = 10.0$, $\\varepsilon = 0.0$, $\\mu_g = 0.0$, $T = 0.5$, $p(\\phi)$ von Mises with $\\kappa_d = 1.5$, $\\mu_d = 0.0$.\n   - Case C (uniform preferred directions but heterogeneous gains):\n     - $N = 80$, $\\theta_0 = -1.4$, $\\kappa = 1.2$, $b = 8.0$, $m = 12.0$, $\\varepsilon = 0.3$, $\\mu_g = 0.5$, $T = 0.4$, $p(\\phi)$ uniform.\n   - Case D (boundary case near wrap-around, clustered near $\\pi$):\n     - $N = 120$, $\\theta_0 = 3.05$, $\\kappa = 3.0$, $b = 3.0$, $m = 9.0$, $\\varepsilon = 0.2$, $\\mu_g = 3.0$, $T = 0.3$, $p(\\phi)$ von Mises with $\\kappa_d = 2.0$, $\\mu_d = 3.0$.\n\n6. Final output format. Your program should produce a single line of output containing a single comma-separated list with the $8$ floating-point results for the above four cases, flattened in the following order and rounded to six decimal places:\n$$\n\\big[ b_A,\\; v_A,\\; b_B,\\; v_B,\\; b_C,\\; v_C,\\; b_D,\\; v_D \\big],\n$$\nwhere $b_\\cdot$ denotes the signed bias $b_{\\mathrm{asym}}$ for the indicated case, and $v_\\cdot$ denotes $\\mathrm{Var}(\\hat{\\theta})$ for the indicated case, all in radians for bias and in squared radians for variance. The printed line must match exactly the bracketed format with values separated by commas and no additional text.",
            "solution": "The problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It describes a canonical model in computational neuroscience for analyzing the performance of a population code. The tasks involve a standard, albeit detailed, derivation from first principles followed by a numerical implementation. The problem is therefore deemed valid. We proceed with the solution.\n\n### Theoretical Derivation\n\n**1. Model Specification**\nWe consider a population of $N$ head-direction neurons. The preferred direction of the $i$-th neuron is $\\phi_i \\in [-\\pi, \\pi)$, drawn from a probability density $p(\\phi)$. For a true head direction $\\theta_0$, neuron $i$ emits spikes as an independent Poisson process with a rate (in spikes/sec) given by the tuning curve:\n$$\n\\lambda(\\phi_i \\mid \\theta_0) = b + m(\\phi_i)\\, \\exp\\!\\big(\\kappa \\cos(\\theta_0 - \\phi_i)\\big)\n$$\nThe peak modulation $m(\\phi_i)$ may be heterogeneous:\n$$\nm(\\phi_i) = m \\big[1 + \\varepsilon \\cos(\\phi_i - \\mu_g)\\big]\n$$\nOver an observation interval of duration $T$, the spike count $r_i$ for neuron $i$ follows a Poisson distribution, $r_i \\sim \\mathrm{Poisson}(\\Lambda(\\phi_i \\mid \\theta_0))$, where the mean spike count is $\\Lambda(\\phi \\mid \\theta_0) = T\\,\\lambda(\\phi \\mid \\theta_0)$.\n\nThe population vector decoder estimates the head direction $\\hat{\\theta}$ from the angle of the complex vector $S$:\n$$\nS = \\sum_{i=1}^{N} r_i \\, e^{\\mathrm{i}\\,\\phi_i}, \\quad \\hat{\\theta} = \\mathrm{arg}\\,S\n$$\n\n**2. Asymptotic Bias**\nThe bias of the estimator is the difference between its expected value and the true value $\\theta_0$. For a nonlinear estimator like $\\hat{\\theta} = \\mathrm{arg}\\,S$, it is standard to approximate the expectation of the estimate, $\\mathbb{E}[\\hat{\\theta}]$, by the angle of the expected vector, $\\mathrm{arg}\\,\\mathbb{E}[S]$. This is the large-$N$ limit where the fluctuations of $S$ around its mean become small.\n\nWe define the mean complex population vector $\\mu = \\mathbb{E}[S]$. Using the linearity of expectation and the independence of neurons:\n$$\n\\mu = \\mathbb{E}\\left[\\sum_{i=1}^{N} r_i \\, e^{\\mathrm{i}\\,\\phi_i}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[r_i] \\, e^{\\mathrm{i}\\,\\phi_i}\n$$\nThe expectation of the spike count for neuron $i$ is $\\mathbb{E}[r_i] = \\Lambda(\\phi_i \\mid \\theta_0)$.\n$$\n\\mu = \\sum_{i=1}^{N} \\Lambda(\\phi_i \\mid \\theta_0) \\, e^{\\mathrm{i}\\,\\phi_i}\n$$\nIn the limit of a large population $N$, the law of large numbers allows us to replace the sum over the empirical sample of neurons with an expectation over the distribution of preferred directions $p(\\phi)$. This converts the sum into an integral:\n$$\n\\mu = N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0)\\, e^{\\mathrm{i}\\,\\phi}\\, p(\\phi)\\, d\\phi\n$$\nThis is the integral to be evaluated numerically.\nThe asymptotic decoded angle is defined as the angle of this mean vector:\n$$\n\\theta_\\star = \\mathrm{arg}\\,\\mu\n$$\nThe asymptotic signed bias, $b_{\\mathrm{asym}}$, is the wrapped difference between this asymptotic angle and the true stimulus angle, ensuring the result is in $[-\\pi, \\pi)$:\n$$\nb_{\\mathrm{asym}} = \\mathrm{wrap}(\\theta_\\star - \\theta_0) \\equiv (\\theta_\\star - \\theta_0 + \\pi) \\pmod{2\\pi} - \\pi\n$$\n\n**3. Asymptotic Variance**\nThe variance of the estimator, $\\mathrm{Var}(\\hat{\\theta})$, is analyzed using a first-order perturbation approximation, also known as the delta method. We consider small fluctuations, $\\delta S = S - \\mu$, of the population vector around its mean. The decoded angle is $\\hat{\\theta} = \\arg(\\mu + \\delta S)$.\n\nFor small $\\delta S$, the change in angle, $\\delta\\hat{\\theta} = \\hat{\\theta} - \\theta_\\star$, is primarily caused by the component of $\\delta S$ that is orthogonal to the mean vector $\\mu$. Writing $\\mu = |\\mu| e^{\\mathrm{i}\\theta_\\star}$, we can approximate the angle fluctuation as the projection of the fluctuation vector $\\delta S$ onto the direction orthogonal to $\\mu$, divided by the magnitude of $\\mu$. Let $\\delta S = \\delta S_x + \\mathrm{i}\\delta S_y$. The direction vector for $\\mu$ is $(\\cos\\theta_\\star, \\sin\\theta_\\star)$ and the orthogonal direction is $(-\\sin\\theta_\\star, \\cos\\theta_\\star)$. The fluctuation in angle is:\n$$\n\\delta\\hat{\\theta} \\approx \\frac{\\delta S_y \\cos\\theta_\\star - \\delta S_x \\sin\\theta_\\star}{|\\mu|}\n$$\nThe variance is $\\mathrm{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\delta\\hat{\\theta})^2]$ (since $\\mathbb{E}[\\delta\\hat{\\theta}] \\approx 0$ to first order).\n$$\n\\mathrm{Var}(\\hat{\\theta}) \\approx \\frac{1}{|\\mu|^2} \\mathrm{Var}\\left( S_y \\cos\\theta_\\star - S_x \\sin\\theta_\\star \\right)\n$$\nwhere $S_x = \\sum_i r_i \\cos\\phi_i$ and $S_y = \\sum_i r_i \\sin\\phi_i$. The variance of the linear combination is:\n$$\n\\mathrm{Var}\\left( S_y \\cos\\theta_\\star - S_x \\sin\\theta_\\star \\right) = \\cos^2\\theta_\\star \\mathrm{Var}(S_y) + \\sin^2\\theta_\\star \\mathrm{Var}(S_x) - 2\\sin\\theta_\\star\\cos\\theta_\\star \\mathrm{Cov}(S_x, S_y)\n$$\nSince the neurons are independent, the variances and covariances of $S_x$ and $S_y$ are sums of the contributions from individual neurons:\n$$\n\\mathrm{Var}(S_x) = \\sum_{i=1}^N \\mathrm{Var}(r_i \\cos\\phi_i) = \\sum_{i=1}^N \\cos^2\\phi_i \\mathrm{Var}(r_i)\n$$\n$$\n\\mathrm{Var}(S_y) = \\sum_{i=1}^N \\mathrm{Var}(r_i \\sin\\phi_i) = \\sum_{i=1}^N \\sin^2\\phi_i \\mathrm{Var}(r_i)\n$$\n$$\n\\mathrm{Cov}(S_x, S_y) = \\sum_{i=1}^N \\mathrm{Cov}(r_i \\cos\\phi_i, r_i \\sin\\phi_i) = \\sum_{i=1}^N \\cos\\phi_i \\sin\\phi_i \\mathrm{Var}(r_i)\n$$\nA key property of the Poisson distribution is that the variance equals the mean: $\\mathrm{Var}(r_i) = \\mathbb{E}[r_i] = \\Lambda(\\phi_i \\mid \\theta_0)$. Substituting this and taking the large-$N$ limit by converting sums to integrals:\n$$\n\\mathrm{Var}(S_x) \\approx N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) \\cos^2\\phi \\, p(\\phi) \\, d\\phi\n$$\n$$\n\\mathrm{Var}(S_y) \\approx N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) \\sin^2\\phi \\, p(\\phi) \\, d\\phi\n$$\n$$\n\\mathrm{Cov}(S_x, S_y) \\approx N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) \\cos\\phi \\sin\\phi \\, p(\\phi) \\, d\\phi\n$$\nSubstituting these back into the expression for $\\mathrm{Var}(S_y \\cos\\theta_\\star - S_x \\sin\\theta_\\star)$:\n\\begin{align*}\n\\text{Numerator} &= N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) p(\\phi) \\left[ \\cos^2\\theta_\\star \\sin^2\\phi + \\sin^2\\theta_\\star \\cos^2\\phi - 2\\sin\\theta_\\star\\cos\\theta_\\star\\cos\\phi\\sin\\phi \\right] d\\phi \\\\\n&= N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) p(\\phi) \\left( \\sin\\phi\\cos\\theta_\\star - \\cos\\phi\\sin\\theta_\\star \\right)^2 d\\phi \\\\\n&= N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0) p(\\phi) \\sin^2(\\phi - \\theta_\\star) d\\phi\n\\end{align*}\nFinally, the asymptotic variance is the ratio of this numerator to the squared magnitude of the mean vector, $|\\mu|^2$:\n$$\n\\mathrm{Var}(\\hat{\\theta}) \\approx \\frac{N \\int_{-\\pi}^{\\pi} \\Lambda(\\phi \\mid \\theta_0)\\, \\sin^2(\\phi - \\theta_\\star)\\, p(\\phi)\\, d\\phi}{\\lvert \\mu \\rvert^2}\n$$\nThis completes the derivation of the required expressions. They are now ready for numerical implementation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the asymptotic bias and variance of a population vector decoder\n    for head direction cells based on a set of test cases.\n    \"\"\"\n\n    def compute_decoder_performance(params, num_points=4097):\n        \"\"\"\n        Calculates the bias and variance for a single parameter set.\n\n        Args:\n            params (dict): A dictionary containing all model parameters.\n            num_points (int): The number of points for the numerical integration grid.\n\n        Returns:\n            tuple: A tuple containing the asymptotic bias (radians) and variance.\n        \"\"\"\n        # Unpack parameters\n        N = params['N']\n        theta_0 = params['theta_0']\n        kappa = params['kappa']\n        b = params['b']\n        m = params['m']\n        epsilon = params['epsilon']\n        mu_g = params['mu_g']\n        T = params['T']\n        p_phi_type = params['p_phi_type']\n        kappa_d = params['kappa_d']\n        mu_d = params['mu_d']\n        \n        # Set up the integration grid for phi\n        phi = np.linspace(-np.pi, np.pi, num_points)\n\n        # 1. Define the probability density of preferred directions, p(phi)\n        if p_phi_type == 'uniform':\n            p_phi = np.full_like(phi, 1.0 / (2 * np.pi))\n        else:  # 'von_mises'\n            # Numerically normalize p(phi) as required by the problem statement\n            unnormalized_p_phi = np.exp(kappa_d * np.cos(phi - mu_d))\n            norm_const = np.trapz(unnormalized_p_phi, phi)\n            p_phi = unnormalized_p_phi / norm_const\n\n        # 2. Define the heterogeneous modulation term, m(phi)\n        m_phi = m * (1 + epsilon * np.cos(phi - mu_g))\n\n        # 3. Define the mean total spike count, Lambda(phi|theta_0)\n        Lambda_phi = T * (b + m_phi * np.exp(kappa * np.cos(theta_0 - phi)))\n\n        # 4. Compute the mean complex population vector, mu\n        integrand_mu = Lambda_phi * np.exp(1j * phi) * p_phi\n        mu_per_neuron = np.trapz(integrand_mu, phi)\n        mu = N * mu_per_neuron\n\n        # 5. Compute the asymptotic bias\n        theta_star = np.angle(mu)\n        bias = theta_star - theta_0\n        # Wrap the bias to the interval [-pi, pi)\n        bias = (bias + np.pi) % (2 * np.pi) - np.pi\n\n        # 6. Compute the asymptotic variance\n        mu_magnitude_sq = np.abs(mu)**2\n        \n        # Numerator of the variance expression\n        integrand_var = Lambda_phi * (np.sin(phi - theta_star))**2 * p_phi\n        numerator_integral = np.trapz(integrand_var, phi)\n        numerator = N * numerator_integral\n\n        if mu_magnitude_sq == 0:\n            # This case is unlikely with the given parameters\n            variance = np.inf\n        else:\n            variance = numerator / mu_magnitude_sq\n            \n        return bias, variance\n\n    # Test cases as defined in the problem description\n    test_cases = [\n        # Case A: Uniform p(phi), no heterogeneity\n        {\n            'N': 60, 'theta_0': 0.7, 'kappa': 2.0, 'b': 5.0, 'm': 15.0, \n            'epsilon': 0.0, 'mu_g': 0.0, 'T': 0.25, 'p_phi_type': 'uniform', \n            'kappa_d': None, 'mu_d': None\n        },\n        # Case B: Non-uniform p(phi)\n        {\n            'N': 100, 'theta_0': 1.2, 'kappa': 2.0, 'b': 5.0, 'm': 10.0, \n            'epsilon': 0.0, 'mu_g': 0.0, 'T': 0.5, 'p_phi_type': 'von_mises', \n            'kappa_d': 1.5, 'mu_d': 0.0\n        },\n        # Case C: Uniform p(phi), heterogeneous gain\n        {\n            'N': 80, 'theta_0': -1.4, 'kappa': 1.2, 'b': 8.0, 'm': 12.0, \n            'epsilon': 0.3, 'mu_g': 0.5, 'T': 0.4, 'p_phi_type': 'uniform', \n            'kappa_d': None, 'mu_d': None\n        },\n        # Case D: Non-uniform p(phi) and heterogeneous gain\n        {\n            'N': 120, 'theta_0': 3.05, 'kappa': 3.0, 'b': 3.0, 'm': 9.0, \n            'epsilon': 0.2, 'mu_g': 3.0, 'T': 0.3, 'p_phi_type': 'von_mises', \n            'kappa_d': 2.0, 'mu_d': 3.0\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        bias, variance = compute_decoder_performance(params)\n        results.extend([bias, variance])\n\n    # Print the final output in the required format\n    print(f\"[{','.join(f'{val:.6f}' for val in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Navigation requires integrating egocentric sensory information (like the distance and direction to an object) with our allocentric sense of direction to build a stable map of the world. However, all these signals are inherently noisy. This practice explores how uncertainty propagates when transforming from an egocentric to an allocentric reference frame, revealing how the geometry of this transformation itself shapes the structure of estimation error .",
            "id": "3998156",
            "problem": "A mobile agent estimates the allocentric location of a visual landmark using egocentric sensorimotor variables in its spatial navigation system. The agent represents the landmark in egocentric polar coordinates by a range $\\rho$ and a bearing $\\beta$ relative to its head-centered frame, and it also maintains an estimate of its allocentric head direction $\\phi$. The allocentric Cartesian coordinates $x$ and $y$ of the landmark, expressed in a world-centered frame, are given by the smooth mapping $x = x(\\rho,\\beta,\\phi)$ and $y = y(\\rho,\\beta,\\phi)$ determined by the composition of a rotation by $\\phi$ and the conversion from polar to Cartesian coordinates. Assume that the egocentric measurements are corrupted by small, zero-mean, independent Gaussian noise with covariance matrix $\\Sigma_{\\text{ego}} = \\operatorname{diag}(\\sigma_{\\rho}^{2},\\,\\sigma_{\\beta}^{2},\\,\\sigma_{\\phi}^{2})$, and that the allocentric estimate is formed by substituting the noisy variables into the smooth mapping.\n\nStarting from the first-order approximation of how small perturbations propagate through a differentiable transformation, derive the allocentric covariance of $\\begin{pmatrix}x \\\\ y\\end{pmatrix}$. Then, interpret how the choice of coordinate frame (egocentric polar plus head direction versus allocentric Cartesian) shapes the principal directions of uncertainty amplification. Finally, provide a single closed-form analytic expression for the largest eigenvalue of the allocentric covariance matrix in terms of $\\rho$, $\\sigma_{\\rho}^{2}$, $\\sigma_{\\beta}^{2}$, and $\\sigma_{\\phi}^{2}$. No rounding is required, and no units are to be included in the final expression.",
            "solution": "The problem is deemed valid as it is scientifically grounded in estimation theory and computational neuroscience, well-posed with a clear objective and sufficient information, and free of contradictions or ambiguities.\n\nThe task is to derive the allocentric covariance matrix for the estimated landmark position, interpret the structure of the uncertainty, and find the largest eigenvalue of this covariance matrix.\n\nFirst, we establish the explicit transformation from the sensorimotor variables $(\\rho, \\beta, \\phi)$ to the allocentric Cartesian coordinates $(x, y)$. The variable $\\rho$ is the egocentric range to the landmark, $\\beta$ is the egocentric bearing (angle) relative to the agent's head direction, and $\\phi$ is the allocentric head direction. The total allocentric angle of the landmark relative to the world frame's x-axis is the sum of the head direction and the egocentric bearing, which is $\\phi + \\beta$. The transformation from polar coordinates $(\\text{distance}, \\text{angle})$ to Cartesian coordinates $(x,y)$ is $x = \\text{distance} \\times \\cos(\\text{angle})$ and $y = \\text{distance} \\times \\sin(\\text{angle})$. Applying this, we get the mapping:\n$$x(\\rho, \\beta, \\phi) = \\rho \\cos(\\phi + \\beta)$$\n$$y(\\rho, \\beta, \\phi) = \\rho \\sin(\\phi + \\beta)$$\n\nThe problem states that the egocentric variables are corrupted by small, zero-mean, independent Gaussian noise. Let the vector of these variables be $v = \\begin{pmatrix} \\rho \\\\ \\beta \\\\ \\phi \\end{pmatrix}$. The covariance matrix for these variables is given as:\n$$\\Sigma_{\\text{ego}} = \\begin{pmatrix} \\sigma_{\\rho}^{2} & 0 & 0 \\\\ 0 & \\sigma_{\\beta}^{2} & 0 \\\\ 0 & 0 & \\sigma_{\\phi}^{2} \\end{pmatrix}$$\nTo find the covariance of the allocentric coordinates $f(v) = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$, we use the first-order approximation for the propagation of uncertainty. For a small perturbation $\\delta v$ in the input variables, the perturbation in the output $\\delta f$ is approximately $\\delta f \\approx J \\delta v$, where $J$ is the Jacobian matrix of the transformation $f$. The covariance matrix of the output, $\\Sigma_{\\text{allo}}$, is then given by:\n$$\\Sigma_{\\text{allo}} = E[\\delta f \\delta f^T] \\approx E[J \\delta v (J \\delta v)^T] = J E[\\delta v \\delta v^T] J^T = J \\Sigma_{\\text{ego}} J^T$$\nThe Jacobian matrix $J$ is a $2 \\times 3$ matrix of partial derivatives:\n$$J = \\frac{\\partial(x, y)}{\\partial(\\rho, \\beta, \\phi)} =\n\\begin{pmatrix}\n\\frac{\\partial x}{\\partial \\rho} & \\frac{\\partial x}{\\partial \\beta} & \\frac{\\partial x}{\\partial \\phi} \\\\\n\\frac{\\partial y}{\\partial \\rho} & \\frac{\\partial y}{\\partial \\beta} & \\frac{\\partial y}{\\partial \\phi}\n\\end{pmatrix}$$\nWe compute the required partial derivatives:\n$\\frac{\\partial x}{\\partial \\rho} = \\cos(\\phi + \\beta)$\n$\\frac{\\partial x}{\\partial \\beta} = -\\rho \\sin(\\phi + \\beta)$\n$\\frac{\\partial x}{\\partial \\phi} = -\\rho \\sin(\\phi + \\beta)$\n$\\frac{\\partial y}{\\partial \\rho} = \\sin(\\phi + \\beta)$\n$\\frac{\\partial y}{\\partial \\beta} = \\rho \\cos(\\phi + \\beta)$\n$\\frac{\\partial y}{\\partial \\phi} = \\rho \\cos(\\phi + \\beta)$\n\nThe Jacobian matrix is therefore:\n$$J = \\begin{pmatrix}\n\\cos(\\phi + \\beta) & -\\rho \\sin(\\phi + \\beta) & -\\rho \\sin(\\phi + \\beta) \\\\\n\\sin(\\phi + \\beta) & \\rho \\cos(\\phi + \\beta) & \\rho \\cos(\\phi + \\beta)\n\\end{pmatrix}$$\nNow we compute the product $J\\Sigma_{\\text{ego}}J^T$.\n$$\\Sigma_{\\text{allo}} = (J\\Sigma_{\\text{ego}})J^T = \\begin{pmatrix}\n\\sigma_{\\rho}^{2}\\cos(\\phi + \\beta) & -\\sigma_{\\beta}^{2}\\rho \\sin(\\phi + \\beta) & -\\sigma_{\\phi}^{2}\\rho \\sin(\\phi + \\beta) \\\\\n\\sigma_{\\rho}^{2}\\sin(\\phi + \\beta) & \\sigma_{\\beta}^{2}\\rho \\cos(\\phi + \\beta) & \\sigma_{\\phi}^{2}\\rho \\cos(\\phi + \\beta)\n\\end{pmatrix} \\begin{pmatrix}\n\\cos(\\phi + \\beta) & \\sin(\\phi + \\beta) \\\\\n-\\rho \\sin(\\phi + \\beta) & \\rho \\cos(\\phi + \\beta) \\\\\n-\\rho \\sin(\\phi + \\beta) & \\rho \\cos(\\phi + \\beta)\n\\end{pmatrix}$$\nLet $c = \\cos(\\phi+\\beta)$ and $s = \\sin(\\phi+\\beta)$.\nThe $(1,1)$ element of $\\Sigma_{\\text{allo}}$ is:\n$\\Sigma_{11} = \\sigma_{\\rho}^{2}c^2 + (\\sigma_{\\beta}^{2}+\\sigma_{\\phi}^{2})\\rho^2 s^2$\nThe $(2,2)$ element of $\\Sigma_{\\text{allo}}$ is:\n$\\Sigma_{22} = \\sigma_{\\rho}^{2}s^2 + (\\sigma_{\\beta}^{2}+\\sigma_{\\phi}^{2})\\rho^2 c^2$\nThe $(1,2)$ (and $(2,1)$) element of $\\Sigma_{\\text{allo}}$ is:\n$\\Sigma_{12} = (\\sigma_{\\rho}^{2} - (\\sigma_{\\beta}^{2}+\\sigma_{\\phi}^{2})\\rho^2)sc$\nThis completes the derivation of the allocentric covariance matrix.\n\nFor the interpretation, it is insightful to consider a local coordinate frame aligned with the landmark. Let this frame have its u-axis pointing radially from the agent to the landmark, and its v-axis tangentially. This frame is rotated by $\\theta = \\phi+\\beta$ with respect to the world frame. In this $(u,v)$ frame, the uncertainty is simpler to describe. A small error in range, $\\delta\\rho$, causes an error only along the radial u-axis. A small error in angle, coming from either $\\beta$ or $\\phi$, causes an error along the tangential v-axis. Since $\\beta$ and $\\phi$ are independent, the variance of their sum is the sum of their variances: $\\sigma_{\\text{angle}}^2 = \\sigma_{\\beta}^{2} + \\sigma_{\\phi}^{2}$.\nThe variance contribution in the radial direction is simply $\\sigma_{u}^2 = \\sigma_{\\rho}^{2}$.\nThe variance contribution in the tangential direction is $(\\rho \\times \\text{angular error})^2$, so $\\sigma_{v}^2 = \\rho^2 \\sigma_{\\text{angle}}^2 = \\rho^2(\\sigma_{\\beta}^{2} + \\sigma_{\\phi}^{2})$.\nIn this radial-tangential frame, the uncertainties are decoupled, and the covariance matrix is diagonal:\n$$\\Sigma_{uv} = \\begin{pmatrix} \\sigma_{\\rho}^{2} & 0 \\\\ 0 & \\rho^2(\\sigma_{\\beta}^{2} + \\sigma_{\\phi}^{2}) \\end{pmatrix}$$\nThe principal directions of uncertainty are thus aligned with the radial and tangential directions relative to the agent's location. The choice of an egocentric polar coordinate system is \"natural\" because its measurement axes (range and bearing) map cleanly onto these principal geometric axes of uncertainty (radial and tangential). The allocentric covariance matrix $\\Sigma_{\\text{allo}}$ is simply a rotation of this diagonal matrix $\\Sigma_{uv}$ by the angle $\\phi+\\beta$.\n\nFinally, we need to find the largest eigenvalue of $\\Sigma_{\\text{allo}}$. The eigenvalues of a matrix are invariant under rotation (change of basis). Therefore, the eigenvalues of $\\Sigma_{\\text{allo}}$ are the same as the eigenvalues of the simpler diagonal matrix $\\Sigma_{uv}$. For a diagonal matrix, the eigenvalues are its diagonal entries:\n$$\\lambda_1 = \\sigma_{\\rho}^{2}$$\n$$\\lambda_2 = \\rho^2(\\sigma_{\\beta}^{2} + \\sigma_{\\phi}^{2})$$\nThe problem asks for a single closed-form analytic expression for the largest eigenvalue, $\\lambda_{\\max}$. This is given by $\\max(\\lambda_1, \\lambda_2)$. A general formula for the maximum of two numbers $a$ and $b$ is $\\max(a, b) = \\frac{1}{2}(a+b+|a-b|)$. Applying this formula, we get:\n$$\\lambda_{\\max} = \\frac{1}{2} \\left( \\sigma_{\\rho}^{2} + \\rho^2(\\sigma_{\\beta}^{2} + \\sigma_{\\phi}^{2}) + \\left|\\sigma_{\\rho}^{2} - \\rho^2(\\sigma_{\\beta}^{2} + \\sigma_{\\phi}^{2})\\right| \\right)$$\nThis expression provides the largest eigenvalue in terms of the given parameters.",
            "answer": "$$\\boxed{\\frac{1}{2} \\left( \\sigma_{\\rho}^{2} + \\rho^{2}(\\sigma_{\\beta}^{2} + \\sigma_{\\phi}^{2}) + \\left|\\sigma_{\\rho}^{2} - \\rho^{2}(\\sigma_{\\beta}^{2} + \\sigma_{\\phi}^{2})\\right| \\right)}$$"
        },
        {
            "introduction": "Path integration, or dead reckoning, is the process of updating our position by integrating self-motion cues, but it is notoriously prone to error accumulation. This final exercise formalizes this process to analyze how noise in speed and heading measurements leads to growing position uncertainty. You will then investigate the power of intermittent landmark-based corrections, a crucial mechanism the brain uses to create a robust and globally consistent sense of place .",
            "id": "3998165",
            "problem": "Consider a two-dimensional planar path-integration estimator for spatial navigation, as is standard in models of hippocampal and entorhinal systems. At discrete time step $k \\in \\{1,\\dots,T\\}$, the estimator updates its position by integrating speed and heading according to the rule\n$$\n\\widehat{\\boldsymbol{x}}_{k} = \\widehat{\\boldsymbol{x}}_{k-1} + v_k \\, \\Delta t \\, \\begin{bmatrix}\\cos \\theta_k \\\\ \\sin \\theta_k \\end{bmatrix},\n$$\nwhere $\\Delta t$ is the fixed step duration in seconds, $v_k$ is the measured speed in meters per second, and $\\theta_k$ is the measured heading angle in radians. The true motion is a constant-speed, constant-heading trajectory with speed $v_0$ and heading $\\theta_0$, so the true displacement per step is\n$$\n\\boldsymbol{d}_0 = v_0 \\, \\Delta t \\, \\begin{bmatrix}\\cos \\theta_0 \\\\ \\sin \\theta_0 \\end{bmatrix}.\n$$\nMeasurements are corrupted by zero-mean Gaussian noise that is independent across time and between speed and heading:\n$$\nv_k = v_0 + \\varepsilon_{v,k}, \\quad \\varepsilon_{v,k} \\sim \\mathcal{N}(0, \\sigma_v^2),\n$$\n$$\n\\theta_k = \\theta_0 + \\varepsilon_{\\theta,k}, \\quad \\varepsilon_{\\theta,k} \\sim \\mathcal{N}(0, \\sigma_\\theta^2),\n$$\nwith $(\\varepsilon_{v,k})$ and $(\\varepsilon_{\\theta,k})$ independent sequences and independent of each other. The estimation error at step $T$ without any landmark corrections is\n$$\n\\boldsymbol{e}_T^{\\mathrm{nc}} = \\widehat{\\boldsymbol{x}}_{T} - \\boldsymbol{x}_{T},\n$$\nwhere $\\boldsymbol{x}_{T} = \\boldsymbol{x}_{0} + T \\boldsymbol{d}_0$ is the true position.\n\nNow consider intermittent landmark corrections every $M$ steps. At correction times $k \\in \\{M, 2M, 3M, \\dots\\}$, the estimator receives a landmark-based position measurement that resets the position estimate to\n$$\n\\widehat{\\boldsymbol{x}}_{k} \\leftarrow \\boldsymbol{x}_{k} + \\boldsymbol{\\eta}_{k},\n$$\nwhere $\\boldsymbol{\\eta}_{k} \\sim \\mathcal{N}\\!\\left(\\boldsymbol{0}, \\sigma_L^2 \\boldsymbol{I}_2\\right)$ is an isotropic zero-mean Gaussian correction noise in meters, independent of all other noise and independent across corrections, and $\\boldsymbol{I}_2$ is the $2 \\times 2$ identity matrix. Between corrections, the estimator continues to integrate $v_k$ and $\\theta_k$ as above. The estimation error at step $T$ with corrections is denoted\n$$\n\\boldsymbol{e}_T^{\\mathrm{wc}} = \\widehat{\\boldsymbol{x}}_{T} - \\boldsymbol{x}_{T}.\n$$\n\nYour tasks:\n1. Starting from the discrete-time path-integration rule, independence of Gaussian noise, and standard expectations for trigonometric functions of Gaussian variables, derive exact expressions for the bias vector $\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]$ and the covariance matrix $\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}})$ as functions of $T$, $\\Delta t$, $v_0$, $\\theta_0$, $\\sigma_v$, and $\\sigma_\\theta$. Then, derive the corresponding bias vector and covariance matrix for $\\boldsymbol{e}_T^{\\mathrm{wc}}$ under intermittent landmark corrections with period $M$ and correction noise variance $\\sigma_L^2$.\n2. Based on your derivations, implement a program that computes, for each test case, the Euclidean norm of the bias vector $\\|\\mathbb{E}[\\boldsymbol{e}_T]\\|$ in meters and the trace of the covariance matrix $\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T))$ in square meters, both without corrections and with corrections. All angles must be in radians. Express speeds in meters per second, time in seconds, positions in meters, and covariance traces in square meters. Do not round; output raw floating-point numbers.\n3. Compare scaling laws: use your formulas to characterize how the bias norm and covariance trace scale with $T$ without corrections and with intermittent corrections (perfect and noisy), and ensure the program numerically reflects these laws for the given test suite.\n\nTest suite (each case is a tuple $(v_0, \\theta_0, \\Delta t, T, \\sigma_v, \\sigma_\\theta, M, \\sigma_L)$ with $v_0$ in meters per second, $\\theta_0$ in radians, $\\Delta t$ in seconds, $T$ in steps, $\\sigma_v$ in meters per second, $\\sigma_\\theta$ in radians, $M$ in steps, and $\\sigma_L$ in meters):\n- Case 1 (general case): $(1.2, 0.5, 0.1, 100, 0.15, 0.05, 20, 0.0)$.\n- Case 2 (boundary case with zero heading noise): $(0.8, 1.0, 0.05, 200, 0.2, 0.0, 25, 0.0)$.\n- Case 3 (stronger heading noise with noisy corrections): $\\left(1.0, \\frac{\\pi}{3}, 0.2, 300, 0.05, 0.2, 30, 0.02\\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order for each case: $[\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]\\|,\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}})),\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]\\|,\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}}))]$. Concatenate all three cases in order, resulting in a list of $12$ floats. For example, the output format must be:\n$$\n[\\text{case1\\_bias\\_nc},\\text{case1\\_trace\\_nc},\\text{case1\\_bias\\_wc},\\text{case1\\_trace\\_wc},\\text{case2\\_bias\\_nc},\\dots,\\text{case3\\_trace\\_wc}].\n$$",
            "solution": "### Derivation of Bias and Covariance\n\nWe are tasked with finding the bias vector $\\mathbb{E}[\\boldsymbol{e}_T]$ and the covariance matrix $\\mathrm{Cov}(\\boldsymbol{e}_T)$ for the estimation error, both without and with landmark corrections.\n\n#### 1. Analysis Without Corrections (nc)\n\nAssuming the initial estimate is perfect, $\\widehat{\\boldsymbol{x}}_0 = \\boldsymbol{x}_0$, the error at step $T$ is the sum of single-step errors:\n$$\n\\boldsymbol{e}_T^{\\mathrm{nc}} = \\widehat{\\boldsymbol{x}}_T - \\boldsymbol{x}_T = \\sum_{k=1}^T \\left( \\widehat{\\boldsymbol{d}}_k - \\boldsymbol{d}_0 \\right)\n$$\nwhere $\\widehat{\\boldsymbol{d}}_k = v_k \\Delta t \\begin{bmatrix} \\cos\\theta_k \\\\ \\sin\\theta_k \\end{bmatrix}$ is the estimated displacement and $\\boldsymbol{d}_0 = v_0 \\Delta t \\begin{bmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 \\end{bmatrix}$ is the true displacement. Let $\\boldsymbol{\\delta}_k = \\widehat{\\boldsymbol{d}}_k - \\boldsymbol{d}_0$ be the single-step error. Since the noise terms $(\\varepsilon_{v,k}, \\varepsilon_{\\theta,k})$ are independent and identically distributed (i.i.d.) over time, the single-step errors $\\boldsymbol{\\delta}_k$ are also i.i.d.\n\n**Bias Vector $\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]$**\nBy linearity of expectation, $\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}] = \\sum_{k=1}^T \\mathbb{E}[\\boldsymbol{\\delta}_k] = T \\mathbb{E}[\\boldsymbol{\\delta}_1]$. We compute $\\mathbb{E}[\\boldsymbol{\\delta}_1] = \\mathbb{E}[\\widehat{\\boldsymbol{d}}_1] - \\boldsymbol{d}_0$.\n$$\n\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1] = \\Delta t \\, \\mathbb{E}\\left[ v_1 \\begin{bmatrix} \\cos\\theta_1 \\\\ \\sin\\theta_1 \\end{bmatrix} \\right] = \\Delta t \\begin{bmatrix} \\mathbb{E}[v_1 \\cos\\theta_1] \\\\ \\mathbb{E}[v_1 \\sin\\theta_1] \\end{bmatrix}\n$$\nSince $v_1$ and $\\theta_1$ are independent, $\\mathbb{E}[v_1 \\cos\\theta_1] = \\mathbb{E}[v_1]\\mathbb{E}[\\cos\\theta_1]$. We have $\\mathbb{E}[v_1] = \\mathbb{E}[v_0 + \\varepsilon_{v,1}] = v_0$.\nFor a Gaussian variable $\\theta_1 \\sim \\mathcal{N}(\\theta_0, \\sigma_\\theta^2)$, we use the standard results: $\\mathbb{E}[\\cos\\theta_1] = \\cos(\\theta_0) e^{-\\sigma_\\theta^2/2}$ and $\\mathbb{E}[\\sin\\theta_1] = \\sin(\\theta_0) e^{-\\sigma_\\theta^2/2}$.\n$$\n\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1] = \\Delta t \\, v_0 e^{-\\sigma_\\theta^2/2} \\begin{bmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 \\end{bmatrix} = e^{-\\sigma_\\theta^2/2} \\boldsymbol{d}_0\n$$\nThe bias per step is $\\mathbb{E}[\\boldsymbol{\\delta}_1] = (e^{-\\sigma_\\theta^2/2} - 1)\\boldsymbol{d}_0$. The total bias is:\n$$\n\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}] = T(e^{-\\sigma_\\theta^2/2} - 1)\\boldsymbol{d}_0 = T v_0 \\Delta t (e^{-\\sigma_\\theta^2/2} - 1) \\begin{bmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 \\end{bmatrix}\n$$\nThe Euclidean norm of the bias vector is:\n$$\n\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]\\| = |T v_0 \\Delta t (e^{-\\sigma_\\theta^2/2} - 1)| = T v_0 \\Delta t (1 - e^{-\\sigma_\\theta^2/2})\n$$\n\n**Covariance Matrix $\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}})$**\nSince the step errors $\\boldsymbol{\\delta}_k$ are i.i.d., $\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}}) = \\sum_{k=1}^T \\mathrm{Cov}(\\boldsymbol{\\delta}_k) = T \\mathrm{Cov}(\\boldsymbol{\\delta}_1)$. Also $\\mathrm{Cov}(\\boldsymbol{\\delta}_1) = \\mathrm{Cov}(\\widehat{\\boldsymbol{d}}_1)$. The trace is rotation-invariant, which simplifies the calculation.\n$$\n\\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\boldsymbol{d}}_1)) = \\mathrm{tr}(\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1 \\widehat{\\boldsymbol{d}}_1^T] - \\mathbb{E}[\\widehat{\\boldsymbol{d}}_1]\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1]^T) = \\mathbb{E}[\\|\\widehat{\\boldsymbol{d}}_1\\|^2] - \\|\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1]\\|^2\n$$\nWe have $\\|\\widehat{\\boldsymbol{d}}_1\\|^2 = (v_1 \\Delta t)^2$. So, $\\mathbb{E}[\\|\\widehat{\\boldsymbol{d}}_1\\|^2] = (\\Delta t)^2 \\mathbb{E}[v_1^2] = (\\Delta t)^2(v_0^2 + \\sigma_v^2)$.\nWe also have $\\|\\mathbb{E}[\\widehat{\\boldsymbol{d}}_1]\\|^2 = \\|e^{-\\sigma_\\theta^2/2} \\boldsymbol{d}_0\\|^2 = (e^{-\\sigma_\\theta^2/2})^2 \\|v_0 \\Delta t \\begin{bmatrix}\\cos\\theta_0 \\\\ \\sin\\theta_0\\end{bmatrix}\\|^2 = e^{-\\sigma_\\theta^2} (v_0 \\Delta t)^2$.\nThus, the trace of the single-step covariance is:\n$$\n\\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\boldsymbol{d}}_1)) = (\\Delta t)^2(v_0^2 + \\sigma_v^2) - e^{-\\sigma_\\theta^2}(v_0\\Delta t)^2 = (\\Delta t)^2 [v_0^2(1 - e^{-\\sigma_\\theta^2}) + \\sigma_v^2]\n$$\nThe trace of the total error covariance is:\n$$\n\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}})) = T (\\Delta t)^2 [v_0^2(1 - e^{-\\sigma_\\theta^2}) + \\sigma_v^2]\n$$\n\n#### 2. Analysis With Corrections (wc)\n\nLandmark corrections reset the estimate every $M$ steps. The error at time $T$ depends on the number of steps since the last correction.\nLet $k_{last} = M \\lfloor T/M \\rfloor$ be the time of the last correction at or before $T$. If $T < M$, no correction has occurred, and $k_{last}=0$. The number of steps since the last reset is $T_{rem} = T - k_{last}$.\n\nCase 1: $T < M$. No corrections have happened. The error is identical to the no-correction case.\n$$\n\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]\\| = \\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]\\| \\quad \\text{and} \\quad \\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}})) = \\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}}))\n$$\n\nCase 2: $T \\ge M$. The error at time $T$ originates from the error at the last correction time $k_{last}$ and the path integration from $k_{last}+1$ to $T$.\n$$\n\\boldsymbol{e}_T^{\\mathrm{wc}} = (\\widehat{\\boldsymbol{x}}_{k_{last}} - \\boldsymbol{x}_{k_{last}}) + \\sum_{j=1}^{T_{rem}} (\\widehat{\\boldsymbol{d}}_{k_{last}+j} - \\boldsymbol{d}_0)\n$$\nThe first term is the error immediately after correction. If $k_{last} > 0$, this is $\\boldsymbol{\\eta}_{k_{last}}$. If $k_{last}=0$ (i.e. $T<M$), this is $\\boldsymbol{0}$. The second term is an independent path integration error accumulated over $T_{rem}$ steps, which we can denote $\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}}$.\n$$\n\\boldsymbol{e}_T^{\\mathrm{wc}} = \\boldsymbol{\\eta}_{k_{last}} + \\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}} \\quad (\\text{for } T \\ge M \\text{, where } \\boldsymbol{\\eta}_0 \\equiv \\boldsymbol{0})\n$$\n\n**Bias Vector $\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]$**\nSince $\\mathbb{E}[\\boldsymbol{\\eta}_{k_{last}}] = \\boldsymbol{0}$, we have $\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}] = \\mathbb{E}[\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}}]$.\n$$\n\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]\\| = \\|\\mathbb{E}[\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}}]\\| = T_{rem} v_0 \\Delta t (1 - e^{-\\sigma_\\theta^2/2})\n$$\nwhere $T_{rem} = T \\pmod M$. If $T$ is a multiple of $M$, then $T_{rem}=0$ and the bias norm is $0$.\n\n**Covariance Matrix $\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}})$**\nSince $\\boldsymbol{\\eta}_{k_{last}}$ and $\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}}$ are independent:\n$$\n\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}}) = \\mathrm{Cov}(\\boldsymbol{\\eta}_{k_{last}}) + \\mathrm{Cov}(\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}})\n$$\nFor $T \\ge M$, $k_{last} > 0$, so $\\mathrm{Cov}(\\boldsymbol{\\eta}_{k_{last}}) = \\sigma_L^2 \\boldsymbol{I}_2$.\n$$\n\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}})) = \\mathrm{tr}(\\sigma_L^2 \\boldsymbol{I}_2) + \\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_{T_{rem}}^{\\mathrm{nc}})) = 2\\sigma_L^2 + T_{rem} (\\Delta t)^2 [v_0^2(1 - e^{-\\sigma_\\theta^2}) + \\sigma_v^2]\n$$\nwhere again $T_{rem} = T \\pmod M$. If $T_{rem}=0$, the second term vanishes, and the trace is simply $2\\sigma_L^2$.\n\n#### 3. Scaling Laws\n\n- **Without Corrections**: The bias norm grows linearly with time, $\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]\\| \\propto T$, and the error variance (trace of covariance) also grows linearly, $\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}})) \\propto T$. This is characteristic of a random walk, where the uncertainty grows without bound.\n- **With Corrections**: The error accumulation is reset every $M$ steps. Both the bias norm and the covariance trace are periodic functions of time with period $M$. Their values are bounded, and do not grow with $T$ over long periods, i.e., $\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]\\| = O(1)$ and $\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}})) = O(1)$ for large $T$. Intermittent corrections effectively tame the error growth.\n\n### Summary of Formulas for Implementation\n\nLet `path_integ_bias_norm(k)` and `path_integ_trace(k)` be the formulas for the no-correction case over $k$ steps.\n1.  $\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{nc}}]\\|$: `path_integ_bias_norm(T)`\n2.  $\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{nc}}))`: `path_integ_trace(T)`\n3.  $\\|\\mathbb{E}[\\boldsymbol{e}_T^{\\mathrm{wc}}]\\|$:\n    - If $T < M$: `path_integ_bias_norm(T)`\n    - If $T \\ge M$: `path_integ_bias_norm(T % M)`\n4.  $\\mathrm{tr}(\\mathrm{Cov}(\\boldsymbol{e}_T^{\\mathrm{wc}}))`:\n    - If $T < M$: `path_integ_trace(T)`\n    - If $T \\ge M$: $2\\sigma_L^2 + \\text{path\\_integ\\_trace}(T \\pmod M)$\n\nwhere\n`path_integ_bias_norm(k)` $= k \\cdot v_0 \\Delta t (1 - e^{-\\sigma_\\theta^2/2})$\n`path_integ_trace(k)` $= k \\cdot (\\Delta t)^2 [v_0^2(1 - e^{-\\sigma_\\theta^2}) + \\sigma_v^2]$\n\nThe program will implement this logic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the spatial navigation estimator problem for the given test suite.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1 (general case)\n        (1.2, 0.5, 0.1, 100, 0.15, 0.05, 20, 0.0),\n        # Case 2 (boundary case with zero heading noise)\n        (0.8, 1.0, 0.05, 200, 0.2, 0.0, 25, 0.0),\n        # Case 3 (stronger heading noise with noisy corrections)\n        (1.0, np.pi/3, 0.2, 300, 0.05, 0.2, 30, 0.02),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        v0, theta0, delta_t, T, sigma_v, sigma_theta, M, sigma_L = case\n\n        # Helper functions for base path integration error (no corrections)\n        def path_integ_bias_norm(k, v0, delta_t, sigma_theta):\n            if k == 0:\n                return 0.0\n            sigma_theta_sq = sigma_theta**2\n            # Formula: k * v0 * delta_t * (1 - exp(-sigma_theta^2 / 2))\n            return k * v0 * delta_t * (1.0 - np.exp(-sigma_theta_sq / 2.0))\n\n        def path_integ_trace(k, v0, delta_t, sigma_v, sigma_theta):\n            if k == 0:\n                return 0.0\n            sigma_v_sq = sigma_v**2\n            sigma_theta_sq = sigma_theta**2\n            v0_sq = v0**2\n            delta_t_sq = delta_t**2\n            # Formula: k * delta_t^2 * [v0^2 * (1 - exp(-sigma_theta^2)) + sigma_v^2]\n            return k * delta_t_sq * (v0_sq * (1.0 - np.exp(-sigma_theta_sq)) + sigma_v_sq)\n\n        # --- NO CORRECTIONS (nc) ---\n        bias_nc_norm = path_integ_bias_norm(T, v0, delta_t, sigma_theta)\n        trace_nc = path_integ_trace(T, v0, delta_t, sigma_v, sigma_theta)\n        \n        # --- WITH CORRECTIONS (wc) ---\n        bias_wc_norm = 0.0\n        trace_wc = 0.0\n        \n        if T  M:\n            # No corrections have occurred yet, so error is the same as the 'nc' case.\n            bias_wc_norm = bias_nc_norm\n            trace_wc = trace_nc\n        else:\n            # At least one correction period has passed.\n            t_rem = T % M\n            \n            # The landmark error contribution to variance is always present\n            # after the first correction at step M.\n            landmark_var_contrib = 2.0 * sigma_L**2\n            \n            # Bias and variance from path integration accumulate since the last reset.\n            # If t_rem is 0, the process is exactly at a correction step, so no\n            # path integration error has accumulated since the reset.\n            pi_bias_contrib = path_integ_bias_norm(t_rem, v0, delta_t, sigma_theta)\n            pi_trace_contrib = path_integ_trace(t_rem, v0, delta_t, sigma_v, sigma_theta)\n            \n            bias_wc_norm = pi_bias_contrib\n            trace_wc = landmark_var_contrib + pi_trace_contrib\n            \n        results.extend([bias_nc_norm, trace_nc, bias_wc_norm, trace_wc])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}