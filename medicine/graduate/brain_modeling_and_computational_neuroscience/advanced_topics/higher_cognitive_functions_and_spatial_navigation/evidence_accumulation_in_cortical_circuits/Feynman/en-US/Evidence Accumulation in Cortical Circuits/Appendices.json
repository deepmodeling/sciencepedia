{
    "hands_on_practices": [
        {
            "introduction": "To make optimal decisions, a cortical circuit must correctly interpret the evidence carried by incoming spike trains. This exercise guides you through the derivation of the log-likelihood ratio ($LLR$), the statistically optimal quantity for accumulating evidence between two hypotheses. By working through the case of a Poisson spike train , you will see how evidence manifests as discrete jumps with each spike and a continuous drift over time, providing a foundational model for decision-making circuits.",
            "id": "3979258",
            "problem": "A cortical decision circuit receives a single sensory spike train that can be modeled as a homogeneous Poisson point process whose rate depends on the true hypothesis: under hypothesis $H_1$ the rate is $\\lambda_1$, and under hypothesis $H_0$ the rate is $\\lambda_0$, with $\\lambda_1 \\neq \\lambda_0$ and $\\lambda_1, \\lambda_0 > 0$. The circuit computes the log-likelihood ratio (LLR) $\\Lambda_t$, defined as $\\Lambda_t \\equiv \\ln\\!\\big(p(\\mathcal{N}_{[0,t]} \\mid H_1)/p(\\mathcal{N}_{[0,t]} \\mid H_0)\\big)$, where $\\mathcal{N}_{[0,t]}$ denotes the entire spike-train history up to time $t$.\n\nStarting only from the definitions of a homogeneous Poisson process (independent increments with rate $\\lambda$ and constant hazard) and the definition of the likelihood ratio, derive an explicit expression for $\\Lambda_t$ in terms of the total spike count $N_t$ up to time $t$ and $t$ itself. From this expression, identify:\n- the jump increment in $\\Lambda_t$ occurring at each observed spike time, and\n- the expected instantaneous drift of $\\Lambda_t$ under hypothesis $H_1$.\n\nExpress your final results as two closed-form analytic expressions in terms of $\\lambda_1$ and $\\lambda_0$. No numerical evaluation is required.",
            "solution": "The problem asks for the derivation of the log-likelihood ratio (LLR) for a homogeneous Poisson process and for the subsequent identification of the jump increment and expected drift of this LLR. The problem is well-posed and scientifically grounded in the theory of sequential analysis and its application to neural coding.\n\nFirst, we must establish the likelihood of observing a specific spike-train history, $\\mathcal{N}_{[0,t]}$, under a given hypothesis. Let $\\mathcal{N}_{[0,t]}$ be characterized by the number of spikes, $N_t$, and their precise timings, $\\{t_1, t_2, \\dots, t_{N_t}\\}$ within the interval $[0, t]$.\n\nFor a general point process defined by a time-varying rate function $\\lambda(s)$, the likelihood of observing a specific realization with spikes at $\\{t_j\\}_{j=1}^{N_t}$ is given by the likelihood functional:\n$$ p(\\mathcal{N}_{[0,t]}) \\propto \\left( \\prod_{j=1}^{N_t} \\lambda(t_j) \\right) \\exp\\left( -\\int_0^t \\lambda(s) ds \\right) $$\nThe problem states the spike train is a homogeneous Poisson process, which means the rate $\\lambda(s)$ is a constant, $\\lambda$, for all $s$. Under hypothesis $H_i$ (for $i \\in \\{0, 1\\}$), the rate is $\\lambda_i$. Therefore, the likelihood functional simplifies.\n\nUnder hypothesis $H_1$, the rate is $\\lambda_1$. The likelihood of observing the spike train history $\\mathcal{N}_{[0,t]}$ is:\n$$ p(\\mathcal{N}_{[0,t]} \\mid H_1) \\propto \\left( \\prod_{j=1}^{N_t} \\lambda_1 \\right) \\exp\\left( -\\int_0^t \\lambda_1 ds \\right) = \\lambda_1^{N_t} \\exp(-\\lambda_1 t) $$\nSimilarly, under hypothesis $H_0$, the rate is $\\lambda_0$, and the likelihood is:\n$$ p(\\mathcal{N}_{[0,t]} \\mid H_0) \\propto \\lambda_0^{N_t} \\exp(-\\lambda_0 t) $$\nThe proportionality constants, which relate to the infinitesimal time bins used to define the probability, are identical for both hypotheses and will cancel in the likelihood ratio.\n\nThe log-likelihood ratio, $\\Lambda_t$, is defined as $\\Lambda_t \\equiv \\ln(p(\\mathcal{N}_{[0,t]} \\mid H_1) / p(\\mathcal{N}_{[0,t]} \\mid H_0))$. We can now substitute our derived likelihoods:\n$$ \\Lambda_t = \\ln\\left( \\frac{\\lambda_1^{N_t} \\exp(-\\lambda_1 t)}{\\lambda_0^{N_t} \\exp(-\\lambda_0 t)} \\right) $$\nUsing the properties of logarithms, we can separate the terms:\n$$ \\Lambda_t = \\ln(\\lambda_1^{N_t}) - \\ln(\\lambda_0^{N_t}) + \\ln(\\exp(-\\lambda_1 t)) - \\ln(\\exp(-\\lambda_0 t)) $$\n$$ \\Lambda_t = N_t \\ln(\\lambda_1) - N_t \\ln(\\lambda_0) - \\lambda_1 t + \\lambda_0 t $$\nThis gives the explicit expression for $\\Lambda_t$ in terms of the spike count $N_t$ and time $t$:\n$$ \\Lambda_t = N_t \\ln\\left(\\frac{\\lambda_1}{\\lambda_0}\\right) - (\\lambda_1 - \\lambda_0) t $$\nThis completes the first part of the derivation.\n\nNext, we identify the jump increment in $\\Lambda_t$. The expression for $\\Lambda_t$ consists of two parts. The term $-(\\lambda_1 - \\lambda_0) t$ evolves continuously and linearly in time. The term $N_t \\ln(\\lambda_1/\\lambda_0)$ changes value only when $N_t$ changes. As $N_t$ is the spike count, it increases by $1$ at the time of each spike. Let a spike occur at time $t_{spike}$. The value of the LLR just before the spike is $\\Lambda_{t_{spike}^-} = (N_{t_{spike}}-1) \\ln(\\frac{\\lambda_1}{\\lambda_0}) - (\\lambda_1 - \\lambda_0) t_{spike}$. The value just after the spike is $\\Lambda_{t_{spike}^+} = N_{t_{spike}} \\ln(\\frac{\\lambda_1}{\\lambda_0}) - (\\lambda_1 - \\lambda_0) t_{spike}$. The jump, $\\Delta\\Lambda$, is the difference:\n$$ \\Delta\\Lambda = \\Lambda_{t_{spike}^+} - \\Lambda_{t_{spike}^-} = \\left( N_{t_{spike}} - (N_{t_{spike}}-1) \\right) \\ln\\left(\\frac{\\lambda_1}{\\lambda_0}\\right) = \\ln\\left(\\frac{\\lambda_1}{\\lambda_0}\\right) $$\nThe jump increment at each observed spike is a constant amount, equal to the log-ratio of the rates.\n\nFinally, we determine the expected instantaneous drift of $\\Lambda_t$ under hypothesis $H_1$. We can express the dynamics of $\\Lambda_t$ in differential form. Let $dN_t$ be the number of spikes in an infinitesimal interval $dt$.\n$$ d\\Lambda_t = d\\left( N_t \\ln\\left(\\frac{\\lambda_1}{\\lambda_0}\\right) - (\\lambda_1 - \\lambda_0) t \\right) = \\ln\\left(\\frac{\\lambda_1}{\\lambda_0}\\right) dN_t - (\\lambda_1 - \\lambda_0) dt $$\nThe drift is defined as the expected rate of change, $E[d\\Lambda_t / dt]$. To find this, we take the expectation of $d\\Lambda_t$ under the statistics of $H_1$:\n$$ E[d\\Lambda_t \\mid H_1] = E\\left[ \\ln\\left(\\frac{\\lambda_1}{\\lambda_0}\\right) dN_t - (\\lambda_1 - \\lambda_0) dt \\mid H_1 \\right] $$\nBy linearity of expectation:\n$$ E[d\\Lambda_t \\mid H_1] = \\ln\\left(\\frac{\\lambda_1}{\\lambda_0}\\right) E[dN_t \\mid H_1] - (\\lambda_1 - \\lambda_0) dt $$\nUnder hypothesis $H_1$, the process has a constant rate $\\lambda_1$. The expected number of spikes in an interval $dt$ is therefore $E[dN_t \\mid H_1] = \\lambda_1 dt$. Substituting this into the equation:\n$$ E[d\\Lambda_t \\mid H_1] = \\ln\\left(\\frac{\\lambda_1}{\\lambda_0}\\right) (\\lambda_1 dt) - (\\lambda_1 - \\lambda_0) dt $$\nThe expected instantaneous drift is obtained by dividing by $dt$:\n$$ \\frac{E[d\\Lambda_t \\mid H_1]}{dt} = \\lambda_1 \\ln\\left(\\frac{\\lambda_1}{\\lambda_0}\\right) - (\\lambda_1 - \\lambda_0) $$\nThis expression represents the average rate at which evidence accumulates in favor of $H_1$ when $H_1$ is indeed the true state of the world.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\ln\\left(\\frac{\\lambda_1}{\\lambda_0}\\right) & \\lambda_1 \\ln\\left(\\frac{\\lambda_1}{\\lambda_0}\\right) - (\\lambda_1 - \\lambda_0)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In realistic scenarios, the quality or reliability of sensory evidence is rarely constant. This practice explores how a decision-making system should adapt to such changes . Through a combination of theoretical derivation and numerical implementation, you will compare a standard, stationary Drift-Diffusion Model with an adaptive accumulator that optimally weights evidence according to its time-varying reliability, quantifying the performance benefits of adaptive integration.",
            "id": "3979278",
            "problem": "You are to analyze a two-alternative forced choice decision under a continuous-time Gaussian evidence model and derive, implement, and compare the probability of a correct choice for two accumulators: a stationary drift-diffusion model (DDM) that uses a constant integration weight and an adaptive accumulator that tracks time-varying evidence reliability. The model is intended to capture evidence accumulation in cortical circuits.\n\nAssume a fixed decision horizon of $T$ seconds and momentary evidence $m(t)$ given by the linear Gaussian model\n$$\nm(t) = s \\,\\rho(t) + \\xi(t),\n$$\nwhere $s \\in \\{-1, +1\\}$ denotes the true state, $\\rho(t)$ is a nonnegative time-varying reliability profile (dimensionless), and $\\xi(t)$ is zero-mean white Gaussian noise with unit spectral density so that the noise variance accumulated by weighting $w(t)$ over time is $\\int_0^T w(t)^2 \\, dt$. An accumulator computes\n$$\nx(T) = \\int_0^T w(t) \\, m(t) \\, dt,\n$$\nand makes the choice $\\hat{s} = \\mathrm{sign}(x(T))$ at the deadline $T$. The task is to derive from first principles the probability of a correct choice $p_{\\mathrm{corr}}$ under this model for the following two weighting strategies:\n- Stationary drift-diffusion model: $w(t) = c$, where $c$ is a positive constant independent of $t$.\n- Adaptive reliability-tracking accumulator: $w(t) = \\rho(t)$.\n\nAfter deriving $p_{\\mathrm{corr}}$ for these two cases in terms of integrals of $\\rho(t)$, define the performance loss of the stationary DDM relative to the adaptive accumulator as\n$$\n\\Delta = p_{\\mathrm{corr}}^{\\mathrm{adaptive}} - p_{\\mathrm{corr}}^{\\mathrm{stationary}},\n$$\nexpressed as a decimal (dimensionless). Your final program must compute $\\Delta$ numerically by time discretization with uniform time step $\\Delta t$ seconds, approximating the required integrals by Riemann sums.\n\nImplement the following reliability profiles $\\rho(t)$ over the interval $t \\in [0, T)$ and compute the corresponding performance losses. Use $T = 1$ second and $\\Delta t = 10^{-3}$ seconds in all cases. Ensure that the reliability remains nonnegative at all times.\n\n- Test case 1 (baseline constant reliability): $\\rho(t) = \\rho_0$ with $\\rho_0 = 1$ for all $t \\in [0, T)$. This case serves as a boundary condition where no performance loss is expected.\n- Test case 2 (linearly decreasing reliability): $\\rho(t) = \\max\\{\\rho_0 - k t, 0\\}$ with $\\rho_0 = 1$ and $k = 0.8$ per second.\n- Test case 3 (exponential decay of reliability): $\\rho(t) = \\rho_0 \\, e^{-\\lambda t}$ with $\\rho_0 = 1$ and $\\lambda = 2$ per second.\n- Test case 4 (stepwise drop in reliability): $\\rho(t) = r_1$ for $t < t_s$ and $\\rho(t) = r_2$ for $t \\ge t_s$, with $r_1 = 1$, $r_2 = 0.2$, and $t_s = 0.5$ seconds.\n- Test case 5 (fast triangular drop to zero): $\\rho(t) = \\max\\{\\rho_0 \\, (1 - t/t_0), 0\\}$ with $\\rho_0 = 1$ and $t_0 = 0.2$ seconds.\n\nYour program should produce a single line of output containing the five performance losses for the test cases in the same order, as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (e.g., $[0.000000,0.012345,0.098765,0.123456,0.654321]$). No other output is permitted. All results are dimensionless decimals; no physical units are required in the output, although the computations must respect the specified time units (seconds) and discretization step (seconds).",
            "solution": "The problem requires the derivation and computation of the performance difference between two evidence accumulation strategies in a continuous-time decision-making model. We begin with a formal derivation of the probability of a correct choice, $p_{\\mathrm{corr}}$, for a general weighting function $w(t)$.\n\nThe decision variable, $x(T)$, is the integral of the weighted momentary evidence $m(t)$ over a fixed horizon $[0, T]$:\n$$\nx(T) = \\int_0^T w(t) \\, m(t) \\, dt\n$$\nThe momentary evidence is given by $m(t) = s \\rho(t) + \\xi(t)$, where $s \\in \\{-1, +1\\}$ is the true state, $\\rho(t)$ is the time-varying evidence reliability, and $\\xi(t)$ is zero-mean white Gaussian noise with unit spectral density. Substituting $m(t)$ into the expression for $x(T)$:\n$$\nx(T) = \\int_0^T w(t) [s \\rho(t) + \\xi(t)] dt = s \\int_0^T w(t) \\rho(t) dt + \\int_0^T w(t) \\xi(t) dt\n$$\nThe decision variable $x(T)$ is a random variable. Since it is a linear functional of the Gaussian process $\\xi(t)$, $x(T)$ itself is Gaussian-distributed. We determine its mean and variance.\n\nThe mean of $x(T)$ is its expected value, $E[x(T)]$:\n$$\n\\mu_x = E\\left[s \\int_0^T w(t) \\rho(t) dt + \\int_0^T w(t) \\xi(t) dt\\right]\n$$\nThe first term is deterministic. By linearity of expectation and because $E[\\xi(t)] = 0$, the second term has a mean of zero. Thus, the mean of $x(T)$ is:\n$$\n\\mu_x = s \\int_0^T w(t) \\rho(t) dt\n$$\nThe variance of $x(T)$, $\\sigma_x^2$, is given by:\n$$\n\\sigma_x^2 = \\mathrm{Var}\\left[s \\int_0^T w(t) \\rho(t) dt + \\int_0^T w(t) \\xi(t) dt\\right] = \\mathrm{Var}\\left[\\int_0^T w(t) \\xi(t) dt\\right]\n$$\nAs stated in the problem, for white noise $\\xi(t)$ with unit spectral density, the variance of the integrated noise is the integral of the squared weighting function:\n$$\n\\sigma_x^2 = \\int_0^T w(t)^2 dt\n$$\nSo, the distribution of the decision variable is $x(T) \\sim \\mathcal{N}(\\mu_x, \\sigma_x^2)$. A correct decision $\\hat{s}$ is made if $\\mathrm{sign}(\\hat{s}) = \\mathrm{sign}(x(T)) = s$.\n\nWithout loss of generality, let the true state be $s=+1$. A correct decision is made if $x(T) > 0$. The probability of a correct choice, $p_{\\mathrm{corr}}$, is $P(x(T) > 0)$. For a Gaussian variable, this probability is:\n$$\np_{\\mathrm{corr}} = P(x(T) > 0) = P\\left(\\frac{x(T) - \\mu_x}{\\sigma_x} > \\frac{0 - \\mu_x}{\\sigma_x}\\right) = 1 - \\Phi\\left(-\\frac{\\mu_x}{\\sigma_x}\\right) = \\Phi\\left(\\frac{\\mu_x}{\\sigma_x}\\right)\n$$\nwhere $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0, 1)$. The argument of $\\Phi$ is the signal-to-noise ratio (SNR) of the decision variable. For $s=+1$:\n$$\n\\mathrm{SNR} = \\frac{\\mu_x}{\\sigma_x} = \\frac{\\int_0^T w(t) \\rho(t) dt}{\\sqrt{\\int_0^T w(t)^2 dt}}\n$$\n(If we had chosen $s=-1$, $\\mu_x$ would be negative and a correct choice requires $x(T)<0$. The resulting probability would be identical, thus this derivation is general).\n\nNow we derive $p_{\\text{corr}}$ for the two specified weighting strategies.\n\n1.  **Adaptive reliability-tracking accumulator:** The weighting function is $w(t) = \\rho(t)$. This strategy corresponds to a matched filter, which is known to maximize the SNR.\n    The SNR for this case is:\n    $$\n    \\mathrm{SNR}_{\\mathrm{adaptive}} = \\frac{\\int_0^T \\rho(t) \\cdot \\rho(t) dt}{\\sqrt{\\int_0^T \\rho(t)^2 dt}} = \\frac{\\int_0^T \\rho(t)^2 dt}{\\sqrt{\\int_0^T \\rho(t)^2 dt}} = \\sqrt{\\int_0^T \\rho(t)^2 dt}\n    $$\n    The corresponding probability of a correct choice is:\n    $$\n    p_{\\mathrm{corr}}^{\\mathrm{adaptive}} = \\Phi\\left(\\sqrt{\\int_0^T \\rho(t)^2 dt}\\right)\n    $$\n\n2.  **Stationary drift-diffusion model (DDM):** The weighting function is a constant, $w(t) = c$, for some $c > 0$.\n    The SNR for this case is:\n    $$\n    \\mathrm{SNR}_{\\mathrm{stationary}} = \\frac{\\int_0^T c \\cdot \\rho(t) dt}{\\sqrt{\\int_0^T c^2 dt}} = \\frac{c \\int_0^T \\rho(t) dt}{\\sqrt{c^2 T}} = \\frac{c \\int_0^T \\rho(t) dt}{c\\sqrt{T}} = \\frac{\\int_0^T \\rho(t) dt}{\\sqrt{T}}\n    $$\n    The value of the constant $c$ cancels, so the performance of the stationary DDM is independent of its specific gain. The probability of a correct choice is:\n    $$\n    p_{\\mathrm{corr}}^{\\mathrm{stationary}} = \\Phi\\left(\\frac{\\int_0^T \\rho(t) dt}{\\sqrt{T}}\\right)\n    $$\n\nThe performance loss $\\Delta$ of the stationary DDM relative to the adaptive accumulator is defined as the difference in their respective probabilities of a correct choice:\n$$\n\\Delta = p_{\\mathrm{corr}}^{\\mathrm{adaptive}} - p_{\\mathrm{corr}}^{\\mathrm{stationary}} = \\Phi\\left(\\sqrt{\\int_0^T \\rho(t)^2 dt}\\right) - \\Phi\\left(\\frac{\\int_0^T \\rho(t) dt}{\\sqrt{T}}\\right)\n$$\nBy the Cauchy-Schwarz inequality, $\\left(\\int_0^T \\rho(t) dt\\right)^2 = \\left(\\int_0^T 1 \\cdot \\rho(t) dt\\right)^2 \\le \\left(\\int_0^T 1^2 dt\\right) \\left(\\int_0^T \\rho(t)^2 dt\\right) = T \\int_0^T \\rho(t)^2 dt$. This implies $\\mathrm{SNR}_{\\mathrm{stationary}}^2 \\le \\mathrm{SNR}_{\\mathrm{adaptive}}^2$. Since the SNR is non-negative and $\\Phi$ is a monotonically increasing function, we have $p_{\\mathrm{corr}}^{\\mathrm{stationary}} \\le p_{\\mathrm{corr}}^{\\mathrm{adaptive}}$, and therefore $\\Delta \\ge 0$. Equality holds if and only if $\\rho(t)$ is constant, as in test case 1.\n\nFor the numerical implementation, we discretize the time interval $[0, T)$ into $N = T/\\Delta t$ steps. The integrals are approximated by Riemann sums:\n$$\n\\int_0^T f(t) dt \\approx \\sum_{i=0}^{N-1} f(t_i) \\Delta t\n$$\nwhere $t_i = i \\cdot \\Delta t$.\nThe formulae for the SNRs become:\n$$\n\\mathrm{SNR}_{\\mathrm{adaptive}} \\approx \\sqrt{\\left(\\sum_{i=0}^{N-1} \\rho(t_i)^2\\right) \\Delta t}\n$$\n$$\n\\mathrm{SNR}_{\\mathrm{stationary}} \\approx \\frac{\\left(\\sum_{i=0}^{N-1} \\rho(t_i)\\right) \\Delta t}{\\sqrt{T}}\n$$\nThese expressions will be implemented to compute $\\Delta$ for the specified reliability profiles $\\rho(t)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\n# The problem specifies numpy 1.23.5 and scipy 1.11.4.\n# The `from scipy.stats import norm` import is valid under these requirements.\n\ndef solve():\n    \"\"\"\n    Computes the performance loss of a stationary drift-diffusion model (DDM)\n    relative to an adaptive accumulator for several reliability profiles.\n    \"\"\"\n    T = 1.0  # Decision horizon in seconds\n    dt = 1e-3  # Time step for numerical integration in seconds\n\n    def calculate_delta(rho_func, T_val, dt_val):\n        \"\"\"\n        Calculates the performance loss for a given reliability profile.\n\n        Args:\n            rho_func (callable): A function that takes a time array and returns the reliability profile.\n            T_val (float): Total time horizon.\n            dt_val (float): Time step for discretization.\n\n        Returns:\n            float: The performance loss Delta.\n        \"\"\"\n        # Create the discrete time vector\n        t = np.arange(0, T_val, dt_val)\n\n        # Evaluate the reliability profile at each time step\n        rho_t = rho_func(t)\n        \n        # Ensure reliability is non-negative as per problem statement\n        # (covered by max() in function definitions, but good practice to check)\n        if np.any(rho_t  0):\n            # This case should not be reached with the given test cases\n            raise ValueError(\"Reliability profile rho(t) must be non-negative.\")\n\n        # Approximate integrals using Riemann sums\n        # integral_rho_sq = sum(rho(t_i)^2 * dt)\n        integral_rho_sq = np.sum(rho_t**2) * dt_val\n        # integral_rho = sum(rho(t_i) * dt)\n        integral_rho = np.sum(rho_t) * dt_val\n\n        # Calculate the Signal-to-Noise Ratio (SNR) for both models\n        snr_adaptive = np.sqrt(integral_rho_sq)\n        snr_stationary = integral_rho / np.sqrt(T_val)\n\n        # Calculate the probability of correct choice for each model\n        # using the standard normal CDF (Phi function)\n        p_corr_adaptive = norm.cdf(snr_adaptive)\n        p_corr_stationary = norm.cdf(snr_stationary)\n\n        # Calculate the performance loss\n        delta = p_corr_adaptive - p_corr_stationary\n        return delta\n\n    # Define the reliability profiles for the five test cases\n    test_cases = [\n        # Test case 1: constant reliability\n        {\"rho_func\": lambda t: np.full_like(t, 1.0), \"params\": {}},\n        \n        # Test case 2: linearly decreasing reliability\n        {\"rho_func\": lambda t, rho0, k: np.maximum(rho0 - k * t, 0), \"params\": {\"rho0\": 1.0, \"k\": 0.8}},\n        \n        # Test case 3: exponential decay of reliability\n        {\"rho_func\": lambda t, rho0, lam: rho0 * np.exp(-lam * t), \"params\": {\"rho0\": 1.0, \"lam\": 2.0}},\n        \n        # Test case 4: stepwise drop in reliability\n        {\"rho_func\": lambda t, r1, r2, ts: np.where(t  ts, r1, r2), \"params\": {\"r1\": 1.0, \"r2\": 0.2, \"ts\": 0.5}},\n        \n        # Test case 5: fast triangular drop to zero\n        {\"rho_func\": lambda t, rho0, t0: np.maximum(rho0 * (1 - t / t0), 0), \"params\": {\"rho0\": 1.0, \"t0\": 0.2}},\n    ]\n\n    results = []\n    for case in test_cases:\n        rho_func = case[\"rho_func\"]\n        params = case[\"params\"]\n        # Create a partial function with the specific parameters for the case\n        profile_func = lambda t: rho_func(t, **params)\n        delta = calculate_delta(profile_func, T, dt)\n        results.append(delta)\n\n    # Format the final output as a comma-separated list of strings,\n    # with each value rounded to six decimal places, enclosed in brackets.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Abstract models of evidence accumulation must ultimately be grounded in the biophysics of neural circuits. This problem investigates how the dynamics of excitatory and inhibitory synapses, along with a neuron's own membrane properties, combine to implement an integration process . By deriving the system's net temporal kernel and its effective time constant, you will develop an appreciation for how circuit parameters shape the timescale of evidence accumulation.",
            "id": "3979223",
            "problem": "Consider a minimal linear model of an evidence accumulation variable $x(t)$ in a cortical circuit receiving both excitatory and inhibitory synaptic inputs driven by a common stimulus $s(t)$. The synaptic currents are modeled by first-order kinetics and the accumulator is a leaky integrator. Specifically, let the excitatory and inhibitory synaptic state variables $u_{E}(t)$ and $u_{I}(t)$ satisfy\n$$\n\\tau_{E}\\,\\frac{d u_{E}(t)}{dt} = -u_{E}(t) + s(t), \\qquad \\tau_{I}\\,\\frac{d u_{I}(t)}{dt} = -u_{I}(t) + s(t),\n$$\nand let the accumulator obey\n$$\n\\tau_{m}\\,\\frac{d x(t)}{dt} = -x(t) + g_{E}\\,u_{E}(t) - g_{I}\\,u_{I}(t),\n$$\nwhere $\\tau_{E}  0$, $\\tau_{I}  0$, and $\\tau_{m}  0$ are the excitatory, inhibitory, and membrane time constants, respectively, and $g_{E}  0$, $g_{I}  0$ are effective synaptic coupling gains. Assume zero initial conditions and that all signals are causal.\n\nDefine the linear time-invariant mapping from $s(t)$ to $x(t)$ by $x(t) = \\int_{0}^{\\infty} k(t')\\,s(t - t')\\,dt'$, where $k(t)$ is the net excitation-inhibition kernel. Starting from the above dynamical equations and using only fundamental properties of linear time-invariant systems and first-order synaptic kinetics, do the following:\n\n1. Derive the explicit causal expression for the net kernel $k(t)$ as a sum of decaying exponentials that reflects the excitation-inhibition balance.\n\n2. Define the effective time constant $\\tau_{\\mathrm{eff}}$ for $x(t)$ as the parameter of the best first-order low-frequency approximation of the transfer function from $s(t)$ to $x(t)$, i.e., for Laplace variable $p$ near $p=0$,\n$$\nG(p) \\equiv \\frac{X(p)}{S(p)} \\approx \\frac{G(0)}{1 + p\\,\\tau_{\\mathrm{eff}}}\n$$\nwith $G(0) \\equiv \\lim_{p\\to 0} G(p)$. Derive $\\tau_{\\mathrm{eff}}$ in closed form in terms of $\\tau_{m}$, $\\tau_{E}$, $\\tau_{I}$, $g_{E}$, and $g_{I}$, assuming $g_{E} \\neq g_{I}$.\n\nProvide your final answer as a single symbolic expression for $\\tau_{\\mathrm{eff}}$. No numerical evaluation is required. If one were to evaluate it numerically, time should be expressed in seconds.",
            "solution": "The problem describes a linear time-invariant (LTI) system linking a stimulus $s(t)$ to an evidence accumulation variable $x(t)$. The dynamics are governed by a set of coupled first-order linear ordinary differential equations. The most effective method for analyzing such a system is the Laplace transform, which converts differential equations in the time domain into algebraic equations in the frequency domain (represented by the Laplace variable $p$). We are given zero initial conditions, simplifying the transform.\n\nLet $S(p)$, $U_E(p)$, $U_I(p)$, and $X(p)$ be the Laplace transforms of $s(t)$, $u_E(t)$, $u_I(t)$, and $x(t)$, respectively. Applying the Laplace transform to the given dynamical equations yields:\n$$\n\\tau_{E}\\,p\\,U_{E}(p) = -U_{E}(p) + S(p) \\quad \\implies \\quad U_{E}(p) = \\frac{S(p)}{1 + p\\,\\tau_{E}}\n$$\n$$\n\\tau_{I}\\,p\\,U_{I}(p) = -U_{I}(p) + S(p) \\quad \\implies \\quad U_{I}(p) = \\frac{S(p)}{1 + p\\,\\tau_{I}}\n$$\n$$\n\\tau_{m}\\,p\\,X(p) = -X(p) + g_{E}\\,U_{E}(p) - g_{I}\\,U_{I}(p) \\quad \\implies \\quad X(p) = \\frac{g_{E}\\,U_{E}(p) - g_{I}\\,U_{I}(p)}{1 + p\\,\\tau_{m}}\n$$\nBy substituting the expressions for $U_E(p)$ and $U_I(p)$ into the equation for $X(p)$, we can find the overall transfer function $G(p) = X(p)/S(p)$:\n$$\nX(p) = \\frac{1}{1 + p\\,\\tau_{m}} \\left( g_{E}\\,\\frac{S(p)}{1 + p\\,\\tau_{E}} - g_{I}\\,\\frac{S(p)}{1 + p\\,\\tau_{I}} \\right)\n$$\n$$\nG(p) = \\frac{X(p)}{S(p)} = \\frac{1}{1 + p\\,\\tau_{m}} \\left( \\frac{g_{E}}{1 + p\\,\\tau_{E}} - \\frac{g_{I}}{1 + p\\,\\tau_{I}} \\right)\n$$\nThis can be written as the difference of two terms:\n$$\nG(p) = \\frac{g_{E}}{(1 + p\\,\\tau_{m})(1 + p\\,\\tau_{E})} - \\frac{g_{I}}{(1 + p\\,\\tau_{m})(1 + p\\,\\tau_{I})}\n$$\n\n**1. Derivation of the Net Kernel $k(t)$**\n\nThe kernel $k(t)$ is the impulse response of the system, which is obtained by finding the inverse Laplace transform of the transfer function $G(p)$. We will find the inverse transform of each term separately. The form of the solution depends on whether the time constants are distinct. The problem asks for a sum of decaying exponentials, which suggests we should assume the time constants are distinct. Let's assume $\\tau_m \\ne \\tau_E$ and $\\tau_m \\ne \\tau_I$.\n\nConsider the first term, $G_E(p) = \\frac{g_{E}}{(1 + p\\,\\tau_{m})(1 + p\\,\\tau_{E})}$. We use partial fraction expansion:\n$$\nG_E(p) = \\frac{A}{1 + p\\,\\tau_{m}} + \\frac{B}{1 + p\\,\\tau_{E}}\n$$\nUsing the Heaviside cover-up method:\n$$\nA = \\left. \\frac{g_{E}}{1 + p\\,\\tau_{E}} \\right|_{p = -1/\\tau_m} = \\frac{g_{E}}{1 - \\tau_{E}/\\tau_{m}} = \\frac{g_{E}\\,\\tau_{m}}{\\tau_{m} - \\tau_{E}}\n$$\n$$\nB = \\left. \\frac{g_{E}}{1 + p\\,\\tau_{m}} \\right|_{p = -1/\\tau_E} = \\frac{g_{E}}{1 - \\tau_{m}/\\tau_{E}} = \\frac{g_{E}\\,\\tau_{E}}{\\tau_{E} - \\tau_{m}}\n$$\nThe inverse Laplace transform of a term $\\frac{C}{1+p\\tau}$ is $\\frac{C}{\\tau} \\exp(-t/\\tau) H(t)$, where $H(t)$ is the Heaviside step function ensuring causality. Applying this:\n$$\n\\mathcal{L}^{-1}\\{G_E(p)\\}(t) = \\left( \\frac{A}{\\tau_m}\\exp(-t/\\tau_m) + \\frac{B}{\\tau_E}\\exp(-t/\\tau_E) \\right) H(t)\n$$\n$$\n\\mathcal{L}^{-1}\\{G_E(p)\\}(t) = \\left( \\frac{g_{E}}{\\tau_{m} - \\tau_{E}}\\exp(-t/\\tau_m) + \\frac{g_{E}}{\\tau_{E} - \\tau_{m}}\\exp(-t/\\tau_E) \\right) H(t)\n$$\n$$\n\\mathcal{L}^{-1}\\{G_E(p)\\}(t) = \\frac{g_{E}}{\\tau_{m} - \\tau_{E}} \\left( \\exp(-t/\\tau_m) - \\exp(-t/\\tau_E) \\right) H(t)\n$$\nBy symmetry, the inverse Laplace transform of the second term, $-\\frac{g_{I}}{(1 + p\\,\\tau_{m})(1 + p\\,\\tau_{I})}$, is:\n$$\n-\\frac{g_{I}}{\\tau_{m} - \\tau_{I}} \\left( \\exp(-t/\\tau_m) - \\exp(-t/\\tau_I) \\right) H(t)\n$$\nThe total kernel $k(t)$ is the sum of these two results:\n$$\nk(t) = \\left[ \\frac{g_{E}}{\\tau_{m} - \\tau_{E}} \\left( \\exp(-t/\\tau_m) - \\exp(-t/\\tau_E) \\right) - \\frac{g_{I}}{\\tau_{m} - \\tau_{I}} \\left( \\exp(-t/\\tau_m) - \\exp(-t/\\tau_I) \\right) \\right] H(t)\n$$\nGrouping terms by their exponential decay:\n$$\nk(t) = \\left[ \\left(\\frac{g_{E}}{\\tau_{m} - \\tau_{E}} - \\frac{g_{I}}{\\tau_{m} - \\tau_{I}}\\right)\\exp(-t/\\tau_m) - \\frac{g_{E}}{\\tau_{m} - \\tau_{E}}\\exp(-t/\\tau_E) + \\frac{g_{I}}{\\tau_{m} - \\tau_{I}}\\exp(-t/\\tau_I) \\right] H(t)\n$$\nThis provides the explicit causal expression for $k(t)$ as a sum of decaying exponentials.\n\n**2. Derivation of the Effective Time Constant $\\tau_{\\mathrm{eff}}$**\n\nThe effective time constant $\\tau_{\\mathrm{eff}}$ is defined by the first-order low-frequency approximation of $G(p)$ for $p \\to 0$:\n$$\nG(p) \\approx \\frac{G(0)}{1 + p\\,\\tau_{\\mathrm{eff}}}\n$$\nThe Taylor series of $G(p)$ around $p=0$ is $G(p) \\approx G(0) + p\\,G'(0)$. The approximation on the right-hand side can also be expanded for small $p$: $\\frac{G(0)}{1 + p\\,\\tau_{\\mathrm{eff}}} \\approx G(0)(1 - p\\,\\tau_{\\mathrm{eff}}) = G(0) - p\\,G(0)\\,\\tau_{\\mathrm{eff}}$.\nBy comparing the first-order terms in $p$ from both expansions, we find $G'(0) = -G(0)\\,\\tau_{\\mathrm{eff}}$. This yields a formula for $\\tau_{\\mathrm{eff}}$, provided $G(0) \\neq 0$:\n$$\n\\tau_{\\mathrm{eff}} = -\\frac{G'(0)}{G(0)}\n$$\nFirst, we calculate $G(0)$ by setting $p=0$ in the expression for $G(p)$:\n$$\nG(0) = \\frac{1}{1 + 0} \\left( \\frac{g_{E}}{1 + 0} - \\frac{g_{I}}{1 + 0} \\right) = g_{E} - g_{I}\n$$\nThe problem states that $g_E \\ne g_I$, so $G(0) \\ne 0$, and $\\tau_{\\mathrm{eff}}$ is well-defined.\n\nNext, we calculate $G'(0)$. We differentiate $G(p)$ with respect to $p$ and then evaluate at $p=0$. Using the product rule on $G(p) = (1+p\\tau_m)^{-1} \\left( \\frac{g_E}{1+p\\tau_E} - \\frac{g_I}{1+p\\tau_I} \\right)$:\n$$\nG'(p) = \\frac{d}{dp}\\left[(1+p\\tau_m)^{-1}\\right] \\left( \\frac{g_E}{1+p\\tau_E} - \\frac{g_I}{1+p\\tau_I} \\right) + (1+p\\tau_m)^{-1} \\frac{d}{dp}\\left[ \\frac{g_E}{1+p\\tau_E} - \\frac{g_I}{1+p\\tau_I} \\right]\n$$\n$$\nG'(p) = -\\tau_m(1+p\\tau_m)^{-2} \\left( \\frac{g_E}{1+p\\tau_E} - \\frac{g_I}{1+p\\tau_I} \\right) + (1+p\\tau_m)^{-1} \\left( \\frac{-g_E \\tau_E}{(1+p\\tau_E)^2} + \\frac{g_I \\tau_I}{(1+p\\tau_I)^2} \\right)\n$$\nNow, we evaluate this expression at $p=0$:\n$$\nG'(0) = -\\tau_m(1)^{-2} \\left( g_E - g_I \\right) + (1)^{-1} \\left( -g_E \\tau_E + g_I \\tau_I \\right)\n$$\n$$\nG'(0) = -\\tau_m(g_E - g_I) - g_E \\tau_E + g_I \\tau_I = -g_E\\tau_m + g_I\\tau_m - g_E\\tau_E + g_I\\tau_I\n$$\n$$\nG'(0) = -g_E(\\tau_m + \\tau_E) + g_I(\\tau_m + \\tau_I)\n$$\nFinally, we substitute $G(0)$ and $G'(0)$ into the expression for $\\tau_{\\mathrm{eff}}$:\n$$\n\\tau_{\\mathrm{eff}} = -\\frac{-g_E(\\tau_m + \\tau_E) + g_I(\\tau_m + \\tau_I)}{g_E - g_I} = \\frac{g_E(\\tau_m + \\tau_E) - g_I(\\tau_m + \\tau_I)}{g_E - g_I}\n$$\nThis expression can be rearranged to a more insightful form:\n$$\n\\tau_{\\mathrm{eff}} = \\frac{g_E\\tau_m - g_I\\tau_m + g_E\\tau_E - g_I\\tau_I}{g_E - g_I} = \\frac{(g_E - g_I)\\tau_m}{g_E - g_I} + \\frac{g_E\\tau_E - g_I\\tau_I}{g_E - g_I}\n$$\n$$\n\\tau_{\\mathrm{eff}} = \\tau_m + \\frac{g_E\\tau_E - g_I\\tau_I}{g_E - g_I}\n$$\nThis is the closed-form expression for the effective time constant.",
            "answer": "$$\n\\boxed{\\tau_{m} + \\frac{g_{E}\\tau_{E} - g_{I}\\tau_{I}}{g_{E} - g_{I}}}\n$$"
        }
    ]
}