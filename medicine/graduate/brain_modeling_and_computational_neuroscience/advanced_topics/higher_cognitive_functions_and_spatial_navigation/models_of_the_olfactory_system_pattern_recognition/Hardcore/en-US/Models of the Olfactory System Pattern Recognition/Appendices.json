{
    "hands_on_practices": [
        {
            "introduction": "In the olfactory system, the responses of different glomeruli to an odor are often correlated. To build efficient downstream representations for pattern recognition, a common and powerful preprocessing step is to decorrelate and normalize these signals. This exercise  walks you through this process, known as whitening, from first principles using the covariance matrix of a hypothetical glomerular population. By mastering this, you will gain a practical understanding of Principal Component Analysis (PCA) and its role in preparing neural data for further computation.",
            "id": "4000518",
            "problem": "In computational models of the mammalian olfactory system, the glomerular layer aggregates receptor neuron inputs into a population response vector $g \\in \\mathbb{R}^{n}$ whose variability across odor presentations is often modeled as a zero-mean multivariate Gaussian with covariance matrix $\\Sigma_{g} \\in \\mathbb{R}^{n \\times n}$. A standard step in pattern recognition and downstream processing (for example, in Principal Component Analysis (PCA)-based preprocessing or whitening prior to sparse coding) is to decorrelate and normalize the response variability by mapping $g$ into a linearly transformed representation $m \\in \\mathbb{R}^{n}$ whose covariance is the identity matrix. This transformation should be derived from first principles: use the definition of covariance and the orthogonal diagonalization of symmetric positive definite matrices.\n\nConsider a glomerular population of size $n = 3$ with covariance\n$$\n\\Sigma_{g} =\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 2 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix},\n$$\nand a single-trial response\n$$\ng =\n\\begin{pmatrix}\n3 \\\\\n1 \\\\\n2\n\\end{pmatrix}.\n$$\nPerform the following:\n\n1. Starting from the facts that covariance transforms as $\\mathrm{Cov}(A x) = A \\,\\mathrm{Cov}(x)\\, A^{\\top}$ for any deterministic matrix $A$, that $\\Sigma_{g}$ is symmetric positive definite, and that any such matrix admits an orthogonal diagonalization, compute the principal components by finding the orthonormal eigenvectors and corresponding eigenvalues of $\\Sigma_{g}$. Order the eigenvalues in strictly descending order and order the eigenvectors accordingly. Fix each eigenvector’s sign by the convention that its first nonzero entry is positive.\n\n2. Using only the above principles (without invoking any shortcut formulas stated in the problem), derive a linear transform $W$ such that the transformed variable $m = W g$ has identity covariance, that is, $\\mathrm{Cov}(m) = I$. Explain why your constructed $W$ achieves $\\mathrm{Cov}(m) = I$ for arbitrary zero-mean $g$ with covariance $\\Sigma_{g}$.\n\n3. Compute the whitened representation $m$ for the given $g$ using your derived $W$, and present the result in exact analytic form. Do not approximate; no rounding is required.\n\nExpress your final answer for $m$ as a single row vector using the LaTeX $\\verb|\\pmatrix|$ environment inside a single pair of parentheses (for example, $\\verb|\\begin{pmatrix} \\cdots \\end{pmatrix}|$). No units are required.",
            "solution": "The problem is subjected to validation and is deemed valid. It is scientifically grounded in computational neuroscience and linear algebra, well-posed with a unique and stable solution, and expressed in objective, formal language. All necessary data and conditions for the calculation are provided and are internally consistent. The problem is a standard exercise in data transformation (whitening) and is solvable using the specified first principles.\n\nThe solution proceeds in three parts as requested by the problem statement.\n\n### Part 1: Principal Component Analysis of $\\Sigma_g$\n\nThe principal components of the response distribution are the eigenvectors of the covariance matrix $\\Sigma_g$. We begin by finding the eigenvalues and corresponding eigenvectors of $\\Sigma_g$.\n\nThe given covariance matrix is:\n$$\n\\Sigma_g = \\begin{pmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(\\Sigma_g - \\lambda I) = 0$, where $I$ is the $3 \\times 3$ identity matrix.\n$$\n\\det \\begin{pmatrix} 2-\\lambda & 1 & 0 \\\\ 1 & 2-\\lambda & 0 \\\\ 0 & 0 & 2-\\lambda \\end{pmatrix} = 0\n$$\nExpanding the determinant along the third row:\n$$\n(2-\\lambda) \\det \\begin{pmatrix} 2-\\lambda & 1 \\\\ 1 & 2-\\lambda \\end{pmatrix} = (2-\\lambda) \\left[ (2-\\lambda)^2 - 1 \\right] = 0\n$$\nThis equation yields three eigenvalues. One root is clearly $\\lambda = 2$. The other two are found from $(2-\\lambda)^2 - 1 = 0$, which implies $2-\\lambda = \\pm 1$.\nIf $2-\\lambda = 1$, then $\\lambda = 1$.\nIf $2-\\lambda = -1$, then $\\lambda = 3$.\nThe eigenvalues are $\\{3, 2, 1\\}$. Ordering them in strictly descending order as required:\n$\\lambda_1 = 3$, $\\lambda_2 = 2$, $\\lambda_3 = 1$.\nSince all eigenvalues are positive, the matrix $\\Sigma_g$ is positive definite, as stated in the problem.\n\nNext, we find the corresponding orthonormal eigenvectors $v_i$ for each eigenvalue $\\lambda_i$.\n\nFor $\\lambda_1 = 3$:\nWe solve $(\\Sigma_g - 3I)v = 0$:\n$$\n\\begin{pmatrix} -1 & 1 & 0 \\\\ 1 & -1 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis system gives the equations $-x+y=0$ (or $x=y$) and $-z=0$. An eigenvector is of the form $\\begin{pmatrix} c \\\\ c \\\\ 0 \\end{pmatrix}$. For it to be a unit vector, $c^2 + c^2 = 1 \\implies 2c^2=1 \\implies c = \\pm \\frac{1}{\\sqrt{2}}$. The convention is that the first nonzero entry must be positive, so we choose $c = \\frac{1}{\\sqrt{2}}$.\n$$\nv_1 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}\n$$\n\nFor $\\lambda_2 = 2$:\nWe solve $(\\Sigma_g - 2I)v = 0$:\n$$\n\\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives $y=0$ and $x=0$. The component $z$ is free. An eigenvector is of the form $\\begin{pmatrix} 0 \\\\ 0 \\\\ c \\end{pmatrix}$. For it to be a unit vector, $c^2=1 \\implies c=\\pm 1$. The first nonzero entry is $z$, so we choose $c=1$.\n$$\nv_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n\nFor $\\lambda_3 = 1$:\nWe solve $(\\Sigma_g - 1I)v = 0$:\n$$\n\\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives $x+y=0$ (or $y=-x$) and $z=0$. An eigenvector is of the form $\\begin{pmatrix} c \\\\ -c \\\\ 0 \\end{pmatrix}$. For it to be a unit vector, $c^2 + (-c)^2 = 1 \\implies 2c^2=1 \\implies c = \\pm \\frac{1}{\\sqrt{2}}$. The first nonzero entry is $x$, so we choose $c = \\frac{1}{\\sqrt{2}}$.\n$$\nv_3 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}\n$$\n\nThe principal components (eigenvectors) and their corresponding eigenvalues (variances) are:\n$\\lambda_1=3, v_1 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix}^\\top$\n$\\lambda_2=2, v_2 = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}^\\top$\n$\\lambda_3=1, v_3 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix}^\\top$\n\n### Part 2: Derivation of the Whitening Transform $W$\n\nWe are tasked to find a linear transformation matrix $W$ such that for a transformed variable $m = Wg$, its covariance matrix is the identity matrix, i.e., $\\mathrm{Cov}(m) = I$.\nWe are given that $\\mathrm{Cov}(g) = \\Sigma_g$ and the covariance transformation law $\\mathrm{Cov}(Wg) = W \\mathrm{Cov}(g) W^\\top$.\nTherefore, we require:\n$$\nW \\Sigma_g W^\\top = I\n$$\nFrom Part 1, we know that because $\\Sigma_g$ is a symmetric matrix, it admits an orthogonal diagonalization $\\Sigma_g = V \\Lambda V^\\top$, where $V$ is the orthogonal matrix whose columns are the orthonormal eigenvectors $v_1, v_2, v_3$, and $\\Lambda$ is the diagonal matrix of corresponding eigenvalues $\\lambda_1, \\lambda_2, \\lambda_3$.\n$$\nV = \\begin{pmatrix} v_1 & v_2 & v_3 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\\\ 0 & 1 & 0 \\end{pmatrix} \\quad \\text{and} \\quad \\Lambda = \\begin{pmatrix} 3 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nSubstituting the diagonalization into our requirement:\n$$\nW (V \\Lambda V^\\top) W^\\top = I\n$$\nTo solve for $W$, we can propose a form for $W$ that simplifies this equation. The process of whitening involves two steps: first rotating the data to align with the principal axes, and then scaling each new coordinate. The rotation is achieved by $V^\\top$. The scaling is achieved by a diagonal matrix. Let's propose $W = S V^\\top$ where $S$ is a scaling matrix to be determined.\n$$\n(S V^\\top) (V \\Lambda V^\\top) (S V^\\top)^\\top = I\n$$\nUsing the property $(AB)^\\top = B^\\top A^\\top$ and the orthogonality of $V$ ($V^\\top V = I$):\n$$\nS (V^\\top V) \\Lambda (V^\\top V) S^\\top = S \\Lambda S^\\top = I\n$$\nSince $\\Lambda$ is a diagonal matrix with positive entries $\\lambda_i$, we can choose $S$ to be a diagonal matrix that \"inverts\" $\\Lambda$. Let $S = \\Lambda^{-1/2}$, defined as the diagonal matrix with entries $\\frac{1}{\\sqrt{\\lambda_i}}$.\n$$\n\\Lambda^{-1/2} = \\begin{pmatrix} \\frac{1}{\\sqrt{\\lambda_1}} & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{\\lambda_2}} & 0 \\\\ 0 & 0 & \\frac{1}{\\sqrt{\\lambda_3}} \\end{pmatrix}\n$$\nSince $\\Lambda^{-1/2}$ is diagonal, it is symmetric, so $S^\\top = S = \\Lambda^{-1/2}$. Our equation becomes:\n$$\n\\Lambda^{-1/2} \\Lambda \\Lambda^{-1/2} = I\n$$\nThis is true because for each diagonal element $i$, we have $(\\frac{1}{\\sqrt{\\lambda_i}}) (\\lambda_i) (\\frac{1}{\\sqrt{\\lambda_i}}) = 1$.\nThus, a valid whitening matrix is $W = \\Lambda^{-1/2} V^\\top$. This construction achieves $\\mathrm{Cov}(m)=I$ for any zero-mean random vector $g$ with a symmetric positive definite covariance matrix $\\Sigma_g$, as it relies only on the existence of the spectral decomposition of $\\Sigma_g$.\n\n### Part 3: Computation of the Whitened Representation $m$\n\nWe now compute $m = Wg$ using the derived $W$ and the given vector $g$.\nThe components are:\n$$\n\\Lambda^{-1/2} = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n$$\nV^\\top = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix}\n$$\n$$\ng = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix}\n$$\nThe transformation is $m = (\\Lambda^{-1/2} V^\\top) g$. First, we compute the projection $V^\\top g$:\n$$\nV^\\top g = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} \\\\ 2 \\\\ \\frac{3}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{\\sqrt{2}} \\\\ 2 \\\\ \\frac{2}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 2\\sqrt{2} \\\\ 2 \\\\ \\sqrt{2} \\end{pmatrix}\n$$\nNext, we apply the scaling matrix $\\Lambda^{-1/2}$:\n$$\nm = \\Lambda^{-1/2} (V^\\top g) = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 2\\sqrt{2} \\\\ 2 \\\\ \\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{2\\sqrt{2}}{\\sqrt{3}} \\\\ \\frac{2}{\\sqrt{2}} \\\\ \\sqrt{2} \\end{pmatrix}\n$$\nSimplifying each component into its exact analytic form:\n$$\nm_1 = \\frac{2\\sqrt{2}}{\\sqrt{3}} = \\frac{2\\sqrt{2}\\sqrt{3}}{3} = \\frac{2\\sqrt{6}}{3}\n$$\n$$\nm_2 = \\frac{2}{\\sqrt{2}} = \\frac{2\\sqrt{2}}{2} = \\sqrt{2}\n$$\n$$\nm_3 = \\sqrt{2}\n$$\nThe resulting whitened vector is $m = \\begin{pmatrix} \\frac{2\\sqrt{6}}{3} \\\\ \\sqrt{2} \\\\ \\sqrt{2} \\end{pmatrix}$. The final answer is to be presented as a row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2\\sqrt{6}}{3} & \\sqrt{2} & \\sqrt{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond describing the statistical structure of neural responses, a central goal is to quantify how much information these responses carry about the outside world. This practice  introduces the Fisher information matrix, a key tool from information theory that sets the fundamental limit on how precisely a stimulus can be estimated from noisy neural signals. You will derive an expression that explicitly links the biophysical properties of olfactory receptors—such as their gain and tuning curve shape—to the information they provide, revealing the principles of optimal sensory encoding.",
            "id": "4000563",
            "problem": "An olfactory receptor ensemble is used to discriminate odor mixtures in a $d$-dimensional concentration space. Let the stimulus be the concentration vector $c \\in \\mathbb{R}^{d}$ and let $M$ receptor channels be indexed by $i \\in \\{1,\\dots,M\\}$. The measured response of receptor $i$ to stimulus $c$ is modeled as\n$$\nr_i \\;=\\; f_i\\!\\left(K_i^\\top c\\right) \\;+\\; \\eta_i,\n$$\nwhere $K_i \\in \\mathbb{R}^{d}$ is the receptor’s linear tuning vector, $f_i:\\mathbb{R}\\to\\mathbb{R}$ is a smooth static nonlinearity, and $\\eta_i \\sim \\mathcal{N}(0,\\sigma_i^2)$ are independent and identically distributed across trials but independent across $i$ with fixed variances $\\sigma_i^2>0$ that do not depend on $c$. Assume the observation vector $r=(r_1,\\dots,r_M)^\\top$ is conditionally Gaussian given $c$, with mean components $\\mu_i(c)=f_i(K_i^\\top c)$ and diagonal covariance with entries $\\sigma_i^2$.\n\nStarting from the definition of the Fisher information matrix for the parameter $c$,\n$$\nI(c) \\;=\\; \\mathbb{E}_{r \\mid c}\\!\\left[ \\left(\\nabla_c \\ln p(r \\mid c)\\right)\\left(\\nabla_c \\ln p(r \\mid c)\\right)^\\top \\right],\n$$\nderive a closed-form expression for $I(c)$ in terms of $f_i$, $K_i$, and $\\sigma_i$.\n\nThen, assume each receptor has a separable gain and shape, $f_i(s) = g_i\\,\\phi_i(s)$, where $g_i>0$ is a receptor-specific gain and $\\phi_i$ is a smooth, monotonically increasing tuning nonlinearity. Express $I(c)$ in terms of $g_i$, $\\phi_i$, $K_i$, and $\\sigma_i$, and thereby show explicitly how receptor gain and tuning shape modulate the contributions of each receptor to $I(c)$.\n\nExpress your final answer as a single closed-form analytic expression for $I(c)$ using $g_i$, $\\phi_i$, $K_i$, $\\sigma_i$, and $c$. No numerical evaluation or rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded in computational neuroscience, well-posed, and all terms are formally defined. We proceed with the derivation.\n\nThe response of the $i$-th receptor channel is given by the model\n$$\nr_i = f_i(K_i^\\top c) + \\eta_i\n$$\nwhere $\\eta_i$ is a Gaussian noise term with mean $0$ and variance $\\sigma_i^2$, i.e., $\\eta_i \\sim \\mathcal{N}(0, \\sigma_i^2)$. The stimulus is the concentration vector $c \\in \\mathbb{R}^d$. The observation vector is $r = (r_1, \\dots, r_M)^\\top$. Given the stimulus $c$, the response $r_i$ follows a Gaussian distribution with mean $\\mu_i(c) = f_i(K_i^\\top c)$ and variance $\\sigma_i^2$. The problem states that the noise terms $\\eta_i$ are independent across receptors for $i \\in \\{1, \\dots, M\\}$. This implies that the responses $r_i$ are conditionally independent given $c$.\n\nThe probability density function (PDF) of the $i$-th response $r_i$ given $c$ is\n$$\np(r_i \\mid c) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left(-\\frac{(r_i - \\mu_i(c))^2}{2\\sigma_i^2}\\right)\n$$\nDue to the independence of the receptor channels, the joint PDF of the observation vector $r$ is the product of the individual PDFs:\n$$\np(r \\mid c) = \\prod_{i=1}^{M} p(r_i \\mid c) = \\prod_{i=1}^{M} \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left(-\\frac{(r_i - \\mu_i(c))^2}{2\\sigma_i^2}\\right)\n$$\nThe log-likelihood function, $\\ln p(r \\mid c)$, is the sum of the individual log-likelihoods:\n$$\n\\ln p(r \\mid c) = \\sum_{i=1}^{M} \\ln p(r_i \\mid c) = \\sum_{i=1}^{M} \\left( -\\frac{1}{2}\\ln(2\\pi\\sigma_i^2) - \\frac{(r_i - \\mu_i(c))^2}{2\\sigma_i^2} \\right)\n$$\nTo find the Fisher information matrix $I(c)$, we first need to compute the gradient of the log-likelihood with respect to the parameter vector $c$. This vector is also known as the score.\n$$\n\\nabla_c \\ln p(r \\mid c) = \\nabla_c \\sum_{i=1}^{M} \\left( -\\frac{1}{2}\\ln(2\\pi\\sigma_i^2) - \\frac{(r_i - \\mu_i(c))^2}{2\\sigma_i^2} \\right)\n$$\nThe first term in the sum is constant with respect to $c$. We apply the gradient to the second term:\n$$\n\\nabla_c \\ln p(r \\mid c) = \\sum_{i=1}^{M} \\nabla_c \\left( - \\frac{(r_i - \\mu_i(c))^2}{2\\sigma_i^2} \\right)\n$$\nUsing the chain rule, for each term $i$:\n$$\n\\nabla_c \\left( - \\frac{(r_i - \\mu_i(c))^2}{2\\sigma_i^2} \\right) = -\\frac{1}{2\\sigma_i^2} \\cdot 2(r_i - \\mu_i(c)) \\cdot (-\\nabla_c \\mu_i(c)) = \\frac{r_i - \\mu_i(c)}{\\sigma_i^2} \\nabla_c \\mu_i(c)\n$$\nNext, we compute the gradient of the mean response $\\mu_i(c) = f_i(K_i^\\top c)$. Let $s_i = K_i^\\top c$. Then $\\mu_i(c) = f_i(s_i)$. Applying the chain rule again:\n$$\n\\nabla_c \\mu_i(c) = \\frac{d f_i}{ds_i} \\nabla_c s_i = f_i'(s_i) \\nabla_c (K_i^\\top c)\n$$\nThe gradient of the linear projection $K_i^\\top c$ with respect to $c$ is simply the vector $K_i$. Thus,\n$$\n\\nabla_c \\mu_i(c) = f_i'(K_i^\\top c) K_i\n$$\nSubstituting this back into the expression for the score, we get:\n$$\n\\nabla_c \\ln p(r \\mid c) = \\sum_{i=1}^{M} \\frac{r_i - \\mu_i(c)}{\\sigma_i^2} f_i'(K_i^\\top c) K_i\n$$\nThe Fisher information matrix is defined as the expectation of the outer product of the score with itself:\n$$\nI(c) = \\mathbb{E}_{r \\mid c} \\!\\left[ \\left(\\nabla_c \\ln p(r \\mid c)\\right)\\left(\\nabla_c \\ln p(r \\mid c)\\right)^\\top \\right]\n$$\nLet's first write out the outer product:\n$$\n\\left(\\nabla_c \\ln p(r \\mid c)\\right)\\left(\\nabla_c \\ln p(r \\mid c)\\right)^\\top = \\left(\\sum_{i=1}^{M} \\frac{r_i - \\mu_i(c)}{\\sigma_i^2} f_i'(K_i^\\top c) K_i\\right) \\left(\\sum_{j=1}^{M} \\frac{r_j - \\mu_j(c)}{\\sigma_j^2} f_j'(K_j^\\top c) K_j^\\top \\right)\n$$\nThis expands to a double summation:\n$$\n\\sum_{i=1}^{M} \\sum_{j=1}^{M} \\frac{(r_i - \\mu_i(c))(r_j - \\mu_j(c))}{\\sigma_i^2 \\sigma_j^2} f_i'(K_i^\\top c) f_j'(K_j^\\top c) (K_i K_j^\\top)\n$$\nNow we take the expectation with respect to the distribution of $r$ given $c$. The expectation operator $\\mathbb{E}_{r|c}$ only affects terms involving $r$.\n$$\nI(c) = \\sum_{i=1}^{M} \\sum_{j=1}^{M} \\frac{f_i'(K_i^\\top c) f_j'(K_j^\\top c)}{\\sigma_i^2 \\sigma_j^2} (K_i K_j^\\top) \\mathbb{E}_{r \\mid c} \\left[ (r_i - \\mu_i(c))(r_j - \\mu_j(c)) \\right]\n$$\nThe expectation term is the covariance of $r_i$ and $r_j$. Since the responses are conditionally independent, their covariance is zero for $i \\neq j$. For $i=j$, it is the variance of $r_i$.\n$$\n\\mathbb{E}_{r \\mid c} \\left[ (r_i - \\mu_i(c))(r_j - \\mu_j(c)) \\right] = \\text{Cov}(r_i, r_j) = \\delta_{ij} \\sigma_i^2\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. Substituting this into the double summation collapses it into a single sum over $i=j$:\n$$\nI(c) = \\sum_{i=1}^{M} \\sum_{j=1}^{M} \\frac{f_i'(K_i^\\top c) f_j'(K_j^\\top c)}{\\sigma_i^2 \\sigma_j^2} (K_i K_j^\\top) \\delta_{ij} \\sigma_i^2 = \\sum_{i=1}^{M} \\frac{[f_i'(K_i^\\top c)]^2}{(\\sigma_i^2)^2} (K_i K_i^\\top) \\sigma_i^2\n$$\nSimplifying this expression gives the first required result:\n$$\nI(c) = \\sum_{i=1}^{M} \\frac{[f_i'(K_i^\\top c)]^2}{\\sigma_i^2} K_i K_i^\\top\n$$\nNow, we use the specified separable form for the nonlinearity, $f_i(s) = g_i\\,\\phi_i(s)$, where $s=K_i^\\top c$. The derivative $f_i'(s)$ is:\n$$\nf_i'(s) = \\frac{d}{ds}(g_i \\phi_i(s)) = g_i \\phi_i'(s)\n$$\nwhere $g_i > 0$ is a constant gain and $\\phi_i$ is a smooth, monotonically increasing function. Substituting this derivative into our expression for $I(c)$:\n$$\nI(c) = \\sum_{i=1}^{M} \\frac{[g_i \\phi_i'(K_i^\\top c)]^2}{\\sigma_i^2} K_i K_i^\\top = \\sum_{i=1}^{M} \\frac{g_i^2}{\\sigma_i^2} [\\phi_i'(K_i^\\top c)]^2 K_i K_i^\\top\n$$\nThis is the final closed-form expression for the Fisher information matrix $I(c)$.\n\nThis expression explicitly shows how receptor properties modulate the Fisher information. The total information is a sum of contributions from each receptor $i$. Each contribution is a rank-one matrix $K_i K_i^\\top$, which defines the direction in concentration space about which the receptor provides information. This direction is determined by its linear tuning vector $K_i$. The magnitude of this contribution is modulated by three factors:\n1.  Receptor Gain ($g_i$): The contribution is proportional to the square of the gain, $g_i^2$. Higher gain amplifies the signal change, increasing the information content.\n2.  Noise Variance ($\\sigma_i^2$): The contribution is inversely proportional to the noise variance, $1/\\sigma_i^2$. Lower noise means a more reliable signal, hence more information. The combined term $(g_i/\\sigma_i)^2$ can be interpreted as the squared signal-to-noise ratio of the receptor's output.\n3.  Tuning Shape ($\\phi_i$): The contribution depends on the square of the slope of the tuning nonlinearity, $[\\phi_i'(K_i^\\top c)]^2$. A receptor provides the most information about stimuli $c$ that fall in the steepest region of its tuning curve $\\phi_i$. For stimuli in a flat region of the curve where $\\phi_i' \\approx 0$, the receptor is saturated or unresponsive and provides little to no information.",
            "answer": "$$\n\\boxed{\\sum_{i=1}^{M} \\frac{g_i^2}{\\sigma_i^2} \\left[\\phi_i'(K_i^\\top c)\\right]^2 K_i K_i^\\top}\n$$"
        },
        {
            "introduction": "The brain must perform complex computations under strict metabolic constraints, leading to the hypothesis of energy-efficient or \"sparse\" coding. This exercise  models this principle in the piriform cortex, where the goal is to classify an odor, by balancing fidelity to the input signal with a sparsity-inducing penalty. You will use convex optimization to find the minimal-energy neural activity pattern that can still achieve a reliable classification, a core concept in modern computational neuroscience.",
            "id": "4000509",
            "problem": "You are modeling the sparse, energy-efficient activity of the piriform cortex in the olfactory system during odor classification. Assume a linear feedforward drive from the olfactory bulb produces a vector $z \\in \\mathbb{R}^p$ of pre-activation inputs to piriform neurons, and a fixed linear readout vector $c \\in \\mathbb{R}^p$ is used to classify odors with binary labels $y \\in \\{-1,+1\\}$. The classification requirement is enforced as a margin constraint: the signed readout must satisfy $y \\, c^\\top a \\ge m$ for a specified margin $m > 0$, where $a \\in \\mathbb{R}^p$ is the piriform activity vector to be determined. The energy of a code is taken to be the sum of a quadratic deviation cost from the feedforward drive and a sparsity-inducing penalty: \n$$\nE(a;z,\\lambda) = \\frac{1}{2} \\left\\| a - z \\right\\|_2^2 + \\lambda \\left\\| a \\right\\|_1,\n$$\nwhere $\\lambda > 0$ is the coefficient controlling sparsity. The task is: for each test case, compute the minimal-energy piriform activity $a^\\star$ and report the corresponding minimal energy $E(a^\\star;z,\\lambda)$.\n\nBase assumptions and definitions:\n- Efficient coding and sparse representation: Sparse codes minimize an energy functional that trades off fidelity to inputs with sparsity of activations, commonly modeled by an $\\ell_1$ norm penalty.\n- Linear classification margin: Accuracy is relaxed to a convex constraint requiring a minimum margin on the linear readout, which is a standard surrogate for classification performance and yields a convex feasibility region.\n- All quantities are unitless scalars and vectors; no physical units are involved. All angles are irrelevant and not used in this problem.\n\nFormally, for each test case, solve the convex program\n$$\n\\min_{a \\in \\mathbb{R}^p} \\ \\frac{1}{2} \\left\\| a - z \\right\\|_2^2 + \\lambda \\left\\| a \\right\\|_1 \\quad \\text{subject to} \\quad y \\, c^\\top a \\ge m.\n$$\n\nYour program must solve this optimization problem numerically for the following three test cases, each with dimensionality $p = 8$, and produce the minimal energy values as outputs:\n\n- Test case $1$ (general case):\n  - $z = [0.8, 0.1, 0.0, 0.2, -0.3, 0.5, 0.0, -0.1]$\n  - $c = [0.6, -0.2, 0.0, 0.3, 0.1, -0.5, 0.0, 0.2]$\n  - $y = +1$\n  - $m = 0.4$\n  - $\\lambda = 0.15$\n\n- Test case $2$ (boundary-axis readout with strong margin):\n  - $z = [-0.2, 0.4, 0.0, -0.1, 0.0, 0.3, 0.05, 0.0]$\n  - $c = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$\n  - $y = +1$\n  - $m = 0.5$\n  - $\\lambda = 0.05$\n\n- Test case $3$ (negative label and mixed-sign readout):\n  - $z = [0.1, 0.0, 0.4, -0.2, 0.2, 0.0, 0.0, 0.3]$\n  - $c = [-0.2, 0.5, 0.3, 0.0, -0.4, 0.0, 0.1, 0.2]$\n  - $y = -1$\n  - $m = 0.3$\n  - $\\lambda = 0.2$\n\nAlgorithmic and mathematical constraints:\n- You must treat the problem as a convex optimization with an $\\ell_1$ penalty and a single linear inequality constraint.\n- You must not assume a closed-form solution; derive and implement a principled numerical method that correctly handles the non-differentiability of the $\\ell_1$ norm and the linear constraint.\n- The classification margin constraint must be satisfied by the computed solution $a^\\star$ to within numerical tolerance.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry being the minimal energy for one test case, expressed as a floating-point number with six decimal places, for example, $[0.123456,0.234567,0.345678]$.",
            "solution": "We start from efficient coding and sparse representation principles: to obtain energy-efficient neural activity, one minimizes a combination of a fidelity term and a sparsity term. The fidelity term here is quadratic, $\\frac{1}{2}\\left\\| a - z \\right\\|_2^2$, capturing deviation from the feedforward drive, while sparsity is promoted by an $\\ell_1$ penalty $\\lambda \\left\\| a \\right\\|_1$, which is widely accepted to induce sparse codes. Classification accuracy is modeled as a convex margin constraint $y \\, c^\\top a \\ge m$, which is a relaxation of discrete accuracy into a geometric margin on the linear readout.\n\nThe optimization problem is\n$$\n\\min_{a \\in \\mathbb{R}^p} \\ \\frac{1}{2} \\left\\| a - z \\right\\|_2^2 + \\lambda \\left\\| a \\right\\|_1 \\quad \\text{subject to} \\quad y \\, c^\\top a \\ge m.\n$$\nThis problem is convex because the objective is the sum of a convex smooth function and a convex non-smooth function, and the feasible set defined by a single linear inequality is convex.\n\nTo solve this, we employ the Alternating Direction Method of Multipliers (ADMM). ADMM is suitable for problems where the objective decomposes into sums of terms with simple proximal operators and linear constraints. We introduce two auxiliary variables to separate the non-smooth $\\ell_1$ penalty and the indicator of the half-space constraint:\n- $u \\in \\mathbb{R}^p$ to handle the $\\ell_1$ term,\n- $v \\in \\mathbb{R}^p$ to handle the constraint.\n\nWe rewrite the problem as\n$$\n\\min_{a,u,v} \\ \\frac{1}{2} \\left\\| a - z \\right\\|_2^2 + \\lambda \\left\\| u \\right\\|_1 + I_{\\mathcal{H}}(v) \\quad \\text{subject to} \\quad a = u, \\quad a = v,\n$$\nwhere $I_{\\mathcal{H}}(v)$ is the indicator function of the half-space $\\mathcal{H} = \\{ v \\in \\mathbb{R}^p \\mid y \\, c^\\top v \\ge m \\}$, equal to $0$ if $v \\in \\mathcal{H}$ and $+\\infty$ otherwise.\n\nThe scaled ADMM iterations with penalty parameter $\\rho > 0$ and dual variables $p_u, p_v$ proceed by minimizing the augmented Lagrangian with respect to each variable in turn:\n- $a$-update: minimize a strictly convex quadratic,\n- $u$-update: proximal step of the $\\ell_1$ norm,\n- $v$-update: projection onto the half-space,\n- dual updates: gradient ascent on the dual variables.\n\nWe derive each update explicitly.\n\nFor the $a$-update, we minimize\n$$\n\\frac{1}{2}\\left\\| a - z \\right\\|_2^2 + \\frac{\\rho}{2} \\left\\| a - u + p_u \\right\\|_2^2 + \\frac{\\rho}{2} \\left\\| a - v + p_v \\right\\|_2^2.\n$$\nTaking the gradient with respect to $a$, setting it to zero, and solving yields\n$$\n(1 + 2\\rho) \\, a = z + \\rho \\left( u - p_u \\right) + \\rho \\left( v - p_v \\right),\n$$\nso the closed-form update is\n$$\na \\leftarrow \\frac{z + \\rho (u - p_u) + \\rho (v - p_v)}{1 + 2\\rho}.\n$$\n\nFor the $u$-update, we solve\n$$\n\\min_u \\ \\lambda \\left\\| u \\right\\|_1 + \\frac{\\rho}{2} \\left\\| u - (a + p_u) \\right\\|_2^2.\n$$\nThis is the proximal operator of the $\\ell_1$ norm, given by the soft-thresholding operation applied elementwise:\n$$\nu \\leftarrow \\operatorname{soft}(a + p_u, \\lambda/\\rho),\n$$\nwhere for each component $w_i$,\n$$\n\\operatorname{soft}(w_i, \\tau) = \\operatorname{sign}(w_i) \\cdot \\max\\left( |w_i| - \\tau, \\, 0 \\right).\n$$\n\nFor the $v$-update, we solve\n$$\n\\min_v \\ I_{\\mathcal{H}}(v) + \\frac{\\rho}{2} \\left\\| v - (a + p_v) \\right\\|_2^2,\n$$\nwhich is the Euclidean projection of $a + p_v$ onto the half-space $\\mathcal{H}$. Let $w = a + p_v$. If $y \\, c^\\top w \\ge m$, then $w \\in \\mathcal{H}$ and the projection is $v \\leftarrow w$. Otherwise, we project orthogonally onto the supporting hyperplane $y \\, c^\\top v = m$. The projection in $\\mathbb{R}^p$ onto a half-space defined by a single linear inequality has the closed-form:\n$$\nv \\leftarrow w + \\frac{m - y \\, c^\\top w}{\\left\\| c \\right\\|_2^2} \\, y \\, c.\n$$\nThis follows from minimizing $\\left\\| v - w \\right\\|_2^2$ subject to $y \\, c^\\top v = m$ using Lagrange multipliers; the Lagrangian $\\mathcal{L}(v,\\alpha) = \\left\\| v - w \\right\\|_2^2 + \\alpha (y \\, c^\\top v - m)$ yields the optimality condition $v - w + \\frac{\\alpha}{2} y c = 0$, and eliminating $\\alpha$ enforces the constraint with the minimal-norm correction along $c$.\n\nThe dual variable updates enforce consistency of the constraints:\n$$\np_u \\leftarrow p_u + a - u, \\quad p_v \\leftarrow p_v + a - v.\n$$\n\nConvergence of ADMM for convex problems is guaranteed under mild conditions; here the strong convexity of the quadratic term further aids convergence. We monitor the primal residual $\\left\\| a - u \\right\\|_2$ and $\\left\\| a - v \\right\\|_2$, aggregating them into a single norm, and the dual residual based on changes in $u$ and $v$, stopping when both are below a specified tolerance.\n\nAfter convergence, the minimal energy is computed directly as\n$$\nE(a^\\star; z, \\lambda) = \\frac{1}{2} \\left\\| a^\\star - z \\right\\|_2^2 + \\lambda \\left\\| a^\\star \\right\\|_1.\n$$\n\nWe apply this procedure to the three specified test cases:\n- Test case $1$: $z = [0.8, 0.1, 0.0, 0.2, -0.3, 0.5, 0.0, -0.1]$, $c = [0.6, -0.2, 0.0, 0.3, 0.1, -0.5, 0.0, 0.2]$, $y = +1$, $m = 0.4$, $\\lambda = 0.15$. The constraint $c^\\top a \\ge 0.4$ ensures adequate positive margin; sparsity reduces activity magnitudes.\n- Test case $2$: $z = [-0.2, 0.4, 0.0, -0.1, 0.0, 0.3, 0.05, 0.0]$, $c = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$, $y = +1$, $m = 0.5$, $\\lambda = 0.05$. The constraint reduces to $a_1 \\ge 0.5$, forcing a large correction on the first component despite the sparsity penalty.\n- Test case $3$: $z = [0.1, 0.0, 0.4, -0.2, 0.2, 0.0, 0.0, 0.3]$, $c = [-0.2, 0.5, 0.3, 0.0, -0.4, 0.0, 0.1, 0.2]$, $y = -1$, $m = 0.3$, $\\lambda = 0.2$. The constraint becomes $c^\\top a \\le -0.3$, shifting activity to reduce the linear readout while maintaining sparsity.\n\nThe final program implements the ADMM solver described above and computes the minimal energy for each test case, outputting a single line in the specified format with six decimal places for each energy value.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef soft_threshold(x, tau):\n    # Elementwise soft-thresholding\n    return np.sign(x) * np.maximum(np.abs(x) - tau, 0.0)\n\ndef project_halfspace(w, c, y, m):\n    # Project w onto the half-space {v | y * c^T v >= m}\n    s = y * np.dot(c, w)\n    if s >= m:\n        return w.copy()\n    # Minimal correction along c direction to reach the margin hyperplane\n    c_norm_sq = np.dot(c, c)\n    if c_norm_sq == 0.0:\n        # Degenerate case: cannot enforce margin with zero classifier; return w unchanged\n        return w.copy()\n    tau = (m - s) / c_norm_sq\n    return w + tau * y * c\n\ndef admm_min_energy(z, c, y, m, lam, rho=1.0, max_iters=2000, tol=1e-10):\n    \"\"\"\n    Solve: minimize 0.5||a - z||_2^2 + lam*||a||_1 subject to y * c^T a >= m\n    via ADMM with splitting a = u, a = v.\n    \"\"\"\n    p = z.shape[0]\n    # Initialize variables\n    a = z.copy()\n    u = a.copy()\n    v = a.copy()\n    pu = np.zeros(p)\n    pv = np.zeros(p)\n\n    # Precompute denominator for a-update\n    denom = 1.0 + 2.0 * rho\n\n    for k in range(max_iters):\n        # Save previous u, v for dual residual\n        u_prev = u.copy()\n        v_prev = v.copy()\n\n        # a-update: closed-form for quadratic minimization\n        a = (z + rho * (u - pu) + rho * (v - pv)) / denom\n\n        # u-update: soft-threshold for L1 proximal\n        u = soft_threshold(a + pu, lam / rho)\n\n        # v-update: projection onto half-space\n        v = project_halfspace(a + pv, c, y, m)\n\n        # Dual updates\n        pu += a - u\n        pv += a - v\n\n        # Primal residuals: a - u, a - v\n        r1 = a - u\n        r2 = a - v\n        r_norm = np.sqrt(np.dot(r1, r1) + np.dot(r2, r2))\n\n        # Dual residuals: rho*(u - u_prev), rho*(v - v_prev)\n        s1 = rho * (u - u_prev)\n        s2 = rho * (v - v_prev)\n        s_norm = np.sqrt(np.dot(s1, s1) + np.dot(s2, s2))\n\n        if r_norm < tol and s_norm < tol:\n            break\n\n    # Compute minimal energy\n    energy = 0.5 * np.dot(a - z, a - z) + lam * np.sum(np.abs(a))\n    # Optional: ensure margin satisfied (not printed)\n    # margin_val = y * np.dot(c, a)\n    return energy\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"z\": np.array([0.8, 0.1, 0.0, 0.2, -0.3, 0.5, 0.0, -0.1]),\n            \"c\": np.array([0.6, -0.2, 0.0, 0.3, 0.1, -0.5, 0.0, 0.2]),\n            \"y\": 1.0,\n            \"m\": 0.4,\n            \"lam\": 0.15,\n        },\n        # Test case 2\n        {\n            \"z\": np.array([-0.2, 0.4, 0.0, -0.1, 0.0, 0.3, 0.05, 0.0]),\n            \"c\": np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"y\": 1.0,\n            \"m\": 0.5,\n            \"lam\": 0.05,\n        },\n        # Test case 3\n        {\n            \"z\": np.array([0.1, 0.0, 0.4, -0.2, 0.2, 0.0, 0.0, 0.3]),\n            \"c\": np.array([-0.2, 0.5, 0.3, 0.0, -0.4, 0.0, 0.1, 0.2]),\n            \"y\": -1.0,\n            \"m\": 0.3,\n            \"lam\": 0.2,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        energy = admm_min_energy(z=case[\"z\"], c=case[\"c\"], y=case[\"y\"], m=case[\"m\"], lam=case[\"lam\"], rho=1.0, max_iters=2000, tol=1e-10)\n        results.append(f\"{energy:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}