{
    "hands_on_practices": [
        {
            "introduction": "多项选择模型的核心在于如何处理各个选项之间的相互作用。最简单的模型，如遵循“独立于无关选项”（IIA）原则的模型，假设选项间的随机效用是独立的。本练习将带领您超越这一简化假设，通过一个具体的计算，探索当两个选项的效用存在相关性时，选择概率会如何变化 。这个练习是理解更高级、更具心理学现实性的选择模型（如多变量Probit模型）的关键一步。",
            "id": "3999876",
            "problem": "考虑一个三择一随机效用模型 (RUM)，其中备选项 $i \\in \\{1,2,3\\}$ 的效用为 $U_{i} = \\mu_{i} + \\varepsilon_{i}$。确定性分量 $\\mu_{1}$、$\\mu_{2}$ 和 $\\mu_{3}$ 是固定的实数。噪声向量 $(\\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{3})$ 服从联合高斯分布，其均值为零，协方差矩阵为 $\\Sigma$。该矩阵对角线上的方差为1，且仅在 $\\varepsilon_{1}$ 和 $\\varepsilon_{2}$ 之间存在相关性 $\\rho$，即：\n$$\n\\Sigma = \\begin{pmatrix}\n1  \\rho  0 \\\\\n\\rho  1  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\n假设在三元组内进行成对选择时采用标准的二元probit近似，即选择一个备选项而非另一个的概率仅由相应两个效用噪声的联合分布计算得出。设 $P_{1 \\succ 2}(\\rho)$ 表示在此近似下，成对比较中备选项1被选择优于备选项2的概率，即 $P_{1 \\succ 2}(\\rho) = \\mathbb{P}(U_{1}  U_{2})$。\n\n从随机效用模型的基本定义和联合高斯变量的性质出发，推导 $P_{1 \\succ 2}(\\rho)$ 作为 $\\rho$、$\\mu_{1}$ 和 $\\mu_{2}$ 函数的闭式表达式。请使用标准正态累积分布函数 (CDF) $\\Phi(\\cdot)$ 来表示你的最终答案。无需进行数值计算。",
            "solution": "所述问题在形式上是合理的，其科学基础在于随机效用理论和概率论的原理，并且是适定的。所有必要信息都已提供，可以推导出唯一且有意义的解。因此，我们可以开始推导。\n\n问题要求计算备选项1被选择优于备选项2的概率，记为 $P_{1 \\succ 2}(\\rho)$。在随机效用模型 (RUM) 的框架内，这对应于备选项1的效用 $U_{1}$ 大于备选项2的效用 $U_{2}$ 的事件。\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(U_{1}  U_{2})\n$$\n每个备选项 $i$ 的效用由表达式 $U_{i} = \\mu_{i} + \\varepsilon_{i}$ 给出，其中 $\\mu_{i}$ 是效用的确定性分量，$\\varepsilon_{i}$ 是一个随机噪声项。将此定义代入概率表达式中，我们得到：\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(\\mu_{1} + \\varepsilon_{1}  \\mu_{2} + \\varepsilon_{2})\n$$\n这个不等式可以重新整理，将随机项和确定性项分别置于两侧：\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(\\varepsilon_{2} - \\varepsilon_{1}  \\mu_{1} - \\mu_{2})\n$$\n为了计算这个概率，我们必须确定两个随机变量之差 $\\varepsilon_{2} - \\varepsilon_{1}$ 的概率分布。让我们定义一个新的随机变量 $Y = \\varepsilon_{2} - \\varepsilon_{1}$。问题陈述噪声向量 $(\\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{3})$ 服从联合高斯分布。多元高斯分布的一个基本性质是，其分量的任何线性组合也是一个高斯随机变量。因此，$Y$ 服从正态分布。\n\n为了完全确定 $Y$ 的分布，我们需要求出它的均值 $E[Y]$ 和方差 $\\text{Var}(Y)$。\n\n$Y$ 的均值可由期望算子的线性性质得出：\n$$\nE[Y] = E[\\varepsilon_{2} - \\varepsilon_{1}] = E[\\varepsilon_{2}] - E[\\varepsilon_{1}]\n$$\n问题指明噪声向量的均值为零，所以 $E[\\varepsilon_{1}] = 0$ 且 $E[\\varepsilon_{2}] = 0$。因此，$Y$ 的均值为：\n$$\nE[Y] = 0 - 0 = 0\n$$\n$Y$ 的方差使用两个相关随机变量线性组合的方差公式计算：\n$$\n\\text{Var}(Y) = \\text{Var}(\\varepsilon_{2} - \\varepsilon_{1}) = \\text{Var}(\\varepsilon_{2}) + \\text{Var}((-1)\\varepsilon_{1}) + 2 \\cdot \\text{Cov}(\\varepsilon_{2}, (-1)\\varepsilon_{1})\n$$\n$$\n\\text{Var}(Y) = \\text{Var}(\\varepsilon_{2}) + (-1)^{2}\\text{Var}(\\varepsilon_{1}) - 2 \\cdot \\text{Cov}(\\varepsilon_{1}, \\varepsilon_{2})\n$$\n问题提供了协方差矩阵 $\\Sigma$：\n$$\n\\Sigma = \\begin{pmatrix}\n1  \\rho  0 \\\\\n\\rho  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\n从该矩阵中，我们提取出所需的方差和协方差：\n- $\\varepsilon_{1}$ 的方差为 $\\text{Var}(\\varepsilon_{1}) = \\Sigma_{11} = 1$。\n- $\\varepsilon_{2}$ 的方差为 $\\text{Var}(\\varepsilon_{2}) = \\Sigma_{22} = 1$。\n- $\\varepsilon_{1}$ 和 $\\varepsilon_{2}$ 的协方差为 $\\text{Cov}(\\varepsilon_{1}, \\varepsilon_{2}) = \\Sigma_{12} = \\rho$。\n\n将这些值代入 $Y$ 的方差公式中：\n$$\n\\text{Var}(Y) = 1 + 1 - 2\\rho = 2 - 2\\rho = 2(1-\\rho)\n$$\n所以，随机变量 $Y = \\varepsilon_{2} - \\varepsilon_{1}$ 服从均值为 $0$、方差为 $2(1-\\rho)$ 的正态分布。我们可以记为 $Y \\sim \\mathcal{N}(0, 2(1-\\rho))$。$Y$ 的标准差为 $\\sigma_{Y} = \\sqrt{2(1-\\rho)}$。\n\n现在我们可以用 $Y$ 来表示概率 $P_{1 \\succ 2}(\\rho)$：\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(Y  \\mu_{1} - \\mu_{2})\n$$\n为了使用标准正态累积分布函数 (CDF) $\\Phi(\\cdot)$，我们必须对随机变量 $Y$ 进行标准化。一个标准正态变量 $Z \\sim \\mathcal{N}(0, 1)$ 定义为 $Z = \\frac{Y - E[Y]}{\\sqrt{\\text{Var}(Y)}}$。代入 $Y$ 的均值和方差：\n$$\nZ = \\frac{Y - 0}{\\sqrt{2(1-\\rho)}} = \\frac{Y}{\\sqrt{2(1-\\rho)}}\n$$\n我们可以通过将不等式两边同时除以标准差 $\\sigma_{Y} = \\sqrt{2(1-\\rho)}$ 来变换概率表达式中的不等式。由于标准差是一个正数（我们假设 $|\\rho|  1$ 以保证协方差矩阵为非退化的正定矩阵），不等号的方向保持不变：\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}\\left(\\frac{Y}{\\sqrt{2(1-\\rho)}}  \\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)\n$$\n识别出左侧的标准化变量 $Z$，我们得到：\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}\\left(Z  \\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)\n$$\n根据定义，标准正态分布的累积分布函数为 $\\Phi(x) = \\mathbb{P}(Z  x)$。因此，我们可以将概率的最终表达式写为：\n$$\nP_{1 \\succ 2}(\\rho) = \\Phi\\left(\\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)\n$$\n这就是备选项1被选择优于备选项2的概率的闭式表达式，它是它们的确定性效用 $\\mu_{1}$ 和 $\\mu_{2}$ 以及它们噪声项之间的相关性 $\\rho$ 的函数。使用“二元probit近似”的指令说明了在本次计算中完全忽略第三个备选项是合理的。",
            "answer": "$$\n\\boxed{\\Phi\\left(\\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)}\n$$"
        },
        {
            "introduction": "在多项选择的建模领域，存在着源于不同理论背景的多种框架。本练习旨在对比两种极具影响力的模型：基于经济学效用理论的Softmax（或称多项Logit）模型，以及作为神经计算标准模型的除法归一化（Divisive Normalization）模型 。通过拟合一个模型的参数以匹配另一个模型的行为，您将亲身体会到这两种模型在预测上的异同，并深入理解它们各自的结构如何塑造了最终的选择概率。",
            "id": "3999866",
            "problem": "考虑一个三择一决策的神经环路模型，其竞争性抑制通过分裂归一化 (DN) 实现。设每个选项 $i \\in \\{1,2,3\\}$ 都有一个输入驱动 (增益) $g_i$，并且该环路通过一个归一化项来实例化对其他选项的侧向抑制池化。在感觉和决策环路中，一个被广泛接受的 DN 响应公式是，选项 $i$ 的稳态响应由 $r_i = \\frac{g_i}{\\sigma + \\sum_{j \\neq i} g_j}$ 给出，其中 $\\sigma  0$ 是一个加性半饱和常数，用于反映背景驱动和抑制池偏移。为获得选择概率，假设选择选项 $i$ 的概率与 $r_i$ 成正比，并对所有选项进行归一化：$p_i^{\\mathrm{DN}} = \\frac{r_i}{\\sum_{k=1}^{3} r_k}$。与此同时，考虑一个基于效用的多项式 Logit (MNL，也称为 softmax) 变换的概率决策模型，其中选择概率为 $p_i^{\\mathrm{SM}} = \\frac{\\exp(\\beta g_i)}{\\sum_{k=1}^{3} \\exp(\\beta g_k)}$，其中 $\\beta  0$ 是逆温度参数。\n\n给定三个选项的增益 $g = (2,1,1)$ 和 DN 半饱和常数 $\\sigma = 1$，请基于这些定义比较 DN 和 softmax 的选择概率向量。通过令 softmax 的首选概率 $p_1^{\\mathrm{SM}}$ 等于从环路中获得的 DN 首选概率 $p_1^{\\mathrm{DN}}$，来拟合 softmax 参数 $\\beta$。然后，分析拟合后两个完整分布 $\\{p_i^{\\mathrm{DN}}\\}$ 和 $\\{p_i^{\\mathrm{SM}}\\}$ 之间的任何残差，并根据底层模型结构和增益的对称性解释其存在与否。\n\n以单一闭式解析表达式的形式给出拟合的逆温度参数 $\\beta$。不要对最终答案进行四舍五入。$\\beta$ 不需要物理单位。",
            "solution": "起点是分裂归一化 (DN) 的响应定义 $r_i = \\frac{g_i}{\\sigma + \\sum_{j \\neq i} g_j}$ 和通过归一化构建概率的方法 $p_i^{\\mathrm{DN}} = \\frac{r_i}{\\sum_{k=1}^{3} r_k}$。给定 $g = (2,1,1)$ 和 $\\sigma = 1$。\n\n计算每个选项的 DN 响应：\n- 对于 $i=1$，抑制池排除 $i=1$ 并将其他增益相加，因此 $\\sum_{j \\neq 1} g_j = g_2 + g_3 = 1 + 1 = 2$。所以，\n$$\nr_1 = \\frac{g_1}{\\sigma + \\sum_{j \\neq 1} g_j} = \\frac{2}{1 + 2} = \\frac{2}{3}.\n$$\n- 对于 $i=2$，抑制池排除 $i=2$ 并将 $g_1 + g_3 = 2 + 1 = 3$ 相加，所以\n$$\nr_2 = \\frac{g_2}{\\sigma + \\sum_{j \\neq 2} g_j} = \\frac{1}{1 + 3} = \\frac{1}{4}.\n$$\n- 对于 $i=3$，抑制池排除 $i=3$ 并将 $g_1 + g_2 = 2 + 1 = 3$ 相加，所以\n$$\nr_3 = \\frac{g_3}{\\sigma + \\sum_{j \\neq 3} g_j} = \\frac{1}{1 + 3} = \\frac{1}{4}.\n$$\n\n对响应进行归一化以获得 DN 选择概率：\n$$\n\\sum_{k=1}^{3} r_k = \\frac{2}{3} + \\frac{1}{4} + \\frac{1}{4} = \\frac{2}{3} + \\frac{1}{2} = \\frac{4}{6} + \\frac{3}{6} = \\frac{7}{6}.\n$$\n因此，\n$$\np_1^{\\mathrm{DN}} = \\frac{r_1}{\\sum_k r_k} = \\frac{\\frac{2}{3}}{\\frac{7}{6}} = \\frac{2}{3} \\cdot \\frac{6}{7} = \\frac{4}{7},\n$$\n$$\np_2^{\\mathrm{DN}} = \\frac{r_2}{\\sum_k r_k} = \\frac{\\frac{1}{4}}{\\frac{7}{6}} = \\frac{1}{4} \\cdot \\frac{6}{7} = \\frac{3}{14},\n$$\n$$\np_3^{\\mathrm{DN}} = \\frac{r_3}{\\sum_k r_k} = \\frac{\\frac{1}{4}}{\\frac{7}{6}} = \\frac{3}{14}.\n$$\n根据选项 $2$ 和 $3$ 增益 $(1,1)$ 的对称性，我们有 $p_2^{\\mathrm{DN}} = p_3^{\\mathrm{DN}}$。\n\n接下来，考虑 softmax (多项式 Logit, MNL) 概率 $p_i^{\\mathrm{SM}} = \\frac{\\exp(\\beta g_i)}{\\sum_{k=1}^{3} \\exp(\\beta g_k)}$ 并拟合 $\\beta$，使得首选概率相匹配：$p_1^{\\mathrm{SM}} = p_1^{\\mathrm{DN}} = \\frac{4}{7}$。\n\n令 $x = \\exp(\\beta)$，则 $\\exp(\\beta g_1) = \\exp(2\\beta) = x^{2}$ 且 $\\exp(\\beta g_2) = \\exp(\\beta g_3) = \\exp(\\beta) = x$。那么 softmax 的首选概率是\n$$\np_1^{\\mathrm{SM}} = \\frac{x^{2}}{x^{2} + x + x} = \\frac{x^{2}}{x^{2} + 2x}.\n$$\n我们将其与 DN 的首选概率相等：\n$$\n\\frac{x^{2}}{x^{2} + 2x} = \\frac{4}{7}.\n$$\n两边同乘以 $x^{2} + 2x$ 以避免除法：\n$$\nx^{2} = \\frac{4}{7} \\left(x^{2} + 2x\\right).\n$$\n展开右边：\n$$\nx^{2} = \\frac{4}{7} x^{2} + \\frac{8}{7} x.\n$$\n两边同减去 $\\frac{4}{7}x^{2}$：\n$$\nx^{2} - \\frac{4}{7} x^{2} = \\frac{8}{7} x,\n$$\n$$\n\\left(1 - \\frac{4}{7}\\right) x^{2} = \\frac{8}{7} x,\n$$\n$$\n\\frac{3}{7} x^{2} = \\frac{8}{7} x.\n$$\n两边同乘以 $\\frac{7}{x}$ (假设 $x  0$，因为 $x = \\exp(\\beta)$ 所以该假设成立)：\n$$\n3 x = 8,\n$$\n$$\nx = \\frac{8}{3}.\n$$\n因此，\n$$\n\\beta = \\ln(x) = \\ln\\!\\left(\\frac{8}{3}\\right).\n$$\n\n使用这个拟合的 $\\beta$，完整的 softmax 分布是\n$$\np_1^{\\mathrm{SM}} = \\frac{x^{2}}{x^{2} + 2x} = \\frac{\\left(\\frac{8}{3}\\right)^{2}}{\\left(\\frac{8}{3}\\right)^{2} + 2 \\cdot \\frac{8}{3}} = \\frac{\\frac{64}{9}}{\\frac{64}{9} + \\frac{16}{3}} = \\frac{\\frac{64}{9}}{\\frac{64}{9} + \\frac{48}{9}} = \\frac{\\frac{64}{9}}{\\frac{112}{9}} = \\frac{64}{112} = \\frac{4}{7},\n$$\n并且，根据 $g_2 = g_3$ 的对称性，\n$$\np_2^{\\mathrm{SM}} = p_3^{\\mathrm{SM}} = \\frac{x}{x^{2} + 2x} = \\frac{1}{x + 2} = \\frac{1}{\\frac{8}{3} + 2} = \\frac{1}{\\frac{8}{3} + \\frac{6}{3}} = \\frac{1}{\\frac{14}{3}} = \\frac{3}{14}.\n$$\n因此，在这种对称配置下，拟合的 softmax 精确地复现了整个 DN 概率向量：$\\left(p_1^{\\mathrm{SM}}, p_2^{\\mathrm{SM}}, p_3^{\\mathrm{SM}}\\right) = \\left(\\frac{4}{7}, \\frac{3}{14}, \\frac{3}{14}\\right) = \\left(p_1^{\\mathrm{DN}}, p_2^{\\mathrm{DN}}, p_3^{\\mathrm{DN}}\\right)$。一个自然的残差度量，例如 Kullback-Leibler (KL) 散度，将会是\n$$\nD_{\\mathrm{KL}}\\!\\left(p^{\\mathrm{DN}} \\,\\|\\, p^{\\mathrm{SM}}\\right) = \\sum_{i=1}^{3} p_i^{\\mathrm{DN}} \\ln\\!\\left(\\frac{p_i^{\\mathrm{DN}}}{p_i^{\\mathrm{SM}}}\\right) = 0,\n$$\n因为每个比率 $\\frac{p_i^{\\mathrm{DN}}}{p_i^{\\mathrm{SM}}} = 1$。这里没有残差是由于两个结构性特征：(i) 所使用的竞争性 DN 公式在存在高增益竞争者时，对较低增益的选项产生更高的抑制作用；(ii) 带有单个参数 $\\beta$ 的 softmax 必须为具有相等增益的选项分配相等的概率。当较低增益选项的增益对称时，通过匹配首选概率所确定的 $\\beta$ 值可以确保所有概率都完全一致。在更一般的非对称场景（例如 $g = (2,1,0.8)$）或使用非均匀抑制权重 $\\sum_{j \\neq i} w_{ij} g_j$ 的情况下，即使在拟合 $\\beta$ 以匹配 $p_1$ 之后，DN 和 softmax 通常也会产生不同的分布，从而产生非零的残差，这反映了它们不同的归一化架构和非线性特性。",
            "answer": "$$\\boxed{\\ln\\!\\left(\\frac{8}{3}\\right)}$$"
        },
        {
            "introduction": "现实世界中的许多决策并非一蹴而就，而是在一个动态过程中不断学习和调整的。本练习将引导您从静态选择模型转向动态序贯决策的领域，直面经典的“探索-利用”（exploration-exploitation）困境 。您将通过动态规划来构建一个贝叶斯最优策略，并将其与一个基于启发式的次优策略进行比较，这是一个兼具理论深度与计算挑战的实践，反映了决策神经科学的前沿研究方向。",
            "id": "3999877",
            "problem": "考虑一个由三个独立的二元结果选项（“臂”）组成的集合，其索引为 $i \\in \\{1,2,3\\}$。在任何离散的决策时期，利用（exploitation）每个臂 $i$ 都会产生一个伯努利奖励，其成功概率 $\\theta_i \\in [0,1]$ 未知。贝叶斯决策者对每个臂 $i$ 都有一个共轭贝塔先验，参数为 $(\\alpha_{i0},\\beta_{i0})$，因此 $\\theta_i$ 的先验密度与 $\\theta_i^{\\alpha_{i0}-1}(1-\\theta_i)^{\\beta_{i0}-1}$ 成正比。时间被划分为 $T$ 个离散的决策时期，索引为 $t \\in \\{0,1,\\dots,T-1\\}$。在每个时期 $t$，决策者可以选择对一个臂进行采样，以观察单个伯努利结果并更新相应的后验（此次观察无直接奖励），这会产生一个确定的时间成本 $c  0$；或者停止采样，并通过在所有剩余时期内专攻一个臂来进行利用，从而在每个剩余时期获得伯努利奖励。目标是最大化利用所产生的预期累积奖励减去采样成本的总和。\n\n将此问题形式化为一个基于贝叶斯后验充分统计量和时间的有限期决策过程。令状态为 $s_t = \\big(t,(\\alpha_{i,t},\\beta_{i,t})_{i=1}^3\\big)$，其中 $(\\alpha_{i,t},\\beta_{i,t})$ 是截至时间 $t$ 的采样历史之后的后验贝塔参数。在时间 $t$，臂 $i$ 的后验预测成功概率为 $\\mu_{i,t} = \\frac{\\alpha_{i,t}}{\\alpha_{i,t}+\\beta_{i,t}}$，$\\theta_i$ 的后验方差为 $v_{i,t} = \\frac{\\alpha_{i,t}\\beta_{i,t}}{(\\alpha_{i,t}+\\beta_{i,t})^2(\\alpha_{i,t}+\\beta_{i,t}+1)}$。如果决策者在时间 $t$ 停止并专攻臂 $i^\\star \\in \\arg\\max_i \\mu_{i,t}$，预期的利用价值为 $(T-t)\\max_i \\mu_{i,t}$。如果决策者在时间 $t$ 采样臂 $i$，下一个状态以概率 $\\mu_{i,t}$（观察到成功）变为 $s_{t+1}^+ = \\big(t+1,(\\alpha_{i,t}+1,\\beta_{i,t}),(\\alpha_{j,t},\\beta_{j,t})_{j\\neq i}\\big)$，或者以概率 $1-\\mu_{i,t}$（观察到失败）变为 $s_{t+1}^- = \\big(t+1,(\\alpha_{i,t},\\beta_{i,t}+1),(\\alpha_{j,t},\\beta_{j,t})_{j\\neq i}\\big)$，并且在时间 $t$ 产生采样成本 $c$。\n\n从有限期序贯决策的贝叶斯最优性原理和 Bellman 递归出发，推导并实现一个动态规划算法，该算法计算每个可达状态下的贝叶斯最优停止规则和期望价值函数 $V^\\star(s_t)$。推导必须从不确定性下的期望效用最大化、贝塔-伯努利共轭更新以及有限期决策过程的 Bellman 最优性原理的定义开始。\n\n同时定义一个紧急度门控的 softmax 启发式策略：在状态 $s_t$ 下，为停止动作赋予效用 $U_{\\text{stop}}(s_t) = (T-t)\\max_i \\mu_{i,t}$，为采样臂 $i$ 的动作赋予效用 $U_{\\text{sample},i}(s_t) = \\alpha_{\\text{vi}}\\sqrt{v_{i,t}}\\cdot \\max(T-t-1,0) - c$，其中 $\\alpha_{\\text{vi}}  0$ 是一个标量“信息价值”权重。设 softmax 温度为 $\\tau(t) = \\frac{\\tau_0}{1+\\gamma t}$，其中 $\\tau_0  0$ 且 $\\gamma \\ge 0$。启发式动作概率为\n$$\n\\pi(\\text{stop}\\mid s_t) = \\frac{\\exp\\left(\\frac{U_{\\text{stop}}(s_t)}{\\tau(t)}\\right)}{\\exp\\left(\\frac{U_{\\text{stop}}(s_t)}{\\tau(t)}\\right) + \\sum_{i=1}^3 \\exp\\left(\\frac{U_{\\text{sample},i}(s_t)}{\\tau(t)}\\right)},\n\\quad\n\\pi(\\text{sample},i\\mid s_t) = \\frac{\\exp\\left(\\frac{U_{\\text{sample},i}(s_t)}{\\tau(t)}\\right)}{\\exp\\left(\\frac{U_{\\text{stop}}(s_t)}{\\tau(t)}\\right) + \\sum_{j=1}^3 \\exp\\left(\\frac{U_{\\text{sample},j}(s_t)}{\\tau(t)}\\right)}.\n$$\n通过有限期 Bellman 期望递归，并对后验预测结果进行精确积分，计算由该启发式策略导出的期望价值函数 $V_\\pi(s_t)$。\n\n对于下述每个测试用例，您的程序必须计算：(1) 一个布尔值，指示贝叶斯最优策略是否在 $t=0$ 时立即停止（若在 $t=0$ 选择停止则为 true，否则为 false），(2) 在贝叶斯最优策略下的期望价值 $V^\\star(s_0)$，(3) 在指定参数下，紧急度门控 softmax 启发式策略的期望价值 $V_\\pi(s_0)$，以及 (4) 差值 $V^\\star(s_0) - V_\\pi(s_0)$。所有期望值必须是相对于后验预测分布的精确计算，而不是蒙特卡洛近似。不涉及物理单位，答案为实值浮点数或布尔值。\n\n使用以下参数集测试套件（每个用例指定 $T$、$c$、以及 $i\\in\\{1,2,3\\}$ 的先验 $(\\alpha_{i0},\\beta_{i0})$，连同启发式参数 $\\tau_0$、$\\gamma$、$\\alpha_{\\text{vi}}$）：\n\n- 用例 1（“顺利路径”）：$T=4$，$c=0.1$，先验 $(\\alpha_{10},\\beta_{10})=(1,1)$，$(\\alpha_{20},\\beta_{20})=(1,1)$，$(\\alpha_{30},\\beta_{30})=(1,1)$，启发式参数 $\\tau_0=0.5$，$\\gamma=1.0$，$\\alpha_{\\text{vi}}=1.0$。\n- 用例 2（边界情况，最小期限）：$T=1$，$c=0.5$，先验 $(\\alpha_{10},\\beta_{10})=(1,1)$，$(\\alpha_{20},\\beta_{20})=(1,1)$，$(\\alpha_{30},\\beta_{30})=(1,1)$，启发式参数 $\\tau_0=0.5$，$\\gamma=1.0$，$\\alpha_{\\text{vi}}=1.0$。\n- 用例 3（单臂信息先验，低采样成本）：$T=5$，$c=0.05$，先验 $(\\alpha_{10},\\beta_{10})=(8,2)$，$(\\alpha_{20},\\beta_{20})=(2,8)$，$(\\alpha_{30},\\beta_{30})=(1,1)$，启发式参数 $\\tau_0=0.4$，$\\gamma=1.0$，$\\alpha_{\\text{vi}}=1.0$。\n- 用例 4（高采样成本）：$T=5$，$c=0.6$，先验 $(\\alpha_{10},\\beta_{10})=(1,1)$，$(\\alpha_{20},\\beta_{20})=(1,1)$，$(\\alpha_{30},\\beta_{30})=(1,1)$，启发式参数 $\\tau_0=0.4$，$\\gamma=1.0$，$\\alpha_{\\text{vi}}=1.0$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果本身就是一个包含上述四个量的列表。例如，输出格式必须类似于 $[[$布尔值$, $浮点数$, $浮点数$, $浮点数$],[$布尔值$, $浮点数$, $浮点数$, $浮点数$],\\dots]$。",
            "solution": "该问题描述了一个有限期贝叶斯最优停止问题，这是一种特定类型的马尔可夫决策过程 (MDP)。目标是设计一个策略，以最大化预期净奖励，该奖励定义为利用所产生的累积奖励减去采样的累积成本。我们将根据 Bellman 最优性原理的规定，通过反向归纳法使用动态规划来解决此问题。\n\n### 1. 形式化问题规范\n\n让我们首先形式化决策过程的各个组成部分。\n- **状态空间**：在决策时期 $t \\in \\{0, 1, \\dots, T-1\\}$ 的状态由 $s_t = \\big(t, (\\alpha_{1,t}, \\beta_{1,t}), (\\alpha_{2,t}, \\beta_{2,t}), (\\alpha_{3,t}, \\beta_{3,t})\\big)$ 给出。这里，$t$ 是当前时间，$(\\alpha_{i,t}, \\beta_{i,t})$ 是臂 $i$ 的未知成功概率 $\\theta_i$ 的贝塔后验分布的参数。初始状态为 $s_0 = \\big(0, (\\alpha_{10}, \\beta_{10}), (\\alpha_{20}, \\beta_{20}), (\\alpha_{30}, \\beta_{30})\\big)$。\n\n- **动作空间**：在任何 $t  T$ 的状态 $s_t$ 下，智能体可以从集合 $A = \\{\\text{停止}\\} \\cup \\{\\text{采样臂 } i\\}_{i=1}^3$ 中选择一个动作 $a_t$。\n\n- **转移和奖励**：\n    - 如果 $a_t = \\text{停止}$，过程终止。智能体收到一个终期奖励，该奖励等于在剩余的 $T-t$ 个时期内利用表现最好的臂所获得的预期奖励总和。臂 $i$ 的后验预测平均成功概率为 $\\mu_{i,t} = E[\\theta_i | s_t] = \\frac{\\alpha_{i,t}}{\\alpha_{i,t}+\\beta_{i,t}}$。要利用的最优臂是 $i^\\star = \\arg\\max_i \\mu_{i,t}$。因此，终期奖励为 $(T-t) \\max_i \\mu_{i,t}$。\n    - 如果 $a_t = \\text{采样臂 } i$，智能体支付成本 $c  0$。对臂 $i$ 进行一次伯努利试验。结果为成功的概率是 $\\mu_{i,t}$（后验预测成功概率），此时状态转移到 $s_{t+1}^+ = \\big(t+1, \\dots, (\\alpha_{i,t}+1, \\beta_{i,t}), \\dots\\big)$。结果为失败的概率是 $1-\\mu_{i,t}$，此时状态转移到 $s_{t+1}^- = \\big(t+1, \\dots, (\\alpha_{i,t}, \\beta_{i,t}+1), \\dots\\big)$。对于臂 $j \\neq i$，其参数保持不变。\n\n### 2. 贝叶斯最优策略和价值函数 $V^\\star(s_t)$\n\n目标是找到一个最优策略 $\\pi^\\star$，以最大化预期总净奖励。该最优策略的价值 $V^\\star(s_t) = \\max_{\\pi} E_\\pi \\left[ \\sum_{k=t}^{T-1} R(s_k, a_k) | s_t \\right]$ 可以使用 Bellman 最优性原理找到。对于有限期问题，这意味着一个反向归纳过程。\n\n在终止时间 $t=T$ 的价值函数为 $V^\\star(s_T) = 0$，因为没有进一步的动作或奖励是可能的。\n\n对于任何时间 $t \\in \\{T-1, T-2, \\dots, 0\\}$，最优价值函数 $V^\\star(s_t)$ 是在状态 $s_t$ 下所有可能动作的价值的最大值。我们定义动作价值函数 $Q^\\star(s_t, a_t)$ 为在状态 $s_t$ 采取动作 $a_t$ 并在此后遵循最优策略的预期回报。\n\n- **停止的价值**：如果动作为停止，过程结束，价值即为直接的终期奖励。\n$$Q^\\star(s_t, \\text{停止}) = (T-t) \\max_{i \\in \\{1,2,3\\}} \\mu_{i,t}$$\n\n- **采样的价值**：如果动作为采样臂 $i$，智能体产生一个成本 $c$，然后转移到一个新状态 $s_{t+1}$，并从该状态获得最优价值 $V^\\star(s_{t+1})$。该价值是成本加上下一状态的预期最优价值。\n$$Q^\\star(s_t, \\text{采样臂 } i) = -c + E[V^\\star(s_{t+1}) | s_t, a_t=\\text{采样臂 } i]$$\n期望是针对样本的两种可能结果（成功或失败）计算的：\n$$Q^\\star(s_t, \\text{采样臂 } i) = -c + \\mu_{i,t} V^\\star(s_{t+1}^+) + (1-\\mu_{i,t}) V^\\star(s_{t+1}^-)$$\n其中 $s_{t+1}^+$ 和 $s_{t+1}^-$ 分别是在臂 $i$ 上观察到成功或失败后的后继状态。\n\n因此，Bellman 最优性方程为：\n$$V^\\star(s_t) = \\max \\left( Q^\\star(s_t, \\text{停止}), \\max_{i \\in \\{1,2,3\\}} Q^\\star(s_t, \\text{采样臂 } i) \\right)$$\n最优动作 $a^\\star(s_t)$ 是实现此最大值的动作。这个递归可以通过从 $t=T-1$ 开始并向后递推到 $t=0$ 来求解。\n\n### 3. 启发式策略和价值函数 $V_\\pi(s_t)$\n\n该问题还定义了一个基于动作效用的 softmax 函数的启发式策略 $\\pi$。该策略的价值函数 $V_\\pi(s_t)$ 可以使用 Bellman 期望方程来计算，该方程与最优性方程相似，但用对策略动作概率的期望代替了 $\\max$ 算子。\n\n效用定义如下：\n- $U_{\\text{stop}}(s_t) = (T-t)\\max_i \\mu_{i,t}$\n- $U_{\\text{sample},i}(s_t) = \\alpha_{\\text{vi}}\\sqrt{v_{i,t}}\\cdot \\max(T-t-1,0) - c$\n\n其中 $v_{i,t} = \\frac{\\alpha_{i,t}\\beta_{i,t}}{(\\alpha_{i,t}+\\beta_{i,t})^2(\\alpha_{i,t}+\\beta_{i,t}+1)}$ 是 $\\theta_i$ 的后验方差。温度为 $\\tau(t) = \\frac{\\tau_0}{1+\\gamma t}$。动作概率为：\n$$\n\\pi(\\text{动作}|s_t) = \\frac{\\exp\\left( \\frac{U_{\\text{动作}}(s_t)}{\\tau(t)} \\right)}{\\sum_{\\text{动作}'} \\exp\\left( \\frac{U_{\\text{动作}'}(s_t)}{\\tau(t)} \\right)}\n$$\n$V_\\pi$ 的 Bellman 期望方程为：\n$$V_\\pi(s_t) = \\sum_{a_t \\in A} \\pi(a_t|s_t) Q_\\pi(s_t, a_t)$$\n其中 $Q_\\pi(s_t, a_t)$ 是采取动作 $a_t$ 然后遵循策略 $\\pi$ 的价值。\n\n- 如果 $a_t = \\text{停止}$：$Q_\\pi(s_t, \\text{停止}) = (T-t) \\max_i \\mu_{i,t}$。\n- 如果 $a_t = \\text{采样臂 } i$：$Q_\\pi(s_t, \\text{采样臂 } i) = -c + E[V_\\pi(s_{t+1}) | s_t, a_t=\\text{采样臂 } i]$。这个期望像之前一样展开：\n$$Q_\\pi(s_t, \\text{采样臂 } i) = -c + \\mu_{i,t}V_\\pi(s_{t+1}^+) + (1-\\mu_{i,t})V_\\pi(s_{t+1}^-)$$\n\n将这些代入 Bellman 方程，得到 $V_\\pi$ 的递归式：\n$$\nV_\\pi(s_t) = \\pi(\\text{停止}|s_t) \\left( (T-t)\\max_i \\mu_{i,t} \\right) + \\sum_{i=1}^3 \\pi(\\text{采样},i|s_t) \\left( -c + \\mu_{i,t}V_\\pi(s_{t+1}^+) + (1-\\mu_{i,t})V_\\pi(s_{t+1}^-) \\right)\n$$\n与 $V^\\star$ 一样，这也是通过从边界条件 $V_\\pi(s_T)=0$ 开始的反向归纳来求解。\n\n### 4. 算法实现\n\n为了求解这些递归，我们实现了一个动态规划算法。我们使用带有记忆化（缓存）的自顶向下递归方法，以避免对同一状态重复计算值。使用一个字典作为缓存，其中键是状态元组 $(t, \\text{priors})$。\n\n实现了两个递归函数，`compute_V_star` 和 `compute_V_pi`。每个函数都接受一个状态 $(t, \\text{priors})$ 作为输入。\n1. 首先检查记忆化缓存中是否存在该状态的预计算值。\n2. 如果 $t=T$，则返回 $0$。\n3. 否则，它通过递归调用自身来计算时间 $t+1$ 状态下的停止和采样价值。\n4. 然后，它根据 a) $V^\\star$ 的 Bellman 最优性方程 或 b) $V_\\pi$ 的 Bellman 期望方程来组合这些价值。\n5. 结果被存储在缓存中并返回。\n\n为了确定初始状态 $s_0$ 的最优动作，我们显式地计算所有 $i$ 的 $Q^\\star(s_0, \\text{停止})$ 和 $Q^\\star(s_0, \\text{采样臂 } i)$。这需要所有可能的后继状态的 $V^\\star(s_1)$ 值，这些值从递归求解器中获得。然后通过比较这些 Q 值来找到最优动作。如果停止的价值大于或等于采样的最大价值，则最优策略选择停止。最终程序遍历所提供的测试用例，为每个用例设置参数，并调用求解器来计算所需的量。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n\n    class Solver:\n        \"\"\"\n        Solves the Bayesian optimal stopping problem for a given set of parameters.\n        \"\"\"\n\n        def __init__(self, T, c, priors_0, tau_0, gamma, alpha_vi):\n            self.T = T\n            self.c = c\n            self.priors_0 = tuple(tuple(p) for p in priors_0)\n            self.tau_0 = tau_0\n            self.gamma = gamma\n            self.alpha_vi = alpha_vi\n            \n            # Memoization caches for the value functions\n            self.memo_v_star = {}\n            self.memo_v_pi = {}\n\n        def run(self):\n            \"\"\"\n            Computes the four required quantities for the initial state s_0.\n            \"\"\"\n            # 1. Optimal Policy: V_star and immediate action\n            alphas_0 = np.array([p[0] for p in self.priors_0], dtype=float)\n            betas_0 = np.array([p[1] for p in self.priors_0], dtype=float)\n            mus_0 = alphas_0 / (alphas_0 + betas_0)\n\n            # Value of stopping at t=0\n            v_stop_s0 = self.T * np.max(mus_0)\n            \n            # Value of sampling each arm at t=0\n            v_samples_s0 = []\n            for i in range(3):\n                # Success state\n                priors_succ_list = list(self.priors_0)\n                priors_succ_list[i] = (self.priors_0[i][0] + 1, self.priors_0[i][1])\n                v_succ = self._compute_v_star(1, tuple(priors_succ_list))\n\n                # Failure state\n                priors_fail_list = list(self.priors_0)\n                priors_fail_list[i] = (self.priors_0[i][0], self.priors_0[i][1] + 1)\n                v_fail = self._compute_v_star(1, tuple(priors_fail_list))\n                \n                v_sample_i = -self.c + mus_0[i] * v_succ + (1 - mus_0[i]) * v_fail\n                v_samples_s0.append(v_sample_i)\n\n            v_star_s0 = max(v_stop_s0, max(v_samples_s0))\n            is_stop_optimal = (v_stop_s0 = max(v_samples_s0))\n\n            # 2. Heuristic Policy: V_pi\n            v_pi_s0 = self._compute_v_pi(0, self.priors_0)\n\n            # 3. Difference\n            diff = v_star_s0 - v_pi_s0\n            \n            return [is_stop_optimal, v_star_s0, v_pi_s0, diff]\n\n        def _compute_v_star(self, t, priors):\n            \"\"\"\n            Computes the optimal value function V* using backward recursion.\n            \"\"\"\n            state = (t, priors)\n            if state in self.memo_v_star:\n                return self.memo_v_star[state]\n            \n            if t == self.T:\n                return 0.0\n\n            alphas = np.array([p[0] for p in priors], dtype=float)\n            betas = np.array([p[1] for p in priors], dtype=float)\n            mus = alphas / (alphas + betas)\n            \n            # Value of stopping\n            v_stop = (self.T - t) * np.max(mus)\n            \n            # Value of sampling\n            v_samples = []\n            for i in range(3):\n                priors_succ_list = list(priors)\n                priors_succ_list[i] = (alphas[i] + 1, betas[i])\n                v_succ = self._compute_v_star(t + 1, tuple(priors_succ_list))\n\n                priors_fail_list = list(priors)\n                priors_fail_list[i] = (alphas[i], betas[i] + 1)\n                v_fail = self._compute_v_star(t + 1, tuple(priors_fail_list))\n                \n                v_sample_i = -self.c + mus[i] * v_succ + (1 - mus[i]) * v_fail\n                v_samples.append(v_sample_i)\n\n            v_star = max([v_stop] + v_samples)\n            self.memo_v_star[state] = v_star\n            return v_star\n\n        def _compute_v_pi(self, t, priors):\n            \"\"\"\n            Computes the heuristic policy's value function V_pi using backward recursion.\n            \"\"\"\n            state = (t, priors)\n            if state in self.memo_v_pi:\n                return self.memo_v_pi[state]\n            \n            if t == self.T:\n                return 0.0\n\n            alphas = np.array([p[0] for p in priors], dtype=float)\n            betas = np.array([p[1] for p in priors], dtype=float)\n            mus = alphas / (alphas + betas)\n            denominators = alphas + betas\n            variances = (alphas * betas) / (denominators**2 * (denominators + 1))\n            \n            # Heuristic utilities\n            u_stop = (self.T - t) * np.max(mus)\n            rem_exploitation_steps = max(self.T - t - 1, 0)\n            u_samples = self.alpha_vi * np.sqrt(variances) * rem_exploitation_steps - self.c\n\n            # Softmax probabilities\n            temp = self.tau_0 / (1 + self.gamma * t)\n            \n            utils = np.concatenate(([u_stop], u_samples))\n            \n            if temp == 0: # Zero-temperature limit (greedy)\n                pi_actions = np.zeros_like(utils)\n                winner_indices = np.where(utils == np.max(utils))[0]\n                pi_actions[winner_indices] = 1.0 / len(winner_indices)\n            else: # Standard softmax with stability trick\n                utils_shifted = utils - np.max(utils)\n                exp_utils = np.exp(utils_shifted / temp)\n                pi_actions = exp_utils / np.sum(exp_utils)\n            \n            pi_stop = pi_actions[0]\n            pi_samples = pi_actions[1:]\n            \n            # Bellman Expectation\n            # Contribution from stopping\n            v_pi = pi_stop * u_stop\n            \n            # Contribution from sampling\n            for i in range(3):\n                if pi_samples[i]  0:\n                    priors_succ_list = list(priors)\n                    priors_succ_list[i] = (alphas[i] + 1, betas[i])\n                    v_succ = self._compute_v_pi(t + 1, tuple(priors_succ_list))\n\n                    priors_fail_list = list(priors)\n                    priors_fail_list[i] = (alphas[i], betas[i] + 1)\n                    v_fail = self._compute_v_pi(t + 1, tuple(priors_fail_list))\n                    \n                    v_sample_i_future = -self.c + mus[i] * v_succ + (1 - mus[i]) * v_fail\n                    v_pi += pi_samples[i] * v_sample_i_future\n\n            self.memo_v_pi[state] = v_pi\n            return v_pi\n\n    test_cases = [\n        {'T': 4, 'c': 0.1, 'priors_0': [[1,1],[1,1],[1,1]], 'tau_0': 0.5, 'gamma': 1.0, 'alpha_vi': 1.0},\n        {'T': 1, 'c': 0.5, 'priors_0': [[1,1],[1,1],[1,1]], 'tau_0': 0.5, 'gamma': 1.0, 'alpha_vi': 1.0},\n        {'T': 5, 'c': 0.05, 'priors_0': [[8,2],[2,8],[1,1]], 'tau_0': 0.4, 'gamma': 1.0, 'alpha_vi': 1.0},\n        {'T': 5, 'c': 0.6, 'priors_0': [[1,1],[1,1],[1,1]], 'tau_0': 0.4, 'gamma': 1.0, 'alpha_vi': 1.0},\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        solver_instance = Solver(**case_params)\n        result = solver_instance.run()\n        all_results.append(result)\n    \n    # Format the final output string\n    result_strings = [str(res) for res in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}