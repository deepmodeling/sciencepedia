{
    "hands_on_practices": [
        {
            "introduction": "在多项选择模型中，一个核心假设是决策者对每个选项的效用或价值的评估都伴随着随机“噪声”。这个练习将探讨当这些噪声项之间存在相关性时会发生什么，这种情况在神经表征中很常见。通过亲手计算在一个三选项情景中，两个选项间的噪声相关性 $\\rho$ 如何影响它们之间的选择概率 ，你将深刻理解随机效用模型（Random Utility Model）的一个关键特征，并为理解更复杂的选择行为（如“相似性”效应）打下基础。",
            "id": "3999876",
            "problem": "考虑一个有三个备选项的随机效用模型 (RUM)，其中备选项 $i \\in \\{1,2,3\\}$ 的效用为 $U_{i} = \\mu_{i} + \\varepsilon_{i}$。确定性分量 $\\mu_{1}$、$\\mu_{2}$ 和 $\\mu_{3}$ 是固定的实数。噪声向量 $(\\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{3})$ 服从联合高斯分布，其均值为零，协方差矩阵为 $\\Sigma$。该矩阵对角线上的方差为单位值，且仅在 $\\varepsilon_{1}$ 和 $\\varepsilon_{2}$ 之间存在相关性 $\\rho$，即：\n$$\n\\Sigma = \\begin{pmatrix}\n1  \\rho  0 \\\\\n\\rho  1  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\n假设对于三元组内的成对选择，使用标准的二元probit近似，即选择一个备选项优于另一个的概率仅由对应的两个效用噪声的联合分布计算得出。令 $P_{1 \\succ 2}(\\rho)$ 表示在此近似下，成对比较中备选项1优于备选项2的概率，即 $P_{1 \\succ 2}(\\rho) = \\mathbb{P}(U_{1} > U_{2})$。\n\n从随机效用模型的基本定义和联合高斯变量的性质出发，推导 $P_{1 \\succ 2}(\\rho)$ 作为 $\\rho$、$\\mu_{1}$ 和 $\\mu_{2}$ 的函数的闭式表达式。使用标准正态累积分布函数 (CDF) $\\Phi(\\cdot)$ 表示您的最终答案。无需进行数值计算。",
            "solution": "所述问题在形式上是合理的，科学上基于随机效用理论和概率论的原理，且提法恰当。所有推导唯一且有意义解的必要信息均已提供。因此，我们可以开始进行推导。\n\n问题要求计算备选项1优于备选项2的概率，记为 $P_{1 \\succ 2}(\\rho)$。在随机效用模型 (RUM) 的框架内，这对应于备选项1的效用 $U_{1}$ 大于备选项2的效用 $U_{2}$ 的事件。\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(U_{1} > U_{2})\n$$\n每个备选项 $i$ 的效用由表达式 $U_{i} = \\mu_{i} + \\varepsilon_{i}$ 给出，其中 $\\mu_{i}$ 是效用的确定性分量，$\\varepsilon_{i}$ 是一个随机噪声项。将此定义代入概率表达式，我们得到：\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(\\mu_{1} + \\varepsilon_{1} > \\mu_{2} + \\varepsilon_{2})\n$$\n可以对这个不等式进行整理，将随机项和确定性项分别置于不等号的两侧：\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(\\varepsilon_{2} - \\varepsilon_{1}  \\mu_{1} - \\mu_{2})\n$$\n为计算此概率，我们必须确定两个随机变量之差 $\\varepsilon_{2} - \\varepsilon_{1}$ 的概率分布。我们定义一个新的随机变量 $Y = \\varepsilon_{2} - \\varepsilon_{1}$。问题指出，噪声向量 $(\\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{3})$ 服从联合高斯分布。多元高斯分布的一个基本性质是，其分量的任何线性组合也是一个高斯随机变量。因此，$Y$ 服从正态分布。\n\n为了完全刻画 $Y$ 的分布，我们需要求出其均值 $E[Y]$ 和方差 $\\text{Var}(Y)$。\n\n根据期望算子的线性性质，$Y$ 的均值为：\n$$\nE[Y] = E[\\varepsilon_{2} - \\varepsilon_{1}] = E[\\varepsilon_{2}] - E[\\varepsilon_{1}]\n$$\n问题指明噪声向量的均值为零，所以 $E[\\varepsilon_{1}] = 0$ 且 $E[\\varepsilon_{2}] = 0$。因此，$Y$ 的均值为：\n$$\nE[Y] = 0 - 0 = 0\n$$\n$Y$ 的方差使用两个相关随机变量的线性组合的方差公式进行计算：\n$$\n\\text{Var}(Y) = \\text{Var}(\\varepsilon_{2} - \\varepsilon_{1}) = \\text{Var}(\\varepsilon_{2}) + \\text{Var}((-1)\\varepsilon_{1}) + 2 \\cdot \\text{Cov}(\\varepsilon_{2}, (-1)\\varepsilon_{1})\n$$\n$$\n\\text{Var}(Y) = \\text{Var}(\\varepsilon_{2}) + (-1)^{2}\\text{Var}(\\varepsilon_{1}) - 2 \\cdot \\text{Cov}(\\varepsilon_{1}, \\varepsilon_{2})\n$$\n问题提供了协方差矩阵 $\\Sigma$：\n$$\n\\Sigma = \\begin{pmatrix}\n1  \\rho  0 \\\\\n\\rho  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\n从该矩阵中，我们提取所需的方差和协方差：\n- $\\varepsilon_{1}$ 的方差为 $\\text{Var}(\\varepsilon_{1}) = \\Sigma_{11} = 1$。\n- $\\varepsilon_{2}$ 的方差为 $\\text{Var}(\\varepsilon_{2}) = \\Sigma_{22} = 1$。\n- $\\varepsilon_{1}$ 和 $\\varepsilon_{2}$ 的协方差为 $\\text{Cov}(\\varepsilon_{1}, \\varepsilon_{2}) = \\Sigma_{12} = \\rho$。\n\n将这些值代入 $Y$ 的方差公式中：\n$$\n\\text{Var}(Y) = 1 + 1 - 2\\rho = 2 - 2\\rho = 2(1-\\rho)\n$$\n所以，随机变量 $Y = \\varepsilon_{2} - \\varepsilon_{1}$ 服从均值为 0、方差为 $2(1-\\rho)$ 的正态分布。我们可以将其写作 $Y \\sim \\mathcal{N}(0, 2(1-\\rho))$。$Y$ 的标准差是 $\\sigma_{Y} = \\sqrt{2(1-\\rho)}$。\n\n现在我们可以用 $Y$ 来表示概率 $P_{1 \\succ 2}(\\rho)$：\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(Y  \\mu_{1} - \\mu_{2})\n$$\n为了使用标准正态累积分布函数 (CDF) $\\Phi(\\cdot)$，我们必须对随机变量 $Y$ 进行标准化。一个标准正态变量 $Z \\sim \\mathcal{N}(0, 1)$ 定义为 $Z = \\frac{Y - E[Y]}{\\sqrt{\\text{Var}(Y)}}$。代入 $Y$ 的均值和方差：\n$$\nZ = \\frac{Y - 0}{\\sqrt{2(1-\\rho)}} = \\frac{Y}{\\sqrt{2(1-\\rho)}}\n$$\n我们可以通过将概率表达式内部不等式的两边同时除以标准差 $\\sigma_{Y} = \\sqrt{2(1-\\rho)}$ 来变换该不等式。由于标准差是一个正数（我们假设对于一个非退化的正定协方差矩阵，有 $|\\rho|  1$），不等号的方向保持不变：\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}\\left(\\frac{Y}{\\sqrt{2(1-\\rho)}}  \\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)\n$$\n识别出左侧是标准化变量 $Z$，我们得到：\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}\\left(Z  \\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)\n$$\n根据定义，标准正态分布的CDF为 $\\Phi(x) = \\mathbb{P}(Z \\le x)$。因此，我们可以将该概率的最终表达式写为：\n$$\nP_{1 \\succ 2}(\\rho) = \\Phi\\left(\\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)\n$$\n这就是备选项1优于备选项2的概率的闭式表达式，它以它们的确定性效用 $\\mu_{1}$ 和 $\\mu_{2}$ 以及它们噪声项之间的相关性 $\\rho$ 为函数。使用“二元probit近似”的指令，为本次计算中完全忽略第三个备选项提供了依据。",
            "answer": "$$\n\\boxed{\\Phi\\left(\\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)}\n$$"
        },
        {
            "introduction": "在将潜在价值转化为选择概率时，计算神经科学家和经济学家提出了不同的模型。这个练习将对比两种极具影响力的模型：受神经环路启发的“除法归一化”（Divisive Normalization）模型和源于理性选择理论的Softmax模型。通过为一个假设的输入场景计算并拟合这两个模型 ，你将能够直接比较它们的功能形式，并深入理解不同的计算架构是如何实现选项间“竞争”这一核心概念的。",
            "id": "3999866",
            "problem": "考虑一个三择一决策的神经回路模型，其中竞争性抑制通过除法归一化（DN）实现。设每个选项 $i \\in \\{1,2,3\\}$ 都有一个输入驱动（增益）$g_i$，并且该回路通过一个归一化项来实例化对其他选项的侧向抑制池。在感觉和决策回路中，一个广泛接受的DN响应公式是，选项 $i$ 的稳态响应由 $r_i = \\frac{g_i}{\\sigma + \\sum_{j \\neq i} g_j}$ 给出，其中 $\\sigma  0$ 是一个加性半饱和常数，用于捕捉背景驱动和抑制池偏移。为了获得选择概率，假设选择选项 $i$ 的概率与 $r_i$ 成正比，并对所有选项进行归一化：$p_i^{\\mathrm{DN}} = \\frac{r_i}{\\sum_{k=1}^{3} r_k}$。与此同时，考虑一个基于效用的多项逻辑模型（MNL，也称为softmax）变换的概率决策模型，其中选择概率为 $p_i^{\\mathrm{SM}} = \\frac{\\exp(\\beta g_i)}{\\sum_{k=1}^{3} \\exp(\\beta g_k)}$，带有逆温度参数 $\\beta  0$。\n\n给定三个选项的增益 $g = (2,1,1)$ 和DN半饱和常数 $\\sigma = 1$，请从这些定义出发，比较DN和softmax的选择概率向量。通过将softmax的首选概率 $p_1^{\\mathrm{SM}}$ 与从该回路获得的DN首选概率 $p_1^{\\mathrm{DN}}$ 相等，来拟合softmax参数 $\\beta$。然后，分析拟合后两个完整分布 $\\{p_i^{\\mathrm{DN}}\\}$ 和 $\\{p_i^{\\mathrm{SM}}\\}$ 之间的任何残余差异，并根据底层模型结构和增益的对称性解释其存在与否。\n\n请以单个闭式解析表达式的形式给出拟合的逆温度参数 $\\beta$。不要对最终答案进行四舍五入。$\\beta$ 无需物理单位。",
            "solution": "起点是除法归一化（DN）响应的定义 $r_i = \\frac{g_i}{\\sigma + \\sum_{j \\neq i} g_j}$ 和通过归一化构建概率的方法 $p_i^{\\mathrm{DN}} = \\frac{r_i}{\\sum_{k=1}^{3} r_k}$。给定 $g = (2,1,1)$ 和 $\\sigma = 1$。\n\n计算每个选项的DN响应：\n- 对于 $i=1$，抑制池排除 $i=1$ 并对其他增益求和，因此 $\\sum_{j \\neq 1} g_j = g_2 + g_3 = 1 + 1 = 2$。所以，\n$$\nr_1 = \\frac{g_1}{\\sigma + \\sum_{j \\neq 1} g_j} = \\frac{2}{1 + 2} = \\frac{2}{3}.\n$$\n- 对于 $i=2$，抑制池排除 $i=2$ 并对 $g_1 + g_3 = 2 + 1 = 3$ 求和，所以\n$$\nr_2 = \\frac{g_2}{\\sigma + \\sum_{j \\neq 2} g_j} = \\frac{1}{1 + 3} = \\frac{1}{4}.\n$$\n- 对于 $i=3$，抑制池排除 $i=3$ 并对 $g_1 + g_2 = 2 + 1 = 3$ 求和，所以\n$$\nr_3 = \\frac{g_3}{\\sigma + \\sum_{j \\neq 3} g_j} = \\frac{1}{1 + 3} = \\frac{1}{4}.\n$$\n\n归一化响应以获得DN选择概率：\n$$\n\\sum_{k=1}^{3} r_k = \\frac{2}{3} + \\frac{1}{4} + \\frac{1}{4} = \\frac{2}{3} + \\frac{1}{2} = \\frac{4}{6} + \\frac{3}{6} = \\frac{7}{6}.\n$$\n因此，\n$$\np_1^{\\mathrm{DN}} = \\frac{r_1}{\\sum_k r_k} = \\frac{\\frac{2}{3}}{\\frac{7}{6}} = \\frac{2}{3} \\cdot \\frac{6}{7} = \\frac{4}{7},\n$$\n$$\np_2^{\\mathrm{DN}} = \\frac{r_2}{\\sum_k r_k} = \\frac{\\frac{1}{4}}{\\frac{7}{6}} = \\frac{1}{4} \\cdot \\frac{6}{7} = \\frac{3}{14},\n$$\n$$\np_3^{\\mathrm{DN}} = \\frac{r_3}{\\sum_k r_k} = \\frac{\\frac{1}{4}}{\\frac{7}{6}} = \\frac{3}{14}.\n$$\n由于选项2和3的增益 $(1,1)$ 具有对称性，我们有 $p_2^{\\mathrm{DN}} = p_3^{\\mathrm{DN}}$。\n\n接下来，考虑softmax（多项逻辑模型，MNL）概率 $p_i^{\\mathrm{SM}} = \\frac{\\exp(\\beta g_i)}{\\sum_{k=1}^{3} \\exp(\\beta g_k)}$ 并拟合 $\\beta$ 以使首选概率匹配：$p_1^{\\mathrm{SM}} = p_1^{\\mathrm{DN}} = \\frac{4}{7}$。\n\n令 $x = \\exp(\\beta)$，则 $\\exp(\\beta g_1) = \\exp(2\\beta) = x^{2}$ 且 $\\exp(\\beta g_2) = \\exp(\\beta g_3) = \\exp(\\beta) = x$。那么softmax的首选概率是\n$$\np_1^{\\mathrm{SM}} = \\frac{x^{2}}{x^{2} + x + x} = \\frac{x^{2}}{x^{2} + 2x}.\n$$\n我们将其与DN的首选概率相等：\n$$\n\\frac{x^{2}}{x^{2} + 2x} = \\frac{4}{7}.\n$$\n两边同乘以 $x^{2} + 2x$：\n$$\nx^{2} = \\frac{4}{7} \\left(x^{2} + 2x\\right).\n$$\n展开右侧：\n$$\nx^{2} = \\frac{4}{7} x^{2} + \\frac{8}{7} x.\n$$\n两边同减去 $\\frac{4}{7}x^{2}$：\n$$\nx^{2} - \\frac{4}{7} x^{2} = \\frac{8}{7} x,\n$$\n$$\n\\left(1 - \\frac{4}{7}\\right) x^{2} = \\frac{8}{7} x,\n$$\n$$\n\\frac{3}{7} x^{2} = \\frac{8}{7} x.\n$$\n两边同乘以 $\\frac{7}{x}$（假设 $x  0$，因为 $x = \\exp(\\beta)$，所以该假设成立）：\n$$\n3 x = 8,\n$$\n$$\nx = \\frac{8}{3}.\n$$\n因此，\n$$\n\\beta = \\ln(x) = \\ln\\!\\left(\\frac{8}{3}\\right).\n$$\n\n使用这个拟合的 $\\beta$，完整的softmax分布是\n$$\np_1^{\\mathrm{SM}} = \\frac{x^{2}}{x^{2} + 2x} = \\frac{\\left(\\frac{8}{3}\\right)^{2}}{\\left(\\frac{8}{3}\\right)^{2} + 2 \\cdot \\frac{8}{3}} = \\frac{\\frac{64}{9}}{\\frac{64}{9} + \\frac{16}{3}} = \\frac{\\frac{64}{9}}{\\frac{64}{9} + \\frac{48}{9}} = \\frac{\\frac{64}{9}}{\\frac{112}{9}} = \\frac{64}{112} = \\frac{4}{7},\n$$\n并且，根据 $g_2 = g_3$ 的对称性，\n$$\np_2^{\\mathrm{SM}} = p_3^{\\mathrm{SM}} = \\frac{x}{x^{2} + 2x} = \\frac{1}{x + 2} = \\frac{1}{\\frac{8}{3} + 2} = \\frac{1}{\\frac{8}{3} + \\frac{6}{3}} = \\frac{1}{\\frac{14}{3}} = \\frac{3}{14}.\n$$\n因此，在这种对称配置下，拟合的softmax精确地复现了整个DN概率向量：$\\left(p_1^{\\mathrm{SM}}, p_2^{\\mathrm{SM}}, p_3^{\\mathrm{SM}}\\right) = \\left(\\frac{4}{7}, \\frac{3}{14}, \\frac{3}{14}\\right) = \\left(p_1^{\\mathrm{DN}}, p_2^{\\mathrm{DN}}, p_3^{\\mathrm{DN}}\\right)$。一个自然的残余差异度量，例如Kullback-Leibler（KL）散度，将是\n$$\nD_{\\mathrm{KL}}\\!\\left(p^{\\mathrm{DN}} \\,\\|\\, p^{\\mathrm{SM}}\\right) = \\sum_{i=1}^{3} p_i^{\\mathrm{DN}} \\ln\\!\\left(\\frac{p_i^{\\mathrm{DN}}}{p_i^{\\mathrm{SM}}}\\right) = 0,\n$$\n因为每个比率 $\\frac{p_i^{\\mathrm{DN}}}{p_i^{\\mathrm{SM}}} = 1$。此处不存在残余差异是由于两个结构性特征所致：（i）所使用的竞争性DN公式在存在高增益竞争者时，对低增益选项产生更高的抑制，以及（ii）具有单个参数 $\\beta$ 的softmax必须为具有相等增益的选项分配相等的概率。当较低选项的增益对称时，匹配首选概率所确定的 $\\beta$ 值能确保所有概率都重合。在更一般、非对称的情况下（例如，$g = (2,1,0.8)$）或使用非均匀抑制权重 $\\sum_{j \\neq i} w_{ij} g_j$ 时，即使在拟合 $\\beta$ 以匹配 $p_1$ 之后，DN和softmax通常也会产生不同的分布，从而产生非零的残余差异，这反映了不同的归一化架构和非线性。",
            "answer": "$$\\boxed{\\ln\\!\\left(\\frac{8}{3}\\right)}$$"
        },
        {
            "introduction": "现实世界中的许多决策并非一次性的，而是在一段时间内动态展开的，这便引出了经典的“探索-利用”权衡问题。这个实践练习将带你进入序贯决策的世界，要求你为一个多臂老虎机问题实现一个动态规划算法，以计算出贝叶斯最优策略 。通过编写代码来解决这个问题，并将其最优解与一个基于启发式规则的次优策略进行比较，你将获得构建和评估学习与决策模型的宝贵实践经验。",
            "id": "3999877",
            "problem": "考虑一个由 $i \\in \\{1,2,3\\}$ 索引的、包含三个独立的二元结果选项（“臂”）的三元组。在任何离散决策时期，利用每个臂 $i$ 都会产生一个伯努利奖励，其成功概率 $\\theta_i \\in [0,1]$ 未知。贝叶斯决策者对每个臂 $i$ 都有一个共轭贝塔先验，参数为 $(\\alpha_{i0},\\beta_{i0})$，因此 $\\theta_i$ 的先验密度与 $\\theta_i^{\\alpha_{i0}-1}(1-\\theta_i)^{\\beta_{i0}-1}$ 成正比。时间被划分为 $T$ 个离散决策时期，由 $t \\in \\{0,1,\\dots,T-1\\}$ 索引。在每个时期 $t$，决策者可以对一个臂进行抽样以观察单个伯努利结果并更新相应的后验（此次观察无直接奖励），这会产生一个确定性的时间成本 $c  0$；或者停止抽样，并通过在所有剩余时期内选定一个臂进行利用，从而在每个剩余时期获得伯努利奖励。目标是最大化利用所带来的预期累积奖励减去抽样成本的总和。\n\n将此问题形式化为一个基于贝叶斯后验充分统计量和时间的有限期决策过程。设状态为 $s_t = \\big(t,(\\alpha_{i,t},\\beta_{i,t})_{i=1}^3\\big)$，其中 $(\\alpha_{i,t},\\beta_{i,t})$ 是截至时间 $t$ 的抽样历史所对应的后验贝塔参数。在时间 $t$ 时，臂 $i$ 的后验预测成功概率为 $\\mu_{i,t} = \\frac{\\alpha_{i,t}}{\\alpha_{i,t}+\\beta_{i,t}}$，$\\theta_i$ 的后验方差为 $v_{i,t} = \\frac{\\alpha_{i,t}\\beta_{i,t}}{(\\alpha_{i,t}+\\beta_{i,t})^2(\\alpha_{i,t}+\\beta_{i,t}+1)}$。如果决策者在时间 $t$ 停止并选定臂 $i^\\star \\in \\arg\\max_i \\mu_{i,t}$，则预期利用价值为 $(T-t)\\max_i \\mu_{i,t}$。如果决策者在时间 $t$ 对臂 $i$ 进行抽样，则下一状态有 $\\mu_{i,t}$ 的概率为 $s_{t+1}^+ = \\big(t+1,(\\alpha_{i,t}+1,\\beta_{i,t}),(\\alpha_{j,t},\\beta_{j,t})_{j\\neq i}\\big)$（观察到成功），或有 $1-\\mu_{i,t}$ 的概率为 $s_{t+1}^- = \\big(t+1,(\\alpha_{i,t},\\beta_{i,t}+1),(\\alpha_{j,t},\\beta_{j,t})_{j\\neq i}\\big)$（观察到失败），并且在时间 $t$ 抽样会产生成本 $c$。\n\n从有限期序贯决策的贝叶斯最优性原理和贝尔曼递归出发，推导并实现一个动态规划算法，该算法计算每个可达状态下的贝叶斯最优停止规则和预期价值函数 $V^\\star(s_t)$。推导过程必须从不确定性下的预期效用最大化、贝塔-伯努利共轭更新以及有限期决策过程的贝尔曼最优性原理的定义开始。\n\n此外，定义一个紧急度门控的softmax启发式策略：在状态 $s_t$ 下，为停止行动分配一个效用 $U_{\\text{stop}}(s_t) = (T-t)\\max_i \\mu_{i,t}$，为抽样臂 $i$ 分配一个效用 $U_{\\text{sample},i}(s_t) = \\alpha_{\\text{vi}}\\sqrt{v_{i,t}}\\cdot \\max(T-t-1,0) - c$，其中 $\\alpha_{\\text{vi}}  0$ 是一个标量“信息价值”权重。设softmax温度为 $\\tau(t) = \\frac{\\tau_0}{1+\\gamma t}$，其中 $\\tau_0  0$ 且 $\\gamma \\ge 0$。该启发式策略的行动概率为\n$$\n\\pi(\\text{stop}\\mid s_t) = \\frac{\\exp\\left(\\frac{U_{\\text{stop}}(s_t)}{\\tau(t)}\\right)}{\\exp\\left(\\frac{U_{\\text{stop}}(s_t)}{\\tau(t)}\\right) + \\sum_{i=1}^3 \\exp\\left(\\frac{U_{\\text{sample},i}(s_t)}{\\tau(t)}\\right)},\n\\quad\n\\pi(\\text{sample},i\\mid s_t) = \\frac{\\exp\\left(\\frac{U_{\\text{sample},i}(s_t)}{\\tau(t)}\\right)}{\\exp\\left(\\frac{U_{\\text{stop}}(s_t)}{\\tau(t)}\\right) + \\sum_{j=1}^3 \\exp\\left(\\frac{U_{\\text{sample},j}(s_t)}{\\tau(t)}\\right)}.\n$$\n通过有限期贝尔曼期望递归，并对后验预测结果进行精确积分，计算由该启发式策略引出的预期价值函数 $V_\\pi(s_t)$。\n\n对于下述每个测试用例，你的程序必须计算：(1) 一个布尔值，指示贝叶斯最优策略是否在 $t=0$ 时立即停止（若在 $t=0$ 时选择停止，则为真，否则为假），(2) 贝叶斯最优策略下的预期价值 $V^\\star(s_0)$，(3) 在指定参数下，紧急度门控的softmax启发式策略的预期价值 $V_\\pi(s_0)$，以及 (4) 差值 $V^\\star(s_0) - V_\\pi(s_0)$。所有预期值必须是相对于后验预测分布的精确值，而不是蒙特卡洛近似值。不涉及物理单位，答案为实值浮点数或布尔值。\n\n使用以下参数集测试套件（每个案例指定 $T$、$c$ 和 $i\\in\\{1,2,3\\}$ 的先验 $(\\alpha_{i0},\\beta_{i0})$，以及启发式参数 $\\tau_0$、$\\gamma$、$\\alpha_{\\text{vi}}$）：\n\n- 案例1（“顺利路径”）：$T=4$, $c=0.1$, 先验 $(\\alpha_{10},\\beta_{10})=(1,1)$, $(\\alpha_{20},\\beta_{20})=(1,1)$, $(\\alpha_{30},\\beta_{30})=(1,1)$, 启发式参数 $\\tau_0=0.5$, $\\gamma=1.0$, $\\alpha_{\\text{vi}}=1.0$。\n- 案例2（边界，最小期限）：$T=1$, $c=0.5$, 先验 $(\\alpha_{10},\\beta_{10})=(1,1)$, $(\\alpha_{20},\\beta_{20})=(1,1)$, $(\\alpha_{30},\\beta_{30})=(1,1)$, 启发式参数 $\\tau_0=0.5$, $\\gamma=1.0$, $\\alpha_{\\text{vi}}=1.0$。\n- 案例3（单臂信息先验，低抽样成本）：$T=5$, $c=0.05$, 先验 $(\\alpha_{10},\\beta_{10})=(8,2)$, $(\\alpha_{20},\\beta_{20})=(2,8)$, $(\\alpha_{30},\\beta_{30})=(1,1)$, 启发式参数 $\\tau_0=0.4$, $\\gamma=1.0$, $\\alpha_{\\text{vi}}=1.0$。\n- 案例4（高抽样成本）：$T=5$, $c=0.6$, 先验 $(\\alpha_{10},\\beta_{10})=(1,1)$, $(\\alpha_{20},\\beta_{20})=(1,1)$, $(\\alpha_{30},\\beta_{30})=(1,1)$, 启发式参数 $\\tau_0=0.4$, $\\gamma=1.0$, $\\alpha_{\\text{vi}}=1.0$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个测试用例的结果本身是按上述顺序排列的四个量组成的列表。例如，输出格式必须类似于 `[[$布尔值$, $浮点数$, $浮点数$, $浮点数$],[$布尔值$, $浮点数$, $浮点数$, $浮点数$],\\dots]`。",
            "solution": "该问题描述了一个有限期贝叶斯最优停止问题，这是一种特定类型的马尔可夫决策过程（MDP）。目标是设计一个策略，以最大化预期净回报，该净回报定义为利用所获得的累积奖励减去抽样的累积成本。我们将根据贝尔曼最优性原理的规定，通过反向归纳法使用动态规划来解决此问题。\n\n### 1. 形式化问题规范\n\n我们首先将决策过程的组成部分形式化。\n- **状态空间**：在决策时期 $t \\in \\{0, 1, \\dots, T-1\\}$ 的状态由 $s_t = \\big(t, (\\alpha_{1,t}, \\beta_{1,t}), (\\alpha_{2,t}, \\beta_{2,t}), (\\alpha_{3,t}, \\beta_{3,t})\\big)$ 给出。这里，$t$ 是当前时间，$(\\alpha_{i,t}, \\beta_{i,t})$ 是臂 $i$ 的未知成功概率 $\\theta_i$ 的贝塔后验分布的参数。初始状态是 $s_0 = \\big(0, (\\alpha_{10}, \\beta_{10}), (\\alpha_{20}, \\beta_{20}), (\\alpha_{30}, \\beta_{30})\\big)$。\n\n- **行动空间**：在任何状态 $s_t$ 且 $t  T$ 时，智能体可以从集合 $A = \\{\\text{stop}\\} \\cup \\{\\text{sample } i\\}_{i=1}^3$ 中选择一个行动 $a_t$。\n\n- **转移和奖励**：\n    - 如果 $a_t = \\text{stop}$，过程终止。智能体收到一个终端奖励，其值等于在剩余的 $T-t$ 个时期内利用看起来最优的臂所获得的期望奖励总和。臂 $i$ 的后验预测平均成功概率为 $\\mu_{i,t} = E[\\theta_i | s_t] = \\frac{\\alpha_{i,t}}{\\alpha_{i,t}+\\beta_{i,t}}$。最优的利用臂为 $i^\\star = \\arg\\max_i \\mu_{i,t}$。因此，终端奖励为 $(T-t) \\max_i \\mu_{i,t}$。\n    - 如果 $a_t = \\text{sample } i$，智能体支付成本 $c  0$。对臂 $i$ 进行一次伯努利试验。以概率 $\\mu_{i,t}$（后验预测成功概率）得到成功结果，状态转移到 $s_{t+1}^+ = \\big(t+1, \\dots, (\\alpha_{i,t}+1, \\beta_{i,t}), \\dots\\big)$。以概率 $1-\\mu_{i,t}$ 得到失败结果，状态转移到 $s_{t+1}^- = \\big(t+1, \\dots, (\\alpha_{i,t}, \\beta_{i,t}+1), \\dots\\big)$。臂 $j \\neq i$ 的参数保持不变。\n\n### 2. 贝叶斯最优策略和价值函数 $V^\\star(s_t)$\n\n目标是找到一个最优策略 $\\pi^\\star$，以最大化预期总净回报。这个最优策略的价值 $V^\\star(s_t) = \\max_{\\pi} E_\\pi \\left[ \\sum_{k=t}^{T-1} R(s_k, a_k) | s_t \\right]$，可以使用贝尔曼最优性原理找到。对于有限期问题，这意味着一个反向归纳过程。\n\n在终端时间 $t=T$ 时的价值函数为 $V^\\star(s_T) = 0$，因为没有更多的行动或奖励是可能的。\n\n对于任何时间 $t \\in \\{T-1, T-2, \\dots, 0\\}$，最优价值函数 $V^\\star(s_t)$ 是在状态 $s_t$ 下所有可能行动的价值的最大值。我们将行动价值函数 $Q^\\star(s_t, a_t)$ 定义为在状态 $s_t$ 采取行动 $a_t$ 并在此后遵循最优策略的预期回报。\n\n- **停止的价值**：如果行动是停止，则过程结束，价值是即时的终端奖励。\n$$Q^\\star(s_t, \\text{stop}) = (T-t) \\max_{i \\in \\{1,2,3\\}} \\mu_{i,t}$$\n\n- **抽样的价值**：如果行动是抽样臂 $i$，智能体产生一个成本 $c$，然后转移到一个新状态 $s_{t+1}$，从该状态它将获得最优价值 $V^\\star(s_{t+1})$。其价值是成本加上下一个状态的预期最优价值。\n$$Q^\\star(s_t, \\text{sample } i) = -c + E[V^\\star(s_{t+1}) | s_t, a_t=\\text{sample } i]$$\n期望是针对样本的两种可能结果（成功或失败）计算的：\n$$Q^\\star(s_t, \\text{sample } i) = -c + \\mu_{i,t} V^\\star(s_{t+1}^+) + (1-\\mu_{i,t}) V^\\star(s_{t+1}^-)$$\n其中 $s_{t+1}^+$ 和 $s_{t+1}^-$ 分别是在臂 $i$ 上观察到成功或失败后的后继状态。\n\n那么贝尔曼最优性方程为：\n$$V^\\star(s_t) = \\max \\left( Q^\\star(s_t, \\text{stop}), \\max_{i \\in \\{1,2,3\\}} Q^\\star(s_t, \\text{sample } i) \\right)$$\n最优行动 $a^\\star(s_t)$ 是实现这个最大值的行动。这个递归可以通过从 $t=T-1$ 开始并向后推导至 $t=0$ 来解决。\n\n### 3. 启发式策略和价值函数 $V_\\pi(s_t)$\n\n该问题还定义了一个基于行动效用的softmax函数的启发式策略 $\\pi$。该策略的价值函数 $V_\\pi(s_t)$ 可以使用贝尔曼期望方程计算，它与最优性方程类似，但用对策略行动概率的期望取代了 $\\max$ 算子。\n\n效用定义如下：\n- $U_{\\text{stop}}(s_t) = (T-t)\\max_i \\mu_{i,t}$\n- $U_{\\text{sample},i}(s_t) = \\alpha_{\\text{vi}}\\sqrt{v_{i,t}}\\cdot \\max(T-t-1,0) - c$\n\n其中 $v_{i,t} = \\frac{\\alpha_{i,t}\\beta_{i,t}}{(\\alpha_{i,t}+\\beta_{i,t})^2(\\alpha_{i,t}+\\beta_{i,t}+1)}$ 是 $\\theta_i$ 的后验方差。温度是 $\\tau(t) = \\frac{\\tau_0}{1+\\gamma t}$。行动概率为：\n$$\n\\pi(\\text{action}|s_t) = \\frac{\\exp\\left( \\frac{U_{\\text{action}}(s_t)}{\\tau(t)} \\right)}{\\sum_{\\text{action}'} \\exp\\left( \\frac{U_{\\text{action}'}(s_t)}{\\tau(t)} \\right)}\n$$\n$V_\\pi$ 的贝尔曼期望方程为：\n$$V_\\pi(s_t) = \\sum_{a_t \\in A} \\pi(a_t|s_t) Q_\\pi(s_t, a_t)$$\n其中 $Q_\\pi(s_t, a_t)$ 是采取行动 $a_t$ 然后遵循策略 $\\pi$ 的价值。\n\n- 如果 $a_t = \\text{stop}$：$Q_\\pi(s_t, \\text{stop}) = (T-t) \\max_i \\mu_{i,t}$。\n- 如果 $a_t = \\text{sample } i$：$Q_\\pi(s_t, \\text{sample } i) = -c + E[V_\\pi(s_{t+1}) | s_t, a_t=\\text{sample } i]$。这个期望如前所述展开：\n$$Q_\\pi(s_t, \\text{sample } i) = -c + \\mu_{i,t}V_\\pi(s_{t+1}^+) + (1-\\mu_{i,t})V_\\pi(s_{t+1}^-)$$\n\n将这些代入贝尔曼方程，得到 $V_\\pi$ 的递归式：\n$$\nV_\\pi(s_t) = \\pi(\\text{stop}|s_t) \\left( (T-t)\\max_i \\mu_{i,t} \\right) + \\sum_{i=1}^3 \\pi(\\text{sample},i|s_t) \\left( -c + \\mu_{i,t}V_\\pi(s_{t+1}^+) + (1-\\mu_{i,t})V_\\pi(s_{t+1}^-) \\right)\n$$\n与 $V^\\star$ 一样，这也是从边界条件 $V_\\pi(s_T)=0$ 开始通过反向归纳法解决的。\n\n### 4. 算法实现\n\n我们实现了一个动态规划算法来解决这些递归问题。我们使用带有备忘录（缓存）的自顶向下递归方法，以避免对相同状态重复计算值。使用一个字典作为缓存，其键是状态元组 $(t, \\text{priors})$。\n\n实现了两个递归函数，`compute_V_star` 和 `compute_V_pi`。每个函数都以一个状态 $(t, \\text{priors})$ 作为输入。\n1. 它首先检查备忘录缓存中是否存在该状态的预计算值。\n2. 如果 $t=T$，则返回 $0$。\n3. 否则，它通过对时间 $t+1$ 的状态进行递归调用来计算停止和抽样的价值。\n4. 然后，它根据 a) $V^\\star$ 的贝尔曼最优性方程 或 b) $V_\\pi$ 的贝尔曼期望方程 来组合这些价值。\n5. 结果被存储在缓存中并返回。\n\n为了确定初始状态 $s_0$ 的最优行动，我们明确计算所有 $i$ 的 $Q^\\star(s_0, \\text{stop})$ 和 $Q^\\star(s_0, \\text{sample } i)$。这需要所有可能的后继状态的 $V^\\star(s_1)$ 值，这些值从递归求解器中获得。然后通过比较这些Q值找到最优行动。如果停止的价值大于或等于抽样的最大价值，则最优策略选择停止。最终的程序遍历所提供的测试用例，为每个用例设置参数，并调用求解器来计算所需的量。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n\n    class Solver:\n        \"\"\"\n        Solves the Bayesian optimal stopping problem for a given set of parameters.\n        \"\"\"\n\n        def __init__(self, T, c, priors_0, tau_0, gamma, alpha_vi):\n            self.T = T\n            self.c = c\n            self.priors_0 = tuple(tuple(p) for p in priors_0)\n            self.tau_0 = tau_0\n            self.gamma = gamma\n            self.alpha_vi = alpha_vi\n            \n            # Memoization caches for the value functions\n            self.memo_v_star = {}\n            self.memo_v_pi = {}\n\n        def run(self):\n            \"\"\"\n            Computes the four required quantities for the initial state s_0.\n            \"\"\"\n            # 1. Optimal Policy: V_star and immediate action\n            alphas_0 = np.array([p[0] for p in self.priors_0], dtype=float)\n            betas_0 = np.array([p[1] for p in self.priors_0], dtype=float)\n            mus_0 = alphas_0 / (alphas_0 + betas_0)\n\n            # Value of stopping at t=0\n            v_stop_s0 = self.T * np.max(mus_0)\n            \n            # Value of sampling each arm at t=0\n            v_samples_s0 = []\n            for i in range(3):\n                # Success state\n                priors_succ_list = list(self.priors_0)\n                priors_succ_list[i] = (self.priors_0[i][0] + 1, self.priors_0[i][1])\n                v_succ = self._compute_v_star(1, tuple(priors_succ_list))\n\n                # Failure state\n                priors_fail_list = list(self.priors_0)\n                priors_fail_list[i] = (self.priors_0[i][0], self.priors_0[i][1] + 1)\n                v_fail = self._compute_v_star(1, tuple(priors_fail_list))\n                \n                v_sample_i = -self.c + mus_0[i] * v_succ + (1 - mus_0[i]) * v_fail\n                v_samples_s0.append(v_sample_i)\n\n            v_star_s0 = max(v_stop_s0, max(v_samples_s0))\n            is_stop_optimal = (v_stop_s0 = max(v_samples_s0))\n\n            # 2. Heuristic Policy: V_pi\n            v_pi_s0 = self._compute_v_pi(0, self.priors_0)\n\n            # 3. Difference\n            diff = v_star_s0 - v_pi_s0\n            \n            return [is_stop_optimal, v_star_s0, v_pi_s0, diff]\n\n        def _compute_v_star(self, t, priors):\n            \"\"\"\n            Computes the optimal value function V* using backward recursion.\n            \"\"\"\n            state = (t, priors)\n            if state in self.memo_v_star:\n                return self.memo_v_star[state]\n            \n            if t == self.T:\n                return 0.0\n\n            alphas = np.array([p[0] for p in priors], dtype=float)\n            betas = np.array([p[1] for p in priors], dtype=float)\n            mus = alphas / (alphas + betas)\n            \n            # Value of stopping\n            v_stop = (self.T - t) * np.max(mus)\n            \n            # Value of sampling\n            v_samples = []\n            for i in range(3):\n                priors_succ_list = list(priors)\n                priors_succ_list[i] = (alphas[i] + 1, betas[i])\n                v_succ = self._compute_v_star(t + 1, tuple(priors_succ_list))\n\n                priors_fail_list = list(priors)\n                priors_fail_list[i] = (alphas[i], betas[i] + 1)\n                v_fail = self._compute_v_star(t + 1, tuple(priors_fail_list))\n                \n                v_sample_i = -self.c + mus[i] * v_succ + (1 - mus[i]) * v_fail\n                v_samples.append(v_sample_i)\n\n            v_star = max([v_stop] + v_samples)\n            self.memo_v_star[state] = v_star\n            return v_star\n\n        def _compute_v_pi(self, t, priors):\n            \"\"\"\n            Computes the heuristic policy's value function V_pi using backward recursion.\n            \"\"\"\n            state = (t, priors)\n            if state in self.memo_v_pi:\n                return self.memo_v_pi[state]\n            \n            if t == self.T:\n                return 0.0\n\n            alphas = np.array([p[0] for p in priors], dtype=float)\n            betas = np.array([p[1] for p in priors], dtype=float)\n            mus = alphas / (alphas + betas)\n            denominators = alphas + betas\n            variances = (alphas * betas) / (denominators**2 * (denominators + 1))\n            \n            # Heuristic utilities\n            u_stop = (self.T - t) * np.max(mus)\n            rem_exploitation_steps = max(self.T - t - 1, 0)\n            u_samples = self.alpha_vi * np.sqrt(variances) * rem_exploitation_steps - self.c\n\n            # Softmax probabilities\n            temp = self.tau_0 / (1 + self.gamma * t)\n            \n            utils = np.concatenate(([u_stop], u_samples))\n            \n            if temp == 0: # Zero-temperature limit (greedy)\n                pi_actions = np.zeros_like(utils)\n                winner_indices = np.where(utils == np.max(utils))[0]\n                pi_actions[winner_indices] = 1.0 / len(winner_indices)\n            else: # Standard softmax with stability trick\n                utils_shifted = utils - np.max(utils)\n                exp_utils = np.exp(utils_shifted / temp)\n                pi_actions = exp_utils / np.sum(exp_utils)\n            \n            pi_stop = pi_actions[0]\n            pi_samples = pi_actions[1:]\n            \n            # Bellman Expectation\n            # Contribution from stopping\n            v_pi = pi_stop * u_stop\n            \n            # Contribution from sampling\n            for i in range(3):\n                if pi_samples[i]  0:\n                    priors_succ_list = list(priors)\n                    priors_succ_list[i] = (alphas[i] + 1, betas[i])\n                    v_succ = self._compute_v_pi(t + 1, tuple(priors_succ_list))\n\n                    priors_fail_list = list(priors)\n                    priors_fail_list[i] = (alphas[i], betas[i] + 1)\n                    v_fail = self._compute_v_pi(t + 1, tuple(priors_fail_list))\n                    \n                    v_sample_i_future = -self.c + mus[i] * v_succ + (1 - mus[i]) * v_fail\n                    v_pi += pi_samples[i] * v_sample_i_future\n\n            self.memo_v_pi[state] = v_pi\n            return v_pi\n\n    test_cases = [\n        {'T': 4, 'c': 0.1, 'priors_0': [[1,1],[1,1],[1,1]], 'tau_0': 0.5, 'gamma': 1.0, 'alpha_vi': 1.0},\n        {'T': 1, 'c': 0.5, 'priors_0': [[1,1],[1,1],[1,1]], 'tau_0': 0.5, 'gamma': 1.0, 'alpha_vi': 1.0},\n        {'T': 5, 'c': 0.05, 'priors_0': [[8,2],[2,8],[1,1]], 'tau_0': 0.4, 'gamma': 1.0, 'alpha_vi': 1.0},\n        {'T': 5, 'c': 0.6, 'priors_0': [[1,1],[1,1],[1,1]], 'tau_0': 0.4, 'gamma': 1.0, 'alpha_vi': 1.0},\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        solver_instance = Solver(**case_params)\n        result = solver_instance.run()\n        all_results.append(result)\n    \n    # Format the final output string\n    result_strings = [str(res) for res in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}