{
    "hands_on_practices": [
        {
            "introduction": "A critical skill in applying linear mixed-effects models is translating between the high-level formula syntax used in statistical software and the underlying matrix algebra of the model. This exercise  provides hands-on practice with this translation by asking you to construct the random-effects design matrix, $\\mathbf{Z}$, for two distinct model specifications. Understanding how this matrix is formed is fundamental to correctly specifying your model's random-effects structure and interpreting its results.",
            "id": "4175329",
            "problem": "A cortical excitability experiment measures motor-evoked potential amplitude across two sessions per participant: a baseline session at time $t=0$ and a post-intervention session at time $t=1$. There are $3$ participants (grouping factor levels), labeled $S1$, $S2$, and $S3$. Each participant is measured at both sessions, yielding $6$ observations. The rows are ordered as $(S1,t=0)$, $(S1,t=1)$, $(S2,t=0)$, $(S2,t=1)$, $(S3,t=0)$, $(S3,t=1)$. Assume $t$ is a numeric covariate taking values $0$ and $1$ as given.\n\nYou plan to fit linear mixed-effects models using the standard linear mixed model formulation\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b} + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\mathbf{y}$ is the $6\\times 1$ response vector, $\\mathbf{X}$ is the fixed-effects design matrix, $\\boldsymbol{\\beta}$ is the fixed-effects coefficient vector, $\\mathbf{Z}$ is the random-effects design matrix, $\\mathbf{b}$ is the random-effects coefficient vector with $\\mathbf{b}\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{G})$, and $\\boldsymbol{\\varepsilon}\\sim \\mathcal{N}(\\mathbf{0},\\sigma^2\\mathbf{I})$ independent of $\\mathbf{b}$. The random-effects design matrix $\\mathbf{Z}$ maps each observation to the appropriate subject-specific random coefficients by combining group membership indicators with the random-effects covariates.\n\nConsider the two random-effects specifications in formula syntax: $(1\\mid \\text{subject})$ and $(0 + t \\mid \\text{subject})$, where the grouping factor is the subject identifier and $t$ is the numeric time covariate defined above. Which option correctly explains the difference between these two specifications and derives the corresponding random-effects design matrices $\\mathbf{Z}$ for this $6$-row dataset?\n\nA. $(1\\mid \\text{subject})$ specifies a subject-specific random intercept only; $(0 + t \\mid \\text{subject})$ specifies a subject-specific random slope for $t$ only (no random intercept). Thus $\\mathbf{Z}$ has $3$ columns in both cases, one per subject. For $(1\\mid \\text{subject})$,\n$$\n\\mathbf{Z}_{(1\\mid \\text{subject})}=\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\\\\\n0 & 0 & 1\n\\end{bmatrix},\n$$\nand for $(0 + t \\mid \\text{subject})$,\n$$\n\\mathbf{Z}_{(0+t\\mid \\text{subject})}=\n\\begin{bmatrix}\n0 & 0 & 0\\\\\n1 & 0 & 0\\\\\n0 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix},\n$$\nbecause each column corresponds to a subject and carries that subject’s value of $t$ for each row, with zeros elsewhere.\n\nB. $(1\\mid \\text{subject})$ and $(0 + t \\mid \\text{subject})$ both include a subject-specific random intercept, but $(0 + t \\mid \\text{subject})$ additionally includes a subject-specific random slope. Therefore $\\mathbf{Z}$ for $(1\\mid \\text{subject})$ is $6\\times 3$ with subject indicators, while for $(0 + t \\mid \\text{subject})$ it is $6\\times 6$ with two columns per subject:\n$$\n\\mathbf{Z}_{(1\\mid \\text{subject})}=\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\\\\\n0 & 0 & 1\n\\end{bmatrix},\\quad\n\\mathbf{Z}_{(0+t\\mid \\text{subject})}=\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 1 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 1\n\\end{bmatrix},\n$$\nwhere the first three columns are subject intercepts and the last three are subject slopes.\n\nC. $(1\\mid \\text{subject})$ specifies a subject-specific random slope for $t$; $(0 + t \\mid \\text{subject})$ specifies a single global random slope for $t$ shared by all subjects. Hence $\\mathbf{Z}$ for $(1\\mid \\text{subject})$ equals the $6\\times 3$ matrix with entries equal to the observed $t$ in the subject’s column, while $\\mathbf{Z}$ for $(0 + t \\mid \\text{subject})$ is the $6\\times 1$ vector\n$$\n\\mathbf{Z}_{(0+t\\mid \\text{subject})}=\n\\begin{bmatrix}\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\n\\end{bmatrix}.\n$$\n\nD. $(1\\mid \\text{subject})$ specifies a random intercept per subject that must be orthonormalized across groups for identifiability, so its $\\mathbf{Z}$ uses subject indicators scaled by $1/\\sqrt{2}$ for each subject with $2$ observations. $(0 + t \\mid \\text{subject})$ specifies a random slope for $t$ per subject with $t$ mean-centered within subject, so its $\\mathbf{Z}$ uses within-subject centered $t$ values $\\{-\\tfrac{1}{2},+\\tfrac{1}{2}\\}$ in the appropriate subject column. Concretely,\n$$\n\\mathbf{Z}_{(1\\mid \\text{subject})}=\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & 0 & 0\\\\\n\\frac{1}{\\sqrt{2}} & 0 & 0\\\\\n0 & \\frac{1}{\\sqrt{2}} & 0\\\\\n0 & \\frac{1}{\\sqrt{2}} & 0\\\\\n0 & 0 & \\frac{1}{\\sqrt{2}}\\\\\n0 & 0 & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix},\\quad\n\\mathbf{Z}_{(0+t\\mid \\text{subject})}=\n\\begin{bmatrix}\n-\\tfrac{1}{2} & 0 & 0\\\\\n+\\tfrac{1}{2} & 0 & 0\\\\\n0 & -\\tfrac{1}{2} & 0\\\\\n0 & +\\tfrac{1}{2} & 0\\\\\n0 & 0 & -\\tfrac{1}{2}\\\\\n0 & 0 & +\\tfrac{1}{2}\n\\end{bmatrix}.\n$$\n\nSelect the single best option.",
            "solution": "The problem statement is well-defined and valid. It presents a standard scenario in hierarchical data analysis, utilizing established terminology and notation for linear mixed-effects models (LMMs). The task is to construct the random-effects design matrix, $\\mathbf{Z}$, for two different random-effects specifications, which is a mathematically precise and solvable problem.\n\nThe general LMM is given by $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b} + \\boldsymbol{\\varepsilon}$. The random-effects design matrix $\\mathbf{Z}$ connects the observations $\\mathbf{y}$ to the subject-specific random effects $\\mathbf{b}$. The structure of $\\mathbf{Z}$ is determined by the formula `(effects|group)`. In this problem, the grouping factor is `subject`, with $3$ levels ($S1$, $S2$, $S3$). The total number of observations is $N=6$. The data are ordered such that the first two rows correspond to subject $S1$ (at times $t=0$ and $t=1$), the next two to subject $S2$ ($t=0, 1$), and the final two to subject $S3$ ($t=0, 1$). The numeric time covariate vector is thus $\\mathbf{t} = [0, 1, 0, 1, 0, 1]^T$.\n\n### Analysis of the Random-Effects Specifications\n\nThe construction of the matrix $\\mathbf{Z}$ follows from the specified random effects and grouping structure. The matrix $\\mathbf{Z}$ has $N=6$ rows. The number of columns equals the total number of random effects. For a given random effect (e.g., the random slope for subject $j$), the corresponding column in $\\mathbf{Z}$ contains the value of the associated covariate for all observations belonging to subject $j$, and $0$ for all other observations.\n\n**Case 1: Random-effects specification $(1 \\mid \\text{subject})$**\n\nThis formula specifies a random intercept for each level of the grouping factor `subject`. The `1` represents the intercept.\n-   There are $3$ subjects, so there are $3$ random intercepts, one for each subject. Let these be $b_{0,1}$, $b_{0,2}$, and $b_{0,3}$ for subjects $S1$, $S2$, and $S3$ respectively. The random effects vector is $\\mathbf{b} = [b_{0,1}, b_{0,2}, b_{0,3}]^T$.\n-   The matrix $\\mathbf{Z}$ will have $6$ rows and $3$ columns.\n-   The first column corresponds to the random intercept for $S1$, $b_{0,1}$. The covariate for an intercept is always $1$. So, this column will have a $1$ for rows corresponding to $S1$ (rows $1$ and $2$) and $0$ otherwise.\n-   The second column corresponds to the random intercept for $S2$, $b_{0,2}$. It will have a $1$ for rows corresponding to $S2$ (rows $3$ and $4$) and $0$ otherwise.\n-   The third column corresponds to the random intercept for $S3$, $b_{0,3}$. It will have a $1$ for rows corresponding to $S3$ (rows $5$ and $6$) and $0$ otherwise.\n\nFollowing this construction, the matrix $\\mathbf{Z}$ for the specification $(1 \\mid \\text{subject})$ is:\n$$\n\\mathbf{Z}_{(1\\mid \\text{subject})} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\n\n**Case 2: Random-effects specification $(0 + t \\mid \\text{subject})$**\n\nThis formula specifies a random slope for the covariate $t$ for each level of the grouping factor `subject`. The `0 +` term explicitly suppresses the default random intercept.\n-   There are $3$ subjects, so there are $3$ random slopes, one for each subject. Let these be $b_{t,1}$, $b_{t,2}$, and $b_{t,3}$ for subjects $S1$, $S2$, and $S3$ respectively. The random effects vector is $\\mathbf{b} = [b_{t,1}, b_{t,2}, b_{t,3}]^T$.\n-   The matrix $\\mathbf{Z}$ will have $6$ rows and $3$ columns.\n-   The first column corresponds to the random slope for $S1$, $b_{t,1}$. The associated covariate is $t$. This column will contain the values of $t$ for the rows corresponding to $S1$ (rows $1$ and $2$, where $t$ is $0$ and $1$) and $0$ otherwise.\n-   The second column corresponds to the random slope for $S2$, $b_{t,2}$. It will contain the values of $t$ for the rows corresponding to $S2$ (rows $3$ and $4$, where $t$ is $0$ and $1$) and $0$ otherwise.\n-   The third column corresponds to the random slope for $S3$, $b_{t,3}$. It will contain the values of $t$ for the rows corresponding to $S3$ (rows $5$ and $6$, where $t$ is $0$ and $1$) and $0$ otherwise.\n\nFollowing this construction, the matrix $\\mathbf{Z}$ for the specification $(0 + t \\mid \\text{subject})$ is:\n$$\n\\mathbf{Z}_{(0+t\\mid \\text{subject})} =\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\n\n### Evaluation of Options\n\n**Option A:**\nThis option states that $(1\\mid \\text{subject})$ specifies a subject-specific random intercept only, and $(0 + t \\mid \\text{subject})$ specifies a subject-specific random slope for $t$ only. This interpretation is correct. It provides the following matrices:\n$$\n\\mathbf{Z}_{(1\\mid \\text{subject})}=\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\\\\\n0 & 0 & 1\n\\end{bmatrix},\n\\quad\n\\mathbf{Z}_{(0+t\\mid \\text{subject})}=\n\\begin{bmatrix}\n0 & 0 & 0\\\\\n1 & 0 & 0\\\\\n0 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\nThese matrices perfectly match the ones derived above based on the standard interpretation of the LMM formula syntax.\nVerdict: **Correct**.\n\n**Option B:**\nThis option incorrectly states that both specifications include a subject-specific random intercept. The formula $(0 + t \\mid \\text{subject})$ explicitly excludes the random intercept. The matrix it presents for this second specification corresponds to a model with both random intercepts and random slopes, which would be specified as $(1 + t \\mid \\text{subject})$ or $(t \\mid \\text{subject})$. The interpretation of the model formula is fundamentally flawed.\nVerdict: **Incorrect**.\n\n**Option C:**\nThis option incorrectly interprets both formulas. It claims $(1\\mid \\text{subject})$ specifies a random slope, which is wrong; the `1` specifies an intercept. It also claims $(0 + t \\mid \\text{subject})$ specifies a single *global* random slope. This is also wrong; the grouping syntax `| subject` makes the effect specific to each subject, not global. Furthermore, the proposed $\\mathbf{Z}$ matrix for the second model is a single $6\\times 1$ vector, which implies a single random effect common to all subjects modulated by $t$, rather than subject-specific random slopes.\nVerdict: **Incorrect**.\n\n**Option D:**\nThis option introduces transformations that are not implied by the LMM specification itself. It claims that for $(1\\mid \\text{subject})$, the columns of $\\mathbf{Z}$ must be orthonormalized, leading to a scaling by $1/\\sqrt{2}$. This is not a standard requirement for the construction of $\\mathbf{Z}$, which typically consists of indicator variables ($0$s and $1$s). It also claims that for $(0 + t \\mid \\text{subject})$, the covariate $t$ must be mean-centered within each subject. While mean-centering can be a useful preprocessing step for numerical stability or interpretation, it is not inherent to the formula `(0 + t | subject)`. The matrix $\\mathbf{Z}$ is constructed from the covariates as provided, which are $0$ and $1$. The proposed matrices are therefore not the standard representation for the given problem.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once a linear mixed-effects model is fitted, the estimated variance components offer deep insights into the sources of variability in your data. This exercise  moves from model specification to interpretation, challenging you to perform a variance decomposition. By calculating the proportion of total variance attributable to between-subject differences in a hypothetical fMRI study, you will practice connecting the abstract random-effects covariance matrix, $G$, to a tangible and scientifically meaningful conclusion about your data's hierarchical structure.",
            "id": "4175390",
            "problem": "A cognitive neuroscience experiment measures Blood Oxygenation Level Dependent (BOLD) amplitude in functional Magnetic Resonance Imaging (fMRI) across multiple trials per subject under two conditions: rest and task. Let the trial-level BOLD amplitude for subject $i$ on trial $j$ be modeled by a linear mixed-effects model with a subject-specific random intercept and a subject-specific random slope for the binary task indicator. Specifically, let $y_{ij}$ denote BOLD amplitude and $c_{ij} \\in \\{0,1\\}$ denote the condition ($c_{ij}=0$ for rest, $c_{ij}=1$ for task). Consider the hierarchical model\n$$\ny_{ij} = \\beta_{0} + \\beta_{1} c_{ij} + b_{0i} + b_{1i} c_{ij} + \\varepsilon_{ij},\n$$\nwith subject-level random effects $\\begin{pmatrix} b_{0i} \\\\ b_{1i} \\end{pmatrix} \\sim \\mathcal{N}\\!\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\\, G\\right)$ and observation-level errors $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^{2})$, independent of the random effects. Suppose the estimated covariance matrices are\n$$\nG = \\begin{pmatrix} 0.8 & 0.2 \\\\ 0.2 & 0.5 \\end{pmatrix}, \\quad R = \\sigma^{2} I \\text{ with } \\sigma^{2} = 1.2,\n$$\nwhere $I$ is the identity matrix of appropriate size. Trials are independently presented with task probability $\\mathbb{P}(c_{ij} = 1) = 0.6$ and rest probability $\\mathbb{P}(c_{ij} = 0) = 0.4$ across the study, and assume this reflects the design distribution at which marginal variances are assessed.\n\nUsing the principles of linear mixed-effects modeling and variance decomposition of the marginal distribution of $y_{ij}$, compute the design-averaged proportion of the total marginal variance in trial-level BOLD amplitude that is attributable to between-subject differences (i.e., to the random effects at the subject level), where the averaging is with respect to the stated condition probabilities. Report your answer as an exact fraction. Do not express your answer using a percent sign.",
            "solution": "The problem asks for the proportion of the total marginal variance of the Blood Oxygenation Level Dependent (BOLD) amplitude, $y_{ij}$, that is attributable to between-subject differences. The total variance must be averaged over the distribution of the experimental condition, $c_{ij}$.\n\nThe specified linear mixed-effects model is:\n$$\ny_{ij} = \\beta_{0} + \\beta_{1} c_{ij} + b_{0i} + b_{1i} c_{ij} + \\varepsilon_{ij}\n$$\nHere, $\\beta_{0}$ and $\\beta_{1}$ are fixed-effects parameters, representing the population-average intercept and task effect, respectively. The terms $b_{0i}$ and $b_{1i}$ are the subject-specific random effects, representing each subject's deviation from the population intercept and task effect. The term $\\varepsilon_{ij}$ is the residual error for trial $j$ of subject $i$.\n\nThe total variance of an observation $y_{ij}$ arises from the random components of the model. The fixed-effects part, $\\beta_{0} + \\beta_{1} c_{ij}$, determines the conditional mean of $y_{ij}$ but does not contribute to its variance. The variance decomposition thus concerns the random part of the model: $b_{0i} + b_{1i} c_{ij} + \\varepsilon_{ij}$.\n\nThe random effects $\\begin{pmatrix} b_{0i} \\\\ b_{1i} \\end{pmatrix}$ and the residual errors $\\varepsilon_{ij}$ are assumed to be independent. Therefore, the variance of $y_{ij}$, conditional on the task condition $c_{ij}$, can be expressed as the sum of the variance due to the random effects and the variance of the residual error:\n$$\n\\mathrm{Var}(y_{ij} | c_{ij}) = \\mathrm{Var}(b_{0i} + b_{1i} c_{ij}) + \\mathrm{Var}(\\varepsilon_{ij})\n$$\nThe first term, $\\mathrm{Var}(b_{0i} + b_{1i} c_{ij})$, represents the between-subject variance for a given condition $c_{ij}$. The second term, $\\mathrm{Var}(\\varepsilon_{ij})$, is the within-subject (or residual) variance.\n\nLet's calculate the between-subject variance component, which is dependent on $c_{ij}$. Using the properties of variance and the given random effects covariance matrix $G$, we have:\n$$\n\\mathrm{Var}(b_{0i} + b_{1i} c_{ij}) = \\mathrm{Var}(b_{0i}) + c_{ij}^2 \\mathrm{Var}(b_{1i}) + 2 c_{ij} \\mathrm{Cov}(b_{0i}, b_{1i})\n$$\nThe components of the covariance matrix $G = \\begin{pmatrix} 0.8 & 0.2 \\\\ 0.2 & 0.5 \\end{pmatrix}$ are $\\mathrm{Var}(b_{0i}) = G_{11} = 0.8$, $\\mathrm{Var}(b_{1i}) = G_{22} = 0.5$, and $\\mathrm{Cov}(b_{0i}, b_{1i}) = G_{12} = 0.2$.\nSince $c_{ij}$ is a binary indicator taking values in $\\{0, 1\\}$, we have $c_{ij}^2 = c_{ij}$.\nSubstituting the values from $G$:\n$$\n\\mathrm{Var}(b_{0i} + b_{1i} c_{ij}) = 0.8 + c_{ij}(0.5) + 2 c_{ij}(0.2) = 0.8 + 0.9 c_{ij}\n$$\nThis is the between-subject variance, conditional on the task condition $c_{ij}$.\n\nThe residual variance is given as $\\mathrm{Var}(\\varepsilon_{ij}) = \\sigma^2 = 1.2$. This variance is constant across conditions.\n\nThe total variance of $y_{ij}$, conditional on $c_{ij}$, is therefore:\n$$\n\\mathrm{Var}(y_{ij} | c_{ij}) = (0.8 + 0.9 c_{ij}) + 1.2 = 2.0 + 0.9 c_{ij}\n$$\nThe problem requires computing a \"design-averaged\" proportion. This means we must average the variance components over the specified probability distribution of the condition indicator $c_{ij}$, which is $\\mathbb{P}(c_{ij} = 1) = 0.6$ and $\\mathbb{P}(c_{ij} = 0) = 0.4$. The expected value of $c_{ij}$ is $\\mathbb{E}[c_{ij}] = 1 \\times \\mathbb{P}(c_{ij} = 1) + 0 \\times \\mathbb{P}(c_{ij} = 0) = 0.6$.\n\nLet $V_{\\text{subject}}$ be the design-averaged variance attributable to between-subject differences. We compute this by taking the expectation of the conditional between-subject variance with respect to $c_{ij}$:\n$$\nV_{\\text{subject}} = \\mathbb{E}_{c_{ij}}[\\mathrm{Var}(b_{0i} + b_{1i} c_{ij})] = \\mathbb{E}[0.8 + 0.9 c_{ij}] = 0.8 + 0.9 \\mathbb{E}[c_{ij}]\n$$\nSubstituting $\\mathbb{E}[c_{ij}] = 0.6$:\n$$\nV_{\\text{subject}} = 0.8 + 0.9 \\times 0.6 = 0.8 + 0.54 = 1.34\n$$\n\nLet $V_{\\text{total}}$ be the design-averaged total marginal variance. This is the sum of the averaged between-subject variance and the (constant) residual variance.\n$$\nV_{\\text{total}} = \\mathbb{E}_{c_{ij}}[\\mathrm{Var}(y_{ij} | c_{ij})] = \\mathbb{E}[V_{\\text{subject}}(c_{ij}) + \\sigma^2] = V_{\\text{subject}} + \\sigma^2\n$$\nSubstituting the values:\n$$\nV_{\\text{total}} = 1.34 + 1.2 = 2.54\n$$\nThe proportion of the total marginal variance attributable to between-subject differences is the ratio of $V_{\\text{subject}}$ to $V_{\\text{total}}$:\n$$\n\\text{Proportion} = \\frac{V_{\\text{subject}}}{V_{\\text{total}}} = \\frac{1.34}{2.54}\n$$\nTo express this as an exact fraction, we can write the ratio as:\n$$\n\\frac{1.34}{2.54} = \\frac{134}{254}\n$$\nDividing the numerator and the denominator by their greatest common divisor, which is $2$:\n$$\n\\frac{134 \\div 2}{254 \\div 2} = \\frac{67}{127}\n$$\nBoth $67$ and $127$ are prime numbers, so the fraction is in its simplest form.",
            "answer": "$$\\boxed{\\frac{67}{127}}$$"
        },
        {
            "introduction": "Fitting complex hierarchical models often comes with practical challenges, and one of the most common is the 'singular fit' warning, which indicates that your model is likely too complex for the data to support. This exercise  delves into the diagnosis and resolution of this issue. You will learn to identify the mathematical signature of a singular fit and explore statistically sound strategies for simplifying your model's random-effects structure, a crucial skill for building robust and parsimonious models.",
            "id": "4175331",
            "problem": "A neuroscience lab analyzes trial-level spike rate responses in a cortical area, recorded from $N$ subjects over $T$ trials per subject during a binary sensory task where the within-subject experimental condition is coded by $x_{it} \\in \\{-1, +1\\}$ for subject index $i \\in \\{1, \\dots, N\\}$ and trial index $t \\in \\{1, \\dots, T\\}$. For each subject, a linear mixed-effects model is specified for response $y_{it}$ with a fixed intercept and fixed condition effect, and subject-specific deviations modeled as a random intercept and a random slope for $x_{it}$. The model in matrix notation is\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b} + \\boldsymbol{\\epsilon},\n$$\nwhere $\\mathbf{y}$ collects $y_{it}$ across subjects and trials, $\\mathbf{X}$ contains a column of ones and $x_{it}$, $\\boldsymbol{\\beta}$ is the vector of fixed effects, $\\mathbf{Z}$ contains the subject-level random-intercept and random-slope design columns, $\\mathbf{b} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{G}(\\boldsymbol{\\theta}))$ with $\\mathbf{G}(\\boldsymbol{\\theta})$ the random-effects covariance parameterized by variance components and correlation, and $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$. The model is fitted by Restricted Maximum Likelihood (REML).\n\nOn this dataset, the fitted model reports a singular fit and the estimated variance of the random slope for $x_{it}$ is $\\hat{\\sigma}^2_{\\text{slope}} \\approx 0$, with an estimated correlation between random intercept and random slope of magnitude less than $1$. Assume standard regularity conditions for linear mixed-effects models and that the random-effects covariance must be positive semidefinite. Starting from the definitions above and properties of positive semidefinite covariance matrices, explain how to diagnose that the singular fit arises from an effectively redundant random slope and propose a scientifically justified model simplification that removes redundancy without compromising valid inference on $\\boldsymbol{\\beta}$.\n\nWhich of the following actions are both theoretically justified and appropriate for diagnosing the source of singularity and simplifying the model by removing redundant random effects in this setting?\n\nA. Compute the eigenvalues of the estimated random-effects covariance $\\hat{\\mathbf{G}}$ and verify that at least one eigenvalue is near $0$, consistent with $\\hat{\\sigma}^2_{\\text{slope}} \\approx 0$; conclude that the random slope contributes negligible variability and refit the nested model with the random slope removed. Under Maximum Likelihood and Restricted Maximum Likelihood, the nested refit retains valid inference for $\\boldsymbol{\\beta}$ if the random slope variance is truly $0$.\n\nB. Retain the random slope but constrain the intercept-slope correlation to $0$ to force a block-diagonal $\\mathbf{G}$; this eliminates singularity whenever $\\hat{\\sigma}^2_{\\text{slope}} \\approx 0$ and guarantees unbiased fixed-effect estimates without removing any random effects.\n\nC. Formally test $H_0: \\sigma^2_{\\text{slope}} = 0$ versus $H_1: \\sigma^2_{\\text{slope}} > 0$ using a Likelihood Ratio Test (LRT) computed from nested models, and evaluate the statistic under the correct boundary distribution for a variance component at $0$ (a mixture of a point mass at $0$ and a one-degree-of-freedom chi-square). If the test is not significant, remove the random slope and any associated correlation parameters, thereby eliminating the singular fit.\n\nD. Remove the fixed condition effect from $\\mathbf{X}$ when $\\hat{\\sigma}^2_{\\text{slope}} \\approx 0$, since the lack of random-slope variability implies the condition effect is not present; refit an intercept-only mixed model to avoid singularity.\n\nE. Aggregate each subject’s trials into a single subject-level mean response and fit a standard ordinary least squares model, thereby avoiding singularity by eliminating the hierarchical structure that produced the random-effects design $\\mathbf{Z}$.",
            "solution": "The problem asks for a valid procedure to diagnose and correct a singular fit in a linear mixed-effects model (LMM) where the estimated random slope variance is near zero.\n\nFirst, let us formalize the random-effects structure. The random effects for a single subject $i$ are the deviation from the fixed intercept, $b_{0i}$, and the deviation from the fixed slope, $b_{1i}$. The random effects vector for subject $i$ is $\\mathbf{b}_i = [b_{0i}, b_{1i}]^T$. The problem states that $\\mathbf{b} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{G}(\\boldsymbol{\\theta}))$. This implies that the vectors $\\mathbf{b}_i$ are independent and identically distributed, with $\\mathbf{b}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{G}_{\\text{subject}})$, where $\\mathbf{G}_{\\text{subject}}$ is the $2 \\times 2$ covariance matrix for the random intercept and slope within a subject. This matrix is parameterized by the variance and correlation components $\\boldsymbol{\\theta}$. The full covariance matrix $\\mathbf{G}(\\boldsymbol{\\theta})$ is a block-diagonal matrix with $N$ copies of $\\mathbf{G}_{\\text{subject}}$ on its diagonal.\n\nThe per-subject random-effects covariance matrix $\\mathbf{G}_{\\text{subject}}$ is given by:\n$$\n\\mathbf{G}_{\\text{subject}} = \\begin{pmatrix} \\sigma^2_{\\text{intercept}} & \\rho \\sigma_{\\text{intercept}} \\sigma_{\\text{slope}} \\\\ \\rho \\sigma_{\\text{intercept}} \\sigma_{\\text{slope}} & \\sigma^2_{\\text{slope}} \\end{pmatrix}\n$$\nwhere $\\sigma^2_{\\text{intercept}}$ and $\\sigma^2_{\\text{slope}}$ are the variances of the random intercept and random slope, respectively, and $\\rho$ is their correlation.\n\nA fundamental property of any covariance matrix is that it must be positive semidefinite. A matrix is positive semidefinite if and only if all of its eigenvalues are non-negative. A matrix is singular if at least one of its eigenvalues is zero, which is equivalent to its determinant being zero. The determinant of $\\mathbf{G}_{\\text{subject}}$ is:\n$$\n\\det(\\mathbf{G}_{\\text{subject}}) = (\\sigma^2_{\\text{intercept}})(\\sigma^2_{\\text{slope}}) - (\\rho \\sigma_{\\text{intercept}} \\sigma_{\\text{slope}})^2 = \\sigma^2_{\\text{intercept}} \\sigma^2_{\\text{slope}}(1 - \\rho^2)\n$$\nThe determinant is zero if $\\sigma^2_{\\text{intercept}} = 0$, or $\\sigma^2_{\\text{slope}} = 0$, or $|\\rho| = 1$.\n\nA \"singular fit\" indicates that the estimated random-effects covariance matrix, $\\hat{\\mathbf{G}}_{\\text{subject}}$, is on the boundary of the space of positive semidefinite matrices, meaning it is singular. The problem states that the fit yielded an estimated random slope variance $\\hat{\\sigma}^2_{\\text{slope}} \\approx 0$. Substituting this into the estimated covariance matrix gives:\n$$\n\\hat{\\mathbf{G}}_{\\text{subject}} \\approx \\begin{pmatrix} \\hat{\\sigma}^2_{\\text{intercept}} & \\hat{\\rho} \\hat{\\sigma}_{\\text{intercept}} \\cdot 0 \\\\ \\hat{\\rho} \\hat{\\sigma}_{\\text{intercept}} \\cdot 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\hat{\\sigma}^2_{\\text{intercept}} & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe determinant of this matrix is approximately $0$, confirming singularity. The eigenvalues of this matrix are approximately $\\hat{\\sigma}^2_{\\text{intercept}}$ and $0$. The presence of a zero eigenvalue is the mathematical signature of singularity and directly results from the zero variance estimate for the random slope. This indicates that the data do not support the inclusion of subject-to-subject variability in the slope for condition $x_{it}$; the random slope is a redundant parameter.\n\nThe appropriate action is to simplify the model by removing the redundant random effect. This involves refitting the model without the random slope and its associated correlation parameter. The simplified model would only include a random intercept for subjects. To formally justify this simplification, one can perform a hypothesis test comparing the full model (with random intercept and slope) to the reduced model (with random intercept only). The null hypothesis is that the simpler model is sufficient, which corresponds to testing $H_0: \\sigma^2_{\\text{slope}} = 0$.\n\nNow, we evaluate each option based on this understanding.\n\nA. Compute the eigenvalues of the estimated random-effects covariance $\\hat{\\mathbf{G}}$ and verify that at least one eigenvalue is near $0$, consistent with $\\hat{\\sigma}^2_{\\text{slope}} \\approx 0$; conclude that the random slope contributes negligible variability and refit the nested model with the random slope removed. Under Maximum Likelihood and Restricted Maximum Likelihood, the nested refit retains valid inference for $\\boldsymbol{\\beta}$ if the random slope variance is truly $0$.\n**Evaluation:** This option describes a completely valid and practical procedure. Computing the eigenvalues of $\\hat{\\mathbf{G}}_{\\text{subject}}$ is a direct way to diagnose the mathematical source of the singularity, confirming that it stems from the near-zero variance component. A near-zero eigenvalue supports the conclusion that the random slope contributes negligible variability. Removing the random slope is the correct simplification to resolve the singularity and reduce model complexity. The final statement is also correct: simplifying the model by removing a random effect whose true variance is zero does not invalidate inference on the fixed effects $\\boldsymbol{\\beta}$ and typically improves the stability and precision of their estimates.\n**Verdict:** **Correct**\n\nB. Retain the random slope but constrain the intercept-slope correlation to $0$ to force a block-diagonal $\\mathbf{G}$; this eliminates singularity whenever $\\hat{\\sigma}^2_{\\text{slope}} \\approx 0$ and guarantees unbiased fixed-effect estimates without removing any random effects.\n**Evaluation:** This option is flawed. First, the term 'block-diagonal' is misused; setting $\\rho=0$ would make the $2 \\times 2$ matrix $\\mathbf{G}_{\\text{subject}}$ diagonal, not block-diagonal. More importantly, this action does not solve the singularity problem. If $\\hat{\\sigma}^2_{\\text{slope}} \\approx 0$, the constrained matrix would be approximately $\\text{diag}(\\hat{\\sigma}^2_{\\text{intercept}}, 0)$, which is still singular. The singularity is caused by the zero variance, not the correlation, especially since the problem states $|\\hat{\\rho}| < 1$.\n**Verdict:** **Incorrect**\n\nC. Formally test $H_0: \\sigma^2_{\\text{slope}} = 0$ versus $H_1: \\sigma^2_{\\text{slope}} > 0$ using a Likelihood Ratio Test (LRT) computed from nested models, and evaluate the statistic under the correct boundary distribution for a variance component at $0$ (a mixture of a point mass at $0$ and a one-degree-of-freedom chi-square). If the test is not significant, remove the random slope and any associated correlation parameters, thereby eliminating the singular fit.\n**Evaluation:** This option describes the most rigorous statistical procedure for model simplification in this context. The Likelihood Ratio Test is the standard method for comparing nested models. Critically, the option correctly identifies that the null hypothesis $H_0: \\sigma^2_{\\text{slope}} = 0$ lies on the boundary of the parameter space (since variances cannot be negative), and therefore the LRT statistic does not follow a standard chi-square distribution. The correct asymptotic distribution is a mixture, typically $0.5\\chi^2_0 + 0.5\\chi^2_1$. Basing the decision to simplify the model on the outcome of this formal test is a cornerstone of principled statistical modeling. Removing the random slope and its correlation parameter upon a non-significant result is the correct action.\n**Verdict:** **Correct**\n\nD. Remove the fixed condition effect from $\\mathbf{X}$ when $\\hat{\\sigma}^2_{\\text{slope}} \\approx 0$, since the lack of random-slope variability implies the condition effect is not present; refit an intercept-only mixed model to avoid singularity.\n**Evaluation:** This option demonstrates a fundamental misunderstanding of mixed-effects models. The random slope variance, $\\sigma^2_{\\text{slope}}$, quantifies the variability of the condition's effect *across subjects*. An estimate of $\\hat{\\sigma}^2_{\\text{slope}} \\approx 0$ suggests that the effect of the condition is consistent across subjects, not that the effect itself is absent. The average effect across all subjects is captured by the *fixed effect* slope, which is a component of $\\boldsymbol{\\beta}$. It is entirely possible to have a large and significant fixed effect even when its corresponding random effect variance is zero. Removing the fixed effect is scientifically and statistically unjustifiable based on the provided information.\n**Verdict:** **Incorrect**\n\nE. Aggregate each subject’s trials into a single subject-level mean response and fit a standard ordinary least squares model, thereby avoiding singularity by eliminating the hierarchical structure that produced the random-effects design $\\mathbf{Z}$.\n**Evaluation:** This procedure, known as a summary-statistics approach, does avoid the singularity issue by abandoning the mixed-effects framework. However, it is a suboptimal alternative, not a justified simplification of the original model. Aggregating data to the subject level discards all information about trial-level variability, leading to a significant loss of statistical power. Furthermore, it can be biased if the data are unbalanced. The LMM framework is specifically designed to handle such hierarchical data efficiently and correctly. Discarding this powerful approach instead of correctly simplifying it is poor practice and \"compromises valid inference\" by using a less efficient estimator with potentially inaccurate standard errors.\n**Verdict:** **Incorrect**\n\nBoth options A and C describe valid and theoretically justified actions. Option A focuses on the direct mathematical diagnosis of the singularity, while Option C details the formal statistical hypothesis testing procedure for model simplification. Both are appropriate and lead to the correct conclusion and action.",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}