{
    "hands_on_practices": [
        {
            "introduction": "The first and most critical challenge in simultaneous EEG-fMRI is correcting the massive artifacts induced by the MRI scanner's magnetic field gradients. This practice delves into the statistical underpinnings of Average Artifact Subtraction (AAS), a cornerstone correction technique. By deriving the expected residual artifact power for different averaging schemes, you will gain a quantitative understanding of how AAS works and how its performance depends on the number of volumes $N$ available for averaging. ",
            "id": "4179392",
            "problem": "You are given a mathematical model of electroencephalography (EEG) recorded during functional magnetic resonance imaging (fMRI) where the magnetic resonance (MR) gradient artifact appears repetitively across volumes. For each volume index $k$ and discrete time index $t \\in \\{0,1,\\dots,T-1\\}$, the artifact segment is modeled as $A_k(t) = \\mu(t) + \\epsilon_k(t)$, where $\\mu(t)$ is a deterministic artifact template (identical across volumes) and $\\epsilon_k(t)$ is a zero-mean random deviation across volumes. Assume that for each fixed $t$, the set $\\{\\epsilon_k(t)\\}_{k}$ is independent and identically distributed with variance $\\sigma_a^2$ (not necessarily independent across $t$), and that $\\epsilon_k(t)$ is independent of $\\mu(t)$. The artifact power per segment is defined as $P_{\\text{pre}} = \\sum_{t=0}^{T-1} \\mathbb{E}\\big[A_k(t)^2\\big]$, and the residual artifact power after average artifact subtraction is defined as $P_{\\text{res}} = \\sum_{t=0}^{T-1} \\mathbb{E}\\big[R_k(t)^2\\big]$, where $R_k(t)$ is the residual artifact after subtracting an average template computed from $N$ volumes.\n\nAverage Artifact Subtraction (AAS) uses an empirical average of observed artifact segments to estimate the template. Consider two estimator designs for the template used to subtract from a target volume $k^\\star$:\n\n- Inclusive average: $\\hat{\\mu}_N^{\\text{inc}}(t) = \\frac{1}{N}\\sum_{i=1}^{N} A_i(t)$ where the target volume $k^\\star$ is one of the $N$ volumes used to compute the average. The residual is $R_{k^\\star}^{\\text{inc}}(t) = A_{k^\\star}(t) - \\hat{\\mu}_N^{\\text{inc}}(t)$.\n\n- Leave-one-out (exclusive) average: $\\hat{\\mu}_N^{\\text{exc}}(t) = \\frac{1}{N}\\sum_{i \\in \\mathcal{I}} A_i(t)$ where $\\mathcal{I}$ is a set of $N$ volumes that excludes the target $k^\\star$. The residual is $R_{k^\\star}^{\\text{exc}}(t) = A_{k^\\star}(t) - \\hat{\\mu}_N^{\\text{exc}}(t)$.\n\nStarting from the above definitions and the properties of expectation and variance for independent random variables, derive expressions for the residual artifact power $P_{\\text{res}}$ under both designs and analyze their convergence as $N$ increases. Specifically, compute the residual artifact power ratio $R = P_{\\text{res}} / P_{\\text{pre}}$, which is dimensionless. All angles used in any sinusoidal construction of $\\mu(t)$ must be in radians. All powers must be expressed in microvolt squared ($\\mu\\text{V}^2$).\n\nFor numerical evaluation, use the following deterministic template construction for $\\mu(t)$ at sample count $T$:\n- $\\mu(t) = A_1 \\sin\\left(2\\pi f_1 \\frac{t}{T}\\right) + A_2 \\cos\\left(2\\pi f_2 \\frac{t}{T}\\right)$, with angles in radians.\n- Use $T = 256$, $A_1 = 50$ $\\mu\\text{V}$, $A_2 = 30$ $\\mu\\text{V}$, $f_1 = 5$, and $f_2 = 13$.\n- Use $\\sigma_a = 10$ $\\mu\\text{V}$ for the random deviation standard deviation across volumes at each $t$, so the variance is $\\sigma_a^2$.\n\nDefine the following test suite of parameter sets, each specifying the averaging design (inclusive or exclusive), the number of volumes $N$ used to compute the average, and the template $\\mu(t)$ condition. In all cases, report the residual artifact power ratio $R$ as a decimal number (dimensionless). For the final output, list the results in the exact order provided here:\n\n1. Inclusive design with $N=1$, $T=256$, $\\sigma_a=10$ $\\mu\\text{V}$, $\\mu(t)$ as above.\n2. Inclusive design with $N=5$, $T=256$, $\\sigma_a=10$ $\\mu\\text{V}$, $\\mu(t)$ as above.\n3. Inclusive design with $N=20$, $T=256$, $\\sigma_a=10$ $\\mu\\text{V}$, $\\mu(t)$ as above.\n4. Inclusive design with $N=100$, $T=256$, $\\sigma_a=10$ $\\mu\\text{V}$, $\\mu(t)$ as above.\n5. Exclusive design with $N=1$, $T=256$, $\\sigma_a=10$ $\\mu\\text{V}$, $\\mu(t)$ as above.\n6. Exclusive design with $N=5$, $T=256$, $\\sigma_a=10$ $\\mu\\text{V}$, $\\mu(t)$ as above.\n7. Exclusive design with $N=20$, $T=256$, $\\sigma_a=10$ $\\mu\\text{V}$, $\\mu(t)$ as above.\n8. Exclusive design with $N=100$, $T=256$, $\\sigma_a=10$ $\\mu\\text{V}$, $\\mu(t)$ as above.\n9. Exclusive design with $N=5$, $T=256$, $\\sigma_a=10$ $\\mu\\text{V}$, and $\\mu(t) = 0$ for all $t$ (boundary case).\n\nYour program should produce a single line of output containing the residual artifact power ratios for the nine test cases as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,\\dots,r_9]$), where each $r_i$ is a decimal number.",
            "solution": "The problem is assessed as valid as it is scientifically grounded in signal processing principles, internally consistent, and well-posed with all necessary information provided for a unique solution.\n\nThe objective is to derive the residual artifact power ratio $R = P_{\\text{res}} / P_{\\text{pre}}$ for two averaging designs, inclusive and exclusive, and then evaluate this ratio for a given set of parameters.\n\nThe model for an artifact segment for volume $k$ is given by:\n$$A_k(t) = \\mu(t) + \\epsilon_k(t)$$\nwhere $t \\in \\{0, 1, \\dots, T-1\\}$, $\\mu(t)$ is a deterministic template, and $\\epsilon_k(t)$ is a random deviation with $\\mathbb{E}[\\epsilon_k(t)] = 0$ and $\\text{Var}(\\epsilon_k(t)) = \\sigma_a^2$. The deviations $\\{\\epsilon_k(t)\\}_{k}$ for a fixed time $t$ are independent and identically distributed (i.i.d.).\n\n**Step 1: Derivation of the Pre-correction Power, $P_{\\text{pre}}$**\nThe pre-correction power per segment is defined as $P_{\\text{pre}} = \\sum_{t=0}^{T-1} \\mathbb{E}[A_k(t)^2]$.\nTo find $\\mathbb{E}[A_k(t)^2]$, we use the relation $\\mathbb{E}[X^2] = \\text{Var}(X) + (\\mathbb{E}[X])^2$.\nThe expectation of $A_k(t)$ is:\n$$\\mathbb{E}[A_k(t)] = \\mathbb{E}[\\mu(t) + \\epsilon_k(t)] = \\mu(t) + \\mathbb{E}[\\epsilon_k(t)] = \\mu(t)$$\nThe variance of $A_k(t)$ is:\n$$\\text{Var}(A_k(t)) = \\text{Var}(\\mu(t) + \\epsilon_k(t)) = \\text{Var}(\\epsilon_k(t)) = \\sigma_a^2$$\nsince $\\mu(t)$ is deterministic.\nTherefore, the expected squared amplitude at time $t$ is:\n$$\\mathbb{E}[A_k(t)^2] = \\text{Var}(A_k(t)) + (\\mathbb{E}[A_k(t)])^2 = \\sigma_a^2 + \\mu(t)^2$$\nSumming over all time points $t$ from $0$ to $T-1$:\n$$P_{\\text{pre}} = \\sum_{t=0}^{T-1} (\\mu(t)^2 + \\sigma_a^2) = \\left(\\sum_{t=0}^{T-1} \\mu(t)^2\\right) + T\\sigma_a^2$$\nLet $P_\\mu = \\sum_{t=0}^{T-1} \\mu(t)^2$ be the power of the deterministic template.\n$$P_{\\text{pre}} = P_\\mu + T\\sigma_a^2$$\n\n**Step 2: Derivation of Residual Power for Inclusive Design, $P_{\\text{res}}^{\\text{inc}}$**\nThe template is estimated using $N$ volumes, including the target volume $k^\\star$. Let $k^\\star$ be one of $\\{1, \\dots, N\\}$.\n$$\\hat{\\mu}_N^{\\text{inc}}(t) = \\frac{1}{N}\\sum_{i=1}^{N} A_i(t)$$\nThe residual for the target volume $k^\\star$ is $R_{k^\\star}^{\\text{inc}}(t) = A_{k^\\star}(t) - \\hat{\\mu}_N^{\\text{inc}}(t)$.\n$$R_{k^\\star}^{\\text{inc}}(t) = A_{k^\\star}(t) - \\frac{1}{N}\\sum_{i=1}^{N} A_i(t) = \\left(1 - \\frac{1}{N}\\right)A_{k^\\star}(t) - \\frac{1}{N}\\sum_{i \\neq k^\\star} A_i(t)$$\nSubstituting $A_i(t) = \\mu(t) + \\epsilon_i(t)$:\n$$R_{k^\\star}^{\\text{inc}}(t) = \\left(1 - \\frac{1}{N}\\right)(\\mu(t) + \\epsilon_{k^\\star}(t)) - \\frac{1}{N}\\sum_{i \\neq k^\\star} (\\mu(t) + \\epsilon_i(t))$$\nThe terms involving $\\mu(t)$ cancel out: $\\left(1 - \\frac{1}{N}\\right)\\mu(t) - \\frac{N-1}{N}\\mu(t) = 0$.\n$$R_{k^\\star}^{\\text{inc}}(t) = \\left(1 - \\frac{1}{N}\\right)\\epsilon_{k^\\star}(t) - \\frac{1}{N}\\sum_{i \\neq k^\\star} \\epsilon_i(t)$$\nSince $\\mathbb{E}[\\epsilon_i(t)] = 0$ for all $i$, we have $\\mathbb{E}[R_{k^\\star}^{\\text{inc}}(t)] = 0$.\nThe expected squared residual at time $t$ is its variance:\n$$\\mathbb{E}[(R_{k^\\star}^{\\text{inc}}(t))^2] = \\text{Var}(R_{k^\\star}^{\\text{inc}}(t))$$\nGiven that all $\\epsilon_i(t)$ are independent for a fixed $t$, we have:\n$$\\text{Var}(R_{k^\\star}^{\\text{inc}}(t)) = \\left(1 - \\frac{1}{N}\\right)^2 \\text{Var}(\\epsilon_{k^\\star}(t)) + \\sum_{i \\neq k^\\star} \\left(-\\frac{1}{N}\\right)^2 \\text{Var}(\\epsilon_i(t))$$\n$$= \\left(\\frac{N-1}{N}\\right)^2 \\sigma_a^2 + (N-1) \\frac{1}{N^2} \\sigma_a^2 = \\frac{(N-1)^2 + (N-1)}{N^2} \\sigma_a^2 = \\frac{(N-1)N}{N^2} \\sigma_a^2 = \\frac{N-1}{N} \\sigma_a^2$$\nThe total residual power is the sum over all time points:\n$$P_{\\text{res}}^{\\text{inc}} = \\sum_{t=0}^{T-1} \\frac{N-1}{N} \\sigma_a^2 = T\\sigma_a^2\\left(1 - \\frac{1}{N}\\right)$$\n\n**Step 3: Derivation of Residual Power for Exclusive Design, $P_{\\text{res}}^{\\text{exc}}$**\nThe template is estimated using $N$ volumes that do not include the target volume $k^\\star$.\n$$\\hat{\\mu}_N^{\\text{exc}}(t) = \\frac{1}{N}\\sum_{i \\in \\mathcal{I}} A_i(t), \\quad k^\\star \\notin \\mathcal{I}$$\nThe residual is $R_{k^\\star}^{\\text{exc}}(t) = A_{k^\\star}(t) - \\hat{\\mu}_N^{\\text{exc}}(t)$.\n$$R_{k^\\star}^{\\text{exc}}(t) = (\\mu(t) + \\epsilon_{k^\\star}(t)) - \\frac{1}{N}\\sum_{i \\in \\mathcal{I}} (\\mu(t) + \\epsilon_i(t))$$\nThe terms involving $\\mu(t)$ again cancel: $\\mu(t) - \\frac{N}{N}\\mu(t) = 0$.\n$$R_{k^\\star}^{\\text{exc}}(t) = \\epsilon_{k^\\star}(t) - \\frac{1}{N}\\sum_{i \\in \\mathcal{I}} \\epsilon_i(t)$$\nAs before, $\\mathbb{E}[R_{k^\\star}^{\\text{exc}}(t)] = 0$, so $\\mathbb{E}[(R_{k^\\star}^{\\text{exc}}(t))^2] = \\text{Var}(R_{k^\\star}^{\\text{exc}}(t))$.\nSince $k^\\star \\notin \\mathcal{I}$, $\\epsilon_{k^\\star}(t)$ is independent of all $\\epsilon_i(t)$ for $i \\in \\mathcal{I}$.\n$$\\text{Var}(R_{k^\\star}^{\\text{exc}}(t)) = \\text{Var}(\\epsilon_{k^\\star}(t)) + \\text{Var}\\left(-\\frac{1}{N}\\sum_{i \\in \\mathcal{I}} \\epsilon_i(t)\\right) = \\sigma_a^2 + \\frac{1}{N^2} \\sum_{i \\in \\mathcal{I}} \\text{Var}(\\epsilon_i(t))$$\n$$= \\sigma_a^2 + \\frac{1}{N^2} (N \\sigma_a^2) = \\sigma_a^2\\left(1 + \\frac{1}{N}\\right)$$\nThe total residual power is:\n$$P_{\\text{res}}^{\\text{exc}} = \\sum_{t=0}^{T-1} \\sigma_a^2\\left(1 + \\frac{1}{N}\\right) = T\\sigma_a^2\\left(1 + \\frac{1}{N}\\right)$$\n\n**Step 4: The Residual Power Ratio $R$ and Convergence**\nAs $N \\to \\infty$, both $P_{\\text{res}}^{\\text{inc}}$ and $P_{\\text{res}}^{\\text{exc}}$ converge to $T\\sigma_a^2$, which is the total power of the noise component. The inclusive design slightly underestimates this residual noise power for finite $N$, while the exclusive design overestimates it.\n\nThe ratios $R = P_{\\text{res}} / P_{\\text{pre}}$ are:\n$$R^{\\text{inc}} = \\frac{T\\sigma_a^2(1 - 1/N)}{P_\\mu + T\\sigma_a^2}$$\n$$R^{\\text{exc}} = \\frac{T\\sigma_a^2(1 + 1/N)}{P_\\mu + T\\sigma_a^2}$$\n\n**Step 5: Numerical Evaluation**\nThe parameters are: $T=256$, $A_1=50$, $A_2=30$, $f_1=5$, $f_2=13$, $\\sigma_a=10$. This gives $\\sigma_a^2=100$.\nThe template power $P_\\mu$ is calculated first:\n$$P_\\mu = \\sum_{t=0}^{T-1} \\left( A_1 \\sin\\left(\\frac{2\\pi f_1 t}{T}\\right) + A_2 \\cos\\left(\\frac{2\\pi f_2 t}{T}\\right) \\right)^2$$\nDue to the orthogonality of the sinusoidal functions over the interval $[0, T-1]$ for the given integer frequencies $f_1, f_2$ (where $f_1 \\neq f_2$ and neither is $0$ or $T/2$), the cross-term in the expansion sums to zero, and we have:\n$$\\sum_{t=0}^{T-1} \\sin^2\\left(\\frac{2\\pi f t}{T}\\right) = \\sum_{t=0}^{T-1} \\cos^2\\left(\\frac{2\\pi f t}{T}\\right) = \\frac{T}{2}$$\nThus, $P_\\mu = A_1^2 \\frac{T}{2} + A_2^2 \\frac{T}{2} = \\frac{T}{2}(A_1^2 + A_2^2)$.\n$$P_\\mu = \\frac{256}{2}(50^2 + 30^2) = 128(2500 + 900) = 128(3400) = 435200 \\, \\mu\\text{V}^2$$\nThe noise power term is $T\\sigma_a^2 = 256 \\times 100 = 25600 \\, \\mu\\text{V}^2$.\nThe total pre-correction power is $P_{\\text{pre}} = 435200 + 25600 = 460800 \\, \\mu\\text{V}^2$.\n\nThe ratio formulas can be simplified by dividing the numerator and denominator by $T$:\n$$R = \\frac{\\sigma_a^2(1 \\pm 1/N)}{P_\\mu/T + \\sigma_a^2} = \\frac{\\sigma_a^2(1 \\pm 1/N)}{\\frac{1}{2}(A_1^2 + A_2^2) + \\sigma_a^2}$$\nThe denominator term is $\\frac{1}{2}(50^2 + 30^2) + 10^2 = 1700 + 100 = 1800$.\nThe general formulas become:\n$$R^{\\text{inc}}(N) = \\frac{100(1 - 1/N)}{1800} = \\frac{1}{18}\\left(1 - \\frac{1}{N}\\right)$$\n$$R^{\\text{exc}}(N) = \\frac{100(1 + 1/N)}{1800} = \\frac{1}{18}\\left(1 + \\frac{1}{N}\\right)$$\n\nFor the special case $\\mu(t)=0$, $P_\\mu=0$. The ratio becomes $R^{\\text{exc}} = \\frac{T\\sigma_a^2(1+1/N)}{T\\sigma_a^2} = 1 + 1/N$.\n\nCalculations for the test cases:\n1. Inclusive, $N=1$: $R = \\frac{1}{18}(1 - 1/1) = 0$\n2. Inclusive, $N=5$: $R = \\frac{1}{18}(1 - 1/5) = \\frac{1}{18}(\\frac{4}{5}) = \\frac{4}{90} = \\frac{2}{45} \\approx 0.0444...$\n3. Inclusive, $N=20$: $R = \\frac{1}{18}(1 - 1/20) = \\frac{1}{18}(\\frac{19}{20}) = \\frac{19}{360} \\approx 0.0527...$\n4. Inclusive, $N=100$: $R = \\frac{1}{18}(1 - 1/100) = \\frac{1}{18}(\\frac{99}{100}) = \\frac{11}{200} = 0.055$\n5. Exclusive, $N=1$: $R = \\frac{1}{18}(1 + 1/1) = \\frac{2}{18} = \\frac{1}{9} \\approx 0.1111...$\n6. Exclusive, $N=5$: $R = \\frac{1}{18}(1 + 1/5) = \\frac{1}{18}(\\frac{6}{5}) = \\frac{6}{90} = \\frac{1}{15} \\approx 0.0666...$\n7. Exclusive, $N=20$: $R = \\frac{1}{18}(1 + 1/20) = \\frac{1}{18}(\\frac{21}{20}) = \\frac{7}{120} \\approx 0.0583...$\n8. Exclusive, $N=100$: $R = \\frac{1}{18}(1 + 1/100) = \\frac{101}{1800} \\approx 0.0561...$\n9. Exclusive, $N=5$, $\\mu=0$: $R = 1 + 1/5 = 1.2$",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the residual artifact power ratio for nine test cases based on\n    a model of EEG-fMRI artifacts.\n    \"\"\"\n    # Define the constants from the problem statement\n    A1 = 50.0  # microV\n    A2 = 30.0  # microV\n    sigma_a = 10.0 # microV\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # (design, N, mu_is_zero)\n        ('inclusive', 1, False),\n        ('inclusive', 5, False),\n        ('inclusive', 20, False),\n        ('inclusive', 100, False),\n        ('exclusive', 1, False),\n        ('exclusive', 5, False),\n        ('exclusive', 20, False),\n        ('exclusive', 100, False),\n        ('exclusive', 5, True),\n    ]\n\n    results = []\n\n    # Pre-calculate squared terms for efficiency and clarity\n    sigma_a_sq = sigma_a**2\n    A1_sq = A1**2\n    A2_sq = A2**2\n\n    # The denominator of the ratio R can be simplified from the full power expressions.\n    # The full ratio is R = (T*sigma_a^2 * F(N)) / (P_mu + T*sigma_a^2), where\n    # P_mu = T/2 * (A1^2 + A2^2).\n    # Dividing by T simplifies the ratio to R = (sigma_a^2 * F(N)) / (1/2*(A1^2+A2^2) + sigma_a^2).\n    # This avoids large numbers and dependence on T.\n    denominator_term_mu_present = 0.5 * (A1_sq + A2_sq) + sigma_a_sq\n\n    for design, N, mu_is_zero in test_cases:\n        if mu_is_zero:\n            # Special case 9: mu(t) = 0 for all t.\n            # R = (T*sigma_a^2 * (1 + 1/N)) / (0 + T*sigma_a^2) = 1 + 1/N\n            # This applies to the exclusive design as specified in the test case.\n            if design == 'exclusive':\n                R = 1.0 + 1.0 / N\n            else: # For completeness, if the design were inclusive\n                R = 1.0 - 1.0 / N\n        else:\n            # Cases 1-8 where mu(t) is defined by the sinusoidal formula\n            if design == 'inclusive':\n                # R_inc = (sigma_a^2 * (1 - 1/N)) / (1/2*(A1^2+A2^2) + sigma_a^2)\n                numerator = sigma_a_sq * (1.0 - 1.0 / N)\n                R = numerator / denominator_term_mu_present\n            elif design == 'exclusive':\n                # R_exc = (sigma_a^2 * (1 + 1/N)) / (1/2*(A1^2+A2^2) + sigma_a^2)\n                numerator = sigma_a_sq * (1.0 + 1.0 / N)\n                R = numerator / denominator_term_mu_present\n        \n        results.append(R)\n\n    # Format the output as a comma-separated list enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After artifact correction, we can leverage the high temporal resolution of EEG to inform our analysis of fMRI data in an asymmetric fusion approach. This hands-on coding exercise guides you through creating fMRI regressors from EEG microstate dynamics, which represent quasi-stable global brain states. You will learn to translate these fast electrophysiological events into a signal the fMRI General Linear Model (GLM) can understand by convolving them with the Hemodynamic Response Function (HRF), a crucial step in linking neural activity to the BOLD signal. ",
            "id": "4179375",
            "problem": "You are given a formal task to construct event-related regressors for a functional Magnetic Resonance Imaging (fMRI) General Linear Model (GLM) using Electroencephalography (EEG) microstate information. Assume the following foundational base in signal processing and multimodal integration: discrete spatial correlation between vectors, indicator functions, discrete-time impulse trains for event encoding, double-gamma Hemodynamic Response Function (HRF), and linear time-invariant convolution. You must compute microstate-specific event functions by assigning each EEG time sample to a single microstate based on maximum spatial correlation with template maps, then convolve those event functions with a canonical HRF, and finally sample the convolved signals at the fMRI repetition time to produce regressors. The final output must be a single line containing the regressors for multiple test cases in the exact format specified below.\n\nDefinitions and required steps:\n- Let $X \\in \\mathbb{R}^{T \\times M}$ be the EEG scalp potential matrix for $T$ discrete time samples and $M$ channels, sampled at frequency $f_{\\mathrm{EEG}}$ in $\\mathrm{Hz}$, with sampling period $dt = 1 / f_{\\mathrm{EEG}}$ in $\\mathrm{s}$. Let $G \\in \\mathbb{R}^{K \\times M}$ be $K$ microstate template maps.\n- For each time sample $t \\in \\{0,1,\\dots,T-1\\}$, define the mean-centered EEG map $\\tilde{x}_t \\in \\mathbb{R}^M$ by subtracting its mean. For each template $k \\in \\{1,\\dots,K\\}$, define the mean-centered template $\\tilde{g}_k \\in \\mathbb{R}^M$ by subtracting its mean. Compute the spatial correlation\n$$\nr_k(t) \\;=\\; \\frac{\\langle \\tilde{x}_t, \\tilde{g}_k \\rangle}{\\|\\tilde{x}_t\\|_2 \\, \\|\\tilde{g}_k\\|_2} \\, ,\n$$\nand assign the microstate label by\n$$\n\\hat{z}(t) \\;=\\; \\underset{k \\in \\{1,\\dots,K\\}}{\\arg\\max}\\; r_k(t) \\, .\n$$\nDefine the indicator time series for each microstate $k$ as\n$$\ns_k(t) \\;=\\; \\mathbb{I}\\big[\\hat{z}(t) = k\\big] \\, .\n$$\n- Let an event-related paradigm be encoded by an impulse train $e(t)$ at the EEG sampling grid, defined by $e(t_j) = 1$ at event indices $t_j = \\lfloor \\tau_j / dt \\rfloor$ where $\\tau_j$ are event times in $\\mathrm{s}$ and $e(t) = 0$ otherwise. If an event time maps beyond the last EEG sample, clamp to $t = T-1$. Construct microstate-specific event functions\n$$\nu_k(t) \\;=\\; s_k(t) \\cdot e(t) \\, .\n$$\n- Use a canonical double-gamma Hemodynamic Response Function sampled at the EEG grid, with parameters $\\alpha_1 = 6$, $\\beta_1 = 1$, $\\alpha_2 = 16$, $\\beta_2 = 1$, and $c = 1/6$. Define, for $t \\ge 0$,\n$$\n\\mathrm{GammaPDF}(t;\\alpha,\\beta) \\;=\\; \\frac{t^{\\alpha-1}\\, e^{-t/\\beta}}{\\beta^{\\alpha}\\, \\Gamma(\\alpha)} \\, ,\n$$\nand\n$$\nh(t) \\;=\\; \\mathrm{GammaPDF}(t;\\alpha_1,\\beta_1) \\;-\\; c \\cdot \\mathrm{GammaPDF}(t;\\alpha_2,\\beta_2) \\, .\n$$\nSample $h(t)$ on $t = 0, dt, 2dt, \\dots, L_{\\mathrm{HRF}}$ with $L_{\\mathrm{HRF}} = 32$ in $\\mathrm{s}$. Normalize $h$ to unit area by enforcing $\\sum_{m} h[m] \\cdot dt = 1$.\n- Convolve each $u_k$ with $h$ in discrete time to obtain $c_k = u_k * h$ using full convolution. Let the fMRI repetition time be $TR$ in $\\mathrm{s}$, and the number of volumes be $N_{\\mathrm{vol}}$. Sample the convolved signal at $t_n = n \\cdot TR$ for $n \\in \\{0,1,\\dots,N_{\\mathrm{vol}}-1\\}$ by nearest-lower indexing $y_k[n] = c_k[\\lfloor t_n / dt \\rfloor]$, provided the index lies within the bounds of the full convolution; if an index exceeds the length, clamp to the last available index.\n- Normalize each regressor $y_k$ to unit $\\ell_2$ norm if its norm is nonzero; otherwise leave it as a zero vector.\n\nUnits: All time quantities must be in $\\mathrm{s}$ (seconds). Angles are not used. Percentages are not used.\n\nYour program must implement the above definitions exactly for the following test suite. Each test case specifies $(M,K,f_{\\mathrm{EEG}},TR,N_{\\mathrm{vol}},T,\\text{events},G,\\text{EEG generation})$. For EEG generation, construct $X$ by selecting the microstate template indicated and adding independent Gaussian noise with the given standard deviation, using the provided random seed; that is, for each time $t$ in a segment associated to microstate $k$, set $x_{t} = g_k + \\epsilon_t$ with $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2 I_M)$, and then proceed with mean-centering before correlation. All template maps must be mean-centered and $\\ell_2$-normalized before use.\n\nTest case 1 (happy path):\n- $M = 4$, $K = 4$, $f_{\\mathrm{EEG}} = 250$, $TR = 1$, $N_{\\mathrm{vol}} = 4$, $T = 1000$.\n- Events at times $\\tau = [0.5, 1.5, 2.5, 3.5]$.\n- Templates $G$ (before mean-centering and normalization):\n  $g_1 = [1, -1, 1, -1]$, $g_2 = [1, 1, -1, -1]$, $g_3 = [1, -1, -1, 1]$, $g_4 = [-1, 1, -1, 1]$.\n- EEG generation: segments $[0,249] \\rightarrow k=1$, $[250,499] \\rightarrow k=2$, $[500,749] \\rightarrow k=3$, $[750,999] \\rightarrow k=4$; Gaussian noise standard deviation $\\sigma = 0.05$, random seed $42$.\n\nTest case 2 (boundary length):\n- $M = 3$, $K = 3$, $f_{\\mathrm{EEG}} = 100$, $TR = 0.6$, $N_{\\mathrm{vol}} = 3$, $T = 120$.\n- Events at times $\\tau = [0.2, 0.8, 1.1]$.\n- Templates $G$ (before mean-centering and normalization):\n  $g_1 = [1, -1, 0]$, $g_2 = [-1, 1, 0]$, $g_3 = [0, 1, -1]$.\n- EEG generation: segments $[0,39] \\rightarrow k=1$, $[40,79] \\rightarrow k=2$, $[80,119] \\rightarrow k=3$; Gaussian noise standard deviation $\\sigma = 0.05$, random seed $7$.\n\nTest case 3 (edge events at bounds):\n- $M = 2$, $K = 2$, $f_{\\mathrm{EEG}} = 200$, $TR = 0.75$, $N_{\\mathrm{vol}} = 4$, $T = 600$.\n- Events at times $\\tau = [0.0, 1.5, 3.0]$.\n- Templates $G$ (before mean-centering and normalization):\n  $g_1 = [1, -1]$, $g_2 = [-1, 1]$.\n- EEG generation: segments $[0,299] \\rightarrow k=1$, $[300,599] \\rightarrow k=2$; Gaussian noise standard deviation $\\sigma = 0.02$, random seed $123$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the regressors for all test cases, structured as a list of test cases, where each test case is a list of $K$ lists of length $N_{\\mathrm{vol}}$ (microstate regressors sampled at $TR$), with no spaces. For example, the outer structure should look like\n$[[$reg\\_case1$],[ $reg\\_case2$],[ $reg\\_case3$]]$, where each $reg\\_case$ is itself $[y_1,y_2,\\dots,y_K]$ and each $y_k$ is a list of floats of length $N_{\\mathrm{vol}}$. The printed string must contain no spaces anywhere.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in established principles of EEG-fMRI integration, well-posed with a complete and consistent set of definitions and parameters, and expressed in objective, formal language. The task is to construct fMRI General Linear Model (GLM) regressors modulated by EEG microstate dynamics, a recognized a-priori approach in neuroimaging data analysis. The problem is a non-trivial, multi-step computational task that is fully specified and computationally feasible. We will proceed with a detailed solution.\n\nThe solution is implemented by following the sequence of steps specified in the problem statement.\n\n**1. Hemodynamic Response Function (HRF) Generation**\nThe canonical double-gamma Hemodynamic Response Function, $h(t)$, is defined for $t \\ge 0$ as the difference of two Gamma probability density functions. The Gamma PDF is given by:\n$$\n\\mathrm{GammaPDF}(t;\\alpha,\\beta) = \\frac{t^{\\alpha-1}\\, e^{-t/\\beta}}{\\beta^{\\alpha}\\, \\Gamma(\\alpha)}\n$$\nwhere $\\Gamma(\\alpha)$ is the Gamma function. The HRF is then:\n$$\nh(t) = \\mathrm{GammaPDF}(t;\\alpha_1,\\beta_1) - c \\cdot \\mathrm{GammaPDF}(t;\\alpha_2,\\beta_2)\n$$\nThe parameters are provided as $\\alpha_1 = 6$, $\\beta_1 = 1$, $\\alpha_2 = 16$, $\\beta_2 = 1$, and $c = 1/6$.\n\nThis continuous function is sampled at the EEG sampling period, $dt = 1/f_{\\mathrm{EEG}}$, over a duration of $L_{\\mathrm{HRF}} = 32$ s. The resulting discrete time series, let's call it $h_{samp}$, is then normalized to have a unit area, which in the discrete domain corresponds to ensuring its Riemann sum approximates an integral of $1$:\n$$\n\\sum_{i} h_{samp}[i] \\cdot dt = 1\n$$\nThis is achieved by dividing the sampled vector $h_{samp}$ by the sum of its elements multiplied by $dt$. The resulting vector $h$ is used as the convolution kernel.\n\n**2. Template Pre-processing and EEG Data Generation**\nThe provided microstate template maps $G \\in \\mathbb{R}^{K \\times M}$ are first pre-processed. For each raw template vector $g_{k,raw}$, we compute its mean-centered version $g'_k$ and then normalize it to have a unit $\\ell_2$ norm.\n$$\ng'_k = g_{k,raw} - \\text{mean}(g_{k,raw})\n$$\n$$\ng_k = \\frac{g'_k}{\\|g'_k\\|_2}\n$$\nThese processed templates $g_k$ are used for two purposes: generating the synthetic EEG data and for the subsequent correlation analysis.\n\nThe EEG data matrix $X \\in \\mathbb{R}^{T \\times M}$ is synthesized based on a piecewise-constant microstate sequence. For each time sample $t$, if it falls within a segment assigned to microstate $k$, the corresponding EEG scalp map $x_t$ is generated by taking the processed template $g_k$ and adding independent Gaussian noise:\n$$\nx_t = g_k + \\epsilon_t \\quad \\text{where} \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2 I_M)\n$$\nA specified random seed ensures reproducibility.\n\n**3. Microstate Assignment**\nThe generated EEG time series is labeled by assigning each time point $t$ to one of the $K$ microstates. This is achieved by finding the template map $g_k$ that has the highest spatial correlation with the EEG map $x_t$. The spatial correlation $r_k(t)$ is calculated as the cosine similarity between the mean-centered EEG map $\\tilde{x}_t$ and the mean-centered template map $\\tilde{g}_k$.\n$$\n\\tilde{x}_t = x_t - \\text{mean}(x_t) \\quad , \\quad \\tilde{g}_k = g_k - \\text{mean}(g_k)\n$$\nAs the templates $g_k$ were already mean-centered during pre-processing, $\\tilde{g}_k = g_k$. The correlation is:\n$$\nr_k(t) = \\frac{\\langle \\tilde{x}_t, \\tilde{g}_k \\rangle}{\\|\\tilde{x}_t\\|_2 \\, \\|\\tilde{g}_k\\|_2}\n$$\nThe microstate label for time $t$, denoted $\\hat{z}(t)$, is the index of the template with the maximum correlation:\n$$\n\\hat{z}(t) = \\underset{k \\in \\{1,\\dots,K\\}}{\\arg\\max}\\; r_k(t)\n$$\n\n**4. Microstate-Specific Event Function Construction**\nFirst, an impulse train $e(t)$ is created on the EEG time grid to represent the timing of experimental events. For each event time $\\tau_j$ (in seconds), an impulse is placed at the corresponding discrete time index $t_j = \\lfloor \\tau_j / dt \\rfloor$. If an index falls outside the valid range $[0, T-1]$, it is clamped to the nearest boundary.\n$$\ne(t) = \\begin{cases} 1 & \\text{if } t = \\min(\\lfloor \\tau_j / dt \\rfloor, T-1) \\text{ for some } j \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nNext, for each microstate $k$, an indicator time series $s_k(t)$ is defined, which is $1$ if the assigned label at time $t$ is $k$, and $0$ otherwise:\n$$\ns_k(t) = \\mathbb{I}\\big[\\hat{z}(t) = k\\big]\n$$\nThe microstate-specific event function $u_k(t)$ is then the element-wise product of the indicator series and the event impulse train. This function marks only those events that occurred while microstate $k$ was active.\n$$\nu_k(t) = s_k(t) \\cdot e(t)\n$$\n\n**5. Convolution and Downsampling**\nEach microstate-specific event function $u_k(t)$ is convolved with the normalized HRF kernel $h$ to model the expected BOLD signal. A 'full' convolution is used:\n$$\nc_k = u_k * h\n$$\nThe resulting continuous-time signal proxies, $c_k(t)$, are sampled at the fMRI acquisition times. These times are given by $t_n = n \\cdot TR$ for each fMRI volume $n \\in \\{0, 1, \\dots, N_{\\mathrm{vol}}-1\\}$. The sampling is performed by taking the value of the convolved signal at the nearest-lower index on the EEG time grid:\n$$\ny'_k[n] = c_k[\\lfloor n \\cdot TR / dt \\rfloor]\n$$\nAn index clamping rule is applied: if $\\lfloor n \\cdot TR / dt \\rfloor$ exceeds the length of the convolved signal $c_k$, the last value of $c_k$ is used.\n\n**6. Final Regressor Normalization**\nAs a final step, each sampled regressor vector $y'_k$ is normalized to have a unit $\\ell_2$ norm. This is a common practice in GLM analysis to ensure that the scale of the regressor does not bias the estimation of its corresponding beta coefficient. If a regressor vector is all zeros (i.e., its norm is zero), it remains unchanged.\n$$\ny_k = \\begin{cases} \\frac{y'_k}{\\|y'_k\\|_2} & \\text{if } \\|y'_k\\|_2 > 0 \\\\ y'_k & \\text{if } \\|y'_k\\|_2 = 0 \\end{cases}\n$$\nThese vectors $y_k$ are the final fMRI regressors. The process is repeated for each test case, and the results are aggregated.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma as gamma_func\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"M\": 4, \"K\": 4, \"f_EEG\": 250, \"TR\": 1, \"N_vol\": 4, \"T\": 1000,\n            \"events\": [0.5, 1.5, 2.5, 3.5],\n            \"templates_raw\": np.array([\n                [1, -1, 1, -1], [1, 1, -1, -1],\n                [1, -1, -1, 1], [-1, 1, -1, 1]\n            ], dtype=float),\n            \"eeg_gen\": {\n                \"segments\": [\n                    (0, 249, 0), (250, 499, 1),\n                    (500, 749, 2), (750, 999, 3)\n                ],\n                \"sigma\": 0.05,\n                \"seed\": 42\n            }\n        },\n        {\n            \"M\": 3, \"K\": 3, \"f_EEG\": 100, \"TR\": 0.6, \"N_vol\": 3, \"T\": 120,\n            \"events\": [0.2, 0.8, 1.1],\n            \"templates_raw\": np.array([\n                [1, -1, 0], [-1, 1, 0], [0, 1, -1]\n            ], dtype=float),\n            \"eeg_gen\": {\n                \"segments\": [\n                    (0, 39, 0), (40, 79, 1), (80, 119, 2)\n                ],\n                \"sigma\": 0.05,\n                \"seed\": 7\n            }\n        },\n        {\n            \"M\": 2, \"K\": 2, \"f_EEG\": 200, \"TR\": 0.75, \"N_vol\": 4, \"T\": 600,\n            \"events\": [0.0, 1.5, 3.0],\n            \"templates_raw\": np.array([\n                [1, -1], [-1, 1]\n            ], dtype=float),\n            \"eeg_gen\": {\n                \"segments\": [\n                    (0, 299, 0), (300, 599, 1)\n                ],\n                \"sigma\": 0.02,\n                \"seed\": 123\n            }\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        case_results = run_case(case)\n        all_results.append(case_results)\n    \n    # Custom formatter to produce a JSON-like string with no spaces\n    def format_no_space(obj):\n        if isinstance(obj, list) or isinstance(obj, tuple):\n            return '[' + ','.join(format_no_space(item) for item in obj) + ']'\n        elif isinstance(obj, np.ndarray):\n            return format_no_space(obj.tolist())\n        else:\n            return str(obj)\n\n    print(format_no_space(all_results))\n\ndef run_case(params):\n    \"\"\"\n    Processes a single test case.\n    \"\"\"\n    M, K, f_EEG, TR, N_vol, T = params[\"M\"], params[\"K\"], params[\"f_EEG\"], params[\"TR\"], params[\"N_vol\"], params[\"T\"]\n    event_times = params[\"events\"]\n    templates_raw = params[\"templates_raw\"]\n    eeg_gen = params[\"eeg_gen\"]\n    \n    dt = 1.0 / f_EEG\n\n    # 1. HRF Generation\n    def gamma_pdf(t, alpha, beta):\n        return (t**(alpha - 1) * np.exp(-t / beta)) / (beta**alpha * gamma_func(alpha))\n\n    L_HRF = 32\n    hrf_t = np.arange(0, L_HRF + dt, dt)\n    h_sampled = gamma_pdf(hrf_t, 6, 1) - (1/6) * gamma_pdf(hrf_t, 16, 1)\n    \n    h_norm_factor = np.sum(h_sampled) * dt\n    h = h_sampled / h_norm_factor if h_norm_factor != 0 else h_sampled\n\n    # 2. Template Pre-processing\n    templates = np.zeros_like(templates_raw)\n    for i in range(K):\n        g_raw = templates_raw[i]\n        g_mean_centered = g_raw - np.mean(g_raw)\n        norm = np.linalg.norm(g_mean_centered)\n        templates[i] = g_mean_centered / norm if norm > 0 else g_mean_centered\n\n    # 3. EEG Data Generation\n    np.random.seed(eeg_gen[\"seed\"])\n    X = np.zeros((T, M))\n    for start, end, k_idx in eeg_gen[\"segments\"]:\n        num_samples = end - start + 1\n        noise = np.random.normal(0, eeg_gen[\"sigma\"], size=(num_samples, M))\n        X[start:end+1, :] = templates[k_idx] + noise\n\n    # 4. Microstate Assignment\n    z_hat = np.zeros(T, dtype=int)\n    g_tilde = templates # templates are already mean-centered and norm=1\n    g_tilde_norms = np.linalg.norm(g_tilde, axis=1)\n\n    for t in range(T):\n        x_t = X[t, :]\n        x_t_tilde = x_t - np.mean(x_t)\n        x_t_tilde_norm = np.linalg.norm(x_t_tilde)\n        \n        if x_t_tilde_norm == 0:\n            z_hat[t] = 0 # Default assignment if signal is flat\n            continue\n\n        corrs = np.dot(g_tilde, x_t_tilde) / (g_tilde_norms * x_t_tilde_norm)\n        z_hat[t] = np.argmax(corrs)\n\n    # 5. Microstate-Specific Event Functions\n    e = np.zeros(T)\n    for tau in event_times:\n        t_j = int(np.floor(tau / dt))\n        t_j = min(t_j, T - 1)\n        e[t_j] = 1\n\n    u = np.zeros((K, T))\n    for k in range(K):\n        s_k = (z_hat == k)\n        u[k, :] = s_k * e\n\n    # 6. Convolution and Downsampling\n    final_regressors = []\n    for k in range(K):\n        u_k = u[k, :]\n        \n        if np.sum(u_k) == 0:\n            final_regressors.append(np.zeros(N_vol).tolist())\n            continue\n\n        c_k = np.convolve(u_k, h, mode='full')\n        \n        y_k = np.zeros(N_vol)\n        for n in range(N_vol):\n            t_n = n * TR\n            idx = int(np.floor(t_n / dt))\n            # Clamp index to valid range for convolved signal\n            idx = min(idx, len(c_k) - 1)\n            y_k[n] = c_k[idx]\n\n        # 7. Final Normalization\n        norm_y_k = np.linalg.norm(y_k)\n        if norm_y_k > 0:\n            y_k /= norm_y_k\n        \n        final_regressors.append(y_k.tolist())\n        \n    return final_regressors\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Beyond using one modality to constrain the other, we can seek to uncover shared neurophysiological processes through symmetric, data-driven fusion. This practice focuses on the mathematical derivation of regularized Canonical Correlation Analysis (CCA), a powerful technique for finding maximally correlated components between EEG and fMRI feature sets. By working through the derivation from first principles, you will master the core optimization problem that underpins many advanced fusion methods and understand how to identify shared patterns of variance across modalities. ",
            "id": "4179362",
            "problem": "Consider Electroencephalography (EEG) and Functional Magnetic Resonance Imaging (fMRI) recordings collected simultaneously over $T$ time points from the same participant during a cognitive task. Let the EEG feature matrix be $X \\in \\mathbb{R}^{T \\times p}$ (rows are time points, columns are features such as band-limited source-space power) and the fMRI feature matrix be $Y \\in \\mathbb{R}^{T \\times q}$ (rows are time points, columns are features such as regional principal component scores). Assume both $X$ and $Y$ have been mean-centered across time so that each column has zero temporal mean. Define the empirical covariance matrices\n$$\nS_{xx} \\equiv \\frac{1}{T} X^{\\top} X \\in \\mathbb{R}^{p \\times p}, \\quad\nS_{yy} \\equiv \\frac{1}{T} Y^{\\top} Y \\in \\mathbb{R}^{q \\times q}, \\quad\nS_{xy} \\equiv \\frac{1}{T} X^{\\top} Y \\in \\mathbb{R}^{p \\times q}.\n$$\nTo address potential ill-conditioning common in high-dimensional EEG-fMRI fusion, consider ridge-regularized covariances\n$$\nS_{xx,\\lambda} \\equiv S_{xx} + \\lambda_{x} I_{p}, \\quad\nS_{yy,\\lambda} \\equiv S_{yy} + \\lambda_{y} I_{q},\n$$\nwith regularization parameters $\\lambda_{x} > 0$ and $\\lambda_{y} > 0$, and $I_{p}$, $I_{q}$ denoting identity matrices of sizes $p$ and $q$, respectively. These ensure $S_{xx,\\lambda}$ and $S_{yy,\\lambda}$ are symmetric positive definite.\n\nDefine projected latent time courses $z_{x} \\equiv X a$ and $z_{y} \\equiv Y b$ for weight vectors $a \\in \\mathbb{R}^{p}$ and $b \\in \\mathbb{R}^{q}$. Starting only from the definition of Pearson correlation between $z_{x}$ and $z_{y}$ and the above covariance definitions, derive the constrained optimization problem whose objective maximizes the correlation between $z_{x}$ and $z_{y}$ under unit-variance constraints for $z_{x}$ and $z_{y}$, using the regularized covariances. Then, solve this optimization problem to obtain the closed-form analytic expressions for the first pair of canonical weight vectors $a^{\\star}$ and $b^{\\star}$ in terms of a spectral decomposition of an appropriately whitened cross-covariance matrix.\n\nYour final answer must be a single closed-form analytic expression for the pair $(a^{\\star}, b^{\\star})$ in terms of $S_{xx,\\lambda}$, $S_{yy,\\lambda}$, $S_{xy}$, and the singular vectors of a whitened cross-covariance. No numerical computation is required. If you introduce any matrix square roots or inverses, clearly state the principal branch and assume all required matrices are invertible due to $\\lambda_{x} > 0$ and $\\lambda_{y} > 0$. Do not use any shortcut formulas; derive from first principles and recognized definitions. The final boxed expression must not include any units.",
            "solution": "The task is to derive and solve the optimization problem for finding the first pair of canonical weight vectors, $a^{\\star}$ and $b^{\\star}$, that maximize the correlation between two projected time series, $z_{x} = Xa$ and $z_{y} = Yb$, under specific regularized variance constraints. The derivation must start from first principles.\n\nFirst, we formulate the objective function, which is the Pearson correlation between $z_{x}$ and $z_{y}$. The Pearson correlation coefficient $\\rho$ is defined as:\n$$\n\\rho(z_x, z_y) = \\frac{\\text{cov}(z_x, z_y)}{\\sqrt{\\text{var}(z_x) \\text{var}(z_y)}}\n$$\nThe problem states that the data matrices $X \\in \\mathbb{R}^{T \\times p}$ and $Y \\in \\mathbb{R}^{T \\times q}$ are mean-centered, meaning each column has a temporal mean of zero. Consequently, the projected time courses $z_{x} = Xa$ and $z_{y} = Yb$ are also mean-centered. Using the provided empirical covariance definitions, we can express the terms in the correlation formula.\n\nThe covariance between $z_x$ and $z_y$ is:\n$$\n\\text{cov}(z_x, z_y) = \\frac{1}{T} z_x^{\\top} z_y = \\frac{1}{T} (Xa)^{\\top}(Yb) = \\frac{1}{T} a^{\\top}X^{\\top}Yb = a^{\\top} \\left( \\frac{1}{T} X^{\\top}Y \\right) b = a^{\\top}S_{xy}b\n$$\nThe variance of $z_x$ is:\n$$\n\\text{var}(z_x) = \\frac{1}{T} z_x^{\\top} z_x = \\frac{1}{T} (Xa)^{\\top}(Xa) = \\frac{1}{T} a^{\\top}X^{\\top}Xa = a^{\\top} \\left( \\frac{1}{T} X^{\\top}X \\right) a = a^{\\top}S_{xx}a\n$$\nSimilarly, the variance of $z_y$ is:\n$$\n\\text{var}(z_y) = \\frac{1}{T} z_y^{\\top} z_y = \\frac{1}{T} (Yb)^{\\top}(Yb) = \\frac{1}{T} b^{\\top}Y^{\\top}Yb = b^{\\top} \\left( \\frac{1}{T} Y^{\\top}Y \\right) b = b^{\\top}S_{yy}b\n$$\nThe problem specifies unit-variance constraints for $z_x$ and $z_y$ using the *regularized* covariances. This implies the constraints are not $\\text{var}(z_x) = 1$ and $\\text{var}(z_y) = 1$, but rather are based on the regularized matrices $S_{xx,\\lambda} \\equiv S_{xx} + \\lambda_{x} I_{p}$ and $S_{yy,\\lambda} \\equiv S_{yy} + \\lambda_{y} I_{q}$. The constraints are thus:\n$$\na^{\\top}S_{xx,\\lambda}a = a^{\\top}(S_{xx} + \\lambda_{x} I_{p})a = 1\n$$\n$$\nb^{\\top}S_{yy,\\lambda}b = b^{\\top}(S_{yy} + \\lambda_{y} I_{q})b = 1\n$$\nMaximizing the correlation $\\rho$ is equivalent to maximizing its numerator, $a^{\\top}S_{xy}b$, while the denominator is fixed to $1$ by the constraints. Therefore, the constrained optimization problem is:\n$$\n\\underset{a, b}{\\text{maximize}} \\quad a^{\\top}S_{xy}b\n$$\n$$\n\\text{subject to} \\quad a^{\\top}S_{xx,\\lambda}a = 1 \\quad \\text{and} \\quad b^{\\top}S_{yy,\\lambda}b = 1\n$$\nThis is the first part of the problem. To solve it, we use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(a, b, \\mu_x, \\mu_y) = a^{\\top}S_{xy}b - \\frac{\\mu_x}{2}(a^{\\top}S_{xx,\\lambda}a - 1) - \\frac{\\mu_y}{2}(b^{\\top}S_{yy,\\lambda}b - 1)\n$$\nwhere $\\frac{\\mu_x}{2}$ and $\\frac{\\mu_y}{2}$ are the Lagrange multipliers. Taking the partial derivatives with respect to $a$ and $b$ and setting them to zero yields the stationarity conditions:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a} = S_{xy}b - \\mu_x S_{xx,\\lambda}a = 0 \\quad \\implies \\quad S_{xy}b = \\mu_x S_{xx,\\lambda}a \\quad (1)\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b} = S_{xy}^{\\top}a - \\mu_y S_{yy,\\lambda}b = 0 \\quad \\implies \\quad S_{xy}^{\\top}a = \\mu_y S_{yy,\\lambda}b \\quad (2)\n$$\nLeft-multiplying equation $(1)$ by $a^{\\top}$ gives $a^{\\top}S_{xy}b = \\mu_x a^{\\top}S_{xx,\\lambda}a$. Using the constraint $a^{\\top}S_{xx,\\lambda}a = 1$, we find $a^{\\top}S_{xy}b = \\mu_x$.\nLeft-multiplying equation $(2)$ by $b^{\\top}$ gives $b^{\\top}S_{xy}^{\\top}a = \\mu_y b^{\\top}S_{yy,\\lambda}b$. Using the constraint $b^{\\top}S_{yy,\\lambda}b = 1$, we find $b^{\\top}S_{xy}^{\\top}a = \\mu_y$.\nSince $b^{\\top}S_{xy}^{\\top}a = (a^{\\top}S_{xy}b)^{\\top} = a^{\\top}S_{xy}b$ (as it is a scalar), we must have $\\mu_x = \\mu_y$. Let us denote this common multiplier by $\\mu$. The value $\\mu = a^{\\top}S_{xy}b$ is the correlation we aim to maximize. The problem reduces to finding the largest $\\mu$ that satisfies the system of equations.\nThe system becomes:\n$$\nS_{xy}b = \\mu S_{xx,\\lambda}a \\quad (3)\n$$\n$$\nS_{xy}^{\\top}a = \\mu S_{yy,\\lambda}b \\quad (4)\n$$\nThis is a generalized eigenvalue problem. To solve it in the form requested, we introduce a change of variables that \"whitens\" the constraints. Since $\\lambda_x > 0$ and $\\lambda_y > 0$, and $S_{xx}, S_{yy}$ are positive semi-definite, the regularized matrices $S_{xx,\\lambda}$ and $S_{yy,\\lambda}$ are symmetric positive definite (SPD). An SPD matrix $M$ has a unique SPD square root, denoted $M^{1/2}$, which is the principal branch. Their inverses $M^{-1}$ and $M^{-1/2} \\equiv (M^{1/2})^{-1}$ exist.\n\nWe define whitened weight vectors $\\tilde{a} \\in \\mathbb{R}^{p}$ and $\\tilde{b} \\in \\mathbb{R}^{q}$ as:\n$$\na = S_{xx,\\lambda}^{-1/2} \\tilde{a} \\quad \\text{and} \\quad b = S_{yy,\\lambda}^{-1/2} \\tilde{b}\n$$\nSubstituting these into the constraints simplifies them greatly:\n$$\na^{\\top}S_{xx,\\lambda}a = (S_{xx,\\lambda}^{-1/2}\\tilde{a})^{\\top} S_{xx,\\lambda} (S_{xx,\\lambda}^{-1/2}\\tilde{a}) = \\tilde{a}^{\\top}S_{xx,\\lambda}^{-1/2}S_{xx,\\lambda}S_{xx,\\lambda}^{-1/2}\\tilde{a} = \\tilde{a}^{\\top}I_p\\tilde{a} = \\tilde{a}^{\\top}\\tilde{a} = 1\n$$\n$$\nb^{\\top}S_{yy,\\lambda}b = (S_{yy,\\lambda}^{-1/2}\\tilde{b})^{\\top} S_{yy,\\lambda} (S_{yy,\\lambda}^{-1/2}\\tilde{b}) = \\tilde{b}^{\\top}S_{yy,\\lambda}^{-1/2}S_{yy,\\lambda}S_{yy,\\lambda}^{-1/2}\\tilde{b} = \\tilde{b}^{\\top}I_q\\tilde{b} = \\tilde{b}^{\\top}\\tilde{b} = 1\n$$\nThe objective function becomes:\n$$\na^{\\top}S_{xy}b = (S_{xx,\\lambda}^{-1/2}\\tilde{a})^{\\top} S_{xy} (S_{yy,\\lambda}^{-1/2}\\tilde{b}) = \\tilde{a}^{\\top} (S_{xx,\\lambda}^{-1/2} S_{xy} S_{yy,\\lambda}^{-1/2}) \\tilde{b}\n$$\nLet us define the whitened cross-covariance matrix $M \\in \\mathbb{R}^{p \\times q}$ as:\n$$\nM \\equiv S_{xx,\\lambda}^{-1/2} S_{xy} S_{yy,\\lambda}^{-1/2}\n$$\nThe optimization problem is transformed into finding $\\tilde{a}$ and $\\tilde{b}$ that solve:\n$$\n\\underset{\\tilde{a}, \\tilde{b}}{\\text{maximize}} \\quad \\tilde{a}^{\\top}M\\tilde{b}\n$$\n$$\n\\text{subject to} \\quad \\tilde{a}^{\\top}\\tilde{a} = 1 \\quad \\text{and} \\quad \\tilde{b}^{\\top}\\tilde{b} = 1\n$$\nThis is a standard problem whose solution is given by the singular value decomposition (SVD) of $M$. Let the SVD of $M$ be $M = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{p \\times p}$ and $V \\in \\mathbb{R}^{q \\times q}$ are orthogonal matrices whose columns are the left and right singular vectors, respectively, and $\\Sigma \\in \\mathbb{R}^{p \\times q}$ is a rectangular diagonal matrix of singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq 0$.\n\nThe objective is $\\tilde{a}^{\\top}U \\Sigma V^{\\top}\\tilde{b}$. The solution is found by choosing $\\tilde{a}$ to be the first left singular vector of $M$, denoted $u_1$ (the first column of $U$), and $\\tilde{b}$ to be the first right singular vector of $M$, denoted $v_1$ (the first column of $V$). This choice yields the maximum possible value for the objective, which is the largest singular value, $\\sigma_1$. Thus, the optimal whitened vectors are $\\tilde{a}^{\\star} = u_1$ and $\\tilde{b}^{\\star} = v_1$.\n\nFinally, we transform back to the original coordinate system to find the optimal weight vectors $a^{\\star}$ and $b^{\\star}$:\n$$\na^{\\star} = S_{xx,\\lambda}^{-1/2} \\tilde{a}^{\\star} = S_{xx,\\lambda}^{-1/2} u_1\n$$\n$$\nb^{\\star} = S_{yy,\\lambda}^{-1/2} \\tilde{b}^{\\star} = S_{yy,\\lambda}^{-1/2} v_1\n$$\nwhere $u_1$ and $v_1$ are the left and right singular vectors, respectively, corresponding to the largest singular value of the whitened cross-covariance matrix $M = S_{xx,\\lambda}^{-1/2} S_{xy} S_{yy,\\lambda}^{-1/2}$. The matrices $S_{xx,\\lambda}^{-1/2}$ and $S_{yy,\\lambda}^{-1/2}$ are the inverse principal square roots of the regularized covariance matrices. This provides the closed-form analytic expression for the first pair of canonical weight vectors.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} S_{xx,\\lambda}^{-1/2} u_1 & S_{yy,\\lambda}^{-1/2} v_1 \\end{pmatrix}}\n$$"
        }
    ]
}