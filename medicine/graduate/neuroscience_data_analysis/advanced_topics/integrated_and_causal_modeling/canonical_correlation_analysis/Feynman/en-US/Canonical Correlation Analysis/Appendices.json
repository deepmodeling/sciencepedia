{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Canonical Correlation Analysis (CCA), it is invaluable to connect its abstract definition as a constrained optimization problem to the concrete mechanics of its solution. This exercise guides you through the fundamental derivation, starting from the goal of maximizing the correlation between two projected datasets. By applying the method of Lagrange multipliers to a small, well-defined problem, you will see firsthand how the search for optimal projection vectors transforms into a standard eigenvalue problem, providing a solid mathematical foundation for all subsequent applications. ",
            "id": "4322614",
            "problem": "In systems biomedicine, integrating complementary molecular views (for example, messenger ribonucleic acid (mRNA) gene expression and metabolite abundances) can reveal coordinated biological variation across subjects. Consider two centered data views with $p=2$ gene expression variables collected alongside $q=2$ metabolite variables across a cohort. Assume the sample covariance between the gene expression view is given by\n$$\n\\Sigma_{XX} = \\begin{pmatrix}\n1 & 0.3 \\\\\n0.3 & 1.2\n\\end{pmatrix},\n$$\nthe sample covariance between the metabolite view is given by\n$$\n\\Sigma_{YY} = \\begin{pmatrix}\n1.5 & 0.4 \\\\\n0.4 & 0.8\n\\end{pmatrix},\n$$\nand the cross-covariance is given by\n$$\n\\Sigma_{XY} = \\begin{pmatrix}\n0.7 & 0.2 \\\\\n0.1 & 0.5\n\\end{pmatrix}.\n$$\nCanonical Correlation Analysis (CCA) seeks linear projections $a \\in \\mathbb{R}^{2}$ and $b \\in \\mathbb{R}^{2}$ that maximize the covariance of projected views subject to unit variance constraints in their respective covariance geometries. Formally, solve the constrained maximization problem\n$$\n\\max_{a \\in \\mathbb{R}^{2},\\, b \\in \\mathbb{R}^{2}} \\; a^{\\top}\\Sigma_{XY} b \\quad \\text{subject to} \\quad a^{\\top}\\Sigma_{XX} a = 1,\\; b^{\\top}\\Sigma_{YY} b = 1.\n$$\nDerive the first canonical correlation and associated canonical directions by starting from the fundamental definition above, formulating the necessary optimality conditions, and reducing the problem to an eigenvalue problem that you solve explicitly for the provided matrices. Present the numerical value of the first canonical correlation rounded to four significant figures. Express the final numerical answer as a pure decimal (no units). Only the numerical value of the first canonical correlation should appear in your final answer box.",
            "solution": "The problem is to find the first canonical correlation and associated canonical directions for two data views, $X$ and $Y$, with given sample covariance matrices $\\Sigma_{XX}$ and $\\Sigma_{YY}$, and a cross-covariance matrix $\\Sigma_{XY}$. The problem is formulated as a constrained maximization problem:\n$$ \\max_{a \\in \\mathbb{R}^{2},\\, b \\in \\mathbb{R}^{2}} \\; a^{\\top}\\Sigma_{XY} b \\quad \\text{subject to} \\quad a^{\\top}\\Sigma_{XX} a = 1,\\; b^{\\top}\\Sigma_{YY} b = 1. $$\nThe quantity $a^{\\top}\\Sigma_{XY} b$ represents the covariance between the projected variables $X a$ and $Y b$. The constraints normalize the variances of these projected variables to $1$. The quantity being maximized is therefore the correlation between the canonical variates, $\\text{corr}(a^{\\top}X, b^{\\top}Y)$.\n\nTo solve this constrained optimization problem, we use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$ \\mathcal{L}(a, b, \\lambda_a, \\lambda_b) = a^{\\top}\\Sigma_{XY} b - \\frac{\\lambda_a}{2}(a^{\\top}\\Sigma_{XX} a - 1) - \\frac{\\lambda_b}{2}(b^{\\top}\\Sigma_{YY} b - 1) $$\nwhere $\\frac{\\lambda_a}{2}$ and $\\frac{\\lambda_b}{2}$ are Lagrange multipliers (the factor of $\\frac{1}{2}$ is for algebraic convenience).\n\nTo find the optimal $a$ and $b$, we take the partial derivatives of $\\mathcal{L}$ with respect to $a$ and $b$ and set them to zero.\nUsing the identities $\\frac{\\partial}{\\partial v}(u^{\\top}Av) = A^{\\top}u$ and $\\frac{\\partial}{\\partial v}(v^{\\top}Bv) = 2Bv$ for a symmetric matrix $B$:\n\n1.  Derivative with respect to $a^{\\top}$:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial a} = \\Sigma_{XY} b - \\lambda_a \\Sigma_{XX} a = 0 \\implies \\Sigma_{XY} b = \\lambda_a \\Sigma_{XX} a $$\n\n2.  Derivative with respect to $b^{\\top}$:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\Sigma_{XY}^{\\top} a - \\lambda_b \\Sigma_{YY} b = 0 \\implies \\Sigma_{XY}^{\\top} a = \\lambda_b \\Sigma_{YY} b $$\n\nNow, let's determine the meaning of the Lagrange multipliers. Left-multiply the first equation by $a^{\\top}$:\n$$ a^{\\top}\\Sigma_{XY} b = \\lambda_a a^{\\top}\\Sigma_{XX} a $$\nUsing the constraint $a^{\\top}\\Sigma_{XX} a = 1$, we find that $\\lambda_a = a^{\\top}\\Sigma_{XY} b$. This is the quantity we want to maximize.\n\nSimilarly, left-multiply the second equation by $b^{\\top}$:\n$$ b^{\\top}\\Sigma_{XY}^{\\top} a = \\lambda_b b^{\\top}\\Sigma_{YY} b $$\nUsing the constraint $b^{\\top}\\Sigma_{YY} b = 1$, we find that $\\lambda_b = b^{\\top}\\Sigma_{XY}^{\\top} a$.\nSince $a^{\\top}\\Sigma_{XY} b$ is a scalar, it is equal to its transpose, $(a^{\\top}\\Sigma_{XY} b)^{\\top} = b^{\\top}\\Sigma_{XY}^{\\top} a$. Therefore, $\\lambda_a = \\lambda_b$. Let us denote this common value by $\\rho$, which is the canonical correlation.\n\nThe optimality conditions become a system of two coupled equations:\n$$ \\Sigma_{XY} b = \\rho \\Sigma_{XX} a \\quad (1) $$\n$$ \\Sigma_{XY}^{\\top} a = \\rho \\Sigma_{YY} b \\quad (2) $$\n\nTo solve this system, we can reduce it to an eigenvalue problem. The covariance matrices $\\Sigma_{XX}$ and $\\Sigma_{YY}$ are given as symmetric and are positive definite (as their determinants are $1.11 > 0$ and $1.04 > 0$ with positive diagonal entries), so their inverses $\\Sigma_{XX}^{-1}$ and $\\Sigma_{YY}^{-1}$ exist.\n\nFrom equation (1), we can express $a$ in terms of $b$ (assuming $\\rho \\neq 0$):\n$$ a = \\frac{1}{\\rho} \\Sigma_{XX}^{-1} \\Sigma_{XY} b $$\nSubstituting this expression for $a$ into equation (2):\n$$ \\Sigma_{XY}^{\\top} \\left( \\frac{1}{\\rho} \\Sigma_{XX}^{-1} \\Sigma_{XY} b \\right) = \\rho \\Sigma_{YY} b $$\n$$ \\frac{1}{\\rho} \\Sigma_{XY}^{\\top} \\Sigma_{XX}^{-1} \\Sigma_{XY} b = \\rho \\Sigma_{YY} b $$\nMultiplying by $\\rho$ and then by $\\Sigma_{YY}^{-1}$ from the left gives:\n$$ \\Sigma_{YY}^{-1} \\Sigma_{XY}^{\\top} \\Sigma_{XX}^{-1} \\Sigma_{XY} b = \\rho^2 b $$\nThis is a standard eigenvalue problem of the form $K b = \\lambda b$, where the matrix is $K = \\Sigma_{YY}^{-1} \\Sigma_{XY}^{\\top} \\Sigma_{XX}^{-1} \\Sigma_{XY}$ and the eigenvalues are $\\lambda = \\rho^2$, the squared canonical correlations. The eigenvectors $b$ are the canonical direction vectors for the $Y$ view.\n\nWe now perform the explicit calculations with the given matrices:\n$$ \\Sigma_{XX} = \\begin{pmatrix} 1 & 0.3 \\\\ 0.3 & 1.2 \\end{pmatrix}, \\quad \\Sigma_{YY} = \\begin{pmatrix} 1.5 & 0.4 \\\\ 0.4 & 0.8 \\end{pmatrix}, \\quad \\Sigma_{XY} = \\begin{pmatrix} 0.7 & 0.2 \\\\ 0.1 & 0.5 \\end{pmatrix} $$\nFirst, we compute the inverse matrices:\n$$ \\det(\\Sigma_{XX}) = 1(1.2) - (0.3)^2 = 1.11 \\implies \\Sigma_{XX}^{-1} = \\frac{1}{1.11} \\begin{pmatrix} 1.2 & -0.3 \\\\ -0.3 & 1 \\end{pmatrix} $$\n$$ \\det(\\Sigma_{YY}) = 1.5(0.8) - (0.4)^2 = 1.04 \\implies \\Sigma_{YY}^{-1} = \\frac{1}{1.04} \\begin{pmatrix} 0.8 & -0.4 \\\\ -0.4 & 1.5 \\end{pmatrix} $$\nNow we construct the matrix $K$ step-by-step.\nLet's compute $M_1 = \\Sigma_{XX}^{-1} \\Sigma_{XY}$:\n$$ M_1 = \\frac{1}{1.11} \\begin{pmatrix} 1.2 & -0.3 \\\\ -0.3 & 1 \\end{pmatrix} \\begin{pmatrix} 0.7 & 0.2 \\\\ 0.1 & 0.5 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.84 - 0.03 & 0.24 - 0.15 \\\\ -0.21 + 0.1 & -0.06 + 0.5 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.81 & 0.09 \\\\ -0.11 & 0.44 \\end{pmatrix} $$\nNext, we compute $M_2 = \\Sigma_{XY}^{\\top} M_1$:\n$$ M_2 = \\begin{pmatrix} 0.7 & 0.1 \\\\ 0.2 & 0.5 \\end{pmatrix} \\frac{1}{1.11} \\begin{pmatrix} 0.81 & 0.09 \\\\ -0.11 & 0.44 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.567 - 0.011 & 0.063 + 0.044 \\\\ 0.162 - 0.055 & 0.018 + 0.22 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.556 & 0.107 \\\\ 0.107 & 0.238 \\end{pmatrix} $$\nFinally, we compute $K = \\Sigma_{YY}^{-1} M_2$:\n$$ K = \\frac{1}{1.04} \\begin{pmatrix} 0.8 & -0.4 \\\\ -0.4 & 1.5 \\end{pmatrix} \\frac{1}{1.11} \\begin{pmatrix} 0.556 & 0.107 \\\\ 0.107 & 0.238 \\end{pmatrix} $$\n$$ K = \\frac{1}{1.1544} \\begin{pmatrix} 0.8(0.556) - 0.4(0.107) & 0.8(0.107) - 0.4(0.238) \\\\ -0.4(0.556) + 1.5(0.107) & -0.4(0.107) + 1.5(0.238) \\end{pmatrix} $$\n$$ K = \\frac{1}{1.1544} \\begin{pmatrix} 0.4448 - 0.0428 & 0.0856 - 0.0952 \\\\ -0.2224 + 0.1605 & -0.0428 + 0.357 \\end{pmatrix} = \\frac{1}{1.1544} \\begin{pmatrix} 0.402 & -0.0096 \\\\ -0.0619 & 0.3142 \\end{pmatrix} $$\n$$ K \\approx \\begin{pmatrix} 0.348233 & -0.008316 \\\\ -0.053612 & 0.272176 \\end{pmatrix} $$\nThe eigenvalues $\\lambda=\\rho^2$ of $K$ are found by solving the characteristic equation $\\det(K-\\lambda I)=0$:\n$$ (0.348233 - \\lambda)(0.272176 - \\lambda) - (-0.008316)(-0.053612) = 0 $$\n$$ \\lambda^2 - (\\text{tr}(K))\\lambda + \\det(K) = 0 $$\n$$ \\text{tr}(K) \\approx 0.348233 + 0.272176 = 0.620409 $$\n$$ \\det(K) \\approx (0.348233)(0.272176) - (0.0004458) \\approx 0.094781 - 0.000446 = 0.094335 $$\nThe characteristic equation is approximately:\n$$ \\lambda^2 - 0.620409 \\lambda + 0.094335 = 0 $$\nSolving this quadratic equation for $\\lambda$:\n$$ \\lambda = \\frac{0.620409 \\pm \\sqrt{(0.620409)^2 - 4(0.094335)}}{2} = \\frac{0.620409 \\pm \\sqrt{0.384907 - 0.37734}}{2} $$\n$$ \\lambda = \\frac{0.620409 \\pm \\sqrt{0.007567}}{2} = \\frac{0.620409 \\pm 0.086988}{2} $$\nThe two eigenvalues are:\n$$ \\lambda_1 = \\frac{0.620409 + 0.086988}{2} = \\frac{0.707397}{2} \\approx 0.35370 $$\n$$ \\lambda_2 = \\frac{0.620409 - 0.086988}{2} = \\frac{0.533421}{2} \\approx 0.26671 $$\nThe goal is to maximize the correlation, so we take the largest eigenvalue, $\\lambda_1$. The first canonical correlation $\\rho_1$ is its square root:\n$$ \\rho_1 = \\sqrt{\\lambda_1} \\approx \\sqrt{0.35370} \\approx 0.594726 $$\nThe problem requests the value rounded to four significant figures.\n$$ \\rho_1 \\approx 0.5947 $$\nThe associated canonical direction $b_1$ is the eigenvector of $K$ for $\\lambda_1$.\nThe direction vector $a_1$ is then found via $a_1 = \\frac{1}{\\rho_1} \\Sigma_{XX}^{-1} \\Sigma_{XY} b_1$, where $b_1$ and $a_1$ are scaled to satisfy the unit variance constraints. The explicit computation of these vectors, while part of a full analysis, is secondary to finding the correlation value itself as per the problem's final answer requirement.",
            "answer": "$$\\boxed{0.5947}$$"
        },
        {
            "introduction": "Before applying any statistical model, a crucial first step is to diagnose your data. In CCA, the relationship between the number of samples ($n$) and the number of features in each dataset ($p$ and $q$) has profound implications for the analysis. This practice explores how data dimensions dictate the rank of the covariance matrices, which in turn determines the feasibility of the classical CCA algorithm and the maximum number of canonical dimensions you can extract. This hands-on coding exercise will equip you with the essential skill of assessing your data's structure to anticipate analytical challenges, particularly the need for regularization or alternative solution methods in high-dimensional scenarios. ",
            "id": "4144760",
            "problem": "You are analyzing two multivariate datasets in the context of Canonical Correlation Analysis (CCA) applied to neuroscience data, where one dataset represents neural features and the other represents behavioral or stimulus features. Let the neural dataset be an $n \\times p$ matrix $X$ and the behavioral dataset be an $n \\times q$ matrix $Y$, where $n$ is the number of observations (for example, trials), $p$ is the number of neural features, and $q$ is the number of behavioral features. Both $X$ and $Y$ are column-wise standardized (z-scored), meaning each column has zero mean and unit variance.\n\nStarting from the foundational definitions:\n- The sample covariance matrices for centered data are defined as\n$$\nS_{xx} = \\frac{1}{n-1} X^\\top X, \\quad S_{yy} = \\frac{1}{n-1} Y^\\top Y, \\quad S_{xy} = \\frac{1}{n-1} X^\\top Y,\n$$\nwhich rely on the well-tested principles of empirical estimation of covariances from centered data.\n- For any centered $n \\times p$ matrix, the rank of its sample covariance matrix is bounded above by $\\min(p, n-1)$. For the cross-covariance, the rank is bounded above by $\\min(p, q, n-1)$.\n- In Canonical Correlation Analysis (CCA), whether the classical generalized eigenvalue formulation is feasible depends on the invertibility of $S_{xx}$ and $S_{yy}$, which in turn depends on whether these matrices are full rank. If $S_{xx}$ and $S_{yy}$ are not full rank, one must use regularization or a singular value decomposition (SVD) based approach.\n\nYour task is to write a complete, runnable program that, for a specified test suite, constructs z-scored matrices $X$ and $Y$, computes $S_{xx}$, $S_{yy}$, and $S_{xy}$, and reports:\n1. The dimensions of $S_{xx}$, $S_{yy}$, and $S_{xy}$.\n2. The numerically estimated ranks of $S_{xx}$, $S_{yy}$, and $S_{xy}$ using a singular value threshold based on machine precision.\n3. The theoretical upper bounds on the ranks: $\\min(p, n-1)$ for $S_{xx}$, $\\min(q, n-1)$ for $S_{yy}$, and $\\min(p, q, n-1)$ for $S_{xy}$.\n4. A boolean indicating whether the classical generalized eigenvalue CCA is feasible, defined as whether $S_{xx}$ and $S_{yy}$ are both full rank (i.e., $p$ and $q$, respectively).\n5. The maximum number of canonical dimensions available from the data, defined as $\\min(\\operatorname{rank}(S_{xx}), \\operatorname{rank}(S_{yy}))$.\n\nTo ensure scientific realism and reproducibility, use the following test suite. For each case, generate $X$ and $Y$ as independent standard normal matrices with a fixed random seed, and then z-score each column by subtracting the empirical mean and dividing by the empirical standard deviation. In cases where collinearity is specified, construct $Y$ by creating exact duplicate columns from a subset of its base columns to reduce rank.\n\nTest Suite:\n- Case $1$: $n=200$, $p=50$, $q=60$, seed $0$, no induced collinearity.\n- Case $2$: $n=40$, $p=50$, $q=10$, seed $1$, no induced collinearity.\n- Case $3$: $n=120$, $p=80$, $q=90$, seed $2$, induce $30$ duplicate columns in $Y$ (so $Y$ is constructed by first making a base matrix of size $n \\times (q-30)$ and then appending $30$ columns that are exact duplicates of randomly selected base columns).\n- Case $4$: $n=51$, $p=50$, $q=10$, seed $3$, no induced collinearity.\n\nNumerical rank estimation rule:\n- For a matrix $M$, compute its singular values $\\sigma_i$ via singular value decomposition. Let $\\varepsilon$ be machine precision for double-precision floating point numbers. Define the threshold as\n$$\n\\tau = \\max(\\text{number of rows of } M, \\text{number of columns of } M) \\cdot \\varepsilon \\cdot \\max_i \\sigma_i.\n$$\nThe numerical rank is the count of singular values greater than $\\tau$.\n\nImplications for computing canonical vectors:\n- Report a boolean that is $true$ if and only if $S_{xx}$ has rank $p$ and $S_{yy}$ has rank $q$, indicating that the classical generalized eigenvalue formulation of CCA can be directly applied. Otherwise, report $false$, indicating that regularization or SVD-based CCA is required.\n- Report the maximum number of canonical dimensions as the integer $\\min(\\operatorname{rank}(S_{xx}), \\operatorname{rank}(S_{yy}))$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list containing the following elements in order:\n$$\n[\\; p,\\; q,\\; n,\\; \\text{dim}(S_{xx})\\_1,\\; \\text{dim}(S_{xx})\\_2,\\; \\text{dim}(S_{yy})\\_1,\\; \\text{dim}(S_{yy})\\_2,\\; \\text{dim}(S_{xy})\\_1,\\; \\text{dim}(S_{xy})\\_2,\\; \\operatorname{rank}(S_{xx}),\\; \\operatorname{rank}(S_{yy}),\\; \\operatorname{rank}(S_{xy}),\\; \\min(p, n-1),\\; \\min(q, n-1),\\; \\min(p, q, n-1),\\; \\text{eigen\\_CCA\\_possible},\\; \\text{max\\_canonical\\_dims}\\; ].\n$$\nFor example, the overall output should look like:\n$$\n[\\;[\\text{case1\\_values}],\\;[\\text{case2\\_values}],\\;[\\text{case3\\_values}],\\;[\\text{case4\\_values}]\\;]\n$$\nAngles and physical units do not apply. All outputs are integers, booleans, or lists of these types. The program must be self-contained and require no user input, external files, or network access.",
            "solution": "The problem requires an analysis of the properties of sample covariance matrices derived from two multivariate datasets, denoted by matrices $X$ and $Y$, and the implications of these properties for Canonical Correlation Analysis (CCA). The analysis will be conducted for a specified test suite, examining matrix dimensions, numerical ranks, theoretical rank bounds, and the feasibility of classical CCA.\n\nFirst, we establish the procedure for data generation and preprocessing. For each test case, we are given the number of observations $n$, the number of features in the first dataset $p$, and the number of features in the second dataset $q$. We generate the initial data matrices $X$ and $Y$ by drawing from a standard normal distribution. This is achieved using a seeded pseudo-random number generator for reproducibility. The problem specifies that the matrices should be column-wise standardized (z-scored). For an arbitrary data matrix $M$, each column $m_j$ is transformed into a standardized column $z_j$ according to the formula:\n$$\nz_j = \\frac{m_j - \\bar{m}_j}{s_j}\n$$\nwhere $\\bar{m}_j$ is the sample mean of the column and $s_j$ is its sample standard deviation. The sample standard deviation is calculated with $n-1$ in the denominator to be consistent with the definition of the sample covariance matrix. This standardization ensures each column of the resulting matrices $X$ and $Y$ has a mean of $0$ and a sample variance of $1$.\n\nNext, we compute the sample covariance matrices. Since the data matrices $X$ and $Y$ are centered (zero mean columns), the sample covariance matrices are given by the expressions:\n$$\nS_{xx} = \\frac{1}{n-1} X^\\top X\n$$\n$$\nS_{yy} = \\frac{1}{n-1} Y^\\top Y\n$$\n$$\nS_{xy} = \\frac{1}{n-1} X^\\top Y\n$$\nThe dimensions of these matrices are determined by the dimensions of $X$ ($n \\times p$) and $Y$ ($n \\times q$). $S_{xx}$ is a $p \\times p$ matrix, $S_{yy}$ is a $q \\times q$ matrix, and the cross-covariance matrix $S_{xy}$ is a $p \\times q$ matrix.\n\nA central part of this analysis is the determination of matrix rank. We consider both theoretical bounds and numerical estimates.\nThe rank of a sample covariance matrix is limited by the dimensions of the data from which it is derived. After centering the $n$ observation vectors (the rows of $X$), they reside in a subspace of dimension at most $n-1$. The rank of the matrix $X$ is therefore bounded by $\\min(p, n-1)$. Since $\\operatorname{rank}(S_{xx}) = \\operatorname{rank}(X^\\top X) = \\operatorname{rank}(X)$, the theoretical upper bound on the rank of $S_{xx}$ is:\n$$\n\\operatorname{rank}(S_{xx}) \\le \\min(p, n-1)\n$$\nSimilarly, for $S_{yy}$:\n$$\n\\operatorname{rank}(S_{yy}) \\le \\min(q, n-1)\n$$\nFor the cross-covariance matrix $S_{xy}$, its rank is bounded by the minimum of the ranks of $X$ and $Y$. This leads to the theoretical bound:\n$$\n\\operatorname{rank}(S_{xy}) \\le \\min(\\operatorname{rank}(X), \\operatorname{rank}(Y)) \\le \\min(\\min(p, n-1), \\min(q, n-1)) = \\min(p, q, n-1)\n$$\nIn practice, due to finite-precision arithmetic, we must estimate rank numerically. The prescribed method involves Singular Value Decomposition (SVD). For any matrix $M$, we compute its singular values $\\sigma_i$, ordered from largest to smallest. A threshold $\\tau$ is defined as:\n$$\n\\tau = \\max(\\text{rows of } M, \\text{columns of } M) \\cdot \\varepsilon \\cdot \\sigma_1\n$$\nwhere $\\varepsilon$ is the machine precision for the floating-point type and $\\sigma_1$ is the largest singular value. The numerical rank is the count of singular values that are strictly greater than this threshold $\\tau$. This robustly differentiates significant singular values from those that are numerically indistinguishable from zero.\n\nFinally, we assess the implications for CCA. The classical formulation of CCA solves a generalized eigenvalue problem, for instance, $(S_{xx}^{-1} S_{xy} S_{yy}^{-1} S_{yx}) w_x = \\rho^2 w_x$. This formulation critically requires that the matrices $S_{xx}$ and $S_{yy}$ be invertible. A square matrix is invertible if and only if it has full rank. Therefore, the feasibility of classical, direct eigenvalue-based CCA is true if and only if $\\operatorname{rank}(S_{xx}) = p$ and $\\operatorname{rank}(S_{yy}) = q$. If this condition is not met (e.g., in high-dimensional settings where $p > n-1$ or $q > n-1$, or in cases of multicollinearity), alternative methods like regularized CCA or SVD-based CCA must be employed. The maximum number of canonical variate pairs (and thus, non-zero canonical correlations) that can be extracted from the data is limited by the \"information content\" of each dataset, which is captured by the rank of their respective covariance matrices. This number is thus given by $\\min(\\operatorname{rank}(S_{xx}), \\operatorname{rank}(S_{yy}))$.\n\nWe will now apply this entire procedure to each case in the test suite.\n\nCase 1: $n=200$, $p=50$, $q=60$. Since $p < n-1$ and $q < n-1$, we expect $S_{xx}$ and $S_{yy}$ to be full rank, i.e., $\\operatorname{rank}(S_{xx})=50$ and $\\operatorname{rank}(S_{yy})=60$. Classical CCA should be feasible. The maximum number of canonical dimensions should be $\\min(50, 60)=50$.\n\nCase 2: $n=40$, $p=50$, $q=10$. Here, $p > n-1$ ($50 > 39$). Thus, $S_{xx}$ must be rank-deficient. Its rank will be at most $n-1=39$. $S_{yy}$ should be full rank, as $q < n-1$ ($10 < 39$). Since $S_{xx}$ is not full rank, classical CCA is not feasible. The maximum number of canonical dimensions is $\\min(\\operatorname{rank}(S_{xx}), \\operatorname{rank}(S_{yy})) = \\min(39, 10)=10$.\n\nCase 3: $n=120$, $p=80$, $q=90$, with induced collinearity in $Y$. $Y$ is constructed to have $30$ duplicate columns, so it has only $90-30=60$ linearly independent columns. While the theoretical bound for $\\operatorname{rank}(S_{yy})$ is $\\min(90, 119)=90$, the explicit construction limits the actual rank to $60$. $S_{xx}$ should be full rank ($80$). Because $\\operatorname{rank}(S_{yy}) = 60 \\neq q=90$, classical CCA is not feasible. The maximum number of canonical dimensions is $\\min(\\operatorname{rank}(S_{xx}), \\operatorname{rank}(S_{yy})) = \\min(80, 60)=60$.\n\nCase 4: $n=51$, $p=50$, $q=10$. This is a boundary case where $p = n-1 = 50$. The $50$ columns of the centered matrix $X$ are drawn from a continuous distribution and lie in a subspace of dimension $50$. They will be linearly independent with probability $1$. Thus, we expect $\\operatorname{rank}(S_{xx}) = 50 = p$. For $Y$, $q < n-1$, so we expect $\\operatorname{rank}(S_{yy})=10=q$. Both matrices should be full rank, making classical CCA feasible. Maximum dimensions: $\\min(50, 10)=10$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n\n    def zscore(matrix):\n        \"\"\"\n        Standardizes a matrix column-wise (z-scoring).\n        Uses sample standard deviation (ddof=1) for consistency with the\n        sample covariance matrix definition.\n        \"\"\"\n        mean = matrix.mean(axis=0)\n        # Use ddof=1 for sample standard deviation\n        std = matrix.std(axis=0, ddof=1)\n        # Handle columns with zero standard deviation to avoid division by zero\n        std[std == 0] = 1.0\n        return (matrix - mean) / std\n\n    def compute_numerical_rank(matrix):\n        \"\"\"\n        Computes the numerical rank of a matrix based on its singular values.\n        \"\"\"\n        if matrix.size == 0:\n            return 0\n            \n        sv = np.linalg.svd(matrix, compute_uv=False)\n        \n        # The largest singular value is the first element\n        sigma_max = sv[0] if sv.size > 0 else 0\n        if sigma_max == 0:\n            return 0\n            \n        # Machine precision for the matrix's data type\n        eps = np.finfo(matrix.dtype).eps\n        \n        # Threshold for considering a singular value as non-zero\n        threshold = max(matrix.shape) * eps * sigma_max\n        \n        # Count singular values greater than the threshold\n        rank = np.sum(sv > threshold)\n        return int(rank)\n\n    def run_case(n, p, q, seed, num_collinear=0):\n        \"\"\"\n        Processes a single test case.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate data matrices\n        X_raw = rng.standard_normal(size=(n, p))\n        \n        if num_collinear > 0:\n            q_base = q - num_collinear\n            Y_base = rng.standard_normal(size=(n, q_base))\n            # Randomly select columns from the base to duplicate\n            duplicate_indices = rng.choice(q_base, size=num_collinear, replace=True)\n            Y_duplicates = Y_base[:, duplicate_indices]\n            Y_raw = np.hstack((Y_base, Y_duplicates))\n        else:\n            Y_raw = rng.standard_normal(size=(n, q))\n\n        # 2. Z-score the matrices\n        X = zscore(X_raw)\n        Y = zscore(Y_raw)\n\n        # 3. Compute sample covariance matrices\n        S_xx = (X.T @ X) / (n - 1)\n        S_yy = (Y.T @ Y) / (n - 1)\n        S_xy = (X.T @ Y) / (n - 1)\n\n        # 4. Report dimensions\n        dim_sxx = S_xx.shape\n        dim_syy = S_yy.shape\n        dim_sxy = S_xy.shape\n\n        # 5. Estimate numerical ranks\n        rank_sxx = compute_numerical_rank(S_xx)\n        rank_syy = compute_numerical_rank(S_yy)\n        rank_sxy = compute_numerical_rank(S_xy)\n\n        # 6. Compute theoretical rank bounds\n        bound_sxx = min(p, n - 1)\n        bound_syy = min(q, n - 1)\n        bound_sxy = min(p, q, n - 1)\n\n        # 7. Determine CCA feasibility and max dimensions\n        eigen_cca_possible = (rank_sxx == p) and (rank_syy == q)\n        max_canonical_dims = min(rank_sxx, rank_syy)\n\n        return [\n            p, q, n,\n            dim_sxx[0], dim_sxx[1],\n            dim_syy[0], dim_syy[1],\n            dim_sxy[0], dim_sxy[1],\n            rank_sxx, rank_syy, rank_sxy,\n            bound_sxx, bound_syy, bound_sxy,\n            eigen_cca_possible,\n            max_canonical_dims\n        ]\n\n    test_cases = [\n        {'n': 200, 'p': 50, 'q': 60, 'seed': 0, 'num_collinear': 0},\n        {'n': 40, 'p': 50, 'q': 10, 'seed': 1, 'num_collinear': 0},\n        {'n': 120, 'p': 80, 'q': 90, 'seed': 2, 'num_collinear': 30},\n        {'n': 51, 'p': 50, 'q': 10, 'seed': 3, 'num_collinear': 0},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_case(**case)\n        all_results.append(str(result).replace(\" \", \"\"))\n\n    # Print the final output in the required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having established that the classical generalized eigenvalue formulation of CCA is not always feasible—especially with high-dimensional or collinear data—we now turn to a more robust and numerically stable implementation. This practice introduces a powerful solution based on Singular Value Decomposition (SVD). By first \"whitening\" the data spaces using the pseudo-inverse square roots of the covariance matrices, the problem of finding canonical correlations elegantly simplifies to computing the singular values of a transformed cross-covariance matrix. This exercise provides practical experience with an industry-standard algorithm that gracefully handles the ill-conditioned or rank-deficient cases often encountered in real-world neuroscience data. ",
            "id": "4144749",
            "problem": "Consider two multivariate neural measurement spaces with dimensions $p$ and $q$, and three numeric matrices: a symmetric covariance-like matrix $S_{xx} \\in \\mathbb{R}^{p \\times p}$, a symmetric covariance-like matrix $S_{yy} \\in \\mathbb{R}^{q \\times q}$, and a cross-covariance-like matrix $S_{xy} \\in \\mathbb{R}^{p \\times q}$. Assume all matrices are real-valued and represent well-posed second-order statistics from jointly observed random vectors, as is standard in Canonical Correlation Analysis (CCA). The goal is to compute the canonical correlation values by first constructing inverse square-root transforms of $S_{xx}$ and $S_{yy}$ via eigendecomposition and then analyzing the singular values of a whitened cross-covariance.\n\nFundamental base and definitions:\n- The covariance matrix of a zero-mean random vector $x$ is defined as $S_{xx} = \\mathbb{E}[x x^\\top]$, where $\\mathbb{E}[\\cdot]$ denotes expectation. Similarly, for a zero-mean random vector $y$ measured simultaneously, $S_{yy} = \\mathbb{E}[y y^\\top]$, and the cross-covariance is $S_{xy} = \\mathbb{E}[x y^\\top]$.\n- A real symmetric matrix admits an eigendecomposition $S = Q \\Lambda Q^\\top$ with $Q$ orthonormal and $\\Lambda$ diagonal. A pseudo-inverse square-root of $S$ is constructed by inverting the square roots of the positive eigenvalues and setting contributions from nonpositive eigenvalues to zero.\n- The whitened cross-covariance is the product $C = S_{xx}^{-1/2} S_{xy} S_{yy}^{-1/2}$.\n- The Singular Value Decomposition (SVD) of a real matrix $C$ yields nonnegative singular values.\n\nAlgorithmic requirements:\n- Compute $S_{xx}^{-1/2}$ and $S_{yy}^{-1/2}$ via eigendecomposition $S = Q \\Lambda Q^\\top$. Use a numerical threshold $\\tau = 10^{-10}$: for each eigenvalue $\\lambda_i$, set its inverse square-root weight to $1/\\sqrt{\\lambda_i}$ if $\\lambda_i > \\tau$, and to $0$ otherwise. Construct $S^{-1/2} = Q \\operatorname{diag}(w_i) Q^\\top$, where $w_i$ are the weights.\n- Form $C = S_{xx}^{-1/2} S_{xy} S_{yy}^{-1/2}$ and compute its singular values.\n- Let $m = \\min\\left(p, q, \\operatorname{rank}(S_{xy})\\right)$, where $\\operatorname{rank}(S_{xy})$ is computed as the number of singular values of $S_{xy}$ strictly greater than $\\tau$.\n- Return the first $m$ singular values of $C$ in descending order. Round each value to $6$ decimal places and express as decimal floats.\n\nTest suite:\nFor each case below, $p$ and $q$ are implied by the matrix shapes.\n\nCase $1$ (general well-conditioned):\n$$\nS_{xx} = \\begin{bmatrix}\n2.0 & 0.5 & 0.0 \\\\\n0.5 & 1.5 & 0.1 \\\\\n0.0 & 0.1 & 1.2\n\\end{bmatrix},\\quad\nS_{yy} = \\begin{bmatrix}\n1.3 & 0.2 \\\\\n0.2 & 1.1\n\\end{bmatrix},\\quad\nS_{xy} = \\begin{bmatrix}\n0.5 & 0.1 \\\\\n0.3 & 0.4 \\\\\n0.2 & 0.6\n\\end{bmatrix}.\n$$\n\nCase $2$ (no cross-covariance signal):\n$$\nS_{xx} = \\begin{bmatrix}\n1.0 & 0.1 \\\\\n0.1 & 1.5\n\\end{bmatrix},\\quad\nS_{yy} = \\begin{bmatrix}\n1.2 & 0.0 & 0.1 \\\\\n0.0 & 1.1 & 0.0 \\\\\n0.1 & 0.0 & 1.3\n\\end{bmatrix},\\quad\nS_{xy} = \\begin{bmatrix}\n0.0 & 0.0 & 0.0 \\\\\n0.0 & 0.0 & 0.0\n\\end{bmatrix}.\n$$\n\nCase $3$ (rank-deficient cross-covariance):\n$$\nS_{xx} = \\begin{bmatrix}\n1.0 & 0.2 \\\\\n0.2 & 2.0\n\\end{bmatrix},\\quad\nS_{yy} = \\begin{bmatrix}\n1.5 & 0.1 \\\\\n0.1 & 1.4\n\\end{bmatrix},\\quad\nS_{xy} = \\begin{bmatrix}\n1.0 & 2.0 \\\\\n0.5 & 1.0\n\\end{bmatrix}.\n$$\n\nCase $4$ (ill-conditioned $S_{xx}$ handled by pseudo-inverse square-root):\n$$\nS_{xx} = \\begin{bmatrix}\n1.0 & 0.0 & 0.0 \\\\\n0.0 & 0.5 & 0.0 \\\\\n0.0 & 0.0 & 10^{-8}\n\\end{bmatrix},\\quad\nS_{yy} = \\begin{bmatrix}\n1.2 & 0.0 & 0.0 \\\\\n0.0 & 0.8 & 0.0 \\\\\n0.0 & 0.0 & 0.9\n\\end{bmatrix},\\quad\nS_{xy} = \\begin{bmatrix}\n0.3 & 0.0 & 0.2 \\\\\n0.1 & 0.4 & 0.0 \\\\\n0.5 & 0.2 & 0.3\n\\end{bmatrix}.\n$$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to a test case and is itself a list of floats in descending order, rounded to $6$ decimal places as specified. For example, the output should look like $[ [\\dots], [\\dots], [\\dots], [\\dots] ]$ but printed with no spaces as $[[\\dots],[\\dots],[\\dots],[\\dots]]$.",
            "solution": "The user has provided a computational problem in the domain of multivariate statistics, specifically Canonical Correlation Analysis (CCA), often applied in neuroscience data analysis. I will first validate the problem statement for its scientific validity, consistency, and completeness before proceeding to a solution.\n\n### Step 1: Extract Givens\n\nThe problem provides the following data, definitions, and requirements:\n- **Data**: Two multivariate measurement spaces of dimensions $p$ and $q$. Three real-valued matrices are given for each test case: a symmetric matrix $S_{xx} \\in \\mathbb{R}^{p \\times p}$, a symmetric matrix $S_{yy} \\in \\mathbb{R}^{q \\times q}$, and a cross-covariance matrix $S_{xy} \\in \\mathbb{R}^{p \\times q}$.\n- **Definitions**:\n    - For zero-mean random vectors $x$ and $y$, the covariance matrices are $S_{xx} = \\mathbb{E}[x x^\\top]$ and $S_{yy} = \\mathbb{E}[y y^\\top]$, and the cross-covariance is $S_{xy} = \\mathbb{E}[x y^\\top]$.\n    - The eigendecomposition of a real symmetric matrix $S$ is $S = Q \\Lambda Q^\\top$.\n    - The pseudo-inverse square-root of $S$, denoted $S^{-1/2}$, is constructed as $S^{-1/2} = Q \\operatorname{diag}(w_i) Q^\\top$. The weights $w_i$ are defined based on the eigenvalues $\\lambda_i$ of $S$ and a threshold $\\tau = 10^{-10}$: $w_i = 1/\\sqrt{\\lambda_i}$ if $\\lambda_i > \\tau$, and $w_i = 0$ otherwise.\n    - The whitened cross-covariance matrix is $C = S_{xx}^{-1/2} S_{xy} S_{yy}^{-1/2}$.\n    - The canonical correlations are the singular values of $C$.\n- **Algorithmic Requirements**:\n    1.  Compute $S_{xx}^{-1/2}$ and $S_{yy}^{-1/2}$ using the specified pseudo-inverse square-root method.\n    2.  Construct the matrix $C$.\n    3.  Compute the singular values of $C$.\n    4.  Determine the number of values to return, $m = \\min\\left(p, q, \\operatorname{rank}(S_{xy})\\right)$. The rank of $S_{xy}$ is defined as the number of its singular values that are strictly greater than $\\tau = 10^{-10}$.\n    5.  Return the first $m$ singular values of $C$ sorted in descending order, with each value rounded to $6$ decimal places.\n- **Test Suite**: Four specific cases of matrices $S_{xx}$, $S_{yy}$, and $S_{xy}$ are provided.\n\n### Step 2: Validate Using Extracted Givens\n\nI will now assess the problem's validity based on the extracted information.\n\n- **Scientifically Grounded**: The problem is based on Canonical Correlation Analysis (CCA), a cornerstone of multivariate statistical analysis. The method described—computing canonical correlations as the singular values of the whitened cross-covariance matrix—is a standard and computationally stable approach. The use of eigendecomposition to find the matrix square root and a regularization threshold $\\tau$ to construct a pseudo-inverse square-root are well-established numerical techniques for handling potentially singular or ill-conditioned covariance matrices. The entire formulation is rooted in linear algebra and statistics.\n- **Well-Posed**: The problem is well-posed. All required inputs are specified. The algorithmic steps are deterministic and unambiguous, leading to a unique solution for each test case. The definition of $m$ ensures that the number of returned values is well-defined and corresponds to the effective rank of the relationship between the two variable sets.\n- **Objective**: The problem is stated in precise, objective mathematical language, free of any subjectivity or ambiguity.\n- **Flaw Checklist**:\n    1.  **Scientific/Factual Unsoundness**: None. The principles of CCA and the associated linear algebra are sound.\n    2.  **Non-Formalizable/Irrelevant**: None. The problem is a formal request to implement a specific statistical algorithm relevant to the specified domain.\n    3.  **Incomplete/Contradictory Setup**: None. All necessary matrices, parameters ($\\tau$), and procedures are explicitly defined.\n    4.  **Unrealistic/Infeasible**: None. The matrices provided are small-dimensional representations of covariance structures, and the test cases are designed to probe different valid scenarios (well-conditioned, rank-deficient, zero-covariance, ill-conditioned), which are all plausible in real data.\n    5.  **Ill-Posed/Poorly Structured**: None. The problem is clearly structured and leads to a unique, stable solution.\n    6.  **Pseudo-Profound/Trivial**: None. The problem requires a correct implementation of a non-trivial statistical method, including numerical stability considerations.\n    7.  **Outside Scientific Verifiability**: None. The results are mathematically derivable and verifiable.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a clear, consistent, and scientifically sound request to implement a standard statistical procedure. I will now provide a complete solution.\n\n### Solution\n\nThe objective is to compute the canonical correlations between two sets of variables, represented by their covariance structures $S_{xx}$, $S_{yy}$, and $S_{xy}$. The canonical correlations measure the linear correlation between linear combinations of the variables in the first set and linear combinations of the variables in the second set. The standard method, which we will follow, involves transforming the problem into finding the singular values of a \"whitened\" cross-covariance matrix.\n\nThe process comprises four main steps:\n1.  **Constructing the Whitening Transformations**: The transformations that decorrelate the variables within each set are given by the inverse square-roots of their respective covariance matrices, $S_{xx}^{-1/2}$ and $S_{yy}^{-1/2}$. To handle cases where $S_{xx}$ or $S_{yy}$ may be singular (i.e., not invertible), we construct a regularized pseudo-inverse square-root. For a given symmetric matrix $S \\in \\mathbb{R}^{k \\times k}$, this is done via its eigendecomposition, $S = Q \\Lambda Q^\\top$, where $Q$ is an orthonormal matrix of eigenvectors and $\\Lambda$ is a diagonal matrix of the corresponding eigenvalues $\\{\\lambda_i\\}_{i=1}^k$. The pseudo-inverse square-root, $S^{-1/2}$, is then:\n    $$ S^{-1/2} = Q W Q^\\top $$\n    where $W$ is a diagonal matrix with entries $w_i$ calculated as:\n    $$\n    w_i =\n    \\begin{cases}\n    1 / \\sqrt{\\lambda_i} & \\text{if } \\lambda_i > \\tau \\\\\n    0 & \\text{if } \\lambda_i \\le \\tau\n    \\end{cases}\n    $$\n    with the numerical stability threshold $\\tau = 10^{-10}$. This procedure is applied to both $S_{xx}$ and $S_{yy}$.\n\n2.  **Whitening the Cross-Covariance**: We apply the whitening transformations to the cross-covariance matrix $S_{xy}$. The resulting whitened cross-covariance matrix, $C$, is given by:\n    $$ C = S_{xx}^{-1/2} S_{xy} S_{yy}^{-1/2} $$\n    The matrix $C$ represents the covariance between the whitened variables of the first set and the whitened variables of the second set. The singular values of this matrix $C$ are precisely the canonical correlations.\n\n3.  **Determining the Number of Meaningful Correlations**: The number of non-zero canonical correlations is limited by the dimensions of the datasets and the rank of the cross-covariance matrix. The problem defines this number, $m$, as:\n    $$ m = \\min\\left(p, q, \\operatorname{rank}(S_{xy})\\right) $$\n    where $p$ and $q$ are the dimensions of $S_{xx}$ and $S_{yy}$ respectively. The rank of $S_{xy}$ is calculated numerically as the number of its singular values that are greater than the threshold $\\tau$.\n\n4.  **Computing Canonical Correlations**: The canonical correlations are the singular values of the matrix $C$. We compute the Singular Value Decomposition (SVD) of $C$. The resulting singular values are non-negative and, by convention, returned in descending order. We then select the first $m$ of these singular values as our result.\n\nThis complete procedure will be applied to each of the four test cases provided. The final numerical results, rounded to $6$ decimal places, will be presented in the specified format. The implementation will use `numpy.linalg.eigh` for the eigendecomposition of the symmetric matrices $S_{xx}$ and $S_{yy}$, and `numpy.linalg.svd` for computing singular values for rank determination and for finding the final canonical correlations.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes canonical correlations for a series of test cases based on a\n    specified algorithm involving pseudo-inverse square-roots and SVD.\n    \"\"\"\n    \n    # Numerical threshold for regularization and rank calculation.\n    TAU = 1e-10\n\n    test_cases = [\n        {\n            \"Sxx\": np.array([\n                [2.0, 0.5, 0.0],\n                [0.5, 1.5, 0.1],\n                [0.0, 0.1, 1.2]\n            ]),\n            \"Syy\": np.array([\n                [1.3, 0.2],\n                [0.2, 1.1]\n            ]),\n            \"Sxy\": np.array([\n                [0.5, 0.1],\n                [0.3, 0.4],\n                [0.2, 0.6]\n            ])\n        },\n        {\n            \"Sxx\": np.array([\n                [1.0, 0.1],\n                [0.1, 1.5]\n            ]),\n            \"Syy\": np.array([\n                [1.2, 0.0, 0.1],\n                [0.0, 1.1, 0.0],\n                [0.1, 0.0, 1.3]\n            ]),\n            \"Sxy\": np.array([\n                [0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0]\n            ])\n        },\n        {\n            \"Sxx\": np.array([\n                [1.0, 0.2],\n                [0.2, 2.0]\n            ]),\n            \"Syy\": np.array([\n                [1.5, 0.1],\n                [0.1, 1.4]\n            ]),\n            \"Sxy\": np.array([\n                [1.0, 2.0],\n                [0.5, 1.0]\n            ])\n        },\n        {\n            \"Sxx\": np.array([\n                [1.0, 0.0, 0.0],\n                [0.0, 0.5, 0.0],\n                [0.0, 0.0, 1e-8]\n            ]),\n            \"Syy\": np.array([\n                [1.2, 0.0, 0.0],\n                [0.0, 0.8, 0.0],\n                [0.0, 0.0, 0.9]\n            ]),\n            \"Sxy\": np.array([\n                [0.3, 0.0, 0.2],\n                [0.1, 0.4, 0.0],\n                [0.5, 0.2, 0.3]\n            ])\n        }\n    ]\n\n    def compute_pseudo_inv_sqrt(S, tau):\n        \"\"\"\n        Computes the pseudo-inverse square-root of a symmetric matrix S\n        using eigendecomposition and a regularization threshold.\n        \"\"\"\n        # eigh is for hermitian (symmetric real) matrices.\n        # It returns eigenvalues in ascending order.\n        eigvals, eigvecs = np.linalg.eigh(S)\n        \n        # Apply threshold to compute inverse square-root of eigenvalues.\n        inv_sqrt_eigvals = np.where(eigvals > tau, 1.0 / np.sqrt(eigvals), 0)\n        \n        # Reconstruct the matrix S^{-1/2} = Q W Q^T\n        S_inv_sqrt = eigvecs @ np.diag(inv_sqrt_eigvals) @ eigvecs.T\n        return S_inv_sqrt\n\n    results = []\n    for case in test_cases:\n        Sxx = case[\"Sxx\"]\n        Syy = case[\"Syy\"]\n        Sxy = case[\"Sxy\"]\n\n        p = Sxx.shape[0]\n        q = Syy.shape[0]\n\n        # Step 1: Compute pseudo-inverse square-roots\n        Sxx_inv_sqrt = compute_pseudo_inv_sqrt(Sxx, TAU)\n        Syy_inv_sqrt = compute_pseudo_inv_sqrt(Syy, TAU)\n\n        # Step 2: Form the whitened cross-covariance matrix C\n        C = Sxx_inv_sqrt @ Sxy @ Syy_inv_sqrt\n\n        # Step 3: Determine the number of correlations to return, m\n        # compute_uv=False is more efficient as we only need the singular values.\n        s_Sxy = np.linalg.svd(Sxy, compute_uv=False)\n        rank_Sxy = np.sum(s_Sxy > TAU)\n        m = min(p, q, rank_Sxy)\n\n        # Step 4: Compute the canonical correlations (singular values of C)\n        # SVD returns singular values in descending order.\n        canonical_correlations = np.linalg.svd(C, compute_uv=False)\n\n        # Select the top m correlations and round them\n        final_values = [round(val, 6) for val in canonical_correlations[:m]]\n        results.append(final_values)\n    \n    # Format the final output string to be without spaces\n    # Example: '[[val1,val2],[val3]]'\n    string_results = [str(res_list).replace(\" \", \"\") for res_list in results]\n    final_output = f\"[{','.join(string_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}