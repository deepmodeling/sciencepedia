{
    "hands_on_practices": [
        {
            "introduction": "The largest Lyapunov exponent, $\\lambda$, quantifies the rate of exponential divergence and fundamentally has units of inverse time (e.g., $\\mathrm{s}^{-1}$). This physical meaning can be obscured when estimating $\\lambda$ from discrete time series, where it is often first computed in dimensionless \"per sample\" units. This practice  challenges you to connect these two perspectives by deriving, from first principles, how the exponent transforms under a uniform rescaling of time and how to properly convert between discrete-time and continuous-time estimates. Mastering this conversion is essential for correctly interpreting and reporting the timescale of chaotic dynamics discovered in experimental data.",
            "id": "4182450",
            "problem": "A cortical population signal is recorded as a scalar time series from a microelectrode array and used to reconstruct a state space via delay-coordinate embedding. To quantify sensitivity to initial conditions, the largest Lyapunov exponent is estimated from the short-time growth of separations between nearby reconstructed trajectories. Specifically, for small integer lags $k$ (in samples), the ensemble-averaged logarithmic separation behaves approximately linearly as $\\ln\\!\\big(d(k)/d(0)\\big) \\approx \\mu\\,k$, where $d(k)$ is the mean separation at lag $k$ and $\\mu$ is the slope in units of “per sample.”\n\nAssume the underlying continuous-time neural dynamics are described generically by $\\dot{\\mathbf{x}}(t)=\\mathbf{F}(\\mathbf{x}(t))$, and recall the core definition of the largest Lyapunov exponent as the asymptotic exponential separation rate per unit time,\n$$\n\\lambda \\;=\\; \\lim_{t\\to\\infty}\\frac{1}{t}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right),\n$$\nfor sufficiently small $\\|\\delta\\mathbf{x}(0)\\|$. You consider a uniform rescaling of time by a factor $c>0$ through the change of variables $t' = c\\,t$, which induces a new flow $\\dfrac{d\\mathbf{x}}{dt'} = \\dfrac{1}{c}\\,\\mathbf{F}(\\mathbf{x})$.\n\nStarting from the above definition and without invoking any shortcut formulas, derive how the largest Lyapunov exponent transforms under the uniform time rescaling, and explain how the discrete-time slope $\\mu$ (per sample) relates to the continuous-time $\\lambda$ (per second) given a sampling interval $\\Delta t$. Then, apply your derivation to the following measured quantities:\n- sampling interval $\\Delta t = 1\\,\\mathrm{ms} = 0.001\\,\\mathrm{s}$,\n- measured short-lag slope $\\mu = 0.015$ (per sample),\n- uniform time rescaling factor $c = 5$ (that is, $t' = c\\,t$).\n\nCompute the rescaled largest Lyapunov exponent $\\lambda'$ in $\\mathrm{s}^{-1}$ for the time-rescaled system. Express the final answer in $\\mathrm{s}^{-1}$. No rounding is required.",
            "solution": "The problem requires the derivation of the transformation rule for the largest Lyapunov exponent under a uniform time rescaling, the relationship between a discretely measured slope and the continuous-time exponent, and finally the calculation of a rescaled exponent given specific data.\n\nFirst, we establish how the largest Lyapunov exponent, $\\lambda$, transforms under a uniform time rescaling. The original system dynamics are given by $\\dot{\\mathbf{x}}(t)=\\mathbf{F}(\\mathbf{x}(t))$, and the largest Lyapunov exponent is defined as:\n$$ \\lambda = \\lim_{t\\to\\infty}\\frac{1}{t}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) $$\nwhere $\\delta\\mathbf{x}(t)$ is an infinitesimal separation vector between two nearby trajectories at time $t$.\n\nA new time variable $t'$ is introduced such that $t' = c\\,t$ for a constant $c > 0$. The dynamics in terms of this new time variable are $\\frac{d\\mathbf{x}}{dt'} = \\frac{1}{c}\\,\\mathbf{F}(\\mathbf{x})$. The largest Lyapunov exponent for this rescaled system, which we denote by $\\lambda'$, is defined with respect to the time $t'$:\n$$ \\lambda' = \\lim_{t'\\to\\infty}\\frac{1}{t'}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t')\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) $$\nThe state of the system at the rescaled time $t'$ is the same as the state of the original system at time $t = t'/c$. Consequently, the separation vector $\\delta\\mathbf{x}$ at time $t'$ in the rescaled system is identical to the separation vector in the original system evaluated at time $t=t'/c$. We can express this as $\\|\\delta\\mathbf{x}(t')\\|_{\\text{rescaled}} = \\|\\delta\\mathbf{x}(t'/c)\\|_{\\text{original}}$. Substituting this into the definition of $\\lambda'$:\n$$ \\lambda' = \\lim_{t'\\to\\infty}\\frac{1}{t'}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t'/c)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) $$\nTo relate this back to the original definition of $\\lambda$, we perform a change of variable in the limit, using $t = t'/c$. As $t' \\to \\infty$ and $c>0$, it follows that $t \\to \\infty$. Substituting $t' = ct$ gives:\n$$ \\lambda' = \\lim_{t\\to\\infty}\\frac{1}{ct}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) $$\nThe constant factor $1/c$ is independent of the limit and can be factored out:\n$$ \\lambda' = \\frac{1}{c} \\left( \\lim_{t\\to\\infty}\\frac{1}{t}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) \\right) $$\nThe expression in the parentheses is the definition of the original Lyapunov exponent, $\\lambda$. Thus, the transformation rule is:\n$$ \\lambda' = \\frac{\\lambda}{c} $$\n\nSecond, we determine the relationship between the discrete-time slope $\\mu$ and the continuous-time exponent $\\lambda$. For a continuous system, the definition of $\\lambda$ implies that for a small initial separation, the separation grows approximately as $\\|\\delta\\mathbf{x}(t)\\| \\approx \\|\\delta\\mathbf{x}(0)\\| \\exp(\\lambda t)$. Taking the natural logarithm yields:\n$$ \\ln\\left(\\frac{\\|\\delta\\mathbf{x}(t)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) \\approx \\lambda t $$\nIn the experimental context, the signal is a time series sampled at a uniform interval $\\Delta t$. The separation $d(k)$ is measured after $k$ integer time steps, which corresponds to a total time of $t = k\\Delta t$. Thus, $d(k)$ corresponds to $\\|\\delta\\mathbf{x}(k\\Delta t)\\|$ and $d(0)$ to $\\|\\delta\\mathbf{x}(0)\\|$. Substituting these into the continuous relation gives:\n$$ \\ln\\left(\\frac{d(k)}{d(0)}\\right) \\approx \\lambda (k\\Delta t) $$\nThe problem provides the empirically observed linear relationship for small $k$:\n$$ \\ln\\left(\\frac{d(k)}{d(0)}\\right) \\approx \\mu k $$\nwhere $\\mu$ is a dimensionless slope measured \"per sample\". By equating the right-hand sides of these two approximations, we obtain:\n$$ \\mu k = \\lambda k \\Delta t $$\nDividing by the non-zero integer $k$ yields the relationship:\n$$ \\mu = \\lambda \\Delta t $$\nThis equation connects the experimentally measured dimensionless quantity $\\mu$ to the fundamental continuous-time physical quantity $\\lambda$. To find $\\lambda$ from the measured slope, we can rearrange this to $\\lambda = \\mu / \\Delta t$.\n\nThird, we apply these results to compute the rescaled exponent $\\lambda'$ using the provided numerical values:\n- Sampling interval $\\Delta t = 1\\,\\mathrm{ms} = 0.001\\,\\mathrm{s}$\n- Measured slope $\\mu = 0.015$ (per sample)\n- Time rescaling factor $c = 5$\n\nFirst, we calculate the largest Lyapunov exponent $\\lambda$ of the original system from the measured data:\n$$ \\lambda = \\frac{\\mu}{\\Delta t} = \\frac{0.015}{0.001\\,\\mathrm{s}} = 15\\,\\mathrm{s}^{-1} $$\nNext, we use the derived transformation rule to calculate the largest Lyapunov exponent $\\lambda'$ of the time-rescaled system:\n$$ \\lambda' = \\frac{\\lambda}{c} = \\frac{15\\,\\mathrm{s}^{-1}}{5} = 3\\,\\mathrm{s}^{-1} $$\nThe rescaled largest Lyapunov exponent is $3\\,\\mathrm{s}^{-1}$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "While a positive largest Lyapunov exponent is the smoking gun for chaos, the full spectrum of exponents, $\\{\\lambda_i\\}$, paints a much richer picture of a system's dynamics. The set of all exponents describes the stretching and folding of the state space volume in every dimension. This exercise  guides you through the calculation of the Kaplan-Yorke dimension, $D_{KY}$, a remarkable quantity that uses the entire Lyapunov spectrum to estimate the fractal dimension of the system's attractor. By completing this calculation, you will gain a deeper appreciation for how dynamics give rise to geometry, and how to quantify the effective \"degrees of freedom\" or complexity of neural activity.",
            "id": "4182471",
            "problem": "A cortical Local Field Potential (LFP) time series $x(t)$ was recorded from an anesthetized rodent primary somatosensory cortex for $1{,}200$ seconds at a sampling rate of $1{,}000$ Hz. After bandpass filtering between $1$ Hz and $120$ Hz, the attractor was reconstructed via delay-coordinate embedding per Takens’ theorem using a time delay of $\\tau = 5$ ms and an embedding dimension of $m = 12$ selected by the false-nearest neighbors criterion. The set of Lyapunov exponents $\\{\\lambda_i\\}_{i=1}^{m}$ was estimated from the reconstructed dynamics using local linear Jacobian approximations with Gram–Schmidt reorthonormalization (Benettin method) and then sorted in nonincreasing order. The estimated spectrum (in $\\mathrm{s}^{-1}$) is\n$$\n\\lambda_1 = 0.58,\\ \\lambda_2 = 0.17,\\ \\lambda_3 = 0.04,\\ \\lambda_4 = 0.01,\\ \\lambda_5 = -0.05,\\ \\lambda_6 = -0.22,\\ \\lambda_7 = -0.31,\\ \\lambda_8 = -0.44,\\ \\lambda_9 = -0.62,\\ \\lambda_{10} = -0.88,\\ \\lambda_{11} = -1.15,\\ \\lambda_{12} = -1.50.\n$$\nStarting from fundamental definitions of Lyapunov exponents as asymptotic exponential rates of separation of nearby trajectories and the concept of partial sums of the spectrum as net volume growth rates in subspaces of increasing dimension, determine the Kaplan–Yorke (Lyapunov) dimension $D_{KY}$ of the reconstructed attractor by identifying the appropriate index $j$ at which the cumulative volume growth transitions from nonnegative to negative and then interpolating to obtain the fractional dimension between $j$ and $j+1$. Interpret the computed $D_{KY}$ as the effective system dimensionality of the cortical dynamics. Round your final numerical answer to four significant figures. The final quantity $D_{KY}$ is dimensionless; express your final answer as a pure number without units.",
            "solution": "The task is to compute the Kaplan–Yorke dimension, denoted $D_{KY}$, from a given spectrum of Lyapunov exponents. Lyapunov exponents, $\\{\\lambda_i\\}$, quantify the average exponential rates of divergence or convergence of nearby trajectories in a reconstructed phase space. For an $m$-dimensional system, a set of $m$ exponents is defined, typically sorted in descending order: $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_m$. Positive exponents indicate sensitive dependence on initial conditions (chaos), as they correspond to directions of local stretching of the phase space volume. Negative exponents correspond to directions of local contraction.\n\nThe Kaplan–Yorke dimension is a conjecture for the information dimension of a dynamical system's attractor and provides an estimate of the system's effective degrees of freedom. It is calculated from the Lyapunov spectrum. The cumulative sum of the first $k$ exponents, $\\sum_{i=1}^{k} \\lambda_i$, represents the average exponential rate of change of a $k$-dimensional volume element in phase space. The Kaplan–Yorke dimension is defined as the dimension at which this volume change rate transitions from non-negative (expansion) to negative (contraction).\n\nThe formula for the Kaplan–Yorke dimension is:\n$$\nD_{KY} = j + \\frac{\\sum_{i=1}^{j} \\lambda_i}{|\\lambda_{j+1}|}\n$$\nwhere $j$ is the largest integer for which the sum of the first $j$ Lyapunov exponents is non-negative, i.e., $\\sum_{i=1}^{j} \\lambda_i \\ge 0$ and $\\sum_{i=1}^{j+1} \\lambda_i  0$. The integer part, $j$, represents the dimensionality of the subspace which is, on average, expanding or stable. The fractional part represents the contribution from the first contracting direction required to balance the expansion, effectively \"filling up\" the next dimension.\n\nWe are given the embedding dimension $m=12$ and the sorted Lyapunov spectrum (in units of $\\mathrm{s}^{-1}$):\n$\\lambda_1 = 0.58$, $\\lambda_2 = 0.17$, $\\lambda_3 = 0.04$, $\\lambda_4 = 0.01$, $\\lambda_5 = -0.05$, $\\lambda_6 = -0.22$, $\\lambda_7 = -0.31$, $\\lambda_8 = -0.44$, $\\lambda_9 = -0.62$, $\\lambda_{10} = -0.88$, $\\lambda_{11} = -1.15$, $\\lambda_{12} = -1.50$.\n\nFirst, we must find the index $j$ by calculating the cumulative sums of the exponents, $S_k = \\sum_{i=1}^{k} \\lambda_i$.\n$$S_1 = \\lambda_1 = 0.58$$\n$$S_2 = S_1 + \\lambda_2 = 0.58 + 0.17 = 0.75$$\n$$S_3 = S_2 + \\lambda_3 = 0.75 + 0.04 = 0.79$$\n$$S_4 = S_3 + \\lambda_4 = 0.79 + 0.01 = 0.80$$\n$$S_5 = S_4 + \\lambda_5 = 0.80 + (-0.05) = 0.75$$\n$$S_6 = S_5 + \\lambda_6 = 0.75 + (-0.22) = 0.53$$\n$$S_7 = S_6 + \\lambda_7 = 0.53 + (-0.31) = 0.22$$\n$$S_8 = S_7 + \\lambda_8 = 0.22 + (-0.44) = -0.22$$\n\nThe cumulative sum $S_k$ is non-negative for $k \\le 7$ and becomes negative for $k=8$. Therefore, the index $j$ is $7$.\n\nNow, we can apply the Kaplan–Yorke formula with $j=7$:\n$$D_{KY} = j + \\frac{\\sum_{i=1}^{j} \\lambda_i}{|\\lambda_{j+1}|} = 7 + \\frac{S_7}{|\\lambda_8|}$$\n\nWe have the necessary values:\n- The sum up to $j=7$ is $S_7 = 0.22$.\n- The next Lyapunov exponent is $\\lambda_8 = -0.44$.\n- The absolute value is $|\\lambda_8| = |-0.44| = 0.44$.\n\nSubstituting these values into the formula:\n$$D_{KY} = 7 + \\frac{0.22}{0.44} = 7 + 0.5 = 7.5$$\n\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $7.5$, which in the required format is $7.500$.\n\nThe interpretation of this result is that the complex dynamics of the cortical local field potential, although embedded in a $12$-dimensional space for analysis, unfold on a geometric structure (a strange attractor) with a fractal dimension of approximately $7.500$. This non-integer dimension is a characteristic feature of chaotic systems. It implies that to model the essential dynamics of this neural activity, a minimum of $\\lceil D_{KY} \\rceil = \\lceil 7.500 \\rceil = 8$ ordinary differential equations would be required, and the attractor does not completely fill the $8$-dimensional space it inhabits. The computed value provides a quantitative measure of the complexity of the underlying brain dynamics.",
            "answer": "$$\\boxed{7.500}$$"
        },
        {
            "introduction": "The elegant theory of phase space reconstruction often meets the harsh reality of measurement noise. Additive noise in a time series does not simply add random jitter to the attractor; it introduces a systematic bias into the geometric quantities we estimate from it, particularly the distances between points that are fundamental to neighbor searches and Lyapunov exponent calculation. This practice  explores the statistical consequences of noise on distance metrics in the reconstructed space. Working through this problem will build your intuition for how noise corrupts nonlinear dynamics estimates and why noise-aware methods, such as using a Mahalanobis distance, are critical for robust analysis of real-world neural data.",
            "id": "4182542",
            "problem": "A single-channel neural time series $\\{x_t^{\\text{obs}}\\}_{t=1}^T$ (for example, a local field potential amplitude) is modeled as additive measurement noise superimposed on a deterministic dynamical signal: $x_t^{\\text{obs}} = x_t + \\eta_t$, where $\\eta_t \\sim \\mathcal{N}(0,\\sigma^2)$ are independent across time $t$. To reconstruct the state space for estimating the largest Lyapunov exponent from neighbor divergence, you form $m$-dimensional delay vectors $y_t^{\\text{obs}} = \\left(x_t^{\\text{obs}}, x_{t-\\tau}^{\\text{obs}}, \\dots, x_{t-(m-1)\\tau}^{\\text{obs}}\\right) \\in \\mathbb{R}^m$ with delay $\\tau \\ge 1$, and enforce a Theiler window to avoid temporally adjacent points. Let $y_t$ denote the noise-free delay vector built from $\\{x_t\\}$, and define the observed difference between two embedded points $i \\neq j$ as $\\Delta y_{ij}^{\\text{obs}} = y_i^{\\text{obs}} - y_j^{\\text{obs}}$ and the noise-free difference as $\\Delta y_{ij} = y_i - y_j$.\n\nAssume first that the measurement noise across the $m$ delayed coordinates remains independent and identically distributed with variance $\\sigma^2$ (which holds when the delays are at least $\\tau \\ge 1$ samples and the measurement process is temporally white). Then consider a generalization in which the $m$-dimensional noise affecting $y_t^{\\text{obs}}$ has zero mean and covariance matrix $\\Sigma \\in \\mathbb{R}^{m \\times m}$ (for example, due to differential sensor noise across channels, or preprocessing that introduces cross-delay correlations), with independent noise across distinct time indices $t$.\n\nChoose all statements that are correct about how additive measurement noise affects neighbor searches in delay-embedded space and about noise-aware distance metrics for neighbor selection and Lyapunov exponent estimation:\n\nA. Under independent, identically distributed Gaussian noise with variance $\\sigma^2$ across the $m$ delayed coordinates, the conditional expectation of the observed squared interpoint distance satisfies $\\mathbb{E}\\!\\left[\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 \\mid \\Delta y_{ij}\\right] = \\left\\|\\Delta y_{ij}\\right\\|^2 + 2 m \\sigma^2$.\n\nB. For zero-mean Gaussian noise with covariance $\\Sigma$ per embedded point, the noise difference $\\Delta n_{ij} = n_i - n_j$ has covariance $2\\Sigma$, and thus $\\mathbb{E}\\!\\left[\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 \\mid \\Delta y_{ij}\\right] = \\left\\|\\Delta y_{ij}\\right\\|^2 + 2\\,\\mathrm{tr}(\\Sigma)$. Subtracting $2\\,\\mathrm{tr}(\\Sigma)$ from the observed squared distance yields an unbiased estimator (in expectation) of the true squared distance $\\left\\|\\Delta y_{ij}\\right\\|^2$.\n\nC. When noise is isotropic with covariance $\\sigma^2 I_m$, replacing the Euclidean distance by the Mahalanobis distance using covariance $2 \\sigma^2 I_m$ changes the neighbor ranking to be more robust against noise.\n\nD. If the delayed coordinates have heterogeneous noise variances $\\sigma_1^2,\\dots,\\sigma_m^2$ (so $\\Sigma = \\mathrm{diag}(\\sigma_1^2,\\dots,\\sigma_m^2)$), ranking neighbors by the weighted Euclidean metric $\\sum_{k=1}^m \\left(\\Delta y_{ij,k}^{\\text{obs}}\\right)^2 / \\sigma_k^2$ (equivalently, the Mahalanobis distance with covariance $2\\Sigma$) is the maximum-likelihood rule for selecting nearest neighbors under Gaussian noise.\n\nE. To reduce noise bias in short-time divergence curves used in largest Lyapunov exponent estimation, replacing the initial observed separation $\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|$ by $\\sqrt{\\max\\!\\left(\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 - 2 m \\sigma^2,\\; 0\\right)}$ yields an unbiased estimator of the true separation magnitude $\\left\\|\\Delta y_{ij}\\right\\|$ in expectation.",
            "solution": "Let the observed $m$-dimensional delay vector at time $t$ be $y_t^{\\text{obs}}$ and the true, noise-free vector be $y_t$. The relationship is $y_t^{\\text{obs}} = y_t + n_t$, where $n_t$ is the noise vector. The problem states that the noise vectors $n_t$ are independent across different times $t$. Each noise vector $n_t$ is drawn from a multivariate Gaussian distribution with zero mean, $n_t \\sim \\mathcal{N}(0, \\Sigma)$.\n\nWe are interested in the distance between two distinct points, indexed by $i$ and $j$ ($i \\neq j$). The observed difference vector is $\\Delta y_{ij}^{\\text{obs}} = y_i^{\\text{obs}} - y_j^{\\text{obs}}$. The true difference vector is $\\Delta y_{ij} = y_i - y_j$. The noise difference vector is $\\Delta n_{ij} = n_i - n_j$. The relationship between them is:\n$$ \\Delta y_{ij}^{\\text{obs}} = (y_i + n_i) - (y_j + n_j) = (y_i - y_j) + (n_i - n_j) = \\Delta y_{ij} + \\Delta n_{ij} $$\nSince $n_i$ and $n_j$ are independent for $i \\neq j$ and both are from $\\mathcal{N}(0, \\Sigma)$, their difference $\\Delta n_{ij}$ is also a Gaussian random vector.\nThe mean of the noise difference is $\\mathbb{E}[\\Delta n_{ij}] = \\mathbb{E}[n_i] - \\mathbb{E}[n_j] = 0 - 0 = 0$.\nThe covariance of the noise difference is $\\mathrm{Cov}(\\Delta n_{ij}) = \\mathrm{Cov}(n_i - n_j) = \\mathrm{Cov}(n_i) + \\mathrm{Cov}(n_j) = \\Sigma + \\Sigma = 2\\Sigma$, because $n_i$ and $n_j$ are independent. So, $\\Delta n_{ij} \\sim \\mathcal{N}(0, 2\\Sigma)$.\n\nWe are frequently interested in the squared Euclidean distance, $\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2$. Let's expand this:\n$$ \\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 = \\left\\| \\Delta y_{ij} + \\Delta n_{ij} \\right\\|^2 = (\\Delta y_{ij} + \\Delta n_{ij})^T (\\Delta y_{ij} + \\Delta n_{ij}) $$\n$$ = \\Delta y_{ij}^T \\Delta y_{ij} + 2 \\Delta y_{ij}^T \\Delta n_{ij} + \\Delta n_{ij}^T \\Delta n_{ij} = \\left\\|\\Delta y_{ij}\\right\\|^2 + 2 \\Delta y_{ij}^T \\Delta n_{ij} + \\left\\|\\Delta n_{ij}\\right\\|^2 $$\nNow we take the conditional expectation with respect to the noise, given the true deterministic vectors (and thus, given $\\Delta y_{ij}$):\n$$ \\mathbb{E}\\!\\left[\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 \\mid \\Delta y_{ij}\\right] = \\mathbb{E}\\!\\left[\\left\\|\\Delta y_{ij}\\right\\|^2 \\mid \\Delta y_{ij}\\right] + 2 \\Delta y_{ij}^T \\mathbb{E}\\!\\left[\\Delta n_{ij} \\mid \\Delta y_{ij}\\right] + \\mathbb{E}\\!\\left[\\left\\|\\Delta n_{ij}\\right\\|^2 \\mid \\Delta y_{ij}\\right] $$\nSince $\\Delta n_{ij}$ is independent of the deterministic signal, the conditioning is irrelevant for the noise terms. As $\\mathbb{E}[\\Delta n_{ij}] = 0$, the middle term vanishes.\n$$ \\mathbb{E}\\!\\left[\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 \\mid \\Delta y_{ij}\\right] = \\left\\|\\Delta y_{ij}\\right\\|^2 + \\mathbb{E}[\\left\\|\\Delta n_{ij}\\right\\|^2] $$\nFor any random vector $Z$ with mean $\\mu$ and covariance $C$, the expected squared norm is $\\mathbb{E}[\\left\\|Z\\right\\|^2] = \\mathrm{tr}(C) + \\left\\|\\mu\\right\\|^2$. For our noise difference vector $Z = \\Delta n_{ij}$, the mean is $\\mu = 0$ and the covariance is $C = 2\\Sigma$.\nThus, $\\mathbb{E}[\\left\\|\\Delta n_{ij}\\right\\|^2] = \\mathrm{tr}(2\\Sigma) = 2\\,\\mathrm{tr}(\\Sigma)$.\nThis gives us the general formula for the expected observed squared distance:\n$$ \\mathbb{E}\\!\\left[\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 \\mid \\Delta y_{ij}\\right] = \\left\\|\\Delta y_{ij}\\right\\|^2 + 2\\,\\mathrm{tr}(\\Sigma) $$\nThis expression shows that additive noise introduces a positive bias to the squared Euclidean distance, proportional to the trace of the noise covariance matrix.\n\nNow we evaluate each option.\n\n**A. Under independent, identically distributed Gaussian noise with variance $\\sigma^2$ across the $m$ delayed coordinates, the conditional expectation of the observed squared interpoint distance satisfies $\\mathbb{E}\\!\\left[\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 \\mid \\Delta y_{ij}\\right] = \\left\\|\\Delta y_{ij}\\right\\|^2 + 2 m \\sigma^2$.**\n\nThis corresponds to the special case where the noise components in the delay vector are i.i.d. The covariance matrix of the noise vector $n_t$ is $\\Sigma = \\sigma^2 I_m$, where $I_m$ is the $m \\times m$ identity matrix.\nWe use the general formula derived above, substituting this specific $\\Sigma$:\nThe trace of $\\Sigma$ is $\\mathrm{tr}(\\Sigma) = \\mathrm{tr}(\\sigma^2 I_m) = \\sigma^2 \\mathrm{tr}(I_m) = \\sigma^2 \\sum_{k=1}^m 1 = m\\sigma^2$.\nSubstituting this into the general expression for the expected squared distance:\n$$ \\mathbb{E}\\!\\left[\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 \\mid \\Delta y_{ij}\\right] = \\left\\|\\Delta y_{ij}\\right\\|^2 + 2(m\\sigma^2) = \\left\\|\\Delta y_{ij}\\right\\|^2 + 2 m \\sigma^2 $$\nThis expression matches the statement in option A.\nVerdict: **Correct**.\n\n**B. For zero-mean Gaussian noise with covariance $\\Sigma$ per embedded point, the noise difference $\\Delta n_{ij} = n_i - n_j$ has covariance $2\\Sigma$, and thus $\\mathbb{E}\\!\\left[\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 \\mid \\Delta y_{ij}\\right] = \\left\\|\\Delta y_{ij}\\right\\|^2 + 2\\,\\mathrm{tr}(\\Sigma)$. Subtracting $2\\,\\mathrm{tr}(\\Sigma)$ from the observed squared distance yields an unbiased estimator (in expectation) of the true squared distance $\\left\\|\\Delta y_{ij}\\right\\|^2$.**\n\nThis statement has two parts. The first part is $\\mathrm{Cov}(\\Delta n_{ij}) = 2\\Sigma$ and $\\mathbb{E}\\!\\left[\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 \\mid \\Delta y_{ij}\\right] = \\left\\|\\Delta y_{ij}\\right\\|^2 + 2\\,\\mathrm{tr}(\\Sigma)$. As shown in the initial derivation, both of these are correct.\nThe second part claims that $\\hat{d^2} = \\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 - 2\\,\\mathrm{tr}(\\Sigma)$ is an unbiased estimator of the true squared distance $\\left\\|\\Delta y_{ij}\\right\\|^2$. An estimator is unbiased if its expectation equals the true value of the parameter being estimated. Let's compute the expectation of $\\hat{d^2}$:\n$$ \\mathbb{E}[\\hat{d^2} \\mid \\Delta y_{ij}] = \\mathbb{E}[\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 - 2\\,\\mathrm{tr}(\\Sigma) \\mid \\Delta y_{ij}] $$\n$$ = \\mathbb{E}[\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 \\mid \\Delta y_{ij}] - 2\\,\\mathrm{tr}(\\Sigma) $$\nUsing the result from the first part, we substitute the expression for the expected observed squared distance:\n$$ \\mathbb{E}[\\hat{d^2} \\mid \\Delta y_{ij}] = (\\left\\|\\Delta y_{ij}\\right\\|^2 + 2\\,\\mathrm{tr}(\\Sigma)) - 2\\,\\mathrm{tr}(\\Sigma) = \\left\\|\\Delta y_{ij}\\right\\|^2 $$\nThe expectation of the estimator is indeed the true squared distance. Hence, it is an unbiased estimator. Both parts of the statement are correct.\nVerdict: **Correct**.\n\n**C. When noise is isotropic with covariance $\\sigma^2 I_m$, replacing the Euclidean distance by the Mahalanobis distance using covariance $2 \\sigma^2 I_m$ changes the neighbor ranking to be more robust against noise.**\n\nThe squared Mahalanobis distance between $y_i^{\\text{obs}}$ and $y_j^{\\text{obs}}$ with respect to a covariance matrix $S$ is given by $d_M^2 = (\\Delta y_{ij}^{\\text{obs}})^T S^{-1} (\\Delta y_{ij}^{\\text{obs}})$.\nIn this case, the noise on the difference vector $\\Delta n_{ij}$ has covariance $S = 2\\Sigma = 2\\sigma^2 I_m$.\nThe inverse is $S^{-1} = (2\\sigma^2 I_m)^{-1} = \\frac{1}{2\\sigma^2} I_m$.\nThe squared Mahalanobis distance is:\n$$ d_M^2 = (\\Delta y_{ij}^{\\text{obs}})^T \\left(\\frac{1}{2\\sigma^2} I_m\\right) (\\Delta y_{ij}^{\\text{obs}}) = \\frac{1}{2\\sigma^2} (\\Delta y_{ij}^{\\text{obs}})^T (\\Delta y_{ij}^{\\text{obs}}) = \\frac{1}{2\\sigma^2} \\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 $$\nThe Mahalanobis distance is $d_M = \\sqrt{\\frac{1}{2\\sigma^2} \\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2} = \\frac{1}{\\sqrt{2}\\sigma} \\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|$.\nSince $\\sigma$ is a constant, the Mahalanobis distance is simply the Euclidean distance $\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|$ scaled by a constant factor $1/(\\sqrt{2}\\sigma)$. When ranking potential neighbors for a given point, multiplying all distances by the same positive constant does not change their order. Therefore, the neighbor ranking is identical to the one obtained using the Euclidean distance. The statement that the ranking \"changes\" is false.\nVerdict: **Incorrect**.\n\n**D. If the delayed coordinates have heterogeneous noise variances $\\sigma_1^2,\\dots,\\sigma_m^2$ (so $\\Sigma = \\mathrm{diag}(\\sigma_1^2,\\dots,\\sigma_m^2)$), ranking neighbors by the weighted Euclidean metric $\\sum_{k=1}^m \\left(\\Delta y_{ij,k}^{\\text{obs}}\\right)^2 / \\sigma_k^2$ (equivalently, the Mahalanobis distance with covariance $2\\Sigma$) is the maximum-likelihood rule for selecting nearest neighbors under Gaussian noise.**\n\nFirst, let's verify the equivalence of the metrics for ranking. The covariance of the noise vector is $\\Sigma = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_m^2)$. The covariance of the noise difference is $S = 2\\Sigma = \\mathrm{diag}(2\\sigma_1^2, \\dots, 2\\sigma_m^2)$. The inverse is $S^{-1} = \\frac{1}{2} \\mathrm{diag}(1/\\sigma_1^2, \\dots, 1/\\sigma_m^2)$.\nThe squared Mahalanobis distance is:\n$$ d_M^2 = (\\Delta y_{ij}^{\\text{obs}})^T S^{-1} (\\Delta y_{ij}^{\\text{obs}}) = \\sum_{k=1}^m \\frac{1}{2\\sigma_k^2} (\\Delta y_{ij,k}^{\\text{obs}})^2 = \\frac{1}{2} \\sum_{k=1}^m \\frac{(\\Delta y_{ij,k}^{\\text{obs}})^2}{\\sigma_k^2} $$\nThe weighted Euclidean metric in the statement is $d_W^2 = \\sum_{k=1}^m (\\Delta y_{ij,k}^{\\text{obs}})^2 / \\sigma_k^2$. Since $d_M^2 = \\frac{1}{2} d_W^2$, ranking by either metric gives the same result. The equivalence claim is correct for ranking.\n\nNow, we assess if this is a maximum-likelihood (ML) rule. Finding \"nearest neighbors\" in a noisy setting can be interpreted as finding the pair $(i,j)$ that is most likely to be a \"true\" neighbor, i.e., for which the true separation $\\Delta y_{ij}$ is zero or very small. Let's consider the hypothesis $H_0: \\Delta y_{ij}=0$. Under this hypothesis, $\\Delta y_{ij}^{\\text{obs}} = \\Delta n_{ij} \\sim \\mathcal{N}(0, 2\\Sigma)$. The log-likelihood of observing $\\Delta y_{ij}^{\\text{obs}}$ under $H_0$ is:\n$$ \\log p(\\Delta y_{ij}^{\\text{obs}} \\mid H_0) = -\\frac{1}{2} (\\Delta y_{ij}^{\\text{obs}})^T (2\\Sigma)^{-1} (\\Delta y_{ij}^{\\text{obs}}) - \\frac{1}{2}\\log\\det(2\\Sigma) - \\frac{m}{2}\\log(2\\pi) $$\nTo maximize this likelihood over the choice of neighbor $j$, we must minimize the quadratic term $(\\Delta y_{ij}^{\\text{obs}})^T (2\\Sigma)^{-1} (\\Delta y_{ij}^{\\text{obs}})$, which is precisely the squared Mahalanobis distance with covariance $2\\Sigma$. This metric appropriately down-weights coordinates with high noise variance ($\\sigma_k^2$), making it the optimal way to identify pairs that are close in the presence of heteroscedastic Gaussian noise. This is a standard result from statistical estimation theory.\nVerdict: **Correct**.\n\n**E. To reduce noise bias in short-time divergence curves used in largest Lyapunov exponent estimation, replacing the initial observed separation $\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|$ by $\\sqrt{\\max\\!\\left(\\left\\|\\Delta y_{ij}^{\\text{obs}}\\right\\|^2 - 2 m \\sigma^2,\\; 0\\right)}$ yields an unbiased estimator of the true separation magnitude $\\left\\|\\Delta y_{ij}\\right\\|$ in expectation.**\n\nThis option proposes an estimator for the true distance magnitude $\\|\\Delta y_{ij}\\|$, not the squared distance. Let $d_{\\text{true}} = \\|\\Delta y_{ij}\\|$ and $d_{\\text{obs}} = \\|\\Delta y_{ij}^{\\text{obs}}\\|$.\nFrom option A, we know that $\\mathbb{E}[d_{\\text{obs}}^2] = d_{\\text{true}}^2 + 2m\\sigma^2$.\nLet $X = d_{\\text{obs}}^2 - 2m\\sigma^2$. Then $\\mathbb{E}[X] = d_{\\text{true}}^2$. So, $X$ is an unbiased estimator of the squared distance.\nThe proposed estimator for the distance magnitude is $\\hat{d} = \\sqrt{\\max(X, 0)}$.\nWe need to check if $\\mathbb{E}[\\hat{d}] = d_{\\text{true}}$.\nThe function $f(X) = \\sqrt{\\max(X, 0)}$ is a non-linear, concave function. By Jensen's inequality, for a concave function $f$ and a random variable $X$, we have $\\mathbb{E}[f(X)] \\le f(\\mathbb{E}[X])$. The inequality is strict if $f$ is strictly concave and $X$ is not a constant.\nApplying this here:\n$$ \\mathbb{E}[\\hat{d}] = \\mathbb{E}[\\sqrt{\\max(X, 0)}] \\le \\sqrt{\\max(\\mathbb{E}[X], 0)} $$\nSince $\\mathbb{E}[X] = d_{\\text{true}}^2 \\ge 0$:\n$$ \\mathbb{E}[\\hat{d}] \\le \\sqrt{d_{\\text{true}}^2} = d_{\\text{true}} $$\nBecause $d_{\\text{obs}}^2$ is a random variable (due to noise), $X$ is also a random variable, and the inequality is strict. This means $\\mathbb{E}[\\hat{d}]  d_{\\text{true}}$. The estimator is biased; it systematically underestimates the true distance magnitude. The statement that it is an unbiased estimator is false.\nVerdict: **Incorrect**.\n\nFinal consolidation: Options A, B, and D are correct. Options C and E are incorrect.",
            "answer": "$$\\boxed{ABD}$$"
        }
    ]
}