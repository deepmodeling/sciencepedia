{
    "hands_on_practices": [
        {
            "introduction": "Before we can confidently estimate Lyapunov exponents from data, it is crucial to understand their fundamental properties. This first practice explores the physical meaning of a Lyapunov exponent as a rate and how it behaves under a simple transformation like time rescaling . By working through this derivation, you will clarify the connection between the theoretical continuous-time quantity and the discrete-time slope estimated from sampled data, a vital bridge between theory and practical application.",
            "id": "4182450",
            "problem": "A cortical population signal is recorded as a scalar time series from a microelectrode array and used to reconstruct a state space via delay-coordinate embedding. To quantify sensitivity to initial conditions, the largest Lyapunov exponent is estimated from the short-time growth of separations between nearby reconstructed trajectories. Specifically, for small integer lags $k$ (in samples), the ensemble-averaged logarithmic separation behaves approximately linearly as $\\ln\\!\\big(d(k)/d(0)\\big) \\approx \\mu\\,k$, where $d(k)$ is the mean separation at lag $k$ and $\\mu$ is the slope in units of “per sample.”\n\nAssume the underlying continuous-time neural dynamics are described generically by $\\dot{\\mathbf{x}}(t)=\\mathbf{F}(\\mathbf{x}(t))$, and recall the core definition of the largest Lyapunov exponent as the asymptotic exponential separation rate per unit time,\n$$\n\\lambda \\;=\\; \\lim_{t\\to\\infty}\\frac{1}{t}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right),\n$$\nfor sufficiently small $\\|\\delta\\mathbf{x}(0)\\|$. You consider a uniform rescaling of time by a factor $c0$ through the change of variables $t' = c\\,t$, which induces a new flow $\\dfrac{d\\mathbf{x}}{dt'} = \\dfrac{1}{c}\\,\\mathbf{F}(\\mathbf{x})$.\n\nStarting from the above definition and without invoking any shortcut formulas, derive how the largest Lyapunov exponent transforms under the uniform time rescaling, and explain how the discrete-time slope $\\mu$ (per sample) relates to the continuous-time $\\lambda$ (per second) given a sampling interval $\\Delta t$. Then, apply your derivation to the following measured quantities:\n- sampling interval $\\Delta t = 1\\,\\mathrm{ms} = 0.001\\,\\mathrm{s}$,\n- measured short-lag slope $\\mu = 0.015$ (per sample),\n- uniform time rescaling factor $c = 5$ (that is, $t' = c\\,t$).\n\nCompute the rescaled largest Lyapunov exponent $\\lambda'$ in $\\mathrm{s}^{-1}$ for the time-rescaled system. Express the final answer in $\\mathrm{s}^{-1}$. No rounding is required.",
            "solution": "The problem requires the derivation of the transformation rule for the largest Lyapunov exponent under a uniform time rescaling, the relationship between a discretely measured slope and the continuous-time exponent, and finally the calculation of a rescaled exponent given specific data.\n\nFirst, we establish how the largest Lyapunov exponent, $\\lambda$, transforms under a uniform time rescaling. The original system dynamics are given by $\\dot{\\mathbf{x}}(t)=\\mathbf{F}(\\mathbf{x}(t))$, and the largest Lyapunov exponent is defined as:\n$$ \\lambda = \\lim_{t\\to\\infty}\\frac{1}{t}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) $$\nwhere $\\delta\\mathbf{x}(t)$ is an infinitesimal separation vector between two nearby trajectories at time $t$.\n\nA new time variable $t'$ is introduced such that $t' = c\\,t$ for a constant $c > 0$. The dynamics in terms of this new time variable are $\\frac{d\\mathbf{x}}{dt'} = \\frac{1}{c}\\,\\mathbf{F}(\\mathbf{x})$. The largest Lyapunov exponent for this rescaled system, which we denote by $\\lambda'$, is defined with respect to the time $t'$:\n$$ \\lambda' = \\lim_{t'\\to\\infty}\\frac{1}{t'}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t')\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) $$\nThe state of the system at the rescaled time $t'$ is the same as the state of the original system at time $t = t'/c$. Consequently, the separation vector $\\delta\\mathbf{x}$ at time $t'$ in the rescaled system is identical to the separation vector in the original system evaluated at time $t=t'/c$. We can express this as $\\|\\delta\\mathbf{x}(t')\\|_{\\text{rescaled}} = \\|\\delta\\mathbf{x}(t'/c)\\|_{\\text{original}}$. Substituting this into the definition of $\\lambda'$:\n$$ \\lambda' = \\lim_{t'\\to\\infty}\\frac{1}{t'}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t'/c)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) $$\nTo relate this back to the original definition of $\\lambda$, we perform a change of variable in the limit, using $t = t'/c$. As $t' \\to \\infty$ and $c>0$, it follows that $t \\to \\infty$. Substituting $t' = ct$ gives:\n$$ \\lambda' = \\lim_{t\\to\\infty}\\frac{1}{ct}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) $$\nThe constant factor $1/c$ is independent of the limit and can be factored out:\n$$ \\lambda' = \\frac{1}{c} \\left( \\lim_{t\\to\\infty}\\frac{1}{t}\\,\\ln\\!\\left(\\frac{\\|\\delta\\mathbf{x}(t)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) \\right) $$\nThe expression in the parentheses is the definition of the original Lyapunov exponent, $\\lambda$. Thus, the transformation rule is:\n$$ \\lambda' = \\frac{\\lambda}{c} $$\n\nSecond, we determine the relationship between the discrete-time slope $\\mu$ and the continuous-time exponent $\\lambda$. For a continuous system, the definition of $\\lambda$ implies that for a small initial separation, the separation grows approximately as $\\|\\delta\\mathbf{x}(t)\\| \\approx \\|\\delta\\mathbf{x}(0)\\| \\exp(\\lambda t)$. Taking the natural logarithm yields:\n$$ \\ln\\left(\\frac{\\|\\delta\\mathbf{x}(t)\\|}{\\|\\delta\\mathbf{x}(0)\\|}\\right) \\approx \\lambda t $$\nIn the experimental context, the signal is a time series sampled at a uniform interval $\\Delta t$. The separation $d(k)$ is measured after $k$ integer time steps, which corresponds to a total time of $t = k\\Delta t$. Thus, $d(k)$ corresponds to $\\|\\delta\\mathbf{x}(k\\Delta t)\\|$ and $d(0)$ to $\\|\\delta\\mathbf{x}(0)\\|$. Substituting these into the continuous relation gives:\n$$ \\ln\\left(\\frac{d(k)}{d(0)}\\right) \\approx \\lambda (k\\Delta t) $$\nThe problem provides the empirically observed linear relationship for small $k$:\n$$ \\ln\\left(\\frac{d(k)}{d(0)}\\right) \\approx \\mu k $$\nwhere $\\mu$ is a dimensionless slope measured \"per sample\". By equating the right-hand sides of these two approximations, we obtain:\n$$ \\mu k = \\lambda k \\Delta t $$\nDividing by the non-zero integer $k$ yields the relationship:\n$$ \\mu = \\lambda \\Delta t $$\nThis equation connects the experimentally measured dimensionless quantity $\\mu$ to the fundamental continuous-time physical quantity $\\lambda$. To find $\\lambda$ from the measured slope, we can rearrange this to $\\lambda = \\mu / \\Delta t$.\n\nThird, we apply these results to compute the rescaled exponent $\\lambda'$ using the provided numerical values:\n- Sampling interval $\\Delta t = 1\\,\\mathrm{ms} = 0.001\\,\\mathrm{s}$\n- Measured slope $\\mu = 0.015$ (per sample)\n- Time rescaling factor $c = 5$\n\nFirst, we calculate the largest Lyapunov exponent $\\lambda$ of the original system from the measured data:\n$$ \\lambda = \\frac{\\mu}{\\Delta t} = \\frac{0.015}{0.001\\,\\mathrm{s}} = 15\\,\\mathrm{s}^{-1} $$\nNext, we use the derived transformation rule to calculate the largest Lyapunov exponent $\\lambda'$ of the time-rescaled system:\n$$ \\lambda' = \\frac{\\lambda}{c} = \\frac{15\\,\\mathrm{s}^{-1}}{5} = 3\\,\\mathrm{s}^{-1} $$\nThe rescaled largest Lyapunov exponent is $3\\,\\mathrm{s}^{-1}$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "A positive largest Lyapunov exponent signals chaos, but the full spectrum of exponents holds far more information about a system's geometric structure. This exercise guides you in using the entire set of exponents to estimate the fractal dimension of an attractor via the Kaplan-Yorke conjecture . This practice will strengthen your ability to translate the abstract Lyapunov spectrum into a tangible, quantitative measure of the complexity and effective degrees of freedom of the underlying dynamics.",
            "id": "4182471",
            "problem": "A cortical Local Field Potential (LFP) time series $x(t)$ was recorded from an anesthetized rodent primary somatosensory cortex for $1{,}200$ seconds at a sampling rate of $1{,}000$ Hz. After bandpass filtering between $1$ Hz and $120$ Hz, the attractor was reconstructed via delay-coordinate embedding per Takens’ theorem using a time delay of $\\tau = 5$ ms and an embedding dimension of $m = 12$ selected by the false-nearest neighbors criterion. The set of Lyapunov exponents $\\{\\lambda_i\\}_{i=1}^{m}$ was estimated from the reconstructed dynamics using local linear Jacobian approximations with Gram–Schmidt reorthonormalization (Benettin method) and then sorted in nonincreasing order. The estimated spectrum (in $\\mathrm{s}^{-1}$) is\n$$\n\\lambda_1 = 0.58,\\ \\lambda_2 = 0.17,\\ \\lambda_3 = 0.04,\\ \\lambda_4 = 0.01,\\ \\lambda_5 = -0.05,\\ \\lambda_6 = -0.22,\\ \\lambda_7 = -0.31,\\ \\lambda_8 = -0.44,\\ \\lambda_9 = -0.62,\\ \\lambda_{10} = -0.88,\\ \\lambda_{11} = -1.15,\\ \\lambda_{12} = -1.50.\n$$\nStarting from fundamental definitions of Lyapunov exponents as asymptotic exponential rates of separation of nearby trajectories and the concept of partial sums of the spectrum as net volume growth rates in subspaces of increasing dimension, determine the Kaplan–Yorke (Lyapunov) dimension $D_{KY}$ of the reconstructed attractor by identifying the appropriate index $j$ at which the cumulative volume growth transitions from nonnegative to negative and then interpolating to obtain the fractional dimension between $j$ and $j+1$. Interpret the computed $D_{KY}$ as the effective system dimensionality of the cortical dynamics. Round your final numerical answer to four significant figures. The final quantity $D_{KY}$ is dimensionless; express your final answer as a pure number without units.",
            "solution": "The problem is valid. It presents a well-posed question in computational neuroscience, grounded in the established principles of nonlinear dynamics. All necessary data and definitions are provided, and there are no scientific or logical inconsistencies.\n\nThe task is to compute the Kaplan–Yorke dimension, denoted $D_{KY}$, from a given spectrum of Lyapunov exponents. Lyapunov exponents, $\\{\\lambda_i\\}$, quantify the average exponential rates of divergence or convergence of nearby trajectories in a reconstructed phase space. For an $m$-dimensional system, a set of $m$ exponents is defined, typically sorted in descending order: $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_m$. Positive exponents indicate sensitive dependence on initial conditions (chaos), as they correspond to directions of local stretching of the phase space volume. Negative exponents correspond to directions of local contraction.\n\nThe Kaplan–Yorke dimension is a conjecture for the information dimension of a dynamical system's attractor and provides an estimate of the system's effective degrees of freedom. It is calculated from the Lyapunov spectrum. The cumulative sum of the first $k$ exponents, $\\sum_{i=1}^{k} \\lambda_i$, represents the average exponential rate of change of a $k$-dimensional volume element in phase space. The Kaplan–Yorke dimension is defined as the dimension at which this volume change rate transitions from non-negative (expansion) to negative (contraction).\n\nThe formula for the Kaplan–Yorke dimension is:\n$$\nD_{KY} = j + \\frac{\\sum_{i=1}^{j} \\lambda_i}{|\\lambda_{j+1}|}\n$$\nwhere $j$ is the largest integer for which the sum of the first $j$ Lyapunov exponents is non-negative, i.e., $\\sum_{i=1}^{j} \\lambda_i \\ge 0$ and $\\sum_{i=1}^{j+1} \\lambda_i  0$. The integer part, $j$, represents the dimensionality of the subspace which is, on average, expanding or stable. The fractional part represents the contribution from the first contracting direction required to balance the expansion, effectively \"filling up\" the next dimension.\n\nWe are given the embedding dimension $m=12$ and the sorted Lyapunov spectrum (in units of $\\mathrm{s}^{-1}$):\n$\\lambda_1 = 0.58$, $\\lambda_2 = 0.17$, $\\lambda_3 = 0.04$, $\\lambda_4 = 0.01$, $\\lambda_5 = -0.05$, $\\lambda_6 = -0.22$, $\\lambda_7 = -0.31$, $\\lambda_8 = -0.44$, $\\lambda_9 = -0.62$, $\\lambda_{10} = -0.88$, $\\lambda_{11} = -1.15$, $\\lambda_{12} = -1.50$.\n\nFirst, we must find the index $j$ by calculating the cumulative sums of the exponents, $S_k = \\sum_{i=1}^{k} \\lambda_i$.\n$$S_1 = \\lambda_1 = 0.58$$\n$$S_2 = S_1 + \\lambda_2 = 0.58 + 0.17 = 0.75$$\n$$S_3 = S_2 + \\lambda_3 = 0.75 + 0.04 = 0.79$$\n$$S_4 = S_3 + \\lambda_4 = 0.79 + 0.01 = 0.80$$\n$$S_5 = S_4 + \\lambda_5 = 0.80 + (-0.05) = 0.75$$\n$$S_6 = S_5 + \\lambda_6 = 0.75 + (-0.22) = 0.53$$\n$$S_7 = S_6 + \\lambda_7 = 0.53 + (-0.31) = 0.22$$\n$$S_8 = S_7 + \\lambda_8 = 0.22 + (-0.44) = -0.22$$\n\nThe cumulative sum $S_k$ is non-negative for $k \\le 7$ and becomes negative for $k=8$. Therefore, the index $j$ is $7$.\n\nNow, we can apply the Kaplan–Yorke formula with $j=7$:\n$$D_{KY} = j + \\frac{\\sum_{i=1}^{j} \\lambda_i}{|\\lambda_{j+1}|} = 7 + \\frac{S_7}{|\\lambda_8|}$$\n\nWe have the necessary values:\n- The sum up to $j=7$ is $S_7 = 0.22$.\n- The next Lyapunov exponent is $\\lambda_8 = -0.44$.\n- The absolute value is $|\\lambda_8| = |-0.44| = 0.44$.\n\nSubstituting these values into the formula:\n$$D_{KY} = 7 + \\frac{0.22}{0.44} = 7 + 0.5 = 7.5$$\n\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $7.5$, which in the required format is $7.500$.\n\nThe interpretation of this result is that the complex dynamics of the cortical local field potential, although embedded in a $12$-dimensional space for analysis, unfold on a geometric structure (a strange attractor) with a fractal dimension of approximately $7.500$. This non-integer dimension is a characteristic feature of chaotic systems. It implies that to model the essential dynamics of this neural activity, a minimum of $\\lceil D_{KY} \\rceil = \\lceil 7.500 \\rceil = 8$ ordinary differential equations would be required, and the attractor does not completely fill the $8$-dimensional space it inhabits. The computed value provides a quantitative measure of the complexity of the underlying brain dynamics.",
            "answer": "$$\\boxed{7.500}$$"
        },
        {
            "introduction": "This final capstone practice integrates the concepts of phase space reconstruction, recurrence analysis, and Lyapunov exponents into a complete computational workflow. You are tasked with designing and implementing a program to segment a non-stationary time series, a common challenge in analyzing real-world signals like EEG data where brain states change over time . This project will challenge you to use Recurrence Quantification Analysis to identify dynamic shifts and then validate your findings by showing that the identified segments possess distinct chaotic properties, as measured by their largest Lyapunov exponent.",
            "id": "4182472",
            "problem": "You are given the task of designing and implementing a complete, runnable program that segments a univariate, discrete-time Electroencephalography (EEG)-like signal into regimes with differing dynamics using recurrence quantification analysis, and validates the segmentation by differences in estimated maximum Lyapunov exponents. The program must be fully self-contained and must not require any external input or files. It must produce a single line of output with the validation results for a test suite of parameter sets.\n\nFundamental base:\n- Takens' embedding theorem provides that for a generic observable of a dynamical system, delay-coordinate vectors reconstruct a diffeomorphic image of the underlying attractor, under suitable embedding dimension and delay. Given a univariate time series $x_0, x_1, \\dots, x_{N-1}$, a delay embedding is defined by vectors\n$$\n\\mathbf{y}_i = \\left( x_i, x_{i+\\tau}, x_{i+2\\tau}, \\dots, x_{i+(m-1)\\tau} \\right),\n$$\nwhere $m$ is the embedding dimension and $\\tau$ is the delay, producing $M = N - (m-1)\\tau$ embedded points $\\mathbf{y}_i \\in \\mathbb{R}^m$ for $i = 0,\\dots,M-1$.\n\n- A recurrence plot is constructed from pairwise distances between embedded vectors, defining a binary recurrence matrix $R \\in \\{0,1\\}^{M \\times M}$ with entries\n$$\nR_{ij} =\n\\begin{cases}\n1,  \\text{if } \\lVert \\mathbf{y}_i - \\mathbf{y}_j \\rVert \\le \\varepsilon \\text{ and } |i-j|  T,\\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\nwhere $\\varepsilon$ is a fixed threshold and $T$ is a Theiler window excluding temporally adjacent points to avoid trivial recurrences.\n\n- Recurrence Quantification Analysis (RQA) computes summary metrics from $R$. In particular, define the total number of recurrence points as $N_R = \\sum_{i,j} R_{ij}$. Determinism is defined as the fraction of recurrence points that belong to diagonal lines of length at least $L_{\\min}$, where a diagonal line is a contiguous run of ones along $R_{i+k,j+k}$:\n$$\n\\mathrm{DET} = \\frac{\\sum_{\\ell \\ge L_{\\min}} \\ell \\cdot \\mathcal{N}(\\ell)}{N_R},\n$$\nwhere $\\mathcal{N}(\\ell)$ is the number of diagonal lines of length $\\ell$ in $R$.\n\n- The maximum Lyapunov exponent $\\lambda_{\\max}$ quantifies the average exponential rate of divergence of nearby trajectories. Given two close embedded points $\\mathbf{y}_i$ and $\\mathbf{y}_j$, their separation over discrete time $k$ steps is $d_{ij}(k) = \\lVert \\mathbf{y}_{i+k} - \\mathbf{y}_{j+k} \\rVert$. If the system exhibits sensitive dependence on initial conditions, then for small $k$ one expects\n$$\n\\ln d_{ij}(k) \\approx \\ln d_{ij}(0) + \\lambda_{\\max} \\cdot k.\n$$\nA practical estimator averages over many nearest-neighbor pairs and fits a straight line to $\\ln d(k)$ versus $k$ for an initial range of $k$.\n\nProgram requirements:\n1. Implement delay embedding and recurrence plot construction as defined above for sliding windows of the time series. In each window:\n   - Standardize the data to zero mean and unit variance before embedding.\n   - Use a fixed threshold $\\varepsilon$ for the recurrence matrix after standardization.\n   - Use a Theiler window of $T$ samples to exclude trivial recurrences.\n   - Compute determinism $\\mathrm{DET}$ with minimum diagonal length $L_{\\min}$.\n\n2. Segmentation rule:\n   - Compute $\\mathrm{DET}$ on overlapping sliding windows of length $W$ with stride $S$ across the entire series.\n   - Detect change points at window centers whenever $|\\mathrm{DET}_{t} - \\mathrm{DET}_{t-1}| \\ge \\gamma$, where $\\gamma$ is a specified threshold.\n   - Form contiguous segments between change points in the original sample index space.\n   - Discard segments shorter than a specified minimum length $L_{\\text{seg}}$.\n\n3. Validation rule:\n   - For each segment, estimate $\\lambda_{\\max}$ using delay embedding, a Theiler window $T_{\\lambda}$, and a maximum evolution step $K_{\\max}$. For each embedded point index $i$ choose the nearest neighbor index $j$ with $|i-j|  T_{\\lambda}$, compute $d_{ij}(k)$ for $k=1,\\dots,K_{\\max}$ while indices remain valid, average $\\ln d_{ij}(k)$ across all valid pairs, and fit a line to infer $\\lambda_{\\max}$ as the slope with respect to discrete steps $k$.\n   - Define the validation boolean as true if there are at least two segments with finite $\\lambda_{\\max}$ estimates and the range of segment-wise $\\lambda_{\\max}$ values, $\\Delta \\lambda = \\max(\\lambda_{\\max}) - \\min(\\lambda_{\\max})$, satisfies $\\Delta \\lambda \\ge \\Delta_{\\min}$, where $\\Delta_{\\min}$ is a specified minimum difference.\n\nSignal generation:\n- The program must internally synthesize EEG-like signals by concatenating regimes with differing dynamics, using well-studied dynamical sources:\n  - A baseline oscillatory regime defined by $x_n = A \\sin(2\\pi f n / F_s) + \\eta_n$, with discrete time index $n$, amplitude $A$, frequency $f$, sampling rate $F_s$, and independent Gaussian noise $\\eta_n$.\n  - A chaotic regime defined by the logistic map $x_{n+1} = r x_n (1 - x_n)$ in its chaotic regime and optional additive Gaussian noise.\n  - A fully oscillatory regime is defined by concatenating multiple oscillatory segments with potentially different frequencies but remaining quasi-periodic.\n\nTest suite specification:\nProvide the following three parameter sets to form the test suite. Each parameter set is a tuple containing:\n- Series configuration: a list of segments, each defined by a type string and length in samples, followed by parameters controlling dynamics and noise. The legal segment types are \"osc\" (oscillatory) and \"logistic\" (chaotic logistic map).\n- Embedding parameters: $(m, \\tau)$.\n- Recurrence parameters: $(\\varepsilon, T, L_{\\min})$.\n- Windowing and segmentation parameters: $(W, S, \\gamma, L_{\\text{seg}})$.\n- Lyapunov estimation parameters: $(K_{\\max}, T_{\\lambda})$.\n- Validation parameter: $\\Delta_{\\min}$.\n\nUse the exact following test cases:\n1. Case 1 (clear alternation of oscillatory–chaotic–oscillatory):\n   - Series configuration: segments $[\\text{\"osc\"}:1800,\\ \\text{\"logistic\"}:1800,\\ \\text{\"osc\"}:1800]$, with oscillatory $A=1.0$, $f=10$, $F_s=256$, noise standard deviation $\\sigma_{\\text{osc}}=0.05$; logistic parameter $r=3.9$, initial condition $x_0=0.4$, noise standard deviation $\\sigma_{\\text{log}}=0.02$.\n   - Embedding: $(m,\\tau)=(3,8)$.\n   - Recurrence: $(\\varepsilon, T, L_{\\min})=(0.5, 2, 2)$.\n   - Windowing and segmentation: $(W,S,\\gamma,L_{\\text{seg}})=(700, 200, 0.12, 800)$.\n   - Lyapunov: $(K_{\\max}, T_{\\lambda})=(20, 12)$.\n   - Validation: $\\Delta_{\\min}=0.3$.\n\n2. Case 2 (noisier data and stricter validation difference):\n   - Series configuration: segments $[\\text{\"osc\"}:1800,\\ \\text{\"logistic\"}:1800,\\ \\text{\"osc\"}:1800]$, with oscillatory $A=1.0$, $f=10$, $F_s=256$, noise standard deviation $\\sigma_{\\text{osc}}=0.15$; logistic parameter $r=3.7$, initial condition $x_0=0.41$, noise standard deviation $\\sigma_{\\text{log}}=0.10$.\n   - Embedding: $(m,\\tau)=(3,8)$.\n   - Recurrence: $(\\varepsilon, T, L_{\\min})=(0.5, 2, 2)$.\n   - Windowing and segmentation: $(W,S,\\gamma,L_{\\text{seg}})=(700, 200, 0.12, 800)$.\n   - Lyapunov: $(K_{\\max}, T_{\\lambda})=(20, 12)$.\n   - Validation: $\\Delta_{\\min}=0.9$.\n\n3. Case 3 (fully oscillatory data without true dynamic change):\n   - Series configuration: segments $[\\text{\"osc\"}:1800,\\ \\text{\"osc\"}:1800,\\ \\text{\"osc\"}:1800]$, with oscillatory $A=1.0$, base frequency $f=9$ for first segment, $f=10$ for second, $f=11$ for third, $F_s=256$, noise standard deviation $\\sigma_{\\text{osc}}=0.05$ for all segments.\n   - Embedding: $(m,\\tau)=(3,8)$.\n   - Recurrence: $(\\varepsilon, T, L_{\\min})=(0.5, 2, 2).\n   - Windowing and segmentation: $(W,S,\\gamma,L_{\\text{seg}})=(700, 200, 0.12, 800)$.\n   - Lyapunov: $(K_{\\max}, T_{\\lambda})=(20, 12)$.\n   - Validation: $\\Delta_{\\min}=0.3$.\n\nOutput specification:\n- For each of the three test cases, compute the segmentation and validation boolean as defined. The final output must be a single line containing a comma-separated list of the three booleans enclosed in square brackets, for example, $[\\text{True},\\text{False},\\text{True}]$. No additional text may be printed.",
            "solution": "The problem statement has been meticulously reviewed and is determined to be **valid**. It is scientifically grounded in the principles of nonlinear dynamics, specifically phase space reconstruction, recurrence quantification analysis (RQA), and Lyapunov exponent estimation. The problem is well-posed, with all necessary parameters, models, and algorithms defined explicitly, enabling a unique, verifiable computational solution. The language is objective and unambiguous.\n\nThe implementation will proceed by constructing a series of functions that systematically address the required tasks: synthesizing the time series, segmenting it based on changes in dynamical complexity as measured by RQA, and validating this segmentation by comparing the rate of chaotic divergence (maximum Lyapunov exponent) across segments.\n\n### 1. Signal Synthesis\nThe program first synthesizes a discrete-time signal $x_n$ for a time index $n$ by concatenating segments of differing dynamical characteristics.\n-   An **oscillatory regime** is modeled by a sine wave with additive Gaussian noise:\n    $$x_n = A \\sin\\left(\\frac{2\\pi f n}{F_s}\\right) + \\eta_n$$\n    where $A$ is the amplitude, $f$ is the frequency, $F_s$ is the sampling rate, and $\\eta_n$ is a sample from a Gaussian distribution with zero mean and standard deviation $\\sigma_{\\text{osc}}$. This regime is periodic and dynamically simple.\n-   A **chaotic regime** is generated using the logistic map:\n    $$x_{n+1} = r x_n (1 - x_n)$$\n    For the specified parameter values ($r=3.9$ and $r=3.7$), the map exhibits chaotic behavior, characterized by sensitive dependence on initial conditions. Additive Gaussian noise with standard deviation $\\sigma_{\\text{log}}$ can also be included.\n\n### 2. Time Series Segmentation via Recurrence Quantification Analysis (RQA)\n\nThe core of the segmentation logic relies on quantifying changes in the signal's determinism. This is achieved by applying RQA to sliding windows of the time series.\n\n**2.1. Phase Space Reconstruction**\nFor each window of the time series of length $W$, the data is first standardized to have a mean of $0$ and a standard deviation of $1$. Then, its dynamics are reconstructed in a higher-dimensional phase space using the method of time-delay embedding, as justified by Takens' theorem. A scalar time series $\\{x_i\\}$ is transformed into a set of $M$ vectors in $\\mathbb{R}^m$:\n$$\\mathbf{y}_i = (x_i, x_{i+\\tau}, x_{i+2\\tau}, \\dots, x_{i+(m-1)\\tau})$$\nfor $i = 0, \\dots, M-1$, where $m$ is the embedding dimension, $\\tau$ is the time delay, and $M = W - (m-1)\\tau$.\n\n**2.2. Recurrence Matrix and Determinism (DET)**\nA recurrence matrix $R$ is constructed to capture the recurrences of states in the reconstructed phase space. Its elements are defined as:\n$$R_{ij} = \\Theta(\\varepsilon - \\lVert \\mathbf{y}_i - \\mathbf{y}_j \\rVert) \\cdot \\Theta(|i-j| - T)$$\nwhere $\\lVert \\cdot \\rVert$ is the Euclidean norm, $\\varepsilon$ is a distance threshold, and $\\Theta$ is the Heaviside step function. The term involving the Theiler window $T$ excludes points that are close in time, thus removing trivial correlations along the main diagonal.\n\nFrom this matrix, we compute the Determinism ($\\mathrm{DET}$), which measures the fraction of recurrence points forming diagonal lines. These lines correspond to segments of the trajectory that run parallel to each other, indicating deterministic structure. $\\mathrm{DET}$ is defined as:\n$$\\mathrm{DET} = \\frac{\\sum_{\\ell \\ge L_{\\min}} \\ell \\cdot \\mathcal{N}(\\ell)}{\\sum_{i,j} R_{ij}}$$\nwhere $\\mathcal{N}(\\ell)$ is the number of diagonal lines of exactly length $\\ell$, and $L_{\\min}$ is the minimum length considered significant. High $\\mathrm{DET}$ values are characteristic of periodic or quasi-periodic signals, while lower values suggest chaotic or stochastic dynamics.\n\n**2.3. Change Point Detection**\n$\\mathrm{DET}$ is computed for overlapping windows of length $W$ with a stride of $S$. This yields a time series of $\\mathrm{DET}$ values. A change point is identified at the center of a window $t$ if the magnitude of the change in determinism from the previous window exceeds a threshold $\\gamma$:\n$$|\\mathrm{DET}_{t} - \\mathrm{DET}_{t-1}| \\ge \\gamma$$\nThe collection of these change points, along with the start and end of the entire signal, defines the boundaries of the segments. Segments with a length less than $L_{\\text{seg}}$ are discarded.\n\n### 3. Segmentation Validation via Lyapunov Exponents\n\nThe final stage validates the RQA-based segmentation by estimating the maximum Lyapunov exponent ($\\lambda_{\\max}$) for each segment. $\\lambda_{\\max}$ quantifies the average exponential rate of divergence of infinitesimally close trajectories. A positive $\\lambda_{\\max}$ is a hallmark of chaos.\n\n**3.1. Estimating $\\lambda_{\\max}$**\nFor each identified segment, the following procedure is applied:\n1.  The segment's time series is standardized and embedded in phase space with dimension $m$ and delay $\\tau$.\n2.  For each point $\\mathbf{y}_i$ in the embedded space, its nearest neighbor $\\mathbf{y}_j$ is found, subject to the temporal constraint $|i-j|  T_{\\lambda}$ to ensure the points are from different parts of the trajectory.\n3.  The divergence $d_{ij}(k) = \\lVert \\mathbf{y}_{i+k} - \\mathbf{y}_{j+k} \\rVert$ is tracked for $k=1, \\dots, K_{\\max}$ steps forward in time.\n4.  For a chaotic system, the divergence is expected to grow exponentially for small $k$: $d_{ij}(k) \\approx d_{ij}(0) e^{\\lambda_{\\max} k}$. This implies a linear relationship for the logarithm:\n    $$\\ln(d_{ij}(k)) \\approx \\ln(d_{ij}(0)) + \\lambda_{\\max} k$$\n5.  To obtain a robust estimate, the quantity $\\langle \\ln(d(k)) \\rangle$, averaged over all valid starting pairs $(i,j)$, is computed for each evolution step $k$.\n6.  A linear regression is performed on $\\langle \\ln(d(k)) \\rangle$ versus $k$. The slope of the best-fit line provides the estimate for $\\lambda_{\\max}$. The estimate is per discrete time step, as specified.\n\n**3.2. Validation Rule**\nThe segmentation is considered successfully validated if two conditions are met:\n1.  At least two segments yield finite, non-trivial estimates for $\\lambda_{\\max}$.\n2.  The range of these estimates, $\\Delta \\lambda = \\max(\\lambda_{\\max}) - \\min(\\lambda_{\\max})$, is greater than or equal to a specified threshold $\\Delta_{\\min}$.\n\nThis ensures that the segmentation has identified at least two distinct dynamical regimes with a quantitatively significant difference in their chaotic properties. The entire procedure is automated for a test suite of parameter sets, and the boolean validation outcome for each is reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.spatial import cKDTree\n\ndef generate_signal(config, params):\n    \"\"\"\n    Generates a time series by concatenating segments with different dynamics.\n    \"\"\"\n    np.random.seed(0)  # For reproducibility\n    full_series = []\n    time_offset = 0\n\n    osc_params_used = 0\n    for seg_type, seg_length in config:\n        n = np.arange(time_offset, time_offset + seg_length)\n        if seg_type == \"osc\":\n            if 'f_osc' not in params:\n                 params['f_osc'] = 10 # default if not provided\n            if isinstance(params['f_osc'], list):\n                f = params['f_osc'][osc_params_used]\n                osc_params_used += 1\n            else:\n                f = params['f_osc']\n            \n            A = params['A']\n            Fs = params['Fs']\n            sigma_osc = params['sigma_osc']\n            \n            series = A * np.sin(2 * np.pi * f * n / Fs)\n            noise = np.random.normal(0, sigma_osc, seg_length)\n            full_series.append(series + noise)\n            \n        elif seg_type == \"logistic\":\n            r = params['r_log']\n            x0 = params['x0_log']\n            sigma_log = params['sigma_log']\n            \n            series = np.zeros(seg_length)\n            series[0] = x0\n            for i in range(seg_length - 1):\n                series[i+1] = r * series[i] * (1 - series[i])\n            noise = np.random.normal(0, sigma_log, seg_length)\n            full_series.append(series + noise)\n\n        time_offset += seg_length\n        \n    return np.concatenate(full_series)\n\ndef embed_series(series, m, tau):\n    \"\"\"\n    Performs time-delay embedding on a time series.\n    \"\"\"\n    N = len(series)\n    if N  (m - 1) * tau + 1:\n        return np.array([[]])\n    M = N - (m - 1) * tau\n    embedded = np.zeros((M, m))\n    for i in range(M):\n        embedded[i] = series[i : i + m * tau : tau]\n    return embedded\n\ndef calculate_det(series, m, tau, epsilon, T_theiler, L_min):\n    \"\"\"\n    Calculates the Determinism (DET) for a time series segment.\n    \"\"\"\n    # 1. Standardize\n    std = np.std(series)\n    if std == 0: return 1.0  # Perfectly deterministic\n    standardized_series = (series - np.mean(series)) / std\n\n    # 2. Embed\n    Y = embed_series(standardized_series, m, tau)\n    if Y.shape[0]  2: return 0.0\n\n    # 3. Recurrence Matrix\n    dist_matrix = squareform(pdist(Y, 'euclidean'))\n    R = dist_matrix = epsilon\n    \n    # Apply Theiler window\n    M = R.shape[0]\n    for i in range(M):\n        for j in range(max(0, i - T_theiler), min(M, i + T_theiler + 1)):\n            R[i, j] = 0\n            \n    # 4. Calculate DET\n    N_R = np.sum(R)\n    if N_R == 0: return 0.0\n\n    diag_sum = 0\n    for offset in range(1, M - L_min + 1): # Only check upper triangle diagonals\n        diag = np.diagonal(R, offset=offset)\n        if diag.size  L_min: continue\n        \n        padded_diag = np.concatenate(([0], diag, [0]))\n        diffs = np.diff(padded_diag)\n        starts = np.where(diffs == 1)[0]\n        ends = np.where(diffs == -1)[0]\n        lengths = ends - starts\n        \n        for l in lengths:\n            if l >= L_min:\n                diag_sum += l\n                \n    return diag_sum / N_R if N_R > 0 else 0.0\n\ndef segment_series(series, m, tau, epsilon, T, L_min, W, S, gamma, L_seg):\n    \"\"\"\n    Segments the time series based on changes in DET.\n    \"\"\"\n    det_values = []\n    window_centers = []\n    \n    for i in range(0, len(series) - W + 1, S):\n        window = series[i : i + W]\n        det = calculate_det(window, m, tau, epsilon, T, L_min)\n        det_values.append(det)\n        window_centers.append(i + W // 2)\n\n    change_points = []\n    for i in range(1, len(det_values)):\n        if np.abs(det_values[i] - det_values[i-1]) >= gamma:\n            center_idx = (i * S) + W // 2\n            change_points.append(center_idx)\n    \n    boundaries = sorted(list(set([0] + change_points + [len(series)])))\n    \n    segments = []\n    for i in range(len(boundaries) - 1):\n        start, end = boundaries[i], boundaries[i+1]\n        if end - start >= L_seg:\n            segments.append((start, end))\n\n    if not segments:\n        if len(series) >= L_seg:\n            segments.append((0, len(series)))\n\n    return segments\n\ndef estimate_lambda_max(segment, m, tau, K_max, T_lambda):\n    \"\"\"\n    Estimates the maximum Lyapunov exponent for a time series segment.\n    \"\"\"\n    std = np.std(segment)\n    if std == 0: return 0.0\n    series = (segment - np.mean(segment)) / std\n    \n    Y = embed_series(series, m, tau)\n    M = Y.shape[0]\n    \n    if M  2 or M  K_max or M  T_lambda + 2:\n        return np.nan\n\n    try:\n        tree = cKDTree(Y)\n    except Exception:\n        return np.nan\n\n    log_divergences = np.zeros(K_max)\n    counts = np.zeros(K_max, dtype=int)\n    \n    for i in range(M):\n        try:\n            dists, inds = tree.query(Y[i], k=T_lambda + 10)\n        except Exception:\n            continue\n            \n        neighbor_idx = -1\n        for j in range(1, len(inds)): # Start at 1 to skip self\n            if abs(i - inds[j]) > T_lambda:\n                neighbor_idx = inds[j]\n                break\n        \n        if neighbor_idx == -1:\n            continue\n\n        for k in range(1, K_max + 1):\n            if i + k  M and neighbor_idx + k  M:\n                dist = np.linalg.norm(Y[i+k] - Y[neighbor_idx+k])\n                if dist > 1e-12:\n                    log_divergences[k-1] += np.log(dist)\n                    counts[k-1] += 1\n\n    avg_log_d = np.full(K_max, np.nan)\n    valid_k = counts > 0\n    avg_log_d[valid_k] = log_divergences[valid_k] / counts[valid_k]\n    \n    valid_points = ~np.isnan(avg_log_d)\n    if np.sum(valid_points)  2:\n        return np.nan\n        \n    k_steps = np.arange(1, K_max + 1)\n    k_fit = k_steps[valid_points]\n    d_fit = avg_log_d[valid_points]\n    \n    try:\n        slope, _, _, _, _ = linregress(k_fit, d_fit)\n    except ValueError:\n        return np.nan\n        \n    return slope\n\ndef process_case(params):\n    \"\"\"\n    Processes a single test case from generation to validation.\n    \"\"\"\n    series_config = params['series_config']\n    series_params = params['series_params']\n    m, tau = params['embedding']\n    epsilon, T, L_min = params['recurrence']\n    W, S, gamma, L_seg = params['windowing']\n    K_max, T_lambda = params['lyapunov']\n    Delta_min = params['validation']\n    \n    signal = generate_signal(series_config, series_params)\n    segments = segment_series(signal, m, tau, epsilon, T, L_min, W, S, gamma, L_seg)\n    \n    lambda_estimates = []\n    for start, end in segments:\n        segment_data = signal[start:end]\n        l_max = estimate_lambda_max(segment_data, m, tau, K_max, T_lambda)\n        lambda_estimates.append(l_max)\n    \n    finite_estimates = [x for x in lambda_estimates if np.isfinite(x)]\n    \n    if len(finite_estimates)  2:\n        return False\n        \n    delta_lambda = np.max(finite_estimates) - np.min(finite_estimates)\n    \n    return delta_lambda >= Delta_min\n\ndef solve():\n    test_cases = [\n        # Case 1\n        {\n            \"series_config\": [(\"osc\", 1800), (\"logistic\", 1800), (\"osc\", 1800)],\n            \"series_params\": {\n                \"A\": 1.0, \"f_osc\": 10, \"Fs\": 256, \"sigma_osc\": 0.05,\n                \"r_log\": 3.9, \"x0_log\": 0.4, \"sigma_log\": 0.02\n            },\n            \"embedding\": (3, 8),\n            \"recurrence\": (0.5, 2, 2),\n            \"windowing\": (700, 200, 0.12, 800),\n            \"lyapunov\": (20, 12),\n            \"validation\": 0.3\n        },\n        # Case 2\n        {\n            \"series_config\": [(\"osc\", 1800), (\"logistic\", 1800), (\"osc\", 1800)],\n            \"series_params\": {\n                \"A\": 1.0, \"f_osc\": 10, \"Fs\": 256, \"sigma_osc\": 0.15,\n                \"r_log\": 3.7, \"x0_log\": 0.41, \"sigma_log\": 0.10\n            },\n            \"embedding\": (3, 8),\n            \"recurrence\": (0.5, 2, 2),\n            \"windowing\": (700, 200, 0.12, 800),\n            \"lyapunov\": (20, 12),\n            \"validation\": 0.9\n        },\n        # Case 3\n        {\n            \"series_config\": [(\"osc\", 1800), (\"osc\", 1800), (\"osc\", 1800)],\n            \"series_params\": {\n                \"A\": 1.0, \"f_osc\": [9, 10, 11], \"Fs\": 256, \"sigma_osc\": 0.05,\n                \"r_log\": 3.9, \"x0_log\": 0.4, \"sigma_log\": 0.02 # Not used, but for structure\n            },\n            \"embedding\": (3, 8),\n            \"recurrence\": (0.5, 2, 2),\n            \"windowing\": (700, 200, 0.12, 800),\n            \"lyapunov\": (20, 12),\n            \"validation\": 0.3\n        }\n    ]\n\n    results = [process_case(case) for case in test_cases]\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}