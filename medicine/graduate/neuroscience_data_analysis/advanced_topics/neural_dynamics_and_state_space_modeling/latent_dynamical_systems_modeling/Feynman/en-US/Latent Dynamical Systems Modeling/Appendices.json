{
    "hands_on_practices": [
        {
            "introduction": "The first step in applying latent dynamical systems is fitting the model to observed neural data, a process that typically involves maximizing a log-likelihood function. To perform this optimization efficiently, we need the function's derivatives with respect to the model parameters. This exercise  walks you through the essential calculus of deriving the gradient and Hessian for a Poisson observation model, providing the core components for the powerful gradient-based algorithms used to train these systems.",
            "id": "4173371",
            "problem": "Consider a latent dynamical systems model used in neuroscience data analysis, where multi-neuron spike counts are modeled as conditionally independent Poisson random variables given a fixed latent trajectory. Let there be $n$ neurons recorded over $T$ time bins, and a latent state of dimension $k$ represented by $\\{x_t\\}_{t=1}^{T}$ with $x_t \\in \\mathbb{R}^{k}$. The observed spike counts are $\\{y_t\\}_{t=1}^{T}$ with $y_t \\in \\mathbb{N}^{n}$, and the conditional intensity at time $t$ is modeled by the element-wise exponential link\n$$\n\\lambda_t = \\exp(C x_t + d),\n$$\nwhere $C \\in \\mathbb{R}^{n \\times k}$ and $d \\in \\mathbb{R}^{n}$ are the parameters of interest and $\\exp(\\cdot)$ acts element-wise. The Poisson log-likelihood, up to an additive constant independent of $(C,d)$, is\n$$\n\\mathcal{L}(C,d) = \\sum_{t=1}^{T} y_t^{\\top} (C x_t + d) - \\mathbf{1}^{\\top} \\exp(C x_t + d),\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{n}$ is the vector of ones.\n\nStarting from first principles of the Poisson model, chain rule for multivariate calculus, and basic matrix calculus identities, derive closed-form expressions for:\n- the gradient of $\\mathcal{L}(C,d)$ with respect to $C$ and $d$, and\n- the Hessian of $\\mathcal{L}(C,d)$ with respect to $(C,d)$ in block form.\n\nYour derivation must clearly indicate the dependence on $\\{x_t\\}_{t=1}^{T}$ and $\\{y_t\\}_{t=1}^{T}$ and use standard operators, including the vectorization operator $\\operatorname{vec}(\\cdot)$ and the Kronecker product $\\otimes$ when needed. You may use the identity $\\operatorname{vec}(A X B) = (B^{\\top} \\otimes A)\\operatorname{vec}(X)$ for conformable matrices $A, X, B$. Assume $\\exp(\\cdot)$ denotes the element-wise exponential and $\\operatorname{diag}(v)$ denotes the diagonal matrix formed from the vector $v$.\n\nExpress your final answer as analytic expressions. The final answer must present both the gradient and the Hessian together as a single composite object in a row matrix using the $\\operatorname{pmatrix}$ environment. No numerical approximation is required.",
            "solution": "The user wants me to act as a meticulous STEM professor and solve a problem from the field of computational neuroscience. The first step is to validate the problem statement.\n\n### Problem Validation\n**1. Extraction of Givens:**\n- **Model:** A latent dynamical systems model for multi-neuron spike counts.\n- **Data:** $n$ neurons, $T$ time bins.\n- **Latent States:** $\\{x_t\\}_{t=1}^{T}$, where $x_t \\in \\mathbb{R}^{k}$.\n- **Observed Data:** $\\{y_t\\}_{t=1}^{T}$, where $y_t \\in \\mathbb{N}^{n}$ (spike counts).\n- **Parameters:** $C \\in \\mathbb{R}^{n \\times k}$ and $d \\in \\mathbb{R}^{n}$.\n- **Conditional Intensity Function:** $\\lambda_t = \\exp(C x_t + d)$, where $\\exp(\\cdot)$ is element-wise.\n- **Objective Function:** The Poisson log-likelihood, up to a constant, is given by $\\mathcal{L}(C,d) = \\sum_{t=1}^{T} \\left( y_t^{\\top} (C x_t + d) - \\mathbf{1}^{\\top} \\exp(C x_t + d) \\right)$, where $\\mathbf{1} \\in \\mathbb{R}^{n}$ is a vector of ones.\n- **Task:** Derive expressions for the gradient of $\\mathcal{L}(C,d)$ with respect to $(C,d)$ and the Hessian of $\\mathcal{L}(C,d)$ with respect to $(C,d)$ in block form.\n- **Provided Identities/Tools:** Chain rule, matrix calculus identities, $\\operatorname{vec}(\\cdot)$, $\\otimes$, $\\operatorname{diag}(\\cdot)$, and $\\operatorname{vec}(A X B) = (B^{\\top} \\otimes A)\\operatorname{vec}(X)$.\n\n**2. Validation using Extracted Givens:**\n- **Scientific Grounding:** The problem describes a Poisson Generalized Linear Model (GLM) with an exponential link function, conditioned on a latent state. This is a standard and widely used framework in statistical neuroscience for modeling spike train data. The log-likelihood function provided is the correct form for a Poisson distribution, omitting terms that are constant with respect to the parameters $(C,d)$. The problem is scientifically sound and factually correct.\n- **Well-Posedness:** The task is to compute the first and second derivatives of a well-defined, smooth function. This is a standard calculus problem that admits a unique and meaningful solution, essential for optimization algorithms used to fit such models.\n- **Objectivity:** The problem is stated in precise mathematical language with no subjective or ambiguous terms.\n\nThe problem is self-contained, consistent, and scientifically grounded. It does not violate any of the specified invalidity criteria. Therefore, the problem is deemed **valid**.\n\n### Solution Derivation\nThe log-likelihood function is $\\mathcal{L}(C,d) = \\sum_{t=1}^{T} \\mathcal{L}_t(C,d)$, where $\\mathcal{L}_t(C,d) = y_t^{\\top} (C x_t + d) - \\mathbf{1}^{\\top} \\exp(C x_t + d)$. The derivative operators are linear, so we can compute the derivatives for a single time step $\\mathcal{L}_t$ and then sum over $t$.\n\nLet's define the linear input to the exponential function as $u_t = C x_t + d$, where $u_t \\in \\mathbb{R}^{n}$. The conditional intensity is then $\\lambda_t = \\exp(u_t)$, where the exponential is applied element-wise. The single time-step log-likelihood is $\\mathcal{L}_t = y_t^{\\top} u_t - \\mathbf{1}^{\\top} \\lambda_t$.\n\n**1. Gradient Calculation**\n\nFirst, we compute the gradient of $\\mathcal{L}_t$ with respect to the intermediate variable $u_t$. This is a vector of partial derivatives, $\\frac{\\partial \\mathcal{L}_t}{\\partial u_{it}}$, for $i \\in \\{1, \\dots, n\\}$.\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial u_{it}} = \\frac{\\partial}{\\partial u_{it}} \\left( \\sum_{j=1}^{n} y_{jt} u_{jt} - \\sum_{j=1}^{n} \\exp(u_{jt}) \\right) = y_{it} - \\exp(u_{it}) = y_{it} - \\lambda_{it}\n$$\nIn vector form, the gradient with respect to $u_t$ is:\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial u_t} = y_t - \\lambda_t\n$$\n\nNow we use the multivariate chain rule to find the gradients with respect to $d$ and $C$.\n\n**Gradient with respect to $d$**:\nThe derivative of $u_t = C x_t + d$ with respect to $d$ is the identity matrix, $\\frac{\\partial u_t}{\\partial d} = I_n \\in \\mathbb{R}^{n \\times n}$.\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial d} = \\left(\\frac{\\partial u_t}{\\partial d}\\right)^{\\top} \\frac{\\partial \\mathcal{L}_t}{\\partial u_t} = I_n^{\\top} (y_t - \\lambda_t) = y_t - \\lambda_t\n$$\nSumming over all time steps $t$:\n$$\n\\nabla_d \\mathcal{L}(C,d) = \\sum_{t=1}^{T} (y_t - \\lambda_t) = \\sum_{t=1}^{T} (y_t - \\exp(C x_t + d))\n$$\n\n**Gradient with respect to $C$**:\nThis is a matrix derivative. The $(i,j)$-th element of the gradient matrix is $\\frac{\\partial \\mathcal{L}_t}{\\partial C_{ij}}$.\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial C_{ij}} = \\sum_{p=1}^{n} \\frac{\\partial \\mathcal{L}_t}{\\partial u_{pt}} \\frac{\\partial u_{pt}}{\\partial C_{ij}}\n$$\nThe term $u_{pt} = \\sum_{q=1}^{k} C_{pq} x_{qt} + d_p$. Its derivative with respect to $C_{ij}$ is $\\frac{\\partial u_{pt}}{\\partial C_{ij}} = \\delta_{pi} x_{jt}$, where $\\delta_{pi}$ is the Kronecker delta.\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial C_{ij}} = \\frac{\\partial \\mathcal{L}_t}{\\partial u_{it}} \\frac{\\partial u_{it}}{\\partial C_{ij}} = (y_{it} - \\lambda_{it}) x_{jt}\n$$\nThis is the $(i,j)$-th entry of an $n \\times k$ matrix. The full matrix is the outer product of the vector $(y_t - \\lambda_t)$ and the vector $x_t$.\n$$\n\\nabla_C \\mathcal{L}_t(C,d) = (y_t - \\lambda_t) x_t^{\\top}\n$$\nSumming over all time steps $t$:\n$$\n\\nabla_C \\mathcal{L}(C,d) = \\sum_{t=1}^{T} (y_t - \\lambda_t) x_t^{\\top} = \\sum_{t=1}^{T} (y_t - \\exp(C x_t + d)) x_t^{\\top}\n$$\n\n**2. Hessian Calculation**\n\nThe Hessian is the matrix of second derivatives. We consider a combined parameter vector $\\theta = \\begin{pmatrix} \\operatorname{vec}(C) \\\\ d \\end{pmatrix}$. The Hessian matrix $H = \\nabla^2_{\\theta} \\mathcal{L}(C,d)$ has a $2 \\times 2$ block structure:\n$$\nH = \\begin{pmatrix} \\frac{\\partial^2 \\mathcal{L}}{\\partial \\operatorname{vec}(C) \\partial \\operatorname{vec}(C)^\\top}  \\frac{\\partial^2 \\mathcal{L}}{\\partial \\operatorname{vec}(C) \\partial d^\\top} \\\\ \\frac{\\partial^2 \\mathcal{L}}{\\partial d \\partial \\operatorname{vec}(C)^\\top}  \\frac{\\partial^2 \\mathcal{L}}{\\partial d \\partial d^\\top} \\end{pmatrix} = \\sum_{t=1}^{T} H_t\n$$\nWe will calculate the blocks for a single time step $t$ and then sum.\n\n**Bottom-right block ($H_{dd}$)**:\nWe differentiate $\\frac{\\partial \\mathcal{L}_t}{\\partial d} = y_t - \\lambda_t$ with respect to $d^{\\top}$.\n$$\n\\frac{\\partial^2 \\mathcal{L}_t}{\\partial d \\partial d^{\\top}} = \\frac{\\partial}{\\partial d^{\\top}}(y_t - \\lambda_t) = -\\frac{\\partial \\lambda_t}{\\partial d^{\\top}} = -\\frac{\\partial \\exp(u_t)}{\\partial d^{\\top}} = -\\frac{\\partial \\exp(u_t)}{\\partial u_t^{\\top}} \\frac{\\partial u_t}{\\partial d^{\\top}}\n$$\nThe Jacobian $\\frac{\\partial \\exp(u_t)}{\\partial u_t^{\\top}}$ is a diagonal matrix with entries $\\exp(u_{it}) = \\lambda_{it}$, so it is $\\operatorname{diag}(\\lambda_t)$. The Jacobian $\\frac{\\partial u_t}{\\partial d^{\\top}}$ is $I_n$.\n$$\n\\frac{\\partial^2 \\mathcal{L}_t}{\\partial d \\partial d^{\\top}} = -\\operatorname{diag}(\\lambda_t) I_n = -\\operatorname{diag}(\\lambda_t)\n$$\n\n**Off-diagonal blocks ($H_{dC}$ and $H_{Cd}$)**:\nFor the bottom-left block, we differentiate $\\frac{\\partial \\mathcal{L}_t}{\\partial d} = y_t - \\lambda_t$ with respect to $\\operatorname{vec}(C)^{\\top}$.\n$$\n\\frac{\\partial^2 \\mathcal{L}_t}{\\partial d \\partial \\operatorname{vec}(C)^{\\top}} = -\\frac{\\partial \\lambda_t}{\\partial \\operatorname{vec}(C)^{\\top}} = -\\frac{\\partial \\lambda_t}{\\partial u_t^{\\top}} \\frac{\\partial u_t}{\\partial \\operatorname{vec}(C)^{\\top}} = -\\operatorname{diag}(\\lambda_t) \\frac{\\partial u_t}{\\partial \\operatorname{vec}(C)^{\\top}}\n$$\nWe have $u_t = C x_t + d$. Using the provided identity $\\operatorname{vec}(AXB) = (B^\\top \\otimes A) \\operatorname{vec}(X)$, we write $C x_t = C x_t I_1$ and get $\\operatorname{vec}(C x_t) = (x_t^\\top \\otimes C) \\operatorname{vec}(I_1) = (x_t^\\top \\otimes C)$. This is not helpful. Instead, let's write $C x_t = I_n C x_t$. Then $\\operatorname{vec}(C x_t) = u_t - d = (x_t^\\top \\otimes I_n) \\operatorname{vec}(C)$.\nSo, $\\frac{\\partial u_t}{\\partial \\operatorname{vec}(C)^{\\top}} = x_t^\\top \\otimes I_n$.\n$$\nH_{dC,t} = \\frac{\\partial^2 \\mathcal{L}_t}{\\partial d \\partial \\operatorname{vec}(C)^{\\top}} = -\\operatorname{diag}(\\lambda_t) (x_t^\\top \\otimes I_n)\n$$\nIt can be shown that $\\operatorname{diag}(v)(w^\\top \\otimes I) = w^\\top \\otimes \\operatorname{diag}(v)$. So, $H_{dC,t} = -x_t^\\top \\otimes \\operatorname{diag}(\\lambda_t)$.\nThe top-right block is its transpose: $H_{Cd,t} = (H_{dC,t})^{\\top} = (-x_t^\\top \\otimes \\operatorname{diag}(\\lambda_t))^{\\top} = -x_t \\otimes \\operatorname{diag}(\\lambda_t)$.\n\n**Top-left block ($H_{CC}$)**:\nWe differentiate $\\nabla_C \\mathcal{L}_t = (y_t - \\lambda_t) x_t^\\top$ with respect to $C$. It is easier to differentiate the vectorized form $\\operatorname{vec}(\\nabla_C \\mathcal{L}_t)$ with respect to $\\operatorname{vec}(C)^\\top$.\nUsing $\\operatorname{vec}(ab^\\top) = b \\otimes a$, we have $\\operatorname{vec}((y_t - \\lambda_t)x_t^\\top) = x_t \\otimes (y_t - \\lambda_t)$.\n$$\nH_{CC,t} = \\frac{\\partial \\operatorname{vec}(\\nabla_C \\mathcal{L}_t)}{\\partial \\operatorname{vec}(C)^\\top} = \\frac{\\partial}{\\partial \\operatorname{vec}(C)^\\top} \\left( x_t \\otimes (y_t - \\lambda_t) \\right) = x_t \\otimes \\frac{\\partial(y_t - \\lambda_t)}{\\partial \\operatorname{vec}(C)^\\top}\n$$\nThe last step holds because $x_t$ is not a function of $C$. The derivative term is $\\frac{-\\partial \\lambda_t}{\\partial \\operatorname{vec}(C)^\\top} = - \\operatorname{diag}(\\lambda_t) (x_t^\\top \\otimes I_n)$.\n$$\nH_{CC,t} = x_t \\otimes \\left(-\\operatorname{diag}(\\lambda_t) (x_t^\\top \\otimes I_n)\\right) = -(x_t \\otimes I_n) (\\operatorname{diag}(\\lambda_t) (x_t^\\top \\otimes I_n))\n$$\nUsing the identity $(A \\otimes B)(C \\otimes D)=(AC \\otimes BD)$ is complicated here. As demonstrated by direct expansion, $(A \\otimes I)D(A^\\top \\otimes I) = (A A^\\top) \\otimes D$ where $D$ is diagonal.\n$$\nH_{CC,t} = -(x_t x_t^\\top) \\otimes \\operatorname{diag}(\\lambda_t)\n$$\n\n**Summary of Results**\nLet $\\lambda_t = \\exp(C x_t + d)$.\n\nThe gradient components are:\n$$\n\\nabla_C \\mathcal{L}(C,d) = \\sum_{t=1}^{T} (y_t - \\lambda_t) x_t^{\\top}\n$$\n$$\n\\nabla_d \\mathcal{L}(C,d) = \\sum_{t=1}^{T} (y_t - \\lambda_t)\n$$\nThe vectorized gradient for the full parameter vector $\\theta = \\begin{pmatrix} \\operatorname{vec}(C) \\\\ d \\end{pmatrix}$ is:\n$$\n\\nabla_\\theta \\mathcal{L} = \\begin{pmatrix} \\operatorname{vec}(\\nabla_C \\mathcal{L}) \\\\ \\nabla_d \\mathcal{L} \\end{pmatrix} = \\sum_{t=1}^{T} \\begin{pmatrix} x_t \\otimes (y_t - \\lambda_t) \\\\ y_t - \\lambda_t \\end{pmatrix}\n$$\nThe Hessian matrix in block form is $H = \\sum_{t=1}^{T} H_t$, where:\n$$\nH_t = -\\begin{pmatrix} (x_t x_t^\\top) \\otimes \\operatorname{diag}(\\lambda_t)  x_t \\otimes \\operatorname{diag}(\\lambda_t) \\\\ x_t^\\top \\otimes \\operatorname{diag}(\\lambda_t)  \\operatorname{diag}(\\lambda_t) \\end{pmatrix}\n$$\nThis can be written compactly. Let $\\tilde{x}_t = \\begin{pmatrix} x_t \\\\ 1 \\end{pmatrix}$. Then the Hessian block is $H_t = -(\\tilde{x}_t \\tilde{x}_t^\\top) \\otimes \\operatorname{diag}(\\lambda_t)$.\nThe total Hessian is:\n$$\nH = -\\sum_{t=1}^{T} (\\tilde{x}_t \\tilde{x}_t^\\top) \\otimes \\operatorname{diag}(\\lambda_t)\n$$\n\nFinal expressions for the required objects:\n**Gradient:**\n$$ \\nabla \\mathcal{L}(C,d) = \\begin{pmatrix} \\operatorname{vec}\\left(\\sum_{t=1}^{T} (y_t - \\lambda_t) x_t^{\\top}\\right) \\\\ \\sum_{t=1}^{T} (y_t - \\lambda_t) \\end{pmatrix} $$\n**Hessian:**\n$$ H(C,d) = -\\sum_{t=1}^{T} \\begin{pmatrix} (x_t x_t^\\top) \\otimes \\operatorname{diag}(\\lambda_t)  x_t \\otimes \\operatorname{diag}(\\lambda_t) \\\\ x_t^\\top \\otimes \\operatorname{diag}(\\lambda_t)  \\operatorname{diag}(\\lambda_t) \\end{pmatrix} $$\nwhere in both expressions, $\\lambda_t = \\exp(C x_t + d)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\begin{pmatrix} \\operatorname{vec}\\left(\\sum_{t=1}^{T} (y_t - \\exp(C x_t + d)) x_t^{\\top}\\right) \\\\ \\sum_{t=1}^{T} (y_t - \\exp(C x_t + d)) \\end{pmatrix}\n\n-\\sum_{t=1}^{T} \\begin{pmatrix} (x_t x_t^\\top) \\otimes \\operatorname{diag}(\\exp(C x_t + d))  x_t \\otimes \\operatorname{diag}(\\exp(C x_t + d)) \\\\ x_t^\\top \\otimes \\operatorname{diag}(\\exp(C x_t + d))  \\operatorname{diag}(\\exp(C x_t + d)) \\end{pmatrix}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "After fitting a model, the learned vector field $f_{\\theta}(\\mathbf{x})$ represents a concrete hypothesis about how neural population activity evolves. A powerful method for interpreting these complex, nonlinear dynamics is to analyze the system's fixed points and their stability, which may correspond to persistent or default states of a neural circuit. In this practice , you will apply fundamental principles of dynamical systems theory to linearize a system around a candidate fixed point and use the eigenvalues of the Jacobian matrix to assess its stability.",
            "id": "4173330",
            "problem": "Consider a latent autonomous continuous-time dynamical system for neural population activity modeled as an Ordinary Differential Equation (ODE): $$\\frac{d\\mathbf{x}}{dt} = f_{\\theta}(\\mathbf{x}),$$ where the estimated vector field is defined componentwise via the elementwise hyperbolic tangent as $$f_{\\theta}(\\mathbf{x}) = \\mathbf{W}\\,\\tanh\\!\\big(\\mathbf{V}\\,\\mathbf{x} + \\mathbf{b}\\big) + \\mathbf{C}\\,\\mathbf{x} + \\mathbf{d},$$ with parameter tuple $\\theta = (\\mathbf{W}, \\mathbf{V}, \\mathbf{C}, \\mathbf{b}, \\mathbf{d})$. A candidate fixed point $\\mathbf{x}^{\\star}$ satisfies $f_{\\theta}(\\mathbf{x}^{\\star}) = \\mathbf{0}$. Local linearization around $\\mathbf{x}^{\\star}$ uses the Jacobian matrix $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$, defined by core multivariable calculus principles (the chain rule and linearization definition), and local stability for continuous-time systems is determined by the eigenvalues of $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$: the fixed point is asymptotically stable if and only if the real parts of all eigenvalues are strictly negative. No physical units are involved in this problem.\n\nYour task is to write a complete program that, for each provided test case, performs the following steps purely from first principles:\n- Compute the Jacobian $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$ at the given candidate fixed point using the definition of the Jacobian and the chain rule for compositions of linear maps and elementwise $\\tanh(\\cdot)$.\n- Compute the eigenvalues of $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$.\n- Decide the stability of the candidate fixed point by checking whether all eigenvalues have strictly negative real parts.\n- Return a boolean result for each test case: `True` if the candidate fixed point is asymptotically stable, and `False` otherwise.\n\nThe elementwise derivative of the hyperbolic tangent is a well-tested mathematical fact: $$\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^{2}(z).$$ Use this fact together with the chain rule to construct the Jacobian for the composite vector field.\n\nTest Suite. Use the following scientifically consistent parameter sets. Matrices and vectors are specified explicitly:\n\n- Test case $1$ (dimension $2$, nonlinearity present, stable):\n  $$\\mathbf{W} = \\begin{bmatrix} -0.3  0 \\\\ 0  -0.3 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.4  0 \\\\ 0  -0.4 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.$$\n\n- Test case $2$ (dimension $2$, saddle-type behavior):\n  $$\\mathbf{W} = \\begin{bmatrix} 0.2  0 \\\\ 0  0.2 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 3.0  0 \\\\ 0  0.2 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.1  0 \\\\ 0  -0.05 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.$$\n\n- Test case $3$ (dimension $3$, purely linear unstable):\n  $$\\mathbf{W} = \\begin{bmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} 0.1  0  0 \\\\ 0  0.2  0 \\\\ 0  0  0.3 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.$$\n\n- Test case $4$ (dimension $2$, marginal with a zero eigenvalue):\n  $$\\mathbf{W} = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.1  0 \\\\ 0  0.0 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.$$\n\n- Test case $5$ (dimension $1$, nontrivial nonlinearity at nonzero argument, stable):\n  $$\\mathbf{W} = \\begin{bmatrix} -0.8 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 2.5 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.05 \\end{bmatrix},$$\n  $$\\mathbf{x}^{\\star} = \\begin{bmatrix} 0.5 \\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix} -0.05 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0.6919232 \\end{bmatrix}.$$\n\nFor each test case, compute the boolean stability result as described. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example `[result1,result2,result3,result4,result5]`.",
            "solution": "The problem requires an analysis of the local asymptotic stability of candidate fixed points for a given continuous-time autonomous dynamical system. The system's evolution is described by the ordinary differential equation (ODE) $\\frac{d\\mathbf{x}}{dt} = f_{\\theta}(\\mathbf{x})$, where the state vector $\\mathbf{x}$ represents neural population activity and the vector field $f_{\\theta}(\\mathbf{x})$ is parameterized by $\\theta = (\\mathbf{W}, \\mathbf{V}, \\mathbf{C}, \\mathbf{b}, \\mathbf{d})$. A point $\\mathbf{x}^{\\star}$ is a fixed point if it satisfies the condition $f_{\\theta}(\\mathbf{x}^{\\star}) = \\mathbf{0}$.\n\nAccording to the Hartman-Grobman theorem and the principle of linearization for continuous-time systems, the local stability of a fixed point $\\mathbf{x}^{\\star}$ is determined by the eigenvalues of the Jacobian matrix of the vector field, $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x})$, evaluated at that fixed point. A fixed point $\\mathbf{x}^{\\star}$ is locally asymptotically stable if and only if all eigenvalues of the Jacobian matrix $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$ have strictly negative real parts.\n\nThe core of the task is to derive the analytical form of the Jacobian matrix $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x})$ and then use it to assess the stability for each provided test case. The vector field is given by:\n$$f_{\\theta}(\\mathbf{x}) = \\mathbf{W}\\,\\tanh\\!\\big(\\mathbf{V}\\,\\mathbf{x} + \\mathbf{b}\\big) + \\mathbf{C}\\,\\mathbf{x} + \\mathbf{d}$$\nThe Jacobian matrix $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x})$ is a matrix of partial derivatives, where the element $(i, j)$ is given by $J_{ij} = \\frac{\\partial f_i}{\\partial x_j}$. Due to the linearity of the differentiation operator, the Jacobian of a sum of vector functions is the sum of their individual Jacobians. We can thus analyze each term separately.\n\n$1$. The term $\\mathbf{C}\\mathbf{x}$ is a linear transformation of $\\mathbf{x}$. Its Jacobian is simply the matrix $\\mathbf{C}$.\n$2$. The term $\\mathbf{d}$ is a constant vector. Its derivative with respect to $\\mathbf{x}$ is the zero matrix, $\\mathbf{0}$.\n$3$. The first term, $\\mathbf{g}(\\mathbf{x}) = \\mathbf{W}\\,\\tanh(\\mathbf{V}\\,\\mathbf{x} + \\mathbf{b})$, is a composition of functions. We must apply the multivariable chain rule. Let us define the composition as follows:\n- Let $\\mathbf{u}(\\mathbf{x}) = \\mathbf{V}\\mathbf{x} + \\mathbf{b}$. This is an affine transformation. Its Jacobian with respect to $\\mathbf{x}$ is $\\mathbf{J}_{\\mathbf{u}} = \\mathbf{V}$.\n- Let $\\mathbf{h}(\\mathbf{u}) = \\tanh(\\mathbf{u})$, where the $\\tanh$ function is applied elementwise. The $k$-th component is $h_k(\\mathbf{u}) = \\tanh(u_k)$. The partial derivative $\\frac{\\partial h_k}{\\partial u_l}$ is non-zero only if $k=l$. Therefore, the Jacobian of $\\mathbf{h}$ with respect to $\\mathbf{u}$ is a diagonal matrix. Using the provided identity $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$, the diagonal entries of this Jacobian are $\\frac{d h_k}{d u_k} = 1 - \\tanh^2(u_k)$. We can represent this Jacobian matrix as:\n$$ \\mathbf{J}_{\\mathbf{h}}(\\mathbf{u}) = \\text{diag}\\left(1 - \\tanh^2(\\mathbf{u})\\right) $$\nwhere the expression in the $\\text{diag}(\\cdot)$ operator is a vector whose components are $1 - \\tanh^2(u_k)$.\n- The full term is a linear transformation by $\\mathbf{W}$ applied to $\\mathbf{h}(\\mathbf{u}(\\mathbf{x}))$. The Jacobian of this final linear transformation is just $\\mathbf{W}$.\n\nBy the chain rule, the Jacobian of the composite function $\\mathbf{g}(\\mathbf{x}) = \\mathbf{W}(\\mathbf{h}(\\mathbf{u}(\\mathbf{x})))$ is the product of the Jacobians of its constituent parts, evaluated at the appropriate points:\n$$ \\mathbf{J}_{\\mathbf{g}}(\\mathbf{x}) = \\mathbf{W} \\cdot \\mathbf{J}_{\\mathbf{h}}(\\mathbf{u}(\\mathbf{x})) \\cdot \\mathbf{J}_{\\mathbf{u}}(\\mathbf{x}) $$\nSubstituting the expressions for the component Jacobians, we get:\n$$ \\mathbf{J}_{\\mathbf{g}}(\\mathbf{x}) = \\mathbf{W} \\, \\text{diag}\\left(1 - \\tanh^2(\\mathbf{V}\\mathbf{x} + \\mathbf{b})\\right) \\, \\mathbf{V} $$\n\nCombining the Jacobians of all terms, the full Jacobian of the vector field $f_{\\theta}(\\mathbf{x})$ is:\n$$ \\mathbf{J}_{f_{\\theta}}(\\mathbf{x}) = \\mathbf{W} \\, \\text{diag}\\left(1 - \\tanh^2(\\mathbf{V}\\mathbf{x} + \\mathbf{b})\\right) \\, \\mathbf{V} + \\mathbf{C} $$\n\nThe algorithmic procedure to solve the problem for each test case is as follows:\n$1$. For a given set of parameters $(\\mathbf{W}, \\mathbf{V}, \\mathbf{C}, \\mathbf{b}, \\mathbf{d})$ and a candidate fixed point $\\mathbf{x}^{\\star}$, first compute the argument of the hyperbolic tangent function: $\\mathbf{u}^{\\star} = \\mathbf{V}\\mathbf{x}^{\\star} + \\mathbf{b}$.\n$2$. Construct the diagonal matrix $\\mathbf{D}^{\\star} = \\text{diag}(1 - \\tanh^2(\\mathbf{u}^{\\star}))$, where the operations are performed elementwise.\n$3$. Compute the numerical Jacobian matrix at the fixed point: $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star}) = \\mathbf{W}\\mathbf{D}^{\\star}\\mathbf{V} + \\mathbf{C}$.\n$4$. Calculate the eigenvalues of $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$.\n$5$. Determine stability by checking if the real part of every eigenvalue is strictly less than zero. If this condition holds, the fixed point is asymptotically stable (`True`); otherwise, it is not (`False`).\nThis sequence of operations will be performed for each test case to generate the final list of results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the stability of fixed points for several\n    dynamical systems.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (dimension 2, nonlinearity present, stable)\n        {\n            \"W\": np.array([[-0.3, 0], [0, -0.3]]),\n            \"V\": np.array([[1, 0], [0, 1]]),\n            \"C\": np.array([[-0.4, 0], [0, -0.4]]),\n            \"b\": np.array([[0], [0]]),\n            \"d\": np.array([[0], [0]]),\n            \"x_star\": np.array([[0], [0]]),\n        },\n        # Test case 2 (dimension 2, saddle-type behavior)\n        {\n            \"W\": np.array([[0.2, 0], [0, 0.2]]),\n            \"V\": np.array([[3.0, 0], [0, 0.2]]),\n            \"C\": np.array([[-0.1, 0], [0, -0.05]]),\n            \"b\": np.array([[0], [0]]),\n            \"d\": np.array([[0], [0]]),\n            \"x_star\": np.array([[0], [0]]),\n        },\n        # Test case 3 (dimension 3, purely linear unstable)\n        {\n            \"W\": np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n            \"V\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]),\n            \"C\": np.array([[0.1, 0, 0], [0, 0.2, 0], [0, 0, 0.3]]),\n            \"b\": np.array([[0], [0], [0]]),\n            \"d\": np.array([[0], [0], [0]]),\n            \"x_star\": np.array([[0], [0], [0]]),\n        },\n        # Test case 4 (dimension 2, marginal with a zero eigenvalue)\n        {\n            \"W\": np.array([[0, 0], [0, 0]]),\n            \"V\": np.array([[1, 0], [0, 1]]),\n            \"C\": np.array([[-0.1, 0], [0, 0.0]]),\n            \"b\": np.array([[0], [0]]),\n            \"d\": np.array([[0], [0]]),\n            \"x_star\": np.array([[0], [0]]),\n        },\n        # Test case 5 (dimension 1, nontrivial nonlinearity at nonzero argument, stable)\n        {\n            \"W\": np.array([[-0.8]]),\n            \"V\": np.array([[2.5]]),\n            \"C\": np.array([[-0.05]]),\n            \"b\": np.array([[-0.05]]),\n            \"d\": np.array([[0.6919232]]),\n            \"x_star\": np.array([[0.5]]),\n        },\n    ]\n\n    def check_stability(params):\n        \"\"\"\n        Computes the Jacobian and checks the stability of the given fixed point.\n\n        Args:\n            params (dict): A dictionary containing the parameters W, V, C, b, and x_star.\n\n        Returns:\n            bool: True if the fixed point is asymptotically stable, False otherwise.\n        \"\"\"\n        W = params[\"W\"]\n        V = params[\"V\"]\n        C = params[\"C\"]\n        b = params[\"b\"]\n        x_star = params[\"x_star\"]\n\n        # 1. Compute the argument of the tanh function\n        u_star = V @ x_star + b\n\n        # 2. Compute the diagonal elements of the Jacobian of the nonlinearity\n        # The derivative of tanh(z) is 1 - tanh^2(z).\n        # We need to flatten the result for np.diag as it expects a 1D array.\n        tanh_deriv_vec = 1 - np.tanh(u_star)**2\n        D_star = np.diag(tanh_deriv_vec.flatten())\n\n        # 3. Compute the full Jacobian matrix at the fixed point\n        # J = W * D * V + C\n        J = W @ D_star @ V + C\n\n        # 4. Calculate the eigenvalues of the Jacobian\n        eigenvalues = np.linalg.eigvals(J)\n        \n        # 5. Check if all eigenvalues have strictly negative real parts\n        is_stable = np.all(np.real(eigenvalues)  0)\n\n        return is_stable\n\n    results = []\n    for case in test_cases:\n        result = check_stability(case)\n        results.append(result)\n\n    # Format the final output as a comma-separated list of lowercase strings.\n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A model's true utility is measured by its ability to generalize and make accurate predictions on unseen data. This practice  focuses on the critical workflow of model validation and comparison, where you will evaluate a model's performance on held-out neurons. By computing the predictive log-likelihood for both a full Poisson Linear Dynamical System (PLDS) and a simpler Static Poisson Factorization (SPF) model, you will gain hands-on experience in quantifying the practical benefits of capturing temporal dynamics.",
            "id": "4173354",
            "problem": "You are given a fitted Poisson Linear Dynamical System (PLDS) and a fitted Static Poisson Factorization (SPF) model for neural spike counts in a multi-neuron, multi-time-bin recording. The task is to compute the expected spike counts and the predictive log-likelihood for held-out neurons under the PLDS, and compare these metrics to those obtained under the SPF model. The comparison must be done on multiple test cases to evaluate performance under different posterior covariance structures and data regimes.\n\nBegin from the following fundamental base:\n- The definition of a linear dynamical system: at each time bin $t$, the latent state $\\mathbf{x}_t \\in \\mathbb{R}^K$ evolves according to $\\mathbf{x}_t = \\mathbf{A}\\mathbf{x}_{t-1} + \\mathbf{b} + \\mathbf{w}_t$, where $\\mathbf{w}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q})$ is Gaussian process noise.\n- The definition of the conditional Poisson observation model for spike counts: for neuron $n$, the spike count $y_{t,n}$ conditional on $\\mathbf{x}_t$ is distributed as a Poisson random variable with rate $\\lambda_{t,n} = \\exp(\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n)$, where $\\mathbf{c}_n \\in \\mathbb{R}^K$ is the loading vector for neuron $n$ and $d_n \\in \\mathbb{R}$ is its log-rate offset.\n- The definition of the Static Poisson Factorization model: $y_{t,n} \\mid \\mathbf{u}_t$ is Poisson with rate $\\tilde{\\lambda}_{t,n} = \\exp(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n)$, where $\\mathbf{u}_t \\in \\mathbb{R}^K$ is the (static) time-factor vector and $\\mathbf{v}_n \\in \\mathbb{R}^K$ is the neuron-factor vector, with $b_n \\in \\mathbb{R}$ being its log-rate offset.\n- The definition of predictive log-likelihood: for spike counts $y_{t,n}$ and a model specified by parameters, the predictive log-likelihood is the expected value of the log-likelihood under the relevant posterior distribution of the latent variables. For PLDS, assume a Gaussian variational posterior $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{m}_t, \\mathbf{S}_t)$ is available. For SPF, take $\\mathbf{u}_t$ as deterministic point estimates.\n\nUsing only these definitions and well-tested probabilistic facts, derive the formulas required to compute:\n- The expected spike counts for held-out neurons under PLDS and SPF.\n- The predictive log-likelihood for held-out neurons under PLDS (as an expectation under the Gaussian posterior) and under SPF (as a deterministic evaluation).\n\nThen implement a program that, for each test case described below, computes two summary metrics:\n1. The difference in total predictive log-likelihood for the held-out neurons across all time bins, defined as $L_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}}$.\n2. The mean absolute difference between the modelsâ€™ expected spike counts for the held-out neurons across all time bins.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The results must be ordered by test case, with two numbers per test case: first the predictive log-likelihood difference, then the mean absolute difference in expected counts. For three test cases, the output must therefore contain six numbers in total, in the order $[L_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}} \\text{ for case } 1, \\text{ MAD}_{\\text{counts}} \\text{ for case } 1, L_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}} \\text{ for case } 2, \\text{ MAD}_{\\text{counts}} \\text{ for case } 2, L_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}} \\text{ for case } 3, \\text{ MAD}_{\\text{counts}} \\text{ for case } 3]$.\n\nNo physical units or angle units are involved. All numerical answers in the output must be floating point numbers.\n\nThe fitted model parameters and data for each test case are as follows. The latent dimension is $K = 2$, the number of neurons is $N = 4$, the number of time bins is $T = 5$, and the held-out neuron index set is $\\{2, 3\\}$ (zero-based indexing).\n\nShared parameters across all test cases:\n- PLDS observation parameters:\n  - $\\mathbf{C} = \\begin{bmatrix} 0.7  -0.3 \\\\ -0.5  0.6 \\\\ 0.2  0.8 \\\\ 1.0  -0.4 \\end{bmatrix}$, where row $n$ is $\\mathbf{c}_n^\\top$.\n  - $\\mathbf{d} = \\begin{bmatrix} -1.0 \\\\ -0.7 \\\\ -1.2 \\\\ -0.5 \\end{bmatrix}$.\n- SPF observation parameters:\n  - $\\mathbf{V} = \\begin{bmatrix} 0.6  -0.2 \\\\ -0.4  0.5 \\\\ 0.15  0.7 \\\\ 0.9  -0.1 \\end{bmatrix}$, where row $n$ is $\\mathbf{v}_n^\\top$.\n  - $\\mathbf{b} = \\begin{bmatrix} -1.0 \\\\ -0.7 \\\\ -1.1 \\\\ -0.6 \\end{bmatrix}$.\n\nTest Case $1$ (nonzero posterior covariance for PLDS):\n- Observed spike counts matrix $\\mathbf{Y}^{(1)}$ of shape $T \\times N$:\n  - Row $1$: $\\begin{bmatrix} 5  2  3  0 \\end{bmatrix}$,\n  - Row $2$: $\\begin{bmatrix} 6  3  4  1 \\end{bmatrix}$,\n  - Row $3$: $\\begin{bmatrix} 4  1  2  0 \\end{bmatrix}$,\n  - Row $4$: $\\begin{bmatrix} 7  2  5  1 \\end{bmatrix}$,\n  - Row $5$: $\\begin{bmatrix} 3  2  1  0 \\end{bmatrix}$.\n- PLDS variational posterior means $\\mathbf{m}_t^{(1)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{m}_1^{(1)} = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$,\n  - $\\mathbf{m}_2^{(1)} = \\begin{bmatrix} 0.4 \\\\ -0.05 \\end{bmatrix}$,\n  - $\\mathbf{m}_3^{(1)} = \\begin{bmatrix} 0.35 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\mathbf{m}_4^{(1)} = \\begin{bmatrix} 0.3 \\\\ 0.1 \\end{bmatrix}$,\n  - $\\mathbf{m}_5^{(1)} = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$.\n- PLDS variational posterior covariances $\\mathbf{S}_t^{(1)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{S}_t^{(1)} = \\begin{bmatrix} 0.2  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$ for all $t$.\n- SPF time factors $\\mathbf{u}_t^{(1)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{u}_1^{(1)} = \\begin{bmatrix} 0.25 \\\\ -0.15 \\end{bmatrix}$,\n  - $\\mathbf{u}_2^{(1)} = \\begin{bmatrix} 0.45 \\\\ -0.10 \\end{bmatrix}$,\n  - $\\mathbf{u}_3^{(1)} = \\begin{bmatrix} 0.40 \\\\ -0.05 \\end{bmatrix}$,\n  - $\\mathbf{u}_4^{(1)} = \\begin{bmatrix} 0.35 \\\\ 0.05 \\end{bmatrix}$,\n  - $\\mathbf{u}_5^{(1)} = \\begin{bmatrix} 0.15 \\\\ 0.15 \\end{bmatrix}$.\n\nTest Case $2$ (zero posterior covariance for PLDS):\n- Observed spike counts matrix $\\mathbf{Y}^{(2)} = \\mathbf{Y}^{(1)}$ (identical to Test Case $1$).\n- PLDS variational posterior means $\\mathbf{m}_t^{(2)} = \\mathbf{m}_t^{(1)}$ (identical to Test Case $1$).\n- PLDS variational posterior covariances $\\mathbf{S}_t^{(2)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{S}_t^{(2)} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}$ for all $t$.\n- SPF time factors $\\mathbf{u}_t^{(2)} = \\mathbf{m}_t^{(2)}$ for all $t$.\n\nTest Case $3$ (zeros in held-out neuron counts, nonzero posterior covariance for PLDS):\n- Observed spike counts matrix $\\mathbf{Y}^{(3)}$ of shape $T \\times N$:\n  - Row $1$: $\\begin{bmatrix} 3  1  2  0 \\end{bmatrix}$,\n  - Row $2$: $\\begin{bmatrix} 4  2  3  0 \\end{bmatrix}$,\n  - Row $3$: $\\begin{bmatrix} 2  1  1  0 \\end{bmatrix}$,\n  - Row $4$: $\\begin{bmatrix} 5  2  4  0 \\end{bmatrix}$,\n  - Row $5$: $\\begin{bmatrix} 3  1  2  0 \\end{bmatrix}$.\n- PLDS variational posterior means $\\mathbf{m}_t^{(3)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{m}_t^{(3)} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$ for all $t$.\n- PLDS variational posterior covariances $\\mathbf{S}_t^{(3)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{S}_t^{(3)} = \\begin{bmatrix} 0.3  0.0 \\\\ 0.0  0.3 \\end{bmatrix}$ for all $t$.\n- SPF time factors $\\mathbf{u}_t^{(3)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{u}_t^{(3)} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$ for all $t$.\n\nImplementation requirements:\n- For PLDS, treat $\\mathbf{m}_t$ and $\\mathbf{S}_t$ as the fitted Gaussian variational posterior for $\\mathbf{x}_t$ obtained from training neurons (neurons with indices $0$ and $1$). Use these to compute expectations for the held-out neurons (indices $2$ and $3$).\n- For SPF, use $\\mathbf{u}_t$ as deterministic fitted time factors; compute expectations accordingly for held-out neurons.\n- For predictive log-likelihoods, use the observed counts for held-out neurons only.\n- Your program should produce a single line of output containing the six floating point results in the order specified above, as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\dots]$). No other text should be printed.",
            "solution": "The problem statement is parsed and validated. All definitions are mathematically and scientifically sound, all data and parameters are provided, and the dimensions are consistent. The problem is well-posed, objective, and resides within the established framework of statistical modeling for neuroscience. It requires the application of standard principles of probability theory to derive and compute specific quantities for comparing two common models of neural activity, the Poisson Linear Dynamical System (PLDS) and Static Poisson Factorization (SPF). The problem is deemed valid.\n\nWe begin by deriving the necessary formulas for the two models. The evaluation is performed on a set of held-out neurons, indexed by $n \\in \\mathcal{H}$, over a number of time bins, indexed by $t=1, \\dots, T$.\n\n**Static Poisson Factorization (SPF) Model**\n\nIn the SPF model, the latent time factors $\\mathbf{u}_t \\in \\mathbb{R}^K$ are treated as deterministic point estimates. The spike count $y_{t,n}$ for neuron $n$ at time $t$ is assumed to follow a Poisson distribution with rate $\\tilde{\\lambda}_{t,n} = \\exp(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n)$, where $\\mathbf{v}_n$ is the neuron-factor vector and $b_n$ is the log-rate offset.\n\nExpected Spike Count (SPF):\nThe expected value of a Poisson random variable is its rate parameter. Since all variables on the right-hand side are deterministic, the expected spike count is simply the rate:\n$$\n\\mathbb{E}[y_{t,n}]_{\\mathrm{SPF}} = \\tilde{\\lambda}_{t,n} = \\exp(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n)\n$$\n\nPredictive Log-Likelihood (SPF):\nThe log-likelihood of observing a spike count $y_{t,n}$ is given by the Poisson log-probability mass function:\n$$\n\\log P(y_{t,n} | \\tilde{\\lambda}_{t,n}) = y_{t,n} \\log(\\tilde{\\lambda}_{t,n}) - \\tilde{\\lambda}_{t,n} - \\log(y_{t,n}!)\n$$\nSubstituting $\\log(\\tilde{\\lambda}_{t,n}) = \\mathbf{v}_n^\\top \\mathbf{u}_t + b_n$ and $\\tilde{\\lambda}_{t,n} = \\exp(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n)$, we get:\n$$\nL_{t,n}^{\\mathrm{SPF}} = y_{t,n}(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n) - \\exp(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n) - \\log(y_{t,n}!)\n$$\nSince $\\mathbf{u}_t$ is deterministic, this is the predictive log-likelihood for a single observation. The total predictive log-likelihood for the SPF model, $L_{\\mathrm{SPF}}$, is the sum of $L_{t,n}^{\\mathrm{SPF}}$ over all $t \\in \\{1, \\dots, T\\}$ and $n \\in \\mathcal{H}$.\n\n**Poisson Linear Dynamical System (PLDS) Model**\n\nIn the PLDS model, the latent state $\\mathbf{x}_t \\in \\mathbb{R}^K$ is a random variable. We are given that its variational posterior distribution is a Gaussian, $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{m}_t, \\mathbf{S}_t)$. The spike count $y_{t,n}$ is Poisson with rate $\\lambda_{t,n} = \\exp(\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n)$. To compute the required quantities, we must take expectations over the posterior distribution of $\\mathbf{x}_t$.\n\nLet's define the argument of the exponential, $\\alpha_{t,n} = \\mathbf{c}_n^\\top \\mathbf{x}_t + d_n$. Since $\\alpha_{t,n}$ is a linear transformation of a Gaussian random variable $\\mathbf{x}_t$, $\\alpha_{t,n}$ is also a Gaussian random variable. Its mean and variance are:\n$$\n\\mu_{\\alpha_{t,n}} = \\mathbb{E}[\\alpha_{t,n}] = \\mathbb{E}[\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n] = \\mathbf{c}_n^\\top \\mathbb{E}[\\mathbf{x}_t] + d_n = \\mathbf{c}_n^\\top \\mathbf{m}_t + d_n\n$$\n$$\n\\sigma^2_{\\alpha_{t,n}} = \\mathrm{Var}[\\alpha_{t,n}] = \\mathrm{Var}[\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n] = \\mathbf{c}_n^\\top \\mathrm{Var}[\\mathbf{x}_t] \\mathbf{c}_n = \\mathbf{c}_n^\\top \\mathbf{S}_t \\mathbf{c}_n\n$$\nSo, $\\alpha_{t,n} \\sim \\mathcal{N}(\\mu_{\\alpha_{t,n}}, \\sigma^2_{\\alpha_{t,n}})$.\n\nExpected Spike Count (PLDS):\nThe expected spike count is the expectation of the rate parameter $\\lambda_{t,n}$ over the posterior of $\\mathbf{x}_t$. The rate is $\\lambda_{t,n} = e^{\\alpha_{t,n}}$.\n$$\n\\mathbb{E}[y_{t,n}]_{\\mathrm{PLDS}} = \\mathbb{E}_{\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{m}_t, \\mathbf{S}_t)}[\\lambda_{t,n}] = \\mathbb{E}_{\\alpha_{t,n}}[e^{\\alpha_{t,n}}]\n$$\nThis is the moment-generating function of the Gaussian variable $\\alpha_{t,n}$ evaluated at $s=1$. For a Gaussian $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$, its MGF is $M_Z(s) = \\exp(s\\mu + \\frac{1}{2}s^2\\sigma^2)$. Setting $s=1$, we get:\n$$\n\\mathbb{E}[y_{t,n}]_{\\mathrm{PLDS}} = \\exp(\\mu_{\\alpha_{t,n}} + \\frac{1}{2}\\sigma^2_{\\alpha_{t,n}}) = \\exp\\left( (\\mathbf{c}_n^\\top \\mathbf{m}_t + d_n) + \\frac{1}{2}\\mathbf{c}_n^\\top \\mathbf{S}_t \\mathbf{c}_n \\right)\n$$\n\nPredictive Log-Likelihood (PLDS):\nThe predictive log-likelihood is the expectation of the log-likelihood function with respect to the posterior distribution of $\\mathbf{x}_t$.\n$$\nL_{t,n}^{\\mathrm{PLDS}} = \\mathbb{E}_{\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{m}_t, \\mathbf{S}_t)}[\\log P(y_{t,n} | \\mathbf{x}_t)] = \\mathbb{E}_{\\mathbf{x}_t}[y_{t,n}(\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n) - \\exp(\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n) - \\log(y_{t,n}!)]\n$$\nBy linearity of expectation:\n$$\nL_{t,n}^{\\mathrm{PLDS}} = y_{t,n}\\mathbb{E}[\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n] - \\mathbb{E}[\\exp(\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n)] - \\log(y_{t,n}!)\n$$\nThe first term's expectation is $\\mu_{\\alpha_{t,n}}$. The second term's expectation is the expected spike count $\\mathbb{E}[y_{t,n}]_{\\mathrm{PLDS}}$.\n$$\nL_{t,n}^{\\mathrm{PLDS}} = y_{t,n}(\\mathbf{c}_n^\\top \\mathbf{m}_t + d_n) - \\exp\\left( \\mathbf{c}_n^\\top \\mathbf{m}_t + d_n + \\frac{1}{2}\\mathbf{c}_n^\\top \\mathbf{S}_t \\mathbf{c}_n \\right) - \\log(y_{t,n}!)\n$$\nThe total predictive log-likelihood for the PLDS model, $L_{\\mathrm{PLDS}}$, is the sum of $L_{t,n}^{\\mathrm{PLDS}}$ over all $t \\in \\{1, \\dots, T\\}$ and $n \\in \\mathcal{H}$.\n\n**Summary Metrics**\n\nWe are asked to compute two metrics for each test case.\n\n1. Difference in total predictive log-likelihood, $L_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}}$:\n$$\nL_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}} = \\sum_{t=1}^T \\sum_{n \\in \\mathcal{H}} (L_{t,n}^{\\mathrm{PLDS}} - L_{t,n}^{\\mathrm{SPF}})\n$$\nNote that the term $\\log(y_{t,n}!)$ is common to both $L_{t,n}^{\\mathrm{PLDS}}$ and $L_{t,n}^{\\mathrm{SPF}}$, so it cancels out in the difference. Thus, we only need to compute:\n$$\nL_{t,n}^{\\mathrm{PLDS}} - L_{t,n}^{\\mathrm{SPF}} = \\left(y_{t,n}(\\mathbf{c}_n^\\top \\mathbf{m}_t + d_n) - \\mathbb{E}[y_{t,n}]_{\\mathrm{PLDS}}\\right) - \\left(y_{t,n}(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n) - \\mathbb{E}[y_{t,n}]_{\\mathrm{SPF}}\\right)\n$$\n\n2. Mean absolute difference between expected spike counts:\nThis is the average absolute difference between the two models' predictions for the expected number of spikes, averaged over all held-out neurons and time bins.\n$$\n\\mathrm{MAD}_{\\mathrm{counts}} = \\frac{1}{T \\cdot |\\mathcal{H}|} \\sum_{t=1}^T \\sum_{n \\in \\mathcal{H}} \\left| \\mathbb{E}[y_{t,n}]_{\\mathrm{PLDS}} - \\mathbb{E}[y_{t,n}]_{\\mathrm{SPF}} \\right|\n$$\nwhere $|\\mathcal{H}|$ is the number of held-out neurons, which is $2$. The total number of points for the average is $T \\times |\\mathcal{H}| = 5 \\times 2 = 10$.\n\nWith these formulas, we can proceed to the implementation and computation for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes model comparison metrics for PLDS and SPF on held-out neural data.\n    \"\"\"\n\n    # --- Shared Parameters ---\n    C = np.array([\n        [0.7, -0.3],\n        [-0.5, 0.6],\n        [0.2, 0.8],\n        [1.0, -0.4]\n    ])\n    d = np.array([-1.0, -0.7, -1.2, -0.5])\n    V = np.array([\n        [0.6, -0.2],\n        [-0.4, 0.5],\n        [0.15, 0.7],\n        [0.9, -0.1]\n    ])\n    b = np.array([-1.0, -0.7, -1.1, -0.6])\n    held_out_neurons = [2, 3]\n\n    # --- Test Case Data ---\n    \n    # Test Case 1\n    Y1 = np.array([\n        [5, 2, 3, 0],\n        [6, 3, 4, 1],\n        [4, 1, 2, 0],\n        [7, 2, 5, 1],\n        [3, 2, 1, 0]\n    ])\n    m1 = np.array([\n        [0.2, -0.1],\n        [0.4, -0.05],\n        [0.35, 0.0],\n        [0.3, 0.1],\n        [0.1, 0.2]\n    ])\n    S1 = np.array([\n        [0.2, 0.0],\n        [0.0, 0.1]\n    ])\n    u1 = np.array([\n        [0.25, -0.15],\n        [0.45, -0.10],\n        [0.40, -0.05],\n        [0.35, 0.05],\n        [0.15, 0.15]\n    ])\n\n    # Test Case 2\n    Y2 = Y1\n    m2 = m1\n    S2 = np.zeros((2, 2))\n    u2 = m2\n\n    # Test Case 3\n    Y3 = np.array([\n        [3, 1, 2, 0],\n        [4, 2, 3, 0],\n        [2, 1, 1, 0],\n        [5, 2, 4, 0],\n        [3, 1, 2, 0]\n    ])\n    m3 = np.zeros((5, 2))\n    S3 = np.array([\n        [0.3, 0.0],\n        [0.0, 0.3]\n    ])\n    u3 = np.zeros((5, 2))\n\n    test_cases = [\n        (Y1, m1, S1, u1),\n        (Y2, m2, S2, u2),\n        (Y3, m3, S3, u3)\n    ]\n\n    all_results = []\n\n    for Y, M, S, U in test_cases:\n        total_ll_plds_part = 0.0\n        total_ll_spf_part = 0.0\n        exp_count_diffs = []\n        T = Y.shape[0]\n\n        for t in range(T):\n            m_t = M[t]\n            u_t = U[t]\n            S_t = S  # S is constant across time in all given test cases\n\n            for n_idx in held_out_neurons:\n                y_tn = Y[t, n_idx]\n                c_n = C[n_idx]\n                d_n = d[n_idx]\n                v_n = V[n_idx]\n                b_n = b[n_idx]\n\n                # --- PLDS Calculations ---\n                mu_alpha = c_n @ m_t + d_n\n                var_alpha = c_n @ S_t @ c_n\n                \n                exp_count_plds = np.exp(mu_alpha + 0.5 * var_alpha)\n                ll_plds_part = y_tn * mu_alpha - exp_count_plds\n                total_ll_plds_part += ll_plds_part\n\n                # --- SPF Calculations ---\n                log_rate_spf = v_n @ u_t + b_n\n                exp_count_spf = np.exp(log_rate_spf)\n                \n                ll_spf_part = y_tn * log_rate_spf - exp_count_spf\n                total_ll_spf_part += ll_spf_part\n\n                # --- Metric Calculations ---\n                exp_count_diffs.append(np.abs(exp_count_plds - exp_count_spf))\n\n        # Calculate summary metrics for the current test case\n        ll_diff = total_ll_plds_part - total_ll_spf_part\n        mad_counts = np.mean(exp_count_diffs)\n        \n        all_results.append(ll_diff)\n        all_results.append(mad_counts)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}