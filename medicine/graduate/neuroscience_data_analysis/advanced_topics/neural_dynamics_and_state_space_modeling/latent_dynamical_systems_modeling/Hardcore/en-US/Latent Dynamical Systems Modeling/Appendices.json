{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in fitting and comparing latent variable models is to compute the marginal likelihood of the data, often called the model evidence. For the special case of Linear Gaussian State-Space Models (LGSSMs), this quantity is analytically tractable and can be calculated efficiently using the output of the Kalman filter. This foundational exercise involves deriving this likelihood, which solidifies the connection between the model's generative process, the Kalman filter's recursive estimation, and the principles of statistical inference .",
            "id": "4173383",
            "problem": "Consider a Linear Gaussian State Space Model (LGSSM) for latent population activity in a mesoscale two-photon calcium imaging experiment. The latent state $\\mathbf{x}_t \\in \\mathbb{R}^n$ captures the low-dimensional neural dynamics and the observation $\\mathbf{y}_t \\in \\mathbb{R}^m$ represents the fluorescence at $m$ regions of interest. The model is specified by the following generative assumptions:\n$$\n\\mathbf{x}_1 \\sim \\mathcal{N}(\\mathbf{m}_0, \\mathbf{P}_0), \\quad \\mathbf{x}_{t+1} = \\mathbf{A}\\,\\mathbf{x}_t + \\mathbf{w}_t, \\quad \\mathbf{w}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q}),\n$$\n$$\n\\mathbf{y}_t = \\mathbf{C}\\,\\mathbf{x}_t + \\mathbf{v}_t, \\quad \\mathbf{v}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R}),\n$$\nwhere $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{C} \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$, $\\mathbf{m}_0 \\in \\mathbb{R}^n$, and $\\mathbf{P}_0 \\in \\mathbb{R}^{n \\times n}$ are known. The random variables $\\mathbf{x}_1$, $\\{\\mathbf{w}_t\\}_{t=1}^{T-1}$, and $\\{\\mathbf{v}_t\\}_{t=1}^{T}$ are mutually independent, and the process is first-order Markov in $\\mathbf{x}_t$ with conditionally independent observations given $\\mathbf{x}_t$. Assume that $\\mathbf{y}_{1:T}$ are the observed fluorescence measurements over $T$ time steps, with $T \\in \\mathbb{N}$.\n\nStarting from the joint distribution factorization implied by these assumptions and using only properties of multivariate Gaussian distributions and the tower property of conditional expectations, perform recursive Gaussian integration of the latent states to obtain the marginal likelihood $p(\\mathbf{y}_{1:T})$. Your derivation must proceed by expressing $p(\\mathbf{y}_{1:T})$ as a product of one-step predictive observation densities constructed from the innovation sequence of the Kalman filter. Define the innovations and their covariances in terms of the Kalman predictive mean and covariance, but do not assume any steady-state or asymptotic formulas.\n\nProvide the final answer as a single closed-form analytical expression for $p(\\mathbf{y}_{1:T})$ in terms of the innovation vectors $\\{\\mathbf{e}_t\\}_{t=1}^{T}$ and innovation covariances $\\{\\mathbf{S}_t\\}_{t=1}^{T}$ generated by the Kalman filter operating on $\\mathbf{y}_{1:T}$. Additionally, state the minimal conditions on the model parameters and the resulting predictive covariances under which this computation is tractable (i.e., the integrals and required matrix inverses and determinants exist and are finite). No numerical evaluation is required. Express the final answer symbolically; do not include any units.",
            "solution": "### Step 1: Extract Givens\nThe problem defines a Linear Gaussian State Space Model (LGSSM) with the following components:\n- Latent state: $\\mathbf{x}_t \\in \\mathbb{R}^n$\n- Observation: $\\mathbf{y}_t \\in \\mathbb{R}^m$\n- Generative assumptions:\n  - Initial state distribution: $\\mathbf{x}_1 \\sim \\mathcal{N}(\\mathbf{m}_0, \\mathbf{P}_0)$\n  - State transition equation: $\\mathbf{x}_{t+1} = \\mathbf{A}\\,\\mathbf{x}_t + \\mathbf{w}_t$, where the process noise $\\mathbf{w}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q})$\n  - Observation equation: $\\mathbf{y}_t = \\mathbf{C}\\,\\mathbf{x}_t + \\mathbf{v}_t$, where the measurement noise $\\mathbf{v}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R})$\n- Model parameters are known: $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{C} \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ (symmetric positive semi-definite), $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ (symmetric positive semi-definite), $\\mathbf{m}_0 \\in \\mathbb{R}^n$, and $\\mathbf{P}_0 \\in \\mathbb{R}^{n \\times n}$ (symmetric positive semi-definite).\n- Independence assumptions: The random variables $\\mathbf{x}_1$, $\\{\\mathbf{w}_t\\}_{t=1}^{T-1}$, and $\\{\\mathbf{v}_t\\}_{t=1}^{T}$ are mutually independent.\n- Markov properties: The process is first-order Markov in $\\mathbf{x}_t$, and observations $\\mathbf{y}_t$ are conditionally independent given $\\mathbf{x}_t$.\n- Data: A sequence of observations $\\mathbf{y}_{1:T} = \\{\\mathbf{y}_1, \\dots, \\mathbf{y}_T\\}$ for $T \\in \\mathbb{N}$ time steps.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes a standard Linear Gaussian State Space Model, also known as a Kalman filter model. This is a cornerstone of modern signal processing, control theory, and time-series analysis, with widespread applications, including neuroscience data analysis. The premises are scientifically sound and firmly established in mathematical statistics.\n- **Well-Posed**: The problem asks for the derivation of the marginal likelihood $p(\\mathbf{y}_{1:T})$, a well-defined quantity in statistical inference. The model is fully specified with all necessary parameters and distributions. The task is to provide a specific analytical expression, which is a standard result for this model.\n- **Objective**: The problem is stated in precise mathematical language, free from any subjective or biased terminology.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is scientifically sound, well-posed, objective, complete, and asks for a non-trivial derivation within the specified topic of latent dynamical systems modeling.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Derivation of the Marginal Likelihood\n\nThe goal is to compute the marginal likelihood of the observations, $p(\\mathbf{y}_{1:T})$. This quantity is obtained by integrating the joint probability distribution over all latent states $\\mathbf{x}_{1:T}$:\n$$\np(\\mathbf{y}_{1:T}) = \\int p(\\mathbf{y}_{1:T}, \\mathbf{x}_{1:T}) d\\mathbf{x}_{1:T}\n$$\nThe problem specifies that the derivation should proceed by expressing $p(\\mathbf{y}_{1:T})$ as a product of one-step predictive observation densities. This can be achieved by applying the chain rule of probability to the sequence of observations:\n$$\np(\\mathbf{y}_{1:T}) = p(\\mathbf{y}_T | \\mathbf{y}_{1:T-1}) p(\\mathbf{y}_{T-1} | \\mathbf{y}_{1:T-2}) \\dots p(\\mathbf{y}_2 | \\mathbf{y}_1) p(\\mathbf{y}_1)\n$$\nThis can be written compactly as:\n$$\np(\\mathbf{y}_{1:T}) = \\prod_{t=1}^{T} p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})\n$$\nwhere the conditioning set for $t=1$ is empty, so the first term is simply the marginal density $p(\\mathbf{y}_1)$.\n\nThe core of the derivation is to find an expression for each term in this product, $p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})$. Since the overall model is a Linear Gaussian State Space Model, the entire joint distribution $p(\\mathbf{x}_{1:T}, \\mathbf{y}_{1:T})$ is Gaussian. Consequently, all marginal and conditional distributions derived from it are also Gaussian. Therefore, the one-step predictive observation density $p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})$ is Gaussian, and our task reduces to finding its mean and covariance.\n\nThis is precisely what the Kalman filter algorithm computes. The Kalman filter provides a recursive method for obtaining the posterior distribution of the state. Let us denote the mean and covariance of the state distribution, conditioned on observations up to time $t-1$, as:\n$$\np(\\mathbf{x}_t | \\mathbf{y}_{1:t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\mathbf{m}_{t|t-1}, \\mathbf{P}_{t|t-1})\n$$\nThis is the one-step-ahead predictive distribution for the state. The density $p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})$ can be found by marginalizing out $\\mathbf{x}_t$:\n$$\np(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\int p(\\mathbf{y}_t | \\mathbf{x}_t, \\mathbf{y}_{1:t-1}) p(\\mathbf{x}_t | \\mathbf{y}_{1:t-1}) d\\mathbf{x}_t\n$$\nDue to the conditional independence assumption in the model, $p(\\mathbf{y}_t | \\mathbf{x}_t, \\mathbf{y}_{1:t-1}) = p(\\mathbf{y}_t | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{y}_t; \\mathbf{C}\\mathbf{x}_t, \\mathbf{R})$. We are thus integrating the product of two Gaussians, which yields a Gaussian.\n\nWe can find the mean and covariance of $p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})$ using the laws of conditional expectation and covariance.\nThe mean is found using the tower property of expectation:\n$$\nE[\\mathbf{y}_t | \\mathbf{y}_{1:t-1}] = E[E[\\mathbf{y}_t | \\mathbf{x}_t, \\mathbf{y}_{1:t-1}] | \\mathbf{y}_{1:t-1}] = E[E[\\mathbf{C}\\mathbf{x}_t + \\mathbf{v}_t | \\mathbf{x}_t] | \\mathbf{y}_{1:t-1}]\n$$\n$$\nE[\\mathbf{y}_t | \\mathbf{y}_{1:t-1}] = E[\\mathbf{C}\\mathbf{x}_t | \\mathbf{y}_{1:t-1}] = \\mathbf{C} E[\\mathbf{x}_t | \\mathbf{y}_{1:t-1}] = \\mathbf{C} \\mathbf{m}_{t|t-1}\n$$\nThe covariance is:\n$$\n\\text{Cov}(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\text{Cov}(\\mathbf{C}\\mathbf{x}_t + \\mathbf{v}_t | \\mathbf{y}_{1:t-1})\n$$\nSince $\\mathbf{v}_t$ is independent of $\\mathbf{x}_t$ and all prior observations $\\mathbf{y}_{1:t-1}$, its covariance is independent of the conditioning:\n$$\n\\text{Cov}(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\text{Cov}(\\mathbf{C}\\mathbf{x}_t | \\mathbf{y}_{1:t-1}) + \\text{Cov}(\\mathbf{v}_t) = \\mathbf{C} \\text{Cov}(\\mathbf{x}_t | \\mathbf{y}_{1:t-1}) \\mathbf{C}^T + \\mathbf{R}\n$$\n$$\n\\text{Cov}(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T + \\mathbf{R}\n$$\nThis quantity is defined as the innovation covariance, $\\mathbf{S}_t$.\n$$\n\\mathbf{S}_t \\equiv \\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T + \\mathbf{R}\n$$\nSo, the one-step predictive observation distribution is:\n$$\np(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\mathcal{N}(\\mathbf{y}_t; \\mathbf{C} \\mathbf{m}_{t|t-1}, \\mathbf{S}_t)\n$$\nThe innovation vector $\\mathbf{e}_t$ is defined as the difference between the actual observation $\\mathbf{y}_t$ and its one-step prediction:\n$$\n\\mathbf{e}_t \\equiv \\mathbf{y}_t - E[\\mathbf{y}_t | \\mathbf{y}_{1:t-1}] = \\mathbf{y}_t - \\mathbf{C} \\mathbf{m}_{t|t-1}\n$$\nThe distribution of the innovation (conditional on past observations) is thus $\\mathbf{e}_t | \\mathbf{y}_{1:t-1} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{S}_t)$. Therefore, the density $p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})$ can be expressed elegantly as the probability density of the innovation vector:\n$$\np(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\frac{1}{\\sqrt{(2\\pi)^m \\det(\\mathbf{S}_t)}} \\exp\\left(-\\frac{1}{2} (\\mathbf{y}_t - \\mathbf{C}\\mathbf{m}_{t|t-1})^T \\mathbf{S}_t^{-1} (\\mathbf{y}_t - \\mathbf{C}\\mathbf{m}_{t|t-1})\\right)\n$$\n$$\np(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\frac{1}{\\sqrt{(2\\pi)^m \\det(\\mathbf{S}_t)}} \\exp\\left(-\\frac{1}{2} \\mathbf{e}_t^T \\mathbf{S}_t^{-1} \\mathbf{e}_t\\right)\n$$\nHere, $m$ is the dimension of the observation vector $\\mathbf{y}_t$. The quantities $\\mathbf{m}_{t|t-1}$ and $\\mathbf{P}_{t|t-1}$ are produced by the prediction step of the Kalman filter, which updates the filtered estimates from the previous step, $\\mathbf{m}_{t-1|t-1}$ and $\\mathbf{P}_{t-1|t-1}$. The recursion starts at $t=1$ with the prior information $p(\\mathbf{x}_1) = \\mathcal{N}(\\mathbf{x}_1; \\mathbf{m}_0, \\mathbf{P}_0)$, which means we set $\\mathbf{m}_{1|0} = \\mathbf{m}_0$ and $\\mathbf{P}_{1|0} = \\mathbf{P}_0$.\n\nSubstituting this expression back into the product form of the marginal likelihood, we get:\n$$\np(\\mathbf{y}_{1:T}) = \\prod_{t=1}^{T} \\left[ \\frac{1}{\\sqrt{(2\\pi)^m \\det(\\mathbf{S}_t)}} \\exp\\left(-\\frac{1}{2} \\mathbf{e}_t^T \\mathbf{S}_t^{-1} \\mathbf{e}_t\\right) \\right]\n$$\nThis expression can be rewritten by separating the product into a normalization constant term and an exponential term:\n$$\np(\\mathbf{y}_{1:T}) = \\left(\\prod_{t=1}^{T} (2\\pi)^{-m/2} (\\det(\\mathbf{S}_t))^{-1/2} \\right) \\exp\\left(-\\frac{1}{2} \\sum_{t=1}^{T} \\mathbf{e}_t^T \\mathbf{S}_t^{-1} \\mathbf{e}_t\\right)\n$$\nThis is the final analytical expression for the marginal likelihood in terms of the innovations and their covariances.\n\n### Conditions for Tractability\n\nFor the computation of the marginal likelihood $p(\\mathbf{y}_{1:T})$ to be tractable, each term in the product must be well-defined and finite. The formula for the Gaussian density at each step $t$ involves two critical operations: the determinant $\\det(\\mathbf{S}_t)$ and the inverse $\\mathbf{S}_t^{-1}$.\n1.  The determinant $\\det(\\mathbf{S}_t)$ appears in the denominator, so it must be non-zero.\n2.  The matrix inverse $\\mathbf{S}_t^{-1}$ must exist.\nBoth conditions are met if and only if the matrix $\\mathbf{S}_t$ is invertible for all $t=1, \\dots, T$.\n\nThe matrix $\\mathbf{S}_t = \\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T + \\mathbf{R}$ is a sum of two symmetric positive semi-definite matrices (since $\\mathbf{P}_{t|t-1}$ and $\\mathbf{R}$ are covariance matrices), and thus $\\mathbf{S}_t$ is itself symmetric and positive semi-definite. For a symmetric positive semi-definite matrix, being invertible is equivalent to being positive definite.\n\nTherefore, the minimal necessary and sufficient condition for the computation to be tractable is that the innovation covariance matrix $\\mathbf{S}_t$ must be positive definite for all $t \\in \\{1, \\dots, T\\}$.\n\nA common and practical sufficient condition on the model parameters that ensures this is that the observation noise covariance matrix $\\mathbf{R}$ is positive definite. If $\\mathbf{R}$ is positive definite, then for any non-zero vector $\\mathbf{z} \\in \\mathbb{R}^m$, $\\mathbf{z}^T \\mathbf{R} \\mathbf{z}  0$. Since $\\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T$ is positive semi-definite, $\\mathbf{z}^T (\\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T) \\mathbf{z} \\ge 0$. The sum $\\mathbf{z}^T \\mathbf{S}_t \\mathbf{z} = \\mathbf{z}^T (\\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T) \\mathbf{z} + \\mathbf{z}^T \\mathbf{R} \\mathbf{z}$ will therefore be strictly positive, establishing that $\\mathbf{S}_t$ is positive definite.",
            "answer": "$$\n\\boxed{\\left(\\prod_{t=1}^{T} (2\\pi)^{-m/2} (\\det(\\mathbf{S}_t))^{-1/2} \\right) \\exp\\left(-\\frac{1}{2} \\sum_{t=1}^{T} \\mathbf{e}_t^T \\mathbf{S}_t^{-1} \\mathbf{e}_t\\right)}\n$$"
        },
        {
            "introduction": "Once a model is fitted, we must rigorously assess its performance, typically by evaluating its ability to predict new or held-out data. A powerful approach is to compare a sophisticated model, like a Poisson Linear Dynamical System (PLDS), against a simpler baseline, such as a static model that neglects temporal dynamics. This practical exercise guides you through computing key predictive metrics, requiring you to properly handle the posterior uncertainty over the latent states to generate predictions and teaching a fundamental workflow for model validation .",
            "id": "4173354",
            "problem": "You are given a fitted Poisson Linear Dynamical System (PLDS) and a fitted Static Poisson Factorization (SPF) model for neural spike counts in a multi-neuron, multi-time-bin recording. The task is to compute the expected spike counts and the predictive log-likelihood for held-out neurons under the PLDS, and compare these metrics to those obtained under the SPF model. The comparison must be done on multiple test cases to evaluate performance under different posterior covariance structures and data regimes.\n\nBegin from the following fundamental base:\n- The definition of a linear dynamical system: at each time bin $t$, the latent state $\\mathbf{x}_t \\in \\mathbb{R}^K$ evolves according to $\\mathbf{x}_t = \\mathbf{A}\\mathbf{x}_{t-1} + \\mathbf{b} + \\mathbf{w}_t$, where $\\mathbf{w}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q})$ is Gaussian process noise.\n- The definition of the conditional Poisson observation model for spike counts: for neuron $n$, the spike count $y_{t,n}$ conditional on $\\mathbf{x}_t$ is distributed as a Poisson random variable with rate $\\lambda_{t,n} = \\exp(\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n)$, where $\\mathbf{c}_n \\in \\mathbb{R}^K$ is the loading vector for neuron $n$ and $d_n \\in \\mathbb{R}$ is its log-rate offset.\n- The definition of the Static Poisson Factorization model: $y_{t,n} \\mid \\mathbf{u}_t$ is Poisson with rate $\\tilde{\\lambda}_{t,n} = \\exp(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n)$, where $\\mathbf{u}_t \\in \\mathbb{R}^K$ is the (static) time-factor vector and $\\mathbf{v}_n \\in \\mathbb{R}^K$ is the neuron-factor vector, with $b_n \\in \\mathbb{R}$ being its log-rate offset.\n- The definition of predictive log-likelihood: for spike counts $y_{t,n}$ and a model specified by parameters, the predictive log-likelihood is the expected value of the log-likelihood under the relevant posterior distribution of the latent variables. For PLDS, assume a Gaussian variational posterior $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{m}_t, \\mathbf{S}_t)$ is available. For SPF, take $\\mathbf{u}_t$ as deterministic point estimates.\n\nUsing only these definitions and well-tested probabilistic facts, derive the formulas required to compute:\n- The expected spike counts for held-out neurons under PLDS and SPF.\n- The predictive log-likelihood for held-out neurons under PLDS (as an expectation under the Gaussian posterior) and under SPF (as a deterministic evaluation).\n\nThen implement a program that, for each test case described below, computes two summary metrics:\n1. The difference in total predictive log-likelihood for the held-out neurons across all time bins, defined as $L_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}}$.\n2. The mean absolute difference between the modelsâ€™ expected spike counts for the held-out neurons across all time bins.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The results must be ordered by test case, with two numbers per test case: first the predictive log-likelihood difference, then the mean absolute difference in expected counts. For three test cases, the output must therefore contain six numbers in total, in the order $[L_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}} \\text{ for case } 1, \\text{ MAD}_{\\text{counts}} \\text{ for case } 1, L_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}} \\text{ for case } 2, \\text{ MAD}_{\\text{counts}} \\text{ for case } 2, L_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}} \\text{ for case } 3, \\text{ MAD}_{\\text{counts}} \\text{ for case } 3]$.\n\nNo physical units or angle units are involved. All numerical answers in the output must be floating point numbers.\n\nThe fitted model parameters and data for each test case are as follows. The latent dimension is $K = 2$, the number of neurons is $N = 4$, the number of time bins is $T = 5$, and the held-out neuron index set is $\\{2, 3\\}$ (zero-based indexing).\n\nShared parameters across all test cases:\n- PLDS observation parameters:\n  - $\\mathbf{C} = \\begin{bmatrix} 0.7  -0.3 \\\\ -0.5  0.6 \\\\ 0.2  0.8 \\\\ 1.0  -0.4 \\end{bmatrix}$, where row $n$ is $\\mathbf{c}_n^\\top$.\n  - $\\mathbf{d} = \\begin{bmatrix} -1.0 \\\\ -0.7 \\\\ -1.2 \\\\ -0.5 \\end{bmatrix}$.\n- SPF observation parameters:\n  - $\\mathbf{V} = \\begin{bmatrix} 0.6  -0.2 \\\\ -0.4  0.5 \\\\ 0.15  0.7 \\\\ 0.9  -0.1 \\end{bmatrix}$, where row $n$ is $\\mathbf{v}_n^\\top$.\n  - $\\mathbf{b} = \\begin{bmatrix} -1.0 \\\\ -0.7 \\\\ -1.1 \\\\ -0.6 \\end{bmatrix}$.\n\nTest Case $1$ (nonzero posterior covariance for PLDS):\n- Observed spike counts matrix $\\mathbf{Y}^{(1)}$ of shape $T \\times N$:\n  - Row $1$: $\\begin{bmatrix} 5  2  3  0 \\end{bmatrix}$,\n  - Row $2$: $\\begin{bmatrix} 6  3  4  1 \\end{bmatrix}$,\n  - Row $3$: $\\begin{bmatrix} 4  1  2  0 \\end{bmatrix}$,\n  - Row $4$: $\\begin{bmatrix} 7  2  5  1 \\end{bmatrix}$,\n  - Row $5$: $\\begin{bmatrix} 3  2  1  0 \\end{bmatrix}$.\n- PLDS variational posterior means $\\mathbf{m}_t^{(1)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{m}_1^{(1)} = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$,\n  - $\\mathbf{m}_2^{(1)} = \\begin{bmatrix} 0.4 \\\\ -0.05 \\end{bmatrix}$,\n  - $\\mathbf{m}_3^{(1)} = \\begin{bmatrix} 0.35 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\mathbf{m}_4^{(1)} = \\begin{bmatrix} 0.3 \\\\ 0.1 \\end{bmatrix}$,\n  - $\\mathbf{m}_5^{(1)} = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$.\n- PLDS variational posterior covariances $\\mathbf{S}_t^{(1)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{S}_t^{(1)} = \\begin{bmatrix} 0.2  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$ for all $t$.\n- SPF time factors $\\mathbf{u}_t^{(1)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{u}_1^{(1)} = \\begin{bmatrix} 0.25 \\\\ -0.15 \\end{bmatrix}$,\n  - $\\mathbf{u}_2^{(1)} = \\begin{bmatrix} 0.45 \\\\ -0.10 \\end{bmatrix}$,\n  - $\\mathbf{u}_3^{(1)} = \\begin{bmatrix} 0.40 \\\\ -0.05 \\end{bmatrix}$,\n  - $\\mathbf{u}_4^{(1)} = \\begin{bmatrix} 0.35 \\\\ 0.05 \\end{bmatrix}$,\n  - $\\mathbf{u}_5^{(1)} = \\begin{bmatrix} 0.15 \\\\ 0.15 \\end{bmatrix}$.\n\nTest Case $2$ (zero posterior covariance for PLDS):\n- Observed spike counts matrix $\\mathbf{Y}^{(2)} = \\mathbf{Y}^{(1)}$ (identical to Test Case $1$).\n- PLDS variational posterior means $\\mathbf{m}_t^{(2)} = \\mathbf{m}_t^{(1)}$ (identical to Test Case $1$).\n- PLDS variational posterior covariances $\\mathbf{S}_t^{(2)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{S}_t^{(2)} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}$ for all $t$.\n- SPF time factors $\\mathbf{u}_t^{(2)} = \\mathbf{m}_t^{(2)}$ for all $t$.\n\nTest Case $3$ (zeros in held-out neuron counts, nonzero posterior covariance for PLDS):\n- Observed spike counts matrix $\\mathbf{Y}^{(3)}$ of shape $T \\times N$:\n  - Row $1$: $\\begin{bmatrix} 3  1  2  0 \\end{bmatrix}$,\n  - Row $2$: $\\begin{bmatrix} 4  2  3  0 \\end{bmatrix}$,\n  - Row $3$: $\\begin{bmatrix} 2  1  1  0 \\end{bmatrix}$,\n  - Row $4$: $\\begin{bmatrix} 5  2  4  0 \\end{bmatrix}$,\n  - Row $5$: $\\begin{bmatrix} 3  1  2  0 \\end{bmatrix}$.\n- PLDS variational posterior means $\\mathbf{m}_t^{(3)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{m}_t^{(3)} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$ for all $t$.\n- PLDS variational posterior covariances $\\mathbf{S}_t^{(3)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{S}_t^{(3)} = \\begin{bmatrix} 0.3  0.0 \\\\ 0.0  0.3 \\end{bmatrix}$ for all $t$.\n- SPF time factors $\\mathbf{u}_t^{(3)}$ for $t = 1, \\dots, 5$:\n  - $\\mathbf{u}_t^{(3)} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$ for all $t$.\n\nImplementation requirements:\n- For PLDS, treat $\\mathbf{m}_t$ and $\\mathbf{S}_t$ as the fitted Gaussian variational posterior for $\\mathbf{x}_t$ obtained from training neurons (neurons with indices $0$ and $1$). Use these to compute expectations for the held-out neurons (indices $2$ and $3$).\n- For SPF, use $\\mathbf{u}_t$ as deterministic fitted time factors; compute expectations accordingly for held-out neurons.\n- For predictive log-likelihoods, use the observed counts for held-out neurons only.\n- Your program should produce a single line of output containing the six floating point results in the order specified above, as a comma-separated list enclosed in square brackets (for example, `[result_1,result_2,...]`). No other text should be printed.",
            "solution": "The problem statement is parsed and validated. All definitions are mathematically and scientifically sound, all data and parameters are provided, and the dimensions are consistent. The problem is well-posed, objective, and resides within the established framework of statistical modeling for neuroscience. It requires the application of standard principles of probability theory to derive and compute specific quantities for comparing two common models of neural activity, the Poisson Linear Dynamical System (PLDS) and Static Poisson Factorization (SPF). The problem is deemed valid.\n\nWe begin by deriving the necessary formulas for the two models. The evaluation is performed on a set of held-out neurons, indexed by $n \\in \\mathcal{H}$, over a number of time bins, indexed by $t=1, \\dots, T$.\n\n**Static Poisson Factorization (SPF) Model**\n\nIn the SPF model, the latent time factors $\\mathbf{u}_t \\in \\mathbb{R}^K$ are treated as deterministic point estimates. The spike count $y_{t,n}$ for neuron $n$ at time $t$ is assumed to follow a Poisson distribution with rate $\\tilde{\\lambda}_{t,n} = \\exp(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n)$, where $\\mathbf{v}_n$ is the neuron-factor vector and $b_n$ is the log-rate offset.\n\nExpected Spike Count (SPF):\nThe expected value of a Poisson random variable is its rate parameter. Since all variables on the right-hand side are deterministic, the expected spike count is simply the rate:\n$$\n\\mathbb{E}[y_{t,n}]_{\\mathrm{SPF}} = \\tilde{\\lambda}_{t,n} = \\exp(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n)\n$$\n\nPredictive Log-Likelihood (SPF):\nThe log-likelihood of observing a spike count $y_{t,n}$ is given by the Poisson log-probability mass function:\n$$\n\\log P(y_{t,n} | \\tilde{\\lambda}_{t,n}) = y_{t,n} \\log(\\tilde{\\lambda}_{t,n}) - \\tilde{\\lambda}_{t,n} - \\log(y_{t,n}!)\n$$\nSubstituting $\\log(\\tilde{\\lambda}_{t,n}) = \\mathbf{v}_n^\\top \\mathbf{u}_t + b_n$ and $\\tilde{\\lambda}_{t,n} = \\exp(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n)$, we get:\n$$\nL_{t,n}^{\\mathrm{SPF}} = y_{t,n}(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n) - \\exp(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n) - \\log(y_{t,n}!)\n$$\nSince $\\mathbf{u}_t$ is deterministic, this is the predictive log-likelihood for a single observation. The total predictive log-likelihood for the SPF model, $L_{\\mathrm{SPF}}$, is the sum of $L_{t,n}^{\\mathrm{SPF}}$ over all $t \\in \\{1, \\dots, T\\}$ and $n \\in \\mathcal{H}$.\n\n**Poisson Linear Dynamical System (PLDS) Model**\n\nIn the PLDS model, the latent state $\\mathbf{x}_t \\in \\mathbb{R}^K$ is a random variable. We are given that its variational posterior distribution is a Gaussian, $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{m}_t, \\mathbf{S}_t)$. The spike count $y_{t,n}$ is Poisson with rate $\\lambda_{t,n} = \\exp(\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n)$. To compute the required quantities, we must take expectations over the posterior distribution of $\\mathbf{x}_t$.\n\nLet's define the argument of the exponential, $\\alpha_{t,n} = \\mathbf{c}_n^\\top \\mathbf{x}_t + d_n$. Since $\\alpha_{t,n}$ is a linear transformation of a Gaussian random variable $\\mathbf{x}_t$, $\\alpha_{t,n}$ is also a Gaussian random variable. Its mean and variance are:\n$$\n\\mu_{\\alpha_{t,n}} = \\mathbb{E}[\\alpha_{t,n}] = \\mathbb{E}[\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n] = \\mathbf{c}_n^\\top \\mathbb{E}[\\mathbf{x}_t] + d_n = \\mathbf{c}_n^\\top \\mathbf{m}_t + d_n\n$$\n$$\n\\sigma^2_{\\alpha_{t,n}} = \\mathrm{Var}[\\alpha_{t,n}] = \\mathrm{Var}[\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n] = \\mathbf{c}_n^\\top \\mathrm{Var}[\\mathbf{x}_t] \\mathbf{c}_n = \\mathbf{c}_n^\\top \\mathbf{S}_t \\mathbf{c}_n\n$$\nSo, $\\alpha_{t,n} \\sim \\mathcal{N}(\\mu_{\\alpha_{t,n}}, \\sigma^2_{\\alpha_{t,n}})$.\n\nExpected Spike Count (PLDS):\nThe expected spike count is the expectation of the rate parameter $\\lambda_{t,n}$ over the posterior of $\\mathbf{x}_t$. The rate is $\\lambda_{t,n} = e^{\\alpha_{t,n}}$.\n$$\n\\mathbb{E}[y_{t,n}]_{\\mathrm{PLDS}} = \\mathbb{E}_{\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{m}_t, \\mathbf{S}_t)}[\\lambda_{t,n}] = \\mathbb{E}_{\\alpha_{t,n}}[e^{\\alpha_{t,n}}]\n$$\nThis is the moment-generating function of the Gaussian variable $\\alpha_{t,n}$ evaluated at $s=1$. For a Gaussian $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$, its MGF is $M_Z(s) = \\exp(s\\mu + \\frac{1}{2}s^2\\sigma^2)$. Setting $s=1$, we get:\n$$\n\\mathbb{E}[y_{t,n}]_{\\mathrm{PLDS}} = \\exp(\\mu_{\\alpha_{t,n}} + \\frac{1}{2}\\sigma^2_{\\alpha_{t,n}}) = \\exp\\left( (\\mathbf{c}_n^\\top \\mathbf{m}_t + d_n) + \\frac{1}{2}\\mathbf{c}_n^\\top \\mathbf{S}_t \\mathbf{c}_n \\right)\n$$\n\nPredictive Log-Likelihood (PLDS):\nThe predictive log-likelihood is the expectation of the log-likelihood function with respect to the posterior distribution of $\\mathbf{x}_t$.\n$$\nL_{t,n}^{\\mathrm{PLDS}} = \\mathbb{E}_{\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{m}_t, \\mathbf{S}_t)}[\\log P(y_{t,n} | \\mathbf{x}_t)] = \\mathbb{E}_{\\mathbf{x}_t}[y_{t,n}(\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n) - \\exp(\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n) - \\log(y_{t,n}!)]\n$$\nBy linearity of expectation:\n$$\nL_{t,n}^{\\mathrm{PLDS}} = y_{t,n}\\mathbb{E}[\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n] - \\mathbb{E}[\\exp(\\mathbf{c}_n^\\top \\mathbf{x}_t + d_n)] - \\log(y_{t,n}!)\n$$\nThe first term's expectation is $\\mu_{\\alpha_{t,n}}$. The second term's expectation is the expected spike count $\\mathbb{E}[y_{t,n}]_{\\mathrm{PLDS}}$.\n$$\nL_{t,n}^{\\mathrm{PLDS}} = y_{t,n}(\\mathbf{c}_n^\\top \\mathbf{m}_t + d_n) - \\exp\\left( \\mathbf{c}_n^\\top \\mathbf{m}_t + d_n + \\frac{1}{2}\\mathbf{c}_n^\\top \\mathbf{S}_t \\mathbf{c}_n \\right) - \\log(y_{t,n}!)\n$$\nThe total predictive log-likelihood for the PLDS model, $L_{\\mathrm{PLDS}}$, is the sum of $L_{t,n}^{\\mathrm{PLDS}}$ over all $t \\in \\{1, \\dots, T\\}$ and $n \\in \\mathcal{H}$.\n\n**Summary Metrics**\n\nWe are asked to compute two metrics for each test case.\n\n1. Difference in total predictive log-likelihood, $L_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}}$:\n$$\nL_{\\mathrm{PLDS}} - L_{\\mathrm{SPF}} = \\sum_{t=1}^T \\sum_{n \\in \\mathcal{H}} (L_{t,n}^{\\mathrm{PLDS}} - L_{t,n}^{\\mathrm{SPF}})\n$$\nNote that the term $\\log(y_{t,n}!)$ is common to both $L_{t,n}^{\\mathrm{PLDS}}$ and $L_{t,n}^{\\mathrm{SPF}}$, so it cancels out in the difference. Thus, we only need to compute:\n$$\nL_{t,n}^{\\mathrm{PLDS}} - L_{t,n}^{\\mathrm{SPF}} = \\left(y_{t,n}(\\mathbf{c}_n^\\top \\mathbf{m}_t + d_n) - \\mathbb{E}[y_{t,n}]_{\\mathrm{PLDS}}\\right) - \\left(y_{t,n}(\\mathbf{v}_n^\\top \\mathbf{u}_t + b_n) - \\mathbb{E}[y_{t,n}]_{\\mathrm{SPF}}\\right)\n$$\n\n2. Mean absolute difference between expected spike counts:\nThis is the average absolute difference between the two models' predictions for the expected number of spikes, averaged over all held-out neurons and time bins.\n$$\n\\mathrm{MAD}_{\\mathrm{counts}} = \\frac{1}{T \\cdot |\\mathcal{H}|} \\sum_{t=1}^T \\sum_{n \\in \\mathcal{H}} \\left| \\mathbb{E}[y_{t,n}]_{\\mathrm{PLDS}} - \\mathbb{E}[y_{t,n}]_{\\mathrm{SPF}} \\right|\n$$\nwhere $|\\mathcal{H}|$ is the number of held-out neurons, which is $2$. The total number of points for the average is $T \\times |\\mathcal{H}| = 5 \\times 2 = 10$.\n\nWith these formulas, we can proceed to the implementation and computation for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes model comparison metrics for PLDS and SPF on held-out neural data.\n    \"\"\"\n\n    # --- Shared Parameters ---\n    C = np.array([\n        [0.7, -0.3],\n        [-0.5, 0.6],\n        [0.2, 0.8],\n        [1.0, -0.4]\n    ])\n    d = np.array([-1.0, -0.7, -1.2, -0.5])\n    V = np.array([\n        [0.6, -0.2],\n        [-0.4, 0.5],\n        [0.15, 0.7],\n        [0.9, -0.1]\n    ])\n    b = np.array([-1.0, -0.7, -1.1, -0.6])\n    held_out_neurons = [2, 3]\n\n    # --- Test Case Data ---\n    \n    # Test Case 1\n    Y1 = np.array([\n        [5, 2, 3, 0],\n        [6, 3, 4, 1],\n        [4, 1, 2, 0],\n        [7, 2, 5, 1],\n        [3, 2, 1, 0]\n    ])\n    m1 = np.array([\n        [0.2, -0.1],\n        [0.4, -0.05],\n        [0.35, 0.0],\n        [0.3, 0.1],\n        [0.1, 0.2]\n    ])\n    S1 = np.array([\n        [0.2, 0.0],\n        [0.0, 0.1]\n    ])\n    u1 = np.array([\n        [0.25, -0.15],\n        [0.45, -0.10],\n        [0.40, -0.05],\n        [0.35, 0.05],\n        [0.15, 0.15]\n    ])\n\n    # Test Case 2\n    Y2 = Y1\n    m2 = m1\n    S2 = np.zeros((2, 2))\n    u2 = m2\n\n    # Test Case 3\n    Y3 = np.array([\n        [3, 1, 2, 0],\n        [4, 2, 3, 0],\n        [2, 1, 1, 0],\n        [5, 2, 4, 0],\n        [3, 1, 2, 0]\n    ])\n    m3 = np.zeros((5, 2))\n    S3 = np.array([\n        [0.3, 0.0],\n        [0.0, 0.3]\n    ])\n    u3 = np.zeros((5, 2))\n\n    test_cases = [\n        (Y1, m1, S1, u1),\n        (Y2, m2, S2, u2),\n        (Y3, m3, S3, u3)\n    ]\n\n    all_results = []\n\n    for Y, M, S, U in test_cases:\n        total_ll_plds_part = 0.0\n        total_ll_spf_part = 0.0\n        exp_count_diffs = []\n        T = Y.shape[0]\n\n        for t in range(T):\n            m_t = M[t]\n            u_t = U[t]\n            S_t = S  # S is constant across time in all given test cases\n\n            for n_idx in held_out_neurons:\n                y_tn = Y[t, n_idx]\n                c_n = C[n_idx]\n                d_n = d[n_idx]\n                v_n = V[n_idx]\n                b_n = b[n_idx]\n\n                # --- PLDS Calculations ---\n                mu_alpha = c_n @ m_t + d_n\n                var_alpha = c_n @ S_t @ c_n\n                \n                exp_count_plds = np.exp(mu_alpha + 0.5 * var_alpha)\n                ll_plds_part = y_tn * mu_alpha - exp_count_plds\n                total_ll_plds_part += ll_plds_part\n\n                # --- SPF Calculations ---\n                log_rate_spf = v_n @ u_t + b_n\n                exp_count_spf = np.exp(log_rate_spf)\n                \n                ll_spf_part = y_tn * log_rate_spf - exp_count_spf\n                total_ll_spf_part += ll_spf_part\n\n                # --- Metric Calculations ---\n                exp_count_diffs.append(np.abs(exp_count_plds - exp_count_spf))\n\n        # Calculate summary metrics for the current test case\n        ll_diff = total_ll_plds_part - total_ll_spf_part\n        mad_counts = np.mean(exp_count_diffs)\n        \n        all_results.append(ll_diff)\n        all_results.append(mad_counts)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond prediction, a primary goal of dynamical systems modeling in neuroscience is to uncover the computational structure embedded in neural activity. This involves analyzing the geometry of the learned vector field, for instance by identifying its fixed points and characterizing their stability. This exercise applies principles from dynamical systems theory to a learned model, using the Jacobian matrix and its eigenvalues to determine if fixed points are stable, unstable, or saddle-like, thereby providing a window into the underlying computational dynamics of the neural circuit .",
            "id": "4173330",
            "problem": "Consider a latent autonomous continuous-time dynamical system for neural population activity modeled as an Ordinary Differential Equation (ODE): $$\\frac{d\\mathbf{x}}{dt} = f_{\\theta}(\\mathbf{x}),$$ where the estimated vector field is defined componentwise via the elementwise hyperbolic tangent as $$f_{\\theta}(\\mathbf{x}) = \\mathbf{W}\\,\\tanh\\!\\big(\\mathbf{V}\\,\\mathbf{x} + \\mathbf{b}\\big) + \\mathbf{C}\\,\\mathbf{x} + \\mathbf{d},$$ with parameter tuple $\\theta = (\\mathbf{W}, \\mathbf{V}, \\mathbf{C}, \\mathbf{b}, \\mathbf{d})$. A candidate fixed point $\\mathbf{x}^{\\star}$ satisfies $f_{\\theta}(\\mathbf{x}^{\\star}) = \\mathbf{0}$. Local linearization around $\\mathbf{x}^{\\star}$ uses the Jacobian matrix $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$, defined by core multivariable calculus principles (the chain rule and linearization definition), and local stability for continuous-time systems is determined by the eigenvalues of $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$: the fixed point is asymptotically stable if and only if the real parts of all eigenvalues are strictly negative. No physical units are involved in this problem.\n\nYour task is to write a complete program that, for each provided test case, performs the following steps purely from first principles:\n- Compute the Jacobian $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$ at the given candidate fixed point using the definition of the Jacobian and the chain rule for compositions of linear maps and elementwise $\\tanh(\\cdot)$.\n- Compute the eigenvalues of $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$.\n- Decide the stability of the candidate fixed point by checking whether all eigenvalues have strictly negative real parts.\n- Return a boolean result for each test case: `True` if the candidate fixed point is asymptotically stable, and `False` otherwise.\n\nThe elementwise derivative of the hyperbolic tangent is a well-tested mathematical fact: $$\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^{2}(z).$$ Use this fact together with the chain rule to construct the Jacobian for the composite vector field.\n\nTest Suite. Use the following scientifically consistent parameter sets. Matrices and vectors are specified explicitly:\n\n- Test case $1$ (dimension $2$, nonlinearity present, stable):\n  $$\\mathbf{W} = \\begin{bmatrix} -0.3  0 \\\\ 0  -0.3 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.4  0 \\\\ 0  -0.4 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.$$\n\n- Test case $2$ (dimension $2$, saddle-type behavior):\n  $$\\mathbf{W} = \\begin{bmatrix} 0.2  0 \\\\ 0  0.2 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 3.0  0 \\\\ 0  0.2 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.1  0 \\\\ 0  -0.05 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.$$\n\n- Test case $3$ (dimension $3$, purely linear unstable):\n  $$\\mathbf{W} = \\begin{bmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} 0.1  0  0 \\\\ 0  0.2  0 \\\\ 0  0  0.3 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.$$\n\n- Test case $4$ (dimension $2$, marginal with a zero eigenvalue):\n  $$\\mathbf{W} = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.1  0 \\\\ 0  0.0 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.$$\n\n- Test case $5$ (dimension $1$, nontrivial nonlinearity at nonzero argument, stable):\n  $$\\mathbf{W} = \\begin{bmatrix} -0.8 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 2.5 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.05 \\end{bmatrix},$$\n  $$\\mathbf{x}^{\\star} = \\begin{bmatrix} 0.5 \\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix} -0.05 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0.6919232 \\end{bmatrix}.$$\n\nFor each test case, compute the boolean stability result as described. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example `[result1,result2,result3,result4,result5]`.",
            "solution": "The problem requires an analysis of the local asymptotic stability of candidate fixed points for a given continuous-time autonomous dynamical system. The system's evolution is described by the ordinary differential equation (ODE) $\\frac{d\\mathbf{x}}{dt} = f_{\\theta}(\\mathbf{x})$, where the state vector $\\mathbf{x}$ represents neural population activity and the vector field $f_{\\theta}(\\mathbf{x})$ is parameterized by $\\theta = (\\mathbf{W}, \\mathbf{V}, \\mathbf{C}, \\mathbf{b}, \\mathbf{d})$. A point $\\mathbf{x}^{\\star}$ is a fixed point if it satisfies the condition $f_{\\theta}(\\mathbf{x}^{\\star}) = \\mathbf{0}$.\n\nAccording to the Hartman-Grobman theorem and the principle of linearization for continuous-time systems, the local stability of a fixed point $\\mathbf{x}^{\\star}$ is determined by the eigenvalues of the Jacobian matrix of the vector field, $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x})$, evaluated at that fixed point. A fixed point $\\mathbf{x}^{\\star}$ is locally asymptotically stable if and only if all eigenvalues of the Jacobian matrix $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$ have strictly negative real parts.\n\nThe core of the task is to derive the analytical form of the Jacobian matrix $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x})$ and then use it to assess the stability for each provided test case. The vector field is given by:\n$$f_{\\theta}(\\mathbf{x}) = \\mathbf{W}\\,\\tanh\\!\\big(\\mathbf{V}\\,\\mathbf{x} + \\mathbf{b}\\big) + \\mathbf{C}\\,\\mathbf{x} + \\mathbf{d}$$\nThe Jacobian matrix $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x})$ is a matrix of partial derivatives, where the element $(i, j)$ is given by $J_{ij} = \\frac{\\partial f_i}{\\partial x_j}$. Due to the linearity of the differentiation operator, the Jacobian of a sum of vector functions is the sum of their individual Jacobians. We can thus analyze each term separately.\n\n$1$. The term $\\mathbf{C}\\mathbf{x}$ is a linear transformation of $\\mathbf{x}$. Its Jacobian is simply the matrix $\\mathbf{C}$.\n$2$. The term $\\mathbf{d}$ is a constant vector. Its derivative with respect to $\\mathbf{x}$ is the zero matrix, $\\mathbf{0}$.\n$3$. The first term, $\\mathbf{g}(\\mathbf{x}) = \\mathbf{W}\\,\\tanh(\\mathbf{V}\\,\\mathbf{x} + \\mathbf{b})$, is a composition of functions. We must apply the multivariable chain rule. Let us define the composition as follows:\n- Let $\\mathbf{u}(\\mathbf{x}) = \\mathbf{V}\\mathbf{x} + \\mathbf{b}$. This is an affine transformation. Its Jacobian with respect to $\\mathbf{x}$ is $\\mathbf{J}_{\\mathbf{u}} = \\mathbf{V}$.\n- Let $\\mathbf{h}(\\mathbf{u}) = \\tanh(\\mathbf{u})$, where the $\\tanh$ function is applied elementwise. The $k$-th component is $h_k(\\mathbf{u}) = \\tanh(u_k)$. The partial derivative $\\frac{\\partial h_k}{\\partial u_l}$ is non-zero only if $k=l$. Therefore, the Jacobian of $\\mathbf{h}$ with respect to $\\mathbf{u}$ is a diagonal matrix. Using the provided identity $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$, the diagonal entries of this Jacobian are $\\frac{d h_k}{d u_k} = 1 - \\tanh^2(u_k)$. We can represent this Jacobian matrix as:\n$$ \\mathbf{J}_{\\mathbf{h}}(\\mathbf{u}) = \\text{diag}\\left(1 - \\tanh^2(\\mathbf{u})\\right) $$\nwhere the expression in the $\\text{diag}(\\cdot)$ operator is a vector whose components are $1 - \\tanh^2(u_k)$.\n- The full term is a linear transformation by $\\mathbf{W}$ applied to $\\mathbf{h}(\\mathbf{u}(\\mathbf{x}))$. The Jacobian of this final linear transformation is just $\\mathbf{W}$.\n\nBy the chain rule, the Jacobian of the composite function $\\mathbf{g}(\\mathbf{x}) = \\mathbf{W}(\\mathbf{h}(\\mathbf{u}(\\mathbf{x})))$ is the product of the Jacobians of its constituent parts, evaluated at the appropriate points:\n$$ \\mathbf{J}_{\\mathbf{g}}(\\mathbf{x}) = \\mathbf{W} \\cdot \\mathbf{J}_{\\mathbf{h}}(\\mathbf{u}(\\mathbf{x})) \\cdot \\mathbf{J}_{\\mathbf{u}}(\\mathbf{x}) $$\nSubstituting the expressions for the component Jacobians, we get:\n$$ \\mathbf{J}_{\\mathbf{g}}(\\mathbf{x}) = \\mathbf{W} \\, \\text{diag}\\left(1 - \\tanh^2(\\mathbf{V}\\mathbf{x} + \\mathbf{b})\\right) \\, \\mathbf{V} $$\n\nCombining the Jacobians of all terms, the full Jacobian of the vector field $f_{\\theta}(\\mathbf{x})$ is:\n$$ \\mathbf{J}_{f_{\\theta}}(\\mathbf{x}) = \\mathbf{W} \\, \\text{diag}\\left(1 - \\tanh^2(\\mathbf{V}\\mathbf{x} + \\mathbf{b})\\right) \\, \\mathbf{V} + \\mathbf{C} $$\n\nThe algorithmic procedure to solve the problem for each test case is as follows:\n$1$. For a given set of parameters $(\\mathbf{W}, \\mathbf{V}, \\mathbf{C}, \\mathbf{b}, \\mathbf{d})$ and a candidate fixed point $\\mathbf{x}^{\\star}$, first compute the argument of the hyperbolic tangent function: $\\mathbf{u}^{\\star} = \\mathbf{V}\\mathbf{x}^{\\star} + \\mathbf{b}$.\n$2$. Construct the diagonal matrix $\\mathbf{D}^{\\star} = \\text{diag}(1 - \\tanh^2(\\mathbf{u}^{\\star}))$, where the operations are performed elementwise.\n$3$. Compute the numerical Jacobian matrix at the fixed point: $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star}) = \\mathbf{W}\\mathbf{D}^{\\star}\\mathbf{V} + \\mathbf{C}$.\n$4$. Calculate the eigenvalues of $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$.\n$5$. Determine stability by checking if the real part of every eigenvalue is strictly less than zero. If this condition holds, the fixed point is asymptotically stable ($\\texttt{True}$); otherwise, it is not ($\\texttt{False}$).\nThis sequence of operations will be performed for each test case to generate the final list of results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the stability of fixed points for several\n    dynamical systems.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (dimension 2, nonlinearity present, stable)\n        {\n            \"W\": np.array([[-0.3, 0], [0, -0.3]]),\n            \"V\": np.array([[1, 0], [0, 1]]),\n            \"C\": np.array([[-0.4, 0], [0, -0.4]]),\n            \"b\": np.array([[0], [0]]),\n            \"d\": np.array([[0], [0]]),\n            \"x_star\": np.array([[0], [0]]),\n        },\n        # Test case 2 (dimension 2, saddle-type behavior)\n        {\n            \"W\": np.array([[0.2, 0], [0, 0.2]]),\n            \"V\": np.array([[3.0, 0], [0, 0.2]]),\n            \"C\": np.array([[-0.1, 0], [0, -0.05]]),\n            \"b\": np.array([[0], [0]]),\n            \"d\": np.array([[0], [0]]),\n            \"x_star\": np.array([[0], [0]]),\n        },\n        # Test case 3 (dimension 3, purely linear unstable)\n        {\n            \"W\": np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n            \"V\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]),\n            \"C\": np.array([[0.1, 0, 0], [0, 0.2, 0], [0, 0, 0.3]]),\n            \"b\": np.array([[0], [0], [0]]),\n            \"d\": np.array([[0], [0], [0]]),\n            \"x_star\": np.array([[0], [0], [0]]),\n        },\n        # Test case 4 (dimension 2, marginal with a zero eigenvalue)\n        {\n            \"W\": np.array([[0, 0], [0, 0]]),\n            \"V\": np.array([[1, 0], [0, 1]]),\n            \"C\": np.array([[-0.1, 0], [0, 0.0]]),\n            \"b\": np.array([[0], [0]]),\n            \"d\": np.array([[0], [0]]),\n            \"x_star\": np.array([[0], [0]]),\n        },\n        # Test case 5 (dimension 1, nontrivial nonlinearity at nonzero argument, stable)\n        {\n            \"W\": np.array([[-0.8]]),\n            \"V\": np.array([[2.5]]),\n            \"C\": np.array([[-0.05]]),\n            \"b\": np.array([[-0.05]]),\n            \"d\": np.array([[0.6919232]]),\n            \"x_star\": np.array([[0.5]]),\n        },\n    ]\n\n    def check_stability(params):\n        \"\"\"\n        Computes the Jacobian and checks the stability of the given fixed point.\n\n        Args:\n            params (dict): A dictionary containing the parameters W, V, C, b, and x_star.\n\n        Returns:\n            bool: True if the fixed point is asymptotically stable, False otherwise.\n        \"\"\"\n        W = params[\"W\"]\n        V = params[\"V\"]\n        C = params[\"C\"]\n        b = params[\"b\"]\n        x_star = params[\"x_star\"]\n\n        # 1. Compute the argument of the tanh function\n        u_star = V @ x_star + b\n\n        # 2. Compute the diagonal elements of the Jacobian of the nonlinearity\n        # The derivative of tanh(z) is 1 - tanh^2(z).\n        # We need to flatten the result for np.diag as it expects a 1D array.\n        tanh_deriv_vec = 1 - np.tanh(u_star)**2\n        D_star = np.diag(tanh_deriv_vec.flatten())\n\n        # 3. Compute the full Jacobian matrix at the fixed point\n        # J = W * D * V + C\n        J = W @ D_star @ V + C\n\n        # 4. Calculate the eigenvalues of the Jacobian\n        eigenvalues = np.linalg.eigvals(J)\n        \n        # 5. Check if all eigenvalues have strictly negative real parts\n        is_stable = np.all(np.real(eigenvalues)  0)\n\n        return is_stable\n\n    results = []\n    for case in test_cases:\n        result = check_stability(case)\n        results.append(result)\n\n    # Format the final output as a comma-separated list of lowercase strings.\n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}