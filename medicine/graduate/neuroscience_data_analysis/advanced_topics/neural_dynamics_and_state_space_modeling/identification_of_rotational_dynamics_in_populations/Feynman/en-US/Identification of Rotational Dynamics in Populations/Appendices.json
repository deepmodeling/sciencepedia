{
    "hands_on_practices": [
        {
            "introduction": "At the heart of identifying rotational dynamics lies the challenge of quantifying the rate of rotation from observed state trajectories. This first practice grounds this abstract goal in a concrete mathematical procedure . By deriving the estimator for the mean angular velocity from the first principles of least squares, you will see precisely how the relationship between a state vector and its time derivative can be leveraged to extract a single, powerful descriptor of the system's rotational tendency.",
            "id": "4169453",
            "problem": "Consider a two-dimensional neural population state trajectory $x(t) \\in \\mathbb{R}^{2}$ obtained by projecting high-dimensional firing-rate vectors onto a plane that captures dominant rotational structure. Assume the dynamics on this plane are well-approximated by a linear time-invariant model with a skew-symmetric generator, so that the time derivative $\\dot{x}(t)$ satisfies $\\dot{x}(t) \\approx M x(t)$ with $M$ skew-symmetric. In two dimensions, any skew-symmetric matrix can be written as $M = \\omega J$, where $\\omega \\in \\mathbb{R}$ is an angular velocity and $J$ is the $90^{\\circ}$ counterclockwise rotation matrix\n$$\nJ \\;=\\; \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix}.\n$$\nDefine the perpendicular velocity as $\\dot{x}_{\\perp}(t) := J^{\\top} \\dot{x}(t)$, which rotates $\\dot{x}(t)$ by $90^{\\circ}$ clockwise so that, for ideal rotational dynamics, $\\dot{x}_{\\perp}(t) = \\omega x(t)$ holds instantaneously.\n\nStarting only from these definitions and the least-squares principle, derive the closed-form expression for the mean angular velocity $\\bar{\\omega}$ across time obtained by regressing $\\dot{x}_{\\perp}(t)$ onto $x(t)$ with zero intercept over $K$ samples $\\{x(t_k), \\dot{x}(t_k)\\}_{k=1}^{K}$. Your derivation must make clear why this slope is interpretable as a mean angular velocity and how the weighting by the state magnitude arises.\n\nThen, compute the numerical value of $\\bar{\\omega}$ (in radians per second) from the following three samples. Treat these as synchronized samples $(x(t_k), \\dot{x}(t_k))$ and take $J$ as above:\n- $x(t_1) = \\begin{pmatrix} 1.0 \\\\ 0.0 \\end{pmatrix}$, $\\dot{x}(t_1) = \\begin{pmatrix} 0.05 \\\\ 2.05 \\end{pmatrix}$,\n- $x(t_2) = \\begin{pmatrix} 0.0 \\\\ 1.0 \\end{pmatrix}$, $\\dot{x}(t_2) = \\begin{pmatrix} -2.02 \\\\ -0.03 \\end{pmatrix}$,\n- $x(t_3) = \\begin{pmatrix} 1.0 \\\\ 1.0 \\end{pmatrix}$, $\\dot{x}(t_3) = \\begin{pmatrix} -2.10 \\\\ 1.90 \\end{pmatrix}$.\n\nRound your final numerical answer to four significant figures. Express the final value in radians per second.",
            "solution": "### Solution Derivation\nThe objective is to find the scalar $\\bar{\\omega}$ that best fits the model $\\dot{x}_{\\perp}(t_k) \\approx \\bar{\\omega} x(t_k)$ for a set of $K$ discrete samples. This is a linear regression problem with a zero intercept. The \"dependent variable\" is the vector $\\dot{x}_{\\perp}(t_k)$ and the \"independent variable\" is the vector $x(t_k)$.\n\nAccording to the least-squares principle, we must minimize the sum of the squared errors. The error for the $k$-th sample is the vector difference $e_k = \\dot{x}_{\\perp}(t_k) - \\bar{\\omega} x(t_k)$. The quantity to be minimized is the sum of the squared Euclidean norms of these error vectors:\n$$\nS(\\bar{\\omega}) = \\sum_{k=1}^{K} \\| e_k \\|^2 = \\sum_{k=1}^{K} \\| \\dot{x}_{\\perp}(t_k) - \\bar{\\omega} x(t_k) \\|^2\n$$\nThe squared norm of a vector $v$ is $v^{\\top}v$. Expanding the term for a single sample (omitting the index $k$ for clarity):\n$$\n\\| \\dot{x}_{\\perp} - \\bar{\\omega} x \\|^2 = ( \\dot{x}_{\\perp} - \\bar{\\omega} x )^{\\top} ( \\dot{x}_{\\perp} - \\bar{\\omega} x ) = \\dot{x}_{\\perp}^{\\top}\\dot{x}_{\\perp} - 2\\bar{\\omega} x^{\\top}\\dot{x}_{\\perp} + \\bar{\\omega}^2 x^{\\top}x\n$$\nNow, we restore the summation over all $K$ samples:\n$$\nS(\\bar{\\omega}) = \\sum_{k=1}^{K} \\left( \\| \\dot{x}_{\\perp}(t_k) \\|^2 - 2\\bar{\\omega} x(t_k)^{\\top}\\dot{x}_{\\perp}(t_k) + \\bar{\\omega}^2 \\|x(t_k)\\|^2 \\right)\n$$\nTo find the value of $\\bar{\\omega}$ that minimizes $S$, we take the derivative of $S$ with respect to $\\bar{\\omega}$ and set it to zero:\n$$\n\\frac{dS}{d\\bar{\\omega}} = \\sum_{k=1}^{K} \\left( -2 x(t_k)^{\\top}\\dot{x}_{\\perp}(t_k) + 2\\bar{\\omega} \\|x(t_k)\\|^2 \\right) = 0\n$$\nDividing by $2$ and rearranging the terms to solve for $\\bar{\\omega}$:\n$$\n\\bar{\\omega} \\sum_{k=1}^{K} \\|x(t_k)\\|^2 = \\sum_{k=1}^{K} x(t_k)^{\\top}\\dot{x}_{\\perp}(t_k)\n$$\nThis yields the closed-form expression for $\\bar{\\omega}$:\n$$\n\\bar{\\omega} = \\frac{\\sum_{k=1}^{K} x(t_k)^{\\top}\\dot{x}_{\\perp}(t_k)}{\\sum_{k=1}^{K} \\|x(t_k)\\|^2}\n$$\nSubstituting the definition $\\dot{x}_{\\perp}(t_k) = J^{\\top}\\dot{x}(t_k)$, we get:\n$$\n\\bar{\\omega} = \\frac{\\sum_{k=1}^{K} x(t_k)^{\\top} J^{\\top} \\dot{x}(t_k)}{\\sum_{k=1}^{K} x(t_k)^{\\top}x(t_k)}\n$$\nThis expression is interpretable as a mean angular velocity. For each sample $k$, one could estimate an instantaneous angular velocity as $\\omega_k = (x_k^{\\top}\\dot{x}_{\\perp,k}) / \\|x_k\\|^2$. The formula for $\\bar{\\omega}$ is a weighted average of these instantaneous estimates, where the weight for each sample is its squared magnitude, $\\|x_k\\|^2$. This weighting scheme is sensible: states far from the origin (large $\\|x_k\\|$) have larger tangential velocities for a given angular velocity and thus provide a more robust signal for its estimation. Conversely, states near the origin are more affected by noise and provide a weaker signal, so they are down-weighted.\n\n### Numerical Calculation\nWe now apply this formula to the $K=3$ provided data samples.\nGiven $J = \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix}$, its transpose is $J^{\\top} = \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}$.\nThe numerator is $N = \\sum_{k=1}^{3} x(t_k)^{\\top} J^{\\top} \\dot{x}(t_k)$ and the denominator is $D = \\sum_{k=1}^{3} \\|x(t_k)\\|^2$.\n\nFor $k=1$:\n- $x_1 = \\begin{pmatrix} 1.0 \\\\ 0.0 \\end{pmatrix}$, $\\dot{x}_1 = \\begin{pmatrix} 0.05 \\\\ 2.05 \\end{pmatrix}$\n- Numerator term: $x_1^{\\top}J^{\\top}\\dot{x}_1 = \\begin{pmatrix} 1.0  0.0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix} \\begin{pmatrix} 0.05 \\\\ 2.05 \\end{pmatrix} = \\begin{pmatrix} 1.0  0.0 \\end{pmatrix} \\begin{pmatrix} 2.05 \\\\ -0.05 \\end{pmatrix} = 2.05$\n- Denominator term: $\\|x_1\\|^2 = (1.0)^2 + (0.0)^2 = 1.0$\n\nFor $k=2$:\n- $x_2 = \\begin{pmatrix} 0.0 \\\\ 1.0 \\end{pmatrix}$, $\\dot{x}_2 = \\begin{pmatrix} -2.02 \\\\ -0.03 \\end{pmatrix}$\n- Numerator term: $x_2^{\\top}J^{\\top}\\dot{x}_2 = \\begin{pmatrix} 0.0  1.0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix} \\begin{pmatrix} -2.02 \\\\ -0.03 \\end{pmatrix} = \\begin{pmatrix} 0.0  1.0 \\end{pmatrix} \\begin{pmatrix} -0.03 \\\\ 2.02 \\end{pmatrix} = 2.02$\n- Denominator term: $\\|x_2\\|^2 = (0.0)^2 + (1.0)^2 = 1.0$\n\nFor $k=3$:\n- $x_3 = \\begin{pmatrix} 1.0 \\\\ 1.0 \\end{pmatrix}$, $\\dot{x}_3 = \\begin{pmatrix} -2.10 \\\\ 1.90 \\end{pmatrix}$\n- Numerator term: $x_3^{\\top}J^{\\top}\\dot{x}_3 = \\begin{pmatrix} 1.0  1.0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix} \\begin{pmatrix} -2.10 \\\\ 1.90 \\end{pmatrix} = \\begin{pmatrix} 1.0  1.0 \\end{pmatrix} \\begin{pmatrix} 1.90 \\\\ 2.10 \\end{pmatrix} = 1.90 + 2.10 = 4.00$\n- Denominator term: $\\|x_3\\|^2 = (1.0)^2 + (1.0)^2 = 2.0$\n\nNow, we sum the terms to calculate the total numerator $N$ and denominator $D$:\n- $N = 2.05 + 2.02 + 4.00 = 8.07$\n- $D = 1.0 + 1.0 + 2.0 = 4.0$\n\nFinally, we compute $\\bar{\\omega}$:\n$$\n\\bar{\\omega} = \\frac{N}{D} = \\frac{8.07}{4.0} = 2.0175\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n\\bar{\\omega} \\approx 2.018\n$$\nThe units are radians per second.",
            "answer": "$$\n\\boxed{2.018}\n$$"
        },
        {
            "introduction": "While the ideal model of rotation produces perfect circles, the trajectories we observe in data are often elliptical. This exercise explores the geometric reasons for this phenomenon, treating the dimensionality reduction step as a linear transformation from a 'latent' space of pure rotation to an 'observed' jPCA plane . By analyzing this transformation, you will understand the conditions that produce elliptical trajectories and learn how to quantify their shape, a key skill for correctly interpreting the geometry of neural dynamics.",
            "id": "4169442",
            "problem": "Consider a two-dimensional latent dynamical system intended to model the dominant rotational mode identified by the method commonly referred to as jPCA in a neural population. Let the latent state be $z(t) \\in \\mathbb{R}^{2}$, evolving according to a time-invariant linear ordinary differential equation (ODE)\n$$\n\\frac{d z}{d t} = \\Omega J z, \\quad J = \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix},\n$$\nwith constant angular frequency $\\Omega \\in \\mathbb{R}$. Suppose the two-dimensional plane displayed to the analyst (the “jPC plane”) is related to the latent plane via an invertible linear map $y = B z$, where $B \\in \\mathbb{R}^{2 \\times 2}$ is full rank. Assume that Euclidean distances and angles in the displayed plane are measured with the standard inner product on $\\mathbb{R}^{2}$.\n\nUsing only the fundamental facts that (i) solutions of linear time-invariant ODEs are given by matrix exponentials, (ii) skew-symmetric generators preserve Euclidean norms in their native coordinates, and (iii) linear maps send circles to ellipses except in the special case of orthogonal-scaling maps, carry out the following:\n\n1. Prove that in the latent coordinates $z(t)$, trajectories lie on circles centered at the origin and give the conserved quantity associated with this invariance.\n\n2. Transform the dynamics to the displayed plane and show that the trajectories $y(t)$ lie on level sets of a positive-definite quadratic form. Derive the necessary and sufficient condition on $B$ for these level sets to be circles in the displayed plane.\n\n3. Derive a closed-form expression for the ratio of the semi-major to semi-minor axis lengths of the trajectory in the displayed plane in terms of $B$ alone (independent of $\\Omega$ and the initial condition), expressed using only matrix operations that do not depend on a particular coordinate system.\n\n4. For the specific nonorthogonal scaling\n$$\nB = \\begin{pmatrix} 2  1 \\\\ 0  1 \\end{pmatrix},\n$$\ncompute the exact value of the semi-axis length ratio you derived in step 3. Give your final answer as a single simplified closed-form expression. Do not approximate; no rounding is required.",
            "solution": "The problem asks for a four-part derivation concerning a two-dimensional linear dynamical system. We will address each part in sequence.\n\n1. Trajectories in the latent space $z(t)$.\n\nThe dynamics in the latent space are given by the linear time-invariant ordinary differential equation (ODE):\n$$\n\\frac{d z}{d t} = \\Omega J z, \\quad \\text{where} \\quad z(t) = \\begin{pmatrix} z_1(t) \\\\ z_2(t) \\end{pmatrix} \\in \\mathbb{R}^2, \\quad J = \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix}, \\quad \\Omega \\in \\mathbb{R}.\n$$\nThe generator of the dynamics is the matrix $A_z = \\Omega J$. This matrix is skew-symmetric because $A_z^T = (\\Omega J)^T = \\Omega J^T = \\Omega (-J) = -A_z$. As stated in fundamental fact (ii), skew-symmetric generators preserve the Euclidean norm in their native coordinates. To prove this explicitly, we consider the squared Euclidean norm of the state vector, $\\|z(t)\\|^2 = z(t)^T z(t)$. We differentiate this quantity with respect to time $t$:\n$$\n\\frac{d}{dt} \\left( z^T z \\right) = \\left( \\frac{dz}{dt} \\right)^T z + z^T \\left( \\frac{dz}{dt} \\right).\n$$\nSubstituting the given ODE, $\\frac{dz}{dt} = A_z z$:\n$$\n\\frac{d}{dt} \\left( z^T z \\right) = (A_z z)^T z + z^T (A_z z) = z^T A_z^T z + z^T A_z z.\n$$\nUsing the skew-symmetry, $A_z^T = -A_z$, we have:\n$$\n\\frac{d}{dt} \\left( z^T z \\right) = z^T (-A_z) z + z^T A_z z = -z^T A_z z + z^T A_z z = 0.\n$$\nSince the time derivative of $z^T z$ is zero, this quantity is a constant of motion, or a conserved quantity. Let this constant be $R^2$, determined by the initial condition $z(0)$: $R^2 = \\|z(0)\\|^2 = z_1(0)^2 + z_2(0)^2$. The equation $\\|z(t)\\|^2 = R^2$ describes a circle of radius $R$ centered at the origin in the $z_1$-$z_2$ plane. Therefore, trajectories $z(t)$ lie on circles centered at the origin. The associated conserved quantity is the squared Euclidean norm of the latent state, $C = \\|z(t)\\|^2 = z_1(t)^2 + z_2(t)^2$.\n\n2. Trajectories in the displayed plane $y(t)$.\n\nThe displayed coordinates $y(t)$ are related to the latent coordinates $z(t)$ by the invertible linear map $y = B z$. This implies $z = B^{-1} y$. To find the dynamics of $y(t)$, we differentiate its definition with respect to time:\n$$\n\\frac{dy}{dt} = \\frac{d}{dt} (B z) = B \\frac{dz}{dt}.\n$$\nSubstituting the latent dynamics $\\frac{dz}{dt} = \\Omega J z$:\n$$\n\\frac{dy}{dt} = B (\\Omega J z).\n$$\nTo express the dynamics solely in terms of $y$, we substitute $z = B^{-1} y$:\n$$\n\\frac{dy}{dt} = B (\\Omega J B^{-1} y) = \\Omega (B J B^{-1}) y.\n$$\nThe dynamics in the displayed plane are governed by the linear ODE $\\frac{dy}{dt} = A_y y$, where the generator is $A_y = \\Omega B J B^{-1}$.\n\nThe trajectories $y(t)$ lie on level sets of the conserved quantity from the latent space, transformed into the $y$ coordinates. The conserved quantity is $z^T z = R^2$. Substituting $z = B^{-1} y$:\n$$\n(B^{-1} y)^T (B^{-1} y) = R^2 \\implies y^T (B^{-1})^T B^{-1} y = R^2 \\implies y^T (B B^T)^{-1} y = R^2.\n$$\nThis equation defines the level sets for the trajectories $y(t)$. The matrix $Q = (B B^T)^{-1}$ is symmetric and positive-definite because $B$ is invertible. Therefore, the equation $y^T Q y = R^2$ describes a family of ellipses centered at the origin. This demonstrates that trajectories in the displayed plane lie on level sets of a positive-definite quadratic form.\n\nFor these elliptical trajectories to be circles, the level sets must be of the form $\\|y\\|^2 = \\text{constant}$. This occurs if and only if the quadratic form matrix $Q = (B B^T)^{-1}$ is a scalar multiple of the identity matrix, i.e., $Q = c I$ for some positive scalar $c$.\n$$\n(B B^T)^{-1} = c I \\implies B B^T = (c I)^{-1} = \\frac{1}{c} I.\n$$\nLet $k^2 = 1/c$. The condition is $B B^T = k^2 I$. A matrix $B$ satisfying this condition is a scaled orthogonal matrix. This can be seen by considering a matrix $U = \\frac{1}{k} B$. Then $U U^T = (\\frac{1}{k} B)(\\frac{1}{k} B)^T = \\frac{1}{k^2} B B^T = \\frac{1}{k^2} (k^2 I) = I$, which is the definition of an orthogonal matrix. Thus, the necessary and sufficient condition on $B$ for the trajectories to be circles in the displayed plane is that $B$ must be a scaled orthogonal matrix. This aligns with fundamental fact (iii) that only special linear maps (orthogonal-scaling) send circles to circles.\n\n3. Ratio of semi-axis lengths.\n\nThe trajectories $y(t)$ are ellipses defined by the equation $y^T (B B^T)^{-1} y = R^2$. The lengths of the semi-axes of an ellipse defined by $y^T Q y = 1$ are given by $1/\\sqrt{\\lambda_i}$, where $\\lambda_i$ are the eigenvalues of the matrix $Q$. In our case, the equation is $y^T (B B^T)^{-1} y = R^2$, which can be written as $y^T (\\frac{1}{R^2} (B B^T)^{-1}) y = 1$. The semi-axis lengths are thus $1/\\sqrt{\\lambda_i'}$, where $\\lambda_i'$ are the eigenvalues of $\\frac{1}{R^2} (B B^T)^{-1}$.\nThe eigenvalues of $\\frac{1}{R^2} (B B^T)^{-1}$ are $\\frac{1}{R^2} \\lambda_i( (B B^T)^{-1} )$. The eigenvalues of an inverse matrix are the reciprocals of the eigenvalues of the original matrix. Let $\\mu_i$ be the eigenvalues of $B B^T$. Then the eigenvalues of $(B B^T)^{-1}$ are $1/\\mu_i$. So, $\\lambda_i' = \\frac{1}{R^2 \\mu_i}$.\nThe semi-axis lengths are $a_j = 1/\\sqrt{\\lambda_j'} = \\sqrt{R^2 \\mu_j} = R \\sqrt{\\mu_j}$.\nThe semi-major axis corresponds to the maximum eigenvalue of $B B^T$, $\\mu_{\\max}$, and the semi-minor axis corresponds to the minimum eigenvalue, $\\mu_{\\min}$.\nSemi-major axis length: $a_{\\text{major}} = R \\sqrt{\\mu_{\\max}(B B^T)}$.\nSemi-minor axis length: $a_{\\text{minor}} = R \\sqrt{\\mu_{\\min}(B B^T)}$.\nThe ratio of these lengths is:\n$$\n\\text{Ratio} = \\frac{a_{\\text{major}}}{a_{\\text{minor}}} = \\frac{R \\sqrt{\\mu_{\\max}(B B^T)}}{R \\sqrt{\\mu_{\\min}(B B^T)}} = \\sqrt{\\frac{\\mu_{\\max}(B B^T)}{\\mu_{\\min}(B B^T)}}.\n$$\nThe eigenvalues of $B B^T$ are the squares of the singular values of $B$, denoted $\\sigma_i$. So, $\\mu_i = \\sigma_i^2$. The ratio becomes:\n$$\n\\text{Ratio} = \\sqrt{\\frac{\\sigma_{\\max}(B)^2}{\\sigma_{\\min}(B)^2}} = \\frac{\\sigma_{\\max}(B)}{\\sigma_{\\min}(B)}.\n$$\nThis ratio is the condition number of the matrix $B$, $\\kappa_2(B)$. To express this in a closed form using matrix operations, we find the eigenvalues of the symmetric positive-definite matrix $C = B^T B$, which are identical to the eigenvalues of $B B^T$. For a $2 \\times 2$ matrix $C$, the eigenvalues $\\mu$ are the roots of the characteristic polynomial $\\lambda^2 - \\text{tr}(C)\\lambda + \\det(C) = 0$.\nThe roots are $\\mu = \\frac{\\text{tr}(C) \\pm \\sqrt{\\text{tr}(C)^2 - 4\\det(C)}}{2}$.\nSo, $\\mu_{\\max} = \\frac{\\text{tr}(B^T B) + \\sqrt{(\\text{tr}(B^T B))^2 - 4\\det(B^T B)}}{2}$ and $\\mu_{\\min} = \\frac{\\text{tr}(B^T B) - \\sqrt{(\\text{tr}(B^T B))^2 - 4\\det(B^T B)}}{2}$.\nThe ratio of the semi-axis lengths is $\\sqrt{\\mu_{\\max}/\\mu_{\\min}}$. Let $T = \\text{tr}(B^T B)$ and $D = \\det(B^T B)$.\n$$\n\\left(\\frac{a_{\\text{major}}}{a_{\\text{minor}}}\\right)^2 = \\frac{T + \\sqrt{T^2 - 4D}}{T - \\sqrt{T^2 - 4D}} = \\frac{(T + \\sqrt{T^2 - 4D})^2}{(T - \\sqrt{T^2 - 4D})(T + \\sqrt{T^2 - 4D})} = \\frac{(T + \\sqrt{T^2 - 4D})^2}{T^2 - (T^2 - 4D)} = \\frac{(T + \\sqrt{T^2 - 4D})^2}{4D}.\n$$\nTaking the square root and noting that $D = \\det(B^T B) = \\det(B^T)\\det(B) = (\\det(B))^2  0$:\n$$\n\\text{Ratio} = \\frac{T + \\sqrt{T^2 - 4D}}{2\\sqrt{D}} = \\frac{\\text{tr}(B^T B) + \\sqrt{(\\text{tr}(B^T B))^2 - 4(\\det(B))^2}}{2|\\det(B)|}.\n$$\nThis is the desired closed-form expression using coordinate-independent matrix operations.\n\n4. Computation for a specific matrix $B$.\n\nWe are given the specific matrix:\n$$\nB = \\begin{pmatrix} 2  1 \\\\ 0  1 \\end{pmatrix}.\n$$\nWe first compute $B^T B$:\n$$\nB^T = \\begin{pmatrix} 2  0 \\\\ 1  1 \\end{pmatrix},\n$$\n$$\nB^T B = \\begin{pmatrix} 2  0 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 4  2 \\\\ 2  2 \\end{pmatrix}.\n$$\nNext, we compute the required quantities for our formula: the trace of $B^T B$ and the determinant of $B$.\n$$\n\\text{tr}(B^T B) = 4 + 2 = 6.\n$$\n$$\n\\det(B) = (2)(1) - (1)(0) = 2.\n$$\nThus, $|\\det(B)| = 2$.\nNow, we substitute these values into the expression for the ratio derived in part 3:\n$$\n\\text{Ratio} = \\frac{6 + \\sqrt{6^2 - 4(2)^2}}{2|2|} = \\frac{6 + \\sqrt{36 - 16}}{4} = \\frac{6 + \\sqrt{20}}{4}.\n$$\nSimplifying the square root, $\\sqrt{20} = \\sqrt{4 \\times 5} = 2\\sqrt{5}$:\n$$\n\\text{Ratio} = \\frac{6 + 2\\sqrt{5}}{4} = \\frac{2(3 + \\sqrt{5})}{4} = \\frac{3 + \\sqrt{5}}{2}.\n$$\nThe exact value of the semi-axis length ratio is $\\frac{3 + \\sqrt{5}}{2}$. This is recognized as the golden ratio, $\\phi$.",
            "answer": "$$\n\\boxed{\\frac{3 + \\sqrt{5}}{2}}\n$$"
        },
        {
            "introduction": "The successful application of any linear model depends critically on appropriate data preprocessing. This final practice addresses a foundational step: data centering, and its importance for defining a meaningful origin for the dynamics . You will investigate how fitting a linear dynamical system is affected by arbitrary translations of the data, and in doing so, appreciate why centering is essential for ensuring the resulting dynamics matrix reflects the system's behavior around a well-defined operating point.",
            "id": "4169521",
            "problem": "You are analyzing condition-averaged neural population trajectories using the method that fits a linear time-invariant dynamical system in a low-dimensional subspace identified by Principal Component Analysis (PCA). Let the state be $x(t) \\in \\mathbb{R}^{k}$ for $t \\in \\{t_{1},\\dots,t_{T}\\}$, where $k$ is the number of retained principal components and $\\dot{x}(t)$ denotes the time derivative. A common step in identifying rotational dynamics (as in the jPCA method that fits a skew-symmetric linear operator) is to estimate a linear operator $A \\in \\mathbb{R}^{k \\times k}$ from the least-squares problem\n$$\n\\min_{A,b}\\ \\sum_{t=1}^{T} \\left\\| \\dot{x}(t) - A\\,x(t) - b \\right\\|_{2}^{2},\n$$\npossibly with the intercept $b \\in \\mathbb{R}^{k}$ either included ($b$ free) or excluded ($b \\equiv 0$). Consider forming an offset version of the data by adding a constant vector $c \\in \\mathbb{R}^{k}$ to all time points, $x_{c}(t) \\equiv x(t) + c$, while keeping the same $\\dot{x}(t)$. You are told that centering the data across time (and, when applicable, across conditions) is standard before fitting $A$, i.e., replacing $x(t)$ by $x(t) - \\bar{x}$ and $\\dot{x}(t)$ by $\\dot{x}(t) - \\overline{\\dot{x}}$, where $\\bar{x} \\equiv \\frac{1}{T}\\sum_{t=1}^{T} x(t)$ and $\\overline{\\dot{x}} \\equiv \\frac{1}{T}\\sum_{t=1}^{T} \\dot{x}(t)$.\n\nFrom first principles involving the linearity of differentiation and the normal equations for least squares, decide which statements are correct:\n\nA. If the intercept $b$ is included in the regression, the estimated $\\hat{A}$ is invariant to adding any constant offset $c$ to the predictors, i.e., fitting $\\dot{x}(t)$ from $x(t)$ or from $x_{c}(t)$ yields the same $\\hat{A}$, with $\\hat{b}$ shifting to absorb the offset.\n\nB. If the intercept $b$ is excluded and the data are not centered, adding a constant offset $c$ can change $\\hat{A}$; centering $x(t)$ and $\\dot{x}(t)$ removes this dependence and restores invariance of $\\hat{A}$ to constant offsets.\n\nC. Adding a constant offset $c$ to $x(t)$ changes $\\dot{x}(t)$ and therefore necessarily changes the estimated $\\hat{A}$ regardless of whether $b$ is included or the data are centered.\n\nD. Under a skew-symmetric constraint on $A$ (as used to emphasize rotations), the constraint itself makes centering unnecessary, because a skew-symmetric $A$ implicitly eliminates any effect of constant offsets on $\\hat{A}$ even when $b$ is excluded.\n\nE. Centering is essential for correct interpretation of rotational dynamics because it aligns the fitted $\\hat{A}$ with the local linearization (Jacobian) of the flow around a well-defined operating point (e.g., the mean state), ensuring that extracted rotations reflect dynamics about that point rather than artifacts of translation.\n\nSelect all that apply.",
            "solution": "We begin from two foundational facts. First, differentiation is linear: if $x_{c}(t) \\equiv x(t) + c$ with constant $c \\in \\mathbb{R}^{k}$, then $\\dot{x}_{c}(t) = \\dot{x}(t) + \\dot{c} = \\dot{x}(t)$ because $\\dot{c} = 0$. Second, the ordinary least squares solution with an intercept equals the solution obtained by regressing centered responses on centered predictors; and the normal equations reveal explicitly how means enter when the intercept is excluded.\n\nIntroduce matrices $X \\in \\mathbb{R}^{k \\times T}$ and $\\dot{X} \\in \\mathbb{R}^{k \\times T}$ with columns $x(t)$ and $\\dot{x}(t)$, respectively. Let $\\mathbf{1} \\in \\mathbb{R}^{T}$ be the vector of ones and define the centering matrix $H \\equiv I - \\frac{1}{T}\\,\\mathbf{1}\\mathbf{1}^{\\top}$. Then $XH$ subtracts the column mean: $XH = [x(t) - \\bar{x}]_{t=1}^{T}$, and similarly $\\dot{X}H = [\\dot{x}(t) - \\overline{\\dot{x}}]_{t=1}^{T}$. The offset data are $X_{c} \\equiv X + c\\,\\mathbf{1}^{\\top}$, and $X_{c}H = XH$ because $H\\mathbf{1} = 0$.\n\nCase with intercept. The least-squares objective with $(A,b)$ is\n$$\n\\min_{A,b}\\ \\left\\| \\dot{X} - A X - b\\,\\mathbf{1}^{\\top} \\right\\|_{F}^{2}.\n$$\nA standard, well-tested fact in linear regression is that the optimal $\\hat{A}$ from this problem equals the minimizer of\n$$\n\\min_{A}\\ \\left\\| \\dot{X}H - A\\,XH \\right\\|_{F}^{2},\n$$\nwith the intercept subsequently given by $\\hat{b} = \\overline{\\dot{x}} - \\hat{A}\\,\\bar{x}$. This follows directly from the normal equations obtained by differentiating the objective with respect to $A$ and $b$, setting derivatives to zero, and solving; equivalently, by noting that projecting both predictors and responses onto the mean-zero subspace via $H$ decouples the intercept. Crucially, if we replace $X$ by $X_{c} \\equiv X + c\\,\\mathbf{1}^{\\top}$, the centered predictors are unchanged: $X_{c}H = XH$. Therefore the minimizer $\\hat{A}$ of $\\|\\dot{X}H - A\\,XH\\|_{F}^{2}$ is invariant to adding $c$. The intercept adjusts to $\\hat{b}_{c} = \\overline{\\dot{x}} - \\hat{A}\\,\\overline{x_{c}} = \\overline{\\dot{x}} - \\hat{A}\\,(\\bar{x} + c) = \\hat{b} - \\hat{A}\\,c$. Thus, with an intercept, $\\hat{A}$ is unaffected and $\\hat{b}$ shifts to absorb the offset.\n\nCase without intercept and without centering. If $b \\equiv 0$ and we do not center, the least-squares problem is\n$$\n\\min_{A}\\ \\left\\| \\dot{X} - A X \\right\\|_{F}^{2}.\n$$\nIts normal equations are\n$$\n\\dot{X} X^{\\top} = A \\, (X X^{\\top}).\n$$\nAssuming $X X^{\\top}$ invertible, the solution is $\\hat{A} = \\dot{X} X^{\\top} (X X^{\\top})^{-1}$. After adding the constant offset, the predictors become $X_{c} = X + c\\,\\mathbf{1}^{\\top}$, and the normal equations are\n$$\n\\dot{X} X_{c}^{\\top} = A \\, (X_{c} X_{c}^{\\top}).\n$$\nExpand both sides using $\\sum_{t=1}^{T} x(t) = T\\,\\bar{x}$:\n$$\n\\dot{X} X_{c}^{\\top} = \\dot{X} X^{\\top} + \\dot{X}\\,\\mathbf{1}\\,c^{\\top} = \\dot{X} X^{\\top} + T\\,\\overline{\\dot{x}}\\,c^{\\top},\n$$\n$$\nX_{c} X_{c}^{\\top} = X X^{\\top} + X\\,\\mathbf{1}\\,c^{\\top} + c\\,\\mathbf{1}^{\\top} X^{\\top} + c\\,\\mathbf{1}^{\\top}\\mathbf{1}\\,c^{\\top} = X X^{\\top} + T\\,\\bar{x}\\,c^{\\top} + T\\,c\\,\\bar{x}^{\\top} + T\\,c\\,c^{\\top}.\n$$\nUnless both $\\bar{x} = 0$ and $\\overline{\\dot{x}} = 0$, these additional mean-dependent terms change the normal equations and hence $\\hat{A}$. Therefore, without an intercept and without centering, adding a constant offset generally alters $\\hat{A}$.\n\nRole of centering. If we center $X$ and $\\dot{X}$, replacing them by $XH$ and $\\dot{X}H$, then adding any constant offset $c$ leaves $XH$ unchanged because $X_{c}H = (X + c\\,\\mathbf{1}^{\\top})H = XH + c\\,(\\mathbf{1}^{\\top}H) = XH$. Similarly, $\\dot{X}H$ is unaffected by adding a constant to $\\dot{x}(t)$, and in any case $\\dot{x}(t)$ does not change when adding $c$ to $x(t)$. Thus, the centered least-squares estimate of $A$ is invariant to constant offsets even when no intercept is included.\n\nInterpretation for rotational dynamics and jPCA. In jPCA, one constrains $A$ to be skew-symmetric to isolate rotational structure in the reduced state. The skew-symmetric constraint does not eliminate the influence of translations in the predictors if the data are not centered and no intercept is fit; the cross-covariance driving the estimate still depends on means. Centering defines a meaningful origin (often near a latent fixed point or the mean trajectory), so that the fitted $\\hat{A}$ can be interpreted as the best linear approximation (a Jacobian) to the flow about that operating point. This makes the extracted rotations reflect true local dynamics rather than artifacts introduced by an arbitrary choice of origin.\n\nOption-by-option analysis:\n\nA. With intercept, the regression is equivalent to regressing centered responses on centered predictors. Since adding $c$ leaves centered predictors $XH$ unchanged and $\\dot{x}(t)$ is unchanged, $\\hat{A}$ is invariant, and $\\hat{b}$ shifts by $-\\hat{A}c$. Verdict: Correct.\n\nB. Without intercept and without centering, the normal equations acquire mean-dependent terms when $c \\neq 0$, altering $\\hat{A}$. Centering removes these terms and restores invariance. Verdict: Correct.\n\nC. Adding $c$ to $x(t)$ does not change $\\dot{x}(t)$ because $\\dot{c} = 0$. Therefore, the premise that $\\dot{x}(t)$ changes is false. Verdict: Incorrect.\n\nD. A skew-symmetric constraint on $A$ does not in itself neutralize the influence of nonzero means when $b$ is excluded; without centering, mean-dependent terms persist in the normal equations. Centering remains necessary. Verdict: Incorrect.\n\nE. Centering establishes a coherent origin so that $\\hat{A}$ approximates the Jacobian around that point, enabling interpretable rotations about a defined operating point rather than conflating dynamics with translation. Verdict: Correct.",
            "answer": "$$\\boxed{\\text{A, B, E}}$$"
        }
    ]
}