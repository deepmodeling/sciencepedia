{
    "hands_on_practices": [
        {
            "introduction": "To begin, we explore the fundamental properties of the simplest latent variable model: the linear-Gaussian model, also known as Factor Analysis. This exercise demonstrates how the covariance matrix of a neural population's activity is composed of a low-rank component, reflecting shared variability, and a full-rank component from independent noise. Mastering this derivation  is crucial for understanding how these models capture and separate the different sources of neural variability.",
            "id": "4173653",
            "problem": "Consider a linear-Gaussian latent variable model for a population of neurons. Let the observed population activity at time $t$ be $y_t \\in \\mathbb{R}^{n}$ generated from latent variables $x_t \\in \\mathbb{R}^{k}$ by\n$$\ny_t \\;=\\; C x_t \\;+\\; \\epsilon_t,\n$$\nwhere $C \\in \\mathbb{R}^{n \\times k}$ is a fixed loading matrix, the latent state satisfies $x_t \\sim \\mathcal{N}(0,Q)$ with $Q \\in \\mathbb{R}^{k \\times k}$ symmetric positive semidefinite, and the observation noise is $\\epsilon_t \\sim \\mathcal{N}(0,R)$ with $R \\in \\mathbb{R}^{n \\times n}$ symmetric positive semidefinite. Assume $x_t$ and $\\epsilon_t$ are independent, and all processes are wide-sense stationary so that time indices can be dropped when discussing second-order moments.\n\nStarting from core definitions in probability and statistics, including the definition of covariance, independence, and the law of total covariance, derive the population covariance $\\Sigma_y = \\operatorname{Cov}(y_t)$ in closed form in terms of $C$, $Q$, and $R$. Then analyze how the rank of $\\Sigma_y$ depends on the latent dimension $k$ and the observation noise covariance $R$ under the following assumptions: $Q$ is positive definite and $C$ has full column rank $k$. Provide rigorous arguments for the two regimes:\n(i) $R = 0$,\n(ii) $R = \\sigma^{2} I_{n}$ with $\\sigma^{2}  0$.\n\nExpress your final answer as a row matrix whose first entry is the closed-form expression for $\\Sigma_y$ and whose second entry is a single piecewise analytic expression giving $\\operatorname{rank}(\\Sigma_y)$ in cases (i) and (ii). No numerical rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution.\n\nFirst, we derive the population covariance $\\Sigma_y = \\operatorname{Cov}(y_t)$. Since the processes are wide-sense stationary, we can drop the time index $t$. The model is given by $y = C x + \\epsilon$.\nThe problem requests starting from core definitions, including the law of total covariance, which states that for random vectors $y$ and $x$, $\\operatorname{Cov}(y) = \\mathbb{E}[\\operatorname{Cov}(y|x)] + \\operatorname{Cov}(\\mathbb{E}[y|x])$.\n\nLet's compute the two terms on the right-hand side.\n\n1.  The conditional expectation of $y$ given $x$:\n    $$\n    \\mathbb{E}[y | x] = \\mathbb{E}[C x + \\epsilon | x]\n    $$\n    Using the linearity of expectation and treating $x$ as a constant within the conditional expectation:\n    $$\n    \\mathbb{E}[y | x] = C x + \\mathbb{E}[\\epsilon | x]\n    $$\n    Since the latent variables $x$ and the noise $\\epsilon$ are independent, the conditional expectation of $\\epsilon$ given $x$ is equal to its unconditional expectation: $\\mathbb{E}[\\epsilon | x] = \\mathbb{E}[\\epsilon]$. We are given that $\\epsilon \\sim \\mathcal{N}(0, R)$, so $\\mathbb{E}[\\epsilon] = 0$.\n    Thus, the conditional expectation is:\n    $$\n    \\mathbb{E}[y | x] = C x\n    $$\n    Now, we find the covariance of this term. By the properties of covariance of a linear transformation of a random vector, and given $\\operatorname{Cov}(x) = Q$:\n    $$\n    \\operatorname{Cov}(\\mathbb{E}[y|x]) = \\operatorname{Cov}(C x) = C \\operatorname{Cov}(x) C^T = C Q C^T\n    $$\n\n2.  The conditional covariance of $y$ given $x$:\n    $$\n    \\operatorname{Cov}(y|x) = \\operatorname{Cov}(C x + \\epsilon | x)\n    $$\n    Given $x$, the term $C x$ is a constant vector. The covariance of a random vector plus a constant is the covariance of the original random vector.\n    $$\n    \\operatorname{Cov}(y|x) = \\operatorname{Cov}(\\epsilon | x)\n    $$\n    Due to the independence of $x$ and $\\epsilon$, the conditional covariance of $\\epsilon$ is equal to its unconditional covariance: $\\operatorname{Cov}(\\epsilon | x) = \\operatorname{Cov}(\\epsilon)$. We are given that $\\operatorname{Cov}(\\epsilon) = R$.\n    $$\n    \\operatorname{Cov}(y|x) = R\n    $$\n    Now, we find the expectation of this term. Since $R$ is a constant matrix, its expectation is itself:\n    $$\n    \\mathbb{E}[\\operatorname{Cov}(y|x)] = \\mathbb{E}[R] = R\n    $$\n\nSubstituting these two results back into the law of total covariance, we obtain the population covariance $\\Sigma_y$:\n$$\n\\Sigma_y = \\operatorname{Cov}(y) = R + C Q C^T\n$$\nThis is the closed-form expression for $\\Sigma_y$.\n\nNext, we analyze the rank of $\\Sigma_y$ under the assumptions that $Q \\in \\mathbb{R}^{k \\times k}$ is positive definite and $C \\in \\mathbb{R}^{n \\times k}$ has full column rank $k$. A positive definite matrix is invertible and has full rank, so $\\operatorname{rank}(Q) = k$. Full column rank for $C$ means $\\operatorname{rank}(C) = k$. Note that for an $n \\times k$ matrix to have rank $k$, it must be that $n \\geq k$.\n\n(i) Case $R=0$:\nIn this case, the covariance matrix is $\\Sigma_y = C Q C^T$. We want to find its rank.\nSince $Q$ is symmetric and positive definite, there exists a unique symmetric positive definite square root matrix $Q^{1/2}$ such that $Q = Q^{1/2} Q^{1/2}$. Because $Q$ is invertible, $Q^{1/2}$ is also invertible, and $\\operatorname{rank}(Q^{1/2}) = k$.\nWe can write $\\Sigma_y$ as:\n$$\n\\Sigma_y = C Q^{1/2} Q^{1/2} C^T = (C Q^{1/2}) (C Q^{1/2})^T\n$$\nLet $A = C Q^{1/2}$. Then $\\Sigma_y = A A^T$. A fundamental result in linear algebra is that $\\operatorname{rank}(M M^T) = \\operatorname{rank}(M)$ for any real matrix $M$. Therefore, $\\operatorname{rank}(\\Sigma_y) = \\operatorname{rank}(A) = \\operatorname{rank}(C Q^{1/2})$.\nThe matrix $C$ is $n \\times k$ with $\\operatorname{rank}(C)=k$, and $Q^{1/2}$ is a $k \\times k$ invertible matrix with $\\operatorname{rank}(Q^{1/2})=k$. When a matrix is multiplied by an invertible square matrix, its rank is unchanged.\n$$\n\\operatorname{rank}(C Q^{1/2}) = \\operatorname{rank}(C) = k\n$$\nThus, for $R=0$, the rank of the population covariance is:\n$$\n\\operatorname{rank}(\\Sigma_y) = k\n$$\n\n(ii) Case $R = \\sigma^{2} I_{n}$ with $\\sigma^{2}  0$:\nIn this case, the covariance matrix is $\\Sigma_y = C Q C^T + \\sigma^2 I_n$.\nLet's analyze the properties of the two terms in the sum.\nThe first term, $M = C Q C^T$, is a positive semidefinite matrix. This can be shown by considering $v^T M v$ for any vector $v \\in \\mathbb{R}^n$:\n$$\nv^T M v = v^T C Q C^T v = (C^T v)^T Q (C^T v)\n$$\nLet $w = C^T v$. Then $v^T M v = w^T Q w$. Since $Q$ is positive definite, $w^T Q w \\geq 0$ for all $w$, meaning $M$ is positive semidefinite.\nThe second term, $\\sigma^2 I_n$, is a positive definite matrix because $\\sigma^2  0$. For any non-zero vector $v \\in \\mathbb{R}^n$:\n$$\nv^T (\\sigma^2 I_n) v = \\sigma^2 v^T I_n v = \\sigma^2 v^T v = \\sigma^2 \\|v\\|_2^2  0\n$$\nThe sum of a positive semidefinite matrix ($C Q C^T$) and a positive definite matrix ($\\sigma^2 I_n$) is a positive definite matrix. To prove this, consider $v^T \\Sigma_y v$ for any non-zero $v \\in \\mathbb{R}^n$:\n$$\nv^T \\Sigma_y v = v^T(C Q C^T + \\sigma^2 I_n) v = v^T C Q C^T v + \\sigma^2 \\|v\\|_2^2\n$$\nSince $v^T C Q C^T v \\geq 0$ and $\\sigma^2 \\|v\\|_2^2  0$, their sum must be strictly positive:\n$$\nv^T \\Sigma_y v  0 \\quad \\text{for all } v \\neq 0\n$$\nThis is the definition of a positive definite matrix. A positive definite matrix is necessarily invertible and has full rank. Since $\\Sigma_y$ is an $n \\times n$ matrix, its rank is $n$.\nThus, for $R = \\sigma^{2} I_{n}$ with $\\sigma^{2}  0$, the rank is:\n$$\n\\operatorname{rank}(\\Sigma_y) = n\n$$\n\nCombining the results from cases (i) and (ii), we can write a single piecewise expression for the rank. Case (i) corresponds to $R=0$, and case (ii) corresponds to $R$ being a strictly positive multiple of the identity.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nC Q C^T + R  \n\\begin{cases}\nk  \\text{if } R = 0 \\\\\nn  \\text{if } R = \\sigma^2 I_n \\text{ with } \\sigma^2  0\n\\end{cases}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While a standard latent variable model can capture shared variability, its results are often difficult to interpret biologically. This practice explores how to build more interpretable models by incorporating prior knowledge through a Bayesian framework, which mathematically encourages sparsity in the model's loading matrix. By deriving the MAP objective functions for two different sparsity-promoting priors , you will connect the high-level goal of scientific interpretability to the concrete mechanics of model fitting.",
            "id": "4173645",
            "problem": "A laboratory has recorded a neural population of size $N$ over $T$ time bins, and hypothesizes that shared variability is captured by $K$ latent factors. They adopt a linear-Gaussian latent variable model (also known as factor analysis) defined by the following generative assumptions: for each time bin $t \\in \\{1,\\dots,T\\}$, latent factors $x_{t} \\in \\mathbb{R}^{K}$ are drawn independently as $x_{t} \\sim \\mathcal{N}(0, I_{K})$, and the observed spike-count residuals $y_{t} \\in \\mathbb{R}^{N}$ (preprocessed by subtracting a known mean offset $d \\in \\mathbb{R}^{N}$) are generated as $y_{t} \\mid x_{t} \\sim \\mathcal{N}(C x_{t}, R)$, where $C \\in \\mathbb{R}^{N \\times K}$ is the loading matrix and $R \\in \\mathbb{R}^{N \\times N}$ is a known diagonal covariance with strictly positive diagonal entries. In an expectation-maximization procedure, suppose the expectation step has already produced fixed summaries $\\{x_{t}\\}_{t=1}^{T}$ to be treated as given, and the goal in the maximization step is to estimate $C$ by Maximum A Posteriori (MAP) under sparsity-promoting priors.\n\nStarting from Bayes’ rule and the definition of the Gaussian likelihood, and treating $\\{x_{t}\\}_{t=1}^{T}$ and $R$ as fixed, derive the MAP objective for $C$ (the negative log-posterior as a function of $C$) up to additive terms that do not depend on $C$, under each of the following independent priors on the entries $\\{c_{ij}\\}$ of $C$:\n\n1. The Laplace prior with scale $b0$, i.e., $p(c_{ij}) = \\frac{1}{2 b} \\exp\\!\\left(-\\frac{|c_{ij}|}{b}\\right)$.\n2. A spike-and-slab mixture-of-Gaussians prior with mixing weight $\\pi \\in (0,1)$, slab variance $\\sigma_{s}^{2}0$, and spike variance $\\sigma_{0}^{2} \\in (0, \\sigma_{s}^{2})$, i.e., $p(c_{ij}) = (1-\\pi)\\,\\mathcal{N}(c_{ij}; 0, \\sigma_{s}^{2}) + \\pi\\,\\mathcal{N}(c_{ij}; 0, \\sigma_{0}^{2})$.\n\nYour derivation must begin from first principles for the Gaussian likelihood and Bayes’ rule, explicitly showing how the penalty terms arise from the priors. Also explain, in the context of neural population analysis, why imposing sparsity on $C$ enhances interpretability of neuron participation in latent factors.\n\nProvide the two resulting MAP objectives as explicit functions of $C$, $\\{x_{t}\\}_{t=1}^{T}$, $\\{y_{t}\\}_{t=1}^{T}$, $R$, $b$, $\\pi$, $\\sigma_{s}^{2}$, and $\\sigma_{0}^{2}$, each up to additive constants that do not depend on $C$. The final answer must consist of those two objective expressions only, formatted together as a single row matrix. No numerical rounding is required.",
            "solution": "The problem is valid as it presents a standard, well-posed statistical estimation task within the established framework of latent variable modeling for neuroscience data. It is scientifically grounded, objective, and contains all necessary information to derive the requested objective functions.\n\nThe goal is to find the Maximum A Posteriori (MAP) estimate for the loading matrix $C$. The MAP estimate $\\hat{C}_{\\text{MAP}}$ maximizes the posterior probability of $C$ given the observed data $\\{y_t\\}_{t=1}^{T}$ and the estimated latent factors $\\{x_t\\}_{t=1}^{T}$. Using Bayes' rule, the posterior is given by:\n$$\np(C \\mid \\{y_t\\}, \\{x_t\\}) \\propto p(\\{y_t\\} \\mid C, \\{x_t\\}) p(C)\n$$\nwhere $p(\\{y_t\\} \\mid C, \\{x_t\\})$ is the likelihood of the data and $p(C)$ is the prior on the parameters. To simplify notation, let $Y = \\{y_t\\}_{t=1}^{T}$ and $X = \\{x_t\\}_{t=1}^{T}$.\n\nMaximizing the posterior is equivalent to minimizing its negative logarithm. The MAP objective function, which we denote as $J(C)$, is therefore the negative log-posterior, up to additive constants that do not depend on $C$:\n$$\nJ(C) = -\\ln p(C \\mid Y, X) = -\\ln p(Y \\mid C, X) - \\ln p(C) + \\text{constant}\n$$\nThe term $-\\ln p(Y \\mid C, X)$ is the negative log-likelihood, and $-\\ln p(C)$ is the negative log-prior, which acts as a regularization term.\n\nFirst, let's derive the negative log-likelihood term, which is common to both parts of the problem. The observations $y_t$ at different time bins are conditionally independent given $C$ and $\\{x_t\\}$. Therefore, the total likelihood is the product of the likelihoods for each time bin:\n$$\np(Y \\mid C, X) = \\prod_{t=1}^{T} p(y_t \\mid C, x_t)\n$$\nThe model specifies that $y_t \\mid x_t \\sim \\mathcal{N}(C x_t, R)$. The probability density function for a multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$ of dimension $N$ is:\n$$\np(z; \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^{N} \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (z-\\mu)^T \\Sigma^{-1} (z-\\mu)\\right)\n$$\nIn our case, for each time bin $t$, the variable is $y_t$, the mean is $\\mu = C x_t$, and the covariance is $\\Sigma = R$. The log-likelihood for a single time bin is:\n$$\n\\ln p(y_t \\mid C, x_t) = -\\frac{N}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(R)) - \\frac{1}{2}(y_t - C x_t)^T R^{-1} (y_t - C x_t)\n$$\nThe first two terms do not depend on $C$, so they can be absorbed into the additive constant. The negative log-likelihood for all time bins is the sum over $t$:\n$$\n-\\ln p(Y \\mid C, X) = \\frac{1}{2} \\sum_{t=1}^{T} (y_t - C x_t)^T R^{-1} (y_t - C x_t) + \\text{constant}\n$$\nSince $R$ is a diagonal matrix with strictly positive diagonal entries $R_{ii}$, its inverse $R^{-1}$ is also a diagonal matrix with entries $1/R_{ii}$. We can write the quadratic form more explicitly. Let $c_{ij}$ be the entry in the $i$-th row and $j$-th column of $C$, let $y_{it}$ be the $i$-th component of $y_t$, and let $x_{jt}$ be the $j$-th component of $x_t$. The $i$-th component of the vector $C x_t$ is $\\sum_{j=1}^{K} c_{ij} x_{jt}$.\nThe negative log-likelihood becomes:\n$$\n\\mathcal{L}(C) = -\\ln p(Y \\mid C, X) = \\frac{1}{2} \\sum_{t=1}^{T} \\sum_{i=1}^{N} \\frac{(y_{it} - \\sum_{j=1}^{K} c_{ij} x_{jt})^{2}}{R_{ii}} + \\text{constant}\n$$\n\nNow we derive the full objective function $J(C) = \\mathcal{L}(C) - \\ln p(C)$ for each prior.\n\n**1. Laplace Prior**\n\nThe prior assumes that each entry $c_{ij}$ of the matrix $C$ is drawn independently from a Laplace distribution:\n$$\np(c_{ij}) = \\frac{1}{2b} \\exp\\left(-\\frac{|c_{ij}|}{b}\\right)\n$$\nThe prior on the entire matrix $C$ is the product of the priors on its entries:\n$$\np(C) = \\prod_{i=1}^{N} \\prod_{j=1}^{K} p(c_{ij}) = \\prod_{i=1}^{N} \\prod_{j=1}^{K} \\frac{1}{2b} \\exp\\left(-\\frac{|c_{ij}|}{b}\\right)\n$$\nThe log-prior is:\n$$\n\\ln p(C) = \\sum_{i=1}^{N} \\sum_{j=1}^{K} \\ln\\left(\\frac{1}{2b} \\exp\\left(-\\frac{|c_{ij}|}{b}\\right)\\right) = \\sum_{i=1}^{N} \\sum_{j=1}^{K} \\left(\\ln\\left(\\frac{1}{2b}\\right) - \\frac{|c_{ij}|}{b}\\right)\n$$\nThe negative log-prior, dropping the term $\\sum_{i,j}\\ln(2b)$ as it is constant with respect to $C$, is:\n$$\n-\\ln p(C) = \\frac{1}{b} \\sum_{i=1}^{N} \\sum_{j=1}^{K} |c_{ij}| + \\text{constant}\n$$\nThis term is proportional to the $L_1$ norm of the elements of $C$. Combining with the negative log-likelihood, the MAP objective function for the Laplace prior is:\n$$\nJ_1(C) = \\frac{1}{2} \\sum_{t=1}^{T} \\sum_{i=1}^{N} \\frac{1}{R_{ii}} \\left(y_{it} - \\sum_{j=1}^{K} c_{ij} x_{jt}\\right)^2 + \\frac{1}{b} \\sum_{i=1}^{N} \\sum_{j=1}^{K} |c_{ij}|\n$$\n\n**2. Spike-and-Slab Mixture-of-Gaussians Prior**\n\nThe prior for each entry $c_{ij}$ is a mixture of two zero-mean Gaussian distributions:\n$$\np(c_{ij}) = (1-\\pi)\\,\\mathcal{N}(c_{ij}; 0, \\sigma_{s}^{2}) + \\pi\\,\\mathcal{N}(c_{ij}; 0, \\sigma_{0}^{2})\n$$\nwhere the \"spike\" component $\\mathcal{N}(c_{ij}; 0, \\sigma_{0}^{2})$ has a small variance $\\sigma_{0}^{2}$ to model coefficients near zero, and the \"slab\" component $\\mathcal{N}(c_{ij}; 0, \\sigma_{s}^{2})$ has a larger variance $\\sigma_{s}^{2}$ to model non-zero coefficients. Writing out the Gaussian PDFs:\n$$\n\\mathcal{N}(c_{ij}; 0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{c_{ij}^2}{2\\sigma^2}\\right)\n$$\nThe prior for one element is:\n$$\np(c_{ij}) = (1-\\pi) \\frac{1}{\\sqrt{2\\pi\\sigma_{s}^2}} \\exp\\left(-\\frac{c_{ij}^2}{2\\sigma_{s}^2}\\right) + \\pi \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^2}} \\exp\\left(-\\frac{c_{ij}^2}{2\\sigma_{0}^2}\\right)\n$$\nThe log-prior for the entire matrix $C$ is $\\ln p(C) = \\sum_{i=1}^{N} \\sum_{j=1}^{K} \\ln p(c_{ij})$. The negative log-prior is thus:\n$$\n-\\ln p(C) = - \\sum_{i=1}^{N} \\sum_{j=1}^{K} \\ln\\left( (1-\\pi) \\frac{1}{\\sqrt{2\\pi\\sigma_{s}^2}} \\exp\\left(-\\frac{c_{ij}^2}{2\\sigma_{s}^2}\\right) + \\pi \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^2}} \\exp\\left(-\\frac{c_{ij}^2}{2\\sigma_{0}^2}\\right) \\right)\n$$\nWe can factor out $\\frac{1}{\\sqrt{2\\pi}}$ from the term inside the logarithm. The resulting $\\ln(1/\\sqrt{2\\pi})$ can be dropped as an additive constant. The negative log-prior term becomes:\n$$\n-\\ln p(C) = - \\sum_{i=1}^{N} \\sum_{j=1}^{K} \\ln\\left( \\frac{1-\\pi}{\\sigma_s} \\exp\\left(-\\frac{c_{ij}^2}{2\\sigma_{s}^2}\\right) + \\frac{\\pi}{\\sigma_0} \\exp\\left(-\\frac{c_{ij}^2}{2\\sigma_{0}^2}\\right) \\right) + \\text{constant}\n$$\nCombining with the negative log-likelihood, the MAP objective function for the spike-and-slab prior is:\n$$\nJ_2(C) = \\frac{1}{2} \\sum_{t=1}^{T} \\sum_{i=1}^{N} \\frac{1}{R_{ii}} \\left(y_{it} - \\sum_{j=1}^{K} c_{ij} x_{jt}\\right)^2 - \\sum_{i=1}^{N} \\sum_{j=1}^{K} \\ln\\left( \\frac{1-\\pi}{\\sigma_s} \\exp\\left(-\\frac{c_{ij}^2}{2\\sigma_{s}^2}\\right) + \\frac{\\pi}{\\sigma_0} \\exp\\left(-\\frac{c_{ij}^2}{2\\sigma_{0}^2}\\right) \\right)\n$$\n\n**Interpretability of Sparse Loadings**\n\nIn the context of neural population analysis, the loading matrix $C \\in \\mathbb{R}^{N \\times K}$ links the $N$ observed neurons to the $K$ unobserved latent factors. The element $c_{ij}$ quantifies the strength of participation of neuron $i$ in latent factor $j$. A latent factor represents a pattern of co-fluctuation across the neural population, hypothesized to correspond to a specific computational or cognitive process.\n\nIf $C$ is a dense matrix, nearly every neuron participates in every latent factor. This makes it exceedingly difficult to interpret the functional role of any single factor, as its activity reflects a complex combination of inputs from a vast number of neurons. Similarly, the activity of a single neuron is a mixture of influences from all factors.\n\nImposing sparsity on $C$ forces many of the $c_{ij}$ entries to be zero or negligibly small. This leads to a much more interpretable structure:\n1.  **Factor-centric view:** Each latent factor (a column of $C$) is associated with a small, specific subset of neurons (those with non-zero loadings). This allows scientists to interpret the factor as the coordinated activity of an identifiable neural ensemble or \"cell assembly.\" Researchers can then investigate the common properties of this neuronal subset (e.g., their anatomical location, tuning properties, or projection targets) to understand the factor's function.\n2.  **Neuron-centric view:** Each neuron (a row of $C$) participates in only a few, or perhaps just one, latent factor. This suggests that the neuron's activity, beyond its private variability, is modulated by a limited number of shared cognitive or network states, simplifying the model of that neuron's function within the broader circuit.\n\nIn summary, sparsity provides a parsimonious description of the relationship between neurons and latent population dynamics, turning a mathematically abstract decomposition into a scientifically interpretable model of neural circuit organization. The Laplace and spike-and-slab priors are two principled statistical mechanisms for achieving this sparsity.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2} \\sum_{t=1}^{T} \\sum_{i=1}^{N} \\frac{1}{R_{ii}} \\left(y_{it} - \\sum_{j=1}^{K} c_{ij} x_{jt}\\right)^2 + \\frac{1}{b} \\sum_{i=1}^{N} \\sum_{j=1}^{K} |c_{ij}|  \\frac{1}{2} \\sum_{t=1}^{T} \\sum_{i=1}^{N} \\frac{1}{R_{ii}} \\left(y_{it} - \\sum_{j=1}^{K} c_{ij} x_{jt}\\right)^2 - \\sum_{i=1}^{N} \\sum_{j=1}^{K} \\ln\\left( \\frac{1-\\pi}{\\sigma_s} \\exp\\left(-\\frac{c_{ij}^2}{2\\sigma_{s}^2}\\right) + \\frac{\\pi}{\\sigma_0} \\exp\\left(-\\frac{c_{ij}^2}{2\\sigma_{0}^2}\\right) \\right) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "After fitting a model, we often need to justify its complexity. A key question is whether the inclusion of latent variables provides a statistically meaningful improvement over a simpler model where all neurons are independent. This practice introduces the Likelihood Ratio Test, a classic and powerful statistical tool for comparing nested models. By working through this hypothesis testing procedure , you will gain a practical method for model selection and for quantifying the evidence for shared variability in neural data.",
            "id": "4173676",
            "problem": "You recorded activity from $N = 50$ simultaneously monitored neurons across $T = 1000$ repeat trials of a stationary condition, yielding zero-mean trial-to-trial responses $\\{y_t \\in \\mathbb{R}^{50}\\}_{t=1}^T$ and empirical covariance $\\hat{\\Sigma}_y = \\frac{1}{T}\\sum_{t=1}^T y_t y_t^{\\top}$. You wish to test whether a $k = 3$ latent factor model explains a significant fraction of shared noise correlations relative to a null independent model. Assume a multivariate normal generative model for trial-to-trial variability, where the independent (null) model has covariance $\\Sigma = \\mathrm{diag}(\\psi_1,\\dots,\\psi_N)$ and the latent factor (alternative) model has covariance $\\Sigma = L L^{\\top} + \\mathrm{diag}(\\psi_1,\\dots,\\psi_N)$ with $L \\in \\mathbb{R}^{N \\times k}$.\n\nWhich of the following is a valid hypothesis test, with a correct test statistic and reference distribution, to assess whether the $k=3$ latent factor model provides a statistically significant improvement over the independent model?\n\nA. Construct a Likelihood Ratio Test (LRT) under the multivariate normal assumption. Fit maximum likelihood estimates under the null, $\\hat{\\Sigma}_{\\text{diag}} = \\mathrm{diag}(\\hat{\\psi}_1,\\dots,\\hat{\\psi}_N)$, and under the alternative, $\\hat{\\Sigma}_{\\text{FA}} = \\hat{L}\\hat{L}^{\\top} + \\mathrm{diag}(\\hat{\\psi}_1,\\dots,\\hat{\\psi}_N)$ with $k=3$. Using the log-likelihood for the multivariate normal,\n$$\n\\ell(\\Sigma) = -\\frac{T}{2}\\left[\\log|\\Sigma| + \\operatorname{tr}\\left(\\Sigma^{-1}\\hat{\\Sigma}_y\\right)\\right] + \\text{const},\n$$\nform the LRT statistic\n$$\n\\lambda = 2\\left(\\ell\\!\\left(\\hat{\\Sigma}_{\\text{FA}}\\right) - \\ell\\!\\left(\\hat{\\Sigma}_{\\text{diag}}\\right)\\right),\n$$\nand, under regularity conditions and large $T$, compare $\\lambda$ to a chi-square reference distribution with degrees of freedom $\\nu = Nk - \\frac{k(k-1)}{2}$. For $N=50$ and $k=3$, $\\nu = 150 - 3 = 147$.\n\nB. Perform a Principal Component Analysis (PCA) and compare the top $3$ eigenvalues of $\\hat{\\Sigma}_y$ to the Marchenko–Pastur upper edge $\\lambda_{+} = \\sigma^2\\left(1 + \\sqrt{N/T}\\right)^2$. If the top $3$ eigenvalues exceed $\\lambda_{+}$, conclude that $k=3$ latent factors significantly explain shared variability.\n\nC. Compute the proportion of total variance explained by the top $3$ eigenvalues of $\\hat{\\Sigma}_y$, $\\sum_{i=1}^3 \\lambda_i / \\sum_{i=1}^N \\lambda_i$, and reject the independent model if this fraction exceeds $0.5$.\n\nD. Construct a permutation test by permuting the trial indices identically across all neurons, refitting the $k=3$ latent factor model to the permuted data, recomputing the likelihood improvement, and using the permutation distribution to obtain a $p$-value. Reject the independent model at the $\\alpha = 0.05$ level if the original likelihood improvement exceeds the $95\\%$ quantile of the permuted improvements.\n\nSelect the option that correctly specifies a valid test procedure, its test statistic, and the appropriate reference distribution for this problem.",
            "solution": "The user wants to identify a valid statistical hypothesis test to compare a $k=3$ factor model against an independent model for neural population data.\n\n### Step 1: Extract Givens\n-   Number of neurons: $N = 50$\n-   Number of trials: $T = 1000$\n-   Data: Zero-mean trial responses $\\{y_t \\in \\mathbb{R}^{50}\\}_{t=1}^T$\n-   Empirical covariance: $\\hat{\\Sigma}_y = \\frac{1}{T}\\sum_{t=1}^T y_t y_t^{\\top}$\n-   Generative model: Multivariate normal distribution\n-   Null Hypothesis ($H_0$): Independent model. The covariance matrix is $\\Sigma_0 = \\mathrm{diag}(\\psi_1,\\dots,\\psi_N)$.\n-   Alternative Hypothesis ($H_1$): Latent factor model. The covariance matrix is $\\Sigma_1 = L L^{\\top} + \\mathrm{diag}(\\psi_1,\\dots,\\psi_N)$, with $L \\in \\mathbb{R}^{N \\times k}$ and $k=3$.\n-   Objective: Find a valid hypothesis test (test statistic and reference distribution) to assess if the $k=3$ latent factor model provides a statistically significant improvement over the independent model.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientific Grounding:** The problem is firmly grounded in computational neuroscience and statistics. The use of factor analysis (FA) to model shared variability (noise correlations) in neural populations, under the assumption of multivariate normality, is a standard and widely accepted methodology. Comparing this model to a simpler null model of independent neurons is a classic model selection problem.\n-   **Well-Posed:** The problem is well-posed. It presents two specific, nested statistical models and asks for a standard procedure to compare them. A unique and meaningful answer exists within the framework of statistical hypothesis testing.\n-   **Objective:** The problem is stated in precise, objective mathematical language.\n-   **No Flaws Detected:** The problem statement is scientifically sound, complete for the task, and uses clearly defined terms. The data dimensions ($N=50$, $T=1000$) are realistic for modern neurophysiology experiments. There are no contradictions or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with the derivation of the solution and evaluation of the options.\n\n### Derivation and Option-by-Option Analysis\n\nThe core of the problem is to perform a hypothesis test between two nested statistical models. The null model ($H_0$) proposes a diagonal covariance matrix, which is a constrained version of the alternative model's ($H_1$) covariance matrix. Specifically, the null model corresponds to the case where the factor loading matrix $L$ is a zero matrix. For nested models under a specified parametric distribution like the multivariate normal, the Likelihood Ratio Test (LRT) is a standard and asymptotically most powerful test.\n\nThe general procedure for an LRT is as follows:\n1.  Define the null ($H_0$) and alternative ($H_1$) hypotheses. Here, $H_0: \\Sigma = \\mathrm{diag}(\\psi_1, \\dots, \\psi_N)$ and $H_1: \\Sigma = LL^\\top + \\mathrm{diag}(\\psi_1, \\dots, \\psi_N)$ with $L \\in \\mathbb{R}^{N \\times k}$.\n2.  Find the maximum likelihood estimates (MLEs) of the parameters under both models, yielding $\\hat{\\Sigma}_{\\text{diag}}$ and $\\hat{\\Sigma}_{\\text{FA}}$.\n3.  Calculate the maximized log-likelihoods for both models, $\\ell(\\hat{\\Sigma}_{\\text{diag}})$ and $\\ell(\\hat{\\Sigma}_{\\text{FA}})$.\n4.  Form the LRT statistic: $\\lambda = 2 \\left( \\ell(\\hat{\\Sigma}_{\\text{FA}}) - \\ell(\\hat{\\Sigma}_{\\text{diag}}) \\right)$.\n5.  By Wilks' theorem, under $H_0$ and for a large number of samples ($T$), the statistic $\\lambda$ asymptotically follows a chi-square distribution, $\\lambda \\sim \\chi^2(\\nu)$. The degrees of freedom, $\\nu$, is the difference in the number of free parameters between the alternative and null models.\n\nLet's calculate the degrees of freedom $\\nu$.\n-   **Parameters under $H_0$ (Independent Model):** The model is specified by the $N$ private variances, $\\{\\psi_i\\}_{i=1}^N$. So, the number of free parameters is $p_0 = N$.\n-   **Parameters under $H_1$ (Factor Analysis Model):** The model is specified by the $N$ private variances, $\\{\\psi_i\\}_{i=1}^N$, and the $N \\times k$ loading matrix, $L$. This gives an initial count of $N + Nk$ parameters. However, the factor analysis model has a rotational ambiguity: for any $k \\times k$ orthogonal matrix $R$ (where $R R^\\top = I$), the loading matrix $L' = LR$ produces the same covariance structure since $(LR)(LR)^\\top = LRR^\\top L^\\top = LL^\\top$. To ensure identifiability, constraints must be placed on $L$. The number of constraints needed corresponds to the dimensionality of the group of $k \\times k$ orthogonal matrices, which is $\\frac{k(k-1)}{2}$. Therefore, the number of effective free parameters in the FA model is $p_1 = (N + Nk) - \\frac{k(k-1)}{2}$.\n-   **Difference in Degrees of Freedom:** $\\nu = p_1 - p_0 = \\left(N + Nk - \\frac{k(k-1)}{2}\\right) - N = Nk - \\frac{k(k-1)}{2}$.\n\nFor the given values of $N=50$ and $k=3$:\n$\\nu = (50)(3) - \\frac{3(3-1)}{2} = 150 - \\frac{6}{2} = 150 - 3 = 147$.\n\nNow, let's evaluate each option based on this framework.\n\n**A. Construct a Likelihood Ratio Test...**\nThis option describes the LRT procedure precisely as derived above.\n-   It correctly identifies the test as an LRT.\n-   It uses the correct log-likelihood function for the multivariate normal distribution. Note that the MLE for the covariance under the normal model is the empirical covariance, $\\hat{\\Sigma}_y$. When plugging the MLE of the parameters for a structured covariance model ($\\hat{\\Sigma}_{\\text{model}}$) into the likelihood, the term $\\operatorname{tr}(\\hat{\\Sigma}_{\\text{model}}^{-1}\\hat{\\Sigma}_y)$ simplifies under certain conditions, but the general form of the likelihood function to be maximized is correct. The procedure correctly evaluates this function at the MLEs for the null and alternative models.\n-   The LRT statistic is correctly formulated as $\\lambda = 2\\left(\\ell\\!\\left(\\hat{\\Sigma}_{\\text{FA}}\\right) - \\ell\\!\\left(\\hat{\\Sigma}_{\\text{diag}}\\right)\\right)$.\n-   The reference distribution is correctly identified as a chi-square distribution for large $T$.\n-   The calculation of the degrees of freedom, $\\nu = Nk - \\frac{k(k-1)}{2} = 147$, is exactly correct.\n**Verdict: Correct.**\n\n**B. Perform a Principal Component Analysis (PCA)...**\nThis option proposes using Random Matrix Theory (RMT), specifically the Marchenko-Pastur law, to test for significant factors.\n-   The Marchenko-Pastur law describes the bulk distribution of eigenvalues for a sample covariance matrix when the underlying variables are i.i.d. with variance $\\sigma^2$ (i.e., true covariance is $\\sigma^2 I$). The null model in this problem is $\\Sigma_0 = \\mathrm{diag}(\\psi_1,\\dots,\\psi_N)$, where variances can differ, which violates the assumptions of the basic Marchenko-Pastur law. While extensions exist for heterogeneous variances, the option presents the simplest form, which is inapplicable.\n-   This approach is primarily a test for *any* correlation against a highly specific null (i.i.d. noise), not a specific test of a $k=3$ factor structure against a more general independent model.\n-   RMT provides heuristics for model order selection (estimating $k$), but it is not a formal hypothesis test in the same framework as LRT for comparing the specific, pre-defined $H_0$ and $H_1$.\n**Verdict: Incorrect.**\n\n**C. Compute the proportion of total variance explained by the top $3$ eigenvalues...**\nThis option suggests using the proportion of variance explained (PVE) by the top $3$ principal components.\n-   This is a descriptive statistic, not a formal hypothesis test. A hypothesis test requires a null distribution for the test statistic to compute a p-value or compare against a critical value derived from said distribution.\n-   The threshold of $0.5$ is completely arbitrary and has no statistical justification. A meaningful threshold would depend on $N$, $T$, and the specific null model. For example, for large $N$, even under the null model of independent neurons, the top few eigenvalues can by chance explain a non-trivial fraction of variance.\n-   This method conflates PCA with Factor Analysis. While related, they are distinct models. PCA seeks to maximize explained variance, while FA seeks to model the covariance structure.\n**Verdict: Incorrect.**\n\n**D. Construct a permutation test...**\nThis option proposes a permutation test. A valid permutation test must break the statistical dependencies present under the alternative hypothesis while preserving the structure of the data under the null hypothesis.\n-   The null hypothesis is that the activities of the $N$ neurons are independent of each other within any given trial $t$. The correlation structure is across neurons.\n-   The proposed permutation is to \"permute the trial indices identically across all neurons.\" Let the original data matrix be $Y \\in \\mathbb{R}^{N \\times T}$, where column $t$ is $y_t$. A permutation $\\pi$ of the trial indices $\\{1, ..., T\\}$ creates a new data matrix whose columns are a reordering of the columns of $Y$. The set of data vectors $\\{y_t\\}_{t=1}^T$ remains unchanged.\n-   The empirical covariance matrix $\\hat{\\Sigma}_y = \\frac{1}{T} \\sum_{t=1}^T y_t y_t^\\top$ is invariant under such a permutation because the sum is over the same set of vectors, just in a different order.\n-   Consequently, the maximum likelihood estimates and the LRT statistic $\\lambda$ will be identical for the original data and any such permuted dataset. The resulting \"permutation distribution\" would be a single point, making it impossible to calculate a p-value.\n-   A correct permutation scheme for this problem would involve permuting the trial indices *independently for each neuron*. This would preserve the marginal distribution of each neuron's activity (as required by $H_0$) while destroying the cross-neuron correlation structure (which is absent under $H_0$). The proposed procedure is fundamentally flawed.\n**Verdict: Incorrect.**\n\nConclusion: Only option A describes a valid, standard, and correctly specified hypothesis test for the problem at hand.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}