{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of Unitary Event analysis is comparing an observed pattern of neural activity to what would be expected by chance. Before we can claim that a synchronous event is statistically 'surprising', we must first build a rigorous null hypothesis that defines what 'chance' looks like. This first practice guides you through the process of calculating the expected number of coincidences under the assumption of independent firing, starting from the fundamental principles of probability theory . Mastering this calculation is essential for understanding the baseline against which all potential unitary events are measured.",
            "id": "4202846",
            "problem": "Consider a dataset analyzed with Unitary Event (UE) analysis, where spike trains are discretized into non-overlapping bins of width $\\Delta = 5 \\times 10^{-3}$ seconds, and a coincidence is defined when each neuron in a specified subset spikes at least once within the same bin. Assume that within a single bin $t$, the spiking of different neurons is modeled as independent Bernoulli trials with success probabilities $p_i(t)$ that can vary across bins due to nonstationary firing rates. For the subset $S = \\{1, 2\\}$, the estimated per-bin spike probabilities for bins $t = 1, \\dots, 12$ are given by the sequences\n$p_1(1), \\dots, p_1(12) = (0.11, 0.12, 0.10, 0.09, 0.13, 0.15, 0.18, 0.16, 0.14, 0.12, 0.11, 0.10)$\nand\n$p_2(1), \\dots, p_2(12) = (0.08, 0.09, 0.07, 0.06, 0.10, 0.12, 0.15, 0.13, 0.11, 0.09, 0.08, 0.07)$.\nStarting from core principles of probability (indicator random variables and linearity of expectation) and the Bernoulli approximation of per-bin spiking under the Poisson-like small-bin assumption, derive the expected number of coincidences in the window $\\mathcal{W} = \\{1, \\dots, 12\\}$ for the subset $S$ and compute its numerical value. In addition, explain the modeling and mathematical assumptions that justify summation over bins to obtain the expected count from per-bin coincidence probabilities. Express your final numerical answer as a dimensionless count rounded to four significant figures.",
            "solution": "The problem asks for the derivation and computation of the expected number of coincidences in a dataset analyzed with Unitary Event (UE) analysis. I will first validate the problem statement, then proceed with a rigorous derivation based on first principles of probability, explain the underlying assumptions, and finally compute the numerical value.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Analysis Method:** Unitary Event (UE) analysis.\n- **Bin Width:** $\\Delta = 5 \\times 10^{-3}$ seconds.\n- **Coincidence Definition:** For a subset of neurons, each neuron spikes at least once within the same bin.\n- **Neuron Spiking Model:** Within a single bin $t$, the spiking of different neurons is modeled as independent Bernoulli trials with non-stationary success probabilities $p_i(t)$.\n- **Neuron Subset:** $S = \\{1, 2\\}$.\n- **Time Window:** $\\mathcal{W} = \\{1, \\dots, 12\\}$ bins.\n- **Spike Probabilities for Neuron 1:** The sequence of $p_1(t)$ for $t=1, \\dots, 12$ is $(0.11, 0.12, 0.10, 0.09, 0.13, 0.15, 0.18, 0.16, 0.14, 0.12, 0.11, 0.10)$.\n- **Spike Probabilities for Neuron 2:** The sequence of $p_2(t)$ for $t=1, \\dots, 12$ is $(0.08, 0.09, 0.07, 0.06, 0.10, 0.12, 0.15, 0.13, 0.11, 0.09, 0.08, 0.07)$.\n- **Objective:** Derive the expected number of coincidences in $\\mathcal{W}$, compute its numerical value rounded to four significant figures, and explain the justifications for the method.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, employing a standard model in computational neuroscience (Bernoulli approximation for binned spike trains). The mathematical formulation is consistent and self-contained, providing all necessary dataâ€”the sequences of probabilities $p_1(t)$ and $p_2(t)$ have lengths consistent with the specified time window. The terminology is precise and objective. The problem is well-posed, leading to a unique and meaningful solution. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Derivation and Solution\n\nLet us begin from the core principles of probability as requested. The analysis considers spike trains from two neurons, indexed $i \\in \\{1, 2\\}$, over a time window of $T=12$ discrete bins.\n\nLet $X_{i,t}$ be the indicator random variable representing the spiking of neuron $i$ in bin $t$, where $t \\in \\{1, \\dots, 12\\}$. Since the spiking is modeled as a Bernoulli trial with success probability $p_i(t)$, we have:\n$$\nX_{i,t} =\n\\begin{cases}\n1 & \\text{if neuron } i \\text{ spikes in bin } t \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThe probability of a spike is $P(X_{i,t} = 1) = p_i(t)$, and the probability of no spike is $P(X_{i,t} = 0) = 1 - p_i(t)$. The expectation of this indicator variable is $E[X_{i,t}] = 1 \\cdot P(X_{i,t}=1) + 0 \\cdot P(X_{i,t}=0) = p_i(t)$.\n\nA coincidence in bin $t$ is defined as the event where both neuron $1$ and neuron $2$ spike in that bin. Let $C_t$ be the indicator random variable for a coincidence in bin $t$. For a coincidence to occur ($C_t = 1$), we must have both $X_{1,t} = 1$ and $X_{2,t} = 1$. This means the indicator variable for the joint event can be expressed as the product of the individual indicator variables:\n$$C_t = X_{1,t} X_{2,t}$$\nThe expected number of coincidences in a single bin $t$ is the expectation of $C_t$, which is $E[C_t]$. Since $C_t$ is an indicator variable, its expectation is equal to the probability of the event it indicates:\n$$E[C_t] = P(C_t = 1) = P(X_{1,t}=1 \\text{ and } X_{2,t}=1)$$\nThe problem states that the spiking of different neurons within a single bin is modeled as independent. This is a critical assumption. Under this assumption of independence, the joint probability is the product of the marginal probabilities:\n$$P(X_{1,t}=1 \\text{ and } X_{2,t}=1) = P(X_{1,t}=1) \\times P(X_{2,t}=1) = p_1(t) p_2(t)$$\nTherefore, the expected number of coincidences in bin $t$ is:\n$$E[C_t] = p_1(t) p_2(t)$$\nThe total number of coincidences, $N_C$, across the entire window $\\mathcal{W}$ is the sum of the coincidences in each bin:\n$$N_C = \\sum_{t=1}^{12} C_t$$\nTo find the expected value of this sum, $E[N_C]$, we apply the principle of linearity of expectation. This principle states that the expectation of a sum of random variables is equal to the sum of their individual expectations, $E[\\sum_k Y_k] = \\sum_k E[Y_k]$. It is a fundamental property that holds regardless of whether the random variables $Y_k$ are independent. In our case, this means we do not need to assume that coincidences in different bins are independent events.\nApplying linearity of expectation to $N_C$:\n$$E[N_C] = E\\left[\\sum_{t=1}^{12} C_t\\right] = \\sum_{t=1}^{12} E[C_t]$$\nSubstituting our expression for $E[C_t]$, we arrive at the final formula for the expected total number of coincidences:\n$$E[N_C] = \\sum_{t=1}^{12} p_1(t) p_2(t)$$\n\n### Justification of Assumptions\n\nThe problem explicitly asks for the justification for summing per-bin quantities and for a statement of the modeling assumptions.\n\n1.  **Summation over Bins:** The summation of the per-bin expected values, $\\sum_{t} E[C_t]$, to obtain the total expected count $E[N_C]$ is rigorously justified by the **linearity of expectation**. As derived above, this property allows the decomposition of the total expectation into a sum of expectations for each bin, without requiring the assumption of independence of spiking activity *across* bins.\n\n2.  **Modeling Assumptions:**\n    -   **Bernoulli Spike Model:** The representation of spiking in a bin as a Bernoulli trial is an approximation. It is justified if the bin width $\\Delta$ is sufficiently small. This is referred to as the \"Poisson-like small-bin assumption\". If spiking is a Poisson process with an instantaneous rate $\\lambda(t)$, the probability of one or more spikes in a small interval $\\Delta$ is $1 - \\exp(-\\lambda(t)\\Delta)$. For small $\\lambda(t)\\Delta$, a Taylor expansion gives $1 - (1 - \\lambda(t)\\Delta + O((\\lambda(t)\\Delta)^2)) \\approx \\lambda(t)\\Delta$. This gives the Bernoulli probability $p(t) = \\lambda(t)\\Delta$. The probability of more than one spike in the bin is negligible under this assumption.\n    -   **Independence of Neurons (within a bin):** The calculation $E[C_t] = p_1(t) p_2(t)$ relies critically on the assumption that the spiking of neuron 1 and neuron 2 are statistically independent events *within the same bin*. In the context of UE analysis, this assumption forms the basis of the null hypothesis ($H_0$) that coincidences occur at a rate predicted by chance. The observed number of coincidences is then compared to this expected number to test for excess correlation (a \"unitary event\").\n\n### Numerical Computation\n\nWe now compute the value of $E[N_C]$ using the provided data.\nThe sequences are:\n$P_1 = (0.11, 0.12, 0.10, 0.09, 0.13, 0.15, 0.18, 0.16, 0.14, 0.12, 0.11, 0.10)$\n$P_2 = (0.08, 0.09, 0.07, 0.06, 0.10, 0.12, 0.15, 0.13, 0.11, 0.09, 0.08, 0.07)$\n\nWe calculate the product $p_1(t) p_2(t)$ for each bin $t=1, \\dots, 12$:\n- $t=1$: $0.11 \\times 0.08 = 0.0088$\n- $t=2$: $0.12 \\times 0.09 = 0.0108$\n- $t=3$: $0.10 \\times 0.07 = 0.0070$\n- $t=4$: $0.09 \\times 0.06 = 0.0054$\n- $t=5$: $0.13 \\times 0.10 = 0.0130$\n- $t=6$: $0.15 \\times 0.12 = 0.0180$\n- $t=7$: $0.18 \\times 0.15 = 0.0270$\n- $t=8$: $0.16 \\times 0.13 = 0.0208$\n- $t=9$: $0.14 \\times 0.11 = 0.0154$\n- $t=10$: $0.12 \\times 0.09 = 0.0108$\n- $t=11$: $0.11 \\times 0.08 = 0.0088$\n- $t=12$: $0.10 \\times 0.07 = 0.0070$\n\nSumming these values:\n$$E[N_C] = 0.0088 + 0.0108 + 0.0070 + 0.0054 + 0.0130 + 0.0180 + 0.0270 + 0.0208 + 0.0154 + 0.0108 + 0.0088 + 0.0070$$\n$$E[N_C] = 0.1528$$\nThe problem requires the answer to be rounded to four significant figures. The calculated value $0.1528$ already has four significant figures.",
            "answer": "$$\\boxed{0.1528}$$"
        },
        {
            "introduction": "With the per-bin chance probabilities established, the next step is to quantify the probability of observing a certain number of events, or more, under the null model; this probability is the p-value. This exercise delves into the statistical heart of UEA, challenging you to compute this p-value using two different methods: an exact calculation via the Poisson binomial distribution and a common simplification using the Poisson approximation. By implementing both , you will gain a deep appreciation for the trade-offs between computational feasibility and statistical precision in real-world data analysis.",
            "id": "4202842",
            "problem": "Consider a window-based Unitary Event Analysis (UEA), where one tests for excess synchronous spike coincidences beyond chance under a null hypothesis of conditional independence across time bins. Under the null model, per-bin synchronous coincidence indicators are modeled as independent Bernoulli random variables with heterogeneous success probabilities. Let the window contain $T$ discrete bins indexed by $t \\in \\{1, 2, \\dots, T\\}$, and let $q(t) \\in [0,1]$ denote the chance probability of a coincidence in bin $t$ under independence. Define the random variable $X = \\sum_{t=1}^{T} B_t$, where $B_t \\sim \\text{Bernoulli}(q(t))$ are independent across $t$. The distribution of $X$ is the Poisson binomial distribution. Suppose an observed coincidence count $k_{\\text{obs}}$ is measured in the window. The statistical question is to compute the tail probability (a one-sided p-value) of observing at least $k_{\\text{obs}}$ coincidences under the null:\n$$\np_{\\text{PB}} = \\mathbb{P}\\big(X \\ge k_{\\text{obs}}\\big).\n$$\nA common approximation used in neuroscience data analysis is to replace the Poisson binomial distribution by a Poisson distribution with rate parameter $\\lambda = \\sum_{t=1}^{T} q(t)$, yielding the approximate tail\n$$\np_{\\text{Pois}} = \\mathbb{P}\\big(Y \\ge k_{\\text{obs}}\\big), \\quad Y \\sim \\text{Poisson}(\\lambda).\n$$\nStarting only from the fundamental probability laws for independent Bernoulli trials and the definition of the Poisson distribution, derive an algorithm that computes $p_{\\text{PB}}$ exactly via a dynamic programming method based on successive convolution of per-bin two-point distributions. Then implement both the exact tail $p_{\\text{PB}}$ and the Poisson approximation $p_{\\text{Pois}}$ for a given set $\\{q(t)\\}$ and $k_{\\text{obs}}$, and compare them quantitatively.\n\nYour program must:\n- Compute the exact Poisson binomial tail $p_{\\text{PB}}$ using dynamic programming (successive convolution with per-bin distributions).\n- Compute the Poisson approximation tail $p_{\\text{Pois}}$ using the rate $\\lambda = \\sum_{t=1}^{T} q(t)$.\n- For each test case, output a list of three decimal floats $[p_{\\text{PB}}, p_{\\text{Pois}}, \\lvert p_{\\text{PB}} - p_{\\text{Pois}} \\rvert]$, each rounded to exactly $12$ decimal places.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets (for example, $[ [\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot], \\dots ]$).\n\nUse the following test suite, which includes a general case, boundary conditions, and skewed-probability scenarios. In each case, the sample window is the length of the specified list $\\{q(t)\\}$, and the observed count is $k_{\\text{obs}}$.\n\n- Test case $1$ (general, heterogeneous small probabilities):\n  - $\\{q(t)\\} = [\\, 0.02, 0.03, 0.025, 0.015, 0.04, 0.03, 0.02, 0.05, 0.01, 0.035, 0.025, 0.02, 0.03, 0.015, 0.04, 0.025, 0.02, 0.03, 0.02, 0.05, 0.015, 0.02, 0.03, 0.025, 0.02, 0.035, 0.025, 0.02, 0.015, 0.04, 0.03, 0.02, 0.015, 0.05, 0.02, 0.025, 0.03, 0.02, 0.035, 0.015 \\,]$\n  - $k_{\\text{obs}} = 4$\n- Test case $2$ (uniform small probabilities, longer window):\n  - $\\{q(t)\\} = [\\, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02 \\,]$\n  - $k_{\\text{obs}} = 3$\n- Test case $3$ (skewed with a few large probabilities):\n  - $\\{q(t)\\} = [\\, 0.30, 0.25, 0.20, 0.05, 0.01, 0.02, 0.01, 0.05, 0.02, 0.01, 0.03, 0.01, 0.02, 0.01, 0.01 \\,]$\n  - $k_{\\text{obs}} = 5$\n- Test case $4$ (boundary: zero threshold):\n  - $\\{q(t)\\} = [\\, 0.05, 0.02, 0.03, 0.04, 0.01, 0.02, 0.05, 0.03, 0.02, 0.01 \\,]$\n  - $k_{\\text{obs}} = 0$\n- Test case $5$ (boundary: full successes threshold):\n  - $\\{q(t)\\} = [\\, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10 \\,]$\n  - $k_{\\text{obs}} = 10$\n\nAngle units are not applicable. No physical units are required. Your program must format its single-line output as a comma-separated list of per-test-case lists, each inner list containing three decimal floats with exactly $12$ digits after the decimal point.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information to derive and implement a solution.\n\nThe core of the problem is to compute the tail probability of a random variable $X$, which is the sum of $T$ independent but not identically distributed Bernoulli random variables.\nLet $X = \\sum_{t=1}^{T} B_t$, where $B_t \\sim \\text{Bernoulli}(q(t))$ are independent for $t \\in \\{1, 2, \\dots, T\\}$. The distribution of $X$ is known as the Poisson binomial distribution. We are tasked with computing the exact tail probability $p_{\\text{PB}} = \\mathbb{P}(X \\ge k_{\\text{obs}})$ and comparing it to a Poisson approximation, $p_{\\text{Pois}}$.\n\nFirst, we derive an algorithm to compute the exact probability mass function (PMF) of $X$, from which $p_{\\text{PB}}$ can be obtained. The algorithm is based on dynamic programming, leveraging the principle of successive convolution.\nLet $X_n = \\sum_{t=1}^{n} B_t$ be the sum of the first $n$ Bernoulli variables. The PMF of $X_n$ can be computed recursively. The base case is for $n=1$, where $X_1 = B_1$. The PMF of $X_1$ is given by:\n$$ \\mathbb{P}(X_1=0) = 1 - q(1) $$\n$$ \\mathbb{P}(X_1=1) = q(1) $$\nFor the recursive step, we express $X_n$ in terms of $X_{n-1}$ and $B_n$:\n$$ X_n = X_{n-1} + B_n $$\nSince $B_n$ is independent of $X_{n-1}$ (which is a sum of $B_1, \\dots, B_{n-1}$), the PMF of $X_n$ is the convolution of the PMFs of $X_{n-1}$ and $B_n$. Let $P_n(k) = \\mathbb{P}(X_n=k)$. The convolution formula is:\n$$ P_n(k) = \\sum_{j=0}^{k} \\mathbb{P}(X_{n-1}=j) \\cdot \\mathbb{P}(B_n=k-j) $$\nSince $B_n$ can only take values $0$ or $1$, the sum simplifies to two terms:\n$$ P_n(k) = \\mathbb{P}(X_{n-1}=k) \\cdot \\mathbb{P}(B_n=0) + \\mathbb{P}(X_{n-1}=k-1) \\cdot \\mathbb{P}(B_n=1) $$\nSubstituting the probabilities for the Bernoulli trial $B_n$, we get the recurrence relation:\n$$ P_n(k) = P_{n-1}(k) \\cdot (1 - q(n)) + P_{n-1}(k-1) \\cdot q(n) $$\nwith the understanding that $P_{n-1}(k)=0$ if $k < 0$ or $k > n-1$.\n\nThis recurrence forms the basis of a dynamic programming algorithm. We can represent the PMF of $X_n$ as an array of probabilities, say `pmf_n`. We start with the PMF of $X_0$, a degenerate random variable equal to $0$ with probability $1$. This is represented by an array `[1.0]`. We then iterate from $n=1$ to $T$, at each step $n$ using `pmf_{n-1}` and $q(n)$ to compute `pmf_n` according to the recurrence.\nThe algorithm proceeds as follows:\n1. Initialize a probability vector `pmf` of size $T+1$ to represent the PMF of the sum. For $X_0$, this is `[1.0, 0.0, ..., 0.0]`.\n2. For each trial $t$ from $1$ to $T$, with probability $q(t)$:\n   Update the `pmf` vector. Let the vector before this step be $P_{t-1}$. The new vector $P_t$ is computed by iterating backwards from $k=t$ down to $1$:\n   $P_t(k) \\leftarrow P_{t-1}(k) \\cdot (1 - q(t)) + P_{t-1}(k-1) \\cdot q(t)$. The $k=0$ case is special: $P_t(0) \\leftarrow P_{t-1}(0) \\cdot (1 - q(t))$.\n   The backward iteration is crucial for an in-place update of the `pmf` array.\n3. After iterating through all $T$ trials, the `pmf` vector holds the PMF of $X = X_T$.\nThe desired tail probability is then the sum of the probabilities for all outcomes greater than or equal to $k_{\\text{obs}}$:\n$$ p_{\\text{PB}} = \\mathbb{P}(X \\ge k_{\\text{obs}}) = \\sum_{k=k_{\\text{obs}}}^{T} P_T(k) $$\n\nNext, we address the Poisson approximation. The Poisson binomial distribution is approximated by a Poisson distribution with a rate parameter $\\lambda$ equal to the sum of the individual Bernoulli probabilities:\n$$ \\lambda = \\sum_{t=1}^{T} q(t) $$\nLet $Y \\sim \\text{Poisson}(\\lambda)$. The PMF of $Y$ is:\n$$ \\mathbb{P}(Y=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\quad \\text{for } k=0, 1, 2, \\dots $$\nThe approximate tail probability, $p_{\\text{Pois}}$, is $\\mathbb{P}(Y \\ge k_{\\text{obs}})$. It is more numerically stable to compute this via the complement of the cumulative distribution function (CDF):\n$$ p_{\\text{Pois}} = \\mathbb{P}(Y \\ge k_{\\text{obs}}) = 1 - \\mathbb{P}(Y < k_{\\text{obs}}) = 1 - \\sum_{k=0}^{k_{\\text{obs}}-1} \\mathbb{P}(Y=k) $$\nFor $k_{\\text{obs}}=0$, this sum is empty and has value $0$, so $p_{\\text{Pois}}=1$. For $k_{\\text{obs}}>0$, we can compute the sum iteratively. Let $S_{j} = \\sum_{k=0}^{j} \\mathbb{P}(Y=k)$. The terms of the sum can be computed recursively:\n$$ \\mathbb{P}(Y=k) = \\mathbb{P}(Y=k-1) \\cdot \\frac{\\lambda}{k} $$\nstarting with $\\mathbb{P}(Y=0)=e^{-\\lambda}$. We sum these terms from $k=0$ to $k_{\\text{obs}}-1$ to find the CDF value, and subtract from $1$ to get the tail probability $p_{\\text{Pois}}$.\n\nFinally, the quantitative comparison is the absolute difference $|p_{\\text{PB}} - p_{\\text{Pois}}|$. The implementation will compute these three values for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_poisson_binomial_tail(q_list, k_obs):\n    \"\"\"\n    Computes the exact tail probability P(X >= k_obs) for a Poisson Binomial distribution.\n    The distribution is the sum of independent Bernoulli trials with probabilities q_list.\n    This is achieved using a dynamic programming approach based on successive convolutions.\n    \"\"\"\n    T = len(q_list)\n\n    if k_obs > T:\n        return 0.0\n    if k_obs < 0:\n        k_obs = 0\n    \n    # pmf array stores P(X_n = k) at index k.\n    # Initialize for a sum of 0 trials, where P(sum=0) = 1.\n    pmf = np.zeros(T + 1, dtype=np.float64)\n    pmf[0] = 1.0\n    \n    # Iterate through each Bernoulli trial\n    # num_trials tracks the number of trials processed so far (from 1 to T)\n    num_trials = 0\n    for q_t in q_list:\n        num_trials += 1\n        # Update the pmf from a sum of num_trials-1 Bernoullis to num_trials.\n        # The recurrence is: P_n(k) = P_{n-1}(k)*(1-q_t) + P_{n-1}(k-1)*q_t\n        # We iterate backwards to use the values from the previous step (pmf_{n-1})\n        # before they are overwritten.\n        # The loop range covers a max possible sum of num_trials.\n        for k in range(num_trials, 0, -1):\n            pmf[k] = pmf[k] * (1.0 - q_t) + pmf[k - 1] * q_t\n        \n        # Update the k=0 case separately\n        pmf[0] = pmf[0] * (1.0 - q_t)\n\n    # The tail probability is the sum of probabilities from k_obs to T.\n    p_pb_tail = np.sum(pmf[k_obs:])\n    return p_pb_tail\n\ndef compute_poisson_tail(q_list, k_obs):\n    \"\"\"\n    Computes the tail probability P(Y >= k_obs) for a Poisson distribution\n    approximating the Poisson Binomial distribution.\n    The rate lambda is the sum of the Bernoulli probabilities.\n    \"\"\"\n    if k_obs <= 0:\n        return 1.0\n\n    lam = np.sum(q_list)\n\n    # If lambda is 0, the only possible outcome is 0.\n    # P(Y=0) = 1, P(Y>0) = 0.\n    # The tail P(Y >= k_obs) is 0 for k_obs > 0.\n    if lam == 0.0:\n        return 0.0\n\n    # Compute CDF P(Y < k_obs) = sum_{k=0}^{k_obs-1} P(Y=k)\n    # P(Y=k) = exp(-lam) * lam^k / k!\n    # We compute terms iteratively: term_k = term_{k-1} * lam / k\n    # Start with k=0 term\n    term = np.exp(-lam)\n    cdf_sum = term\n    \n    for k in range(1, k_obs):\n        term = term * lam / k\n        cdf_sum += term\n        \n    p_pois_tail = 1.0 - cdf_sum\n    # Due to floating point errors, result can be slightly negative. Clip at 0.\n    return max(0.0, p_pois_tail)\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (general, heterogeneous small probabilities)\n        ([0.02, 0.03, 0.025, 0.015, 0.04, 0.03, 0.02, 0.05, 0.01, 0.035, 0.025, 0.02, 0.03, 0.015, 0.04, 0.025, 0.02, 0.03, 0.02, 0.05, 0.015, 0.02, 0.03, 0.025, 0.02, 0.035, 0.025, 0.02, 0.015, 0.04, 0.03, 0.02, 0.015, 0.05, 0.02, 0.025, 0.03, 0.02, 0.035, 0.015], 4),\n        # Test case 2 (uniform small probabilities, longer window)\n        ([0.02] * 50, 3),\n        # Test case 3 (skewed with a few large probabilities)\n        ([0.30, 0.25, 0.20, 0.05, 0.01, 0.02, 0.01, 0.05, 0.02, 0.01, 0.03, 0.01, 0.02, 0.01, 0.01], 5),\n        # Test case 4 (boundary: zero threshold)\n        ([0.05, 0.02, 0.03, 0.04, 0.01, 0.02, 0.05, 0.03, 0.02, 0.01], 0),\n        # Test case 5 (boundary: full successes threshold)\n        ([0.10] * 10, 10),\n    ]\n\n    all_results_formatted = []\n    for q_list, k_obs in test_cases:\n        \n        p_pb = compute_poisson_binomial_tail(q_list, k_obs)\n        p_pois = compute_poisson_tail(q_list, k_obs)\n        diff = abs(p_pb - p_pois)\n        \n        case_results = [p_pb, p_pois, diff]\n        formatted_case_results = [f\"{val:.12f}\" for val in case_results]\n        all_results_formatted.append(f\"[{','.join(formatted_case_results)}]\")\n\n    print(f\"[[{','.join(all_results_formatted)}]]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A small p-value indicates a rare event, but how do we translate this into a more intuitive measure of significance? In UEA, the concept of 'surprise' provides a logarithmic scale to quantify how unlikely an observation is. This final practice focuses on this interpretive step, using the common Poisson approximation to link an observed count of coincidences to its surprise value. You will determine the critical threshold at which an observation is deemed significant , solidifying your understanding of how statistical evidence is used to declare the discovery of a unitary event.",
            "id": "4202879",
            "problem": "Consider a Unitary Event (UE) analysis of coincident spikes in two simultaneously recorded neurons. Assume the null hypothesis that coincidences arise from independent Poisson processes aggregated over many time bins so that the number of coincidences in the analysis window is modeled as a single Poisson count with mean (expected count) $E$. The one-sided null probability of observing at least $O$ coincidences, given $E$, is the Poisson upper-tail probability $p(O;E)$ defined as $p(O;E) = \\sum_{k=O}^{\\infty} \\exp(-E) \\frac{E^{k}}{k!}$. Define the surprise index $S$ by $S = -\\log_{10}(p)$, so that $S \\ge 2$ if and only if $p \\le 10^{-2}$.\n\nSuppose the expected number of coincidences in the analysis window is $E = 6.0$. For observed coincidence counts $O \\in \\{10, 11, 12, 13, 14\\}$:\n- Compute the surprise index $S(O;E)$ under the Poisson null model, starting from the fundamental definition of the Poisson distribution and its tail probability.\n- Determine the smallest integer threshold $O^{\\ast}$ such that $S(O^{\\ast};6.0) \\ge 2$.\n\nExpress all intermediate mathematical quantities in exact symbolic form where possible, and perform numerical evaluation only as needed to identify $O^{\\ast}$. The final answer must be the single integer $O^{\\ast}$. No units are required.",
            "solution": "The problem is valid as it is scientifically grounded in statistical analysis methods common in neuroscience, is well-posed, objective, and internally consistent.\n\nThe objective is to find the smallest integer number of observed coincidences, $O^{\\ast}$, for which the surprise index $S(O^{\\ast}; E) \\ge 2$, given an expected count of $E = 6.0$.\n\nThe surprise index $S$ is defined as $S = -\\log_{10}(p)$, where $p$ is the one-sided null probability. The condition $S \\ge 2$ is equivalent to:\n$$ -\\log_{10}(p) \\ge 2 $$\nMultiplying by $-1$ and reversing the inequality sign gives:\n$$ \\log_{10}(p) \\le -2 $$\nApplying the monotonically increasing function $10^x$ to both sides yields:\n$$ p \\le 10^{-2} \\quad \\text{or} \\quad p \\le 0.01 $$\nThe null probability $p(O;E)$ for observing at least $O$ events, given an expected count $E$, is defined by the Poisson upper-tail probability:\n$$ p(O;E) = \\sum_{k=O}^{\\infty} \\exp(-E) \\frac{E^{k}}{k!} $$\nThus, we need to find the smallest integer $O$ for which $p(O; 6.0) \\le 0.01$.\n\nComputing the infinite sum directly is impractical. It is more convenient to use the complementary probability, which involves a finite sum. The sum of all Poisson probabilities is $1$:\n$$ \\sum_{k=0}^{\\infty} \\exp(-E) \\frac{E^{k}}{k!} = 1 $$\nTherefore, the tail probability can be expressed as:\n$$ p(O;E) = 1 - \\sum_{k=0}^{O-1} \\exp(-E) \\frac{E^{k}}{k!} $$\nThe term $\\sum_{k=0}^{O-1} \\exp(-E) \\frac{E^{k}}{k!}$ is the cumulative distribution function (CDF) of the Poisson distribution evaluated at $O-1$, which we can denote as $F(O-1; E)$.\nThe condition becomes:\n$$ 1 - F(O-1; E) \\le 0.01 $$\n$$ F(O-1; E) \\ge 0.99 $$\nWe are given $E=6.0$ and need to test values of $O$ from the set $\\{10, 11, 12, 13, 14\\}$ to find the smallest integer $O^{\\ast}$ that satisfies this condition. Let's calculate the CDF, $F(m; 6) = \\exp(-6) \\sum_{k=0}^{m} \\frac{6^k}{k!}$, for $m = O-1$.\n\nFor $O=10$, we evaluate $F(9; 6)$:\n$$ F(9; 6) = \\exp(-6) \\sum_{k=0}^{9} \\frac{6^k}{k!} \\approx 0.91608 $$\nThis gives $p(10; 6) = 1 - F(9; 6) \\approx 1 - 0.91608 = 0.08392$.\nThe surprise index is $S(10; 6) = -\\log_{10}(0.08392) \\approx 1.076$. Since $1.076 < 2$, $O=10$ is not the threshold.\n\nFor $O=11$, we evaluate $F(10; 6)$:\n$$ F(10; 6) = \\exp(-6) \\sum_{k=0}^{10} \\frac{6^k}{k!} \\approx 0.95738 $$\nThis gives $p(11; 6) = 1 - F(10; 6) \\approx 1 - 0.95738 = 0.04262$.\nThe surprise index is $S(11; 6) = -\\log_{10}(0.04262) \\approx 1.370$. Since $1.370 < 2$, $O=11$ is not the threshold.\n\nFor $O=12$, we evaluate $F(11; 6)$:\n$$ F(11; 6) = \\exp(-6) \\sum_{k=0}^{11} \\frac{6^k}{k!} \\approx 0.97991 $$\nThis gives $p(12; 6) = 1 - F(11; 6) \\approx 1 - 0.97991 = 0.02009$.\nThe surprise index is $S(12; 6) = -\\log_{10}(0.02009) \\approx 1.697$. Since $1.697 < 2$, $O=12$ is not the threshold.\n\nFor $O=13$, we evaluate $F(12; 6)$:\n$$ F(12; 6) = \\exp(-6) \\sum_{k=0}^{12} \\frac{6^k}{k!} \\approx 0.99118 $$\nHere, $F(12; 6) \\approx 0.99118 \\ge 0.99$. This is the first value of $O-1$ that satisfies the condition.\nLet's calculate the corresponding $p$-value and surprise index.\nThis gives $p(13; 6) = 1 - F(12; 6) \\approx 1 - 0.99118 = 0.00882$.\nThe surprise index is $S(13; 6) = -\\log_{10}(0.00882) \\approx 2.055$. Since $2.055 \\ge 2$, $O=13$ meets the significance criterion.\n\nSince $S(12; 6) < 2$ and $S(13; 6) \\ge 2$, and the surprise index $S(O;E)$ is a monotonically increasing function of $O$ for a fixed $E$, the smallest integer threshold is $O^{\\ast} = 13$.\n\nFor completeness, we can also compute for $O=14$:\nFor $O=14$, we evaluate $F(13; 6)$:\n$$ F(13; 6) = \\exp(-6) \\sum_{k=0}^{13} \\frac{6^k}{k!} \\approx 0.99636 $$\nThis gives $p(14; 6) = 1 - F(13; 6) \\approx 1 - 0.99636 = 0.00364$.\nThe surprise index is $S(14; 6) = -\\log_{10}(0.00364) \\approx 2.439$. This also satisfies $S \\ge 2$, but $13$ is the smallest integer for which this is true.\n\nThe smallest integer threshold is therefore $O^{\\ast}=13$.",
            "answer": "$$\n\\boxed{13}\n$$"
        }
    ]
}