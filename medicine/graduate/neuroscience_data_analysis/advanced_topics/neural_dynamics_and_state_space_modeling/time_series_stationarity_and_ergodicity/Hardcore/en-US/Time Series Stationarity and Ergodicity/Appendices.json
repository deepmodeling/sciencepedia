{
    "hands_on_practices": [
        {
            "introduction": "The autocovariance function is a cornerstone of time series analysis, revealing the temporal dependence structure of a process. Its standard estimators, however, are derived under the assumption of wide-sense stationarity. This practice problem  challenges you to explore the fragility of this assumption by deriving, from first principles, the analytical bias introduced into the autocovariance estimate by a simple linear trend, a common form of non-stationarity in neurophysiological recordings.",
            "id": "4200519",
            "problem": "A research group is analyzing a single-trial Local Field Potential (LFP) time series in a sensory cortex recording, sampled at uniform intervals and indexed as $X_{1}, X_{2}, \\dots, X_{T}$. They are interested in estimating the temporal dependence structure using the sample autocovariance estimator at lag $\\tau$, defined via the sample mean. Let $\\bar{X} = \\frac{1}{T} \\sum_{t=1}^{T} X_{t}$ and consider lags $\\tau \\in \\{0, 1, \\dots, T-1\\}$. The estimator is\n$$\n\\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} \\left( X_{t} - \\bar{X} \\right) \\left( X_{t+\\tau} - \\bar{X} \\right).\n$$\nPart A (stationarity): Assume the data are generated by a wide-sense stationary (WSS) process $\\{Y_{t}\\}$ with mean $0$ and autocovariance function $\\gamma_{Y}(\\tau) = \\mathbb{E}[Y_{t} Y_{t+\\tau}]$ such that the process is ergodic in the mean and autocovariance. Express $\\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right]$ in terms of $T$, $\\tau$, and $\\gamma_{Y}(\\cdot)$, and justify the finite-sample bias of $\\hat{\\gamma}(\\tau)$ relative to $\\gamma_{Y}(\\tau)$ under WSS. Explain, using first principles and the definition of ergodicity, why the estimator is consistent as $T \\to \\infty$.\n\nPart B (linear trend violation): Now suppose the LFP is contaminated by a deterministic linear trend and an offset,\n$$\nX_{t} = \\mu + \\beta t + Y_{t},\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$ are fixed, and $\\{Y_{t}\\}$ is the same WSS, mean $0$ process with autocovariance $\\gamma_{Y}(\\tau)$. Derive, from first principles, the exact closed-form expression for the additive bias in $\\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right]$ that arises purely due to the linear trend (i.e., the component of $\\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right]$ contributed by $\\mu + \\beta t$ after sample-mean centering). Define the bias due to the trend as\n$$\n\\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right] - \\mathbb{E}_{Y}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right],\n$$\nwhere $\\hat{\\gamma}_{Y}(\\tau)$ denotes the same estimator applied to the stationary component $Y_{t}$ alone. Provide your final answer as a single, closed-form analytical expression in terms of $\\beta$, $T$, and $\\tau$. No numerical approximation is required.",
            "solution": "The problem statement has been validated and is deemed a well-posed, scientifically grounded problem in time series analysis. It contains sufficient information and is internally consistent.\n\nThe problem is divided into two parts. Part A addresses the properties of the sample autocovariance estimator for a wide-sense stationary (WSS) process. Part B investigates the bias introduced into this estimator by a deterministic linear trend, a common violation of stationarity.\n\n### Part A: Stationarity, Bias, and Consistency\n\nFor this part, the time series is $X_{t} = Y_{t}$, where $\\{Y_{t}\\}$ is a WSS process with $\\mathbb{E}[Y_{t}] = 0$ and autocovariance function $\\gamma_{Y}(\\tau) = \\mathbb{E}[Y_{t}Y_{t+\\tau}]$. The sample autocovariance estimator is given by $\\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} (X_{t} - \\bar{X})(X_{t+\\tau} - \\bar{X})$. Since $X_{t}=Y_{t}$, we can write this as $\\hat{\\gamma}_{Y}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} (Y_{t} - \\bar{Y})(Y_{t+\\tau} - \\bar{Y})$, with $\\bar{Y} = \\frac{1}{T}\\sum_{t=1}^{T} Y_{t}$.\n\n**1. Expectation of the Estimator**\n\nTo find the expected value, we expand the terms and use the linearity of the expectation operator.\n$$ \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} \\mathbb{E}\\left[ (Y_{t} - \\bar{Y})(Y_{t+\\tau} - \\bar{Y}) \\right] $$\n$$ \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} \\mathbb{E}\\left[ Y_{t}Y_{t+\\tau} - Y_{t}\\bar{Y} - Y_{t+\\tau}\\bar{Y} + \\bar{Y}^2 \\right] $$\n$$ \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} \\left( \\mathbb{E}[Y_{t}Y_{t+\\tau}] - \\mathbb{E}[Y_{t}\\bar{Y}] - \\mathbb{E}[Y_{t+\\tau}\\bar{Y}] + \\mathbb{E}[\\bar{Y}^2] \\right) $$\nLet's evaluate each expectation term:\n- $\\mathbb{E}[Y_{t}Y_{t+\\tau}] = \\gamma_{Y}(\\tau)$ by definition, as the process is WSS.\n- $\\mathbb{E}[Y_{t}\\bar{Y}] = \\mathbb{E}\\left[Y_{t} \\frac{1}{T}\\sum_{s=1}^{T}Y_{s}\\right] = \\frac{1}{T}\\sum_{s=1}^{T}\\mathbb{E}[Y_{t}Y_{s}] = \\frac{1}{T}\\sum_{s=1}^{T}\\gamma_{Y}(t-s)$.\n- Similarly, $\\mathbb{E}[Y_{t+\\tau}\\bar{Y}] = \\frac{1}{T}\\sum_{s=1}^{T}\\gamma_{Y}(t+\\tau-s)$.\n- $\\mathbb{E}[\\bar{Y}^2] = \\mathrm{Var}(\\bar{Y}) + (\\mathbb{E}[\\bar{Y}])^2$. Since $\\mathbb{E}[Y_{t}]=0$, we have $\\mathbb{E}[\\bar{Y}] = \\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}[Y_t] = 0$. Thus, $\\mathbb{E}[\\bar{Y}^2] = \\mathrm{Var}(\\bar{Y})$.\n$$ \\mathrm{Var}(\\bar{Y}) = \\mathrm{Var}\\left(\\frac{1}{T}\\sum_{t=1}^{T}Y_{t}\\right) = \\frac{1}{T^2} \\sum_{t=1}^{T}\\sum_{s=1}^{T} \\mathrm{Cov}(Y_{t},Y_{s}) = \\frac{1}{T^2}\\sum_{t=1}^{T}\\sum_{s=1}^{T}\\gamma_{Y}(t-s) $$\nCombining these results, the expectation of the estimator is:\n$$ \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} \\left( \\gamma_{Y}(\\tau) - \\frac{1}{T}\\sum_{s=1}^{T}\\gamma_{Y}(t-s) - \\frac{1}{T}\\sum_{s=1}^{T}\\gamma_{Y}(t+\\tau-s) + \\frac{1}{T^2}\\sum_{u=1}^{T}\\sum_{v=1}^{T}\\gamma_{Y}(u-v) \\right) $$\nSumming over $t$ from $1$ to $T-\\tau$ gives:\n$$ \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{T-\\tau}{T}\\gamma_{Y}(\\tau) - \\frac{1}{T^2}\\sum_{t=1}^{T-\\tau}\\sum_{s=1}^{T}\\left(\\gamma_{Y}(t-s)+\\gamma_{Y}(t+\\tau-s)\\right) + \\frac{T-\\tau}{T^3}\\sum_{u=1}^{T}\\sum_{v=1}^{T}\\gamma_{Y}(u-v) $$\nThis is the expression for $\\mathbb{E}[\\hat{\\gamma}_{Y}(\\tau)]$ in terms of $T$, $\\tau$, and $\\gamma_{Y}(\\cdot)$.\n\n**2. Finite-Sample Bias**\nThe bias of the estimator is $\\mathrm{Bias}[\\hat{\\gamma}_{Y}(\\tau)] = \\mathbb{E}[\\hat{\\gamma}_{Y}(\\tau)] - \\gamma_{Y}(\\tau)$.\nFrom the expression above, the bias has two main sources for finite $T$:\na) The leading term is $\\frac{T-\\tau}{T}\\gamma_{Y}(\\tau) = (1-\\frac{\\tau}{T})\\gamma_{Y}(\\tau)$, which differs from $\\gamma_{Y}(\\tau)$. This is partly due to the divisor $1/T$ instead of a more natural choice like $1/(T-\\tau)$. This component of the bias is $-\\frac{\\tau}{T}\\gamma_{Y}(\\tau)$.\nb) The remaining terms arise from using the sample mean $\\bar{Y}$ instead of the true mean $\\mathbb{E}[Y_t]=0$. Subtracting $\\bar{Y}$ introduces dependencies between all terms in the sum, as each $(Y_t - \\bar{Y})$ term is a function of the entire sample. The additional terms involving sums over the autocovariance function capture these complex dependencies. For large $T$, these terms are of order $O(1/T)$, assuming the autocovariance function decays sufficiently quickly (i.e., $\\sum_{k=-\\infty}^{\\infty}|\\gamma_{Y}(k)|  \\infty$). Therefore, for any finite sample size $T$, $\\hat{\\gamma}_{Y}(\\tau)$ is a biased estimator of $\\gamma_{Y}(\\tau)$.\n\n**3. Consistency**\nAn estimator is consistent if it converges in probability to the true value as the sample size approaches infinity. We need to show that $\\hat{\\gamma}_{Y}(\\tau) \\xrightarrow{p} \\gamma_{Y}(\\tau)$ as $T \\to \\infty$. This can be established by showing its bias and variance both tend to zero.\n\nFirst, consider the bias as $T \\to \\infty$. The lead term in the expectation, $(1-\\frac{\\tau}{T})\\gamma_{Y}(\\tau)$, converges to $\\gamma_{Y}(\\tau)$. All other terms are scaled by factors of at least $1/T$. For a process that is ergodic in the mean, $\\lim_{T\\to\\infty} T \\cdot \\mathrm{Var}(\\bar{Y})$ is finite, which means $\\mathrm{Var}(\\bar{Y})$ is $O(1/T)$. The other summed terms, when normalized, also vanish. Hence, $\\lim_{T\\to\\infty} \\mathbb{E}[\\hat{\\gamma}_{Y}(\\tau)] = \\gamma_{Y}(\\tau)$, meaning the estimator is asymptotically unbiased.\n\nSecond, we rely on the given property of ergodicity. Ergodicity implies that time averages of the process converge to their corresponding ensemble averages.\nWe can write the estimator as:\n$$ \\hat{\\gamma}_{Y}(\\tau) = \\frac{1}{T}\\sum_{t=1}^{T-\\tau}Y_{t}Y_{t+\\tau} - \\frac{\\bar{Y}}{T}\\sum_{t=1}^{T-\\tau}Y_{t} - \\frac{\\bar{Y}}{T}\\sum_{t=1}^{T-\\tau}Y_{t+\\tau} + \\frac{T-\\tau}{T}\\bar{Y}^2 $$\nAs $T\\to\\infty$:\n- The process being ergodic in the mean implies that the sample mean converges to the ensemble mean: $\\bar{Y} \\xrightarrow{p} \\mathbb{E}[Y_{t}]=0$.\n- The process being ergodic in the autocovariance implies that time averages of products converge to their ensemble averages. Specifically, for the stationary process $Z_{t} = Y_{t}Y_{t+\\tau}$, its time average converges to its expectation:\n$$ \\frac{1}{T-\\tau}\\sum_{t=1}^{T-\\tau}Y_{t}Y_{t+\\tau} \\xrightarrow{p} \\mathbb{E}[Y_{t}Y_{t+\\tau}] = \\gamma_{Y}(\\tau) $$\nConsidering the first term of $\\hat{\\gamma}_{Y}(\\tau)$: $\\frac{1}{T}\\sum_{t=1}^{T-\\tau}Y_{t}Y_{t+\\tau} = \\frac{T-\\tau}{T} \\left( \\frac{1}{T-\\tau}\\sum_{t=1}^{T-\\tau}Y_{t}Y_{t+\\tau} \\right)$. Since $\\frac{T-\\tau}{T} \\to 1$ and the term in parentheses converges to $\\gamma_{Y}(\\tau)$, this whole term converges in probability to $\\gamma_{Y}(\\tau)$.\n- All other terms involve $\\bar{Y}$ or $\\bar{Y}^2$. Since $\\bar{Y} \\xrightarrow{p} 0$, and the other sums remain bounded in probability, all subsequent terms converge to $0$ by Slutsky's theorem.\nTherefore, $\\hat{\\gamma}_{Y}(\\tau) \\xrightarrow{p} \\gamma_{Y}(\\tau)$ as $T\\to\\infty$, establishing the estimator's consistency under the assumption of ergodicity.\n\n### Part B: Bias from a Linear Trend\n\nNow, $X_{t} = \\mu + \\beta t + Y_{t}$. We want to find the additive bias $\\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right] - \\mathbb{E}_{Y}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right]$.\n\nFirst, we compute the sample mean $\\bar{X}$:\n$$ \\bar{X} = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu + \\beta t + Y_t) = \\mu + \\beta \\left(\\frac{1}{T} \\sum_{t=1}^{T} t \\right) + \\frac{1}{T}\\sum_{t=1}^{T} Y_t $$\nUsing the formula for the sum of the first $T$ integers, $\\sum_{t=1}^{T} t = \\frac{T(T+1)}{2}$, we get:\n$$ \\bar{X} = \\mu + \\beta \\frac{T+1}{2} + \\bar{Y} $$\nNext, we find the centered term $X_t - \\bar{X}$:\n$$ X_t - \\bar{X} = (\\mu + \\beta t + Y_t) - \\left( \\mu + \\beta \\frac{T+1}{2} + \\bar{Y} \\right) = \\beta\\left(t - \\frac{T+1}{2}\\right) + (Y_t - \\bar{Y}) $$\nLet $d_t = \\beta(t - \\frac{T+1}{2})$ be the deterministic component. Then $X_t - \\bar{X} = d_t + (Y_t - \\bar{Y})$.\n\nNow we substitute this into the estimator $\\hat{\\gamma}(\\tau)$:\n$$ \\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} (X_t - \\bar{X})(X_{t+\\tau} - \\bar{X}) $$\n$$ \\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} [d_t + (Y_t - \\bar{Y})] [d_{t+\\tau} + (Y_{t+\\tau} - \\bar{Y})] $$\nExpanding the product:\n$$ \\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} d_t d_{t+\\tau} + \\frac{1}{T} \\sum_{t=1}^{T-\\tau} d_t (Y_{t+\\tau} - \\bar{Y}) + \\frac{1}{T} \\sum_{t=1}^{T-\\tau} d_{t+\\tau} (Y_t - \\bar{Y}) + \\frac{1}{T} \\sum_{t=1}^{T-\\tau} (Y_t - \\bar{Y})(Y_{t+\\tau} - \\bar{Y}) $$\nThe last term is exactly $\\hat{\\gamma}_{Y}(\\tau)$. Let's take the expectation over the randomness in $Y_t$:\n$$ \\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right] = \\mathbb{E}\\left[ \\frac{1}{T}\\sum d_t d_{t+\\tau} \\right] + \\mathbb{E}\\left[ \\frac{1}{T}\\sum d_t (Y_{t+\\tau} - \\bar{Y}) \\right] + \\mathbb{E}\\left[ \\frac{1}{T}\\sum d_{t+\\tau} (Y_t - \\bar{Y}) \\right] + \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] $$\nThe terms $d_t$ are deterministic. The cross-terms' expectations are zero because $\\mathbb{E}[Y_t - \\bar{Y}] = \\mathbb{E}[Y_t] - \\mathbb{E}[\\bar{Y}] = 0 - 0 = 0$.\n$$ \\mathbb{E}\\left[ \\frac{1}{T}\\sum d_t (Y_{t+\\tau} - \\bar{Y}) \\right] = \\frac{1}{T}\\sum d_t \\mathbb{E}[Y_{t+\\tau} - \\bar{Y}] = 0 $$\nThus, we have:\n$$ \\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} d_t d_{t+\\tau} + \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] $$\nThe additive bias due to the trend is therefore purely deterministic:\n$$ \\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right] - \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} d_t d_{t+\\tau} $$\nSubstituting the expression for $d_t$:\n$$ \\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\frac{\\beta^2}{T} \\sum_{t=1}^{T-\\tau} \\left(t - \\frac{T+1}{2}\\right) \\left(t+\\tau - \\frac{T+1}{2}\\right) $$\nLet $K = \\frac{T+1}{2}$ and $N = T-\\tau$. The summation becomes:\n$$ S = \\sum_{t=1}^{N} (t - K)(t+\\tau-K) = \\sum_{t=1}^{N} [t^2 + (\\tau-2K)t + (K^2 - \\tau K)] $$\n$$ S = \\sum_{t=1}^{N} t^2 + (\\tau-2K)\\sum_{t=1}^{N} t + N(K^2 - \\tau K) $$\nUsing the standard sum formulas $\\sum_{t=1}^{N} t = \\frac{N(N+1)}{2}$ and $\\sum_{t=1}^{N} t^2 = \\frac{N(N+1)(2N+1)}{6}$:\nSubstituting $K=\\frac{T+1}{2}$ and $N=T-\\tau$, we have $\\tau-2K = \\tau-(T+1) = -(T-\\tau+1)=-(N+1)$.\n$$ S = \\frac{N(N+1)(2N+1)}{6} - (N+1)\\frac{N(N+1)}{2} + N(K^2 - \\tau K) $$\nThe first two terms simplify:\n$$ \\frac{N(N+1)}{6} [ (2N+1) - 3(N+1) ] = \\frac{N(N+1)}{6}[-N-2] = -\\frac{N(N+1)(N+2)}{6} $$\nThe last term is $N(K^2 - \\tau K) = N K(K-\\tau) = N \\frac{T+1}{2}(\\frac{T+1}{2}-\\tau) = N \\frac{(T+1)(T+1-2\\tau)}{4}$.\nSubstitute $T=N+\\tau$: $T+1=N+\\tau+1$ and $T+1-2\\tau = N-\\tau+1$.\n$N(K^2 - \\tau K) = N \\frac{(N+\\tau+1)(N-\\tau+1)}{4} = N \\frac{(N+1)^2-\\tau^2}{4}$.\nSo, $S = -\\frac{N(N+1)(N+2)}{6} + \\frac{N((N+1)^2-\\tau^2)}{4}$.\nCombining over a common denominator of $12$:\n$$ S = \\frac{N}{12} [-2(N+1)(N+2) + 3((N+1)^2 - \\tau^2)] $$\n$$ S = \\frac{N}{12} [-2(N^2+3N+2) + 3(N^2+2N+1-\\tau^2)] $$\n$$ S = \\frac{N}{12} [-2N^2 - 6N - 4 + 3N^2 + 6N + 3 - 3\\tau^2] = \\frac{N}{12} [N^2 - 1 - 3\\tau^2] $$\nSubstituting $N=T-\\tau$ back:\n$$ S = \\frac{T-\\tau}{12} [(T-\\tau)^2 - 1 - 3\\tau^2] = \\frac{T-\\tau}{12} [T^2 - 2T\\tau + \\tau^2 - 1 - 3\\tau^2] $$\n$$ S = \\frac{T-\\tau}{12} [T^2 - 2T\\tau - 2\\tau^2 - 1] $$\nThe bias is $\\frac{\\beta^2}{T}S$:\n$$ \\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\frac{\\beta^2(T-\\tau)}{12T} (T^2 - 2T\\tau - 2\\tau^2 - 1) $$\nExpanding this expression gives:\n$$ \\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\frac{\\beta^2}{12T} (T(T^2 - 2T\\tau - 2\\tau^2 - 1) - \\tau(T^2 - 2T\\tau - 2\\tau^2 - 1)) $$\n$$ = \\frac{\\beta^2}{12T} (T^3 - 2T^2\\tau - 2T\\tau^2 - T - \\tau T^2 + 2T\\tau^2 + 2\\tau^3 + \\tau) $$\n$$ = \\frac{\\beta^2}{12T} (T^3 - 3T^2\\tau + 2\\tau^3 - T + \\tau) $$\nThis is the final closed-form expression for the additive bias due to the linear trend.",
            "answer": "$$\n\\boxed{\\frac{\\beta^2}{12T} \\left( T^{3} - 3T^{2}\\tau + 2\\tau^{3} - T + \\tau \\right)}\n$$"
        },
        {
            "introduction": "Having established how non-stationarity can corrupt statistical estimates, a crucial next step is to formally test for its presence. The Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test provides a rigorous framework for this, testing the null hypothesis that a series is stationary around a constant level. This exercise  guides you through the theoretical construction of the KPSS statistic, building it from the partial sums of residuals and linking it to the concept of the long-run variance.",
            "id": "4200528",
            "problem": "You are analyzing a baseline segment of a single-channel local field potential recorded from rodent dorsal hippocampus during immobility. Let the discrete-time series be $\\{y_t\\}_{t=1}^{T}$ with sampling frequency $f_s$, and suppose the series admits the decomposition\n$$\ny_t \\;=\\; \\mu \\;+\\; r_t \\;+\\; \\varepsilon_t,\n$$\nwhere $r_t = r_{t-1} + \\xi_t$ with $\\{ \\xi_t \\}$ independent and identically distributed mean-zero innovations, and $\\{ \\varepsilon_t \\}$ is a zero-mean weakly stationary and ergodic process with finite long-run variance. Assume $\\{ \\xi_t \\}$ is independent of $\\{ \\varepsilon_t \\}$, and that $\\{ \\varepsilon_t \\}$ has autocovariance function $\\gamma_k = \\operatorname{Cov}(\\varepsilon_t,\\varepsilon_{t-k})$ satisfying $\\sum_{k=-\\infty}^{\\infty} |\\gamma_k|  \\infty$.\n\nYour tasks are:\n1) Using the above decomposition as the foundational representation, formally define the null and alternative hypotheses for the Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test of level stationarity of $\\{y_t\\}$.\n\n2) Starting from the core definitions of weak stationarity and the long-run variance, derive the KPSS test statistic for level stationarity based on partial sums of residuals from an intercept-only regression $y_t = \\mu + u_t$. Define the residuals $\\hat{\\varepsilon}_t = y_t - \\hat{\\mu}$, where $\\hat{\\mu}$ is the ordinary least squares (OLS) estimator of $\\mu$, and the partial sums $S_t = \\sum_{i=1}^{t} \\hat{\\varepsilon}_i$. Use a heteroskedasticity-and-autocorrelation consistent (HAC) estimator for the long-run variance with a Bartlett kernel and bandwidth $L$, i.e., $\\hat{\\sigma}^2 = \\hat{\\gamma}_0 + 2\\sum_{j=1}^{L}\\left(1 - \\frac{j}{L+1}\\right)\\hat{\\gamma}_j$, where $\\hat{\\gamma}_j = \\frac{1}{T}\\sum_{t=j+1}^{T} \\hat{\\varepsilon}_t \\hat{\\varepsilon}_{t-j}$.\n\n3) Provide the final closed-form analytic expression for the KPSS test statistic in terms of $T$, the partial sums $S_t$, and the HAC long-run variance estimate $\\hat{\\sigma}^2$. No numerical evaluation is required. The final answer must be a single analytic expression or a single row matrix of analytic expressions. No units should be included in the final answer.",
            "solution": "The problem is valid as it is scientifically grounded in established statistical theory, well-posed, and objective. All components, including the time series decomposition, definitions for residuals and partial sums, and the structure of the long-run variance estimator, are standard in the analysis of non-stationary time series. We proceed with the solution.\n\nThe problem requires a three-part answer concerning the Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test for a time series $\\{y_t\\}_{t=1}^{T}$.\n\n1) Definition of the null and alternative hypotheses.\n\nThe provided model for the time series is\n$$\ny_t = \\mu + r_t + \\varepsilon_t\n$$\nwhere $\\{ \\varepsilon_t \\}$ is a zero-mean weakly stationary process and $r_t$ is a random walk defined by $r_t = r_{t-1} + \\xi_t$, with $\\{ \\xi_t \\}$ being an independent and identically distributed process with mean $E[\\xi_t]=0$ and variance $\\operatorname{Var}(\\xi_t) = \\sigma_\\xi^2$. We can assume without loss of generality that the initial value of the random walk is $r_0=0$, as any non-zero constant $r_0$ can be absorbed into the intercept term $\\mu$.\n\nThe process $\\{y_t\\}$ is level stationary if and only if the random walk component $r_t$ is absent, meaning it remains constant over time. Since $r_t = r_{t-1} + \\xi_t$, for $r_t$ to be constant, each innovation $\\xi_t$ must be zero. Given that $E[\\xi_t]=0$, this is equivalent to the condition that the variance of the innovations is zero, i.e., $\\sigma_\\xi^2 = 0$. If $\\sigma_\\xi^2 > 0$, the variance of $r_t$, which is $\\operatorname{Var}(r_t) = t\\sigma_\\xi^2$, grows with time, inducing a stochastic trend and rendering $\\{y_t\\}$ non-stationary.\n\nTherefore, the KPSS test for level stationarity evaluates the following hypotheses:\nThe null hypothesis, $H_0$, states that the time series is level stationary. This corresponds to the variance of the random walk innovations being zero.\n$$\nH_0: \\sigma_\\xi^2 = 0\n$$\nUnder $H_0$, the model simplifies to $y_t = \\mu + \\varepsilon_t$, which describes a stationary process fluctuating around a constant level $\\mu$.\n\nThe alternative hypothesis, $H_1$, states that the time series has a unit root (is difference-stationary), which implies the presence of a stochastic trend. This corresponds to a positive variance for the random walk innovations.\n$$\nH_1: \\sigma_\\xi^2 > 0\n$$\nUnder $H_1$, the random walk component $r_t$ is present, and $\\{y_t\\}$ is non-stationary.\n\n2) Derivation of the KPSS test statistic.\n\nThe KPSS test is a Lagrange Multiplier (LM) test designed to detect the presence of a random walk component. The statistic is constructed using the residuals from a regression of $y_t$ under the null hypothesis. For level stationarity, the regression model under $H_0$ is $y_t = \\mu + u_t$.\n\nFirst, we estimate the intercept $\\mu$ via Ordinary Least Squares (OLS). The OLS estimator for $\\mu$ is the sample mean of $y_t$:\n$$\n\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^{T} y_t\n$$\nThe residuals are then calculated as the deviation from this sample mean:\n$$\n\\hat{\\varepsilon}_t = y_t - \\hat{\\mu}\n$$\nThese residuals, $\\hat{\\varepsilon}_t$, are the empirical counterpart to the unobserved stationary component $\\varepsilon_t$ (after demeaning).\n\nThe core of the test statistic is built upon the partial sums of these residuals. The partial sum process is defined as:\n$$\nS_t = \\sum_{i=1}^{t} \\hat{\\varepsilon}_i\n$$\nUnder the null hypothesis of stationarity, the process $\\{\\varepsilon_t\\}$ has a constant mean (assumed to be zero), so the partial sums of the residuals, $S_t$, are expected to fluctuate around zero. Under the alternative hypothesis, the presence of the unit root component would cause the residuals to accumulate and the partial sum process $S_t$ to diverge, behaving like a random walk itself. The test statistic is designed to measure the magnitude of this partial sum process.\n\nThe KPSS statistic is based on the sum of the squared partial sums, $\\sum_{t=1}^{T} S_t^2$. To obtain a statistic with a well-defined asymptotic distribution, this sum must be properly scaled. Based on the Functional Central Limit Theorem applied to weakly dependent processes, the scaled partial sum process $\\frac{1}{\\sqrt{T}\\sigma} S_{\\lfloor Tr \\rfloor}$ (for $r \\in [0,1]$) converges in distribution to a standard Brownian bridge, $V(r) = W(r) - rW(1)$, where $W(r)$ is a standard Wiener process. The term $\\sigma^2$ is the long-run variance of the process $\\{\\varepsilon_t\\}$, defined as:\n$$\n\\sigma^2 = \\lim_{T \\to \\infty} \\frac{1}{T} \\operatorname{Var}\\left(\\sum_{t=1}^{T} \\varepsilon_t\\right) = \\sum_{k=-\\infty}^{\\infty} \\gamma_k = \\gamma_0 + 2\\sum_{k=1}^{\\infty}\\gamma_k\n$$\nThe condition $\\sum_{k=-\\infty}^{\\infty} |\\gamma_k|  \\infty$ ensures this limit is finite.\n\nThe test statistic is constructed as an empirical analogue of the integral of the squared Brownian bridge, $\\int_0^1 V(r)^2 dr$. The sum $\\sum_{t=1}^{T} S_t^2$ corresponds to the integral, and the appropriate scaling factor is $\\frac{1}{T^2}$. The full statistic is:\n$$\n\\text{KPSS} = \\frac{1}{T^2\\sigma^2} \\sum_{t=1}^{T} S_t^2\n$$\nSince the true long-run variance $\\sigma^2$ is unknown, it must be replaced with a consistent estimator, $\\hat{\\sigma}^2$. The problem specifies a heteroskedasticity-and-autocorrelation consistent (HAC) estimator using a Bartlett kernel with bandwidth $L$:\n$$\n\\hat{\\sigma}^2 = \\hat{\\gamma}_0 + 2\\sum_{j=1}^{L}\\left(1 - \\frac{j}{L+1}\\right)\\hat{\\gamma}_j\n$$\nwhere $\\hat{\\gamma}_j = \\frac{1}{T}\\sum_{t=j+1}^{T} \\hat{\\varepsilon}_t \\hat{\\varepsilon}_{t-j}$ is the sample autocovariance at lag $j$.\n\nSubstituting this estimator into the expression for the statistic gives the final, operational form of the KPSS test statistic for level stationarity.\n\n3) Final closed-form analytic expression for the KPSS test statistic.\n\nCombining the components derived above, the KPSS test statistic, often denoted $\\eta_{\\mu}$, for level stationarity is given by the sum of squared partial sums of residuals, scaled by the sample size squared and the estimated long-run variance. In terms of the given quantities $T$, $S_t$, and $\\hat{\\sigma}^2$, the expression is:\n$$\n\\eta_{\\mu} = \\frac{\\sum_{t=1}^{T} S_t^2}{T^2 \\hat{\\sigma}^2}\n$$\nThis expression constitutes the final answer.",
            "answer": "$$\n\\boxed{\\frac{\\sum_{t=1}^{T} S_t^2}{T^2 \\hat{\\sigma}^2}}\n$$"
        },
        {
            "introduction": "Neuroscience data analysis rarely begins with raw signals; preprocessing steps are essential for artifact removal and signal enhancement. A key question is how these transformations affect the underlying statistical properties of the data. This problem  focuses on the common average reference (CAR) in multichannel EEG, asking you to prove that such linear time-invariant operations preserve stationarity while simultaneously deriving how they reshape the covariance structure to eliminate common-mode signals.",
            "id": "4200531",
            "problem": "Electroencephalography (EEG) multichannel time series analysis often employs re-referencing to reduce artifacts and common-mode signals. Consider an $m$-channel EEG signal modeled as a zero-mean, jointly wide-sense stationary stochastic process $x(t) \\in \\mathbb{R}^{m}$, where $t$ indexes discrete time. Wide-sense stationarity is defined by constant mean and an autocovariance function that depends only on time lag. Electroencephalography (EEG) re-referencing is a linear operation applied across channels at each time point. Common average reference (CAR) subtracts the average across all channels from each channel, defined component-wise by\n$$\ny_{i}(t) = x_{i}(t) - \\frac{1}{m} \\sum_{j=1}^{m} x_{j}(t), \\quad i = 1, \\dots, m.\n$$\nLet $1 \\in \\mathbb{R}^{m}$ denote the all-ones vector and $I_{m} \\in \\mathbb{R}^{m \\times m}$ the identity matrix. Suppose the lag-zero covariance matrix of $x(t)$ is\n$$\n\\Sigma_{x} = \\sigma^{2} I_{m} + \\beta \\, 1 1^{\\top},\n$$\nwith $\\sigma^{2} > 0$ and $\\beta \\in \\mathbb{R}$, representing independent channel fluctuations plus a global common-mode component. Assume $x(t)$ is ergodic in the second order (time averages converge to ensemble averages of autocovariance).\n\nTasks:\n1. Starting from the definitions of wide-sense stationarity and second-order ergodicity, and the properties of linear time-invariant transformations, explain theoretically when re-referencing preserves stationarity and ergodicity, and when it can induce nonstationarity. Your explanation should rely only on core definitions and well-tested facts about linear transformations of stochastic processes.\n2. Using the CAR operation defined above, derive from first principles the closed-form expression for the lag-zero covariance matrix of the re-referenced process $y(t)$ in terms of $m$, $\\sigma^{2}$, and $1$. Provide the final result for the lag-zero covariance matrix as a single analytic expression. No numerical rounding is required, and no physical units are involved.\n\nYour final answer must be a single analytical expression for the lag-zero covariance matrix of $y(t)$.",
            "solution": "The problem presents two tasks: first, to explain theoretically how re-referencing affects the properties of stationarity and ergodicity of a time series, and second, to derive the lag-zero covariance matrix of a signal after applying a Common Average Reference (CAR) transformation. The solution will address these two tasks in sequence.\n\nFirst, we analyze the effect of a general linear time-invariant (LTI) transformation on a wide-sense stationary (WSS) and ergodic stochastic process. The CAR operation is a specific instance of such a transformation.\n\nLet $x(t)$ be an $m$-dimensional vector stochastic process. A linear transformation maps $x(t)$ to another process $y(t)$ via a matrix $L \\in \\mathbb{R}^{m \\times m}$ as $y(t) = L x(t)$. If the matrix $L$ does not depend on time $t$, the transformation is time-invariant.\n\nA process $x(t)$ is wide-sense stationary (WSS) if its mean vector is constant and its autocovariance matrix depends only on the time lag $\\tau = t_1 - t_2$.\n$1$. The mean of $x(t)$ is $E[x(t)] = \\mu_x$, a constant vector.\n$2$. The autocovariance of $x(t)$ is $R_x(t_1, t_2) = E[(x(t_1) - \\mu_x)(x(t_2) - \\mu_x)^{\\top}] = R_x(t_1 - t_2)$.\n\nLet us examine the properties of the output process $y(t) = L x(t)$.\nThe mean of $y(t)$ is:\n$$E[y(t)] = E[L x(t)] = L E[x(t)] = L \\mu_x$$\nSince $L$ and $\\mu_x$ are constant, the resulting mean vector $\\mu_y = L \\mu_x$ is also constant.\n\nThe autocovariance of $y(t)$ is:\n$$R_y(t_1, t_2) = E[(y(t_1) - \\mu_y)(y(t_2) - \\mu_y)^{\\top}]$$\nSubstituting $y(t) = L x(t)$ and $\\mu_y = L \\mu_x$:\n$$R_y(t_1, t_2) = E[(L x(t_1) - L \\mu_x)(L x(t_2) - L \\mu_x)^{\\top}]$$\n$$R_y(t_1, t_2) = E[L(x(t_1) - \\mu_x)(x(t_2) - \\mu_x)^{\\top}L^{\\top}]$$\nBy the linearity of the expectation operator:\n$$R_y(t_1, t_2) = L E[(x(t_1) - \\mu_x)(x(t_2) - \\mu_x)^{\\top}] L^{\\top} = L R_x(t_1, t_2) L^{\\top}$$\nSince $x(t)$ is WSS, $R_x(t_1, t_2) = R_x(t_1 - t_2)$. Therefore:\n$$R_y(t_1, t_2) = L R_x(t_1 - t_2) L^{\\top}$$\nThe right-hand side depends only on the time lag $\\tau = t_1 - t_2$. Let us define $R_y(\\tau) = L R_x(\\tau) L^{\\top}$. Thus, the autocovariance of $y(t)$ also depends only on the time lag. As $y(t)$ has a constant mean and an autocovariance function that is only a function of time lag, $y(t)$ is also WSS.\nThis demonstrates that any LTI transformation preserves wide-sense stationarity. Non-stationarity could be induced only if the linear transformation were time-varying, i.e., $y(t) = L(t) x(t)$, which is not the case for CAR.\n\nSecond-order ergodicity implies that time averages of second-order moments (like the autocovariance) converge to their respective ensemble averages. For a WSS process, this means the time-averaged autocovariance estimator $\\hat{R}_x(\\tau)$ converges to the true autocovariance $R_x(\\tau)$ as the observation interval tends to infinity.\nThe time-averaged autocovariance for $y(t)$ is $\\hat{R}_y(\\tau)$. Because $y(t)$ is an LTI-filtered version of $x(t)$, its time-averaged statistics are related to those of $x(t)$ by the same transformation:\n$$\\hat{R}_y(\\tau) = L \\hat{R}_x(\\tau) L^{\\top}$$\nTaking the limit as the observation interval approaches infinity and using the ergodicity of $x(t)$:\n$$\\lim \\hat{R}_y(\\tau) = \\lim (L \\hat{R}_x(\\tau) L^{\\top}) = L (\\lim \\hat{R}_x(\\tau)) L^{\\top} = L R_x(\\tau) L^{\\top}$$\nWe have already established that $R_y(\\tau) = L R_x(\\tau) L^{\\top}$. Therefore, $\\lim \\hat{R}_y(\\tau) = R_y(\\tau)$, which means the process $y(t)$ is also second-order ergodic. An LTI transformation preserves ergodicity of a WSS process.\n\nNow, we proceed to the second task: deriving the lag-zero covariance matrix for the re-referenced process $y(t)$.\nThe CAR operation is given by:\n$$y_{i}(t) = x_{i}(t) - \\frac{1}{m} \\sum_{j=1}^{m} x_{j}(t)$$\nLet's express this in vector form. The average across channels is $\\frac{1}{m} \\sum_{j=1}^{m} x_{j}(t) = \\frac{1}{m} 1^{\\top}x(t)$. Subtracting this scalar from each component of $x(t)$ is equivalent to subtracting the vector $1 \\left( \\frac{1}{m} 1^{\\top}x(t) \\right)$.\nSo, the vector process $y(t)$ is:\n$$y(t) = x(t) - 1 \\left( \\frac{1}{m} 1^{\\top}x(t) \\right) = x(t) - \\left(\\frac{1}{m} 1 1^{\\top}\\right)x(t)$$\nThis can be written as $y(t) = P x(t)$, where $P$ is the transformation matrix:\n$$P = I_m - \\frac{1}{m} 1 1^{\\top}$$\nThis matrix $P$ is known as the centering matrix. It is a projection matrix, meaning it is idempotent ($P^2=P$) and symmetric ($P^{\\top}=P$).\n\nThe problem states that $x(t)$ is a zero-mean process. Therefore, the output process $y(t)$ is also zero-mean:\n$$E[y(t)] = E[P x(t)] = P E[x(t)] = P \\cdot 0 = 0$$\nThe lag-zero covariance matrix of $y(t)$, denoted $\\Sigma_y$, is given by:\n$$\\Sigma_y = E[y(t) y(t)^{\\top}] = E[(P x(t))(P x(t))^{\\top}] = E[P x(t) x(t)^{\\top} P^{\\top}]$$\nSince $P$ is a constant matrix:\n$$\\Sigma_y = P E[x(t) x(t)^{\\top}] P^{\\top}$$\nThe term $E[x(t) x(t)^{\\top}]$ is the lag-zero covariance matrix of $x(t)$, which is given as $\\Sigma_x$. The matrix $P$ is symmetric, so $P^{\\top} = P$.\n$$\\Sigma_y = P \\Sigma_x P$$\nWe are given $\\Sigma_x = \\sigma^{2} I_{m} + \\beta \\, 1 1^{\\top}$. Let's substitute this into the expression for $\\Sigma_y$:\n$$\\Sigma_y = P (\\sigma^{2} I_{m} + \\beta \\, 1 1^{\\top}) P$$\nLet's first compute the product $P \\Sigma_x$:\n$$P \\Sigma_x = \\left(I_m - \\frac{1}{m} 1 1^{\\top}\\right)(\\sigma^{2} I_{m} + \\beta \\, 1 1^{\\top})$$\n$$P \\Sigma_x = \\sigma^{2} I_{m} \\left(I_m - \\frac{1}{m} 1 1^{\\top}\\right) + \\beta \\left(I_m - \\frac{1}{m} 1 1^{\\top}\\right) (1 1^{\\top})$$\n$$P \\Sigma_x = \\sigma^{2} P + \\beta (1 1^{\\top} - \\frac{1}{m} (1 1^{\\top})(1 1^{\\top}))$$\nThe term $(1 1^{\\top})(1 1^{\\top}) = 1 (1^{\\top}1) 1^{\\top} = 1(m)1^{\\top} = m(1 1^{\\top})$.\n$$P \\Sigma_x = \\sigma^{2} P + \\beta \\left(1 1^{\\top} - \\frac{1}{m} m(1 1^{\\top})\\right) = \\sigma^{2} P + \\beta (1 1^{\\top} - 1 1^{\\top}) = \\sigma^{2} P + 0 = \\sigma^{2} P$$\nThis is a significant simplification. The CAR operator annihilates the common-mode component of the covariance structure. Now, we substitute this back into the expression for $\\Sigma_y$:\n$$\\Sigma_y = (P \\Sigma_x) P = (\\sigma^2 P) P = \\sigma^2 P^2$$\nSince $P$ is a projection matrix, $P^2 = P$. Therefore:\n$$\\Sigma_y = \\sigma^2 P$$\nSubstituting the definition of $P$ gives the final expression for the lag-zero covariance matrix of the re-referenced process $y(t)$:\n$$\\Sigma_y = \\sigma^2 \\left(I_m - \\frac{1}{m} 1 1^{\\top}\\right)$$\nThis result shows that the CAR transformation completely removes the global common-mode component of covariance (parameterized by $\\beta$) and scales the independent variance component $\\sigma^2$ by the projection matrix $P$. The resulting covariance matrix is singular, as its rows and columns sum to zero, reflecting the linear dependency $\\sum_{i=1}^m y_i(t) = 0$ introduced by the CAR operation.",
            "answer": "$$\\boxed{\\sigma^{2}\\left(I_{m} - \\frac{1}{m} 1 1^{\\top}\\right)}$$"
        }
    ]
}