{
    "hands_on_practices": [
        {
            "introduction": "Before we can interpret or even estimate a VAR model, we must ensure it describes a stable, stationary process. This exercise  introduces a fundamental diagnostic tool: transforming a higher-order VAR system into its first-order companion form. By calculating the spectral radius of this companion matrix, you can directly assess whether the modeled system is stable, a critical prerequisite for meaningful analysis of neural dynamics.",
            "id": "4203481",
            "problem": "A two-channel neural time series $\\mathbf{y}_t \\in \\mathbb{R}^2$ captures simultaneous Local Field Potential (LFP) activity from two cortical sites. Assume the dynamics of $\\mathbf{y}_t$ follow a bivariate Vector Autoregressive (VAR) model of order $2$, defined by\n$$\n\\mathbf{y}_t = A_1 \\mathbf{y}_{t-1} + A_2 \\mathbf{y}_{t-2} + \\mathbf{e}_t,\n$$\nwhere $\\mathbf{e}_t$ is a zero-mean, covariance-stationary innovation process with finite second moments and no temporal correlation, and\n$$\nA_1 = \\begin{pmatrix}\n\\frac{3}{5}  0 \\\\\n0  \\frac{1}{5}\n\\end{pmatrix},\n\\qquad\nA_2 = \\begin{pmatrix}\n-\\frac{2}{5}  0 \\\\\n0  \\frac{1}{2}\n\\end{pmatrix}.\n$$\nStarting from the recurrence relation above and the principle of state augmentation, construct the first-order companion form for the VAR($2$) by defining an augmented state that turns the order-$2$ recurrence into an order-$1$ system. From this construction, obtain the companion matrix $F$ and compute its spectral radius, defined as the maximum modulus among the eigenvalues of $F$. Report only the spectral radius as a simplified exact expression using radicals. Do not provide a numerical approximation, and do not include units. The spectral radius will be used to assess stability in the sense that all eigenvalues must lie strictly inside the unit circle for covariance-stationarity.",
            "solution": "The problem requires the construction of the companion form for a given bivariate Vector Autoregressive model of order $2$, or VAR($2$), and the subsequent computation of the spectral radius of the companion matrix.\n\nThe given VAR($2$) model is:\n$$\n\\mathbf{y}_t = A_1 \\mathbf{y}_{t-1} + A_2 \\mathbf{y}_{t-2} + \\mathbf{e}_t\n$$\nwhere $\\mathbf{y}_t \\in \\mathbb{R}^2$ and the coefficient matrices are:\n$$\nA_1 = \\begin{pmatrix}\n\\frac{3}{5}  0 \\\\\n0  \\frac{1}{5}\n\\end{pmatrix},\n\\qquad\nA_2 = \\begin{pmatrix}\n-\\frac{2}{5}  0 \\\\\n0  \\frac{1}{2}\n\\end{pmatrix}\n$$\n\nTo convert this second-order system into a first-order system, we utilize the principle of state augmentation. We define an augmented state vector $\\mathbf{z}_t$ that includes the current and first-lagged state of $\\mathbf{y}_t$. Let the dimension of $\\mathbf{y}_t$ be $K=2$ and the order of the process be $p=2$. The augmented state vector $\\mathbf{z}_t \\in \\mathbb{R}^{Kp}$ will be of dimension $4 \\times 1$.\nLet us define $\\mathbf{z}_t$ as:\n$$\n\\mathbf{z}_t = \\begin{pmatrix} \\mathbf{y}_t \\\\ \\mathbf{y}_{t-1} \\end{pmatrix}\n$$\nNow, we express $\\mathbf{z}_t$ in terms of its own lag, $\\mathbf{z}_{t-1}$:\n$$\n\\mathbf{z}_t = \\begin{pmatrix} \\mathbf{y}_t \\\\ \\mathbf{y}_{t-1} \\end{pmatrix} = \\begin{pmatrix} A_1 \\mathbf{y}_{t-1} + A_2 \\mathbf{y}_{t-2} + \\mathbf{e}_t \\\\ \\mathbf{y}_{t-1} \\end{pmatrix}\n$$\nThis can be written in a matrix equation form. By recognizing that $\\mathbf{z}_{t-1} = \\begin{pmatrix} \\mathbf{y}_{t-1} \\\\ \\mathbf{y}_{t-2} \\end{pmatrix}$, we have:\n$$\n\\mathbf{z}_t = \\begin{pmatrix} A_1  A_2 \\\\ I_K  0_K \\end{pmatrix} \\mathbf{z}_{t-1} + \\begin{pmatrix} \\mathbf{e}_t \\\\ \\mathbf{0} \\end{pmatrix}\n$$\nwhere $I_K$ is the $K \\times K$ identity matrix and $0_K$ is the $K \\times K$ zero matrix. This is the first-order companion form $\\mathbf{z}_t = F \\mathbf{z}_{t-1} + \\mathbf{v}_t$, where the companion matrix $F$ is:\n$$\nF = \\begin{pmatrix} A_1  A_2 \\\\ I_K  0_K \\end{pmatrix}\n$$\nSubstituting the given $2 \\times 2$ matrices $A_1$, $A_2$, and the identity matrix $I_2$, we obtain the $4 \\times 4$ companion matrix $F$:\n$$\nF = \\begin{pmatrix}\n\\frac{3}{5}  0  -\\frac{2}{5}  0 \\\\\n0  \\frac{1}{5}  0  \\frac{1}{2} \\\\\n1  0  0  0 \\\\\n0  1  0  0\n\\end{pmatrix}\n$$\nThe spectral radius $\\rho(F)$ is the maximum modulus of the eigenvalues of $F$. The eigenvalues $\\lambda$ are the solutions to the characteristic equation $\\det(F - \\lambda I_4) = 0$.\nThe matrix $A_1$ and $A_2$ are diagonal. This implies that the two time series components are not mutually coupled. The dynamics of the system can be decomposed into two independent scalar AR($2$) processes. We can reorder the rows and columns of $F$ to reveal a block-diagonal structure, which simplifies the eigenvalue calculation. The eigenvalues of $F$ are the union of the eigenvalues of the sub-matrices corresponding to each independent process.\n\nThe two independent scalar AR($2$) processes are:\n$y_{1,t} = \\frac{3}{5} y_{1,t-1} - \\frac{2}{5} y_{1,t-2} + e_{1,t}$\n$y_{2,t} = \\frac{1}{5} y_{2,t-1} + \\frac{1}{2} y_{2,t-2} + e_{2,t}$\n\nThe companion matrix can be permutation-similar to a block-diagonal matrix with blocks:\n$F_1 = \\begin{pmatrix} \\frac{3}{5}  -\\frac{2}{5} \\\\ 1  0 \\end{pmatrix}$ for the first process, and\n$F_2 = \\begin{pmatrix} \\frac{1}{5}  \\frac{1}{2} \\\\ 1  0 \\end{pmatrix}$ for the second process.\n\nWe compute the eigenvalues for each block.\nFor $F_1$, the characteristic equation is $\\det(F_1 - \\lambda I_2) = 0$:\n$$\n\\lambda^2 - \\frac{3}{5}\\lambda + \\frac{2}{5} = 0\n$$\nUsing the quadratic formula, the eigenvalues $\\lambda_{1,2}$ are:\n$$\n\\lambda_{1,2} = \\frac{\\frac{3}{5} \\pm \\sqrt{(\\frac{3}{5})^2 - 4(1)(\\frac{2}{5})}}{2} = \\frac{\\frac{3}{5} \\pm \\sqrt{\\frac{9}{25} - \\frac{8}{5}}}{2} = \\frac{\\frac{3}{5} \\pm \\sqrt{\\frac{9-40}{25}}}{2} = \\frac{\\frac{3}{5} \\pm i\\frac{\\sqrt{31}}{5}}{2} = \\frac{3}{10} \\pm i\\frac{\\sqrt{31}}{10}\n$$\nThese are a complex conjugate pair. Their modulus is:\n$$\n|\\lambda_{1,2}| = \\sqrt{\\left(\\frac{3}{10}\\right)^2 + \\left(\\frac{\\sqrt{31}}{10}\\right)^2} = \\sqrt{\\frac{9+31}{100}} = \\sqrt{\\frac{40}{100}} = \\frac{\\sqrt{40}}{10} = \\frac{2\\sqrt{10}}{10} = \\frac{\\sqrt{10}}{5}\n$$\n\nFor $F_2$, the characteristic equation is $\\det(F_2 - \\lambda I_2) = 0$:\n$$\n\\lambda^2 - \\frac{1}{5}\\lambda - \\frac{1}{2} = 0\n$$\nUsing the quadratic formula, the eigenvalues $\\lambda_{3,4}$ are:\n$$\n\\lambda_{3,4} = \\frac{\\frac{1}{5} \\pm \\sqrt{(-\\frac{1}{5})^2 - 4(1)(-\\frac{1}{2})}}{2} = \\frac{\\frac{1}{5} \\pm \\sqrt{\\frac{1}{25} + 2}}{2} = \\frac{\\frac{1}{5} \\pm \\sqrt{\\frac{1+50}{25}}}{2} = \\frac{\\frac{1}{5} \\pm \\frac{\\sqrt{51}}{5}}{2} = \\frac{1 \\pm \\sqrt{51}}{10}\n$$\nThese eigenvalues are real. Their moduli are:\n$$\n|\\lambda_3| = \\left|\\frac{1 + \\sqrt{51}}{10}\\right| = \\frac{1 + \\sqrt{51}}{10}\n$$\n$$\n|\\lambda_4| = \\left|\\frac{1 - \\sqrt{51}}{10}\\right| = \\frac{\\sqrt{51} - 1}{10}\n$$\nThe spectral radius $\\rho(F)$ is the maximum of these moduli: $\\{\\frac{\\sqrt{10}}{5}, \\frac{1+\\sqrt{51}}{10}, \\frac{\\sqrt{51}-1}{10}\\}$.\nClearly, $\\frac{1+\\sqrt{51}}{10}  \\frac{\\sqrt{51}-1}{10}$. We must compare $\\frac{\\sqrt{10}}{5}$ and $\\frac{1+\\sqrt{51}}{10}$.\nLet's compare their squares:\n$$\n\\left(\\frac{\\sqrt{10}}{5}\\right)^2 = \\frac{10}{25} = \\frac{2}{5} = \\frac{40}{100}\n$$\n$$\n\\left(\\frac{1+\\sqrt{51}}{10}\\right)^2 = \\frac{1^2 + 2\\sqrt{51} + (\\sqrt{51})^2}{100} = \\frac{1 + 2\\sqrt{51} + 51}{100} = \\frac{52 + 2\\sqrt{51}}{100}\n$$\nTo compare $\\frac{40}{100}$ and $\\frac{52 + 2\\sqrt{51}}{100}$, we compare $40$ and $52 + 2\\sqrt{51}$. Since $\\sqrt{51}0$, it is evident that $52 + 2\\sqrt{51}  52  40$.\nTherefore, $\\left(\\frac{1+\\sqrt{51}}{10}\\right)^2  \\left(\\frac{\\sqrt{10}}{5}\\right)^2$. Since both bases are positive, their magnitudes follow the same inequality: $\\frac{1+\\sqrt{51}}{10}  \\frac{\\sqrt{10}}{5}$.\n\nThe largest modulus among all eigenvalues is $\\frac{1+\\sqrt{51}}{10}$. This is the spectral radius of the companion matrix $F$.\nAll eigenvalues have moduli less than $1$, confirming that the process is covariance-stationary.\nFor instance, $\\sqrt{51}  \\sqrt{81}=9$, so $1+\\sqrt{51}  10$, which means $\\frac{1+\\sqrt{51}}{10}  1$. Also, $\\sqrt{10}  \\sqrt{25}=5$, so $\\frac{\\sqrt{10}}{5}  1$.\n\nThe spectral radius is $\\rho(F) = \\frac{1 + \\sqrt{51}}{10}$.",
            "answer": "$$\n\\boxed{\\frac{1 + \\sqrt{51}}{10}}\n$$"
        },
        {
            "introduction": "A fitted VAR model's coefficient matrices, $A_i$, can be difficult to interpret directly, especially in a multivariate setting. This practice  delves into the more intuitive concept of the Impulse Response Function (IRF), which reveals how the system reacts over time to an external shock. You will learn to derive the IRF matrices, $\\Psi_j$, from the VAR coefficients, translating the model's parameters into a clear narrative of dynamic causal effects between time series.",
            "id": "4203498",
            "problem": "In a neuroscience experiment, simultaneous local field potentials from two interconnected cortical areas are recorded and modeled as a bivariate vector autoregressive model of order $2$ (vector autoregressive (VAR) model of order $2$). Let the $2 \\times 1$ time series be denoted by $\\mathbf{x}_t$, and assume the data-generating process is a stable, linear, time-invariant system given by\n$$ \\mathbf{x}_t = \\sum_{i=1}^{p} A_i \\mathbf{x}_{t-i} + \\boldsymbol{\\varepsilon}_t $$\nwhere $p=2$, $A_i$ are $2 \\times 2$ coefficient matrices, and $\\boldsymbol{\\varepsilon}_t$ is a zero-mean innovation process (white noise) with covariance matrix equal to the $2 \\times 2$ identity matrix. Under stability, the Wold representation exists:\n$$ \\mathbf{x}_t = \\sum_{j=0}^{\\infty} \\Psi_j \\boldsymbol{\\varepsilon}_{t-j} $$\nwhere $\\Psi_j$ are $2 \\times 2$ impulse-response (moving average) coefficient matrices and $\\Psi_0$ equals the $2 \\times 2$ identity matrix.\n\nTask:\n1) Starting from the definitions above and the assumption of stability, derive a general recursion for the matrices $\\Psi_j$ in terms of the autoregressive coefficient matrices $A_i$ and lower-lag $\\Psi_{j-i}$, without using any pre-stated recursion formulas.\n2) Consider a specific model for two cortical areas with\n$$A_1 = \\begin{pmatrix} 0.6  0.1 \\\\ 0.2  0.4 \\end{pmatrix} \\text{ and } A_2 = \\begin{pmatrix} -0.2  0.05 \\\\ 0  0.1 \\end{pmatrix}.$$\nCompute $\\Psi_1$ and $\\Psi_2$ explicitly for this model.\n3) Interpret the $(1,2)$ entry of $\\Psi_2$ as the lag-$2$ impulse response of area $1$ to a unit shock in area $2$. Report this scalar as your final answer. No rounding is necessary. Provide only a single real number with no units as your final answer.",
            "solution": "### Solution\n\nThe problem is divided into three parts. We address each in sequence.\n\n**Part 1: Derivation of the recursion for $\\Psi_j$**\n\nWe are given two representations for the time series $\\mathbf{x}_t$: the VAR($p$) form and its corresponding VMA($\\infty$) or Wold representation. The stability of the VAR process is the condition that guarantees the existence of the latter.\n\nThe VAR($p$) representation is:\n$$ \\mathbf{x}_t = \\sum_{i=1}^{p} A_i \\mathbf{x}_{t-i} + \\boldsymbol{\\varepsilon}_t $$\nThe VMA($\\infty$) representation is:\n$$ \\mathbf{x}_t = \\sum_{j=0}^{\\infty} \\Psi_j \\boldsymbol{\\varepsilon}_{t-j} $$\n_Equation (1)_\n\nwith the initial condition $\\Psi_0 = I$, where $I$ is the identity matrix of appropriate dimension (here, $2 \\times 2$).\n\nTo derive the relationship between the autoregressive coefficients $A_i$ and the impulse-response coefficients $\\Psi_j$, we substitute the VMA($\\infty$) representation of $\\mathbf{x}_t$ into the VAR($p$) equation. For each lagged term $\\mathbf{x}_{t-i}$, we have:\n$$ \\mathbf{x}_{t-i} = \\sum_{j=0}^{\\infty} \\Psi_j \\boldsymbol{\\varepsilon}_{t-i-j} $$\n\nSubstituting this into the VAR equation yields:\n$$ \\sum_{j=0}^{\\infty} \\Psi_j \\boldsymbol{\\varepsilon}_{t-j} = \\sum_{i=1}^{p} A_i \\left( \\sum_{j=0}^{\\infty} \\Psi_j \\boldsymbol{\\varepsilon}_{t-i-j} \\right) + \\boldsymbol{\\varepsilon}_t $$\n\nThe core of the derivation is to equate the matrix coefficients of the innovation terms $\\boldsymbol{\\varepsilon}_{t-k}$ for each lag $k \\ge 0$.\n\nFor $k=0$ (the coefficient of $\\boldsymbol{\\varepsilon}_t$):\nThe left-hand side (LHS) has the term $\\Psi_0 \\boldsymbol{\\varepsilon}_t$. The right-hand side (RHS) has the term $I \\boldsymbol{\\varepsilon}_t$. Equating the coefficients gives:\n$$ \\Psi_0 = I $$\nThis confirms the initial condition provided in the problem statement.\n\nFor $k  0$ (the coefficient of $\\boldsymbol{\\varepsilon}_{t-k}$):\nThe coefficient of $\\boldsymbol{\\varepsilon}_{t-k}$ on the LHS is $\\Psi_k$.\nTo find the coefficient of $\\boldsymbol{\\varepsilon}_{t-k}$ on the RHS, we examine the double summation $\\sum_{i=1}^{p} A_i \\left( \\sum_{j=0}^{\\infty} \\Psi_j \\boldsymbol{\\varepsilon}_{t-i-j} \\right)$. A term $\\boldsymbol{\\varepsilon}_{t-k}$ arises when the time index is $t-k$, i.e., $t-i-j = t-k$, which implies $j = k-i$. Since the index $j$ for $\\Psi_j$ must be non-negative, we must have $k-i \\ge 0$, or $i \\le k$.\nThus, for a given $i$ in the outer sum, the coefficient of $\\boldsymbol{\\varepsilon}_{t-k}$ is $A_i \\Psi_{k-i}$. We sum this over all possible values of $i$, which are $i \\in \\{1, 2, \\dots, p\\}$ and subject to the constraint $i \\le k$. This means we sum $i$ from $1$ up to $\\min(k, p)$.\nEquating the coefficients of $\\boldsymbol{\\varepsilon}_{t-k}$ on both sides for $k  0$:\n$$ \\Psi_k = \\sum_{i=1}^{\\min(k, p)} A_i \\Psi_{k-i} $$\nFor convenience, let's use the index $j$ as in the problem statement for the impulse-response matrices.\n$$ \\Psi_j = \\sum_{i=1}^{\\min(j, p)} A_i \\Psi_{j-i} \\quad \\text{for } j  0 $$\n_Equation (2)_\n\nThis is the general recursion for the impulse-response matrices $\\Psi_j$.\n\n**Part 2: Computation of $\\Psi_1$ and $\\Psi_2$**\n\nWe are given a specific VAR($2$) model, so $p=2$. The coefficient matrices are:\n$$ A_1 = \\begin{pmatrix} 0.6  0.1 \\\\ 0.2  0.4 \\end{pmatrix}, \\quad A_2 = \\begin{pmatrix} -0.2  0.05 \\\\ 0  0.1 \\end{pmatrix} $$\nWe also know $\\Psi_0 = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$.\n\nTo compute $\\Psi_1$, we use the recursion formula (Equation 2) with $j=1$ and $p=2$:\n$$ \\Psi_1 = \\sum_{i=1}^{\\min(1, 2)} A_i \\Psi_{1-i} = A_1 \\Psi_0 $$\nSince $\\Psi_0 = I$, we have:\n$$ \\Psi_1 = A_1 = \\begin{pmatrix} 0.6  0.1 \\\\ 0.2  0.4 \\end{pmatrix} $$\n\nTo compute $\\Psi_2$, we use the recursion with $j=2$ and $p=2$:\n$$ \\Psi_2 = \\sum_{i=1}^{\\min(2, 2)} A_i \\Psi_{2-i} = \\sum_{i=1}^{2} A_i \\Psi_{2-i} = A_1 \\Psi_1 + A_2 \\Psi_0 $$\nSubstituting the known matrices:\n$$ \\Psi_2 = A_1 (A_1) + A_2 (I) = A_1^2 + A_2 $$\nFirst, we compute $A_1^2$:\n$$ A_1^2 = \\begin{pmatrix} 0.6  0.1 \\\\ 0.2  0.4 \\end{pmatrix} \\begin{pmatrix} 0.6  0.1 \\\\ 0.2  0.4 \\end{pmatrix} = \\begin{pmatrix} (0.6)(0.6) + (0.1)(0.2)  (0.6)(0.1) + (0.1)(0.4) \\\\ (0.2)(0.6) + (0.4)(0.2)  (0.2)(0.1) + (0.4)(0.4) \\end{pmatrix} $$\n$$ A_1^2 = \\begin{pmatrix} 0.36 + 0.02  0.06 + 0.04 \\\\ 0.12 + 0.08  0.02 + 0.16 \\end{pmatrix} = \\begin{pmatrix} 0.38  0.10 \\\\ 0.20  0.18 \\end{pmatrix} $$\nNow, we add $A_2$ to find $\\Psi_2$:\n$$ \\Psi_2 = A_1^2 + A_2 = \\begin{pmatrix} 0.38  0.10 \\\\ 0.20  0.18 \\end{pmatrix} + \\begin{pmatrix} -0.2  0.05 \\\\ 0  0.1 \\end{pmatrix} $$\n$$ \\Psi_2 = \\begin{pmatrix} 0.38 - 0.20  0.10 + 0.05 \\\\ 0.20 + 0  0.18 + 0.10 \\end{pmatrix} = \\begin{pmatrix} 0.18  0.15 \\\\ 0.20  0.28 \\end{pmatrix} $$\n\nSo, the impulse response matrices are $\\Psi_1 = \\begin{pmatrix} 0.6  0.1 \\\\ 0.2  0.4 \\end{pmatrix}$ and $\\Psi_2 = \\begin{pmatrix} 0.18  0.15 \\\\ 0.20  0.28 \\end{pmatrix}$.\n\n**Part 3: Report the (1,2) entry of $\\Psi_2$**\n\nThe matrix $\\Psi_2$ is $\\begin{pmatrix} 0.18  0.15 \\\\ 0.20  0.28 \\end{pmatrix}$. The entry in the first row and second column, denoted $[\\Psi_2]_{12}$, is $0.15$.\n\nThis value represents the response of the first variable (activity in cortical area $1$) at time $t$ to a unit impulse shock to the second variable (activity in cortical area $2$) that occurred two time steps prior, at time $t-2$. This is precisely the \"lag-$2$ impulse response of area $1$ to a unit shock in area $2$\".\nThe final required scalar value is $0.15$.",
            "answer": "$$\n\\boxed{0.15}\n$$"
        },
        {
            "introduction": "Choosing the correct lag order, $p$, is one of the most critical steps in building a VAR model, as it dictates the model's complexity. This exercise  presents a realistic scenario where you must balance model fit against the risk of overfitting. By calculating and comparing the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), you will practice making a principled, data-driven decision on model order, a fundamental skill in applied time series analysis.",
            "id": "4203478",
            "problem": "A laboratory records simultaneous local field potential (LFP) activity from $K=5$ cortical sites during a cognitive task, yielding $T=1200$ time points after preprocessing. To model directed interactions and spectral dependencies, the team fits a $K$-variate Gaussian vector autoregressive (VAR) model of order $p$, for $p \\in \\{1,2,3,4,5\\}$, using maximum likelihood. For each $p$, the maximized log-likelihood $\\ell_{p}$ (evaluated on the same fixed dataset) is reported as follows:\n- $p=1$: $\\ell_{1} = -15500.0$,\n- $p=2$: $\\ell_{2} = -15080.0$,\n- $p=3$: $\\ell_{3} = -14940.0$,\n- $p=4$: $\\ell_{4} = -14890.0$,\n- $p=5$: $\\ell_{5} = -14870.0$.\n\nAssume a full (unconstrained) noise covariance and an intercept are included in all models. Treat the number of free parameters as the exact count of distinct quantities estimated in a $K$-variate VAR($p$) with full noise covariance, and for the Bayesian Information Criterion (BIC) penalty use the effective sample size $n_{p} = T - p$.\n\nUsing the standard definitions of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) for multivariate Gaussian models, compute the AIC and BIC for each $p \\in \\{1,2,3,4,5\\}$, identify which $p$ each criterion favors, and then select a single order $p$ for subsequent neuroscientific analysis of interactions in these LFPs, explicitly articulating the trade-offs you are making between bias, variance, and interpretability in this context.\n\nReport as your final answer the single integer $p$ you choose. No rounding instruction is needed for the final answer.",
            "solution": "The problem requires the selection of an optimal model order $p$ for a vector autoregressive (VAR) model applied to neuroscience data. This selection will be based on the Akaike Information Criterion ($\\text{AIC}$) and the Bayesian Information Criterion ($\\text{BIC}$), followed by a reasoned judgment that considers the trade-offs between model complexity and scientific interpretability.\n\nFirst, we must determine the number of free parameters, $m_p$, in a $K$-variate $\\text{VAR}(p)$ model. The model is given by:\n$$\n\\mathbf{y}_t = \\mathbf{c} + \\sum_{i=1}^{p} \\mathbf{A}_i \\mathbf{y}_{t-i} + \\boldsymbol{\\epsilon}_t\n$$\nwhere $\\mathbf{y}_t$ is the $K \\times 1$ vector of observations at time $t$, $\\mathbf{c}$ is a $K \\times 1$ intercept vector, $\\mathbf{A}_i$ are the $K \\times K$ coefficient matrices for lag $i$, and $\\boldsymbol{\\epsilon}_t$ is a $K \\times 1$ white noise vector with covariance matrix $\\mathbf{\\Sigma}$.\n\nThe number of free parameters $m_p$ is the sum of the parameters from each component of the model as specified:\n1.  The intercept vector $\\mathbf{c}$ contains $K$ parameters.\n2.  There are $p$ coefficient matrices $\\mathbf{A}_i$, each of size $K \\times K$. This contributes $p \\times K^2$ parameters.\n3.  The noise covariance matrix $\\mathbf{\\Sigma}$ is a $K \\times K$ symmetric matrix. The number of unique parameters in a symmetric matrix is $\\frac{K(K+1)}{2}$.\n\nTherefore, the total number of free parameters $m_p$ is:\n$$\nm_p = K + p K^2 + \\frac{K(K+1)}{2}\n$$\nGiven $K=5$, we can compute the terms:\n- Intercepts: $K=5$ parameters.\n- Covariance matrix: $\\frac{5(5+1)}{2} = \\frac{30}{2} = 15$ parameters.\n- Autoregressive coefficients: $p K^2 = p \\times 5^2 = 25p$ parameters.\n\nSo, for $K=5$, the number of parameters is:\n$$\nm_p = 5 + 25p + 15 = 20 + 25p\n$$\nWe can now calculate $m_p$ for each candidate order $p \\in \\{1, 2, 3, 4, 5\\}$:\n- For $p=1$: $m_1 = 20 + 25(1) = 45$.\n- For $p=2$: $m_2 = 20 + 25(2) = 70$.\n- For $p=3$: $m_3 = 20 + 25(3) = 95$.\n- For $p=4$: $m_4 = 20 + 25(4) = 120$.\n- For $p=5$: $m_5 = 20 + 25(5) = 145$.\n\nNext, we define the $\\text{AIC}$ and $\\text{BIC}$. Given the maximized log-likelihood $\\ell_p$, the number of parameters $m_p$, and the number of observations $n$, the criteria are:\n$$\n\\text{AIC}_p = -2 \\ell_p + 2 m_p\n$$\n$$\n\\text{BIC}_p = -2 \\ell_p + m_p \\ln(n)\n$$\nThe problem specifies using an effective sample size of $n_p = T - p$ for the $\\text{BIC}$, where $T=1200$. We now compute the $\\text{AIC}$ and $\\text{BIC}$ for each model order.\n\nFor $p=1$:\n$\\ell_1 = -15500.0$, $m_1 = 45$, $n_1 = 1200 - 1 = 1199$.\n$\\text{AIC}_1 = -2(-15500.0) + 2(45) = 31000 + 90 = 31090.0$.\n$\\text{BIC}_1 = -2(-15500.0) + 45 \\ln(1199) \\approx 31000 + 45(7.08918) \\approx 31319.01$.\n\nFor $p=2$:\n$\\ell_2 = -15080.0$, $m_2 = 70$, $n_2 = 1200 - 2 = 1198$.\n$\\text{AIC}_2 = -2(-15080.0) + 2(70) = 30160 + 140 = 30300.0$.\n$\\text{BIC}_2 = -2(-15080.0) + 70 \\ln(1198) \\approx 30160 + 70(7.08835) \\approx 30656.18$.\n\nFor $p=3$:\n$\\ell_3 = -14940.0$, $m_3 = 95$, $n_3 = 1200 - 3 = 1197$.\n$\\text{AIC}_3 = -2(-14940.0) + 2(95) = 29880 + 190 = 30070.0$.\n$\\text{BIC}_3 = -2(-14940.0) + 95 \\ln(1197) \\approx 29880 + 95(7.08752) \\approx 30553.31$.\n\nFor $p=4$:\n$\\ell_4 = -14890.0$, $m_4 = 120$, $n_4 = 1200 - 4 = 1196$.\n$\\text{AIC}_4 = -2(-14890.0) + 2(120) = 29780 + 240 = 30020.0$.\n$\\text{BIC}_4 = -2(-14890.0) + 120 \\ln(1196) \\approx 29780 + 120(7.08668) \\approx 30630.40$.\n\nFor $p=5$:\n$\\ell_5 = -14870.0$, $m_5 = 145$, $n_5 = 1200 - 5 = 1195$.\n$\\text{AIC}_5 = -2(-14870.0) + 2(145) = 29740 + 290 = 30030.0$.\n$\\text{BIC}_5 = -2(-14870.0) + 145 \\ln(1195) \\approx 29740 + 145(7.08585) \\approx 30767.45$.\n\nWe seek the model order $p$ that minimizes each criterion.\n- The $\\text{AIC}$ values are: $31090.0$, $30300.0$, $30070.0$, $30020.0$, $30030.0$. The minimum $\\text{AIC}$ is $30020.0$, which favors $p=4$.\n- The $\\text{BIC}$ values are approximately: $31319.01$, $30656.18$, $30553.31$, $30630.40$, $30767.45$. The minimum $\\text{BIC}$ is approximately $30553.31$, which favors $p=3$.\n\nThe two criteria suggest different model orders. We must now make a final selection based on theoretical properties and the scientific context.\nThe penalty term for $\\text{AIC}$ is $2m_p$, whereas for $\\text{BIC}$ it is $m_p\\ln(n_p)$. For this problem, $\\ln(n_p) \\approx \\ln(1200) \\approx 7.09$, which is substantially larger than $2$. Thus, $\\text{BIC}$ imposes a much stronger penalty on model complexity (number of parameters) than $\\text{AIC}$.\n- $\\text{AIC}$ is known to be asymptotically efficient, meaning it excels at selecting models that minimize prediction error. However, it is not consistent; it has a non-zero probability of selecting a model more complex than the true model, even with an infinite amount of data. This tendency towards overfitting can be problematic.\n- $\\text{BIC}$ is consistent. Under the assumption that the true data-generating process is among the candidate models, $\\text{BIC}$ will select the true model order with probability approaching $1$ as the sample size increases. It favors parsimony more strongly.\n\nIn the context of neuroscience data analysis, the goal is often to understand the underlying structure of interactions, which requires an interpretable and robust model. Overfitting, as potentially encouraged by $\\text{AIC}$, is a significant concern because it can lead to the identification of spurious connections or dynamics that are mere artifacts of sampling noise. This would undermine the scientific validity of the conclusions.\nThe $\\text{BIC}$-selected model, $\\text{VAR}(3)$, is more parsimonious than the $\\text{AIC}$-selected model, $\\text{VAR}(4)$. While the $\\text{VAR}(4)$ model provides a slightly better fit to the data (a lower value of $-2\\ell_p$), the $\\text{BIC}$ suggests this improvement is not substantial enough to justify the addition of $m_4 - m_3 = 120-95=25$ extra parameters. By choosing $p=3$, we adhere to the principle of parsimony (Occam's razor), selecting the simplest model that provides a reasonable explanation of the data. This choice reduces the risk of overfitting and is more likely to yield a model whose estimated parameters are stable and scientifically interpretable. Therefore, for the purpose of analyzing interactions in these LFP signals, the more conservative choice of $p=3$ is superior.",
            "answer": "$$\n\\boxed{3}\n$$"
        }
    ]
}