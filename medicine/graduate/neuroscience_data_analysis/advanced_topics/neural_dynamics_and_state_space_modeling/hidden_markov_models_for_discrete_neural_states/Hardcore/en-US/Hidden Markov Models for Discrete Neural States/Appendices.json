{
    "hands_on_practices": [
        {
            "introduction": "This first exercise grounds your understanding in the cornerstone of Hidden Markov Model analysis: the Viterbi algorithm. You will implement this dynamic programming approach to find the single most probable sequence of latent neural states given a series of spike counts. This practice goes a step further by connecting the decoded path back to the model's parameters, asking you to compare the empirical state durations with their theoretical expectation, reinforcing the link between a model's structure and the dynamics it can generate .",
            "id": "4168489",
            "problem": "You are given a set of parameters defining a Hidden Markov Model (HMM) for discrete neural states and sequences of binned spike counts. A Hidden Markov Model (HMM) consists of a discrete latent state process that is a first-order Markov chain with an initial distribution and a transition matrix, coupled to an observation process whose distribution depends on the current latent state. In neuroscience data analysis, spike counts aggregated in fixed-width bins are commonly modeled using a Poisson distribution conditioned on the latent neural state. The objective is to compute the most probable latent sequence given the observed spike counts using the Viterbi algorithm, then assess the empirical state durations against the durations implied by the Markov property.\n\nStarting from core definitions:\n- The latent sequence has states from the finite set $\\{0,1,\\dots,S-1\\}$, where $S$ denotes the number of states.\n- The initial state distribution is denoted by the vector $\\boldsymbol{\\pi}$ with entries $\\pi_i$ for $i \\in \\{0,1,\\dots,S-1\\}$, where $\\pi_i$ represents the probability of starting in state $i$.\n- The state transition probabilities are represented by a matrix $\\mathbf{A}$ with entries $a_{ij}$, defined as the probability of transitioning from state $i$ to state $j$ in one time step, with $i,j \\in \\{0,1,\\dots,S-1\\}$.\n- Observations at time $t$, denoted $o_t$, are spike counts modeled as a Poisson random variable conditioned on the state $z_t$: $o_t \\sim \\mathrm{Poisson}(\\lambda_{z_t})$, where $\\lambda_i$ is the Poisson rate parameter associated with state $i$. All spike counts $o_t$ are nonnegative integers.\n\nUnder the Markov property and conditional independence assumptions inherent to the HMM, the most probable latent sequence $z_{0:T-1}$ given observations $o_{0:T-1}$ can be obtained using dynamic programming (Viterbi algorithm), which maximizes the joint log-probability over paths. State durations in discrete-time Markov chains are geometrically distributed due to the memoryless property: the number of consecutive time steps $D_i$ spent in state $i$ before transitioning to a different state has a geometric distribution on the support $\\{1,2,3,\\dots\\}$ with success parameter $q_i = 1 - a_{ii}$, implying that the expected duration is $\\mathbb{E}[D_i] = \\frac{1}{q_i} = \\frac{1}{1 - a_{ii}}$.\n\nYour task is to implement a program that:\n1. For each provided test case, computes the Viterbi path $z_{0:T-1}$ using the given $\\boldsymbol{\\pi}$, $\\mathbf{A}$, and $\\boldsymbol{\\lambda}$ and the provided observation sequence $o_{0:T-1}$. Use log-probabilities to avoid numerical underflow. In the case of ties between candidate predecessors or states, break ties deterministically by choosing the smallest state index.\n2. Parses the Viterbi path to obtain empirical run-lengths for each state: contiguous segments of identical states yield durations measured in time steps. For each state $i$, compute the empirical mean of its run-lengths. If a state $i$ does not appear in the Viterbi path, define its empirical mean duration to be $0$ by convention.\n3. For each state $i$, compute the expected mean duration using $\\mathbb{E}[D_i] = \\frac{1}{1 - a_{ii}}$. Then compute the absolute difference between the empirical mean and the expected mean for each state.\n4. Round each absolute difference to six decimal places.\n5. Aggregate results across all test cases into a single output as specified.\n\nAll quantities are dimensionless counts and probabilities, so no physical units are required. Angles are not used. The final output must be a single line representing a list of lists of floats, consistent across test cases. The line must be exactly formatted as described below.\n\nTest Suite:\n- Test Case $1$:\n  - Number of states $S = 3$.\n  - Initial distribution $\\boldsymbol{\\pi} = [\\,0.6,\\,0.3,\\,0.1\\,]$.\n  - Transition matrix $\\mathbf{A} = \\begin{bmatrix} 0.90 & 0.08 & 0.02 \\\\ 0.05 & 0.92 & 0.03 \\\\ 0.04 & 0.06 & 0.90 \\end{bmatrix}$.\n  - Poisson rates $\\boldsymbol{\\lambda} = [\\,2.0,\\,8.0,\\,15.0\\,]$.\n  - Observation sequence $o_{0:25} = [\\,2,\\,1,\\,3,\\,2,\\,2,\\,9,\\,8,\\,7,\\,10,\\,15,\\,14,\\,16,\\,17,\\,13,\\,12,\\,1,\\,2,\\,3,\\,8,\\,9,\\,8,\\,12,\\,18,\\,15,\\,14,\\,16\\,]$.\n- Test Case $2$:\n  - Number of states $S = 3$.\n  - Initial distribution $\\boldsymbol{\\pi} = [\\,0.33,\\,0.33,\\,0.34\\,]$.\n  - Transition matrix $\\mathbf{A} = \\begin{bmatrix} 0.97 & 0.02 & 0.01 \\\\ 0.01 & 0.97 & 0.02 \\\\ 0.02 & 0.01 & 0.97 \\end{bmatrix}$.\n  - Poisson rates $\\boldsymbol{\\lambda} = [\\,3.0,\\,9.0,\\,14.0\\,]$.\n  - Observation sequence $o_{0:44} = [\\,14,\\,15,\\,13,\\,16,\\,14,\\,14,\\,15,\\,13,\\,17,\\,12,\\,14,\\,16,\\,15,\\,13,\\,14,\\,15,\\,16,\\,13,\\,15,\\,14,\\,9,\\,10,\\,8,\\,9,\\,10,\\,7,\\,9,\\,8,\\,9,\\,11,\\,9,\\,8,\\,10,\\,9,\\,9,\\,3,\\,2,\\,4,\\,3,\\,3,\\,2,\\,4,\\,3,\\,2,\\,3\\,]$.\n- Test Case $3$:\n  - Number of states $S = 2$.\n  - Initial distribution $\\boldsymbol{\\pi} = [\\,0.5,\\,0.5\\,]$.\n  - Transition matrix $\\mathbf{A} = \\begin{bmatrix} 0.60 & 0.40 \\\\ 0.40 & 0.60 \\end{bmatrix}$.\n  - Poisson rates $\\boldsymbol{\\lambda} = [\\,5.0,\\,5.0\\,]$.\n  - Observation sequence $o_{0:11} = [\\,4,\\,6,\\,5,\\,7,\\,5,\\,5,\\,4,\\,6,\\,5,\\,5,\\,5,\\,6\\,]$.\n\nProgram Requirements:\n- Implement the Viterbi algorithm in the log domain for numerical stability.\n- Compute empirical mean durations from the decoded path and absolute differences to $\\frac{1}{1-a_{ii}}$ for each state $i$.\n- Round differences to six decimal places.\n- Deterministic tie-breaking: when two or more predecessor states yield equal scores (within floating-point equality) for the same current state and time, select the smallest index predecessor; similarly, if two states tie at initialization or final backtracking comparison, select the smallest index.\n- Final Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a sublist of comma-separated rounded floats per test case. For example, the output must look exactly like $\\texttt{[[0.123456,0.000001,2.500000],[0.100000,0.200000,0.300000],[3.141593,2.718282]]}$.\n\nYour program must be self-contained and must not read input from the user. It must use the specified runtime environment and produce deterministic output. The observations are provided as exact integer counts per bin; do not infer any physical units beyond dimensionless counts.",
            "solution": "The user has provided a problem that requires the application of a Hidden Markov Model (HMM) to analyze simulated neural spike count data. The core tasks are to determine the most probable sequence of latent neural states given a sequence of observations using the Viterbi algorithm, and then to compare the empirical state durations from this sequence with their theoretical expected values.\n\n### Step 1: Problem Validation\n\nThe problem statement is critically evaluated against the required criteria.\n\n**1. Extraction of Givens:**\nThe problem provides the following quantities and models for three distinct test cases:\n- A finite set of latent states $\\{0, 1, \\dots, S-1\\}$, where $S$ is the number of states.\n- An initial state distribution vector $\\boldsymbol{\\pi}$, where $\\pi_i = P(z_0=i)$.\n- A state transition probability matrix $\\mathbf{A}$, where $a_{ij} = P(z_t=j | z_{t-1}=i)$.\n- An observation model where spike counts $o_t$ are Poisson distributed conditioned on the latent state $z_t$: $p(o_t | z_t=i) = \\mathrm{Poisson}(o_t; \\lambda_i)$. The Poisson rate parameters are given as a vector $\\boldsymbol{\\lambda}$.\n- A sequence of observations $o_{0:T-1}$ representing binned spike counts.\n- A formula for the expected duration of a state: $\\mathbb{E}[D_i] = \\frac{1}{1-a_{ii}}$.\n- A deterministic tie-breaking rule: choose the smallest state index in cases of equal probability.\n- Specific parameter sets ($\\boldsymbol{\\pi}$, $\\mathbf{A}$, $\\boldsymbol{\\lambda}$) and observation sequences for three test cases.\n\n**2. Validation using Extracted Givens:**\n- **Scientifically Grounded:** The problem is firmly rooted in standard practices of computational neuroscience and statistical modeling. The use of HMMs with Poisson observations is a well-established method for analyzing spike train data. The Viterbi algorithm is the correct and standard dynamic programming approach for finding the most likely state sequence (the Viterbi path). The formula for the expected state duration in a discrete-time Markov chain is a fundamental result from probability theory. All premises are scientifically and mathematically sound.\n- **Well-Posed:** The problem is well-posed. All necessary parameters ($S, \\boldsymbol{\\pi}, \\mathbf{A}, \\boldsymbol{\\lambda}$) and data ($o_{0:T-1}$) are provided for each test case. The objective is clearly defined: compute the absolute difference between empirical and expected state durations. The explicit tie-breaking rule ensures that the Viterbi algorithm yields a unique output path, making the entire computation deterministic and reproducible.\n- **Objective:** The problem is stated in precise, objective language. There are no subjective claims or ambiguities. The task is purely computational and analytical.\n- **Completeness and Consistency:** The inputs for each test case are complete. The provided probability distributions ($\\boldsymbol{\\pi}$) and transition matrices ($\\mathbf{A}$) are valid (i.e., entries are non-negative and rows/elements sum to $1$). The Poisson rates ($\\boldsymbol{\\lambda}$) are positive, and the observations are non-negative integers as required. There are no internal contradictions.\n- **Conclusion:** The problem statement is valid. It is scientifically sound, well-posed, objective, and self-contained.\n\n### Step 2: Algorithmic and Scientific Principles\n\nThe solution involves three main components: decoding the latent states, calculating empirical statistics from the decoded path, and comparing these with theoretical expectations derived from the model parameters.\n\n**1. Viterbi Algorithm for Path Decoding:**\nThe objective is to find the state sequence $z^*_{0:T-1} = (z^*_0, z^*_1, \\dots, z^*_{T-1})$ that maximizes the joint probability $P(z_{0:T-1}, o_{0:T-1})$. To prevent numerical underflow with long sequences, we work with log-probabilities. The Viterbi algorithm efficiently solves this maximization problem using dynamic programming.\n\nLet $\\delta_t(i)$ be the maximum log-probability of any state sequence ending in state $i$ at time $t$, given the observations up to time $t$.\n$$\n\\delta_t(i) = \\max_{z_{0:t-1}} \\log P(z_{0:t-1}, z_t=i, o_{0:t})\n$$\nThe algorithm proceeds as follows:\n\n- **Initialization ($t=0$):**\nFor each state $i \\in \\{0, \\dots, S-1\\}$, the initial log-probability is the sum of the log-initial-probability and the log-emission-probability for the first observation $o_0$.\n$$\n\\delta_0(i) = \\log \\pi_i + \\log p(o_0 | z_0=i)\n$$\nThe log-probability for a Poisson emission is $\\log p(o_t | z_t=i) = o_t \\log \\lambda_i - \\lambda_i - \\log(o_t!)$. The term $\\log(o_t!)$ is constant for all states $i$ at a given time $t$ and can be omitted from the maximization steps, as it does not affect the outcome of any comparison.\n\n- **Recursion ($t=1, \\dots, T-1$):**\nFor each state $j \\in \\{0, \\dots, S-1\\}$, we find the most probable path leading to it by considering all possible predecessor states $i$ at time $t-1$.\n$$\n\\delta_t(j) = \\left( \\max_{i \\in \\{0, \\dots, S-1\\}} (\\delta_{t-1}(i) + \\log a_{ij}) \\right) + \\log p(o_t | z_t=j)\n$$\nA backpointer table, $\\psi_t(j)$, stores the predecessor state $i$ that achieved this maximum for each $j$.\n$$\n\\psi_t(j) = \\arg\\max_{i \\in \\{0, \\dots, S-1\\}} (\\delta_{t-1}(i) + \\log a_{ij})\n$$\nThe specified tie-breaking rule (smallest index) is applied here.\n\n- **Termination and Path Backtracking:**\nThe most probable final state is found by maximizing $\\delta_{T-1}(i)$ over all states $i$.\n$$\nz^*_{T-1} = \\arg\\max_{i \\in \\{0, \\dots, S-1\\}} \\delta_{T-1}(i)\n$$\nThe rest of the path is recovered by backtracking from $t=T-2$ down to $0$ using the stored pointers:\n$$\nz^*_t = \\psi_{t+1}(z^*_{t+1})\n$$\n\n**2. Empirical and Theoretical State Durations:**\n- **Empirical Duration:** After obtaining the Viterbi path $z^*_{0:T-1}$, we parse it to identify contiguous segments of identical states. For each state $i$, we collect the lengths of all such segments (run-lengths). The empirical mean duration for state $i$ is the arithmetic average of these run-lengths. If a state does not appear in the path, its empirical mean duration is defined as $0$.\n\n- **Theoretical Duration:** The latent state process is a discrete-time Markov chain. The memoryless property of the Markov chain implies that the duration of a stay in any state $i$ (i.e., the number of consecutive time steps spent in $i$ before transitioning out) follows a geometric distribution. The probability of self-transition is $a_{ii}$, so the probability of leaving the state is $q_i = 1 - a_{ii}$. The expected number of steps until this \"leave\" event occurs, including the first step, is given by:\n$$\n\\mathbb{E}[D_i] = \\frac{1}{q_i} = \\frac{1}{1 - a_{ii}}\n$$\n\n**3. Final Calculation:**\nFor each state $i \\in \\{0, \\dots, S-1\\}$, we compute the absolute difference between the empirical mean duration and the theoretical expected duration. This difference is then rounded to six decimal places as required. The results for all states within a test case form a list, and the lists for all test cases are aggregated into a final list of lists.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef viterbi(obs, S, pi, A, lmbdas):\n    \"\"\"\n    Computes the most probable latent state sequence using the Viterbi algorithm.\n    This implementation operates in the log domain for numerical stability and adheres\n    to the problem's tie-breaking rule (smallest index).\n\n    Args:\n        obs (np.array): Sequence of observations.\n        S (int): Number of states.\n        pi (np.array): Initial state distribution.\n        A (np.array): State transition matrix.\n        lmbdas (np.array): Poisson rate parameters for each state.\n\n    Returns:\n        np.array: The most probable sequence of latent states (Viterbi path).\n    \"\"\"\n    T = len(obs)\n    delta = np.zeros((T, S))\n    psi = np.zeros((T, S), dtype=int)\n\n    with np.errstate(divide='ignore'):\n        log_pi = np.log(pi)\n        log_A = np.log(A)\n        log_lmbdas = np.log(lmbdas)\n\n    # Initialization (t=0)\n    for i in range(S):\n        # Poisson log-probability without the constant log(o_t!) term\n        log_p_emission = obs[0] * log_lmbdas[i] - lmbdas[i]\n        delta[0, i] = log_pi[i] + log_p_emission\n\n    # Recursion (t=1 to T-1)\n    for t in range(1, T):\n        for j in range(S):\n            # Log-probabilities of paths ending in state j at time t\n            trans_log_probs = delta[t-1, :] + log_A[:, j]\n            \n            # The smallest-index tie-breaking rule is naturally handled by np.argmax\n            argmax_state = np.argmax(trans_log_probs)\n            max_log_prob = trans_log_probs[argmax_state]\n            \n            log_p_emission = obs[t] * log_lmbdas[j] - lmbdas[j]\n            \n            delta[t, j] = max_log_prob + log_p_emission\n            psi[t, j] = argmax_state\n\n    # Path backtracking\n    path = np.zeros(T, dtype=int)\n    # Tie-breaking for the final state is also handled by np.argmax\n    path[T-1] = np.argmax(delta[T-1, :])\n    \n    for t in range(T-2, -1, -1):\n        path[t] = psi[t+1, path[t+1]]\n        \n    return path\n\ndef get_run_lengths(path, S):\n    \"\"\"\n    Parses a sequence to find contiguous runs of each state and groups them.\n\n    Args:\n        path (np.array): A sequence of states.\n        S (int): The total number of possible states.\n\n    Returns:\n        dict: A dictionary mapping each state index to a list of its run-lengths.\n    \"\"\"\n    run_lengths = {i: [] for i in range(S)}\n    if len(path) == 0:\n        return run_lengths\n\n    current_state = path[0]\n    current_length = 1\n    for i in range(1, len(path)):\n        if path[i] == current_state:\n            current_length += 1\n        else:\n            run_lengths[current_state].append(current_length)\n            current_state = path[i]\n            current_length = 1\n    \n    # Append the last run\n    run_lengths[current_state].append(current_length)\n    return run_lengths\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and produce the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"S\": 3,\n            \"pi\": [0.6, 0.3, 0.1],\n            \"A\": [[0.90, 0.08, 0.02], [0.05, 0.92, 0.03], [0.04, 0.06, 0.90]],\n            \"lmbdas\": [2.0, 8.0, 15.0],\n            \"obs\": [2, 1, 3, 2, 2, 9, 8, 7, 10, 15, 14, 16, 17, 13, 12, 1, 2, 3, 8, 9, 8, 12, 18, 15, 14, 16],\n        },\n        {\n            \"S\": 3,\n            \"pi\": [0.33, 0.33, 0.34],\n            \"A\": [[0.97, 0.02, 0.01], [0.01, 0.97, 0.02], [0.02, 0.01, 0.97]],\n            \"lmbdas\": [3.0, 9.0, 14.0],\n            \"obs\": [14, 15, 13, 16, 14, 14, 15, 13, 17, 12, 14, 16, 15, 13, 14, 15, 16, 13, 15, 14, 9, 10, 8, 9, 10, 7, 9, 8, 9, 11, 9, 8, 10, 9, 9, 3, 2, 4, 3, 3, 2, 4, 3, 2, 3],\n        },\n        {\n            \"S\": 2,\n            \"pi\": [0.5, 0.5],\n            \"A\": [[0.60, 0.40], [0.40, 0.60]],\n            \"lmbdas\": [5.0, 5.0],\n            \"obs\": [4, 6, 5, 7, 5, 5, 4, 6, 5, 5, 5, 6],\n        },\n    ]\n\n    all_case_results = []\n    for case in test_cases:\n        S = case[\"S\"]\n        pi = np.array(case[\"pi\"])\n        A = np.array(case[\"A\"])\n        lmbdas = np.array(case[\"lmbdas\"])\n        obs = np.array(case[\"obs\"])\n\n        viterbi_path = viterbi(obs, S, pi, A, lmbdas)\n        \n        # Calculate empirical mean durations\n        run_lengths = get_run_lengths(viterbi_path, S)\n        empirical_means = np.zeros(S)\n        for i in range(S):\n            if run_lengths[i]:\n                empirical_means[i] = np.mean(run_lengths[i])\n            else:\n                # Per problem spec, mean duration is 0 if state does not appear\n                empirical_means[i] = 0.0\n\n        # Calculate expected mean durations from the Markov property\n        expected_means = 1.0 / (1.0 - np.diag(A))\n\n        # Compute absolute differences and round to six decimal places\n        abs_diffs = np.abs(empirical_means - expected_means)\n        rounded_diffs = [round(d, 6) for d in abs_diffs]\n        \n        all_case_results.append(rounded_diffs)\n    \n    # Format the final output string as a list of lists with no spaces\n    sublist_strings = [f\"[{','.join(map(str, res))}]\" for res in all_case_results]\n    final_output = f\"[{','.join(sublist_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Standard HMMs assume all state transitions are possible, but often we have prior knowledge about the process we are modeling. This practice explores how to tailor the HMM framework for sequentially structured neural data, such as activity during a multi-stage behavioral task. You will modify the Viterbi algorithm to enforce a \"left-to-right\" constraint, where the system can only stay in the current state or advance to the next, providing a powerful way to embed domain knowledge directly into the model's structure .",
            "id": "4168504",
            "problem": "You are given a discrete Hidden Markov Model (HMM) applied to task-structured neural data, where hidden neural states represent latent task phases and the observations are discretized neural features (for example, binned spike-count categories). A Hidden Markov Model (HMM) consists of a hidden state process $\\{z_t\\}_{t=0}^{T-1}$ with $z_t \\in \\{0,1,\\dots,N-1\\}$ and an observation process $\\{O_t\\}_{t=0}^{T-1}$ with $O_t \\in \\{0,1,\\dots,M-1\\}$. The HMM is specified by an initial state distribution $\\boldsymbol{\\pi}$, a state transition matrix $\\mathbf{A}$, and an emission probability matrix $\\mathbf{B}$. The fundamental base is the Markov property (conditionally independent transitions), the independence of emissions given the current state, and the product rule for probabilities. You must incorporate a left-to-right topology constraint appropriate for task-structured neural sequences: at each time step, transitions are only allowed from state $i$ to state $j$ if $j \\in \\{i, i+1\\}$, with the last state $N-1$ allowed to self-transition only. The goal is to derive and implement a Viterbi dynamic programming algorithm under this constraint in the log domain to compute the most probable state path.\n\nDefinitions:\n- The initial distribution is $\\boldsymbol{\\pi} = (\\pi_0, \\pi_1, \\dots, \\pi_{N-1})$, where $\\pi_i = \\mathbb{P}(z_0 = i)$.\n- The transition matrix is $\\mathbf{A}$ with entries $A_{ij} = \\mathbb{P}(z_t = j \\mid z_{t-1} = i)$.\n- The emission matrix is $\\mathbf{B}$ with entries $B_{j,k} = \\mathbb{P}(O_t = k \\mid z_t = j)$.\n- The observation sequence is $(O_0, O_1, \\dots, O_{T-1})$.\n- Use $0$-based indexing for states and observation symbols in both the math and the code.\n\nRequirements:\n1. Derive the constrained Viterbi recursion from first principles using the product rule and the Markov property, and implement it in the log domain. Use $-\\infty$ to represent $\\log(0)$.\n2. Enforce the left-to-right constraint by restricting predecessor states of $j$ at time $t$ to the set $\\{j-1, j\\}$, with $\\{0\\}$ for $j=0$. All disallowed transitions must be excluded from the maximization.\n3. Implement a deterministic tie-breaking rule: whenever $\\arg\\max$ is not unique, choose the smallest state index among the maximizers (this yields the lexicographically smallest most probable path).\n4. Return, for each test case, the most probable state path as a list of integers using $0$-based state indices.\n\nTest Suite:\nImplement your program to process the following four parameter sets. All probabilities are real, valid, and normalized per definition.\n\n- Test Case $1$ (happy path, three states, four symbols):\n    - $N = 3$, $M = 4$, $T = 6$.\n    - $\\boldsymbol{\\pi} = [\\, $0.9$, $0.1$, $0.0$ \\,]$.\n    - $\\mathbf{A} =$ \n      $\\begin{bmatrix}\n      $0.7$ & $0.3$ & $0.0$ \\\\\n      $0.0$ & $0.6$ & $0.4$ \\\\\n      $0.0$ & $0.0$ & $1.0$\n      \\end{bmatrix}$.\n    - $\\mathbf{B} =$ \n      $\\begin{bmatrix}\n      $0.6$ & $0.3$ & $0.05$ & $0.05$ \\\\\n      $0.1$ & $0.6$ & $0.2$ & $0.1$ \\\\\n      $0.05$ & $0.15$ & $0.7$ & $0.1$\n      \\end{bmatrix}$.\n    - Observations: $(\\, $0$, $1$, $1$, $2$, $2$, $3$ \\,)$.\n\n- Test Case $2$ (boundary handling with near-zero emissions and absorbing last state):\n    - $N = 4$, $M = 3$, $T = 5$.\n    - $\\boldsymbol{\\pi} = [\\, $1.0$, $0.0$, $0.0$, $0.0$ \\,]$.\n    - $\\mathbf{A} =$ \n      $\\begin{bmatrix}\n      $0.5$ & $0.5$ & $0.0$ & $0.0$ \\\\\n      $0.0$ & $0.5$ & $0.5$ & $0.0$ \\\\\n      $0.0$ & $0.0$ & $0.5$ & $0.5$ \\\\\n      $0.0$ & $0.0$ & $0.0$ & $1.0$\n      \\end{bmatrix}$.\n    - $\\mathbf{B} =$ \n      $\\begin{bmatrix}\n      $0.9$ & $0.1$ & $1\\times 10^{-12}$ \\\\\n      $0.1$ & $0.8$ & $0.1$ \\\\\n      $1\\times 10^{-12}$ & $0.2$ & $0.8$ \\\\\n      $0.05$ & $0.05$ & $0.9$\n      \\end{bmatrix}$.\n    - Observations: $(\\, $0$, $1$, $2$, $2$, $2$ \\,)$.\n\n- Test Case $3$ (edge case with ties; two states, two symbols; enforce lexicographic tie-breaking):\n    - $N = 2$, $M = 2$, $T = 3$.\n    - $\\boldsymbol{\\pi} = [\\, $0.5$, $0.5$ \\,]$.\n    - $\\mathbf{A} =$ \n      $\\begin{bmatrix}\n      $0.5$ & $0.5$ \\\\\n      $0.0$ & $1.0$\n      \\end{bmatrix}$.\n    - $\\mathbf{B} =$ \n      $\\begin{bmatrix}\n      $0.6$ & $0.4$ \\\\\n      $0.6$ & $0.4$\n      \\end{bmatrix}$.\n    - Observations: $(\\, $0$, $0$, $1$ \\,)$.\n\n- Test Case $4$ (irreversible progression consistent with task phases; last state absorbing):\n    - $N = 3$, $M = 3$, $T = 8$.\n    - $\\boldsymbol{\\pi} = [\\, $0.2$, $0.8$, $0.0$ \\,]$.\n    - $\\mathbf{A} =$ \n      $\\begin{bmatrix}\n      $0.4$ & $0.6$ & $0.0$ \\\\\n      $0.0$ & $0.7$ & $0.3$ \\\\\n      $0.0$ & $0.0$ & $1.0$\n      \\end{bmatrix}$.\n    - $\\mathbf{B} =$ \n      $\\begin{bmatrix}\n      $0.7$ & $0.2$ & $0.1$ \\\\\n      $0.1$ & $0.7$ & $0.2$ \\\\\n      $0.05$ & $0.25$ & $0.7$\n      \\end{bmatrix}$.\n    - Observations: $(\\, $1$, $1$, $2$, $2$, $2$, $2$, $0$, $0$ \\,)$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and is itself a list of integers giving the most probable state path, for example, $[\\,[0,1,2],[1,1,2]\\,]$ with no spaces in the final output. The output for the four test cases must follow this exact format: a single line like $[\\,[\\dots],[\\dots],[\\dots],[\\dots]\\,]$, using $0$-based indices throughout. No physical units apply, and angles or percentages are not involved. The program must be fully self-contained and must not read any input.",
            "solution": "The problem asks for the derivation and implementation of a constrained Viterbi algorithm to find the most probable sequence of hidden states for a Hidden Markov Model (HMM) with a specific left-to-right topology. The algorithm must be implemented in the logarithmic domain to maintain numerical stability.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n\n- **Model:** A discrete Hidden Markov Model (HMM).\n- **Hidden States:** $z_t \\in \\{0, 1, \\dots, N-1\\}$ for $t \\in \\{0, \\dots, T-1\\}$.\n- **Observations:** $O_t \\in \\{0, 1, \\dots, M-1\\}$ for $t \\in \\{0, \\dots, T-1\\}$.\n- **Parameters:**\n    - Initial state distribution: $\\boldsymbol{\\pi} = (\\pi_i)$, where $\\pi_i = \\mathbb{P}(z_0 = i)$.\n    - State transition matrix: $\\mathbf{A} = (A_{ij})$, where $A_{ij} = \\mathbb{P}(z_t = j \\mid z_{t-1} = i)$.\n    - Emission probability matrix: $\\mathbf{B} = (B_{j,k})$, where $B_{j,k} = \\mathbb{P}(O_t = k \\mid z_t = j)$.\n- **Observation Sequence:** A given sequence $(O_0, O_1, \\dots, O_{T-1})$.\n- **Topology Constraint:** Transitions from state $i$ to state $j$ are allowed only if $j \\in \\{i, i+1\\}$. The last state, $N-1$, can only transition to itself. This implies that for a state $j$, the only possible predecessor states are $\\{j-1, j\\}$ (or just $\\{0\\}$ for state $j=0$).\n- **Implementation Requirements:**\n    - Use the log domain for all calculations. $\\log(0)$ is represented by $-\\infty$.\n    - Implement a deterministic tie-breaking rule: when $\\arg\\max$ is not unique, choose the smallest state index.\n- **Output:** The most probable state path $(z_0^*, z_1^*, \\dots, z_{T-1}^*)$ as a list of $0$-based integers.\n- **Test Cases:** Four specific sets of parameters ($\\boldsymbol{\\pi}$, $\\mathbf{A}$, $\\mathbf{B}$) and observation sequences are provided.\n\n**1.2. Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem is a standard application of HMMs, a cornerstone of statistical modeling in many STEM fields, including neuroscience for analyzing neural state dynamics. The Viterbi algorithm is the canonical method for decoding the most likely hidden state sequence. The left-to-right topology is a common and meaningful constraint for modeling processes that proceed through ordered stages, such as task-structured neural activity. All definitions and principles are based on standard probability theory and are mathematically sound.\n- **Well-Posed:** The problem is well-posed. Given a complete set of HMM parameters and an observation sequence, the Viterbi algorithm guarantees the existence of a most probable path. The specified tie-breaking rule ensures that this path is unique. The problem is self-contained and provides all necessary data for each test case.\n- **Objective:** The problem is stated in precise, objective mathematical language. Definitions are formal. The requirements are unambiguous.\n\n**1.3. Verdict and Action**\n\nThe problem statement is valid. It is scientifically grounded, well-posed, objective, and complete. I will proceed with deriving and implementing the solution.\n\n### Step 2: Derivation of the Constrained Viterbi Algorithm\n\nThe objective is to find the state sequence $z = (z_0, z_1, \\dots, z_{T-1})$ that maximizes the conditional probability $\\mathbb{P}(z \\mid O)$, where $O = (O_0, O_1, \\dots, O_{T-1})$ is the sequence of observations. By Bayes' theorem, $\\mathbb{P}(z \\mid O) = \\frac{\\mathbb{P}(z, O)}{\\mathbb{P}(O)}$. Since $\\mathbb{P}(O)$ is constant for a given observation sequence, maximizing $\\mathbb{P}(z \\mid O)$ is equivalent to maximizing the joint probability $\\mathbb{P}(z, O)$.\n\nThe fundamental assumptions of an HMM (Markov property for transitions and state-conditional independence of emissions) allow us to express the joint probability as:\n$$\n\\mathbb{P}(z, O) = \\mathbb{P}(z_0) \\mathbb{P}(O_0 \\mid z_0) \\prod_{t=1}^{T-1} \\mathbb{P}(z_t \\mid z_{t-1}) \\mathbb{P}(O_t \\mid z_t)\n$$\nUsing the provided HMM parameters, this becomes:\n$$\n\\mathbb{P}(z, O) = \\pi_{z_0} B_{z_0, O_0} \\prod_{t=1}^{T-1} A_{z_{t-1}, z_t} B_{z_t, O_t}\n$$\nThe Viterbi algorithm finds the maximizing sequence $z^*$ using dynamic programming. Let $\\delta_t(j)$ be the maximum probability of any state sequence of length $t+1$ ending in state $j$ at time $t$, having generated the first $t+1$ observations $O_0, \\dots, O_t$:\n$$\n\\delta_t(j) = \\max_{z_0, \\dots, z_{t-1}} \\mathbb{P}(z_0, \\dots, z_{t-1}, z_t=j, O_0, \\dots, O_t)\n$$\nTo reconstruct the path, we also define a backpointer variable $\\psi_t(j)$ that stores the most likely predecessor state for a path ending in state $j$ at time $t$.\n\n**Initialization ($t=0$):**\nThe probability of starting in state $j$ and observing $O_0$ is:\n$$\n\\delta_0(j) = \\pi_j B_{j, O_0}\n$$\n**Recursion ($1 \\le t < T$):**\nWe can define $\\delta_t(j)$ in terms of the values at time $t-1$:\n$$\n\\delta_t(j) = \\max_{i=0,\\dots,N-1} \\left[ \\delta_{t-1}(i) \\cdot A_{ij} \\right] \\cdot B_{j, O_t}\n$$\nThe backpointer is the state $i$ that achieves this maximum:\n$$\n\\psi_t(j) = \\arg\\max_{i=0,\\dots,N-1} \\left[ \\delta_{t-1}(i) \\cdot A_{ij} \\right]\n$$\n**Log-Domain Formulation:**\nTo prevent numerical underflow with long sequences, we work with log-probabilities. Let $v_t(j) = \\log \\delta_t(j)$. The product-maximization becomes a sum-maximization:\n$$\nv_t(j) = \\max_{i} \\left[ v_{t-1}(i) + \\log A_{ij} \\right] + \\log B_{j, O_t}\n$$\nThe backpointer in the log domain is:\n$$\n\\psi_t(j) = \\arg\\max_{i} \\left[ v_{t-1}(i) + \\log A_{ij} \\right]\n$$\nThe initialization becomes:\n$$\nv_0(j) = \\log \\pi_j + \\log B_{j, O_0}\n$$\nwhere probabilities of $0$ become $\\log(0) = -\\infty$.\n\n**Applying the Left-to-Right Constraint:**\nThe problem specifies that transitions from state $i$ are only allowed to states $j \\in \\{i, i+1\\}$. This means the only possible predecessors for a state $j$ at time $t$ are states $j$ and $j-1$ at time $t-1$. This greatly simplifies the maximization.\n\n- For state $j=0$, the only predecessor is state $0$.\n  $$\n  v_t(0) = \\left( v_{t-1}(0) + \\log A_{00} \\right) + \\log B_{0, O_t}\n  $$\n  $$\n  \\psi_t(0) = 0\n  $$\n- For states $j \\in \\{1, \\dots, N-1\\}$, the predecessors can be $j-1$ or $j$.\n  $$\n  v_t(j) = \\max \\left( v_{t-1}(j-1) + \\log A_{j-1, j}, v_{t-1}(j) + \\log A_{jj} \\right) + \\log B_{j, O_t}\n  $$\n  The backpointer $\\psi_t(j)$ is the state index ($j-1$ or $j$) that yields the maximum. The specified tie-breaking rule requires choosing the smallest index in case of a tie.\n  $$\n  \\psi_t(j) = \\begin{cases} j-1 & \\text{if } v_{t-1}(j-1) + \\log A_{j-1, j} \\ge v_{t-1}(j) + \\log A_{jj} \\\\ j & \\text{otherwise} \\end{cases}\n  $$\n\n**Termination and Path Backtracking:**\nAfter filling the dynamic programming tables up to time $T-1$:\n1.  Find the most likely final state, $z_{T-1}^*$:\n    $$\n    z_{T-1}^* = \\arg\\max_{j=0,\\dots,N-1} v_{T-1}(j)\n    $$\n    Again, the tie-breaking rule implies choosing the smallest index $j$ in case of a tie.\n2.  Backtrack to find the rest of the path:\n    $$\n    z_{t-1}^* = \\psi_t(z_t^*) \\quad \\text{for } t = T-1, T-2, \\dots, 1\n    $$\nThe sequence $(z_0^*, z_1^*, \\dots, z_{T-1}^*)$ is the most probable state path.\n\n### Algorithm Summary\n\n1.  **Preprocessing:** Convert the parameters $\\boldsymbol{\\pi}$, $\\mathbf{A}$, $\\mathbf{B}$ to the log domain, handling zeros correctly (e.g., using `numpy.log`).\n2.  **Initialization ($t=0$):**\n    - Create a $T \\times N$ log-probability table, $v$, and a $T \\times N$ backpointer table, $\\psi$.\n    - For each state $j=0, \\dots, N-1$, compute $v_{0,j} = \\log\\pi_j + \\log B_{j, O_0}$.\n3.  **Recursion ($t=1, \\dots, T-1$):**\n    - For $j=0$: Set $v_{t,0} = v_{t-1,0} + \\log A_{00} + \\log B_{0, O_t}$ and $\\psi_{t,0} = 0$.\n    - For $j=1, \\dots, N-1$:\n        - Calculate path probabilities from predecessors:\n          - $p_{j-1} = v_{t-1, j-1} + \\log A_{j-1, j}$\n          - $p_j = v_{t-1, j} + \\log A_{jj}$\n        - If $p_{j-1} \\ge p_j$, set the max probability to $p_{j-1}$ and $\\psi_{t,j} = j-1$.\n        - Else, set the max probability to $p_j$ and $\\psi_{t,j} = j$.\n        - Set $v_{t,j} = (\\text{max probability}) + \\log B_{j, O_t}$.\n4.  **Termination:** Find the final state $z_{T-1}^* = \\arg\\max_j v_{T-1,j}$.\n5.  **Backtracking:**\n    - Initialize the path `path` of length $T$. Set `path[T-1] = z_{T-1}^*$.\n    - For $t=T-2, \\dots, 0$: `path[t] = \\psi_{t+1, path[t+1]}`.\n6.  **Return:** The computed path.\n\nThis structured procedure will be implemented to solve the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Viterbi decoding problem for four different HMM test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1\n        {\n            \"N\": 3, \"M\": 4, \"T\": 6,\n            \"pi\": np.array([0.9, 0.1, 0.0]),\n            \"A\": np.array([\n                [0.7, 0.3, 0.0],\n                [0.0, 0.6, 0.4],\n                [0.0, 0.0, 1.0]\n            ]),\n            \"B\": np.array([\n                [0.6, 0.3, 0.05, 0.05],\n                [0.1, 0.6, 0.2, 0.1],\n                [0.05, 0.15, 0.7, 0.1]\n            ]),\n            \"O\": np.array([0, 1, 1, 2, 2, 3])\n        },\n        # Test Case 2\n        {\n            \"N\": 4, \"M\": 3, \"T\": 5,\n            \"pi\": np.array([1.0, 0.0, 0.0, 0.0]),\n            \"A\": np.array([\n                [0.5, 0.5, 0.0, 0.0],\n                [0.0, 0.5, 0.5, 0.0],\n                [0.0, 0.0, 0.5, 0.5],\n                [0.0, 0.0, 0.0, 1.0]\n            ]),\n            \"B\": np.array([\n                [0.9, 0.1, 1e-12],\n                [0.1, 0.8, 0.1],\n                [1e-12, 0.2, 0.8],\n                [0.05, 0.05, 0.9]\n            ]),\n            \"O\": np.array([0, 1, 2, 2, 2])\n        },\n        # Test Case 3\n        {\n            \"N\": 2, \"M\": 2, \"T\": 3,\n            \"pi\": np.array([0.5, 0.5]),\n            \"A\": np.array([\n                [0.5, 0.5],\n                [0.0, 1.0]\n            ]),\n            \"B\": np.array([\n                [0.6, 0.4],\n                [0.6, 0.4]\n            ]),\n            \"O\": np.array([0, 0, 1])\n        },\n        # Test Case 4\n        {\n            \"N\": 3, \"M\": 3, \"T\": 8,\n            \"pi\": np.array([0.2, 0.8, 0.0]),\n            \"A\": np.array([\n                [0.4, 0.6, 0.0],\n                [0.0, 0.7, 0.3],\n                [0.0, 0.0, 1.0]\n            ]),\n            \"B\": np.array([\n                [0.7, 0.2, 0.1],\n                [0.1, 0.7, 0.2],\n                [0.05, 0.25, 0.7]\n            ]),\n            \"O\": np.array([1, 1, 2, 2, 2, 2, 0, 0])\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        path = constrained_viterbi(params)\n        results.append(path)\n\n    # Format the final output string exactly as required.\n    # e.g., [[0,1,2],[1,1,2]]\n    formatted_results = [f\"[{','.join(map(str, p))}]\" for p in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef constrained_viterbi(params):\n    \"\"\"\n    Implements the constrained Viterbi algorithm in the log domain.\n    \"\"\"\n    N = params[\"N\"]  # Number of states\n    T = params[\"T\"]  # Length of observation sequence\n    \n    # Use np.log which correctly handles log(0) = -inf\n    with np.errstate(divide='ignore'):\n        log_pi = np.log(params[\"pi\"])\n        log_A = np.log(params[\"A\"])\n        log_B = np.log(params[\"B\"])\n\n    O = params[\"O\"]\n\n    # Dynamic programming tables\n    # v_table stores the log probabilities (delta in standard literature)\n    v_table = np.full((T, N), -np.inf)\n    # psi_table stores backpointers to reconstruct the path\n    psi_table = np.zeros((T, N), dtype=int)\n\n    # Initialization step (t=0)\n    obs_0 = O[0]\n    for j in range(N):\n        v_table[0, j] = log_pi[j] + log_B[j, obs_0]\n\n    # Recursion step (t=1 to T-1)\n    for t in range(1, T):\n        obs_t = O[t]\n        \n        # State j=0 can only be reached from state 0\n        v_table[t, 0] = v_table[t-1, 0] + log_A[0, 0] + log_B[0, obs_t]\n        psi_table[t, 0] = 0\n\n        # States j=1 to N-1\n        for j in range(1, N):\n            # Probability of path from predecessor j-1\n            prob_from_j_minus_1 = v_table[t-1, j-1] + log_A[j-1, j]\n            # Probability of path from predecessor j\n            prob_from_j = v_table[t-1, j] + log_A[j, j]\n\n            # Tie-breaking rule: choose smallest state index (j-1)\n            if prob_from_j_minus_1 >= prob_from_j:\n                max_prob = prob_from_j_minus_1\n                psi_table[t, j] = j-1\n            else:\n                max_prob = prob_from_j\n                psi_table[t, j] = j\n            \n            v_table[t, j] = max_prob + log_B[j, obs_t]\n\n    # Termination and Path Backtracking\n    path = np.zeros(T, dtype=int)\n    \n    # Find the most likely final state\n    # np.argmax breaks ties by choosing the first occurrence (smallest index)\n    path[T-1] = np.argmax(v_table[T-1, :])\n\n    # Backtrack to find the full path\n    for t in range(T-2, -1, -1):\n        path[t] = psi_table[t+1, path[t+1]]\n\n    return path.tolist()\n\nsolve()\n```"
        },
        {
            "introduction": "A model is only as good as its underlying assumptions. This final exercise moves from model application to model assessment, a critical step in any scientific inquiry. You will confront a common limitation of basic HMMs—the assumption of conditional independence—and explore a generative model designed to capture cross-neuron synchrony, a ubiquitous feature of neural populations. By comparing pairwise correlations in observed data to those from model-generated simulations, you will learn a quantitative method for evaluating a model's biological realism .",
            "id": "4168501",
            "problem": "You are given a framework to assess cross-neuron synchrony within discrete latent states of a Hidden Markov Model (HMM). Consider a set of $S$ discrete latent states indexing neural population activity and a set of $N$ neurons. For each state $z \\in \\{1,\\dots,S\\}$, let the observed data be a matrix $D_z \\in \\mathbb{N}^{T_z \\times N}$ containing spike counts per time bin (rows) and neuron (columns), where $T_z$ denotes the number of time bins assigned to state $z$. The HMM is used to generate simulated spike counts within each state via an emission model, and the goal is to quantify the model’s ability to reproduce cross-neuron synchrony by comparing observed and simulated pairwise spike-count correlations within states. The program must be self-contained and must not read external input.\n\nFundamental base and definitions:\n- A Hidden Markov Model (HMM) is defined by discrete latent states $z \\in \\{1,\\dots,S\\}$ and an emission distribution for observed data conditioned on the current state. This problem focuses on within-state analysis, not on state transition dynamics.\n- For each state $z$, define a per-neuron rate vector $\\boldsymbol{\\lambda}_z = [\\lambda_{z,1}, \\ldots, \\lambda_{z,N}]$ with $\\lambda_{z,n} \\ge 0$ specifying the expected spike count per bin for neuron $n$ in state $z$.\n- Cross-neuron synchrony within a state $z$ is operationalized by pairwise Pearson correlation coefficients of spike counts across time bins for all neuron pairs $(i,j)$ with $1 \\le i < j \\le N$. Given a $T_z \\times N$ observed count matrix $D_z$, the sample Pearson correlation coefficient between neuron $i$ and $j$ across $T_z$ bins is\n$$\nr_{z,ij}^{\\text{obs}} \\triangleq \\frac{\\sum_{t=1}^{T_z} \\left( D_z[t,i] - \\bar{D}_{z,i} \\right)\\left( D_z[t,j] - \\bar{D}_{z,j} \\right)}{\\sqrt{\\sum_{t=1}^{T_z} \\left( D_z[t,i] - \\bar{D}_{z,i} \\right)^2} \\sqrt{\\sum_{t=1}^{T_z} \\left( D_z[t,j] - \\bar{D}_{z,j} \\right)^2}},\n$$\nwhere $\\bar{D}_{z,i}$ is the sample mean of the $i$-th neuron’s counts in $D_z$. As a practical convention to handle degenerate cases where either denominator is zero (sample variance equals zero for a neuron), define $r_{z,ij}^{\\text{obs}} \\triangleq 0$ for such pairs.\n\nModel-based simulation within a state:\n- Within each state $z$, simulate counts for neuron $n$ at time $t$ as follows. First draw a shared multiplicative gain $g_{z,t}$ with\n$$\ng_{z,t} \\sim \\text{Gamma}(k_z, \\theta_z), \\quad \\mathbb{E}[g_{z,t}] = 1, \\quad \\mathrm{Var}(g_{z,t}) = v_z \\ge 0.\n$$\nA valid parameterization is $k_z = 1/v_z$ and $\\theta_z = v_z$ for $v_z > 0$, and $g_{z,t} \\equiv 1$ for $v_z = 0$. Then, conditional on $g_{z,t}$, draw spike counts independently across neurons:\n$$\nX_{z,t,n} \\mid g_{z,t} \\sim \\text{Poisson}(g_{z,t} \\lambda_{z,n}).\n$$\nThis Gamma-Poisson mixture induces cross-neuron correlations when $v_z > 0$ and reduces to independent Poisson counts when $v_z = 0$.\n- For each state $z$, with $T_z^{\\text{sim}}$ simulation bins, form the simulated matrix $X_z \\in \\mathbb{N}^{T_z^{\\text{sim}} \\times N}$ and compute pairwise sample correlations $r_{z,ij}^{\\text{sim}}$ analogously to $r_{z,ij}^{\\text{obs}}$, using the same convention $r_{z,ij}^{\\text{sim}} \\triangleq 0$ when either simulated neuron exhibits zero sample variance.\n\nAssessment metric:\n- For each state $z$, define the Root Mean Square Error (RMSE) between observed and simulated pairwise correlations across all pairs as\n$$\n\\mathrm{RMSE}_z \\triangleq \\sqrt{\\frac{1}{M} \\sum_{1 \\le i < j \\le N} \\left( r_{z,ij}^{\\text{sim}} - r_{z,ij}^{\\text{obs}} \\right)^2},\n$$\nwhere $M = N(N-1)/2$ is the number of neuron pairs. The final output for a case is the list $[\\mathrm{RMSE}_1, \\ldots, \\mathrm{RMSE}_S]$.\n\nYour task:\n- Implement the simulation and assessment as specified. Use a fixed random seed for reproducibility.\n- For each defined test case, generate observed data $D_z$ as described below, simulate $X_z$ using the specified $(\\boldsymbol{\\lambda}_z, v_z, T_z^{\\text{sim}})$, compute $r_{z,ij}^{\\text{obs}}$ and $r_{z,ij}^{\\text{sim}}$, and return the per-state RMSE values.\n- The final program output must be a single line containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list of per-state RMSEs. For example, the output format is $[[a_1,\\ldots,a_S],[b_1,\\ldots,b_{S'}],\\ldots]$ with all entries as floating-point numbers.\n\nTest suite:\n- Case $1$ (general happy path, moderate positive synchrony reproduced by the model): $S = 2$, $N = 4$, observed data generated by the same Gamma-Poisson scheme with the following parameters per state:\n    - State $1$: $\\boldsymbol{\\lambda}_1 = [2.0, 3.0, 4.0, 5.0]$, $v_1^{\\text{obs}} = 0.2$, $T_1 = 400$. Model parameters: $v_1 = 0.2$, $T_1^{\\text{sim}} = 20000$.\n    - State $2$: $\\boldsymbol{\\lambda}_2 = [1.0, 1.5, 2.0, 2.5]$, $v_2^{\\text{obs}} = 0.05$, $T_2 = 400$. Model parameters: $v_2 = 0.05$, $T_2^{\\text{sim}} = 20000$.\n- Case $2$ (boundary condition: one neuron has zero rate and thus zero sample variance): $S = 1$, $N = 3$. Observed data for state $1$ is generated via the Gamma-Poisson scheme with $\\boldsymbol{\\lambda}_1 = [0.0, 2.0, 3.0]$, $v_1^{\\text{obs}} = 0.1$, $T_1 = 200$. Model parameters: $v_1 = 0.1$, $T_1^{\\text{sim}} = 20000$.\n- Case $3$ (edge case: strong negative correlation in observed counts not captured by an independent Poisson model): $S = 1$, $N = 3$. Observed data for state $1$ is explicitly specified as a deterministic matrix $D_1 \\in \\mathbb{N}^{30 \\times 3}$ with rows\n    - For $t \\in \\{1,\\dots,15\\}$: $[5, 1, 4]$,\n    - For $t \\in \\{16,\\dots,30\\}$: $[1, 5, 2]$.\n  This induces strong negative correlation between the first two neurons. Model parameters: use independent Poisson ($v_1 = 0$) with $\\boldsymbol{\\lambda}_1 = [3.0, 3.0, 3.0]$ and $T_1^{\\text{sim}} = 20000$.\n\nImplementation details:\n- Use a fixed random seed $987654321$ for all stochastic generation to ensure reproducibility.\n- Use the parameterization $k_z = 1/v_z$ and $\\theta_z = v_z$ when $v_z > 0$; use $g_{z,t} \\equiv 1$ when $v_z = 0$.\n- When computing Pearson correlations, apply the stated convention that pairs involving a neuron with zero sample variance produce correlation $0$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case mapped to its list of RMSEs. For the three cases specified above, the output should therefore be of the form $[[\\mathrm{RMSE}_1^{(1)},\\mathrm{RMSE}_2^{(1)}],[\\mathrm{RMSE}_1^{(2)}],[\\mathrm{RMSE}_1^{(3)}]]$ where superscripts index the case.\n\nThe final answers must be floating-point numbers. No physical units are involved.",
            "solution": "The problem requires the implementation of a computational framework to assess a generative model's fidelity in reproducing cross-neuron synchrony. The analysis is performed on a state-by-state basis, as determined by a Hidden Markov Model (HMM). The core of the problem lies in the emission model for neural spike counts within each latent state. This model, a Gamma-Poisson mixture, is designed to capture shared variability among neurons, which manifests as pairwise correlations. The assessment involves comparing the pairwise Pearson correlations from observed data to those from data simulated by the model, and summarizing the discrepancy using the Root Mean Square Error (RMSE).\n\nThe solution proceeds in three logical steps: first, detailing the data generation process; second, specifying the correlation calculation method; and third, defining the error metric. These components are then integrated to solve the provided test cases.\n\n**1. Generative Model: The Gamma-Poisson Mixture**\n\nThe spike counts for a population of $N$ neurons within a given state $z$ are modeled as conditionally independent Poisson random variables, where the shared randomness is induced by a common multiplicative gain factor $g_{z,t}$ that fluctuates over time bins $t$.\n\nFor each time bin $t=1, \\dots, T_z$, a gain factor $g_{z,t}$ is drawn from a Gamma distribution:\n$$\ng_{z,t} \\sim \\text{Gamma}(k_z, \\theta_z)\n$$\nThe parameters of the Gamma distribution, shape $k_z$ and scale $\\theta_z$, are chosen to fix the mean of the gain to $1$ and its variance to a specified value $v_z \\ge 0$. The relationships are $\\mathbb{E}[g_{z,t}] = k_z \\theta_z = 1$ and $\\mathrm{Var}(g_{z,t}) = k_z \\theta_z^2 = v_z$. For $v_z > 0$, this yields the parameterization $k_z = 1/v_z$ and $\\theta_z = v_z$. If the variance $v_z = 0$, the gain becomes deterministic, $g_{z,t} \\equiv 1$ for all $t$.\n\nConditional on this shared gain $g_{z,t}$, the spike count for each neuron $n$ in state $z$ at time bin $t$, denoted $X_{z,t,n}$, is drawn from a Poisson distribution with a rate modulated by both the neuron's intrinsic firing rate $\\lambda_{z,n}$ and the common gain $g_{z,t}$:\n$$\nX_{z,t,n} \\mid g_{z,t} \\sim \\text{Poisson}(g_{z,t} \\lambda_{z,n})\n$$\nWhen $v_z > 0$, the shared gain $g_{z,t}$ induces positive correlations between the spike counts of any two neurons in the same time bin. When $v_z=0$, $g_{z,t}=1$, and the model simplifies to a set of independent Poisson processes for each neuron, implying zero correlation between neurons. This entire procedure is used to generate both the \"observed\" data matrices $D_z$ (for Cases $1$ and $2$) and the \"simulated\" data matrices $X_z$ (for all cases).\n\n**2. Quantification of Synchrony: Pairwise Pearson Correlation**\n\nCross-neuron synchrony within a state $z$ is measured using the sample Pearson correlation coefficient, $r_{z,ij}$, for each pair of neurons $(i,j)$ with $1 \\le i < j \\le N$. Given a data matrix $A$ (which can be either observed data $D_z$ or simulated data $X_z$) of size $T \\times N$, the correlation is calculated as:\n$$\nr_{ij} = \\frac{\\sum_{t=1}^{T} (A[t,i] - \\bar{A}_i)(A[t,j] - \\bar{A}_j)}{\\sqrt{\\sum_{t=1}^{T} (A[t,i] - \\bar{A}_i)^2} \\sqrt{\\sum_{t=1}^{T} (A[t,j] - \\bar{A}_j)^2}}\n$$\nwhere $\\bar{A}_i$ is the sample mean of the counts for neuron $i$.\n\nA crucial an operational requirement is the handling of degenerate cases where a neuron's activity has zero sample variance over the observation period (i.e., its spike count is constant across all time bins). This causes the denominator in the correlation formula to become zero. As specified, in any such case, the correlation coefficient $r_{ij}$ involving that neuron is defined to be $0$. A robust implementation strategy is to use a standard numerical library function (e.g., `numpy.corrcoef`), which typically yields `NaN` (Not a Number) for these cases, and then systematically replace all `NaN` values in the resulting correlation matrix with $0$. From the full $N \\times N$ correlation matrix, we extract the $M = N(N-1)/2$ unique pairwise correlation values corresponding to pairs $(i,j)$ with $i < j$.\n\n**3. Assessment Metric: Root Mean Square Error (RMSE)**\n\nThe performance of the model for a given state $z$ is evaluated by the Root Mean Square Error (RMSE) between the vector of observed correlations, $\\mathbf{r}_z^{\\text{obs}}$, and the vector of simulated correlations, $\\mathbf{r}_z^{\\text{sim}}$.\n$$\n\\mathrm{RMSE}_z = \\sqrt{\\frac{1}{M} \\sum_{1 \\le i < j \\le N} \\left( r_{z,ij}^{\\text{sim}} - r_{z,ij}^{\\text{obs}} \\right)^2}\n$$\nThis metric provides a single value summarizing the average deviation between the model-predicted synchrony structure and the empirically observed one.\n\n**Execution of Test Cases**\n\nA fixed random seed of $987654321$ is used for all stochastic operations to ensure reproducibility.\n\n- **Case 1**: This is the standard scenario. For two states, observed data are generated using the Gamma-Poisson model with specified parameters ($v_1^{\\text{obs}} = 0.2$, $v_2^{\\text{obs}} = 0.05$). The simulation model is then tasked to reproduce this, using the same variance parameters ($v_1 = 0.2$, $v_2 = 0.05$). We expect a low RMSE, as the simulation model perfectly matches the true data-generating process, with differences arising only from sampling variability.\n\n- **Case 2**: This case tests the handling of a boundary condition. The rate for the first neuron is $\\lambda_{1,1} = 0.0$. Consequently, this neuron will have zero spike counts in all time bins, leading to zero sample variance in both the observed and simulated datasets. The implementation must correctly apply the rule to set all pairwise correlations involving this neuron to $0$.\n\n- **Case 3**: This case is designed to demonstrate a model mismatch. The observed data $D_1$ is deterministic and constructed to exhibit perfect negative correlation between neurons $1$ and $2$ ($r_{1,12}^{\\text{obs}} = -1$) and perfect positive correlation between neurons $1$ and $3$ ($r_{1,13}^{\\text{obs}} = 1$). The simulation model, however, is an independent Poisson model ($v_1 = 0$), which is structurally incapable of producing any correlation. The simulated correlations $r_{1,ij}^{\\text{sim}}$ will therefore be close to $0$. The RMSE will be large, correctly quantifying the model's failure to capture the strong, structured correlations present in the data. The model's rates, $\\boldsymbol{\\lambda}_1 = [3.0, 3.0, 3.0]$, are set to match the sample means of the observed data, isolating the failure to the model's correlation structure.\n\nThe final program implements these steps, computes the RMSE for each state in each case, and formats the output as a list of lists.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    \n    # Define the random number generator with a fixed seed for reproducibility.\n    rng = np.random.default_rng(987654321)\n\n    def generate_gamma_poisson_data(lambdas, v, T, rng_instance):\n        \"\"\"\n        Generates spike count data from a Gamma-Poisson mixture model.\n        \n        Args:\n            lambdas (np.ndarray): Per-neuron rate vector of shape (N,).\n            v (float): Shared variance parameter for the Gamma gain.\n            T (int): Number of time bins to generate.\n            rng_instance (np.random.Generator): A numpy random number generator.\n\n        Returns:\n            np.ndarray: A (T, N) matrix of spike counts.\n        \"\"\"\n        N = len(lambdas)\n        if v > 0:\n            # Parameterize Gamma distribution to have mean 1 and variance v\n            k = 1.0 / v  # shape\n            theta = v    # scale\n            g_t = rng_instance.gamma(k, theta, size=T)\n        else:\n            # If v=0, gain is deterministically 1 (independent Poisson)\n            g_t = np.ones(T)\n        \n        # Reshape for broadcasting\n        g_t = g_t.reshape(T, 1)\n        lambdas = lambdas.reshape(1, N)\n        \n        # Calculate rates for each neuron at each time bin\n        rates = g_t * lambdas\n        \n        # Generate Poisson-distributed spike counts\n        counts = rng_instance.poisson(rates)\n        return counts\n\n    def compute_pairwise_correlations(data):\n        \"\"\"\n        Computes pairwise Pearson correlations for all unique neuron pairs.\n        Handles the case where a neuron has zero variance by setting its correlations to 0.\n\n        Args:\n            data (np.ndarray): A (T, N) matrix of spike counts.\n\n        Returns:\n            np.ndarray: A 1D array of correlation coefficients for pairs (i, j) where i < j.\n        \"\"\"\n        N = data.shape[1]\n        if N < 2:\n            return np.array([])\n            \n        # np.corrcoef produces NaNs for columns with zero variance.\n        # This is the desired behavior before applying the problem's convention.\n        with np.errstate(divide='ignore', invalid='ignore'):\n            corr_matrix = np.corrcoef(data, rowvar=False)\n\n        # Apply the convention: if a neuron has zero variance, its correlations are 0.\n        # We achieve this by converting all NaNs in the correlation matrix to 0.\n        corr_matrix = np.nan_to_num(corr_matrix, nan=0.0)\n\n        # Extract the upper triangle of the matrix (for pairs i < j)\n        upper_triangle_indices = np.triu_indices(N, k=1)\n        return corr_matrix[upper_triangle_indices]\n\n    test_cases = [\n        # Case 1: S=2, N=4, general happy path\n        {\n            \"S\": 2, \"N\": 4,\n            \"states\": [\n                {\n                    \"obs_params\": {\"lambdas\": np.array([2.0, 3.0, 4.0, 5.0]), \"v\": 0.2, \"T\": 400},\n                    \"sim_params\": {\"v\": 0.2, \"T_sim\": 20000}\n                },\n                {\n                    \"obs_params\": {\"lambdas\": np.array([1.0, 1.5, 2.0, 2.5]), \"v\": 0.05, \"T\": 400},\n                    \"sim_params\": {\"v\": 0.05, \"T_sim\": 20000}\n                }\n            ]\n        },\n        # Case 2: S=1, N=3, boundary condition with zero-rate neuron\n        {\n            \"S\": 1, \"N\": 3,\n            \"states\": [\n                {\n                    \"obs_params\": {\"lambdas\": np.array([0.0, 2.0, 3.0]), \"v\": 0.1, \"T\": 200},\n                    \"sim_params\": {\"v\": 0.1, \"T_sim\": 20000}\n                }\n            ]\n        },\n        # Case 3: S=1, N=3, deterministic observed data with strong anti-correlation\n        {\n            \"S\": 1, \"N\": 3,\n            \"states\": [\n                {\n                    \"obs_data\": np.vstack([np.tile([5, 1, 4], (15, 1)), np.tile([1, 5, 2], (15, 1))]),\n                    \"sim_params\": {\"lambdas\": np.array([3.0, 3.0, 3.0]), \"v\": 0, \"T_sim\": 20000}\n                }\n            ]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_rmses = []\n        for state_params in case[\"states\"]:\n            # Generate or load observed data and compute correlations\n            if \"obs_data\" in state_params:\n                D_s = state_params[\"obs_data\"]\n                # model lambdas are in sim_params for this case\n                sim_lambdas = state_params[\"sim_params\"][\"lambdas\"]\n            else:\n                p_obs = state_params[\"obs_params\"]\n                D_s = generate_gamma_poisson_data(p_obs[\"lambdas\"], p_obs[\"v\"], p_obs[\"T\"], rng)\n                sim_lambdas = p_obs[\"lambdas\"]\n\n            r_obs = compute_pairwise_correlations(D_s)\n            \n            # Generate simulated data and compute correlations\n            p_sim = state_params[\"sim_params\"]\n            X_s = generate_gamma_poisson_data(sim_lambdas, p_sim[\"v\"], p_sim[\"T_sim\"], rng)\n            r_sim = compute_pairwise_correlations(X_s)\n            \n            # Compute RMSE\n            if len(r_obs) > 0:\n                rmse = np.sqrt(np.mean((r_sim - r_obs)**2))\n            else:\n                rmse = 0.0 # No pairs, no error.\n            case_rmses.append(rmse)\n        \n        all_results.append(case_rmses)\n\n    # Format the final output string\n    # str() on a list gives a string like '[0.1, 0.2]', which is what's required.\n    formatted_results = ','.join(map(str, all_results))\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        }
    ]
}