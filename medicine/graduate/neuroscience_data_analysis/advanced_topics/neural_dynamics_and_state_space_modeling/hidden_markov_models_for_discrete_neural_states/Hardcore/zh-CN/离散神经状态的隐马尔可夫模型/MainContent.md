## 引言
大脑的活动并非一成不变，而是在不同的功能状态之间动态切换。从记录到的高维、嘈杂的神经元尖峰序列中，如何识别出这些潜在的、离散的“神经状态”，是现代神经科学面临的核心挑战之一。隐马尔可夫模型（Hidden Markov Model, HMM）为此提供了一个强大而灵活的概率框架，它假设观测到的复杂神经活动是由一个不可见的、遵循[马尔可夫过程](@entry_id:1127634)的离散状态序列所驱动的。

本文旨在为研究生及研究人员提供一份关于HMM在神经状态分析中应用的全面指南。我们将带领读者从基础理论出发，逐步深入到高级应用和前沿扩展。

在第一章“原理与机制”中，我们将奠定理论基石，详细拆解HMM的数学定义、核心假设，并阐明用于状态推断的[前向-后向算法](@entry_id:194772)和[维特比算法](@entry_id:269328)。

接着，在第二章“应用与跨学科联系”中，我们将理论与实践相结合，探讨如何为真实数据选择合适的模型、如何扩展HMM来捕捉更复杂的动力学（如非几何状态持续时间），以及如何将推断出的状态与行为和认知功能联系起来。

最后，第三章“动手实践”将通过一系列精心设计的编程练习，帮助读者将理论知识转化为解决实际问题的能力。

通过这三个章节的学习，您将不仅理解HMM“是什么”，更能掌握“如何用”它来揭示神经数据背后隐藏的动力学结构，从而将原始数据转化为深刻的科学洞见。现在，让我们从其核心原理开始。

## 原理与机制

本章旨在深入阐述用于分析离散神经状态的隐马尔可夫模型（Hidden Markov Model, HMM）的基本原理和核心机制。我们将从其数学形式化定义出发，探讨模型如何捕捉神经活动的动态特性，并详细介绍用于从观测数据中推断潜在状态的关键算法。

### 隐马尔可夫模型的形式化定义

隐马尔可夫模型是一个双重[随机过程](@entry_id:268487)，由一个不可观测的（“隐藏的”）[随机过程](@entry_id:268487)和一个与该隐藏过程相关的可观测的[随机过程](@entry_id:268487)组成。在神经科学的背景下，隐藏过程代表了大脑在不同离散“状态”间的切换，而观测过程则是我们记录到的神经活动，如尖峰发放计数。

#### 隐状态过程：马尔可夫链

模型的核心是一个潜在的、随时间演变的离散状态变量 $z_t$，它在每个时间步 $t$ 从一个包含 $K$ 个可能状态的集合 $\{1, 2, \dots, K\}$ 中取值。我们假设这个状态序列 $\{z_t\}_{t=1}^T$ 遵循一个**一阶、时齐的[马尔可夫链](@entry_id:150828)**。这个假设包含以下三个关键要素：

1.  **[状态空间](@entry_id:160914) (State Space)**：系统在任何时刻都必须处于 $K$ 个预定义状态中的某一个。这些状态被认为是[互斥](@entry_id:752349)且完备的。

2.  **初始状态分布 (Initial State Distribution)**：在观测开始时（$t=1$），系统处于每个状态 $k$ 的概率由一个初始分布向量 $\pi = (\pi_1, \pi_2, \dots, \pi_K)$ 给出，其中 $\pi_k = P(z_1 = k)$。作为一个概率分布，它必须满足 $\sum_{k=1}^K \pi_k = 1$ 且 $\pi_k \ge 0$。

3.  **转移[概率矩阵](@entry_id:274812) (Transition Probability Matrix)**：状态随时间的演变由一个 $K \times K$ 的[转移矩阵](@entry_id:145510) $A$ 控制。该矩阵的元素 $A_{ij}$ 定义了从状态 $i$ 转移到状态 $j$ 的概率：$A_{ij} = P(z_t = j \mid z_{t-1} = i)$。**一阶[马尔可夫性质](@entry_id:139474)**指出，未来状态的概率只依赖于当前状态，而与过去的所有状态无关，即 $P(z_t \mid z_{t-1}, z_{t-2}, \dots, z_1) = P(z_t \mid z_{t-1})$。**时齐性**（time-homogeneity）假设意味着这个[转移矩阵](@entry_id:145510) $A$ 不随时间 $t$ 变化。矩阵的每一行都是一个概率分布，因此行和必须为1，即 $\sum_{j=1}^K A_{ij} = 1$ 对所有状态 $i$ 成立。

#### 观测过程：发射概率

在每个时间步 $t$，当系统处于隐藏状态 $z_t$ 时，它会“发射”（generate）一个可观测的信号 $y_t$。在[神经数据分析](@entry_id:1128577)中，$y_t$ 通常是一个向量，表示在时间窗 $t$ 内从 $N$ 个同时记录的神经元中观测到的尖峰发放计数，$y_t = (y_{t,1}, y_{t,2}, \dots, y_{t,N})$。

HMM 的第二个核心假设是**观测独立性假设**：在给定当前隐藏状态 $z_t$ 的条件下，当前观测 $y_t$ 独立于所有其他时刻的状态和观测。其概率分布，即**发射概率**（emission probability），写作 $p(y_t \mid z_t = k)$。

对于多神经元尖峰计数数据，通常会进一步假设，在给定[隐藏状态](@entry_id:634361) $z_t=k$ 的条件下，不同神经元的发放计数是条件独立的。如果每个神经元的计数 $y_{t,n}$ 都遵循[泊松分布](@entry_id:147769)，那么发射概率可以具体化为：
$$
p(y_t \mid z_t = k) = \prod_{n=1}^N p(y_{t,n} \mid z_t = k) = \prod_{n=1}^N \frac{e^{-\lambda_{k,n}} \lambda_{k,n}^{y_{t,n}}}{y_{t,n}!}
$$
其中，$\lambda_{k,n}$ 是神经元 $n$ 在状态 $k$ 下的平均发放率。这个参数集 $\Lambda = \{\lambda_{k,n}\}$ 捕捉了每个离散神经状态是如何通过特定的、可观测的群体发放模式来体现的。

#### 图模型视角与[条件独立性](@entry_id:262650)

HMM 的结构可以用一个[有向图](@entry_id:920596)模型清晰地表示，其中节点代表[随机变量](@entry_id:195330)，箭头代表直接的概率依赖关系。该图的结构为：
$$
\dots \to z_{t-1} \to z_t \to z_{t+1} \to \dots
$$
$$
\dots \quad \downarrow \quad\quad \downarrow \quad\quad \downarrow \quad\quad \dots
$$
$$
\dots \quad y_{t-1} \quad y_t \quad y_{t+1} \quad \dots
$$
这个结构蕴含了模型的关键[条件独立性](@entry_id:262650)假设。利用 [d-分离](@entry_id:748152)（d-separation）的概念，我们可以精确地描述这些关系。一个节点 $V$ 的**[马尔可夫毯](@entry_id:1127637)**（Markov blanket），记作 $\operatorname{MB}(V)$，是指能够使 $V$ 与图中所有其他节点条件独立的最小节点集。在一个[有向图](@entry_id:920596)中，一个节点的[马尔可夫毯](@entry_id:1127637)由其父节点、子节点以及其子节点的其他父节点组成。

对于 HMM 中的一个内部时间点 $t$：
-   **观测 $y_t$ 的[马尔可夫毯](@entry_id:1127637)**是其唯一的父节点 $z_t$，即 $\operatorname{MB}(y_t) = \{z_t\}$。这意味着，一旦我们知道了当前时刻的隐藏状态 $z_t$，该时刻的观测 $y_t$ 就与模型中所有其他变量（包括所有其他状态和其他观测）都条件独立。这是发射概率 $p(y_t \mid z_t)$ 的直接体现。

-   **隐藏状态 $z_t$ 的[马尔可夫毯](@entry_id:1127637)**由其父节点 $z_{t-1}$、子节点 $z_{t+1}$ 和 $y_t$ 构成，即 $\operatorname{MB}(z_t) = \{z_{t-1}, z_{t+1}, y_t\}$。这意味着，给定相邻的状态和当前观测，状态 $z_t$ 与网络中的所有其他节点都条件独立。这个性质是所有 HMM 推断算法（如[前向-后向算法](@entry_id:194772)）的基础。

### 生成过程与联合概率

HMM 是一个[生成模型](@entry_id:177561)，意味着我们可以根据其参数 $(\pi, A, \Lambda)$ 生成一个状态序列和观测序列。这个过程如下：
1.  根据初始分布 $\pi$ 抽取第一个状态 $z_1$。
2.  在 $t=1$，根据发射概率 $p(y_1 \mid z_1)$ 生成观测 $y_1$。
3.  对于 $t=2, 3, \dots, T$：
    a. 根据转移概率 $p(z_t \mid z_{t-1}) = A_{z_{t-1}, z_t}$ 从前一状态 $z_{t-1}$ 转移到当前状态 $z_t$。
    b. 根据发射概率 $p(y_t \mid z_t)$ 生成当前观测 $y_t$。

根据这个生成过程和链式法则，我们可以写出整个状态序列 $z_{1:T}$ 和观测序列 $y_{1:T}$ 的**完全[联合概率](@entry_id:266356)**分布：
$$
p(z_{1:T}, y_{1:T}) = p(z_1) \left( \prod_{t=2}^T p(z_t \mid z_{t-1}) \right) \left( \prod_{t=1}^T p(y_t \mid z_t) \right)
$$
代入我们模型的具体参数，得到：
$$
p(z_{1:T}, y_{1:T}) = \pi_{z_1} \left( \prod_{t=2}^T A_{z_{t-1}, z_t} \right) \left( \prod_{t=1}^T \prod_{n=1}^N \frac{e^{-\lambda_{z_t,n}} \lambda_{z_t,n}^{y_{t,n}}}{y_{t,n}!} \right)
$$
这个表达式是所有 HMM 计算的理论出发点。例如，给定一个特定的状态序列 $z_{1:3} = (1, 2, 2)$ 和观测序列 $y_1=(1,0), y_2=(2,1), y_3=(0,1)$，以及模型参数 $(\pi, A, \Lambda)$，我们可以通过简单地将相应的概率值相乘来计算这个特定事件发生的总概率。

### 核心诠释性概念

除了其数学形式，理解 HMM 如何概念化神经活动也至关重要。

#### 分段[平稳性](@entry_id:143776)：将神经活动模态化

HMM 最强大的功能之一是它将看似复杂、非平稳的神经[时间序列数据](@entry_id:262935)解释为由一系列**分段平稳**（piecewise stationary）的片段组成。

具体来说，在任何一个隐藏状态 $z_t$ 保持恒定为 $k$ 的时间段内，观测 $y_t$ 是从一个固定的发射分布 $p(y \mid z=k)$ 中[独立同分布](@entry_id:169067)（IID）地抽取的。IID 序列是[平稳过程](@entry_id:196130)的最强形式。因此，HMM 将整个观测序列建模为多个片段的拼接，每个片段对应一个隐藏状态，并具有其自身的、时不变的统计特性。当隐藏状态发生转换时（例如，从 $k$ 到 $j$），生成观测的统计规则也随之切换（从 $p(y \mid z=k)$ 切换到 $p(y \mid z=j)$）。

这种分段平稳的视角与神经科学中“脑状态”或“网络状态”的概念不谋而合，即大脑在不同的、具有相对稳定功能和动力学特性的离散状态之间切换。

值得注意的是，由 HMM 生成的**边际观测过程** $\{y_t\}$ 本身通常**不是**一个马尔可夫链。这是因为预测未来的观测 $y_{t+1}$ 不仅需要当前的观测 $y_t$，还需要过去的所有观测历史 $y_{1:t-1}$。过去的信息有助于我们更精确地推断当前隐藏状态 $z_t$ 的概率分布，从而更好地预测 $z_{t+1}$ 和 $y_{t+1}$。只有当隐藏状态被揭示时，观测的[马尔可夫性质](@entry_id:139474)（[条件独立性](@entry_id:262650)）才成立。

#### 状态持续性与驻留时间

HMM 的参数具有直接的物理解释。特别是，[转移矩阵](@entry_id:145510) $A$ 的对角线元素 $A_{kk} = P(z_t=k \mid z_{t-1}=k)$ 控制着状态 $k$ 的**持续性**（persistence）或“粘性”。

我们可以定义一个状态的**驻留时间**（dwell time）$D$ 为系统进入该状态后，连续保持在该状态的时间步数。对于状态 $k$，事件 $\{D=d\}$ 表示系统在该状态停留了 $d-1$ 次，然后在第 $d$ 次转移时离开了该状态。由于[马尔可夫性质](@entry_id:139474)，每次转移都是独立的，因此驻留时间的[概率质量函数](@entry_id:265484)（PMF）为：
$$
P(D=d) = (A_{kk})^{d-1} (1-A_{kk})
$$
这是一个[几何分布](@entry_id:154371)。利用[几何分布](@entry_id:154371)的性质，我们可以导出驻留时间的期望和方差：
-   **期望驻留时间**：$\mathbb{E}[D] = \frac{1}{1-A_{kk}}$
-   **驻留时间方差**：$\operatorname{Var}(D) = \frac{A_{kk}}{(1-A_{kk})^2}$

这些公式提供了从模型参数到可解释的神经现象的直接桥梁。一个高的自转移概率 $A_{kk}$（例如，接近1）意味着该神经状态是高度持续的，平均会持续很长时间，但其持续时间的变化也很大。相反，一个低的 $A_{kk}$ 则意味着一个短暂、瞬态的状态。

### 基本推断问题与算法

在 HMM 的应用中，我们通常面临三个核心的计算问题：
1.  **评估（Evaluation）**：给定模型参数和观测序列，计算该序列出现的总概率 $p(y_{1:T})$。
2.  **解码（Decoding）**：给定模型参数和观测序列，找到最有可能的隐藏状态序列 $z_{1:T}^*$。
3.  **学习（Learning）**：给定观测序列，找到能最大化解释该数据的模型参数 $(\pi, A, \Lambda)$。

本章主要关注前两个问题，它们是进行神经状态推断的基础。

#### 评估问题：[前向算法](@entry_id:165467)与序列[似然](@entry_id:167119)

直接计算 $p(y_{1:T}) = \sum_{z_{1:T}} p(y_{1:T}, z_{1:T})$ 需要对所有 $K^T$ 条可能的状态路径进行求和，这在计算上是不可行的。**[前向算法](@entry_id:165467)**（Forward Algorithm）利用模型的[动态规划](@entry_id:141107)结构，高效地解决了这个问题。

我们定义**前向变量** $\alpha_t(k)$ 为观测到序列的前 $t$ 个值且在时刻 $t$ 处于状态 $k$ 的[联合概率](@entry_id:266356)：
$$
\alpha_t(k) \triangleq p(y_{1:t}, z_t = k)
$$

该算法包含两个步骤：
1.  **初始化 (t=1)**：
    $$
    \alpha_1(k) = p(y_1, z_1=k) = p(z_1=k) p(y_1 \mid z_1=k) = \pi_k p(y_1 \mid z_t=k)
    $$
2.  **递推 (t = 2, ..., T)**：
    $$
    \alpha_t(k) = p(y_{1:t}, z_t=k) = \sum_{j=1}^K p(y_{1:t}, z_t=k, z_{t-1}=j)
    $$
    利用 HMM 的[条件独立性](@entry_id:262650)假设，上式可以分解为：
    $$
    \alpha_t(k) = p(y_t \mid z_t=k) \sum_{j=1}^K p(z_t=k \mid z_{t-1}=j) p(y_{1:t-1}, z_{t-1}=j)
    $$
    代入定义，我们得到[递推公式](@entry_id:149465)：
    $$
    \alpha_t(k) = p(y_t \mid z_t=k) \left( \sum_{j=1}^K A_{jk} \alpha_{t-1}(j) \right)
    $$
在完成从 $t=1$ 到 $T$ 的所有递推后，整个观测序列的似然值可以通过对最终的前向变量求和得到：
$$
p(y_{1:T}) = \sum_{k=1}^K p(y_{1:T}, z_T=k) = \sum_{k=1}^K \alpha_T(k)
$$

**数值稳定性考虑**：在实际应用中，前向变量 $\alpha_t(k)$ 是许多小于1的概率的乘积，会随着 $t$ 的增长而指数级减小，很快就会导致**数值[下溢](@entry_id:635171)**（numerical underflow），即计算机无法表示如此小的[浮点数](@entry_id:173316)，将其近似为零。对于长度稍长的神经记录（例如 $T \ge 500$），这个问题是不可避免的。

为了解决这个问题，通常采用两种策略：
1.  **尺度缩放 (Scaling)**：在每一步递推后，对 $\alpha_t$ 向量进行归一化，使其元素之和为1。这些归一化因子被存储起来，最后用于计算总的[对数似然](@entry_id:273783)。
2.  **对[数域](@entry_id:155558)计算 (Log-domain computation)**：整个算法在对[数域](@entry_id:155558)中进行，即计算 $\log \alpha_t(k)$。[递推公式](@entry_id:149465)中的乘法变为加法。然而，求和操作 $\sum_j A_{jk} \alpha_{t-1}(j)$ 变得复杂，需要使用**log-sum-exp**技巧进行稳定计算：
    $$
    \log \sum_{i} \exp(x_i) = m + \log \sum_{i} \exp(x_i - m), \quad \text{其中 } m = \max_i x_i
    $$
    这可以防止对非常大的负数取指数时发生[下溢](@entry_id:635171)。[前向算法](@entry_id:165467)的对[数域](@entry_id:155558)版本是 HMM 实践中的标准方法。 

算法的计算复杂度为 $O(TK^2)$，对于中等规模的 $K$ 和 $T$ 是完全可行的。

#### 状态估计I：平滑[后验概率](@entry_id:153467)

评估问题计算了整个序列的[似然](@entry_id:167119)，但通常我们更关心在给定**全部**观测数据 $y_{1:T}$ 的情况下，系统在某个特定时刻 $t$ 处于状态 $k$ 的概率。这个概率被称为**平滑后验概率**（smoothed posterior probability），记作 $\gamma_t(k) = p(z_t=k \mid y_{1:T})$。

为了计算 $\gamma_t(k)$，我们需要引入**后向变量** $\beta_t(k)$，它被定义为在时刻 $t$ 处于状态 $k$ 的条件下，观测到未来序列 $y_{t+1:T}$ 的[条件概率](@entry_id:151013)：
$$
\beta_t(k) \triangleq p(y_{t+1:T} \mid z_t=k)
$$
$\beta_t(k)$ 也可以通过一个从 $t=T-1$ 回溯到 $t=1$ 的递推算法（后向算法）来高效计算。

利用前向和后向变量，平滑[后验概率](@entry_id:153467)可以被简洁地表示。根据[条件概率](@entry_id:151013)的定义和 HMM 的独立性假设：
$$
p(z_t=k \mid y_{1:T}) = \frac{p(y_{1:T}, z_t=k)}{p(y_{1:T})} = \frac{p(y_{1:t}, z_t=k) p(y_{t+1:T} \mid z_t=k)}{p(y_{1:T})}
$$
这恰好是：
$$
\gamma_t(k) = \frac{\alpha_t(k) \beta_t(k)}{\sum_{j=1}^K \alpha_T(j)}
$$
这个公式优雅地结合了来自过去的信息（由 $\alpha_t(k)$ 编码）和来自未来的信息（由 $\beta_t(k)$ 编码），从而对时刻 $t$ 的状态给出了最完备的估计。在神经科学应用中，$\gamma_t(k)$ 的时间[序列图](@entry_id:165947)揭示了大脑状态随时间演变的概率轨迹。

#### 状态估计II：[维特比算法](@entry_id:269328)与最可能路径

另一个重要的[解码问题](@entry_id:264478)是找到**单一的最可能的状态序列** $z_{1:T}^* = \arg\max_{z_{1:T}} p(z_{1:T} \mid y_{1:T})$。这不同于计算每个时刻的平滑概率，因为可能存在这样一种情况：由每个时刻最可能的状态组成的路径，其整体概率并非最高。

这个问题可以通过**[维特比算法](@entry_id:269328)**（Viterbi Algorithm）高效求解。该算法也采用[动态规划](@entry_id:141107)，但将[前向算法](@entry_id:165467)中的求和操作替换为求最大值操作。我们定义一个新变量 $\delta_t(k)$，表示到达时刻 $t$ 且处于状态 $k$ 的**所有部分路径**中概率最大的那一条路径的概率：
$$
\delta_t(k) \triangleq \max_{z_1, \dots, z_{t-1}} p(z_1, \dots, z_{t-1}, z_t=k, y_{1:t})
$$

[维特比算法](@entry_id:269328)的[递推关系](@entry_id:189264)如下：
1.  **初始化 (t=1)**：
    $$
    \delta_1(k) = \pi_k p(y_1 \mid z_1=k)
    $$
2.  **递推 (t = 2, ..., T)**：
    $$
    \delta_t(k) = p(y_t \mid z_t=k) \max_{j \in \{1, \dots, K\}} \left( \delta_{t-1}(j) A_{jk} \right)
    $$
在递推过程中，我们还需要一个回溯指针数组 $\psi_t(k)$ 来记录在时刻 $t$ 到达状态 $k$ 的最大概率路径是从哪个前一状态 $j$ 转移过来的：
$$
\psi_t(k) = \arg\max_{j \in \{1, \dots, K\}} \left( \delta_{t-1}(j) A_{jk} \right)
$$
递推完成后，整个序列的最大概率为 $P^* = \max_{k} \delta_T(k)$，最后一个状态为 $z_T^* = \arg\max_{k} \delta_T(k)$。然后，我们可以通过回溯指针从 $t=T-1$ 到 $1$ 依次确定最优路径上的其他状态：
$$
z_t^* = \psi_{t+1}(z_{t+1}^*)
$$
[维特比算法](@entry_id:269328)找到的单一最佳路径在许多应用中非常有用，例如，当需要对时间序列进行离散分割时。

### [模型辨识](@entry_id:139651)性：一个关键的限制

最后，一个重要的理论和实践问题是**[模型辨识](@entry_id:139651)性**（model identifiability）。即使 HMM 能够很好地拟[合数](@entry_id:263553)据，我们能够从数据中唯一地确定模型参数吗？答案是否定的，HMM 存在固有的辨识性问题。

最明显的例子是标签交换（label switching）：如果我们交换两个状态（例如，状态1和2）的所有相关参数（即交换 $\pi_1, \pi_2$；交换 $A$ 的第1、2行和第1、2列；交换发射参数 $\lambda_{1, \cdot}$ 和 $\lambda_{2, \cdot}$），新模型将产生与原模型完全相同的观测数据分布。

更微妙的辨识性问题发生在不同状态的**发射分布高度重叠**时。考虑一个两状态模型，其发射概率 $p(y \mid z=1)$ 和 $p(y \mid z=2)$ 非常相似。在这种情况下，观测数据本身几乎不能提供任何信息来区分这两个状态。结果是，尽[管模型](@entry_id:140303)可能能够很好地预测数据，但关于隐藏状态的动力学参数（即[转移矩阵](@entry_id:145510) $A$）可能极难甚至不可能被准确估计。

例如，两个HMM模型可能具有截然不同的转移矩阵（一个倾向于状态持续，另一个倾向于快速切换），但如果它们的发射分布几乎无法区分，那么它们生成的观测序列的统计分布可能非常接近。这可以用两个分布之间的**总变差距离**（total variation distance）来量化。如果距离很小，那么有限的数据将无法可靠地区分这两个模型。

在解释从神经数据中拟合出的 HMM 参数时，必须牢记这一限制。如果拟合出的状态具有非常相似的发放模式，那么关于这些状态之间转换动力学的结论应当持谨慎态度。