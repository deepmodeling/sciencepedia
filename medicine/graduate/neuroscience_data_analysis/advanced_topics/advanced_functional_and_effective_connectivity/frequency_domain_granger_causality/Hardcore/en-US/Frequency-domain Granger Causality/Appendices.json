{
    "hands_on_practices": [
        {
            "introduction": "Calculating frequency-domain Granger causality from a vector autoregressive ($\\mathrm{VAR}$) model is more than just plugging numbers into a formula; it requires a precise computational sequence. This exercise challenges you to identify the correct procedural pipeline, from fitting the $\\mathrm{VAR}$ coefficients to deriving the final causality spectrum . Successfully navigating this problem will solidify your understanding of how to properly handle contemporaneously correlated innovations, a critical step that ensures the resulting measure reflects directed, time-lagged influences.",
            "id": "4165323",
            "problem": "A bivariate local field potential (LFP) recording $\\mathbf{x}_t = [x_t, y_t]^\\top$ from two cortical sites is sampled at frequency $f_s$ and modeled as a stable vector autoregression (VAR) of order $p$, with $\\mathbf{x}_t = \\sum_{k=1}^{p} A_k \\mathbf{x}_{t-k} + \\boldsymbol{\\varepsilon}_t$, where $\\boldsymbol{\\varepsilon}_t$ is a zero-mean wide-sense white innovation with contemporaneous covariance $\\Sigma$. You plan to compute the frequency-domain Granger causality from $y$ to $x$, denoted $f_{Y\\to X}(\\omega)$, over a grid of $N$ angular frequencies $\\{\\omega_m\\}_{m=0}^{N-1}$ spanning $[0,\\pi]$ in radians per sample. The computation must proceed from first principles by: fitting $\\mathrm{VAR}(p)$, obtaining the transfer function $H(\\omega)$, obtaining the spectral density $S(\\omega)$, identifying the intrinsic spectrum of $x$, and finally forming $f_{Y\\to X}(\\omega)$ at each grid frequency.\n\nWhich option most correctly and completely enumerates the computational steps that take you from the fitted $\\mathrm{VAR}(p)$ to $H(\\omega)$, $S(\\omega)$, the intrinsic spectrum of $x$, and finally $f_{Y\\to X}(\\omega)$ across the frequency grid, while preserving the directed, dynamical, and innovation-based foundations of the measure?\n\nA. Fit $\\mathrm{VAR}(p)$ to $\\mathbf{x}_t$ by ordinary least squares (OLS), verify stability by checking that all zeros of $\\det\\!\\big(I - \\sum_{k=1}^{p} A_k z^k\\big)$ lie outside the unit disc, and estimate $\\Sigma$. For each grid frequency $\\omega_m$, construct the lag polynomial $A(e^{-i\\omega_m}) = I - \\sum_{k=1}^{p} A_k e^{-i\\omega_m k}$ and invert to obtain the transfer function $H(\\omega_m) = A(e^{-i\\omega_m})^{-1}$. Compute a lower-triangular contemporaneous transform $T$ such that $T \\Sigma T^\\ast$ is diagonal (for example, via a Cholesky-based triangular orthogonalization), and define the transformed transfer function $H_T(\\omega_m) = H(\\omega_m) T^{-1}$ together with the diagonal innovation covariance $D = T \\Sigma T^\\ast$. Form the spectral density as $S(\\omega_m) = H_T(\\omega_m) D H_T(\\omega_m)^\\ast$, and identify the intrinsic spectrum of $x$ as the contribution of $x$’s own innovation in the orthogonalized representation, i.e., the power from the first transformed innovation alone. Finally, at each $\\omega_m$, compute the directed measure by taking the natural logarithm of the ratio of the total spectrum of $x$ to its intrinsic spectrum, and report $\\{f_{Y\\to X}(\\omega_m)\\}_{m=0}^{N-1}$.\n\nB. Fit $\\mathrm{VAR}(p)$ to $\\mathbf{x}_t$ and compute the magnitude-squared coherence between $x_t$ and $y_t$ from the periodogram of the residuals. Convert the coherence to a causality spectrum by applying the transformation $-\\log(1 - \\text{coherence}^2)$ at each frequency. This avoids transfer-function inversion and innovation orthogonalization and directly yields $f_{Y\\to X}(\\omega)$ from the frequency-domain dependence structure.\n\nC. Skip the $\\mathrm{VAR}(p)$ model and compute $H(\\omega)$ by taking the Fast Fourier Transform (FFT) of the raw time series $\\mathbf{x}_t$ to obtain an empirical transfer function. Use this $H(\\omega)$ to form $S(\\omega) = H(\\omega) H(\\omega)^\\ast$, define the intrinsic spectrum of $x$ by zeroing the cross-spectrum, and then compute $f_{Y\\to X}(\\omega)$ as the difference between the total and intrinsic spectra across the frequency grid.\n\nD. Fit $\\mathrm{VAR}(p)$ and estimate $\\Sigma$, but bypass contemporaneous orthogonalization. For each $\\omega_m$, compute $A(e^{+i\\omega_m}) = I - \\sum_{k=1}^{p} A_k e^{+i\\omega_m k}$, invert to get $H(\\omega_m) = A(e^{+i\\omega_m})^{-1}$, and form $S(\\omega_m) = H(\\omega_m) \\Sigma H(\\omega_m)^\\ast$. Define the intrinsic spectrum of $x$ by setting all cross-lag autoregressive coefficients $A_{xy}(k)$ to zero in the fitted model and recomputing the spectrum, and then compute $f_{Y\\to X}(\\omega)$ as a ratio of the original and modified spectra.\n\nE. Fit $\\mathrm{VAR}(p)$ and estimate $\\Sigma$. For each $\\omega_m$, compute $H(\\omega_m) = \\big(I - \\sum_{k=1}^{p} A_k e^{-i\\omega_m k}\\big)^{-1}$. Whiten innovations by a symmetric square-root transform $W$ such that $W \\Sigma W^\\ast = I$ (with $W$ symmetric), set $H_W(\\omega_m) = H(\\omega_m) W$, and compute $S(\\omega_m) = H_W(\\omega_m) H_W(\\omega_m)^\\ast$. Define the intrinsic spectrum of $x$ as the power associated with the first column of $H_W(\\omega_m)$ and then compute $f_{Y\\to X}(\\omega)$ by a logarithmic ratio across the frequency grid. This symmetric whitening preserves energy while eliminating instantaneous correlation.",
            "solution": "The user wants a critical evaluation of a problem concerning the computation of frequency-domain Granger causality. I will first validate the problem statement and then, if valid, proceed to a detailed derivation and evaluation of the provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   The observed data is a bivariate local field potential (LFP) recording, represented as a vector time series $\\mathbf{x}_t = [x_t, y_t]^\\top$.\n-   The sampling frequency is $f_s$.\n-   The data is modeled as a stable vector autoregression of order $p$, denoted $\\mathrm{VAR}(p)$:\n    $$ \\mathbf{x}_t = \\sum_{k=1}^{p} A_k \\mathbf{x}_{t-k} + \\boldsymbol{\\varepsilon}_t $$\n-   The innovation process $\\boldsymbol{\\varepsilon}_t$ is zero-mean, wide-sense white noise.\n-   The contemporaneous covariance matrix of the innovation is $\\Sigma = E[\\boldsymbol{\\varepsilon}_t \\boldsymbol{\\varepsilon}_t^\\top]$.\n-   The goal is to compute the frequency-domain Granger causality from $y$ to $x$, denoted $f_{Y\\to X}(\\omega)$.\n-   The computation is to be performed over a grid of $N$ angular frequencies $\\{\\omega_m\\}_{m=0}^{N-1}$ spanning the interval $[0,\\pi]$ radians per sample.\n-   The computational procedure must start from first principles and include these stages: fitting the $\\mathrm{VAR}(p)$ model, obtaining the transfer function $H(\\omega)$, obtaining the spectral density matrix $S(\\omega)$, identifying the intrinsic spectrum of $x$, and finally forming the causality measure $f_{Y\\to X}(\\omega)$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is firmly rooted in the theory of time series analysis and its application to neuroscience, specifically the calculation of Granger causality in the frequency domain as developed by Geweke. All concepts—VAR models, transfer functions, spectral density, innovation covariance, and their interrelations—are standard and well-established in the scientific literature.\n-   **Well-Posed:** The problem is well-posed. It asks for the correct enumeration of computational steps required to calculate a well-defined mathematical quantity ($f_{Y\\to X}(\\omega)$) from a standard statistical model ($\\mathrm{VAR}(p)$). Given the established theory, a uniquely correct procedure exists.\n-   **Objective:** The problem statement is expressed in precise, objective, and unambiguous mathematical language. It is free from subjective claims or opinions.\n\n**Flaw Checklist:**\n1.  **Scientific or Factual Unsoundness:** None. The premises are standard definitions from time series theory.\n2.  **Non-Formalizable or Irrelevant:** None. The problem is directly and formally about the specified topic.\n3.  **Incomplete or Contradictory Setup:** None. The problem provides all necessary theoretical constructs to define the required computational path.\n4.  **Unrealistic or Infeasible:** None. The described analysis is a common procedure in computational neuroscience.\n5.  **Ill-Posed or Poorly Structured:** None. The question is clear and has a definite answer based on established theory.\n6.  **Pseudo-Profound, Trivial, or Tautological:** None. The problem tests a critical and often misunderstood aspect of Granger causality: the proper handling of contemporaneously correlated innovations. The distinction between different matrix decompositions (triangular vs. symmetric) is a substantive conceptual point.\n7.  **Outside Scientific Verifiability:** None. The procedure is based on verifiable mathematical derivations.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. I will proceed with deriving the solution and evaluating the options.\n\n### Derivation of Frequency-Domain Granger Causality\nThe foundation of Granger causality is predictability. In the frequency domain, we assess how much the predictive power for a signal $x_t$ at a given frequency $\\omega$ is improved by including the history of another signal $y_t$. This is formally quantified by comparing the total power spectrum of $x_t$ with its \"intrinsic\" power spectrum—the power generated by its own innovation process, independent of influences from $y_t$.\n\n1.  **VAR Model and Transfer Function:**\n    The $\\mathrm{VAR}(p)$ model is $\\left(I - \\sum_{k=1}^{p} A_k L^k\\right) \\mathbf{x}_t = \\boldsymbol{\\varepsilon}_t$, where $L$ is the lag operator. In the frequency domain, this becomes $A(e^{-i\\omega}) \\mathbf{x}(\\omega) = \\boldsymbol{\\varepsilon}(\\omega)$, where $A(z) = I - \\sum_{k=1}^{p} A_k z^k$.\n    The transfer function matrix $H(\\omega)$ maps the innovations to the observed signals, $\\mathbf{x}(\\omega) = H(\\omega)\\boldsymbol{\\varepsilon}(\\omega)$, and is given by the inverse of the matrix polynomial:\n    $$ H(\\omega) = \\left(I - \\sum_{k=1}^{p} A_k e^{-i\\omega k}\\right)^{-1} $$\n\n2.  **Spectral Density Matrix:**\n    The cross-spectral density matrix $S(\\omega)$ is derived from the transfer function and the innovation covariance:\n    $$ S(\\omega) = H(\\omega) \\Sigma H(\\omega)^\\ast $$\n    where $H(\\omega)^\\ast$ denotes the conjugate transpose of $H(\\omega)$. The diagonal element $S_{xx}(\\omega)$ represents the total power spectrum of the process $x_t$.\n\n3.  **Innovation Orthogonalization:**\n    A critical step is to handle the contemporaneous correlation in the innovations, captured by the off-diagonal elements of $\\Sigma$. The innovations $\\varepsilon_{xt}$ and $\\varepsilon_{yt}$ are generally correlated. To separate the causal contributions, we must transform them into an orthogonal set of innovations. This requires imposing a causal ordering. To calculate $f_{Y \\to X}$, we consider the ordering $(x, y)$, which implies that any shared instantaneous variance is attributed to $x$. This is achieved using a decomposition of $\\Sigma$ that is triangular.\n    The standard method is the Cholesky decomposition, $\\Sigma = P P^\\ast$, where $P$ is a lower-triangular matrix. For our bivariate case with $\\mathbf{x}_t = [x_t, y_t]^\\top$, $P$ has the form:\n    $$ P = \\begin{pmatrix} p_{11} & 0 \\\\ p_{21} & p_{22} \\end{pmatrix} $$\n    We can define a new set of uncorrelated innovations $\\boldsymbol{\\eta}_t$ (with identity covariance) such that $\\boldsymbol{\\varepsilon}_t = P \\boldsymbol{\\eta}_t$. The process can then be written as $\\mathbf{x}(\\omega) = H(\\omega) P \\boldsymbol{\\eta}(\\omega)$. The matrix $H'(\\omega) = H(\\omega) P$ is the transfer function from the orthogonal innovations $\\boldsymbol{\\eta}$ to the signals $\\mathbf{x}$.\n\n4.  **Intrinsic and Total Spectra:**\n    The new spectral density matrix is $S(\\omega) = H'(\\omega) (H'(\\omega))^\\ast$. The $(1,1)$ element, $S_{xx}(\\omega)$, is the total power of $x$:\n    $$ S_{xx}(\\omega) = |H'_{11}(\\omega)|^2 + |H'_{12}(\\omega)|^2 $$\n    The first term, $|H'_{11}(\\omega)|^2$, represents the power in $x$ contributed by the first orthogonal innovation, $\\eta_{1t}$, which by our construction is the intrinsic part of $x$'s innovation. This is defined as the *intrinsic spectrum* of $x$:\n    $$ S_{xx, \\text{int}}(\\omega) = |H'_{11}(\\omega)|^2 = |(H(\\omega)P)_{11}|^2 = |H_{xx}(\\omega)p_{11} + H_{xy}(\\omega)p_{21}|^2 $$\n    The second term, $|H'_{12}(\\omega)|^2$, represents the power transferred from the second orthogonal innovation, $\\eta_{2t}$, which is causally dependent on $y$.\n\n5.  **Causality Measure:**\n    The frequency-domain Granger causality from $y$ to $x$ is defined as the natural logarithm of the ratio of the total power of $x$ to its intrinsic power:\n    $$ f_{Y\\to X}(\\omega) = \\ln\\left(\\frac{S_{xx}(\\omega)}{S_{xx, \\text{int}}(\\omega)}\\right) $$\n\n### Option-by-Option Analysis\n\n**A. Fit $\\mathrm{VAR}(p)$... Cholesky-based triangular orthogonalization... logarithmic ratio.**\nThis option correctly describes the entire procedure. Let's trace its steps against our derivation:\n- It correctly identifies fitting the VAR model, checking stability, and estimating $\\Sigma$ as the first step.\n- It correctly defines the calculation of the transfer function $H(\\omega_m)$ from the VAR coefficients.\n- It correctly identifies the need for a triangular transformation to orthogonalize innovations to resolve contemporaneous correlation. The description of \"a lower-triangular contemporaneous transform $T$ such that $T \\Sigma T^\\ast$ is diagonal\" accurately describes the outcome of using the inverse of a Cholesky factor or an LDL decomposition, which preserves the necessary causal ordering.\n- It correctly defines the transformed transfer function ($H_T = H T^{-1}$) and the resulting spectral density ($S = H_T D H_T^\\ast$).\n- It correctly identifies the intrinsic spectrum as the power contribution from the first component of the transformed innovation vector, which corresponds to the variable $x$.\n- Finally, it correctly defines the Granger causality measure as the natural logarithm of the ratio of the total spectrum of $x$ to its intrinsic spectrum.\nEvery part of this option is consistent with the established theory.\n**Verdict: Correct.**\n\n**B. ...compute the magnitude-squared coherence... apply the transformation $-\\log(1 - \\text{coherence}^2)$...**\nThis describes a measure of total spectral interdependence, not directed causality. Coherence, $C_{xy}(\\omega)$, is symmetric ($C_{xy}(\\omega) = C_{yx}(\\omega)$) and cannot distinguish the direction of influence between $x$ and $y$. Granger causality is fundamentally asymmetric. This option confuses two distinct concepts.\n**Verdict: Incorrect.**\n\n**C. Skip the $\\mathrm{VAR}(p)$ model and compute $H(\\omega)$ by taking the Fast Fourier Transform (FFT) of the raw time series...**\nThis a a fundamental misunderstanding of the concepts. The FFT of a time series, $\\mathbf{x}(\\omega)$, is an estimate of the signal's Fourier representation, not the system's transfer function $H(\\omega)$. The VAR model is essential for estimating the system dynamics encapsulated in $H(\\omega)$. Furthermore, it incorrectly formulates the spectral density as $S(\\omega) = H(\\omega)H(\\omega)^\\ast$, omitting the crucial innovation covariance matrix $\\Sigma$. The proposed method is theoretically unsound.\n**Verdict: Incorrect.**\n\n**D. ...bypass contemporaneous orthogonalization... Define the intrinsic spectrum by setting all cross-lag autoregressive coefficients $A_{xy}(k)$ to zero...**\nThis option makes two critical errors. First, it uses the term $e^{+i\\omega_m k}$, which is a non-standard convention. Second, and more importantly, it \"bypasses contemporaneous orthogonalization.\" This is a fatal flaw. Failure to properly account for the correlated innovations in $\\Sigma$ means that any \"instantaneous causality\" (influence within one sampling interval) is ignored, and the measure can be grossly inaccurate. The described procedure of setting cross-coefficients to zero to define an intrinsic model is related to a different, less robust formulation of causality that is known to suffer from this very problem. The prompt specifically asks for a method that preserves the \"innovation-based foundations,\" which this approach fails to do.\n**Verdict: Incorrect.**\n\n**E. ...Whiten innovations by a symmetric square-root transform $W$ such that $W \\Sigma W^\\ast = I$...**\nThis option proposes using a symmetric whitening matrix, such as $W=\\Sigma^{-1/2}$. While this does produce uncorrelated innovations, it does so by mixing the original innovations ($\\varepsilon_{xt}$ and $\\varepsilon_{yt}$) in a symmetric way. The resulting orthogonal innovations, $\\eta_1$ and $\\eta_2$, are both mixtures of the original ones. This destroys the causal ordering that is essential for attributing influence. The power derived from the \"first column\" has no clear interpretation as \"intrinsic\" power, as that column's effect is driven by a mix of all original sources. A triangular decomposition (as in option A) is required to create a recursive structure that allows for a causal interpretation.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "An observed peak in a Granger causality spectrum is not evidence of interaction until its statistical significance is established. This practice guides you through the implementation of a complete statistical assessment workflow using permutation testing across trials, a powerful non-parametric method to generate an empirical null distribution . By completing this exercise, you will master not only the generation of p-values but also the application of essential techniques like False Discovery Rate (FDR) and Family-Wise Error Rate (FWER) control to address the problem of multiple comparisons across frequencies.",
            "id": "4165291",
            "problem": "You are given a collection of paired trial recordings of two neural activity variables, denoted by $x_t$ and $y_t$, each sampled at $f_s$ Hz for $T$ time points per trial, with $N$ independent trials. Assume the signals follow a stationary Bivariate Vector Autoregressive (VAR) process of order $1$ with Gaussian innovations, and that trials are independent realizations of the same process. The analysis goal is to estimate and assess the statistical significance of the frequency-domain Granger causality (GC) spectrum from $x$ to $y$, using an empirical null distribution constructed by permutation testing across trials, and with correction for multiple comparisons across frequencies.\n\nStart from the following fundamental bases and definitions:\n\n- Stationarity of linear autoregressive processes: a VAR process with stable coefficients has well-defined spectral properties, and linear innovations with covariance matrix provide a complete description of its second-order statistics.\n- The spectral density of a VAR process is the Fourier transform of its autocovariance function and can be expressed via the frequency-domain transfer function, which relates innovations to observed signals.\n- Granger causality (GC) is defined in the frequency domain by comparing the full spectrum of a target component to the spectrum of its intrinsic component when the influence of a source component is removed through linear prediction with Gaussian innovations.\n\nPermutation testing across trials for an empirical null spectrum is to be implemented by independently permuting the trial pairing of the candidate source variable $x_t$ relative to the target variable $y_t$. This destroys cross-trial alignment of potential causal influence while preserving the within-trial time structure of each variable. For each permutation, re-estimate the VAR model from the permuted pairs and compute the GC spectrum. This yields, for each frequency, an empirical null distribution of GC values under the null hypothesis of no causal influence from $x$ to $y$.\n\nMultiple comparisons across frequencies must be corrected using two distinct procedures:\n\n1. Benjamini–Hochberg False Discovery Rate (FDR) control at level $q = 0.05$.\n2. Family-Wise Error Rate (FWER) control at level $\\alpha = 0.05$ using the max-statistic approach, which uses the distribution of permutation maxima across the frequency grid to adjust p-values.\n\nImplement the following computational tasks:\n\n1. Simulation model: For each test case, generate $N$ trials of length $T$ time points of a VAR($1$) bivariate process with coefficients $a_x$ and $a_y$ on the self-lags and cross-lag coefficient $b$ from $x$ to $y$. Innovations are zero-mean Gaussian with covariance\n   $$\\Sigma = \\begin{bmatrix} \\sigma_x^2 & \\rho \\sigma_x \\sigma_y \\\\ \\rho \\sigma_x \\sigma_y & \\sigma_y^2 \\end{bmatrix},$$\n   where $\\rho$ is the correlation coefficient between the innovations of $x$ and $y$. Use a burn-in of $B$ samples per trial to reach stationarity before collecting $T$ samples. All trials are independent draws from the same VAR process and noise distribution. Ensure coefficients yield a stable process.\n\n2. VAR($1$) estimation: Given the paired trials, estimate the VAR($1$) coefficient matrix and the innovations covariance $\\Sigma$ across all trials by ordinary least squares, pooling all time points after the initial lag across trials.\n\n3. Frequency-domain Granger causality: Using the estimated VAR($1$) matrix and innovations covariance, compute the GC spectrum from $x$ to $y$ over a set of $M$ evenly spaced frequencies in the band $[1, 40]$ Hz. You must compute the full spectral density via the frequency-domain transfer function and form the GC spectrum by comparing the full power of $y$ to its intrinsic component when the influence of $x$ is linearly removed under Gaussian innovations.\n\n4. Empirical null spectra via trial permutation: For each of $P$ independent random permutations of trial indices, permute the $x$-trial assignments relative to the $y$-trial assignments, re-estimate VAR($1$), and recompute the GC spectrum over the same frequencies. For each frequency, this yields a sample of $P$ null GC values.\n\n5. P-values: For each frequency, compute the empirical p-value as the fraction of null GC values greater than or equal to the observed GC value, using the standard permutation p-value with $+1$ in both numerator and denominator.\n\n6. Multiple comparisons correction:\n   - Apply Benjamini–Hochberg False Discovery Rate (FDR) control at level $q = 0.05$ to the set of per-frequency p-values and produce a boolean decision for each frequency.\n   - Apply max-statistic Family-Wise Error Rate (FWER) control at level $\\alpha = 0.05$ by computing, for each permutation, the maximum null GC across frequencies; then, for each observed frequency, compute the adjusted p-value against this max-null distribution and produce a boolean decision at the specified $\\alpha$.\n\n7. Output specification: For each test case, count the number of frequencies declared significant under FDR and under max-statistic FWER, respectively. Your program should produce a single line of output containing, for the test suite below, a list of lists of two integers per test case, in the exact format:\n   $$\\texttt{[[n\\_FDR\\_1,n\\_FWER\\_1],[n\\_FDR\\_2,n\\_FWER\\_2],\\dots]}.$$\n\nUse the following test suite of parameter sets, which cover a range of scenarios:\n\n- Case A (expected causal influence from $x$ to $y$):\n  - $N = 40$, $T = 900$, $f_s = 200$ Hz, $M = 64$ frequencies in $[1, 40]$ Hz\n  - VAR coefficients: $a_x = 0.5$, $a_y = 0.5$, $b = 0.6$\n  - Innovations: $\\sigma_x = 1.0$, $\\sigma_y = 1.0$, $\\rho = 0.0$\n  - Burn-in: $B = 100$\n  - Permutations: $P = 150$\n\n- Case B (no causal influence and correlated noise):\n  - $N = 40$, $T = 900$, $f_s = 200$ Hz, $M = 64$ frequencies in $[1, 40]$ Hz\n  - VAR coefficients: $a_x = 0.5$, $a_y = 0.5$, $b = 0.0$\n  - Innovations: $\\sigma_x = 1.0$, $\\sigma_y = 1.0$, $\\rho = 0.7$\n  - Burn-in: $B = 100$\n  - Permutations: $P = 150$\n\n- Case C (weak causal influence and fewer trials):\n  - $N = 20$, $T = 600$, $f_s = 200$ Hz, $M = 64$ frequencies in $[1, 40]$ Hz\n  - VAR coefficients: $a_x = 0.5$, $a_y = 0.5$, $b = 0.2$\n  - Innovations: $\\sigma_x = 1.0$, $\\sigma_y = 1.0$, $\\rho = 0.0$\n  - Burn-in: $B = 100$\n  - Permutations: $P = 100$\n\nAnswer requirements:\n\n- Your program must be a complete, runnable implementation that performs the simulation, estimation, spectral GC computation, permutation testing, multiple-comparison corrections, and outputs the counts per test case.\n- Angles and frequencies must be treated in radians or Hertz consistently; you must compute frequencies in Hertz and convert to angular frequency in radians where needed.\n- The final output must be a single line in the specified format. No units need to be printed in the output.",
            "solution": "The problem requires a comprehensive implementation of frequency-domain Granger causality (GC) analysis for a simulated bivariate Vector Autoregressive (VAR) process, including statistical assessment using permutation testing and correction for multiple comparisons. The solution is structured around the computational tasks specified.\n\n### 1. Bivariate Vector Autoregressive (VAR) Model and Simulation\n\nThe underlying process is a stationary bivariate VAR model of order $p=1$. Let the two time series be denoted by the vector $\\mathbf{z}_t = [x_t, y_t]^\\top$. The VAR($1$) model is expressed as:\n$$\n\\mathbf{z}_t = A_1 \\mathbf{z}_{t-1} + \\mathbf{e}_t\n$$\nwhere $A_1$ is a $2 \\times 2$ coefficient matrix, and $\\mathbf{e}_t = [e_{x,t}, e_{y,t}]^\\top$ is a vector of white-noise innovations with zero mean and covariance matrix $\\Sigma$.\n$$\nA_1 = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}, \\quad \\Sigma = \\text{cov}(\\mathbf{e}_t) = \\begin{bmatrix} \\sigma_x^2 & \\rho \\sigma_x \\sigma_y \\\\ \\rho \\sigma_x \\sigma_y & \\sigma_y^2 \\end{bmatrix}\n$$\nThe problem specifies a directional causal structure from $x$ to $y$, implying that $y_{t-1}$ does not influence $x_t$. This corresponds to setting $a_{12}=0$. The coefficients are given as $a_{11} = a_x$ (self-lag for $x$), $a_{22} = a_y$ (self-lag for $y$), and $a_{21} = b$ (cross-lag from $x$ to $y$). The coefficient matrix for simulation is therefore:\n$$\nA_1 = \\begin{bmatrix} a_x & 0 \\\\ b & a_y \\end{bmatrix}\n$$\nThe stability of the VAR($1$) process is guaranteed if all eigenvalues of $A_1$ have a modulus less than $1$. For this lower triangular matrix, the eigenvalues are its diagonal elements, $a_x$ and $a_y$. The given values ($a_x=0.5, a_y=0.5$) satisfy this condition ($|0.5| < 1$).\n\nSimulation of $N$ independent trials, each of length $T$, proceeds by first generating innovation vectors $\\mathbf{e}_t$ from a multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$. The time series are then generated iteratively, starting from $\\mathbf{z}_0 = [0, 0]^\\top$. A burn-in period of $B$ samples is generated and discarded for each trial to ensure the process reaches its stationary distribution before collecting the $T$ samples for analysis.\n\n### 2. VAR(1) Parameter Estimation\n\nGiven $N$ trials of paired time series data $\\{x_t^{(i)}, y_t^{(i)}\\}_{t=1..T}^{i=1..N}$, the parameters $\\hat{A}_1$ and $\\hat{\\Sigma}$ are estimated. To maximize statistical power, data from all trials are pooled. The model for estimation is the full bivariate VAR($1$), so $\\hat{A}_1$ is a full $2 \\times 2$ matrix.\nThe regression equation is $\\mathbf{Z}_\\text{target} = A_1 \\mathbf{Z}_\\text{predictor} + \\mathbf{E}$, where $\\mathbf{Z}_\\text{target}$ is a $2 \\times (N(T-1))$ matrix containing vectors $[\\mathbf{z}_1^{(1)}, \\dots, \\mathbf{z}_{T-1}^{(1)}, \\mathbf{z}_1^{(2)}, \\dots, \\mathbf{z}_{T-1}^{(N)}]$ and $\\mathbf{Z}_\\text{predictor}$ is a $2 \\times (N(T-1))$ matrix of the corresponding lagged vectors $[\\mathbf{z}_0^{(1)}, \\dots, \\mathbf{z}_{T-2}^{(1)}, \\mathbf{z}_0^{(2)}, \\dots, \\mathbf{z}_{T-2}^{(N)}]$.\n\nThe Ordinary Least Squares (OLS) estimator for $A_1$ is:\n$$\n\\hat{A}_1 = (\\mathbf{Z}_\\text{target} \\mathbf{Z}_\\text{predictor}^\\top) (\\mathbf{Z}_\\text{predictor} \\mathbf{Z}_\\text{predictor}^\\top)^{-1}\n$$\nThe residuals are computed as $\\hat{\\mathbf{E}} = \\mathbf{Z}_\\text{target} - \\hat{A}_1 \\mathbf{Z}_\\text{predictor}$. The innovations covariance matrix $\\Sigma$ is then estimated from the residuals:\n$$\n\\hat{\\Sigma} = \\frac{1}{N(T-1)} \\hat{\\mathbf{E}} \\hat{\\mathbf{E}}^\\top\n$$\n\n### 3. Frequency-Domain Granger Causality\n\nThe spectral representation of the VAR process is key to computing frequency-domain GC. The Fourier transform of the VAR($1$) equation yields:\n$$\n\\mathbf{Z}(f) = A_1 e^{-i\\omega} \\mathbf{Z}(f) + \\mathbf{E}(f) \\implies (\\mathbf{I} - A_1 e^{-i\\omega}) \\mathbf{Z}(f) = \\mathbf{E}(f)\n$$\nwhere $\\omega = 2\\pi f / f_s$ is the angular frequency for a given frequency $f$ in Hz. The transfer function matrix $H(f)$, which maps innovations to the observed process, is:\n$$\nH(f) = (\\mathbf{I} - \\hat{A}_1 e^{-i\\omega})^{-1}\n$$\nThe spectral density matrix $S(f)$ is then given by:\n$$\nS(f) = H(f) \\hat{\\Sigma} H(f)^*\n$$\nwhere $H(f)^*$ denotes the conjugate transpose of $H(f)$. The diagonal elements of $S(f)$ are the power spectra of $x_t$ and $y_t$.\n\nGranger causality from $x$ to $y$ at frequency $f$, denoted $GC_{x \\to y}(f)$, measures the fraction of power in $y_t$ at that frequency that is attributable to $x_t$. This is formally defined as the logarithm of the ratio of the total power of $y_t$ to its intrinsic power (the power not explained by $x_t$). A standard method to separate these components involves orthogonalizing the innovations via a Cholesky decomposition of the covariance matrix, $\\hat{\\Sigma} = G G^\\top$, where $G$ is lower triangular.\nThis yields an updated transfer function from orthogonal innovations $\\mathbf{W}(f)$ to the outputs: $\\tilde{H}(f) = H(f)G$. The spectral matrix can be written as $S(f) = \\tilde{H}(f) \\tilde{H}(f)^*$.\nThe power spectrum of $y_t$ (the second component, index $1$) is $S_{yy}(f) = |\\tilde{H}_{10}(f)|^2 + |\\tilde{H}_{11}(f)|^2$. The first term $|\\tilde{H}_{10}(f)|^2$ represents the power in $y$ derived from the orthogonal innovation of $x$, while the second term $|\\tilde{H}_{11}(f)|^2$ is the intrinsic power of $y$ derived from its own orthogonal innovation.\nThus, the GC spectrum is:\n$$\nGC_{x \\to y}(f) = \\ln \\left( \\frac{S_{yy}(f)}{\\text{Intrinsic Power}_y(f)} \\right) = \\ln \\left( \\frac{|\\tilde{H}_{10}(f)|^2 + |\\tilde{H}_{11}(f)|^2}{|\\tilde{H}_{11}(f)|^2} \\right) = \\ln \\left( 1 + \\frac{|\\tilde{H}_{10}(f)|^2}{|\\tilde{H}_{11}(f)|^2} \\right)\n$$\nThis computation is performed for $M$ discrete frequencies in the specified $[1, 40]$ Hz range.\n\n### 4. Statistical Significance Testing via Permutation\n\nTo assess statistical significance, we generate an empirical null distribution for the GC spectrum under the null hypothesis of no causal influence from $x$ to $y$. This is achieved by trial-wise permutation. The association between signals across trials is broken by randomly permuting the trial indices of the source signal, $x_t$, while keeping the trial order of the target signal, $y_t$, fixed. This preserves the auto-spectral properties and within-trial temporal structure of both signals but destroys any consistent trial-over-trial causal relationship.\n\nFor each of $P$ permutations, a permuted dataset is formed, the VAR($1$) model is re-estimated, and a null GC spectrum is computed. This procedure results in a set of $P$ null GC values for each frequency, forming the empirical null distribution at that frequency.\n\nThe empirical p-value for the observed GC value at each frequency $f_k$, $GC_\\text{obs}(f_k)$, is calculated as:\n$$\np(f_k) = \\frac{1 + \\sum_{j=1}^{P} \\mathbb{I}(GC_\\text{null}^{(j)}(f_k) \\ge GC_\\text{obs}(f_k))}{P + 1}\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. This formula prevents p-values of $0$ and is standard for permutation tests.\n\n### 5. Multiple Comparison Correction\n\nSince we perform a test at each of $M$ frequencies, we must correct for multiple comparisons to control the error rate over the family of tests.\n\n#### Benjamini–Hochberg False Discovery Rate (FDR)\nThis procedure controls the expected proportion of false positives among all rejected null hypotheses. For a desired FDR level $q$:\n1.  Sort the $M$ uncorrected p-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(M)}$.\n2.  Find the largest index $k$ such that $p_{(k)} \\le \\frac{k}{M} q$.\n3.  Reject the null hypotheses for all tests corresponding to p-values $p_{(1)}, \\dots, p_{(k)}$.\nThe number of significant frequencies is the count of rejected hypotheses.\n\n#### Max-Statistic Family-Wise Error Rate (FWER)\nThis method controls the probability of making at least one false positive discovery across all frequencies. It is more stringent than FDR.\n1.  For each of the $P$ permutations, compute the null GC spectrum and find its maximum value across all $M$ frequencies: $GC_\\text{max}^{(j)} = \\max_{k=1..M} \\{GC_\\text{null}^{(j)}(f_k)\\}$.\n2.  This yields a single null distribution of $P$ maximum-statistic values, $\\{GC_\\text{max}^{(j)}\\}_{j=1..P}$.\n3.  The FWER-corrected p-value for the observed GC at each frequency $f_k$ is computed against this max-statistic distribution:\n    $$\n    p_\\text{adj}(f_k) = \\frac{1 + \\sum_{j=1}^{P} \\mathbb{I}(GC_\\text{max}^{(j)} \\ge GC_\\text{obs}(f_k))}{P + 1}\n    $$\n4.  A frequency is declared significant if its adjusted p-value $p_\\text{adj}(f_k)$ is less than or equal to the FWER level $\\alpha$. The number of such frequencies is counted.\n\nThe final output is the pair of counts [number of FDR significant frequencies, number of FWER significant frequencies] for each test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef simulate_bivar_var1(N, T, B, A, Sigma, seed):\n    \"\"\"\n    Simulates N trials of a bivariate VAR(1) process.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    num_channels = 2\n    total_len = T + B\n    \n    data = np.zeros((N, num_channels, total_len))\n    \n    for i in range(N):\n        innovations = rng.multivariate_normal(np.zeros(num_channels), Sigma, size=total_len)\n        for t in range(1, total_len):\n            data[i, :, t] = A @ data[i, :, t-1] + innovations[t, :]\n            \n    return data[:, :, B:]\n\ndef estimate_var1_ols(data):\n    \"\"\"\n    Estimates VAR(1) parameters A and Sigma using OLS on pooled data.\n    data shape: (N, num_channels, T)\n    \"\"\"\n    N, num_channels, T = data.shape\n    num_points = N * (T - 1)\n\n    Y = data[:, :, 1:].transpose(1, 0, 2).reshape(num_channels, -1)\n    X = data[:, :, :-1].transpose(1, 0, 2).reshape(num_channels, -1)\n    \n    # OLS estimation: A = (Y @ X.T) @ inv(X @ X.T)\n    XX_T = X @ X.T\n    YX_T = Y @ X.T\n    \n    try:\n        A_hat = YX_T @ np.linalg.inv(XX_T)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if matrix is singular.\n        A_hat = YX_T @ np.linalg.pinv(XX_T)\n\n    residuals = Y - A_hat @ X\n    Sigma_hat = (residuals @ residuals.T) / num_points\n    \n    return A_hat, Sigma_hat\n\ndef compute_gc_spectrum(A, Sigma, freqs, fs):\n    \"\"\"\n    Computes the Granger Causality spectrum from x to y.\n    x is channel 0, y is channel 1.\n    \"\"\"\n    M = len(freqs)\n    gc_spectrum = np.zeros(M)\n    \n    try:\n        G = np.linalg.cholesky(Sigma)\n    except np.linalg.LinAlgError:\n        # If Sigma is not positive definite (can happen with estimation noise)\n        # return a zero spectrum.\n        return gc_spectrum\n\n    g_11 = G[0, 0]\n    g_21 = G[1, 0]\n    g_22 = G[1, 1]\n\n    # Pre-calculate components for GC formula that don't depend on frequency\n    # We use the formulation GC_xy = log(1 + |H_21*g11 + H_22*g21|^2 / |H_22*g22|^2)\n    # The term inside log simplifies to log( S_yy / S_yy_intrinsic )\n    # This reduces to evaluating H_21 and H_22 at each frequency\n    # H = (I - A*z)^-1\n    \n    for i, f in enumerate(freqs):\n        if f == 0:\n            gc_spectrum[i] = 0\n            continue\n        omega = 2 * np.pi * f / fs\n        z = np.exp(-1j * omega)\n        \n        I_minus_Az = np.eye(2) - A * z\n        try:\n            H = np.linalg.inv(I_minus_Az)\n        except np.linalg.LinAlgError:\n            continue\n            \n        H_21 = H[1, 0]\n        H_22 = H[1, 1]\n        \n        # Numerator: causal flow from x to y's orthogonalized innovations\n        num_term = H_21 * g_11 + H_22 * g_21\n        \n        # Denominator: intrinsic component from y's own orthogonalized innovation\n        den_term = H_22 * g_22\n        \n        if np.abs(den_term) > 1e-12: # Avoid division by zero\n            gc_spectrum[i] = np.log(1 + (np.abs(num_term)**2) / (np.abs(den_term)**2))\n\n    return gc_spectrum\n\ndef solve():\n    test_cases = [\n        # Case A\n        {'N': 40, 'T': 900, 'fs': 200, 'M': 64, 'freq_range': [1, 40],\n         'a_x': 0.5, 'a_y': 0.5, 'b': 0.6,\n         'sigma_x': 1.0, 'sigma_y': 1.0, 'rho': 0.0,\n         'B': 100, 'P': 150, 'q': 0.05, 'alpha': 0.05, 'seed': 0},\n        # Case B\n        {'N': 40, 'T': 900, 'fs': 200, 'M': 64, 'freq_range': [1, 40],\n         'a_x': 0.5, 'a_y': 0.5, 'b': 0.0,\n         'sigma_x': 1.0, 'sigma_y': 1.0, 'rho': 0.7,\n         'B': 100, 'P': 150, 'q': 0.05, 'alpha': 0.05, 'seed': 1},\n        # Case C\n        {'N': 20, 'T': 600, 'fs': 200, 'M': 64, 'freq_range': [1, 40],\n         'a_x': 0.5, 'a_y': 0.5, 'b': 0.2,\n         'sigma_x': 1.0, 'sigma_y': 1.0, 'rho': 0.0,\n         'B': 100, 'P': 100, 'q': 0.05, 'alpha': 0.05, 'seed': 2},\n    ]\n\n    all_results = []\n\n    for params in test_cases:\n        N, T, B, P = params['N'], params['T'], params['B'], params['P']\n        fs, M, q, alpha = params['fs'], params['M'], params['q'], params['alpha']\n        seed = params['seed']\n        rng = np.random.default_rng(seed)\n\n        A_sim = np.array([[params['a_x'], 0], [params['b'], params['a_y']]])\n        cov_xy = params['rho'] * params['sigma_x'] * params['sigma_y']\n        Sigma_sim = np.array([[params['sigma_x']**2, cov_xy], [cov_xy, params['sigma_y']**2]])\n        \n        freqs = np.linspace(params['freq_range'][0], params['freq_range'][1], M)\n\n        data = simulate_bivar_var1(N, T, B, A_sim, Sigma_sim, seed)\n        data_x, data_y = data[:, 0, :], data[:, 1, :]\n        \n        # 1. Observed GC\n        A_obs, Sigma_obs = estimate_var1_ols(data)\n        gc_obs = compute_gc_spectrum(A_obs, Sigma_obs, freqs, fs)\n\n        # 2. Permutation testing\n        null_gc_spectra = np.zeros((P, M))\n        max_null_gc = np.zeros(P)\n        \n        for p in range(P):\n            perm_indices = rng.permutation(N)\n            perm_data_x = data_x[perm_indices, :]\n            perm_data = np.stack([perm_data_x, data_y], axis=1)\n            \n            A_perm, Sigma_perm = estimate_var1_ols(perm_data)\n            gc_perm = compute_gc_spectrum(A_perm, Sigma_perm, freqs, fs)\n            \n            null_gc_spectra[p, :] = gc_perm\n            if gc_perm.size > 0:\n                max_null_gc[p] = np.max(gc_perm)\n\n        # 3. P-values\n        p_vals = (1 + np.sum(null_gc_spectra >= gc_obs, axis=0)) / (P + 1)\n        \n        # 4. FDR correction\n        sorted_indices = np.argsort(p_vals)\n        sorted_p_vals = p_vals[sorted_indices]\n        \n        fdr_thresholds = q * (np.arange(1, M + 1)) / M\n        significant_mask = sorted_p_vals = fdr_thresholds\n        \n        n_fdr = 0\n        if np.any(significant_mask):\n            k = np.where(significant_mask)[0].max()\n            n_fdr = k + 1\n\n        # 5. Max-statistic FWER correction\n        fwer_p_vals = (1 + np.sum(max_null_gc[:, np.newaxis] >= gc_obs, axis=0)) / (P + 1)\n        n_fwer = np.sum(fwer_p_vals = alpha)\n\n        all_results.append([n_fdr, n_fwer])\n    \n    print(f\"{all_results}\")\n\nsolve()\n```"
        },
        {
            "introduction": "Before applying a sophisticated analysis method like Granger causality to experimental data, it is imperative to validate its performance to ensure the results are trustworthy. This exercise puts you in the role of a methodologist, tasking you with building a validation pipeline from the ground up using synthetic data with known properties . Through this process, you will learn how to systematically quantify the accuracy, robustness, and specificity of your analysis, providing a blueprint for verifying any computational tool in your research.",
            "id": "4165340",
            "problem": "You are tasked with designing and implementing a complete validation approach for frequency-domain Granger causality using synthetic neural data with known ground truth, suitable for advanced graduate-level neuroscience data analysis. Your program must generate synthetic two-channel time series from a Multivariate Autoregressive (MVAR) model, estimate MVAR parameters, compute a frequency-domain Granger-causal measure, and quantify accuracy and robustness under different experimental conditions.\n\nStart from the following fundamental base:\n- The Granger causality principle: a process $x$ Granger-causes a process $y$ (in the sense of Clive Granger) if the past of $x$ improves the prediction of $y$ beyond the past of $y$ alone.\n- Multivariate Autoregressive (MVAR) modeling: a stationary $n$-variate discrete-time process $\\mathbf{X}_t$ can be modeled as $\\mathbf{X}_t = \\sum_{k=1}^{p} \\mathbf{A}_k \\mathbf{X}_{t-k} + \\mathbf{E}_t$, where $p$ is the model order, $\\mathbf{A}_k$ are coefficient matrices, and $\\mathbf{E}_t$ is the innovation process with covariance $\\boldsymbol{\\Sigma}$.\n- Frequency-domain measures that quantify directional influence derived from MVAR parameters are used in neuroscience to detect directed interactions at specific oscillatory frequencies.\n\nYour implementation must:\n1. Generate synthetic two-channel neural time series using a stable two-dimensional MVAR$(2)$ model with oscillatory dynamics. Use a sampling rate $f_s$ in hertz (Hz), and design auto-regressive coefficients to produce an oscillation at a specified frequency $f_0$ in Hz. Introduce a unidirectional coupling from channel $x$ to channel $y$ by adding off-diagonal coefficients in the first-lag autoregressive matrix. The innovation covariance must be positive definite and realistic.\n2. Estimate MVAR parameters from the generated data using ordinary least squares on stacked lagged regressors, and compute the innovation covariance.\n3. Derive a frequency-domain Granger-causal measure based on the MVAR representation. This measure must reflect directional influence as a function of frequency and be computable from the estimated MVAR parameters without refitting reduced models.\n4. Validate accuracy by checking whether the dominant directed influence from $x$ to $y$ concentrates near the known ground-truth oscillation frequency $f_0$.\n5. Validate robustness by bootstrapping over segments and quantifying the proportion of resamples that identify the correct direction within a target frequency band.\n6. Validate specificity by quantifying false positive detection in a null model with no coupling.\n\nYour program must implement the following pipeline steps:\n- Synthetic data generation: simulate from an MVAR$(2)$ model for two channels $(x, y)$ with oscillatory autoregressive dynamics centered at $f_0$ Hz, and a coupling from $x$ to $y$ at lag $1$ of strength $c$. Ensure model stability and perform a burn-in period to remove transients.\n- Parameter estimation: fit the MVAR$(p)$ model to the data using least squares and compute the innovation covariance $\\boldsymbol{\\Sigma}$.\n- Frequency-domain analysis: compute a directional frequency-domain measure across a frequency grid $f \\in [1, f_s/2]$ Hz with uniform resolution of $1$ Hz.\n- Accuracy metric: compute the absolute error (in Hz) between the peak frequency of the directed influence from $x$ to $y$ and the ground-truth oscillation frequency $f_0$.\n- Robustness metric: perform bootstrap resampling by segments and compute the fraction (as a decimal) of resamples where the integrated directed influence from $x$ to $y$ within the band $[f_0-5, f_0+5]$ Hz exceeds that of the reverse direction.\n- Specificity metric: for a null coupling case, compute whether the maximum directed influence across all frequencies for either direction is below a given threshold (return a boolean).\n\nTest suite:\nImplement the following three test cases to comprehensively assess the pipeline:\n- Case A (happy path): $f_s = 200$ Hz, $f_0 = 40$ Hz, $p = 2$, coupling $c = 0.25$, noise standard deviations for both channels $0.5$, time series length $N = 20000$ samples, burn-in $= 1000$ samples. Output the absolute peak frequency error in Hz (float).\n- Case B (robustness under noise and short data): $f_s = 200$ Hz, $f_0 = 40$ Hz, $p = 2$, coupling $c = 0.25$, noise standard deviations for both channels $1.0$, time series length $N = 5000$ samples, burn-in $= 1000$ samples, number of bootstrap resamples $B = 50$, segment length $L = 500$ samples. Output the robustness fraction as a decimal (float).\n- Case C (null model for specificity): $f_s = 200$ Hz, $f_0 = 40$ Hz, $p = 2$, coupling $c = 0.0$, noise standard deviations for both channels $0.5$, time series length $N = 20000$ samples, burn-in $= 1000$ samples, detection threshold $T = 0.25$. Output whether the maximum directed influence for either direction stays below $T$ (boolean).\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). The three results must correspond to Case A, Case B, and Case C, respectively. The first result must be a float in hertz (Hz), the second a float as a decimal fraction, and the third a boolean.",
            "solution": "The problem of validating a frequency-domain Granger causality measure is addressed by implementing a comprehensive simulation and analysis pipeline. The process involves generating synthetic data from a known ground-truth model, estimating the model parameters from the data, computing the causality measure, and quantifying its performance against the known ground truth. This approach is fundamental in computational neuroscience for ensuring that an analysis method is accurate, robust, and specific before applying it to real experimental data where the underlying connectivity is unknown.\n\nThe solution is structured into four main components:\n1.  **Synthetic Data Generation**: A two-channel multivariate autoregressive (MVAR) model of order $p=2$ serves as the ground-truth data generating process.\n2.  **MVAR Parameter Estimation**: The model parameters are estimated from the generated time series using ordinary least squares (OLS).\n3.  **Frequency-Domain Causality Computation**: Partial Directed Coherence (PDC) is chosen as the frequency-domain measure of directional influence.\n4.  **Performance Metric Calculation**: Accuracy, robustness, and specificity are quantified using the metrics specified in the problem statement.\n\n**1. Synthetic Data Generation: MVAR(2) Model**\n\nA a two-channel, or bivariate, discrete-time process $\\mathbf{X}_t = [x_t, y_t]^T$ is modeled by an MVAR($p$) process:\n$$\n\\mathbf{X}_t = \\sum_{k=1}^{p} \\mathbf{A}_k \\mathbf{X}_{t-k} + \\mathbf{E}_t\n$$\nwhere $p$ is the model order, $\\mathbf{A}_k$ are $2 \\times 2$ coefficient matrices, and $\\mathbf{E}_t = [\\epsilon_{x,t}, \\epsilon_{y,t}]^T$ is a vector of uncorrelated white noise innovations drawn from a zero-mean Gaussian distribution with a diagonal covariance matrix $\\boldsymbol{\\Sigma} = \\text{diag}(\\sigma_x^2, \\sigma_y^2)$.\n\nTo introduce oscillatory dynamics at a specific frequency $f_0$, the poles of the system's transfer function are placed near the unit circle at an angle corresponding to $f_0$. For a stable second-order autoregressive process, the coefficients are designed based on a pole pair $z = r e^{\\pm i\\omega_0}$, where $\\omega_0 = 2\\pi f_0 / f_s$ is the normalized angular frequency and $r  1$ is a damping factor controlling the sharpness of the oscillation (values closer to $1$ yield sharper peaks). This leads to diagonal elements of the coefficient matrices:\n- $a_{ii}^{(1)} = 2r \\cos(\\omega_0)$\n- $a_{ii}^{(2)} = -r^2$\n\nFor this problem, these coefficients are set for both the $x$ and $y$ processes' intrinsic dynamics. A unidirectional causal link from $x$ to $y$ is introduced by setting the off-diagonal element $a_{21}^{(1)} = c$, where $c$ is the coupling strength. All other off-diagonal elements are set to $0$. The coefficient matrices for a system with coupling $x \\to y$ are:\n$$\n\\mathbf{A}_1 = \\begin{pmatrix} 2r \\cos(\\omega_0)  0 \\\\ c  2r \\cos(\\omega_0) \\end{pmatrix}, \\quad \\mathbf{A}_2 = \\begin{pmatrix} -r^2  0 \\\\ 0  -r^2 \\end{pmatrix}\n$$\nThe stability of the resulting MVAR model is critical and is verified by confirming that all eigenvalues of its companion matrix $\\mathbf{F}$ have a magnitude less than $1$. For a bivariate MVAR(2) model, the $4 \\times 4$ companion matrix is:\n$$\n\\mathbf{F} = \\begin{pmatrix} \\mathbf{A}_1  \\mathbf{A}_2 \\\\ \\mathbf{I}  \\mathbf{0} \\end{pmatrix}\n$$\nwhere $\\mathbf{I}$ is the $2 \\times 2$ identity matrix and $\\mathbf{0}$ is the $2 \\times 2$ zero matrix. Data are generated iteratively using the MVAR equation, and an initial burn-in period is discarded to ensure the process has reached stationarity.\n\n**2. MVAR Parameter Estimation**\n\nThe MVAR model coefficients are estimated from the generated time series $\\mathbf{X}$ of length $N$. The model equation can be written in a linear regression form $\\mathbf{Y} = \\mathbf{B} \\mathbf{Z} + \\text{errors}$, where:\n- $\\mathbf{B} = [\\mathbf{A}_1, \\mathbf{A}_2, \\dots, \\mathbf{A}_p]$ is the $n \\times np$ matrix of all coefficients.\n- $\\mathbf{Y} = [\\mathbf{X}_{p+1}, \\mathbf{X}_{p+2}, \\dots, \\mathbf{X}_N]$ is the $n \\times (N-p)$ matrix of dependent variables.\n- $\\mathbf{Z}$ is the $np \\times (N-p)$ matrix of regressors, constructed from lagged data:\n$$\n\\mathbf{Z}_t = [\\mathbf{X}_{t-1}^T, \\mathbf{X}_{t-2}^T, \\dots, \\mathbf{X}_{t-p}^T]^T \\quad \\text{for } t=p+1, \\dots, N\n$$\nThe ordinary least squares (OLS) estimate of $\\mathbf{B}$ is given by $\\hat{\\mathbf{B}} = \\mathbf{Y} \\mathbf{Z}^T (\\mathbf{Z} \\mathbf{Z}^T)^{-1}$. This is solved efficiently using numerical linear algebra routines. Once $\\hat{\\mathbf{B}}$ is found, the estimated residuals $\\hat{\\mathbf{E}}_t = \\mathbf{X}_t - \\sum_{k=1}^p \\hat{\\mathbf{A}}_k \\mathbf{X}_{t-k}$ are calculated, and their covariance matrix is estimated as $\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N-p} \\sum_{t=p+1}^{N} \\hat{\\mathbf{E}}_t \\hat{\\mathbf{E}}_t^T$.\n\n**3. Frequency-Domain Causality: Partial Directed Coherence (PDC)**\n\nThe problem requires a frequency-domain Granger-causal measure that can be computed from the MVAR parameters without refitting reduced models. Partial Directed Coherence (PDC) satisfies this requirement and is a well-established method for identifying direct causal influences in multivariate systems.\n\nPDC is derived from the frequency-domain representation of the MVAR model. Let $\\mathbf{A}(f)$ be the Fourier transform of the coefficient matrices:\n$$\n\\mathbf{A}(f) = \\mathbf{I} - \\sum_{k=1}^{p} \\mathbf{A}_k e^{-i 2\\pi f k / f_s}\n$$\nThe element $A_{ij}(f)$ of this matrix represents the linear influence of channel $j$ on channel $i$ at frequency $f$. The PDC from channel $j$ to channel $i$, denoted $\\pi_{ij}(f)$, is defined by normalizing this influence by the total influence emanating from channel $j$:\n$$\n\\pi_{ij}(f) = \\frac{A_{ij}(f)}{\\sqrt{\\sum_{k=1}^{n} |A_{kj}(f)|^2}}\n$$\nThe squared modulus, $|\\pi_{ij}(f)|^2$, is used as the Granger-causal measure. It is a value between $0$ and $1$ that reflects the proportion of directed influence from channel $j$ to channel $i$ at frequency $f$ relative to the total influence from channel $j$ to all channels (including itself). For our two-channel system ($x=1$, $y=2$), we compute $|\\pi_{21}(f)|^2$ for the influence $x \\to y$ and $|\\pi_{12}(f)|^2$ for $y \\to x$.\n\n**4. Performance Metrics**\n\nThe pipeline is validated against the following three metrics:\n- **Accuracy (Case A)**: After generating data with a known coupling $x \\to y$ and oscillation at $f_0=40$ Hz, the PDC spectrum $|\\pi_{21}(f)|^2$ is computed. The accuracy is the absolute error $|f_{\\text{peak}} - f_0|$, where $f_{\\text{peak}}$ is the frequency at which the PDC from $x \\to y$ reaches its maximum.\n- **Robustness (Case B)**: This metric assesses performance with shorter, noisier data. The time series is divided into non-overlapping segments. Bootstrap resampling is performed by creating new time series by sampling these segments with replacement. For each resample, the PDC is calculated, and the integrated power in a band around $f_0$ ($[35, 45]$ Hz) is compared for the two directions ($x \\to y$ vs. $y \\to x$). The robustness is the fraction of bootstraps for which the influence in the correct direction ($x \\to y$) is stronger.\n- **Specificity (Case C)**: To test for false positives, data is generated from a null model with no coupling between channels ($c=0$). The PDC is calculated for both directions. Specificity is confirmed if the maximum PDC value across all frequencies and both directions remains below a predefined threshold $T$. The function returns a boolean value indicating if this condition is met.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef design_mvar_coeffs(f0, fs, r, c):\n    \"\"\"\n    Designs MVAR(2) coefficients for a 2-channel system with oscillatory dynamics.\n    \n    Args:\n        f0 (float): Target oscillation frequency in Hz.\n        fs (float): Sampling frequency in Hz.\n        r (float): Damping factor for oscillations (pole radius).\n        c (float): Coupling strength from channel 1 to 2 at lag 1.\n\n    Returns:\n        list: A list of two 2x2 numpy arrays [A1, A2].\n    \"\"\"\n    w0 = 2 * np.pi * f0 / fs\n    a1_diag = 2 * r * np.cos(w0)\n    a2_diag = -r**2\n\n    A1 = np.array([\n        [a1_diag, 0.0],\n        [c, a1_diag]\n    ])\n    A2 = np.array([\n        [a2_diag, 0.0],\n        [0.0, a2_diag]\n    ])\n    return [A1, A2]\n\ndef check_stability(A_coeffs):\n    \"\"\"\n    Checks the stability of an MVAR model.\n    A_coeffs is a list of coefficient matrices [A1, A2, ..., Ap].\n    \"\"\"\n    n, _ = A_coeffs[0].shape\n    p = len(A_coeffs)\n    \n    # Companion matrix\n    F = np.zeros((n * p, n * p))\n    for i in range(p):\n        F[:n, i*n:(i+1)*n] = A_coeffs[i]\n    for i in range(p - 1):\n        F[(i+1)*n:(i+2)*n, i*n:(i+1)*n] = np.eye(n)\n        \n    eigenvalues = np.linalg.eigvals(F)\n    return np.all(np.abs(eigenvalues)  1)\n\ndef generate_mvar_data(N, A_coeffs, sigma_cov, burn_in):\n    \"\"\"\n    Generates synthetic data from a stable MVAR model.\n    \"\"\"\n    if not check_stability(A_coeffs):\n        raise ValueError(\"MVAR model is not stable.\")\n\n    n, _ = A_coeffs[0].shape\n    p = len(A_coeffs)\n    total_samples = N + burn_in\n\n    # Generate noise\n    E = np.random.multivariate_normal(np.zeros(n), sigma_cov, total_samples).T\n    \n    # Initialize time series\n    X = np.zeros((n, total_samples))\n    \n    # Generate data\n    for t in range(p, total_samples):\n        for k in range(p):\n            X[:, t] += A_coeffs[k] @ X[:, t - (k + 1)]\n        X[:, t] += E[:, t]\n        \n    return X[:, burn_in:]\n\ndef fit_mvar(X, p):\n    \"\"\"\n    Fits an MVAR(p) model to time series data X using OLS.\n    X is an n x N matrix (channels x time).\n    \"\"\"\n    n, N = X.shape\n    \n    # Create regressor matrix Z and target matrix Y\n    Y = X[:, p:]\n    Z = np.zeros((n * p, N - p))\n    for t in range(p, N):\n        for k in range(p):\n            Z[k*n:(k+1)*n, t-p] = X[:, t - (k + 1)]\n\n    # Solve for coefficients using least squares\n    # We solve Y = B @ Z => Y.T = Z.T @ B.T\n    B_T, _, _, _ = np.linalg.lstsq(Z.T, Y.T, rcond=None)\n    B = B_T.T\n\n    # Reshape B into A_k matrices\n    A_coeffs_est = [B[:, k*n:(k+1)*n] for k in range(p)]\n\n    # Compute innovation covariance\n    E_est = Y - B @ Z\n    Sigma_est = (E_est @ E_est.T) / (N - p)\n    \n    return A_coeffs_est, Sigma_est\n\ndef compute_pdc(A_coeffs, fs):\n    \"\"\"\n    Computes Partial Directed Coherence (PDC) from MVAR coefficients.\n    \"\"\"\n    n, _ = A_coeffs[0].shape\n    p = len(A_coeffs)\n    \n    freqs = np.arange(1, fs / 2 + 1)\n    pdc_12 = np.zeros(len(freqs)) # y -> x\n    pdc_21 = np.zeros(len(freqs)) # x -> y\n\n    for i, f in enumerate(freqs):\n        Af = np.eye(n, dtype=complex)\n        for k in range(p):\n            Af -= A_coeffs[k] * np.exp(-1j * 2 * np.pi * f * (k + 1) / fs)\n\n        # Denominators for PDC (norm of columns of Af)\n        col_norms = np.sum(np.abs(Af)**2, axis=0)\n\n        # PDC from j to i is |Aij|^2 / sum_k(|Akj|^2)\n        # We need PDC x->y (1->2) and y->x (2->1)\n        if col_norms[0] > 0:\n            pdc_21[i] = np.abs(Af[1, 0])**2 / col_norms[0]\n        if col_norms[1] > 0:\n            pdc_12[i] = np.abs(Af[0, 1])**2 / col_norms[1]\n\n    return freqs, pdc_12, pdc_21\n\ndef solve():\n    np.random.seed(42) # For reproducibility\n    results = []\n    \n    # --- Case A: Accuracy ---\n    fs_a = 200.0\n    f0_a = 40.0\n    p_a = 2\n    c_a = 0.25\n    noise_std_a = 0.5\n    N_a = 20000\n    burn_in_a = 1000\n    r_a = 0.98\n\n    A_coeffs_true_a = design_mvar_coeffs(f0_a, fs_a, r_a, c_a)\n    sigma_cov_true_a = np.diag([noise_std_a**2, noise_std_a**2])\n    X_a = generate_mvar_data(N_a, A_coeffs_true_a, sigma_cov_true_a, burn_in_a)\n    A_coeffs_est_a, _ = fit_mvar(X_a, p_a)\n    freqs_a, _, pdc_21_a = compute_pdc(A_coeffs_est_a, fs_a)\n    peak_freq_a = freqs_a[np.argmax(pdc_21_a)]\n    error_a = float(abs(peak_freq_a - f0_a))\n    results.append(error_a)\n\n    # --- Case B: Robustness ---\n    fs_b = 200.0\n    f0_b = 40.0\n    p_b = 2\n    c_b = 0.25\n    noise_std_b = 1.0\n    N_b = 5000\n    burn_in_b = 1000\n    B_b = 50\n    L_b = 500\n    r_b = 0.98\n\n    A_coeffs_true_b = design_mvar_coeffs(f0_b, fs_b, r_b, c_b)\n    sigma_cov_true_b = np.diag([noise_std_b**2, noise_std_b**2])\n    X_b = generate_mvar_data(N_b, A_coeffs_true_b, sigma_cov_true_b, burn_in_b)\n    \n    n_segments = N_b // L_b\n    segments = [X_b[:, i*L_b:(i+1)*L_b] for i in range(n_segments)]\n    \n    correct_direction_count = 0\n    freq_band = (freqs_a >= f0_b - 5)  (freqs_a = f0_b + 5)\n\n    for _ in range(B_b):\n        bootstrap_indices = np.random.choice(n_segments, size=n_segments, replace=True)\n        X_boot = np.hstack([segments[i] for i in bootstrap_indices])\n        \n        A_coeffs_est_b, _ = fit_mvar(X_boot, p_b)\n        _, pdc_12_b, pdc_21_b = compute_pdc(A_coeffs_est_b, fs_b)\n        \n        integ_21 = np.sum(pdc_21_b[freq_band])\n        integ_12 = np.sum(pdc_12_b[freq_band])\n        \n        if integ_21 > integ_12:\n            correct_direction_count += 1\n            \n    robustness_b = float(correct_direction_count / B_b)\n    results.append(robustness_b)\n\n    # --- Case C: Specificity ---\n    fs_c = 200.0\n    f0_c = 40.0\n    p_c = 2\n    c_c = 0.0 # Null model\n    noise_std_c = 0.5\n    N_c = 20000\n    burn_in_c = 1000\n    T_c = 0.25\n    r_c = 0.98\n\n    A_coeffs_true_c = design_mvar_coeffs(f0_c, fs_c, r_c, c_c)\n    sigma_cov_true_c = np.diag([noise_std_c**2, noise_std_c**2])\n    X_c = generate_mvar_data(N_c, A_coeffs_true_c, sigma_cov_true_c, burn_in_c)\n    A_coeffs_est_c, _ = fit_mvar(X_c, p_c)\n    _, pdc_12_c, pdc_21_c = compute_pdc(A_coeffs_est_c, fs_c)\n    \n    max_spurious_pdc = max(np.max(pdc_12_c), np.max(pdc_21_c))\n    specificity_c = bool(max_spurious_pdc  T_c)\n    results.append(specificity_c)\n    \n    # Final print statement\n    print(f\"[{results[0]},{results[1]},{str(results[2]).lower()}]\")\n\n\nsolve()\n```"
        }
    ]
}