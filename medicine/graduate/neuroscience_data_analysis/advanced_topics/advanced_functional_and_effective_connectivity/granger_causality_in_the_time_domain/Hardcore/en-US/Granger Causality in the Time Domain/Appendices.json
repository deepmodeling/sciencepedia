{
    "hands_on_practices": [
        {
            "introduction": "The first step in testing for Granger causality is to translate the conceptual hypothesis into a precise statistical framework. This exercise requires you to construct the specific linear models—unrestricted and restricted—that represent the presence and absence of a conditional causal link. By determining the exact number of parameter constraints, you will solidify your understanding of how a hypothesis test is structured within a vector autoregressive (VAR) context, a crucial skill for applying and interpreting Granger causality analysis .",
            "id": "4166695",
            "problem": "You are analyzing simultaneous recordings from cortical area M during a visuomotor task. Let $y_{t} \\in \\mathbb{R}$ denote the firing rate of a single neuron $Y$, let $x_{t} \\in \\mathbb{R}^{2}$ collect the firing rates of two upstream neurons $X_{1}$ and $X_{2}$, and let $z_{t} \\in \\mathbb{R}^{3}$ contain three local field potential features extracted from the same area. Assume $\\{(y_{t}, x_{t}^{\\top}, z_{t}^{\\top})^{\\top}\\}$ is a zero-mean, covariance-stationary, linear, Gaussian, discrete-time stochastic process sampled uniformly in time. A lag order of $p=4$ is selected for modeling the conditional mean of $y_{t}$ from past information.\n\nStarting only from the definition that, in the time domain, $x$ does not Granger-cause $y$ conditional on $z$ if and only if the optimal linear predictor of $y_{t}$ based on $\\sigma$-algebras generated by the past of $(y,z)$ up to time $t-1$ is equal to the optimal linear predictor of $y_{t}$ based on the past of $(y,x,z)$ up to time $t-1$, proceed as follows:\n\n1. Construct the unrestricted linear model for the conditional mean of $y_{t}$ given its own past and the past of $(x_{t}, z_{t})$ up to lag $p=4$. Your construction must clearly indicate how $y_{t}$ depends on its own lags, the lags of $x_{t}$, and the lags of $z_{t}$.\n2. Construct the restricted linear model for the same conditional mean under the null hypothesis that $x$ does not Granger-cause $y$ conditional on $z$.\n3. Treating the coefficients on lagged $x_{t}$ in the $y_{t}$ equation as the only parameters constrained under the null, determine the total number of linearly independent equality restrictions that define the restricted model relative to the unrestricted model.\n\nGive your final answer as a single integer equal to the number of restrictions. No data fitting is required. There are no deterministic trends or other exogenous regressors beyond an intercept, and all models include the same intercept term. Assume no contemporaneous regressors are used, only lags $1$ through $4$. The final answer must be a single real-valued number with no units. No rounding is required.",
            "solution": "The problem requires the formulation of two linear models for a scalar time series $y_t$ and the determination of the number of parameter restrictions that distinguish them. This is a standard procedure for testing for Granger causality in the time domain using a vector autoregressive (VAR) framework.\n\nThe provided variables are:\n- $y_t \\in \\mathbb{R}$: the target variable, a scalar time series.\n- $x_t \\in \\mathbb{R}^2$: a vector time series of potential causes, with dimension $k_x = 2$. We denote its components by $x_{1,t}$ and $x_{2,t}$.\n- $z_t \\in \\mathbb{R}^3$: a vector time series of conditioning variables, with dimension $k_z = 3$. We denote its components by $z_{1,t}$, $z_{2,t}$, and $z_{3,t}$.\n\nThe model uses a lag order of $p=4$. All models are linear and include an intercept term $c$.\n\n1.  **Unrestricted Model Construction**\n\nThe unrestricted model describes the conditional mean of $y_t$ given the past of all variables in the system up to lag $p$. The information set at time $t-1$ is $\\mathcal{F}_{t-1}^{y,x,z} = \\{y_{t-j}, x_{t-j}, z_{t-j}\\}_{j=1}^{p}$. The optimal linear predictor of $y_t$ is its conditional expectation, which we model as a linear function of the variables in this information set.\n\nThe unrestricted autoregressive model for $y_t$ is:\n$$\ny_t = c + \\sum_{j=1}^{p} \\alpha_j y_{t-j} + \\sum_{j=1}^{p} \\beta_j^{\\top} x_{t-j} + \\sum_{j=1}^{p} \\gamma_j^{\\top} z_{t-j} + \\varepsilon_t\n$$\nwhere $\\varepsilon_t$ is a zero-mean, white-noise error term.\n\nGiven $p=4$, $x_t \\in \\mathbb{R}^2$, and $z_t \\in \\mathbb{R}^3$, we can expand this expression to be more explicit. The coefficients are:\n- $c$: a scalar intercept.\n- $\\{\\alpha_j\\}_{j=1}^4$: $4$ scalar coefficients for the lags of $y_t$.\n- $\\{\\beta_j\\}_{j=1}^4$: $4$ coefficient vectors, where each $\\beta_j \\in \\mathbb{R}^2$. We can write $\\beta_j = (\\beta_{j,1}, \\beta_{j,2})^{\\top}$.\n- $\\{\\gamma_j\\}_{j=1}^4$: $4$ coefficient vectors, where each $\\gamma_j \\in \\mathbb{R}^3$. We can write $\\gamma_j = (\\gamma_{j,1}, \\gamma_{j,2}, \\gamma_{j,3})^{\\top}$.\n\nThe full unrestricted model is:\n$$\ny_t = c + \\sum_{j=1}^{4} \\alpha_j y_{t-j} + \\sum_{j=1}^{4} \\left( \\beta_{j,1} x_{1, t-j} + \\beta_{j,2} x_{2, t-j} \\right) + \\sum_{j=1}^{4} \\left( \\gamma_{j,1} z_{1, t-j} + \\gamma_{j,2} z_{2, t-j} + \\gamma_{j,3} z_{3, t-j} \\right) + \\varepsilon_t\n$$\n\nThis model represents the conditional mean of $y_t$ based on the full history of $(y, x, z)$ up to lag $p=4$.\n\n2.  **Restricted Model Construction**\n\nThe null hypothesis $H_0$ is that \"$x$ does not Granger-cause $y$ conditional on $z$\". The problem defines this condition as the equivalence of two optimal linear predictors: the one based on the past of $(y, z)$ and the one based on the past of $(y, x, z)$.\n\nIn the context of the linear model, this means that the past values of $x_t$ provide no additional predictive power for $y_t$ once the past values of $y_t$ and $z_t$ are already included in the model. This is equivalent to stating that all coefficients associated with the lagged variables of $x_t$ in the unrestricted model are zero.\n\nThe null hypothesis imposes the following restrictions on the parameters of the unrestricted model:\n$$\nH_0: \\beta_j = \\mathbf{0} \\quad \\text{for all } j \\in \\{1, 2, 3, 4\\}\n$$\nwhere $\\mathbf{0}$ is the zero vector in $\\mathbb{R}^2$. This is equivalent to setting each component of each $\\beta_j$ vector to zero:\n$$\n\\beta_{j,k} = 0 \\quad \\text{for all } j \\in \\{1, 2, 3, 4\\} \\text{ and } k \\in \\{1, 2\\}\n$$\n\nApplying these restrictions to the unrestricted model yields the restricted model:\n$$\ny_t = c + \\sum_{j=1}^{4} \\alpha_j y_{t-j} + \\sum_{j=1}^{4} \\gamma_j^{\\top} z_{t-j} + \\nu_t\n$$\nwhere $\\nu_t$ is the error term of the restricted model. This model predicts $y_t$ using only its own past and the past of the conditioning variable $z_t$.\n\n3.  **Determination of the Number of Restrictions**\n\nThe task is to determine the total number of linearly independent equality restrictions imposed on the unrestricted model to obtain the restricted model. These restrictions are precisely the coefficients of the lagged $x_t$ terms that are set to zero under the null hypothesis.\n\nThe lagged variables from the process $x_t$ included in the unrestricted model are:\n- At lag $j=1$: $x_{1,t-1}, x_{2,t-1}$\n- At lag $j=2$: $x_{1,t-2}, x_{2,t-2}$\n- At lag $j=3$: $x_{1,t-3}, x_{2,t-3}$\n- At lag $j=4$: $x_{1,t-4}, x_{2,t-4}$\n\nThe dimension of the vector $x_t$ is $k_x = 2$.\nThe number of lags is $p = 4$.\n\nFor each lag $j$ from $1$ to $4$, there are $k_x=2$ coefficients ($\\beta_{j,1}$ and $\\beta_{j,2}$) that are constrained to be zero. The total number of restrictions is the product of the number of lags and the dimension of the $x_t$ process.\n\nNumber of restrictions = (dimension of $x_t$) $\\times$ (number of lags)\n$$\n\\text{Number of restrictions} = k_x \\times p = 2 \\times 4 = 8\n$$\n\nThese $8$ restrictions are:\n- For lag $j=1$: $\\beta_{1,1} = 0$, $\\beta_{1,2} = 0$\n- For lag $j=2$: $\\beta_{2,1} = 0$, $\\beta_{2,2} = 0$\n- For lag $j=3$: $\\beta_{3,1} = 0$, $\\beta_{3,2} = 0$\n- For lag $j=4$: $\\beta_{4,1} = 0$, $\\beta_{4,2} = 0$\n\nEach of these $8$ equations constrains a unique parameter that is otherwise freely estimated in the unrestricted model. Therefore, they constitute $8$ linearly independent equality restrictions.",
            "answer": "$$\\boxed{8}$$"
        },
        {
            "introduction": "One of the most significant challenges in causal inference is the presence of unobserved confounding variables. This practice provides a hands-on simulation to demonstrate how a hidden common driver can generate spurious Granger causality between two observed time series, a frequent pitfall in analyzing neural data. By implementing the standard parametric F-test and comparing pairwise versus conditional causality results, you will gain practical insight into identifying and correcting for such confounding effects .",
            "id": "4166623",
            "problem": "You are tasked with designing and executing a numerical simulation to study Granger causality in the time domain for neuroscientific time series. The objective is to demonstrate that a hidden common input can induce spurious pairwise Granger causality between two observed variables, and that conditioning on the hidden driver removes this spurious effect. You must write a complete program that simulates trivariate time series data with specified autoregressive dependencies and computes pairwise and conditional Granger causality using linear regression and the classical $F$-test for nested models.\n\nFundamental base:\n- Granger causality is defined in terms of predictive capability: a time series $x_t$ is said not to Granger-cause $y_t$ if, for all positive integers $p$, past values $\\{x_{t-1},\\dots,x_{t-p}\\}$ do not improve the prediction of $y_t$ beyond what is achievable using $\\{y_{t-1},\\dots,y_{t-p}\\}$ alone. Formally, it is tested via nested linear models using Ordinary Least Squares (OLS) regression and the $F$-test.\n- Consider a vector autoregressive setting where $z_t$ is an unobserved (or observed) common driver influencing both $x_t$ and $y_t$. Spurious pairwise Granger causality may arise solely due to the driver’s lag structure, even in the absence of a direct causal pathway from $x_t$ to $y_t$ or vice versa. Conditioning on $z_t$ can resolve the confound by comparing the restricted model that includes $y_t$ and $z_t$ past to the full model that additionally includes $x_t$ past.\n\nData-generating model:\n- Simulate three scalar time series, $x_t$, $y_t$, and $z_t$, all of length $T$, from a stable autoregressive process of order $p$ with Gaussian innovations.\n- The driver $z_t$ follows\n$$\nz_t = a_{z,1} z_{t-1} + a_{z,2} z_{t-2} + \\varepsilon_{z,t},\n$$\nwhile the observed processes follow\n$$\nx_t = a_{x,1} x_{t-1} + a_{x,2} x_{t-2} + b_{x,1} z_{t-1} + b_{x,2} z_{t-2} + \\varepsilon_{x,t},\n$$\n$$\ny_t = a_{y,1} y_{t-1} + a_{y,2} y_{t-2} + b_{y,1} z_{t-1} + b_{y,2} z_{t-2} + c_{xy,1} x_{t-1} + c_{xy,2} x_{t-2} + \\varepsilon_{y,t},\n$$\nwhere $\\varepsilon_{x,t}$, $\\varepsilon_{y,t}$, and $\\varepsilon_{z,t}$ are independent, identically distributed Gaussian noise terms with zero mean and specified variances. In this problem, all direct $y \\to x$ couplings are set to zero, and the parameters $c_{xy,1}$ and $c_{xy,2}$ represent potential direct $x \\to y$ influence.\n\nGranger causality testing:\n- For a chosen lag order $p$, define the restricted model for predicting $y_t$ using the past $\\{y_{t-1},\\dots,y_{t-p}\\}$ (and optionally the driver’s past in conditional tests), and the unrestricted model that additionally includes the past of $x_t$, $\\{x_{t-1},\\dots,x_{t-p}\\}$.\n- Fit both models by OLS, compute the residual sum of squares $\\mathrm{RSS}_{\\mathrm{r}}$ and $\\mathrm{RSS}_{\\mathrm{u}}$ for restricted and unrestricted models, respectively, and evaluate the $F$-statistic\n$$\nF = \\frac{\\left(\\mathrm{RSS}_{\\mathrm{r}} - \\mathrm{RSS}_{\\mathrm{u}}\\right)/m}{\\mathrm{RSS}_{\\mathrm{u}}/(N - k_{\\mathrm{u}})},\n$$\nwhere $m$ is the number of added parameters (the number of $x_t$ lag terms), $N$ is the number of usable time points, and $k_{\\mathrm{u}}$ is the number of parameters in the unrestricted model (including the intercept). Under the null hypothesis of no Granger causality, $F$ follows a Fisher–Snedecor distribution with $m$ and $N - k_{\\mathrm{u}}$ degrees of freedom.\n- The same construction applies for testing $x_t$ Granger-causing $y_t$ pairwise (without $z_t$ in the regressors) and conditionally (including $\\{z_{t-1},\\dots,z_{t-p}\\}$ in both restricted and unrestricted models). Similarly, to test $y_t$ potentially Granger-causing $x_t$, swap the target and predictor roles.\n\nProgram requirements:\n- Implement the simulation and Granger causality testing from first principles using OLS with a constant intercept term; only the Python Standard Library, NumPy, and SciPy are permitted.\n- Use a burn-in of $500$ samples to minimize transients before analyzing the last $T$ samples.\n- Use a fixed significance level $\\alpha = 0.01$; declare Granger causality present if the $p$-value is strictly less than $\\alpha$.\n- Angle units are not applicable; no physical units are involved.\n\nTest suite:\nYour program must run the following four test cases with fixed random seeds to ensure reproducibility. Each case specifies $(T, p)$, autoregressive coefficients, driver couplings, direct couplings, innovation variances, and the random seed. The coefficients are given as $2$-tuples for order $p = 2$.\n\n- Case $1$ (hidden driver induces lead-lag spurious causality):\n  - $T = 4000$, $p = 2$, seed $= 12345$.\n  - $a_z = (0.6, -0.5)$.\n  - $a_x = (0.2, 0.0)$, $a_y = (0.2, 0.0)$.\n  - $b_x = (0.9, 0.0)$, $b_y = (0.0, 0.9)$.\n  - $c_{xy} = (0.0, 0.0)$.\n  - Noise standard deviations $(\\sigma_x, \\sigma_y, \\sigma_z) = (0.5, 0.5, 0.5)$.\n  - Expected phenomenon: pairwise $x \\to y$ Granger causality is spurious due to the driver’s lead-lag; conditional on $z_t$, it should vanish.\n\n- Case $2$ (no common driver, no direct coupling):\n  - $T = 3000$, $p = 2$, seed $= 23456$.\n  - $a_z = (0.6, -0.5)$.\n  - $a_x = (0.2, 0.0)$, $a_y = (0.2, 0.0)$.\n  - $b_x = (0.0, 0.0)$, $b_y = (0.0, 0.0)$.\n  - $c_{xy} = (0.0, 0.0)$.\n  - Noise $(\\sigma_x, \\sigma_y, \\sigma_z) = (0.5, 0.5, 0.5)$.\n  - Expected phenomenon: no Granger causality in any direction, pairwise or conditional.\n\n- Case $3$ (hidden driver plus true direct $x \\to y$ influence):\n  - $T = 4000$, $p = 2$, seed $= 54321$.\n  - $a_z = (0.6, -0.5)$.\n  - $a_x = (0.2, 0.0)$, $a_y = (0.2, 0.0)$.\n  - $b_x = (0.9, 0.0)$, $b_y = (0.0, 0.9)$.\n  - $c_{xy} = (0.5, 0.0)$.\n  - Noise $(\\sigma_x, \\sigma_y, \\sigma_z) = (0.5, 0.5, 0.5)$.\n  - Expected phenomenon: $x \\to y$ Granger causality persists even conditionally on $z_t$ due to the true direct pathway.\n\n- Case $4$ (common driver with identical lag, reduces spurious lead-lag):\n  - $T = 4000$, $p = 2$, seed $= 34567$.\n  - $a_z = (0.6, -0.5)$.\n  - $a_x = (0.2, 0.0)$, $a_y = (0.2, 0.0)$.\n  - $b_x = (0.9, 0.0)$, $b_y = (0.9, 0.0)$.\n  - $c_{xy} = (0.0, 0.0)$.\n  - Noise $(\\sigma_x, \\sigma_y, \\sigma_z) = (0.5, 0.5, 0.5)$.\n  - Expected phenomenon: absence of lead-lag reduces spurious pairwise causality; conditional tests should also show no causality.\n\nOutput specification:\n- For each test case, compute four booleans:\n  - Pairwise $x \\to y$ significance,\n  - Pairwise $y \\to x$ significance,\n  - Conditional $x \\to y$ significance (conditioning on $z_t$),\n  - Conditional $y \\to x$ significance (conditioning on $z_t$).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the $4$-boolean list for one test case. For example: $[ [\\text{True},\\text{False},\\text{False},\\text{False}], \\dots ]$.",
            "solution": "The user wants a Python program to simulate a trivariate vector autoregressive (VAR) process and perform Granger causality (GC) analysis. The goal is to demonstrate how a hidden common driver can create spurious causal links and how conditioning on this driver can remove them. The analysis must be implemented from first principles using Ordinary Least Squares (OLS) and a classical $F$-test.\n\n### Step 1: Extract Givens\n- **Task**: Simulate a trivariate time series $(x_t, y_t, z_t)$ and test for pairwise and conditional Granger causality.\n- **Model Order**: Autoregressive order $p=2$.\n- **Time Series Length**: $T$, specified per test case.\n- **Burn-in Period**: $500$ samples.\n- **Data-Generating Process (VAR(2))**:\n  $$z_t = a_{z,1} z_{t-1} + a_{z,2} z_{t-2} + \\varepsilon_{z,t}$$\n  $$x_t = a_{x,1} x_{t-1} + a_{x,2} x_{t-2} + b_{x,1} z_{t-1} + b_{x,2} z_{t-2} + \\varepsilon_{x,t}$$\n  $$y_t = a_{y,1} y_{t-1} + a_{y,2} y_{t-2} + b_{y,1} z_{t-1} + b_{y,2} z_{t-2} + c_{xy,1} x_{t-1} + c_{xy,2} x_{t-2} + \\varepsilon_{y,t}$$\n- **Noise Terms**: $\\varepsilon_{x,t}, \\varepsilon_{y,t}, \\varepsilon_{z,t}$ are independent and identically distributed (i.i.d.) Gaussian random variables with mean $0$ and specified standard deviations.\n- **Granger Causality Test**:\n  - Based on nested OLS linear models with an intercept.\n  - **Statistic**: $F = \\frac{\\left(\\mathrm{RSS}_{\\mathrm{r}} - \\mathrm{RSS}_{\\mathrm{u}}\\right)/m}{\\mathrm{RSS}_{\\mathrm{u}}/(N - k_{\\mathrm{u}})}$.\n  - **Parameters**: $m$ = number of added regressors (source lags); $N$ = number of samples for regression; $k_{\\mathrm{u}}$ = number of parameters in the unrestricted model.\n  - **Null Hypothesis ($H_0$)**: No Granger causality. The $F$-statistic follows an $F(m, N - k_{\\mathrm{u}})$ distribution under $H_0$.\n- **Significance Level**: $\\alpha = 0.01$. A result is significant if the $p$-value is less than $\\alpha$.\n- **Test Cases**: Four specific cases are provided, each with values for $T$, $p$, random seed, all model coefficients $(a_x, a_y, a_z, b_x, b_y, c_{xy})$, and noise standard deviations $(\\sigma_x, \\sigma_y, \\sigma_z)$.\n- **Output**: For each case, a list of four booleans: (pairwise $x \\to y$, pairwise $y \\to x$, conditional $x \\to y$, conditional $y \\to x$). The final output is a single-line string representing a list of these lists.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is scientifically sound. It is based on the well-established theory of Granger causality and vector autoregressive models, which are standard tools in econometrics and computational neuroscience. The statistical test ($F$-test for nested models) is the canonical method for this analysis.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable and is directly relevant to the topic of Granger causality in neuroscience data analysis.\n3.  **Incomplete or Contradictory Setup**: The problem statement is complete. All parameters for simulation and analysis are specified. The slightly ambiguous term $c_{yx,1} y_{t-1}^{(x\\text{-to-}y)}$ in the equation for $y_t$ is clarified by the text \"the terms $c_{yx,1}$ and $c_{yx,2}$ are not used,\" removing any contradiction.\n4.  **Unrealistic or Infeasible**: The VAR model is a standard, feasible model for time series analysis. The chosen parameters for the AR processes result in stable systems (the roots of the characteristic polynomial lie within the unit circle), ensuring that the simulated time series do not diverge.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The use of fixed random seeds ensures that the simulation is reproducible, leading to a unique and stable solution.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires a correct implementation of time series simulation, linear regression, and statistical hypothesis testing, representing a substantive task in computational methods.\n7.  **Outside Scientific Verifiability**: The results are computationally verifiable by executing the code with the specified parameters and seeds.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Design\n\nThe solution will be structured as a single Python program that executes the four specified test cases. The logic is divided into three main parts: time series simulation, Granger causality testing, and a main execution loop.\n\n**1. Time Series Simulation**\nA function will be implemented to generate the trivariate time series $(x_t, y_t, z_t)$ according to the specified VAR($p$) model.\n- **Inputs**: Time series length $T$, model order $p$, all model coefficients, noise standard deviations, a random seed, and a burn-in length.\n- **Process**:\n    1. A random number generator is seeded for reproducibility.\n    2. Three time series arrays of length $T + \\text{burn\\_in}$ are initialized to zeros.\n    3. Three corresponding Gaussian noise vectors are generated.\n    4. The function iterates from time step $t=p$ to $T+\\text{burn\\_in}-1$. In each step, it calculates $z_t$, $x_t$, and $y_t$ using their respective equations, which involve dot products of coefficient vectors with vectors of past values.\n    5. After the loop, the initial `burn_in` samples are discarded from each series to ensure the process has reached its stationary distribution.\n- **Output**: The three simulated time series $x, y, z$ of length $T$.\n\n**2. Granger Causality Testing**\nA second core function will perform the Granger causality test for a source series causing a target series, with an option for a conditioning series.\n- **Inputs**: A `target` series, a `source` series, model order $p$, significance level $\\alpha$, and an optional `conditioner` series.\n- **Process**:\n    1.  **Data Preparation**: The number of regression samples is $N = T - p$. A target vector $Y_{\\text{vec}}$ is formed from the target series as $[y_p, y_{p+1}, \\dots, y_{T-1}]$.\n    2.  **Restricted Model**: A design matrix $X_r$ is constructed for the restricted regression. Its columns consist of:\n        - An intercept (a column of ones).\n        - $p$ columns for the lagged values of the target series, $\\{y_{t-1}, \\dots, y_{t-p}\\}$.\n        - If performing a conditional test, $p$ columns for the lagged values of the conditioning series, $\\{z_{t-1}, \\dots, z_{t-p}\\}$.\n    3.  **Unrestricted Model**: A design matrix $X_u$ is constructed for the unrestricted regression. It contains all columns from $X_r$ plus $p$ additional columns for the lagged values of the source series, $\\{x_{t-1}, \\dots, x_{t-p}\\}$.\n    4.  **OLS Regression**: OLS is performed for both models by regressing $Y_{\\text{vec}}$ on $X_r$ and $X_u$. We use `numpy.linalg.lstsq` for this, which conveniently returns the residual sum of squares (RSS). Let these be $\\mathrm{RSS}_{\\mathrm{r}}$ and $\\mathrm{RSS}_{\\mathrm{u}}$.\n    5.  **F-Statistic Calculation**: The F-statistic is computed using the formula:\n        $$F = \\frac{(\\mathrm{RSS}_{\\mathrm{r}} - \\mathrm{RSS}_{\\mathrm{u}}) / m}{\\mathrm{RSS}_{\\mathrm{u}} / (N - k_{\\mathrm{u}})}$$\n        where $m=p$ is the number of source lags, $N = T - p$, and $k_{\\mathrm{u}}$ is the total number of regressors in the unrestricted model (i.e., the number of columns in $X_u$).\n    6.  **Hypothesis Test**: The $p$-value is calculated from the survival function (1 - CDF) of the F-distribution with degrees of freedom $df_1 = m$ and $df_2 = N - k_{\\mathrm{u}}$. This is done using `scipy.stats.f.sf`.\n- **Output**: The function returns `True` if the $p$-value is less than $\\alpha$, indicating significant Granger causality, and `False` otherwise.\n\n**3. Main Execution Logic**\nThe main part of the program defines the four test cases. It iterates through them, calling the simulation function for each to generate data. Then, for each dataset, it calls the Granger causality test function four times to assess the required relationships:\n- Pairwise $x \\to y$ (target=`y`, source=`x`)\n- Pairwise $y \\to x$ (target=`x`, source=`y`)\n- Conditional $x \\to y$ | $z$ (target=`y`, source=`x`, conditioner=`z`)\n- Conditional $y \\to x$ | $z$ (target=`x`, source=`y`, conditioner=`z`)\nThe four boolean results for each case are collected. Finally, the program formats this list of lists into a single-line string as specified and prints it to standard output.",
            "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Simulates trivariate time series and performs pairwise and conditional\n    Granger causality analysis to demonstrate the effect of a hidden common driver.\n    \"\"\"\n\n    def _simulate_var_process(T, p, a_z, a_x, a_y, b_x, b_y, c_xy, sigmas, seed, burn_in):\n        \"\"\"\n        Simulates a Vector Autoregressive (VAR) process of order p.\n        \"\"\"\n        np.random.seed(seed)\n        total_len = T + burn_in\n        sigma_x, sigma_y, sigma_z = sigmas\n\n        x = np.zeros(total_len)\n        y = np.zeros(total_len)\n        z = np.zeros(total_len)\n\n        eps_x = np.random.normal(0, sigma_x, total_len)\n        eps_y = np.random.normal(0, sigma_y, total_len)\n        eps_z = np.random.normal(0, sigma_z, total_len)\n\n        # Convert coefficients to numpy arrays for dot products\n        a_z, a_x, a_y = np.array(a_z), np.array(a_x), np.array(a_y)\n        b_x, b_y, c_xy = np.array(b_x), np.array(b_y), np.array(c_xy)\n\n        for t in range(p, total_len):\n            # Lags are extracted in order [t-1, t-2, ..., t-p]\n            z_lags = z[t-p:t][::-1]\n            x_lags = x[t-p:t][::-1]\n            y_lags = y[t-p:t][::-1]\n\n            z[t] = np.dot(a_z, z_lags) + eps_z[t]\n            x[t] = np.dot(a_x, x_lags) + np.dot(b_x, z_lags) + eps_x[t]\n            y[t] = np.dot(a_y, y_lags) + np.dot(b_y, z_lags) + np.dot(c_xy, x_lags) + eps_y[t]\n\n        return x[burn_in:], y[burn_in:], z[burn_in:]\n\n    def _test_granger_causality(target, source, p, alpha, conditioner=None):\n        \"\"\"\n        Performs a Granger causality F-test.\n        Tests if `source` Granger-causes `target`, optionally conditioning on `conditioner`.\n        \"\"\"\n        T = len(target)\n        N = T - p\n        Y_vec = target[p:]\n\n        def create_lag_matrix(series, p, N):\n            lag_matrix = np.zeros((N, p))\n            for i in range(1, p + 1):\n                lag_matrix[:, i-1] = series[p-i : N+p-i]\n            return lag_matrix\n\n        # Restricted model (regresses target on its own past and conditioner's past)\n        target_lags = create_lag_matrix(target, p, N)\n        regressors_r_list = [np.ones((N, 1)), target_lags]\n        if conditioner is not None:\n            conditioner_lags = create_lag_matrix(conditioner, p, N)\n            regressors_r_list.append(conditioner_lags)\n        X_r = np.hstack(regressors_r_list)\n\n        _, rss_r_array, _, _ = np.linalg.lstsq(X_r, Y_vec, rcond=None)\n        # If the regression is perfect, lstsq returns an empty array for residuals\n        rss_r = rss_r_array[0] if rss_r_array.size > 0 else 0.0\n\n        # Unrestricted model (adds source's past to the restricted model)\n        source_lags = create_lag_matrix(source, p, N)\n        X_u = np.hstack([X_r, source_lags])\n\n        _, rss_u_array, _, _ = np.linalg.lstsq(X_u, Y_vec, rcond=None)\n        rss_u = rss_u_array[0] if rss_u_array.size > 0 else 0.0\n\n        # F-test\n        m = p  # number of added parameters (lags of the source variable)\n        k_u = X_u.shape[1]  # number of parameters in the unrestricted model\n        \n        df1 = m\n        df2 = N - k_u\n\n        if df2 <= 0:\n            return False # Not enough degrees of freedom for the test\n\n        if rss_u < np.finfo(float).eps:\n            # A perfect fit in the unrestricted model implies infinite F-statistic,\n            # which means p-value is 0, hence significant.\n            # However, this scenario is unlikely with noise.\n            # If RSS_r is also 0, F-stat is NaN, result should be False.\n            return rss_r > np.finfo(float).eps\n\n        f_stat = ((rss_r - rss_u) / df1) / (rss_u / df2)\n        p_value = stats.f.sf(f_stat, df1, df2)\n\n        return p_value < alpha\n\n    # --- Test Suite ---\n    # Each tuple: (T, p, seed, a_z, a_x, a_y, b_x, b_y, c_xy, sigmas)\n    test_cases = [\n        # Case 1: Hidden driver induces lead-lag spurious causality\n        (4000, 2, 12345, (0.6, -0.5), (0.2, 0.0), (0.2, 0.0), (0.9, 0.0), (0.0, 0.9), (0.0, 0.0), (0.5, 0.5, 0.5)),\n        # Case 2: No common driver, no direct coupling\n        (3000, 2, 23456, (0.6, -0.5), (0.2, 0.0), (0.2, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.5, 0.5, 0.5)),\n        # Case 3: Hidden driver plus true direct x -> y influence\n        (4000, 2, 54321, (0.6, -0.5), (0.2, 0.0), (0.2, 0.0), (0.9, 0.0), (0.0, 0.9), (0.5, 0.0), (0.5, 0.5, 0.5)),\n        # Case 4: Common driver with identical lag\n        (4000, 2, 34567, (0.6, -0.5), (0.2, 0.0), (0.2, 0.0), (0.9, 0.0), (0.9, 0.0), (0.0, 0.0), (0.5, 0.5, 0.5)),\n    ]\n\n    alpha = 0.01\n    burn_in = 500\n    all_results = []\n\n    for case_params in test_cases:\n        T, p, seed, a_z, a_x, a_y, b_x, b_y, c_xy, sigmas = case_params\n        \n        x, y, z = _simulate_var_process(\n            T, p, a_z, a_x, a_y, b_x, b_y, c_xy, sigmas, seed, burn_in\n        )\n\n        # Pairwise tests\n        gc_xy_pair = _test_granger_causality(y, x, p, alpha)\n        gc_yx_pair = _test_granger_causality(x, y, p, alpha)\n\n        # Conditional tests (conditioning on z)\n        gc_xy_cond = _test_granger_causality(y, x, p, alpha, conditioner=z)\n        gc_yx_cond = _test_granger_causality(x, y, p, alpha, conditioner=z)\n\n        case_results = [gc_xy_pair, gc_yx_pair, gc_xy_cond, gc_yx_cond]\n        all_results.append(case_results)\n\n    # Format the output string to be a comma-separated list of lists, with no spaces.\n    inner_parts = [str(r).replace(\" \", \"\") for r in all_results]\n    body = \",\".join(inner_parts)\n    final_output = f\"[{body}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "While parametric tests like the F-test are powerful, their validity hinges on specific assumptions about the data's underlying distribution. This exercise introduces a robust non-parametric alternative using surrogate data, a method widely employed in neuroscience. You will learn to generate phase-randomized surrogates that preserve the autocorrelation structure of each time series while nullifying their cross-variable dependencies, allowing you to build an empirical null distribution for your test statistic and perform a more assumption-free hypothesis test .",
            "id": "4166692",
            "problem": "You are given two simultaneously recorded neural time series modeled as a bivariate linear, weakly stationary process. The objective is to test whether one time series $x_t$ \"Granger-causes\" another time series $y_t$ in the time domain, while constructing a valid null distribution via phase-randomized surrogates that preserve univariate autocorrelation but destroy cross-lag dependencies. Your task is to design and implement a program that, for each provided test case, simulates data, computes a time-domain Granger causality statistic from $x_t$ to $y_t$, builds a surrogate-based null distribution using independent phase randomization for each series, and finally outputs a boolean indicating whether the observed statistic exceeds the $95$-th percentile of the null distribution.\n\nFoundational base:\n- A stationary linear process has second-order structure characterized by its autocovariance and power spectral density. By the Wiener–Khinchin theorem, the autocovariance function is the inverse Fourier transform of the power spectral density. For a finite, real-valued discrete-time signal, preserving the magnitude of the Discrete Fourier Transform of each univariate series preserves its periodogram and associated second-order autocorrelation structure.\n- Granger causality in the time domain is defined as follows: $x_t$ Granger-causes $y_t$ if and only if the past of $x_t$ provides predictive information about $y_t$ beyond what is captured by the past of $y_t$ alone. This can be assessed via a nested linear regression comparison using Ordinary Least Squares (OLS).\n\nYou must implement the following components:\n- Simulation of a bivariate Vector Autoregression (VAR) of order $p$ (Vector Autoregression (VAR)) with Gaussian innovations:\n  $$ x_t = a^{(x)}_1 x_{t-1} + a^{(x)}_2 x_{t-2} + \\varepsilon^{(x)}_t, \\quad y_t = a^{(y)}_1 y_{t-1} + a^{(y)}_2 y_{t-2} + b_1 x_{t-1} + b_2 x_{t-2} + \\varepsilon^{(y)}_t, $$\n  where $t$ indexes discrete time samples, $\\varepsilon^{(x)}_t$ and $\\varepsilon^{(y)}_t$ are independent Gaussian innovations with zero mean and unit variance, and $(a^{(x)}_1,a^{(x)}_2,a^{(y)}_1,a^{(y)}_2,b_1,b_2)$ are model coefficients chosen to ensure stability. Use a burn-in of $B$ samples to reach stationarity before collecting $T$ samples for analysis. All time indices and counts are integers, with $p=2$ in the test suite.\n- Phase-randomized surrogate generation for each univariate time series independently:\n  1. Compute the real-valued Fast Fourier Transform (FFT) (Fast Fourier Transform (FFT)) for a series of length $T$, obtaining non-negative frequency coefficients via the real FFT.\n  2. Preserve the magnitude spectrum and randomize phases independently by drawing angles uniformly from $[0,2\\pi)$ for all non-DC, non-Nyquist frequencies, while enforcing Hermitian symmetry through use of the real inverse FFT to reconstruct a real-valued surrogate.\n  3. Repeat independently for $x_t$ and $y_t$ to destroy cross-lag dependencies while preserving each series' autocorrelation structure.\n- Compute the time-domain Granger causality statistic from $x_t$ to $y_t$ using OLS:\n  1. Construct the restricted model using only $y_{t-1},y_{t-2},\\dots,y_{t-p}$ to predict $y_t$ and compute its residual sum of squares.\n  2. Construct the unrestricted model using $y_{t-1},\\dots,y_{t-p}$ and $x_{t-1},\\dots,x_{t-p}$ to predict $y_t$ and compute its residual sum of squares.\n  3. Use the nested-model comparison framework to compute a scalar test statistic that increases when adding $x$-lags significantly improves prediction of $y_t$ beyond $y$-lags alone.\n- Null distribution construction:\n  1. Generate $S$ independent surrogate pairs $(\\tilde{x}^{(s)}_t,\\tilde{y}^{(s)}_t)$ using the phase randomization procedure described above.\n  2. Compute the same Granger causality statistic for each surrogate pair, obtaining a surrogate null distribution for the statistic under destroyed cross-lag dependencies but preserved univariate autocorrelation.\n  3. Determine the $95$-th percentile of the surrogate distribution and compare the original statistic to this threshold to produce a boolean decision indicating significance.\n\nImplementation constraints:\n- Use Ordinary Least Squares with deterministic design matrices.\n- Demean the series before fitting models; do not include an intercept term.\n- Use a fixed random seed to ensure reproducibility.\n\nTest suite:\nFor all test cases, use $p=2$, a burn-in of $B=200$ samples, $S=200$ surrogates, and independent Gaussian innovations with unit variance, with random seed fixed to $1$.\n\n- Test case $1$ (happy path, strong coupling): $T=1024$, coefficients $a^{(x)}_1=0.5$, $a^{(x)}_2=-0.2$, $a^{(y)}_1=0.4$, $a^{(y)}_2=-0.3$, $b_1=0.35$, $b_2=0.0$.\n- Test case $2$ (null coupling): $T=1024$, coefficients $a^{(x)}_1=0.5$, $a^{(x)}_2=-0.2$, $a^{(y)}_1=0.4$, $a^{(y)}_2=-0.3$, $b_1=0.0$, $b_2=0.0$.\n- Test case $3$ (boundary, short record): $T=128$, coefficients $a^{(x)}_1=0.5$, $a^{(x)}_2=-0.2$, $a^{(y)}_1=0.3$, $a^{(y)}_2=-0.2$, $b_1=0.45$, $b_2=0.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the significance decisions for the three test cases as a comma-separated Python-style list of booleans enclosed in square brackets (e.g., $[True,False,True]$).",
            "solution": "The user's problem is rigorously validated and found to be scientifically sound, well-posed, and objective. It describes a standard and complete procedure for non-parametric Granger causality analysis, a common task in neuroscience and time series analysis. All necessary parameters, models, and constraints are provided, and the underlying theoretical foundations are correctly stated. We may therefore proceed with a complete solution.\n\nThe core objective is to determine if a time series $x_t$ has a predictive, causal influence on another time series $y_t$, a concept formalized by Clive Granger. The methodology specified involves fitting a bivariate Vector Autoregressive (VAR) model of order $p$, calculating a statistic based on nested model comparison, and assessing its significance against a null distribution constructed from phase-randomized surrogate data.\n\n### 1. Vector Autoregressive (VAR) Model Simulation\n\nWe begin by simulating data from a bivariate VAR($p$) process. For this problem, the order is fixed at $p=2$. The model equations are given as:\n$$\nx_t = a^{(x)}_1 x_{t-1} + a^{(x)}_2 x_{t-2} + \\varepsilon^{(x)}_t\n$$\n$$\ny_t = a^{(y)}_1 y_{t-1} + a^{(y)}_2 y_{t-2} + b_1 x_{t-1} + b_2 x_{t-2} + \\varepsilon^{(y)}_t\n$$\nHere, $\\varepsilon^{(x)}_t$ and $\\varepsilon^{(y)}_t$ are independent and identically distributed (i.i.d.) random variables drawn from a standard normal distribution, $\\mathcal{N}(0, 1)$. The coefficients $b_1$ and $b_2$ represent the direct influence of past $x_t$ on the present $y_t$. The process is simulated for a total of $T+B$ time steps, where $B$ is the burn-in period to allow the system to approach its stationary distribution. The first $B$ samples are discarded, yielding two time series, $x_t$ and $y_t$, each of length $T$. Finally, as per the problem's constraints, both series are demeaned by subtracting their respective sample means.\n\n### 2. Time-Domain Granger Causality Statistic\n\nGranger causality posits that $x_t$ causes $y_t$ if past values of $x_t$ contain information that helps predict $y_t$ above and beyond the information contained in past values of $y_t$ alone. This is tested by comparing two nested linear regression models using Ordinary Least Squares (OLS).\n\n**Restricted Model:** $y_t$ is predicted using only its own past values up to lag $p$.\n$$\ny_t = \\sum_{j=1}^{p} \\alpha_j y_{t-j} + \\eta_t\n$$\nWe fit this model to the data for $t \\in [p, T-1]$, which provides $N = T-p$ observations. Let the design matrix for this model be $\\mathbf{X}_{\\text{restr}} \\in \\mathbb{R}^{N \\times p}$ and the target vector be $\\mathbf{y}_{\\text{target}} \\in \\mathbb{R}^{N}$. The OLS solution for the coefficients is $\\hat{\\boldsymbol{\\alpha}} = (\\mathbf{X}_{\\text{restr}}^T \\mathbf{X}_{\\text{restr}})^{-1} \\mathbf{X}_{\\text{restr}}^T \\mathbf{y}_{\\text{target}}$. The residuals are $\\hat{\\boldsymbol{\\eta}} = \\mathbf{y}_{\\text{target}} - \\mathbf{X}_{\\text{restr}} \\hat{\\boldsymbol{\\alpha}}$, and their sum of squares is $RSS_{\\text{restr}} = \\sum_{t=p}^{T-1} \\hat{\\eta}_t^2$.\n\n**Unrestricted Model:** $y_t$ is predicted using its own past and the past of $x_t$.\n$$\ny_t = \\sum_{j=1}^{p} \\alpha'_j y_{t-j} + \\sum_{j=1}^{p} \\beta_j x_{t-j} + \\xi_t\n$$\nThis model is fit to the same $N = T-p$ observations. The design matrix $\\mathbf{X}_{\\text{unrestr}} \\in \\mathbb{R}^{N \\times 2p}$ now includes lagged versions of both $x_t$ and $y_t$. The OLS fit yields residuals $\\hat{\\boldsymbol{\\xi}}$ and a corresponding residual sum of squares $RSS_{\\text{unrestr}} = \\sum_{t=p}^{T-1} \\hat{\\xi}_t^2$.\n\nThe unrestricted model will always fit the data at least as well as the restricted one, so $RSS_{\\text{unrestr}} \\le RSS_{\\text{restr}}$. A significant drop in $RSS$ when including the $x_t$ terms indicates that $x_t$ adds predictive power. A common statistic to quantify this improvement is the logarithm of the ratio of the residual sums of squares:\n$$\nG_{x \\to y} = \\ln \\left( \\frac{RSS_{\\text{restr}}}{RSS_{\\text{unrestr}}} \\right)\n$$\nThis statistic is non-negative and increases as the predictive contribution of $x_t$ grows.\n\n### 3. Null Hypothesis Testing with Phase-Randomized Surrogates\n\nTo determine if the observed value of $G_{x \\to y}$ is statistically significant, we compare it to a null distribution. The null hypothesis, $H_0$, is that $x_t$ does not Granger-cause $y_t$. A powerful non-parametric method to generate data under $H_0$ is to create surrogate time series that preserve the properties of the original data that are irrelevant to the hypothesis, while destroying the property that is being tested.\n\nHere, we want to destroy any cross-lagged dependencies between $x_t$ and $y_t$ while preserving the autocorrelation structure (and thus, the power spectrum) of each series individually. This is achieved through phase randomization in the Fourier domain. The procedure for a time series $z_t$ of length $T$ is:\n\n1.  **Fourier Transform:** Compute the Discrete Fourier Transform (DFT) of the real-valued series $z_t$, typically using a real-valued Fast Fourier Transform (FFT) algorithm, yielding complex coefficients $\\hat{z}[k]$.\n    $$\n    \\hat{z}[k] = |\\hat{z}[k]| e^{i\\theta_k}\n    $$\n2.  **Phase Randomization:** Generate a new set of phases $\\phi_k$ by drawing samples from a uniform distribution $U[0, 2\\pi)$. Construct new Fourier coefficients $\\hat{z}_{\\text{surr}}[k]$ by combining the original magnitudes with the new random phases.\n    $$\n    \\hat{z}_{\\text{surr}}[k] = |\\hat{z}[k]| e^{i\\phi_k}\n    $$\n    To ensure the resulting surrogate time series is real-valued, certain symmetries must be respected. The phase of the DC component ($k=0$) and the Nyquist component (if $T$ is even, at $k=T/2$) must be zero. For all other positive frequencies $k$, the phase for the corresponding negative frequency must be $\\phi_{-k} = -\\phi_k$. Using a real FFT and its inverse (`rfft`, `irfft`) handles these symmetries implicitly. We only need to explicitly set the phases for the DC and Nyquist frequencies to zero. By the Wiener-Khinchin theorem, preserving the magnitude spectrum $|\\hat{z}[k]|$ is equivalent to preserving the power spectral density and, consequently, the autocovariance function of the series.\n\n3.  **Inverse Fourier Transform:** Compute the inverse DFT of $\\hat{z}_{\\text{surr}}[k]$ to obtain the real-valued surrogate time series $\\tilde{z}_t$.\n\nThis procedure is applied independently to $x_t$ and $y_t$ to create a surrogate pair $(\\tilde{x}_t, \\tilde{y}_t)$. Because the phase randomization is independent for each series, any systematic phase/lag relationship between them is destroyed. By repeating this process $S$ times, we generate $S$ surrogate pairs. For each pair, we compute the Granger causality statistic $G^{\\text{surr}}_{x \\to y}$. The collection of these $S$ values forms an empirical null distribution.\n\n### 4. Statistical Decision\n\nThe observed statistic, $G^{\\text{obs}}_{x \\to y}$, calculated from the original data, is compared to the null distribution. The problem specifies a significance level of $\\alpha = 0.05$. We find the $95$-th percentile of the surrogate distribution, which serves as the critical value or threshold. If the observed statistic exceeds this threshold, we reject the null hypothesis and conclude that the Granger causality from $x_t$ to $y_t$ is statistically significant.\n$$\n\\text{Decision} = (G^{\\text{obs}}_{x \\to y} > \\text{percentile}( \\{G^{\\text{surr}}\\}_{s=1}^S, 95 ))\n$$\nThis entire procedure is then repeated for each test case defined in the problem statement.",
            "answer": "```python\nimport numpy as np\nfrom numpy.linalg import lstsq\nfrom numpy.fft import rfft, irfft\n\ndef simulate_var(T, B, coeffs, p, rng):\n    \"\"\"\n    Simulates a bivariate VAR(p) process.\n    \"\"\"\n    total_len = T + B\n    x = np.zeros(total_len)\n    y = np.zeros(total_len)\n    \n    # Generate innovations for the entire series at once\n    innovations = rng.standard_normal(size=(2, total_len))\n    \n    # Coefficients\n    ax1, ax2 = coeffs['ax1'], coeffs['ax2']\n    ay1, ay2 = coeffs['ay1'], coeffs['ay2']\n    b1, b2 = coeffs['b1'], coeffs['b2']\n    \n    # Time-stepping loop\n    for t in range(p, total_len):\n        x[t] = ax1 * x[t-1] + ax2 * x[t-2] + innovations[0, t]\n        y[t] = ay1 * y[t-1] + ay2 * y[t-2] + b1 * x[t-1] + b2 * x[t-2] + innovations[1, t]\n        \n    # Discard burn-in period\n    x_final = x[B:]\n    y_final = y[B:]\n    \n    return x_final, y_final\n\ndef generate_phase_randomized_surrogate(z, rng):\n    \"\"\"\n    Generates a phase-randomized surrogate preserving the power spectrum.\n    \"\"\"\n    T = len(z)\n    ft = rfft(z)\n    mags = np.abs(ft)\n    \n    # Generate random phases\n    phases = rng.uniform(0, 2 * np.pi, len(ft))\n    \n    # Set DC and Nyquist phases to 0 for a real-valued signal\n    phases[0] = 0\n    if T % 2 == 0:\n        phases[-1] = 0\n        \n    # Create new Fourier coefficients with original magnitudes and random phases\n    new_ft = mags * np.exp(1j * phases)\n    \n    # Inverse FFT to get the surrogate time series\n    surrogate = irfft(new_ft, n=T)\n    \n    return surrogate\n\ndef calculate_gc_stat(x, y, p):\n    \"\"\"\n    Calculates the time-domain Granger causality statistic from x to y.\n    \"\"\"\n    T = len(x)\n    n_obs = T - p\n    \n    # Construct the target vector y[t] for t=p,...,T-1\n    y_target = y[p:]\n\n    # Restricted model: y_t = f(y_{t-1}, ..., y_{t-p})\n    X_restr_cols = [y[p-k:T-k] for k in range(1, p + 1)]\n    X_restr = np.column_stack(X_restr_cols)\n    \n    # Solve OLS and compute residual sum of squares\n    beta_restr, _, _, _ = lstsq(X_restr, y_target, rcond=None)\n    residuals_restr = y_target - X_restr @ beta_restr\n    rss_restr = np.sum(residuals_restr**2)\n\n    # Unrestricted model: y_t = f(y_{t-1}, ..., y_{t-p}, x_{t-1}, ..., x_{t-p})\n    X_unrestr_cols = [y[p-k:T-k] for k in range(1, p + 1)] + \\\n                     [x[p-k:T-k] for k in range(1, p + 1)]\n    X_unrestr = np.column_stack(X_unrestr_cols)\n    \n    # Solve OLS and compute residual sum of squares\n    beta_unrestr, _, _, _ = lstsq(X_unrestr, y_target, rcond=None)\n    residuals_unrestr = y_target - X_unrestr @ beta_unrestr\n    rss_unrestr = np.sum(residuals_unrestr**2)\n    \n    # Avoid division by zero or log of non-positive\n    if rss_unrestr <= 1e-10:\n        return 0.0\n\n    gc_stat = np.log(rss_restr / rss_unrestr)\n    \n    return gc_stat\n\ndef run_granger_test(T, B, S, p, coeffs, rng):\n    \"\"\"\n    Runs the full Granger causality test for a single case.\n    \"\"\"\n    # 1. Simulate data and demean\n    x_sim, y_sim = simulate_var(T, B, coeffs, p, rng)\n    x = x_sim - np.mean(x_sim)\n    y = y_sim - np.mean(y_sim)\n    \n    # 2. Compute observed Granger causality statistic\n    original_gc = calculate_gc_stat(x, y, p)\n    \n    # 3. Build null distribution from surrogates\n    surrogate_stats = []\n    for _ in range(S):\n        # Generate independent surrogates\n        x_surr = generate_phase_randomized_surrogate(x, rng)\n        y_surr = generate_phase_randomized_surrogate(y, rng)\n        \n        # Demean surrogates to match original data preprocessing\n        x_surr_demeaned = x_surr - np.mean(x_surr)\n        y_surr_demeaned = y_surr - np.mean(y_surr)\n        \n        gc_surr = calculate_gc_stat(x_surr_demeaned, y_surr_demeaned, p)\n        surrogate_stats.append(gc_surr)\n        \n    # 4. Perform the hypothesis test against the 95th percentile\n    threshold = np.percentile(surrogate_stats, 95)\n    is_significant = original_gc > threshold\n    \n    return is_significant\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Global parameters\n    SEED = 1\n    P = 2\n    B = 200\n    S = 200\n    \n    # Test suite\n    test_cases = [\n        # Case 1: Strong coupling\n        {'T': 1024, 'coeffs': {'ax1': 0.5, 'ax2': -0.2, 'ay1': 0.4, 'ay2': -0.3, 'b1': 0.35, 'b2': 0.0}},\n        # Case 2: Null coupling\n        {'T': 1024, 'coeffs': {'ax1': 0.5, 'ax2': -0.2, 'ay1': 0.4, 'ay2': -0.3, 'b1': 0.0, 'b2': 0.0}},\n        # Case 3: Boundary case, short record\n        {'T': 128, 'coeffs': {'ax1': 0.5, 'ax2': -0.2, 'ay1': 0.3, 'ay2': -0.2, 'b1': 0.45, 'b2': 0.0}}\n    ]\n\n    # Initialize a single random number generator for reproducibility\n    rng = np.random.default_rng(SEED)\n    results = []\n\n    for case in test_cases:\n        # The RNG state advances with each call, ensuring simulations are \n        # independent across test cases while the whole process remains deterministic.\n        is_significant = run_granger_test(case['T'], B, S, P, case['coeffs'], rng)\n        results.append(is_significant)\n    \n    # Format and print the final output exactly as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}