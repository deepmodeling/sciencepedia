{
    "hands_on_practices": [
        {
            "introduction": "Before applying any statistical tool, it is essential to understand its mathematical underpinnings. This exercise takes you to the core of time-domain Granger causality by asking you to derive the F-statistic used for hypothesis testing . By starting from the principles of nested linear models and the distribution of sums of squares, you will see precisely how the comparison between a restricted model (without the influence of $x_t$) and an unrestricted model (with the influence of $x_t$) is quantified, providing a solid foundation for interpreting the test's output.",
            "id": "4166697",
            "problem": "Consider two simultaneously recorded neural time series $x_t$ and $y_t$ (for $t=1,\\dots,T$) modeled by a bivariate Vector Autoregression (VAR) of order $p$, where the $y_t$ equation is\n$$\ny_t \\;=\\; a_0 \\;+\\; \\sum_{i=1}^{p} a_i\\,y_{t-i} \\;+\\; \\sum_{i=1}^{p} b_i\\,x_{t-i} \\;+\\; \\varepsilon_t,\n$$\nand the $x_t$ equation is analogously specified but is not directly needed for the hypothesis about $y_t$. Assume $t$ runs from $p+1$ to $T$ in estimation so that $N = T - p$ usable observations are available, the design matrix has full column rank, and the innovations $\\varepsilon_t$ are independent and identically distributed Gaussian with mean $0$ and variance $\\sigma^2$. You wish to test the null hypothesis of no Granger causality from $x_t$ to $y_t$, which corresponds to $H_0: b_1=\\cdots=b_p=0$, against the alternative that at least one $b_i \\neq 0$. Fit the $y_t$ equation by Ordinary Least Squares (OLS) under the unrestricted model (including the $p$ lags of $x_t$) and the restricted model (excluding the $p$ lags of $x_t$), and let $RSS_u$ and $RSS_r$ denote the residual sums of squares from the unrestricted and restricted $y_t$ regressions, respectively.\n\nStarting from the classical linear model assumptions and distributions of sums of squares under $H_0$, derive the $F$-statistic for testing $H_0$ in terms of $RSS_u$, $RSS_r$, $p$, and $N$. Then specify the numerator and denominator degrees of freedom of the corresponding $F$ distribution under $H_0$, in terms of $p$ and $N$.\n\nExpress your final answer as a single analytic expression for the $F$-statistic together with its numerator and denominator degrees of freedom, reported as a row matrix. No numerical evaluation or rounding is required.",
            "solution": "The problem asks for the derivation of the $F$-statistic for testing the null hypothesis of no Granger causality, $H_0: b_1=\\cdots=b_p=0$. This is a standard test of $q=p$ linear restrictions in a general linear model.\n\nThe unrestricted model is:\n$$ y_t = a_0 + \\sum_{i=1}^{p} a_i y_{t-i} + \\sum_{i=1}^{p} b_i x_{t-i} + \\varepsilon_t $$\nThis model has $k_u = 1 + p + p = 1 + 2p$ parameters (including the intercept). It is estimated over $N=T-p$ observations, yielding a residual sum of squares $RSS_u$.\n\nThe restricted model, under $H_0$, is:\n$$ y_t = a_0 + \\sum_{i=1}^{p} a_i y_{t-i} + \\varepsilon_t $$\nThis model has $k_r = 1 + p$ parameters, yielding a residual sum of squares $RSS_r$.\n\nThe general formula for the $F$-statistic for testing $q$ linear restrictions is:\n$$ F = \\frac{(RSS_r - RSS_u)/q}{RSS_u / (N - k_u)} $$\nFor this specific problem, the number of restrictions is $q=p$, and the number of parameters in the unrestricted model is $k_u = 1+2p$. Substituting these values gives:\n$$ F = \\frac{(RSS_r - RSS_u)/p}{RSS_u / (N - (1 + 2p))} = \\frac{(RSS_r - RSS_u)/p}{RSS_u / (N - 1 - 2p)} $$\nUnder the null hypothesis $H_0$ and the assumption of i.i.d. Gaussian errors, this statistic follows an $F$ distribution with numerator degrees of freedom $df_1 = q = p$ and denominator degrees of freedom $df_2 = N - k_u = N - 1 - 2p$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{(RSS_r - RSS_u)/p}{RSS_u / (N - 1 - 2p)}  p  N - 1 - 2p\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A frequent and critical error in connectivity analysis is confusing statistical prediction with simple correlation. This practice directly addresses this pitfall by exploring the relationship between zero-lag correlation, $\\operatorname{Cov}(x_t, y_t)$, and time-domain Granger causality . Through the construction of minimal, yet powerful, counterexamples, you will demonstrate that correlation is neither necessary nor sufficient for Granger causality, a crucial insight for correctly interpreting results from neurophysiological data where shared inputs or volume conduction can create instantaneous correlations without true predictive influence.",
            "id": "4166700",
            "problem": "You are analyzing two univariate neural time series $\\{x_t\\}$ and $\\{y_t\\}$ recorded from two brain regions at uniform sampling. Consider linear time-domain Granger causality (GC), defined as follows: $x$ Granger-causes $y$ if, for some finite order $p$, the optimal mean-square prediction of $y_t$ based on its own past $\\{y_{t-1},\\dots,y_{t-p}\\}$ is strictly improved by adding the past of $x$, $\\{x_{t-1},\\dots,x_{t-p}\\}$, when both series are modeled as a jointly wide-sense stationary vector autoregression (Vector Autoregression, VAR) with zero-mean innovations that are serially uncorrelated. Zero-lag correlation refers to $\\operatorname{Cov}(x_t,y_t)$, which captures instantaneous dependence but no predictive lag structure. In neuroscience data analysis, limited sensor mixing and shared inputs often induce contemporaneous correlations without necessarily introducing predictive structure. \n\nSelect the option(s) that correctly explain why zero-lag correlation between $x_t$ and $y_t$ is neither necessary nor sufficient for time-domain Granger causality and that provide valid minimal counterexamples demonstrating both directions (non-necessity and non-sufficiency) using simple lagged dependencies and white-noise innovations. Minimal here means first-order lag structures with independent, identically distributed, zero-mean Gaussian innovations and parameters chosen to ensure wide-sense stationarity.\n\nA. Explanation: Zero-lag correlation $\\operatorname{Cov}(x_t,y_t)$ quantifies contemporaneous dependence, whereas time-domain GC is about whether past values of $x$ contain unique predictive information for $y$ beyond $y$’s past. Thus $\\operatorname{Cov}(x_t,y_t)$ is neither necessary nor sufficient for GC. Non-necessity counterexample: let $x_t=\\eta_t$, $y_t=a\\,x_{t-1}+\\nu_t$ with $a\\neq 0$ and $\\eta_t,\\nu_t$ independent and identically distributed (i.i.d.) zero-mean Gaussian white noises. Then $\\operatorname{Cov}(x_t,y_t)=0$ while $x$ Granger-causes $y$. Non-sufficiency counterexample: let $x_t=u_t$, $y_t=u_t+\\epsilon_t$ with $u_t,\\epsilon_t$ i.i.d. zero-mean Gaussian white noises, independent of each other. Then $\\operatorname{Cov}(x_t,y_t)=\\operatorname{Var}(u_t)0$ but $x$ does not Granger-cause $y$.\n\nB. Explanation: Zero-lag correlation is unnecessary but sufficient for GC because contemporaneous dependence implies some predictive content. Non-necessity counterexample: let $x_t=\\eta_t+\\eta_{t-1}$, $y_t=a\\,x_{t-1}+\\nu_t$ with $a\\neq 0$ and $\\eta_t,\\nu_t$ i.i.d. zero-mean Gaussian white noises, independent. Non-sufficiency counterexample: let $x_t=u_t$, $y_t=u_{t-1}+\\epsilon_t$, with $u_t,\\epsilon_t$ i.i.d. zero-mean Gaussian white noises, independent.\n\nC. Explanation: Zero-lag correlation is not sufficient for GC because instantaneous mixing can induce contemporaneous dependence without predictive directionality, but it is necessary when both processes are autoregressive. Non-sufficiency counterexample: let $x_t=\\alpha x_{t-1}+u_t$, $y_t=\\beta y_{t-1}+v_t+\\gamma u_t$, with $|\\alpha|1$, $|\\beta|1$, $\\gamma\\neq 0$, and $u_t,v_t$ i.i.d. zero-mean Gaussian white noises, independent. Non-necessity counterexample: let $x_t=\\eta_t$, $y_t=\\nu_t$, with $\\eta_t,\\nu_t$ i.i.d. zero-mean Gaussian white noises, independent.\n\nD. Explanation: Zero-lag correlation is necessary for GC because if past $x$ helps predict $y_t$, then $x_t$ and $y_t$ must be contemporaneously correlated by temporal continuity. Non-necessity counterexample: none needed. Non-sufficiency counterexample: none needed.",
            "solution": "The question requires demonstrating that zero-lag correlation is neither necessary nor sufficient for time-domain Granger causality (GC). This involves providing a correct conceptual explanation and valid counterexamples.\n\n1.  **Non-Necessity**: To show correlation is not necessary for GC, we need an example where GC from $x$ to $y$ exists ($x \\to y$), but the zero-lag correlation is zero ($\\operatorname{Cov}(x_t, y_t) = 0$). Option A provides the system: $x_t = \\eta_t$ and $y_t = a x_{t-1} + \\nu_t$, with $a \\neq 0$ and $\\eta_t, \\nu_t$ being independent white noise. Here, the past of $x$ clearly helps predict $y$, so $x \\to y$. However, the covariance is $\\operatorname{Cov}(x_t, y_t) = E[\\eta_t (a \\eta_{t-1} + \\nu_t)] = 0$, since $\\eta_t$ is uncorrelated with its own past and with $\\nu_t$. This counterexample is correct.\n\n2.  **Non-Sufficiency**: To show correlation is not sufficient for GC, we need an example with non-zero correlation but no GC. This often arises from a shared, instantaneous common cause. Option A provides the system: $x_t = u_t$ and $y_t = u_t + \\epsilon_t$, with $u_t, \\epsilon_t$ being independent white noise. Here, the covariance is $\\operatorname{Cov}(x_t, y_t) = E[u_t(u_t + \\epsilon_t)] = E[u_t^2] = \\operatorname{Var}(u_t)  0$. However, the past of $x$, $\\{x_{t-1}, x_{t-2}, \\ldots\\} = \\{u_{t-1}, u_{t-2}, \\ldots\\}$, contains no information that helps predict the current value of $y_t = u_t + \\epsilon_t$, as all past noise terms are independent of the current ones. Thus, there is no GC from $x$ to $y$. This counterexample is also correct.\n\nSince Option A provides a correct explanation and valid, minimal counterexamples for both cases, it is the correct choice. The other options contain flawed reasoning or incorrect counterexamples.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Theoretical knowledge becomes truly powerful when applied to practical scenarios. This coding exercise challenges you to simulate a situation that is ubiquitous in neuroscience: a hidden common driver influencing two observed neural signals . By building a trivariate system where you know the ground-truth connections, you will see firsthand how a simple pairwise Granger causality analysis can yield a spurious result. This practice provides invaluable experience in demonstrating how confounding effects arise and how conditional Granger causality can serve as a critical tool to dissect more complex network interactions.",
            "id": "4166623",
            "problem": "You are tasked with designing and executing a numerical simulation to study Granger causality in the time domain for neuroscientific time series. The objective is to demonstrate that a hidden common input can induce spurious pairwise Granger causality between two observed variables, and that conditioning on the hidden driver removes this spurious effect. You must write a complete program that simulates trivariate time series data with specified autoregressive dependencies and computes pairwise and conditional Granger causality using linear regression and the classical $F$-test for nested models.\n\nFundamental base:\n- Granger causality is defined in terms of predictive capability: a time series $x_t$ is said not to Granger-cause $y_t$ if, for all positive integers $p$, past values $\\{x_{t-1},\\dots,x_{t-p}\\}$ do not improve the prediction of $y_t$ beyond what is achievable using $\\{y_{t-1},\\dots,y_{t-p}\\}$ alone. Formally, it is tested via nested linear models using Ordinary Least Squares (OLS) regression and the $F$-test.\n- Consider a vector autoregressive setting where $z_t$ is an unobserved (or observed) common driver influencing both $x_t$ and $y_t$. Spurious pairwise Granger causality may arise solely due to the driver’s lag structure, even in the absence of a direct causal pathway from $x_t$ to $y_t$ or vice versa. Conditioning on $z_t$ can resolve the confound by comparing the restricted model that includes $y_t$ and $z_t$ past to the full model that additionally includes $x_t$ past.\n\nData-generating model:\n- Simulate three scalar time series, $x_t$, $y_t$, and $z_t$, all of length $T$, from a stable autoregressive process of order $p$ with Gaussian innovations.\n- The driver $z_t$ follows\n$$\nz_t = a_{z,1} z_{t-1} + a_{z,2} z_{t-2} + \\varepsilon_{z,t},\n$$\nwhile the observed processes follow\n$$\nx_t = a_{x,1} x_{t-1} + a_{x,2} x_{t-2} + b_{x,1} z_{t-1} + b_{x,2} z_{t-2} + \\varepsilon_{x,t},\n$$\n$$\ny_t = a_{y,1} y_{t-1} + a_{y,2} y_{t-2} + b_{y,1} z_{t-1} + b_{y,2} z_{t-2} + c_{xy,1} x_{t-1} + c_{xy,2} x_{t-2} + c_{yx,1} y_{t-1}^{(x\\text{-to-}y)} + c_{yx,2} y_{t-2}^{(x\\text{-to-}y)} + \\varepsilon_{y,t},\n$$\nwhere $\\varepsilon_{x,t}$, $\\varepsilon_{y,t}$, and $\\varepsilon_{z,t}$ are independent, identically distributed Gaussian noise terms with zero mean and specified variances. In this problem, all direct $y \\to x$ couplings are set to zero, and the terms $c_{yx,1}$ and $c_{yx,2}$ are not used; the parameters $c_{xy,1}$ and $c_{xy,2}$ represent potential direct $x \\to y$ influence.\n\nGranger causality testing:\n- For a chosen lag order $p$, define the restricted model for predicting $y_t$ using the past $\\{y_{t-1},\\dots,y_{t-p}\\}$ (and optionally the driver’s past in conditional tests), and the unrestricted model that additionally includes the past of $x_t$, $\\{x_{t-1},\\dots,x_{t-p}\\}$.\n- Fit both models by OLS, compute the residual sum of squares $\\mathrm{RSS}_{\\mathrm{r}}$ and $\\mathrm{RSS}_{\\mathrm{u}}$ for restricted and unrestricted models, respectively, and evaluate the $F$-statistic\n$$\nF = \\frac{\\left(\\mathrm{RSS}_{\\mathrm{r}} - \\mathrm{RSS}_{\\mathrm{u}}\\right)/m}{\\mathrm{RSS}_{\\mathrm{u}}/(N - k_{\\mathrm{u}})},\n$$\nwhere $m$ is the number of added parameters (the number of $x_t$ lag terms), $N$ is the number of usable time points, and $k_{\\mathrm{u}}$ is the number of parameters in the unrestricted model (including the intercept). Under the null hypothesis of no Granger causality, $F$ follows a Fisher–Snedecor distribution with $m$ and $N - k_{\\mathrm{u}}$ degrees of freedom.\n- The same construction applies for testing $x_t$ Granger-causing $y_t$ pairwise (without $z_t$ in the regressors) and conditionally (including $\\{z_{t-1},\\dots,z_{t-p}\\}$ in both restricted and unrestricted models). Similarly, to test $y_t$ potentially Granger-causing $x_t$, swap the target and predictor roles.\n\nProgram requirements:\n- Implement the simulation and Granger causality testing from first principles using OLS with a constant intercept term; only the Python Standard Library, NumPy, and SciPy are permitted.\n- Use a burn-in of $500$ samples to minimize transients before analyzing the last $T$ samples.\n- Use a fixed significance level $\\alpha = 0.01$; declare Granger causality present if the $p$-value is strictly less than $\\alpha$.\n- Angle units are not applicable; no physical units are involved.\n\nTest suite:\nYour program must run the following four test cases with fixed random seeds to ensure reproducibility. Each case specifies $(T, p)$, autoregressive coefficients, driver couplings, direct couplings, innovation variances, and the random seed. The coefficients are given as $2$-tuples for order $p = 2$.\n\n- Case $1$ (hidden driver induces lead-lag spurious causality):\n  - $T = 4000$, $p = 2$, seed $= 12345$.\n  - $a_z = (0.6, -0.5)$.\n  - $a_x = (0.2, 0.0)$, $a_y = (0.2, 0.0)$.\n  - $b_x = (0.9, 0.0)$, $b_y = (0.0, 0.9)$.\n  - $c_{xy} = (0.0, 0.0)$.\n  - Noise standard deviations $(\\sigma_x, \\sigma_y, \\sigma_z) = (0.5, 0.5, 0.5)$.\n  - Expected phenomenon: pairwise $x \\to y$ Granger causality is spurious due to the driver’s lead-lag; conditional on $z_t$, it should vanish.\n\n- Case $2$ (no common driver, no direct coupling):\n  - $T = 3000$, $p = 2$, seed $= 23456$.\n  - $a_z = (0.6, -0.5)$.\n  - $a_x = (0.2, 0.0)$, $a_y = (0.2, 0.0)$.\n  - $b_x = (0.0, 0.0)$, $b_y = (0.0, 0.0)$.\n  - $c_{xy} = (0.0, 0.0)$.\n  - Noise $(\\sigma_x, \\sigma_y, \\sigma_z) = (0.5, 0.5, 0.5)$.\n  - Expected phenomenon: no Granger causality in any direction, pairwise or conditional.\n\n- Case $3$ (hidden driver plus true direct $x \\to y$ influence):\n  - $T = 4000$, $p = 2$, seed $= 54321$.\n  - $a_z = (0.6, -0.5)$.\n  - $a_x = (0.2, 0.0)$, $a_y = (0.2, 0.0)$.\n  - $b_x = (0.9, 0.0)$, $b_y = (0.0, 0.9)$.\n  - $c_{xy} = (0.5, 0.0)$.\n  - Noise $(\\sigma_x, \\sigma_y, \\sigma_z) = (0.5, 0.5, 0.5)$.\n  - Expected phenomenon: $x \\to y$ Granger causality persists even conditionally on $z_t$ due to the true direct pathway.\n\n- Case $4$ (common driver with identical lag, reduces spurious lead-lag):\n  - $T = 4000$, $p = 2$, seed $= 34567$.\n  - $a_z = (0.6, -0.5)$.\n  - $a_x = (0.2, 0.0)$, $a_y = (0.2, 0.0)$.\n  - $b_x = (0.9, 0.0)$, $b_y = (0.9, 0.0)$.\n  - $c_{xy} = (0.0, 0.0)$.\n  - Noise $(\\sigma_x, \\sigma_y, \\sigma_z) = (0.5, 0.5, 0.5)$.\n  - Expected phenomenon: absence of lead-lag reduces spurious pairwise causality; conditional tests should also show no causality.\n\nOutput specification:\n- For each test case, compute four booleans:\n  - Pairwise $x \\to y$ significance,\n  - Pairwise $y \\to x$ significance,\n  - Conditional $x \\to y$ significance (conditioning on $z_t$),\n  - Conditional $y \\to x$ significance (conditioning on $z_t$).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the $4$-boolean list for one test case. For example: $[ [\\text{True},\\text{False},\\text{False},\\text{False}], \\dots ]$.",
            "solution": "The provided Python code implements a numerical simulation to demonstrate the principles of Granger causality in the presence of a common driver. The program is composed of three main functions:\n\n1.  `_simulate_var_process`: This function generates a trivariate time series $(x_t, y_t, z_t)$ based on a specified second-order vector autoregressive (VAR(2)) model. It takes as input the model coefficients, time series length, noise levels, and a random seed for reproducibility. It includes a burn-in period to ensure the simulated series are drawn from the stationary distribution of the process.\n\n2.  `_test_granger_causality`: This function performs the Granger causality test from a `source` to a `target` series. It constructs nested linear models (restricted and unrestricted) using Ordinary Least Squares (OLS), where the unrestricted model includes past values of the source series. It calculates the residual sums of squares (RSS) for both models and computes the F-statistic. The function then determines statistical significance by comparing the resulting p-value from the F-distribution to a predefined alpha level. It can perform both pairwise tests and conditional tests (by including a `conditioner` series in both the restricted and unrestricted models).\n\n3.  `solve`: This is the main function that defines four distinct test cases, each with different parameters for the VAR model to simulate scenarios such as spurious causality from a hidden driver, no causality, and true causality. For each case, it runs the simulation and then performs four GC tests (pairwise $x \\to y$, pairwise $y \\to x$, conditional $x \\to y | z$, and conditional $y \\to x | z$). The final results are aggregated and printed in the specified list-of-lists format.",
            "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Simulates trivariate time series and performs pairwise and conditional\n    Granger causality analysis to demonstrate the effect of a hidden common driver.\n    \"\"\"\n\n    def _simulate_var_process(T, p, a_z, a_x, a_y, b_x, b_y, c_xy, sigmas, seed, burn_in):\n        \"\"\"\n        Simulates a Vector Autoregressive (VAR) process of order p.\n        \"\"\"\n        np.random.seed(seed)\n        total_len = T + burn_in\n        sigma_x, sigma_y, sigma_z = sigmas\n\n        x = np.zeros(total_len)\n        y = np.zeros(total_len)\n        z = np.zeros(total_len)\n\n        eps_x = np.random.normal(0, sigma_x, total_len)\n        eps_y = np.random.normal(0, sigma_y, total_len)\n        eps_z = np.random.normal(0, sigma_z, total_len)\n\n        # Convert coefficients to numpy arrays for dot products\n        a_z, a_x, a_y = np.array(a_z), np.array(a_x), np.array(a_y)\n        b_x, b_y, c_xy = np.array(b_x), np.array(b_y), np.array(c_xy)\n\n        for t in range(p, total_len):\n            # Lags are extracted in order [t-1, t-2, ..., t-p]\n            z_lags = z[t-p:t][::-1]\n            x_lags = x[t-p:t][::-1]\n            y_lags = y[t-p:t][::-1]\n\n            z[t] = np.dot(a_z, z_lags) + eps_z[t]\n            x[t] = np.dot(a_x, x_lags) + np.dot(b_x, z_lags) + eps_x[t]\n            y[t] = np.dot(a_y, y_lags) + np.dot(b_y, z_lags) + np.dot(c_xy, x_lags) + eps_y[t]\n\n        return x[burn_in:], y[burn_in:], z[burn_in:]\n\n    def _test_granger_causality(target, source, p, alpha, conditioner=None):\n        \"\"\"\n        Performs a Granger causality F-test.\n        Tests if `source` Granger-causes `target`, optionally conditioning on `conditioner`.\n        \"\"\"\n        T = len(target)\n        N = T - p\n        Y_vec = target[p:]\n\n        def create_lag_matrix(series, p, N):\n            lag_matrix = np.zeros((N, p))\n            for i in range(1, p + 1):\n                lag_matrix[:, i-1] = series[p-i : N+p-i]\n            return lag_matrix\n\n        # Restricted model (regresses target on its own past and conditioner's past)\n        target_lags = create_lag_matrix(target, p, N)\n        regressors_r_list = [np.ones((N, 1)), target_lags]\n        if conditioner is not None:\n            conditioner_lags = create_lag_matrix(conditioner, p, N)\n            regressors_r_list.append(conditioner_lags)\n        X_r = np.hstack(regressors_r_list)\n\n        _, rss_r_array, _, _ = np.linalg.lstsq(X_r, Y_vec, rcond=None)\n        # If the regression is perfect, lstsq returns an empty array for residuals\n        rss_r = rss_r_array[0] if rss_r_array.size > 0 else 0.0\n\n        # Unrestricted model (adds source's past to the restricted model)\n        source_lags = create_lag_matrix(source, p, N)\n        X_u = np.hstack([X_r, source_lags])\n\n        _, rss_u_array, _, _ = np.linalg.lstsq(X_u, Y_vec, rcond=None)\n        rss_u = rss_u_array[0] if rss_u_array.size > 0 else 0.0\n\n        # F-test\n        m = p  # number of added parameters (lags of the source variable)\n        k_u = X_u.shape[1]  # number of parameters in the unrestricted model\n        \n        df1 = m\n        df2 = N - k_u\n\n        if df2 = 0:\n            return False # Not enough degrees of freedom for the test\n\n        if rss_u  np.finfo(float).eps:\n            # A perfect fit in the unrestricted model implies infinite F-statistic,\n            # which means p-value is 0, hence significant.\n            # However, this scenario is unlikely with noise.\n            # If RSS_r is also 0, F-stat is NaN, result should be False.\n            return rss_r > np.finfo(float).eps\n\n        f_stat = ((rss_r - rss_u) / df1) / (rss_u / df2)\n        p_value = stats.f.sf(f_stat, df1, df2)\n\n        return p_value  alpha\n\n    # --- Test Suite ---\n    # Each tuple: (T, p, seed, a_z, a_x, a_y, b_x, b_y, c_xy, sigmas)\n    test_cases = [\n        # Case 1: Hidden driver induces lead-lag spurious causality\n        (4000, 2, 12345, (0.6, -0.5), (0.2, 0.0), (0.2, 0.0), (0.9, 0.0), (0.0, 0.9), (0.0, 0.0), (0.5, 0.5, 0.5)),\n        # Case 2: No common driver, no direct coupling\n        (3000, 2, 23456, (0.6, -0.5), (0.2, 0.0), (0.2, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.5, 0.5, 0.5)),\n        # Case 3: Hidden driver plus true direct x -> y influence\n        (4000, 2, 54321, (0.6, -0.5), (0.2, 0.0), (0.2, 0.0), (0.9, 0.0), (0.0, 0.9), (0.5, 0.0), (0.5, 0.5, 0.5)),\n        # Case 4: Common driver with identical lag\n        (4000, 2, 34567, (0.6, -0.5), (0.2, 0.0), (0.2, 0.0), (0.9, 0.0), (0.9, 0.0), (0.0, 0.0), (0.5, 0.5, 0.5)),\n    ]\n\n    alpha = 0.01\n    burn_in = 500\n    all_results = []\n\n    for case_params in test_cases:\n        T, p, seed, a_z, a_x, a_y, b_x, b_y, c_xy, sigmas = case_params\n        \n        x, y, z = _simulate_var_process(\n            T, p, a_z, a_x, a_y, b_x, b_y, c_xy, sigmas, seed, burn_in\n        )\n\n        # Pairwise tests\n        gc_xy_pair = _test_granger_causality(y, x, p, alpha)\n        gc_yx_pair = _test_granger_causality(x, y, p, alpha)\n\n        # Conditional tests (conditioning on z)\n        gc_xy_cond = _test_granger_causality(y, x, p, alpha, conditioner=z)\n        gc_yx_cond = _test_granger_causality(x, y, p, alpha, conditioner=z)\n\n        case_results = [gc_xy_pair, gc_yx_pair, gc_xy_cond, gc_yx_cond]\n        all_results.append(case_results)\n\n    # Format the output string to be a comma-separated list of lists, with no spaces.\n    inner_parts = [str(r).replace(\" \", \"\") for r in all_results]\n    body = \",\".join(inner_parts)\n    final_output = f\"[{body}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}