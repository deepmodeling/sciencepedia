{
    "hands_on_practices": [
        {
            "introduction": "A fundamental challenge in functional connectivity is distinguishing direct interactions from spurious correlations caused by a common driver. This practice provides a concrete, hands-on calculation to demonstrate how two time series can show a strong Pearson correlation that vanishes when the influence of a third, confounding signal is removed using partial correlation. Mastering this distinction is crucial for drawing valid inferences about network architecture from neuroimaging data .",
            "id": "4165717",
            "problem": "Consider a simple functional connectivity (FC) scenario in which a latent common driver signal induces strong correlation between two observed nodes. Let there be three scalar time series sampled at $n=5$ time points, denoted by $X(t)$, $Y(t)$, and $Z(t)$ for $t \\in \\{1,2,3,4,5\\}$. Assume the following generative structure: $X(t) = Z(t) + \\epsilon_{X}(t)$ and $Y(t) = Z(t) + \\epsilon_{Y}(t)$, where $\\epsilon_{X}(t)$ and $\\epsilon_{Y}(t)$ are small perturbations. The observed values are:\n- $Z$: $(-2, -1, 0, 1, 2)$,\n- $\\epsilon_{X}$: $(0.1, -0.2, 0, 0.2, -0.1)$,\n- $\\epsilon_{Y}$: $(0.15, -0.15, 0, -0.15, 0.15)$,\nand thus\n- $X$: $(-1.9, -1.2, 0, 1.2, 1.9)$,\n- $Y$: $(-1.85, -1.15, 0, 0.85, 2.15)$.\n\nStarting from the core definitions of sample mean, sample covariance, Pearson correlation, and ordinary least squares linear regression (i.e., fitting $X$ as a linear function of $Z$ and $Y$ as a linear function of $Z$ and then forming residuals), do the following:\n1. Compute the Pearson correlation between $X$ and $Y$ to illustrate the apparent strong FC induced by the common driver $Z$.\n2. Compute the partial correlation between $X$ and $Y$ conditional on $Z$ by first regressing $X$ on $Z$ and $Y$ on $Z$, forming the residual time series, and then computing the Pearson correlation between those residuals.\n\nExpress the final answer as the partial correlation value as a dimensionless quantity. If rounding is necessary, round your final numerical answer to four significant figures. No other intermediate results need to be rounded.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and self-consistent. The data and generative model provided are standard for illustrating concepts in functional connectivity analysis. We may proceed with the solution.\n\nThe objective is to compute the Pearson correlation between time series $X(t)$ and $Y(t)$, and then the partial correlation between $X(t)$ and $Y(t)$ conditional on $Z(t)$. The problem involves $n=5$ time points.\n\nThe provided time series are:\n$X = (-1.9, -1.2, 0, 1.2, 1.9)$\n$Y = (-1.85, -1.15, 0, 0.85, 2.15)$\n$Z = (-2, -1, 0, 1, 2)$\n\nFirst, we compute the sample mean for each time series. The sample mean $\\bar{V}$ for a time series $V(t)$ of length $n$ is defined as $\\bar{V} = \\frac{1}{n} \\sum_{t=1}^{n} V(t)$.\nFor $X(t)$:\n$$ \\bar{X} = \\frac{1}{5} (-1.9 - 1.2 + 0 + 1.2 + 1.9) = \\frac{0}{5} = 0 $$\nFor $Y(t)$:\n$$ \\bar{Y} = \\frac{1}{5} (-1.85 - 1.15 + 0 + 0.85 + 2.15) = \\frac{0}{5} = 0 $$\nFor $Z(t)$:\n$$ \\bar{Z} = \\frac{1}{5} (-2 - 1 + 0 + 1 + 2) = \\frac{0}{5} = 0 $$\nThe fact that all sample means are zero simplifies subsequent calculations.\n\n**1. Pearson Correlation between $X$ and $Y$**\n\nThe Pearson correlation coefficient $\\rho_{XY}$ between two series $X(t)$ and $Y(t)$ is defined as:\n$$ \\rho_{XY} = \\frac{\\sum_{t=1}^{n}(X(t) - \\bar{X})(Y(t) - \\bar{Y})}{\\sqrt{\\sum_{t=1}^{n}(X(t) - \\bar{X})^2 \\sum_{t=1}^{n}(Y(t) - \\bar{Y})^2}} $$\nSince $\\bar{X} = 0$ and $\\bar{Y} = 0$, this simplifies to:\n$$ \\rho_{XY} = \\frac{\\sum_{t=1}^{n}X(t)Y(t)}{\\sqrt{\\left(\\sum_{t=1}^{n}X(t)^2\\right) \\left(\\sum_{t=1}^{n}Y(t)^2\\right)}} $$\nWe compute the necessary sums:\nThe numerator term, the sum of products:\n$$ \\sum_{t=1}^{5}X(t)Y(t) = (-1.9)(-1.85) + (-1.2)(-1.15) + (0)(0) + (1.2)(0.85) + (1.9)(2.15) $$\n$$ = 3.515 + 1.38 + 0 + 1.02 + 4.085 = 10.0 $$\nThe sum of squares for $X(t)$:\n$$ \\sum_{t=1}^{5}X(t)^2 = (-1.9)^2 + (-1.2)^2 + 0^2 + 1.2^2 + 1.9^2 $$\n$$ = 3.61 + 1.44 + 0 + 1.44 + 3.61 = 10.1 $$\nThe sum of squares for $Y(t)$:\n$$ \\sum_{t=1}^{5}Y(t)^2 = (-1.85)^2 + (-1.15)^2 + 0^2 + (0.85)^2 + (2.15)^2 $$\n$$ = 3.4225 + 1.3225 + 0 + 0.7225 + 4.6225 = 10.09 $$\nNow, we compute the Pearson correlation:\n$$ \\rho_{XY} = \\frac{10.0}{\\sqrt{(10.1)(10.09)}} = \\frac{10.0}{\\sqrt{101.909}} \\approx 0.99058 $$\nThis confirms the strong apparent functional connectivity induced by the common driver $Z(t)$.\n\n**2. Partial Correlation between $X$ and $Y$ conditional on $Z$**\n\nTo find the partial correlation $\\rho_{XY|Z}$, we first perform ordinary least squares (OLS) regression of $X$ on $Z$ and $Y$ on $Z$, and then compute the Pearson correlation of the resulting residuals.\n\n**Regression of $X$ on $Z$**:\nWe fit the model $X(t) = \\alpha_{XZ} + \\beta_{XZ} Z(t)$. The OLS estimates for the intercept $\\alpha_{XZ}$ and slope $\\beta_{XZ}$ are:\n$$ \\beta_{XZ} = \\frac{\\sum_{t=1}^{n}(X(t) - \\bar{X})(Z(t) - \\bar{Z})}{\\sum_{t=1}^{n}(Z(t) - \\bar{Z})^2} \\quad \\text{and} \\quad \\alpha_{XZ} = \\bar{X} - \\beta_{XZ} \\bar{Z} $$\nSince $\\bar{X} = 0$ and $\\bar{Z} = 0$, we have $\\alpha_{XZ} = 0$. The slope simplifies to:\n$$ \\beta_{XZ} = \\frac{\\sum_{t=1}^{n}X(t)Z(t)}{\\sum_{t=1}^{n}Z(t)^2} $$\nWe compute the required sums:\n$$ \\sum_{t=1}^{5}Z(t)^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 4 + 1 + 0 + 1 + 4 = 10 $$\n$$ \\sum_{t=1}^{5}X(t)Z(t) = (-1.9)(-2) + (-1.2)(-1) + (0)(0) + (1.2)(1) + (1.9)(2) $$\n$$ = 3.8 + 1.2 + 0 + 1.2 + 3.8 = 10 $$\nSo, the slope is $\\beta_{XZ} = \\frac{10}{10} = 1$. The predicted values are $\\hat{X}(t) = 1 \\cdot Z(t) = Z(t)$.\nThe residuals for $X$, which we denote $R_X(t)$, are:\n$$ R_X(t) = X(t) - \\hat{X}(t) = X(t) - Z(t) $$\nUsing the given values, the residual series is:\n$R_X = (-1.9 - (-2), -1.2 - (-1), 0 - 0, 1.2 - 1, 1.9 - 2) = (0.1, -0.2, 0, 0.2, -0.1)$.\nThis is, by construction, the noise series $\\epsilon_X(t)$.\n\n**Regression of $Y$ on $Z$**:\nSimilarly, for the model $Y(t) = \\alpha_{YZ} + \\beta_{YZ} Z(t)$, since $\\bar{Y} = 0$ and $\\bar{Z} = 0$, we have $\\alpha_{YZ} = 0$. The slope is:\n$$ \\beta_{YZ} = \\frac{\\sum_{t=1}^{n}Y(t)Z(t)}{\\sum_{t=1}^{n}Z(t)^2} $$\nThe denominator is already known to be $10$. We compute the numerator:\n$$ \\sum_{t=1}^{5}Y(t)Z(t) = (-1.85)(-2) + (-1.15)(-1) + (0)(0) + (0.85)(1) + (2.15)(2) $$\n$$ = 3.7 + 1.15 + 0 + 0.85 + 4.3 = 10 $$\nSo, the slope is $\\beta_{YZ} = \\frac{10}{10} = 1$. The predicted values are $\\hat{Y}(t) = 1 \\cdot Z(t) = Z(t)$.\nThe residuals for $Y$, which we denote $R_Y(t)$, are:\n$$ R_Y(t) = Y(t) - \\hat{Y}(t) = Y(t) - Z(t) $$\nUsing the given values, the residual series is:\n$R_Y = (-1.85 - (-2), -1.15 - (-1), 0 - 0, 0.85 - 1, 2.15 - 2) = (0.15, -0.15, 0, -0.15, 0.15)$.\nThis is, by construction, the noise series $\\epsilon_Y(t)$.\n\n**Correlation of Residuals**:\nThe partial correlation $\\rho_{XY|Z}$ is the Pearson correlation between the residual series $R_X(t)$ and $R_Y(t)$.\nFirst, we compute the means of the residuals:\n$$ \\bar{R}_X = \\frac{1}{5}(0.1 - 0.2 + 0 + 0.2 - 0.1) = 0 $$\n$$ \\bar{R}_Y = \\frac{1}{5}(0.15 - 0.15 + 0 - 0.15 + 0.15) = 0 $$\nSince the means are zero, the correlation formula is again simplified:\n$$ \\rho_{XY|Z} = \\rho_{R_X R_Y} = \\frac{\\sum_{t=1}^{n}R_X(t)R_Y(t)}{\\sqrt{\\left(\\sum_{t=1}^{n}R_X(t)^2\\right) \\left(\\sum_{t=1}^{n}R_Y(t)^2\\right)}} $$\nWe compute the numerator, which is the sum of products of the residuals:\n$$ \\sum_{t=1}^{5}R_X(t)R_Y(t) = (0.1)(0.15) + (-0.2)(-0.15) + (0)(0) + (0.2)(-0.15) + (-0.1)(0.15) $$\n$$ = 0.015 + 0.03 + 0 - 0.03 - 0.015 = 0 $$\nSince the numerator of the correlation expression is $0$, and the denominator will be non-zero as the residual series are not identically zero, the partial correlation must be $0$.\nThis result demonstrates that after accounting for the linear effect of the common driver $Z(t)$, there is no remaining linear relationship between $X(t)$ and $Y(t)$. The apparent strong correlation was spurious and entirely mediated by $Z(t)$.\nThe final answer is the value of this partial correlation.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "While partial correlation helps isolate direct connections, computing it for every pair in a large network can be cumbersome. This exercise introduces an elegant and efficient matrix-based approach using the precision matrix, the inverse of the covariance matrix, which encodes the entire network's conditional independence structure. You will implement an algorithm to derive a partial correlation network from a covariance matrix and learn to interpret its structure, a core skill in network neuroscience .",
            "id": "4165697",
            "problem": "You are given a symmetric, positive semidefinite covariance matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$ estimated from multivariate time series, where each variable represents a brain region of interest. Under the Gaussian Graphical Model (GGM) assumption, conditional independence structure is encoded by the precision matrix $\\Theta = \\Sigma^{-1}$, and functional connectivity can be quantified via partial correlations between variables. Starting from the fundamental definitions of covariance and conditional independence in multivariate normal distributions, derive an algorithm to compute the partial correlation matrix from $\\Sigma$ through $\\Theta$ and to interpret sparsity patterns by thresholding partial correlations.\n\nYour program must implement the following steps for each test case:\n- Given $\\Sigma$, construct a ridge-regularized covariance $\\Sigma_{\\epsilon} = \\Sigma + \\epsilon I_p$ for numerical stability, where $I_p$ is the $p \\times p$ identity matrix and $\\epsilon \\ge 0$ is provided.\n- Compute the precision matrix $\\Theta = \\Sigma_{\\epsilon}^{-1}$.\n- From $\\Theta$, derive the partial correlation matrix $\\Pi$ such that each off-diagonal entry $\\Pi_{ij}$ equals the partial correlation between variable $i$ and variable $j$ conditioning on all other variables. The diagonal may be set to $1$ by convention.\n- Define an undirected adjacency matrix $A$ by thresholding the absolute partial correlations: $A_{ij} = 1$ if $|\\Pi_{ij}| \\ge \\tau$ and $i \\ne j$, otherwise $A_{ij} = 0$, where the threshold $\\tau > 0$ is provided.\n- Compute the integer $E$ equal to the number of undirected edges, that is $E = \\sum_{1 \\le i < j \\le p} A_{ij}$.\n- Compute the integer $C$ equal to the number of connected components in the undirected graph induced by $A$.\n\nYour program must process the following test suite of matrices and parameters:\n- Test case $1$ (happy path, tridiagonal precision inducing a chain structure):\n  The covariance matrix is\n  $$\\Sigma^{(1)} = \\begin{bmatrix}\n  1.2352941176470589 & 0.5882352941176471 & 0.23529411764705885 \\\\\n  0.5882352941176471 & 1.4705882352941178 & 0.5882352941176471 \\\\\n  0.23529411764705885 & 0.5882352941176471 & 1.2352941176470589\n  \\end{bmatrix},$$\n  with $\\epsilon^{(1)} = 10^{-12}$ and $\\tau^{(1)} = 0.05$.\n- Test case $2$ (near-singular boundary, compound symmetry with strong collinearity):\n  The covariance matrix is\n  $$\\Sigma^{(2)} = \\begin{bmatrix}\n  1.0 & 0.95 & 0.95 & 0.95 \\\\\n  0.95 & 1.0 & 0.95 & 0.95 \\\\\n  0.95 & 0.95 & 1.0 & 0.95 \\\\\n  0.95 & 0.95 & 0.95 & 1.0\n  \\end{bmatrix},$$\n  with $\\epsilon^{(2)} = 10^{-6}$ and $\\tau^{(2)} = 0.2$.\n- Test case $3$ (edge case, block-diagonal structure implying segregated subnetworks):\n  The covariance matrix is\n  $$\\Sigma^{(3)} = \\begin{bmatrix}\n  1.0 & 0.6 & 0.0 & 0.0 \\\\\n  0.6 & 1.0 & 0.0 & 0.0 \\\\\n  0.0 & 0.0 & 1.0 & 0.6 \\\\\n  0.0 & 0.0 & 0.6 & 1.0\n  \\end{bmatrix},$$\n  with $\\epsilon^{(3)} = 0.0$ and $\\tau^{(3)} = 0.2$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-integer list $[E, C]$ in order. For example, the output format must be exactly\n$$[[E^{(1)},C^{(1)}],[E^{(2)},C^{(2)}],[E^{(3)},C^{(3)}]].$$\nNo additional text should be printed.",
            "solution": "The fundamental base is the multivariate normal model and the definitions of covariance and conditional independence. Let $\\mathbf{x} \\in \\mathbb{R}^{p}$ be jointly Gaussian with mean $\\mathbf{0}$ and covariance $\\Sigma \\in \\mathbb{R}^{p \\times p}$, that is $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$. The precision matrix is defined by $\\Theta = \\Sigma^{-1}$. A well-tested result in Gaussian Graphical Models (GGM) is that $\\Theta_{ij} = 0$ if and only if variables $i$ and $j$ are conditionally independent given all other variables. This connects sparsity in $\\Theta$ to conditional independence structure, which is the foundation for interpreting functional connectivity via partial correlations.\n\nWe derive the partial correlation formula from first principles. Partition $\\mathbf{x}$ into $(x_i, x_j, \\mathbf{x}_{-ij})$, where $\\mathbf{x}_{-ij}$ denotes all variables except $x_i$ and $x_j$. The conditional distribution of $(x_i, x_j)$ given $\\mathbf{x}_{-ij}$ is bivariate normal. The conditional covariance is given by the Schur complement of the block corresponding to $(i,j)$ in $\\Sigma$, but an equivalent and more direct route uses the precision matrix. For a Gaussian vector, the regression of $x_i$ on all other variables has coefficient vector proportional to the corresponding row of $\\Theta$. In particular, the partial regression coefficient of $x_i$ on $x_j$ (controlling for all other variables) is\n$$\\beta_{i \\leftarrow j \\mid -ij} = -\\frac{\\Theta_{ij}}{\\Theta_{ii}}.$$\nSymmetrically,\n$$\\beta_{j \\leftarrow i \\mid -ij} = -\\frac{\\Theta_{ij}}{\\Theta_{jj}}.$$\nThe partial correlation $\\Pi_{ij}$ between $x_i$ and $x_j$ given $\\mathbf{x}_{-ij}$ is the correlation between their regression residuals. For Gaussian variables, this correlation equals the normalized negative of the off-diagonal precision entry:\n$$\\Pi_{ij} = -\\frac{\\Theta_{ij}}{\\sqrt{\\Theta_{ii} \\Theta_{jj}}}, \\quad i \\ne j,$$\nand by convention $\\Pi_{ii} = 1$. This normalization arises because the residual variances are inversely proportional to the corresponding diagonal entries of $\\Theta$, and correlation is covariance normalized by the geometric mean of standard deviations.\n\nAlgorithmically:\n- Numerical stability may require regularization when $\\Sigma$ is near singular. We form $\\Sigma_{\\epsilon} = \\Sigma + \\epsilon I_p$ for a small ridge parameter $\\epsilon \\ge 0$, which preserves symmetry and improves conditioning.\n- Compute $\\Theta = \\Sigma_{\\epsilon}^{-1}$ using a stable inversion routine, and symmetrize by $(\\Theta + \\Theta^\\top)/2$ to reduce floating-point asymmetry.\n- Compute $\\Pi$ using the formula above. Let $d = \\sqrt{\\operatorname{diag}(\\Theta)} \\in \\mathbb{R}^p$, then\n$$\\Pi = -\\Theta \\oslash (d d^\\top),$$\nwhere $\\oslash$ denotes elementwise division and we set the diagonal of $\\Pi$ to $1$.\n- To interpret sparsity, we threshold the absolute off-diagonal entries: $A_{ij} = 1$ if $|\\Pi_{ij}| \\ge \\tau$ and $i \\ne j$, and $A_{ij} = 0$ otherwise. This yields an undirected graph. Zeros in $\\Theta$ imply exact conditional independence and hence zero partial correlation in the population; in finite samples and with numerical error, small-magnitude $\\Pi_{ij}$ suggests weak or absent direct functional connectivity, so thresholding captures sparsity.\n- The number of edges is $E = \\sum_{1 \\le i < j \\le p} A_{ij}$, counting each undirected edge once.\n- The number of connected components $C$ is computed by graph traversal (for example, depth-first search or breadth-first search) on the adjacency graph, counting isolated nodes as components.\n\nApplying to the test suite:\n- Test case $1$ is constructed from a tridiagonal precision matrix with zero entry at $(1,3)$; its given covariance $\\Sigma^{(1)}$ is the inverse of a precision with $\\Theta^{(1)}_{12} = \\Theta^{(1)}_{23} \\ne 0$ and $\\Theta^{(1)}_{13} = 0$. The result is a chain graph with two edges and one connected component when using a small threshold $\\tau^{(1)} = 0.05$.\n- Test case $2$ exhibits compound symmetry with strong collinearity ($\\Sigma^{(2)}$ has off-diagonal $0.95$), which is ill-conditioned. The ridge $\\epsilon^{(2)} = 10^{-6}$ stabilizes inversion. Partial correlations are nonzero for most pairs, producing a dense graph with many edges; with $\\tau^{(2)} = 0.2$ the graph is expected to be fully connected.\n- Test case $3$ is block-diagonal, implying two segregated subnetworks. Thresholding with $\\tau^{(3)} = 0.2$ yields edges only within blocks and produces two connected components.\n\nThe program implements this pipeline and outputs the list $[[E^{(1)},C^{(1)}],[E^{(2)},C^{(2)}],[E^{(3)},C^{(3)}]]$ as specified, ensuring scientific realism by deriving partial correlations from the precision matrix and interpreting sparsity via conditional independence structure in the GGM framework.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_partial_correlation_from_cov(Sigma: np.ndarray, eps: float = 0.0) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the partial correlation matrix from a covariance matrix via the precision matrix.\n    Returns (Pi, Theta), where Pi is the partial correlation matrix and Theta is the precision.\n    \"\"\"\n    p = Sigma.shape[0]\n    # Ridge regularization for numerical stability\n    Sigma_reg = Sigma + eps * np.eye(p)\n    # Invert to get precision\n    Theta = np.linalg.inv(Sigma_reg)\n    # Symmetrize to reduce numerical asymmetry\n    Theta = (Theta + Theta.T) / 2.0\n    # Compute partial correlations: Pi_ij = -Theta_ij / sqrt(Theta_ii * Theta_jj)\n    d = np.sqrt(np.diag(Theta))\n    # Avoid division by zero: if any diagonal is zero (should not happen for SPD), fallback to tiny epsilon\n    d = np.where(d == 0.0, 1e-18, d)\n    denom = np.outer(d, d)\n    Pi = -Theta / denom\n    # Set diagonal to 1 by convention\n    np.fill_diagonal(Pi, 1.0)\n    return Pi, Theta\n\ndef adjacency_from_partial_corr(Pi: np.ndarray, tau: float) -> np.ndarray:\n    \"\"\"\n    Build undirected adjacency matrix by thresholding absolute partial correlations.\n    \"\"\"\n    p = Pi.shape[0]\n    A = (np.abs(Pi) >= tau)\n    # No self-loops\n    np.fill_diagonal(A, False)\n    # Symmetrize adjacency\n    A = np.logical_or(A, A.T)\n    return A\n\ndef count_edges(A: np.ndarray) -> int:\n    \"\"\"\n    Count undirected edges in adjacency matrix (upper triangle).\n    \"\"\"\n    return int(np.sum(np.triu(A, 1)))\n\ndef count_connected_components(A: np.ndarray) -> int:\n    \"\"\"\n    Count connected components in an undirected graph with adjacency matrix A.\n    \"\"\"\n    p = A.shape[0]\n    visited = np.zeros(p, dtype=bool)\n    components = 0\n    for start in range(p):\n        if not visited[start]:\n            components += 1\n            # BFS\n            queue = [start]\n            visited[start] = True\n            while queue:\n                u = queue.pop(0)\n                neighbors = np.where(A[u])[0]\n                for v in neighbors:\n                    if not visited[v]:\n                        visited[v] = True\n                        queue.append(v)\n    return components\n\ndef solve():\n    # Define the test cases from the problem statement.\n    Sigma1 = np.array([\n        [1.2352941176470589, 0.5882352941176471, 0.23529411764705885],\n        [0.5882352941176471, 1.4705882352941178, 0.5882352941176471],\n        [0.23529411764705885, 0.5882352941176471, 1.2352941176470589]\n    ], dtype=float)\n    eps1 = 1e-12\n    tau1 = 0.05\n\n    Sigma2 = np.array([\n        [1.0, 0.95, 0.95, 0.95],\n        [0.95, 1.0, 0.95, 0.95],\n        [0.95, 0.95, 1.0, 0.95],\n        [0.95, 0.95, 0.95, 1.0]\n    ], dtype=float)\n    eps2 = 1e-6\n    tau2 = 0.2\n\n    Sigma3 = np.array([\n        [1.0, 0.6, 0.0, 0.0],\n        [0.6, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.6],\n        [0.0, 0.0, 0.6, 1.0]\n    ], dtype=float)\n    eps3 = 0.0\n    tau3 = 0.2\n\n    test_cases = [\n        (Sigma1, eps1, tau1),\n        (Sigma2, eps2, tau2),\n        (Sigma3, eps3, tau3),\n    ]\n\n    results = []\n    for Sigma, eps, tau in test_cases:\n        Pi, Theta = compute_partial_correlation_from_cov(Sigma, eps)\n        A = adjacency_from_partial_corr(Pi, tau)\n        E = count_edges(A)\n        C = count_connected_components(A)\n        results.append([E, C])\n\n    # Final print statement in the exact required format.\n    print(f\"[[{results[0][0]},{results[0][1]}],[{results[1][0]},{results[1][1]}],[{results[2][0]},{results[2][1]}]]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Functional brain networks are not static; their connections can fluctuate rapidly over time. This practice moves beyond time-averaged connectivity to explore dynamic phase synchrony, a powerful method for tracking transient coupling between oscillating neural signals. You will use the Hilbert transform to extract instantaneous phase and compute the Phase Locking Value (PLV) in sliding windows, providing practical experience in analyzing the time-varying nature of brain communication .",
            "id": "4165636",
            "problem": "You are given pairs of discrete-time signals intended to model neural time series with narrowband oscillatory content. For each pair, you must compute instantaneous phases via the analytic signal obtained from the Hilbert transform, form samplewise phase differences, and assess time-varying phase synchrony by evaluating the Phase Locking Value across contiguous, overlapping sliding windows. Angles must be handled in radians. All computations should be expressed in dimensionless units.\n\nFundamental base and core definitions to use:\n- Let $x[n]$ be a real-valued sampled time series and let $f_s$ denote the sampling frequency in $\\mathrm{Hz}$. The continuous-time analytic signal associated with $x(t)$ is defined as $x_a(t) = x(t) + i \\, \\mathcal{H}\\{x(t)\\}$, where $\\mathcal{H}\\{\\cdot\\}$ is the Hilbert transform and $i$ is the imaginary unit. In discrete time, the analytic signal $z_x[n]$ is obtained by applying the Hilbert transform to $x[n]$ using a standard discrete approximation. The instantaneous phase is then $\\phi_x[n] = \\arg(z_x[n])$, where $\\arg(\\cdot)$ returns the complex argument in radians.\n- Given two signals $x[n]$ and $y[n]$, define the samplewise phase difference as $\\Delta \\phi[n] = \\phi_x[n] - \\phi_y[n]$ in radians. The time-varying synchrony should be evaluated across sliding windows, where each window is specified by a duration $L$ seconds and a step size $S$ seconds. Let $L_s = \\mathrm{round}(L \\cdot f_s)$ be the number of samples per window and $S_s = \\mathrm{round}(S \\cdot f_s)$ the stride in samples. Use only windows that fit entirely within the signal, with start indices $s_m = m \\, S_s$ for integer $m$ such that $s_m + L_s \\leq N$, where $N$ is the total number of samples.\n- In each window, quantify the degree of phase synchrony using the magnitude of the average unit phasor constructed from the samplewise phase differences in that window. Angles must be treated in radians throughout.\n\nImplement the algorithmic pipeline strictly following the above fundamental definitions. The program must synthesize the test signals internally, compute the analytic signals via a standard discrete Hilbert transform, extract instantaneous phases, compute samplewise phase differences, and then evaluate the synchrony measure per window as described.\n\nUse the following test suite. Each test case is a parameter set; in all cases use $f_s$ in $\\mathrm{Hz}$ and durations in seconds. Whenever randomness is used, initialize your random number generator to a fixed seed so that outputs are reproducible. In each case below, $t[n] = n / f_s$ for $n = 0, 1, \\dots, N-1$.\n\n- Test Case A (perfect phase locking):\n    - $N = 2048$, $f_s = 512$, $f = 10$.\n    - $x[n] = \\sin(2\\pi f \\, t[n])$.\n    - $y[n] = \\sin(2\\pi f \\, t[n] + \\Delta)$ with $\\Delta = \\pi / 4$.\n    - Window duration $L = 1.0$, step $S = 0.5$.\n    - No added noise.\n- Test Case B (frequency mismatch, low synchrony):\n    - $N = 2048$, $f_s = 512$, $f_x = 10$, $f_y = 12$.\n    - $x[n] = \\sin(2\\pi f_x \\, t[n])$.\n    - $y[n] = \\sin(2\\pi f_y \\, t[n])$.\n    - Window duration $L = 2.0$, step $S = 0.5$.\n    - No added noise.\n- Test Case C (transient synchrony with noise):\n    - $N = 4096$, $f_s = 512$, $f_1 = 8$, $f_2 = 8.5$.\n    - $x[n] = \\sin(2\\pi f_1 \\, t[n]) + \\eta_x[n]$, where $\\eta_x[n]$ is zero-mean Gaussian noise with standard deviation $0.1$ in dimensionless units.\n    - $y[n]$ is piecewise:\n        - For $t[n] \\in [0, 1.5)$ seconds: $y[n] = \\sin(2\\pi f_1 \\, t[n] + 0)$.\n        - For $t[n] \\in [1.5, 5.5)$ seconds: $y[n] = \\sin(2\\pi f_2 \\, t[n] + 0)$.\n        - For $t[n] \\in [5.5, 8.0)$ seconds: $y[n] = \\sin(2\\pi f_1 \\, t[n] + \\pi / 6)$.\n      Then add independent noise $\\eta_y[n]$ of the same distribution as $\\eta_x[n]$.\n    - Window duration $L = 1.0$, step $S = 0.25$.\n- Test Case D (single full-length window with noise):\n    - $N = 1024$, $f_s = 512$, $f = 6$, $\\Delta = \\pi / 3$.\n    - $x[n] = \\sin(2\\pi f \\, t[n]) + \\xi_x[n]$, $y[n] = \\sin(2\\pi f \\, t[n] + \\Delta) + \\xi_y[n]$, where $\\xi_x[n]$ and $\\xi_y[n]$ are independent zero-mean Gaussian noises with standard deviation $0.2$ in dimensionless units.\n    - Window duration $L = 2.0$, step $S = 2.0$.\n\nYour program must produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each element is itself a list of floating-point values corresponding to the per-window synchrony measure for that test case. Each floating-point value must be rounded to six decimal places. For example, the output format must be of the form $[[r_{A,1},r_{A,2},\\dots],[r_{B,1},\\dots],[r_{C,1},\\dots],[r_{D,1},\\dots]]$ with no spaces anywhere in the line. Angles must be interpreted in radians for all steps, and the outputs are dimensionless floats rounded to six decimal places.",
            "solution": "The problem requires the implementation of a computational pipeline to assess time-varying phase synchrony between pairs of discrete-time signals. The methodology is based on the concept of the analytic signal, obtained via the Hilbert transform, from which instantaneous phase is derived. The degree of synchrony is quantified by the Phase Locking Value (PLV) computed over a series of sliding windows.\n\nThe analysis proceeds in four main stages:\n1.  **Analytic Signal and Instantaneous Phase Extraction:** For a given real-valued discrete signal $x[n]$, its associated analytic signal $z_x[n]$ is a complex-valued signal whose real part is $x[n]$ and whose imaginary part is the Hilbert transform of $x[n]$, denoted $\\mathcal{H}\\{x[n]\\}$. Thus, $z_x[n] = x[n] + i\\mathcal{H}\\{x[n]\\}$. The Hilbert transform is implemented using a standard discrete approximation, typically based on the Fast Fourier Transform (FFT). The instantaneous phase $\\phi_x[n]$ of the signal $x[n]$ at each time point $n$ is then the argument of the corresponding analytic signal sample:\n    $$\n    \\phi_x[n] = \\arg(z_x[n])\n    $$\n    This operation is performed for both signals in a pair, $x[n]$ and $y[n]$, to yield their respective instantaneous phase series, $\\phi_x[n]$ and $\\phi_y[n]$. All angles are computed in radians.\n\n2.  **Samplewise Phase Difference:** The relationship between the phases of the two signals is captured by their samplewise difference, $\\Delta\\phi[n]$:\n    $$\n    \\Delta\\phi[n] = \\phi_x[n] - \\phi_y[n]\n    $$\n    If the two signals are phase-locked, this difference will tend to be constant over time. If their frequencies differ, this phase difference will drift.\n\n3.  **Sliding Window Analysis:** To capture the time-varying nature of phase synchrony, the analysis is segmented into contiguous, overlapping windows. Each window has a duration of $L$ seconds, corresponding to $L_s = \\mathrm{round}(L \\cdot f_s)$ samples, and the windows are advanced by a step size of $S$ seconds, corresponding to a stride of $S_s = \\mathrm{round}(S \\cdot f_s)$ samples. The analysis is performed on all windows that fit entirely within the total signal length $N$. The start index of the $m$-th window (for $m=0, 1, 2, \\dots$) is given by $s_m = m \\cdot S_s$, and the last valid window is the one for which $s_m + L_s \\leq N$.\n\n4.  **Phase Locking Value (PLV) Computation:** Within each window, a single value is computed to quantify the consistency of the phase difference. This measure, the PLV, is defined as the magnitude of the mean of the unit phasors constructed from the phase differences in that window. For the $m$-th window, starting at index $s_m$ and of length $L_s$, the PLV is:\n    $$\n    \\mathrm{PLV}_m = \\left| \\frac{1}{L_s} \\sum_{k=0}^{L_s-1} e^{i \\Delta\\phi[s_m + k]} \\right|\n    $$\n    The PLV ranges from $0$ to $1$. A value of $1$ indicates perfect phase locking (the phase difference $\\Delta\\phi$ is constant within the window), while a value near $0$ indicates a lack of locking (the phase differences are uniformly distributed, causing the unit vectors to cancel out).\n\nThe implementation will process four distinct test cases designed to evaluate the algorithm's performance under different conditions:\n- **Test Case A:** Two sinusoids of the same frequency but with a constant phase offset. This should yield a PLV very close to $1.0$ in all windows, demonstrating perfect phase synchrony.\n- **Test Case B:** Two sinusoids with different frequencies. The phase difference will drift linearly, resulting in low PLV values, correctly identifying asynchrony.\n- **Test Case C:** A more complex scenario with noise and piecewise signal definitions, simulating transient synchrony. The PLV is expected to be high during periods of frequency matching and low when frequencies differ, with noise slightly reducing the overall values.\n- **Test Case D:** Two noisy sinusoids of the same frequency analyzed over a single window covering the entire signal. This tests the robustness of the measure to additive noise.\n\nFor reproducibility, a fixed seed is used for the pseudo-random number generator when creating the noisy signals. The algorithmic pipeline strictly follows the definitions provided, culminating in a series of PLV values for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import signal\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for phase synchrony analysis.\n    \"\"\"\n\n    # Use a fixed seed for the random number generator for reproducibility.\n    rng = np.random.default_rng(0)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case A (perfect phase locking)\n        {\n            \"N\": 2048, \"fs\": 512, \"L\": 1.0, \"S\": 0.5,\n            \"signal_params\": {\"type\": \"A\", \"f\": 10, \"delta\": np.pi / 4}\n        },\n        # Test Case B (frequency mismatch, low synchrony)\n        {\n            \"N\": 2048, \"fs\": 512, \"L\": 2.0, \"S\": 0.5,\n            \"signal_params\": {\"type\": \"B\", \"fx\": 10, \"fy\": 12}\n        },\n        # Test Case C (transient synchrony with noise)\n        {\n            \"N\": 4096, \"fs\": 512, \"L\": 1.0, \"S\": 0.25,\n            \"signal_params\": {\"type\": \"C\", \"f1\": 8, \"f2\": 8.5, \"noise_std\": 0.1}\n        },\n        # Test Case D (single full-length window with noise)\n        {\n            \"N\": 1024, \"fs\": 512, \"L\": 2.0, \"S\": 2.0,\n            \"signal_params\": {\"type\": \"D\", \"f\": 6, \"delta\": np.pi / 3, \"noise_std\": 0.2}\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        N = params[\"N\"]\n        fs = params[\"fs\"]\n        L = params[\"L\"]\n        S = params[\"S\"]\n        sp = params[\"signal_params\"]\n\n        # Generate signals\n        t = np.arange(N) / fs\n        if sp[\"type\"] == \"A\":\n            x = np.sin(2 * np.pi * sp[\"f\"] * t)\n            y = np.sin(2 * np.pi * sp[\"f\"] * t + sp[\"delta\"])\n        elif sp[\"type\"] == \"B\":\n            x = np.sin(2 * np.pi * sp[\"fx\"] * t)\n            y = np.sin(2 * np.pi * sp[\"fy\"] * t)\n        elif sp[\"type\"] == \"C\":\n            x = np.sin(2 * np.pi * sp[\"f1\"] * t) + rng.normal(0, sp[\"noise_std\"], N)\n            \n            y = np.zeros(N)\n            t1_end_idx = int(1.5 * fs)\n            t2_end_idx = int(5.5 * fs)\n            \n            y[:t1_end_idx] = np.sin(2 * np.pi * sp[\"f1\"] * t[:t1_end_idx])\n            y[t1_end_idx:t2_end_idx] = np.sin(2 * np.pi * sp[\"f2\"] * t[t1_end_idx:t2_end_idx])\n            y[t2_end_idx:] = np.sin(2 * np.pi * sp[\"f1\"] * t[t2_end_idx:] + np.pi / 6)\n            \n            y += rng.normal(0, sp[\"noise_std\"], N)\n        elif sp[\"type\"] == \"D\":\n            noise_x = rng.normal(0, sp[\"noise_std\"], N)\n            noise_y = rng.normal(0, sp[\"noise_std\"], N)\n            x = np.sin(2 * np.pi * sp[\"f\"] * t) + noise_x\n            y = np.sin(2 * np.pi * sp[\"f\"] * t + sp[\"delta\"]) + noise_y\n        \n        # Calculate analytic signals and instantaneous phases\n        z_x = signal.hilbert(x)\n        z_y = signal.hilbert(y)\n        phi_x = np.angle(z_x)\n        phi_y = np.angle(z_y)\n\n        # Calculate phase difference\n        delta_phi = phi_x - phi_y\n        \n        # Calculate window and step sizes in samples\n        L_s = int(np.round(L * fs))\n        S_s = int(np.round(S * fs))\n\n        # Perform sliding window PLV calculation\n        window_results = []\n        start_idx = 0\n        while start_idx + L_s = N:\n            delta_phi_window = delta_phi[start_idx : start_idx + L_s]\n            \n            # PLV = |mean(exp(i * delta_phi))|\n            plv = np.abs(np.mean(np.exp(1j * delta_phi_window)))\n            window_results.append(plv)\n            \n            start_idx += S_s\n            \n        all_results.append(window_results)\n\n    # Format output according to specifications\n    outer_list = []\n    for res_list in all_results:\n        inner_list = [f\"{x:.6f}\" for x in res_list]\n        outer_list.append(f\"[{','.join(inner_list)}]\")\n    final_string = f\"[{','.join(outer_list)}]\"\n    print(final_string)\n\nsolve()\n```"
        }
    ]
}