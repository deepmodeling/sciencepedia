{
    "hands_on_practices": [
        {
            "introduction": "尽管传递熵源于信息论，但它在预测建模领域有一个非常直观的解释。本练习将揭示，在一个常见的特定情况（高斯过程）下，传递熵与我们利用源信号信息后，对目标信号未来的预测能力提升程度直接相关 ()。这个练习旨在阐明传递熵中的“传递”即为可预测性的改善，从而将抽象概念具体化。",
            "id": "4201601",
            "problem": "您正在分析两个同步记录的神经时间序列，一个是在均匀时间步长 $t \\in \\mathbb{Z}$ 采样的单变量突触后膜电位代理 $Y_t$，另一个是单变量突触前局部活动代理 $X_t$。假设向量过程 $\\{(X_t, Y_t)\\}$ 是零均值、协方差平稳且联合高斯的。您通过两种方式使用一个阶数为 $p \\in \\mathbb{N}$ 的向量自回归 (VAR) 模型对 $Y_t$ 进行建模：\n- 一个受限预测器，仅使用 $Y_t$ 的 $p$ 阶滞后历史，\n- 一个完整预测器，使用 $Y_t$ 和 $X_t$ 的 $p$ 阶滞后历史。\n\n设最优线性预测器（在联合高斯假设下，其等同于条件期望）写为\n$$\nY_t = \\widehat{Y}_t^{(R)} + \\varepsilon_t^{(R)}, \\quad \\text{其中} \\quad \\widehat{Y}_t^{(R)} = \\mathbb{E}\\!\\left[ Y_t \\mid Y_{t-1}, \\dots, Y_{t-p} \\right],\n$$\n以及\n$$\nY_t = \\widehat{Y}_t^{(F)} + \\varepsilon_t^{(F)}, \\quad \\text{其中} \\quad \\widehat{Y}_t^{(F)} = \\mathbb{E}\\!\\left[ Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p} \\right].\n$$\n将残差方差记为 $\\sigma_{R}^{2} = \\mathbb{E}\\!\\left[(\\varepsilon_t^{(R)})^{2}\\right]$ 和 $\\sigma_{F}^{2} = \\mathbb{E}\\!\\left[(\\varepsilon_t^{(F)})^{2}\\right]$，由于平稳性，假定它们不随时间 $t$ 变化。\n\n从适用于此情景的基本原理出发，即：\n- 转移熵 (TE) 的定义，即从 $X_t$ 的 $p$ 阶滞后历史到 $Y_t$ 在给定 $Y_t$ 的 $p$ 阶滞后历史下的条件互信息，\n- 高斯变量的微分熵性质，\n- 以及在联合高斯假设下，最优线性预测误差方差与条件方差的等价性，\n\n推导从 $X$到 $Y$ 的转移熵的闭式解析表达式，该表达式以自然单位（奈特）表示，且仅用残差方差 $\\sigma_{R}^{2}$ 和 $\\sigma_{F}^{2}$ 表示。请给出您的最终表达式。您的最终答案必须是一个单一的闭式表达式。在最终的方框表达式中不要包含单位；将结果解释为以奈特为单位。",
            "solution": "该问题要求在联合高斯、平稳向量过程的特定条件下，为从时间序列 $X$到时间序列 $Y$ 的转移熵 (TE) 推导一个闭式解析表达式。推导过程必须从所提供的基本原理出发。\n\n记过程的过去历史为变量集 $Y_{\\text{past}} = \\{Y_{t-1}, \\dots, Y_{t-p}\\}$ 和 $X_{\\text{past}} = \\{X_{t-1}, \\dots, X_{t-p}\\}$。\n\n所提供的第一个基本原理是转移熵作为条件互信息的定义。具体来说，从 $X$ 到 $Y$ 的转移熵，记为 $TE_{X \\to Y}$，是 $Y$ 的当前值 $Y_t$ 与 $X$ 的过去历史 $X_{\\text{past}}$ 之间，在给定 $Y$ 的过去历史 $Y_{\\text{past}}$ 的条件下的互信息。用数学符号表示为：\n$$\nTE_{X \\to Y} = I(Y_t \\,;\\, X_{t-1}, \\dots, X_{t-p} \\mid Y_{t-1}, \\dots, Y_{t-p})\n$$\n对于连续变量，条件互信息用微分熵表示的标准定义是 $I(A; B \\mid C) = h(A \\mid C) - h(A \\mid B, C)$。将此定义应用于转移熵的表达式，我们得到：\n$$\nTE_{X \\to Y} = h(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}) - h(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p})\n$$\n该方程将转移熵表示为，在已经知晓 $Y$ 的过去历史 $Y_{\\text{past}}$ 的基础上，通过知晓 $X$ 的过去历史 $X_{\\text{past}}$ 而获得的关于 $Y_t$ 的不确定性（以其条件微分熵衡量）的减少量。\n\n要使用的第二个原理是高斯变量微分熵的公式。对于一个方差为 $\\sigma_Z^2$ 的单变量高斯随机变量 $Z$，其以自然单位（奈特）表示的微分熵 $h(Z)$ 由下式给出：\n$$\nh(Z) = \\frac{1}{2} \\ln(2\\pi e \\sigma_Z^2)\n$$\n由于向量过程 $\\{(X_t, Y_t)\\}$ 是联合高斯的，任何条件分布也都是高斯的。因此，给定自身历史的 $Y_t$ 的条件分布 $p(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p})$ 是高斯的。类似地，给定两个过程历史的 $Y_t$ 的条件分布 $p(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p})$ 也是高斯的。一个变量在其他变量条件下的微分熵是条件分布熵的期望值。对于联合高斯变量，条件分布的方差是恒定的，因此条件熵具有一种简单的形式。具体来说，$h(A \\mid C) = \\frac{1}{2} \\ln(2\\pi e \\cdot \\text{Var}(A \\mid C))$。\n\n将此性质应用于我们转移熵表达式中的两项：\n第一项是 $Y_t$ 在其自身历史条件下的熵，可以写为：\n$$\nh(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}) = \\frac{1}{2} \\ln\\left(2\\pi e \\cdot \\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p})\\right)\n$$\n第二项是 $Y_t$ 在 $X$ 和 $Y$ 共同历史条件下的熵：\n$$\nh(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p}) = \\frac{1}{2} \\ln\\left(2\\pi e \\cdot \\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p})\\right)\n$$\n将这些代入转移熵的方程：\n$$\nTE_{X \\to Y} = \\frac{1}{2} \\ln\\left(2\\pi e \\cdot \\text{Var}(Y_t \\mid Y_{\\text{past}})\\right) - \\frac{1}{2} \\ln\\left(2\\pi e \\cdot \\text{Var}(Y_t \\mid Y_{\\text{past}}, X_{\\text{past}})\\right)\n$$\n利用对数的性质 $\\ln(a) - \\ln(b) = \\ln(a/b)$，上式可简化为：\n$$\nTE_{X \\to Y} = \\frac{1}{2} \\ln\\left( \\frac{\\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p})}{\\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p})} \\right)\n$$\n\n所提供的第三个基本原理是，对于联合高斯过程，最优线性预测误差方差与条件方差是等价的。条件期望 $\\mathbb{E}[A \\mid C]$ 是在给定 $C$ 的情况下 $A$ 的最小均方误差估计量，对于高斯过程，它是 $C$ 的线性函数。估计误差的方差 $\\mathbb{E}[(A - \\mathbb{E}[A \\mid C])^2]$ 等于条件方差 $\\text{Var}(A \\mid C)$。\n\n给定两个对 $Y_t$ 的预测器：\n受限预测器为 $\\widehat{Y}_t^{(R)} = \\mathbb{E}[Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}]$。相应的残差是 $\\varepsilon_t^{(R)} = Y_t - \\widehat{Y}_t^{(R)}$，其方差为 $\\sigma_R^2 = \\mathbb{E}[(\\varepsilon_t^{(R)})^2]$。根据所述原理：\n$$\n\\sigma_R^2 = \\mathbb{E}\\left[ (Y_t - \\mathbb{E}[Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}])^2 \\right] = \\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p})\n$$\n完整预测器为 $\\widehat{Y}_t^{(F)} = \\mathbb{E}[Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p}]$。相应的残差是 $\\varepsilon_t^{(F)} = Y_t - \\widehat{Y}_t^{(F)}$，其方差为 $\\sigma_F^2 = \\mathbb{E}[(\\varepsilon_t^{(F)})^2]$。类似地：\n$$\n\\sigma_F^2 = \\mathbb{E}\\left[ (Y_t - \\mathbb{E}[Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p}])^2 \\right] = \\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p})\n$$\n因此，转移熵表达式中的条件方差正是两个预测模型的残差方差。将 $\\sigma_R^2$ 和 $\\sigma_F^2$ 代入我们的转移熵表达式，我们得到最终的闭式结果：\n$$\nTE_{X \\to Y} = \\frac{1}{2} \\ln\\left( \\frac{\\sigma_R^2}{\\sigma_F^2} \\right)\n$$\n这个表达式通过当 $X_t$ 的历史被包含在预测器中时 $Y_t$ 可预测性的提升，来量化从 $X$到 $Y$ 的有向影响。如果 $X$ 的历史没有为预测 $Y$ 提供额外信息，那么 $\\sigma_F^2 = \\sigma_R^2$ 且 $TE_{X \\to Y} = \\frac{1}{2} \\ln(1) = 0$。否则，由于包含更多信息不会使最优预测变得更差，我们必然有 $\\sigma_F^2 \\le \\sigma_R^2$，这确保了 $TE_{X \\to Y} \\ge 0$，这是作为信息度量的要求。因为使用了自然对数，结果的单位是奈特。",
            "answer": "$$\n\\boxed{\\frac{1}{2} \\ln\\left(\\frac{\\sigma_R^2}{\\sigma_F^2}\\right)}\n$$"
        },
        {
            "introduction": "在实际应用传递熵时，一个关键的步骤是选择要在模型中包含多少历史信息，即确定历史长度 $k$ 和 $l$。这个决策需要在捕捉所有相关影响（较长历史）和避免模型过拟合（较短历史）之间做出权衡。本练习探讨如何系统地运用赤池信息准则 (AIC)、贝叶斯信息准则 (BIC) 和交叉验证等统计工具来做出这一重要选择 ()。",
            "id": "4201564",
            "problem": "在分析源过程 $X_t$ 对目标过程 $Y_t$ 的定向影响的脉冲序列分析中，转移熵（TE）通过插件式方法进行估计，该方法需要拟合在给定 $Y$ 和 $X$ 的过去延迟的情况下 $Y_t$ 的条件分布。模型类别是用于条件分布 $p(y_t \\mid Y_{t-1}^{(l)}, X_{t-1}^{(k)})$ 的广义线性模型（GLM; Generalized Linear Model），具有典范链接函数、长度为 $k$ 和 $l$ 的线性历史滤波器以及一个截距。每个候选对 $(k,l)$ 定义一个参数维度为 $d(k,l) = k + l + 1$ 的参数模型。为选择 $(k,l)$，您将使用赤池信息准则 (AIC; Akaike Information Criterion)、贝叶斯信息准则 (BIC; Bayesian Information Criterion) 和考虑时间依赖性的分块预测交叉验证 (CV; Cross-Validation) 来比较模型。\n\n给定长度为 $n = 20000$ 个时间步的分箱数据（时间窗宽度 $w = 5\\,\\mathrm{ms}$）。对于五个候选历史长度对 $(k,l)$，训练负二倍对数似然（记为 $R(k,l) \\equiv -2 \\log L(k,l)$）和10折分块交叉验证的负二倍对数损失（记为 $R_{\\mathrm{cv}}(k,l)$）如下：\n\n- $(1,1)$: $d = 3$, $R = 30250$, $R_{\\mathrm{cv}} = 30300$。\n- $(5,5)$: $d = 11$, $R = 29500$, $R_{\\mathrm{cv}} = 29580$。\n- $(10,10)$: $d = 21$, $R = 29420$, $R_{\\mathrm{cv}} = 29510$。\n- $(10,5)$: $d = 16$, $R = 29470$, $R_{\\mathrm{cv}} = 29450$。\n- $(5,10)$: $d = 16$, $R = 29460$, $R_{\\mathrm{cv}} = 29455$。\n\n回顾定义：\n- AIC 选择使 $R(k,l) + 2 d(k,l)$ 最小化的模型，这是在宽泛的正则性条件下，对样本外期望 Kullback–Leibler (KL; Kullback–Leibler) 散度的无偏估计。\n- BIC 选择使 $R(k,l) + d(k,l) \\ln n$ 最小化的模型，这是在正确指定的有限维参数族下，对数后验模型概率的-2倍的近似，当 $n \\to \\infty$ 时，能得到真实模型阶数的一致性。\n- 分块预测 CV 选择使 $R_{\\mathrm{cv}}(k,l)$ 最小化的模型，直接估计 $Y_t$ 条件分布的样本外预测风险。\n\n在此 TE 估计背景下，关于选择 $(k,l)$ 的下列陈述中，哪些是正确的？\n\nA. 在 AIC 准则下，选择 $(10,10)$；在 BIC 准则下，选择 $(5,5)$；在分块预测 CV 下，选择 $(10,5)$。因为 TE 是从 $Y_t$ 的估计条件分布计算得出的，如果优先考虑最小化该条件分布的样本外预测误差，那么最具说服力的主要选择是 CV 选择的 $(10,5)$，并辅以对 AIC 和 BIC 选择的模型对进行敏感性分析。\n\nB. 总是选择最长的历史，这里是 $(10,10)$，因为当 $n$ 很大时，增加 $k$ 和 $l$ 不会增加方差；TE 在模型大小上是单调的，所以惩罚是不必要的。\n\nC. 由于 $n = 20000$，BIC 惩罚项 $d \\ln n$ 可以忽略不计；BIC 也会选择 $(10,10)$，因此 AIC、BIC 和 CV 都一致选择 $(10,10)$。\n\nD. 对于 TE 估计，偏好选择 $(5,5)$，因为 BIC 最小化期望预测误差，而 CV 在时间序列中会系统性地高估模型复杂度；因此 $(5,5)$ 是最佳的预测选择。\n\nE. 如果 GLM 族包含具有有限记忆的真实数据生成过程，则 BIC 是真实阶数的一致性准则，此处选择 $(5,5)$。在模型设定错误的情况下，或者当为了 TE 的稳定性而优先考虑 $p(y_t \\mid Y_{t-1}^{(l)}, X_{t-1}^{(k)})$ 的预测准确性时，AIC 和分块预测 CV 更为可取；在数值上，AIC 选择 $(10,10)$，而 CV 选择 $(10,5)$。",
            "solution": "在进行求解之前，首先评估问题陈述的有效性。\n\n### 第1步：提取已知信息\n- 源过程：$X_t$\n- 目标过程：$Y_t$\n- 待估计量：转移熵 (TE)\n- 估计方法：通过拟合条件分布 $p(y_t \\mid Y_{t-1}^{(l)}, X_{t-1}^{(k)})$ 的插件式方法。\n- 模型类别：具有典范链接函数的广义线性模型 (GLM)。\n- 历史长度：过程 $X$ 为 $k$，过程 $Y$ 为 $l$。\n- 参数维度：$d(k,l) = k + l + 1$。\n- 模型选择准则：赤池信息准则 (AIC)，贝叶斯信息准则 (BIC)，分块预测交叉验证 (CV)。\n- 数据长度：$n = 20000$ 个时间步。\n- 时间窗宽度：$w = 5\\,\\mathrm{ms}$。\n- 候选模型及相关数据：\n  - $(k,l) = (1,1)$：$d = 3$，训练负二倍对数似然 $R = 30250$，10折分块交叉验证的负二倍对数损失 $R_{\\mathrm{cv}} = 30300$。\n  - $(k,l) = (5,5)$：$d = 11$， $R = 29500$，$R_{\\mathrm{cv}} = 29580$。\n  - $(k,l) = (10,10)$：$d = 21$，$R = 29420$，$R_{\\mathrm{cv}} = 29510$。\n  - $(k,l) = (10,5)$：$d = 16$，$R = 29470$，$R_{\\mathrm{cv}} = 29450$。\n  - $(k,l) = (5,10)$：$d = 16$，$R = 29460$，$R_{\\mathrm{cv}} = 29455$。\n- 准则定义：\n  - AIC: 最小化 $R(k,l) + 2 d(k,l)$。\n  - BIC: 最小化 $R(k,l) + d(k,l) \\ln n$。\n  - 分块 CV: 最小化 $R_{\\mathrm{cv}}(k,l)$。\n\n### 第2步：使用提取的已知信息进行验证\n问题陈述在科学上是合理的、提法明确且客观。它描述了在从神经脉冲序列数据中估计定向功能连接（转移熵）的背景下，一种标准而严谨的模型选择方法论。GLM、信息准则（AIC, BIC）和分块交叉验证的使用都是计算神经科学和时间序列分析中的成熟技术。所提供的数据对于此类分析是内部一致且合理的。各项准则的定义是正确的。该问题没有验证清单中列出的任何缺陷。\n\n### 第3步：结论与行动\n问题有效。将推导出解决方案。\n\n### 推导与模型选择\n任务是根据 AIC、BIC 和分块 CV 这三个不同的准则来选择最佳模型。我们必须首先计算每个候选模型 $(k,l)$ 的 AIC 和 BIC 分数。CV 分数直接由 $R_{\\mathrm{cv}}(k,l)$ 给出。\n\n样本量为 $n = 20000$。用于 BIC 计算的自然对数项为 $\\ln(n) = \\ln(20000) \\approx 9.9035$。\n\n每个模型的分数计算如下：\n\n1.  **模型 $(k,l) = (1,1)$**：\n    - $d = 3$, $R = 30250$, $R_{\\mathrm{cv}} = 30300$。\n    - $\\text{AIC} = R + 2d = 30250 + 2(3) = 30256$。\n    - $\\text{BIC} = R + d \\ln n = 30250 + 3 \\times 9.9035 = 30250 + 29.7105 = 30279.7105$。\n    - $\\text{CV 分数} = 30300$。\n\n2.  **模型 $(k,l) = (5,5)$**：\n    - $d = 11$, $R = 29500$, $R_{\\mathrm{cv}} = 29580$。\n    - $\\text{AIC} = R + 2d = 29500 + 2(11) = 29522$。\n    - $\\text{BIC} = R + d \\ln n = 29500 + 11 \\times 9.9035 = 29500 + 108.9385 = 29608.9385$。\n    - $\\text{CV 分数} = 29580$。\n\n3.  **模型 $(k,l) = (10,10)$**：\n    - $d = 21$, $R = 29420$, $R_{\\mathrm{cv}} = 29510$。\n    - $\\text{AIC} = R + 2d = 29420 + 2(21) = 29462$。\n    - $\\text{BIC} = R + d \\ln n = 29420 + 21 \\times 9.9035 = 29420 + 207.9735 = 29627.9735$。\n    - $\\text{CV 分数} = 29510$。\n\n4.  **模型 $(k,l) = (10,5)$**：\n    - $d = 16$, $R = 29470$, $R_{\\mathrm{cv}} = 29450$。\n    - $\\text{AIC} = R + 2d = 29470 + 2(16) = 29502$。\n    - $\\text{BIC} = R + d \\ln n = 29470 + 16 \\times 9.9035 = 29470 + 158.456 = 29628.456$。\n    - $\\text{CV 分数} = 29450$。\n\n5.  **模型 $(k,l) = (5,10)$**：\n    - $d = 16$, $R = 29460$, $R_{\\mathrm{cv}} = 29455$。\n    - $\\text{AIC} = R + 2d = 29460 + 2(16) = 29492$。\n    - $\\text{BIC} = R + d \\ln n = 29460 + 16 \\times 9.9035 = 29460 + 158.456 = 29618.456$。\n    - $\\text{CV 分数} = 29455$。\n\n**选择总结：**\n-   **AIC**：最小的 AIC 分数是 $29462$，对应于模型 $(k,l) = (10,10)$。\n-   **BIC**：最小的 BIC 分数是 $29608.9385$，对应于模型 $(k,l) = (5,5)$。\n-   **分块 CV**：最小的 CV 分数是 $29450$，为模型 $(k,l) = (10,5)$ 给出。\n\n### 逐项分析\n\n**A. 在 AIC 准则下，选择 $(10,10)$；在 BIC 准则下，选择 $(5,5)$；在分块预测 CV 下，选择 $(10,5)$。因为 TE 是从 $Y_t$ 的估计条件分布计算得出的，如果优先考虑最小化该条件分布的样本外预测误差，那么最具说服力的主要选择是 CV 选择的 $(10,5)$，并辅以对 AIC 和 BIC 选择的模型对进行敏感性分析。**\n- 第一句话根据上面的计算正确地指出了每个准则选择的模型。\n- 第二句话的推理是合理的。通过插件式方法估计 TE 对估计的条件分布 $p(y_t \\mid Y_{t-1}^{(l)}, X_{t-1}^{(k)})$ 的准确性很敏感。分块 CV 直接估计该分布的样本外预测性能（对数损失），这正是我们为了获得稳定和准确的 TE 估计所希望最小化的量。因此，优先考虑 CV 的结果是合乎逻辑的。建议使用其他准则选择的模型进行敏感性分析是评估稳健性的优秀科学实践。\n- **结论：正确**\n\n**B. 总是选择最长的历史，这里是 $(10,10)$，因为当 $n$ 很大时，增加 $k$ 和 $l$ 不会增加方差；TE 在模型大小上是单调的，所以惩罚是不必要的。**\n- 这种说法鼓励过拟合。选择最大的模型忽略了偏差-方差权衡。\n- “当 $n$ 很大时，增加 $k$ 和 $l$ 不会增加方差”的说法是错误的。对于任何有限的 $n$，增加模型复杂度（更多参数）会增加参数估计的方差。\n- “TE 在模型大小上是单调的”这一说法具有误导性。TE 的*样本内*估计可能是单调的，但这是过拟合的症状，并不能反映真实的底层 TE。\n- “惩罚是不必要的”的结论是根本错误的，与所有公认的统计模型选择原则相悖。\n- **结论：不正确**\n\n**C. 由于 $n = 20000$，BIC 惩罚项 $d \\ln n$ 可以忽略不计；BIC 也会选择 $(10,10)$，因此 AIC、BIC 和 CV 都一致选择 $(10,10)$。**\n- BIC 惩罚项可以忽略不计的前提是错误的。对于 $(10,10)$ 模型，惩罚项为 $d \\ln n = 21 \\times \\ln(20000) \\approx 207.97$。$(5,5)$ 模型和 $(10,10)$ 模型的对数似然项 $R$ 的差异为 $R(5,5) - R(10,10) = 29500 - 29420 = 80$。惩罚项的差异为 $(21-11)\\ln n = 10 \\ln n \\approx 99.035$。对于 $(10,10)$ 模型，更大的惩罚超过了其更好的拟合度，这就是为什么 BIC 会选择更小的 $(5,5)$ 模型。惩罚项不仅不可忽略，而且是决定性的。\n- 如我们的计算所示，“BIC 也会选择 $(10,10)$”的结论在事实上是不正确的。\n- 所有准则都达成一致的最终断言在事实上也是不正确的。\n- **结论：不正确**\n\n**D. 对于 TE 估计，偏好选择 $(5,5)$，因为 BIC 最小化期望预测误差，而 CV 在时间序列中会系统性地高估模型复杂度；因此 $(5,5)$ 是最佳的预测选择。**\n- “BIC 最小化期望预测误差”的前提是不正确的。AIC（和 CV）旨在选择具有良好预测性能的模型（通过以最小化到真实分布的 KL 散度为目标）。BIC 旨在成为真实模型阶数的一致性选择器，前提是真实模型在候选集中。从预测的角度来看，由于其更强的惩罚项，它通常相对于 AIC/CV 倾向于欠拟合。\n- “CV 在时间序列中会系统性地高估模型复杂度”的说法是一个强烈的、通常没有依据的概括。虽然标准的 k 折交叉验证可能存在问题，但分块 CV 是专门设计用来减轻由时间依赖性引起的问题，并提供更好的预测误差估计。\n- “$(5,5)$ 是最佳预测选择”的结论与最直接衡量预测性能的准则——CV 相矛盾，后者选择了 $(10,5)$。\n- **结论：不正确**\n\n**E. 如果 GLM 族包含具有有限记忆的真实数据生成过程，则 BIC 是真实阶数的一致性准则，此处选择 $(5,5)$。在模型设定错误的情况下，或者当为了 TE 的稳定性而优先考虑 $p(y_t \\mid Y_{t-1}^{(l)}, X_{t-1}^{(k)})$ 的预测准确性时，AIC 和分块预测 CV 更为可取；在数值上，AIC 选择 $(10,10)$，而 CV 选择 $(10,5)$。**\n- 这个陈述正确地阐述了 BIC 和 AIC/CV 之间的核心理论区别。BIC 的优势在于模型选择的一致性，前提是真实的、有限阶的模型存在于候选模型集合中。\n- 它正确地指出了在何种情况下 AIC 和 CV 更可取：当目标是预测准确性时，尤其是在可能出现模型设定错误（即没有候选模型是完全“真实”的）的情况下。\n- 根据我们的计算，所有的事实性断言都是正确的：BIC 选择 $(5,5)$，AIC 选择 $(10,10)$，CV 选择 $(10,5)$。所呈现的推理是现代统计模型选择理论的标准、正确的总结。\n- **结论：正确**\n\n陈述 A 和 E 都是正确的。陈述 E 提供了正确的理论背景和数值结果。陈述 A 利用这些结果为分析提出了一个正确且合理的实践建议。两者都是关于该情况的有效陈述。",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "从真实数据中计算传递熵时，我们可能会遇到一个令人困惑的结果：得到一个负值，尽管信息论理论规定传递熵必须为非负。本练习旨在解决这个常见问题，阐明负的估计值是有限样本造成的统计偏差，并不意味着存在“负信息流” ()。通过本练习，您将学会如何正确解读这些结果并评估其统计显著性，这是严谨科学分析的关键一步。",
            "id": "4201624",
            "problem": "您正在分析两个同时记录的神经过程之间的定向相互作用：一个来自假定上游区域的源脉冲计数时间序列 $X_t$ 和一个来自下游区域的目标局部场电位 (LFP) 振幅时间序列 $Y_t$。您使用长度分别为 $k$ 和 $\\ell$ 的延迟嵌入过去，对源和目标进行计算，样本数量为 $N$，从而计算从 $X_t$ 到 $Y_t$ 的转移熵 (TE)。在实践中，您使用最近邻估计器，在 $N=5000$ 个样本和时间窗宽度 $\\Delta t=5$ ms 的条件下，得到了一个小的负估计值，例如 $-0.02$ 比特。\n\n请仅使用以下基本原理来推断 TE 的理论符号以及解释负的经验估计值：\n- Kullback–Leibler散度 (Kullback–Leibler (KL))：对于在相同支撑集上的分布 $P$ 和 $Q$，散度 $D_{\\mathrm{KL}}(P \\parallel Q) = \\sum p \\log \\frac{p}{q}$（或类似积分）满足 $D_{\\mathrm{KL}}(P \\parallel Q) \\ge 0$，等号成立当且仅当 $P=Q$ 几乎必然成立。\n- 条件互信息 (Conditional Mutual Information (CMI)) $I(X;Y \\mid Z)$ 可以表示为条件概率 $p(y \\mid x,z)$ 与边缘条件概率 $p(y \\mid z)$ 之间的期望Kullback–Leibler散度，即 $I(X;Y \\mid Z) = \\mathbb{E}_{x,z}\\!\\left[ D_{\\mathrm{KL}}\\!\\big(p(\\cdot \\mid x,z)\\parallel p(\\cdot \\mid z)\\big)\\right]$，因此 $I(X;Y \\mid Z) \\ge 0$。\n- 在标准神经科学实践中，从源的过去到目标的未来，以目标的过去为条件的TE，被定义为与一个合适的CMI重合，该CMI是源的过去和目标的未来以目标的过去为条件。\n\n考虑以上情景和负的经验估计值。以下哪些陈述共同为这种负的TE估计值提供了正确、有原则的解释和处理方法，从而调和了实践与理论？\n\nA. 真实的TE是非负的，因为它是一种条件互信息，可以表示为期望Kullback–Leibler散度；小的负经验估计值源于有限样本偏差和方差，不应被解释为负向定向信息流的证据。在实践中，可以使用基于置换的零分布来评估显著性，并使用偏差校正方法来减轻有限样本效应。\n\nB. 负的TE估计值表明从源到目标存在抑制性因果影响；估计值越负，抑制作用越强，这与估计器属性或样本大小无关。\n\nC. 出现负的TE估计值证明该估计器是不一致的；一个一致的估计器在任何有限样本大小下都不能产生负值。\n\nD. 即使使用一致的估计器，有限样本波动以及熵和密度估计中的偏差也可能产生负的TE估计值；可以通过将估计值在零处截断或减去一个基于置换的偏差估计来强制非负性，但截断会引入正向偏差，并应予以透明地报告。\n\nE. 在嵌入指定错误（例如，不正确的延迟或历史长度）的情况下，真实的TE可以合理地为负，因为对错误的变量进行条件化可能会从目标的未来中移除过多的信息。\n\n选择所有适用项。",
            "solution": "我们从基本原理出发。根据所提供的基础，条件互信息 (CMI) $I(X;Y \\mid Z)$ 可以表示为\n$$\nI(X;Y \\mid Z) \\;=\\; \\mathbb{E}_{x,z}\\!\\left[ D_{\\mathrm{KL}}\\!\\big(p(\\cdot \\mid x,z)\\parallel p(\\cdot \\mid z)\\big)\\right],\n$$\n根据Gibbs不等式，Kullback–Leibler散度具有非负性，因此 $D_{\\mathrm{KL}}(P\\parallel Q) \\ge 0$。所以，\n$$\nI(X;Y \\mid Z) \\;\\ge\\; 0,\n$$\n等号成立的条件是，当且仅当在 $p(x,z)$ 下几乎必然有 $p(y \\mid x,z) = p(y \\mid z)$，即源除了条件变量之外，没有为目标未来提供额外的预测信息。\n\n在标准神经科学实践中，从源的过去到目标的未来，以目标的过去为条件的转移熵 (TE)，被定义为与这样一个CMI重合。因此，真实的TE是非负的：\n$$\n\\mathrm{TE} \\;=\\; I\\!\\left(\\text{源的过去};\\ \\text{目标的未来} \\mid \\text{目标的过去}\\right) \\;\\ge\\; 0.\n$$\n任何理论上的负值都将与 $I(\\cdot;\\cdot \\mid \\cdot)$ 的非负性相矛盾。\n\n然而，在有限样本中，TE的估计器依赖于有偏的熵、密度或概率比估计。例如，熵的插件式直方图估计器具有 $O\\!\\left(\\frac{1}{N}\\right)$ 阶的负向偏差，而最近邻方法则表现出依赖于邻域大小和维度的有限样本偏差和方差。因为TE是熵的差值，或是由估计量构成的对数似然比的平均值，所以对于给定的数据集，净估计误差可能为负，即使真实TE为 $0$ 或正数，也可能产生像 $-0.02$ 比特这样的估计值。估计器的一致性只保证当 $N \\to \\infty$ 时收敛到真实的非负值，而不能保证有限样本的实现会保持非负性。\n\n在实践中，为了在尊重理论约束的同时解释这种负估计值，可以：\n- 使用置换或时间平移代理数据来构建一个零分布，该分布反映了在无定向影响下的有限样本偏差和依赖结构，然后评估观察到的估计值是否与零分布有显著差异。通常，负值落在零分布范围内，表明没有定向影响的证据。\n- 应用偏差校正（例如，用于离散插件式的Miller–Madow校正，或减去代理数据均值作为估计的有限样本偏差），以使估计值向真实的非负值移动。\n- 在报告效应量时，可选择在零处截断，但需要注意截断会引入正向偏差，并应予以公开说明；统计显著性应基于零分布而不是截断。\n\n我们现在评估这些选项：\n\nA. 该陈述指出，真实的TE是非负的，因为它是一种可以表示为期望Kullback–Leibler散度的CMI，并将负的经验估计值归因于有限样本偏差和方差。它建议使用基于置换的零分布和偏差校正。这与推导和标准实践完全一致。结论 — 正确。\n\nB. 该陈述声称负的TE估计值表示抑制性因果影响，且其大小反映了抑制强度。这将估计器的伪影误解为生理属性，并忽略了TE的理论非负性。TE中没有“负信息流”的概念；抑制性影响可以降低TE，但不能使真实的TE为负。结论 — 错误。\n\nC. 该陈述断言，任何负估计值都意味着估计器不一致，并且一致的估计器在有限样本大小下不能产生负值。一致性涉及 $N \\to \\infty$ 时的渐近行为，并不排除由于偏差/方差导致的有限样本对非负性等约束的违反。因此，这是错误的。结论 — 错误。\n\nD. 该陈述认识到，即使是一致的估计器也可能产生负的有限样本估计值，并建议通过截断或减去基于置换的偏差估计来强制非负性，同时警告截断会引入正向偏差。这符合有原则的实践，并承认了其中的权衡。结论 — 正确。\n\nE. 该陈述声称，嵌入的错误指定可以使真实的TE为负。无论嵌入如何，所定义的TE仍然是良定义的随机变量之间的CMI，因此是非负的。错误指定可以减少真实的TE或使其为零，但不能使其为负。结论 — 错误。\n\n因此，正确的选项是 A 和 D。",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}