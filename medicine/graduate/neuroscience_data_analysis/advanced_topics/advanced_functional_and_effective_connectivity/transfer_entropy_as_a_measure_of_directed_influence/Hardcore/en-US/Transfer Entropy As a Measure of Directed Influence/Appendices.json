{
    "hands_on_practices": [
        {
            "introduction": "Transfer Entropy (TE) is a powerful, model-free concept. However, its connection to more traditional methods like Granger causality becomes clear in the special case of linear-Gaussian systems. This exercise bridges this conceptual gap by demonstrating how, under these assumptions, TE elegantly reduces to a logarithmic ratio of prediction error variances from autoregressive models. Mastering this derivation provides a crucial intuition: TE quantifies the fractional improvement in predictability gained from including the source's history .",
            "id": "4201601",
            "problem": "You are analyzing two simultaneously recorded neural time series, a univariate postsynaptic membrane potential proxy $Y_t$ and a univariate presynaptic local activity proxy $X_t$, sampled at uniform time steps $t \\in \\mathbb{Z}$. Assume the vector process $\\{(X_t, Y_t)\\}$ is zero-mean, covariance-stationary, and jointly Gaussian. You model $Y_t$ using a Vector Autoregression (VAR) of order $p \\in \\mathbb{N}$ in two ways:\n- A restricted predictor that uses only the $p$-lag history of $Y_t$,\n- A full predictor that uses the $p$-lag histories of both $Y_t$ and $X_t$.\n\nLet the optimal linear predictors (which coincide with the conditional expectations under joint Gaussianity) be written as\n$$\nY_t = \\widehat{Y}_t^{(R)} + \\varepsilon_t^{(R)}, \\quad \\text{with} \\quad \\widehat{Y}_t^{(R)} = \\mathbb{E}\\!\\left[ Y_t \\mid Y_{t-1}, \\dots, Y_{t-p} \\right],\n$$\nand\n$$\nY_t = \\widehat{Y}_t^{(F)} + \\varepsilon_t^{(F)}, \\quad \\text{with} \\quad \\widehat{Y}_t^{(F)} = \\mathbb{E}\\!\\left[ Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p} \\right].\n$$\nDenote the residual variances by $\\sigma_{R}^{2} = \\mathbb{E}\\!\\left[(\\varepsilon_t^{(R)})^{2}\\right]$ and $\\sigma_{F}^{2} = \\mathbb{E}\\!\\left[(\\varepsilon_t^{(F)})^{2}\\right]$, assumed constant over $t$ due to stationarity.\n\nStarting from first principles appropriate to this setting, namely:\n- the definition of Transfer Entropy (TE) as conditional mutual information from the $p$-lag history of $X_t$ to $Y_t$ given the $p$-lag history of $Y_t$,\n- the properties of differential entropy for Gaussian variables,\n- and the equivalence between optimal linear prediction error variance and conditional variance under joint Gaussianity,\n\nderive a closed-form analytic expression for the Transfer Entropy from $X$ to $Y$, expressed in natural units (nats), in terms of the residual variances $\\sigma_{R}^{2}$ and $\\sigma_{F}^{2}$ only. Provide your final expression. Your final answer must be a single closed-form expression. Do not include units in your final boxed expression; interpret the result as being in nats.",
            "solution": "The problem asks for a closed-form analytic expression for the Transfer Entropy (TE) from a time series $X$ to a time series $Y$, under the specific conditions of a jointly Gaussian, stationary vector process. The derivation must proceed from the first principles provided.\n\nLet the past histories of the processes be denoted by the sets of variables $Y_{\\text{past}} = \\{Y_{t-1}, \\dots, Y_{t-p}\\}$ and $X_{\\text{past}} = \\{X_{t-1}, \\dots, X_{t-p}\\}$.\n\nThe first principle provided is the definition of Transfer Entropy as a conditional mutual information. Specifically, the TE from $X$ to $Y$, denoted $TE_{X \\to Y}$, is the mutual information between the present value of $Y$, which is $Y_t$, and the past of $X$, which is $X_{\\text{past}}$, conditioned on the past of $Y$, which is $Y_{\\text{past}}$. In mathematical notation, this is:\n$$\nTE_{X \\to Y} = I(Y_t \\,;\\, X_{t-1}, \\dots, X_{t-p} \\mid Y_{t-1}, \\dots, Y_{t-p})\n$$\nThe standard definition of conditional mutual information in terms of differential entropies for continuous variables is $I(A; B \\mid C) = h(A \\mid C) - h(A \\mid B, C)$. Applying this definition to the expression for TE, we get:\n$$\nTE_{X \\to Y} = h(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}) - h(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p})\n$$\nThis equation expresses the TE as the reduction in the uncertainty about $Y_t$ (as measured by its conditional differential entropy) gained from knowing the past of $X$, $X_{\\text{past}}$, in addition to already knowing the past of $Y$, $Y_{\\text{past}}$.\n\nThe second principle to be used is the formula for the differential entropy of a Gaussian variable. For a univariate Gaussian random variable $Z$ with variance $\\sigma_Z^2$, its differential entropy $h(Z)$ in natural units (nats) is given by:\n$$\nh(Z) = \\frac{1}{2} \\ln(2\\pi e \\sigma_Z^2)\n$$\nSince the vector process $\\{(X_t, Y_t)\\}$ is jointly Gaussian, any conditional distribution is also Gaussian. Therefore, the conditional distribution of $Y_t$ given its own past, $p(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p})$, is Gaussian. Similarly, the conditional distribution of $Y_t$ given the past of both processes, $p(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p})$, is also Gaussian. The differential entropy of a variable conditioned on other variables is the expected value of the entropy of the conditional distribution. For jointly Gaussian variables, the variance of the conditional distribution is constant, so the conditional entropy takes a simple form. Specifically, $h(A \\mid C) = \\frac{1}{2} \\ln(2\\pi e \\cdot \\text{Var}(A \\mid C))$.\n\nApplying this property to the two terms in our expression for TE:\nThe first term is the entropy of $Y_t$ conditioned on its own past, which can be written as:\n$$\nh(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}) = \\frac{1}{2} \\ln\\left(2\\pi e \\cdot \\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p})\\right)\n$$\nThe second term is the entropy of $Y_t$ conditioned on the past of both $X$ and $Y$:\n$$\nh(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p}) = \\frac{1}{2} \\ln\\left(2\\pi e \\cdot \\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p})\\right)\n$$\nSubstituting these into the equation for TE:\n$$\nTE_{X \\to Y} = \\frac{1}{2} \\ln\\left(2\\pi e \\cdot \\text{Var}(Y_t \\mid Y_{\\text{past}})\\right) - \\frac{1}{2} \\ln\\left(2\\pi e \\cdot \\text{Var}(Y_t \\mid Y_{\\text{past}}, X_{\\text{past}})\\right)\n$$\nUsing the property of logarithms, $\\ln(a) - \\ln(b) = \\ln(a/b)$, this simplifies to:\n$$\nTE_{X \\to Y} = \\frac{1}{2} \\ln\\left( \\frac{\\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p})}{\\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p})} \\right)\n$$\n\nThe third principle provided is the equivalence between the optimal linear prediction error variance and the conditional variance for jointly Gaussian processes. The conditional expectation $\\mathbb{E}[A \\mid C]$ is the minimum mean square error estimator of $A$ given $C$, and for Gaussian processes, it is a linear function of $C$. The variance of the estimation error, $\\mathbb{E}[(A - \\mathbb{E}[A \\mid C])^2]$, is equal to the conditional variance, $\\text{Var}(A \\mid C)$.\n\nWe are given two predictors for $Y_t$:\nThe restricted predictor is $\\widehat{Y}_t^{(R)} = \\mathbb{E}[Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}]$. The corresponding residual is $\\varepsilon_t^{(R)} = Y_t - \\widehat{Y}_t^{(R)}$, and its variance is $\\sigma_R^2 = \\mathbb{E}[(\\varepsilon_t^{(R)})^2]$. According to the stated principle:\n$$\n\\sigma_R^2 = \\mathbb{E}\\left[ (Y_t - \\mathbb{E}[Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}])^2 \\right] = \\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p})\n$$\nThe full predictor is $\\widehat{Y}_t^{(F)} = \\mathbb{E}[Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p}]$. The corresponding residual is $\\varepsilon_t^{(F)} = Y_t - \\widehat{Y}_t^{(F)}$, and its variance is $\\sigma_F^2 = \\mathbb{E}[(\\varepsilon_t^{(F)})^2]$. Similarly:\n$$\n\\sigma_F^2 = \\mathbb{E}\\left[ (Y_t - \\mathbb{E}[Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p}])^2 \\right] = \\text{Var}(Y_t \\mid Y_{t-1}, \\dots, Y_{t-p}, X_{t-1}, \\dots, X_{t-p})\n$$\nThus, the conditional variances in the TE expression are precisely the residual variances from the two prediction models. Substituting $\\sigma_R^2$ and $\\sigma_F^2$ into our expression for TE, we obtain the final closed-form result:\n$$\nTE_{X \\to Y} = \\frac{1}{2} \\ln\\left( \\frac{\\sigma_R^2}{\\sigma_F^2} \\right)\n$$\nThis expression quantifies the directed influence from $X$ to $Y$ in terms of the improvement in predictability of $Y_t$ when the history of $X_t$ is included in the predictor. If the history of $X$ provides no additional information for predicting $Y$, then $\\sigma_F^2 = \\sigma_R^2$ and $TE_{X \\to Y} = \\frac{1}{2} \\ln(1) = 0$. Otherwise, since including more information cannot worsen the optimal prediction, we must have $\\sigma_F^2 \\le \\sigma_R^2$, which ensures that $TE_{X \\to Y} \\ge 0$, as required for a measure of information. The result is in nats due to the use of the natural logarithm.",
            "answer": "$$\n\\boxed{\\frac{1}{2} \\ln\\left(\\frac{\\sigma_R^2}{\\sigma_F^2}\\right)}\n$$"
        },
        {
            "introduction": "A frequent point of confusion when first applying TE estimators to real data is the appearance of small, negative values, which seems to contradict the theoretical non-negativity of information measures. This exercise directly confronts this practical issue, clarifying that such results are artifacts of finite-sample bias and not evidence of \"negative information flow.\" By reasoning from the first principles of information theory, you will learn to correctly interpret these common estimation artifacts and understand the importance of statistical testing to distinguish true information transfer from estimator noise .",
            "id": "4201624",
            "problem": "You are analyzing directed interactions between two simultaneously recorded neural processes: a source spike-count time series $X_t$ from a putative upstream region and a target local field potential (LFP) amplitude time series $Y_t$ from a downstream region. You compute Transfer Entropy (TE) from $X_t$ to $Y_t$ using delay-embedded pasts of lengths $k$ and $\\ell$ for the source and target, respectively, with $N$ samples. In practice, you obtain a small negative estimate, for example $-0.02$ bits, using a nearest-neighbor estimator on $N=5000$ samples and bin width $\\Delta t=5$ ms.\n\nUse only the following fundamental base to reason about the theoretical sign of TE and the interpretation of negative empirical estimates:\n- Kullback–Leibler (KL) divergence: for distributions $P$ and $Q$ on the same support, the divergence $D_{\\mathrm{KL}}(P \\parallel Q) = \\sum p \\log \\frac{p}{q}$ (or the analogous integral) satisfies $D_{\\mathrm{KL}}(P \\parallel Q) \\ge 0$ with equality if and only if $P=Q$ almost surely.\n- Conditional Mutual Information (CMI) $I(X;Y \\mid Z)$ can be expressed as the expected Kullback–Leibler divergence between the conditional $p(y \\mid x,z)$ and the marginal conditional $p(y \\mid z)$, i.e., $I(X;Y \\mid Z) = \\mathbb{E}_{x,z}\\!\\left[ D_{\\mathrm{KL}}\\!\\big(p(\\cdot \\mid x,z)\\parallel p(\\cdot \\mid z)\\big)\\right]$, and hence $I(X;Y \\mid Z) \\ge 0$.\n- In standard neuroscience practice, TE from a source past to a target future conditional on the target past is defined to coincide with a suitable CMI between the source past and the target future conditioned on the target past.\n\nConsider the scenario above and the negative empirical estimate. Which of the following statements together provide a correct, principled interpretation and handling of such negative TE estimates that reconciles practice with theory?\n\nA. The true TE is non-negative because it is a conditional mutual information expressible as an expected Kullback–Leibler divergence; small negative empirical estimates arise from finite-sample bias and variance and should not be interpreted as evidence of negative directed information flow. In practice, one can use permutation-based nulls to assess significance and bias-correction methods to mitigate finite-sample effects.\n\nB. A negative TE estimate demonstrates inhibitory causal influence from the source to the target; the more negative the estimate, the stronger the inhibition, irrespective of estimator properties or sample size.\n\nC. The appearance of a negative TE estimate proves the estimator is inconsistent; a consistent estimator cannot yield negative values at any finite sample size.\n\nD. Even with consistent estimators, finite-sample fluctuations and bias in entropy and density estimates can produce negative TE estimates; one may enforce non-negativity by truncating estimates at zero or by subtracting a permutation-based bias estimate, but truncation can introduce upward bias and should be transparently reported.\n\nE. With misspecified embeddings (for example, incorrect delay or history lengths), the true TE can legitimately be negative because conditioning on the wrong variables can remove too much information from the target future.\n\nSelect all that apply.",
            "solution": "We begin from first principles. By the provided base, Conditional Mutual Information (CMI) $I(X;Y \\mid Z)$ can be represented as\n$$\nI(X;Y \\mid Z) \\;=\\; \\mathbb{E}_{x,z}\\!\\left[ D_{\\mathrm{KL}}\\!\\big(p(\\cdot \\mid x,z)\\parallel p(\\cdot \\mid z)\\big)\\right],\n$$\nwhere $D_{\\mathrm{KL}}(P\\parallel Q) \\ge 0$ by the non-negativity of Kullback–Leibler divergence from Gibbs' inequality. Therefore,\n$$\nI(X;Y \\mid Z) \\;\\ge\\; 0,\n$$\nwith equality if and only if $p(y \\mid x,z) = p(y \\mid z)$ almost surely under $p(x,z)$, i.e., the source provides no additional predictive information about the target future beyond the conditioned variables.\n\nIn standard neuroscience practice, Transfer Entropy (TE) from a source past to a target future conditional on the target past is defined to coincide with such a CMI. Consequently, the true TE is non-negative:\n$$\n\\mathrm{TE} \\;=\\; I\\!\\left(\\text{source past};\\ \\text{target future} \\mid \\text{target past}\\right) \\;\\ge\\; 0.\n$$\nAny theoretical negativity would contradict the non-negativity of $I(\\cdot;\\cdot \\mid \\cdot)$.\n\nHowever, in finite samples, estimators of TE rely on entropy, density, or probability ratio estimates that are biased. For instance, plug-in histogram estimators of entropy have downward bias of order $O\\!\\left(\\frac{1}{N}\\right)$, and nearest-neighbor methods exhibit finite-sample bias and variance depending on neighborhood size and dimensionality. Because TE is a difference of entropies or an average of log-likelihood ratios assembled from estimated quantities, the net estimation error can be negative for a given dataset, producing estimates like $-0.02$ bits even when the true TE is $0$ or positive. Consistency of an estimator only guarantees convergence to the true non-negative value as $N \\to \\infty$, not that finite-sample realizations preserve non-negativity.\n\nIn practice, to interpret such negative estimates while respecting the theoretical constraint, one can:\n- Use permutation or time-shift surrogates to build a null distribution that reflects finite-sample bias and dependence structure under no directed influence, then assess whether the observed estimate is distinguishable from null. Often, negative values fall within the null and indicate no evidence for directed influence.\n- Apply bias correction (for example, Miller–Madow for discrete plug-in, or subtracting the surrogate mean as an estimated finite-sample bias) to move the estimate toward the true non-negative value.\n- Optionally truncate at zero for reporting effect sizes, with the caveat that truncation induces upward bias and should be disclosed; statistical significance should be based on null distributions rather than truncation.\n\nWe now evaluate the options:\n\nA. This states that the true TE is non-negative because it is a CMI expressible as an expected Kullback–Leibler divergence, and attributes negative empirical estimates to finite-sample bias and variance. It recommends permutation-based nulls and bias correction. This aligns exactly with the derivation and standard practice. Verdict — Correct.\n\nB. This claims negative TE estimates indicate inhibitory causal influence and that magnitude reflects inhibitory strength. This misinterprets an estimator artifact as a physiological property and ignores the theoretical non-negativity of TE. There is no notion of “negative information flow” in TE; inhibitory influences can reduce TE but cannot make the true TE negative. Verdict — Incorrect.\n\nC. This asserts that any negative estimate implies estimator inconsistency, and that consistent estimators cannot produce negative finite-sample values. Consistency concerns asymptotic behavior as $N \\to \\infty$ and does not preclude finite-sample violations of constraints like non-negativity due to bias/variance. Therefore, this is false. Verdict — Incorrect.\n\nD. This recognizes that even consistent estimators can yield negative finite-sample estimates and suggests enforcing non-negativity by truncation or subtracting a permutation-based bias estimate, while warning that truncation introduces upward bias. This matches principled practice and acknowledges trade-offs. Verdict — Correct.\n\nE. This claims that embedding misspecification can make the true TE negative. Regardless of embedding, TE as defined remains a CMI between well-defined random variables and is therefore non-negative. Misspecification can reduce true TE or render it zero, but not negative. Verdict — Incorrect.\n\nThus, the correct choices are A and D.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "Having explored the theoretical underpinnings and interpretive nuances of Transfer Entropy, this final practice challenges you to synthesize your knowledge by building a complete end-to-end analysis pipeline. You will simulate coupled spike train data, implement a non-parametric TE estimator with appropriate smoothing, and perform rigorous significance testing using time-shifted surrogates. This capstone exercise solidifies the practical skills needed to move from raw time-series data to a statistically validated inference about directed functional connectivity .",
            "id": "4201578",
            "problem": "You are tasked with designing and implementing a complete computational pipeline to infer directed functional connectivity from multichannel spike train recordings typical of a Multielectrode Array (MEA) using Transfer Entropy (TE). The pipeline must span from preprocessing through surrogate-based significance testing and produce a single quantitative summary per dataset. The inferential target is the presence of directed influence between channels, operationalized via TE, computed from discretized time series of spike events. Your implementation should adhere to first principles starting from widely accepted foundational definitions of information-theoretic quantities and use only nonparametric, count-based estimators consistent with those foundations. The code must be self-contained and runnable without external inputs.\n\nAssume each channel is a binary time series representing the presence or absence of a spike in a fixed time bin. You will simulate realistic spike trains for a network of channels driven by a logistic point-process model with first-order coupling and auto-history. For channel index $j \\in \\{0, \\ldots, C-1\\}$ at time index $t \\in \\{0, \\ldots, N-2\\}$, the spike probability for the next time bin is defined by\n$$\np_j(t+1) \\triangleq \\sigma\\left(b + a \\, x_j(t) + \\sum_{i=0}^{C-1} W_{i,j} \\, x_i(t)\\right),\n$$\nwhere $x_j(t) \\in \\{0,1\\}$ is the spike state for channel $j$ at time $t$, $a$ is the auto-history weight, $W_{i,j}$ is the directed coupling weight from channel $i$ to channel $j$, $b$ is the bias determined by the desired base spike rate $r$ via $b = \\log\\left(\\frac{r}{1-r}\\right)$, and $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ is the logistic function. Initialize $x_j(0)=0$ for all $j$ and generate $x_j(t+1)$ by drawing from a Bernoulli distribution with parameter $p_j(t+1)$.\n\nYour TE estimator must use a discrete, nonparametric, plug-in approach with additive smoothing. For a source channel $X$ and a target channel $Y$, with target past length $k$ and source past length $l$ (both integers), discretize the state space by enumerating all combinations of $Y$'s past of length $k$, $X$'s past of length $l$, and $Y$'s next value. Use additive pseudo-count smoothing with parameter $\\alpha_s > 0$ applied uniformly to all joint-count cells to avoid zero probabilities. The TE from $X$ to $Y$ is defined as the conditional mutual information from the past of $X$ to the next value of $Y$ given the past of $Y$. You must not introduce any parametric assumptions beyond the binary discretization and the logistic spike generation model described above.\n\nFor surrogate-based significance testing, construct $S$ surrogate source sequences by circularly shifting the source channel $X$ by a random offset uniformly sampled from the set $\\{1,2,\\ldots,N-1\\}$, recompute the TE for each surrogate, and form an empirical null distribution. Compute the $p$-value for the observed TE as\n$$\np \\triangleq \\frac{1 + \\#\\{\\text{surrogates with } \\mathrm{TE}_{\\text{sur}} \\ge \\mathrm{TE}_{\\text{obs}}\\}}{S+1},\n$$\nand declare significance if $p < \\gamma$, where $\\gamma$ is the significance level. This test is to be applied independently for each directed pair $(i \\to j)$ with $i \\ne j$.\n\nPreprocessing should ensure signals are binary and aligned in time bins, but you must include a preprocessing step in your pipeline that validates and coerces inputs to $\\{0,1\\}$ where necessary.\n\nImplement the following pipeline components:\n- Preprocessing: validate and coerce inputs to binary $\\{0,1\\}$ arrays of shape $N \\times C$.\n- Embedding selection: use $k$ and $l$ specified in the test suite below.\n- TE estimation: a discrete plug-in estimator with additive smoothing $\\alpha_s$.\n- Surrogate-based significance testing: circular-shift surrogates, $S$ samples, significance level $\\gamma$.\n- Connectivity inference: for each dataset, test all directed pairs $(i \\to j)$, $i \\ne j$, count the number of significant directed edges, and return this count.\n\nYour program must implement a test suite of $4$ datasets specified below. For each dataset, simulate spike trains using the logistic model given above with the specified parameters. Then run the pipeline to infer directed connectivity and output the number of significant directed edges detected in each dataset in the specified final format.\n\nTest suite parameter sets:\n- Dataset $1$ (happy path, unidirectional coupling): $C=2$, $N=4000$, $r=0.05$, $a=0.6$, $W$ with $W_{0,1}=2.0$ and all other entries $0$, embedding $k=1$, $l=1$, smoothing $\\alpha_s=0.5$, surrogates $S=50$, significance level $\\gamma=0.05$, random seed $123$.\n- Dataset $2$ (no coupling): $C=2$, $N=4000$, $r=0.05$, $a=0.6$, $W$ with all entries $0$, embedding $k=1$, $l=1$, smoothing $\\alpha_s=0.5$, surrogates $S=50$, significance level $\\gamma=0.05$, random seed $456$.\n- Dataset $3$ (reciprocal coupling): $C=2$, $N=5000$, $r=0.05$, $a=0.6$, $W$ with $W_{0,1}=1.8$, $W_{1,0}=1.8$, and other entries $0$, embedding $k=1$, $l=1$, smoothing $\\alpha_s=0.5$, surrogates $S=60$, significance level $\\gamma=0.05$, random seed $789$.\n- Dataset $4$ (boundary case, weak coupling and shorter recording): $C=2$, $N=1000$, $r=0.05$, $a=0.6$, $W$ with $W_{0,1}=0.3$ and other entries $0$, embedding $k=1$, $l=1$, smoothing $\\alpha_s=0.5$, surrogates $S=100$, significance level $\\gamma=0.05$, random seed $101112$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\texttt{[result1,result2,result3,result4]}$), where each result is the integer count of significant directed edges detected for the corresponding dataset.\n\nNo physical units are involved; all quantities are dimensionless probabilities or counts. Angles are not used. Express all final numerical outputs as integers within the specified format.",
            "solution": "The problem statement is valid. It is scientifically grounded in the principles of information theory and computational neuroscience, well-posed with all necessary parameters and definitions provided, and objective in its formulation. The task is to construct a complete computational pipeline to infer directed functional connectivity from simulated spike train data using Transfer Entropy (TE), including data generation, non-parametric TE estimation, and surrogate-based significance testing.\n\nThe solution is designed as a modular pipeline composed of several components, each grounded in established principles.\n\n**1. Spike Train Generation via a Logistic Point-Process Model**\n\nThe foundation of this analysis is the generation of synthetic spike train data that emulates coupled neuronal activity. We model a network of $C$ channels over $N$ discrete time bins. The state of each channel $j \\in \\{0, \\ldots, C-1\\}$ at time $t$ is a binary variable $x_j(t) \\in \\{0, 1\\}$, where $1$ signifies a spike and $0$ signifies silence.\n\nThe temporal evolution of the network is governed by a logistic point-process model. The probability $p_j(t+1)$ of channel $j$ spiking in the next time bin $t+1$ is conditioned on the state of the entire network at time $t$. This dependency is formulated using the logistic function $\\sigma(z) = (1 + e^{-z})^{-1}$:\n$$\np_j(t+1) = \\sigma\\left(b + a \\, x_j(t) + \\sum_{i=0}^{C-1} W_{i,j} \\, x_i(t)\\right)\n$$\nHere, $a$ is the auto-history weight, capturing the influence of a channel's own past activity on its future. The matrix $W$ contains the coupling weights, where $W_{i,j}$ represents the directed influence from source channel $i$ to target channel $j$. The term $b$ is a bias, which sets the baseline firing probability. It is determined by the desired base spike rate $r \\in (0, 1)$ under conditions of no prior activity ($x_i(t) = 0$ for all $i$). Setting $p_j(t+1) = r$ in the absence of input yields $r = \\sigma(b)$, which is inverted to find the bias:\n$$\nb = \\text{logit}(r) = \\log\\left(\\frac{r}{1-r}\\right)\n$$\nThe simulation starts with an initial state of quiescence, $x_j(0) = 0$ for all $j$. Subsequently, for each time step $t$ from $0$ to $N-2$, the spike states $x_j(t+1)$ are generated by drawing from a Bernoulli distribution with the calculated probability $p_j(t+1)$. This process is repeated for all channels to generate the full data matrix of size $N \\times C$.\n\n**2. Transfer Entropy (TE) Estimation**\n\nTransfer Entropy from a source process $X$ to a target process $Y$ quantifies the reduction in uncertainty about the future of $Y$ from knowing the past of $X$, given the past of $Y$. It is formally defined as a conditional mutual information:\n$$\nT_{X \\to Y} = I(X_{\\text{past}}; Y_{\\text{next}} | Y_{\\text{past}})\n$$\nFor discrete time series, this is expressed as:\n$$\nT_{X \\to Y}(k, l) = \\sum p(y_{t+1}, y_t^{(k)}, x_t^{(l)}) \\log_2 \\frac{p(y_{t+1} | y_t^{(k)}, x_t^{(l)})}{p(y_{t+1} | y_t^{(k)})}\n$$\nwhere $y_{t}^{(k)} = (y_t, \\ldots, y_{t-k+1})$ is the past of the target $Y$ of length $k$, and $x_{t}^{(l)} = (x_t, \\ldots, x_{t-l+1})$ is the past of the source $X$ of length $l$. The sum is over all possible state configurations.\n\nThis can be rewritten in terms of joint and marginal probabilities:\n$$\nT_{X \\to Y} = \\sum p(y_{t+1}, y_t^{(k)}, x_t^{(l)}) \\log_2 \\frac{p(y_{t+1}, y_t^{(k)}, x_t^{(l)}) \\, p(y_t^{(k)})}{p(y_t^{(k)}, x_t^{(l)}) \\, p(y_{t+1}, y_t^{(k)})}\n$$\nFor this problem, the embedding parameters are fixed at $k=1$ and $l=1$. This simplifies the state vectors to $y_t^{(1)} = (y_t)$ and $x_t^{(1)} = (x_t)$. The relevant joint probability distribution is $p(y_{t+1}, y_t, x_t)$.\n\nThe probabilities are estimated using a non-parametric, plug-in estimator based on frequency counts from the time series data. We construct a $3$-dimensional histogram for the counts of each state triplet $(y_{t+1}, y_t, x_t)$ over the available data points, which span from $t=0$ to $t=N-2$.\n\nTo prevent issues with zero probabilities from unobserved states, an additive smoothing (or pseudo-count) method is employed. A small positive constant $\\alpha_s$ is added to the count of every possible state configuration. For binary-state variables with $k=1$ and $l=1$, there are $2^{k+l+1} = 2^3 = 8$ possible states. If $C(y_{t+1}, y_t, x_t)$ is the raw count for a state, the smoothed probability is:\n$$\np(y_{t+1}, y_t, x_t) = \\frac{C(y_{t+1}, y_t, x_t) + \\alpha_s}{(N-1) + 2^{k+l+1} \\alpha_s}\n$$\nThe marginal probabilities required for the TE formula are then computed by summing over the appropriate dimensions of this smoothed joint probability distribution. With these estimated probabilities, the TE is calculated by summing the terms over all $8$ states.\n\n**3. Surrogate-Based Significance Testing**\n\nThe raw TE value is a measure of information transfer, but it can be non-zero even for uncoupled processes due to finite sample effects. To assess statistical significance, we test the null hypothesis of no true information transfer from $X$ to $Y$. This is achieved by generating an empirical null distribution of TE values from surrogate data.\n\nSurrogates are created by applying a transformation to the original data that disrupts the specific relationship being tested while preserving other statistical properties. Here, we use the circular time-shift method. The source time series $X$ is circularly shifted by a random integer offset drawn uniformly from $\\{1, 2, \\ldots, N-1\\}$. This procedure destroys the specific temporal relationship between the source $X$ and the target $Y$ but preserves the auto-correlation structure and marginal distribution of $X$.\n\nFor each directed pair $(i \\to j)$, we compute the observed TE, $\\mathrm{TE}_{\\text{obs}}$. Then, we generate $S$ surrogate source time series and calculate the TE for each, forming a distribution of $\\mathrm{TE}_{\\text{sur}}$. The one-sided $p$-value is then computed as:\n$$\np = \\frac{1 + \\#\\{\\text{surrogates with } \\mathrm{TE}_{\\text{sur}} \\ge \\mathrm{TE}_{\\text{obs}}\\}}{S+1}\n$$\nThe inclusion of $1$ in the numerator and denominator accounts for the observed value itself and provides a more conservative estimate.\n\n**4. Full Connectivity Inference Pipeline**\n\nThe complete pipeline integrates these components to analyze each dataset:\n1.  **Simulation**: For each parameter set in the test suite, generate spike train data of shape $N \\times C$ using the logistic model with the specified random seed.\n2.  **Preprocessing**: The simulated data is already in the required binary format. This step formally validates the data type.\n3.  **Inference Loop**: Iterate through all ordered pairs of distinct channels $(i, j)$ with $i \\ne j$.\n    a.  **Source/Target Assignment**: For a pair $(i \\to j)$, designate channel $i$ as the source $X$ and channel $j$ as the target $Y$.\n    b.  **Observed TE**: Calculate $\\mathrm{TE}_{\\text{obs}}$ for the pair $(X, Y)$ using the specified embedding ($k=1, l=1$) and smoothing ($\\alpha_s$).\n    c.  **Significance Test**: Generate $S$ surrogate TEs by circularly shifting the source channel $X$. Compute the $p$-value based on the comparison of $\\mathrm{TE}_{\\text{obs}}$ with the surrogate distribution.\n    d.  **Decision**: If the computed $p$-value is less than the significance level $\\gamma$, the directed edge $(i \\to j)$ is considered significant.\n4.  **Aggregation**: Count the total number of significant directed edges detected for the dataset. This integer count is the final result for that dataset.\n\nThis procedure is repeated for all four datasets in the test suite to produce the final list of results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ...\n\ndef simulate_spike_trains(n_channels, n_timesteps, base_rate, auto_history_weight, w_matrix, rng):\n    \"\"\"\n    Simulates spike trains using a logistic point-process model.\n    \"\"\"\n    spikes = np.zeros((n_timesteps, n_channels), dtype=np.int8)\n    \n    # Calculate bias term from base rate\n    # b = logit(r) = log(r / (1-r))\n    bias = np.log(base_rate / (1 - base_rate))\n\n    # sigmoid function for calculating probability\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    for t in range(n_timesteps - 1):\n        # Calculate the linear term for the logistic function for all channels at once\n        # logit_values = b + a*x_t + W^T . x_t\n        # Note: problem statement sum is over i for a fixed j, which corresponds to X @ W\n        logit_values = bias + auto_history_weight * spikes[t, :] + spikes[t, :] @ w_matrix\n        \n        # Calculate spike probabilities for the next time step\n        probabilities = sigmoid(logit_values)\n        \n        # Generate spikes for the next time step using Bernoulli trials\n        spikes[t + 1, :] = rng.random(n_channels)  probabilities\n        \n    return spikes\n\ndef calculate_te(source_ts, target_ts, k, l, alpha_s):\n    \"\"\"\n    Calculates Transfer Entropy from source to target using a discrete plug-in estimator.\n    This implementation is specialized for k=1 and l=1 as specified in the problem.\n    \"\"\"\n    if k != 1 or l != 1:\n        raise ValueError(\"This implementation is specialized for k=1 and l=1.\")\n\n    n_timesteps = len(source_ts)\n    n_samples = n_timesteps - 1\n    \n    # State vectors\n    y_next = target_ts[1:]\n    y_past = target_ts[:-1]\n    x_past = source_ts[:-1]\n\n    # Count occurrences of (y_next, y_past, x_past) states\n    # Dimensions: [y_next, y_past, x_past]\n    counts = np.zeros((2, 2, 2))\n    for i in range(n_samples):\n        counts[y_next[i], y_past[i], x_past[i]] += 1\n    \n    # Apply additive smoothing\n    counts_smoothed = counts + alpha_s\n    \n    # Calculate total count for probability normalization\n    total_count = n_samples + 8 * alpha_s\n\n    # Joint probability p(y_next, y_past, x_past)\n    p_joint = counts_smoothed / total_count\n    \n    # Marginal probabilities required for TE formula\n    p_y_past_x_past = np.sum(p_joint, axis=0) # Sum over y_next\n    p_y_next_y_past = np.sum(p_joint, axis=2) # Sum over x_past\n    p_y_past = np.sum(p_y_past_x_past, axis=1) # Sum over x_past from p_y_past_x_past\n\n    # Calculate Transfer Entropy\n    te = 0.0\n    with np.errstate(divide='ignore', invalid='ignore'):\n      # We iterate over indices, which is safe\n      for i_yn in range(2):\n          for i_yp in range(2):\n              for i_xp in range(2):\n                  p_j = p_joint[i_yn, i_yp, i_xp]\n                  \n                  if p_j > 0: # This check is redundant with smoothing but good practice\n                      p_yp_xp = p_y_past_x_past[i_yp, i_xp]\n                      p_yn_yp = p_y_next_y_past[i_yn, i_yp]\n                      p_yp = p_y_past[i_yp]\n                      \n                      # log_term = log2( (p(y_next,y_past,x_past) * p(y_past)) / (p(y_past,x_past) * p(y_next,y_past)) )\n                      log_term = np.log2(p_j * p_yp / (p_yp_xp * p_yn_yp))\n                      te += p_j * log_term\n                      \n    return te\n\ndef run_pipeline_for_dataset(params):\n    \"\"\"\n    Runs the full analysis pipeline for a single dataset's parameters.\n    \"\"\"\n    (C, N, r, a, W, k, l, alpha_s, S, gamma, seed) = params\n    \n    rng = np.random.default_rng(seed)\n\n    # 1. Simulate spike trains\n    spike_data = simulate_spike_trains(C, N, r, a, np.array(W), rng)\n\n    # 2. Preprocessing (data is already binary {0,1} from simulation)\n    # The problem asks for this step, so we ensure the type is correct.\n    spike_data = spike_data.astype(np.int8)\n\n    significant_edges_count = 0\n    \n    # 3. Connectivity Inference Loop\n    # Iterate over all directed pairs (i -> j) where i != j\n    for i in range(C): # source channel\n        for j in range(C): # target channel\n            if i == j:\n                continue\n\n            source_ts = spike_data[:, i]\n            target_ts = spike_data[:, j]\n\n            # 4. Calculate observed TE\n            te_obs = calculate_te(source_ts, target_ts, k, l, alpha_s)\n            \n            # 5. Surrogate-based significance testing\n            te_surrogates = np.zeros(S)\n            for s_idx in range(S):\n                # Generate surrogate source by circular shift\n                shift = rng.integers(1, N)\n                source_surr = np.roll(source_ts, shift)\n                \n                te_surrogates[s_idx] = calculate_te(source_surr, target_ts, k, l, alpha_s)\n\n            # 6. Calculate p-value\n            n_exceed = np.sum(te_surrogates >= te_obs)\n            p_value = (1 + n_exceed) / (S + 1)\n            \n            # 7. Check for significance\n            if p_value  gamma:\n                significant_edges_count += 1\n                \n    return significant_edges_count\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset 1\n        (2, 4000, 0.05, 0.6, [[0.0, 2.0], [0.0, 0.0]], 1, 1, 0.5, 50, 0.05, 123),\n        # Dataset 2\n        (2, 4000, 0.05, 0.6, [[0.0, 0.0], [0.0, 0.0]], 1, 1, 0.5, 50, 0.05, 456),\n        # Dataset 3\n        (2, 5000, 0.05, 0.6, [[0.0, 1.8], [1.8, 0.0]], 1, 1, 0.5, 60, 0.05, 789),\n        # Dataset 4\n        (2, 1000, 0.05, 0.6, [[0.0, 0.3], [0.0, 0.0]], 1, 1, 0.5, 100, 0.05, 101112),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_pipeline_for_dataset(params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}