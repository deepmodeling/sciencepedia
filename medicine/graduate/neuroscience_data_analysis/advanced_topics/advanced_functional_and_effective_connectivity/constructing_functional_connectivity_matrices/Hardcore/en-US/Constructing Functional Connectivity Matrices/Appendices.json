{
    "hands_on_practices": [
        {
            "introduction": "A core challenge in network neuroscience is distinguishing genuine pairwise interactions from spurious correlations induced by a common driver. This foundational exercise uses a simple, hypothetical three-node model to illustrate this principle vividly . By deriving both the full and partial correlation from first principles, you will see how a seemingly strong connection can vanish once the influence of a confounding third region is statistically removed, demonstrating the importance of moving beyond simple correlation.",
            "id": "4147921",
            "problem": "Consider three Regions of Interest (ROI) in a functional neuroimaging experiment, labeled $i$, $j$, and $k$. Let the latent neural signal in the confounding region $k$ be a zero-mean Gaussian random variable $X_{k}$ with variance $\\operatorname{Var}(X_{k})=1$. Let the observed signals in regions $i$ and $j$ be generated by a linear mixture of $X_{k}$ and independent additive noise, specifically\n$$\nX_{i} = 2 X_{k} + \\epsilon_{i}, \\quad X_{j} = - X_{k} + \\epsilon_{j},\n$$\nwhere $\\epsilon_{i}$ and $\\epsilon_{j}$ are zero-mean Gaussian noises, mutually independent and also independent of $X_{k}$, with $\\operatorname{Var}(\\epsilon_{i}) = 1$ and $\\operatorname{Var}(\\epsilon_{j}) = 1$. Assume all variables are jointly Gaussian and consider single-time-point population covariances.\n\nUsing first principles—namely, the definitions of covariance and correlation for jointly Gaussian variables, and the foundational relationship between conditional independence and partial correlation in multivariate normal models—derive the population-level full correlation between $X_{i}$ and $X_{j}$ and explain why it suggests a connection under a correlation-based functional connectivity matrix. Then, construct the partial correlation between $X_{i}$ and $X_{j}$ that removes the influence of the confounder $X_{k}$. Compute this partial correlation exactly.\n\nYour final answer must be the single real-valued number equal to the partial correlation between $X_{i}$ and $X_{j}$ controlling for $X_{k}$, denoted $\\rho_{ij \\cdot k}$. Express the result exactly; no rounding is required. Correlations are unitless.",
            "solution": "The problem asks for the calculation of the full correlation and the partial correlation between two observed signals, $X_i$ and $X_j$, which are influenced by a common latent signal $X_k$. The analysis will be based on first principles of statistics for jointly Gaussian variables.\n\nFirst, we establish the statistical properties of the observed signals $X_i$ and $X_j$ based on the given generative model.\nThe signals are defined as:\n$$\nX_{i} = 2 X_{k} + \\epsilon_{i}\n$$\n$$\nX_{j} = - X_{k} + \\epsilon_{j}\n$$\nAll variables $X_k$, $\\epsilon_i$, and $\\epsilon_j$ are given as zero-mean Gaussian random variables. Due to the linearity of expectation, the means of $X_i$ and $X_j$ are also zero:\n$$\nE[X_i] = E[2 X_k + \\epsilon_i] = 2 E[X_k] + E[\\epsilon_i] = 2(0) + 0 = 0\n$$\n$$\nE[X_j] = E[-X_k + \\epsilon_j] = -E[X_k] + E[\\epsilon_j] = -0 + 0 = 0\n$$\n\nThe variances of the observed signals are calculated using the property that for independent random variables $A$ and $B$, $\\operatorname{Var}(aA + bB) = a^2\\operatorname{Var}(A) + b^2\\operatorname{Var}(B)$. We are given that $X_k$ is independent of both $\\epsilon_i$ and $\\epsilon_j$.\n$$\n\\operatorname{Var}(X_i) = \\operatorname{Var}(2 X_k + \\epsilon_i) = 2^2 \\operatorname{Var}(X_k) + \\operatorname{Var}(\\epsilon_i) = 4(1) + 1 = 5\n$$\n$$\n\\operatorname{Var}(X_j) = \\operatorname{Var}(- X_k + \\epsilon_j) = (-1)^2 \\operatorname{Var}(X_k) + \\operatorname{Var}(\\epsilon_j) = 1(1) + 1 = 2\n$$\nThe standard deviations are $\\sigma_i = \\sqrt{\\operatorname{Var}(X_i)} = \\sqrt{5}$ and $\\sigma_j = \\sqrt{\\operatorname{Var}(X_j)} = \\sqrt{2}$.\n\nNext, we compute the full correlation between $X_i$ and $X_j$, denoted $\\rho_{ij}$. This requires the covariance, $\\operatorname{Cov}(X_i, X_j)$. Using the bilinearity of the covariance operator:\n$$\n\\operatorname{Cov}(X_i, X_j) = \\operatorname{Cov}(2 X_k + \\epsilon_i, -X_k + \\epsilon_j)\n$$\n$$\n= \\operatorname{Cov}(2X_k, -X_k) + \\operatorname{Cov}(2X_k, \\epsilon_j) + \\operatorname{Cov}(\\epsilon_i, -X_k) + \\operatorname{Cov}(\\epsilon_i, \\epsilon_j)\n$$\nGiven that $\\epsilon_i$, $\\epsilon_j$, and $X_k$ are mutually independent, the covariances of pairs of different variables are zero: $\\operatorname{Cov}(X_k, \\epsilon_j)=0$, $\\operatorname{Cov}(\\epsilon_i, X_k)=0$, and $\\operatorname{Cov}(\\epsilon_i, \\epsilon_j)=0$. The expression simplifies to:\n$$\n\\operatorname{Cov}(X_i, X_j) = \\operatorname{Cov}(2X_k, -X_k) = 2(-1)\\operatorname{Cov}(X_k, X_k) = -2\\operatorname{Var}(X_k)\n$$\nSince $\\operatorname{Var}(X_k)=1$, we have:\n$$\n\\operatorname{Cov}(X_i, X_j) = -2\n$$\nThe full correlation coefficient $\\rho_{ij}$ is the covariance normalized by the product of the standard deviations:\n$$\n\\rho_{ij} = \\frac{\\operatorname{Cov}(X_i, X_j)}{\\sqrt{\\operatorname{Var}(X_i)\\operatorname{Var}(X_j)}} = \\frac{-2}{\\sqrt{5 \\cdot 2}} = \\frac{-2}{\\sqrt{10}}\n$$\nThis is a non-zero, moderately strong negative correlation ($\\rho_{ij} \\approx -0.632$). In a standard functional connectivity analysis based on correlation, this finding would suggest a significant functional relationship between ROIs $i$ and $j$. However, the generative model reveals that there is no direct causal link or shared variance between $X_i$ and $X_j$ other than their common dependence on $X_k$. The observed correlation is entirely induced by this common input from the confounding region $k$.\n\nTo correctly assess the relationship between $X_i$ and $X_j$, we must account for the influence of $X_k$. In a multivariate normal model, this is precisely the purpose of the partial correlation, $\\rho_{ij \\cdot k}$. The partial correlation measures the correlation between $X_i$ and $X_j$ after removing the linear effects of $X_k$. For jointly Gaussian variables, $\\rho_{ij \\cdot k}$ is fundamentally defined as the correlation between the residuals of $X_i$ and $X_j$ after they are linearly regressed on $X_k$.\n\nLet the residual of $X_i$ after regressing on $X_k$ be $R_i = X_i - E[X_i | X_k]$.\nLet the residual of $X_j$ after regressing on $X_k$ be $R_j = X_j - E[X_j | X_k]$.\nFor the linear Gaussian model given, the conditional expectations are:\n$$\nE[X_i | X_k] = E[2X_k + \\epsilon_i | X_k] = 2E[X_k|X_k] + E[\\epsilon_i|X_k] = 2X_k + 0 = 2X_k\n$$\nThe last step follows because $\\epsilon_i$ is independent of $X_k$, so $E[\\epsilon_i|X_k] = E[\\epsilon_i] = 0$.\nSimilarly,\n$$\nE[X_j | X_k] = E[-X_k + \\epsilon_j | X_k] = -E[X_k|X_k] + E[\\epsilon_j|X_k] = -X_k + 0 = -X_k\n$$\nThe residuals are therefore:\n$$\nR_i = X_i - 2X_k = (2X_k + \\epsilon_i) - 2X_k = \\epsilon_i\n$$\n$$\nR_j = X_j - (-X_k) = (-X_k + \\epsilon_j) + X_k = \\epsilon_j\n$$\nThe partial correlation is the correlation between these residuals:\n$$\n\\rho_{ij \\cdot k} = \\operatorname{Corr}(R_i, R_j) = \\operatorname{Corr}(\\epsilon_i, \\epsilon_j)\n$$\nThe problem states that the noise terms $\\epsilon_i$ and $\\epsilon_j$ are mutually independent. For independent variables, the covariance is zero, and thus the correlation is also zero.\n$$\n\\operatorname{Cov}(\\epsilon_i, \\epsilon_j) = 0\n$$\nTherefore,\n$$\n\\rho_{ij \\cdot k} = \\frac{\\operatorname{Cov}(\\epsilon_i, \\epsilon_j)}{\\sqrt{\\operatorname{Var}(\\epsilon_i)\\operatorname{Var}(\\epsilon_j)}} = \\frac{0}{\\sqrt{1 \\cdot 1}} = 0\n$$\nThe partial correlation between $X_i$ and $X_j$ after controlling for $X_k$ is exactly zero. This result correctly reflects the underlying structure of the model: there is no direct connection between regions $i$ and $j$. Their full correlation was spurious, caused entirely by the common influence of region $k$. This demonstrates the critical importance of accounting for confounders in functional connectivity studies. The value of $\\rho_{ij \\cdot k}$ is the single real-valued number requested.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "To address the issue of confounding signals, analysts routinely employ nuisance regression as a critical preprocessing step. This practice moves from the conceptual to the practical, tasking you with deriving the mathematical operator for nuisance regression from the principles of ordinary least squares . Applying this derivation to a concrete dataset demystifies how raw BOLD time series are \"cleaned\" and prepares you to correctly calculate connectivity from preprocessed data.",
            "id": "4147895",
            "problem": "In resting-state functional connectivity analysis, raw Blood Oxygenation Level Dependent (BOLD) time series are commonly preprocessed by regressing out nuisance signals such as head motion, cerebrospinal fluid (CSF), and white matter (WM) before computing correlations between regions. Consider a single-session dataset with $T$ time points. Let $X \\in \\mathbb{R}^{T \\times p}$ denote a nuisance design matrix whose columns span the nuisance subspace, and let $y \\in \\mathbb{R}^{T}$ denote a raw regional BOLD time series. Residualization is performed by fitting an ordinary least squares model and subtracting the fitted nuisance component from $y$. Starting from the definition of ordinary least squares as the minimizer of the sum of squared residuals and the definition of orthogonal projection onto a subspace, derive the linear operator that maps $y$ to its regression residuals with respect to the column space of $X$, and then apply it to the following concrete example for constructing functional connectivity.\n\nYou are given $T=5$ time points for two regions with raw BOLD time series $y^{(1)}$ and $y^{(2)}$:\n$$\ny^{(1)} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 5 \\\\ 7 \\end{pmatrix}, \n\\quad\ny^{(2)} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 4 \\\\ 3 \\\\ 6 \\end{pmatrix}.\n$$\nFor nuisance regression, use a two-regressor design matrix $X = [x_{0}, x_{1}] \\in \\mathbb{R}^{5 \\times 2}$ whose columns are\n$$\nx_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\quad \\text{(intercept)}, \n\\qquad\nx_{1} = \\begin{pmatrix} -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix} \\quad \\text{(a composite nuisance capturing CSF and WM physiology and head motion, standardized to zero mean)}.\n$$\nUsing only first principles of ordinary least squares and orthogonal projections, compute the Pearson correlation coefficient between the residualized time series for the two regions obtained by regressing $y^{(1)}$ and $y^{(2)}$ on the columns of $X$. Express your final answer as a single exact simplified expression with no units. Do not round.",
            "solution": "The problem is valid as it is scientifically grounded in standard statistical methods used in neuroscience data analysis (ordinary least squares, functional connectivity), is well-posed with a unique solution, and is expressed objectively with all necessary data provided.\n\nThe first step is to derive the linear operator that maps a raw time series vector $y \\in \\mathbb{R}^{T}$ to its regression residuals with respect to the column space of a design matrix $X \\in \\mathbb{R}^{T \\times p}$. The ordinary least squares (OLS) model is $y = X\\beta + e$, where $\\beta \\in \\mathbb{R}^{p}$ is the vector of coefficients and $e \\in \\mathbb{R}^{T}$ is the vector of residuals. OLS seeks to find the estimate $\\hat{\\beta}$ that minimizes the sum of squared residuals, which is the squared Euclidean norm of the residual vector $e$:\n$$S(\\beta) = \\|e\\|^2 = \\|y - X\\beta\\|^2 = (y - X\\beta)^T(y - X\\beta)$$\nExpanding this expression gives:\n$$S(\\beta) = y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta$$\nSince $\\beta^T X^T y$ is a scalar, it is equal to its transpose $(y^T X\\beta)^T$, which is $y^T X\\beta$. So, we can write:\n$$S(\\beta) = y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta$$\nTo find the minimum, we compute the gradient of $S(\\beta)$ with respect to $\\beta$ and set it to zero:\n$$\\frac{\\partial S}{\\partial \\beta} = -2X^T y + 2X^T X \\beta = 0$$\nThis yields the normal equations:\n$$X^T X \\hat{\\beta} = X^T y$$\nAssuming the columns of $X$ are linearly independent, the matrix $X^T X$ is invertible. The OLS estimate for the coefficients is then:\n$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\nThe vector of fitted values, $\\hat{y}$, is the projection of $y$ onto the column space of $X$. It is given by:\n$$\\hat{y} = X\\hat{\\beta} = X(X^T X)^{-1} X^T y$$\nThe matrix $P = X(X^T X)^{-1} X^T$ is the orthogonal projection matrix onto the column space of $X$. The regression residuals are the difference between the original data and the fitted values:\n$$e = y - \\hat{y} = y - Py = (I - P)y$$\nTherefore, the linear operator that maps $y$ to its residuals is the matrix $M = I - P = I - X(X^T X)^{-1} X^T$. This operator is itself an orthogonal projection matrix onto the subspace orthogonal to the column space of $X$.\n\nNow, we apply this to the given data. We are given $T=5$ and the design matrix $X = [x_0, x_1]$:\n$$X = \\begin{pmatrix} 1 & -2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$$\nFirst, we compute $X^T X$:\n$$X^T X = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ -2 & -1 & 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & -2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 5 & 0 \\\\ 0 & 10 \\end{pmatrix}$$\nThe off-diagonal elements are zero, which confirms that the columns $x_0$ and $x_1$ are orthogonal. This simplifies the calculations. The inverse is:\n$$(X^T X)^{-1} = \\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{10} \\end{pmatrix}$$\nWe need to find the residuals for $y^{(1)}$ and $y^{(2)}$. Let's denote them $e^{(1)}$ and $e^{(2)}$.\n\nFor $y^{(1)} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 5 \\\\ 7 \\end{pmatrix}$:\nWe first compute $X^T y^{(1)}$:\n$$X^T y^{(1)} = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ -2 & -1 & 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 5 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1+3+2+5+7 \\\\ -2-3+0+5+14 \\end{pmatrix} = \\begin{pmatrix} 18 \\\\ 14 \\end{pmatrix}$$\nNow we find the coefficients $\\hat{\\beta}^{(1)}$:\n$$\\hat{\\beta}^{(1)} = (X^T X)^{-1} X^T y^{(1)} = \\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 18 \\\\ 14 \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{5} \\\\ \\frac{14}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{5} \\\\ \\frac{7}{5} \\end{pmatrix}$$\nThe fitted values are $\\hat{y}^{(1)} = X\\hat{\\beta}^{(1)}$:\n$$\\hat{y}^{(1)} = \\begin{pmatrix} 1 & -2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{18}{5} \\\\ \\frac{7}{5} \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 18 - 14 \\\\ 18 - 7 \\\\ 18 - 0 \\\\ 18 + 7 \\\\ 18 + 14 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 4 \\\\ 11 \\\\ 18 \\\\ 25 \\\\ 32 \\end{pmatrix}$$\nThe residuals $e^{(1)}$ are $y^{(1)} - \\hat{y}^{(1)}$:\n$$e^{(1)} = \\frac{1}{5} \\begin{pmatrix} 5 \\\\ 15 \\\\ 10 \\\\ 25 \\\\ 35 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 4 \\\\ 11 \\\\ 18 \\\\ 25 \\\\ 32 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 1 \\\\ 4 \\\\ -8 \\\\ 0 \\\\ 3 \\end{pmatrix}$$\n\nFor $y^{(2)} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 4 \\\\ 3 \\\\ 6 \\end{pmatrix}$:\nWe compute $X^T y^{(2)}$:\n$$X^T y^{(2)} = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ -2 & -1 & 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ 4 \\\\ 3 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 2+1+4+3+6 \\\\ -4-1+0+3+12 \\end{pmatrix} = \\begin{pmatrix} 16 \\\\ 10 \\end{pmatrix}$$\nThe coefficients $\\hat{\\beta}^{(2)}$ are:\n$$\\hat{\\beta}^{(2)} = (X^T X)^{-1} X^T y^{(2)} = \\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 16 \\\\ 10 \\end{pmatrix} = \\begin{pmatrix} \\frac{16}{5} \\\\ \\frac{10}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{16}{5} \\\\ 1 \\end{pmatrix}$$\nThe fitted values are $\\hat{y}^{(2)} = X\\hat{\\beta}^{(2)}$:\n$$\\hat{y}^{(2)} = \\begin{pmatrix} 1 & -2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{16}{5} \\\\ 1 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 16 - 10 \\\\ 16 - 5 \\\\ 16 - 0 \\\\ 16 + 5 \\\\ 16 + 10 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 6 \\\\ 11 \\\\ 16 \\\\ 21 \\\\ 26 \\end{pmatrix}$$\nThe residuals $e^{(2)}$ are $y^{(2)} - \\hat{y}^{(2)}$:\n$$e^{(2)} = \\frac{1}{5} \\begin{pmatrix} 10 \\\\ 5 \\\\ 20 \\\\ 15 \\\\ 30 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 6 \\\\ 11 \\\\ 16 \\\\ 21 \\\\ 26 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 4 \\\\ -6 \\\\ 4 \\\\ -6 \\\\ 4 \\end{pmatrix}$$\n\nFinally, we compute the Pearson correlation coefficient, $r$, between $e^{(1)}$ and $e^{(2)}$. Since the regression included an intercept term ($x_0$), the residuals for both time series have a mean of zero. This simplifies the Pearson correlation formula to the cosine of the angle between the two vectors:\n$$r = \\frac{ \\sum_{i=1}^T e_i^{(1)} e_i^{(2)} }{ \\sqrt{\\sum_{i=1}^T (e_i^{(1)})^2} \\sqrt{\\sum_{i=1}^T (e_i^{(2)})^2} } = \\frac{e^{(1) T} e^{(2)}}{\\|e^{(1)}\\| \\|e^{(2)}\\|}$$\nWe calculate the dot product $e^{(1) T} e^{(2)}$:\n$$e^{(1) T} e^{(2)} = \\left(\\frac{1}{5}\\right)^2 \\begin{pmatrix} 1 & 4 & -8 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -6 \\\\ 4 \\\\ -6 \\\\ 4 \\end{pmatrix} = \\frac{1}{25}(1(4) + 4(-6) + (-8)(4) + 0(-6) + 3(4))$$\n$$e^{(1) T} e^{(2)} = \\frac{1}{25}(4 - 24 - 32 + 0 + 12) = \\frac{1}{25}(16 - 56) = \\frac{-40}{25} = -\\frac{8}{5}$$\nNext, we calculate the squared norms of the residual vectors:\n$$\\|e^{(1)}\\|^2 = \\left(\\frac{1}{5}\\right)^2 (1^2 + 4^2 + (-8)^2 + 0^2 + 3^2) = \\frac{1}{25}(1+16+64+0+9) = \\frac{90}{25} = \\frac{18}{5}$$\n$$\\|e^{(2)}\\|^2 = \\left(\\frac{1}{5}\\right)^2 (4^2 + (-6)^2 + 4^2 + (-6)^2 + 4^2) = \\frac{1}{25}(16+36+16+36+16) = \\frac{1}{25}(48+72) = \\frac{120}{25} = \\frac{24}{5}$$\nNow we assemble the correlation coefficient:\n$$r = \\frac{-\\frac{8}{5}}{\\sqrt{\\frac{18}{5}} \\sqrt{\\frac{24}{5}}} = \\frac{-\\frac{8}{5}}{\\sqrt{\\frac{18 \\times 24}{25}}} = \\frac{-\\frac{8}{5}}{\\frac{\\sqrt{432}}{5}} = \\frac{-8}{\\sqrt{432}}$$\nTo simplify the square root, we factor $432$: $432 = 144 \\times 3 = 12^2 \\times 3$. So, $\\sqrt{432} = 12\\sqrt{3}$.\n$$r = \\frac{-8}{12\\sqrt{3}} = \\frac{-2}{3\\sqrt{3}}$$\nRationalizing the denominator gives the final answer:\n$$r = \\frac{-2\\sqrt{3}}{3\\sqrt{3}\\sqrt{3}} = \\frac{-2\\sqrt{3}}{9}$$\nThe Pearson correlation coefficient between the residualized time series is $-\\frac{2\\sqrt{3}}{9}$.",
            "answer": "$$\\boxed{-\\frac{2\\sqrt{3}}{9}}$$"
        },
        {
            "introduction": "After cleaning time series, the final step is to estimate the covariance matrix, which forms the basis of the functional connectivity network. When the number of brain regions is large relative to the number of time points, the sample covariance matrix can be noisy and ill-conditioned. This advanced practice introduces Ledoit-Wolf shrinkage, a powerful regularization technique that produces a more stable and accurate covariance estimate by optimally combining the empirical data with a simplified target structure .",
            "id": "4147959",
            "problem": "You are given a zero-mean time series matrix $X \\in \\mathbb{R}^{n \\times p}$ representing neural activity from $p$ brain regions across $n$ time points acquired via functional magnetic resonance imaging (fMRI). The functional connectivity matrix is constructed from the covariance structure of the regions. Let the population covariance be $\\Sigma \\in \\mathbb{R}^{p \\times p}$, and let the sample covariance be defined by the core definition $ \\hat{\\Sigma} = \\frac{1}{n} X^{\\top} X $. Consider a shrinkage estimator for the covariance of the form $\\hat{\\Sigma}_{\\text{shrink}}(\\delta) = \\delta F + (1-\\delta) \\hat{\\Sigma}$, where $F \\in \\mathbb{R}^{p \\times p}$ is a deterministic target matrix that depends only on population quantities (for instance, a scaled identity $F = \\nu I_p$ with $\\nu = \\frac{1}{p} \\operatorname{tr}(\\Sigma)$), and $\\delta \\in [0,1]$ is the shrinkage intensity.\n\nStarting from first principles in multivariate statistics and matrix analysis, and using only the core definitions and well-tested facts stated below, derive the value of $\\delta$ that minimizes the expected squared Frobenius loss $\\mathbb{E}\\left[ \\| \\hat{\\Sigma}_{\\text{shrink}}(\\delta) - \\Sigma \\|_{F}^{2} \\right]$ under the assumptions that the rows $\\{x_t^{\\top}\\}_{t=1}^{n}$ of $X$ are independent and identically distributed with mean zero, covariance $\\Sigma$, and finite fourth moments. Show that the optimal $\\delta$ can be expressed in terms of the expected squared error of $\\hat{\\Sigma}$ and the expected discrepancy from the target, and then derive a fully data-driven, computable estimator $\\hat{\\delta}$ that depends only on $X$, $\\hat{\\Sigma}$, and $F$ (no unknown population quantities). Your derivation should begin from the definitions of covariance and the Frobenius norm, use the facts $\\mathbb{E}[\\hat{\\Sigma}] = \\Sigma$ and $\\langle A, B \\rangle = \\operatorname{tr}(A^{\\top} B)$ (the Frobenius inner product), and follow a logically complete chain of reasoning. Assume $F$ is symmetric and independent of the sample $X$. Conclude with a closed-form analytic expression for the shrinkage intensity $\\hat{\\delta}$ expressed entirely in terms of $X$, $\\hat{\\Sigma}$, $F$, $n$, and $p$. The final answer must be a single analytic expression. If any numerical approximation were needed it would be specified, but here none is required; do not include units.\n\nCore definitions and facts to use:\n- Sample covariance: $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{t=1}^{n} x_t x_t^{\\top}$.\n- Frobenius norm: $\\|A\\|_{F}^{2} = \\operatorname{tr}(A^{\\top} A)$ and inner product $\\langle A, B \\rangle = \\operatorname{tr}(A^{\\top} B)$.\n- Linearity of expectation and independence across time points $\\{x_t\\}_{t=1}^{n}$.\n- For any deterministic $F$ that does not depend on the sample $X$, $\\mathbb{E}[\\langle F-\\Sigma, \\hat{\\Sigma}-\\Sigma \\rangle] = 0$.\n\nYour final answer must be the closed-form expression for $\\hat{\\delta}$ that would be used to construct the Ledoit–Wolf shrinkage estimator for the covariance, which in turn underlies a well-conditioned functional connectivity matrix. No rounding is required.",
            "solution": "The objective is to find the shrinkage intensity $\\delta$ that minimizes the expected squared Frobenius loss between the shrinkage estimator $\\hat{\\Sigma}_{\\text{shrink}}(\\delta)$ and the true population covariance $\\Sigma$. The loss function is given by\n$$\nL(\\delta) = \\mathbb{E}\\left[ \\| \\hat{\\Sigma}_{\\text{shrink}}(\\delta) - \\Sigma \\|_{F}^{2} \\right]\n$$\nwhere $\\hat{\\Sigma}_{\\text{shrink}}(\\delta) = \\delta F + (1-\\delta) \\hat{\\Sigma}$. The derivation will proceed in two stages: first, we find the optimal $\\delta^*$ in terms of unknown population quantities, and second, we construct a data-driven estimator $\\hat{\\delta}$ by replacing the population quantities with their sample-based estimators.\n\nFirst, we substitute the definition of the shrinkage estimator into the loss function. To simplify the expression inside the norm, we add and subtract terms involving $\\Sigma$:\n$$\n\\hat{\\Sigma}_{\\text{shrink}}(\\delta) - \\Sigma = \\delta F + (1-\\delta) \\hat{\\Sigma} - \\Sigma = \\delta F - \\delta\\Sigma + (1-\\delta) \\hat{\\Sigma} - (1-\\delta)\\Sigma = \\delta(F - \\Sigma) + (1-\\delta)(\\hat{\\Sigma} - \\Sigma)\n$$\nLet the matrices $A = \\delta(F - \\Sigma)$ and $B = (1-\\delta)(\\hat{\\Sigma} - \\Sigma)$. The squared Frobenius norm of their sum is $\\|A+B\\|_F^2 = \\|A\\|_F^2 + \\|B\\|_F^2 + 2\\langle A, B \\rangle_F$. The loss function becomes:\n$$\nL(\\delta) = \\mathbb{E}\\left[ \\| \\delta(F - \\Sigma) + (1-\\delta)(\\hat{\\Sigma} - \\Sigma) \\|_{F}^{2} \\right] = \\mathbb{E}\\left[ \\delta^2 \\|F - \\Sigma\\|_F^2 + (1-\\delta)^2 \\|\\hat{\\Sigma} - \\Sigma\\|_F^2 + 2\\delta(1-\\delta) \\langle F - \\Sigma, \\hat{\\Sigma} - \\Sigma \\rangle \\right]\n$$\nUsing the linearity of the expectation operator, we can take the expectation of each term separately:\n$$\nL(\\delta) = \\mathbb{E}[\\delta^2 \\|F - \\Sigma\\|_F^2] + \\mathbb{E}[(1-\\delta)^2 \\|\\hat{\\Sigma} - \\Sigma\\|_F^2] + \\mathbb{E}[2\\delta(1-\\delta) \\langle F - \\Sigma, \\hat{\\Sigma} - \\Sigma \\rangle]\n$$\nSince $\\delta$ is a non-random scalar, and $F$ and $\\Sigma$ are deterministic population quantities, they can be pulled out of the expectation:\n$$\nL(\\delta) = \\delta^2 \\|F - \\Sigma\\|_F^2 + (1-\\delta)^2 \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2] + 2\\delta(1-\\delta) \\mathbb{E}[\\langle F - \\Sigma, \\hat{\\Sigma} - \\Sigma \\rangle]\n$$\nThe problem statement provides the fact that $\\mathbb{E}[\\langle F - \\Sigma, \\hat{\\Sigma} - \\Sigma \\rangle] = 0$. This is because $\\mathbb{E}[\\hat{\\Sigma}]=\\Sigma$, so $\\mathbb{E}[\\hat{\\Sigma}-\\Sigma]=0$, and the inner product with a constant matrix $F-\\Sigma$ is zero. Applying this fact, the cross-term vanishes. The loss function simplifies to a quadratic function of $\\delta$:\n$$\nL(\\delta) = \\delta^2 \\|F - \\Sigma\\|_F^2 + (1-\\delta)^2 \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2]\n$$\nTo find the value of $\\delta$ that minimizes this loss, we differentiate $L(\\delta)$ with respect to $\\delta$ and set the derivative to zero:\n$$\n\\frac{dL(\\delta)}{d\\delta} = 2\\delta \\|F - \\Sigma\\|_F^2 + 2(1-\\delta)(-1) \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2] = 0\n$$\n$$\n\\delta \\|F - \\Sigma\\|_F^2 = (1-\\delta) \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2]\n$$\nSolving for the optimal $\\delta$, which we denote $\\delta^*$:\n$$\n\\delta^* \\|F - \\Sigma\\|_F^2 = \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2] - \\delta^* \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2]\n$$\n$$\n\\delta^* \\left( \\|F - \\Sigma\\|_F^2 + \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2] \\right) = \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2]\n$$\n$$\n\\delta^* = \\frac{\\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2]}{\\|F - \\Sigma\\|_F^2 + \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2]}\n$$\nThis expression demonstrates that the optimal shrinkage intensity is indeed expressed in terms of the expected squared error of the sample covariance, $N_{pop} = \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2]$, and the squared discrepancy of the target from the population covariance, $D_{pop} = \\|F - \\Sigma\\|_F^2$. The denominator can also be written as $\\mathbb{E}[\\|F - \\hat{\\Sigma}\\|_F^2]$, since $\\mathbb{E}[\\|F - \\hat{\\Sigma}\\|_F^2] = \\mathbb{E}[\\|(F - \\Sigma) - (\\hat{\\Sigma} - \\Sigma)\\|_F^2] = \\|F - \\Sigma\\|_F^2 - 2 \\mathbb{E}[\\langle F - \\Sigma, \\hat{\\Sigma} - \\Sigma \\rangle] + \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2] = \\|F-\\Sigma\\|_F^2 + \\mathbb{E}[\\|\\hat{\\Sigma}-\\Sigma\\|_F^2]$.\n\nNext, we derive a data-driven estimator $\\hat{\\delta}$ by finding consistent estimators for the numerator and denominator of $\\delta^*$.\nLet's define $N_{pop} = \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2]$ and $D_{total} = \\|F - \\Sigma\\|_F^2 + \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2]$. Thus, $\\delta^* = \\frac{N_{pop}}{D_{total}}$.\n\nTo estimate $N_{pop}$, we use the i.i.d. property of the data $\\{x_t\\}_{t=1}^n$. Let $Z_t = x_t x_t^{\\top} - \\Sigma$. Since $\\mathbb{E}[x_t x_t^{\\top}] = \\Sigma$, we have $\\mathbb{E}[Z_t] = 0$.\n$$\nN_{pop} = \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2] = \\mathbb{E}\\left[ \\left\\| \\frac{1}{n} \\sum_{t=1}^n x_t x_t^{\\top} - \\Sigma \\right\\|_F^2 \\right] = \\mathbb{E}\\left[ \\left\\| \\frac{1}{n} \\sum_{t=1}^n (x_t x_t^{\\top} - \\Sigma) \\right\\|_F^2 \\right] = \\mathbb{E}\\left[ \\left\\| \\frac{1}{n} \\sum_{t=1}^n Z_t \\right\\|_F^2 \\right]\n$$\nUsing the inner product definition of the norm and independence of $Z_t, Z_u$ for $t \\neq u$:\n$$\nN_{pop} = \\frac{1}{n^2} \\mathbb{E}\\left[ \\left\\langle \\sum_t Z_t, \\sum_u Z_u \\right\\rangle \\right] = \\frac{1}{n^2} \\sum_{t,u} \\mathbb{E}[\\langle Z_t, Z_u \\rangle] = \\frac{1}{n^2} \\sum_t \\mathbb{E}[\\|Z_t\\|_F^2] = \\frac{n}{n^2} \\mathbb{E}[\\|x_t x_t^{\\top} - \\Sigma\\|_F^2] = \\frac{1}{n} \\mathbb{E}[\\|x_t x_t^{\\top} - \\Sigma\\|_F^2]\n$$\nA consistent estimator for $\\mathbb{E}[\\|x_t x_t^{\\top} - \\Sigma\\|_F^2]$ is its sample average, where we replace the unknown $\\Sigma$ with its consistent estimator $\\hat{\\Sigma}$: $\\frac{1}{n} \\sum_{t=1}^n \\|x_t x_t^{\\top} - \\hat{\\Sigma}\\|_F^2$.\nTherefore, a consistent estimator for $N_{pop}$ is:\n$$\n\\hat{N}_{pop} = \\frac{1}{n} \\left( \\frac{1}{n} \\sum_{t=1}^n \\|x_t x_t^{\\top} - \\hat{\\Sigma}\\|_F^2 \\right) = \\frac{1}{n^2} \\sum_{t=1}^n \\|x_t x_t^{\\top} - \\hat{\\Sigma}\\|_F^2\n$$\n\nTo estimate the denominator $D_{total} = \\|F - \\Sigma\\|_F^2 + N_{pop}$, we consider the quantity $\\|F - \\hat{\\Sigma}\\|_F^2$. Its expectation is:\n$$\n\\mathbb{E}[\\|F - \\hat{\\Sigma}\\|_F^2] = \\mathbb{E}[\\|(F - \\Sigma) - (\\hat{\\Sigma} - \\Sigma)\\|_F^2] = \\mathbb{E}[\\|F - \\Sigma\\|_F^2 - 2\\langle F - \\Sigma, \\hat{\\Sigma} - \\Sigma \\rangle + \\|\\hat{\\Sigma} - \\Sigma\\|_F^2]\n$$\n$$\n\\mathbb{E}[\\|F - \\hat{\\Sigma}\\|_F^2] = \\|F - \\Sigma\\|_F^2 - 0 + \\mathbb{E}[\\|\\hat{\\Sigma} - \\Sigma\\|_F^2] = D_{pop} + N_{pop} = D_{total}\n$$\nThis shows that $\\|F - \\hat{\\Sigma}\\|_F^2$ is an unbiased estimator for the entire denominator $D_{total}$.\nSo, we define our estimator $\\hat{D}_{total} = \\|F - \\hat{\\Sigma}\\|_F^2$.\n\nFinally, we construct the data-driven estimator $\\hat{\\delta}$ by taking the ratio of the estimator for the numerator and the estimator for the denominator:\n$$\n\\hat{\\delta} = \\frac{\\hat{N}_{pop}}{\\hat{D}_{total}} = \\frac{\\frac{1}{n^2} \\sum_{t=1}^{n} \\|x_t x_t^{\\top} - \\hat{\\Sigma}\\|_F^2}{\\|F-\\hat{\\Sigma}\\|_F^2}\n$$\nThis expression for $\\hat{\\delta}$ depends only on the data sample (via $x_t$ and $\\hat{\\Sigma} = \\frac{1}{n}X^\\top X$), the chosen target $F$, and the sample size $n$. All quantities are computable from the given information, yielding the final closed-form expression.",
            "answer": "$$\\boxed{\\frac{\\sum_{t=1}^{n} \\|x_t x_t^{\\top} - \\hat{\\Sigma}\\|_F^2}{n^2 \\|F-\\hat{\\Sigma}\\|_F^2}}$$"
        }
    ]
}