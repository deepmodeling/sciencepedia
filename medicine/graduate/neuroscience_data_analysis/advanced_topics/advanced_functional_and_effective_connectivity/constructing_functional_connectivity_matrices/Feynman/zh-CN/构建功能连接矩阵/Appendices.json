{
    "hands_on_practices": [
        {
            "introduction": "在计算功能连接之前，从fMRI时间序列中移除源于非神经活动的噪声至关重要。本练习将深入探讨伪信号回归的数学基础，这是一个标准的数据预处理步骤。通过从第一性原理推导残差算子，您将具体理解头部运动等信号是如何被移除的，从而确保后续的相关性分析能更准确地反映大脑活动。",
            "id": "4147895",
            "problem": "在静息态功能连接分析中，原始的血氧水平依赖（BOLD）时间序列通常在计算区域间相关性之前，通过回归去除头部运动、脑脊液（CSF）和白质（WM）等干扰信号进行预处理。考虑一个具有 $T$ 个时间点的单次会话数据集。令 $X \\in \\mathbb{R}^{T \\times p}$ 表示一个干扰设计矩阵，其列张成干扰子空间，令 $y \\in \\mathbb{R}^{T}$ 表示一个原始的区域BOLD时间序列。残差化是通过拟合一个普通最小二乘模型，并从 $y$ 中减去拟合的干扰分量来执行的。从普通最小二乘法作为残差平方和最小化器的定义以及到子空间的正交投影的定义出发，推导出将 $y$ 映射到其相对于 $X$ 的列空间的回归残差的线性算子，然后将其应用于以下构建功能连接的具体示例。\n\n给定两个区域的 $T=5$ 个时间点的原始BOLD时间序列 $y^{(1)}$ 和 $y^{(2)}$：\n$$\ny^{(1)} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 5 \\\\ 7 \\end{pmatrix}, \n\\quad\ny^{(2)} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 4 \\\\ 3 \\\\ 6 \\end{pmatrix}.\n$$\n对于干扰回归，使用一个双回归量设计矩阵 $X = [x_{0}, x_{1}] \\in \\mathbb{R}^{5 \\times 2}$，其列为\n$$\nx_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\quad \\text{(截距项)}, \n\\qquad\nx_{1} = \\begin{pmatrix} -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix} \\quad \\text{(一个捕获脑脊液和白质生理学及头部运动的复合干扰项，已标准化至零均值)}。\n$$\n仅使用普通最小二乘法和正交投影的基本原理，计算通过将 $y^{(1)}$ 和 $y^{(2)}$ 对 $X$ 的列进行回归得到的两个区域的残差化时间序列之间的皮尔逊相关系数。将最终答案表示为一个不带单位的单个精确简化表达式。不要四舍五入。",
            "solution": "该问题是有效的，因为它在科学上基于神经科学数据分析中使用的标准统计方法（普通最小二乘法，功能连接），问题是适定的且有唯一解，并且表述客观，提供了所有必要的数据。\n\n第一步是推导将原始时间序列向量 $y \\in \\mathbb{R}^{T}$ 映射到其相对于设计矩阵 $X \\in \\mathbb{R}^{T \\times p}$ 的列空间的回归残差的线性算子。普通最小二乘（OLS）模型为 $y = X\\beta + e$，其中 $\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$e \\in \\mathbb{R}^{T}$ 是残差向量。OLS旨在找到最小化残差平方和的估计值 $\\hat{\\beta}$，即残差向量 $e$ 的欧几里得范数的平方：\n$$S(\\beta) = \\|e\\|^2 = \\|y - X\\beta\\|^2 = (y - X\\beta)^T(y - X\\beta)$$\n展开此表达式得到：\n$$S(\\beta) = y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta$$\n由于 $\\beta^T X^T y$ 是一个标量，它等于其转置 $(y^T X\\beta)^T$，即 $y^T X\\beta$。因此，我们可以写成：\n$$S(\\beta) = y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta$$\n为了找到最小值，我们计算 $S(\\beta)$ 关于 $\\beta$ 的梯度并将其设为零：\n$$\\frac{\\partial S}{\\partial \\beta} = -2X^T y + 2X^T X \\beta = 0$$\n这得到了正规方程：\n$$X^T X \\hat{\\beta} = X^T y$$\n假设 $X$ 的列是线性无关的，则矩阵 $X^T X$ 是可逆的。那么系数的OLS估计值为：\n$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\n拟合值向量 $\\hat{y}$ 是 $y$ 在 $X$ 的列空间上的投影。它由下式给出：\n$$\\hat{y} = X\\hat{\\beta} = X(X^T X)^{-1} X^T y$$\n矩阵 $P = X(X^T X)^{-1} X^T$ 是到 $X$ 的列空间上的正交投影矩阵。回归残差是原始数据与拟合值之间的差：\n$$e = y - \\hat{y} = y - Py = (I - P)y$$\n因此，将 $y$ 映射到其残差的线性算子是矩阵 $M = I - P = I - X(X^T X)^{-1} X^T$。这个算子本身是到与 $X$ 的列空间正交的子空间上的一个正交投影矩阵。\n\n现在，我们将此应用于给定的数据。给定 $T=5$ 和设计矩阵 $X = [x_0, x_1]$：\n$$X = \\begin{pmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix}$$\n首先，我们计算 $X^T X$：\n$$X^T X = \\begin{pmatrix} 1  1  1  1  1 \\\\ -2  -1  0  1  2 \\end{pmatrix} \\begin{pmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} = \\begin{pmatrix} 5  0 \\\\ 0  10 \\end{pmatrix}$$\n非对角元素为零，这证实了列 $x_0$ 和 $x_1$ 是正交的。这简化了计算。其逆矩阵为：\n$$(X^T X)^{-1} = \\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  \\frac{1}{10} \\end{pmatrix}$$\n我们需要求出 $y^{(1)}$ 和 $y^{(2)}$ 的残差。让我们将它们表示为 $e^{(1)}$ 和 $e^{(2)}$。\n\n对于 $y^{(1)} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 5 \\\\ 7 \\end{pmatrix}$：\n我们首先计算 $X^T y^{(1)}$：\n$$X^T y^{(1)} = \\begin{pmatrix} 1  1  1  1  1 \\\\ -2  -1  0  1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 5 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1+3+2+5+7 \\\\ -2-3+0+5+14 \\end{pmatrix} = \\begin{pmatrix} 18 \\\\ 14 \\end{pmatrix}$$\n现在我们求系数 $\\hat{\\beta}^{(1)}$：\n$$\\hat{\\beta}^{(1)} = (X^T X)^{-1} X^T y^{(1)} = \\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 18 \\\\ 14 \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{5} \\\\ \\frac{14}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{5} \\\\ \\frac{7}{5} \\end{pmatrix}$$\n拟合值为 $\\hat{y}^{(1)} = X\\hat{\\beta}^{(1)}$：\n$$\\hat{y}^{(1)} = \\begin{pmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} \\frac{18}{5} \\\\ \\frac{7}{5} \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 18 - 14 \\\\ 18 - 7 \\\\ 18 - 0 \\\\ 18 + 7 \\\\ 18 + 14 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 4 \\\\ 11 \\\\ 18 \\\\ 25 \\\\ 32 \\end{pmatrix}$$\n残差 $e^{(1)}$ 为 $y^{(1)} - \\hat{y}^{(1)}$：\n$$e^{(1)} = \\frac{1}{5} \\begin{pmatrix} 5 \\\\ 15 \\\\ 10 \\\\ 25 \\\\ 35 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 4 \\\\ 11 \\\\ 18 \\\\ 25 \\\\ 32 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 1 \\\\ 4 \\\\ -8 \\\\ 0 \\\\ 3 \\end{pmatrix}$$\n\n对于 $y^{(2)} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 4 \\\\ 3 \\\\ 6 \\end{pmatrix}$：\n我们计算 $X^T y^{(2)}$：\n$$X^T y^{(2)} = \\begin{pmatrix} 1  1  1  1  1 \\\\ -2  -1  0  1  2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ 4 \\\\ 3 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 2+1+4+3+6 \\\\ -4-1+0+3+12 \\end{pmatrix} = \\begin{pmatrix} 16 \\\\ 10 \\end{pmatrix}$$\n系数 $\\hat{\\beta}^{(2)}$ 为：\n$$\\hat{\\beta}^{(2)} = (X^T X)^{-1} X^T y^{(2)} = \\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 16 \\\\ 10 \\end{pmatrix} = \\begin{pmatrix} \\frac{16}{5} \\\\ \\frac{10}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{16}{5} \\\\ 1 \\end{pmatrix}$$\n拟合值为 $\\hat{y}^{(2)} = X\\hat{\\beta}^{(2)}$：\n$$\\hat{y}^{(2)} = \\begin{pmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} \\frac{16}{5} \\\\ 1 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 16 - 10 \\\\ 16 - 5 \\\\ 16 - 0 \\\\ 16 + 5 \\\\ 16 + 10 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 6 \\\\ 11 \\\\ 16 \\\\ 21 \\\\ 26 \\end{pmatrix}$$\n残差 $e^{(2)}$ 为 $y^{(2)} - \\hat{y}^{(2)}$：\n$$e^{(2)} = \\frac{1}{5} \\begin{pmatrix} 10 \\\\ 5 \\\\ 20 \\\\ 15 \\\\ 30 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 6 \\\\ 11 \\\\ 16 \\\\ 21 \\\\ 26 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 4 \\\\ -6 \\\\ 4 \\\\ -6 \\\\ 4 \\end{pmatrix}$$\n\n最后，我们计算 $e^{(1)}$ 和 $e^{(2)}$ 之间的皮尔逊相关系数 $r$。由于回归包含了截距项（$x_0$），两个时间序列的残差均值为零。这使得皮尔遜相关公式简化为两个向量之间夹角的余弦：\n$$r = \\frac{ \\sum_{i=1}^T e_i^{(1)} e_i^{(2)} }{ \\sqrt{\\sum_{i=1}^T (e_i^{(1)})^2} \\sqrt{\\sum_{i=1}^T (e_i^{(2)})^2} } = \\frac{e^{(1) T} e^{(2)}}{\\|e^{(1)}\\| \\|e^{(2)}\\|}$$\n我们计算点积 $e^{(1) T} e^{(2)}$：\n$$e^{(1) T} e^{(2)} = \\left(\\frac{1}{5}\\right)^2 \\begin{pmatrix} 1  4  -8  0  3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -6 \\\\ 4 \\\\ -6 \\\\ 4 \\end{pmatrix} = \\frac{1}{25}(1(4) + 4(-6) + (-8)(4) + 0(-6) + 3(4))$$\n$$e^{(1) T} e^{(2)} = \\frac{1}{25}(4 - 24 - 32 + 0 + 12) = \\frac{1}{25}(16 - 56) = \\frac{-40}{25} = -\\frac{8}{5}$$\n接下来，我们计算残差向量的范数的平方：\n$$\\|e^{(1)}\\|^2 = \\left(\\frac{1}{5}\\right)^2 (1^2 + 4^2 + (-8)^2 + 0^2 + 3^2) = \\frac{1}{25}(1+16+64+0+9) = \\frac{90}{25} = \\frac{18}{5}$$\n$$\\|e^{(2)}\\|^2 = \\left(\\frac{1}{5}\\right)^2 (4^2 + (-6)^2 + 4^2 + (-6)^2 + 4^2) = \\frac{1}{25}(16+36+16+36+16) = \\frac{1}{25}(48+72) = \\frac{120}{25} = \\frac{24}{5}$$\n现在我们组合出相关系数：\n$$r = \\frac{-\\frac{8}{5}}{\\sqrt{\\frac{18}{5}} \\sqrt{\\frac{24}{5}}} = \\frac{-\\frac{8}{5}}{\\sqrt{\\frac{18 \\times 24}{25}}} = \\frac{-\\frac{8}{5}}{\\frac{\\sqrt{432}}{5}} = \\frac{-8}{\\sqrt{432}}$$\n为了化简平方根，我们分解 $432$：$432 = 144 \\times 3 = 12^2 \\times 3$。所以，$\\sqrt{432} = 12\\sqrt{3}$。\n$$r = \\frac{-8}{12\\sqrt{3}} = \\frac{-2}{3\\sqrt{3}}$$\n对分母进行有理化，得到最终答案：\n$$r = \\frac{-2\\sqrt{3}}{3\\sqrt{3}\\sqrt{3}} = \\frac{-2\\sqrt{3}}{9}$$\n残差化时间序列之间的皮尔逊相关系数为 $-\\frac{2\\sqrt{3}}{9}$。",
            "answer": "$$\\boxed{-\\frac{2\\sqrt{3}}{9}}$$"
        },
        {
            "introduction": "两个脑区之间的简单相关性并不保证它们之间存在直接的功能联系，因为这种相关性可能由第三个脑区的共同输入所引起。本练习提供了一个清晰、可计算的例子，以阐明这一关键的混淆概念。通过比较全相关和偏相关，您将学会如何区分虚假连接和更直接的关系，这是准确解释功能连接矩阵的一项核心技能。",
            "id": "4147921",
            "problem": "考虑一个功能性神经影像实验中的三个感兴趣区域 (ROI)，分别标记为 $i$、$j$ 和 $k$。设混淆区域 $k$ 中的潜在神经信号为零均值高斯随机变量 $X_{k}$，其方差为 $\\operatorname{Var}(X_{k})=1$。设区域 $i$ 和 $j$ 中的观测信号由 $X_{k}$ 和独立加性噪声的线性混合生成，具体为\n$$\nX_{i} = 2 X_{k} + \\epsilon_{i}, \\quad X_{j} = - X_{k} + \\epsilon_{j},\n$$\n其中 $\\epsilon_{i}$ 和 $\\epsilon_{j}$ 是零均值高斯噪声，它们相互独立，并且也独立于 $X_{k}$，其方差分别为 $\\operatorname{Var}(\\epsilon_{i}) = 1$ 和 $\\operatorname{Var}(\\epsilon_{j}) = 1$。假设所有变量服从联合高斯分布，并考虑单一时间点的总体协方差。\n\n使用第一性原理——即联合高斯变量的协方差和相关性的定义，以及多元正态模型中条件独立性与偏相关性之间的基本关系——推导 $X_{i}$ 和 $X_{j}$ 之间的总体全相关性，并解释为什么在基于相关性的功能连接矩阵下它表明存在一种连接。然后，构建 $X_{i}$ 和 $X_{j}$ 之间移除了混淆变量 $X_{k}$ 影响的偏相关性。精确计算这个偏相关性。\n\n你的最终答案必须是单个实数值，该值等于在控制了 $X_{k}$ 之后 $X_{i}$ 和 $X_{j}$ 之间的偏相关性，记为 $\\rho_{ij \\cdot k}$。精确表示结果；无需四舍五入。相关性是无量纲的。",
            "solution": "问题要求计算两个观测信号 $X_i$ 和 $X_j$ 之间的全相关性和偏相关性，这两个信号受到一个共同的潜在信号 $X_k$ 的影响。分析将基于针对联合高斯变量的统计学第一性原理。\n\n首先，我们基于给定的生成模型来确定观测信号 $X_i$ 和 $X_j$ 的统计特性。\n信号定义如下：\n$$\nX_{i} = 2 X_{k} + \\epsilon_{i}\n$$\n$$\nX_{j} = - X_{k} + \\epsilon_{j}\n$$\n所有变量 $X_k$、$\\epsilon_i$ 和 $\\epsilon_j$ 都被给定为零均值高斯随机变量。根据期望的线性性质，$X_i$ 和 $X_j$ 的均值也为零：\n$$\nE[X_i] = E[2 X_k + \\epsilon_i] = 2 E[X_k] + E[\\epsilon_i] = 2(0) + 0 = 0\n$$\n$$\nE[X_j] = E[-X_k + \\epsilon_j] = -E[X_k] + E[\\epsilon_j] = -0 + 0 = 0\n$$\n\n观测信号的方差通过以下性质计算：对于独立随机变量 $A$ 和 $B$，$\\operatorname{Var}(aA + bB) = a^2\\operatorname{Var}(A) + b^2\\operatorname{Var}(B)$。给定 $X_k$ 独立于 $\\epsilon_i$ 和 $\\epsilon_j$。\n$$\n\\operatorname{Var}(X_i) = \\operatorname{Var}(2 X_k + \\epsilon_i) = 2^2 \\operatorname{Var}(X_k) + \\operatorname{Var}(\\epsilon_i) = 4(1) + 1 = 5\n$$\n$$\n\\operatorname{Var}(X_j) = \\operatorname{Var}(- X_k + \\epsilon_j) = (-1)^2 \\operatorname{Var}(X_k) + \\operatorname{Var}(\\epsilon_j) = 1(1) + 1 = 2\n$$\n标准差为 $\\sigma_i = \\sqrt{\\operatorname{Var}(X_i)} = \\sqrt{5}$ 和 $\\sigma_j = \\sqrt{\\operatorname{Var}(X_j)} = \\sqrt{2}$。\n\n接下来，我们计算 $X_i$ 和 $X_j$ 之间的全相关性，记为 $\\rho_{ij}$。这需要协方差 $\\operatorname{Cov}(X_i, X_j)$。使用协方差算子的双线性性质：\n$$\n\\operatorname{Cov}(X_i, X_j) = \\operatorname{Cov}(2 X_k + \\epsilon_i, -X_k + \\epsilon_j)\n$$\n$$\n= \\operatorname{Cov}(2X_k, -X_k) + \\operatorname{Cov}(2X_k, \\epsilon_j) + \\operatorname{Cov}(\\epsilon_i, -X_k) + \\operatorname{Cov}(\\epsilon_i, \\epsilon_j)\n$$\n鉴于 $\\epsilon_i$、$\\epsilon_j$ 和 $X_k$ 相互独立，不同变量对之间的协方差为零：$\\operatorname{Cov}(X_k, \\epsilon_j)=0$、$\\operatorname{Cov}(\\epsilon_i, X_k)=0$ 和 $\\operatorname{Cov}(\\epsilon_i, \\epsilon_j)=0$。该表达式简化为：\n$$\n\\operatorname{Cov}(X_i, X_j) = \\operatorname{Cov}(2X_k, -X_k) = 2(-1)\\operatorname{Cov}(X_k, X_k) = -2\\operatorname{Var}(X_k)\n$$\n由于 $\\operatorname{Var}(X_k)=1$，我们有：\n$$\n\\operatorname{Cov}(X_i, X_j) = -2\n$$\n全相关系数 $\\rho_{ij}$ 是协方差通过标准差的乘积进行归一化：\n$$\n\\rho_{ij} = \\frac{\\operatorname{Cov}(X_i, X_j)}{\\sqrt{\\operatorname{Var}(X_i)\\operatorname{Var}(X_j)}} = \\frac{-2}{\\sqrt{5 \\cdot 2}} = \\frac{-2}{\\sqrt{10}}\n$$\n这是一个非零的、中等强度的负相关（$\\rho_{ij} \\approx -0.632$）。在一个基于相关性的标准功能连接分析中，这一发现将表明 ROI $i$ 和 $j$ 之间存在显著的功能关系。然而，生成模型揭示了，除了对 $X_k$ 的共同依赖外，$X_i$ 和 $X_j$ 之间没有直接的因果联系或共享方差。观测到的相关性完全是由来自混淆区域 $k$ 的共同输入所引起的。\n\n为了正确评估 $X_i$ 和 $X_j$ 之间的关系，我们必须考虑 $X_k$ 的影响。在多元正态模型中，这正是偏相关性 $\\rho_{ij \\cdot k}$ 的目的。偏相关性测量的是在移除了 $X_k$ 的线性效应后，$X_i$ 和 $X_j$ 之间的相关性。对于联合高斯变量，$\\rho_{ij \\cdot k}$ 从根本上被定义为 $X_i$ 和 $X_j$ 对 $X_k$ 进行线性回归后，两者残差之间的相关性。\n\n令 $X_i$ 对 $X_k$ 回归后的残差为 $R_i = X_i - E[X_i | X_k]$。\n令 $X_j$ 对 $X_k$ 回归后的残差为 $R_j = X_j - E[X_j | X_k]$。\n对于给定的线性高斯模型，条件期望为：\n$$\nE[X_i | X_k] = E[2X_k + \\epsilon_i | X_k] = 2E[X_k|X_k] + E[\\epsilon_i|X_k] = 2X_k + 0 = 2X_k\n$$\n最后一步成立是因为 $\\epsilon_i$ 独立于 $X_k$，所以 $E[\\epsilon_i|X_k] = E[\\epsilon_i] = 0$。\n类似地，\n$$\nE[X_j | X_k] = E[-X_k + \\epsilon_j | X_k] = -E[X_k|X_k] + E[\\epsilon_j|X_k] = -X_k + 0 = -X_k\n$$\n因此，残差为：\n$$\nR_i = X_i - 2X_k = (2X_k + \\epsilon_i) - 2X_k = \\epsilon_i\n$$\n$$\nR_j = X_j - (-X_k) = (-X_k + \\epsilon_j) + X_k = \\epsilon_j\n$$\n偏相关性是这些残差之间的相关性：\n$$\n\\rho_{ij \\cdot k} = \\operatorname{Corr}(R_i, R_j) = \\operatorname{Corr}(\\epsilon_i, \\epsilon_j)\n$$\n问题陈述了噪声项 $\\epsilon_i$ 和 $\\epsilon_j$ 是相互独立的。对于独立的变量，协方差为零，因此相关性也为零。\n$$\n\\operatorname{Cov}(\\epsilon_i, \\epsilon_j) = 0\n$$\n因此，\n$$\n\\rho_{ij \\cdot k} = \\frac{\\operatorname{Cov}(\\epsilon_i, \\epsilon_j)}{\\sqrt{\\operatorname{Var}(\\epsilon_i)\\operatorname{Var}(\\epsilon_j)}} = \\frac{0}{\\sqrt{1 \\cdot 1}} = 0\n$$\n在控制了 $X_k$ 之后，$X_i$ 和 $X_j$ 之间的偏相关性恰好为零。这个结果正确地反映了模型的底层结构：区域 $i$ 和 $j$ 之间没有直接的联系。它们的全相关性是虚假的，完全由区域 $k$ 的共同影响所导致。这证明了在功能连接研究中考虑混淆变量的关键重要性。$\\rho_{ij \\cdot k}$ 的值就是所要求的单个实数值。",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "构建功能连接矩阵后，研究人员通常会通过设定阈值将其简化为二值图，但这可能会丢弃关于连接强度的宝贵信息。本计算练习将引导您完成从生成模拟数据到比较加权图和二值图的网络级指标（如聚类系数和特征路径长度）的整个分析流程。这种动手实践将突显方法学选择对脑网络拓扑表征的深远影响。",
            "id": "4147918",
            "problem": "您的任务是根据合成的多变量时间序列构建功能连接矩阵，并评估加权连接的二值化对两个规范图度量（聚类系数和特征路径长度）的影响。您必须生成一个单一的可运行程序，为指定的测试套件计算所需的量，并以文末描述的精确格式打印最终结果。所有计算必须源自图论和统计学中的核心定义和经过充分检验的公式。\n\n本问题的基本基础如下。您将从一个零均值多元正态分布 $\\mathcal{N}(0, \\Sigma)$ 中生成合成的多变量时间序列 $X \\in \\mathbb{R}^{T \\times N}$，其协方差矩阵 $\\Sigma \\in \\mathbb{R}^{N \\times N}$ 在每个测试用例中指定。从 $X$ 中，您将计算每对节点时间序列之间的皮尔逊相关系数 (PCC)，以获得一个相关矩阵 $R \\in \\mathbb{R}^{N \\times N}$，其中每个条目 $r_{ij}$ 是节点 $i$ 和节点 $j$ 之间的 PCC。您必须使用 $w_{ij} = |r_{ij}|$ (当 $i \\neq j$) 和 $w_{ii} = 0$ (对所有 $i$) 构建一个加权无向功能连接邻接矩阵 $W \\in \\mathbb{R}^{N \\times N}$。\n\n为了评估二值化的效果，使用一个阈值 $\\tau > 0$ 定义一个二值邻接矩阵 $B \\in \\{0,1\\}^{N \\times N}$，使得如果 $w_{ij} \\ge \\tau$ 则 $b_{ij} = 1$，否则 $b_{ij} = 0$，并设 $b_{ii} = 0$。对于 $B$，所有图均假定为无向无权图；对于 $W$，则假定为具有正权重的无向图。\n\n您将计算以下图度量：\n- 二值图的逐节点聚类系数。节点 $i$ 的聚类系数是与节点 $i$ 相关的三角形数量与可能的三角形数量之比，全局聚类系数是所有节点的平均值。对于度小于 $2$ 的节点，聚类系数定义为 $0$。\n- 使用 Onnela 公式的加权图的逐节点加权聚类系数：设 $\\hat{W}$ 为权重归一化邻接矩阵，其中 $\\hat{w}_{ij} = \\frac{w_{ij}}{\\max_{p,q} w_{pq}}$。节点 $i$ 的加权聚类系数是与节点 $i$ 相邻的三角形边权重的几何平均值的平均值，并由潜在邻居对的数量进行归一化，全局系数是所有节点的平均值。对于度小于 $2$ 的节点，该系数定义为 $0$。\n- 二值图的特征路径长度，定义为所有已连接的不同节点对之间的平均最短路径长度，其中每条边的长度为单位长度。\n- 加权图的特征路径长度，定义为所有不同节点对之间的平均最短路径长度，其中穿越一条边的成本为 $c_{ij} = \\frac{1}{w_{ij}}$ (当 $w_{ij} > 0$)。如果在二值图下没有任何节点对是连接的，则将特征路径长度定义为 $\\mathrm{NaN}$。\n\n对于每个测试用例，您将计算二值化对这些度量的影响，即以下差值\n$$\n\\Delta C = \\bar{C}_B - \\bar{C}_W, \\quad \\Delta L = L_B - L_W,\n$$\n其中 $\\bar{C}_B$ 是二值图的全局聚类系数，$\\bar{C}_W$ 是加权图的全局加权聚类系数，$L_B$ 是二值特征路径长度，$L_W$ 是加权特征路径长度。\n\n所有量必须使用以下核心定义进行计算：\n- 两个实值序列 $x, y \\in \\mathbb{R}^{T}$ 之间的皮尔逊相关系数 (PCC) 是\n$$\nr(x,y) = \\frac{\\sum_{t=1}^{T} \\left(x_t - \\bar{x}\\right)\\left(y_t - \\bar{y}\\right)}{\\sqrt{\\sum_{t=1}^{T} \\left(x_t - \\bar{x}\\right)^2} \\sqrt{\\sum_{t=1}^{T} \\left(y_t - \\bar{y}\\right)^2}},\n$$\n其中 $\\bar{x}$ 和 $\\bar{y}$ 是样本均值。\n- 节点 $i$ 的二值聚类系数是\n$$\nC_i = \\frac{\\text{与节点 } i \\text{ 相关的三角形数量}}{\\binom{k_i}{2}},\n$$\n对于度 $k_i \\ge 2$，否则 $C_i = 0$。全局二值聚类系数是\n$$\n\\bar{C}_B = \\frac{1}{N} \\sum_{i=1}^{N} C_i.\n$$\n- 节点 $i$ 的加权聚类系数 (Onnela) 是\n$$\nC_i^w = \\frac{1}{\\binom{k_i}{2}} \\sum_{\\{j,k\\} \\subset \\mathcal{N}(i)} \\left(\\hat{w}_{ij} \\hat{w}_{ik} \\hat{w}_{jk}\\right)^{1/3},\n$$\n对于度 $k_i \\ge 2$，否则 $C_i^w = 0$，其中 $\\mathcal{N}(i)$ 是加权图中节点 $i$ 的邻居集合，$\\hat{w}_{ij}$ 是归一化权重。全局加权聚类系数是\n$$\n\\bar{C}_W = \\frac{1}{N} \\sum_{i=1}^{N} C_i^w.\n$$\n- 具有距离矩阵 $D$ 的图的特征路径长度是\n$$\nL = \\frac{1}{|\\{(i,j): i",
            "solution": "问题已经过分析，被认为是有效的。它具有科学依据，提法明确，客观，并包含了进行求解所需的所有必要信息和定义。所提供的协方差矩阵是正定的，确保可以定义有效的多元正态分布。计算任务已通过明确的数学公式和算法要求清晰地指定。\n\n解决方案通过为每个测试用例实现指定的步骤来进行。\n\n**1. 合成时间序列的生成**\n对于每个测试用例，我们给定了节点数 $N$、时间点数 $T$、一个协方差矩阵 $\\Sigma \\in \\mathbb{R}^{N \\times N}$ 和一个随机数生成器种子 $s$。我们通过从零均值多元正态分布 $X \\sim \\mathcal{N}(0, \\Sigma)$ 中抽取 $T$ 个样本来生成一个多变量时间序列矩阵 $X \\in \\mathbb{R}^{T \\times N}$。使用特定的种子可确保可复现性。\n\n**2. 连接矩阵的构建**\n从时间序列数据 $X$ 中，我们推导出两个功能连接矩阵：一个加权邻接矩阵 $W$ 和一个二值邻接矩阵 $B$。\n\n*   **皮尔逊相关矩阵 ($R$)**: 首先，我们计算 $X$ 中每对时间序列（列）之间的皮尔逊相关系数 (PCC)。两个序列 $x, y \\in \\mathbb{R}^T$ 之间的 PCC 由以下公式给出：\n    $$\n    r(x,y) = \\frac{\\sum_{t=1}^{T} (x_t - \\bar{x})(y_t - \\bar{y})}{\\sqrt{\\sum_{t=1}^{T} (x_t - \\bar{x})^2} \\sqrt{\\sum_{t=1}^{T} (y_t - \\bar{y})^2}}\n    $$\n    这将产生一个 $N \\times N$ 的相关矩阵 $R$，其中 $R_{ij} = r(X_i, X_j)$，$X_i$ 是节点 $i$ 的时间序列。\n\n*   **加权邻接矩阵 ($W$)**: 加权无向邻接矩阵 $W$ 是通过取相关系数的绝对值来构建的：\n    $$\n    w_{ij} = |r_{ij}| \\quad \\text{for } i \\neq j, \\quad \\text{and} \\quad w_{ii} = 0.\n    $$\n\n*   **二值邻接矩阵 ($B$)**: 二值鄰接矩陣 $B$ 是通过对加权矩阵 $W$ 应用阈值 $\\tau$ 获得的：\n    $$\n    b_{ij} = \\begin{cases} 1  \\text{if } w_{ij} \\ge \\tau \\\\ 0  \\text{otherwise} \\end{cases}, \\quad \\text{with } b_{ii} = 0.\n    $$\n\n**3. 图论度量的计算**\n\n**3.1. 聚类系数**\n聚类系数衡量图中节点聚集在一起的趋势。我们计算二值图和加权图的全局聚类系数。\n\n*   **二值全局聚类系数 ($\\bar{C}_B$)**: 对于二值图 $B$ 中的每个节点 $i$，局部聚类系数 $C_i$ 是其邻居节点之间也相互连接的比例。\n    $$\n    C_i = \\frac{\\text{节点 } i \\text{ 周围的三角形数量}}{\\text{节点 } i \\text{ 周围可能的三角形数量}} = \\frac{|\\{(j,k) \\mid b_{ij}=1, b_{ik}=1, b_{jk}=1\\}|}{\\binom{k_i}{2}}\n    $$\n    其中 $k_i$ 是节点 $i$ 的度 ($k_i = \\sum_j b_{ij}$)。如果 $k_i  2$，$C_i$ 定义为 $0$。全局系数是所有节点的平均值：$\\bar{C}_B = \\frac{1}{N} \\sum_{i=1}^{N} C_i$。\n\n*   **加权全局聚类系数 ($\\bar{C}_W$)**: 我们使用 Onnela 公式。首先，权重矩阵 $W$ 通过其最大条目进行归一化：$\\hat{w}_{ij} = w_{ij} / \\max_{p,q} w_{pq}$。节点 $i$ 的加权聚类系数为：\n    $$\n    C_i^w = \\frac{1}{\\binom{k_i}{2}} \\sum_{\\{j,k\\} \\subset \\mathcal{N}(i)} \\left(\\hat{w}_{ij} \\hat{w}_{ik} \\hat{w}_{jk}\\right)^{1/3}\n    $$\n    这里，$\\mathcal{N}(i)$ 是节点 $i$ 的邻居集合（$w_{ij}>0$ 的节点 $j$），$k_i = |\\mathcal{N}(i)|$。如果 $k_i  2$，$C_i^w=0$。全局系数是平均值：$\\bar{C}_W = \\frac{1}{N} \\sum_{i=1}^{N} C_i^w$。\n\n**3.2. 特征路径长度**\n特征路径长度是衡量网络中节点之间平均分离程度的指标。我们被要求使用 Floyd-Warshall 算法计算所有节点对之间的最短路径。\n\n*   **Floyd-Warshall 算法**: 给定一个初始距离矩阵 `Dist`，其中 `Dist`$[i][j]$ 是节点 $i$ 和 $j$ 之间的直接距离（或成本）（如果未连接则为`无穷大`），该算法通过迭代地将每个节点 $k$ 视为任意两个节点 $i$ 和 $j$ 之间路径上的中间点来找到所有节点对的最短路径：\n    `for k from 1 to N: for i from 1 to N: for j from 1 to N: Dist[i][j] = min(Dist[i][j], Dist[i][k] + Dist[k][j])`\n\n*   **二值特征路径长度 ($L_B$)**: 我们首先构建一个距离矩阵，其中 $B$ 中每条边的长度为 $1$。对于任意一对 $(i,j)$，如果 $b_{ij}=1$ 则 $d_{ij}=1$，否则 $d_{ij}=\\infty$，并设 $d_{ii}=0$。运行 Floyd-Warshall 算法后，我们得到最短路径长度矩阵。$L_B$ 是不同节点对之间所有有限最短路径长度的平均值。如果任意节点对之间不存在路径，$L_B$ 定义为 `NaN`。\n\n*   **加权特征路径长度 ($L_W$)**: 在加权图 $W$ 中穿越边 $(i, j)$ 的成本定义为其权重的倒数，$c_{ij} = 1/w_{ij}$（对于 $w_{ij} > 0$）。初始距离矩阵用这些成本填充（如果 $w_{ij}>0$ 则 $d_{ij} = c_{ij}$，否则为 $\\infty$，且 $d_{ii}=0$）。运行 Floyd-Warshall 算法后，$L_W$ 是不同节点对之间所有最短路径长度的平均值。由于采样的相关矩阵几乎必然会产生一个全连接的加权图，因此所有节点对都将具有有限的路径长度。\n\n**4. 差值的最终计算**\n最后，对于每个测试用例，我们计算二值化图和加权图度量之间的差值，并将结果四舍五入到 6 位小数：\n$$\n\\Delta C = \\bar{C}_B - \\bar{C}_W\n$$\n$$\n\\Delta L = L_B - L_W\n$$\n这些值根据指定的输出结构进行格式化和打印。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _floyd_warshall(dist_matrix):\n    \"\"\"Computes all-pairs shortest paths using the Floyd-Warshall algorithm.\"\"\"\n    n = dist_matrix.shape[0]\n    sp_matrix = dist_matrix.copy()\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                if sp_matrix[i, k] + sp_matrix[k, j]  sp_matrix[i, j]:\n                    sp_matrix[i, j] = sp_matrix[i, k] + sp_matrix[k, j]\n    return sp_matrix\n\ndef calculate_global_clustering(adj_matrix, is_weighted):\n    \"\"\"\n    Computes the global clustering coefficient for a given adjacency matrix.\n    Handles both binary and weighted (Onnela) cases.\n    \"\"\"\n    n = adj_matrix.shape[0]\n    node_coeffs = []\n    \n    if is_weighted:\n        max_w = np.max(adj_matrix)\n        if max_w == 0:\n            return 0.0\n        w_hat = adj_matrix / max_w\n    \n    for i in range(n):\n        if is_weighted:\n            neighbors = np.where(adj_matrix[i] > 0)[0]\n        else:\n            neighbors = np.where(adj_matrix[i] == 1)[0]\n        \n        k_i = len(neighbors)\n        \n        if k_i  2:\n            node_coeffs.append(0.0)\n            continue\n            \n        num_possible_triangles = k_i * (k_i - 1) / 2.0\n        \n        if is_weighted:\n            triangle_sum = 0.0\n            for j_idx in range(k_i):\n                for k_idx in range(j_idx + 1, k_i):\n                    j, k = neighbors[j_idx], neighbors[k_idx]\n                    triangle_sum += (w_hat[i, j] * w_hat[i, k] * w_hat[j, k])**(1.0/3.0)\n            c_i = triangle_sum / num_possible_triangles if num_possible_triangles > 0 else 0.0\n        else:\n            num_actual_triangles = 0\n            for j_idx in range(k_i):\n                for k_idx in range(j_idx + 1, k_i):\n                    j, k = neighbors[j_idx], neighbors[k_idx]\n                    if adj_matrix[j, k] == 1:\n                        num_actual_triangles += 1\n            c_i = num_actual_triangles / num_possible_triangles if num_possible_triangles > 0 else 0.0\n        \n        node_coeffs.append(c_i)\n        \n    return np.mean(node_coeffs)\n\ndef calculate_char_path_length(adj_matrix, is_weighted):\n    \"\"\"\n    Computes the characteristic path length for a given adjacency matrix.\n    Handles both binary and weighted cases.\n    \"\"\"\n    n = adj_matrix.shape[0]\n    dist_matrix = np.full((n, n), np.inf)\n    \n    if is_weighted:\n        non_zero_edges = adj_matrix > 0\n        dist_matrix[non_zero_edges] = 1.0 / adj_matrix[non_zero_edges]\n    else:\n        edges = adj_matrix == 1\n        dist_matrix[edges] = 1.0\n        \n    np.fill_diagonal(dist_matrix, 0)\n    \n    sp_matrix = _floyd_warshall(dist_matrix)\n    \n    # Extract unique paths from the upper triangle\n    paths = sp_matrix[np.triu_indices(n, k=1)]\n    \n    if is_weighted:\n        # Weighted graph from correlation is almost always fully connected\n        return np.mean(paths)\n    else:\n        # For binary graph, only consider connected pairs\n        connected_paths = paths[np.isfinite(paths)]\n        if len(connected_paths) == 0:\n            return np.nan\n        else:\n            return np.mean(connected_paths)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute results.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 6, \"T\": 3000, \"tau\": 0.4, \"seed\": 123,\n            \"Sigma\": np.array([\n                [1, 0.8, 0.8, 0.2, 0.2, 0.2],\n                [0.8, 1, 0.8, 0.2, 0.2, 0.2],\n                [0.8, 0.8, 1, 0.2, 0.2, 0.2],\n                [0.2, 0.2, 0.2, 1, 0.5, 0.5],\n                [0.2, 0.2, 0.2, 0.5, 1, 0.5],\n                [0.2, 0.2, 0.2, 0.5, 0.5, 1]\n            ])\n        },\n        {\n            \"N\": 6, \"T\": 3000, \"tau\": 0.5, \"seed\": 456,\n            \"Sigma\": np.array([\n                [1, 0.6, 0.6, 0.6, 0.6, 0.6],\n                [0.6, 1, 0.6, 0.6, 0.6, 0.6],\n                [0.6, 0.6, 1, 0.6, 0.6, 0.6],\n                [0.6, 0.6, 0.6, 1, 0.6, 0.6],\n                [0.6, 0.6, 0.6, 0.6, 1, 0.6],\n                [0.6, 0.6, 0.6, 0.6, 0.6, 1]\n            ])\n        },\n        {\n            \"N\": 6, \"T\": 3000, \"tau\": 0.2, \"seed\": 789,\n            \"Sigma\": np.array([\n                [1, 0.7, 0.7, 0.7, 0.7, 0.7],\n                [0.7, 1, 0.05, 0.05, 0.05, 0.05],\n                [0.7, 0.05, 1, 0.05, 0.05, 0.05],\n                [0.7, 0.05, 0.05, 1, 0.05, 0.05],\n                [0.7, 0.05, 0.05, 0.05, 1, 0.05],\n                [0.7, 0.05, 0.05, 0.05, 0.05, 1]\n            ])\n        },\n        {\n            \"N\": 5, \"T\": 5000, \"tau\": 0.25, \"seed\": 101112,\n            \"Sigma\": np.array([\n                [1, 0.3, 0.05, 0.05, 0.3],\n                [0.3, 1, 0.3, 0.05, 0.05],\n                [0.05, 0.3, 1, 0.3, 0.05],\n                [0.05, 0.05, 0.3, 1, 0.3],\n                [0.3, 0.05, 0.05, 0.3, 1]\n            ])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N, T, Sigma, tau, seed = case[\"N\"], case[\"T\"], case[\"Sigma\"], case[\"tau\"], case[\"seed\"]\n\n        rng = np.random.default_rng(seed)\n        mean = np.zeros(N)\n        X = rng.multivariate_normal(mean, Sigma, size=T, check_valid='warn')\n\n        R = np.corrcoef(X, rowvar=False)\n        W = np.abs(R)\n        np.fill_diagonal(W, 0)\n        B = (W >= tau).astype(int)\n\n        C_B = calculate_global_clustering(B, is_weighted=False)\n        C_W = calculate_global_clustering(W, is_weighted=True)\n        L_B = calculate_char_path_length(B, is_weighted=False)\n        L_W = calculate_char_path_length(W, is_weighted=True)\n        \n        delta_C = C_B - C_W\n        delta_L = L_B - L_W\n        \n        all_results.append([delta_C, delta_L])\n\n    formatted_results = []\n    for dc, dl in all_results:\n        dc_str = f\"{dc:.6f}\"\n        dl_str = \"nan\" if np.isnan(dl) else f\"{dl:.6f}\"\n        formatted_results.append(f\"[{dc_str},{dl_str}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}