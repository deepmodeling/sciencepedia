{
    "hands_on_practices": [
        {
            "introduction": "Before launching a full analysis, it's essential to understand the fundamental parameters that define the sliding window scheme. This exercise  focuses on the relationship between total time series length ($T$), window length ($L$), and step size ($S$). Mastering this calculation is the first step toward balancing the trade-offs between temporal resolution, statistical power, and computational load in your own dFC studies.",
            "id": "4193710",
            "problem": "A resting-state functional magnetic resonance imaging (fMRI) analysis aims to estimate dynamic functional connectivity (DFC) between two brain regions by computing a connectivity estimate within successive overlapping temporal windows of fixed length. Consider a univariate time series of length $T$ samples. For sliding-window DFC, define windows as contiguous segments of $L$ samples, with the first window starting at sample index $1$, and subsequent windows starting every $S$ samples thereafter. Only windows that are fully contained within the $T$ samples are retained.\n\nStarting from these definitions, derive an expression for the number of windows $W$ that can be formed as a function of $T$, $L$, and $S$. Then, evaluate $W$ for $T=600$, $L=60$, and $S=30$. Next, discuss how decreasing the stride to $S=10$ (keeping $T$ and $L$ fixed) affects the computational burden of DFC estimation and the temporal correlation (serial dependence) between consecutive windowed connectivity estimates, using first principles about overlap and sampling.\n\nReport your final numerical answer as the number of windows for $T=600$, $L=60$, and $S=30$. No rounding is required, and no units are needed in the final answer.",
            "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and complete. It describes a standard procedure in time series analysis, specifically dynamic functional connectivity (DFC) estimation in neuroscience, and requests a derivation and calculation based on clear, formalizable definitions. The provided parameters are realistic for fMRI data analysis. The problem is valid.\n\nWe begin by deriving a general expression for the number of windows, $W$, as a function of the total time series length, $T$, the window length, $L$, and the stride, $S$.\n\nLet the sample indices of the time series range from $1$ to $T$. A window is defined as a contiguous block of $L$ samples. The first window begins at sample index $1$. Subsequent windows are shifted by a stride of $S$ samples.\n\nLet us index the windows starting with $k=0$ for the first window. The starting sample index, $s_k$, for the $k$-th window can be expressed as:\n$$s_k = 1 + kS$$\nwhere $k$ is an integer $k \\ge 0$.\n\nA window starting at index $s_k$ comprises samples from $s_k$ to $s_k + L - 1$. The problem states that only windows fully contained within the $T$ samples are to be retained. This imposes the following constraint on the end index of any valid window:\n$$s_k + L - 1 \\le T$$\n\nSubstituting the expression for $s_k$ into this inequality, we get:\n$$(1 + kS) + L - 1 \\le T$$\nSimplifying this expression gives:\n$$kS + L \\le T$$\n\nTo find the maximum possible value for the index $k$, which we will denote as $k_{max}$, we solve for $k$:\n$$kS \\le T - L$$\n$$k \\le \\frac{T - L}{S}$$\n\nSince $k$ must be an integer, the maximum valid value for $k$ is the floor of this expression:\n$$k_{max} = \\left\\lfloor \\frac{T - L}{S} \\right\\rfloor$$\n\nThe valid indices for the windows are $k = 0, 1, 2, \\dots, k_{max}$. The total number of windows, $W$, is the count of these possible integer values, which is $k_{max} + 1$.\nTherefore, the general expression for the number of windows is:\n$$W = \\left\\lfloor \\frac{T - L}{S} \\right\\rfloor + 1$$\n\nNow, we evaluate $W$ for the specific parameters given: $T = 600$, $L = 60$, and $S = 30$.\nSubstituting these values into the derived expression:\n$$W = \\left\\lfloor \\frac{600 - 60}{30} \\right\\rfloor + 1$$\n$$W = \\left\\lfloor \\frac{540}{30} \\right\\rfloor + 1$$\n$$W = \\lfloor 18 \\rfloor + 1$$\n$$W = 18 + 1$$\n$$W = 19$$\nThus, for the given parameters, $19$ windows can be formed.\n\nFinally, we discuss the effects of decreasing the stride to $S = 10$, while keeping $T = 600$ and $L = 60$ constant.\n\nFirst, let us calculate the new number of windows, which we denote as $W'$.\n$$W' = \\left\\lfloor \\frac{600 - 60}{10} \\right\\rfloor + 1$$\n$$W' = \\left\\lfloor \\frac{540}{10} \\right\\rfloor + 1$$\n$$W' = \\lfloor 54 \\rfloor + 1$$\n$$W' = 54 + 1 = 55$$\nThe number of windows increases from $19$ to $55$.\n\nEffect on computational burden:\nThe estimation of DFC involves computing a connectivity metric (such as Pearson correlation) for each of the $W$ windows. The total computational cost is therefore approximately proportional to the number of windows. By decreasing $S$ from $30$ to $10$, the number of windows $W$ increases from $19$ to $55$. This leads to a substantial increase in the computational burden, by a factor of $\\frac{55}{19} \\approx 2.89$. From the derived formula, it is evident that $W$ is roughly inversely proportional to $S$ for $T \\gg L$, so a smaller stride results in more windows and a higher computational load.\n\nEffect on temporal correlation between consecutive estimates:\nThe temporal correlation, or serial dependence, between connectivity estimates from consecutive windows is primarily driven by the degree of data overlap between those windows. Let's quantify this overlap. Consider two consecutive windows, indexed by $k$ and $k+1$.\nWindow $k$ spans samples $[s_k, s_k + L - 1]$.\nWindow $k+1$ spans samples $[s_{k+1}, s_{k+1} + L - 1] = [s_k + S, s_k + S + L - 1]$.\n\nThe number of overlapping samples between these two windows is given by the length of the intersection of their sample index sets. Assuming $S < L$ (which is true in both cases), the overlap is given by:\n$$N_{overlap} = (s_k + L - 1) - (s_k + S) + 1 = L - S$$\nThe fractional overlap, relative to the window length $L$, is:\n$$f_{overlap} = \\frac{L - S}{L} = 1 - \\frac{S}{L}$$\n\nFor the initial case, $S=30$ and $L=60$:\n$$f_{overlap} = 1 - \\frac{30}{60} = 1 - 0.5 = 0.5$$\nSo, consecutive windows overlap by $50\\%$.\n\nFor the second case, $S=10$ and $L=60$:\n$$f_{overlap} = 1 - \\frac{10}{60} = 1 - \\frac{1}{6} \\approx 0.833$$\nConsecutive windows now overlap by approximately $83.3\\%$.\n\nDecreasing the stride $S$ significantly increases the overlap between consecutive windows. When connectivity is estimated from two datasets that share a large fraction of their data points, the resulting estimates are expected to be highly correlated. Therefore, from first principles, decreasing the stride from $S=30$ to $S=10$ increases the temporal correlation (serial dependence) of the resulting DFC time series. This creates a smoother, more finely sampled estimate of connectivity dynamics, but at the cost of introducing statistical redundancy, which can complicate subsequent analyses such as identifying significant changes in connectivity.",
            "answer": "$$\\boxed{19}$$"
        },
        {
            "introduction": "Real-world fMRI data is never perfect, with head motion being a primary source of confounding artifacts. A crucial quality control step involves censoring high-motion time points, which has direct consequences for sliding window analysis. This practice problem  provides a hands-on scenario to quantify the impact of motion censoring, challenging you to determine how many windows are contaminated and must be excluded, a vital calculation for ensuring the integrity of your results.",
            "id": "4193717",
            "problem": "A researcher is performing a sliding window analysis to estimate dynamic functional connectivity from a preprocessed resting-state functional magnetic resonance imaging time series. Windows are defined by contiguous runs of volumes, and motion contamination is addressed by censoring volumes that exceed a specified Framewise Displacement (FD) threshold. In this experimental protocol, any window that contains at least one censored volume is considered contaminated and excluded from downstream connectivity estimation.\n\nYou are given the following acquisition and analysis parameters and quality control outcomes:\n- The total number of volumes is $T = 240$.\n- The sliding window length is $L = 30$ volumes.\n- The step size between consecutive window start indices is $S = 5$ volumes.\n- Windows are anchored at start indices $s \\in \\{1, 1+S, 1+2S, \\dots, T-L+1\\}$, and each window spans volumes $\\{s, s+1, \\dots, s+L-1\\}$.\n- The set of censored volumes (due to high instantaneous motion with FD above threshold) is \n$$\\mathcal{C} = \\{14, 15, 16, 45, 90, 91, 92, 93, 94, 95, 120, 121, 150, 151, 152, 153, 154, 155, 200, 201, 202, 203, 204, 205, 230\\}.$$\n\nUsing only these definitions and parameters, compute:\n1. The proportion of windows that are contaminated by motion, defined as the ratio of the number of contaminated windows to the total number of windows. Express this proportion as an exact fraction with no rounding.\n2. The effective number of clean windows available for downstream dynamic analyses, defined as the number of windows that do not intersect $\\mathcal{C}$.\n\nProvide your final answer as two quantities in a single row matrix using LaTeX's `pmatrix` environment, where the first entry is the exact fraction for the contaminated proportion and the second entry is the integer count of clean windows. No rounding is required, and no units should be included in your final answer.",
            "solution": "Dynamic functional connectivity via sliding windows relies on estimating time-varying statistical dependencies within short, contiguous segments of the time series. The foundational elements for this computation are:\n- The definition of a sliding window: a window starting at index $s$ covers the set $\\{s, s+1, \\dots, s+L-1\\}$.\n- The window grid: start indices $s$ are spaced by the step size $S$, beginning at $1$ and ending at $T-L+1$.\n- The contamination rule: a window is contaminated if it contains any censored volume $v \\in \\mathcal{C}$.\n\nFirst, compute the total number of windows. The valid start indices are\n$$s \\in \\{1, 1+S, 1+2S, \\dots, T-L+1\\}.$$\nWith $T=240$, $L=30$, and $S=5$, the largest start index is\n$$T - L + 1 = 240 - 30 + 1 = 211.$$\nThus, the start indices form the arithmetic progression $s_k = 1 + 5k$ for $k = 0, 1, 2, \\dots, K$, where $s_K = 211$. Solving $1 + 5K = 211$ gives\n$$5K = 210 \\Rightarrow K = 42,$$\nso the total number of windows is\n$$N_{\\text{total}} = K + 1 = 43.$$\n\nNext, determine which windows are contaminated. A window starting at $s$ is contaminated if there exists $v \\in \\mathcal{C}$ such that $v \\in \\{s, s+1, \\dots, s+L-1\\}$, which is equivalent to\n$$s \\leq v \\leq s + L - 1.$$\nThis condition can be inverted to a constraint on $s$ for a given $v$:\n$$v - L + 1 \\leq s \\leq v.$$\nFor a block of censored volumes $\\{a, a+1, \\dots, b\\}$, the union of constraints across $v \\in [a,b]$ simplifies to\n$$a - L + 1 \\leq s \\leq b.$$\nWe now partition $\\mathcal{C}$ into contiguous blocks and derive the corresponding start-index contamination intervals, clipping them to the valid start range $[1, T-L+1] = [1, 211]$:\n- Block $\\{14, 15, 16\\}$ yields $[14 - L + 1, 16] = [14 - 29, 16] = [-15, 16]$, which clips to $[1, 16]$.\n- Block $\\{45\\}$ yields $[45 - 29, 45] = [16, 45]$.\n- Block $\\{90, 91, 92, 93, 94, 95\\}$ yields $[90 - 29, 95] = [61, 95]$.\n- Block $\\{120, 121\\}$ yields $[120 - 29, 121] = [91, 121]$.\n- Block $\\{150, 151, 152, 153, 154, 155\\}$ yields $[150 - 29, 155] = [121, 155]$.\n- Block $\\{200, 201, 202, 203, 204, 205\\}$ yields $[200 - 29, 205] = [171, 205]$.\n- Block $\\{230\\}$ yields $[230 - 29, 230] = [201, 230]$, which clips to $[201, 211]$.\n\nForm the union of these intervals, merging overlaps:\n- The intervals $[1, 16]$ and $[16, 45]$ merge to $[1, 45]$.\n- The intervals $[61, 95]$, $[91, 121]$, and $[121, 155]$ merge to $[61, 155]$.\n- The intervals $[171, 205]$ and $[201, 211]$ merge and clip to $[171, 211]$.\n\nThus, the set of contaminated start indices is the union of three disjoint intervals:\n$$[1, 45] \\cup [61, 155] \\cup [171, 211].$$\n\nWe now count how many start indices from the grid $s \\in \\{1, 6, 11, \\dots, 211\\}$ fall inside each interval. Because start indices satisfy $s \\equiv 1 \\pmod{5}$, counting can be done either by listing or by using bounds on $k$ where $s = 1 + 5k$.\n\n- For $[1, 45]$: the allowable $s$ values are $1, 6, 11, 16, 21, 26, 31, 36, 41$, giving\n$$N_{[1,45]} = 9.$$\n\n- For $[61, 155]$: the allowable $s$ values start at $61$ and end at $151$ in steps of $5$, i.e., $61, 66, 71, 76, 81, 86, 91, 96, 101, 106, 111, 116, 121, 126, 131, 136, 141, 146, 151$, giving\n$$N_{[61,155]} = 19.$$\n\n- For $[171, 211]$: the allowable $s$ values start at $171$ and end at $211$ in steps of $5$, i.e., $171, 176, 181, 186, 191, 196, 201, 206, 211$, giving\n$$N_{[171,211]} = 9.$$\n\nSumming these,\n$$N_{\\text{contaminated}} = N_{[1,45]} + N_{[61,155]} + N_{[171,211]} = 9 + 19 + 9 = 37.$$\n\nTherefore, the proportion of contaminated windows is\n$$p_{\\text{contaminated}} = \\frac{N_{\\text{contaminated}}}{N_{\\text{total}}} = \\frac{37}{43}.$$\n\nThe effective number of clean windows available for downstream analyses is\n$$N_{\\text{clean}} = N_{\\text{total}} - N_{\\text{contaminated}} = 43 - 37 = 6.$$\n\nNo rounding is required. The final answer should be reported as a row matrix with the exact fraction followed by the integer:\n$$\\begin{pmatrix} \\frac{37}{43} & 6 \\end{pmatrix}.$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{37}{43} & 6\\end{pmatrix}}$$"
        },
        {
            "introduction": "Observing fluctuations in connectivity is one thing; proving they are statistically significant is another. This comprehensive exercise  guides you through the implementation of a full hypothesis test for dynamic functional connectivity. By creating phase-randomized surrogates, you will construct a null model that preserves the autocorrelation of each signal, allowing you to rigorously test whether the observed connectivity dynamics exceed what is expected by chance.",
            "id": "4193720",
            "problem": "You are given two discrete, real-valued time series $x_t$ and $y_t$ representing neural signals sampled at uniform intervals, each of length $T$. The goal is to assess whether fluctuations in sliding-window Dynamic Functional Connectivity (DFC) between $x_t$ and $y_t$ exceed what is expected under a spectral-constrained null in which each series preserves its power spectrum but lacks genuine time-varying cross-dependencies. Dynamic Functional Connectivity (DFC) is operationalized as the Pearson correlation computed within successive, overlapping windows. The test must be implemented using a phase-randomization surrogate procedure.\n\nFundamental basis to use:\n- The Discrete Fourier Transform (DFT) of a real time series $x_t$ is defined as $$X_k = \\sum_{t=0}^{T-1} x_t e^{-i 2\\pi kt/T}, \\quad k \\in \\{0,1,\\dots,T-1\\}.$$ The magnitude $|X_k|$ determines the power at frequency index $k$. For real signals, the spectrum satisfies Hermitian symmetry.\n- The Pearson correlation within a window of length $W$ starting at index $s$ is $$r_s = \\frac{\\sum_{t=s}^{s+W-1} \\left(x_t - \\bar{x}_s\\right)\\left(y_t - \\bar{y}_s\\right)}{\\sqrt{\\sum_{t=s}^{s+W-1} \\left(x_t - \\bar{x}_s\\right)^2}\\sqrt{\\sum_{t=s}^{s+W-1} \\left(y_t - \\bar{y}_s\\right)^2}},$$ where $\\bar{x}_s$ and $\\bar{y}_s$ are sample means within the window. The Fisher $z$-transform $z_s = \\operatorname{arctanh}(r_s)$ stabilizes correlation variance across windows under stationarity assumptions.\n- The phase-randomization surrogate for a time series $x_t$ is a new series $\\tilde{x}_t$ whose DFT satisfies $|\\tilde{X}_k| = |X_k|$ for all $k$, and whose phases for positive frequency indices are independently and identically distributed uniformly on $[0,2\\pi)$, with phases for negative indices set by Hermitian symmetry to ensure $\\tilde{x}_t$ is real. The direct current (DC) component and the Nyquist component (if $T$ is even) are kept real-valued.\n\nYour program must:\n1. Implement a sliding window analysis that computes $r_s$ for successive windows of length $W$ with a stride (step) of $S$, over all valid starting indices $s \\in \\{0, S, 2S, \\dots\\}$ such that $s+W \\le T$. Transform each $r_s$ into $z_s = \\operatorname{arctanh}(r_s)$, and summarize the fluctuation magnitude by the standard deviation $\\sigma_z$ of $\\{z_s\\}$ across all windows.\n2. Implement a phase-randomization surrogate generator that, given $x_t$, returns $\\tilde{x}_t$ and, given $y_t$, returns $\\tilde{y}_t$, each preserving their respective magnitude spectra $|X_k|$ and $|Y_k|$ while independently randomizing phases (with Hermitian symmetry enforced).\n3. Generate a null distribution of $\\sigma_z$ by computing $\\sigma_z^{(b)}$ for $b \\in \\{1,\\dots,B\\}$ using independently phase-randomized surrogates $(\\tilde{x}_t^{(b)}, \\tilde{y}_t^{(b)})$, and obtain the $0.95$ quantile $q_{0.95}$ of $\\{\\sigma_z^{(b)}\\}_{b=1}^B$.\n4. Return a boolean indicating whether $\\sigma_z$ from the observed data exceeds $q_{0.95}$.\n\nDesign the test suite with synthetic, scientifically plausible scenarios that produce clearly interpretable DFC behavior. For reproducibility, set a specified random seed for each case before generating data and surrogates.\n\nTest suite:\n- Case $1$ (happy path, clear time-varying coupling):\n  - $T = 1024$, $W = 128$, $S = 64$, $B = 200$.\n  - Generate $x_t$ as an Autoregressive of order $1$ (AR($1$)) process $x_t = \\phi_x x_{t-1} + \\epsilon_t$ with $\\phi_x = 0.9$ and innovations $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_x^2)$ chosen to yield unit variance, and similarly $y_t$ with $\\phi_y = 0.9$. Introduce a coupling segment $[256,768)$ by setting $y_t \\leftarrow y_t + c x_t$ for $t \\in [256,768)$ with $c = 0.8$. Use random seed $42$.\n  - Expected outcome: the observed $\\sigma_z$ should exceed $q_{0.95}$, returning boolean $True$.\n- Case $2$ (boundary, stationary independence):\n  - $T = 1024$, $W = 128$, $S = 64$, $B = 200$.\n  - Generate independent AR($1$) processes as in Case $1$ with $\\phi_x = 0.9$, $\\phi_y = 0.9$, but no coupling. Use random seed $123$.\n  - Expected outcome: the observed $\\sigma_z$ should not exceed $q_{0.95}$, returning boolean $False$.\n- Case $3$ (edge, shorter length with narrowband content and brief event):\n  - $T = 400$, $W = 100$, $S = 50$, $B = 200$.\n  - Generate $x_t$ as $x_t = \\sin(2\\pi f t + \\varphi_x) + \\eta_t$ with $f = 0.08$, $\\varphi_x$ uniformly drawn on $[0,2\\pi)$, and $\\eta_t \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma = 0.5$. Generate $y_t$ similarly with an independent phase $\\varphi_y$ and independent noise. Introduce a coupling segment $[150,250)$ by setting $y_t \\leftarrow y_t + c x_t$ for $t \\in [150,250)$ with $c = 1.0$. Use random seed $7$.\n  - Expected outcome: the observed $\\sigma_z$ may exceed $q_{0.95}$ depending on spectral content and event strength; this case tests sensitivity under narrowband dominance.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list of booleans enclosed in square brackets (e.g., $\"[True,False,True]\"$). No additional text should be printed.\n\nAll answers are unitless real numbers or booleans; no physical units or angle units are required. All numeric answers and parameters must be treated as dimensionless.",
            "solution": "The problem requires the implementation of a statistical test to determine if the fluctuations in Dynamic Functional Connectivity (DFC) between two time series, $x_t$ and $y_t$, are statistically significant. The method prescribed is a non-parametric test based on phase-randomized surrogate data, a standard technique in neuroscience for establishing a null hypothesis that preserves the spectral properties (and thus, the autocorrelation structure) of individual signals while destroying their specific temporal cross-correlation.\n\nThe solution is designed based on three core principles: (1) quantifying DFC fluctuation, (2) constructing a spectrally-constrained null distribution, and (3) performing statistical inference via a Monte Carlo simulation.\n\nFirst, we operationalize the DFC and its fluctuation. The dynamic connectivity between the two time series $x_t$ and $y_t$ (each of length $T$) is measured by computing the Pearson correlation coefficient, $r_s$, within a sliding window of a specified length $W$ that moves with a stride $S$. For each window starting at time index $s$, the correlation is calculated as:\n$$r_s = \\frac{\\sum_{t=s}^{s+W-1} (x_t - \\bar{x}_s)(y_t - \\bar{y}_s)}{\\sqrt{\\sum_{t=s}^{s+W-1} (x_t - \\bar{x}_s)^2}\\sqrt{\\sum_{t=s}^{s+W-1} (y_t - \\bar{y}_s)^2}}$$\nwhere $\\bar{x}_s$ and $\\bar{y}_s$ are the means of the respective series within that window. The Pearson correlation coefficient $r$ is bounded between $-1$ and $1$. Its sampling distribution is skewed, especially for values of $r$ near the bounds. To mitigate this and stabilize the variance, we apply the Fisher $z$-transform to each correlation value:\n$$z_s = \\operatorname{arctanh}(r_s) = \\frac{1}{2}\\ln\\left(\\frac{1+r_s}{1-r_s}\\right)$$\nThe resulting time series of $z$-scores, $\\{z_s\\}$, represents the trajectory of connectivity strength over time. The overall magnitude of DFC fluctuation is then quantified by the standard deviation of this series, denoted as $\\sigma_z$. This value, $\\sigma_z^{\\text{obs}}$, is the statistic we seek to test.\n\nSecond, we must establish a null hypothesis. The null hypothesis, $H_0$, posits that the observed DFC fluctuations are no greater than what would be expected from two independent stationary processes that happen to share the same power spectra as the original signals $x_t$ and $y_t$. To generate data under this null hypothesis, we use the phase-randomization surrogate method. For a given time series $x_t$, its surrogate $\\tilde{x}_t$ is constructed to have the same power spectrum but randomized temporal structure. This is achieved in the frequency domain. We compute the Discrete Fourier Transform (DFT) of the signal, $X_k$. The magnitudes $|X_k|$ of the Fourier coefficients, which define the power spectrum, are preserved. However, the phases $\\arg(X_k)$ are replaced with new phases drawn independently from a uniform distribution on $[0, 2\\pi)$. To ensure the resulting surrogate time series $\\tilde{x}_t$ is real-valued, Hermitian symmetry must be enforced on the complex spectrum. Specifically, the phase for frequency $-k$ must be the negative of the phase for frequency $k$. Additionally, the DC component ($k=0$) and, if the series length $T$ is even, the Nyquist component ($k=T/2$) must remain real-valued; their original values are preserved. The new complex spectrum, $\\tilde{X}_k$, is then transformed back to the time domain via the inverse DFT to yield the surrogate signal $\\tilde{x}_t$. This procedure is applied independently to both $x_t$ and $y_t$.\n\nThird, we perform statistical inference. We generate a large number, $B$, of surrogate pairs $(\\tilde{x}_t^{(b)}, \\tilde{y}_t^{(b)})$ for $b=1, \\dots, B$. For each pair, we repeat the entire DFC analysis pipeline to compute a surrogate DFC fluctuation statistic, $\\sigma_z^{(b)}$. This collection of $B$ values, $\\{\\sigma_z^{(b)}\\}$, forms an empirical null distribution. By comparing our observed statistic, $\\sigma_z^{\\text{obs}}$, to this distribution, we can assess its significance. Specifically, we find the $0.95$ quantile of the null distribution, $q_{0.95}$. If $\\sigma_z^{\\text{obs}} > q_{0.95}$, we conclude that the observed DFC fluctuations are significantly larger than expected by chance under the stationary null model, and we reject $H_0$. This corresponds to a one-tailed test with a significance level of $\\alpha=0.05$.\n\nThe implementation involves several functions. A data generation function for each test case creates synthetic signals with known properties (e.g., with or without a period of true time-varying coupling). A function `generate_surrogate` implements the phase randomization using `numpy.fft.rfft` for efficiency with real signals. A function `calculate_dfc_fluctuation` carries out the sliding window correlation analysis, including the Fisher $z$-transform and the final standard deviation calculation, with careful handling of numerical edge cases such as perfect correlations (which would yield `inf` from `arctanh`) or windows with zero variance. A main `run_dfc_test` function orchestrates the entire process: computing the observed $\\sigma_z$, generating the null distribution from $B$ surrogates, and returning the final boolean decision. The specified random seeds ensure reproducibility of both the synthetic data and the surrogate generation process.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import lfilter\n\ndef generate_surrogate(signal: np.ndarray, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generates a phase-randomized surrogate of a real-valued time series.\n\n    The surrogate has the same power spectrum (and autocorrelation) as the\n    original signal but is otherwise temporally decorrelated.\n    \"\"\"\n    T = len(signal)\n    \n    # Compute the Fourier transform of the real signal\n    X = np.fft.rfft(signal)\n    \n    # Preserve the magnitudes\n    magnitudes = np.abs(X)\n    \n    # Generate random phases uniformly from [0, 2*pi)\n    # The number of phases corresponds to the output of rfft\n    random_phases = rng.uniform(0, 2 * np.pi, len(X))\n    \n    # Create the new complex spectrum by combining original magnitudes with random phases\n    surrogate_X = magnitudes * np.exp(1j * random_phases)\n    \n    # Enforce constraints for a real-valued time series:\n    # 1. DC component (k=0) must be real. We preserve its original value.\n    surrogate_X[0] = X[0]\n    \n    # 2. Nyquist component (k=T/2), if T is even, must be real. Preserve original.\n    if T % 2 == 0:\n        surrogate_X[-1] = X[-1]\n        \n    # Perform the inverse Fourier transform to get the surrogate time series\n    surrogate_signal = np.fft.irfft(surrogate_X, n=T)\n    \n    return surrogate_signal\n\ndef calculate_dfc_fluctuation(x: np.ndarray, y: np.ndarray, W: int, S: int) -> float:\n    \"\"\"\n    Calculates the fluctuation of Dynamic Functional Connectivity (DFC).\n\n    DFC is computed using sliding window Pearson correlation, followed by Fisher's\n    z-transform. Fluctuation is the standard deviation of the z-scores.\n    \"\"\"\n    T = len(x)\n    z_scores = []\n    \n    for s in range(0, T - W + 1, S):\n        x_win = x[s : s + W]\n        y_win = y[s : s + W]\n\n        # Handle windows with zero variance\n        if np.std(x_win) == 0 or np.std(y_win) == 0:\n            z_scores.append(0.0)\n            continue\n            \n        r = np.corrcoef(x_win, y_win)[0, 1]\n        \n        # Clip r to avoid inf from arctanh at r = +/- 1\n        r_clipped = np.clip(r, -1.0 + 1e-12, 1.0 - 1e-12)\n        \n        z = np.arctanh(r_clipped)\n        z_scores.append(z)\n        \n    if not z_scores:\n        return 0.0\n\n    sigma_z = np.std(z_scores)\n    return sigma_z\n\ndef run_dfc_test(x: np.ndarray, y: np.ndarray, W: int, S: int, B: int, rng: np.random.Generator) -> bool:\n    \"\"\"\n    Runs the full statistical test for DFC significance.\n    \"\"\"\n    # 1. Compute the observed DFC fluctuation\n    sigma_z_obs = calculate_dfc_fluctuation(x, y, W, S)\n    \n    # 2. Generate the null distribution\n    surrogate_sigma_z = []\n    for _ in range(B):\n        x_surr = generate_surrogate(x, rng)\n        y_surr = generate_surrogate(y, rng)\n        sigma_z_surr = calculate_dfc_fluctuation(x_surr, y_surr, W, S)\n        surrogate_sigma_z.append(sigma_z_surr)\n        \n    # 3. Find the 0.95 quantile of the null distribution\n    q_095 = np.quantile(surrogate_sigma_z, 0.95)\n    \n    # 4. Compare observed statistic to the null distribution threshold\n    return sigma_z_obs > q_095\n\ndef generate_ar1_data(T: int, phi: float, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generates a mean-zero, unit-variance AR(1) process.\n    \"\"\"\n    # Variance of innovation to yield unit variance for the process\n    sigma_eps = np.sqrt(1 - phi**2)\n    \n    # Generate innovations\n    eps = rng.normal(0, sigma_eps, T)\n    \n    # AR(1) filter parameters: a_1*y[n] = b_0*x[n] - a_1*y[n-1]\n    # y[n] - phi*y[n-1] = eps[n], so a=[1, -phi], b=[1]\n    ar_process = lfilter([1], [1, -phi], eps)\n    \n    # Burn-in might be needed for perfect stationarity, but for long T, this is fine.\n    # Normalize to enforce mean-zero, unit-variance\n    ar_process = (ar_process - np.mean(ar_process)) / np.std(ar_process)\n    return ar_process\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, clear time-varying coupling)\n        {'case_id': 1, 'T': 1024, 'W': 128, 'S': 64, 'B': 200, 'seed': 42,\n         'phi': 0.9, 'c': 0.8, 'coupling_interval': (256, 768)},\n        \n        # Case 2 (boundary, stationary independence)\n        {'case_id': 2, 'T': 1024, 'W': 128, 'S': 64, 'B': 200, 'seed': 123,\n         'phi': 0.9, 'c': 0.0, 'coupling_interval': (0, 0)},\n        \n        # Case 3 (edge, shorter length with narrowband content)\n        {'case_id': 3, 'T': 400, 'W': 100, 'S': 50, 'B': 200, 'seed': 7,\n         'f': 0.08, 'noise_sigma': 0.5, 'c': 1.0, 'coupling_interval': (150, 250)}\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        \n        if case['case_id'] in [1, 2]:\n            x = generate_ar1_data(case['T'], case['phi'], rng)\n            y = generate_ar1_data(case['T'], case['phi'], rng)\n            start, end = case['coupling_interval']\n            if end > start:\n                y[start:end] += case['c'] * x[start:end]\n                # Renormalize to not bias variance-based metrics\n                y = (y - np.mean(y)) / np.std(y)\n\n        elif case['case_id'] == 3:\n            t_ax = np.arange(case['T'])\n            \n            phi_x = rng.uniform(0, 2 * np.pi)\n            eta_x = rng.normal(0, case['noise_sigma'], case['T'])\n            x = np.sin(2 * np.pi * case['f'] * t_ax + phi_x) + eta_x\n            \n            phi_y = rng.uniform(0, 2 * np.pi)\n            eta_y = rng.normal(0, case['noise_sigma'], case['T'])\n            y = np.sin(2 * np.pi * case['f'] * t_ax + phi_y) + eta_y\n            \n            start, end = case['coupling_interval']\n            y[start:end] += case['c'] * x[start:end]\n\n            # Normalize after adding coupling\n            x = (x - np.mean(x)) / np.std(x)\n            y = (y - np.mean(y)) / np.std(y)\n\n        # Run the DFC significance test with a new RNG for surrogate generation\n        # as per \"set a specified random seed for each case before ... surrogates\"\n        test_rng = np.random.default_rng(case['seed'])\n        result = run_dfc_test(x, y, case['W'], case['S'], case['B'], test_rng)\n        results.append(result)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}