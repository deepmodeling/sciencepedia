{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any rigorous statistical test is a well-defined model and a precise hypothesis. In connectomics, the General Linear Model (GLM) provides a flexible framework for modeling how edge weights relate to experimental variables like group membership or subject-specific covariates. This first practice  challenges you to construct the design matrix for a typical multi-group study and formulate the exact contrast vector needed to test a specific hypothesis, a critical first step in any NBS analysis.",
            "id": "4181134",
            "problem": "You are analyzing a functional connectome comprising $n$ subjects partitioned into three mutually exclusive groups: Group A, Group B, and Group C. For each subject $i \\in \\{1,\\dots,n\\}$ and each edge $e$ in the connectome, you observe an edge weight $y_{i}^{(e)}$. You plan to use the Network-Based Statistic (NBS) approach to test for group differences at the level of connected edge components using an edgewise General Linear Model (GLM) as the base model for constructing the test statistic at each edge.\n\nStarting from the standard General Linear Model foundation, in which for each edge $e$ the response vector $y^{(e)} \\in \\mathbb{R}^{n}$ satisfies the model\n$$\ny^{(e)} \\;=\\; X \\beta^{(e)} + \\varepsilon^{(e)},\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta^{(e)} \\in \\mathbb{R}^{p}$ is the edge-specific parameter vector, and $\\varepsilon^{(e)} \\in \\mathbb{R}^{n}$ is the error term with zero mean and finite variance, you will incorporate subject-level covariates for age and sex to control for confounding.\n\nConstruct a scientifically valid, full-rank design matrix $X$ for a three-group comparison that includes:\n- An intercept,\n- Group membership encoding for Group A and Group B using binary indicator variables with Group C as the reference category,\n- A mean-centered age covariate,\n- A sex covariate coded as $0$ for female and $1$ for male.\n\nEnsure that your construction avoids perfect multicollinearity and is suitable for edgewise GLM estimation in the context of NBS. Then, based on this $X$, specify a single contrast vector that tests the null hypothesis that the mean edge weight in Group A equals the mean edge weight in Group B, while controlling for age and sex, at each edge.\n\nYour final reported answer must be the contrast vector expressed as a single closed-form analytic expression. If you introduce any numerical constants in the construction, they must be defined symbolically and not instantiated numerically. No rounding is required. Provide the contrast vector as your final answer.",
            "solution": "The problem is valid. It is scientifically grounded in the established principles of the General Linear Model (GLM) as applied to statistical hypothesis testing in neuroscience. The problem is well-posed, objective, and contains all necessary information to construct the required design matrix and contrast vector without ambiguity or contradiction.\n\nThe objective is to construct a contrast vector for testing a specific hypothesis within the framework of a GLM. The model for the observed edge weight $y^{(e)} \\in \\mathbb{R}^n$ at a given edge $e$ is:\n$$\ny^{(e)} = X \\beta^{(e)} + \\varepsilon^{(e)}\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta^{(e)} \\in \\mathbb{R}^p$ is the vector of parameters, and $\\varepsilon^{(e)}$ is the error term.\n\nFirst, we construct the design matrix $X$ according to the specified requirements. The problem states there are three groups (A, B, C), and we need to include an intercept, group indicators for A and B with C as the reference, a mean-centered age covariate, and a sex covariate. This defines a model with $p=5$ parameters. Let us define the columns of the design matrix $X$. For each subject $i \\in \\{1, \\dots, n\\}$, the $i$-th row of $X$, denoted $x_i^T$, is constructed as follows:\n\nLet $G_{iA}$ be an indicator variable such that $G_{iA} = 1$ if subject $i$ is in Group A, and $G_{iA} = 0$ otherwise.\nLet $G_{iB}$ be an indicator variable such that $G_{iB} = 1$ if subject $i$ is in Group B, and $G_{iB} = 0$ otherwise.\nIf subject $i$ is in Group C, then $G_{iA} = 0$ and $G_{iB} = 0$, making Group C the reference category as required.\n\nLet $Age_i$ be the age of subject $i$. The problem specifies a mean-centered age covariate. Let $\\bar{A} = \\frac{1}{n} \\sum_{j=1}^n Age_j$ be the mean age across all subjects. The mean-centered age for subject $i$ is $A_i = Age_i - \\bar{A}$.\n\nLet $S_i$ be the sex covariate for subject $i$, coded as $S_i = 0$ for female and $S_i = 1$ for male.\n\nThe design matrix $X$ is an $n \\times 5$ matrix. The $i$-th row of $X$ is given by the vector $x_i^T$:\n$$\nx_i^T = \\begin{pmatrix} 1 & G_{iA} & G_{iB} & A_i & S_i \\end{pmatrix}\n$$\nThe corresponding parameter vector for edge $e$, $\\beta^{(e)}$, is a column vector in $\\mathbb{R}^5$:\n$$\n\\beta^{(e)} = \\begin{pmatrix} \\beta_0^{(e)} \\\\ \\beta_1^{(e)} \\\\ \\beta_2^{(e)} \\\\ \\beta_3^{(e)} \\\\ \\beta_4^{(e)} \\end{pmatrix}\n$$\nThe expected edge weight for subject $i$ at edge $e$, $E[y_i^{(e)}]$, is given by the model:\n$$\nE[y_i^{(e)}] = \\beta_0^{(e)} \\cdot 1 + \\beta_1^{(e)} G_{iA} + \\beta_2^{(e)} G_{iB} + \\beta_3^{(e)} A_i + \\beta_4^{(e)} S_i\n$$\nThe interpretation of the parameters (omitting the superscript $(e)$ for clarity) is as follows:\n- $\\beta_0$: The mean edge weight for a subject in the reference group (Group C), of average age ($A_i=0$), and of the reference sex (female, $S_i=0$).\n- $\\beta_1$: The difference in mean edge weight between Group A and Group C, controlling for age and sex.\n- $\\beta_2$: The difference in mean edge weight between Group B and Group C, controlling for age and sex.\n- $\\beta_3$: The effect of age on edge weight (slope).\n- $\\beta_4$: The difference in mean edge weight between males and females, controlling for group and age.\n\nNext, we formulate the null hypothesis: the mean edge weight in Group A equals the mean edge weight in Group B, controlling for age and sex. To express this in terms of the model parameters, we write the adjusted mean for each group. The adjusted mean for a group is the expected edge weight for a subject in that group with covariates set to a common reference value (e.g., average age $A_i=0$ and a specific sex $s$).\n\nThe adjusted mean for Group A, $\\mu_A$, is:\n$$\n\\mu_A = E[y^{(e)} | \\text{Group A, avg age, sex } s] = \\beta_0 + \\beta_1(1) + \\beta_2(0) + \\beta_3(0) + \\beta_4 s = \\beta_0 + \\beta_1 + \\beta_4 s\n$$\nThe adjusted mean for Group B, $\\mu_B$, is:\n$$\n\\mu_B = E[y^{(e)} | \\text{Group B, avg age, sex } s] = \\beta_0 + \\beta_1(0) + \\beta_2(1) + \\beta_3(0) + \\beta_4 s = \\beta_0 + \\beta_2 + \\beta_4 s\n$$\nThe null hypothesis, $H_0: \\mu_A = \\mu_B$, can now be written using the parameters:\n$$\nH_0: \\beta_0 + \\beta_1 + \\beta_4 s = \\beta_0 + \\beta_2 + \\beta_4 s\n$$\nThis simplifies to:\n$$\nH_0: \\beta_1 = \\beta_2\n$$\nor equivalently,\n$$\nH_0: \\beta_1 - \\beta_2 = 0\n$$\nA linear hypothesis is expressed in the form $c^T \\beta^{(e)} = 0$, where $c$ is the contrast vector. Comparing our hypothesis to this form:\n$$\nc^T \\beta^{(e)} = c_0 \\beta_0 + c_1 \\beta_1 + c_2 \\beta_2 + c_3 \\beta_3 + c_4 \\beta_4 = 0\n$$\nWe require this to be equivalent to $1 \\cdot \\beta_1 + (-1) \\cdot \\beta_2 = 0$. By matching the coefficients for each $\\beta_j$, we find the components of the contrast vector $c$:\n$c_0 = 0$\n$c_1 = 1$\n$c_2 = -1$\n$c_3 = 0$\n$c_4 = 0$\n\nThus, the contrast vector, written as a row vector, is $[0, 1, -1, 0, 0]$. This vector specifies the linear combination of parameter estimates whose sum is tested to be zero under the null hypothesis. The order of elements in the contrast vector must match the order of parameters in $\\beta^{(e)}$, which is determined by the order of columns in the design matrix $X$: intercept, Group A indicator, Group B indicator, mean-centered age, and sex.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 1 & -1 & 0 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once the model is specified, the NBS procedure tests the hypothesis at every edge in the connectome to identify an initial set of candidate edges. This exercise  guides you through the statistical derivation of the edge-wise $t$-statistic, the workhorse of many NBS analyses. Mastering the components of this formula—the contrast of parameter estimates, the estimated error variance, and the influence of the experimental design—is essential for a deep understanding of what NBS is testing.",
            "id": "4181109",
            "problem": "In Network-Based Statistic (NBS) hypothesis testing for connectomes, one computes an edge-wise test statistic across subjects and then applies a component-level correction. Consider an advanced graduate-level setting with $n$ subjects and $p$ regressors, where each edge $e$ yields a subject-wise response vector $y_e \\in \\mathbb{R}^n$. Assume the General Linear Model (GLM) $y_e = X \\beta_e + \\varepsilon_e$ with a fixed, full-column-rank design matrix $X \\in \\mathbb{R}^{n \\times p}$ and errors $\\varepsilon_e \\sim \\mathcal{N}(0, \\sigma_e^2 I_n)$ that are independent and identically distributed across subjects for that edge. A scientific contrast of interest is encoded by a vector $c \\in \\mathbb{R}^p$, and the null hypothesis for edge $e$ is $H_0: c^\\top \\beta_e = 0$.\n\nStarting from the GLM assumptions and ordinary least squares, derive the edge-wise studentized test statistic for $H_0: c^\\top \\beta_e = 0$, and precisely define the constituent quantities and their estimators in terms of $X$ and $y_e$. Then, identify which option below correctly gives the final expression and all definitions needed to compute it in practice for an NBS pipeline. Select the single best answer.\n\nA. $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\hat{\\sigma}_e \\sqrt{c^\\top (X^\\top X)^{-1} c}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, $\\mathrm{RSS}_e = \\|y_e - X \\hat{\\beta}_e\\|_2^2$, $\\hat{\\sigma}_e^2 = \\dfrac{\\mathrm{RSS}_e}{n - p}$, and under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n - p$ degrees of freedom.\n\nB. $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\sqrt{\\hat{\\sigma}_e^2 \\, c^\\top (X X^\\top)^{-1} c}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, $\\mathrm{RSS}_e = \\|y_e - X \\hat{\\beta}_e\\|_2^2$, $\\hat{\\sigma}_e^2 = \\dfrac{\\mathrm{RSS}_e}{n}$, and under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n$ degrees of freedom.\n\nC. $t_e = \\dfrac{\\mathbf{1}^\\top \\hat{\\beta}_e}{\\hat{\\sigma} \\sqrt{\\mathbf{1}^\\top (X^\\top X)^{-1} \\mathbf{1}}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, and $\\hat{\\sigma}^2$ is a pooled variance estimated by aggregating $\\mathrm{RSS}_e$ across all edges and dividing by $n$; under $H_0$ the statistic $t_e$ follows a standard normal distribution.\n\nD. $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\hat{s}_e}$, where $\\hat{s}_e^2 = \\dfrac{\\|y_e\\|_2^2}{n - p}$, with no dependence on $X$ in the variance estimate; under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n - p$ degrees of freedom.",
            "solution": "The user wants me to validate the problem statement and, if valid, derive the solution and evaluate the given options.\n\n### Problem Validation\n\nI will first validate the problem statement according to the specified procedure.\n\n**Step 1: Extract Givens**\n\n*   Number of subjects: $n$\n*   Number of regressors: $p$\n*   Subject-wise response vector for edge $e$: $y_e \\in \\mathbb{R}^n$\n*   General Linear Model (GLM): $y_e = X \\beta_e + \\varepsilon_e$\n*   Design matrix: $X \\in \\mathbb{R}^{n \\times p}$, fixed and of full-column-rank\n*   Error distribution: $\\varepsilon_e \\sim \\mathcal{N}(0, \\sigma_e^2 I_n)$, independent and identically distributed (i.i.d.) across subjects for edge $e$\n*   Contrast vector: $c \\in \\mathbb{R}^p$\n*   Null hypothesis for edge $e$: $H_0: c^\\top \\beta_e = 0$\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem is firmly grounded in the theory of the General Linear Model, a cornerstone of statistical analysis in many scientific fields, including neuroscience. The formulation—using a design matrix, contrast vector, and hypothesis testing—is standard practice for analyzing fMRI, EEG/MEG, and structural connectome data (as in NBS).\n2.  **Well-Posed:** The problem is well-posed. The assumption that the design matrix $X$ is of full-column-rank ensures that the matrix $X^\\top X$ is invertible, which is necessary for finding a unique Ordinary Least Squares (OLS) solution for the parameters. The statistical assumptions (i.i.d. Gaussian errors) lead to a well-defined distributional theory for the test statistic.\n3.  **Objective:** The problem is stated in precise, objective mathematical and statistical language, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. It provides all necessary information to derive the required test statistic from first principles. I will now proceed with the solution derivation.\n\n### Solution Derivation\n\nThe objective is to derive the studentized test statistic for the null hypothesis $H_0: c^\\top \\beta_e = 0$ based on the specified GLM.\n\n**1. Estimation of Parameters $\\beta_e$**\n\nThe parameters $\\beta_e$ are estimated using Ordinary Least Squares (OLS), which minimizes the Residual Sum of Squares, $\\mathrm{RSS}_e = \\|y_e - X\\beta_e\\|_2^2$. The OLS estimator $\\hat{\\beta}_e$ is the solution to the normal equations:\n$$ (X^\\top X) \\hat{\\beta}_e = X^\\top y_e $$\nSince $X$ is full-column-rank, $X^\\top X$ is a $p \\times p$ invertible matrix. Thus, the estimator is unique:\n$$ \\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e $$\n\n**2. Distribution of the Estimator $\\hat{\\beta}_e$**\n\nThe estimator $\\hat{\\beta}_e$ is a linear transformation of the observation vector $y_e$. Since $y_e$ is normally distributed ($y_e \\sim \\mathcal{N}(X\\beta_e, \\sigma_e^2 I_n)$), $\\hat{\\beta}_e$ is also normally distributed.\n*   **Mean:**\n    $E[\\hat{\\beta}_e] = E[(X^\\top X)^{-1} X^\\top y_e] = (X^\\top X)^{-1} X^\\top E[y_e] = (X^\\top X)^{-1} X^\\top (X\\beta_e) = \\beta_e$.\n    The estimator is unbiased.\n*   **Variance-Covariance:**\n    $\\mathrm{Var}(\\hat{\\beta}_e) = \\mathrm{Var}((X^\\top X)^{-1} X^\\top y_e) = ((X^\\top X)^{-1} X^\\top) \\mathrm{Var}(y_e) ((X^\\top X)^{-1} X^\\top)^\\top$\n    $= (X^\\top X)^{-1} X^\\top (\\sigma_e^2 I_n) X (X^\\top X)^{-1} = \\sigma_e^2 (X^\\top X)^{-1} (X^\\top X) (X^\\top X)^{-1} = \\sigma_e^2 (X^\\top X)^{-1}$.\nSo, the distribution of the estimator is $\\hat{\\beta}_e \\sim \\mathcal{N}(\\beta_e, \\sigma_e^2 (X^\\top X)^{-1})$.\n\n**3. Distribution of the Contrast Estimate $c^\\top \\hat{\\beta}_e$**\n\nThe quantity of interest, $c^\\top \\beta_e$, is estimated by $c^\\top \\hat{\\beta}_e$. As a linear combination of the components of the multivariate normal variable $\\hat{\\beta}_e$, it is also normally distributed.\n*   **Mean:** $E[c^\\top \\hat{\\beta}_e] = c^\\top E[\\hat{\\beta}_e] = c^\\top \\beta_e$.\n*   **Variance:** $\\mathrm{Var}(c^\\top \\hat{\\beta}_e) = c^\\top \\mathrm{Var}(\\hat{\\beta}_e) c = c^\\top (\\sigma_e^2 (X^\\top X)^{-1}) c = \\sigma_e^2 c^\\top (X^\\top X)^{-1} c$.\n\nUnder the null hypothesis $H_0: c^\\top \\beta_e = 0$, we have $E[c^\\top \\hat{\\beta}_e] = 0$. The distribution is:\n$$ c^\\top \\hat{\\beta}_e \\sim \\mathcal{N}(0, \\sigma_e^2 c^\\top (X^\\top X)^{-1} c) \\quad (\\text{under } H_0) $$\n\n**4. Estimation of the Error Variance $\\sigma_e^2$**\n\nThe error variance $\\sigma_e^2$ is unknown and must be estimated from the data. The standard unbiased estimator for $\\sigma_e^2$ is derived from the Residual Sum of Squares ($\\mathrm{RSS}_e$):\n$$ \\mathrm{RSS}_e = \\|y_e - X\\hat{\\beta}_e\\|_2^2 $$\nThe degrees of freedom for the error is the number of observations ($n$) minus the number of estimated parameters in $\\beta_e$ ($p$), i.e., $df = n-p$. The unbiased estimator for $\\sigma_e^2$ is:\n$$ \\hat{\\sigma}_e^2 = \\frac{\\mathrm{RSS}_e}{n - p} $$\nIt is a known result from linear model theory that $\\frac{(n-p)\\hat{\\sigma}_e^2}{\\sigma_e^2} = \\frac{\\mathrm{RSS}_e}{\\sigma_e^2}$ follows a chi-squared distribution with $n-p$ degrees of freedom, $\\chi_{n-p}^2$. Furthermore, $\\hat{\\sigma}_e^2$ is statistically independent of $\\hat{\\beta}_e$.\n\n**5. Construction of the Student's $t$-statistic**\n\nA Student's $t$-distributed random variable is formed by the ratio of a standard normal variable to the square root of an independent chi-squared variable divided by its degrees of freedom.\nLet's define a standard normal variable $Z$ under $H_0$:\n$$ Z = \\frac{c^\\top \\hat{\\beta}_e - E[c^\\top \\hat{\\beta}_e]}{\\sqrt{\\mathrm{Var}(c^\\top \\hat{\\beta}_e)}} = \\frac{c^\\top \\hat{\\beta}_e}{\\sigma_e \\sqrt{c^\\top (X^\\top X)^{-1} c}} \\sim \\mathcal{N}(0, 1) $$\nAnd an independent chi-squared variable divided by its degrees of freedom, $V$:\n$$ V = \\frac{\\hat{\\sigma}_e^2}{\\sigma_e^2} = \\frac{\\mathrm{RSS}_e / (n-p)}{\\sigma_e^2} \\sim \\frac{\\chi_{n-p}^2}{n-p} $$\nThe $t$-statistic is $t_e = Z / \\sqrt{V}$:\n$$ t_e = \\frac{ \\frac{c^\\top \\hat{\\beta}_e}{\\sigma_e \\sqrt{c^\\top (X^\\top X)^{-1} c}} }{ \\sqrt{\\frac{\\hat{\\sigma}_e^2}{\\sigma_e^2}} } = \\frac{ \\frac{c^\\top \\hat{\\beta}_e}{\\sigma_e \\sqrt{c^\\top (X^\\top X)^{-1} c}} }{ \\frac{\\hat{\\sigma}_e}{\\sigma_e} } = \\frac{c^\\top \\hat{\\beta}_e}{\\hat{\\sigma}_e \\sqrt{c^\\top (X^\\top X)^{-1} c}} $$\nThis statistic, under $H_0$, follows a Student's $t$-distribution with $n-p$ degrees of freedom.\n\n### Option-by-Option Analysis\n\n*   **Option A**: $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\hat{\\sigma}_e \\sqrt{c^\\top (X^\\top X)^{-1} c}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, $\\mathrm{RSS}_e = \\|y_e - X \\hat{\\beta}_e\\|_2^2$, $\\hat{\\sigma}_e^2 = \\dfrac{\\mathrm{RSS}_e}{n - p}$, and under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n - p$ degrees of freedom.\n\n    This option correctly states the formula for the $t$-statistic. It correctly defines the OLS estimator $\\hat{\\beta}_e$. It correctly defines the Residual Sum of Squares $\\mathrm{RSS}_e$ and the unbiased estimator for the error variance $\\hat{\\sigma}_e^2$ using the correct degrees of freedom, $n-p$. Finally, it correctly identifies the distribution of the statistic under the null hypothesis as a Student's $t$-distribution with $n-p$ degrees of freedom. All components are correct.\n    **Verdict: Correct.**\n\n*   **Option B**: $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\sqrt{\\hat{\\sigma}_e^2 \\, c^\\top (X X^\\top)^{-1} c}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, $\\mathrm{RSS}_e = \\|y_e - X \\hat{\\beta}_e\\|_2^2$, $\\hat{\\sigma}_e^2 = \\dfrac{\\mathrm{RSS}_e}{n}$, and under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n$ degrees of freedom.\n\n    This option contains multiple errors.\n    1. The denominator uses $(X X^\\top)^{-1}$ instead of $(X^\\top X)^{-1}$. $X X^\\top$ is an $n \\times n$ matrix, which is typically not invertible in the common case where $n > p$. Even if it were, the dimensions would be incorrect for multiplication with the $p$-dimensional contrast vector $c$.\n    2. The estimator for the variance is $\\hat{\\sigma}_e^2 = \\dfrac{\\mathrm{RSS}_e}{n}$. This is the biased maximum likelihood estimator, not the unbiased estimator used for the $t$-statistic.\n    3. The degrees of freedom for the $t$-distribution are stated as $n$, which is incorrect. The correct degrees of freedom are $n-p$.\n    **Verdict: Incorrect.**\n\n*   **Option C**: $t_e = \\dfrac{\\mathbf{1}^\\top \\hat{\\beta}_e}{\\hat{\\sigma} \\sqrt{\\mathbf{1}^\\top (X^\\top X)^{-1} \\mathbf{1}}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, and $\\hat{\\sigma}^2$ is a pooled variance estimated by aggregating $\\mathrm{RSS}_e$ across all edges and dividing by $n$; under $H_0$ the statistic $t_e$ follows a standard normal distribution.\n\n    This option contains multiple errors.\n    1. It replaces the general contrast vector $c$ with a vector of ones, $\\mathbf{1}$. This is a specific contrast that tests $H_0: \\sum_{i=1}^p \\beta_{ei} = 0$, which is not the general hypothesis $H_0: c^\\top \\beta_e = 0$.\n    2. It uses a pooled variance $\\hat{\\sigma}^2$ estimated across all edges. The problem defines the model with an edge-specific variance $\\sigma_e^2$, so pooling is not part of the specified model.\n    3. It claims the statistic follows a standard normal distribution. For a finite sample size $n$ where the variance is estimated, the distribution is Student's $t$, not normal.\n    **Verdict: Incorrect.**\n\n*   **Option D**: $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\hat{s}_e}$, where $\\hat{s}_e^2 = \\dfrac{\\|y_e\\|_2^2}{n - p}$, with no dependence on $X$ in the variance estimate; under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n - p$ degrees of freedom.\n\n    This option is incorrect because it misspecifies the standard error of the numerator. The denominator of the $t$-statistic should be the estimated standard deviation of $c^\\top \\hat{\\beta}_e$, which is $\\sqrt{\\hat{\\sigma}_e^2 c^\\top(X^\\top X)^{-1}c}$. The proposed quantity $\\hat{s}_e^2 = \\dfrac{\\|y_e\\|_2^2}{n-p}$ is an estimate of the total variance of the data $y_e$ (divided by $n-p$), not the variance of the estimated contrast. It incorrectly uses the total sum of squares $\\|y_e\\|_2^2$ instead of the residual sum of squares, and it entirely omits the crucial term $c^\\top (X^\\top X)^{-1} c$ that accounts for the design and the specific contrast being tested.\n    **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The unique power of NBS lies in shifting the statistical inference from individual edges to the level of connected components, or networks. After thresholding the edge-wise statistics, the crucial next step is to efficiently identify these components. This practice  delves into the computational heart of NBS, exploring the highly efficient union-find algorithm for partitioning the graph and analyzing its performance, a vital skill for implementing a fast and scalable analysis pipeline.",
            "id": "4181097",
            "problem": "You are analyzing functional connectome data with the Network-Based Statistic (NBS), which identifies connected components of supra-threshold edges in a graph induced by a primary threshold on edgewise test statistics. Let there be $V$ regions of interest (ROIs) and an undirected, loop-free graph $G=(\\mathcal{V}, \\mathcal{E}_{\\tau})$ where $\\lvert \\mathcal{V} \\rvert = V$ and $\\mathcal{E}_{\\tau} = \\{ (i,j) : i \\neq j,\\, T_{ij} \\ge \\tau \\}$ denotes the set of supra-threshold edges after thresholding the test statistic matrix at level $\\tau$. Let $E = \\lvert \\mathcal{E}_{\\tau} \\rvert$. The goal within each permutation of NBS is to extract all connected components of $G$ in order to compute a component-wise statistic (e.g., maximal component size). Assume the standard random access machine model of computation, and that $\\mathcal{E}_{\\tau}$ is provided as a list of unordered vertex pairs.\n\nSelect the option that both outlines a correct and efficient disjoint-set union (union–find) approach to extract connected components in this NBS step and states a correct tight amortized time complexity bound in terms of $E$ and $V$.\n\nA. Initialize a disjoint-set data structure with a singleton set for each of the $V$ vertices. For each supra-threshold edge $(i,j) \\in \\mathcal{E}_{\\tau}$, perform a union of the sets containing $i$ and $j$ using union by rank and path compression. After processing all edges, traverse all vertices once, apply find to obtain each vertex’s representative, and group vertices by representative to produce components. The amortized running time over all operations is $O\\big((E+V)\\,\\alpha(V)\\big)$, where $\\alpha(\\cdot)$ is the inverse Ackermann function, which simplifies to $O\\big(E\\,\\alpha(V)\\big)$ when $E \\ge V$.\n\nB. Build an adjacency list for the supra-threshold graph and run a depth-first search (DFS) from each unvisited vertex to label components, which runs in $O(E+V)$ time. This is strictly faster than any union–find approach and therefore the preferred method in NBS.\n\nC. Use a disjoint-set structure without union by rank and without path compression. For each supra-threshold edge, perform a union of its endpoints. Finally, iterate over all vertices and apply find to label components. The amortized running time is $O\\big(E \\log V\\big)$.\n\nD. Sort the supra-threshold edges by their test statistic values and then run a Kruskal-style scan, uniting endpoints as you go; the sorting step yields an overall $O\\big(E \\log E\\big)$ complexity, which is optimal for extracting connected components in this setting and necessary for correctness when used within NBS.",
            "solution": "The problem asks for the selection of an option that correctly describes an efficient disjoint-set union (DSU) or union–find algorithm to find the connected components of a graph and provides the correct time complexity for this task. The context is a specific step within the Network-Based Statistic (NBS) method in neuroscience, where a graph $G=(\\mathcal{V}, \\mathcal{E}_{\\tau})$ is formed by thresholding a matrix of test statistics. The graph has $V = \\lvert \\mathcal{V} \\rvert$ vertices and $E = \\lvert \\mathcal{E}_{\\tau} \\rvert$ edges, and the input is given as a list of these $E$ edges.\n\nFirst, let us establish the most efficient method for finding connected components using a disjoint-set union data structure.\n\nThe DSU data structure maintains a collection of disjoint sets. The three primary operations are:\n1.  `MAKESET(x)`: Creates a new set containing only the element $x$.\n2.  `FIND(x)`: Returns the representative (or root) of the set containing $x$.\n3.  `UNION(x, y)`: Merges the two sets that contain $x$ and $y$.\n\nThe algorithm to find connected components of a graph with $V$ vertices and an edge list of $E$ edges is as follows:\n1.  Initialize the DSU data structure by creating a singleton set for each of the $V$ vertices. This requires $V$ calls to `MAKESET`.\n2.  Iterate through the $E$ edges. For each edge $(u, v)$:\n    a. Check if $u$ and $v$ are already in the same component by comparing their representatives: `FIND(u) == FIND(v)`.\n    b. If they are not, merge their sets: `UNION(u, v)`.\n3.  After processing all $E$ edges, all vertices belonging to the same connected component will be in the same set within the DSU structure. To explicitly retrieve the components, one can iterate through all $V$ vertices and group them by their set representative, which is found using the `FIND` operation.\n\nThe time complexity of this algorithm depends on the implementation of the `UNION` and `FIND` operations. The most efficient implementation uses two heuristics:\n- **Union by size or rank:** When merging two sets, the root of the tree representing the smaller set is attached to the root of the tree representing the larger set. This keeps the trees from becoming unnecessarily deep.\n- **Path compression:** During a `FIND(x)` operation, every node on the path from $x$ to the root is made a direct child of the root. This flattens the tree structure over time.\n\nWhen both union by rank/size and path compression are used, the amortized time complexity for a sequence of $m$ operations on $n$ elements is $O(m \\, \\alpha(n))$, where $\\alpha(n)$ is the inverse Ackermann function. The function $\\alpha(n)$ grows extremely slowly; for any practical value of $n$, $\\alpha(n) < 5$.\n\nIn our specific problem:\n- The number of elements is $n = V$.\n- We perform $V$ `MAKESET` operations for initialization.\n- We process $E$ edges, which involves $E$ pairs of `FIND` operations and at most $E-1$ `UNION` operations. This amounts to approximately $3E$ operations.\n- Finally, to explicitly list the components, we perform $V$ `FIND` operations.\n- The total number of operations $m$ is therefore in the order of $V + 3E + V$, which is $O(E+V)$.\n\nSubstituting these into the general complexity formula, the total amortized time is $O\\big((E+V)\\,\\alpha(V)\\big)$. In many connectomics applications, the graph is not extremely sparse, so it is common that $E \\ge V$. In this case, the $E$ term dominates, and the complexity can be simplified to $O\\big(E\\,\\alpha(V)\\big)$.\n\nNow we evaluate each option.\n\n**A. Initialize a disjoint-set data structure with a singleton set for each of the $V$ vertices. For each supra-threshold edge $(i,j) \\in \\mathcal{E}_{\\tau}$, perform a union of the sets containing $i$ and $j$ using union by rank and path compression. After processing all edges, traverse all vertices once, apply find to obtain each vertex’s representative, and group vertices by representative to produce components. The amortized running time over all operations is $O\\big((E+V)\\,\\alpha(V)\\big)$, where $\\alpha(\\cdot)$ is the inverse Ackermann function, which simplifies to $O\\big(E\\,\\alpha(V)\\big)$ when $E \\ge V$.**\n\nThis option precisely describes the optimal disjoint-set union algorithm.\n- The procedural description is correct: initialize $V$ sets, process $E$ edges using `UNION` (which relies on `FIND`), and then finalize the component structure by iterating through the vertices and using `FIND`.\n- It correctly identifies the use of \"union by rank and path compression\" as the key optimizations.\n- The stated amortized time complexity, $O\\big((E+V)\\,\\alpha(V)\\big)$, is the correct tight bound for this procedure.\n- The simplification to $O\\big(E\\,\\alpha(V)\\big)$ under the reasonable condition $E \\ge V$ is also correct.\n**Verdict: Correct.**\n\n**B. Build an adjacency list for the supra-threshold graph and run a depth-first search (DFS) from each unvisited vertex to label components, which runs in $O(E+V)$ time. This is strictly faster than any union–find approach and therefore the preferred method in NBS.**\n\nThis option describes an alternative algorithm (DFS/BFS) for finding connected components. The description of the DFS algorithm and its time complexity of $O(E+V)$ are correct. However, the problem statement explicitly asks for an option that outlines a **disjoint-set union (union–find) approach**. This option describes a completely different method. Furthermore, the claim that DFS is \"strictly faster\" is technically true asymptotically, as $\\alpha(V) > 1$ for $V>2$, but for practical purposes, the performance is comparable, and the claim that it's \"therefore the preferred method\" is a subjective judgment, not a formal statement of fact. Because it does not describe a union-find approach, it fails to answer the question.\n**Verdict: Incorrect.**\n\n**C. Use a disjoint-set structure without union by rank and without path compression. For each supra-threshold edge, perform a union of its endpoints. Finally, iterate over all vertices and apply find to label components. The amortized running time is $O\\big(E \\log V\\big)$.**\n\nThis option describes a naive implementation of the DSU algorithm, explicitly stating it is \"without union by rank and without path compression\". In this case, the data structure can degenerate into a linked list, where a `FIND` operation can take up to $O(V)$ time in the worst case. A sequence of $E$ edge-processing steps could therefore take up to $O(EV)$ time. The stated complexity of $O(E \\log V)$ is incorrect for this naive implementation; it corresponds to an implementation that uses either union by rank *or* path compression, but not both, and not neither.\n**Verdict: Incorrect.**\n\n**D. Sort the supra-threshold edges by their test statistic values and then run a Kruskal-style scan, uniting endpoints as you go; the sorting step yields an overall $O\\big(E \\log E\\big)$ complexity, which is optimal for extracting connected components in this setting and necessary for correctness when used within NBS.**\n\nThis option introduces an unnecessary and computationally expensive step: sorting the edges. The problem is to find connected components on the graph $G$ defined by a *single, fixed threshold* $\\tau$. For this task, all edges in $\\mathcal{E}_{\\tau}$ are equivalent; their original test statistic values are irrelevant to the graph's topology. The sorting step, which takes $O(E \\log E)$ time, is not needed. The claim that this is \"optimal\" is false, as both the DSU approach in option A ($O((E+V)\\alpha(V))$) and the DFS approach in option B ($O(E+V)$) are faster than $O(E \\log E)$. The claim that it is \"necessary for correctness\" is also false. This procedure mimics Kruskal's algorithm for finding a minimum spanning tree, which is not the problem being solved.\n**Verdict: Incorrect.**\n\nBased on the analysis, only option A provides a correct description of an efficient DSU algorithm and its corresponding tight time complexity for the specified problem.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}