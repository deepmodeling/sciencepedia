## 引言
随着神经网络在处理复杂[高维数据](@entry_id:138874)方面取得巨大成功，它们已成为神经科学研究不可或缺的工具。然而，这些模型的“黑箱”特性也带来了严峻的挑战：我们如何信任一个我们不理解的模型的预测？更重要的是，我们如何利用这些模型来获得关于大脑工作方式的深刻科学见解，而不仅仅是满足于预测的准确性？解决这一知识鸿沟，正是[神经网络可解释性](@entry_id:1128597)与可说明性研究的核心任务。

本文旨在系统性地剖析这一前沿领域。我们将深入探讨从“黑箱”中提取有意义信息所需的原理、技术及其在科学实践中的应用。通过本文的学习，读者将能够理解不同解释方法的数学基础和适用场景，学会批判性地评估解释结果的有效性，并最终掌握如何利用这些工具来推动神经科学的发现。

文章将分为三个核心部分展开。在**“原理与机制”**一章中，我们将建立精确的概念框架，区分可解释性与可说明性，并深入剖析梯度方法、LIME、[Shapley值](@entry_id:634984)以及概念激活向量等关键技术，同时揭示它们在实践中可能遇到的陷阱。随后，在**“应用与跨学科连接”**一章中，我们将展示这些技术如何应用于解释神经表征、[解耦](@entry_id:160890)混杂因素、并最终生成可供实验验证的因果假说，同时探讨其在医疗、法律等高风险领域的意义。最后，**“动手实践”**部分将提供一系列编程练习，让读者亲手实现和检验文中所学的核心概念。现在，让我们从构建理解的基础开始，深入探索这些强大模型的内部世界。

## 原理与机制

本章深入探讨了将神经网络模型应用于[神经科学数据分析](@entry_id:1128665)时，其可解释性（interpretability）与可说明性（explainability）的核心原理和机制。继前一章对该领域背景的介绍之后，我们将系统性地剖析用于理解模型内部工作方式的各种概念、方法及其评估标准。我们将从基本定义出发，逐步过渡到具体的解释技术，最终探讨在复杂的神经科学背景下评估这些解释时所面临的关键挑战。

### 概念基础：界定理解的层次

在探究“黑箱”模型的内部世界时，精确的术语至关重要。不同的术语代表了不同深度的理解，混淆它们会导致错误的科学结论。我们需要严格区分几个核心概念：**透明性（transparency）**、**可模拟性（simulatability）**、**可说明性（explainability）** 和 **可解释性（interpretability）**。

**透明性**指的是我们对模型的所有组成部分拥有完全的访问权限。对于一个典型的神经网络 $f_{\theta}$，透明性意味着我们知晓其完整的架构或[计算图](@entry_id:636350) $G_{f}$、所有参数 $\theta$（例如权重和偏置）以及其训练过程。然而，拥有访问权限并不等同于理解。一个拥有数十亿参数的透明模型，其复杂性可能依然令人望而生畏。

**可模拟性**则更进一步，指人类认知或一个简单的算法能够以合理的认知或计算代价，追踪并执行模型对于一个典型输入 $x$ 的计算过程 $f_{\theta}(x)$。通常，只有具备较低描述复杂度或计算复杂度的模型，如稀疏线性模型或小型决策树，才具备高度的可模拟性。对于[深度神经网络](@entry_id:636170)而言，可模拟性几乎是不可能实现的。

与上述两者不同，可说明性和[可解释性](@entry_id:637759)专注于揭示模型行为背后的“原因”。

**可说明性**通常指代一系列**事后（post hoc）**或**内在（intrinsic）**的技术，它们旨在为模型的特定预测提供人类可理解的理由或归因。例如，对于一个给定的输入 $x$，一个[特征归因](@entry_id:926392)（feature attribution）方法会生成一个映射 $E(x)$，量化每个输入特征对模型输出 $f_{\theta}(x)$ 的贡献。这些解释致力于**忠实于（faithful to）**模型的局部输入-输出行为，但它们本身并不必然要求与真实世界的因果机制存在任何对应关系。可说明性工具帮助我们调试模型或理解模型自身的逻辑，但它们本身并不足以支撑关于神经机制的科学论断。

**可解释性**则是一个更深邃、更具科学抱负的概念。一个模型是可解释的，当且仅当其内部组件（如单元、参数或计[算子图](@entry_id:261846)）与一个关于真实世界神经科学系统的**[结构因果模型](@entry_id:911144)（Structural Causal Model, SCM）**的变量之间，存在一个明确定义的对应关系 $\pi$。这个SCM拥有一个因果图 $G_S$，描述了系统中各变量（如不同脑区的活动）之间的因果关系。最关键的是，这种对应关系必须满足**干预对齐（intervention alignment）**的准则：对SCM中的某个变量 $Z_j$ 进行干预（例如，通过实验技术抑制某个脑区，记为 $do(Z_j=z')$），其在真实世界中引起的后果，能够通过对模型中对应的组件进行相应干预（例如，钳制某个单元的激活值或修改一个子图的计算）来准确预测。换言之，模型不仅要能预测观测数据，还要能预测干[预实验](@entry_id:172791)的结果。只有当模型达到这种深度可解释性，我们才能有理有据地宣称它揭示了真实的**神经机制（neural mechanisms）** 。

基于此，我们将后续的讨论分为两大类：旨在提供局部说明性的方法，以及追求全局[可解释性](@entry_id:637759)的更高级策略。

### 局部说明：解构单次预测

局部说明方法旨在回答这样一个问题：“对于这一个特定的输入，模型为什么会做出这样的预测？”

#### 梯度方法及其陷阱

最直观的局部说明方法是基于**梯度（gradient）**的。对于一个给定的输入 $x$ 和一个标量输出 $y = f(x)$，我们可以计算输出相对于每个输入特征 $x_i$ 的偏导数 $\frac{\partial y}{\partial x_i}$。这个值，通常被称为**显著性（saliency）**，直观地表示了输入特征的一个微小变动会对输出产生多大的影响。[显著性图](@entry_id:635441)（saliency maps）通过将这些梯度值可视化，来高亮显示模型“关注”的输入区域。

然而，这种看似简单的方法隐藏着一个重要的陷阱，尤其是在处理经过预处理的神经科学数据时。例如，在分析多通道脑电图（EEG）数据时，一个常见的预处理步骤是**标准化（standardization）**，即对每个通道 $i$ 的原始信号 $x_i(t)$ 进行如下变换：$z_i(t) = (x_i(t) - \mu_i) / \sigma_i$，其中 $\mu_i$ 和 $\sigma_i$ 是该通道在训练集上的均值和标准差。模型 $f$ 直接处理的是标准化后的数据 $z$。

如果我们想知道模型对原始输入 $x$ 的敏感度，就需要使用[链式法则](@entry_id:190743)。显著性 $S_{i,t} = \frac{\partial y}{\partial x_i(t)}$ 与模型对标准化输入的敏感度 $\frac{\partial y}{\partial z_i(t)}$ 之间的关系是：

$$
S_{i,t} = \frac{\partial y}{\partial x_i(t)} = \frac{\partial y}{\partial z_i(t)} \frac{\partial z_i(t)}{\partial x_i(t)} = \frac{1}{\sigma_i} \frac{\partial y}{\partial z_i(t)}
$$

这个简单的公式揭示了一个严重的问题：原始输入的显著性 $S_{i,t}$ 被其所在通道的固有标准差 $\sigma_i$ 反向缩放了。假设模型对两个不同通道的标准化输入具有完全相同的敏感度（即 $|\frac{\partial y}{\partial z_1(t)}| = |\frac{\partial y}{\partial z_2(t)}|$），但通道1的原始信号方差远大于通道2（即 $\sigma_1 > \sigma_2$）。那么，计算出的原始输入显著性将显示通道1的重要性反而更小（$|S_{1,t}| < |S_{2,t}|)$）。这种由方差引起的衰减会严重扭曲我们对跨通道[特征重要性](@entry_id:171930)的比较。

为了得到一个更忠实的解释，我们应该直接比较模型对其直接输入（即标准化数据）的敏感度 $|\frac{\partial y}{\partial z_i(t)}|$，或者通过乘以 $\sigma_i$ 来校正原始输入的显著性，即计算 $\tilde{S}_{i,t} = \sigma_i \frac{\partial y}{\partial x_i(t)} = \frac{\partial y}{\partial z_i(t)}$。这个例子警示我们，即使是基础的解释方法，也必须结合[数据预处理](@entry_id:197920)流程进行审慎的数学分析 。

#### 局部代理模型：LIME

梯度方法的一个局限是它们只提供了关于模型行为的[局部线性近似](@entry_id:263289)，并且只适用于可微模型。**局部[可解释模型](@entry_id:637962)无关说明（Local Interpretable Model-agnostic Explanations, LIME）**提供了一种更通用的方法。LIME的核心思想是：尽管我们可能无法理解一个复杂模型 $f$ 的全局行为，但我们可以用一个简单的、可解释的**代理模型（surrogate model）** $g$（如线性模型或[决策树](@entry_id:265930)）来近似它在某个特定输入 $\mathbf{x}_0$ 周围的局部行为。

LIME的执行过程如下：
1.  **定义可解释表示**：首先，将原始的高维输入 $\mathbf{x}_0$ 转换为一个人类可理解的、通常是低维的二元表示。例如，在功能性磁共振成像（fMRI）数据中，单个体素（voxel）的意义不大。一个更有意义的表示是将空间上相邻的体素聚合成**[超体素](@entry_id:907697)（supervoxels）**。解释将基于这些[超体素](@entry_id:907697)的“开启”或“关闭”。
2.  **生成局部邻域样本**：在 $\mathbf{x}_0$ 的可解释表示的邻域内生成一系列扰动样本 $\mathbf{z}$。例如，随机“关闭”某些[超体素](@entry_id:907697)。然后将这些扰动样本转换回原始数据空间，并用[黑箱模型](@entry_id:1121697) $f$ 得到它们的预测值 $f(\mathbf{z})$。
3.  **加权拟合代理模型**：使用生成的样本对 $\{(\mathbf{z}, f(\mathbf{z}))\}$ 来训练可解释的代理模型 $g$。关键在于，这是一个**加权回归**过程。每个样本 $\mathbf{z}$ 的权重由一个**[核函数](@entry_id:145324)（kernel）** $\pi_{\mathbf{x}_0}(\mathbf{z})$ 决定，该函数使其权重随样本与原始输入 $\mathbf{x}_0$ 之间距离 $d(\mathbf{x}_0, \mathbf{z})$ 的增加而单调递减。这确保了代理模型 $g$ 优先忠实于 $f$ 在 $\mathbf{x}_0$ 紧邻区域的行为。
4.  **提取解释**：[可解释模型](@entry_id:637962) $g$ 的参数（如[线性模型](@entry_id:178302)的系数）本身就构成了对 $\mathbf{x}_0$ 预测的解释。

LIME方法的有效性高度依赖于**邻域（neighborhood）**的选择，即[核函数](@entry_id:145324)的宽度。一个非常窄的邻域（小核宽）会强制代理模型极其忠实于 $f$ 在 $\mathbf{x}_0$ 点的局部行为，这被称为高**局部保真度（local fidelity）**。但由于有效加权的样本数量较少，这可能导致解释的**方差（variance）**较高，即对扰动样本的微小变化非常敏感。相反，一个宽泛的邻域会使用更多样本，使解释更稳定（低方差），但可能因为试图用一个简单模型去拟合一个大的、高度[非线性](@entry_id:637147)的区域而牺牲局部保真度，引入**偏差（bias）** 。

#### 基于博弈论的归因：[Shapley值](@entry_id:634984)

LIME提供了一种模型无关的框架，但其生成的解释的属性（如解释的稳定性）可能不够理想。一个更具理论基础的方法源于合作博弈论，即**[Shapley值](@entry_id:634984)（Shapley values）**。该方法为将模型的总“收益”（即预测值）公平地分配给各个“玩家”（即输入特征）提供了一种唯一满足特定公理的方案。

这些理想的公理包括：
1.  **效率性（Efficiency）**：所有特征的归因值之和等于模型的总输出与没有任何特征时的基线输出之差，即 $\sum_{i=1}^{p} \phi_i = f(N) - f(\varnothing)$，其中 $\phi_i$ 是特征 $i$ 的归因，$N$ 是所有特征的集合，$f(N)$ 是使用所有特征时的模型预测，$f(\varnothing)$ 是基线预测。
2.  **对称性（Symmetry）**：如果两个特征对于任何特征组合的边际贡献都相同，那么它们的归因值应该相等。
3.  **虚拟性（Dummy）**：如果一个特征对于任何特征组合都没有任何边际贡献，那么它的归因值为零。

[Shapley值](@entry_id:634984) $\phi_i$ 的计算方式是，对于特征 $i$，考虑它加入到不包含它的所有可能特征子集（**联盟**，$S \subseteq N \setminus \{i\}$）时所带来的边际贡献 $f(S \cup \{i\}) - f(S)$，然后对这些边际贡献进行加权平均。其计算公式为：
$$
\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!\,(p-|S|-1)!}{p!} [f(S \cup \{i\}) - f(S)]
$$
其中 $p$ 是总特征数。这等价于计算当所有特征以随机顺序排列加入时，特征 $i$ 的期望边际贡献。

在实践中，一个核心挑战是如何定义“缺少”某些特征时的模型输出 $f(S)$。一种理论上最合理的方法是，将缺失的特征 $N \setminus S$ 从它们的**背景分布（background distribution）**中进行积分（或取期望），以模拟它们“未知”的状态。这意味着 $f(S) = \mathbb{E}_{X_{N \setminus S}}[f(X_S, X_{N \setminus S}) | X_S = x_S]$。选择合适的背景分布至关重要，它通常是训练数据的[经验分布](@entry_id:274074)。在[神经影像分析](@entry_id:918693)中，如果模型是在经过[标准化](@entry_id:637219)和伪影回归等预处理的特征上训练的，那么这个背景分布也必须是基于这些[预处理](@entry_id:141204)后的特征，以确保解释的有效性 。SHAP (SHapley Additive exPlanations) 是一个流行的框架，它提供了高效的算法来近似计算[Shapley值](@entry_id:634984)。

### 全局可解释性：理解模型的整体策略

局部说明方法虽然强大，但它们一次只能解释一个预测。为了获得对模型整体计算策略的理解，我们需要全局[可解释性方法](@entry_id:636310)。这些方法旨在回答更宏观的问题，例如“模型学到了哪些不变性？”或“模型是否依赖于某个特定的神经科学概念？”

#### 基于概念的探针：概念激活向量 (CAV)

我们能否用人类可理解的概念来探查神经网络的内部表征？**概念激活向量（Concept Activation Vectors, CAVs）**提供了一种实现这一目标的优雅方法。CAV旨在识别模型内部某个隐藏层激活空间中，与某个特定概念（如“运动敏感区域”或“条纹图案”）相对应的方向。

CAV的构建过程是一个监督学习任务：
1.  **收集概念样本**：首先，为目标概念准备一个正例集 $\mathcal{S}_+$（例如，包含显著运动的视频）和一个负例集或随机基线集 $\mathcal{S}_-$（例如，静态或随机运动的视频）。
2.  **提取激活**：将这些样本输入到已经训练好的神经网络中，并提取出目标隐藏层 $\ell$ 的激活向量 $\mathbf{h}_\ell(\mathbf{x})$。
3.  **训练[线性分类器](@entry_id:637554)**：在激活空间 $\mathbb{R}^d$ 中，我们现在有两组点：$\{\mathbf{h}_\ell(\mathbf{x}) | \mathbf{x} \in \mathcal{S}_+\}$ 和 $\{\mathbf{h}_\ell(\mathbf{x}) | \mathbf{x} \in \mathcal{S}_-\}$。我们的目标是找到一个能够区分这两组点的方向。这可以通过训练一个**线性[二元分类器](@entry_id:911934)**（如逻辑回归或线性[支持向量机](@entry_id:172128)SVM）来实现。
4.  **定义CAV**：[线性分类器](@entry_id:637554)学习到一个[决策边界](@entry_id:146073)，其形式为 $\mathbf{w}^\top\mathbf{h} + b = 0$。这里的权重向量 $\mathbf{w}$ 正是该决策[超平面](@entry_id:268044)的**[法向量](@entry_id:264185)（normal vector）**，它指向了能够将正负样本最大化分离的方向。这个（通常被归一化的）向量 $\mathbf{w}$ 就是代表该概念的CAV，记为 $\mathbf{v}_{\text{concept}}$ 。

一旦获得了某个概念的CAV，我们就可以量化该概念对模型最终预测的重要性。例如，通过[计算模型](@entry_id:637456)某个输出类别 $k$ 的梯度与CAV的点积（一种[方向导数](@entry_id:189133)），我们可以判断增加该概念的强度是会增加还是减少模型对类别 $k$ 的预测概率。这使得我们能够超越输入特征，用更抽象、更具科学意义的语言来检验模型的内部逻辑。

#### 寻找机制：从模型到科学发现

全局[可解释性](@entry_id:637759)的终极目标是**机制性解释（mechanistic explanation）**，即从模型中提取出与真实世界现象相对应的因果机制。根据科学哲学的定义，一个机制由**实体（entities）**、**活动（activities）**和**组织（organization）**构成，它们协同作用产生特定现象。我们可以将这个框架映射到神经网络上：
- **实体**：模型的结构组件，如单个神经元、神经元层或参数。
- **活动**：这些组件执行的计算，如加权求和与[非线性变换](@entry_id:636115)。
- **组织**：将实体和活动连接起来的结构，即网络的拓扑结构和权重。

在神经网络中寻找一个机制，等同于在模型的[计算图](@entry_id:636350) $G$ 中识别出一个子图 $M$，该子图负责实现某个特定的计算现象 $\mathcal{P}$（例如，在模拟初级[视觉皮层](@entry_id:1133852)的模型中实现[方向选择性](@entry_id:899156)）。为了验证 $M$ 确实是产生 $\mathcal{P}$ 的因果机制，我们需要进行**干预性测试**：
- **充分性（Sufficiency）**：隔离[子图](@entry_id:273342) $M$。如果将模型其余部分的功能替换为简单的“直通”连接，而 $M$ 本身仍然能够（在一定容差内）产生现象 $\mathcal{P}$，那么 $M$ 对于 $\mathcal{P}$ 是充分的。
- **必要性（Necessity）**：破坏[子图](@entry_id:273342) $M$。如果通过**损毁（ablation）**（例如，将 $M$ 中某些权重设为零）或重[参数化](@entry_id:265163) $M$，能够可预测地、显著地削弱或消除现象 $\mathcal{P}$，那么 $M$ 对于 $\mathcal{P}$ 是必要的。

这种基于干预的因果分析，将模型解释从单纯的观察性描述提升到了类似于实验室实验的假设检验层面，是连接模型与科学发现的关键桥梁 。

### 批判性评估：忠实性、合理性与混杂因素

掌握了解释方法之后，一个更为关键的问题是：我们如何评估这些解释的质量？一个好的解释应该具备哪些属性？

#### 忠实性：解释是否真实反映模型？

评估解释的首要标准是**忠实性（faithfulness）**。一个忠实的解释必须准确地反映模型做出预测的真实原因，而不是提供一个看似合理但与模型实际计算过程无关的故事。

如何量化忠实性？一个严谨的方法是基于**干预（intervention）**。如果一个解释声称某些特征是重要的，那么移除或扰动这些特征应该比扰动不重要的特征对模型的输出产生更大的影响。这个简单的想法需要被精确化。在神经科学数据（如fMRI）中，特征之间存在复杂的时空相关性。简单地将某些体素的值设为零或随机值会产生**分布外（out-of-distribution）**的、不切实际的输入，模型在这些输入上的行为可能是未定义的或具有误导性的。

因此，一个更合理的干预方案是**流形上（on-manifold）**的干预。对于一个特征子集 $S$，我们可以用从其[条件分布](@entry_id:138367) $p(x_S | x_{\bar{S}})$ 中抽样得到的新值来替换原始值，这里 $x_{\bar{S}}$ 是所有其他未被扰动的特征。这需要一个能够捕捉数据内在结构的生成模型。有了这个工具，我们可以定义忠实性：对于一个解释 $\phi(x)$，如果特征集 $S_1$ 的总归因得分高于 $S_2$（即 $\sum_{(v,t) \in S_1} \phi_{v,t}(x) \ge \sum_{(v,t) \in S_2} \phi_{v,t}(x)$），那么对 $S_1$ 进行流形上干预所引起的模型输出期望变化应该大于或等于对 $S_2$ 进行干预所引起的变化。我们可以通过计算归因得分与实际干预效果之间的秩相关系数，并进行统计检验，来对一个解释方法的忠实性进行可证伪的、定量的评估 。

#### 合理性与忠实性的分歧

另一个常被提及的属性是**合理性（plausibility）**，即解释是否符合人类专家的先验知识或直觉。理想情况下，一个忠实的解释也应该是合理的。然而，在实践中，两者可能并且经常会发生分歧。

1.  **测量过程的扭曲**：[神经科学模型](@entry_id:1128668)操作的是观测数据 $x$，而非潜在的神经活动 $z$。观测过程 $x = m(z) + \eta$ 会引入模糊、滤波和噪声。如果模型 $f$ 学会了利用这些测量过程引入的伪影（artifacts）来进行预测，那么一个忠实的解释就必须高亮这些伪影。然而，神经科学家专家会认为这种解释是“不合理的”，因为他们的先验知识是关于神经活动 $z$ 的。反之，一个通过平滑或聚合来迎合专家解剖学先验的解释可能看起来很合理，但如果模型实际利用的是伪影中的高频信息，那么这个解释就是不忠实的 。

2.  **特征[共线性](@entry_id:270224)**：当两个或多个输入特征高度相关时（例如，fMRI中经过平滑处理的相邻体素），模型可能依赖于它们的任意组合。归因方法可能会将重要性分配给其中一个特征，而专家可能认为另一个特征才是“真正”重要的。在这种情况下，多个不同的解释可能在数学上同样忠实于模型的局部行为，但只有一个会显得合理 。

因此，我们必须警惕所谓的“安慰剂解释”——那些看似合理但却不忠实于模型的解释。在科学发现的语境下，忠实性永远是第一位的。一个不合理但忠实的解释可能揭示了模型利用了我们未曾预料到的数据伪影，或者发现了反直觉的真实生物学模式，这两种情况都具有重要的科学价值。

#### 混杂因素的挑战

最后，即便我们拥有一个忠实地解释了模型的工具，我们仍然可能被**混杂因素（confounding factors）**所误导。在因果推断中，混杂是指一个外部变量同时影响了我们关心的原因和结果，从而产生虚假的关联。

在[fMRI分析](@entry_id:1125162)中，一个典型的混杂因素是**头动（head motion）**。假设我们训练一个模型 $N$ 从fMRI信号 $Y$ 中解码任务刺激 $S$。我们知道，刺激 $S$ 会引起真实的神经活动 $B$，同时，任务难度或时长也可能引起被试的头动 $M$。神经活动 $B$ 和头动 $M$ 都会对观测到的fMRI信号 $Y$ 产生影响。因此，信号 $Y$ 是 $B$ 和 $M$ 的混合体。模型 $N$ 和由它产生的解释 $E$ 都是基于混合信号 $Y$ 的。

其因果图可以表示为：$S$ 是 $B$ 和 $M$ 的[共同原因](@entry_id:266381)（$B \leftarrow S \rightarrow M$）。$B$ 和 $M$ 又都是 $Y$ 的原因（$B \rightarrow Y \leftarrow M$）。$Y$ 则是 $N$ 和 $E$ 的原因（$Y \rightarrow N \rightarrow E$）。在这种结构下，存在一条从神经活动 $B$ 到解释 $E$ 的**后门路径（backdoor path）**：$B \leftarrow S \rightarrow M \rightarrow Y \rightarrow N \rightarrow E$。这条路径是非因果的，它通过共同的原因 $S$ 建立了 $B$ 和 $M$ 之间的关联，而 $M$ 又通过 $Y$ 影响 $E$。这条开放的后门路径意味着，我们观测到的 $B$ 和 $E$ 之间的关联，部分是真实的因果效应（$B \rightarrow Y \rightarrow \dots \rightarrow E$），部分是由于头动 $M$ 造成的[虚假关联](@entry_id:910909)。一个忠实的解释可能会高亮显示与头动相关的信号模式，不是因为它们反映了与任务相关的神经计算，而是因为头动本身与任务刺激相关联，从而成为一个有用的预测特征。忽略这种混杂结构，会导致我们将基于伪影的解释错误地归因为神经活动 。

总之，对神经网络模型的解释是一项多层次的、充满挑战的任务。它要求我们不仅要掌握各种解释技术，还要深刻理解它们的数学原理、内在局限，并具备使用因果推断和严格评估标准来批判性地审视解释结果的能力。只有这样，这些强大的模型才能真正成为推动神经科学发现的可靠工具。