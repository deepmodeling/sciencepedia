## 引言
在过去的十年里，[卷积神经网络](@entry_id:178973)（CNN）已经从计算机视觉领域的工程奇迹，转变为[系统神经科学](@entry_id:173923)中理解大脑计算原理的核心工具。作为一类强大的[编码模型](@entry_id:1124422)，CNN能够以前所未有的准确性预测神经元对复杂自然刺激（如图像和声音）的响应。然而，这种成功也带来了一个挑战：如何系统性地理解这些复杂模型的内部运作机制，并将其与神经科学的理论框架相结合？我们如何将一个在ImageNet上训练的模型，有效地应用于fMRI或[电生理记录](@entry_id:198351)？这篇文章旨在填补这一知识鸿沟。

本文将通过三个章节，为读者提供一个关于[CNN编码模型](@entry_id:1122553)的全面指南。在第一章“原理与机制”中，我们将深入剖析CNN的架构基础，从编码与解码的根本区别，到[平移等变性](@entry_id:636340)等关键归纳偏置，再到[反向传播](@entry_id:199535)和[批量归一化](@entry_id:634986)等学习机制。接着，在第二章“应用与跨学科联系”中，我们将展示如何将这些原理应用于实际的神经科学问题，例如为fMRI和脉冲计数[数据建模](@entry_id:141456)，以及通过[表征相似性分析](@entry_id:1130877)比较大脑与模型的内在表征，并探索其在基因组学等领域的跨学科应用。最后，第三章“动手实践”将提供一系列精心设计的问题，帮助读者将理论知识转化为实践技能。让我们从深入理解CNN作为[神经编码](@entry_id:263658)模型的核心原理开始。

## 原理与机制

在理解了卷积神经网络（CNN）作为[神经编码](@entry_id:263658)模型的总体目标之后，本章将深入探讨其核心的原理与机制。我们将从区分编码与解码的基本概念出发，逐步解构CNN的架构蓝图，阐明其关键的对称性与[归纳偏置](@entry_id:137419)。随后，我们将探讨模型如何通过[基于梯度的优化](@entry_id:169228)进行学习，以及诸如归一化等关键技术如何[稳定训练](@entry_id:635987)过程。最后，我们将讨论如何从神经科学的视角诠释这些模型，并介绍一些高级主题，包括模型的可辨识性以及如何通过精心设计的实验来检验其作为[机制模型](@entry_id:202454)的有效性。

### 编码与解码：一个根本性的区分

在[系统神经科学](@entry_id:173923)中，我们的核心任务是理解刺激（stimulus）与神经响应（neural response）之间的关系。这一任务可以从两个互补的角度来探讨：编码（encoding）与解码（decoding）。明确区分这两者对于构建和评估[计算模型](@entry_id:637456)至关重要。

**编码模型**旨在预测给定刺激时神经元的响应。在概率的语言中，如果我们将刺激表示为[随机变量](@entry_id:195330) $X$，神经响应表示为[随机变量](@entry_id:195330) $Y$，那么编码模型的目标就是对[条件概率分布](@entry_id:163069) $p(y | x, \theta)$ 进行建模。这里的 $\theta$ 代表了模型的一组可学习参数。例如，一个[CNN编码模型](@entry_id:1122553) $f_\theta$ 会将一个图像 $x$ 映射到一个或多个参数，这些参数定义了神经响应的分布，比如一个[泊松分布](@entry_id:147769)的率参数 $\lambda_\theta(x)$。训练这样的模型通常采用**[最大似然估计](@entry_id:142509)**（Maximum Likelihood Estimation, MLE）。对于一个[独立同分布](@entry_id:169067)的数据集 $\mathcal{D}=\{(x_i,y_i)\}_{i=1}^N$，其目标是最大化观测到响应数据的（对数）条件[似然](@entry_id:167119)：

$$ \mathcal{L}_{\text{enc}}(\theta) = \sum_{i=1}^N \log p(y_i | x_i, \theta) $$

**解码模型**则执行相反的任务：它试图从神经响应中推断出导致该响应的刺激。这对应于对[条件概率分布](@entry_id:163069) $p(x | y, \phi)$ 进行建模。

一个常见的误解是，编码和解码本质上是同一问题的两个侧面，因此优化其中一个就等于优化另一个。然而，从统计学角度看，这是不正确的。通过**贝叶斯定理**，我们可以清晰地看到二者的关系：

$$ p(x | y, \theta) = \frac{p(y | x, \theta) p(x)}{p(y | \theta)} $$

其中 $p(y | \theta) = \int p(y | x, \theta) p(x) dx$ 是边缘[似然](@entry_id:167119)或证据（evidence）。这个公式表明，即使我们拥有一个完美的编码模型 $p(y | x, \theta)$，要构建一个最优的[贝叶斯解码](@entry_id:1121462)器，我们还必须指定一个关于刺激的**先验分布** $p(x)$。在没有关于刺激统计的先验知识的情况下，我们无法唯一地从编码模型中得到解码模型。因此，最大化解码[似然](@entry_id:167119) $\sum_i \log p(x_i | y_i, \theta)$ 与最大化编码[似然](@entry_id:167119) $\sum_i \log p(y_i | x_i, \theta)$ 是两个不同的优化问题，通常会得到不同的参数解 。

对编码模型的评估，其核心标准是其在未见过的数据（留出集）上的预测性能。一个良好校准的[编码模型](@entry_id:1124422)应该能准确预测响应的完整分布，而不仅仅是均值。因此，评估指标应该是**严格的评分规则**（strictly proper scoring rules），它能够对整个预测分布的质量进行打分。留出数据的平均[对数似然](@entry_id:273783)，$\frac{1}{N_{\text{test}}} \sum_{i=1}^{N_{\text{test}}} \log p(y_i | x_i, \theta_{\text{trained}})$，正是这样一个严格的评分规则。它奖励那些不仅预测准确，而且对其预测的不确定性有良好估计的模型 。

### CNN编码器的架构蓝图

CNN之所以在视觉神经科学中成为强大的编码模型，源于其独特的架构设计，该设计巧妙地融合了一系列操作，使其特别适合处理具有空间结构的刺激数据。一个典型的CNN层块通常按顺序包含卷积、偏置添加、[非线性激活](@entry_id:635291)、池化和归一化等操作 。

#### [卷积算子](@entry_id:747865)与[权重共享](@entry_id:633885)

**卷积**（Convolution）是CNN的核心。对于一个输入为 $H \times W \times C_{\text{in}}$（高、宽、通道数）的刺激张量 $X$，一个卷积层使用一组可学习的**核**（kernels）或**滤波器**（filters）$K$ 来提取特征。每个滤波器在输入张量的空间维度上滑动，计算[局部感受野](@entry_id:634395)内的加权和，从而生成一个二维的**[特征图](@entry_id:637719)**（feature map）。如果一个层有 $M$ 个滤波器，它就会产生 $M$ 个[特征图](@entry_id:637719)。对于第 $m$ 个[特征图](@entry_id:637719)，在空间位置 $(i, j)$ 的值可以通过以下[离散卷积](@entry_id:160939)公式计算：

$$ (K * X)_{i,j,m} = \sum_{u=0}^{k_h-1} \sum_{v=0}^{k_w-1} \sum_{c=1}^{C_{\text{in}}} K_{u,v,c,m} X_{i-u, j-v, c} $$

这里，$K \in \mathbb{R}^{k_h \times k_w \times C_{\text{in}} \times M}$ 是滤波器张量，在[深度学习](@entry_id:142022)实践中，通常实现的是**[互相关](@entry_id:143353)**（cross-correlation）操作，其形式为 $X_{i+u, j+v, c}$，但这只是符号约定上的差异。卷积之后，通常会为每个[特征图](@entry_id:637719)添加一个可学习的标量**偏置**（bias）$b_m$。

卷积操作最关键的特性是**[权重共享](@entry_id:633885)**（weight sharing）。同一个滤波器 $K_{\cdot,\cdot,\cdot,m}$ 在输入的所有空间位置上被重复使用，以生成第 $m$ 个[特征图](@entry_id:637719)。这一特性极大地提高了模型的参数效率。为了量化这种效率，我们可以将其与一个没有[权重共享](@entry_id:633885)的**局部连接层**（locally connected layer）进行比较。在局部连接层中，输出的每个空间位置都有自己独立的一套权重。假设输入和输出的空间维度均为 $H \times W$，[感受野大小](@entry_id:634995)为 $k \times k$，输入通道为 $C_{\text{in}}$，输出通道为 $C_{\text{out}}$。

-   对于卷积层，参数数量 $N_{\text{conv}}$ 包括 $C_{\text{out}}$ 个大小为 $k \times k \times C_{\text{in}}$ 的滤波器和 $C_{\text{out}}$ 个偏置，总计 $N_{\text{conv}} = (k^2 C_{\text{in}} C_{\text{out}}) + C_{\text{out}}$。
-   对于局部连接层，每个输出位置 $(H \times W)$ 都需要一套独立的 $k^2 C_{\text{in}} C_{\text{out}}$ 个权重和 $C_{\text{out}}$ 个偏置，总计 $N_{\text{loc}} = (H W k^2 C_{\text{in}} C_{\text{out}}) + (H W C_{\text{out}})$。

两者参数数量之比为：

$$ \rho = \frac{N_{\text{conv}}}{N_{\text{loc}}} = \frac{C_{\text{out}}(k^2 C_{\text{in}} + 1)}{H W C_{\text{out}}(k^2 C_{\text{in}} + 1)} = \frac{1}{HW} $$

这个简单的结果  惊人地揭示了[权重共享](@entry_id:633885)的威力：参数数量减少了 $H \times W$ 倍，即输出空间位置的总数。这种效率使得我们可以在有限的参数预算内构建更深、更宽的网络。

#### [非线性激活](@entry_id:635291)与池化

在[卷积和](@entry_id:263238)偏置相加之后，会对结果进行逐元素的**[非线性激活](@entry_id:635291)**（non-linear activation）。常用的[激活函数](@entry_id:141784)包括**[修正线性单元](@entry_id:636721)**（Rectified Linear Unit, ReLU），其定义为 $\phi(z) = \max(0, z)$，以及其平滑近似**softplus**函数 $\phi(z) = \ln(1 + \exp(z))$。这些[非线性](@entry_id:637147)操作是至关重要的，因为没有它们，多层网络将退化为一个简单的[线性模型](@entry_id:178302)，从而失去其强大的[表达能力](@entry_id:149863)。

**池化**（Pooling）是一种[下采样](@entry_id:926727)操作，它聚合了[特征图](@entry_id:637719)上一个局部邻域内的信息。常见的池化操作有**[平均池化](@entry_id:635263)**（average pooling）和**[最大池化](@entry_id:636121)**（max pooling）。例如，在一个 $s_h \times s_w$ 的非重叠窗口上，[平均池化](@entry_id:635263)计算窗口内所有值的平均值。池化的作用有二：一是通过降低空间分辨率来减少计算量和参数数量；二是在特征表示中引入一定程度的局部[平移不变性](@entry_id:195885)，因为一个特征在池化窗口内的微小移动可能不会改变池化的输出。

### 对称性与[归纳偏置](@entry_id:137419)：等变性与[不变性](@entry_id:140168)

CNN 的成功不仅在于其参数效率，更在于其架构中蕴含的强大**[归纳偏置](@entry_id:137419)**（inductive bias），即**[平移等变性](@entry_id:636340)**（translation equivariance）。这个性质使其天然适合处理像自然图像这样的信号。

我们可以使用群论的语言来精确地形式化这个概念 。让 $G=(\mathbb{R}^2, +)$ 成为平移群，其元素 $a \in \mathbb{R}^2$ 对一个函数 $x: \mathbb{R}^2 \to \mathbb{R}$ 的作用定义为 $(T_a x)(u) = x(u-a)$。一个函数（或算子）$f$ 如果满足 $f(T_a x) = T_a f(x)$，则称其为平移等变的。这意味着对输入进行平移，其输出也会相应地平移，但输出的模式本身保持不变。

可以证明，卷积操作正是一个平移等变算子。令 $y = f_k(x) = x * k$，其中 $*$ 表示卷积。我们可以推导：

$$ (f_k(T_a x))(u) = \int (T_a x)(u-z) k(z) dz = \int x(u-z-a) k(z) dz $$

令 $v = u-a$，则 $u = v+a$，上式变为：

$$ \int x(v-z) k(z) dz = (x * k)(v) = (x * k)(u-a) = (T_a (x*k))(u) = (T_a f_k(x))(u) $$

因此，$f_k(T_a x) = T_a f_k(x)$，证明了卷积的[平移等变性](@entry_id:636340)。逐点[非线性激活函数](@entry_id:635291)，如ReLU，由于其独立作用于每个点，也保持了等变性。

然而，当CNN中引入带**步幅**（stride）的池化或卷积时，严格的[平移等变性](@entry_id:636340)会被打破。如果步幅为 $s > 1$，网络输出的空间网格会比输入更粗糙。此时，模型只对特定大小的平移——即步幅 $s$ 的整数倍——保持等变性 。这种性质有时被称为**近似[等变性](@entry_id:636671)**，它在构建[特征层次结构](@entry_id:636197)中扮演了关键角色，使得高层特征对输入的微小扰动更加鲁棒。

最终，通过在网络顶层使用**全局池化**（global pooling），例如对整个[特征图](@entry_id:637719)取平均值或最大值，模型可以实现完全的**[平移不变性](@entry_id:195885)**（translation invariance）。一个不变的表示 $G(y)$ 满足 $G(T_a y) = G(y)$，即无论特征在空间中如何移动，最终的输出都保持不变。这对于[分类任务](@entry_id:635433)非常有用，因为物体的类别不应依赖于其在图像中的位置。

### 学习机制：训练编码器

一个CNN编码器包含大量可学习的参数（卷积核、偏置等），这些参数需要通过在大量的“刺激-响应”数据对上进行训练来确定。这个过程依赖于基于梯度的优化算法。

#### 通过卷积层进行反向传播

训练的核心是计算[损失函数](@entry_id:634569)关于模型参数的梯度，然后使用梯度下降（或其变体，如Adam）来更新参数。**[反向传播](@entry_id:199535)**（backpropagation）算法通过链式法则高效地计算这些梯度。

让我们推导损失函数 $L$ 关于一个卷积核权重 $W$ 的梯度 。假设一个[前向传播](@entry_id:193086)过程为 $y = W \star x$（这里 $\star$ 表示互相关），损失 $L$ 是关于输出 $y$ 的函数。我们定义误差图 $\delta = \frac{\partial L}{\partial y}$，它表示损失函数关于每个输出单元的梯度。根据链式法则，损失关于特定权重 $W_{o,c,i,j}$ 的梯度为：

$$ \frac{\partial L}{\partial W_{o,c,i,j}} = \sum_{n,u,v} \frac{\partial L}{\partial y_{n,o,u,v}} \frac{\partial y_{n,o,u,v}}{\partial W_{o,c,i,j}} = \sum_{n,u,v} \delta_{n,o,u,v} \frac{\partial y_{n,o,u,v}}{\partial W_{o,c,i,j}} $$

其中 $y_{n,o,u,v} = \sum_{c',i',j'} W_{o,c',i',j'} X_{n,c',u+i',v+j'}$。对 $W_{o,c,i,j}$ 求导，只有当索引匹配时才不为零，因此 $\frac{\partial y_{n,o,u,v}}{\partial W_{o,c,i,j}} = X_{n,c,u+i,v+j}$（假设 $o$ 匹配）。代入后得到：

$$ \frac{\partial L}{\partial W_{o,c,i,j}} = \sum_{n=1}^{N} \sum_{u=0}^{H'-1} \sum_{v=0}^{W'-1} X_{n,c,u+i,v+j} \delta_{n,o,u,v} $$

这个表达式的形式正是输入张量 $X$ 与误差图 $\delta$ 之间的互相关。这个优雅的结果揭示了一个深刻的联系：用于更新权重的梯度计算本身就是一个类卷积操作。这使得整个学习过程可以在现代硬件（如GPU）上高效实现。

#### [稳定训练](@entry_id:635987)：[批量归一化](@entry_id:634986)的作用

训练[深度神经网络](@entry_id:636170)的一个主要挑战是**[内部协变量偏移](@entry_id:637601)**（internal covariate shift）。这个术语指的是在训练过程中，由于前一层参数的不断更新，导致后一层网络输入的分布持续发生变化的现象。这种不稳定性会减慢训练速度，并使得模型对参数初始化更加敏感。

**[批量归一化](@entry_id:634986)**（Batch Normalization, BN）是一种被广泛采用的有效技术，用于缓解这一问题 。其核心思想是在网络的每一层激活函数之前，对输入进行归一化处理。对于一个小批量（mini-batch）的数据，BN层会计算该批量内特征的均值 $\mu_{\mathcal{B}}$ 和方差 $\sigma_{\mathcal{B}}^2$。然后，它使用这些统计量将每个特征值 $x_i$ [标准化](@entry_id:637219)，使其具有零均值和单位方差：

$$ \hat{x}_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}} $$

其中 $\epsilon$ 是一个很小的正数，用于防止除以零，保证数值稳定性。

然而，强制每一层的输入都为[标准正态分布](@entry_id:184509)可能会限制模型的表达能力。例如，对于一个sigmoid激活函数，这会将其输入限制在非饱和的[线性区](@entry_id:1127283)域。为了恢复模型的全部[表达能力](@entry_id:149863)，BN引入了两个可学习的参数：一个缩放因子 $\gamma$ 和一个平移因子 $\beta$。最终的输出为：

$$ y_i = \gamma \hat{x}_i + \beta $$

通过学习 $\gamma$ 和 $\beta$，网络可以选择性地调整归一化后特征的尺度和位置，甚至在必要时可以学习恢复原始的激活分布（通过令 $\gamma = \sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}$ 和 $\beta = \mu_{\mathcal{B}}$）。这样，BN在[稳定训练](@entry_id:635987)的同时，保留了模型的全部[表示能力](@entry_id:636759)。

### 诠释模型：从架构到神经科学

一个成功的[编码模型](@entry_id:1124422)不仅应能准确预测神经响应，还应能为我们理解大脑的计算原理提供洞见。这需要我们将模型的架构和学习到的参数与神经科学的理论联系起来。

#### 连接经典模型：LN与GLM

经典的[神经编码](@entry_id:263658)模型，如**线性-[非线性](@entry_id:637147)（LN）模型**和**广义线性模型（GLM）**，为我们理解神经计算提供了简洁而强大的框架。一个单层的CNN编码器可以被看作是这些经典模型的推广 。

一个**LN模型**首先通过一个线性滤波器作用于刺激，然后将结果通过一个静态的[非线性](@entry_id:637147)函数。一个包含卷积层和逐点[非线性](@entry_id:637147)的单层CNN正符合这个描述：卷积本身是一个线性操作（尽管是多输出的），其后跟一个静态的[非线性激活函数](@entry_id:635291)。因此，它可以被视为一个**广义LN模型**，其中“线性”部分由一组[滤波器实现](@entry_id:267605)，“[非线性](@entry_id:637147)”部分由激活函数和后续的线性读出等操作构成。

更有趣的是，在特定条件下，这个[CNN架构](@entry_id:635079)可以精确地简化为一个**GLM**。GLM假设神经响应（例如，脉冲计数）服从某个[指数族](@entry_id:263444)分布（如[泊松分布](@entry_id:147769)），其均值由一个**[线性预测](@entry_id:180569)器** $\eta(t)$ 通过一个**链接函数**（link function）的逆 $g^{-1}$ 来确定，即 $\mathbb{E}[y(t)|x] = g^{-1}(\eta(t))$。要使CNN编码器退化为GLM，必须满足以下条件：
1.  卷积层后的内部[非线性激活函数](@entry_id:635291) $\phi$ 必须是[恒等函数](@entry_id:152136)（或任何[仿射变换](@entry_id:144885)），以确保进入最后一环的预测器是刺激的线性函数。
2.  网络的最终输出[非线性](@entry_id:637147) $\psi$ 必须精确地等于所选观测模型的逆链接函数。例如，如果假设脉冲发放服从[泊松分布](@entry_id:147769)，那么逆链接函数是指数函数，即 $\psi(\eta) = \exp(\eta)$。

在这种情况下，整个CNN模型就等价于一个GLM，其[线性滤波器](@entry_id:1127279)是由卷积核与最终线性读出权重组合而成的有效滤波器。这个联系为我们从经典模型的角度理解和分析CNN的行为提供了理论基础。

#### 涌现的表征与高效编码

当一个CNN编码器在自然图像数据集上进行训练时，其第一层的滤波器往往会自发地学习到类似于**[Gabor滤波器](@entry_id:1125441)**的结构——即局域化的、有方向的、带通的模式。这一现象并非偶然，它深刻地反映了自然图像的统计特性以及大脑视觉系统可能遵循的**[高效编码假说](@entry_id:893603)**（efficient coding hypothesis）。

这一现象的出现可以通过**[稀疏编码](@entry_id:180626)**（sparse coding）和**[独立成分分析](@entry_id:261857)**（Independent Component Analysis, ICA）的理论来解释 。自然图像的一个显著特征是，尽管像素之间高度相关，但它们的结构（如边缘和轮廓）在空间上是稀疏的。稀疏编码理论认为，一个高效的表示应该用少数几个“激活”的基函数来描述一个信号。为了用最少的基函数表示自然图像，这些基函数本身必须匹配图像中的基本结构，即边缘。根据海森堡不确定性原理，[Gabor函数](@entry_id:1125442)是在空间域和频率域同时达到最佳局部化的函数，因此它们是编码局部边缘特征的最优选择。

当训练CNN时，如果[目标函数](@entry_id:267263)中包含一个促进激活值稀疏性的正则化项（如 $\ell_1$ 范数），模型就会被激励去寻找一种[稀疏表示](@entry_id:191553)。在对移除了[二阶相关](@entry_id:190427)性（即“白化”处理）的自然图像进行训练时，算法被迫去发现更高阶的统计结构，即稀疏的边缘成分。这与ICA的目标不谋而合，ICA旨在寻找一组滤波器，使得其输出尽可能地统计独立。将ICA应用于自然图像，得到的基函数正是[Gabor函数](@entry_id:1125442)。

因此，一个在自然图像上训练、旨在预测V1神经元（其本身对方向和频率敏感）响应、并带有[稀疏性](@entry_id:136793)约束的CNN，其第一层学习到[Gabor滤波器](@entry_id:1125441)，可以被看作是模型发现了数据中固有的、统计上高效的表征方式。

### 高级主题与模型属性

随着我们对CNN编码器应用的深入，一些更高级的理论和实践问题也浮出水面。

#### 控制[感受野](@entry_id:636171)：[空洞卷积](@entry_id:636365)

在[处理时间](@entry_id:196496)序列或大尺寸图像时，我们希望模型能够在不牺牲分辨率或急剧增加计算成本的情况下，整合大范围的上下文信息。**[空洞卷积](@entry_id:636365)**（dilated convolution），也称[扩张卷积](@entry_id:636365)，提供了一种优雅的解决方案 。

在标准的卷积中，滤波器在输入的相邻元素上进行操作。而在[空洞卷积](@entry_id:636365)中，滤波器元素之间被插入了“空洞”，其间距由**扩张率**（dilation rate）$d$ 控制。扩张率为 $d$ 意味着滤波器的权重会应用于每隔 $d$ 个位置的输入。通过在堆叠的卷积层中指数级地增加扩张率（例如，$d = [1, 2, 4, 8, \dots]$），模型的**[有效感受野](@entry_id:637760)**（effective receptive field）可以呈指数级增长，而参数数量仅线性增加。对于一个由 $L$ 层组成的网络，如果每层的核大小为 $k$，扩张率分别为 $d_1, \dots, d_L$，则其最终的[有效感受野](@entry_id:637760)长度 $R_L$ 为：

$$ R_L = 1 + (k-1) \sum_{i=1}^{L} d_i $$

例如，一个三层网络，核大小 $k=3$，扩张率 $d=[1, 2, 4]$，其[感受野大小](@entry_id:634995)为 $1 + (3-1)(1+2+4) = 15$。这种能力对于捕捉[长程依赖](@entry_id:181727)关系至关重要。

#### 模型的可辨识性

**[参数可辨识性](@entry_id:197485)**（parameter identifiability）是统计建模中的一个基本概念。一个模型被称为可辨识的，如果其参数到输入-输出映射的函数是[单射](@entry_id:183792)的。换句话说，如果两组不同的参数 $\theta$ 和 $\theta'$ 产生了完全相同的输入-输出函数，那么这个模型就是不可辨识的。

具有ReLU[非线性](@entry_id:637147)的CNN通常是**不可辨识的** 。这种[不可辨识性](@entry_id:1128800)源于网络内部的对称性。考虑一个中间层 $l$，我们可以对其输出通道进行任意的**置换**（permutation），并对下一层（第 $l+1$ 层）的输入通道应用该置换的逆操作，而网络的最终输出保持不变。此外，由于ReLU具有**[正齐次性](@entry_id:262235)**（positive homogeneity），即 $\phi(\alpha u) = \alpha \phi(u)$ 对所有 $\alpha \ge 0$ 成立，我们可以用任意正数缩放一个通道的输出，并在下一层用其倒数进行补偿，同样不会改变网络的功能。

综合这两种对称性，任何由通道置换和正向缩放构成的变换，都可以应用于某一层，并通过在下一层应用其逆变换来精确抵消，从而产生一组全新的、功能上等价的参数。这种内在的对称性意味着损失函数的景观中存在连续的谷或多个等价的[全局最小值](@entry_id:165977)，这对参数的解释提出了挑战，但通常不影响模型的预测性能。

#### [超越函数](@entry_id:271750)逼近：检验机制性假说

当一个[CNN编码模型](@entry_id:1122553)在预测神经响应方面取得成功时，我们面临一个更深层次的问题：这个模型仅仅是一个性能优越的“黑箱”**[函数逼近](@entry_id:141329)器**，还是它在某种意义上捕捉了大脑进行相关计算的**真实机制**？这是一个从相关性到因果性、从描述到解释的关键一步。

要回答这个问题，我们不能仅仅依赖模型在训练数据分布内的表现。一个真正的[机制模型](@entry_id:202454)应该能对其所声称的机制做出可[证伪](@entry_id:260896)的预测，这些预测应该在训练分布之外的、专门设计的**分布外**（out-of-distribution）刺激下依然成立。

让我们以一个具体的例子来说明 。假设一个关于V1复杂细胞的机制性假说 $\mathcal{H}$（能量模型）声称，神经元的响应只依赖于刺激在特定方向和[空间频率](@entry_id:270500)带内的“能量”，而对该频带内的傅里叶相位不敏感。一个仅在自然图像上训练的CNN可能表现出[相位不变性](@entry_id:1129584)，但这可能只是因为它利用了自然图像中固有的相位相关性。

一个严谨的检验方法是，创建一个新的刺激分布 $q(x)$，其中的图像与自然图像具有相同的振幅谱，但其相位被随机打乱。然后，我们可以向模型呈现成对的刺激 $(\mathbf{x}, T(\mathbf{x}))$，其中 $T$ 是一个只改变相位而不改变振幅的变换。如果机制假说 $\mathcal{H}$ 是正确的，那么模型的预测应该对这种变换不敏感，即 $f_\theta(\mathbf{x}) \approx f_\theta(T(\mathbf{x}))$。我们可以通过计算一个差异统计量 $D = \mathbb{E}_{\mathbf{x}\sim q} |f_{\theta}(\mathbf{x}) - f_{\theta}(T(\mathbf{x}))|$ 来定量评估这一点。如果 $D$ 显著大于一个预设的阈值，我们就可以证伪假说 $\mathcal{H}$。

这种通过挑战模型的特定不变性来进行检验的方法，是推动[编码模型](@entry_id:1124422)从纯粹的工程工具转变为真正的科学理论的关键。它迫使我们明确模型所做的假设，并以一种严谨的、可[证伪](@entry_id:260896)的方式来面对数据。