## 引言
随着神经影像技术的发展，[网络神经科学](@entry_id:1128529)已成为理解大脑结构与功能的关键范式。研究者能够构建出描绘大脑区域间物理连接（结构连接体）和功能协作（[功能连接](@entry_id:196282)体）的复杂网络。然而，如何从这些高维、非欧几里得结构的连接体数据中提取有意义的生物学见解和[临床生物标志物](@entry_id:183949)，仍然是一个巨大的挑战。传统的机器学习方法往往难以充分利用网络固有的拓扑信息，从而限制了我们对大脑的系统级理解。

[图神经网络](@entry_id:136853)（Graph Neural Networks, GNNs）作为深度学习领域的一项革命性技术，为解决这一难题提供了强有力的工具。GNN专门设计用于处理图结构数据，能够学习到同时编码节[点特征](@entry_id:155984)和[网络拓扑](@entry_id:141407)的深度表示，完美契合了大脑连接体分析的需求。本文旨在全面阐述如何将GNN应用于大脑连接体研究，为相关领域的科研人员和学生提供一个从理论到实践的系统性指南。

为实现这一目标，本文将分为三个核心章节。在“原理与机制”一章中，我们将深入探讨GNN的基础理论，包括如何将大脑连接体数据转化为[图表示](@entry_id:273102)，GNN的核心消息传递机制，以及在处理大[脑网络](@entry_id:912843)时需要考虑的关键设计原则与挑战。接下来，在“应用与跨学科连接”一章中，我们将展示GNN在解决实际神经科学问题中的广泛应用，例如用于临床诊断与预测、建模大脑[时空动力学](@entry_id:1132003)，以及整合[多模态数据](@entry_id:635386)，并探讨其如何促进与临床神经学、[精神病](@entry_id:893734)学等领域的交叉融合。最后，“动手实践”部分将提供一系列编程练习，引导读者亲手实现将大脑数据转换为图、处理数据不完整性等关键步骤，巩固理论知识并培养实践技能。通过这一结构化的学习路径，读者将能够系统地掌握使用GNN分析大脑连接体的前沿方法。

## 原理与机制

本章深入探讨了将[图神经网络](@entry_id:136853)（GNN）应用于大脑连接体分析的核心科学原理与计算机制。我们将从如何将不同类型的神经影像[数据表示](@entry_id:636977)为数学上的图开始，然后阐述GNN如何处理这些图，并讨论其在信息传播、[表达能力](@entry_id:149863)以及架构设计方面所面临的关键挑战与先进解决方案。我们的目标是为读者构建一个坚实的理论基础，以便理解、设计和批判性地评估用于神经科学研究的GNN模型。

### 从大脑连接体到图：[基本表示](@entry_id:157678)

将大[脑连接](@entry_id:152765)体分析问题转化为GNN可以处理的格式，第一步是精确地定义图的节点（nodes）和边（edges）。在宏观尺度的大[脑网络](@entry_id:912843)中，**节点**通常代表根据特定解剖学或功能图谱（atlas）划分的**感兴趣区域（Regions of Interest, ROIs）**。而连接这些节点的**边**则代表了区域间的关系，其定义取决于所使用的神经影像模态和分析目标。理解不同连接性（connectivity）的生物学起源和数学属性，对于构建有意义的GNN模型至关重要。

#### 结构连接（Structural Connectivity, SC）

**结构连接**代表大脑区域间物理存在的轴突通路，通常通过**[扩散磁共振成像](@entry_id:1123713)（diffusion Magnetic Resonance Imaging, dMRI）**的纤维束追踪（tractography）技术进行估计。从dMRI数据构建的结构连接矩阵，我们通常记为 $A \in \mathbb{R}^{n \times n}$（其中 $n$ 是ROI的数量），其元素 $A_{ij}$ 表示连接区域 $i$ 和区域 $j$ 的白质纤维束的某种量化指标，例如纤维束数量或密度。基于其生物物理基础和当前技术的限制，结构连接图具有以下关键属性 ：
*   **非负性（Non-negativity）**：$A_{ij} \ge 0$。纤维束追踪量化的是物理连接的存在与强度，而非其兴奋性或抑制性，因此权重为非负值。
*   **对称性（Symmetry）**：$A_{ij} = A_{ji}$。标准的dMRI纤维束追踪技术无法确定轴突信号传递的方向，因此连接被建模为无向的。
*   **零对角线（Zero Diagonal）**：$A_{ii} = 0$。结构连接矩阵描述的是区域*之间*的连接。

在GNN模型中，结构连接矩阵 $A$ 自然地扮演了**邻接矩阵**的角色，定义了信息可以在哪些路径上进行传递。它构成了大脑通信网络的物理骨架（physical substrate）。

#### 功能连接（Functional Connectivity, FC）

**功能连接**则衡量不同大脑区域神经活动在时间上的统计依赖关系，通常通过分析**功能[磁共振成像](@entry_id:153995)（functional MRI, fMRI）**的血氧水平依赖（Blood-Oxygen-Level-Dependent, BOLD）信号或脑电图（EEG）时间序列来计算。例如，[功能连接矩阵](@entry_id:1125379) $C \in \mathbb{R}^{n \times n}$ 的元素 $C_{ij}$ 可以定义为区域 $i$ 和 $j$ 的BOLD时间序列之间的[皮尔逊相关系数](@entry_id:918491)。[功能连接](@entry_id:196282)具有与结构连接截然不同的属性 ：
*   **可正可负（Signed）**：$C_{ij} \in [-1, 1]$。正值表示区域活动同步（协同激活），负值表示反相关（如默认网络与任务正相关网络之间的关系），零值表示无线性关系。
*   **对称性（Symmetry）**：$C_{ij} = C_{ji}$。因为相关性是衡量两个变量间对称关系的指标。
*   **单位对角线（Unit Diagonal）**：$C_{ii} = 1$。任何时间序列与其自身的相关性都为1。
*   **稠密性（Density）**：[功能连接矩阵](@entry_id:1125379)通常是稠密的，即几乎所有区域对之间都存在非零的相关性。

一个至关重要的概念是，[功能连接](@entry_id:196282)是在结构连接这一物理骨架上涌现出的动态现象。如一个简化的动力学模型 $\frac{d\mathbf{x}(t)}{dt} = -\alpha \mathbf{x}(t) + \beta A \mathbf{x}(t) + \boldsymbol{\xi}(t)$ 所示，神经活动 $\mathbf{x}(t)$ 的传播受到[结构矩阵](@entry_id:635736) $A$ 的约束，而我们观测到的fMRI信号又是神经活动经过血流动力学[响应函数](@entry_id:142629)（HRF）卷积的结果。因此，功能连接 $C$ 是底层结构 $A$、神经动力学和[血流动力学](@entry_id:1121718)共同作用的结果。

基于此，将功能连接直接用作GNN的[邻接矩阵](@entry_id:151010)在方法论上是值得商榷的，尤其当任务目标是预测与功能相关的变量时。这样做会陷入循[环论](@entry_id:143825)证，因为功能连接是待解释的现象而非解释机制。更合理的做法是，将结构连接 $A$作为定义消息传递路径的邻接矩阵，而将功能连接 $C$ 作为**边级别特征（edge-level features）**或**监督信号（supervision signal）**，从而让GNN学习如何从[结构预测](@entry_id:1132571)功能。例如，可以设计GNN架构，利用 $C$ 中的值来调节（modulate）在 $A$ 定义的路径上传递的消息 。

#### 有效连接（Effective Connectivity, EC）

**有效连接**旨在描述一个大脑区域对另一个区域的**有向因果影响（directed causal influence）**。与[功能连接](@entry_id:196282)不同，有效连接本质上是**有向的（directed）**。例如，通过格兰杰因果分析（Granger Causality）等方法从EEG时间序列中估计出的有效连接矩阵 $A_{EC}$，其元素 $(A_{EC})_{ij}$ 表示区域 $i$ 对区域 $j$ 的影响强度，通常 $(A_{EC})_{ij} \neq (A_{EC})_{ji}$。标准EC估计算法（如DTF、PDC）通常产生非负权重。

当使用有效连接作为GNN的邻接矩阵时，必须尊重其有[向性](@entry_id:144651)。对于有向图，[消息传递](@entry_id:751915)和归一化方案需要做出相应调整。例如，对于[无向图](@entry_id:270905)常用的对称归一化 $D^{-1/2} A D^{-1/2}$ 不再适用，而应采用适合[有向图](@entry_id:920596)的归一化方法，如行归一化（row-normalization）$D_{\text{out}}^{-1} A$，其中 $D_{\text{out}}$ 是出度矩阵。这种归一化方法创建了一个[行随机矩阵](@entry_id:1131129)，与有向[图上的[随机游](@entry_id:273358)走过程](@entry_id:171699)相对应，为模拟信息在[有向网络](@entry_id:920596)中的流动提供了稳定的数学基础 。

### 图神经网络的核心机制：[消息传递](@entry_id:751915)

GNN的核心思想是通过迭代式的**[消息传递](@entry_id:751915)（message passing）**，让每个节点能够聚合其邻域的信息，从而学习到能够反映其在网络中所处结构环境的表示（representation）。一个通用的**[消息传递神经网络](@entry_id:751916)（Message Passing Neural Network, [MPN](@entry_id:910658)N）**框架可以概括为以下三个步骤的重复执行：

1.  **消息计算（Message Computation）**：对于图中的每一对邻居节点 $(i, j)$，一个共享的**消息函数** $\psi$ 会根据它们的特征计算出从 $j$ 发送到 $i$ 的消息 $m_{i \leftarrow j}$。
2.  **消息聚合（Message Aggregation）**：每个节点 $i$ 会聚合所有来自其邻居 $\mathcal{N}(i)$ 的消息。聚合操作 $\bigoplus$ 必须是**置换不变的（permutation-invariant）**，即无论邻居的顺序如何，聚合结果都应相同。常见的聚合函数包括求和（sum）、均值（mean）或最大值（max）。聚合后的消息为 $M_i = \bigoplus_{j \in \mathcal{N}(i)} m_{i \leftarrow j}$。
3.  **节点更新（Node Update）**：最后，一个共享的**[更新函数](@entry_id:275392)** $\gamma$ 会结合节点 $i$ 当前的表示 $h_i$ 和聚合后的消息 $M_i$，计算出该节点在下一层的新表示 $h_i' = \gamma(h_i, M_i)$。

#### [置换对称性](@entry_id:185825)：GNN的基石

在处理大脑图谱时，ROI的索引（例如，从1到$N$的编号）通常是任意的。一个鲁棒的分析模型不应该因为我们改变了对ROI的编号顺序而改变其对大脑的分析结果。这引出了GNN设计中两个至关重要的对称性原则 ：

*   **置换[等变性](@entry_id:636671)（Permutation Equivariance）**：GNN的每一层（节点更新操作）必须是等变的。这意味着，如果我们将输入节点的顺序进行任意置换（permuting），输出的节点表示也应该以完全相同的顺序进行置换。换言之，节点的计算只依赖于其自身的[特征和](@entry_id:189446)邻域结构，而与其任意分配的索引无关。
*   **[置换不变性](@entry_id:753356)（Permutation Invariance）**：对于图级别的预测任务（例如，预测受试者的临床诊断），最终的图表示（readout）必须是置换不变的。这意味着无论节点如何排序，最终的输出都保持不变。

[MPN](@entry_id:910658)N框架通过**共享函数**（$\psi, \gamma$对所有节点和边都相同）和**置换不变的聚合器**（如求和）来天生满足置换[等变性](@entry_id:636671)。对于图级别任务，可以通过对所有最终节点表示进行置换不变的读出操作（readout function）来实现[置换不变性](@entry_id:753356)，例如，对所有节点的表示求和或求均值，然后再输入到一个分类器中。任何破坏这些原则的设计，例如为每个ROI[索引分配](@entry_id:750607)一个独立的网络模块，或在聚合邻居信息时依赖于其索引顺序，都会引入对任意节点编号的依赖，这在科学上是无根据的，并且会损害模型的泛化能力 。

当处理包含节点和边特征的[多模态数据](@entry_id:635386)时，这些特征可以被无缝地整合到[消息传递](@entry_id:751915)框架中。例如，我们可以定义一个[节点嵌入](@entry_id:1128746)函数 $h_i = \phi_x(x_i)$ 和一个边嵌入函数 $g_{ij} = \phi_e(e_{ij})$，然后将它们作为消息函数 $\psi(h_i, h_j, g_{ij})$ 的输入。只要这些嵌入函数和消息函数在所有节点和边之间共享，整个架构就能保持必要的[置换对称性](@entry_id:185825)。

### 信息在网络中的传播：感受野与过平滑

GNN通过堆叠多个消息传递层来让信息在网络中传播更远。一个节点的最终表示受到其在图中一定范围内的邻居的影响，这个影响范围被称为该节点的**[感受野](@entry_id:636171)（receptive field）**。

#### 感受野的扩展

在一个标准的GCN（Graph Convolutional Network）中，每一层将信息从节点的1跳（1-hop）邻居传递过来。因此，经过 $K$ 层[消息传递](@entry_id:751915)后，一个节点的表示会受到其 $K$ 跳距离内所有节点初始特征的影响。从理论上讲，一个 $K$ 层GCN的感受野最大拓扑跳数就是 $K$。更一般地，如果GNN的每一层都设计为可以聚合来自 $r$ 跳邻域的信息，那么经过 $K$ 层后，[感受野](@entry_id:636171)的最大跳数将是 $K \times r$ 。

这个线性扩展的感受野是GNN捕捉**中尺度（mesoscale）**网络模式的关键。中尺度模式，如[功能模块](@entry_id:275097)内部的连接模式或跨模块的特定通路，其空间跨度通常超出了节点的直接邻域。为了让GNN能够“看到”并学习一个跨越 $h$ 跳的模式，其[感受野](@entry_id:636171)必须足够大，即 $Kr \ge h$。

#### 过平滑的挑战

然而，[感受野](@entry_id:636171)并非越大越好。随着GNN层数 $K$ 的增加，一个节点的感受野会迅速扩展，最终可能覆盖图中的大部分甚至所有节点。当感受野接近或超过[图的直径](@entry_id:271355)（graph diameter）时，所有节点的表示都会趋于收敛到一个相同的值（一种[加权平均值](@entry_id:894528)）。这种现象被称为**过平滑（over-smoothing）**。过平滑会“洗掉”节点之间具有区分性的局部信息，导致模型无法学习到有意义的节点或图级别的表示，从而损害性能。

因此，GNN的设计需要在捕捉必要的大尺度信息和避免破坏性过平滑之间进行权衡。为了有效地分析大脑连接体，必须仔细选择[网络深度](@entry_id:635360) $K$ 和层级半径 $r$，确保[感受野](@entry_id:636171) $Kr$ 足够大以覆盖感兴趣的生物模式，同时又远小于[图的直径](@entry_id:271355)，以保留节点的个性化信息 。

### [图信号处理](@entry_id:183351)视角：[谱方法](@entry_id:141737)与同质性

[图信号处理](@entry_id:183351)（Graph Signal Processing, GSP）为理解GNN的工作原理提供了另一个深刻的理论视角。它将图上的节[点特征](@entry_id:155984)视为“图信号”，并利用[谱图论](@entry_id:150398)的工具来分析这些信号的性质以及图算子（如GNN层）对其施加的变换。

#### [图傅里叶变换](@entry_id:187801)与[谱滤波](@entry_id:755173)

对于一个由加权邻接矩阵 $W$ 定义的[无向图](@entry_id:270905)，我们可以定义其**组合[拉普拉斯算子](@entry_id:146319)（combinatorial Laplacian）** $L = D - W$，其中 $D$ 是对角线上为节点加权度的度矩阵。由于 $L$ 是一个[对称半正定矩阵](@entry_id:163376)，它可以被[正交对角化](@entry_id:149411)为 $L = U \Lambda U^\top$。其中， $U$ 的列是 $L$ 的[特征向量](@entry_id:151813)，构成了**图[傅里叶基](@entry_id:201167)（graph Fourier basis）**；对角矩阵 $\Lambda$ 包含对应的特征值 $\lambda_1 \le \lambda_2 \le \dots \le \lambda_n$，它们可以被看作是图上的**频率（frequencies）**。

*   **低频**（小的 $\lambda_i$）对应于在图上平滑变化的[基向量](@entry_id:199546)。特征值 $\lambda_1=0$ 对应的[特征向量](@entry_id:151813)是一个常数向量，是图上最平滑的信号。
*   **高频**（大的 $\lambda_i$）对应于在连接的节点之间剧烈振荡的基向量。

任何定义在节点上的信号 $x \in \mathbb{R}^n$（例如，fMRI在各个ROI上的激活值）可以通过**[图傅里叶变换](@entry_id:187801)** $\hat{x} = U^\top x$ 投影到这个基上。许多GNN的卷积操作可以被理解为一种**谱[图滤波](@entry_id:193076)（spectral graph filtering）** 。一个[谱滤波](@entry_id:755173)器 $H$ 通过在频率域应用一个**滤波器响应函数** $g(\lambda)$ 来实现，其操作可以写为 $y = Hx = U g(\Lambda) U^\top x$。这类滤波器的一个重要性质是它们与拉普拉斯算子 $L$ 可交换（$HL = LH$），这标志着它们是图上的**线性移不变（linear shift-invariant）**算子。例如，选择 $g(\lambda) = \exp(-\tau\lambda)$ 就对应于图上的热[扩散过程](@entry_id:268015)，其作用是平滑信号 。

需要注意的是，由于每个受试者的大脑结构都是独一无二的，他们的拉普拉斯算子 $L^{(s)}$ 和对应的[傅里叶基](@entry_id:201167) $U^{(s)}$ 也各不相同。这意味着在不同受试者之间直接比较经过[谱滤波](@entry_id:755173)后的信号是困难的，因为这些信号是在不兼容的“频率”基上定义的。这是一个多受试者[图分析](@entry_id:750011)中的核心挑战 。

#### [同质性](@entry_id:636502)与异质性

[谱滤波](@entry_id:755173)的概念与图信号的**同质性（homophily）**和**[异质性](@entry_id:275678)（heterophily）**密切相关。
*   **[同质性](@entry_id:636502)**描述了“物以类聚”的现象，即在图上相互连接的节点倾向于具有相似的属性或特征。对于一个节点属性 $f$（例如，各区域的[髓鞘](@entry_id:149566)含量），高同质性意味着对于大多数边 $(u,v)$， $f(u)$ 和 $f(v)$ 的值都很接近。
*   **[异质性](@entry_id:275678)**则相反，描述了相互连接的节点倾向于具有不同属性的现象。

信号的平滑度可以通过**[狄利克雷能量](@entry_id:276589)（Dirichlet energy）** $E(f) = f^\top L f = \sum_{(u,v) \in E} W_{uv}(f(u)-f(v))^2$ 来量化。[狄利克雷能量](@entry_id:276589)越小，信号在图上就越平滑，同质性就越高 。

标准的[消息传递](@entry_id:751915)操作（如邻域平均）本质上是一种**低通滤波器**。它在同质性高的图上表现出色，因为它能够通过平均化来滤除节[点特征](@entry_id:155984)中的噪声，从而强化共同的“信号”。然而，在[异质性](@entry_id:275678)高的图上，节点间的差异本身就是重要的判别信息（即高频信号）。此时，低通滤波反而会“洗掉”这些宝贵的信息，导致模型性能下降 。在这种情况下，需要设计特殊的GNN架构，例如**符号感知（sign-aware）**的消息传递，它能够根据边的类型（例如，合作或对抗关系）来调整消息的聚合方式，从而在[异质性](@entry_id:275678)结构中也能有效地传播信息 。

### GNN架构的高级主题

随着研究的深入，一系列高级GNN架构被提出，以解决标准[消息传递](@entry_id:751915)模型的局限性。

#### [注意力机制](@entry_id:917648)

**[图注意力网络](@entry_id:1125735)（Graph Attention Network, GAT）**引入了**[注意力机制](@entry_id:917648)（attention mechanism）**，允许模型在聚合邻居信息时，为不同的邻居动态地分配不同的重要性权重。对于节点 $i$ 和其邻居 $j$，注意力系数 $\alpha_{ij}$ 通常通过一个可学习的函数计算得出，然后使用 softmax 函数进行归一化 ：
$$ \alpha_{ij} = \frac{\exp(\text{score}(h_i, h_j) / \tau)}{\sum_{k \in \mathcal{N}(i)} \exp(\text{score}(h_i, h_k) / \tau)} $$
其中 $\text{score}(\cdot, \cdot)$ 是一个衡量兼容性的函数，$\tau$ 是一个**温度（temperature）**参数，用于控制注意力分布的锐度。减小 $\tau$ 会使注意力更集中于少数几个邻居，而增大 $\tau$ 则会使分布更平滑。

虽然注意力权重在解释ROI间的重要性方面很有吸[引力](@entry_id:189550)，但必须谨慎解读。首先，注意力是**有向的**，即 $\alpha_{ij}$（$j$ 对 $i$ 的重要性）通常不等于 $\alpha_{ji}$（$i$ 对 $j$ 的重要性）。其次，由于 softmax 归一化的存在，一个边的注意力权重不仅取决于这对节点本身，还取决于中心节点的所有其他邻居。这意味着，在不同节点或不同图中（例如，由于阈值不同导致邻域大小不同），注意力权重的大小是不可直接比较的  。

#### 过挤压问题

**过挤压（Over-squashing）**是指在图的某些区域（瓶颈），大量长距离信息必须通过少数几条路径进行传递，导致信息被过度压缩和丢失。这在具有明显模块化结构的大[脑网络](@entry_id:912843)中尤为突出，其中模块间的连接（边割）相对稀疏 。

当信息需要从一个大模块通过一个小的边割传递到另一个模块时，每一层[消息传递](@entry_id:751915)都构成了信息流的瓶颈。仅仅增加GNN的深度并不能解决这个问题，因为这只是让信息在同一个瓶颈处反复排队，甚至可能因为多次[非线性变换](@entry_id:636115)而加剧信息损失。

有效的解决方案旨在拓宽瓶颈或创建绕过瓶颈的“捷径”。这些方法包括：
*   **增加边（Edge Augmentation）**：在图中增加新的边，特别是跨越瓶颈的长程连接，直接拓宽[信息通道](@entry_id:266393)。
*   **虚拟节点（Virtual Node）**：引入一个连接到图中所有节点的“全局虚拟节点”，任何两个节点都可以通过这个虚拟节点在两跳内完成通信。
*   **全局注意力（Global Attention）**：使用能够[计算图](@entry_id:636350)中所有节点对之间注意力的机制，从而创建一个有效的全连接图，信息可以直接在任意节点间流动，彻底绕开拓扑瓶颈 。

### [GNN的表达能力](@entry_id:637052)与大脑网络拓扑

GNN的**[表达能力](@entry_id:149863)（expressivity）**指的是其区分不同图结构的能力。一个表达能力有限的GNN可能会将两个结构不同（非同构）的图错误地映射到相同的表示，从而无法区分它们。

#### [表达能力](@entry_id:149863)与[WL测试](@entry_id:1134117)

标准[MPN](@entry_id:910658)N的[表达能力](@entry_id:149863)上限由**1维Weisfeiler-Lehman（1-WL）[图同构](@entry_id:143072)测试**确定 。1-[WL测试](@entry_id:1134117)是一个通过迭代地聚合邻居节点的离散标签（颜色）来为节点生成规范化标签的算法。如果两个图在某一步迭代后产生的标签（颜色）多重集不同，则它们被判定为非同构。[MPN](@entry_id:910658)N的消息传递过程在功能上与1-WL的迭代[更新过程](@entry_id:275714)等价。因此，任何[MPN](@entry_id:910658)N的判别能力都不会超过1-[WL测试](@entry_id:1134117)。

这一理论限制意味着，存在一些1-WL无法区分的[非同构图](@entry_id:274028)，任何标准的[MPN](@entry_id:910658)N也同样无法区分它们。一个典型的例子是某些**[正则图](@entry_id:265877)（regular graphs）**，即所有[节点度](@entry_id:1128744)都相同的图。对于这类图，如果所有节点初始特征相同，标准[MPN](@entry_id:910658)N在每一层都会为所有节点计算出完全相同的表示，从而无法捕捉到它们之间微妙的结构差异 。为了突破这一限制，研究者们提出了更高阶的GNN，其表达能力与更强大的 $k$-WL 测试相对应，但代价是计算复杂度的急剧增加。

#### 将[网络拓扑](@entry_id:141407)融入GNN设计

最后，一个富有成效的设计思路是将大[脑网络](@entry_id:912843)中已知的、具有生物学意义的拓扑特性，直接融入到GNN的架构设计或正则化项中。这不仅可能增强模型的性能，还能提高其[可解释性](@entry_id:637759) 。

*   **模块性（Modularity）**：大脑网络具有高度模块化的社区结构。这启发我们可以设计**层次化池化（hierarchical pooling）**操作，将一个模块内的节点聚合成一个超节点，或者为模块内和模块间的[消息传递](@entry_id:751915)使用不同的参数。
*   **小世界性（Small-worldness）**：大脑网络兼具高集群和短路径长度的特点。这表明模型应同时重视局部信息整合和全局[信息传播](@entry_id:1126500)。可以通过结合标准的多层局部[消息传递](@entry_id:751915)与稀疏的**长程连接（long-range connections）**或“图[跳跃连接](@entry_id:637548)”来实现。
*   **富人俱乐部（Rich-club）**：网络中的高度节点（“富节点”）倾向于彼此[紧密连接](@entry_id:170497)，形成一个高效的通信核心。GAT中的[注意力机制](@entry_id:917648)或**度感知（degree-aware）**的边权重可以被用来学习并强化这些核心通路上的信息流。
*   **同配性（Assortativity）**：大脑结构网络通常表现出正的[度同配性](@entry_id:1123505)，即高度节点倾向于连接其他高度节点。这是一种结构上的[同质性](@entry_id:636502)，提示我们可以选择与[同质性](@entry_id:636502)假设相符的聚合器和归一化方案，以更好地利用这一结构偏好。

通过将这些先验知识融入模型设计，我们可以引导GNN学习到更符合生物学现实的表示，从而推动神经科学的发现。