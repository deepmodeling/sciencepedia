{
    "hands_on_practices": [
        {
            "introduction": "图神经网络（GNNs）在图结构数据上运行，但大脑活动的原始数据通常是以时间序列的形式记录的（例如，功能性磁共振成像 fMRI）。因此，应用 GNN 的第一步，也是最关键的一步，是将这些时间序列数据转换为图，即功能连接体。本练习将引导你完成一个从原始时间序列构建功能连接体的标准流程，包括计算皮尔逊相关性、应用 Fisher $z$ 变换以稳定方差，以及通过比例阈值法生成稀疏图。通过这个实践，你将掌握如何为 GNN 准备功能性大脑数据，并理解图构建过程中的选择（如稀疏度）如何直接影响最终图的拓扑结构以及 GNN 的输入特征 。",
            "id": "4167854",
            "problem": "给定三个旨在模拟脑连接性的人工合成的感兴趣区域 (ROI) 时间序列数据集，每个数据集包含 $n$ 个区域和 $T$ 个时间点。任务是通过计算皮尔逊相关矩阵，应用费雪Z变换，并执行比例阈值法以保留指定比例的最强费雪绝对值，从而从这些时间序列构建无向图。然后，您必须量化阈值水平如何影响图的度分布以及将提供给图神经网络 (GNN; Graph Neural Network) 的节点特征。\n\n使用的基本原理和定义：\n- 令 $X \\in \\mathbb{R}^{T \\times n}$ 为 ROI 时间序列矩阵，其列由区域索引。ROI $i$ 和 ROI $j$ 之间的样本皮尔逊相关性定义为\n$$\nr_{ij} = \\frac{\\sum_{t=1}^{T} \\left(X_{t,i} - \\bar{X}_i\\right)\\left(X_{t,j} - \\bar{X}_j\\right)}{\\sqrt{\\sum_{t=1}^{T} \\left(X_{t,i} - \\bar{X}_i\\right)^2} \\, \\sqrt{\\sum_{t=1}^{T} \\left(X_{t,j} - \\bar{X}_j\\right)^2}},\n$$\n其中 $\\bar{X}_i$ 是 ROI $i$ 时间序列的样本均值。\n- $r_{ij}$ 的费雪Z变换为\n$$\nz_{ij} = \\operatorname{atanh}(r_{ij}) = \\frac{1}{2}\\ln\\left(\\frac{1 + r_{ij}}{1 - r_{ij}}\\right),\n$$\n该变换将相关系数从 $(-1,1)$ 映射到 $\\mathbb{R}$，并在相关性估计的标准假设下用于稳定方差。\n- 对于具有 $n$ 个节点的无向图，在水平 $p \\in (0,1]$ 进行的比例阈值法保留了 $K$ 条最强的无向边，其中\n$$\nK = \\left\\lfloor p \\cdot \\frac{n(n-1)}{2} \\right\\rfloor.\n$$\n定义加权邻接矩阵 $W^{(p)} \\in \\mathbb{R}^{n \\times n}$，使得对于所选的 $K$ 个无向对 $(i,j)$（其中 $i  j$），权重为 $W^{(p)}_{ij} = W^{(p)}_{ji} = \\left|z_{ij}\\right|$，且 $W^{(p)}_{ii} = 0$。定义相应的二元邻接矩阵 $A^{(p)} \\in \\{0,1\\}^{n \\times n}$，如果 $(i,j)$ 被选择，则 $A^{(p)}_{ij} = 1$，否则 $A^{(p)}_{ij} = 0$，且 $A^{(p)}_{ii} = 0$。\n- 对于由 $A^{(p)}$ 表示的图，节点 $i$ 的度为\n$$\nk_i^{(p)} = \\sum_{j=1}^{n} A^{(p)}_{ij}.\n$$\n度分布的变异系数为\n$$\n\\operatorname{CV}^{(p)} = \\frac{\\operatorname{std}\\left(\\{k_i^{(p)}\\}_{i=1}^n\\right)}{\\operatorname{mean}\\left(\\{k_i^{(p)}\\}_{i=1}^n\\right)},\n$$\n约定如果均值为 $0$，则 $\\operatorname{CV}^{(p)} = 0$。\n- 定义在 $W^{(p)}$ 下的节点强度为\n$$\ns_i^{(p)} = \\sum_{j=1}^{n} W^{(p)}_{ij}.\n$$\n令完整的、未经阈值处理的节点强度参考值为\n$$\ns_i^{\\mathrm{full}} = \\sum_{j=1}^{n} \\left|z_{ij}\\right| \\quad \\text{其中} \\quad z_{ii} = 0.\n$$\n定义节点 $i$ 的归一化度和归一化强度为\n$$\n\\tilde{k}_i^{(p)} = \\frac{k_i^{(p)}}{n-1}, \\quad \\tilde{s}_i^{(p)} = \n\\begin{cases}\n\\frac{s_i^{(p)}}{s_i^{\\mathrm{full}}},  \\text{若 } s_i^{\\mathrm{full}}  0,\\\\\n0,  \\text{其他情况}.\n\\end{cases}\n$$\n定义在 $A^{(p)}$ 下节点 $i$ 的局部二元聚类系数为\n$$\nc_i^{(p)} = \n\\begin{cases}\n\\frac{2\\, \\tau_i^{(p)}}{k_i^{(p)}\\left(k_i^{(p)} - 1\\right)},  \\text{若 } k_i^{(p)} \\ge 2,\\\\\n0,  \\text{其他情况},\n\\end{cases}\n$$\n其中 $\\tau_i^{(p)}$ 是在 $A^{(p)}$ 上计数的包含节点 $i$ 的三角形数量。\n- 定义在阈值 $p$ 下，用于图神经网络 (GNN; Graph Neural Network) 的节点特征向量为\n$$\n\\mathbf{f}_i^{(p)} = \\left[\\tilde{k}_i^{(p)}, \\, \\tilde{s}_i^{(p)}, \\, c_i^{(p)}\\right] \\in \\mathbb{R}^3,\n$$\n特征矩阵 $F^{(p)} \\in \\mathbb{R}^{n \\times 3}$ 将这些向量按行堆叠。通过弗罗贝尼乌斯范数量化连续阈值之间的特征变化\n$$\n\\Delta^{(p_\\ell)} = \n\\begin{cases}\n0,  \\text{若 } \\ell = 1,\\\\\n\\left\\|F^{(p_\\ell)} - F^{(p_{\\ell-1})}\\right\\|_F,  \\text{若 } \\ell \\ge 2,\n\\end{cases}\n$$\n对于一个有序阈值列表 $\\{p_\\ell\\}_{\\ell=1}^m$。\n\n合成数据集与阈值：\n对于下面的每个数据集，根据指定的确定性信号和混合方式构建 $X$，然后遵循上述流程。使用阈值列表 $\\{p_\\ell\\} = \\{0.2, 0.5, 1.0\\}$。\n\n- 数据集 $1$ ($n = 5$, $T = 200$):\n  - 基准信号，对于 $t \\in \\{0,1,\\dots,199\\}$:\n    $s_1[t] = \\sin(2\\pi \\cdot 0.03 \\cdot t)$,\n    $s_2[t] = \\cos(2\\pi \\cdot 0.07 \\cdot t + \\pi/4)$,\n    $s_3[t] = \\sin(2\\pi \\cdot 0.13 \\cdot t)$.\n  - 混合矩阵 $W \\in \\mathbb{R}^{5 \\times 3}$:\n    $W = \\begin{bmatrix}\n    1.0  0.5  0.0\\\\\n    0.9  -0.4  0.1\\\\\n    1.1  0.6  -0.2\\\\\n    -1.0  -0.5  0.3\\\\\n    0.2  0.0  1.0\n    \\end{bmatrix}$.\n  - 确定性噪声：对于 ROI $i \\in \\{0,1,2,3,4\\}$,\n    $\\eta_i[t] = 0.1 \\cdot \\sin\\left(2\\pi \\cdot (0.23 + 0.01 i) \\cdot t\\right)$.\n  - 构建 $S \\in \\mathbb{R}^{200 \\times 3}$，其列为 $s_1, s_2, s_3$，且 $X = S W^\\top + N$，其中 $N_{t,i} = \\eta_i[t]$。\n\n- 数据集 $2$ ($n = 4$, $T = 30$):\n  - 基准信号，对于 $t \\in \\{0,1,\\dots,29\\}$:\n    $s_1[t] = \\sin(2\\pi \\cdot 0.12 \\cdot t)$,\n    $s_2[t] = \\sin(2\\pi \\cdot 0.12 \\cdot t + \\pi)$,\n    $s_3[t] = \\cos(2\\pi \\cdot 0.05 \\cdot t)$.\n  - 混合矩阵 $W \\in \\mathbb{R}^{4 \\times 3}$:\n    $W = \\begin{bmatrix}\n    1.0  0.0  0.5\\\\\n    -1.0  0.0  -0.2\\\\\n    0.3  0.7  0.0\\\\\n    0.0  -0.5  0.8\n    \\end{bmatrix}$.\n  - 确定性噪声：对于 ROI $i \\in \\{0,1,2,3\\}$,\n    $\\eta_i[t] = 0.2 \\cdot \\cos\\left(2\\pi \\cdot (0.31 + 0.02 i) \\cdot t\\right)$.\n  - 构建 $S \\in \\mathbb{R}^{30 \\times 3}$，其列为 $s_1, s_2, s_3$，且 $X = S W^\\top + N$。\n\n- 数据集 $3$ ($n = 6$, $T = 100$):\n  - 基准信号，对于 $t \\in \\{0,1,\\dots,99\\}$:\n    $s_A[t] = \\sin(2\\pi \\cdot 0.04 \\cdot t)$,\n    $s_B[t] = \\sin(2\\pi \\cdot 0.08 \\cdot t + \\pi/3)$,\n    $s_C[t] = 0.3 \\cdot \\cos(2\\pi \\cdot 0.02 \\cdot t)$.\n  - 混合矩阵：\n    $W_{AB} \\in \\mathbb{R}^{6 \\times 2}$:\n    $W_{AB} = \\begin{bmatrix}\n    0.8  0.1\\\\\n    0.9  0.0\\\\\n    0.85  -0.1\\\\\n    0.0  0.9\\\\\n    -0.1  0.8\\\\\n    0.2  0.85\n    \\end{bmatrix}$,\n    以及 $w_C \\in \\mathbb{R}^{6}$:\n    $w_C = \\begin{bmatrix}\n    0.2\\\\ -0.2\\\\ 0.1\\\\ -0.1\\\\ 0.05\\\\ -0.05\n    \\end{bmatrix}$.\n  - 确定性噪声：对于 ROI $i \\in \\{0,1,2,3,4,5\\}$,\n    $\\eta_i[t] = 0.05 \\cdot \\sin\\left(2\\pi \\cdot (0.17 + 0.01 i) \\cdot t\\right)$.\n  - 构建 $S_{AB} \\in \\mathbb{R}^{100 \\times 2}$，其列为 $s_A, s_B$，且 $X_{AB} = S_{AB} W_{AB}^\\top$。从 $s_C$ 构建 $S_C \\in \\mathbb{R}^{100}$ 且 $X_C = S_C \\cdot w_C^\\top$。然后 $X = X_{AB} + X_C + N$。\n\n每个数据集的处理和量化任务：\n1. 从 $X$ 计算皮尔逊相关矩阵 $C \\in \\mathbb{R}^{n \\times n}$。\n2. 从 $C$ 逐元素计算费雪Z变换矩阵 $Z \\in \\mathbb{R}^{n \\times n}$。为保证数值稳定性，在应用 $\\operatorname{atanh}$ 之前，通过将 $r_{ij}$ 裁剪到 $(-1+\\epsilon, 1-\\epsilon)$（对于一个很小的 $\\epsilon$）来确保 $r_{ij} \\in (-1,1)$。\n3. 对于每个阈值 $p \\in \\{0.2, 0.5, 1.0\\}$，对 $\\left|Z\\right|$ 执行比例阈值法，精确选择 $K = \\left\\lfloor p \\cdot \\frac{n(n-1)}{2} \\right\\rfloor$ 条具有最大 $\\left|Z_{ij}\\right|$ 值（其中 $i",
            "solution": "用户提供的问题是网络神经科学和图信号处理领域中一个明确定义的计算任务。它要求从合成时间序列数据构建和分析图，这是对图神经网络 (GNN) 等方法进行基准测试的常用程序。该问题具有科学依据、逻辑一致，并为获得唯一的确定性解提供了所有必要信息。所有的定义和计算步骤都已正式指定。因此，该问题是有效的，下面将给出完整的解决方案。\n\n该解决方案对所提供的三个数据集均遵循一个多步骤流程。核心步骤包括生成时间序列、构建脑连接图、在不同稀疏度水平上提取节点特征，并最终量化这些特征的变化。\n\n### 第1步：时间序列生成\n\n对于每个数据集，都会生成一个多元时间序列矩阵 $X \\in \\mathbb{R}^{T \\times n}$，其中 $n$ 是感兴趣区域 (ROI) 的数量，T 是时间点的数量。其构建基于线性混合模型，这是一种创建具有已知真实结构（ground-truth）的合成数据的标准方法。\n\n- 定义一组基准信号（正弦波），形成矩阵 $S$ 的列。\n- 这些信号使用特定于数据集的混合矩阵 $W$进行线性组合。\n- 添加一个确定性噪声信号 $N$。\n\n最终的时间序列矩阵形成为 $X = \\text{信号} + \\text{噪声}$。每个数据集的具体情况如下：\n\n- **数据集1 ($n=5, T=200$):** $X = S W^\\top + N$，其中 $S \\in \\mathbb{R}^{200 \\times 3}$ 包含三个基准正弦信号。\n- **数据集2 ($n=4, T=30$):** $X = S W^\\top + N$，其中 $S \\in \\mathbb{R}^{30 \\times 3}$ 包含三个基准信号。一个基准信号与另一个完全反相关 ($s_2[t] = -s_1[t]$)。\n- **数据集3 ($n=6, T=100$):** $X = S_{AB} W_{AB}^\\top + S_C w_C^\\top + N$。这涉及到在求和之前，通过两个独立的混合操作对三个基准信号 ($s_A, s_B, s_C$) 进行更复杂的混合。\n\n### 第2步：相关性与费雪Z变换\n\n从生成的时间序列矩阵 $X$ 中，推导出一个功能连接图。\n\n- **皮尔逊相关性：**计算样本皮尔逊相关矩阵 $C \\in \\mathbb{R}^{n \\times n}$。每个元素 $r_{ij}$ 衡量 ROI $i$ 和 ROI $j$ 时间序列之间的线性相关性。\n$$\nr_{ij} = \\frac{\\operatorname{Cov}(X_i, X_j)}{\\sigma_i \\sigma_j}\n$$\n- **费雪Z变换：**使用费雪变换将相关系数 $r_{ij}$ 转换为 $z_{ij}$ 分数。此变换可以稳定相关性估计的方差，并将范围 $(-1, 1)$ 映射到 $(-\\infty, \\infty)$。\n$$\nz_{ij} = \\operatorname{atanh}(r_{ij}) = \\frac{1}{2}\\ln\\left(\\frac{1 + r_{ij}}{1 - r_{ij}}\\right)\n$$\n在数值上，如果任何 $r_{ij}$ 恰好为 $\\pm 1$，则 $\\operatorname{atanh}(r_{ij})$ 是未定义的。通过将相关值裁剪到略微在 $(-1, 1)$ 范围内的区间，例如 $[-1 + \\epsilon, 1 - \\epsilon]$（对于一个小的机器精度常数 $\\epsilon  0$），可以处理此问题。结果矩阵记为 $Z$。对角线元素 $z_{ii}$ 被设置为 $0$。\n\n### 第3步：比例阈值法\n\n为了研究图稀疏性的影响，对密集连接矩阵 $|Z|$ 进行阈值处理，以保留特定比例 $p$ 的最强连接。\n\n- 对于给定的阈值水平 $p \\in \\{0.2, 0.5, 1.0\\}$，要保留的边数计算为 $K = \\left\\lfloor p \\cdot \\frac{n(n-1)}{2} \\right\\rfloor$。\n- 将 $i  j$ 的 $\\frac{n(n-1)}{2}$ 个唯一的非对角线费雪绝对值分数 $|z_{ij}|$ 按降序排序。选择前 $K$ 条边。\n- 此过程为每个 $p$ 产生两个矩阵：\n    1. 一个二元邻接矩阵 $A^{(p)}$，如果边 $(i, j)$ 被保留，则 $A^{(p)}_{ij} = 1$，否则为 $0$。\n    2. 一个加权邻接矩阵 $W^{(p)}$，对于保留的边，$W^{(p)}_{ij} = |z_{ij}|$，否则为 $0$。\n\n### 第4步：图特征提取\n\n对于每个经过阈值处理的图，会计算一组节点级特征。这些特征通常用作 GNN 模型的输入，用于节点分类或图分类等任务。\n\n- **度与变异系数 (CV)：**计算每个节点的度 $k_i^{(p)} = \\sum_j A^{(p)}_{ij}$。度分布的异质性通过其变异系数来衡量：\n$$\n\\operatorname{CV}^{(p)} = \\frac{\\operatorname{std}\\left(\\{k_i^{(p)}\\}\\right)}{\\operatorname{mean}\\left(\\{k_i^{(p)}\\}\\right)}\n$$\n- **节点强度：**从加权矩阵计算每个节点的强度 $s_i^{(p)} = \\sum_j W^{(p)}_{ij}$。\n- **局部聚类系数：**该指标量化了一个节点的邻居节点们形成一个团（clique）的紧密程度。其定义为：\n$$\nc_i^{(p)} = \n\\begin{cases}\n\\frac{2\\, \\tau_i^{(p)}}{k_i^{(p)}\\left(k_i^{(p)} - 1\\right)},  \\text{若 } k_i^{(p)} \\ge 2,\\\\\n0,  \\text{其他情况},\n\\end{cases}\n$$\n其中 $\\tau_i^{(p)} = \\frac{1}{2}((A^{(p)})^3)_{ii}$ 是连接到节点 $i$ 的三角形数量。\n- **节点特征向量：**将计算出的指标进行归一化，并为每个节点组合成一个三维特征向量：\n$$\n\\mathbf{f}_i^{(p)} = \\left[\\tilde{k}_i^{(p)}, \\, \\tilde{s}_i^{(p)}, \\, c_i^{(p)}\\right]\n$$\n归一化度为 $\\tilde{k}_i^{(p)} = k_i^{(p)} / (n-1)$。归一化强度 $\\tilde{s}_i^{(p)} = s_i^{(p)} / s_i^{\\mathrm{full}}$ 是相对于节点在完整的、未经阈值处理的图中的强度而言的，其中 $s_i^{\\mathrm{full}} = \\sum_{j \\neq i} |z_{ij}|$。这些向量被堆叠起来形成特征矩阵 $F^{(p)} \\in \\mathbb{R}^{n \\times 3}$。\n\n### 第5步：特征变化量化\n\n最后，为了评估节点特征在不同稀疏度水平下的稳定性，使用弗罗贝尼乌斯范数量化连续阈值下特征矩阵之间的变化。\n\n- 对于一个有序的阈值列表 $\\{p_1, p_2, \\dots\\}$，变化计算如下：\n$$\n\\Delta^{(p_\\ell)} = \\left\\|F^{(p_\\ell)} - F^{(p_{\\ell-1})}\\right\\|_F\n$$\n按照惯例，$\\Delta^{(p_1)} = 0$。对于本问题，序列是 $p_1=0.2, p_2=0.5, p_3=1.0$。\n\n这个综合流程被应用于三个数据集中的每一个，所得的 $\\operatorname{CV}^{(p)}$ 和 $\\Delta^{(p)}$ 值列表被收集并按照问题规范进行格式化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the three specified datasets.\n    \"\"\"\n\n    def process_dataset(n, T, create_X_func, thresholds):\n        \"\"\"\n        Executes the full pipeline for a single dataset.\n        \n        Args:\n            n (int): Number of ROIs.\n            T (int): Number of time points.\n            create_X_func (function): A function that returns the time series matrix X.\n            thresholds (list): A list of proportional thresholds p.\n\n        Returns:\n            tuple: A tuple containing the list of CVs and the list of feature change norms.\n        \"\"\"\n        # Step 1: Generate Time Series Data\n        X = create_X_func(n, T)\n\n        # Step 2: Compute Correlation and Fisher Z-transform\n        # rowvar=False because columns are variables (ROIs)\n        C = np.corrcoef(X, rowvar=False)\n        \n        # Handle potential perfect correlations leading to inf in atanh\n        # A small epsilon for numerical stability\n        epsilon = 1e-15\n        C_clipped = np.clip(C, -1.0 + epsilon, 1.0 - epsilon)\n        \n        Z = np.arctanh(C_clipped)\n        np.fill_diagonal(Z, 0)\n        \n        # Step 4 (Part 1): Compute full node strength for normalization\n        s_full = np.sum(np.abs(Z), axis=1)\n        # Handle case where s_full could be zero\n        s_full_inv = np.zeros_like(s_full)\n        s_full_inv[s_full  0] = 1.0 / s_full[s_full  0]\n        \n        # Initialize storage for results\n        cv_list = []\n        delta_list = []\n        feature_matrices = {}\n\n        # Get upper triangle indices for edge selection\n        iu_indices = np.triu_indices(n, k=1)\n        \n        # Get flattened upper triangle of |Z|\n        z_abs_flat = np.abs(Z[iu_indices])\n        \n        # Get indices that would sort the edge weights in descending order\n        sorted_edge_indices = np.argsort(z_abs_flat)[::-1]\n        \n        num_possible_edges = n * (n - 1) // 2\n\n        for i, p in enumerate(thresholds):\n            # Step 3: Proportional Thresholding\n            K = int(np.floor(p * num_possible_edges))\n            \n            A_p = np.zeros((n, n), dtype=int)\n            W_p = np.zeros((n, n), dtype=float)\n            \n            if K  0:\n                # Select top K edges\n                top_k_flat_indices = sorted_edge_indices[:K]\n                \n                # Get the row and column indices for the top K edges\n                top_k_rows = iu_indices[0][top_k_flat_indices]\n                top_k_cols = iu_indices[1][top_k_flat_indices]\n\n                # Populate A_p and W_p\n                A_p[top_k_rows, top_k_cols] = 1\n                A_p[top_k_cols, top_k_rows] = 1\n                W_p[top_k_rows, top_k_cols] = np.abs(Z[top_k_rows, top_k_cols])\n                W_p[top_k_cols, top_k_rows] = W_p[top_k_rows, top_k_cols]\n\n            # Step 4 (Part 2): Compute Graph Metrics and Features\n            k_p = np.sum(A_p, axis=1)\n            \n            # Coefficient of Variation\n            mean_k = np.mean(k_p)\n            cv_p = np.std(k_p) / mean_k if mean_k  0 else 0.0\n            cv_list.append(cv_p)\n            \n            # Node strength\n            s_p = np.sum(W_p, axis=1)\n            \n            # Normalized degree and strength\n            k_tilde_p = k_p / (n - 1)\n            s_tilde_p = s_p * s_full_inv\n            \n            # Clustering Coefficient\n            c_p = np.zeros(n, dtype=float)\n            if K  0: # Avoid matrix power of zero matrix\n                A_p_cubed = np.linalg.matrix_power(A_p, 3)\n                triangles_i = 0.5 * np.diag(A_p_cubed)\n                \n                for node_idx in range(n):\n                    if k_p[node_idx] = 2:\n                        c_p[node_idx] = (2 * triangles_i[node_idx]) / (k_p[node_idx] * (k_p[node_idx] - 1))\n            \n            # Assemble feature matrix\n            F_p = np.stack([k_tilde_p, s_tilde_p, c_p], axis=1)\n            feature_matrices[p] = F_p\n\n            # Step 5: Quantify Feature Change\n            if i == 0:\n                delta_p = 0.0\n            else:\n                F_prev = feature_matrices[thresholds[i-1]]\n                delta_p = np.linalg.norm(F_p - F_prev, 'fro')\n            delta_list.append(delta_p)\n            \n        return cv_list, delta_list\n\n    # --- Dataset Definitions ---\n    thresholds = [0.2, 0.5, 1.0]\n    \n    # Dataset 1\n    def create_X1(n, T):\n        t = np.arange(T)\n        s1 = np.sin(2 * np.pi * 0.03 * t)\n        s2 = np.cos(2 * np.pi * 0.07 * t + np.pi / 4)\n        s3 = np.sin(2 * np.pi * 0.13 * t)\n        S = np.stack([s1, s2, s3], axis=1)\n        W = np.array([\n            [1.0, 0.5, 0.0],\n            [0.9, -0.4, 0.1],\n            [1.1, 0.6, -0.2],\n            [-1.0, -0.5, 0.3],\n            [0.2, 0.0, 1.0]\n        ])\n        N = np.zeros((T, n))\n        for i in range(n):\n            N[:, i] = 0.1 * np.sin(2 * np.pi * (0.23 + 0.01 * i) * t)\n        return S @ W.T + N\n\n    # Dataset 2\n    def create_X2(n, T):\n        t = np.arange(T)\n        s1 = np.sin(2 * np.pi * 0.12 * t)\n        s2 = np.sin(2 * np.pi * 0.12 * t + np.pi)\n        s3 = np.cos(2 * np.pi * 0.05 * t)\n        S = np.stack([s1, s2, s3], axis=1)\n        W = np.array([\n            [1.0, 0.0, 0.5],\n            [-1.0, 0.0, -0.2],\n            [0.3, 0.7, 0.0],\n            [0.0, -0.5, 0.8]\n        ])\n        N = np.zeros((T, n))\n        for i in range(n):\n            N[:, i] = 0.2 * np.cos(2 * np.pi * (0.31 + 0.02 * i) * t)\n        return S @ W.T + N\n        \n    # Dataset 3\n    def create_X3(n, T):\n        t = np.arange(T)\n        sA = np.sin(2 * np.pi * 0.04 * t)\n        sB = np.sin(2 * np.pi * 0.08 * t + np.pi / 3)\n        sC = 0.3 * np.cos(2 * np.pi * 0.02 * t)\n        S_AB = np.stack([sA, sB], axis=1)\n        W_AB = np.array([\n            [0.8, 0.1],\n            [0.9, 0.0],\n            [0.85, -0.1],\n            [0.0, 0.9],\n            [-0.1, 0.8],\n            [0.2, 0.85]\n        ])\n        w_C = np.array([0.2, -0.2, 0.1, -0.1, 0.05, -0.05])\n        X_AB = S_AB @ W_AB.T\n        X_C = np.outer(sC, w_C)\n        N = np.zeros((T, n))\n        for i in range(n):\n            N[:, i] = 0.05 * np.sin(2 * np.pi * (0.17 + 0.01 * i) * t)\n        return X_AB + X_C + N\n\n    datasets = [\n        {'n': 5, 'T': 200, 'func': create_X1},\n        {'n': 4, 'T': 30, 'func': create_X2},\n        {'n': 6, 'T': 100, 'func': create_X3}\n    ]\n    \n    all_results = []\n    for d in datasets:\n        cvs, deltas = process_dataset(d['n'], d['T'], d['func'], thresholds)\n        all_results.append([cvs, deltas])\n        \n    # Format the final output string\n    result_str = str(all_results).replace(\" \", \"\")\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "除了功能数据，源自扩散磁共振成像（dMRI）的结构连接体为我们提供了大脑布线的另一种关键视角。这些通常基于纤维束追踪计数构建的图，同样需要精心的预处理才能用于 GNN 模型。这个练习聚焦于一个核心的预处理步骤：归一化。原始的纤维束计数可能受到脑区体积大小的系统性影响，我们将比较直接使用原始计数与使用体积归一化后的邻接矩阵作为 GCN 输入的效果，并量化这一选择对图卷积网络（GCN）中信息传递机制的影响 。这项练习清晰地揭示了 GNN 的一个关键特性：模型中的信息缩放（message scaling）对邻接矩阵中的权重极为敏感，理解这一点有助于设计更稳定、更可靠的大脑连接体 GNN 模型。",
            "id": "4167796",
            "problem": "给定扩散磁共振成像 (dMRI) 纤维束示踪得到的脑部感兴趣区域 (ROI) 之间的流线计数，表示为一个具有非负条目 $s_{ij}$ 的对称计数矩阵 $S \\in \\mathbb{R}^{n \\times n}$，以及一个 ROI 体积向量 $\\mathrm{vol} \\in \\mathbb{R}^{n}$，其条目 $\\mathrm{vol}_i$ 为单位是 $\\mathrm{mm}^3$ 的严格正值。您必须构建一个体积归一化的结构邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其按元素定义为 $A_{ij} = \\dfrac{s_{ij}}{\\mathrm{vol}_i \\mathrm{vol}_j}$，并分析归一化选择如何影响图卷积网络 (GCN) 单层中消息的缩放。考虑一个节点特征矩阵 $X \\in \\mathbb{R}^{n \\times f}$，每个 ROI 有 $f$ 个特征。令 $W$ 表示用于消息传递的类邻接权重矩阵。用于消息传递的对称度归一化传播算子定义为 $P = D^{-1/2} W D^{-1/2}$，其中 $D \\in \\mathbb{R}^{n \\times n}$ 是对角度矩阵，其对角元素为 $D_{ii} = \\sum_{j=1}^n W_{ij}$。如果一个节点的度为零，则将相应的度的负二分之一次方设置为 $0$，即当 $D_{ii} = 0$ 时，使用 $(D^{-1/2})_{ii} = 0$。对于给定的 $X$，将消息缩放比 $\\rho(W, X)$ 定义为 $\\rho(W, X) = \\dfrac{\\lVert P X \\rVert_F}{\\lVert X \\rVert_F}$，其中 $\\lVert \\cdot \\rVert_F$ 表示弗罗贝尼乌斯范数。您必须为下面的每个测试用例计算并比较 $\\rho(S, X)$ 和 $\\rho(A, X)$，以量化体积归一化如何与度归一化相互作用，从而影响 GCN 层中消息的缩放。\n\n所有物理体积都必须以 $\\mathrm{mm}^3$ 为单位处理，流线计数是无单位的，最终报告的缩放比也是无单位的。本问题不涉及角度。本问题不涉及百分比。\n\n测试套件包含三个用例，它们共同探究了典型行为、ROI 尺寸的异质性，以及一个包含孤立 ROI 的边缘情况。对于每个用例，都提供了 $S$、$\\mathrm{vol}$ 和 $X$：\n\n用例 1（中等连接度和均衡的 ROI 体积）：\n$$\nS^{(1)} = \\begin{pmatrix}\n0  120  80  60 \\\\\n120  0  90  70 \\\\\n80  90  0  50 \\\\\n60  70  50  0\n\\end{pmatrix}, \\quad\n\\mathrm{vol}^{(1)} = \\begin{pmatrix} 3200 \\\\ 2800 \\\\ 4000 \\\\ 3500 \\end{pmatrix}\\,\\mathrm{mm}^3, \\quad\nX^{(1)} = \\begin{pmatrix}\n2.0  0.5  -1.0 \\\\\n1.5  -0.2  0.3 \\\\\n-0.7  1.2  0.8 \\\\\n0.0  -1.0  0.5\n\\end{pmatrix}.\n$$\n\n用例 2（强枢纽连接到非常小的 ROI 以及大的体积差异）：\n$$\nS^{(2)} = \\begin{pmatrix}\n0  300  250  200 \\\\\n300  0  60  40 \\\\\n250  60  0  30 \\\\\n200  40  30  0\n\\end{pmatrix}, \\quad\n\\mathrm{vol}^{(2)} = \\begin{pmatrix} 800 \\\\ 5000 \\\\ 4800 \\\\ 5100 \\end{pmatrix}\\,\\mathrm{mm}^3, \\quad\nX^{(2)} = \\begin{pmatrix}\n1.0  -0.5  0.2 \\\\\n-0.3  0.8  -0.1 \\\\\n0.5  0.5  0.5 \\\\\n-1.2  0.0  0.7\n\\end{pmatrix}.\n$$\n\n用例 3（在原始计数中度为零的孤立 ROI）：\n$$\nS^{(3)} = \\begin{pmatrix}\n0  50  0  0 \\\\\n50  0  40  0 \\\\\n0  40  0  0 \\\\\n0  0  0  0\n\\end{pmatrix}, \\quad\n\\mathrm{vol}^{(3)} = \\begin{pmatrix} 3000 \\\\ 3100 \\\\ 2900 \\\\ 3200 \\end{pmatrix}\\,\\mathrm{mm}^3, \\quad\nX^{(3)} = \\begin{pmatrix}\n0.2  0.1  -0.4 \\\\\n1.0  -1.0  0.0 \\\\\n-0.5  0.7  0.3 \\\\\n0.9  -0.8  0.2\n\\end{pmatrix}.\n$$\n\n您的程序必须对每个用例 $(S^{(k)}, \\mathrm{vol}^{(k)}, X^{(k)})$（其中 $k \\in \\{1,2,3\\}$），使用上面定义的对称度归一化传播算子 $P = D^{-1/2} W D^{-1/2}$ 来计算两个缩放比 $\\rho(S^{(k)}, X^{(k)})$ 和 $\\rho(A^{(k)}, X^{(k)})$，其中 $A^{(k)}_{ij} = \\dfrac{s^{(k)}_{ij}}{\\mathrm{vol}^{(k)}_i \\mathrm{vol}^{(k)}_j}$。请完全按照规定，使用 $(D^{-1/2})_{ii} = 0$ 来处理度为零的情况。最终输出格式必须是单行，包含一个含有三个内部列表的列表，每个内部列表按顺序 $[\\rho(S^{(k)}, X^{(k)}), \\rho(A^{(k)}, X^{(k)})]$ 包含两个浮点数结果。例如，输出形式必须为 $[[r_{1,\\mathrm{raw}}, r_{1,\\mathrm{vol}}],[r_{2,\\mathrm{raw}}, r_{2,\\mathrm{vol}}],[r_{3,\\mathrm{raw}}, r_{3,\\mathrm{vol}}]]$，所有条目都表示为十进制浮点数，且无附加文本。\n\n所有测试用例的答案都是浮点数。程序必须完全自包含，不得需要任何用户输入或外部文件，并且必须在指定的执行环境下确定性地运行。",
            "solution": "问题陈述已经过验证，被认为是合理的。它在计算神经科学和基于图的机器学习领域有科学依据，在数学上是适定的，并为得到唯一、可验证的解提供了所有必要信息。\n\n目标是分析两种不同的图构建方法对单个图卷积网络 (GCN) 层消息缩放属性的影响。构建图的权重矩阵 $W$ 的两种方法是：\n1.  使用原始流线计数，$W = S$。\n2.  使用体积归一化的计数，$W = A$，其中 $A_{ij} = \\dfrac{s_{ij}}{\\mathrm{vol}_i \\mathrm{vol}_j}$。\n\n消息的缩放由比率 $\\rho(W, X)$ 来量化，该比率衡量节点特征矩阵 $X$ 经过单次 GCN 传播步骤后其弗罗贝尼乌斯范数的变化。\n\nGCN 传播规则的核心是对称度归一化算子 $P$。给定一个权重矩阵 $W \\in \\mathbb{R}^{n \\times n}$ 和一个节点特征矩阵 $X \\in \\mathbb{R}^{n \\times f}$，传播定义为：\n$Y = P X$\n其中 $Y \\in \\mathbb{R}^{n \\times f}$ 是更新后的节点特征矩阵。\n\n传播算子 $P$ 按如下方式构建：\n$P = D^{-1/2} W D^{-1/2}$\n\n此处，$D$ 是对角度矩阵，其元素是每个节点的权重之和：\n$D_{ii} = \\sum_{j=1}^{n} W_{ij}$\n\n矩阵 $D^{-1/2}$ 是一个对角矩阵，其对角线上的元素是度的平方根的倒数。一个关键细节是处理度为零的节点（$D_{ii}=0$），否则会导致除以零。按照规定，对于此类节点，$D^{-1/2}$ 中的相应条目设置为零：\n$$\n(D^{-1/2})_{ii} = \n\\begin{cases} \n(D_{ii})^{-1/2}  \\text{if } D_{ii}  0 \\\\\n0  \\text{if } D_{ii} = 0 \n\\end{cases}\n$$\n\n消息缩放比 $\\rho(W, X)$ 则定义为输出和输入特征矩阵的弗罗贝尼乌斯范数之比：\n$$\n\\rho(W, X) = \\frac{\\lVert P X \\rVert_F}{\\lVert X \\rVert_F}\n$$\n矩阵 $M \\in \\mathbb{R}^{n \\times f}$ 的弗罗贝尼乌斯范数由 $\\lVert M \\rVert_F = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{f} M_{ij}^2}$ 给出。比率 $\\rho  1$ 表示特征信号的放大，而 $\\rho  1$ 表示衰减。\n\n每个测试用例 $(S^{(k)}, \\mathrm{vol}^{(k)}, X^{(k)})$ 的计算过程涉及两个并行的计算：\n\n1.  **原始计数计算 ($W=S^{(k)}$)**:\n    a. 从 $S^{(k)}$ 计算度矩阵 $D_S$。\n    b. 遵循度为零的指定规则，构建度的负二分之一次方矩阵 $D_S^{-1/2}$。\n    c. 计算传播算子 $P_S = D_S^{-1/2} S^{(k)} D_S^{-1/2}$。\n    d. 计算传播后的特征 $Y_S = P_S X^{(k)}$。\n    e. 计算范数 $\\lVert Y_S \\rVert_F$ 和 $\\lVert X^{(k)} \\rVert_F$。\n    f. 计算比率 $\\rho(S^{(k)}, X^{(k)}) = \\lVert Y_S \\rVert_F / \\lVert X^{(k)} \\rVert_F$。\n\n2.  **体积归一化计数计算 ($W=A^{(k)}$)**:\n    a. 首先，构建体积归一化矩阵 $A^{(k)}$，其元素为 $A^{(k)}_{ij} = \\frac{s^{(k)}_{ij}}{\\mathrm{vol}^{(k)}_i \\mathrm{vol}^{(k)}_j}$。\n    b. 从 $A^{(k)}$ 计算度矩阵 $D_A$。\n    c. 构建度的负二分之一次方矩阵 $D_A^{-1/2}$。\n    d. 计算传播算子 $P_A = D_A^{-1/2} A^{(k)} D_A^{-1/2}$。\n    e. 计算传播后的特征 $Y_A = P_A X^{(k)}$。\n    f. 计算范数 $\\lVert Y_A \\rVert_F$ 和 $\\lVert X^{(k)} \\rVert_F$。\n    g. 计算比率 $\\rho(A^{(k)}, X^{(k)}) = \\lVert Y_A \\rVert_F / \\lVert X^{(k)} \\rVert_F$。\n\n该过程被系统地应用于问题陈述中提供的所有三个测试用例。用例 3 尤为重要，因为它测试了对孤立节点（节点 4）的指定处理方式，该节点在 $S$ 和 $A$ 矩阵中的度均为零。实现正确地将 $D^{-1/2}$ 中的相应条目设置为 $0$，确保该节点的传播结果为一个零特征向量，从而有效地将其与消息传递过程隔离。最终结果对每个用例 $k \\in \\{1, 2, 3\\}$ 均以配对 $[\\rho(S^{(k)}, X^{(k)}), \\rho(A^{(k)}, X^{(k)})]$ 的形式呈现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares GCN message scaling ratios for raw and volume-normalized\n    brain connectivity matrices across three test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1\n        {\n            \"S\": np.array([\n                [0, 120, 80, 60],\n                [120, 0, 90, 70],\n                [80, 90, 0, 50],\n                [60, 70, 50, 0]\n            ], dtype=float),\n            \"vol\": np.array([3200, 2800, 4000, 3500], dtype=float),\n            \"X\": np.array([\n                [2.0, 0.5, -1.0],\n                [1.5, -0.2, 0.3],\n                [-0.7, 1.2, 0.8],\n                [0.0, -1.0, 0.5]\n            ], dtype=float),\n        },\n        # Case 2\n        {\n            \"S\": np.array([\n                [0, 300, 250, 200],\n                [300, 0, 60, 40],\n                [250, 60, 0, 30],\n                [200, 40, 30, 0]\n            ], dtype=float),\n            \"vol\": np.array([800, 5000, 4800, 5100], dtype=float),\n            \"X\": np.array([\n                [1.0, -0.5, 0.2],\n                [-0.3, 0.8, -0.1],\n                [0.5, 0.5, 0.5],\n                [-1.2, 0.0, 0.7]\n            ], dtype=float),\n        },\n        # Case 3\n        {\n            \"S\": np.array([\n                [0, 50, 0, 0],\n                [50, 0, 40, 0],\n                [0, 40, 0, 0],\n                [0, 0, 0, 0]\n            ], dtype=float),\n            \"vol\": np.array([3000, 3100, 2900, 3200], dtype=float),\n            \"X\": np.array([\n                [0.2, 0.1, -0.4],\n                [1.0, -1.0, 0.0],\n                [-0.5, 0.7, 0.3],\n                [0.9, -0.8, 0.2]\n            ], dtype=float),\n        }\n    ]\n\n    def compute_scaling_ratio(W, X):\n        \"\"\"\n        Calculates the message scaling ratio rho(W, X).\n        \"\"\"\n        norm_X = np.linalg.norm(X, 'fro')\n        \n        # If X is a zero matrix, the ratio is undefined, but for this problem, we can return 0.\n        if norm_X == 0:\n            return 0.0\n\n        # Calculate degree vector d from the weight matrix W\n        d = W.sum(axis=1)\n\n        # Calculate D^{-1/2} handling zero degrees\n        # np.power with `where` argument efficiently handles d_ii  0\n        d_inv_sqrt_vals = np.power(d, -0.5, where=(d  0))\n        D_inv_sqrt = np.diag(d_inv_sqrt_vals)\n\n        # Compute the symmetric degree-normalized propagation operator P\n        P = D_inv_sqrt @ W @ D_inv_sqrt\n\n        # Propagate features: Y = P * X\n        Y = P @ X\n\n        # Calculate the Frobenius norm of the propagated features\n        norm_Y = np.linalg.norm(Y, 'fro')\n\n        # Compute the scaling ratio\n        rho = norm_Y / norm_X\n        \n        return rho\n\n    results = []\n    for case in test_cases:\n        S = case[\"S\"]\n        vol = case[\"vol\"]\n        X = case[\"X\"]\n\n        # Compute rho for raw streamline counts S\n        rho_S = compute_scaling_ratio(S, X)\n\n        # Compute volume-normalized adjacency matrix A\n        vol_outer = np.outer(vol, vol)\n        # Element-wise division, handling potential division by zero\n        # Since vol_i  0, vol_outer will have all non-zero entries.\n        A = S / vol_outer\n        \n        # Compute rho for volume-normalized matrix A\n        rho_A = compute_scaling_ratio(A, X)\n\n        results.append([rho_S, rho_A])\n    \n    # Format the final output string as required\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在实际研究中，我们获得的连接体数据往往是不完整的。例如，dMRI 纤维束追踪技术可能会因为技术限制而未能检测到某些真实存在的神经通路，这在我们的图中表现为“缺失的边”。本练习将介绍一种基于图扩散（graph diffusion）的复杂数据插补方法来应对这一挑战。通过模拟一个“信号”如何在已知的网络结构上传播（利用图热核），我们可以为那些缺失的连接推断出合理的权重 。这个动手实践不仅能让你掌握一项提升数据质量的高级技术，还要求你量化这种插补方法如何改变图的基本属性（如节点度）以及后续 GNN 层的信息聚合行为，从而全面评估干预措施所带来的影响。",
            "id": "4167865",
            "problem": "给定无向、加权的脑连接图，其由对角线为零的对称邻接矩阵表示。在基于扩散的插补中，其直觉是结构连接性支持信号在图上的扩散。当由于纤维束成像失败而导致连接缺失时，可以通过扩散将相应节点对耦合的强度来估计其权重。使用以下基本原理：具有邻接矩阵 $A$ 的无向加权图的组合图拉普拉斯算子 $L$ 定义为 $L = D - A$，其中 $D$ 是对角度矩阵 $D_{ii} = \\sum_j A_{ij}$。图热方程是线性常微分方程 $dZ(t)/dt = -L Z(t)$，其解为 $Z(t) = \\exp(-t L) Z(0)$，其中 $\\exp(\\cdot)$ 表示矩阵指数，$t \\ge 0$ 是扩散时间。对于一对节点 $i$ 和 $j$，项 $\\left[\\exp(-t L)\\right]_{ij}$ 量化了在时间 $t$ 时节点 $i$ 和 $j$ 之间的扩散亲和度。\n\n请按如下方式设计并实现一个基于图扩散的插补方案。给定一个观测到的邻接矩阵 $A_{\\mathrm{obs}}$ 和一个缺失边的对称二元掩码 $M$（对角线为零），通过将仅在缺失对之间的扩散亲和度（由一个非负标量 $\\lambda$ 缩放）加到 $A_{\\mathrm{obs}}$ 上，来构建一个插补后的邻接矩阵 $A_{\\mathrm{imp}}$。具体来说，计算热核 $K_t = \\exp(-t L_{\\mathrm{obs}})$，其中 $L_{\\mathrm{obs}} = D_{\\mathrm{obs}} - A_{\\mathrm{obs}}$，并设置 $B = K_t$，同时将其对角线强制为零。然后，仅使用哈达玛积更新缺失的条目，以获得 $A_{\\mathrm{imp}} = \\mathrm{sym}\\big(A_{\\mathrm{obs}} + \\lambda \\,(M \\circ B)\\big)$，其中 $\\mathrm{sym}(X) = \\tfrac{1}{2}(X + X^\\top)$ 强制对称性。所有矩阵均为实值，$A_{\\mathrm{obs}}$ 的项为非负且对角线为零。\n\n量化插补对以下两个量的影响：\n- 节点度：计算度变化向量 $\\Delta d = d_{\\mathrm{imp}} - d_{\\mathrm{obs}}$，其中 $d_{\\mathrm{obs}} = A_{\\mathrm{obs}} \\mathbf{1}$ 和 $d_{\\mathrm{imp}} = A_{\\mathrm{imp}} \\mathbf{1}$，$\\mathbf{1}$ 是全1向量。\n- 单层图卷积网络 (GCN)（一种图神经网络 GNN）使用的单步消息传递：令 $\\tilde{A} = A + I$ 添加自环，$\\tilde{D}$ 是相应的度矩阵，$\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$，且 $S = \\tilde{D}^{-1/2} \\tilde{A}\\,\\tilde{D}^{-1/2}$。对于收集在矩阵 $X$ 中的节点特征，具有单位权重的单层网络后的激活前消息为 $M = S X$。计算使用 $A_{\\mathrm{obs}}$ 和使用 $A_{\\mathrm{imp}}$ 之间消息的每个节点的欧几里得范数变化，即向量 $r$，其条目为 $r_i = \\lVert (S_{\\mathrm{imp}} - S_{\\mathrm{obs}}) X \\rVert_2$。\n\n实现一个程序，对下面的每个测试用例，计算并输出：\n- 度变化向量 $\\Delta d$，作为浮点数列表。\n- 每个节点的消息变化向量 $r$，作为浮点数列表。\n将所有浮点输出四舍五入到恰好 $6$ 位小数。\n\n使用以下测试套件。在所有情况下，特征矩阵 $X \\in \\mathbb{R}^{4 \\times 2}$ 为\n$$\nX = \\begin{bmatrix}\n1  0\\\\\n0  1\\\\\n1  1\\\\\n2  -1\n\\end{bmatrix}.\n$$\n测试用例 1（正常路径）：\n- $A_{\\mathrm{obs}} \\in \\mathbb{R}^{4 \\times 4}$:\n$$\n\\begin{bmatrix}\n0  0.8  0  0.2\\\\\n0.8  0  0.6  0\\\\\n0  0.6  0  0.7\\\\\n0.2  0  0.7  0\n\\end{bmatrix}.\n$$\n- 缺失边掩码 $M$：\n$$\n\\begin{bmatrix}\n0  0  1  0\\\\\n0  0  0  1\\\\\n1  0  0  0\\\\\n0  1  0  0\n\\end{bmatrix}.\n$$\n- 扩散时间 $t = 0.5$，缩放因子 $\\lambda = 1.0$。\n\n测试用例 2（无缺失边）：\n- 与测试用例 1 中相同的 $A_{\\mathrm{obs}}$。\n- 缺失边掩码 $M$ 是大小为 $4 \\times 4$ 的零矩阵。\n- 扩散时间 $t = 0.5$，缩放因子 $\\lambda = 1.0$。\n\n测试用例 3（不连通分量；分量间存在缺失边）：\n- $A_{\\mathrm{obs}} \\in \\mathbb{R}^{4 \\times 4}$:\n$$\n\\begin{bmatrix}\n0  0.5  0  0\\\\\n0.5  0  0  0\\\\\n0  0  0  0.5\\\\\n0  0  0.5  0\n\\end{bmatrix}.\n$$\n- 缺失边掩码 $M$：\n$$\n\\begin{bmatrix}\n0  0  0  1\\\\\n0  0  1  0\\\\\n0  1  0  0\\\\\n1  0  0  0\n\\end{bmatrix}.\n$$\n- 扩散时间 $t = 1.0$，缩放因子 $\\lambda = 1.0$。\n\n测试用例 4（时间边界，零扩散）：\n- 与测试用例 1 中相同的 $A_{\\mathrm{obs}}$ 和 $M$。\n- 扩散时间 $t = 0.0$，缩放因子 $\\lambda = 1.0$。\n\n测试用例 5（强扩散与部分缩放）：\n- 与测试用例 1 中相同的 $A_{\\mathrm{obs}}$ 和 $M$。\n- 扩散时间 $t = 3.0$，缩放因子 $\\lambda = 0.5$。\n\n对于每个测试用例，计算度变化向量 $\\Delta d$ 和每个节点的消息变化向量 $r$，均四舍五入至 $6$ 位小数。您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表形式的结果，其中每个元素对应一个测试用例，并且本身是两个列表的列表：第一个是 $\\Delta d$，第二个是 $r$。例如，输出格式必须像\n$[\\,[\\,[\\Delta d\\_\\mathrm{case1}], [r\\_\\mathrm{case1}]\\,], \\ldots, [\\,[\\Delta d\\_\\mathrm{case5}], [r\\_\\mathrm{case5}]\\,]\\,]$,\n并用实际的数字列表替换。不应打印额外的文本。",
            "solution": "问题陈述经评估有效。它在科学上基于图论和线性代数，特别是利用图拉普拉斯算子和矩阵指数来执行基于扩散的插补任务。该问题是良构的，为所定义的测试用例提供了清晰的算法描述和所有必要的数据。目标是定量的，不含主观因素。在每个节点消息变化向量 $r$ 的定义中存在一个微小的歧义，通过将“每个节点的欧几里得范数”解释为变化矩阵的行范数来解决，这是图机器学习领域的标准惯例。\n\n任务是实现一个图插补方案，并评估其对两个图属性的影响：节点度和单步图卷积网络（GCN）消息传递操作的输出。该过程是确定性的，可以使用标准的数值线性代数例程来实现。\n\n插补方法如下：\n给定一个观测到的邻接矩阵 $A_{\\mathrm{obs}}$（对称、非负、对角线为零）、一个指示缺失边的对称二元掩码 $M$、一个扩散时间 $t \\ge 0$ 和一个缩放因子 $\\lambda \\ge 0$。\n\n1.  **构建图拉普拉斯算子**：组合拉普拉斯算子 $L_{\\mathrm{obs}}$ 是从观测图中计算出来的。\n    $$\n    L_{\\mathrm{obs}} = D_{\\mathrm{obs}} - A_{\\mathrm{obs}}\n    $$\n    其中 $D_{\\mathrm{obs}}$ 是对角度矩阵，其对角线元素为 $(D_{\\mathrm{obs}})_{ii} = \\sum_j (A_{\\mathrm{obs}})_{ij}$。\n\n2.  **计算热核**：热核 $K_t$ 是图热方程的解，通过矩阵指数计算。\n    $$\n    K_t = \\exp(-t L_{\\mathrm{obs}})\n    $$\n    由于对于无向图，$L_{\\mathrm{obs}}$ 是实对称的，因此 $K_t$ 也是实对称的。\n\n3.  **形成插补矩阵**：通过取热核 $K_t$ 并将其对角线项设置为零来形成矩阵 $B$。这移除了自扩散项。\n    $$\n    B_{ij} = \\begin{cases} (K_t)_{ij}  i \\neq j \\\\ 0  i = j \\end{cases}\n    $$\n    新的边权重由 $B$ 中对应于掩码 $M$ 指定的缺失边的项确定，并由 $\\lambda$ 缩放。更新是逐元素应用的（哈达玛积 $\\circ$）。\n\n4.  **计算插补后的邻接矩阵**：通过将新的边权重加到 $A_{\\mathrm{obs}}$ 上并确保对称性，来形成插补后的邻接矩阵 $A_{\\mathrm{imp}}$。\n    $$\n    A_{\\mathrm{imp}} = \\mathrm{sym}\\big(A_{\\mathrm{obs}} + \\lambda \\,(M \\circ B)\\big) = \\frac{1}{2}\\left( (A_{\\mathrm{obs}} + \\lambda \\,(M \\circ B)) + (A_{\\mathrm{obs}} + \\lambda \\,(M \\circ B))^\\top \\right)\n    $$\n    由于 $A_{\\mathrm{obs}}$、$M$ 和 $B$ 都是对称的，它们的和与哈达玛积也是对称的，这使得 $\\mathrm{sym}(\\cdot)$ 操作在形式上是多余的，但却是一个好习惯。\n\n计算出 $A_{\\mathrm{imp}}$ 后，我们评估其影响。\n\n1.  **度变化向量 $\\Delta d$**：节点的度是其入射边权重的总和。度向量 $d$ 由 $d = A \\mathbf{1}$ 给出，其中 $\\mathbf{1}$ 是全1向量。度的变化是：\n    $$\n    \\Delta d = d_{\\mathrm{imp}} - d_{\\mathrm{obs}} = A_{\\mathrm{imp}}\\mathbf{1} - A_{\\mathrm{obs}}\\mathbf{1}\n    $$\n\n2.  **每个节点的消息变化向量 $r$**：这量化了单个GCN层输出的变化。归一化传播矩阵是 $S = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$，其中 $\\tilde{A} = A + I$，$\\tilde{D}$ 是 $\\tilde{A}$ 的度矩阵。给定一个节点特征矩阵 $X$，传播后特征的变化是 $(S_{\\mathrm{imp}} - S_{\\mathrm{obs}})X$。“每个节点”的变化是这个结果矩阵的每一行的欧几里得范数。设 $\\Delta M = (S_{\\mathrm{imp}} - S_{\\mathrm{obs}})X$。向量 $r$ 的项为：\n    $$\n    r_i = \\left\\lVert (\\Delta M)_{i,:} \\right\\rVert_2 = \\sqrt{\\sum_{k} (\\Delta M_{ik})^2}\n    $$\n    其中 $(\\Delta M)_{i,:}$ 表示矩阵 $\\Delta M$ 的第 $i$ 行。\n\n我们将实现一个为每个给定的测试用例执行这些步骤的程序。矩阵指数 $\\exp(\\cdot)$ 将使用 `scipy.linalg.expm` 计算，所有其他操作将使用 `numpy`。向量 $\\Delta d$ 和 $r$ 的最终数值结果将四舍五入到6位小数。\n\n特殊情况提供了有用的验证。如果 $M$ 是零矩阵（测试用例2）或者 $t=0$（测试用例4），那么 $K_t = I$，导致 $B=0$。在这两种情况下，插补项 $\\lambda(M \\circ B)$ 为零，导致 $A_{\\mathrm{imp}} = A_{\\mathrm{obs}}$。因此，$\\Delta d$ 和 $r$ 都必须是零向量，这可以作为实现的合理性检查。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    \n    # Node feature matrix X, common to all test cases.\n    X = np.array([\n        [1.0, 0.0],\n        [0.0, 1.0],\n        [1.0, 1.0],\n        [2.0, -1.0]\n    ])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A_obs\": np.array([\n                [0.0, 0.8, 0.0, 0.2],\n                [0.8, 0.0, 0.6, 0.0],\n                [0.0, 0.6, 0.0, 0.7],\n                [0.2, 0.0, 0.7, 0.0]\n            ]),\n            \"M\": np.array([\n                [0.0, 0.0, 1.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0],\n                [1.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0]\n            ]),\n            \"t\": 0.5,\n            \"lam\": 1.0\n        },\n        {\n            \"A_obs\": np.array([\n                [0.0, 0.8, 0.0, 0.2],\n                [0.8, 0.0, 0.6, 0.0],\n                [0.0, 0.6, 0.0, 0.7],\n                [0.2, 0.0, 0.7, 0.0]\n            ]),\n            \"M\": np.zeros((4, 4)),\n            \"t\": 0.5,\n            \"lam\": 1.0\n        },\n        {\n            \"A_obs\": np.array([\n                [0.0, 0.5, 0.0, 0.0],\n                [0.5, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.5],\n                [0.0, 0.0, 0.5, 0.0]\n            ]),\n            \"M\": np.array([\n                [0.0, 0.0, 0.0, 1.0],\n                [0.0, 0.0, 1.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0],\n                [1.0, 0.0, 0.0, 0.0]\n            ]),\n            \"t\": 1.0,\n            \"lam\": 1.0\n        },\n        {\n            \"A_obs\": np.array([\n                [0.0, 0.8, 0.0, 0.2],\n                [0.8, 0.0, 0.6, 0.0],\n                [0.0, 0.6, 0.0, 0.7],\n                [0.2, 0.0, 0.7, 0.0]\n            ]),\n            \"M\": np.array([\n                [0.0, 0.0, 1.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0],\n                [1.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0]\n            ]),\n            \"t\": 0.0,\n            \"lam\": 1.0\n        },\n        {\n            \"A_obs\": np.array([\n                [0.0, 0.8, 0.0, 0.2],\n                [0.8, 0.0, 0.6, 0.0],\n                [0.0, 0.6, 0.0, 0.7],\n                [0.2, 0.0, 0.7, 0.0]\n            ]),\n            \"M\": np.array([\n                [0.0, 0.0, 1.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0],\n                [1.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0]\n            ]),\n            \"t\": 3.0,\n            \"lam\": 0.5\n        }\n    ]\n\n    def compute_effects(A_obs, M, t, lam, X_feat):\n        \"\"\"\n        Computes the imputed adjacency and the resulting changes.\n        \"\"\"\n        n = A_obs.shape[0]\n\n        # Step 1: Compute A_imp\n        D_obs_diag = A_obs.sum(axis=1)\n        D_obs = np.diag(D_obs_diag)\n        L_obs = D_obs - A_obs\n        \n        K_t = expm(-t * L_obs)\n        B = K_t - np.diag(np.diag(K_t))\n        \n        A_imp_pre_sym = A_obs + lam * (M * B)\n        A_imp = 0.5 * (A_imp_pre_sym + A_imp_pre_sym.T)\n\n        # Step 2: Compute degree change Delta d\n        d_obs = A_obs.sum(axis=1)\n        d_imp = A_imp.sum(axis=1)\n        delta_d = d_imp - d_obs\n\n        # Step 3: Compute message change r\n        def get_S(A):\n            A_tilde = A + np.eye(n)\n            d_tilde = A_tilde.sum(axis=1)\n            # Create D_tilde^(-1/2) safely\n            d_tilde_inv_sqrt = np.power(d_tilde, -0.5, where=d_tilde  0)\n            D_tilde_inv_sqrt = np.diag(d_tilde_inv_sqrt)\n            return D_tilde_inv_sqrt @ A_tilde @ D_tilde_inv_sqrt\n\n        S_obs = get_S(A_obs)\n        S_imp = get_S(A_imp)\n        \n        delta_M = (S_imp - S_obs) @ X_feat\n        r = np.linalg.norm(delta_M, axis=1)\n\n        return delta_d, r\n\n    all_results = []\n    for case in test_cases:\n        delta_d, r = compute_effects(case[\"A_obs\"], case[\"M\"], case[\"t\"], case[\"lam\"], X)\n        \n        # Round results to 6 decimal places\n        delta_d_rounded = [round(val, 6) for val in delta_d]\n        r_rounded = [round(val, 6) for val in r]\n        \n        all_results.append([delta_d_rounded, r_rounded])\n\n    # Manually construct the final output string to match the required format\n    def format_list(data_list):\n        return f\"[{','.join(f'{x:.6f}' for x in data_list)}]\"\n\n    output_parts = []\n    for result_pair in all_results:\n        delta_d_str = format_list(result_pair[0])\n        r_str = format_list(result_pair[1])\n        pair_str = f\"[{delta_d_str},{r_str}]\"\n        output_parts.append(pair_str)\n\n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}