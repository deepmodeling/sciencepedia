{
    "hands_on_practices": [
        {
            "introduction": "Before deploying complex models like LSTMs, it is crucial to understand their internal structure and complexity. This first exercise provides a foundational look into the mechanics of a standard LSTM cell. By systematically counting the number of trainable parameters based on its gate structure, you will gain a concrete understanding of how model architecture translates into computational cost and model capacity .",
            "id": "4175979",
            "problem": "A laboratory is building a recurrent neural model to analyze sequences of fluorescence intensity extracted from two-photon calcium imaging in mouse visual cortex. Each time step provides a feature vector encoding preprocessed indicators such as deconvolved spike probability, local neuropil correction, motion-corrected region-of-interest intensity, and task covariates. The model uses a single-layer Long Short-Term Memory (LSTM) unit to predict a latent neural state trajectory from these inputs, without any peephole connections or output projections.\n\nLet the input at each time step be a vector in $\\mathbb{R}^{n_{x}}$ and the hidden state be a vector in $\\mathbb{R}^{n_{h}}$. In a standard LSTM, the pre-activation for each of the following components is computed by an affine transformation of the current input and the previous hidden state, followed by a nonlinearity that does not introduce additional trainable parameters:\n\n- the input gate,\n- the forget gate,\n- the output gate,\n- the candidate cell-state update.\n\nAssume that for each component, the weights mapping the input and the previous hidden state, as well as the biases, are all distinct across components and time-invariant. Under these assumptions, use the core definition that an affine map from $\\mathbb{R}^{a}$ to $\\mathbb{R}^{b}$ contains a matrix with $a \\times b$ scalar entries and a bias vector with $b$ scalar entries. Derive, in closed form, the total number of trainable scalar parameters in this LSTM layer as a function of $n_{x}$ and $n_{h}$, accounting for all weights and biases for the input gate, forget gate, output gate, and candidate.\n\nYour final answer must be a single analytic expression in terms of $n_{x}$ and $n_{h}$ with no units.",
            "solution": "The problem requires the derivation of the total number of trainable parameters in a single-layer Long Short-Term Memory (LSTM) unit as a function of the input vector dimension, $n_{x}$, and the hidden state dimension, $n_{h}$. The LSTM architecture is specified as standard, without peephole connections or output projections.\n\nFirst, we must validate the problem statement.\n\n### Step 1: Extract Givens\n- The model is a single-layer Long Short-Term Memory (LSTM) unit.\n- The input vector at each time step, $x_t$, is in $\\mathbb{R}^{n_{x}}$, so its dimension is $n_{x}$.\n- The hidden state vector, $h_t$, and cell state vector, $c_t$, are in $\\mathbb{R}^{n_{h}}$, so their dimension is $n_{h}$.\n- The LSTM architecture has no peephole connections and no output projections.\n- The pre-activation for each of four components (input gate, forget gate, output gate, and candidate cell-state update) is computed via an affine transformation.\n- The input to each affine transformation is the current input vector, $x_t$, and the previous hidden state vector, $h_{t-1}$.\n- The trainable parameters (weights and biases) are distinct for each of the four components and are time-invariant.\n- An affine map from $\\mathbb{R}^{a}$ to $\\mathbb{R}^{b}$ comprises a weight matrix with $a \\times b$ scalar entries and a bias vector with $b$ scalar entries.\n- The task is to derive the total number of trainable scalar parameters in terms of $n_{x}$ and $n_{h}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the standard mathematical definition of an LSTM, a cornerstone of recurrent neural networks. The application context, neuroscience data analysis, is a valid and common use case for such models. The formulation is correct.\n- **Well-Posed**: The problem is clearly defined with all necessary variables ($n_{x}$, $n_{h}$), components, and constraints (no peepholes). A unique mathematical expression can be derived.\n- **Objective**: The problem is stated using precise, objective mathematical and technical language.\n\nThe problem is found to be valid as it is scientifically sound, well-posed, and objective. It does not violate any of the invalidity criteria. We may proceed to the solution.\n\n### Derivation of the Solution\n\nThe total number of trainable parameters in the LSTM layer is the sum of the parameters from its constituent components. The problem specifies four components whose parameters we must count: the input gate, the forget gate, the output gate, and the candidate cell-state update.\n\nLet $x_t \\in \\mathbb{R}^{n_{x}}$ be the input vector at time step $t$, and let $h_{t-1} \\in \\mathbb{R}^{n_{h}}$ be the hidden state vector from the previous time step $t-1$. In a standard LSTM, the input to the affine transformation for each component is the concatenation of $x_t$ and $h_{t-1}$. This concatenated vector, which we can represent as $[h_{t-1}, x_t]$, has a dimension of $n_{h} + n_{x}$.\n\nThe output of the affine transformation for each of the four components is a vector of dimension $n_{h}$, which matches the dimension of the hidden and cell states.\n\nThe problem provides the rule for counting parameters in an affine map from a vector space of dimension $a$ to one of dimension $b$: the number of parameters is the sum of the elements in the weight matrix ($a \\times b$) and the elements in the bias vector ($b$). Thus, the total number of parameters for one such transformation is $ab + b$.\n\nWe will now apply this rule to each of the four components of the LSTM. For each component, the input dimension is $a = n_{x} + n_{h}$ and the output dimension is $b = n_{h}$.\n\n1.  **Input Gate ($i_t$)**: The pre-activation is $W_i [h_{t-1}, x_t] + b_i$.\n    - The weight matrix $W_i$ has dimensions $n_{h} \\times (n_{x} + n_{h})$. The number of parameters in $W_i$ is $n_{h}(n_{x} + n_{h})$.\n    - The bias vector $b_i$ has dimension $n_{h}$. The number of parameters in $b_i$ is $n_{h}$.\n    - Total parameters for the input gate: $n_{h}(n_{x} + n_{h}) + n_{h}$.\n\n2.  **Forget Gate ($f_t$)**: The pre-activation is $W_f [h_{t-1}, x_t] + b_f$.\n    - The weight matrix $W_f$ has dimensions $n_{h} \\times (n_{x} + n_{h})$. Number of parameters: $n_{h}(n_{x} + n_{h})$.\n    - The bias vector $b_f$ has dimension $n_{h}$. Number of parameters: $n_{h}$.\n    - Total parameters for the forget gate: $n_{h}(n_{x} + n_{h}) + n_{h}$.\n\n3.  **Output Gate ($o_t$)**: The pre-activation is $W_o [h_{t-1}, x_t] + b_o$.\n    - The weight matrix $W_o$ has dimensions $n_{h} \\times (n_{x} + n_{h})$. Number of parameters: $n_{h}(n_{x} + n_{h})$.\n    - The bias vector $b_o$ has dimension $n_{h}$. Number of parameters: $n_{h}$.\n    - Total parameters for the output gate: $n_{h}(n_{x} + n_{h}) + n_{h}$.\n\n4.  **Candidate Cell-State Update ($\\tilde{c}_t$)**: The pre-activation is $W_c [h_{t-1}, x_t] + b_c$.\n    - The weight matrix $W_c$ has dimensions $n_{h} \\times (n_{x} + n_{h})$. Number of parameters: $n_{h}(n_{x} + n_{h})$.\n    - The bias vector $b_c$ has dimension $n_{h}$. Number of parameters: $n_{h}$.\n    - Total parameters for the candidate cell-state update: $n_{h}(n_{x} + n_{h}) + n_{h}$.\n\nThe problem states that the parameters for each component are distinct. Therefore, the total number of trainable parameters, $N_{total}$, is the sum of the parameters from these four components. Since each component has the same number of parameters, we can multiply the count for a single component by $4$.\n\n$$N_{total} = 4 \\times (\\text{Parameters for one component})$$\n$$N_{total} = 4 \\times (n_{h}(n_{x} + n_{h}) + n_{h})$$\n\nTo obtain the final closed-form expression, we simplify this equation:\n\n$$N_{total} = 4(n_{h}n_{x} + n_{h}^{2} + n_{h})$$\n\nFactoring out the common term $n_h$ from the expression inside the parentheses yields a more compact form:\n\n$$N_{total} = 4n_{h}(n_{x} + n_{h} + 1)$$\n\nThis is the total number of trainable scalar parameters in the specified LSTM layer.",
            "answer": "$$\n\\boxed{4n_{h}(n_{x} + n_{h} + 1)}\n$$"
        },
        {
            "introduction": "While the LSTM is a powerful architecture, variations like the Gated Recurrent Unit (GRU) have been developed to optimize performance and efficiency. This practice pits the two architectures against each other in a direct comparison of model complexity. Deriving and calculating the parameter counts for both models highlights the GRU's more streamlined design and prompts consideration of the important trade-off between model capacity and sample efficiency, a common challenge in data-limited fields like neuroscience .",
            "id": "4175962",
            "problem": "A neuroscience laboratory is designing recurrent neural network layers to decode population spiking activity into kinematic variables. At each time bin, the input feature vector has dimension $n_{x}$ (e.g., binned spike counts across channels and exogenous covariates), and the recurrent hidden state has dimension $n_{h}$. Consider two candidate architectures for a single recurrent layer: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). Long Short-Term Memory (LSTM) layers use four distinct gating computations per time step (input gate, forget gate, output gate, and a cell candidate transformation), while Gated Recurrent Unit (GRU) layers use two gating computations (update and reset) and a hidden candidate transformation. In standard formulations without peephole connections and without any output projection, each gate or candidate transformation computes an affine map of the current input and the previous hidden state followed by a pointwise nonlinearity, and each such affine map has its own learned bias.\n\nStarting solely from these definitions and the fact that an affine map from an input of dimension $n$ to an output of dimension $m$ is parameterized by an $m \\times n$ weight matrix together with an $m$-dimensional bias vector, derive the exact total number of scalar trainable parameters (including biases) in an LSTM layer and in a GRU layer, each with input dimension $n_{x}$ and hidden dimension $n_{h}$. Then, for the realistic setting $n_{x} = 128$ and $n_{h} = 256$, compute the numerical parameter counts for both layers and the ratio of the LSTM parameter count to the GRU parameter count. Report your final answer as a row matrix containing, in order, the LSTM parameter count, the GRU parameter count, and their ratio. Provide exact counts; no rounding is required. Finally, explain, based on your derivation, the implications of these parameter counts for sample efficiency when training on limited neuroscience datasets with relatively few trials compared to the number of parameters.",
            "solution": "We begin from the definition that each gate or candidate transformation in a recurrent neural network layer computes an affine map from the concatenated inputs comprising the current external input and the previous hidden state to the gate or candidate output, followed by a nonlinearity. An affine map from an input of dimension $n$ to an output of dimension $m$ is parameterized by an $m \\times n$ weight matrix together with an $m$-dimensional bias vector, yielding $m \\cdot n + m$ scalar parameters.\n\nIn the standard Long Short-Term Memory (LSTM) layer without peephole connections, there are four distinct affine computations per time step:\n- The input gate, denoted $i_{t}$, which maps $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional gate output.\n- The forget gate, denoted $f_{t}$, which maps $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional gate output.\n- The output gate, denoted $o_{t}$, which maps $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional gate output.\n- The cell candidate transformation, denoted $\\tilde{c}_{t}$, which maps $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional candidate.\n\nEach of these four computations uses two weight matrices: one mapping the current input $x_{t}$ of dimension $n_{x}$ into an $n_{h}$-dimensional output, and one mapping the previous hidden state $h_{t-1}$ of dimension $n_{h}$ into an $n_{h}$-dimensional output, along with an $n_{h}$-dimensional bias. Therefore, per gate or candidate transformation, the parameter count is\n$$\nn_{h} \\cdot n_{x} \\;+\\; n_{h} \\cdot n_{h} \\;+\\; n_{h}.\n$$\nBecause there are four such computations in an LSTM, the total LSTM parameter count is\n$$\nP_{\\mathrm{LSTM}}(n_{x}, n_{h}) \\;=\\; 4 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right).\n$$\n\nIn the standard Gated Recurrent Unit (GRU) layer, there are three distinct affine computations per time step:\n- The update gate, denoted $z_{t}$, mapping $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional gate output.\n- The reset gate, denoted $r_{t}$, mapping $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional gate output.\n- The hidden candidate transformation, denoted $\\tilde{h}_{t}$, mapping $(x_{t}, r_{t} \\odot h_{t-1})$ to an $n_{h}$-dimensional candidate.\n\nEach of these three computations similarly uses two weight matrices and one bias vector sized to produce an $n_{h}$-dimensional output, so the parameter count per computation is again\n$$\nn_{h} \\cdot n_{x} \\;+\\; n_{h} \\cdot n_{h} \\;+\\; n_{h}.\n$$\nBecause there are three such computations in a GRU, the total GRU parameter count is\n$$\nP_{\\mathrm{GRU}}(n_{x}, n_{h}) \\;=\\; 3 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right).\n$$\n\nWe are asked to compute the numerical counts for $n_{x} = 128$ and $n_{h} = 256$. First compute the shared inner quantity:\n$$\nn_{h} n_{x} + n_{h}^{2} + n_{h}\n\\;=\\;\n256 \\cdot 128 \\;+\\; 256^{2} \\;+\\; 256.\n$$\nEvaluate each term:\n$$\n256 \\cdot 128 \\;=\\; 32768, \\quad\n256^{2} \\;=\\; 65536, \\quad\n256 \\;=\\; 256.\n$$\nSumming gives\n$$\n32768 + 65536 + 256 \\;=\\; 98560.\n$$\nTherefore,\n$$\nP_{\\mathrm{LSTM}}(128, 256) \\;=\\; 4 \\cdot 98560 \\;=\\; 394240,\n$$\nand\n$$\nP_{\\mathrm{GRU}}(128, 256) \\;=\\; 3 \\cdot 98560 \\;=\\; 295680.\n$$\nThe ratio of the LSTM parameter count to the GRU parameter count simplifies to\n$$\n\\frac{P_{\\mathrm{LSTM}}(n_{x}, n_{h})}{P_{\\mathrm{GRU}}(n_{x}, n_{h})}\n\\;=\\;\n\\frac{4 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right)}{3 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right)}\n\\;=\\;\n\\frac{4}{3},\n$$\nwhich is dimension-independent under the stated assumptions.\n\nImplications for sample efficiency in limited neuroscience datasets: The dominant term in both $P_{\\mathrm{LSTM}}$ and $P_{\\mathrm{GRU}}$ is $n_{h}^{2}$, reflecting the quadratic growth in parameters due to recurrent connections. The LSTM has a constant-factor increase of $\\frac{4}{3}$ in parameters relative to the GRU because it instantiates four affine computations per time step rather than three. In regimes where the number of training trials or sequences is limited relative to the number of parameters, the larger parameter count generally increases the risk of overfitting and demands stronger regularization or more data to achieve comparable generalization. Conversely, the GRU’s smaller parameterization can be more sample-efficient, potentially yielding better generalization with fewer trials, provided its representational capacity suffices for the temporal dependencies in the neural data. The exact trade-off depends on task complexity, regularization strength, prior constraints, and architectural details, but the derived counts provide a principled way to anticipate and compare sample efficiency constraints when choosing between these recurrent layer types.",
            "answer": "$$\\boxed{\\begin{pmatrix}394240 & 295680 & \\frac{4}{3}\\end{pmatrix}}$$"
        },
        {
            "introduction": "The primary motivation for developing LSTMs and GRUs was the failure of simple Recurrent Neural Networks (RNNs) to capture long-range dependencies, largely due to the vanishing gradient problem. This exercise brings this theoretical issue to life through a simplified but highly illustrative \"adding problem.\" By modeling and comparing the decay of the training signal across an RNN, GRU, and LSTM, you will gain a tangible intuition for why gating mechanisms are so effective at preserving information over long temporal sequences .",
            "id": "3191191",
            "problem": "You are asked to formalize and analyze the well-known \"adding problem\" in sequence learning, focusing on the issue of long-term dependencies in deep learning. The adding problem is defined as follows: given a sequence of real numbers in the interval $[0,1]$ of length $L$, exactly two positions are marked, and the target is the sum of the values at the marked positions. A model processes the sequence step by step and outputs a single scalar at the final time step. To isolate the effect of long-time dependencies, consider the worst case in which the first marked position is at $t=1$ and the final output is produced at $t=L$, so the learning signal for the earliest relevant time step must traverse $L-1$ recurrent transitions.\n\nWork with three architectures:\n- Recurrent Neural Network (RNN) with recurrent update using a smooth nonlinearity,\n- Gated Recurrent Unit (GRU),\n- Long Short-Term Memory (LSTM).\n\nAssume the following scientifically standard and widely accepted foundations:\n- The chain rule of differentiation for compositions of functions applies to backpropagation through time, so that gradients at earlier time steps are products of Jacobians across time steps.\n- For stability, the magnitude of backpropagated gradients is governed by the operator norm of the recurrent Jacobian factors along the path that carries memory across time steps.\n- The recurrent Jacobian for a Recurrent Neural Network (RNN) near the origin is dominated by the recurrent weight matrix, and its long-time behavior is controlled by its spectral radius.\n- The gating mechanisms in Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) architectures multiply the previous state by gate values, thereby directly scaling the gradient that flows through the main memory path.\n\nTo make the comparison explicit and computationally tractable, adopt the following consistent simplifications for the analysis:\n- Treat the recurrent Jacobian of the Recurrent Neural Network (RNN) as approximately time-invariant with spectral radius $\\rho$.\n- Treat the forget gate of the Long Short-Term Memory (LSTM) and the update gate of the Gated Recurrent Unit (GRU) as constant scalars across time: the LSTM forget gate is $f$, and the GRU update gate is $z$.\n- Use a normalized output and loss so that constant multiplicative factors outside the recurrent pathway can be absorbed into a single positive scalar $\\alpha$.\n\nParameter values to use in all computations:\n- $\\rho = 0.90$ for the Recurrent Neural Network (RNN),\n- $f = 0.99$ for the Long Short-Term Memory (LSTM),\n- $z = 0.05$ for the Gated Recurrent Unit (GRU),\n- $\\alpha = 1.0$.\n\nDefine the \"training-signal magnitude\" for the earliest marked input as the magnitude of the gradient of the loss with respect to the earliest marked input, propagated along the principal memory path from time $t=L$ back to $t=1$. Using the foundations stated above (chain rule and product of Jacobians), derive expressions for the training-signal magnitude for each architecture as functions of $L$ and the given parameters. Then implement these expressions in a program.\n\nTest suite:\n- Sequence lengths $L \\in \\{50, 75, 100, 500, 1000\\}$.\n- For each $L$, compute the training-signal magnitude for each architecture in the order: Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where the list is flattened across the test suite in ascending order of $L$. That is, the output must be:\n$[$RNN$(50)$,GRU$(50)$,LSTM$(50)$,RNN$(75)$,GRU$(75)$,LSTM$(75)$,RNN$(100)$,GRU$(100)$,LSTM$(100)$,RNN$(500)$,GRU$(500)$,LSTM$(500)$,RNN$(1000)$,GRU$(1000)$,LSTM$(1000)]$,\nwhere each entry is a float representing the training-signal magnitude (unitless). The final outputs are floats; no physical units or angles are involved, and no percentages must be printed.",
            "solution": "The user requests a formal analysis of the \"adding problem\" to compare the ability of three recurrent architectures—Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), and Long Short-Term Memory (LSTM)—to handle long-term dependencies. The analysis focuses on the magnitude of the gradient signal propagated from the final time step back to the first relevant input.\n\nThe problem is validated as follows:\n- **Step 1: Extract Givens**\n  - Problem: The \"adding problem\" for a sequence of length $L$.\n  - Goal: Compute the \"training-signal magnitude\" for an input at time $t=1$ when the output is at $t=L$.\n  - Architectures: RNN, GRU, LSTM.\n  - Foundations:\n    - Backpropagation through time is based on the chain rule.\n    - Gradient magnitude is governed by the operator norm of recurrent Jacobians.\n    - RNN Jacobian behavior is controlled by the spectral radius of the recurrent weight matrix.\n    - Gating mechanisms in GRU and LSTM scale the gradient flow.\n  - Simplifications for analysis:\n    - RNN recurrent Jacobian is treated as time-invariant with spectral radius $\\rho$.\n    - LSTM forget gate is a constant scalar $f$.\n    - GRU update gate is a constant scalar $z$.\n    - A single positive scalar $\\alpha$ absorbs all constant multiplicative factors outside the recurrent pathway.\n  - Parameter values:\n    - $\\rho = 0.90$\n    - $f = 0.99$\n    - $z = 0.05$\n    - $\\alpha = 1.0$\n  - Test Suite:\n    - Sequence lengths $L \\in \\{50, 75, 100, 500, 1000\\}$.\n    - Computation order: RNN, GRU, LSTM for each $L$.\n\n- **Step 2: Validate Using Extracted Givens**\n  - **Scientifically Grounded:** The problem is firmly rooted in the established principles of deep learning, specifically the analysis of gradient flow in recurrent networks (the vanishing/exploding gradient problem). The simplifications are standard for creating a tractable analytical model of a complex system.\n  - **Well-Posed:** The problem is clearly defined with all necessary parameters and assumptions, leading to a unique, computable solution for each case.\n  - **Objective:** The language is precise and technical, free of subjectivity.\n\n- **Step 3: Verdict and Action**\n  - The problem is deemed **valid**. The simplifications are explicitly stated and serve to isolate the core mechanism of gradient propagation, which is a standard and informative analytical technique. Proceeding with the solution.\n\nThe core principle for analyzing the training-signal magnitude is backpropagation through time (BPTT). The gradient of the loss function $\\mathcal{L}$ with respect to the hidden state $h_t$ at some time step $t$ is computed via the chain rule by propagating the gradient from a later time step $t+1$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_t} = \\frac{\\partial \\mathcal{L}}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_t}\n$$\nTo find the gradient with respect to the state at time $t=1$ from the loss computed at time $t=L$, we must apply this rule recursively for $L-1$ steps:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_1} = \\frac{\\partial \\mathcal{L}}{\\partial h_L} \\left( \\prod_{t=2}^{L} \\frac{\\partial h_t}{\\partial h_{t-1}} \\right)\n$$\nThe term $\\frac{\\partial h_t}{\\partial h_{t-1}}$ is the recurrent Jacobian matrix at time $t$. The \"training-signal magnitude\" for the input at $t=1$ is dominated by the magnitude of this product of Jacobians, which determines how much the error signal from the output at $t=L$ is amplified or diminished as it travels back to $t=1$. The problem defines a constant $\\alpha$ to absorb all non-recurrent factors, such as $\\frac{\\partial \\mathcal{L}}{\\partial h_L}$ and the final step $\\frac{\\partial h_1}{\\partial x_1}$. Thus, the signal magnitude $S(L)$ is proportional to the norm of the product of Jacobians. Let's analyze this for each architecture.\n\n**Recurrent Neural Network (RNN)**\nThe RNN hidden state update is of the form $h_t = \\phi(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$, where $\\phi$ is a nonlinear activation function like $\\tanh$. The recurrent Jacobian is $\\frac{\\partial h_t}{\\partial h_{t-1}} = \\text{diag}(\\phi'(...))W_{hh}$. The long-term behavior of the product of these matrices is governed by the spectral radius $\\rho$ of the recurrent weight matrix $W_{hh}$. The problem simplifies the analysis by assuming the effective magnitude contribution of each Jacobian step is a constant factor $\\rho$. Propagating the signal across $L-1$ time steps results in this factor being multiplied $L-1$ times.\nThe training-signal magnitude $S_{RNN}$ for a sequence of length $L$ is therefore:\n$$\nS_{RNN}(L) = \\alpha \\rho^{L-1}\n$$\nGiven $\\alpha = 1.0$ and $\\rho = 0.90$, we have:\n$$\nS_{RNN}(L) = (0.90)^{L-1}\n$$\n\n**Long Short-Term Memory (LSTM)**\nThe key to the LSTM's ability to handle long dependencies is its cell state, $c_t$, which is updated via a gating mechanism: $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$. Here, $f_t$ is the forget gate and $\\odot$ denotes element-wise multiplication. The gradient pathway through the cell state from $c_t$ to $c_{t-1}$ is primarily scaled by the forget gate, i.e., $\\frac{\\partial c_t}{\\partial c_{t-1}} \\approx \\text{diag}(f_t)$. The problem simplifies this by assuming the forget gate is a constant scalar $f$ for all time steps. The gradient signal flowing back through the main memory path is scaled by $f$ at each of the $L-1$ steps.\nThe training-signal magnitude $S_{LSTM}$ is:\n$$\nS_{LSTM}(L) = \\alpha f^{L-1}\n$$\nGiven $\\alpha = 1.0$ and $f = 0.99$, we have:\n$$\nS_{LSTM}(L) = (0.99)^{L-1}\n$$\n\n**Gated Recurrent Unit (GRU)**\nThe GRU state update is $h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$, where $z_t$ is the update gate. The term $(1-z_t)$ acts as a dynamic forget gate, controlling how much of the previous state $h_{t-1}$ is passed to the current state $h_t$. The gradient of $h_t$ with respect to $h_{t-1}$ is therefore directly scaled by this factor. The problem simplifies the analysis by assuming a constant scalar update gate $z$. Consequently, the scaling factor at each of the $L-1$ backpropagation steps is $(1-z)$.\nThe training-signal magnitude $S_{GRU}$ is:\n$$\nS_{GRU}(L) = \\alpha (1-z)^{L-1}\n$$\nGiven $\\alpha = 1.0$ and $z = 0.05$, the scaling factor is $(1-0.05) = 0.95$. We have:\n$$\nS_{GRU}(L) = (0.95)^{L-1}\n$$\n\nThese derived expressions will now be implemented to compute the required values for the specified test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the training-signal magnitude for RNN, GRU, and LSTM architectures\n    for the \"adding problem\" over various sequence lengths.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    rho_rnn = 0.90  # Spectral radius for RNN\n    f_lstm = 0.99   # Forget gate value for LSTM\n    z_gru = 0.05    # Update gate value for GRU\n    alpha = 1.0     # Constant multiplicative factor (can be omitted as it's 1.0)\n    \n    # Define the test cases for sequence length L from the problem statement.\n    test_cases_L = [50, 75, 100, 500, 1000]\n\n    results = []\n    for L in test_cases_L:\n        # The number of recurrent transitions is L-1.\n        exponent = L - 1\n\n        # Calculate the training-signal magnitude for each architecture.\n        # S(L) = alpha * (base)^(L-1)\n        \n        # RNN\n        s_rnn = alpha * (rho_rnn ** exponent)\n        \n        # GRU\n        # The scaling factor is (1 - z)\n        s_gru = alpha * ((1 - z_gru) ** exponent)\n        \n        # LSTM\n        s_lstm = alpha * (f_lstm ** exponent)\n        \n        # Append results in the specified order: RNN, GRU, LSTM\n        results.append(s_rnn)\n        results.append(s_gru)\n        results.append(s_lstm)\n\n    # Format the final output as a comma-separated list of floats in brackets.\n    # e.g., [val1,val2,val3,...]\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}