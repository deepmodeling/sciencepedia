{
    "hands_on_practices": [
        {
            "introduction": "在应用任何神经网络时，第一步是理解其架构和复杂性。本练习将引导您推导并比较 LSTM 和 GRU 单元中的参数数量。通过这样做，您将对它们的结构差异以及在模型选择中的实际意义（尤其是在处理有限的神经科学数据时）有一个具体的认识。",
            "id": "4175962",
            "problem": "一个神经科学实验室正在设计循环神经网络层，用于将群体脉冲活动解码为运动学变量。在每个时间窗内，输入特征向量的维度为 $n_{x}$（例如，跨通道的分箱脉冲计数和外生协变量），循环隐藏状态的维度为 $n_{h}$。考虑单个循环层的两种候选架构：长短期记忆（Long Short-Term Memory, LSTM）和门控循环单元（Gated Recurrent Unit, GRU）。长短期记忆（LSTM）层在每个时间步使用四种不同的门控计算（输入门、遗忘门、输出门以及一个单元候选变换），而门控循环单元（GRU）层使用两种门控计算（更新门和重置门）以及一个隐藏候选变换。在没有窥视孔连接且没有任何输出投影的标准公式中，每个门或候选变换都会计算当前输入和前一隐藏状态的仿射映射，然后进行逐点非线性变换，并且每个这样的仿射映射都有其自身学习到的偏置。\n\n仅从这些定义以及以下事实出发：一个从维度为 $n$ 的输入到维度为 $m$ 的输出的仿射映射由一个 $m \\times n$ 的权重矩阵和一个 $m$ 维的偏置向量参数化，请推导出输入维度为 $n_{x}$、隐藏维度为 $n_{h}$ 的 LSTM 层和 GRU 层中可训练标量参数（包括偏置）的确切总数。然后，在 $n_{x} = 128$ 和 $n_{h} = 256$ 的实际设置下，计算两种层的参数数量，以及 LSTM 参数数量与 GRU 参数数量的比率。将你的最终答案以一个行矩阵的形式报告，该矩阵按顺序包含 LSTM 的参数数量、GRU 的参数数量以及它们的比率。提供确切的计数值；无需四舍五入。最后，根据你的推导，解释当在试验次数相对于参数数量较少的有限神经科学数据集上进行训练时，这些参数数量对样本效率有何影响。",
            "solution": "我们从以下定义开始：循环神经网络层中的每个门或候选变换都计算一个从包含当前外部输入和前一隐藏状态的拼接输入到门或候选输出的仿射映射，然后进行非线性变换。一个从维度为 $n$ 的输入到维度为 $m$ 的输出的仿射映射由一个 $m \\times n$ 的权重矩阵和一个 $m$ 维的偏置向量参数化，产生 $m \\cdot n + m$ 个标量参数。\n\n在没有窥视孔连接的标准长短期记忆（LSTM）层中，每个时间步有四种不同的仿射计算：\n- 输入门，记作 $i_{t}$，它将 $(x_{t}, h_{t-1})$ 映射到一个 $n_{h}$ 维的门输出。\n- 遗忘门，记作 $f_{t}$，它将 $(x_{t}, h_{t-1})$ 映射到一个 $n_{h}$ 维的门输出。\n- 输出门，记作 $o_{t}$，它将 $(x_{t}, h_{t-1})$ 映射到一个 $n_{h}$ 维的门输出。\n- 单元候选变换，记作 $\\tilde{c}_{t}$，它将 $(x_{t}, h_{t-1})$ 映射到一个 $n_{h}$ 维的候选值。\n\n这四种计算中的每一种都使用两个权重矩阵：一个将维度为 $n_{x}$ 的当前输入 $x_{t}$ 映射到 $n_{h}$ 维的输出，另一个将维度为 $n_{h}$ 的前一隐藏状态 $h_{t-1}$ 映射到 $n_{h}$ 维的输出，同时还有一个 $n_{h}$ 维的偏置。因此，每个门或候选变换的参数数量为\n$$\nn_{h} \\cdot n_{x} \\;+\\; n_{h} \\cdot n_{h} \\;+\\; n_{h}.\n$$\n因为 LSTM 中有四个这样的计算，所以 LSTM 的总参数数量为\n$$\nP_{\\mathrm{LSTM}}(n_{x}, n_{h}) \\;=\\; 4 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right).\n$$\n\n在标准门控循环单元（GRU）层中，每个时间步有三种不同的仿射计算：\n- 更新门，记作 $z_{t}$，将 $(x_{t}, h_{t-1})$ 映射到一个 $n_{h}$ 维的门输出。\n- 重置门，记作 $r_{t}$，将 $(x_{t}, h_{t-1})$ 映射到一个 $n_{h}$ 维的门输出。\n- 隐藏候选变换，记作 $\\tilde{h}_{t}$，将 $(x_{t}, r_{t} \\odot h_{t-1})$ 映射到一个 $n_{h}$ 维的候选值。\n\n这三种计算中的每一种同样使用两个权重矩阵和一个偏置向量，其大小设定为产生一个 $n_{h}$ 维的输出，因此每次计算的参数数量同样是\n$$\nn_{h} \\cdot n_{x} \\;+\\; n_{h} \\cdot n_{h} \\;+\\; n_{h}.\n$$\n因为 GRU 中有三个这样的计算，所以 GRU 的总参数数量为\n$$\nP_{\\mathrm{GRU}}(n_{x}, n_{h}) \\;=\\; 3 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right).\n$$\n\n题目要求我们计算当 $n_{x} = 128$ 和 $n_{h} = 256$ 时的具体数值。首先计算共享的内部量：\n$$\nn_{h} n_{x} + n_{h}^{2} + n_{h}\n\\;=\\;\n256 \\cdot 128 \\;+\\; 256^{2} \\;+\\; 256.\n$$\n计算每一项：\n$$\n256 \\cdot 128 \\;=\\; 32768, \\quad\n256^{2} \\;=\\; 65536, \\quad\n256 \\;=\\; 256.\n$$\n求和得到\n$$\n32768 + 65536 + 256 \\;=\\; 98560.\n$$\n因此，\n$$\nP_{\\mathrm{LSTM}}(128, 256) \\;=\\; 4 \\cdot 98560 \\;=\\; 394240,\n$$\n和\n$$\nP_{\\mathrm{GRU}}(128, 256) \\;=\\; 3 \\cdot 98560 \\;=\\; 295680.\n$$\nLSTM 参数数量与 GRU 参数数量的比率简化为\n$$\n\\frac{P_{\\mathrm{LSTM}}(n_{x}, n_{h})}{P_{\\mathrm{GRU}}(n_{x}, n_{h})}\n\\;=\\;\n\\frac{4 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right)}{3 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right)}\n\\;=\\;\n\\frac{4}{3},\n$$\n在所述假设下，该比率与维度无关。\n\n对有限神经科学数据集的样本效率的影响：$P_{\\mathrm{LSTM}}$ 和 $P_{\\mathrm{GRU}}$ 中的主导项都是 $n_{h}^{2}$，反映了由于循环连接导致的参数二次方增长。相对于 GRU，LSTM 的参数数量有一个 $\\frac{4}{3}$ 的常数因子增长，因为它在每个时间步实例化了四次仿射计算，而不是三次。在训练试验或序列数量相对于参数数量有限的情况下，较大的参数数量通常会增加过拟合的风险，并需要更强的正则化或更多的数据才能实现相当的泛化能力。相反，GRU 较小的参数化可能具有更高的样本效率，在试验次数较少的情况下可能产生更好的泛化能力，前提是其表示能力足以处理神经数据中的时间依赖性。确切的权衡取决于任务复杂性、正则化强度、先验约束和架构细节，但推导出的参数数量提供了一种在这些循环层类型之间进行选择时，预测和比较样本效率约束的原则性方法。",
            "answer": "$$\\boxed{\\begin{pmatrix}394240 & 295680 & \\frac{4}{3}\\end{pmatrix}}$$"
        },
        {
            "introduction": "为什么 LSTM 和 GRU 在捕捉长程依赖方面如此有效，而简单的 RNN 却会失败？本练习通过分析经典的“加法问题”来探讨其核心原因。您将使用一个简化模型来量化不同架构中梯度信号随时间衰减的情况，从而清晰地、从分析的角度洞察门控机制的力量。",
            "id": "3191191",
            "problem": "要求您形式化并分析序列学习中著名的“加法问题”，重点关注深度学习中的长期依赖问题。加法问题的定义如下：给定一个长度为 $L$、数值在 $[0,1]$ 区间内的实数序列，其中恰好有两个位置被标记，目标是这两个标记位置上数值的总和。模型逐个时间步处理该序列，并在最后一个时间步输出一个标量。为了分离出长期依赖的影响，考虑最坏情况，即第一个标记位置在 $t=1$，而最终输出在 $t=L$ 产生。因此，针对最早相关时间步的学习信号必须穿过 $L-1$ 个循环转换。\n\n使用以下三种架构进行研究：\n- 循环神经网络 (Recurrent Neural Network, RNN)，其循环更新使用平滑非线性函数\n- 门控循环单元 (Gated Recurrent Unit, GRU)\n- 长短期记忆网络 (Long Short-Term Memory, LSTM)\n\n假设以下科学上标准且广为接受的基础：\n- 用于复合函数的微分链式法则适用于随时间反向传播，因此较早时间步的梯度是跨时间步的雅可比矩阵的乘积。\n- 为保证稳定性，反向传播梯度的幅值由沿跨时间步承载记忆路径的循环雅可比因子的算子范数所决定。\n- 循环神经网络 (RNN) 在原点附近的循环雅可比矩阵由其循环权重矩阵主导，其长期行为由该矩阵的谱半径控制。\n- 门控循环单元 (GRU) 和长短期记忆网络 (LSTM) 架构中的门控机制将前一状态与门控值相乘，从而直接缩放流经主记忆路径的梯度。\n\n为使比较明确且计算上可行，分析中采用以下一致的简化：\n- 将循环神经网络 (RNN) 的循环雅可比矩阵近似视为时不变的，其谱半径为 $\\rho$。\n- 将长短期记忆网络 (LSTM) 的遗忘门和门控循环单元 (GRU) 的更新门视为跨时间步的恒定标量：LSTM 的遗忘门为 $f$，GRU 的更新门为 $z$。\n- 使用归一化的输出和损失，以便将循环路径之外的恒定乘法因子吸收到一个单一的正标量 $\\alpha$ 中。\n\n所有计算中使用的参数值：\n- 循环神经网络 (RNN) 的 $\\rho = 0.90$\n- 长短期记忆网络 (LSTM) 的 $f = 0.99$\n- 门控循环单元 (GRU) 的 $z = 0.05$\n- $\\alpha = 1.0$\n\n将最早标记输入的“训练信号幅值”定义为：损失函数关于最早标记输入的梯度的幅值，该梯度沿主记忆路径从时间 $t=L$ 反向传播至 $t=1$。使用上述基础（链式法则和雅可比矩阵乘积），为每种架构推导出训练信号幅值作为 $L$ 和给定参数的函数表达式。然后在程序中实现这些表达式。\n\n测试套件：\n- 序列长度 $L \\in \\{50, 75, 100, 500, 1000\\}$。\n- 对于每个 $L$，按以下顺序计算各架构的训练信号幅值：循环神经网络 (RNN)、门控循环单元 (GRU)、长短期记忆网络 (LSTM)。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来、无空格的逗号分隔列表。该列表是按 $L$ 的升序对整个测试套件的结果进行展平得到的。也就是说，输出必须是：\n$[$RNN$(50)$,GRU$(50)$,LSTM$(50)$,RNN$(75)$,GRU$(75)$,LSTM$(75)$,RNN$(100)$,GRU$(100)$,LSTM$(100)$,RNN$(500)$,GRU$(500)$,LSTM$(500)$,RNN$(1000)$,GRU$(1000)$,LSTM$(1000)]$，\n其中每个条目都是一个代表训练信号幅值（无单位）的浮点数。最终输出为浮点数；不涉及物理单位或角度，且不得打印百分比。",
            "solution": "分析训练信号幅值的核心原理是随时间反向传播 (BPTT)。损失函数 $\\mathcal{L}$ 关于某个时间步 $t$ 的隐藏状态 $h_t$ 的梯度是通过链式法则，从更晚的时间步 $t+1$ 传播梯度来计算的：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_t} = \\frac{\\partial \\mathcal{L}}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_t}\n$$\n为了从在时间 $t=L$ 计算的损失中找到关于时间 $t=1$ 状态的梯度，我们必须递归地应用此规则 $L-1$ 步：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_1} = \\frac{\\partial \\mathcal{L}}{\\partial h_L} \\left( \\prod_{t=2}^{L} \\frac{\\partial h_t}{\\partial h_{t-1}} \\right)\n$$\n项 $\\frac{\\partial h_t}{\\partial h_{t-1}}$ 是时间 $t$ 的循环雅可比矩阵。对于 $t=1$ 处输入的“训练信号幅值”，主要由这个雅可比矩阵乘积的幅值决定，它确定了从 $t=L$ 的输出传回 $t=1$ 时，误差信号被放大或减小的程度。问题定义了一个常数 $\\alpha$ 来吸收所有非循环因子，例如 $\\frac{\\partial \\mathcal{L}}{\\partial h_L}$ 和最后一步的 $\\frac{\\partial h_1}{\\partial x_1}$。因此，信号幅值 $S(L)$ 与雅可比矩阵乘积的范数成正比。让我们针对每种架构分析这一点。\n\n**循环神经网络 (RNN)**\nRNN 的隐藏状态更新形式为 $h_t = \\phi(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$，其中 $\\phi$ 是一个非线性激活函数，如 $\\tanh$。循环雅可比矩阵为 $\\frac{\\partial h_t}{\\partial h_{t-1}} = \\text{diag}(\\phi'(...))W_{hh}$。这些矩阵乘积的长期行为由循环权重矩阵 $W_{hh}$ 的谱半径 $\\rho$ 决定。该问题通过假设每个雅可比步骤的有效幅值贡献是一个常数因子 $\\rho$ 来简化分析。将信号跨越 $L-1$ 个时间步传播，导致这个因子被乘以 $L-1$ 次。\n因此，对于长度为 $L$ 的序列，其训练信号幅值 $S_{RNN}$ 为：\n$$\nS_{RNN}(L) = \\alpha \\rho^{L-1}\n$$\n给定 $\\alpha = 1.0$ 和 $\\rho = 0.90$，我们有：\n$$\nS_{RNN}(L) = (0.90)^{L-1}\n$$\n\n**长短期记忆网络 (LSTM)**\nLSTM 处理长依赖能力的关键在于其单元状态 $c_t$，它通过一个门控机制进行更新：$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$。这里，$f_t$ 是遗忘门，$\\odot$ 表示逐元素乘法。从 $c_t$ 到 $c_{t-1}$ 穿过单元状态的梯度路径主要由遗忘门进行缩放，即 $\\frac{\\partial c_t}{\\partial c_{t-1}} \\approx \\text{diag}(f_t)$。该问题通过假设所有时间步的遗忘门都是一个恒定标量 $f$ 来简化这一点。沿主记忆路径反向流动的梯度信号在 $L-1$ 个步骤中的每一步都被 $f$ 缩放。\n训练信号幅值 $S_{LSTM}$ 为：\n$$\nS_{LSTM}(L) = \\alpha f^{L-1}\n$$\n给定 $\\alpha = 1.0$ 和 $f = 0.99$，我们有：\n$$\nS_{LSTM}(L) = (0.99)^{L-1}\n$$\n\n**门控循环单元 (GRU)**\nGRU 的状态更新为 $h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$，其中 $z_t$ 是更新门。项 $(1-z_t)$ 充当一个动态遗忘门，控制前一状态 $h_{t-1}$ 有多少被传递到当前状态 $h_t$。因此，$h_t$ 相对于 $h_{t-1}$ 的梯度直接被这个因子缩放。该问题通过假设一个恒定的标量更新门 $z$ 来简化分析。因此，在 $L-1$ 个反向传播步骤中的每一步，缩放因子都是 $(1-z)$。\n训练信号幅值 $S_{GRU}$ 为：\n$$\nS_{GRU}(L) = \\alpha (1-z)^{L-1}\n$$\n给定 $\\alpha = 1.0$ 和 $z = 0.05$，缩放因子为 $(1-0.05) = 0.95$。我们有：\n$$\nS_{GRU}(L) = (0.95)^{L-1}\n$$\n\n现在将实现这些推导出的表达式，以计算指定测试套件所需的值。\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the training-signal magnitude for RNN, GRU, and LSTM architectures\n    for the \"adding problem\" over various sequence lengths.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    rho_rnn = 0.90  # Spectral radius for RNN\n    f_lstm = 0.99   # Forget gate value for LSTM\n    z_gru = 0.05    # Update gate value for GRU\n    alpha = 1.0     # Constant multiplicative factor (can be omitted as it's 1.0)\n    \n    # Define the test cases for sequence length L from the problem statement.\n    test_cases_L = [50, 75, 100, 500, 1000]\n\n    results = []\n    for L in test_cases_L:\n        # The number of recurrent transitions is L-1.\n        exponent = L - 1\n\n        # Calculate the training-signal magnitude for each architecture.\n        # S(L) = alpha * (base)^(L-1)\n        \n        # RNN\n        s_rnn = alpha * (rho_rnn ** exponent)\n        \n        # GRU\n        # The scaling factor is (1 - z)\n        s_gru = alpha * ((1 - z_gru) ** exponent)\n        \n        # LSTM\n        s_lstm = alpha * (f_lstm ** exponent)\n        \n        # Append results in the specified order: RNN, GRU, LSTM\n        results.append(s_rnn)\n        results.append(s_gru)\n        results.append(s_lstm)\n\n    # Format the final output as a comma-separated list of floats in brackets.\n    # e.g., [val1,val2,val3,...]\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\n# The following call is for generating the answer. It is commented out\n# to adhere to the rule of not having executable code at the top level.\n# solve()\n```",
            "answer": "[0.005790048698188269,0.07953932115160846,0.6111100049444391,0.000473266542259103,0.02116035989260717,0.4754541573030389,3.869032483863417e-05,0.005920530009662768,0.3697116812836262,1.547167664151746e-23,1.666993310344583e-11,0.006691452445829986,2.4045558917897217e-46,2.784451006540679e-22,4.317124741065786e-05]"
        },
        {
            "introduction": "在理解了 LSTM 的结构和功能优势之后，我们现在深入探讨它们学习的机制。这个高级练习要求您使用时间反向传播算法（BPTT）推导 LSTM 遗忘门的梯度更新规则。完成此练习将使您对门控循环网络内部的学习过程有一个深刻的、机械的理解。",
            "id": "4175988",
            "problem": "考虑一个用于为神经科学多变量时间序列的突触输入特征建模的长短期记忆（LSTM）循环神经网络。设时间 $t$ 的输入为 $x_t \\in \\mathbb{R}^{n}$，隐藏状态为 $h_t \\in \\mathbb{R}^{m}$，细胞状态为 $c_t \\in \\mathbb{R}^{m}$。LSTM 的动态由以下门控方程定义，对于 $t = 1, 2, \\dots, T$：\n$$\na^{f}_t = W_f x_t + U_f h_{t-1} + b_f,\\quad f_t = \\sigma(a^{f}_t),\n$$\n$$\na^{i}_t = W_i x_t + U_i h_{t-1} + b_i,\\quad i_t = \\sigma(a^{i}_t),\n$$\n$$\na^{o}_t = W_o x_t + U_o h_{t-1} + b_o,\\quad o_t = \\sigma(a^{o}_t),\n$$\n$$\na^{g}_t = W_g x_t + U_g h_{t-1} + b_g,\\quad g_t = \\tanh(a^{g}_t),\n$$\n$$\nc_t = f_t \\odot c_{t-1} + i_t \\odot g_t,\\quad h_t = o_t \\odot \\tanh(c_t),\n$$\n其中 $\\sigma(z)$ 是按元素应用的 logistic sigmoid 函数，$\\tanh(z)$ 是按元素应用的双曲正切函数，$\\odot$ 表示按元素（哈达玛）积。假设 $h_0$ 和 $c_0$ 已给定。预测观测到的神经响应 $r_t \\in \\mathbb{R}^{p}$ 的读出由下式给出\n$$\ny_t = W_y h_t + b_y,\n$$\n序列上的总损失是平方误差和\n$$\nL = \\sum_{t=1}^{T} \\frac{1}{2} \\|y_t - r_t\\|_2^2.\n$$\n仅使用基本定义（微积分的链式法则、上述 LSTM 前向方程以及标准元素级导数 $\\sigma'(z) = \\sigma(z)\\big(1 - \\sigma(z)\\big)$ 和 $\\tanh'(z) = 1 - \\tanh^2(z)$），推导随时间反向传播（BPTT）关于遗忘门参数 $W_f \\in \\mathbb{R}^{m \\times n}$，$U_f \\in \\mathbb{R}^{m \\times m}$ 和 $b_f \\in \\mathbb{R}^{m}$ 的梯度的闭式解析表达式。您的最终表达式必须写成关于时间的显式求和形式，其中包含适当的误差信号与输入或隐藏状态之间的外积，并展开到遗忘门预激活层级。将您的最终答案表示为单个符号解析表达式，使用 $\\mathrm{pmatrix}$ 环境将三个梯度收集在一行中，并且不包含任何单位。不需要进行数值评估或四舍五入。",
            "solution": "目标是推导总损失 $L$ 关于遗忘门参数 $W_f$、$U_f$ 和 $b_f$ 的梯度的解析表达式。总损失由 $L = \\sum_{t=1}^{T} L_t$ 给出，其中 $L_t = \\frac{1}{2} \\|y_t - r_t\\|_2^2$。由于参数在所有时间步上共享，因此关于一个参数的总梯度是其在每个时间步上贡献的总和。\n\n遗忘门参数 $W_f$、$U_f$ 和 $b_f$ 在每个时间步 $t$ 通过遗忘门预激活向量 $a^f_t$ 直接影响损失 $L$。使用链式法则，我们可以将梯度表示如下：\n$$\n\\frac{\\partial L}{\\partial W_f} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial a^f_t} \\frac{\\partial a^f_t}{\\partial W_f}\n$$\n$$\n\\frac{\\partial L}{\\partial U_f} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial a^f_t} \\frac{\\partial a^f_t}{\\partial U_f}\n$$\n$$\n\\frac{\\partial L}{\\partial b_f} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial a^f_t} \\frac{\\partial a^f_t}{\\partial b_f} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial a^f_t}\n$$\n我们定义在遗忘门预激活处的误差信号为 $\\delta_t^{a^f} = \\frac{\\partial L}{\\partial a^f_t} \\in \\mathbb{R}^m$。\n\n根据定义 $a^f_t = W_f x_t + U_f h_{t-1} + b_f$，关于参数的偏导数是 $\\frac{\\partial a^f_t}{\\partial W_f} = (x_t)^T$（在适当的雅可比意义上，导致外积）、$\\frac{\\partial a^f_t}{\\partial U_f} = (h_{t-1})^T$，而 $\\frac{\\partial a^f_t}{\\partial b_f}$ 会导致一个求和。因此，梯度可以写成：\n$$\n\\frac{\\partial L}{\\partial W_f} = \\sum_{t=1}^{T} \\delta_t^{a^f} (x_t)^T\n$$\n$$\n\\frac{\\partial L}{\\partial U_f} = \\sum_{t=1}^{T} \\delta_t^{a^f} (h_{t-1})^T\n$$\n$$\n\\frac{\\partial L}{\\partial b_f} = \\sum_{t=1}^{T} \\delta_t^{a^f}\n$$\n问题的核心是找到误差信号 $\\delta_t^{a^f}$ 的表达式。我们应用链式法则，将误差从损失函数向后传播。\n\n首先，我们将 $\\delta_t^{a^f}$ 表示为细胞状态处的误差信号 $\\delta_t^c = \\frac{\\partial L}{\\partial c_t} \\in \\mathbb{R}^m$ 的形式。\n路径是 $L \\rightarrow c_t \\rightarrow f_t \\rightarrow a^f_t$。\n$$\n\\delta_t^{a^f} = \\frac{\\partial L}{\\partial a^f_t} = \\left(\\frac{\\partial c_t}{\\partial a^f_t}\\right)^T \\frac{\\partial L}{\\partial c_t} = \\left(\\frac{\\partial c_t}{\\partial a^f_t}\\right)^T \\delta_t^c\n$$\n由于所有运算都是按元素的，雅可比矩阵是对角矩阵，这简化为按元素的乘积：\n$$\n\\delta_t^{a^f} = \\frac{\\partial c_t}{\\partial a^f_t} \\odot \\delta_t^c\n$$\n从 $c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$ 和 $f_t = \\sigma(a^f_t)$，我们求得导数：\n$$\n\\frac{\\partial c_t}{\\partial a^f_t} = \\frac{\\partial f_t}{\\partial a^f_t} \\odot c_{t-1} = \\sigma'(a^f_t) \\odot c_{t-1}\n$$\n使用给定的导数 $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$，我们有 $\\sigma'(a^f_t) = f_t \\odot (1-f_t)$。\n因此，$\\delta_t^{a^f}$ 的表达式是：\n$$\n\\delta_t^{a^f} = \\delta_t^c \\odot c_{t-1} \\odot f_t \\odot (1 - f_t)\n$$\n接下来，我们必须定义 $\\delta_t^c$。细胞状态 $c_t$ 通过两条路径影响损失 $L$：通过当前隐藏状态 $h_t$ 和通过下一个细胞状态 $c_{t+1}$。\n$$\n\\delta_t^c = \\frac{\\partial L}{\\partial c_t} = \\left(\\frac{\\partial h_t}{\\partial c_t}\\right)^T \\frac{\\partial L}{\\partial h_t} + \\left(\\frac{\\partial c_{t+1}}{\\partial c_t}\\right)^T \\frac{\\partial L}{\\partial c_{t+1}}\n$$\n设 $\\delta_t^h = \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^m$。该表达式成为 $\\delta_t^c$ 的一个递推关系：\n$$\n\\delta_t^c = \\left(\\frac{\\partial h_t}{\\partial c_t} \\odot \\delta_t^h\\right) + \\left(\\frac{\\partial c_{t+1}}{\\partial c_t} \\odot \\delta_{t+1}^c\\right)\n$$\n从 $h_t = o_t \\odot \\tanh(c_t)$，我们得到 $\\frac{\\partial h_t}{\\partial c_t} = o_t \\odot \\tanh'(c_t) = o_t \\odot (1 - \\tanh^2(c_t))$。\n从 $c_{t+1} = f_{t+1} \\odot c_t + i_{t+1} \\odot g_{t+1}$，我们得到 $\\frac{\\partial c_{t+1}}{\\partial c_t} = f_{t+1}$。\n因此，$\\delta_t^c$ 的递推关系为：\n$$\n\\delta_t^c = \\delta_t^h \\odot o_t \\odot (1 - \\tanh^2(c_t)) + \\delta_{t+1}^c \\odot f_{t+1}\n$$\n边界条件为 $\\delta_{T+1}^c = \\vec{0}$，因为不存在时间步 $T+1$。\n\n最后，我们需要定义 $\\delta_t^h$。隐藏状态 $h_t$ 通过当前输出 $y_t$ 以及下一个时间步的所有门预激活 $a^f_{t+1}, a^i_{t+1}, a^o_{t+1}, a^g_{t+1}$ 来影响损失。\n$$\n\\delta_t^h = \\frac{\\partial L}{\\partial h_t} = \\left(\\frac{\\partial y_t}{\\partial h_t}\\right)^T \\frac{\\partial L}{\\partial y_t} + \\left(\\frac{\\partial a^f_{t+1}}{\\partial h_t}\\right)^T \\frac{\\partial L}{\\partial a^f_{t+1}} + \\left(\\frac{\\partial a^i_{t+1}}{\\partial h_t}\\right)^T \\frac{\\partial L}{\\partial a^i_{t+1}} + \\left(\\frac{\\partial a^o_{t+1}}{\\partial h_t}\\right)^T \\frac{\\partial L}{\\partial a^o_{t+1}} + \\left(\\frac{\\partial a^g_{t+1}}{\\partial h_t}\\right)^T \\frac{\\partial L}{\\partial a^g_{t+1}}\n$$\n从给定的方程可知：$\\frac{\\partial y_t}{\\partial h_t} = W_y$，$\\frac{\\partial a^f_{t+1}}{\\partial h_t} = U_f$，$\\frac{\\partial a^i_{t+1}}{\\partial h_t} = U_i$，$\\frac{\\partial a^o_{t+1}}{\\partial h_t} = U_o$，$\\frac{\\partial a^g_{t+1}}{\\partial h_t} = U_g$。输出误差是 $\\frac{\\partial L}{\\partial y_t} = \\frac{\\partial L_t}{\\partial y_t} = y_t - r_t$。\n$\\delta_t^h$ 的递推关系是：\n$$\n\\delta_t^h = W_y^T (y_t - r_t) + U_f^T \\delta_{t+1}^{a^f} + U_i^T \\delta_{t+1}^{a^i} + U_o^T \\delta_{t+1}^{a^o} + U_g^T \\delta_{t+1}^{a^g}\n$$\n所有门的边界条件为 $\\delta_{T+1}^{a^\\cdot} = \\vec{0}$，这意味着递推从 $\\delta_T^h = W_y^T (y_T - r_T)$ 开始。时间步 $t+1$ 所需的门误差信号本身也以类似于 $\\delta_t^{a^f}$ 的方式推导得出：\n$$\n\\delta_{t+1}^{a^f} = \\delta_{t+1}^c \\odot c_{t} \\odot f_{t+1} \\odot (1 - f_{t+1})\n$$\n$$\n\\delta_{t+1}^{a^i} = \\delta_{t+1}^c \\odot g_{t+1} \\odot i_{t+1} \\odot (1 - i_{t+1})\n$$\n$$\n\\delta_{t+1}^{a^o} = \\delta_{t+1}^h \\odot \\tanh(c_{t+1}) \\odot o_{t+1} \\odot (1 - o_{t+1})\n$$\n$$\n\\delta_{t+1}^{a^g} = \\delta_{t+1}^c \\odot i_{t+1} \\odot (1 - g_{t+1}^2)\n$$\n这些递推关系从时间 $t=T$ 反向计算到 $t=1$，定义了计算梯度所需的量。因此，梯度的最终解析表达式由涉及信号 $\\delta_t^{a^f}$ 的时间总和给出，而 $\\delta_t^{a^f}$ 由上述 BPTT 方程组确定。\n梯度的最终表达式是：\n$\n\\frac{\\partial L}{\\partial W_f} = \\sum_{t=1}^{T} \\delta_t^{a^f} (x_t)^T\n$,\n$\n\\frac{\\partial L}{\\partial U_f} = \\sum_{t=1}^{T} \\delta_t^{a^f} (h_{t-1})^T\n$, 以及\n$\n\\frac{\\partial L}{\\partial b_f} = \\sum_{t=1}^{T} \\delta_t^{a^f}\n$.\n误差项 $\\delta_t^{a^f}$ 定义为 $\\delta_t^{a^f} = \\delta_t^c \\odot c_{t-1} \\odot f_t \\odot (1 - f_t)$，其中 $\\delta_t^c$ 由完整的 BPTT 递推关系定义。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sum_{t=1}^{T} \\delta_t^{a^f} (x_t)^T  & \\sum_{t=1}^{T} \\delta_t^{a^f} (h_{t-1})^T  & \\sum_{t=1}^{T} \\delta_t^{a^f}\n\\end{pmatrix}\n\\text{，其中 } \\delta_t^{a^f} = \\left( \\delta_t^h \\odot o_t \\odot (1 - \\tanh^2(c_t)) + \\delta_{t+1}^c \\odot f_{t+1} \\right) \\odot c_{t-1} \\odot f_t \\odot (1-f_t)\n}\n$$"
        }
    ]
}