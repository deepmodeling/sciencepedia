{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we first address the core motivation for developing gated recurrent architectures: the vanishing gradient problem. This exercise uses a classic thought experiment, the \"adding problem,\" to provide a clear and quantitative illustration of how gradient signals decay over time in a simple Recurrent Neural Network (RNN) compared to how they are preserved in LSTMs and GRUs. By analyzing a simplified model under clear assumptions, you will develop a foundational intuition for why gating mechanisms are essential for capturing long-term dependencies in sequential data .",
            "id": "3191191",
            "problem": "You are asked to formalize and analyze the well-known \"adding problem\" in sequence learning, focusing on the issue of long-term dependencies in deep learning. The adding problem is defined as follows: given a sequence of real numbers in the interval $[0,1]$ of length $L$, exactly two positions are marked, and the target is the sum of the values at the marked positions. A model processes the sequence step by step and outputs a single scalar at the final time step. To isolate the effect of long-time dependencies, consider the worst case in which the first marked position is at $t=1$ and the final output is produced at $t=L$, so the learning signal for the earliest relevant time step must traverse $L-1$ recurrent transitions.\n\nWork with three architectures:\n- Recurrent Neural Network (RNN) with recurrent update using a smooth nonlinearity,\n- Gated Recurrent Unit (GRU),\n- Long Short-Term Memory (LSTM).\n\nAssume the following scientifically standard and widely accepted foundations:\n- The chain rule of differentiation for compositions of functions applies to backpropagation through time, so that gradients at earlier time steps are products of Jacobians across time steps.\n- For stability, the magnitude of backpropagated gradients is governed by the operator norm of the recurrent Jacobian factors along the path that carries memory across time steps.\n- The recurrent Jacobian for a Recurrent Neural Network (RNN) near the origin is dominated by the recurrent weight matrix, and its long-time behavior is controlled by its spectral radius.\n- The gating mechanisms in Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) architectures multiply the previous state by gate values, thereby directly scaling the gradient that flows through the main memory path.\n\nTo make the comparison explicit and computationally tractable, adopt the following consistent simplifications for the analysis:\n- Treat the recurrent Jacobian of the Recurrent Neural Network (RNN) as approximately time-invariant with spectral radius $\\rho$.\n- Treat the forget gate of the Long Short-Term Memory (LSTM) and the update gate of the Gated Recurrent Unit (GRU) as constant scalars across time: the LSTM forget gate is $f$, and the GRU update gate is $z$.\n- Use a normalized output and loss so that constant multiplicative factors outside the recurrent pathway can be absorbed into a single positive scalar $\\alpha$.\n\nParameter values to use in all computations:\n- $\\rho = 0.90$ for the Recurrent Neural Network (RNN),\n- $f = 0.99$ for the Long Short-Term Memory (LSTM),\n- $z = 0.05$ for the Gated Recurrent Unit (GRU),\n- $\\alpha = 1.0$.\n\nDefine the \"training-signal magnitude\" for the earliest marked input as the magnitude of the gradient of the loss with respect to the earliest marked input, propagated along the principal memory path from time $t=L$ back to $t=1$. Using the foundations stated above (chain rule and product of Jacobians), derive expressions for the training-signal magnitude for each architecture as functions of $L$ and the given parameters. Then implement these expressions in a program.\n\nTest suite:\n- Sequence lengths $L \\in \\{50, 75, 100, 500, 1000\\}$.\n- For each $L$, compute the training-signal magnitude for each architecture in the order: Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where the list is flattened across the test suite in ascending order of $L$. That is, the output must be:\n$[$RNN$(50)$,GRU$(50)$,LSTM$(50)$,RNN$(75)$,GRU$(75)$,LSTM$(75)$,RNN$(100)$,GRU$(100)$,LSTM$(100)$,RNN$(500)$,GRU$(500)$,LSTM$(500)$,RNN$(1000)$,GRU$(1000)$,LSTM$(1000)]$,\nwhere each entry is a float representing the training-signal magnitude (unitless). The final outputs are floats; no physical units or angles are involved, and no percentages must be printed.",
            "solution": "The user requests a formal analysis of the \"adding problem\" to compare the ability of three recurrent architectures—Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), and Long Short-Term Memory (LSTM)—to handle long-term dependencies. The analysis focuses on the magnitude of the gradient signal propagated from the final time step back to the first relevant input.\n\nThe problem is validated as follows:\n- **Step 1: Extract Givens**\n  - Problem: The \"adding problem\" for a sequence of length $L$.\n  - Goal: Compute the \"training-signal magnitude\" for an input at time $t=1$ when the output is at $t=L$.\n  - Architectures: RNN, GRU, LSTM.\n  - Foundations:\n    - Backpropagation through time is based on the chain rule.\n    - Gradient magnitude is governed by the operator norm of recurrent Jacobians.\n    - RNN Jacobian behavior is controlled by the spectral radius of the recurrent weight matrix.\n    - Gating mechanisms in GRU and LSTM scale the gradient flow.\n  - Simplifications for analysis:\n    - RNN recurrent Jacobian is treated as time-invariant with spectral radius $\\rho$.\n    - LSTM forget gate is a constant scalar $f$.\n    - GRU update gate is a constant scalar $z$.\n    - A single positive scalar $\\alpha$ absorbs all constant multiplicative factors outside the recurrent pathway.\n  - Parameter values:\n    - $\\rho = 0.90$\n    - $f = 0.99$\n    - $z = 0.05$\n    - $\\alpha = 1.0$\n  - Test Suite:\n    - Sequence lengths $L \\in \\{50, 75, 100, 500, 1000\\}$.\n    - Computation order: RNN, GRU, LSTM for each $L$.\n\n- **Step 2: Validate Using Extracted Givens**\n  - **Scientifically Grounded:** The problem is firmly rooted in the established principles of deep learning, specifically the analysis of gradient flow in recurrent networks (the vanishing/exploding gradient problem). The simplifications are standard for creating a tractable analytical model of a complex system.\n  - **Well-Posed:** The problem is clearly defined with all necessary parameters and assumptions, leading to a unique, computable solution for each case.\n  - **Objective:** The language is precise and technical, free of subjectivity.\n\n- **Step 3: Verdict and Action**\n  - The problem is deemed **valid**. The simplifications are explicitly stated and serve to isolate the core mechanism of gradient propagation, which is a standard and informative analytical technique. Proceeding with the solution.\n\nThe core principle for analyzing the training-signal magnitude is backpropagation through time (BPTT). The gradient of the loss function $\\mathcal{L}$ with respect to the hidden state $h_t$ at some time step $t$ is computed via the chain rule by propagating the gradient from a later time step $t+1$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_t} = \\frac{\\partial \\mathcal{L}}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_t}\n$$\nTo find the gradient with respect to the state at time $t=1$ from the loss computed at time $t=L$, we must apply this rule recursively for $L-1$ steps:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_1} = \\frac{\\partial \\mathcal{L}}{\\partial h_L} \\left( \\prod_{t=2}^{L} \\frac{\\partial h_t}{\\partial h_{t-1}} \\right)\n$$\nThe term $\\frac{\\partial h_t}{\\partial h_{t-1}}$ is the recurrent Jacobian matrix at time $t$. The \"training-signal magnitude\" for the input at $t=1$ is dominated by the magnitude of this product of Jacobians, which determines how much the error signal from the output at $t=L$ is amplified or diminished as it travels back to $t=1$. The problem defines a constant $\\alpha$ to absorb all non-recurrent factors, such as $\\frac{\\partial \\mathcal{L}}{\\partial h_L}$ and the final step $\\frac{\\partial h_1}{\\partial x_1}$. Thus, the signal magnitude $S(L)$ is proportional to the norm of the product of Jacobians. Let's analyze this for each architecture.\n\n**Recurrent Neural Network (RNN)**\nThe RNN hidden state update is of the form $h_t = \\phi(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$, where $\\phi$ is a nonlinear activation function like $\\tanh$. The recurrent Jacobian is $\\frac{\\partial h_t}{\\partial h_{t-1}} = \\text{diag}(\\phi'(...))W_{hh}$. The long-term behavior of the product of these matrices is governed by the spectral radius $\\rho$ of the recurrent weight matrix $W_{hh}$. The problem simplifies the analysis by assuming the effective magnitude contribution of each Jacobian step is a constant factor $\\rho$. Propagating the signal across $L-1$ time steps results in this factor being multiplied $L-1$ times.\nThe training-signal magnitude $S_{RNN}$ for a sequence of length $L$ is therefore:\n$$\nS_{RNN}(L) = \\alpha \\rho^{L-1}\n$$\nGiven $\\alpha = 1.0$ and $\\rho = 0.90$, we have:\n$$\nS_{RNN}(L) = (0.90)^{L-1}\n$$\n\n**Long Short-Term Memory (LSTM)**\nThe key to the LSTM's ability to handle long dependencies is its cell state, $c_t$, which is updated via a gating mechanism: $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$. Here, $f_t$ is the forget gate and $\\odot$ denotes element-wise multiplication. The gradient pathway through the cell state from $c_t$ to $c_{t-1}$ is primarily scaled by the forget gate, i.e., $\\frac{\\partial c_t}{\\partial c_{t-1}} \\approx \\text{diag}(f_t)$. The problem simplifies this by assuming the forget gate is a constant scalar $f$ for all time steps. The gradient signal flowing back through the main memory path is scaled by $f$ at each of the $L-1$ steps.\nThe training-signal magnitude $S_{LSTM}$ is:\n$$\nS_{LSTM}(L) = \\alpha f^{L-1}\n$$\nGiven $\\alpha = 1.0$ and $f = 0.99$, we have:\n$$\nS_{LSTM}(L) = (0.99)^{L-1}\n$$\n\n**Gated Recurrent Unit (GRU)**\nThe GRU state update is $h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$, where $z_t$ is the update gate. The term $(1-z_t)$ acts as a dynamic forget gate, controlling how much of the previous state $h_{t-1}$ is passed to the current state $h_t$. The gradient of $h_t$ with respect to $h_{t-1}$ is therefore directly scaled by this factor. The problem simplifies the analysis by assuming a constant scalar update gate $z$. Consequently, the scaling factor at each of the $L-1$ backpropagation steps is $(1-z)$.\nThe training-signal magnitude $S_{GRU}$ is:\n$$\nS_{GRU}(L) = \\alpha (1-z)^{L-1}\n$$\nGiven $\\alpha = 1.0$ and $z = 0.05$, the scaling factor is $(1-0.05) = 0.95$. We have:\n$$\nS_{GRU}(L) = (0.95)^{L-1}\n$$\n\nThese derived expressions will now be implemented to compute the required values for the specified test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the training-signal magnitude for RNN, GRU, and LSTM architectures\n    for the \"adding problem\" over various sequence lengths.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    rho_rnn = 0.90  # Spectral radius for RNN\n    f_lstm = 0.99   # Forget gate value for LSTM\n    z_gru = 0.05    # Update gate value for GRU\n    alpha = 1.0     # Constant multiplicative factor (can be omitted as it's 1.0)\n    \n    # Define the test cases for sequence length L from the problem statement.\n    test_cases_L = [50, 75, 100, 500, 1000]\n\n    results = []\n    for L in test_cases_L:\n        # The number of recurrent transitions is L-1.\n        exponent = L - 1\n\n        # Calculate the training-signal magnitude for each architecture.\n        # S(L) = alpha * (base)^(L-1)\n        \n        # RNN\n        s_rnn = alpha * (rho_rnn ** exponent)\n        \n        # GRU\n        # The scaling factor is (1 - z)\n        s_gru = alpha * ((1 - z_gru) ** exponent)\n        \n        # LSTM\n        s_lstm = alpha * (f_lstm ** exponent)\n        \n        # Append results in the specified order: RNN, GRU, LSTM\n        results.append(s_rnn)\n        results.append(s_gru)\n        results.append(s_lstm)\n\n    # Format the final output as a comma-separated list of floats in brackets.\n    # e.g., [val1,val2,val3,...]\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Having established why gating is necessary, we now examine the architectural consequences of implementing these solutions. The sophisticated gating mechanisms that empower LSTMs and GRUs introduce additional parameters, affecting model complexity and computational cost. This practice challenges you to quantify these differences by deriving and comparing the total number of trainable parameters in standard LSTM and GRU layers, which reveals that a GRU is roughly 25% more parameter-efficient. Understanding this trade-off between representational capacity and model parsimony is a critical skill for selecting the appropriate architecture in practical applications, especially when working with limited neuroscience datasets .",
            "id": "4175962",
            "problem": "A neuroscience laboratory is designing recurrent neural network layers to decode population spiking activity into kinematic variables. At each time bin, the input feature vector has dimension $n_{x}$ (e.g., binned spike counts across channels and exogenous covariates), and the recurrent hidden state has dimension $n_{h}$. Consider two candidate architectures for a single recurrent layer: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). Long Short-Term Memory (LSTM) layers use four distinct gating computations per time step (input gate, forget gate, output gate, and a cell candidate transformation), while Gated Recurrent Unit (GRU) layers use two gating computations (update and reset) and a hidden candidate transformation. In standard formulations without peephole connections and without any output projection, each gate or candidate transformation computes an affine map of the current input and the previous hidden state followed by a pointwise nonlinearity, and each such affine map has its own learned bias.\n\nStarting solely from these definitions and the fact that an affine map from an input of dimension $n$ to an output of dimension $m$ is parameterized by an $m \\times n$ weight matrix together with an $m$-dimensional bias vector, derive the exact total number of scalar trainable parameters (including biases) in an LSTM layer and in a GRU layer, each with input dimension $n_{x}$ and hidden dimension $n_{h}$. Then, for the realistic setting $n_{x} = 128$ and $n_{h} = 256$, compute the numerical parameter counts for both layers and the ratio of the LSTM parameter count to the GRU parameter count. Report your final answer as a row matrix containing, in order, the LSTM parameter count, the GRU parameter count, and their ratio. Provide exact counts; no rounding is required. Finally, explain, based on your derivation, the implications of these parameter counts for sample efficiency when training on limited neuroscience datasets with relatively few trials compared to the number of parameters.",
            "solution": "We begin from the definition that each gate or candidate transformation in a recurrent neural network layer computes an affine map from the concatenated inputs comprising the current external input and the previous hidden state to the gate or candidate output, followed by a nonlinearity. An affine map from an input of dimension $n$ to an output of dimension $m$ is parameterized by an $m \\times n$ weight matrix together with an $m$-dimensional bias vector, yielding $m \\cdot n + m$ scalar parameters.\n\nIn the standard Long Short-Term Memory (LSTM) layer without peephole connections, there are four distinct affine computations per time step:\n- The input gate, denoted $i_{t}$, which maps $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional gate output.\n- The forget gate, denoted $f_{t}$, which maps $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional gate output.\n- The output gate, denoted $o_{t}$, which maps $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional gate output.\n- The cell candidate transformation, denoted $\\tilde{c}_{t}$, which maps $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional candidate.\n\nEach of these four computations uses two weight matrices: one mapping the current input $x_{t}$ of dimension $n_{x}$ into an $n_{h}$-dimensional output, and one mapping the previous hidden state $h_{t-1}$ of dimension $n_{h}$ into an $n_{h}$-dimensional output, along with an $n_{h}$-dimensional bias. Therefore, per gate or candidate transformation, the parameter count is\n$$\nn_{h} \\cdot n_{x} \\;+\\; n_{h} \\cdot n_{h} \\;+\\; n_{h}.\n$$\nBecause there are four such computations in an LSTM, the total LSTM parameter count is\n$$\nP_{\\mathrm{LSTM}}(n_{x}, n_{h}) \\;=\\; 4 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right).\n$$\n\nIn the standard Gated Recurrent Unit (GRU) layer, there are three distinct affine computations per time step:\n- The update gate, denoted $z_{t}$, mapping $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional gate output.\n- The reset gate, denoted $r_{t}$, mapping $(x_{t}, h_{t-1})$ to an $n_{h}$-dimensional gate output.\n- The hidden candidate transformation, denoted $\\tilde{h}_{t}$, mapping $(x_{t}, r_{t} \\odot h_{t-1})$ to an $n_{h}$-dimensional candidate.\n\nEach of these three computations similarly uses two weight matrices and one bias vector sized to produce an $n_{h}$-dimensional output, so the parameter count per computation is again\n$$\nn_{h} \\cdot n_{x} \\;+\\; n_{h} \\cdot n_{h} \\;+\\; n_{h}.\n$$\nBecause there are three such computations in a GRU, the total GRU parameter count is\n$$\nP_{\\mathrm{GRU}}(n_{x}, n_{h}) \\;=\\; 3 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right).\n$$\n\nWe are asked to compute the numerical counts for $n_{x} = 128$ and $n_{h} = 256$. First compute the shared inner quantity:\n$$\nn_{h} n_{x} + n_{h}^{2} + n_{h}\n\\;=\\;\n256 \\cdot 128 \\;+\\; 256^{2} \\;+\\; 256.\n$$\nEvaluate each term:\n$$\n256 \\cdot 128 \\;=\\; 32768, \\quad\n256^{2} \\;=\\; 65536, \\quad\n256 \\;=\\; 256.\n$$\nSumming gives\n$$\n32768 + 65536 + 256 \\;=\\; 98560.\n$$\nTherefore,\n$$\nP_{\\mathrm{LSTM}}(128, 256) \\;=\\; 4 \\cdot 98560 \\;=\\; 394240,\n$$\nand\n$$\nP_{\\mathrm{GRU}}(128, 256) \\;=\\; 3 \\cdot 98560 \\;=\\; 295680.\n$$\nThe ratio of the LSTM parameter count to the GRU parameter count simplifies to\n$$\n\\frac{P_{\\mathrm{LSTM}}(n_{x}, n_{h})}{P_{\\mathrm{GRU}}(n_{x}, n_{h})}\n\\;=\\;\n\\frac{4 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right)}{3 \\left( n_{h} n_{x} + n_{h}^{2} + n_{h} \\right)}\n\\;=\\;\n\\frac{4}{3},\n$$\nwhich is dimension-independent under the stated assumptions.\n\nImplications for sample efficiency in limited neuroscience datasets: The dominant term in both $P_{\\mathrm{LSTM}}$ and $P_{\\mathrm{GRU}}$ is $n_{h}^{2}$, reflecting the quadratic growth in parameters due to recurrent connections. The LSTM has a constant-factor increase of $\\frac{4}{3}$ in parameters relative to the GRU because it instantiates four affine computations per time step rather than three. In regimes where the number of training trials or sequences is limited relative to the number of parameters, the larger parameter count generally increases the risk of overfitting and demands stronger regularization or more data to achieve comparable generalization. Conversely, the GRU’s smaller parameterization can be more sample-efficient, potentially yielding better generalization with fewer trials, provided its representational capacity suffices for the temporal dependencies in the neural data. The exact trade-off depends on task complexity, regularization strength, prior constraints, and architectural details, but the derived counts provide a principled way to anticipate and compare sample efficiency constraints when choosing between these recurrent layer types.",
            "answer": "$$\\boxed{\\begin{pmatrix}394240 & 295680 & \\frac{4}{3}\\end{pmatrix}}$$"
        },
        {
            "introduction": "In our final practice, we transition from architectural considerations to the underlying mechanics of learning. This advanced exercise requires a deep dive into the calculus of model training by deriving the gradient expressions for an LSTM's forget gate parameters using the Backpropagation Through Time (BPTT) algorithm. By meticulously tracing the error signals back through the network's gates and states, you will gain a fundamental understanding of how an LSTM's structure facilitates the flow of information over long time intervals, enabling the learning of complex temporal dynamics .",
            "id": "4175988",
            "problem": "Consider a Long Short-Term Memory (LSTM) recurrent neural network used to model a multivariate neuroscience time series of synaptic input features. Let the input at time $t$ be $x_t \\in \\mathbb{R}^{n}$, the hidden state be $h_t \\in \\mathbb{R}^{m}$, and the cell state be $c_t \\in \\mathbb{R}^{m}$. The LSTM dynamics are defined by the following gating equations for $t = 1, 2, \\dots, T$:\n$$\na^{f}_t = W_f x_t + U_f h_{t-1} + b_f,\\quad f_t = \\sigma(a^{f}_t),\n$$\n$$\na^{i}_t = W_i x_t + U_i h_{t-1} + b_i,\\quad i_t = \\sigma(a^{i}_t),\n$$\n$$\na^{o}_t = W_o x_t + U_o h_{t-1} + b_o,\\quad o_t = \\sigma(a^{o}_t),\n$$\n$$\na^{g}_t = W_g x_t + U_g h_{t-1} + b_g,\\quad g_t = \\tanh(a^{g}_t),\n$$\n$$\nc_t = f_t \\odot c_{t-1} + i_t \\odot g_t,\\quad h_t = o_t \\odot \\tanh(c_t),\n$$\nwhere $\\sigma(z)$ is the logistic sigmoid function applied elementwise, $\\tanh(z)$ is the hyperbolic tangent applied elementwise, and $\\odot$ denotes the elementwise (Hadamard) product. Assume $h_0$ and $c_0$ are given. The readout predicting an observed neural response $r_t \\in \\mathbb{R}^{p}$ is given by\n$$\ny_t = W_y h_t + b_y,\n$$\nand the total loss over the sequence is the sum of squared errors\n$$\nL = \\sum_{t=1}^{T} \\frac{1}{2} \\|y_t - r_t\\|_2^2.\n$$\nUsing only fundamental definitions (chain rule of calculus, the LSTM forward equations above, and the standard elementwise derivatives $\\sigma'(z) = \\sigma(z)\\big(1 - \\sigma(z)\\big)$ and $\\tanh'(z) = 1 - \\tanh^2(z)$), derive a closed-form analytical expression for the Backpropagation Through Time (BPTT) gradients with respect to the forget gate parameters $W_f \\in \\mathbb{R}^{m \\times n}$, $U_f \\in \\mathbb{R}^{m \\times m}$, and $b_f \\in \\mathbb{R}^{m}$. Your final expressions must be written as explicit summations over time involving outer products between appropriate error signals and inputs or hidden states, expanded down to the forget gate preactivation level. Express your final answer as a single symbolic analytical expression, collecting the three gradients in one row using the $\\mathrm{pmatrix}$ environment, and do not include any units. No numerical evaluation or rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains a complete and consistent set of definitions for a standard Long Short-Term Memory (LSTM) network and a corresponding optimization problem. The task is to derive the gradients of the total loss function with respect to the parameters of the forget gate, which is a standard and solvable problem in machine learning using the Backpropagation Through Time (BPTT) algorithm.\n\nThe objective is to derive the analytical expressions for the gradients of the total loss $L$ with respect to the forget gate parameters $W_f$, $U_f$, and $b_f$. The total loss is given by $L = \\sum_{t=1}^{T} L_t$, where $L_t = \\frac{1}{2} \\|y_t - r_t\\|_2^2$. Since the parameters are shared across all time steps, the total gradient with respect to a parameter is the sum of its contributions at each time step.\n\nThe forget gate parameters $W_f$, $U_f$, and $b_f$ directly influence the loss $L$ through the forget gate pre-activation vector $a^f_t$ at each time step $t$. Using the chain rule, we can express the gradients as follows:\n$$\n\\frac{\\partial L}{\\partial W_f} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial a^f_t} \\frac{\\partial a^f_t}{\\partial W_f}\n$$\n$$\n\\frac{\\partial L}{\\partial U_f} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial a^f_t} \\frac{\\partial a^f_t}{\\partial U_f}\n$$\n$$\n\\frac{\\partial L}{\\partial b_f} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial a^f_t} \\frac{\\partial a^f_t}{\\partial b_f} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial a^f_t}\n$$\nLet us define the error signal at the forget gate pre-activation as $\\delta_t^{a^f} = \\frac{\\partial L}{\\partial a^f_t} \\in \\mathbb{R}^m$.\n\nFrom the definition $a^f_t = W_f x_t + U_f h_{t-1} + b_f$, the partial derivatives with respect to the parameters are $\\frac{\\partial a^f_t}{\\partial W_f} = (x_t)^T$ (in the appropriate Jacobian sense leading to an outer product), $\\frac{\\partial a^f_t}{\\partial U_f} = (h_{t-1})^T$, and $\\frac{\\partial a^f_t}{\\partial b_f}$ results in a summation. The gradients can thus be written as:\n$$\n\\frac{\\partial L}{\\partial W_f} = \\sum_{t=1}^{T} \\delta_t^{a^f} (x_t)^T\n$$\n$$\n\\frac{\\partial L}{\\partial U_f} = \\sum_{t=1}^{T} \\delta_t^{a^f} (h_{t-1})^T\n$$\n$$\n\\frac{\\partial L}{\\partial b_f} = \\sum_{t=1}^{T} \\delta_t^{a^f}\n$$\nThe core of the problem is to find an expression for the error signal $\\delta_t^{a^f}$. We apply the chain rule, propagating the error backwards from the loss function.\n\nFirst, we express $\\delta_t^{a^f}$ in terms of the error signal at the cell state, $\\delta_t^c = \\frac{\\partial L}{\\partial c_t} \\in \\mathbb{R}^m$.\nThe path is $L \\rightarrow c_t \\rightarrow f_t \\rightarrow a^f_t$.\n$$\n\\delta_t^{a^f} = \\frac{\\partial L}{\\partial a^f_t} = \\left(\\frac{\\partial c_t}{\\partial a^f_t}\\right)^T \\frac{\\partial L}{\\partial c_t} = \\left(\\frac{\\partial c_t}{\\partial a^f_t}\\right)^T \\delta_t^c\n$$\nSince all operations are elementwise, the Jacobian is diagonal, and this simplifies to an elementwise product:\n$$\n\\delta_t^{a^f} = \\frac{\\partial c_t}{\\partial a^f_t} \\odot \\delta_t^c\n$$\nFrom $c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$ and $f_t = \\sigma(a^f_t)$, we find the derivative:\n$$\n\\frac{\\partial c_t}{\\partial a^f_t} = \\frac{\\partial f_t}{\\partial a^f_t} \\odot c_{t-1} = \\sigma'(a^f_t) \\odot c_{t-1}\n$$\nUsing the given derivative $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$, we have $\\sigma'(a^f_t) = f_t \\odot (1-f_t)$.\nThus, the expression for $\\delta_t^{a^f}$ is:\n$$\n\\delta_t^{a^f} = \\delta_t^c \\odot c_{t-1} \\odot f_t \\odot (1 - f_t)\n$$\nNext, we must define $\\delta_t^c$. The cell state $c_t$ influences the loss $L$ via two paths: through the current hidden state $h_t$ and through the next cell state $c_{t+1}$.\n$$\n\\delta_t^c = \\frac{\\partial L}{\\partial c_t} = \\left(\\frac{\\partial h_t}{\\partial c_t}\\right)^T \\frac{\\partial L}{\\partial h_t} + \\left(\\frac{\\partial c_{t+1}}{\\partial c_t}\\right)^T \\frac{\\partial L}{\\partial c_{t+1}}\n$$\nLet $\\delta_t^h = \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^m$. The expression becomes a recurrence relation for $\\delta_t^c$:\n$$\n\\delta_t^c = \\left(\\frac{\\partial h_t}{\\partial c_t} \\odot \\delta_t^h\\right) + \\left(\\frac{\\partial c_{t+1}}{\\partial c_t} \\odot \\delta_{t+1}^c\\right)\n$$\nFrom $h_t = o_t \\odot \\tanh(c_t)$, we get $\\frac{\\partial h_t}{\\partial c_t} = o_t \\odot \\tanh'(c_t) = o_t \\odot (1 - \\tanh^2(c_t))$.\nFrom $c_{t+1} = f_{t+1} \\odot c_t + i_{t+1} \\odot g_{t+1}$, we get $\\frac{\\partial c_{t+1}}{\\partial c_t} = f_{t+1}$.\nThe recurrence for $\\delta_t^c$ is therefore:\n$$\n\\delta_t^c = \\delta_t^h \\odot o_t \\odot (1 - \\tanh^2(c_t)) + \\delta_{t+1}^c \\odot f_{t+1}\n$$\nwith boundary condition $\\delta_{T+1}^c = \\vec{0}$, since there is no time step $T+1$.\n\nFinally, we need to define $\\delta_t^h$. The hidden state $h_t$ influences the loss through the current output $y_t$ and through all the gate pre-activations at the next time step, $a^f_{t+1}, a^i_{t+1}, a^o_{t+1}, a^g_{t+1}$.\n$$\n\\delta_t^h = \\frac{\\partial L}{\\partial h_t} = \\left(\\frac{\\partial y_t}{\\partial h_t}\\right)^T \\frac{\\partial L}{\\partial y_t} + \\left(\\frac{\\partial a^f_{t+1}}{\\partial h_t}\\right)^T \\frac{\\partial L}{\\partial a^f_{t+1}} + \\left(\\frac{\\partial a^i_{t+1}}{\\partial h_t}\\right)^T \\frac{\\partial L}{\\partial a^i_{t+1}} + \\left(\\frac{\\partial a^o_{t+1}}{\\partial h_t}\\right)^T \\frac{\\partial L}{\\partial a^o_{t+1}} + \\left(\\frac{\\partial a^g_{t+1}}{\\partial h_t}\\right)^T \\frac{\\partial L}{\\partial a^g_{t+1}}\n$$\nFrom the given equations: $\\frac{\\partial y_t}{\\partial h_t} = W_y$, $\\frac{\\partial a^f_{t+1}}{\\partial h_t} = U_f$, $\\frac{\\partial a^i_{t+1}}{\\partial h_t} = U_i$, $\\frac{\\partial a^o_{t+1}}{\\partial h_t} = U_o$, $\\frac{\\partial a^g_{t+1}}{\\partial h_t} = U_g$. The output error is $\\frac{\\partial L}{\\partial y_t} = \\frac{\\partial L_t}{\\partial y_t} = y_t - r_t$.\nThe recurrence for $\\delta_t^h$ is:\n$$\n\\delta_t^h = W_y^T (y_t - r_t) + U_f^T \\delta_{t+1}^{a^f} + U_i^T \\delta_{t+1}^{a^i} + U_o^T \\delta_{t+1}^{a^o} + U_g^T \\delta_{t+1}^{a^g}\n$$\nwith boundary condition $\\delta_{T+1}^{a^\\cdot} = \\vec{0}$ for all gates, which implies the recurrence starts with $\\delta_T^h = W_y^T (y_T - r_T)$. The required gate error signals at time $t+1$ are themselves derived similarly to $\\delta_t^{a^f}$:\n$$\n\\delta_{t+1}^{a^f} = \\delta_{t+1}^c \\odot c_{t} \\odot f_{t+1} \\odot (1 - f_{t+1})\n$$\n$$\n\\delta_{t+1}^{a^i} = \\delta_{t+1}^c \\odot g_{t+1} \\odot i_{t+1} \\odot (1 - i_{t+1})\n$$\n$$\n\\delta_{t+1}^{a^o} = \\delta_{t+1}^h \\odot \\tanh(c_{t+1}) \\odot o_{t+1} \\odot (1 - o_{t+1})\n$$\n$$\n\\delta_{t+1}^{a^g} = \\delta_{t+1}^c \\odot i_{t+1} \\odot (1 - g_{t+1}^2)\n$$\nThese recurrence relations, computed backwards in time from $t=T$ to $t=1$, define the quantities needed to calculate the gradients. The final analytical expressions for the gradients are thus given by the sums over time involving the signal $\\delta_t^{a^f}$, which is determined by the system of BPTT equations above.\nThe final expressions for the gradients are:\n$\n\\frac{\\partial L}{\\partial W_f} = \\sum_{t=1}^{T} \\delta_t^{a^f} (x_t)^T\n$,\n$\n\\frac{\\partial L}{\\partial U_f} = \\sum_{t=1}^{T} \\delta_t^{a^f} (h_{t-1})^T\n$, and\n$\n\\frac{\\partial L}{\\partial b_f} = \\sum_{t=1}^{T} \\delta_t^{a^f}\n$.\nThe error term $\\delta_t^{a^f}$ is defined as $\\delta_t^{a^f} = \\delta_t^c \\odot c_{t-1} \\odot f_t \\odot (1 - f_t)$, where $\\delta_t^c$ is defined by the full BPTT recurrence.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sum_{t=1}^{T} \\delta_t^{a^f} (x_t)^T & \\sum_{t=1}^{T} \\delta_t^{a^f} (h_{t-1})^T & \\sum_{t=1}^{T} \\delta_t^{a^f}\n\\end{pmatrix}\n\\text{, where } \\delta_t^{a^f} = \\left( \\delta_t^h \\odot o_t \\odot (1 - \\tanh^2(c_t)) + \\delta_{t+1}^c \\odot f_{t+1} \\right) \\odot c_{t-1} \\odot f_t \\odot (1-f_t)\n}\n$$"
        }
    ]
}