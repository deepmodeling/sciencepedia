{
    "hands_on_practices": [
        {
            "introduction": "变分自编码器（VAE）的一个核心是证据下界（ELBO）中的KL散度项。该项作为正则化器，促使编码器产生的后验分布 $q(\\boldsymbol{z}|\\boldsymbol{x})$ 接近于一个标准正态先验分布 $p(\\boldsymbol{z})$。这项实践将引导你从第一性原理出发，推导高斯分布之间的KL散度解析表达式，这对于深刻理解VAE如何塑造和约束其潜空间至关重要 。",
            "id": "4139921",
            "problem": "一个变分自编码器 (VAE) 被训练用于建模低维潜变量，这些潜变量捕捉了同时记录的神经元群体中的协同神经活动。对于每个观测到的神经活动向量，编码器会生成一个对角高斯近似后验，形式为 $q(\\boldsymbol{z}\\mid \\boldsymbol{x})=\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$，其中 $\\boldsymbol{\\Sigma}=\\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{k}^{2})$，并且方差由 $\\log \\sigma_{i}^{2}$ 逐元素参数化。先验是标准正态分布 $p(\\boldsymbol{z})=\\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I})$。对于来自钙成像数据集的某个特定留出试验，编码器输出 $k=4$ 个潜维度，其参数为\n$\\boldsymbol{\\mu}=(0.6,\\,-0.8,\\,0.1,\\,-0.3)^{\\top}$ 和 $\\log \\boldsymbol{\\sigma}^{2}=(-0.2,\\,0.7,\\,-1.0,\\,-0.5)^{\\top}$。\n仅从Kullback–Leibler散度的定义 $ \\mathrm{KL}(q\\parallel p)=\\mathbb{E}_{q}[\\ln q(\\boldsymbol{z})-\\ln p(\\boldsymbol{z})] $ 和多元高斯分布的概率密度函数出发，推导出一个关于 $\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\Sigma}$ 的$\\mathrm{KL}(q\\parallel p)$的解析表达式，并针对此对角情形进行特化，然后用给定值对其进行数值计算。以奈特（nats）为单位报告最终数值，并将您的答案四舍五入至$4$位有效数字。",
            "solution": "该问题要求推导对角协方差高斯分布 $q(\\boldsymbol{z})$ 和标准正态分布 $p(\\boldsymbol{z})$ 之间的Kullback-Leibler (KL) 散度的解析表达式，然后根据给定参数进行数值计算。\n\n### 第一步：KL散度的解析推导\n\n我们已知KL散度的定义：\n$$\n\\mathrm{KL}(q\\parallel p) = \\mathbb{E}_{q}[\\ln q(\\boldsymbol{z}) - \\ln p(\\boldsymbol{z})] = \\mathbb{E}_{q}[\\ln q(\\boldsymbol{z})] - \\mathbb{E}_{q}[\\ln p(\\boldsymbol{z})]\n$$\n近似后验 $q(\\boldsymbol{z})$ 是一个 $k$ 维多元高斯分布，$q(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$。其概率密度函数 (PDF) 为：\n$$\nq(\\boldsymbol{z}) = \\frac{1}{(2\\pi)^{k/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\\right)\n$$\n此PDF的自然对数为：\n$$\n\\ln q(\\boldsymbol{z}) = -\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2}(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\n$$\n先验 $p(\\boldsymbol{z})$ 是一个标准的 $k$ 维正态分布，$p(\\boldsymbol{z})=\\mathcal{N}(\\boldsymbol{z} \\mid \\boldsymbol{0}, \\boldsymbol{I})$，其中 $\\boldsymbol{I}$ 是单位矩阵。其PDF为：\n$$\np(\\boldsymbol{z}) = \\frac{1}{(2\\pi)^{k/2} |\\boldsymbol{I}|^{1/2}} \\exp\\left(-\\frac{1}{2}\\boldsymbol{z}^{\\top}\\boldsymbol{I}^{-1}\\boldsymbol{z}\\right) = \\frac{1}{(2\\pi)^{k/2}} \\exp\\left(-\\frac{1}{2}\\boldsymbol{z}^{\\top}\\boldsymbol{z}\\right)\n$$\n先验的PDF的自然对数为：\n$$\n\\ln p(\\boldsymbol{z}) = -\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\boldsymbol{z}^{\\top}\\boldsymbol{z}\n$$\n现在，我们计算这些对数PDF关于 $q(\\boldsymbol{z})$ 的期望。\n\n首先，对于 $\\mathbb{E}_{q}[\\ln q(\\boldsymbol{z})]$：\n$$\n\\mathbb{E}_{q}[\\ln q(\\boldsymbol{z})] = \\mathbb{E}_{q}\\left[-\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2}(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\\right]\n$$\n前两项相对于 $\\boldsymbol{z}$ 是常数，因此它们的期望就是其本身。我们只需要计算二次型的期望。利用期望的线性和迹算子技巧：\n\\begin{align*} \\mathbb{E}_{q}\\left[(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\\right] = \\mathbb{E}_{q}\\left[\\mathrm{tr}\\left((\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\\right)\\right] \\\\ = \\mathbb{E}_{q}\\left[\\mathrm{tr}\\left(\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\right)\\right] \\\\ = \\mathrm{tr}\\left(\\boldsymbol{\\Sigma}^{-1}\\mathbb{E}_{q}\\left[(\\boldsymbol{z}-\\boldsymbol{\\mu})(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\right]\\right)\\end{align*}\n根据定义，$\\mathbb{E}_{q}\\left[(\\boldsymbol{z}-\\boldsymbol{\\mu})(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\right]$ 是 $q(\\boldsymbol{z})$ 的协方差矩阵，即 $\\boldsymbol{\\Sigma}$。\n$$\n\\mathrm{tr}\\left(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\Sigma}\\right) = \\mathrm{tr}(\\boldsymbol{I}_{k}) = k\n$$\n因此，对数后验的期望为：\n$$\n\\mathbb{E}_{q}[\\ln q(\\boldsymbol{z})] = -\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{k}{2}\n$$\n接下来，对于 $\\mathbb{E}_{q}[\\ln p(\\boldsymbol{z})]$：\n$$\n\\mathbb{E}_{q}[\\ln p(\\boldsymbol{z})] = \\mathbb{E}_{q}\\left[-\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\boldsymbol{z}^{\\top}\\boldsymbol{z}\\right] = -\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\mathbb{E}_{q}[\\boldsymbol{z}^{\\top}\\boldsymbol{z}]\n$$\n为了计算 $\\mathbb{E}_{q}[\\boldsymbol{z}^{\\top}\\boldsymbol{z}]$，我们使用协方差矩阵的性质：$\\mathrm{Cov}_{q}(\\boldsymbol{z}) = \\mathbb{E}_{q}[\\boldsymbol{z}\\boldsymbol{z}^{\\top}] - \\mathbb{E}_{q}[\\boldsymbol{z}]\\mathbb{E}_{q}[\\boldsymbol{z}]^{\\top}$。\n对于 $q(\\boldsymbol{z})$，$\\mathrm{Cov}_{q}(\\boldsymbol{z}) = \\boldsymbol{\\Sigma}$ 且 $\\mathbb{E}_{q}[\\boldsymbol{z}] = \\boldsymbol{\\mu}$。因此：\n$$\n\\boldsymbol{\\Sigma} = \\mathbb{E}_{q}[\\boldsymbol{z}\\boldsymbol{z}^{\\top}] - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top} \\implies \\mathbb{E}_{q}[\\boldsymbol{z}\\boldsymbol{z}^{\\top}] = \\boldsymbol{\\Sigma} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top}\n$$\n项 $\\boldsymbol{z}^{\\top}\\boldsymbol{z}$ 是一个标量，它是外积 $\\boldsymbol{z}\\boldsymbol{z}^{\\top}$ 的迹。\n$$\n\\mathbb{E}_{q}[\\boldsymbol{z}^{\\top}\\boldsymbol{z}] = \\mathbb{E}_{q}[\\mathrm{tr}(\\boldsymbol{z}\\boldsymbol{z}^{\\top})] = \\mathrm{tr}(\\mathbb{E}_{q}[\\boldsymbol{z}\\boldsymbol{z}^{\\top}]) = \\mathrm{tr}(\\boldsymbol{\\Sigma} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top}) = \\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\mathrm{tr}(\\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top})\n$$\n因为 $\\mathrm{tr}(\\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top}) = \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu}$，我们有：\n$$\n\\mathbb{E}_{q}[\\boldsymbol{z}^{\\top}\\boldsymbol{z}] = \\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu}\n$$\n所以，对数先验的期望为：\n$$\n\\mathbb{E}_{q}[\\ln p(\\boldsymbol{z})] = -\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}(\\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu})\n$$\n将这些期望代入KL散度公式：\n\\begin{align*}\n\\mathrm{KL}(q\\parallel p) = \\left(-\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{k}{2}\\right) - \\left(-\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}(\\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu})\\right) \\\\\n= -\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{k}{2} + \\frac{1}{2}\\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\frac{1}{2}\\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu} \\\\\n= \\frac{1}{2}\\left( \\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu} - k - \\ln|\\boldsymbol{\\Sigma}| \\right)\n\\end{align*}\n这是通用公式。对于对角协方差矩阵 $\\boldsymbol{\\Sigma} = \\mathrm{diag}(\\sigma_{1}^{2}, \\dots, \\sigma_{k}^{2})$ 的情况，我们有：\n-   $\\mathrm{tr}(\\boldsymbol{\\Sigma}) = \\sum_{i=1}^{k} \\sigma_i^2$\n-   $|\\boldsymbol{\\Sigma}| = \\prod_{i=1}^{k} \\sigma_i^2$\n-   $\\ln|\\boldsymbol{\\Sigma}| = \\sum_{i=1}^{k} \\ln(\\sigma_i^2)$\n此外，$\\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu} = \\sum_{i=1}^{k} \\mu_i^2$。将这些代入公式：\n$$\n\\mathrm{KL}(q\\parallel p) = \\frac{1}{2}\\left( \\sum_{i=1}^{k} \\sigma_i^2 + \\sum_{i=1}^{k} \\mu_i^2 - k - \\sum_{i=1}^{k} \\ln(\\sigma_i^2) \\right)\n$$\n按维度重新整理各项，得到最终的解析表达式：\n$$\n\\mathrm{KL}(q\\parallel p) = \\frac{1}{2} \\sum_{i=1}^{k} \\left( \\sigma_i^2 + \\mu_i^2 - 1 - \\ln(\\sigma_i^2) \\right)\n$$\n\n### 第二步：数值计算\n\n我们已知潜空间维度 $k = 4$ 时的以下值：\n-   $\\boldsymbol{\\mu}=(0.6,\\,-0.8,\\,0.1,\\,-0.3)^{\\top}$\n-   $\\log \\boldsymbol{\\sigma}^{2}=(-0.2,\\,0.7,\\,-1.0,\\,-0.5)^{\\top}$，其中向量分量为 $\\ln(\\sigma_i^2)$。\n\n让我们计算每个维度 $i \\in \\{1, 2, 3, 4\\}$ 的求和分量。注意 $\\sigma_i^2 = \\exp(\\ln(\\sigma_i^2))$。\n\n对于 $i=1$：\n-   $\\mu_1 = 0.6 \\implies \\mu_1^2 = 0.36$\n-   $\\ln(\\sigma_1^2) = -0.2 \\implies \\sigma_1^2 = \\exp(-0.2)$\n-   第1项: $\\exp(-0.2) + 0.36 - 1 - (-0.2) = \\exp(-0.2) - 0.44$\n\n对于 $i=2$：\n-   $\\mu_2 = -0.8 \\implies \\mu_2^2 = 0.64$\n-   $\\ln(\\sigma_2^2) = 0.7 \\implies \\sigma_2^2 = \\exp(0.7)$\n-   第2项: $\\exp(0.7) + 0.64 - 1 - 0.7 = \\exp(0.7) - 1.06$\n\n对于 $i=3$：\n-   $\\mu_3 = 0.1 \\implies \\mu_3^2 = 0.01$\n-   $\\ln(\\sigma_3^2) = -1.0 \\implies \\sigma_3^2 = \\exp(-1.0)$\n-   第3项: $\\exp(-1.0) + 0.01 - 1 - (-1.0) = \\exp(-1.0) + 0.01$\n\n对于 $i=4$：\n-   $\\mu_4 = -0.3 \\implies \\mu_4^2 = 0.09$\n-   $\\ln(\\sigma_4^2) = -0.5 \\implies \\sigma_4^2 = \\exp(-0.5)$\n-   第4项: $\\exp(-0.5) + 0.09 - 1 - (-0.5) = \\exp(-0.5) - 0.41$\n\n总和为：\n\\begin{align*}\n\\sum_{i=1}^{4} \\left( \\sigma_i^2 + \\mu_i^2 - 1 - \\ln(\\sigma_i^2) \\right) = (\\exp(-0.2) - 0.44) + (\\exp(0.7) - 1.06) + (\\exp(-1.0) + 0.01) + (\\exp(-0.5) - 0.41) \\\\\n= \\sum_{i=1}^4 \\sigma_i^2 + \\sum_{i=1}^4 \\mu_i^2 - \\sum_{i=1}^4 1 - \\sum_{i=1}^4 \\ln(\\sigma_i^2) \\\\\n\\end{align*}\n让我们计算各分量的总和：\n-   $\\sum \\mu_i^2 = (0.6)^2 + (-0.8)^2 + (0.1)^2 + (-0.3)^2 = 0.36 + 0.64 + 0.01 + 0.09 = 1.1$\n-   $\\sum \\ln(\\sigma_i^2) = -0.2 + 0.7 - 1.0 - 0.5 = -1.0$\n-   $\\sum \\sigma_i^2 = \\exp(-0.2) + \\exp(0.7) + \\exp(-1.0) + \\exp(-0.5)$\n    $\\approx 0.81873075 + 2.01375271 + 0.36787944 + 0.60653066 \\approx 3.80689356$\n-   $k = 4$\n\n括号内的总和为：\n$$\n\\sum (\\dots) = 3.80689356 + 1.1 - 4 - (-1.0) = 3.80689356 + 1.1 - 4 + 1.0 = 1.90689356\n$$\n最后，KL散度为：\n$$\n\\mathrm{KL}(q\\parallel p) = \\frac{1}{2} \\times 1.90689356 \\approx 0.95344678\n$$\n四舍五入到4位有效数字，我们得到 $0.9534$。单位是奈特（nats），因为定义中使用了自然对数。",
            "answer": "$$\\boxed{0.9534}$$"
        },
        {
            "introduction": "在掌握了正则化项之后，我们将注意力转向VAE损失函数的另一半：重构项。神经科学中的许多数据（例如神经元发放计数）并非高斯分布，因此我们需要为模型选择一个更合适的似然函数。这项实践探讨了如何通过使用泊松（Poisson）似然函数来改造VAE，使其适用于分析计数数据，这对于将生成模型应用于真实的神经科学问题至关重要 。",
            "id": "4139965",
            "problem": "考虑被建模为在给定潜变量下条件独立的泊松观测的计数值神经数据。令观测计数为跨越 $D$ 个神经元的向量 $x \\in \\mathbb{N}^{D}$，潜变量为 $z \\in \\mathbb{R}^{K}$。假设似然分解为 $p(x \\mid z) = \\prod_{d=1}^{D} p(x_{d} \\mid z)$，其中每个 $p(x_{d} \\mid z)$ 是速率为 $\\lambda_{d}(z)$ 的 $\\mathrm{Poisson}$ 分布。变分自编码器 (VAE) 的证据下界中的重构项定义为期望负对数似然 $\\mathbb{E}_{q(z \\mid x)}\\left[-\\ln p(x \\mid z)\\right]$，其中 $q(z \\mid x)$ 是变分后验。从泊松概率质量函数的定义和期望的基本性质出发，推导泊松似然的重构项 $\\mathbb{E}_{q(z \\mid x)}\\left[-\\ln p(x \\mid z)\\right]$ 的一般形式。然后，在速率具有指数非线性的线性解码器 $\\lambda_{d}(z) = \\exp\\left(b_{d} + w_{d}^{\\top} z\\right)$（对于 $d \\in \\{1,\\dots,D\\}$，其中 $w_{d} \\in \\mathbb{R}^{K}$ 和 $b_{d} \\in \\mathbb{R}$）下，并假设一个多元正态变分后验 $q(z \\mid x) = \\mathcal{N}(\\mu, \\Sigma)$，其均值为 $\\mu \\in \\mathbb{R}^{K}$，协方差为对称半正定矩阵 $\\Sigma \\in \\mathbb{R}^{K \\times K}$，为重构项中出现的两个期望提供闭式解析表达式。你的最终答案应为关于 $x$、$\\{w_{d}\\}_{d=1}^{D}$、$\\{b_{d}\\}_{d=1}^{D}$、$\\mu$ 和 $\\Sigma$ 的 $\\mathbb{E}_{q(z \\mid x)}\\left[-\\ln p(x \\mid z)\\right]$ 的单个闭式解析表达式。无需四舍五入。",
            "solution": "目标是推导在变分后验下泊松似然的重构项。我们从基本定义开始。\n\n首先，回顾泊松概率质量函数的定义。对于速率为 $\\lambda_{d}(z) > 0$ 的单个计数 $x_{d} \\in \\mathbb{N}$，其似然为\n$$\np(x_{d} \\mid z) = \\frac{\\lambda_{d}(z)^{x_{d}} \\exp\\left(-\\lambda_{d}(z)\\right)}{x_{d}!}.\n$$\n在 $d \\in \\{1,\\dots,D\\}$ 上条件独立的情况下，\n$$\np(x \\mid z) = \\prod_{d=1}^{D} p(x_{d} \\mid z) = \\prod_{d=1}^{D} \\frac{\\lambda_{d}(z)^{x_{d}} \\exp\\left(-\\lambda_{d}(z)\\right)}{x_{d}!}.\n$$\n取自然对数并加上负号，负对数似然在各个维度上分解为：\n$$\n-\\ln p(x \\mid z) = \\sum_{d=1}^{D} \\left[ \\lambda_{d}(z) - x_{d} \\ln \\lambda_{d}(z) + \\ln\\left(x_{d}!\\right) \\right].\n$$\n重构项是在变分后验 $q(z \\mid x)$ 下该量的期望：\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[-\\ln p(x \\mid z)\\right] = \\sum_{d=1}^{D} \\left[ \\mathbb{E}_{q(z \\mid x)}\\left[\\lambda_{d}(z)\\right] - x_{d} \\,\\mathbb{E}_{q(z \\mid x)}\\left[\\ln \\lambda_{d}(z)\\right] + \\ln\\left(x_{d}!\\right) \\right].\n$$\n因此，问题简化为对每个 $d$ 计算 $\\mathbb{E}_{q(z \\mid x)}\\left[\\lambda_{d}(z)\\right]$ 和 $\\mathbb{E}_{q(z \\mid x)}\\left[\\ln \\lambda_{d}(z)\\right]$。\n\n在指定的具有指数非线性的线性解码器下，\n$$\n\\lambda_{d}(z) = \\exp\\left(b_{d} + w_{d}^{\\top} z\\right),\n$$\n且变分后验是多元正态的，\n$$\nq(z \\mid x) = \\mathcal{N}(\\mu, \\Sigma),\n$$\n其中 $\\mu \\in \\mathbb{R}^{K}$，$\\Sigma \\in \\mathbb{R}^{K \\times K}$ 是对称半正定矩阵。\n\n我们首先计算 $\\mathbb{E}_{q(z \\mid x)}\\left[\\ln \\lambda_{d}(z)\\right]$。使用 $\\lambda_{d}(z)$ 的对数，\n$$\n\\ln \\lambda_{d}(z) = b_{d} + w_{d}^{\\top} z,\n$$\n以及期望的线性性质，\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[\\ln \\lambda_{d}(z)\\right] = b_{d} + w_{d}^{\\top} \\,\\mathbb{E}_{q(z \\mid x)}[z] = b_{d} + w_{d}^{\\top} \\mu.\n$$\n\n接下来，我们计算 $\\mathbb{E}_{q(z \\mid x)}\\left[\\lambda_{d}(z)\\right] = \\mathbb{E}_{q(z \\mid x)}\\left[\\exp\\left(b_{d} + w_{d}^{\\top} z\\right)\\right]$。根据多元正态分布的性质，当 $z \\sim \\mathcal{N}(\\mu, \\Sigma)$ 时，线性形式 $w_{d}^{\\top} z$ 的矩生成函数意味着\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[\\exp\\left(w_{d}^{\\top} z\\right)\\right] = \\exp\\left(w_{d}^{\\top} \\mu + \\frac{1}{2} \\, w_{d}^{\\top} \\Sigma \\, w_{d}\\right).\n$$\n因此，\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[\\lambda_{d}(z)\\right] = \\exp\\left(b_{d}\\right) \\,\\mathbb{E}_{q(z \\mid x)}\\left[\\exp\\left(w_{d}^{\\top} z\\right)\\right] = \\exp\\left(b_{d} + w_{d}^{\\top} \\mu + \\frac{1}{2} \\, w_{d}^{\\top} \\Sigma \\, w_{d}\\right).\n$$\n\n将这些期望代入重构项的分解中，得到\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[-\\ln p(x \\mid z)\\right]\n= \\sum_{d=1}^{D} \\left[\n\\exp\\left(b_{d} + w_{d}^{\\top} \\mu + \\frac{1}{2} \\, w_{d}^{\\top} \\Sigma \\, w_{d}\\right)\n- x_{d} \\left(b_{d} + w_{d}^{\\top} \\mu\\right)\n+ \\ln\\left(x_{d}!\\right)\n\\right].\n$$\n这是一个关于 $x$、$\\{w_{d}\\}_{d=1}^{D}$、$\\{b_{d}\\}_{d=1}^{D}$、$\\mu$ 和 $\\Sigma$ 的闭式解析表达式。这完成了在多元正态变分后验下，对具有指数线性解码器的泊松似然的重构项的推导。",
            "answer": "$$\\boxed{\\sum_{d=1}^{D}\\left[\\exp\\left(b_{d}+w_{d}^{\\top}\\mu+\\frac{1}{2}\\,w_{d}^{\\top}\\Sigma\\,w_{d}\\right)-x_{d}\\left(b_{d}+w_{d}^{\\top}\\mu\\right)+\\ln\\!\\left(x_{d}!\\right)\\right]}$$"
        },
        {
            "introduction": "构建一个VAE模型只是研究过程的开始，评估其学到的表征质量同等重要。一个好的潜变量模型应该能以一种可解释的方式捕捉数据的内在结构，例如，将不同的生成因子“解耦”（disentangle）到不同的潜维度上。这项编码实践介绍了两种关键的诊断技术——潜空间遍历（latent traversal）和DCI分数，它们提供了量化和评估表征质量的实用工具，这对于验证模型的科学有效性至关重要 。",
            "id": "4139993",
            "problem": "给定一个变分自编码器中标准的潜变量模型，其中潜向量 $z \\in \\mathbb{R}^L$ 通过解码器映射到重建的神经特征 $y \\in \\mathbb{R}^D$。解码器定义为 $y = W \\, s(z)$，其中 $W \\in \\mathbb{R}^{D \\times L}$ 是一个权重矩阵，$s(\\cdot)$ 对每个潜在维度应用指定的逐元素非线性函数。对于每个测试用例，通过 $z \\sim \\mathcal{N}(0, I_L)$ 生成一个潜变量数据集，并通过线性映射加上少量高斯噪声从 $z$ 构建一个真实生成因子向量 $v \\in \\mathbb{R}^K$。您的任务是实现两个诊断指标：\n\n- 潜在遍历单调性 (Latent traversal monotonicity)：在保持其他维度固定的情况下，改变 $z$ 的一个维度，并测量有多少重建的特征维度是单调变化的。\n- 解耦-完整性-信息量 (Disentanglement-Completeness-Informativeness, DCI) 分数：通过训练一个从 $z$ 到 $v$ 的线性预测器，并计算解耦、完整性和信息量指标来量化解耦质量。\n\n使用以下基础：\n\n- 潜变量定义：$z \\sim \\mathcal{N}(0, I_L)$。\n- 解码器定义：$y = W \\, s(z)$。\n- 通过最小化均方误差进行线性预测：给定样本 $\\{(z_i, v_i)\\}_{i=1}^N$，找到最小化 $\\sum_{i=1}^N \\| v_i - Z_i \\, \\Theta \\|_2^2$ 的系数，其中 $Z_i$ 是 $z_i$ 的行向量，$\\Theta \\in \\mathbb{R}^{L \\times K}$ 是回归系数；实现带有小正则化参数的岭回归以确保数值稳定性。\n\n精确定义潜在遍历单调性诊断如下。对于每个潜在索引 $j \\in \\{1,\\dots,L\\}$，构造一个遍历路径 $\\gamma_j(t)$，其中 $t$ 在区间 $[-a,a]$ 内的 $T$ 个点的均匀网格上采样：设置 $(\\gamma_j(t))_j = t$，对于所有 $j' \\neq j$，设置 $(\\gamma_j(t))_{j'} = 0$。对于每个 $t$，计算 $y(t) = W \\, s(\\gamma_j(t))$。对于固定的神经元索引 $d \\in \\{1,\\dots,D\\}$，考虑沿网格的序列 $\\{ y_d(t_m) \\}_{m=1}^T$。定义离散差分 $\\Delta_m = y_d(t_{m+1}) - y_d(t_m)$，其中 $m=1,\\dots,T-1$。如果所有非零的 $\\Delta_m$ 符号相同，并且至少存在一个非零的 $\\Delta_m$，则神经元 $d$ 沿遍历 $j$ 的响应被声明为单调的。潜在维度 $j$ 的单调性分数是沿 $\\gamma_j$ 单调的神经元所占的比例，而总体单调性分数 $M$ 是这些比例在 $j=1,\\dots,L$ 上的平均值。\n\n定义解耦-完整性-信息量 (DCI) 指标如下。拟合一个系数矩阵为 $\\Theta \\in \\mathbb{R}^{L \\times K}$ 的岭回归模型来从 $z$ 预测 $v$。定义重要性矩阵 $R \\in \\mathbb{R}^{L \\times K}$，其元素为 $r_{j,k} = | \\theta_{j,k} |$。令总重要性为 $S = \\sum_{j=1}^L \\sum_{k=1}^K r_{j,k}$。对于每个潜在维度 $j$，定义一个关于因子的分布 $p_{j,k} = r_{j,k} / \\sum_{k'=1}^K r_{j,k'}$，如果分母非零；否则，潜在维度 $j$ 的解耦度取为零。每个潜在维度的解耦度为 $d_j = 1 - H(p_j)/\\log K$，其中 $H(p_j) = -\\sum_{k=1}^K p_{j,k} \\log p_{j,k}$ 是以自然单位计的香农熵。用 $\\rho_j = \\left(\\sum_{k=1}^K r_{j,k}\\right) / S$ 对每个潜在维度加权，并定义总体解耦分数为 $D = \\sum_{j=1}^L \\rho_j \\, d_j$。对于完整性，为每个因子 $k$ 定义一个关于潜在维度的分布 $q_{k,j} = r_{j,k} / \\sum_{j'=1}^L r_{j',k}$，如果分母非零；否则，因子 $k$ 的完整性取为零。每个因子的完整性为 $c_k = 1 - H(q_k)/\\log L$，用 $\\pi_k = \\left(\\sum_{j=1}^L r_{j,k}\\right) / S$ 加权，得到总体完整性分数 $C = \\sum_{k=1}^K \\pi_k \\, c_k$。对于信息量，在数据集上计算岭回归预测 $\\hat{v} = Z \\, \\Theta$，并定义每个因子的误差 $e_k = \\mathrm{MSE}_k / \\mathrm{Var}(v_k)$，其中 $\\mathrm{MSE}_k = \\frac{1}{N} \\sum_{i=1}^N ( \\hat{v}_{i,k} - v_{i,k} )^2$，$\\mathrm{Var}(v_k)$ 是因子 $k$ 的经验方差。定义信息量为 $I = 1 - \\frac{1}{K} \\sum_{k=1}^K e_k$，如果 $\\mathrm{Var}(v_k)$ 在数值上为零，则在分母中使用一个小的正常数。\n\n为以下三个测试用例实现上述功能，每个用例都具有 $L = 3$，$K = 3$，$D = 12$，$N = 800$，岭回归正则化参数 $\\lambda = 10^{-3}$，遍历幅度 $a = 3$，以及 $T = 101$ 个遍历点。所有随机抽样必须使用指定的种子。\n\n- 用例 1（解耦，单调）：使用种子 $0$ 采样 $z \\sim \\mathcal{N}(0, I_3)$。设置 $v = z + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_v^2 I_3)$，$\\sigma_v = 0.05$，种子 $10$。对所有 $j \\in \\{1,2,3\\}$ 使用解码器非线性函数 $s_j(u) = \\tanh(u)$。使用按潜在维度列指定的权重矩阵 $W \\in \\mathbb{R}^{12 \\times 3}$，\n  - 第 1 列 ($j=1$)：神经元 $d=1,\\dots,12$ 的权重为 $[1.0, 0.7, -0.9, 1.3, 0, 0, 0, 0, 0, 0, 0, 0]$，\n  - 第 2 列 ($j=2$)：$[0, 0, 0, 0, 0.5, -1.1, 0.8, 0.9, 0, 0, 0, 0]$，\n  - 第 3 列 ($j=3$)：$[0, 0, 0, 0, 0, 0, 0, 0, 1.2, -0.6, 0.4, -1.0]$。\n\n- 用例 2（纠缠因子，单调解码器）：使用种子 $1$ 采样 $z \\sim \\mathcal{N}(0, I_3)$。设置 $v = B z + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_v^2 I_3)$，$\\sigma_v = 0.05$，种子 $11$，且\n  $$\n  B = \\begin{bmatrix}\n  0.7  -0.2  0.5 \\\\\n  0.4  0.9  -0.3 \\\\\n  -0.6  0.1  0.8\n  \\end{bmatrix}.\n  $$\n  对所有 $j \\in \\{1,2,3\\}$ 使用解码器非线性函数 $s_j(u) = \\tanh(u)$。使用一个密集的解码器权重矩阵 $W \\in \\mathbb{R}^{12 \\times 3}$，其独立元素使用种子 $123$ 从均值为 $0$、标准差为 $0.6$ 的正态分布中抽取生成。\n\n- 用例 3（解耦因子，混合单调与非单调解码器，含平坦神经元）：使用种子 $2$ 采样 $z \\sim \\mathcal{N}(0, I_3)$。设置 $v = z + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_v^2 I_3)$，$\\sigma_v = 0.02$，种子 $12$。使用解码器非线性函数 $s_1(u) = \\tanh(u)$，$s_2(u) = u^2$，$s_3(u) = \\tanh(u)$。使用按潜在维度列指定的权重矩阵 $W \\in \\mathbb{R}^{12 \\times 3}$，\n  - 第 1 列 ($j=1$)：$[1.0, -0.5, 0.8, 0.0, 0, 0, 0, 0, 0, 0, 0, 0]$，\n  - 第 2 列 ($j=2$)：$[0, 0, 0, 0, 1.0, 0.0, 1.2, 0.0, 0, 0, 0, 0]$，\n  - 第 3 列 ($j=3$)：$[0, 0, 0, 0, 0, 0, 0, 0, 0.3, -0.7, 0.0, 0.5]$。\n\n对于所有用例，使用偏置向量 $b = 0$。通过计算 $\\Theta = (Z^\\top Z + \\lambda I)^{-1} Z^\\top V$ 来实现岭回归，其中 $Z \\in \\mathbb{R}^{N \\times L}$ 且 $V \\in \\mathbb{R}^{N \\times K}$。评估如上定义的 $D$、$C$、$I$ 和 $M$。您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表的结果，每个用例一个子列表，顺序为 $[D,C,I,M]$。例如，格式必须与 $[[D_1,C_1,I_1,M_1],[D_2,C_2,I_2,M_2],[D_3,C_3,I_3,M_3]]$ 完全一样。所有返回的数字必须是十进制形式。此问题中不需要单位或角度。",
            "solution": "问题陈述已经过仔细审查，并被确定为**有效**。它在科学上基于潜变量模型和表示学习的原理，特别是与变分自编码器（VAEs）相关的概念。该问题是适定的，为获得唯一且可复现的解提供了所有必要的参数、定义和随机种子。语言是客观的，所需指标——潜在遍历单调性（$M$）和解耦-完整性-信息量（DCI）分数（$D$、$C$、$I$）——的定义在数学上是精确且自包含的。没有矛盾、歧义或事实错误。\n\n解决方案将通过首先为三个测试用例中的每一个构建特定的数据集和模型参数来实现。然后，对于每个用例，将执行两个主要的诊断函数：一个用于计算 DCI 分数，另一个用于计算单调性分数。\n\n**1. 解耦、完整性和信息量 (DCI) 指标**\n\nDCI 指标量化了潜在表示 $z \\in \\mathbb{R}^L$ 相对于一组真实生成因子 $v \\in \\mathbb{R}^K$ 的质量。这涉及训练一个线性模型来从 $z$ 预测 $v$ 并分析得到的回归系数。\n\n**1.1. 数据生成和回归**\n对于每个有 $N$ 个样本的测试用例，我们首先通过从 $\\mathcal{N}(0, I_L)$ 中抽取每个样本 $z_i$ 来生成潜在数据矩阵 $Z \\in \\mathbb{R}^{N \\times L}$。相应的真实因子矩阵 $V \\in \\mathbb{R}^{N \\times K}$ 根据特定用例的线性映射和噪声模型生成。\n\n然后拟合一个岭回归模型来从 $Z$ 预测 $V$。通过求解正则化最小二乘问题的正规方程找到回归系数 $\\Theta \\in \\mathbb{R}^{L \\times K}$：\n$$\n\\Theta = (Z^\\top Z + \\lambda I_L)^{-1} Z^\\top V\n$$\n其中 $\\lambda$ 是正则化参数，$I_L$ 是 $L \\times L$ 的单位矩阵。\n\n**1.2. 重要性矩阵**\n重要性矩阵 $R \\in \\mathbb{R}^{L \\times K}$ 通过取每个回归系数的绝对值来定义：\n$$\nR_{jk} = |\\Theta_{jk}|\n$$\n该矩阵量化了每个潜在维度 $j$ 和每个真实因子 $k$ 之间线性关系的强度。\n\n**1.3. 解耦 ($D$)**\n解耦度量了单个潜在维度对应于单个因子的程度。对于每个潜在维度 $j$，我们定义一个在因子 $k$ 上的概率分布 $p_j$：\n$$\np_{j,k} = \\frac{R_{jk}}{\\sum_{k'=1}^K R_{jk'}}\n$$\n如果 $\\sum_{k'} R_{jk'} = 0$，则潜在维度 $j$ 不具预测性，其解耦分数 $d_j$ 设置为 $0$。否则，每个潜在维度的解耦度由以下公式给出：\n$$\nd_j = 1 - \\frac{H(p_j)}{\\log K}\n$$\n其中 $H(p_j) = -\\sum_{k=1}^K p_{j,k} \\log p_{j,k}$ 是以自然单位计的香农熵。总体解耦分数 $D$ 是每个潜在维度分数的加权平均值，其中权重 $\\rho_j$ 与每个潜在维度的总重要性成正比：\n$$\n\\rho_j = \\frac{\\sum_{k=1}^K R_{jk}}{\\sum_{j'=1}^L\\sum_{k'=1}^K R_{j'k'}}, \\quad D = \\sum_{j=1}^L \\rho_j d_j\n$$\n\n**1.4. 完整性 ($C$)**\n完整性度量了关于单个因子的所有信息是否被单个潜在维度捕获。对于每个因子 $k$，我们定义一个在潜在维度 $j$ 上的概率分布 $q_k$：\n$$\nq_{k,j} = \\frac{R_{jk}}{\\sum_{j'=1}^L R_{j'k}}\n$$\n如果 $\\sum_{j'} R_{j'k} = 0$，则因子 $k$ 是不可预测的，其完整性分数 $c_k$ 设置为 $0$。否则，每个因子的完整性为：\n$$\nc_k = 1 - \\frac{H(q_k)}{\\log L}\n$$\n其中 $H(q_k) = -\\sum_{j=1}^L q_{k,j} \\log q_{k,j}$。总体完整性分数 $C$ 是每个因子分数的加权平均值，权重 $\\pi_k$ 与每个因子相关的总重要性成正比：\n$$\n\\pi_k = \\frac{\\sum_{j=1}^L R_{jk}}{\\sum_{j'=1}^L\\sum_{k'=1}^K R_{j'k'}}, \\quad C = \\sum_{k=1}^K \\pi_k c_k\n$$\n\n**1.5. 信息量 ($I$)**\n信息量度量了潜变量能多好地预测真实因子。使用回归预测 $\\hat{V} = Z \\Theta$，我们计算每个因子 $k$ 的归一化均方误差：\n$$\ne_k = \\frac{\\text{MSE}_k}{\\text{Var}(v_k)} = \\frac{\\frac{1}{N} \\sum_{i=1}^N (\\hat{v}_{i,k} - v_{i,k})^2}{\\text{Var}(v_k)}\n$$\n如果 $\\text{Var}(v_k)$ 在数值上为零，则用一个小的正常数代替以确保稳定性。总体信息量分数 $I$ 定义为：\n$$\nI = 1 - \\frac{1}{K} \\sum_{k=1}^K e_k\n$$\n这等同于所有因子的平均决定系数 ($R^2$)。\n\n**2. 潜在遍历单调性 ($M$)**\n\n该诊断评估了当单个潜在维度变化时，重建特征 $y \\in \\mathbb{R}^D$ 的单调变化程度。\n\n**2.1. 遍历和特征重建**\n对于每个潜在维度 $j \\in \\{1,\\dots,L\\}$，我们定义一个遍历路径 $\\gamma_j(t)$，其中第 $j$ 个分量在 $[-a, a]$ 内的 $T$ 个点的网格上变化，而所有其他分量保持为 $0$。然后使用解码器沿此路径计算重建特征：\n$$\ny(t) = W s(\\gamma_j(t))\n$$\n其中 $s(\\cdot)$ 是逐元素非线性函数，$W \\in \\mathbb{R}^{D \\times L}$ 是解码器权重矩阵。\n\n**2.2. 单调性评估**\n对于每个神经元 $d \\in \\{1,\\dots,D\\}$，我们评估其响应序列 $\\{y_d(t_m)\\}_{m=1}^T$。我们计算离散差分 $\\Delta_m = y_d(t_{m+1}) - y_d(t_m)$。当且仅当至少存在一个非零差分，并且所有非零差分的符号相同时，响应被定义为单调。因此，恒定响应（所有 $\\Delta_m = 0$）不被视为单调。\n潜在维度 $j$ 的单调性分数，记为 $M_j$，是其响应沿 $\\gamma_j$ 单调的 $D$ 个神经元所占的比例。最终分数 $M$ 是这些分数在所有潜在维度上的平均值：\n$$\nM_j = \\frac{1}{D} \\sum_{d=1}^D \\mathbb{I}(\\text{神经元 } d \\text{ 沿 } \\gamma_j \\text{ 是单调的}), \\quad M = \\frac{1}{L} \\sum_{j=1}^L M_j\n$$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n\n实现将通过为每个计算步骤定义 Python 函数，并将它们应用于问题中指定的三个测试用例的每个参数来进行。",
            "answer": "```python\nimport numpy as np\n\ndef _calculate_dci(Z, V, L, K, lambda_reg):\n    \"\"\"Calculates Disentanglement, Completeness, and Informativeness.\"\"\"\n    # 1. Ridge Regression\n    # Theta = (Z.T @ Z + lambda * I)^-1 @ Z.T @ V\n    ZT_Z = Z.T @ Z\n    reg_term = lambda_reg * np.identity(L)\n    try:\n        inv_term = np.linalg.inv(ZT_Z + reg_term)\n    except np.linalg.LinAlgError:\n         # Add jitter for singular matrix\n        inv_term = np.linalg.inv(ZT_Z + reg_term + 1e-6 * np.identity(L))\n\n    ZT_V = Z.T @ V\n    Theta = inv_term @ ZT_V\n\n    # 2. Importance Matrix\n    R = np.abs(Theta)\n    S = R.sum()\n\n    # Handle case where S is zero\n    if S == 0:\n        return 0.0, 0.0, 0.0\n\n    # 3. Disentanglement (D)\n    # Sum importances for each latent (row sums of R)\n    latent_importance = R.sum(axis=1)\n    \n    # Probabilities p_j(k) = R_jk / sum_k'(R_jk')\n    # Use broadcasting to divide each row of R by its sum\n    p_dist = R / (latent_importance[:, np.newaxis] + 1e-9)\n\n    # Entropy H(p_j)\n    # Handle p log p where p=0\n    log_p = np.log(p_dist + 1e-9)\n    entropy_d = -np.sum(p_dist * log_p, axis=1)\n\n    # Per-latent disentanglement d_j\n    # log(K) can be 0 if K=1, but K=3 here\n    d_scores = 1 - entropy_d / np.log(K)\n    # Set d_j = 0 for non-predictive latents\n    d_scores[latent_importance == 0] = 0\n\n    # Weighting rho_j and final score D\n    rho = latent_importance / S\n    D = np.sum(rho * d_scores)\n\n    # 4. Completeness (C)\n    # Sum importances for each factor (col sums of R)\n    factor_importance = R.sum(axis=0)\n\n    # Probabilities q_k(j) = R_jk / sum_j'(R_j'k)\n    q_dist = R / (factor_importance[np.newaxis, :] + 1e-9)\n\n    # Entropy H(q_k)\n    log_q = np.log(q_dist + 1e-9)\n    entropy_c = -np.sum(q_dist * log_q, axis=0)\n\n    # Per-factor completeness c_k\n    # log(L) can be 0 if L=1, but L=3 here\n    c_scores = 1 - entropy_c / np.log(L)\n    # Set c_k = 0 for non-predictable factors\n    c_scores[factor_importance == 0] = 0\n\n    # Weighting pi_k and final score C\n    pi_k = factor_importance / S\n    C = np.sum(pi_k * c_scores)\n\n    # 5. Informativeness (I)\n    V_hat = Z @ Theta\n    mse_k = np.mean((V - V_hat)**2, axis=0)\n    \n    # Use ddof=0 for population variance to match np.mean\n    var_k = np.var(V, axis=0, ddof=0)\n    \n    # Avoid division by zero\n    var_k_safe = np.maximum(var_k, 1e-9)\n    \n    error_k = mse_k / var_k_safe\n    I = 1 - np.mean(error_k)\n\n    return D, C, I\n\ndef _calculate_monotonicity(W, s_funcs, L, D, a, T):\n    \"\"\"Calculates Latent Traversal Monotonicity.\"\"\"\n    t_grid = np.linspace(-a, a, T)\n    \n    total_monotonicity_fraction = 0.0\n    \n    for j in range(L):\n        # Create traversal paths\n        gamma = np.zeros((T, L))\n        gamma[:, j] = t_grid\n        \n        # Apply nonlinearities\n        s_gamma = np.zeros_like(gamma)\n        for i in range(L):\n            s_gamma[:, i] = s_funcs[i](gamma[:, i])\n            \n        # Reconstruct features: Y = s(gamma) @ W.T\n        # W is D x L, s_gamma is T x L. Result is T x D\n        Y = s_gamma @ W.T\n        \n        # Check monotonicity for each neuron\n        monotonic_neuron_count = 0\n        for d in range(D):\n            y_d_sequence = Y[:, d]\n            diffs = np.diff(y_d_sequence)\n            \n            # Filter out numerically zero differences\n            nonzero_diffs = diffs[np.abs(diffs) > 1e-9]\n            \n            # Condition: at least one non-zero diff, all of the same sign\n            is_monotonic = False\n            if len(nonzero_diffs) > 0:\n                if np.all(nonzero_diffs > 0) or np.all(nonzero_diffs < 0):\n                    is_monotonic = True\n            \n            if is_monotonic:\n                monotonic_neuron_count += 1\n                \n        # Fraction for current latent j\n        total_monotonicity_fraction += monotonic_neuron_count / D\n        \n    # Average across all latents\n    M = total_monotonicity_fraction / L\n    return M\n\ndef solve_case(params):\n    \"\"\"Solves a single test case.\"\"\"\n    L, K, D, N = params['L'], params['K'], params['D'], params['N']\n    lambda_reg = params['lambda_reg']\n    a, T = params['a'], params['T']\n    \n    # Generate data\n    rng_z = np.random.default_rng(params['seed_z'])\n    Z = rng_z.normal(size=(N, L))\n    \n    rng_v = np.random.default_rng(params['seed_v'])\n    epsilon = rng_v.normal(loc=0, scale=params['sigma_v'], size=(N, K))\n    \n    if 'B' in params:\n        V = Z @ params['B'].T + epsilon\n    else:\n        # Assuming K=L for v=z+eps\n        V = Z + epsilon\n        \n    # Calculate DCI scores\n    D_score, C_score, I_score = _calculate_dci(Z, V, L, K, lambda_reg)\n    \n    # Calculate Monotonicity\n    s_funcs = params['s_funcs']\n    W = params['W']\n    M_score = _calculate_monotonicity(W, s_funcs, L, D, a, T)\n\n    # Format output for this case\n    return [D_score, C_score, I_score, M_score]\n\ndef solve():\n    \"\"\"Main function to run all test cases.\"\"\"\n    \n    # Shared parameters\n    common_params = {\n        'L': 3, 'K': 3, 'D': 12, 'N': 800,\n        'lambda_reg': 1e-3, 'a': 3, 'T': 101\n    }\n\n    # Case 1\n    W1 = np.zeros((12, 3))\n    W1[0:4, 0] = [1.0, 0.7, -0.9, 1.3]\n    W1[4:8, 1] = [0.5, -1.1, 0.8, 0.9]\n    W1[8:12, 2] = [1.2, -0.6, 0.4, -1.0]\n    case1_params = {\n        **common_params, 'seed_z': 0, 'seed_v': 10, 'sigma_v': 0.05,\n        's_funcs': [np.tanh, np.tanh, np.tanh], 'W': W1\n    }\n\n    # Case 2\n    rng_w2 = np.random.default_rng(123)\n    W2 = rng_w2.normal(loc=0, scale=0.6, size=(12, 3))\n    B2 = np.array([[0.7, -0.2, 0.5], [0.4, 0.9, -0.3], [-0.6, 0.1, 0.8]])\n    case2_params = {\n        **common_params, 'seed_z': 1, 'seed_v': 11, 'sigma_v': 0.05,\n        'B': B2, 's_funcs': [np.tanh, np.tanh, np.tanh], 'W': W2\n    }\n\n    # Case 3\n    W3 = np.zeros((12, 3))\n    W3[0:4, 0] = [1.0, -0.5, 0.8, 0.0]\n    W3[4:8, 1] = [1.0, 0.0, 1.2, 0.0]\n    W3[8:12, 2] = [0.3, -0.7, 0.0, 0.5]\n    case3_params = {\n        **common_params, 'seed_z': 2, 'seed_v': 12, 'sigma_v': 0.02,\n        's_funcs': [np.tanh, lambda u: u**2, np.tanh], 'W': W3\n    }\n\n    test_cases = [case1_params, case2_params, case3_params]\n    \n    results = [solve_case(params) for params in test_cases]\n    \n    # Format the final output string\n    # str(list) automatically adds spaces, which might be an issue. Let's format manually.\n    result_strs = []\n    for res_list in results:\n        res_list_str = '[' + ','.join(f'{x:.8f}' for x in res_list) + ']'\n        result_strs.append(res_list_str)\n\n    print(f\"[{','.join(result_strs)}]\")\n\n# Execute the solver\nsolve()\n\n```"
        }
    ]
}