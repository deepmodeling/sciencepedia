{
    "hands_on_practices": [
        {
            "introduction": "A core principle of the Variational Autoencoder ($VAE$) is the balance between accurately reconstructing data and maintaining a well-structured latent space. This practice focuses on the latter, diving into the Kullback-Leibler ($KL$) divergence term of the VAE's objective function. By deriving and computing this term for a Gaussian encoder, you will gain a first-principles understanding of how VAEs regularize their latent representations to prevent overfitting and encourage generalization. ",
            "id": "4139921",
            "problem": "A Variational Autoencoder (VAE) is trained to model low-dimensional latent variables that capture coordinated neural activity in a population of simultaneously recorded neurons. The encoder produces, for each observed neural activity vector, a diagonal Gaussian approximate posterior of the form $q(\\boldsymbol{z}\\mid \\boldsymbol{x})=\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ where $\\boldsymbol{\\Sigma}=\\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{k}^{2})$ and the variances are parameterized elementwise by $\\log \\sigma_{i}^{2}$. The prior is the standard normal $p(\\boldsymbol{z})=\\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I})$. For a particular held-out trial from a calcium imaging dataset, the encoder outputs $k=4$ latent dimensions with\n$\\boldsymbol{\\mu}=(0.6,\\,-0.8,\\,0.1,\\,-0.3)^{\\top}$ and $\\log \\boldsymbol{\\sigma}^{2}=(-0.2,\\,0.7,\\,-1.0,\\,-0.5)^{\\top}$.\nStarting only from the definition of Kullback–Leibler divergence $ \\mathrm{KL}(q\\parallel p)=\\mathbb{E}_{q}[\\ln q(\\boldsymbol{z})-\\ln p(\\boldsymbol{z})] $ and the probability density function of a multivariate Gaussian, derive an analytic expression for $\\mathrm{KL}(q\\parallel p)$ in terms of $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ specialized to this diagonal case, and then evaluate it numerically for the given values. Report the final numerical value in nats and round your answer to $4$ significant figures.",
            "solution": "The problem requires the derivation of an analytic expression for the Kullback-Leibler (KL) divergence between a diagonal-covariance Gaussian distribution $q(\\boldsymbol{z})$ and a standard normal distribution $p(\\boldsymbol{z})$, followed by a numerical evaluation for given parameters.\n\n### Step 1: Analytical Derivation of the KL Divergence\n\nWe are given the definition of the KL divergence:\n$$\n\\mathrm{KL}(q\\parallel p) = \\mathbb{E}_{q}[\\ln q(\\boldsymbol{z}) - \\ln p(\\boldsymbol{z})] = \\mathbb{E}_{q}[\\ln q(\\boldsymbol{z})] - \\mathbb{E}_{q}[\\ln p(\\boldsymbol{z})]\n$$\nThe approximate posterior $q(\\boldsymbol{z})$ is a $k$-dimensional multivariate Gaussian distribution, $q(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$. Its probability density function (PDF) is:\n$$\nq(\\boldsymbol{z}) = \\frac{1}{(2\\pi)^{k/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\\right)\n$$\nThe natural logarithm of this PDF is:\n$$\n\\ln q(\\boldsymbol{z}) = -\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2}(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\n$$\nThe prior $p(\\boldsymbol{z})$ is a standard $k$-dimensional normal distribution, $p(\\boldsymbol{z})=\\mathcal{N}(\\boldsymbol{z} \\mid \\boldsymbol{0}, \\boldsymbol{I})$, where $\\boldsymbol{I}$ is the identity matrix. Its PDF is:\n$$\np(\\boldsymbol{z}) = \\frac{1}{(2\\pi)^{k/2} |\\boldsymbol{I}|^{1/2}} \\exp\\left(-\\frac{1}{2}\\boldsymbol{z}^{\\top}\\boldsymbol{I}^{-1}\\boldsymbol{z}\\right) = \\frac{1}{(2\\pi)^{k/2}} \\exp\\left(-\\frac{1}{2}\\boldsymbol{z}^{\\top}\\boldsymbol{z}\\right)\n$$\nThe natural logarithm of the prior's PDF is:\n$$\n\\ln p(\\boldsymbol{z}) = -\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\boldsymbol{z}^{\\top}\\boldsymbol{z}\n$$\nNow, we compute the expectation of these log-PDFs with respect to $q(\\boldsymbol{z})$.\n\nFirst, for $\\mathbb{E}_{q}[\\ln q(\\boldsymbol{z})]$:\n$$\n\\mathbb{E}_{q}[\\ln q(\\boldsymbol{z})] = \\mathbb{E}_{q}\\left[-\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2}(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\\right]\n$$\nThe first two terms are constants with respect to $\\boldsymbol{z}$, so their expectation is just themselves. We only need to evaluate the expectation of the quadratic form. Using the linearity of expectation and the trace operator trick:\n\\begin{align*} \\mathbb{E}_{q}\\left[(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\\right] = \\mathbb{E}_{q}\\left[\\mathrm{tr}\\left((\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\\right)\\right] \\\\ = \\mathbb{E}_{q}\\left[\\mathrm{tr}\\left(\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\right)\\right] \\\\ = \\mathrm{tr}\\left(\\boldsymbol{\\Sigma}^{-1}\\mathbb{E}_{q}\\left[(\\boldsymbol{z}-\\boldsymbol{\\mu})(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\right]\\right)\\end{align*}\nBy definition, $\\mathbb{E}_{q}\\left[(\\boldsymbol{z}-\\boldsymbol{\\mu})(\\boldsymbol{z}-\\boldsymbol{\\mu})^{\\top}\\right]$ is the covariance matrix of $q(\\boldsymbol{z})$, which is $\\boldsymbol{\\Sigma}$.\n$$\n\\mathrm{tr}\\left(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\Sigma}\\right) = \\mathrm{tr}(\\boldsymbol{I}_{k}) = k\n$$\nThus, the expectation of the log-posterior is:\n$$\n\\mathbb{E}_{q}[\\ln q(\\boldsymbol{z})] = -\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{k}{2}\n$$\nNext, for $\\mathbb{E}_{q}[\\ln p(\\boldsymbol{z})]$:\n$$\n\\mathbb{E}_{q}[\\ln p(\\boldsymbol{z})] = \\mathbb{E}_{q}\\left[-\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\boldsymbol{z}^{\\top}\\boldsymbol{z}\\right] = -\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\mathbb{E}_{q}[\\boldsymbol{z}^{\\top}\\boldsymbol{z}]\n$$\nTo evaluate $\\mathbb{E}_{q}[\\boldsymbol{z}^{\\top}\\boldsymbol{z}]$, we use the property of covariance matrices: $\\mathrm{Cov}_{q}(\\boldsymbol{z}) = \\mathbb{E}_{q}[\\boldsymbol{z}\\boldsymbol{z}^{\\top}] - \\mathbb{E}_{q}[\\boldsymbol{z}]\\mathbb{E}_{q}[\\boldsymbol{z}]^{\\top}$.\nFor $q(\\boldsymbol{z})$, $\\mathrm{Cov}_{q}(\\boldsymbol{z}) = \\boldsymbol{\\Sigma}$ and $\\mathbb{E}_{q}[\\boldsymbol{z}] = \\boldsymbol{\\mu}$. Therefore:\n$$\n\\boldsymbol{\\Sigma} = \\mathbb{E}_{q}[\\boldsymbol{z}\\boldsymbol{z}^{\\top}] - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top} \\implies \\mathbb{E}_{q}[\\boldsymbol{z}\\boldsymbol{z}^{\\top}] = \\boldsymbol{\\Sigma} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top}\n$$\nThe term $\\boldsymbol{z}^{\\top}\\boldsymbol{z}$ is a scalar, which is the trace of the outer product $\\boldsymbol{z}\\boldsymbol{z}^{\\top}$.\n$$\n\\mathbb{E}_{q}[\\boldsymbol{z}^{\\top}\\boldsymbol{z}] = \\mathbb{E}_{q}[\\mathrm{tr}(\\boldsymbol{z}\\boldsymbol{z}^{\\top})] = \\mathrm{tr}(\\mathbb{E}_{q}[\\boldsymbol{z}\\boldsymbol{z}^{\\top}]) = \\mathrm{tr}(\\boldsymbol{\\Sigma} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top}) = \\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\mathrm{tr}(\\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top})\n$$\nSince $\\mathrm{tr}(\\boldsymbol{\\mu}\\boldsymbol{\\mu}^{\\top}) = \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu}$, we have:\n$$\n\\mathbb{E}_{q}[\\boldsymbol{z}^{\\top}\\boldsymbol{z}] = \\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu}\n$$\nSo, the expectation of the log-prior is:\n$$\n\\mathbb{E}_{q}[\\ln p(\\boldsymbol{z})] = -\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}(\\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu})\n$$\nSubstituting these expectations into the KL divergence formula:\n\\begin{align*}\n\\mathrm{KL}(q\\parallel p) = \\left(-\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{k}{2}\\right) - \\left(-\\frac{k}{2}\\ln(2\\pi) - \\frac{1}{2}(\\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu})\\right) \\\\\n= -\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{k}{2} + \\frac{1}{2}\\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\frac{1}{2}\\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu} \\\\\n= \\frac{1}{2}\\left( \\mathrm{tr}(\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu} - k - \\ln|\\boldsymbol{\\Sigma}| \\right)\n\\end{align*}\nThis is the general formula. For the case of a diagonal covariance matrix $\\boldsymbol{\\Sigma} = \\mathrm{diag}(\\sigma_{1}^{2}, \\dots, \\sigma_{k}^{2})$, we have:\n-   $\\mathrm{tr}(\\boldsymbol{\\Sigma}) = \\sum_{i=1}^{k} \\sigma_i^2$\n-   $|\\boldsymbol{\\Sigma}| = \\prod_{i=1}^{k} \\sigma_i^2$\n-   $\\ln|\\boldsymbol{\\Sigma}| = \\sum_{i=1}^{k} \\ln(\\sigma_i^2)$\nAlso, $\\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\mu} = \\sum_{i=1}^{k} \\mu_i^2$. Substituting these into the formula:\n$$\n\\mathrm{KL}(q\\parallel p) = \\frac{1}{2}\\left( \\sum_{i=1}^{k} \\sigma_i^2 + \\sum_{i=1}^{k} \\mu_i^2 - k - \\sum_{i=1}^{k} \\ln(\\sigma_i^2) \\right)\n$$\nRearranging the terms to group by dimension gives the final analytical expression:\n$$\n\\mathrm{KL}(q\\parallel p) = \\frac{1}{2} \\sum_{i=1}^{k} \\left( \\sigma_i^2 + \\mu_i^2 - 1 - \\ln(\\sigma_i^2) \\right)\n$$\n\n### Step 2: Numerical Evaluation\n\nWe are given the following values for a latent space of dimension $k = 4$:\n-   $\\boldsymbol{\\mu}=(0.6,\\,-0.8,\\,0.1,\\,-0.3)^{\\top}$\n-   $\\log \\boldsymbol{\\sigma}^{2}=(-0.2,\\,0.7,\\,-1.0,\\,-0.5)^{\\top}$, where the vector components are $\\ln(\\sigma_i^2)$.\n\nLet's compute the components of the sum for each dimension $i \\in \\{1, 2, 3, 4\\}$. Note that $\\sigma_i^2 = \\exp(\\ln(\\sigma_i^2))$.\n\nFor $i=1$:\n-   $\\mu_1 = 0.6 \\implies \\mu_1^2 = 0.36$\n-   $\\ln(\\sigma_1^2) = -0.2 \\implies \\sigma_1^2 = \\exp(-0.2)$\n-   Term 1: $\\exp(-0.2) + 0.36 - 1 - (-0.2) = \\exp(-0.2) - 0.44$\n\nFor $i=2$:\n-   $\\mu_2 = -0.8 \\implies \\mu_2^2 = 0.64$\n-   $\\ln(\\sigma_2^2) = 0.7 \\implies \\sigma_2^2 = \\exp(0.7)$\n-   Term 2: $\\exp(0.7) + 0.64 - 1 - 0.7 = \\exp(0.7) - 1.06$\n\nFor $i=3$:\n-   $\\mu_3 = 0.1 \\implies \\mu_3^2 = 0.01$\n-   $\\ln(\\sigma_3^2) = -1.0 \\implies \\sigma_3^2 = \\exp(-1.0)$\n-   Term 3: $\\exp(-1.0) + 0.01 - 1 - (-1.0) = \\exp(-1.0) + 0.01$\n\nFor $i=4$:\n-   $\\mu_4 = -0.3 \\implies \\mu_4^2 = 0.09$\n-   $\\ln(\\sigma_4^2) = -0.5 \\implies \\sigma_4^2 = \\exp(-0.5)$\n-   Term 4: $\\exp(-0.5) + 0.09 - 1 - (-0.5) = \\exp(-0.5) - 0.41$\n\nThe sum is:\n\\begin{align*}\n\\sum_{i=1}^{4} \\left( \\sigma_i^2 + \\mu_i^2 - 1 - \\ln(\\sigma_i^2) \\right) = (\\exp(-0.2) - 0.44) + (\\exp(0.7) - 1.06) + (\\exp(-1.0) + 0.01) + (\\exp(-0.5) - 0.41) \\\\\n= \\sum_{i=1}^4 \\sigma_i^2 + \\sum_{i=1}^4 \\mu_i^2 - \\sum_{i=1}^4 1 - \\sum_{i=1}^4 \\ln(\\sigma_i^2) \\\\\n\\end{align*}\nLet's calculate the sums of the components:\n-   $\\sum \\mu_i^2 = (0.6)^2 + (-0.8)^2 + (0.1)^2 + (-0.3)^2 = 0.36 + 0.64 + 0.01 + 0.09 = 1.1$\n-   $\\sum \\ln(\\sigma_i^2) = -0.2 + 0.7 - 1.0 - 0.5 = -1.0$\n-   $\\sum \\sigma_i^2 = \\exp(-0.2) + \\exp(0.7) + \\exp(-1.0) + \\exp(-0.5)$\n    $\\approx 0.81873075 + 2.01375271 + 0.36787944 + 0.60653066 \\approx 3.80689356$\n-   $k = 4$\n\nThe total sum inside the parenthesis is:\n$$\n\\sum (\\dots) = 3.80689356 + 1.1 - 4 - (-1.0) = 3.80689356 + 1.1 - 4 + 1.0 = 1.90689356\n$$\nFinally, the KL divergence is:\n$$\n\\mathrm{KL}(q\\parallel p) = \\frac{1}{2} \\times 1.90689356 \\approx 0.95344678\n$$\nRounding to $4$ significant figures, we get $0.9534$. The units are nats, as the natural logarithm was used in the definition.",
            "answer": "$$\\boxed{0.9534}$$"
        },
        {
            "introduction": "While many $VAE$ examples use a simple squared-error reconstruction loss, this implicitly assumes Gaussian data. Neuroscience data, such as neural spike counts, often follows different statistical distributions. This exercise demonstrates how to adapt the $VAE$ framework by deriving the reconstruction loss term for a Poisson likelihood, a model well-suited for count data.  Mastering this allows you to build more principled and accurate generative models for specific biological datasets.",
            "id": "4139965",
            "problem": "Consider count-valued neural data modeled as conditionally independent Poisson observations given a latent code. Let the observed counts be a vector $x \\in \\mathbb{N}^{D}$ across $D$ neurons and the latent variable be $z \\in \\mathbb{R}^{K}$. Assume the likelihood factorizes as $p(x \\mid z) = \\prod_{d=1}^{D} p(x_{d} \\mid z)$, where each $p(x_{d} \\mid z)$ is a $\\mathrm{Poisson}$ distribution with rate $\\lambda_{d}(z)$. The reconstruction term in the Evidence Lower Bound for a Variational Autoencoder (VAE) is defined as the expected negative log-likelihood $\\mathbb{E}_{q(z \\mid x)}\\left[-\\ln p(x \\mid z)\\right]$, where $q(z \\mid x)$ is the variational posterior. Starting from the definition of the Poisson probability mass function and basic properties of expectations, derive the general form of the reconstruction term $\\mathbb{E}_{q(z \\mid x)}\\left[-\\ln p(x \\mid z)\\right]$ for a Poisson likelihood. Then, under a linear decoder with an exponential nonlinearity for the rates, $\\lambda_{d}(z) = \\exp\\left(b_{d} + w_{d}^{\\top} z\\right)$ for $d \\in \\{1,\\dots,D\\}$ where $w_{d} \\in \\mathbb{R}^{K}$ and $b_{d} \\in \\mathbb{R}$, and assuming a multivariate normal variational posterior $q(z \\mid x) = \\mathcal{N}(\\mu, \\Sigma)$ with mean $\\mu \\in \\mathbb{R}^{K}$ and covariance $\\Sigma \\in \\mathbb{R}^{K \\times K}$ that is symmetric positive semidefinite, provide closed-form analytic expressions for the two expectations that appear in the reconstruction term. Your final answer should be a single closed-form analytic expression for $\\mathbb{E}_{q(z \\mid x)}\\left[-\\ln p(x \\mid z)\\right]$ in terms of $x$, $\\{w_{d}\\}_{d=1}^{D}$, $\\{b_{d}\\}_{d=1}^{D}$, $\\mu$, and $\\Sigma$. No rounding is required.",
            "solution": "The goal is to derive the reconstruction term for a Poisson likelihood under a variational posterior. We proceed from fundamental definitions.\n\nFirst, recall the definition of the Poisson probability mass function. For a single count $x_{d} \\in \\mathbb{N}$ with rate $\\lambda_{d}(z)  0$, the likelihood is\n$$\np(x_{d} \\mid z) = \\frac{\\lambda_{d}(z)^{x_{d}} \\exp\\left(-\\lambda_{d}(z)\\right)}{x_{d}!}.\n$$\nUnder conditional independence across $d \\in \\{1,\\dots,D\\}$,\n$$\np(x \\mid z) = \\prod_{d=1}^{D} p(x_{d} \\mid z) = \\prod_{d=1}^{D} \\frac{\\lambda_{d}(z)^{x_{d}} \\exp\\left(-\\lambda_{d}(z)\\right)}{x_{d}!}.\n$$\nTaking the natural logarithm and the negative sign, the negative log-likelihood decomposes across dimensions:\n$$\n-\\ln p(x \\mid z) = \\sum_{d=1}^{D} \\left[ \\lambda_{d}(z) - x_{d} \\ln \\lambda_{d}(z) + \\ln\\left(x_{d}!\\right) \\right].\n$$\nThe reconstruction term is the expectation of this quantity under the variational posterior $q(z \\mid x)$:\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[-\\ln p(x \\mid z)\\right] = \\sum_{d=1}^{D} \\left[ \\mathbb{E}_{q(z \\mid x)}\\left[\\lambda_{d}(z)\\right] - x_{d} \\,\\mathbb{E}_{q(z \\mid x)}\\left[\\ln \\lambda_{d}(z)\\right] + \\ln\\left(x_{d}!\\right) \\right].\n$$\nThus, the problem reduces to computing $\\mathbb{E}_{q(z \\mid x)}\\left[\\lambda_{d}(z)\\right]$ and $\\mathbb{E}_{q(z \\mid x)}\\left[\\ln \\lambda_{d}(z)\\right]$ for each $d$.\n\nUnder the specified linear decoder with an exponential nonlinearity,\n$$\n\\lambda_{d}(z) = \\exp\\left(b_{d} + w_{d}^{\\top} z\\right),\n$$\nand the variational posterior is multivariate normal,\n$$\nq(z \\mid x) = \\mathcal{N}(\\mu, \\Sigma),\n$$\nwhere $\\mu \\in \\mathbb{R}^{K}$ and $\\Sigma \\in \\mathbb{R}^{K \\times K}$ is symmetric positive semidefinite.\n\nWe first compute $\\mathbb{E}_{q(z \\mid x)}\\left[\\ln \\lambda_{d}(z)\\right]$. Using the logarithm of $\\lambda_{d}(z)$,\n$$\n\\ln \\lambda_{d}(z) = b_{d} + w_{d}^{\\top} z,\n$$\nand linearity of expectation,\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[\\ln \\lambda_{d}(z)\\right] = b_{d} + w_{d}^{\\top} \\,\\mathbb{E}_{q(z \\mid x)}[z] = b_{d} + w_{d}^{\\top} \\mu.\n$$\n\nNext, we compute $\\mathbb{E}_{q(z \\mid x)}\\left[\\lambda_{d}(z)\\right] = \\mathbb{E}_{q(z \\mid x)}\\left[\\exp\\left(b_{d} + w_{d}^{\\top} z\\right)\\right]$. By properties of the multivariate normal distribution, the moment generating function of a linear form $w_{d}^{\\top} z$ under $z \\sim \\mathcal{N}(\\mu, \\Sigma)$ implies\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[\\exp\\left(w_{d}^{\\top} z\\right)\\right] = \\exp\\left(w_{d}^{\\top} \\mu + \\frac{1}{2} \\, w_{d}^{\\top} \\Sigma \\, w_{d}\\right).\n$$\nTherefore,\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[\\lambda_{d}(z)\\right] = \\exp\\left(b_{d}\\right) \\,\\mathbb{E}_{q(z \\mid x)}\\left[\\exp\\left(w_{d}^{\\top} z\\right)\\right] = \\exp\\left(b_{d} + w_{d}^{\\top} \\mu + \\frac{1}{2} \\, w_{d}^{\\top} \\Sigma \\, w_{d}\\right).\n$$\n\nSubstituting these expectations into the decomposition of the reconstruction term yields\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[-\\ln p(x \\mid z)\\right]\n= \\sum_{d=1}^{D} \\left[\n\\exp\\left(b_{d} + w_{d}^{\\top} \\mu + \\frac{1}{2} \\, w_{d}^{\\top} \\Sigma \\, w_{d}\\right)\n- x_{d} \\left(b_{d} + w_{d}^{\\top} \\mu\\right)\n+ \\ln\\left(x_{d}!\\right)\n\\right].\n$$\nThis is a closed-form analytic expression in terms of $x$, $\\{w_{d}\\}_{d=1}^{D}$, $\\{b_{d}\\}_{d=1}^{D}$, $\\mu$, and $\\Sigma$. It completes the derivation of the reconstruction term for a Poisson likelihood with an exponential-linear decoder under a multivariate normal variational posterior.",
            "answer": "$$\\boxed{\\sum_{d=1}^{D}\\left[\\exp\\left(b_{d}+w_{d}^{\\top}\\mu+\\frac{1}{2}\\,w_{d}^{\\top}\\Sigma\\,w_{d}\\right)-x_{d}\\left(b_{d}+w_{d}^{\\top}\\mu\\right)+\\ln\\!\\left(x_{d}!\\right)\\right]}$$"
        },
        {
            "introduction": "A significant challenge in modern neuroscience is integrating information from multiple experimental modalities, especially in the presence of missing data. This practice explores how the $VAE$ framework can be extended to elegantly solve this problem through a multimodal, product-of-experts approach. You will analyze how the model's posterior uncertainty adapts when one data modality is dropped, providing a quantitative understanding of how VAEs can perform robust inference and data fusion. ",
            "id": "4139982",
            "problem": "Consider a multimodal generative model for neural recordings with two observation modalities: $x_A$ (for example, electrophysiology features) and $x_B$ (for example, calcium imaging features). Let the latent variable be $z \\in \\mathbb{R}^k$. The generative assumptions are as follows: the prior is $p(z) = \\mathcal{N}(0, \\Sigma_0)$, and the conditionals are independent given $z$, with $p(x_A \\mid z) = \\mathcal{N}(W_A z + b_A, \\Sigma_A)$ and $p(x_B \\mid z) = \\mathcal{N}(W_B z + b_B, \\Sigma_B)$. Assume $\\Sigma_0$, $\\Sigma_A$, and $\\Sigma_B$ are positive definite covariance matrices (for simplicity in computation, they are diagonal in the test suite below), and $W_A$, $W_B$ are known observation matrices.\n\nTraining a Variational Autoencoder (VAE) with modality dropout means that during optimization of the Evidence Lower Bound (ELBO), one randomly drops modalities with a fixed probability $d \\in (0,1)$, and the encoder must produce a valid approximate posterior $q(z \\mid x_S)$ for any subset $S \\subseteq \\{A, B\\}$ of observed modalities. A principled encoder design for linear-Gaussian models uses a product-of-experts structure, which yields a Gaussian posterior whose precision equals the sum of the prior precision and the experts’ precisions contributed by observed modalities. In the exact linear-Gaussian setting, the true posterior $p(z \\mid x_S)$ is Gaussian with covariance\n$$\n\\Sigma_{\\text{post}}(S) \\;=\\; \\left( \\Sigma_0^{-1} \\;+\\; \\sum_{i \\in S} W_i^\\top \\Sigma_i^{-1} W_i \\right)^{-1}.\n$$\nWhen only one modality is observed, the posterior variance should adapt by increasing relative to the both-modalities case in proportion to the information lost.\n\nYour task is to implement a program that, for a given test suite of parameters, computes an “adaptation factor” when only one modality is observed, defined as\n$$\n\\alpha(S_{\\text{single}}) \\;=\\; \\frac{\\operatorname{tr}\\!\\left(\\Sigma_{\\text{post}}(S_{\\text{single}})\\right)}{\\operatorname{tr}\\!\\left(\\Sigma_{\\text{post}}(\\{A,B\\})\\right)}.\n$$\nThe trace $\\operatorname{tr}(\\cdot)$ serves as a scalar summary of posterior variance. Note that the modality dropout probability $d$ does not explicitly appear in the exact posterior covariance for a given observed subset; it is a training device ensuring the encoder can handle missing modalities. Your program must compute $\\alpha(S_{\\text{single}})$ for the specified test cases.\n\nUse the following test suite with latent dimension $k=2$ and the provided matrices:\n\n- Test Case $1$ (happy path, only $A$ observed):\n  - Prior covariance $\\,\\Sigma_0 = \\operatorname{diag}(1.0, 1.0)$.\n  - Modality $A$: $\\,W_A = \\begin{bmatrix} 1.0  0.0 \\\\ 0.5  0.6 \\\\ 0.0  1.0 \\end{bmatrix}$, $\\,\\Sigma_A = \\operatorname{diag}(0.2, 0.2, 0.2)$.\n  - Modality $B$: $\\,W_B = \\begin{bmatrix} 0.4  0.3 \\\\ 0.7  -0.1 \\end{bmatrix}$, $\\,\\Sigma_B = \\operatorname{diag}(0.5, 0.3)$.\n\n- Test Case $2$ (happy path, only $B$ observed):\n  - Same $\\,\\Sigma_0$, $\\,W_A$, $\\,\\Sigma_A$, $\\,W_B$, $\\,\\Sigma_B$ as Test Case $1$.\n\n- Test Case $3$ (boundary: extremely noisy $B$, only $B$ observed):\n  - Prior covariance $\\,\\Sigma_0 = \\operatorname{diag}(1.0, 1.0)$.\n  - Modality $A$: $\\,W_A = \\begin{bmatrix} 1.0  0.0 \\\\ 0.5  0.6 \\\\ 0.0  1.0 \\end{bmatrix}$, $\\,\\Sigma_A = \\operatorname{diag}(0.2, 0.2, 0.2)$.\n  - Modality $B$: $\\,W_B = \\begin{bmatrix} 0.4  0.3 \\\\ 0.7  -0.1 \\end{bmatrix}$, $\\,\\Sigma_B = \\operatorname{diag}(1000.0, 1000.0)$.\n\n- Test Case $4$ (edge case: uninformative $B$, only $B$ observed):\n  - Prior covariance $\\,\\Sigma_0 = \\operatorname{diag}(0.5, 0.5)$.\n  - Modality $A$: $\\,W_A = \\begin{bmatrix} 0.9  0.1 \\\\ 0.4  0.7 \\\\ 0.0  0.5 \\end{bmatrix}$, $\\,\\Sigma_A = \\operatorname{diag}(0.3, 0.3, 0.3)$.\n  - Modality $B$: $\\,W_B = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}$, $\\,\\Sigma_B = \\operatorname{diag}(1.0, 1.0)$.\n\n- Test Case $5$ (anisotropic prior, only $A$ observed):\n  - Prior covariance $\\,\\Sigma_0 = \\operatorname{diag}(0.1, 2.0)$.\n  - Modality $A$: $\\,W_A = \\begin{bmatrix} 1.2  -0.3 \\\\ 0.0  0.8 \\\\ 0.5  0.0 \\end{bmatrix}$, $\\,\\Sigma_A = \\operatorname{diag}(0.15, 0.4, 0.25)$.\n  - Modality $B$: $\\,W_B = \\begin{bmatrix} 0.2  0.1 \\\\ 0.3  0.5 \\end{bmatrix}$, $\\,\\Sigma_B = \\operatorname{diag}(0.6, 0.6)$.\n\nFor each test case, compute:\n- The precision contribution of each modality as $\\,J_i = W_i^\\top \\Sigma_i^{-1} W_i$.\n- The prior precision $\\,J_0 = \\Sigma_0^{-1}$.\n- The posterior covariance for the observed subset $\\,\\Sigma_{\\text{post}}(S)$ using the formula above.\n- The posterior covariance for both modalities $\\,\\Sigma_{\\text{post}}(\\{A,B\\})$.\n- The adaptation factor $\\,\\alpha(S_{\\text{single}})$ as defined.\n\nYour program should produce a single line of output containing the adaptation factors for Test Cases $1$ through $5$ as a comma-separated list enclosed in square brackets (for example, $[a_1,a_2,a_3,a_4,a_5]$). The outputs must be floating-point numbers. No physical units or angles are involved, and results should be presented as raw decimal floats without percentage signs.",
            "solution": "The problem asks us to compute an adaptation factor, $\\alpha(S_{\\text{single}})$, which quantifies the increase in posterior variance when a modality is dropped in a multimodal generative model. The model is of the linear-Gaussian form, a standard and well-understood framework in statistical modeling.\n\nThe generative process is defined as:\n1.  A latent variable $z \\in \\mathbb{R}^k$ is drawn from a prior distribution: $p(z) = \\mathcal{N}(z \\mid 0, \\Sigma_0)$.\n2.  Observed data for each modality $i \\in \\{A, B\\}$ are generated independently conditioned on $z$: $p(x_i \\mid z) = \\mathcal{N}(x_i \\mid W_i z + b_i, \\Sigma_i)$.\n\nOur goal is to determine the posterior distribution over the latent variable, $p(z \\mid x_S)$, given observations from a subset of modalities $S \\subseteq \\{A, B\\}$. According to Bayes' theorem, the posterior is proportional to the product of the prior and the likelihoods of the observed modalities:\n$$\np(z \\mid x_S) \\propto p(z) \\prod_{i \\in S} p(x_i \\mid z)\n$$\nFor linear-Gaussian models, the product of Gaussian distributions is also a Gaussian (up to a normalization constant). A convenient way to work with products of Gaussians is by summing their parameters in the \"information form,\" where a Gaussian $\\mathcal{N}(\\mu, \\Sigma)$ is represented by its precision matrix $J = \\Sigma^{-1}$ and potential vector $h = \\Sigma^{-1}\\mu$. The probability density function is then proportional to $\\exp\\left(-\\frac{1}{2}z^\\top J z + h^\\top z\\right)$.\n\nThe precision matrix of the posterior, $J_{\\text{post}}(S)$, is the sum of the precision matrix of the prior, $J_0 = \\Sigma_0^{-1}$, and the precision contributions from the likelihood of each observed modality. The precision contribution from modality $i$ can be derived from the exponent of its likelihood function $p(x_i \\mid z)$:\n$$\n-\\frac{1}{2} (x_i - W_i z - b_i)^\\top \\Sigma_i^{-1} (x_i - W_i z - b_i)\n$$\nExpanding this quadratic form with respect to $z$ yields terms that are constant, linear in $z$, and quadratic in $z$. The quadratic term in $z$ is $-\\frac{1}{2} z^\\top (W_i^\\top \\Sigma_i^{-1} W_i) z$. This reveals that the precision contribution from modality $i$ is $J_i = W_i^\\top \\Sigma_i^{-1} W_i$.\n\nTherefore, the posterior precision matrix for an observed set of modalities $S$ is the sum of the individual precision matrices:\n$$\nJ_{\\text{post}}(S) = J_0 + \\sum_{i \\in S} J_i = \\Sigma_0^{-1} + \\sum_{i \\in S} W_i^\\top \\Sigma_i^{-1} W_i\n$$\nThe posterior covariance, $\\Sigma_{\\text{post}}(S)$, is the inverse of the posterior precision matrix:\n$$\n\\Sigma_{\\text{post}}(S) = \\left( J_{\\text{post}}(S) \\right)^{-1} = \\left( \\Sigma_0^{-1} + \\sum_{i \\in S} W_i^\\top \\Sigma_i^{-1} W_i \\right)^{-1}\n$$\nThis formula is precisely what is provided in the problem statement. The trace of the covariance matrix, $\\operatorname{tr}(\\Sigma)$, is the sum of its diagonal elements, which corresponds to the sum of the variances of the individual components of the random vector. It serves as a scalar measure of the total uncertainty or volume of the posterior distribution.\n\nThe problem defines the adaptation factor $\\alpha(S_{\\text{single}})$ as the ratio of the total posterior variance when a single modality is observed to the total posterior variance when both modalities are observed:\n$$\n\\alpha(S_{\\text{single}}) = \\frac{\\operatorname{tr}\\! \\left(\\Sigma_{\\text{post}}(S_{\\text{single}})\\right)}{\\operatorname{tr}\\! \\left(\\Sigma_{\\text{post}}(\\{A,B\\})\\right)}\n$$\nThis factor is expected to be greater than or equal to $1$, as observing less data (dropping a modality) cannot decrease uncertainty (reduce posterior variance). The denominator, $\\operatorname{tr}\\! \\left(\\Sigma_{\\text{post}}(\\{A,B\\})\\right)$, represents the minimal achievable posterior variance with the full dataset.\n\nThe computational procedure for each test case is as follows:\n1.  Given the parameter matrices $\\Sigma_0$, $W_A$, $\\Sigma_A$, $W_B$, and $\\Sigma_B$. All covariance matrices are given as diagonal, which simplifies their inversion: the inverse of a diagonal matrix is a diagonal matrix with the reciprocals of the original diagonal entries. Let $k=2$.\n2.  Calculate the prior precision: $J_0 = \\Sigma_0^{-1}$.\n3.  Calculate the precision contribution from modality $A$: $J_A = W_A^\\top \\Sigma_A^{-1} W_A$.\n4.  Calculate the precision contribution from modality $B$: $J_B = W_B^\\top \\Sigma_B^{-1} W_B$. All $J$ matrices will be of size $k \\times k$, which is $2 \\times 2$.\n5.  Calculate the posterior covariance for the full observation set $\\{A,B\\}$:\n    $$\n    \\Sigma_{\\text{post}}(\\{A,B\\}) = (J_0 + J_A + J_B)^{-1}\n    $$\n6.  Compute its trace, $T_{AB} = \\operatorname{tr}\\! \\left(\\Sigma_{\\text{post}}(\\{A,B\\})\\right)$.\n7.  For the given single-modality subset $S_{\\text{single}}$ (e.g., $\\{A\\}$), calculate the corresponding posterior covariance:\n    $$\n    \\Sigma_{\\text{post}}(S_{\\text{single}}) = (J_0 + J_i)^{-1} \\quad \\text{where } i \\in S_{\\text{single}}\n    $$\n8.  Compute its trace, $T_{S} = \\operatorname{tr}\\! \\left(\\Sigma_{\\text{post}}(S_{\\text{single}})\\right)$.\n9.  The adaptation factor is the ratio $\\alpha(S_{\\text{single}}) = T_{S} / T_{AB}$.\n\nThis procedure will be implemented for each of the five test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used.\n\ndef solve():\n    \"\"\"\n    Computes the adaptation factor for multimodal VAEs under modality dropout\n    for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1 (happy path, only A observed)\n        {\n            \"S_single\": \"A\",\n            \"Sigma0_diag\": [1.0, 1.0],\n            \"WA\": np.array([[1.0, 0.0], [0.5, 0.6], [0.0, 1.0]]),\n            \"SigmaA_diag\": [0.2, 0.2, 0.2],\n            \"WB\": np.array([[0.4, 0.3], [0.7, -0.1]]),\n            \"SigmaB_diag\": [0.5, 0.3],\n        },\n        # Test Case 2 (happy path, only B observed)\n        {\n            \"S_single\": \"B\",\n            \"Sigma0_diag\": [1.0, 1.0],\n            \"WA\": np.array([[1.0, 0.0], [0.5, 0.6], [0.0, 1.0]]),\n            \"SigmaA_diag\": [0.2, 0.2, 0.2],\n            \"WB\": np.array([[0.4, 0.3], [0.7, -0.1]]),\n            \"SigmaB_diag\": [0.5, 0.3],\n        },\n        # Test Case 3 (boundary: extremely noisy B, only B observed)\n        {\n            \"S_single\": \"B\",\n            \"Sigma0_diag\": [1.0, 1.0],\n            \"WA\": np.array([[1.0, 0.0], [0.5, 0.6], [0.0, 1.0]]),\n            \"SigmaA_diag\": [0.2, 0.2, 0.2],\n            \"WB\": np.array([[0.4, 0.3], [0.7, -0.1]]),\n            \"SigmaB_diag\": [1000.0, 1000.0],\n        },\n        # Test Case 4 (edge case: uninformative B, only B observed)\n        {\n            \"S_single\": \"B\",\n            \"Sigma0_diag\": [0.5, 0.5],\n            \"WA\": np.array([[0.9, 0.1], [0.4, 0.7], [0.0, 0.5]]),\n            \"SigmaA_diag\": [0.3, 0.3, 0.3],\n            \"WB\": np.array([[0.0, 0.0], [0.0, 0.0]]),\n            \"SigmaB_diag\": [1.0, 1.0],\n        },\n        # Test Case 5 (anisotropic prior, only A observed)\n        {\n            \"S_single\": \"A\",\n            \"Sigma0_diag\": [0.1, 2.0],\n            \"WA\": np.array([[1.2, -0.3], [0.0, 0.8], [0.5, 0.0]]),\n            \"SigmaA_diag\": [0.15, 0.4, 0.25],\n            \"WB\": np.array([[0.2, 0.1], [0.3, 0.5]]),\n            \"SigmaB_diag\": [0.6, 0.6],\n        },\n    ]\n\n    results = []\n    \n    def calculate_adaptation_factor(params):\n        \"\"\"Helper function to perform the calculation for one test case.\"\"\"\n        # Extract parameters\n        S_single = params[\"S_single\"]\n        Sigma0 = np.diag(params[\"Sigma0_diag\"])\n        WA = params[\"WA\"]\n        SigmaA_inv = np.diag(1.0 / np.array(params[\"SigmaA_diag\"]))\n        WB = params[\"WB\"]\n        SigmaB_inv = np.diag(1.0 / np.array(params[\"SigmaB_diag\"]))\n\n        # Calculate precision matrices\n        J0 = np.linalg.inv(Sigma0)\n        JA = WA.T @ SigmaA_inv @ WA\n        JB = WB.T @ SigmaB_inv @ WB\n\n        # Calculate posterior covariance and trace for both modalities\n        J_post_AB = J0 + JA + JB\n        Sigma_post_AB = np.linalg.inv(J_post_AB)\n        trace_AB = np.trace(Sigma_post_AB)\n\n        # Calculate posterior covariance and trace for the single modality case\n        if S_single == \"A\":\n            J_post_single = J0 + JA\n        elif S_single == \"B\":\n            J_post_single = J0 + JB\n        else:\n            # This case should not be reached based on problem description\n            raise ValueError(\"Invalid single modality specified.\")\n            \n        Sigma_post_single = np.linalg.inv(J_post_single)\n        trace_single = np.trace(Sigma_post_single)\n\n        # Compute adaptation factor\n        alpha = trace_single / trace_AB\n        return alpha\n\n    for case in test_cases:\n        result = calculate_adaptation_factor(case)\n        results.append(result)\n\n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}