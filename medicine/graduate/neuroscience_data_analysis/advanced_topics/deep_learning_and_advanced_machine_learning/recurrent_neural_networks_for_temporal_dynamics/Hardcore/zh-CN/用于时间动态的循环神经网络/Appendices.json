{
    "hands_on_practices": [
        {
            "introduction": "为了深刻理解循环神经网络（RNN）如何捕捉时间动态，我们可以将其视为一个动力学系统。这种视角在神经科学中尤为重要，它让我们能够运用分析工具来理解网络的内在行为。本练习  将指导你对一个RNN在其不动点附近进行线性化，并利用雅可比矩阵的特征结构来预测局部动态，从而将抽象的神经网络与稳定流形、不稳定流形等具体概念联系起来。",
            "id": "4189514",
            "problem": "给定一个由动力系统定义的连续时间二维（$2$D）基于速率的循环神经网络（RNN）\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x}) + \\mathbf{b},\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^2$ 是状态向量，$\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ 是循环连接矩阵，$\\mathbf{b} \\in \\mathbb{R}^2$ 是一个恒定输入偏置，$\\boldsymbol{\\phi}(\\mathbf{x})$ 是一个逐元素非线性函数，其分量为 $\\phi(x_i) = \\tanh(x_i)$，对于 $i \\in \\{1,2\\}$。假设 $\\mathbf{b} = \\mathbf{0}$，因此 $\\mathbf{x}^\\star = \\mathbf{0}$ 始终是一个不动点。该向量场在不动点处的雅可比矩阵为\n$$\n\\mathbf{J} = \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star} = -\\mathbf{I} + \\mathbf{W}\\,\\mathrm{diag}\\left(\\phi'(x_1^\\star), \\phi'(x_2^\\star)\\right),\n$$\n由于 $\\phi'(0) = 1$，这可简化为 $\\mathbf{J} = -\\mathbf{I} + \\mathbf{W}$。\n\n从自治动力系统的不动点和线性化的基本定义出发，并利用双曲不动点附近的局部行为由雅可比矩阵的特征结构决定的公认事实，您的任务是：\n- 计算 $\\mathbf{J}$ 在 $\\mathbf{x}^\\star$ 处的特征值和特征向量。\n- 对局部不变流形进行分类：稳定流形的维数等于 $\\mathbf{J}$ 的具有严格负实部的特征值的数量，不稳定流形的维数等于具有严格正实部的特征值的数量，中心流形的维数等于实部为零的特征值的数量。\n- 利用 $\\mathbf{J}$ 的特征向量构建用于轨迹模拟的初始方向。对于实特征向量，使用该特征向量及其负向量。对于二维中的复共轭特征对，使用一个复特征向量的实部和虚部形成一个实基，并包含每个实方向的正负两种情况。\n- 对于上面构建的每个方向 $\\mathbf{d}$，在有限时间范围 $T$ 内，使用一个小的位移幅度 $\\varepsilon$，模拟从 $\\mathbf{x}(0) = \\mathbf{x}^\\star + \\varepsilon\\,\\mathbf{d}$ 初始化的前向轨迹。使用上面给出的连续时间动力学，其中 $\\boldsymbol{\\phi}(\\mathbf{x}) = \\tanh(\\mathbf{x})$。\n\n对所有测试用例使用以下固定的模拟参数：\n- 位移幅度 $\\varepsilon = 10^{-3}$。\n- 时间范围 $T = 3$（时间单位是一个抽象的建模单位；您不需要输出任何物理单位）。\n- 谱分类容差 $\\delta = 10^{-8}$，应用于特征值的实部：如果特征值的实部小于 $-\\delta$，则为稳定；如果大于 $\\delta$，则为不稳定；如果其绝对值小于或等于 $\\delta$，则为中心。\n\n对于每个测试用例，计算：\n1. 整数 $n_s$，稳定流形的维数。\n2. 整数 $n_u$，不稳定流形的维数。\n3. 整数 $n_c$，中心流形的维数。\n4. 一个布尔值 $q$，表示沿与稳定特征值相关的方向的所有模拟轨迹到 $\\mathbf{x}^\\star$ 的欧几里得距离是否在时间 $T$ 内严格减小，以及沿与不稳定特征值相关的方向的所有模拟轨迹到 $\\mathbf{x}^\\star$ 的欧几里得距离是否在时间 $T$ 内严格增大。如果没有稳定或不稳定方向，则将相应条件视为空真 (vacuously true)。\n\n您的程序必须实现以上内容，并生成一行输出，其中包含所有测试用例的结果，形式为逗号分隔的列表的列表，每个内部列表的形式为 $[n_s, n_u, n_c, q]$，并用方括号括起来。例如，输出格式应如下所示\n$$\n[[n_{s,1}, n_{u,1}, n_{c,1}, q_1],[n_{s,2}, n_{u,2}, n_{c,2}, q_2],\\dots]\n$$\n不带任何附加文本。\n\n测试套件：\n使用以下五个循环连接矩阵 $\\mathbf{W}$，每个矩阵都隐式地与 $\\mathbf{b} = \\mathbf{0}$ 和 $\\boldsymbol{\\phi}(x) = \\tanh(x)$ 配对：\n- 情况 $1$ (鞍点): $\\mathbf{W} = \\begin{bmatrix} 1.5  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$。\n- 情况 $2$ (稳定节点): $\\mathbf{W} = \\begin{bmatrix} 0.2  0.0 \\\\ 0.0  0.3 \\end{bmatrix}$。\n- 情况 $3$ (不稳定节点): $\\mathbf{W} = \\begin{bmatrix} 1.3  0.0 \\\\ 0.0  1.4 \\end{bmatrix}$。\n- 情况 $4$ (稳定螺线): $\\mathbf{W} = \\begin{bmatrix} 0.5  1.8 \\\\ -1.8  0.5 \\end{bmatrix}$。\n- 情况 $5$ (中心，纯虚线性化): $\\mathbf{W} = \\begin{bmatrix} 1.0  1.0 \\\\ -1.0  1.0 \\end{bmatrix}$。\n\n您的程序必须是自包含的，不得读取任何输入，并且必须将上述过程应用于提供的测试套件。最终输出必须严格按照指定格式单行打印。",
            "solution": "该问题要求分析一个二维连续时间循环神经网络（RNN）在不动点附近的局部动力学。该分析涉及线性化、所得雅可比矩阵的特征分解、不变流形的分类以及沿特征方向的动力学数值验证。\n\n该动力系统由常微分方程（ODE）给出：\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x}) + \\mathbf{b}\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^2$ 是状态，$\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ 是连接矩阵，$\\mathbf{b} \\in \\mathbb{R}^2$ 是偏置，$\\boldsymbol{\\phi}$ 是逐元素的双曲正切激活函数，$\\phi(x_i) = \\tanh(x_i)$。\n\n首先，我们根据动力系统的基本原理建立分析框架。\n\n**1. 不动点和线性化**\n\n系统的一个不动点 $\\mathbf{x}^\\star$ 是动力学停止的状态，即 $\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}^\\star) = \\mathbf{0}$。我们已知偏置 $\\mathbf{b} = \\mathbf{0}$。我们可以验证原点 $\\mathbf{x}^\\star = \\mathbf{0}$ 是一个不动点：\n$$\n\\mathbf{f}(\\mathbf{0}) = -\\mathbf{0} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{0}) = \\mathbf{0} + \\mathbf{W}\\,\\mathbf{0} = \\mathbf{0}\n$$\n因为 $\\tanh(0) = 0$。\n\n不动点附近的系统行为可以通过一个线性系统来近似。这通过对 $\\mathbf{f}(\\mathbf{x})$ 在 $\\mathbf{x}^\\star$ 周围进行一阶泰勒展开来实现。令 $\\mathbf{x} = \\mathbf{x}^\\star + \\delta\\mathbf{x}$，其中 $\\delta\\mathbf{x}$ 是一个小扰动。\n$$\n\\frac{d(\\mathbf{x}^\\star + \\delta\\mathbf{x})}{dt} = \\frac{d\\delta\\mathbf{x}}{dt} \\approx \\mathbf{f}(\\mathbf{x}^\\star) + \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star} \\delta\\mathbf{x}\n$$\n由于 $\\mathbf{f}(\\mathbf{x}^\\star) = \\mathbf{0}$，线性化动力学由雅可比矩阵 $\\mathbf{J} = \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star}$ 决定：\n$$\n\\frac{d\\delta\\mathbf{x}}{dt} \\approx \\mathbf{J}\\,\\delta\\mathbf{x}\n$$\n$\\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x})$ 的雅可比矩阵为：\n$$\n\\mathbf{J} = \\frac{\\partial}{\\partial \\mathbf{x}} (-\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x})) = -\\mathbf{I} + \\mathbf{W}\\,\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}}\n$$\n其中 $\\mathbf{I}$ 是 $2 \\times 2$ 单位矩阵，$\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}}$ 是逐元素激活函数的雅可比矩阵。对于 $\\phi(x_i) = \\tanh(x_i)$，其导数为 $\\phi'(x_i) = 1 - \\tanh^2(x_i) = \\operatorname{sech}^2(x_i)$。$\\boldsymbol{\\phi}$ 的雅可比矩阵是一个对角矩阵：\n$$\n\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}} = \\mathrm{diag}(\\phi'(x_1), \\phi'(x_2))\n$$\n在不动点 $\\mathbf{x}^\\star = \\mathbf{0}$ 处求值，我们有 $\\phi'(0) = \\operatorname{sech}^2(0) = 1$。因此，该对角矩阵成为单位矩阵 $\\mathbf{I}$。原点处的雅可比矩阵简化为：\n$$\n\\mathbf{J} = -\\mathbf{I} + \\mathbf{W}\\,\\mathbf{I} = -\\mathbf{I} + \\mathbf{W}\n$$\n\n**2. 特征结构和稳定性分类**\n\nHartman-Grobman 定理指出，对于一个双曲不动点（其雅可比矩阵没有实部为零的特征值），非线性系统在不动点附近的定性行为在拓扑上等价于其线性化系统的行为。不动点的稳定性由 $\\mathbf{J}$ 的特征值 $\\{\\lambda_i\\}$ 决定。\n- 如果对所有 $i$ 都有 $\\mathrm{Re}(\\lambda_i)  0$，则不动点是稳定的。\n- 如果至少有一个 $i$ 使得 $\\mathrm{Re}(\\lambda_i) > 0$，则不动点是不稳定的。\n- 如果对某个 $i$ 有 $\\mathrm{Re}(\\lambda_i) = 0$，而对所有其他 $j$ 有 $\\mathrm{Re}(\\lambda_j) \\le 0$，则不动点是非双曲的，如果不考察高阶项，线性分析可能无法得出关于稳定性的结论。然而，我们仍然可以对不变流形进行分类。\n\n稳定、不稳定和中心流形的维数（$n_s$, $n_u$, $n_c$）通过根据特征值实部的符号对 $\\mathbf{J}$ 的特征值进行计数来确定。使用指定的容差 $\\delta = 10^{-8}$：\n- $n_s$ 是特征值 $\\lambda_i$ 中满足 $\\mathrm{Re}(\\lambda_i)  -\\delta$ 的数量。\n- $n_u$ 是特征值 $\\lambda_i$ 中满足 $\\mathrm{Re}(\\lambda_i) > \\delta$ 的数量。\n- $n_c$ 是特征值 $\\lambda_i$ 中满足 $|\\mathrm{Re}(\\lambda_i)| \\le \\delta$ 的数量。\n对于一个二维系统，必须有 $n_s + n_u + n_c = 2$。\n\n**3. 从特征向量构建初始方向**\n\n$\\mathbf{J}$ 的特征向量张成线性系统的不变子空间。通过沿这些方向启动轨迹，我们可以观察到每个流形特有的行为。设 $\\{\\lambda_i, \\mathbf{v}_i\\}$ 是 $\\mathbf{J}$ 的特征对。\n- **实特征向量**：如果一个特征向量 $\\mathbf{v}$ 是实的，它定义了一条穿过原点且在线性流下不变的直线。我们通过在 $\\mathbf{x}(0) = \\varepsilon \\mathbf{v}$ 和 $\\mathbf{x}(0) = -\\varepsilon \\mathbf{v}$ 处初始化轨迹来测试动力学，其中 $\\varepsilon$ 很小。\n- **复特征向量**：如果 $\\mathbf{J}$ 是实矩阵，其复特征值以共轭对 $\\lambda, \\bar{\\lambda}$ 出现，对应的特征向量也是复共轭的 $\\mathbf{v}, \\bar{\\mathbf{v}}$。设 $\\mathbf{v} = \\mathbf{a} + i\\mathbf{b}$。实向量 $\\mathbf{a}$ 和 $\\mathbf{b}$ 为与这对特征值相关的二维不变子空间构成一个基。我们通过沿四个方向初始化轨迹来测试这个平面内的动力学：$\\mathbf{x}(0) = \\pm\\varepsilon\\mathbf{a}$ 和 $\\mathbf{x}(0) = \\pm\\varepsilon\\mathbf{b}$。\n\n**4. 数值模拟与验证**\n\n为了验证非线性动力学在小扰动下符合线性预测，我们对原始 ODE $\\frac{d\\mathbf{x}}{dt} = -\\mathbf{x} + \\mathbf{W}\\,\\tanh(\\mathbf{x})$ 进行数值积分，针对每个构建的方向 $\\mathbf{d}$。初始条件为 $\\mathbf{x}(0) = \\varepsilon \\mathbf{d}$，其中 $\\varepsilon = 10^{-3}$，模拟在时间范围 $T = 3$ 内运行。\n\n然后我们计算布尔标志 $q$。对于每条轨迹，我们在多个时间点计算到不动点的欧几里得距离 $\\|\\mathbf{x}(t)\\|$。\n- 对于与稳定特征值（$\\mathrm{Re}(\\lambda)  -\\delta$）相关的方向 $\\mathbf{d}$，距离 $\\|\\mathbf{x}(t)\\|$ 必须在整个模拟区间内严格减小。\n- 对于与不稳定特征值（$\\mathrm{Re}(\\lambda) > \\delta$）相关的方向 $\\mathbf{d}$，距离 $\\|\\mathbf{x}(t)\\|$ 必须严格增大。\n\n当且仅当所有与稳定方向相关的轨迹都表现出严格减小的距离，并且所有与不稳定方向相关的轨迹都表现出严格增大的距离时，标志 $q$ 为真。如果没有稳定或不稳定方向，则相应的条件视为空真。\n\n实现过程将通过遍历每个给定的矩阵 $\\mathbf{W}$，执行这四个步骤，并收集结果 $[n_s, n_u, n_c, q]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the RNN dynamics analysis problem for a suite of test cases.\n    \"\"\"\n    \n    # Fixed simulation parameters\n    epsilon = 1e-3\n    T = 3.0\n    delta = 1e-8\n    \n    # Test suite of connectivity matrices W\n    test_cases = [\n        # Case 1 (saddle)\n        np.array([[1.5, 0.0], [0.0, 0.5]]),\n        # Case 2 (stable node)\n        np.array([[0.2, 0.0], [0.0, 0.3]]),\n        # Case 3 (unstable node)\n        np.array([[1.3, 0.0], [0.0, 1.4]]),\n        # Case 4 (stable spiral)\n        np.array([[0.5, 1.8], [-1.8, 0.5]]),\n        # Case 5 (center, purely imaginary linearization)\n        np.array([[1.0, 1.0], [-1.0, 1.0]]),\n    ]\n    \n    # List to store results for all test cases\n    all_results = []\n    \n    # ODE definition for the RNN dynamics\n    def f(t, x, W):\n        return -x + W @ np.tanh(x)\n\n    for W in test_cases:\n        # 1. Compute Jacobian at the origin\n        J = -np.eye(2) + W\n        \n        # 2. Compute eigenvalues and eigenvectors\n        eigvals, eigvecs = np.linalg.eig(J)\n        \n        # 3. Classify manifolds and count dimensions\n        ns, nu, nc = 0, 0, 0\n        real_parts = np.real(eigvals)\n        \n        for r in real_parts:\n            if r  -delta:\n                ns += 1\n            elif r > delta:\n                nu += 1\n            else:\n                nc += 1\n        \n        # 4. Construct initialization directions\n        stable_dirs = []\n        unstable_dirs = []\n        \n        # The eigenvalues are either both real or a complex conjugate pair\n        if np.iscomplexobj(eigvals):\n            # Complex conjugate pair\n            v = eigvecs[:, 0]\n            real_part = np.real(v)\n            imag_part = np.imag(v)\n            \n            # Normalize for consistency, although epsilon scaling dominates\n            if np.linalg.norm(real_part) > 1e-9: real_part /= np.linalg.norm(real_part)\n            if np.linalg.norm(imag_part) > 1e-9: imag_part /= np.linalg.norm(imag_part)\n\n            dirs_from_complex = [real_part, -real_part, imag_part, -imag_part]\n            \n            if real_parts[0]  -delta:\n                stable_dirs.extend(dirs_from_complex)\n            elif real_parts[0] > delta:\n                unstable_dirs.extend(dirs_from_complex)\n            # Center manifold directions are not checked for the 'q' flag\n        else:\n            # Two real eigenvalues\n            for i in range(2):\n                v = eigvecs[:, i]\n                # eig gives normalized eigenvectors\n                dirs_from_real = [v, -v]\n                \n                if real_parts[i]  -delta:\n                    stable_dirs.extend(dirs_from_real)\n                elif real_parts[i] > delta:\n                    unstable_dirs.extend(dirs_from_real)\n        \n        # 5. Simulate trajectories and verify behavior for 'q' flag\n        q = True\n        \n        # Check stable directions\n        if stable_dirs:\n            is_stable_ok = True\n            for d in stable_dirs:\n                if np.linalg.norm(d)  1e-9: continue\n                x0 = epsilon * d\n                sol = solve_ivp(f, [0, T], x0, args=(W,), dense_output=True, t_eval=np.linspace(0, T, num=100))\n                distances = np.linalg.norm(sol.y, axis=0)\n                # Check for strict decrease; allow for numerical noise near zero\n                if not np.all(np.diff(distances)  0):\n                    # Check if it settles at the origin making diffs zero.\n                    if not (np.all(distances[-10:]  1e-9) and distances[0] > distances[-1]):\n                         is_stable_ok = False\n                         break\n            if not is_stable_ok:\n                q = False\n\n        # Check unstable directions if stable ones passed\n        if q and unstable_dirs:\n            is_unstable_ok = True\n            for d in unstable_dirs:\n                if np.linalg.norm(d)  1e-9: continue\n                x0 = epsilon * d\n                sol = solve_ivp(f, [0, T], x0, args=(W,), dense_output=True, t_eval=np.linspace(0, T, num=100))\n                distances = np.linalg.norm(sol.y, axis=0)\n                if not np.all(np.diff(distances) > 0):\n                    is_unstable_ok = False\n                    break\n            if not is_unstable_ok:\n                q = False\n                \n        all_results.append([ns, nu, nc, q])\n        \n    # Format the final output string\n    result_str = \",\".join([str(res) for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理解了简单RNN的基本动态后，我们将深入探究一种更强大的变体——长短期记忆（LSTM）网络的内部工作原理。这个练习  要求你一步步手动完成一次前向传播计算，揭示输入门、遗忘门和输出门如何协同工作，以精确控制信息流动，从而克服简单RNN的梯度消失等局限性。",
            "id": "4189572",
            "problem": "考虑一个一维长短期记忆（LSTM）循环神经网络，该网络用于在神经科学数据分析场景中对试验平均神经信号的潜在时间动态进行建模。在时间步 $t$，$LSTM$ 根据以下标准定义更新其门和状态：输入门 $i_t = \\sigma(z_{i,t})$，遗忘门 $f_t = \\sigma(z_{f,t})$，输出门 $o_t = \\sigma(z_{o,t})$，以及细胞候选状态 $g_t = \\tanh(z_{g,t})$，其中 $\\sigma(u) = \\frac{1}{1+\\exp(-u)}$ 是 logistic 函数，$\\tanh(u)$ 是双曲正切函数。细胞状态和隐藏状态的演化如下：$c_t = f_t \\, c_{t-1} + i_t \\, g_t$ 和 $h_t = o_t \\, \\tanh(c_t)$。激活前函数是当前输入 $x_t$ 和前一个隐藏状态 $h_{t-1}$ 的仿射函数：$z_{i,t} = W_{xi}\\,x_t + W_{hi}\\,h_{t-1} + b_i$，$z_{f,t} = W_{xf}\\,x_t + W_{hf}\\,h_{t-1} + b_f$，$z_{o,t} = W_{xo}\\,x_t + W_{ho}\\,h_{t-1} + b_o$，以及 $z_{g,t} = W_{xg}\\,x_t + W_{hg}\\,h_{t-1} + b_g$。\n\n给定一个在 $T=3$ 个时间步上的输入序列，该序列设计为在 $t=2$ 时包含一个瞬态输入：$x_1 = 0$，$x_2 = \\ln(3)$，$x_3 = 0$。初始状态为 $c_0 = 0$ 和 $h_0 = 0$。LSTM 参数均为标量，具体如下：\n- $W_{xi} = 1$, $W_{hi} = 0$, $b_i = 0$,\n- $W_{xf} = -\\frac{\\ln(2)}{\\ln(3)}$, $W_{hf} = 0$, $b_f = \\ln(4)$,\n- $W_{xo} = 0$, $W_{ho} = 0$, $b_o = \\ln(2)$,\n- $W_{xg} = 0$, $W_{hg} = 0$, $b_g = \\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)$.\n\n请仅使用所提供的定义，计算前向传播，以获得 $t=1,2,3$ 时的门和状态，并以此明确验证遗忘门 $f_t$ 是如何调节对瞬态输入的保留的。作为最终结果，请报告最终细胞状态 $c_3$ 中仅由 $t=2$ 时的瞬态输入所产生的标量贡献（即 $c_3$ 中通过 LSTM 动态过程可归因于 $x_2$ 的部分）。请提供精确值，无需四舍五入。",
            "solution": "该问题要求对一个一维长短期记忆（LSTM）网络进行 3 个时间步的前向传播计算，并计算对最终细胞状态的特定贡献。该问题是适定的，并提供了所有必要的参数和初始条件。\n\n首先，我们将给定的标量参数代入通用的 LSTM 方程。激活前函数为：\n$z_{i,t} = W_{xi}\\,x_t + W_{hi}\\,h_{t-1} + b_i = (1)x_t + (0)h_{t-1} + 0 = x_t$\n$z_{f,t} = W_{xf}\\,x_t + W_{hf}\\,h_{t-1} + b_f = \\left(-\\frac{\\ln(2)}{\\ln(3)}\\right)x_t + (0)h_{t-1} + \\ln(4) = -\\frac{\\ln(2)}{\\ln(3)}x_t + 2\\ln(2)$\n$z_{o,t} = W_{xo}\\,x_t + W_{ho}\\,h_{t-1} + b_o = (0)x_t + (0)h_{t-1} + \\ln(2) = \\ln(2)$\n$z_{g,t} = W_{xg}\\,x_t + W_{hg}\\,h_{t-1} + b_g = (0)x_t + (0)h_{t-1} + \\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)$\n\n门和细胞候选状态的更新如下：\n$i_t = \\sigma(z_{i,t}) = \\sigma(x_t)$\n$f_t = \\sigma(z_{f,t}) = \\sigma\\left(-\\frac{\\ln(2)}{\\ln(3)}x_t + 2\\ln(2)\\right)$\n$o_t = \\sigma(z_{o,t}) = \\sigma(\\ln(2))$\n$g_t = \\tanh(z_{g,t}) = \\tanh\\left(\\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)\\right)$\n\n输出门 $o_t$ 和细胞候选状态 $g_t$ 在所有时间步中都是恒定的。我们来计算它们的值。sigmoid 函数为 $\\sigma(u) = (1+\\exp(-u))^{-1}$。\n$o_t = \\sigma(\\ln(2)) = \\frac{1}{1+\\exp(-\\ln(2))} = \\frac{1}{1+\\frac{1}{2}} = \\frac{1}{\\frac{3}{2}} = \\frac{2}{3}$。\n$g_t = \\tanh\\left(\\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)\\right) = \\frac{1}{2}$。\n\n细胞状态和隐藏状态的更新为：\n$c_t = f_t c_{t-1} + i_t g_t = f_t c_{t-1} + \\frac{1}{2} i_t$\n$h_t = o_t \\tanh(c_t) = \\frac{2}{3} \\tanh(c_t)$\n\n初始条件为 $c_0 = 0$ 和 $h_0 = 0$。输入序列为 $x_1 = 0$，$x_2 = \\ln(3)$，$x_3 = 0$。我们进行前向传播。\n\n**时间步 $t=1$：**\n输入：$x_1 = 0$。\n输入门和遗忘门的激活前函数为：\n$z_{i,1} = x_1 = 0$\n$z_{f,1} = -\\frac{\\ln(2)}{\\ln(3)}(0) + 2\\ln(2) = 2\\ln(2) = \\ln(4)$\n门激活值为：\n$i_1 = \\sigma(0) = \\frac{1}{1+\\exp(0)} = \\frac{1}{2}$\n$f_1 = \\sigma(\\ln(4)) = \\frac{1}{1+\\exp(-\\ln(4))} = \\frac{1}{1+\\frac{1}{4}} = \\frac{4}{5}$\n细胞状态和隐藏状态更新如下：\n$c_1 = f_1 c_0 + i_1 g_1 = \\left(\\frac{4}{5}\\right)(0) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4}$\n$h_1 = \\frac{2}{3}\\tanh(c_1) = \\frac{2}{3}\\tanh\\left(\\frac{1}{4}\\right)$\n\n**时间步 $t=2$：**\n输入：$x_2 = \\ln(3)$。这是指定的瞬态输入。\n输入门和遗忘门的激活前函数为：\n$z_{i,2} = x_2 = \\ln(3)$\n$z_{f,2} = -\\frac{\\ln(2)}{\\ln(3)} (\\ln(3)) + 2\\ln(2) = -\\ln(2) + 2\\ln(2) = \\ln(2)$\n门激活值为：\n$i_2 = \\sigma(\\ln(3)) = \\frac{1}{1+\\exp(-\\ln(3))} = \\frac{1}{1+\\frac{1}{3}} = \\frac{3}{4}$\n$f_2 = \\sigma(\\ln(2)) = \\frac{1}{1+\\exp(-\\ln(2))} = \\frac{1}{1+\\frac{1}{2}} = \\frac{2}{3}$\n细胞状态更新如下：\n$c_2 = f_2 c_1 + i_2 g_2 = \\left(\\frac{2}{3}\\right)\\left(\\frac{1}{4}\\right) + \\left(\\frac{3}{4}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{6} + \\frac{3}{8} = \\frac{4}{24} + \\frac{9}{24} = \\frac{13}{24}$\n隐藏状态为 $h_2 = \\frac{2}{3}\\tanh(c_2) = \\frac{2}{3}\\tanh\\left(\\frac{13}{24}\\right)$。\n\n**时间步 $t=3$：**\n输入：$x_3 = 0$。\n输入门和遗忘门的激活前函数与 $t=1$ 时相同：\n$z_{i,3} = x_3 = 0 \\implies i_3 = \\frac{1}{2}$\n$z_{f,3} = \\ln(4) \\implies f_3 = \\frac{4}{5}$\n最终的细胞状态 $c_3$ 是：\n$c_3 = f_3 c_2 + i_3 g_3 = \\left(\\frac{4}{5}\\right)\\left(\\frac{13}{24}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{13}{5 \\cdot 6} + \\frac{1}{4} = \\frac{13}{30} + \\frac{1}{4} = \\frac{26}{60} + \\frac{15}{60} = \\frac{41}{60}$\n最终的隐藏状态是 $h_3 = \\frac{2}{3}\\tanh(c_3) = \\frac{2}{3}\\tanh\\left(\\frac{41}{60}\\right)$。\n\n问题要求计算瞬态输入在 $t=2$ 时对最终细胞状态 $c_3$ 的贡献。为了确定这一点，我们可以将 $c_3$ 完全用每一步的输入来表示。\n$c_3 = f_3 c_2 + i_3 g_3$\n代入 $c_2 = f_2 c_1 + i_2 g_2$：\n$c_3 = f_3 (f_2 c_1 + i_2 g_2) + i_3 g_3 = f_3 f_2 c_1 + f_3 i_2 g_2 + i_3 g_3$\n代入 $c_1 = f_1 c_0 + i_1 g_1$ 并使用 $c_0=0$：\n$c_3 = f_3 f_2 (i_1 g_1) + f_3 i_2 g_2 + i_3 g_3$\n\n此表达式将 $c_3$ 分解为来自每个时间步输入的贡献之和：\n- 来自 $x_1$ 的贡献：$f_3 f_2 i_1 g_1$\n- 来自 $x_2$ 的贡献：$f_3 i_2 g_2$\n- 来自 $x_3$ 的贡献：$i_3 g_3$\n\n项 $i_2 g_2$ 表示在 $t=2$ 时由于输入 $x_2$ 而添加到细胞状态的新信息。下一步的遗忘门 $f_3$ 调节了这些信息保留到状态 $c_3$ 中的程度。因此，$c_3$ 中可归因于 $x_2$ 的部分是 $f_3 i_2 g_2$。\n\n我们使用已找到的门值来计算该项的值：\n$f_3 = \\frac{4}{5}$\n$i_2 = \\frac{3}{4}$\n$g_2 = \\frac{1}{2}$\n\n来自 $x_2$ 对 $c_3$ 的贡献 = $f_3 \\, i_2 \\, g_2 = \\left(\\frac{4}{5}\\right) \\left(\\frac{3}{4}\\right) \\left(\\frac{1}{2}\\right) = \\frac{12}{40} = \\frac{3}{10}$。\n\n此计算明确地展示了遗忘门 $f_3 = \\frac{4}{5}$ 如何作用于因瞬态输入而写入细胞的信息 $i_2 g_2 = \\frac{3}{8}$，在下一个时间步中保留了其 $80\\%$。与输入为零时（$f_1 = f_3 = \\frac{4}{5}$）相比，瞬态输入 $x_2 = \\ln(3)$ 导致了更小的遗忘门值 $f_2 = \\frac{2}{3}$，这使得网络在重要事件发生时“忘记”了更多其过去的状态 $c_1$。\n\n要求的最终结果是在 $t=2$ 时的输入对 $c_3$ 产生的标量贡献。",
            "answer": "$$\\boxed{\\frac{3}{10}}$$"
        },
        {
            "introduction": "为了将理论付诸实践，我们将实现一个完整的RNN训练循环，这要求我们处理一个关键的现实挑战：处理批次中长度可变的序列。这个高级练习  将引导你为前向传播的损失计算和反向传播（BPTT）过程实现掩码（masking）。通过这种方式，你将学会如何确保填充数据不会干扰模型的学习过程，从而掌握构建稳健RNN模型的关键技能。",
            "id": "4189567",
            "problem": "您将为一个单层循环神经网络（RNN）实现一个完整的前向和后向传播过程。该 RNN 具有加性输入和循环连接，使用掩码（masking）处理可变长度序列的小批量（minibatch）数据。目标是构建带掩码的序列，计算带掩码的均方误差损失，使用链式法则推导并实现梯度，并数值验证梯度不会从序列末端的填充时间步（padded timesteps）泄露。此外，您还将测试并展示出现在序列中部的被掩码时间步的独特行为，这些时间步由于因果传播可能仍会携带梯度，并处理序列中有效时间步为零的边界情况。\n\n初始定义和规则：\n- 使用以下前向循环公式，适用于一个带有双曲正切激活函数的单层循环神经网络（RNN）。对于批次索引 $b$ 和时间索引 $t$，隐藏状态为\n$\n\\mathbf{h}_{b,t} = \\tanh\\left(\\mathbf{a}_{b,t}\\right)\n$, $\\quad \\mathbf{a}_{b,t} = \\mathbf{x}_{b,t}\\mathbf{W}_{xh} + \\mathbf{h}_{b,t-1}\\mathbf{W}_{hh} + \\mathbf{b}_h$,\n初始状态为 $\\mathbf{h}_{b,-1} = \\mathbf{0}$。输出为\n$$\n\\mathbf{y}_{b,t} = \\mathbf{h}_{b,t}\\mathbf{W}_{hy} + \\mathbf{b}_y.\n$$\n- 设每个时间步、每个序列的损失为半平方误差\n$\n\\ell_{b,t} = \\frac{1}{2}\\left\\|\\mathbf{y}_{b,t} - \\mathbf{r}_{b,t}\\right\\|_2^2\n$,\n并设 $\\mathbf{m}_{b,t} \\in \\{0,1\\}$ 为一个二进制掩码，用于指示有效时间步（$1$）和填充时间步（$0$）。带掩码的损失是所有有效时间步的平均值，\n$$\nL = \\frac{\\sum_{b=1}^{B}\\sum_{t=0}^{T-1} \\mathbf{m}_{b,t} \\, \\ell_{b,t}}{\\sum_{b=1}^{B}\\sum_{t=0}^{T-1} \\mathbf{m}_{b,t}},\n$$\n约定如果分母为 $0$，则 $L = 0$ 且所有梯度均为零。\n- 使用微积分中的标准链式法则进行微分，以及双曲正切函数的导数，即\n$\n\\frac{\\partial}{\\partial \\mathbf{a}} \\tanh(\\mathbf{a}) = \\mathbf{1} - \\tanh(\\mathbf{a}) \\odot \\tanh(\\mathbf{a})\n$,\n其中 $\\odot$ 表示逐元素乘法。\n- 所有计算必须在实数算术中执行。不涉及物理单位或角度，所有量均为无量纲。\n\n您的程序必须：\n- 构建带掩码的小批量序列并计算带掩码的损失 $L$。\n- 使用带掩码的随时间反向传播（backpropagation through time）算法，计算关于所有参数 $\\mathbf{W}_{xh}$、$\\mathbf{W}_{hh}$、$\\mathbf{W}_{hy}$、$\\mathbf{b}_h$ 和 $\\mathbf{b}_y$ 的精确梯度。\n- 数值验证对于尾部填充的时间步（序列最后一个有效时间步之后掩码中的零值），其直接输出梯度和隐藏状态梯度在指定的绝对容差范围内均为零。\n- 证明对序列中部的时间步进行掩码会使其直接输出梯度为零，但由于来自未来有效时间步的因果传播，其隐藏状态梯度可以为非零。\n- 正确处理批次中所有时间步都被掩码（分母为零）的边界情况，此时应产生零损失和零梯度。\n\n模型维度和固定参数：\n- 输入维度 $d_x = 2$，隐藏维度 $d_h = 3$，以及输出维度 $d_y = 2$。\n- 参数矩阵和向量是固定的，由下式给出\n$$\n\\mathbf{W}_{xh} = \\begin{bmatrix} 0.1  -0.2  0.3 \\\\ 0.05  0.4  -0.1 \\end{bmatrix}, \\quad\n\\mathbf{W}_{hh} = \\begin{bmatrix} 0.2  0.0  -0.1 \\\\ 0.1  0.3  0.0 \\\\ -0.2  0.05  0.25 \\end{bmatrix},\n$$\n$$\n\\mathbf{b}_h = \\begin{bmatrix} 0.0  0.0  0.0 \\end{bmatrix}, \\quad\n\\mathbf{W}_{hy} = \\begin{bmatrix} 0.3  -0.2 \\\\ -0.1  0.4 \\\\ 0.2  0.1 \\end{bmatrix}, \\quad\n\\mathbf{b}_y = \\begin{bmatrix} 0.0  0.0 \\end{bmatrix}.\n$$\n- 对于所有测试用例，输入和目标均按以下方式确定性地构建。对于批次大小 $B$ 和序列长度 $T$，为每个 $b \\in \\{0, \\ldots, B-1\\}$ 和每个 $t \\in \\{0, \\ldots, T-1\\}$ 定义：\n$$\n\\mathbf{x}_{b,t} = 0.1 \\cdot \\begin{bmatrix} b+1 \\\\ t+1 \\end{bmatrix}, \\quad\n\\mathbf{r}_{b,t} = \\begin{bmatrix} 0.05\\,(b+1)\\,(t+1) \\\\ -0.02\\,(t+1) \\end{bmatrix}.\n$$\n每个测试用例的掩码都如下唯一指定。\n\n测试套件：\n- 测试用例 1（带有尾部填充的正常流程）：\n  - 批次大小 $B = 3$，序列长度 $T = 5$，每个序列的有效长度为 $\\left[5, 3, 4\\right]$。\n  - 当 $t  \\text{length}_b$ 时，掩码 $\\mathbf{m}_{b,t} = 1$，否则 $\\mathbf{m}_{b,t} = 0$。\n  - 输出：一个列表，包含带掩码的损失 $L$（浮点数）和一个布尔值，该布尔值指示所有在尾部填充时间步上的梯度是否在绝对容差 $\\varepsilon = 10^{-12}$ 内数值上为零。\n- 测试用例 2（包含一个零长度序列）：\n  - 批次大小 $B = 2$，序列长度 $T = 4$，每个序列的有效长度为 $\\left[0, 4\\right]$。\n  - 掩码定义如上。第一个序列的有效时间步为零。\n  - 输出：一个列表，包含 $L$（浮点数）和一个布尔值，该布尔值指示所有源自零长度序列的梯度是否在 $\\varepsilon = 10^{-12}$ 内数值上为零。\n- 测试用例 3（所有序列均为零长度，分母为零）：\n  - 批次大小 $B = 2$，序列长度 $T = 3$，每个序列的有效长度为 $\\left[0, 0\\right]$。\n  - 掩码定义如上。\n  - 输出：一个列表，包含 $L$（浮点数，必须为 $0.0$）和一个布尔值，该布尔值指示所有参数梯度是否在 $\\varepsilon = 10^{-12}$ 内数值上为零。\n- 测试用例 4（混合序列中部掩码和尾部填充）：\n  - 批次大小 $B = 2$，序列长度 $T = 5$，每个序列的有效长度为 $\\left[5, 3\\right]$。\n  - 对于第一个序列（$b = 0$），设置所有 $t$ 的 $\\mathbf{m}_{0,t} = 1$，除了 $\\mathbf{m}_{0,2} = 0$（被掩码的序列中部时间步）。\n  - 对于第二个序列（$b = 1$），使用尾部填充，当 $t \\in \\{0,1,2\\}$ 时 $\\mathbf{m}_{1,t} = 1$，否则 $\\mathbf{m}_{1,t} = 0$。\n  - 输出：一个列表，包含 $L$（浮点数）、一个指示尾部填充时间步的梯度是否在 $\\varepsilon = 10^{-12}$ 内数值上为零的布尔值，以及一个指示第一个序列中被掩码的中部时间步是否因因果传播而携带非零隐藏状态梯度，而其直接输出梯度为零的布尔值。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例必须按上文规定生成一个列表，并且程序应将这些按测试用例生成的列表聚合到一个外部列表中。例如，输出必须类似于 $[\\text{case1\\_list},\\text{case2\\_list},\\text{case3\\_list},\\text{case4\\_list}]$。\n\n所有对被掩码时间步上梯度的数值相等性检查都必须使用绝对容差 $\\varepsilon = 10^{-12}$，并且所有输出必须是布尔值、整数、浮点数或这些基本类型的列表。",
            "solution": "该问题要求为一个单层循环神经网络（RNN）实现并验证一个完整的前向和后向传播过程，其中使用掩码来处理可变长度的序列。解决方案涉及使用随时间反向传播（BPTT）推导梯度表达式，实现该算法，并在几个探究掩码行为的测试用例上验证其正确性。\n\n### 1. 模型和损失函数\n\nRNN 的动态由以下方程定义，其中 $b$ 为批次索引，$t$ 为时间索引：\n隐藏层的预激活值：\n$$ \\mathbf{a}_{b,t} = \\mathbf{x}_{b,t}\\mathbf{W}_{xh} + \\mathbf{h}_{b,t-1}\\mathbf{W}_{hh} + \\mathbf{b}_h $$\n隐藏状态，初始状态为 $\\mathbf{h}_{b,-1} = \\mathbf{0}$：\n$$ \\mathbf{h}_{b,t} = \\tanh(\\mathbf{a}_{b,t}) $$\n网络输出：\n$$ \\mathbf{y}_{b,t} = \\mathbf{h}_{b,t}\\mathbf{W}_{hy} + \\mathbf{b}_y $$\n损失函数是带掩码的均方误差。每个时间步的损失为：\n$$ \\ell_{b,t} = \\frac{1}{2}\\left\\|\\mathbf{y}_{b,t} - \\mathbf{r}_{b,t}\\right\\|_2^2 $$\n总损失 $L$ 是在所有有效时间步上的归一化总和，由二进制掩码 $\\mathbf{m}_{b,t} \\in \\{0,1\\}$ 指示：\n$$ L = \\frac{\\sum_{b,t} \\mathbf{m}_{b,t} \\, \\ell_{b,t}}{\\sum_{b,t} \\mathbf{m}_{b,t}} = \\frac{1}{N_{valid}} \\sum_{b,t} \\mathbf{m}_{b,t} \\, \\ell_{b,t} $$\n其中 $N_{valid} = \\sum_{b,t} \\mathbf{m}_{b,t}$。如果 $N_{valid} = 0$，则 $L=0$ 且所有梯度均为零。\n\n### 2. 梯度推导（随时间反向传播）\n\n为了训练网络，我们必须计算损失 $L$ 相对于所有参数：$\\mathbf{W}_{xh}$、$\\mathbf{W}_{hh}$、$\\mathbf{b}_h$、$\\mathbf{W}_{hy}$ 和 $\\mathbf{b}_y$ 的梯度。我们使用链式法则。\n\n$L$ 相对于输出 $\\mathbf{y}_{b,t}$ 的梯度是反向传播的起点。\n$$ \\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} = \\frac{\\partial L}{\\partial \\ell_{b,t}} \\frac{\\partial \\ell_{b,t}}{\\partial \\mathbf{y}_{b,t}} = \\frac{\\mathbf{m}_{b,t}}{N_{valid}} (\\mathbf{y}_{b,t} - \\mathbf{r}_{b,t}) $$\n我们将这个梯度表示为 $\\mathbf{gY}_{b,t}$。掩码 $\\mathbf{m}_{b,t}$ 确保填充的时间步（$m_{b,t}=0$）对梯度没有任何贡献。\n\n#### 2.1. 输出层的梯度（$\\mathbf{W}_{hy}, \\mathbf{b}_y$）\n\n输出层参数的梯度是通过对所有时间步的贡献求和来计算的。\n$$ \\frac{\\partial L}{\\partial \\mathbf{W}_{hy}} = \\sum_{b,t} \\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{W}_{hy}} = \\sum_{b,t} \\mathbf{h}_{b,t}^T \\mathbf{gY}_{b,t} $$\n$$ \\frac{\\partial L}{\\partial \\mathbf{b}_y} = \\sum_{b,t} \\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{b}_y} = \\sum_{b,t} \\mathbf{gY}_{b,t} $$\n这些梯度可以在前向传播之后，当所有的 $\\mathbf{h}_{b,t}$ 和 $\\mathbf{gY}_{b,t}$ 都已知时计算。\n\n#### 2.2. 循环层的梯度\n\n关于隐藏状态 $\\mathbf{h}_{b,t}$ 的梯度有两个分量：一个来自同一时间步的输出 $\\mathbf{y}_{b,t}$，另一个从后续隐藏状态 $\\mathbf{h}_{b,t+1}$ 传播而来。这导致了一个后向循环。\n\n设 $\\mathbf{gH}_{b,t} = \\frac{\\partial L}{\\partial \\mathbf{h}_{b,t}}$ 和 $\\mathbf{gA}_{b,t} = \\frac{\\partial L}{\\partial \\mathbf{a}_{b,t}}$。$\\mathbf{gH}_{b,t}$ 的循环关系为：\n$$ \\mathbf{gH}_{b,t} = \\underbrace{\\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{h}_{b,t}}}_{\\text{来自输出}} + \\underbrace{\\frac{\\partial L}{\\partial \\mathbf{a}_{b,t+1}} \\frac{\\partial \\mathbf{a}_{b,t+1}}{\\partial \\mathbf{h}_{b,t}}}_{\\text{来自未来}} $$\n计算各项：\n$$ \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{h}_{b,t}} = \\mathbf{W}_{hy}^T \\quad \\text{和} \\quad \\frac{\\partial \\mathbf{a}_{b,t+1}}{\\partial \\mathbf{h}_{b,t}} = \\mathbf{W}_{hh} $$\n因此，循环关系变为：\n$$ \\mathbf{gH}_{b,t} = \\mathbf{gY}_{b,t} \\mathbf{W}_{hy}^T + \\mathbf{gA}_{b,t+1} \\mathbf{W}_{hh} $$\n我们还需要关于预激活值 $\\mathbf{a}_{b,t}$ 的梯度，这可以通过链式法则和 $\\tanh$ 的导数求得：\n$$ \\mathbf{gA}_{b,t} = \\frac{\\partial L}{\\partial \\mathbf{h}_{b,t}} \\frac{\\partial \\mathbf{h}_{b,t}}{\\partial \\mathbf{a}_{b,t}} = \\mathbf{gH}_{b,t} \\odot (\\mathbf{1} - \\mathbf{h}_{b,t} \\odot \\mathbf{h}_{b,t}) $$\n其中 $\\odot$ 是逐元素乘法。\n\nBPTT 算法通过从 $t=T-1$ 到 $t=0$ 的时间逆向迭代来进行。我们将从未来流回的梯度初始化为零，即 $\\mathbf{gA}_{b,T} = \\mathbf{0}$。\n\n对于 $t = T-1, \\dots, 0$：\n1. 计算 $\\mathbf{gH}_{b,t} = \\mathbf{gY}_{b,t} \\mathbf{W}_{hy}^T + \\mathbf{gA}_{b,t+1} \\mathbf{W}_{hh}$。（其中 $\\mathbf{gA}_{b,T}=\\mathbf{0}$）\n2. 计算 $\\mathbf{gA}_{b,t} = \\mathbf{gH}_{b,t} \\odot (\\mathbf{1} - \\mathbf{h}_{b,t}^2)$。\n3. 累加循环层参数的梯度：\n   $$ \\frac{\\partial L}{\\partial \\mathbf{W}_{xh}} += \\sum_{b} \\mathbf{x}_{b,t}^T \\mathbf{gA}_{b,t} $$\n   $$ \\frac{\\partial L}{\\partial \\mathbf{W}_{hh}} += \\sum_{b} \\mathbf{h}_{b,t-1}^T \\mathbf{gA}_{b,t} \\quad (\\text{其中 } \\mathbf{h}_{b,-1} = \\mathbf{0})$$\n   $$ \\frac{\\partial L}{\\partial \\mathbf{b}_h} += \\sum_{b} \\mathbf{gA}_{b,t} $$\n\n### 3. 实现与验证策略\n\n总体算法如下：\n1.  **前向传播**：从 $t=0$ 迭代到 $T-1$，计算并存储所有的 $\\mathbf{h}_{b,t}$ 和 $\\mathbf{y}_{b,t}$。\n2.  **损失计算**：计算有效时间步的总数 $N_{valid}$ 和最终损失 $L$。如果 $N_{valid}=0$，则设置 $L=0$ 并且所有梯度为零。\n3.  **后向传播**：\n    a. 计算初始输出梯度 $\\mathbf{gY}_{b,t}$，并应用掩码。\n    b. 计算并存储输出层参数 $\\mathbf{W}_{hy}$ 和 $\\mathbf{b}_y$ 的梯度。\n    c. 将总隐藏状态梯度张量 $\\mathbf{gH}$ 初始化为零。\n    d. 从 $t=T-1$ 向下迭代到 $0$，使用循环关系计算 $\\mathbf{gH}_{:,t,:}$。这会捕获每个隐藏状态的完整梯度。\n    e. 从最终的 $\\mathbf{gH}$ 和存储的激活值计算预激活梯度 $\\mathbf{gA}$。\n    f. 通过在整个 $\\mathbf{gA}$ 上进行张量缩并（例如 `einsum`）来计算循环层参数的梯度（$\\mathbf{W}_{xh}$、$\\mathbf{W}_{hh}$、$\\mathbf{b}_h$）。\n\n#### 掩码行为验证：\n-   **尾部填充**：对于长度为 $L_b  T$ 的序列 $b$，所有时间步 $t \\ge L_b$ 都被填充。我们必须验证对于这些时间步，$\\mathbf{gY}_{b,t}$ 和 $\\mathbf{gH}_{b,t}$ 在数值上均为零。这证明了梯度不会“泄漏”到填充区域。\n-   **序列中部掩码**：一个时间步 $t$ 即使被有效时间步包围也可能被掩码。在这种情况下，其直接输出梯度 $\\mathbf{gY}_{b,t}$ 将为零。然而，由于梯度从未来的有效时间步（$t+1, t+2, \\dots$）回传，其隐藏状态梯度 $\\mathbf{gH}_{b,t}$ 可以是非零的。我们必须验证这种独特的行为。\n-   **零长度序列**：对于长度为 $0$ 的序列，其掩码全为零。这意味着所有相关的梯度（$\\mathbf{gY}_{b,:}$, $\\mathbf{gH}_{b,:}$）都为零。如果一个批次中的所有序列长度都为零，则 $N_{valid}=0$，并且所有最终的参数梯度都必须为零。\n\n这些原则在提供的 Python 代码中得以实现，该代码系统地执行每个测试用例并执行所需的数值检查。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the RNN forward/backward pass.\n    \"\"\"\n\n    # Fixed parameters\n    W_xh = np.array([[0.1, -0.2, 0.3], [0.05, 0.4, -0.1]])\n    W_hh = np.array([[0.2, 0.0, -0.1], [0.1, 0.3, 0.0], [-0.2, 0.05, 0.25]])\n    b_h = np.array([0.0, 0.0, 0.0])\n    W_hy = np.array([[0.3, -0.2], [-0.1, 0.4], [0.2, 0.1]])\n    b_y = np.array([0.0, 0.0])\n    params = (W_xh, W_hh, b_h, W_hy, b_y)\n    \n    tol = 1e-12\n\n    test_cases = [\n        {'B': 3, 'T': 5, 'lengths': [5, 3, 4], 'id': 1},\n        {'B': 2, 'T': 4, 'lengths': [0, 4], 'id': 2},\n        {'B': 2, 'T': 3, 'lengths': [0, 0], 'id': 3},\n        {'B': 2, 'T': 5, 'lengths': [5, 3], 'id': 4}\n    ]\n\n    results = []\n    for case in test_cases:\n        B, T, lengths = case['B'], case['T'], case['lengths']\n        \n        # --- Data and Mask Generation ---\n        X = np.zeros((B, T, W_xh.shape[0]))\n        R = np.zeros((B, T, W_hy.shape[1]))\n        for b in range(B):\n            for t in range(T):\n                X[b, t] = 0.1 * np.array([b + 1, t + 1])\n                R[b, t] = np.array([0.05 * (b + 1) * (t + 1), -0.02 * (t + 1)])\n\n        M = np.zeros((B, T))\n        if case['id'] == 4:\n            # Special mask for TC4\n            M[0] = np.array([1, 1, 0, 1, 1])\n            M[1, :lengths[1]] = 1\n        else:\n            for b in range(B):\n                M[b, :lengths[b]] = 1\n\n        # --- Run RNN pass ---\n        loss, grads, aux_data = rnn_forward_backward(X, R, M, params)\n        gW_xh, gW_hh, gb_h, gW_hy, gb_y = grads\n        gY, gH = aux_data['gY'], aux_data['gH']\n\n        case_result = []\n        if case['id'] == 1:\n            # TC1: Check tail padding gradients\n            case_result.append(loss)\n            tail_grads_are_zero = True\n            for b in range(B):\n                len_b = lengths[b]\n                if len_b  T:\n                    # Check gY (direct output grad) and gH (total hidden state grad)\n                    gY_tail = gY[b, len_b:]\n                    gH_tail = gH[b, len_b:]\n                    if np.any(np.abs(gY_tail) > tol) or np.any(np.abs(gH_tail) > tol):\n                        tail_grads_are_zero = False\n                        break\n            case_result.append(tail_grads_are_zero)\n\n        elif case['id'] == 2:\n            # TC2: Check zero-length sequence gradients\n            case_result.append(loss)\n            zero_len_grads_are_zero = True\n            # Sequence 0 has length 0\n            if np.any(np.abs(gY[0]) > tol) or np.any(np.abs(gH[0]) > tol):\n                zero_len_grads_are_zero = False\n            case_result.append(zero_len_grads_are_zero)\n\n        elif case['id'] == 3:\n            # TC3: Check zero loss and zero gradients for fully masked batch\n            case_result.append(loss) # Should be 0.0\n            all_grads_zero = True\n            for g in grads:\n                if np.any(np.abs(g) > tol):\n                    all_grads_zero = False\n                    break\n            case_result.append(all_grads_zero)\n\n        elif case['id'] == 4:\n            # TC4: Check tail padding and mid-sequence mask\n            case_result.append(loss)\n            \n            # 1. Check tail padding on sequence 1 (length 3)\n            tail_grads_are_zero = True\n            len_b1 = lengths[1]\n            if len_b1  T:\n                gY_tail = gY[1, len_b1:]\n                gH_tail = gH[1, len_b1:]\n                if np.any(np.abs(gY_tail) > tol) or np.any(np.abs(gH_tail) > tol):\n                    tail_grads_are_zero = False\n            case_result.append(tail_grads_are_zero)\n\n            # 2. Check mid-sequence mask on sequence 0 at t=2\n            mid_mask_t = 2\n            gY_mid_is_zero = np.all(np.abs(gY[0, mid_mask_t])  tol)\n            gH_mid_is_nonzero = np.any(np.abs(gH[0, mid_mask_t]) > tol)\n            mid_mask_behavior_correct = gY_mid_is_zero and gH_mid_is_nonzero\n            case_result.append(mid_mask_behavior_correct)\n            \n        results.append(case_result)\n\n    # Final print statement\n    print(str(results).replace(\" \", \"\"))\n\ndef rnn_forward_backward(X, R, M, params):\n    \"\"\"\n    Performs a full forward and backward pass for the specified RNN.\n    \"\"\"\n    W_xh, W_hh, b_h, W_hy, b_y = params\n    B, T, d_x = X.shape\n    d_h = W_hh.shape[0]\n    d_y = W_hy.shape[1]\n\n    # --- Forward Pass ---\n    H_padded = np.zeros((B, T + 1, d_h))\n    H = H_padded[:, 1:, :]  # H is a view into H_padded\n    Y = np.zeros((B, T, d_y))\n\n    for t in range(T):\n        h_prev = H_padded[:, t, :]\n        A_t = X[:, t, :] @ W_xh + h_prev @ W_hh + b_h\n        H[:, t, :] = np.tanh(A_t)\n        Y[:, t, :] = H[:, t, :] @ W_hy + b_y\n\n    # --- Loss Calculation ---\n    num_valid_steps = np.sum(M)\n    if num_valid_steps == 0:\n        loss = 0.0\n        grads = [np.zeros_like(p) for p in params]\n        aux_data = {'gY': np.zeros_like(Y), 'gH': np.zeros_like(H)}\n        return loss, grads, aux_data\n\n    errors = Y - R\n    loss_per_step = 0.5 * np.sum(errors**2, axis=2)\n    masked_loss_sum = np.sum(loss_per_step * M)\n    loss = masked_loss_sum / num_valid_steps\n\n    # --- Backward Pass ---\n    # Initialize parameter gradients\n    gW_xh, gW_hh, gb_h, gW_hy, gb_y = [np.zeros_like(p) for p in params]\n\n    # Gradient of loss w.r.t. network outputs Y\n    gY = errors / num_valid_steps\n    gY *= M[:, :, np.newaxis]  # Apply mask\n\n    # Gradients for output layer\n    gW_hy = np.einsum('bth,bty->hy', H, gY)\n    gb_y = np.sum(gY, axis=(0, 1))\n\n    # Gradients for recurrent layer (BPTT)\n    gH = np.zeros_like(H)\n    gh_carry = np.zeros((B, d_h))\n    gH_from_Y = gY @ W_hy.T\n    \n    # First pass: compute total gradient w.r.t hidden states H\n    for t in reversed(range(T)):\n        gh_total_t = gH_from_Y[:, t, :] + gh_carry\n        gH[:, t, :] = gh_total_t\n        ga_t = gh_total_t * (1 - H[:, t, :]**2)\n        gh_carry = ga_t @ W_hh\n        \n    # Second pass: compute parameter gradients from gH\n    gA = gH * (1 - H**2)\n    gW_xh = np.einsum('btx,bth->xh', X, gA)\n    gW_hh = np.einsum('bth,btH->hH', H_padded[:, :-1, :], gA)\n    gb_h = np.sum(gA, axis=(0, 1))\n\n    grads = (gW_xh, gW_hh, gb_h, gW_hy, gb_y)\n    aux_data = {'gY': gY, 'gH': gH}\n\n    return loss, grads, aux_data\n\nsolve()\n```"
        }
    ]
}