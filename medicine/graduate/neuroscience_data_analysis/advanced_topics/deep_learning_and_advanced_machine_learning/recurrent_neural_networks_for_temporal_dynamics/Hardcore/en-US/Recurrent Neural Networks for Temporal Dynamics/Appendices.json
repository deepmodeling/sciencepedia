{
    "hands_on_practices": [
        {
            "introduction": "The ability of Recurrent Neural Networks to model temporal sequences is greatly enhanced by architectures like the Long Short-Term Memory (LSTM). To move beyond a black-box understanding, this exercise requires a manual forward pass through an LSTM cell, offering a concrete look at how its internal gates—forget, input, and output—interact to regulate the flow of information through the cell state. By working through the calculations with specifically chosen parameters, you will gain a tangible intuition for how an LSTM can selectively retain or discard information over time .",
            "id": "4189572",
            "problem": "Consider a one-dimensional Long Short-Term Memory (LSTM) recurrent neural network used to model the latent temporal dynamics of a trial-averaged neural signal in a neuroscience data analysis setting. The LSTM at time step $t$ updates its gates and states according to the standard definitions: the input gate $i_t = \\sigma(z_{i,t})$, forget gate $f_t = \\sigma(z_{f,t})$, output gate $o_t = \\sigma(z_{o,t})$, and cell candidate $g_t = \\tanh(z_{g,t})$, where $\\sigma(u) = \\frac{1}{1+\\exp(-u)}$ is the logistic function and $\\tanh(u)$ is the hyperbolic tangent. The cell state and hidden state evolve as $c_t = f_t \\, c_{t-1} + i_t \\, g_t$ and $h_t = o_t \\, \\tanh(c_t)$. The pre-activations are affine functions of the current input $x_t$ and previous hidden state $h_{t-1}$: $z_{i,t} = W_{xi}\\,x_t + W_{hi}\\,h_{t-1} + b_i$, $z_{f,t} = W_{xf}\\,x_t + W_{hf}\\,h_{t-1} + b_f$, $z_{o,t} = W_{xo}\\,x_t + W_{ho}\\,h_{t-1} + b_o$, and $z_{g,t} = W_{xg}\\,x_t + W_{hg}\\,h_{t-1} + b_g$.\n\nYou are given a sequence of inputs over $T=3$ time steps designed to contain a transient at $t=2$: $x_1 = 0$, $x_2 = \\ln(3)$, and $x_3 = 0$. The initial states are $c_0 = 0$ and $h_0 = 0$. The LSTM parameters are scalars and specified as follows:\n- $W_{xi} = 1$, $W_{hi} = 0$, $b_i = 0$,\n- $W_{xf} = -\\frac{\\ln(2)}{\\ln(3)}$, $W_{hf} = 0$, $b_f = \\ln(4)$,\n- $W_{xo} = 0$, $W_{ho} = 0$, $b_o = \\ln(2)$,\n- $W_{xg} = 0$, $W_{hg} = 0$, $b_g = \\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)$.\n\nUsing only the definitions provided, compute the forward pass to obtain the gates and states for $t=1,2,3$ and use this to explicitly verify how the forget gate $f_t$ modulates the retention of the transient input. Report, as your final result, the scalar contribution to the final cell state $c_3$ that arises solely from the transient input at $t=2$ (i.e., the portion of $c_3$ attributable to $x_2$ through the LSTM dynamics). Provide the exact value; no rounding is required.",
            "solution": "The problem requires the computation of a forward pass through a one-dimensional Long Short-Term Memory (LSTM) network for $3$ time steps and the calculation of a specific contribution to the final cell state. The problem is well-posed and all necessary parameters and initial conditions are provided.\n\nFirst, we substitute the given scalar parameters into the general LSTM equations. The pre-activation functions are:\n$z_{i,t} = W_{xi}\\,x_t + W_{hi}\\,h_{t-1} + b_i = (1)x_t + (0)h_{t-1} + 0 = x_t$\n$z_{f,t} = W_{xf}\\,x_t + W_{hf}\\,h_{t-1} + b_f = \\left(-\\frac{\\ln(2)}{\\ln(3)}\\right)x_t + (0)h_{t-1} + \\ln(4) = -\\frac{\\ln(2)}{\\ln(3)}x_t + 2\\ln(2)$\n$z_{o,t} = W_{xo}\\,x_t + W_{ho}\\,h_{t-1} + b_o = (0)x_t + (0)h_{t-1} + \\ln(2) = \\ln(2)$\n$z_{g,t} = W_{xg}\\,x_t + W_{hg}\\,h_{t-1} + b_g = (0)x_t + (0)h_{t-1} + \\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)$\n\nThe gates and cell candidate are updated as follows:\n$i_t = \\sigma(z_{i,t}) = \\sigma(x_t)$\n$f_t = \\sigma(z_{f,t}) = \\sigma\\left(-\\frac{\\ln(2)}{\\ln(3)}x_t + 2\\ln(2)\\right)$\n$o_t = \\sigma(z_{o,t}) = \\sigma(\\ln(2))$\n$g_t = \\tanh(z_{g,t}) = \\tanh\\left(\\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)\\right)$\n\nThe output gate $o_t$ and the cell candidate $g_t$ are constant for all time steps. Let us compute their values. The sigmoid function is $\\sigma(u) = (1+\\exp(-u))^{-1}$.\n$o_t = \\sigma(\\ln(2)) = \\frac{1}{1+\\exp(-\\ln(2))} = \\frac{1}{1+\\frac{1}{2}} = \\frac{1}{\\frac{3}{2}} = \\frac{2}{3}$.\n$g_t = \\tanh\\left(\\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)\\right) = \\frac{1}{2}$.\n\nThe cell state and hidden state updates are:\n$c_t = f_t c_{t-1} + i_t g_t = f_t c_{t-1} + \\frac{1}{2} i_t$\n$h_t = o_t \\tanh(c_t) = \\frac{2}{3} \\tanh(c_t)$\n\nThe initial conditions are $c_0 = 0$ and $h_0 = 0$. The input sequence is $x_1 = 0$, $x_2 = \\ln(3)$, and $x_3 = 0$. We proceed with the forward pass.\n\n**Time step $t=1$:**\nInput: $x_1 = 0$.\nThe pre-activations for the input and forget gates are:\n$z_{i,1} = x_1 = 0$\n$z_{f,1} = -\\frac{\\ln(2)}{\\ln(3)}(0) + 2\\ln(2) = 2\\ln(2) = \\ln(4)$\nThe gate activations are:\n$i_1 = \\sigma(0) = \\frac{1}{1+\\exp(0)} = \\frac{1}{2}$\n$f_1 = \\sigma(\\ln(4)) = \\frac{1}{1+\\exp(-\\ln(4))} = \\frac{1}{1+\\frac{1}{4}} = \\frac{4}{5}$\nThe cell and hidden states are updated:\n$c_1 = f_1 c_0 + i_1 g_1 = \\left(\\frac{4}{5}\\right)(0) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4}$\n$h_1 = \\frac{2}{3}\\tanh(c_1) = \\frac{2}{3}\\tanh\\left(\\frac{1}{4}\\right)$\n\n**Time step $t=2$:**\nInput: $x_2 = \\ln(3)$. This is the specified transient input.\nThe pre-activations for the input and forget gates are:\n$z_{i,2} = x_2 = \\ln(3)$\n$z_{f,2} = -\\frac{\\ln(2)}{\\ln(3)} (\\ln(3)) + 2\\ln(2) = -\\ln(2) + 2\\ln(2) = \\ln(2)$\nThe gate activations are:\n$i_2 = \\sigma(\\ln(3)) = \\frac{1}{1+\\exp(-\\ln(3))} = \\frac{1}{1+\\frac{1}{3}} = \\frac{3}{4}$\n$f_2 = \\sigma(\\ln(2)) = \\frac{1}{1+\\exp(-\\ln(2))} = \\frac{1}{1+\\frac{1}{2}} = \\frac{2}{3}$\nThe cell state is updated:\n$c_2 = f_2 c_1 + i_2 g_2 = \\left(\\frac{2}{3}\\right)\\left(\\frac{1}{4}\\right) + \\left(\\frac{3}{4}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{6} + \\frac{3}{8} = \\frac{4}{24} + \\frac{9}{24} = \\frac{13}{24}$\nThe hidden state is $h_2 = \\frac{2}{3}\\tanh(c_2) = \\frac{2}{3}\\tanh\\left(\\frac{13}{24}\\right)$.\n\n**Time step $t=3$:**\nInput: $x_3 = 0$.\nThe pre-activations for the input and forget gates are the same as for $t=1$:\n$z_{i,3} = x_3 = 0 \\implies i_3 = \\frac{1}{2}$\n$z_{f,3} = \\ln(4) \\implies f_3 = \\frac{4}{5}$\nThe final cell state $c_3$ is:\n$c_3 = f_3 c_2 + i_3 g_3 = \\left(\\frac{4}{5}\\right)\\left(\\frac{13}{24}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{13}{5 \\cdot 6} + \\frac{1}{4} = \\frac{13}{30} + \\frac{1}{4} = \\frac{26}{60} + \\frac{15}{60} = \\frac{41}{60}$\nThe final hidden state is $h_3 = \\frac{2}{3}\\tanh(c_3) = \\frac{2}{3}\\tanh\\left(\\frac{41}{60}\\right)$.\n\nThe problem asks for the contribution to the final cell state $c_3$ from the transient input at $t=2$. To determine this, we can express $c_3$ purely in terms of the inputs at each step.\n$c_3 = f_3 c_2 + i_3 g_3$\nSubstituting $c_2 = f_2 c_1 + i_2 g_2$:\n$c_3 = f_3 (f_2 c_1 + i_2 g_2) + i_3 g_3 = f_3 f_2 c_1 + f_3 i_2 g_2 + i_3 g_3$\nSubstituting $c_1 = f_1 c_0 + i_1 g_1$ and using $c_0=0$:\n$c_3 = f_3 f_2 (i_1 g_1) + f_3 i_2 g_2 + i_3 g_3$\n\nThis expression decomposes $c_3$ into a sum of contributions from the inputs at each time step:\n- Contribution from $x_1$: $f_3 f_2 i_1 g_1$\n- Contribution from $x_2$: $f_3 i_2 g_2$\n- Contribution from $x_3$: $i_3 g_3$\n\nThe term $i_2 g_2$ represents the new information added to the cell state at $t=2$ due to the input $x_2$. The forget gate at the next step, $f_3$, modulates the retention of this information into the state $c_3$. Thus, the portion of $c_3$ attributable to $x_2$ is $f_3 i_2 g_2$.\n\nWe calculate the value of this term using the gate values we found:\n$f_3 = \\frac{4}{5}$\n$i_2 = \\frac{3}{4}$\n$g_2 = \\frac{1}{2}$\n\nContribution from $x_2$ to $c_3 = f_3 \\, i_2 \\, g_2 = \\left(\\frac{4}{5}\\right) \\left(\\frac{3}{4}\\right) \\left(\\frac{1}{2}\\right) = \\frac{12}{40} = \\frac{3}{10}$.\n\nThis calculation explicitly demonstrates how the forget gate $f_3 = \\frac{4}{5}$ acts on the information $i_2 g_2 = \\frac{3}{8}$ that was written to the cell due to the transient input, retaining $80\\%$ of it in the next time step. The transient input $x_2 = \\ln(3)$ induced a smaller forget gate value $f_2 = \\frac{2}{3}$ compared to when the input was zero ($f_1 = f_3 = \\frac{4}{5}$), causing the network to \"forget\" more of its past state $c_1$ when the significant event occurred.\n\nThe required final result is the scalar contribution to $c_3$ arising from the input at $t=2$.",
            "answer": "$$\\boxed{\\frac{3}{10}}$$"
        },
        {
            "introduction": "Training an RNN on real-world neuroscience data requires handling sequences of varying lengths, such as neural recordings from trials of different durations. This practice addresses this core implementation challenge by using masking to create uniform batches while ensuring that the loss and gradients are computed only over valid time steps. You will implement a full forward and backward pass, using backpropagation through time (BPTT) to correctly update network parameters and verify that gradients from padded portions of the sequence do not corrupt the learning process .",
            "id": "4189567",
            "problem": "You will implement a complete forward and backward pass for a single-layer Recurrent Neural Network (RNN) with additive inputs and recurrent connections, operating on minibatches of sequences with variable lengths using masking. The goal is to construct masked sequences, compute a masked mean squared error loss, derive and implement gradients using the chain rule, and numerically verify that gradients do not leak from padded timesteps at the ends of sequences. Additionally, you will test and demonstrate the distinct behavior of masked timesteps that occur mid-sequence, which may still carry gradient due to causal propagation, and handle the boundary case of sequences with zero valid timesteps.\n\nStarting point definitions and rules:\n- Use the following forward recurrence for a single-layer Recurrent Neural Network (RNN) with hyperbolic tangent activation. For batch index $b$ and time index $t$, the hidden state is\n$$\n\\mathbf{h}_{b,t} = \\tanh\\left(\\mathbf{a}_{b,t}\\right), \\quad \\mathbf{a}_{b,t} = \\mathbf{x}_{b,t}\\mathbf{W}_{xh} + \\mathbf{h}_{b,t-1}\\mathbf{W}_{hh} + \\mathbf{b}_h,\n$$\nwith initial state $\\mathbf{h}_{b,-1} = \\mathbf{0}$. The output is\n$$\n\\mathbf{y}_{b,t} = \\mathbf{h}_{b,t}\\mathbf{W}_{hy} + \\mathbf{b}_y.\n$$\n- Let the per-timestep per-sequence loss be the half-squared error\n$$\n\\ell_{b,t} = \\frac{1}{2}\\left\\|\\mathbf{y}_{b,t} - \\mathbf{r}_{b,t}\\right\\|_2^2,\n$$\nand let $\\mathbf{m}_{b,t} \\in \\{0,1\\}$ be a binary mask indicating valid timesteps ($1$) and padded timesteps ($0$). The masked loss is the average over all valid timesteps,\n$$\nL = \\frac{\\sum_{b=1}^{B}\\sum_{t=0}^{T-1} \\mathbf{m}_{b,t} \\, \\ell_{b,t}}{\\sum_{b=1}^{B}\\sum_{t=0}^{T-1} \\mathbf{m}_{b,t}},\n$$\nwith the convention that if the denominator equals $0$, then $L = 0$ and all gradients are zero.\n- Use the standard chain rule of calculus for differentiation and the derivative of the hyperbolic tangent, namely\n$$\n\\frac{\\partial}{\\partial \\mathbf{a}} \\tanh(\\mathbf{a}) = \\mathbf{1} - \\tanh(\\mathbf{a}) \\odot \\tanh(\\mathbf{a}),\n$$\nwhere $\\odot$ denotes element-wise multiplication.\n- All computations must be performed in real-valued arithmetic. No physical units or angles are involved, and all quantities are dimensionless.\n\nYour program must:\n- Construct the masked minibatch sequences and compute the masked loss $L$.\n- Compute exact gradients with respect to all parameters $\\mathbf{W}_{xh}$, $\\mathbf{W}_{hh}$, $\\mathbf{W}_{hy}$, $\\mathbf{b}_h$, and $\\mathbf{b}_y$ using backpropagation through time with masking.\n- Numerically verify that for tail-padded timesteps (zeros in the mask after the final valid timestep of a sequence), both the direct output gradient at those timesteps and the hidden-state gradient at those timesteps are zero to within a specified absolute tolerance.\n- Demonstrate that masking mid-sequence timesteps zeroes the direct output gradient at those timesteps while permitting non-zero hidden-state gradients at those timesteps due to causal propagation from future valid timesteps.\n- Correctly handle the boundary case where all timesteps in the batch are masked (zero denominator), by producing zero loss and zero gradients.\n\nModel dimensions and fixed parameters:\n- Input dimension $d_x = 2$, hidden dimension $d_h = 3$, and output dimension $d_y = 2$.\n- Parameter matrices and vectors are fixed and given by\n$$\n\\mathbf{W}_{xh} = \\begin{bmatrix} 0.1  -0.2  0.3 \\\\ 0.05  0.4  -0.1 \\end{bmatrix}, \\quad\n\\mathbf{W}_{hh} = \\begin{bmatrix} 0.2  0.0  -0.1 \\\\ 0.1  0.3  0.0 \\\\ -0.2  0.05  0.25 \\end{bmatrix},\n$$\n$$\n\\mathbf{b}_h = \\begin{bmatrix} 0.0  0.0  0.0 \\end{bmatrix}, \\quad\n\\mathbf{W}_{hy} = \\begin{bmatrix} 0.3  -0.2 \\\\ -0.1  0.4 \\\\ 0.2  0.1 \\end{bmatrix}, \\quad\n\\mathbf{b}_y = \\begin{bmatrix} 0.0  0.0 \\end{bmatrix}.\n$$\n- For all test cases, inputs and targets are constructed deterministically as follows. For batch size $B$ and sequence length $T$, define for each $b \\in \\{0, \\ldots, B-1\\}$ and each $t \\in \\{0, \\ldots, T-1\\}$:\n$$\n\\mathbf{x}_{b,t} = 0.1 \\cdot \\begin{bmatrix} b+1 \\\\ t+1 \\end{bmatrix}, \\quad\n\\mathbf{r}_{b,t} = \\begin{bmatrix} 0.05\\,(b+1)\\,(t+1) \\\\ -0.02\\,(t+1) \\end{bmatrix}.\n$$\nMasks are specified uniquely per test case as below.\n\nTest suite:\n- Test Case $1$ (happy path with tail padding):\n  - Batch size $B = 3$, sequence length $T = 5$, per-sequence valid lengths $\\left[5, 3, 4\\right]$.\n  - Mask $\\mathbf{m}_{b,t} = 1$ for $t  \\text{length}_b$ and $\\mathbf{m}_{b,t} = 0$ otherwise.\n  - Output: a list containing the masked loss $L$ (float) and a boolean indicating whether all gradients at tail-padded timesteps are numerically zero within absolute tolerance $\\varepsilon = 10^{-12}$.\n- Test Case $2$ (includes a zero-length sequence):\n  - Batch size $B = 2$, sequence length $T = 4$, per-sequence valid lengths $\\left[0, 4\\right]$.\n  - Mask defined as above. The first sequence has zero valid timesteps.\n  - Output: a list containing $L$ (float) and a boolean indicating whether all gradients originating from the zero-length sequence are numerically zero within $\\varepsilon = 10^{-12}$.\n- Test Case $3$ (all sequences zero-length, zero denominator):\n  - Batch size $B = 2$, sequence length $T = 3$, per-sequence valid lengths $\\left[0, 0\\right]$.\n  - Mask defined as above.\n  - Output: a list containing $L$ (float, which must be $0.0$) and a boolean indicating whether all parameter gradients are numerically zero within $\\varepsilon = 10^{-12}$.\n- Test Case $4$ (mixed mid-sequence mask and tail padding):\n  - Batch size $B = 2$, sequence length $T = 5$, per-sequence valid lengths $\\left[5, 3\\right]$.\n  - For the first sequence ($b = 0$), set $\\mathbf{m}_{0,t} = 1$ for all $t$ except $\\mathbf{m}_{0,2} = 0$ (masked mid-sequence timestep).\n  - For the second sequence ($b = 1$), use tail padding with $\\mathbf{m}_{1,t} = 1$ for $t \\in \\{0,1,2\\}$ and $\\mathbf{m}_{1,t} = 0$ otherwise.\n  - Output: a list containing $L$ (float), a boolean indicating whether gradients at tail-padded timesteps are numerically zero within $\\varepsilon = 10^{-12}$, and a boolean indicating whether the masked mid-sequence timestep for the first sequence carries a non-zero hidden-state gradient due to causal propagation while its direct output gradient is zero.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case must produce a list as specified above, and the program should aggregate these per-test-case lists into a single outer list. For example, the output must look like $[\\text{case1\\_list},\\text{case2\\_list},\\text{case3\\_list},\\text{case4\\_list}]$.\n\nAll numeric equality checks for gradients at masked timesteps must use the absolute tolerance $\\varepsilon = 10^{-12}$, and all outputs must be either booleans, integers, floats, or lists of these fundamental types.",
            "solution": "The problem requires the implementation and validation of a complete forward and backward pass for a single-layer Recurrent Neural Network (RNN) with masking for variable-length sequences. The solution involves deriving the gradient expressions using backpropagation through time (BPTT), implementing the algorithm, and verifying its correctness on several test cases that probe the behavior of masking.\n\n### 1. Model and Loss Function\n\nThe RNN dynamics are defined by the following equations for batch index $b$ and time index $t$:\nThe pre-activation of the hidden layer:\n$$ \\mathbf{a}_{b,t} = \\mathbf{x}_{b,t}\\mathbf{W}_{xh} + \\mathbf{h}_{b,t-1}\\mathbf{W}_{hh} + \\mathbf{b}_h $$\nThe hidden state, with initial state $\\mathbf{h}_{b,-1} = \\mathbf{0}$:\n$$ \\mathbf{h}_{b,t} = \\tanh(\\mathbf{a}_{b,t}) $$\nThe network output:\n$$ \\mathbf{y}_{b,t} = \\mathbf{h}_{b,t}\\mathbf{W}_{hy} + \\mathbf{b}_y $$\nThe loss function is the masked mean squared error. The per-timestep loss is:\n$$ \\ell_{b,t} = \\frac{1}{2}\\left\\|\\mathbf{y}_{b,t} - \\mathbf{r}_{b,t}\\right\\|_2^2 $$\nThe total loss $L$ is a normalized sum over valid timesteps, indicated by a binary mask $\\mathbf{m}_{b,t} \\in \\{0,1\\}$:\n$$ L = \\frac{\\sum_{b,t} \\mathbf{m}_{b,t} \\, \\ell_{b,t}}{\\sum_{b,t} \\mathbf{m}_{b,t}} = \\frac{1}{N_{valid}} \\sum_{b,t} \\mathbf{m}_{b,t} \\, \\ell_{b,t} $$\nwhere $N_{valid} = \\sum_{b,t} \\mathbf{m}_{b,t}$. If $N_{valid} = 0$, then $L=0$ and all gradients are zero.\n\n### 2. Gradient Derivation (Backpropagation Through Time)\n\nTo train the network, we must compute the gradient of the loss $L$ with respect to all parameters: $\\mathbf{W}_{xh}$, $\\mathbf{W}_{hh}$, $\\mathbf{b}_h$, $\\mathbf{W}_{hy}$, and $\\mathbf{b}_y$. We use the chain rule.\n\nThe gradient of $L$ with respect to the output $\\mathbf{y}_{b,t}$ is the starting point for backpropagation.\n$$ \\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} = \\frac{\\partial L}{\\partial \\ell_{b,t}} \\frac{\\partial \\ell_{b,t}}{\\partial \\mathbf{y}_{b,t}} = \\frac{\\mathbf{m}_{b,t}}{N_{valid}} (\\mathbf{y}_{b,t} - \\mathbf{r}_{b,t}) $$\nLet's denote this gradient as $\\mathbf{gY}_{b,t}$. The mask $\\mathbf{m}_{b,t}$ ensures that padded timesteps ($m_{b,t}=0$) contribute nothing to the gradients.\n\n#### 2.1. Gradients for the Output Layer ($\\mathbf{W}_{hy}, \\mathbf{b}_y$)\n\nThe gradients for the output layer parameters are calculated by summing the contributions from all timesteps.\n$$ \\frac{\\partial L}{\\partial \\mathbf{W}_{hy}} = \\sum_{b,t} \\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{W}_{hy}} = \\sum_{b,t} \\mathbf{h}_{b,t}^T \\mathbf{gY}_{b,t} $$\n$$ \\frac{\\partial L}{\\partial \\mathbf{b}_y} = \\sum_{b,t} \\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{b}_y} = \\sum_{b,t} \\mathbf{gY}_{b,t} $$\nThese gradients can be computed after the forward pass, once all $\\mathbf{h}_{b,t}$ and $\\mathbf{gY}_{b,t}$ are known.\n\n#### 2.2. Gradients for the Recurrent Layer\n\nThe gradient with respect to a hidden state $\\mathbf{h}_{b,t}$ has two components: one from the output at the same timestep, $\\mathbf{y}_{b,t}$, and one propagated from the subsequent hidden state, $\\mathbf{h}_{b,t+1}$. This leads to a backward recurrence.\n\nLet $\\mathbf{gH}_{b,t} = \\frac{\\partial L}{\\partial \\mathbf{h}_{b,t}}$ and $\\mathbf{gA}_{b,t} = \\frac{\\partial L}{\\partial \\mathbf{a}_{b,t}}$. The recurrence relation for $\\mathbf{gH}_{b,t}$ is:\n$$ \\mathbf{gH}_{b,t} = \\underbrace{\\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{h}_{b,t}}}_{\\text{from output}} + \\underbrace{\\frac{\\partial L}{\\partial \\mathbf{a}_{b,t+1}} \\frac{\\partial \\mathbf{a}_{b,t+1}}{\\partial \\mathbf{h}_{b,t}}}_{\\text{from future}} $$\nEvaluating the terms:\n$$ \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{h}_{b,t}} = \\mathbf{W}_{hy}^T \\quad \\text{and} \\quad \\frac{\\partial \\mathbf{a}_{b,t+1}}{\\partial \\mathbf{h}_{b,t}} = \\mathbf{W}_{hh} $$\nSo the recurrence becomes:\n$$ \\mathbf{gH}_{b,t} = \\mathbf{gY}_{b,t} \\mathbf{W}_{hy}^T + \\mathbf{gA}_{b,t+1} \\mathbf{W}_{hh} $$\nWe also need the gradient with respect to the pre-activation $\\mathbf{a}_{b,t}$, which is found via the chain rule and the derivative of $\\tanh$:\n$$ \\mathbf{gA}_{b,t} = \\frac{\\partial L}{\\partial \\mathbf{h}_{b,t}} \\frac{\\partial \\mathbf{h}_{b,t}}{\\partial \\mathbf{a}_{b,t}} = \\mathbf{gH}_{b,t} \\odot (\\mathbf{1} - \\mathbf{h}_{b,t} \\odot \\mathbf{h}_{b,t}) $$\nwhere $\\odot$ is element-wise multiplication.\n\nThe BPTT algorithm proceeds by iterating backward in time from $t=T-1$ to $t=0$. We initialize the gradient flowing from the future as zero, i.e., $\\mathbf{gA}_{b,T} = \\mathbf{0}$.\n\nFor $t = T-1, \\dots, 0$:\n1. Compute $\\mathbf{gH}_{b,t} = \\mathbf{gY}_{b,t} \\mathbf{W}_{hy}^T + \\mathbf{gA}_{b,t+1} \\mathbf{W}_{hh}$. (where $\\mathbf{gA}_{b,T}=\\mathbf{0}$)\n2. Compute $\\mathbf{gA}_{b,t} = \\mathbf{gH}_{b,t} \\odot (\\mathbf{1} - \\mathbf{h}_{b,t}^2)$.\n3. Accumulate gradients for the recurrent layer parameters:\n   $$ \\frac{\\partial L}{\\partial \\mathbf{W}_{xh}} += \\sum_{b} \\mathbf{x}_{b,t}^T \\mathbf{gA}_{b,t} $$\n   $$ \\frac{\\partial L}{\\partial \\mathbf{W}_{hh}} += \\sum_{b} \\mathbf{h}_{b,t-1}^T \\mathbf{gA}_{b,t} \\quad (\\text{with } \\mathbf{h}_{b,-1} = \\mathbf{0})$$\n   $$ \\frac{\\partial L}{\\partial \\mathbf{b}_h} += \\sum_{b} \\mathbf{gA}_{b,t} $$\n\n### 3. Implementation and Verification Strategy\n\nThe overall algorithm is as follows:\n1.  **Forward Pass**: Iterate from $t=0$ to $T-1$, computing and storing all $\\mathbf{h}_{b,t}$ and $\\mathbf{y}_{b,t}$.\n2.  **Loss Calculation**: Compute the total number of valid timesteps $N_{valid}$ and the final loss $L$. If $N_{valid}=0$, set $L=0$ and all gradients to zero.\n3.  **Backward Pass**:\n    a. Compute the initial output gradients $\\mathbf{gY}_{b,t}$, applying the mask.\n    b. Compute and store gradients for the output layer parameters $\\mathbf{W}_{hy}$ and $\\mathbf{b}_y$.\n    c. Initialize a total hidden-state gradient tensor, $\\mathbf{gH}$, to zeros.\n    d. Iterate from $t=T-1$ down to $0$, calculating $\\mathbf{gH}_{:,t,:}$ using the recurrence. This captures the full gradient w.r.t each hidden state.\n    e. Compute the pre-activation gradients $\\mathbf{gA}$ from the final $\\mathbf{gH}$ and the stored activations.\n    f. Compute the recurrent layer parameter gradients ($\\mathbf{W}_{xh}$, $\\mathbf{W}_{hh}$, $\\mathbf{b}_h$) via tensor contractions (e.g., `einsum`) over the full $\\mathbf{gA}$.\n\n#### Masking Behavior Verification:\n-   **Tail Padding**: For a sequence $b$ of length $L_b  T$, all timesteps $t \\ge L_b$ are padded. We must verify that for these timesteps, $\\mathbf{gY}_{b,t}$ and $\\mathbf{gH}_{b,t}$ are numerically zero. This demonstrates that gradients do not \"leak\" into padded regions.\n-   **Mid-sequence Masking**: A timestep $t$ can be masked even if it's surrounded by valid timesteps. In this case, the direct output gradient $\\mathbf{gY}_{b,t}$ will be zero. However, the hidden state gradient $\\mathbf{gH}_{b,t}$ can be non-zero because of gradients flowing back from future valid timesteps ($t+1, t+2, \\dots$). We must verify this distinct behavior.\n-   **Zero-Length Sequences**: For sequences with length $0$, the mask is all zeros. This implies all associated gradients ($\\mathbf{gY}_{b,:}$, $\\mathbf{gH}_{b,:}$) are zero. If all sequences in a batch have length zero, $N_{valid}=0$, and all final parameter gradients must be zero.\n\nThese principles are implemented in the provided Python code, which systematically executes each test case and performs the required numerical checks.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the RNN forward/backward pass.\n    \"\"\"\n\n    # Fixed parameters\n    W_xh = np.array([[0.1, -0.2, 0.3], [0.05, 0.4, -0.1]])\n    W_hh = np.array([[0.2, 0.0, -0.1], [0.1, 0.3, 0.0], [-0.2, 0.05, 0.25]])\n    b_h = np.array([0.0, 0.0, 0.0])\n    W_hy = np.array([[0.3, -0.2], [-0.1, 0.4], [0.2, 0.1]])\n    b_y = np.array([0.0, 0.0])\n    params = (W_xh, W_hh, b_h, W_hy, b_y)\n    \n    tol = 1e-12\n\n    test_cases = [\n        {'B': 3, 'T': 5, 'lengths': [5, 3, 4], 'id': 1},\n        {'B': 2, 'T': 4, 'lengths': [0, 4], 'id': 2},\n        {'B': 2, 'T': 3, 'lengths': [0, 0], 'id': 3},\n        {'B': 2, 'T': 5, 'lengths': [5, 3], 'id': 4}\n    ]\n\n    results = []\n    for case in test_cases:\n        B, T, lengths = case['B'], case['T'], case['lengths']\n        \n        # --- Data and Mask Generation ---\n        X = np.zeros((B, T, W_xh.shape[0]))\n        R = np.zeros((B, T, W_hy.shape[1]))\n        for b in range(B):\n            for t in range(T):\n                X[b, t] = 0.1 * np.array([b + 1, t + 1])\n                R[b, t] = np.array([0.05 * (b + 1) * (t + 1), -0.02 * (t + 1)])\n\n        M = np.zeros((B, T))\n        if case['id'] == 4:\n            # Special mask for TC4\n            M[0] = np.array([1, 1, 0, 1, 1])\n            M[1, :lengths[1]] = 1\n        else:\n            for b in range(B):\n                M[b, :lengths[b]] = 1\n\n        # --- Run RNN pass ---\n        loss, grads, aux_data = rnn_forward_backward(X, R, M, params)\n        gW_xh, gW_hh, gb_h, gW_hy, gb_y = grads\n        gY, gH = aux_data['gY'], aux_data['gH']\n\n        case_result = []\n        if case['id'] == 1:\n            # TC1: Check tail padding gradients\n            case_result.append(loss)\n            tail_grads_are_zero = True\n            for b in range(B):\n                len_b = lengths[b]\n                if len_b  T:\n                    # Check gY (direct output grad) and gH (total hidden state grad)\n                    gY_tail = gY[b, len_b:]\n                    gH_tail = gH[b, len_b:]\n                    if np.any(np.abs(gY_tail)  tol) or np.any(np.abs(gH_tail)  tol):\n                        tail_grads_are_zero = False\n                        break\n            case_result.append(tail_grads_are_zero)\n\n        elif case['id'] == 2:\n            # TC2: Check zero-length sequence gradients\n            case_result.append(loss)\n            zero_len_grads_are_zero = True\n            # Sequence 0 has length 0\n            if np.any(np.abs(gY[0])  tol) or np.any(np.abs(gH[0])  tol):\n                zero_len_grads_are_zero = False\n            case_result.append(zero_len_grads_are_zero)\n\n        elif case['id'] == 3:\n            # TC3: Check zero loss and zero gradients for fully masked batch\n            case_result.append(loss) # Should be 0.0\n            all_grads_zero = True\n            for g in grads:\n                if np.any(np.abs(g)  tol):\n                    all_grads_zero = False\n                    break\n            case_result.append(all_grads_zero)\n\n        elif case['id'] == 4:\n            # TC4: Check tail padding and mid-sequence mask\n            case_result.append(loss)\n            \n            # 1. Check tail padding on sequence 1 (length 3)\n            tail_grads_are_zero = True\n            len_b1 = lengths[1]\n            if len_b1  T:\n                gY_tail = gY[1, len_b1:]\n                gH_tail = gH[1, len_b1:]\n                if np.any(np.abs(gY_tail)  tol) or np.any(np.abs(gH_tail)  tol):\n                    tail_grads_are_zero = False\n            case_result.append(tail_grads_are_zero)\n\n            # 2. Check mid-sequence mask on sequence 0 at t=2\n            mid_mask_t = 2\n            gY_mid_is_zero = np.all(np.abs(gY[0, mid_mask_t])  tol)\n            gH_mid_is_nonzero = np.any(np.abs(gH[0, mid_mask_t])  tol)\n            mid_mask_behavior_correct = gY_mid_is_zero and gH_mid_is_nonzero\n            case_result.append(mid_mask_behavior_correct)\n            \n        results.append(case_result)\n\n    # Final print statement\n    print(str(results).replace(\" \", \"\"))\n\ndef rnn_forward_backward(X, R, M, params):\n    \"\"\"\n    Performs a full forward and backward pass for the specified RNN.\n    \"\"\"\n    W_xh, W_hh, b_h, W_hy, b_y = params\n    B, T, d_x = X.shape\n    d_h = W_hh.shape[0]\n    d_y = W_hy.shape[1]\n\n    # --- Forward Pass ---\n    H_padded = np.zeros((B, T + 1, d_h))\n    H = H_padded[:, 1:, :]  # H is a view into H_padded\n    Y = np.zeros((B, T, d_y))\n\n    for t in range(T):\n        h_prev = H_padded[:, t, :]\n        A_t = X[:, t, :] @ W_xh + h_prev @ W_hh + b_h\n        H[:, t, :] = np.tanh(A_t)\n        Y[:, t, :] = H[:, t, :] @ W_hy + b_y\n\n    # --- Loss Calculation ---\n    num_valid_steps = np.sum(M)\n    if num_valid_steps == 0:\n        loss = 0.0\n        grads = [np.zeros_like(p) for p in params]\n        aux_data = {'gY': np.zeros_like(Y), 'gH': np.zeros_like(H)}\n        return loss, grads, aux_data\n\n    errors = Y - R\n    loss_per_step = 0.5 * np.sum(errors**2, axis=2)\n    masked_loss_sum = np.sum(loss_per_step * M)\n    loss = masked_loss_sum / num_valid_steps\n\n    # --- Backward Pass ---\n    # Initialize parameter gradients\n    gW_xh, gW_hh, gb_h, gW_hy, gb_y = [np.zeros_like(p) for p in params]\n\n    # Gradient of loss w.r.t. network outputs Y\n    gY = errors / num_valid_steps\n    gY *= M[:, :, np.newaxis]  # Apply mask\n\n    # Gradients for output layer\n    gW_hy = np.einsum('bth,bty-hy', H, gY)\n    gb_y = np.sum(gY, axis=(0, 1))\n\n    # Gradients for recurrent layer (BPTT)\n    gH = np.zeros_like(H)\n    gh_carry = np.zeros((B, d_h))\n    gH_from_Y = gY @ W_hy.T\n    \n    # First pass: compute total gradient w.r.t hidden states H\n    for t in reversed(range(T)):\n        gh_total_t = gH_from_Y[:, t, :] + gh_carry\n        gH[:, t, :] = gh_total_t\n        ga_t = gh_total_t * (1 - H[:, t, :]**2)\n        gh_carry = ga_t @ W_hh\n        \n    # Second pass: compute parameter gradients from gH\n    gA = gH * (1 - H**2)\n    gW_xh = np.einsum('btx,bth-xh', X, gA)\n    gW_hh = np.einsum('bth,btH-hH', H_padded[:, :-1, :], gA)\n    gb_h = np.sum(gA, axis=(0, 1))\n\n    grads = (gW_xh, gW_hh, gb_h, gW_hy, gb_y)\n    aux_data = {'gY': gY, 'gH': gH}\n\n    return loss, grads, aux_data\n\nsolve()\n```"
        },
        {
            "introduction": "After training, an RNN can be understood as a nonlinear dynamical system that generates the complex temporal patterns seen in neural data. This exercise provides hands-on experience with this powerful analytical framework, guiding you to linearize the RNN dynamics around a fixed point and analyze the resulting Jacobian matrix. By identifying the stable and unstable manifolds from the Jacobian's eigenstructure, you can predict and simulate the local geometry of the state space, revealing the fundamental dynamic motifs learned by the network .",
            "id": "4189514",
            "problem": "You are given a continuous-time two-dimensional ($2$D) rate-based Recurrent Neural Network (RNN) defined by the dynamical system\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x}) + \\mathbf{b},\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^2$ is the state vector, $\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ is the recurrent connectivity matrix, $\\mathbf{b} \\in \\mathbb{R}^2$ is a constant input bias, and $\\boldsymbol{\\phi}(\\mathbf{x})$ is an elementwise nonlinearity with components $\\phi(x_i) = \\tanh(x_i)$ for $i \\in \\{1,2\\}$. Assume $\\mathbf{b} = \\mathbf{0}$ so that $\\mathbf{x}^\\star = \\mathbf{0}$ is always a fixed point. The Jacobian of the vector field at the fixed point is\n$$\n\\mathbf{J} = \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star} = -\\mathbf{I} + \\mathbf{W}\\,\\mathrm{diag}\\left(\\phi'(x_1^\\star), \\phi'(x_2^\\star)\\right),\n$$\nand since $\\phi'(0) = 1$, this simplifies to $\\mathbf{J} = -\\mathbf{I} + \\mathbf{W}$.\n\nStarting from the fundamental definitions of fixed points and linearization for autonomous dynamical systems, and using the well-established fact that the local behavior near a hyperbolic fixed point is governed by the eigenstructure of the Jacobian, your task is to:\n- Compute the eigenvalues and eigenvectors of $\\mathbf{J}$ at $\\mathbf{x}^\\star$.\n- Classify the local invariant manifolds: the stable manifold has dimension equal to the number of eigenvalues of $\\mathbf{J}$ with strictly negative real part, the unstable manifold has dimension equal to the number with strictly positive real part, and the center manifold has dimension equal to the number with real part equal to zero.\n- Construct initialization directions for trajectory simulation by using the eigenvectors of $\\mathbf{J}$. For real eigenvectors, use the eigenvector and its negative. For complex-conjugate eigenpairs in two dimensions, form a real basis using the real and imaginary parts of one complex eigenvector, and include both signs of each real direction.\n- Simulate forward trajectories initialized at $\\mathbf{x}(0) = \\mathbf{x}^\\star + \\varepsilon\\,\\mathbf{d}$ for each direction $\\mathbf{d}$ constructed above, with a small displacement magnitude $\\varepsilon$, over a finite time horizon $T$. Use the continuous-time dynamics given above, with $\\boldsymbol{\\phi}(\\mathbf{x}) = \\tanh(\\mathbf{x})$.\n\nUse the following fixed simulation parameters for all test cases:\n- Displacement magnitude $\\varepsilon = 10^{-3}$.\n- Time horizon $T = 3$ (time unit is an abstract modeling unit; you do not need to output any physical unit).\n- Spectral classification tolerance $\\delta = 10^{-8}$, applied to the real parts of eigenvalues: real part $ -\\delta$ is stable, $ \\delta$ is unstable, and $|\\cdot| \\le \\delta$ is center.\n\nFor each test case, compute:\n1. The integer $n_s$, the dimension of the stable manifold.\n2. The integer $n_u$, the dimension of the unstable manifold.\n3. The integer $n_c$, the dimension of the center manifold.\n4. A boolean $q$ indicating whether all simulated trajectories along directions associated with stable eigenvalues strictly decrease in Euclidean distance to $\\mathbf{x}^\\star$ over time $T$, and all simulated trajectories along directions associated with unstable eigenvalues strictly increase in Euclidean distance to $\\mathbf{x}^\\star$ over time $T$. If there are no stable or no unstable directions, treat the corresponding condition as vacuously true.\n\nYour program must implement the above and produce a single line of output containing the results for all test cases as a comma-separated list of lists, each inner list in the form $[n_s, n_u, n_c, q]$, enclosed in square brackets. For example, the output format should be like\n$$\n[[n_{s,1}, n_{u,1}, n_{c,1}, q_1],[n_{s,2}, n_{u,2}, n_{c,2}, q_2],\\dots]\n$$\nwith no additional text.\n\nTest Suite:\nUse the following five recurrent connectivity matrices $\\mathbf{W}$, each paired implicitly with $\\mathbf{b} = \\mathbf{0}$ and $\\boldsymbol{\\phi}(x) = \\tanh(x)$:\n- Case $1$ (saddle): $\\mathbf{W} = \\begin{bmatrix} 1.5  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$.\n- Case $2$ (stable node): $\\mathbf{W} = \\begin{bmatrix} 0.2  0.0 \\\\ 0.0  0.3 \\end{bmatrix}$.\n- Case $3$ (unstable node): $\\mathbf{W} = \\begin{bmatrix} 1.3  0.0 \\\\ 0.0  1.4 \\end{bmatrix}$.\n- Case $4$ (stable spiral): $\\mathbf{W} = \\begin{bmatrix} 0.5  1.8 \\\\ -1.8  0.5 \\end{bmatrix}$.\n- Case $5$ (center, purely imaginary linearization): $\\mathbf{W} = \\begin{bmatrix} 1.0  1.0 \\\\ -1.0  1.0 \\end{bmatrix}$.\n\nYour program must be self-contained, must not read any input, and must apply the above procedure to the provided test suite. The final output must be printed exactly as specified, on a single line.",
            "solution": "The problem requires an analysis of the local dynamics of a two-dimensional continuous-time Recurrent Neural Network (RNN) around a fixed point. The analysis involves linearization, eigen-decomposition of the resulting Jacobian matrix, classification of invariant manifolds, and numerical verification of the dynamics along eigendirections.\n\nThe dynamical system is given by the ordinary differential equation (ODE):\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x}) + \\mathbf{b}\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^2$ is the state, $\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ is the connectivity matrix, $\\mathbf{b} \\in \\mathbb{R}^2$ is a bias, and $\\boldsymbol{\\phi}$ is the elementwise hyperbolic tangent activation function, $\\phi(x_i) = \\tanh(x_i)$.\n\nFirst, we establish the framework for the analysis based on fundamental principles of dynamical systems.\n\n**1. Fixed Point and Linearization**\n\nA fixed point $\\mathbf{x}^\\star$ of the system is a state where the dynamics cease, i.e., $\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}^\\star) = \\mathbf{0}$. We are given that the bias $\\mathbf{b} = \\mathbf{0}$. We can verify that the origin $\\mathbf{x}^\\star = \\mathbf{0}$ is a fixed point:\n$$\n\\mathbf{f}(\\mathbf{0}) = -\\mathbf{0} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{0}) = \\mathbf{0} + \\mathbf{W}\\,\\mathbf{0} = \\mathbf{0}\n$$\nsince $\\tanh(0) = 0$.\n\nThe behavior of the system in the vicinity of a fixed point can be approximated by a linear system. This is achieved by taking the first-order Taylor expansion of $\\mathbf{f}(\\mathbf{x})$ around $\\mathbf{x}^\\star$. Let $\\mathbf{x} = \\mathbf{x}^\\star + \\delta\\mathbf{x}$, where $\\delta\\mathbf{x}$ is a small perturbation.\n$$\n\\frac{d(\\mathbf{x}^\\star + \\delta\\mathbf{x})}{dt} = \\frac{d\\delta\\mathbf{x}}{dt} \\approx \\mathbf{f}(\\mathbf{x}^\\star) + \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star} \\delta\\mathbf{x}\n$$\nSince $\\mathbf{f}(\\mathbf{x}^\\star) = \\mathbf{0}$, the linearized dynamics are governed by the Jacobian matrix $\\mathbf{J} = \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star}$:\n$$\n\\frac{d\\delta\\mathbf{x}}{dt} \\approx \\mathbf{J}\\,\\delta\\mathbf{x}\n$$\nThe Jacobian of $\\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x})$ is:\n$$\n\\mathbf{J} = \\frac{\\partial}{\\partial \\mathbf{x}} (-\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x})) = -\\mathbf{I} + \\mathbf{W}\\,\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}}\n$$\nwhere $\\mathbf{I}$ is the $2 \\times 2$ identity matrix and $\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}}$ is the Jacobian of the elementwise activation function. For $\\phi(x_i) = \\tanh(x_i)$, the derivative is $\\phi'(x_i) = 1 - \\tanh^2(x_i) = \\mathrm{sech}^2(x_i)$. The Jacobian of $\\boldsymbol{\\phi}$ is a diagonal matrix:\n$$\n\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}} = \\mathrm{diag}(\\phi'(x_1), \\phi'(x_2))\n$$\nEvaluating at the fixed point $\\mathbf{x}^\\star = \\mathbf{0}$, we have $\\phi'(0) = \\mathrm{sech}^2(0) = 1$. Thus, the diagonal matrix becomes the identity matrix, $\\mathbf{I}$. The Jacobian at the origin simplifies to:\n$$\n\\mathbf{J} = -\\mathbf{I} + \\mathbf{W}\\,\\mathbf{I} = -\\mathbf{I} + \\mathbf{W}\n$$\n\n**2. Eigenstructure and Stability Classification**\n\nThe Hartman-Grobman theorem states that for a hyperbolic fixed point (one for which the Jacobian has no eigenvalues with zero real part), the qualitative behavior of the nonlinear system near the fixed point is topologically equivalent to that of its linearization. The stability of the fixed point is determined by the eigenvalues $\\{\\lambda_i\\}$ of $\\mathbf{J}$.\n- If $\\mathrm{Re}(\\lambda_i)  0$ for all $i$, the fixed point is stable.\n- If $\\mathrm{Re}(\\lambda_i)  0$ for at least one $i$, the fixed point is unstable.\n- If $\\mathrm{Re}(\\lambda_i) = 0$ for some $i$ and $\\mathrm{Re}(\\lambda_j) \\le 0$ for all other $j$, the fixed point is non-hyperbolic, and linear analysis may be inconclusive about stability without examining higher-order terms. However, we can still classify the invariant manifolds.\n\nThe dimensions of the stable, unstable, and center manifolds ($n_s$, $n_u$, $n_c$) are determined by counting the eigenvalues of $\\mathbf{J}$ based on the sign of their real parts. Using the specified tolerance $\\delta = 10^{-8}$:\n- $n_s$ is the number of eigenvalues $\\lambda_i$ with $\\mathrm{Re}(\\lambda_i)  -\\delta$.\n- $n_u$ is the number of eigenvalues $\\lambda_i$ with $\\mathrm{Re}(\\lambda_i)  \\delta$.\n- $n_c$ is the number of eigenvalues $\\lambda_i$ with $|\\mathrm{Re}(\\lambda_i)| \\le \\delta$.\nFor a $2$D system, we must have $n_s + n_u + n_c = 2$.\n\n**3. Initialization Directions from Eigenvectors**\n\nThe eigenvectors of $\\mathbf{J}$ span the invariant subspaces of the linear system. By initiating trajectories along these directions, we can observe the behavior characteristic of each manifold. Let $\\{\\lambda_i, \\mathbf{v}_i\\}$ be the eigenpairs of $\\mathbf{J}$.\n- **Real Eigenvectors**: If an eigenvector $\\mathbf{v}$ is real, it defines a line through the origin that is invariant under the linear flow. We test the dynamics by initializing trajectories at $\\mathbf{x}(0) = \\varepsilon \\mathbf{v}$ and $\\mathbf{x}(0) = -\\varepsilon \\mathbf{v}$ for a small $\\varepsilon$.\n- **Complex Eigenvectors**: If $\\mathbf{J}$ is real, its complex eigenvalues come in conjugate pairs, $\\lambda, \\bar{\\lambda}$, with corresponding complex conjugate eigenvectors $\\mathbf{v}, \\bar{\\mathbf{v}}$. Let $\\mathbf{v} = \\mathbf{a} + i\\mathbf{b}$. The real vectors $\\mathbf{a}$ and $\\mathbf{b}$ form a basis for the two-dimensional invariant subspace associated with this pair of eigenvalues. We test the dynamics within this plane by initializing trajectories along four directions: $\\mathbf{x}(0) = \\pm\\varepsilon\\mathbf{a}$ and $\\mathbf{x}(0) = \\pm\\varepsilon\\mathbf{b}$.\n\n**4. Numerical Simulation and Verification**\n\nTo verify that the nonlinear dynamics conform to the linear prediction for small perturbations, we numerically integrate the original ODE, $\\frac{d\\mathbf{x}}{dt} = -\\mathbf{x} + \\mathbf{W}\\,\\tanh(\\mathbf{x})$, for each constructed direction $\\mathbf{d}$. The initial condition is $\\mathbf{x}(0) = \\varepsilon \\mathbf{d}$ with $\\varepsilon = 10^{-3}$, and the simulation runs for a time horizon of $T = 3$.\n\nWe then compute the boolean flag $q$. For each trajectory, we calculate the Euclidean distance to the fixed point, $\\|\\mathbf{x}(t)\\|$, at multiple time points.\n- For a direction $\\mathbf{d}$ associated with a stable eigenvalue ($\\mathrm{Re}(\\lambda)  -\\delta$), the distance $\\|\\mathbf{x}(t)\\|$ must strictly decrease over the entire simulation interval.\n- For a direction $\\mathbf{d}$ associated with an unstable eigenvalue ($\\mathrm{Re}(\\lambda)  \\delta$), the distance $\\|\\mathbf{x}(t)\\|$ must strictly increase.\n\nThe flag $q$ is true if and only if all trajectories associated with stable directions exhibit strictly decreasing distance and all trajectories associated with unstable directions exhibit strictly increasing distance. If there are no stable or no unstable directions, the respective condition is vacuously true.\n\nThe implementation will proceed by iterating through each provided matrix $\\mathbf{W}$, performing these four steps, and collecting the results $[n_s, n_u, n_c, q]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the RNN dynamics analysis problem for a suite of test cases.\n    \"\"\"\n    \n    # Fixed simulation parameters\n    epsilon = 1e-3\n    T = 3.0\n    delta = 1e-8\n    \n    # Test suite of connectivity matrices W\n    test_cases = [\n        # Case 1 (saddle)\n        np.array([[1.5, 0.0], [0.0, 0.5]]),\n        # Case 2 (stable node)\n        np.array([[0.2, 0.0], [0.0, 0.3]]),\n        # Case 3 (unstable node)\n        np.array([[1.3, 0.0], [0.0, 1.4]]),\n        # Case 4 (stable spiral)\n        np.array([[0.5, 1.8], [-1.8, 0.5]]),\n        # Case 5 (center, purely imaginary linearization)\n        np.array([[1.0, 1.0], [-1.0, 1.0]]),\n    ]\n    \n    # List to store results for all test cases\n    all_results = []\n    \n    # ODE definition for the RNN dynamics\n    def f(t, x, W):\n        return -x + W @ np.tanh(x)\n\n    for W in test_cases:\n        # 1. Compute Jacobian at the origin\n        J = -np.eye(2) + W\n        \n        # 2. Compute eigenvalues and eigenvectors\n        eigvals, eigvecs = np.linalg.eig(J)\n        \n        # 3. Classify manifolds and count dimensions\n        ns, nu, nc = 0, 0, 0\n        real_parts = np.real(eigvals)\n        \n        for r in real_parts:\n            if r  -delta:\n                ns += 1\n            elif r  delta:\n                nu += 1\n            else:\n                nc += 1\n        \n        # 4. Construct initialization directions\n        stable_dirs = []\n        unstable_dirs = []\n        \n        # The eigenvalues are either both real or a complex conjugate pair\n        if np.iscomplexobj(eigvals):\n            # Complex conjugate pair\n            v = eigvecs[:, 0]\n            real_part = np.real(v)\n            imag_part = np.imag(v)\n            \n            # Normalize for consistency, although epsilon scaling dominates\n            if np.linalg.norm(real_part)  1e-9: real_part /= np.linalg.norm(real_part)\n            if np.linalg.norm(imag_part)  1e-9: imag_part /= np.linalg.norm(imag_part)\n\n            dirs_from_complex = [real_part, -real_part, imag_part, -imag_part]\n            \n            if real_parts[0]  -delta:\n                stable_dirs.extend(dirs_from_complex)\n            elif real_parts[0]  delta:\n                unstable_dirs.extend(dirs_from_complex)\n            # Center manifold directions are not checked for the 'q' flag\n        else:\n            # Two real eigenvalues\n            for i in range(2):\n                v = eigvecs[:, i]\n                # eig gives normalized eigenvectors\n                dirs_from_real = [v, -v]\n                \n                if real_parts[i]  -delta:\n                    stable_dirs.extend(dirs_from_real)\n                elif real_parts[i]  delta:\n                    unstable_dirs.extend(dirs_from_real)\n        \n        # 5. Simulate trajectories and verify behavior for 'q' flag\n        q = True\n        \n        # Check stable directions\n        if stable_dirs:\n            is_stable_ok = True\n            for d in stable_dirs:\n                if np.linalg.norm(d)  1e-9: continue\n                x0 = epsilon * d\n                sol = solve_ivp(f, [0, T], x0, args=(W,), dense_output=True, t_eval=np.linspace(0, T, num=100))\n                distances = np.linalg.norm(sol.y, axis=0)\n                # Check for strict decrease; allow for numerical noise near zero\n                if not np.all(np.diff(distances)  0):\n                    # Check if it settles at the origin making diffs zero.\n                    if not (np.all(distances[-10:]  1e-9) and distances[0]  distances[-1]):\n                         is_stable_ok = False\n                         break\n            if not is_stable_ok:\n                q = False\n\n        # Check unstable directions if stable ones passed\n        if q and unstable_dirs:\n            is_unstable_ok = True\n            for d in unstable_dirs:\n                if np.linalg.norm(d)  1e-9: continue\n                x0 = epsilon * d\n                sol = solve_ivp(f, [0, T], x0, args=(W,), dense_output=True, t_eval=np.linspace(0, T, num=100))\n                distances = np.linalg.norm(sol.y, axis=0)\n                if not np.all(np.diff(distances)  0):\n                    is_unstable_ok = False\n                    break\n            if not is_unstable_ok:\n                q = False\n                \n        all_results.append([ns, nu, nc, q])\n        \n    # Format the final output string\n    result_str = \",\".join([str(res) for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}