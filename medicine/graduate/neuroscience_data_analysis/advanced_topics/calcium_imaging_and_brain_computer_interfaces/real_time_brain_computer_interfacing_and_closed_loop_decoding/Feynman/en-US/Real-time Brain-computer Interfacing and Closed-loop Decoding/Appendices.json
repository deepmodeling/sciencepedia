{
    "hands_on_practices": [
        {
            "introduction": "Before we can decode complex neural patterns, we must first reliably detect the underlying signals amidst a sea of noise. This exercise grounds our work in fundamental physics by exploring the limits of what is measurable. By applying the principles of Johnson-Nyquist thermal noise, we will calculate the noise floor of a microelectrode and determine the minimum spike amplitude required for detection, a critical parameter for designing the front-end of any BCI system .",
            "id": "4188846",
            "problem": "A single microelectrode is used for real-time Brain-Computer Interface (BCI) closed-loop spike detection. The acquisition chain applies a causal bandpass filter with passband from $300\\,\\mathrm{Hz}$ to $3000\\,\\mathrm{Hz}$ to isolate spike activity. The microelectrode’s impedance in this band is dominated by a purely resistive component that can be modeled as a resistor of value $R = 5.0 \\times 10^{5}\\,\\Omega$. The preparation is at temperature $T = 310\\,\\mathrm{K}$. The spike detection module uses an amplitude threshold chosen to achieve a specified signal-to-noise ratio (SNR), defined here as the ratio of the spike peak amplitude to the root-mean-square (RMS) noise amplitude in the filtered signal. The SNR threshold is $\\gamma = 5.0$.\n\nStarting from fundamental definitions and the fluctuation-dissipation theorem for Johnson–Nyquist thermal noise, and assuming that the thermal noise is stationary and white across the passband and that the electrode’s real impedance is constant across the passband, derive the band-limited RMS thermal voltage noise at the electrode and then compute the minimum spike peak amplitude required to meet the SNR threshold in this closed-loop setting. Treat the total noise as arising solely from Johnson–Nyquist thermal noise of the electrode, and neglect any amplifier or $1/f$ noise contributions.\n\nLet $k_{B}$ denote Boltzmann’s constant. The filter bandwidth is $B = f_{2} - f_{1}$ with $f_{1} = 300\\,\\mathrm{Hz}$ and $f_{2} = 3000\\,\\mathrm{Hz}$. Round each final value to three significant figures. Express the final noise amplitude and the minimum detectable spike amplitude in microvolts. Your final answer must contain both numbers in the form of a single row matrix.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective. All givens are extracted and assessed.\n- **Givens**:\n    - Filter passband lower frequency: $f_{1} = 300\\,\\mathrm{Hz}$.\n    - Filter passband upper frequency: $f_{2} = 3000\\,\\mathrm{Hz}$.\n    - Electrode resistance: $R = 5.0 \\times 10^{5}\\,\\Omega$.\n    - Temperature: $T = 310\\,\\mathrm{K}$.\n    - Signal-to-Noise Ratio threshold: $\\gamma = 5.0$.\n    - Constant: Boltzmann’s constant, $k_{B}$.\n- **Assumptions**:\n    - The electrode impedance is purely resistive and constant across the passband.\n    - The noise is solely Johnson–Nyquist thermal noise.\n    - The thermal noise is stationary and white across the passband.\n    - Amplifier and $1/f$ noise are negligible.\n- **Validation Verdict**: The problem is valid. It is based on fundamental principles of physics (Johnson-Nyquist noise, fluctuation-dissipation theorem) and involves realistic parameters for a neurophysiological recording scenario. The problem is well-posed, with sufficient information provided for a unique solution.\n\nThe solution proceeds by first deriving the expression for the root-mean-square (RMS) thermal noise voltage and then using the given Signal-to-Noise Ratio (SNR) to find the minimum detectable spike amplitude.\n\nThe problem asks to start from the fluctuation-dissipation theorem. For a system in thermal equilibrium, this theorem connects the magnitude of spontaneous fluctuations to the system's dissipative properties. In the case of an electrical conductor with impedance $Z(f)$, the theorem yields the power spectral density (PSD) of the open-circuit thermal voltage noise. The one-sided PSD, $S_v(f)$, which accounts for power at positive frequencies, is given by:\n$$S_v(f) = 4 k_B T \\operatorname{Re}\\{Z(f)\\}$$\nwhere $k_{B}$ is the Boltzmann constant (approximately $1.380649 \\times 10^{-23}\\,\\mathrm{J/K}$), $T$ is the absolute temperature in Kelvin, and $\\operatorname{Re}\\{Z(f)\\}$ is the real (resistive) part of the impedance at frequency $f$.\n\nThe problem states that the electrode's impedance over the frequency band of interest is purely resistive and constant, with resistance $R = 5.0 \\times 10^{5}\\,\\Omega$. Therefore, $\\operatorname{Re}\\{Z(f)\\} = R$ for all frequencies in the passband. The PSD of the noise voltage is constant across this band:\n$$S_v(f) = 4 k_B T R$$\nThis confirms the problem's assumption that the noise is white across the passband.\n\nThe total mean-square noise voltage, $\\langle V_n^2 \\rangle$, within the filter's passband is found by integrating the PSD over the bandwidth of the filter, $B$. The bandwidth is $B = f_2 - f_1$.\n$$\\langle V_n^2 \\rangle = \\int_{f_1}^{f_2} S_v(f) \\, df$$\nSince $S_v(f)$ is constant over the integration interval $[f_1, f_2]$, we have:\n$$\\langle V_n^2 \\rangle = 4 k_B T R \\int_{f_1}^{f_2} df = 4 k_B T R (f_2 - f_1) = 4 k_B T R B$$\n\nThe RMS noise voltage, $V_{n, \\text{rms}}$, is the square root of the mean-square voltage:\n$$V_{n, \\text{rms}} = \\sqrt{\\langle V_n^2 \\rangle} = \\sqrt{4 k_B T R B}$$\nNow we substitute the given values into this expression.\nThe bandwidth is $B = f_2 - f_1 = 3000\\,\\mathrm{Hz} - 300\\,\\mathrm{Hz} = 2700\\,\\mathrm{Hz}$.\nThe values for the constants are:\n$k_B \\approx 1.380649 \\times 10^{-23}\\,\\mathrm{J/K}$\n$T = 310\\,\\mathrm{K}$\n$R = 5.0 \\times 10^{5}\\,\\Omega$\n\nSubstituting these values:\n$$\\langle V_n^2 \\rangle = 4 \\times (1.380649 \\times 10^{-23}\\,\\mathrm{J/K}) \\times (310\\,\\mathrm{K}) \\times (5.0 \\times 10^{5}\\,\\Omega) \\times (2700\\,\\mathrm{Hz})$$\n$$\\langle V_n^2 \\rangle \\approx 2.31289 \\times 10^{-11}\\,\\mathrm{V}^2$$\nThe RMS noise voltage is then:\n$$V_{n, \\text{rms}} = \\sqrt{2.31289 \\times 10^{-11}\\,\\mathrm{V}^2} \\approx 4.80925 \\times 10^{-6}\\,\\mathrm{V}$$\nTo express this value in microvolts ($1\\,\\mu\\mathrm{V} = 10^{-6}\\,\\mathrm{V}$):\n$$V_{n, \\text{rms}} \\approx 4.80925\\,\\mu\\mathrm{V}$$\nRounding to three significant figures, the RMS noise voltage is $4.81\\,\\mu\\mathrm{V}$.\n\nNext, we calculate the minimum spike peak amplitude, $A_{\\text{peak, min}}$, required to meet the SNR threshold $\\gamma$. The SNR is defined as the ratio of the spike peak amplitude to the RMS noise amplitude:\n$$\\gamma = \\frac{A_{\\text{peak}}}{V_{n, \\text{rms}}}$$\nTherefore, the minimum detectable peak amplitude is:\n$$A_{\\text{peak, min}} = \\gamma \\cdot V_{n, \\text{rms}}$$\nUsing the given SNR threshold of $\\gamma = 5.0$ and the unrounded value for $V_{n, \\text{rms}}$:\n$$A_{\\text{peak, min}} = 5.0 \\times (4.80925 \\times 10^{-6}\\,\\mathrm{V}) \\approx 2.404625 \\times 10^{-5}\\,\\mathrm{V}$$\nExpressing this in microvolts:\n$$A_{\\text{peak, min}} \\approx 24.04625\\,\\mu\\mathrm{V}$$\nRounding to three significant figures, the minimum spike peak amplitude is $24.0\\,\\mu\\mathrm{V}$.\n\nThe two required values are the RMS noise amplitude and the minimum detectable spike amplitude, both in microvolts and rounded to three significant figures.\nThe RMS noise amplitude is $4.81\\,\\mu\\mathrm{V}$.\nThe minimum detectable spike amplitude is $24.0\\,\\mu\\mathrm{V}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4.81 & 24.0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A real-time BCI relies on recursive algorithms, such as the Kalman filter, to continuously estimate a hidden brain state from noisy measurements. However, theoretical formulas can behave poorly when implemented on digital hardware with finite precision. This practice  delves into the numerical stability of the Kalman filter, demonstrating how the standard update can fail and guiding you through the derivation of a superior square-root implementation that guarantees robustness.",
            "id": "4188915",
            "problem": "Consider a one-dimensional real-time Brain-Computer Interface (BCI) measurement update step for a linear Gaussian decoder implemented as a Kalman Filter (KF). The latent cursor velocity $x$ has a prior $x \\sim \\mathcal{N}(m^{-}, p)$ with mean $m^{-}$ and variance $p > 0$. The measurement $y$ is given by $y = h x + v$, where $h \\in \\mathbb{R}$ is a scalar observation coefficient and $v \\sim \\mathcal{N}(0, r)$ is independent measurement noise with variance $r > 0$. The measurement update is computed on a digital signal processor with finite precision. Assume a standard floating-point rounding model in which each elementary arithmetic operation introduces a relative error bounded in magnitude by $u$, where $0  u \\ll 1$ is the unit roundoff.\n\nStarting only from the definitions of linear Gaussian models, Bayes’ rule for conditioning of jointly Gaussian variables, and basic facts from numerical linear algebra (including that orthogonal transformations preserve inner products and condition numbers), perform the following tasks:\n\n- Derive the exact posterior variance $p^{+}$ for the measurement update in terms of $p$, $h$, and $r$ using conditioning of jointly Gaussian variables.\n- Show how the “naïve” variance update written as a subtraction of two nearly equal quantities can suffer from catastrophic cancellation when $r \\ll h^{2} p$, and derive a first-order bound on the relative error of this subtraction in terms of $u$, $p$, $h$, and $r$.\n- Construct a square-root implementation that guarantees positive definiteness by maintaining an upper-triangular square root factor. Let $s^{-} > 0$ and $s_{R} > 0$ satisfy $p = (s^{-})^{2}$ and $r = (s_{R})^{2}$. Using an orthogonal transformation that triangularizes a $2 \\times 2$ block built from $s^{-}$, $s_{R}$, and $h$, derive the closed-form expression for the updated square-root variance $s^{+} > 0$ such that $p^{+} = (s^{+})^{2}$.\n- Express your final answer as a single analytic expression for $s^{+}$ in terms of $h$, $p$, and $r$.\n\nNo numerical evaluation is required. Your final answer must be a single closed-form analytic expression. Do not include any units.",
            "solution": "The problem is valid as it is scientifically grounded in Bayesian filtering theory and numerical analysis, well-posed, objective, and contains a complete and consistent setup for a standard problem in state estimation.\n\nThe problem asks for a series of derivations concerning the measurement update step of a scalar Kalman filter, culminating in a numerically stable square-root implementation. We will address each task in sequence.\n\nThe system is defined by a latent state $x$ and a measurement $y$:\n$x \\sim \\mathcal{N}(m^{-}, p)$ (prior distribution)\n$y = hx + v$, where $v \\sim \\mathcal{N}(0, r)$ and is independent of $x$.\n\nThe goal is to find the posterior distribution $x|y \\sim \\mathcal{N}(m^{+}, p^{+})$.\n\n**1. Derivation of the Exact Posterior Variance $p^{+}$**\n\nTo find the posterior variance, we first determine the joint distribution of the vector $\\begin{pmatrix} x \\\\ y \\end{pmatrix}$. Since $x$ and $y$ are jointly Gaussian, this distribution is fully characterized by its mean vector and covariance matrix.\n\nThe mean vector is:\n$$\nE\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} E[x] \\\\ E[hx+v] \\end{pmatrix} = \\begin{pmatrix} m^{-} \\\\ hE[x] + E[v] \\end{pmatrix} = \\begin{pmatrix} m^{-} \\\\ hm^{-} \\end{pmatrix}\n$$\n\nThe covariance matrix $\\Sigma$ is:\n$$\n\\Sigma = \\text{Cov}\\left(\\begin{pmatrix} x \\\\ y \\end{pmatrix}\\right) = \\begin{pmatrix} \\text{Var}(x)  \\text{Cov}(x,y) \\\\ \\text{Cov}(y,x)  \\text{Var}(y) \\end{pmatrix}\n$$\nThe components are:\n- $\\Sigma_{11} = \\text{Var}(x) = p$\n- $\\Sigma_{12} = \\text{Cov}(x,y) = \\text{Cov}(x, hx+v) = h\\text{Var}(x) + \\text{Cov}(x,v) = hp$ (since $x$ and $v$ are independent, $\\text{Cov}(x,v)=0$)\n- $\\Sigma_{21} = \\text{Cov}(y,x) = hp$\n- $\\Sigma_{22} = \\text{Var}(y) = \\text{Var}(hx+v) = h^2\\text{Var}(x) + \\text{Var}(v) = h^2p+r$ (since $x$ and $v$ are independent)\n\nThus, the joint distribution is:\n$$\n\\begin{pmatrix} x \\\\ y \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} m^{-} \\\\ hm^{-} \\end{pmatrix}, \\begin{pmatrix} p  hp \\\\ hp  h^2p+r \\end{pmatrix}\\right)\n$$\n\nThe posterior variance $p^{+} = \\text{Var}(x|y)$ is given by the formula for conditional variance of jointly Gaussian variables: $p^{+} = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}$.\nSubstituting the components:\n$$\np^{+} = p - (hp)(h^2p+r)^{-1}(hp) = p - \\frac{h^2p^2}{h^2p+r}\n$$\nSimplifying the expression by finding a common denominator:\n$$\np^{+} = \\frac{p(h^2p+r) - h^2p^2}{h^2p+r} = \\frac{h^2p^2 + pr - h^2p^2}{h^2p+r} = \\frac{pr}{h^2p+r}\n$$\nThis is the exact posterior variance.\n\n**2. Analysis of Catastrophic Cancellation**\n\nThe \"naïve\" update formula is the one involving subtraction:\n$$\np^{+} = p - \\frac{h^2p^2}{h^2p+r}\n$$\nCatastrophic cancellation occurs in finite-precision arithmetic when two nearly equal quantities are subtracted. This leads to a loss of relative precision in the result. In this context, the condition for this phenomenon is when the term being subtracted is close to $p$:\n$$\n\\frac{h^2p^2}{h^2p+r} \\approx p\n$$\nThis approximation holds when $r$ is much smaller than $h^2p$, i.e., $r \\ll h^{2} p$. In this regime, the measurement $y$ is highly informative about the state $x$, and the posterior variance $p^{+}$ is much smaller than the prior variance $p$.\n\nTo derive a first-order bound on the relative error, let's consider the general case of computing $z = a-b$ where $\\hat{a} = a(1+\\epsilon_a)$ and $\\hat{b} = b(1+\\epsilon_b)$ are the machine-representable approximations of $a$ and $b$. The computed result is $\\hat{z} = \\text{fl}(\\hat{a}-\\hat{b}) = (\\hat{a}-\\hat{b})(1+\\delta_s)$, where $|\\epsilon_a|$, $|\\epsilon_b|$, $|\\delta_s|$ are bounded by the unit roundoff $u$.\nThe absolute error is $\\Delta z = \\hat{z} - z = (a(1+\\epsilon_a) - b(1+\\epsilon_b))(1+\\delta_s) - (a-b)$. Expanding to first order in the errors:\n$$\n\\Delta z \\approx (a-b+a\\epsilon_a - b\\epsilon_b)(1+\\delta_s) - (a-b) \\approx a\\epsilon_a - b\\epsilon_b + (a-b)\\delta_s\n$$\nThe relative error is $\\frac{\\Delta z}{z} = \\frac{\\Delta z}{a-b} \\approx \\frac{a\\epsilon_a - b\\epsilon_b}{a-b} + \\delta_s$.\nThe magnitude of the relative error is bounded by:\n$$\n\\left|\\frac{\\Delta z}{z}\\right| \\lessapprox \\frac{|a||\\epsilon_a| + |b||\\epsilon_b|}{|a-b|} + |\\delta_s| \\le \\frac{|a|+|b|}{|a-b|}u + u \\approx \\frac{|a|+|b|}{|a-b|}u\n$$\nfor $a \\approx b$.\nIn our problem, $a = p$ and $b = \\frac{h^2p^2}{h^2p+r}$. When $r \\ll h^2p$:\n- $a = p  0$\n- $b = \\frac{h^2p^2}{h^2p(1+r/(h^2p))} = p\\left(1+\\frac{r}{h^2p}\\right)^{-1} \\approx p\\left(1-\\frac{r}{h^2p}\\right)$. Thus $b \\approx p$ and $b0$.\n- $|a|+|b| \\approx p+p = 2p$.\n- $|a-b| = p^{+} = \\frac{pr}{h^2p+r}$.\n\nSubstituting these into the error bound formula:\n$$\n\\text{Relative Error} \\approx \\frac{2p}{\\frac{pr}{h^2p+r}}u = \\frac{2p(h^2p+r)}{pr}u = \\frac{2(h^2p+r)}{r}u\n$$\nSince $r \\ll h^2p$, the term $h^2p+r \\approx h^2p$. The bound on the relative error is approximately:\n$$\n\\left|\\frac{\\hat{p}^{+}-p^{+}}{p^{+}}\\right| \\approx \\frac{2h^2p}{r} u\n$$\nAs $r \\to 0$, the ratio $\\frac{h^2p}{r} \\to \\infty$. This means the relative error in the computed posterior variance is amplified by a large factor, demonstrating catastrophic cancellation.\n\n**3. Derivation of the Square-Root Update**\n\nA numerically stable approach is to update the square root of the variance, a method which avoids subtraction and guarantees the positivity of the result. We are given $p=(s^{-})^2$ and $r=(s_R)^2$, and we seek $s^{+}$ such that $p^{+}=(s^{+})^2$.\n\nThis can be achieved using a QR decomposition of a specially constructed matrix. A standard method in square-root filtering (e.g., the Bierman update) involves post-multiplying a pre-array by an orthogonal matrix to introduce a zero. Consider the $2 \\times 2$ matrix built from the given square-root factors:\n$$\nM = \\begin{pmatrix} s_R  h s^{-} \\\\ 0  s^{-} \\end{pmatrix}\n$$\nWe seek a $2 \\times 2$ orthogonal matrix (a Givens rotation) $\\Theta = \\begin{pmatrix} c  -s \\\\ s  c \\end{pmatrix}$, where $c^2+s^2=1$, such that the post-multiplication $M\\Theta$ results in a lower-triangular matrix.\n$$\nM\\Theta = \\begin{pmatrix} s_R  h s^{-} \\\\ 0  s^{-} \\end{pmatrix} \\begin{pmatrix} c  -s \\\\ s  c \\end{pmatrix} = \\begin{pmatrix} c s_R + s h s^{-}  -s s_R + c h s^{-} \\\\ s s^{-}  c s^{-} \\end{pmatrix}\n$$\nTo make this matrix lower triangular, we must zero out the element at position $(1,2)$:\n$$\n-s s_R + c h s^{-} = 0 \\implies s s_R = c h s^{-}\n$$\nWe can satisfy this condition and $c^2+s^2=1$ by choosing:\n$$\nc = \\frac{s_R}{\\sqrt{(s_R)^2 + (h s^{-})^2}} \\quad \\text{and} \\quad s = \\frac{h s^{-}}{\\sqrt{(s_R)^2 + (h s^{-})^2}}\n$$\nWith these choices for $c$ and $s$, the resulting lower-triangular matrix, let's call it $L = M\\Theta$, has diagonal elements:\n- $L_{11} = c s_R + s h s^{-} = \\frac{(s_R)^2}{\\sqrt{\\dots}} + \\frac{(h s^{-})^2}{\\sqrt{\\dots}} = \\sqrt{(s_R)^2 + (h s^{-})^2}$\n- $L_{22} = c s^{-} = \\frac{s_R}{\\sqrt{(s_R)^2 + (h s^{-})^2}} s^{-}$\n\nAccording to the theory of square-root Kalman filtering, the $(2,2)$ element of this resulting triangular matrix is the updated square-root variance, $s^{+}$.\n$$\ns^{+} = L_{22} = \\frac{s_R s^{-}}{\\sqrt{(s_R)^2 + (h s^{-})^2}}\n$$\nThis expression for $s^{+}$ is numerically stable as it involves only multiplications, additions, and a square root, all of which are well-behaved operations. It inherently guarantees $s^{+}  0$ as long as $s_R, s^{-}  0$.\n\n**4. Final Expression for $s^{+}$**\n\nThe final task is to express $s^{+}$ in terms of $h$, $p$, and $r$. We substitute $s^{-} = \\sqrt{p}$ and $s_R = \\sqrt{r}$ into the derived expression for $s^{+}$:\n$$\ns^{+} = \\frac{\\sqrt{r} \\sqrt{p}}{\\sqrt{(\\sqrt{r})^2 + h^2 (\\sqrt{p})^2}} = \\frac{\\sqrt{pr}}{\\sqrt{r + h^2 p}}\n$$\nThis expression can be written more compactly as:\n$$\ns^{+} = \\sqrt{\\frac{pr}{h^2p+r}}\n$$\nThis is the closed-form expression for the updated square-root variance. Note that $(s^{+})^2 = \\frac{pr}{h^2p+r} = p^{+}$, confirming consistency with our initial derivation.",
            "answer": "$$\\boxed{\\sqrt{\\frac{pr}{h^{2}p+r}}}$$"
        },
        {
            "introduction": "The true power of a BCI is realized when the loop is closed—when the system's output modulates the very neural activity it measures. This feedback introduces the risk of instability, a critical concern for both performance and safety. This exercise  introduces the powerful separation principle from control theory, providing a rigorous method to analyze and ensure the stability of the entire neurostimulation-decoder system by examining its controller and estimator components independently.",
            "id": "4188920",
            "problem": "You are given a discrete-time linear time-invariant neural population model driven by a stimulation input and observed through a linear measurement process. The plant dynamics and observation equations are modeled as follows: for each integer time index $t \\ge 0$,\n$x_{t+1} = A x_t + B u_t + w_t$ and $y_t = C x_t + v_t$,\nwhere $x_t \\in \\mathbb{R}^{n}$ is the neural state, $u_t \\in \\mathbb{R}^{m_u}$ is the stimulation amplitude delivered by the controller, $y_t \\in \\mathbb{R}^{m_y}$ is the observation, and $w_t \\in \\mathbb{R}^{n}$ and $v_t \\in \\mathbb{R}^{m_y}$ are independent zero-mean Gaussian noise processes with covariances $W \\in \\mathbb{S}_{+}^{n}$ and $V \\in \\mathbb{S}_{+}^{m_y}$, respectively. The controller has access only to the observation $y_t$ and uses a causal state estimate $\\hat{x}_t$ generated by a linear estimator. The controller modulates the stimulation amplitude linearly via $u_t = -K \\hat{x}_t$, where $K \\in \\mathbb{R}^{m_u \\times n}$ is a feedback gain to be designed based on a quadratic cost with state and input weights. The estimator is chosen to be a steady-state linear minimum variance estimator with gain $L \\in \\mathbb{R}^{n \\times m_y}$ derived from noise covariances $W$ and $V$.\n\nTask 1 (Derivation): Starting from the fundamental definitions of discrete-time linear systems, Gaussian noise, linear quadratic regulation, and linear minimum variance estimation, derive conditions under which the combined neurostimulation–decoder closed-loop system is exponentially stable in the mean (that is, the expected state norm decays to zero exponentially fast for zero-mean noise). In your derivation, express the closed-loop deterministic mean dynamics of the augmented system in terms of the estimated state and the estimation error. Then, derive a necessary and sufficient condition for exponential stability in terms of the spectral radii of matrices constructed from $A$, $B$, $C$, $K$, and $L$. Your derivation must be presented in mathematical form only, without resorting to any domain-specific heuristics.\n\nTask 2 (Algorithm Design): Design an algorithm that, for each given test case below, constructs:\n- A stabilizing linear quadratic regulator (LQR) state feedback gain $K$ by solving the discrete-time algebraic Riccati equation associated with a quadratic cost $\\sum_{t=0}^{\\infty} \\left( x_t^\\top Q_c x_t + u_t^\\top R_c u_t \\right)$, with given $Q_c \\in \\mathbb{S}_{+}^{n}$ and $R_c \\in \\mathbb{S}_{++}^{m_u}$.\n- A steady-state linear minimum variance estimator (Kalman filter) gain $L$ by solving the discrete-time algebraic Riccati equation associated with the dual estimation problem using $W$ and $V$.\n\nThen, evaluate the spectral radius conditions for closed-loop stability derived in Task 1. For each test case, output a boolean indicating whether the combined system is exponentially stable.\n\nTest Suite:\nProvide a program that uses the following four test cases. All matrices are given explicitly. In each case, $n = 2$.\n\n- Test case $1$ (happy path, stabilizable and detectable):\n  - $A = \\begin{bmatrix} 1.1  0.1 \\\\ 0.0  0.95 \\end{bmatrix}$,\n  - $B = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix}$,\n  - $C = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $Q_c = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $R_c = \\begin{bmatrix} 0.1 \\end{bmatrix}$,\n  - $W = \\begin{bmatrix} 0.01  0.0 \\\\ 0.0  0.01 \\end{bmatrix}$,\n  - $V = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$.\n\n- Test case $2$ (boundary case with a marginally unstable mode that is stabilized via coupling; detectable):\n  - $A = \\begin{bmatrix} 1.0  0.2 \\\\ 0.0  0.9 \\end{bmatrix}$,\n  - $B = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $C = \\begin{bmatrix} 1.0  0.0 \\end{bmatrix}$,\n  - $Q_c = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $R_c = \\begin{bmatrix} 0.5 \\end{bmatrix}$,\n  - $W = \\begin{bmatrix} 0.02  0.0 \\\\ 0.0  0.02 \\end{bmatrix}$,\n  - $V = \\begin{bmatrix} 0.2 \\end{bmatrix}$.\n\n- Test case $3$ (not stabilizable: an unstable mode is uncontrollable):\n  - $A = \\begin{bmatrix} 1.2  0.0 \\\\ 0.0  0.9 \\end{bmatrix}$,\n  - $B = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $C = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $Q_c = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $R_c = \\begin{bmatrix} 0.1 \\end{bmatrix}$,\n  - $W = \\begin{bmatrix} 0.01  0.0 \\\\ 0.0  0.01 \\end{bmatrix}$,\n  - $V = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$.\n\n- Test case $4$ (not detectable: an unstable mode is unobservable):\n  - $A = \\begin{bmatrix} 1.1  0.0 \\\\ 0.0  0.95 \\end{bmatrix}$,\n  - $B = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix}$,\n  - $C = \\begin{bmatrix} 0.0  1.0 \\end{bmatrix}$,\n  - $Q_c = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $R_c = \\begin{bmatrix} 0.1 \\end{bmatrix}$,\n  - $W = \\begin{bmatrix} 0.01  0.0 \\\\ 0.0  0.01 \\end{bmatrix}$,\n  - $V = \\begin{bmatrix} 0.05 \\end{bmatrix}$.\n\nAnswer specification:\n- Your program must compute, for each test case, the boolean indicating whether the combined closed-loop system with the designed $K$ and $L$ is exponentially stable in the mean, according to the stability conditions you derive in Task $1$.\n- The final output must be a single line containing the results as a comma-separated list enclosed in square brackets. For example, if there are four booleans $b_1,b_2,b_3,b_4$, then the program should print exactly the line $[b_1,b_2,b_3,b_4]$ with no additional text or whitespace differences.\n- There are no physical units in this problem; all quantities are dimensionless.",
            "solution": "The problem requires the derivation of stability conditions for a discrete-time Linear-Quadratic-Gaussian (LQG) control system and the subsequent design and analysis of such a system for several test cases. The analysis is predicated upon the fundamental principles of linear systems theory, optimal control, and state estimation.\n\n### **Validation of the Problem Statement**\n\n**Step 1: Extract Givens**\n\n*   **System Model (Plant Dynamics):** $x_{t+1} = A x_t + B u_t + w_t$, for integer time index $t \\ge 0$.\n*   **System Model (Observation):** $y_t = C x_t + v_t$.\n*   **State and Signal Dimensions:** $x_t \\in \\mathbb{R}^{n}$, $u_t \\in \\mathbb{R}^{m_u}$, $y_t \\in \\mathbb{R}^{m_y}$.\n*   **System Matrices:** $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times m_u}$, $C \\in \\mathbb{R}^{m_y \\times n}$.\n*   **Noise Processes:** $w_t \\in \\mathbb{R}^{n}$ and $v_t \\in \\mathbb{R}^{m_y}$ are independent, zero-mean Gaussian processes with covariances $W \\in \\mathbb{S}_{+}^{n}$ and $V \\in \\mathbb{S}_{+}^{m_y}$, respectively. $\\mathbb{S}_{+}^{k}$ denotes the set of $k \\times k$ positive semi-definite matrices.\n*   **Controller:** $u_t = -K \\hat{x}_t$, where $K \\in \\mathbb{R}^{m_u \\times n}$ is the feedback gain and $\\hat{x}_t$ is a causal state estimate.\n*   **Estimator:** A steady-state linear minimum variance estimator (Kalman filter) with gain $L \\in \\mathbb{R}^{n \\times m_y}$.\n*   **LQR Cost Function:** $\\sum_{t=0}^{\\infty} \\left( x_t^\\top Q_c x_t + u_t^\\top R_c u_t \\right)$, with given $Q_c \\in \\mathbb{S}_{+}^{n}$ and $R_c \\in \\mathbb{S}_{++}^{m_u}$. $\\mathbb{S}_{++}^{k}$ denotes the set of $k \\times k$ positive definite matrices.\n*   **Tasks:**\n    1.  Derive conditions for exponential stability in the mean of the closed-loop system, expressed in terms of the spectral radii of matrices constructed from $A, B, C, K, L$.\n    2.  Design an algorithm to find $K$ and $L$ by solving discrete-time algebraic Riccati equations (DAREs) and evaluate the stability for given test cases.\n*   **Test Suite:** Four distinct test cases are provided with explicit numerical matrices for $A, B, C, Q_c, R_c, W, V$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is scientifically grounded, well-posed, and objective. It represents a standard and fundamental problem in modern control theory: the design and analysis of an LQG controller.\n1.  **Scientific Soundness:** The model is a discrete-time linear time-invariant system with Gaussian noise, and the controller is based on Linear Quadratic Regulation and Kalman filtering. These are canonical, well-established concepts in control engineering and applied mathematics.\n2.  **Formalizability:** The problem is stated in precise mathematical terms, and all components can be formalized and solved algorithmically.\n3.  **Completeness:** All necessary matrices and parameters ($A, B, C, Q_c, R_c, W, V$) are provided for the specific algorithmic task. The derivation task relies on first principles, not on missing information.\n4.  **Realism:** The problem is an idealized mathematical model but is not physically impossible or inconsistent.\n5.  **Well-Posedness:** The problem of finding stabilizing gains $K$ and $L$ is well-posed under conditions of stabilizability and detectability, respectively. The test suite is designed to explore these conditions, making the problem's structure a feature, not a flaw. The stability analysis leads to a unique conclusion for each case.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. A solution will be provided.\n\n### **Task 1: Derivation of Closed-Loop Stability Conditions**\n\nThe objective is to determine the conditions under which the expected state, $E[x_t]$, converges to zero. This is defined as exponential stability in the mean. The analysis relies on the separation principle of LQG control.\n\nWe begin with the system dynamics and the definitions of the controller and estimator.\nThe plant state evolves according to:\n$$x_{t+1} = A x_t + B u_t + w_t$$\nThe observation is:\n$$y_t = C x_t + v_t$$\nThe control law is based on the state estimate $\\hat{x}_t$:\n$$u_t = -K \\hat{x}_t$$\nThe state estimate $\\hat{x}_t$ is generated by a steady-state Kalman filter. The filter dynamics are given by a prediction step and a correction step. A combined one-step update equation for the estimate $\\hat{x}_t$ (interpreted as $\\hat{x}_{t|t}$) is:\n$$\\hat{x}_{t+1} = A \\hat{x}_t + B u_t + L (y_{t+1} - C(A \\hat{x}_t + B u_t))$$\nHowever, a more direct approach is to analyze the dynamics of the estimate and the estimation error. Let the estimation error be $e_t = x_t - \\hat{x}_t$. The standard steady-state a posteriori Kalman filter update equation is:\n$$\\hat{x}_{t+1} = A\\hat{x}_t + B u_t + L(y_t - C\\hat{x}_t)$$\nSubstituting $y_t = C x_t + v_t = C(\\hat{x}_t + e_t) + v_t$ and $u_t = -K\\hat{x}_t$:\n$$\\hat{x}_{t+1} = A\\hat{x}_t - B K \\hat{x}_t + L(C(\\hat{x}_t + e_t) + v_t - C\\hat{x}_t)$$\n$$\\hat{x}_{t+1} = (A - B K)\\hat{x}_t + L C e_t + L v_t$$\nThis equation describes the evolution of the state estimate.\n\nNext, we derive the dynamics for the estimation error $e_t = x_t - \\hat{x}_t$.\n$$e_{t+1} = x_{t+1} - \\hat{x}_{t+1}$$\nSubstitute the expressions for $x_{t+1}$ and $\\hat{x}_{t+1}$:\n$$x_{t+1} = A x_t - B K \\hat{x}_t + w_t = A(\\hat{x}_t + e_t) - B K \\hat{x}_t + w_t = (A-BK)\\hat{x}_t + A e_t + w_t$$\n$$e_{t+1} = ((A - BK)\\hat{x}_t + A e_t + w_t) - ((A - B K)\\hat{x}_t + L C e_t + L v_t)$$\n$$e_{t+1} = (A - L C)e_t + w_t - L v_t$$\nThis equation describes the evolution of the estimation error, which is independent of the state estimate $\\hat{x}_t$.\n\nWe can now form an augmented state vector $\\xi_t = \\begin{bmatrix} \\hat{x}_t \\\\ e_t \\end{bmatrix}$. The dynamics of this augmented system are:\n$$\n\\begin{bmatrix} \\hat{x}_{t+1} \\\\ e_{t+1} \\end{bmatrix} =\n\\begin{pmatrix} A-BK  LC \\\\ 0  A-LC \\end{pmatrix}\n\\begin{bmatrix} \\hat{x}_t \\\\ e_t \\end{bmatrix}\n+\n\\begin{pmatrix} L  0 \\\\ -L  I \\end{pmatrix}\n\\begin{bmatrix} v_t \\\\ w_t \\end{bmatrix}\n$$\nLet the closed-loop state-transition matrix be $A_{cl} = \\begin{pmatrix} A-BK  LC \\\\ 0  A-LC \\end{pmatrix}$.\n\nThe problem asks for exponential stability in the mean. We take the expectation of the augmented system dynamics. Since $w_t$ and $v_t$ are zero-mean processes, $E[w_t]=0$ and $E[v_t]=0$.\nLet $\\bar{\\xi}_t = E[\\xi_t] = \\begin{bmatrix} E[\\hat{x}_t] \\\\ E[e_t] \\end{bmatrix}$. The dynamics of the mean are:\n$$\n\\bar{\\xi}_{t+1} = E\\left[ A_{cl} \\xi_t + \\begin{pmatrix} L  0 \\\\ -L  I \\end{pmatrix} \\begin{bmatrix} v_t \\\\ w_t \\end{bmatrix} \\right] = A_{cl} E[\\xi_t] + 0 = A_{cl} \\bar{\\xi}_t\n$$\nThe mean of the original state is $E[x_t] = E[\\hat{x}_t + e_t]$. The system is exponentially stable in the mean if $\\bar{\\xi}_t \\to 0$ as $t \\to \\infty$ for any initial condition $\\bar{\\xi}_0$. For a discrete-time linear system $\\bar{\\xi}_{t+1} = A_{cl} \\bar{\\xi}_t$, this is true if and only if all eigenvalues of $A_{cl}$ lie strictly within the unit circle in the complex plane. This is equivalent to the condition that the spectral radius of $A_{cl}$, denoted $\\rho(A_{cl})$, is less than $1$.\n$$\\rho(A_{cl})  1$$\nSince $A_{cl}$ is a block upper triangular matrix, its eigenvalues are the union of the eigenvalues of its diagonal blocks, $(A-BK)$ and $(A-LC)$. Therefore, the condition $\\rho(A_{cl})  1$ is equivalent to the two separate conditions:\n$$\n\\rho(A-BK)  1 \\quad \\text{and} \\quad \\rho(A-LC)  1\n$$\nThis is the separation principle for discrete-time systems. The stability of the controller, governed by the eigenvalues of the regulator matrix $(A-BK)$, can be analyzed independently of the stability of the estimator, governed by the eigenvalues of the error dynamics matrix $(A-LC)$.\n\n### **Task 2: Algorithm Design**\n\nThe algorithm must first compute the controller gain $K$ and the estimator gain $L$, and then check the stability conditions derived in Task 1.\n\n**1. Design of the LQR Controller Gain $K$**\nThe gain $K$ is designed to minimize the quadratic cost $\\sum_{t=0}^{\\infty} ( x_t^\\top Q_c x_t + u_t^\\top R_c u_t )$. The optimal gain is given by:\n$$K = (R_c + B^\\top P_c B)^{-1} B^\\top P_c A$$\nwhere $P_c$ is the unique positive semi-definite solution to the discrete-time algebraic Riccati equation (DARE) for control:\n$$P_c = A^\\top P_c A - (A^\\top P_c B)(R_c + B^\\top P_c B)^{-1}(B^\\top P_c A) + Q_c$$\nA stabilizing solution exists if and only if the pair $(A, B)$ is stabilizable. If an uncontrollable mode of $A$ is unstable (has magnitude $\\ge 1$), no such stabilizing $K$ can be found.\n\n**2. Design of the Kalman Filter Gain $L$**\nThe gain $L$ is the steady-state gain for the linear minimum variance estimator (Kalman filter). By the principle of duality, this problem is equivalent to an LQR problem for a dual system. The gain $L$ is given by:\n$$L = P_e C^\\top (V + C P_e C^\\top)^{-1}$$\nwhere $P_e$ is the steady-state error covariance, which is the unique positive semi-definite solution to the DARE for filtering:\n$$P_e = A P_e A^\\top - (A P_e C^\\top)(V + C P_e C^\\top)^{-1}(C P_e A^\\top) + W$$\nTo solve this using a standard DARE solver designed for the control problem, we use the dual substitutions: $A \\to A^\\top$, $B \\to C^\\top$, $Q \\to W$, $R \\to V$. A stabilizing solution (i.e., one that makes $(A-LC)$ stable) exists if and only if the pair $(A, C)$ is detectable (or equivalently, $(A^\\top, C^\\top)$ is stabilizable). If an unobservable mode of $A$ is unstable (has magnitude $\\ge 1$), no such stabilizing $L$ can be found.\n\n**3. Overall Algorithm**\nFor each test case:\na. Attempt to solve the control DARE for $P_c$ using the matrices $A, B, Q_c, R_c$. If the solver fails (e.g., if $(A,B)$ is not stabilizable), conclude the system is unstable.\nb. If successful, compute the gain $K$ and the regulator matrix $A_{reg} = A - B K$. Calculate its spectral radius $\\rho(A_{reg})$.\nc. Attempt to solve the filtering DARE for $P_e$ using the dual matrices $A^\\top, C^\\top, W, V$. If the solver fails (e.g., if $(A,C)$ is not detectable), conclude the system is unstable.\nd. If successful, compute the gain $L$ and the estimator error matrix $A_{est} = A - L C$. Calculate its spectral radius $\\rho(A_{est})$.\ne. The combined system is exponentially stable in the mean if and only if both $\\rho(A_{reg})  1$ and $\\rho(A_{est})  1$. The algorithm will return a boolean indicating if this condition is met. If any step failed, the result is `False`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_discrete_are\n\ndef solve():\n    \"\"\"\n    Solves the LQG stability problem for the given test cases.\n\n    For each case, it designs an LQR controller and a Kalman filter, then\n    evaluates the stability of the combined closed-loop system based on the\n    separation principle.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (happy path, stabilizable and detectable)\n        {\n            \"A\": np.array([[1.1, 0.1], [0.0, 0.95]]),\n            \"B\": np.array([[0.1], [0.05]]),\n            \"C\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"Q_c\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"R_c\": np.array([[0.1]]),\n            \"W\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n            \"V\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n        },\n        # Test case 2 (boundary case, stabilizable and detectable)\n        {\n            \"A\": np.array([[1.0, 0.2], [0.0, 0.9]]),\n            \"B\": np.array([[0.0], [1.0]]),\n            \"C\": np.array([[1.0, 0.0]]),\n            \"Q_c\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"R_c\": np.array([[0.5]]),\n            \"W\": np.array([[0.02, 0.0], [0.0, 0.02]]),\n            \"V\": np.array([[0.2]]),\n        },\n        # Test case 3 (not stabilizable)\n        {\n            \"A\": np.array([[1.2, 0.0], [0.0, 0.9]]),\n            \"B\": np.array([[0.0], [1.0]]),\n            \"C\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"Q_c\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"R_c\": np.array([[0.1]]),\n            \"W\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n            \"V\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n        },\n        # Test case 4 (not detectable)\n        {\n            \"A\": np.array([[1.1, 0.0], [0.0, 0.95]]),\n            \"B\": np.array([[0.1], [0.05]]),\n            \"C\": np.array([[0.0, 1.0]]),\n            \"Q_c\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"R_c\": np.array([[0.1]]),\n            \"W\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n            \"V\": np.array([[0.05]]),\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        A, B, C = params[\"A\"], params[\"B\"], params[\"C\"]\n        Q_c, R_c = params[\"Q_c\"], params[\"R_c\"]\n        W, V = params[\"W\"], params[\"V\"]\n\n        # --- Sub-problem 1: Controller Stability ---\n        is_controller_stable = False\n        try:\n            # Solve the discrete-time algebraic Riccati equation for the controller\n            # P = A'PA - (A'PB)(R + B'PB)^-1(B'PA) + Q\n            P_c = solve_discrete_are(A, B, Q_c, R_c)\n            \n            # Compute the LQR gain K\n            # K = (R + B'PB)^-1 B'PA\n            K = np.linalg.inv(R_c + B.T @ P_c @ B) @ (B.T @ P_c @ A)\n            \n            # Form the closed-loop regulator matrix\n            A_reg = A - B @ K\n            \n            # Check stability: spectral radius of (A - BK) must be  1\n            eigvals_reg = np.linalg.eigvals(A_reg)\n            spectral_radius_reg = np.max(np.abs(eigvals_reg))\n            \n            if spectral_radius_reg  1.0:\n                is_controller_stable = True\n        except np.linalg.LinAlgError:\n            # DARE solver fails if (A,B) is not stabilizable.\n            is_controller_stable = False\n\n        # --- Sub-problem 2: Estimator Stability ---\n        is_estimator_stable = False\n        try:\n            # Solve the DARE for the filter using duality:\n            # A -> A', B -> C', Q -> W, R -> V\n            P_e = solve_discrete_are(A.T, C.T, W, V)\n            \n            # Compute the Kalman gain L\n            # L = PC'(V + CPC')^-1\n            L = P_e @ C.T @ np.linalg.inv(V + C @ P_e @ C.T)\n            \n            # Form the closed-loop estimator matrix\n            A_est = A - L @ C\n            \n            # Check stability: spectral radius of (A - LC) must be  1\n            eigvals_est = np.linalg.eigvals(A_est)\n            spectral_radius_est = np.max(np.abs(eigvals_est))\n            \n            if spectral_radius_est  1.0:\n                is_estimator_stable = True\n        except np.linalg.LinAlgError:\n            # DARE solver fails if (A',C') is not stabilizable,\n            # which means (A,C) is not detectable.\n            is_estimator_stable = False\n            \n        # The overall system is stable iff both controller and estimator are stable\n        is_system_stable = is_controller_stable and is_estimator_stable\n        results.append(is_system_stable)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}