{
    "hands_on_practices": [
        {
            "introduction": "Neuropil correction is a critical step for obtaining clean neuronal signals, but standard methods often rely on a simplified assumption of a constant contamination factor. This exercise delves into the consequences of violating this assumption, a common scenario in real data due to factors like animal movement or changes in brain state. By deriving the analytical form of the resulting bias, we can gain a deeper, quantitative understanding of the potential artifacts introduced by simple neuropil subtraction models .",
            "id": "4143823",
            "problem": "You are analyzing two-photon calcium imaging data with Suite2p, which performs neuropil subtraction on each Region of Interest (ROI). Consider a single ROI whose observed fluorescence trace is modeled as a linear mixture of the somatic signal, neuropil contamination, and measurement noise. The trace is given by $F_{\\mathrm{ROI}}(t) = S(t) + \\alpha(t) N(t) + \\varepsilon(t)$, where $S(t)$ is the baseline-subtracted somatic fluorescence (zero mean), $N(t)$ is the neuropil fluorescence, $\\alpha(t)$ is the true time-varying contamination fraction, and $\\varepsilon(t)$ is additive measurement noise. Suite2p applies a fixed neuropil coefficient $\\beta$ to subtract from the ROI trace, producing the corrected estimate $S_{\\mathrm{hat}}(t) = F_{\\mathrm{ROI}}(t) - \\beta N(t)$.\n\nDefine the neuropil baseline and fluctuations by $N(t) = \\mu_{N} + \\tilde{N}(t)$, where $\\tilde{N}(t)$ is zero mean. Model the contamination fraction as $\\alpha(t) = \\alpha_{0} + \\alpha_{1} X(t)$, where $X(t)$ is a zero-mean stationary scalar process representing slow state changes (for example, depth-dependent scattering or perivascular activity) that modulate the contamination fraction. Assume the following:\n- $S(t)$ and $\\varepsilon(t)$ are both zero-mean and independent of $N(t)$ and $X(t)$.\n- The neuropil fluctuation $\\tilde{N}(t)$ and the modulator $X(t)$ are jointly stationary with variances $\\sigma_{N}^{2}$ and $\\sigma_{X}^{2}$, respectively, and correlation coefficient $\\rho$, so that $\\operatorname{Cov}(X(t), \\tilde{N}(t)) = \\rho \\sigma_{X} \\sigma_{N}$.\n- All processes are ergodic so time averages equal expectations.\n\nLet the bias of the Suite2p estimate relative to the true somatic signal be defined as the expected difference $B = \\mathbb{E}[S_{\\mathrm{hat}}(t) - S(t)]$. Using only the definitions above, and without assuming any specific parametric form beyond the given second-order statistics, derive a closed-form analytic expression for $B$ in terms of $\\alpha_{0}$, $\\alpha_{1}$, $\\mu_{N}$, $\\sigma_{X}$, $\\sigma_{N}$, $\\rho$, and $\\beta$.\n\nYour final answer must be a single closed-form expression. No numerical evaluation is required.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\nThe given definitions, models, and assumptions are as follows:\n- Observed ROI fluorescence: $F_{\\mathrm{ROI}}(t) = S(t) + \\alpha(t) N(t) + \\varepsilon(t)$.\n- $S(t)$: baseline-subtracted somatic fluorescence with zero mean, $\\mathbb{E}[S(t)] = 0$.\n- $N(t)$: neuropil fluorescence.\n- $\\alpha(t)$: true time-varying contamination fraction.\n- $\\varepsilon(t)$: additive measurement noise with zero mean, $\\mathbb{E}[\\varepsilon(t)] = 0$.\n- Corrected fluorescence estimate: $S_{\\mathrm{hat}}(t) = F_{\\mathrm{ROI}}(t) - \\beta N(t)$.\n- $\\beta$: fixed neuropil subtraction coefficient.\n- Neuropil model: $N(t) = \\mu_{N} + \\tilde{N}(t)$, where $\\mu_{N}$ is the mean and $\\tilde{N}(t)$ are zero-mean fluctuations, $\\mathbb{E}[\\tilde{N}(t)] = 0$.\n- Contamination fraction model: $\\alpha(t) = \\alpha_{0} + \\alpha_{1} X(t)$, where $X(t)$ is a zero-mean stationary process, $\\mathbb{E}[X(t)] = 0$.\n- Statistical assumptions:\n    - $S(t)$ and $\\varepsilon(t)$ are independent of $N(t)$ and $X(t)$.\n    - The processes $\\tilde{N}(t)$ and $X(t)$ are jointly stationary.\n    - Variance of $\\tilde{N}(t)$ is $\\operatorname{Var}(\\tilde{N}(t)) = \\sigma_{N}^{2}$.\n    - Variance of $X(t)$ is $\\operatorname{Var}(X(t)) = \\sigma_{X}^{2}$.\n    - Covariance of $X(t)$ and $\\tilde{N}(t)$ is $\\operatorname{Cov}(X(t), \\tilde{N}(t)) = \\rho \\sigma_{X} \\sigma_{N}$.\n- Ergodicity is assumed, so time averages equal ensemble expectations.\n- The bias $B$ is defined as the expected difference: $B = \\mathbb{E}[S_{\\mathrm{hat}}(t) - S(t)]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded:** The problem presents a simplified but standard linear model for neuropil contamination in two-photon calcium imaging, a well-established issue in neuroscience data analysis. All components of the model ($S(t)$, $N(t)$, $\\alpha(t)$, etc.) represent physically meaningful quantities or processes. This is a valid scientific abstraction.\n- **Well-Posed:** All necessary statistical properties (means, variances, covariance) required to compute the expectation are provided. The objective is to derive a single analytical expression for the bias $B$, which is a well-defined mathematical task.\n- **Objective:** The problem is stated using precise, formal mathematical language, free from any subjective or ambiguous terminology.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, formally specified, complete, consistent, and solvable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full derivation of the solution will be provided.\n\n### Solution Derivation\nThe objective is to derive an expression for the bias, $B$, defined as the expectation of the difference between the estimated somatic signal, $S_{\\mathrm{hat}}(t)$, and the true somatic signal, $S(t)$.\n$$B = \\mathbb{E}[S_{\\mathrm{hat}}(t) - S(t)]$$\nFirst, substitute the definition of $S_{\\mathrm{hat}}(t) = F_{\\mathrm{ROI}}(t) - \\beta N(t)$ into the expression for $B$:\n$$B = \\mathbb{E}[ (F_{\\mathrm{ROI}}(t) - \\beta N(t)) - S(t) ]$$\nNext, substitute the model for the observed fluorescence, $F_{\\mathrm{ROI}}(t) = S(t) + \\alpha(t) N(t) + \\varepsilon(t)$:\n$$B = \\mathbb{E}[ (S(t) + \\alpha(t) N(t) + \\varepsilon(t) - \\beta N(t)) - S(t) ]$$\nThe terms for the true somatic signal, $S(t)$, cancel out:\n$$B = \\mathbb{E}[ \\alpha(t) N(t) - \\beta N(t) + \\varepsilon(t) ]$$\nThe expression can be factored and, by the linearity of the expectation operator, separated into two terms:\n$$B = \\mathbb{E}[ (\\alpha(t) - \\beta) N(t) ] + \\mathbb{E}[\\varepsilon(t)]$$\nAccording to the problem statement, the measurement noise $\\varepsilon(t)$ is zero-mean, so $\\mathbb{E}[\\varepsilon(t)] = 0$. The equation for the bias simplifies to:\n$$B = \\mathbb{E}[ (\\alpha(t) - \\beta) N(t) ]$$\nNow, we substitute the given models for the time-varying contamination fraction, $\\alpha(t) = \\alpha_{0} + \\alpha_{1} X(t)$, and the neuropil fluorescence, $N(t) = \\mu_{N} + \\tilde{N}(t)$:\n$$B = \\mathbb{E}[ (\\alpha_{0} + \\alpha_{1} X(t) - \\beta) (\\mu_{N} + \\tilde{N}(t)) ]$$\nWe expand the product inside the expectation:\n$$B = \\mathbb{E}[ (\\alpha_{0} - \\beta)\\mu_{N} + (\\alpha_{0} - \\beta)\\tilde{N}(t) + \\alpha_{1} X(t)\\mu_{N} + \\alpha_{1} X(t)\\tilde{N}(t) ]$$\nAgain, we apply the linearity of expectation to each term in the sum:\n$$B = \\mathbb{E}[(\\alpha_{0} - \\beta)\\mu_{N}] + \\mathbb{E}[(\\alpha_{0} - \\beta)\\tilde{N}(t)] + \\mathbb{E}[\\alpha_{1} X(t)\\mu_{N}] + \\mathbb{E}[\\alpha_{1} X(t)\\tilde{N}(t)]$$\nLet us evaluate each term individually.\n1. The first term, $\\mathbb{E}[(\\alpha_{0} - \\beta)\\mu_{N}]$, consists entirely of constants. The expectation of a constant is the constant itself:\n$$\\mathbb{E}[(\\alpha_{0} - \\beta)\\mu_{N}] = (\\alpha_{0} - \\beta)\\mu_{N}$$\n2. For the second term, we can factor out the constant part:\n$$\\mathbb{E}[(\\alpha_{0} - \\beta)\\tilde{N}(t)] = (\\alpha_{0} - \\beta)\\mathbb{E}[\\tilde{N}(t)]$$\nIt is given that the neuropil fluctuations $\\tilde{N}(t)$ are zero-mean, so $\\mathbb{E}[\\tilde{N}(t)] = 0$. Therefore, this entire term is zero.\n3. For the third term, we factor out the constants $\\alpha_{1}$ and $\\mu_{N}$:\n$$\\mathbb{E}[\\alpha_{1} X(t)\\mu_{N}] = \\alpha_{1}\\mu_{N}\\mathbb{E}[X(t)]$$\nIt is given that the modulator process $X(t)$ is zero-mean, so $\\mathbb{E}[X(t)] = 0$. This term is also zero.\n4. For the fourth term, we factor out the constant $\\alpha_{1}$:\n$$\\mathbb{E}[\\alpha_{1} X(t)\\tilde{N}(t)] = \\alpha_{1}\\mathbb{E}[X(t)\\tilde{N}(t)]$$\nThe term $\\mathbb{E}[X(t)\\tilde{N}(t)]$ is the covariance between $X(t)$ and $\\tilde{N}(t)$, because both processes are zero-mean: $\\operatorname{Cov}(X(t), \\tilde{N}(t)) = \\mathbb{E}[(X(t)-\\mathbb{E}[X(t)])(\\tilde{N}(t)-\\mathbb{E}[\\tilde{N}(t)])] = \\mathbb{E}[X(t)\\tilde{N}(t)]$.\nThe problem states that this covariance is $\\operatorname{Cov}(X(t), \\tilde{N}(t)) = \\rho \\sigma_{X} \\sigma_{N}$. Thus, the fourth term evaluates to:\n$$\\alpha_{1} \\rho \\sigma_{X} \\sigma_{N}$$\nFinally, we sum the results for all four terms to obtain the final expression for the bias $B$:\n$$B = (\\alpha_{0} - \\beta)\\mu_{N} + 0 + 0 + \\alpha_{1} \\rho \\sigma_{X} \\sigma_{N}$$\n$$B = (\\alpha_{0} - \\beta)\\mu_{N} + \\alpha_{1} \\rho \\sigma_{X} \\sigma_{N}$$\nThis is the closed-form analytic expression for the bias in terms of the specified parameters.",
            "answer": "$$\n\\boxed{(\\alpha_{0} - \\beta)\\mu_{N} + \\alpha_{1} \\rho \\sigma_{X} \\sigma_{N}}\n$$"
        },
        {
            "introduction": "After performing neuropil correction, how can we be sure we haven't \"over-corrected\" and inadvertently removed true neural signal? This practice explores the tell-tale signs of over-subtraction, where the correction coefficient $\\beta$ is larger than the true contamination $\\alpha$, leading to characteristic artifacts like negative-going transients. You will reason through the effects of over-subtraction and develop principled diagnostics based on signal distribution and event polarity to ensure the fidelity of your corrected data .",
            "id": "4143838",
            "problem": "Consider one neuronal Region of Interest (ROI) fluorescence time series and its surrounding neuropil in two-photon calcium imaging. Let the measured ROI fluorescence be modeled as a linear mixture of a cellular component, a neuropil contamination component, and noise: \n$$F_{\\text{ROI}}(t) = F_{\\text{cell}}(t) + \\alpha F_{\\text{np}}(t) + \\varepsilon(t),$$ \nwhere $F_{\\text{cell}}(t)$ is the cellular fluorescence, $F_{\\text{np}}(t)$ is the neuropil fluorescence, $\\alpha \\in [0,1]$ is the contamination fraction, and $\\varepsilon(t)$ is zero-mean noise with symmetric distribution. Assume the cellular fluorescence $F_{\\text{cell}}(t)$ arises from a nonnegative latent calcium concentration driven by spikes convolved with a strictly nonnegative impulse response $h(t)$, so that the instantaneous event amplitude in $F_{\\text{cell}}(t)$ is nonnegative. Likewise, suppose $F_{\\text{np}}(t)$ has nonnegative transients when the local population is active. A common neuropil correction subtracts a scaled neuropil trace from the ROI trace, defining \n$$F_{\\text{corr}}(t) = F_{\\text{ROI}}(t) - \\beta F_{\\text{np}}(t),$$ \nwhere $\\beta \\ge 0$ is the user-chosen subtraction coefficient.\n\nAssume a transient occurs at time $t_0$ such that both $F_{\\text{cell}}(t)$ and $F_{\\text{np}}(t)$ increase above their baselines by amplitudes $A_{\\text{cell}} \\ge 0$ and $A_{\\text{np}} \\ge 0$, respectively, over a short window where the impulse response is effectively nonnegative. You may treat $A_{\\text{cell}}$ and $A_{\\text{np}}$ as instantaneous increments relative to baselines, and $\\varepsilon(t)$ as a small perturbation with zero mean.\n\nUsing only the linear mixing model, nonnegativity of the calcium impulse response, and symmetry of the noise, reason about the effect of choosing $\\beta$ larger than the true contamination fraction $\\alpha$ on the polarity of transients in $F_{\\text{corr}}(t)$. Then, propose principled diagnostics that do not require ground-truth spikes to detect over-subtraction. In particular, consider diagnostics based on the distribution of $F_{\\text{corr}}(t)$ around its baseline and on the polarity of detected events using a matched filter $h(t)$.\n\nWhich of the following statements are correct?\n\nA. Under the stated model, the corrected transient amplitude at $t_0$ equals $A_{\\text{cell}} + (\\alpha - \\beta) A_{\\text{np}}$. If $\\beta$ exceeds $\\alpha$ by more than $A_{\\text{cell}}/A_{\\text{np}}$, the corrected transient becomes negative, producing negative-going deflections in $F_{\\text{corr}}(t)$.\n\nB. Because the noise $\\varepsilon(t)$ is symmetric and the calcium impulse response $h(t)$ is nonnegative, legitimate events should bias the distribution of $F_{\\text{corr}}(t)$ toward a right tail under correct subtraction. Over-subtraction ($\\beta > \\alpha$) induces a left-heavy tail. A practical diagnostic is to compute a tail-imbalance ratio \n$$R_k = \\frac{\\mathbb{P}\\!\\left(F_{\\text{corr}}(t) < m - k \\hat{\\sigma}\\right)}{\\mathbb{P}\\!\\left(F_{\\text{corr}}(t) > m + k \\hat{\\sigma}\\right)},$$ \nwhere $m$ is the sample median of $F_{\\text{corr}}(t)$ and $\\hat{\\sigma}$ is a robust scale estimate; values $R_k \\gg 1$ indicate over-subtraction.\n\nC. Using a matched filter $h(t)$ proportional to the nonnegative calcium impulse response, form event scores by projection $s(t_0) = \\sum_{t} F_{\\text{corr}}(t) h(t - t_0)$ and threshold on $|s(t_0)|$. If a statistically significant fraction of high-amplitude detections have $s(t_0) < 0$ (negative polarity), this is evidence of over-subtraction.\n\nD. Selecting $\\beta$ to minimize the variance of $F_{\\text{corr}}(t)$ guarantees an optimal correction and cannot produce over-subtraction, because variance reduction implies improved signal quality.\n\nE. The lag-zero covariance $\\operatorname{Cov}\\!\\left(F_{\\text{corr}}(t), F_{\\text{np}}(t)\\right)$ must be negative whenever $\\beta > \\alpha$, regardless of the dependence between $F_{\\text{cell}}(t)$ and $F_{\\text{np}}(t)$; thus, a negative lag-zero covariance is both necessary and sufficient to diagnose over-subtraction.",
            "solution": "The user wants me to act as a STEM professor. My task is to validate the problem statement, then, if it is valid, provide a detailed solution evaluating each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\nThe problem provides the following model and conditions:\n-   ROI fluorescence model: $F_{\\text{ROI}}(t) = F_{\\text{cell}}(t) + \\alpha F_{\\text{np}}(t) + \\varepsilon(t)$.\n-   $F_{\\text{cell}}(t)$: True cellular fluorescence.\n-   $F_{\\text{np}}(t)$: Neuropil fluorescence.\n-   $\\alpha$: True contamination fraction, with $\\alpha \\in [0,1]$.\n-   $\\varepsilon(t)$: Zero-mean noise with a symmetric distribution.\n-   $F_{\\text{cell}}(t)$ has a nonnegative impulse response $h(t)$, leading to nonnegative event amplitudes.\n-   $F_{\\text{np}}(t)$ has nonnegative transients.\n-   Corrected fluorescence formula: $F_{\\text{corr}}(t) = F_{\\text{ROI}}(t) - \\beta F_{\\text{np}}(t)$.\n-   $\\beta$: User-chosen subtraction coefficient, with $\\beta \\ge 0$.\n-   At a transient time $t_0$, the amplitudes above baseline are $A_{\\text{cell}} \\ge 0$ and $A_{\\text{np}} \\ge 0$. These are treated as instantaneous increments.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is based on a standard linear model for neuropil contamination in two-photon calcium imaging, a widely accepted and utilized framework in computational neuroscience. The assumptions about non-negativity of signals and symmetric noise are standard simplifications in this context. The problem is scientifically sound.\n-   **Well-Posed**: The problem is mathematically well-defined. It provides a clear set of equations and assumptions and asks for logical deductions about the system's behavior under specific conditions ($\\beta > \\alpha$) and for methods to diagnose this condition. A unique set of logical conclusions can be derived.\n-   **Objective**: The problem is stated using precise, objective mathematical and scientific language. It is free from ambiguity and subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically grounded, well-posed, and objective. I will proceed with the solution derivation.\n\n### Solution Derivation\n\nFirst, we establish the fundamental equation for the corrected fluorescence, $F_{\\text{corr}}(t)$. We substitute the expression for $F_{\\text{ROI}}(t)$ into the formula for $F_{\\text{corr}}(t)$:\n$$F_{\\text{corr}}(t) = F_{\\text{ROI}}(t) - \\beta F_{\\text{np}}(t)$$\n$$F_{\\text{corr}}(t) = (F_{\\text{cell}}(t) + \\alpha F_{\\text{np}}(t) + \\varepsilon(t)) - \\beta F_{\\text{np}}(t)$$\n$$F_{\\text{corr}}(t) = F_{\\text{cell}}(t) + (\\alpha - \\beta) F_{\\text{np}}(t) + \\varepsilon(t)$$\nThis equation forms the basis for analyzing the effects of the subtraction coefficient $\\beta$. We are particularly interested in the case of over-subtraction, where $\\beta > \\alpha$. In this regime, the term $(\\alpha - \\beta)$ is negative.\n\n### Option-by-Option Analysis\n\n**A. Under the stated model, the corrected transient amplitude at $t_0$ equals $A_{\\text{cell}} + (\\alpha - \\beta) A_{\\text{np}}$. If $\\beta$ exceeds $\\alpha$ by more than $A_{\\text{cell}}/A_{\\text{np}}$, the corrected transient becomes negative, producing negative-going deflections in $F_{\\text{corr}}(t)$.**\n\nLet's analyze the amplitude of the transient in the corrected signal. The problem defines the amplitudes $A_{\\text{cell}}$ and $A_{\\text{np}}$ as instantaneous increments above their respective baselines. The change in the corrected signal, which we can call the corrected amplitude $A_{\\text{corr}}$, is derived from our fundamental equation, ignoring the zero-mean noise term $\\varepsilon(t)$ for the amplitude calculation:\n$$A_{\\text{corr}} = \\Delta F_{\\text{corr}}(t_0) = \\Delta F_{\\text{cell}}(t_0) + (\\alpha - \\beta) \\Delta F_{\\text{np}}(t_0)$$\nGiven that $\\Delta F_{\\text{cell}}(t_0) = A_{\\text{cell}}$ and $\\Delta F_{\\text{np}}(t_0) = A_{\\text{np}}$, we have:\n$$A_{\\text{corr}} = A_{\\text{cell}} + (\\alpha - \\beta) A_{\\text{np}}$$\nThis confirms the first part of the statement.\n\nFor the corrected transient to become negative, we require $A_{\\text{corr}} < 0$:\n$$A_{\\text{cell}} + (\\alpha - \\beta) A_{\\text{np}} < 0$$\nRearranging the inequality to solve for the over-subtraction amount $(\\beta - \\alpha)$:\n$$A_{\\text{cell}} < -(\\alpha - \\beta) A_{\\text{np}}$$\n$$A_{\\text{cell}} < (\\beta - \\alpha) A_{\\text{np}}$$\nAssuming a neuropil transient exists, so $A_{\\text{np}} > 0$, we can divide by $A_{\\text{np}}$ without changing the inequality direction:\n$$\\frac{A_{\\text{cell}}}{A_{\\text{np}}} < \\beta - \\alpha$$\nThis means that the corrected transient amplitude is negative if the over-subtraction amount, $\\beta - \\alpha$, is greater than the ratio of the cell transient amplitude to the neuropil transient amplitude, $A_{\\text{cell}}/A_{\\text{np}}$. This precisely matches the condition stated in the option. Negative-going deflections are a direct result of $A_{\\text{corr}}$ becoming negative.\n\nVerdict: **Correct**.\n\n**B. Because the noise $\\varepsilon(t)$ is symmetric and the calcium impulse response $h(t)$ is nonnegative, legitimate events should bias the distribution of $F_{\\text{corr}}(t)$ toward a right tail under correct subtraction. Over-subtraction ($\\beta > \\alpha$) induces a left-heavy tail. A practical diagnostic is to compute a tail-imbalance ratio $R_k = \\frac{\\mathbb{P}\\!\\left(F_{\\text{corr}}(t) < m - k \\hat{\\sigma}\\right)}{\\mathbb{P}\\!\\left(F_{\\text{corr}}(t) > m + k \\hat{\\sigma}\\right)}$, where $m$ is the sample median of $F_{\\text{corr}}(t)$ and $\\hat{\\sigma}$ is a robust scale estimate; values $R_k \\gg 1$ indicate over-subtraction.**\n\nUnder correct subtraction (ideally $\\beta = \\alpha$), the corrected signal is $F_{\\text{corr}}(t) = F_{\\text{cell}}(t) + \\varepsilon(t)$. The signal $F_{\\text{cell}}(t)$ consists of a baseline and nonnegative transients, so its probability distribution is skewed to the right. The noise $\\varepsilon(t)$ is symmetric. The convolution of a right-skewed distribution with a symmetric one results in a right-skewed distribution. Thus, the distribution of $F_{\\text{corr}}(t)$ under correct subtraction will have a right tail.\n\nUnder over-subtraction ($\\beta > \\alpha$), the corrected signal is $F_{\\text{corr}}(t) = F_{\\text{cell}}(t) - (\\beta - \\alpha)F_{\\text{np}}(t) + \\varepsilon(t)$. The term $-(\\beta - \\alpha)F_{\\text{np}}(t)$ is the product of a positive-valued signal ($F_{\\text{np}}(t)$) and a negative constant $-(\\beta - \\alpha)$. This component will therefore have non-positive (negative) transients, contributing a left-heavy tail to the overall distribution. If this effect is strong enough, it will cause the distribution of $F_{\\text{corr}}(t)$ to become negatively skewed or \"left-heavy.\"\n\nThe proposed diagnostic, $R_k$, quantifies this asymmetry. It is the ratio of the probability mass in the far left tail to the probability mass in the far right tail. The use of the median $m$ and a robust scale estimate $\\hat{\\sigma}$ (like the median absolute deviation) makes the measure robust to outliers and non-Gaussian distributions. If the distribution has a heavy left tail, the numerator $\\mathbb{P}(F_{\\text{corr}}(t) < m - k \\hat{\\sigma})$ will be significantly larger than the denominator $\\mathbb{P}(F_{\\text{corr}}(t) > m + k \\hat{\\sigma})$, leading to $R_k \\gg 1$. This is a principled and practical diagnostic for over-subtraction.\n\nVerdict: **Correct**.\n\n**C. Using a matched filter $h(t)$ proportional to the nonnegative calcium impulse response, form event scores by projection $s(t_0) = \\sum_{t} F_{\\text{corr}}(t) h(t - t_0)$ and threshold on $|s(t_0)|$. If a statistically significant fraction of high-amplitude detections have $s(t_0) < 0$ (negative polarity), this is evidence of over-subtraction.**\n\nLet's analyze the event score $s(t_0)$. The projection operation is linear.\n$$s(t_0) = \\sum_{t} \\left[F_{\\text{cell}}(t) + (\\alpha - \\beta) F_{\\text{np}}(t) + \\varepsilon(t)\\right] h(t - t_0)$$\n$$s(t_0) = \\underbrace{\\sum_{t} F_{\\text{cell}}(t) h(t-t_0)}_{\\text{Term 1}} + \\underbrace{(\\alpha - \\beta) \\sum_{t} F_{\\text{np}}(t) h(t-t_0)}_{\\text{Term 2}} + \\underbrace{\\sum_{t} \\varepsilon(t) h(t-t_0)}_{\\text{Term 3}}$$\nTerm 1 represents the score from the true cellular signal. Since $F_{\\text{cell}}(t)$ consists of transients with a shape similar to $h(t)$ and nonnegative amplitude, and $h(t)$ is nonnegative, this term will be nonnegative.\nTerm 3 is the score from filtered noise, which will have a mean of zero.\nTerm 2 is the critical component. If $\\beta > \\alpha$, the factor $(\\alpha - \\beta)$ is negative. The summation $\\sum_{t} F_{\\text{np}}(t) h(t-t_0)$ is a projection of the neuropil signal onto the filter $h(t)$. Since both $F_{\\text{np}}(t)$ transients and $h(t)$ are nonnegative, this sum will be nonnegative. Therefore, for $\\beta > \\alpha$, Term 2 is non-positive (i.e., negative during neuropil transients).\n\nIf a significant neuropil transient occurs at a time when there is no cellular transient ($F_{\\text{cell}}(t)$ is at baseline), the score will be dominated by Term 2, resulting in $s(t_0) < 0$. If an event detection algorithm thresholds on the absolute value $|s(t_0)|$, it will detect these negative-going \"events.\" The presence of a significant population of detected events with negative scores is a direct consequence of over-subtraction and serves as a powerful diagnostic.\n\nVerdict: **Correct**.\n\n**D. Selecting $\\beta$ to minimize the variance of $F_{\\text{corr}}(t)$ guarantees an optimal correction and cannot produce over-subtraction, because variance reduction implies improved signal quality.**\n\nTo find the value of $\\beta$ that minimizes $\\operatorname{Var}(F_{\\text{corr}}(t))$, we can compute the variance and find the minimum. Assuming the noise $\\varepsilon(t)$ is uncorrelated with the fluorescence signals:\n$$\\operatorname{Var}(F_{\\text{corr}}) = \\operatorname{Var}(F_{\\text{cell}} + (\\alpha - \\beta)F_{\\text{np}} + \\varepsilon)$$\n$$\\operatorname{Var}(F_{\\text{corr}}) = \\operatorname{Var}(F_{\\text{cell}}) + (\\alpha - \\beta)^2 \\operatorname{Var}(F_{\\text{np}}) + 2(\\alpha - \\beta)\\operatorname{Cov}(F_{\\text{cell}}, F_{\\text{np}}) + \\operatorname{Var}(\\varepsilon)$$\nThis is a quadratic function of $\\beta$. To find the minimum, we set the derivative with respect to $\\beta$ to zero:\n$$\\frac{d}{d\\beta} \\operatorname{Var}(F_{\\text{corr}}) = 2(\\alpha - \\beta)(-1)\\operatorname{Var}(F_{\\text{np}}) + 2(-1)\\operatorname{Cov}(F_{\\text{cell}}, F_{\\text{np}}) = 0$$\n$$-(\\alpha - \\beta)\\operatorname{Var}(F_{\\text{np}}) - \\operatorname{Cov}(F_{\\text{cell}}, F_{\\text{np}}) = 0$$\n$$\\beta \\operatorname{Var}(F_{\\text{np}}) = \\alpha \\operatorname{Var}(F_{\\text{np}}) + \\operatorname{Cov}(F_{\\text{cell}}, F_{\\text{np}})$$\nThe variance-minimizing $\\beta$, let's call it $\\beta^*$, is:\n$$\\beta^{*} = \\alpha + \\frac{\\operatorname{Cov}(F_{\\text{cell}}, F_{\\text{np}})}{\\operatorname{Var}(F_{\\text{np}})}$$\nIn practice, the activity of a neuron ($F_{\\text{cell}}$) is often positively correlated with the activity of the surrounding neuropil ($F_{\\text{np}}$), implying $\\operatorname{Cov}(F_{\\text{cell}}, F_{\\text{np}}) > 0$. In this common scenario, $\\beta^{*} > \\alpha$. Thus, minimizing the variance of the corrected trace actively *causes* over-subtraction. The underlying reason is that this procedure incorrectly interprets the part of the true cellular signal that is correlated with the neuropil as contamination and subtracts it. The premise that variance reduction guarantees optimal correction is false; it can degrade the signal by removing true neural activity.\n\nVerdict: **Incorrect**.\n\n**E. The lag-zero covariance $\\operatorname{Cov}\\!\\left(F_{\\text{corr}}(t), F_{\\text{np}}(t)\\right)$ must be negative whenever $\\beta > \\alpha$, regardless of the dependence between $F_{\\text{cell}}(t)$ and $F_{\\text{np}}(t)$; thus, a negative lag-zero covariance is both necessary and sufficient to diagnose over-subtraction.**\n\nLet's compute the covariance between the corrected signal and the neuropil signal. Assuming noise $\\varepsilon(t)$ is independent of $F_{\\text{np}}(t)$:\n$$\\operatorname{Cov}(F_{\\text{corr}}, F_{\\text{np}}) = \\operatorname{Cov}(F_{\\text{cell}} + (\\alpha - \\beta)F_{\\text{np}} + \\varepsilon, F_{\\text{np}})$$\n$$\\operatorname{Cov}(F_{\\text{corr}}, F_{\\text{np}}) = \\operatorname{Cov}(F_{\\text{cell}}, F_{\\text{np}}) + \\operatorname{Cov}((\\alpha - \\beta)F_{\\text{np}}, F_{\\text{np}}) + \\operatorname{Cov}(\\varepsilon, F_{\\text{np}})$$\n$$\\operatorname{Cov}(F_{\\text{corr}}, F_{\\text{np}}) = \\operatorname{Cov}(F_{\\text{cell}}, F_{\\text{np}}) + (\\alpha - \\beta)\\operatorname{Var}(F_{\\text{np}})$$\nThe statement claims this covariance *must* be negative whenever $\\beta > \\alpha$. Let's test this. If $\\beta > \\alpha$, then $\\alpha - \\beta < 0$, so the second term is negative. However, the first term, $\\operatorname{Cov}(F_{\\text{cell}}, F_{\\text{np}})$, is typically positive. The sign of the overall expression depends on the relative magnitudes of the two terms. It is entirely possible that $\\operatorname{Cov}(F_{\\text{cell}}, F_{\\text{np}}) > -(\\alpha - \\beta)\\operatorname{Var}(F_{\\text{np}})$, which would make the total covariance positive even with over-subtraction. Therefore, a negative covariance is *not a necessary* condition for over-subtraction. Since the statement claims it is both necessary and sufficient, the statement is false.\n\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "While simple subtraction models correct signals on a pre-defined ROI, more advanced methods can simultaneously identify a neuron's spatial footprint and extract its temporal activity. This exercise introduces the powerful framework of Constrained Nonnegative Matrix Factorization (CNMF), treating the data as a product of spatial and temporal components. By deriving the alternating optimization updates for these components, you will gain a fundamental understanding of how this generative model performs source separation to demix neural signals from background contamination .",
            "id": "4143876",
            "problem": "Consider a motion-corrected two-dimensional fluorescence movie segmented into a fixed Region of Interest (ROI) for a single neuron, represented over $p$ pixels and $T$ time points by the matrix $Y \\in \\mathbb{R}^{p \\times T}$. Let the spatial footprint be $A \\in \\mathbb{R}^{p \\times 1}$ and the temporal activity trace be $C \\in \\mathbb{R}^{1 \\times T}$, so that the rank-one factorization $A C$ models the neuron’s contribution. Assume the movie is contaminated by neuropil fluorescence $S \\in \\mathbb{R}^{p \\times T}$ with contamination coefficient $\\rho \\in \\mathbb{R}_{+}$, and define the corrected data $Y' = Y - \\rho S$. The goal is to estimate $A$ and $C$ via alternating optimization under probabilistic observation models while enforcing constraints relevant for Constrained Nonnegative Matrix Factorization (CNMF).\n\nStarting from well-tested observation models and core definitions:\n- Under a Gaussian noise model, assume $Y'$ is generated as $Y' = A C + E$ with $E$ composed of independent, zero-mean Gaussian random variables with variance $\\sigma^{2}$.\n- Under a Poisson count model, assume $Y'$ represents photon counts with rate $\\Lambda = A C$, so $Y'_{i,t} \\sim \\mathrm{Poisson}(\\Lambda_{i,t})$.\n\nYou will derive alternating optimization updates for $A$ and $C$ for both models starting from their negative log-likelihoods augmented by regularization and constraints consistent with CNMF practice, and then compute a specific scalar update under the Gaussian model.\n\nTasks:\n1. Derive the update for $C$ conditional on $A$ under the Gaussian model by minimizing the penalized negative log-likelihood\n$$\n\\mathcal{L}_{\\mathrm{G}}(A,C) = \\frac{1}{2 \\sigma^{2}} \\| Y' - A C \\|_{F}^{2} + \\frac{\\lambda_{C}}{2} \\| C \\|_{F}^{2}\n$$\nsubject to $C \\geq 0$. Show the unconstrained minimizer and describe how the nonnegativity constraint is enforced.\n\n2. Derive the update for $A$ conditional on $C$ under the Gaussian model by minimizing\n$$\n\\mathcal{L}_{\\mathrm{G}}(A,C) = \\frac{1}{2 \\sigma^{2}} \\| Y' - A C \\|_{F}^{2} + \\frac{\\lambda_{A}}{2} \\| A \\|_{2}^{2}\n$$\nsubject to $A \\geq 0$. Show the unconstrained minimizer and describe how the nonnegativity constraint is enforced. In your derivation, clearly indicate how the alternating optimization decouples in the rank-one case.\n\n3. Starting from the Poisson negative log-likelihood\n$$\n\\mathcal{L}_{\\mathrm{P}}(A,C) = \\sum_{i=1}^{p} \\sum_{t=1}^{T} \\left[ (A C)_{i,t} - Y'_{i,t} \\ln\\left( (A C)_{i,t} \\right) \\right],\n$$\nderive multiplicative updates for $A$ and $C$ that respect $A \\geq 0$ and $C \\geq 0$. Provide the expressions that result from enforcing the Karush–Kuhn–Tucker optimality conditions or an appropriate auxiliary-function argument for nonnegative matrix factorization under the generalized Kullback–Leibler divergence.\n\n4. Explain, using the structure of the derived objectives and constraints, why constraints such as nonnegativity, spatial regularization (for example, a quadratic penalty on $A$ or a discrete Laplacian penalty), and explicit support constraints prevent degenerate solutions such as collapsed footprints (for example, $A$ concentrating all mass in a single pixel).\n\n5. Compute a single numerical update under the Gaussian model. Consider $p=3$ pixels and $T=2$ time points with\n$$\nY = \\begin{pmatrix}\n10 & 12 \\\\\n22 & 20 \\\\\n11 & 15\n\\end{pmatrix}, \\quad\nS = \\begin{pmatrix}\n4 & 3 \\\\\n5 & 4 \\\\\n3 & 2\n\\end{pmatrix}, \\quad\n\\rho = 0.2,\n$$\nand an initial spatial footprint\n$$\nA = \\begin{pmatrix}\n1 \\\\\n2 \\\\\n1\n\\end{pmatrix}.\n$$\nUse the $C$-update derived in Task $1$ with $\\lambda_{C} = 1$ to compute the updated activity at time $t=2$, denoted $c_{2}$, after neuropil correction. Express the numerical result in arbitrary fluorescence units. Round your answer to four significant figures.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the established principles of computational neuroscience and optimization, specifically concerning the analysis of calcium imaging data using constrained matrix factorization. The problem is well-posed, providing all necessary definitions, models, and data to perform the required derivations and calculations. The language is objective and mathematically precise. The problem is a standard, albeit simplified, formulation of a core task in neuroscience data analysis and is suitable for a rigorous solution.\n\nWe will proceed by addressing each of the five tasks in sequence.\n\n### Task 1: Update for C under the Gaussian Model\n\nThe objective function to be minimized with respect to the temporal component $C \\in \\mathbb{R}^{1 \\times T}$ for a fixed spatial component $A \\in \\mathbb{R}^{p \\times 1}$ is the penalized negative log-likelihood:\n$$\n\\mathcal{L}_{\\mathrm{G}}(C) = \\frac{1}{2 \\sigma^{2}} \\| Y' - A C \\|_{F}^{2} + \\frac{\\lambda_{C}}{2} \\| C \\|_{F}^{2}\n$$\nsubject to the constraint $C \\geq 0$ (element-wise).\n\nTo find the unconstrained minimizer, we compute the gradient of $\\mathcal{L}_{\\mathrm{G}}$ with respect to $C$ and set it to zero. The Frobenius norm squared is the sum of squared elements, $\\|X\\|_F^2 = \\mathrm{Tr}(X^T X)$. Let's express the objective using trace notation:\n$$\n\\mathcal{L}_{\\mathrm{G}}(C) = \\frac{1}{2 \\sigma^{2}} \\mathrm{Tr}((Y' - AC)^T(Y' - AC)) + \\frac{\\lambda_{C}}{2} \\mathrm{Tr}(C^T C)\n$$\nSince $C$ is a $1 \\times T$ row vector, it is more convenient to compute the derivative with respect to each component $c_t$ for $t=1, \\dots, T$.\n$$\n\\| Y' - A C \\|_{F}^{2} = \\sum_{i=1}^{p} \\sum_{t=1}^{T} (Y'_{i,t} - a_i c_t)^2\n$$\nThe derivative of the first term with respect to a specific component $c_t$ is:\n$$\n\\frac{\\partial}{\\partial c_t} \\left( \\sum_{i, \\tau} (Y'_{i,\\tau} - a_i c_\\tau)^2 \\right) = \\sum_{i=1}^{p} 2(Y'_{i,t} - a_i c_t)(-a_i) = -2 \\sum_{i=1}^{p} a_i(Y'_{i,t} - a_i c_t) = -2 (A^T Y'_t - (A^T A) c_t)\n$$\nwhere $Y'_t$ is the $t$-th column of $Y'$. The derivative of the regularization term is:\n$$\n\\frac{\\partial}{\\partial c_t} \\left( \\frac{\\lambda_C}{2} \\|C\\|_F^2 \\right) = \\frac{\\partial}{\\partial c_t} \\left( \\frac{\\lambda_C}{2} \\sum_{\\tau=1}^T c_{\\tau}^2 \\right) = \\lambda_C c_t\n$$\nCombining these, the derivative of the full objective with respect to $c_t$ is:\n$$\n\\frac{\\partial \\mathcal{L}_{\\mathrm{G}}}{\\partial c_t} = \\frac{1}{2 \\sigma^{2}} [-2 (A^T Y'_t - (A^T A) c_t)] + \\lambda_C c_t = \\frac{1}{\\sigma^2} ( (A^T A) c_t - A^T Y'_t) + \\lambda_C c_t\n$$\nSetting the derivative to zero to find the unconstrained minimum:\n$$\n\\left( \\frac{A^T A}{\\sigma^2} + \\lambda_C \\right) c_t = \\frac{A^T Y'_t}{\\sigma^2} \\implies c_t = \\frac{A^T Y'_t}{A^T A + \\sigma^2 \\lambda_C}\n$$\nThis can be written for the entire vector $C$ as:\n$$\nC_{\\mathrm{unconstrained}} = \\frac{A^T Y'}{A^T A + \\sigma^2 \\lambda_C}\n$$\nNote that $A^T A = \\|A\\|_2^2$ is a scalar, so the division is well-defined.\n\nThe optimization problem is a quadratic program with a simple nonnegativity constraint. The solution is found by projecting the unconstrained minimizer onto the feasible set (the nonnegative orthant). This projection is simply taking the element-wise maximum with zero. Therefore, the update for $C$ that enforces $C \\geq 0$ is:\n$$\nC \\leftarrow \\left( \\frac{A^T Y'}{A^T A + \\sigma^2 \\lambda_C} \\right)_+\n$$\nwhere $(\\cdot)_+$ denotes the rectification function $\\max(0, \\cdot)$ applied element-wise.\n\n### Task 2: Update for A under the Gaussian Model\n\nThe objective function to be minimized with respect to the spatial component $A \\in \\mathbb{R}^{p \\times 1}$ for a fixed temporal component $C \\in \\mathbb{R}^{1 \\times T}$ is:\n$$\n\\mathcal{L}_{\\mathrm{G}}(A) = \\frac{1}{2 \\sigma^{2}} \\| Y' - A C \\|_{F}^{2} + \\frac{\\lambda_{A}}{2} \\| A \\|_{2}^{2}\n$$\nsubject to $A \\geq 0$. This is analogous to the update for $C$. We take the derivative with respect to each component $a_i$ of $A$:\n$$\n\\frac{\\partial}{\\partial a_i} \\left( \\sum_{j=1}^{p} \\sum_{t=1}^{T} (Y'_{j,t} - a_j c_t)^2 \\right) = \\sum_{t=1}^{T} 2(Y'_{i,t} - a_i c_t)(-c_t) = -2 ( (Y'C^T)_i - a_i(CC^T) )\n$$\nwhere $(Y'C^T)_i$ is the $i$-th element of the vector $Y'C^T$. The derivative of the regularization term is $\\frac{\\partial}{\\partial a_i} (\\frac{\\lambda_A}{2} \\sum_j a_j^2) = \\lambda_A a_i$.\n\nThe derivative of the full objective w.r.t. $a_i$ is:\n$$\n\\frac{\\partial \\mathcal{L}_{\\mathrm{G}}}{\\partial a_i} = \\frac{1}{\\sigma^2}( a_i(CC^T) - (Y'C^T)_i) + \\lambda_A a_i\n$$\nSetting to zero:\n$$\n\\left( \\frac{CC^T}{\\sigma^2} + \\lambda_A \\right) a_i = \\frac{(Y'C^T)_i}{\\sigma^2} \\implies a_i = \\frac{(Y'C^T)_i}{CC^T + \\sigma^2 \\lambda_A}\n$$\nFor the entire vector $A$:\n$$\nA_{\\mathrm{unconstrained}} = \\frac{Y'C^T}{CC^T + \\sigma^2 \\lambda_A}\n$$\nNote that $CC^T = \\|C\\|_F^2$ is a scalar.\n\nThis update equation demonstrates the decoupling of the optimization for $A$. The update for each pixel's weight, $a_i$, depends only on the corresponding row of the data matrix, $Y'_{i,:}$, and is independent of all other pixels $j \\neq i$. This is a direct consequence of the rank-one model structure, where each pixel's time series is modeled as an independent scaling of the common temporal component $C$.\n\nEnforcing the nonnegativity constraint $A \\geq 0$ is done, as before, by projection:\n$$\nA \\leftarrow \\left( \\frac{Y'C^T}{CC^T + \\sigma^2 \\lambda_A} \\right)_+\n$$\n\n### Task 3: Multiplicative Updates under the Poisson Model\n\nThe objective function is the negative log-likelihood of the Poisson model, which is equivalent to minimizing the generalized Kullback–Leibler (KL) divergence $D_{KL}(Y' || AC)$:\n$$\n\\mathcal{L}_{\\mathrm{P}}(A,C) = \\sum_{i=1}^{p} \\sum_{t=1}^{T} \\left[ (A C)_{i,t} - Y'_{i,t} \\ln\\left( (A C)_{i,t} \\right) \\right]\n$$\nWe seek multiplicative updates for $A \\geq 0$ and $C \\geq 0$. These can be derived using an auxiliary function method or from the Karush–Kuhn–Tucker (KKT) conditions. The standard multiplicative update rules for Nonnegative Matrix Factorization (NMF) with KL divergence for a factorization $V \\approx WH$ are:\n$$\nH \\leftarrow H \\odot \\frac{W^T (V ./ (WH))}{W^T \\mathbf{1}} \\quad \\text{and} \\quad W \\leftarrow W \\odot \\frac{(V ./ (WH)) H^T}{\\mathbf{1} H^T}\n$$\nwhere $\\odot$ and $./$ denote element-wise multiplication and division, and $\\mathbf{1}$ is a matrix of ones of appropriate size.\n\nIn our case, $V=Y'$, $W=A$ (a $p \\times 1$ matrix), and $H=C$ (a $1 \\times T$ matrix).\n\nFor the $C$ update:\nThe term $W^T \\mathbf{1}$ becomes a $1 \\times T$ row vector where each element is $\\sum_{i=1}^p a_i$.\nThe term $(Y' ./ (AC))_{i,t} = Y'_{i,t} / (a_i c_t)$.\nThe term $W^T (V ./ (WH))$ becomes a $1 \\times T$ vector. Its $t$-th component is:\n$$\n\\sum_{i=1}^p a_i \\frac{Y'_{i,t}}{a_i c_t} = \\frac{1}{c_t} \\sum_{i=1}^p Y'_{i,t}\n$$\nThe multiplicative update for element $c_t$ is:\n$$\nc_t \\leftarrow c_t \\odot \\frac{\\frac{1}{c_t} \\sum_{i=1}^p Y'_{i,t}}{\\sum_{i=1}^p a_i} = \\frac{\\sum_{i=1}^p Y'_{i,t}}{\\sum_{i=1}^p a_i}\n$$\n\nFor the $A$ update:\nThe term $\\mathbf{1} H^T$ becomes a $p \\times 1$ column vector where each element is $\\sum_{t=1}^T c_t$.\nThe term $(V ./ (WH)) H^T$ becomes a $p \\times 1$ vector. Its $i$-th component is:\n$$\n\\sum_{t=1}^T \\frac{Y'_{i,t}}{a_i c_t} c_t = \\frac{1}{a_i} \\sum_{t=1}^T Y'_{i,t}\n$$\nThe multiplicative update for element $a_i$ is:\n$$\na_i \\leftarrow a_i \\odot \\frac{\\frac{1}{a_i} \\sum_{t=1}^T Y'_{i,t}}{\\sum_{t=1}^T c_t} = \\frac{\\sum_{t=1}^T Y'_{i,t}}{\\sum_{t=1}^T c_t}\n$$\nSo, the derived multiplicative updates, after simplification for the rank-one case, are:\n$$\n\\text{For } t=1, \\dots, T: \\quad c_t \\leftarrow \\frac{\\sum_{i=1}^p Y'_{i,t}}{\\sum_{i=1}^p a_i}\n$$\n$$\n\\text{For } i=1, \\dots, p: \\quad a_i \\leftarrow \\frac{\\sum_{t=1}^T Y'_{i,t}}{\\sum_{t=1}^T c_t}\n$$\nThese updates intrinsically preserve nonnegativity, provided $Y'$ and the initial $A, C$ are nonnegative.\n\n### Task 4: Role of Constraints in Preventing Degenerate Solutions\n\nThe unconstrained factorization problem $Y' \\approx AC$ is ill-posed. Without constraints, it admits infinite degenerate solutions. For example, the scaling ambiguity allows for $(kA)(k^{-1}C) = AC$ for any scalar $k \\neq 0$. This means the magnitudes of $A$ and $C$ are not uniquely determined. More severe degeneracies include solutions that are not physically meaningful.\n\n1.  **Nonnegativity ($A \\geq 0, C \\geq 0$):** This is the most fundamental constraint, motivated by the physics of fluorescence. It restricts solutions to a cone in the solution space and limits the scaling ambiguity to $k > 0$. However, it is insufficient on its own to prevent overfitting or non-physiological spatial footprints.\n\n2.  **Spatial Regularization on $A$:** This encodes prior beliefs about the shape of a neuron's spatial footprint.\n    *   **Quadratic ($L_2$) Penalty ($\\lambda_A \\|A\\|_2^2$):** This penalty discourages solutions with large-magnitude entries in $A$. A collapsed footprint, where all energy is in one pixel $a_k=M$ and others are zero, has a penalty of $\\frac{\\lambda_A}{2}M^2$. A footprint where the same energy is spread over $N$ pixels (e.g., $a_i=M/N$) would have a penalty proportional to $N(M/N)^2 = M^2/N$, which is smaller. Thus, an $L_2$ penalty favors spatially distributed footprints over highly concentrated ones, mitigating the risk of collapsing.\n    *   **Laplacian Penalty ($A^T L A$):** This form of regularization explicitly promotes spatial smoothness. The Laplacian operator $L$ penalizes large differences between adjacent pixels. A collapsed footprint on a single pixel represents a sharp spatial discontinuity (a large value next to zeros), which would incur a very high penalty. This constraint is highly effective at enforcingfootprints to be contiguous and smooth, consistent with the microscopic view of a cell body.\n\n3.  **Explicit Support Constraints:** This involves forcing $a_i = 0$ for all pixels $i$ outside a predefined region of interest. This is the most direct way to prevent a collapsed footprint, as one can define a support region of a plausible size and shape, making it impossible for the solution to exist on only a single pixel within that region (unless the region itself is a single pixel).\n\nIn summary, these constraints are not arbitrary but are essential for regularizing the ill-posed factorization problem. They incorporate critical prior knowledge about the physical and biological nature of the signal source, guiding the optimization toward solutions that are not just mathematically valid but also scientifically meaningful and robust to noise.\n\n### Task 5: Numerical Update Calculation\n\nWe are asked to compute the updated value for $c_2$ using the Gaussian model C-update from Task 1, given the initial state.\nThe update formula is:\n$$\nc_t = \\left( \\frac{A^T Y'_t}{A^T A + \\sigma^2 \\lambda_C} \\right)_+\n$$\nThe problem does not specify the noise variance $\\sigma^2$. In optimization contexts derived from probabilistic models, this parameter is often set to $1$ without loss of generality, as it effectively rescales the regularization parameter $\\lambda_C$. We will proceed under the standard assumption that $\\sigma^2=1$.\n\nThe given data are:\n$$\nY = \\begin{pmatrix} 10 & 12 \\\\ 22 & 20 \\\\ 11 & 15 \\end{pmatrix}, \\quad S = \\begin{pmatrix} 4 & 3 \\\\ 5 & 4 \\\\ 3 & 2 \\end{pmatrix}, \\quad \\rho = 0.2\n$$\n$$\nA = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad \\lambda_C = 1\n$$\nFirst, we compute the neuropil-corrected data $Y' = Y - \\rho S$:\n$$\n\\rho S = 0.2 \\begin{pmatrix} 4 & 3 \\\\ 5 & 4 \\\\ 3 & 2 \\end{pmatrix} = \\begin{pmatrix} 0.8 & 0.6 \\\\ 1.0 & 0.8 \\\\ 0.6 & 0.4 \\end{pmatrix}\n$$\n$$\nY' = \\begin{pmatrix} 10 & 12 \\\\ 22 & 20 \\\\ 11 & 15 \\end{pmatrix} - \\begin{pmatrix} 0.8 & 0.6 \\\\ 1.0 & 0.8 \\\\ 0.6 & 0.4 \\end{pmatrix} = \\begin{pmatrix} 9.2 & 11.4 \\\\ 21.0 & 19.2 \\\\ 10.4 & 14.6 \\end{pmatrix}\n$$\nWe need to compute $c_2$, so we use the second column of $Y'$, denoted $Y'_2$:\n$$\nY'_2 = \\begin{pmatrix} 11.4 \\\\ 19.2 \\\\ 14.6 \\end{pmatrix}\n$$\nNext, we compute the terms in the update formula:\nThe numerator is $A^T Y'_2$:\n$$\nA^T Y'_2 = \\begin{pmatrix} 1 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 11.4 \\\\ 19.2 \\\\ 14.6 \\end{pmatrix} = (1)(11.4) + (2)(19.2) + (1)(14.6) = 11.4 + 38.4 + 14.6 = 64.4\n$$\nThe denominator contains $A^T A$ and $\\sigma^2 \\lambda_C$:\n$$\nA^T A = \\begin{pmatrix} 1 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = 1^2 + 2^2 + 1^2 = 1 + 4 + 1 = 6\n$$\n$$\n\\sigma^2 \\lambda_C = (1)(1) = 1\n$$\nNow, substitute these values into the formula for $c_2$:\n$$\nc_2 = \\left( \\frac{64.4}{6 + 1} \\right)_+ = \\left( \\frac{64.4}{7} \\right)_+ = (9.2)_+\n$$\nSince $9.2 > 0$, the rectification has no effect.\n$$\nc_2 = 9.2\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\nc_2 = 9.200\n$$",
            "answer": "$$\n\\boxed{9.200}\n$$"
        }
    ]
}