{
    "hands_on_practices": [
        {
            "introduction": "A beamformer's effectiveness hinges on the accuracy of the underlying forward model. This exercise explores one of the most significant sources of model error: the assumed orientation of the neural source. By deriving the beamformer's sensitivity to orientation mismatch from first principles, you will gain a quantitative understanding of how this error impacts the reconstructed source power, highlighting the critical importance of robust orientation estimation in source localization studies. ",
            "id": "4141729",
            "problem": "Consider a magnetoencephalography forward model in which a current dipole at location $r$ with unit orientation vector $q_{\\text{true}} \\in \\mathbb{R}^{3}$ generates sensor data $x(t) \\in \\mathbb{R}^{M}$ via $x(t) = L_{r} q_{\\text{true}} s(t) + n(t)$, where $L_{r} \\in \\mathbb{R}^{M \\times 3}$ is the location-specific leadfield, $s(t)$ is a scalar source time course, and $n(t)$ is additive noise. The sensor covariance is $C = \\mathbb{E}[x(t) x(t)^{\\top}]$. A Linearly Constrained Minimum Variance (LCMV) beamformer constructs weights $w \\in \\mathbb{R}^{M}$ by minimizing $w^{\\top} C w$ subject to a unit-gain constraint $w^{\\top} L_{r} q_{\\text{est}} = 1$ for a design orientation $q_{\\text{est}} \\in \\mathbb{R}^{3}$ of unit norm. The Dynamic Imaging of Coherent Sources (DICS) beamformer uses the same principle in the frequency domain, replacing $C$ by the cross-spectral density $S_{xx}(f)$ and $L_{r}$ by the complex-valued frequency-domain leadfield $L_{r}(f)$, and imposing $w(f)^{\\mathrm{H}} L_{r}(f) q_{\\text{est}} = 1$, where ${}^{\\mathrm{H}}$ denotes the Hermitian transpose.\n\nAssume the following physically grounded conditions:\n- The design orientation $q_{\\text{est}}$ is misaligned from $q_{\\text{true}}$ by an angle $\\theta \\in [0,\\pi]$ in radians, with $q_{\\text{est}}^{\\top} q_{\\text{true}} = \\cos(\\theta)$ and $\\|q_{\\text{true}}\\|_{2} = \\|q_{\\text{est}}\\|_{2} = 1$.\n- Sensors have been whitened so that orientation interactions at location $r$ are isotropic in the whitened space. Concretely, define the orientation metric $A = L_{r}^{\\top} C^{-1} L_{r}$ for LCMV and $A(f) = L_{r}(f)^{\\mathrm{H}} S_{xx}(f)^{-1} L_{r}(f)$ for DICS, and assume $A = \\alpha I_{3}$ and $A(f) = \\alpha(f) I_{3}$ with scalar $\\alpha > 0$ and $\\alpha(f) > 0$.\n- The source contribution to output is quantified by the beamformer gain to the true leadfield, defined for LCMV as $g = w^{\\top} L_{r} q_{\\text{true}}$ and for DICS at frequency $f$ as $g(f) = w(f)^{\\mathrm{H}} L_{r}(f) q_{\\text{true}}$. Output power due to the source scales as $g^{2}$ for LCMV and $|g(f)|^{2}$ for DICS.\n\nStarting only from these definitions and assumptions, derive the sensitivity curve—defined as the output power attenuation factor normalized to its value at perfect alignment—as a function of the angular mismatch $\\theta$ for both the Linearly Constrained Minimum Variance beamformer and the Dynamic Imaging of Coherent Sources beamformer under the isotropy assumption above. Express your final sensitivity curve as a single closed-form analytic expression in terms of $\\theta$ (in radians). No intermediate numeric substitution is required, and no additional modeling assumptions beyond those stated above may be introduced. The final answer must be an analytic expression and must not include units.",
            "solution": "The problem asks for the sensitivity of a beamformer to a mismatch between the true source orientation ($q_{\\text{true}}$) and the estimated orientation used in the filter design ($q_{\\text{est}}$). We will derive this for the general case, which applies to both Linearly Constrained Minimum Variance (LCMV) and Dynamic Imaging of Coherent Sources (DICS) beamformers under the given assumptions.\n\nLet the general form of the optimization problem be:\n$$ \\min_{w} w^{\\dagger} K w \\quad \\text{subject to} \\quad w^{\\dagger} l_{\\text{est}} = 1 $$\nwhere $\\dagger$ denotes the transpose for LCMV and the Hermitian transpose for DICS, $K$ is the covariance matrix $C$ (for LCMV) or the cross-spectral density matrix $S_{xx}(f)$ (for DICS), and $l_{\\text{est}} = L_r q_{\\text{est}}$ is the estimated leadfield vector.\n\nUsing the method of Lagrange multipliers, the optimal weight vector $w$ is found to be:\n$$ w = \\frac{K^{-1} l_{\\text{est}}}{l_{\\text{est}}^{\\dagger} K^{-1} l_{\\text{est}}} $$\nThe beamformer's gain $g$ for the true source signal, which has a leadfield $l_{\\text{true}} = L_r q_{\\text{true}}$, is given by $g = w^{\\dagger} l_{\\text{true}}$. Substituting the expression for $w$:\n$$ g = \\left( \\frac{K^{-1} l_{\\text{est}}}{l_{\\text{est}}^{\\dagger} K^{-1} l_{\\text{est}}} \\right)^{\\dagger} l_{\\text{true}} = \\frac{l_{\\text{est}}^{\\dagger} (K^{-1})^{\\dagger} l_{\\text{true}}}{l_{\\text{est}}^{\\dagger} K^{-1} l_{\\text{est}}} $$\nSince $K$ is a covariance or CSD matrix, it is Hermitian ($K^{\\dagger} = K$), and so is its inverse. Thus:\n$$ g = \\frac{l_{\\text{est}}^{\\dagger} K^{-1} l_{\\text{true}}}{l_{\\text{est}}^{\\dagger} K^{-1} l_{\\text{est}}} $$\nSubstituting $l_{\\text{est}} = L_r q_{\\text{est}}$ and $l_{\\text{true}} = L_r q_{\\text{true}}$, we get:\n$$ g = \\frac{q_{\\text{est}}^{\\dagger} L_r^{\\dagger} K^{-1} L_r q_{\\text{true}}}{q_{\\text{est}}^{\\dagger} L_r^{\\dagger} K^{-1} L_r q_{\\text{est}}} $$\nThe problem defines the orientation metric as $A = L_r^{\\dagger} K^{-1} L_r$. So the gain is:\n$$ g = \\frac{q_{\\text{est}}^{\\dagger} A q_{\\text{true}}}{q_{\\text{est}}^{\\dagger} A q_{\\text{est}}} $$\nNow, we apply the isotropy assumption, $A = \\alpha I_3$, where $\\alpha$ is a positive scalar and $I_3$ is the $3 \\times 3$ identity matrix. For both LCMV and DICS, the orientation vectors are real, so $q^{\\dagger} = q^{\\top}$.\n$$ g = \\frac{q_{\\text{est}}^{\\top} (\\alpha I_3) q_{\\text{true}}}{q_{\\text{est}}^{\\top} (\\alpha I_3) q_{\\text{est}}} = \\frac{\\alpha (q_{\\text{est}}^{\\top} q_{\\text{true}})}{\\alpha (q_{\\text{est}}^{\\top} q_{\\text{est}})} $$\nGiven that $q_{\\text{est}}$ and $q_{\\text{true}}$ are unit vectors, $q_{\\text{est}}^{\\top} q_{\\text{est}} = \\|q_{\\text{est}}\\|_2^2 = 1$. The cosine of the angle $\\theta$ between them is $q_{\\text{est}}^{\\top} q_{\\text{true}} = \\cos(\\theta)$. The gain simplifies to:\n$$ g = \\frac{\\cos(\\theta)}{1} = \\cos(\\theta) $$\nThe output power is proportional to the squared magnitude of the gain. For LCMV this is $g^2$, and for DICS it is $|g(f)|^2$. Since the gain is real in this case, for both methods the power is proportional to $\\cos^2(\\theta)$.\n\nThe sensitivity curve is defined as the output power normalized by its value at perfect alignment ($\\theta=0$).\n$$ \\text{Sensitivity}(\\theta) = \\frac{\\text{Power}(\\theta)}{\\text{Power}(0)} = \\frac{k \\cdot \\cos^2(\\theta)}{k \\cdot \\cos^2(0)} = \\frac{\\cos^2(\\theta)}{1} = \\cos^2(\\theta) $$\nwhere $k$ is the constant of proportionality. Thus, the sensitivity curve for both beamformers under the isotropy assumption is $\\cos^2(\\theta)$.",
            "answer": "$$ \\boxed{\\cos^{2}(\\theta)} $$"
        },
        {
            "introduction": "Theoretical knowledge comes to life through implementation. This hands-on practice guides you through the process of building a complete Monte Carlo simulation to evaluate LCMV beamformer performance. You will generate synthetic data, apply the beamforming algorithm, and systematically analyze how key parameters like data length and regularization strength impact localization accuracy, providing a concrete understanding of the trade-offs involved in practical applications. ",
            "id": "4141681",
            "problem": "You are tasked with designing and implementing a Monte Carlo simulation to quantify the localization error distribution of the Linearly Constrained Minimum Variance (LCMV) beamformer under varying sample sizes and regularization strengths, in a controlled synthetic Electroencephalography (EEG) sensor array scenario. The simulation must start from the linear forward model and the definition of the LCMV optimization problem, derive the algorithmic steps needed for source scanning, and compute summary statistics of the localization error distribution. All quantities involving physical distances must be expressed in meters. Your final program must produce the specified aggregate summary outputs for a predefined test suite.\n\nFundamental base and model assumptions:\n- Use the linear forward model for multichannel neural measurements. Let there be $M$ sensors with measurements collected over $N$ time samples. The sensor array is modeled by placing $M$ sensors on a circle of radius $R$ in the plane. A single neural source at location $\\mathbf{r}_{\\text{true}}$ generates a scalar time series $s(t)$ that linearly maps to sensors via a lead field vector $\\mathbf{l}(\\mathbf{r}_{\\text{true}}) \\in \\mathbb{R}^M$. The observed data at time $t$ is given by\n$$\n\\mathbf{x}(t) = s(t)\\,\\mathbf{l}(\\mathbf{r}_{\\text{true}}) + \\mathbf{n}(t),\n$$\nwhere $\\mathbf{n}(t)$ is additive sensor noise.\n- The sample covariance matrix $\\widehat{\\mathbf{C}} \\in \\mathbb{R}^{M \\times M}$ is computed from the demeaned data using the standard estimator\n$$\n\\widehat{\\mathbf{C}} = \\frac{1}{N} \\sum_{t=1}^{N} \\left( \\mathbf{x}(t) - \\bar{\\mathbf{x}} \\right)\\left( \\mathbf{x}(t) - \\bar{\\mathbf{x}} \\right)^\\top,\n$$\nwhere $\\bar{\\mathbf{x}}$ is the sample mean across time.\n- Regularization is applied as additive Tikhonov regularization\n$$\n\\widehat{\\mathbf{C}}_{\\lambda} = \\widehat{\\mathbf{C}} + \\lambda \\,\\alpha\\, \\mathbf{I},\n$$\nwhere $\\lambda \\ge 0$ is the regularization strength, $\\alpha = \\frac{\\operatorname{trace}(\\widehat{\\mathbf{C}})}{M}$ provides unit scaling, and $\\mathbf{I}$ is the identity matrix.\n\nBeamforming objective:\n- The LCMV beamformer (Linearly Constrained Minimum Variance) is defined by the constrained optimization\n$$\n\\min_{\\mathbf{w} \\in \\mathbb{R}^M} \\quad \\mathbf{w}^\\top \\widehat{\\mathbf{C}}_{\\lambda}\\, \\mathbf{w} \\quad \\text{subject to} \\quad \\mathbf{w}^\\top \\mathbf{l}(\\mathbf{r}) = 1,\n$$\nfor each candidate source location $\\mathbf{r}$ in a predefined scanning grid. You must derive from this formulation all necessary quantities used to score each location and to select the estimated source location $\\widehat{\\mathbf{r}}$.\n\nLead field:\n- Use a simplified quasistatic forward model in a homogeneous medium where the lead field for a scalar source at $\\mathbf{r}$ is defined component-wise at sensor position $\\mathbf{s}_m$ by\n$$\nl_m(\\mathbf{r}) = \\frac{1}{\\|\\mathbf{r} - \\mathbf{s}_m\\|^2},\n$$\nfor $m = 1, \\ldots, M$, with $\\|\\cdot\\|$ the Euclidean norm. This choice ensures physical decay with distance and a nontrivial spatial pattern across sensors.\n\nSensor array and scanning grid:\n- Place $M = 12$ sensors on a circle of radius $R = 0.09$ meters, evenly spaced in angle in the plane $z=0$ and centered at the origin. The true source location is $\\mathbf{r}_{\\text{true}} = (0.03, -0.02, 0)$ meters.\n- Define a two-dimensional scanning grid of candidate source locations in the plane $z=0$, spanning $x,y \\in [-0.06, 0.06]$ meters with uniform steps of $0.01$ meters. For each grid location $\\mathbf{r}$, compute the lead field $\\mathbf{l}(\\mathbf{r})$.\n\nNoise and signal generation:\n- For each Monte Carlo trial, generate source samples $s(t)$ as independent Gaussian random variables with zero mean and variance $\\sigma_s^2$, i.e.,\n$$\ns(t) \\sim \\mathcal{N}(0, \\sigma_s^2).\n$$\n- Generate sensor noise $\\mathbf{n}(t)$ from a zero-mean multivariate Gaussian with covariance\n$$\n\\boldsymbol{\\Sigma}_n = \\sigma_n^2 \\left[ (1-\\rho)\\,\\mathbf{I} + \\rho\\,\\mathbf{1}\\mathbf{1}^\\top \\right],\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^M$ is a vector of ones, $\\rho \\in [0,1)$ controls cross-sensor correlation, and $\\sigma_n^2$ controls noise power. Use $\\sigma_s^2 = 1.0$, $\\sigma_n = 0.1$, and $\\rho = 0.1$. These settings provide a realistic signal-to-noise ratio and ensure positive definiteness of $\\boldsymbol{\\Sigma}_n$.\n\nLocalization error metric:\n- For each trial, perform source scanning over the grid using the LCMV criterion derived from the optimization problem above to select the estimated location $\\widehat{\\mathbf{r}}$. Compute the localization error\n$$\ne = \\left\\| \\widehat{\\mathbf{r}} - \\mathbf{r}_{\\text{true}} \\right\\|,\n$$\nexpressed in meters.\n\nMonte Carlo protocol:\n- For each parameter pair $(N, \\lambda)$, run $T$ independent trials (with independent signal and noise draws), compute the sample covariance $\\widehat{\\mathbf{C}}$, apply regularization, perform source scanning to find $\\widehat{\\mathbf{r}}$, and record the localization error $e$ in meters.\n- Use $T = 200$ trials for each parameter pair.\n\nTest suite:\n- Evaluate the following parameter pairs $(N, \\lambda)$:\n    1. $(24, 0.0)$\n    2. $(24, 10^{-3})$\n    3. $(64, 0.0)$\n    4. $(64, 10^{-3})$\n    5. $(256, 0.0)$\n    6. $(256, 10^{-2})$\n\nRequired output:\n- For each parameter pair, compute and return the following summary statistics of the localization error distribution across the $T$ trials:\n    1. The mean localization error $\\overline{e}$ in meters.\n    2. The standard deviation of the localization error $\\operatorname{std}(e)$ in meters.\n    3. The empirical $0.95$ quantile (ninety-fifth percentile) of the localization error $q_{0.95}$ in meters.\n- Your program should produce a single line of output containing the results for all parameter pairs, formatted as a comma-separated list enclosed in square brackets. Each element corresponding to a parameter pair must itself be a list of the form $[N, \\lambda, \\overline{e}, \\operatorname{std}(e), q_{0.95}]$, for example:\n$$\n\\left[ [24, 0.0, 0.0123, 0.0045, 0.0199], [24, 0.001, \\ldots], \\ldots \\right]\n$$\nAll distances must be expressed in meters, and all numeric outputs should be in decimal form (no percentages or angle units are involved).",
            "solution": "The problem is well-posed and scientifically grounded. We will proceed with a solution.\n\nThe objective is to quantify the localization performance of a Linearly Constrained Minimum Variance (LCMV) beamformer through a Monte Carlo simulation. The analysis is based on a synthetic Electroencephalography (EEG) scenario defined by a linear forward model, specific sensor geometry, and a statistical model for the signal and noise.\n\n**1. Theoretical Framework**\n\n**1.1. Forward Model and Data Generation**\nThe relationship between the neural source activity and the sensor measurements is described by the linear forward model. For a single source at location $\\mathbf{r}_{\\text{true}}$ with time series $s(t)$, the measurements $\\mathbf{x}(t) \\in \\mathbb{R}^M$ from an array of $M$ sensors are given by:\n$$\n\\mathbf{x}(t) = s(t)\\,\\mathbf{l}(\\mathbf{r}_{\\text{true}}) + \\mathbf{n}(t)\n$$\nHere, $\\mathbf{l}(\\mathbf{r}_{\\text{true}}) \\in \\mathbb{R}^M$ is the lead field vector, which maps the source to the sensors, and $\\mathbf{n}(t) \\in \\mathbb{R}^M$ is additive sensor noise.\n\nFor the simulation, the source signal $s(t)$ for each time sample $t$ is drawn independently from a zero-mean Gaussian distribution with variance $\\sigma_s^2 = 1.0$:\n$$\ns(t) \\sim \\mathcal{N}(0, \\sigma_s^2)\n$$\nThe sensor noise $\\mathbf{n}(t)$ is drawn from a zero-mean multivariate Gaussian distribution with a structured covariance matrix $\\boldsymbol{\\Sigma}_n$:\n$$\n\\mathbf{n}(t) \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_n)\n$$\nwhere $\\boldsymbol{\\Sigma}_n = \\sigma_n^2 \\left[ (1-\\rho)\\,\\mathbf{I} + \\rho\\,\\mathbf{1}\\mathbf{1}^\\top \\right]$. The parameters are set to $\\sigma_n = 0.1$ and $\\rho = 0.1$, introducing a baseline noise level and spatial correlation across sensors. $\\mathbf{I}$ is the $M \\times M$ identity matrix and $\\mathbf{1}$ is a vector of ones. The signal and noise are assumed to be uncorrelated.\n\n**1.2. Lead Field Model**\nThe lead field models the sensitivity of each sensor to the source. The component $l_m(\\mathbf{r})$ for the $m$-th sensor at position $\\mathbf{s}_m$ due to a source at $\\mathbf{r}$ is given by a simplified quasistatic model:\n$$\nl_m(\\mathbf{r}) = \\frac{1}{\\|\\mathbf{r} - \\mathbf{s}_m\\|^2}\n$$\nThis model represents the decay of signal strength with the square of the distance, a common feature in physical fields.\n\n**1.3. Covariance Estimation and Regularization**\nThe LCMV beamformer relies on the data's second-order statistics, captured by the covariance matrix. Given $N$ time samples of data, the sample covariance matrix $\\widehat{\\mathbf{C}}$ is estimated as:\n$$\n\\widehat{\\mathbf{C}} = \\frac{1}{N} \\sum_{t=1}^{N} \\left( \\mathbf{x}(t) - \\bar{\\mathbf{x}} \\right)\\left( \\mathbf{x}(t) - \\bar{\\mathbf{x}} \\right)^\\top\n$$\nwhere $\\bar{\\mathbf{x}}$ is the time-averaged measurement vector. In practice, especially for a small number of samples $N$, $\\widehat{\\mathbf{C}}$ can be ill-conditioned. To improve its stability, Tikhonov regularization is applied:\n$$\n\\widehat{\\mathbf{C}}_{\\lambda} = \\widehat{\\mathbf{C}} + \\lambda \\,\\alpha\\, \\mathbf{I}\n$$\nThe regularization parameter is $\\lambda \\ge 0$. The scaling factor $\\alpha = \\frac{\\operatorname{trace}(\\widehat{\\mathbf{C}})}{M}$ normalizes the regularization strength relative to the average sensor power, making the choice of $\\lambda$ less dependent on the overall signal magnitude.\n\n**2. LCMV Beamformer Derivation and Source Localization**\n\n**2.1. Optimization Problem**\nThe LCMV beamformer estimates the source activity at a specific location $\\mathbf{r}$ by constructing a spatial filter $\\mathbf{w}(\\mathbf{r}) \\in \\mathbb{R}^M$. The filter is designed to pass signal from the location $\\mathbf{r}$ with unit gain while minimizing the filter's output power (variance), thereby suppressing contributions from noise and other locations. This is formulated as a constrained optimization problem:\n$$\n\\min_{\\mathbf{w} \\in \\mathbb{R}^M} \\mathbf{w}^\\top \\widehat{\\mathbf{C}}_{\\lambda}\\, \\mathbf{w} \\quad \\text{subject to} \\quad \\mathbf{w}^\\top \\mathbf{l}(\\mathbf{r}) = 1\n$$\n**2.2. Derivation of the Spatial Filter**\nThis problem can be solved using the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(\\mathbf{w}, \\mu) = \\mathbf{w}^\\top \\widehat{\\mathbf{C}}_{\\lambda}\\, \\mathbf{w} - \\mu \\left(\\mathbf{w}^\\top \\mathbf{l}(\\mathbf{r}) - 1\\right)\n$$\nTaking the gradient with respect to $\\mathbf{w}$ and setting it to zero gives:\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L} = 2 \\widehat{\\mathbf{C}}_{\\lambda} \\mathbf{w} - \\mu \\mathbf{l}(\\mathbf{r}) = \\mathbf{0} \\implies \\mathbf{w} = \\frac{\\mu}{2} \\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r})\n$$\nThe Lagrange multiplier $\\mu$ is found by enforcing the constraint $\\mathbf{w}^\\top \\mathbf{l}(\\mathbf{r}) = 1$:\n$$\n\\left(\\frac{\\mu}{2} \\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r})\\right)^\\top \\mathbf{l}(\\mathbf{r}) = 1 \\implies \\frac{\\mu}{2} \\left( \\mathbf{l}(\\mathbf{r})^\\top \\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r}) \\right) = 1\n$$\nSolving for $\\frac{\\mu}{2}$ yields $\\frac{\\mu}{2} = \\frac{1}{\\mathbf{l}(\\mathbf{r})^\\top \\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r})}$. Substituting this back into the expression for $\\mathbf{w}$ gives the optimal spatial filter for location $\\mathbf{r}$:\n$$\n\\mathbf{w}(\\mathbf{r}) = \\frac{\\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r})}{\\mathbf{l}(\\mathbf{r})^\\top \\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r})}\n$$\n\n**2.3. Source Localization via Power Scanning**\nThe output power of the beamformer for a source at location $\\mathbf{r}$ is $P(\\mathbf{r}) = \\mathbf{w}(\\mathbf{r})^\\top \\widehat{\\mathbf{C}}_{\\lambda} \\mathbf{w}(\\mathbf{r})$. Substituting the optimal filter $\\mathbf{w}(\\mathbf{r})$ gives:\n$$\nP(\\mathbf{r}) = \\frac{\\left(\\mathbf{l}(\\mathbf{r})^\\top \\widehat{\\mathbf{C}}_{\\lambda}^{-1}\\right) \\widehat{\\mathbf{C}}_{\\lambda} \\left(\\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r})\\right)}{\\left(\\mathbf{l}(\\mathbf{r})^\\top \\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r})\\right)^2} = \\frac{\\mathbf{l}(\\mathbf{r})^\\top \\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r})}{\\left(\\mathbf{l}(\\mathbf{r})^\\top \\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r})\\right)^2} = \\frac{1}{\\mathbf{l}(\\mathbf{r})^\\top \\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r})}\n$$\nTo localize the source, we scan over a predefined grid of candidate locations. For each grid point $\\mathbf{r}_j$, we compute the output power $P(\\mathbf{r}_j)$. The location that yields the maximum power is selected as the estimated source location, $\\widehat{\\mathbf{r}}$:\n$$\n\\widehat{\\mathbf{r}} = \\arg\\max_{\\mathbf{r} \\in \\text{grid}} P(\\mathbf{r}) = \\arg\\max_{\\mathbf{r} \\in \\text{grid}} \\left( \\frac{1}{\\mathbf{l}(\\mathbf{r})^\\top \\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r})} \\right)\n$$\n\n**3. Monte Carlo Simulation Protocol**\n\nThe simulation quantifies the distribution of the localization error, $e = \\| \\widehat{\\mathbf{r}} - \\mathbf{r}_{\\text{true}} \\|$, for different sample sizes $N$ and regularization strengths $\\lambda$.\n\n**Setup:**\n- Sensor array: $M = 12$ sensors are placed on a circle of radius $R = 0.09$ m in the $z=0$ plane, centered at the origin. The coordinates are $\\mathbf{s}_m = (R \\cos(2\\pi m/M), R \\sin(2\\pi m/M), 0)$ for $m=0, \\ldots, 11$.\n- True source location: $\\mathbf{r}_{\\text{true}} = (0.03, -0.02, 0)$ m.\n- Scanning grid: A $2$D grid in the $z=0$ plane, with $x$ and $y$ coordinates spanning $[-0.06, 0.06]$ m with a step of $0.01$ m.\n- Trials: $T = 200$ independent trials are run for each $(N, \\lambda)$ pair.\n\n**For each trial:**\n1.  **Generate Data**: For a given $N$, generate $N$ samples of the source signal $s(t)$ and sensor noise $\\mathbf{n}(t)$ according to the models specified. Construct the $M \\times N$ data matrix $\\mathbf{X}$.\n2.  **Estimate Covariance**: Compute the sample covariance matrix $\\widehat{\\mathbf{C}}$ from $\\mathbf{X}$.\n3.  **Regularize Covariance**: Compute $\\widehat{\\mathbf{C}}_{\\lambda} = \\widehat{\\mathbf{C}} + \\lambda \\frac{\\operatorname{trace}(\\widehat{\\mathbf{C}})}{M} \\mathbf{I}$.\n4.  **Perform Scanning**:\n    - Invert the regularized covariance matrix to get $\\widehat{\\mathbf{C}}_{\\lambda}^{-1}$.\n    - For each point $\\mathbf{r}_j$ on the scanning grid, calculate the lead field $\\mathbf{l}(\\mathbf{r}_j)$ and then the beamformer power $P(\\mathbf{r}_j) = (\\mathbf{l}(\\mathbf{r}_j)^\\top \\widehat{\\mathbf{C}}_{\\lambda}^{-1} \\mathbf{l}(\\mathbf{r}_j))^{-1}$.\n    - Identify the estimated source location $\\widehat{\\mathbf{r}}$ as the grid point with the maximum power.\n5.  **Calculate Error**: Compute the Euclidean distance $e = \\| \\widehat{\\mathbf{r}} - \\mathbf{r}_{\\text{true}} \\|$.\n\n**Analysis:**\nFor each $(N, \\lambda)$ pair from the test suite, the set of $T=200$ localization errors $\\{e_i\\}_{i=1}^{200}$ is collected. The following summary statistics are then computed:\n- Mean error: $\\overline{e} = \\frac{1}{T} \\sum_{i=1}^T e_i$.\n- Standard deviation of error: $\\operatorname{std}(e) = \\sqrt{\\frac{1}{T-1} \\sum_{i=1}^T (e_i - \\overline{e})^2}$.\n- $95^{th}$ percentile of error: $q_{0.95}$, the value below which $95\\%$ of the observed errors fall.\n\nThese steps are implemented programmatically to generate the required output for the specified test suite.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and implements a Monte Carlo simulation to quantify the localization\n    error distribution of the LCMV beamformer.\n    \"\"\"\n    # Set a seed for reproducibility of random number generation.\n    np.random.seed(0)\n\n    # --- Fixed Parameters ---\n    M = 12  # Number of sensors\n    R = 0.09  # Radius of sensor circle in meters\n    r_true = np.array([0.03, -0.02, 0.0])  # True source location in meters\n    sigma_s2 = 1.0  # Source signal variance\n    sigma_n = 0.1  # Sensor noise standard deviation\n    rho = 0.1  # Sensor noise correlation\n    T = 200  # Number of Monte Carlo trials per condition\n\n    # --- Sensor Array and Scanning Grid Setup ---\n    # Sensor positions\n    thetas = np.linspace(0, 2 * np.pi, M, endpoint=False)\n    sensor_pos = np.zeros((M, 3))\n    sensor_pos[:, 0] = R * np.cos(thetas)\n    sensor_pos[:, 1] = R * np.sin(thetas)\n\n    # Scanning grid positions\n    grid_coords = np.arange(-0.06, 0.061, 0.01) # Use 0.061 to include endpoint\n    grid_x, grid_y = np.meshgrid(grid_coords, grid_coords)\n    grid_points = np.vstack([grid_x.ravel(), grid_y.ravel(), np.zeros(grid_x.size)]).T\n\n    # --- Pre-computation ---\n    # Lead fields for all grid points\n    # L_grid has shape (M, num_grid_points)\n    dist_sq_grid = np.sum((grid_points[np.newaxis, :, :] - sensor_pos[:, np.newaxis, :])**2, axis=2)\n    # Avoid division by zero if a grid point coincides with a sensor\n    dist_sq_grid[dist_sq_grid == 0] = 1e-12 \n    L_grid = 1.0 / dist_sq_grid\n    \n    # Lead field for the true source location\n    dist_sq_true = np.sum((r_true - sensor_pos)**2, axis=1)\n    l_true = 1.0 / dist_sq_true\n    l_true = l_true[:, np.newaxis] # Shape (M, 1)\n\n    # Noise covariance matrix\n    I = np.eye(M)\n    ones_vec = np.ones((M, M))\n    Sigma_n = sigma_n**2 * ((1 - rho) * I + rho * ones_vec)\n    noise_mean = np.zeros(M)\n    \n    # Test suite from the problem statement\n    test_cases = [\n        (24, 0.0),\n        (24, 10**-3),\n        (64, 0.0),\n        (64, 10**-3),\n        (256, 0.0),\n        (256, 10**-2),\n    ]\n\n    results = []\n\n    for N, lam in test_cases:\n        localization_errors = np.zeros(T)\n\n        for t in range(T):\n            # 1. Data Generation\n            s = np.random.normal(0, np.sqrt(sigma_s2), size=N)\n            n = np.random.multivariate_normal(noise_mean, Sigma_n, size=N).T # Shape (M, N)\n            x = l_true * s + n  # Signal + Noise, shape (M, N)\n\n            # 2. Covariance Estimation\n            x_demeaned = x - np.mean(x, axis=1, keepdims=True)\n            C_hat = (x_demeaned @ x_demeaned.T) / N\n\n            # 3. Regularization\n            alpha = np.trace(C_hat) / M\n            C_lam = C_hat + lam * alpha * I\n\n            # 4. Source Scanning\n            try:\n                C_lam_inv = np.linalg.inv(C_lam)\n                # Denominator of power expression: l(r)^T * C_inv * l(r)\n                # Vectorized computation for all grid points\n                denominators = np.sum(L_grid * (C_lam_inv @ L_grid), axis=0)\n                # Power is 1/denominator; find max power\n                best_idx = np.argmax(1.0 / denominators)\n            except np.linalg.LinAlgError:\n                # If matrix is singular (can happen with lambda=0), localization fails.\n                # A robust simulation would handle this, but for this specific problem\n                # setup, N > M so the matrix is invertible with probability 1.\n                # We add the handler as a safeguard.\n                max_grid_err = np.max(np.linalg.norm(grid_points - r_true, axis=1))\n                localization_errors[t] = max_grid_err\n                continue\n\n            r_hat = grid_points[best_idx]\n\n            # 5. Error Calculation\n            error = np.linalg.norm(r_hat - r_true)\n            localization_errors[t] = error\n        \n        # --- Summary Statistics ---\n        mean_e = np.mean(localization_errors)\n        std_e = np.std(localization_errors, ddof=1) # Sample standard deviation\n        q95_e = np.quantile(localization_errors, 0.95)\n        \n        result_tuple = [N, lam, mean_e, std_e, q95_e]\n        results.append(result_tuple)\n\n    # Format the final output string\n    # str(list) produces '[val1, val2, ...]' which matches the format\n    formatted_results = ','.join(map(str, results))\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful beamforming analysis depends on more than just the core equations; it requires a robust implementation that can handle the challenges of real-world data. One of the most common issues is the numerical instability caused by ill-conditioned or rank-deficient covariance matrices. This exercise challenges you to critically evaluate a range of diagnostic checks and remedies, helping you build the practical wisdom needed to ensure your analyses are both stable and scientifically valid. ",
            "id": "4141744",
            "problem": "You are analyzing magnetoencephalography data for cortical source localization using the Linear Constrained Minimum Variance (LCMV) beamformer. The forward model maps a source to sensors via a leadfield matrix, and the LCMV method seeks sensor weights that minimize array output power subject to unit-gain constraints for the candidate source. Your sensor array has $M$ channels and you estimate the data covariance matrix $C$ from short time windows with $N$ samples per window. In practice, when $N < M$, the sample covariance $C$ becomes rank-deficient, which can make the LCMV weights unstable because the computation involves solving linear systems governed by $C$ and enforcing linear constraints. The Dynamic Imaging of Coherent Sources (DICS) method is a frequency-domain analog that uses cross-spectral density at a target frequency; analogous conditioning issues can arise depending on the number of segments/tapers.\n\nStarting from the definitions that (i) the sample covariance is $C = \\frac{1}{N} X X^{\\top}$ for data matrix $X \\in \\mathbb{R}^{M \\times N}$, (ii) $\\mathrm{rank}(C) \\leq \\min(M, N)$, and (iii) the LCMV weights arise from minimizing $w^{\\top} C w$ subject to linear constraints on $w$, identify those procedures that are scientifically sound as checks and remedies to ensure stable LCMV weights in the presence of $N < M$, collinearity across sensors, and finite-sample noise. Select all that apply.\n\nA. Inspect the eigenvalue spectrum of $C$ and compute the condition number $\\kappa(C)$; if several eigenvalues are near zero or $\\kappa(C)$ is large, expect instability and proceed accordingly.\n\nB. Apply Tikhonov regularization by forming $C_{\\mathrm{reg}} = C + \\lambda I$ with $\\lambda > 0$ chosen by principled criteria (for example, cross-validation or an L-curve), and then use $C_{\\mathrm{reg}}$ in the LCMV optimization.\n\nC. Increase $N$ under reasonable stationarity assumptions by aggregating more time samples or trials to improve the rank and conditioning of $C$.\n\nD. Use the Moore–Penrose pseudoinverse of $C$ with no singular-value thresholding (accepting the default numerical tolerance), thereby avoiding explicit regularization while still handling $N < M$.\n\nE. Project the sensor data onto the top $r$ principal components with $r \\leq N$ prior to covariance estimation, perform the LCMV computation in the reduced space, and back-project the resulting weights to the full sensor space.\n\nF. Replace $C$ with cross-spectral density at the target frequency and perform Dynamic Imaging of Coherent Sources (DICS) with the same constraints; the frequency-domain averaging guarantees full rank even when $N < M$.\n\nG. Orthonormalize leadfield columns for vector-source orientation handling and verify constraint feasibility by checking that the constraint matrix has full column rank; this reduces numerical coupling and improves stability.\n\nH. Whiten the sensor data using an independently estimated noise covariance $C_{n}$ from a baseline period, with rank-aware whitening that discards near-zero eigenvalues, then estimate $C$ in the whitened space and compute weights; assess stability by re-applying weights across held-out trials.\n\nI. Z-score each sensor to unit variance before estimating $C$, which ensures $C = I$ and hence a perfectly conditioned problem.",
            "solution": "The problem statement describes a common and critical issue in magnetoencephalography (MEG) and electroencephalography (EEG) source localization using beamforming techniques like the Linear Constrained Minimum Variance (LCMV) method. The core of the problem is the instability of the beamformer weights when the sample covariance matrix is rank-deficient or ill-conditioned, which frequently occurs when the number of time samples ($N$) used for estimation is smaller than the number of sensors ($M$). The problem asks to identify scientifically sound procedures for diagnosing and remedying this instability.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- **Method:** Linear Constrained Minimum Variance (LCMV) beamformer for cortical source localization from magnetoencephalography (MEG) data.\n- **System Components:** $M$ sensor channels, data covariance matrix $C$, forward model represented by a leadfield matrix.\n- **LCMV Objective:** Minimize sensor weight output power, $w^{\\top} C w$, subject to unit-gain constraints for a candidate source.\n- **Key Condition:** The number of samples per window, $N$, is less than the number of channels, $M$ ($N < M$).\n- **Consequence:** The sample covariance $C$ becomes rank-deficient, leading to unstable LCMV weights.\n- **Reason for Instability:** The computation requires solving linear systems involving $C$, which is ill-posed if $C$ is singular.\n- **Analogous Method:** Dynamic Imaging of Coherent Sources (DICS), a frequency-domain analog, faces similar conditioning issues.\n- **Formal Definitions:**\n    - (i) Sample covariance matrix: $C = \\frac{1}{N} X X^{\\top}$ for data matrix $X \\in \\mathbb{R}^{M \\times N}$.\n    - (ii) Rank property: $\\mathrm{rank}(C) \\leq \\min(M, N)$.\n    - (iii) Optimization problem: Minimize $w^{\\top} C w$ subject to linear constraints on $w$.\n- **Question:** Identify sound procedures to check and remedy the instability of LCMV weights under the conditions of $N < M$, sensor collinearity, and finite-sample noise.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Groundedness:** The problem is firmly rooted in the established fields of statistical signal processing and neuroimaging data analysis. LCMV beamforming, covariance estimation, rank deficiency due to $N < M$, and regularization methods are all standard, well-documented concepts. The formulation is accurate. (Verdict: Valid)\n- **Well-Posed:** The task is to evaluate a set of proposed procedures against established scientific principles. This is a well-posed problem that has a definite set of correct answers based on the literature of numerical linear algebra and beamforming. (Verdict: Valid)\n- **Objectivity:** The language is technical and precise. No subjective or opinion-based statements are present. (Verdict: Valid)\n- **Completeness and Consistency:** The problem provides sufficient context to understand the challenge of inverting a rank-deficient sample covariance matrix in the LCMV formula. There are no internal contradictions. The premise that $N < M$ leads to a rank-deficient $C$ (since $\\mathrm{rank}(XX^{\\top}) = \\mathrm{rank}(X) \\leq N$) is mathematically correct. (Verdict: Valid)\n- **Realism:** The scenario of $N < M$ is highly realistic and a common challenge in time-resolved M/EEG analysis, where short time windows are used to capture neural dynamics. (Verdict: Valid)\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically sound, well-posed, and accurately describes a real-world challenge in neuroscience data analysis. I will now proceed to derive the correct answer by evaluating each option.\n\n### Derivation and Option Analysis\n\nThe LCMV beamformer determines the optimal weight vector $w$ for a source at a given location by solving the constrained optimization problem:\n$$ \\min_{w} w^{\\top} C w \\quad \\text{subject to} \\quad L^{\\top} w = \\mathbf{1} $$\nwhere $C$ is the $M \\times M$ data covariance matrix and $L$ is the $M \\times d$ leadfield matrix for the source (with $d=1$ for a scalar source, $d=3$ for a vector source). The solution is found using Lagrange multipliers and is given by:\n$$ w = C^{-1} L (L^{\\top} C^{-1} L)^{-1} \\mathbf{1} $$\nThe critical computational step is the inversion of the covariance matrix, $C^{-1}$. If $C$ is rank-deficient (its determinant is zero) or ill-conditioned (its determinant is close to zero), the matrix inverse $C^{-1}$ is either non-existent or numerically unstable. This instability propagates to the weights $w$, which may become arbitrarily large and dominated by noise. The condition $N < M$ guarantees that the sample covariance $C = \\frac{1}{N} XX^{\\top}$ is rank-deficient, since its rank cannot exceed $N$. We must therefore employ methods to diagnose or fix this issue.\n\n**A. Inspect the eigenvalue spectrum of $C$ and compute the condition number $\\kappa(C)$; if several eigenvalues are near zero or $\\kappa(C)$ is large, expect instability and proceed accordingly.**\nThe condition number $\\kappa(C) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $C$, directly quantifies the sensitivity of the matrix inversion to numerical errors. For a positive semi-definite matrix like $C$, these are the largest and smallest eigenvalues. A very large $\\kappa(C)$ or the presence of eigenvalues near zero are definitive signs of ill-conditioning or singularity. This inspection is a standard, essential diagnostic step in numerical linear algebra before attempting to solve any linear system. It confirms whether a remedy is necessary.\n**Verdict: Correct**\n\n**B. Apply Tikhonov regularization by forming $C_{\\mathrm{reg}} = C + \\lambda I$ with $\\lambda > 0$ chosen by principled criteria (for example, cross-validation or an L-curve), and then use $C_{\\mathrm{reg}}$ in the LCMV optimization.**\nTikhonov regularization is a classic and robust method for solving ill-posed inverse problems. By adding a small, positive multiple of the identity matrix, $C_{\\mathrm{reg}} = C + \\lambda I$, the resulting matrix is guaranteed to be invertible. If the eigenvalues of $C$ are $\\mu_i \\ge 0$, the eigenvalues of $C_{\\mathrm{reg}}$ are $\\mu_i + \\lambda > 0$ for $\\lambda > 0$. This \"lifts\" the eigenvalues away from zero, making the matrix well-conditioned and its inverse stable. The regularization parameter $\\lambda$ controls the trade-off between fidelity to the original data and the stability of the solution. Choosing $\\lambda$ through principled methods is crucial for good performance. This is a primary and widely used remedy.\n**Verdict: Correct**\n\n**C. Increase $N$ under reasonable stationarity assumptions by aggregating more time samples or trials to improve the rank and conditioning of $C$.**\nThe root cause of the rank deficiency is that $N < M$. A direct solution is to increase the amount of data used to estimate $C$ such that $N \\ge M$. If more data from the same stationary process is available (e.g., longer time windows or more trials of an experiment), aggregating them will increase $N$. As $N$ approaches and exceeds $M$, the sample covariance $C$ will typically become full-rank and better conditioned, assuming the underlying signals are not perfectly collinear. The caveat regarding stationarity is scientifically critical and correctly noted.\n**Verdict: Correct**\n\n**D. Use the Moore–Penrose pseudoinverse of $C$ with no singular-value thresholding (accepting the default numerical tolerance), thereby avoiding explicit regularization while still handling $N < M$.**\nThe Moore-Penrose pseudoinverse $C^{+}$ provides a least-squares solution to linear systems. It is computed from the singular value decomposition (SVD) by inverting only the non-zero singular values. In practice, a numerical tolerance is used to distinguish non-zero from zero singular values. However, in the presence of noise, the covariance matrix has many small, non-zero singular values that are primarily noise-driven. Inverting these small values leads to amplification of noise, which is precisely the problem we seek to avoid. Using a generic, default numerical tolerance is not adapted to the signal-to-noise ratio of the data and is not a sound scientific procedure. A robust application would require careful thresholding of the singular values, which is itself a form of regularization. The procedure as described is naive and prone to failure.\n**Verdict: Incorrect**\n\n**E. Project the sensor data onto the top $r$ principal components with $r \\leq N$ prior to covariance estimation, perform the LCMV computation in the reduced space, and back-project the resulting weights to the full sensor space.**\nThis procedure is a form of dimensionality reduction, also known as a truncated SVD or Principal Component Analysis (PCA) based regularization. Since the data space has an effective rank of at most $N$, projecting the data onto the first $r \\le N$ principal components (the eigenvectors of $C$ corresponding to the largest eigenvalues) captures the data subspace while discarding dimensions with zero or near-zero variance (which are empty or noise-dominated). The LCMV problem is then solved in this smaller, well-conditioned $r$-dimensional space. The resulting weights can be projected back into the original $M$-dimensional sensor space. This is a standard and effective method for regularizing the covariance matrix.\n**Verdict: Correct**\n\n**F. Replace $C$ with cross-spectral density at the target frequency and perform Dynamic Imaging of Coherent Sources (DICS) with the same constraints; the frequency-domain averaging guarantees full rank even when $N < M$.**\nDICS is the frequency-domain analogue of LCMV and uses the cross-spectral density (CSD) matrix. The CSD is typically estimated by averaging spectrograms over multiple short-term segments or trials. The rank of the estimated CSD matrix is limited by the number of segments averaged, let's call it $K$. If $K < M$, the CSD matrix will be rank-deficient, just as the time-domain covariance matrix is when $N < M$. The statement that frequency-domain averaging \"guarantees\" full rank is false. Switching from LCMV to DICS does not inherently solve the rank-deficiency problem; it merely reframes it in terms of the number of averaged segments/tapers.\n**Verdict: Incorrect**\n\n**G. Orthonormalize leadfield columns for vector-source orientation handling and verify constraint feasibility by checking that the constraint matrix has full column rank; this reduces numerical coupling and improves stability.**\nThe stability of the LCMV weight calculation depends not only on the conditioning of $C$, but also on the conditioning of the constraint matrix $L$ and the final matrix to be inverted, $(L^{\\top} C^{-1} L)$. For vector beamformers, the columns of $L$ corresponding to different dipole orientations can be nearly collinear. Orthonormalizing them improves the numerical properties of the constraint set. Additionally, verifying that the constraint matrix $L$ has full column rank is a prerequisite for the problem to be well-posed. These are valid and important procedures for ensuring the overall numerical stability and correctness of the beamformer solution, complementing the regularization of $C$.\n**Verdict: Correct**\n\n**H. Whiten the sensor data using an independently estimated noise covariance $C_{n}$ from a baseline period, with rank-aware whitening that discards near-zero eigenvalues, then estimate $C$ in the whitened space and compute weights; assess stability by re-applying weights across held-out trials.**\nPre-whitening is a sophisticated regularization technique. It uses an independent estimate of the noise covariance, $C_n$, to transform the data such that the noise in the new space is identity-covariance. The whitening matrix $W$ is related to $C_n^{-1/2}$. The phrase \"rank-aware whitening\" correctly implies that one must use a regularized inverse of $C_n$ to compute $W$, preventing amplification of noise during the whitening step itself. The LCMV problem is then solved on the whitened data, where the structure of the noise has been accounted for. This is a powerful and scientifically sound method. Assessing stability on held-out data is a standard model validation technique.\n**Verdict: Correct**\n\n**I. Z-score each sensor to unit variance before estimating $C$, which ensures $C = I$ and hence a perfectly conditioned problem.**\nZ-scoring a variable means subtracting its mean and dividing by its standard deviation. Applying this to each sensor channel forces the diagonal elements of the subsequent covariance matrix to be $1$. This resulting matrix is the correlation matrix, not the identity matrix $I$. The off-diagonal elements, representing the correlations between sensors, are generally non-zero. If sensors are highly correlated (which is expected in M/EEG), the correlation matrix can be just as ill-conditioned or rank-deficient as the original covariance matrix. The claim that z-scoring ensures $C=I$ is factually incorrect. Therefore, the proposed rationale is fundamentally flawed.\n**Verdict: Incorrect**\n\nSummary of Correct Options: A, B, C, E, G, H.",
            "answer": "$$\\boxed{ABCEGH}$$"
        }
    ]
}