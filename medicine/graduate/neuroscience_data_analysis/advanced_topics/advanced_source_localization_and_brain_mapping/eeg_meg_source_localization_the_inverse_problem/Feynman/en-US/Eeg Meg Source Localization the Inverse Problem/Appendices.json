{
    "hands_on_practices": [
        {
            "introduction": "The first step in tackling the inverse problem is to accurately define the forward model, $y = Gx$. This exercise focuses on a critical modeling choice: how we represent the neural sources themselves. You will explore how constraining current dipoles to be perpendicular to the cortical surface, a common and physiologically-motivated assumption, alters the structure and dimensionality of the gain matrix $G$ compared to a model with unconstrained, free-orientations. This practice is fundamental to understanding how our physical assumptions about the brain directly shape the mathematical formulation of the inverse problem. ",
            "id": "4158015",
            "problem": "You are given a linear forward model for Electroencephalography (EEG) and Magnetoencephalography (MEG) under the quasi-static approximation of Maxwellâ€™s equations, where the measured sensor data are a linear superposition of fields generated by primary current dipoles on the cortical surface. Consider a triangulated cortical surface with $N$ vertices, where each vertex $i$ has a unit surface normal $\\mathbf{n}_{i} \\in \\mathbb{R}^{3}$. Under the free-orientation model, the current at vertex $i$ is a three-dimensional vector $\\mathbf{j}_{i} \\in \\mathbb{R}^{3}$, unconstrained in orientation. Under the fixed-orientation model, the current at vertex $i$ is constrained to lie along $\\mathbf{n}_{i}$, that is $\\mathbf{j}_{i} = q_{i}\\,\\mathbf{n}_{i}$ with $q_{i} \\in \\mathbb{R}$. The linearity of the forward model implies that the sensor data $\\mathbf{y} \\in \\mathbb{R}^{M}$ can be written as a linear combination of contributions from each vertex.\n\nStarting from the linearity of the forward model and the definitions above, do the following:\n\n1. Derive, from first principles, the sensor-space mapping (gain) matrices for the free- and fixed-orientation source models in terms of the lead-field blocks $\\mathbf{L}_{i} \\in \\mathbb{R}^{M \\times 3}$, where $\\mathbf{L}_{i}$ maps the three orthogonal unit dipoles at vertex $i$ to the $M$ sensors. Explicitly define $\\mathbf{G}_{\\mathrm{free}}$ and $\\mathbf{G}_{\\mathrm{fix}}$ as functions of $\\{\\mathbf{L}_{i}\\}_{i=1}^{N}$ and $\\{\\mathbf{n}_{i}\\}_{i=1}^{N}$.\n\n2. To quantify how the gain matrix changes under the fixed-orientation assumption relative to the free-orientation model, consider the dimensionless ratio\n$$\nr \\equiv \\frac{\\|\\mathbf{G}_{\\mathrm{fix}}\\|_{F}^{2}}{\\|\\mathbf{G}_{\\mathrm{free}}\\|_{F}^{2}},\n$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. For a concrete configuration with $M=3$ sensors and $N=2$ vertices, suppose the lead-field blocks and unit normals are\n$$\n\\mathbf{L}_{1} = \\begin{pmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & 1 \\\\\n-1 & 2 & 0\n\\end{pmatrix}, \\quad\n\\mathbf{L}_{2} = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n1 & 1 & 1 \\\\\n0 & 2 & -2\n\\end{pmatrix},\n$$\n$$\n\\mathbf{n}_{1} = \\frac{1}{\\sqrt{6}}\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad\n\\mathbf{n}_{2} = \\frac{1}{\\sqrt{14}}\\begin{pmatrix} 3 \\\\ 1 \\\\ -2 \\end{pmatrix}.\n$$\nCompute the exact value of $r$ for this configuration.\n\nYour final answer must be a single real number written in exact form. No rounding is required and no units are to be reported.",
            "solution": "The problem requires a two-part solution: first, the derivation of the gain matrices for free- and fixed-orientation source models, and second, the computation of a ratio of their squared Frobenius norms for a specific configuration.\n\n### Part 1: Derivation of Gain Matrices\n\nThe relationship between the source currents and the measured sensor data is linear and can be expressed as $\\mathbf{y} = \\mathbf{G}\\mathbf{J}$, where $\\mathbf{y} \\in \\mathbb{R}^{M}$ is the vector of sensor measurements, $\\mathbf{J}$ is the vector describing the source currents, and $\\mathbf{G}$ is the gain matrix (or lead-field matrix). The structure of $\\mathbf{J}$ and $\\mathbf{G}$ depends on the assumptions made about the source currents. The total sensor signal is the linear superposition of the signals generated by the current at each vertex $i$:\n$$\n\\mathbf{y} = \\sum_{i=1}^{N} (\\text{contribution from vertex } i)\n$$\nThe contribution from the current $\\mathbf{j}_i \\in \\mathbb{R}^3$ at vertex $i$ is given by $\\mathbf{L}_i \\mathbf{j}_i$, where $\\mathbf{L}_i \\in \\mathbb{R}^{M \\times 3}$ is the lead-field block for that vertex. Thus, the general expression for the sensor data is:\n$$\n\\mathbf{y} = \\sum_{i=1}^{N} \\mathbf{L}_{i} \\mathbf{j}_{i}\n$$\n\n**1. Free-Orientation Model ($\\mathbf{G}_{\\mathrm{free}}$)**\n\nIn the free-orientation model, the current at each vertex $i$, $\\mathbf{j}_i \\in \\mathbb{R}^3$, is unconstrained. The entire source distribution can be described by concatenating all $N$ current vectors into a single large source vector $\\mathbf{J}_{\\mathrm{free}} \\in \\mathbb{R}^{3N}$:\n$$\n\\mathbf{J}_{\\mathrm{free}} = \\begin{pmatrix}\n\\mathbf{j}_{1} \\\\\n\\mathbf{j}_{2} \\\\\n\\vdots \\\\\n\\mathbf{j}_{N}\n\\end{pmatrix}\n$$\nWe need to find the matrix $\\mathbf{G}_{\\mathrm{free}}$ such that $\\mathbf{y} = \\mathbf{G}_{\\mathrm{free}} \\mathbf{J}_{\\mathrm{free}}$. We can rewrite the summation for $\\mathbf{y}$ using block matrix multiplication:\n$$\n\\mathbf{y} = \\sum_{i=1}^{N} \\mathbf{L}_{i} \\mathbf{j}_{i} =\n\\begin{pmatrix}\n\\mathbf{L}_{1} & \\mathbf{L}_{2} & \\cdots & \\mathbf{L}_{N}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{j}_{1} \\\\\n\\mathbf{j}_{2} \\\\\n\\vdots \\\\\n\\mathbf{j}_{N}\n\\end{pmatrix}\n$$\nBy comparing this with $\\mathbf{y} = \\mathbf{G}_{\\mathrm{free}} \\mathbf{J}_{\\mathrm{free}}$, we identify the free-orientation gain matrix $\\mathbf{G}_{\\mathrm{free}} \\in \\mathbb{R}^{M \\times 3N}$ as the horizontal concatenation of the lead-field blocks:\n$$\n\\mathbf{G}_{\\mathrm{free}} = \\begin{pmatrix}\n\\mathbf{L}_{1} & \\mathbf{L}_{2} & \\cdots & \\mathbf{L}_{N}\n\\end{pmatrix}\n$$\n\n**2. Fixed-Orientation Model ($\\mathbf{G}_{\\mathrm{fix}}$)**\n\nIn the fixed-orientation model, the current at each vertex $i$ is constrained to be parallel to the local surface normal $\\mathbf{n}_i$, i.e., $\\mathbf{j}_i = q_i \\mathbf{n}_i$, where $q_i \\in \\mathbb{R}$ is the scalar current magnitude. The source distribution is now described by the vector of these scalar magnitudes, $\\mathbf{q} \\in \\mathbb{R}^{N}$:\n$$\n\\mathbf{q} = \\begin{pmatrix}\nq_{1} \\\\\nq_{2} \\\\\n\\vdots \\\\\nq_{N}\n\\end{pmatrix}\n$$\nWe substitute the constraint into the general expression for $\\mathbf{y}$:\n$$\n\\mathbf{y} = \\sum_{i=1}^{N} \\mathbf{L}_{i} \\mathbf{j}_{i} = \\sum_{i=1}^{N} \\mathbf{L}_{i} (q_{i} \\mathbf{n}_{i})\n$$\nSince $q_i$ is a scalar, we can rearrange the terms:\n$$\n\\mathbf{y} = \\sum_{i=1}^{N} q_{i} (\\mathbf{L}_{i} \\mathbf{n}_{i})\n$$\nLet's define a new vector $\\mathbf{g}_i = \\mathbf{L}_i \\mathbf{n}_i \\in \\mathbb{R}^M$. This vector represents the sensor pattern generated by a unit current at vertex $i$ oriented along $\\mathbf{n}_i$. The summation becomes:\n$$\n\\mathbf{y} = \\sum_{i=1}^{N} q_{i} \\mathbf{g}_{i} =\n\\begin{pmatrix}\n\\mathbf{g}_{1} & \\mathbf{g}_{2} & \\cdots & \\mathbf{g}_{N}\n\\end{pmatrix}\n\\begin{pmatrix}\nq_{1} \\\\\nq_{2} \\\\\n\\vdots \\\\\nq_{N}\n\\end{pmatrix}\n$$\nBy comparing this with $\\mathbf{y} = \\mathbf{G}_{\\mathrm{fix}} \\mathbf{q}$, we identify the fixed-orientation gain matrix $\\mathbf{G}_{\\mathrm{fix}} \\in \\mathbb{R}^{M \\times N}$ as the horizontal concatenation of the vectors $\\mathbf{g}_i$:\n$$\n\\mathbf{G}_{\\mathrm{fix}} = \\begin{pmatrix}\n\\mathbf{L}_{1}\\mathbf{n}_{1} & \\mathbf{L}_{2}\\mathbf{n}_{2} & \\cdots & \\mathbf{L}_{N}\\mathbf{n}_{N}\n\\end{pmatrix}\n$$\n\n### Part 2: Calculation of the Ratio $r$\n\nWe are asked to compute $r = \\frac{\\|\\mathbf{G}_{\\mathrm{fix}}\\|_{F}^{2}}{\\|\\mathbf{G}_{\\mathrm{free}}\\|_{F}^{2}}$ for the given configuration with $N=2$. The Frobenius norm squared, $\\|\\mathbf{A}\\|_F^2$, is the sum of the squares of all matrix entries.\n\n**1. Compute $\\|\\mathbf{G}_{\\mathrm{free}}\\|_{F}^{2}$**\n\nFor $N=2$, $\\mathbf{G}_{\\mathrm{free}} = \\begin{pmatrix} \\mathbf{L}_{1} & \\mathbf{L}_{2} \\end{pmatrix}$. The squared Frobenius norm of a block matrix is the sum of the squared Frobenius norms of its blocks.\n$$\n\\|\\mathbf{G}_{\\mathrm{free}}\\|_{F}^{2} = \\|\\mathbf{L}_{1}\\|_{F}^{2} + \\|\\mathbf{L}_{2}\\|_{F}^{2}\n$$\nGiven $\\mathbf{L}_{1} = \\begin{pmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 1 \\\\ -1 & 2 & 0 \\end{pmatrix}$ and $\\mathbf{L}_{2} = \\begin{pmatrix} 2 & -1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 2 & -2 \\end{pmatrix}$:\n$$\n\\|\\mathbf{L}_{1}\\|_{F}^{2} = 1^{2} + 0^{2} + 2^{2} + 0^{2} + 1^{2} + 1^{2} + (-1)^{2} + 2^{2} + 0^{2} = 1 + 4 + 1 + 1 + 1 + 4 = 12\n$$\n$$\n\\|\\mathbf{L}_{2}\\|_{F}^{2} = 2^{2} + (-1)^{2} + 0^{2} + 1^{2} + 1^{2} + 1^{2} + 0^{2} + 2^{2} + (-2)^{2} = 4 + 1 + 1 + 1 + 1 + 4 + 4 = 16\n$$\nTherefore:\n$$\n\\|\\mathbf{G}_{\\mathrm{free}}\\|_{F}^{2} = 12 + 16 = 28\n$$\n\n**2. Compute $\\|\\mathbf{G}_{\\mathrm{fix}}\\|_{F}^{2}$**\n\nFor $N=2$, $\\mathbf{G}_{\\mathrm{fix}} = \\begin{pmatrix} \\mathbf{g}_{1} & \\mathbf{g}_{2} \\end{pmatrix}$, where $\\mathbf{g}_{1} = \\mathbf{L}_{1}\\mathbf{n}_{1}$ and $\\mathbf{g}_{2} = \\mathbf{L}_{2}\\mathbf{n}_{2}$. The squared Frobenius norm is the sum of the squared Euclidean norms of its columns.\n$$\n\\|\\mathbf{G}_{\\mathrm{fix}}\\|_{F}^{2} = \\|\\mathbf{g}_{1}\\|_{2}^{2} + \\|\\mathbf{g}_{2}\\|_{2}^{2}\n$$\nFirst, we compute $\\mathbf{g}_1$ and $\\mathbf{g}_2$.\n$$\n\\mathbf{g}_{1} = \\mathbf{L}_{1}\\mathbf{n}_{1} = \\begin{pmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 1 \\\\ -1 & 2 & 0 \\end{pmatrix} \\frac{1}{\\sqrt{6}}\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{6}} \\begin{pmatrix} 1(1) + 0(2) + 2(1) \\\\ 0(1) + 1(2) + 1(1) \\\\ -1(1) + 2(2) + 0(1) \\end{pmatrix} = \\frac{1}{\\sqrt{6}} \\begin{pmatrix} 3 \\\\ 3 \\\\ 3 \\end{pmatrix}\n$$\n$$\n\\mathbf{g}_{2} = \\mathbf{L}_{2}\\mathbf{n}_{2} = \\begin{pmatrix} 2 & -1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 2 & -2 \\end{pmatrix} \\frac{1}{\\sqrt{14}}\\begin{pmatrix} 3 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\frac{1}{\\sqrt{14}} \\begin{pmatrix} 2(3) - 1(1) + 0(-2) \\\\ 1(3) + 1(1) + 1(-2) \\\\ 0(3) + 2(1) - 2(-2) \\end{pmatrix} = \\frac{1}{\\sqrt{14}} \\begin{pmatrix} 5 \\\\ 2 \\\\ 6 \\end{pmatrix}\n$$\nNext, we compute their squared Euclidean norms:\n$$\n\\|\\mathbf{g}_{1}\\|_{2}^{2} = \\left(\\frac{1}{\\sqrt{6}}\\right)^{2} (3^{2} + 3^{2} + 3^{2}) = \\frac{1}{6}(9+9+9) = \\frac{27}{6} = \\frac{9}{2}\n$$\n$$\n\\|\\mathbf{g}_{2}\\|_{2}^{2} = \\left(\\frac{1}{\\sqrt{14}}\\right)^{2} (5^{2} + 2^{2} + 6^{2}) = \\frac{1}{14}(25+4+36) = \\frac{65}{14}\n$$\nNow, we sum them to find $\\|\\mathbf{G}_{\\mathrm{fix}}\\|_{F}^{2}$:\n$$\n\\|\\mathbf{G}_{\\mathrm{fix}}\\|_{F}^{2} = \\frac{9}{2} + \\frac{65}{14} = \\frac{9 \\cdot 7}{2 \\cdot 7} + \\frac{65}{14} = \\frac{63}{14} + \\frac{65}{14} = \\frac{128}{14} = \\frac{64}{7}\n$$\n\n**3. Compute the ratio $r$**\n\nFinally, we compute the ratio $r$:\n$$\nr = \\frac{\\|\\mathbf{G}_{\\mathrm{fix}}\\|_{F}^{2}}{\\|\\mathbf{G}_{\\mathrm{free}}\\|_{F}^{2}} = \\frac{64/7}{28} = \\frac{64}{7 \\times 28} = \\frac{64}{196}\n$$\nTo simplify the fraction, we divide the numerator and denominator by their greatest common divisor, which is $4$.\n$$\nr = \\frac{64 \\div 4}{196 \\div 4} = \\frac{16}{49}\n$$",
            "answer": "$$\n\\boxed{\\frac{16}{49}}\n$$"
        },
        {
            "introduction": "Once we have a forward model, we can formulate an inverse operator, $W$, to estimate the underlying source activity via $\\hat{x} = Wy$. However, not all inverse solutions are created equal. This practice introduces the resolution matrix, $R = WG$, a powerful tool for quantitatively evaluating the performance of any linear inverse method. By analyzing the resolution matrix, we can characterize an estimator's spatial accuracy and understand its inherent limitations, such as spatial blurring and signal leakage. Mastering these evaluation techniques is essential for the critical interpretation of source localization results. ",
            "id": "4158059",
            "problem": "You are given a linear forward model for Electroencephalography (EEG) and Magnetoencephalography (MEG), where measurements are modeled in the quasi-static regime of Maxwell's equations as a linear superposition of neural current sources plus additive noise. In this setting, the forward model is expressed as $y = G x + n$, where $y \\in \\mathbb{R}^{m}$ are the sensor measurements, $x \\in \\mathbb{R}^{p}$ are source amplitudes on a $p$-dimensional source grid, $G \\in \\mathbb{R}^{m \\times p}$ is the lead-field (gain) matrix determined by electromagnetism and head conductivities, and $n \\in \\mathbb{R}^{m}$ is additive noise. Assume $n$ is zero-mean Gaussian with covariance $R_n \\in \\mathbb{R}^{m \\times m}$, and $x$ is zero-mean Gaussian with covariance $R_x \\in \\mathbb{R}^{p \\times p}$. Consider the minimum norm solution that arises from minimizing the expected squared error under these Gaussian assumptions and linearity. The inverse estimator is linear, $ \\hat{x} = W y$, and the resolution kernel (resolution matrix) is $R = W G \\in \\mathbb{R}^{p \\times p}$. The column $R_{:,j}$ is the point-spread function for a unit-amplitude focal source at location $j$.\n\nYour task is to derive $W$ from first principles using the Gaussian linear model and compute the resolution kernel $R$, then analyze the point-spread function for a specified focal source index $j_0$. For analysis, define the amplitude point-spread vector $p \\in \\mathbb{R}^{p}$ as $p_i = |R_{i,j_0}|$. Normalize it to unit $\\ell_1$ mass as $\\tilde{p} = p / \\sum_{i=1}^{p} p_i$ if the denominator is nonzero, otherwise set $\\tilde{p} = 0$. Let the peak index be $\\arg\\max_i p_i$. The full-width at half-maximum (FWHM) is defined on the one-dimensional source grid as the contiguous width around $j_0$ including indices where $p_i \\ge \\frac{1}{2} p_{j_0}$; if $p_{j_0} = 0$, define the FWHM to be $0$. For a neighborhood radius $r \\in \\mathbb{N}$, the leakage fraction outside the radius-$r$ neighborhood is $1 - \\sum_{|i-j_0|\\le r} \\tilde{p}_i$. Return the following triple for each test case: an indicator (as $1$ for true or $0$ for false) of whether the peak is at $j_0$, the FWHM as a nonnegative integer, and the leakage fraction as a float rounded to six decimal places.\n\nUse the following test suite. Each test case consists of a lead-field matrix $G$, a noise covariance $R_n$, a source covariance $R_x$, a focal index $j_0$, and a neighborhood radius $r$. All matrices are real-valued. The grids are one-dimensional and ordered by index.\n\nTest case $1$ (happy path, homogeneous source prior, homogeneous noise):\n- $m = 4$, $p = 6$.\n- $G = \\begin{bmatrix}\n0.8 & 0.1 & 0.0 & -0.2 & 0.3 & 0.5 \\\\\n0.0 & 0.5 & 0.4 & 0.1 & -0.1 & 0.2 \\\\\n0.2 & -0.3 & 0.6 & 0.0 & 0.4 & -0.2 \\\\\n-0.1 & 0.2 & -0.2 & 0.7 & 0.1 & 0.0\n\\end{bmatrix}$.\n- $R_n = 0.05 I_{4}$.\n- $R_x = I_{6}$.\n- $j_0 = 3$.\n- $r = 1$.\n\nTest case $2$ (ill-conditioned forward model columns, homogeneous source prior, low noise, boundary focal index):\n- $m = 3$, $p = 5$.\n- $G = \\begin{bmatrix}\n0.6 & 0.9 & 0.91 & 0.1 & 0.0 \\\\\n0.0 & 0.2 & 0.21 & 0.0 & 0.5 \\\\\n0.1 & 0.3 & 0.29 & 0.0 & -0.1\n\\end{bmatrix}$.\n- $R_n = 0.01 I_{3}$.\n- $R_x = I_{5}$.\n- $j_0 = 0$.\n- $r = 0$.\n\nTest case $3$ (anisotropic source prior and anisotropic noise, boundary focal index):\n- $m = 4$, $p = 7$.\n- $G = \\begin{bmatrix}\n0.4 & -0.2 & 0.0 & 0.1 & 0.5 & 0.3 & 0.0 \\\\\n0.0 & 0.1 & 0.2 & -0.4 & 0.2 & 0.0 & 0.1 \\\\\n0.1 & 0.0 & -0.3 & 0.2 & 0.0 & 0.4 & -0.2 \\\\\n0.2 & 0.3 & 0.1 & 0.0 & -0.1 & 0.2 & 0.5\n\\end{bmatrix}$.\n- $R_n = \\mathrm{diag}(0.02, 0.05, 0.03, 0.08)$.\n- $R_x = \\mathrm{diag}(1.0, 0.5, 0.2, 0.2, 0.5, 1.0, 1.5)$.\n- $j_0 = 6$.\n- $r = 2$.\n\nRequirements:\n- Derive $W$ from the Gaussian linear model and compute $R = W G$ for each test case without using any external data beyond the matrices listed.\n- For each test case, compute:\n  $1)$ the peak correctness indicator as $1$ if $\\arg\\max_i p_i = j_0$, else $0$,\n  $2)$ the FWHM as a nonnegative integer as defined above,\n  $3)$ the leakage fraction outside radius $r$ based on $\\tilde{p}$, rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the triple for a test case encoded as a bracketed comma-separated list. For example, an output for three test cases should look like $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$, where $a_k$ is an integer in $\\{0,1\\}$, $b_k$ is a nonnegative integer, and $c_k$ is a float with six digits after the decimal point.",
            "solution": "The objective is to find a linear estimator $\\hat{x} = Wy$ that minimizes the expected squared error, $J(W) = E[\\|x - \\hat{x}\\|^2]$. This is a classic problem in estimation theory, and the solution is known as the Wiener filter.\n\n**1. Derivation of the Minimum Mean Square Error (MMSE) Estimator**\n\nThe estimation error is $e = x - \\hat{x} = x - Wy$. Substituting the forward model $y = Gx + n$, we get:\n$$ e = x - W(Gx + n) = (I - WG)x - Wn $$\nThe cost function is the trace of the error covariance matrix $C_e = E[ee^T]$:\n$$ J(W) = E[\\|e\\|^2] = \\mathrm{Tr}(C_e) $$\nThe error covariance is:\n$$ C_e = E[((I - WG)x - Wn)((I - WG)x - Wn)^T] $$\nSince the source signal $x$ and noise $n$ are independent and have zero mean, the cross-term $E[xn^T]$ is zero. Thus, the expectation expands to:\n$$ C_e = E[(I - WG)xx^T(I - WG)^T] + E[Wnn^TW^T] $$\nUsing the definitions $R_x = E[xx^T]$ and $R_n = E[nn^T]$, we have:\n$$ C_e = (I - WG)R_x(I - WG)^T + WR_nW^T $$\nThe cost function to minimize is:\n$$ J(W) = \\mathrm{Tr}((I - WG)R_x(I - WG)^T + WR_nW^T) $$\nTo find the optimal $W$, we compute the matrix derivative of $J(W)$ with respect to $W$ and set it to zero. Expanding the trace expression:\n$$ J(W) = \\mathrm{Tr}(R_x - WGR_x - R_xG^TW^T + WGR_xG^TW^T + WR_nW^T) $$\nUsing standard matrix calculus identities, the derivative is:\n$$ \\frac{\\partial J(W)}{\\partial W} = -2R_xG^T + 2W(GR_xG^T + R_n) = 0 $$\nSolving for $W$ yields:\n$$ W(GR_xG^T + R_n) = R_xG^T $$\n$$ W = R_xG^T(GR_xG^T + R_n)^{-1} $$\nThe term $(GR_xG^T + R_n)$ is the covariance of the measurements $y$, as $C_y = E[yy^T] = E[(Gx+n)(Gx+n)^T] = GR_xG^T + R_n$. This term is an $m \\times m$ matrix. Its invertibility is guaranteed as $R_n$ is positive definite. This expression for $W$ is the Wiener filter, which is the optimal linear MMSE estimator.\n\n**2. Resolution Analysis**\n\nThe resolution matrix $R$ describes how the estimator $\\hat{x}$ responds to the true source distribution $x$. For a noise-free scenario ($n=0$, so $y=Gx$), the estimate would be $\\hat{x} = W y = WG x = Rx$. The resolution matrix $R$ thus maps the true source activity to the estimated source activity.\n$$ R = WG = R_xG^T(GR_xG^T + R_n)^{-1}G $$\nThe $j$-th column of $R$, denoted $R_{:,j}$, is the point-spread function (PSF). It represents the estimated source distribution when the true activity is a unit-amplitude focal source at location $j$ (i.e., $x$ is a vector of zeros with a $1$ at index $j$).\n\nThe analysis metrics are computed from this PSF.\n- The amplitude PSF vector $p$ is defined by $p_i = |R_{i,j_0}|$.\n- The normalized amplitude PSF vector $\\tilde{p}$ is $p$ normalized to have a unit $\\ell_1$ norm: $\\tilde{p} = p / \\sum_i p_i$.\n\n**3. Metrics Calculation**\n\nFor each test case with a given focal index $j_0$ and radius $r$, we compute the following three metrics:\n\n- **Peak Correctness**: We find the index of the maximum value of the amplitude PSF vector $p$, i.e., $i_{peak} = \\arg\\max_i p_i$. The indicator is $1$ if $i_{peak} = j_0$, and $0$ otherwise.\n\n- **FWHM**: The Full-Width at Half-Maximum is calculated based on the amplitude PSF vector $p$. First, we check if the amplitude at the focal source location, $p_{j_0}$, is zero. If so, the FWHM is $0$. Otherwise, we define a threshold $T = 0.5 \\times p_{j_0}$. The FWHM is the size of the largest contiguous block of indices containing $j_0$ for which $p_i \\ge T$. This is found by starting with a width of $1$ (for $j_0$ itself) and expanding leftwards and rightwards from $j_0$ as long as the condition $p_i \\ge T$ holds for all intermediate indices.\n\n- **Leakage Fraction**: This metric quantifies how much of the estimated activity \"leaks\" outside a defined neighborhood of the true source. Using the normalized amplitude PSF $\\tilde{p}$, we sum the probabilities within a radius $r$ of the focal source $j_0$. The neighborhood includes indices $i$ such that $|i - j_0| \\le r$. The leakage is the complement of this sum:\n$$ \\text{Leakage} = 1 - \\sum_{i = \\max(0, j_0-r)}^{\\min(p-1, j_0+r)} \\tilde{p}_i $$\n\nThe computational procedure involves implementing these steps for each test case.\n1.  Define the matrices $G$, $R_n$, $R_x$ and parameters $j_0$, $r$.\n2.  Compute the matrix $M = GR_xG^T + R_n$.\n3.  Compute its inverse, $M^{-1}$.\n4.  Compute the inverse operator $W = R_xG^T M^{-1}$.\n5.  Compute the resolution matrix $R = WG$.\n6.  Extract the PSF, which is the column $R_{:,j_0}$.\n7.  Calculate the amplitude PSF vector $p$ by taking the absolute value of the PSF.\n8.  Calculate the normalized amplitude PSF vector $\\tilde{p}$.\n9.  Compute the peak correctness indicator, the FWHM, and the leakage fraction according to the definitions above.\n10. Format and store the resulting triple of values.",
            "answer": "```python\nimport numpy as np\n\ndef compute_metrics(G, Rn, Rx, j0, r):\n    \"\"\"\n    Computes the resolution metrics for a given EEG/MEG inverse problem setup.\n\n    Args:\n        G (np.ndarray): The lead-field matrix (m x p).\n        Rn (np.ndarray): The noise covariance matrix (m x m).\n        Rx (np.ndarray): The source covariance matrix (p x p).\n        j0 (int): The index of the focal source.\n        r (int): The radius for the leakage calculation.\n\n    Returns:\n        tuple: (peak_correctness, fwhm, leakage_fraction)\n    \"\"\"\n    p_dim = G.shape[1]\n\n    # Step 1: Compute the Wiener filter W\n    # W = Rx @ G^T @ inv(G @ Rx @ G^T + Rn)\n    M = G @ Rx @ G.T + Rn\n    try:\n        M_inv = np.linalg.inv(M)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if inversion fails, though not expected for this problem's setup\n        M_inv = np.linalg.pinv(M)\n        \n    W = Rx @ G.T @ M_inv\n\n    # Step 2: Compute the resolution matrix R\n    R = W @ G\n\n    # Step 3: Analyze the point-spread function (PSF) for source j0\n    psf_column = R[:, j0]\n    p_vec = np.abs(psf_column)\n\n    # Step 4: Compute the metrics\n    \n    # Metric 1: Peak Correctness\n    peak_idx = np.argmax(p_vec)\n    peak_correctness = 1 if peak_idx == j0 else 0\n\n    # Metric 2: Full-Width at Half-Maximum (FWHM)\n    p_j0 = p_vec[j0]\n    fwhm = 0\n    if p_j0 > 1e-12: # Use a small tolerance for floating point zero\n        threshold = 0.5 * p_j0\n        \n        # Check contiguous indices including j0\n        contiguous_indices = [j0]\n        # Search right from j0\n        for i in range(j0 + 1, p_dim):\n            if p_vec[i] >= threshold:\n                contiguous_indices.append(i)\n            else:\n                break\n        \n        # Search left from j0\n        for i in range(j0 - 1, -1, -1):\n            if p_vec[i] >= threshold:\n                contiguous_indices.insert(0, i)\n            else:\n                break\n        \n        # Verify contiguity of the final set around j0\n        if len(contiguous_indices) > 0:\n            if contiguous_indices == list(range(min(contiguous_indices), max(contiguous_indices)+1)):\n                 fwhm = len(contiguous_indices)\n        if fwhm == 0 and p_j0 >= threshold: # if only j0 is above threshold\n            fwhm = 1\n\n\n    # Metric 3: Leakage Fraction\n    p_sum = np.sum(p_vec)\n    if p_sum > 1e-12:\n        p_tilde = p_vec / p_sum\n        \n        start_idx = max(0, j0 - r)\n        end_idx = min(p_dim - 1, j0 + r)\n        \n        neighborhood_sum = np.sum(p_tilde[start_idx : end_idx + 1])\n        leakage_fraction = 1.0 - neighborhood_sum\n    else:\n        leakage_fraction = 1.0 if r  p_dim - 1 else 0.0 # If all p_i are 0, leakage is total unless radius covers everything.\n\n    return peak_correctness, fwhm, leakage_fraction\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, compute results, and print the output.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"G\": np.array([\n                [0.8, 0.1, 0.0, -0.2, 0.3, 0.5],\n                [0.0, 0.5, 0.4, 0.1, -0.1, 0.2],\n                [0.2, -0.3, 0.6, 0.0, 0.4, -0.2],\n                [-0.1, 0.2, -0.2, 0.7, 0.1, 0.0]\n            ]),\n            \"Rn\": 0.05 * np.identity(4),\n            \"Rx\": np.identity(6),\n            \"j0\": 3,\n            \"r\": 1\n        },\n        # Test case 2\n        {\n            \"G\": np.array([\n                [0.6, 0.9, 0.91, 0.1, 0.0],\n                [0.0, 0.2, 0.21, 0.0, 0.5],\n                [0.1, 0.3, 0.29, 0.0, -0.1]\n            ]),\n            \"Rn\": 0.01 * np.identity(3),\n            \"Rx\": np.identity(5),\n            \"j0\": 0,\n            \"r\": 0\n        },\n        # Test case 3\n        {\n            \"G\": np.array([\n                [0.4, -0.2, 0.0, 0.1, 0.5, 0.3, 0.0],\n                [0.0, 0.1, 0.2, -0.4, 0.2, 0.0, 0.1],\n                [0.1, 0.0, -0.3, 0.2, 0.0, 0.4, -0.2],\n                [0.2, 0.3, 0.1, 0.0, -0.1, 0.2, 0.5]\n            ]),\n            \"Rn\": np.diag([0.02, 0.05, 0.03, 0.08]),\n            \"Rx\": np.diag([1.0, 0.5, 0.2, 0.2, 0.5, 1.0, 1.5]),\n            \"j0\": 6,\n            \"r\": 2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # A simpler FWHM calculation that matches the definition.\n        # The provided code has a slightly more complex logic for FWHM.\n        # This implementation simplifies it based on the problem description.\n        p_corr, _, leak = compute_metrics(case[\"G\"], case[\"Rn\"], case[\"Rx\"], case[\"j0\"], case[\"r\"])\n\n        # Recompute FWHM using a direct interpretation\n        p_dim = case[\"G\"].shape[1]\n        M = case[\"G\"] @ case[\"Rx\"] @ case[\"G\"].T + case[\"Rn\"]\n        M_inv = np.linalg.inv(M)\n        W = case[\"Rx\"] @ case[\"G\"].T @ M_inv\n        R = W @ case[\"G\"]\n        p_vec = np.abs(R[:, case[\"j0\"]])\n        p_j0_val = p_vec[case[\"j0\"]]\n        \n        fwhm_val = 0\n        if p_j0_val > 1e-12:\n            threshold = 0.5 * p_j0_val\n            fwhm_val = 1\n            # Go right\n            for i in range(case[\"j0\"] + 1, p_dim):\n                if p_vec[i] >= threshold:\n                    fwhm_val += 1\n                else:\n                    break\n            # Go left\n            for i in range(case[\"j0\"] - 1, -1, -1):\n                if p_vec[i] >= threshold:\n                    fwhm_val += 1\n                else:\n                    break\n\n        results.append((p_corr, fwhm_val, leak))\n    \n    # Format the final output string exactly as specified.\n    formatted_results = [f'[{r[0]},{r[1]},{r[2]:.6f}]' for r in results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Many modern inverse methods are built on the assumption that neural activity is sparse. The success of these methods depends critically on the geometric properties of the lead-field matrix $G$, which can make the signals from distinct sources either distinguishable or confusingly similar at the sensor level. This advanced exercise bridges the physics of bioelectromagnetism with the theory of sparse signal recovery by introducing the concept of mutual coherence. You will compute how this property of the lead-field matrix changes with source spacing and see how it imposes a fundamental theoretical limit on the ability of certain algorithms to resolve sparse brain activity. ",
            "id": "4157990",
            "problem": "Consider the Electroencephalography (EEG) and Magnetoencephalography (MEG) source localization inverse problem in a simplified, yet scientifically plausible, forward modeling scenario. In the quasi-static regime of Maxwell's equations relevant to bioelectric fields, the electric potential generated by a neural current dipole in a homogeneous, isotropic volume conductor is well described by a classical solution derived from the Poisson equation for the electric potential. You will use this model to build a lead-field matrix and analyze how spatial spacing of sources affects the mutual coherence of the lead field, and consequently the sparsity level that can be recovered by convex sparse estimation methods.\n\nYou are given a homogeneous conductor with conductivity $\\sigma$ equal to $0.33$ $\\mathrm{S/m}$. Sensors are placed uniformly on a circle of radius $R_{\\mathrm{e}}=0.09$ $\\mathrm{m}$ in the $x$-$y$ plane, modeling an idealized ring of scalp EEG sensors. The number of sensors is $M=64$. Putative neural sources are modeled as ideal current dipoles located on a concentric inner circle of radius $R_{\\mathrm{s}}=0.08$ $\\mathrm{m}$ in the same plane, each oriented radially outward. The dipole moment magnitude for each source is $p_0=1\\times 10^{-8}$ $\\mathrm{A\\cdot m}$.\n\nThe forward model shall map unit source amplitudes (with fixed dipole moment magnitude $p_0$ and radial orientation) to sensor potentials using the standard quasi-static potential of a point current dipole in an infinite homogeneous conductor. From this, construct the lead-field matrix $G\\in\\mathbb{R}^{M\\times N}$ whose $j$-th column contains the potentials at all $M$ sensors produced by the $j$-th source, for a set of $N$ sources spaced along a half-circle arc (angle $\\pi$ radians) on the source ring.\n\nDefine the mutual coherence $\\mu(G)$ of the lead-field matrix $G$ as the maximum absolute inner product between distinct columns of $G$ after normalizing each column to unit $\\ell_2$ norm. Formally, if $\\hat{\\mathbf{g}}_j=\\mathbf{g}_j/\\|\\mathbf{g}_j\\|_2$ denotes the $j$-th normalized column of $G$, then\n$$\n\\mu(G)=\\max_{i\\neq j}\\left|\\hat{\\mathbf{g}}_i^\\top \\hat{\\mathbf{g}}_j\\right|.\n$$\nUsing a coherence-based recovery guarantee from sparse approximation theory, predict the largest integer sparsity level $s_{\\max}$ for which exact recovery is theoretically guaranteed under $\\ell_1$ minimization based on $\\mu(G)$.\n\nImplement a program that, for the following test suite of source spacings $d$ (interpreted as target arc-length spacing along the $\\pi$-radian half-circle), constructs the corresponding lead-field matrix $G$, computes $\\mu(G)$, and predicts $s_{\\max}$:\n- Test case $1$: $d=0.08$ $\\mathrm{m}$.\n- Test case $2$: $d=0.04$ $\\mathrm{m}$.\n- Test case $3$: $d=0.02$ $\\mathrm{m}$.\n- Test case $4$: $d=0.01$ $\\mathrm{m}$.\n- Edge case $5$: $d=0.002$ $\\mathrm{m}$.\n\nFor each test case, use an arc of angle $\\pi$ radians centered on the $x$-axis, and place sources uniformly along the arc such that the number of sources $N$ is the largest integer for which the nominal spacing $d$ fits along the arc, with endpoints included. Specifically, let the arc length be $L=\\pi R_{\\mathrm{s}}$, and set $N=\\left\\lfloor L/d\\right\\rfloor+1$, then place sources at equally spaced angles over the arc so that the actual spacing is $L/(N-1)$. All angles must be in radians, all distances in meters, and potentials in volts. You may treat $p_0$ and $\\sigma$ as constants, and you must use the infinite homogeneous conductor model. The program must:\n- Construct sensor positions $\\mathbf{r}_i$ for $i=1,\\dots,M$ uniformly on the ring of radius $R_{\\mathrm{e}}$.\n- Construct source positions $\\mathbf{r}_0^{(j)}$ for $j=1,\\dots,N$ uniformly on the half-circle of radius $R_{\\mathrm{s}}$, with dipole vectors $\\mathbf{p}^{(j)}$ oriented radially outward with magnitude $p_0$.\n- Compute the lead-field matrix $G$ by evaluating the potential at each sensor due to each source using the quasi-static dipole potential in an infinite homogeneous conductor.\n- Compute $\\mu(G)$ as defined above and the corresponding predicted integer sparsity limit $s_{\\max}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list of the form $[\\mu(G), s_{\\max}]$. For example, the output should look like $[[\\mu_1,s_1],[\\mu_2,s_2],\\dots]$. The values of $\\mu(G)$ should be floating-point numbers and $s_{\\max}$ integers. No other text should be printed.",
            "solution": "### 1. Fundamental Principles\n\n#### 1.1. The Forward Model: Electric Potential of a Current Dipole\nIn the quasi-static regime relevant to bioelectric fields, the electric potential $\\Phi$ generated by a current source distribution in a conductive medium is governed by the Poisson equation. For a homogeneous and isotropic medium with conductivity $\\sigma$, the potential at a position $\\mathbf{r}$ due to an ideal current dipole with moment $\\mathbf{p}$ located at $\\mathbf{r}_0$ is given by:\n$$\n\\Phi(\\mathbf{r}) = \\frac{1}{4\\pi\\sigma} \\frac{\\mathbf{p} \\cdot (\\mathbf{r} - \\mathbf{r}_0)}{\\|\\mathbf{r} - \\mathbf{r}_0\\|_2^3}\n$$\nThis formula is the solution to $\\nabla \\cdot (\\sigma \\nabla \\Phi) = I_v$ where the volume current source $I_v$ for a dipole is $I_v = -\\nabla \\cdot (\\mathbf{p}\\delta(\\mathbf{x}-\\mathbf{r}_0))$, and $\\delta(\\cdot)$ is the Dirac delta function.\n\n#### 1.2. The Lead-Field Matrix\nThe lead-field matrix $G \\in \\mathbb{R}^{M \\times N}$ represents the linear forward model, mapping the amplitudes of $N$ sources to the measured potentials at $M$ sensors. The element $G_{ij}$ is the potential at the $i$-th sensor position $\\mathbf{r}_i$ due to the $j$-th source dipole with position $\\mathbf{r}_0^{(j)}$ and moment $\\mathbf{p}^{(j)}$. Based on the formula above, with sensor $i$ and source $j$:\n$$\nG_{ij} = \\frac{1}{4\\pi\\sigma} \\frac{\\mathbf{p}^{(j)} \\cdot (\\mathbf{r}_i - \\mathbf{r}_0^{(j)})}{\\|\\mathbf{r}_i - \\mathbf{r}_0^{(j)}\\|_2^3}\n$$\nThe problem specifies that the dipole moment magnitude is fixed at $p_0=1\\times 10^{-8}\\,\\mathrm{A\\cdot m}$ and the orientation is radial. This means for a source at $\\mathbf{r}_0^{(j)}$, the moment is $\\mathbf{p}^{(j)} = p_0 \\frac{\\mathbf{r}_0^{(j)}}{\\|\\mathbf{r}_0^{(j)}\\|_2}$.\n\n#### 1.3. Mutual Coherence and Sparsity Recovery\nMutual coherence, $\\mu(G)$, is a key property of a sensing matrix in sparse approximation theory. It is defined as the largest absolute inner product between any two distinct, normalized columns of the matrix:\n$$\n\\mu(G) = \\max_{j \\neq k} |\\langle \\hat{\\mathbf{g}}_j, \\hat{\\mathbf{g}}_k \\rangle| = \\max_{j \\neq k} \\left| \\frac{\\mathbf{g}_j^\\top \\mathbf{g}_k}{\\|\\mathbf{g}_j\\|_2 \\|\\mathbf{g}_k\\|_2} \\right|\n$$\nwhere $\\mathbf{g}_j$ is the $j$-th column of $G$ and $\\hat{\\mathbf{g}}_j$ is its $\\ell_2$-normalized version.\n\nA fundamental result in compressed sensing guarantees that if a signal $\\mathbf{s}$ is $s$-sparse (i.e., has at most $s$ non-zero elements), it can be uniquely recovered from measurements $\\mathbf{y} = G\\mathbf{s}$ via $\\ell_1$-norm minimization (Basis Pursuit), provided the sparsity level $s$ satisfies:\n$$\ns  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(G)}\\right)\n$$\nThe problem asks for the largest integer sparsity level, $s_{\\max}$, for which this guarantee holds. This is the largest integer $s$ satisfying the strict inequality. Let $X = \\frac{1}{2}(1 + 1/\\mu(G))$. The value $s_{\\max}$ is the largest integer strictly less than $X$, which can be computed as $s_{\\max} = \\lceil X \\rceil - 1$.\n\n### 2. Algorithmic Implementation\n\nFor each given source spacing $d$, we perform the following steps:\n\n**Step 1: Define Geometry and Constants**\nThe constants are given: sensor radius $R_{\\mathrm{e}}=0.09\\,\\mathrm{m}$, source radius $R_{\\mathrm{s}}=0.08\\,\\mathrm{m}$, number of sensors $M=64$, conductivity $\\sigma=0.33\\,\\mathrm{S/m}$, and dipole moment magnitude $p_0=1\\times 10^{-8}\\,\\mathrm{A\\cdot m}$. The sensors are placed on a circle in the $x$-$y$ plane. The sensor positions $\\mathbf{r}_i$ for $i \\in \\{1, \\dots, M\\}$ are:\n$$\n\\mathbf{r}_i = (R_{\\mathrm{e}}\\cos(\\theta_i), R_{\\mathrm{e}}\\sin(\\theta_i), 0) \\quad \\text{with} \\quad \\theta_i = \\frac{2\\pi(i-1)}{M}\n$$\n\n**Step 2: Determine Source Locations**\nThe sources lie on a half-circle arc of radius $R_{\\mathrm{s}}$ centered on the $x$-axis (from angle $-\\pi/2$ to $\\pi/2$). The arc length is $L=\\pi R_{\\mathrm{s}}$. For a given nominal spacing $d$, the number of sources is $N = \\lfloor L/d \\rfloor + 1$. The sources are then placed uniformly along this arc. The angle for the $j$-th source, for $j \\in \\{1, \\dots, N\\}$, is:\n$$\n\\phi_j = -\\frac{\\pi}{2} + \\frac{\\pi(j-1)}{N-1} \\quad (\\text{for } N  1)\n$$\nThe position of the $j$-th source is $\\mathbf{r}_0^{(j)} = (R_{\\mathrm{s}}\\cos(\\phi_j), R_{\\mathrm{s}}\\sin(\\phi_j), 0)$. The dipole moment, being radially oriented, is $\\mathbf{p}^{(j)} = p_0 (\\cos(\\phi_j), \\sin(\\phi_j), 0)$.\n\n**Step 3: Construct the Lead-Field Matrix $G$**\nWe compute each element $G_{ij}$. The dot product and norm can be simplified:\n$$\n\\mathbf{p}^{(j)} \\cdot (\\mathbf{r}_i - \\mathbf{r}_0^{(j)}) = p_0(R_{\\mathrm{e}}\\cos(\\theta_i - \\phi_j) - R_{\\mathrm{s}})\n$$\n$$\n\\|\\mathbf{r}_i - \\mathbf{r}_0^{(j)}\\|_2^2 = R_{\\mathrm{e}}^2 + R_{\\mathrm{s}}^2 - 2R_{\\mathrm{e}}R_{\\mathrm{s}}\\cos(\\theta_i - \\phi_j)\n$$\nTherefore,\n$$\nG_{ij} = \\frac{p_0}{4\\pi\\sigma} \\frac{R_{\\mathrm{e}}\\cos(\\theta_i - \\phi_j) - R_{\\mathrm{s}}}{(R_{\\mathrm{e}}^2 + R_{\\mathrm{s}}^2 - 2R_{\\mathrm{e}}R_{\\mathrm{s}}\\cos(\\theta_i - \\phi_j))^{3/2}}\n$$\nFor calculating the mutual coherence $\\mu(G)$, the constant factor $C = p_0/(4\\pi\\sigma)$ will cancel out during column normalization. Thus, we can compute a matrix $G'$ without this factor.\n\n**Step 4: Compute Mutual Coherence $\\mu(G)$}\nUsing the matrix $G'$ computed in the previous step:\n1. Normalize each column $\\mathbf{g}'_j$ to have unit $\\ell_2$ norm: $\\hat{\\mathbf{g}}'_j = \\mathbf{g}'_j / \\|\\mathbf{g}'_j\\|_2$. Let the resulting matrix be $\\hat{G}'$.\n2. Compute the Gram matrix of inner products: $K = (\\hat{G}')^\\top \\hat{G}'$.\n3. The mutual coherence is the maximum absolute value of the off-diagonal elements of $K$:\n   $\\mu(G) = \\max_{j \\neq k} |K_{jk}|$.\n\n**Step 5: Predict Maximum Sparsity $s_{\\max}$**\nUsing the computed value of $\\mu(G)$:\n1. Calculate the recovery threshold $X = \\frac{1}{2}(1 + 1/\\mu(G))$.\n2. Find the largest integer $s_{\\max}$ strictly less than $X$: $s_{\\max} = \\lceil X \\rceil - 1$.\n\nThis procedure is repeated for each test case value of $d$. As $d$ decreases, $N$ increases, sources become more closely packed, leading to an expected increase in $\\mu(G)$ and a decrease in the recoverable sparsity $s_{\\max}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the EEG/MEG source localization problem for multiple test cases.\n    For each source spacing `d`, it constructs the lead-field matrix G,\n    computes its mutual coherence mu(G), and predicts the maximum guaranteed\n    recoverable sparsity s_max.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases_d = [0.08, 0.04, 0.02, 0.01, 0.002]\n\n    # --- Model Parameters ---\n    # Number of sensors\n    M = 64\n    # Radius of sensor ring (m)\n    R_e = 0.09\n    # Radius of source ring (m)\n    R_s = 0.08\n    # Conductivity and dipole moment are not needed as they form a constant\n    # factor that cancels out in the coherence calculation.\n    # sigma = 0.33\n    # p0 = 1e-8\n\n    # --- Sensor Positions ---\n    # Angles for M sensors uniformly spaced on a full circle\n    theta = np.linspace(0, 2 * np.pi, M, endpoint=False)\n\n    results = []\n\n    for d in test_cases_d:\n        # --- Source Configuration for current test case ---\n        # Total arc length of the half-circle for sources\n        arc_length = np.pi * R_s\n        \n        # Number of sources N based on spacing d\n        # Using np.floor and adding 1 as per problem spec\n        N = int(np.floor(arc_length / d)) + 1\n\n        if N = 1:\n            # Mutual coherence is not defined for N = 1\n            # This case is not expected for the given test values.\n            # We can append a placeholder and continue, e.g. [nan, 0]\n            # however, problem ensures N >= 2 for given d.\n            continue\n        \n        # Angles for N sources uniformly spaced on a half-circle arc (-pi/2 to pi/2)\n        phi = np.linspace(-np.pi / 2, np.pi / 2, N)\n\n        # --- Lead-Field Matrix (G) Construction ---\n        # We can compute the matrix efficiently using numpy broadcasting.\n        # The constant factor p0/(4*pi*sigma) is ignored as it cancels out.\n        \n        # cos(theta_i - phi_j) for all i, j pairs\n        cos_diff = np.cos(theta[:, np.newaxis] - phi[np.newaxis, :])\n        \n        # Numerator of the potential formula\n        numerator = R_e * cos_diff - R_s\n        \n        # Denominator of the potential formula\n        denominator = (R_e**2 + R_s**2 - 2 * R_e * R_s * cos_diff)**1.5\n        \n        # The lead-field matrix G (without the physical constant factor)\n        G = numerator / denominator\n\n        # --- Mutual Coherence (mu) Calculation ---\n        # Normalize each column of G to have unit l2-norm\n        G_norms = np.linalg.norm(G, axis=0)\n        G_normalized = G / G_norms[np.newaxis, :]\n        \n        # Compute the Gram matrix (inner products of normalized columns)\n        gram_matrix = G_normalized.T @ G_normalized\n        \n        # Set diagonal to 0 to ignore self-correlations (i=j)\n        np.fill_diagonal(gram_matrix, 0)\n        \n        # Mutual coherence is the max absolute off-diagonal element\n        mu = np.max(np.abs(gram_matrix))\n\n        # --- Sparsity Limit (s_max) Calculation ---\n        # The recovery guarantee holds for sparsity s  0.5 * (1 + 1/mu)\n        # s_max is the largest integer satisfying this strict inequality.\n        if mu > 0:\n            recovery_threshold = 0.5 * (1.0 + 1.0 / mu)\n            # s_max = ceil(X) - 1\n            s_max = int(math.ceil(recovery_threshold) - 1)\n        else:\n            # If mu is 0 (perfectly orthogonal), sparsity limit is theoretically infinite.\n            # However, N is the maximum possible sparsity.\n            # This case is unlikely here.\n            s_max = N \n\n        results.append([mu, s_max])\n    \n    # --- Final Output Formatting ---\n    # The output must be a single line, e.g., [[mu1,s1],[mu2,s2],...]\n    output_str = \"[\" + \",\".join([f\"[{mu_val},{s_max_val}]\" for mu_val, s_max_val in results]) + \"]\"\n\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}