{
    "hands_on_practices": [
        {
            "introduction": "A robust physical model should behave predictably when the units of measurement are changed. This exercise explores the fundamental scaling properties of the Minimum Norm Estimate (MNE) solution, forcing us to consider how the regularization parameter must adapt to ensure the solution remains mathematically consistent under such transformations. By working through this problem, you will gain a deeper appreciation for the mathematical structure of Tikhonov-regularized inverse problems and their physical interpretation. ",
            "id": "4157111",
            "problem": "Consider a standard linear forward model for magnetoencephalography/electroencephalography source imaging, $y = L x + \\varepsilon$, where $y \\in \\mathbb{R}^{m}$ are sensor measurements, $L \\in \\mathbb{R}^{m \\times n}$ is the lead field matrix, $x \\in \\mathbb{R}^{n}$ are distributed source amplitudes, and $\\varepsilon \\in \\mathbb{R}^{m}$ is additive sensor noise. Assume a Gaussian likelihood with independent and identically distributed noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$ and a Gaussian isotropic prior $x \\sim \\mathcal{N}(0, \\tau^{2} I_{n})$. The Maximum A Posteriori (MAP) estimator underlying the classical Minimum Norm Estimate (MNE) is defined as the minimizer of the posterior negative log-density,\n$$\n\\hat{x}_{\\mathrm{MNE}} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{\\sigma^{2}} \\| y - L x \\|_{2}^{2} + \\frac{1}{\\tau^{2}} \\| x \\|_{2}^{2} \\right\\}.\n$$\nNow apply the following scaling transformation: replace $L$ by $L' = a L$ and $y$ by $y' = b y$, where $a>0$ and $b>0$ are real constants. For physical unit-consistency, treat the noise and prior in their native units: under the transformation, the noise standard deviation rescales as $\\sigma' = b \\sigma$ (since $y$ is in sensor units), while the source prior scale remains $\\tau' = \\tau$ (since $x$ is in source units). Define $\\lambda = \\sigma^{2}/\\tau^{2}$ and $\\lambda' = (\\sigma')^{2}/(\\tau')^{2}$.\n\nImpose the additional modeling requirement that after the scaling transformation there exists a scalar $s(a,b)$, independent of $L$ and $y$, such that the transformed MNE satisfies $ \\hat{x}'_{\\mathrm{MNE}} = s(a,b)\\, \\hat{x}_{\\mathrm{MNE}}$ for all admissible $L$ and $y$. Determine the unique choice of $\\lambda'$ in terms of $a$, $b$, and $\\lambda$ that enforces this colinearity for all $L$ and $y$, and then compute the corresponding scalar $s(a,b)$.\n\nYour final answer must be the closed-form analytic expression for $s(a,b)$. Do not include any units in your final answer. If you obtain a numerical expression, provide it in exact form; no rounding is required.",
            "solution": "The user wants to find the scaling factor $s(a,b)$ for the Minimum Norm Estimate (MNE) solution under a specific linear transformation of the data and the lead field matrix, given the constraint that the transformed solution must be collinear with the original solution.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- Forward model: $y = L x + \\varepsilon$, where $y \\in \\mathbb{R}^{m}$, $L \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, $\\varepsilon \\in \\mathbb{R}^{m}$.\n- Noise distribution: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$.\n- Prior distribution: $x \\sim \\mathcal{N}(0, \\tau^{2} I_{n})$.\n- MNE estimator objective function: $\\hat{x}_{\\mathrm{MNE}} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{\\sigma^{2}} \\| y - L x \\|_{2}^{2} + \\frac{1}{\\tau^{2}} \\| x \\|_{2}^{2} \\right\\}$.\n- Scaling transformations: $L' = a L$ and $y' = b y$, with $a>0$, $b>0$.\n- Transformed parameters based on physical units: $\\sigma' = b \\sigma$, $\\tau' = \\tau$.\n- Regularization parameter definitions: $\\lambda = \\sigma^{2}/\\tau^{2}$ and $\\lambda' = (\\sigma')^{2}/(\\tau')^{2}$.\n- Colinearity requirement: There exists a scalar $s(a,b)$, independent of $L$ and $y$, such that $\\hat{x}'_{\\mathrm{MNE}} = s(a,b)\\, \\hat{x}_{\\mathrm{MNE}}$ for all admissible $L$ and $y$.\n- Goal: Determine the choice of $\\lambda'$ that enforces this colinearity and find the corresponding scalar $s(a,b)$.\n\n**1.2. Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is set within the standard mathematical framework of regularized linear inverse problems, specifically MNE source localization in neuroscience, which is a well-established and scientifically sound domain.\n- **Well-Posed:** The problem provides all necessary mathematical definitions and constraints to determine a unique solution for the scaling factor $s(a,b)$.\n- **Objective:** The problem is stated in precise, objective mathematical language.\n- **Flaw Checklist:**\n    1.  No scientific or factual unsoundness. The model is a standard representation in the field.\n    2.  The problem is formalizable and directly relevant to the topic of distributed source models.\n    3.  The problem is self-contained and not contradictory. It poses a question about the consequences of a modeling requirement.\n    4.  The conditions are mathematically abstract and not physically impossible.\n    5.  The structure is clear, and the constraint that the relation must hold \"for all admissible $L$ and $y$\" is strong enough to ensure a unique solution.\n    6.  The problem is not trivial; it requires a rigorous application of linear algebra to an inverse problem.\n    7.  The derivation is mathematically verifiable.\n\n**1.3. Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Step 2: Derivation of the MNE Estimator\n\nThe MNE estimator minimizes the objective function $J(x)$:\n$$\nJ(x) = \\frac{1}{\\sigma^{2}} \\| y - L x \\|_{2}^{2} + \\frac{1}{\\tau^{2}} \\| x \\|_{2}^{2}\n$$\nThis is a convex quadratic function of $x$. The minimum is found by setting the gradient with respect to $x$ to zero.\n$$\n\\nabla_{x} J(x) = \\frac{1}{\\sigma^{2}} \\nabla_{x} \\left( (y - L x)^{T} (y - L x) \\right) + \\frac{1}{\\tau^{2}} \\nabla_{x} (x^{T} x)\n$$\nUsing standard matrix calculus identities, $\\nabla_{x} (c^{T} A x) = A^{T} c$ and $\\nabla_{x} (x^{T} B x) = (B+B^T)x$, we get:\n$$\n\\nabla_{x} J(x) = \\frac{1}{\\sigma^{2}} (-2 L^{T} y + 2 L^{T} L x) + \\frac{1}{\\tau^{2}} (2x)\n$$\nSetting the gradient to zero for the optimal $\\hat{x}_{\\mathrm{MNE}}$:\n$$\n\\frac{1}{\\sigma^{2}} (L^{T} L \\hat{x}_{\\mathrm{MNE}} - L^{T} y) + \\frac{1}{\\tau^{2}} \\hat{x}_{\\mathrm{MNE}} = 0\n$$\nMultiplying by $\\sigma^{2}$ and rearranging terms:\n$$\n(L^{T} L) \\hat{x}_{\\mathrm{MNE}} - L^{T} y + \\frac{\\sigma^{2}}{\\tau^{2}} \\hat{x}_{\\mathrm{MNE}} = 0\n$$\nIntroducing the regularization parameter $\\lambda = \\sigma^{2}/\\tau^{2}$:\n$$\n(L^{T} L + \\lambda I_{n}) \\hat{x}_{\\mathrm{MNE}} = L^{T} y\n$$\nAssuming the matrix $(L^{T} L + \\lambda I_{n})$ is invertible (which is guaranteed for $\\lambda > 0$), the unique MNE solution is:\n$$\n\\hat{x}_{\\mathrm{MNE}} = (L^{T} L + \\lambda I_{n})^{-1} L^{T} y\n$$\n\n### Step 3: Derivation of the Transformed MNE Estimator\n\nThe transformed problem seeks to minimize:\n$$\nJ'(x') = \\frac{1}{(\\sigma')^{2}} \\| y' - L' x' \\|_{2}^{2} + \\frac{1}{(\\tau')^{2}} \\| x' \\|_{2}^{2}\n$$\nBy analogy with the original problem, the solution $\\hat{x}'_{\\mathrm{MNE}}$ is:\n$$\n\\hat{x}'_{\\mathrm{MNE}} = ((L')^{T} L' + \\lambda' I_{n})^{-1} (L')^{T} y'\n$$\nwhere $\\lambda' = (\\sigma')^{2}/(\\tau')^{2}$. The problem asks us to find the specific value of this $\\lambda'$ that satisfies the colinearity constraint.\n\nWe substitute the given transformations $L' = aL$ and $y' = by$:\n- $(L')^{T} L' = (aL)^{T}(aL) = a^{2} L^{T} L$\n- $(L')^{T} y' = (aL)^{T}(by) = ab L^{T} y$\n\nSubstituting these into the expression for $\\hat{x}'_{\\mathrm{MNE}}$:\n$$\n\\hat{x}'_{\\mathrm{MNE}} = (a^{2} L^{T} L + \\lambda' I_{n})^{-1} (ab L^{T} y)\n$$\n\n### Step 4: Imposing the Colinearity Constraint\n\nThe problem requires that there exists a scalar $s(a,b)$ such that $\\hat{x}'_{\\mathrm{MNE}} = s(a,b) \\hat{x}_{\\mathrm{MNE}}$ for all admissible $L$ and $y$. We equate the two expressions for the estimators:\n$$\n(a^{2} L^{T} L + \\lambda' I_{n})^{-1} (ab L^{T} y) = s(a,b) (L^{T} L + \\lambda I_{n})^{-1} L^{T} y\n$$\nThis equality must hold for any choice of $L$ and $y$. This implies it must hold for any vector of the form $v = L^T y$. Let $G = L^T L$. The matrix $G$ is symmetric and positive semi-definite. The equation becomes:\n$$\n(a^{2} G + \\lambda' I_{n})^{-1} ab\\, v = s(a,b) (G + \\lambda I_{n})^{-1} v\n$$\nfor any valid $G$ and any vector $v$ in the range of $G$'s corresponding $L^T$. To ensure this holds universally, the operators applied to $v$ must be identical. A rigorous way to enforce this is to analyze the equation in the eigenspace of $G$.\n\nLet $d_i \\geq 0$ be an eigenvalue of $G$. The corresponding filter functions (the scalar multipliers for each eigencomponent) must be equal. For any $d_i$, we must have:\n$$\n\\frac{ab}{a^{2} d_i + \\lambda'} = \\frac{s(a,b)}{d_i + \\lambda}\n$$\nThis must hold for any possible non-negative value of $d_i$, which can be varied by choosing different matrices $L$. For this equality between two rational functions of $d_i$ to hold for all $d_i \\ge 0$, the functions must be identical. We can cross-multiply to get a polynomial identity in $d_i$:\n$$\nab (d_i + \\lambda) = s(a,b) (a^{2} d_i + \\lambda')\n$$\n$$\n(ab) d_i + (ab\\lambda) = (s(a,b) a^{2}) d_i + (s(a,b) \\lambda')\n$$\nFor this polynomial identity in $d_i$ to be true, the coefficients of like powers of $d_i$ must be equal.\n\n1.  **Equating coefficients of $d_i$:**\n    $$\n    ab = s(a,b) a^{2}\n    $$\n    Since $a>0$, we can solve for $s(a,b)$:\n    $$\n    s(a,b) = \\frac{ab}{a^{2}} = \\frac{b}{a}\n    $$\n    This determines the scaling factor.\n\n2.  **Equating constant terms (coefficients of $d_i^0$):**\n    $$\n    ab\\lambda = s(a,b) \\lambda'\n    $$\n    We substitute the expression for $s(a,b)$ that we just found:\n    $$\n    ab\\lambda = \\left(\\frac{b}{a}\\right) \\lambda'\n    $$\n    Since $b>0$, we can solve for $\\lambda'$:\n    $$\n    \\lambda' = \\frac{a(ab\\lambda)}{b} = a^{2} \\lambda\n    $$\nThis establishes that the unique choice for the transformed regularization parameter that ensures the colinearity property is $\\lambda' = a^{2} \\lambda$. The problem asks for the corresponding scalar $s(a,b)$.\n\nBased on our derivation, the scalar is $s(a,b) = b/a$. This result is independent of $L$ and $y$, as required. Note that the \"physically-motivated\" scaling given in the problem, $\\sigma' = b\\sigma, \\tau' = \\tau$, would imply $\\lambda' = b^2\\lambda$. This would only satisfy the colinearity constraint if $a^2\\lambda = b^2\\lambda$, which for $\\lambda \\ne 0$ means $a=b$. The problem asks for a general solution for all $a,b>0$, which is only possible if we adopt the mathematically derived scaling rule for the regularization parameter, $\\lambda' = a^2 \\lambda$.\n\nThe final answer is the expression for $s(a,b)$.",
            "answer": "$$\\boxed{\\frac{b}{a}}$$"
        },
        {
            "introduction": "The choice of the regularization parameter, $\\lambda$, is central to all minimum norm methods, as it governs a fundamental compromise in the inverse solution. This practice provides a concrete, hands-on derivation of this trade-off using a simplified two-source model. By analyzing how $\\lambda$ affects the balance between spatial sharpness and amplitude accuracy, you will develop a tangible intuition for how regularization shapes the characteristics of the final source estimate. ",
            "id": "4157012",
            "problem": "Consider the linear forward model of magnetoencephalography/electroencephalography (MEG/EEG) distributed source imaging, where the sensor measurements $y \\in \\mathbb{R}^{m}$ are related to the cortical sources $x \\in \\mathbb{R}^{n}$ by $y = L x + n$, with $L \\in \\mathbb{R}^{m \\times n}$ the lead-field matrix and $n$ zero-mean Gaussian sensor noise that has been whitened so that its covariance is the identity. The Minimum Norm Estimate (MNE) solves $\\hat{x} = \\arg\\min_{x} \\|y - L x\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$ and yields the linear estimator $\\hat{x} = G(\\lambda) y$ with $G(\\lambda) = (L^{\\top} L + \\lambda I)^{-1} L^{\\top}$. The point-spread function (PSF) is characterized by the resolution matrix $R(\\lambda) = G(\\lambda) L = (L^{\\top} L + \\lambda I)^{-1} L^{\\top} L$. Dynamic Statistical Parametric Mapping (dSPM) and standardized Low Resolution Electromagnetic Tomography (sLORETA) are diagonal normalizations of the MNE that rescale the amplitudes of $\\hat{x}$, but the spatial spread embodied in $R(\\lambda)$ originates from the same Tikhonov regularization trade-off.\n\nTo analyze how the regularization parameter $\\lambda$ trades off PSF width versus amplitude bias, consider an idealized two-source system ($n=2$) whose lead-field columns are unit-norm and have mutual correlation $\\rho \\in [0,1)$, so that the Gram matrix is $L^{\\top} L = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$. Assume a unit-amplitude focal source at index $1$, so the PSF is the first column of $R(\\lambda)$. Define the amplitude bias at the true location by $R_{11}(\\lambda)$ and a two-point width proxy by the leakage-to-peak ratio $w(\\lambda) = R_{21}(\\lambda) / R_{11}(\\lambda)$.\n\nStarting from these definitions and the stated model, derive a closed-form analytic expression for $w(\\lambda)$ in terms of $\\lambda$ and $\\rho$, and use it to obtain the limiting behaviors as $\\lambda \\to 0$ and as $\\lambda \\to \\infty$. Express your final answer as the single closed-form formula $w(\\lambda)$ in terms of $\\lambda$ and $\\rho$. No rounding is required.",
            "solution": "The problem is well-defined, scientifically grounded, and possesses a unique, derivable solution. All necessary information is provided.\n\nThe starting point is the definition of the resolution matrix, $R(\\lambda)$, which characterizes the point-spread function (PSF) of the Minimum Norm Estimate (MNE) inverse operator. It is given by $R(\\lambda) = (L^{\\top} L + \\lambda I)^{-1} L^{\\top} L$, where $L$ is the lead-field matrix, $\\lambda > 0$ is the Tikhonov regularization parameter, and $I$ is the identity matrix. The problem considers an idealized system with $n=2$ sources, where the Gram matrix $L^{\\top} L$ is given as:\n$$\nL^{\\top} L = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\n$$\nwhere $\\rho \\in [0,1)$ is the correlation between the two lead-field columns.\n\nOur first step is to compute the matrix $(L^{\\top} L + \\lambda I)^{-1}$. Let $A = L^{\\top} L$. We need to compute $(A + \\lambda I)^{-1}$.\n$$\nA + \\lambda I = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} + \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1+\\lambda & \\rho \\\\ \\rho & 1+\\lambda \\end{pmatrix}\n$$\nTo find the inverse of this $2 \\times 2$ matrix, we first compute its determinant:\n$$\n\\det(A + \\lambda I) = (1+\\lambda)(1+\\lambda) - \\rho \\cdot \\rho = (1+\\lambda)^2 - \\rho^2\n$$\nThe inverse is then given by:\n$$\n(A + \\lambda I)^{-1} = (L^{\\top} L + \\lambda I)^{-1} = \\frac{1}{(1+\\lambda)^2 - \\rho^2} \\begin{pmatrix} 1+\\lambda & -\\rho \\\\ -\\rho & 1+\\lambda \\end{pmatrix}\n$$\nNote that since $\\rho \\in [0,1)$, we have $\\rho^2 < 1$. For any $\\lambda > 0$, we have $1+\\lambda > 1 > \\rho$, so $(1+\\lambda)^2 - \\rho^2 > 0$, and the inverse is always well-defined.\n\nNext, we compute the resolution matrix $R(\\lambda)$:\n$$\nR(\\lambda) = (L^{\\top} L + \\lambda I)^{-1} (L^{\\top} L) = \\frac{1}{(1+\\lambda)^2 - \\rho^2} \\begin{pmatrix} 1+\\lambda & -\\rho \\\\ -\\rho & 1+\\lambda \\end{pmatrix} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\n\\begin{pmatrix} 1+\\lambda & -\\rho \\\\ -\\rho & 1+\\lambda \\end{pmatrix} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} = \\begin{pmatrix} (1+\\lambda)(1) + (-\\rho)(\\rho) & (1+\\lambda)(\\rho) + (-\\rho)(1) \\\\ (-\\rho)(1) + (1+\\lambda)(\\rho) & (-\\rho)(\\rho) + (1+\\lambda)(1) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 1+\\lambda - \\rho^2 & \\rho + \\lambda\\rho - \\rho \\\\ -\\rho + \\rho + \\lambda\\rho & -\\rho^2 + 1 + \\lambda \\end{pmatrix} = \\begin{pmatrix} 1+\\lambda - \\rho^2 & \\lambda\\rho \\\\ \\lambda\\rho & 1+\\lambda - \\rho^2 \\end{pmatrix}\n$$\nSubstituting this back into the expression for $R(\\lambda)$:\n$$\nR(\\lambda) = \\frac{1}{(1+\\lambda)^2 - \\rho^2} \\begin{pmatrix} 1+\\lambda - \\rho^2 & \\lambda\\rho \\\\ \\lambda\\rho & 1+\\lambda - \\rho^2 \\end{pmatrix}\n$$\nThe problem states that the PSF for a unit-amplitude source at index $1$ is the first column of $R(\\lambda)$. The amplitude bias at the true location is $R_{11}(\\lambda)$ and the leakage to the second source is $R_{21}(\\lambda)$. From the matrix $R(\\lambda)$, we can identify these elements:\n$$\nR_{11}(\\lambda) = \\frac{1+\\lambda - \\rho^2}{(1+\\lambda)^2 - \\rho^2}\n$$\n$$\nR_{21}(\\lambda) = \\frac{\\lambda\\rho}{(1+\\lambda)^2 - \\rho^2}\n$$\nThe width proxy $w(\\lambda)$ is defined as the leakage-to-peak ratio, $w(\\lambda) = R_{21}(\\lambda) / R_{11}(\\lambda)$. We can now compute this ratio:\n$$\nw(\\lambda) = \\frac{\\frac{\\lambda\\rho}{(1+\\lambda)^2 - \\rho^2}}{\\frac{1+\\lambda - \\rho^2}{(1+\\lambda)^2 - \\rho^2}}\n$$\nThe denominator term $(1+\\lambda)^2 - \\rho^2$ cancels out, provided that the numerator term $1+\\lambda - \\rho^2$ is non-zero. Since $\\lambda > 0$ and $\\rho^2 < 1$, we have $1 - \\rho^2 > 0$, so $1+\\lambda-\\rho^2 > 0$. Therefore, we arrive at the closed-form analytic expression:\n$$\nw(\\lambda) = \\frac{\\lambda\\rho}{1+\\lambda - \\rho^2}\n$$\nThis is the required expression for the width proxy.\n\nThe problem also asks for the limiting behaviors.\nFirst, consider the limit as $\\lambda \\to 0$:\n$$\n\\lim_{\\lambda \\to 0} w(\\lambda) = \\lim_{\\lambda \\to 0} \\frac{\\lambda\\rho}{1+\\lambda - \\rho^2} = \\frac{0 \\cdot \\rho}{1+0 - \\rho^2} = \\frac{0}{1 - \\rho^2} = 0\n$$\nThis result indicates that for very small regularization, the PSF becomes perfectly localized, with no leakage to the neighboring source.\n\nNext, consider the limit as $\\lambda \\to \\infty$:\n$$\n\\lim_{\\lambda \\to \\infty} w(\\lambda) = \\lim_{\\lambda \\to \\infty} \\frac{\\lambda\\rho}{1+\\lambda - \\rho^2}\n$$\nThis is a limit of a ratio of polynomials in $\\lambda$. We can divide the numerator and the denominator by the highest power of $\\lambda$, which is $\\lambda^1$:\n$$\n\\lim_{\\lambda \\to \\infty} \\frac{\\frac{\\lambda\\rho}{\\lambda}}{\\frac{1+\\lambda - \\rho^2}{\\lambda}} = \\lim_{\\lambda \\to \\infty} \\frac{\\rho}{\\frac{1}{\\lambda} + 1 - \\frac{\\rho^2}{\\lambda}}\n$$\nAs $\\lambda \\to \\infty$, the terms $\\frac{1}{\\lambda}$ and $\\frac{\\rho^2}{\\lambda}$ both approach $0$. Therefore, the limit is:\n$$\n\\lim_{\\lambda \\to \\infty} w(\\lambda) = \\frac{\\rho}{0 + 1 - 0} = \\rho\n$$\nThis result indicates that for very strong regularization, the spatial smearing (width) of the PSF becomes maximal, and the leakage to the second source is directly proportional to the correlation $\\rho$ between the source topographies. The solution spatial profile mimics the source correlation structure.\nThe final answer required is the closed-form expression for $w(\\lambda)$.",
            "answer": "$$\n\\boxed{\\frac{\\lambda\\rho}{1+\\lambda - \\rho^2}}\n$$"
        },
        {
            "introduction": "While advanced methods like sLORETA are designed to have compelling properties such as zero localization error for single sources, real brain activity is rarely so simple. This exercise demonstrates a critical limitation by showing how cross-talk between two simultaneously active sources can cause the algorithm to identify a single \"ghost\" peak. This practice underscores the importance of understanding an estimator's point-spread function when interpreting results from complex, multi-source brain activity. ",
            "id": "4157039",
            "problem": "Consider a linear electroencephalography/magnetoencephalography forward model with measurements $y$ and distributed cortical current $j$ satisfying $y = L j + n$, where $L$ is the leadfield operator and $n$ is additive sensor noise. Distributed source reconstructions such as Minimum Norm Estimate (MNE), Dynamic Statistical Parametric Mapping (dSPM), and Standardized Low-Resolution Brain Electromagnetic Tomography (sLORETA) produce linear estimates of $j$ from $y$ that can be analyzed via their resolution properties. Assume a one-dimensional cortical manifold parameterized by position $x \\in \\mathbb{R}$ along which sources are indexed, and suppose the head model and noise are homogeneous enough that the point-spread function of any linear estimator is translation invariant and even in displacement.\n\nUnder Standardized Low-Resolution Brain Electromagnetic Tomography (sLORETA), single-source unbiasedness is asserted in the sense that, for a noise-free single active source at $x_0$, the standardized map achieves its maximum at $x_0$. In this problem, analyze the limitation of this property when multiple sources are simultaneously active. Assume:\n- White sensor noise covariance with $C_n = \\sigma^{2} I$ and identity source covariance,\n- Fixed and identical source orientations,\n- A translation-invariant, even, strictly unimodal point-spread function $r(u)$ for the underlying linear estimator before standardization,\n- A diagonal variance standardization for sLORETA that is constant across position under the stated homogeneity.\n\nConsider two simultaneously active dipolar sources of equal amplitude located at $x_1 = 0$ and $x_2 = d$, with $d = 28$ mm. Starting from the linear forward model, the definition of the resolution matrix, and the variance standardization underlying sLORETA, derive from first principles how cross-talk between sources causes the sLORETA standardized map to form a single peak between $x_1$ and $x_2$, thereby reintroducing localization error relative to either true source. Then, compute the magnitude of this localization error with respect to $x_1$.\n\nExpress your final answer in mm and round your answer to four significant figures.",
            "solution": "The user wants me to solve a problem related to sLORETA source localization in neuroscience.\n\nHere's my plan:\n\n1.  **Problem Validation:**\n    *   **Step 1: Extract Givens:** I will list all the information provided in the problem statement without any modification.\n    *   **Step 2: Validate:** I will check if the problem is scientifically grounded, well-posed, objective, and complete. It seems to be a standard problem in EEG/MEG source imaging, so it should be valid. The concepts mentioned (MNE, dSPM, sLORETA, leadfield, point-spread function, resolution matrix) are all standard in the field. The assumptions (translation-invariant PSF, constant variance standardization) are simplifying but common for theoretical analysis. The problem asks for a derivation and a specific calculation, making it well-posed.\n    *   **Step 3: Verdict:** Based on the validation, I will decide whether to proceed or declare the problem invalid.\n\n2.  **Solution Derivation (if valid):**\n    *   **Step 2.1: Formalize the problem.**\n        *   Forward model: A continuous version is implied by $x \\in \\mathbb{R}$. The measurements $y$ are a vector, the source distribution $j(x)$ is a function, and the leadfield $L$ is an operator. $y = \\int L(x) j(x) dx + n$.\n        *   Linear estimator: A linear estimator $W$ gives an estimate of the source activity, $\\hat{j} = W y$. So, $\\hat{j} = W (L j + n)$.\n        *   Point-Spread Function (PSF): The PSF describes how the estimate of a point source at $x_0$ is spread out. Let $j(x) = \\delta(x-x_0)$. Then the estimated activity is $\\hat{j}(x) = (WL)(x, x_0)$. This is the PSF. The problem states it is translation-invariant and even, so $(WL)(x, x_0) = r(x - x_0)$ with $r(u) = r(-u)$.\n        *   Resolution Matrix: In the discrete case, the resolution matrix is $R = WL$. The rows of $R$ are the PSFs. In the continuous case, this is the kernel $(WL)(x, x_0)$.\n        *   sLORETA: This method standardizes the MNE estimate. The estimate is $\\hat{j}_{MNE} = W y$, where $W = L^T (L L^T + \\lambda C_n)^{-1}$ for MNE. The problem simplifies some aspects, saying the underlying linear estimator has a PSF $r(u)$. Let's call the unstandardized estimate $\\hat{j}_{unstd}(x)$.\n        *   The sLORETA estimate is $\\hat{j}_{sLORETA}(x) = \\frac{\\hat{j}_{unstd}(x)}{\\sqrt{\\text{Var}(\\hat{j}_{unstd}(x))}}$.\n        *   The problem states the variance standardization is constant across position. This is a key simplification. It means $\\text{Var}(\\hat{j}_{unstd}(x))$ is independent of $x$. This is consistent with the assumption of a homogeneous head model, white noise, and identity source covariance. The variance of the estimate at location $x$ is given by $\\text{Var}(\\hat{j}_{unstd}(x)) = \\text{Var}((Wy)(x)) = W_x C_n W_x^T$, where $W_x$ is the row of the estimator matrix corresponding to location $x$. If this is constant, say $V$, then standardization is just division by a constant $\\sqrt{V}$.\n        *   The problem states: \"for a noise-free single active source at $x_0$, the standardized map achieves its maximum at $x_0$\". Let's check this. The noise-free estimate for a source at $x_0$ is $\\hat{j}_{unstd}(x) = r(x-x_0)$. The sLORETA estimate is $\\hat{j}_{sLORETA}(x) = \\frac{r(x-x_0)}{\\sqrt{V}}$. The problem states $r(u)$ is strictly unimodal. This means $r(u)$ has a single maximum, which must be at $u=0$ because it's an even function. Thus, $\\hat{j}_{sLORETA}(x)$ has its maximum at $x=x_0$. This is the \"zero localization error\" or \"unbiasedness\" property of sLORETA for single sources.\n\n    *   **Step 2.2: Analyze the two-source case.**\n        *   The true source distribution is $j_{true}(x) = A \\delta(x-x_1) + A \\delta(x-x_2)$, where $A$ is the amplitude, $x_1=0$ and $x_2=d$. The problem states \"equal amplitude\". Let's set the amplitude to $1$ without loss of generality since it's a linear system. So, $j_{true}(x) = \\delta(x) + \\delta(x-d)$.\n        *   The noise-free unstandardized estimate is the superposition of the PSFs for each source.\n            $\\hat{j}_{unstd}(x) = \\text{PSF for source at } x_1 + \\text{PSF for source at } x_2$.\n            $\\hat{j}_{unstd}(x) = r(x - x_1) + r(x - x_2) = r(x) + r(x - d)$.\n        *   Since the standardization variance is constant, the sLORETA map is proportional to the unstandardized map.\n            $\\hat{j}_{sLORETA}(x) \\propto \\hat{j}_{unstd}(x) = r(x) + r(x - d)$.\n        *   We need to find the maximum of this combined map. Let $f(x) = r(x) + r(x - d)$. To find the maximum, we need to solve $f'(x) = 0$.\n            $f'(x) = r'(x) + r'(x - d) = 0$.\n            This gives $r'(x) = -r'(x - d)$.\n\n    *   **Step 2.3: Use the properties of the PSF.**\n        *   The problem does not specify the functional form of $r(u)$. This is a major hurdle. Is there an implied form? The problem states \"strictly unimodal point-spread function $r(u)$\". Let's think about typical PSFs in this context. They are often modeled as a Gaussian or a Lorentzian. A Gaussian function is a very common choice for modeling blurring or spreading. Let's assume $r(u)$ is a Gaussian-like function.\n        *   $r(u)$ is even, so $r(u) = r(-u)$. This means $r'(u) = -r'(-u)$, so $r'(u)$ is an odd function.\n        *   Using the odd property of $r'(u)$: $r'(x) = -r'(x - d) = r'(-(x - d)) = r'(d - x)$.\n        *   So we need to solve $r'(x) = r'(d - x)$.\n        *   A clear solution to this equation is $x = d-x$, which means $2x=d$, or $x = d/2$.\n        *   This seems to be the only solution. Let's confirm. The problem states that a single peak is formed. The whole setup is symmetric w.r.t the midpoint $d/2$. Let's show the symmetry of the resulting map $f(x) = r(x) + r(x-d)$ around $x=d/2$.\n        Let's evaluate the function at points symmetric around $d/2$, say $x = d/2 + \\delta$ and $x = d/2 - \\delta$.\n        $f(d/2 + \\delta) = r(d/2+\\delta)+r(d/2+\\delta-d) = r(d/2+\\delta) + r(-d/2+\\delta)$.\n        $f(d/2 - \\delta) = r(d/2-\\delta)+r(d/2-\\delta-d) = r(d/2-\\delta) + r(-d/2-\\delta)$.\n        Since $r(u)$ is an even function, $r(u) = r(-u)$. Therefore, $r(-d/2-\\delta) = r(d/2+\\delta)$ and $r(d/2-\\delta) = r(-(d/2-\\delta)) = r(-d/2+\\delta)$.\n        Substituting these into the expression for $f(d/2-\\delta)$ gives:\n        $f(d/2 - \\delta) = r(-d/2+\\delta) + r(d/2+\\delta) = f(d/2 + \\delta)$.\n        This confirms that the function $f(x)$ is symmetric about the axis $x=d/2$.\n        *   Since $f(x)$ is a symmetric function and the problem states it forms a single peak, this peak must lie on the axis of symmetry. Therefore, the peak location is $x_{peak} = d/2$.\n\n    *   **Step 2.4: The logic for a single peak at d/2.**\n        1.  The true source distribution for two equal-amplitude sources at $x_1=0$ and $x_2=d$ is $j(x) = A\\delta(x) + A\\delta(x-d)$.\n        2.  By linearity, the unstandardized estimated source map is $\\hat{j}_{unstd}(x) = A \\cdot r(x-0) + A \\cdot r(x-d)$, where $r(u)$ is the PSF.\n        3.  The problem states that sLORETA standardization is by a constant factor in this homogeneous case. Therefore, the sLORETA map $\\hat{j}_{sLORETA}(x)$ is proportional to $\\hat{j}_{unstd}(x)$, and they have the same peak location(s). We can analyze $f(x) = r(x) + r(x-d)$ to find the peak.\n        4.  The function $f(x)$ is symmetric around the midpoint $x=d/2$.\n        5.  The problem states that the map forms a *single peak*. This is an assumption we are given. It implies that the sources are close enough ($d$ is small enough relative to the width of $r(u)$) that their individual peaks merge into one. Given this, the point of symmetry $x=d/2$ must be the location of this single peak.\n\n    *   **Step 2.5: Compute the localization error.**\n        *   The true sources are at $x_1=0$ and $x_2=d$.\n        *   The estimated peak location is at $x_{peak} = d/2$.\n        *   The localization error is the distance between the estimated peak and a true source.\n        *   Localization error with respect to $x_1$: $|x_{peak} - x_1| = |d/2 - 0| = d/2$.\n        *   Localization error with respect to $x_2$: $|x_{peak} - x_2| = |d/2 - d| = |-d/2| = d/2$.\n        *   The error is the same for both sources, as expected from symmetry.\n\n    *   **Step 2.6: Calculate the final numerical answer.**\n        *   Given: $d = 28$ mm.\n        *   Localization error = $d/2 = 28 / 2 = 14$ mm.\n        *   The question asks to round to four significant figures. The result is exactly 14. This can be written as $14.00$.\n\nThe solution seems sound. I will write it down.",
            "answer": "$$\n\\boxed{14.00}\n$$"
        }
    ]
}