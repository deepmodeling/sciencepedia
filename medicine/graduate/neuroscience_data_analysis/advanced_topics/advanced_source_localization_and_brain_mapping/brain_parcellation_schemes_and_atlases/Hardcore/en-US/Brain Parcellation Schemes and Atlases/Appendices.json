{
    "hands_on_practices": [
        {
            "introduction": "Before we can use or create brain atlases, we need a robust way to quantify their similarity. This fundamental practice guides you through deriving and applying two of the most common metrics for comparing segmentation overlaps: the Dice similarity coefficient and the Jaccard index. By starting from set-theoretic first principles, you will gain a deep understanding of what these metrics represent and how they relate to each other, a crucial skill for evaluating atlas accuracy and consistency .",
            "id": "4143491",
            "problem": "You are given two structural Magnetic Resonance Imaging (MRI) brain atlases that each provide a binary mask for the same cortical label, defined on a shared voxel grid with isotropic voxel size $1 \\ \\mathrm{mm}^{3}$. Let the set of voxels labeled by atlas $\\mathcal{A}$ be denoted by $S_{\\mathcal{A}} \\subset \\Omega$ and the set of voxels labeled by atlas $\\mathcal{B}$ be denoted by $S_{\\mathcal{B}} \\subset \\Omega$, where $\\Omega$ is the finite set of all voxels in the brain volume. The binary masks encode membership in sets by assigning the value $1$ to voxels in the label and $0$ otherwise. The following counts are empirically computed:\n- The cardinality of the atlas $\\mathcal{A}$ label, $|S_{\\mathcal{A}}|$, is $12{,}800$ voxels.\n- The cardinality of the atlas $\\mathcal{B}$ label, $|S_{\\mathcal{B}}|$, is $13{,}600$ voxels.\n- The cardinality of the intersection, $|S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|$, is $12{,}000$ voxels.\n\nStarting from core set-theoretic definitions and properties (such as the definition of set intersection, set union, and the inclusion-exclusion identity $|S_{\\mathcal{A}} \\cup S_{\\mathcal{B}}| = |S_{\\mathcal{A}}| + |S_{\\mathcal{B}}| - |S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|$), do the following:\n\n1. Derive an expression for the Dice similarity coefficient in terms of $|S_{\\mathcal{A}}|$, $|S_{\\mathcal{B}}|$, and $|S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|$, starting from first principles by interpreting similarity as the harmonic mean of set-wise precision and recall.\n2. Derive an expression for the Jaccard index in terms of $|S_{\\mathcal{A}}|$, $|S_{\\mathcal{B}}|$, and $|S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|$, starting from first principles by interpreting similarity as intersection-over-union.\n3. Using your derived expressions, compute the Dice similarity coefficient and Jaccard index for the given masks.\n\nExpress both metrics as dimensionless decimals rounded to four significant figures. The final answer must be given as two values corresponding to $\\text{Dice}$ and $\\text{Jaccard}$, respectively.",
            "solution": "We formalize the problem using basic set-theoretic definitions over the voxel domain $\\Omega$. For any two label sets $S_{\\mathcal{A}}, S_{\\mathcal{B}} \\subset \\Omega$, the intersection $S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}$ contains precisely the voxels labeled by both atlases, and the union $S_{\\mathcal{A}} \\cup S_{\\mathcal{B}}$ contains the voxels labeled by at least one atlas. The inclusion-exclusion principle ensures\n$$\n|S_{\\mathcal{A}} \\cup S_{\\mathcal{B}}| = |S_{\\mathcal{A}}| + |S_{\\mathcal{B}}| - |S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|.\n$$\n\nWe define set-wise precision and recall for $S_{\\mathcal{A}}$ (as a reference) and $S_{\\mathcal{B}}$ (as a candidate) as follows:\n- Precision is the fraction of voxels labeled by $\\mathcal{B}$ that are also labeled by $\\mathcal{A}$, given by\n$$\nP = \\frac{|S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|}{|S_{\\mathcal{B}}|}.\n$$\n- Recall is the fraction of voxels labeled by $\\mathcal{A}$ that are also labeled by $\\mathcal{B}$, given by\n$$\nR = \\frac{|S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|}{|S_{\\mathcal{A}}|}.\n$$\n\nThe Dice similarity coefficient can be derived as the harmonic mean of precision and recall:\n$$\n\\text{Dice} = \\frac{2 \\, P \\, R}{P + R}.\n$$\nSubstituting $P$ and $R$ yields\n$$\n\\text{Dice} = \\frac{2 \\left( \\frac{|S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|}{|S_{\\mathcal{B}}|} \\right) \\left( \\frac{|S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|}{|S_{\\mathcal{A}}|} \\right)}{\\frac{|S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|}{|S_{\\mathcal{B}}|} + \\frac{|S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|}{|S_{\\mathcal{A}}|}} = \\frac{2 \\, |S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|}{|S_{\\mathcal{A}}| + |S_{\\mathcal{B}}|}.\n$$\n\nThe Jaccard index is defined as the ratio of intersection to union:\n$$\n\\text{Jaccard} = \\frac{|S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|}{|S_{\\mathcal{A}} \\cup S_{\\mathcal{B}}|} = \\frac{|S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|}{|S_{\\mathcal{A}}| + |S_{\\mathcal{B}}| - |S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|}.\n$$\n\nThese definitions yield a functional relationship between the two metrics. Let $I = |S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}|$ and $T = |S_{\\mathcal{A}}| + |S_{\\mathcal{B}}|$. Then\n$$\n\\text{Dice} = \\frac{2 I}{T}, \\quad \\text{Jaccard} = \\frac{I}{T - I}.\n$$\nSolving for $I$ from the Dice expression gives $I = \\frac{\\text{Dice} \\cdot T}{2}$. Substituting into the Jaccard expression:\n$$\n\\text{Jaccard} = \\frac{\\frac{\\text{Dice} \\cdot T}{2}}{T - \\frac{\\text{Dice} \\cdot T}{2}} = \\frac{\\text{Dice}}{2 - \\text{Dice}},\n$$\nand the inverse relationship\n$$\n\\text{Dice} = \\frac{2 \\, \\text{Jaccard}}{1 + \\text{Jaccard}}.\n$$\n\nWe now compute the metrics with the provided counts:\n$$\n|S_{\\mathcal{A}}| = 12{,}800, \\quad |S_{\\mathcal{B}}| = 13{,}600, \\quad |S_{\\mathcal{A}} \\cap S_{\\mathcal{B}}| = 12{,}000.\n$$\nFirst, compute the union cardinality using inclusion-exclusion:\n$$\n|S_{\\mathcal{A}} \\cup S_{\\mathcal{B}}| = 12{,}800 + 13{,}600 - 12{,}000 = 14{,}400.\n$$\nCompute the Dice similarity coefficient:\n$$\n\\text{Dice} = \\frac{2 \\times 12{,}000}{12{,}800 + 13{,}600} = \\frac{24{,}000}{26{,}400} = \\frac{240}{264} = \\frac{10}{11}.\n$$\nThus,\n$$\n\\text{Dice} = \\frac{10}{11} \\approx 0.909090\\ldots\n$$\nCompute the Jaccard index:\n$$\n\\text{Jaccard} = \\frac{12{,}000}{14{,}400} = \\frac{120}{144} = \\frac{5}{6} \\approx 0.833333\\ldots\n$$\n\nFinally, round both decimal values to four significant figures as required:\n$$\n\\text{Dice} \\approx 0.9091, \\quad \\text{Jaccard} \\approx 0.8333.\n$$\nThese are dimensionless quantities.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.9091 & 0.8333\\end{pmatrix}}$$"
        },
        {
            "introduction": "Neuroimaging analyses often require data to be in a common resolution, forcing us to resample atlases. However, since atlas labels are categorical, standard interpolation methods like linear or cubic are inappropriate as they would create meaningless intermediate label values. This exercise  simulates the downsampling of a high-resolution atlas, allowing you to compare the impact of two common label-preserving strategies—nearest-neighbor and majority-vote interpolation—on label fidelity across various spatial patterns.",
            "id": "4143470",
            "problem": "You will analyze the effect of resampling a labeled brain parcellation atlas from a $1$ millimeter isotropic grid to a $2$ millimeter isotropic grid on label fidelity using two interpolation strategies and compute the resulting mislabeling rates. Treat the atlas as a three-dimensional array of integer region labels (parcels), where each element corresponds to a voxel. The base principles are: a brain parcellation atlas is a piecewise-constant label field on a voxel grid, and resampling a label field must not create new labels but reassign labels according to a rule. You must adhere to the following precise definitions to make the problem purely mathematical and fully reproducible.\n\nDefinitions and operators:\n- Let $A \\in \\mathbb{N}^{X \\times Y \\times Z}$ denote the original label atlas at $1$ millimeter resolution, with $X=Y=Z=8$. Let the set of labels be positive integers. Define a downsampling factor $s=2$.\n- The $2$ millimeter grid partitions the $1$ millimeter grid into non-overlapping blocks of size $2 \\times 2 \\times 2$. Index coarse-grid voxels by $(I,J,K)$ with $I \\in \\{0,\\dots,\\frac{X}{2}-1\\}$, $J \\in \\{0,\\dots,\\frac{Y}{2}-1\\}$, $K \\in \\{0,\\dots,\\frac{Z}{2}-1\\}$. The corresponding fine-grid indices in the block are $i \\in \\{2I,2I+1\\}$, $j \\in \\{2J,2J+1\\}$, $k \\in \\{2K,2K+1\\}$.\n- Define two downsampling operators:\n  1. Nearest-neighbor decimation $D^{(\\mathrm{nn})}$: for each coarse voxel $(I,J,K)$,\n     $$B^{(\\mathrm{nn})}_{I,J,K} \\equiv D^{(\\mathrm{nn})}(A)_{I,J,K} := A_{2I,\\,2J,\\,2K}.$$\n     This corresponds to picking the label at the top-left-front corner of each $2 \\times 2 \\times 2$ block.\n  2. Majority-vote block mode $D^{(\\mathrm{mv})}$: for each coarse voxel $(I,J,K)$,\n     $$B^{(\\mathrm{mv})}_{I,J,K} \\equiv D^{(\\mathrm{mv})}(A)_{I,J,K} := \\arg\\max_{\\ell \\in \\mathcal{L}} c_{\\ell}(I,J,K),$$\n     where $\\mathcal{L}$ is the set of labels present in the block and $c_{\\ell}(I,J,K)$ counts how many of the $8$ fine voxels in the block have label $\\ell$. In case of a tie in the maximum count, choose the smallest label value among the tied labels.\n- Define an upsampling-by-replication operator $U$ that maps a coarse grid back to the fine grid by filling each $2 \\times 2 \\times 2$ block with its coarse label:\n  $$C_{i,j,k} \\equiv U(B)_{i,j,k} := B_{\\left\\lfloor \\frac{i}{2} \\right\\rfloor,\\, \\left\\lfloor \\frac{j}{2} \\right\\rfloor,\\, \\left\\lfloor \\frac{k}{2} \\right\\rfloor}.$$\n- Define the mislabeling rate $r$ of a resampling pipeline $A \\mapsto B \\mapsto C$ as the fraction of fine-grid voxels whose upsampled label differs from the original:\n  $$r := \\frac{1}{XYZ} \\sum_{i=0}^{X-1} \\sum_{j=0}^{Y-1} \\sum_{k=0}^{Z-1} \\mathbf{1}\\big[ C_{i,j,k} \\neq A_{i,j,k} \\big],$$\n  where $\\mathbf{1}[\\cdot]$ is the indicator function.\n\nTasks:\n- For each test atlas defined below, compute the mislabeling rate $r^{(\\mathrm{nn})}$ using $D^{(\\mathrm{nn})}$ followed by $U$ and the mislabeling rate $r^{(\\mathrm{mv})}$ using $D^{(\\mathrm{mv})}$ followed by $U$. Report both $r^{(\\mathrm{nn})}$ and $r^{(\\mathrm{mv})}$ for each test.\n\nTest suite (each atlas is defined deterministically over indices $i \\in \\{0,\\dots,7\\}$, $j \\in \\{0,\\dots,7\\}$, $k \\in \\{0,\\dots,7\\}$):\n- Test $1$ (Uniform atlas): $A_{i,j,k} = 1$ for all $(i,j,k)$.\n- Test $2$ (Sparse corner impurity per block): $A_{i,j,k} = 1$ for all $(i,j,k)$ except that for every block start $(i,j,k)$ with $i \\in \\{0,2,4,6\\}$, $j \\in \\{0,2,4,6\\}$, $k \\in \\{0,2,4,6\\}$, set $A_{i,j,k} = 2$.\n- Test $3$ (Aligned planar boundary): $A_{i,j,k} = 1$ if $i < 4$, else $A_{i,j,k} = 2$.\n- Test $4$ (Misaligned planar boundary): $A_{i,j,k} = 1$ if $i < 5$, else $A_{i,j,k} = 2$.\n- Test $5$ (Three-dimensional checkerboard): $A_{i,j,k} = 1$ if $(i + j + k) \\bmod 2 = 0$, else $A_{i,j,k} = 2$.\n- Test $6$ (Deterministic multi-label heterogeneity): $A_{i,j,k} = \\big( (7 i + 11 j + 13 k) \\bmod 3 \\big) + 1$.\n\nOutput specification:\n- Your program must produce a single line containing a list of results for the $6$ tests in order. Each result is a two-element list $[r^{(\\mathrm{nn})}, r^{(\\mathrm{mv})}]$, with each float rounded to six decimals. The final output must be of the form\n  $$\\big[ [r^{(\\mathrm{nn})}_1, r^{(\\mathrm{mv})}_1], [r^{(\\mathrm{nn})}_2, r^{(\\mathrm{mv})}_2], \\dots, [r^{(\\mathrm{nn})}_6, r^{(\\mathrm{mv})}_6] \\big],$$\n  printed exactly as a single line string without additional text.\n\nScientific realism note:\n- A brain parcellation atlas is a label field representing Regions of Interest, and it is standard practice to resample using nearest-neighbor or majority-vote operations to preserve labels without averaging. The above definitions instantiate these principles on a discrete grid so that results are reproducible and testable.\n\nDeliverable:\n- Implement a complete, runnable program that constructs the test atlases, applies both resampling pipelines, computes the mislabeling rates as defined, and prints the results in the required format with six-decimal rounding.",
            "solution": "This problem requires implementing and comparing two distinct resampling pipelines for a 3D categorical label atlas. The goal is to quantify the information loss (mislabeling rate) incurred by downsampling the atlas from a fine grid to a coarse grid and then upsampling it back to the original resolution.\n\nThe overall algorithm proceeds by iterating through each of the six test cases. For each case, an $8 \\times 8 \\times 8$ 'ground truth' atlas, $A$, is generated according to its mathematical definition.\n\nThen, two separate downsampling pipelines are applied to create a $4 \\times 4 \\times 4$ coarse atlas, $B$:\n1.  **Nearest-Neighbor ($D^{(\\mathrm{nn})}$):** For each $2 \\times 2 \\times 2$ block in the fine atlas, this operator simply takes the label from the voxel at the corner with the lowest indices ($A_{2I, 2J, 2K}$). This method is computationally fast but sensitive to the alignment of features with the sampling grid.\n2.  **Majority-Vote ($D^{(\\mathrm{mv})}$):** This operator identifies the most frequent label within each $2 \\times 2 \\times 2$ block. This provides a more robust representation of the block's content. A deterministic tie-breaking rule (choosing the smallest label value) ensures a unique result.\n\nEach of the resulting coarse atlases, $B^{(\\mathrm{nn})}$ and $B^{(\\mathrm{mv})}$, is then upsampled back to the original $8 \\times 8 \\times 8$ resolution using a replication operator, $U$. This operator fills each $2 \\times 2 \\times 2$ block in the new fine grid, $C$, with the single label value from its corresponding coarse voxel.\n\nFinally, the mislabeling rate, $r$, is computed for each pipeline by comparing the upsampled atlas ($C^{(\\mathrm{nn})}$ or $C^{(\\mathrm{mv})}$) to the original atlas $A$. The rate is the fraction of total voxels where the label has changed ($C_{i,j,k} \\neq A_{i,j,k}$). This systematic process allows for a direct comparison of how well each resampling method preserves label fidelity across various spatial patterns.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import mode\n\ndef solve():\n    \"\"\"\n    Solves the brain atlas resampling problem for all specified test cases.\n    \"\"\"\n    X, Y, Z = 8, 8, 8\n    S = 2\n    TOTAL_VOXELS = X * Y * Z\n    \n    # List of functions to generate each test atlas\n    test_generators = [\n        generate_test_1,\n        generate_test_2,\n        generate_test_3,\n        generate_test_4,\n        generate_test_5,\n        generate_test_6,\n    ]\n\n    results = []\n    for gen_func in test_generators:\n        atlas_a = gen_func(X, Y, Z)\n        \n        # Pipeline 1: Nearest-Neighbor\n        atlas_b_nn = downsample_nn(atlas_a, S)\n        atlas_c_nn = upsample_replicate(atlas_b_nn, S)\n        r_nn = np.sum(atlas_c_nn != atlas_a) / TOTAL_VOXELS\n        \n        # Pipeline 2: Majority-Vote\n        atlas_b_mv = downsample_mv(atlas_a, S)\n        atlas_c_mv = upsample_replicate(atlas_b_mv, S)\n        r_mv = np.sum(atlas_c_mv != atlas_a) / TOTAL_VOXELS\n        \n        # Format the result for the current test case as specified\n        # Rounding to 6 decimal places. The format string handles this.\n        results.append(f\"[{r_nn:.6f},{r_mv:.6f}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\ndef generate_test_1(X, Y, Z):\n    \"\"\"Test 1 (Uniform atlas): A_ijk = 1 for all (i,j,k).\"\"\"\n    return np.ones((X, Y, Z), dtype=int)\n\ndef generate_test_2(X, Y, Z):\n    \"\"\"Test 2 (Sparse corner impurity per block): A_ijk = 1, except A_ijk = 2 at block corners.\"\"\"\n    A = np.ones((X, Y, Z), dtype=int)\n    for i in range(0, X, 2):\n        for j in range(0, Y, 2):\n            for k in range(0, Z, 2):\n                A[i, j, k] = 2\n    return A\n\ndef generate_test_3(X, Y, Z):\n    \"\"\"Test 3 (Aligned planar boundary): A_ijk = 1 if i  4, else A_ijk = 2.\"\"\"\n    A = np.ones((X, Y, Z), dtype=int)\n    A[4:, :, :] = 2\n    return A\n\ndef generate_test_4(X, Y, Z):\n    \"\"\"Test 4 (Misaligned planar boundary): A_ijk = 1 if i  5, else A_ijk = 2.\"\"\"\n    A = np.ones((X, Y, Z), dtype=int)\n    A[5:, :, :] = 2\n    return A\n\ndef generate_test_5(X, Y, Z):\n    \"\"\"Test 5 (Three-dimensional checkerboard): A_ijk = 1 if (i+j+k) mod 2 = 0, else 2.\"\"\"\n    i, j, k = np.mgrid[0:X, 0:Y, 0:Z]\n    A = ((i + j + k) % 2) + 1\n    return A.astype(int)\n\ndef generate_test_6(X, Y, Z):\n    \"\"\"Test 6 (Deterministic multi-label heterogeneity): A_ijk = ((7i+11j+13k) mod 3) + 1.\"\"\"\n    i, j, k = np.mgrid[0:X, 0:Y, 0:Z]\n    A = ((7 * i + 11 * j + 13 * k) % 3) + 1\n    return A.astype(int)\n\ndef downsample_nn(A, s):\n    \"\"\"Downsamples a 3D atlas using nearest-neighbor decimation.\"\"\"\n    return A[::s, ::s, ::s]\n\ndef downsample_mv(A, s):\n    \"\"\"Downsamples a 3D atlas using majority-vote block mode.\"\"\"\n    X, Y, Z = A.shape\n    Xc, Yc, Zc = X // s, Y // s, Z // s\n    B = np.zeros((Xc, Yc, Zc), dtype=A.dtype)\n    \n    for I in range(Xc):\n        for J in range(Yc):\n            for K in range(Zc):\n                block = A[I*s:(I+1)*s, J*s:(J+1)*s, K*s:(K+1)*s]\n                # scipy.stats.mode with default parameters breaks ties by choosing the smallest value,\n                # which matches the problem specification.\n                # The returned object has attributes .mode and .count. We only need the mode.\n                B[I, J, K] = mode(block, axis=None, keepdims=False).mode\n    return B\n\ndef upsample_replicate(B, s):\n    \"\"\"Upsamples a 3D atlas by replicating coarse voxels into blocks.\"\"\"\n    # np.kron is a convenient way to perform block replication\n    return np.kron(B, np.ones((s, s, s), dtype=B.dtype))\n\nsolve()\n```"
        },
        {
            "introduction": "A key challenge in neuroscience is creating a group-representative atlas from the brains of many individuals, each with unique anatomical features. This practice  introduces powerful techniques to evaluate the quality of inter-subject registration and quantify anatomical variability. By computing vertex-wise label entropy and agreement, you will learn to identify brain regions with high cross-subject variability and distinguish this from potential registration errors, a critical step in interpreting connectomics and group-level neuroimaging findings.",
            "id": "4143439",
            "problem": "You are given a finite vertex set representing a discretized cortical surface, an adjacency structure encoding the surface topology, a set of individual subject parcellation labelings, and a group atlas labeling. Your goal is to evaluate cross-subject alignment by computing the Shannon entropy of labels at each vertex, quantify agreement with the group atlas, and relate high entropy to either anatomical variability or registration errors using an adjacency-based boundary sensitivity index. All quantities in this problem are unitless.\n\nFundamental base and core definitions:\n- Let there be $V$ vertices indexed by $v \\in \\{0,1,\\dots,V-1\\}$, and $S$ subjects indexed by $s \\in \\{0,1,\\dots,S-1\\}$.\n- Let the label set be a finite set $L = \\{0,1,\\dots,K-1\\}$.\n- Let $l_{s,v} \\in L$ denote the label of subject $s$ at vertex $v$.\n- Let $g_v \\in L$ denote the group atlas label at vertex $v$.\n- Let $N(v)$ denote the set of neighbors of vertex $v$ under the adjacency relation.\n\nYou must perform the following computations grounded in well-tested formulas and definitions:\n\n1. Empirical label distribution at each vertex:\n   For each vertex $v$, compute the empirical probabilities\n   $$p_{v,l} = \\frac{1}{S} \\sum_{s=0}^{S-1} \\mathbf{1}\\{l_{s,v} = l\\}, \\quad \\text{for all } l \\in L,$$\n   where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\n2. Shannon entropy at each vertex:\n   Using the empirical distribution $\\{p_{v,l}\\}_{l \\in L}$, compute the entropy\n   $$H_v = -\\sum_{l \\in L} p_{v,l} \\log p_{v,l},$$\n   with the convention $0 \\log 0 = 0$ and logarithm base $e$ (natural logarithm). The entropy is expressed in nats.\n\n3. Agreement rate with the group atlas:\n   For each vertex $v$, compute\n   $$A_v = \\frac{1}{S} \\sum_{s=0}^{S-1} \\mathbf{1}\\{l_{s,v} = g_v\\}.$$\n\n4. Boundary proximity index:\n   Define a boundary sensitivity based solely on the group atlas labels and adjacency:\n   $$B_v = \\frac{1}{|N(v)|} \\sum_{u \\in N(v)} \\mathbf{1}\\{g_u \\neq g_v\\},$$\n   where $|N(v)|$ is the degree of vertex $v$. This index is high near atlas boundaries and zero away from boundaries.\n\n5. Correlation between entropy and boundary proximity:\n   Compute the Pearson correlation coefficient between $\\{H_v\\}_{v=0}^{V-1}$ and $\\{B_v\\}_{v=0}^{V-1}$:\n   $$\\rho = \\frac{\\sum_{v=0}^{V-1} (H_v - \\bar{H})(B_v - \\bar{B})}{\\sqrt{\\sum_{v=0}^{V-1} (H_v - \\bar{H})^2}\\sqrt{\\sum_{v=0}^{V-1} (B_v - \\bar{B})^2}},$$\n   where $\\bar{H}$ and $\\bar{B}$ are the means over vertices. If either denominator term is zero (zero variance in $\\{H_v\\}$ or $\\{B_v\\}$), define $\\rho = 0$ by convention.\n\n6. Registration-sensitive vertex classification:\n   Using thresholds $\\tau_B$ and $\\tau_A$, classify vertex $v$ as registration-sensitive if\n   $$R_v = \\mathbf{1}\\{B_v \\ge \\tau_B \\ \\land \\ A_v  \\tau_A\\}.$$\n   Then compute the fraction of registration-sensitive vertices\n   $$F = \\frac{1}{V}\\sum_{v=0}^{V-1} R_v.$$\n\nYour program must implement the computations above and, for each test case, return the triple $[\\bar{H}, \\rho, F]$, where\n$$\\bar{H} = \\frac{1}{V}\\sum_{v=0}^{V-1} H_v.$$\n\nTest suite:\nYou must use the following three test cases with the specified parameters. All label values are integers and unitless.\n\n- Test Case 1 (happy path with boundary-related misregistration):\n  - $V = 12$, $S = 10$, $L = \\{0,1\\}$.\n  - Adjacency: a path graph on $12$ vertices, $N(v) = \\{v-1, v+1\\}$ where defined; ends have a single neighbor.\n  - Group atlas labels: $g_v = 0$ for $v \\in \\{0,1,2,3,4,5\\}$ and $g_v = 1$ for $v \\in \\{6,7,8,9,10,11\\}$.\n  - Individual labels: start from $l_{s,v} = g_v$ for all $s,v$, then introduce boundary misregistrations:\n    - At $v = 5$, set $l_{s,5} = 1$ for $s \\in \\{0,3,7\\}$.\n    - At $v = 6$, set $l_{s,6} = 0$ for $s \\in \\{2,4\\}$.\n  - Thresholds: $\\tau_B = 0.5$, $\\tau_A = 0.85$.\n\n- Test Case 2 (perfect alignment):\n  - $V = 12$, $S = 10$, $L = \\{0,1\\}$.\n  - Adjacency: same path graph as Test Case 1.\n  - Group atlas labels: same as Test Case 1.\n  - Individual labels: $l_{s,v} = g_v$ for all $s,v$.\n  - Thresholds: $\\tau_B = 0.5$, $\\tau_A = 0.85$.\n\n- Test Case 3 (anatomical variability away from boundaries):\n  - $V = 12$, $S = 10$, $L = \\{0,1\\}$.\n  - Adjacency: same path graph as Test Case 1.\n  - Group atlas labels: same as Test Case 1.\n  - Individual labels: start from $l_{s,v} = g_v$ then introduce interior variability:\n    - At $v = 2$, set $l_{s,2} = 1$ for $s \\in \\{0,1,2,3,4\\}$.\n    - At $v = 9$, set $l_{s,9} = 0$ for $s \\in \\{5,6,7,8,9\\}$.\n  - Thresholds: $\\tau_B = 0.5$, $\\tau_A = 0.85$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes its own three-element list in order. The required format is\n$$[[\\bar{H}_1,\\rho_1,F_1],[\\bar{H}_2,\\rho_2,F_2],[\\bar{H}_3,\\rho_3,F_3]],$$\nwith no additional text printed.",
            "solution": "The problem requires the implementation of a series of standard neuroinformatic metrics to assess inter-subject variability and registration quality in cortical surface parcellations. The computational pipeline processes the given data for each test case—vertex count $V$, subject count $S$, label set $L$, adjacency structure, group atlas labels $g_v$, and individual subject labels $l_{s,v}$—to produce the required output triple $[\\bar{H}, \\rho, F]$.\n\nThe step-by-step procedure for a single test case is as follows:\n\n1.  **Data Representation**: The inputs are represented using numerical arrays for efficient computation. The individual subject labels $l_{s,v}$ form a matrix of size $S \\times V$, the group atlas labels $g_v$ form a vector of size $V$, and the adjacency structure is represented as a list of lists.\n\n2.  **Empirical Label Distribution ($p_{v,l}$)**: For each vertex $v$, we count the occurrences of each possible label $l \\in L$ across all $S$ subjects. These counts are then normalized by $S$ to yield the empirical probability distribution $\\{p_{v,l}\\}_{l \\in L}$ at that vertex.\n\n3.  **Shannon Entropy ($H_v$)**: Using the computed probabilities $p_{v,l}$, the Shannon entropy for each vertex $v$ is calculated as $H_v = -\\sum_{l \\in L} p_{v,l} \\log_e p_{v,l}$. The natural logarithm is used, and the convention $0 \\log_e 0 = 0$ is handled. The mean entropy $\\bar{H}$ is then computed by averaging this vector.\n\n4.  **Group Atlas Agreement ($A_v$)**: For each vertex $v$, the agreement rate is the fraction of subjects whose label $l_{s,v}$ matches the group atlas label $g_v$. This is calculated as $A_v = \\frac{1}{S} \\sum_{s=0}^{S-1} \\mathbf{1}\\{l_{s,v} = g_v\\}$.\n\n5.  **Boundary Proximity Index ($B_v$)**: This index quantifies how close a vertex $v$ is to a boundary in the group atlas $g$. For each vertex $v$, the index $B_v$ is the fraction of its neighbors $u \\in N(v)$ whose atlas label $g_u$ differs from $g_v$: $B_v = \\frac{1}{|N(v)|} \\sum_{u \\in N(v)} \\mathbf{1}\\{g_u \\neq g_v\\}$.\n\n6.  **Correlation ($\\rho$)**: The Pearson correlation coefficient $\\rho$ is computed between the entropy vector $\\{H_v\\}$ and the boundary proximity vector $\\{B_v\\}$. If either vector has zero variance, $\\rho$ is defined as $0$.\n\n7.  **Registration-Sensitive Fraction ($F$)**: Each vertex $v$ is classified as \"registration-sensitive\" if it lies near an atlas boundary ($B_v \\ge \\tau_B$) and simultaneously exhibits low agreement with the group atlas ($A_v  \\tau_A$). The total fraction of such vertices, $F$, is computed by summing the classification indicator over all vertices and dividing by $V$.\n\nThis complete sequence of computations is applied to each of the three test cases, and the final results are aggregated into the required output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_metrics(V, S, L, adj, g_v, l_sv, tau_A, tau_B):\n    \"\"\"\n    Computes all required metrics for a single test case.\n    \"\"\"\n    K = len(L)\n\n    # 1. Empirical label distribution p_{v,l}\n    p_vl = np.zeros((V, K))\n    for v in range(V):\n        # np.bincount is efficient for counting occurrences of integers in an array.\n        labels_at_v = l_sv[:, v]\n        counts = np.bincount(labels_at_v, minlength=K)\n        p_vl[v, :] = counts / S\n\n    # 2. Shannon entropy H_v and its mean H_bar\n    # The 'where' argument prevents log(0) warnings.\n    log_p = np.log(p_vl, where=(p_vl > 0), out=np.zeros_like(p_vl, dtype=float))\n    H_v = -np.sum(p_vl * log_p, axis=1)\n    H_bar = np.mean(H_v)\n\n    # 3. Agreement rate A_v\n    # Compare each subject's labels with the group atlas labels.\n    # g_v is broadcasted from (V,) to (S, V) to match l_sv's shape.\n    agreement_matrix = (l_sv == g_v[None, :])\n    A_v = np.mean(agreement_matrix, axis=0)\n\n    # 4. Boundary proximity index B_v\n    B_v = np.zeros(V, dtype=float)\n    for v in range(V):\n        neighbors = adj[v]\n        if not neighbors:\n            continue\n        num_neighbors = len(neighbors)\n        neighbor_labels = g_v[neighbors]\n        num_mismatches = np.sum(neighbor_labels != g_v[v])\n        B_v[v] = num_mismatches / num_neighbors\n\n    # 5. Correlation rho\n    # Check for zero variance as per problem specification.\n    var_H = np.var(H_v)\n    var_B = np.var(B_v)\n\n    if var_H == 0 or var_B == 0:\n        rho = 0.0\n    else:\n        # Standard Pearson correlation formula.\n        mean_H = np.mean(H_v)\n        mean_B = np.mean(B_v)\n        numerator = np.sum((H_v - mean_H) * (B_v - mean_B))\n        denominator_H = np.sqrt(np.sum((H_v - mean_H)**2))\n        denominator_B = np.sqrt(np.sum((B_v - mean_B)**2))\n        rho = numerator / (denominator_H * denominator_B)\n\n    # 6. Registration-sensitive fraction F\n    R_v_mask = (B_v >= tau_B)  (A_v  tau_A)\n    F = np.mean(R_v_mask.astype(float))\n\n    return [H_bar, rho, F]\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    V = 12\n    S = 10\n    L = {0, 1}\n    adj = [([v - 1, v + 1] if 1 = v = V - 2 else ([1] if v == 0 else [V - 2])) for v in range(V)]\n    g_v = np.array([0] * 6 + [1] * 6, dtype=int)\n    tau_A = 0.85\n    tau_B = 0.5\n\n    test_cases_data = []\n\n    # Test Case 1\n    l_sv_1 = np.tile(g_v, (S, 1))\n    l_sv_1[[0, 3, 7], 5] = 1\n    l_sv_1[[2, 4], 6] = 0\n    test_cases_data.append(l_sv_1)\n\n    # Test Case 2\n    l_sv_2 = np.tile(g_v, (S, 1))\n    test_cases_data.append(l_sv_2)\n\n    # Test Case 3\n    l_sv_3 = np.tile(g_v, (S, 1))\n    l_sv_3[list(range(5)), 2] = 1\n    l_sv_3[list(range(5, 10)), 9] = 0\n    test_cases_data.append(l_sv_3)\n\n    results = []\n    for l_sv in test_cases_data:\n        case_result = compute_metrics(V, S, L, adj, g_v, l_sv, tau_A, tau_B)\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}