{
    "hands_on_practices": [
        {
            "introduction": "A primary goal of hyperalignment is to map subjects' neural data into a common space while preserving the intrinsic representational geometry within each subject. This geometry, which captures the relationships between neural patterns evoked by different stimuli, can be summarized in a Representational Dissimilarity Matrix (RDM). This first exercise provides a concrete, hands-on demonstration of this foundational principle by having you compute an RDM and verify its invariance under an orthogonal rotation, the very class of transformations used in hyperalignment.",
            "id": "4168872",
            "problem": "Consider a simplified functional magnetic resonance imaging (fMRI) representational dataset with $T=6$ time points and $d=3$ features, organized as a matrix $X \\in \\mathbb{R}^{T \\times d}$, where each row is a multivoxel pattern at a time point. The time points are partitioned into three stimulus conditions, labeled $1$, $2$, and $3$, with two repetitions per condition. Let the data and labels be given by\n$$\nX = \\begin{pmatrix}\n1  2  0 \\\\\n0  1  1 \\\\\n2  0  1 \\\\\n3  1  -1 \\\\\n-1  2  2 \\\\\n4  -1  2\n\\end{pmatrix}, \\quad c = \\begin{pmatrix}\n1,\\, 2,\\, 3,\\, 1,\\, 2,\\, 3\n\\end{pmatrix},\n$$\nwhere $c_t \\in \\{1,2,3\\}$ is the condition label at time $t$. In hyperalignment, subject-specific multivariate patterns are aligned via orthogonal transformations to preserve representational geometry across subjects. In this problem, we examine the invariance of the Representational Dissimilarity Matrix (RDM) under such orthogonal rotations.\n\nPerform the following steps:\n\n1. For each condition $i \\in \\{1,2,3\\}$, compute the condition-specific pattern vector $\\mu_i \\in \\mathbb{R}^{3}$ as the arithmetic mean of the rows of $X$ whose label equals $i$.\n\n2. Construct the $3 \\times 3$ Representational Dissimilarity Matrix (RDM), denoted $R$, where the off-diagonal entries are $R_{ij} = \\|\\mu_i - \\mu_j\\|_2^2$ for $i \\neq j$, and the diagonal entries are $R_{ii} = 0$. Here $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n\n3. Let $Q \\in \\mathbb{R}^{3 \\times 3}$ be the orthogonal rotation\n$$\nQ = \\begin{pmatrix}\n0  -1  0 \\\\\n1  0  0 \\\\\n0  0  1\n\\end{pmatrix},\n$$\nwhich satisfies $Q^{\\top}Q = I$. Compute the rotated condition vectors $\\nu_i = Q \\mu_i$, and construct the rotated RDM $R'$ with entries $R'_{ij} = \\|\\nu_i - \\nu_j\\|_2^2$ for $i \\neq j$ and $R'_{ii} = 0$.\n\n4. Using only the definitions of the Euclidean norm and orthogonal matrices, determine whether $R$ and $R'$ must be identical.\n\n5. Compute the Frobenius norm of the difference between the two RDMs, $\\|R - R'\\|_F$, where $\\|A\\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$ for any matrix $A$. Express your final answer as a single real number. No rounding is required.",
            "solution": "The problem requires us to analyze the effect of an orthogonal rotation on a Representational Dissimilarity Matrix (RDM) derived from a simplified fMRI dataset. We are given the data matrix $X \\in \\mathbb{R}^{T \\times d}$ with $T=6$ and $d=3$, and a vector of condition labels $c$.\n$$\nX = \\begin{pmatrix}\n1  2  0 \\\\\n0  1  1 \\\\\n2  0  1 \\\\\n3  1  -1 \\\\\n-1  2  2 \\\\\n4  -1  2\n\\end{pmatrix}, \\quad c = \\begin{pmatrix}\n1  2  3  1  2  3\n\\end{pmatrix}\n$$\n\nWe will proceed through the specified steps.\n\n**Step 1: Compute condition-specific pattern vectors $\\mu_i$**\n\nThe vector $\\mu_i$ for condition $i \\in \\{1, 2, 3\\}$ is the arithmetic mean of the rows of $X$ corresponding to that condition.\n\nFor condition $i=1$, the relevant rows are the $1$-st and $4$-th rows of $X$:\n$$\n\\mu_1 = \\frac{1}{2} \\left( \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} 4 \\\\ 3 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3/2 \\\\ -1/2 \\end{pmatrix}\n$$\n\nFor condition $i=2$, the relevant rows are the $2$-nd and $5$-th rows of $X$:\n$$\n\\mu_2 = \\frac{1}{2} \\left( \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 2 \\\\ 2 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} -1 \\\\ 3 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 3/2 \\\\ 3/2 \\end{pmatrix}\n$$\n\nFor condition $i=3$, the relevant rows are the $3$-rd and $6$-th rows of $X$:\n$$\n\\mu_3 = \\frac{1}{2} \\left( \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 4 \\\\ -1 \\\\ 2 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} 6 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1/2 \\\\ 3/2 \\end{pmatrix}\n$$\n\n**Step 2: Construct the Representational Dissimilarity Matrix $R$**\n\nThe RDM $R$ is a $3 \\times 3$ matrix with entries $R_{ij} = \\|\\mu_i - \\mu_j\\|_2^2$ for $i \\neq j$ and $R_{ii} = 0$.\n\nThe diagonal entries are $R_{11} = R_{22} = R_{33} = 0$.\nFor the off-diagonal entries:\n$R_{12} = \\|\\mu_1 - \\mu_2\\|_2^2$:\n$$\n\\mu_1 - \\mu_2 = \\begin{pmatrix} 2 \\\\ 3/2 \\\\ -1/2 \\end{pmatrix} - \\begin{pmatrix} -1/2 \\\\ 3/2 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} 5/2 \\\\ 0 \\\\ -2 \\end{pmatrix}\n$$\n$$\nR_{12} = \\left(\\frac{5}{2}\\right)^2 + 0^2 + (-2)^2 = \\frac{25}{4} + 4 = \\frac{25+16}{4} = \\frac{41}{4}\n$$\n\n$R_{13} = \\|\\mu_1 - \\mu_3\\|_2^2$:\n$$\n\\mu_1 - \\mu_3 = \\begin{pmatrix} 2 \\\\ 3/2 \\\\ -1/2 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ -1/2 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 2 \\\\ -2 \\end{pmatrix}\n$$\n$$\nR_{13} = (-1)^2 + 2^2 + (-2)^2 = 1 + 4 + 4 = 9\n$$\n\n$R_{23} = \\|\\mu_2 - \\mu_3\\|_2^2$:\n$$\n\\mu_2 - \\mu_3 = \\begin{pmatrix} -1/2 \\\\ 3/2 \\\\ 3/2 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ -1/2 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} -7/2 \\\\ 2 \\\\ 0 \\end{pmatrix}\n$$\n$$\nR_{23} = \\left(-\\frac{7}{2}\\right)^2 + 2^2 + 0^2 = \\frac{49}{4} + 4 = \\frac{49+16}{4} = \\frac{65}{4}\n$$\n\nSince the squared Euclidean distance is symmetric, $R_{ij} = R_{ji}$. The complete RDM is:\n$$\nR = \\begin{pmatrix} 0  \\frac{41}{4}  9 \\\\ \\frac{41}{4}  0  \\frac{65}{4} \\\\ 9  \\frac{65}{4}  0 \\end{pmatrix}\n$$\n\n**Step 3: Compute rotated vectors $\\nu_i$ and RDM $R'$**\n\nThe orthogonal rotation matrix is $Q = \\begin{pmatrix} 0  -1  0 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix}$. We compute $\\nu_i = Q \\mu_i$.\n$$\n\\nu_1 = Q \\mu_1 = \\begin{pmatrix} 0  -1  0 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3/2 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} -3/2 \\\\ 2 \\\\ -1/2 \\end{pmatrix}\n$$\n$$\n\\nu_2 = Q \\mu_2 = \\begin{pmatrix} 0  -1  0 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} -1/2 \\\\ 3/2 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} -3/2 \\\\ -1/2 \\\\ 3/2 \\end{pmatrix}\n$$\n$$\n\\nu_3 = Q \\mu_3 = \\begin{pmatrix} 0  -1  0 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1/2 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 3 \\\\ 3/2 \\end{pmatrix}\n$$\nNow we construct the rotated RDM $R'$ with entries $R'_{ij} = \\|\\nu_i - \\nu_j\\|_2^2$.\n$R'_{12} = \\|\\nu_1 - \\nu_2\\|_2^2$:\n$$\n\\nu_1 - \\nu_2 = \\begin{pmatrix} -3/2 \\\\ 2 \\\\ -1/2 \\end{pmatrix} - \\begin{pmatrix} -3/2 \\\\ -1/2 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 5/2 \\\\ -2 \\end{pmatrix}\n$$\n$$\nR'_{12} = 0^2 + \\left(\\frac{5}{2}\\right)^2 + (-2)^2 = \\frac{25}{4} + 4 = \\frac{41}{4}\n$$\nThis is identical to $R_{12}$.\n\n$R'_{13} = \\|\\nu_1 - \\nu_3\\|_2^2$:\n$$\n\\nu_1 - \\nu_3 = \\begin{pmatrix} -3/2 \\\\ 2 \\\\ -1/2 \\end{pmatrix} - \\begin{pmatrix} 1/2 \\\\ 3 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1 \\\\ -2 \\end{pmatrix}\n$$\n$$\nR'_{13} = (-2)^2 + (-1)^2 + (-2)^2 = 4 + 1 + 4 = 9\n$$\nThis is identical to $R_{13}$.\n\n$R'_{23} = \\|\\nu_2 - \\nu_3\\|_2^2$:\n$$\n\\nu_2 - \\nu_3 = \\begin{pmatrix} -3/2 \\\\ -1/2 \\\\ 3/2 \\end{pmatrix} - \\begin{pmatrix} 1/2 \\\\ 3 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -7/2 \\\\ 0 \\end{pmatrix}\n$$\n$$\nR'_{23} = (-2)^2 + \\left(-\\frac{7}{2}\\right)^2 + 0^2 = 4 + \\frac{49}{4} = \\frac{16+49}{4} = \\frac{65}{4}\n$$\nThis is identical to $R_{23}$.\nThe resulting RDM $R'$ is identical to $R$:\n$$\nR' = \\begin{pmatrix} 0  \\frac{41}{4}  9 \\\\ \\frac{41}{4}  0  \\frac{65}{4} \\\\ 9  \\frac{65}{4}  0 \\end{pmatrix}\n$$\n\n**Step 4: Determine whether $R$ and $R'$ must be identical**\n\nYes, $R$ and $R'$ must be identical. This is a general property of orthogonal transformations.\nLet $v$ be any vector in $\\mathbb{R}^d$. The squared Euclidean norm is $\\|v\\|_2^2 = v^\\top v$.\nConsider the squared norm of a vector $v$ transformed by an orthogonal matrix $Q$:\n$$\n\\|Qv\\|_2^2 = (Qv)^\\top (Qv)\n$$\nUsing the transpose property $(AB)^\\top = B^\\top A^\\top$, we have:\n$$\n\\|Qv\\|_2^2 = v^\\top Q^\\top Q v\n$$\nBy definition, an orthogonal matrix $Q$ satisfies $Q^\\top Q = I$, where $I$ is the identity matrix. Substituting this into the equation:\n$$\n\\|Qv\\|_2^2 = v^\\top I v = v^\\top v = \\|v\\|_2^2\n$$\nThis shows that orthogonal transformations preserve the norm of any vector. This property is known as isometry.\n\nNow, consider the entries of the RDMs. For any $i \\neq j$:\n$$\nR_{ij} = \\|\\mu_i - \\mu_j\\|_2^2\n$$\n$$\nR'_{ij} = \\|\\nu_i - \\nu_j\\|_2^2 = \\|Q\\mu_i - Q\\mu_j\\|_2^2\n$$\nUsing the linearity of matrix multiplication, $Q\\mu_i - Q\\mu_j = Q(\\mu_i - \\mu_j)$. Let the difference vector be $v_{ij} = \\mu_i - \\mu_j$.\nThen $R'_{ij} = \\|Q v_{ij}\\|_2^2$.\nFrom our proof above, $\\|Q v_{ij}\\|_2^2 = \\|v_{ij}\\|_2^2$.\nTherefore,\n$$\nR'_{ij} = \\|v_{ij}\\|_2^2 = \\|\\mu_i - \\mu_j\\|_2^2 = R_{ij}\n$$\nThe diagonal entries are $R_{ii} = 0$ and $R'_{ii} = 0$, which are also equal. Since all corresponding entries of the matrices $R$ and $R'$ are equal for any choice of vectors $\\mu_i$ and any orthogonal matrix $Q$, the matrices must be identical, $R = R'$.\n\n**Step 5: Compute the Frobenius norm of the difference, $\\|R - R'\\|_F$**\n\nBased on the theoretical argument in Step 4 and confirmed by the explicit calculations in Steps 2 and 3, we have established that $R = R'$.\nThe difference matrix is therefore the zero matrix:\n$$\nR - R' = \\begin{pmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\nThe Frobenius norm of a matrix $A$ is defined as $\\|A\\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$. For the zero matrix, all elements are $0$, so their squares are also $0$.\n$$\n\\|R - R'\\|_F = \\sqrt{0^2 + 0^2 + \\dots + 0^2} = \\sqrt{0} = 0\n$$\nThe Frobenius norm of the difference between the two RDMs is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Having established that orthogonal transformations preserve representational geometry, we now explore the nature of the alignment solution itself in an idealized setting. This thought experiment models one subject's brain activity as a simple permutation of another's, a scenario that allows for perfect alignment. By reasoning through the implications, you will develop a deeper intuition for the orthogonal Procrustes problem and the critical role of mathematical constraints, such as restricting the transformation to be a pure rotation.",
            "id": "4168877",
            "problem": "Consider two subjects observed with Functional Magnetic Resonance Imaging (fMRI) during the same time-locked naturalistic stimulus protocol. For subject $1$, let the time-by-voxel data matrix be $X \\in \\mathbb{R}^{T \\times V}$, where $T$ is the number of time points and $V$ is the number of voxels. Assume that the columns of $X$ are linearly independent and in general position (no repeated or zero columns). For subject $2$, suppose the same underlying functional representation is measured, but the voxel ordering is permuted by an even permutation $\\pi$ of $\\{1,\\dots,V\\}$, yielding $Y = X P$, where $P \\in \\mathbb{R}^{V \\times V}$ is the permutation matrix associated with $\\pi$. Recall that a permutation matrix $P$ satisfies $P^\\top P = I$ and, if $\\pi$ is even, $\\det(P) = +1$, so $P \\in \\mathrm{SO}(V)$, the Special Orthogonal Group.\n\nHyperalignment can be modeled as an orthogonal Procrustes problem: find $R \\in \\mathbb{R}^{V \\times V}$ that minimizes the objective $\\lVert Y R - X \\rVert_F^2$ subject to $R^\\top R = I$, and optionally with the constraint $\\det(R) = +1$. Here $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm. The representational geometry across voxels can be characterized by the Gram matrix $G_X = X^\\top X$, which encodes inner products of voxel response profiles over time.\n\nUsing only basic properties of orthogonal matrices, permutation matrices, Euclidean norms, and inner products, reason about the following statements in the context above. Select all statements that are correct.\n\nA. In this scenario with $Y = X P$ and $\\pi$ even, there exists $R \\in \\mathrm{SO}(V)$ that achieves exact alignment with zero error, specifically $R = P^\\top$, which yields $Y R = X$ and $\\lVert Y R - X \\rVert_F^2 = 0$.\n\nB. For any permutation (even or odd), the pairwise Euclidean distances between the rows of $X$ (time-point vectors in voxel space) are identical to those between the rows of $Y$, because right-multiplication by any permutation matrix acts as an orthogonal transformation on the voxel coordinates and preserves Euclidean distances.\n\nC. If the hyperalignment transformation is constrained to the Special Orthogonal Group $\\mathrm{SO}(V)$ and the voxel permutation between subjects is odd (so $\\det(P) = -1$), then exact recovery is impossible under the stated assumptions; the minimum alignment error is strictly positive.\n\nD. The Gram matrix $G_X = X^\\top X$ changes under column permutations in a way that destroys shared geometry; therefore, shared geometry is not preserved across subjects when voxel orderings are permuted.\n\nE. In the setting $Y = X P$ with full column rank $X$, the orthogonal Procrustes solution $R$ is unique and equal to $P^\\top$ regardless of whether the determinant constraint $\\det(R) = +1$ is enforced.\n\nChoose the correct option(s).",
            "solution": "The user wants a critical validation of the problem statement and, if valid, a full solution with an analysis of each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Data for subject 1: $X \\in \\mathbb{R}^{T \\times V}$ ($T$ time points, $V$ voxels).\n- The columns of $X$ are linearly independent and in general position.\n- Data for subject 2: $Y = X P$, where $P \\in \\mathbb{R}^{V \\times V}$ is the permutation matrix for an even permutation $\\pi$.\n- Properties of the permutation matrix $P$: $P^\\top P = I$ and $\\det(P) = +1$, so $P \\in \\mathrm{SO}(V)$ (Special Orthogonal Group).\n- Hyperalignment objective: Minimize $\\lVert Y R - X \\rVert_F^2$ with respect to $R \\in \\mathbb{R}^{V \\times V}$.\n- Constraint on $R$: $R^\\top R = I$ (Orthogonal Group, $\\mathrm{O}(V)$).\n- Optional constraint on $R$: $\\det(R) = +1$ (Special Orthogonal Group, $\\mathrm{SO}(V)$).\n- Definition of Gram matrix: $G_X = X^\\top X$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses the orthogonal Procrustes framework to model hyperalignment of fMRI data, which is a standard and well-established method in computational neuroscience. The variables $X$, $Y$, $P$, $R$ and the objective function are defined consistently with the scientific literature on this topic.\n- **Well-Posed:** The problem is mathematically well-defined. It poses a clear optimization problem with specified matrices and constraints. The assumption that the columns of $X$ are linearly independent ensures that the matrix $X^\\top X$ is invertible, which guarantees a unique solution for the Procrustes problem.\n- **Objective:** The problem statement is expressed in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically grounded, well-posed, and objective. There are no identifiable flaws. I will proceed with the solution derivation and option analysis.\n\n### Solution Derivation\n\nThe core of the problem is the minimization of the objective function $f(R) = \\lVert YR - X \\rVert_F^2$ subject to orthogonality constraints on $R$. We are given the idealized relationship $Y = XP$, where $P$ is a permutation matrix.\n\nLet's substitute $Y=XP$ into the objective function:\n$$ \\lVert YR - X \\rVert_F^2 = \\lVert (XP)R - X \\rVert_F^2 $$\nThe Frobenius norm is induced by the trace inner product, $\\lVert A \\rVert_F^2 = \\text{tr}(A^\\top A)$. Its minimum possible value is $0$, which occurs if and only if the argument is the zero matrix. We seek an orthogonal matrix $R$ that makes the error zero, if possible.\n$$ (XP)R - X = 0 $$\n$$ X(PR - I) = 0 $$\nwhere $I$ is the $V \\times V$ identity matrix.\nThe problem states that the columns of $X$ are linearly independent. This means that if $X A = 0$ for some matrix $A \\in \\mathbb{R}^{V \\times V}$, then it must be that $A=0$. In our case, this implies we need:\n$$ PR - I = 0 \\implies PR = I $$\nSince $P$ is a permutation matrix, it is orthogonal, so its inverse is its transpose: $P^{-1} = P^\\top$.\n$$ R = P^{-1} = P^\\top $$\nThis shows that if we choose $R = P^\\top$, the alignment error is exactly zero:\n$$ \\lVert Y(P^\\top) - X \\rVert_F^2 = \\lVert (XP)P^\\top - X \\rVert_F^2 = \\lVert X(PP^\\top) - X \\rVert_F^2 = \\lVert XI - X \\rVert_F^2 = \\lVert 0 \\rVert_F^2 = 0 $$\nSince $P$ is orthogonal, $P^\\top$ is also orthogonal, so $R = P^\\top$ is a valid solution for the unconstrained orthogonal Procrustes problem (where $R \\in \\mathrm{O}(V)$). As the minimum possible error is $0$, this is the optimal solution.\n\nThe uniqueness of the solution to the orthogonal Procrustes problem $\\min_{R^\\top R=I} \\lVert AR-B \\rVert_F^2$ is guaranteed if the matrix $B^\\top A$ is of full rank. In our case, this matrix is $X^\\top Y = X^\\top (XP) = (X^\\top X)P$. Since the columns of $X$ are linearly independent, $X^\\top X$ is positive definite and thus invertible. $P$ is also invertible. Their product $(X^\\top X)P$ is therefore invertible (full rank), and the solution is unique.\n\nWe will now use these findings to evaluate each statement.\n\n### Option-by-Option Analysis\n\n**A. In this scenario with $Y = X P$ and $\\pi$ even, there exists $R \\in \\mathrm{SO}(V)$ that achieves exact alignment with zero error, specifically $R = P^\\top$, which yields $Y R = X$ and $\\lVert Y R - X \\rVert_F^2 = 0$.**\n\nAs derived above, the choice $R = P^\\top$ results in zero alignment error: $\\lVert Y P^\\top - X \\rVert_F^2 = 0$, which means $Y P^\\top = X$.\nThe statement requires that this solution $R$ belongs to the Special Orthogonal Group, $\\mathrm{SO}(V)$. This means two conditions must be met:\n1. $R$ must be orthogonal: $R^\\top R = I$.\n2. The determinant of $R$ must be $+1$: $\\det(R) = +1$.\n\nFor $R = P^\\top$:\n1. Since $P$ is a permutation matrix, it is orthogonal ($P^\\top P = I$). Its transpose $R=P^\\top$ is also orthogonal, since $R^\\top R = (P^\\top)^\\top P^\\top = P P^\\top = I$.\n2. The determinant of a transpose of a matrix is equal to the determinant of the matrix itself: $\\det(R) = \\det(P^\\top) = \\det(P)$. The problem states that the permutation $\\pi$ is even, for which the corresponding permutation matrix $P$ has $\\det(P) = +1$. Therefore, $\\det(R) = +1$.\n\nSince $R = P^\\top$ is orthogonal and has a determinant of $+1$, it lies in $\\mathrm{SO}(V)$. All parts of the statement are true.\n**Verdict: Correct.**\n\n**B. For any permutation (even or odd), the pairwise Euclidean distances between the rows of $X$ (time-point vectors in voxel space) are identical to those between the rows of $Y$, because right-multiplication by any permutation matrix acts as an orthogonal transformation on the voxel coordinates and preserves Euclidean distances.**\n\nLet $x_i^\\top$ and $x_j^\\top$ be the $i$-th and $j$-th rows of matrix $X$, respectively. These are vectors in $\\mathbb{R}^{1 \\times V}$. The squared Euclidean distance between these two time points in the voxel space is $\\lVert x_i^\\top - x_j^\\top \\rVert_2^2$.\nThe corresponding rows of $Y = XP$ are $y_i^\\top = x_i^\\top P$ and $y_j^\\top = x_j^\\top P$.\nThe squared Euclidean distance for subject 2 is:\n$$ \\lVert y_i^\\top - y_j^\\top \\rVert_2^2 = \\lVert x_i^\\top P - x_j^\\top P \\rVert_2^2 = \\lVert (x_i^\\top - x_j^\\top)P \\rVert_2^2 $$\nLet $v^\\top = x_i^\\top - x_j^\\top$. The squared norm is $\\lVert v^\\top P \\rVert_2^2 = (v^\\top P)(v^\\top P)^\\top = v^\\top P P^\\top v$.\nAny permutation matrix $P$ (for both even and odd permutations) is orthogonal, meaning $P P^\\top = I$.\n$$ v^\\top P P^\\top v = v^\\top I v = v^\\top v = \\lVert v^\\top \\rVert_2^2 $$\nThus, $\\lVert y_i^\\top - y_j^\\top \\rVert_2^2 = \\lVert x_i^\\top - x_j^\\top \\rVert_2^2$. The Euclidean distances are preserved. The reasoning provided in the statement is also correct: right-multiplication by an orthogonal matrix ($P$) preserves the Euclidean norm.\n**Verdict: Correct.**\n\n**C. If the hyperalignment transformation is constrained to the Special Orthogonal Group $\\mathrm{SO}(V)$ and the voxel permutation between subjects is odd (so $\\det(P) = -1$), then exact recovery is impossible under the stated assumptions; the minimum alignment error is strictly positive.**\n\nIn this scenario, we have $Y=XP$ with $\\det(P)=-1$. We seek to minimize $\\lVert YR - X \\rVert_F^2$ subject to $R \\in \\mathrm{SO}(V)$.\nAs shown in the initial derivation, zero error ($\\lVert YR - X \\rVert_F^2 = 0$) is achieved if and only if $R = P^\\top$.\nLet's check if this optimal solution $R=P^\\top$ satisfies the constraint $R \\in \\mathrm{SO}(V)$.\nThe matrix $P^\\top$ is orthogonal. However, its determinant is $\\det(P^\\top) = \\det(P) = -1$.\nSince $\\det(P^\\top) = -1$, the matrix $R=P^\\top$ is in $\\mathrm{O}(V)$ but not in $\\mathrm{SO}(V)$.\nThe constraint requires the solution $R$ to be in $\\mathrm{SO}(V)$, so the one and only transformation that gives zero error, $R=P^\\top$, is forbidden.\nAny other transformation $R' \\in \\mathrm{SO}(V)$ will not be equal to $P^\\top$. For any $R' \\neq P^\\top$, we have $PR' \\neq I$.\nSince the columns of $X$ are linearly independent, $X(PR' - I) \\neq 0$.\nTherefore, for any $R' \\in \\mathrm{SO}(V)$, the error $\\lVert YR' - X \\rVert_F^2 = \\lVert X(PR' - I) \\rVert_F^2$ must be strictly greater than $0$.\nSo, exact recovery is impossible, and the minimum error is strictly positive.\n**Verdict: Correct.**\n\n**D. The Gram matrix $G_X = X^\\top X$ changes under column permutations in a way that destroys shared geometry; therefore, shared geometry is not preserved across subjects when voxel orderings are permuted.**\n\nLet's compute the Gram matrix for subject 2, $G_Y = Y^\\top Y$.\n$$ G_Y = (XP)^\\top(XP) = P^\\top X^\\top X P = P^\\top G_X P $$\nThe matrix $G_Y$ is related to $G_X$ by a similarity transformation via the orthogonal matrix $P$. Such a transformation preserves fundamental geometric properties, most notably the eigenvalues. The set of pairwise inner products between voxel response profiles is preserved, even though their arrangement in the matrix is permuted. Specifically, if the columns of $Y$ are $y_k$ and columns of $X$ are $x_j$, with $y_k = x_{\\pi(k)}$, then the $(i, j)$ entry of $G_Y$ is $y_i^\\top y_j = x_{\\pi(i)}^\\top x_{\\pi(j)}$, which is the $(\\pi(i), \\pi(j))$ entry of $G_X$. This is a relabeling of the voxel indices, not a destruction of the geometry. The core assumption of many alignment methods is precisely that this geometry is preserved and can be recovered. The statement is fundamentally incorrect.\n**Verdict: Incorrect.**\n\n**E. In the setting $Y = X P$ with full column rank $X$, the orthogonal Procrustes solution $R$ is unique and equal to $P^\\top$ regardless of whether the determinant constraint $\\det(R) = +1$ is enforced.**\n\nThis statement claims that the solution is $R = P^\\top$ for both the unconstrained orthogonal problem ($R \\in \\mathrm{O}(V)$) and the constrained special orthogonal problem ($R \\in \\mathrm{SO}(V)$).\nLet's analyze this claim.\nCase 1: Unconstrained problem ($R \\in \\mathrm{O}(V)$). As established, the unique solution minimizing $\\lVert YR - X \\rVert_F^2$ is indeed $R = P^\\top$.\nCase 2: Constrained problem ($R \\in \\mathrm{SO}(V)$). The solution must have $\\det(R)=+1$.\n\nConsider what happens if the permutation $P$ is odd, so $\\det(P)=-1$.\nThe unconstrained solution is $R = P^\\top$. Its determinant is $\\det(P^\\top) = \\det(P) = -1$.\nThe constrained solution must have a determinant of $+1$. Therefore, the constrained solution cannot be $P^\\top$.\nIn this case, the solution to the unconstrained problem is different from the solution to the constrained problem. The statement \"regardless of whether the determinant constraint is enforced\" is therefore false. It takes only one counterexample to falsify this universal claim, and the case of an odd permutation provides it.\n**Verdict: Incorrect.**\n\nFinal correct statements are A, B, and C.",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "While the orthogonal Procrustes model provides a powerful theoretical framework, real-world fMRI data is inevitably noisy and ill-conditioned, which can lead to unstable alignment solutions. To address this, a common practical approach is to introduce ridge regularization, which trades a small amount of bias for a large reduction in variance. This practice guides you through the derivation of the regularized hyperalignment solution, revealing how and why it deviates from a perfect orthogonal transformation and forcing you to consider the trade-offs inherent in analyzing noisy data.",
            "id": "4168818",
            "problem": "In hyperalignment of functional brain representations, one seeks to align subject-specific multivariate time series into a common representational space so that homologous stimuli evoke similar response patterns across subjects. Consider two subjects observed with Functional Magnetic Resonance Imaging (fMRI), producing time-aligned response matrices $X \\in \\mathbb{R}^{T \\times d}$ and $Y \\in \\mathbb{R}^{T \\times d}$, where $T$ is the number of time points and $d$ is the number of features (for example, voxels within a parcel or principal components). Hyperalignment often models between-subject alignment as a linear map $R \\in \\mathbb{R}^{d \\times d}$ from $X$ to $Y$. \n\nStarting from the standard least-squares alignment principle, the alignment map $R$ is chosen to minimize the sum of squared discrepancies between $X R$ and $Y$ measured by the Frobenius norm. In the presence of noise and ill-conditioning, a quadratic penalty on the energy of $R$ is introduced (ridge regularization). Using the fundamental definition of least-squares with a Frobenius norm and the principle of Tikhonov (ridge) regularization, derive the ridge-regularized Procrustes objective of the form\n$$\n\\min_{R \\in \\mathbb{R}^{d \\times d}} \\ \\|X R - Y\\|_{F}^{2} + \\lambda \\|R\\|_{F}^{2},\n$$\nwhere $\\lambda  0$ is the regularization parameter, and show from first principles of matrix calculus how its unconstrained global minimizer $R^{\\star}$ deviates from orthogonality (i.e., how $R^{\\star \\top} R^{\\star} \\neq I$ in general). Explain, based on the singular value structure of $X$ and the effect of the ridge penalty, the advantages and disadvantages of this deviation for noisy data.\n\nThen, for a concrete two-dimensional toy hyperalignment instance with $T = 3$ and $d = 2$, let\n$$\nX = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1\n\\end{pmatrix},\n\\qquad\nY = \\begin{pmatrix}\n1  1 \\\\\n1  0 \\\\\n2  1\n\\end{pmatrix},\n$$\nand choose $\\lambda = 1$. Compute the global minimizer $R^{\\star}$ and quantify its deviation from orthogonality by the Frobenius-norm discrepancy\n$$\n\\Delta \\equiv \\|R^{\\star \\top} R^{\\star} - I\\|_{F}.\n$$\nExpress your final $\\Delta$ as an exact closed-form analytic expression; do not approximate or round. No physical units are required, since $\\Delta$ is dimensionless.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is based on established principles of linear algebra, matrix calculus, and regularization theory as applied to a standard problem in computational neuroscience. All necessary data and definitions are provided, and there are no internal contradictions. The problem is therefore deemed valid and a solution can be constructed.\n\nThe problem asks for two main parts: first, a general derivation and analysis of the ridge-regularized Procrustes objective, and second, a specific calculation for a toy example.\n\n**Part 1: General Derivation and Analysis**\n\nThe objective function to be minimized is given by\n$$\nL(R) = \\|X R - Y\\|_{F}^{2} + \\lambda \\|R\\|_{F}^{2}\n$$\nwhere $X \\in \\mathbb{R}^{T \\times d}$, $Y \\in \\mathbb{R}^{T \\times d}$, $R \\in \\mathbb{R}^{d \\times d}$, and $\\lambda  0$. The Frobenius norm of a matrix $A$ is defined as $\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}$, and its square can be conveniently expressed using the trace operator: $\\|A\\|_{F}^{2} = \\text{Tr}(A^{\\top} A)$.\n\nApplying this property to the objective function, we have:\n$$\nL(R) = \\text{Tr}((X R - Y)^{\\top}(X R - Y)) + \\lambda \\text{Tr}(R^{\\top}R)\n$$\nWe expand the first term:\n$$\nL(R) = \\text{Tr}((R^{\\top}X^{\\top} - Y^{\\top})(X R - Y)) + \\lambda \\text{Tr}(R^{\\top}R)\n$$\n$$\nL(R) = \\text{Tr}(R^{\\top}X^{\\top}XR - R^{\\top}X^{\\top}Y - Y^{\\top}XR + Y^{\\top}Y) + \\lambda \\text{Tr}(R^{\\top}R)\n$$\nUsing the linearity of the trace operator, $\\text{Tr}(A+B) = \\text{Tr}(A) + \\text{Tr}(B)$, and its cyclic property, $\\text{Tr}(ABC) = \\text{Tr}(CAB)$, we note that $\\text{Tr}(Y^{\\top}XR) = \\text{Tr}(R Y^{\\top}X)$. Also, since the trace of a scalar is the scalar itself, and the trace of a matrix is equal to the trace of its transpose, $\\text{Tr}(A) = \\text{Tr}(A^\\top)$, we have $\\text{Tr}(Y^\\top X R) = \\text{Tr}((Y^\\top X R)^\\top) = \\text{Tr}(R^\\top X^\\top Y)$. Thus, the two cross-terms are identical. The objective function becomes:\n$$\nL(R) = \\text{Tr}(R^{\\top}X^{\\top}XR) - 2\\text{Tr}(R^{\\top}X^{\\top}Y) + \\text{Tr}(Y^{\\top}Y) + \\lambda \\text{Tr}(R^{\\top}R)\n$$\nTo find the minimizer $R^{\\star}$, we compute the gradient of $L(R)$ with respect to $R$ and set it to the zero matrix. Using standard rules of matrix calculus:\n$$\n\\frac{\\partial}{\\partial R} \\text{Tr}(R^{\\top}A R B) = A^{\\top} R B^{\\top} + A R B \\quad (\\text{here, } A=X^\\top X, B=I) \\implies \\frac{\\partial}{\\partial R} \\text{Tr}(R^{\\top}X^{\\top}XR) = 2 X^{\\top}XR\n$$\n$$\n\\frac{\\partial}{\\partial R} \\text{Tr}(R^{\\top}C) = C \\quad (\\text{here, } C=X^\\top Y) \\implies \\frac{\\partial}{\\partial R} \\text{Tr}(R^{\\top}X^{\\top}Y) = X^{\\top}Y\n$$\n$$\n\\frac{\\partial}{\\partial R} \\text{Tr}(R^{\\top}R) = 2R\n$$\n$$\n\\frac{\\partial}{\\partial R} \\text{Tr}(Y^{\\top}Y) = 0\n$$\nThe gradient of $L(R)$ is therefore:\n$$\n\\nabla_R L(R) = 2X^{\\top}XR - 2X^{\\top}Y + 2\\lambda R\n$$\nSetting the gradient to zero to find the optimal $R^{\\star}$:\n$$\n2X^{\\top}XR^{\\star} - 2X^{\\top}Y + 2\\lambda R^{\\star} = 0\n$$\n$$\nX^{\\top}XR^{\\star} + \\lambda R^{\\star} = X^{\\top}Y\n$$\n$$\n(X^{\\top}X + \\lambda I)R^{\\star} = X^{\\top}Y\n$$\nThe matrix $X^{\\top}X$ is positive semi-definite. Since $\\lambda  0$, the matrix $\\lambda I$ is positive definite. The sum $(X^{\\top}X + \\lambda I)$ is therefore positive definite and, consequently, invertible. We can solve for $R^{\\star}$:\n$$\nR^{\\star} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top}Y\n$$\nThis is the unique global minimizer, as the objective function is strictly convex.\n\nNow we analyze its deviation from orthogonality. An orthogonal matrix $R_o$ satisfies $R_o^{\\top}R_o = I$. The derived solution $R^{\\star}$ is not, in general, orthogonal. The term $\\lambda \\|R\\|_F^2$ penalizes solutions with a large Frobenius norm. Since $\\|R\\|_F^2 = \\text{Tr}(R^\\top R) = \\sum_i \\sigma_i^2$, where $\\sigma_i$ are the singular values of $R$, this penalty \"shrinks\" the singular values of the solution towards zero. An orthogonal matrix has all its singular values equal to $1$. The regularization term pulls the solution away from this constraint, resulting in a transformation that is not a pure rotation/reflection but includes scaling.\n\nThe **advantages** of this deviation stem from the stabilization of the solution. The matrix $X$ from real data is often ill-conditioned, meaning its columns are highly correlated. This makes $X^{\\top}X$ nearly singular, with some very small eigenvalues. In the unregularized case ($\\lambda=0$), the solution $R_0 = (X^{\\top}X)^{-1}X^{\\top}Y$ would involve inverting this ill-conditioned matrix. The small eigenvalues of $X^{\\top}X$ would become very large eigenvalues in its inverse, amplifying any noise present in $Y$. The solution $R_0$ would be highly unstable and overfit the data. By adding $\\lambda I$, we add the positive constant $\\lambda$ to each eigenvalue of $X^{\\top}X$ before inversion. If $\\sigma_i^2$ are the eigenvalues of $X^\\top X$, the eigenvalues of $(X^\\top X + \\lambda I)^{-1}$ are $1/(\\sigma_i^2 + \\lambda)$. This prevents the denominator from being close to zero, stabilizing the inverse and thus the solution $R^{\\star}$. This is a classic bias-variance trade-off: we introduce a bias (the solution is no longer the true least-squares fit) to reduce the variance (the solution is more robust to noise).\n\nThe **disadvantages** are that the resulting transformation $R^{\\star}$ is biased and not an isometry. It shrinks the data, particularly along directions of low variance in $X$ (corresponding to small eigenvalues of $X^\\top X$). If the underlying scientific hypothesis is that brain representations are related by a pure rotation (an isometry), then this shrinkage distorts the geometric relationships, which may be undesirable. The regularization might attenuate weak but meaningful signals that happen to lie in low-variance directions.\n\n**Part 2: Concrete Calculation**\n\nWe are given $T=3$, $d=2$, $\\lambda=1$, and the matrices:\n$$\nX = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix}, \\qquad Y = \\begin{pmatrix} 1  1 \\\\ 1  0 \\\\ 2  1 \\end{pmatrix}\n$$\nFirst, we compute $X^{\\top}X$ and $X^{\\top}Y$:\n$$\nX^{\\top}X = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 1+0+1  0+0+1 \\\\ 0+0+1  0+1+1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}\n$$\n$$\nX^{\\top}Y = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  0 \\\\ 2  1 \\end{pmatrix} = \\begin{pmatrix} 1+0+2  1+0+1 \\\\ 0+1+2  0+0+1 \\end{pmatrix} = \\begin{pmatrix} 3  2 \\\\ 3  1 \\end{pmatrix}\n$$\nNext, we compute $R^{\\star} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top}Y$ with $\\lambda=1$:\n$$\nX^{\\top}X + \\lambda I = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} + 1 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 3  1 \\\\ 1  3 \\end{pmatrix}\n$$\nThe inverse of this matrix is:\n$$\n(X^{\\top}X + \\lambda I)^{-1} = \\frac{1}{3 \\cdot 3 - 1 \\cdot 1} \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix}\n$$\nNow, we can find $R^{\\star}$:\n$$\nR^{\\star} = \\frac{1}{8} \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix} \\begin{pmatrix} 3  2 \\\\ 3  1 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 9-3  6-1 \\\\ -3+9  -2+3 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 6  5 \\\\ 6  1 \\end{pmatrix}\n$$\nTo quantify the deviation from orthogonality, we compute $\\Delta = \\|R^{\\star \\top} R^{\\star} - I\\|_{F}$. First, we calculate $R^{\\star \\top} R^{\\star}$:\n$$\nR^{\\star \\top} = \\frac{1}{8} \\begin{pmatrix} 6  6 \\\\ 5  1 \\end{pmatrix}\n$$\n$$\nR^{\\star \\top} R^{\\star} = \\left(\\frac{1}{8}\\right)^2 \\begin{pmatrix} 6  6 \\\\ 5  1 \\end{pmatrix} \\begin{pmatrix} 6  5 \\\\ 6  1 \\end{pmatrix} = \\frac{1}{64} \\begin{pmatrix} 36+36  30+6 \\\\ 30+6  25+1 \\end{pmatrix} = \\frac{1}{64} \\begin{pmatrix} 72  36 \\\\ 36  26 \\end{pmatrix}\n$$\nNext, we subtract the identity matrix $I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$:\n$$\nR^{\\star \\top} R^{\\star} - I = \\frac{1}{64} \\begin{pmatrix} 72  36 \\\\ 36  26 \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\frac{1}{64} \\begin{pmatrix} 72-64  36 \\\\ 36  26-64 \\end{pmatrix} = \\frac{1}{64} \\begin{pmatrix} 8  36 \\\\ 36  -38 \\end{pmatrix}\n$$\nFinally, we compute the Frobenius norm of this difference matrix:\n$$\n\\Delta^2 = \\|R^{\\star \\top} R^{\\star} - I\\|_{F}^{2} = \\left(\\frac{1}{64}\\right)^2 \\left( 8^2 + 36^2 + 36^2 + (-38)^2 \\right)\n$$\n$$\n\\Delta^2 = \\frac{1}{4096} \\left( 64 + 1296 + 1296 + 1444 \\right) = \\frac{1}{4096} \\left( 4100 \\right) = \\frac{4100}{4096}\n$$\nWe can simplify this fraction by dividing the numerator and denominator by $4$:\n$$\n\\Delta^2 = \\frac{1025}{1024}\n$$\nTaking the square root to find $\\Delta$:\n$$\n\\Delta = \\sqrt{\\frac{1025}{1024}} = \\frac{\\sqrt{1025}}{\\sqrt{1024}} = \\frac{\\sqrt{25 \\cdot 41}}{32} = \\frac{5\\sqrt{41}}{32}\n$$\nThis is the final exact expression for the deviation from orthogonality.",
            "answer": "$$\n\\boxed{\\frac{5\\sqrt{41}}{32}}\n$$"
        }
    ]
}