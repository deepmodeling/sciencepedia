{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the insights provided by topological data analysis, it is essential to start with the fundamentals. This exercise requires you to compute simplicial homology groups from first principles for a simple complex, representing a structure that might emerge from the Mapper algorithm. By calculating the Betti numbers $\\beta_0$ and $\\beta_1$, you will directly connect the abstract algebraic machinery to concrete, interpretable features of the neural state space: its connectivity and its one-dimensional loops .",
            "id": "4201288",
            "problem": "You are analyzing neuronal population activity from a hippocampal circuit, where a time-delay embedding of calcium imaging traces has been constructed and then clustered to produce a point cloud in a high-dimensional state space. Using a standard pipeline in topological data analysis, you build a Vietoris–Rips complex at scale parameter $\\varepsilon$ and then apply the Mapper algorithm to visualize coarse connectivity. In a specific parameter regime, the Mapper nerve and the Vietoris–Rips `$2$-skeleton` agree on a substructure that can be modeled as a simplicial complex $K$ consisting of two `$2$-simplices` that share exactly one vertex and otherwise have disjoint vertex sets. Concretely, let the vertex set be $\\{v_0,v_1,v_2,v_3,v_4\\}$, with `$2$-simplices` $\\sigma_1=[v_0,v_1,v_2]$ and $\\sigma_2=[v_0,v_3,v_4]$, and all faces of these simplices included; there are no other simplices.\n\nStarting only from the core definitions of simplicial homology with coefficients in $\\mathbb{Z}_2$ (i.e., chains as formal sums of simplices with coefficients in $\\{0,1\\}$, boundary maps as alternating sums reduced modulo $2$, and homology groups $H_k(K;\\mathbb{Z}_2)=\\ker \\partial_k / \\operatorname{im} \\partial_{k+1}$), compute $H_0(K;\\mathbb{Z}_2)$ and $H_1(K;\\mathbb{Z}_2)$ and interpret these in terms of connectivity and one-dimensional recurrent structure of the neural state space.\n\nFinally, using only the Betti numbers you have derived, compute the Euler characteristic\n$$\n\\chi(K)=\\sum_{k=0}^{2} (-1)^k \\beta_k(K),\n$$\nwhere $\\beta_k(K)$ is the `$k$-th` Betti number over $\\mathbb{Z}_2$. Report $\\chi(K)$ as a single integer. No rounding is required, and no units apply. The final numeric answer you provide must be this Euler characteristic.",
            "solution": "We begin with the core definitions of simplicial homology over $\\mathbb{Z}_2$. For each $k\\in \\mathbb{N}$, the `$k$-chains` $C_k(K;\\mathbb{Z}_2)$ are formal sums of `$k$-simplices` with coefficients in $\\mathbb{Z}_2$. Boundary maps $\\partial_k: C_k \\to C_{k-1}$ act on an oriented `$k$-simplex` by the alternating sum of its `$(k-1)$-faces` and extend linearly; over $\\mathbb{Z}_2$ the signs are immaterial, so each face appears with coefficient $1$ modulo $2$. The `$k$-th` homology is $H_k(K;\\mathbb{Z}_2)=\\ker \\partial_k / \\operatorname{im} \\partial_{k+1}$, with Betti number $\\beta_k(K)=\\dim_{\\mathbb{Z}_2} H_k(K;\\mathbb{Z}_2)$.\n\nThe simplicial complex $K$ has vertex set $\\{v_0,v_1,v_2,v_3,v_4\\}$, two `$2$-simplices` $\\sigma_1=[v_0,v_1,v_2]$ and $\\sigma_2=[v_0,v_3,v_4]$, and all faces of these simplices. Thus:\n- The `$0$-simplices` are $v_0,v_1,v_2,v_3,v_4$, so $\\dim C_0 = 5$.\n- The `$1$-simplices` are the edges of the two triangles: $[v_0,v_1]$, $[v_1,v_2]$, $[v_2,v_0]$, $[v_0,v_3]$, $[v_3,v_4]$, $[v_4,v_0]$, so $\\dim C_1 = 6$.\n- The `$2$-simplices` are $\\sigma_1$ and $\\sigma_2$, so $\\dim C_2 = 2$.\n- There are no `$k$-simplices` for $k\\geq 3$, so $C_k=\\{0\\}$ for $k\\geq 3$.\n\nWe compute the boundary maps over $\\mathbb{Z}_2$:\n- For edges, $\\partial_1([v_i,v_j])=v_i+v_j$.\n- For triangles,\n$$\n\\partial_2([v_0,v_1,v_2])=[v_0,v_1]+[v_1,v_2]+[v_2,v_0], \\quad\n\\partial_2([v_0,v_3,v_4])=[v_0,v_3]+[v_3,v_4]+[v_4,v_0].\n$$\n\nFirst, compute $H_1(K;\\mathbb{Z}_2)=\\ker \\partial_1 / \\operatorname{im} \\partial_2$.\n- The `$1$-skeleton` graph has $5$ vertices and $6$ edges. Its connected components count is $1$ because all vertices connect through $v_0$. The cycle rank of a finite graph is $m-n+c$, where $m$ is the number of edges, $n$ is the number of vertices, and $c$ is the number of connected components; this is a standard consequence of rank–nullity for the incidence map over a field. Hence the dimension of $\\ker \\partial_1$ equals $6-5+1=2$. Concretely, the two independent `$1$-cycles` in $\\ker \\partial_1$ are the edge-boundaries of the two triangles:\n$$\nz_1=[v_0,v_1]+[v_1,v_2]+[v_2,v_0], \\quad\nz_2=[v_0,v_3]+[v_3,v_4]+[v_4,v_0].\n$$\n- The image $\\operatorname{im}\\partial_2$ is spanned by $\\partial_2(\\sigma_1)=z_1$ and $\\partial_2(\\sigma_2)=z_2$. These two `$1$-chains` are supported on disjoint edge sets, hence are linearly independent over $\\mathbb{Z}_2$, so $\\dim \\operatorname{im}\\partial_2 = 2$.\n\nTherefore,\n$$\n\\beta_1(K)=\\dim H_1=\\dim \\ker \\partial_1 - \\dim \\operatorname{im}\\partial_2 = 2-2=0,\n$$\nso $H_1(K;\\mathbb{Z}_2)\\cong 0$.\n\nNext, compute $H_0(K;\\mathbb{Z}_2)=\\ker \\partial_0 / \\operatorname{im} \\partial_1$. The map $\\partial_0$ is the zero map, so $\\ker \\partial_0 = C_0$ and $\\dim \\ker \\partial_0 = 5$. The rank of $\\partial_1$ equals $n-c$, where $n=5$ and $c=1$ (for a connected graph, the rank of the incidence map over a field is $n-1$), hence $\\dim \\operatorname{im}\\partial_1 = 4$. Thus\n$$\n\\beta_0(K)=\\dim H_0=\\dim \\ker \\partial_0 - \\dim \\operatorname{im}\\partial_1 = 5-4=1,\n$$\nso $H_0(K;\\mathbb{Z}_2)\\cong \\mathbb{Z}_2$.\n\nFor completeness, $H_2(K;\\mathbb{Z}_2)=\\ker \\partial_2 / \\operatorname{im}\\partial_3$. Since $C_3=\\{0\\}$, $\\operatorname{im}\\partial_3=\\{0\\}$. The map $\\partial_2$ sends the two basis elements to $z_1$ and $z_2$, which are independent, so $\\partial_2$ is injective and $\\ker \\partial_2=\\{0\\}$. Hence $H_2(K;\\mathbb{Z}_2)=0$ and $\\beta_2(K)=0$.\n\nInterpretation in neuroscience data analysis terms: $\\beta_0(K)=1$ indicates a single connected component of the neural state space at the examined resolution $\\varepsilon$, consistent with a coherent, connected regime of population activity. $\\beta_1(K)=0$ indicates the absence of one-dimensional topological loops after accounting for the filled `$2$-simplices`, meaning there is no persistent circular structure suggestive of robust periodic dynamics at this scale; the apparent `$1$-cycles` in the `$1$-skeleton` are boundaries of `$2$-simplices` and thus not topological holes.\n\nFinally, compute the Euler characteristic from the Betti numbers:\n$$\n\\chi(K)=\\sum_{k=0}^{2} (-1)^k \\beta_k(K) = \\beta_0 - \\beta_1 + \\beta_2 = 1 - 0 + 0 = 1.\n$$\nThis is the required single integer.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Persistent homology summarizes the topological features of a dataset across multiple scales in a persistence diagram. A critical task in scientific applications is to compare these topological summaries, for instance, between two different experimental conditions. This practice introduces the bottleneck distance, a standard metric for quantifying the difference between two persistence diagrams, and challenges you to compute it for a concrete example, building intuition for the stability and robustness of topological features .",
            "id": "4201354",
            "problem": "A hippocampal place-cell ensemble is recorded under two behavioral contexts, yielding two point clouds in a low-dimensional neural state space via time-delay embeddings. Using a standard Vietoris–Rips filtration computed on each point cloud, you obtain the following one-dimensional persistence diagrams (as multisets of off-diagonal points) in a normalized filtration parameter: diagram $\\mathcal{D}_A$ contains the two points $(0,3)$ and $(1,2)$; diagram $\\mathcal{D}_B$ contains the two points $\\left(0,\\tfrac{13}{5}\\right)$ and $\\left(\\tfrac{9}{10},\\tfrac{21}{10}\\right)$. Assume that diagonal points $(t,t)$ are available with infinite multiplicity in each diagram.\n\nFirst, starting from core definitions in Topological Data Analysis (TDA), give a precise mathematical definition of the bottleneck distance $d_B$ between two persistence diagrams as the infimum over partial matchings that minimize the $L^{\\infty}$ cost, explicitly stating how the $L^{\\infty}$ norm is used and how matching to the diagonal is handled. Then, using that definition only, compute $d_B\\!\\left(\\mathcal{D}_A,\\mathcal{D}_B\\right)$ by constructing an explicit optimal matching and justifying its optimality via a rigorous lower bound argument. Express your final answer as an exact rational number with no rounding.",
            "solution": "The foundational objects are persistence diagrams and the bottleneck distance from Topological Data Analysis (TDA). A persistence diagram $\\mathcal{D}$ is a multiset of points $(b,d)$ with $b<d$ in $\\mathbb{R}^2$, representing the birth and death parameters of topological features, supplemented by the diagonal $\\Delta=\\{(t,t):t\\in\\mathbb{R}\\}$ with infinite multiplicity. The $L^{\\infty}$ norm on $\\mathbb{R}^2$ is defined by $\\|(x_1,x_2)\\|_{\\infty}=\\max\\{|x_1|,|x_2|\\}$, and the corresponding metric is $\\|(x_1,x_2)-(y_1,y_2)\\|_{\\infty}=\\max\\{|x_1-y_1|,|x_2-y_2|\\}$.\n\nThe bottleneck distance $d_B$ between two persistence diagrams $\\mathcal{D}_1$ and $\\mathcal{D}_2$ is defined as follows. Consider all bijections $\\gamma:\\mathcal{D}_1\\cup\\Delta\\to\\mathcal{D}_2\\cup\\Delta$, where points may be matched to diagonal points in $\\Delta$ and vice versa, and only finitely many off-diagonal points are matched to off-diagonal points (all but finitely many points are matched to diagonal points). The cost of a bijection $\\gamma$ is\n$$\n\\operatorname{cost}(\\gamma)\\;=\\;\\sup_{x\\in\\mathcal{D}_1\\cup\\Delta}\\,\\|x-\\gamma(x)\\|_{\\infty}.\n$$\nThen the bottleneck distance is\n$$\nd_B(\\mathcal{D}_1,\\mathcal{D}_2)\\;=\\;\\inf_{\\gamma}\\,\\operatorname{cost}(\\gamma).\n$$\nEquivalently, one may describe $d_B$ via partial matchings between the finite off-diagonal points of $\\mathcal{D}_1$ and $\\mathcal{D}_2$, allowing any unmatched off-diagonal point to be matched to the diagonal. In this formulation, the cost associated with matching a point $(b,d)$ to the diagonal is the $L^{\\infty}$ distance from $(b,d)$ to $\\Delta$, which is the minimum over $t\\in\\mathbb{R}$ of $\\|(b,d)-(t,t)\\|_{\\infty}$. To compute this, note that\n$$\n\\min_{t\\in\\mathbb{R}}\\,\\max\\{|b-t|,|d-t|\\}\n$$\nis achieved when $t$ lies midway between $b$ and $d$, namely at $t=\\tfrac{b+d}{2}$, yielding a minimal value of $\\tfrac{d-b}{2}$. Therefore, the cost of matching $(b,d)$ to the diagonal is $\\tfrac{d-b}{2}$.\n\nWe now compute $d_B\\!\\left(\\mathcal{D}_A,\\mathcal{D}_B\\right)$ for the given diagrams. Write\n$$\n\\mathcal{D}_A=\\{p_1,p_2\\},\\quad p_1=(0,3),\\; p_2=(1,2),\n$$\n$$\n\\mathcal{D}_B=\\{q_1,q_2\\},\\quad q_1=\\left(0,\\tfrac{13}{5}\\right),\\; q_2=\\left(\\tfrac{9}{10},\\tfrac{21}{10}\\right).\n$$\nWe will evaluate the $L^{\\infty}$ distances between these points and also their distances to the diagonal to assess candidate matchings.\n\nFirst, compute pairwise $L^{\\infty}$ distances:\n$$\n\\|p_1-q_1\\|_{\\infty}=\\max\\{|0-0|,|3-\\tfrac{13}{5}|\\}=\\max\\{0,|\\,\\tfrac{15}{5}-\\tfrac{13}{5}\\,|\\}=\\tfrac{2}{5},\n$$\n$$\n\\|p_1-q_2\\|_{\\infty}=\\max\\left\\{\\left|0-\\tfrac{9}{10}\\right|,\\left|3-\\tfrac{21}{10}\\right|\\right\\}=\\max\\left\\{\\tfrac{9}{10},\\left|\\tfrac{30}{10}-\\tfrac{21}{10}\\right|\\right\\}=\\tfrac{9}{10},\n$$\n$$\n\\|p_2-q_1\\|_{\\infty}=\\max\\left\\{|1-0|,\\left|2-\\tfrac{13}{5}\\right|\\right\\}=\\max\\left\\{1,\\left|\\tfrac{10}{5}-\\tfrac{13}{5}\\right|\\right\\}=1,\n$$\n$$\n\\|p_2-q_2\\|_{\\infty}=\\max\\left\\{\\left|1-\\tfrac{9}{10}\\right|,\\left|2-\\tfrac{21}{10}\\right|\\right\\}=\\max\\left\\{\\tfrac{1}{10},\\left|\\tfrac{20}{10}-\\tfrac{21}{10}\\right|\\right\\}=\\tfrac{1}{10}.\n$$\nNext, compute distances of points to the diagonal using $\\operatorname{dist}_{\\infty}((b,d),\\Delta)=\\tfrac{d-b}{2}$:\n$$\n\\operatorname{dist}_{\\infty}(p_1,\\Delta)=\\tfrac{3-0}{2}=\\tfrac{3}{2},\\quad \\operatorname{dist}_{\\infty}(p_2,\\Delta)=\\tfrac{2-1}{2}=\\tfrac{1}{2},\n$$\n$$\n\\operatorname{dist}_{\\infty}(q_1,\\Delta)=\\tfrac{\\tfrac{13}{5}-0}{2}=\\tfrac{13}{10},\\quad \\operatorname{dist}_{\\infty}(q_2,\\Delta)=\\tfrac{\\tfrac{21}{10}-\\tfrac{9}{10}}{2}=\\tfrac{\\tfrac{12}{10}}{2}=\\tfrac{6}{10}=\\tfrac{3}{5}.\n$$\n\nWe now construct a candidate matching between off-diagonal points and compute its cost. Consider the matching that pairs $p_1$ with $q_1$ and $p_2$ with $q_2$. The corresponding costs are $\\tfrac{2}{5}$ and $\\tfrac{1}{10}$, so the bottleneck (the maximum over matched pairs) for this matching is\n$$\n\\max\\left\\{\\tfrac{2}{5},\\tfrac{1}{10}\\right\\}=\\tfrac{2}{5}.\n$$\nThus there exists a matching with cost at most $\\tfrac{2}{5}$, so $d_B\\!\\left(\\mathcal{D}_A,\\mathcal{D}_B\\right)\\le\\tfrac{2}{5}$.\n\nTo prove optimality, we produce a lower bound that matches this value. Any admissible matching must assign $p_1$ to either $q_1$, $q_2$, or the diagonal. The three corresponding costs are, respectively, $\\tfrac{2}{5}$, $\\tfrac{9}{10}$, and $\\tfrac{3}{2}$. Therefore, for any matching, the cost contributed by the image of $p_1$ is at least\n$$\n\\min\\left\\{\\tfrac{2}{5},\\tfrac{9}{10},\\tfrac{3}{2}\\right\\}=\\tfrac{2}{5}.\n$$\nHence every matching has bottleneck cost at least $\\tfrac{2}{5}$, implying $d_B\\!\\left(\\mathcal{D}_A,\\mathcal{D}_B\\right)\\ge\\tfrac{2}{5}$.\n\nCombining the upper and lower bounds yields\n$$\nd_B\\!\\left(\\mathcal{D}_A,\\mathcal{D}_B\\right)=\\tfrac{2}{5}.\n$$\nThis exact rational value is achieved by the explicit matching $\\{p_1\\leftrightarrow q_1,\\;p_2\\leftrightarrow q_2\\}$.",
            "answer": "$$\\boxed{\\tfrac{2}{5}}$$"
        },
        {
            "introduction": "Moving from theoretical calculation to applied data science, this final practice asks you to implement a complete computational pipeline to statistically compare topological structures derived from the Mapper algorithm. You will generate synthetic neural data, apply a common filter, construct Mapper graphs, and use a label-shuffling permutation test to derive empirical $p$-values for differences in edge counts. By integrating these steps and applying the Benjamini-Hochberg procedure to control the false discovery rate, you will engage with the challenges and best practices of rigorous hypothesis testing using TDA in a realistic scientific context .",
            "id": "4201320",
            "problem": "You are provided with a synthetic dataset construction that imitates embeddings of neural activity under two experimental conditions. Your task is to implement a rigorous, empirically grounded procedure for evaluating condition differences in graph topology derived from the Mapper algorithm of Topological Data Analysis (TDA), and to control the False Discovery Rate (FDR). The procedure must rely on label shuffling to generate null distributions and compute empirical $p$-values for a set of local edge-count comparisons. Your final program must produce results for a specified test suite and print them in a single line with the exact format described below.\n\nBegin from the following fundamental bases and definitions:\n\n- Topological Data Analysis (TDA): Given a point cloud $X \\subset \\mathbb{R}^d$ and a filter function $f : X \\to \\mathbb{R}$, the Mapper algorithm constructs a graph by covering the range of $f$ with overlapping intervals, clustering points within each interval, and connecting clusters from adjacent intervals that share points. This produces a combinatorial representation of the data’s shape.\n- Mapper Algorithm: Given a filter $f$, a cover of $\\mathrm{range}(f)$ into $m$ overlapping intervals, and a clustering mechanism, nodes correspond to clusters within intervals, and edges connect nodes from adjacent intervals if their corresponding clusters share at least one data point. Let the intervals be indexed by $i \\in \\{0,1,\\dots,m-1\\}$ and consider only edges between adjacent intervals $(i,i+1)$, yielding $m-1$ local edge-counts.\n- Principal Component Analysis (PCA): To define a common filter across conditions, use the first principal component computed on the combined dataset. Let $X$ be centered and let $\\mathbf{v}_1$ be the leading right singular vector from the singular value decomposition of $X$. Define $f(\\mathbf{x}) = \\langle \\mathbf{x}, \\mathbf{v}_1 \\rangle$, and rescale $f$ linearly to the unit interval $[0,1]$.\n- False Discovery Rate (FDR): False Discovery Rate (FDR) is the expected proportion of false rejections among all rejections. Control FDR using the Benjamini–Hochberg (BH) procedure: given empirical $p$-values $p_1,\\dots,p_n$, sort them in ascending order $p_{(1)} \\le \\dots \\le p_{(n)}$ with corresponding ranks $r=1,\\dots,n$, and define the largest $k$ such that $p_{(k)} \\le \\alpha \\cdot k / n$, where $\\alpha$ is the desired FDR level. Reject all hypotheses with $p_{(j)} \\le p_{(k)}$.\n- Empirical $p$-values from label shuffling: Given an observed test statistic $T_{\\mathrm{obs}}$ and $K$ null replicates $T^{(1)},\\dots,T^{(K)}$ from shuffling labels, define the two-sided empirical $p$-value as $p = \\dfrac{1 + \\#\\{k : |T^{(k)}| \\ge |T_{\\mathrm{obs}}|\\}}{K + 1}$.\n\nTask description:\n\n1. Data generation: For each test case, generate two conditions $A$ and $B$ of $N$ points in $\\mathbb{R}^3$ representing embeddings of neural activity. Condition $A$ is either a noisy circle or a noisy line; condition $B$ is either a noisy line or the same shape as $A$, depending on the specific test case. Gaussian noise is added isotropically to make the configuration scientifically plausible. Use the provided random seed to ensure reproducibility.\n\n2. Filter function: Compute the first principal component on the combined dataset of $A \\cup B$, define $f(\\mathbf{x})$ as the projection onto that component, and rescale $f$ linearly to $[0,1]$.\n\n3. Cover construction: Given $m$ and overlap fraction $o$ with $0 \\le o < 1$, construct $m$ intervals $I_i = [a_i,b_i]$ over $[0,1]$ such that adjacent intervals overlap by a fraction approximately equal to $o$. Let the base length be $L = 1/m$ and the step size be $S = L \\cdot (1 - o)$. Define $a_i = i S$ and $b_i = a_i + L$, clipped to $[0,1]$.\n\n4. Clustering within intervals: For any interval $I_i$, consider the set of points whose filter value lies within $I_i$. Cluster these points using a simple connected-components rule with a Euclidean distance threshold $\\varepsilon$: two points are adjacent if their distance is at most $\\varepsilon$, and clusters are the connected components of this adjacency graph.\n\n5. Mapper graph construction: For each condition separately, using the common filter $f$ and the same intervals, construct Mapper nodes as clusters and form edges between clusters from adjacent intervals $(i,i+1)$ when they share at least one data point (by index). Count the number of edges per adjacency pair $(i,i+1)$, denoted $E_A(i)$ and $E_B(i)$ for conditions $A$ and $B$, respectively.\n\n6. Test statistic per adjacency: For each adjacency pair $(i,i+1)$, define the test statistic $T(i) = E_A(i) - E_B(i)$.\n\n7. Null distribution via label shuffling: Generate $K$ null replicates by randomly permuting condition labels across the combined dataset and recomputing $T^{(k)}(i)$ for each adjacency pair. Compute two-sided empirical $p$-values $p(i)$ using the formula above.\n\n8. FDR control: Apply the Benjamini–Hochberg procedure at level $\\alpha$ across the $m-1$ empirical $p$-values for a given test case. Return a boolean vector of length $m-1$ indicating which adjacency pairs are significant discoveries under FDR control.\n\nTest suite and parameters:\n\nImplement your program to run the following three test cases. Each test case specifies $(N, \\text{shape}_A, \\text{shape}_B, m, o, \\varepsilon, K, \\alpha, \\text{seed}, \\text{proportion}_A)$, where $\\text{shape} \\in \\{\\text{circle}, \\text{line}\\}$ and $\\text{proportion}_A$ is the fraction of points assigned to condition $A$.\n\n- Case $1$ (balanced, different shapes, happy path): $(N=180, \\text{shape}_A=\\text{circle}, \\text{shape}_B=\\text{line}, m=8, o=0.5, \\varepsilon=0.15, K=200, \\alpha=0.10, \\text{seed}=42, \\text{proportion}_A=0.5)$. Circle radius $r=1$ with Gaussian noise standard deviation $\\sigma=0.05$. Line segment is along the $x$-axis from $-1$ to $1$ with the same noise level in all coordinates.\n\n- Case $2$ (imbalanced labels, different shapes, boundary): $(N=160, \\text{shape}_A=\\text{circle}, \\text{shape}_B=\\text{line}, m=6, o=0.4, \\varepsilon=0.20, K=150, \\alpha=0.10, \\text{seed}=123, \\text{proportion}_A=0.25)$. Circle and line as above with $\\sigma=0.05$.\n\n- Case $3$ (no difference, edge case): $(N=150, \\text{shape}_A=\\text{line}, \\text{shape}_B=\\text{line}, m=5, o=0.3, \\varepsilon=0.20, K=150, \\alpha=0.10, \\text{seed}=7, \\text{proportion}_A=0.5)$. Both conditions use the same line construction with $\\sigma=0.05$.\n\nAnswer representation and output format:\n\n- For each test case, produce a list of booleans of length $m-1$ indicating which adjacency pairs $(i,i+1)$ are significant under Benjamini–Hochberg FDR control at level $\\alpha$.\n- Your program should produce a single line of output containing the results as a comma-separated list of these boolean lists enclosed in square brackets. For example, if case $1$ has $m-1=7$ adjacency pairs and case $2$ has $m-1=5$, the printout must have the form $[[b_{1,1},\\dots,b_{1,7}],[b_{2,1},\\dots,b_{2,5}],[b_{3,1},\\dots,b_{3,4}]]$ where each $b_{c,i}$ is either $\\mathrm{True}$ or $\\mathrm{False}$.",
            "solution": "The task is to implement a procedure for statistically comparing the topological structures of two datasets, derived from the Mapper algorithm, and to control the False Discovery Rate (FDR). The methodology is grounded in computational topology and non-parametric statistics, involving data generation, PCA-based filtering, Mapper graph construction, and permutation testing.\n\nFirst, we address the problem of data generation. For each of the two conditions, designated $A$ and $B$, we generate a point cloud in $\\mathbb{R}^3$. The total number of points is $N$. Condition $A$ comprises $N_A = \\text{round}(N \\cdot \\text{proportion}_A)$ points, and condition $B$ comprises the remaining $N_B = N - N_A$ points. The geometric shape of the data for each condition is specified as either a 'circle' or a 'line'.\nFor a 'circle' of radius $r=1$, we sample $N_i$ points by first drawing angles $\\theta_j$ from a uniform distribution $U[0, 2\\pi)$ for $j \\in \\{1, \\dots, N_i\\}$. The initial coordinates are given by $(\\cos \\theta_j, \\sin \\theta_j, 0)$.\nFor a 'line', we sample $N_i$ points by drawing positions $u_j$ from a uniform distribution $U[-1, 1]$ for $j \\in \\{1, \\dots, N_i\\}$. The initial coordinates are $(u_j, 0, 0)$.\nTo simulate realistic measurement noise, an isotropic Gaussian noise vector $\\mathbf{\\epsilon}_j \\sim \\mathcal{N}(0, \\sigma^2 I_3)$ is added to each point, where $I_3$ is the $3 \\times 3$ identity matrix and $\\sigma=0.05$ is the noise standard deviation. The final points are thus $\\mathbf{x}_j = \\mathbf{x}_{\\text{shape}, j} + \\mathbf{\\epsilon}_j$.\n\nThe next step is to define a common filter function $f$ for the combined dataset $X = X_A \\cup X_B$, which is an $(N \\times 3)$ matrix. The filter function is derived from Principal Component Analysis (PCA). We first center the data by subtracting its mean vector: $\\bar{X} = X - \\mathbb{E}[X]$. We then perform a Singular Value Decomposition (SVD) on the centered data matrix: $\\bar{X} = U \\Sigma V^T$. The columns of $V$ are the principal components (right singular vectors). The first principal component, $\\mathbf{v}_1$, which is the direction of maximum variance, is the first column of $V$ (or the first row of $V^T$). The filter value for a point $\\mathbf{x}_i \\in X$ is its projection onto this component: $f(\\mathbf{x}_i) = \\langle \\mathbf{x}_i - \\mathbb{E}[X], \\mathbf{v}_1 \\rangle$. These filter values are subsequently rescaled linearly to lie within the unit interval $[0, 1]$.\n\nWith the filter function established, we construct the Mapper graph. This begins with defining a cover for the range of $f$, which is $[0, 1]$. The cover consists of $m$ overlapping intervals. The length of each interval is $L = 1/m$. The overlap between adjacent intervals is determined by a fraction $o \\in [0, 1)$, which dictates the step size $S = L \\cdot (1 - o)$ between the start of consecutive intervals. The $i$-th interval, for $i \\in \\{0, \\dots, m-1\\}$, is $I_i = [a_i, b_i]$, where $a_i = iS$ and $b_i = a_i + L$. The interval bounds are clipped to remain within $[0, 1]$.\n\nFor each interval $I_i$, we identify the subset of data points whose filter values fall within it, known as the pullback set $f^{-1}(I_i)$. For each condition ($A$ and $B$) separately, we cluster the points in its respective pullback set. The clustering is based on a connected-components approach. An undirected graph is formed with the points as vertices. An edge connects two vertices if their Euclidean distance in $\\mathbb{R}^3$ is no greater than a specified threshold $\\varepsilon$. The clusters are the connected components of this graph. Each cluster is a set of original data point indices.\n\nThe nodes of the Mapper graph are these clusters. The structure of the graph is defined by connections between nodes corresponding to adjacent intervals. An edge exists between a cluster $C_p$ from interval $I_i$ and a cluster $C_q$ from interval $I_{i+1}$ if they share at least one common data point, i.e., $C_p \\cap C_q \\neq \\emptyset$. We compute the total number of such edges for each pair of adjacent intervals $(i, i+1)$. This gives us a sequence of local edge counts, $E_A(i)$ and $E_B(i)$, for each condition, where $i \\in \\{0, \\dots, m-2\\}$.\n\nTo assess the statistical significance of the differences between the two conditions, we define a test statistic for each interval adjacency $i$: $T(i) = E_A(i) - E_B(i)$. We first compute the observed value of this statistic, $T_{\\text{obs}}(i)$, using the original data labels.\n\nTo determine if the observed difference is greater than what would be expected by chance, we generate a null distribution using a permutation test. This involves creating $K$ null replicates. For each replicate $k \\in \\{1, \\dots, K\\}$, the condition labels are randomly permuted across the $N$ data points, creating new pseudo-conditions $A^{(k)}$ and $B^{(k)}$. For each such shuffled dataset, the entire procedure of clustering within intervals and counting Mapper edges is repeated, yielding a set of null statistics $T^{(k)}(i) = E_{A^{(k)}}(i) - E_{B^{(k)}}(i)$.\n\nUsing the observed statistic and the null distribution, we calculate a two-sided empirical $p$-value for each adjacency $i$. The formula is $p(i) = \\frac{1 + \\#\\{k : |T^{(k)}(i)| \\ge |T_{\\text{obs}}(i)|\\}}{K+1}$. The addition of $1$ to the numerator and denominator is a standard correction to prevent $p$-values of $0$ and to ensure conservative inference.\n\nFinally, since we are performing multiple comparisons (one for each of the $m-1$ adjacencies), we must control for the increased risk of false positives. We use the Benjamini-Hochberg (BH) procedure to control the False Discovery Rate (FDR) at a specified level $\\alpha$. The $m-1$ p-values, $\\{p(0), \\dots, p(m-2)\\}$, are sorted in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m-1)}$. We then find the largest rank $k_{\\text{max}}$ such that $p_{(k_{\\text{max)})} \\le \\frac{k_{\\text{max}}}{m-1}\\alpha$. All null hypotheses corresponding to the $p$-values $p_{(1)}, \\dots, p_{(k_{\\text{max}})}$ are rejected. The final output for each test case is a boolean vector of length $m-1$, indicating which adjacencies exhibit a statistically significant difference between conditions $A$ and $B$ after FDR control.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse.csgraph import connected_components\nfrom scipy.spatial.distance import pdist, squareform\n\ndef generate_data(n_points, shape, noise_sigma, rng):\n    \"\"\"Generates a 3D point cloud for a given shape with Gaussian noise.\"\"\"\n    if shape == 'circle':\n        # r=1 is specified in the problem implicitly\n        radius = 1.0\n        angles = rng.uniform(0, 2 * np.pi, n_points)\n        points = np.zeros((n_points, 3))\n        points[:, 0] = radius * np.cos(angles)\n        points[:, 1] = radius * np.sin(angles)\n    elif shape == 'line':\n        # line from -1 to 1 on x-axis\n        positions = rng.uniform(-1, 1, n_points)\n        points = np.zeros((n_points, 3))\n        points[:, 0] = positions\n    else:\n        raise ValueError(\"Unknown shape specified.\")\n    \n    noise = rng.normal(0, noise_sigma, (n_points, 3))\n    return points + noise\n\ndef get_pca_filter(points):\n    \"\"\"Computes the filter function based on the first principal component.\"\"\"\n    centered_points = points - np.mean(points, axis=0)\n    _, _, Vt = np.linalg.svd(centered_points, full_matrices=False)\n    v1 = Vt[0, :]\n    filter_values = centered_points @ v1\n    # Rescale to [0, 1]\n    min_val, max_val = np.min(filter_values), np.max(filter_values)\n    if min_val == max_val:\n        return np.zeros_like(filter_values)\n    return (filter_values - min_val) / (max_val - min_val)\n\ndef get_clusters_in_interval(point_indices, all_points, epsilon):\n    \"\"\"Clusters points within an interval using connected components.\"\"\"\n    if len(point_indices) == 0:\n        return []\n    \n    interval_points = all_points[point_indices, :]\n    \n    if len(point_indices) == 1:\n        # A single point is a single cluster\n        return [[point_indices[0]]]\n\n    dist_matrix = squareform(pdist(interval_points))\n    adj_matrix = dist_matrix <= epsilon\n    \n    n_components, labels = connected_components(\n        csgraph=adj_matrix, directed=False, return_labels=True\n    )\n    \n    clusters = []\n    for i in range(n_components):\n        cluster_member_indices = np.where(labels == i)[0]\n        original_indices = [point_indices[j] for j in cluster_member_indices]\n        clusters.append(original_indices)\n        \n    return clusters\n\ndef compute_mapper_edges(point_indices, all_points, filter_values, m, o, epsilon):\n    \"\"\"Computes the number of edges for each adjacent interval pair.\"\"\"\n    intervals = []\n    L = 1.0 / m\n    S = L * (1.0 - o)\n    for i in range(m):\n        a_i = i * S\n        b_i = a_i + L\n        intervals.append((min(a_i, 1.0), min(b_i, 1.0)))\n\n    clusters_per_interval = []\n    for i in range(m):\n        a_i, b_i = intervals[i]\n        # Get points whose filter value is in the interval [a_i, b_i]\n        # Note: b_i is inclusive, except for the last interval's end\n        if i == m-1 and b_i == 1.0:\n            interval_point_mask = (filter_values[point_indices] >= a_i) & (filter_values[point_indices] <= b_i)\n        else:\n            interval_point_mask = (filter_values[point_indices] >= a_i) & (filter_values[point_indices] < b_i)\n        \n        current_interval_indices = np.array(point_indices)[interval_point_mask]\n        \n        clusters = get_clusters_in_interval(list(current_interval_indices), all_points, epsilon)\n        clusters_per_interval.append(clusters)\n\n    edge_counts = np.zeros(m - 1, dtype=int)\n    for i in range(m - 1):\n        clusters1 = clusters_per_interval[i]\n        clusters2 = clusters_per_interval[i+1]\n        count = 0\n        for c1 in clusters1:\n            set_c1 = set(c1)\n            for c2 in clusters2:\n                if not set_c1.isdisjoint(c2):\n                    count += 1\n        edge_counts[i] = count\n        \n    return edge_counts\n\ndef benjamini_hochberg(p_values, alpha):\n    \"\"\"Applies the Benjamini-Hochberg FDR control procedure.\"\"\"\n    n = len(p_values)\n    if n == 0:\n        return []\n    \n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = p_values[sorted_indices]\n    \n    k_vals = np.arange(1, n + 1)\n    thresholds = (k_vals / n) * alpha\n    \n    significant_mask = sorted_p_values <= thresholds\n    \n    if np.any(significant_mask):\n        max_k = np.max(np.where(significant_mask))\n        significant_indices = sorted_indices[:max_k + 1]\n    else:\n        significant_indices = []\n        \n    result = np.zeros(n, dtype=bool)\n    result[significant_indices] = True\n    return result.tolist()\n\ndef run_case(params):\n    \"\"\"Runs the full analysis for a single test case.\"\"\"\n    N, shape_A, shape_B, m, o, epsilon, K, alpha, seed, prop_A = params\n    \n    rng = np.random.default_rng(seed)\n    \n    n_A = int(round(N * prop_A))\n    n_B = N - n_A\n    \n    points_A = generate_data(n_A, shape_A, 0.05, rng)\n    points_B = generate_data(n_B, shape_B, 0.05, rng)\n    \n    all_points = np.vstack([points_A, points_B])\n    labels = np.array([0] * n_A + [1] * n_B) # 0 for A, 1 for B\n    \n    indices_A = list(range(n_A))\n    indices_B = list(range(n_A, N))\n    \n    # Common filter for all computations\n    filter_values = get_pca_filter(all_points)\n    \n    # Observed statistic\n    E_A = compute_mapper_edges(indices_A, all_points, filter_values, m, o, epsilon)\n    E_B = compute_mapper_edges(indices_B, all_points, filter_values, m, o, epsilon)\n    T_obs = E_A - E_B\n    \n    # Null distribution from permutations\n    T_null = np.zeros((K, m - 1))\n    \n    # Use a separate RNG for permutations to ensure stability\n    perm_rng = np.random.default_rng(seed)\n\n    for k in range(K):\n        shuffled_labels = perm_rng.permutation(labels)\n        shuffled_indices_A = list(np.where(shuffled_labels == 0)[0])\n        shuffled_indices_B = list(np.where(shuffled_labels == 1)[0])\n        \n        E_A_shuffled = compute_mapper_edges(shuffled_indices_A, all_points, filter_values, m, o, epsilon)\n        E_B_shuffled = compute_mapper_edges(shuffled_indices_B, all_points, filter_values, m, o, epsilon)\n        T_null[k, :] = E_A_shuffled - E_B_shuffled\n        \n    # Empirical p-values\n    p_values = np.zeros(m - 1)\n    for i in range(m - 1):\n        num_more_extreme = np.sum(np.abs(T_null[:, i]) >= np.abs(T_obs[i]))\n        p_values[i] = (1 + num_more_extreme) / (K + 1)\n        \n    # FDR Control\n    return benjamini_hochberg(p_values, alpha)\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Case 1 (balanced, different shapes, happy path)\n        (180, 'circle', 'line', 8, 0.5, 0.15, 200, 0.10, 42, 0.5),\n        # Case 2 (imbalanced labels, different shapes, boundary)\n        (160, 'circle', 'line', 6, 0.4, 0.20, 150, 0.10, 123, 0.25),\n        # Case 3 (no difference, edge case)\n        (150, 'line', 'line', 5, 0.3, 0.20, 150, 0.10, 7, 0.5)\n    ]\n    \n    results = []\n    for case in test_cases:\n        result = run_case(case)\n        results.append(str(result))\n    \n    # Final print statement in the exact required format.\n    # The required format is [[...],[...]], not \"['[...]', '[...]']\"\n    # So we join the string representations of the lists\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}