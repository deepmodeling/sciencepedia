{
    "hands_on_practices": [
        {
            "introduction": "The first step in any Representational Similarity Analysis (RSA) is to formalize a scientific hypothesis into a quantitative prediction. This is accomplished by constructing a model Representational Dissimilarity Matrix (RDM). This practice provides foundational experience in building a model RDM from a set of abstract feature vectors, using the Euclidean distance as our measure of dissimilarity. Through this exercise , you will not only implement the core mechanics of RDM construction but also explore the critical impact of feature scaling, a key modeling decision that directly shapes the predicted representational geometry.",
            "id": "4148250",
            "problem": "You are given a set of stimulus feature vectors in a finite-dimensional real vector space, together with a specification of feature-wise scaling. The goal is to construct a model Representational Dissimilarity Matrix (RDM) and to quantify how feature scaling changes the predicted representational geometry. Work from fundamental definitions in vector spaces and metrics, without relying on pre-derived shortcuts.\n\nFoundational base to use:\n- A real vector space of dimension $p$ is equipped with the standard inner product and the induced norm. For any vector $v \\in \\mathbb{R}^p$, the Euclidean norm satisfies the norm axioms and is defined by the square root of the inner product of a vector with itself. A metric $d$ on a set is a function that satisfies non-negativity, identity of indiscernibles, symmetry, and the triangle inequality. In $\\mathbb{R}^p$ with the Euclidean norm, the metric between two points is defined as the norm of their difference.\n- A Representational Dissimilarity Matrix (RDM) is a symmetric matrix indexed by stimuli, where each off-diagonal entry is a dissimilarity between the corresponding pair of stimuli, and diagonal entries are zero by identity of indiscernibles when the dissimilarity is induced by a norm-based metric.\n\nTasks:\n1. Given $n$ stimuli with feature vectors $x_i \\in \\mathbb{R}^p$ for $i \\in \\{1,\\dots,n\\}$, and given a feature scaling vector $w \\in \\mathbb{R}^p$ with strictly positive entries, construct a model RDM by:\n   - Scaling each stimulus feature vector component-wise by $w$ to obtain a scaled representation for each stimulus.\n   - Computing the pairwise dissimilarities between stimuli as the Euclidean metric in the scaled feature space, placing each dissimilarity in the corresponding off-diagonal entry of an $n \\times n$ matrix, and setting diagonal entries to $0$.\n   - Vectorizing the upper-triangular part (excluding the diagonal) of the RDM into a one-dimensional array in a fixed, consistent order (for example, row-major over the upper triangle).\n2. For each test case, compute two quantities that quantify geometry change due to scaling relative to the unscaled baseline $w = \\mathbf{1}_p$:\n   - The Pearson correlation coefficient between the upper-triangular vectorization of the baseline RDM and the scaled RDM. If this correlation is undefined due to zero variance in either vector (e.g., all entries identical), return the value $0.0$ for the correlation.\n   - The relative Frobenius change defined as the Frobenius norm of the difference between the scaled and baseline RDMs divided by the Frobenius norm of the baseline RDM. If the baseline Frobenius norm is $0$, return $0.0$ for the relative change.\n3. Implement the above using only definitions of the Euclidean norm and metric, ensuring the RDM is symmetric, non-negative, with zero diagonal, and distances computed from first principles (difference of vectors followed by Euclidean norm).\n\nTest suite:\nUse the following set of matrices and scaling vectors.\n- Case $1$ (happy path, anisotropic scaling): $n = 5$, $p = 3$, stimulus matrix\n$$\nX^{(1)} = \\begin{bmatrix}\n0  1  2 \\\\\n1  0  3 \\\\\n2  2  1 \\\\\n3  1.5  0.5 \\\\\n0.5  3  2.5\n\\end{bmatrix},\n$$\nand scaling vector\n$$\nw^{(1)} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0.5 \\end{bmatrix}.\n$$\n- Case $2$ (boundary, uniform scaling): reuse $X^{(1)}$ and use\n$$\nw^{(2)} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix}.\n$$\n- Case $3$ (edge, extreme anisotropy): reuse $X^{(1)}$ and use\n$$\nw^{(3)} = \\begin{bmatrix} 0.1 \\\\ 10 \\\\ 0.1 \\end{bmatrix}.\n$$\n- Case $4$ (degenerate baseline, identical stimuli): $n = 3$, $p = 2$,\n$$\nX^{(4)} = \\begin{bmatrix}\n1  2 \\\\\n1  2 \\\\\n1  2\n\\end{bmatrix},\n\\quad\nw^{(4)} = \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix}.\n$$\n\nOutput specification:\n- For each case $k \\in \\{1,2,3,4\\}$, compute the Pearson correlation between the baseline RDM (with $w = \\mathbf{1}_p$) and the scaled RDM (with $w = w^{(k)}$), and the relative Frobenius change as defined above. Return the ordered pair $\\left[\\text{corr}^{(k)}, \\text{rel\\_change}^{(k)}\\right]$ as a list of two floats.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list. For example, the output should look like\n$$\n[[c_1,r_1],[c_2,r_2],[c_3,r_3],[c_4,r_4]],\n$$\nwith each $c_k$ and $r_k$ a floating-point number.",
            "solution": "The problem requires the construction of Representational Dissimilarity Matrices (RDMs) from a given set of stimulus feature vectors and a feature-wise scaling vector. Furthermore, it asks for a quantification of the geometric change induced by this scaling, relative to a baseline unscaled representation. The entire process must be grounded in the fundamental definitions of vector spaces, norms, and metrics.\n\nThe solution is developed in three stages: first, defining the scaling of feature vectors; second, constructing the RDM from these scaled vectors using the Euclidean metric; and third, comparing the scaled RDM to a baseline RDM using Pearson correlation and the relative Frobenius norm.\n\nA stimulus is represented as a vector $x_i$ in a $p$-dimensional real vector space, $\\mathbb{R}^p$. The set of $n$ stimuli forms a matrix $X$ of size $n \\times p$. Feature scaling is applied using a vector $w \\in \\mathbb{R}^p$ with strictly positive entries, $w_j  0$. The scaling operation transforms each stimulus vector $x_i = [x_{i,1}, x_{i,2}, \\dots, x_{i,p}]$ into a new vector $x'_i$ by component-wise multiplication:\n$$\nx'_i = [x_{i,1} w_1, x_{i,2} w_2, \\dots, x_{i,p} w_p]\n$$\nThis operation is equivalent to the Hadamard (element-wise) product, $x'_i = x_i \\circ w$. This transformation alters the geometry of the feature space by stretching or compressing it along each feature axis.\n\nThe RDM is an $n \\times n$ symmetric matrix whose entries quantify the dissimilarity between pairs of stimuli. The problem specifies the dissimilarity to be the Euclidean distance in the scaled feature space. According to the foundational definitions, the Euclidean distance $d(u,v)$ between two vectors $u, v \\in \\mathbb{R}^p$ is induced by the Euclidean norm $\\| \\cdot \\|$, such that $d(u,v) = \\|u-v\\|$. The norm itself is defined from the standard inner product $\\langle \\cdot, \\cdot \\rangle$ as $\\|z\\| = \\sqrt{\\langle z, z \\rangle}$. For a vector $z = [z_1, \\dots, z_p]$, this becomes:\n$$\n\\|z\\| = \\sqrt{\\sum_{k=1}^{p} z_k^2}\n$$\nThe dissimilarity $d_{ij}$ between the scaled stimulus vectors $x'_i$ and $x'_j$ is therefore:\n$$\nd_{ij} = \\|x'_i - x'_j\\| = \\sqrt{\\sum_{k=1}^{p} (x'_{i,k} - x'_{j,k})^2}\n$$\nThe RDM, denoted as $R$, is constructed such that its off-diagonal entries are these pairwise dissimilarities, $R_{ij} = d_{ij}$ for $i \\neq j$. By the identity of indiscernibles, a property of metrics, the distance of a point to itself is zero, $d(x'_i, x'_i) = 0$. Consequently, the diagonal entries of the RDM are all zero, $R_{ii} = 0$. The symmetry property, $d(u,v) = d(v,u)$, ensures that the RDM is symmetric, $R_{ij} = R_{ji}$.\n\nFor comparison purposes, the upper-triangular elements of the RDM (excluding the diagonal) are vectorized into a one-dimensional array. This vector contains all $\\frac{n(n-1)}{2}$ unique pairwise dissimilarities in a fixed, row-major order.\n\nTo quantify the effect of scaling, the RDM computed with a given scaling vector $w$ ($RDM_{scaled}$) is compared to a baseline RDM computed without explicit scaling ($RDM_{baseline}$). This baseline corresponds to using a scaling vector of ones, $w_{base} = \\mathbf{1}_p = [1, 1, \\dots, 1]^T$. Two metrics are employed for this comparison:\n\n1.  **Pearson Correlation Coefficient**: This measures the linear relationship between the vectorized upper triangles of $RDM_{scaled}$ and $RDM_{baseline}$. Let the vectorized dissimilarities be $v_{scaled}$ and $v_{baseline}$. The correlation is:\n    $$\n    \\rho = \\frac{\\sum_{k=1}^{N} (v_{scaled,k} - \\bar{v}_{scaled})(v_{baseline,k} - \\bar{v}_{baseline})}{\\sqrt{\\sum_{k=1}^{N} (v_{scaled,k} - \\bar{v}_{scaled})^2} \\sqrt{\\sum_{k=1}^{N} (v_{baseline,k} - \\bar{v}_{baseline})^2}}\n    $$\n    where $N = \\frac{n(n-1)}{2}$. A correlation of $1$ implies that the scaling was uniform across all dissimilarities (i.e., $v_{scaled} = a \\cdot v_{baseline} + b$ for constants $a0, b$). In the context of distances, this simplifies to $v_{scaled} = a \\cdot v_{baseline}$, as distances are non-negative. Deviations from $1$ indicate a non-linear restructuring of the representational geometry. If the variance of either vector is zero (all dissimilarities are identical), the correlation is undefined and is specified to be reported as $0.0$.\n\n2.  **Relative Frobenius Change**: This measures the magnitude of the difference between the two RDM matrices, normalized by the magnitude of the baseline RDM. The Frobenius norm of an $n \\times n$ matrix $A$ is $\\|A\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^n A_{ij}^2}$. The relative change is:\n    $$\n    \\text{rel\\_change} = \\frac{\\|RDM_{scaled} - RDM_{baseline}\\|_F}{\\|RDM_{baseline}\\|_F}\n    $$\n    This metric provides a normalized measure of the overall change in the dissimilarity values. If the baseline RDM is a zero matrix (i.e., $\\|RDM_{baseline}\\|_F = 0$), the relative change is reported as $0.0$.\n\nThe implementation proceeds by first defining a function to compute the RDM and its vectorization from a stimulus matrix and a scaling vector, adhering strictly to the first-principles calculation of the Euclidean distance. Then, for each test case, this function is called for both the specified scaling vector and the baseline scaling vector. Finally, the two resulting RDMs and their vectorizations are used to compute the correlation and relative Frobenius change as defined above, including handling for the specified edge cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _compute_rdm_and_vectorization(X, w):\n    \"\"\"\n    Constructs an RDM and its upper-triangular vectorization from first principles.\n\n    Args:\n        X (np.ndarray): An n x p stimulus matrix.\n        w (np.ndarray): A p-dimensional feature scaling vector.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing:\n            - The n x n RDM.\n            - The vectorized upper-triangular dissimilarities.\n    \"\"\"\n    n, p = X.shape\n    \n    # Scale the stimulus feature vectors component-wise\n    X_scaled = X * w  # Broadcasting w across rows of X\n    \n    rdm = np.zeros((n, n), dtype=np.float64)\n    \n    # Get indices for the upper triangle (excluding the diagonal)\n    # This provides a fixed, consistent row-major order for vectorization.\n    rows, cols = np.triu_indices(n, k=1)\n    \n    num_dissimilarities = len(rows)\n    vec = np.zeros(num_dissimilarities, dtype=np.float64)\n    \n    for k in range(num_dissimilarities):\n        i, j = rows[k], cols[k]\n        \n        # 1. Compute the difference vector\n        diff_vec = X_scaled[i, :] - X_scaled[j, :]\n        \n        # 2. Compute the Euclidean norm from first principles: sqrt(sum of squares)\n        dist = np.sqrt(np.sum(diff_vec**2))\n        \n        # Populate the RDM and the vector\n        rdm[i, j] = dist\n        rdm[j, i] = dist  # Ensure symmetry\n        vec[k] = dist\n        \n    return rdm, vec\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases as specified.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    X1 = np.array([\n        [0, 1, 2],\n        [1, 0, 3],\n        [2, 2, 1],\n        [3, 1.5, 0.5],\n        [0.5, 3, 2.5]\n    ], dtype=np.float64)\n    w1 = np.array([1, 2, 0.5], dtype=np.float64)\n    w2 = np.array([3, 3, 3], dtype=np.float64)\n    w3 = np.array([0.1, 10, 0.1], dtype=np.float64)\n    \n    X4 = np.array([\n        [1, 2],\n        [1, 2],\n        [1, 2]\n    ], dtype=np.float64)\n    w4 = np.array([5, 7], dtype=np.float64)\n    \n    test_cases = [\n        (X1, w1),\n        (X1, w2),\n        (X1, w3),\n        (X4, w4),\n    ]\n\n    results = []\n    # Machine epsilon for floating point comparisons\n    epsilon = np.finfo(float).eps\n\n    for X, w in test_cases:\n        n, p = X.shape\n        w_base = np.ones(p, dtype=np.float64)\n        \n        # Compute baseline RDM and its vectorization (w = 1)\n        rdm_base, vec_base = _compute_rdm_and_vectorization(X, w_base)\n        \n        # Compute scaled RDM and its vectorization\n        rdm_scaled, vec_scaled = _compute_rdm_and_vectorization(X, w)\n        \n        # Task 2.1: Pearson Correlation\n        # Check for zero variance, as correlation is undefined in this case.\n        if np.std(vec_base)  epsilon or np.std(vec_scaled)  epsilon:\n            corr = 0.0\n        else:\n            # np.corrcoef returns a 2x2 matrix, we need the off-diagonal element\n            corr = np.corrcoef(vec_base, vec_scaled)[0, 1]\n            \n        # Task 2.2: Relative Frobenius Change\n        norm_base = np.linalg.norm(rdm_base, 'fro')\n        \n        if norm_base  epsilon:\n            rel_change = 0.0\n        else:\n            norm_diff = np.linalg.norm(rdm_scaled - rdm_base, 'fro')\n            rel_change = norm_diff / norm_base\n            \n        results.append([corr, rel_change])\n\n    # Final print statement in the exact required format.\n    # The format [[c,r],[c,r],...] is achieved by this construction.\n    result_strings = [f\"[{c},{r}]\" for c, r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once you have a model RDM and an RDM derived from brain data, the central scientific question becomes: does the model's proposed structure align with the brain's representational geometry? This practice introduces the core statistical procedure for answering this question. You will implement a permutation test to assess the statistical significance of the correspondence between a model and a brain RDM, learning a robust, non-parametric method for hypothesis testing that is a cornerstone of the RSA framework . This allows you to determine whether an observed model-brain correlation is likely to be a real effect or simply a product of chance.",
            "id": "4148208",
            "problem": "You are given the task of assessing whether a computational model’s hypothesis about stimulus relations aligns with neural representational geometry, using Representational Dissimilarity Matrices (RDMs). A Representational Dissimilarity Matrix (RDM) is defined as the matrix of pairwise dissimilarities between condition-specific representations. Formally, for $n$ conditions with representations $\\{\\mathbf{x}_1,\\dots,\\mathbf{x}_n\\}$, the RDM $D \\in \\mathbb{R}^{n \\times n}$ has elements $D_{ij} = \\delta(\\mathbf{x}_i,\\mathbf{x}_j)$, where $\\delta$ is a dissimilarity function. In this problem, use Euclidean distance, i.e., $\\delta(\\mathbf{x}_i,\\mathbf{x}_j) = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2$.\n\nTo quantify the association between a model RDM and a brain RDM, use Spearman’s rank correlation: compute the correlation between the rank-transformed upper-triangular entries (excluding the diagonal) of the model and brain RDMs. Let $v^{\\text{model}} \\in \\mathbb{R}^{m}$ and $v^{\\text{brain}} \\in \\mathbb{R}^{m}$ denote the vectors obtained by reading off the $m = \\frac{n(n-1)}{2}$ upper-triangular entries of the model and brain RDMs, respectively. The test statistic is the Spearman correlation between $v^{\\text{model}}$ and $v^{\\text{brain}}$.\n\nAssess statistical significance via a permutation test based on shuffling condition labels. The null hypothesis should be clearly articulated: $H_0$: the assignment of condition labels to brain measurements is exchangeable with respect to the model, meaning that any alignment between the model RDM and the brain RDM arises from chance, and the distribution of the test statistic is invariant under permutations of condition labels. Under $H_0$, permuting the labels induces a null distribution for the Spearman correlation. Use a two-sided test that compares $|r_{\\text{perm}}|$ to $|r_{\\text{obs}}|$, where $r_{\\text{obs}}$ is the observed correlation and $r_{\\text{perm}}$ are correlations computed under permutations.\n\nYour program must:\n- Construct the model and brain RDMs using Euclidean distance for each test case (see Test Suite).\n- Compute the observed Spearman correlation $r_{\\text{obs}}$ between $v^{\\text{model}}$ and $v^{\\text{brain}}$.\n- Perform $B$ random permutations of the $n$ condition labels on the brain RDM by simultaneously permuting its rows and columns (equivalently, apply a permutation $\\pi$ to reorder indices and take the upper-triangular entries of the permuted brain RDM). For each permutation, compute $r_{\\text{perm}}$.\n- Compute the two-sided permutation $p$-value as $\\displaystyle p = \\frac{1 + \\sum_{b=1}^B \\mathbf{1}\\left(|r_{\\text{perm},b}| \\ge |r_{\\text{obs}}|\\right)}{B+1}$, where $\\mathbf{1}(\\cdot)$ is the indicator function. This continuity correction ensures a valid $p$-value even when no permutation exceeds the observed statistic.\n- Produce a single line of output containing the $p$-values for all test cases as a comma-separated list enclosed in square brackets, e.g., $[p_1,p_2,p_3,p_4]$.\n\nTest Suite (each case specifies model features, how to generate brain patterns, and permutation count $B$; all random elements use a specified seed for reproducibility):\n\n- Case $1$ (aligned model and brain; cluster structure):\n  - Conditions: $n = 8$, voxels: $v = 50$, model feature dimension: $d = 2$.\n  - Model features $M \\in \\mathbb{R}^{8 \\times 2}$ (rows are conditions):\n    - Cluster $\\mathcal{A}$: $(0.0, 0.0)$, $(0.0, 0.5)$, $(0.5, 0.0)$, $(0.5, 0.5)$.\n    - Cluster $\\mathcal{B}$: $(3.0, 3.0)$, $(3.0, 3.5)$, $(3.5, 3.0)$, $(3.5, 3.5)$.\n  - Brain patterns $X \\in \\mathbb{R}^{8 \\times 50}$ generated by a linear mapping plus noise:\n    - Weight matrix $W \\in \\mathbb{R}^{2 \\times 50}$ sampled from a standard normal distribution with seed $42$.\n    - Add independent Gaussian noise with standard deviation $\\sigma = 0.3$ using seed $24$.\n    - Construction: $X = M W + \\text{noise}$.\n  - Permutations: $B = 2000$, permutation seed $1001$.\n\n- Case $2$ (null case; independent brain patterns):\n  - Conditions: $n = 8$, voxels: $v = 50$, model as in Case $1$ (same $M$).\n  - Brain patterns $X \\in \\mathbb{R}^{8 \\times 50}$ sampled independently from a standard normal distribution with seed $99$ (i.e., no relationship to $M$).\n  - Permutations: $B = 2000$, permutation seed $2002$.\n\n- Case $3$ (boundary case; minimal number of conditions):\n  - Conditions: $n = 3$, voxels: $v = 30$, model feature dimension: $d = 1$.\n  - Model features $M = \\begin{bmatrix}0\\\\1\\\\2\\end{bmatrix}$.\n  - Brain patterns $X \\in \\mathbb{R}^{3 \\times 30}$:\n    - Weight vector $W \\in \\mathbb{R}^{1 \\times 30}$ sampled from a standard normal distribution with seed $7$.\n    - Add independent Gaussian noise with standard deviation $\\sigma = 0.05$ using seed $31$.\n    - Construction: $X = M W + \\text{noise}$.\n  - Permutations: $B = 1000$, permutation seed $3003$.\n\n- Case $4$ (tied distances in the model RDM; categorical structure):\n  - Conditions: $n = 6$, voxels: $v = 40$, model feature dimension: $d = 1$.\n  - Model features $M = \\begin{bmatrix}0\\\\0\\\\0\\\\1\\\\1\\\\1\\end{bmatrix}$ (two categories).\n  - Brain patterns $X \\in \\mathbb{R}^{6 \\times 40}$:\n    - Weight vector $W \\in \\mathbb{R}^{1 \\times 40}$ sampled from a standard normal distribution with seed $123$.\n    - Add independent Gaussian noise with standard deviation $\\sigma = 0.1$ using seed $321$.\n    - Construction: $X = M W + \\text{noise}$.\n  - Permutations: $B = 2000$, permutation seed $4004$.\n\nYour program must implement the above and produce the final output as a single line in the format $[p_1,p_2,p_3,p_4]$, where $p_i$ are the two-sided permutation $p$-values (floats) for Cases $1$, $2$, $3$, and $4$, respectively. No additional text should be printed.",
            "solution": "The problem is valid. It presents a clear, scientifically grounded, and well-posed task within the domain of computational neuroscience, specifically Representational Similarity Analysis (RSA). All givens, including mathematical definitions, algorithmic procedures, and numerical parameters for test cases, are provided completely and without contradiction. The task is to implement a standard statistical analysis pipeline, which is a formalizable and verifiable problem.\n\nThe solution will be developed based on the principles of RSA, hypothesis testing, and computational statistics.\n\n**1. Foundational Principles of Representational Similarity Analysis (RSA)**\n\nThe central goal of RSA is to characterize the information represented in a population of neural responses by abstracting away from the specific activity patterns of individual neurons or voxels. This is achieved by computing a Representational Dissimilarity Matrix (RDM), a square, symmetric matrix that summarizes the pairwise dissimilarity between the brain's responses to a set of experimental conditions.\n\nAn RDM, denoted by $D \\in \\mathbb{R}^{n \\times n}$ for $n$ conditions, is defined by its elements $D_{ij} = \\delta(\\mathbf{x}_i, \\mathbf{x}_j)$, where $\\mathbf{x}_i$ and $\\mathbf{x}_j$ are the neural response patterns for conditions $i$ and $j$, respectively, and $\\delta$ is a chosen dissimilarity measure. The problem specifies the use of Euclidean distance, $\\delta(\\mathbf{x}_i, \\mathbf{x}_j) = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2$, a common and intuitive choice that measures the distance between response vectors in the high-dimensional neural activation space. The diagonal elements $D_{ii}$ are always $0$, as the dissimilarity of a pattern with itself is zero.\n\n**2. Comparing Representational Geometries**\n\nRSA assesses the correspondence between different representational spaces (e.g., a computational model and brain data) by comparing their respective RDMs. To perform this comparison, we first linearize the upper-triangular part of each RDM into a vector. For an $n \\times n$ RDM, this vector contains $m = \\frac{n(n-1)}{2}$ unique pairwise dissimilarity values. Let these vectors be $v^{\\text{model}}$ and $v^{\\text{brain}}$.\n\nThe problem mandates the use of Spearman's rank correlation, $\\rho(v^{\\text{model}}, v^{\\text{brain}})$. This non-parametric statistic measures the strength and direction of the monotonic relationship between the ranked dissimilarity values. Its use is advantageous because it is robust to outliers and does not assume a linear relationship between the dissimilarities of the model and the brain, making it a flexible tool for comparing representational geometries that might differ in scale but share a similar rank ordering of dissimilarities.\n\n**3. Statistical Inference via Permutation Testing**\n\nTo determine if the observed correlation, $r_{\\text{obs}}$, is statistically significant, we must compare it against a null distribution. The problem specifies a permutation test, which is a powerful non-parametric method for hypothesis testing.\n\nThe null hypothesis, $H_0$, posits that there is no systematic relationship between the model's representational structure and the brain's. Under $H_0$, the condition labels on the brain data are exchangeable; any observed correlation is purely due to chance.\n\nThe permutation test simulates this null hypothesis by repeatedly shuffling the condition labels of the brain data. Algorithmically, this is achieved by applying a random permutation $\\pi$ to the indices $\\{0, 1, \\dots, n-1\\}$ of the brain RDM, $D_{\\text{brain}}$. The permuted RDM, $D'_{\\text{brain}}$, has elements $(D'_{\\text{brain}})_{ij} = (D_{\\text{brain}})_{\\pi(i)\\pi(j)}$. For each of $B$ such permutations, we extract the upper-triangular vector from the permuted RDM and compute a \"null\" Spearman correlation, $r_{\\text{perm},b}$ for $b \\in \\{1, \\dots, B\\}$.\n\n**4. Two-Sided p-value Calculation**\n\nThe collection of null correlations, $\\{r_{\\text{perm},b}\\}_{b=1}^B$, forms an empirical null distribution. The problem requires a two-sided test, which assesses whether the observed correlation is significantly different from zero, regardless of sign (positive or negative). This is accomplished by comparing the absolute value of the observed correlation, $|r_{\\text{obs}}|$, to the absolute values of the null correlations, $|r_{\\text{perm},b}|$.\n\nThe $p$-value is calculated as the proportion of permutations that yield a correlation at least as extreme as the observed one. The specified formula is:\n$$p = \\frac{1 + \\sum_{b=1}^B \\mathbf{1}\\left(|r_{\\text{perm},b}| \\ge |r_{\\text{obs}}|\\right)}{B+1}$$\nHere, $\\mathbf{1}(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise. The `$1+$` in the numerator and `$B+1$` in the denominator represent a continuity correction. This accounts for the observed statistic itself as one sample from the null distribution, ensuring the $p$-value is bounded between $\\frac{1}{B+1}$ and $1$, and is valid even in the case where no permuted statistic is more extreme than the observed one.\n\n**5. Algorithmic Implementation**\n\nThe implementation will proceed by systematically addressing each test case as follows:\n\n1.  **Data Generation**: For each case, we generate the model feature matrix $M$ and the brain data matrix $X$. For cases $1$, $3$, and $4$, the brain data $X$ are constructed as $X = M W + \\text{noise}$, where the weight matrix $W$ and the noise term are generated from normal distributions using the specified random seeds for reproducibility. For case $2$, $X$ is generated independently from $M$, also using a specified seed.\n\n2.  **RDM Construction**: A function will compute the $n \\times n$ RDM from an input $n \\times d$ data matrix using Euclidean distances. This function will then extract the $m = \\frac{n(n-1)}{2}$ upper-triangular elements into a vector.\n\n3.  **Observed Correlation**: The model RDM vector, $v^{\\text{model}}$, and the brain RDM vector, $v^{\\text{brain}}$, are computed. The observed Spearman correlation, $r_{\\text{obs}}$, is then calculated from these two vectors.\n\n4.  **Permutation Loop**:\n    a. A random number generator is seeded with the specified permutation seed for the case.\n    b. A loop runs for $B$ iterations. In each iteration, a random permutation of the $n$ condition indices is generated.\n    c. This permutation is applied to the rows and columns of the brain RDM, $D_{\\text{brain}}$, to create a permuted RDM, $D'_{\\text{brain}}$.\n    d. The upper-triangular vector of $D'_{\\text{brain}}$ is extracted and its Spearman correlation with the original (fixed) $v^{\\text{model}}$ is computed, yielding one $r_{\\text{perm}}$.\n\n5.  **p-value Computation**: After the loop, the number of permutations where $|r_{\\text{perm}}| \\ge |r_{\\text{obs}}|$ is counted. This count, plus $1$, is divided by $B+1$ to obtain the final two-sided $p$-value for the test case.\n\nThis procedure will be repeated for all $4$ specified test cases, and the resulting $p$-values will be collected and formatted into the required output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.stats import spearmanr\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for Representational Similarity Analysis.\n    \"\"\"\n\n    def compute_rdm_and_vector(data_matrix):\n        \"\"\"\n        Computes the RDM using Euclidean distance and returns the upper-triangular vector.\n        \n        Args:\n            data_matrix (np.ndarray): An (n_conditions x n_features) matrix.\n        \n        Returns:\n            tuple: A tuple containing:\n                - rdm (np.ndarray): The n_conditions x n_conditions RDM.\n                - rdm_vec (np.ndarray): The flattened upper-triangular vector of the RDM.\n        \"\"\"\n        if data_matrix.shape[0]  2:\n            return np.array([[]]), np.array([])\n            \n        distances = pdist(data_matrix, metric='euclidean')\n        rdm = squareform(distances)\n        \n        n_conditions = data_matrix.shape[0]\n        # Get indices for the upper triangle, excluding the diagonal (k=1)\n        triu_indices = np.triu_indices(n_conditions, k=1)\n        rdm_vec = rdm[triu_indices]\n        \n        return rdm, rdm_vec\n\n    def run_rsa_permutation_test(model_features, brain_patterns, n_permutations, perm_seed):\n        \"\"\"\n        Performs the full RSA permutation test for a given model and brain data.\n\n        Args:\n            model_features (np.ndarray): The model's feature matrix (n_conditions x d_model).\n            brain_patterns (np.ndarray): The brain data matrix (n_conditions x n_voxels).\n            n_permutations (int): The number of permutations (B).\n            perm_seed (int): The seed for the permutation random number generator.\n\n        Returns:\n            float: The two-sided permutation p-value.\n        \"\"\"\n        n_conditions = model_features.shape[0]\n        \n        # 1. Compute RDMs and upper-triangular vectors\n        model_rdm, v_model = compute_rdm_and_vector(model_features)\n        brain_rdm, v_brain = compute_rdm_and_vector(brain_patterns)\n        \n        # 2. Compute observed Spearman correlation\n        if v_model.size == 0 or v_brain.size == 0:\n            return 1.0 # Cannot compute correlation\n        r_obs, _ = spearmanr(v_model, v_brain)\n        \n        # 3. Perform permutation test\n        rng_perm = np.random.default_rng(perm_seed)\n        n_exceeding = 0\n        \n        for _ in range(n_permutations):\n            # Generate a permutation of condition labels\n            perm_indices = rng_perm.permutation(n_conditions)\n            \n            # Permute the brain RDM by reordering rows and columns\n            permuted_brain_rdm = brain_rdm[perm_indices][:, perm_indices]\n            \n            # Get the upper-triangular vector of the permuted RDM\n            triu_indices = np.triu_indices(n_conditions, k=1)\n            v_brain_perm = permuted_brain_rdm[triu_indices]\n            \n            # Compute the null correlation\n            r_perm, _ = spearmanr(v_model, v_brain_perm)\n            \n            # Compare absolute values for a two-sided test\n            if abs(r_perm) >= abs(r_obs):\n                n_exceeding += 1\n        \n        # 4. Compute the p-value with continuity correction\n        p_value = (1 + n_exceeding) / (1 + n_permutations)\n        \n        return p_value\n\n    test_cases = [\n        # Case 1 (aligned model and brain; cluster structure)\n        {\n            \"n\": 8, \"v\": 50, \"d\": 2, \"B\": 2000,\n            \"model_features_coords\": [\n                (0.0, 0.0), (0.0, 0.5), (0.5, 0.0), (0.5, 0.5),\n                (3.0, 3.0), (3.0, 3.5), (3.5, 3.0), (3.5, 3.5)\n            ],\n            \"gen_brain\": {\n                \"method\": \"linear_map\",\n                \"W_seed\": 42, \"noise_sigma\": 0.3, \"noise_seed\": 24\n            },\n            \"perm_seed\": 1001\n        },\n        # Case 2 (null case; independent brain patterns)\n        {\n            \"n\": 8, \"v\": 50, \"d\": 2, \"B\": 2000,\n            \"model_features_coords\": [\n                (0.0, 0.0), (0.0, 0.5), (0.5, 0.0), (0.5, 0.5),\n                (3.0, 3.0), (3.0, 3.5), (3.5, 3.0), (3.5, 3.5)\n            ],\n            \"gen_brain\": {\n                \"method\": \"independent_normal\",\n                \"X_seed\": 99\n            },\n            \"perm_seed\": 2002\n        },\n        # Case 3 (boundary case; minimal number of conditions)\n        {\n            \"n\": 3, \"v\": 30, \"d\": 1, \"B\": 1000,\n            \"model_features_coords\": [[0], [1], [2]],\n            \"gen_brain\": {\n                \"method\": \"linear_map\",\n                \"W_seed\": 7, \"noise_sigma\": 0.05, \"noise_seed\": 31\n            },\n            \"perm_seed\": 3003\n        },\n        # Case 4 (tied distances in the model RDM; categorical structure)\n        {\n            \"n\": 6, \"v\": 40, \"d\": 1, \"B\": 2000,\n            \"model_features_coords\": [[0], [0], [0], [1], [1], [1]],\n            \"gen_brain\": {\n                \"method\": \"linear_map\",\n                \"W_seed\": 123, \"noise_sigma\": 0.1, \"noise_seed\": 321\n            },\n            \"perm_seed\": 4004\n        }\n    ]\n\n    p_values = []\n    \n    for case in test_cases:\n        # Generate Model Features\n        M = np.array(case[\"model_features_coords\"])\n        \n        # Generate Brain Patterns\n        n, v, d = case[\"n\"], case[\"v\"], case[\"d\"]\n        gen_params = case[\"gen_brain\"]\n        \n        if gen_params[\"method\"] == \"linear_map\":\n            rng_w = np.random.default_rng(gen_params[\"W_seed\"])\n            W = rng_w.standard_normal(size=(d, v))\n            \n            rng_noise = np.random.default_rng(gen_params[\"noise_seed\"])\n            noise = rng_noise.normal(loc=0, scale=gen_params[\"noise_sigma\"], size=(n, v))\n            \n            X = M @ W + noise\n        elif gen_params[\"method\"] == \"independent_normal\":\n            rng_x = np.random.default_rng(gen_params[\"X_seed\"])\n            X = rng_x.standard_normal(size=(n, v))\n\n        # Run the permutation test\n        p_val = run_rsa_permutation_test(M, X, case[\"B\"], case[\"perm_seed\"])\n        p_values.append(p_val)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, p_values))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Imagine your model shows a statistically significant correlation with the brain data. The next question is practical: how good is this correlation? To interpret the magnitude of your model's performance, you need a benchmark. This practice introduces the crucial concept of the \"noise ceiling,\" which estimates the theoretical limit of performance any model could achieve given the inherent noise and variability in the experimental data. By implementing a leave-one-subject-out procedure to compute the lower bound of the noise ceiling , you will establish a data-driven benchmark that contextualizes your model's explanatory power.",
            "id": "4148180",
            "problem": "You are given a collection of multi-condition neural response data across multiple subjects, and your task is to construct the Representational Dissimilarity Matrix (RDM) for each subject and estimate the lower noise ceiling using a leave-one-subject-out aggregation. The Representational Dissimilarity Matrix (RDM) is defined from the Representational Similarity Analysis (RSA) framework and encodes the pairwise dissimilarities between condition-specific response vectors. Work entirely in purely mathematical and algorithmic terms.\n\nFundamental base:\n- The data of a single subject is represented as a matrix $X^{(s)} \\in \\mathbb{R}^{K \\times V}$ with $K$ conditions and $V$ features (for example, voxel intensities), where each row $x_{i}^{(s)} \\in \\mathbb{R}^{V}$ corresponds to the response vector for condition $i$ of subject $s$.\n- The Pearson correlation coefficient $\\rho(x, y)$ between two vectors $x \\in \\mathbb{R}^{V}$ and $y \\in \\mathbb{R}^{V}$ is a well-tested measure of statistical association.\n- The Spearman Rank Correlation Coefficient (SRCC) between two equal-length numerical vectors is a well-tested measure that assesses monotonic association, robust to linear rescaling.\n\nFrom these definitions, construct the RDM for each subject $s$ as a symmetric matrix $D^{(s)} \\in \\mathbb{R}^{K \\times K}$ with zero diagonals, where the $(i,j)$ entry, for $i \\neq j$, is the dissimilarity $d_{ij}^{(s)}$ computed from the condition vectors $x_{i}^{(s)}$ and $x_{j}^{(s)}$ via the correlation distance:\n$$\nd_{ij}^{(s)} \\;=\\; 1 - \\rho\\big(x_{i}^{(s)}, \\, x_{j}^{(s)}\\big).\n$$\nTo compare RDMs, work with the vectorized form $u^{(s)} \\in \\mathbb{R}^{K(K-1)/2}$ obtained by stacking the strict upper-triangular entries of $D^{(s)}$ (i.e., indices $(i,j)$ with $1 \\le i  j \\le K$) into a vector.\n\nDefine the Leave-One-Subject-Out (LOSO) group-average RDM for subject $s$ as the arithmetic mean of the RDMs of all subjects except $s$:\n$$\n\\overline{D}^{(-s)} \\;=\\; \\frac{1}{S-1}\\sum_{\\substack{t=1 \\\\ t\\neq s}}^{S} D^{(t)},\n$$\nand its vectorization $\\overline{u}^{(-s)}$ over the strict upper triangle. Using the Spearman Rank Correlation Coefficient (SRCC), compute, for each subject $s$, the correlation $r_s$ between $u^{(s)}$ and $\\overline{u}^{(-s)}$. The lower noise ceiling estimate is then the arithmetic mean of these correlations across subjects:\n$$\n\\widehat{c}_{\\text{lower}} \\;=\\; \\frac{1}{S}\\sum_{s=1}^{S} r_s.\n$$\nInterpretation requirement: After computing $\\widehat{c}_{\\text{lower}}$, explain in the solution why this is a lower bound on the achievable model-to-data correlation for any fixed model evaluated against individual-subject RDMs, and why the leave-one-subject-out procedure avoids circularity.\n\nAlgorithmic task:\n- Implement the above steps to compute $\\widehat{c}_{\\text{lower}}$ using Spearman rank correlation on the vectorized upper triangles of the subject RDMs and their LOSO group-average RDMs.\n- Construct the subject-level data matrices from the following test suite by generating condition response matrices $X^{(s)}$ using seeded pseudorandom numbers, or explicit deterministic matrices, as specified. For each test case, report a single floating-point number equal to $\\widehat{c}_{\\text{lower}}$.\n\nTest suite:\n- Test case $1$ (happy path with heterogeneous noise): $S=5$, $K=6$, $V=30$. Let $B \\in \\mathbb{R}^{K \\times V}$ be a base matrix with entries drawn from a standard normal distribution using seed $42$. For each subject $s \\in \\{1,2,3,4,5\\}$, let independent noise $N^{(s)} \\in \\mathbb{R}^{K \\times V}$ be standard normal with seeds $1,2,3,4,5$, respectively, and set $X^{(s)} = B + \\sigma_s N^{(s)}$ with $\\sigma = [0.2,\\,0.25,\\,0.3,\\,0.35,\\,0.4]$.\n- Test case $2$ (boundary with $S=2$): $S=2$, $K=5$, $V=20$. Generate $X^{(1)}$ entries as standard normal with seed $123$. Let $N^{(2)}$ be standard normal with seed $456$, and set $X^{(2)} = X^{(1)} + 0.1\\,N^{(2)}$.\n- Test case $3$ (edge with identical subjects): $S=3$, $K=4$, $V=15$. Generate a base $B$ from a standard normal with seed $789$ and set $X^{(1)}=X^{(2)}=X^{(3)}=B$.\n- Test case $4$ (ties in dissimilarities via structured design): $S=4$, $K=4$, $V=8$. Define a deterministic base matrix $B$ by rows:\n  - Row $1$: $[0,\\,1,\\,2,\\,3,\\,0,\\,1,\\,2,\\,3]$,\n  - Row $2$: $[3,\\,2,\\,1,\\,0,\\,3,\\,2,\\,1,\\,0]$,\n  - Row $3$: $[1,\\,1,\\,1,\\,1,\\,2,\\,2,\\,2,\\,2]$,\n  - Row $4$: $[2,\\,2,\\,2,\\,2,\\,1,\\,1,\\,1,\\,1]$.\n  Set $X^{(1)}=X^{(2)}=X^{(3)}=X^{(4)}=B$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each element is the computed $\\widehat{c}_{\\text{lower}}$ for the corresponding test case in the order given above.\n- All numbers are unitless scalars. No angles are involved.\n\nEnsure scientific realism by faithfully following the definitions and generation procedures above. Avoid any shortcuts or hard-coded formulae; derive and implement each step from the stated base definitions. The program must be complete and runnable without external input.",
            "solution": "The problem presents a valid and well-posed task from the domain of computational neuroscience, specifically Representational Similarity Analysis (RSA). The objective is to compute the lower noise ceiling, $\\widehat{c}_{\\text{lower}}$, for a collection of multi-subject neural response datasets. This metric provides a benchmark for evaluating computational models by estimating the highest possible correlation a model can achieve with the noisy experimental data. The procedure is grounded in established statistical methods and follows a standard cross-validation paradigm.\n\nThe algorithmic process is as follows:\n\nFirst, we construct a Representational Dissimilarity Matrix (RDM) for each subject. For a given subject $s$ out of a total of $S$ subjects, the data is provided as a matrix $X^{(s)} \\in \\mathbb{R}^{K \\times V}$, where $K$ is the number of experimental conditions and $V$ is the number of features (e.g., voxels in fMRI, neurons in electrophysiology). Each row $x_i^{(s)}$ of $X^{(s)}$ is the response pattern vector for condition $i$. The dissimilarity between the neural responses to two different conditions, $i$ and $j$, is defined using the correlation distance. We compute the Pearson correlation coefficient $\\rho(x_i^{(s)}, x_j^{(s)})$ between the corresponding response vectors. The dissimilarity is then given by:\n$$\nd_{ij}^{(s)} = 1 - \\rho(x_i^{(s)}, x_j^{(s)})\n$$\nThese pairwise dissimilarities for all $i, j \\in \\{1, \\dots, K\\}$ form the entries of a $K \\times K$ symmetric matrix $D^{(s)}$, which is the RDM for subject $s$. By definition, the diagonal entries are $d_{ii}^{(s)} = 1 - \\rho(x_i^{(s)}, x_i^{(s)}) = 1 - 1 = 0$.\n\nSecond, to facilitate comparison between RDMs, we linearize them. The strictly upper-triangular part of each RDM $D^{(s)}$ (i.e., entries $d_{ij}^{(s)}$ where $i  j$) is extracted and reshaped into a vector $u^{(s)} \\in \\mathbb{R}^{M}$, where $M = K(K-1)/2$ is the number of unique pairwise dissimilarities.\n\nThird, we implement the Leave-One-Subject-Out (LOSO) cross-validation procedure to estimate the group-level representation while avoiding circularity. For each subject $s$, we compute an average RDM using data from all other subjects. This LOSO group-average RDM is defined as:\n$$\n\\overline{D}^{(-s)} = \\frac{1}{S-1} \\sum_{\\substack{t=1 \\\\ t \\neq s}}^{S} D^{(t)}\n$$\nThis average matrix $\\overline{D}^{(-s)}$ is also vectorized into $\\overline{u}^{(-s)}$ by extracting its upper-triangular elements.\n\nFourth, we quantify the similarity between the individual subject's RDM and the corresponding LOSO group-average RDM. This is done by computing the Spearman Rank Correlation Coefficient (SRCC) between their vectorized forms:\n$$\nr_s = \\text{SRCC}(u^{(s)}, \\overline{u}^{(-s)})\n$$\nThe Spearman correlation is used instead of Pearson because it assesses the monotonic relationship between the two sets of dissimilarities, making it robust to non-linear but monotonic transformations of the dissimilarity measure. This is a desirable property, as the group-average RDM $\\overline{D}^{(-s)}$ will have a different scale of dissimilarities than the individual subject's RDM $D^{(s)}$.\n\nFinally, the lower noise ceiling, $\\widehat{c}_{\\text{lower}}$, is estimated by taking the arithmetic mean of these individual correlation values across all subjects:\n$$\n\\widehat{c}_{\\text{lower}} = \\frac{1}{S} \\sum_{s=1}^{S} r_s\n$$\n\nInterpretation of the Lower Noise Ceiling and LOSO Procedure:\n\nThe term \"noise ceiling\" refers to the theoretical limit on the performance of any model in explaining the data. The data itself is noisy; thus, even a \"perfect\" model of the underlying neural representations would not perfectly correlate with the measured data. The noise ceiling estimates this limit.\n\nThe lower bound, $\\widehat{c}_{\\text{lower}}$, is derived from the principle that any individual subject's RDM, $D^{(s)}$, is a noisy sample of a hypothetical \"true\" common representational structure. The LOSO-averaged RDM, $\\overline{D}^{(-s)}$, is another, independent sample of this true structure. By averaging across multiple subjects (all except $s$), the idiosyncratic noise of each subject is reduced, making $\\overline{D}^{(-s)}$ a more reliable, though still imperfect, estimate of the true RDM than any single subject's RDM. The correlation $r_s$ between these two noisy estimates, $u^{(s)}$ and $\\overline{u}^{(-s)}$, is therefore expected to be lower than the correlation between the individual subject's RDM $u^{(s)}$ and the hypothetical noiseless true RDM. Averaging these values gives $\\widehat{c}_{\\text{lower}}$, which represents the expected correlation between a single subject's RDM and the RDM of a comparable, independent group. It serves as a \"lower bound\" because any credible model aiming to explain the neural data should at least be able to predict an individual's representational geometry as well as the average of other individuals can.\n\nThe Leave-One-Subject-Out (LOSO) procedure is critical for ensuring the statistical validity of this estimate. A circular analysis, or \"double-dipping,\" would occur if we were to correlate a subject's RDM $D^{(s)}$ with a group average that included $D^{(s)}$ itself. This would artificially inflate the correlation, as $D^{(s)}$ would be correlated with a benchmark containing a component of itself. The LOSO method rigorously avoids this by constructing the benchmark $\\overline{D}^{(-s)}$ from a set of data that is completely independent of the data $X^{(s)}$ being evaluated. This cross-validation approach yields an unbiased estimate of the consistency of representational geometries across the subject pool, providing a sound foundation for model evaluation.\n\nThe full algorithm proceeds by applying these steps to the datasets specified in the test suite. Each case involves generating the subject data matrices, computing all subject RDMs, and then executing the LOSO loop to find each $r_s$ and their final average $\\widehat{c}_{\\textlower}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\nfrom scipy.spatial.distance import squareform\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def compute_lower_noise_ceiling(S, K, V, data_matrices):\n        \"\"\"\n        Computes the lower noise ceiling for a list of subject data matrices.\n\n        Args:\n            S (int): Number of subjects.\n            K (int): Number of conditions.\n            V (int): Number of features.\n            data_matrices (list of np.ndarray): A list of S data matrices,\n                                                 each of shape (K, V).\n\n        Returns:\n            float: The computed lower noise ceiling, c_lower_hat.\n        \"\"\"\n        if S  2:\n            # LOSO procedure is not well-defined for S  2.\n            # Per the problem constraints, all test cases have S >= 2.\n            # Returning NaN for robustness in a general function.\n            return np.nan\n\n        # Step 1  2: Compute RDM and vectorize for each subject.\n        rdm_vectors = []\n        for s in range(S):\n            X_s = data_matrices[s]\n            # Compute Pearson correlation matrix between conditions (rows).\n            corr_matrix = np.corrcoef(X_s)\n            # Define RDM D_s as 1 - correlation matrix.\n            D_s = 1 - corr_matrix\n            # Vectorize the strict upper-triangular part of D_s.\n            u_s = squareform(D_s, checks=False)\n            rdm_vectors.append(u_s)\n        \n        rdm_vectors = np.array(rdm_vectors)\n\n        # Step 3  4: LOSO procedure and Spearman correlation.\n        r_s_values = []\n        for s in range(S):\n            # The RDM vector for the left-out subject.\n            u_s = rdm_vectors[s]\n            \n            # The RDM vectors for all other subjects.\n            u_others_indices = [i for i in range(S) if i != s]\n            u_others = rdm_vectors[u_others_indices]\n            \n            # Compute the LOSO group-average RDM vector.\n            u_bar_minus_s = np.mean(u_others, axis=0)\n            \n            # Compute Spearman correlation between u_s and u_bar_minus_s.\n            # We only need the correlation coefficient, not the p-value.\n            r_s, _ = stats.spearmanr(u_s, u_bar_minus_s)\n            \n            # Handle potential NaN if an input vector is constant.\n            if np.isnan(r_s):\n                # If a vector is constant, its correlation with any other\n                # vector is undefined (NaN). If both are constant, also NaN.\n                # If one vector is constant and the other is not, it should be 0,\n                # but scipy may return NaN. A single constant vector has 0 variance.\n                # A more robust check might be needed for pathological cases,\n                # but problem test cases avoid this. If they are identical vectors\n                # (and not constant), correlation is 1. If both are constant,\n                # they are perfectly related, so we can treat this as 1.\n                if np.all(u_s == u_s[0]) and np.all(u_bar_minus_s == u_bar_minus_s[0]):\n                    r_s = 1.0\n                else: # Should not happen in this problem's cases.\n                    r_s = 0.0\n\n            r_s_values.append(r_s)\n            \n        # Step 5: Compute the final lower noise ceiling estimate.\n        c_lower_hat = np.mean(r_s_values)\n        \n        return c_lower_hat\n\n    test_case_results = []\n\n    # --- Test Case 1 ---\n    S1, K1, V1 = 5, 6, 30\n    rng_base = np.random.default_rng(42)\n    B1 = rng_base.standard_normal((K1, V1))\n    noise_seeds = [1, 2, 3, 4, 5]\n    sigmas = [0.2, 0.25, 0.3, 0.35, 0.4]\n    data_matrices1 = []\n    for s in range(S1):\n        rng_noise = np.random.default_rng(noise_seeds[s])\n        N_s = rng_noise.standard_normal((K1, V1))\n        X_s = B1 + sigmas[s] * N_s\n        data_matrices1.append(X_s)\n    result1 = compute_lower_noise_ceiling(S1, K1, V1, data_matrices1)\n    test_case_results.append(result1)\n\n    # --- Test Case 2 ---\n    S2, K2, V2 = 2, 5, 20\n    rng1 = np.random.default_rng(123)\n    X1 = rng1.standard_normal((K2, V2))\n    rng2 = np.random.default_rng(456)\n    N2 = rng2.standard_normal((K2, V2))\n    X2 = X1 + 0.1 * N2\n    data_matrices2 = [X1, X2]\n    result2 = compute_lower_noise_ceiling(S2, K2, V2, data_matrices2)\n    test_case_results.append(result2)\n\n    # --- Test Case 3 ---\n    S3, K3, V3 = 3, 4, 15\n    rng_base3 = np.random.default_rng(789)\n    B3 = rng_base3.standard_normal((K3, V3))\n    data_matrices3 = [B3, B3, B3]\n    result3 = compute_lower_noise_ceiling(S3, K3, V3, data_matrices3)\n    test_case_results.append(result3)\n\n    # --- Test Case 4 ---\n    S4, K4, V4 = 4, 4, 8\n    B4 = np.array([\n        [0, 1, 2, 3, 0, 1, 2, 3],\n        [3, 2, 1, 0, 3, 2, 1, 0],\n        [1, 1, 1, 1, 2, 2, 2, 2],\n        [2, 2, 2, 2, 1, 1, 1, 1]\n    ], dtype=float)\n    data_matrices4 = [B4, B4, B4, B4]\n    result4 = compute_lower_noise_ceiling(S4, K4, V4, data_matrices4)\n    test_case_results.append(result4)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in test_case_results)}]\")\n\nsolve()\n```"
        }
    ]
}