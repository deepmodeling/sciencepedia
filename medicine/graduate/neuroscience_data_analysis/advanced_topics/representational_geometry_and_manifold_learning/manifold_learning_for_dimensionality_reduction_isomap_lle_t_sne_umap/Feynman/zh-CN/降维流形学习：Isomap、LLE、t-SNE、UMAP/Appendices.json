{
    "hands_on_practices": [
        {
            "introduction": "将流形学习从理论应用于实践，关键在于理解算法行为背后的“为什么”，并掌握其应用与评估。本章的第一个练习  将带你深入 t-SNE 算法的核心，通过一个思想实验剖析其目标函数——Kullback-Leibler 散度。你将清晰地理解 t-SNE 为何能出色地保留局部结构，而这又如何导致其对全局距离的表示产生扭曲。",
            "id": "4176778",
            "problem": "一次神经群体记录产生了一个高维数据集，其中每个样本是一个由 $N$ 个同时记录的神经元的放电率组成的向量。假设 $N$ 很大（例如，$N$ 为数百），并且样本是在多种行为背景下收集的，产生了两个分离良好的神经状态流形：流形 $\\mathcal{M}_{1}$（例如，准备状态）和流形 $\\mathcal{M}_{2}$（例如，执行状态）。根据经验，对于同一流形内的点 $i$ 和 $j$，其高维欧几里得距离很小；而对于跨流形的点，其距离则很大。你应用 t-分布随机邻域嵌入 (t-SNE) 进行降维，并使用一个为捕捉局部邻域（例如，大约 $k$ 个最近邻）而调整的标准困惑度。令 $P_{ij}$ 表示在高维空间中由高斯核导出并经过对称化处理以满足 $\\sum_{i\\neq j} P_{ij} = 1$ 的成对相似度，令 $Q_{ij}$ 表示在低维嵌入空间中由学生 $t$-分布导出并经过对称化处理以满足 $\\sum_{i\\neq j} Q_{ij} = 1$ 的成对相似度。t-SNE 的目标是 Kullback–Leibler 散度\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right).\n$$\n考虑以下为反映此数据集中典型结果而构建的受控情景：\n- 对于一个代表性的流形内邻近点对，$P_{ij} \\approx 5 \\times 10^{-4}$，而嵌入后的低维相似度为 $Q_{ij} \\approx 4 \\times 10^{-4}$。\n- 对于一个代表性的跨流形远点对，$P_{ij} \\approx 1 \\times 10^{-8}$，而由于低维空间中学生 $t$-核的重尾特性和有限的嵌入空间，$Q_{ij} \\approx 1 \\times 10^{-4}$。\n\n要求你从第一性原理出发，利用上述主导目标、基于高斯核的 $P_{ij}$ 和基于学生 $t$-分布的 $Q_{ij}$ 的定性属性以及该经验情景，推理 t-SNE 如何在局部邻域保持与全局距离失真之间取得平衡。哪个选项最能解释为什么局部邻域得以保持而全局距离被扭曲，并正确地将其与远点对的 $P_{ij}$ 和 $Q_{ij}$ 之间的不匹配联系起来？\n\nA. 散度 $D_{\\mathrm{KL}}(P \\parallel Q)$ 通过 $P_{ij}$ 对差异进行加权，因此近邻点对（具有较大的 $P_{ij}$）在目标函数中占主导地位。对于给定的流形内点对，其贡献为 $P_{ij} \\log(P_{ij}/Q_{ij}) \\approx (5 \\times 10^{-4}) \\log\\left(\\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}}\\right)$，这比来自遥远的跨流形点对的贡献 $(1 \\times 10^{-8}) \\log\\left(\\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}}\\right)$ 大几个数量级。因为远点对的 $P_{ij} \\approx 0$，它们与 $Q_{ij}$ 的不匹配受到的惩罚很弱，从而允许全局距离被扭曲，同时保持局部邻域。\n\nB. t-SNE 中最小化的是散度 $D_{\\mathrm{KL}}(Q \\parallel P)$，因此差异由 $Q_{ij}$ 加权，这会严重惩罚分配给远点对的较大 $Q_{ij}$ 值，并防止全局失真。因此，近邻保持是最小化 $D_{\\mathrm{KL}}(Q \\parallel P)$ 的结果。\n\nC. 保持全局测地线距离的流形学习方法，如等度量映射 (Isomap)，通过计算邻域图上的最短路径来防止全局失真；由于 t-SNE 隐式地执行了类似的测地线计算，因此全局距离得以保持，而局部邻域可能被扭曲。\n\nD. 在 t-SNE 中增加困惑度会迫使 $P_{ij}$ 在所有点对上几乎均匀分布，使得 $D_{\\mathrm{KL}}(P \\parallel Q)$ 对近点对和远点对的加权几乎相等。这消除了全局失真，同时保持了局部邻域。\n\nE. 在低维空间中，用于构建 $Q_{ij}$ 的学生 $t$-核是短尾的，这使得除了非常近的邻居外，所有点对的 $Q_{ij}$ 都趋近于 0。这确保了全局距离得以保持，因为远点对的 $P_{ij}$ 和 $Q_{ij}$ 都接近于零，从而使所有点对的散度都保持很小。\n\n选择唯一的最佳选项。",
            "solution": "问题要求解释为什么 t-分布随机邻域嵌入 (t-SNE) 算法在保持局部邻域结构的同时会扭曲全局距离。解释必须从其目标函数——Kullback-Leibler (KL) 散度的性质以及所提供的数值情景中得出。\n\n### 第一步：问题陈述验证\n\n首先，我将验证问题陈述的有效性。\n\n**已知条件：**\n- 一个来自 $N$ 个神经元的高维神经放电率数据集。\n- 数据形成两个分离良好的流形 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$。\n- 高维相似度由 $P_{ij}$ 给出，其源自高斯核，经过对称化处理，并归一化以满足 $\\sum_{i\\neq j} P_{ij} = 1$。\n- 低维相似度由 $Q_{ij}$ 给出，其源自学生 $t$-分布，经过对称化处理，并归一化以满足 $\\sum_{i\\neq j} Q_{ij} = 1$。\n- t-SNE 需要最小化的目标函数是 KL 散度 $D_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right)$。\n- 一个代表性的流形内邻近点对具有 $P_{ij} \\approx 5 \\times 10^{-4}$ 和 $Q_{ij} \\approx 4 \\times 10^{-4}$。\n- 一个代表性的跨流形远点对具有 $P_{ij} \\approx 1 \\times 10^{-8}$ 和 $Q_{ij} \\approx 1 \\times 10^{-4}$。\n\n**验证：**\n1.  **科学依据**：问题陈述在科学上是合理的。它准确地描述了 t-SNE 算法，包括其目标函数、基于高斯核的高维相似度（$P_{ij}$）的定义，以及基于学生 $t$-分布的低维相似度（$Q_{ij}$）的定义。关于神经数据形成流形的描述是计算神经科学中的一个标准概念。所述的算法特性（以牺牲全局结构为代价来保持局部结构）是 t-SNE 广为人知的特点。\n2.  **良定性**：问题是良定的。它提供了一个明确的目标和足够的信息（代价函数的形式和数值示例）来推理其内在机制并评估给定的选项。\n3.  **客观性**：问题以精确、客观和技术性的语言陈述，没有任何主观性或模糊性。\n\n**结论：** 问题陈述有效。我将继续进行解答。\n\n### 第二步：推导与分析\n\nt-SNE 的目标是找到一个低维嵌入，以最小化代表高维空间相似度的联合概率分布 $P$ 与代表低维空间相似度的联合概率分布 $Q$ 之间的 KL 散度。目标函数为：\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right)\n$$\n这可以展开为：\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log(P_{ij}) - \\sum_{i \\neq j} P_{ij} \\log(Q_{ij})\n$$\n由于 $P_{ij}$ 值是固定的（从输入数据中导出），$\\sum_{i \\neq j} P_{ij} \\log(P_{ij})$ 项是一个常数。因此，最小化 $D_{\\mathrm{KL}}(P \\parallel Q)$ 等价于最大化交叉熵项 $\\sum_{i \\neq j} P_{ij} \\log(Q_{ij})$。\n\n分析的核心在于理解单个点对 $(i, j)$ 如何对总 KL 散度做出贡献。单个点对的贡献是 $C_{ij} = P_{ij} \\log(P_{ij}/Q_{ij})$。优化过程通过调整低维空间中点的位置来改变 $Q_{ij}$ 的值，以便最小化所有这些贡献的总和。\n\n这个代价函数的关键特征是前置因子 $P_{ij}$。该因子作为对数项 $\\log(P_{ij}/Q_{ij})$ 的权重，该对数项衡量了相似度之间的“不匹配”程度。\n\n让我们分析所提供的两种情景：\n\n**1. 流形内邻近点对（局部结构）：**\n- 已知：$P_{ij} \\approx 5 \\times 10^{-4}$ 且 $Q_{ij} \\approx 4 \\times 10^{-4}$。\n- 在这里，$P_{ij}$ 相对较大，因为点 $i$ 和 $j$ 在高维空间中是近邻。\n- $P_{ij}$ 和 $Q_{ij}$ 之间的任何显著不匹配都将乘以这个相对较大的 $P_{ij}$，从而对代价产生重大贡献。例如，如果算法将这些点在低维图中放置得相距很远，$Q_{ij}$ 将变得非常小，使得 $\\log(P_{ij}/Q_{ij})$ 成为一个大的正数。由此产生的贡献 $P_{ij} \\log(P_{ij}/Q_{ij})$ 将会很大，从而在优化中产生强大的“力”来拉近这些点（从而增加 $Q_{ij}$ 以更好地匹配 $P_{ij}$）。\n- 因此，该算法对正确表示邻近点的相似度高度敏感。这就是保持局部结构的机制。\n\n**2. 跨流形远点对（全局结构）：**\n- 已知：$P_{ij} \\approx 1 \\times 10^{-8}$ 且 $Q_{ij} \\approx 1 \\times 10^{-4}$。\n- 在这里，$P_{ij}$ 极小，因为点 $i$ 和 $j$ 在高维空间中相距很远。\n- 对代价的贡献是 $C_{ij} = P_{ij} \\log(P_{ij}/Q_{ij})$。\n- 即使 $P_{ij}$ 和 $Q_{ij}$ 之间存在巨大的不匹配（如情景中所给出的），对总代价的贡献也会被微小的 $P_{ij}$ 加权。优化过程在很大程度上对这种不匹配不敏感。让我们定量地计算其近似贡献以观察这一点。\n\n来自近点对的贡献：\n$$\nC_{\\text{near}} = P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right) \\approx (5 \\times 10^{-4}) \\ln \\left( \\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}} \\right) = (5 \\times 10^{-4}) \\ln(1.25) \\approx (5 \\times 10^{-4})(0.223) \\approx 1.12 \\times 10^{-4}\n$$\n来自远点对的贡献：\n$$\nC_{\\text{far}} = P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right) \\approx (1 \\times 10^{-8}) \\ln \\left( \\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}} \\right) = (1 \\times 10^{-8}) \\ln(10^{-4}) \\approx (1 \\times 10^{-8})(-9.21) \\approx -9.21 \\times 10^{-8}\n$$\n近点对贡献的量级 ($|C_{\\text{near}}| \\approx 1.12 \\times 10^{-4}$) 大约是远点对贡献量级 ($|C_{\\text{far}}| \\approx 9.21 \\times 10^{-8}$) 的 1200 倍。\n\n这证实了目标函数主要由邻近点（其中 $P_{ij}$ 较大）的贡献所主导。算法将其精力用于正确排列这些点。对于错误表示远点（其中 $P_{ij}$ 几乎为零）之间的距离，算法受到的惩罚很弱，从而允许全局结构的显著扭曲。\n\n### 第三步：逐项分析\n\n**A. 散度 $D_{\\mathrm{KL}}(P \\parallel Q)$ 通过 $P_{ij}$ 对差异进行加权，因此近邻点对（具有较大的 $P_{ij}$）在目标函数中占主导地位。对于给定的流形内点对，其贡献为 $P_{ij} \\log(P_{ij}/Q_{ij}) \\approx (5 \\times 10^{-4}) \\log\\left(\\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}}\\right)$，这比来自遥远的跨流形点对的贡献 $(1 \\times 10^{-8}) \\log\\left(\\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}}\\right)$ 大几个数量级。因为远点对的 $P_{ij} \\approx 0$，它们与 $Q_{ij}$ 的不匹配受到的惩罚很弱，从而允许全局距离被扭曲，同时保持局部邻域。**\n- 该选项正确地指出代价函数由 $P_{ij}$ 加权。它正确地得出结论，这导致近点对（较大的 $P_{ij}$）的贡献在目标函数中占主导地位。数值计算证实了近点对的贡献比远点对的贡献大几个数量级。最后的结论——这种加权方式对全局不匹配的惩罚很弱，从而允许全局距离失真，同时保持局部邻域——是上述分析的直接且准确的结果。\n- **结论：正确。**\n\n**B. t-SNE 中最小化的是散度 $D_{\\mathrm{KL}}(Q \\parallel P)$，因此差异由 $Q_{ij}$ 加权，这会严重惩罚分配给远点对的较大 $Q_{ij}$ 值，并防止全局失真。因此，近邻保持是最小化 $D_{\\mathrm{KL}}(Q \\parallel P)$ 的结果。**\n- 该选项在事实上是错误的。t-SNE 算法最小化的是 $D_{\\mathrm{KL}}(P \\parallel Q)$，而不是 $D_{\\mathrm{KL}}(Q \\parallel P)$。这两种 KL 散度是不对称的，并且具有非常不同的性质。这里提出的逻辑适用于最小化反向 KL 散度，但 t-SNE 并非如此。\n- **结论：错误。**\n\n**C. 保持全局测地线距离的流形学习方法，如等度量映射 (Isomap)，通过计算邻域图上的最短路径来防止全局失真；由于 t-SNE 隐式地执行了类似的测地线计算，因此全局距离得以保持，而局部邻域可能被扭曲。**\n- 该选项错误地描述了 t-SNE。虽然它正确地描述了 Isomap，但它错误地声称 t-SNE 执行了类似的测地线计算。t-SNE 对亲和度的计算是基于局部高斯核，而不是图中的最短路径距离。如上所示，其目标函数导致全局距离的扭曲，而非保持。\n- **结论：错误。**\n\n**D. 在 t-SNE 中增加困惑度会迫使 $P_{ij}$ 在所有点对上几乎均匀分布，使得 $D_{\\mathrm{KL}}(P \\parallel Q)$ 对近点对和远点对的加权几乎相等。这消除了全局失真，同时保持了局部邻域。**\n- 尽管非常高的困惑度确实会使 $P_{ij}$ 更加均匀，但其结论是有缺陷的。如果 $P_{ij}$ 是均匀的，它将不包含关于数据局部结构的任何信息。算法保持局部邻域的目标从一开始就被颠覆了。这种方法会破坏局部结构信息，而不是保持它。\n- **结论：错误。**\n\n**E. 在低维空间中，用于构建 $Q_{ij}$ 的学生 $t$-核是短尾的，这使得除了非常近的邻居外，所有点对的 $Q_{ij}$ 都趋近于 0。这确保了全局距离得以保持，因为远点对的 $P_{ij}$ 和 $Q_{ij}$ 都接近于零，从而使所有点对的散度都保持很小。**\n- 该选项对学生 $t$-分布做出了一个事实性错误的陈述。在 t-SNE 中使用的一自由度 $t$-分布是*重尾*的，而不是短尾的。其较慢的幂律衰减（与高斯分布的指数衰减相比）是一个关键特征，有助于分离簇并缓解“拥挤问题”。该选项基于核是短尾的这一根本错误的前提，因此由此得出的推理也是无效的。\n- **结论：错误。**\n\n分析证实，选项 A 为问题中所描述的 t-SNE 行为提供了唯一准确且完整的解释。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "在我们理解了 t-SNE 对局部结构的侧重之后，下一个关键问题是如何通过调整其最重要的超参数——困惑度（perplexity）——来优化可视化结果。本练习  演示了困惑度 $\\mathcal{P}$ 如何权衡数据的局部与全局特征。通过一个精心设计的场景，你将学会如何选择合适的困惑度，以分离精细的簇结构或连接更大尺度的流形。",
            "id": "4176842",
            "problem": "您正在分析一个来自小鼠视觉皮层的高维神经元响应数据集，其中每个神经元由一个从跨越 $M$ 个刺激的钙成像中提取的特征向量 $x_i \\in \\mathbb{R}^D$ 表示。该数据集包含群体 $\\mathcal{A}$ 中的 $n_{\\mathcal{A}}$ 个神经元（朝向选择性，其偏好角度平滑变化，形成一个带有稀疏区域的连续流形），以及两个类高斯群体 $\\mathcal{B}$ 和 $\\mathcal{C}$（方向选择性神经元，具有相似但不同的平均响应曲线），其数量分别为 $n_{\\mathcal{B}}$ 和 $n_{\\mathcal{C}}$。您计划使用 t-分布随机邻域嵌入 (t-SNE) 来嵌入数据，该方法定义了高维条件亲和度\n$$\np_{j|i} = \\frac{\\exp\\!\\left(-\\frac{\\|x_i - x_j\\|_2^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\!\\left(-\\frac{\\|x_i - x_k\\|_2^2}{2\\sigma_i^2}\\right)},\n$$\n其中局部带宽 $\\sigma_i$ 的选择使得困惑度与用户指定的目标值 $\\mathcal{P}$ 相匹配。困惑度通过条件分布 $P_i = \\{p_{j|i}\\}_j$ 的 Shannon 熵定义如下：\n$$\n\\mathcal{P} \\equiv 2^{H(P_i)}, \\quad H(P_i) = -\\sum_j p_{j|i} \\log_2 p_{j|i}.\n$$\n经验上，一个基于欧几里得距离 $\\|x_i - x_j\\|_2$ 构建的 k-最近邻（Nearest Neighbor, NN）图揭示了以下结构：\n- 对于群体 $\\mathcal{A}$，稀疏区域造成了一个瓶颈，使得 k-NN 图会碎裂成多个分量，除非 $k \\ge 12$；而当 $k \\ge 12$ 时，群体 $\\mathcal{A}$ 上的图是连通的。\n- 对于群体 $\\mathcal{B}$ 和 $\\mathcal{C}$，当 $k \\le 30$ 时，它们的 k-NN 图保持为两个独立的分量，但在 $k \\ge 50$ 时会合并成一个单一的连通分量。\n\n假设在 t-SNE 中，目标困惑度 $\\mathcal{P}$ 为大多数点设定了一个数量级约为 $\\mathcal{P}$ 的有效邻域大小，因此高维亲和度图的定性连通性行为与 $k \\approx \\mathcal{P}$ 的 k-NN 图的行为相呼应。您必须根据 $p_{j|i}$、$H(P_i)$ 和 $\\mathcal{P}$ 的定义进行推理，以确定 $\\mathcal{P}$ 的选择如何影响低维嵌入中的聚类粒度，特别是群体 $\\mathcal{A}$ 的碎裂以及群体 $\\mathcal{B}$ 与 $\\mathcal{C}$ 的合并。\n\n哪些陈述与上述机制最一致？\n\nA. 使用 $\\mathcal{P} = 5$ 会导致群体 $\\mathcal{A}$ 碎裂，并保持群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 分离。\n\nB. 使用 $\\mathcal{P} = 15$ 会导致群体 $\\mathcal{A}$ 碎裂，同时保持群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 分离。\n\nC. 使用 $\\mathcal{P} = 60$ 会保持群体 $\\mathcal{A}$ 连续，并将群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 合并成一个单一的聚类。\n\nD. 使用 $\\mathcal{P} = 30$ 会保持群体 $\\mathcal{A}$ 连续，并将群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 合并成一个单一的聚类。\n\n选择所有正确的选项。",
            "solution": "用户希望我验证并解决一个关于将 t-SNE 应用于神经科学数据集的问题。\n\n### 问题验证\n\n**1. 提取已知条件**\n- 数据：来自小鼠视觉皮层的神经元响应，每个神经元由一个特征向量 $x_i \\in \\mathbb{R}^D$ 表示。\n- 群体：\n    - $\\mathcal{A}$：$n_{\\mathcal{A}}$ 个神经元，朝向选择性，形成一个带有稀疏区域的连续流形。\n    - $\\mathcal{B}$：$n_{\\mathcal{B}}$ 个神经元，类高斯聚类。\n    - $\\mathcal{C}$：$n_{\\mathcal{C}}$ 个神经元，类高斯聚类，与 $\\mathcal{B}$ 不同但相近。\n- 算法：t-SNE，具有高维条件亲和度 $p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|_2^2 / (2\\sigma_i^2))}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|_2^2 / (2\\sigma_i^2))}$。\n- 困惑度：$\\mathcal{P}$ 是一个用户指定的参数，定义为 $\\mathcal{P} \\equiv 2^{H(P_i)}$，其中 $H(P_i) = -\\sum_j p_{j|i} \\log_2 p_{j|i}$。\n- 来自 k-最近邻（k-NN）图的经验数据结构：\n    - 群体 $\\mathcal{A}$：当 $k  12$ 时图碎裂，当 $k \\ge 12$ 时图连通。\n    - 群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：当 $k \\le 30$ 时图是分离的，当 $k \\ge 50$ 时图合并。\n- 核心假设：t-SNE 中的有效邻域大小与困惑度 $\\mathcal{P}$ 的数量级相当，并且 t-SNE 亲和度图的连通性反映了 $k \\approx \\mathcal{P}$ 的 k-NN 图的连通性。\n\n**2. 验证分析**\n- **科学依据：** 该问题牢固地植根于计算神经科学和机器学习。使用 t-SNE 可视化神经元群体结构是一种标准技术。t-SNE 亲和度和困惑度的数学定义是正确的。困惑度与有效邻域大小之间的关系是应用 t-SNE 时公认的启发式方法。该场景在科学上是合理的。\n- **良构性：** 该问题提供了所有必要信息：k-NN 分析揭示的数据结构、t-SNE 参数 $\\mathcal{P}$ 的定义，以及一个将参数与数据结构联系起来的清晰规则（$k \\approx \\mathcal{P}$）。问题是精确的，并允许一个唯一的、演绎的答案。\n- **客观性：** 问题使用精确、技术性的语言陈述，没有主观性或模糊性。\n\n**3. 结论**\n问题陈述在科学上是合理的、良构的且客观的。它是有效的。我将继续进行解答。\n\n### 解题推导\n\n该问题要求我们根据困惑度参数 $\\mathcal{P}$ 的选择来预测 t-SNE 嵌入的定性结构。关键是使用所提供的假设，即对于给定的 $\\mathcal{P}$，t-SNE 算法的行为可以由 k-最近邻（k-NN）图的行为来近似，其中邻居数 $k$ 约等于困惑度，即 $k \\approx \\mathcal{P}$。\n\n困惑度 $\\mathcal{P} \\equiv 2^{H(P_i)}$（其中 $H(P_i)$ 是 Shannon 熵）衡量了根据概率分布 $\\{p_{j|i}\\}_j$，每个点 $x_i$ 在高维空间中的有效邻居数量。低的 $\\mathcal{P}$ 意味着 t-SNE 关注非常局部的结构（少数邻居），而高的 $\\mathcal{P}$ 意味着它通过考虑更大的邻域来整合更多的全局结构。\n\n问题给出了使用 k-NN 图进行结构分析的结果：\n1.  **群体 $\\mathcal{A}$：** 该群体代表一个连续的流形。如果邻域大小太小而无法跨越稀疏区域，它就会碎裂。给出的临界邻域大小是 $k=12$。\n    - 对于 $k  12$（低 $k$），图会碎裂。\n    - 对于 $k \\ge 12$（足够高的 $k$），图是连通的。\n2.  **群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：** 这是两个不同但邻近的聚类。它们的分离或合并取决于邻域大小是小到足以保持在每个聚类内部，还是大到足以跨越两者。\n    - 对于 $k \\le 30$，聚类保持分离。\n    - 对于 $k \\ge 50$，聚类合并成一个单一分量。\n\n通过应用假设 $k \\approx \\mathcal{P}$，我们可以将这些 k-NN 阈值转换为 t-SNE 嵌入的困惑度阈值：\n- 要将群体 $\\mathcal{A}$ 嵌入为连续的流形，我们需要 $\\mathcal{P} \\gtrsim 12$。对于 $\\mathcal{P} \\lesssim 12$，它可能会碎裂。\n- 要将群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 嵌入为分离的聚类，我们需要 $\\mathcal{P} \\lesssim 30$。对于 $\\mathcal{P} \\gtrsim 50$，它们可能会合并。\n\n我们现在使用这些标准分析每个选项。\n\n**A. 使用 $\\mathcal{P} = 5$ 会导致群体 $\\mathcal{A}$ 碎裂，并保持群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 分离。**\n我们设定 $\\mathcal{P} = 5$，这意味着有效邻域大小为 $k \\approx 5$。\n- 对于群体 $\\mathcal{A}$：由于 $5  12$，邻域太小，无法跨越稀疏区域。这对应于 $k  12$ 的情况，预测会发生碎裂。陈述的第一部分是一致的。\n- 对于群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：由于 $5 \\le 30$，邻域足够小，不会跨越聚类之间的间隙。这对应于 $k \\le 30$ 的情况，预测会分离。陈述的第二部分是一致的。\n两个预测都与陈述相符。因此，选项 A 是 **正确的**。\n\n**B. 使用 $\\mathcal{P} = 15$ 会导致群体 $\\mathcal{A}$ 碎裂，同时保持群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 分离。**\n我们设定 $\\mathcal{P} = 15$，这意味着有效邻域大小为 $k \\approx 15$。\n- 对于群体 $\\mathcal{A}$：由于 $15 \\ge 12$，邻域足够大，可以跨越稀疏区域。这对应于 $k \\ge 12$ 的情况，预测会得到一个连通的、连续的表示。该陈述声称会碎裂，这是一个矛盾。\n- 对于群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：由于 $15 \\le 30$，这预测会分离，与陈述一致。\n然而，由于对群体 $\\mathcal{A}$ 的预测是错误的，整个选项是 **不正确的**。\n\n**C. 使用 $\\mathcal{P} = 60$ 会保持群体 $\\mathcal{A}$ 连续，并将群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 合并成一个单一的聚类。**\n我们设定 $\\mathcal{P} = 60$，这意味着有效邻域大小为 $k \\approx 60$。\n- 对于群体 $\\mathcal{A}$：由于 $60 \\ge 12$，邻域绰绰有余以确保连通性。这对应于 $k \\ge 12$ 的情况，预测会得到一个连续的表示。陈述的第一部分是一致的。\n- 对于群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：由于 $60 \\ge 50$，邻域足够大，可以跨越两个聚类。这对应于 $k \\ge 50$ 的情况，预测聚类将会合并。陈述的第二部分是一致的。\n两个预测都与陈述相符。因此，选项 C 是 **正确的**。\n\n**D. 使用 $\\mathcal{P} = 30$ 会保持群体 $\\mathcal{A}$ 连续，并将群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 合并成一个单一的聚类。**\n我们设定 $\\mathcal{P} = 30$，这意味着有效邻域大小为 $k \\approx 30$。\n- 对于群体 $\\mathcal{A}$：由于 $30 \\ge 12$，邻域足以确保连通性。这对应于 $k \\ge 12$ 的情况，预测会得到一个连续的表示。陈述的第一部分是一致的。\n- 对于群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：问题陈述对于 $k \\le 30$，聚类保持分离。因此，在边界 $k=30$ 时，它们仍然是分离的。该陈述声称它们会合并，这与提供的经验数据（合并发生在 $k \\ge 50$ 时）相矛盾。\n因为对群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 的预测是错误的，整个选项是 **不正确的**。",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "在生成低维嵌入后，我们如何客观地衡量其质量？最后一个练习  为此目的引入了一个强大而直观的评估指标。你将推导并应用一个基于秩相关的邻域保持指数 $R_{NX}(k)$，从而获得一个量化工具，用以评估低维映射在多大程度上忠实地保留了原始高维数据的局部邻域结构。",
            "id": "4176825",
            "problem": "一个实验室记录了在 $n=6$ 个重复条件下，清醒行为动物的 $p$ 个同时记录的神经元的群体活动向量，产生了 $n$ 个高维点 $\\{x_{i} \\in \\mathbb{R}^{p}\\}_{i=1}^{6}$。通过一种非线性降维方法（例如，均匀流形逼近与投影 (UMAP) 或 t-分布随机邻域嵌入 (t-SNE)），从 $\\{x_{i}\\}$ 计算出一个二维嵌入 $\\{y_{i} \\in \\mathbb{R}^{2}\\}_{i=1}^{6}$，旨在保留反映推定神经流形的局部邻域。\n\n令 $d_{X}(i,j)$ 和 $d_{Y}(i,j)$ 分别表示原始空间和嵌入空间中的成对距离，令 $r_{ij}$ 和 $s_{ij}$ 分别为当 $i$ 的邻居按 $d_{X}$ 和 $d_{Y}$ 距离升序排列时，$j$ 对应的排名，其中 $r_{ii}=s_{ii}=0$ 且对于 $i \\neq j$，$r_{ij},s_{ij} \\in \\{1,\\dots,n-1\\}$。对于固定的邻域大小 $k \\in \\{1,\\dots,n-1\\}$ 和每个查询索引 $i$，将原始空间中的 k-近邻索引集定义为 $J_{i}(k) = \\{ j \\neq i : r_{ij} \\leq k \\}$，并按 $r_{ij}$ 递增的顺序将其排列为 $j_{i,1},\\dots,j_{i,k}$，使得对于 $\\ell \\in \\{1,\\dots,k\\}$，$r_{i,j_{i,\\ell}} = \\ell$。令 $\\beta_{i,\\ell}(k)$ 为当项目 $\\{j_{i,1},\\dots,j_{i,k}\\}$ 按 $s_{ij}$ 递增排序时由 $s_{ij}$ 诱导的排名位置，因此 $\\beta_{i,\\ell}(k) \\in \\{1,\\dots,k\\}$ 是 $j_{i,\\ell}$ 在限于 $J_{i}(k)$ 的嵌入诱导排序中的位置。\n\n任务 1. 仅从斯皮尔曼等级相关系数在 $k$ 个项目上的两种排序的定义以及上述定义出发，推导出一个邻域保持指数 $R_{NX}(k)$ 的显式表达式，作为 k-邻域排序的平均斯皮尔曼相关性：\n- 对每个 $i$，定义“理想”排序 $[1,2,\\dots,k]$ 与嵌入诱导的排序 $[\\beta_{i,1}(k),\\dots,\\beta_{i,k}(k)]$ 之间的斯皮尔曼相关性 $\\rho_{i}(k)$，并证明它可以写成关于排名位移平方 $\\Delta_{i,\\ell}(k) = \\ell - \\beta_{i,\\ell}(k)$ 的闭式表达式。\n- 然后将 $R_{NX}(k)$ 定义为 $\\rho_{i}(k)$ 在 $i \\in \\{1,\\dots,n\\}$ 上的平均值。\n\n任务 2. 考虑以下经验观察到的 $n=6$ 个条件下 $k=3$ 的邻域排序。对于每个 $i \\in \\{1,\\dots,6\\}$，您将获得：\n- 原始空间中按 $d_{X}$-排名顺序排列的前3个邻居索引，$[j_{i,1},j_{i,2},j_{i,3}]$。\n- 同样是这三个索引，在嵌入空间中按 $d_{Y}$ 排序的顺序（限于 $J_{i}(3)$ 内），编码为相应的索引序列 $[\\widehat{j}_{i,1},\\widehat{j}_{i,2},\\widehat{j}_{i,3}]$。\n\n使用这些数据为每个 $i$ 计算 $\\beta_{i,\\ell}(3)$，然后精确计算 $R_{NX}(3)$（不要四舍五入）。\n\n数据：\n- $i=1$：原始 $[2,3,4]$，嵌入 $[2,3,4]$。\n- $i=2$：原始 $[1,3,5]$，嵌入 $[1,5,3]$。\n- $i=3$：原始 $[2,4,6]$，嵌入 $[6,4,2]$。\n- $i=4$：原始 $[1,2,5]$，嵌入 $[1,2,5]$。\n- $i=5$：原始 $[2,4,6]$，嵌入 $[2,6,4]$。\n- $i=6$：原始 $[3,4,5]$，嵌入 $[3,4,5]$。\n\n任务 3. 基于秩次统计和曲面流形上 k-近邻稳定性的基本原理，简要论证（不借助均匀流形逼近与投影 (UMAP) 或 t-分布随机邻域嵌入 (t-SNE) 的任何具体算法细节）随着 $k$ 的增加以及在观测噪声方差保持固定的情况下神经流形的采样密度降低，$R_{NX}(k)$ 预计将如何变化。\n\n您最终报告的结果必须是上述数据集的 $R_{NX}(3)$ 的精确值。不要包含单位。在最终答案框中不要包含任何额外的文本。",
            "solution": "该问题要求完成三个任务：第一，推导邻域保持指数 $R_{NX}(k)$ 的公式；第二，针对 $k=3$ 的特定数据集计算该指数；第三，对该指数的行为提供定性论证。该问题定义明确，并在流形学习和秩次统计的原理上有科学依据。\n\n### 任务 1：邻域保持指数 $R_{NX}(k)$ 的推导\n\n问题将指数 $R_{NX}(k)$ 定义为斯皮尔曼等级相关系数 $\\rho_i(k)$ 的均值，该系数为 $n$ 个数据点中的每一个计算。对于每个点 $i$，相关性是在其原始高维空间中的 $k$-近邻的排名顺序与同一邻居在低维嵌入中的排名顺序之间计算的。\n\n假设有 $k$ 个项目。它们在两种不同排序中的排名由向量 $u = [u_1, u_2, \\dots, u_k]$ 和 $v = [v_1, v_2, \\dots, v_k]$ 给出。斯皮尔曼等级相关系数 $\\rho$ 被定义为这些排名变量的皮尔逊相关系数。在没有平级的情况下（这里是这种情况，因为 $u$ 和 $v$ 是 $[1, 2, \\dots, k]$ 的排列），公式简化为：\n$$ \\rho = 1 - \\frac{6 \\sum_{j=1}^{k} d_j^2}{k(k^2-1)} $$\n其中 $d_j = u_j - v_j$ 是第 $j$ 个项目的排名差异。\n\n在我们的具体问题中，对于每个查询点 $i$，“项目”是其在原始空间中的 $k$-近邻，由集合 $J_i(k) = \\{j_{i,1}, \\dots, j_{i,k}\\}$ 给出。\n第一个排序是“理想”排序，源自原始空间距离 $d_X$。邻居 $j_{i,\\ell}$ 被定义为具有第 $\\ell$ 个排名。因此，第一个排名向量可以写为 $L = [1, 2, \\dots, k]$。\n\n第二个排序源自嵌入空间距离 $d_Y$。问题将 $\\beta_{i,\\ell}(k)$ 定义为邻居 $j_{i,\\ell}$（其原始排名为 $\\ell$）在集合 $J_i(k)$ 内按 $d_Y$ 重新排序时的排名（从 $1$ 到 $k$）。因此，对应于按 $j_{i,1}, \\dots, j_{i,k}$ 顺序排列的原始项目的第二个排名向量是 $B_i(k) = [\\beta_{i,1}(k), \\beta_{i,2}(k), \\dots, \\beta_{i,k}(k)]$。\n\n最初排名为 $\\ell$ 的邻居的排名差异为 $\\ell - \\beta_{i,\\ell}(k)$。问题将此定义为排名位移 $\\Delta_{i,\\ell}(k)$。\n因此，点 $i$ 的平方差之和为 $\\sum_{\\ell=1}^{k} (\\ell - \\beta_{i,\\ell}(k))^2 = \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2$。\n\n将此代入斯皮尔曼公式，得到点 $i$ 的相关性 $\\rho_i(k)$：\n$$ \\rho_i(k) = 1 - \\frac{6 \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2}{k(k^2-1)} $$\n这是 $\\rho_i(k)$ 关于排名位移平方的所需表达式。\n\n总的邻域保持指数 $R_{NX}(k)$ 是这些单个相关系数在所有 $n$ 个点上的平均值：\n$$ R_{NX}(k) = \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i(k) $$\n代入 $\\rho_i(k)$ 的表达式：\n$$ R_{NX}(k) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( 1 - \\frac{6 \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2}{k(k^2-1)} \\right) $$\n这可以改写为：\n$$ R_{NX}(k) = 1 - \\frac{6}{n k (k^2-1)} \\sum_{i=1}^{n} \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2 $$\n\n### 任务 2：$R_{NX}(3)$ 的计算\n\n给定 $n=6$ 和 $k=3$。公式 $\\rho_i(3)$ 中分母项为 $k(k^2-1) = 3(3^2-1) = 3(8) = 24$。\n$R_{NX}(3)$ 的公式是：\n$$ R_{NX}(3) = 1 - \\frac{6}{6 \\cdot 24} \\sum_{i=1}^{6} \\sum_{\\ell=1}^{3} \\Delta_{i,\\ell}(3)^2 = 1 - \\frac{1}{24} \\sum_{i=1}^{6} \\sum_{\\ell=1}^{3} (\\ell - \\beta_{i,\\ell}(3))^2 $$\n我们现在必须计算每个点 $i \\in \\{1, \\dots, 6\\}$ 的排名位移平方和。\n\n-   **对于 $i=1$**：\n    -   原始 $k=3$ 邻居，按 $d_X$ 排序：$[j_{1,1}, j_{1,2}, j_{1,3}] = [2, 3, 4]$。理想排名向量是 $[1, 2, 3]$。\n    -   这些相同邻居的嵌入排序：$[2, 3, 4]$。\n    -   $j_{1,1}=2$ 的排名是 1。所以, $\\beta_{1,1}(3)=1$。\n    -   $j_{1,2}=3$ 的排名是 2。所以, $\\beta_{1,2}(3)=2$。\n    -   $j_{1,3}=4$ 的排名是 3。所以, $\\beta_{1,3}(3)=3$。\n    -   嵌入排名向量：$[\\beta_{1,1}(3), \\beta_{1,2}(3), \\beta_{1,3}(3)] = [1, 2, 3]$。\n    -   位移平方和：$\\sum_{\\ell=1}^{3} \\Delta_{1,\\ell}(3)^2 = (1-1)^2 + (2-2)^2 + (3-3)^2 = 0$。\n\n-   **对于 $i=2$**：\n    -   原始排序：$[j_{2,1}, j_{2,2}, j_{2,3}] = [1, 3, 5]$。理想排名：$[1, 2, 3]$。\n    -   嵌入排序：$[1, 5, 3]$。\n    -   $j_{2,1}=1$ 的排名是 1。所以, $\\beta_{2,1}(3)=1$。\n    -   $j_{2,2}=3$ 的排名是 3。所以, $\\beta_{2,2}(3)=3$。\n    -   $j_{2,3}=5$ 的排名是 2。所以, $\\beta_{2,3}(3)=2$。\n    -   嵌入排名向量：$[1, 3, 2]$。\n    -   位移平方和：$\\sum_{\\ell=1}^{3} \\Delta_{2,\\ell}(3)^2 = (1-1)^2 + (2-3)^2 + (3-2)^2 = 0 + 1 + 1 = 2$。\n\n-   **对于 $i=3$**：\n    -   原始排序：$[j_{3,1}, j_{3,2}, j_{3,3}] = [2, 4, 6]$。理想排名：$[1, 2, 3]$。\n    -   嵌入排序：$[6, 4, 2]$。\n    -   $j_{3,1}=2$ 的排名是 3。所以, $\\beta_{3,1}(3)=3$。\n    -   $j_{3,2}=4$ 的排名是 2。所以, $\\beta_{3,2}(3)=2$。\n    -   $j_{3,3}=6$ 的排名是 1。所以, $\\beta_{3,3}(3)=1$。\n    -   嵌入排名向量：$[3, 2, 1]$。\n    -   位移平方和：$\\sum_{\\ell=1}^{3} \\Delta_{3,\\ell}(3)^2 = (1-3)^2 + (2-2)^2 + (3-1)^2 = 4 + 0 + 4 = 8$。\n\n-   **对于 $i=4$**：\n    -   原始排序：$[1, 2, 5]$。嵌入排序：$[1, 2, 5]$。\n    -   这是一个完美的保持，与 $i=1$ 的情况相同。\n    -   嵌入排名向量：$[1, 2, 3]$。\n    -   位移平方和：$\\sum_{\\ell=1}^{3} \\Delta_{4,\\ell}(3)^2 = 0$。\n\n-   **对于 $i=5$**：\n    -   原始排序：$[2, 4, 6]$。嵌入排序：$[2, 6, 4]$。\n    -   $j_{5,1}=2$ 的排名是 1。所以, $\\beta_{5,1}(3)=1$。\n    -   $j_{5,2}=4$ 的排名是 3。所以, $\\beta_{5,2}(3)=3$。\n    -   $j_{5,3}=6$ 的排名是 2。所以, $\\beta_{5,3}(3)=2$。\n    -   嵌入排名向量：$[1, 3, 2]$。\n    -   位移平方和：$\\sum_{\\ell=1}^{3} \\Delta_{5,\\ell}(3)^2 = (1-1)^2 + (2-3)^2 + (3-2)^2 = 0 + 1 + 1 = 2$。\n\n-   **对于 $i=6$**：\n    -   原始排序：$[3, 4, 5]$。嵌入排序：$[3, 4, 5]$。\n    -   这是一个完美的保持，与 $i=1$ 的情况相同。\n    -   嵌入排名向量：$[1, 2, 3]$。\n    -   位移平方和：$\\sum_{\\ell=1}^{3} \\Delta_{6,\\ell}(3)^2 = 0$。\n\n现在，我们将所有 $i$ 的位移平方和相加：\n$$ \\sum_{i=1}^{6} \\sum_{\\ell=1}^{3} \\Delta_{i,\\ell}(3)^2 = 0 + 2 + 8 + 0 + 2 + 0 = 12 $$\n最后，我们计算 $R_{NX}(3)$：\n$$ R_{NX}(3) = 1 - \\frac{1}{24} (12) = 1 - \\frac{12}{24} = 1 - \\frac{1}{2} = \\frac{1}{2} $$\n邻域保持指数的值是 $\\frac{1}{2}$。\n\n### 任务 3：$R_{NX}(k)$ 的定性行为\n\n此任务要求对 $R_{NX}(k)$ 如何随邻域大小 $k$ 和数据质量（采样密度和噪声）变化提供定性论证。\n\n1.  **对 $k$ 的依赖性**：指数 $R_{NX}(k)$ 衡量了排名最高为 $k$ 的邻居的排名顺序的保持情况。像 UMAP 和 t-SNE 这样的非线性降维方法被设计为优先保留局部结构。这意味着最近邻的相对位置能够被高保真度地捕捉。因此，对于较小的 $k$ 值，我们期望 $R_{NX}(k)$ 接近其最大值 $1$。随着 $k$ 的增加，我们包含了离查询点 $i$ 越来越远的点。在曲面流形上，高维环境空间中的欧几里得距离对于沿流形的真实测地线距离来说，其代表性越来越差。嵌入过程旨在将流形“展开”成一个平坦的低维空间。这个过程不可避免地会引入失真，特别是对于不在同一局部邻域内的点对。因此，较远邻居的排名顺序更有可能被嵌入过程改变。所以，随着 $k$ 的增加，$R_{NX}(k)$ 预计会减小，反映了在保留局部结构与全局结构之间的权衡。\n\n2.  **对采样密度和噪声的依赖性**：降低采样密度和增加观测噪声都会降低从中推断流形结构的数据质量。\n    -   **降低采样密度**：当流形采样更稀疏时，环境空间中一个点的 $k$-近邻不太可能代表其沿流形的真实最近邻。算法用于估计局部几何属性（如切平面、曲率）的信息减少，导致嵌入不够准确。这种准确性的降低将表现为原始空间和嵌入空间之间邻居排名顺序的更多不一致，导致对于任何固定的 $k$，$R_{NX}(k)$ 都会减小。\n    -   **增加噪声方差**：当观测噪声很高时，数据点 $\\{x_i\\}$ 会偏离其在流形上的真实位置。这种噪声会掩盖底层的流形结构，使算法难以区分真实的几何特征和噪声引起的假象。观测数据中的局部邻域关系成为真实拓扑结构的不太可靠的指标。任何嵌入算法都难以产生忠实的表示，导致邻居排序的一致性下降。因此，随着噪声方差的增加，$R_{NX}(k)$ 预计会减小。\n\n总之，$R_{NX}(k)$ 通常随着 $k$ 的增加以及数据质量的下降（采样密度降低或噪声增加）而减小。\n\n最终需要的答案是任务 2 的数值结果。",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        }
    ]
}