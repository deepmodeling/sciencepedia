{
    "hands_on_practices": [
        {
            "introduction": "The t-SNE algorithm is celebrated for creating visually compelling visualizations of high-dimensional data, but its power stems from a specific and non-intuitive cost function. This cost function prioritizes preserving local neighborhood relationships over accurately representing global distances. This first exercise challenges you to dissect the Kullback-Leibler divergence at the heart of t-SNE, using a concrete scenario to understand precisely how the algorithm's objective function achieves this trade-off .",
            "id": "4176778",
            "problem": "A neural population recording yields a high-dimensional dataset where each sample is a vector of firing rates from $N$ simultaneously recorded neurons. Suppose $N$ is large (e.g., $N$ in the hundreds), and the samples are collected across multiple behavioral contexts, producing two well-separated manifolds of neural states: manifold $\\mathcal{M}_{1}$ (e.g., preparatory states) and manifold $\\mathcal{M}_{2}$ (e.g., execution states). Empirically, for points $i$ and $j$ within the same manifold, the high-dimensional Euclidean distance is small, while for points across manifolds, the distance is large. You apply t-distributed Stochastic Neighbor Embedding (t-SNE) to reduce dimensionality with a standard perplexity tuned to capture local neighborhoods (e.g., around $k$ nearest neighbors). Let $P_{ij}$ denote the pairwise similarity in the high-dimensional space derived from a Gaussian kernel and symmetrized to satisfy $\\sum_{i\\neq j} P_{ij} = 1$, and let $Q_{ij}$ denote the pairwise similarity in the low-dimensional embedded space derived from a Student $t$-distribution and symmetrized to satisfy $\\sum_{i\\neq j} Q_{ij} = 1$. The t-SNE objective is the Kullback–Leibler divergence\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right).\n$$\nConsider the following controlled scenario constructed to reflect typical outcomes in this dataset:\n- For a representative within-manifold neighbor pair, $P_{ij} \\approx 5 \\times 10^{-4}$, while the low-dimensional similarity after embedding is $Q_{ij} \\approx 4 \\times 10^{-4}$.\n- For a representative cross-manifold far pair, $P_{ij} \\approx 1 \\times 10^{-8}$, while due to the heavy tail of the Student $t$-kernel in low dimensions and limited embedding space, $Q_{ij} \\approx 1 \\times 10^{-4}$.\n\nYou are asked to reason from first principles about how t-SNE balances local neighborhood preservation against global distance distortion, using the governing objective above and the qualitative properties of the Gaussian-based $P_{ij}$ and Student $t$-based $Q_{ij}$, together with the empirical scenario. Which option best explains why local neighborhoods are preserved while global distances are distorted, and correctly relates this to the mismatch between $P_{ij}$ and $Q_{ij}$ for far-away pairs?\n\nA. The divergence $D_{\\mathrm{KL}}(P \\parallel Q)$ weights discrepancies by $P_{ij}$, so near-neighbor pairs (with larger $P_{ij}$) dominate the objective. For the given within-manifold pair, the contribution is $P_{ij} \\log(P_{ij}/Q_{ij}) \\approx (5 \\times 10^{-4}) \\log\\left(\\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}}\\right)$, which is orders of magnitude larger than the contribution from the far cross-manifold pair, $(1 \\times 10^{-8}) \\log\\left(\\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}}\\right)$. Because far-away pairs have $P_{ij} \\approx 0$, their mismatch with $Q_{ij}$ is weakly penalized, allowing global distances to be distorted while preserving local neighborhoods.\n\nB. The divergence $D_{\\mathrm{KL}}(Q \\parallel P)$ is minimized in t-SNE, so discrepancies are weighted by $Q_{ij}$, which heavily penalizes large $Q_{ij}$ assigned to far-away pairs and prevents global distortion. Therefore, near-neighbor preservation is a consequence of minimizing $D_{\\mathrm{KL}}(Q \\parallel P)$.\n\nC. Manifold learning methods that preserve global geodesic distances, such as Isometric Mapping (Isomap), prevent global distortions by computing shortest paths on the neighborhood graph; since t-SNE performs a similar geodesic computation implicitly, global distances are preserved and local neighborhoods may be distorted.\n\nD. Increasing perplexity in t-SNE forces $P_{ij}$ to be nearly uniform across all pairs, making $D_{\\mathrm{KL}}(P \\parallel Q)$ weight near and far pairs almost equally. This eliminates global distortion while preserving local neighborhoods.\n\nE. In low dimensions, the Student $t$-kernel used to construct $Q_{ij}$ is short-tailed, driving $Q_{ij} \\approx 0$ for all but the very nearest neighbors. This ensures global distances are preserved because far-away pairs have both $P_{ij}$ and $Q_{ij}$ near zero, keeping the divergence small uniformly across all pairs.\n\nSelect the single best option.",
            "solution": "The problem asks for an explanation of why the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm preserves local neighborhood structures while distorting global distances. The explanation must be derived from the properties of its objective function, the Kullback-Leibler (KL) divergence, and a provided numerical scenario.\n\n### Step 1: Problem Statement Validation\n\nFirst, I will validate the problem statement.\n\n**Givens:**\n- A high-dimensional dataset of neural firing rates from $N$ neurons.\n- The data forms two well-separated manifolds, $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$.\n- High-dimensional similarities are given by $P_{ij}$, derived from a Gaussian kernel, symmetrized, and normalized such that $\\sum_{i\\neq j} P_{ij} = 1$.\n- Low-dimensional similarities are given by $Q_{ij}$, derived from a Student $t$-distribution, symmetrized, and normalized such that $\\sum_{i\\neq j} Q_{ij} = 1$.\n- The t-SNE objective function to be minimized is the KL divergence $D_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right)$.\n- A representative within-manifold neighbor pair has $P_{ij} \\approx 5 \\times 10^{-4}$ and $Q_{ij} \\approx 4 \\times 10^{-4}$.\n- A representative cross-manifold far pair has $P_{ij} \\approx 1 \\times 10^{-8}$ and $Q_{ij} \\approx 1 \\times 10^{-4}$.\n\n**Validation:**\n1.  **Scientific Groundedness**: The problem statement is scientifically sound. It accurately describes the t-SNE algorithm, including its objective function, the definitions of the high-dimensional similarities ($P_{ij}$) based on a Gaussian kernel, and the low-dimensional similarities ($Q_{ij}$) based on a Student $t$-distribution. The description of neural data forming manifolds is a standard concept in computational neuroscience. The stated properties of the algorithm (preserving local structure at the expense of global structure) are well-documented characteristics of t-SNE.\n2.  **Well-Posedness**: The problem is well-posed. It provides a clear objective and sufficient information (the form of the cost function and numerical examples) to reason about the underlying mechanism and evaluate the given options.\n3.  **Objectivity**: The problem is stated in precise, objective, and technical language, free from any subjectivity or ambiguity.\n\n**Verdict:** The problem statement is valid. I will proceed with the solution.\n\n### Step 2: Derivation and Analysis\n\nThe objective of t-SNE is to find a low-dimensional embedding that minimizes the KL divergence between the joint probability distribution $P$, representing similarities in the high-dimensional space, and the joint probability distribution $Q$, representing similarities in the low-dimensional space. The objective function is:\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right)\n$$\nThis can be expanded as:\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log(P_{ij}) - \\sum_{i \\neq j} P_{ij} \\log(Q_{ij})\n$$\nSince the $P_{ij}$ values are fixed (derived from the input data), the term $\\sum_{i \\neq j} P_{ij} \\log(P_{ij})$ is a constant. Minimizing $D_{\\mathrm{KL}}(P \\parallel Q)$ is therefore equivalent to maximizing the cross-entropy term $\\sum_{i \\neq j} P_{ij} \\log(Q_{ij})$.\n\nThe core of the analysis lies in understanding how individual pairs $(i, j)$ contribute to the total KL divergence. The contribution of a single pair is $C_{ij} = P_{ij} \\log(P_{ij}/Q_{ij})$. The optimization process adjusts the positions of points in the low-dimensional space to change the values of $Q_{ij}$ in order to minimize the sum of all such contributions.\n\nThe crucial feature of this cost function is the pre-factor $P_{ij}$. This factor acts as a weight for the logarithmic term $\\log(P_{ij}/Q_{ij})$, which measures the \"mismatch\" between the similarities.\n\nLet's analyze the two scenarios provided:\n\n**1. Within-manifold neighbor pair (Local Structure):**\n- Given: $P_{ij} \\approx 5 \\times 10^{-4}$ and $Q_{ij} \\approx 4 \\times 10^{-4}$.\n- Here, $P_{ij}$ is relatively large, because points $i$ and $j$ are close neighbors in the high-dimensional space.\n- Any significant mismatch between $P_{ij}$ and $Q_{ij}$ will be multiplied by this relatively large $P_{ij}$, resulting in a substantial contribution to the cost. For example, if the algorithm were to place these points far apart in the low-dimensional map, $Q_{ij}$ would become very small, making $\\log(P_{ij}/Q_{ij})$ a large positive number. The resulting contribution $P_{ij} \\log(P_{ij}/Q_{ij})$ would be large, creating a strong \"force\" in the optimization to bring the points closer together (thus increasing $Q_{ij}$ to better match $P_{ij}$).\n- The algorithm is therefore highly sensitive to getting the similarities right for nearby points. This is the mechanism for local structure preservation.\n\n**2. Cross-manifold far pair (Global Structure):**\n- Given: $P_{ij} \\approx 1 \\times 10^{-8}$ and $Q_{ij} \\approx 1 \\times 10^{-4}$.\n- Here, $P_{ij}$ is extremely small, because points $i$ and $j$ are far apart in the high-dimensional space.\n- The contribution to the cost is $C_{ij} = P_{ij} \\log(P_{ij}/Q_{ij})$.\n- Even if there is a massive mismatch between $P_{ij}$ and $Q_{ij}$ (as given in the scenario), the contribution to the total cost is weighted by the tiny $P_{ij}$. The optimization is largely indifferent to this mismatch. Let's compute the approximate contributions to see this quantitatively.\n\nContribution from the near pair:\n$$\nC_{\\text{near}} = P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right) \\approx (5 \\times 10^{-4}) \\ln \\left( \\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}} \\right) = (5 \\times 10^{-4}) \\ln(1.25) \\approx (5 \\times 10^{-4})(0.223) \\approx 1.12 \\times 10^{-4}\n$$\nContribution from the far pair:\n$$\nC_{\\text{far}} = P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right) \\approx (1 \\times 10^{-8}) \\ln \\left( \\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}} \\right) = (1 \\times 10^{-8}) \\ln(10^{-4}) \\approx (1 \\times 10^{-8})(-9.21) \\approx -9.21 \\times 10^{-8}\n$$\nThe magnitude of the contribution from the near pair ($|C_{\\text{near}}| \\approx 1.12 \\times 10^{-4}$) is about $1200$ times larger than the magnitude of the contribution from the far pair ($|C_{\\text{far}}| \\approx 9.21 \\times 10^{-8}$).\n\nThis confirms that the objective function is dominated by the contributions from nearby points (where $P_{ij}$ is large). The algorithm expends its effort on arranging these points correctly. It is weakly penalized for misrepresenting the distances between far-away points (where $P_{ij}$ is nearly zero), allowing for significant distortion of the global structure.\n\n### Step 3: Option-by-Option Analysis\n\n**A. The divergence $D_{\\mathrm{KL}}(P \\parallel Q)$ weights discrepancies by $P_{ij}$, so near-neighbor pairs (with larger $P_{ij}$) dominate the objective. For the given within-manifold pair, the contribution is $P_{ij} \\log(P_{ij}/Q_{ij}) \\approx (5 \\times 10^{-4}) \\log\\left(\\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}}\\right)$, which is orders of magnitude larger than the contribution from the far cross-manifold pair, $(1 \\times 10^{-8}) \\log\\left(\\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}}\\right)$. Because far-away pairs have $P_{ij} \\approx 0$, their mismatch with $Q_{ij}$ is weakly penalized, allowing global distances to be distorted while preserving local neighborhoods.**\n- This option correctly identifies that the cost function is weighted by $P_{ij}$. It correctly concludes that this causes the contributions from near pairs (large $P_{ij}$) to dominate the objective. The numerical calculation confirms that the near-pair contribution is orders of magnitude larger than the far-pair contribution. The final conclusion—that this weighting weakly penalizes global mismatches, thus allowing global distance distortion while preserving local neighborhoods—is a direct and accurate consequence of the analysis.\n- **Verdict: Correct.**\n\n**B. The divergence $D_{\\mathrm{KL}}(Q \\parallel P)$ is minimized in t-SNE, so discrepancies are weighted by $Q_{ij}$, which heavily penalizes large $Q_{ij}$ assigned to far-away pairs and prevents global distortion. Therefore, near-neighbor preservation is a consequence of minimizing $D_{\\mathrm{KL}}(Q \\parallel P)$.**\n- This option is factually incorrect. The t-SNE algorithm minimizes $D_{\\mathrm{KL}}(P \\parallel Q)$, not $D_{\\mathrm{KL}}(Q \\parallel P)$. These two KL divergences are not symmetric and have very different properties. The logic presented applies to minimizing the reverse KL divergence, which is not what t-SNE does.\n- **Verdict: Incorrect.**\n\n**C. Manifold learning methods that preserve global geodesic distances, such as Isometric Mapping (Isomap), prevent global distortions by computing shortest paths on the neighborhood graph; since t-SNE performs a similar geodesic computation implicitly, global distances are preserved and local neighborhoods may be distorted.**\n- This option incorrectly describes t-SNE. While it correctly describes Isomap, it falsely claims that t-SNE performs a similar geodesic computation. t-SNE's computation of affinities is based on local Gaussian kernels, not on shortest path distances in a graph. Its objective function, as shown above, leads to the distortion of global distances, not their preservation.\n- **Verdict: Incorrect.**\n\n**D. Increasing perplexity in t-SNE forces $P_{ij}$ to be nearly uniform across all pairs, making $D_{\\mathrm{KL}}(P \\parallel Q)$ weight near and far pairs almost equally. This eliminates global distortion while preserving local neighborhoods.**\n- While it is true that a very high perplexity would make $P_{ij}$ more uniform, the conclusion is flawed. If $P_{ij}$ were uniform, it would contain no information about the local structure of the data. The algorithm's goal of preserving local neighborhoods would be subverted from the start. This approach would destroy local structure information, not preserve it.\n- **Verdict: Incorrect.**\n\n**E. In low dimensions, the Student $t$-kernel used to construct $Q_{ij}$ is short-tailed, driving $Q_{ij} \\approx 0$ for all but the very nearest neighbors. This ensures global distances are preserved because far-away pairs have both $P_{ij}$ and $Q_{ij}$ near zero, keeping the divergence small uniformly across all pairs.**\n- This option makes a factually incorrect statement about the Student $t$-distribution. The $t$-distribution with one degree of freedom, used in t-SNE, is *heavy-tailed*, not short-tailed. Its slower, power-law decay (compared to the exponential decay of a Gaussian) is a key feature that helps to separate clusters and alleviate the \"crowding problem\". The premise that the kernel is short-tailed is fundamentally wrong. Consequently, the reasoning that follows from this incorrect premise is also invalid.\n- **Verdict: Incorrect.**\n\nThe analysis confirms that Option A provides the only accurate and complete explanation for the behavior of t-SNE as described in the problem.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Having understood the 'why' behind t-SNE's behavior, we now turn to the practical 'how' of applying it. The perplexity parameter, $\\mathcal{P}$, is the most critical knob for a user to tune, as it effectively sets the scale of the local neighborhoods the algorithm focuses on. This practice problem places you in a realistic neuroscience data analysis scenario, asking you to use your understanding of perplexity to predict how its value will influence the resulting embedding, either by correctly separating distinct neural populations or by erroneously merging or fragmenting them .",
            "id": "4176842",
            "problem": "You are analyzing a high-dimensional dataset of neuronal responses from mouse visual cortex, where each neuron is represented by a feature vector $x_i \\in \\mathbb{R}^D$ extracted from calcium imaging across $M$ stimuli. The dataset comprises $n_{\\mathcal{A}}$ neurons in population $\\mathcal{A}$ (orientation-tuned, smoothly varying preferred angle forming a continuous manifold with a sparse region), and two Gaussian-like populations, $\\mathcal{B}$ and $\\mathcal{C}$ (direction-selective neurons with similar but distinct mean response profiles), with sizes $n_{\\mathcal{B}}$ and $n_{\\mathcal{C}}$. You plan to embed the data using $t$-distributed Stochastic Neighbor Embedding (t-SNE), which defines high-dimensional conditional affinities\n$$\np_{j|i} = \\frac{\\exp\\!\\left(-\\frac{\\|x_i - x_j\\|_2^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\!\\left(-\\frac{\\|x_i - x_k\\|_2^2}{2\\sigma_i^2}\\right)},\n$$\nwith local bandwidths $\\sigma_i$ chosen so that the perplexity matches a user-specified target $\\mathcal{P}$. The perplexity is defined via the Shannon entropy of the conditional distribution $P_i = \\{p_{j|i}\\}_j$ as\n$$\n\\mathcal{P} \\equiv 2^{H(P_i)}, \\quad H(P_i) = -\\sum_j p_{j|i} \\log_2 p_{j|i}.\n$$\nEmpirically, a $k$-nearest neighbor (Nearest Neighbor (NN)) graph built from the Euclidean distances $\\|x_i - x_j\\|_2$ reveals the following structure:\n- For population $\\mathcal{A}$, the sparse region creates a bottleneck such that the $k$-NN graph fragments into multiple components unless $k \\ge 12$, while for $k \\ge 12$ the graph over $\\mathcal{A}$ is connected.\n- For populations $\\mathcal{B}$ and $\\mathcal{C}$, their $k$-NN graphs remain as two separate components for $k \\le 30$, but merge into a single connected component for $k \\ge 50$.\n\nAssume that in t-SNE the target perplexity $\\mathcal{P}$ sets, for most points, an effective neighborhood size on the order of $\\mathcal{P}$, so that the qualitative connectivity behavior of the high-dimensional affinity graph mirrors that of the $k$-NN graph with $k \\approx \\mathcal{P}$. You must reason from the definitions of $p_{j|i}$, $H(P_i)$, and $\\mathcal{P}$ to determine how choice of $\\mathcal{P}$ affects cluster granularity in the low-dimensional embedding, particularly fragmentation of $\\mathcal{A}$ and merging of $\\mathcal{B}$ with $\\mathcal{C}$.\n\nWhich statements are most consistent with the mechanisms above?\n\nA. Using $\\mathcal{P} = 5$ leads to fragmentation of population $\\mathcal{A}$ and keeps populations $\\mathcal{B}$ and $\\mathcal{C}$ separated.\n\nB. Using $\\mathcal{P} = 15$ leads to fragmentation of population $\\mathcal{A}$ while keeping populations $\\mathcal{B}$ and $\\mathcal{C}$ separated.\n\nC. Using $\\mathcal{P} = 60$ keeps population $\\mathcal{A}$ contiguous and merges populations $\\mathcal{B}$ and $\\mathcal{C}$ into a single cluster.\n\nD. Using $\\mathcal{P} = 30$ keeps population $\\mathcal{A}$ contiguous and merges populations $\\mathcal{B}$ and $\\mathcal{C}$ into a single cluster.\n\nSelect all options that are correct.",
            "solution": "The problem requires us to predict the qualitative structure of a t-SNE embedding based on the choice of the perplexity parameter, $\\mathcal{P}$. The key is to use the provided assumption that the behavior of the t-SNE algorithm for a given $\\mathcal{P}$ can be approximated by the behavior of a $k$-nearest neighbor ($k$-NN) graph where the number of neighbors $k$ is approximately equal to the perplexity, i.e., $k \\approx \\mathcal{P}$.\n\nThe perplexity, $\\mathcal{P} \\equiv 2^{H(P_i)}$, where $H(P_i)$ is the Shannon entropy, measures the effective number of neighbors each point $x_i$ has in the high-dimensional space according to the probability distribution $\\{p_{j|i}\\}_j$. A low $\\mathcal{P}$ means t-SNE focuses on very local structure (few neighbors), while a high $\\mathcal{P}$ means it incorporates more global structure by considering a larger neighborhood.\n\nThe problem gives us the results of a structural analysis using a $k$-NN graph:\n1.  **Population $\\mathcal{A}$:** This population represents a continuous manifold. It fragments if the neighborhood size is too small to bridge a sparse region. The critical neighborhood size is given as $k=12$.\n    - For $k < 12$ (low $k$), the graph fragments.\n    - For $k \\ge 12$ (sufficiently high $k$), the graph is connected.\n2.  **Populations $\\mathcal{B}$ and $\\mathcal{C}$:** These are two distinct but nearby clusters. Their separation or merging depends on whether the neighborhood size is small enough to stay within each cluster or large enough to span across both.\n    - For $k \\le 30$, the clusters remain separate.\n    - For $k \\ge 50$, the clusters merge into a single component.\n\nBy applying the assumption $k \\approx \\mathcal{P}$, we can translate these $k$-NN thresholds into perplexity thresholds for the t-SNE embedding:\n- For population $\\mathcal{A}$ to be embedded as a contiguous manifold, we require $\\mathcal{P} \\gtrsim 12$. For $\\mathcal{P} \\lesssim 12$, it will likely fragment.\n- For populations $\\mathcal{B}$ and $\\mathcal{C}$ to be embedded as separate clusters, we require $\\mathcal{P} \\lesssim 30$. For $\\mathcal{P} \\gtrsim 50$, they will likely merge.\n\nWe now analyze each option using these criteria.\n\n**A. Using $\\mathcal{P} = 5$ leads to fragmentation of population $\\mathcal{A}$ and keeps populations $\\mathcal{B}$ and $\\mathcal{C}$ separated.**\nWe set $\\mathcal{P} = 5$, which implies an effective neighborhood size of $k \\approx 5$.\n- For population $\\mathcal{A}$: Since $5 < 12$, the neighborhood is too small to bridge the sparse region. This corresponds to the $k < 12$ case, which predicts fragmentation. The first part of the statement is consistent.\n- For populations $\\mathcal{B}$ and $\\mathcal{C}$: Since $5 \\le 30$, the neighborhood is small enough to not bridge the gap between clusters. This corresponds to the $k \\le 30$ case, which predicts separation. The second part of the statement is consistent.\nBoth predictions match the statement. Thus, option A is **Correct**.\n\n**B. Using $\\mathcal{P} = 15$ leads to fragmentation of population $\\mathcal{A}$ while keeping populations $\\mathcal{B}$ and $\\mathcal{C}$ separated.**\nWe set $\\mathcal{P} = 15$, which implies an effective neighborhood size of $k \\approx 15$.\n- For population $\\mathcal{A}$: Since $15 \\ge 12$, the neighborhood is large enough to bridge the sparse region. This corresponds to the $k \\ge 12$ case, which predicts a connected, contiguous representation. The statement claims fragmentation, which is a contradiction.\n- For populations $\\mathcal{B}$ and $\\mathcal{C}$: Since $15 \\le 30$, this predicts separation, which is consistent with the statement.\nHowever, because the prediction for population $\\mathcal{A}$ is wrong, the entire option is **Incorrect**.\n\n**C. Using $\\mathcal{P} = 60$ keeps population $\\mathcal{A}$ contiguous and merges populations $\\mathcal{B}$ and $\\mathcal{C}$ into a single cluster.**\nWe set $\\mathcal{P} = 60$, which implies an effective neighborhood size of $k \\approx 60$.\n- For population $\\mathcal{A}$: Since $60 \\ge 12$, the neighborhood is more than sufficient to ensure connectivity. This corresponds to the $k \\ge 12$ case, predicting a contiguous representation. The first part of the statement is consistent.\n- For populations $\\mathcal{B}$ and $\\mathcal{C}$: Since $60 \\ge 50$, the neighborhood is large enough to span across both clusters. This corresponds to the $k \\ge 50$ case, which predicts that the clusters will merge. The second part of the statement is consistent.\nBoth predictions match the statement. Thus, option C is **Correct**.\n\n**D. Using $\\mathcal{P} = 30$ keeps population $\\mathcal{A}$ contiguous and merges populations $\\mathcal{B}$ and $\\mathcal{C}$ into a single cluster.**\nWe set $\\mathcal{P} = 30$, which implies an effective neighborhood size of $k \\approx 30$.\n- For population $\\mathcal{A}$: Since $30 \\ge 12$, the neighborhood is sufficient to ensure connectivity. This corresponds to the $k \\ge 12$ case, predicting a contiguous representation. The first part of the statement is consistent.\n- For populations $\\mathcal{B}$ and $\\mathcal{C}$: The problem states that for $k \\le 30$, the clusters remain separate. Thus, at the boundary $k=30$, they are still separate. The statement claims they merge, which contradicts the provided empirical data (merging occurs for $k \\ge 50$).\nBecause the prediction for populations $\\mathcal{B}$ and $\\mathcal{C}$ is wrong, the entire option is **Incorrect**.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "After generating an embedding, a crucial final step is to assess its quality. While visual inspection is useful, it can be subjective and sometimes misleading, making quantitative metrics essential for a rigorous evaluation. This final exercise guides you through the process of deriving and applying a neighborhood preservation index from first principles, providing you with a powerful tool to measure how faithfully the local structure of your original high-dimensional data has been retained in the new low-dimensional map .",
            "id": "4176825",
            "problem": "A laboratory records population activity vectors from $p$ simultaneously recorded neurons in an awake behaving animal during $n=6$ repeated conditions, yielding $n$ high-dimensional points $\\{x_{i} \\in \\mathbb{R}^{p}\\}_{i=1}^{6}$. A two-dimensional embedding $\\{y_{i} \\in \\mathbb{R}^{2}\\}_{i=1}^{6}$ is computed from $\\{x_{i}\\}$ by a nonlinear dimensionality reduction method (for example, Uniform Manifold Approximation and Projection (UMAP) or t-distributed Stochastic Neighbor Embedding (t-SNE)), with the intent of preserving local neighborhoods that reflect putative neural manifolds.\n\nLet $d_{X}(i,j)$ and $d_{Y}(i,j)$ denote pairwise distances in the original and embedded spaces, respectively, and let $r_{ij}$ and $s_{ij}$ be the corresponding ranks of $j$ when the neighbors of $i$ are ordered in ascending distance by $d_{X}$ and $d_{Y}$, respectively, with $r_{ii}=s_{ii}=0$ and $r_{ij},s_{ij} \\in \\{1,\\dots,n-1\\}$ for $i \\neq j$. For a fixed neighborhood size $k \\in \\{1,\\dots,n-1\\}$ and each query index $i$, define the $k$-nearest neighbor index set in the original space by $J_{i}(k) = \\{ j \\neq i : r_{ij} \\leq k \\}$, and order it by increasing $r_{ij}$ as $j_{i,1},\\dots,j_{i,k}$ so that $r_{i,j_{i,\\ell}} = \\ell$ for $\\ell \\in \\{1,\\dots,k\\}$. Let $\\beta_{i,\\ell}(k)$ be the rank position induced by $s_{ij}$ when the items $\\{j_{i,1},\\dots,j_{i,k}\\}$ are ordered by increasing $s_{ij}$, so that $\\beta_{i,\\ell}(k) \\in \\{1,\\dots,k\\}$ is the position of $j_{i,\\ell}$ within the embedding-induced ordering restricted to $J_{i}(k)$.\n\nTask 1. Starting only from the definition of Spearman’s rank correlation coefficient between two orderings on $k$ items and the definitions above, derive an explicit expression for a neighborhood preservation index $R_{NX}(k)$ as the mean Spearman correlation of the $k$-neighbor orderings:\n- For each $i$, define the Spearman correlation $\\rho_{i}(k)$ between the “ideal” ordering $[1,2,\\dots,k]$ and the embedding-induced ordering $[\\beta_{i,1}(k),\\dots,\\beta_{i,k}(k)]$, and show that it can be written in closed form in terms of the squared rank displacements $\\Delta_{i,\\ell}(k) = \\ell - \\beta_{i,\\ell}(k)$.\n- Then define $R_{NX}(k)$ as the average of $\\rho_{i}(k)$ over $i \\in \\{1,\\dots,n\\}$.\n\nTask 2. Consider the following empirically observed $k=3$ neighbor orderings for the $n=6$ conditions. For each $i \\in \\{1,\\dots,6\\}$, you are given:\n- The original-space top-$3$ neighbor indices in their $d_{X}$-rank order, $[j_{i,1},j_{i,2},j_{i,3}]$.\n- The embedding-induced order of those same three indices, restricted to $J_{i}(3)$ and ordered by $d_{Y}$, encoded as the corresponding index sequence $[\\widehat{j}_{i,1},\\widehat{j}_{i,2},\\widehat{j}_{i,3}]$.\n\nUse these to compute $\\beta_{i,\\ell}(3)$ for each $i$ and then evaluate $R_{NX}(3)$ exactly (no rounding).\n\nData:\n- $i=1$: original $[2,3,4]$, embedding $[2,3,4]$.\n- $i=2$: original $[1,3,5]$, embedding $[1,5,3]$.\n- $i=3$: original $[2,4,6]$, embedding $[6,4,2]$.\n- $i=4$: original $[1,2,5]$, embedding $[1,2,5]$.\n- $i=5$: original $[2,4,6]$, embedding $[2,6,4]$.\n- $i=6$: original $[3,4,5]$, embedding $[3,4,5]$.\n\nTask 3. Based on first principles of rank-order statistics and $k$-nearest neighbor stability on curved manifolds, briefly argue, without appealing to any specific algorithmic details of Uniform Manifold Approximation and Projection (UMAP) or t-distributed Stochastic Neighbor Embedding (t-SNE), how $R_{NX}(k)$ is expected to vary as $k$ increases and as the sampling density of the neural manifold decreases while observation noise variance remains fixed.\n\nYour final reported result must be the exact value of $R_{NX}(3)$ for the dataset above. Do not include units. Do not include any additional text in your final answer box.",
            "solution": "The problem asks for three tasks: first, to derive a formula for a neighborhood preservation index $R_{NX}(k)$; second, to compute this index for a specific dataset with $k=3$; and third, to provide a qualitative argument about the behavior of this index. The problem is well-posed and scientifically grounded in the principles of manifold learning and rank-order statistics.\n\n### Task 1: Derivation of the Neighborhood Preservation Index $R_{NX}(k)$\n\nThe problem defines an index $R_{NX}(k)$ as the mean of Spearman's rank correlation coefficients, $\\rho_i(k)$, calculated for each of the $n$ data points. For each point $i$, the correlation is between the rank ordering of its $k$-nearest neighbors in the original high-dimensional space and the rank ordering of the same neighbors in the low-dimensional embedding.\n\nLet there be $k$ items. Let their ranks in two different orderings be given by the vectors $u = [u_1, u_2, \\dots, u_k]$ and $v = [v_1, v_2, \\dots, v_k]$. Spearman's rank correlation coefficient $\\rho$ is defined as the Pearson correlation coefficient of these rank variables. For the case where there are no ties (which is true here, as $u$ and $v$ are permutations of $[1, 2, \\dots, k]$), the formula simplifies to:\n$$ \\rho = 1 - \\frac{6 \\sum_{j=1}^{k} d_j^2}{k(k^2-1)} $$\nwhere $d_j = u_j - v_j$ is the difference in ranks for the $j$-th item.\n\nIn our specific problem, for each query point $i$, the \"items\" are its $k$-nearest neighbors in the original space, given by the set $J_i(k) = \\{j_{i,1}, \\dots, j_{i,k}\\}$.\nThe first ordering is the \"ideal\" one, derived from the original space distances $d_X$. The neighbor $j_{i,\\ell}$ is defined as having the $\\ell$-th rank. Thus, the first rank vector can be written as $L = [1, 2, \\dots, k]$.\n\nThe second ordering is derived from the embedding space distances $d_Y$. The problem defines $\\beta_{i,\\ell}(k)$ as the rank (from $1$ to $k$) of the neighbor $j_{i,\\ell}$ (which had original rank $\\ell$) within the set $J_i(k)$ when these neighbors are re-ordered according to $d_Y$. Therefore, the second rank vector, corresponding to the original items ordered as $j_{i,1}, \\dots, j_{i,k}$, is $B_i(k) = [\\beta_{i,1}(k), \\beta_{i,2}(k), \\dots, \\beta_{i,k}(k)]$.\n\nThe rank difference for the neighbor that was originally at rank $\\ell$ is $\\ell - \\beta_{i,\\ell}(k)$. The problem defines this as the rank displacement $\\Delta_{i,\\ell}(k)$.\nThe sum of squared differences for point $i$ is therefore $\\sum_{\\ell=1}^{k} (\\ell - \\beta_{i,\\ell}(k))^2 = \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2$.\n\nSubstituting this into the Spearman formula gives the correlation $\\rho_i(k)$ for the point $i$:\n$$ \\rho_i(k) = 1 - \\frac{6 \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2}{k(k^2-1)} $$\nThis is the required expression for $\\rho_i(k)$ in terms of the squared rank displacements.\n\nThe overall neighborhood preservation index $R_{NX}(k)$ is the average of these individual correlation coefficients over all $n$ points:\n$$ R_{NX}(k) = \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i(k) $$\nSubstituting the expression for $\\rho_i(k)$:\n$$ R_{NX}(k) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( 1 - \\frac{6 \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2}{k(k^2-1)} \\right) $$\nThis can be rewritten as:\n$$ R_{NX}(k) = 1 - \\frac{6}{n k (k^2-1)} \\sum_{i=1}^{n} \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2 $$\n\n### Task 2: Computation of $R_{NX}(3)$\n\nWe are given $n=6$ and $k=3$. The term in the denominator of the formula for $\\rho_i(3)$ is $k(k^2-1) = 3(3^2-1) = 3(8) = 24$.\nThe formula for $R_{NX}(3)$ is:\n$$ R_{NX}(3) = 1 - \\frac{6}{6 \\cdot 24} \\sum_{i=1}^{6} \\sum_{\\ell=1}^{3} \\Delta_{i,\\ell}(3)^2 = 1 - \\frac{1}{24} \\sum_{i=1}^{6} \\sum_{\\ell=1}^{3} (\\ell - \\beta_{i,\\ell}(3))^2 $$\nWe must now compute the sum of squared rank displacements for each point $i \\in \\{1, \\dots, 6\\}$.\n\n-   **For $i=1$**:\n    -   Original $k=3$ neighbors, ordered by $d_X$: $[j_{1,1}, j_{1,2}, j_{1,3}] = [2, 3, 4]$. The ideal rank vector is $[1, 2, 3]$.\n    -   Embedding order of these same neighbors: $[2, 3, 4]$.\n    -   The rank of $j_{1,1}=2$ is $1$. So, $\\beta_{1,1}(3)=1$.\n    -   The rank of $j_{1,2}=3$ is $2$. So, $\\beta_{1,2}(3)=2$.\n    -   The rank of $j_{1,3}=4$ is $3$. So, $\\beta_{1,3}(3)=3$.\n    -   The embedding rank vector is $[\\beta_{1,1}(3), \\beta_{1,2}(3), \\beta_{1,3}(3)] = [1, 2, 3]$.\n    -   Sum of squared displacements: $\\sum_{\\ell=1}^{3} \\Delta_{1,\\ell}(3)^2 = (1-1)^2 + (2-2)^2 + (3-3)^2 = 0$.\n\n-   **For $i=2$**:\n    -   Original order: $[j_{2,1}, j_{2,2}, j_{2,3}] = [1, 3, 5]$. Ideal ranks: $[1, 2, 3]$.\n    -   Embedding order: $[1, 5, 3]$.\n    -   The rank of $j_{2,1}=1$ is $1$. So, $\\beta_{2,1}(3)=1$.\n    -   The rank of $j_{2,2}=3$ is $3$. So, $\\beta_{2,2}(3)=3$.\n    -   The rank of $j_{2,3}=5$ is $2$. So, $\\beta_{2,3}(3)=2$.\n    -   Embedding rank vector: $[1, 3, 2]$.\n    -   Sum of squared displacements: $\\sum_{\\ell=1}^{3} \\Delta_{2,\\ell}(3)^2 = (1-1)^2 + (2-3)^2 + (3-2)^2 = 0 + 1 + 1 = 2$.\n\n-   **For $i=3$**:\n    -   Original order: $[j_{3,1}, j_{3,2}, j_{3,3}] = [2, 4, 6]$. Ideal ranks: $[1, 2, 3]$.\n    -   Embedding order: $[6, 4, 2]$.\n    -   The rank of $j_{3,1}=2$ is $3$. So, $\\beta_{3,1}(3)=3$.\n    -   The rank of $j_{3,2}=4$ is $2$. So, $\\beta_{3,2}(3)=2$.\n    -   The rank of $j_{3,3}=6$ is $1$. So, $\\beta_{3,3}(3)=1$.\n    -   Embedding rank vector: $[3, 2, 1]$.\n    -   Sum of squared displacements: $\\sum_{\\ell=1}^{3} \\Delta_{3,\\ell}(3)^2 = (1-3)^2 + (2-2)^2 + (3-1)^2 = 4 + 0 + 4 = 8$.\n\n-   **For $i=4$**:\n    -   Original order: $[1, 2, 5]$. Embedding order: $[1, 2, 5]$.\n    -   This is a perfect preservation, identical to $i=1$.\n    -   Embedding rank vector: $[1, 2, 3]$.\n    -   Sum of squared displacements: $\\sum_{\\ell=1}^{3} \\Delta_{4,\\ell}(3)^2 = 0$.\n\n-   **For $i=5$**:\n    -   Original order: $[2, 4, 6]$. Embedding order: $[2, 6, 4]$.\n    -   The rank of $j_{5,1}=2$ is $1$. So, $\\beta_{5,1}(3)=1$.\n    -   The rank of $j_{5,2}=4$ is $3$. So, $\\beta_{5,2}(3)=3$.\n    -   The rank of $j_{5,3}=6$ is $2$. So, $\\beta_{5,3}(3)=2$.\n    -   Embedding rank vector: $[1, 3, 2]$.\n    -   Sum of squared displacements: $\\sum_{\\ell=1}^{3} \\Delta_{5,\\ell}(3)^2 = (1-1)^2 + (2-3)^2 + (3-2)^2 = 0 + 1 + 1 = 2$.\n\n-   **For $i=6$**:\n    -   Original order: $[3, 4, 5]$. Embedding order: $[3, 4, 5]$.\n    -   This is a perfect preservation, identical to $i=1$.\n    -   Embedding rank vector: $[1, 2, 3]$.\n    -   Sum of squared displacements: $\\sum_{\\ell=1}^{3} \\Delta_{6,\\ell}(3)^2 = 0$.\n\nNow, we sum the squared displacements over all $i$:\n$$ \\sum_{i=1}^{6} \\sum_{\\ell=1}^{3} \\Delta_{i,\\ell}(3)^2 = 0 + 2 + 8 + 0 + 2 + 0 = 12 $$\nFinally, we compute $R_{NX}(3)$:\n$$ R_{NX}(3) = 1 - \\frac{1}{24} (12) = 1 - \\frac{12}{24} = 1 - \\frac{1}{2} = \\frac{1}{2} $$\nThe value of the neighborhood preservation index is $\\frac{1}{2}$.\n\n### Task 3: Qualitative Behavior of $R_{NX}(k)$\n\nThis task requires a qualitative argument about how $R_{NX}(k)$ changes with the neighborhood size $k$ and with data quality (sampling density and noise).\n\n1.  **Dependence on $k$**: The index $R_{NX}(k)$ measures the preservation of rank-order for neighbors up to rank $k$. Nonlinear dimensionality reduction methods like UMAP and t-SNE are designed to prioritize the preservation of local structure. This means the relative positions of the very nearest neighbors are captured with high fidelity. As a result, for small values of $k$, we expect $R_{NX}(k)$ to be close to its maximum value of $1$. As $k$ increases, we include points that are progressively farther away from the query point $i$. On a curved manifold, the Euclidean distance in the high-dimensional ambient space becomes an increasingly poor proxy for the true geodesic distance along the manifold. The embedding process seeks to \"unroll\" the manifold into a flat, low-dimensional space. This process inevitably introduces distortions, especially for pairs of points that are not in the same local neighborhood. Consequently, the rank ordering of more distant neighbors is more likely to be altered by the embedding. Therefore, as $k$ increases, $R_{NX}(k)$ is expected to decrease, reflecting the trade-off between preserving local versus global structure.\n\n2.  **Dependence on Sampling Density and Noise**: Both decreasing sampling density and increasing observation noise degrade the quality of the data from which the manifold structure is inferred.\n    -   **Decreasing sampling density**: When the manifold is sampled more sparsely, the $k$-nearest neighbors of a point in the ambient space are less likely to represent its true nearest neighbors along the manifold. The algorithm has less information to estimate local geometric properties (e.g., tangent planes, curvature), leading to a less accurate embedding. This reduced accuracy will manifest as more disagreements in the rank ordering of neighbors between the original space and the embedding, causing $R_{NX}(k)$ to decrease for any fixed $k$.\n    -   **Increasing noise variance**: When observation noise is high, the data points $\\{x_i\\}$ are perturbed from their true locations on the manifold. This noise can obscure the underlying manifold's structure, making it difficult for the algorithm to distinguish true geometric features from noise-induced artifacts. The local neighborhood relationships in the observed data become less reliable indicators of the true topology. Any embedding algorithm will struggle to produce a faithful representation, leading to a degraded alignment of neighbor orderings. Thus, as noise variance increases, $R_{NX}(k)$ is expected to decrease.\n\nIn summary, $R_{NX}(k)$ generally decreases with increasing $k$ and with decreasing data quality (lower sampling density or higher noise).\n\nThe final answer required is the numerical result from Task 2.",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        }
    ]
}