{
    "hands_on_practices": [
        {
            "introduction": "为了有效运用 t-SNE 等流形学习算法，我们必须首先理解其核心工作原理。这项练习将深入探讨 t-SNE 的目标函数 ，揭示为何它在保留局部邻域结构方面表现出色，但这通常以牺牲全局距离的精确性为代价。通过一个具体的神经科学场景，你将学会从根本上理解这种权衡，这是解读和评估降维结果的关键。",
            "id": "4176778",
            "problem": "一次神经元集群记录产生了一个高维数据集，其中每个样本是一个由 $N$ 个同时记录的神经元的发放率组成的向量。假设 $N$ 很大（例如，$N$ 为数百），并且样本是在多个行为情境下收集的，产生了两个分离良好的神经状态流形：流形 $\\mathcal{M}_{1}$（例如，准备状态）和流形 $\\mathcal{M}_{2}$（例如，执行状态）。根据经验，对于同一流形内的点 $i$ 和 $j$，其高维欧几里得距离很小，而对于跨流形的点，其距离则很大。你应用 t-分布随机邻域嵌入 (t-SNE) 来降低维度，使用一个标准的、为捕捉局部邻域（例如，大约 $k$ 个最近邻）而调整的困惑度 (perplexity)。令 $P_{ij}$ 表示在高维空间中由高斯核导出并对称化以满足 $\\sum_{i\\neq j} P_{ij} = 1$ 的成对相似性，令 $Q_{ij}$ 表示在低维嵌入空间中由 Student $t$-分布导出并对称化以满足 $\\sum_{i\\neq j} Q_{ij} = 1$ 的成对相似性。t-SNE 的目标函数是 Kullback–Leibler 散度\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right).\n$$\n考虑以下为反映此数据集中典型结果而构建的受控情景：\n- 对于一个代表性的流形内邻近点对，$P_{ij} \\approx 5 \\times 10^{-4}$，而嵌入后的低维相似性为 $Q_{ij} \\approx 4 \\times 10^{-4}$。\n- 对于一个代表性的跨流形远距离点对，$P_{ij} \\approx 1 \\times 10^{-8}$，而由于低维空间中 Student $t$-核的重尾特性以及有限的嵌入空间，$Q_{ij} \\approx 1 \\times 10^{-4}$。\n\n要求你从第一性原理出发，利用上述主导目标函数、基于高斯核的 $P_{ij}$ 和基于 Student $t$-分布的 $Q_{ij}$ 的定性属性，以及给定的经验情景，来论证 t-SNE 如何在局部邻域保持与全局距离失真之间取得平衡。哪个选项最能解释为什么局部邻域得以保持而全局距离失真，并正确地将其与远距离点对的 $P_{ij}$ 和 $Q_{ij}$ 之间的不匹配联系起来？\n\nA. 散度 $D_{\\mathrm{KL}}(P \\parallel Q)$ 用 $P_{ij}$ 对差异进行加权，因此邻近点对（具有较大的 $P_{ij}$）在目标函数中占主导地位。对于给定的流形内点对，其贡献为 $P_{ij} \\log(P_{ij}/Q_{ij}) \\approx (5 \\times 10^{-4}) \\log\\left(\\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}}\\right)$，这比来自远距离跨流形点对的贡献 $(1 \\times 10^{-8}) \\log\\left(\\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}}\\right)$ 大几个数量级。因为远距离点对的 $P_{ij} \\approx 0$，它们与 $Q_{ij}$ 的不匹配受到的惩罚很弱，从而允许全局距离失真，同时保持局部邻域。\n\nB. t-SNE 中最小化的是散度 $D_{\\mathrm{KL}}(Q \\parallel P)$，因此差异由 $Q_{ij}$ 加权，这会严重惩罚分配给远距离点对的较大 $Q_{ij}$ 值，从而防止全局失真。因此，保持邻近点是最小化 $D_{\\mathrm{KL}}(Q \\parallel P)$ 的结果。\n\nC. 保持全局测地距离的流形学习方法，如 Isometric Mapping (Isomap)，通过计算邻域图上的最短路径来防止全局失真；由于 t-SNE 隐式地执行了类似的测地计算，因此全局距离得以保持，而局部邻域可能会失真。\n\nD. 增加 t-SNE 中的困惑度会迫使 $P_{ij}$ 在所有点对上几乎均匀，使得 $D_{\\mathrm{KL}}(P \\parallel Q)$ 对近处和远处的点对几乎同等加权。这消除了全局失真，同时保持了局部邻域。\n\nE. 在低维空间中，用于构建 $Q_{ij}$ 的 Student $t$-核是短尾的，导致除了最近的邻居外，所有点对的 $Q_{ij} \\approx 0$。这确保了全局距离的保持，因为远距离点对的 $P_{ij}$ 和 $Q_{ij}$ 都接近于零，从而使得所有点对的散度都保持很小。\n\n选择唯一的最佳选项。",
            "solution": "问题要求解释为什么 t-分布随机邻域嵌入 (t-SNE) 算法在保持局部邻域结构的同时会扭曲全局距离。解释必须基于其目标函数——Kullback-Leibler (KL) 散度的性质以及一个给定的数值情景。\n\n### 步骤 1：问题陈述验证\n\n首先，我将验证问题陈述。\n\n**已知条件：**\n- 一个来自 $N$ 个神经元的高维神经元发放率数据集。\n- 数据形成两个分离良好的流形 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$。\n- 高维相似性由 $P_{ij}$ 给出，它由高斯核导出，经过对称化和归一化，使得 $\\sum_{i\\neq j} P_{ij} = 1$。\n- 低维相似性由 $Q_{ij}$ 给出，它由 Student $t$-分布导出，经过对称化和归一化，使得 $\\sum_{i\\neq j} Q_{ij} = 1$。\n- t-SNE 需要最小化的目标函数是 KL 散度 $D_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right)$。\n- 一个代表性的流形内邻近点对具有 $P_{ij} \\approx 5 \\times 10^{-4}$ 和 $Q_{ij} \\approx 4 \\times 10^{-4}$。\n- 一个代表性的跨流形远距离点对具有 $P_{ij} \\approx 1 \\times 10^{-8}$ 和 $Q_{ij} \\approx 1 \\times 10^{-4}$。\n\n**验证：**\n1.  **科学基础**：问题陈述在科学上是合理的。它准确地描述了 t-SNE 算法，包括其目标函数、基于高斯核的高维相似性（$P_{ij}$）的定义以及基于 Student $t$-分布的低维相似性（$Q_{ij}$）的定义。关于神经数据形成流形的描述是计算神经科学中的一个标准概念。该算法所陈述的特性（以牺牲全局结构为代价来保持局部结构）是 t-SNE 备有详细记载的特点。\n2.  **适定性**：这个问题是适定的。它提供了一个明确的目标和充分的信息（成本函数的形式和数值示例），足以用来推断其底层机制并评估给定的选项。\n3.  **客观性**：问题以精确、客观和技术性的语言陈述，没有任何主观性或歧义。\n\n**结论：** 问题陈述有效。我将继续进行解答。\n\n### 步骤 2：推导与分析\n\nt-SNE 的目标是找到一个低维嵌入，该嵌入能最小化联合概率分布 $P$（代表高维空间中的相似性）和联合概率分布 $Q$（代表低维空间中的相似性）之间的 KL 散度。目标函数为：\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right)\n$$\n这可以展开为：\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log(P_{ij}) - \\sum_{i \\neq j} P_{ij} \\log(Q_{ij})\n$$\n由于 $P_{ij}$ 值是固定的（从输入数据中导出），$\\sum_{i \\neq j} P_{ij} \\log(P_{ij})$ 项是一个常数。因此，最小化 $D_{\\mathrm{KL}}(P \\parallel Q)$ 等价于最大化交叉熵项 $\\sum_{i \\neq j} P_{ij} \\log(Q_{ij})$。\n\n分析的核心在于理解单个点对 $(i, j)$ 如何对总 KL 散度做出贡献。单个点对的贡献是 $C_{ij} = P_{ij} \\log(P_{ij}/Q_{ij})$。优化过程通过调整低维空间中点的位置来改变 $Q_{ij}$ 的值，从而最小化所有这些贡献的总和。\n\n这个成本函数的关键特征是前置因子 $P_{ij}$。该因子作为对数项 $\\log(P_{ij}/Q_{ij})$ 的权重，该对数项衡量了相似性之间的“不匹配”程度。\n\n让我们分析所提供的两种情景：\n\n**1. 流形内邻近点对（局部结构）：**\n- 已知：$P_{ij} \\approx 5 \\times 10^{-4}$ 且 $Q_{ij} \\approx 4 \\times 10^{-4}$。\n- 在这里，$P_{ij}$ 相对较大，因为点 $i$ 和 $j$ 在高维空间中是近邻。\n- $P_{ij}$ 和 $Q_{ij}$ 之间的任何显著不匹配都将乘以这个相对较大的 $P_{ij}$，从而对成本产生重大贡献。例如，如果算法将这些点在低维图中放置得相距很远，$Q_{ij}$ 会变得非常小，使得 $\\log(P_{ij}/Q_{ij})$ 成为一个大的正数。由此产生的贡献 $P_{ij} \\log(P_{ij}/Q_{ij})$ 将会很大，在优化过程中产生一股强大的“力”，将这些点拉近（从而增加 $Q_{ij}$ 以更好地匹配 $P_{ij}$）。\n- 因此，该算法对于正确表示邻近点的相似性非常敏感。这就是保持局部结构的机制。\n\n**2. 跨流形远距离点对（全局结构）：**\n- 已知：$P_{ij} \\approx 1 \\times 10^{-8}$ 且 $Q_{ij} \\approx 1 \\times 10^{-4}$。\n- 在这里，$P_{ij}$ 极小，因为点 $i$ 和 $j$ 在高维空间中相距很远。\n- 对成本的贡献是 $C_{ij} = P_{ij} \\log(P_{ij}/Q_{ij})$。\n- 即使 $P_{ij}$ 和 $Q_{ij}$ 之间存在巨大不匹配（如情景中所述），对总成本的贡献也由微小的 $P_{ij}$ 加权。优化过程对这种不匹配基本上不敏感。让我们计算近似贡献来定量地看到这一点。\n\n来自邻近点对的贡献：\n$$\nC_{\\text{near}} = P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right) \\approx (5 \\times 10^{-4}) \\ln \\left( \\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}} \\right) = (5 \\times 10^{-4}) \\ln(1.25) \\approx (5 \\times 10^{-4})(0.223) \\approx 1.12 \\times 10^{-4}\n$$\n来自远距离点对的贡献：\n$$\nC_{\\text{far}} = P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right) \\approx (1 \\times 10^{-8}) \\ln \\left( \\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}} \\right) = (1 \\times 10^{-8}) \\ln(10^{-4}) \\approx (1 \\times 10^{-8})(-9.21) \\approx -9.21 \\times 10^{-8}\n$$\n邻近点对贡献的量级 ($|C_{\\text{near}}| \\approx 1.12 \\times 10^{-4}$) 大约是远距离点对贡献的量级 ($|C_{\\text{far}}| \\approx 9.21 \\times 10^{-8}$) 的 1200 倍。\n\n这证实了目标函数主要由邻近点（其中 $P_{ij}$ 较大）的贡献所主导。算法将其精力用于正确排列这些点。对于错误表示远距离点（其中 $P_{ij}$ 几乎为零）之间的距离，它受到的惩罚很弱，从而允许全局结构发生显著失真。\n\n### 步骤 3：逐项分析\n\n**A. 散度 $D_{\\mathrm{KL}}(P \\parallel Q)$ 用 $P_{ij}$ 对差异进行加权，因此邻近点对（具有较大的 $P_{ij}$）在目标函数中占主导地位。对于给定的流形内点对，其贡献为 $P_{ij} \\log(P_{ij}/Q_{ij}) \\approx (5 \\times 10^{-4}) \\log\\left(\\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}}\\right)$，这比来自远距离跨流形点对的贡献 $(1 \\times 10^{-8}) \\log\\left(\\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}}\\right)$ 大几个数量级。因为远距离点对的 $P_{ij} \\approx 0$，它们与 $Q_{ij}$ 的不匹配受到的惩罚很弱，从而允许全局距离失真，同时保持局部邻域。**\n- 该选项正确地指出成本函数由 $P_{ij}$ 加权。它正确地得出结论，这导致来自邻近点对（$P_{ij}$ 较大）的贡献在目标函数中占主导地位。数值计算证实了邻近点对的贡献比远距离点对的贡献大几个数量级。最终的结论——这种加权方式对全局不匹配的惩罚很弱，从而允许全局距离失真，同时保持局部邻域——是该分析的直接而准确的结果。\n- **结论：正确。**\n\n**B. t-SNE 中最小化的是散度 $D_{\\mathrm{KL}}(Q \\parallel P)$，因此差异由 $Q_{ij}$ 加权，这会严重惩罚分配给远距离点对的较大 $Q_{ij}$ 值，从而防止全局失真。因此，保持邻近点是最小化 $D_{\\mathrm{KL}}(Q \\parallel P)$ 的结果。**\n- 该选项在事实上是错误的。t-SNE 算法最小化的是 $D_{\\mathrm{KL}}(P \\parallel Q)$，而不是 $D_{\\mathrm{KL}}(Q \\parallel P)$。这两种 KL 散度是不对称的，具有非常不同的性质。这里提出的逻辑适用于最小化反向 KL 散度，而 t-SNE 并非如此。\n- **结论：错误。**\n\n**C. 保持全局测地距离的流形学习方法，如 Isometric Mapping (Isomap)，通过计算邻域图上的最短路径来防止全局失真；由于 t-SNE 隐式地执行了类似的测地计算，因此全局距离得以保持，而局部邻域可能会失真。**\n- 该选项错误地描述了 t-SNE。虽然它正确地描述了 Isomap，但它错误地声称 t-SNE 执行了类似的测地计算。t-SNE 对亲和度的计算是基于局部高斯核，而不是图中的最短路径距离。如上所示，其目标函数导致全局距离的失真，而不是保持。\n- **结论：错误。**\n\n**D. 增加 t-SNE 中的困惑度会迫使 $P_{ij}$ 在所有点对上几乎均匀，使得 $D_{\\mathrm{KL}}(P \\parallel Q)$ 对近处和远处的点对几乎同等加权。这消除了全局失真，同时保持了局部邻域。**\n- 虽然非常高的困惑度确实会使 $P_{ij}$ 更趋于均匀，但其结论是有缺陷的。如果 $P_{ij}$ 是均匀的，它将不包含关于数据局部结构的任何信息。算法保持局部邻域的目标从一开始就被颠覆了。这种方法会破坏局部结构信息，而不是保持它。\n- **结论：错误。**\n\n**E. 在低维空间中，用于构建 $Q_{ij}$ 的 Student $t$-核是短尾的，导致除了最近的邻居外，所有点对的 $Q_{ij} \\approx 0$。这确保了全局距离的保持，因为远距离点对的 $P_{ij}$ 和 $Q_{ij}$ 都接近于零，从而使得所有点对的散度都保持很小。**\n- 该选项对 Student $t$-分布做出了一个事实性错误的陈述。t-SNE 中使用的自由度为 1 的 $t$-分布是*重尾*的，而不是短尾的。其较慢的、幂律衰减（与高斯分布的指数衰减相比）是帮助分离簇和缓解“拥挤问题”的关键特征。该核是短尾的前提从根本上是错误的。因此，基于这个错误前提的推理也是无效的。\n- **结论：错误。**\n\n分析证实，选项 A 为问题中所描述的 t-SNE 的行为提供了唯一准确和完整的解释。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "理解了算法的内在机制后，下一步便是学习如何有效调控它。困惑度（Perplexity, $\\mathcal{P}$）是 t-SNE 中最关键的超参数，它直接决定了算法关注的邻域尺度。本练习  将通过一个模拟的神经元群体数据，让你亲手实践并探索不同困惑度设置如何影响最终的可视化结果——过低的设置可能导致数据流形破碎，而过高的设置则可能错误地合并了本应分离的神经元集群。",
            "id": "4176842",
            "problem": "您正在分析一个来自小鼠视觉皮层的高维神经元响应数据集，其中每个神经元由一个特征向量 $x_i \\in \\mathbb{R}^D$ 表示，该向量是从 $M$ 个刺激下的钙成像中提取的。该数据集包含群体 $\\mathcal{A}$ 中的 $n_{\\mathcal{A}}$ 个神经元（方位调谐，其平滑变化的偏好角度形成一个带有稀疏区域的连续流形），以及两个类高斯群体 $\\mathcal{B}$ 和 $\\mathcal{C}$（方向选择性神经元，具有相似但不同的平均响应曲线），其规模分别为 $n_{\\mathcal{B}}$ 和 $n_{\\mathcal{C}}$。您计划使用 t-分布随机邻域嵌入 (t-SNE) 来嵌入数据，该方法定义了高维条件近邻度\n$$\np_{j|i} = \\frac{\\exp\\!\\left(-\\frac{\\|x_i - x_j\\|_2^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\!\\left(-\\frac{\\|x_i - x_k\\|_2^2}{2\\sigma_i^2}\\right)},\n$$\n其中局部带宽 $\\sigma_i$ 的选择使得困惑度与用户指定的目标 $\\mathcal{P}$ 相匹配。困惑度通过条件分布 $P_i = \\{p_{j|i}\\}_j$ 的香农熵定义为\n$$\n\\mathcal{P} \\equiv 2^{H(P_i)}, \\quad H(P_i) = -\\sum_j p_{j|i} \\log_2 p_{j|i}.\n$$\n经验上，一个基于欧几里得距离 $\\|x_i - x_j\\|_2$ 构建的 k-近邻（k-Nearest Neighbor, k-NN）图揭示了以下结构：\n- 对于群体 $\\mathcal{A}$，稀疏区域造成了一个瓶颈，使得除非 $k \\ge 12$，否则 k-NN 图会碎裂成多个分量，而当 $k \\ge 12$ 时，群体 $\\mathcal{A}$ 上的图是连通的。\n- 对于群体 $\\mathcal{B}$ 和 $\\mathcal{C}$，当 $k \\le 30$ 时，它们的 k-NN 图保持为两个独立的分量，但当 $k \\ge 50$ 时，它们会合并成一个单一的连通分量。\n\n假设在 t-SNE 中，对于大多数点，目标困惑度 $\\mathcal{P}$ 设定了一个与 $\\mathcal{P}$ 同数量级的有效邻域大小，因此高维近邻度图的定性连通性行为与 $k \\approx \\mathcal{P}$ 的 k-NN 图的行为相似。您必须根据 $p_{j|i}$、$H(P_i)$ 和 $\\mathcal{P}$ 的定义进行推理，以确定 $\\mathcal{P}$ 的选择如何影响低维嵌入中的聚类粒度，特别是群体 $\\mathcal{A}$ 的碎裂以及群体 $\\mathcal{B}$ 与 $\\mathcal{C}$ 的合并。\n\n哪些陈述与上述机制最一致？\n\nA. 使用 $\\mathcal{P} = 5$ 会导致群体 $\\mathcal{A}$ 的碎裂，并保持群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 分离。\n\nB. 使用 $\\mathcal{P} = 15$ 会导致群体 $\\mathcal{A}$ 的碎裂，同时保持群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 分离。\n\nC. 使用 $\\mathcal{P} = 60$ 会保持群体 $\\mathcal{A}$ 的连续性，并将群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 合并成一个单一的聚类。\n\nD. 使用 $\\mathcal{P} = 30$ 会保持群体 $\\mathcal{A}$ 的连续性，并将群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 合并成一个单一的聚类。\n\n选择所有正确的选项。",
            "solution": "用户希望我验证并解决一个关于 t-SNE 在神经科学数据集上应用的问题。\n\n### 问题验证\n\n**1. 提取已知条件**\n- 数据：来自小鼠视觉皮层的神经元响应，每个神经元由一个特征向量 $x_i \\in \\mathbb{R}^D$ 表示。\n- 群体：\n    - $\\mathcal{A}$：$n_{\\mathcal{A}}$ 个神经元，方位调谐，形成一个带有稀疏区域的连续流形。\n    - $\\mathcal{B}$：$n_{\\mathcal{B}}$ 个神经元，类高斯聚类。\n    - $\\mathcal{C}$：$n_{\\mathcal{C}}$ 个神经元，类高斯聚类，与 $\\mathcal{B}$ 不同但邻近。\n- 算法：t-SNE，其高维条件近邻度为 $p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|_2^2 / (2\\sigma_i^2))}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|_2^2 / (2\\sigma_i^2))}$。\n- 困惑度：$\\mathcal{P}$ 是一个用户指定的参数，定义为 $\\mathcal{P} \\equiv 2^{H(P_i)}$，其中 $H(P_i) = -\\sum_j p_{j|i} \\log_2 p_{j|i}$。\n- 来自 k-近邻（$k$-NN）图的经验数据结构：\n    - 群体 $\\mathcal{A}$：当 $k  12$ 时图碎裂，当 $k \\ge 12$ 时图连通。\n    - 群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：当 $k \\le 30$ 时图分离，当 $k \\ge 50$ 时图合并。\n- 核心假设：t-SNE 中的有效邻域大小与困惑度 $\\mathcal{P}$ 处于同一数量级，且 t-SNE 近邻度图的连通性与 $k \\approx \\mathcal{P}$ 的 k-NN 图的连通性相似。\n\n**2. 验证分析**\n- **科学依据：** 该问题牢固地植根于计算神经科学和机器学习领域。使用 t-SNE 可视化神经群体结构是一种标准技术。t-SNE 近邻度和困惑度的数学定义是正确的。困惑度与有效邻域大小之间的关系是应用 t-SNE 时一个公认的启发式规则。该场景在科学上是合理的。\n- **定义明确：** 该问题提供了所有必要信息：通过 $k$-NN 分析揭示的数据结构、t-SNE 参数 $\\mathcal{P}$ 的定义，以及一条清晰的规则（$k \\approx \\mathcal{P}$）将参数与数据结构联系起来。问题精确，并允许一个唯一的、可推导的答案。\n- **客观性：** 该问题使用精确的技术语言陈述，没有主观性或歧义。\n\n**3. 结论**\n问题陈述具有科学合理性、定义明确且客观。它是有效的。我将继续进行解答。\n\n### 解答推导\n\n该问题要求我们根据困惑度参数 $\\mathcal{P}$ 的选择，预测 t-SNE 嵌入的定性结构。关键在于使用所给的假设，即对于给定的 $\\mathcal{P}$，t-SNE 算法的行为可以由一个 k-近邻（$k$-NN）图的行为来近似，其中邻居数 $k$ 约等于困惑度，即 $k \\approx \\mathcal{P}$。\n\n困惑度 $\\mathcal{P} \\equiv 2^{H(P_i)}$（其中 $H(P_i)$ 是香农熵）衡量了在高维空间中，根据概率分布 $\\{p_{j|i}\\}_j$，每个点 $x_i$ 所拥有的有效邻居数。低 $\\mathcal{P}$ 意味着 t-SNE 关注非常局部的结构（邻居少），而高 $\\mathcal{P}$ 意味着它通过考虑更大的邻域来整合更多的全局结构。\n\n问题给出了使用 k-NN 图进行的结构分析结果：\n1.  **群体 $\\mathcal{A}$：** 该群体代表一个连续流形。如果邻域大小太小，无法跨越稀疏区域，它就会碎裂。给出的临界邻域大小为 $k=12$。\n    - 当 $k  12$（低 $k$ 值）时，图会碎裂。\n    - 当 $k \\ge 12$（足够高的 $k$ 值）时，图是连通的。\n2.  **群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：** 这是两个不同但邻近的聚类。它们是分离还是合并，取决于邻域大小是小到足以保持在每个聚类内部，还是大到足以跨越两者。\n    - 当 $k \\le 30$ 时，聚类保持分离。\n    - 当 $k \\ge 50$ 时，聚类合并成一个单一分量。\n\n通过应用假设 $k \\approx \\mathcal{P}$，我们可以将这些 k-NN 阈值转换为 t-SNE 嵌入的困惑度阈值：\n- 要使群体 $\\mathcal{A}$ 被嵌入为一个连续的流形，我们需要 $\\mathcal{P} \\gtrsim 12$。对于 $\\mathcal{P} \\lesssim 12$，它可能会碎裂。\n- 要使群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 被嵌入为分离的聚类，我们需要 $\\mathcal{P} \\lesssim 30$。对于 $\\mathcal{P} \\gtrsim 50$，它们可能会合并。\n\n我们现在使用这些标准来分析每个选项。\n\n**A. 使用 $\\mathcal{P} = 5$ 会导致群体 $\\mathcal{A}$ 的碎裂，并保持群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 分离。**\n我们设定 $\\mathcal{P} = 5$，这意味着有效邻域大小约为 $k \\approx 5$。\n- 对于群体 $\\mathcal{A}$：由于 $5  12$，邻域太小无法跨越稀疏区域。这对应于 $k  12$ 的情况，预测会发生碎裂。陈述的第一部分是一致的。\n- 对于群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：由于 $5 \\le 30$，邻域足够小，不会跨越聚类之间的间隙。这对应于 $k \\le 30$ 的情况，预测会保持分离。陈述的第二部分是一致的。\n两个预测都与陈述相符。因此，选项 A 是**正确**的。\n\n**B. 使用 $\\mathcal{P} = 15$ 会导致群体 $\\mathcal{A}$ 的碎裂，同时保持群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 分离。**\n我们设定 $\\mathcal{P} = 15$，这意味着有效邻域大小约为 $k \\approx 15$。\n- 对于群体 $\\mathcal{A}$：由于 $15 \\ge 12$，邻域足够大可以跨越稀疏区域。这对应于 $k \\ge 12$ 的情况，预测会形成一个连通的、连续的表示。而陈述声称会发生碎裂，这是一个矛盾。\n- 对于群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：由于 $15 \\le 30$，这预测会保持分离，与陈述一致。\n然而，由于对群体 $\\mathcal{A}$ 的预测是错误的，整个选项是**不正确**的。\n\n**C. 使用 $\\mathcal{P} = 60$ 会保持群体 $\\mathcal{A}$ 的连续性，并将群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 合并成一个单一的聚类。**\n我们设定 $\\mathcal{P} = 60$，这意味着有效邻域大小约为 $k \\approx 60$。\n- 对于群体 $\\mathcal{A}$：由于 $60 \\ge 12$，邻域绰绰有余以确保连通性。这对应于 $k \\ge 12$ 的情况，预测会形成一个连续的表示。陈述的第一部分是一致的。\n- 对于群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：由于 $60 \\ge 50$，邻域足够大可以跨越两个聚类。这对应于 $k \\ge 50$ 的情况，预测聚类将会合并。陈述的第二部分是一致的。\n两个预测都与陈述相符。因此，选项 C 是**正确**的。\n\n**D. 使用 $\\mathcal{P} = 30$ 会保持群体 $\\mathcal{A}$ 的连续性，并将群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 合并成一个单一的聚类。**\n我们设定 $\\mathcal{P} = 30$，这意味着有效邻域大小约为 $k \\approx 30$。\n- 对于群体 $\\mathcal{A}$：由于 $30 \\ge 12$，邻域足以确保连通性。这对应于 $k \\ge 12$ 的情况，预测会形成一个连续的表示。陈述的第一部分是一致的。\n- 对于群体 $\\mathcal{B}$ 和 $\\mathcal{C}$：问题陈述当 $k \\le 30$ 时，聚类保持分离。因此，在边界 $k=30$ 时，它们仍然是分离的。而陈述声称它们会合并，这与提供的经验数据（合并发生在 $k \\ge 50$ 时）相矛盾。\n由于对群体 $\\mathcal{B}$ 和 $\\mathcal{C}$ 的预测是错误的，整个选项是**不正确**的。",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "仅仅依赖视觉检查来判断降维效果是远远不够的，我们需要定量的指标来客观评估嵌入的质量。这项练习  旨在让你掌握一种形式化“邻域保持”概念的方法。通过从第一性原理推导并应用一个基于排序的相关性指数，你将学会如何量化一个低维嵌入在多大程度上忠实地保留了原始高维数据的局部拓扑结构。",
            "id": "4176825",
            "problem": "一个实验室在 $n=6$ 个重复条件下，记录了一只清醒且有行为的动物体内 $p$ 个同时记录的神经元的群体活动向量，从而产生了 $n$ 个高维点 $\\{x_{i} \\in \\mathbb{R}^{p}\\}_{i=1}^{6}$。通过一种非线性降维方法（例如，均匀流形近似与投影（UMAP）或 t-分布随机邻域嵌入（t-SNE））从 $\\{x_{i}\\}$ 计算出一个二维嵌入 $\\{y_{i} \\in \\mathbb{R}^{2}\\}_{i=1}^{6}$，其目的是保留反映假定神经流形的局部邻域。\n\n令 $d_{X}(i,j)$ 和 $d_{Y}(i,j)$ 分别表示原始空间和嵌入空间中的成对距离，并令 $r_{ij}$ 和 $s_{ij}$ 分别为当 $i$ 的邻居按 $d_{X}$ 和 $d_{Y}$ 距离升序排列时，$j$ 的对应排名，其中 $r_{ii}=s_{ii}=0$ 且当 $i \\neq j$ 时 $r_{ij},s_{ij} \\in \\{1,\\dots,n-1\\}$。对于固定的邻域大小 $k \\in \\{1,\\dots,n-1\\}$ 和每个查询索引 $i$，将原始空间中的 $k$-最近邻索引集定义为 $J_{i}(k) = \\{ j \\neq i : r_{ij} \\leq k \\}$，并按 $r_{ij}$ 递增的顺序将其排列为 $j_{i,1},\\dots,j_{i,k}$，使得对于 $\\ell \\in \\{1,\\dots,k\\}$ 有 $r_{i,j_{i,\\ell}} = \\ell$。令 $\\beta_{i,\\ell}(k)$ 为当项目 $\\{j_{i,1},\\dots,j_{i,k}\\}$ 按 $s_{ij}$ 递增排序时，由 $s_{ij}$ 导出的排名位置，因此 $\\beta_{i,\\ell}(k) \\in \\{1,\\dots,k\\}$ 是 $j_{i,\\ell}$ 在限制于 $J_{i}(k)$ 的嵌入导出排序中的位置。\n\n任务1：仅从 $k$ 个项目的两种排序之间的斯皮尔曼等级相关系数的定义以及上述定义出发，推导邻域保持指数 $R_{NX}(k)$ 的显式表达式，该指数为 $k$-邻居排序的平均斯皮尔曼相关性：\n- 对于每个 $i$，定义“理想”排序 $[1,2,\\dots,k]$ 与嵌入导出的排序 $[\\beta_{i,1}(k),\\dots,\\beta_{i,k}(k)]$ 之间的斯皮尔曼相关性 $\\rho_{i}(k)$，并证明它可以写成关于平方等级位移 $\\Delta_{i,\\ell}(k) = \\ell - \\beta_{i,\\ell}(k)$ 的封闭形式。\n- 然后将 $R_{NX}(k)$ 定义为 $\\rho_{i}(k)$ 在 $i \\in \\{1,\\dots,n\\}$ 上的平均值。\n\n任务2：考虑以下为 $n=6$ 个条件凭经验观察到的 $k=3$ 邻居排序。对于每个 $i \\in \\{1,\\dots,6\\}$，您将获得：\n- 原始空间中按 $d_{X}$ 排名顺序的前3个邻居索引，$[j_{i,1},j_{i,2},j_{i,3}]$。\n- 这三个相同索引的嵌入导出顺序，限制于 $J_{i}(3)$ 并按 $d_{Y}$ 排序，编码为相应的索引序列 $[\\widehat{j}_{i,1},\\widehat{j}_{i,2},\\widehat{j}_{i,3}]$。\n\n使用这些数据为每个 $i$ 计算 $\\beta_{i,\\ell}(3)$，然后精确评估 $R_{NX}(3)$（不进行四舍五入）。\n\n数据：\n- $i=1$：原始 $[2,3,4]$，嵌入 $[2,3,4]$。\n- $i=2$：原始 $[1,3,5]$，嵌入 $[1,5,3]$。\n- $i=3$：原始 $[2,4,6]$，嵌入 $[6,4,2]$。\n- $i=4$：原始 $[1,2,5]$，嵌入 $[1,2,5]$。\n- $i=5$：原始 $[2,4,6]$，嵌入 $[2,6,4]$。\n- $i=6$：原始 $[3,4,5]$，嵌入 $[3,4,5]$。\n\n任务3：基于秩次统计和弯曲流形上 $k$-最近邻稳定性的第一性原理，简要论证（无需借助均匀流形近似与投影（UMAP）或 t-分布随机邻居嵌入（t-SNE）的任何具体算法细节），当 $k$ 增加以及在观测噪声方差保持固定的情况下神经流形的采样密度降低时，$R_{NX}(k)$ 预计会如何变化。\n\n您最终报告的结果必须是上述数据集的 $R_{NX}(3)$ 的精确值。不要包含单位。不要在最终答案框中包含任何额外文本。",
            "solution": "该问题要求完成三个任务：第一，推导邻域保持指数 $R_{NX}(k)$ 的公式；第二，为 $k=3$ 的特定数据集计算该指数；第三，就该指数的行为提供定性论证。该问题定义明确，并在流形学习和秩次统计的原理上有科学依据。\n\n### 任务1：邻域保持指数 $R_{NX}(k)$ 的推导\n\n问题将指数 $R_{NX}(k)$ 定义为为 $n$ 个数据点中的每一个计算的斯皮尔曼等级相关系数 $\\rho_i(k)$ 的平均值。对于每个点 $i$，该相关性是其在高维原始空间中的 $k$-最近邻的排名顺序与在低维嵌入中相同邻居的排名顺序之间的相关性。\n\n假设有 $k$ 个项目。它们在两种不同排序中的等级由向量 $u = [u_1, u_2, \\dots, u_k]$ 和 $v = [v_1, v_2, \\dots, v_k]$ 给出。斯皮尔曼等级相关系数 $\\rho$ 被定义为这些等级变量的皮尔逊相关系数。在没有平级的情况下（这里是成立的，因为 $u$ 和 $v$ 是 $[1, 2, \\dots, k]$ 的排列），公式简化为：\n$$ \\rho = 1 - \\frac{6 \\sum_{j=1}^{k} d_j^2}{k(k^2-1)} $$\n其中 $d_j = u_j - v_j$ 是第 $j$ 个项目的等级差异。\n\n在我们的具体问题中，对于每个查询点 $i$，“项目”是其在原始空间中的 $k$-最近邻，由集合 $J_i(k) = \\{j_{i,1}, \\dots, j_{i,k}\\}$ 给出。\n第一个排序是“理想”排序，源自原始空间距离 $d_X$。邻居 $j_{i,\\ell}$ 被定义为具有第 $\\ell$ 的排名。因此，第一个等级向量可以写为 $L = [1, 2, \\dots, k]$。\n\n第二个排序源自嵌入空间距离 $d_Y$。问题将 $\\beta_{i,\\ell}(k)$ 定义为邻居 $j_{i,\\ell}$（原始排名为 $\\ell$）在集合 $J_i(k)$ 内根据 $d_Y$ 重新排序时的排名（从 $1$ 到 $k$）。因此，对应于原始项目按 $j_{i,1}, \\dots, j_{i,k}$ 排序的第二个等级向量是 $B_i(k) = [\\beta_{i,1}(k), \\beta_{i,2}(k), \\dots, \\beta_{i,k}(k)]$。\n\n原始排名为 $\\ell$ 的邻居的等级差异是 $\\ell - \\beta_{i,\\ell}(k)$。问题将此定义为等级位移 $\\Delta_{i,\\ell}(k)$。\n因此，点 $i$ 的平方差之和为 $\\sum_{\\ell=1}^{k} (\\ell - \\beta_{i,\\ell}(k))^2 = \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2$。\n\n将此代入斯皮尔曼公式，得到点 $i$ 的相关性 $\\rho_i(k)$：\n$$ \\rho_i(k) = 1 - \\frac{6 \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2}{k(k^2-1)} $$\n这就是所要求的 $\\rho_i(k)$ 关于平方等级位移的表达式。\n\n总的邻域保持指数 $R_{NX}(k)$ 是这些单个相关系数在所有 $n$ 个点上的平均值：\n$$ R_{NX}(k) = \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i(k) $$\n代入 $\\rho_i(k)$ 的表达式：\n$$ R_{NX}(k) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( 1 - \\frac{6 \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2}{k(k^2-1)} \\right) $$\n这可以重写为：\n$$ R_{NX}(k) = 1 - \\frac{6}{n k (k^2-1)} \\sum_{i=1}^{n} \\sum_{\\ell=1}^{k} \\Delta_{i,\\ell}(k)^2 $$\n\n### 任务2：计算 $R_{NX}(3)$\n\n我们已知 $n=6$ 和 $k=3$。$\\rho_i(3)$ 公式中分母的项是 $k(k^2-1) = 3(3^2-1) = 3(8) = 24$。\n$R_{NX}(3)$ 的公式是：\n$$ R_{NX}(3) = 1 - \\frac{6}{6 \\cdot 24} \\sum_{i=1}^{6} \\sum_{\\ell=1}^{3} \\Delta_{i,\\ell}(3)^2 = 1 - \\frac{1}{24} \\sum_{i=1}^{6} \\sum_{\\ell=1}^{3} (\\ell - \\beta_{i,\\ell}(3))^2 $$\n我们现在必须为每个点 $i \\in \\{1, \\dots, 6\\}$ 计算平方等级位移的总和。\n\n-   **对于 $i=1$**：\n    -   按 $d_X$ 排序的原始 $k=3$ 邻居：$[j_{1,1}, j_{1,2}, j_{1,3}] = [2, 3, 4]$。理想等级向量为 $[1, 2, 3]$。\n    -   这些相同邻居的嵌入顺序：$[2, 3, 4]$。\n    -   $j_{1,1}=2$ 的排名是 1。所以，$\\beta_{1,1}(3)=1$。\n    -   $j_{1,2}=3$ 的排名是 2。所以，$\\beta_{1,2}(3)=2$。\n    -   $j_{1,3}=4$ 的排名是 3。所以，$\\beta_{1,3}(3)=3$。\n    -   嵌入等级向量为 $[\\beta_{1,1}(3), \\beta_{1,2}(3), \\beta_{1,3}(3)] = [1, 2, 3]$。\n    -   平方位移之和：$\\sum_{\\ell=1}^{3} \\Delta_{1,\\ell}(3)^2 = (1-1)^2 + (2-2)^2 + (3-3)^2 = 0$。\n\n-   **对于 $i=2$**：\n    -   原始顺序：$[j_{2,1}, j_{2,2}, j_{2,3}] = [1, 3, 5]$。理想等级：$[1, 2, 3]$。\n    -   嵌入顺序：$[1, 5, 3]$。\n    -   $j_{2,1}=1$ 的排名是 1。所以，$\\beta_{2,1}(3)=1$。\n    -   $j_{2,2}=3$ 的排名是 3。所以，$\\beta_{2,2}(3)=3$。\n    -   $j_{2,3}=5$ 的排名是 2。所以，$\\beta_{2,3}(3)=2$。\n    -   嵌入等级向量：$[1, 3, 2]$。\n    -   平方位移之和：$\\sum_{\\ell=1}^{3} \\Delta_{2,\\ell}(3)^2 = (1-1)^2 + (2-3)^2 + (3-2)^2 = 0 + 1 + 1 = 2$。\n\n-   **对于 $i=3$**：\n    -   原始顺序：$[j_{3,1}, j_{3,2}, j_{3,3}] = [2, 4, 6]$。理想等级：$[1, 2, 3]$。\n    -   嵌入顺序：$[6, 4, 2]$。\n    -   $j_{3,1}=2$ 的排名是 3。所以，$\\beta_{3,1}(3)=3$。\n    -   $j_{3,2}=4$ 的排名是 2。所以，$\\beta_{3,2}(3)=2$。\n    -   $j_{3,3}=6$ 的排名是 1。所以，$\\beta_{3,3}(3)=1$。\n    -   嵌入等级向量：$[3, 2, 1]$。\n    -   平方位移之和：$\\sum_{\\ell=1}^{3} \\Delta_{3,\\ell}(3)^2 = (1-3)^2 + (2-2)^2 + (3-1)^2 = 4 + 0 + 4 = 8$。\n\n-   **对于 $i=4$**：\n    -   原始顺序：$[1, 2, 5]$。嵌入顺序：$[1, 2, 5]$。\n    -   这是一个完美的保持，与 $i=1$ 的情况相同。\n    -   嵌入等级向量：$[1, 2, 3]$。\n    -   平方位移之和：$\\sum_{\\ell=1}^{3} \\Delta_{4,\\ell}(3)^2 = 0$。\n\n-   **对于 $i=5$**：\n    -   原始顺序：$[2, 4, 6]$。嵌入顺序：$[2, 6, 4]$。\n    -   $j_{5,1}=2$ 的排名是 1。所以，$\\beta_{5,1}(3)=1$。\n    -   $j_{5,2}=4$ 的排名是 3。所以，$\\beta_{5,2}(3)=3$。\n    -   $j_{5,3}=6$ 的排名是 2。所以，$\\beta_{5,3}(3)=2$。\n    -   嵌入等级向量：$[1, 3, 2]$。\n    -   平方位移之和：$\\sum_{\\ell=1}^{3} \\Delta_{5,\\ell}(3)^2 = (1-1)^2 + (2-3)^2 + (3-2)^2 = 0 + 1 + 1 = 2$。\n\n-   **对于 $i=6$**：\n    -   原始顺序：$[3, 4, 5]$。嵌入顺序：$[3, 4, 5]$。\n    -   这是一个完美的保持，与 $i=1$ 的情况相同。\n    -   嵌入等级向量：$[1, 2, 3]$。\n    -   平方位移之和：$\\sum_{\\ell=1}^{3} \\Delta_{6,\\ell}(3)^2 = 0$。\n\n现在，我们将所有 $i$ 的平方位移相加：\n$$ \\sum_{i=1}^{6} \\sum_{\\ell=1}^{3} \\Delta_{i,\\ell}(3)^2 = 0 + 2 + 8 + 0 + 2 + 0 = 12 $$\n最后，我们计算 $R_{NX}(3)$：\n$$ R_{NX}(3) = 1 - \\frac{1}{24} (12) = 1 - \\frac{12}{24} = 1 - \\frac{1}{2} = \\frac{1}{2} $$\n邻域保持指数的值是 $\\frac{1}{2}$。\n\n### 任务3：$R_{NX}(k)$ 的定性行为\n\n此任务要求就 $R_{NX}(k)$ 如何随邻域大小 $k$ 和数据质量（采样密度和噪声）变化提供定性论证。\n\n1.  **对 $k$ 的依赖性**：指数 $R_{NX}(k)$ 衡量了排名最高为 $k$ 的邻居的秩次顺序的保持情况。像 UMAP 和 t-SNE 这样的非线性降维方法旨在优先保持局部结构。这意味着最近邻的相对位置被高保真地捕捉。因此，对于较小的 $k$ 值，我们期望 $R_{NX}(k)$ 接近其最大值 1。随着 $k$ 的增加，我们包含了离查询点 $i$ 越来越远的点。在弯曲的流形上，高维环境空间中的欧几里得距离越来越不能代表沿流形的真实测地线距离。嵌入过程试图将流形“展开”到一个平坦的低维空间中。这个过程不可避免地会引入失真，特别是对于不在同一局部邻域中的点对。因此，较远邻居的排名顺序更有可能被嵌入过程改变。因此，随着 $k$ 的增加，$R_{NX}(k)$ 预计会下降，这反映了在保持局部结构与全局结构之间的权衡。\n\n2.  **对采样密度和噪声的依赖性**：采样密度的降低和观测噪声的增加都会降低从中推断流形结构的数据质量。\n    -   **采样密度降低**：当流形被更稀疏地采样时，环境空间中一个点的 $k$-最近邻不太可能代表其沿流形的真实最近邻。算法拥有的信息更少，难以估计局部几何属性（例如，切平面、曲率），导致嵌入不那么准确。这种准确性的降低将表现为原始空间和嵌入之间邻居排名顺序的更多不一致，导致对于任何固定的 $k$，$R_{NX}(k)$ 都会降低。\n    -   **噪声方差增加**：当观测噪声很高时，数据点 $\\{x_i\\}$ 会偏离其在流形上的真实位置。这种噪声会掩盖底层的流形结构，使算法难以区分真实的几何特征和噪声引起的伪影。观测数据中的局部邻域关系变得不再是真实拓扑的可靠指标。任何嵌入算法都将难以生成忠实的表示，导致邻居排序的对齐性下降。因此，随着噪声方差的增加，$R_{NX}(k)$ 预计会下降。\n\n总而言之，$R_{NX}(k)$ 通常随着 $k$ 的增加以及数据质量的下降（更低的采样密度或更高的噪声）而减小。\n\n最终需要的答案是任务2的数值结果。",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        }
    ]
}