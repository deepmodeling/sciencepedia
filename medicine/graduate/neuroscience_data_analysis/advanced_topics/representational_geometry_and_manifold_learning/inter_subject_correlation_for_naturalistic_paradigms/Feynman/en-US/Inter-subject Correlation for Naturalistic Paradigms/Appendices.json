{
    "hands_on_practices": [
        {
            "introduction": "This first exercise grounds our understanding of Inter-Subject Correlation (ISC) in its fundamental statistical definition. Starting with pre-processed time series data from two individuals watching a movie, you will calculate the ISC value from basic summary statistics. This practice reinforces the connection between ISC and the Pearson correlation coefficient and introduces the critical step of evaluating its statistical significance against a properly constructed null distribution .",
            "id": "4170720",
            "problem": "Two participants watched the same naturalistic movie while undergoing functional magnetic resonance imaging. After standard preprocessing appropriate for naturalistic stimuli (including motion regression, high-pass filtering, and removal of low-order trends), the regional Blood Oxygenation Level Dependent (BOLD) time series from a single cortical parcel were extracted for each participant across $T=120$ time points. Let the preprocessed time series be $\\{x_t\\}_{t=1}^{T}$ and $\\{y_t\\}_{t=1}^{T}$. The Inter-Subject Correlation (ISC) is defined as the Pearson correlation coefficient computed across time between the two participants’ time series.\n\nYou are given sufficient statistics computed from the preprocessed time series:\n- $\\sum_{t=1}^{T} x_t = 0$ and $\\sum_{t=1}^{T} y_t = 0$,\n- $\\sum_{t=1}^{T} x_t^2 = 120$ and $\\sum_{t=1}^{T} y_t^2 = 120$,\n- $\\sum_{t=1}^{T} x_t y_t = 41.4$.\n\nA circular time-shift null procedure was used to approximate the chance distribution of ISC while preserving each series’ autocorrelation: one series was circularly shifted by lags spanning the full duration (excluding the zero-lag alignment), producing $K=1000$ surrogate alignments. The resulting surrogate ISC values had empirical mean $\\mu_0 = 0$ and empirical standard deviation $\\sigma_0 = 0.08$. Assume a Gaussian approximation for the surrogate distribution for the purpose of inference.\n\nTasks:\n1) Using only the definitions of sample covariance and Pearson correlation, compute the ISC between the two subjects from the provided sufficient statistics. Report the ISC rounded to four significant figures.\n2) Using the null distribution parameters $\\mu_0$ and $\\sigma_0$, compute a one-tailed $z$-score for the observed ISC and decide whether the observed synchrony reflects shared processing above chance at significance level $\\alpha = 0.05$ (one-tailed). Justify your interpretation; no numerical value needs to be reported as the final answer for this part.\n\nExpress the final numerical answer for the ISC value only (no units).",
            "solution": "The Inter-Subject Correlation (ISC) for two time series $\\{x_t\\}$ and $\\{y_t\\}$ recorded during the same naturalistic stimulus is the Pearson correlation coefficient across time. Starting from the fundamental definitions, the sample covariance between $x$ and $y$ is\n$$\n\\operatorname{cov}(x,y) \\;=\\; \\frac{1}{T-1} \\sum_{t=1}^{T} \\big(x_t - \\bar{x}\\big)\\big(y_t - \\bar{y}\\big),\n$$\nwhere $\\bar{x} = \\frac{1}{T}\\sum_{t=1}^{T} x_t$ and $\\bar{y} = \\frac{1}{T}\\sum_{t=1}^{T} y_t$ are the sample means. The sample variances are\n$$\ns_x^2 \\;=\\; \\frac{1}{T-1} \\sum_{t=1}^{T} \\big(x_t - \\bar{x}\\big)^2, \n\\qquad\ns_y^2 \\;=\\; \\frac{1}{T-1} \\sum_{t=1}^{T} \\big(y_t - \\bar{y}\\big)^2.\n$$\nThe Pearson correlation coefficient is then\n$$\nr \\;=\\; \\frac{\\operatorname{cov}(x,y)}{s_x s_y}\n\\;=\\;\n\\frac{\\sum_{t=1}^{T} \\big(x_t - \\bar{x}\\big)\\big(y_t - \\bar{y}\\big)}{\\sqrt{\\left[\\sum_{t=1}^{T} \\big(x_t - \\bar{x}\\big)^2\\right]\\left[\\sum_{t=1}^{T} \\big(y_t - \\bar{y}\\big)^2\\right]}}.\n$$\n\nGiven $\\sum_{t=1}^{T} x_t = 0$ and $\\sum_{t=1}^{T} y_t = 0$, it follows that $\\bar{x} = 0$ and $\\bar{y} = 0$. Therefore,\n$$\n\\sum_{t=1}^{T} \\big(x_t - \\bar{x}\\big)^2 \\;=\\; \\sum_{t=1}^{T} x_t^2 \\;=\\; 120,\n\\qquad\n\\sum_{t=1}^{T} \\big(y_t - \\bar{y}\\big)^2 \\;=\\; \\sum_{t=1}^{T} y_t^2 \\;=\\; 120,\n$$\nand\n$$\n\\sum_{t=1}^{T} \\big(x_t - \\bar{x}\\big)\\big(y_t - \\bar{y}\\big) \\;=\\; \\sum_{t=1}^{T} x_t y_t \\;=\\; 41.4.\n$$\nThus the ISC is\n$$\nr \\;=\\; \\frac{41.4}{\\sqrt{120 \\times 120}} \\;=\\; \\frac{41.4}{120} \\;=\\; 0.345.\n$$\nRounded to four significant figures, the ISC is $0.3450$.\n\nTo interpret whether this ISC reflects shared processing above chance under naturalistic stimulation, we use the circular time-shift null distribution. This null preserves the autocorrelation structure of each series by circularly shifting one relative to the other, which is important for time series recorded during continuous stimuli, and approximates the distribution of ISC expected from chance temporal alignment.\n\nUnder the Gaussian approximation with null mean $\\mu_0 = 0$ and null standard deviation $\\sigma_0 = 0.08$, the one-tailed $z$-score for the observed ISC is\n$$\nz \\;=\\; \\frac{r - \\mu_0}{\\sigma_0} \\;=\\; \\frac{0.345 - 0}{0.08} \\;=\\; 4.3125.\n$$\nA one-tailed $p$-value under the standard normal is\n$$\np \\;=\\; 1 - \\Phi(z),\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal. For $z = 4.3125$, $p$ is on the order of $10^{-5}$, specifically approximately $8 \\times 10^{-6}$. At $\\alpha = 0.05$ (one-tailed), this is far below the threshold, indicating the observed ISC is well above chance and consistent with shared processing of the naturalistic movie between the two participants.\n\nConceptually, this aligns with the scope of Inter-Subject Correlation analysis in naturalistic paradigms: a positive and statistically significant ISC suggests that time-locked, stimulus-evoked neural dynamics are shared across individuals, beyond what would be expected from autocorrelation or other idiosyncratic fluctuations captured by the circular shift null model.",
            "answer": "$$\\boxed{0.3450}$$"
        },
        {
            "introduction": "Real-world brain data is often influenced by factors other than the shared stimulus, such as synchronized eye movements or physiological noise. This practice addresses how to disentangle true stimulus-driven synchrony from these confounds using partial correlation. You will implement a computational model to compute both raw ISC and partial ISC, demonstrating how regressing out covariates can refine our estimate of shared neural processing .",
            "id": "4170718",
            "problem": "You are given a computational task in the domain of neuroscience data analysis focused on Inter-Subject Correlation (ISC) for naturalistic paradigms. The goal is to compute raw ISC and partial ISC in a simulated visual cortex setting, where partial ISC controls for eye movement covariates using linear regression. The computation must be performed from first principles and expressed in purely mathematical and algorithmic terms. Inter-Subject Correlation (ISC) is defined as the mean correlation across all unique pairs of subjects.\n\nUse the following fundamental base for your derivation and implementation:\n- The Pearson correlation is computed between zero-mean signals and quantifies linear association.\n- Ordinary least squares regression yields residuals by orthogonally projecting the response onto the complement of the covariate subspace.\n- Partial correlation between two variables controlling for covariates can be computed by correlating the regression residuals.\n\nAngle units for all trigonometric functions are in radians. There are no physical units involved. All final numeric outputs must be reported as decimals rounded to $6$ decimal places.\n\nTask:\n1. For each test case below, construct the subject-wise visual cortex time-series and eye movement covariates according to the provided parameters.\n2. Compute the raw ISC as the mean across all pairwise Pearson correlations between subjects’ visual cortex time-series.\n3. Compute the partial ISC as the mean across all pairwise Pearson correlations between subjects’ residual time-series after regressing out the eye movement covariates and an intercept per subject.\n4. If any subject’s residual time-series has zero variance, define the correlation for any pair involving that subject as $0$ by convention to maintain a well-posed result.\n\nDefinitions of the test suite (each test case is fully specified by mathematical parameters and must be generated deterministically; all trigonometric input angles are in radians):\n\nLet $t \\in \\{0,1,\\ldots,T-1\\}$ be discrete time indices.\n\n- Common stimulus component used where specified:\n  $s(t) = \\sin\\left(\\frac{2\\pi t}{12}\\right) + 0.5 \\sin\\left(\\frac{2\\pi t}{6}\\right)$.\n\n- Subject $i$ covariates:\n  $c^{(1)}_i(t)$ and $c^{(2)}_i(t)$ as specified per case.\n  The response for subject $i$ is constructed as:\n  $y_i(t) = \\alpha_i \\, s(t) + \\gamma_i \\, c^{(1)}_i(t) + \\delta_i \\, c^{(2)}_i(t) + \\varepsilon_i(t)$,\n  where $\\varepsilon_i(t)$ is deterministic noise as specified.\n\nTest Suite:\n- Case A (happy path; eye movements are substantial confounds, subject-specific):\n  - Number of subjects $n = 4$, time points $T = 60$.\n  - For all subjects, use the common stimulus $s(t)$.\n  - Covariates:\n    $c^{(1)}_i(t) = \\sin\\left(\\frac{2\\pi t}{10} + \\phi_i\\right)$, \n    $c^{(2)}_i(t) = \\cos\\left(\\frac{2\\pi t}{15} + \\psi_i\\right)$.\n  - Phases:\n    $\\phi = [0.0, 0.3, -0.2, 0.5]$, \n    $\\psi = [0.1, -0.1, 0.2, -0.3]$.\n  - Coefficients:\n    $\\alpha = [1.00, 0.90, 1.10, 1.05]$, \n    $\\gamma = [0.80, 0.70, 0.75, 0.85]$, \n    $\\delta = [0.60, 0.65, 0.55, 0.60]$.\n  - Noise:\n    $\\varepsilon_i(t) = 0.1 \\sin\\left(\\frac{2\\pi t}{7} + \\theta_i\\right)$ with \n    $\\theta = [0.0, 0.4, -0.5, 0.2]$.\n\n- Case B (covariates orthogonal to the response; partial ISC approximately equals raw ISC):\n  - Number of subjects $n = 3$, time points $T = 60$.\n  - For all subjects, use the common stimulus $s(t)$.\n  - Covariates:\n    $c^{(1)}_i(t) = \\sin\\left(\\frac{2\\pi t}{9} + \\phi_i\\right)$, \n    $c^{(2)}_i(t) = \\cos\\left(\\frac{2\\pi t}{11} + \\psi_i\\right)$.\n  - Phases:\n    $\\phi = [0.2, -0.3, 0.4]$, \n    $\\psi = [-0.2, 0.1, -0.4]$.\n  - Coefficients:\n    $\\alpha = [1.00, 0.80, 1.20]$, \n    $\\gamma = [0.00, 0.00, 0.00]$, \n    $\\delta = [0.00, 0.00, 0.00]$.\n  - Noise:\n    $\\varepsilon_i(t) = 0.05 \\sin\\left(\\frac{2\\pi t}{13} + \\theta_i\\right)$ with \n    $\\theta = [0.1, -0.2, 0.3]$.\n\n- Case C (boundary case; perfect collinearity in covariates; response entirely explained by covariates; identical covariates across subjects):\n  - Number of subjects $n = 3$, time points $T = 60$.\n  - No stimulus: set $s(t) = 0$ for all $t$.\n  - Identical covariates for all subjects:\n    $c^{(1)}(t) = \\sin\\left(\\frac{2\\pi t}{8}\\right)$, \n    $c^{(2)}(t) = 2\\, c^{(1)}(t)$.\n    For all subjects $i$, set $c^{(1)}_i(t) = c^{(1)}(t)$ and $c^{(2)}_i(t) = c^{(2)}(t)$.\n  - Coefficients:\n    $\\gamma = [0.50, 0.90, 0.70]$, \n    $\\delta = [1.00, 1.80, 1.40]$.\n  - Noise:\n    $\\varepsilon_i(t) = 0$ for all $i,t$.\n\nAlgorithmic Requirements:\n- For each subject in each case, perform a least squares regression of $y_i(t)$ onto an intercept and the two covariates for that subject to obtain residuals $r_i(t)$.\n- Compute raw ISC as the mean of all pairwise correlations between $\\{y_i(t)\\}$, $i=1,\\ldots,n$, over all $t$.\n- Compute partial ISC as the mean of all pairwise correlations between $\\{r_i(t)\\}$.\n- If any subject’s series in a pair has zero variance, define the correlation for that pair as $0$.\n\nYour program should produce a single line of output containing results aggregated across the three cases in the following format:\n- A comma-separated list enclosed in square brackets, with values ordered as $[\\text{raw\\_A}, \\text{partial\\_A}, \\text{raw\\_B}, \\text{partial\\_B}, \\text{raw\\_C}, \\text{partial\\_C}]$.\n- Each value must be a decimal rounded to $6$ decimal places.\n\nNo user input is allowed; all values must be generated internally using the specified parameters.",
            "solution": "The problem is valid as it is scientifically grounded in statistical methods, well-posed with all necessary parameters defined, and objective in its formulation. The task is to compute raw and partial Inter-Subject Correlation (ISC) for simulated neuroscience data across three distinct test cases.\n\nThe solution is developed by first principles, adhering to the mathematical definitions of Pearson correlation, ordinary least squares (OLS) regression, and partial correlation.\n\n**1. Mathematical Foundations**\n\n**1.1. Pearson Correlation Coefficient**\nThe Pearson correlation coefficient, $\\rho$, quantifies the linear relationship between two time-series vectors, $\\mathbf{u}$ and $\\mathbf{v}$, each of length $T$. It is calculated by dividing the covariance of the two variables by the product of their standard deviations. For two discrete time-series $u(t)$ and $v(t)$ where $t \\in \\{0, 1, \\ldots, T-1\\}$, the formula is:\n$$ \\rho_{\\mathbf{u},\\mathbf{v}} = \\frac{\\sum_{t=0}^{T-1} (u(t) - \\bar{u})(v(t) - \\bar{v})}{\\sqrt{\\sum_{t=0}^{T-1} (u(t) - \\bar{u})^2} \\sqrt{\\sum_{t=0}^{T-1} (v(t) - \\bar{v})^2}} $$\nwhere $\\bar{u}$ and $\\bar{v}$ are the respective means of the time-series. If either time-series has zero variance (i.e., its standard deviation is $0$), the correlation is undefined. As per the problem specification, we define the correlation to be $0$ in such cases.\n\n**1.2. Ordinary Least Squares (OLS) Regression and Residuals**\nTo control for the effects of covariates, we use OLS regression. For a given subject $i$, we model their response time-series $\\mathbf{y}_i$ as a linear combination of covariates and an intercept. The model is:\n$$ y_i(t) = \\beta_0 + \\beta_1 c^{(1)}_i(t) + \\beta_2 c^{(2)}_i(t) + r_i(t) $$\nIn matrix form, this is $\\mathbf{y}_i = X_i \\boldsymbol{\\beta}_i + \\mathbf{r}_i$, where:\n- $\\mathbf{y}_i$ is the $(T \\times 1)$ response vector.\n- $X_i$ is the $(T \\times 3)$ design matrix. Its columns are a vector of ones (for the intercept $\\beta_0$), the first covariate vector $\\mathbf{c}^{(1)}_i$, and the second covariate vector $\\mathbf{c}^{(2)}_i$.\n- $\\boldsymbol{\\beta}_i$ is the $(3 \\times 1)$ vector of regression coefficients.\n- $\\mathbf{r}_i$ is the $(T \\times 1)$ vector of residuals, representing the portion of $\\mathbf{y}_i$ not explained by the linear model.\n\nThe OLS estimate for the coefficients, $\\hat{\\boldsymbol{\\beta}}_i$, is found by minimizing the sum of squared residuals. The solution is given by:\n$$ \\hat{\\boldsymbol{\\beta}}_i = (X_i^T X_i)^{+} X_i^T \\mathbf{y}_i $$\nwhere $(X_i^T X_i)^{+}$ is the Moore-Penrose pseudo-inverse of $X_i^T X_i$. The use of the pseudo-inverse ensures a unique solution even if the columns of $X_i$ are collinear, as in Case C.\n\nThe vector of residuals is then calculated as the difference between the actual and predicted responses:\n$$ \\mathbf{r}_i = \\mathbf{y}_i - X_i \\hat{\\boldsymbol{\\beta}}_i $$\n\n**1.3. Partial Correlation and Inter-Subject Correlation (ISC)**\nPartial correlation between two variables is the correlation between their residuals after regressing out one or more control variables. In this problem, partial ISC is computed based on the residuals $\\mathbf{r}_i$ obtained from the OLS regression.\n\nISC is defined as the mean correlation across all unique pairs of subjects. For $n$ subjects, there are $\\binom{n}{2} = \\frac{n(n-1)}{2}$ unique pairs.\n- **Raw ISC**: The average of pairwise Pearson correlations between the raw response time-series, $\\{\\mathbf{y}_i\\}$.\n$$ \\text{ISC}_{\\text{raw}} = \\frac{1}{\\binom{n}{2}} \\sum_{1 \\le i < j \\le n} \\rho(\\mathbf{y}_i, \\mathbf{y}_j) $$\n- **Partial ISC**: The average of pairwise Pearson correlations between the residual time-series, $\\{\\mathbf{r}_i\\}$.\n$$ \\text{ISC}_{\\text{partial}} = \\frac{1}{\\binom{n}{2}} \\sum_{1 \\le i < j \\le n} \\rho(\\mathbf{r}_i, \\mathbf{r}_j) $$\n\n**2. Algorithmic Procedure**\n\nThe solution will be computed by executing the following steps for each test case (A, B, and C):\n\n1.  **Data Generation**: For each subject $i=1, \\ldots, n$ in the test case:\n    a. Generate the discrete time vector $t = [0, 1, \\ldots, T-1]$.\n    b. Generate the common stimulus $s(t)$, subject-specific covariates $c^{(1)}_i(t)$ and $c^{(2)}_i(t)$, and noise $\\varepsilon_i(t)$ according to the case-specific formulas and parameters.\n    c. Construct the subject's full response time-series $y_i(t) = \\alpha_i s(t) + \\gamma_i c^{(1)}_i(t) + \\delta_i c^{(2)}_i(t) + \\varepsilon_i(t)$.\n\n2.  **Raw ISC Calculation**:\n    a. Collect the set of all response time-series vectors $\\{\\mathbf{y}_1, \\ldots, \\mathbf{y}_n\\}$.\n    b. Iterate through all unique pairs of subjects $(i, j)$ with $i < j$.\n    c. For each pair, compute the Pearson correlation $\\rho(\\mathbf{y}_i, \\mathbf{y}_j)$.\n    d. Compute the raw ISC as the arithmetic mean of these pairwise correlations.\n\n3.  **Partial ISC Calculation**:\n    a. For each subject $i=1, \\ldots, n$:\n        i. Construct the design matrix $X_i = [\\mathbf{1}, \\mathbf{c}^{(1)}_i, \\mathbf{c}^{(2)}_i]$.\n        ii. Perform OLS regression of $\\mathbf{y}_i$ on $X_i$ to find the coefficient estimates $\\hat{\\boldsymbol{\\beta}}_i$.\n        iii. Calculate the residual vector $\\mathbf{r}_i = \\mathbf{y}_i - X_i \\hat{\\boldsymbol{\\beta}}_i$.\n    b. Collect the set of all residual time-series vectors $\\{\\mathbf{r}_1, \\ldots, \\mathbf{r}_n\\}$.\n    c. Iterate through all unique pairs of subjects $(i, j)$ with $i < j$.\n    d. For each pair, compute the Pearson correlation $\\rho(\\mathbf{r}_i, \\mathbf{r}_j)$, applying the rule that the correlation is $0$ if any residual vector has zero variance.\n    e. Compute the partial ISC as the arithmetic mean of these pairwise residual correlations.\n\n**Analysis of Test Cases:**\n\n-   **Case A**: A standard scenario where subjects' responses are a mix of a common stimulus and subject-specific confounds (eye movements). We expect the raw ISC to be inflated by the shared structure of the covariates. Partial ISC, by removing the effect of these covariates, should yield a value more representative of the correlation due to the shared stimulus $s(t)$.\n\n-   **Case B**: The coefficients for the covariates, $\\gamma$ and $\\delta$, are set to zero. Thus, the covariates $c^{(1)}_i$ and $c^{(2)}_i$ do not contribute to the response $y_i$. Regressing them out should have a minimal effect. We expect the partial ISC to be approximately equal to the raw ISC.\n\n-   **Case C**: A boundary case designed to test robustness. The covariates are perfectly collinear ($c^{(2)}(t) = 2c^{(1)}(t)$) and identical across subjects. The response $y_i(t)$ is constructed as a linear combination of these covariates with no stimulus or noise. The raw responses for different subjects will be scalar multiples of each other, e.g., $y_i(t) = k_i c^{(1)}(t)$, resulting in perfect correlation ($\\rho=1$) between all pairs. Thus, the raw ISC is expected to be $1.0$. The OLS regression will perfectly fit the data, resulting in a residual vector of all zeros for every subject. A zero-variance time-series has a defined correlation of $0$ with any other series, so all pairwise correlations of residuals will be $0$. Consequently, the partial ISC is expected to be $0.0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes raw and partial Inter-Subject Correlation (ISC) for three test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"n\": 4, \"T\": 60,\n            \"s_active\": True,\n            \"phi\": [0.0, 0.3, -0.2, 0.5],\n            \"psi\": [0.1, -0.1, 0.2, -0.3],\n            \"alpha\": [1.00, 0.90, 1.10, 1.05],\n            \"gamma\": [0.80, 0.70, 0.75, 0.85],\n            \"delta\": [0.60, 0.65, 0.55, 0.60],\n            \"theta\": [0.0, 0.4, -0.5, 0.2],\n            \"noise_level\": 0.1,\n            \"c1_period\": 10, \"c2_period\": 15, \"noise_period\": 7,\n            \"c1_func\": np.sin, \"c2_func\": np.cos,\n        },\n        {\n            \"name\": \"B\",\n            \"n\": 3, \"T\": 60,\n            \"s_active\": True,\n            \"phi\": [0.2, -0.3, 0.4],\n            \"psi\": [-0.2, 0.1, -0.4],\n            \"alpha\": [1.00, 0.80, 1.20],\n            \"gamma\": [0.00, 0.00, 0.00],\n            \"delta\": [0.00, 0.00, 0.00],\n            \"theta\": [0.1, -0.2, 0.3],\n            \"noise_level\": 0.05,\n            \"c1_period\": 9, \"c2_period\": 11, \"noise_period\": 13,\n            \"c1_func\": np.sin, \"c2_func\": np.cos,\n        },\n        {\n            \"name\": \"C\",\n            \"n\": 3, \"T\": 60,\n            \"s_active\": False,\n            \"gamma\": [0.50, 0.90, 0.70],\n            \"delta\": [1.00, 1.80, 1.40],\n            \"noise_level\": 0.0,\n            \"c1_period\": 8,\n        }\n    ]\n\n    final_results = []\n\n    def pearson_correlation(x, y):\n        \"\"\"\n        Calculates Pearson correlation between two vectors.\n        Returns 0 if either vector has zero variance.\n        \"\"\"\n        x_std = np.std(x)\n        y_std = np.std(y)\n        \n        if x_std == 0 or y_std == 0:\n            return 0.0\n            \n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        \n        x_centered = x - x_mean\n        y_centered = y - y_mean\n        \n        numerator = np.sum(x_centered * y_centered)\n        denominator = np.sqrt(np.sum(x_centered**2) * np.sum(y_centered**2))\n        \n        # This check is theoretically redundant given the std check, but safe.\n        if denominator == 0:\n            return 0.0\n            \n        return numerator / denominator\n\n    for case in test_cases:\n        n = case[\"n\"]\n        T = case[\"T\"]\n        \n        t = np.arange(T)\n        \n        if case[\"s_active\"]:\n            s = np.sin(2 * np.pi * t / 12) + 0.5 * np.sin(2 * np.pi * t / 6)\n        else:\n            s = np.zeros(T)\n\n        y_all_subjects = []\n        r_all_subjects = []\n\n        for i in range(n):\n            if case[\"name\"] == \"C\":\n                c1_i = np.sin(2 * np.pi * t / case[\"c1_period\"])\n                c2_i = 2 * c1_i\n                eps_i = np.zeros(T)\n                alpha_i = 0.0\n            else:\n                c1_i = case[\"c1_func\"](2 * np.pi * t / case[\"c1_period\"] + case[\"phi\"][i])\n                c2_i = case[\"c2_func\"](2 * np.pi * t / case[\"c2_period\"] + case[\"psi\"][i])\n                eps_i = case[\"noise_level\"] * np.sin(2 * np.pi * t / case[\"noise_period\"] + case[\"theta\"][i])\n                alpha_i = case[\"alpha\"][i]\n\n            gamma_i = case[\"gamma\"][i]\n            delta_i = case[\"delta\"][i]\n\n            y_i = alpha_i * s + gamma_i * c1_i + delta_i * c2_i + eps_i\n            y_all_subjects.append(y_i)\n            \n            # Perform OLS regression to find residuals\n            X = np.stack([np.ones(T), c1_i, c2_i], axis=1)\n            \n            # Use lstsq which handles multicollinearity via pseudo-inverse\n            beta_hat, _, _, _ = np.linalg.lstsq(X, y_i, rcond=None)\n            \n            y_i_predicted = X @ beta_hat\n            residuals = y_i - y_i_predicted\n            r_all_subjects.append(residuals)\n\n        # Compute raw and partial ISC\n        raw_correlations = []\n        partial_correlations = []\n        \n        num_pairs = 0\n        for i in range(n):\n            for j in range(i + 1, n):\n                num_pairs += 1\n                \n                # Raw correlation\n                raw_corr = pearson_correlation(y_all_subjects[i], y_all_subjects[j])\n                raw_correlations.append(raw_corr)\n                \n                # Partial correlation\n                partial_corr = pearson_correlation(r_all_subjects[i], r_all_subjects[j])\n                partial_correlations.append(partial_corr)\n        \n        raw_isc = np.mean(raw_correlations) if raw_correlations else 0.0\n        partial_isc = np.mean(partial_correlations) if partial_correlations else 0.0\n        \n        final_results.append(f\"{raw_isc:.6f}\")\n        final_results.append(f\"{partial_isc:.6f}\")\n\n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "An ISC analysis typically produces a statistical map with values for thousands of brain voxels, creating a massive multiple comparisons problem. This final exercise simulates the crucial post-processing step of controlling the False Discovery Rate (FDR) to identify truly significant brain regions. By applying a standard correction procedure to a set of hypothetical $p$-values, you will learn to generate a robust and interpretable brain map of inter-subject synchrony .",
            "id": "4170716",
            "problem": "You are given synthetic but scientifically plausible summary statistics from voxelwise Inter-Subject Correlation (ISC) analyses under naturalistic paradigms. Inter-Subject Correlation (ISC) quantifies the similarity of brain time series responses across subjects at each voxel during a shared stimulus and is typically evaluated against a null model via nonparametric procedures (for example, circular time-shift) to obtain per-voxel $p$-values. The task is to control the False Discovery Rate (FDR) across $V$ voxels, compute corrected $q$-values that are monotone with respect to the rank of the ordered $p$-values, and then report the set of significant anatomical regions using a provided integer-coded atlas. False Discovery Rate (FDR) is the expected proportion of false positives among the discoveries and must be controlled under independence or positive dependence between tests. The corrected $q$-values should represent, for each voxel, the minimal FDR level at which the voxel would be declared significant.\n\nFundamental base to be used:\n- Definition of Pearson correlation for ISC is assumed known; however, you are not required to compute correlations here. You will be provided $p$-values that were obtained by a valid null procedure.\n- False Discovery Rate (FDR) control under independence or positive dependence is to be achieved via a step-up multiple comparison procedure on ordered $p$-values, producing monotone corrected $q$-values across ranks, and significance decisions relative to a chosen discovery level $ \\alpha $.\n- Order statistics on $p$-values: sorting by ascending value, associating ranks, and enforcing monotonicity of corrected $q$-values across ranks.\n\nYour program must:\n1. For each test case, compute corrected $q$-values across all voxels using an FDR-controlling step-up procedure under independence or positive dependence, ensuring $q$-values are non-decreasing with rank of the ordered $p$-values.\n2. Determine which voxels are significant by comparing corrected $q$-values to the specified discovery level $ \\alpha $.\n3. Report the set of significant anatomical regions by aggregating the region codes corresponding to significant voxels and sorting these codes in ascending order.\n4. Output only integer codes of regions, not text labels, to conform to the final output constraints.\n\nAtlas coding:\n- The atlas maps each voxel to an integer code that corresponds to an anatomical label. The integer-to-label mapping is provided for interpretability but must not appear in the final program output. The mapping is:\n  - $1$: Primary Visual Cortex (V1)\n  - $2$: Superior Temporal/Auditory Cortex\n  - $3$: Precuneus/Posterior Medial Cortex\n  - $4$: Angular Gyrus/Temporo-Parietal Junction\n\nTest suite:\nEach test case specifies the number of voxels $V$, a list of $V$ $p$-values in $[0,1]$, a list of $V$ atlas codes (integers), and the discovery level $ \\alpha $.\n\n- Test Case A (happy path, mixed $p$-values, multiple regions, small discoveries):\n  - $V = 10$\n  - $p$-values: [$0.001$, $0.02$, $0.15$, $0.03$, $0.5$, $0.25$, $0.0005$, $0.04$, $0.8$, $0.07$]\n  - Atlas codes: [$1$, $1$, $2$, $2$, $3$, $3$, $1$, $4$, $4$, $2$]\n  - $ \\alpha = 0.05 $\n\n- Test Case B (edge case with ties and boundary small $p$-values including $0$ and $1$):\n  - $V = 8$\n  - $p$-values: [$0.0$, $0.0001$, $0.049$, $0.051$, $0.2$, $0.3$, $1.0$, $0.0001$]\n  - Atlas codes: [$2$, $2$, $3$, $3$, $4$, $4$, $1$, $2$]\n  - $ \\alpha = 0.05 $\n\n- Test Case C (no discoveries, all $p$-values large):\n  - $V = 7$\n  - $p$-values: [$0.3$, $0.4$, $0.6$, $0.9$, $0.2$, $0.7$, $0.55$]\n  - Atlas codes: [$1$, $2$, $3$, $4$, $1$, $2$, $3$]\n  - $ \\alpha = 0.05 $\n\n- Test Case D (borderline ties exactly at step-up thresholds, multiple discoveries across regions):\n  - $V = 12$\n  - $p$-values: [$0.0166667$, $0.0208333$, $0.001$, $0.002$, $0.7$, $0.3$, $0.05$, $0.04$, $0.004$, $0.02$, $0.8$, $0.0005$]\n  - Atlas codes: [$1$, $1$, $2$, $2$, $3$, $3$, $4$, $4$, $2$, $3$, $4$, $1$]\n  - $ \\alpha = 0.05 $\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- Each test case’s result must be a list of integers where the first integer is the number of significant voxels and the subsequent integers are the sorted unique region codes among the significant voxels.\n- For example, the output should look like: \"[[n_A,regionA1,regionA2,...],[n_B,regionB1,...],[n_C,...],[n_D,...]]\" where $n_X$ denotes the count of significant voxels in test case $X$.\n\nYour program must be entirely self-contained, must not read external files, and must not require user input. No physical units or angles are involved. All numeric answers must be integers, floats, booleans, or lists of these types as specified above. The program must compute corrected $q$-values and derive significance based on these $q$-values; it must not shortcut by directly applying decisions on unadjusted $p$-values.",
            "solution": "The problem requires controlling the False Discovery Rate (FDR) across a set of voxel-wise statistical tests from an Inter-Subject Correlation (ISC) analysis. We are given per-voxel $p$-values, an anatomical atlas, and a desired FDR level $\\alpha$. The task is to compute corrected $q$-values, identify significant voxels, and report the count of significant voxels along with the unique, sorted-integer codes of the anatomical regions in which they reside.\n\nFirst, we must rigorously validate the problem statement.\n\n### Step 1: Extract Givens\n- **$V$**: The number of voxels, equivalent to the number of simultaneous statistical tests.\n- **$p$-values**: A list of $V$ floating-point numbers in the range $[0, 1]$, representing the statistical significance of the ISC at each voxel.\n- **Atlas codes**: A list of $V$ integers, where each integer maps a voxel to a specific anatomical region.\n- **$\\alpha$**: The desired False Discovery Rate level, used as the significance threshold for the corrected $q$-values.\n- **Atlas Mapping**:\n  - $1$: Primary Visual Cortex (V1)\n  - $2$: Superior Temporal/Auditory Cortex\n  - $3$: Precuneus/Posterior Medial Cortex\n  - $4$: Angular Gyrus/Temporo-Parietal Junction\n- **Procedure Requirement**: Use a step-up multiple comparison procedure to control FDR under independence or positive dependence, producing monotone corrected $q$-values.\n- **Test Cases**: Four specific test cases (A, B, C, D) are provided with all necessary inputs.\n- **Output Format**: A single-line string representing a list of lists, e.g., `[[n_A,regionA1,...],[n_B,regionB1,...],...]`, where $n_X$ is the count of significant voxels for test case $X$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem is well-grounded in the standard practices of neuroimaging data analysis. ISC is a widely used method, and controlling the FDR for voxel-wise multiple comparisons is a critical and canonical statistical step.\n- **Well-Posed**: The problem is well-posed. It provides all necessary inputs ($V$, $p$-values, atlas codes, $\\alpha$), specifies a clear objective, and describes a standard, implementable procedure (FDR control via a step-up method). This structure ensures a unique and meaningful solution exists for each test case.\n- **Objective**: The problem is stated using precise, objective, and formal scientific language. It is free from ambiguity and subjectivity.\n\nThe problem does not exhibit any of the invalidity flaws:\n1.  **Scientific or Factual Unsoundness**: The procedure described aligns with the seminal Benjamini-Hochberg (BH) method, which is the standard for controlling FDR under independence or positive dependence, as specified.\n2.  **Non-Formalizable or Irrelevant**: The task is a direct, formalizable computational and statistical problem relevant to the specified topic.\n3.  **Incomplete or Contradictory Setup**: All data required for computation are provided. There are no contradictions.\n4.  **Unrealistic or Infeasible**: The data are synthetic but well within plausible ranges for such an analysis, serving as valid inputs for the algorithm.\n5.  **Ill-Posed or Poorly Structured**: The procedure is well-defined. The \"step-up procedure\" and \"monotone corrected q-values\" refer to a specific, standard implementation of FDR control.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires a multi-step algorithm involving sorting, computation, monotonicity enforcement, and re-mapping, which is a non-trivial data processing task.\n7.  **Outside Scientific Verifiability**: The computational procedure is deterministic and its results are fully verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe core of the problem is the computation of FDR-corrected $q$-values using the Benjamini-Hochberg (BH) procedure, ensuring the resulting $q$-values are monotonic.\n\nLet there be $V$ voxels, corresponding to $V$ null hypotheses, with associated $p$-values $\\{p_1, p_2, \\dots, p_V\\}$.\n\n1.  **Order the $p$-values**: The first step is to sort the $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(V)}$. We must keep track of the original index of each $p$-value to map the final results back to the anatomical atlas. Let $i$ be the rank of a $p$-value in the sorted list, from $i=1$ to $V$.\n\n2.  **Compute BH-Adjusted Values**: The BH procedure compares each sorted $p$-value, $p_{(i)}$, to a threshold that depends on its rank, $i$. A test is declared significant if $p_{(i)} \\le \\frac{i}{V}\\alpha$. The $q$-value represents the smallest $\\alpha$ at which a test would be called significant. An initial, uncorrected estimate of the $q$-value for the test with rank $i$ is given by:\n    $$q'_{(i)} = \\frac{p_{(i)} \\cdot V}{i}$$\n\n3.  **Enforce Monotonicity**: The problem explicitly requires that the corrected $q$-values be non-decreasing with the rank of the ordered $p$-values. The raw values $q'_{(i)}$ may not satisfy this property (i.e., it's possible that $q'_{(i)} > q'_{(j)}$ for $i < j$). To enforce monotonicity, we adjust the $q$-values using a cumulative minimum, starting from the largest rank. The final, corrected $q$-value for the test with rank $i$, denoted $q_{(i)}$, is defined as:\n    $$q_{(V)} = q'_{(V)} = p_{(V)}$$\n    $$q_{(i)} = \\min(q_{(i+1)}, q'_{(i)}) \\quad \\text{for } i = V-1, V-2, \\dots, 1$$\n    This is equivalent to setting $q_{(i)} = \\min_{j=i}^{V} \\left( \\frac{p_{(j)} \\cdot V}{j} \\right)$. This \"step-up\" adjustment ensures that $q_{(1)} \\le q_{(2)} \\le \\dots \\le q_{(V)}$.\n\n4.  **Identify Significant Voxels**: After computing the monotonic $q$-values for the sorted list of tests, we must map them back to their original voxel locations. A voxel $k$ is declared significant if its corrected $q$-value, $q_k$, satisfies:\n    $$q_k \\le \\alpha$$\n\n5.  **Aggregate Results**: Once the set of significant voxels is identified, we count them to get $n_{sig}$. We then look up the atlas codes for these specific voxels. The final step is to collect the unique atlas codes, sort them in ascending order, and prepend the count $n_{sig}$ to this list. This process is repeated for each test case.\n\nThe overall algorithm for each test case is as follows:\n- Given $V$, a list of $p$-values, a list of atlas codes, and $\\alpha$.\n- Create a record of the original indices, from $0$ to $V-1$.\n- Sort the $p$-values in ascending order, keeping track of the permutation of original indices.\n- Let the sorted $p$-values be $p_{(i)}$ for ranks $i=1, \\dots, V$.\n- Compute the raw adjusted values $q'_{(i)} = (p_{(i)} \\cdot V) / i$.\n- Compute the final monotonic $q$-values, $q_{(i)}$, by taking the cumulative minimum of $q'$ from the end of the list (from rank $V$ down to $1$).\n- Create a new list for the final $q$-values in their original order. Populate this list by placing each $q_{(i)}$ at the original index corresponding to $p_{(i)}$.\n- Identify all indices $k$ where the final $q_k \\le \\alpha$.\n- The number of significant voxels, $n_{sig}$, is the count of such indices.\n- Collect the atlas codes corresponding to these significant indices.\n- Find the unique set of these atlas codes and sort them.\n- Construct the final result list: $[n_{sig}, \\text{sorted\\_unique\\_region\\_1}, \\text{sorted\\_unique\\_region\\_2}, \\dots]$. If $n_{sig}=0$, the list is just $[0]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Note: scipy is permitted but not necessary for this implementation.\n\ndef solve():\n    \"\"\"\n    Solves the FDR correction problem for the given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case A\n        {\n            \"V\": 10,\n            \"p_values\": [0.001, 0.02, 0.15, 0.03, 0.5, 0.25, 0.0005, 0.04, 0.8, 0.07],\n            \"atlas_codes\": [1, 1, 2, 2, 3, 3, 1, 4, 4, 2],\n            \"alpha\": 0.05\n        },\n        # Test Case B\n        {\n            \"V\": 8,\n            \"p_values\": [0.0, 0.0001, 0.049, 0.051, 0.2, 0.3, 1.0, 0.0001],\n            \"atlas_codes\": [2, 2, 3, 3, 4, 4, 1, 2],\n            \"alpha\": 0.05\n        },\n        # Test Case C\n        {\n            \"V\": 7,\n            \"p_values\": [0.3, 0.4, 0.6, 0.9, 0.2, 0.7, 0.55],\n            \"atlas_codes\": [1, 2, 3, 4, 1, 2, 3],\n            \"alpha\": 0.05\n        },\n        # Test Case D\n        {\n            \"V\": 12,\n            \"p_values\": [0.0166667, 0.0208333, 0.001, 0.002, 0.7, 0.3, 0.05, 0.04, 0.004, 0.02, 0.8, 0.0005],\n            \"atlas_codes\": [1, 1, 2, 2, 3, 3, 4, 4, 2, 3, 4, 1],\n            \"alpha\": 0.05\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        V = case[\"V\"]\n        p_values = np.array(case[\"p_values\"])\n        atlas_codes = np.array(case[\"atlas_codes\"])\n        alpha = case[\"alpha\"]\n\n        # Step 1: Get the sorting order of p-values\n        # np.argsort provides the indices that would sort the array\n        sort_indices = np.argsort(p_values)\n        \n        # Step 2: Sort the p-values\n        p_values_sorted = p_values[sort_indices]\n        \n        # Step 3: Compute BH-adjusted q-values\n        # Create ranks from 1 to V\n        ranks = np.arange(1, V + 1)\n        # Calculate raw q-values: (p_sorted * V) / rank\n        q_values_raw_sorted = p_values_sorted * V / ranks\n        \n        # Step 4: Enforce monotonicity\n        # This is achieved by taking the cumulative minimum from the end of the list.\n        # A numpy-idiomatic way is to reverse the array, compute the cumulative min,\n        # and then reverse it back.\n        q_values_monotone_sorted = np.minimum.accumulate(q_values_raw_sorted[::-1])[::-1]\n        \n        # Step 5: Un-sort the q-values to match the original voxel order\n        # We need an array to store the q-values in their original order.\n        q_values_original_order = np.empty_like(p_values)\n        # The 'sort_indices' tells us where each sorted value came from.\n        # The inverse operation assigns the computed q-values back to their original positions.\n        q_values_original_order[sort_indices] = q_values_monotone_sorted\n        \n        # Step 6: Determine significance\n        significant_mask = q_values_original_order <= alpha\n        \n        # Step 7: Count significant voxels\n        num_significant_voxels = np.sum(significant_mask)\n        \n        # Step 8: Report significant anatomical regions\n        if num_significant_voxels > 0:\n            significant_regions = atlas_codes[significant_mask]\n            # Get unique regions and sort them\n            unique_sorted_regions = sorted(list(np.unique(significant_regions)))\n            case_result = [num_significant_voxels] + unique_sorted_regions\n        else:\n            case_result = [0]\n            \n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The format requires stringifying each inner list and joining them with commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}