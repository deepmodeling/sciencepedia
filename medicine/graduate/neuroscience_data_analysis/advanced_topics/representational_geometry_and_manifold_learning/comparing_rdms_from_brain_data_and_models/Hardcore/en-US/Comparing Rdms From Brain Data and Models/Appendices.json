{
    "hands_on_practices": [
        {
            "introduction": "The first, critical decision in any Representational Similarity Analysis (RSA) is how to define 'dissimilarity.' The choice of metric, such as Euclidean, cosine, or correlation distance, is not arbitrary; it embeds a fundamental assumption about what aspects of neural activity patterns carry meaningful information. This exercise  challenges you to think critically about these choices and the common preprocessing step of unit-norm scaling, forcing you to consider the crucial concept of invariance and how it shapes the comparison between brain and model.",
            "id": "4147084",
            "problem": "Consider Representational Similarity Analysis (RSA), in which one computes a Representational Dissimilarity Matrix (RDM) whose entry for conditions indexed by $i$ and $j$ is a dissimilarity $d(x_i,x_j)$ between multivariate brain activity patterns $x_i \\in \\mathbb{R}^p$ and $x_j \\in \\mathbb{R}^p$ measured across $p$ features (for example, voxels). Let $n$ denote the number of conditions. You are comparing brain RDMs to model RDMs and are considering the preprocessing step of scaling each observed pattern to unit Euclidean norm, $x_i' = x_i / \\|x_i\\|_2$, prior to computing dissimilarities. Assume multiplicative gains induced by measurement are non-negative.\n\nThree commonly used dissimilarities in RSA are:\n- Euclidean distance, $d_E(x,y) = \\|x - y\\|_2$,\n- cosine distance, $d_{\\cos}(x,y) = 1 - \\dfrac{x^\\top y}{\\|x\\|_2 \\|y\\|_2}$,\n- correlation distance, $d_{\\text{corr}}(x,y) = 1 - r(x,y)$, where $r(x,y)$ is the Pearson correlation coefficient across features.\n\nEvaluate the following statements about the consequences of scaling each pattern to unit norm and justify or refute the practice based on invariance considerations. Select all statements that are correct.\n\nA. Scaling each pattern to unit norm leaves RDMs computed with cosine distance and with correlation distance unchanged, but alters RDMs computed with Euclidean distance; moreover, Euclidean distances on unit-normalized patterns are a strictly increasing function of the corresponding cosine distances, so the rank ordering of pairwise dissimilarities is identical between them.\n\nB. Euclidean distance is invariant to independent multiplicative rescaling of each pattern, so unit-norm scaling cannot change an RDM computed with Euclidean distance.\n\nC. When either the computational model or the measurement process is invariant to per-condition gains, applying unit-norm scaling is defensible to align invariances; however, if population amplitude carries representational content, unit-norm scaling is inappropriate, and for cosine or correlation distances the normalization is redundant and should not affect RSA outcomes.\n\nD. Unit-norm scaling removes additive offsets (constant baselines) from patterns, making cosine and correlation distances equivalent, so it is advisable whenever baseline shifts are suspected.\n\nE. If RDMs are compared via Spearman correlation on their dissimilarity entries, Euclidean distances computed after unit-norm scaling and cosine distances will yield exactly the same Spearman correlation with any third RDM, because their dissimilarity vectors have identical ranks.\n\nF. For any set of conditions, the Pearson correlation between the vectorized RDM of Euclidean distances computed after unit-norm scaling and the vectorized RDM of cosine distances is exactly $1$.",
            "solution": "The problem statement will first be validated for scientific soundness, clarity, and completeness.\n\n### Step 1: Extract Givens\n- **Context**: Representational Similarity Analysis (RSA).\n- **Data**: A set of $n$ multivariate brain activity patterns, $x_i \\in \\mathbb{R}^p$ for $i=1, \\dots, n$, where $p$ is the number of features (e.g., voxels).\n- **Object of Analysis**: A Representational Dissimilarity Matrix (RDM) with entries $d(x_i, x_j)$.\n- **Preprocessing Step**: Scaling each pattern to unit Euclidean norm: $x_i' = x_i / \\|x_i\\|_2$.\n- **Assumption**: Multiplicative gains are non-negative.\n- **Dissimilarity Metrics**:\n    1.  Euclidean distance: $d_E(x,y) = \\|x - y\\|_2$.\n    2.  Cosine distance: $d_{\\cos}(x,y) = 1 - \\dfrac{x^\\top y}{\\|x\\|_2 \\|y\\|_2}$.\n    3.  Correlation distance: $d_{\\text{corr}}(x,y) = 1 - r(x,y)$, where $r(x,y)$ is the Pearson correlation coefficient.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in the field of computational neuroscience. RSA is a widely used and validated method. The definitions of Euclidean, cosine, and correlation distances are standard mathematical definitions. The concepts of activity patterns, features, conditions, and multiplicative gains are standard in the analysis of neuroimaging data.\n2.  **Well-Posed**: The problem is well-posed. It asks for an evaluation of several precise mathematical and conceptual statements based on a clear set of definitions and a specified data transformation. A definite answer can be derived for each statement.\n3.  **Objective**: The language is technical, precise, and free of subjective or ambiguous terminology. The statements to be evaluated are objective claims that can be proven or disproven.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. The analysis will proceed by deriving the consequences of the unit-norm scaling on each dissimilarity metric and then evaluating each option.\n\n### Derivations\n\nLet $x_i, x_j \\in \\mathbb{R}^p$ be two activity patterns. The unit-norm scaled patterns are $x_i' = x_i / \\|x_i\\|_2$ and $x_j' = x_j / \\|x_j\\|_2$. By construction, $\\|x_i'\\|_2 = 1$ and $\\|x_j'\\|_2 = 1$. We analyze the effect of this transformation on the three given dissimilarities.\n\n1.  **Cosine Distance**:\n    The cosine distance between the scaled patterns is:\n    $$d_{\\cos}(x_i', x_j') = 1 - \\frac{(x_i')^\\top (x_j')}{\\|x_i'\\|_2 \\|x_j'\\|_2}$$\n    Substituting the definitions of $x_i'$ and $x_j'$, and noting their norms are $1$:\n    $$d_{\\cos}(x_i', x_j') = 1 - \\frac{\\left(\\frac{x_i}{\\|x_i\\|_2}\\right)^\\top \\left(\\frac{x_j}{\\|x_j\\|_2}\\right)}{1 \\cdot 1} = 1 - \\frac{x_i^\\top x_j}{\\|x_i\\|_2 \\|x_j\\|_2} = d_{\\cos}(x_i, x_j)$$\n    Thus, cosine distance is inherently invariant to the scaling of individual patterns. Unit-norm scaling has no effect on an RDM computed with cosine distance.\n\n2.  **Correlation Distance**:\n    The Pearson correlation coefficient $r(x,y)$ is the cosine similarity between mean-centered vectors. Let $\\tilde{x} = x - \\bar{x}\\mathbf{1}$ and $\\tilde{y} = y - \\bar{y}\\mathbf{1}$ be the mean-centered versions of $x$ and $y$, where $\\bar{x}$ is the mean of the elements of $x$ and $\\mathbf{1}$ is a vector of ones. Then $r(x,y) = \\frac{\\tilde{x}^\\top \\tilde{y}}{\\|\\tilde{x}\\|_2 \\|\\tilde{y}\\|_2}$.\n    Now consider the scaled patterns $x_i'$ and $x_j'$. Their means are $\\bar{x_i'} = \\bar{x_i}/\\|x_i\\|_2$ and $\\bar{x_j'} = \\bar{x_j}/\\|x_j\\|_2$.\n    The mean-centered scaled patterns are:\n    $$\\tilde{x_i'} = x_i' - \\bar{x_i'}\\mathbf{1} = \\frac{x_i}{\\|x_i\\|_2} - \\frac{\\bar{x_i}}{\\|x_i\\|_2}\\mathbf{1} = \\frac{x_i - \\bar{x_i}\\mathbf{1}}{\\|x_i\\|_2} = \\frac{\\tilde{x_i}}{\\|x_i\\|_2}$$\n    The correlation coefficient for the scaled patterns is:\n    $$r(x_i', x_j') = \\frac{(\\tilde{x_i'})^\\top (\\tilde{x_j'})}{\\|\\tilde{x_i'}\\|_2 \\|\\tilde{x_j'}\\|_2} = \\frac{\\left(\\frac{\\tilde{x_i}}{\\|x_i\\|_2}\\right)^\\top \\left(\\frac{\\tilde{x_j}}{\\|x_j\\|_2}\\right)}{\\left\\|\\frac{\\tilde{x_i}}{\\|x_i\\|_2}\\right\\|_2 \\left\\|\\frac{\\tilde{x_j}}{\\|x_j\\|_2}\\right\\|_2} = \\frac{\\frac{\\tilde{x_i}^\\top \\tilde{x_j}}{\\|x_i\\|_2 \\|x_j\\|_2}}{\\frac{\\|\\tilde{x_i}\\|_2}{\\|x_i\\|_2} \\frac{\\|\\tilde{x_j}\\|_2}{\\|x_j\\|_2}} = \\frac{\\tilde{x_i}^\\top \\tilde{x_j}}{\\|\\tilde{x_i}\\|_2 \\|\\tilde{x_j}\\|_2} = r(x_i, x_j)$$\n    Therefore, $d_{\\text{corr}}(x_i', x_j') = 1 - r(x_i', x_j') = 1 - r(x_i, x_j) = d_{\\text{corr}}(x_i, x_j)$.\n    Correlation distance is also invariant to the scaling of individual patterns. Unit-norm scaling has no effect on an RDM computed with correlation distance.\n\n3.  **Euclidean Distance**:\n    The Euclidean distance between the scaled patterns is:\n    $$d_E(x_i', x_j') = \\|x_i' - x_j'\\|_2 = \\left\\|\\frac{x_i}{\\|x_i\\|_2} - \\frac{x_j}{\\|x_j\\|_2}\\right\\|_2$$\n    This is generally not equal to $d_E(x_i, x_j) = \\|x_i - x_j\\|_2$. For instance, if $x_j = 2x_i$ for some non-zero $x_i$, then $d_E(x_i, x_j) = \\|x_i - 2x_i\\|_2 = \\|-x_i\\|_2 = \\|x_i\\|_2 > 0$. However, $x_i' = x_i/\\|x_i\\|_2$ and $x_j' = 2x_i/\\|2x_i\\|_2 = 2x_i/(2\\|x_i\\|_2) = x_i/\\|x_i\\|_2 = x_i'$. Thus, $d_E(x_i', x_j') = \\|x_i' - x_i'\\|_2 = 0$.\n    Unit-norm scaling alters RDMs computed with Euclidean distance.\n\n4.  **Relationship between $d_E$ on unit-norm vectors and $d_{\\cos}$**:\n    Let $u=x_i'$ and $v=x_j'$ be two unit-norm vectors.\n    $$d_E(u, v)^2 = \\|u-v\\|_2^2 = (u-v)^\\top(u-v) = u^\\top u - 2u^\\top v + v^\\top v$$\n    Since $\\|u\\|_2^2 = u^\\top u = 1$ and $\\|v\\|_2^2 = v^\\top v = 1$:\n    $$d_E(u, v)^2 = 1 - 2u^\\top v + 1 = 2 - 2u^\\top v$$\n    The cosine distance between these unit vectors is $d_{\\cos}(u, v) = 1 - \\frac{u^\\top v}{\\|u\\|_2 \\|v\\|_2} = 1 - u^\\top v$.\n    From this, $u^\\top v = 1 - d_{\\cos}(u, v)$. Substituting this into the equation for $d_E(u, v)^2$:\n    $$d_E(u, v)^2 = 2 - 2(1 - d_{\\cos}(u, v)) = 2 - 2 + 2d_{\\cos}(u, v) = 2d_{\\cos}(u, v)$$\n    Taking the square root (distances are non-negative):\n    $$d_E(u, v) = \\sqrt{2 d_{\\cos}(u, v)}$$\n    This shows that for unit-norm vectors, the Euclidean distance is a simple monotonic function of the cosine distance. Since $f(z) = \\sqrt{2z}$ is a strictly increasing function for $z \\ge 0$, a larger cosine distance implies a larger Euclidean distance.\n\n### Evaluation of Options\n\n**A. Scaling each pattern to unit norm leaves RDMs computed with cosine distance and with correlation distance unchanged, but alters RDMs computed with Euclidean distance; moreover, Euclidean distances on unit-normalized patterns are a strictly increasing function of the corresponding cosine distances, so the rank ordering of pairwise dissimilarities is identical between them.**\n- Our derivations showed that unit-norm scaling leaves $d_{\\cos}$ and $d_{\\text{corr}}$ RDMs unchanged.\n- Our derivations showed that unit-norm scaling generally alters $d_E$ RDMs.\n- We derived that $d_E(x', y') = \\sqrt{2 d_{\\cos}(x', y')}$. Since $d_{\\cos}(x', y') = d_{\\cos}(x, y)$, this means $d_E(x', y') = \\sqrt{2 d_{\\cos}(x, y)}$. The function $f(z) = \\sqrt{2z}$ is strictly increasing.\n- Because a strictly increasing function preserves order, the rank ordering of dissimilarities in the RDM from $d_E$ on scaled patterns will be identical to the rank ordering of dissimilarities in the RDM from $d_{\\cos}$.\n- All parts of this statement are correct.\nVerdict: **Correct**.\n\n**B. Euclidean distance is invariant to independent multiplicative rescaling of each pattern, so unit-norm scaling cannot change an RDM computed with Euclidean distance.**\n- The premise is false. Independent multiplicative rescaling means transforming $(x_i, x_j)$ to $(c_i x_i, c_j x_j)$ for scalars $c_i, c_j$. The new distance is $\\|c_i x_i - c_j x_j\\|_2$, which is not, in general, equal to $\\|x_i - x_j\\|_2$. The counterexample where $x_j=2x_i$ and the scaling factors are $c_i = 1/\\|x_i\\|_2$ and $c_j = 1/\\|x_j\\|_2$ shows that the distance changes from $\\|x_i\\|_2$ to $0$.\n- Since the premise is false, the conclusion is invalid.\nVerdict: **Incorrect**.\n\n**C. When either the computational model or the measurement process is invariant to per-condition gains, applying unit-norm scaling is defensible to align invariances; however, if population amplitude carries representational content, unit-norm scaling is inappropriate, and for cosine or correlation distances the normalization is redundant and should not affect RSA outcomes.**\n- The first clause correctly states a primary motivation for normalization in RSA. If the theory or model is insensitive to the overall magnitude (gain) of a pattern, the dissimilarity measure should also be. Unit-norm scaling imposes this invariance.\n- The second clause correctly identifies the main danger of this normalization: if the magnitude of the response pattern $\\|x_i\\|_2$ is itself part of the neural code, scaling destroys this information.\n- The final clause correctly states that for distance metrics that are already gain-invariant (cosine and correlation distance), this explicit scaling step is mathematically redundant and does not change the resulting RDM.\n- The entire statement is a sound and accurate summary of the considerations surrounding this preprocessing step in RSA.\nVerdict: **Correct**.\n\n**D. Unit-norm scaling removes additive offsets (constant baselines) from patterns, making cosine and correlation distances equivalent, so it is advisable whenever baseline shifts are suspected.**\n- Unit-norm scaling, $x \\to x/\\|x\\|_2$, normalizes the $L_2$ norm. Removing an additive offset (de-meaning) is achieved by $x \\to x - \\bar{x}\\mathbf{1}$. These are different operations. Scaling a vector does not, in general, change its mean to zero.\n- Cosine and correlation distances become equivalent only when the vectors are mean-centered ($\\bar{x}=0, \\bar{y}=0$). Since unit-norm scaling does not ensure vectors are mean-centered, it does not make these two distances equivalent.\n- The correct procedure to handle suspected baseline shifts is to use a metric that is invariant to them, such as correlation distance, or to explicitly mean-center the patterns.\n- The entire statement is based on a false premise.\nVerdict: **Incorrect**.\n\n**E. If RDMs are compared via Spearman correlation on their dissimilarity entries, Euclidean distances computed after unit-norm scaling and cosine distances will yield exactly the same Spearman correlation with any third RDM, because their dissimilarity vectors have identical ranks.**\n- Let $v_E'$ be the vector of dissimilarities from Euclidean distance on scaled patterns, and $v_C$ be the vector of dissimilarities from cosine distance.\n- As derived, the elements are related by $(v_E')_k = \\sqrt{2 (v_C)_k}$. This is a strictly monotonic increasing relationship.\n- A strictly monotonic transformation preserves the rank order of all elements. Thus, the vector of ranks of $v_E'$ is identical to the vector of ranks of $v_C$. Let this be $R = \\text{rank}(v_E') = \\text{rank}(v_C)$.\n- Spearman's rank correlation $\\rho_S$ between two variables is the Pearson correlation $\\rho_P$ of their rank variables.\n- Let $v_3$ be the vectorized dissimilarities of a third RDM, with rank vector $R_3 = \\text{rank}(v_3)$.\n- The Spearman correlation of $v_E'$ with $v_3$ is $\\rho_S(v_E', v_3) = \\rho_P(R, R_3)$.\n- The Spearman correlation of $v_C$ with $v_3$ is $\\rho_S(v_C, v_3) = \\rho_P(R, R_3)$.\n- These are identical. The reasoning provided in the statement is also correct.\nVerdict: **Correct**.\n\n**F. For any set of conditions, the Pearson correlation between the vectorized RDM of Euclidean distances computed after unit-norm scaling and the vectorized RDM of cosine distances is exactly $1$.**\n- Pearson correlation is a measure of linear association. A correlation of $1$ requires that one vector is a perfect positive linear function of the other: $v_E' = a \\cdot v_C + b$ for some constants $a > 0$ and $b$.\n- The relationship we derived is $(v_E')_k = \\sqrt{2(v_C)_k}$. This is a non-linear, square-root relationship.\n- This relationship is only linear if all the dissimilarity values are identical, which is a trivial and degenerate case. For any non-degenerate RDM, the plot of $v_E'$ against $v_C$ will be a curve, not a line.\n- Therefore, the Pearson correlation between them will be less than $1$ (though typically very high).\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "Moving from theory to practice, this exercise  provides a complete, hands-on walkthrough of a miniature RSA study. You will compute a brain RDM from simulated multi-voxel patterns using the crossnobis distance, a powerful technique for obtaining unbiased dissimilarity estimates from noisy data. By incorporating a shrinkage estimator for the noise covariance, you will also engage with the practical challenge of the bias-variance trade-off, a central theme in high-dimensional data analysis.",
            "id": "4147090",
            "problem": "A laboratory seeks to compare the representational geometry of brain responses with a candidate computational model. The brain responses were measured across two ($2$) independent data partitions (e.g., split-half runs), each providing mean response patterns across $M=2$ voxels for $C=3$ stimulus conditions. Let the mean pattern vectors in partition $\\mathsf{A}$ be\n$\\mathbf{x}_{1}^{\\mathsf{A}} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $\\mathbf{x}_{2}^{\\mathsf{A}} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, and $\\mathbf{x}_{3}^{\\mathsf{A}} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$,\nand in partition $\\mathsf{B}$ be\n$\\mathbf{x}_{1}^{\\mathsf{B}} = \\begin{pmatrix}0.9 \\\\ 0.1\\end{pmatrix}$, $\\mathbf{x}_{2}^{\\mathsf{B}} = \\begin{pmatrix}0.1 \\\\ 0.9\\end{pmatrix}$, and $\\mathbf{x}_{3}^{\\mathsf{B}} = \\begin{pmatrix}1.05 \\\\ 0.95\\end{pmatrix}$.\nFrom trial-level residuals pooled across the two partitions, the sample noise covariance is estimated as\n$\\hat{\\Sigma} = \\begin{pmatrix}2 & 1 \\\\ 1 & 2\\end{pmatrix}$.\nTo improve estimation stability, the laboratory applies a shrinkage estimator of the covariance,\n$\\hat{\\Sigma}_{\\lambda} = (1-\\lambda)\\,\\hat{\\Sigma} + \\lambda\\,I$,\nwhere $I$ is the identity matrix and $\\lambda \\in [0,1]$ is the shrinkage parameter.\n\nStarting from the standard definition of the Representational Dissimilarity Matrix (RDM), where each off-diagonal entry quantifies the dissimilarity between a pair of conditions, and from the principle that Gaussian noise with covariance $\\Sigma$ motivates noise-whitened distances, derive the cross-validated noise-adjusted Mahalanobis dissimilarities (the crossnobis distances) for the three condition pairs $(1,2)$, $(1,3)$, and $(2,3)$ using $\\hat{\\Sigma}_{\\lambda}$ as the noise covariance surrogate. Use $\\lambda = 0.3$. Then, construct the brain RDM entries as these three distances.\n\nThe candidate model specifies representational vectors $\\mathbf{m}_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $\\mathbf{m}_{2} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, and $\\mathbf{m}_{3} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$, and defines its RDM entries as the squared Euclidean distances between model vectors. Compute the Pearson correlation between the brain RDM (crossnobis distances) and the model RDM (squared Euclidean distances) for the three pairs. Round your final correlation to four significant figures and express it as a decimal number.\n\nFinally, discuss the bias-variance trade-off introduced by the shrinkage parameter $\\lambda$ in the resulting RDM, explaining under what circumstances shrinkage reduces variance at the cost of bias and how this manifests in the computed dissimilarities.",
            "solution": "The objective is to compute the Pearson correlation between a brain Representational Dissimilarity Matrix (RDM) and a model RDM, and to discuss the bias-variance trade-off of the shrinkage parameter used in the brain RDM calculation.\n\nFirst, we construct the brain RDM. This requires calculating the cross-validated noise-adjusted Mahalanobis dissimilarities (crossnobis distances) for the $3$ condition pairs. The calculation is based on a shrinkage estimate of the noise covariance matrix, $\\hat{\\Sigma}_{\\lambda}$.\n\nThe given sample covariance matrix is $\\hat{\\Sigma} = \\begin{pmatrix}2 & 1 \\\\ 1 & 2\\end{pmatrix}$, and the shrinkage parameter is $\\lambda = 0.3$. The identity matrix is $I = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}$. The shrinkage-regularized covariance matrix, $\\hat{\\Sigma}_{\\lambda}$, is given by:\n$$ \\hat{\\Sigma}_{\\lambda} = (1-\\lambda)\\,\\hat{\\Sigma} + \\lambda\\,I $$\nSubstituting the given values:\n$$ \\hat{\\Sigma}_{0.3} = (1-0.3) \\begin{pmatrix}2 & 1 \\\\ 1 & 2\\end{pmatrix} + 0.3 \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} $$\n$$ \\hat{\\Sigma}_{0.3} = 0.7 \\begin{pmatrix}2 & 1 \\\\ 1 & 2\\end{pmatrix} + 0.3 \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} = \\begin{pmatrix}1.4 & 0.7 \\\\ 0.7 & 1.4\\end{pmatrix} + \\begin{pmatrix}0.3 & 0 \\\\ 0 & 0.3\\end{pmatrix} = \\begin{pmatrix}1.7 & 0.7 \\\\ 0.7 & 1.7\\end{pmatrix} $$\nTo compute the Mahalanobis distance, we need the inverse of this matrix, $\\hat{\\Sigma}_{0.3}^{-1}$.\nThe determinant is $\\det(\\hat{\\Sigma}_{0.3}) = (1.7)(1.7) - (0.7)(0.7) = 2.89 - 0.49 = 2.4$.\nThe inverse is:\n$$ \\hat{\\Sigma}_{0.3}^{-1} = \\frac{1}{2.4} \\begin{pmatrix}1.7 & -0.7 \\\\ -0.7 & 1.7\\end{pmatrix} $$\nThe crossnobis dissimilarity, $d^2(i,j)$, between conditions $i$ and $j$ is defined as:\n$$ d^2(i,j) = (\\mathbf{x}_i^{\\mathsf{A}} - \\mathbf{x}_j^{\\mathsf{A}})^T \\hat{\\Sigma}_{0.3}^{-1} (\\mathbf{x}_i^{\\mathsf{B}} - \\mathbf{x}_j^{\\mathsf{B}}) $$\nLet's compute the vector differences for each partition:\nFor partition $\\mathsf{A}$:\n$\\Delta \\mathbf{x}_{12}^{\\mathsf{A}} = \\mathbf{x}_1^{\\mathsf{A}} - \\mathbf{x}_2^{\\mathsf{A}} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$\n$\\Delta \\mathbf{x}_{13}^{\\mathsf{A}} = \\mathbf{x}_1^{\\mathsf{A}} - \\mathbf{x}_3^{\\mathsf{A}} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1\\end{pmatrix}$\n$\\Delta \\mathbf{x}_{23}^{\\mathsf{A}} = \\mathbf{x}_2^{\\mathsf{A}} - \\mathbf{x}_3^{\\mathsf{A}} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}-1 \\\\ 0\\end{pmatrix}$\n\nFor partition $\\mathsf{B}$:\n$\\Delta \\mathbf{x}_{12}^{\\mathsf{B}} = \\mathbf{x}_1^{\\mathsf{B}} - \\mathbf{x}_2^{\\mathsf{B}} = \\begin{pmatrix}0.9 \\\\ 0.1\\end{pmatrix} - \\begin{pmatrix}0.1 \\\\ 0.9\\end{pmatrix} = \\begin{pmatrix}0.8 \\\\ -0.8\\end{pmatrix}$\n$\\Delta \\mathbf{x}_{13}^{\\mathsf{B}} = \\mathbf{x}_1^{\\mathsf{B}} - \\mathbf{x}_3^{\\mathsf{B}} = \\begin{pmatrix}0.9 \\\\ 0.1\\end{pmatrix} - \\begin{pmatrix}1.05 \\\\ 0.95\\end{pmatrix} = \\begin{pmatrix}-0.15 \\\\ -0.85\\end{pmatrix}$\n$\\Delta \\mathbf{x}_{23}^{\\mathsf{B}} = \\mathbf{x}_2^{\\mathsf{B}} - \\mathbf{x}_3^{\\mathsf{B}} = \\begin{pmatrix}0.1 \\\\ 0.9\\end{pmatrix} - \\begin{pmatrix}1.05 \\\\ 0.95\\end{pmatrix} = \\begin{pmatrix}-0.95 \\\\ -0.05\\end{pmatrix}$\n\nNow we compute the dissimilarities:\n$d_{\\text{brain}}^2(1,2) = \\begin{pmatrix}1 & -1\\end{pmatrix} \\frac{1}{2.4} \\begin{pmatrix}1.7 & -0.7 \\\\ -0.7 & 1.7\\end{pmatrix} \\begin{pmatrix}0.8 \\\\ -0.8\\end{pmatrix} = \\frac{1}{2.4} \\begin{pmatrix}1 & -1\\end{pmatrix} \\begin{pmatrix}1.92 \\\\ -1.92\\end{pmatrix} = \\frac{1.92 - (-1.92)}{2.4} = \\frac{3.84}{2.4} = 1.6$\n\n$d_{\\text{brain}}^2(1,3) = \\begin{pmatrix}0 & -1\\end{pmatrix} \\frac{1}{2.4} \\begin{pmatrix}1.7 & -0.7 \\\\ -0.7 & 1.7\\end{pmatrix} \\begin{pmatrix}-0.15 \\\\ -0.85\\end{pmatrix} = \\frac{1}{2.4} \\begin{pmatrix}0 & -1\\end{pmatrix} \\begin{pmatrix}0.34 \\\\ -1.34\\end{pmatrix} = \\frac{1.34}{2.4} = \\frac{67}{120}$\n\n$d_{\\text{brain}}^2(2,3) = \\begin{pmatrix}-1 & 0\\end{pmatrix} \\frac{1}{2.4} \\begin{pmatrix}1.7 & -0.7 \\\\ -0.7 & 1.7\\end{pmatrix} \\begin{pmatrix}-0.95 \\\\ -0.05\\end{pmatrix} = \\frac{1}{2.4} \\begin{pmatrix}-1 & 0\\end{pmatrix} \\begin{pmatrix}-1.58 \\\\ 0.58\\end{pmatrix} = \\frac{1.58}{2.4} = \\frac{79}{120}$\n\nThe brain RDM entries form the vector $\\mathbf{d}_{\\text{brain}} = \\begin{pmatrix} 1.6 & \\frac{67}{120} & \\frac{79}{120} \\end{pmatrix}^T$.\n\nSecond, we construct the model RDM. The entries are the squared Euclidean distances between the model vectors $\\mathbf{m}_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $\\mathbf{m}_{2} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, and $\\mathbf{m}_{3} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n$d_{\\text{model}}^2(1,2) = ||\\mathbf{m}_1 - \\mathbf{m}_2||^2 = ||\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}||^2 = 1^2 + (-1)^2 = 2$\n$d_{\\text{model}}^2(1,3) = ||\\mathbf{m}_1 - \\mathbf{m}_3||^2 = ||\\begin{pmatrix}0 \\\\ -1\\end{pmatrix}||^2 = 0^2 + (-1)^2 = 1$\n$d_{\\text{model}}^2(2,3) = ||\\mathbf{m}_2 - \\mathbf{m}_3||^2 = ||\\begin{pmatrix}-1 \\\\ 0\\end{pmatrix}||^2 = (-1)^2 + 0^2 = 1$\nThe model RDM entries form the vector $\\mathbf{d}_{\\text{model}} = \\begin{pmatrix} 2 & 1 & 1 \\end{pmatrix}^T$.\n\nThird, we compute the Pearson correlation, $r$, between $\\mathbf{d}_{\\text{brain}}$ and $\\mathbf{d}_{\\text{model}}$. Let $\\mathbf{u} = \\mathbf{d}_{\\text{brain}}$ and $\\mathbf{v} = \\mathbf{d}_{\\text{model}}$.\nThe means are:\n$\\bar{u} = \\frac{1}{3} (1.6 + \\frac{67}{120} + \\frac{79}{120}) = \\frac{1}{3} (\\frac{192}{120} + \\frac{146}{120}) = \\frac{1}{3} \\frac{338}{120} = \\frac{169}{180}$\n$\\bar{v} = \\frac{1}{3} (2+1+1) = \\frac{4}{3}$\n\nThe centered vectors are:\n$u - \\bar{u} = \\begin{pmatrix} 1.6 - \\frac{169}{180} \\\\ \\frac{67}{120} - \\frac{169}{180} \\\\ \\frac{79}{120} - \\frac{169}{180} \\end{pmatrix} = \\begin{pmatrix} \\frac{288-169}{180} \\\\ \\frac{201-338}{360} \\\\ \\frac{237-338}{360} \\end{pmatrix} = \\begin{pmatrix} \\frac{119}{180} \\\\ -\\frac{137}{360} \\\\ -\\frac{101}{360} \\end{pmatrix}$\n$v - \\bar{v} = \\begin{pmatrix} 2 - \\frac{4}{3} \\\\ 1 - \\frac{4}{3} \\\\ 1 - \\frac{4}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$\n\nThe numerator of the correlation formula is the sum of products of deviations:\n$\\sum (u_i - \\bar{u})(v_i - \\bar{v}) = (\\frac{119}{180})(\\frac{2}{3}) + (-\\frac{137}{360})(-\\frac{1}{3}) + (-\\frac{101}{360})(-\\frac{1}{3})$\n$= \\frac{238}{540} + \\frac{137}{1080} + \\frac{101}{1080} = \\frac{476}{1080} + \\frac{238}{1080} = \\frac{714}{1080} = \\frac{119}{180}$\n\nThe denominator terms are the square roots of the sum of squared deviations:\n$\\sum (u_i - \\bar{u})^2 = (\\frac{119}{180})^2 + (\\frac{-137}{360})^2 + (\\frac{-101}{360})^2 = \\frac{14161}{32400} + \\frac{18769}{129600} + \\frac{10201}{129600}$\n$= \\frac{4 \\times 14161 + 18769 + 10201}{129600} = \\frac{56644 + 18769 + 10201}{129600} = \\frac{85614}{129600} = \\frac{14269}{21600}$\n$\\sum (v_i - \\bar{v})^2 = (\\frac{2}{3})^2 + (-\\frac{1}{3})^2 + (-\\frac{1}{3})^2 = \\frac{4}{9} + \\frac{1}{9} + \\frac{1}{9} = \\frac{6}{9} = \\frac{2}{3}$\n\nThe Pearson correlation coefficient is:\n$$ r = \\frac{\\frac{119}{180}}{\\sqrt{\\frac{14269}{21600}} \\sqrt{\\frac{2}{3}}} = \\frac{\\frac{119}{180}}{\\sqrt{\\frac{14269 \\times 2}{21600 \\times 3}}} = \\frac{\\frac{119}{180}}{\\sqrt{\\frac{28538}{64800}}} $$\nTo simplify, we can compute $r^2$:\n$$ r^2 = \\frac{(\\frac{119}{180})^2}{\\frac{28538}{64800}} = \\frac{\\frac{14161}{32400}}{\\frac{28538}{64800}} = \\frac{14161}{32400} \\frac{64800}{28538} = \\frac{14161 \\times 2}{28538} = \\frac{28322}{28538} $$\n$$ r = \\sqrt{\\frac{28322}{28538}} \\approx \\sqrt{0.992429399} \\approx 0.9962075 $$\nRounding to four significant figures, the correlation is $0.9962$.\n\nFinally, we discuss the bias-variance trade-off introduced by the shrinkage parameter $\\lambda$.\nThe shrinkage estimator for the covariance matrix, $\\hat{\\Sigma}_{\\lambda} = (1-\\lambda)\\hat{\\Sigma} + \\lambda I$, combines the sample covariance $\\hat{\\Sigma}$ with a scaled identity matrix $I$. This technique is a form of regularization.\n\nBias: The sample covariance $\\hat{\\Sigma}$ is an (asymptotically) unbiased estimator of the true noise covariance $\\Sigma$. The identity matrix $I$ is generally a biased estimator of $\\Sigma$, unless the true noise is spherical (i.e., $\\Sigma \\propto I$). By choosing $\\lambda > 0$, we introduce bias into our covariance estimate. The expected value of the estimator is $E[\\hat{\\Sigma}_{\\lambda}] = (1-\\lambda)\\Sigma + \\lambda I$. The bias is $E[\\hat{\\Sigma}_{\\lambda}] - \\Sigma = \\lambda(I - \\Sigma)$. This bias increases with $\\lambda$ and with the extent to which the true covariance structure deviates from the identity matrix (e.g., strong off-diagonal correlations). In our problem, $\\hat{\\Sigma}$ has non-zero off-diagonals, so using $\\lambda=0.3$ introduces a bias that underestimates the true covariance between the voxels.\n\nVariance: The sample covariance $\\hat{\\Sigma}$ can have high variance, meaning its entries can fluctuate substantially across different samples of data, especially if the number of data points used for estimation is small relative to the number of dimensions (voxels). In contrast, the identity matrix $I$ is a constant with zero variance. The variance of the shrinkage estimator is $\\text{Var}(\\hat{\\Sigma}_{\\lambda}) = (1-\\lambda)^2 \\text{Var}(\\hat{\\Sigma})$. As $\\lambda$ increases from $0$ to $1$, the variance of the estimator decreases, making the estimate more stable.\n\nTrade-off: The use of shrinkage embodies a classic bias-variance trade-off. By increasing $\\lambda$, we accept more bias in our covariance estimate in exchange for a reduction in its variance. For noisy or high-dimensional data, the large reduction in variance can outweigh the increased bias, leading to a lower overall mean squared error and a more robust estimate of the true covariance. An optimal $\\lambda$ would balance these two competing factors.\n\nManifestation in dissimilarities: This trade-off directly affects the computed crossnobis distances, $d^2 = \\Delta \\mathbf{x}^{\\mathsf{A} T} \\hat{\\Sigma}_{\\lambda}^{-1} \\Delta \\mathbf{x}^{\\mathsf{B}}$. The matrix $\\hat{\\Sigma}_{\\lambda}^{-1}$ acts as a noise-whitening transformation. If $\\lambda=0$, we use $\\hat{\\Sigma}^{-1}$, which provides an unbiased but potentially unstable whitening. If $\\hat{\\Sigma}$ is ill-conditioned, $\\hat{\\Sigma}^{-1}$ will have huge entries, excessively amplifying noise in certain dimensions and leading to highly variable (high variance) dissimilarity estimates. If $\\lambda=1$, we use $I^{-1}=I$, which assumes noise is uncorrelated and of equal variance across voxels. This is a strong, biased assumption, but it results in a very stable (low variance) dissimilarity calculation (the cross-validated dot product). Using an intermediate $\\lambda$ like $0.3$ regularizes the matrix inversion, making the dissimilarity estimates more stable than the pure Mahalanobis distance ($\\lambda=0$) but at the cost of imperfect noise whitening (bias). The hope is that this leads to a more reliable RDM that better reflects the true representational geometry by not being overwhelmed by sampling noise.",
            "answer": "$$ \\boxed{0.9962} $$"
        },
        {
            "introduction": "After computing brain and model RDMs and measuring their similarity, the final step is to ask: is this similarity statistically significant? This exercise  focuses on this crucial inferential step, guiding you to define the correct permutation test for comparing RDMs. You will learn why naively shuffling dissimilarities is incorrect and how to properly respect the structured dependencies within an RDM by permuting stimulus labels, thereby correctly formulating the null hypothesis of no correspondence between brain and model representations.",
            "id": "4147113",
            "problem": "An investigator seeks to test whether a computational vision model captures the representational geometry of a human ventral visual cortex region measured with functional Magnetic Resonance Imaging (fMRI). For each of $n$ stimuli, the investigator computes a brain response vector and a model feature vector, then constructs two $n \\times n$ Representational Dissimilarity Matrices (RDMs), denoted $D^{(b)}$ for brain and $D^{(m)}$ for model, where $D^{(b)}_{ij}$ and $D^{(m)}_{ij}$ quantify the dissimilarity between stimulus $i$ and stimulus $j$ in brain and model feature spaces, respectively. The investigator summarizes the concordance between $D^{(b)}$ and $D^{(m)}$ using a Spearman rank correlation statistic $t$, defined as the Spearman correlation between the vectors formed by the upper-triangular (excluding diagonal) entries of each RDM:\n$$\nt \\;=\\; \\rho_{S}\\!\\Big(\\operatorname{vec}_{\\triangle}\\big(D^{(b)}\\big),\\, \\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)\\Big).\n$$\nTo assess statistical significance without distributional assumptions, the investigator intends to use a permutation test that shuffles stimulus labels, leveraging the principle of exchangeability under the null hypothesis.\n\nStarting from the following fundamental definitions and facts:\n- An RDM is a symmetric matrix of pairwise dissimilarities across stimuli, with $D_{ii}=0$ and $D_{ij}=D_{ji}$.\n- Under the null hypothesis that stimulus identities do not align between brain and model, stimulus labels are exchangeable; equivalently, any permutation of stimulus labels should not change the joint distribution that links $D^{(b)}$ and $D^{(m)}$.\n- A valid permutation for an RDM must permute stimulus labels consistently across both dimensions (rows and columns), preserving symmetry and the structural dependencies among pairwise entries.\n\nWhich option correctly defines an appropriate permutation test for the Spearman correlation $t$ by shuffling stimulus labels and precisely states the null hypothesis being tested?\n\nA. Apply a random permutation $\\pi$ of the $n$ stimulus labels to the brain RDM by forming $D^{(b)}_{\\pi} = P D^{(b)} P^{\\top}$, where $P$ is the permutation matrix corresponding to $\\pi$. For each $\\pi$, compute\n$$\nt_{\\pi} \\;=\\; \\rho_{S}\\!\\Big(\\operatorname{vec}_{\\triangle}\\big(D^{(b)}_{\\pi}\\big),\\, \\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)\\Big),\n$$\nand use the empirical distribution of $\\{t_{\\pi}\\}$ to assess $t$. The null hypothesis is that stimulus labels are exchangeable between brain and model, i.e., the mapping from stimulus identities in $D^{(b)}$ to those in $D^{(m)}$ carries no information, so the population Spearman correlation between corresponding entries is $0$ and the distribution of $t$ is invariant to $\\pi$.\n\nB. Randomly shuffle the entries of $\\operatorname{vec}_{\\triangle}\\big(D^{(b)}\\big)$ without regard to which pairs they came from, then correlate this shuffled vector with $\\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)$. The null hypothesis is that the individual pairwise dissimilarities in the brain are independent and identically distributed, so the order of entries in the brain RDM is irrelevant.\n\nC. Randomly permute trial repetitions across stimuli in the fMRI data before computing $D^{(b)}$, leaving $D^{(m)}$ unchanged, and then compute the correlation $t$ on the resulting matrices. The null hypothesis is that trial identities are exchangeable within and across stimuli, so any observed correlation must arise from trial-level noise rather than stimulus-level structure.\n\nD. Independently permute the row and column labels of $D^{(b)}$ by applying two unrelated permutations $\\pi_{r}$ and $\\pi_{c}$ to form a matrix $D'^{\\,(b)}$ with entries $D'^{\\,(b)}_{ij} = D^{(b)}_{\\pi_{r}(i)\\,\\pi_{c}(j)}$, then correlate $\\operatorname{vec}_{\\triangle}\\big(D'^{\\,(b)}\\big)$ with $\\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)$. The null hypothesis is that the brain and model RDMs are independent across rows and columns separately, so breaking symmetry does not affect the expected correlation.",
            "solution": "### Step 1: Extract Givens\nThe problem statement provides the following information and definitions:\n- There are $n$ stimuli.\n- Two $n \\times n$ Representational Dissimilarity Matrices (RDMs) are given: $D^{(b)}$ (brain data) and $D^{(m)}$ (model).\n- The entries $D^{(b)}_{ij}$ and $D^{(m)}_{ij}$ represent the dissimilarity between stimulus $i$ and stimulus $j$.\n- A test statistic $t$ is defined as the Spearman rank correlation between the vectorized upper-triangular parts of the RDMs:\n$$\nt \\;=\\; \\rho_{S}\\!\\Big(\\operatorname{vec}_{\\triangle}\\big(D^{(b)}\\big),\\, \\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)\\Big).\n$$\n- A permutation test is to be used to assess the statistical significance of $t$.\n- Fundamental properties of an RDM: It is a symmetric matrix ($D_{ij}=D_{ji}$) with a zero diagonal ($D_{ii}=0$).\n- Principle for the permutation test under the null hypothesis ($H_0$): Stimulus labels are exchangeable between brain and model.\n- Constraint on a valid permutation: It must permute stimulus labels consistently across rows and columns, preserving the RDM's symmetry and the structural dependencies among its entries.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a standard and important procedure in computational neuroscience known as Representational Similarity Analysis (RSA).\n- **Scientifically Grounded:** The entire setup is a cornerstone of modern systems and cognitive neuroscience. The use of RDMs to characterize representational geometry, the comparison between brain data (fMRI) and computational models, the choice of Spearman correlation to allow for non-linear but monotonic relationships, and the use of permutation tests for non-parametric inference are all well-established, standard practices. The problem is firmly grounded in scientific methodology.\n- **Well-Posed:** The question is precise. It asks for the correct permutation procedure to test a specific, well-defined null hypothesis for a given statistic ($t$). The provided principles and constraints allow for a unique and correct answer to be determined through logical and statistical reasoning.\n- **Objective:** The problem is described using formal mathematical and statistical language, free of ambiguity or subjectivity.\n\nThe problem is not unsound, incomplete, contradictory, unrealistic, or ill-posed. All terms are either formally defined ($t$, RDM) or standard in the field (permutation test, exchangeability).\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The solution process will proceed by deriving the correct permutation procedure from first principles and then evaluating each option.\n\n### Derivation and Option Analysis\n\nThe central task is to construct a null distribution for the statistic $t$ under the null hypothesis, $H_0$, that there is no systematic relationship between the representational geometry of the brain and that of the model. This hypothesis implies that the stimulus labels $\\{1, 2, \\dots, n\\}$ are not meaningfully aligned between $D^{(b)}$ and $D^{(m)}$. Any observed correlation is thus attributable to chance. The principle of exchangeability means that if $H_0$ is true, any permutation of the stimulus labels for one of the RDMs should not systematically change the resulting correlation with the other RDM.\n\nLet's formalize the act of \"permuting stimulus labels\" for an RDM, say $D^{(b)}$. An RDM is a matrix where both rows and columns are indexed by the same set of stimuli. If we apply a permutation $\\pi$ to the stimulus labels, the new dissimilarity between what are now labeled as stimulus '$i$' and stimulus '$j$' must be the old dissimilarity between stimulus `$\\pi(i)$` and stimulus `$\\pi(j)$`. This is incorrect. The permutation re-assigns the labels. The new matrix, let's call it $D^{(b)}_{\\pi}$, should have at its $(i,j)$ position the dissimilarity corresponding to the stimuli that were moved to the $i$-th and $j$-th positions. Let $\\pi$ be a permutation of $\\{1, ..., n\\}$. The permutation matrix $P$ corresponding to $\\pi$ is often defined such that it permutes the rows of a matrix it multiplies from the left. For an RDM, we must permute the columns in the same way to maintain the correspondence of a stimulus with its row and column. This operation, a similarity transformation, is given by $D^{(b)}_{\\pi} = P D^{(b)} P^{\\top}$, where $P^{\\top} = P^{-1}$ because permutation matrices are orthogonal. This transformation simultaneously permutes the rows and columns, preserving the symmetry of the matrix ($ (P D P^{\\top})^{\\top} = (P^{\\top})^{\\top} D^{\\top} P^{\\top} = P D P^{\\top}$ since $D$ is symmetric) and its zero diagonal. This correctly implements the shuffling of stimulus labels while preserving the internal dependency structure of the RDM, as required by the problem's constraints.\n\nThe permutation test procedure is therefore:\n1.  Calculate the observed statistic, $t$, from the original matrices $D^{(b)}$ and $D^{(m)}$.\n2.  Repeat for a large number of iterations:\n    a. Generate a random permutation $\\pi$ of the $n$ stimulus labels.\n    b. Construct the corresponding permutation matrix $P$.\n    c. Apply the permutation to one of the RDMs, e.g., $D^{(b)}_{\\pi} = P D^{(b)} P^{\\top}$.\n    d. Calculate the statistic on the permuted data: $t_{\\pi} = \\rho_{S}(\\operatorname{vec}_{\\triangle}(D^{(b)}_{\\pi}), \\operatorname{vec}_{\\triangle}(D^{(m)}))$.\n3.  The collection of $\\{t_{\\pi}\\}$ values forms the empirical null distribution. The p-value is the proportion of these null values that are greater than or equal to the observed value $t$.\n\nNow we evaluate each option based on this correct procedure.\n\n**A. Apply a random permutation $\\pi$ of the $n$ stimulus labels to the brain RDM by forming $D^{(b)}_{\\pi} = P D^{(b)} P^{\\top}$, where $P$ is the permutation matrix corresponding to $\\pi$. For each $\\pi$, compute $t_{\\pi} \\;=\\; \\rho_{S}\\!\\Big(\\operatorname{vec}_{\\triangle}\\big(D^{(b)}_{\\pi}\\big),\\, \\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)\\Big)$, and use the empirical distribution of $\\{t_{\\pi}\\}$ to assess $t$. The null hypothesis is that stimulus labels are exchangeable between brain and model, i.e., the mapping from stimulus identities in $D^{(b)}$ to those in $D^{(m)}$ carries no information, so the population Spearman correlation between corresponding entries is $0$ and the distribution of $t$ is invariant to $\\pi$.**\n\nThis option precisely matches the derived correct procedure. The formula $D^{(b)}_{\\pi} = P D^{(b)} P^{\\top}$ is the correct matrix operation for relabeling stimuli. The subsequent computation of $t_{\\pi}$ and the formation of the null distribution are standard. The statement of the null hypothesis is accurate and comprehensive, correctly linking the concept of exchangeable labels to the lack of informational mapping between the two RDMs.\n\n**Verdict: Correct**\n\n**B. Randomly shuffle the entries of $\\operatorname{vec}_{\\triangle}\\big(D^{(b)}\\big)$ without regard to which pairs they came from, then correlate this shuffled vector with $\\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)$. The null hypothesis is that the individual pairwise dissimilarities in the brain are independent and identically distributed, so the order of entries in the brain RDM is irrelevant.**\n\nThis procedure is fundamentally flawed. The entries of an RDM are not independent. For instance, dissimilarities involving a common stimulus (e.g., $D_{ij}$ and $D_{ik}$) are structurally related. Shuffling the individual entries of $\\operatorname{vec}_{\\triangle}(D^{(b)})$ destroys this inherent structure of the representational space. The problem statement explicitly requires that a valid permutation must preserve \"the structural dependencies among pairwise entries.\" This procedure violates that critical constraint. It tests a much stronger and generally false null hypothesis of independence among all pairwise dissimilarities.\n\n**Verdict: Incorrect**\n\n**C. Randomly permute trial repetitions across stimuli in the fMRI data before computing $D^{(b)}$, leaving $D^{(m)}$ unchanged, and then compute the correlation $t$ on the resulting matrices. The null hypothesis is that trial identities are exchangeable within and across stimuli, so any observed correlation must arise from trial-level noise rather than stimulus-level structure.**\n\nThis procedure tests a different null hypothesis. It asks whether the brain responses contain any stimulus-specific information at all. By permuting trials across different stimuli before even computing the brain RDM, one erases any structure in $D^{(b)}$ that is systematically related to the stimuli. This is a valid test for asking \"Is there any representational structure in the brain data?\", but it is not the correct test for the primary question: \"Is the representational structure in the brain data similar to the structure in the model data?\". The problem is about comparing two given, structured RDMs.\n\n**Verdict: Incorrect**\n\n**D. Independently permute the row and column labels of $D^{(b)}$ by applying two unrelated permutations $\\pi_{r}$ and $\\pi_{c}$ to form a matrix $D'^{\\,(b)}$ with entries $D'^{\\,(b)}_{ij} = D^{(b)}_{\\pi_{r}(i)\\,\\pi_{c}(j)}$, then correlate $\\operatorname{vec}_{\\triangle}\\big(D'^{\\,(b)}\\big)$ with $\\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)$. The null hypothesis is that the brain and model RDMs are independent across rows and columns separately, so breaking symmetry does not affect the expected correlation.**\n\nThis procedure is incorrect because it violates a fundamental property of RDMs. Applying two independent permutations to the rows and columns, $\\pi_r$ and $\\pi_c$, will in general destroy the symmetry of the matrix. The resulting matrix $D'^{\\,(b)}$ will not be an RDM, as $D'^{\\,(b)}_{ij} = D^{(b)}_{\\pi_{r}(i)\\,\\pi_{c}(j)}$ will not equal $D'^{\\,(b)}_{ji} = D^{(b)}_{\\pi_{r}(j)\\,\\pi_{c}(i)}$. The problem statement explicitly requires a valid permutation to \"preserv[e] symmetry\". This procedure fails this requirement.\n\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}