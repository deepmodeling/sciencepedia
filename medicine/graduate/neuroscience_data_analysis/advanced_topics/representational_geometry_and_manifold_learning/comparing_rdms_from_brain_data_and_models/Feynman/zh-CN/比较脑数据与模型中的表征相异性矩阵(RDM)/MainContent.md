## 引言
在理解大脑和人工智能等复杂系统时，我们面临一个核心挑战：如何比较它们表征信息的方式？仅仅观察活动水平是不够的，我们需要一种能深入其内在结构的方法。[表征相似性分析](@entry_id:1130877)（Representational Similarity Analysis, RSA）正是为解决这一知识鸿沟而生，它提供了一个强大的通用框架，通过比较表征的“几何形状”来连接看似不同的领域。本文将带领您全面掌握这一前沿方法。在“原理与机制”一章中，我们将深入探讨RSA的核心概念，从构建表征相异性矩阵（RDM）到比较它们的统计方法。接着，在“应用与跨学科连接”一章，我们将展示RSA如何成为连接大脑、[计算模型](@entry_id:637456)与人类行为的桥梁，并探索其在绘制脑功能图谱和研究意识等方面的强大能力。最后，“动手实践”一章将提供具体的编程练习，帮助您将理论知识转化为实践技能。

## 原理与机制

在引言中，我们了解了[表征相似性分析](@entry_id:1130877)（Representational Similarity Analysis, RSA）的宏伟目标：通过比较不同系统（例如，人脑与人工智能模型）中信息表征的几何结构，来搭建理解的桥梁。现在，让我们卷起袖子，深入其内部，探索其工作的核心原理与精妙机制。这趟旅程将带领我们从抽象的哲学概念，走向严谨的数学工具和巧妙的[实验设计](@entry_id:142447)。

### 什么是表征？一场进入“表征空间”的旅程

想象一下，当你看到一只猫时，你大脑中的特定神经元群体会活跃起来。这个活动模式——数百万[神经元放电频率](@entry_id:175619)的复杂交响乐——就是大脑对“猫”这个概念的**[神经表征](@entry_id:1128614)**。为了能“看见”这个表征，我们可以把每个神经元想象成一个维度，它在该时刻的放电率就是在这个维度上的一个坐标值。如果我们在一个包含数百万个维度的“空间”里，根据这些坐标标出一个点，那么这个点就唯一地代表了“猫”的神经活动模式。

这个高维空间，我们称之为**表征空间（representational space）**。在这个空间里，每一个你所能感知、想象或理解的概念——“狗”、“汽车”、“自由”、“快乐”——都对应着一个独特的点。

现在，真正有趣的事情发生了。这些点在空间中的排布并非杂乱无章，它们的相对位置蕴含着深刻的意义。你可能会直觉地认为，“猫”和“狗”这两个点在空间中的距离，应该比“猫”和“汽车”的距离更近，因为猫和狗在生物学上和概念上都更为相似。这种由点之间的相对远近关系构成的整体结构，我们称之为**[表征几何](@entry_id:1130876)（representational geometry）**。这正是 RSA 试图捕捉和理解的核心：心智的几何形状。

### 表征相异性矩阵（RDM）：绘制心智几何的地图

我们如何才能精确地捕捉和描述这种看不见、摸不着的几何结构呢？答案出奇地简单而优雅：我们只需要系统地测量每对表征点之间的“距离”或“不相似性”。

这就是**表征相异性矩阵（Representational Dissimilarity Matrix, RDM）**的用武之地。假设我们有 $n$ 个不同的刺激（例如，$n$ 张不同的图片），我们就可以构建一个 $n \times n$ 的矩阵。矩阵中的每一个元素 $D_{ij}$，记录的就是刺激 $i$ 和刺激 $j$ 的神经表征之间的相异性程度。

一个典型的 RDM 具有几个鲜明的数学特性 ：

*   **方阵**：对于 $n$ 个刺激，它是一个 $n \times n$ 的矩阵。
*   **空心对角线（Hollow）**：对角线上的元素 $D_{ii}$ 永远为零。这非常符合直觉，因为任何一个刺激的表征与它自身的表征都是完全相同的，没有任何相异性。
*   **对称性**：矩阵是对称的，即 $D_{ij} = D_{ji}$。从“猫”到“狗”的距离和从“狗”到“猫”的距离是一样的。

值得注意的是，RDM 与我们在其他领域常见的**相似性矩阵（similarity matrix）**有所不同。例如，一个由向量[内积](@entry_id:750660)构成的相似性矩阵（[格拉姆矩阵](@entry_id:203297)），其对角线元素代表了向量自身的能量（范数的平方），通常不为零，并且整个矩阵具有正半定性。RDM 则没有这些要求，它只专注于成对的“差异”，从而为我们提供了一幅纯粹的、关于表征空间相对结构的地图。

### 度量“距离”：不只是一把尺子

RDM 的威力源于我们可以灵活选择度量“距离”的“尺子”。不同的“尺子”对[表征几何](@entry_id:1130876)的不同方面敏感，选择哪一把，取决于我们关心表征的何种特性。

让我们来看几种常用的“尺子” ：

*   **[欧几里得距离](@entry_id:143990)（Euclidean distance）**：这是我们最熟悉的距离，就像在三维空间里用尺子量一样。它计算的是两个活动模式向量在表征空间中的直线距离。这种距离对于模式向量的整体幅值和模式的差异都很敏感。

*   **[相关距离](@entry_id:634939)（Correlation distance）**：计算方法是 $1 - r$，其中 $r$ 是两个活动模式向量的[皮尔逊相关系数](@entry_id:918491)。想象一下，我们可能只关心神经活动的“模式”或“形状”，而不关心其整体强度（比如，所有神经元放电率都加倍）。[相关距离](@entry_id:634939)就是为此设计的。它只对向量的“角度”敏感，而对它们的“长度”（范数）不敏感。因此，如果两个活动模式只是在整体强度上不同，但相对活动模式一致，它们的[相关距离](@entry_id:634939)就会很小。

*   **马氏距离（Mahalanobis distance）**：这是一种更高级、也更强大的“尺子”。在真实的神经数据中（例如fMRI的体素），不同的测量通道（神经元或体素）的噪声水平可能不同，而且它们之间的噪声可能还存在相关性。欧几里得距离天真地假设所有维度的“度量衡”和噪声都是一样的。[马氏距离](@entry_id:269828)则聪明地考虑了数据的噪声协方差结构 $\Sigma$。它首先对空间进行“白化”变换，使得噪声在所有方向上都变得均匀，然后再计算欧几里得距离。这相当于在一个被噪声“拉伸”和“扭曲”了的空间里，找到了一条“真正的”最短路径。因此，马氏距离被认为是衡量神经表征差异的一种统计上更优的方法 。

有趣的是，我们通常所说的“距离”在数学上有严格的定义（即“[度量空间](@entry_id:138860)”的公理），包括非负性、同一性、对称性和[三角不等式](@entry_id:143750)。然而，在 RSA 中使用的一些“相异性”度量，比如[相关距离](@entry_id:634939)和平方[欧几里得距离](@entry_id:143990)，并不满足[三角不等式](@entry_id:143750)（即从 A 到 C 的距离不一定小于从 A 到 B 再到 C 的距离之和）。这表明我们使用的“尺子”可能比日常生活中的尺子更广义，它们是所谓的“半度量（semimetric）”。这恰恰体现了该领域的严谨性：我们清楚地知道我们工具的数学属性及其适用范围。

选择哪种距离，实际上是在选择我们所关心的**[不变性](@entry_id:140168)（invariance）**。[欧几里得距离](@entry_id:143990)在旋转下不变，[相关距离](@entry_id:634939)在正向缩放下不变，而马氏距离则在任何可逆的[线性变换](@entry_id:149133)下都保持不变（只要我们相应地变换噪声协方差矩阵）。这个选择本身，就是一种关于“什么才是表征中真正重要的东西”的理论声明 。

### 从脑扫描到 RDM：估计的艺术

理论很美好，但我们如何从真实的、嘈杂的脑成像数据（如 fMRI）中获得构建 RDM 所需的活动模式呢？

这通常通过**通用线性模型（General Linear Model, GLM）** 来实现。我们可以将 fMRI 记录到的、随时间变化的复杂信号，看作是不同刺激引发的已知响应模式、一些我们不感兴趣的噪声源（如头部运动、生理噪声）以及随机噪声的线性叠加。GLM 的任务就是从这个混合体中，为每一个刺激“解”出其对应的、专属的活动模式，即一组[回归系数](@entry_id:634860) $\hat{\beta}$。每一个 $\hat{\beta}$ 向量，就是表征空间中的一个点 。

然而，这些估计出的 $\hat{\beta}$ 向量本身也带有噪声。如果我们直接在这些带噪声的估计上计算距离（尤其是平方距离），噪声会系统性地抬高距离的估计值，导致 RDM 产生偏差。

为了解决这个问题，研究者们发明了一种非常巧妙的技巧：**[交叉验证](@entry_id:164650)（cross-validation）**。核心思想是：不要用同一批数据来估计要比较的两个活动模式。我们可以将数据分成独立的两半（例如，奇数次实验和偶数次实验），在每一半数据上都对所有刺激的活动模式进行一次估计。在计算刺激 $i$ 和 $j$ 的距离时，我们用第一半数据中刺激 $i$ 的模式和第二半数据中刺激 $j$ 的模式来计算。由于两半数据的噪声是独立的，它们在计算中就不会相互“串通”来人为地抬高距离。

**交叉验证[马氏距离](@entry_id:269828)（cross-validated Mahalanobis distance）**，有时也被戏称为“crossnobis”，就是这种思想的完美体现。它能够提供对真实（无噪声）的平方[马氏距离](@entry_id:269828)的[无偏估计](@entry_id:756289)，让我们能够“看穿”[测量噪声](@entry_id:275238)，直达表征本身的结构  。

### 比较世界：大脑、模型与求索之路

至此，我们已经有了一张来自大脑的“心智几何地图”——脑 RDM。接下来，RSA 的核心步骤就是将其与另一张地图——来自某个理论模型或[计算模型](@entry_id:637456)（如深度神经网络）的**模型 RDM**——进行比较。如果两张地图足够相似，我们就有理由相信，这个模型在某种程度上捕捉到了大脑进行信息处理的原理。

如何比较两张地图（两个矩阵）呢？最直接的方法是计算它们之间的相关性。为此，我们需要先将这两个 $n \times n$ 的 RDM “拉平”成向量。一个关键的细节是，我们只取矩阵的**上三角（或下三角）部分，并且不包括对角线**。这是因为 RDM 的对角线总是零，且矩阵是对称的，包含所有元素会引入冗余信息和[共线性](@entry_id:270224)，对统计分析造成困扰。通过只取上三角元素，我们确保了每一个独一无二的刺激对（例如，“猫”和“狗”）的相异性值只被统计一次 。

接下来，我们应该用哪种[相关系数](@entry_id:147037)呢？通常，研究者会选择**[斯皮尔曼等级相关](@entry_id:755150)（Spearman's rank correlation）**。为什么不用更常见的[皮尔逊相关](@entry_id:260880)呢？因为我们往往不能保证从大脑和模型中测量到的“相异性”值是在同一个尺度上的。也许大脑的相异性值从 0.1 到 0.5，而模型的相异性值从 10 到 1000。[皮尔逊相关](@entry_id:260880)衡量的是线性关系，它会因为这种尺度的差异而给出很低的相关值。[斯皮尔曼相关](@entry_id:896527)则不同，它首先将两组数值都转换成它们的“等级”或“排名”，然后计算这些等级之间的[皮尔逊相关](@entry_id:260880)。它只关心顺序：在你的 RDM 中，A-B 对是否比 C-D 对更不相似？只要这个顺[序关系](@entry_id:138937)在两个 RDM 中得以保持，无论具体的数值是多少，[斯皮尔曼相关](@entry_id:896527)都会很高。这种对任意单调变换的**稳健性（robustness）**，使其成为比较 RDM 的理想工具 。

### 直面现实：噪声天花板与混淆因素

在宣称一个模型成功之前，严谨的科学家还需要回答两个问题：我们取得的相关性到底有多好？以及，这种相关性会不会是由某些“无聊”的因素导致的？

#### 噪声天花板

想象一下，你有一张主体有些模糊的照片，你想评价一幅画作画得像不像照片里的人。这幅画作能达到的“最像”程度，必然受到照片本身模糊度的限制。它不可能比另一张同样模糊地拍摄这个人的照片“更像”原始照片。

同样地，我们的大脑数据本身也是带有噪声的。因此，任何一个理论模型与我们测量到的脑 RDM 的相关性，都不可能高过这份脑数据本身的**信度（reliability）**。这个由数据质量决定的理论上限，被称为**噪声[天花](@entry_id:920451)板（noise ceiling）**。它告诉我们，一个“完美”的模型在我们的特定数据集上所能达到的最佳表现。

从数学上讲，观测到的相关性 $\operatorname{Corr}_{\text{observed}}$ 会被噪声所“衰减”，它与“真实”（无噪声）的相关性 $\operatorname{Corr}_{\text{true}}$ 的关系，由数据的信度 $\rho_{b}$ 决定：$\operatorname{Corr}_{\text{observed}} = \operatorname{Corr}_{\text{true}} \sqrt{\rho_{b}}$。这意味着，观测相关性的上限就是 $\sqrt{\rho_{b}}$ 。我们可以通过将数据分成两半，计算两半数据 RDM 之间的相关性来估计信度，进而估算出噪声天花板。反过来，我们还可以利用**衰减矫正公式**，从观测到的相关性和信度中，反推出模型在“无噪声世界”里可能达到的真实相关性，从而更公平地评价模型的优劣 。

#### 控制混淆因素

假设一个视觉模型在预测大脑对自然图像的反应时表现出色。我们必须警惕一种可能性：这种成功或许仅仅是因为模型和大脑都对一些简单的、低级的图像特征敏感。例如，两张图像在模型和大脑中都引发了相似的活动，可能只是因为它们的像素值或[空间频率](@entry_id:270500)成分恰好很相似。

为了排除这种“无聊”的解释，我们需要明确地控制这些**混淆因素（confounds）**。我们可以为这些低级特征（如像素强度、颜色[直方图](@entry_id:178776)、空间频率能量分布）专门构建**混淆 RDM**。然后，在比较我们的[主模](@entry_id:263463)型 RDM 和脑 RDM 时，使用**[多元回归](@entry_id:144007)分析**，将这些混淆 RDM 作为额外的“讨厌的”预测变量（nuisance predictors）放入模型中。通过这种方式，我们可以问一个更尖锐的问题：我们的模型在解释了所有这些低级因素之后，还能**唯一地（uniquely）**解释脑 RDM 中的多少变异？这才是证明模型捕捉到了更高级、更抽象表征的有力证据 。

### 更广阔的视角：RSA 与[编码模型](@entry_id:1124422)

最后，值得强调的是，RSA 并非神经科学中理解表征的唯一工具。它有一个重要的伙伴（有时也被视为对手）：**[编码模型](@entry_id:1124422)（encoding models）**。

*   **[编码模型](@entry_id:1124422)**的目标是建立一个从刺激特征到大脑每个测量单元（如单个体素）活动的直接预测模型。它试图回答：“给定这张图片的特征，我能预测出 V1 区这个体素的激活强度吗？”
*   **RSA** 则采取了不同的策略。它首先将所有体素的活动“打包”成一个高维的[表征几何](@entry_id:1130876)，然后比较这个几何结构与模型预测的几何结构。

这两种方法各有千秋，适用于回答不同的科学问题 ：

*   **RSA 的优势在于**：
    *   **抽象性与跨物种/被试比较**：由于 RDM 抽象掉了具体的体素排布，它对于表征空间的旋转是不变的。这意味着，即使不同被试（甚至不同物种）的大脑在解剖上无法完美对齐，只要他们的表征几何相似，RSA 依然能发现这种共性。
    *   **[高维数据](@entry_id:138874)下的稳健性**：在典型的 fMRI 实验中，刺激数量远少于体素数量，直接拟合[编码模型](@entry_id:1124422)容易[过拟合](@entry_id:139093)。RSA 将问题降维到 $n \times n$ 的 RDM 空间，统计上更为稳健。

*   **[编码模型](@entry_id:1124422)的优势在于**：
    *   **[空间定位](@entry_id:919597)与预测能力**：编码模型可以直接告诉我们“什么信息”在“大脑的什么地方”被表征，生成精细的脑功能图谱。它的终极目标是预测全新的刺激会引发何种大脑活动。

RSA 和[编码模型](@entry_id:1124422)并非相互排斥，而是互为补充的强大工具。它们从不同角度审视着同一个核心问题——大脑如何表征世界。通过结合使用它们，我们能够更全面、更深入地揭开心智的奥秘。