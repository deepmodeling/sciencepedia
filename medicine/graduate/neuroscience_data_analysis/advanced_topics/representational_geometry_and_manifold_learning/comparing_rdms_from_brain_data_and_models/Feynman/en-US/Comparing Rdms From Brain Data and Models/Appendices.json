{
    "hands_on_practices": [
        {
            "introduction": "The choice of dissimilarity metric is a critical first step in Representational Similarity Analysis, as it defines what aspects of the data are considered signal versus noise. This exercise explores the fundamental mathematical relationship between two widely used metrics: cosine distance and correlation distance. By analytically deriving their difference for linearly related brain patterns, you will build a first-principles understanding of how each metric treats scaling and offset components of the neural signal, clarifying the crucial role of mean-centering. ",
            "id": "4147129",
            "problem": "Consider two condition-response vectors in a single brain region, represented by $x \\in \\mathbb{R}^{p}$ and $y \\in \\mathbb{R}^{p}$, where $p \\geq 2$ is the number of measurement channels (e.g., voxels). Let $x$ be the empirical brain pattern for a given condition and suppose the model’s prediction for the same condition is given by a linear transformation $y = \\gamma x + \\beta \\mathbf{1}$, where $\\mathbf{1} \\in \\mathbb{R}^{p}$ is the all-ones vector, $\\gamma \\in \\mathbb{R} \\setminus \\{0\\}$ is a scaling parameter, and $\\beta \\in \\mathbb{R}$ is an additive offset. Denote the mean of $x$ by $\\bar{x} = \\frac{1}{p} \\sum_{i=1}^{p} x_{i}$ and assume $\\|x - \\bar{x}\\mathbf{1}\\| \\neq 0$ so that centered variation is non-degenerate. The Representational Dissimilarity Matrix (RDM) entry for a pair of vectors under a given dissimilarity is defined as one minus the corresponding similarity.\n\nUsing only standard definitions of cosine distance (one minus cosine similarity) and correlation distance (one minus Pearson correlation computed on mean-centered vectors), derive a closed-form analytic expression for the difference between the cosine-based and correlation-based RDM entries,\n$$\\Delta \\equiv d_{\\mathrm{cos}}(x,y) - d_{\\mathrm{corr}}(x,y)$$\nin terms of $p$, $\\gamma$, $\\beta$, $\\bar{x}$, and $\\|x\\|$. Express your final answer as a single simplified analytic expression. No rounding is required and no units are involved.",
            "solution": "The problem asks for a closed-form expression for the difference between the cosine-based and correlation-based RDM entries for two vectors $x, y \\in \\mathbb{R}^p$, where $y$ is a linear transformation of $x$. The difference is denoted by $\\Delta \\equiv d_{\\mathrm{cos}}(x,y) - d_{\\mathrm{corr}}(x,y)$.\n\nThe definitions for the dissimilarities are given as $d = 1 - s$, where $s$ is the corresponding similarity measure.\nTherefore,\n$$ \\Delta = \\left(1 - s_{\\mathrm{cos}}(x,y)\\right) - \\left(1 - \\rho(x,y)\\right) = \\rho(x,y) - s_{\\mathrm{cos}}(x,y) $$\nwhere $s_{\\mathrm{cos}}(x,y)$ is the cosine similarity and $\\rho(x,y)$ is the Pearson correlation coefficient. We will derive each similarity term separately.\n\nFirst, we compute the Pearson correlation coefficient, $\\rho(x,y)$. The definition of the Pearson correlation between two vectors $x$ and $y$ of dimension $p$ is the cosine similarity between their mean-centered versions. Let $x_c = x - \\bar{x}\\mathbf{1}$ and $y_c = y - \\bar{y}\\mathbf{1}$ be the centered vectors, where $\\bar{x}$ and $\\bar{y}$ are the respective means and $\\mathbf{1}$ is the all-ones vector. Then,\n$$ \\rho(x,y) = \\frac{x_c \\cdot y_c}{\\|x_c\\| \\|y_c\\|} $$\nThe vector $y$ is given by the linear transformation $y = \\gamma x + \\beta \\mathbf{1}$, where $\\gamma \\in \\mathbb{R} \\setminus \\{0\\}$ and $\\beta \\in \\mathbb{R}$.\nThe mean of $y$ is:\n$$ \\bar{y} = \\frac{1}{p} \\sum_{i=1}^{p} y_i = \\frac{1}{p} \\sum_{i=1}^{p} (\\gamma x_i + \\beta) = \\gamma \\left(\\frac{1}{p} \\sum_{i=1}^{p} x_i\\right) + \\frac{1}{p} (p\\beta) = \\gamma \\bar{x} + \\beta $$\nNow we find the centered vector $y_c$:\n$$ y_c = y - \\bar{y}\\mathbf{1} = (\\gamma x + \\beta \\mathbf{1}) - (\\gamma \\bar{x} + \\beta)\\mathbf{1} = \\gamma x + \\beta \\mathbf{1} - \\gamma \\bar{x}\\mathbf{1} - \\beta \\mathbf{1} = \\gamma (x - \\bar{x}\\mathbf{1}) = \\gamma x_c $$\nThis shows that the additive offset $\\beta$ is removed by mean-centering. We can now compute the terms for $\\rho(x,y)$.\nThe dot product in the numerator is:\n$$ x_c \\cdot y_c = x_c \\cdot (\\gamma x_c) = \\gamma (x_c \\cdot x_c) = \\gamma \\|x_c\\|^2 = \\gamma \\|x - \\bar{x}\\mathbf{1}\\|^2 $$\nThe norms in the denominator are:\n$$ \\|x_c\\| \\|y_c\\| = \\|x - \\bar{x}\\mathbf{1}\\| \\|\\gamma (x - \\bar{x}\\mathbf{1})\\| = \\|x - \\bar{x}\\mathbf{1}\\| |\\gamma| \\|x - \\bar{x}\\mathbf{1}\\| = |\\gamma| \\|x - \\bar{x}\\mathbf{1}\\|^2 $$\nSubstituting these into the expression for $\\rho(x,y)$:\n$$ \\rho(x,y) = \\frac{\\gamma \\|x - \\bar{x}\\mathbf{1}\\|^2}{|\\gamma| \\|x - \\bar{x}\\mathbf{1}\\|^2} $$\nThe problem states that $\\|x - \\bar{x}\\mathbf{1}\\| \\neq 0$, so we can cancel this term. Since $\\gamma \\neq 0$ is also given, we get:\n$$ \\rho(x,y) = \\frac{\\gamma}{|\\gamma|} = \\mathrm{sgn}(\\gamma) $$\nThus, the correlation distance is $d_{\\mathrm{corr}}(x,y) = 1 - \\rho(x,y) = 1 - \\mathrm{sgn}(\\gamma)$.\n\nNext, we compute the cosine similarity, $s_{\\mathrm{cos}}(x,y)$.\n$$ s_{\\mathrm{cos}}(x,y) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|} $$\nThe dot product in the numerator is:\n$$ x \\cdot y = x \\cdot (\\gamma x + \\beta \\mathbf{1}) = \\gamma (x \\cdot x) + \\beta (x \\cdot \\mathbf{1}) $$\nWe know that $x \\cdot x = \\|x\\|^2$ and $x \\cdot \\mathbf{1} = \\sum_{i=1}^{p} x_i = p \\bar{x}$.\nSo, the numerator is $\\gamma \\|x\\|^2 + p\\beta\\bar{x}$.\nFor the denominator, we need $\\|y\\|$. We compute its squared value first:\n$$ \\|y\\|^2 = y \\cdot y = (\\gamma x + \\beta \\mathbf{1}) \\cdot (\\gamma x + \\beta \\mathbf{1}) = \\gamma^2 (x \\cdot x) + 2\\gamma\\beta (x \\cdot \\mathbf{1}) + \\beta^2 (\\mathbf{1} \\cdot \\mathbf{1}) $$\nUsing $x \\cdot x = \\|x\\|^2$, $x \\cdot \\mathbf{1} = p\\bar{x}$, and $\\mathbf{1} \\cdot \\mathbf{1} = p$, we get:\n$$ \\|y\\|^2 = \\gamma^2 \\|x\\|^2 + 2p\\gamma\\beta\\bar{x} + p\\beta^2 $$\nTherefore, $\\|y\\| = \\sqrt{\\gamma^2 \\|x\\|^2 + 2p\\gamma\\beta\\bar{x} + p\\beta^2}$.\nThe cosine similarity is:\n$$ s_{\\mathrm{cos}}(x,y) = \\frac{\\gamma \\|x\\|^2 + p\\beta\\bar{x}}{\\|x\\| \\sqrt{\\gamma^2 \\|x\\|^2 + 2p\\gamma\\beta\\bar{x} + p\\beta^2}} $$\nThe cosine distance is $d_{\\mathrm{cos}}(x,y) = 1 - s_{\\mathrm{cos}}(x,y)$.\n\nFinally, we compute the difference $\\Delta$:\n$$ \\Delta = \\rho(x,y) - s_{\\mathrm{cos}}(x,y) = \\mathrm{sgn}(\\gamma) - \\frac{\\gamma \\|x\\|^2 + p\\beta\\bar{x}}{\\|x\\| \\sqrt{\\gamma^2 \\|x\\|^2 + 2p\\gamma\\beta\\bar{x} + p\\beta^2}} $$\nThis expression is in terms of the required variables $p$, $\\gamma$, $\\beta$, $\\bar{x}$, and $\\|x\\|$. It represents the final closed-form analytic expression for the difference between the two RDM entries.\nThe sign function $\\mathrm{sgn}(\\gamma)$ is defined as $1$ if $\\gamma > 0$, $-1$ if $\\gamma  0$, and is undefined at $\\gamma=0$, but the problem specifies $\\gamma \\neq 0$.\n\nThe final expression is:\n$$ \\Delta = \\mathrm{sgn}(\\gamma) - \\frac{\\gamma \\|x\\|^2 + p \\beta \\bar{x}}{\\|x\\| \\sqrt{\\gamma^2 \\|x\\|^2 + 2p\\gamma\\beta\\bar{x} + p\\beta^2}} $$\nThis is the simplified result.",
            "answer": "$$\\boxed{\\mathrm{sgn}(\\gamma) - \\frac{\\gamma \\|x\\|^2 + p \\beta \\bar{x}}{\\|x\\| \\sqrt{\\gamma^2 \\|x\\|^2 + 2p\\gamma\\beta\\bar{x} + p\\beta^2}}}$$"
        },
        {
            "introduction": "This exercise synthesizes several core concepts into a complete, practical workflow, guiding you from partitioned brain data to a final brain-model comparison. You will apply advanced techniques such as shrinkage regularization for noise covariance and compute cross-validated Mahalanobis (\"crossnobis\") distances, which are designed to provide unbiased estimates of dissimilarity. By constructing both a brain RDM and a model RDM and then correlating them, this practice provides a tangible, step-by-step experience of a modern RSA pipeline. ",
            "id": "4147090",
            "problem": "A laboratory seeks to compare the representational geometry of brain responses with a candidate computational model. The brain responses were measured across two ($2$) independent data partitions (e.g., split-half runs), each providing mean response patterns across $M=2$ voxels for $C=3$ stimulus conditions. Let the mean pattern vectors in partition $\\mathsf{A}$ be\n$\\mathbf{x}_{1}^{\\mathsf{A}} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $\\mathbf{x}_{2}^{\\mathsf{A}} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, and $\\mathbf{x}_{3}^{\\mathsf{A}} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$,\nand in partition $\\mathsf{B}$ be\n$\\mathbf{x}_{1}^{\\mathsf{B}} = \\begin{pmatrix}0.9 \\\\ 0.1\\end{pmatrix}$, $\\mathbf{x}_{2}^{\\mathsf{B}} = \\begin{pmatrix}0.1 \\\\ 0.9\\end{pmatrix}$, and $\\mathbf{x}_{3}^{\\mathsf{B}} = \\begin{pmatrix}1.05 \\\\ 0.95\\end{pmatrix}$.\nFrom trial-level residuals pooled across the two partitions, the sample noise covariance is estimated as\n$\\hat{\\Sigma} = \\begin{pmatrix}2  1 \\\\ 1  2\\end{pmatrix}$.\nTo improve estimation stability, the laboratory applies a shrinkage estimator of the covariance,\n$\\hat{\\Sigma}_{\\lambda} = (1-\\lambda)\\,\\hat{\\Sigma} + \\lambda\\,I$,\nwhere $I$ is the identity matrix and $\\lambda \\in [0,1]$ is the shrinkage parameter.\n\nStarting from the standard definition of the Representational Dissimilarity Matrix (RDM), where each off-diagonal entry quantifies the dissimilarity between a pair of conditions, and from the principle that Gaussian noise with covariance $\\Sigma$ motivates noise-whitened distances, derive the cross-validated noise-adjusted Mahalanobis dissimilarities (the crossnobis distances) for the three condition pairs $(1,2)$, $(1,3)$, and $(2,3)$ using $\\hat{\\Sigma}_{\\lambda}$ as the noise covariance surrogate. Use $\\lambda = 0.3$. Then, construct the brain RDM entries as these three distances.\n\nThe candidate model specifies representational vectors $\\mathbf{m}_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $\\mathbf{m}_{2} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, and $\\mathbf{m}_{3} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$, and defines its RDM entries as the squared Euclidean distances between model vectors. Compute the Pearson correlation between the brain RDM (crossnobis distances) and the model RDM (squared Euclidean distances) for the three pairs. Round your final correlation to four significant figures and express it as a decimal number.\n\nFinally, discuss the bias-variance trade-off introduced by the shrinkage parameter $\\lambda$ in the resulting RDM, explaining under what circumstances shrinkage reduces variance at the cost of bias and how this manifests in the computed dissimilarities.",
            "solution": "The objective is to compute the Pearson correlation between a brain Representational Dissimilarity Matrix (RDM) and a model RDM, and to discuss the bias-variance trade-off of the shrinkage parameter used in the brain RDM calculation.\n\nFirst, we construct the brain RDM. This requires calculating the cross-validated noise-adjusted Mahalanobis dissimilarities (crossnobis distances) for the $3$ condition pairs. The calculation is based on a shrinkage estimate of the noise covariance matrix, $\\hat{\\Sigma}_{\\lambda}$.\n\nThe given sample covariance matrix is $\\hat{\\Sigma} = \\begin{pmatrix}2  1 \\\\ 1  2\\end{pmatrix}$, and the shrinkage parameter is $\\lambda = 0.3$. The identity matrix is $I = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}$. The shrinkage-regularized covariance matrix, $\\hat{\\Sigma}_{\\lambda}$, is given by:\n$$ \\hat{\\Sigma}_{\\lambda} = (1-\\lambda)\\,\\hat{\\Sigma} + \\lambda\\,I $$\nSubstituting the given values:\n$$ \\hat{\\Sigma}_{0.3} = (1-0.3) \\begin{pmatrix}2  1 \\\\ 1  2\\end{pmatrix} + 0.3 \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} $$\n$$ \\hat{\\Sigma}_{0.3} = 0.7 \\begin{pmatrix}2  1 \\\\ 1  2\\end{pmatrix} + 0.3 \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} = \\begin{pmatrix}1.4  0.7 \\\\ 0.7  1.4\\end{pmatrix} + \\begin{pmatrix}0.3  0 \\\\ 0  0.3\\end{pmatrix} = \\begin{pmatrix}1.7  0.7 \\\\ 0.7  1.7\\end{pmatrix} $$\nTo compute the Mahalanobis distance, we need the inverse of this matrix, $\\hat{\\Sigma}_{0.3}^{-1}$.\nThe determinant is $\\det(\\hat{\\Sigma}_{0.3}) = (1.7)(1.7) - (0.7)(0.7) = 2.89 - 0.49 = 2.4$.\nThe inverse is:\n$$ \\hat{\\Sigma}_{0.3}^{-1} = \\frac{1}{2.4} \\begin{pmatrix}1.7  -0.7 \\\\ -0.7  1.7\\end{pmatrix} $$\nThe crossnobis dissimilarity, $d^2(i,j)$, between conditions $i$ and $j$ is defined as:\n$$ d^2(i,j) = (\\mathbf{x}_i^{\\mathsf{A}} - \\mathbf{x}_j^{\\mathsf{A}})^T \\hat{\\Sigma}_{0.3}^{-1} (\\mathbf{x}_i^{\\mathsf{B}} - \\mathbf{x}_j^{\\mathsf{B}}) $$\nLet's compute the vector differences for each partition:\nFor partition $\\mathsf{A}$:\n$\\Delta \\mathbf{x}_{12}^{\\mathsf{A}} = \\mathbf{x}_1^{\\mathsf{A}} - \\mathbf{x}_2^{\\mathsf{A}} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$\n$\\Delta \\mathbf{x}_{13}^{\\mathsf{A}} = \\mathbf{x}_1^{\\mathsf{A}} - \\mathbf{x}_3^{\\mathsf{A}} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1\\end{pmatrix}$\n$\\Delta \\mathbf{x}_{23}^{\\mathsf{A}} = \\mathbf{x}_2^{\\mathsf{A}} - \\mathbf{x}_3^{\\mathsf{A}} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}-1 \\\\ 0\\end{pmatrix}$\n\nFor partition $\\mathsf{B}$:\n$\\Delta \\mathbf{x}_{12}^{\\mathsf{B}} = \\mathbf{x}_1^{\\mathsf{B}} - \\mathbf{x}_2^{\\mathsf{B}} = \\begin{pmatrix}0.9 \\\\ 0.1\\end{pmatrix} - \\begin{pmatrix}0.1 \\\\ 0.9\\end{pmatrix} = \\begin{pmatrix}0.8 \\\\ -0.8\\end{pmatrix}$\n$\\Delta \\mathbf{x}_{13}^{\\mathsf{B}} = \\mathbf{x}_1^{\\mathsf{B}} - \\mathbf{x}_3^{\\mathsf{B}} = \\begin{pmatrix}0.9 \\\\ 0.1\\end{pmatrix} - \\begin{pmatrix}1.05 \\\\ 0.95\\end{pmatrix} = \\begin{pmatrix}-0.15 \\\\ -0.85\\end{pmatrix}$\n$\\Delta \\mathbf{x}_{23}^{\\mathsf{B}} = \\mathbf{x}_2^{\\mathsf{B}} - \\mathbf{x}_3^{\\mathsf{B}} = \\begin{pmatrix}0.1 \\\\ 0.9\\end{pmatrix} - \\begin{pmatrix}1.05 \\\\ 0.95\\end{pmatrix} = \\begin{pmatrix}-0.95 \\\\ -0.05\\end{pmatrix}$\n\nNow we compute the dissimilarities:\n$d_{\\text{brain}}^2(1,2) = \\begin{pmatrix}1  -1\\end{pmatrix} \\frac{1}{2.4} \\begin{pmatrix}1.7  -0.7 \\\\ -0.7  1.7\\end{pmatrix} \\begin{pmatrix}0.8 \\\\ -0.8\\end{pmatrix} = \\frac{1}{2.4} \\begin{pmatrix}1  -1\\end{pmatrix} \\begin{pmatrix}1.92 \\\\ -1.92\\end{pmatrix} = \\frac{1.92 - (-1.92)}{2.4} = \\frac{3.84}{2.4} = 1.6$\n\n$d_{\\text{brain}}^2(1,3) = \\begin{pmatrix}0  -1\\end{pmatrix} \\frac{1}{2.4} \\begin{pmatrix}1.7  -0.7 \\\\ -0.7  1.7\\end{pmatrix} \\begin{pmatrix}-0.15 \\\\ -0.85\\end{pmatrix} = \\frac{1}{2.4} \\begin{pmatrix}0  -1\\end{pmatrix} \\begin{pmatrix}0.34 \\\\ -1.34\\end{pmatrix} = \\frac{1.34}{2.4} = \\frac{67}{120}$\n\n$d_{\\text{brain}}^2(2,3) = \\begin{pmatrix}-1  0\\end{pmatrix} \\frac{1}{2.4} \\begin{pmatrix}1.7  -0.7 \\\\ -0.7  1.7\\end{pmatrix} \\begin{pmatrix}-0.95 \\\\ -0.05\\end{pmatrix} = \\frac{1}{2.4} \\begin{pmatrix}-1  0\\end{pmatrix} \\begin{pmatrix}-1.58 \\\\ 0.58\\end{pmatrix} = \\frac{1.58}{2.4} = \\frac{79}{120}$\n\nThe brain RDM entries form the vector $\\mathbf{d}_{\\text{brain}} = \\begin{pmatrix} 1.6  \\frac{67}{120}  \\frac{79}{120} \\end{pmatrix}^T$.\n\nSecond, we construct the model RDM. The entries are the squared Euclidean distances between the model vectors $\\mathbf{m}_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $\\mathbf{m}_{2} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, and $\\mathbf{m}_{3} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n$d_{\\text{model}}^2(1,2) = ||\\mathbf{m}_1 - \\mathbf{m}_2||^2 = ||\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}||^2 = 1^2 + (-1)^2 = 2$\n$d_{\\text{model}}^2(1,3) = ||\\mathbf{m}_1 - \\mathbf{m}_3||^2 = ||\\begin{pmatrix}0 \\\\ -1\\end{pmatrix}||^2 = 0^2 + (-1)^2 = 1$\n$d_{\\text{model}}^2(2,3) = ||\\mathbf{m}_2 - \\mathbf{m}_3||^2 = ||\\begin{pmatrix}-1 \\\\ 0\\end{pmatrix}||^2 = (-1)^2 + 0^2 = 1$\nThe model RDM entries form the vector $\\mathbf{d}_{\\text{model}} = \\begin{pmatrix} 2  1  1 \\end{pmatrix}^T$.\n\nThird, we compute the Pearson correlation, $r$, between $\\mathbf{d}_{\\text{brain}}$ and $\\mathbf{d}_{\\text{model}}$. Let $\\mathbf{u} = \\mathbf{d}_{\\text{brain}}$ and $\\mathbf{v} = \\mathbf{d}_{\\text{model}}$.\nThe means are:\n$\\bar{u} = \\frac{1}{3} (1.6 + \\frac{67}{120} + \\frac{79}{120}) = \\frac{1}{3} (\\frac{192}{120} + \\frac{146}{120}) = \\frac{1}{3} \\frac{338}{120} = \\frac{169}{180}$\n$\\bar{v} = \\frac{1}{3} (2+1+1) = \\frac{4}{3}$\n\nThe centered vectors are:\n$u - \\bar{u} = \\begin{pmatrix} 1.6 - \\frac{169}{180} \\\\ \\frac{67}{120} - \\frac{169}{180} \\\\ \\frac{79}{120} - \\frac{169}{180} \\end{pmatrix} = \\begin{pmatrix} \\frac{288-169}{180} \\\\ \\frac{201-338}{360} \\\\ \\frac{237-338}{360} \\end{pmatrix} = \\begin{pmatrix} \\frac{119}{180} \\\\ -\\frac{137}{360} \\\\ -\\frac{101}{360} \\end{pmatrix}$\n$v - \\bar{v} = \\begin{pmatrix} 2 - \\frac{4}{3} \\\\ 1 - \\frac{4}{3} \\\\ 1 - \\frac{4}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$\n\nThe numerator of the correlation formula is the sum of products of deviations:\n$\\sum (u_i - \\bar{u})(v_i - \\bar{v}) = (\\frac{119}{180})(\\frac{2}{3}) + (-\\frac{137}{360})(-\\frac{1}{3}) + (-\\frac{101}{360})(-\\frac{1}{3})$\n$= \\frac{238}{540} + \\frac{137}{1080} + \\frac{101}{1080} = \\frac{476}{1080} + \\frac{238}{1080} = \\frac{714}{1080} = \\frac{119}{180}$\n\nThe denominator terms are the square roots of the sum of squared deviations:\n$\\sum (u_i - \\bar{u})^2 = (\\frac{119}{180})^2 + (\\frac{-137}{360})^2 + (\\frac{-101}{360})^2 = \\frac{14161}{32400} + \\frac{18769}{129600} + \\frac{10201}{129600}$\n$= \\frac{4 \\times 14161 + 18769 + 10201}{129600} = \\frac{56644 + 18769 + 10201}{129600} = \\frac{85614}{129600} = \\frac{14269}{21600}$\n$\\sum (v_i - \\bar{v})^2 = (\\frac{2}{3})^2 + (-\\frac{1}{3})^2 + (-\\frac{1}{3})^2 = \\frac{4}{9} + \\frac{1}{9} + \\frac{1}{9} = \\frac{6}{9} = \\frac{2}{3}$\n\nThe Pearson correlation coefficient is:\n$$ r = \\frac{\\frac{119}{180}}{\\sqrt{\\frac{14269}{21600}} \\sqrt{\\frac{2}{3}}} = \\frac{\\frac{119}{180}}{\\sqrt{\\frac{14269 \\times 2}{21600 \\times 3}}} = \\frac{\\frac{119}{180}}{\\sqrt{\\frac{28538}{64800}}} $$\nTo simplify, we can compute $r^2$:\n$$ r^2 = \\frac{(\\frac{119}{180})^2}{\\frac{28538}{64800}} = \\frac{\\frac{14161}{32400}}{\\frac{28538}{64800}} = \\frac{14161}{32400} \\frac{64800}{28538} = \\frac{14161 \\times 2}{28538} = \\frac{28322}{28538} $$\n$$ r = \\sqrt{\\frac{28322}{28538}} \\approx \\sqrt{0.992429399} \\approx 0.9962075 $$\nRounding to four significant figures, the correlation is $0.9962$.\n\nFinally, we discuss the bias-variance trade-off introduced by the shrinkage parameter $\\lambda$.\nThe shrinkage estimator for the covariance matrix, $\\hat{\\Sigma}_{\\lambda} = (1-\\lambda)\\hat{\\Sigma} + \\lambda I$, combines the sample covariance $\\hat{\\Sigma}$ with a scaled identity matrix $I$. This technique is a form of regularization.\n\nBias: The sample covariance $\\hat{\\Sigma}$ is an (asymptotically) unbiased estimator of the true noise covariance $\\Sigma$. The identity matrix $I$ is generally a biased estimator of $\\Sigma$, unless the true noise is spherical (i.e., $\\Sigma \\propto I$). By choosing $\\lambda > 0$, we introduce bias into our covariance estimate. The expected value of the estimator is $E[\\hat{\\Sigma}_{\\lambda}] = (1-\\lambda)\\Sigma + \\lambda I$. The bias is $E[\\hat{\\Sigma}_{\\lambda}] - \\Sigma = \\lambda(I - \\Sigma)$. This bias increases with $\\lambda$ and with the extent to which the true covariance structure deviates from the identity matrix (e.g., strong off-diagonal correlations). In our problem, $\\hat{\\Sigma}$ has non-zero off-diagonals, so using $\\lambda=0.3$ introduces a bias that underestimates the true covariance between the voxels.\n\nVariance: The sample covariance $\\hat{\\Sigma}$ can have high variance, meaning its entries can fluctuate substantially across different samples of data, especially if the number of data points used for estimation is small relative to the number of dimensions (voxels). In contrast, the identity matrix $I$ is a constant with zero variance. The variance of the shrinkage estimator is $\\text{Var}(\\hat{\\Sigma}_{\\lambda}) = (1-\\lambda)^2 \\text{Var}(\\hat{\\Sigma})$. As $\\lambda$ increases from $0$ to $1$, the variance of the estimator decreases, making the estimate more stable.\n\nTrade-off: The use of shrinkage embodies a classic bias-variance trade-off. By increasing $\\lambda$, we accept more bias in our covariance estimate in exchange for a reduction in its variance. For noisy or high-dimensional data, the large reduction in variance can outweigh the increased bias, leading to a lower overall mean squared error and a more robust estimate of the true covariance. An optimal $\\lambda$ would balance these two competing factors.\n\nManifestation in dissimilarities: This trade-off directly affects the computed crossnobis distances, $d^2 = \\Delta \\mathbf{x}^{\\mathsf{A} T} \\hat{\\Sigma}_{\\lambda}^{-1} \\Delta \\mathbf{x}^{\\mathsf{B}}$. The matrix $\\hat{\\Sigma}_{\\lambda}^{-1}$ acts as a noise-whitening transformation. If $\\lambda=0$, we use $\\hat{\\Sigma}^{-1}$, which provides an unbiased but potentially unstable whitening. If $\\hat{\\Sigma}$ is ill-conditioned, $\\hat{\\Sigma}^{-1}$ will have huge entries, excessively amplifying noise in certain dimensions and leading to highly variable (high variance) dissimilarity estimates. If $\\lambda=1$, we use $I^{-1}=I$, which assumes noise is uncorrelated and of equal variance across voxels. This is a strong, biased assumption, but it results in a very stable (low variance) dissimilarity calculation (the cross-validated dot product). Using an intermediate $\\lambda$ like $0.3$ regularizes the matrix inversion, making the dissimilarity estimates more stable than the pure Mahalanobis distance ($\\lambda=0$) but at the cost of imperfect noise whitening (bias). The hope is that this leads to a more reliable RDM that better reflects the true representational geometry by not being overwhelmed by sampling noise.",
            "answer": "$$ \\boxed{0.9962} $$"
        },
        {
            "introduction": "A key finding in RSA is only as strong as its statistical validation. Given the complex dependency structure within an RDM, how can we robustly test if the observed similarity to a model is more than chance? This exercise addresses this critical question by focusing on non-parametric permutation testing. You will evaluate several potential shuffling schemes to understand why only a specific method—permuting stimulus labels—correctly respects the structure of the RDM and provides a valid test of the null hypothesis. ",
            "id": "4147113",
            "problem": "An investigator seeks to test whether a computational vision model captures the representational geometry of a human ventral visual cortex region measured with functional Magnetic Resonance Imaging (fMRI). For each of $n$ stimuli, the investigator computes a brain response vector and a model feature vector, then constructs two $n \\times n$ Representational Dissimilarity Matrices (RDMs), denoted $D^{(b)}$ for brain and $D^{(m)}$ for model, where $D^{(b)}_{ij}$ and $D^{(m)}_{ij}$ quantify the dissimilarity between stimulus $i$ and stimulus $j$ in brain and model feature spaces, respectively. The investigator summarizes the concordance between $D^{(b)}$ and $D^{(m)}$ using a Spearman rank correlation statistic $t$, defined as the Spearman correlation between the vectors formed by the upper-triangular (excluding diagonal) entries of each RDM:\n$$\nt \\;=\\; \\rho_{S}\\!\\Big(\\operatorname{vec}_{\\triangle}\\big(D^{(b)}\\big),\\, \\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)\\Big).\n$$\nTo assess statistical significance without distributional assumptions, the investigator intends to use a permutation test that shuffles stimulus labels, leveraging the principle of exchangeability under the null hypothesis.\n\nStarting from the following fundamental definitions and facts:\n- An RDM is a symmetric matrix of pairwise dissimilarities across stimuli, with $D_{ii}=0$ and $D_{ij}=D_{ji}$.\n- Under the null hypothesis that stimulus identities do not align between brain and model, stimulus labels are exchangeable; equivalently, any permutation of stimulus labels should not change the joint distribution that links $D^{(b)}$ and $D^{(m)}$.\n- A valid permutation for an RDM must permute stimulus labels consistently across both dimensions (rows and columns), preserving symmetry and the structural dependencies among pairwise entries.\n\nWhich option correctly defines an appropriate permutation test for the Spearman correlation $t$ by shuffling stimulus labels and precisely states the null hypothesis being tested?\n\nA. Apply a random permutation $\\pi$ of the $n$ stimulus labels to the brain RDM by forming $D^{(b)}_{\\pi} = P D^{(b)} P^{\\top}$, where $P$ is the permutation matrix corresponding to $\\pi$. For each $\\pi$, compute\n$$\nt_{\\pi} \\;=\\; \\rho_{S}\\!\\Big(\\operatorname{vec}_{\\triangle}\\big(D^{(b)}_{\\pi}\\big),\\, \\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)\\Big),\n$$\nand use the empirical distribution of $\\{t_{\\pi}\\}$ to assess $t$. The null hypothesis is that stimulus labels are exchangeable between brain and model, i.e., the mapping from stimulus identities in $D^{(b)}$ to those in $D^{(m)}$ carries no information, so the population Spearman correlation between corresponding entries is $0$ and the distribution of $t$ is invariant to $\\pi$.\n\nB. Randomly shuffle the entries of $\\operatorname{vec}_{\\triangle}\\big(D^{(b)}\\big)$ without regard to which pairs they came from, then correlate this shuffled vector with $\\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)$. The null hypothesis is that the individual pairwise dissimilarities in the brain are independent and identically distributed, so the order of entries in the brain RDM is irrelevant.\n\nC. Randomly permute trial repetitions across stimuli in the fMRI data before computing $D^{(b)}$, leaving $D^{(m)}$ unchanged, and then compute the correlation $t$ on the resulting matrices. The null hypothesis is that trial identities are exchangeable within and across stimuli, so any observed correlation must arise from trial-level noise rather than stimulus-level structure.\n\nD. Independently permute the row and column labels of $D^{(b)}$ by applying two unrelated permutations $\\pi_{r}$ and $\\pi_{c}$ to form a matrix $D'^{\\,(b)}$ with entries $D'^{\\,(b)}_{ij} = D^{(b)}_{\\pi_{r}(i)\\,\\pi_{c}(j)}$, then correlate $\\operatorname{vec}_{\\triangle}\\big(D'^{\\,(b)}\\big)$ with $\\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)$. The null hypothesis is that the brain and model RDMs are independent across rows and columns separately, so breaking symmetry does not affect the expected correlation.",
            "solution": "### Step 1: Extract Givens\nThe problem statement provides the following information and definitions:\n- There are $n$ stimuli.\n- Two $n \\times n$ Representational Dissimilarity Matrices (RDMs) are given: $D^{(b)}$ (brain data) and $D^{(m)}$ (model).\n- The entries $D^{(b)}_{ij}$ and $D^{(m)}_{ij}$ represent the dissimilarity between stimulus $i$ and stimulus $j$.\n- A test statistic $t$ is defined as the Spearman rank correlation between the vectorized upper-triangular parts of the RDMs:\n$$\nt \\;=\\; \\rho_{S}\\!\\Big(\\operatorname{vec}_{\\triangle}\\big(D^{(b)}\\big),\\, \\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)\\Big).\n$$\n- A permutation test is to be used to assess the statistical significance of $t$.\n- Fundamental properties of an RDM: It is a symmetric matrix ($D_{ij}=D_{ji}$) with a zero diagonal ($D_{ii}=0$).\n- Principle for the permutation test under the null hypothesis ($H_0$): Stimulus labels are exchangeable between brain and model.\n- Constraint on a valid permutation: It must permute stimulus labels consistently across rows and columns, preserving the RDM's symmetry and the structural dependencies among its entries.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a standard and important procedure in computational neuroscience known as Representational Similarity Analysis (RSA).\n- **Scientifically Grounded:** The entire setup is a cornerstone of modern systems and cognitive neuroscience. The use of RDMs to characterize representational geometry, the comparison between brain data (fMRI) and computational models, the choice of Spearman correlation to allow for non-linear but monotonic relationships, and the use of permutation tests for non-parametric inference are all well-established, standard practices. The problem is firmly grounded in scientific methodology.\n- **Well-Posed:** The question is precise. It asks for the correct permutation procedure to test a specific, well-defined null hypothesis for a given statistic ($t$). The provided principles and constraints allow for a unique and correct answer to be determined through logical and statistical reasoning.\n- **Objective:** The problem is described using formal mathematical and statistical language, free of ambiguity or subjectivity.\n\nThe problem is not unsound, incomplete, contradictory, unrealistic, or ill-posed. All terms are either formally defined ($t$, RDM) or standard in the field (permutation test, exchangeability).\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The solution process will proceed by deriving the correct permutation procedure from first principles and then evaluating each option.\n\n### Derivation and Option Analysis\n\nThe central task is to construct a null distribution for the statistic $t$ under the null hypothesis, $H_0$, that there is no systematic relationship between the representational geometry of the brain and that of the model. This hypothesis implies that the stimulus labels $\\{1, 2, \\dots, n\\}$ are not meaningfully aligned between $D^{(b)}$ and $D^{(m)}$. Any observed correlation is thus attributable to chance. The principle of exchangeability means that if $H_0$ is true, any permutation of the stimulus labels for one of the RDMs should not systematically change the resulting correlation with the other RDM.\n\nLet's formalize the act of \"permuting stimulus labels\" for an RDM, say $D^{(b)}$. An RDM is a matrix where both rows and columns are indexed by the same set of stimuli. If we apply a permutation $\\pi$ to the stimulus labels, the new dissimilarity between what are now labeled as stimulus '$i$' and stimulus '$j$' must be the old dissimilarity between stimulus $\\pi(i)$ and stimulus $\\pi(j)$. This is incorrect. The permutation re-assigns the labels. The new matrix, let's call it $D^{(b)}_{\\pi}$, should have at its $(i,j)$ position the dissimilarity corresponding to the stimuli that were moved to the $i$-th and $j$-th positions. Let $\\pi$ be a permutation of $\\{1, ..., n\\}$. The permutation matrix $P$ corresponding to $\\pi$ is often defined such that it permutes the rows of a matrix it multiplies from the left. For an RDM, we must permute the columns in the same way to maintain the correspondence of a stimulus with its row and column. This operation, a similarity transformation, is given by $D^{(b)}_{\\pi} = P D^{(b)} P^{\\top}$, where $P^{\\top} = P^{-1}$ because permutation matrices are orthogonal. This transformation simultaneously permutes the rows and columns, preserving the symmetry of the matrix ($ (P D P^{\\top})^{\\top} = (P^{\\top})^{\\top} D^{\\top} P^{\\top} = P D P^{\\top}$ since $D$ is symmetric) and its zero diagonal. This correctly implements the shuffling of stimulus labels while preserving the internal dependency structure of the RDM, as required by the problem's constraints.\n\nThe permutation test procedure is therefore:\n1.  Calculate the observed statistic, $t$, from the original matrices $D^{(b)}$ and $D^{(m)}$.\n2.  Repeat for a large number of iterations:\n    a. Generate a random permutation $\\pi$ of the $n$ stimulus labels.\n    b. Construct the corresponding permutation matrix $P$.\n    c. Apply the permutation to one of the RDMs, e.g., $D^{(b)}_{\\pi} = P D^{(b)} P^{\\top}$.\n    d. Calculate the statistic on the permuted data: $t_{\\pi} = \\rho_{S}(\\operatorname{vec}_{\\triangle}(D^{(b)}_{\\pi}), \\operatorname{vec}_{\\triangle}(D^{(m)}))$.\n3.  The collection of $\\{t_{\\pi}\\}$ values forms the empirical null distribution. The p-value is the proportion of these null values that are greater than or equal to the observed value $t$.\n\nNow we evaluate each option based on this correct procedure.\n\n**A. Apply a random permutation $\\pi$ of the $n$ stimulus labels to the brain RDM by forming $D^{(b)}_{\\pi} = P D^{(b)} P^{\\top}$, where $P$ is the permutation matrix corresponding to $\\pi$. For each $\\pi$, compute $t_{\\pi} \\;=\\; \\rho_{S}\\!\\Big(\\operatorname{vec}_{\\triangle}\\big(D^{(b)}_{\\pi}\\big),\\, \\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)\\Big)$, and use the empirical distribution of $\\{t_{\\pi}\\}$ to assess $t$. The null hypothesis is that stimulus labels are exchangeable between brain and model, i.e., the mapping from stimulus identities in $D^{(b)}$ to those in $D^{(m)}$ carries no information, so the population Spearman correlation between corresponding entries is $0$ and the distribution of $t$ is invariant to $\\pi$.**\n\nThis option precisely matches the derived correct procedure. The formula $D^{(b)}_{\\pi} = P D^{(b)} P^{\\top}$ is the correct matrix operation for relabeling stimuli. The subsequent computation of $t_{\\pi}$ and the formation of the null distribution are standard. The statement of the null hypothesis is accurate and comprehensive, correctly linking the concept of exchangeable labels to the lack of informational mapping between the two RDMs.\n\n**Verdict: Correct**\n\n**B. Randomly shuffle the entries of $\\operatorname{vec}_{\\triangle}\\big(D^{(b)}\\big)$ without regard to which pairs they came from, then correlate this shuffled vector with $\\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)$. The null hypothesis is that the individual pairwise dissimilarities in the brain are independent and identically distributed, so the order of entries in the brain RDM is irrelevant.**\n\nThis procedure is fundamentally flawed. The entries of an RDM are not independent. For instance, dissimilarities involving a common stimulus (e.g., $D_{ij}$ and $D_{ik}$) are structurally related. Shuffling the individual entries of $\\operatorname{vec}_{\\triangle}(D^{(b)})$ destroys this inherent structure of the representational space. The problem statement explicitly requires that a valid permutation must preserve \"the structural dependencies among pairwise entries.\" This procedure violates that critical constraint. It tests a much stronger and generally false null hypothesis of independence among all pairwise dissimilarities.\n\n**Verdict: Incorrect**\n\n**C. Randomly permute trial repetitions across stimuli in the fMRI data before computing $D^{(b)}$, leaving $D^{(m)}$ unchanged, and then compute the correlation $t$ on the resulting matrices. The null hypothesis is that trial identities are exchangeable within and across stimuli, so any observed correlation must arise from trial-level noise rather than stimulus-level structure.**\n\nThis procedure tests a different null hypothesis. It asks whether the brain responses contain any stimulus-specific information at all. By permuting trials across different stimuli before even computing the brain RDM, one erases any structure in $D^{(b)}$ that is systematically related to the stimuli. This is a valid test for asking \"Is there any representational structure in the brain data?\", but it is not the correct test for the primary question: \"Is the representational structure in the brain data similar to the structure in the model data?\". The problem is about comparing two given, structured RDMs.\n\n**Verdict: Incorrect**\n\n**D. Independently permute the row and column labels of $D^{(b)}$ by applying two unrelated permutations $\\pi_{r}$ and $\\pi_{c}$ to form a matrix $D'^{\\,(b)}$ with entries $D'^{\\,(b)}_{ij} = D^{(b)}_{\\pi_{r}(i)\\,\\pi_{c}(j)}$, then correlate $\\operatorname{vec}_{\\triangle}\\big(D'^{\\,(b)}\\big)$ with $\\operatorname{vec}_{\\triangle}\\big(D^{(m)}\\big)$. The null hypothesis is that the brain and model RDMs are independent across rows and columns separately, so breaking symmetry does not affect the expected correlation.**\n\nThis procedure is incorrect because it violates a fundamental property of RDMs. Applying two independent permutations to the rows and columns, $\\pi_r$ and $\\pi_c$, will in general destroy the symmetry of the matrix. The resulting matrix $D'^{\\,(b)}$ will not be an RDM, as $D'^{\\,(b)}_{ij} = D^{(b)}_{\\pi_{r}(i)\\,\\pi_{c}(j)}$ will not equal $D'^{\\,(b)}_{ji} = D^{(b)}_{\\pi_{r}(j)\\,\\pi_{c}(i)}$. The problem statement explicitly requires a valid permutation to \"preserv[e] symmetry\". This procedure fails this requirement.\n\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}