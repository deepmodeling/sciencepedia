{
    "hands_on_practices": [
        {
            "introduction": "Understanding multidimensional scaling begins with its classical formulation, which provides a direct analytical solution for embedding points in a Euclidean space. This exercise focuses on the core mechanics of classical MDS, where you will transform a given matrix of squared distances into a geometric configuration. By applying the double-centering technique to derive a Gram matrix and then reconstructing coordinates, you will gain a foundational understanding of how representational structure is recovered from dissimilarity data .",
            "id": "4179052",
            "problem": "A laboratory is analyzing the representational geometry of neural population responses to $3$ visual stimuli. For each stimulus, the trial-averaged response is a vector in a high-dimensional feature space, and representational dissimilarities are measured as squared Euclidean distances between these vectors. The resulting $3 \\times 3$ Representational Dissimilarity Matrix (RDM) of squared distances, denoted by $\\Delta$, is\n$$\n\\Delta \\;=\\; \\begin{pmatrix}\n0  4  9 \\\\\n4  0  13 \\\\\n9  13  0\n\\end{pmatrix}.\n$$\nUsing classical Multidimensional Scaling (MDS), construct a two-dimensional embedding of the three stimuli by: (i) performing double-centering of $\\Delta$ to obtain a Gram matrix, (ii) computing the eigendecomposition, and (iii) reconstructing the coordinates in a two-dimensional space. Treat the configuration equivalence up to rigid transformations as appropriate for classical MDS. From the resulting two-dimensional configuration, compute the area of the triangle formed by the three embedded points as a scalar measure of the spread of the representational geometry.\n\nProvide your final answer as a single real number. If you recognize that an exact value can be obtained, report the exact value without rounding. Do not include any units in your final answer.",
            "solution": "Classical Multidimensional Scaling (MDS) begins from the definition that squared Euclidean distances between points in an unknown Euclidean space can be transformed via double-centering into a Gram matrix whose entries are inner products of centered coordinates. Specifically, let $\\Delta$ be the $n \\times n$ matrix of squared distances with $n = 3$, and let $J = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ be the centering matrix, where $I$ is the identity matrix and $\\mathbf{1}$ is the all-ones vector. The Gram matrix $B$ is obtained by\n$$\nB \\;=\\; -\\frac{1}{2}\\, J \\,\\Delta\\, J.\n$$\nEquivalently, in entrywise form using row means and grand mean, we may write\n$$\nB_{ij} \\;=\\; -\\frac{1}{2}\\Big(\\Delta_{ij} - \\bar{\\Delta}_{i\\cdot} - \\bar{\\Delta}_{\\cdot j} + \\bar{\\Delta}_{\\cdot\\cdot}\\Big),\n$$\nwhere $\\bar{\\Delta}_{i\\cdot} = \\frac{1}{n}\\sum_{k=1}^{n}\\Delta_{ik}$, $\\bar{\\Delta}_{\\cdot j} = \\frac{1}{n}\\sum_{k=1}^{n}\\Delta_{kj}$, and $\\bar{\\Delta}_{\\cdot\\cdot} = \\frac{1}{n^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\Delta_{ij}$.\n\nWe first compute the row means and the grand mean. The row sums are\n$$\n\\sum_{j=1}^{3}\\Delta_{1j} \\;=\\; 0+4+9 \\;=\\; 13,\\quad\n\\sum_{j=1}^{3}\\Delta_{2j} \\;=\\; 4+0+13 \\;=\\; 17,\\quad\n\\sum_{j=1}^{3}\\Delta_{3j} \\;=\\; 9+13+0 \\;=\\; 22,\n$$\nso the row means are\n$$\n\\bar{\\Delta}_{1\\cdot} \\;=\\; \\frac{13}{3},\\quad\n\\bar{\\Delta}_{2\\cdot} \\;=\\; \\frac{17}{3},\\quad\n\\bar{\\Delta}_{3\\cdot} \\;=\\; \\frac{22}{3}.\n$$\nThe grand sum is\n$$\n\\sum_{i=1}^{3}\\sum_{j=1}^{3}\\Delta_{ij} \\;=\\; 13 + 17 + 22 \\;=\\; 52,\n$$\nso the grand mean is\n$$\n\\bar{\\Delta}_{\\cdot\\cdot} \\;=\\; \\frac{52}{9}.\n$$\nApplying the entrywise double-centering formula yields\n\\begin{align*}\nB_{11} = -\\frac{1}{2}\\Big(0 - \\frac{13}{3} - \\frac{13}{3} + \\frac{52}{9}\\Big) \\;=\\; \\frac{13}{9}, \\\\\nB_{22} = -\\frac{1}{2}\\Big(0 - \\frac{17}{3} - \\frac{17}{3} + \\frac{52}{9}\\Big) \\;=\\; \\frac{25}{9}, \\\\\nB_{33} = -\\frac{1}{2}\\Big(0 - \\frac{22}{3} - \\frac{22}{3} + \\frac{52}{9}\\Big) \\;=\\; \\frac{40}{9}, \\\\\nB_{12} = -\\frac{1}{2}\\Big(4 - \\frac{13}{3} - \\frac{17}{3} + \\frac{52}{9}\\Big) \\;=\\; \\frac{1}{9}, \\\\\nB_{13} = -\\frac{1}{2}\\Big(9 - \\frac{13}{3} - \\frac{22}{3} + \\frac{52}{9}\\Big) \\;=\\; -\\frac{14}{9}, \\\\\nB_{23} = -\\frac{1}{2}\\Big(13 - \\frac{17}{3} - \\frac{22}{3} + \\frac{52}{9}\\Big) \\;=\\; -\\frac{26}{9}.\n\\end{align*}\nThus,\n$$\nB \\;=\\; \\begin{pmatrix}\n\\frac{13}{9}  \\frac{1}{9}  -\\frac{14}{9} \\\\\n\\frac{1}{9}  \\frac{25}{9}  -\\frac{26}{9} \\\\\n-\\frac{14}{9}  -\\frac{26}{9}  \\frac{40}{9}\n\\end{pmatrix}.\n$$\n\nBy construction, $B$ is a centered Gram matrix: it is symmetric, and it satisfies $B\\,\\mathbf{1} = \\mathbf{0}$, confirming that the corresponding coordinates are centered. Classical MDS proceeds by eigendecomposing $B$:\n$$\nB \\;=\\; V \\,\\Lambda\\, V^{\\top},\n$$\nwhere $\\Lambda$ is diagonal with eigenvalues in nonincreasing order and $V$ contains the corresponding orthonormal eigenvectors. Since there are $3$ points in a Euclidean space, the centered configuration has rank at most $2$, and one eigenvalue is $0$ with eigenvector proportional to $\\mathbf{1}$. The two positive eigenvalues span the two-dimensional embedding, and coordinate reconstruction yields a matrix $X$ of size $3 \\times 2$ whose rows are the embedded coordinates. The reconstruction satisfies\n$$\nX X^{\\top} \\;=\\; B,\n$$\nand can be written as $X = V_{2}\\,\\Lambda_{2}^{1/2}$, where $V_{2}$ contains the eigenvectors associated with the two positive eigenvalues and $\\Lambda_{2}$ is the corresponding $2 \\times 2$ diagonal matrix of eigenvalues.\n\nTo proceed concretely, notice that the given squared distances correspond to a right triangle with leg lengths $\\sqrt{4} = 2$ and $\\sqrt{9} = 3$, and hypotenuse length $\\sqrt{13}$. A convenient realization in $\\mathbb{R}^{2}$ is\n$$\nP_{1} = (0,0),\\quad P_{2} = (2,0),\\quad P_{3} = (0,3),\n$$\nwhich reproduces the given squared distances. Classical MDS yields coordinates that are centered and equivalent to this configuration up to rigid transformations (translations, rotations, reflections). The centroid of $(P_{1}, P_{2}, P_{3})$ is\n$$\nC \\;=\\; \\Big(\\frac{0+2+0}{3},\\, \\frac{0+0+3}{3}\\Big) \\;=\\; \\Big(\\frac{2}{3},\\, 1\\Big).\n$$\nSubtracting $C$ gives the centered coordinates\n\\begin{align*}\nQ_{1} = P_{1} - C \\;=\\; \\Big(-\\frac{2}{3},\\, -1\\Big), \\\\\nQ_{2} = P_{2} - C \\;=\\; \\Big(\\frac{4}{3},\\, -1\\Big), \\\\\nQ_{3} = P_{3} - C \\;=\\; \\Big(-\\frac{2}{3},\\, 2\\Big).\n\\end{align*}\nThese $Q_{i}$ form a $3 \\times 2$ matrix $X$ whose Gram $X X^{\\top}$ equals $B$:\n\\begin{align*}\nQ_{1}\\cdot Q_{1} = \\frac{4}{9} + 1 \\;=\\; \\frac{13}{9} \\;=\\; B_{11}, \\\\\nQ_{2}\\cdot Q_{2} = \\frac{16}{9} + 1 \\;=\\; \\frac{25}{9} \\;=\\; B_{22}, \\\\\nQ_{3}\\cdot Q_{3} = \\frac{4}{9} + 4 \\;=\\; \\frac{40}{9} \\;=\\; B_{33}, \\\\\nQ_{1}\\cdot Q_{2} = -\\frac{8}{9} + 1 \\;=\\; \\frac{1}{9} \\;=\\; B_{12}, \\\\\nQ_{1}\\cdot Q_{3} = \\frac{4}{9} - 2 \\;=\\; -\\frac{14}{9} \\;=\\; B_{13}, \\\\\nQ_{2}\\cdot Q_{3} = -\\frac{8}{9} - 2 \\;=\\; -\\frac{26}{9} \\;=\\; B_{23}.\n\\end{align*}\nThus, $X$ is a valid two-dimensional classical MDS embedding (up to rotation), and we may compute the area of the triangle formed by $(Q_{1}, Q_{2}, Q_{3})$. The area is invariant under translations and rotations, so it equals the area of the original configuration $(P_{1}, P_{2}, P_{3})$.\n\nUsing the determinant (shoelace) formula or the right-triangle property, the area is\n$$\n\\text{Area} \\;=\\; \\frac{1}{2}\\times (\\text{base}) \\times (\\text{height}) \\;=\\; \\frac{1}{2}\\times 2 \\times 3 \\;=\\; 3.\n$$\nTherefore, the area of the triangle formed by the two-dimensional classical MDS embedding of the given RDM is $3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "While classical MDS operates on the assumption that dissimilarities are metric, nonmetric MDS offers a more flexible approach by seeking an embedding that preserves only the rank order of the dissimilarities. This exercise delves into the heart of the nonmetric MDS algorithm by focusing on monotonic regression and the calculation of Kruskal's stress. You will use the Pool Adjacent Violators Algorithm (PAVA) to find a set of fitted distances that best respects the ordinal information in the data, a crucial skill for assessing goodness-of-fit in nonmetric embeddings .",
            "id": "4179037",
            "problem": "A laboratory is analyzing representational dissimilarities between neural activity patterns evoked by a set of stimuli. From a Representational Dissimilarity Matrix (RDM), the scientist extracts a set of dissimilarity values $\\delta_{ij}$, and from a provisional two-dimensional embedding of the same stimuli, the scientist measures inter-point distances $d_{ij}$. In the spirit of Nonmetric Multidimensional Scaling (MDS), the goodness-of-fit is assessed by finding a monotone increasing transformation from the dissimilarities to fitted distances that best aligns the $d_{ij}$ values with the rank-order of the $\\delta_{ij}$ values.\n\nConsider the following seven paired measurements $(\\delta_{ij}, d_{ij})$ for distinct stimulus pairs, obtained from one subject’s recording session:\n$(0.8,\\, 1.1)$, $(1.2,\\, 0.9)$, $(1.4,\\, 1.3)$, $(1.5,\\, 1.7)$, $(1.9,\\, 1.6)$, $(2.1,\\, 2.0)$, $(2.3,\\, 1.9)$.\n\nAssume equal weights for all pairs. Following the classical nonmetric procedure, perform a monotonic regression fit of the distances onto the dissimilarities using the least-squares criterion and enforce a nondecreasing fitted mapping with respect to $\\delta_{ij}$. Then compute the resulting Kruskal’s stress-1 for this fit.\n\nExpress the final stress value as a dimensionless decimal rounded to four significant figures.",
            "solution": "The problem requires us to compute Kruskal's stress-1 for a given set of dissimilarity values ($\\delta_{ij}$) and corresponding distances ($d_{ij}$) from a provisional embedding. This involves two main steps: first, finding a set of fitted distances ($\\hat{d}_{ij}$) using monotonic regression, and second, using these values to calculate the stress.\n\n**Step 1: Monotonic Regression using PAVA**\n\nWe are given seven pairs of $(\\delta_{ij}, d_{ij})$. The dissimilarities $\\delta_{ij}$ are already sorted in ascending order. We list the corresponding distances $d_{ij}$:\n$$\nD = (1.1, 0.9, 1.3, 1.7, 1.6, 2.0, 1.9)\n$$\nMonotonic regression seeks a sequence of fitted distances $\\hat{D} = (\\hat{d}_{(1)}, \\dots, \\hat{d}_{(7)})$ that is monotonically non-decreasing and minimizes the sum of squared errors $\\sum (d_{(k)} - \\hat{d}_{(k)})^2$. We use the Pool Adjacent Violators Algorithm (PAVA) to find $\\hat{D}$.\n\n1.  We start with the initial sequence $D$. The first violation of monotonicity is $d_{(1)} > d_{(2)}$ (i.e., $1.1 > 0.9$). We \"pool\" these two values by replacing them with their average: $\\frac{1.1 + 0.9}{2} = 1.0$. The sequence becomes:\n    $(1.0, 1.0, 1.3, 1.7, 1.6, 2.0, 1.9)$\n\n2.  We continue scanning. The next violation is $d_{(4)} > d_{(5)}$ (i.e., $1.7 > 1.6$). We pool them: $\\frac{1.7 + 1.6}{2} = 1.65$. The sequence becomes:\n    $(1.0, 1.0, 1.3, 1.65, 1.65, 2.0, 1.9)$\n\n3.  The final violation is $d_{(6)} > d_{(7)}$ (i.e., $2.0 > 1.9$). We pool them: $\\frac{2.0 + 1.9}{2} = 1.95$. The sequence becomes:\n    $(1.0, 1.0, 1.3, 1.65, 1.65, 1.95, 1.95)$\n\nThis final sequence is monotonically non-decreasing. Thus, the fitted distances are:\n$$\n\\hat{D} = (1.0, 1.0, 1.3, 1.65, 1.65, 1.95, 1.95)\n$$\n\n**Step 2: Compute Kruskal's Stress-1**\n\nKruskal's stress-1 is defined as:\n$$\nS_1 = \\sqrt{\\frac{\\sum (d_{ij} - \\hat{d}_{ij})^2}{\\sum d_{ij}^2}}\n$$\n\nFirst, we calculate the numerator, which is the sum of squared residuals:\n\\begin{align*}\n\\sum (d_{ij} - \\hat{d}_{ij})^2 = (1.1 - 1.0)^2 + (0.9 - 1.0)^2 + (1.3 - 1.3)^2 + (1.7 - 1.65)^2 \\\\\n \\quad + (1.6 - 1.65)^2 + (2.0 - 1.95)^2 + (1.9 - 1.95)^2 \\\\\n= (0.1)^2 + (-0.1)^2 + 0^2 + (0.05)^2 + (-0.05)^2 + (0.05)^2 + (-0.05)^2 \\\\\n= 0.01 + 0.01 + 0 + 0.0025 + 0.0025 + 0.0025 + 0.0025 \\\\\n= 0.03\n\\end{align*}\n\nNext, we calculate the denominator, the sum of the squared original distances:\n\\begin{align*}\n\\sum d_{ij}^2 = 1.1^2 + 0.9^2 + 1.3^2 + 1.7^2 + 1.6^2 + 2.0^2 + 1.9^2 \\\\\n= 1.21 + 0.81 + 1.69 + 2.89 + 2.56 + 4.00 + 3.61 \\\\\n= 16.77\n\\end{align*}\n\nFinally, we compute the stress:\n$$\nS_1 = \\sqrt{\\frac{0.03}{16.77}} \\approx \\sqrt{0.00178890876...} \\approx 0.042295500...\n$$\n\nRounding to four significant figures, we get $0.04230$.",
            "answer": "$$\n\\boxed{0.04230}\n$$"
        },
        {
            "introduction": "To truly appreciate the power of MDS in neuroscience, it is essential to trace the path from raw neural data to an interpretable low-dimensional map. This exercise provides an end-to-end application, starting with hypothetical neural response vectors for a set of experimental conditions. You will first construct a Representational Dissimilarity Matrix (RDM) and then apply classical MDS to not only visualize the geometry but also to quantify how much of the representational structure is captured by its principal dimensions .",
            "id": "4179073",
            "problem": "In an experiment measuring multivoxel activity patterns for four visual conditions, you obtain the following response vectors across two voxels: condition $1$ has response $r_{1} = (0, 0)$, condition $2$ has response $r_{2} = (1, 0)$, condition $3$ has response $r_{3} = (0, 2)$, and condition $4$ has response $r_{4} = (1, 2)$. Using the Euclidean distance between response vectors, construct the Representational Dissimilarity Matrix (RDM). Then, apply classical multidimensional scaling to the RDM to obtain the centered configuration and its spectral decomposition. Finally, compute the proportion of total representational variance captured by the first principal coordinate (i.e., the largest positive eigenvalue of the classical multidimensional scaling solution divided by the sum of all positive eigenvalues). Provide your final answer as an exact fraction with no rounding.",
            "solution": "The problem asks for the proportion of total variance captured by the first principal coordinate in a classical MDS solution. The input data is a set of four 2D response vectors.\n\n**Step 1: Center the Original Coordinates**\n\nClassical MDS applied to Euclidean distances will recover a centered version of the original point configuration. A direct way to find the eigenvalues of the MDS solution is to work with the centered coordinates directly, rather than constructing the full dissimilarity matrix.\n\nThe given response vectors are $r_1=(0,0)$, $r_2=(1,0)$, $r_3=(0,2)$, and $r_4=(1,2)$. First, we compute the centroid (mean vector) of these points:\n$$\n\\bar{r} = \\frac{1}{4} (r_1 + r_2 + r_3 + r_4) = \\frac{1}{4} \\left( (0+1+0+1), (0+0+2+2) \\right) = \\frac{1}{4} (2, 4) = \\left(\\frac{1}{2}, 1\\right)\n$$\nNext, we form the centered coordinate matrix, $X_c$, by subtracting the centroid from each response vector:\n$$\nX_c = \\begin{pmatrix}\n0 - 1/2  0 - 1 \\\\\n1 - 1/2  0 - 1 \\\\\n0 - 1/2  2 - 1 \\\\\n1 - 1/2  2 - 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1/2  -1 \\\\\n1/2  -1 \\\\\n-1/2  1 \\\\\n1/2  1\n\\end{pmatrix}\n$$\n\n**Step 2: Find the Eigenvalues**\n\nThe classical MDS solution involves the eigendecomposition of the Gram matrix $B = X_c X_c^T$. A key property of linear algebra states that the non-zero eigenvalues of $X_c X_c^T$ (a $4 \\times 4$ matrix) are the same as the non-zero eigenvalues of $X_c^T X_c$ (a $2 \\times 2$ matrix). The latter is much easier to compute. Let's call this smaller matrix $C$:\n$$\nC = X_c^T X_c =\n\\begin{pmatrix}\n-1/2  1/2  -1/2  1/2 \\\\\n-1  -1  1  1\n\\end{pmatrix}\n\\begin{pmatrix}\n-1/2  -1 \\\\\n1/2  -1 \\\\\n-1/2  1 \\\\\n1/2  1\n\\end{pmatrix}\n$$\nThe entries of $C$ are:\n$$\nC_{11} = (-1/2)^2 + (1/2)^2 + (-1/2)^2 + (1/2)^2 = 1/4 + 1/4 + 1/4 + 1/4 = 1\n$$\n$$\nC_{12} = (-1/2)(-1) + (1/2)(-1) + (-1/2)(1) + (1/2)(1) = 1/2 - 1/2 - 1/2 + 1/2 = 0\n$$\n$$\nC_{22} = (-1)^2 + (-1)^2 + 1^2 + 1^2 = 1 + 1 + 1 + 1 = 4\n$$\nSo, the matrix $C$ is:\n$$\nC = \\begin{pmatrix}\n1  0 \\\\\n0  4\n\\end{pmatrix}\n$$\nSince $C$ is a diagonal matrix, its eigenvalues are simply its diagonal entries: $1$ and $4$. These are the two positive eigenvalues of the classical MDS solution.\n\n**Step 3: Calculate the Proportion of Variance**\n\nThe problem asks for the proportion of total representational variance captured by the first principal coordinate. This is the ratio of the largest positive eigenvalue to the sum of all positive eigenvalues.\n\nThe positive eigenvalues are $\\lambda_1 = 4$ and $\\lambda_2 = 1$.\nThe largest eigenvalue is $\\lambda_{\\max} = 4$.\nThe sum of positive eigenvalues is $4 + 1 = 5$.\n\nThe required proportion is:\n$$\n\\text{Proportion} = \\frac{\\lambda_{\\max}}{\\sum \\lambda_{k0}} = \\frac{4}{5}\n$$",
            "answer": "$$\n\\boxed{\\frac{4}{5}}\n$$"
        }
    ]
}