## 引言
大脑如何表征世界？对刺激产生的神经活动[模式形成](@entry_id:139998)了一个复杂的高维“表征空间”。理解这个空间的几何结构是破译[神经编码](@entry_id:263658)的关键。然而，如何可视化和解释这种高维结构是一个巨大的挑战。[多维尺度分析](@entry_id:635437)（Multidimensional Scaling, MDS）为此提供了一个强大的解决方案，它能将抽象的相异性数据转化为直观的几何构图。本文旨在为运用MDS分析[表征几何](@entry_id:1130876)提供一份全面的指南。第一章 **“原理与机制”** 将深入探讨经典与非度量MDS的数学基础，阐释它们如何从相异性矩阵构建空间嵌入。第二章 **“应用与跨学科联系”** 将展示MDS在实践中的力量，聚焦其在神经科学[表征相似性分析](@entry_id:1130877)（RSA）中的核心作用，并探索其在其他科学领域的应用。最后，**“动手实践”** 部分将通过具体练习，巩固您的理解并提升应用这些技术的技能。读完本文，您将能透彻掌握如何运用MDS来揭示并解读数据中隐藏的几何结构。

## Principles and Mechanisms

[多维尺度分析](@entry_id:635437)（Multidimensional Scaling, MDS）是一种强大的技术，用于从成对项目间的相异性（dissimilarity）信息中，推断并可视化这些项目内在的几何结构。在神经科学中，它已成为[表征相似性分析](@entry_id:1130877)（Representational Similarity Analysis, RSA）的核心工具，用以揭示大脑活动模式所形成的“表征几何”（representational geometry）。本章将深入探讨MDS的基本原理与核心机制，涵盖其两种主要形式：经典（度量）MDS和非度量MDS，并阐述它们的数学基础、[优化算法](@entry_id:147840)、实际应用中的考量以及结果的解释。

### [经典多维尺度分析](@entry_id:1122428)：从距离到几何构型

经典MDS，又称Torgerson-Gower尺度分析，提供了一种从相异性矩阵直接解析求解欧几里得空间嵌入的优雅方法。其核心思想在于一个深刻的几何洞察：如果一组相异性确实是欧几里得空间中点之间的距离，那么这些距离的平方可以被转换为点对之间的[内积](@entry_id:750660)。

#### 算法核心：双中心化与[特征分解](@entry_id:181333)

假设我们有一个$N \times N$的对称、中空（对角线为零）的表征相异性矩阵（Representational Dissimilarity Matrix, RDM）$\Delta$，其元素$\Delta_{ij}$表示项目$i$和$j$之间的相异性。经典MDS的第一步是假设这些相异性近似于一个未知的、中心化（[质心](@entry_id:138352)在原点）的表征空间中点之间的[欧几里得距离](@entry_id:143990)$d_{ij}$。

如果存在这样的点集 $\{x_i\}_{i=1}^N$，它们的坐标构成一个$N \times p$的矩阵$X$，其中第$i$行为$x_i^\top$。这些点之间的平方欧几里得距离可以表示为：
$d_{ij}^2 = \|x_i - x_j\|^2 = (x_i - x_j)^\top(x_i - x_j) = \|x_i\|^2 + \|x_j\|^2 - 2x_i^\top x_j$

令$B = XX^\top$为这些点坐标的**[格拉姆矩阵](@entry_id:203297)**（Gram matrix），其元素$b_{ij} = x_i^\top x_j$是点$i$和点$j$的[内积](@entry_id:750660)。于是，上式可以写为$d_{ij}^2 = b_{ii} + b_{jj} - 2b_{ij}$。MDS的魔力在于能够从已知的$d_{ij}^2$（我们用观测到的$\Delta_{ij}^2$来近似）反推出未知的[格拉姆矩阵](@entry_id:203297)$B$。这一过程通过一个称为**双中心化**（double centering）的线性代数操作完成。

具体而言，我们首先构建平方相异性矩阵$D^{(2)}$，其元素为$\Delta_{ij}^2$。然后，我们计算双中心化矩阵$B$：
$$
B = -\frac{1}{2} J D^{(2)} J
$$
其中，$J = I - \frac{1}{N}\mathbf{1}\mathbf{1}^\top$是**中心化矩阵**，$I$是单位矩阵，$\mathbf{1}$是全1向量。这个操作等价于从每个$d_{ij}^2$中减去其对应的行均值和列均值，再加上整个矩阵的均值。可以证明，如果$D^{(2)}$中的元素确实是中心化点集的平方欧几里得距离，那么这样计算出的$B$正是这些点坐标的[格拉姆矩阵](@entry_id:203297)。

一旦获得了[格拉姆矩阵](@entry_id:203297)$B$，我们就可以通过特征分解来恢复点的坐标。由于$B$是一个对称矩阵，它可以被分解为$B = V \Lambda V^\top$，其中$V$是其[特征向量](@entry_id:151813)构成的[正交矩阵](@entry_id:169220)，$\Lambda$是由对应特征值$\lambda_k$构成的对角矩阵。坐标矩阵$X$可以被重构为：
$$
X_p = V_p \Lambda_p^{1/2}
$$
这里，$V_p$是与$p$个最大正特征值对应的[特征向量](@entry_id:151813)组成的矩阵，$\Lambda_p^{1/2}$是这些特征值的平方根构成的[对角矩阵](@entry_id:637782)。$X_p$的每一行就是[嵌入空间](@entry_id:637157)中一个点的坐标。

#### 嵌入的解释与不确定性

MDS解决方案的几何解释有几个关键点。首先，嵌入的坐标轴是任意的。任何对坐标矩阵$X_p$的[正交变换](@entry_id:155650)（旋转或反射）$X_p R$（其中$R$为[正交矩阵](@entry_id:169220)）都会得到一个不同的坐标矩阵，但点对之间的距离保持不变。因此，单个坐标轴没有内在含义；有意义的是点之间的相对位置和距离。这种不确定性还包括整体的平移，即将所有点移动同一个向量，以及（对于某些MDS变体）整体的缩放。平移、[旋转和反射](@entry_id:136876)统称为**[等距变换](@entry_id:150881)**（isometries），它们不改变RDM。而[均匀缩放](@entry_id:267671)则会改变RDM的数值，但保持距离的比率和排序不变。

其次，特征值$\lambda_k$具有明确的统计意义。可以证明，第$k$个特征值$\lambda_k$精确地量化了嵌入点云在第$k$个[主轴](@entry_id:172691)（由对应[特征向量](@entry_id:151813)$V_k$定义）上的方差。所有点的总方差等于$B$的迹（trace），也就是所有特征值的总和 $\sum \lambda_k$。因此，前$p$维所能解释的总[方差比](@entry_id:162608)例为：
$$
\text{解释方差比例} = \frac{\sum_{k=1}^p \lambda_k}{\sum_{k=1}^r \lambda_k}
$$
其中，分母是所有**正**特征值的总和，$r$是正特征值的数量。这个比例是衡量[降维](@entry_id:142982)后信息保留程度的重要指标。例如，如果一个包含$n$个刺激的实验计算出的$B$矩阵其正特征值为$\lambda_1=8.4, \lambda_2=3.6, \lambda_3=2.0, \lambda_4=1.0, \lambda_5=0.5$，那么前三维解释的[方差比](@entry_id:162608)例就是 $(8.4+3.6+2.0)/(8.4+3.6+2.0+1.0+0.5) = 14.0/15.5 \approx 0.90323$。

#### 非欧几里得相异性的挑战

在实践中，神经科学数据得到的RDM（例如，使用[相关距离](@entry_id:634939)$1-\rho$）并不总是满足[欧几里得几何](@entry_id:634933)的公理，特别是**[三角不等式](@entry_id:143750)**（即对于任意$i,j,k$，应有$\Delta_{ik} \le \Delta_{ij} + \Delta_{jk}$）。当这些公理被违反时，通过双中心化计算出的矩阵$B$将不再是半正定的，这意味着它会出现**负特征值**。

负特征值的出现是一个深刻的信号：输入的不[相似矩阵](@entry_id:155833)$\Delta$无法在任何维度的真实欧几里得空间中被精确地表示出来。换言之，$B$不是一个合法的[格拉姆矩阵](@entry_id:203297)。在这种情况下，精确的欧几里得嵌入是不可能的。常规的做法是忽略与负特征值相关的维度，只使用正特征值来构建一个近似的欧几里得嵌入。这在最小二乘意义上是最佳的欧几里得近似，但必然会引入失真，即[嵌入空间](@entry_id:637157)中的距离无法完全复现原始的相异性。负特征值的大小可以被看作是数据偏离欧几里得结构的严重程度的度量 。

### 迭代优化与应力最小化：SMACOF算法

经典MDS提供了一个解析解，但其目标（保持[内积](@entry_id:750660)）与我们更直观的目标（保持距离）略有不同。迭代方法，如度量MDS，直接最小化一个称为**应力**（Stress）的[损失函数](@entry_id:634569)，它量化了嵌入距离$d_{ij}(X)$与原始相异性$\delta_{ij}$之间的不匹配程度。一个常见的应力函数是**原始应力**（raw stress）：
$$
S(X) = \sum_{1 \le i  j \le N} w_{ij} \left( d_{ij}(X) - \delta_{ij} \right)^{2}
$$
其中$d_{ij}(X) = \|x_i - x_j\|$是[嵌入空间](@entry_id:637157)中的[欧几里得距离](@entry_id:143990)，$w_{ij}$是非负权重。

最小化$S(X)$是一个[非凸优化](@entry_id:634396)问题，因为$d_{ij}(X)$是坐标$X$的[非线性](@entry_id:637147)函数。SMACOF（Scaling by Majorizing a Complicated Function）算法是解决这个问题的标准方法。它属于一类称为**主化-最小化**（Majorize-Minimize, MM）的算法框架。其基本思想是，在每一步迭代中，不去直接最小化复杂的应力函数$S(X)$，而是构建并最小化一个更简单的替代函数$Q(X | X^{(t)})$，该函数在当前解$X^{(t)}$处“主化”$S(X)$（即$Q(X | X^{(t)}) \ge S(X)$且$Q(X^{(t)} | X^{(t)}) = S(X^{(t)})$）。

MM算法的优美之处在于它保证了应力值在迭代过程中单调不减，即$\sigma(X^{(t+1)}) \le \sigma(X^{(t)})$。由于应力有下界0，该算法保证收敛到一个[稳定点](@entry_id:136617)（梯度为零的点）。为了驱动这个优化过程，算法需要计算应力函数相对于每个点坐标$x_k$的梯度。通过[链式法则](@entry_id:190743)，可以推导出这个梯度为：
$$
\nabla_{x_k} S(X) = 2 \sum_{j \neq k} w_{kj} \left( 1 - \frac{\delta_{kj}}{d_{kj}} \right) (x_k - x_j)
$$
这个表达式直观地揭示了作用在每个点上的“力”：如果嵌入距离$d_{kj}$大于目标相异性$\delta_{kj}$，点$k$和$j$会被拉近；反之则被推远。

由于[优化景观](@entry_id:634681)的非[凸性](@entry_id:138568)，迭代MDS的最终解严重依赖于初始构型。一个糟糕的起点可能导致算法陷入一个不好的**局部最小值**。因此，初始化策略至关重要。一个常见的优良策略是使用经典MDS的解作为初始构型，特别是当相异性近似欧几里得时。另一种更稳健的策略是**多次随机重启**：从多个不同的随机初始构型开始优化，最[后选择](@entry_id:154665)应力最小的那个解。

### 非度量[多维尺度分析](@entry_id:635437)：对秩次的追求

在许多神经科学应用中，我们可能只相信RDM中相异性的**排序**（rank order），而不相信它们的具体数值。例如，fMRI信号的[非线性](@entry_id:637147)以及不同脑区或被试间[信噪比](@entry_id:271861)的差异，可能导致相异性标度变得不可靠。在这种情况下，非度量MDS（non-metric MDS, NMDS）是更合适的工具。

NMDS的核心假设是，嵌入距离$d_{ij}(X)$应该与原始相异性$\delta_{ij}$保持**单调关系**。它寻找一个构型$X$和一个单调非减函数$f$，使得$d_{ij}(X)$尽可能接近于$f(\delta_{ij})$。这些经过单调变换后的目标值$f(\delta_{ij})$被称为**不一致度**（disparities）。

NMDS的优化过程通常是一个**交替最小二乘**方案：
1.  **更新不一致度**：固定当前构型$X^{(t)}$及其距离$d_{ij}(X^{(t)})$，找到一组在[单调性](@entry_id:143760)约束下最接近这些距离的不一致度$\hat{d}_{ij}$。这是一个经典的**[保序回归](@entry_id:912334)**（isotonic regression）问题。
2.  **更新坐标**：固定上一步得到的不一致度$\hat{d}_{ij}$，更[新构型](@entry_id:199611)$X^{(t+1)}$以最小化$X$与$\hat{d}_{ij}$之间的应力，$\sum w_{ij}(d_{ij}(X) - \hat{d}_{ij})^2$。这个子问题同样可以用SMACOF算法高效解决。

通过交替执行这两个步骤，NMDS同时优化了几何构型和数据到几何的映射。这种灵活性使得NMDS非常稳健。当原始相异性被未知的[非线性](@entry_id:637147)[单调函数](@entry_id:145115)扭曲时，度量MDS的[目标函数](@entry_id:267263)景观可能会变得非常“崎岖”，布满许多虚假的局部最小值。相比之下，NMDS中的灵活映射$f$能够“吸收”这些扭曲，从而“平滑”[优化景观](@entry_id:634681)，使得算法更容易找到一个有意义的全局最优解。这种对绝对数值的不敏感性，也使得NMDS对异常值（outliers）的鲁棒性更强，起到了一种正则化的作用。

### 实践中的应用与解释

#### 选择[相异性度量](@entry_id:913782)

MDS分析的第一步，也是至关重要的一步，是选择如何从原始的神经活动模式（例如，每个刺激$i$对应一个体素激活向量$r_i$）计算RDM。不同的度量标准关注神经表征的不同方面。

- **欧几里得距离**（Euclidean distance），$\|r_i - r_j\|$，它同时对激活模式的幅值（magnitude）和形状（shape）敏感。如果两个模式向量仅因整体激活水平（基线）或增益（缩放）不同而异，它们的欧几里得距离将不为零。

- **[相关距离](@entry_id:634939)**（Correlation distance），$1 - \text{corr}(r_i, r_j)$，它在计算前会对每个模式向量进行均值中心化和长度归一化。因此，它只对模式的“形状”（即向量间的夹角）敏感，而不受整体基线偏移或增益变化的影响。如果两个模式仅通过全局增益和基线不同，它们的[相关距离](@entry_id:634939)将为零。

选择哪种度量取决于研究者想要探测的表征不变性。如果认为表征的身份信息编码在激活模式的相对关系中，而不依赖于整体激活强度，那么[相关距离](@entry_id:634939)是更合适的选择。

#### 诊断[拟合优度](@entry_id:176037)：谢泼德图

**谢泼德图**（Shepard diagram）是评估和诊断MDS[拟合质量](@entry_id:637026)不可或缺的工具。它为每一对项目$(i,j)$绘制一个点，横坐标是原始相异性$\delta_{ij}$，纵坐标是嵌入后的距离$d_{ij}$。

-   对于**度量MDS**，理想的谢泼德图上的点应紧密地分布在一条穿过原点的直线上。如果点的分布接近一条斜率不为1的直线，这反映了MDS解的尺度不确定性，可以通过对整个构型进行统一缩放来校正，而不会改变拟合的优度。

-   对于**非度量MDS**，理想的图是点紧密地围绕着一条**单调递增**的曲线。这条曲线的形状本身就是一个重要的科学发现，揭示了原始相异性与潜在心理或神经空间距离之间的[非线性](@entry_id:637147)关系。例如，一条凹形曲线表示嵌入过程“压缩”了较大的相异性，而“拉伸”了较小的相异性。

谢泼德图还能揭示拟合的问题。点与趋势线（或曲线）的垂直偏差（残差）很大，表示拟合应力高。如果点的分布不呈单调性（即出现“反转”，较大的$\delta_{ij}$对应较小的$d_{ij}$），则表明存在**单调性违例**，这在应力较高的解中是可能发生的。如果残差在$\delta_{ij}$较小时很小，但在$\delta_{ij}$较大时系统性地增大，这可能暗示[嵌入维度](@entry_id:268956)过低，模型能很好地拟合局部邻域结构，但无法兼顾全局分离，此时增加[嵌入维度](@entry_id:268956)可能会改善拟合。

#### 计算[可扩展性](@entry_id:636611)

最后，在处理大规模数据集时，理解MDS算法的计算成本至关重要。假设有$n$个刺激。

-   **经典MDS** 的计算瓶颈是$n \times n$[格拉姆矩阵](@entry_id:203297)的[特征分解](@entry_id:181333)，其[时间复杂度](@entry_id:145062)为`O(n^3)`。

-   **SMACOF** 是[迭代算法](@entry_id:160288)，其每次迭代的计算成本主要由所有点对之间距离的计算和一些矩阵运算主导，[时间复杂度](@entry_id:145062)为`[O(n^2)](@entry_id:637398)`。总时间为`O(k * n^2)`，其中`k`是迭代次数。

-   **内存**方面，两种方法都需要存储至少一个$n \times n$的RDM，以及计算过程中产生的其他$n \times n$辅助矩阵，因此[空间复杂度](@entry_id:136795)均为`[O(n^2)](@entry_id:637398)`。

这意味着，当$n$变得非常大时（例如，数万），经典MDS的`O(n^3)`计算成本会变得令人望而却步。SMACOF的`[O(n^2)](@entry_id:637398)`迭代成本在计算上更具优势。然而，在实践中，对于非常大的$n$（例如$n=50,000$），`[O(n^2)](@entry_id:637398)`的内存需求本身就可能超出典型工作站的内存容量，成为更主要的限制因素。例如，在内存上限为16GB的机器上，存储两个$50,000 \times 50,000$的双[精度矩阵](@entry_id:264481)就需要约$4 \times 10^{10}$字节，远超上限。因此，分析大规模RDM通常需要专门的算法或近似技术。