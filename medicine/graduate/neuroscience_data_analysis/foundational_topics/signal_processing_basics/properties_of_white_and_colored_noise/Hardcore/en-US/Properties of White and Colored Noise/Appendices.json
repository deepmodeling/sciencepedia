{
    "hands_on_practices": [
        {
            "introduction": "Before we can analyze noise in data, we must solidify our understanding of its theoretical foundation. This first exercise establishes a critical link between the time-domain variance of a white noise signal, which is easily conceptualized, and its power spectral density (PSD) in the frequency domain. Mastering this conversion is essential for interpreting spectral plots and understanding the units of power in signal processing.",
            "id": "4188424",
            "problem": "In a neural recording experiment, you acquire a zero-mean discrete-time noise sequence $\\{x[n]\\}_{n \\in \\mathbb{Z}}$ that is experimentally verified to be wide-sense stationary and white, with time-domain variance $\\sigma_{x}^{2}$ (in $\\mathrm{V}^{2}$). The sequence is produced by ideal uniform sampling at sampling frequency $f_{s}$ (in $\\mathrm{Hz}$). Using the definition that the Power Spectral Density (PSD) is the Fourier transform of the autocorrelation function (Wiener–Khinchin theorem), and the normalization that connects the discrete-time frequency variable $\\omega$ (in $\\mathrm{rad/sample}$) to the continuous frequency $f$ (in $\\mathrm{Hz}$) via $f = \\omega f_{s} / (2\\pi)$, derive the constant level $S_{1}(f)$ of the one-sided PSD (in $\\mathrm{V}^{2}/\\mathrm{Hz}$) for $0  f  f_{s}/2$. Your derivation must start from the autocorrelation function of white noise and enforce that integrating the one-sided PSD over the Nyquist band recovers the time-domain variance.\n\nExpress your final result as a single closed-form analytic expression in terms of $\\sigma_{x}^{2}$ and $f_{s}$. Do not include units in your final boxed answer; assume $\\mathrm{V}^{2}/\\mathrm{Hz}$. No numerical approximation is required.",
            "solution": "The starting point is the definition of white noise in discrete time and the Wiener–Khinchin theorem. For a zero-mean wide-sense stationary white noise sequence $x[n]$ with variance $\\sigma_{x}^{2}$, the autocorrelation function is\n$$\nR_{x}[k] = \\mathbb{E}\\{x[n] x[n+k]\\} = \\sigma_{x}^{2} \\,\\delta[k],\n$$\nwhere $\\delta[k]$ is the Kronecker delta. By the Wiener–Khinchin theorem, the discrete-time power spectral density $S_{x}^{(d)}(\\omega)$ (in conventional discrete-time units associated with $\\omega$ measured in $\\mathrm{rad/sample}$) is the discrete-time Fourier transform of $R_{x}[k]$:\n$$\nS_{x}^{(d)}(\\omega) = \\sum_{k=-\\infty}^{\\infty} R_{x}[k] \\exp(-\\mathrm{i}\\,\\omega k) = \\sigma_{x}^{2},\n$$\nwhich is flat (constant) for $\\omega \\in [-\\pi,\\pi]$.\n\nTo relate this to a PSD expressed per unit Hertz, denoted $S_{x}^{(2)}(f)$ for the two-sided PSD, we use the normalization that preserves total power (variance) under the change of variables from $\\omega$ to $f$:\n$$\nf = \\frac{\\omega f_{s}}{2\\pi}, \\quad \\mathrm{d}f = \\frac{f_{s}}{2\\pi}\\,\\mathrm{d}\\omega.\n$$\nThe variance $\\sigma_{x}^{2}$ must equal the area under the two-sided PSD across the fundamental Nyquist interval:\n$$\n\\sigma_{x}^{2} = \\int_{-f_{s}/2}^{f_{s}/2} S_{x}^{(2)}(f)\\,\\mathrm{d}f.\n$$\nIn discrete-time frequency variables, the same variance is\n$$\n\\sigma_{x}^{2} = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} S_{x}^{(d)}(\\omega)\\,\\mathrm{d}\\omega = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\sigma_{x}^{2}\\,\\mathrm{d}\\omega = \\sigma_{x}^{2}.\n$$\nTo enforce consistency of the integrals under the change of variables, we require\n$$\nS_{x}^{(2)}(f)\\,\\mathrm{d}f = \\frac{1}{2\\pi} S_{x}^{(d)}(\\omega)\\,\\mathrm{d}\\omega,\n$$\nwhich yields\n$$\nS_{x}^{(2)}(f) = \\frac{1}{2\\pi} S_{x}^{(d)}(\\omega)\\,\\frac{\\mathrm{d}\\omega}{\\mathrm{d}f} = \\frac{1}{2\\pi}\\,\\sigma_{x}^{2}\\,\\frac{2\\pi}{f_{s}} = \\frac{\\sigma_{x}^{2}}{f_{s}},\n$$\nfor $f \\in (-f_{s}/2, f_{s}/2)$. Thus, the two-sided PSD in units of $\\mathrm{V}^{2}/\\mathrm{Hz}$ is flat at level $\\sigma_{x}^{2}/f_{s}$.\n\nThe one-sided PSD $S_{1}(f)$ over $f \\in (0, f_{s}/2)$ is defined so that integrating it over the positive frequencies recovers the total variance. This is achieved by doubling the two-sided level for strictly positive frequencies:\n$$\nS_{1}(f) = 2\\,S_{x}^{(2)}(f) = \\frac{2\\,\\sigma_{x}^{2}}{f_{s}}, \\quad 0  f  \\frac{f_{s}}{2}.\n$$\nFinally, we verify that the one-sided PSD integrates to the variance:\n$$\n\\int_{0}^{f_{s}/2} S_{1}(f)\\,\\mathrm{d}f = \\int_{0}^{f_{s}/2} \\frac{2\\,\\sigma_{x}^{2}}{f_{s}}\\,\\mathrm{d}f = \\frac{2\\,\\sigma_{x}^{2}}{f_{s}} \\cdot \\frac{f_{s}}{2} = \\sigma_{x}^{2}.\n$$\nTherefore, the constant one-sided PSD level in $\\mathrm{V}^{2}/\\mathrm{Hz}$ is\n$$\nS_{1}(f) = \\frac{2\\,\\sigma_{x}^{2}}{f_{s}} \\quad \\text{for} \\quad 0  f  \\frac{f_{s}}{2}.\n$$",
            "answer": "$$\\boxed{\\frac{2\\,\\sigma_{x}^{2}}{f_{s}}}$$"
        },
        {
            "introduction": "With the theoretical basis of the white noise spectrum established (), a natural next step is to try and estimate it from a finite data sample. This problem challenges you to analyze the statistical properties of the most direct PSD estimator, the periodogram. You will discover why, despite being intuitive, the periodogram is an inconsistent estimator, a foundational concept that motivates the use of more sophisticated techniques in practice.",
            "id": "4188376",
            "problem": "You are analyzing resting-state local field potential recordings in a cortical area. The acquisition yields a discrete-time series $\\{x[n]\\}_{n=0}^{N-1}$ sampled at frequency $f_s$. You are interested in estimating the power spectral density (PSD) of additive sensor noise that is hypothesized to be white and Gaussian. The periodogram is defined by\n$$\n\\hat S(f)=\\frac{1}{N f_s}\\left|\\sum_{n=0}^{N-1} x[n] e^{-i 2\\pi f n/f_s}\\right|^2.\n$$\nStarting from the definitions of a wide-sense stationary process and the Wiener–Khinchin theorem (PSD is the Fourier transform of the autocovariance function), and using only the independence structure and second-order moments of white noise, reason about the bias and consistency of $\\hat{S}(f)$ for white Gaussian noise. In particular, consider real-valued, zero-mean, independent and identically distributed Gaussian white noise with variance $\\sigma^2$, sampled at rate $f_s$. Assume $f$ is restricted to the discrete Fourier grid $f_k = k f_s/N$ with $1 \\le k \\le \\lfloor N/2 \\rfloor - 1$ (to avoid special cases at direct current and the Nyquist frequency).\n\nWhich statements are correct?\n\nA. For zero-mean independent and identically distributed Gaussian white noise of variance $\\sigma^2$ sampled at rate $f_s$, the periodogram satisfies $\\mathbb{E}[\\hat{S}(f)] = \\sigma^{2}/f_s$ for all $f$ and all $N$, and $\\operatorname{Var}[\\hat{S}(f)]$ does not tend to $0$ as $N \\to \\infty$; hence $\\hat{S}(f)$ is asymptotically unbiased but inconsistent.\n\nB. The periodogram becomes consistent if we let $N \\to \\infty$ without averaging, because more data always reduce variance for any estimator of the power spectral density.\n\nC. The inconsistency of the periodogram for white noise arises because white noise has infinite energy, so the estimator diverges with $N$, i.e., $\\hat{S}(f) \\to \\infty$ almost surely.\n\nD. The asymptotic bias of the periodogram for white noise vanishes only if we taper the data; without tapering the estimator remains biased as $N \\to \\infty$ even for white noise.\n\nE. For Gaussian white noise, $\\hat{S}(f)$ at any fixed $f$ has an exponential distribution with mean $\\sigma^{2}/f_s$, so its coefficient of variation does not shrink with $N$; therefore, averaging independent periodograms (or applying a spectral smoother) is required to obtain a consistent estimator.",
            "solution": "The problem asks for an analysis of the bias and consistency of the periodogram as an estimator for the power spectral density (PSD) of white Gaussian noise. The analysis must be based on first principles.\n\n### Step 1: Extract Givens\n-   Discrete-time series: $\\{x[n]\\}_{n=0}^{N-1}$\n-   Sampling frequency: $f_s$\n-   Noise model: Real-valued, zero-mean, independent and identically distributed (i.i.d.) Gaussian white noise with variance $\\sigma^2$. So, $x[n] \\sim \\mathcal{N}(0, \\sigma^2)$, and $\\mathbb{E}[x[n]x[m]] = \\sigma^2 \\delta_{nm}$.\n-   Periodogram definition: $\\hat S(f) = \\frac{1}{N f_s} \\left| \\sum_{n=0}^{N-1} x[n] e^{-i 2\\pi f n/f_s} \\right|^2$\n-   Frequency of interest: $f = f_k = k f_s/N$ for $1 \\le k \\le \\lfloor N/2 \\rfloor - 1$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem statement is based on standard, well-established principles of digital signal processing and statistical estimation theory. The definition of the periodogram is a common one, and the properties of white Gaussian noise are standard. The Wiener-Khinchin theorem is a cornerstone of spectral analysis.\n-   **Well-Posed:** The problem is well-posed. It asks for the statistical properties (bias and consistency) of a clearly defined estimator for a precisely specified stochastic process. A unique and meaningful analysis is possible.\n-   **Objective:** The language is formal and precise, with no subjective or ambiguous terms.\n-   **Completeness and Consistency:** All necessary information is provided. The signal model, estimator definition, and relevant parameters ($N, f_s, \\sigma^2$) are given. The frequency range is specified to avoid singular cases at DC ($k=0$) and the Nyquist frequency ($k \\approx N/2$), where the properties of the DFT coefficients differ slightly. The provided definitions are internally consistent.\n-   **Other Criteria:** The problem is a standard textbook exercise, so it is not unrealistic, trivial, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The analysis can proceed.\n\n### Derivation of Estimator Properties\n\n**1. True Power Spectral Density (PSD)**\n\nThe process $x[n]$ is a wide-sense stationary (WSS) process because its mean is constant ($\\mathbb{E}[x[n]]=0$) and its autocovariance function depends only on the time lag $m$:\n$$ R_{xx}[m] = \\mathbb{E}[x[n+m]x[n]] = \\sigma^2 \\delta_{m0} $$\nAccording to the Wiener-Khinchin theorem, the two-sided PSD, $S(f)$, of a discrete-time process is the Discrete-Time Fourier Transform (DTFT) of its autocovariance function, scaled by the sampling period $T_s = 1/f_s$.\n$$ S(f) = T_s \\sum_{m=-\\infty}^{\\infty} R_{xx}[m] e^{-i 2\\pi f m T_s} $$\nFor our white noise process, this becomes:\n$$ S(f) = \\frac{1}{f_s} \\sum_{m=-\\infty}^{\\infty} (\\sigma^2 \\delta_{m0}) e^{-i 2\\pi f m / f_s} = \\frac{1}{f_s} (\\sigma^2 \\cdot e^0) = \\frac{\\sigma^2}{f_s} $$\nThe true PSD is constant across all frequencies, which is the definition of \"white\" noise. This value $S(f) = \\sigma^2/f_s$ is the target our estimator should ideally converge to.\n\n**2. Bias Analysis (Expectation of the Periodogram)**\n\nThe periodogram is defined at frequencies $f_k = k f_s/N$. Let's compute its expectation:\n$$ \\mathbb{E}[\\hat S(f_k)] = \\mathbb{E}\\left[ \\frac{1}{N f_s} \\left| \\sum_{n=0}^{N-1} x[n] e^{-i 2\\pi k n / N} \\right|^2 \\right] $$\n$$ \\mathbb{E}[\\hat S(f_k)] = \\frac{1}{N f_s} \\mathbb{E}\\left[ \\left(\\sum_{n=0}^{N-1} x[n] e^{-i 2\\pi k n / N}\\right) \\left(\\sum_{m=0}^{N-1} x[m] e^{i 2\\pi k m / N}\\right) \\right] $$\nUsing the linearity of expectation:\n$$ \\mathbb{E}[\\hat S(f_k)] = \\frac{1}{N f_s} \\sum_{n=0}^{N-1} \\sum_{m=0}^{N-1} \\mathbb{E}[x[n]x[m]] e^{-i 2\\pi k (n-m) / N} $$\nSince the noise samples are independent with variance $\\sigma^2$, we have $\\mathbb{E}[x[n]x[m]] = \\sigma^2 \\delta_{nm}$. The double summation collapses to a single summation where $n=m$:\n$$ \\mathbb{E}[\\hat S(f_k)] = \\frac{1}{N f_s} \\sum_{n=0}^{N-1} \\sigma^2 e^{-i 2\\pi k (n-n) / N} = \\frac{1}{N f_s} \\sum_{n=0}^{N-1} \\sigma^2 = \\frac{1}{N f_s} (N\\sigma^2) = \\frac{\\sigma^2}{f_s} $$\nThe expectation of the periodogram is equal to the true PSD, $\\mathbb{E}[\\hat S(f_k)] = S(f_k)$, for any sample size $N \\ge 1$. Therefore, the periodogram is an **unbiased** estimator. As a consequence, it is also asymptotically unbiased.\n\n**3. Consistency Analysis (Variance of the Periodogram)**\n\nAn estimator is consistent if its variance approaches zero as $N \\to \\infty$. To find the variance, we first determine the distribution of $\\hat S(f_k)$.\nLet $X[k] = \\sum_{n=0}^{N-1} x[n] e^{-i 2\\pi k n / N}$ be the Discrete Fourier Transform (DFT) coefficient. We can write it in terms of its real and imaginary parts:\n$$ X[k] = \\sum_{n=0}^{N-1} x[n] \\cos(2\\pi kn/N) - i \\sum_{n=0}^{N-1} x[n] \\sin(2\\pi kn/N) = X_R[k] + i X_I[k]$$\nwhere $X_I[k]$ is defined as $-\\sum_n x[n] \\sin(\\dots)$. Since $x[n]$ are i.i.d. Gaussian variables with mean $0$, $X_R[k]$ and $X_I[k]$ are linear combinations of Gaussian variables, and are thus themselves Gaussian with mean $0$.\nLet's find their variances. For the frequency range $1 \\le k \\le \\lfloor N/2 \\rfloor - 1$, the discrete sine and cosine vectors are orthogonal, which simplifies calculations.\n$$ \\operatorname{Var}(X_R[k]) = \\mathbb{E}[X_R[k]^2] = \\mathbb{E}\\left[ \\left( \\sum_n x[n]\\cos(\\cdot) \\right)^2 \\right] = \\sum_n \\sum_m \\mathbb{E}[x[n]x[m]] \\cos(\\frac{2\\pi kn}{N})\\cos(\\frac{2\\pi km}{N}) $$\n$$ = \\sum_n \\sigma^2 \\cos^2(\\frac{2\\pi kn}{N}) = \\sigma^2 \\frac{N}{2} $$\nThe last step uses the standard identity $\\sum_{n=0}^{N-1} \\cos^2(2\\pi kn/N) = N/2$ for non-zero and non-Nyquist frequencies.\nSimilarly, $\\operatorname{Var}(X_I[k]) = \\sigma^2 \\sum_{n=0}^{N-1} \\sin^2(2\\pi kn/N) = \\sigma^2 N/2$.\nThe covariance is:\n$$ \\mathbb{E}[X_R[k]X_I[k]] = -\\sum_n \\sum_m \\mathbb{E}[x[n]x[m]] \\cos(\\frac{2\\pi kn}{N})\\sin(\\frac{2\\pi km}{N}) = -\\sigma^2 \\sum_n \\cos(\\frac{2\\pi kn}{N})\\sin(\\frac{2\\pi kn}{N}) $$\n$$ = -\\frac{\\sigma^2}{2} \\sum_n \\sin(\\frac{4\\pi kn}{N}) = 0 $$\nSince the real and imaginary parts are zero-mean, uncorrelated, and Gaussian, they are independent: $X_R[k], X_I[k] \\sim \\text{i.i.d. } \\mathcal{N}(0, N\\sigma^2/2)$.\nThe periodogram can now be written as:\n$$ \\hat S(f_k) = \\frac{1}{N f_s} |X[k]|^2 = \\frac{1}{N f_s} (X_R[k]^2 + X_I[k]^2) $$\nLet's normalize the Gaussian variables: $Z_1 = \\frac{X_R[k]}{\\sqrt{N\\sigma^2/2}}$ and $Z_2 = \\frac{X_I[k]}{\\sqrt{N\\sigma^2/2}}$. Then $Z_1, Z_2 \\sim \\text{i.i.d. } \\mathcal{N}(0, 1)$.\n$$ \\hat S(f_k) = \\frac{1}{N f_s} \\left( \\frac{N\\sigma^2}{2} Z_1^2 + \\frac{N\\sigma^2}{2} Z_2^2 \\right) = \\frac{\\sigma^2}{2 f_s} (Z_1^2 + Z_2^2) $$\nThe sum of two squared standard normal variables, $Z_1^2 + Z_2^2$, follows a chi-squared distribution with $2$ degrees of freedom, $\\chi^2_2$. A $\\chi^2_2$ distribution is equivalent to an exponential distribution with parameter $\\lambda=1/2$ (or mean $\\beta=2$).\nTherefore, $\\hat S(f_k)$ follows a scaled exponential distribution. Its mean is $\\mathbb{E}[\\hat S(f_k)] = \\frac{\\sigma^2}{2 f_s} \\mathbb{E}[\\chi^2_2] = \\frac{\\sigma^2}{2 f_s} \\cdot 2 = \\frac{\\sigma^2}{f_s}$, confirming our earlier result.\nThe variance of an exponential distribution with mean $\\mu$ is $\\mu^2$. So:\n$$ \\operatorname{Var}(\\hat S(f_k)) = \\left( \\frac{\\sigma^2}{f_s} \\right)^2 $$\nThis variance is constant and does not depend on the sample size $N$. As $N \\to \\infty$, the variance does not tend to $0$. Therefore, the periodogram is an **inconsistent** estimator.\n\n### Evaluation of Options\n\n**A. For zero-mean independent and identically distributed Gaussian white noise of variance $\\sigma^2$ sampled at rate $f_s$, the periodogram satisfies $\\mathbb{E}[\\hat S(f)] = \\sigma^{2}/f_s$ for all $f$ and all $N$, and $\\operatorname{Var}[\\hat S(f)]$ does not tend to $0$ as $N \\to \\infty$; hence $\\hat S(f)$ is asymptotically unbiased but inconsistent.**\nOur derivation shows $\\mathbb{E}[\\hat{S}(f_k)] = \\sigma^2/f_s$. The derivation did not depend on $f$ being on the grid, so this holds for all $f$. The estimator is unbiased for all $N$, and is therefore asymptotically unbiased. Our derivation also shows that for frequencies on the DFT grid, the variance is constant $(\\sigma^2/f_s)^2$ and does not tend to $0$ as $N \\to \\infty$. This makes the estimator inconsistent. The statement is fully supported by the derivation.\n**Verdict: Correct.**\n\n**B. The periodogram becomes consistent if we let $N \\to \\infty$ without averaging, because more data always reduce variance for any estimator of the power spectral density.**\nThis is factually incorrect. Our derivation proves that the variance of the raw periodogram does not decrease as $N \\to \\infty$. The claim that \"more data always reduce variance for any estimator\" is a false generalization.\n**Verdict: Incorrect.**\n\n**C. The inconsistency of the periodogram for white noise arises because white noise has infinite energy, so the estimator diverges with $N$, i.e., $\\hat S(f) \\to \\infty$ almost surely.**\nWhile a realization of a white noise process has infinite energy as $N \\to \\infty$, the periodogram estimator $\\hat S(f)$ is a measure of power spectral *density*. It is normalized by $N$. Our analysis shows that the distribution of $\\hat S(f_k)$ is independent of $N$, with a constant mean and variance. The estimator fluctuates around the true value but does not diverge to infinity.\n**Verdict: Incorrect.**\n\n**D. The asymptotic bias of the periodogram for white noise vanishes only if we taper the data; without tapering the estimator remains biased as $N \\to \\infty$ even for white noise.**\nThis is false. Our derivation shows that the untapered periodogram is exactly unbiased for white noise for any $N$. Tapering is typically used to reduce bias caused by spectral leakage when the true spectrum is not flat, but for flat-spectrum white noise, the untapered periodogram is already unbiased.\n**Verdict: Incorrect.**\n\n**E. For Gaussian white noise, $\\hat S(f)$ at any fixed $f$ has an exponential distribution with mean $\\sigma^{2}/f_s$, so its coefficient of variation does not shrink with $N$; therefore, averaging independent periodograms (or applying a spectral smoother) is required to obtain a consistent estimator.**\nOur derivation for the specified frequencies $f_k$ confirms that $\\hat S(f_k)$ has a scaled $\\chi^2_2$ distribution, which is an exponential distribution with mean $\\mu = \\sigma^2/f_s$. The variance is $\\mu^2$. The coefficient of variation is $\\sqrt{\\text{Var}}/\\text{Mean} = \\sqrt{\\mu^2}/\\mu = 1$. This value is constant and does not shrink with $N$. The conclusion that averaging or smoothing is necessary to create a consistent estimator by reducing variance is the standard and correct approach (e.g., Welch's or Bartlett's methods). This statement is a precise and detailed description of the situation.\n**Verdict: Correct.**\n\nSince both options A and E are correct statements, they both represent valid answers.",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "The inconsistency of the raw periodogram () necessitates methods that can reduce variance to produce a reliable spectral estimate. This practice delves into Welch’s method, a cornerstone of modern signal processing, which achieves consistency by averaging modified periodograms from overlapping, windowed data segments. By deriving its performance for a specific case, you will gain deep insight into the trade-offs involved in practical spectral estimation.",
            "id": "4188428",
            "problem": "A laboratory is analyzing a long local field potential time series from cortex using Power Spectral Density (PSD) estimation. To improve variance properties relative to a single periodogram, the team uses Welch’s method: the record is divided into overlapping segments of equal length, each segment is multiplied by a deterministic taper (window), the Discrete Fourier Transform (DFT) is computed for each windowed segment, the squared magnitudes are appropriately normalized to produce modified periodograms with unbiased mean at each frequency, and these are averaged across segments to form the PSD estimate at that frequency.\n\nStart from the following fundamental base appropriate for stationary Gaussian neural fluctuations:\n- A zero-mean, wide-sense stationary, Gaussian process with flat spectrum (white noise) has DFT coefficients that are complex Gaussian linear forms of the time-domain samples; the squared magnitudes of these linear forms yield modified periodogram ordinates whose distribution is exponential when normalized to unit mean.\n- When averaging identically distributed but possibly correlated random variables, the variance of the average depends on their pairwise correlations.\n\nAssume an even segment length $N$, a constant overlap fraction of $50\\%$ so that the hop size $H$ equals $N/2$, and use the periodic Hann window defined on indices $n=0,1,\\dots,N-1$ by $w(n)=\\tfrac{1}{2}\\left(1-\\cos\\left(\\tfrac{2\\pi n}{N}\\right)\\right)$. Let $K$ denote the number of segments contributing to the Welch average at a fixed frequency. Define the Equivalent Degrees of Freedom (EDOF) $\\nu$ at that frequency by the standard relation between normalized variance and degrees of freedom, namely $\\operatorname{Var}(\\widehat{S}(f))/S(f)^{2}=2/\\nu$, where $\\widehat{S}(f)$ is the Welch PSD estimate and $S(f)$ is the true PSD. In the white-noise case, derive from first principles how segment length, overlap, and window choice determine the pairwise correlation of adjacent segment modified periodograms and thus the variance of the Welch average. Then, specialize your derivation to the given periodic Hann window with $50\\%$ overlap and express the resulting EDOF $\\nu$ as a closed-form function of $K$ only.\n\nYour final answer must be this closed-form analytical expression for $\\nu$ in terms of $K$. No numerical rounding is required, and the answer is dimensionless.",
            "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and self-contained. It presents a standard, albeit non-trivial, problem in statistical signal processing. All necessary parameters and definitions are provided, and there are no contradictions or factual errors. The problem is therefore deemed **valid**, and a solution will be derived.\n\nThe objective is to derive the Equivalent Degrees of Freedom ($\\nu$) for a Welch Power Spectral Density (PSD) estimate, $\\widehat{S}(f)$, of a white Gaussian noise process. The Welch estimator is formed by averaging the modified periodograms, $I_k(f)$, of $K$ overlapping data segments.\n\nThe Welch estimate is given by:\n$$\n\\widehat{S}(f) = \\frac{1}{K} \\sum_{k=1}^{K} I_k(f)\n$$\nwhere $I_k(f)$ is the modified periodogram of the $k$-th segment. The segments are stationary, so the statistical properties of $I_k(f)$ are independent of $k$. The problem states that the modified periodograms are normalized such that their expectation is the true PSD, $S(f)$, i.e., $E[I_k(f)] = S(f)$.\n\nThe variance of the Welch estimate is:\n$$\n\\operatorname{Var}(\\widehat{S}(f)) = \\operatorname{Var}\\left(\\frac{1}{K} \\sum_{k=1}^{K} I_k(f)\\right) = \\frac{1}{K^2} \\operatorname{Var}\\left(\\sum_{k=1}^{K} I_k(f)\\right) = \\frac{1}{K^2} \\sum_{k=1}^K \\sum_{l=1}^K \\operatorname{Cov}(I_k(f), I_l(f))\n$$\nDue to stationarity, the covariance $\\operatorname{Cov}(I_k(f), I_l(f))$ depends only on the lag $|k-l|$. Let $\\gamma_m = \\operatorname{Cov}(I_k(f), I_{k+m}(f))$. The double summation can be rewritten as:\n$$\n\\operatorname{Var}(\\widehat{S}(f)) = \\frac{1}{K^2} \\left[ K\\gamma_0 + 2\\sum_{m=1}^{K-1} (K-m)\\gamma_m \\right]\n$$\nwhere $\\gamma_0 = \\operatorname{Var}(I_k(f))$.\n\nThe problem specifies a $50\\%$ overlap, meaning the hop size is $H = N/2$, where $N$ is the segment length. The windowed data segments are constructed from an underlying white noise process. Segments $k$ and $l$ are non-overlapping in time if $|k-l|H \\ge N$. With $H=N/2$, this condition is $|k-l|N/2 \\ge N$, which simplifies to $|k-l| \\ge 2$. Since the underlying process is white noise (uncorrelated in time), the modified periodograms of non-overlapping windowed segments will be uncorrelated. Therefore, $\\gamma_m = 0$ for $m \\ge 2$. The variance expression simplifies to:\n$$\n\\operatorname{Var}(\\widehat{S}(f)) = \\frac{1}{K^2} \\left[ K\\gamma_0 + 2(K-1)\\gamma_1 \\right]\n$$\nBased on the problem statement's premise (and a standard result for periodograms of white noise), the normalized modified periodogram has an exponential distribution. This implies that the variance of a single modified periodogram is equal to the square of its mean:\n$$\n\\gamma_0 = \\operatorname{Var}(I_k(f)) = (E[I_k(f)])^2 = S(f)^2\n$$\nThe covariance $\\gamma_1$ can be expressed in terms of the correlation coefficient $\\rho_1 = \\operatorname{Corr}(I_k, I_{k+1}) = \\gamma_1 / \\sqrt{\\gamma_0 \\gamma_0} = \\gamma_1 / \\gamma_0$. Thus, $\\gamma_1 = \\rho_1 S(f)^2$.\n\nSubstituting these into the variance expression and normalizing gives the fractional variance:\n$$\n\\frac{\\operatorname{Var}(\\widehat{S}(f))}{S(f)^2} = \\frac{1}{K^2} \\left[ K + 2(K-1)\\rho_1 \\right] = \\frac{1}{K} + \\frac{2(K-1)}{K^2}\\rho_1\n$$\nThe problem defines the Equivalent Degrees of Freedom $\\nu$ via the relation:\n$$\n\\frac{\\operatorname{Var}(\\widehat{S}(f))}{S(f)^2} = \\frac{2}{\\nu}\n$$\nEquating the two expressions for the fractional variance gives:\n$$\n\\frac{2}{\\nu} = \\frac{1}{K} \\left( 1 + 2\\frac{K-1}{K}\\rho_1 \\right)\n$$\nTo find $\\nu$, we must first calculate $\\rho_1$. For complex Gaussian DFT coefficients, which is the case here, the correlation of their squared magnitudes is the square of the magnitude of their correlation:\n$$\n\\rho_1 = \\operatorname{Corr}(I_k, I_{k+1}) = |\\operatorname{Corr}(Y_k(f), Y_{k+1}(f))|^2\n$$\nwhere $Y_k(f)$ is the DFT of the $k$-th windowed segment $y_k(n) = w(n) x_k(n)$. Let the underlying white noise process have variance $\\sigma^2_x$, so that $E[x(p)x(q)] = \\sigma^2_x \\delta_{pq}$. The correlation is:\n$$\n\\operatorname{Corr}(Y_k, Y_{k+1}) = \\frac{E[Y_k(f) Y_{k+1}^*(f)]}{\\sqrt{E[|Y_k(f)|^2] E[|Y_{k+1}(f)|^2]}}\n$$\nThe denominator term is $E[|Y_k(f)|^2] = E[\\sum_{n=0}^{N-1} \\sum_{m=0}^{N-1} w(n)x_k(n)w(m)x_k(m) e^{-i2\\pi f(n-m)/f_s}]$. Using the white noise property, this simplifies to:\n$$\nE[|Y_k(f)|^2] = \\sigma^2_x \\sum_{n=0}^{N-1} w(n)^2\n$$\nThe numerator term is $E[Y_k(f) Y_{k+1}^*(f)] = E[\\sum_{n=0}^{N-1} \\sum_{m=0}^{N-1} w(n)x_k(n)w(m)x_{k+1}(m) e^{-i2\\pi f(n-m)/f_s}]$. Since $x_k(n)=x((k-1)H+n)$ and $x_{k+1}(m)=x(kH+m)$, the expectation $E[x_k(n)x_{k+1}(m)]$ is non-zero only if $(k-1)H+n=kH+m$, or $n=m+H$. This leads to:\n$$\nE[Y_k(f) Y_{k+1}^*(f)] = \\sigma^2_x e^{-i2\\pi f H/f_s} \\sum_{m=0}^{N-1-H} w(m+H)w(m)\n$$\nCombining these results, we get:\n$$\n\\rho_1 = \\left| \\frac{\\sum_{m=0}^{N-1-H} w(m+H)w(m)}{\\sum_{n=0}^{N-1} w(n)^2} \\right|^2\n$$\nWe now evaluate the sums for the specified periodic Hann window, $w(n) = \\frac{1}{2}\\left(1-\\cos\\left(\\frac{2\\pi n}{N}\\right)\\right)$, and overlap $H=N/2$.\n\nThe sum of squares of the window function is a standard result:\n$$\n\\sum_{n=0}^{N-1} w(n)^2 = \\sum_{n=0}^{N-1} \\frac{1}{4}\\left(1 - \\cos\\frac{2\\pi n}{N}\\right)^2 = \\frac{1}{4} \\sum_{n=0}^{N-1} \\left(1 - 2\\cos\\frac{2\\pi n}{N} + \\cos^2\\frac{2\\pi n}{N}\\right)\n$$\nUsing $\\cos^2\\theta = \\frac{1}{2}(1+\\cos 2\\theta)$ and noting that sums of cosines over an integer number of periods are zero, we get:\n$$\n\\sum_{n=0}^{N-1} w(n)^2 = \\frac{1}{4} \\sum_{n=0}^{N-1} \\left(\\frac{3}{2}\\right) = \\frac{3N}{8}\n$$\nThe cross-product sum is over $m$ from $0$ to $N-1-H = N/2-1$:\n$$\n\\sum_{m=0}^{N/2-1} w(m) w(m+N/2)\n$$\nWe have $w(m+N/2) = \\frac{1}{2}\\left(1-\\cos\\left(\\frac{2\\pi (m+N/2)}{N}\\right)\\right) = \\frac{1}{2}\\left(1-\\cos\\left(\\frac{2\\pi m}{N}+\\pi\\right)\\right) = \\frac{1}{2}\\left(1+\\cos\\left(\\frac{2\\pi m}{N}\\right)\\right)$.\nThe product is $w(m)w(m+N/2) = \\frac{1}{4}\\left(1-\\cos^2\\left(\\frac{2\\pi m}{N}\\right)\\right) = \\frac{1}{4}\\left(1-\\frac{1}{2}\\left(1+\\cos\\left(\\frac{4\\pi m}{N}\\right)\\right)\\right) = \\frac{1}{8}\\left(1-\\cos\\left(\\frac{4\\pi m}{N}\\right)\\right)$.\nSumming this from $m=0$ to $N/2-1$:\n$$\n\\sum_{m=0}^{N/2-1} \\frac{1}{8}\\left(1-\\cos\\left(\\frac{4\\pi m}{N}\\right)\\right) = \\frac{1}{8} \\left( \\sum_{m=0}^{N/2-1} 1 - \\sum_{m=0}^{N/2-1} \\cos\\left(\\frac{2\\pi m}{N/2}\\right) \\right)\n$$\nThe sum of the cosine term is over a full period, so it is zero. The sum becomes:\n$$\n\\sum_{m=0}^{N/2-1} w(m)w(m+N/2) = \\frac{1}{8} \\left( \\frac{N}{2} \\right) = \\frac{N}{16}\n$$\nNow we can compute $\\rho_1$:\n$$\n\\rho_1 = \\left( \\frac{N/16}{3N/8} \\right)^2 = \\left( \\frac{N}{16} \\frac{8}{3N} \\right)^2 = \\left(\\frac{1}{6}\\right)^2 = \\frac{1}{36}\n$$\nFinally, we substitute $\\rho_1=1/36$ into the equation for $\\nu$:\n$$\n\\frac{2}{\\nu} = \\frac{1}{K} + \\frac{2(K-1)}{K^2} \\left(\\frac{1}{36}\\right) = \\frac{1}{K} + \\frac{K-1}{18K^2}\n$$\n$$\n\\frac{2}{\\nu} = \\frac{18K + K - 1}{18K^2} = \\frac{19K - 1}{18K^2}\n$$\nSolving for $\\nu$ yields the final expression:\n$$\n\\nu = \\frac{2 \\cdot 18K^2}{19K - 1} = \\frac{36K^2}{19K-1}\n$$\nThis is the closed-form expression for the Equivalent Degrees of Freedom as a function of the number of segments $K$.",
            "answer": "$$\\boxed{\\frac{36K^2}{19K-1}}$$"
        }
    ]
}