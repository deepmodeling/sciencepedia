{
    "hands_on_practices": [
        {
            "introduction": "The first step in analyzing any neural recording is to assess its quality. The signal-to-noise ratio (SNR) is the cornerstone metric for this purpose, providing a quantitative measure of signal strength relative to background noise. This foundational exercise  will guide you through the calculation of SNR from basic power estimates, converting between the linear ratio and the logarithmic decibel (dB) scale, which is essential for interpreting and communicating the detectability of neural signals.",
            "id": "4192874",
            "problem": "You are analyzing a long, stationary extracellular recording of a Local Field Potential (LFP) from primary visual cortex. The measured time series is modeled as $x(t) = s(t) + n(t)$, where $s(t)$ is the neural signal and $n(t)$ is additive noise. Assume $s(t)$ and $n(t)$ are wide-sense stationary, mutually independent, and $n(t)$ is zero-mean Additive White Gaussian Noise (AWGN). The mean power of the neural signal has been estimated as $P_{s} = 2 \\times 10^{-9}\\,\\mathrm{W}$, and the mean power of the noise has been estimated as $P_{n} = 5 \\times 10^{-10}\\,\\mathrm{W}$, from long-duration time averages of $x^{2}(t)$ and spectral integration over bandwidth. Starting from the definition of power for stationary stochastic processes, where for a process $y(t)$ with finite second moment the average power is $P_{y} = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} y^{2}(t)\\,dt = \\mathbb{E}[y^{2}(t)]$, and using the standard engineering definition of decibels applied to power ratios, derive the linear signal-to-noise ratio and the decibel signal-to-noise ratio from $P_{s}$ and $P_{n}$. Then compute their numerical values and briefly interpret what these values imply for detectability of $s(t)$ in $x(t)$ under energy-based detection with fixed integration time. Express the linear signal-to-noise ratio as a pure ratio and the decibel signal-to-noise ratio in decibels. Round your numerical results to four significant figures. Use watts ($\\mathrm{W}$) for power where needed, and express the decibel signal-to-noise ratio in decibels (dB).",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a complete solution. It presents a standard signal processing scenario applied to neuroscience data analysis.\n\nWe are given a measured time series $x(t)$ modeled as the sum of a neural signal $s(t)$ and additive noise $n(t)$:\n$$x(t) = s(t) + n(t)$$\nThe problem states that $s(t)$ and $n(t)$ are wide-sense stationary (WSS) stochastic processes, they are mutually independent, and the noise $n(t)$ is zero-mean, i.e., $\\mathbb{E}[n(t)] = 0$.\n\nThe average power of a WSS process $y(t)$ is defined as its second moment:\n$$P_{y} = \\mathbb{E}[y^{2}(t)]$$\nWe are given the mean power of the neural signal, $P_{s}$, and the mean power of the noise, $P_{n}$:\n$$P_{s} = \\mathbb{E}[s^{2}(t)] = 2 \\times 10^{-9}\\,\\mathrm{W}$$\n$$P_{n} = \\mathbb{E}[n^{2}(t)] = 5 \\times 10^{-10}\\,\\mathrm{W}$$\n\nThe linear signal-to-noise ratio, hereafter denoted as $\\text{SNR}_{\\text{linear}}$, is defined as the ratio of the signal power to the noise power.\n$$\\text{SNR}_{\\text{linear}} = \\frac{P_{s}}{P_{n}}$$\nSubstituting the given numerical values:\n$$\\text{SNR}_{\\text{linear}} = \\frac{2 \\times 10^{-9}\\,\\mathrm{W}}{5 \\times 10^{-10}\\,\\mathrm{W}} = \\frac{2 \\times 10^{-9}}{0.5 \\times 10^{-9}} = 4$$\nAs a pure ratio rounded to four significant figures, this is $4.000$.\n\nThe signal-to-noise ratio in decibels, hereafter denoted as $\\text{SNR}_{\\text{dB}}$, is defined based on the base-10 logarithm of the power ratio. The standard engineering definition is:\n$$\\text{SNR}_{\\text{dB}} = 10 \\log_{10}\\left(\\frac{P_{s}}{P_{n}}\\right) = 10 \\log_{10}(\\text{SNR}_{\\text{linear}})$$\nSubstituting the calculated value of the linear SNR:\n$$\\text{SNR}_{\\text{dB}} = 10 \\log_{10}(4)$$\nTo compute the numerical value:\n$$\\text{SNR}_{\\text{dB}} = 10 \\log_{10}(2^{2}) = 20 \\log_{10}(2)$$\nUsing the value $\\log_{10}(2) \\approx 0.30103$, we get:\n$$\\text{SNR}_{\\text{dB}} \\approx 20 \\times 0.30103 = 6.0206\\,\\mathrm{dB}$$\nRounding this result to four significant figures gives $6.021\\,\\mathrm{dB}$.\n\nFor the interpretation, we consider an energy-based detector which integrates the squared signal over a fixed time interval $T$. The total energy of the measured signal $x(t)$ over this interval is $E_x = \\int_0^T x^2(t)\\,dt$. The expected energy is $\\mathbb{E}[E_x] = \\mathbb{E}[\\int_0^T (s(t)+n(t))^2\\,dt]$. Due to the independence of $s(t)$ and $n(t)$ and the zero-mean property of $n(t)$, the total power is $P_x = P_s + P_n$. Thus, the expected total energy is $\\mathbb{E}[E_x] = (P_s + P_n)T$.\nA linear SNR of $4$ means that the signal power is four times greater than the noise power. An SNR in decibels of approximately $6.021\\,\\mathrm{dB}$ confirms that the signal is significantly stronger than the noise; a common rule of thumb is that a signal is considered clearly detectable for $\\text{SNR}_{\\text{dB}} > 3\\,\\mathrm{dB}$ (i.e., when signal power is more than twice the noise power).\nIn this case, the energy contribution from the signal, $P_s T$, is four times larger than the energy contribution from the noise, $P_n T$, within any given integration window. Consequently, an energy-based detector that sets a threshold on the measured energy can readily distinguish between the signal-plus-noise condition and a noise-only condition with a high degree of reliability. The detectability of the signal $s(t)$ within the composite measurement $x(t)$ is therefore quite good.",
            "answer": "$$\\boxed{\\begin{pmatrix} 4.000 & 6.021 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Often, a single-trial neural response is too weak to be analyzed directly, being buried in background noise. In such cases, a common strategy to boost the SNR is synchronous signal averaging, a technique central to fields like electroencephalography (EEG) and magnetoencephalography (MEG). This practice  explores the powerful relationship between the number of averaged trials and the resulting improvement in SNR, providing a practical understanding of how to design experiments to achieve a target data quality.",
            "id": "1333055",
            "problem": "In neuroscience research, a common challenge is to detect a weak, periodic biological signal that is buried in much stronger, random background noise. A standard method for improving the signal's visibility is synchronous averaging. Consider a scenario where an experimenter is measuring an evoked potential using an Electroencephalogram (EEG). The signal of interest is a repetitive neural response time-locked to a stimulus, while the noise consists of uncorrelated background brain activity and instrumental noise.\n\nFor a single trial, the signal-to-noise ratio (SNR), defined as the ratio of signal power to noise power, is measured to be 5.0 dB. To perform a reliable analysis, the researcher needs to process the data such that the final SNR is at least 40.0 dB. This is achieved by synchronously averaging the waveforms recorded over multiple trials.\n\nAssuming the signal adds coherently and the noise adds incoherently (in quadrature) across trials, determine the minimum integer number of trials, $N$, that must be averaged to achieve the desired signal-to-noise ratio.",
            "solution": "Let the single-trial signal-to-noise ratio in linear power units be $S_{1}$. By definition of decibels for power ratios,\n$$\n\\text{SNR}_{1,\\mathrm{dB}}=10\\log_{10}(S_{1})=5.0 \\quad\\Rightarrow\\quad S_{1}=10^{\\frac{5.0}{10}}.\n$$\nWhen averaging $N$ time-locked trials, the signal adds coherently while the noise, being uncorrelated, adds in quadrature. For the averaged waveform (i.e., with a factor $\\frac{1}{N}$ applied to the sum), the signal power remains unchanged, whereas the noise power is reduced by a factor of $N$. Therefore, the averaged SNR in linear power units is\n$$\nS_{N}=N\\,S_{1}.\n$$\nConverting to decibels,\n$$\n\\text{SNR}_{N,\\mathrm{dB}}=10\\log_{10}(S_{N})=10\\log_{10}(N\\,S_{1})=10\\log_{10}(N)+10\\log_{10}(S_{1})=10\\log_{10}(N)+5.0.\n$$\nThe requirement is $\\text{SNR}_{N,\\mathrm{dB}}\\geq 40.0$, hence\n$$\n10\\log_{10}(N)+5.0\\geq 40.0 \\;\\Rightarrow\\; 10\\log_{10}(N)\\geq 35.0 \\;\\Rightarrow\\; \\log_{10}(N)\\geq 3.5 \\;\\Rightarrow\\; N\\geq 10^{3.5}.\n$$\nSince $N$ must be an integer,\n$$\nN_{\\min}=\\left\\lceil 10^{3.5}\\right\\rceil=\\left\\lceil 10^{3}\\sqrt{10}\\right\\rceil=\\left\\lceil 1000\\sqrt{10}\\right\\rceil.\n$$\nUsing $\\sqrt{10}\\approx 3.16227766$, we get $1000\\sqrt{10}\\approx 3162.27766$, so\n$$\nN_{\\min}=3163.\n$$",
            "answer": "$$\\boxed{3163}$$"
        },
        {
            "introduction": "While signal averaging is a powerful tool for noise reduction, its efficacy can be compromised by imperfections in signal timing across trials. This problem delves into the subtle but critical issue of latency jitter, where trial-to-trial variability in neural response timing can attenuate the very signal you are trying to enhance. By deriving the amplitude loss from first principles , you will gain a deeper appreciation for how temporal imprecision acts as a low-pass filter on your averaged data, impacting the final achievable SNR.",
            "id": "4192850",
            "problem": "You are analyzing an event-related potential (ERP) component whose single-trial waveform near its peak can be modeled as a unimodal Gaussian pulse of peak amplitude $A$ and full-width at half-maximum (FWHM) $w$. Across trials, the neural response exhibits random latency jitter: on trial $i$, the underlying signal is time-shifted by a random latency $\\tau_i$, which is independently drawn from a zero-mean Gaussian distribution with standard deviation $\\sigma_{\\tau}$. Assume additive measurement noise is zero-mean, independent across trials and independent of $\\tau_i$, and that ensemble averaging across a very large number of trials is performed.\n\nStarting only from the following fundamental bases:\n- Linearity of expectation and of the averaging operator,\n- The definition of the ensemble-averaged ERP as the expectation over trial-to-trial latency shifts,\n- The fact that averaging time-shifted copies of a signal with a shift distribution is equivalent to convolving the signal with the probability density of the shift,\n- The relationship between the FWHM $w$ and the standard deviation $\\sigma_s$ of a Gaussian, namely that the Gaussian pulse $s(t)=A \\exp\\!\\big(-t^2/(2\\sigma_s^2)\\big)$ has FWHM $w=2\\sqrt{2\\ln 2}\\,\\sigma_s$,\n\nderive, from first principles, the closed-form analytic expression for the expected amplitude attenuation factor $\\alpha$ at the ERP peak due solely to latency jitter, defined as the ratio between the peak amplitude of the ensemble-averaged ERP and the jitter-free peak amplitude $A$.\n\nReport your final answer as a single closed-form expression for $\\alpha$ in terms of $\\sigma_{\\tau}$ and $w$. No numerical evaluation is required.",
            "solution": "The problem requires the derivation of the amplitude attenuation factor $\\alpha$ for an ensemble-averaged event-related potential (ERP) subject to latency jitter. The derivation must proceed from first principles as laid out in the problem statement.\n\nFirst, we formalize the components of the model.\n\nThe single-trial ERP signal, $s(t)$, is a Gaussian pulse with peak amplitude $A$. Based on the provided relationship between the full-width at half-maximum ($w$) and the standard deviation ($\\sigma_s$) for a Gaussian function of the form $G(t) = C \\exp(-t^2/(2\\sigma^2))$, we can express the single-trial signal centered at $t=0$ as:\n$$s(t) = A \\exp\\left(-\\frac{t^2}{2\\sigma_s^2}\\right)$$\nThe problem gives the relationship $w = 2\\sqrt{2\\ln 2}\\,\\sigma_s$. We can rearrange this to express the signal's standard deviation $\\sigma_s$ in terms of its FWHM $w$:\n$$\\sigma_s = \\frac{w}{2\\sqrt{2\\ln 2}}$$\nSquaring both sides gives the variance of the signal shape:\n$$\\sigma_s^2 = \\frac{w^2}{4(2\\ln 2)} = \\frac{w^2}{8\\ln 2}$$\n\nNext, we define the probability density function (PDF) for the random latency jitter, $\\tau$. The jitter is drawn from a zero-mean Gaussian distribution with standard deviation $\\sigma_{\\tau}$. The PDF, denoted $p(\\tau)$, is:\n$$p(\\tau) = \\frac{1}{\\sigma_{\\tau}\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\tau^2}{2\\sigma_{\\tau}^2}\\right)$$\n\nThe problem states that ensemble averaging across a very large number of trials with random time shifts is equivalent to convolving the single-trial signal $s(t)$ with the PDF of the time shifts $p(t)$. The resulting ensemble-averaged ERP, which we denote as $\\bar{s}(t)$, is therefore given by the convolution integral:\n$$\\bar{s}(t) = (s * p)(t) = \\int_{-\\infty}^{\\infty} s(t - \\tau) p(\\tau) d\\tau$$\nThe problem also states that the ensemble-averaged ERP can be understood as the expectation over the trial-to-trial latency shifts. For a given time point $t$, a single trial $i$ has the signal $s(t-\\tau_i)$. The ensemble average is $\\bar{s}(t) = E_{\\tau}[s(t-\\tau)]$, where $E_{\\tau}[\\cdot]$ denotes the expectation with respect to the random variable $\\tau$. This expectation is calculated by the integral $\\int_{-\\infty}^{\\infty} s(t-\\tau)p(\\tau)d\\tau$, confirming the convolution formulation.\n\nWe are interested in the peak amplitude of this averaged ERP, $\\bar{s}(t)$. Since both the signal $s(t)$ and the jitter distribution $p(t)$ are symmetric and centered at zero, their convolution $\\bar{s}(t)$ will also be a symmetric function centered at $t=0$. Therefore, the peak amplitude of the averaged ERP occurs at $t=0$, and is given by $\\bar{s}(0)$. Let's denote this peak amplitude as $\\bar{A}_{\\text{peak}}$.\n\n$$\\bar{A}_{\\text{peak}} = \\bar{s}(0) = \\int_{-\\infty}^{\\infty} s(0 - \\tau) p(\\tau) d\\tau = \\int_{-\\infty}^{\\infty} s(-\\tau) p(\\tau) d\\tau$$\nSince $s(t)$ is a Gaussian function centered at $t=0$, it is an even function, so $s(-\\tau) = s(\\tau)$. The integral becomes:\n$$\\bar{A}_{\\text{peak}} = \\int_{-\\infty}^{\\infty} s(\\tau) p(\\tau) d\\tau$$\nWe now substitute the expressions for $s(\\tau)$ and $p(\\tau)$:\n$$\\bar{A}_{\\text{peak}} = \\int_{-\\infty}^{\\infty} \\left[ A \\exp\\left(-\\frac{\\tau^2}{2\\sigma_s^2}\\right) \\right] \\left[ \\frac{1}{\\sigma_{\\tau}\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\tau^2}{2\\sigma_{\\tau}^2}\\right) \\right] d\\tau$$\nWe can combine the exponential terms and pull the constants out of the integral:\n$$\\bar{A}_{\\text{peak}} = \\frac{A}{\\sigma_{\\tau}\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\tau^2}{2\\sigma_s^2} - \\frac{\\tau^2}{2\\sigma_{\\tau}^2}\\right) d\\tau$$\n$$\\bar{A}_{\\text{peak}} = \\frac{A}{\\sigma_{\\tau}\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left[-\\frac{\\tau^2}{2}\\left(\\frac{1}{\\sigma_s^2} + \\frac{1}{\\sigma_{\\tau}^2}\\right)\\right] d\\tau$$\nLet's simplify the term in the parenthesis:\n$$\\frac{1}{\\sigma_s^2} + \\frac{1}{\\sigma_{\\tau}^2} = \\frac{\\sigma_{\\tau}^2 + \\sigma_s^2}{\\sigma_s^2 \\sigma_{\\tau}^2}$$\nThe integral is now in the form of a standard Gaussian integral, $\\int_{-\\infty}^{\\infty} \\exp(-ax^2)dx = \\sqrt{\\pi/a}$. In our case, the variable is $\\tau$ and the coefficient $a$ is:\n$$a = \\frac{1}{2} \\left(\\frac{\\sigma_s^2 + \\sigma_{\\tau}^2}{\\sigma_s^2 \\sigma_{\\tau}^2}\\right)$$\nThe value of the integral is therefore:\n$$\\int_{-\\infty}^{\\infty} \\exp(-\\tau^2 a) d\\tau = \\sqrt{\\frac{\\pi}{a}} = \\sqrt{\\pi \\cdot \\frac{2\\sigma_s^2 \\sigma_{\\tau}^2}{\\sigma_s^2 + \\sigma_{\\tau}^2}} = \\frac{\\sigma_s \\sigma_{\\tau} \\sqrt{2\\pi}}{\\sqrt{\\sigma_s^2 + \\sigma_{\\tau}^2}}$$\nSubstituting this result back into the expression for $\\bar{A}_{\\text{peak}}$:\n$$\\bar{A}_{\\text{peak}} = \\frac{A}{\\sigma_{\\tau}\\sqrt{2\\pi}} \\left( \\frac{\\sigma_s \\sigma_{\\tau} \\sqrt{2\\pi}}{\\sqrt{\\sigma_s^2 + \\sigma_{\\tau}^2}} \\right)$$\nThe terms $\\sigma_{\\tau}\\sqrt{2\\pi}$ cancel out, leaving:\n$$\\bar{A}_{\\text{peak}} = A \\frac{\\sigma_s}{\\sqrt{\\sigma_s^2 + \\sigma_{\\tau}^2}}$$\nThe amplitude attenuation factor $\\alpha$ is defined as the ratio of the averaged peak amplitude $\\bar{A}_{\\text{peak}}$ to the single-trial (jitter-free) peak amplitude $A$:\n$$\\alpha = \\frac{\\bar{A}_{\\text{peak}}}{A} = \\frac{A \\frac{\\sigma_s}{\\sqrt{\\sigma_s^2 + \\sigma_{\\tau}^2}}}{A} = \\frac{\\sigma_s}{\\sqrt{\\sigma_s^2 + \\sigma_{\\tau}^2}}$$\nWe can rewrite this as:\n$$\\alpha = \\frac{1}{\\sqrt{\\frac{\\sigma_s^2 + \\sigma_{\\tau}^2}{\\sigma_s^2}}} = \\frac{1}{\\sqrt{1 + \\frac{\\sigma_{\\tau}^2}{\\sigma_s^2}}}$$\nFinally, we substitute the expression for the signal variance $\\sigma_s^2 = \\frac{w^2}{8\\ln 2}$ to express $\\alpha$ in terms of $w$ and $\\sigma_{\\tau}$:\n$$\\alpha = \\frac{1}{\\sqrt{1 + \\frac{\\sigma_{\\tau}^2}{w^2 / (8\\ln 2)}}} = \\frac{1}{\\sqrt{1 + \\frac{8(\\ln 2)\\sigma_{\\tau}^2}{w^2}}}$$\nThis is the final closed-form expression for the amplitude attenuation factor.",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{1 + \\frac{8(\\ln 2)\\sigma_{\\tau}^2}{w^2}}}}$$"
        }
    ]
}