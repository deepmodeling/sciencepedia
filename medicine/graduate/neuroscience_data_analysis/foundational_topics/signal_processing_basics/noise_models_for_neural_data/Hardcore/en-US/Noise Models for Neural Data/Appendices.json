{
    "hands_on_practices": [
        {
            "introduction": "We begin with the bedrock of spike count analysis: the homogeneous Poisson process. Before tackling more complex scenarios, it is essential to master the fundamental task of estimating a neuron's constant firing rate, $\\lambda$, from observed data. This exercise guides you through a foundational analytical derivation of the Maximum Likelihood Estimate (MLE) for $\\lambda$, and then establishes the theoretical best-case precision for this estimate by deriving the Cramér-Rao Bound (CRB), providing deep insight into the relationship between observation time and estimation uncertainty .",
            "id": "4181776",
            "problem": "A single neuron is recorded over a fixed observation window of duration $T  0$ seconds. Under a standard noise model for neural spike trains, assume the spike generation in this window is well described by a homogeneous Poisson process with a constant rate parameter $\\lambda  0$, so that the total spike count $K$ observed in the window is a single draw from a Poisson distribution with mean $\\lambda T$. Starting from the fundamental definition of the likelihood for the Poisson model and the definition of Fisher information, derive the Maximum Likelihood Estimate (MLE) of $\\lambda$ based on the observed count $K$, and derive the Cramér-Rao Bound (CRB) for the variance of any unbiased estimator of $\\lambda$ using this observation model. Express your final answers as closed-form symbolic expressions, without units. Provide the two expressions in a single row matrix, where the first entry is the MLE and the second entry is the CRB for $\\operatorname{Var}(\\hat{\\lambda})$ under this model.",
            "solution": "The problem requires the derivation of two quantities related to the estimation of a neural firing rate $\\lambda$. The model for the data is that the total spike count $K$ observed in a fixed time window of duration $T  0$ is a single draw from a Poisson distribution with mean $\\lambda T$, where $\\lambda  0$ is the constant rate parameter.\n\nFirst, we will derive the Maximum Likelihood Estimate (MLE) of $\\lambda$. The foundation for the MLE is the likelihood function, which is the probability of observing the data, $K$, as a function of the parameter, $\\lambda$. The probability mass function for a Poisson distribution with mean $\\mu$ is $P(\\text{count}=k) = \\frac{\\mu^k \\exp(-\\mu)}{k!}$. For our model, the mean is $\\mu = \\lambda T$. The likelihood function $\\mathcal{L}(\\lambda | K)$ for observing a specific count $K$ is therefore:\n$$\\mathcal{L}(\\lambda | K) = \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!}$$\nTo find the MLE, we seek the value of $\\lambda$ that maximizes this function. It is equivalent and mathematically more convenient to maximize the natural logarithm of the likelihood function, the log-likelihood $\\ell(\\lambda | K) = \\ln(\\mathcal{L}(\\lambda | K))$.\n$$\\ell(\\lambda | K) = \\ln\\left( \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!} \\right)$$\nUsing the properties of logarithms, we expand this expression:\n$$\\ell(\\lambda | K) = \\ln((\\lambda T)^K) - \\ln(\\exp(\\lambda T)) - \\ln(K!)$$\n$$\\ell(\\lambda | K) = K \\ln(\\lambda T) - \\lambda T - \\ln(K!)$$\nThis can be further expanded as:\n$$\\ell(\\lambda | K) = K \\ln(\\lambda) + K \\ln(T) - \\lambda T - \\ln(K!)$$\nTo find the maximum, we take the first derivative of $\\ell(\\lambda | K)$ with respect to $\\lambda$ and set the result to zero. Note that $K$ and $T$ are treated as constants in this differentiation.\n$$\\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda} (K \\ln(\\lambda) + K \\ln(T) - \\lambda T - \\ln(K!)) = K \\cdot \\frac{1}{\\lambda} + 0 - T - 0 = \\frac{K}{\\lambda} - T$$\nSetting this derivative to zero yields the equation for the MLE of $\\lambda$, denoted $\\hat{\\lambda}_{MLE}$:\n$$\\frac{K}{\\hat{\\lambda}_{MLE}} - T = 0$$\nSolving for $\\hat{\\lambda}_{MLE}$:\n$$\\hat{\\lambda}_{MLE} = \\frac{K}{T}$$\nTo verify that this value corresponds to a maximum, we examine the second derivative of the log-likelihood:\n$$\\frac{d^2\\ell}{d\\lambda^2} = \\frac{d}{d\\lambda} \\left(\\frac{K}{\\lambda} - T\\right) = -\\frac{K}{\\lambda^2}$$\nSince the spike count $K$ is a non-negative integer ($K \\geq 0$) and the rate $\\lambda$ is positive ($\\lambda  0$), the second derivative $\\frac{d^2\\ell}{d\\lambda^2} = -\\frac{K}{\\lambda^2}$ is always non-positive. If $K  0$, the second derivative is strictly negative, confirming that $\\hat{\\lambda}_{MLE} = \\frac{K}{T}$ is a local maximum. If $K = 0$, the likelihood $\\mathcal{L}(\\lambda|0) = \\exp(-\\lambda T)$ is a strictly decreasing function of $\\lambda$, and its maximum is approached as $\\lambda \\to 0$. Our formula $\\hat{\\lambda}_{MLE} = 0/T = 0$ is consistent with this case. The first expression is thus $\\frac{K}{T}$.\n\nSecond, we will derive the Cramér-Rao Bound (CRB) for the variance of any unbiased estimator of $\\lambda$. The CRB is the reciprocal of the Fisher information, $I(\\lambda)$. The Fisher information for a parameter $\\lambda$ is defined as:\n$$I(\\lambda) = -E\\left[\\frac{d^2}{d\\lambda^2}\\ell(\\lambda | K)\\right]$$\nThe expectation $E[\\cdot]$ is taken with respect to the probability distribution of the data $K$, which is $K \\sim \\text{Poisson}(\\lambda T)$. We have already computed the second derivative of the log-likelihood:\n$$\\frac{d^2\\ell}{d\\lambda^2} = -\\frac{K}{\\lambda^2}$$\nNow, we compute its expectation:\n$$E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = E\\left[-\\frac{K}{\\lambda^2}\\right]$$\nSince $\\lambda$ is the parameter we are estimating, it is treated as a constant within the expectation over $K$:\n$$E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = -\\frac{1}{\\lambda^2}E[K]$$\nThe expected value of a Poisson-distributed random variable is its mean parameter. Here, $E[K] = \\lambda T$. Substituting this into the equation:\n$$E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = -\\frac{1}{\\lambda^2}(\\lambda T) = -\\frac{T}{\\lambda}$$\nUsing this result, we find the Fisher information:\n$$I(\\lambda) = -E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = -\\left(-\\frac{T}{\\lambda}\\right) = \\frac{T}{\\lambda}$$\nThe Cramér-Rao inequality states that for any unbiased estimator $\\hat{\\lambda}$ of $\\lambda$, its variance is bounded from below by the reciprocal of the Fisher information:\n$$\\operatorname{Var}(\\hat{\\lambda}) \\ge \\frac{1}{I(\\lambda)}$$\nThe Cramér-Rao Bound is this lower limit:\n$$\\text{CRB} = \\frac{1}{I(\\lambda)} = \\frac{1}{T/\\lambda} = \\frac{\\lambda}{T}$$\nThis is the second required expression.\n\nThe solution requires the two expressions, the MLE and the CRB, to be provided in a single row matrix.\nThe MLE is $\\frac{K}{T}$.\nThe CRB for $\\operatorname{Var}(\\hat{\\lambda})$ is $\\frac{\\lambda}{T}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{K}{T}  \\frac{\\lambda}{T} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The Poisson model, while fundamental, rests on the strong assumption that the mean and variance of spike counts are equal. In practice, neural data often exhibits \"overdispersion,\" where variability exceeds the mean, violating this core assumption. This hands-on coding exercise addresses this common challenge by introducing the Negative Binomial distribution as a more flexible alternative and teaches a critical skill in data analysis: quantitative model comparison. You will implement and apply proper scoring rules to evaluate which model provides better predictions for held-out data, a crucial step in building robust and scientifically credible statistical models .",
            "id": "4181830",
            "problem": "You are given discrete trial spike counts collected in a fixed-duration observation window, modeled as integer-valued random variables. Two canonical noise models for such counts are the Poisson distribution and the Negative Binomial distribution, frequently used in neuroscience data analysis to capture, respectively, equidispersion and overdispersion. This problem asks you to compare predictive calibration of these models by computing held-out log scores and Brier scores on specified trial counts, using principled estimation procedures and well-defined predictive scoring rules.\n\nUse the following foundational base:\n- The Poisson distribution with rate parameter $\\lambda$ is defined on $\\{0,1,2,\\dots\\}$ and has probability mass function $p(y \\mid \\lambda) = \\exp(-\\lambda) \\lambda^{y} / y!$. In a fixed window, spike count under independent firing assumptions is often modeled as Poisson.\n- The Negative Binomial distribution with parameters $(k, p)$ in the \"number of failures until $k$ successes\" parameterization has probability mass function $p(y \\mid k, p) = \\binom{y + k - 1}{y} (1 - p)^{y} p^{k}$ for $y \\in \\{0,1,2,\\dots\\}$, mean $\\mu = k (1 - p) / p$, and variance $\\sigma^{2} = \\mu + \\mu^{2} / k$. This distribution captures overdispersion relative to Poisson when $\\sigma^{2}  \\mu$.\n- Maximum likelihood estimation for the Poisson rate in independent trials yields the estimator $\\hat{\\lambda}$ equal to the sample mean of the training counts.\n- Method-of-moments estimation for the Negative Binomial with mean $\\mu$ and size $k$ uses the training sample mean $m$ and sample variance $s^{2}$ to solve $m = \\mu$ and $s^{2} = \\mu + \\mu^{2} / k$. If $s^{2}  m$, then $\\hat{k} = m^{2} / (s^{2} - m)$ and $\\hat{p} = \\hat{k} / (\\hat{k} + m)$. If $s^{2} \\le m$, treat the Negative Binomial predictive distribution as the Poisson with mean $m$ (equivalently, $\\hat{k} \\to \\infty$).\n\nDefine the predictive scoring rules:\n- For a test count $y$, the held-out log score is $\\log p(y \\mid \\text{predictive})$, where $p$ is the predictive probability mass under the fitted model. Use the natural logarithm. For a test set $\\{y_{i}\\}_{i=1}^{n}$, report the mean held-out log score $\\frac{1}{n} \\sum_{i=1}^{n} \\log p(y_{i} \\mid \\text{predictive})$.\n- For a test count $y$ and a discrete predictive distribution $p_{k}$ over bins, the Brier score is the squared $\\ell_{2}$ error between the predicted probabilities and the one-hot encoding of the observed bin. For count data with unbounded support, define a truncated multi-category Brier score with bins $\\{0,1,\\dots,K\\}$ and a tail bin $K^{+}$. Let $p_{k} = p(Y = k)$ for $k \\in \\{0,\\dots,K\\}$ and $p_{K^{+}} = 1 - \\sum_{k=0}^{K} p_{k}$. Let $y_{k} = 1$ if $y = k$ and $y_{k} = 0$ otherwise for $k \\in \\{0,\\dots,K\\}$, and let $y_{K^{+}} = 1$ if $y  K$ and $y_{K^{+}} = 0$ otherwise. The Brier score for an observation is $\\sum_{b \\in \\{0,\\dots,K,K^{+}\\}} (p_{b} - y_{b})^{2}$, and the mean Brier score over a test set is the average across test observations.\n\nImplement the following procedure for each test case:\n- Fit the Poisson rate by $\\hat{\\lambda} = m$, where $m$ is the training sample mean.\n- Fit the Negative Binomial by method-of-moments: compute the training sample mean $m$ and unbiased sample variance $s^{2}$, then set $\\hat{k} = m^{2} / (s^{2} - m)$ and $\\hat{p} = \\hat{k} / (\\hat{k} + m)$ if $s^{2}  m$, otherwise use the Poisson predictive distribution for the Negative Binomial model.\n- Compute the mean held-out log score for both models on the test set.\n- Compute the mean truncated Brier score for both models on the test set using the test-case-specific truncation parameter $K$, including the tail bin $K^{+}$.\n\nTest suite:\n- Case $1$: training counts $\\{2,0,3,1,4,0,2,5,1,3\\}$, test counts $\\{0,2,1,4,6\\}$, truncation $K = 10$.\n- Case $2$: training counts $\\{1,0,1,1,0,2,1,0,1,1\\}$, test counts $\\{0,1,1,2,0\\}$, truncation $K = 5$.\n- Case $3$: training counts $\\{0,0,0,1,0,0,2,0,0,0\\}$, test counts $\\{0,0,1,0,3\\}$, truncation $K = 6$.\n- Case $4$: training counts $\\{10,12,8,20,9,17\\}$, test counts $\\{7,20,13\\}$, truncation $K = 30$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case in order, output four floats rounded to $6$ decimal places: the mean held-out log score under the Poisson model, the mean held-out log score under the Negative Binomial model, the mean Brier score under the Poisson model, and the mean Brier score under the Negative Binomial model.\n- The final output list concatenates these four floats across all test cases, resulting in a single list of length $16$. For example, the formatting should be like $\\texttt{[r\\_1,r\\_2,\\dots,r\\_{16}]}$ with each $r_{i}$ a float rounded to $6$ decimal places.",
            "solution": "We begin from the core definitions of the Poisson and Negative Binomial distributions, their roles in modeling spike counts, and standard estimation procedures derived from first principles.\n\nFor the Poisson distribution, the probability mass function for an integer count $y \\in \\{0,1,2,\\dots\\}$ with rate $\\lambda$ is $p(y \\mid \\lambda) = \\exp(-\\lambda) \\lambda^{y} / y!$. Under the assumption of independent spike generation in a fixed observation window, the number of spikes is modeled as Poisson, and the joint likelihood for independent training observations $\\{y_{i}\\}_{i=1}^{n}$ is $L(\\lambda) = \\prod_{i=1}^{n} \\exp(-\\lambda) \\lambda^{y_{i}} / y_{i}!$. The log-likelihood is $\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + y_{i} \\log \\lambda - \\log y_{i}! \\right ) = -n \\lambda + \\left( \\sum_{i=1}^{n} y_{i} \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log y_{i}!$. Differentiating with respect to $\\lambda$ yields $\\frac{\\partial \\ell}{\\partial \\lambda} = -n + \\left( \\sum_{i=1}^{n} y_{i} \\right ) \\frac{1}{\\lambda}$, and setting to zero gives $\\hat{\\lambda} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$, i.e., the sample mean. This maximum likelihood estimator is consistent and unbiased under the Poisson model.\n\nFor the Negative Binomial distribution in the \"number of failures until $k$ successes\" parameterization, with parameters $(k, p)$, the probability mass function for $y \\in \\{0,1,2,\\dots\\}$ is $p(y \\mid k, p) = \\binom{y + k - 1}{y} (1 - p)^{y} p^{k}$. The mean and variance satisfy $\\mu = k (1 - p) / p$ and $\\sigma^{2} = \\mu + \\mu^{2} / k$. This distribution is often used to model overdispersed spike counts (variance exceeding mean), capturing heterogeneity or extra-Poisson variability. A method-of-moments fit uses the training sample mean $m = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$ and unbiased sample variance $s^{2} = \\frac{1}{n - 1} \\sum_{i=1}^{n} (y_{i} - m)^{2}$ to match $\\mu = m$ and $\\sigma^{2} = s^{2}$ via $s^{2} = m + m^{2} / k$, which yields $k = m^{2} / (s^{2} - m)$ provided $s^{2}  m$. Solving for $p$ gives $p = k / (k + m)$. When $s^{2} \\le m$, the Negative Binomial cannot represent underdispersion relative to Poisson, so a principled limit $k \\to \\infty$ reduces the Negative Binomial to the Poisson with mean $m$. In practice, we therefore treat the Negative Binomial predictive distribution as Poisson in this case.\n\nPredictive calibration is evaluated by proper scoring rules. The held-out log score for an observed test count $y$ under a predictive distribution $p(\\cdot)$ is $\\log p(y)$, using the natural logarithm. For a test set $\\{y_{i}\\}_{i=1}^{n}$, the mean held-out log score is $\\frac{1}{n} \\sum_{i=1}^{n} \\log p(y_{i})$. This score is strictly proper: it is maximized in expectation by the true predictive distribution.\n\nFor counts with unbounded support, the Brier score (originally defined for binary outcomes) generalizes to a multi-category setting by considering a finite partition of outcomes into bins. Using the truncation parameter $K$, define bins $\\{0,1,\\dots,K\\}$ and a tail bin $K^{+}$. For a model with predictive mass function $p(y)$, let $p_{k} = p(Y = k)$ for $k \\in \\{0,\\dots,K\\}$ and $p_{K^{+}} = 1 - \\sum_{k=0}^{K} p_{k}$. For a realized test count $y$, define the one-hot vector $y_{b}$ over the bins: $y_{k} = 1$ if $y = k$ else $0$ for $k \\in \\{0,\\dots,K\\}$, and $y_{K^{+}} = 1$ if $y  K$ else $0$. The Brier score is the squared Euclidean distance between the predictive probability vector and the one-hot label, namely $\\sum_{b \\in \\{0,\\dots,K,K^{+}\\}} (p_{b} - y_{b})^{2}$. The mean Brier score over a test set is the average of this quantity across tests. This truncated definition is scientifically meaningful when $K$ is chosen so that tail mass is small or explicitly accounted for via $K^{+}$.\n\nAlgorithmic steps for each test case:\n- Compute the training sample mean $m$ and unbiased sample variance $s^{2}$.\n- Fit the Poisson parameter $\\hat{\\lambda} = m$.\n- Fit the Negative Binomial parameters by method-of-moments: if $s^{2}  m$, set $\\hat{k} = m^{2} / (s^{2} - m)$ and $\\hat{p} = \\hat{k} / (\\hat{k} + m)$; otherwise, set a flag to use the Poisson predictive distribution for Negative Binomial scores.\n- For each test count $y$, compute the log score $\\log p(y)$ using the fitted Poisson and fitted Negative Binomial predictive distributions, averaging across the test set.\n- For each test count $y$, compute the truncated multi-category Brier score with bins $\\{0,\\dots,K,K^{+}\\}$, where $K$ is specified per case. For each model, compute $p_{k}$ for $k \\in \\{0,\\dots,K\\}$ from its probability mass function, and $p_{K^{+}} = 1 - \\sum_{k=0}^{K} p_{k}$ using the cumulative distribution function at $K$. Form the one-hot vector $y_{b}$ and compute the squared error sum. Average across the test set.\n- Round all reported scores to $6$ decimal places.\n- Concatenate four scores per case in the order: mean held-out log score (Poisson), mean held-out log score (Negative Binomial), mean Brier score (Poisson), mean Brier score (Negative Binomial).\n- Aggregate across all cases and print a single line with the comma-separated list enclosed in square brackets.\n\nEdge cases and robustness:\n- The case $s^{2} \\le m$ yields Negative Binomial parameters that imply $k \\to \\infty$; the predictive distribution converges to Poisson with mean $m$. Implementation uses Poisson for Negative Binomial scores in this scenario.\n- Numerical stability is maintained by using well-tested probability mass and cumulative distribution function implementations.\n- Truncation $K$ defines a tail bin $K^{+}$ that aggregates mass beyond $K$, ensuring a proper multi-category Brier score over a finite set of bins.\n\nApplying this procedure to the provided test suite yields a reproducible, programmatic comparison of predictive calibration between Poisson and Negative Binomial models for neural trial counts.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson, nbinom\n\ndef fit_poisson(train_counts):\n    # MLE for Poisson rate: sample mean\n    lam = float(np.mean(train_counts)) if len(train_counts)  0 else 0.0\n    return lam\n\ndef fit_nb_moments(train_counts, eps=1e-12):\n    # Method-of-moments for NB: match mean and variance\n    n = len(train_counts)\n    if n == 0:\n        # Degenerate; default to Poisson with lambda 0\n        return {\"use_poisson\": True, \"mu\": 0.0, \"k\": np.inf, \"p\": 1.0}\n    m = float(np.mean(train_counts))\n    # Unbiased sample variance; if n==1, set variance to 0\n    s2 = float(np.var(train_counts, ddof=1)) if n  1 else 0.0\n    if s2  m + eps:\n        k = m**2 / (s2 - m)\n        p = k / (k + m)\n        return {\"use_poisson\": False, \"mu\": m, \"k\": k, \"p\": p}\n    else:\n        # Underdispersion: NB reduces to Poisson\n        return {\"use_poisson\": True, \"mu\": m, \"k\": np.inf, \"p\": 1.0}\n\ndef mean_log_score(test_counts, model_dist):\n    # model_dist is a scipy.stats distribution object providing logpmf\n    log_scores = model_dist.logpmf(test_counts)\n    return float(np.mean(log_scores))\n\ndef mean_brier_score(test_counts, model_dist, K):\n    # Compute truncated multi-category Brier score with bins 0..K and tail K+\n    ks = np.arange(K + 1, dtype=int)\n    pmf_vals = model_dist.pmf(ks)  # shape (K+1,)\n    # Tail probability K+ = 1 - CDF(K)\n    tail_prob = 1.0 - model_dist.cdf(K)\n    # For each test count, compute Brier score\n    briers = []\n    for y in test_counts:\n        # One-hot vector over bins 0..K and tail\n        y_vec = np.zeros(K + 2, dtype=float)\n        if y = K:\n            y_vec[y] = 1.0\n        else:\n            y_vec[K + 1] = 1.0\n        p_vec = np.concatenate([pmf_vals, np.array([tail_prob])])\n        brier = np.sum((p_vec - y_vec) ** 2)\n        briers.append(brier)\n    return float(np.mean(briers))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (training_counts, test_counts, K)\n    test_cases = [\n        (np.array([2,0,3,1,4,0,2,5,1,3], dtype=int), np.array([0,2,1,4,6], dtype=int), 10),\n        (np.array([1,0,1,1,0,2,1,0,1,1], dtype=int), np.array([0,1,1,2,0], dtype=int), 5),\n        (np.array([0,0,0,1,0,0,2,0,0,0], dtype=int), np.array([0,0,1,0,3], dtype=int), 6),\n        (np.array([10,12,8,20,9,17], dtype=int), np.array([7,20,13], dtype=int), 30),\n    ]\n\n    results = []\n    for train_counts, test_counts, K in test_cases:\n        # Fit Poisson\n        lam = fit_poisson(train_counts)\n        pois_dist = poisson(mu=lam)\n\n        # Fit Negative Binomial\n        nb_params = fit_nb_moments(train_counts)\n        if nb_params[\"use_poisson\"]:\n            nb_dist = poisson(mu=nb_params[\"mu\"])\n        else:\n            # scipy.stats.nbinom parameterization: number of successes n=k, probability p\n            k = nb_params[\"k\"]\n            p = nb_params[\"p\"]\n            nb_dist = nbinom(n=k, p=p)\n\n        # Mean held-out log scores\n        ls_pois = mean_log_score(test_counts, pois_dist)\n        ls_nb = mean_log_score(test_counts, nb_dist)\n\n        # Mean Brier scores (truncated with tail bin)\n        brier_pois = mean_brier_score(test_counts, pois_dist, K)\n        brier_nb = mean_brier_score(test_counts, nb_dist, K)\n\n        # Round to 6 decimals and append in required order\n        results.extend([\n            round(ls_pois, 6),\n            round(ls_nb, 6),\n            round(brier_pois, 6),\n            round(brier_nb, 6),\n        ])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our previous practices assumed a static firing rate, a simplification that often fails to capture the dynamic nature of neural activity. We now advance to a more realistic and powerful framework: the state-space model, which allows us to track a firing rate that evolves over time. This capstone exercise requires you to implement a forward-filtering algorithm for a model where the latent log-rate follows an autoregressive process and observations are Poisson-distributed. Because this realistic combination is not analytically tractable, you will implement a Gaussian approximation based on a second-order expansion, a sophisticated technique that mirrors modern methods used in cutting-edge neuroscience research for inferring latent neural dynamics .",
            "id": "4181816",
            "problem": "Consider a single-neuron discrete-time State-Space Model (SSM) for spike counts with latent log-rate following Auto-Regressive (AR) dynamics. Let the time index be $t \\in \\{1,2,\\dots,T\\}$, the observed spike count be $y_t \\in \\{0,1,2,\\dots\\}$, and the latent log-rate be $x_t = \\log \\Lambda_t \\in \\mathbb{R}$. The latent dynamics are first-order Auto-Regressive (AR) and Gaussian: $x_t = \\mu + \\phi (x_{t-1} - \\mu) + \\sigma \\epsilon_t$, where $\\epsilon_t \\sim \\mathcal{N}(0,1)$ independently across $t$, with parameters $\\mu \\in \\mathbb{R}$, $\\phi \\in (-1,1)$, and $\\sigma  0$. The observation model is Poisson: conditioned on $x_t$, the count $y_t$ is distributed as a Poisson random variable with rate parameter $\\Lambda_t = \\exp(x_t)$, that is, $y_t \\sim \\mathrm{Poisson}(\\Lambda_t)$.\n\nStarting from the foundational definitions of the Poisson likelihood, the Gaussian prior for the AR dynamics, and Bayes' rule, derive the forward filtering equations to infer the latent $x_t$ given observations $y_{1:t}$ for $t = 1,\\dots,T$. Because the Poisson observation with a log link is not conjugate to the Gaussian prior, use a principled approximation based on a second-order expansion of the log-posterior in $x_t$ at its Maximum A Posteriori (MAP) estimate to obtain a Gaussian approximation to the filtering distribution at each time $t$. From this approximation, extract both an approximate posterior mean and variance for $x_t$ given $y_{1:t}$, and then compute the filtered posterior expectation of the rate $E[\\Lambda_t \\mid y_{1:t}]$ using the corresponding moment transformation implied by the Gaussian approximation to $x_t$.\n\nImplement a program that, for each of the following test cases, performs the forward filtering over the full sequence of observations and outputs the final filtered posterior expectation of the rate $E[\\Lambda_T \\mid y_{1:T}]$. All computations are unitless. Your algorithm must be universal and rely only on the definitions and the described second-order approximation; it must not use closed-form shortcut formulas from external sources.\n\nUse the following test suite, which covers a general happy path, near-unit AR coefficient, all-zero counts, and negative AR coefficient:\n\n- Case $1$: $T = 7$, $\\phi = 0.9$, $\\sigma = 0.3$, $\\mu = \\log 3$, initial prior $x_0 \\sim \\mathcal{N}(m_0, P_0)$ with $m_0 = \\log 3$, $P_0 = 0.25$, observations $y_{1:7} = [0,1,2,0,3,1,0]$.\n- Case $2$: $T = 5$, $\\phi = 0.99$, $\\sigma = 0.1$, $\\mu = \\log 10$, initial prior $m_0 = \\log 10$, $P_0 = 0.04$, observations $y_{1:5} = [12,8,15,9,11]$.\n- Case $3$: $T = 6$, $\\phi = 0.2$, $\\sigma = 1.0$, $\\mu = \\log 0.5$, initial prior $m_0 = \\log 0.5$, $P_0 = 1.0$, observations $y_{1:6} = [0,0,0,0,0,0]$.\n- Case $4$: $T = 5$, $\\phi = -0.5$, $\\sigma = 0.5$, $\\mu = \\log 2$, initial prior $m_0 = \\log 2$, $P_0 = 0.5$, observations $y_{1:5} = [2,0,4,1,3]$.\n\nFor each case, run the forward filter across all time steps $t = 1,\\dots,T$ and return the single float $E[\\Lambda_T \\mid y_{1:T}]$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above, rounded to six decimal places (for example, $[1.234567,2.345678,3.456789,4.567890]$).",
            "solution": "The problem requires the derivation and implementation of a forward filtering algorithm for a single-neuron state-space model. This model describes the evolution of a latent log-firing rate, $x_t$, and the generation of observed spike counts, $y_t$. The model is defined for discrete time steps $t \\in \\{1, 2, \\dots, T\\}$.\n\nThe model comprises two main components:\n1.  **State Equation**: The latent log-rate $x_t = \\log \\Lambda_t \\in \\mathbb{R}$ follows a first-order autoregressive, AR($1$), process with Gaussian noise. The dynamics are given by $x_t = \\mu + \\phi (x_{t-1} - \\mu) + \\sigma \\epsilon_t$, where $\\epsilon_t \\sim \\mathcal{N}(0,1)$. This implies that the state transition probability distribution is Gaussian: $p(x_t \\mid x_{t-1}) \\sim \\mathcal{N}(x_t \\mid \\mu + \\phi(x_{t-1} - \\mu), \\sigma^2)$. The parameters are the mean log-rate $\\mu \\in \\mathbb{R}$, the persistence parameter $\\phi \\in (-1,1)$, and the process noise standard deviation $\\sigma  0$.\n2.  **Observation Equation**: The observed spike count $y_t \\in \\{0, 1, 2, \\dots\\}$ is conditionally independent given the latent state $x_t$ and follows a Poisson distribution with rate $\\Lambda_t = \\exp(x_t)$. The observation likelihood is thus $p(y_t \\mid x_t) = \\text{Poisson}(y_t \\mid \\Lambda_t) = \\frac{\\Lambda_t^{y_t} e^{-\\Lambda_t}}{y_t!}$.\n\nThe objective is to perform recursive Bayesian filtering to estimate the posterior distribution of the latent state, $p(x_t \\mid y_{1:t})$, for $t=1, \\dots, T$, given a Gaussian prior on the initial state $p(x_0) \\sim \\mathcal{N}(x_0 \\mid m_0, P_0)$. The filtering recursion consists of two steps for each time point: prediction and update.\n\nWe assume that at the end of time step $t-1$, the filtering distribution $p(x_{t-1} \\mid y_{1:t-1})$ is well-approximated by a Gaussian distribution:\n$$p(x_{t-1} \\mid y_{1:t-1}) \\approx \\mathcal{N}(x_{t-1} \\mid m_{t-1|t-1}, P_{t-1|t-1})$$\nAt the start, for $t=1$, we use the given prior on $x_0$, so we initialize with $m_{0|0} = m_0$ and $P_{0|0} = P_0$.\n\n**1. Prediction Step (Time Update)**\n\nThe prediction step evolves the state distribution from time $t-1$ to time $t$, using only the state dynamics. We aim to compute the predictive distribution $p(x_t \\mid y_{1:t-1})$ by marginalizing over the previous state $x_{t-1}$:\n$$p(x_t \\mid y_{1:t-1}) = \\int p(x_t \\mid x_{t-1}) p(x_{t-1} \\mid y_{1:t-1}) \\, dx_{t-1}$$\nThis integral represents the convolution of two Gaussian distributions: the Gaussian state transition $p(x_t \\mid x_{t-1})$ and the Gaussian posterior from the previous step $p(x_{t-1} \\mid y_{1:t-1})$. Since the state transition is linear in $x_{t-1}$, the resulting predictive distribution is also Gaussian, denoted as $p(x_t \\mid y_{1:t-1}) \\approx \\mathcal{N}(x_t \\mid m_{t|t-1}, P_{t|t-1})$.\n\nThe parameters of this predictive distribution are found as follows:\n-   **Predictive Mean $m_{t|t-1}$**: Using the law of total expectation:\n    $$m_{t|t-1} = E[x_t \\mid y_{1:t-1}] = E[E[x_t \\mid x_{t-1}, y_{1:t-1}] \\mid y_{1:t-1}]$$\n    From the state equation, $E[x_t \\mid x_{t-1}] = \\mu + \\phi(x_{t-1} - \\mu)$. Therefore,\n    $$m_{t|t-1} = E[\\mu + \\phi(x_{t-1} - \\mu) \\mid y_{1:t-1}] = \\mu + \\phi(E[x_{t-1} \\mid y_{1:t-1}] - \\mu) = \\mu + \\phi(m_{t-1|t-1} - \\mu)$$\n-   **Predictive Variance $P_{t|t-1}$**: Using the law of total variance:\n    $$P_{t|t-1} = \\mathrm{Var}(x_t \\mid y_{1:t-1}) = E[\\mathrm{Var}(x_t \\mid x_{t-1})] + \\mathrm{Var}(E[x_t \\mid x_{t-1}] \\mid y_{1:t-1})$$\n    The conditional variance is $\\mathrm{Var}(x_t \\mid x_{t-1}) = \\sigma^2$. The variance of the conditional expectation is $\\mathrm{Var}(\\mu + \\phi(x_{t-1} - \\mu) \\mid y_{1:t-1}) = \\phi^2 \\mathrm{Var}(x_{t-1} \\mid y_{1:t-1}) = \\phi^2 P_{t-1|t-1}$. Thus,\n    $$P_{t|t-1} = \\sigma^2 + \\phi^2 P_{t-1|t-1}$$\nThe predictive distribution is completely characterized by $m_{t|t-1}$ and $P_{t|t-1}$.\n\n**2. Update Step (Measurement Update)**\n\nThe update step incorporates the new observation $y_t$ to refine the predictive distribution into the filtering distribution $p(x_t \\mid y_{1:t})$. According to Bayes' rule:\n$$p(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t) p(x_t \\mid y_{1:t-1})$$\nThe Poisson likelihood $p(y_t \\mid x_t)$ is not conjugate to the Gaussian predictive distribution (our prior in this step). To maintain a Gaussian posterior approximation, the problem specifies using a second-order expansion of the log-posterior at its Maximum A Posteriori (MAP) estimate. This is a Laplace approximation.\n\nThe log-posterior of $x_t$, ignoring terms constant in $x_t$, is:\n$$J(x_t) = \\log p(x_t \\mid y_{1:t}) \\propto \\log p(y_t \\mid x_t) + \\log p(x_t \\mid y_{1:t-1})$$\nSubstituting the Poisson log-likelihood and the Gaussian log-prior density:\n$$J(x_t) = \\left(y_t \\log(\\Lambda_t) - \\Lambda_t \\right) - \\frac{1}{2 P_{t|t-1}}(x_t - m_{t|t-1})^2 = y_t x_t - \\exp(x_t) - \\frac{1}{2 P_{t|t-1}}(x_t - m_{t|t-1})^2$$\nTo find the MAP estimate, $\\hat{x}_t^{\\text{MAP}}$, we set the first derivative of $J(x_t)$ to zero:\n$$J'(x_t) = \\frac{dJ}{dx_t} = y_t - \\exp(x_t) - \\frac{x_t - m_{t|t-1}}{P_{t|t-1}} = 0$$\nThis is a transcendental equation for $\\hat{x}_t^{\\text{MAP}}$ and must be solved numerically. We use the Newton-Raphson method. Let $g(x_t) = J'(x_t)$. The iterative update is $x_t^{(k+1)} = x_t^{(k)} - g(x_t^{(k)})/g'(x_t^{(k)})$, where $g'(x_t)$ is the second derivative of the log-posterior:\n$$J''(x_t) = \\frac{d^2J}{dx_t^2} = -\\exp(x_t) - \\frac{1}{P_{t|t-1}}$$\nThe function $J(x_t)$ is strictly concave since $J''(x_t)  0$ for all $x_t$, which guarantees a unique maximum and robust convergence of the Newton-Raphson algorithm.\n\nAfter computing $\\hat{x}_t^{\\text{MAP}}$, we approximate $J(x_t)$ with a second-order Taylor series centered at this mode:\n$$J(x_t) \\approx J(\\hat{x}_t^{\\text{MAP}}) + J'(\\hat{x}_t^{\\text{MAP}})(x_t - \\hat{x}_t^{\\text{MAP}}) + \\frac{1}{2} J''(\\hat{x}_t^{\\text{MAP}})(x_t - \\hat{x}_t^{\\text{MAP}})^2$$\nSince $J'(\\hat{x}_t^{\\text{MAP}}) = 0$ by definition of the mode, we have:\n$$J(x_t) \\approx \\text{const.} + \\frac{1}{2} J''(\\hat{x}_t^{\\text{MAP}})(x_t - \\hat{x}_t^{\\text{MAP}})^2$$\nExponentiating this approximation shows that $p(x_t \\mid y_{1:t})$ is approximately Gaussian, $p(x_t \\mid y_{1:t}) \\approx \\mathcal{N}(x_t \\mid m_{t|t}, P_{t|t})$. By comparing the exponent to the standard form of a Gaussian log-density, we identify the posterior mean and variance:\n-   **Posterior Mean $m_{t|t}$**: $m_{t|t} = \\hat{x}_t^{\\text{MAP}}$\n-   **Posterior Variance $P_{t|t}$**: $P_{t|t} = (-J''(\\hat{x}_t^{\\text{MAP}}))^{-1} = \\left(\\exp(\\hat{x}_t^{\\text{MAP}}) + \\frac{1}{P_{t|t-1}}\\right)^{-1}$\nThese parameters, $m_{t|t}$ and $P_{t|t}$, define the filtering distribution at time $t$ and serve as the input for the prediction step at time $t+1$.\n\n**3. Final Calculation: Posterior Expected Rate**\n\nThe final required output is the filtered posterior expectation of the rate at the last time step, $E[\\Lambda_T \\mid y_{1:T}]$. Given our Gaussian approximation $p(x_T \\mid y_{1:T}) \\approx \\mathcal{N}(x_T \\mid m_{T|T}, P_{T|T})$ and the relation $\\Lambda_T = \\exp(x_T)$, we compute the expectation of a log-normal variable.\nIf a random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, its moment-generating function is $M_X(s) = E[\\exp(sX)] = \\exp(\\mu s + \\frac{1}{2}\\sigma^2 s^2)$. The desired expectation corresponds to evaluating the MGF at $s=1$:\n$$E[\\Lambda_T \\mid y_{1:T}] = E[\\exp(x_T) \\mid y_{1:T}] \\approx \\exp\\left(m_{T|T} \\cdot 1 + \\frac{1}{2}P_{T|T} \\cdot 1^2\\right) = \\exp\\left(m_{T|T} + \\frac{1}{2}P_{T|T}\\right)$$\nThe program will execute this full recursive procedure for each test case to compute the final value.",
            "answer": "```python\nimport numpy as np\n\ndef run_filter(T, phi, sigma, mu, m0, P0, y):\n    \"\"\"\n    Performs forward filtering for the Poisson-AR(1) state-space model.\n\n    This function implements a Gaussian-approximated filter (Laplace approximation)\n    to recursively estimate the posterior distribution of the latent log-rate.\n\n    Args:\n        T (int): Number of time steps.\n        phi (float): AR(1) coefficient.\n        sigma (float): Standard deviation of process noise.\n        mu (float): Mean of the AR(1) process.\n        m0 (float): Mean of the initial prior distribution for x_0.\n        P0 (float): Variance of the initial prior distribution for x_0.\n        y (list or np.ndarray): Sequence of observed spike counts.\n\n    Returns:\n        float: The filtered posterior expectation of the rate at the final time T,\n               E[Lambda_T | y_{1:T}].\n    \"\"\"\n    # Initialize filtered mean and variance with prior at t=0\n    m_filt = m0\n    P_filt = P0\n\n    for t_idx in range(T):\n        y_t = y[t_idx]\n\n        # 1. Prediction (Time Update)\n        # Predict the state at time t given observations up to t-1.\n        m_pred = mu + phi * (m_filt - mu)\n        P_pred = phi**2 * P_filt + sigma**2\n\n        # 2. Update (Measurement Update)\n        # Find the MAP estimate for x_t using Newton-Raphson method.\n        # The objective is to find the root of the log-posterior's first derivative.\n        \n        # Initial guess for the mode is the predicted mean.\n        x_map = m_pred\n        \n        # We iterate a fixed number of times; convergence is very fast.\n        for _ in range(30):\n            exp_x = np.exp(x_map)\n            \n            # First derivative of log-posterior J'(x)\n            g = y_t - exp_x - (x_map - m_pred) / P_pred\n            \n            # Second derivative of log-posterior J''(x)\n            g_prime = -exp_x - 1.0 / P_pred\n            \n            # Newton-Raphson update step\n            x_map = x_map - g / g_prime\n\n        # The updated (filtered) mean is the MAP estimate.\n        m_filt = x_map\n        \n        # The updated (filtered) variance is the inverse of the negative second derivative\n        # evaluated at the MAP estimate.\n        P_filt = 1.0 / (np.exp(x_map) + 1.0 / P_pred)\n\n    # After the loop, m_filt is m_{T|T} and P_filt is P_{T|T}.\n\n    # 3. Compute Final Expected Rate\n    # E[Lambda_T | y_{1:T}] = exp(m_{T|T} + P_{T|T}/2) due to log-normal property.\n    expected_rate_T = np.exp(m_filt + 0.5 * P_filt)\n    \n    return expected_rate_T\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the filter for each, and prints the results.\n    \"\"\"\n    test_cases = [\n        # Case 1: General happy path\n        {'T': 7, 'phi': 0.9, 'sigma': 0.3, 'mu': np.log(3), 'm0': np.log(3), 'P0': 0.25, 'y': [0, 1, 2, 0, 3, 1, 0]},\n        # Case 2: Near-unit AR coefficient\n        {'T': 5, 'phi': 0.99, 'sigma': 0.1, 'mu': np.log(10), 'm0': np.log(10), 'P0': 0.04, 'y': [12, 8, 15, 9, 11]},\n        # Case 3: All-zero counts\n        {'T': 6, 'phi': 0.2, 'sigma': 1.0, 'mu': np.log(0.5), 'm0': np.log(0.5), 'P0': 1.0, 'y': [0, 0, 0, 0, 0, 0]},\n        # Case 4: Negative AR coefficient\n        {'T': 5, 'phi': -0.5, 'sigma': 0.5, 'mu': np.log(2), 'm0': np.log(2), 'P0': 0.5, 'y': [2, 0, 4, 1, 3]},\n    ]\n    \n    results = []\n    for case in test_cases:\n        result = run_filter(case['T'], case['phi'], case['sigma'], case['mu'], case['m0'], case['P0'], case['y'])\n        results.append(result)\n\n    # Format the output as a comma-separated list of floats rounded to 6 decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n\n```"
        }
    ]
}