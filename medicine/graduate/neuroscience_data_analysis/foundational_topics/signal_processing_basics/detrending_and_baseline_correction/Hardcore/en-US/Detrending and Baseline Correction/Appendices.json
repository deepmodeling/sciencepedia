{
    "hands_on_practices": [
        {
            "introduction": "A common intuitive approach to baseline correction is to subtract a smoothed version of the signal. This exercise formalizes this intuition by showing that subtracting an exponentially weighted moving average is equivalent to applying a first-order high-pass filter. By deriving the filter's properties from first principles, you will gain a deeper understanding of how the choice of a smoothing parameter, $\\gamma$, directly shapes the frequency response and stability of the detrending operation .",
            "id": "4155619",
            "problem": "A continuous extracellular neural signal exhibits slow baseline drift due to thermal and electrode interface fluctuations. To perform detrending, consider the following discrete-time baseline-correction scheme sampled at rate $f_{s}$, where the sampling period is $T_{s} = 1/f_{s}$. Let $x[n]$ denote the observed signal, $b[n]$ the baseline estimate, and $y[n]$ the detrended output. The baseline is computed as an exponentially weighted moving average with parameter $\\gamma \\in (0,1)$:\n$$b[n] = \\gamma\\, b[n-1] + (1 - \\gamma)\\, x[n],$$\nand the detrended signal is formed by subtraction:\n$$y[n] = x[n] - b[n].$$\nAssume $b[-1] = 0$ for causality initialization and that $\\gamma$ is chosen as $\\gamma = \\exp(-T_{s}/\\tau)$ for some baseline time constant $\\tau > 0$.\n\nStarting only from the definitions of linear time-invariant systems and the discrete-time Fourier transform, and without invoking any pre-packaged filter design formulas, do the following:\n1. Show that the mapping from $x[n]$ to $y[n]$ defined above is linear and time-invariant for fixed $\\gamma$.\n2. Eliminate $b[n]$ to derive a first-order difference equation relating $y[n]$, $y[n-1]$, $x[n]$, and $x[n-1]$.\n3. From this difference equation, derive the frequency response $H(\\exp(j\\omega))$ of the induced first-order high-pass Infinite Impulse Response (IIR) filter that maps $x[n]$ to $y[n]$. Express $H(\\exp(j\\omega))$ in closed form in terms of $\\gamma$ and $\\omega$.\n4. Using the pole-zero locations implied by your $H(\\exp(j\\omega))$, discuss the stability condition in terms of $\\gamma$, the qualitative phase response, and the minimum-phase property. Explain how and why the group delay behaves across frequency.\n5. Relate the pole parameter $\\gamma$ to a desired cutoff frequency $f_{c}$ relative to the sampling rate $f_{s}$ by deriving the implicit condition that defines the $-3$ decibel point with respect to the high-frequency asymptotic gain. Use this to write $f_{c}$ in terms of $\\gamma$ and $f_{s}$, and interpret the regime $\\gamma \\to 1$.\n\nYour final answer must be a single closed-form analytic expression for the frequency response $H(\\exp(j\\omega))$ derived in part 3. No numerical approximation or rounding is required. Do not include units in the final answer.",
            "solution": "The problem statement has been validated and is deemed sound, well-posed, and objective. It provides a standard formulation for a first-order high-pass filter derived from a low-pass filter (an exponentially weighted moving average), a common technique in signal processing for detrending. All definitions are clear and mathematically formalizable. We may therefore proceed with the solution.\n\nThe system is defined by two equations:\n1. The baseline estimate: $b[n] = \\gamma\\, b[n-1] + (1 - \\gamma)\\, x[n]$\n2. The detrended output: $y[n] = x[n] - b[n]$\nwith the initial condition $b[-1] = 0$ and the parameter $\\gamma \\in (0,1)$.\n\nPart $1$: Show that the mapping from $x[n]$ to $y[n]$ is linear and time-invariant.\n\nLinearity: A system is linear if for any two inputs $x_1[n]$ and $x_2[n]$ producing outputs $y_1[n]$ and $y_2[n]$ respectively, the input $x[n] = a_1 x_1[n] + a_2 x_2[n]$ produces the output $y[n] = a_1 y_1[n] + a_2 y_2[n]$ for any constants $a_1, a_2$.\nLet $b_1[n]$ and $b_2[n]$ be the baselines corresponding to $x_1[n]$ and $x_2[n]$.\n$b_1[n] = \\gamma b_1[n-1] + (1-\\gamma)x_1[n]$\n$b_2[n] = \\gamma b_2[n-1] + (1-\\gamma)x_2[n]$\nNow consider the input $x[n] = a_1 x_1[n] + a_2 x_2[n]$. Let the corresponding baseline be $b[n]$.\n$b[n] = \\gamma b[n-1] + (1-\\gamma)(a_1 x_1[n] + a_2 x_2[n])$.\nWe can see that the expression $a_1 b_1[n] + a_2 b_2[n]$ satisfies this same recursion:\n$a_1 b_1[n] + a_2 b_2[n] = a_1(\\gamma b_1[n-1] + (1-\\gamma)x_1[n]) + a_2(\\gamma b_2[n-1] + (1-\\gamma)x_2[n])$\n$= \\gamma(a_1 b_1[n-1] + a_2 b_2[n-1]) + (1-\\gamma)(a_1 x_1[n] + a_2 x_2[n])$.\nSince both $b[n]$ and $a_1 b_1[n] + a_2 b_2[n]$ satisfy the same first-order difference equation and share the same initial condition of $0$ (since $b_1[-1]=0$ and $b_2[-1]=0$), they must be identical. Thus, $b[n] = a_1 b_1[n] + a_2 b_2[n]$.\nNow consider the output $y[n]$:\n$y[n] = x[n] - b[n] = (a_1 x_1[n] + a_2 x_2[n]) - (a_1 b_1[n] + a_2 b_2[n])$\n$= a_1(x_1[n] - b_1[n]) + a_2(x_2[n] - b_2[n]) = a_1 y_1[n] + a_2 y_2[n]$.\nThe system is therefore linear.\n\nTime-Invariance: A system is time-invariant if a shift in the input signal by $n_0$ samples causes an identical shift in the output signal. That is, if $x[n] \\to y[n]$, then $x[n-n_0] \\to y[n-n_0]$.\nLet $x_d[n] = x[n-n_0]$ be the delayed input. Let $b_d[n]$ and $y_d[n]$ be the corresponding baseline and output.\n$b_d[n] = \\gamma b_d[n-1] + (1-\\gamma)x_d[n] = \\gamma b_d[n-1] + (1-\\gamma)x[n-n_0]$.\nNow consider the delayed version of the original baseline, $b[n-n_0]$. Shifting the index $n$ to $n-n_0$ in the original baseline equation gives:\n$b[n-n_0] = \\gamma b[n-n_0-1] + (1-\\gamma)x[n-n_0]$.\nThis is the same recurrence relation as for $b_d[n]$. Assuming the system starts from rest (zero initial conditions before the input starts), the solution to the difference equation is unique. Thus, $b_d[n] = b[n-n_0]$.\nThe output for the delayed input is:\n$y_d[n] = x_d[n] - b_d[n] = x[n-n_0] - b[n-n_0]$.\nThe right-hand side is precisely the definition of the original output delayed by $n_0$, i.e., $y[n-n_0]$.\nSo, $y_d[n] = y[n-n_0]$, proving time-invariance.\n\nPart $2$: Derive a first-order difference equation relating $y[n]$, $y[n-1]$, $x[n]$, and $x[n-1]$.\nWe start with the given equations:\n(1) $y[n] = x[n] - b[n]$\n(2) $b[n] = \\gamma b[n-1] + (1-\\gamma)x[n]$\nFrom (1), we express the baseline in terms of the input and output: $b[n] = x[n] - y[n]$.\nConsequently, the delayed baseline is $b[n-1] = x[n-1] - y[n-1]$.\nSubstitute these expressions for $b[n]$ and $b[n-1]$ into equation (2):\n$x[n] - y[n] = \\gamma(x[n-1] - y[n-1]) + (1-\\gamma)x[n]$\nWe now rearrange this equation to group terms involving $y$ and $x$:\n$y[n] = x[n] - \\gamma(x[n-1] - y[n-1]) - (1-\\gamma)x[n]$\n$y[n] = x[n] - \\gamma x[n-1] + \\gamma y[n-1] - x[n] + \\gamma x[n]$\n$y[n] = \\gamma y[n-1] + \\gamma x[n] - \\gamma x[n-1]$\nThis can be written as:\n$y[n] - \\gamma y[n-1] = \\gamma x[n] - \\gamma x[n-1]$\n\nPart $3$: Derive the frequency response $H(\\exp(j\\omega))$.\nThe frequency response is the Discrete-Time Fourier Transform (DTFT) of the system's impulse response, which can be found from the difference equation by applying the DTFT. Let $X(\\exp(j\\omega))$ and $Y(\\exp(j\\omega))$ be the DTFTs of $x[n]$ and $y[n]$, respectively. We use the time-shifting property of the DTFT: $\\mathcal{F}\\{z[n-n_0]\\} = \\exp(-j\\omega n_0) Z(\\exp(j\\omega))$.\nApplying the DTFT to the difference equation from Part $2$:\n$Y(\\exp(j\\omega)) - \\gamma \\exp(-j\\omega) Y(\\exp(j\\omega)) = \\gamma X(\\exp(j\\omega)) - \\gamma \\exp(-j\\omega) X(\\exp(j\\omega))$\nFactor out the transforms $Y(\\exp(j\\omega))$ and $X(\\exp(j\\omega))$:\n$Y(\\exp(j\\omega))(1 - \\gamma \\exp(-j\\omega)) = X(\\exp(j\\omega))(\\gamma - \\gamma \\exp(-j\\omega))$\nThe frequency response $H(\\exp(j\\omega))$ is the ratio $Y(\\exp(j\\omega)) / X(\\exp(j\\omega))$:\n$H(\\exp(j\\omega)) = \\frac{\\gamma - \\gamma \\exp(-j\\omega)}{1 - \\gamma \\exp(-j\\omega)}$\nFactoring out $\\gamma$ from the numerator gives the final closed-form expression:\n$H(\\exp(j\\omega)) = \\frac{\\gamma(1 - \\exp(-j\\omega))}{1 - \\gamma \\exp(-j\\omega)}$\n\nPart $4$: Discuss stability, phase, minimum-phase property, and group delay.\nThe transfer function in the Z-domain is obtained by substituting $z = \\exp(j\\omega)$:\n$H(z) = \\frac{\\gamma(1 - z^{-1})}{1 - \\gamma z^{-1}} = \\frac{\\gamma(z - 1)}{z - \\gamma}$\nPole-zero locations: The filter has a single pole at $p = \\gamma$ and a single zero at $z = 1$.\nStability: A discrete-time LTI system is stable if and only if all its poles lie strictly inside the unit circle in the z-plane. The pole is at $z=\\gamma$. Since the problem specifies $\\gamma \\in (0,1)$, we have $|\\gamma| < 1$. The pole is inside the unit circle, so the system is stable.\nQualitative phase response: The zero at $z=1$ corresponds to a frequency of $\\omega=0$. This means the filter completely blocks DC signals ($H(\\exp(j0))=0$), which is characteristic of a high-pass or band-pass filter. The phase response $\\arg[H(\\exp(j\\omega))]$ can be shown to start at $\\pi/2$ for $\\omega \\to 0^+$ and decrease to $0$ as $\\omega \\to \\pi$. This positive, decreasing phase is characteristic of a simple high-pass filter.\nMinimum-phase property: A causal and stable LTI system is said to be minimum-phase if all its zeros also lie strictly inside the unit circle. The inverse system $H^{-1}(z)$ would have a pole at $z=1$, which is on the unit circle, making the inverse system unstable. Therefore, the system $H(z)$ is not minimum-phase.\nGroup delay: The group delay is $\\tau_g(\\omega) = -\\frac{d}{d\\omega} \\arg[H(\\exp(j\\omega))]$. The phase is $\\arg[H(\\exp(j\\omega))] = \\frac{\\pi}{2} - \\frac{\\omega}{2} - \\arctan\\left(\\frac{\\gamma \\sin(\\omega)}{1-\\gamma\\cos(\\omega)}\\right)$. Differentiating this yields $\\tau_g(\\omega) = \\frac{1}{2} + \\frac{\\gamma\\cos(\\omega) - \\gamma^2}{1 - 2\\gamma\\cos(\\omega) + \\gamma^2}$. At low frequencies ($\\omega \\approx 0$), $\\tau_g(0) = \\frac{1}{2} + \\frac{\\gamma}{1-\\gamma}$. For $\\gamma \\to 1$, this delay becomes very large, reflecting the long memory needed to estimate a very slow trend. At high frequencies ($\\omega = \\pi$), $\\tau_g(\\pi) = \\frac{1-\\gamma}{2(1+\\gamma)}$. For $\\gamma \\to 1$, this delay approaches $0$, meaning high-frequency components pass through nearly instantaneously. The frequency-dependent group delay indicates that the filter introduces phase distortion.\n\nPart $5$: Relate $\\gamma$ to the $-3$ dB cutoff frequency $f_c$.\nThe gain of this high-pass filter is maximized at the highest frequency, $\\omega = \\pi$ (Nyquist frequency). The magnitude a this frequency is the asymptotic high-frequency gain:\n$|H(\\exp(j\\pi))| = \\left|\\frac{\\gamma(1 - \\exp(-j\\pi))}{1 - \\gamma \\exp(-j\\pi)}\\right| = \\left|\\frac{\\gamma(1 - (-1))}{1 - \\gamma(-1)}\\right| = \\frac{2\\gamma}{1+\\gamma}$.\nThe power gain at this frequency is $|H(\\exp(j\\pi))|^2 = \\frac{4\\gamma^2}{(1+\\gamma)^2}$.\nThe $-3$ dB frequency, $\\omega_c$, is where the power gain is half of this maximum value.\n$|H(\\exp(j\\omega_c))|^2 = \\frac{1}{2} |H(\\exp(j\\pi))|^2$.\nThe magnitude squared of the frequency response is:\n$|H(\\exp(j\\omega))|^2 = \\frac{\\gamma^2(1 - \\exp(-j\\omega))(1 - \\exp(j\\omega))}{(1 - \\gamma\\exp(-j\\omega))(1 - \\gamma\\exp(j\\omega))} = \\frac{\\gamma^2(2 - 2\\cos(\\omega))}{1 - 2\\gamma\\cos(\\omega) + \\gamma^2}$.\nSetting up the cutoff condition:\n$\\frac{\\gamma^2(2 - 2\\cos(\\omega_c))}{1 - 2\\gamma\\cos(\\omega_c) + \\gamma^2} = \\frac{1}{2}\\frac{4\\gamma^2}{(1+\\gamma)^2} = \\frac{2\\gamma^2}{(1+\\gamma)^2}$\n$\\frac{1 - \\cos(\\omega_c)}{1 - 2\\gamma\\cos(\\omega_c) + \\gamma^2} = \\frac{1}{(1+\\gamma)^2}$\n$(1 - \\cos(\\omega_c))(1+2\\gamma+\\gamma^2) = 1 - 2\\gamma\\cos(\\omega_c) + \\gamma^2$\n$1 + 2\\gamma + \\gamma^2 - \\cos(\\omega_c) - 2\\gamma\\cos(\\omega_c) - \\gamma^2\\cos(\\omega_c) = 1 - 2\\gamma\\cos(\\omega_c) + \\gamma^2$\n$2\\gamma - \\cos(\\omega_c) - \\gamma^2\\cos(\\omega_c) = 0$\n$2\\gamma = \\cos(\\omega_c)(1 + \\gamma^2)$\nThis gives the implicit condition: $\\cos(\\omega_c) = \\frac{2\\gamma}{1+\\gamma^2}$.\nSolving for $f_c$ using $\\omega_c = 2\\pi f_c / f_s$:\n$f_c = \\frac{f_s}{2\\pi} \\arccos\\left(\\frac{2\\gamma}{1+\\gamma^2}\\right)$.\nIn the regime $\\gamma \\to 1$, the argument of the arccosine approaches $\\frac{2(1)}{1+1^2}=1$. Since $\\arccos(1)=0$, this means $\\omega_c \\to 0$ and thus $f_c \\to 0$. This is physically consistent: as $\\gamma$ approaches $1$, the time constant $\\tau=-T_s/\\ln(\\gamma)$ becomes very long, meaning the EWMA is a very low-pass filter tracking only extremely slow drifts. Consequently, the high-pass filter formed by subtraction has a cutoff frequency that approaches $0$, rejecting only these very slow components.",
            "answer": "$$\\boxed{\\frac{\\gamma (1 - \\exp(-j\\omega))}{1 - \\gamma \\exp(-j\\omega)}}$$"
        },
        {
            "introduction": "Moving from general filter theory to a specific application, this problem addresses a standard practice in fMRI data analysis. Slow signal drifts in fMRI are commonly removed by including a set of cosine functions in the General Linear Model. This practice will guide you through the derivation of how a user-specified cutoff period, defined in seconds, translates into the precise number of Discrete Cosine Transform (DCT) basis regressors needed to implement this high-pass filtering .",
            "id": "4155663",
            "problem": "In functional Magnetic Resonance Imaging (fMRI), the General Linear Model (GLM) is often augmented with low-frequency drift regressors to remove slowly varying confounds via high-pass filtering. A common construction uses a Discrete Cosine Transform (DCT) basis on the acquisition grid. Consider a single run consisting of $T$ samples acquired at uniform Repetition Time (TR) $\\Delta t$ seconds, so the run duration is $L = T \\Delta t$ seconds. Let the set of DCT basis functions be indexed by $m \\in \\{0,1,2,\\dots\\}$ and defined on sample indices $n \\in \\{0,1,\\dots,T-1\\}$.\n\nStarting from the definition of the Discrete Cosine Transform (DCT) type-II basis and standard sampling relationships between discrete-time angular frequency (in radians per sample) and continuous-time angular frequency (in radians per second), perform the following:\n\n1. Define the DCT type-II basis functions $b_{m}(n)$ on the sample grid, including the appropriate normalization constants so that the set is orthonormal on $\\{0,1,\\dots,T-1\\}$.\n2. For each index $m$, determine the continuous-time angular frequency and the corresponding period (in seconds) of $b_{m}(n)$ when mapped to the acquisition times $t_{n} = n \\Delta t$.\n3. Given a high-pass cutoff $c$ seconds, construct the criterion on the index $m$ for which a DCT component should be included to model low-frequency drift (i.e., include those components whose continuous-time period is at least $c$ seconds).\n4. Derive, in closed form, the total number of DCT regressors $N$ included by this criterion in terms of $T$, $\\Delta t$, and $c$.\n\nYour final answer must be a single closed-form analytic expression for $N$ as a function of $T$, $\\Delta t$, and $c$. Do not include any units in the final answer box.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of signal processing as applied to fMRI data analysis, well-posed with a clear objective and sufficient information, and uses objective, formal language. There are no contradictions, ambiguities, or factual errors. I will proceed with the solution.\n\nThe problem asks for the number of Discrete Cosine Transform (DCT) regressors used for high-pass filtering in an fMRI General Linear Model (GLM). This number is determined by a cutoff period, `c`. The derivation will proceed in four steps as outlined in the problem.\n\n**1. Orthonormal DCT Type-II Basis Functions**\n\nWe consider a time series of `T` samples. The indices for the samples are `n \\in \\{0, 1, \\dots, T-1\\}`. The DCT type-II basis functions are indexed by `m \\in \\{0, 1, \\dots, T-1\\}`. A basis function `b_m(n)` is proportional to a cosine function. The general form is:\n$$\nb_m(n) = C_m \\cos\\left(\\frac{\\pi m (2n+1)}{2T}\\right) = C_m \\cos\\left(\\frac{\\pi m (n + 1/2)}{T}\\right)\n$$\nwhere `C_m` is a normalization constant that ensures the basis set is orthonormal. Orthonormality requires that the inner product of any two basis vectors is the Kronecker delta:\n$$\n\\sum_{n=0}^{T-1} b_m(n) b_k(n) = \\delta_{mk}\n$$\nThe orthogonality of these cosine functions for `m \\neq k` is a standard property of the DCT. We need to enforce the normalization condition, `\\sum_{n=0}^{T-1} b_m(n)^2 = 1`.\n\nFor `m=0`:\nThe basis function is constant, as `\\cos(0) = 1`.\n$$\nb_0(n) = C_0\n$$\nThe normalization condition is:\n$$\n\\sum_{n=0}^{T-1} (C_0)^2 = T (C_0)^2 = 1 \\implies C_0 = \\frac{1}{\\sqrt{T}}\n$$\nSo, `b_0(n) = \\frac{1}{\\sqrt{T}}`.\n\nFor `m \\in \\{1, 2, \\dots, T-1\\}`:\nThe normalization condition is:\n$$\n\\sum_{n=0}^{T-1} \\left[C_m \\cos\\left(\\frac{\\pi m (n + 1/2)}{T}\\right)\\right]^2 = 1\n$$\n$$\n(C_m)^2 \\sum_{n=0}^{T-1} \\cos^2\\left(\\frac{\\pi m (n + 1/2)}{T}\\right) = 1\n$$\nUsing the identity `\\cos^2(\\theta) = \\frac{1 + \\cos(2\\theta)}{2}`, the sum becomes:\n$$\n\\sum_{n=0}^{T-1} \\frac{1}{2} \\left[1 + \\cos\\left(\\frac{2\\pi m (n + 1/2)}{T}\\right)\\right] = \\frac{T}{2} + \\frac{1}{2} \\sum_{n=0}^{T-1} \\cos\\left(\\frac{2\\pi m n}{T} + \\frac{\\pi m}{T}\\right)\n$$\nThe sum of a cosine over a full period with a phase offset is zero. Thus, the sum evaluates to `\\frac{T}{2}`. The normalization condition becomes:\n$$\n(C_m)^2 \\left(\\frac{T}{2}\\right) = 1 \\implies C_m = \\sqrt{\\frac{2}{T}}\n$$\nSo, the orthonormal DCT-II basis functions are:\n$$\nb_0(n) = \\frac{1}{\\sqrt{T}}\n$$\n$$\nb_m(n) = \\sqrt{\\frac{2}{T}} \\cos\\left(\\frac{\\pi m (n + 1/2)}{T}\\right) \\quad \\text{for } m \\in \\{1, 2, \\dots, T-1\\}\n$$\n\n**2. Continuous-time Frequency and Period**\n\nA discrete-time signal `x(n)` is represented as `A \\cos(\\omega_d n + \\phi)`, where `\\omega_d` is the discrete-time angular frequency in radians per sample. By inspecting the argument of our basis function `b_m(n)`, we can identify its discrete-time frequency.\n$$\n\\cos\\left(\\frac{\\pi m (n + 1/2)}{T}\\right) = \\cos\\left(\\frac{\\pi m n}{T} + \\frac{\\pi m}{2T}\\right)\n$$\nFrom this, we identify the discrete-time angular frequency for the `m`-th component as:\n$$\n\\omega_{d,m} = \\frac{\\pi m}{T} \\quad (\\text{radians/sample})\n$$\nThe relationship between continuous-time angular frequency `\\omega_c` (in rad/s) and discrete-time angular frequency `\\omega_d` is given by `\\omega_c = \\omega_d / \\Delta t`, where `\\Delta t` is the sampling period (the TR).\nTherefore, the continuous-time angular frequency for the `m`-th component is:\n$$\n\\omega_{c,m} = \\frac{\\omega_{d,m}}{\\Delta t} = \\frac{\\pi m}{T \\Delta t} \\quad (\\text{rad/s})\n$$\nThe period `P` is related to the angular frequency by `P = 2\\pi / \\omega_c`. Thus, the continuous-time period of the `m`-th basis function, `P_m`, is:\n$$\nP_m = \\frac{2\\pi}{\\omega_{c,m}} = \\frac{2\\pi}{\\left(\\frac{\\pi m}{T \\Delta t}\\right)} = \\frac{2 T \\Delta t}{m} \\quad (\\text{seconds})\n$$\nThis formula is valid for `m > 0`. For the `m=0` component, `\\omega_{c,0} = 0`, which corresponds to a DC offset with an infinite period, `P_0 \\to \\infty`. Our formula correctly reflects this in the limit as `m \\to 0`.\n\n**3. Criterion for Including DCT Components**\n\nThe problem requires us to include DCT components that model low-frequency drift. This is achieved by including all components whose period `P_m` is greater than or equal to a specified high-pass cutoff `c`. The criterion is:\n$$\nP_m \\ge c\n$$\nWe apply this criterion to the set of possible indices `m \\in \\{0, 1, 2, \\dots\\}`.\n\nFor `m=0`:\nThe period is infinite, `P_0 = \\infty`. For any finite cutoff `c > 0`, the condition `\\infty \\ge c` is always satisfied. Thus, the `m=0` component (the DC component) is always included.\n\nFor `m > 0`:\nWe apply the criterion to the expression for `P_m`:\n$$\n\\frac{2 T \\Delta t}{m} \\ge c\n$$\nSince `m > 0` and `c > 0`, we can rearrange the inequality without changing its direction:\n$$\n2 T \\Delta t \\ge m c\n$$\n$$\nm \\le \\frac{2 T \\Delta t}{c}\n$$\nSo, for `m > 0`, we include all integer indices `m` that are less than or equal to `\\frac{2 T \\Delta t}{c}`.\n\n**4. Total Number of DCT Regressors**\n\nThe total number of regressors, `N`, is the total count of integer indices `m` that satisfy the criterion. The set of included indices is formed by combining the results for `m=0` and `m>0`.\nThe included indices `m` are the non-negative integers such that:\n- `m = 0`, or\n- `m` is an integer `> 0` and `m \\le \\frac{2 T \\Delta t}{c}`.\n\nThe set of positive integers satisfying the condition is `\\{1, 2, \\dots, \\lfloor \\frac{2 T \\Delta t}{c} \\rfloor \\}`, where `\\lfloor \\cdot \\rfloor` is the floor function, which gives the greatest integer less than or equal to its argument. The number of elements in this set is `\\lfloor \\frac{2 T \\Delta t}{c} \\rfloor`.\n\nThe total number of included regressors `N` is the size of this set of positive integers plus one (for the `m=0` case).\n$$\nN = \\left( \\text{count of } m > 0 \\right) + \\left( \\text{count of } m=0 \\right)\n$$\n$$\nN = \\left\\lfloor \\frac{2 T \\Delta t}{c} \\right\\rfloor + 1\n$$\nThis is the final closed-form expression for the total number of DCT regressors.",
            "answer": "$$\\boxed{\\left\\lfloor \\frac{2 T \\Delta t}{c} \\right\\rfloor + 1}$$"
        },
        {
            "introduction": "Detrending methods are powerful but can be dangerous if applied blindly, as they risk removing the very signal you hope to studyâ€”a phenomenon known as overfitting. This hands-on coding exercise challenges you to simulate a scenario where polynomial detrending incorrectly absorbs a slow, task-related oscillation. You will implement key statistical diagnostics, including analysis of the residual spectrum and model comparison tests, to learn how to detect and guard against this critical pitfall in data analysis .",
            "id": "4155651",
            "problem": "You are given a time series analysis task in the context of neuroscience data analysis focused on detrending and baseline correction. The objective is to construct a scientifically realistic scenario in which polynomial detrending of degree $3$ removes part of a slow task-related oscillatory component (overfitting) and to implement diagnostics to detect this. Assume a one-dimensional time series $y[n]$ sampled at frequency $f_s$ in hertz over $N$ samples, with time $t_n$ in seconds defined by $t_n = n / f_s$ for $n \\in \\{0,1,\\dots,N-1\\}$. The signal is modeled as the sum of a baseline drift (a low-order polynomial in $t_n$), a slow task-related oscillation, and additive independent zero-mean Gaussian noise. All amplitudes are dimensionless. Angles used in trigonometric functions are in radians. No physical units besides seconds and hertz are involved, and all frequencies must be treated in hertz.\n\nStarting from the core definitions below, you must implement a complete program that performs polynomial detrending of degree $3$ (cubic) using ordinary least squares and then computes diagnostics that can indicate overfitting when the detrending removes non-baseline task-related content.\n\nFundamental bases to use:\n\n- Ordinary least squares for linear models with design matrix columns formed by powers of centered and scaled time, where a degree-$p$ polynomial model has $p+1$ parameters and minimizes the sum of squared residuals.\n- The Discrete Fourier Transform (DFT) to define the residual spectrum and the periodogram as squared magnitudes of DFT coefficients at frequencies $k f_s / N$, for integer $k$.\n- The hat matrix definition for linear models and its diagonal entries (leverage), where leverage quantifies the influence of each sample on the fit.\n- The nested-model $F$-test comparing a lower-degree polynomial model to a higher-degree model.\n\nYour program must:\n\n1. Construct the cubic polynomial design matrix with columns $[1, \\tilde{t}, \\tilde{t}^2, \\tilde{t}^3]$, where $\\tilde{t}$ is the centered-and-scaled time defined by $\\tilde{t}_n = (t_n - \\bar{t}) / (t_{\\max} - t_{\\min})$, with $\\bar{t}$ the mean time, $t_{\\max}$ the maximum time, and $t_{\\min}$ the minimum time. Also construct the linear model design matrix with columns $[1, \\tilde{t}]$.\n2. Fit both polynomial models by ordinary least squares to the observed $y[n]$ and compute residuals.\n3. Compute the residual spectrum diagnostic by:\n   - Performing the Discrete Fourier Transform on the original signal and on the cubic-detrended residual using the real-valued DFT over nonnegative frequencies.\n   - Identifying the nearest DFT bin to the known task frequency $f_{\\text{task}}$ and computing the periodogram power at that bin both before detrending and after cubic detrending.\n   - Forming the suppression ratio $r = P_{\\text{res}} / P_{\\text{orig}}$, where $P_{\\text{res}}$ is the residual power and $P_{\\text{orig}}$ is the original power at the nearest bin to $f_{\\text{task}}$; to avoid division by an extremely small value, when $P_{\\text{orig}}$ is less than a small constant $\\epsilon$, set $r = 1$.\n4. Compute the leverage diagnostic by:\n   - Forming the hat matrix $H = X (X^\\top X)^{-1} X^\\top$ for the cubic model, where $X$ is the cubic design matrix.\n   - Extracting the diagonal entries $h_{ii}$ and computing $h_{\\max} = \\max_i h_{ii}$.\n5. Compute the nested-model $F$-test diagnostic by:\n   - Using the linear model as the reduced model with $p_1 = 2$ parameters and the cubic model as the full model with $p_3 = 4$ parameters.\n   - Computing residual sums of squares $\\mathrm{RSS}_1$ and $\\mathrm{RSS}_3$.\n   - Computing the $F$-statistic with numerator degrees $m = p_3 - p_1 = 2$ and denominator degrees $N - p_3$.\n   - Computing the associated tail probability (p-value) for the $F$-distribution.\n6. Decide whether overfitting is detected by requiring that at least two of the three diagnostics indicate overfitting using the following thresholds:\n   - Residual spectrum suppression: $r < 0.3$.\n   - High leverage: $h_{\\max} > 3 p_3 / N$, where $p_3 = 4$.\n   - Significant $F$-test: $p\\text{-value} < 0.01$.\n   The final decision is a boolean indicating whether at least two of these conditions are satisfied.\n\nTest suite specification:\n\nImplement and run your program on the following test cases. In all cases, use sampling frequency $f_s = 1$ hertz, and set the noise standard deviation to the given value. The oscillatory component is $A \\sin(2 \\pi f_{\\text{task}} t_n)$, with amplitude $A$ and frequency $f_{\\text{task}}$. The baseline drift polynomial is $b(t_n) = c_0 + c_1 t_n + c_2 t_n^2 + c_3 t_n^3$.\n\n- Test case $1$ (expected to be a general case where overfitting is present): $N = 600$, $A = 2.0$, $f_{\\text{task}} = 1 / 600$, $(c_0, c_1, c_2, c_3) = (0.0, 0.0, 0.0, 0.0)$, noise standard deviation $= 0.05$.\n- Test case $2$ (boundary case with no task oscillation): $N = 600$, $A = 0.0$, $f_{\\text{task}} = 1 / 600$, $(c_0, c_1, c_2, c_3) = (0.0, 0.01, 0.0, 0.0)$, noise standard deviation $= 0.05$.\n- Test case $3$ (edge case with short duration and half-cycle oscillation): $N = 80$, $A = 1.5$, $f_{\\text{task}} = 1 / 160$, $(c_0, c_1, c_2, c_3) = (0.0, 0.0, 0.0, 0.0)$, noise standard deviation $= 0.10$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result must be a boolean indicating overfitting detected ($\\mathrm{True}$) or not ($\\mathrm{False}$) for the corresponding test case, evaluated by the decision rule in item $6$ above. No additional text should be printed. Use a fixed random seed for the noise to ensure reproducibility.\n\nAll mathematical entities must be written using LaTeX notation in this problem statement. The time unit is seconds and the frequency unit is hertz. No angle units other than radians are used, and no percentages are used; ratios such as $r$ must be treated as decimal numbers.",
            "solution": "We begin from the foundational definitions of ordinary least squares, the Discrete Fourier Transform, the hat matrix and leverage, and the nested-model $F$-test. We will integrate these into an algorithmic pipeline to simulate data, detrend by a cubic polynomial, and compute diagnostics to detect overfitting.\n\nSignal model and sampling: The observed signal is $y[n] = b(t_n) + s(t_n) + \\eta[n]$, where $b(t)$ is a baseline polynomial drift, $s(t)$ is a slow task-related oscillation, and $\\eta[n]$ is additive independent Gaussian noise with zero mean and specified standard deviation. We have $t_n = n / f_s$ for $n \\in \\{0,1,\\dots,N-1\\}$ and $f_s = 1$ hertz, so $t_n$ is in seconds. The oscillatory component is $s(t_n) = A \\sin(2 \\pi f_{\\text{task}} t_n)$ with amplitude $A$ and frequency $f_{\\text{task}}$ in hertz. The baseline drift is a polynomial $b(t_n) = c_0 + c_1 t_n + c_2 t_n^2 + c_3 t_n^3$ in the test cases.\n\nPolynomial regression and detrending: For a polynomial of degree $p$, the linear model can be written as $y = X \\beta + \\varepsilon$, where $X$ is the design matrix, $\\beta$ is the parameter vector, and $\\varepsilon$ is noise. To promote numerical stability and interpretability, we form the time regressor by centering and scaling time:\n$$\n\\tilde{t}_n = \\frac{t_n - \\bar{t}}{t_{\\max} - t_{\\min}},\n$$\nwhere $\\bar{t}$ is the mean of the time samples, $t_{\\max}$ is the maximum time, and $t_{\\min}$ is the minimum time. For the degree-$3$ model, the design matrix is\n$$\nX_3 = \\begin{bmatrix}\n1 & \\tilde{t}_0 & \\tilde{t}_0^2 & \\tilde{t}_0^3 \\\\\n1 & \\tilde{t}_1 & \\tilde{t}_1^2 & \\tilde{t}_1^3 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & \\tilde{t}_{N-1} & \\tilde{t}_{N-1}^2 & \\tilde{t}_{N-1}^3\n\\end{bmatrix},\n$$\nand for the degree-$1$ model, the design matrix is\n$$\nX_1 = \\begin{bmatrix}\n1 & \\tilde{t}_0 \\\\\n1 & \\tilde{t}_1 \\\\\n\\vdots & \\vdots \\\\\n1 & \\tilde{t}_{N-1}\n\\end{bmatrix}.\n$$\nThe ordinary least squares estimate minimizes $\\sum_{n=0}^{N-1} (y[n] - (X \\beta)[n])^2$ and is given by\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y.\n$$\nThe residuals are $r = y - X \\hat{\\beta}$. Detrending consists of subtracting the fitted polynomial component $X_3 \\hat{\\beta}_3$ from $y$ to yield the residual signal $r_3$.\n\nResidual spectrum diagnostic: To assess whether the detrending has removed power at the task frequency, we compute the periodogram at the DFT bin nearest to $f_{\\text{task}}$. Define the real-valued Discrete Fourier Transform over nonnegative frequencies via\n$$\nY[k] = \\sum_{n=0}^{N-1} y[n] e^{-2 \\pi i n k / N}, \\quad k = 0,1,\\dots,\\left\\lfloor \\frac{N}{2} \\right\\rfloor,\n$$\nwith corresponding frequencies $\\omega_k = k f_s / N$. We identify the index $k^*$ that minimizes $| \\omega_k - f_{\\text{task}} |$. The periodogram power at $k^*$ is $P_{\\text{orig}} = |Y[k^*]|^2 / N$. For the residual after cubic detrending, compute its transform $R_3[k]$ and power $P_{\\text{res}} = |R_3[k^*]|^2 / N$. The suppression ratio is\n$$\nr = \\begin{cases}\n\\frac{P_{\\text{res}}}{P_{\\text{orig}}}, & P_{\\text{orig}} \\ge \\epsilon, \\\\\n1, & P_{\\text{orig}} < \\epsilon,\n\\end{cases}\n$$\nwhere $\\epsilon$ is a small positive constant to prevent division by a negligible denominator. Overfitting is indicated if $r < 0.3$, signifying that the cubic detrend has suppressed the task-related power substantially.\n\nLeverage diagnostic: The hat matrix for the cubic model is\n$$\nH_3 = X_3 (X_3^\\top X_3)^{-1} X_3^\\top,\n$$\nwhose diagonal entries $h_{ii}$ quantify the leverage of sample $i$. A property of the hat matrix is $\\mathrm{trace}(H_3) = p_3$, where $p_3 = 4$ is the number of parameters in the cubic model. The average leverage is $p_3 / N$. High leverage relative to the average suggests potential overfitting or instability. We compute $h_{\\max} = \\max_i h_{ii}$ and declare high leverage if\n$$\nh_{\\max} > 3 \\frac{p_3}{N}.\n$$\n\nNested-model $F$-test diagnostic: To compare the linear model (reduced) with parameter count $p_1 = 2$ to the cubic model (full) with parameter count $p_3 = 4$, compute residual sums of squares\n$$\n\\mathrm{RSS}_1 = \\| y - X_1 \\hat{\\beta}_1 \\|_2^2, \\quad \\mathrm{RSS}_3 = \\| y - X_3 \\hat{\\beta}_3 \\|_2^2.\n$$\nThe $F$-statistic is\n$$\nF = \\frac{(\\mathrm{RSS}_1 - \\mathrm{RSS}_3)/m}{\\mathrm{RSS}_3 / (N - p_3)},\n$$\nwhere $m = p_3 - p_1 = 2$. Under the null hypothesis that the additional cubic terms do not improve the fit beyond noise, $F$ follows the $F$-distribution with $m$ and $N - p_3$ degrees of freedom. Compute the tail probability ($p$-value) as $p = \\Pr(F_{m, N-p_3} \\ge F)$. We declare significance if\n$$\np < 0.01.\n$$\n\nDecision rule: We detect overfitting if at least two of the following are true:\n- $r < 0.3$ (residual spectrum suppression),\n- $h_{\\max} > 3 p_3 / N$ (high leverage),\n- $p < 0.01$ (significant $F$-test).\n\nRationale: If cubic detrending removes genuine task-related slow oscillatory content, the periodogram at the task frequency will show reduced power relative to the original signal. The leverage criterion guards against fits dominated by extreme samples, which is more likely in short or poorly conditioned designs. The $F$-test indicates that the additional polynomial degrees provide a statistically significant reduction in residual variance, which may be evidence of modeling content beyond baseline drift. Requiring at least two conditions reduces false positives from any single diagnostic.\n\nTest suite coverage: The specified test cases exercise multiple facets:\n- Test case $1$ simulates a slow oscillation with one cycle across the entire duration ($f_{\\text{task}} = 1/600$ hertz). A cubic polynomial can absorb nontrivial low-frequency structure, leading to suppression and significant model improvement, thus overfitting should be detected.\n- Test case $2$ has no oscillation ($A = 0$), so suppression at $f_{\\text{task}}$ is not expected; the leverage and $F$-test may not simultaneously cross thresholds, and overfitting should not be detected.\n- Test case $3$ has short duration $N = 80$ with a half-cycle oscillation ($f_{\\text{task}} = 1/160$ hertz). The cubic model may overfit due to higher leverage (larger $h_{\\max}$ compared to average) and suppression of low-frequency power, leading to detection.\n\nAlgorithmic implementation steps:\n- For each test case, simulate $y[n]$ given $N$, $f_s$, $A$, $f_{\\text{task}}$, baseline coefficients $(c_0, c_1, c_2, c_3)$, and noise standard deviation. Use a fixed random seed to ensure reproducibility.\n- Build $X_1$ and $X_3$ with centered-and-scaled $\\tilde{t}$.\n- Fit $\\hat{\\beta}_1$ and $\\hat{\\beta}_3$ by ordinary least squares and compute residuals.\n- Compute the real-valued DFT over nonnegative frequencies for original and residual signals, find $k^*$ corresponding to $f_{\\text{task}}$, and compute powers $P_{\\text{orig}}$ and $P_{\\text{res}}$ and the ratio $r$.\n- Compute $h_{ii}$ via $H_3$ and find $h_{\\max}$.\n- Compute the $F$-statistic and $p$-value using the $F$-distribution.\n- Apply the decision rule to produce a boolean for each case.\n\nThe final output will be a single line containing a list of booleans $[\\mathrm{result}_1, \\mathrm{result}_2, \\mathrm{result}_3]$, corresponding to the test cases in order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef build_time(N, Fs):\n    t = np.arange(N) / Fs\n    t_center = t.mean()\n    t_range = t.max() - t.min()\n    # Center and scale to stabilize polynomial columns\n    t_tilde = (t - t_center) / (t_range if t_range != 0 else 1.0)\n    return t, t_tilde\n\ndef design_matrix_poly(t_tilde, degree):\n    # Columns: [1, t, t^2, ..., t^degree]\n    cols = [np.ones_like(t_tilde)]\n    for d in range(1, degree + 1):\n        cols.append(t_tilde ** d)\n    X = np.column_stack(cols)\n    return X\n\ndef ols_fit_and_residual(y, X):\n    XtX = X.T @ X\n    XtX_inv = np.linalg.inv(XtX)\n    beta = XtX_inv @ (X.T @ y)\n    y_hat = X @ beta\n    residual = y - y_hat\n    return beta, residual, XtX_inv\n\ndef periodogram_power_at_freq(signal, Fs, f_task, eps=1e-12):\n    N = signal.size\n    # Real FFT non-negative frequencies\n    spec = np.fft.rfft(signal)\n    freqs = np.fft.rfftfreq(N, d=1.0/Fs)\n    # Nearest bin to task frequency\n    idx = int(np.argmin(np.abs(freqs - f_task)))\n    power = (np.abs(spec[idx]) ** 2) / N\n    return power, idx\n\ndef leverage_diag(X, XtX_inv):\n    # Hat matrix H = X (X^T X)^{-1} X^T\n    # Diagonal h_ii = row_i(X) * (X^T X)^{-1} * row_i(X)^T\n    # Efficient computation: For each row x_i, h_ii = x_i^T * XtX_inv * x_i\n    # Compute B = XtX_inv\n    B = XtX_inv\n    # h_ii for each row\n    h_diag = np.sum((X @ B) * X, axis=1)\n    return h_diag\n\ndef f_test_nested(y, X_reduced, X_full):\n    # Fit reduced and full models\n    _, res_reduced, _ = ols_fit_and_residual(y, X_reduced)\n    _, res_full, _ = ols_fit_and_residual(y, X_full)\n    RSS_reduced = float(res_reduced.T @ res_reduced)\n    RSS_full = float(res_full.T @ res_full)\n    p_reduced = X_reduced.shape[1]\n    p_full = X_full.shape[1]\n    N = y.size\n    m = p_full - p_reduced\n    df_den = N - p_full\n    # Guard against degenerate denominators\n    if df_den <= 0 or m <= 0:\n        return np.nan, 1.0\n    # F statistic\n    num = (RSS_reduced - RSS_full) / m\n    den = RSS_full / df_den\n    # If numerical issues produce negative num, clip at zero (no improvement)\n    if num < 0:\n        num = 0.0\n    F = num / den if den > 0 else np.inf\n    # p-value from F-distribution survival function\n    p_value = f_dist.sf(F, m, df_den) if np.isfinite(F) else 0.0\n    return F, p_value\n\ndef detect_overfitting(y, Fs, f_task, t_tilde, eps=1e-12):\n    # Build design matrices\n    X1 = design_matrix_poly(t_tilde, degree=1)\n    X3 = design_matrix_poly(t_tilde, degree=3)\n    # Fit models\n    _, res1, _ = ols_fit_and_residual(y, X1)\n    beta3, res3, XtX_inv3 = ols_fit_and_residual(y, X3)\n    # Residual spectrum suppression\n    P_orig, idx = periodogram_power_at_freq(y, Fs, f_task, eps=eps)\n    P_res, _ = periodogram_power_at_freq(res3, Fs, f_task, eps=eps)\n    r = P_res / P_orig if P_orig >= eps else 1.0\n    # Leverage\n    h_diag = leverage_diag(X3, XtX_inv3)\n    h_max = float(np.max(h_diag))\n    p3 = X3.shape[1]\n    N = y.size\n    leverage_threshold = 3.0 * p3 / N\n    high_leverage = h_max > leverage_threshold\n    # F-test\n    F_stat, p_value = f_test_nested(y, X1, X3)\n    # Decision: at least two diagnostics positive\n    suppression_positive = r < 0.3\n    ftest_positive = p_value < 0.01\n    positives = sum([suppression_positive, high_leverage, ftest_positive])\n    overfit = positives >= 2\n    return overfit, {\n        \"r\": r,\n        \"h_max\": h_max,\n        \"lev_thr\": leverage_threshold,\n        \"F\": F_stat,\n        \"p_value\": p_value\n    }\n\ndef simulate_signal(N, Fs, A_task, f_task, coeffs, noise_sigma, rng):\n    t, t_tilde = build_time(N, Fs)\n    c0, c1, c2, c3 = coeffs\n    baseline = c0 + c1 * t + c2 * (t ** 2) + c3 * (t ** 3)\n    oscillation = A_task * np.sin(2.0 * np.pi * f_task * t)\n    noise = rng.normal(loc=0.0, scale=noise_sigma, size=N)\n    y = baseline + oscillation + noise\n    return y, t_tilde\n\ndef solve():\n    # Fixed random seed for reproducibility\n    rng = np.random.default_rng(seed=123456)\n    # Define the test cases from the problem statement.\n    # Each case: (N, Fs, A_task, f_task, (c0,c1,c2,c3), noise_sigma)\n    test_cases = [\n        (600, 1.0, 2.0, 1.0/600.0, (0.0, 0.0, 0.0, 0.0), 0.05),   # Case 1: general overfitting\n        (600, 1.0, 0.0, 1.0/600.0, (0.0, 0.01, 0.0, 0.0), 0.05),  # Case 2: boundary, no oscillation\n        (80,  1.0, 1.5, 1.0/160.0, (0.0, 0.0, 0.0, 0.0), 0.10),   # Case 3: edge, short duration half-cycle\n    ]\n\n    results = []\n    for N, Fs, A_task, f_task, coeffs, noise_sigma in test_cases:\n        y, t_tilde = simulate_signal(N, Fs, A_task, f_task, coeffs, noise_sigma, rng)\n        overfit, _ = detect_overfitting(y, Fs, f_task, t_tilde, eps=1e-12)\n        results.append(overfit)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}