{
    "hands_on_practices": [
        {
            "introduction": "To effectively apply convolution in data analysis, one must master its computational implementation. While linear convolution represents the true interaction between a signal and a system's impulse response, its direct computation can be slow. This exercise explores the relationship between linear convolution and its fast counterpart, circular convolution, which is implicitly performed when using the Fast Fourier Transform (FFT). By working through this problem, you will gain a crucial, practical understanding of \"wrap-around\" artifacts and the importance of zero-padding to ensure your computational results accurately reflect the underlying theory .",
            "id": "4149361",
            "problem": "A core operation in neuroscience data analysis is the discrete-time convolution that maps a finite neural activity sequence and a finite synaptic impulse response kernel to a postsynaptic current or filtered activity. To ensure scientific realism and rigor, start from the fundamental base: the definition of discrete linear convolution and the definition of discrete circular convolution as a periodic operation.\n\nLet $x[n]$ be a finite sequence of length $N$ and let $h[n]$ be a finite sequence of length $M$. The discrete linear convolution $y_{\\mathrm{lin}}[n]$ is defined by\n$$\ny_{\\mathrm{lin}}[n] = \\sum_{r=-\\infty}^{\\infty} x[r]\\,h[n-r],\n$$\nwith the understanding that $x[r]=0$ for $r \\notin \\{0,1,\\dots,N-1\\}$ and $h[\\ell]=0$ for $\\ell \\notin \\{0,1,\\dots,M-1\\}$, so that $y_{\\mathrm{lin}}[n]$ is nonzero only for $n \\in \\{0,1,\\dots,N+M-2\\}$.\n\nFor an integer $L \\ge 1$, the $L$-point discrete circular convolution $y_{\\mathrm{circ}}^{(L)}[n]$ is defined by\n$$\ny_{\\mathrm{circ}}^{(L)}[n] = \\sum_{r=0}^{L-1} x[r]\\,h\\big((n-r)\\bmod L\\big), \\quad n \\in \\{0,1,\\dots,L-1\\},\n$$\nwhere $x[r]$ and $h[\\ell]$ are interpreted as $L$-periodic extensions of their first $L$ samples.\n\nIn fast Fourier transform based implementations commonly used in computational neuroscience, performing convolution via multiplication in the discrete Fourier transform domain without sufficient zero-padding implements the $L$-point circular convolution, which introduces wrap-around artifacts when $L  N+M-1$.\n\nYour tasks are:\n\n$1.$ For each provided test case, compute the discrete linear convolution $y_{\\mathrm{lin}}$ and the $L$-point discrete circular convolution $y_{\\mathrm{circ}}^{(L)}$.\n\n$2.$ For each test case, define the discrepancy vector\n$$\nd^{(L)}[n] = y_{\\mathrm{lin}}[n] - y_{\\mathrm{circ}}^{(L)}[n], \\quad n \\in \\{0,1,\\dots,L-1\\},\n$$\nand the wrap-around alias vector\n$$\na^{(L)}[n] = \\sum_{q=1}^{\\infty} y_{\\mathrm{lin}}[n + qL],\n$$\nwith the convention that $y_{\\mathrm{lin}}[m]=0$ for $m \\notin \\{0,1,\\dots,N+M-2\\}$, so only finitely many terms contribute. Relate the discrepancy $d^{(L)}$ to the wrap-around artifacts by verifying, numerically, that\n$$\nd^{(L)}[n] = -\\,a^{(L)}[n]\n$$\nfor all $n \\in \\{0,1,\\dots,L-1\\}$.\n\n$3.$ Quantify the magnitude of wrap-around artifacts in each test case by computing the Euclidean norm\n$$\n\\|d^{(L)}\\|_2 = \\left(\\sum_{n=0}^{L-1} \\big(d^{(L)}[n]\\big)^2\\right)^{1/2}\n$$\nand the infinity norm\n$$\n\\|d^{(L)}\\|_{\\infty} = \\max_{0 \\le n \\le L-1} \\left|d^{(L)}[n]\\right|,\n$$\nand list the indices $n$ where $|d^{(L)}[n]|$ exceeds a numerical tolerance of $10^{-12}$.\n\nUse the following test suite, chosen to probe typical cases, boundary conditions, and edge cases relevant to neuroscience signal processing workflows:\n\nTest case $1$ (typical finite spike train convolved with a short synaptic kernel; $L$ too short, wrap-around present):\n$$\nx^{(1)} = [0, 1, 0, 2, 0, 1, 0, 0, 3, 0, 0, 1], \\quad h^{(1)} = [0.0, 0.7, 0.5, 0.3, 0.1], \\quad L^{(1)} = 12.\n$$\n\nTest case $2$ (same inputs but $L$ chosen to avoid wrap-around by zero-padding to the full linear length):\n$$\nx^{(2)} = [0, 1, 0, 2, 0, 1, 0, 0, 3, 0, 0, 1], \\quad h^{(2)} = [0.0, 0.7, 0.5, 0.3, 0.1], \\quad L^{(2)} = 16.\n$$\n\nTest case $3$ (same inputs with smaller $L$ increasing aliasing severity):\n$$\nx^{(3)} = [0, 1, 0, 2, 0, 1, 0, 0, 3, 0, 0, 1], \\quad h^{(3)} = [0.0, 0.7, 0.5, 0.3, 0.1], \\quad L^{(3)} = 8.\n$$\n\nTest case $4$ (edge case with zero signal where both linear and circular convolution are identically zero):\n$$\nx^{(4)} = [0, 0, 0, 0, 0, 0, 0], \\quad h^{(4)} = [0.2, 0.5, 0.2, 0.1], \\quad L^{(4)} = 7.\n$$\n\nFor each test case $i \\in \\{1,2,3,4\\}$, your program must output a list containing:\n$[$boolean equality check of $d^{(L)}$ and $-a^{(L)}$ within tolerance $10^{-12}$, $\\|d^{(L)}\\|_2$, $\\|d^{(L)}\\|_{\\infty}$, list of indices $n$ with $|d^{(L)}[n]|  10^{-12}]$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the order specified above, for example, $[\\text{result}^{(1)},\\text{result}^{(2)},\\text{result}^{(3)},\\text{result}^{(4)}]$. No units or textual annotations should be printed, and all numeric values should be decimal numbers.",
            "solution": "The user has provided a well-defined problem in the domain of digital signal processing, specifically concerning the relationship between linear and circular discrete-time convolution, a foundational concept in computational neuroscience and other fields relying on time-series analysis.\n\n### Step 1: Extract Givens\n- **Sequences**: A finite neural activity sequence $x[n]$ of length $N$, and a finite synaptic impulse response kernel $h[n]$ of length $M$.\n- **Linear Convolution**: The discrete linear convolution $y_{\\mathrm{lin}}[n]$ is defined as $y_{\\mathrm{lin}}[n] = \\sum_{r=-\\infty}^{\\infty} x[r]\\,h[n-r]$. The resulting sequence is of length $N+M-1$, non-zero for $n \\in \\{0, 1, \\dots, N+M-2\\}$.\n- **Circular Convolution**: For an integer $L \\ge 1$, the $L$-point discrete circular convolution $y_{\\mathrm{circ}}^{(L)}[n]$ is defined as $y_{\\mathrm{circ}}^{(L)}[n] = \\sum_{r=0}^{L-1} x[r]\\,h\\big((n-r)\\bmod L\\big)$ for $n \\in \\{0, 1, \\dots, L-1\\}$, using $L$-periodic extensions of the first $L$ samples of $x$ and $h$.\n- **Discrepancy Vector**: $d^{(L)}[n] = y_{\\mathrm{lin}}[n] - y_{\\mathrm{circ}}^{(L)}[n]$ for $n \\in \\{0, 1, \\dots, L-1\\}$.\n- **Wrap-around Alias Vector**: $a^{(L)}[n] = \\sum_{q=1}^{\\infty} y_{\\mathrm{lin}}[n + qL]$.\n- **Verification Task**: Numerically verify that $d^{(L)}[n] = -a^{(L)}[n]$ for all $n \\in \\{0, 1, \\dots, L-1\\}$.\n- **Quantification Task**: Compute the Euclidean norm $\\|d^{(L)}\\|_2 = \\left(\\sum_{n=0}^{L-1} (d^{(L)}[n])^2\\right)^{1/2}$ and the infinity norm $\\|d^{(L)}\\|_{\\infty} = \\max_{0 \\le n \\le L-1} |d^{(L)}[n]|$.\n- **Index Identification Task**: List indices $n$ where $|d^{(L)}[n]|  10^{-12}$.\n- **Test Cases**:\n    1.  $x^{(1)} = [0, 1, 0, 2, 0, 1, 0, 0, 3, 0, 0, 1]$, $h^{(1)} = [0.0, 0.7, 0.5, 0.3, 0.1]$, $L^{(1)} = 12$.\n    2.  $x^{(2)} = [0, 1, 0, 2, 0, 1, 0, 0, 3, 0, 0, 1]$, $h^{(2)} = [0.0, 0.7, 0.5, 0.3, 0.1]$, $L^{(2)} = 16$.\n    3.  $x^{(3)} = [0, 1, 0, 2, 0, 1, 0, 0, 3, 0, 0, 1]$, $h^{(3)} = [0.0, 0.7, 0.5, 0.3, 0.1]$, $L^{(3)} = 8$.\n    4.  $x^{(4)} = [0, 0, 0, 0, 0, 0, 0]$, $h^{(4)} = [0.2, 0.5, 0.2, 0.1]$, $L^{(4)} = 7$.\n- **Numerical Tolerance**: $10^{-12}$.\n- **Output Format**: For each case, a list containing a boolean for the equality check, $\\|d^{(L)}\\|_2$, $\\|d^{(L)}\\|_{\\infty}$, and a list of indices. The final output is a list of these lists.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is fundamentally based on the definitions and relationship between linear and circular convolution, which are core, well-established principles in digital signal processing. The application context of neuroscience is appropriate and realistic.\n- **Well-Posed**: The problem is entirely self-contained. All necessary definitions, formulas, data, and tasks are provided explicitly. For each test case, the inputs are specified, and the required computations lead to a unique, well-defined numerical result.\n- **Objective**: The problem statement is written in precise, mathematical language, free of ambiguity or subjective claims.\n\nThe problem passes all validation criteria. It is a valid, well-posed, and scientifically sound problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Principle-Based Solution Design\n\nThe core principle to be demonstrated is that an $L$-point circular convolution is mathematically equivalent to a time-aliased version of the corresponding linear convolution. Specifically, the circular convolution $y_{\\mathrm{circ}}^{(L)}[n]$ can be expressed in terms of the linear convolution $y_{\\mathrm{lin}}[n]$ as:\n$$\ny_{\\mathrm{circ}}^{(L)}[n] = \\sum_{q=-\\infty}^{\\infty} y_{\\mathrm{lin}}[n+qL]\n$$\nSince the sequence $y_{\\mathrm{lin}}[n]$ is causal and has a finite length of $N+M-1$, this infinite sum simplifies. The terms $y_{\\mathrm{lin}}[n+qL]$ are zero for $q0$ (since $n \\ge 0$ and $y_{\\mathrm{lin}}$ is zero for negative indices) and for $q$ large enough such that $n+qL \\ge N+M-1$. Thus, the sum becomes:\n$$\ny_{\\mathrm{circ}}^{(L)}[n] = \\sum_{q=0}^{\\infty} y_{\\mathrm{lin}}[n+qL] = y_{\\mathrm{lin}}[n] + \\sum_{q=1}^{\\infty} y_{\\mathrm{lin}}[n+qL]\n$$\nThe problem defines the discrepancy vector as $d^{(L)}[n] = y_{\\mathrm{lin}}[n] - y_{\\mathrm{circ}}^{(L)}[n]$ and the wrap-around alias vector as $a^{(L)}[n] = \\sum_{q=1}^{\\infty} y_{\\mathrm{lin}}[n+qL]$. By substituting the expression for $y_{\\mathrm{circ}}^{(L)}[n]$ into the definition of $d^{(L)}[n]$, we get:\n$$\nd^{(L)}[n] = y_{\\mathrm{lin}}[n] - \\left( y_{\\mathrm{lin}}[n] + \\sum_{q=1}^{\\infty} y_{\\mathrm{lin}}[n+qL] \\right) = - \\sum_{q=1}^{\\infty} y_{\\mathrm{lin}}[n+qL] = -a^{(L)}[n]\n$$\nOur task is to implement this verification numerically and quantify the resulting discrepancy for the specified test cases. The magnitude of the discrepancy, quantified by $\\|d^{(L)}\\|_2$ and $\\|d^{(L)}\\|_{\\infty}$, is a direct measure of the wrap-around artifact. This artifact is zero if and only if $L \\ge N+M-1$, which corresponds to providing sufficient zero-padding for frequency-domain convolution methods to correctly replicate linear convolution.\n\nThe implementation will proceed as follows for each test case $(x, h, L)$:\n1.  **Compute Linear Convolution**: The sequence $y_{\\mathrm{lin}}$ will be computed using the definition of linear convolution. Its length is $N+M-1$, where $N=\\text{length}(x)$ and $M=\\text{length}(h)$.\n2.  **Compute Circular Convolution**: The sequence $y_{\\mathrm{circ}}^{(L)}$ will be computed directly from its summation definition. This involves using the first $L$ samples of $x$ and $h$ (zero-padded if their original lengths are less than $L$) and performing the modulo arithmetic on the kernel index.\n3.  **Compute Discrepancy**: The vector $d^{(L)}$ is found by subtracting $y_{\\mathrm{circ}}^{(L)}$ from the first $L$ samples of $y_{\\mathrm{lin}}$. Care must be taken to handle the case where $y_{\\mathrm{lin}}$ is shorter than $L$.\n4.  **Compute Alias Vector**: The vector $a^{(L)}$ is computed by summing the time-aliased \"tails\" of the $y_{\\mathrm{lin}}$ sequence, as per its definition.\n5.  **Verify and Quantify**: The equality $d^{(L)} = -a^{(L)}$ is checked using a numerical tolerance of $10^{-12}$. The required norms of $d^{(L)}$ are calculated, and the indices of significant discrepancy are identified.\n\nThis procedure will be applied to all four test cases, which are designed to probe the behavior when $L  N+M-1$ (aliasing), $L = N+M-1$ (no aliasing), $L \\ll N+M-1$ (severe aliasing), and the trivial case of a zero input signal.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and print results.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([0, 1, 0, 2, 0, 1, 0, 0, 3, 0, 0, 1], dtype=np.float64),\n         np.array([0.0, 0.7, 0.5, 0.3, 0.1], dtype=np.float64),\n         12),\n        (np.array([0, 1, 0, 2, 0, 1, 0, 0, 3, 0, 0, 1], dtype=np.float64),\n         np.array([0.0, 0.7, 0.5, 0.3, 0.1], dtype=np.float64),\n         16),\n        (np.array([0, 1, 0, 2, 0, 1, 0, 0, 3, 0, 0, 1], dtype=np.float64),\n         np.array([0.0, 0.7, 0.5, 0.3, 0.1], dtype=np.float64),\n         8),\n        (np.array([0, 0, 0, 0, 0, 0, 0], dtype=np.float64),\n         np.array([0.2, 0.5, 0.2, 0.1], dtype=np.float64),\n         7),\n    ]\n\n    results = []\n    for x, h, L in test_cases:\n        result = process_case(x, h, L)\n        results.append(result)\n\n    # Format the final output string as a list of lists.\n    # The str() representation of a list is standard and sufficient.\n    # e.g., str([True, 1.23, [0, 1]]) - '[True, 1.23, [0, 1]]'\n    final_output = f\"[{','.join(map(str, results))}]\"\n    print(final_output)\n\ndef process_case(x, h, L):\n    \"\"\"\n    Performs all computations for a single test case.\n    \n    Args:\n        x (np.ndarray): The input signal sequence.\n        h (np.ndarray): The kernel sequence.\n        L (int): The length for circular convolution.\n        \n    Returns:\n        list: A list containing [equality_check, norm2, norm_inf, indices].\n    \"\"\"\n    N = len(x)\n    M = len(h)\n    tolerance = 1e-12\n\n    # 1. Compute discrete linear convolution y_lin\n    y_lin = np.convolve(x, h)\n    \n    # 2. Compute L-point discrete circular convolution y_circ_L\n    # Prepare L-length versions of x and h by zero-padding or truncating\n    x_L = np.zeros(L, dtype=np.float64)\n    h_L = np.zeros(L, dtype=np.float64)\n    n_x_copy = min(N, L)\n    n_h_copy = min(M, L)\n    x_L[:n_x_copy] = x[:n_x_copy]\n    h_L[:n_h_copy] = h[:n_h_copy]\n\n    y_circ_L = np.zeros(L, dtype=np.float64)\n    for n in range(L):\n        for r in range(L):\n            y_circ_L[n] += x_L[r] * h_L[(n - r) % L]\n\n    # 3. Compute the discrepancy vector d_L\n    # d_L[n] = y_lin[n] - y_circ_L[n] for n in {0, ..., L-1}\n    # We must pad y_lin if its length is less than L\n    y_lin_prefix = np.zeros(L, dtype=np.float64)\n    copy_len = min(len(y_lin), L)\n    y_lin_prefix[:copy_len] = y_lin[:copy_len]\n    d_L = y_lin_prefix - y_circ_L\n\n    # 4. Compute the wrap-around alias vector a_L\n    # a_L[n] = sum_{q=1 to inf} y_lin[n + qL]\n    a_L = np.zeros(L, dtype=np.float64)\n    len_y_lin = len(y_lin)\n    if len_y_lin  L:\n        for n in range(L):\n            for q in range(1, (len_y_lin // L) + 2):\n                idx = n + q * L\n                if idx  len_y_lin:\n                    a_L[n] += y_lin[idx]\n                else:\n                    break\n    \n    # 5. Verify the relationship d_L = -a_L\n    is_equal = np.allclose(d_L, -a_L, rtol=0, atol=tolerance)\n\n    # 6. Quantify the magnitude of wrap-around artifacts\n    norm2 = np.linalg.norm(d_L, ord=2)\n    norm_inf = np.linalg.norm(d_L, ord=np.inf)\n\n    # 7. List indices n where |d_L[n]| exceeds tolerance\n    significant_indices = np.where(np.abs(d_L)  tolerance)[0].tolist()\n\n    return [is_equal, norm2, norm_inf, significant_indices]\n\nsolve()\n```"
        },
        {
            "introduction": "With a firm grasp of the computational nuances, we can now apply convolution to a cornerstone of functional neuroimaging: forward modeling of the Blood Oxygen Level Dependent (BOLD) signal. This practice demonstrates how the convolution of a presumed neural activity pattern with a canonical Hemodynamic Response Function (HRF) allows us to predict and understand fMRI measurements. This exercise provides direct insight into how the relatively slow vascular system temporally blurs and transforms underlying neural events, a fundamental principle of the General Linear Model (GLM) in fMRI analysis .",
            "id": "4149331",
            "problem": "You are given a discrete-time model of Blood Oxygen Level Dependent (BOLD) signal generation in functional Magnetic Resonance Imaging (fMRI) within the linear systems framework. The neurovascular coupling will be modeled as a Linear Time-Invariant (LTI) system: the system is completely characterized by its impulse response, and the output to any input is determined by superposition of scaled, time-shifted impulse responses. The input is a block design neural activity signal, and the output is the predicted BOLD. Time must be expressed in seconds.\n\nDefine the canonical Hemodynamic Response Function (HRF) $h(t)$ as a difference of two gamma functions with parameters chosen to be physiologically plausible. For $t \\ge 0$, let\n$$\nh(t) = \\frac{t^{a_1-1} e^{-t/b_1}}{b_1^{a_1}\\,\\Gamma(a_1)} \\;-\\; c \\,\\frac{t^{a_2-1} e^{-t/b_2}}{b_2^{a_2}\\,\\Gamma(a_2)},\n$$\nwith parameters $a_1 = 6$, $b_1 = 1$, $a_2 = 12$, $b_2 = 1$, and $c = 0.35$. For $t  0$, set $h(t) = 0$. Normalize $h(t)$ to unit area by dividing by its integral over $t \\in [0, t_{\\max}]$, where $t_{\\max} = 32\\,\\mathrm{s}$.\n\nA block design neural activity $n(t)$ is defined as a piecewise-constant function that takes value $A$ during prespecified stimulus blocks and $0$ otherwise. For a block starting at onset time $s$ with duration $D$, $n(t) = A$ for $t \\in [s, s + D)$ and $n(t) = 0$ otherwise. Assume $A = 1$ for all cases in this problem.\n\nThe predicted BOLD $y(t)$ is the output of the LTI system to input $n(t)$. You must compute $y(t)$ on a uniform discrete-time grid using the discrete-time approximation consistent with the LTI definition. Use a sampling interval $\\Delta t = 0.1\\,\\mathrm{s}$. For numerical convolution, implement the Riemann sum approximation of the continuous-time operation on the given grid, ensuring the correct scaling by $\\Delta t$.\n\nTo analyze how overlap of $h(t)$ across blocks leads to sustained responses, define the \"sustained index\" $S$ for a given block schedule as follows. Let the set of block onset times be $\\{s_i\\}$ with a common duration $D$. For each consecutive pair $(s_i, s_{i+1})$ such that $s_{i+1}  s_i + D$, define the inter-block gap interval $G_i = [s_i + D, s_{i+1})$ on which $n(t) = 0$. Compute the mean of the predicted BOLD over each gap interval,\n$$\n\\overline{y}(G_i) \\;=\\; \\frac{1}{|G_i|} \\int_{G_i} y(t)\\, dt,\n$$\nwhere $|G_i|$ is the duration of the gap interval in seconds. Let $y_{\\max} = \\max_{t} y(t)$ over the full analysis window. The sustained index is\n$$\nS \\;=\\; \\begin{cases}\n\\displaystyle \\max_i \\left( \\frac{\\overline{y}(G_i)}{y_{\\max}} \\right),  \\text{if there exists at least one gap interval } G_i, \\\\\n0,  \\text{if there are no gap intervals.}\n\\end{cases}\n$$\nThe sustained index $S$ is a dimensionless decimal in $[0,1]$ that quantifies the degree to which the BOLD remains elevated between blocks due to overlap of $h(t)$.\n\nImplement a program that:\n- Constructs $h(t)$ on $[0, t_{\\max}]$, normalizes it to unit area, and constructs $n(t)$ on $[0, T_{\\text{total}}]$ for each test case.\n- Computes the predicted BOLD $y(t)$ via discrete-time convolution consistent with the LTI framework and the Riemann sum approximation.\n- Computes the sustained index $S$ for each test case as defined above.\n\nExpress all time values in seconds. All returned numeric values must be floating-point numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").\n\nUse the following test suite, where each test case is fully specified by block onset times, common block duration, and total analysis window:\n1. Happy path case with a single block, no gap: onset times $\\{20\\}$, duration $D = 20\\,\\mathrm{s}$, total time $T_{\\text{total}} = 100\\,\\mathrm{s}$.\n2. Boundary case with two blocks separated well beyond the HRF support: onset times $\\{10, 90\\}$, duration $D = 10\\,\\mathrm{s}$, total time $T_{\\text{total}} = 150\\,\\mathrm{s}$.\n3. Overlap case with two blocks whose gap is shorter than the HRF tail: onset times $\\{10, 25\\}$, duration $D = 15\\,\\mathrm{s}$, total time $T_{\\text{total}} = 90\\,\\mathrm{s}$.\n4. High-overlap edge case with many short blocks and small inter-stimulus intervals: onset times $\\{0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112\\}$, duration $D = 6\\,\\mathrm{s}$, total time $T_{\\text{total}} = 120\\,\\mathrm{s}$.\n\nYour program must output the sustained index $S$ for each test case in the order listed above, as a single line \"[S1,S2,S3,S4]\".",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. All definitions and parameters are sufficient to construct a unique, computable solution. The analysis proceeds by implementing the specified model and metrics.\n\nThe solution is developed in four main steps:\n1.  Construction of the normalized Hemodynamic Response Function (HRF).\n2.  Generation of the block-design neural activity signal for each test case.\n3.  Computation of the predicted Blood Oxygen Level Dependent (BOLD) signal via discrete convolution.\n4.  Calculation of the sustained index $S$ from the resulting BOLD signal.\n\n### Step 1: Hemodynamic Response Function (HRF) Construction\n\nThe canonical HRF, denoted by $h(t)$, models the vascular response to a brief, impulse-like neural event. It is defined as a difference of two gamma functions for $t \\ge 0$:\n$$\nh(t) = \\frac{t^{a_1-1} e^{-t/b_1}}{b_1^{a_1}\\,\\Gamma(a_1)} \\;-\\; c \\,\\frac{t^{a_2-1} e^{-t/b_2}}{b_2^{a_2}\\,\\Gamma(a_2)}\n$$\nwhere $\\Gamma(\\cdot)$ is the standard gamma function. The parameters are given as $a_1 = 6$, $b_1 = 1$, $a_2 = 12$, $b_2 = 1$, and $c = 0.35$. For $t  0$, $h(t) = 0$.\n\nThis function is discretized on a time grid from $t=0$ to $t_{\\max} = 32\\,\\mathrm{s}$ with a sampling interval of $\\Delta t = 0.1\\,\\mathrm{s}$. Let this discrete sequence be $h[k] = h(k \\Delta t)$.\n\nThe problem requires normalizing $h(t)$ to have a unit area over the interval $[0, t_{\\max}]$. The integral is approximated using a Riemann sum over the discrete grid:\n$$\nI = \\int_{0}^{t_{\\max}} h(t) \\,dt \\approx \\sum_{k=0}^{N_{\\text{hrf}}-1} h(t_k) \\Delta t\n$$\nwhere $t_k = k \\Delta t$ and $N_{\\text{hrf}}$ is the number of points in the HRF's time vector. The normalized HRF, $h_{\\text{norm}}(t)$, is then:\n$$\nh_{\\text{norm}}(t) = \\frac{h(t)}{I}\n$$\nThe discrete normalized sequence is $h_{\\text{norm}}[k] = h[k] / I$.\n\n### Step 2: Neural Activity Signal Construction\n\nThe input to the system, the neural activity $n(t)$, is modeled as a block design. It is a piecewise-constant function that equals an amplitude $A=1$ during specified stimulus blocks and $0$ otherwise. For a given set of onset times $\\{s_i\\}$ and a common duration $D$, the signal is defined as:\n$$\nn(t) = \\begin{cases} 1  \\text{if } t \\in [s_i, s_i + D) \\text{ for some } i \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nFor each test case, we construct a discrete signal $n[k] = n(k \\Delta t)$ over the total analysis window $[0, T_{\\text{total}}]$.\n\n### Step 3: BOLD Signal Computation via Convolution\n\nThe predicted BOLD signal $y(t)$ is the output of the Linear Time-Invariant (LTI) system, which is given by the convolution of the input signal $n(t)$ with the system's impulse response $h_{\\text{norm}}(t)$:\n$$\ny(t) = (n * h_{\\text{norm}})(t) = \\int_{-\\infty}^{\\infty} n(\\tau) h_{\\text{norm}}(t - \\tau) \\,d\\tau\n$$\nThe problem specifies computing this using a discrete-time approximation consistent with a Riemann sum. The discrete convolution is:\n$$\ny[k] = y(t_k) \\approx \\sum_{j} n(\\tau_j) h_{\\text{norm}}(t_k - \\tau_j) \\Delta \\tau\n$$\nwhere $t_k = k\\Delta t$, $\\tau_j = j\\Delta t$, and $\\Delta \\tau = \\Delta t$. This corresponds to performing a standard discrete convolution of the sequences $n[k]$ and $h_{\\text{norm}}[k]$ and then scaling the result by $\\Delta t$:\n$$\ny[k] = \\Delta t \\cdot \\sum_{j=0}^{M-1} n[j] h_{\\text{norm}}[k-j]\n$$\nwhere $M$ is the length of the signal $n[k]$. This operation is performed for each test case. The output of the convolution is truncated to match the length of the input signal $n[k]$.\n\n### Step 4: Sustained Index Calculation\n\nThe sustained index $S$ quantifies the elevation of the BOLD signal in the gaps between stimulus blocks.\nFirst, the global maximum of the BOLD signal, $y_{\\max} = \\max_k y[k]$, is determined over the entire analysis window.\n\nNext, we identify all inter-block gap intervals. For each pair of consecutive onsets $(s_i, s_{i+1})$ with common duration $D$, a gap interval $G_i$ exists if $s_{i+1}  s_i + D$. The interval is defined as $G_i = [s_i + D, s_{i+1})$.\n\nFor each such gap $G_i$, the mean BOLD signal is computed. The continuous definition is:\n$$\n\\overline{y}(G_i) \\;=\\; \\frac{1}{|G_i|} \\int_{G_i} y(t)\\, dt,\n$$\nwhere $|G_i| = s_{i+1} - (s_i+D)$ is the duration of the gap. Its discrete approximation is the arithmetic mean of the BOLD signal samples within the gap interval:\n$$\n\\overline{y}(G_i) \\approx \\frac{1}{N_{G_i}} \\sum_{t_k \\in G_i} y[k]\n$$\nwhere $N_{G_i}$ is the number of time points in the gap interval $G_i$.\n\nA ratio $r_i = \\overline{y}(G_i) / y_{\\max}$ is calculated for each gap. The sustained index $S$ is the maximum of these ratios over all gaps. If no such gaps exist (e.g., a single block, or contiguous blocks), then $S$ is defined to be $0$.\n$$\nS \\;=\\; \\begin{cases}\n\\displaystyle \\max_i (r_i),  \\text{if there is at least one gap interval } G_i, \\\\\n0,  \\text{if there are no gap intervals.}\n\\end{cases}\n$$\nThis procedure is applied to each of the four test cases specified in the problem statement to obtain the final results.\n\nFor Case 1 (single block) and Case 3 (contiguous blocks where $s_{i+1} = s_i+D$), the condition $s_{i+1}  s_i+D$ is not met. Therefore, no gap intervals exist, and the sustained index $S$ is $0$ by definition for both cases. For Case 2 and Case 4, gap intervals exist, and $S$ is computed as the maximum ratio described above.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gamma\n\ndef solve():\n    \"\"\"\n    Solves the BOLD signal modeling problem by computing the sustained index\n    for several block-design experiments.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\"onsets\": [20.0], \"D\": 20.0, \"T_total\": 100.0},\n        {\"onsets\": [10.0, 90.0], \"D\": 10.0, \"T_total\": 150.0},\n        {\"onsets\": [10.0, 25.0], \"D\": 15.0, \"T_total\": 90.0},\n        {\"onsets\": [0.0, 8.0, 16.0, 24.0, 32.0, 40.0, 48.0, 56.0, 64.0, 72.0, 80.0, 88.0, 96.0, 104.0, 112.0], \"D\": 6.0, \"T_total\": 120.0}\n    ]\n\n    results = []\n    \n    # Global parameters\n    delta_t = 0.1\n    t_max_hrf = 32.0\n    a1, b1, a2, b2, c = 6.0, 1.0, 12.0, 1.0, 0.35\n    A = 1.0\n\n    # 1. Construct and normalize the Hemodynamic Response Function (HRF)\n    \n    def gamma_pdf_term(t, a, b):\n        \"\"\"\n        Computes the unscaled gamma PDF term.\n        Note: t must be a numpy array. a-1  0 is assumed.\n        \"\"\"\n        # The expression t**(a-1) is numerically safe for t=0 since a1.\n        numerator = t**(a-1) * np.exp(-t/b)\n        denominator = b**a * gamma(a)\n        # Handle division by zero just in case, though not expected here.\n        with np.errstate(divide='ignore', invalid='ignore'):\n            term = np.true_divide(numerator, denominator)\n        term[denominator == 0] = 0.0 # defensive\n        return term\n\n    t_hrf = np.arange(0, t_max_hrf + delta_t / 2, delta_t)\n    \n    h_unnormalized = gamma_pdf_term(t_hrf, a1, b1) - c * gamma_pdf_term(t_hrf, a2, b2)\n    \n    integral_h = np.sum(h_unnormalized) * delta_t\n    h_norm = h_unnormalized / integral_h\n\n    # Process each test case\n    for case in test_cases:\n        onsets = case[\"onsets\"]\n        D = case[\"D\"]\n        T_total = case[\"T_total\"]\n\n        # 2. Construct the neural activity signal n(t)\n        num_points = int(round(T_total / delta_t)) + 1\n        t_vec = np.linspace(0, T_total, num_points)\n        n = np.zeros_like(t_vec)\n\n        for s in onsets:\n            start_idx = int(round(s / delta_t))\n            # The interval is [s, s+D), so the end index is exclusive.\n            end_idx = int(round((s + D) / delta_t))\n            n[start_idx:end_idx] = A\n            \n        # 3. Compute predicted BOLD signal y(t) via discrete convolution\n        y = np.convolve(n, h_norm, mode='full')\n        # Truncate to original signal length and scale by delta_t for Riemann sum\n        y = y[:len(n)] * delta_t\n        \n        # 4. Compute the sustained index S\n        y_max = np.max(y)\n        \n        # If no activity, y_max can be 0. Avoid division by zero.\n        if y_max = 0:\n            S = 0.0\n            results.append(S)\n            continue\n            \n        # According to the definition, S=0 if there are no gaps.\n        # This occurs if there is only one block or no blocks.\n        if len(onsets) = 1:\n            S = 0.0\n            results.append(S)\n            continue\n            \n        gap_ratios = []\n        for i in range(len(onsets) - 1):\n            s_i = onsets[i]\n            s_i_plus_1 = onsets[i+1]\n\n            # A gap interval exists only if s_{i+1}  s_i + D\n            if s_i_plus_1  s_i + D:\n                gap_start_t = s_i + D\n                gap_end_t = s_i_plus_1\n                \n                gap_start_idx = int(round(gap_start_t / delta_t))\n                gap_end_idx = int(round(gap_end_t / delta_t))\n                \n                # Ensure the indices define a non-empty slice\n                if gap_start_idx = gap_end_idx:\n                    continue\n                \n                y_gap_segment = y[gap_start_idx:gap_end_idx]\n                \n                # Compute mean BOLD over the gap and its ratio to max BOLD\n                mean_y_gap = np.mean(y_gap_segment)\n                gap_ratios.append(mean_y_gap / y_max)\n        \n        if not gap_ratios:\n            S = 0.0\n        else:\n            S = max(gap_ratios)\n            \n        results.append(S)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond forward modeling, we now tackle the more complex and powerful inverse problem of deconvolution. This advanced practice guides you through estimating an unknown system property—in this case, the impulse response of the neurovascular system—from a known input and a noisy measured output. By reformulating convolution as a linear matrix equation and applying regularization to ensure a stable and plausible solution, you will practice a vital data-driven analysis skill that is essential for system identification across many domains of neuroscience .",
            "id": "4149352",
            "problem": "Consider a discrete-time linear time-invariant system modeling Blood Oxygen Level Dependent (BOLD) measurements in functional magnetic resonance imaging. Let $s[n]$ denote the known stimulus sequence, $h[k]$ denote the unknown impulse response (hemodynamic response function), and $y[n]$ denote the measured BOLD signal. The system obeys the discrete-time convolution relationship\n$$\ny[n] = \\sum_{k=0}^{M-1} s[n-k]\\, h[k] + \\varepsilon[n],\n$$\nfor $n = 0, 1, \\dots, N-1$, where $M$ is the support length of $h[k]$, $N$ is the number of time points, and $\\varepsilon[n]$ represents additive measurement noise. Assume $s[n]$ and $y[n]$ are known, and $h[k]$ is to be estimated. The objective is to estimate $h[k]$ via a regularized least squares approach that penalizes solutions according to a linear operator. The algorithm must arise from first principles by constructing the appropriate linear system implied by the convolution and selecting a quadratic penalty that enforces either magnitude control or smoothness without relying on any shortcut formulas provided in the problem statement.\n\nAssume the ground-truth impulse response $h_{\\text{true}}[k]$ used to generate test data is the discrete-time double-gamma hemodynamic response function, sampled at a uniform interval $\\Delta t$ seconds, defined for $k = 0, 1, \\dots, M-1$ by\n$$\nh_{\\text{true}}[k] = \\alpha_1 \\frac{t_k^{p_1 - 1} e^{-t_k / b_1}}{b_1^{p_1}\\,\\Gamma(p_1)} - \\alpha_2 \\frac{t_k^{p_2 - 1} e^{-t_k / b_2}}{b_2^{p_2}\\,\\Gamma(p_2)},\n$$\nwhere $t_k = k\\,\\Delta t$, $\\Gamma(\\cdot)$ is the gamma function, and $\\alpha_1, \\alpha_2, p_1, p_2, b_1, b_2$ are fixed constants. Normalize $h_{\\text{true}}[k]$ so that its maximum value equals $1$ (this normalization is used only to generate consistent test data). The unknown $h[k]$ to be estimated is not assumed to be normalized.\n\nUse the following constants for the double-gamma function:\n- $\\alpha_1 = 1.0$, $\\alpha_2 = 0.5$,\n- $p_1 = 6$, $p_2 = 16$,\n- $b_1 = 1.0$, $b_2 = 1.0$,\n- sampling interval $\\Delta t = 1.0$ seconds.\n\nNoise $\\varepsilon[n]$ is independently drawn from a zero-mean Gaussian distribution with specified standard deviation for each test case. Use the specified random seeds for reproducibility.\n\nRegularization is defined by a linear operator $L$ that can be either:\n- identity operator for magnitude control: $L = I_M$ (an $M \\times M$ identity matrix),\n- first-difference operator for smoothness: $L \\in \\mathbb{R}^{(M-1)\\times M}$, where for $i = 0, \\dots, M-2$, the $i$th row has entries $-1$ at column $i$ and $+1$ at column $i+1$, and zeros elsewhere.\n\nYour program must, for each test case, construct the convolutional design implied by the model, derive and solve the regularized least squares problem to estimate $h[k]$ of length $M$, and report the root mean squared error between $h[k]$ and $h_{\\text{true}}[k]$:\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{M}\\sum_{k=0}^{M-1} \\left(h[k] - h_{\\text{true}}[k]\\right)^2 }.\n$$\nAll outputs are dimensionless.\n\nTest Suite:\n1. Happy-path event-related design:\n   - $N = 120$, $M = 30$, $\\Delta t = 1.0$ seconds,\n   - Stimulus events at indices $\\{10, 30, 50, 70, 90, 110\\}$: $s[n] = 1$ at these indices and $0$ otherwise,\n   - Noise standard deviation $\\sigma = 0.05$,\n   - Regularization parameter $\\lambda = 0.1$,\n   - Operator $L = I_M$,\n   - Random seed $= 0$.\n\n2. Boundary case (no stimulus):\n   - $N = 120$, $M = 30$, $\\Delta t = 1.0$ seconds,\n   - $s[n] = 0$ for all $n$,\n   - Noise standard deviation $\\sigma = 0.05$,\n   - Regularization parameter $\\lambda = 1.0$,\n   - Operator $L = I_M$,\n   - Random seed $= 1$.\n\n3. Impulse stimulus:\n   - $N = 120$, $M = 30$, $\\Delta t = 1.0$ seconds,\n   - Single impulse at index $10$: $s[10] = 1$ and $s[n] = 0$ otherwise,\n   - Noise standard deviation $\\sigma = 0.01$,\n   - Regularization parameter $\\lambda = 0.001$,\n   - Operator $L = I_M$,\n   - Random seed $= 2$.\n\n4. Ill-conditioned block design with smoothness regularization:\n   - $N = 200$, $M = 30$, $\\Delta t = 1.0$ seconds,\n   - Block stimulus alternating $20$ on and $20$ off starting at $n=0$ (i.e., $s[n] = 1$ for $n$ in $[0,19], [40,59], [80,99], [120,139], [160,179]$ and $s[n] = 0$ otherwise),\n   - Noise standard deviation $\\sigma = 0.10$,\n   - Regularization parameter $\\lambda = 1.0$,\n   - Operator $L$ is the first-difference operator,\n   - Random seed $= 3$.\n\nImplementation Requirements:\n- For each test case, construct the linear system implied by the discrete convolution, set up a regularized least squares objective with the specified operator $L$ and parameter $\\lambda$, and solve for $h[k]$ without using any closed-form solution provided in this problem statement.\n- Compute the $\\text{RMSE}$ between the estimated $h[k]$ and $h_{\\text{true}}[k]$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0.123456,0.234567,0.345678,0.456789]\"), where each value is the $\\text{RMSE}$ for one test case formatted to exactly six decimal places.",
            "solution": "The user has provided a valid, well-posed problem. It is scientifically grounded in the principles of linear systems theory and its application to fMRI data analysis. All parameters and conditions are specified, allowing for a unique and verifiable solution.\n\nThe core of the problem is to estimate an unknown impulse response, $h[k]$, of a linear time-invariant (LTI) system, given a known input stimulus, $s[n]$, and a noisy output signal, $y[n]$. This process is known as deconvolution.\n\nThe governing equation is the discrete-time convolution:\n$$\ny[n] = \\sum_{k=0}^{M-1} s[n-k]\\, h[k] + \\varepsilon[n]\n$$\nwhere $h[k]$ is the impulse response of length $M$, $s[n]$ is the stimulus sequence, $y[n]$ is the measured signal over $N$ time points, and $\\varepsilon[n]$ is additive noise.\n\nThe first step is to reformulate this convolution operation as a matrix-vector product, which is amenable to linear algebra. We define the vectors $\\mathbf{y} \\in \\mathbb{R}^{N}$, $\\mathbf{h} \\in \\mathbb{R}^{M}$, and $\\mathbf{\\varepsilon} \\in \\mathbb{R}^{N}$ as:\n$$\n\\mathbf{y} = \\begin{pmatrix} y[0] \\\\ y[1] \\\\ \\vdots \\\\ y[N-1] \\end{pmatrix}, \\quad\n\\mathbf{h} = \\begin{pmatrix} h[0] \\\\ h[1] \\\\ \\vdots \\\\ h[M-1] \\end{pmatrix}, \\quad\n\\mathbf{\\varepsilon} = \\begin{pmatrix} \\varepsilon[0] \\\\ \\varepsilon[1] \\\\ \\vdots \\\\ \\varepsilon[N-1] \\end{pmatrix}\n$$\nThe convolution can then be expressed as $\\mathbf{y} = S\\mathbf{h} + \\mathbf{\\varepsilon}$, where $S$ is an $N \\times M$ convolution matrix, often called the design matrix. The elements of $S$ are given by $S_{nk} = s[n-k]$, assuming $s[j]=0$ for $j0$. This structure means that each column of $S$ is a time-shifted version of the stimulus sequence $s[n]$.\n\nThe problem of estimating $\\mathbf{h}$ is framed as a regularized least squares problem. We seek to find an estimate, $\\hat{\\mathbf{h}}$, that minimizes a cost function $J(\\mathbf{h})$. This function balances a data fidelity term (how well the model fits the data) and a regularization term (which penalizes undesirable properties of the solution). The cost function is:\n$$\nJ(\\mathbf{h}) = \\|S\\mathbf{h} - \\mathbf{y}\\|_2^2 + \\lambda \\|L\\mathbf{h}\\|_2^2\n$$\nHere, $\\| \\cdot \\|_2^2$ denotes the squared Euclidean norm. The first term, $\\|S\\mathbf{h} - \\mathbf{y}\\|_2^2$, is the sum of squared residuals, measuring the mismatch between the model's prediction, $S\\mathbf{h}$, and the observed data, $\\mathbf{y}$. The second term, $\\lambda \\|L\\mathbf{h}\\|_2^2$, is the penalty. The scalar $\\lambda \\ge 0$ is the regularization parameter that controls the trade-off, and $L$ is a linear operator that defines the property to be penalized.\n\nTo find the minimum of $J(\\mathbf{h})$, we take its gradient with respect to $\\mathbf{h}$ and set it to zero. First, we expand the cost function:\n$$\nJ(\\mathbf{h}) = (S\\mathbf{h} - \\mathbf{y})^T(S\\mathbf{h} - \\mathbf{y}) + \\lambda (L\\mathbf{h})^T(L\\mathbf{h})\n$$\n$$\nJ(\\mathbf{h}) = (\\mathbf{h}^T S^T - \\mathbf{y}^T)(S\\mathbf{h} - \\mathbf{y}) + \\lambda \\mathbf{h}^T L^T L \\mathbf{h}\n$$\n$$\nJ(\\mathbf{h}) = \\mathbf{h}^T S^T S \\mathbf{h} - 2\\mathbf{h}^T S^T \\mathbf{y} + \\mathbf{y}^T \\mathbf{y} + \\lambda \\mathbf{h}^T L^T L \\mathbf{h}\n$$\nThe gradient with respect to $\\mathbf{h}$ is:\n$$\n\\frac{\\partial J(\\mathbf{h})}{\\partial \\mathbf{h}} = 2S^T S \\mathbf{h} - 2S^T \\mathbf{y} + 2\\lambda L^T L \\mathbf{h}\n$$\nSetting the gradient to the zero vector gives the normal equations for this regularized problem:\n$$\n(S^T S + \\lambda L^T L) \\mathbf{h} = S^T \\mathbf{y}\n$$\nThis is a system of linear equations of the form $A\\mathbf{x}=\\mathbf{b}$, where $A = S^T S + \\lambda L^T L$ and $\\mathbf{b} = S^T \\mathbf{y}$. Solving this system yields the desired estimate $\\hat{\\mathbf{h}}$.\n\nTwo forms of the regularization operator $L$ are specified:\n1.  $L = I_M$, the $M \\times M$ identity matrix. The penalty term becomes $\\lambda \\|\\mathbf{h}\\|_2^2$, which penalizes the magnitude of the coefficients in $\\mathbf{h}$. This is known as Tikhonov regularization or ridge regression.\n2.  $L$ is the $(M-1) \\times M$ first-difference operator. The penalty term becomes $\\lambda \\sum_{k=0}^{M-2}(h[k+1]-h[k])^2$, which penalizes large differences between adjacent coefficients, thus promoting a smooth solution.\n\nFor each test case, the following procedure is implemented:\n1.  The ground-truth hemodynamic response function, $h_{\\text{true}}[k]$, is synthesized using the provided double-gamma formula and normalized.\n2.  The stimulus sequence, $s[n]$, is constructed according to the test case specification.\n3.  The $N \\times M$ design matrix, $S$, is formed from $s[n]$.\n4.  The \"clean\" BOLD signal is computed as $\\mathbf{y}_{\\text{clean}} = S \\mathbf{h}_{\\text{true}}$.\n5.  Reproducible, zero-mean Gaussian noise $\\mathbf{\\varepsilon}$ with standard deviation $\\sigma$ is added to create the measured signal $\\mathbf{y} = \\mathbf{y}_{\\text{clean}} + \\mathbf{\\varepsilon}$.\n6.  The appropriate regularization operator matrix, $L$, is constructed.\n7.  The matrices $A = S^T S + \\lambda L^T L$ and vector $\\mathbf{b} = S^T \\mathbf{y}$ are formed.\n8.  The linear system $A\\hat{\\mathbf{h}} = \\mathbf{b}$ is solved for the estimated impulse response, $\\hat{\\mathbf{h}}$.\n9.  Finally, the Root Mean Squared Error (RMSE) between the estimate and the ground truth is calculated:\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{M}\\sum_{k=0}^{M-1} \\left(\\hat{h}[k] - h_{\\text{true}}[k]\\right)^2 }\n$$\nThis principled approach ensures a robust solution even when the design matrix $S$ is ill-conditioned or singular, as demonstrated by the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma\n\ndef solve():\n    \"\"\"\n    Solves the regularized deconvolution problem for a set of test cases.\n    \"\"\"\n\n    test_cases = [\n        {'N': 120, 'M': 30, 'dt': 1.0, 'stim_spec': {10, 30, 50, 70, 90, 110},\n         'sigma': 0.05, 'lam': 0.1, 'L_type': 'identity', 'seed': 0},\n        {'N': 120, 'M': 30, 'dt': 1.0, 'stim_spec': set(),\n         'sigma': 0.05, 'lam': 1.0, 'L_type': 'identity', 'seed': 1},\n        {'N': 120, 'M': 30, 'dt': 1.0, 'stim_spec': {10},\n         'sigma': 0.01, 'lam': 0.001, 'L_type': 'identity', 'seed': 2},\n        {'N': 200, 'M': 30, 'dt': 1.0, 'stim_spec': 'block',\n         'sigma': 0.10, 'lam': 1.0, 'L_type': 'difference', 'seed': 3},\n    ]\n\n    results = []\n\n    def generate_h_true(M, dt):\n        \"\"\"Generates the ground-truth double-gamma HRF.\"\"\"\n        p1, p2 = 6, 16\n        b1, b2 = 1.0, 1.0\n        a1, a2 = 1.0, 0.5\n\n        t = np.arange(0, M * dt, dt)\n\n        # Gamma PDF-like function part\n        with np.errstate(divide='ignore', invalid='ignore'):\n            term1 = (t**(p1 - 1) * np.exp(-t / b1)) / (b1**p1 * gamma(p1))\n            term2 = (t**(p2 - 1) * np.exp(-t / b2)) / (b2**p2 * gamma(p2))\n        \n        term1[np.isnan(term1)] = 0\n        term2[np.isnan(term2)] = 0\n\n        h_unnormalized = a1 * term1 - a2 * term2\n        \n        # Normalize to have a maximum value of 1\n        max_val = np.max(h_unnormalized)\n        if max_val  0:\n            h_true = h_unnormalized / max_val\n        else:\n            h_true = h_unnormalized # Avoid division by zero if all values are = 0\n        \n        return h_true\n\n    for case in test_cases:\n        N, M, dt, stim_spec, sigma, lam, L_type, seed = \\\n            case['N'], case['M'], case['dt'], case['stim_spec'], \\\n            case['sigma'], case['lam'], case['L_type'], case['seed']\n\n        # 1. Generate ground-truth HRF\n        h_true = generate_h_true(M, dt)\n\n        # 2. Construct stimulus sequence s[n]\n        s = np.zeros(N)\n        if stim_spec == 'block':\n            for i in range(N // 40):\n                start = i * 40\n                s[start:start+20] = 1\n        else: # set of indices\n            for idx in stim_spec:\n                if idx  N:\n                    s[idx] = 1\n        \n        # 3. Construct the design matrix S\n        S = np.zeros((N, M))\n        for k in range(M):\n            if N  k:\n                S[k:, k] = s[:N-k]\n\n        # 4. Generate the measured BOLD signal y[n]\n        rng = np.random.default_rng(seed)\n        noise = rng.normal(loc=0.0, scale=sigma, size=N)\n        y_clean = S @ h_true\n        y = y_clean + noise\n\n        # 5. Construct the regularization operator L\n        if L_type == 'identity':\n            L = np.eye(M)\n        elif L_type == 'difference':\n            L = np.zeros((M - 1, M))\n            for i in range(M - 1):\n                L[i, i] = -1\n                L[i, i+1] = 1\n        else:\n            raise ValueError(\"Unknown L_type\")\n\n        # 6. Formulate and solve the regularized least squares problem\n        # (S^T S + lambda * L^T L) h = S^T y\n        A = S.T @ S + lam * (L.T @ L)\n        b = S.T @ y\n        h_hat = np.linalg.solve(A, b)\n\n        # 7. Compute the RMSE\n        rmse = np.sqrt(np.mean((h_hat - h_true)**2))\n        results.append(f\"{rmse:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}