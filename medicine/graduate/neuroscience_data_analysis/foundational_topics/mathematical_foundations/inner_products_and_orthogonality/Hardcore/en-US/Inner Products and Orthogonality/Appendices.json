{
    "hands_on_practices": [
        {
            "introduction": "Understanding neural signals often requires decomposing them into simpler, fundamental components. This practice extends the concept of orthogonal projection from finite vectors to continuous functions, forming the basis of Fourier analysis. By projecting a function onto an orthogonal basis of sinusoids, we can isolate its core frequency components, a fundamental technique for analyzing oscillatory brain activity in EEG or LFP data.",
            "id": "2403755",
            "problem": "In computational engineering, one often approximates a signal by projecting it onto a low-dimensional function space. Let $f(x)=x$ for $x \\in [-\\pi,\\pi]$, where $x$ is measured in radians. Consider the function space spanned by $\\{\\cos(x),\\sin(x)\\}$ equipped with the standard inner product on square-integrable functions over $[-\\pi,\\pi]$, defined by\n$$\n\\langle g,h\\rangle = \\int_{-\\pi}^{\\pi} g(x)\\,h(x)\\,dx.\n$$\nStarting from the core definitions of inner product spaces and the orthogonal projection of a vector onto a subspace, determine the coefficients $a$ and $b$ such that the function $p(x)=a\\cos(x)+b\\sin(x)$ is the orthogonal projection of $f(x)$ onto the span of $\\{\\cos(x),\\sin(x)\\}$. Your derivation should rely only on the properties of inner products, orthogonality, and the parity (evenness/oddness) of functions where appropriate. Provide the exact values of $a$ and $b$ (no rounding), and report your final answer as the ordered pair $(a,b)$.",
            "solution": "The problem is to determine the coefficients $a$ and $b$ of the function $p(x) = a\\cos(x) + b\\sin(x)$ such that $p(x)$ is the orthogonal projection of $f(x) = x$ onto the subspace spanned by the set of functions $\\{\\cos(x), \\sin(x)\\}$ over the interval $[-\\pi, \\pi]$. The inner product is defined as $\\langle g, h \\rangle = \\int_{-\\pi}^{\\pi} g(x)h(x)\\,dx$.\n\nThe validation of the problem statement is performed first.\nGivens:\n1.  Function to project: $f(x) = x$ on $x \\in [-\\pi, \\pi]$.\n2.  Subspace basis: $\\{\\cos(x), \\sin(x)\\}$.\n3.  Projection form: $p(x) = a\\cos(x) + b\\sin(x)$.\n4.  Inner product: $\\langle g,h\\rangle = \\int_{-\\pi}^{\\pi} g(x)\\,h(x)\\,dx$.\n\nValidation against criteria:\nThe problem is scientifically grounded, being a standard application of Fourier analysis and linear algebra in function spaces. It is well-posed, as the projection of a vector (function) onto a finite-dimensional subspace with a defined inner product is unique. The problem is objective, stated with precise mathematical definitions and without ambiguity. It is complete, containing all necessary information. Therefore, the problem is valid, and a solution can be derived.\n\nThe orthogonal projection $p(x)$ of a function $f(x)$ onto the subspace spanned by a set of basis functions $\\{v_1(x), v_2(x), \\dots, v_n(x)\\}$ is given by $p(x) = \\sum_{k=1}^{n} c_k v_k(x)$. If the basis is orthogonal, the coefficients $c_k$ are calculated as $c_k = \\frac{\\langle f, v_k \\rangle}{\\langle v_k, v_k \\rangle}$.\n\nIn our case, the basis functions are $v_1(x) = \\cos(x)$ and $v_2(x) = \\sin(x)$. First, we must verify if this basis is orthogonal with respect to the given inner product. We compute the inner product of the basis functions:\n$$\n\\langle \\cos(x), \\sin(x) \\rangle = \\int_{-\\pi}^{\\pi} \\cos(x) \\sin(x) \\,dx\n$$\nThe integrand, $\\cos(x)\\sin(x)$, is an odd function because it is the product of an even function, $\\cos(x)$, and an odd function, $\\sin(x)$. The integral of an odd function over a symmetric interval like $[-\\pi, \\pi]$ is zero.\n$$\n\\langle \\cos(x), \\sin(x) \\rangle = 0\n$$\nThus, the basis $\\{\\cos(x), \\sin(x)\\}$ is orthogonal.\n\nThe coefficients $a$ and $b$ for the projection $p(x) = a\\cos(x) + b\\sin(x)$ are therefore given by:\n$$\na = \\frac{\\langle f(x), \\cos(x) \\rangle}{\\langle \\cos(x), \\cos(x) \\rangle} = \\frac{\\int_{-\\pi}^{\\pi} x \\cos(x) \\,dx}{\\int_{-\\pi}^{\\pi} \\cos^2(x) \\,dx}\n$$\n$$\nb = \\frac{\\langle f(x), \\sin(x) \\rangle}{\\langle \\sin(x), \\sin(x) \\rangle} = \\frac{\\int_{-\\pi}^{\\pi} x \\sin(x) \\,dx}{\\int_{-\\pi}^{\\pi} \\sin^2(x) \\,dx}\n$$\n\nWe calculate the numerator for coefficient $a$:\n$$\n\\langle f(x), \\cos(x) \\rangle = \\int_{-\\pi}^{\\pi} x \\cos(x) \\,dx\n$$\nThe integrand $x\\cos(x)$ is an odd function, being the product of an odd function $f(x)=x$ and an even function $\\cos(x)$. The integral of this odd function over the symmetric interval $[-\\pi, \\pi]$ is zero.\n$$\n\\int_{-\\pi}^{\\pi} x \\cos(x) \\,dx = 0\n$$\nTherefore, the coefficient $a$ is $0$.\n\nNext, we calculate the numerator for coefficient $b$:\n$$\n\\langle f(x), \\sin(x) \\rangle = \\int_{-\\pi}^{\\pi} x \\sin(x) \\,dx\n$$\nThe integrand $x\\sin(x)$ is an even function, as it is the product of two odd functions, $f(x)=x$ and $\\sin(x)$. Its integral over $[-\\pi, \\pi]$ is not necessarily zero. We use integration by parts, $\\int u \\,dv = uv - \\int v \\,du$. Let $u = x$ and $dv = \\sin(x)dx$. Then $du = dx$ and $v = -\\cos(x)$.\n$$\n\\int x \\sin(x) \\,dx = x(-\\cos(x)) - \\int (-\\cos(x)) \\,dx = -x\\cos(x) + \\sin(x)\n$$\nEvaluating the definite integral:\n$$\n\\int_{-\\pi}^{\\pi} x \\sin(x) \\,dx = [-x\\cos(x) + \\sin(x)]_{-\\pi}^{\\pi}\n$$\n$$\n= (-\\pi\\cos(\\pi) + \\sin(\\pi)) - (-(-\\pi)\\cos(-\\pi) + \\sin(-\\pi))\n$$\nSince $\\cos(\\pi) = -1$, $\\cos(-\\pi) = -1$, and $\\sin(\\pi) = \\sin(-\\pi) = 0$:\n$$\n= (-\\pi(-1) + 0) - (\\pi(-1) + 0) = \\pi - (-\\pi) = 2\\pi\n$$\n\nNow, we must compute the denominators, which are the squared norms of the basis functions.\nFor $\\langle \\cos(x), \\cos(x) \\rangle$:\n$$\n\\int_{-\\pi}^{\\pi} \\cos^2(x) \\,dx = \\int_{-\\pi}^{\\pi} \\frac{1 + \\cos(2x)}{2} \\,dx = \\frac{1}{2} \\left[ x + \\frac{\\sin(2x)}{2} \\right]_{-\\pi}^{\\pi}\n$$\n$$\n= \\frac{1}{2} \\left[ \\left(\\pi + \\frac{\\sin(2\\pi)}{2}\\right) - \\left(-\\pi + \\frac{\\sin(-2\\pi)}{2}\\right) \\right] = \\frac{1}{2} (\\pi - (-\\pi)) = \\pi\n$$\nFor $\\langle \\sin(x), \\sin(x) \\rangle$:\n$$\n\\int_{-\\pi}^{\\pi} \\sin^2(x) \\,dx = \\int_{-\\pi}^{\\pi} \\frac{1 - \\cos(2x)}{2} \\,dx = \\frac{1}{2} \\left[ x - \\frac{\\sin(2x)}{2} \\right]_{-\\pi}^{\\pi}\n$$\n$$\n= \\frac{1}{2} \\left[ \\left(\\pi - \\frac{\\sin(2\\pi)}{2}\\right) - \\left(-\\pi - \\frac{\\sin(-2\\pi)}{2}\\right) \\right] = \\frac{1}{2} (\\pi - (-\\pi)) = \\pi\n$$\n\nFinally, we assemble the coefficients $a$ and $b$:\n$$\na = \\frac{0}{\\pi} = 0\n$$\n$$\nb = \\frac{2\\pi}{\\pi} = 2\n$$\nThe coefficients are $a=0$ and $b=2$. The problem asks for the ordered pair $(a,b)$.",
            "answer": "$$\n\\boxed{(0, 2)}\n$$"
        },
        {
            "introduction": "The geometry of neural population codes is not always Euclidean; it is fundamentally shaped by the noise corrupting the signals. This exercise demonstrates how a naive Euclidean analysis can yield misleading conclusions about the similarity of neural representations. By employing a Mahalanobis inner product that accounts for the anisotropic noise structure, we can uncover the true functional relationships between neural activity patterns.",
            "id": "4170532",
            "problem": "A systems neuroscientist is comparing stimulus-locked population response directions between two conditions in a small neural ensemble. Each condition is summarized by a trial-averaged response vector across $2$ simultaneously recorded neurons: for condition $A$, the mean vector is $\\mathbf{r}_{A} = \\begin{pmatrix} 10 \\\\ 1 \\end{pmatrix}$; for condition $B$, the mean vector is $\\mathbf{r}_{B} = \\begin{pmatrix} 10 \\\\ -1 \\end{pmatrix}$. Across trials, the residual variability (noise) is well-approximated as zero-mean Gaussian with covariance matrix $\\mathbf{S} = \\begin{pmatrix} 100  0 \\\\ 0  1 \\end{pmatrix}$, estimated independently of the means using many repeats.\n\nThe analyst wishes to quantify the alignment between the one-dimensional subspaces spanned by $\\mathbf{r}_{A}$ and $\\mathbf{r}_{B}$. They first use the Euclidean inner product and then correct the analysis using a Mahalanobis inner product induced by the inverse covariance.\n\nUse only the following standard foundations:\n- An inner product on $\\mathbb{R}^{n}$ is a bilinear, symmetric, positive-definite map $\\langle \\cdot, \\cdot \\rangle$. It induces a norm $\\|\\mathbf{x}\\| = \\sqrt{\\langle \\mathbf{x}, \\mathbf{x} \\rangle}$.\n- The angle $\\theta$ between two nonzero vectors $\\mathbf{x}$ and $\\mathbf{y}$ in an inner product space is defined by $\\cos(\\theta) = \\dfrac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\| \\, \\|\\mathbf{y}\\|}$.\n- The Euclidean inner product is $\\langle \\mathbf{x}, \\mathbf{y} \\rangle_{\\mathrm{E}} = \\mathbf{x}^{\\top} \\mathbf{y}$.\n- Given a symmetric positive-definite matrix $\\mathbf{S}$, the Mahalanobis inner product is $\\langle \\mathbf{x}, \\mathbf{y} \\rangle_{\\mathrm{M}} = \\mathbf{x}^{\\top} \\mathbf{S}^{-1} \\mathbf{y}$.\n\nTasks:\n1. Using $\\langle \\cdot, \\cdot \\rangle_{\\mathrm{E}}$, compute $\\cos(\\theta_{\\mathrm{E}})$ and the corresponding angle $\\theta_{\\mathrm{E}}$ between $\\operatorname{span}(\\mathbf{r}_{A})$ and $\\operatorname{span}(\\mathbf{r}_{B})$.\n2. Using $\\langle \\cdot, \\cdot \\rangle_{\\mathrm{M}}$ induced by $\\mathbf{S}$, compute $\\cos(\\theta_{\\mathrm{M}})$ and the corresponding angle $\\theta_{\\mathrm{M}}$ between $\\operatorname{span}(\\mathbf{r}_{A})$ and $\\operatorname{span}(\\mathbf{r}_{B})$.\n\nExplain why the Euclidean inner product can be misleading in this setting, and how the Mahalanobis inner product corrects the analysis in terms of subspace alignment. Express the final reported angle in radians.\n\nReport only the corrected principal angle $\\theta_{\\mathrm{M}}$ in radians as your final answer. Do not round if an exact value is available.",
            "solution": "The problem is assessed to be valid. All provided data and definitions are self-contained, scientifically grounded in the context of neural data analysis and linear algebra, and well-posed. The problem is objective and free of ambiguities, contradictions, or unsound premises. The task is a direct application of defined mathematical concepts to a realistic scenario in systems neuroscience.\n\nWe are given the following information:\nMean response vector for condition A: $\\mathbf{r}_{A} = \\begin{pmatrix} 10 \\\\ 1 \\end{pmatrix}$\nMean response vector for condition B: $\\mathbf{r}_{B} = \\begin{pmatrix} 10 \\\\ -1 \\end{pmatrix}$\nNoise covariance matrix: $\\mathbf{S} = \\begin{pmatrix} 100  0 \\\\ 0  1 \\end{pmatrix}$\n\nThe problem requires us to compute the angle between the subspaces spanned by $\\mathbf{r}_{A}$ and $\\mathbf{r}_{B}$ using two different inner products and to interpret the results. The angle $\\theta$ between two vectors $\\mathbf{x}$ and $\\mathbf{y}$ is given by $\\cos(\\theta) = \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\| \\, \\|\\mathbf{y}\\|}$.\n\nTask 1: Angle using the Euclidean inner product, $\\langle \\mathbf{x}, \\mathbf{y} \\rangle_{\\mathrm{E}} = \\mathbf{x}^{\\top} \\mathbf{y}$.\n\nFirst, we compute the inner product of $\\mathbf{r}_{A}$ and $\\mathbf{r}_{B}$:\n$$ \\langle \\mathbf{r}_{A}, \\mathbf{r}_{B} \\rangle_{\\mathrm{E}} = \\mathbf{r}_{A}^{\\top} \\mathbf{r}_{B} = \\begin{pmatrix} 10  1 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ -1 \\end{pmatrix} = (10)(10) + (1)(-1) = 100 - 1 = 99 $$\n\nNext, we compute the Euclidean norms of $\\mathbf{r}_{A}$ and $\\mathbf{r}_{B}$:\n$$ \\|\\mathbf{r}_{A}\\|_{\\mathrm{E}}^2 = \\langle \\mathbf{r}_{A}, \\mathbf{r}_{A} \\rangle_{\\mathrm{E}} = (10)^2 + (1)^2 = 100 + 1 = 101 $$\n$$ \\|\\mathbf{r}_{B}\\|_{\\mathrm{E}}^2 = \\langle \\mathbf{r}_{B}, \\mathbf{r}_{B} \\rangle_{\\mathrm{E}} = (10)^2 + (-1)^2 = 100 + 1 = 101 $$\nThe norms are $\\|\\mathbf{r}_{A}\\|_{\\mathrm{E}} = \\sqrt{101}$ and $\\|\\mathbf{r}_{B}\\|_{\\mathrm{E}} = \\sqrt{101}$.\n\nNow, we can compute $\\cos(\\theta_{\\mathrm{E}})$:\n$$ \\cos(\\theta_{\\mathrm{E}}) = \\frac{\\langle \\mathbf{r}_{A}, \\mathbf{r}_{B} \\rangle_{\\mathrm{E}}}{\\|\\mathbf{r}_{A}\\|_{\\mathrm{E}} \\, \\|\\mathbf{r}_{B}\\|_{\\mathrm{E}}} = \\frac{99}{\\sqrt{101} \\sqrt{101}} = \\frac{99}{101} $$\nThe angle is $\\theta_{\\mathrm{E}} = \\arccos\\left(\\frac{99}{101}\\right)$. Since $\\frac{99}{101}$ is very close to $1$, this is a small angle, suggesting the vectors are highly aligned. Numerically, this is approximately $0.1987$ radians or $11.4$ degrees.\n\nTask 2: Angle using the Mahalanobis inner product, $\\langle \\mathbf{x}, \\mathbf{y} \\rangle_{\\mathrm{M}} = \\mathbf{x}^{\\top} \\mathbf{S}^{-1} \\mathbf{y}$.\n\nFirst, we must find the inverse of the covariance matrix $\\mathbf{S}$. Since $\\mathbf{S}$ is a diagonal matrix, its inverse is the matrix of reciprocal diagonal entries:\n$$ \\mathbf{S}^{-1} = \\begin{pmatrix} 100  0 \\\\ 0  1 \\end{pmatrix}^{-1} = \\begin{pmatrix} \\frac{1}{100}  0 \\\\ 0  1 \\end{pmatrix} $$\n\nNow, we compute the Mahalanobis inner product of $\\mathbf{r}_{A}$ and $\\mathbf{r}_{B}$:\n$$ \\langle \\mathbf{r}_{A}, \\mathbf{r}_{B} \\rangle_{\\mathrm{M}} = \\mathbf{r}_{A}^{\\top} \\mathbf{S}^{-1} \\mathbf{r}_{B} = \\begin{pmatrix} 10  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{100}  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ -1 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} 10  1 \\end{pmatrix} \\begin{pmatrix} (10) \\cdot \\frac{1}{100} \\\\ (-1) \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 10  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{10} \\\\ -1 \\end{pmatrix} = (10)\\left(\\frac{1}{10}\\right) + (1)(-1) = 1 - 1 = 0 $$\nSince the Mahalanobis inner product is $0$, the vectors are orthogonal with respect to this inner product.\n\nWe can compute the corresponding value for $\\cos(\\theta_{\\mathrm{M}})$ without needing the norms, but for completeness, we calculate them:\n$$ \\|\\mathbf{r}_{A}\\|_{\\mathrm{M}}^2 = \\langle \\mathbf{r}_{A}, \\mathbf{r}_{A} \\rangle_{\\mathrm{M}} = \\mathbf{r}_{A}^{\\top} \\mathbf{S}^{-1} \\mathbf{r}_{A} = \\begin{pmatrix} 10  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{100}  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 1 \\end{pmatrix} = (10)^2\\left(\\frac{1}{100}\\right) + (1)^2(1) = 1 + 1 = 2 $$\n$$ \\|\\mathbf{r}_{B}\\|_{\\mathrm{M}}^2 = \\langle \\mathbf{r}_{B}, \\mathbf{r}_{B} \\rangle_{\\mathrm{M}} = \\mathbf{r}_{B}^{\\top} \\mathbf{S}^{-1} \\mathbf{r}_{B} = \\begin{pmatrix} 10  -1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{100}  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ -1 \\end{pmatrix} = (10)^2\\left(\\frac{1}{100}\\right) + (-1)^2(1) = 1 + 1 = 2 $$\nSo, $\\|\\mathbf{r}_{A}\\|_{\\mathrm{M}} = \\sqrt{2}$ and $\\|\\mathbf{r}_{B}\\|_{\\mathrm{M}} = \\sqrt{2}$.\n\nThen, $\\cos(\\theta_{\\mathrm{M}})$ is:\n$$ \\cos(\\theta_{\\mathrm{M}}) = \\frac{\\langle \\mathbf{r}_{A}, \\mathbf{r}_{B} \\rangle_{\\mathrm{M}}}{\\|\\mathbf{r}_{A}\\|_{\\mathrm{M}} \\, \\|\\mathbf{r}_{B}\\|_{\\mathrm{M}}} = \\frac{0}{\\sqrt{2} \\sqrt{2}} = 0 $$\nThe angle is $\\theta_{\\mathrm{M}} = \\arccos(0) = \\frac{\\pi}{2}$ radians.\n\nExplanation of the discrepancy:\n\nThe Euclidean inner product treats all axes in $\\mathbb{R}^2$ as equally important. In this case, both vectors have a large, identical component of $10$ along the first coordinate axis. This large common component dominates the dot product calculation, resulting in a value of $99$, which is very close to the product of the norms ($101$). Consequently, the angle $\\theta_{\\mathrm{E}}$ is small, giving a misleading impression that the neural response directions for conditions $A$ and $B$ are nearly identical.\n\nHowever, the problem context is crucial. The covariance matrix $\\mathbf{S}$ tells us that the neural activity is extremely variable along the first axis (variance $\\sigma_1^2 = 100$) and very stable along the second axis (variance $\\sigma_2^2 = 1$). A large signal component along a high-noise axis is less statistically reliable than a smaller signal component along a low-noise axis. The Euclidean geometry of the raw vector space is therefore a poor representation of the functional geometry of the neural code.\n\nThe Mahalanobis inner product corrects for this by re-scaling the space. It gives less weight to directions with high variance and more weight to directions with low variance. This is equivalent to transforming the vectors into a \"whitened\" space where the noise is isotropic (uniform in all directions). The transformation is $\\mathbf{r} \\to \\mathbf{S}^{-1/2}\\mathbf{r}$. In our case, $\\mathbf{S}^{-1/2} = \\begin{pmatrix} 1/10  0 \\\\ 0  1 \\end{pmatrix}$. Applying this gives:\n$$ \\mathbf{r}'_{A} = \\mathbf{S}^{-1/2}\\mathbf{r}_{A} = \\begin{pmatrix} 1/10  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 10 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n$$ \\mathbf{r}'_{B} = \\mathbf{S}^{-1/2}\\mathbf{r}_{B} = \\begin{pmatrix} 1/10  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 10 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nIn this statistically whitened space, the Mahalanobis inner product of $\\mathbf{r}_{A}$ and $\\mathbf{r}_{B}$ is equal to the Euclidean inner product of $\\mathbf{r}'_{A}$ and $\\mathbf{r}'_{B}$: $\\langle \\mathbf{r}'_A, \\mathbf{r}'_B \\rangle_E = 1 \\cdot 1 + 1 \\cdot (-1) = 0$. The vectors $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ are clearly orthogonal.\n\nThe Mahalanobis analysis reveals that after accounting for the anisotropic noise structure, the one-dimensional subspaces $\\operatorname{span}(\\mathbf{r}_{A})$ and $\\operatorname{span}(\\mathbf{r}_{B})$ are orthogonal. The seeming alignment was an artifact of a large common signal in a very noisy dimension. The true, distinguishing signals for the two conditions lie in directions that are statistically independent. The corrected angle $\\theta_{\\mathrm{M}} = \\frac{\\pi}{2}$ properly reflects this functional orthogonality.",
            "answer": "$$\n\\boxed{\\frac{\\pi}{2}}\n$$"
        },
        {
            "introduction": "When fitting statistical models like the General Linear Model (GLM) to neural data, the presence of non-uniform or correlated noise complicates parameter estimation. This practice demonstrates a powerful solution: constructing an orthonormal basis for your model's regressors with respect to a weighted inner product that reflects the noise structure. You will use the Gram-Schmidt procedure to see how this simplifies the estimation of model coefficients to a series of simple inner products.",
            "id": "4170518",
            "problem": "A short block-design functional Magnetic Resonance Imaging (fMRI) time series with four repetition times is modeled by a General Linear Model (GLM), where the observed Blood Oxygen Level-Dependent (BOLD) signal is represented as a linear combination of three regressors describing task events and a nuisance effect. Let the regressors be the column vectors\n$$\n\\mathbf{r}_{1} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad\n\\mathbf{r}_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{r}_{3} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\nNeural noise is empirically found to be independent across time but heteroscedastic, with covariance matrix\n$$\n\\mathbf{C} = \\operatorname{diag}(1, 2, 2, 1).\n$$\nConsider the weighted inner product induced by $\\mathbf{C}^{-1}$,\n$$\n\\langle \\mathbf{u}, \\mathbf{v} \\rangle_{\\mathbf{C}} = \\mathbf{u}^{\\top} \\mathbf{C}^{-1} \\mathbf{v},\n$$\nwhich is positive-definite and thus defines a valid inner product on $\\mathbb{R}^{4}$. The observed BOLD vector for this block is\n$$\n\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n\nUsing only the core definitions of inner products, orthogonal projection, and Gram–Schmidt orthonormalization on inner product spaces, do the following:\n\n1. Perform Gram–Schmidt orthonormalization on the set $\\{\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\mathbf{r}_{3}\\}$ with respect to $\\langle \\cdot, \\cdot \\rangle_{\\mathbf{C}}$ to produce an orthonormal set $\\{\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}\\}$ spanning the same subspace.\n2. Starting from the definition of the weighted least squares objective for the GLM with covariance $\\mathbf{C}$, derive why, when the design columns are orthonormal under $\\langle \\cdot, \\cdot \\rangle_{\\mathbf{C}}$, the coefficient estimates can be obtained directly from inner products with $\\mathbf{y}$.\n3. Compute the coefficient vector corresponding to the orthonormal design $\\mathbf{Q} = [\\mathbf{q}_{1}\\ \\mathbf{q}_{2}\\ \\mathbf{q}_{3}]$ for the observed data $\\mathbf{y}$.\n\nExpress your final coefficient vector as exact values in a single row vector. No rounding is required, and no physical units should be included in the final expression.",
            "solution": "We begin by recalling the foundational definitions. Given a symmetric positive-definite matrix $\\mathbf{C}$, the bilinear form $\\langle \\mathbf{u}, \\mathbf{v} \\rangle_{\\mathbf{C}} = \\mathbf{u}^{\\top} \\mathbf{C}^{-1} \\mathbf{v}$ defines an inner product on $\\mathbb{R}^{n}$, with norm $\\|\\mathbf{u}\\|_{\\mathbf{C}} = \\sqrt{\\langle \\mathbf{u}, \\mathbf{u} \\rangle_{\\mathbf{C}}}$. Orthogonal projection onto a subspace with respect to this inner product is characterized by minimizing the squared norm of the residual under $\\|\\cdot\\|_{\\mathbf{C}}$.\n\nThe Gram–Schmidt procedure in an inner product space proceeds as follows: for a set of linearly independent vectors $\\{\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\mathbf{r}_{3}\\}$, define\n$$\n\\mathbf{q}_{1} = \\frac{\\mathbf{r}_{1}}{\\|\\mathbf{r}_{1}\\|_{\\mathbf{C}}}, \\quad\n\\mathbf{u}_{2} = \\mathbf{r}_{2} - \\langle \\mathbf{r}_{2}, \\mathbf{q}_{1} \\rangle_{\\mathbf{C}} \\mathbf{q}_{1}, \\quad\n\\mathbf{q}_{2} = \\frac{\\mathbf{u}_{2}}{\\|\\mathbf{u}_{2}\\|_{\\mathbf{C}}}, \\quad\n\\mathbf{u}_{3} = \\mathbf{r}_{3} - \\langle \\mathbf{r}_{3}, \\mathbf{q}_{1} \\rangle_{\\mathbf{C}} \\mathbf{q}_{1} - \\langle \\mathbf{r}_{3}, \\mathbf{q}_{2} \\rangle_{\\mathbf{C}} \\mathbf{q}_{2}, \\quad\n\\mathbf{q}_{3} = \\frac{\\mathbf{u}_{3}}{\\|\\mathbf{u}_{3}\\|_{\\mathbf{C}}}.\n$$\nThis yields an orthonormal set $\\{\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}\\}$ under $\\langle \\cdot, \\cdot \\rangle_{\\mathbf{C}}$.\n\nCompute the pieces step by step. First, note that\n$$\n\\mathbf{C}^{-1} = \\operatorname{diag}\\!\\left(1, \\frac{1}{2}, \\frac{1}{2}, 1\\right).\n$$\nCompute $\\|\\mathbf{r}_{1}\\|_{\\mathbf{C}}^{2}$:\n$$\n\\|\\mathbf{r}_{1}\\|_{\\mathbf{C}}^{2} = \\mathbf{r}_{1}^{\\top} \\mathbf{C}^{-1} \\mathbf{r}_{1} = 1 \\cdot 1 \\cdot 1 + 2 \\cdot 2 \\cdot \\frac{1}{2} + 0 \\cdot 0 \\cdot \\frac{1}{2} + 1 \\cdot 1 \\cdot 1 = 1 + 2 + 0 + 1 = 4.\n$$\nHence $\\|\\mathbf{r}_{1}\\|_{\\mathbf{C}} = 2$ and\n$$\n\\mathbf{q}_{1} = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\nNext,\n$$\n\\langle \\mathbf{r}_{2}, \\mathbf{q}_{1} \\rangle_{\\mathbf{C}} = \\mathbf{r}_{2}^{\\top} \\mathbf{C}^{-1} \\mathbf{q}_{1} = \\begin{pmatrix} 0  1  1  0 \\end{pmatrix} \\mathbf{C}^{-1} \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0  1  1  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = \\frac{1}{2}.\n$$\nTherefore\n$$\n\\mathbf{u}_{2} = \\mathbf{r}_{2} - \\frac{1}{2} \\mathbf{q}_{1} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{4} \\\\ \\frac{1}{2} \\\\ 1 \\\\ -\\frac{1}{4} \\end{pmatrix}.\n$$\nCompute its norm:\n$$\n\\|\\mathbf{u}_{2}\\|_{\\mathbf{C}}^{2} = \\left(-\\frac{1}{4}\\right)^{2} \\cdot 1 + \\left(\\frac{1}{2}\\right)^{2} \\cdot \\frac{1}{2} + 1^{2} \\cdot \\frac{1}{2} + \\left(-\\frac{1}{4}\\right)^{2} \\cdot 1 = \\frac{1}{16} + \\frac{1}{8} + \\frac{1}{2} + \\frac{1}{16} = \\frac{3}{4}.\n$$\nThus $\\|\\mathbf{u}_{2}\\|_{\\mathbf{C}} = \\frac{\\sqrt{3}}{2}$ and\n$$\n\\mathbf{q}_{2} = \\frac{\\mathbf{u}_{2}}{\\|\\mathbf{u}_{2}\\|_{\\mathbf{C}}} = \\frac{2}{\\sqrt{3}} \\begin{pmatrix} -\\frac{1}{4} \\\\ \\frac{1}{2} \\\\ 1 \\\\ -\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2 \\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{2}{\\sqrt{3}} \\\\ -\\frac{1}{2 \\sqrt{3}} \\end{pmatrix}.\n$$\nProceed to $\\mathbf{r}_{3}$. First,\n$$\n\\langle \\mathbf{r}_{3}, \\mathbf{q}_{1} \\rangle_{\\mathbf{C}} = \\mathbf{r}_{3}^{\\top} \\mathbf{C}^{-1} \\mathbf{q}_{1} = \\begin{pmatrix} 1  0  1  1 \\end{pmatrix} \\mathbf{C}^{-1} \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1  0  1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = 1,\n$$\nso\n$$\n\\mathbf{v} = \\mathbf{r}_{3} - \\langle \\mathbf{r}_{3}, \\mathbf{q}_{1} \\rangle_{\\mathbf{C}} \\mathbf{q}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -1 \\\\ 1 \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\nCheck the remaining projection coefficient:\n$$\n\\langle \\mathbf{v}, \\mathbf{q}_{2} \\rangle_{\\mathbf{C}} = \\mathbf{v}^{\\top} \\mathbf{C}^{-1} \\mathbf{q}_{2} = \\begin{pmatrix} \\frac{1}{2}  -1  1  \\frac{1}{2} \\end{pmatrix} \\mathbf{C}^{-1} \\begin{pmatrix} -\\frac{1}{2 \\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{2}{\\sqrt{3}} \\\\ -\\frac{1}{2 \\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  -1  1  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2 \\sqrt{3}} \\\\ \\frac{1}{2 \\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ -\\frac{1}{2 \\sqrt{3}} \\end{pmatrix} = 0.\n$$\nTherefore $\\mathbf{u}_{3} = \\mathbf{v}$. Its norm is\n$$\n\\|\\mathbf{u}_{3}\\|_{\\mathbf{C}}^{2} = \\left(\\frac{1}{2}\\right)^{2} \\cdot 1 + (-1)^{2} \\cdot \\frac{1}{2} + 1^{2} \\cdot \\frac{1}{2} + \\left(\\frac{1}{2}\\right)^{2} \\cdot 1 = \\frac{1}{4} + \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{2},\n$$\nso $\\|\\mathbf{u}_{3}\\|_{\\mathbf{C}} = \\sqrt{\\frac{3}{2}}$ and\n$$\n\\mathbf{q}_{3} = \\frac{\\mathbf{u}_{3}}{\\|\\mathbf{u}_{3}\\|_{\\mathbf{C}}} = \\sqrt{\\frac{2}{3}} \\begin{pmatrix} \\frac{1}{2} \\\\ -1 \\\\ 1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\sqrt{\\frac{2}{3}} \\\\ -\\sqrt{\\frac{2}{3}} \\\\ \\sqrt{\\frac{2}{3}} \\\\ \\frac{1}{2} \\sqrt{\\frac{2}{3}} \\end{pmatrix}.\n$$\nBy construction, $\\{\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}\\}$ is orthonormal under $\\langle \\cdot, \\cdot \\rangle_{\\mathbf{C}}$.\n\nWe now justify the coefficient simplification. The weighted least squares (WLS) criterion for the GLM with covariance $\\mathbf{C}$ is to minimize\n$$\nJ(\\boldsymbol{\\beta}) = \\|\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}\\|_{\\mathbf{C}}^{2} = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^{\\top} \\mathbf{C}^{-1} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}),\n$$\nwhere $\\mathbf{X} = [\\mathbf{r}_{1}\\ \\mathbf{r}_{2}\\ \\mathbf{r}_{3}]$. Let $\\mathbf{Q} = [\\mathbf{q}_{1}\\ \\mathbf{q}_{2}\\ \\mathbf{q}_{3}]$ be an orthonormal basis for the same column space under $\\langle \\cdot, \\cdot \\rangle_{\\mathbf{C}}$. Any vector in the span can be uniquely expressed as $\\sum_{i=1}^{3} \\alpha_{i} \\mathbf{q}_{i}$, and the orthogonal projection of $\\mathbf{y}$ onto this span is\n$$\n\\mathbf{P}_{\\mathcal{S}} \\mathbf{y} = \\sum_{i=1}^{3} \\langle \\mathbf{y}, \\mathbf{q}_{i} \\rangle_{\\mathbf{C}} \\mathbf{q}_{i},\n$$\nbecause orthonormality implies that the coordinates $\\alpha_{i}$ are exactly the inner products $\\langle \\mathbf{y}, \\mathbf{q}_{i} \\rangle_{\\mathbf{C}}$. Hence, when the design columns are orthonormal under $\\langle \\cdot, \\cdot \\rangle_{\\mathbf{C}}$, the coefficient vector in the $\\mathbf{Q}$-basis is\n$$\n\\boldsymbol{\\beta}_{\\mathbf{Q}} = \\begin{pmatrix} \\langle \\mathbf{y}, \\mathbf{q}_{1} \\rangle_{\\mathbf{C}} \\\\ \\langle \\mathbf{y}, \\mathbf{q}_{2} \\rangle_{\\mathbf{C}} \\\\ \\langle \\mathbf{y}, \\mathbf{q}_{3} \\rangle_{\\mathbf{C}} \\end{pmatrix}.\n$$\n\nWe now compute these inner products explicitly. First,\n$$\n\\langle \\mathbf{y}, \\mathbf{q}_{1} \\rangle_{\\mathbf{C}} = \\mathbf{y}^{\\top} \\mathbf{C}^{-1} \\mathbf{q}_{1} = \\begin{pmatrix} 2  1  0  1 \\end{pmatrix} \\mathbf{C}^{-1} \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 2  1  0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = 2 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} + 0 \\cdot 0 + 1 \\cdot \\frac{1}{2} = 2.\n$$\nSecond,\n$$\n\\langle \\mathbf{y}, \\mathbf{q}_{2} \\rangle_{\\mathbf{C}} = \\mathbf{y}^{\\top} \\mathbf{C}^{-1} \\mathbf{q}_{2} = \\begin{pmatrix} 2  1  0  1 \\end{pmatrix} \\mathbf{C}^{-1} \\begin{pmatrix} -\\frac{1}{2 \\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{2}{\\sqrt{3}} \\\\ -\\frac{1}{2 \\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} 2  1  0  1 \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2 \\sqrt{3}} \\\\ \\frac{1}{2 \\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ -\\frac{1}{2 \\sqrt{3}} \\end{pmatrix} = -\\frac{1}{\\sqrt{3}} + \\frac{1}{2 \\sqrt{3}} + 0 - \\frac{1}{2 \\sqrt{3}} = -\\frac{1}{\\sqrt{3}}.\n$$\nThird,\n$$\n\\langle \\mathbf{y}, \\mathbf{q}_{3} \\rangle_{\\mathbf{C}} = \\mathbf{y}^{\\top} \\mathbf{C}^{-1} \\mathbf{q}_{3} = \\begin{pmatrix} 2  1  0  1 \\end{pmatrix} \\mathbf{C}^{-1} \\begin{pmatrix} \\frac{1}{2} \\sqrt{\\frac{2}{3}} \\\\ -\\sqrt{\\frac{2}{3}} \\\\ \\sqrt{\\frac{2}{3}} \\\\ \\frac{1}{2} \\sqrt{\\frac{2}{3}} \\end{pmatrix} = \\begin{pmatrix} 2  1  0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\sqrt{\\frac{2}{3}} \\\\ -\\frac{1}{2} \\sqrt{\\frac{2}{3}} \\\\ \\frac{1}{2} \\sqrt{\\frac{2}{3}} \\\\ \\frac{1}{2} \\sqrt{\\frac{2}{3}} \\end{pmatrix} = \\sqrt{\\frac{2}{3}} \\left(1 - \\frac{1}{2} + 0 + \\frac{1}{2}\\right) = \\sqrt{\\frac{2}{3}}.\n$$\n\nTherefore, the coefficient vector for the orthonormal design $\\mathbf{Q}$ is\n$$\n\\boldsymbol{\\beta}_{\\mathbf{Q}} = \\begin{pmatrix} 2  -\\frac{1}{\\sqrt{3}}  \\sqrt{\\frac{2}{3}} \\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix} 2  -\\frac{1}{\\sqrt{3}}  \\sqrt{\\frac{2}{3}} \\end{pmatrix}}$$"
        }
    ]
}