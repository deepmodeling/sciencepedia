{
    "hands_on_practices": [
        {
            "introduction": "在神经科学数据分析中，我们常常需要在不同的坐标系下审视同一份数据，例如，从解剖脑区定义的基向量切换到由独立成分分析（Independent Component Analysis, ICA）等统计方法提取的基向量。这个练习将带你从第一性原理出发，推导一个向量在两个不同基之间的坐标变换规则 ()。这不仅能巩固你对基向量和坐标表示的基本理解，也是进行多模态数据融合与比较的必备技能。",
            "id": "4175034",
            "problem": "神经科学数据分析中的一个常见操作是，在由不同基（例如解剖学定义的感兴趣区域与统计提取的成分）所诱导的不同坐标系中，表达相同的神经活动模式。考虑试验平均响应模式的实向量空间 $\\mathbb{R}^{3}$，其坐标对应于从钙成像中提取的三个潜藏特征。设 $\\mathcal{B} = \\{b_{1}, b_{2}, b_{3}\\}$ 是与解剖学定义的区域特征相关联的基，并设 $\\mathcal{B}' = \\{b'_{1}, b'_{2}, b'_{3}\\}$ 是通过独立成分分析 (ICA; Independent Component Analysis) 获得的基。\n\n仅从基、线性无关以及向量关于基的坐标表示的定义出发，推导向量 $y \\in \\mathbb{R}^{3}$ 的坐标如何通过一个基变换矩阵在 $\\mathcal{B}$ 和 $\\mathcal{B}'$ 之间进行变换。假设基变换矩阵 $T$ 将 $\\mathcal{B}$ 中的坐标映射到 $\\mathcal{B}'$ 中的坐标（因此，当定义域用 $\\mathcal{B}$ 表示，上域用 $\\mathcal{B}'$ 表示时，$T$ 是 $\\mathbb{R}^{3}$ 上恒等线性算子的矩阵表示）。解释为什么如果 $\\mathcal{B}$ 和 $\\mathcal{B}'$ 都是基，则 $T$ 必须是可逆的。\n\n然后，对于一个特定的数据集对齐，给定\n$$\nT \\;=\\;\n\\begin{pmatrix}\n2  -1  0 \\\\\n1  1  3 \\\\\n0  2  1\n\\end{pmatrix}.\n$$\n某个特定试验的神经活动模式具有 ICA 基坐标\n$$\n[y]_{\\mathcal{B}'} \\;=\\;\n\\begin{pmatrix}\n9 \\\\ -9 \\\\ 18\n\\end{pmatrix}.\n$$\n使用推导出的变换法则，精确计算解剖区域基坐标 $[y]_{\\mathcal{B}}$。将你的最终答案以最简整数形式的行向量表示；不需要四舍五入，也不需要报告物理单位。",
            "solution": "根据要求，解答分为三部分：变换法则的推导，基变换矩阵可逆性的解释，以及具体的计算。\n\n**1. 坐标变换法则的推导**\n\n设 $y$ 是向量空间 $\\mathbb{R}^{3}$ 中的一个任意向量。\n因为 $\\mathcal{B} = \\{b_1, b_2, b_3\\}$ 是 $\\mathbb{R}^{3}$ 的一个基，所以存在一组唯一的标量 $c_1, c_2, c_3 \\in \\mathbb{R}$ 使得\n$$ y = c_1 b_1 + c_2 b_2 + c_3 b_3 $$\n这些标量构成了 $y$ 关于基 $\\mathcal{B}$ 的坐标向量，记作 $[y]_{\\mathcal{B}}$：\n$$ [y]_{\\mathcal{B}} = \\begin{pmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{pmatrix} $$\n同样，因为 $\\mathcal{B}' = \\{b'_1, b'_2, b'_3\\}$ 也是 $\\mathbb{R}^{3}$ 的一个基，所以存在另一组唯一的标量 $d_1, d_2, d_3 \\in \\mathbb{R}$ 使得\n$$ y = d_1 b'_1 + d_2 b'_2 + d_3 b'_3 $$\n这些标量构成了坐标向量 $[y]_{\\mathcal{B}'}$：\n$$ [y]_{\\mathcal{B}'} = \\begin{pmatrix} d_1 \\\\ d_2 \\\\ d_3 \\end{pmatrix} $$\n问题陈述，矩阵 $T$ 表示恒等线性变换 $id: \\mathbb{R}^{3} \\to \\mathbb{R}^{3}$，其中 $id(y) = y$，定义域空间的基为 $\\mathcal{B}$，上域空间的基为 $\\mathcal{B}'$。根据线性算子矩阵表示的定义，矩阵 $T$ 将输入向量在定义域基下的坐标向量变换为输出向量在上域基下的坐标向量。\n因此，对于任意向量 $y$，我们有：\n$$ [id(y)]_{\\mathcal{B}'} = T [y]_{\\mathcal{B}} $$\n因为 $id(y) = y$，所以从基 $\\mathcal{B}$ 到基 $\\mathcal{B}'$ 的坐标变换法则是：\n$$ [y]_{\\mathcal{B}'} = T [y]_{\\mathcal{B}} $$\n这是基变换的基本关系。$T$ 的列是旧基向量（来自 $\\mathcal{B}$）在新基（$\\mathcal{B}'$）下的坐标向量。也就是说，$T$ 的第 $j$ 列是向量 $[b_j]_{\\mathcal{B}'}$。\n\n**2. 基变换矩阵 $T$ 的可逆性**\n\n一个方阵是可逆的，当且仅当其列向量是线性无关的。如上所述，$T$ 的列是坐标向量 $[b_1]_{\\mathcal{B}'}$、$[b_2]_{\\mathcal{B}'}$ 和 $[b_3]_{\\mathcal{B}'}$。为了证明 $T$ 是可逆的，我们必须证明这些列向量是线性无关的。\n考虑一个等于零向量的 $T$ 的列的线性组合：\n$$ c_1 [b_1]_{\\mathcal{B}'} + c_2 [b_2]_{\\mathcal{B}'} + c_3 [b_3]_{\\mathcal{B}'} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n从向量到其坐标表示的映射是一个线性变换（一个同构）。因此，我们可以组合左边的项：\n$$ [c_1 b_1 + c_2 b_2 + c_3 b_3]_{\\mathcal{B}'} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n唯一一个坐标向量为零向量的向量是向量空间本身的零向量。因此，\n$$ c_1 b_1 + c_2 b_2 + c_3 b_3 = \\mathbf{0} $$\n其中 $\\mathbf{0}$ 是 $\\mathbb{R}^3$ 中的零向量。\n根据定义，一组向量构成一个基，当且仅当其元素是线性无关的。因为 $\\mathcal{B} = \\{b_1, b_2, b_3\\}$ 是一个基，所以向量 $b_1, b_2, b_3$ 是线性无关的。这意味着上述方程的唯一解是平凡解，即所有标量系数都为零：\n$$ c_1 = 0, \\quad c_2 = 0, \\quad c_3 = 0 $$\n这表明，能产生零向量的 $T$ 的列的线性组合只有平凡组合。因此，$T$ 的列是线性无关的。一个具有线性无关列的方阵是可逆的。因此，从一个基到另一个基的基变换矩阵 $T$ 必须是可逆的。\n\n**3. 解剖区域基坐标的计算**\n\n我们已知变换法则 $[y]_{\\mathcal{B}'} = T [y]_{\\mathcal{B}}$、矩阵 $T$ 和坐标向量 $[y]_{\\mathcal{B}'}$。我们需要求出 $[y]_{\\mathcal{B}}$。由于 $T$ 是可逆的，我们可以从左边乘以它的逆矩阵 $T^{-1}$：\n$$ T^{-1} [y]_{\\mathcal{B}'} = T^{-1} T [y]_{\\mathcal{B}} = I [y]_{\\mathcal{B}} = [y]_{\\mathcal{B}} $$\n所以，我们必须计算 $[y]_{\\mathcal{B}} = T^{-1} [y]_{\\mathcal{B}'}$。\n首先，我们求 $T = \\begin{pmatrix} 2  -1  0 \\\\ 1  1  3 \\\\ 0  2  1 \\end{pmatrix}$ 的逆矩阵。\n$T$ 的行列式是：\n$$ \\det(T) = 2(1 \\cdot 1 - 3 \\cdot 2) - (-1)(1 \\cdot 1 - 3 \\cdot 0) + 0(1 \\cdot 2 - 1 \\cdot 0) $$\n$$ \\det(T) = 2(1 - 6) + 1(1) = 2(-5) + 1 = -10 + 1 = -9 $$\n因为 $\\det(T) \\neq 0$，所以逆矩阵存在。\n逆矩阵由 $T^{-1} = \\frac{1}{\\det(T)} \\text{adj}(T)$ 给出，其中 $\\text{adj}(T)$ 是伴随矩阵（代数余子式矩阵的转置）。\n代数余子式矩阵是：\n$$ C = \\begin{pmatrix} +|{1 \\atop 2} {3 \\atop 1}|  -|{1 \\atop 0} {3 \\atop 1}|  +|{1 \\atop 0} {1 \\atop 2}| \\\\ -|{-1 \\atop 2} {0 \\atop 1}|  +|{2 \\atop 0} {0 \\atop 1}|  -|{2 \\atop 0} {-1 \\atop 2}| \\\\ +|{-1 \\atop 1} {0 \\atop 3}|  -|{2 \\atop 1} {0 \\atop 3}|  +|{2 \\atop 1} {-1 \\atop 1}| \\end{pmatrix} = \\begin{pmatrix} -5  -1  2 \\\\ 1  2  -4 \\\\ -3  -6  3 \\end{pmatrix} $$\n伴随矩阵是代数余子式矩阵的转置：\n$$ \\text{adj}(T) = C^T = \\begin{pmatrix} -5  1  -3 \\\\ -1  2  -6 \\\\ 2  -4  3 \\end{pmatrix} $$\n逆矩阵是：\n$$ T^{-1} = \\frac{1}{-9} \\begin{pmatrix} -5  1  -3 \\\\ -1  2  -6 \\\\ 2  -4  3 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 5  -1  3 \\\\ 1  -2  6 \\\\ -2  4  -3 \\end{pmatrix} $$\n现在我们可以计算 $[y]_{\\mathcal{B}}$：\n$$ [y]_{\\mathcal{B}} = T^{-1} [y]_{\\mathcal{B}'} = \\frac{1}{9} \\begin{pmatrix} 5  -1  3 \\\\ 1  -2  6 \\\\ -2  4  -3 \\end{pmatrix} \\begin{pmatrix} 9 \\\\ -9 \\\\ 18 \\end{pmatrix} $$\n首先，我们进行矩阵-向量乘法：\n$$ \\begin{pmatrix} 5  -1  3 \\\\ 1  -2  6 \\\\ -2  4  -3 \\end{pmatrix} \\begin{pmatrix} 9 \\\\ -9 \\\\ 18 \\end{pmatrix} = \\begin{pmatrix} 5(9) + (-1)(-9) + 3(18) \\\\ 1(9) + (-2)(-9) + 6(18) \\\\ -2(9) + 4(-9) + (-3)(18) \\end{pmatrix} = \\begin{pmatrix} 45 + 9 + 54 \\\\ 9 + 18 + 108 \\\\ -18 - 36 - 54 \\end{pmatrix} = \\begin{pmatrix} 108 \\\\ 135 \\\\ -108 \\end{pmatrix} $$\n最后，我们乘以标量 $\\frac{1}{9}$：\n$$ [y]_{\\mathcal{B}} = \\frac{1}{9} \\begin{pmatrix} 108 \\\\ 135 \\\\ -108 \\end{pmatrix} = \\begin{pmatrix} \\frac{108}{9} \\\\ \\frac{135}{9} \\\\ \\frac{-108}{9} \\end{pmatrix} = \\begin{pmatrix} 12 \\\\ 15 \\\\ -12 \\end{pmatrix} $$\n$y$ 关于解剖基 $\\mathcal{B}$ 的坐标向量是 $\\begin{pmatrix} 12 \\\\ 15 \\\\ -12 \\end{pmatrix}$。题目要求将此答案表示为行向量。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 12 & 15 & -12 \\end{pmatrix} } $$"
        },
        {
            "introduction": "当我们构建统计模型（如通用线性模型，GLM）来解释神经活动时，一个关键要求是模型中的解释变量（即回归量）是线性无关的。如果出现共线性（线性相关），我们将无法唯一地确定每个变量对观测信号的贡献。本练习通过一个设计矩阵存在共线性的具体案例，让你亲手探究其导致的后果：你将找到该矩阵的非平凡零空间，并描述所有能够给出相同最优拟合结果的参数解集，从而将模型不可辨识性这一抽象概念具体化 ()。",
            "id": "4175036",
            "problem": "您正在神经科学数据分析中分析一个通用线性模型（GLM），该模型用于一个短时记录的四个时间点的局部场电位（LFP）振幅。该振幅被建模为三个回归量的线性组合：一个基线项，一个条件A的刺激事件，以及一个被无意中构造为基线和条件A回归量之和的复合无关回归量。设响应向量为 $$\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix},$$ 设计矩阵为 $$\\mathbf{X} = \\begin{pmatrix} 1  0  1 \\\\ 1  1  2 \\\\ 1  1  2 \\\\ 1  0  1 \\end{pmatrix},$$ 其中各列分别为基线、条件A和复合无关回归量。\n\n从线性无关、矩阵的秩、矩阵的零空间以及通过最小化平方误差 $$\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^{2}$$ 定义的最小二乘估计量的核心定义出发，完成以下任务：\n\n1. 使用零空间的代数定义，确定 $$\\mathbf{X}$$ 的零空间中的一个非零向量，从而证明回归量之间的共线性产生了一个非平凡的零空间。\n\n2. 使用最小二乘准则和零空间的性质，来刻画能产生相同拟合响应的参数向量的流形。选择一个特定的代表解，该解通过仅拟合基线和条件A回归量（即，排除复合无关回归量列）得到，然后将此解嵌入到完整的三参数空间中，以获得一个最小二乘解 $$\\boldsymbol{\\beta}_{0} \\in \\mathbb{R}^{3}$$。\n\n3. 结合您的结果，将等价参数向量的一维仿射子空间（在此设计下不可区分解的流形）以闭合形式写成关于自由标量参数 $$t \\in \\mathbb{R}$$ 的参数表达式。\n\n请将这个 $$\\mathbb{R}^{3}$$ 中流形的单一参数表达式作为您的最终答案。无需四舍五入。最终答案中不带单位。",
            "solution": "该问题要求分析一个最小二乘估计问题，其中设计矩阵 $\\mathbf{X}$ 的列是线性相关的。目标是找到最小化平方误差 $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^{2}$ 的参数向量 $\\boldsymbol{\\beta}$ 的完整集合。这组解构成一个仿射子空间，我们将对其进行刻画。\n\n给定的响应向量为 $\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix}$，设计矩阵为 $\\mathbf{X} = \\begin{pmatrix} 1  0  1 \\\\ 1  1  2 \\\\ 1  1  2 \\\\ 1  0  1 \\end{pmatrix}$。参数向量为 $\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix}$，分别对应于基线、条件A和复合无关回归量。\n\n**1. 确定 $\\mathbf{X}$ 的零空间**\n\n问题陈述，$\\mathbf{X}$ 的第三列（我们记为 $\\mathbf{c}_3$）是前两列 $\\mathbf{c}_1$ 和 $\\mathbf{c}_2$ 的和。我们可以验证这一点：\n$$\\mathbf{c}_1 + \\mathbf{c}_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\mathbf{c}_3$$\n这种线性相关性可以写为 $\\mathbf{c}_1 + \\mathbf{c}_2 - \\mathbf{c}_3 = \\mathbf{0}$。这个方程表示了 $\\mathbf{X}$ 的各列的一个线性组合，其结果为零向量。\n\n根据定义，矩阵 $\\mathbf{X}$ 的零空间，记作 $N(\\mathbf{X})$，是所有满足 $\\mathbf{X}\\mathbf{v} = \\mathbf{0}$ 的向量 $\\mathbf{v}$ 的集合。方程 $\\mathbf{c}_1 + \\mathbf{c}_2 - \\mathbf{c}_3 = \\mathbf{0}$ 可以用矩阵形式表示为：\n$$\\mathbf{X} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\mathbf{0}$$\n因此，非零向量 $\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}$ 在 $\\mathbf{X}$ 的零空间中。$\\mathbf{X}$ 的前两列是线性无关的，所以 $\\mathbf{X}$ 的秩为 $2$。根据秩-零度定理，零空间的维度（零度）是列数减去秩，即 $3 - 2 = 1$。因此，$\\mathbf{X}$ 的零空间是由 $\\mathbf{v}$ 张成的一维子空间：\n$$N(\\mathbf{X}) = \\{ t \\cdot \\mathbf{v} \\mid t \\in \\mathbb{R} \\} = \\left\\{ t \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} \\mid t \\in \\mathbb{R} \\right\\}$$\n\n**2. 寻找一个特解最小二乘解 $\\boldsymbol{\\beta}_0$**\n\n最小二乘解是使 $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^{2}$ 最小化的向量 $\\boldsymbol{\\beta}$。这些解满足正规方程 $\\mathbf{X}^{T}\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^{T}\\mathbf{y}$。因为 $\\mathbf{X}$ 的列线性相关，矩阵 $\\mathbf{X}^{T}\\mathbf{X}$ 是奇异的，所以 $\\boldsymbol{\\beta}$ 没有唯一解。然而，解集非空。\n\n为了找到一个特解，我们可以求解一个等价的、适定的问题。由于 $\\mathbf{c}_3$ 是冗余的，$\\mathbf{X}$ 的列空间与由其前两个（线性无关的）列构成的矩阵 $\\mathbf{X}_{red}$ 的列空间相同。\n$$\\mathbf{X}_{red} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  1 \\\\ 1  0 \\end{pmatrix}$$\n我们可以通过求解简化的最小二乘问题来找到唯一的参数向量 $\\boldsymbol{\\beta}_{red} = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}$：最小化 $\\|\\mathbf{y} - \\mathbf{X}_{red}\\boldsymbol{\\beta}_{red}\\|^{2}$。相应的正规方程为 $\\mathbf{X}_{red}^{T}\\mathbf{X}_{red}\\boldsymbol{\\beta}_{red} = \\mathbf{X}_{red}^{T}\\mathbf{y}$。\n\n首先，我们计算 $\\mathbf{X}_{red}^{T}\\mathbf{X}_{red}$：\n$$\\mathbf{X}_{red}^{T}\\mathbf{X}_{red} = \\begin{pmatrix} 1  1  1  1 \\\\ 0  1  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  1 \\\\ 1  0 \\end{pmatrix} = \\begin{pmatrix} 4  2 \\\\ 2  2 \\end{pmatrix}$$\n接着，我们计算 $\\mathbf{X}_{red}^{T}\\mathbf{y}$：\n$$\\mathbf{X}_{red}^{T}\\mathbf{y} = \\begin{pmatrix} 1  1  1  1 \\\\ 0  1  1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2+4+5+3 \\\\ 0+4+5+0 \\end{pmatrix} = \\begin{pmatrix} 14 \\\\ 9 \\end{pmatrix}$$\n现在我们求解这个线性方程组：\n$$\\begin{pmatrix} 4  2 \\\\ 2  2 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix} = \\begin{pmatrix} 14 \\\\ 9 \\end{pmatrix}$$\n这得到方程组：\n1. $$4\\beta_1 + 2\\beta_2 = 14 \\implies 2\\beta_1 + \\beta_2 = 7$$\n2. $$2\\beta_1 + 2\\beta_2 = 9$$\n第二个方程减去第一个方程得到 $\\beta_2 = 2$。将此结果代回第一个方程：$2\\beta_1 + 2 = 7 \\implies 2\\beta_1 = 5 \\implies \\beta_1 = \\frac{5}{2}$。\n所以，$\\boldsymbol{\\beta}_{red} = \\begin{pmatrix} 5/2 \\\\ 2 \\end{pmatrix}$。\n\n我们可以将此解嵌入到三维参数空间中，方法是将冗余回归量的系数设为零。这给出了原问题的一个特解 $\\boldsymbol{\\beta}_0$：\n$$\\boldsymbol{\\beta}_{0} = \\begin{pmatrix} 5/2 \\\\ 2 \\\\ 0 \\end{pmatrix}$$\n向量 $\\boldsymbol{\\beta}_0$ 是一个有效的最小二乘解，因为预测响应 $\\mathbf{X}\\boldsymbol{\\beta}_0 = \\mathbf{X}_{red}\\boldsymbol{\\beta}_{red}$ 是 $\\mathbf{y}$ 到 $\\mathbf{X}$ 列空间上的正交投影。\n\n**3. 所有解的流形的刻画**\n\n所有最小二乘解的集合，由所有能产生相同最小平方误差的向量 $\\boldsymbol{\\beta}$ 组成。这等价于找到所有与我们的特解 $\\boldsymbol{\\beta}_0$ 产生相同预测响应向量 $\\hat{\\mathbf{y}}$ 的 $\\boldsymbol{\\beta}$。\n$$\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}\\boldsymbol{\\beta}_{0}$$\n重排此方程，我们得到：\n$$\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{X}\\boldsymbol{\\beta}_{0} = \\mathbf{0} \\implies \\mathbf{X}(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_{0}) = \\mathbf{0}$$\n这个方程表明，差值 $\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_{0}$ 必须是 $\\mathbf{X}$ 零空间中的一个向量。\n从第1部分我们知道，$N(\\mathbf{X})$ 中的任意向量可以写为 $t \\cdot \\mathbf{v}$，其中 $t \\in \\mathbb{R}$ 是某个标量，且 $\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}$。\n因此，$\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_{0} = t\\mathbf{v}$，这意味着任何最小二乘解的一般形式是：\n$$\\boldsymbol{\\beta}(t) = \\boldsymbol{\\beta}_{0} + t\\mathbf{v}$$\n代入我们找到的向量 $\\boldsymbol{\\beta}_{0}$ 和 $\\mathbf{v}$：\n$$\\boldsymbol{\\beta}(t) = \\begin{pmatrix} 5/2 \\\\ 2 \\\\ 0 \\end{pmatrix} + t \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 5/2 + t \\\\ 2 + t \\\\ 0 - t \\end{pmatrix} = \\begin{pmatrix} 5/2 + t \\\\ 2 + t \\\\ -t \\end{pmatrix}$$\n这个参数表达式定义了所有使最小二乘准则最小化的参数向量构成的一维仿射子空间，它代表了这个秩亏模型下不可区分解的流形。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{5}{2} + t \\\\\n2 + t \\\\\n-t\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "与模型中出现不希望的线性相关性相反，我们也可以主动利用基变换来简化数据结构，发现其内在规律。这项练习将向你展示如何通过精心选择一组新的基向量（协方差矩阵的特征向量）来对相关的神经特征进行解耦 ()。通过推导并应用这一变换，你将把相关的特征转化为一组线性无关的主成分，这是神经科学中降维和特征工程的基石技术。",
            "id": "4175038",
            "problem": "在一项感觉解码实验中，您分析了来自单个皮层位点的试验平均神经特征。在 $T$ 次试验中的每一次，您记录一个特征向量 $x \\in \\mathbb{R}^{2}$，其分量为：$x_{1}$，一个固定刺激后窗口内的平均脉冲计数；以及 $x_{2}$，同一窗口内的平均局部场电位振幅。在对特征进行中心化以使其在所有试验上的经验均值为零后，您计算出样本协方差矩阵\n$$\nC \\;=\\; \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix}.\n$$\n仅使用核心的线性代数和概率论定义，构建一个正交基变换来对角化 $C$，从而使变换后的特征去相关。具体来说，考虑一个角度为 $\\theta$ 的旋转，由正交矩阵表示\n$$\nR(\\theta) \\;=\\; \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix},\n$$\n并从第一性原理出发，推导出在何种关于 $\\theta$ 的条件下，旋转坐标系下的协方差矩阵 $C' \\;=\\; R(\\theta)^{\\top} C \\, R(\\theta)$ 是对角矩阵。将 $R(\\theta)$ 的列向量解释为特征空间的基向量，并解释去相关如何对应于选择一个与数据协方差结构对齐的线性无关方向基。\n\n最后，对于上面给出的特定矩阵 $C$，确定在 $\\theta \\in \\left(0, \\frac{\\pi}{4}\\right)$ 范围内的唯一旋转角度（以弧度为单位），该角度可实现去相关。将您的最终答案表示为以弧度为单位的精确解析表达式。不要对结果进行近似或四舍五入。最终答案必须仅为 $\\theta$ 的值。",
            "solution": "该问题要求我们找到一个正交基变换，具体来说是一个旋转，以对角化给定的样本协方差矩阵 $C$。这个过程等价于寻找数据的主成分，即最大方差的方向。对特征进行去相关意味着将它们变换到一个新的坐标系中，在该坐标系中它们的协方差为零。\n\n让我们将原始协方差矩阵一般地表示为：\n$$\nC = \\begin{pmatrix} C_{11}  & C_{12} \\\\ C_{21}  & C_{22} \\end{pmatrix}\n$$\n其中由于协方差矩阵的对称性，有 $C_{12} = C_{21}$。旋转坐标系中的变换后协方差矩阵为 $C' = R(\\theta)^{\\top} C R(\\theta)$。旋转矩阵 $R(\\theta)$ 的转置是：\n$$\nR(\\theta)^{\\top} = \\begin{pmatrix} \\cos\\theta  & \\sin\\theta \\\\ -\\sin\\theta  & \\cos\\theta \\end{pmatrix}\n$$\n我们执行矩阵乘法来求出 $C'$ 的各项。让我们使用简写 $c = \\cos\\theta$ 和 $s = \\sin\\theta$。\n$$\nC' = \\begin{pmatrix} c  & s \\\\ -s  & c \\end{pmatrix} \\begin{pmatrix} C_{11}  & C_{12} \\\\ C_{12}  & C_{22} \\end{pmatrix} \\begin{pmatrix} c  & -s \\\\ s  & c \\end{pmatrix}\n$$\n首先，我们计算乘积 $C R(\\theta)$：\n$$\nC R(\\theta) = \\begin{pmatrix} C_{11}c + C_{12}s & -C_{11}s + C_{12}c \\\\ C_{12}c + C_{22}s & -C_{12}s + C_{22}c \\end{pmatrix}\n$$\n接下来，我们左乘 $R(\\theta)^{\\top}$：\n$$\nC' = \\begin{pmatrix} c(C_{11}c + C_{12}s) + s(C_{12}c + C_{22}s)  & c(-C_{11}s + C_{12}c) + s(-C_{12}s + C_{22}c) \\\\ -s(C_{11}c + C_{12}s) + c(C_{12}c + C_{22}s)  & -s(-C_{11}s + C_{12}c) + c(-C_{12}s + C_{22}c) \\end{pmatrix}\n$$\n为了使变换后的特征去相关，$C'$ 的非对角元素必须为零。让我们关注元素 $C'_{12}$：\n$$\nC'_{12} = c(-C_{11}s + C_{12}c) + s(-C_{12}s + C_{22}c)\n$$\n$$\nC'_{12} = -C_{11}sc + C_{12}c^2 - C_{12}s^2 + C_{22}sc\n$$\n$$\nC'_{12} = (C_{22} - C_{11})sc + C_{12}(c^2 - s^2)\n$$\n使用二倍角三角恒等式 $\\sin(2\\theta) = 2\\sin\\theta\\cos\\theta = 2sc$ 和 $\\cos(2\\theta) = \\cos^2\\theta - \\sin^2\\theta = c^2 - s^2$，我们可以重写 $C'_{12}$ 的表达式：\n$$\nC'_{12} = (C_{22} - C_{11})\\frac{\\sin(2\\theta)}{2} + C_{12}\\cos(2\\theta)\n$$\n去相关的条件是 $C'_{12} = 0$：\n$$\nC_{12}\\cos(2\\theta) = -(C_{22} - C_{11})\\frac{\\sin(2\\theta)}{2} = (C_{11} - C_{22})\\frac{\\sin(2\\theta)}{2}\n$$\n假设 $\\cos(2\\theta) \\neq 0$ 且 $C_{11} \\neq C_{22}$，我们可以重新整理这个等式来找到关于 $\\theta$ 的条件：\n$$\n\\frac{\\sin(2\\theta)}{\\cos(2\\theta)} = \\tan(2\\theta) = \\frac{2C_{12}}{C_{11} - C_{22}}\n$$\n这就是将任意 $2 \\times 2$ 对称矩阵 $C$ 对角化的旋转角度 $\\theta$ 所需满足的一般条件。\n\n现在，进行解释。旋转矩阵 $R(\\theta)$ 的列向量是特征空间的新基向量：\n$$\nv_1 = \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}, \\quad v_2 = \\begin{pmatrix} -\\sin\\theta \\\\ \\cos\\theta \\end{pmatrix}\n$$\n这些向量是正交的且长度为单位1，构成一个标准正交基。我们为 $\\theta$ 推导出的条件，正是使这些基向量成为协方差矩阵 $C$ 的特征向量的条件。当基向量是 $C$ 的特征向量时，由 $C$ 定义的线性变换在这个新基下的矩阵表示是对角的。对角线上的元素是 $C$ 的特征值，它们代表了数据沿新基向量方向（即主成分）的方差。通过将基向量与数据协方差结构的主轴（即最大和最小方差的方向）对齐，我们确保数据在这些新轴上的投影是不相关的。因此，去相关对应于选择一个由协方差矩阵的特征向量组成的线性无关基。\n\n最后，我们将此结果应用于给定的特定协方差矩阵：\n$$\nC = \\begin{pmatrix} 5  & 2 \\\\ 2  & 1 \\end{pmatrix}\n$$\n此处，$C_{11} = 5$，$C_{22} = 1$，且 $C_{12} = 2$。\n将这些值代入我们推导出的条件中：\n$$\n\\tan(2\\theta) = \\frac{2 C_{12}}{C_{11} - C_{22}} = \\frac{2(2)}{5 - 1} = \\frac{4}{4} = 1\n$$\n我们需要求解 $\\tan(2\\theta) = 1$，其中 $\\theta \\in \\left(0, \\frac{\\pi}{4}\\right)$。\n$2\\theta$ 的通解是 $2\\theta = \\arctan(1) + n\\pi$，其中 $n$ 为任意整数。$\\arctan(1)$ 的主值是 $\\frac{\\pi}{4}$。\n所以，$2\\theta = \\frac{\\pi}{4} + n\\pi$。\n这得到 $\\theta = \\frac{\\pi}{8} + \\frac{n\\pi}{2}$。\n我们必须在区间 $\\theta \\in \\left(0, \\frac{\\pi}{4}\\right)$ 内找到唯一的解。\n- 当 $n = 0$ 时，$\\theta = \\frac{\\pi}{8}$。由于 $0  \\frac{\\pi}{8}  \\frac{2\\pi}{8} = \\frac{\\pi}{4}$，此解在指定区间内。\n- 当 $n = 1$ 时，$\\theta = \\frac{\\pi}{8} + \\frac{\\pi}{2} = \\frac{5\\pi}{8}$，大于 $\\frac{\\pi}{4}$。\n- 当 $n = -1$ 时，$\\theta = \\frac{\\pi}{8} - \\frac{\\pi}{2} = -\\frac{3\\pi}{8}$，小于 $0$。\n因此，在给定范围内的唯一角度是 $\\frac{\\pi}{8}$。\n这个角度对应一个旋转，该旋转将新的基向量与 $C$ 的特征向量对齐，从而将 $C$ 对角化并使变换后的特征去相关。",
            "answer": "$$\\boxed{\\frac{\\pi}{8}}$$"
        }
    ]
}