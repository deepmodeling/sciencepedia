{
    "hands_on_practices": [
        {
            "introduction": "A primary goal in experimental design is maximizing the ability to detect a true effect, a concept known as statistical power. This exercise guides you through a first-principles analysis of how design choices impact power . You will explore quantitatively why within-subject designs are often more efficient and discover how to best allocate resources in a between-subjects study to achieve the most powerful test under budget constraints.",
            "id": "4161680",
            "problem": "A cognitive neuroscience laboratory is planning an experiment comparing condition-level neural responses (for example, Blood Oxygenation Level Dependent amplitudes in functional Magnetic Resonance Imaging (fMRI)) between two task conditions, denoted condition $A$ and condition $B$. The quantity of scientific interest is the mean difference $\\Delta = \\mu_{A} - \\mu_{B}$ in population-averaged neural response estimates. The team is considering either a between-subjects design (independent groups for the two conditions) or a within-subjects design (paired measurements from the same individuals in both conditions). You are asked to reason from first principles of sampling distributions and the Central Limit Theorem to analyze power as a function of allocation and to determine an optimal group-size ratio under a cost constraint.\n\nAssume the following modeling assumptions are justified by prior pilot data and standard preprocessing:\n- In the between-subjects design, each subject contributes a single condition-level estimate per assigned condition. Let the subject-level variability in condition-level estimates be $\\sigma_{1}^{2}$ for condition $A$ and $\\sigma_{2}^{2}$ for condition $B$. Suppose $n_{1}$ subjects are assigned to condition $A$ and $n_{2}$ subjects are assigned to condition $B$. The total number of condition-wise observations is $N_{\\mathrm{tot}} = n_{1} + n_{2}$.\n- In the within-subjects design, each of $m$ subjects is measured in both conditions, with per-condition subject-level variance $\\sigma^{2}$ and within-subject correlation $\\rho$ between the two condition-level estimates from the same person. The total number of condition-wise observations is $N_{\\mathrm{tot}} = 2 m$. The across-subject effect $\\Delta$ is the same target in both designs.\n- For large samples, use the Central Limit Theorem to model sample means as approximately Gaussian with the appropriate variances determined by independence and correlation structures.\n\nPart 1 (effect of unequal group sizes at fixed total observations). Suppose the between-subjects design has equal per-condition variance $\\sigma_{1}^{2} = \\sigma_{2}^{2} = \\sigma^{2}$ but potentially unequal group sizes with allocation ratio $r = n_{1} / n_{2}$. Holding the total number of condition-wise observations fixed at $N_{\\mathrm{tot}}$, derive the noncentrality parameter for the large-sample $Z$-test of $H_{0} : \\Delta = 0$ under the between-subjects design as a function of $r$ and $N_{\\mathrm{tot}}$. Do the same for the within-subjects design as a function of $N_{\\mathrm{tot}}$ and $\\rho$. Then derive an analytic expression for the ratio of noncentrality parameters (within-subjects relative to between-subjects) as a function of $r$ and $\\rho$, and interpret the dependence on $r$ and $\\rho$.\n\nPart 2 (optimal allocation under a linear cost constraint). Now return to the general between-subjects case with possibly unequal variances $\\sigma_{1}^{2}$ and $\\sigma_{2}^{2}$. Suppose per-subject cost for assigning a participant to condition $A$ is $c_{1}$ and for condition $B$ is $c_{2}$, with a fixed budget $C$ so that $c_{1} n_{1} + c_{2} n_{2} = C$. Using only the variance structure of the mean difference estimator and the Central Limit Theorem, derive from first principles the allocation ratio $r^{\\ast} = n_{1} / n_{2}$ that maximizes large-sample power for testing $H_{0} : \\Delta = 0$ under this budget.\n\nProvide your final answer as a single closed-form analytic expression for $r^{\\ast}$ in terms of $\\sigma_{1}$, $\\sigma_{2}$, $c_{1}$, and $c_{2}$. Do not include any additional commentary in your final answer. If you perform any intermediate numerical simplification, retain exact symbolic form in the final expression. Express no units in the final answer.",
            "solution": "The problem asks for an analysis of statistical power for two common experimental designs and for the derivation of an optimal sample size allocation under a budget constraint. The analysis will proceed from the first principles of sampling distributions.\n\nStatistical power for a test of the null hypothesis $H_{0} : \\Delta = 0$ against an alternative $H_{A} : \\Delta \\neq 0$ is a monotonically increasing function of the magnitude of the noncentrality parameter (NCP) of the test statistic's distribution under the alternative hypothesis. For a large-sample $Z$-test, the test statistic is given by $Z = \\frac{\\hat{\\Delta}}{\\text{SE}(\\hat{\\Delta})}$, where $\\hat{\\Delta}$ is the sample estimator of the mean difference $\\Delta = \\mu_{A} - \\mu_{B}$ and $\\text{SE}(\\hat{\\Delta})$ is its standard error. Under the alternative hypothesis, $\\mathbb{E}[\\hat{\\Delta}] = \\Delta$, and the test statistic follows a non-central normal distribution with a noncentrality parameter $\\lambda$ given by:\n$$\n\\lambda = \\frac{\\mathbb{E}[\\hat{\\Delta}]}{\\text{SE}(\\hat{\\Delta})} = \\frac{\\Delta}{\\sqrt{\\text{Var}(\\hat{\\Delta})}}\n$$\nMaximizing power is therefore equivalent to maximizing $|\\lambda|$, which in turn is equivalent to minimizing the variance of the estimator, $\\text{Var}(\\hat{\\Delta})$.\n\nPart 1: Comparison of Designs with Fixed Total Observations\n\nFirst, we analyze the between-subjects design. The estimator for the mean difference is $\\hat{\\Delta}_{\\text{between}} = \\bar{X}_{1} - \\bar{X}_{2}$, where $\\bar{X}_{1}$ and $\\bar{X}_{2}$ are the sample means for condition $A$ and condition $B$, respectively. Since the two groups of subjects are independent, the variance of the estimator is the sum of the variances of the sample means:\n$$\n\\text{Var}(\\hat{\\Delta}_{\\text{between}}) = \\text{Var}(\\bar{X}_{1}) + \\text{Var}(\\bar{X}_{2})\n$$\nFrom the Central Limit Theorem, for large sample sizes $n_{1}$ and $n_{2}$, the variances of the sample means are approximately $\\frac{\\sigma_{1}^{2}}{n_{1}}$ and $\\frac{\\sigma_{2}^{2}}{n_{2}}$. For Part $1$, we are given that $\\sigma_{1}^{2} = \\sigma_{2}^{2} = \\sigma^{2}$. Thus,\n$$\n\\text{Var}(\\hat{\\Delta}_{\\text{between}}) = \\frac{\\sigma^{2}}{n_{1}} + \\frac{\\sigma^{2}}{n_{2}} = \\sigma^{2} \\left( \\frac{1}{n_{1}} + \\frac{1}{n_{2}} \\right)\n$$\nWe need to express this variance in terms of the total number of condition-wise observations $N_{\\mathrm{tot}} = n_{1} + n_{2}$ and the allocation ratio $r = n_{1} / n_{2}$. We have $n_{1} = r n_{2}$. Substituting into the expression for $N_{\\mathrm{tot}}$ gives $N_{\\mathrm{tot}} = r n_{2} + n_{2} = (r+1)n_{2}$, which implies $n_{2} = \\frac{N_{\\mathrm{tot}}}{r+1}$ and $n_{1} = r \\frac{N_{\\mathrm{tot}}}{r+1}$. Substituting these into the variance expression:\n$$\n\\text{Var}(\\hat{\\Delta}_{\\text{between}}) = \\sigma^{2} \\left( \\frac{r+1}{r N_{\\mathrm{tot}}} + \\frac{r+1}{N_{\\mathrm{tot}}} \\right) = \\frac{\\sigma^{2}(r+1)}{N_{\\mathrm{tot}}} \\left( \\frac{1}{r} + 1 \\right) = \\frac{\\sigma^{2}(r+1)}{N_{\\mathrm{tot}}} \\frac{1+r}{r} = \\frac{\\sigma^{2}(r+1)^{2}}{r N_{\\mathrm{tot}}}\n$$\nThe noncentrality parameter for the between-subjects design, $\\lambda_{\\text{between}}$, is:\n$$\n\\lambda_{\\text{between}} = \\frac{\\Delta}{\\sqrt{\\frac{\\sigma^{2}(r+1)^{2}}{r N_{\\mathrm{tot}}}}} = \\frac{\\Delta \\sqrt{r N_{\\mathrm{tot}}}}{\\sigma (r+1)}\n$$\nNext, we analyze the within-subjects design. Here, $m$ subjects each provide a measurement for both conditions. The estimator $\\hat{\\Delta}_{\\text{within}}$ is the mean of the paired differences, $\\bar{D}$, where $D_{i} = X_{Ai} - X_{Bi}$ for the $i$-th subject. The variance of a single difference $D_{i}$ is:\n$$\n\\text{Var}(D_{i}) = \\text{Var}(X_{Ai} - X_{Bi}) = \\text{Var}(X_{Ai}) + \\text{Var}(X_{Bi}) - 2 \\text{Cov}(X_{Ai}, X_{Bi})\n$$\nGiven the per-condition variance is $\\sigma^{2}$ and the within-subject correlation is $\\rho$, we have $\\text{Cov}(X_{Ai}, X_{Bi}) = \\rho \\sqrt{\\sigma^{2} \\sigma^{2}} = \\rho \\sigma^{2}$. Therefore,\n$$\n\\text{Var}(D_{i}) = \\sigma^{2} + \\sigma^{2} - 2\\rho\\sigma^{2} = 2\\sigma^{2}(1-\\rho)\n$$\nThe variance of the sample mean difference $\\hat{\\Delta}_{\\text{within}} = \\bar{D}$ is:\n$$\n\\text{Var}(\\hat{\\Delta}_{\\text{within}}) = \\frac{\\text{Var}(D_{i})}{m} = \\frac{2\\sigma^{2}(1-\\rho)}{m}\n$$\nThe total number of condition-wise observations is $N_{\\mathrm{tot}} = 2m$, so $m = N_{\\mathrm{tot}}/2$. Substituting this into the variance expression:\n$$\n\\text{Var}(\\hat{\\Delta}_{\\text{within}}) = \\frac{2\\sigma^{2}(1-\\rho)}{N_{\\mathrm{tot}}/2} = \\frac{4\\sigma^{2}(1-\\rho)}{N_{\\mathrm{tot}}}\n$$\nThe noncentrality parameter for the within-subjects design, $\\lambda_{\\text{within}}$, is:\n$$\n\\lambda_{\\text{within}} = \\frac{\\Delta}{\\sqrt{\\frac{4\\sigma^{2}(1-\\rho)}{N_{\\mathrm{tot}}}}} = \\frac{\\Delta \\sqrt{N_{\\mathrm{tot}}}}{2\\sigma\\sqrt{1-\\rho}}\n$$\nThe ratio of the noncentrality parameters (within-subjects relative to between-subjects) is:\n$$\n\\frac{\\lambda_{\\text{within}}}{\\lambda_{\\text{between}}} = \\frac{\\frac{\\Delta \\sqrt{N_{\\mathrm{tot}}}}{2\\sigma\\sqrt{1-\\rho}}}{\\frac{\\Delta \\sqrt{r N_{\\mathrm{tot}}}}{\\sigma (r+1)}} = \\frac{\\Delta \\sqrt{N_{\\mathrm{tot}}}}{2\\sigma\\sqrt{1-\\rho}} \\cdot \\frac{\\sigma(r+1)}{\\Delta \\sqrt{r N_{\\mathrm{tot}}}} = \\frac{r+1}{2\\sqrt{r}\\sqrt{1-\\rho}}\n$$\nInterpretation of the ratio:\n1.  Dependence on $r$: The term $\\frac{r+1}{2\\sqrt{r}}$ is minimized when $r=1$ (balanced groups, $n_1=n_2$), where it equals $1$. Any imbalance ($r \\neq 1$) increases this term, making the between-subjects design less efficient and thus increasing the relative efficiency of the within-subjects design.\n2.  Dependence on $\\rho$: The ratio is inversely proportional to $\\sqrt{1-\\rho}$. As the within-subject correlation $\\rho$ increases towards $1$, the term $\\sqrt{1-\\rho}$ approaches $0$, and the ratio approaches infinity. This signifies a dramatic power advantage for the within-subjects design when measurements on the same subject are highly correlated, as the differencing operation effectively cancels out stable inter-subject variability. If $\\rho=0$, the ratio becomes $\\frac{r+1}{2\\sqrt{r}}$. If additionally $r=1$, the ratio is $1$, indicating equal power when the number of observations $N_{\\mathrm{tot}}$ is fixed and there is no within-subject correlation.\n\nPart 2: Optimal Allocation under a Budget Constraint\n\nWe now return to the general between-subjects case where variances $\\sigma_{1}^{2}$ and $\\sigma_{2}^{2}$ may differ. We seek to maximize power, which is equivalent to minimizing the variance of the estimator, $\\text{Var}(\\hat{\\Delta}_{\\text{between}})$, subject to a fixed budget $C$.\nThe optimization problem is:\nMinimize $V(n_{1}, n_{2}) = \\frac{\\sigma_{1}^{2}}{n_{1}} + \\frac{\\sigma_{2}^{2}}{n_{2}}$\nSubject to the constraint $g(n_{1}, n_{2}) = c_{1}n_{1} + c_{2}n_{2} - C = 0$.\n\nWe use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(n_{1}, n_{2}, \\gamma) = \\left( \\frac{\\sigma_{1}^{2}}{n_{1}} + \\frac{\\sigma_{2}^{2}}{n_{2}} \\right) - \\gamma (c_{1}n_{1} + c_{2}n_{2} - C)\n$$\nTo find the minimum, we set the partial derivatives with respect to $n_{1}$ and $n_{2}$ to zero, treating them as continuous variables:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_{1}} = -\\frac{\\sigma_{1}^{2}}{n_{1}^{2}} - \\gamma c_{1} = 0 \\implies \\frac{\\sigma_{1}^{2}}{n_{1}^{2}} = -\\gamma c_{1}\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_{2}} = -\\frac{\\sigma_{2}^{2}}{n_{2}^{2}} - \\gamma c_{2} = 0 \\implies \\frac{\\sigma_{2}^{2}}{n_{2}^{2}} = -\\gamma c_{2}\n$$\nFrom the first equation, we have $-\\gamma = \\frac{\\sigma_{1}^{2}}{c_{1}n_{1}^{2}}$. From the second, $-\\gamma = \\frac{\\sigma_{2}^{2}}{c_{2}n_{2}^{2}}$. Equating these two expressions for $-\\gamma$ gives:\n$$\n\\frac{\\sigma_{1}^{2}}{c_{1}n_{1}^{2}} = \\frac{\\sigma_{2}^{2}}{c_{2}n_{2}^{2}}\n$$\nWe now solve for the allocation ratio $r^{\\ast} = n_{1}/n_{2}$. Rearranging the terms:\n$$\n\\frac{n_{1}^{2}}{n_{2}^{2}} = \\frac{\\sigma_{1}^{2} c_{2}}{\\sigma_{2}^{2} c_{1}}\n$$\nTaking the square root of both sides (since sample sizes $n_{1}, n_{2}$ must be positive) yields the optimal ratio:\n$$\nr^{\\ast} = \\frac{n_{1}}{n_{2}} = \\sqrt{\\frac{\\sigma_{1}^{2} c_{2}}{\\sigma_{2}^{2} c_{1}}} = \\frac{\\sigma_{1}}{\\sigma_{2}} \\sqrt{\\frac{c_{2}}{c_{1}}}\n$$\nThis result shows that to maximize power under a budget constraint, one should allocate more subjects to the condition that is more variable (higher $\\sigma$) and less expensive (lower $c$). Specifically, the optimal sample size for a group is proportional to its standard deviation and inversely proportional to the square root of its per-subject cost.",
            "answer": "$$\n\\boxed{\\frac{\\sigma_{1}}{\\sigma_{2}} \\sqrt{\\frac{c_{2}}{c_{1}}}}\n$$"
        },
        {
            "introduction": "After establishing the relative power of different designs, it is crucial to understand how to quantify and compare the magnitude of effects across them. This practice delves into the world of standardized effect sizes, tasking you with deriving the formal relationship between the metrics used for between-subject (Cohen's $d$) and within-subject ($d_z$) comparisons . Mastering this conversion is an essential skill for meta-analysis and for planning new studies based on existing literature.",
            "id": "4161688",
            "problem": "A cognitive neuroscience laboratory is designing a new study to compare two task conditions, labeled condition $1$ and condition $2$, on the same participants (a within-subject design). The primary outcome is a continuous neural measure (for example, a single-trial averaged event-related potential amplitude). In a previous study using a within-subject design, the reported effect size was the standardized mean change $d_z$. The new laboratory intends to plan an independent-groups (between-subjects) study and therefore needs the independent-groups Cohen’s $d$ that would be equivalent to the previously reported $d_z$ under known distributional properties of the measurements.\n\nCarefully define and distinguish the following effect size metrics from first principles:\n- Cohen’s $d$ for two independent groups, using the pooled standard deviation,\n- Hedges’ $g$ as the small-sample bias-corrected version of Cohen’s $d$,\n- The standardized mean change $d_z$ for paired measurements in a within-subject design.\n\nAssume the following scientifically plausible conditions for the neural measure:\n- The two condition-specific measurements $X_1$ and $X_2$ are each approximately normally distributed and have finite second moments, with condition-specific population standard deviations $\\sigma_1$ and $\\sigma_2$, and the same sample size $n$ per condition if treated as independent groups.\n- In the within-subject design, the paired measurements have a population correlation $\\rho$ between $X_1$ and $X_2$.\n- The variance of the difference score in the within-subject design is given by the well-tested identity $\\operatorname{Var}(X_1 - X_2) = \\sigma_1^2 + \\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2$.\n- When planning the independent-groups study, the pooled standard deviation corresponds to the equal-$n$ pooled form based on the two condition-specific dispersions.\n\nStarting from these bases and without invoking any unproven shortcuts, derive the closed-form conversion that maps the within-subject standardized mean change $d_z$ to the between-subject Cohen’s $d$ as a function of $d_z$, $\\sigma_1$, $\\sigma_2$, and $\\rho$. Express your final answer as a single closed-form analytic expression for $d$ in terms of $d_z$, $\\sigma_1$, $\\sigma_2$, and $\\rho$. Do not include any units, and do not provide numerical approximations.",
            "solution": "The problem requires a rigorous derivation of the conversion formula between the standardized mean change for a within-subject design, $d_z$, and the Cohen’s $d$ for a between-subject design. The solution will be established from first principles, beginning with the formal definitions of the relevant effect size metrics.\n\nFirst, we define the three effect size metrics as requested. Let $\\mu_1$ and $\\mu_2$ be the population means for condition $1$ and condition $2$, respectively. Let $\\sigma_1$ and $\\sigma_2$ be the corresponding population standard deviations. The measurements under the two conditions are denoted by the random variables $X_1$ and $X_2$.\n\n1.  **Cohen’s $d$ for two independent groups**: This metric quantifies the magnitude of the difference between the means of two independent populations, standardized by their pooled standard deviation. The population parameter $d$ is defined as the difference between the population means divided by the population pooled standard deviation, $\\sigma_p$. For two groups with potentially different standard deviations, $\\sigma_1$ and $\\sigma_2$, and an equal number of subjects $n$ in the planned study, the pooled standard deviation is given by:\n    $$\n    \\sigma_p = \\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}}\n    $$\n    Therefore, Cohen's $d$ is defined as:\n    $$\n    d = \\frac{\\mu_1 - \\mu_2}{\\sigma_p} = \\frac{\\mu_1 - \\mu_2}{\\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}}}\n    $$\n\n2.  **Hedges’ $g$**: When Cohen's $d$ is estimated from sample data, it is a biased estimator of the population parameter $d$, especially for small sample sizes. Hedges' $g$ is a correction for this bias. It is calculated by multiplying the sample Cohen's $d$ by a correction factor, $J$, which is a function of the total degrees of freedom, $df$. For two independent groups of size $n_1$ and $n_2$, $df = n_1 + n_2 - 2$. The exact form of the correction factor is:\n    $$\n    J(df) = \\frac{\\Gamma\\left(\\frac{df}{2}\\right)}{\\sqrt{\\frac{df}{2}} \\Gamma\\left(\\frac{df-1}{2}\\right)}\n    $$\n    where $\\Gamma$ is the gamma function. Hedges' $g$ is then $g = d_{\\text{sample}} \\times J(df)$. This metric provides a more accurate estimate of the population effect size but is not required for the present derivation, which concerns the relationship between the population parameters $d$ and $d_z$.\n\n3.  **Standardized mean change $d_z$**: This effect size is used for within-subject (paired or repeated-measures) designs. It standardizes the mean difference between two paired measurements, $X_1$ and $X_2$, by the standard deviation of the difference scores. Let $D = X_1 - X_2$ be the difference score. The mean of the difference scores is $\\mu_D = E[D] = E[X_1 - X_2] = \\mu_1 - \\mu_2$. The standard deviation of the difference scores is $\\sigma_D = \\sqrt{\\operatorname{Var}(D)}$. The standardized mean change $d_z$ is defined as:\n    $$\n    d_z = \\frac{\\mu_D}{\\sigma_D} = \\frac{\\mu_1 - \\mu_2}{\\sqrt{\\operatorname{Var}(X_1 - X_2)}}\n    $$\n\nWith these definitions established, we can now derive the conversion formula. The objective is to express $d$ as a function of $d_z$, $\\sigma_1$, $\\sigma_2$, and the correlation $\\rho$ between the paired measurements $X_1$ and $X_2$.\n\nThe definitions for $d$ and $d_z$ are:\n$$\nd = \\frac{\\mu_1 - \\mu_2}{\\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}}}\n$$\n$$\nd_z = \\frac{\\mu_1 - \\mu_2}{\\sigma_{X_1 - X_2}}\n$$\nBoth expressions share the numerator $\\mu_1 - \\mu_2$. We can express this term using the definition of $d_z$:\n$$\n\\mu_1 - \\mu_2 = d_z \\cdot \\sigma_{X_1 - X_2}\n$$\nThe problem statement provides the identity for the variance of the difference between two correlated variables:\n$$\n\\operatorname{Var}(X_1 - X_2) = \\sigma_1^2 + \\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2\n$$\nThe standard deviation of the difference score, $\\sigma_{X_1 - X_2}$, is the square root of this variance:\n$$\n\\sigma_{X_1 - X_2} = \\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2}\n$$\nSubstituting this into the expression for the mean difference gives:\n$$\n\\mu_1 - \\mu_2 = d_z \\cdot \\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2}\n$$\nNow, we substitute this expression for $\\mu_1 - \\mu_2$ into the definition of Cohen's $d$:\n$$\nd = \\frac{d_z \\cdot \\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2}}{\\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}}}\n$$\nThis expression relates $d$ to $d_z$ and the other parameters. It can be simplified by combining the terms under a single square root:\n$$\nd = d_z \\cdot \\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2}{\\left(\\frac{\\sigma_1^2 + \\sigma_2^2}{2}\\right)}}\n$$\nMultiplying the numerator and denominator inside the square root by $2$ yields the final closed-form expression:\n$$\nd = d_z \\cdot \\sqrt{\\frac{2(\\sigma_1^2 + \\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2)}{\\sigma_1^2 + \\sigma_2^2}}\n$$\nThis equation provides the direct conversion from the within-subject effect size $d_z$ to the equivalent between-subject effect size Cohen's $d$, given the standard deviations of the two conditions and their correlation.",
            "answer": "$$\\boxed{d_z \\sqrt{\\frac{2(\\sigma_1^2 + \\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2)}{\\sigma_1^2 + \\sigma_2^2}}}$$"
        },
        {
            "introduction": "Real-world experimental data is often affected by nuisance variables that are not of primary scientific interest but can bias results if ignored. This exercise addresses the critical issue of confounding, showing how a systematic factor like time-related drift can create a spurious experimental effect . By working through the derivation of a regression-based adjustment, you will learn a powerful and general method for statistically controlling for confounds, thereby isolating the true effect of interest.",
            "id": "4161717",
            "problem": "A laboratory performs a within-subject experiment to measure a neural response $Y$ over the course of a single session. Each trial $i \\in \\{1,\\dots,n\\}$ is assigned to one of two conditions, encoded as $C_i \\in \\{0,1\\}$, and occurs at session-relative time $T_i \\in \\mathbb{R}$. It is well established that neuronal baselines can drift approximately linearly over time within a session due to arousal or fatigue. Investigators suspect that a linear drift with slope $\\beta$ is present. They also note that the scheduling placed the $C_i = 1$ trials disproportionately early in the session relative to the $C_i = 0$ trials. Consider the generative model\n$$\nY_i \\;=\\; \\theta \\;+\\; \\alpha\\, C_i \\;+\\; \\beta\\, T_i \\;+\\; \\varepsilon_i,\n$$\nwhere $\\theta \\in \\mathbb{R}$ is a baseline intercept, $\\alpha \\in \\mathbb{R}$ is the true condition effect, $\\beta \\in \\mathbb{R}$ is the linear drift slope, and $\\varepsilon_i$ are independent, mean-zero errors with $\\mathbb{E}[\\varepsilon_i] = 0$ and finite variance. Let $\\bar{Y}_c$ denote the sample mean of $Y_i$ within condition $C_i = c$, and similarly define $\\bar{T}_c$ as the sample mean of $T_i$ within condition $C_i = c$, for $c \\in \\{0,1\\}$.\n\nPart 1 (spurious difference): Assume $\\alpha = 0$. Using the above model and the definitions of conditional sample means, derive the expected value of the naive condition mean difference $\\bar{Y}_1 - \\bar{Y}_0$ when the linear drift is ignored. Express your final expression in terms of $\\beta$, $\\bar{T}_1$, and $\\bar{T}_0$.\n\nPart 2 (regression adjustment): Propose a regression adjustment that removes the bias induced by the correlation between $C$ and $T$. Derive the closed-form ordinary least squares (OLS) estimator of the adjusted condition effect, $\\hat{\\alpha}_{\\mathrm{adj}}$, when regressing $Y$ on both $C$ and $T$. Write the estimator in terms of the sample second moments\n$$\nS_{XY} \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} \\big(X_i - \\bar{X}\\big)\\big(Y_i - \\bar{Y}\\big),\n$$\nwhere $X$ and $Y$ can be any of $Y$, $C$, or $T$, and $\\bar{X}$ and $\\bar{Y}$ denote sample means. Your final expression for $\\hat{\\alpha}_{\\mathrm{adj}}$ must be in closed form using $S_{YC}$, $S_{YT}$, $S_{CT}$, $S_{CC}$, and $S_{TT}$ only.\n\nDiscuss, in your derivation, how the same bias mechanism can arise in a between-subject design (where $C$ is constant per subject) if the average session time or day of testing differs systematically between groups, and why the proposed regression adjustment applies equally to both within-subject and between-subject designs.\n\nYour final answer must be a closed-form analytic expression. If multiple expressions are required, present them as a single row matrix. No numerical approximation is required.",
            "solution": "The problem is addressed in two parts as requested.\n\nPart 1: Derivation of the expected spurious difference\n\nWe are asked to derive the expected value of the naive condition mean difference, $\\bar{Y}_1 - \\bar{Y}_0$, assuming the true condition effect is zero, i.e., $\\alpha = 0$.\n\nThe generative model under this assumption is:\n$$\nY_i = \\theta + \\beta T_i + \\varepsilon_i\n$$\nwhere $\\mathbb{E}[\\varepsilon_i] = 0$.\n\nThe sample mean of the response $Y$ for a given condition $c \\in \\{0, 1\\}$ is defined as $\\bar{Y}_c = \\frac{1}{n_c} \\sum_{i: C_i=c} Y_i$, where $n_c$ is the number of trials in condition $c$.\n\nLet us write out the expression for $\\bar{Y}_1$ and $\\bar{Y}_0$:\n$$\n\\bar{Y}_1 = \\frac{1}{n_1} \\sum_{i: C_i=1} Y_i = \\frac{1}{n_1} \\sum_{i: C_i=1} (\\theta + \\beta T_i + \\varepsilon_i)\n$$\nBy linearity of summation, this becomes:\n$$\n\\bar{Y}_1 = \\frac{1}{n_1} (n_1 \\theta + \\beta \\sum_{i: C_i=1} T_i + \\sum_{i: C_i=1} \\varepsilon_i) = \\theta + \\beta \\left( \\frac{1}{n_1} \\sum_{i: C_i=1} T_i \\right) + \\left( \\frac{1}{n_1} \\sum_{i: C_i=1} \\varepsilon_i \\right)\n$$\nUsing the definition of the conditional sample mean for time, $\\bar{T}_1 = \\frac{1}{n_1} \\sum_{i: C_i=1} T_i$, and defining $\\bar{\\varepsilon}_1 = \\frac{1}{n_1} \\sum_{i: C_i=1} \\varepsilon_i$, we have:\n$$\n\\bar{Y}_1 = \\theta + \\beta \\bar{T}_1 + \\bar{\\varepsilon}_1\n$$\nSimilarly, for condition $c=0$:\n$$\n\\bar{Y}_0 = \\theta + \\beta \\bar{T}_0 + \\bar{\\varepsilon}_0\n$$\nThe naive difference in condition means is therefore:\n$$\n\\bar{Y}_1 - \\bar{Y}_0 = (\\theta + \\beta \\bar{T}_1 + \\bar{\\varepsilon}_1) - (\\theta + \\beta \\bar{T}_0 + \\bar{\\varepsilon}_0) = \\beta(\\bar{T}_1 - \\bar{T}_0) + (\\bar{\\varepsilon}_1 - \\bar{\\varepsilon}_0)\n$$\nTo find the expected value of this difference, we take the expectation of both sides. In this context, the experimental design parameters, including the trial times $T_i$ and condition assignments $C_i$, are considered fixed. Thus, $\\bar{T}_1$ and $\\bar{T}_0$ are non-stochastic quantities. The only random variables are the error terms $\\varepsilon_i$.\n$$\n\\mathbb{E}[\\bar{Y}_1 - \\bar{Y}_0] = \\mathbb{E}[\\beta(\\bar{T}_1 - \\bar{T}_0) + (\\bar{\\varepsilon}_1 - \\bar{\\varepsilon}_0)] = \\beta(\\bar{T}_1 - \\bar{T}_0) + \\mathbb{E}[\\bar{\\varepsilon}_1] - \\mathbb{E}[\\bar{\\varepsilon}_0]\n$$\nThe expectation of the mean error in each condition is:\n$$\n\\mathbb{E}[\\bar{\\varepsilon}_c] = \\mathbb{E}\\left[\\frac{1}{n_c} \\sum_{i: C_i=c} \\varepsilon_i\\right] = \\frac{1}{n_c} \\sum_{i: C_i=c} \\mathbb{E}[\\varepsilon_i]\n$$\nSince $\\mathbb{E}[\\varepsilon_i] = 0$ for all $i$, we have $\\mathbb{E}[\\bar{\\varepsilon}_c] = 0$ for $c \\in \\{0, 1\\}$.\nSubstituting this into the expression for the expected difference:\n$$\n\\mathbb{E}[\\bar{Y}_1 - \\bar{Y}_0] = \\beta(\\bar{T}_1 - \\bar{T}_0)\n$$\nThis result shows that a spurious difference in condition means is expected if there is a non-zero linear drift ($\\beta \\neq 0$) and a systematic difference in the average timing of trials between the two conditions ($\\bar{T}_1 \\neq \\bar{T}_0$).\n\nPart 2: Regression adjustment and derivation of the OLS estimator\n\nThe proposed regression adjustment is to fit the full multiple linear model, which includes the confounding variable $T$ as a covariate:\n$$\nY_i = \\theta + \\alpha C_i + \\beta T_i + \\varepsilon_i\n$$\nWe seek the Ordinary Least Squares (OLS) estimator for the parameter $\\alpha$, which represents the condition effect adjusted for the linear time drift. We denote this estimator $\\hat{\\alpha}_{\\mathrm{adj}}$.\n\nThe OLS estimators for a multiple regression are commonly derived using centered variables. Let $y_i = Y_i - \\bar{Y}$, $c_i = C_i - \\bar{C}$, and $t_i = T_i - \\bar{T}$, where $\\bar{Y}$, $\\bar{C}$, and $\\bar{T}$ are the overall sample means. The regression of $y_i$ on $c_i$ and $t_i$ yields the estimators for $\\alpha$ and $\\beta$. The model for centered variables is $y_i = \\alpha c_i + \\beta t_i + \\varepsilon'_i$. The OLS estimators $\\hat{\\alpha}_{\\mathrm{adj}}$ and $\\hat{\\beta}_{\\mathrm{adj}}$ minimize the sum of squared residuals, $RSS = \\sum_{i=1}^{n} (y_i - \\alpha c_i - \\beta t_i)^2$.\n\nThe normal equations are obtained by setting the partial derivatives of $RSS$ with respect to $\\alpha$ and $\\beta$ to zero:\n$$\n\\frac{\\partial RSS}{\\partial \\alpha} = -2\\sum_{i=1}^{n} c_i(y_i - \\alpha c_i - \\beta t_i) = 0 \\quad \\implies \\quad \\alpha \\sum_{i=1}^{n} c_i^2 + \\beta \\sum_{i=1}^{n} c_i t_i = \\sum_{i=1}^{n} c_i y_i\n$$\n$$\n\\frac{\\partial RSS}{\\partial \\beta} = -2\\sum_{i=1}^{n} t_i(y_i - \\alpha c_i - \\beta t_i) = 0 \\quad \\implies \\quad \\alpha \\sum_{i=1}^{n} c_i t_i + \\beta \\sum_{i=1}^{n} t_i^2 = \\sum_{i=1}^{n} t_i y_i\n$$\nWe use the provided definition for sample second moments, $S_{XY} = \\frac{1}{n}\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) = \\frac{1}{n}\\sum_{i=1}^{n} x_i y_i$. The sums in the normal equations can be expressed in terms of these quantities:\n$\\sum c_i y_i = n S_{CY} = n S_{YC}$, $\\sum c_i^2 = n S_{CC}$, $\\sum c_i t_i = n S_{CT}$, $\\sum t_i y_i = n S_{TY} = n S_{YT}$, and $\\sum t_i^2 = n S_{TT}$.\n\nSubstituting these into the normal equations and dividing by $n$ gives a system of two linear equations for the estimators $\\hat{\\alpha}_{\\mathrm{adj}}$ and $\\hat{\\beta}_{\\mathrm{adj}}$:\n$$\n\\hat{\\alpha}_{\\mathrm{adj}} S_{CC} + \\hat{\\beta}_{\\mathrm{adj}} S_{CT} = S_{YC} \\quad (1)\n$$\n$$\n\\hat{\\alpha}_{\\mathrm{adj}} S_{CT} + \\hat{\\beta}_{\\mathrm{adj}} S_{TT} = S_{YT} \\quad (2)\n$$\nTo solve for $\\hat{\\alpha}_{\\mathrm{adj}}$, we first isolate $\\hat{\\beta}_{\\mathrm{adj}}$ from equation $(2)$:\n$$\n\\hat{\\beta}_{\\mathrm{adj}} = \\frac{S_{YT} - \\hat{\\alpha}_{\\mathrm{adj}} S_{CT}}{S_{TT}}\n$$\nNext, we substitute this expression into equation $(1)$:\n$$\n\\hat{\\alpha}_{\\mathrm{adj}} S_{CC} + \\left( \\frac{S_{YT} - \\hat{\\alpha}_{\\mathrm{adj}} S_{CT}}{S_{TT}} \\right) S_{CT} = S_{YC}\n$$\nTo clear the denominator, we multiply the entire equation by $S_{TT}$:\n$$\n\\hat{\\alpha}_{\\mathrm{adj}} S_{CC} S_{TT} + (S_{YT} - \\hat{\\alpha}_{\\mathrm{adj}} S_{CT}) S_{CT} = S_{YC} S_{TT}\n$$\nExpanding the terms:\n$$\n\\hat{\\alpha}_{\\mathrm{adj}} S_{CC} S_{TT} + S_{YT} S_{CT} - \\hat{\\alpha}_{\\mathrm{adj}} S_{CT}^2 = S_{YC} S_{TT}\n$$\nWe collect terms involving $\\hat{\\alpha}_{\\mathrm{adj}}$ on one side:\n$$\n\\hat{\\alpha}_{\\mathrm{adj}} (S_{CC} S_{TT} - S_{CT}^2) = S_{YC} S_{TT} - S_{YT} S_{CT}\n$$\nFinally, solving for $\\hat{\\alpha}_{\\mathrm{adj}}$ yields the closed-form OLS estimator:\n$$\n\\hat{\\alpha}_{\\mathrm{adj}} = \\frac{S_{YC} S_{TT} - S_{YT} S_{CT}}{S_{CC} S_{TT} - S_{CT}^2}\n$$\nThis estimator represents the partial effect of the condition $C$ on the response $Y$, having statistically controlled for the linear effect of time $T$.\n\nThe same bias mechanism and a corresponding solution apply to between-subject designs. In a between-subject design, the index $i$ would represent different subjects, and the condition variable $C_i$ would indicate the experimental group to which subject $i$ belongs. If another variable, $T_i$ (e.g., the time of day, day of the week, or age of the subject), is correlated with the group assignment $C_i$, it becomes a confounder. For instance, if the treatment group ($C_i = 1$) is tested predominantly in the morning and the control group ($C_i = 0$) in the afternoon, any diurnal variation in the neural response $Y$ will be aliased into the estimated treatment effect. The statistical structure of this problem is identical to the within-subject case: a non-zero correlation between $C$ and $T$ biases the simple estimate of the effect of $C$. The multiple regression model $Y_i = \\theta + \\alpha C_i + \\beta T_i + \\varepsilon_i$ is a general tool for handling linear confounding. The derivation of $\\hat{\\alpha}_{\\mathrm{adj}}$ is purely algebraic and does not depend on the specific interpretation of the index $i$ (trials vs. subjects). Therefore, including the confounding variable $T$ as a regressor is the appropriate adjustment in both within-subject and between-subject designs, and the resulting estimator for the adjusted effect is the same.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\beta (\\bar{T}_1 - \\bar{T}_0) & \\frac{S_{YC} S_{TT} - S_{YT} S_{CT}}{S_{CC} S_{TT} - S_{CT}^2} \\end{pmatrix}}\n$$"
        }
    ]
}