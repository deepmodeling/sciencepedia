{
    "hands_on_practices": [
        {
            "introduction": "在神经科学数据分析中，Z分数是一种检测异常值的常用方法，但它严重依赖于数据服从高斯分布的假设。本练习将通过一个基于钙成像荧光信号的假设场景，量化当数据呈现常见偏态分布时，使用Z分数导致的错误肯定率偏差。通过这个实践 ，您将深刻理解在应用统计方法前检验其基本假设的重要性。",
            "id": "4183422",
            "problem": "您的任务是设计并实现一个程序，用于在钙成像荧光轨迹的背景下，评估使用z-score的经典离群点检测方法的可靠性。钙成像信号是非负的，并且由于稀疏的向上瞬变，通常表现出非对称的正偏态分布。经典的z-score离群点检测器基于均值和标准差定义了一个单侧高阈值，并假设基线为高斯分布。当潜在的信号分布是偏态时，这个假设可能会错误估计假阳性率。\n\n您的程序必须将以下步骤操作化，这些步骤纯粹以数学和逻辑术语表述，不使用特定领域的文件格式或外部数据。所有量都应被视为无量纲。\n\n1. 形式化经典的单侧z-score阈值。设一个标量随机变量 $X$ 的总体均值为 $ \\mu $，总体标准差为 $ \\sigma $。对于一个选定的非负常数 $ c $，将经典的高离群点阈值定义为值 $ \\tau $，使得满足 $ X > \\tau $ 的值被标记为离群点，其中 $ \\tau = \\mu + c \\sigma $。\n\n2. 定义高斯假设下的名义假阳性率。如果标准化变量 $ Z = (X - \\mu)/\\sigma $ 是标准正态分布，那么对应于阈值 $ \\tau $ 的名义单侧假阳性率将是 $ \\mathbb{P}(Z > c) $。此量将用作名义基准。\n\n3. 在指定的偏态分布下量化实际假阳性率。对于产生正偏态的指定分布族和参数，计算实际的单侧假阳性率 $ \\mathbb{P}(X > \\tau) $，其中 $ \\tau $ 是根据该分布的真实 $ \\mu $ 和 $ \\sigma $ 计算的。使用总体级别的参数；不要使用有限样本来近似 $ \\mu $ 或 $ \\sigma $。\n\n4. 量化由偏态引起的误差。计算名义基准与实际假阳性率之间的绝对偏差，定义为 $ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right| $。\n\n5. 计算给定轨迹长度的预期误报数。对于一个正整数轨迹长度 $ T $（独立样本数），计算预期误报数为 $ T \\times \\mathbb{P}(X > \\tau) $。\n\n您的程序必须为以下五个测试用例（构成测试套件）中的每一个实现上述步骤。在每个用例中，随机变量 $ X $ 服从一个指定的正偏态分布，这是非负荧光强度的典型特征。对于对数正态分布和伽马分布，参数解释如下：对于对数正态分布，$ \\mu_{\\log} $ 是 $ \\log X $ 的均值，$ \\sigma_{\\log} $ 是 $ \\log X $ 的标准差；对于伽马分布，$ \\kappa $ 是形状参数，$ \\theta $ 是尺度参数。使用每个分布的总体矩来计算 $ \\mu $ 和 $ \\sigma $。测试套件如下：\n\n- 用例 $1$：对数正态分布，$ \\mu_{\\log} = 0 $，$ \\sigma_{\\log} = 0.5 $，$ c = 3 $，$ T = 10000 $。\n- 用例 $2$：对数正态分布，$ \\mu_{\\log} = 0 $，$ \\sigma_{\\log} = 0.5 $，$ c = 0 $，$ T = 10000 $。\n- 用例 $3$：伽马分布，$ \\kappa = 20 $，$ \\theta = 0.05 $，$ c = 3 $，$ T = 100000 $。\n- 用例 $4$：伽马分布，$ \\kappa = 1 $，$ \\theta = 1 $，$ c = 3 $，$ T = 10000 $。\n- 用例 $5$：对数正态分布，$ \\mu_{\\log} = 0 $，$ \\sigma_{\\log} = 1.0 $，$ c = 3 $，$ T = 10000 $。\n\n对于每个用例，您的程序必须计算：\n- 经典的单侧高阈值 $ \\tau $。\n- 高斯假设下的名义单侧假阳性率 $ \\mathbb{P}(Z > c) $。\n- 指定偏态分布下的实际单侧假阳性率 $ \\mathbb{P}(X > \\tau) $。\n- 绝对误差 $ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right| $。\n- 预期误报数 $ T \\times \\mathbb{P}(X > \\tau) $。\n\n最终输出格式：您的程序应生成单行输出，其中包含所有五个测试用例的聚合结果，格式为一个由方括号括起来的逗号分隔列表，每个测试用例由一个包含五个计算出的浮点数的内部列表表示，顺序为 $ [\\tau, \\mathbb{P}(Z > c), \\mathbb{P}(X > \\tau), \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right|, T \\times \\mathbb{P}(X > \\tau)] $。例如，两个假设用例的有效输出应如下所示：$ [[a_1,b_1,c_1,d_1,e_1],[a_2,b_2,c_2,d_2,e_2]] $，其中每个符号表示一个计算出的浮点数。所有概率和期望值都必须表示为小数，而不是百分比。",
            "solution": "目标是衡量当真实信号分布为正偏态时，假定高斯性的经典z-score阈值法会产生多大的偏差。我们从标准化、累积分布函数和总体矩的核心定义出发构建推导过程。\n\n1. 从经典z-score构建阈值。设 $ X $ 是一个实值随机变量，其总体均值为 $ \\mu $，总体标准差为 $ \\sigma $。一个观测值 $ x $ 的经典z-score定义为 $ z = (x - \\mu)/\\sigma $。对于一个选定的 $ c \\ge 0 $，经典的单侧高离群点阈值为 $ \\tau = \\mu + c \\sigma $，因为如果 $ z > c $，即等价于 $ x > \\mu + c \\sigma $，则 $ x $ 被标记。\n\n2. 高斯假设下的名义假阳性率。如果标准化变量 $ Z = (X - \\mu)/\\sigma $ 是标准正态分布，$ Z \\sim \\mathcal{N}(0,1) $，那么在阈值 $ c $ 下将一个基线观测值错误标记为离群点的概率是标准正态生存概率 $ \\mathbb{P}(Z > c) $。使用 $ \\mathcal{N}(0,1) $ 的累积分布函数 $ \\Phi $，该概率为 $ 1 - \\Phi(c) $。这个名义基准仅取决于 $ c $。\n\n3. 真实偏态分布下的实际假阳性率。实际的基线分布可能是正偏态的。我们考虑两个经过充分检验的分布族，它们可以模拟钙成像中常见的非负偏态：\n\n   - 对数正态分布：如果 $ Y = \\log X $ 是均值为 $ \\mu_{\\log} $、标准差为 $ \\sigma_{\\log} $ 的正态分布，那么 $ X $ 就是对数正态分布。其总体均值和方差可由正态变量的指数的性质得出。具体来说，\n     $$ \\mu = \\mathbb{E}[X] = \\exp\\left(\\mu_{\\log} + \\frac{\\sigma_{\\log}^{2}}{2}\\right), $$\n     $$ \\operatorname{Var}(X) = \\left(\\exp(\\sigma_{\\log}^{2}) - 1\\right) \\exp\\left(2 \\mu_{\\log} + \\sigma_{\\log}^{2}\\right), $$\n     所以总体标准差是 $ \\sigma = \\sqrt{\\operatorname{Var}(X)} $。生存函数 $ \\mathbb{P}(X > x) $ 由在 $ x $ 处使用参数 $ \\mu_{\\log} $ 和 $ \\sigma_{\\log} $ 求值的对数正态生存函数给出。对于阈值 $ \\tau = \\mu + c \\sigma $，实际假阳性率是来自对数正态生存函数的 $ \\mathbb{P}(X > \\tau) $。\n\n   - 伽马分布：如果 $ X $ 服从形状参数为 $ \\kappa $、尺度参数为 $ \\theta $ 的伽马分布，其总体均值和方差为\n     $$ \\mu = \\kappa \\theta, \\quad \\operatorname{Var}(X) = \\kappa \\theta^{2}, $$\n     得到 $ \\sigma = \\sqrt{\\kappa} \\theta $。生存函数 $ \\mathbb{P}(X > x) $ 由在 $ x $ 处使用参数 $ \\kappa $ 和 $ \\theta $ 求值的伽马生存函数给出。对于 $ \\tau = \\mu + c \\sigma $，实际假阳性率是来自伽马生存函数的 $ \\mathbb{P}(X > \\tau) $。\n\n   对于这两个族，我们依赖概率论中经过充分检验的总体矩公式和标准生存函数，以确保进行总体级别的计算，避免有限样本的变异性。\n\n4. 由偏态引起的误差量化。经典高斯基准与实际偏态分布之间的绝对误差为\n   $$ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right|. $$\n   这衡量了将高斯阈值应用于偏态数据时的校准误差。对于单侧高阈值，钙成像的正向瞬变使其具有现实意义。\n\n5. 轨迹长度的预期误报数。假设样本独立，在长度为 $ T $ 的轨迹中，基线样本超过阈值的预期数量为\n   $$ T \\times \\mathbb{P}(X > \\tau). $$\n   这将概率转换为预期计数，为分析流程中的性能提供了切实的衡量标准。\n\n6. 测试套件覆盖范围。我们选择五个用例来探究不同方面：\n   - 用例 $1$（常规情况）：对数正态分布，具有中等偏度（$ \\mu_{\\log} = 0 $，$ \\sigma_{\\log} = 0.5 $）和 $ c = 3 $，以评估名义和实际假阳性率之间的实际不匹配程度。\n   - 用例 $2$（边界条件）：相同的对数正态分布，但 $ c = 0 $，得到 $ \\tau = \\mu $；名义概率为 $ \\mathbb{P}(Z > 0) = 0.5 $，而实际概率 $ \\mathbb{P}(X > \\mu) $ 反映了这样一个事实：对于正偏态分布，均值超过中位数，因此高于均值的概率通常小于 $ 0.5 $。\n   - 用例 $3$（接近正态）：伽马分布，$ \\kappa = 20 $ 和 $ \\theta = 0.05 $，其均值为 $ \\mu = 1 $，方差为 $ \\operatorname{Var}(X) = 0.05 $，导致相对较小的偏度；当 $ c = 3 $ 时，高斯名义值和实际值应非常接近。\n   - 用例 $4$（极端偏态）：伽马分布，$ \\kappa = 1 $ 和 $ \\theta = 1 $，类似于指数分布，产生显著的偏度；$ c = 3 $ 的阈值在名义值和实际值之间会有显著差异。\n   - 用例 $5$（对数正态的重度偏态）：对数正态分布，$ \\mu_{\\log} = 0 $ 和 $ \\sigma_{\\log} = 1.0 $，显著增加了偏度和重尾，用于在 $ c = 3 $ 时测试高斯假设的稳健性。\n\n7. 输出规范。对于每个用例，计算 $ \\tau $、$ \\mathbb{P}(Z > c) $、$ \\mathbb{P}(X > \\tau) $、$ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right| $ 和 $ T \\times \\mathbb{P}(X > \\tau) $。将所有五个用例的结果聚合到一个列表的列表中，并按要求的格式单行打印。所有概率和期望值必须是小数，不带百分号。\n\n该程序使用标准正态生存函数计算 $ \\mathbb{P}(Z > c) $，使用对数正态和伽马生存函数计算 $ \\mathbb{P}(X > \\tau) $，从而确保对指定参数进行准确的总体级别计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, lognorm, gamma\n\ndef compute_lognormal_moments(mu_log: float, sigma_log: float):\n    \"\"\"\n    Compute population mean and std for a log-normal with log-mean mu_log\n    and log-std sigma_log.\n    \"\"\"\n    # Mean: exp(mu_log + 0.5 * sigma_log^2)\n    mean = np.exp(mu_log + 0.5 * (sigma_log ** 2))\n    # Variance: (exp(sigma_log^2) - 1) * exp(2*mu_log + sigma_log^2)\n    var = (np.exp(sigma_log ** 2) - 1.0) * np.exp(2.0 * mu_log + (sigma_log ** 2))\n    std = np.sqrt(var)\n    return mean, std\n\ndef compute_gamma_moments(kappa: float, theta: float):\n    \"\"\"\n    Compute population mean and std for a gamma with shape kappa and scale theta.\n    \"\"\"\n    mean = kappa * theta\n    var = kappa * (theta ** 2)\n    std = np.sqrt(var)\n    return mean, std\n\ndef nominal_fp_rate(c: float) - float:\n    \"\"\"\n    Nominal one-sided false positive rate under N(0,1): P(Z  c).\n    \"\"\"\n    return float(norm.sf(c))\n\ndef actual_fp_rate_lognorm(tau: float, mu_log: float, sigma_log: float) - float:\n    \"\"\"\n    Actual one-sided false positive rate for log-normal: P(X  tau).\n    SciPy's lognorm parameterization uses shape=sigma_log, scale=exp(mu_log).\n    \"\"\"\n    s = sigma_log\n    scale = np.exp(mu_log)\n    return float(lognorm.sf(tau, s=s, scale=scale))\n\ndef actual_fp_rate_gamma(tau: float, kappa: float, theta: float) - float:\n    \"\"\"\n    Actual one-sided false positive rate for gamma: P(X  tau).\n    SciPy's gamma parameterization uses a=shape (kappa), scale=theta.\n    \"\"\"\n    return float(gamma.sf(tau, a=kappa, scale=theta))\n\ndef format_nested_list_no_spaces(nested):\n    \"\"\"\n    Format a nested list into a compact string representation with no spaces.\n    \"\"\"\n    def format_item(x):\n        if isinstance(x, list):\n            return \"[\" + \",\".join(format_item(y) for y in x) + \"]\"\n        else:\n            # Ensure standard float/integer string without spaces\n            return str(x)\n    return format_item(nested)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: dict with keys 'dist', parameters, 'c', 'T'.\n    test_cases = [\n        # Case 1: Log-normal moderate skew\n        {\"dist\": \"lognorm\", \"mu_log\": 0.0, \"sigma_log\": 0.5, \"c\": 3.0, \"T\": 10000},\n        # Case 2: Log-normal boundary c=0\n        {\"dist\": \"lognorm\", \"mu_log\": 0.0, \"sigma_log\": 0.5, \"c\": 0.0, \"T\": 10000},\n        # Case 3: Gamma near-normal (large shape)\n        {\"dist\": \"gamma\", \"kappa\": 20.0, \"theta\": 0.05, \"c\": 3.0, \"T\": 100000},\n        # Case 4: Gamma extreme skew (exponential-like)\n        {\"dist\": \"gamma\", \"kappa\": 1.0, \"theta\": 1.0, \"c\": 3.0, \"T\": 10000},\n        # Case 5: Log-normal heavy skew\n        {\"dist\": \"lognorm\", \"mu_log\": 0.0, \"sigma_log\": 1.0, \"c\": 3.0, \"T\": 10000},\n    ]\n\n    results = []\n    for case in test_cases:\n        c = float(case[\"c\"])\n        T = int(case[\"T\"])\n\n        if case[\"dist\"] == \"lognorm\":\n            mu_log = float(case[\"mu_log\"])\n            sigma_log = float(case[\"sigma_log\"])\n            # Population moments\n            mu, sigma = compute_lognormal_moments(mu_log, sigma_log)\n            tau = mu + c * sigma\n            p_nom = nominal_fp_rate(c)\n            p_act = actual_fp_rate_lognorm(tau, mu_log, sigma_log)\n        elif case[\"dist\"] == \"gamma\":\n            kappa = float(case[\"kappa\"])\n            theta = float(case[\"theta\"])\n            # Population moments\n            mu, sigma = compute_gamma_moments(kappa, theta)\n            tau = mu + c * sigma\n            p_nom = nominal_fp_rate(c)\n            p_act = actual_fp_rate_gamma(tau, kappa, theta)\n        else:\n            # Should not happen in the defined test suite.\n            raise ValueError(\"Unsupported distribution type\")\n\n        abs_err = abs(p_nom - p_act)\n        exp_false_alarms = T * p_act\n\n        results.append([tau, p_nom, p_act, abs_err, exp_false_alarms])\n\n    # Final print statement in the exact required format: single line, no spaces.\n    print(format_nested_list_no_spaces(results))\n\nsolve()\n```"
        },
        {
            "introduction": "真实世界的神经科学数据通常是高维的，这使得单变量异常检测方法不再适用。本练习将指导您实现马氏距离（Mahalanobis distance），这是一种强大的多变量异常值检测工具，它能同时考虑多个特征的方差和协方差。您将学习如何基于卡方分布  设定一个有统计学依据的阈值来识别例如脑电图（EEG）数据中的伪迹成分。",
            "id": "4183436",
            "problem": "给定您几个数值数据集，每个数据集代表从脑电图（EEG）成分的独立成分分析（ICA）中提取的特征。对于每个数据集，将行视为观测值（成分），将列视为特征。您的任务是仅从基础统计定义出发，使用马氏距离（Mahalanobis distance）和卡方分布（chi-square distribution）实现一个规范的多元离群点检测程序。您必须编写一个程序，该程序对每个数据集，标记出平方马氏距离超过基于卡方分位数阈值的观测值的索引（从零开始），然后将所有数据集中标记的索引列表聚合到一个顶层列表中。\n\n使用的基本原理和定义：\n- 设 $X \\in \\mathbb{R}^{n \\times p}$ 表示具有 $n$ 个观测值和 $p$ 个特征的特征矩阵。设 $\\mathbf{x}_i \\in \\mathbb{R}^p$ 为 $X$ 的第 $i$ 行。\n- 样本均值向量为 $\\boldsymbol{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{x}_i$。\n- 无偏样本协方差矩阵为 $\\boldsymbol{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^{n} (\\mathbf{x}_i - \\boldsymbol{\\mu})(\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top}$。\n- 如果 $\\boldsymbol{\\Sigma}$ 是奇异或病态的，则使用 Moore–Penrose 伪逆，记作 $\\boldsymbol{\\Sigma}^{+}$。\n- $\\mathbf{x}_i$ 的平方马氏距离为 $\\delta_i^2 = (\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{+} (\\mathbf{x}_i - \\boldsymbol{\\mu})$。\n- 在特征近似服从联合高斯分布且能被 $\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\Sigma}$ 很好地建模的假设下，统计量 $\\delta_i^2$ 与自由度为 $p$ 的卡方分布的上分位数进行比较。对于给定的显著性水平 $\\alpha \\in (0,1)$，定义阈值 $\\tau = F^{-1}_{\\chi^2_p}(1-\\alpha)$，其中 $F^{-1}_{\\chi^2_p}$ 是自由度为 $p$ 的卡方分布的反累积分布函数（分位数函数）。如果 $\\delta_i^2 > \\tau$，则标记观测值 $i$。\n\n您的程序必须使用以下测试套件（每个测试用例是一对 $(X, \\alpha)$）实现上述过程。所有索引都是从零开始的，并且对于每个用例，您必须按严格递增的顺序返回标记的索引。\n\n测试用例 1（正常路径，中等维度 $p = 4$）：\n- $X \\in \\mathbb{R}^{8 \\times 4}$，其行向量为\n  - $[0.2, -0.1, 0.3, 0.0]$\n  - $[-0.3, 0.4, -0.2, 0.5]$\n  - $[0.1, 0.2, 0.0, -0.1]$\n  - $[-0.2, -0.3, 0.1, 0.2]$\n  - $[0.0, 0.1, -0.1, 0.1]$\n  - $[0.3, -0.2, 0.2, -0.2]$\n  - $[6.0, 6.5, 5.8, 6.2]$\n  - $[4.0, -4.5, 3.8, -4.2]$\n- $\\alpha = 0.01$.\n\n测试用例 2（接近奇异的协方差，$p = 3$）：\n- $X \\in \\mathbb{R}^{5 \\times 3}$，其行向量为\n  - $[-1.0, -2.0, 1.0]$\n  - $[0.0, 0.1, 0.0]$\n  - $[1.0, 2.1, -1.1]$\n  - $[1.1, 2.2, -1.05]$\n  - $[5.0, 10.2, -5.1]$\n- $\\alpha = 0.05$.\n\n测试用例 3（小样本 $n = p = 3$）：\n- $X \\in \\mathbb{R}^{3 \\times 3}$，其行向量为\n  - $[0.0, 0.0, 0.0]$\n  - $[0.2, -0.1, 0.1]$\n  - $[3.5, 3.5, 3.5]$\n- $\\alpha = 0.05$.\n\n测试用例 4（在严格阈值下无离群点，$p = 2$）：\n- $X \\in \\mathbb{R}^{6 \\times 2}$，其行向量为\n  - $[0.1, -0.2]$\n  - $[-0.2, 0.3]$\n  - $[0.2, -0.1]$\n  - $[0.0, 0.0]$\n  - $[0.3, 0.2]$\n  - $[-0.1, -0.3]$\n- $\\alpha = 0.001$.\n\n要求输出：\n- 对于每个测试用例，计算集合 $\\{\\,i \\in \\{0,1,\\dots,n-1\\} \\mid \\delta_i^2 > \\tau \\,\\}$，并将其作为按升序排列的索引列表返回。\n- 将这些按用例生成的列表按照测试用例的顺序聚合到一个顶层列表中。\n- 您的程序应生成包含此聚合列表的单行输出，格式为用方括号括起来的逗号分隔列表，且不含空格，例如：$[\\,[\\,]\\, , \\,[\\,0,2\\,]\\,]$ 必须打印为 \"[[],[0,2]]\"。注意，打印的行中任何地方都不能有空格。\n- 所有量都是纯数字。不要使用百分号；显著性水平必须按小数处理，例如 $\\alpha = 0.05$。\n\n假设：\n- 只要需要，就必须使用伪逆 $\\boldsymbol{\\Sigma}^{+}$。您可以使用基于奇异值分解的伪逆或数值上等效的稳定方法。\n- 本问题不涉及角度和物理单位。\n\n您的代码必须是一个完整的、可运行的程序，实现上述要求并以指定格式打印最终结果。",
            "solution": "该问题要求实现一个基于马氏距离（Mahalanobis distance）的规范的多元离群点检测算法。这种统计程序特别适用于识别多维特征空间中的异常观测值，例如从脑电图（EEG）数据的独立成分分析（ICA）中派生的特征。该方法利用数据的协方差结构来定义一个对特征空间的线性变换不变的距离度量。其核心逻辑包括对数据分布进行建模，并标记出位于该分布低概率区域中的点。\n\n解决方案从第一性原理出发，阐述如下：\n\n1.  **统计参数估计**：对于由矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 表示的给定数据集（其中有 $n$ 个观测值和 $p$ 个特征），第一步是估计基础数据分布的参数。我们假设数据可以通过其前两个矩来充分描述：中心和离散程度。\n    -   数据云的中心由**样本均值向量** $\\boldsymbol{\\mu} \\in \\mathbb{R}^p$ 估计。它被计算为观测向量的算术平均值：\n        $$ \\boldsymbol{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{x}_i $$\n        其中 $\\mathbf{x}_i$ 是 $X$ 的第 $i$ 行（观测值）。\n    -   离散程度，以及更重要的，特征之间的相关结构，由**无偏样本协方差矩阵** $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$ 估计。其定义为：\n        $$ \\boldsymbol{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^{n} (\\mathbf{x}_i - \\boldsymbol{\\mu})(\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} $$\n        分母 $n-1$ 代表贝塞尔校正（Bessel's correction），它能产生对真实总体协方差的无偏估计。\n\n2.  **马氏距离计算**：马氏距离提供了一个度量，用于衡量点 $\\mathbf{x}_i$ 离数据质量中心的距离，同时考虑了数据集的协方差。平方马氏距离由以下公式给出：\n    $$ \\delta_i^2 = (\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}) $$\n    当协方差矩阵 $\\boldsymbol{\\Sigma}$ 是奇异的（不可逆）或病态的时，会出现一个关键问题。如果观测值的数量小于或等于特征的数量（$n \\le p$），这种情况就一定会发生，因为 $\\boldsymbol{\\Sigma}$ 的秩最多为 $n-1$。为了处理这个问题，标准的矩阵逆 $\\boldsymbol{\\Sigma}^{-1}$ 被**Moore-Penrose 伪逆**（记作 $\\boldsymbol{\\Sigma}^{+}$）所取代。伪逆为任何矩阵提供了一个稳定且唯一的广义逆。因此，稳健的平方马氏距离公式为：\n    $$ \\delta_i^2 = (\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{+} (\\mathbf{x}_i - \\boldsymbol{\\mu}) $$\n\n3.  **通过卡方分布进行离群点阈值设定**：最后一步是确定哪些距离足够大，可以被认为是异常的。在数据服从多元正态分布的假设下，平方马氏距离统计量 $\\delta^2$ 服从自由度为 $p$ 的卡方（$\\chi^2$）分布，其中 $p$ 是数据的维度。这一统计特性使我们能够为离群点检测建立一个正式的假设检验。\n    -   给定一个显著性水平 $\\alpha$，它代表将一个典型观测值错误分类为离群点的概率，我们定义一个临界值或阈值 $\\tau$。\n    -   这个阈值来自于自由度为 $p$（$df=p$）的卡方分布的反累积分布函数（CDF），或称分位数函数。具体来说，我们选择上 $(1-\\alpha)$ 分位数：\n        $$ \\tau = F^{-1}_{\\chi^2_p}(1-\\alpha) $$\n    -   如果一个观测值 $\\mathbf{x}_i$ 的平方马氏距离超过此阈值，则将其标记为离群点：\n        $$ \\delta_i^2 > \\tau $$\n\n**算法流程**\n\n对于每个由数据矩阵 $X$ 和显著性水平 $\\alpha$ 组成的测试用例，执行以下计算步骤：\n1.  从 $X$ 的维度确定观测值数量 $n$ 和特征数量 $p$。\n2.  计算形状为 $(p,)$ 的均值向量 $\\boldsymbol{\\mu}$。\n3.  计算形状为 $(p,p)$ 的无偏样本协方差矩阵 $\\boldsymbol{\\Sigma}$。\n4.  计算协方差矩阵的 Moore-Penrose 伪逆 $\\boldsymbol{\\Sigma}^{+}$。\n5.  通过从每个观测值中减去均值向量来中心化数据，创建一个偏差矩阵 $X - \\boldsymbol{\\mu}$。\n6.  为每个观测值 $i=0, \\dots, n-1$ 计算平方马氏距离 $\\delta_i^2$。\n7.  使用自由度为 $p$ 的 $\\chi^2$ 分布在概率为 $1-\\alpha$ 处的分位数函数计算阈值 $\\tau$。\n8.  识别所有满足 $\\delta_i^2 > \\tau$ 的从零开始的索引 $i$。\n9.  将这些索引作为排序后的列表返回。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\nfrom scipy.linalg import pinv\n\ndef solve():\n    \"\"\"\n    Solves the multivariate outlier detection problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [0.2, -0.1, 0.3, 0.0],\n            [-0.3, 0.4, -0.2, 0.5],\n            [0.1, 0.2, 0.0, -0.1],\n            [-0.2, -0.3, 0.1, 0.2],\n            [0.0, 0.1, -0.1, 0.1],\n            [0.3, -0.2, 0.2, -0.2],\n            [6.0, 6.5, 5.8, 6.2],\n            [4.0, -4.5, 3.8, -4.2]\n        ]), 0.01),\n        (np.array([\n            [-1.0, -2.0, 1.0],\n            [0.0, 0.1, 0.0],\n            [1.0, 2.1, -1.1],\n            [1.1, 2.2, -1.05],\n            [5.0, 10.2, -5.1]\n        ]), 0.05),\n        (np.array([\n            [0.0, 0.0, 0.0],\n            [0.2, -0.1, 0.1],\n            [3.5, 3.5, 3.5]\n        ]), 0.05),\n        (np.array([\n            [0.1, -0.2],\n            [-0.2, 0.3],\n            [0.2, -0.1],\n            [0.0, 0.0],\n            [0.3, 0.2],\n            [-0.1, -0.3]\n        ]), 0.001)\n    ]\n\n    all_results = []\n    for X, alpha in test_cases:\n        # 1. Get dimensions n (observations) and p (features).\n        n, p = X.shape\n\n        # 2. Compute the sample mean vector.\n        mu = np.mean(X, axis=0)\n\n        # 3. Compute the unbiased sample covariance matrix.\n        # np.cov with rowvar=False treats columns as variables.\n        # The default ddof=1 computes the unbiased sample covariance (divides by n-1).\n        # Handle the case where n = p, np.cov may fail if n=1. \n        # For n=1, covariance is undefined. Problem constraints ensure n1.\n        if n > 1:\n            cov = np.cov(X, rowvar=False)\n        else: # Covariance is ill-defined for a single point\n            cov = np.zeros((p, p))\n\n        # 4. Compute the Moore-Penrose pseudoinverse.\n        # This handles singular and ill-conditioned matrices, which is essential\n        # when n = p, as the covariance matrix will be singular.\n        cov_inv = pinv(cov)\n        \n        # 5. Center the data.\n        X_centered = X - mu\n\n        # 6. Calculate squared Mahalanobis distances.\n        # A numerically efficient way is to compute (X_c @ C_inv) * X_c and sum rows.\n        # This is equivalent to diag(X_c @ C_inv @ X_c.T) but avoids computing the full n x n matrix.\n        mahal_sq_distances = np.sum((X_centered @ cov_inv) * X_centered, axis=1)\n\n        # 7. Compute the chi-square threshold.\n        # The degrees of freedom (df) is the number of features, p.\n        # We need the (1 - alpha) quantile of the chi-square distribution.\n        threshold = chi2.ppf(1 - alpha, df=p)\n\n        # 8. Find indices where distance exceeds the threshold.\n        # np.where returns a tuple of arrays; we take the first element.\n        outlier_indices = np.where(mahal_sq_distances > threshold)[0]\n\n        # 9. Store the sorted list of indices.\n        all_results.append(outlier_indices.tolist())\n\n    # Final print statement in the exact required format.\n    # str() creates a string representation '[[], [1, 2]]'\n    # .replace(\" \", \"\") removes all spaces to match the format '[[],[1,2]]'\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "经典统计估计量（如样本均值和协方差）本身对异常值非常敏感，这可能导致“掩蔽效应”，即异常值会“伪装”自己，从而逃避检测。本练习旨在通过比较经典马氏距离与基于最小协方差行列式（MCD）的稳健马氏距离，来揭示这一问题。通过亲手实现并对比这两种方法 ，您将体会到稳健统计在构建可靠异常检测系统中的关键作用。",
            "id": "4183406",
            "problem": "一位研究人员正在分析多通道局部场电位 (LFP) 特征向量，以检测表现为离群点的伪迹和生理异常。每个观测值是一个 $p$ 维向量，代表从 LFP 信号中提取的特征。该研究人员希望比较使用经典协方差估计量与基于最小协方差行列式 (MCD) 的稳健策略在离群点检测上的差异，并为两者计算马氏距离。目标是量化两种策略标记出的离群点数量上的差异。\n\n从以下基本定义和事实出发：\n- 点 $\\mathbf{x} \\in \\mathbb{R}^p$ 与以均值 $\\boldsymbol{\\mu}$ 和正定协方差矩阵 $\\boldsymbol{\\Sigma}$ 为特征的分布之间的马氏距离定义为 $d(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\sqrt{(\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})}$。\n- 当数据能被维度为 $p$ 的多元正态分布很好地建模时，马氏距离的平方近似服从自由度为 $p$ 的卡方分布，因此在理想模型下有 $d(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})^2 \\sim \\chi^2_p$。\n- 最小协方差行列式 (MCD) 估计量是一种稳健的位置和散布估计量，它旨在寻找一个大小为 $h$（其中 $h \\in \\{p+1, \\dots, n\\}$）的子集，该子集的协方差行列式最小。稳健位置是该子集的均值，稳健散布是其协方差。\n\n为每个测试数据集实现以下任务：\n1. 通过从具有给定均值 $\\boldsymbol{\\mu}$ 和协方差 $\\boldsymbol{\\Sigma}$ 的多元正态分布中抽样内点，并通过向均值 $\\boldsymbol{\\mu}$ 添加一个确定性偏移向量 $\\boldsymbol{\\delta}$ 和小的独立高斯噪声来形成指定数量的离群点，并将其附加到数据集中，从而构建数据集。总样本数为 $n$。为保证可复现性，请使用指定的随机种子。\n2. 使用全样本均值和全样本协方差计算经典的马氏距离。\n3. 使用通过精确组合搜索获得的 MCD 估计量计算稳健的马氏距离：设置 $h = \\lfloor \\alpha n \\rfloor$ 且 $\\alpha = 0.75$，确保 $h \\ge p+1$，枚举所有大小为 $h$ 的子集，并选择其样本协方差行列式最小的子集（忽略协方差为奇异或近奇异的子集）。使用该子集的均值和协方差分别作为稳健位置和稳健散布。\n4. 对于经典距离和稳健距离，如果观测值的距离 $d$ 超过阈值 $\\tau_p = \\sqrt{F^{-1}_{\\chi^2_p}(0.975)}$，则将其标记为离群点，其中 $F^{-1}_{\\chi^2_p}$ 表示自由度为 $p$ 的卡方分布在概率 $0.975$ 处的逆累积分布函数。\n5. 对于每个数据集，计算一个单一的整数指标，该指标等于稳健策略标记的离群点数量与经典策略标记的数量之差。\n\n您的程序必须在没有任何用户输入的情况下实现上述功能，并且必须生成单行输出，其中包含三个整数结果，格式为方括号括起来的逗号分隔列表。\n\n测试套件规范：\n- 情况 A（维度适中且有几个强离群点的顺利路径）：\n  - $n = 10$, $p = 3$,\n  - $\\boldsymbol{\\mu} = [\\,0.0,\\,0.0,\\,0.0\\,]$,\n  - $\\boldsymbol{\\Sigma} = \\mathrm{diag}([\\,1.0,\\,0.5,\\,0.8\\,])$,\n  - 离群点数量 $= 2$,\n  - $\\boldsymbol{\\delta} = [\\,9.0,\\,9.0,\\,9.0\\,]$,\n  - 随机种子 $= 1234$。\n- 情况 B（维度较高、特征相关且有多个离群点）：\n  - $n = 12$, $p = 4$,\n  - $\\boldsymbol{\\mu} = [\\,0.3,\\,-0.2,\\,0.0,\\,0.1\\,]$,\n  - 构建 $\\boldsymbol{\\Sigma}$ 为 $\\boldsymbol{\\Sigma} = \\mathbf{A}\\mathbf{A}^\\top$，其中\n    $\\mathbf{A} = \\begin{bmatrix}\n    1.0 0.2 -0.1 0.0 \\\\\n    0.0 0.9 0.3 -0.2 \\\\\n    0.1 0.0 1.1 0.2 \\\\\n    0.0 -0.1 0.2 0.8\n    \\end{bmatrix}$，\n  - 离群点数量 $= 3$,\n  - $\\boldsymbol{\\delta} = [\\,7.0,\\,-8.0,\\,9.0,\\,-7.0\\,]$,\n  - 随机种子 $= 42$。\n- 情况 C（边界情况的小样本量和单个极端离群点）：\n  - $n = 9$, $p = 3$,\n  - $\\boldsymbol{\\mu} = [\\,0.0,\\,0.0,\\,0.0\\,]$,\n  - 构建 $\\boldsymbol{\\Sigma}$ 为 $\\boldsymbol{\\Sigma} = \\mathbf{B}\\mathbf{B}^\\top$，其中\n    $\\mathbf{B} = \\begin{bmatrix}\n    1.0 0.4 0.0 \\\\\n    0.2 0.8 0.3 \\\\\n    0.0 0.1 0.9\n    \\end{bmatrix}$，\n  - 离群点数量 $= 1$,\n  - $\\boldsymbol{\\delta} = [\\,10.0,\\,0.0,\\,-10.0\\,]$,\n  - 随机种子 $= 2021$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，格式为方括号括起来的逗号分隔列表（例如，$[r_A,r_B,r_C]$），其中 $r_A$、$r_B$ 和 $r_C$ 分别是情况 A、B 和 C 的整数差值（稳健方法标记数减去经典方法标记数）。不应打印任何其他文本。",
            "solution": "该问题要求比较一种经典统计方法和一种稳健统计方法在多元数据离群点检测方面的表现。任务的核心是实现这两种方法，将它们应用于综合生成的数据集，并量化其性能差异。该分析依赖于马氏距离，这是多元统计中的一个基本工具。\n\n### 原理 1：马氏距离与离群点检测\n\n离群点是样本中与其他观测值显著偏离的观测值。对于 $p$ 维数据，必须在 $p$ 维空间中评估这种偏离。马氏距离为此评估提供了坚实的基础。给定一个假设从均值为 $\\boldsymbol{\\mu} \\in \\mathbb{R}^p$、正定协方差矩阵为 $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$ 的分布中抽样的数据集，一个点 $\\mathbf{x} \\in \\mathbb{R}^p$ 到分布中心的马氏距离定义为：\n$$\nd(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\sqrt{(\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})}\n$$\n此距离优于欧几里得距离，因为它是尺度不变的，并且考虑了数据的相关性结构。它有效地以沿数据主成分轴的标准差为单位，度量了一个点到均值的距离。\n\n在数据服从多元正态分布 $N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ 的原假设下，马氏距离的平方 $d^2$ 已知服从自由度为 $p$ 的卡方分布，即 $d^2 \\sim \\chi^2_p$。这一统计特性使我们能够为离群点检测建立一个正式的阈值。如果一个观测值的距离超过了从 $\\chi^2_p$ 分布导出的临界值，它就会被标记为离群点。问题指定了一个对应于概率 $0.975$ 的阈值，因此阈值距离为 $\\tau_p = \\sqrt{F^{-1}_{\\chi^2_p}(0.975)}$，其中 $F^{-1}_{\\chi^2_p}$ 是 $\\chi^2_p$ 分布的分位数函数（逆累积分布函数）。\n\n### 原理 2：经典方法及其局限性\n\n在实践中，真实的总体参数 $\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\Sigma}$ 是未知的，必须从数据样本 $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$ 中进行估计。经典方法使用样本均值 $\\hat{\\boldsymbol{\\mu}}$ 和样本协方差矩阵 $\\hat{\\boldsymbol{\\Sigma}}$ 作为估计量：\n$$\n\\hat{\\boldsymbol{\\mu}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i \\quad \\text{和} \\quad \\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{n-1} \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})^\\top\n$$\n然后，经典的马氏距离计算为 $d(\\mathbf{x}_i; \\hat{\\boldsymbol{\\mu}}, \\hat{\\boldsymbol{\\Sigma}})$。\n\n这种方法的关键缺陷在于，估计量 $\\hat{\\boldsymbol{\\mu}}$ 和 $\\hat{\\boldsymbol{\\Sigma}}$ 本身对离群点高度敏感。单个极端离群点会将其方向上的样本均值拉向自身，并夸大样本协方差。这种现象被称为**掩蔽效应**：离群点的存在污染了位置和散布的估计，这反过来又可能导致离群点的马氏距离被人为地缩小，从而可能妨碍其被检测到。\n\n### 原理 3：使用最小协方差行列式 (MCD) 的稳健方法\n\n为了克服掩蔽效应，采用了稳健的位置和散布估计量。最小协方差行列式 (MCD) 估计量是一种著名的稳健方法。MCD 的目标是从总共 $n$ 个观测值中找到一个包含 $h$ 个观测值（其中 $p+1 \\le h \\le n$）的子集，该子集的经典协方差矩阵具有最小的可能行列式。其直觉在于，协方差矩阵的行列式与包含数据的椭球体体积的平方成正比。找到具有最小行列式的子集等同于找到最集中的由 $h$ 个点组成的“核心”组。这个核心子集被假定为不含离群点。\n\n根据问题规范，位置的稳健估计 $\\boldsymbol{\\mu}_{\\text{MCD}}$ 和散布的稳健估计 $\\boldsymbol{\\Sigma}_{\\text{MCD}}$ 就是这个最优 $h$ 点子集的样本均值和样本协方差。然后，使用这些稳健估计为所有 $n$ 个点计算稳健马氏距离：$d(\\mathbf{x}_i; \\boldsymbol{\\mu}_{\\text{MCD}}, \\boldsymbol{\\Sigma}_{\\text{MCD}})$。因为 $\\boldsymbol{\\mu}_{\\text{MCD}}$ 和 $\\boldsymbol{\\Sigma}_{\\text{MCD}}$ 不受离群点的影响，所以真实离群点的马氏距离会很大，而内点的马氏距离会很小，从而实现更可靠的分离。\n\n### 算法实现\n\n该解决方案通过为每个测试用例执行以下步骤来实现：\n\n1.  **数据生成**：构建包含 $n$ 个点的数据集 $\\mathbf{X}$。首先，从指定的多元正态分布 $N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ 中抽样 $n_{inliers} = n - n_{outliers}$ 个内点。其次，根据问题中“小的独立高斯噪声”条款，通过从一个以显著偏移的均值为中心的分布 $N(\\boldsymbol{\\mu} + \\boldsymbol{\\delta}, \\sigma^2 \\mathbf{I})$ 中抽样来生成 $n_{outliers}$ 个离群点，其中 $\\sigma^2$ 是一个很小的方差（例如，$0.01$）以引入微小变化。为保证可复现性，使用特定的随机种子。\n\n2.  **经典分析**：从完整数据集 $\\mathbf{X}$ 计算经典样本均值 $\\hat{\\boldsymbol{\\mu}}$ 和协方差 $\\hat{\\boldsymbol{\\Sigma}}$。计算协方差矩阵的逆 $\\hat{\\boldsymbol{\\Sigma}}^{-1}$，并为 $n$ 个点中的每一个计算经典马氏距离 $d_{classical}$。\n\n3.  **稳健 MCD 分析**：参数 $h$ 设置为 $\\lfloor \\alpha n \\rfloor$，其中 $\\alpha=0.75$。对所有大小为 $h$ 的 $\\binom{n}{h}$ 个索引子集执行精确组合搜索。对于每个子集：\n    *   计算其样本协方差矩阵。\n    *   计算该协方差矩阵的行列式。行列式接近于零的子集因其奇异或病态而被丢弃。\n    *   确定产生最小行列式的子集。将此最优子集的均值和协方差存储为 $\\boldsymbol{\\mu}_{\\text{MCD}}$ 和 $\\boldsymbol{\\Sigma}_{\\text{MCD}}$。\n\n    然后使用稳健协方差矩阵的逆 $\\boldsymbol{\\Sigma}_{\\text{MCD}}^{-1}$ 来计算原始数据集 $\\mathbf{X}$ 中所有 $n$ 个点的稳健马氏距离 $d_{robust}$。\n\n4.  **离群点标记与比较**：使用卡方分布的逆累积分布函数计算检测阈值 $\\tau_p = \\sqrt{F^{-1}_{\\chi^2_p}(0.975)}$。通过计算马氏距离大于 $\\tau_p$ 的点数来确定每种方法标记的离群点数量。\n    *   $N_{classical} = |\\{ i \\mid d_{classical}(\\mathbf{x}_i) > \\tau_p \\}|$\n    *   $N_{robust} = |\\{ i \\mid d_{robust}(\\mathbf{x}_i) > \\tau_p \\}|$\n\n5.  **指标计算**：测试用例的最终结果是整数差 $N_{robust} - N_{classical}$。该指标量化了与经典方法相比，稳健方法多识别（或少识别）了多少个离群点。正值表示稳健方法更敏感，这很可能是因为它成功克服了阻碍经典方法的掩蔽效应。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves the outlier detection problem for three specified test cases.\n    \"\"\"\n\n    # Case A: Happy path with moderate dimension and a couple of strong outliers\n    case_a = {\n        \"n\": 10, \"p\": 3,\n        \"mu\": [0.0, 0.0, 0.0],\n        \"Sigma\": np.diag([1.0, 0.5, 0.8]),\n        \"n_out\": 2,\n        \"delta\": [9.0, 9.0, 9.0],\n        \"seed\": 1234\n    }\n\n    # Case B: Higher dimension with correlated features and several outliers\n    A_b = np.array([\n        [1.0, 0.2, -0.1, 0.0],\n        [0.0, 0.9, 0.3, -0.2],\n        [0.1, 0.0, 1.1, 0.2],\n        [0.0, -0.1, 0.2, 0.8]\n    ])\n    Sigma_b = A_b @ A_b.T\n    case_b = {\n        \"n\": 12, \"p\": 4,\n        \"mu\": [0.3, -0.2, 0.0, 0.1],\n        \"Sigma\": Sigma_b,\n        \"n_out\": 3,\n        \"delta\": [7.0, -8.0, 9.0, -7.0],\n        \"seed\": 42\n    }\n\n    # Case C: Boundary-like small sample size and a single extreme outlier\n    B_c = np.array([\n        [1.0, 0.4, 0.0],\n        [0.2, 0.8, 0.3],\n        [0.0, 0.1, 0.9]\n    ])\n    Sigma_c = B_c @ B_c.T\n    case_c = {\n        \"n\": 9, \"p\": 3,\n        \"mu\": [0.0, 0.0, 0.0],\n        \"Sigma\": Sigma_c,\n        \"n_out\": 1,\n        \"delta\": [10.0, 0.0, -10.0],\n        \"seed\": 2021\n    }\n\n    test_cases = [case_a, case_b, case_c]\n    results = []\n\n    for case in test_cases:\n        n, p = case[\"n\"], case[\"p\"]\n        mu, Sigma = np.array(case[\"mu\"]), np.array(case[\"Sigma\"])\n        n_out, delta, seed = case[\"n_out\"], np.array(case[\"delta\"]), case[\"seed\"]\n\n        np.random.seed(seed)\n\n        # 1. Construct the dataset\n        n_in = n - n_out\n        inliers = np.random.multivariate_normal(mu, Sigma, n_in)\n        \n        # Outlier generation from N(mu + delta, 0.01 * I)\n        outlier_mean = mu + delta\n        outlier_cov = np.eye(p) * 0.01\n        outliers = np.random.multivariate_normal(outlier_mean, outlier_cov, n_out)\n        \n        X = np.vstack((inliers, outliers))\n\n        # 4. Outlier flagging threshold\n        threshold_sq = chi2.ppf(0.975, df=p)\n        threshold = np.sqrt(threshold_sq)\n\n        # 2. Compute classical Mahalanobis distances\n        mu_classical = np.mean(X, axis=0)\n        Sigma_classical = np.cov(X, rowvar=False)\n        inv_Sigma_classical = np.linalg.inv(Sigma_classical)\n        \n        diffs_classical = X - mu_classical\n        d_classical_sq = np.sum(diffs_classical @ inv_Sigma_classical * diffs_classical, axis=1)\n        d_classical = np.sqrt(d_classical_sq)\n        \n        n_outliers_classical = np.sum(d_classical > threshold)\n\n        # 3. Compute robust Mahalanobis distances (MCD)\n        h = int(0.75 * n)\n        \n        min_det = np.inf\n        mu_mcd, Sigma_mcd = None, None\n        \n        indices = range(n)\n        for subset_indices in combinations(indices, h):\n            subset = X[list(subset_indices), :]\n            current_cov = np.cov(subset, rowvar=False)\n            \n            # Using slogdet to handle potential underflow/overflow of small/large determinants\n            sign, logdet = np.linalg.slogdet(current_cov)\n\n            # Check for positive definite (sign=1) and near-singularity\n            if sign == 1:\n                # In log space, smaller logdet means smaller determinant\n                if logdet  min_det:\n                    min_det = logdet\n                    mu_mcd = np.mean(subset, axis=0)\n                    Sigma_mcd = current_cov\n\n        if Sigma_mcd is None:\n            # Fallback in case no valid subset is found (highly unlikely for this problem)\n            raise ValueError(\"Could not find a non-singular subset for MCD.\")\n            \n        inv_Sigma_mcd = np.linalg.inv(Sigma_mcd)\n        diffs_robust = X - mu_mcd\n        d_robust_sq = np.sum(diffs_robust @ inv_Sigma_mcd * diffs_robust, axis=1)\n        d_robust = np.sqrt(d_robust_sq)\n        \n        n_outliers_robust = np.sum(d_robust > threshold)\n\n        # 5. Compute the final metric\n        diff = n_outliers_robust - n_outliers_classical\n        results.append(diff)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}