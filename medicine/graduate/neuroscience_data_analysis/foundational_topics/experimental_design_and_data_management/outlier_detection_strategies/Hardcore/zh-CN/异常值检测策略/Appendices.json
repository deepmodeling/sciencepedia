{
    "hands_on_practices": [
        {
            "introduction": "$Z$-score 是一种广泛应用的离群点检测方法，但其有效性依赖于数据呈高斯分布的严格假设。在神经科学等领域，许多数据集（如钙成像信号）本质上是倾斜的，直接应用 $Z$-score 会导致对错误发现率的严重误判。本练习  旨在通过量化在倾斜分布下 $Z$-score 的实际表现与理论预期的偏差，来揭示这一基本方法的局限性。",
            "id": "4183422",
            "problem": "您的任务是设计并实现一个程序，用于评估在钙成像荧光轨迹的背景下，使用 z-score 的经典离群点检测方法的可靠性。钙成像信号是非负的，并且由于稀疏的向上瞬变，常表现出不对称的正偏态分布。经典的 z-score 离群点检测器基于均值和标准差定义了一个单侧高阈值，并假设基线为高斯分布。当底层信号分布是偏态时，这一假设可能会错误估计假阳性率。\n\n您的程序必须将以下纯粹以数学和逻辑术语表述的步骤操作化，不使用任何特定领域的文件格式或外部数据。所有量都应视为无量纲。\n\n1. 形式化经典的单侧 z-score 阈值。设标量随机变量 $X$ 的总体均值为 $ \\mu $，总体标准差为 $ \\sigma $。对于一个选定的非负常数 $ c $，将经典高离群点阈值定义为值 $ \\tau $，使得满足 $ X > \\tau $ 的值被标记为离群点，其中 $ \\tau = \\mu + c \\sigma $。\n\n2. 定义高斯假设下的名义假阳性率。如果标准化变量 $ Z = (X - \\mu)/\\sigma $ 是标准正态分布，则对应于阈值 $ \\tau $ 的名义单侧假阳性率将是 $ \\mathbb{P}(Z > c) $。此量将用作名义基准。\n\n3. 量化指定偏态分布下的实际假阳性率。对于产生正偏态的指定分布族和参数，计算实际的单侧假阳性率 $ \\mathbb{P}(X > \\tau) $，其中 $ \\tau $ 是根据该分布的真实 $ \\mu $ 和 $ \\sigma $ 计算的。使用总体级别的参数；不要使用有限样本来近似 $ \\mu $ 或 $ \\sigma $。\n\n4. 量化由偏度引起的误差。计算名义基准与实际假阳性率之间的绝对偏差，定义为 $ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right| $。\n\n5. 计算给定轨迹长度的预期误报数。对于一个正整数轨迹长度 $ T $（独立样本数），计算预期误报数为 $ T \\times \\mathbb{P}(X > \\tau) $。\n\n您的程序必须为构成测试套件的以下五个测试用例中的每一个实施上述步骤。在每种情况下，随机变量 $ X $ 都遵循一种典型的非负荧光强度的正偏态分布。对于对数正态分布和伽马分布，参数解释如下：对于对数正态分布，$ \\mu_{\\log} $ 是 $ \\log X $ 的均值，$ \\sigma_{\\log} $ 是 $ \\log X $ 的标准差；对于伽马分布，$ \\kappa $ 是形状参数，$ \\theta $ 是尺度参数。使用每种分布的总体矩来计算 $ \\mu $ 和 $ \\sigma $。测试套件如下：\n\n- 案例 $1$：对数正态分布，参数为 $ \\mu_{\\log} = 0 $，$ \\sigma_{\\log} = 0.5 $，$ c = 3 $，$ T = 10000 $。\n- 案例 $2$：对数正态分布，参数为 $ \\mu_{\\log} = 0 $，$ \\sigma_{\\log} = 0.5 $，$ c = 0 $，$ T = 10000 $。\n- 案例 $3$：伽马分布，参数为 $ \\kappa = 20 $，$ \\theta = 0.05 $，$ c = 3 $，$ T = 100000 $。\n- 案例 $4$：伽马分布，参数为 $ \\kappa = 1 $，$ \\theta = 1 $，$ c = 3 $，$ T = 10000 $。\n- 案例 $5$：对数正态分布，参数为 $ \\mu_{\\log} = 0 $，$ \\sigma_{\\log} = 1.0 $，$ c = 3 $，$ T = 10000 $。\n\n对于每个案例，您的程序必须计算：\n- 经典的单侧高阈值 $ \\tau $。\n- 高斯假设下的名义单侧假阳性率 $ \\mathbb{P}(Z > c) $。\n- 指定偏态分布下的实际单侧假阳性率 $ \\mathbb{P}(X > \\tau) $。\n- 绝对误差 $ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right| $。\n- 预期误报数 $ T \\times \\mathbb{P}(X > \\tau) $。\n\n最终输出格式：您的程序应生成一行输出，其中包含所有五个测试用例的汇总结果，格式为一个逗号分隔的列表，并用方括号括起来。每个测试用例由一个内部列表表示，其中包含五个按顺序 $ [\\tau, \\mathbb{P}(Z > c), \\mathbb{P}(X > \\tau), \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right|, T \\times \\mathbb{P}(X > \\tau)] $ 计算出的浮点数。例如，两个假设案例的有效输出将类似于 $ [[a_1,b_1,c_1,d_1,e_1],[a_2,b_2,c_2,d_2,e_2]] $，其中每个符号表示一个计算出的浮点数。所有概率和期望值必须表示为小数，而不是百分比。",
            "solution": "目标是衡量假定高斯性的经典 z-score 阈值法，在真实信号分布为正偏态时会产生多大的偏差。我们从标准化、累积分布函数和总体矩的核心定义出发构建推导。\n\n1. 从经典 z-score 构建阈值。设 $ X $ 是一个实值随机变量，其总体均值为 $ \\mu $，总体标准差为 $ \\sigma $。观测值 $ x $ 的经典 z-score 定义为 $ z = (x - \\mu)/\\sigma $。对于一个选定的 $ c \\ge 0 $，经典的单侧高离群点阈值为 $ \\tau = \\mu + c \\sigma $，因为如果 $ z > c $，即 $ x > \\mu + c \\sigma $，则 $ x $ 被标记为离群点。\n\n2. 高斯假设下的名义假阳性率。如果标准化变量 $ Z = (X - \\mu)/\\sigma $ 服从标准正态分布，即 $ Z \\sim \\mathcal{N}(0,1) $，那么在阈值 $ c $ 下错误地将基线观测值标记为离群点的概率是标准正态生存概率 $ \\mathbb{P}(Z > c) $。使用 $ \\mathcal{N}(0,1) $ 的累积分布函数 $ \\Phi $，这个概率是 $ 1 - \\Phi(c) $。这个名义基准仅取决于 $ c $。\n\n3. 真实偏态分布下的实际假阳性率。实际的基线分布可能是正偏态的。我们考虑两个经过充分检验的、用于模拟钙成像中常见的非负偏态的分布族：\n\n   - 对数正态分布：如果 $ Y = \\log X $ 是均值为 $ \\mu_{\\log} $、标准差为 $ \\sigma_{\\log} $ 的正态分布，则 $ X $ 是对数正态分布。其总体均值和方差可由正态变量的指数性质导出。具体而言，\n     $$ \\mu = \\mathbb{E}[X] = \\exp\\left(\\mu_{\\log} + \\frac{\\sigma_{\\log}^{2}}{2}\\right), $$\n     $$ \\operatorname{Var}(X) = \\left(\\exp(\\sigma_{\\log}^{2}) - 1\\right) \\exp\\left(2 \\mu_{\\log} + \\sigma_{\\log}^{2}\\right), $$\n     因此总体标准差为 $ \\sigma = \\sqrt{\\operatorname{Var}(X)} $。生存函数 $ \\mathbb{P}(X > x) $ 由对数正态生存函数在 $ x $ 处（参数为 $ \\mu_{\\log} $ 和 $ \\sigma_{\\log} $）求值得到。对于阈值 $ \\tau = \\mu + c \\sigma $，实际假阳性率是来自对数正态生存函数的 $ \\mathbb{P}(X > \\tau) $。\n\n   - 伽马分布：如果 $ X $ 服从形状参数为 $ \\kappa $、尺度参数为 $ \\theta $ 的伽马分布，其总体均值和方差为\n     $$ \\mu = \\kappa \\theta, \\quad \\operatorname{Var}(X) = \\kappa \\theta^{2}, $$\n     得到 $ \\sigma = \\sqrt{\\kappa} \\theta $。生存函数 $ \\mathbb{P}(X > x) $ 由伽马生存函数在 $ x $ 处（参数为 $ \\kappa $ 和 $ \\theta $）求值得到。对于 $ \\tau = \\mu + c \\sigma $，实际假阳性率是来自伽马生存函数的 $ \\mathbb{P}(X > \\tau) $。\n\n   对于这两个分布族，我们依赖于经过充分检验的总体矩公式和概率论中的标准生存函数，确保进行总体级别的计算，避免有限样本的变异性。\n\n4. 由偏度引起的误差量化。经典高斯基准与实际偏态分布之间的绝对误差为\n   $$ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right|. $$\n   这衡量了将高斯阈值应用于偏态数据时出现的校准误差。对于单侧高阈值，钙成像的正向瞬变使得这一点尤为重要。\n\n5. 给定轨迹长度的预期误报数。假设样本是独立的，长度为 $ T $ 的轨迹中基线样本超过阈值的预期数量为\n   $$ T \\times \\mathbb{P}(X > \\tau). $$\n   这将概率转换为预期计数，为分析流程提供了一种切实的性能衡量标准。\n\n6. 测试套件覆盖范围。我们选择五个案例来探讨不同方面：\n   - 案例 $1$（正常情况）：具有中等偏度（$ \\mu_{\\log} = 0 $，$ \\sigma_{\\log} = 0.5 $）的对数正态分布和 $ c = 3 $，以评估名义和实际假阳性率之间的实际差异。\n   - 案例 $2$（边界条件）：相同的对数正态分布，但 $ c = 0 $，得到 $ \\tau = \\mu $；名义概率为 $ \\mathbb{P}(Z > 0) = 0.5 $，而实际概率 $ \\mathbbP(X > \\mu) $ 反映了这样一个事实：对于正偏态分布，均值超过中位数，因此高于均值的概率通常小于 $ 0.5 $。\n   - 案例 $3$（接近正态）：伽马分布，参数为 $ \\kappa = 20 $ 和 $ \\theta = 0.05 $，其均值为 $ \\mu = 1 $，方差为 $ \\operatorname{Var}(X) = 0.05 $，导致相对较小的偏度；当 $ c = 3 $ 时，高斯名义值和实际值应该很接近。\n   - 案例 $4$（极端偏态）：伽马分布，参数为 $ \\kappa = 1 $ 和 $ \\theta = 1 $，类似指数分布，产生显著的偏度；$ c = 3 $ 的阈值在名义值和实际值之间将有显著差异。\n   - 案例 $5$（对数正态分布的重度偏态）：对数正态分布，参数为 $ \\mu_{\\log} = 0 $ 和 $ \\sigma_{\\log} = 1.0 $，显著增加了偏度和重尾性，用于在 $ c = 3 $ 时测试高斯假设的鲁棒性。\n\n7. 输出规范。对于每个案例，计算 $ \\tau $、$ \\mathbb{P}(Z > c) $、$ \\mathbb{P}(X > \\tau) $、$ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right| $ 和 $ T \\times \\mathbb{P}(X > \\tau) $。将所有五个案例汇总到一个列表的列表中，并以所要求的格式单行打印。所有概率和期望值必须是小数，不带百分号。\n\n该程序使用标准正态生存函数计算 $ \\mathbb{P}(Z > c) $，使用对数正态和伽马生存函数计算 $ \\mathbb{P}(X > \\tau) $，确保对指定参数进行准确的总体级别计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, lognorm, gamma\n\ndef compute_lognormal_moments(mu_log: float, sigma_log: float):\n    \"\"\"\n    Compute population mean and std for a log-normal with log-mean mu_log\n    and log-std sigma_log.\n    \"\"\"\n    # Mean: exp(mu_log + 0.5 * sigma_log^2)\n    mean = np.exp(mu_log + 0.5 * (sigma_log ** 2))\n    # Variance: (exp(sigma_log^2) - 1) * exp(2*mu_log + sigma_log^2)\n    var = (np.exp(sigma_log ** 2) - 1.0) * np.exp(2.0 * mu_log + (sigma_log ** 2))\n    std = np.sqrt(var)\n    return mean, std\n\ndef compute_gamma_moments(kappa: float, theta: float):\n    \"\"\"\n    Compute population mean and std for a gamma with shape kappa and scale theta.\n    \"\"\"\n    mean = kappa * theta\n    var = kappa * (theta ** 2)\n    std = np.sqrt(var)\n    return mean, std\n\ndef nominal_fp_rate(c: float) -> float:\n    \"\"\"\n    Nominal one-sided false positive rate under N(0,1): P(Z > c).\n    \"\"\"\n    return float(norm.sf(c))\n\ndef actual_fp_rate_lognorm(tau: float, mu_log: float, sigma_log: float) -> float:\n    \"\"\"\n    Actual one-sided false positive rate for log-normal: P(X > tau).\n    SciPy's lognorm parameterization uses shape=sigma_log, scale=exp(mu_log).\n    \"\"\"\n    s = sigma_log\n    scale = np.exp(mu_log)\n    return float(lognorm.sf(tau, s=s, scale=scale))\n\ndef actual_fp_rate_gamma(tau: float, kappa: float, theta: float) -> float:\n    \"\"\"\n    Actual one-sided false positive rate for gamma: P(X > tau).\n    SciPy's gamma parameterization uses a=shape (kappa), scale=theta.\n    \"\"\"\n    return float(gamma.sf(tau, a=kappa, scale=theta))\n\ndef format_nested_list_no_spaces(nested):\n    \"\"\"\n    Format a nested list into a compact string representation with no spaces.\n    \"\"\"\n    def format_item(x):\n        if isinstance(x, list):\n            return \"[\" + \",\".join(format_item(y) for y in x) + \"]\"\n        else:\n            # Ensure standard float/integer string without spaces\n            return str(x)\n    return format_item(nested)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: dict with keys 'dist', parameters, 'c', 'T'.\n    test_cases = [\n        # Case 1: Log-normal moderate skew\n        {\"dist\": \"lognorm\", \"mu_log\": 0.0, \"sigma_log\": 0.5, \"c\": 3.0, \"T\": 10000},\n        # Case 2: Log-normal boundary c=0\n        {\"dist\": \"lognorm\", \"mu_log\": 0.0, \"sigma_log\": 0.5, \"c\": 0.0, \"T\": 10000},\n        # Case 3: Gamma near-normal (large shape)\n        {\"dist\": \"gamma\", \"kappa\": 20.0, \"theta\": 0.05, \"c\": 3.0, \"T\": 100000},\n        # Case 4: Gamma extreme skew (exponential-like)\n        {\"dist\": \"gamma\", \"kappa\": 1.0, \"theta\": 1.0, \"c\": 3.0, \"T\": 10000},\n        # Case 5: Log-normal heavy skew\n        {\"dist\": \"lognorm\", \"mu_log\": 0.0, \"sigma_log\": 1.0, \"c\": 3.0, \"T\": 10000},\n    ]\n\n    results = []\n    for case in test_cases:\n        c = float(case[\"c\"])\n        T = int(case[\"T\"])\n\n        if case[\"dist\"] == \"lognorm\":\n            mu_log = float(case[\"mu_log\"])\n            sigma_log = float(case[\"sigma_log\"])\n            # Population moments\n            mu, sigma = compute_lognormal_moments(mu_log, sigma_log)\n            tau = mu + c * sigma\n            p_nom = nominal_fp_rate(c)\n            p_act = actual_fp_rate_lognorm(tau, mu_log, sigma_log)\n        elif case[\"dist\"] == \"gamma\":\n            kappa = float(case[\"kappa\"])\n            theta = float(case[\"theta\"])\n            # Population moments\n            mu, sigma = compute_gamma_moments(kappa, theta)\n            tau = mu + c * sigma\n            p_nom = nominal_fp_rate(c)\n            p_act = actual_fp_rate_gamma(tau, kappa, theta)\n        else:\n            # Should not happen in the defined test suite.\n            raise ValueError(\"Unsupported distribution type\")\n\n        abs_err = abs(p_nom - p_act)\n        exp_false_alarms = T * p_act\n\n        results.append([tau, p_nom, p_act, abs_err, exp_false_alarms])\n\n    # Final print statement in the exact required format: single line, no spaces.\n    print(format_nested_list_no_spaces(results))\n\nsolve()\n```"
        },
        {
            "introduction": "当数据从单变量扩展到多变量时，简单地对每个特征独立应用 $Z$-score 会忽略特征间的相关性，从而导致检测失效。马氏距离 (Mahalanobis distance) 提供了一种在多维空间中度量点到分布中心距离的有效方法，它考虑了数据的协方差结构。本练习  将指导您实现基于马氏距离和卡方分布阈值的多维离群点检测器，这是多维数据分析中的一项核心技能。",
            "id": "4183436",
            "problem": "您将收到几个数值数据集，每个数据集代表从脑电图（EEG）成分的独立成分分析（ICA）中提取的特征。对于每个数据集，将行视为观测值（成分），将列视为特征。您的任务是仅从基本统计定义出发，使用马氏距离和卡方分布实现一个规范的多元离群点检测程序。您必须编写一个程序，对于每个数据集，标记出其马氏距离平方超过基于卡方分位数阈值的观测值的索引（从零开始），然后将所有数据集中标记的索引列表汇总到一个顶层列表中。\n\n使用的基本原理和定义：\n- 设 $X \\in \\mathbb{R}^{n \\times p}$ 表示特征矩阵，有 $n$ 个观测值和 $p$ 个特征。令 $\\mathbf{x}_i \\in \\mathbb{R}^p$ 为 $X$ 的第 $i$ 行。\n- 样本均值向量为 $\\boldsymbol{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{x}_i$。\n- 无偏样本协方差矩阵为 $\\boldsymbol{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^{n} (\\mathbf{x}_i - \\boldsymbol{\\mu})(\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top}$。\n- 如果 $\\boldsymbol{\\Sigma}$ 是奇异或病态的，则使用 Moore–Penrose 伪逆，记为 $\\boldsymbol{\\Sigma}^{+}$。\n- $\\mathbf{x}_i$ 的马氏距离平方为 $\\delta_i^2 = (\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{+} (\\mathbf{x}_i - \\boldsymbol{\\mu})$。\n- 在特征近似服从联合高斯分布且能被 $\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\Sigma}$ 很好地建模的假设下，统计量 $\\delta_i^2$ 与自由度为 $p$ 的卡方分布的上分位数进行比较。对于给定的显著性水平 $\\alpha \\in (0,1)$，定义阈值 $\\tau = F^{-1}_{\\chi^2_p}(1-\\alpha)$，其中 $F^{-1}_{\\chi^2_p}$ 是自由度为 $p$ 的卡方分布的逆累积分布函数（分位数函数）。如果 $\\delta_i^2 > \\tau$，则标记观测值 $i$。\n\n您的程序必须使用以下测试套件实现上述过程（每个测试用例是一个对 $(X, \\alpha)$）。所有索引都是从零开始的，并且对于每个用例，您必须按严格递增的顺序返回标记的索引。\n\n测试用例 1（正常路径，中等维度 $p = 4$）：\n- $X \\in \\mathbb{R}^{8 \\times 4}$，其行向量为\n  - `[0.2, -0.1, 0.3, 0.0]`\n  - `[-0.3, 0.4, -0.2, 0.5]`\n  - `[0.1, 0.2, 0.0, -0.1]`\n  - `[-0.2, -0.3, 0.1, 0.2]`\n  - `[0.0, 0.1, -0.1, 0.1]`\n  - `[0.3, -0.2, 0.2, -0.2]`\n  - `[6.0, 6.5, 5.8, 6.2]`\n  - `[4.0, -4.5, 3.8, -4.2]`\n- $\\alpha = 0.01$.\n\n测试用例 2（近奇异协方差，$p = 3$）：\n- $X \\in \\mathbb{R}^{5 \\times 3}$，其行向量为\n  - `[-1.0, -2.0, 1.0]`\n  - `[0.0, 0.1, 0.0]`\n  - `[1.0, 2.1, -1.1]`\n  - `[1.1, 2.2, -1.05]`\n  - `[5.0, 10.2, -5.1]`\n- $\\alpha = 0.05$.\n\n测试用例 3（小样本 $n = p = 3$）：\n- $X \\in \\mathbb{R}^{3 \\times 3}$，其行向量为\n  - `[0.0, 0.0, 0.0]`\n  - `[0.2, -0.1, 0.1]`\n  - `[3.5, 3.5, 3.5]`\n- $\\alpha = 0.05$.\n\n测试用例 4（在严格阈值下无离群点，$p = 2$）：\n- $X \\in \\mathbb{R}^{6 \\times 2}$，其行向量为\n  - `[0.1, -0.2]`\n  - `[-0.2, 0.3]`\n  - `[0.2, -0.1]`\n  - `[0.0, 0.0]`\n  - `[0.3, 0.2]`\n  - `[-0.1, -0.3]`\n- $\\alpha = 0.001$.\n\n要求的输出：\n- 对于每个测试用例，计算集合 $\\{\\,i \\in \\{0,1,\\dots,n-1\\} \\mid \\delta_i^2 > \\tau \\,\\}$，并将其作为升序索引列表返回。\n- 将这些每个用例的列表按测试用例的顺序汇总到一个顶层列表中。\n- 您的程序应生成单行输出，包含此汇总列表，格式为方括号内以逗号分隔的列表，不含空格，例如 `[[],[0,2]]`。\n- 所有量都是纯数字。不要使用百分比符号；显著性水平必须视为小数，例如 $\\alpha = 0.05$。\n\n假设：\n- 需要时必须使用伪逆 $\\boldsymbol{\\Sigma}^{+}$。您可以使用基于奇异值分解的伪逆或数值稳定的等效方法。\n- 本问题不涉及角度和物理单位。\n\n您的代码必须是一个完整的、可运行的程序，实现上述要求并以指定格式打印最终结果。",
            "solution": "该问题要求实现一种基于马氏距离的规范多元离群点检测算法。这种统计方法特别适用于识别多维特征空间中的异常观测值，例如从脑电图（EEG）数据的独立成分分析（ICA）中派生的特征。该方法利用数据的协方差结构来定义一个对特征空间的线性变换不变的距离度量。其核心逻辑包括对数据分布进行建模，并标记出位于该分布低概率区域的点。\n\n该解决方案从第一性原理出发，步骤如下：\n\n1.  **统计参数估计**：对于由矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 表示的给定数据集，其中有 $n$ 个观测值和 $p$ 个特征，第一步是估计基础数据分布的参数。我们假设数据可以用其前两个矩来充分描述：中心和离散程度。\n    -   数据云的中心通过**样本均值向量** $\\boldsymbol{\\mu} \\in \\mathbb{R}^p$ 来估计。它被计算为观测向量的算术平均值：\n        $$ \\boldsymbol{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{x}_i $$\n        其中 $\\mathbf{x}_i$ 是 $X$ 的第 $i$ 行（观测值）。\n    -   离散程度，更重要的是，特征之间的相关结构，通过**无偏样本协方差矩阵** $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$ 来估计。其定义为：\n        $$ \\boldsymbol{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^{n} (\\mathbf{x}_i - \\boldsymbol{\\mu})(\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} $$\n        分母 $n-1$ 代表 Bessel's 校正，它能产生对真实总体协方差的无偏估计。\n\n2.  **马氏距离计算**：马氏距离提供了一个度量，衡量一个点 $\\mathbf{x}_i$ 距离数据质量中心的远近，同时考虑了数据集的协方差。马氏距离的平方由下式给出：\n    $$ \\delta_i^2 = (\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}) $$\n    当协方差矩阵 $\\boldsymbol{\\Sigma}$ 是奇异（不可逆）或病态时，会出现一个关键问题。如果观测数小于或等于特征数（$n \\le p$），这种情况是必然发生的，因为 $\\boldsymbol{\\Sigma}$ 的秩最多为 $n-1$。为了处理这个问题，标准的矩阵逆 $\\boldsymbol{\\Sigma}^{-1}$ 被**Moore-Penrose 伪逆**所取代，记为 $\\boldsymbol{\\Sigma}^{+}$。伪逆为任何矩阵提供了一个稳定且唯一的广义逆。因此，马氏距离平方的稳健公式为：\n    $$ \\delta_i^2 = (\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{+} (\\mathbf{x}_i - \\boldsymbol{\\mu}) $$\n\n3.  **通过卡方分布进行离群点阈值设定**：最后一步是确定哪些距离大到足以被认为是异常的。在数据服从多元正态分布的假设下，马氏距离平方统计量 $\\delta^2$ 服从自由度为 $p$ 的卡方（$\\chi^2$）分布，其中 $p$ 是数据的维度。这一统计特性使我们能够建立一个用于离群点检测的正式假设检验。\n    -   给定一个显著性水平 $\\alpha$，它代表将一个典型观测值错误分类为离群点的概率，我们定义一个临界值或阈值 $\\tau$。\n    -   这个阈值源自自由度为 $p$（$df=p$）的卡方分布的逆累积分布函数（CDF）或分位数函数。具体来说，我们选择上 $(1-\\alpha)$ 分位数：\n        $$ \\tau = F^{-1}_{\\chi^2_p}(1-\\alpha) $$\n    -   如果一个观测值 $\\mathbf{x}_i$ 的马氏距离平方超过此阈值，则将其标记为离群点：\n        $$ \\delta_i^2 > \\tau $$\n\n**算法流程**\n\n对于每个测试用例，包含一个数据矩阵 X 和一个显著性水平 α，执行以下计算步骤：\n1.  从 $X$ 的维度确定观测数 $n$ 和特征数 $p$。\n2.  计算形状为 $(p,)$ 的均值向量 $\\boldsymbol{\\mu}$。\n3.  计算形状为 $(p,p)$ 的无偏样本协方差矩阵 $\\boldsymbol{\\Sigma}$。\n4.  计算协方差矩阵的 Moore-Penrose 伪逆 $\\boldsymbol{\\Sigma}^{+}$。\n5.  通过从每个观测值中减去均值向量来中心化数据，创建一个偏差矩阵 $X - \\boldsymbol{\\mu}$。\n6.  为每个观测值 $i=0, \\dots, n-1$ 计算马氏距离平方 $\\delta_i^2$。\n7.  使用自由度为 $p$ 的 $\\chi^2$ 分布在概率 $1-\\alpha$ 处的的分位数函数计算阈值 $\\tau$。\n8.  识别所有满足 $\\delta_i^2 > \\tau$ 的从零开始的索引 $i$。\n9.  将这些索引作为排序后的列表返回。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\nfrom scipy.linalg import pinv\n\ndef solve():\n    \"\"\"\n    Solves the multivariate outlier detection problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [0.2, -0.1, 0.3, 0.0],\n            [-0.3, 0.4, -0.2, 0.5],\n            [0.1, 0.2, 0.0, -0.1],\n            [-0.2, -0.3, 0.1, 0.2],\n            [0.0, 0.1, -0.1, 0.1],\n            [0.3, -0.2, 0.2, -0.2],\n            [6.0, 6.5, 5.8, 6.2],\n            [4.0, -4.5, 3.8, -4.2]\n        ]), 0.01),\n        (np.array([\n            [-1.0, -2.0, 1.0],\n            [0.0, 0.1, 0.0],\n            [1.0, 2.1, -1.1],\n            [1.1, 2.2, -1.05],\n            [5.0, 10.2, -5.1]\n        ]), 0.05),\n        (np.array([\n            [0.0, 0.0, 0.0],\n            [0.2, -0.1, 0.1],\n            [3.5, 3.5, 3.5]\n        ]), 0.05),\n        (np.array([\n            [0.1, -0.2],\n            [-0.2, 0.3],\n            [0.2, -0.1],\n            [0.0, 0.0],\n            [0.3, 0.2],\n            [-0.1, -0.3]\n        ]), 0.001)\n    ]\n\n    all_results = []\n    for X, alpha in test_cases:\n        # 1. Get dimensions n (observations) and p (features).\n        n, p = X.shape\n\n        # 2. Compute the sample mean vector.\n        mu = np.mean(X, axis=0)\n\n        # 3. Compute the unbiased sample covariance matrix.\n        # np.cov with rowvar=False treats columns as variables.\n        # The default ddof=1 computes the unbiased sample covariance (divides by n-1).\n        # Handle the case where n = p, np.cov may fail if n=1. \n        # For n=1, covariance is undefined. Problem constraints ensure n>1.\n        if n > 1:\n            cov = np.cov(X, rowvar=False)\n        else: # Covariance is ill-defined for a single point\n            cov = np.zeros((p, p))\n\n        # 4. Compute the Moore-Penrose pseudoinverse.\n        # This handles singular and ill-conditioned matrices, which is essential\n        # when n = p, as the covariance matrix will be singular.\n        cov_inv = pinv(cov)\n        \n        # 5. Center the data.\n        X_centered = X - mu\n\n        # 6. Calculate squared Mahalanobis distances.\n        # A numerically efficient way is to compute (X_c @ C_inv) * X_c and sum rows.\n        # This is equivalent to diag(X_c @ C_inv @ X_c.T) but avoids computing the full n x n matrix.\n        mahal_sq_distances = np.sum((X_centered @ cov_inv) * X_centered, axis=1)\n\n        # 7. Compute the chi-square threshold.\n        # The degrees of freedom (df) is the number of features, p.\n        # We need the (1 - alpha) quantile of the chi-square distribution.\n        threshold = chi2.ppf(1 - alpha, df=p)\n\n        # 8. Find indices where distance exceeds the threshold.\n        # np.where returns a tuple of arrays; we take the first element.\n        outlier_indices = np.where(mahal_sq_distances > threshold)[0]\n\n        # 9. Store the sorted list of indices.\n        all_results.append(outlier_indices.tolist())\n\n    # Final print statement in the exact required format.\n    # str() creates a string representation '[[], [1, 2]]'\n    # .replace(\" \", \"\") removes all spaces to match the format '[[],[1,2]]'\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "经典统计量（如样本均值和协方差）本身对离群点非常敏感，这导致了“掩蔽效应”：离群点会污染用于检测它们的统计估计，从而逃避检测。为了解决这个问题，我们需要使用稳健统计方法。本练习  将通过实现最小协方差行列式 (Minimum Covariance Determinant, MCD) 估计器，并将其与经典马氏距离进行对比，让您亲身体验稳健方法在克服掩蔽效应、准确识别离群点方面的强大优势。",
            "id": "4183406",
            "problem": "一位研究人员正在分析多通道局部场电位（LFP）特征向量，以检测表现为离群点的伪影和生理异常。每个观测值是一个 $p$ 维向量，代表从 LFP 信号中提取的特征。研究人员希望比较使用经典协方差估计器与基于最小协方差行列式（MCD）的稳健策略在离群点检测上的差异，并计算两者的马氏距离。目标是量化两种策略所标记的离群点数量上的差异。\n\n从以下基本定义和事实出发：\n- 点 $\\mathbf{x} \\in \\mathbb{R}^p$ 与以均值 $\\boldsymbol{\\mu}$ 和正定协方差矩阵 $\\boldsymbol{\\Sigma}$ 为特征的分布之间的马氏距离定义为 $d(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\sqrt{(\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})}$。\n- 当数据能够被维度为 $p$ 的多元正态分布很好地建模时，马氏距离的平方近似服从自由度为 $p$ 的卡方分布，因此在理想模型下 $d(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})^2 \\sim \\chi^2_p$。\n- 最小协方差行列式（MCD）估计器是一种稳健的位置和散度估计器，它寻找一个大小为 $h$（其中 $h \\in \\{p+1, \\dots, n\\}$）的子集，该子集的协方差行列式最小。稳健位置是该子集的均值，稳健散度是其协方差。\n\n为每个测试数据集实现以下任务：\n1. 通过从给定均值 $\\boldsymbol{\\mu}$ 和协方差 $\\boldsymbol{\\Sigma}$ 的多元正态分布中抽样正常值（inliers），并附加指定数量的离群点来构建数据集。离群点通过在均值 $\\boldsymbol{\\mu}$ 上加上一个确定性偏移向量 $\\boldsymbol{\\delta}$ 和少量独立高斯噪声形成。样本总数为 $n$。为保证可复现性，请使用指定的随机种子。\n2. 使用全样本均值和全样本协方差计算经典马氏距离。\n3. 使用通过精确组合搜索获得的 MCD 估计器计算稳健马氏距离：设置 $h = \\lfloor \\alpha n \\rfloor$ 且 $\\alpha = 0.75$，确保 $h \\ge p+1$，枚举所有大小为 $h$ 的子集，并选择其样本协方差行列式最小的子集（忽略协方差为奇异或近奇异的子集）。使用該子集的均值和协方差分别作为稳健位置和散度。\n4. 对于经典距离和稳健距离，如果观测值的距离 $d$ 超过阈值 $\\tau_p = \\sqrt{F^{-1}_{\\chi^2_p}(0.975)}$，则将其标记为离群点。其中 $F^{-1}_{\\chi^2_p}$ 表示自由度为 $p$ 的卡方分布在概率 $0.975$ 处的逆累积分布函数。\n5. 对每个数据集，计算一个单一整数指标，该指标等于稳健策略标记的离群点数与经典策略标记的离群点数之差。\n\n您的程序必须实现以上内容，无需任何用户输入，并且必须生成单行输出，其中包含三个整数结果，格式为方括号括起来的逗号分隔列表。\n\n测试套件规范：\n- 情况 A（维度适中且有几个强离群点的理想路径）：\n  - $n = 10$, $p = 3$,\n  - $\\boldsymbol{\\mu} = [0.0, 0.0, 0.0]$,\n  - $\\boldsymbol{\\Sigma} = \\mathrm{diag}([1.0, 0.5, 0.8])$,\n  - 离群点数量 $= 2$,\n  - $\\boldsymbol{\\delta} = [9.0, 9.0, 9.0]$,\n  - 随机种子 $= 1234$。\n- 情况 B（维度较高、特征相关且有多个离群点）：\n  - $n = 12$, $p = 4$,\n  - $\\boldsymbol{\\mu} = [0.3, -0.2, 0.0, 0.1]$,\n  - 构建 $\\boldsymbol{\\Sigma}$ 为 $\\boldsymbol{\\Sigma} = \\mathbf{A}\\mathbf{A}^\\top$，其中\n    $\\mathbf{A} = \\begin{bmatrix}\n    1.0  & 0.2  & -0.1 & 0.0 \\\\\n    0.0  & 0.9  & 0.3  & -0.2 \\\\\n    0.1  & 0.0  & 1.1  & 0.2 \\\\\n    0.0  & -0.1 & 0.2  & 0.8\n    \\end{bmatrix}$,\n  - 离群点数量 $= 3$,\n  - $\\boldsymbol{\\delta} = [7.0, -8.0, 9.0, -7.0]$,\n  - 随机种子 $= 42$。\n- 情况 C（边界情况的小样本量和单个极端离群点）：\n  - $n = 9$, $p = 3$,\n  - $\\boldsymbol{\\mu} = [0.0, 0.0, 0.0]$,\n  - 构建 $\\boldsymbol{\\Sigma}$ 为 $\\boldsymbol{\\Sigma} = \\mathbf{B}\\mathbf{B}^\\top$，其中\n    $\\mathbf{B} = \\begin{bmatrix}\n    1.0  & 0.4 & 0.0 \\\\\n    0.2  & 0.8 & 0.3 \\\\\n    0.0  & 0.1 & 0.9\n    \\end{bmatrix}$,\n  - 离群点数量 $= 1$,\n  - $\\boldsymbol{\\delta} = [10.0, 0.0, -10.0]$,\n  - 随机种子 $= 2021$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，格式为方括号括起来的逗号分隔列表（例如 `[r_A,r_B,r_C]`），其中 $r_A$、$r_B$ 和 $r_C$ 分别是情况 A、B 和 C 的整数差值（稳健方法标记数减去经典方法标记数）。不应打印任何其他文本。",
            "solution": "该问题要求在多元数据离群点检测方面，对一种经典统计方法和一种稳健统计方法进行比较。任务的核心是实现这两种方法，将它们应用于综合生成的数据集，并量化其性能差异。该分析取决于马氏距离，这是多元统计学中的一个基本工具。\n\n### 原理1：马氏距离与离群点检测\n\n离群点是样本中与其他观测值显著偏离的观测值。对于 $p$ 维数据，这种偏离必须在 $p$ 维空间中进行评估。马氏距离为此评估提供了坚实的基础。给定一个假定从均值为 $\\boldsymbol{\\mu} \\in \\mathbb{R}^p$、正定协方差矩阵为 $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$ 的分布中抽样的数据集，一个点 $\\mathbf{x} \\in \\mathbb{R}^p$ 到该分布中心的马氏距离定义为：\n$$\nd(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\sqrt{(\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})}\n$$\n此距离优于欧几里得距离，因为它是尺度不变的，并且考虑了数据的相关性结构。它有效地测量了一个点沿数据主成分轴方向与均值相差的标准差单位数。\n\n在数据服从多元正态分布 $N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ 的零假设下，马氏距离的平方 $d^2$ 已知服从自由度为 $p$ 的卡方分布，即 $d^2 \\sim \\chi^2_p$。这一统计特性使我们能够为离群点检测建立一个正式的阈值。如果一个观测值的距离超过了从 $\\chi^2_p$ 分布推导出的临界值，它就会被标记为离群点。问题指定了一个对应于概率 $0.975$ 的阈值，因此阈值距离为 $\\tau_p = \\sqrt{F^{-1}_{\\chi^2_p}(0.975)}$，其中 $F^{-1}_{\\chi^2_p}$ 是 $\\chi^2_p$ 分布的分位数函数（逆累积分布函数）。\n\n### 原理2：经典方法及其局限性\n\n在实践中，真实的总体参数 $\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\Sigma}$ 是未知的，必须从数据样本 $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$ 中估计。经典方法使用样本均值 $\\hat{\\boldsymbol{\\mu}}$ 和样本协方差矩阵 $\\hat{\\boldsymbol{\\Sigma}}$ 作为估计量：\n$$\n\\hat{\\boldsymbol{\\mu}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i \\quad \\text{和} \\quad \\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{n-1} \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})^\\top\n$$\n然后，经典马氏距离计算为 $d(\\mathbf{x}_i; \\hat{\\boldsymbol{\\mu}}, \\hat{\\boldsymbol{\\Sigma}})$。\n\n这种方法的关键缺陷在于，估计量 $\\hat{\\boldsymbol{\\mu}}$ 和 $\\hat{\\boldsymbol{\\Sigma}}$ 本身对离群点高度敏感。单个极端离群点会将其方向上的样本均值拉向自身，并夸大样本协方差，尤其是在离群点的方向上。这种现象被称为**遮蔽效应**：离群点的存在污染了位置和散度的估计，这反过来可能导致离群点的马氏距离被人为地减小，从而可能妨碍其被检测到。\n\n### 原理3：使用最小协方差行列式（MCD）的稳健方法\n\n为了克服遮蔽效应，我们采用稳健的位置和散度估计器。最小协方差行列式（MCD）估计器是一种著名的稳健方法。MCD 的目标是从总共 $n$ 个观测值中找到一个包含 $h$ 个观测值（其中 $p+1 \\le h \\le n$）的子集，该子集的经典协方差矩阵具有最小的可能行列式。其直觉在于，协方差矩阵的行列式与包含数据的椭球体积的平方成正比。寻找具有最小行列式的子集等同于寻找 $h$ 个点最集中的“核心”组。这个核心子集被假定不含离群点。\n\n根据问题规范，位置的稳健估计 $\\boldsymbol{\\mu}_{\\text{MCD}}$ 和散度的稳健估计 $\\boldsymbol{\\Sigma}_{\\text{MCD}}$ 就是这个最优 $h$ 点子集的样本均值和样本协方差。然后，使用这些稳健估计计算所有 $n$ 个点的稳健马氏距离：$d(\\mathbf{x}_i; \\boldsymbol{\\mu}_{\\text{MCD}}, \\boldsymbol{\\Sigma}_{\\text{MCD}})$。由于 $\\boldsymbol{\\mu}_{\\text{MCD}}$ 和 $\\boldsymbol{\\Sigma}_{\\text{MCD}}$ 不受离群点的影响，因此真实离群点的马氏距离会很大，而正常值的马氏距离会很小，从而实现更可靠的分离。\n\n### 算法实现\n\n该解决方案通过对每个测试案例执行以下步骤来实现：\n\n1.  **数据生成**：构建包含 $n$ 个点的数据集 $\\mathbf{X}$。首先，从指定的多元正态分布 $N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ 中抽样 $n_{inliers} = n - n_{outliers}$ 个正常值。其次，通过从一个以显著偏移的均值为中心的分布 $N(\\boldsymbol{\\mu} + \\boldsymbol{\\delta}, \\sigma^2 \\mathbf{I})$ 中抽样来生成 $n_{outliers}$ 个离群点。其中，$\\sigma^2$ 是一个很小的方差（例如 0.01），以根据问题中“少量独立高斯噪声”的条款引入微小变化。为保证可复现性，使用特定的随机种子。\n\n2.  **经典分析**：从完整数据集 $\\mathbf{X}$ 计算经典样本均值 $\\hat{\\boldsymbol{\\mu}}$ 和协方差 $\\hat{\\boldsymbol{\\Sigma}}$。计算逆协方差矩阵 $\\hat{\\boldsymbol{\\Sigma}}^{-1}$，并为 $n$ 个点中的每一个计算经典马氏距离 $d_{classical}$。\n\n3.  **稳健 MCD 分析**：参数 $h$ 设置为 $\\lfloor \\alpha n \\rfloor$，其中 $\\alpha=0.75$。对所有大小为 $h$ 的 $\\binom{n}{h}$ 个索引子集执行精确组合搜索。对于每个子集：\n    *   计算其样本协方差矩阵。\n    *   计算该协方差矩阵的行列式。行列式接近于零的子集因其奇异或病态而被丢弃。\n    *   识别出产生最小行列式的子集。该最优子集的均值和协方差被存储为 $\\boldsymbol{\\mu}_{\\text{MCD}}$ 和 $\\boldsymbol{\\Sigma}_{\\text{MCD}}$。\n\n    然后，使用稳健协方差矩阵的逆 $\\boldsymbol{\\Sigma}_{\\text{MCD}}^{-1}$ 来计算原始数据集 $\\mathbf{X}$ 中所有 $n$ 个点的稳健马氏距离 $d_{robust}$。\n\n4.  **离群点标记与比较**：使用卡方分布的逆累积分布函数计算检测阈值 $\\tau_p = \\sqrt{F^{-1}_{\\chi^2_p}(0.975)}$。通过计算马氏距离大于 $\\tau_p$ 的点数来确定每种方法标记的离群点数量。\n    *   $N_{classical} = |\\{ i \\mid d_{classical}(\\mathbf{x}_i) > \\tau_p \\}|$\n    *   $N_{robust} = |\\{ i \\mid d_{robust}(\\mathbf{x}_i) > \\tau_p \\}|$\n\n5.  **指标计算**：测试用例的最终结果是整数差 $N_{robust} - N_{classical}$。该指标量化了稳健方法相对于经典方法识别出更多（或更少）离群点的数量。正值表示稳健方法更敏感，这很可能是因为它成功克服了妨碍经典方法的遮蔽效应。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves the outlier detection problem for three specified test cases.\n    \"\"\"\n\n    # Case A: Happy path with moderate dimension and a couple of strong outliers\n    case_a = {\n        \"n\": 10, \"p\": 3,\n        \"mu\": np.array([0.0, 0.0, 0.0]),\n        \"Sigma\": np.diag([1.0, 0.5, 0.8]),\n        \"n_out\": 2,\n        \"delta\": np.array([9.0, 9.0, 9.0]),\n        \"seed\": 1234\n    }\n\n    # Case B: Higher dimension with correlated features and several outliers\n    A_b = np.array([\n        [1.0, 0.2, -0.1, 0.0],\n        [0.0, 0.9, 0.3, -0.2],\n        [0.1, 0.0, 1.1, 0.2],\n        [0.0, -0.1, 0.2, 0.8]\n    ])\n    Sigma_b = A_b @ A_b.T\n    case_b = {\n        \"n\": 12, \"p\": 4,\n        \"mu\": np.array([0.3, -0.2, 0.0, 0.1]),\n        \"Sigma\": Sigma_b,\n        \"n_out\": 3,\n        \"delta\": np.array([7.0, -8.0, 9.0, -7.0]),\n        \"seed\": 42\n    }\n\n    # Case C: Boundary-like small sample size and a single extreme outlier\n    B_c = np.array([\n        [1.0, 0.4, 0.0],\n        [0.2, 0.8, 0.3],\n        [0.0, 0.1, 0.9]\n    ])\n    Sigma_c = B_c @ B_c.T\n    case_c = {\n        \"n\": 9, \"p\": 3,\n        \"mu\": np.array([0.0, 0.0, 0.0]),\n        \"Sigma\": Sigma_c,\n        \"n_out\": 1,\n        \"delta\": np.array([10.0, 0.0, -10.0]),\n        \"seed\": 2021\n    }\n\n    test_cases = [case_a, case_b, case_c]\n    results = []\n\n    for case in test_cases:\n        n, p = case[\"n\"], case[\"p\"]\n        mu, Sigma = case[\"mu\"], case[\"Sigma\"]\n        n_out, delta, seed = case[\"n_out\"], case[\"delta\"], case[\"seed\"]\n\n        np.random.seed(seed)\n\n        # 1. Construct the dataset\n        n_in = n - n_out\n        inliers = np.random.multivariate_normal(mu, Sigma, n_in)\n        \n        # Outlier generation from N(mu + delta, 0.01 * I)\n        outlier_mean = mu + delta\n        outlier_cov = np.eye(p) * 0.01\n        outliers = np.random.multivariate_normal(outlier_mean, outlier_cov, n_out)\n        \n        X = np.vstack((inliers, outliers))\n\n        # 4. Outlier flagging threshold\n        threshold_sq = chi2.ppf(0.975, df=p)\n        threshold = np.sqrt(threshold_sq)\n\n        # 2. Compute classical Mahalanobis distances\n        mu_classical = np.mean(X, axis=0)\n        Sigma_classical = np.cov(X, rowvar=False)\n        inv_Sigma_classical = np.linalg.inv(Sigma_classical)\n        \n        diffs_classical = X - mu_classical\n        d_classical_sq = np.sum((diffs_classical @ inv_Sigma_classical) * diffs_classical, axis=1)\n        d_classical = np.sqrt(d_classical_sq)\n        \n        n_outliers_classical = np.sum(d_classical > threshold)\n\n        # 3. Compute robust Mahalanobis distances (MCD)\n        h = int(0.75 * n)\n        if h < p + 1:\n            h = p + 1\n\n        min_det = np.inf\n        best_mu_mcd, best_Sigma_mcd = None, None\n        \n        indices = range(n)\n        for subset_indices in combinations(indices, h):\n            subset = X[list(subset_indices), :]\n            current_cov = np.cov(subset, rowvar=False)\n            \n            # Using slogdet to handle potential underflow/overflow\n            sign, logdet = np.linalg.slogdet(current_cov)\n\n            if sign == 1:\n                if logdet < min_det:\n                    min_det = logdet\n                    best_mu_mcd = np.mean(subset, axis=0)\n                    best_Sigma_mcd = current_cov\n\n        if best_Sigma_mcd is None:\n            raise ValueError(\"Could not find a non-singular subset for MCD.\")\n            \n        inv_Sigma_mcd = np.linalg.inv(best_Sigma_mcd)\n        diffs_robust = X - best_mu_mcd\n        d_robust_sq = np.sum((diffs_robust @ inv_Sigma_mcd) * diffs_robust, axis=1)\n        d_robust = np.sqrt(d_robust_sq)\n        \n        n_outliers_robust = np.sum(d_robust > threshold)\n\n        # 5. Compute the final metric\n        diff = n_outliers_robust - n_outliers_classical\n        results.append(diff)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}