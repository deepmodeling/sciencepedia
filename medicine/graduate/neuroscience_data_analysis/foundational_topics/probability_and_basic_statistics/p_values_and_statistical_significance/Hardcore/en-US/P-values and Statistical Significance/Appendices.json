{
    "hands_on_practices": [
        {
            "introduction": "The journey into statistical inference begins with mastering the fundamental mechanics. This first practice exercise grounds the abstract definition of a p-value in a concrete, step-by-step calculation. By working through a classic scenario of testing a single hypothesis in a neural response experiment, you will solidify your understanding of how an observed sample mean is converted into a test statistic and, subsequently, into the p-value that quantifies evidence against the null hypothesis .",
            "id": "4183927",
            "problem": "A systems neuroscience lab analyzes the evoked membrane-potential deflection per stimulus presentation in a recurrent cortical microcircuit preparation. The experimenters collect $n = 50$ independent and identically distributed (i.i.d.) trial measurements of the evoked response amplitude (in arbitrary units), with an empirically observed sample mean $\\bar{x} = 1.2$. From prior, independent calibration experiments on the same setup and cell type, the measurement noise standard deviation is treated as known and equal to $\\sigma = 1$. The scientific question is whether the mean evoked response differs from zero. Formally, they wish to test the two-sided hypothesis $H_{0}\\!:\\mu = \\mu_{0}$ versus $H_{1}\\!:\\mu \\neq \\mu_{0}$ with $\\mu_{0} = 0$. Assume trials are i.i.d., and that, by either exact normality of the underlying response distribution or by the Central Limit Theorem (CLT), the sampling distribution of the standardized sample mean under $H_{0}$ is well approximated by a standard normal distribution.\n\nStarting from the core definitions of the sampling distribution of the mean under $H_{0}$ and the definition of a two-sided $p$-value as the probability, under $H_{0}$, of observing a test statistic at least as extreme as the realized one, compute the two-sided $z$-test $p$-value for this experiment. Express your final answer as a decimal number rounded to four significant figures.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded, well-posed, and objective. It presents a standard, solvable problem in elementary statistics and hypothesis testing, with all necessary parameters and assumptions explicitly provided.\n\nThe objective is to compute the two-sided $p$-value for a $z$-test. The problem provides the following information:\n- The sample size, $n = 50$.\n- The sample mean, $\\bar{x} = 1.2$.\n- The known population standard deviation, $\\sigma = 1$.\n- The null hypothesis, $H_{0}\\!:\\mu = \\mu_{0}$, where the hypothesized mean is $\\mu_{0} = 0$.\n- The alternative hypothesis, $H_{1}\\!:\\mu \\neq \\mu_{0}$, which specifies a two-sided test.\n\nThe sampling distribution of the sample mean $\\bar{X}$ under the null hypothesis has a mean $\\mu_{\\bar{X}} = \\mu_{0} = 0$ and a standard deviation, known as the standard error of the mean (SEM), given by $\\sigma_{\\bar{x}}$. The SEM is calculated as:\n$$\n\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\n$$\nSubstituting the given values, we find the SEM:\n$$\n\\sigma_{\\bar{x}} = \\frac{1}{\\sqrt{50}}\n$$\nThe problem states that the sampling distribution of the standardized sample mean is well-approximated by a standard normal distribution. This allows us to use a $z$-test. The test statistic, denoted as $z_{obs}$, quantifies how many standard errors the observed sample mean $\\bar{x}$ is from the hypothesized population mean $\\mu_0$. The formula for the $z$-statistic is:\n$$\nz_{obs} = \\frac{\\bar{x} - \\mu_{0}}{\\sigma_{\\bar{x}}} = \\frac{\\bar{x} - \\mu_{0}}{\\sigma / \\sqrt{n}}\n$$\nSubstituting the given values into this formula:\n$$\nz_{obs} = \\frac{1.2 - 0}{1 / \\sqrt{50}} = 1.2 \\times \\sqrt{50}\n$$\nWe can simplify the expression for $z_{obs}$:\n$$\nz_{obs} = 1.2 \\times \\sqrt{25 \\times 2} = 1.2 \\times 5\\sqrt{2} = 6\\sqrt{2}\n$$\nNumerically, this value is approximately:\n$$\nz_{obs} \\approx 8.485281\n$$\nThe $p$-value for a two-sided test is the probability of observing a test statistic at least as extreme as the one computed, assuming the null hypothesis $H_0$ is true. \"At least as extreme\" for a two-sided test means the test statistic is either greater than or equal to $|z_{obs}|$ or less than or equal to $-|z_{obs}|$. If $Z$ is a random variable following the standard normal distribution, $\\mathcal{N}(0, 1)$, the $p$-value is given by:\n$$\np = P(Z \\geq |z_{obs}|) + P(Z \\leq -|z_{obs}|)\n$$\nDue to the symmetry of the standard normal distribution, $P(Z \\geq |z_{obs}|) = P(Z \\leq -|z_{obs}|)$. Therefore, the expression for the $p$-value simplifies to:\n$$\np = 2 \\times P(Z \\geq |z_{obs}|)\n$$\nSubstituting our calculated value of $z_{obs} = 6\\sqrt{2}$:\n$$\np = 2 \\times P(Z \\geq 6\\sqrt{2})\n$$\nThe value of $6\\sqrt{2}$ is very large for a standard normal distribution, indicating that the observed sample mean $\\bar{x}=1.2$ is extremely unlikely to have occurred if the true mean were $\\mu=0$. The corresponding probability in the tail of the distribution will be exceedingly small. This probability cannot be found in standard printed tables and must be computed using numerical software.\nUsing a high-precision calculator for the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z)$, the $p$-value is:\n$$\np = 2 \\times (1 - \\Phi(6\\sqrt{2})) \\approx 2 \\times (1 - \\Phi(8.485281))\n$$\nThis computation yields:\n$$\np \\approx 2.220446 \\times 10^{-17}\n$$\nThe problem requires the final answer to be expressed as a decimal number rounded to four significant figures. Rounding the computed value gives:\n$$\np \\approx 2.220 \\times 10^{-17}\n$$\nThis extremely small $p$-value would lead to the rejection of the null hypothesis $H_0$ at any conventional significance level (e.g., $\\alpha = 0.05, 0.01, 0.001$), providing very strong evidence that the true mean evoked response differs from zero.",
            "answer": "$$\\boxed{2.220 \\times 10^{-17}}$$"
        },
        {
            "introduction": "Neuroscience research rarely involves just a single hypothesis test. From voxel-wise fMRI analysis to multi-unit recordings, we often perform tens, thousands, or even millions of tests simultaneously. This practice explores the critical consequences of this multiplicity . By calculating the Family-Wise Error Rate (FWER), you will see firsthand how quickly the probability of making at least one false discovery inflates, making correction methods an absolute necessity for valid scientific conclusions.",
            "id": "4183945",
            "problem": "A neuroscientist conducts a voxel-wise analysis within a small region-of-interest in functional magnetic resonance imaging (fMRI), testing $m=20$ independent null hypotheses, one per voxel. Each hypothesis test uses a per-voxel significance threshold $\\alpha=0.05$ with no multiple-comparisons correction. Assume the global null holds (that is, all $m$ null hypotheses are true), and that the test statistics across voxels are independent.\n\nStarting only from the core definitions that (i) a Type I error is a false rejection of a true null hypothesis, (ii) the Family-Wise Error Rate (FWER) is the probability of making at least one Type I error among the family of tests, and (iii) each independent hypothesis test under a true null has probability $\\alpha$ of a Type I error, derive expressions for:\n- the FWER across the $m$ tests, and\n- the expected number of false positives across the $m$ tests.\n\nThen evaluate these expressions for $m=20$ and $\\alpha=0.05$. Provide a brief interpretation in the context of voxel-wise inference in fMRI. Express your final answer as a two-entry row vector $\\big[$FWER, expected number of false positives$\\big]$ in exact analytic form. Do not round. Do not include units.",
            "solution": "The problem statement is evaluated for validity.\n\n**Step 1: Extract Givens**\n-   Number of independent null hypotheses: $m=20$.\n-   Per-voxel significance threshold: $\\alpha=0.05$.\n-   Correction for multiple comparisons: None.\n-   Global null hypothesis: Assumed to be true (all $m$ null hypotheses are true).\n-   Voxel test statistics: Assumed to be independent.\n-   Definition (i): A Type I error is a false rejection of a true null hypothesis.\n-   Definition (ii): The Family-Wise Error Rate (FWER) is the probability of making at least one Type I error among the family of tests.\n-   Definition (iii): Each independent hypothesis test under a true null has probability $\\alpha$ of a Type I error.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It presents a canonical scenario in statistical hypothesis testing, specifically the issue of multiple comparisons, which is a critical topic in fields like neuroscience, genomics, and medical imaging. The assumptions provided (independence, global null true) are standard for deriving the baseline error rates in the absence of correction. The problem is self-contained, with all necessary parameters ($m$, $\\alpha$) and definitions provided. There are no contradictions, ambiguities, or scientifically unsound premises.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\nThe objective is to derive expressions for the Family-Wise Error Rate (FWER) and the expected number of false positives, and then to evaluate these for the given parameters. The derivation will proceed from the provided first principles.\n\n**Derivation of the Family-Wise Error Rate (FWER)**\n\nThe FWER is defined as the probability of making at least one Type I error across the family of $m$ tests. Let $E_i$ be the event of a Type I error occurring for the $i$-th hypothesis test, where $i \\in \\{1, 2, \\dots, m\\}$.\n\nSince all $m$ null hypotheses are stipulated to be true (the global null holds), any rejection of a null hypothesis constitutes a Type I error (a false positive). The probability of a Type I error for any single test is given as $P(E_i) = \\alpha$.\n\nThe probability of *not* making a Type I error on a single test is the complement, $P(\\text{not } E_i) = 1 - \\alpha$.\n\nThe FWER is the probability of the union of the events $E_i$:\n$$\n\\text{FWER} = P(E_1 \\cup E_2 \\cup \\dots \\cup E_m)\n$$\nCalculating this directly is cumbersome. It is more straightforward to calculate the probability of the complementary event: making *no* Type I errors across all $m$ tests. The event of making no Type I errors is the intersection of the events of not making a Type I error on each individual test:\n$$\nP(\\text{no Type I errors}) = P(\\text{not } E_1 \\cap \\text{not } E_2 \\cap \\dots \\cap \\text{not } E_m)\n$$\nThe problem states that the tests are independent. Therefore, the probability of the intersection of these events is the product of their individual probabilities:\n$$\nP(\\text{no Type I errors}) = \\prod_{i=1}^{m} P(\\text{not } E_i) = \\prod_{i=1}^{m} (1 - \\alpha) = (1 - \\alpha)^m\n$$\nThe FWER is $1$ minus the probability of this complementary event:\n$$\n\\text{FWER} = 1 - P(\\text{no Type I errors}) = 1 - (1 - \\alpha)^m\n$$\n\n**Derivation of the Expected Number of False Positives**\n\nLet $N_{FP}$ be the random variable representing the total number of false positives (Type I errors) across the $m$ tests. We can express $N_{FP}$ as the sum of $m$ indicator random variables, $X_i$, for $i = 1, \\dots, m$. Each $X_i$ is defined as:\n$$\nX_i = \\begin{cases} 1  \\text{if a Type I error occurs for test } i \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe total number of false positives is $N_{FP} = \\sum_{i=1}^{m} X_i$.\n\nThe expected value of $N_{FP}$ is, by the linearity of expectation, the sum of the expected values of the individual indicator variables:\n$$\nE[N_{FP}] = E\\left[\\sum_{i=1}^{m} X_i\\right] = \\sum_{i=1}^{m} E[X_i]\n$$\nThe expected value of an indicator variable $X_i$ is the probability of the event it indicates. In this case, it is the probability of a Type I error for test $i$:\n$$\nE[X_i] = (1) \\cdot P(X_i=1) + (0) \\cdot P(X_i=0) = P(\\text{Type I error for test } i) = \\alpha\n$$\nThis holds for all $i$ from $1$ to $m$. Substituting this back into the sum:\n$$\nE[N_{FP}] = \\sum_{i=1}^{m} \\alpha = m \\alpha\n$$\nNote that the linearity of expectation does not require the random variables to be independent, although independence was given and used for the FWER derivation.\n\n**Evaluation and Interpretation**\n\nNow, we evaluate these expressions for the given parameters: $m=20$ and $\\alpha=0.05$.\n\nThe FWER is:\n$$\n\\text{FWER} = 1 - (1 - 0.05)^{20} = 1 - (0.95)^{20}\n$$\nThe expected number of false positives is:\n$$\nE[N_{FP}] = m \\alpha = 20 \\times 0.05 = 1\n$$\n**Interpretation:**\nFor an analysis of just $m=20$ independent voxels under the global null hypothesis (i.e., no true activation exists anywhere), using a conventional per-voxel significance level of $\\alpha=0.05$ without any correction for multiple comparisons leads to a Family-Wise Error Rate of $1 - (0.95)^{20} \\approx 0.6415$. This means there is a greater than $64\\%$ probability of obtaining at least one statistically significant result (a false positive) purely by chance. Furthermore, the expected number of such false positive voxels is $1$. This result demonstrates a fundamental problem in neuroimaging data analysis: even for a very small region of interest, the failure to correct for multiple comparisons dramatically inflates the likelihood of reporting false discoveries. In a typical whole-brain fMRI analysis involving tens or hundreds of thousands of voxels, this problem becomes severe, making correction for multiple comparisons a procedural necessity.\n\nThe expressions in their exact analytic form are $1 - (0.95)^{20}$ for the FWER and $1$ for the expected number of false positives.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1 - (0.95)^{20}  1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Having established the problem of inflated error rates in multiple testing, we now turn to a practical and powerful solution. The Benjamini-Hochberg procedure offers a way to control the False Discovery Rate (FDR), balancing the need to make discoveries with the need to limit false positives. This exercise provides a hands-on opportunity to apply this cornerstone of modern statistical analysis to a realistic set of p-values, allowing you to determine which results are statistically significant and to compute the corresponding adjusted q-values .",
            "id": "4183925",
            "problem": "A systems neuroscience laboratory tests the effect of a pharmacological manipulation on spike-field coupling strength across $m=20$ pre-specified cortical regions of interest in the primate motor system. For each region, a linear model is fit to assess the condition effect, and a two-sided test yields a $p$-value. The $p$-values (sorted in ascending order) are\n$$\np_{(1)}=0.0005,\\quad p_{(2)}=0.004,\\quad p_{(3)}=0.02,\\quad p_{(4)}=0.021,\\quad p_{(5)}=0.024,\\quad p_{(6)}=0.03,\\quad p_{(7)}=0.031,\\quad p_{(8)}=0.034,\\quad p_{(9)}=0.04,\\quad p_{(10)}=0.043,\\\\\np_{(11)}=0.048,\\quad p_{(12)}=0.051,\\quad p_{(13)}=0.06,\\quad p_{(14)}=0.072,\\quad p_{(15)}=0.081,\\quad p_{(16)}=0.12,\\quad p_{(17)}=0.2,\\quad p_{(18)}=0.3,\\quad p_{(19)}=0.4,\\quad p_{(20)}=0.7.\n$$\nUsing the False Discovery Rate (FDR) target $q=0.1$ and the Benjamini–Hochberg step-up procedure, determine the number of Benjamini–Hochberg-selected discoveries and compute the Benjamini–Hochberg-adjusted $q$-values (that is, adjusted $p$-values under the Benjamini–Hochberg procedure) for each ordered hypothesis $i=1,\\dots,20$. Express each adjusted $q$-value as a simplified rational number (exact form, no rounding). Provide your final answer as a single row matrix whose first entry is the number of discoveries and whose next $20$ entries are the adjusted $q$-values corresponding to $p_{(1)},\\dots,p_{(20)}$, in that order. Do not include any units.",
            "solution": "The problem requires the application of the Benjamini–Hochberg (BH) procedure to a set of $p$-values from a neuroscience experiment to control the False Discovery Rate (FDR). We are given $m=20$ sorted $p$-values, $p_{(1)} \\leq p_{(2)} \\leq \\dots \\leq p_{(20)}$, and an FDR target of $q=0.1$. We need to find the number of discoveries (rejected null hypotheses) and the corresponding adjusted $p$-values (also known as $q$-values) for all $20$ tests.\n\nFirst, we determine the number of discoveries. The BH step-up procedure specifies finding the largest integer $k$ (where $1 \\leq k \\leq m$) such that the $k$-th ordered $p$-value, $p_{(k)}$, satisfies the inequality:\n$$p_{(k)} \\leq \\frac{k}{m}q$$\nIf such a $k$ exists, we reject the null hypotheses corresponding to the $p$-values $p_{(1)}, p_{(2)}, \\dots, p_{(k)}$. The number of discoveries is then $k$. If no such $k$ exists, we reject no hypotheses, and the number of discoveries is $0$.\n\nIn this problem, $m=20$ and $q=0.1$. The condition is $p_{(k)} \\leq \\frac{k}{20}(0.1) = \\frac{k}{200}$. We search for the largest $k$ that satisfies this, starting from $k=m=20$ and proceeding downwards.\n\nFor $k=20$, $p_{(20)}=0.7$. The threshold is $\\frac{20}{200} = 0.1$. $0.7 > 0.1$, so the condition is not met.\nFor $k=19$, $p_{(19)}=0.4$. The threshold is $\\frac{19}{200} = 0.095$. $0.4 > 0.095$, not met.\n...\nWe continue this process. Let's check $k=14$ and $k=13$:\nFor $k=14$, $p_{(14)}=0.072$. The threshold is $\\frac{14}{200} = 0.07$. Since $0.072 > 0.07$, the condition is not met.\nFor $k=13$, $p_{(13)}=0.06$. The threshold is $\\frac{13}{200} = 0.065$. Since $0.06 \\leq 0.065$, the condition is met.\n\nSince $k=13$ is the largest index for which the condition $p_{(k)} \\leq \\frac{k}{m}q$ holds, we reject the $13$ null hypotheses corresponding to $p_{(1)}, \\dots, p_{(13)}$. The number of Benjamini–Hochberg-selected discoveries is $13$.\n\nNext, we compute the BH-adjusted $p$-values, denoted as $q_{(i)}$, for each ordered hypothesis $i=1, \\dots, 20$. The adjusted $p$-value for the $i$-th ordered $p$-value is defined as:\n$$q_{(i)} = \\min_{j=i, \\dots, m} \\left( \\frac{m}{j} p_{(j)} \\right)$$\nThis definition ensures that the sequence of adjusted $p$-values is monotonically non-decreasing, i.e., $q_{(1)} \\leq q_{(2)} \\leq \\dots \\leq q_{(m)}$.\nA practical way to compute these values is to first calculate the intermediate values $p'_{(i)} = \\frac{m}{i}p_{(i)}$ for $i=1, \\dots, m$. Then, the adjusted values are found by enforcing monotonicity from the largest index downwards:\n$$q_{(m)} = p'_{(m)} = p_{(m)}$$\n$$q_{(i)} = \\min(p'_{(i)}, q_{(i+1)}) \\quad \\text{for } i = m-1, m-2, \\dots, 1$$\n\nWe have $m=20$. We will calculate $p'_{(i)} = \\frac{20}{i}p_{(i)}$ for each $i$ and then determine $q_{(i)}$. The problem asks for simplified rational numbers.\n\n$q_{(20)} = \\frac{20}{20}p_{(20)} = 0.7 = \\frac{7}{10}$.\n$q_{(19)} = \\min(\\frac{20}{19}p_{(19)}, q_{(20)}) = \\min(\\frac{20}{19} \\times 0.4, \\frac{7}{10}) = \\min(\\frac{8}{19}, \\frac{7}{10}) = \\frac{8}{19}$.\n$q_{(18)} = \\min(\\frac{20}{18}p_{(18)}, q_{(19)}) = \\min(\\frac{10}{9} \\times 0.3, \\frac{8}{19}) = \\min(\\frac{1}{3}, \\frac{8}{19}) = \\frac{1}{3}$.\n$q_{(17)} = \\min(\\frac{20}{17}p_{(17)}, q_{(18)}) = \\min(\\frac{20}{17} \\times 0.2, \\frac{1}{3}) = \\min(\\frac{4}{17}, \\frac{1}{3}) = \\frac{4}{17}$.\n$q_{(16)} = \\min(\\frac{20}{16}p_{(16)}, q_{(17)}) = \\min(\\frac{5}{4} \\times 0.12, \\frac{4}{17}) = \\min(\\frac{3}{20}, \\frac{4}{17}) = \\frac{3}{20}$.\n$q_{(15)} = \\min(\\frac{20}{15}p_{(15)}, q_{(16)}) = \\min(\\frac{4}{3} \\times 0.081, \\frac{3}{20}) = \\min(\\frac{27}{250}, \\frac{3}{20}) = \\frac{27}{250}$.\n$q_{(14)} = \\min(\\frac{20}{14}p_{(14)}, q_{(15)}) = \\min(\\frac{10}{7} \\times 0.072, \\frac{27}{250}) = \\min(\\frac{18}{175}, \\frac{27}{250}) = \\frac{18}{175}$.\n$q_{(13)} = \\min(\\frac{20}{13}p_{(13)}, q_{(14)}) = \\min(\\frac{20}{13} \\times 0.06, \\frac{18}{175}) = \\min(\\frac{6}{65}, \\frac{18}{175}) = \\frac{6}{65}$.\n$q_{(12)} = \\min(\\frac{20}{12}p_{(12)}, q_{(13)}) = \\min(\\frac{5}{3} \\times 0.051, \\frac{6}{65}) = \\min(\\frac{17}{200}, \\frac{6}{65}) = \\frac{17}{200}$.\n$q_{(11)} = \\min(\\frac{20}{11}p_{(11)}, q_{(12)}) = \\min(\\frac{20}{11} \\times 0.048, \\frac{17}{200}) = \\min(\\frac{24}{275}, \\frac{17}{200}) = \\frac{17}{200}$.\n$q_{(10)} = \\min(\\frac{20}{10}p_{(10)}, q_{(11)}) = \\min(2 \\times 0.043, \\frac{17}{200}) = \\min(\\frac{43}{500}, \\frac{17}{200}) = \\frac{17}{200}$.\n$q_{(9)} = \\min(\\frac{20}{9}p_{(9)}, q_{(10)}) = \\min(\\frac{20}{9} \\times 0.04, \\frac{17}{200}) = \\min(\\frac{4}{45}, \\frac{17}{200}) = \\frac{17}{200}$.\n$q_{(8)} = \\min(\\frac{20}{8}p_{(8)}, q_{(9)}) = \\min(\\frac{5}{2} \\times 0.034, \\frac{17}{200}) = \\min(\\frac{17}{200}, \\frac{17}{200}) = \\frac{17}{200}$.\n$q_{(7)} = \\min(\\frac{20}{7}p_{(7)}, q_{(8)}) = \\min(\\frac{20}{7} \\times 0.031, \\frac{17}{200}) = \\min(\\frac{31}{350}, \\frac{17}{200}) = \\frac{17}{200}$.\n$q_{(6)} = \\min(\\frac{20}{6}p_{(6)}, q_{(7)}) = \\min(\\frac{10}{3} \\times 0.03, \\frac{17}{200}) = \\min(\\frac{1}{10}, \\frac{17}{200}) = \\frac{17}{200}$.\n$q_{(5)} = \\min(\\frac{20}{5}p_{(5)}, q_{(6)}) = \\min(4 \\times 0.024, \\frac{17}{200}) = \\min(\\frac{12}{125}, \\frac{17}{200}) = \\frac{17}{200}$.\n$q_{(4)} = \\min(\\frac{20}{4}p_{(4)}, q_{(5)}) = \\min(5 \\times 0.021, \\frac{17}{200}) = \\min(\\frac{21}{200}, \\frac{17}{200}) = \\frac{17}{200}$.\n$q_{(3)} = \\min(\\frac{20}{3}p_{(3)}, q_{(4)}) = \\min(\\frac{20}{3} \\times 0.02, \\frac{17}{200}) = \\min(\\frac{2}{15}, \\frac{17}{200}) = \\frac{17}{200}$.\n$q_{(2)} = \\min(\\frac{20}{2}p_{(2)}, q_{(3)}) = \\min(10 \\times 0.004, \\frac{17}{200}) = \\min(\\frac{1}{25}, \\frac{17}{200}) = \\frac{1}{25}$.\n$q_{(1)} = \\min(\\frac{20}{1}p_{(1)}, q_{(2)}) = \\min(20 \\times 0.0005, \\frac{1}{25}) = \\min(\\frac{1}{100}, \\frac{1}{25}) = \\frac{1}{100}$.\n\nThe number of discoveries is 13. The 20 adjusted $q$-values in order are:\n$q_{(1)} = \\frac{1}{100}$, $q_{(2)} = \\frac{1}{25}$, $q_{(3)} = \\frac{17}{200}$, $q_{(4)} = \\frac{17}{200}$, $q_{(5)} = \\frac{17}{200}$, $q_{(6)} = \\frac{17}{200}$, $q_{(7)} = \\frac{17}{200}$, $q_{(8)} = \\frac{17}{200}$, $q_{(9)} = \\frac{17}{200}$, $q_{(10)} = \\frac{17}{200}$, $q_{(11)} = \\frac{17}{200}$, $q_{(12)} = \\frac{17}{200}$, $q_{(13)} = \\frac{6}{65}$, $q_{(14)} = \\frac{18}{175}$, $q_{(15)} = \\frac{27}{250}$, $q_{(16)} = \\frac{3}{20}$, $q_{(17)} = \\frac{4}{17}$, $q_{(18)} = \\frac{1}{3}$, $q_{(19)} = \\frac{8}{19}$, $q_{(20)} = \\frac{7}{10}$.\n\nThe final answer is a row matrix containing the number of discoveries followed by these 20 values in order.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n13  \\frac{1}{100}  \\frac{1}{25}  \\frac{17}{200}  \\frac{17}{200}  \\frac{17}{200}  \\frac{17}{200}  \\frac{17}{200}  \\frac{17}{200}  \\frac{17}{200}  \\frac{17}{200}  \\frac{17}{200}  \\frac{17}{200}  \\frac{6}{65}  \\frac{18}{175}  \\frac{27}{250}  \\frac{3}{20}  \\frac{4}{17}  \\frac{1}{3}  \\frac{8}{19}  \\frac{7}{10}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}