{
    "hands_on_practices": [
        {
            "introduction": "Often in neuroscience, the parameter we directly estimate from data is not the final quantity of interest. For example, we might model inter-spike intervals with an exponential distribution and estimate its rate parameter $\\lambda$, but the scientific question might concern a non-linear function of this rate. This exercise introduces the Delta method, a powerful tool that uses the Central Limit Theorem to determine the statistical properties, like asymptotic variance, of such transformed parameters. Mastering this allows you to quantify uncertainty for a wide range of derived quantities in your models .",
            "id": "852388",
            "problem": "Consider a set of $n$ independent and identically distributed (i.i.d.) random variables, $X_1, X_2, \\ldots, X_n$, drawn from an exponential distribution with a rate parameter $\\lambda  0$. The probability density function (PDF) for a single observation $X_i$ is given by:\n$$f(x_i; \\lambda) = \\lambda e^{-\\lambda x_i} \\quad \\text{for} \\quad x_i \\ge 0$$\n\nLet $\\hat{\\lambda}_n$ be the maximum likelihood estimator (MLE) for the parameter $\\lambda$. From the asymptotic theory of MLEs, it is known that $\\hat{\\lambda}_n$ is asymptotically normal. Specifically,\n$$\n\\sqrt{n}(\\hat{\\lambda}_n - \\lambda) \\xrightarrow{d} N\\left(0, I(\\lambda)^{-1}\\right)\n$$\nas $n \\to \\infty$, where $I(\\lambda)$ is the Fisher information contained in a single observation from this distribution.\n\nThe Delta method provides a way to find the asymptotic distribution of a transformed estimator. If a sequence of random variables $T_n$ satisfies $\\sqrt{n}(T_n - \\theta) \\xrightarrow{d} N(0, \\sigma^2)$, then for a continuously differentiable function $g$ with $g'(\\theta) \\neq 0$, the transformed sequence $g(T_n)$ satisfies:\n$$\n\\sqrt{n}(g(T_n) - g(\\theta)) \\xrightarrow{d} N\\left(0, [g'(\\theta)]^2 \\sigma^2\\right)\n$$\nThe term $[g'(\\theta)]^2 \\sigma^2$ is defined as the asymptotic variance of the estimator $g(T_n)$.\n\nYour task is to derive the asymptotic variance of the MLE for the parameter $\\theta = \\lambda^2$. The estimator for $\\theta$ is constructed by substituting the MLE of $\\lambda$, i.e., $\\hat{\\theta}_n = (\\hat{\\lambda}_n)^2$.",
            "solution": "1. Fisher information for one exponential observation with rate $\\lambda$:\n$$I(\\lambda)=\\mathrm{E}\\Bigl[\\Bigl(\\frac{\\partial}{\\partial\\lambda}\\ln f(X;\\lambda)\\Bigr)^2\\Bigr]=\\frac{1}{\\lambda^2}.$$\nHence the MLE satisfies\n$$\\sqrt{n}(\\hat\\lambda_n-\\lambda)\\xrightarrow{d}N\\bigl(0,\\lambda^2\\bigr).$$\n2. Define $\\theta=\\lambda^2$ and $g(\\lambda)=\\lambda^2$. Then $g'(\\lambda)=2\\lambda$.  By the Delta method,\n$$\\sqrt{n}\\bigl(g(\\hat\\lambda_n)-g(\\lambda)\\bigr)\\xrightarrow{d}N\\bigl(0,(g'(\\lambda))^2\\lambda^2\\bigr).$$\n3. Compute the asymptotic variance:\n$$(g'(\\lambda))^2\\,\\lambda^2=(2\\lambda)^2\\lambda^2=4\\lambda^4.$$",
            "answer": "$$\\boxed{4\\lambda^4}$$"
        },
        {
            "introduction": "Neuroscience data is inherently multidimensional, from the coordinated firing of neural populations to the kinematic parameters of movement. This practice scales up our analysis from a single variable to a vector, introducing the multivariate Delta method. You will explore how to transform a 2D sample mean from Cartesian coordinates to polar coordinates and find the resulting asymptotic covariance matrix, a procedure analogous to calculating the uncertainty in the magnitude and direction of a neural population vector .",
            "id": "852428",
            "problem": "Let $(X_i, Y_i)$ for $i=1, \\dots, n$ be a sequence of $n$ independent and identically distributed (i.i.d.) random vectors, where each $(X_i, Y_i)$ is drawn from a uniform distribution on the unit square $[0, 1] \\times [0, 1]$. Let $(\\bar{X}_n, \\bar{Y}_n)^T = \\frac{1}{n} \\sum_{i=1}^n (X_i, Y_i)^T$ be the sample mean vector.\n\nThe multivariate Central Limit Theorem states that if $\\mathbf{Z}_n$ is the sample mean of i.i.d. random vectors with population mean $\\mu$ and covariance matrix $\\Sigma$, then the random vector $\\sqrt{n}(\\mathbf{Z}_n - \\mu)$ converges in distribution to a multivariate normal distribution $N(\\mathbf{0}, \\Sigma)$.\n\nThe multivariate Delta method provides the asymptotic distribution for a transformed random vector. If $g: \\mathbb{R}^k \\to \\mathbb{R}^m$ is a continuously-differentiable function, then $\\sqrt{n}(g(\\mathbf{Z}_n) - g(\\mu))$ converges in distribution to $N(\\mathbf{0}, G \\Sigma G^T)$, where $G$ is the $m \\times k$ Jacobian matrix of $g$ with entries $G_{ij} = \\frac{\\partial g_i}{\\partial z_j}$, evaluated at the point $\\mu$.\n\nLet $(R_n, \\Theta_n)$ be the representation of the sample mean vector $(\\bar{X}_n, \\bar{Y}_n)$ in polar coordinates, where the radius is $R_n = \\sqrt{\\bar{X}_n^2 + \\bar{Y}_n^2}$ and the angle is $\\Theta_n = \\arctan(\\bar{Y}_n / \\bar{X}_n)$. Let $(r_\\mu, \\theta_\\mu)$ be the polar coordinates corresponding to the population mean vector $\\mu = E[(X_i, Y_i)^T]$.\n\nLet $\\Sigma_{R\\Theta}$ be the $2 \\times 2$ covariance matrix of the limiting normal distribution for the vector $(\\sqrt{n}(R_n - r_\\mu), \\sqrt{n}(\\Theta_n - \\theta_\\mu))^T$.\n\nDerive the trace of this asymptotic covariance matrix, $\\text{Tr}(\\Sigma_{R\\Theta})$.",
            "solution": "1. Population mean μ=(E[X],E[Y])=(1/2,1/2), covariance Σ=diag(Var(X),Var(Y))=diag(1/12,1/12).  \n2. Define g(x,y)=(r,θ) with r=√(x²+y²), θ=arctan(y/x). Jacobian G has entries  \n   ∂r/∂x=x/r, ∂r/∂y=y/r, ∂θ/∂x=−y/(x²+y²), ∂θ/∂y=x/(x²+y²).  \n3. At μ: r_μ=√((½)²+(½)²)=1/√2, hence ∂r/∂x=∂r/∂y=(½)/(1/√2)=√2/2, ∂θ/∂x=−(½)/(½)=−1, ∂θ/∂y=1.  \n4. G=[ [√2/2,√2/2];[−1,1] ], so Σ_{RΘ}=GΣG^T=(1/12)GG^T.  \n5. Tr(Σ_{RΘ})=(1/12)Tr(GG^T)=(1/12)∑_{i,j}G_{ij}²  \n   =(1/12)[(√2/2)²+(√2/2)²+(-1)²+1²]=(1/12)(1/2+1/2+1+1)=(1/12)·3=1/4.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "A cornerstone of experimental science involves comparing outcomes between two groups, such as the firing probability of a neuron under a stimulus versus a control condition. This advanced practice demonstrates how the CLT and the multivariate Delta method are combined to analyze the log-odds ratio, a statistically robust metric for comparing proportions. By deriving its asymptotic variance, you build the theoretical foundation needed to perform hypothesis tests and construct confidence intervals for comparing binary outcomes in neuroscience experiments .",
            "id": "852421",
            "problem": "Consider two independent random samples, of sizes $n_1$ and $n_2$ respectively, drawn from two distinct Bernoulli populations. The first population has a success probability of $p_1$, and the second has a success probability of $p_2$. Let $X_1$ and $X_2$ be the number of successes observed in the first and second samples, respectively. The sample proportions are then given by $\\hat{p}_1 = \\frac{X_1}{n_1}$ and $\\hat{p}_2 = \\frac{X_2}{n_2}$.\n\nIn many statistical analyses, particularly in epidemiology and clinical trials, the log-odds ratio is a quantity of significant interest. The true log-odds ratio is defined as $\\theta = \\log\\left(\\frac{p_1/(1-p_1)}{p_2/(1-p_2)}\\right)$. An estimator for this quantity, based on the sample proportions, is the sample log-odds ratio:\n$$\n\\hat{\\theta} = \\log\\left(\\frac{\\hat{p}_1/(1-\\hat{p}_1)}{\\hat{p}_2/(1-\\hat{p}_2)}\\right)\n$$\nAssuming that the sample sizes $n_1$ and $n_2$ are sufficiently large, the Central Limit Theorem can be extended via the Delta Method to find the approximate distribution of $\\hat{\\theta}$.\n\nDerive the asymptotic variance of the log-odds ratio estimator, $\\text{Var}(\\hat{\\theta})$.",
            "solution": "The problem asks for the asymptotic variance of the log-odds ratio estimator $\\hat{\\theta}$. We can find this using the multivariate Delta Method.\n\nFirst, by the Central Limit Theorem, for large sample sizes $n_1$ and $n_2$, the sample proportions $\\hat{p}_1$ and $\\hat{p}_2$ are approximately normally distributed:\n$$\n\\hat{p}_1 \\approx N\\left(p_1, \\frac{p_1(1-p_1)}{n_1}\\right)\n$$\n$$\n\\hat{p}_2 \\approx N\\left(p_2, \\frac{p_2(1-p_2)}{n_2}\\right)\n$$\nSince the two samples are independent, the random variables $\\hat{p}_1$ and $\\hat{p}_2$ are also independent. Therefore, the vector of sample proportions $\\hat{\\mathbf{p}} = (\\hat{p}_1, \\hat{p}_2)^T$ has an asymptotic bivariate normal distribution with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$:\n$$\n\\boldsymbol{\\mu} = E[\\hat{\\mathbf{p}}] = \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix}\n$$\n$$\n\\boldsymbol{\\Sigma} = \\text{Cov}(\\hat{\\mathbf{p}}) = \\begin{pmatrix} \\text{Var}(\\hat{p}_1)  \\text{Cov}(\\hat{p}_1, \\hat{p}_2) \\\\ \\text{Cov}(\\hat{p}_1, \\hat{p}_2)  \\text{Var}(\\hat{p}_2) \\end{pmatrix} = \\begin{pmatrix} \\frac{p_1(1-p_1)}{n_1}  0 \\\\ 0  \\frac{p_2(1-p_2)}{n_2} \\end{pmatrix}\n$$\nThe estimator $\\hat{\\theta}$ is a function of $\\hat{p}_1$ and $\\hat{p}_2$. Let this function be $g(x, y)$:\n$$\ng(x, y) = \\log\\left(\\frac{x/(1-x)}{y/(1-y)}\\right) = \\log(x) - \\log(1-x) - \\log(y) + \\log(1-y)\n$$\nThe multivariate Delta Method states that the asymptotic variance of a function $g(\\hat{\\mathbf{p}})$ is given by:\n$$\n\\text{Var}(g(\\hat{\\mathbf{p}})) \\approx (\\nabla g(\\boldsymbol{\\mu}))^T \\boldsymbol{\\Sigma} (\\nabla g(\\boldsymbol{\\mu}))\n$$\nwhere $\\nabla g(\\boldsymbol{\\mu})$ is the gradient of $g$ evaluated at the mean vector $\\boldsymbol{\\mu} = (p_1, p_2)^T$.\n\nFirst, we compute the gradient of $g(x, y)$:\n$$\n\\frac{\\partial g}{\\partial x} = \\frac{1}{x} - \\frac{1}{1-x}(-1) = \\frac{1}{x} + \\frac{1}{1-x} = \\frac{1-x+x}{x(1-x)} = \\frac{1}{x(1-x)}\n$$\n$$\n\\frac{\\partial g}{\\partial y} = -\\frac{1}{y} + \\frac{1}{1-y}(-1) = -\\frac{1}{y} - \\frac{1}{1-y} = -\\frac{1-y+y}{y(1-y)} = -\\frac{1}{y(1-y)}\n$$\nSo the gradient vector is $\\nabla g(x,y) = \\left(\\frac{1}{x(1-x)}, -\\frac{1}{y(1-y)}\\right)^T$.\n\nNext, we evaluate the gradient at the mean $\\boldsymbol{\\mu} = (p_1, p_2)^T$:\n$$\n\\nabla g(\\boldsymbol{\\mu}) = \\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n$$\nNow, we can substitute the gradient and the covariance matrix into the Delta Method formula for the variance:\n$$\n\\text{Var}(\\hat{\\theta}) \\approx \n\\begin{pmatrix} \\frac{1}{p_1(1-p_1)}  -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n\\begin{pmatrix} \\frac{p_1(1-p_1)}{n_1}  0 \\\\ 0  \\frac{p_2(1-p_2)}{n_2} \\end{pmatrix}\n\\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n$$\nWe perform the matrix multiplication from left to right. First, multiply the row vector (the transpose of the gradient) by the covariance matrix:\n$$\n\\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\cdot \\frac{p_1(1-p_1)}{n_1} + (-\\frac{1}{p_2(1-p_2)}) \\cdot 0  \\frac{1}{p_1(1-p_1)} \\cdot 0 + (-\\frac{1}{p_2(1-p_2)}) \\cdot \\frac{p_2(1-p_2)}{n_2} \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} \\frac{1}{n_1}  -\\frac{1}{n_2} \\end{pmatrix}\n$$\nFinally, multiply this resulting row vector by the column vector (the gradient):\n$$\n\\text{Var}(\\hat{\\theta}) \\approx \\begin{pmatrix} \\frac{1}{n_1}  -\\frac{1}{n_2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n$$\n$$\n= \\left(\\frac{1}{n_1}\\right) \\left(\\frac{1}{p_1(1-p_1)}\\right) + \\left(-\\frac{1}{n_2}\\right) \\left(-\\frac{1}{p_2(1-p_2)}\\right)\n$$\n$$\n= \\frac{1}{n_1 p_1 (1-p_1)} + \\frac{1}{n_2 p_2 (1-p_2)}\n$$\nThis is the asymptotic variance of the log-odds ratio estimator.",
            "answer": "$$\n\\boxed{\\frac{1}{n_1 p_1 (1-p_1)} + \\frac{1}{n_2 p_2 (1-p_2)}}\n$$"
        }
    ]
}