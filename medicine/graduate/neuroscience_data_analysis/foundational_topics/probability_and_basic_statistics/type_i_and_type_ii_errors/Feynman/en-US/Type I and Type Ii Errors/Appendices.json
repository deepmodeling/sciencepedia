{
    "hands_on_practices": [
        {
            "introduction": "At the heart of hypothesis testing lies an inescapable trade-off between controlling false positives (Type I errors) and minimizing false negatives (Type II errors). The Neyman–Pearson lemma provides the theoretical foundation for navigating this balance by defining the most powerful test for a given significance level $\\alpha$. This foundational exercise  walks you through deriving this optimal test, forcing you to engage with the mathematical mechanics that directly link the probabilities of Type I and Type II errors, $\\alpha$ and $\\beta$, to a single decision threshold.",
            "id": "4202599",
            "problem": "In a single-trial electrophysiology experiment, a scalar feature $X$ (for example, the mean membrane potential over a fixed post-stimulus window) is modeled as Gaussian with known variance $\\sigma^{2}$ and mean $\\mu$. You wish to detect a positive shift in the mean due to a stimulus. Consider the simple hypothesis test $H_{0}: \\mu=0$ versus $H_{1}: \\mu=\\mu_{1}$ with $\\mu_{1}>0$. Use the Neyman–Pearson lemma as the fundamental base and the definition of the likelihood ratio to derive the form of the optimal Likelihood Ratio Test (LRT) that maximizes power for a fixed type I error. Then, using the standard normal cumulative distribution function $\\Phi$, express the type I error $\\alpha(\\tau)$ and type II error $\\beta(\\tau)$ as functions of a decision threshold $\\tau$ that defines the rejection region. Finally, choose the threshold that attains a pre-specified type I error level $\\alpha_{0}$ with $0<\\alpha_{0}<1$, and express the resulting type II error $\\beta$ as a closed-form analytic function of $\\alpha_{0}$, $\\mu_{1}$, and $\\sigma$. Provide your final answer as that closed-form expression for $\\beta(\\alpha_{0},\\mu_{1},\\sigma)$, with no units and no numerical rounding.",
            "solution": "The user wants to derive an expression for the type II error $\\beta$ as a function of the type I error $\\alpha_0$, the alternative mean $\\mu_1$, and the standard deviation $\\sigma$, for a hypothesis test on the mean of a Gaussian distribution. This requires applying the Neyman-Pearson lemma to find the form of the most powerful test.\n\nFirst, we validate the problem statement.\n\n### Step 1: Extract Givens\n- A scalar feature $X$ is modeled as a Gaussian random variable, $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n- The variance $\\sigma^2$ is known.\n- The hypothesis test is $H_0: \\mu=0$ versus $H_1: \\mu=\\mu_1$.\n- It is given that $\\mu_1 > 0$.\n- The derivation must be based on the Neyman-Pearson lemma and the likelihood ratio.\n- The goal is to find the optimal Likelihood Ratio Test (LRT) that maximizes power for a fixed type I error.\n- Type I error $\\alpha(\\tau)$ and type II error $\\beta(\\tau)$ must be expressed as functions of a decision threshold $\\tau$, using the standard normal cumulative distribution function ($\\Phi$).\n- A specific type I error level $\\alpha_0$ is set, where $0 < \\alpha_0 < 1$.\n- The final output is an expression for the resulting type II error $\\beta$ as a closed-form analytic function of $\\alpha_0$, $\\mu_1$, and $\\sigma$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard and fundamental problem in statistical signal detection theory, with a direct and common application in neuroscience data analysis. The premises are mathematically sound and self-consistent, and all necessary information is provided to derive a unique, meaningful solution. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe Neyman-Pearson lemma states that the most powerful test for a simple hypothesis $H_0: \\theta=\\theta_0$ versus a simple alternative $H_1: \\theta=\\theta_1$ has a rejection region $\\mathcal{R}$ defined by the likelihood ratio. Specifically, we reject $H_0$ if the likelihood ratio $\\Lambda(x) = \\frac{L(\\theta_1|x)}{L(\\theta_0|x)}$ exceeds a certain threshold $k$.\n\nIn this problem, the random variable is $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. A single observation is denoted by $x$. The likelihood function is the probability density function (PDF) of the Gaussian distribution:\n$$\nL(\\mu|x) = f(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n$$\nUnder the null hypothesis $H_0$, the mean is $\\mu=0$. The likelihood is:\n$$\nL_0 = L(\\mu=0|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)\n$$\nUnder the alternative hypothesis $H_1$, the mean is $\\mu=\\mu_1$. The likelihood is:\n$$\nL_1 = L(\\mu=\\mu_1|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu_1)^2}{2\\sigma^2}\\right)\n$$\nThe likelihood ratio $\\Lambda(x)$ is the ratio of these two likelihoods:\n$$\n\\Lambda(x) = \\frac{L_1}{L_0} = \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu_1)^2}{2\\sigma^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)} = \\exp\\left(\\frac{-(x - \\mu_1)^2 + x^2}{2\\sigma^2}\\right)\n$$\nSimplifying the exponent:\n$$\n-(x - \\mu_1)^2 + x^2 = -(x^2 - 2x\\mu_1 + \\mu_1^2) + x^2 = -x^2 + 2x\\mu_1 - \\mu_1^2 + x^2 = 2x\\mu_1 - \\mu_1^2\n$$\nSo the likelihood ratio is:\n$$\n\\Lambda(x) = \\exp\\left(\\frac{2x\\mu_1 - \\mu_1^2}{2\\sigma^2}\\right)\n$$\nThe Neyman-Pearson test rejects $H_0$ for $\\Lambda(x) > k$, where $k$ is a constant.\n$$\n\\exp\\left(\\frac{2x\\mu_1 - \\mu_1^2}{2\\sigma^2}\\right) > k\n$$\nSince the natural logarithm is a monotonically increasing function, we can take the logarithm of both sides without changing the inequality's direction:\n$$\n\\frac{2x\\mu_1 - \\mu_1^2}{2\\sigma^2} > \\ln(k)\n$$\nWe solve for $x$. Since it is given that $\\mu_1 > 0$:\n$$\n2x\\mu_1 > 2\\sigma^2 \\ln(k) + \\mu_1^2\n$$\n$$\nx > \\frac{2\\sigma^2 \\ln(k) + \\mu_1^2}{2\\mu_1}\n$$\nThe right-hand side is a constant, which we define as the decision threshold $\\tau$. Thus, the rejection region for the optimal test is $\\mathcal{R} = \\{x : x > \\tau\\}$.\n\nNext, we express the type I and type II errors as functions of this threshold $\\tau$.\nThe type I error, $\\alpha$, is the probability of rejecting $H_0$ when it is true:\n$$\n\\alpha(\\tau) = P(X > \\tau | H_0) = P(X > \\tau | \\mu=0)\n$$\nUnder $H_0$, $X \\sim \\mathcal{N}(0, \\sigma^2)$. To use the standard normal CDF $\\Phi$, we standardize the variable $X$. Let $Z = X/\\sigma$, where $Z \\sim \\mathcal{N}(0, 1)$.\n$$\n\\alpha(\\tau) = P\\left(\\frac{X}{\\sigma} > \\frac{\\tau}{\\sigma}\\right) = P\\left(Z > \\frac{\\tau}{\\sigma}\\right) = 1 - P\\left(Z \\le \\frac{\\tau}{\\sigma}\\right)\n$$\nUsing the definition of the standard normal CDF, $\\Phi(z) = P(Z \\le z)$:\n$$\n\\alpha(\\tau) = 1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\n$$\nThe type II error, $\\beta$, is the probability of failing to reject $H_0$ when $H_1$ is true. The region for failing to reject is $x \\le \\tau$.\n$$\n\\beta(\\tau) = P(X \\le \\tau | H_1) = P(X \\le \\tau | \\mu=\\mu_1)\n$$\nUnder $H_1$, $X \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$. We standardize again by letting $Z = (X-\\mu_1)/\\sigma$, where $Z \\sim \\mathcal{N}(0, 1)$.\n$$\n\\beta(\\tau) = P\\left(\\frac{X - \\mu_1}{\\sigma} \\le \\frac{\\tau - \\mu_1}{\\sigma}\\right) = P\\left(Z \\le \\frac{\\tau - \\mu_1}{\\sigma}\\right)\n$$\n$$\n\\beta(\\tau) = \\Phi\\left(\\frac{\\tau - \\mu_1}{\\sigma}\\right)\n$$\nNow, we choose the threshold $\\tau$ to achieve a pre-specified type I error level $\\alpha_0$.\n$$\n\\alpha(\\tau) = \\alpha_0 \\implies 1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) = \\alpha_0\n$$\nSolving for $\\tau$:\n$$\n\\Phi\\left(\\frac{\\tau}{\\sigma}\\right) = 1 - \\alpha_0\n$$\nApplying the inverse standard normal CDF, $\\Phi^{-1}$:\n$$\n\\frac{\\tau}{\\sigma} = \\Phi^{-1}(1 - \\alpha_0)\n$$\n$$\n\\tau = \\sigma \\Phi^{-1}(1 - \\alpha_0)\n$$\nFinally, we substitute this expression for $\\tau$ into the equation for the type II error $\\beta$ to express it as a function of $\\alpha_0$, $\\mu_1$, and $\\sigma$.\n$$\n\\beta = \\Phi\\left(\\frac{\\tau - \\mu_1}{\\sigma}\\right) = \\Phi\\left(\\frac{\\sigma \\Phi^{-1}(1 - \\alpha_0) - \\mu_1}{\\sigma}\\right)\n$$\nSimplifying the argument of $\\Phi$:\n$$\n\\beta = \\Phi\\left(\\frac{\\sigma \\Phi^{-1}(1 - \\alpha_0)}{\\sigma} - \\frac{\\mu_1}{\\sigma}\\right)\n$$\n$$\n\\beta = \\Phi\\left(\\Phi^{-1}(1 - \\alpha_0) - \\frac{\\mu_1}{\\sigma}\\right)\n$$\nThis is the desired closed-form analytic expression for the type II error.",
            "answer": "$$\n\\boxed{\\Phi\\left(\\Phi^{-1}(1 - \\alpha_{0}) - \\frac{\\mu_{1}}{\\sigma}\\right)}\n$$"
        },
        {
            "introduction": "Moving from theory to practical experimental design, one of the most critical questions a neuroscientist faces is \"How many participants do I need?\". Answering this requires a priori power analysis, a formal process to ensure a study has a high probability of detecting a scientifically meaningful effect, should one exist. This hands-on practice  guides you through a sample size calculation for a correlation study, translating the abstract concepts of significance ($\\alpha$), power ($1-\\beta$), and effect size into the concrete, resource-defining number of subjects needed for a robust experiment.",
            "id": "4202615",
            "problem": "A team of neuroscientists plans a cross-subject analysis to quantify the association between averaged hippocampal gamma-band power and delayed recall performance scores across $n$ independent participants. They will test the null hypothesis $H_{0}: \\rho = 0$ for the Pearson product-moment correlation, where $\\rho$ is the population correlation between the electrophysiological measure and behavior, using a two-sided test with significance level $\\alpha = 0.05$. The alternative hypothesis is $H_{1}: \\rho \\neq 0$, and the team considers a scientifically meaningful effect size to be $\\rho = 0.3$ under $H_{1}$. The test will be based on Fisher’s $z$-transform of the sample correlation, and the inference should be designed so that the probability of Type I error (rejecting $H_{0}$ when $H_{0}$ is true) equals $\\alpha$, and the probability of Type II error (failing to reject $H_{0}$ when $H_{1}$ is true) equals $\\beta$, with $1 - \\beta = 0.8$ as the desired power.\n\nAssume the standard large-sample approximation for Fisher’s $z$-transform: if $\\hat{r}$ is the sample correlation, then $z = \\operatorname{arctanh}(\\hat{r})$ is approximately normally distributed with mean $\\operatorname{arctanh}(\\rho)$ and variance $1/(n - 3)$ for sufficiently large $n$, where $\\operatorname{arctanh}(x) = \\frac{1}{2}\\ln\\!\\big(\\frac{1 + x}{1 - x}\\big)$. The decision rule is to reject $H_{0}$ for the two-sided test if the standardized statistic exceeds the critical value determined by $\\alpha$ under $H_{0}$.\n\nUsing only these foundations, and modeling the distribution under $H_{1}$ as a normal with nonzero mean (noncentrality), derive the power function in terms of $n$, and compute the smallest integer sample size $n$ that achieves power $1 - \\beta = 0.8$ at $\\rho = 0.3$ with $\\alpha = 0.05$. Report the final answer as the minimal integer $n$ that satisfies the power requirement. No units are needed, and no rounding to significant figures is required beyond choosing the minimal integer that meets the power target.",
            "solution": "The problem asks for the minimum sample size $n$ required to achieve a specified statistical power for a hypothesis test on a Pearson correlation coefficient. The problem formulation is statistically sound and provides all necessary parameters.\n\nThe null and alternative hypotheses for the two-sided test are:\n$$H_{0}: \\rho = 0$$\n$$H_{1}: \\rho \\neq 0$$\nwhere $\\rho$ is the population Pearson correlation coefficient.\n\nThe test is performed at a significance level of $\\alpha = 0.05$. The desired power is $1 - \\beta = 0.8$, which implies a Type II error probability of $\\beta = 0.2$, for a specific alternative hypothesis where the true correlation is $\\rho_1 = 0.3$.\n\nThe test statistic is based on Fisher's $z$-transform of the sample correlation $\\hat{r}$, defined as $z = \\operatorname{arctanh}(\\hat{r})$. The distribution of $z$ is approximated as normal with mean $\\operatorname{arctanh}(\\rho)$ and variance $\\sigma^2 = \\frac{1}{n-3}$.\n$$z \\sim \\mathcal{N}\\left(\\operatorname{arctanh}(\\rho), \\frac{1}{n-3}\\right)$$\n\nFirst, we establish the decision rule under the null hypothesis, $H_{0}: \\rho = 0$.\nUnder $H_{0}$, the mean of $z$ is $\\operatorname{arctanh}(0) = 0$. The distribution is $z \\sim \\mathcal{N}\\left(0, \\frac{1}{n-3}\\right)$. The standardized test statistic under $H_{0}$ is:\n$$Z_{stat} = \\frac{z - 0}{\\sqrt{1/(n-3)}} = \\operatorname{arctanh}(\\hat{r})\\sqrt{n-3}$$\nThis statistic $Z_{stat}$ follows a standard normal distribution, $\\mathcal{N}(0, 1)$. For a two-sided test with significance level $\\alpha = 0.05$, we reject $H_{0}$ if the absolute value of the test statistic exceeds the critical value $z_{\\alpha/2}$. The value $z_{\\alpha/2}$ is the upper quantile of the standard normal distribution such that $P(Z > z_{\\alpha/2}) = \\alpha/2$. For $\\alpha = 0.05$, we consider $\\alpha/2 = 0.025$, and the corresponding critical value is $z_{0.025}$.\nThe rejection region for the unstandardized statistic $z = \\operatorname{arctanh}(\\hat{r})$ is therefore:\n$$z < -\\frac{z_{0.025}}{\\sqrt{n-3}} \\quad \\text{or} \\quad z > \\frac{z_{0.025}}{\\sqrt{n-3}}$$\n\nNext, we derive the power function. Power is the probability of rejecting $H_{0}$ given that the alternative hypothesis $H_1$ is true. We evaluate this for the specific case where $\\rho = \\rho_1 = 0.3$.\nUnder this alternative, the distribution of $z$ is:\n$$z \\sim \\mathcal{N}\\left(\\operatorname{arctanh}(0.3), \\frac{1}{n-3}\\right)$$\nLet $\\mu_1 = \\operatorname{arctanh}(0.3)$ and the standard deviation be $\\sigma_z = \\frac{1}{\\sqrt{n-3}}$.\nThe power, as a function of $n$, is the probability that $z$ falls into the rejection region:\n$$\\text{Power}(n) = P\\left(z < -\\frac{z_{0.025}}{\\sqrt{n-3}} \\Bigg| \\rho=0.3 \\right) + P\\left(z > \\frac{z_{0.025}}{\\sqrt{n-3}} \\Bigg| \\rho=0.3 \\right)$$\nTo compute these probabilities, we standardize $z$ with respect to its distribution under $H_1$. Let $Z' = \\frac{z - \\mu_1}{\\sigma_z} = (z - \\mu_1)\\sqrt{n-3}$. $Z'$ follows a standard normal distribution $\\mathcal{N}(0,1)$.\n$$\\text{Power}(n) = P\\left(Z' < \\frac{-z_{0.025}/\\sqrt{n-3} - \\mu_1}{1/\\sqrt{n-3}}\\right) + P\\left(Z' > \\frac{z_{0.025}/\\sqrt{n-3} - \\mu_1}{1/\\sqrt{n-3}}\\right)$$\n$$\\text{Power}(n) = P\\left(Z' < -z_{0.025} - \\mu_1\\sqrt{n-3}\\right) + P\\left(Z' > z_{0.025} - \\mu_1\\sqrt{n-3}\\right)$$\nUsing the cumulative distribution function $\\Phi(\\cdot)$ of the standard normal distribution, the power function is:\n$$\\text{Power}(n) = \\Phi(-z_{0.025} - \\mu_1\\sqrt{n-3}) + \\left[1 - \\Phi(z_{0.025} - \\mu_1\\sqrt{n-3})\\right]$$\nSince we are testing for an effect $\\rho_1 = 0.3 > 0$, the sample correlations $\\hat{r}$ are expected to be positive, and thus the transformed values $z$ will be centered at the positive value $\\mu_1 = \\operatorname{arctanh}(0.3)$. Consequently, the first term $\\Phi(-z_{0.025} - \\mu_1\\sqrt{n-3})$ is the probability of observing an extremely large negative result, which is negligible. The power is dominated by the second term.\nTherefore, we can approximate the power as:\n$$\\text{Power}(n) \\approx 1 - \\Phi(z_{0.025} - \\mu_1\\sqrt{n-3})$$\nWe require the power to be at least $1-\\beta = 0.8$.\n$$0.8 \\le 1 - \\Phi(z_{0.025} - \\mu_1\\sqrt{n-3})$$\n$$\\Phi(z_{0.025} - \\mu_1\\sqrt{n-3}) \\le 0.2$$\nApplying the inverse CDF, $\\Phi^{-1}$, to both sides of the inequality:\n$$z_{0.025} - \\mu_1\\sqrt{n-3} \\le \\Phi^{-1}(0.2)$$\nLet $z_{\\beta}$ be the upper $\\beta$-quantile, such that $P(Z' > z_{\\beta}) = \\beta$. For $\\beta=0.2$, we have $P(Z' \\le -z_{0.2}) = \\Phi(-z_{0.2}) = 0.2$, so $\\Phi^{-1}(0.2) = -z_{0.2}$.\nThe inequality becomes:\n$$z_{0.025} - \\mu_1\\sqrt{n-3} \\le -z_{0.2}$$\nRearranging to solve for $n$:\n$$\\mu_1\\sqrt{n-3} \\ge z_{0.025} + z_{0.2}$$\n$$\\sqrt{n-3} \\ge \\frac{z_{0.025} + z_{0.2}}{\\mu_1}$$\n$$n \\ge \\left(\\frac{z_{0.025} + z_{0.2}}{\\operatorname{arctanh}(0.3)}\\right)^2 + 3$$\nNow, we substitute the numerical values.\nThe critical value $z_{0.025}$ corresponds to the upper $2.5\\%$ tail of the standard normal distribution, which is approximately $1.95996$.\nThe critical value $z_{0.2}$ corresponds to the upper $20\\%$ tail, which is approximately $0.84162$.\nThe value for $\\operatorname{arctanh}(0.3)$ is:\n$$\\operatorname{arctanh}(0.3) = \\frac{1}{2}\\ln\\left(\\frac{1+0.3}{1-0.3}\\right) = \\frac{1}{2}\\ln\\left(\\frac{1.3}{0.7}\\right) \\approx 0.30952$$\nSubstituting these values into the inequality for $n$:\n$$n \\ge \\left(\\frac{1.95996 + 0.84162}{0.30952}\\right)^2 + 3$$\n$$n \\ge \\left(\\frac{2.80158}{0.30952}\\right)^2 + 3$$\n$$n \\ge (9.0514)^2 + 3$$\n$$n \\ge 81.928 + 3$$\n$$n \\ge 84.928$$\nSince the sample size $n$ must be an integer, and power increases with $n$, we must choose the smallest integer that satisfies this condition. Therefore, we round up to the next integer.\n$$n = 85$$\nThe smallest integer sample size required to achieve the desired power is $85$.",
            "answer": "$$\\boxed{85}$$"
        },
        {
            "introduction": "Once an experiment is complete and a statistically significant result is found, what is the probability that it reflects a true underlying effect? This question is not answered by the p-value alone; it requires understanding the Positive Predictive Value (PPV) of a test. This exercise  demonstrates how to derive the PPV by combining a test's error rates, $\\alpha$ and $\\beta$, with the prior probability of the hypothesis being true. This is a crucial skill for critically evaluating research findings, revealing how the baseline prevalence of true effects in a field can dramatically influence the credibility of a 'significant' result.",
            "id": "4202612",
            "problem": "A systems neuroscience laboratory screens a large set of candidate synaptic biomarkers for true condition-specific modulation. For each biomarker, the laboratory conducts a hypothesis test at significance level $\\alpha$ and reports a result as \"significant\" if the computed $p$-value is at most $\\alpha$. Let the prior probability that a randomly chosen biomarker truly has a nonzero effect be $\\pi_{1}$, and denote the prior probability of no true effect by $\\pi_{0}=1-\\pi_{1}$. The statistical test has power $1-\\beta$ for detecting a true effect and a Type I error rate $\\alpha$ for falsely declaring significance when no true effect exists.\n\nUsing only the core definitions of Type I error, Type II error, and the laws of conditional probability together with the law of total probability, derive the Positive Predictive Value (PPV), defined as the posterior probability that a randomly selected significant finding corresponds to a true effect, that is, the quantity $\\Pr(\\text{true effect} \\mid \\text{significant})$. Express your final result as a closed-form analytic expression in terms of $\\pi_{1}$, $\\alpha$, and $\\beta$.\n\nFinally, interpret your derived expression in the context of low-prevalence neuroscience hypotheses (that is, small $\\pi_{1}$), explaining what the formula implies for the credibility of significant findings in such settings. No numerical evaluation is required. Your final answer must be the single closed-form expression for $PPV$ and must not include any units.",
            "solution": "The problem statement will first be validated for scientific soundness, self-consistency, and clarity before a solution is attempted.\n\n### Step 1: Extract Givens\n- Significance level of the hypothesis test: $\\alpha$.\n- A result is \"significant\" if the $p$-value is at most $\\alpha$.\n- Prior probability of a true nonzero effect: $\\pi_{1}$.\n- Prior probability of no true effect: $\\pi_{0} = 1 - \\pi_{1}$.\n- Statistical power for detecting a true effect: $1 - \\beta$.\n- Type I error rate for falsely declaring significance: $\\alpha$.\n- The quantity to be derived is the Positive Predictive Value (PPV), defined as $\\Pr(\\text{true effect} \\mid \\text{significant})$.\n- The final expression should be in terms of $\\pi_{1}$, $\\alpha$, and $\\beta$.\n- An interpretation of the result in the context of small $\\pi_{1}$ is required.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It poses a standard question in Bayesian inference applied to frequentist hypothesis testing outcomes. The scenario of screening for biomarkers is a common application where these concepts are critical. All terms such as significance level ($\\alpha$), Type I error, power ($1-\\beta$), Type II error ($\\beta$), and prior probability ($\\pi_1$) are standard in statistics and are used consistently within the problem statement. The problem is well-posed, as it provides all necessary information to derive a unique analytical expression for the requested quantity, the Positive Predictive Value (PPV). There are no contradictions, ambiguities, or factual inaccuracies.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full, reasoned solution will now be provided.\n\n### Solution Derivation\n\nLet us define the relevant events:\n- Let $H_{1}$ be the event that a randomly chosen biomarker corresponds to a \"true effect\" (i.e., the null hypothesis of no effect is false).\n- Let $H_{0}$ be the event that the biomarker corresponds to \"no true effect\" (i.e., the null hypothesis is true).\n- Let $S$ be the event that the statistical test for the biomarker yields a \"significant\" result.\n\nFrom the problem statement, we can formalize the given probabilities in terms of these events:\n- The prior probability of a true effect is $\\Pr(H_{1}) = \\pi_{1}$.\n- The prior probability of no true effect is $\\Pr(H_{0}) = 1 - \\pi_{1} = \\pi_{0}$.\n- The Type I error rate, $\\alpha$, is the probability of finding a significant result when there is no true effect. This is the conditional probability $\\Pr(S \\mid H_{0}) = \\alpha$.\n- The statistical power, $1-\\beta$, is the probability of finding a significant result when there is a true effect. This is the conditional probability $\\Pr(S \\mid H_{1}) = 1-\\beta$. The quantity $\\beta$ is the Type II error rate, $\\Pr(\\text{not } S \\mid H_{1})$.\n\nWe are asked to derive the Positive Predictive Value (PPV), which is defined as the posterior probability that a significant finding corresponds to a true effect. In our notation, this is $\\Pr(H_{1} \\mid S)$.\n\nWe will use the definition of conditional probability, which is the foundation of Bayes' theorem:\n$$\n\\Pr(H_{1} \\mid S) = \\frac{\\Pr(H_{1} \\cap S)}{\\Pr(S)}\n$$\nThe numerator, $\\Pr(H_{1} \\cap S)$, can be rewritten using the definition of conditional probability again: $\\Pr(H_{1} \\cap S) = \\Pr(S \\mid H_{1}) \\Pr(H_{1})$.\n\nThe denominator, $\\Pr(S)$, is the total probability of obtaining a significant result. We can calculate this using the law of total probability, by partitioning the event space based on whether a true effect exists ($H_{1}$) or not ($H_{0}$):\n$$\n\\Pr(S) = \\Pr(S \\mid H_{1})\\Pr(H_{1}) + \\Pr(S \\mid H_{0})\\Pr(H_{0})\n$$\nNow, we substitute the given symbolic values into these expressions.\n\nFor the numerator:\n$$\n\\Pr(H_{1} \\cap S) = \\Pr(S \\mid H_{1})\\Pr(H_{1}) = (1 - \\beta)\\pi_{1}\n$$\n\nFor the denominator:\n$$\n\\Pr(S) = \\Pr(S \\mid H_{1})\\Pr(H_{1}) + \\Pr(S \\mid H_{0})\\Pr(H_{0}) = (1 - \\beta)\\pi_{1} + \\alpha\\pi_{0}\n$$\nSince $\\pi_{0} = 1 - \\pi_{1}$, we have:\n$$\n\\Pr(S) = (1 - \\beta)\\pi_{1} + \\alpha(1 - \\pi_{1})\n$$\n\nCombining these parts, we obtain the expression for the PPV:\n$$\n\\text{PPV} = \\Pr(H_{1} \\mid S) = \\frac{\\Pr(S \\mid H_{1})\\Pr(H_{1})}{\\Pr(S \\mid H_{1})\\Pr(H_{1}) + \\Pr(S \\mid H_{0})\\Pr(H_{0})} = \\frac{(1 - \\beta)\\pi_{1}}{(1 - \\beta)\\pi_{1} + \\alpha(1 - \\pi_{1})}\n$$\nThis is the required closed-form analytic expression for the Positive Predictive Value.\n\n### Interpretation\n\nThe derived expression for the PPV is:\n$$\n\\text{PPV} = \\frac{(1 - \\beta)\\pi_{1}}{(1 - \\beta)\\pi_{1} + \\alpha(1 - \\pi_{1})}\n$$\nIn the context of low-prevalence neuroscience hypotheses, the prior probability of a true effect, $\\pi_{1}$, is very small (i.e., $\\pi_{1} \\ll 1$). Consequently, $1 - \\pi_{1}$ is approximately equal to $1$.\n\nLet us analyze the behavior of the PPV expression as $\\pi_{1} \\to 0$.\nThe numerator, $(1 - \\beta)\\pi_{1}$, is directly proportional to $\\pi_{1}$ and thus approaches $0$.\nThe denominator consists of two terms. The first term, $(1 - \\beta)\\pi_{1}$, also approaches $0$. The second term, $\\alpha(1 - \\pi_{1})$, approaches $\\alpha(1 - 0) = \\alpha$.\nTherefore, for very small $\\pi_{1}$, the expression for PPV can be approximated as:\n$$\n\\text{PPV} \\approx \\frac{(1 - \\beta)\\pi_{1}}{\\alpha}\n$$\nThis approximation reveals a crucial insight. Even if the statistical power ($1-\\beta$) is high (e.g., $0.8$ or $0.9$) and the significance level $\\alpha$ is set to a conventionally stringent value (e.g., $0.05$), the PPV is scaled by the low prevalence $\\pi_{1}$. If $\\pi_{1}$ is sufficiently small, the PPV can be very low. For example, if $\\pi_1$ is on the same order of magnitude as $\\alpha$, the PPV will be far from $1$. If $\\pi_{1} < \\alpha$, the PPV will be less than $1-\\beta$, often substantially so.\n\nThis implies that in research domains where true hypotheses are rare (e.g., large-scale exploratory screens for genetic markers, pharmacological agents, or synaptic biomarkers), a \"significant\" result from a single study is not, by itself, strong evidence for a true effect. The majority of such significant findings may be false positives. The formula demonstrates mathematically why the base rate of true effects is a critical factor in determining the post-test credibility of a research finding, and why findings from low-probability hypotheses require exceptionally strong evidence (e.g., extremely small $\\alpha$ values) or replication to be considered credible.",
            "answer": "$$\n\\boxed{\\frac{(1 - \\beta)\\pi_{1}}{(1 - \\beta)\\pi_{1} + \\alpha(1 - \\pi_{1})}}\n$$"
        }
    ]
}