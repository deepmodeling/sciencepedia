{
    "hands_on_practices": [
        {
            "introduction": "When comparing outcomes across multiple groups, a significant result from an Analysis of Variance (ANOVA) is only the first step; it tells us that at least one group differs from the others, but not by how much. To understand the practical importance of the group differences, we must quantify the proportion of the total outcome variability that can be attributed to group membership. This exercise  walks you through the fundamental calculation of eta-squared ($\\eta^2$), the effect size for ANOVA, by directly partitioning the total sum of squares into its constituent parts.",
            "id": "4909836",
            "problem": "A clinical trial investigates how three dietary interventions affect changes in low-density lipoprotein cholesterol (LDL-C; measured in $\\mathrm{mg/dL}$) after $12$ weeks. Participants are randomized into $G=3$ groups with equal allocation, and the total sample size is $N=60$. Let the group sample sizes be $n_{1}=n_{2}=n_{3}=20$. The sample mean LDL-C reductions for the groups are $\\bar{y}_{1}=47$, $\\bar{y}_{2}=50$, and $\\bar{y}_{3}=53$ (all in $\\mathrm{mg/dL}$). The within-group sums of squared deviations from their respective group means are $W_{1}=280$, $W_{2}=280$, and $W_{3}=280$ (all in $(\\mathrm{mg/dL})^{2}$). The grand mean reduction is $\\bar{y}=50$ (in $\\mathrm{mg/dL}$). The investigators report the total sum of squares $SS_{T}=1200$ (in $(\\mathrm{mg/dL})^{2}$).\n\nUsing only the core definitions of Analysis of Variance (ANOVA), namely the definitions of the total sum of squares, the between-group sum of squares, and the within-group sum of squares in terms of deviations from the grand mean and the group means, perform the following:\n\n- Compute the between-group sum of squares $SS_{B}$ from the given group means and sizes.\n- Compute the within-group sum of squares $SS_{W}$ from the given within-group sums $W_{g}$, confirm its value, and explicitly verify the variance decomposition by showing how $SS_{T}$ is partitioned.\n- Conclude the proportion of explained variance attributable to group membership, expressed as a decimal between $0$ and $1$.\n\nProvide your final answer as a single real number equal to the proportion of explained variance. No rounding is required.",
            "solution": "The problem is first assessed for validity. All provided data are self-contained, scientifically grounded in the principles of biostatistics (specifically Analysis of Variance), and internally consistent. The number of groups is $G=3$, with equal sample sizes $n_{1}=n_{2}=n_{3}=20$, resulting in a total sample size $N = n_{1}+n_{2}+n_{3} = 20+20+20=60$, which matches the given $N=60$. The grand mean $\\bar{y}=50$ is also consistent with the group means and sizes: $\\bar{y} = \\frac{\\sum_{g=1}^{G} n_{g}\\bar{y}_{g}}{N} = \\frac{20(47) + 20(50) + 20(53)}{60} = \\frac{20(47+50+53)}{60} = \\frac{150}{3} = 50$. The problem is well-posed and objective. Thus, a solution may be derived.\n\nThe problem requires the computation of several quantities based on the core definitions of ANOVA.\n\nFirst, we compute the between-group sum of squares, $SS_{B}$. This quantity measures the variation among the group means. Its definition is the sum of the squared differences between each group mean and the grand mean, weighted by the respective group sample size.\nThe formula for $SS_{B}$ is:\n$$SS_{B} = \\sum_{g=1}^{G} n_{g} (\\bar{y}_{g} - \\bar{y})^{2}$$\nSubstituting the given values: $G=3$, $n_{1}=n_{2}=n_{3}=20$, $\\bar{y}_{1}=47$, $\\bar{y}_{2}=50$, $\\bar{y}_{3}=53$, and the grand mean $\\bar{y}=50$.\n$$SS_{B} = n_{1}(\\bar{y}_{1} - \\bar{y})^{2} + n_{2}(\\bar{y}_{2} - \\bar{y})^{2} + n_{3}(\\bar{y}_{3} - \\bar{y})^{2}$$\n$$SS_{B} = 20(47 - 50)^{2} + 20(50 - 50)^{2} + 20(53 - 50)^{2}$$\n$$SS_{B} = 20(-3)^{2} + 20(0)^{2} + 20(3)^{2}$$\n$$SS_{B} = 20(9) + 0 + 20(9)$$\n$$SS_{B} = 180 + 180 = 360$$\n\nNext, we compute the within-group sum of squares, $SS_{W}$. This quantity measures the variation of individual observations within their respective groups. It is defined as the sum of the squared deviations of each observation from its group's mean. The problem provides the within-group sums of squared deviations for each group, $W_{g}$, where $W_{g} = \\sum_{i=1}^{n_{g}} (y_{gi} - \\bar{y}_{g})^{2}$. The total within-group sum of squares is the sum of these values across all groups.\n$$SS_{W} = \\sum_{g=1}^{G} W_{g}$$\nUsing the given values $W_{1}=280$, $W_{2}=280$, and $W_{3}=280$:\n$$SS_{W} = W_{1} + W_{2} + W_{3} = 280 + 280 + 280 = 3 \\times 280 = 840$$\n\nThe problem asks to explicitly verify the variance decomposition. The fundamental principle of ANOVA is that the total sum of squares, $SS_{T}$, is partitioned into the between-group sum of squares and the within-group sum of squares:\n$$SS_{T} = SS_{B} + SS_{W}$$\nUsing our computed values for $SS_{B}$ and $SS_{W}$:\n$$SS_{B} + SS_{W} = 360 + 840 = 1200$$\nThe problem statement provides that $SS_{T} = 1200$. Since our calculated sum $SS_{B} + SS_{W}$ equals the given $SS_{T}$, the decomposition is verified, confirming the consistency of all provided information.\n\nFinally, we are asked to find the proportion of explained variance attributable to group membership. This is a measure of effect size known as eta-squared, denoted by $\\eta^{2}$. It is defined as the ratio of the between-group sum of squares to the total sum of squares.\n$$\\eta^{2} = \\frac{SS_{B}}{SS_{T}}$$\nSubstituting the values for $SS_{B}$ and $SS_{T}$:\n$$\\eta^{2} = \\frac{360}{1200}$$\nSimplifying the fraction gives the proportion as a decimal:\n$$\\eta^{2} = \\frac{36}{120} = \\frac{3}{10} = 0.3$$\nThus, $30\\%$ of the total variance in LDL-C reduction is attributable to the differences between the three dietary interventions.",
            "answer": "$$\\boxed{0.3}$$"
        },
        {
            "introduction": "Many neuroscience experiments yield binary outcomes, such as the presence or absence of a seizure or a correct versus incorrect response. In these scenarios, we use effect sizes like the Risk Ratio (RR) and Odds Ratio (OR) to quantify how an intervention changes the likelihood of the event. This practice  guides you through constructing confidence intervals for these crucial measures, introducing the standard and robust technique of performing calculations on the logarithmic scale to stabilize variance and ensure the final interval is well-behaved.",
            "id": "4158376",
            "problem": "A multicenter randomized trial in systems neuroscience evaluates whether closed-loop hippocampal deep brain stimulation (DBS) reduces the probability of experiencing at least one clinically verified seizure during a fixed follow-up period compared to sham stimulation. Patients are randomized independently, and outcomes are assessed by blinded raters. In the DBS arm, $120$ patients are followed; $30$ experience at least one seizure and $90$ do not. In the sham arm, $120$ patients are followed; $50$ experience at least one seizure and $70$ do not. Assume independent binomial sampling across arms and within-arm independence across patients.\n\nStarting only from the core definitions of risk, odds, risk ratio, and odds ratio, and from large-sample normal approximations justified by the Central Limit Theorem and the delta method, construct two-sided confidence intervals at confidence level $0.95$ for:\n- the risk ratio (RR), defined as the ratio of the seizure risk in the DBS arm over the seizure risk in the sham arm, and\n- the odds ratio (OR), defined as the ratio of the seizure odds in the DBS arm over the seizure odds in the sham arm.\n\nWork on the logarithmic scale for the effect size, assume asymptotic normality on that scale, and then back-transform to the natural scale to obtain interpretable intervals. Do not apply any continuity corrections. Round each endpoint of each interval to three significant figures. Express the final answer as a row matrix containing, in order, the lower and upper endpoints for the risk ratio, followed by the lower and upper endpoints for the odds ratio. The final answer is unitless.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains a complete and consistent set of data and instructions for a standard biostatistical analysis.\n\nLet the DBS arm be denoted by subscript $1$ and the sham arm by subscript $2$. The data provided can be summarized as follows:\nFor the DBS arm, the number of patients is $n_1 = 120$, with $x_1 = 30$ experiencing at least one seizure and $y_1 = n_1 - x_1 = 90$ not experiencing a seizure.\nFor the sham arm, the number of patients is $n_2 = 120$, with $x_2 = 50$ experiencing at least one seizure and $y_2 = n_2 - x_2 = 70$ not experiencing a seizure.\nThe problem assumes independent binomial sampling for the number of events $X_1$ and $X_2$ in each arm, i.e., $X_i \\sim \\text{Binomial}(n_i, p_i)$, where $p_i$ is the true seizure risk in arm $i$.\nWe are asked to construct two-sided confidence intervals at a confidence level of $1-\\alpha = 0.95$. This corresponds to $\\alpha = 0.05$ and a critical value from the standard normal distribution of $z_{1-\\alpha/2} = z_{0.975} \\approx 1.96$.\n\nFirst, we will construct the confidence interval for the risk ratio (RR).\nThe risk of seizure in arm $i$ is $p_i$. The sample estimate of the risk is the proportion $\\hat{p}_i = \\frac{x_i}{n_i}$.\nFor the DBS arm, the estimated risk is $\\hat{p}_1 = \\frac{30}{120} = 0.25$.\nFor the sham arm, the estimated risk is $\\hat{p}_2 = \\frac{50}{120} = \\frac{5}{12}$.\nThe risk ratio is defined as $RR = \\frac{p_1}{p_2}$. The point estimate is $\\widehat{RR} = \\frac{\\hat{p}_1}{\\hat{p}_2} = \\frac{0.25}{5/12} = 0.25 \\times \\frac{12}{5} = 0.6$.\n\nTo construct the confidence interval, we work on the logarithmic scale. The parameter of interest is $\\ln(RR) = \\ln(p_1) - \\ln(p_2)$. Its point estimate is $\\ln(\\widehat{RR}) = \\ln(0.6)$.\nThe variance of $\\ln(\\widehat{RR})$ is found using the delta method. For a large sample size $n_i$, the Central Limit Theorem states that $\\hat{p}_i$ is approximately normally distributed with mean $p_i$ and variance $\\text{Var}(\\hat{p}_i) = \\frac{p_i(1-p_i)}{n_i}$.\nLet $g(p) = \\ln(p)$. The derivative is $g'(p) = \\frac{1}{p}$. By the delta method, the variance of the transformed variable is $\\text{Var}(\\ln(\\hat{p}_i)) \\approx [g'(p_i)]^2 \\text{Var}(\\hat{p}_i) = (\\frac{1}{p_i})^2 \\frac{p_i(1-p_i)}{n_i} = \\frac{1-p_i}{n_i p_i}$.\nSince the two arms are independent, the variance of the difference of the log-risks is the sum of their variances: $\\text{Var}(\\ln(\\widehat{RR})) = \\text{Var}(\\ln(\\hat{p}_1)) + \\text{Var}(\\ln(\\hat{p}_2))$.\nThe standard error of $\\ln(\\widehat{RR})$, denoted $SE(\\ln(\\widehat{RR}))$, is the square root of the estimated variance, where we substitute the sample estimates $\\hat{p}_i$ for the true parameters $p_i$:\n$$SE(\\ln(\\widehat{RR})) = \\sqrt{\\frac{1-\\hat{p}_1}{n_1 \\hat{p}_1} + \\frac{1-\\hat{p}_2}{n_2 \\hat{p}_2}}$$\nSubstituting the given values:\n$$SE(\\ln(\\widehat{RR})) = \\sqrt{\\frac{1-0.25}{120 \\times 0.25} + \\frac{1 - 5/12}{120 \\times 5/12}} = \\sqrt{\\frac{0.75}{30} + \\frac{7/12}{50}} = \\sqrt{\\frac{1}{40} + \\frac{7}{600}} = \\sqrt{\\frac{15}{600} + \\frac{7}{600}} = \\sqrt{\\frac{22}{600}} = \\sqrt{\\frac{11}{300}}$$\nNumerically, $SE(\\ln(\\widehat{RR})) \\approx 0.191485$.\nThe $95\\%$ confidence interval for $\\ln(RR)$ is given by $\\ln(\\widehat{RR}) \\pm z_{0.975} \\times SE(\\ln(\\widehat{RR}))$.\n$\\ln(0.6) \\approx -0.510826$.\nThe margin of error is $1.96 \\times \\sqrt{\\frac{11}{300}} \\approx 1.96 \\times 0.191485 \\approx 0.375311$.\nThe confidence interval for $\\ln(RR)$ is $-0.510826 \\pm 0.375311$, which is $[-0.886137, -0.135515]$.\nTo obtain the confidence interval for the risk ratio $RR$, we back-transform by exponentiating the endpoints:\nLower endpoint: $\\exp(-0.886137) \\approx 0.41224$.\nUpper endpoint: $\\exp(-0.135515) \\approx 0.87326$.\nRounding to three significant figures, the $95\\%$ confidence interval for the risk ratio is $[0.412, 0.873]$.\n\nNext, we construct the confidence interval for the odds ratio (OR).\nThe odds of seizure in arm $i$ is $o_i = \\frac{p_i}{1-p_i}$. The sample estimate of the odds is $\\hat{o}_i = \\frac{\\hat{p}_i}{1-\\hat{p}_i} = \\frac{x_i/n_i}{y_i/n_i} = \\frac{x_i}{y_i}$.\nFor the DBS arm, the estimated odds are $\\hat{o}_1 = \\frac{30}{90} = \\frac{1}{3}$.\nFor the sham arm, the estimated odds are $\\hat{o}_2 = \\frac{50}{70} = \\frac{5}{7}$.\nThe odds ratio is defined as $OR = \\frac{o_1}{o_2}$. The point estimate is $\\widehat{OR} = \\frac{\\hat{o}_1}{\\hat{o}_2} = \\frac{1/3}{5/7} = \\frac{7}{15}$.\n\nWe again work on the logarithmic scale. The parameter is $\\ln(OR) = \\ln(o_1) - \\ln(o_2)$, with point estimate $\\ln(\\widehat{OR}) = \\ln(7/15)$.\nTo find the variance, we use the delta method. Let $h(p) = \\ln(\\frac{p}{1-p}) = \\ln(p) - \\ln(1-p)$. The derivative is $h'(p) = \\frac{1}{p} + \\frac{1}{1-p} = \\frac{1}{p(1-p)}$.\nThe variance is $\\text{Var}(\\ln(\\hat{o}_i)) \\approx [h'(p_i)]^2 \\text{Var}(\\hat{p}_i) = \\left(\\frac{1}{p_i(1-p_i)}\\right)^2 \\frac{p_i(1-p_i)}{n_i} = \\frac{1}{n_i p_i(1-p_i)}$.\nThe estimated variance is obtained by plugging in sample estimates: $\\widehat{\\text{Var}}(\\ln(\\hat{o}_i)) = \\frac{1}{n_i \\hat{p}_i(1-\\hat{p}_i)} = \\frac{1}{n_i(x_i/n_i)(y_i/n_i)} = \\frac{n_i}{x_i y_i} = \\frac{x_i+y_i}{x_i y_i} = \\frac{1}{y_i} + \\frac{1}{x_i}$.\nThe standard error of $\\ln(\\widehat{OR})$ is the square root of the sum of the estimated variances for each arm:\n$$SE(\\ln(\\widehat{OR})) = \\sqrt{\\left(\\frac{1}{x_1} + \\frac{1}{y_1}\\right) + \\left(\\frac{1}{x_2} + \\frac{1}{y_2}\\right)}$$\nSubstituting the data:\n$$SE(\\ln(\\widehat{OR})) = \\sqrt{\\frac{1}{30} + \\frac{1}{90} + \\frac{1}{50} + \\frac{1}{70}}$$\nTo sum the fractions, we find a common denominator. The least common multiple of $30$, $90$, $50$, and $70$ is $3150$.\n$$SE(\\ln(\\widehat{OR})) = \\sqrt{\\frac{105}{3150} + \\frac{35}{3150} + \\frac{63}{3150} + \\frac{45}{3150}} = \\sqrt{\\frac{248}{3150}} = \\sqrt{\\frac{124}{1575}}$$\nNumerically, $SE(\\ln(\\widehat{OR})) \\approx 0.280589$.\nThe $95\\%$ confidence interval for $\\ln(OR)$ is $\\ln(\\widehat{OR}) \\pm z_{0.975} \\times SE(\\ln(\\widehat{OR}))$.\n$\\ln(7/15) \\approx -0.762140$.\nThe margin of error is $1.96 \\times \\sqrt{\\frac{124}{1575}} \\approx 1.96 \\times 0.280589 \\approx 0.549954$.\nThe confidence interval for $\\ln(OR)$ is $-0.762140 \\pm 0.549954$, which is $[-1.312094, -0.212186]$.\nTo obtain the confidence interval for the odds ratio $OR$, we back-transform by exponentiating the endpoints:\nLower endpoint: $\\exp(-1.312094) \\approx 0.26927$.\nUpper endpoint: $\\exp(-0.212186) \\approx 0.80881$.\nRounding to three significant figures, the $95\\%$ confidence interval for the odds ratio is $[0.269, 0.809]$.\n\nThe final result is a row matrix of the four endpoints in the specified order.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.412  0.873  0.269  0.809\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Real-world neuroscience data, such as reaction times, rarely conform to the idealized assumption of a normal distribution; they are often right-skewed with heavy tails. Standardized mean differences like Cohen’s $d$ are sensitive to this skewness and to outliers, which can lead to misleading conclusions. This exercise  challenges you to think critically about estimator choice by contrasting a classic parametric effect size with robust, rank-based alternatives like Cliff’s $\\delta$ and the Area Under the Curve (AUC), which are invariant to monotonic data transformations.",
            "id": "4158355",
            "problem": "In a speeded visual discrimination experiment, reaction time (RT) is recorded for two groups, an auditory distraction condition and a quiet control condition. Let the RT distributions be represented by the random variables $X$ (distraction) and $Y$ (control). Empirically, both $X$ and $Y$ are continuous, unimodal, right-skewed, and heavy-tailed, with few ties due to millisecond-resolution timing. Consider three effect size measures that are commonly used in neuroscience data analysis: the standardized mean difference known as Cohen’s $d$, Cliff’s $\\delta$ (a nonparametric measure of stochastic dominance), and the Area Under the Receiver Operating Characteristic Curve (AUC) for distinguishing $X$ from $Y$.\n\nYou are asked to compare the sensitivity of these measures to skewness in RT distributions and propose appropriate diagnostic checks. Your reasoning must start from core definitions of the sample mean, sample variance, pairwise ordering probabilities, and invariance of order under strictly increasing transformations. Suppose the data exhibit the following realistic features: sample sizes $n_X = 120$, $n_Y = 130$; sample medians near $350\\,\\mathrm{ms}$ for $Y$ and $370\\,\\mathrm{ms}$ for $X$; sample interquartile ranges indicating wider dispersion for $X$; and a non-negligible proportion of long-latency responses above $2\\,\\mathrm{s}$ in both groups but more frequent in $X$.\n\nWhich option best characterizes the comparative sensitivity of $d$, $\\delta$, and AUC to skewness in RT and specifies scientifically appropriate diagnostic checks to assess whether skewness materially biases each measure?\n\nA. Cohen’s $d$ is more sensitive to skewness and variance inequality because it uses means and a pooled standard deviation; Cliff’s $\\delta$ and AUC depend only on ranks or pairwise orderings and are invariant to strictly increasing transformations, so they are comparatively robust to skewness. Diagnostics should include inspecting sample skewness $\\,\\hat{\\gamma}_1\\,$ and two-sample quantile/shift functions, testing equality of variances (e.g., Levene or Brown–Forsythe), checking tie rates, comparing $d$ on raw versus log-transformed RT to gauge sensitivity, and computing $\\delta$ or AUC as complementary robustness checks.\n\nB. Cohen’s $d$, Cliff’s $\\delta$, and AUC are equally sensitive to skewness because all three summarize the same central tendency. Diagnostics should focus on normality tests (e.g., Shapiro–Wilk) only; monotone transformations will leave Cohen’s $d$ unchanged, so comparing raw and log-transformed RT is unnecessary.\n\nC. Cliff’s $\\delta$ is more sensitive to skewness than Cohen’s $d$ because it overweights the tails; AUC is unaffected by strictly increasing transformations but is biased by skewness. Diagnostics should rely on Pearson correlation between RT and group labels, plus residual plots from linear models.\n\nD. Cohen’s $d$ is invariant to strictly increasing transformations and therefore robust to skewness; Cliff’s $\\delta$ and AUC depend on the exact marginal distributions and will be distorted by skewness. Diagnostics should be limited to histogram inspection without formal variance checks or quantile-based analyses.",
            "solution": "The user wants a detailed analysis and solution to a problem in neuroscience data analysis, focusing on the properties of different effect size measures. As a STEM professor, I will first validate the problem statement and then proceed to a rigorous, principle-based solution.\n\n### Step 1: Extract Givens\n\n-   **Experiment**: Speeded visual discrimination.\n-   **Measurement**: Reaction time (RT).\n-   **Groups**: Auditory distraction vs. quiet control.\n-   **Random Variables**: $X$ for distraction RT, $Y$ for control RT.\n-   **Distributional Properties**: Both $X$ and $Y$ are continuous, unimodal, right-skewed, and heavy-tailed.\n-   **Ties**: Few ties in the data.\n-   **Effect Size Measures**:\n    1.  Cohen’s $d$ (standardized mean difference).\n    2.  Cliff’s $\\delta$ (nonparametric measure of stochastic dominance).\n    3.  Area Under the Receiver Operating Characteristic Curve (AUC).\n-   **Task**: Compare the sensitivity of $d$, $\\delta$, and AUC to skewness and propose appropriate diagnostic checks.\n-   **Illustrative Data Features**:\n    -   Sample sizes: $n_X = 120$, $n_Y = 130$.\n    -   Sample medians: $\\approx 350\\,\\mathrm{ms}$ for $Y$, $\\approx 370\\,\\mathrm{ms}$ for $X$.\n    -   Dispersion: Sample interquartile ranges suggest wider dispersion for $X$.\n    -   Tails: Long-latency responses ($ 2\\,\\mathrm{s}$) are present, more frequent in group $X$.\n-   **Required Foundational Concepts**: Sample mean, sample variance, pairwise ordering probabilities, invariance under strictly increasing transformations.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is evaluated against the validation criteria.\n\n-   **Scientifically Grounded**: The problem is well-grounded in statistics and cognitive neuroscience. Reaction time distributions are empirically known to be right-skewed and often possess heavy tails (e.g., fit by ex-Gaussian or log-normal distributions). The comparison of parametric (Cohen's $d$) and non-parametric/rank-based (Cliff's $\\delta$, AUC) effect sizes is a standard and important topic in applied statistics.\n-   **Well-Posed**: The question is well-posed. It asks for a qualitative and analytical comparison of the properties of three well-defined statistical measures under specified distributional characteristics. A unique and correct answer can be derived from statistical first principles.\n-   **Objective**: The language is technical, precise, and objective. It poses a question about the mathematical and statistical properties of estimators, not subjective opinions.\n-   **Flaw Assessment**:\n    1.  **Scientific/Factual Unsoundness**: None. The premises are factually sound.\n    2.  **Non-Formalizable/Irrelevant**: None. The problem is directly formalizable and central to the topic of effect size measures.\n    3.  **Incomplete/Contradictory**: None. The problem provides sufficient information to reason about the robustness of the measures. The sample data details provide a realistic context.\n    4.  **Unrealistic/Infeasible**: None. The described experimental scenario and data characteristics are highly realistic for RT research.\n    5.  **Ill-Posed/Ambiguous**: None. The terms ($d$, $\\delta$, AUC, skewness) have precise definitions.\n    6.  **Trivial/Tautological**: None. The problem requires substantive reasoning about the influence of distribution shape on different types of estimators.\n    7.  **Unverifiable**: None. The claims can be verified analytically and via simulation.\n\n### Step 3: Verdict and Action\n\n**Verdict**: The problem statement is valid. I will proceed with a full derivation and analysis.\n\n### Derivation and Analysis\n\nThe core of the problem is to understand how skewness, a property of the shape of a probability distribution, affects different summary statistics.\n\n**1. Cohen’s $d$**\nThe population parameter for Cohen's $d$ is defined as the difference in means divided by a pooled standard deviation:\n$$d = \\frac{\\mu_X - \\mu_Y}{\\sigma_{\\text{pooled}}}$$\nIts sample estimator is:\n$$\\hat{d} = \\frac{\\bar{X} - \\bar{Y}}{S_{\\text{pooled}}}$$\nwhere $\\bar{X} = \\frac{1}{n_X}\\sum_{i=1}^{n_X} x_i$ is the sample mean and $S_{\\text{pooled}} = \\sqrt{\\frac{(n_X-1)S_X^2 + (n_Y-1)S_Y^2}{n_X+n_Y-2}}$ is the pooled sample standard deviation (with $S^2$ being the sample variance).\n\n-   **Sensitivity to Skewness**: The sample mean ($\\bar{X}$) is known to be a non-robust estimator of central tendency. For a right-skewed distribution, the mean is pulled towards the long right tail, making it larger than the median (the center of the data mass). Similarly, the sample variance ($S^2$) is based on squared deviations from the mean, so a few extreme values (outliers or a heavy tail) can inflate it dramatically. Since $\\hat{d}$ is a function of two means and a pooled standard deviation, it is highly sensitive to skewness, heavy tails, and outliers. The presence of long-latency responses, as described, will disproportionately influence both the numerator and the denominator of $\\hat{d}$, potentially providing a distorted picture of the typical group difference. Furthermore, the validity of polling the variances rests on the assumption of homoscedasticity ($\\sigma_X^2 = \\sigma_Y^2$), which is also mentioned as being questionable in the problem description.\n\n**2. Cliff’s $\\delta$ and AUC**\nCliff's $\\delta$ is a nonparametric measure of stochastic dominance, defined as:\n$$\\delta = P(X  Y) - P(X  Y)$$\nThe Area Under the ROC Curve (AUC) is, in this context, the probability that a randomly selected observation from group $X$ is larger than one from group $Y$:\n$$AUC = P(X  Y)$$\n(assuming negligible ties, as stated in the problem).\n\n-   **Relationship**: The two measures are linearly related. Since $P(X  Y) + P(X  Y) + P(X=Y) = 1$, and $P(X=Y) \\approx 0$, we have $P(X  Y) \\approx 1 - P(X  Y)$. Substituting into the definition of $\\delta$:\n$$\\delta \\approx P(X  Y) - (1 - P(X  Y)) = 2 \\cdot P(X  Y) - 1 = 2 \\cdot AUC - 1$$\nThis linear relationship implies they share the same fundamental properties regarding robustness.\n\n-   **Sensitivity to Skewness**: The key property of $\\delta$ and AUC is that they are based on pairwise *orderings*. The sample estimate for $\\delta$, for instance, is $\\hat{\\delta} = \\frac{\\# (x_i  y_j) - \\# (x_i  y_j)}{n_X n_Y}$. The calculation only depends on the results of comparisons ($$, $$, $=$), not the magnitude of the differences.\nLet $f(\\cdot)$ be any strictly increasing monotonic transformation (e.g., $f(z) = \\log(z)$ for $z0$). Then, $x_i  y_j$ if and only if $f(x_i)  f(y_j)$. This means the count of $(x_i, y_j)$ pairs where $x_i  y_j$ is unchanged by the transformation. Consequently, $\\hat{\\delta}$ and the sample AUC are invariant to any such transformation. Since transformations like the logarithm are commonly used to reduce or remove skewness from RT data, the invariance of $\\delta$ and AUC to these transformations demonstrates their robustness to skewness. They are rank-based measures and are not influenced by the magnitude of extreme values, only by their rank order.\n\n**3. Comparison and Diagnostic Checks**\n\n-   **Sensitivity**: Cohen's $d$ is highly sensitive to skewness and variance heterogeneity. Cliff's $\\delta$ and AUC are highly robust to skewness due to their invariance to monotonic transformations.\n-   **Diagnostics for $d$**: To trust Cohen's $d$, one must assess the conditions it is sensitive to. This includes:\n    -   Checking for skewness: Visually with histograms/Q-Q plots, and quantitatively with statistics like the sample skewness coefficient ($\\hat{\\gamma}_1$).\n    -   Checking for variance equality: Using tests robust to non-normality, such as Levene's test or the Brown-Forsythe test.\n    -   Assessing impact: A powerful diagnostic is to compute $\\hat{d}$ on both raw and transformed (e.g., log-transformed) data. A large discrepancy between the two values is a clear sign that skewness is materially biasing the result on the raw scale.\n-   **Diagnostics for $\\delta$/AUC**:\n    -   Since these measures are robust, they are less in need of diagnostics for their own validity, but it is good practice to check the rate of ties, as a high rate can affect the estimator.\n    -   Their most important role in this context is to serve as a *robustness check* or complement to Cohen's $d$. If $\\hat{d}$ suggests a large effect but $\\hat{\\delta}$ is near zero, it indicates the mean difference is likely driven by a few outliers rather than a systematic shift in the distributions.\n-   **General Diagnostics**: Plotting two-sample quantile functions (or shift functions) provides a comprehensive view of how the two distributions differ across their entire range, rather than summarizing the difference with a single number. This can reveal, for instance, that an effect is present only in the tails of the distributions.\n\n### Evaluation of Options\n\n**A. Cohen’s $d$ is more sensitive to skewness and variance inequality because it uses means and a pooled standard deviation; Cliff’s $\\delta$ and AUC depend only on ranks or pairwise orderings and are invariant to strictly increasing transformations, so they are comparatively robust to skewness. Diagnostics should include inspecting sample skewness $\\,\\hat{\\gamma}_1\\,$ and two-sample quantile/shift functions, testing equality of variances (e.g., Levene or Brown–Forsythe), checking tie rates, comparing $d$ on raw versus log-transformed RT to gauge sensitivity, and computing $\\delta$ or AUC as complementary robustness checks.**\nThis option correctly identifies that Cohen's $d$ is sensitive to skewness and variance heterogeneity due to its reliance on means and variances. It correctly states that Cliff's $\\delta$ and AUC are robust because they are based on orderings and are invariant to monotonic transformations. The list of proposed diagnostic checks is comprehensive, scientifically appropriate, and directly addresses the vulnerabilities and strengths of the respective measures.\n**Verdict: Correct.**\n\n**B. Cohen’s $d$, Cliff’s $\\delta$, and AUC are equally sensitive to skewness because all three summarize the same central tendency. Diagnostics should focus on normality tests (e.g., Shapiro–Wilk) only; monotone transformations will leave Cohen’s $d$ unchanged, so comparing raw and log-transformed RT is unnecessary.**\nThis option contains several fundamental errors. First, the measures are not equally sensitive to skewness. Second, they do not summarize the same concept of central tendency; $d$ is based on the mean, while $\\delta$/AUC relate to stochastic dominance, which is more aligned with the median in skewed distributions. Third, the claim that \"monotone transformations will leave Cohen’s $d$ unchanged\" is patently false. For a transformation $f$, the means of the transformed variables, $\\overline{f(X)}$ and $\\overline{f(Y)}$, are not generally equal to the transformations of the means, $f(\\bar{X})$ and $f(\\bar{Y})$. Therefore, $\\hat{d}$ is not invariant.\n**Verdict: Incorrect.**\n\n**C. Cliff’s $\\delta$ is more sensitive to skewness than Cohen’s $d$ because it overweights the tails; AUC is unaffected by strictly increasing transformations but is biased by skewness. Diagnostics should rely on Pearson correlation between RT and group labels, plus residual plots from linear models.**\nThis option incorrectly reverses the sensitivity. Rank-based measures like $\\delta$ *down-weight* the influence of tails/outliers, whereas the mean (used in $d$) \"overweights\" them. The statement \"AUC is unaffected by strictly increasing transformations but is biased by skewness\" is a direct contradiction. Invariance to monotonic transformations is precisely *why* it is robust to skewness.\n**Verdict: Incorrect.**\n\n**D. Cohen’s $d$ is invariant to strictly increasing transformations and therefore robust to skewness; Cliff’s $\\delta$ and AUC depend on the exact marginal distributions and will be distorted by skewness. Diagnostics should be limited to histogram inspection without formal variance checks or quantile-based analyses.**\nThis option completely inverts the properties of the measures. As explained for option B, Cohen's $d$ is *not* invariant to monotonic transformations. Conversely, Cliff's $\\delta$ and AUC *are* invariant to such transformations and are therefore robust to skewness, not \"distorted\" by it. The suggestion to limit diagnostics is also poor statistical practice.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}