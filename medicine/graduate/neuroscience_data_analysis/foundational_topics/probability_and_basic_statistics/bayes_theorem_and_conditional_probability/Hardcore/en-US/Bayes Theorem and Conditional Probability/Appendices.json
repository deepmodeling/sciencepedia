{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of Bayesian data analysis is updating our knowledge in light of new evidence. This exercise walks you through the fundamental mechanics of this process for a ubiquitous problem in neuroscience: estimating a neuron's firing rate, $\\lambda$. By working through the derivation of the posterior distribution for a Poisson spike count model with a Gamma prior , you will gain hands-on experience with the interplay between the prior, the likelihood, and the resulting posterior.",
            "id": "4140543",
            "problem": "A single isolated neuron is recorded extracellularly for a duration $\\,\\tau>0\\,$ seconds during a stable behavioral epoch. Let $\\,K\\,$ denote the observed spike count in this window, modeled as conditionally Poisson given the unknown firing rate $\\,\\lambda>0\\,$ (spikes per second), with $\\,K \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda \\tau)\\,$. To capture prior uncertainty about $\\,\\lambda\\,$ based on previous sessions, assume a Gamma prior with shape parameter $\\,a>0\\,$ and rate parameter $\\,b>0\\,$, with prior density $\\,p(\\lambda) = \\frac{b^{a}}{\\Gamma(a)} \\lambda^{a-1} \\exp(-b \\lambda)\\,$ for $\\,\\lambda>0\\,$. Using Bayes’ theorem and the core definitions of conditional probability, derive the posterior density $\\,p(\\lambda \\mid K=k)\\,$ as a normalized function of $\\,\\lambda\\,$ for fixed observed $\\,k \\in \\{0,1,2,\\dots\\}\\,$ and fixed $\\,\\tau,a,b>0\\,$. Compute the normalizing constant explicitly by integration over $\\,\\lambda \\in (0,\\infty)\\,$ and verify that the posterior integrates to $\\,1\\,$. Express your final result as a single closed-form analytic expression for $\\,p(\\lambda \\mid K=k)\\,$ in terms of $\\,a,b,\\tau,k,\\lambda\\,$. No numerical approximation is required, and no rounding is needed. State your answer without units.",
            "solution": "The supplied problem is deemed valid after a thorough evaluation. It is scientifically grounded, well-posed, objective, and contains sufficient information for a unique and meaningful solution. The problem requires the derivation of a posterior probability density function using Bayes' theorem, which is a standard procedure in Bayesian statistics and its application to neuroscience data analysis.\n\nThe objective is to derive the posterior density $p(\\lambda \\mid K=k)$ for the firing rate $\\lambda$. According to Bayes' theorem, the posterior density is proportional to the product of the likelihood of the data given the parameter and the prior density of the parameter.\n$$p(\\lambda \\mid K=k) = \\frac{p(K=k \\mid \\lambda) p(\\lambda)}{\\int_{0}^{\\infty} p(K=k \\mid \\lambda) p(\\lambda) d\\lambda}$$\n\nFirst, we identify the two components of the numerator: the likelihood $p(K=k \\mid \\lambda)$ and the prior $p(\\lambda)$.\n\nThe likelihood is given by the Poisson distribution. The problem states that the spike count $K$ in a time window of duration $\\tau$ is modeled as $K \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda \\tau)$. For an observed count $K=k$, where $k \\in \\{0, 1, 2, \\dots\\}$, the probability mass function, viewed as a function of $\\lambda$ for fixed $k$, is the likelihood function $L(\\lambda; k)$:\n$$L(\\lambda; k) = p(K=k \\mid \\lambda) = \\frac{(\\lambda\\tau)^k \\exp(-\\lambda\\tau)}{k!}$$\nThis is valid for $\\lambda > 0$.\n\nThe prior distribution for the firing rate $\\lambda$ is given as a Gamma distribution with shape parameter $a>0$ and rate parameter $b>0$. The probability density function is:\n$$p(\\lambda) = \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} \\exp(-b\\lambda)$$\nThis is valid for $\\lambda > 0$.\n\nNext, we form the unnormalized posterior density by taking the product of the likelihood and the prior:\n$$p(\\lambda \\mid K=k) \\propto p(K=k \\mid \\lambda) p(\\lambda)$$\n$$p(\\lambda \\mid K=k) \\propto \\left( \\frac{(\\lambda\\tau)^k \\exp(-\\lambda\\tau)}{k!} \\right) \\left( \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} \\exp(-b\\lambda) \\right)$$\nWe can group terms that depend on $\\lambda$ and terms that are constant with respect to $\\lambda$. The terms $1/k!$, $\\tau^k$, $b^a$, and $1/\\Gamma(a)$ are all constants with respect to $\\lambda$ and can be absorbed into the proportionality constant.\n$$p(\\lambda \\mid K=k) \\propto (\\lambda^k \\exp(-\\lambda\\tau)) (\\lambda^{a-1} \\exp(-b\\lambda))$$\nCombining the terms with $\\lambda$:\n$$p(\\lambda \\mid K=k) \\propto \\lambda^{k+a-1} \\exp(-\\lambda\\tau - b\\lambda)$$\n$$p(\\lambda \\mid K=k) \\propto \\lambda^{(a+k)-1} \\exp(-(b+\\tau)\\lambda)$$\nThis expression is the kernel of a probability distribution for $\\lambda$. We recognize this as the kernel of a Gamma distribution, $\\lambda^{\\text{shape}-1} \\exp(-\\text{rate} \\cdot \\lambda)$. By comparing terms, we can identify the parameters of the posterior distribution. The posterior distribution is a Gamma distribution with updated shape parameter $a' = a+k$ and updated rate parameter $b' = b+\\tau$.\n\nLet this posterior be denoted $\\mathrm{Gamma}(a', b')$. The normalized probability density function for a Gamma distribution is given by $p(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x)$. Substituting our posterior parameters $a'$ and $b'$, we obtain the normalized posterior density for $\\lambda$:\n$$p(\\lambda \\mid K=k) = \\frac{(b+\\tau)^{a+k}}{\\Gamma(a+k)} \\lambda^{a+k-1} \\exp(-(b+\\tau)\\lambda)$$\n\nThe problem requires us to compute the normalizing constant explicitly. The normalizing constant is the integral of the unnormalized posterior over the domain of $\\lambda$, which is $(0, \\infty)$. Let this constant be $C$.\n$$C = \\int_{0}^{\\infty} p(K=k \\mid \\lambda) p(\\lambda) d\\lambda$$\n$$C = \\int_{0}^{\\infty} \\left( \\frac{(\\lambda\\tau)^k \\exp(-\\lambda\\tau)}{k!} \\right) \\left( \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} \\exp(-b\\lambda) \\right) d\\lambda$$\nWe can pull out all factors that are not functions of $\\lambda$:\n$$C = \\frac{\\tau^k b^a}{k! \\Gamma(a)} \\int_{0}^{\\infty} \\lambda^k \\exp(-\\lambda\\tau) \\lambda^{a-1} \\exp(-b\\lambda) d\\lambda$$\nCombine the terms inside the integral:\n$$C = \\frac{\\tau^k b^a}{k! \\Gamma(a)} \\int_{0}^{\\infty} \\lambda^{a+k-1} \\exp(-(b+\\tau)\\lambda) d\\lambda$$\nThe integral is of the form of a Gamma function. The integral part can be solved using the definition of the Gamma function, $\\Gamma(z) = \\int_0^\\infty t^{z-1} e^{-t} dt$. We can use a substitution $u = (b+\\tau)\\lambda$, which implies $d\\lambda = du/(b+\\tau)$ and $\\lambda = u/(b+\\tau)$.\n$$\\int_{0}^{\\infty} \\lambda^{a+k-1} \\exp(-(b+\\tau)\\lambda) d\\lambda = \\int_{0}^{\\infty} \\left(\\frac{u}{b+\\tau}\\right)^{a+k-1} \\exp(-u) \\frac{du}{b+\\tau}$$\n$$= \\frac{1}{(b+\\tau)^{a+k}} \\int_{0}^{\\infty} u^{a+k-1} \\exp(-u) du = \\frac{\\Gamma(a+k)}{(b+\\tau)^{a+k}}$$\nSubstituting this result back into the expression for $C$:\n$$C = \\frac{\\tau^k b^a}{k! \\Gamma(a)} \\frac{\\Gamma(a+k)}{(b+\\tau)^{a+k}}$$\nThis constant $C$ is the marginal likelihood $p(K=k)$. The posterior density is then $p(\\lambda \\mid K=k) = \\frac{p(K=k \\mid \\lambda)p(\\lambda)}{C}$.\n$$p(\\lambda \\mid K=k) = \\frac{\\frac{(\\lambda\\tau)^k \\exp(-\\lambda\\tau)}{k!} \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} \\exp(-b\\lambda)}{\\frac{\\tau^k b^a \\Gamma(a+k)}{k! \\Gamma(a) (b+\\tau)^{a+k}}} = \\frac{(\\lambda\\tau)^k \\exp(-\\lambda\\tau) b^a \\lambda^{a-1} \\exp(-b\\lambda) k! \\Gamma(a) (b+\\tau)^{a+k}}{k! \\Gamma(a) \\tau^k b^a \\Gamma(a+k)}$$\n$$p(\\lambda \\mid K=k) = \\frac{\\lambda^{a+k-1} \\exp(-\\lambda(b+\\tau)) (b+\\tau)^{a+k}}{\\Gamma(a+k)}$$\nThis matches the expression derived by recognizing the posterior kernel.\n\nFinally, we verify that this posterior density integrates to $1$.\n$$\\int_{0}^{\\infty} p(\\lambda \\mid K=k) d\\lambda = \\int_{0}^{\\infty} \\frac{(b+\\tau)^{a+k}}{\\Gamma(a+k)} \\lambda^{a+k-1} \\exp(-(b+\\tau)\\lambda) d\\lambda$$\n$$= \\frac{(b+\\tau)^{a+k}}{\\Gamma(a+k)} \\int_{0}^{\\infty} \\lambda^{a+k-1} \\exp(-(b+\\tau)\\lambda) d\\lambda$$\nUsing our previous integral evaluation:\n$$= \\frac{(b+\\tau)^{a+k}}{\\Gamma(a+k)} \\left( \\frac{\\Gamma(a+k)}{(b+\\tau)^{a+k}} \\right) = 1$$\nThe verification is successful. The posterior is a proper probability density.\n\nThe final closed-form analytic expression for the posterior density $p(\\lambda \\mid K=k)$ is therefore the probability density function of a Gamma distribution with shape $a+k$ and rate $b+\\tau$.",
            "answer": "$$\\boxed{\\frac{(b+\\tau)^{a+k}}{\\Gamma(a+k)} \\lambda^{a+k-1} \\exp\\left(-(b+\\tau)\\lambda\\right)}$$"
        },
        {
            "introduction": "Experimental data in neuroscience is often collected over extended periods, making it crucial to have methods that can learn from data streams efficiently. This practice builds upon the static Bayesian update by introducing the concept of online or recursive inference . You will see how the posterior distribution after one observation elegantly becomes the prior for the next, allowing you to sequentially update your estimate of a neuron's firing rate, $\\lambda$, without ever needing to reprocess past data.",
            "id": "4140604",
            "problem": "You are analyzing spike train data from a single neuron recorded in discrete time bins with variable durations. In each bin, you observe a nonnegative integer spike count. The scientific model is as follows: given a constant firing rate parameter $\\lambda$ in Hertz (Hz), the spike counts across bins are conditionally independent, and each bin $t$ has a count $k_t$ distributed according to a Poisson distribution with mean $\\lambda d_t$, where $d_t$ is the duration of bin $t$ in seconds. The complete data up to bin $t$ is denoted by $D_{1:t} = \\{(k_1,d_1),\\ldots,(k_t,d_t)\\}$. The prior distribution for the firing rate $\\lambda$ is a Gamma distribution parameterized by shape $\\alpha_0$ and rate $\\beta_0$, with density proportional to $\\lambda^{\\alpha_0 - 1} \\exp(-\\beta_0 \\lambda)$. All variables and numbers should be interpreted within their mathematical definitions as stated.\n\nStarting from the definition of conditional probability and Bayes’ theorem applied to sequences of observations, and from the assumption of conditional independence of bins given $\\lambda$, derive the recursive posterior update in an online form, where the posterior at time $t$ can be updated from the posterior at time $t-1$ and the new observation $(k_t,d_t)$ without revisiting past data. Use only the definitions and properties of conditional probability, Bayes’ theorem, and the Poisson and Gamma distributions; do not invoke any pre-derived formulas for the posterior or predictive distributions. Then derive the one-step-ahead posterior predictive probability that the next bin of duration $d_{\\text{next}}$ seconds will contain at least one spike, expressed as a decimal (not a percentage).\n\nImplement an algorithm that:\n- Initializes the posterior with the Gamma prior parameters $\\alpha_0$ and $\\beta_0$.\n- Processes the sequence $D_{1:t}$ online (one bin at a time) applying the recursive posterior update for the Poisson observations.\n- After all $t$ bins, computes the posterior mean firing rate $m_t = \\mathbb{E}[\\lambda \\mid D_{1:t}]$ in spikes per second (Hz).\n- Computes the posterior predictive probability $q_t$ that the next bin of duration $d_{\\text{next}}$ seconds contains at least one spike.\n\nExpress $m_t$ in Hertz (Hz). Express $q_t$ as a decimal in $[0,1]$. Both $m_t$ and $q_t$ must be rounded to six decimal places in the final output.\n\nTest suite:\n- Case $1$ (general): prior $(\\alpha_0,\\beta_0) = (2.0, 1.0)$, data $k$ sequence $[0, 1, 0, 2]$, durations $[0.2, 0.2, 0.2, 0.4]$ seconds, next duration $d_{\\text{next}} = 0.5$ seconds.\n- Case $2$ (boundary: no data): prior $(\\alpha_0,\\beta_0) = (1.5, 0.5)$, data $k$ sequence $[]$, durations $[]$, next duration $d_{\\text{next}} = 0.1$ seconds.\n- Case $3$ (edge: many zeros): prior $(\\alpha_0,\\beta_0) = (5.0, 2.0)$, data $k$ sequence $[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]$, durations $[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]$ seconds, next duration $d_{\\text{next}} = 1.0$ seconds.\n- Case $4$ (high counts and varied durations): prior $(\\alpha_0,\\beta_0) = (0.5, 0.1)$, data $k$ sequence $[5, 7, 3, 4, 6]$, durations $[1.0, 1.0, 0.5, 0.5, 2.0]$ seconds, next duration $d_{\\text{next}} = 0.5$ seconds.\n- Case $5$ (strong prior, sparse data): prior $(\\alpha_0,\\beta_0) = (20.0, 10.0)$, data $k$ sequence $[0, 0, 1, 0, 0]$, durations $[0.1, 0.1, 0.1, 0.1, 0.1]$ seconds, next duration $d_{\\text{next}} = 0.2$ seconds.\n\nYour program should produce a single line of output containing the results as a comma-separated list of two-element lists enclosed in square brackets, in the same order as the test suite, where each inner list is $[m_t,q_t]$. For example, the output format must be like $[[m_1,q_1],[m_2,q_2],\\ldots]$, with all floating-point numbers rounded to six decimal places.",
            "solution": "The user wants me to solve a problem on Bayesian inference for neural spike train data. I will first validate the problem statement and then provide a detailed derivation and a Python implementation.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n-   **Model**: Spike count $k_t$ in a discrete time bin $t$ of duration $d_t$ follows a Poisson distribution with mean $\\lambda d_t$, where $\\lambda$ is a constant firing rate parameter in Hz. $p(k_t | \\lambda, d_t) = \\text{Poisson}(k_t; \\lambda d_t)$.\n-   **Conditional Independence**: Spike counts across bins are conditionally independent given $\\lambda$.\n-   **Data**: The data up to bin $t$ is $D_{1:t} = \\{(k_1,d_1),\\ldots,(k_t,d_t)\\}$.\n-   **Prior Distribution**: The prior for $\\lambda$ is a Gamma distribution, $p(\\lambda) \\sim \\text{Gamma}(\\alpha_0, \\beta_0)$, with probability density function (PDF) $p(\\lambda) \\propto \\lambda^{\\alpha_0 - 1} \\exp(-\\beta_0 \\lambda)$.\n-   **Task 1**: Derive the recursive posterior update rule for the parameters of the posterior distribution of $\\lambda$.\n-   **Task 2**: Derive the one-step-ahead posterior predictive probability that the next bin of duration $d_{\\text{next}}$ contains at least one spike, $P(k_{\\text{next}} \\geq 1 | D_{1:t}, d_{\\text{next}})$.\n-   **Task 3**: Implement an algorithm to compute the posterior mean firing rate $m_t = \\mathbb{E}[\\lambda | D_{1:t}]$ and the predictive probability $q_t = P(k_{\\text{next}} \\geq 1 | D_{1:t}, d_{\\text{next}})$.\n-   **Output Format**: Results for test cases as $[[m_1,q_1],[m_2,q_2],\\ldots]$, with numbers rounded to six decimal places.\n-   **Test Cases**:\n    -   Case 1: $(\\alpha_0,\\beta_0) = (2.0, 1.0)$, $k=[0, 1, 0, 2]$, $d=[0.2, 0.2, 0.2, 0.4]$, $d_{\\text{next}}=0.5$.\n    -   Case 2: $(\\alpha_0,\\beta_0) = (1.5, 0.5)$, $k=[]$, $d=[]$, $d_{\\text{next}}=0.1$.\n    -   Case 3: $(\\alpha_0,\\beta_0) = (5.0, 2.0)$, $k=[0]*20$, $d=[0.05]*20$, $d_{\\text{next}}=1.0$.\n    -   Case 4: $(\\alpha_0,\\beta_0) = (0.5, 0.1)$, $k=[5, 7, 3, 4, 6]$, $d=[1.0, 1.0, 0.5, 0.5, 2.0]$, $d_{\\text{next}}=0.5$.\n    -   Case 5: $(\\alpha_0,\\beta_0) = (20.0, 10.0)$, $k=[0, 0, 1, 0, 0]$, $d=[0.1, 0.1, 0.1, 0.1, 0.1]$, $d_{\\text{next}}=0.2$.\n\n#### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in Bayesian statistics and its application to neuroscience, a standard practice. The Poisson model for spike counts and the Gamma prior for the rate parameter are canonical choices (a conjugate prior pair), forming the basis of many statistical analyses in this field.\n2.  **Well-Posed**: The problem is well-posed. The use of a conjugate prior (Gamma) for a Poisson likelihood ensures that the posterior is also a Gamma distribution, leading to a unique and stable solution. All parameters and objectives are clearly defined.\n3.  **Objective**: The language is formal, precise, and free of any subjective or opinion-based statements.\n4.  **No Flaws**: The problem does not violate any scientific principles, is not metaphorical, is directly related to the specified topic, provides all necessary information, is scientifically plausible, and is not ill-posed, trivial, or unverifiable.\n\n#### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation and Solution\n\nThe problem requires the derivation of a recursive Bayesian update scheme for the parameters of the posterior distribution of a neural firing rate $\\lambda$, and the subsequent derivation of the posterior predictive probability.\n\nLet the prior distribution for the firing rate $\\lambda$ be a Gamma distribution, denoted as $\\text{Gamma}(\\lambda; \\alpha_0, \\beta_0)$. Its probability density function (PDF) is:\n$$p(\\lambda) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} \\lambda^{\\alpha_0 - 1} e^{-\\beta_0 \\lambda}$$\nwhere $\\Gamma(\\cdot)$ is the Gamma function.\n\nThe likelihood of observing $k_t$ spikes in a bin of duration $d_t$ is given by the Poisson distribution:\n$$P(k_t | \\lambda, d_t) = \\frac{(\\lambda d_t)^{k_t} e^{-\\lambda d_t}}{k_t!}$$\n\n#### Recursive Posterior Update\n\nWe aim to find the posterior distribution of $\\lambda$ after observing data up to time $t$, $D_{1:t} = \\{(k_1, d_1), \\dots, (k_t, d_t)\\}$. We will derive the recursive form, updating the posterior at time $t-1$ to the posterior at time $t$ using the new observation $(k_t, d_t)$.\n\nLet the posterior distribution after observing $D_{1:t-1}$ be $p(\\lambda | D_{1:t-1})$. By Bayes' theorem, the posterior after observing $D_{1:t}$ is:\n$$p(\\lambda | D_{1:t}) = p(\\lambda | D_{1:t-1}, (k_t, d_t)) \\propto p((k_t, d_t) | \\lambda, D_{1:t-1}) p(\\lambda | D_{1:t-1})$$\nThe problem states that given $\\lambda$, observations are conditionally independent. Thus, the likelihood of the new observation $(k_t, d_t)$ depends only on $\\lambda$, not on past data $D_{1:t-1}$.\n$$p((k_t, d_t) | \\lambda, D_{1:t-1}) = P(k_t | \\lambda, d_t)$$\nThe expression for the posterior becomes:\n$$p(\\lambda | D_{1:t}) \\propto P(k_t | \\lambda, d_t) p(\\lambda | D_{1:t-1})$$\nThis recursive relationship forms the basis of the online update. Let's assume the posterior at time $t-1$ is a Gamma distribution, $p(\\lambda | D_{1:t-1}) = \\text{Gamma}(\\lambda; \\alpha_{t-1}, \\beta_{t-1})$. The base case is at $t=0$ (before any data), where the posterior is the prior, so $(\\alpha_0, \\beta_0)$ are the initial parameters.\n$$p(\\lambda | D_{1:t-1}) \\propto \\lambda^{\\alpha_{t-1} - 1} e^{-\\beta_{t-1} \\lambda}$$\nSubstituting the likelihood and the prior into the recursive formula:\n$$p(\\lambda | D_{1:t}) \\propto \\left( \\frac{(\\lambda d_t)^{k_t} e^{-\\lambda d_t}}{k_t!} \\right) \\left( \\lambda^{\\alpha_{t-1} - 1} e^{-\\beta_{t-1} \\lambda} \\right)$$\nWe can group terms and drop constants that do not depend on $\\lambda$ (i.e., $d_t^{k_t}$ and $k_t!$):\n$$p(\\lambda | D_{1:t}) \\propto \\left( \\lambda^{k_t} e^{-\\lambda d_t} \\right) \\left( \\lambda^{\\alpha_{t-1} - 1} e^{-\\beta_{t-1} \\lambda} \\right)$$\n$$p(\\lambda | D_{1:t}) \\propto \\lambda^{k_t + \\alpha_{t-1} - 1} e^{-(\\beta_{t-1} + d_t)\\lambda}$$\nThis expression is the kernel of a Gamma distribution. Thus, the posterior $p(\\lambda | D_{1:t})$ is also a Gamma distribution, which we denote as $\\text{Gamma}(\\lambda; \\alpha_t, \\beta_t)$, with updated parameters:\n$$\\alpha_t = \\alpha_{t-1} + k_t$$\n$$\\beta_t = \\beta_{t-1} + d_t$$\nThis confirms that the Gamma distribution is a conjugate prior for the Poisson likelihood, and provides the required recursive update rules. Starting from $(\\alpha_0, \\beta_0)$, after processing all $t$ observations, the final posterior parameters are $\\alpha_t = \\alpha_0 + \\sum_{i=1}^t k_i$ and $\\beta_t = \\beta_0 + \\sum_{i=1}^t d_i$.\n\n#### Posterior Mean Firing Rate ($m_t$)\n\nThe mean of a Gamma distribution $\\text{Gamma}(x; \\alpha, \\beta)$ is $\\mathbb{E}[x] = \\alpha/\\beta$. Therefore, the posterior mean firing rate after observing $D_{1:t}$ is:\n$$m_t = \\mathbb{E}[\\lambda | D_{1:t}] = \\frac{\\alpha_t}{\\beta_t} = \\frac{\\alpha_0 + \\sum_{i=1}^t k_i}{\\beta_0 + \\sum_{i=1}^t d_i}$$\n\n#### Posterior Predictive Probability ($q_t$)\n\nWe need to compute the probability of observing at least one spike in a future time bin of duration $d_{\\text{next}}$, given the data $D_{1:t}$. This is denoted as $q_t = P(k_{\\text{next}} \\geq 1 | D_{1:t}, d_{\\text{next}})$. It is easier to compute this as $1 - P(k_{\\text{next}} = 0 | D_{1:t}, d_{\\text{next}})$.\n\nThe posterior predictive probability for a new observation $k_{\\text{next}}$ is obtained by marginalizing the likelihood of the new data point over the posterior distribution of the parameter $\\lambda$:\n$$P(k_{\\text{next}} | D_{1:t}, d_{\\text{next}}) = \\int_{0}^{\\infty} P(k_{\\text{next}} | \\lambda, d_{\\text{next}}) p(\\lambda | D_{1:t}) d\\lambda$$\nHere, $P(k_{\\text{next}} | \\lambda, d_{\\text{next}})$ is the Poisson likelihood, and $p(\\lambda | D_{1:t})$ is the Gamma posterior $\\text{Gamma}(\\lambda; \\alpha_t, \\beta_t)$.\nSubstituting the respective PDF expressions:\n$$P(k_{\\text{next}}=k | D_{1:t}, d_{\\text{next}}) = \\int_{0}^{\\infty} \\left( \\frac{(\\lambda d_{\\text{next}})^k e^{-\\lambda d_{\\text{next}}}{k!} \\right) \\left( \\frac{\\beta_t^{\\alpha_t}}{\\Gamma(\\alpha_t)} \\lambda^{\\alpha_t - 1} e^{-\\beta_t \\lambda} \\right) d\\lambda$$\n$$= \\frac{d_{\\text{next}}^k \\beta_t^{\\alpha_t}}{k! \\Gamma(\\alpha_t)} \\int_{0}^{\\infty} \\lambda^{k+\\alpha_t-1} e^{-(\\beta_t + d_{\\text{next}})\\lambda} d\\lambda$$\nThe integral is the unnormalized kernel of a Gamma PDF. Using the identity $\\int_0^\\infty x^{z-1} e^{-ax} dx = \\frac{\\Gamma(z)}{a^z}$, with $z = k+\\alpha_t$ and $a = \\beta_t + d_{\\text{next}}$, the integral evaluates to $\\frac{\\Gamma(k+\\alpha_t)}{(\\beta_t + d_{\\text{next}})^{k+\\alpha_t}}$.\nSubstituting this back, we get the PMF of the posterior predictive distribution, which is a Negative Binomial distribution:\n$$P(k_{\\text{next}}=k | \\dots) = \\frac{\\Gamma(k+\\alpha_t)}{k! \\Gamma(\\alpha_t)} \\left( \\frac{\\beta_t}{\\beta_t + d_{\\text{next}}} \\right)^{\\alpha_t} \\left( \\frac{d_{\\text{next}}}{\\beta_t + d_{\\text{next}}} \\right)^k$$\nTo find $q_t$, we first calculate $P(k_{\\text{next}}=0 | \\dots)$ by setting $k=0$ in the above formula:\n$$P(k_{\\text{next}}=0 | \\dots) = \\frac{\\Gamma(\\alpha_t)}{0! \\Gamma(\\alpha_t)} \\left( \\frac{\\beta_t}{\\beta_t + d_{\\text{next}}} \\right)^{\\alpha_t} \\left( \\frac{d_{\\text{next}}}{\\beta_t + d_{\\text{next}}} \\right)^0$$\nSince $0! = 1$ and any number to the power of $0$ is $1$:\n$$P(k_{\\text{next}}=0 | \\dots) = \\left( \\frac{\\beta_t}{\\beta_t + d_{\\text{next}}} \\right)^{\\alpha_t}$$\nFinally, the probability of observing at least one spike is:\n$$q_t = P(k_{\\text{next}} \\geq 1 | \\dots) = 1 - P(k_{\\text{next}}=0 | \\dots) = 1 - \\left( \\frac{\\beta_t}{\\beta_t + d_{\\text{next}}} \\right)^{\\alpha_t}$$\n\n### Algorithmic Implementation\n\nThe algorithm proceeds as follows for each test case:\n1.  Initialize posterior parameters with the prior parameters: $\\alpha \\leftarrow \\alpha_0$, $\\beta \\leftarrow \\beta_0$.\n2.  Iterate through the observed data pairs $(k_i, d_i)$ for $i=1, \\dots, t$. In each iteration, update the parameters using the derived recursive rules:\n    -   $\\alpha \\leftarrow \\alpha + k_i$\n    -   $\\beta \\leftarrow \\beta + d_i$\n3.  After processing all data, the final parameters are $\\alpha_t$ and $\\beta_t$.\n4.  Calculate the posterior mean firing rate: $m_t = \\alpha_t / \\beta_t$.\n5.  Calculate the posterior predictive probability of at least one spike: $q_t = 1 - (\\beta_t / (\\beta_t + d_{\\text{next}}))^{\\alpha_t}$.\n6.  Round both $m_t$ and $q_t$ to six decimal places and store the results.\n\nThis procedure is implemented in the Python code below.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inference problem for spike train data for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general)\n        {'alpha0': 2.0, 'beta0': 1.0, 'k_seq': [0, 1, 0, 2], 'd_seq': [0.2, 0.2, 0.2, 0.4], 'd_next': 0.5},\n        # Case 2 (boundary: no data)\n        {'alpha0': 1.5, 'beta0': 0.5, 'k_seq': [], 'd_seq': [], 'd_next': 0.1},\n        # Case 3 (edge: many zeros)\n        {'alpha0': 5.0, 'beta0': 2.0, 'k_seq': [0]*20, 'd_seq': [0.05]*20, 'd_next': 1.0},\n        # Case 4 (high counts and varied durations)\n        {'alpha0': 0.5, 'beta0': 0.1, 'k_seq': [5, 7, 3, 4, 6], 'd_seq': [1.0, 1.0, 0.5, 0.5, 2.0], 'd_next': 0.5},\n        # Case 5 (strong prior, sparse data)\n        {'alpha0': 20.0, 'beta0': 10.0, 'k_seq': [0, 0, 1, 0, 0], 'd_seq': [0.1, 0.1, 0.1, 0.1, 0.1], 'd_next': 0.2},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        alpha0 = case['alpha0']\n        beta0 = case['beta0']\n        k_seq = case['k_seq']\n        d_seq = case['d_seq']\n        d_next = case['d_next']\n        \n        # Initialize posterior parameters with prior parameters\n        alpha_t = float(alpha0)\n        beta_t = float(beta0)\n        \n        # Perform online update for each observation\n        # This is equivalent to summing up all counts and durations\n        if k_seq:\n            total_k = sum(k_seq)\n            total_d = sum(d_seq)\n            alpha_t += total_k\n            beta_t += total_d\n        \n        # Calculate posterior mean firing rate (m_t)\n        # Handle the case where beta_t is zero, though unlikely with given constraints\n        if beta_t == 0:\n            m_t = float('inf') # Or handle as an error\n        else:\n            m_t = alpha_t / beta_t\n        \n        # Calculate posterior predictive probability of at least one spike (q_t)\n        if beta_t + d_next == 0:\n             # This can't happen if d_next > 0 and beta_0 >= 0\n             q_t = 1.0\n        else:\n            # The formula is 1 - (beta_t / (beta_t + d_next)) ** alpha_t\n            base = beta_t / (beta_t + d_next)\n            prob_zero_spikes = np.power(base, alpha_t)\n            q_t = 1.0 - prob_zero_spikes\n            \n        # Round results to six decimal places\n        m_t_rounded = round(m_t, 6)\n        q_t_rounded = round(q_t, 6)\n        \n        results.append([m_t_rounded, q_t_rounded])\n\n    # Format the final output string as specified\n    # Example: [[1.23,0.45],[6.78,0.90]]\n    output_str = \"[\" + \",\".join([f\"[{m},{q}]\" for m, q in results]) + \"]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Building a statistical model is only half the battle; we must also rigorously check whether it provides a good description of our data. This practice introduces a cornerstone of Bayesian model evaluation: the posterior predictive check . You will learn to assess the adequacy of the Poisson model by simulating replicated datasets from its posterior and comparing a key feature—the data's dispersion—to what the model expects, a critical step in responsible data analysis.",
            "id": "4140541",
            "problem": "You will implement a posterior predictive check for spike count data under a Poisson model using Bayes’ theorem and conditional probability. Consider an experiment with repeated trials, where each trial yields a spike count. Assume conditional independence of counts across trials given a common rate parameter. The generative model is: conditional on a latent rate parameter $\\lambda$, each spike count $y_i$ is drawn independently from a Poisson distribution with parameter $\\lambda$. Adopt a Gamma prior with shape-rate parameterization for $\\lambda$. Your task is to evaluate the model adequacy with a posterior predictive check using a dispersion statistic.\n\nStarting from the following foundational base:\n- Conditional independence: given $\\lambda$, the observations are independent and identically distributed.\n- Poisson probability mass function: for an integer $k \\ge 0$ and rate $\\lambda > 0$, the probability of $k$ is $p(k \\mid \\lambda) = \\exp(-\\lambda)\\lambda^{k}/k!$.\n- Gamma prior (shape-rate): for $\\lambda > 0$ with shape $\\alpha_0 > 0$ and rate $\\beta_0 > 0$, $p(\\lambda \\mid \\alpha_0,\\beta_0) = \\dfrac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} \\lambda^{\\alpha_0-1}\\exp(-\\beta_0 \\lambda)$.\n- Bayes’ theorem: $p(\\lambda \\mid y) \\propto p(y \\mid \\lambda)\\,p(\\lambda)$, where $p(y \\mid \\lambda)$ is the likelihood and $p(\\lambda)$ is the prior.\n\nDefine the dispersion statistic for a vector of counts $y = (y_1,\\dots,y_n)$ with sample mean $\\bar{y}$ and unbiased sample variance $s^2$ (degrees of freedom equal to $1$) as\n$$\nD(y) \\;=\\; \\frac{s^2(y)}{\\bar{y}}.\n$$\nIf $\\bar{y} = 0$, then $D(y)$ is undefined; for this problem, all provided observed datasets will have $\\bar{y} > 0$.\n\nImplement the following posterior predictive check:\n1. Compute the posterior distribution $p(\\lambda \\mid y)$ using Bayes’ theorem and the specified Gamma prior.\n2. For $S$ independent posterior predictive replicates, repeat:\n   - Draw $\\lambda^{(s)} \\sim p(\\lambda \\mid y)$.\n   - Draw a replicated dataset $y^{(s)}_{\\mathrm{rep}} = (y^{(s)}_{\\mathrm{rep},1},\\dots,y^{(s)}_{\\mathrm{rep},n})$ with $y^{(s)}_{\\mathrm{rep},i} \\mid \\lambda^{(s)} \\sim \\text{Poisson}(\\lambda^{(s)})$ independently across $i$.\n   - Compute $D\\!\\left(y^{(s)}_{\\mathrm{rep}}\\right)$ using the same definition as above. If $\\bar{y}^{(s)}_{\\mathrm{rep}} = 0$, then $D\\!\\left(y^{(s)}_{\\mathrm{rep}}\\right)$ is undefined; omit such replicates from tail probability calculations.\n3. Let $D_{\\mathrm{obs}} = D(y)$. Estimate the upper-tail posterior predictive probability\n$$\np_{\\mathrm{upper}} \\;=\\; \\mathbb{P}\\!\\left(D\\!\\left(y_{\\mathrm{rep}}\\right) \\ge D_{\\mathrm{obs}} \\,\\middle|\\, y\\right)\n$$\nby the fraction of valid replicates with $D\\!\\left(y^{(s)}_{\\mathrm{rep}}\\right) \\ge D_{\\mathrm{obs}}$. Similarly, estimate the lower-tail posterior predictive probability\n$$\np_{\\mathrm{lower}} \\;=\\; \\mathbb{P}\\!\\left(D\\!\\left(y_{\\mathrm{rep}}\\right) \\le D_{\\mathrm{obs}} \\,\\middle|\\, y\\right)\n$$\nby the fraction of valid replicates with $D\\!\\left(y^{(s)}_{\\mathrm{rep}}\\right) \\le D_{\\mathrm{obs}}$.\n\nYour program must:\n- Use the Gamma prior with shape-rate parameters $(\\alpha_0,\\beta_0)$ provided in each test case.\n- Use the unbiased sample variance with degrees of freedom equal to $1$ when computing $s^2$.\n- Use a fixed random seed of $2025$ for reproducibility.\n- Omit posterior predictive replicates with mean equal to zero from tail probability estimation; if, exceptionally, no valid replicates remain, return $p_{\\mathrm{upper}} = 0.5$ and $p_{\\mathrm{lower}} = 0.5$ for that test case.\n- Return results as decimals (not percentages).\n\nTest suite:\n- Case $1$: $y = [\\,3,\\,2,\\,5,\\,4,\\,1,\\,0,\\,6,\\,2,\\,4,\\,3\\,]$, $\\alpha_0 = 1.0$, $\\beta_0 = 1.0$, $S = 20000$.\n- Case $2$ (overdispersed relative to Poisson): $y = [\\,0,\\,7,\\,0,\\,12,\\,3,\\,9,\\,1,\\,15,\\,2,\\,0\\,]$, $\\alpha_0 = 1.0$, $\\beta_0 = 1.0$, $S = 20000$.\n- Case $3$ (underdispersed relative to Poisson): $y = [\\,5,\\,5,\\,4,\\,5,\\,6,\\,5,\\,5,\\,5,\\,5,\\,4\\,]$, $\\alpha_0 = 1.0$, $\\beta_0 = 1.0$, $S = 20000$.\n- Case $4$ (boundary behavior): $y = [\\,0,\\,1\\,]$, $\\alpha_0 = 1.0$, $\\beta_0 = 1.0$, $S = 20000$.\n- Case $5$ (strong prior influence): $y = [\\,2,\\,2\\,]$, $\\alpha_0 = 10.0$, $\\beta_0 = 5.0$, $S = 20000$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list $[p_{\\mathrm{upper}}, p_{\\mathrm{lower}}]$ with each probability rounded to six decimal places, for example:\n\"[ [p1_upper,p1_lower], [p2_upper,p2_lower], ... ]\" without extra spaces. Concretely, the exact required format is a single line:\n\"[[p1_upper,p1_lower],[p2_upper,p2_lower],[p3_upper,p3_lower],[p4_upper,p4_lower],[p5_upper,p5_lower]]\".",
            "solution": "The problem requires the implementation of a posterior predictive check for a hierarchical Poisson-Gamma model of spike count data. The objective is to assess the model's adequacy by comparing a dispersion statistic computed from the observed data to the distribution of the same statistic computed from simulated data replicated from the model's posterior predictive distribution.\n\nFirst, the Bayesian model must be formally specified. The data consist of a vector of $n$ spike counts, $y = (y_1, \\dots, y_n)$. The generative model posits that, conditional on a latent rate parameter $\\lambda > 0$, each count $y_i$ is an independent draw from a Poisson distribution. The likelihood function is therefore:\n$$\np(y \\mid \\lambda) = \\prod_{i=1}^{n} p(y_i \\mid \\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!} = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} y_i}}{\\prod_{i=1}^{n} y_i!}\n$$\nFor the rate parameter $\\lambda$, a Gamma distribution is chosen as the prior, parameterized by a shape parameter $\\alpha_0 > 0$ and a rate parameter $\\beta_0 > 0$. This choice is standard due to the conjugacy of the Gamma distribution to the Poisson likelihood. The prior probability density function is:\n$$\np(\\lambda \\mid \\alpha_0, \\beta_0) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} \\lambda^{\\alpha_0-1} e^{-\\beta_0 \\lambda}\n$$\nAccording to Bayes' theorem, the posterior distribution of $\\lambda$ is proportional to the product of the likelihood and the prior: $p(\\lambda \\mid y) \\propto p(y \\mid \\lambda) p(\\lambda)$. We can derive the functional form of the posterior by combining the terms involving $\\lambda$:\n$$\np(\\lambda \\mid y) \\propto \\left( e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} y_i} \\right) \\left( \\lambda^{\\alpha_0-1} e^{-\\beta_0 \\lambda} \\right)\n$$\n$$\np(\\lambda \\mid y) \\propto \\lambda^{(\\alpha_0 + \\sum_{i=1}^{n} y_i) - 1} e^{-(\\beta_0 + n) \\lambda}\n$$\nThis expression is the kernel of a Gamma distribution. By inspection, we can identify the parameters of the posterior distribution, which we denote as $\\alpha_n$ and $\\beta_n$:\n- Posterior shape: $\\alpha_n = \\alpha_0 + \\sum_{i=1}^{n} y_i$\n- Posterior rate: $\\beta_n = \\beta_0 + n$\nThus, the posterior distribution for $\\lambda$ is $p(\\lambda \\mid y) = \\text{Gamma}(\\lambda \\mid \\alpha_n, \\beta_n)$.\n\nThe next step is to define the test statistic for model checking. The problem specifies a dispersion statistic, $D(y)$, defined as the ratio of the sample variance to the sample mean:\n$$\nD(y) = \\frac{s^2(y)}{\\bar{y}}\n$$\nwhere $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$ is the sample mean. The term $s^2(y)$ is the unbiased sample variance. The problem statement describes this as having \"degrees of freedom equal to $1$\". This is interpreted as a slightly imprecise reference to the standard formula for unbiased sample variance which uses a divisor of $n-1$, corresponding to the `ddof=1` (Delta Degrees of Freedom) argument in numerical libraries like NumPy. This estimator is associated with a sampling distribution having $n-1$ degrees of freedom. For a sample of size $n$, the formula is:\n$$\ns^2(y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n$$\nThis interpretation is adopted as it is the standard definition for \"unbiased sample variance\". For the observed data $y$, all provided test cases have $n \\ge 2$ and $\\bar{y} > 0$, ensuring $D(y)$ is well-defined.\n\nThe posterior predictive check is performed via a simulation-based algorithm:\n1.  Compute the dispersion statistic for the observed data, $D_{\\mathrm{obs}} = D(y)$.\n2.  Generate a large number, $S$, of independent posterior predictive replicates. For each replicate $s$ from $1$ to $S$:\n    a. Draw a rate parameter $\\lambda^{(s)}$ from the posterior distribution: $\\lambda^{(s)} \\sim \\text{Gamma}(\\alpha_n, \\beta_n)$.\n    b. Generate a replicated dataset $y^{(s)}_{\\mathrm{rep}} = (y^{(s)}_{\\mathrm{rep},1}, \\dots, y^{(s)}_{\\mathrm{rep},n})$ by drawing each count independently from a Poisson distribution with the sampled rate: $y^{(s)}_{\\mathrm{rep},i} \\sim \\text{Poisson}(\\lambda^{(s)})$.\n    c. Compute the dispersion statistic for the replicate, $D(y^{(s)}_{\\mathrm{rep}})$. Replicates where the sample mean $\\bar{y}^{(s)}_{\\mathrm{rep}}$ is $0$ are considered invalid and are omitted from the subsequent analysis, as $D(y^{(s)}_{\\mathrm{rep}})$ would be undefined.\n3.  Estimate the upper-tail and lower-tail posterior predictive probabilities ($p$-values). These quantities measure the probability that the replicated data are as extreme or more extreme than the observed data, according to the chosen statistic. They are estimated as the fraction of valid replicates satisfying the conditions:\n$$\np_{\\mathrm{upper}} = \\mathbb{P}(D(y_{\\mathrm{rep}}) \\ge D_{\\mathrm{obs}} \\mid y) \\approx \\frac{1}{N_{\\mathrm{valid}}} \\sum_{s=1}^{S} \\mathbb{I}\\left(\\bar{y}^{(s)}_{\\mathrm{rep}} > 0 \\text{ and } D(y^{(s)}_{\\mathrm{rep}}) \\ge D_{\\mathrm{obs}}\\right)\n$$\n$$\np_{\\mathrm{lower}} = \\mathbb{P}(D(y_{\\mathrm{rep}}) \\le D_{\\mathrm{obs}} \\mid y) \\approx \\frac{1}{N_{\\mathrm{valid}}} \\sum_{s=1}^{S} \\mathbb{I}\\left(\\bar{y}^{(s)}_{\\mathrm{rep}} > 0 \\text{ and } D(y^{(s)}_{\\mathrm{rep}}) \\le D_{\\mathrm{obs}}\\right)\n$$\nHere, $\\mathbb{I}(\\cdot)$ is the indicator function and $N_{\\mathrm{valid}}$ is the total number of valid replicates (where $\\bar{y}^{(s)}_{\\mathrm{rep}} > 0$). If $N_{\\mathrm{valid}} = 0$, the probabilities are set to $0.5$ as per the problem specification.\n\nThis procedure will be implemented for each test case using a fixed random seed of $2025$ to ensure reproducibility of the simulation results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a posterior predictive check for spike count data modeled\n    with a Poisson-Gamma hierarchical model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'y': np.array([3, 2, 5, 4, 1, 0, 6, 2, 4, 3]), 'alpha0': 1.0, 'beta0': 1.0, 'S': 20000},\n        {'y': np.array([0, 7, 0, 12, 3, 9, 1, 15, 2, 0]), 'alpha0': 1.0, 'beta0': 1.0, 'S': 20000},\n        {'y': np.array([5, 5, 4, 5, 6, 5, 5, 5, 5, 4]), 'alpha0': 1.0, 'beta0': 1.0, 'S': 20000},\n        {'y': np.array([0, 1]), 'alpha0': 1.0, 'beta0': 1.0, 'S': 20000},\n        {'y': np.array([2, 2]), 'alpha0': 10.0, 'beta0': 5.0, 'S': 20000},\n    ]\n\n    # Initialize a random number generator with the specified seed.\n    rng = np.random.default_rng(2025)\n    \n    results = []\n\n    def dispersion_statistic(counts):\n        \"\"\"\n        Calculates the dispersion statistic D(y) = s^2(y) / y_bar.\n        Returns None if y_bar is 0 or if variance is ill-defined (n<2).\n        \"\"\"\n        n = len(counts)\n        if n < 2:\n            return 0.0 # Variance is 0 for n=1, or undefined. Treat as 0. \n                       # Problem cases all have n>=2, so this is for internal robustness.\n        \n        mean_val = np.mean(counts)\n        if mean_val == 0.0:\n            return None # Undefined\n        \n        # Use ddof=1 for unbiased sample variance (divisor n-1).\n        var_val = np.var(counts, ddof=1)\n        return var_val / mean_val\n\n    for case in test_cases:\n        y_obs = case['y']\n        alpha0 = case['alpha0']\n        beta0 = case['beta0']\n        S = case['S']\n        n = len(y_obs)\n\n        # 1. Compute observed statistic\n        d_obs = dispersion_statistic(y_obs)\n\n        # 2. Compute posterior parameters for lambda ~ Gamma(alpha_n, beta_n)\n        alpha_post = alpha0 + np.sum(y_obs)\n        beta_post = beta0 + n\n\n        # 3. Draw S samples from the posterior predictive distribution\n        # Draw lambda samples from the Gamma posterior\n        # numpy.random.gamma uses shape and scale, where scale = 1 / rate.\n        lambda_samples = rng.gamma(shape=alpha_post, scale=1.0/beta_post, size=S)\n\n        # Draw replicated datasets y_rep from Poisson(lambda_sample)\n        # Reshape lambda_samples to (S, 1) to broadcast for generating (S, n) replicates\n        y_reps = rng.poisson(lam=lambda_samples[:, np.newaxis], size=(S, n))\n        \n        # 4. Compute dispersion statistic for each replicate\n        mean_reps = np.mean(y_reps, axis=1)\n        \n        # Create a mask for valid replicates (mean > 0)\n        valid_mask = mean_reps > 0\n        \n        if not np.any(valid_mask):\n            # Special case: no valid replicates generated.\n            p_upper, p_lower = 0.5, 0.5\n        else:\n            valid_y_reps = y_reps[valid_mask]\n            valid_mean_reps = mean_reps[valid_mask]\n            \n            # Using ddof=1 for unbiased sample variance\n            var_reps = np.var(valid_y_reps, axis=1, ddof=1)\n\n            # Handle cases where mean is > 0 but var is 0 (e.g., [1, 1, 1])\n            # The calculation is still valid: D_rep will be 0.\n            d_reps = var_reps / valid_mean_reps\n            \n            # 5. Estimate tail probabilities\n            num_valid_reps = len(d_reps)\n            \n            p_upper = np.sum(d_reps >= d_obs) / num_valid_reps\n            p_lower = np.sum(d_reps <= d_obs) / num_valid_reps\n\n        # Format results for the final output string\n        results.append(f\"[{p_upper:.6f},{p_lower:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[[{','.join(results)}]]\")\n\nsolve()\n```"
        }
    ]
}