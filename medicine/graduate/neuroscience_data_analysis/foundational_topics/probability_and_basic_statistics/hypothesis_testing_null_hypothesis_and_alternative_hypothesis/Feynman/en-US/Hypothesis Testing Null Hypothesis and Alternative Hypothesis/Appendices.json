{
    "hands_on_practices": [
        {
            "introduction": "The General Linear Model (GLM) is a cornerstone of fMRI data analysis, allowing researchers to model brain activity in response to various experimental conditions. A critical skill is formulating and testing precise scientific questions within this framework. This exercise provides hands-on practice in translating a hypothesis about the differential effect of two conditions into a specific linear contrast and then calculating the corresponding $t$-statistic, a fundamental procedure in statistical parametric mapping .",
            "id": "4169094",
            "problem": "A single voxel time series from a functional Magnetic Resonance Imaging (fMRI) experiment is modeled using the General Linear Model (GLM). The design matrix has $p=4$ columns corresponding to: an intercept (baseline), a regressor encoding condition $\\mathcal{A}$ (e.g., faces), a regressor encoding condition $\\mathcal{B}$ (e.g., shapes), and a nuisance regressor for head motion. Let the number of scans be $N=240$. The fitted ordinary least squares estimator yields the coefficient vector\n$$\n\\hat{\\boldsymbol{\\beta}}=\\begin{pmatrix}510.0 \\\\ 1.5 \\\\ 0.6 \\\\ 0.02\\end{pmatrix},\n$$\nand the residual sum of squares is\n$$\n\\mathrm{RSS}=432.0.\n$$\nAssume the standard homoscedastic Gaussian noise model with errors $\\boldsymbol{\\varepsilon}\\sim \\mathcal{N}(\\boldsymbol{0},\\sigma^{2}\\boldsymbol{I})$, and that the estimator covariance is given by $\\sigma^{2}( \\boldsymbol{X}^{\\top}\\boldsymbol{X})^{-1}$, where the provided matrix\n$$\n(\\boldsymbol{X}^{\\top}\\boldsymbol{X})^{-1}=\\begin{pmatrix}\n0.002 & 0 & 0 & 0 \\\\\n0 & 0.010 & -0.004 & 0 \\\\\n0 & -0.004 & 0.012 & 0 \\\\\n0 & 0 & 0 & 0.005\n\\end{pmatrix}\n$$\nis symmetric.\n\nYou are interested in testing the experimental effect defined as the difference between the condition $\\mathcal{A}$ and condition $\\mathcal{B}$ regressors, while holding the intercept and motion regressors fixed. Formulate the null hypothesis $H_{0}$ and the alternative hypothesis $H_{1}$ for this effect using a linear contrast, construct the appropriate contrast vector $\\boldsymbol{c}$, and compute the corresponding one-degree contrast $t$-statistic for testing $H_{0}$ against $H_{1}$. Use the unbiased residual variance estimator $\\hat{\\sigma}^{2}=\\mathrm{RSS}/(N-p)$.\n\nProvide the final numerical value of the $t$-statistic. The $t$-statistic is dimensionless. Round your answer to four significant figures.",
            "solution": "The problem requires the computation of a $t$-statistic for a specific linear contrast within the framework of a General Linear Model (GLM) applied to fMRI data.\n\nThe goal is to test the hypothesis that the brain's response to condition $\\mathcal{A}$ is different from its response to condition $\\mathcal{B}$. The coefficients corresponding to these conditions are $\\beta_2$ and $\\beta_3$, respectively. The null hypothesis ($H_0$) is that there is no difference between the effects of these two conditions. The alternative hypothesis ($H_1$) is that there is a difference.\n\nMathematically, the hypotheses are:\n$$\nH_0: \\beta_2 = \\beta_3 \\quad \\text{which is equivalent to} \\quad \\beta_2 - \\beta_3 = 0\n$$\n$$\nH_1: \\beta_2 \\neq \\beta_3 \\quad \\text{which is equivalent to} \\quad \\beta_2 - \\beta_3 \\neq 0\n$$\n\nThis hypothesis can be expressed as a linear contrast of the parameter vector $\\boldsymbol{\\beta} = (\\beta_1, \\beta_2, \\beta_3, \\beta_4)^\\top$. We seek a contrast vector $\\boldsymbol{c}$ such that the null hypothesis is $\\boldsymbol{c}^\\top \\boldsymbol{\\beta} = 0$. For our hypothesis, the linear combination is $0 \\cdot \\beta_1 + 1 \\cdot \\beta_2 - 1 \\cdot \\beta_3 + 0 \\cdot \\beta_4$. Therefore, the contrast vector is:\n$$\n\\boldsymbol{c} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\n\nThe one-degree contrast $t$-statistic is given by the general formula:\n$$\nt = \\frac{\\boldsymbol{c}^\\top \\hat{\\boldsymbol{\\beta}}}{\\sqrt{\\widehat{\\mathrm{Var}}(\\boldsymbol{c}^\\top \\hat{\\boldsymbol{\\beta}})}} = \\frac{\\boldsymbol{c}^\\top \\hat{\\boldsymbol{\\beta}}}{\\sqrt{\\hat{\\sigma}^2 \\boldsymbol{c}^\\top (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{c}}}\n$$\nThe degrees of freedom for this $t$-statistic are $\\nu = N - p$.\n\nWe will compute the components of this formula.\n\n1.  **Numerator (Contrast Effect Size)**: $\\boldsymbol{c}^\\top \\hat{\\boldsymbol{\\beta}}$\n    This is the estimated difference between the coefficients for condition $\\mathcal{A}$ and $\\mathcal{B}$.\n    $$\n    \\boldsymbol{c}^\\top \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} 0 & 1 & -1 & 0 \\end{pmatrix} \\begin{pmatrix} 510.0 \\\\ 1.5 \\\\ 0.6 \\\\ 0.02 \\end{pmatrix} = (0)(510.0) + (1)(1.5) + (-1)(0.6) + (0)(0.02) = 1.5 - 0.6 = 0.9\n    $$\n\n2.  **Unbiased Estimator of Residual Variance**: $\\hat{\\sigma}^2$\n    The degrees of freedom are $\\nu = N - p = 240 - 4 = 236$. The residual sum of squares is $\\mathrm{RSS}=432.0$.\n    $$\n    \\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{N-p} = \\frac{432.0}{236}\n    $$\n    We will keep this as a fraction to maintain precision until the final step.\n\n3.  **Variance of the Contrast (scaled by $\\sigma^2$)**: $\\boldsymbol{c}^\\top (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{c}$\n    This term represents the variance of the contrast estimate, scaled by the unknown noise variance $\\sigma^2$.\n    $$\n    \\boldsymbol{c}^\\top (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{c} = \\begin{pmatrix} 0 & 1 & -1 & 0 \\end{pmatrix} \\begin{pmatrix}\n    0.002 & 0 & 0 & 0 \\\\\n    0 & 0.010 & -0.004 & 0 \\\\\n    0 & -0.004 & 0.012 & 0 \\\\\n    0 & 0 & 0 & 0.005\n    \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n    $$\n    First, we compute the product of the matrix and the contrast vector:\n    $$\n    (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{c} = \\begin{pmatrix}\n    0.002(0) + 0(1) + 0(-1) + 0(0) \\\\\n    0(0) + 0.010(1) + (-0.004)(-1) + 0(0) \\\\\n    0(0) + (-0.004)(1) + 0.012(-1) + 0(0) \\\\\n    0(0) + 0(1) + 0(-1) + 0.005(0)\n    \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0.010 + 0.004 \\\\ -0.004 - 0.012 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0.014 \\\\ -0.016 \\\\ 0 \\end{pmatrix}\n    $$\n    Next, we compute the dot product with $\\boldsymbol{c}^\\top$:\n    $$\n    \\boldsymbol{c}^\\top ((\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{c}) = \\begin{pmatrix} 0 & 1 & -1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0.014 \\\\ -0.016 \\\\ 0 \\end{pmatrix} = (0)(0) + (1)(0.014) + (-1)(-0.016) + (0)(0) = 0.014 + 0.016 = 0.030\n    $$\n\n4.  **Assemble the $t$-statistic**\n    Now we can substitute all the computed parts back into the $t$-statistic formula:\n    $$\n    t = \\frac{0.9}{\\sqrt{\\left(\\frac{432.0}{236}\\right) (0.030)}}\n    $$\n    Let's evaluate the expression in the denominator:\n    $$\n    \\hat{\\sigma}^2 \\boldsymbol{c}^\\top (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{c} = \\frac{432.0 \\times 0.030}{236} = \\frac{12.96}{236} \\approx 0.05491525\n    $$\n    The standard error of the contrast is the square root of this value:\n    $$\n    \\mathrm{SE}(\\boldsymbol{c}^\\top \\hat{\\boldsymbol{\\beta}}) = \\sqrt{\\frac{12.96}{236}} \\approx 0.23434004\n    $$\n    Finally, the $t$-statistic is:\n    $$\n    t = \\frac{0.9}{0.23434004} \\approx 3.8405026\n    $$\n\nRounding the result to four significant figures gives $3.841$.",
            "answer": "$$\n\\boxed{3.841}\n$$"
        },
        {
            "introduction": "Standard hypothesis testing aims to find evidence of a difference, but what if the scientific goal is to demonstrate that two conditions are practically equivalent? Simply failing to reject the null hypothesis of no difference is not sufficient evidence for equivalence. This exercise introduces the Two One-Sided Tests (TOST) procedure, the formally correct framework for testing for statistical equivalence, applying it to a common scenario in EEG research . This practice will equip you with the tools to rigorously argue for the absence of a meaningful effect.",
            "id": "4169101",
            "problem": "A research team investigates the amplitude of the early visual P1 component measured with electroencephalography (EEG) under two visual stimulation protocols, denoted Protocol A and Protocol B. Each protocol is applied to distinct, independent cohorts due to differing artifact rejection rates, so treat the samples as independent with unequal variances. The amplitude is measured in microvolts and is modeled as arising from approximately normal populations with unknown means and variances.\n\nFrom quality-controlled segments, the following sample summaries are obtained:\n- Protocol A: sample size $n_{A} = 120$, sample mean $\\bar{x}_{A} = 5.3$, sample standard deviation $s_{A} = 2.8$.\n- Protocol B: sample size $n_{B} = 130$, sample mean $\\bar{x}_{B} = 4.9$, sample standard deviation $s_{B} = 3.2$.\n\nTo determine whether the two protocols produce equivalent mean P1 amplitudes within a scientifically justified margin $\\Delta = 1.5$, conduct a two one-sided tests (TOST) equivalence test under the unequal-variance (Welch) framework at significance level $\\alpha = 0.05$ for the difference in population means $\\mu_{A} - \\mu_{B}$.\n\nUse the following hypothesis structure:\n- Null hypothesis (composite): $H_{0}: \\mu_{A} - \\mu_{B} \\leq -\\Delta \\ \\text{or} \\ \\mu_{A} - \\mu_{B} \\geq \\Delta$.\n- Alternative hypothesis (equivalence): $H_{1}: -\\Delta < \\mu_{A} - \\mu_{B} < \\Delta$.\n\nDefine the equivalence decision index $D$ as\n$$\nD = \\begin{cases}\n1, & \\text{if equivalence is concluded at level } \\alpha \\text{ under the TOST procedure}, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n\nCompute $D$ from the given summaries under scientifically standard assumptions for Welchâ€™s $t$-based inference. Report the final answer as the single number $D$. No rounding is required for the final answer.",
            "solution": "This problem requires conducting a two one-sided tests (TOST) equivalence test. The conclusion of equivalence, defined by the alternative hypothesis $H_1: -\\Delta < \\mu_{A} - \\mu_{B} < \\Delta$, is reached if and only if we can reject the composite null hypothesis $H_{0}: \\mu_{A} - \\mu_{B} \\leq -\\Delta \\ \\text{or} \\ \\mu_{A} - \\mu_{B} \\geq \\Delta$. This composite null is decomposed into two separate one-sided null hypotheses:\n$1$. $H_{01}: \\mu_{A} - \\mu_{B} \\geq \\Delta$\n$2$. $H_{02}: \\mu_{A} - \\mu_{B} \\leq -\\Delta$\n\nEquivalence is concluded at a significance level $\\alpha$ if both $H_{01}$ and $H_{02}$ are rejected, each at level $\\alpha$.\n\nThe problem specifies using the Welch framework for independent samples with unequal variances. The test statistic for the difference in means is Welch's $t$-statistic.\n\nFirst, we calculate the standard error of the difference in sample means, $SE_{(\\bar{x}_A - \\bar{x}_B)}$. The sample variances are $s_A^2 = 2.8^2 = 7.84$ and $s_B^2 = 3.2^2 = 10.24$.\n$$ SE_{(\\bar{x}_A - \\bar{x}_B)} = \\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}} $$\nSubstituting the given values:\n$$ SE = \\sqrt{\\frac{2.8^2}{120} + \\frac{3.2^2}{130}} = \\sqrt{\\frac{7.84}{120} + \\frac{10.24}{130}} \\approx \\sqrt{0.065333... + 0.078769...} \\approx \\sqrt{0.144102...} \\approx 0.379608 $$\n\nNext, we calculate the degrees of freedom $\\nu$ using the Welch-Satterthwaite equation:\n$$ \\nu = \\frac{\\left(\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}\\right)^2}{\\frac{(s_A^2/n_A)^2}{n_A - 1} + \\frac{(s_B^2/n_B)^2}{n_B - 1}} $$\nSubstituting the values:\n$$ \\nu = \\frac{\\left(\\frac{7.84}{120} + \\frac{10.24}{130}\\right)^2}{\\frac{(7.84/120)^2}{120 - 1} + \\frac{(10.24/130)^2}{130 - 1}} \\approx \\frac{(0.144102...)^2}{\\frac{(0.065333...)^2}{119} + \\frac{(0.078769...)^2}{129}} $$\n$$ \\nu \\approx \\frac{0.0207655}{3.5869 \\times 10^{-5} + 4.8098 \\times 10^{-5}} \\approx \\frac{0.0207655}{8.3967 \\times 10^{-5}} \\approx 247.30 $$\nFor calculating the critical value, the degrees of freedom are floored to the nearest integer, so we use $\\nu = 247$.\n\nThe critical value for a one-sided test at significance level $\\alpha = 0.05$ with $\\nu = 247$ degrees of freedom is $t_{crit} = t_{\\alpha, \\nu} = t_{0.05, 247}$. For large degrees of freedom, this value is close to the corresponding standard normal quantile $z_{0.05} \\approx 1.645$. A precise calculation yields $t_{0.05, 247} \\approx 1.6511$. We will use this value for our decision thresholds.\n\nNow we perform the two one-sided tests.\n\nTest for $H_{01}: \\mu_{A} - \\mu_{B} \\geq \\Delta$:\nThe alternative is $H_{A1}: \\mu_{A} - \\mu_{B} < \\Delta$. We test against the boundary case $\\mu_A - \\mu_B = \\Delta$.\nThe test statistic is:\n$$ t_1 = \\frac{(\\bar{x}_A - \\bar{x}_B) - \\Delta}{SE} $$\nSubstituting the values, with $\\bar{x}_A - \\bar{x}_B = 5.3 - 4.9 = 0.4$ and $\\Delta = 1.5$:\n$$ t_1 = \\frac{0.4 - 1.5}{0.379608} = \\frac{-1.1}{0.379608} \\approx -2.8977 $$\nWe reject $H_{01}$ if $t_1 < -t_{crit}$. Here, $-2.8977 < -1.6511$. Thus, we reject $H_{01}$.\n\nTest for $H_{02}: \\mu_{A} - \\mu_{B} \\leq -\\Delta$:\nThe alternative is $H_{A2}: \\mu_{A} - \\mu_{B} > -\\Delta$. We test against the boundary case $\\mu_A - \\mu_B = -\\Delta$.\nThe test statistic is:\n$$ t_2 = \\frac{(\\bar{x}_A - \\bar{x}_B) - (-\\Delta)}{SE} $$\nSubstituting the values:\n$$ t_2 = \\frac{0.4 - (-1.5)}{0.379608} = \\frac{1.9}{0.379608} \\approx 5.0051 $$\nWe reject $H_{02}$ if $t_2 > t_{crit}$. Here, $5.0051 > 1.6511$. Thus, we reject $H_{02}$.\n\nSince both one-sided null hypotheses, $H_{01}$ and $H_{02}$, are rejected at the significance level $\\alpha = 0.05$, we conclude in favor of the alternative hypothesis $H_1$. This means the difference in population means is statistically equivalent to zero within the specified margin $\\Delta = 1.5$.\n\nAccording to the problem definition, the decision index $D$ is set to $1$ if equivalence is concluded. As we have concluded equivalence, $D = 1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Neuroimaging datasets are often high-dimensional, involving tests at thousands of locations in space, time, or frequency, which massively inflates the chance of false positives. Cluster-based permutation testing is a powerful, non-parametric method to control this family-wise error rate by leveraging the inherent structure of the data. This exercise focuses on the final and most crucial step of this method: computing a corrected $p$-value by comparing an observed cluster statistic to the empirical null distribution of the maximum cluster statistic generated via permutations .",
            "id": "4169103",
            "problem": "A neuroscientist analyzing Electroencephalography (EEG) and Magnetoencephalography (MEG) time-frequency maps uses cluster-based permutation testing to control the Family-Wise Error (FWE) rate. The statistic field is thresholded, clusters are formed by spatiotemporal adjacency, and each cluster is summarized by a nonnegative \"cluster mass\" (for instance, the sum of thresholded $t$-values within the cluster). For each random relabeling of the data under the null hypothesis $H_0$ (\"no association between experimental condition and the measured signal; labels are exchangeable\"), the maximum cluster mass across the statistic field is recorded, producing an empirical permutation distribution of maximum cluster masses. Let $c_{\\text{obs}}$ denote the observed cluster mass for a particular cluster found in the original, unpermuted data. Let $\\{C^{\\max}_i\\}_{i=1}^m$ denote $m$ empirical samples of the maximum cluster mass, one for each permutation under $H_0$.\n\nYour task is to write a complete, runnable program that, for a given set of test cases, computes the FWE-corrected $p$-value for $c_{\\text{obs}}$ using the empirical permutation distribution of the maximum cluster mass. The corrected $p$-value is defined as the tail probability under $H_0$ that the maximum cluster mass is at least as large as $c_{\\text{obs}}$. In finite samples, implement a correction that prevents a zero $p$-value when $c_{\\text{obs}}$ exceeds all observed permutation maxima, and treat ties (equality) as exceedances. Express each final $p$-value as a decimal rounded to six places.\n\nThe program must implement the following, from first principles:\n\n- Precisely define the null hypothesis $H_0$ and the alternative hypothesis $H_1$, clarify why permutation samples $\\{C^{\\max}_i\\}_{i=1}^m$ are appropriate under $H_0$, and use these samples to estimate the corrected tail probability for $c_{\\text{obs}}$.\n- Compute the corrected $p$-value by counting how many permutation maxima are greater than or equal to $c_{\\text{obs}}$, and apply a finite-sample correction that also accounts for the original arrangement under $H_0$.\n- Ensure numerical robustness in boundary cases, including when $c_{\\text{obs}}$ is smaller than all permutation maxima and when it is larger than all permutation maxima, and handle ties by counting equality as exceedance.\n\nUse the following test suite. For all stochastic test cases, use a pseudorandom number generator initialized with the specified seed so that the results are deterministic:\n\n- Test case $1$ (typical, right-skewed positive masses): $m = 5000$, seed $= 12345$, draw $\\{C^{\\max}_i\\}_{i=1}^m$ independently from a Gamma distribution with shape $k = 4$ and scale $\\theta = 3$; set $c_{\\text{obs}} = 25.0$.\n- Test case $2$ (boundary where $c_{\\text{obs}}$ is at the lower extreme): $m = 100$, seed $= 202$, draw $\\{C^{\\max}_i\\}_{i=1}^m$ independently from a Lognormal distribution with parameters $\\mu = 2.0$ and $\\sigma = 0.5$ (parameters are for the underlying Normal distribution); set $c_{\\text{obs}} = 0.0$.\n- Test case $3$ (boundary where $c_{\\text{obs}}$ is beyond all permutation maxima): $m = 50$, seed $= 1$, draw $\\{C^{\\max}_i\\}_{i=1}^m$ independently from a Uniform distribution on $[0, 1]$; set $c_{\\text{obs}} = 2.0$.\n- Test case $4$ (ties present, deterministic array): let $m = 8$, $\\{C^{\\max}_i\\}_{i=1}^m = [1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 4.0, 4.0]$; set $c_{\\text{obs}} = 3.0$.\n\nYour program should produce a single line of output containing the corrected $p$-values for the four test cases, in order, formatted as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (e.g., $[0.123456,0.654321,0.000999,0.875000]$). No physical units are involved in this problem; all outputs must be dimensionless decimals.",
            "solution": "The core of the problem is to compute an FWE-corrected $p$-value for an observed statistic by comparing it against an empirical null distribution generated via permutations.\n\n**1. Formal Definition of Hypotheses**\n\nIn the context of this neuroscientific experiment, the statistical hypotheses are:\n\n-   **Null Hypothesis ($H_0$)**: There is no association between the experimental conditions and the measured neural signal. Under $H_0$, the assignment of condition labels to segments of the data is arbitrary. Consequently, any permutation of these labels is an equally plausible arrangement of the data. This property is known as exchangeability.\n\n-   **Alternative Hypothesis ($H_1$)**: There is a systematic association between the experimental conditions and the measured neural signal. The observed data are not consistent with a random arrangement of labels, suggesting the experimental manipulation had a real effect.\n\n**2. The Permutation Testing Framework for FWE Control**\n\nPermutation testing leverages the exchangeability property under $H_0$. By repeatedly shuffling the condition labels and recomputing the entire analysis pipeline, we can generate an empirical distribution of the test statistic as it would appear under the null hypothesis.\n\nTo control the FWE rate (the probability of one or more false positives across the entire dataset), the \"maximum statistic\" method is employed. The procedure is as follows:\n1.  For the original, unpermuted data, a test statistic (e.g., a $t$-value) is computed at each data point (e.g., each time-frequency-channel coordinate).\n2.  These statistics are thresholded. Spatiotemporally adjacent points exceeding the threshold are grouped into clusters.\n3.  Each cluster is assigned a \"cluster mass,\" a single non-negative value summarizing its size and strength, such as the sum of the $t$-values within it. We are given an observed cluster mass, denoted $c_{\\text{obs}}$.\n4.  The entire process is repeated $m$ times. In each iteration $i$, the condition labels are randomly permuted. This results in a new field of statistics, new clusters, and a new set of cluster masses. From this set, only the single largest cluster mass, $C^{\\max}_i$, is retained.\n5.  The collection of these maxima, $\\{C^{\\max}_i\\}_{i=1}^m$, forms the empirical null distribution of the *maximum* cluster mass.\n\n**3. Calculation of the FWE-Corrected $p$-value**\n\nThe FWE-corrected $p$-value for an observed cluster with mass $c_{\\text{obs}}$ is the estimated probability of observing a maximum cluster mass anywhere in the data that is at least as extreme as $c_{\\text{obs}}$, assuming $H_0$ is true.\n\nA naive estimate would be the proportion of permutations where the maximum cluster mass exceeded or equaled $c_{\\text{obs}}$. However, this can yield a $p$-value of $0$ if $c_{\\text{obs}}$ is larger than all sampled $C^{\\max}_i$ values, which is an undesirable artifact of finite sampling. A more principled approach, as specified in the problem, involves a correction that accounts for the finite number of permutations and includes the original data's statistic as a possible draw from the null distribution.\n\nLet $N_{\\text{exceed}}$ be the number of permutations for which the maximum cluster mass is greater than or equal to the observed cluster mass:\n$$\nN_{\\text{exceed}} = \\left| \\{ i \\in \\{1, \\dots, m\\} \\mid C^{\\max}_i \\ge c_{\\text{obs}} \\} \\right|\n$$\nThe corrected $p$-value, $p_{\\text{corr}}$, is then computed by considering a total of $m+1$ arrangements (the $m$ permutations plus the original unpermuted arrangement). The \"+$1$\" in the numerator accounts for the observed data's own statistic being at least as large as itself.\n$$\np_{\\text{corr}} = \\frac{ N_{\\text{exceed}} + 1 }{ m + 1 }\n$$\nThis formula correctly prevents $p$-values of $0$ (the minimum possible value is $1/(m+1)$) and handles ties by including cases where $C^{\\max}_i = c_{\\text{obs}}$ in the count, as stipulated.\n\n**4. Application to Test Cases**\n\nWe now apply this formula to the four specified test cases.\n\n-   **Test Case 1**:\n    Given $m = 5000$, $c_{\\text{obs}} = 25.0$. The permutation maxima $\\{C^{\\max}_i\\}_{i=1}^{5000}$ are drawn from a Gamma distribution with shape $k=4$ and scale $\\theta=3$. We generate these $5000$ values and count how many are greater than or equal to $25.0$. Let this count be $N_1$. The $p$-value is $p_1 = (N_1 + 1) / (5000 + 1)$.\n\n-   **Test Case 2**:\n    Given $m = 100$, $c_{\\text{obs}} = 0.0$. The permutation maxima $\\{C^{\\max}_i\\}_{i=1}^{100}$ are drawn from a Lognormal distribution. Since cluster masses are defined as non-negative, all generated $C^{\\max}_i$ values will be greater than or equal to $0.0$. Therefore, the count of exceedances will be $N_2 = 100$. The $p$-value is $p_2 = (100 + 1) / (100 + 1) = 1.0$. An observed mass of $0.0$ is the least extreme value possible and thus yields the highest possible $p$-value.\n\n-   **Test Case 3**:\n    Given $m = 50$, $c_{\\text{obs}} = 2.0$. The permutation maxima $\\{C^{\\max}_i\\}_{i=1}^{50}$ are drawn from a Uniform distribution on the interval $[0, 1]$. Since all generated $C^{\\max}_i$ values will be less than or equal to $1.0$, none can be greater than or equal to $c_{\\text{obs}} = 2.0$. The count of exceedances is $N_3 = 0$. The $p$-value is $p_3 = (0 + 1) / (50 + 1) = 1/51 \\approx 0.019608$. This demonstrates the finite-sample correction preventing a zero $p$-value.\n\n-   **Test Case 4**:\n    Given $m = 8$, $c_{\\text{obs}} = 3.0$, and the deterministic set of permutation maxima $\\{C^{\\max}_i\\}_{i=1}^8 = [1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 4.0, 4.0]$. We count the number of values in this array that are greater than or equal to $3.0$. The values are $\\{3.0, 3.0, 3.0, 4.0, 4.0\\}$, so the count is $N_4 = 5$. The $p$-value is $p_4 = (5 + 1) / (8 + 1) = 6/9 = 2/3 \\approx 0.666667$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes FWE-corrected p-values for cluster-based permutation testing.\n    \"\"\"\n\n    test_cases = [\n        {'type': 'gamma', 'm': 5000, 'seed': 12345, 'c_obs': 25.0, 'params': {'shape': 4, 'scale': 3}},\n        {'type': 'lognormal', 'm': 100, 'seed': 202, 'c_obs': 0.0, 'params': {'mean': 2.0, 'sigma': 0.5}},\n        {'type': 'uniform', 'm': 50, 'seed': 1, 'c_obs': 2.0, 'params': {'low': 0, 'high': 1}},\n        {'type': 'deterministic', 'm': 8, 'c_obs': 3.0, 'data': np.array([1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 4.0, 4.0])}\n    ]\n\n    results = []\n    for case in test_cases:\n        m = case['m']\n        c_obs = case['c_obs']\n        \n        if case['type'] == 'deterministic':\n            C_max_dist = case['data']\n        else:\n            # Initialize a random number generator with the specified seed for reproducibility\n            rng = np.random.default_rng(case['seed'])\n            if case['type'] == 'gamma':\n                C_max_dist = rng.gamma(\n                    shape=case['params']['shape'], \n                    scale=case['params']['scale'], \n                    size=m\n                )\n            elif case['type'] == 'lognormal':\n                C_max_dist = rng.lognormal(\n                    mean=case['params']['mean'], \n                    sigma=case['params']['sigma'], \n                    size=m\n                )\n            elif case['type'] == 'uniform':\n                C_max_dist = rng.uniform(\n                    low=case['params']['low'],\n                    high=case['params']['high'],\n                    size=m\n                )\n\n        # Count the number of permutation maxima >= observed cluster mass\n        # The problem specifies treating ties (equality) as exceedances.\n        # np.sum() on a boolean array counts the number of True values.\n        N_exceed = np.sum(C_max_dist >= c_obs)\n\n        # Calculate the FWE-corrected p-value using the standard formula for finite permutations,\n        # which prevents a p-value of zero and accounts for the original data arrangement.\n        # p_corr = (Number of exceedances + 1) / (Number of permutations + 1)\n        p_value = (N_exceed + 1) / (m + 1)\n        \n        results.append(p_value)\n\n    # Format the final output as a comma-separated list of strings,\n    # with each p-value rounded and formatted to six decimal places with trailing zeros.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}