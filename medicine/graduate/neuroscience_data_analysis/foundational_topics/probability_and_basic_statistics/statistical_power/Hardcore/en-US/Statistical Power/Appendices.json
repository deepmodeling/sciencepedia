{
    "hands_on_practices": [
        {
            "introduction": "A fundamental decision in experimental design is whether to use a within-subject (repeated measures) or a between-subject (independent groups) approach, a choice with profound implications for statistical power. This exercise provides a chance to derive and quantify the efficiency gain of a paired-samples design, demonstrating how leveraging the correlation between measurements within the same subject can substantially reduce the required sample size to detect a given effect. ",
            "id": "4196378",
            "problem": "A laboratory studying cognitive training effects on the P300 component of Event-Related Potentials (ERP) recorded with Electroencephalography (EEG) plans two alternative designs to detect a change in peak amplitude. In the repeated-measures design, the same participants are measured before and after training; in the independent-groups design, different participants are measured before and after in separate groups with equal per-group sample sizes. Let the true mean difference be $\\Delta\\mu = 1.0$ in microvolts. Empirically, the baseline variability is $\\sigma_{1} = 3.2$ in microvolts and the post-training variability is $\\sigma_{2} = 3.0$ in microvolts. The within-subject correlation between pre- and post-training amplitudes is $\\rho = 0.60$. Assume measurements across subjects are independent and approximately Gaussian, and use the large-sample normal approximation to the Student’s $t$ statistic for power calculations. Work with a two-sided significance level $\\alpha = 0.05$ and desired power $1-\\beta = 0.80$.\n\nStarting from the definitions of statistical power and the distributional properties of the mean difference estimators under both designs, derive the required per-group sample size for the independent two-sample test and the required number of pairs for the paired-samples test so that both achieve the specified $\\alpha$ and power. Then compute the ratio of the required independent per-group sample size to the required paired-samples size, thereby quantifying the efficiency gain from repeated measures.\n\nRound your final ratio to four significant figures. Express the final ratio as a unitless number.",
            "solution": "The problem requires the derivation and calculation of the ratio of required sample sizes for an independent-groups design versus a repeated-measures (paired) design to achieve a specified statistical power. The derivation will be based on the large-sample normal approximation for a two-sided hypothesis test.\n\nThe general formula for a sample size calculation in this context is:\n$$ n = \\frac{\\sigma_{unit}^2 (z_{1-\\alpha/2} + z_{1-\\beta})^2}{(\\Delta\\mu)^2} $$\nwhere $n$ is the required sample size (per group for an independent design, or number of pairs for a paired design), $\\Delta\\mu$ is the true mean difference to be detected, $\\sigma_{unit}^2$ is the variance of a single observation unit relevant to the test statistic, $\\alpha$ is the significance level, and $1-\\beta$ is the desired power. The terms $z_{1-\\alpha/2}$ and $z_{1-\\beta}$ are the critical values from the standard normal distribution corresponding to the probabilities $1-\\alpha/2$ and $1-\\beta$, respectively.\n\nThe problem specifies $\\Delta\\mu = 1.0$, $\\alpha = 0.05$, and $1-\\beta = 0.80$. The corresponding $z$-values are $z_{1-0.05/2} = z_{0.975}$ and $z_{1-0.20} = z_{0.80}$. These values, along with $\\Delta\\mu$, are common to both design calculations and will cancel out when we compute the ratio.\n\nFirst, we derive the required sample size, $n_{ind}$, for the independent-groups design. Let the pre-training group have sample size $n_{ind}$ with population variance $\\sigma_1^2$, and the post-training group have sample size $n_{ind}$ with population variance $\\sigma_2^2$. The estimator for the mean difference is $\\hat{\\Delta\\mu}_{ind} = \\bar{X}_2 - \\bar{X}_1$. Since the groups are independent, the variance of this estimator is the sum of the variances of the individual means:\n$$ \\text{Var}(\\hat{\\Delta\\mu}_{ind}) = \\text{Var}(\\bar{X}_2) + \\text{Var}(\\bar{X}_1) = \\frac{\\sigma_2^2}{n_{ind}} + \\frac{\\sigma_1^2}{n_{ind}} = \\frac{\\sigma_1^2 + \\sigma_2^2}{n_{ind}} $$\nIn our general formula, the \"unit variance\", $\\sigma_{unit, ind}^2$, corresponds to the variance component that is scaled by $1/n$. Thus, $\\sigma_{unit, ind}^2 = \\sigma_1^2 + \\sigma_2^2$. The required per-group sample size is:\n$$ n_{ind} = \\frac{(\\sigma_1^2 + \\sigma_2^2)(z_{1-\\alpha/2} + z_{1-\\beta})^2}{(\\Delta\\mu)^2} $$\n\nNext, we derive the required sample size, $n_{pair}$, for the repeated-measures (paired-samples) design. Here, we have $n_{pair}$ participants, each measured twice. We define a difference score for each participant, $D_i = X_{i2} - X_{i1}$, where $X_{i1}$ and $X_{i2}$ are a participant's pre- and post-training measurements. The estimator for the mean difference is the mean of these scores, $\\bar{D}$. The variance of a single difference score, $\\sigma_D^2$, is given by:\n$$ \\sigma_D^2 = \\text{Var}(X_2 - X_1) = \\text{Var}(X_1) + \\text{Var}(X_2) - 2\\text{Cov}(X_1, X_2) $$\nUsing the definition of correlation, $\\text{Cov}(X_1, X_2) = \\rho\\sigma_1\\sigma_2$, we have:\n$$ \\sigma_D^2 = \\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2 $$\nThe variance of the estimator $\\bar{D}$ is $\\text{Var}(\\bar{D}) = \\frac{\\sigma_D^2}{n_{pair}}$. The \"unit variance\" for this design is $\\sigma_{unit, pair}^2 = \\sigma_D^2$. The required number of pairs is:\n$$ n_{pair} = \\frac{\\sigma_D^2 (z_{1-\\alpha/2} + z_{1-\\beta})^2}{(\\Delta\\mu)^2} = \\frac{(\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2)(z_{1-\\alpha/2} + z_{1-\\beta})^2}{(\\Delta\\mu)^2} $$\n\nThe problem asks for the ratio of the required independent per-group sample size to the required paired-samples size, which is $\\frac{n_{ind}}{n_{pair}}$. We can now form this ratio using the derived expressions:\n$$ \\frac{n_{ind}}{n_{pair}} = \\frac{\\frac{(\\sigma_1^2 + \\sigma_2^2)(z_{1-\\alpha/2} + z_{1-\\beta})^2}{(\\Delta\\mu)^2}}{\\frac{(\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2)(z_{1-\\alpha/2} + z_{1-\\beta})^2}{(\\Delta\\mu)^2}} $$\nThe common factor $(z_{1-\\alpha/2} + z_{1-\\beta})^2 / (\\Delta\\mu)^2$ cancels, yielding a simplified expression for the ratio that depends only on the variances and their correlation:\n$$ \\frac{n_{ind}}{n_{pair}} = \\frac{\\sigma_1^2 + \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2} $$\nThis ratio quantifies the efficiency gain of the paired design over the independent-groups design.\n\nWe now substitute the given numerical values into this expression:\n$\\sigma_1 = 3.2$\n$\\sigma_2 = 3.0$\n$\\rho = 0.60$\n\nFirst, we compute the necessary variance and covariance terms:\n$\\sigma_1^2 = (3.2)^2 = 10.24$\n$\\sigma_2^2 = (3.0)^2 = 9.00$\nThe sum of variances is $\\sigma_1^2 + \\sigma_2^2 = 10.24 + 9.00 = 19.24$.\nThe covariance-related term is $2\\rho\\sigma_1\\sigma_2 = 2 \\times 0.60 \\times 3.2 \\times 3.0 = 1.2 \\times 9.6 = 11.52$.\n\nNow, we can calculate the numerator and denominator of the ratio:\nNumerator = $\\sigma_1^2 + \\sigma_2^2 = 19.24$\nDenominator = $\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2 = 19.24 - 11.52 = 7.72$\n\nFinally, we compute the ratio:\n$$ \\frac{n_{ind}}{n_{pair}} = \\frac{19.24}{7.72} \\approx 2.492227979... $$\nRounding the result to four significant figures, as requested, we obtain $2.492$. This means that for each participant required in the paired-samples study, one would need approximately $2.492$ participants *per group* in the independent-groups study to achieve the same statistical power, holding all other parameters constant. The total number of participants in the independent-groups study would be $2 \\times n_{ind}$, making a paired design substantially more efficient in this case.",
            "answer": "$$\\boxed{2.492}$$"
        },
        {
            "introduction": "In fMRI analysis, the General Linear Model (GLM) is a cornerstone, but its power depends critically on the experimental design. When regressors, such as those modeling overlapping neural responses, are correlated—a condition known as multicollinearity—the variance of their estimated effects can become inflated, diminishing statistical power. This practice problem delves into this issue by asking you to formally link the design matrix's condition number to the variance of a regression coefficient and compute the resulting power to detect an effect of a given size. ",
            "id": "4196301",
            "problem": "Consider a functional Magnetic Resonance Imaging (fMRI) experiment analyzed with a General Linear Model (GLM). The voxelwise time series is modeled as $y = X \\beta + \\varepsilon$, where $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^{p}$ are regression coefficients, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ is Gaussian noise with variance $\\sigma^{2}$. Suppose that $p = 2$ regressors encode overlapping Hemodynamic Response Functions (HRFs), each column of $X$ has mean zero and Euclidean norm $\\sqrt{n}$, and the singular-value condition number of $X$ is $\\kappa_{X} = s_{\\max}(X)/s_{\\min}(X)$. You are given $n = 300$, $\\sigma^{2} = 1$, $\\kappa_{X} = 5$, and a true effect size for the first regressor $\\beta_{1} = 0.30$.\n\nStarting from the GLM model assumptions and without invoking any pre-stated formulas for estimator variance or power, derive an analytic expression for $\\operatorname{Var}(\\hat{\\beta}_{1})$ in terms of $\\kappa_{X}$, $n$, and $\\sigma^{2}$ that explicitly quantifies how collinearity (via $\\kappa_{X}$) inflates $\\operatorname{Var}(\\hat{\\beta}_{1})$. Then, using this variance and a large-sample normal approximation to the two-sided $t$-test for $H_{0} : \\beta_{1} = 0$ at significance level $\\alpha = 0.05$, compute the approximate statistical power to detect the nonzero $\\beta_{1}$.\n\nRound your final numerical answer for the power to four significant figures and express it as a decimal fraction (no percentage sign).",
            "solution": "**Part 1: Derivation of $\\operatorname{Var}(\\hat{\\beta}_{1})$**\n\nThe General Linear Model (GLM) is given by $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$, $X \\in \\mathbb{R}^{n \\times p}$, $\\beta \\in \\mathbb{R}^{p}$, and the noise term $\\varepsilon$ follows a multivariate normal distribution $\\mathcal{N}(0, \\sigma^{2} I_{n})$. The Ordinary Least Squares (OLS) estimator for the regression coefficients $\\beta$ is given by:\n$$ \\hat{\\beta} = (X^T X)^{-1} X^T y $$\nThe covariance matrix of this estimator is derived as follows:\n$$ \\operatorname{Cov}(\\hat{\\beta}) = \\operatorname{Cov}((X^T X)^{-1} X^T y) $$\nSince the design matrix $X$ is considered fixed, we can write:\n$$ \\operatorname{Cov}(\\hat{\\beta}) = (X^T X)^{-1} X^T \\operatorname{Cov}(y) ((X^T X)^{-1} X^T)^T $$\nGiven that $\\operatorname{Cov}(y) = \\operatorname{Cov}(\\varepsilon) = \\sigma^2 I_n$, and noting that $(X^T X)$ is symmetric, this simplifies to:\n$$ \\operatorname{Cov}(\\hat{\\beta}) = (X^T X)^{-1} X^T (\\sigma^2 I_n) X (X^T X)^{-1} = \\sigma^2 (X^T X)^{-1} (X^T X) (X^T X)^{-1} = \\sigma^2 (X^T X)^{-1} $$\nThe variance of the specific estimator $\\hat{\\beta}_1$ is the first diagonal element of this covariance matrix:\n$$ \\operatorname{Var}(\\hat{\\beta}_{1}) = \\sigma^2 [(X^T X)^{-1}]_{11} $$\nThe problem specifies $p=2$ regressors. Let the columns of the design matrix be $x_1$ and $x_2$. We are given that each column has mean zero and a Euclidean norm of $\\sqrt{n}$. This implies $x_1^T x_1 = \\|x_1\\|_2^2 = (\\sqrt{n})^2 = n$ and $x_2^T x_2 = \\|x_2\\|_2^2 = n$.\nThe matrix $X^T X$ can be written as:\n$$ X^T X = \\begin{pmatrix} x_1^T x_1 & x_1^T x_2 \\\\ x_2^T x_1 & x_2^T x_2 \\end{pmatrix} $$\nThe off-diagonal term $x_1^T x_2$ is related to the correlation coefficient $\\rho$ between the two regressors: $\\rho = \\frac{x_1^T x_2}{\\|x_1\\|_2 \\|x_2\\|_2} = \\frac{x_1^T x_2}{\\sqrt{n}\\sqrt{n}} = \\frac{x_1^T x_2}{n}$. Thus, $x_1^T x_2 = n\\rho$.\nSubstituting these into the expression for $X^T X$:\n$$ X^T X = \\begin{pmatrix} n & n\\rho \\\\ n\\rho & n \\end{pmatrix} = n \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} $$\nNext, we find the inverse of $X^T X$:\n$$ (X^T X)^{-1} = \\left( n \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\right)^{-1} = \\frac{1}{n} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}^{-1} = \\frac{1}{n} \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1 & -\\rho \\\\ -\\rho & 1 \\end{pmatrix} $$\nFrom this, the first diagonal element is $[(X^T X)^{-1}]_{11} = \\frac{1}{n(1-\\rho^2)}$.\nThe variance is therefore:\n$$ \\operatorname{Var}(\\hat{\\beta}_{1}) = \\frac{\\sigma^2}{n(1-\\rho^2)} $$\nThe term $(1-\\rho^2)^{-1}$ is the Variance Inflation Factor (VIF). The problem requires expressing this variance in terms of the condition number $\\kappa_X$. The singular-value condition number is $\\kappa_X = s_{\\max}(X)/s_{\\min}(X)$. The singular values of $X$, denoted $s_i$, are the square roots of the eigenvalues of $X^T X$, denoted $\\lambda_i$. For $p=2$, let the eigenvalues be $\\lambda_{\\max}$ and $\\lambda_{\\min}$. Then $\\kappa_X = \\sqrt{\\lambda_{\\max}} / \\sqrt{\\lambda_{\\min}}$, which implies $\\kappa_X^2 = \\lambda_{\\max} / \\lambda_{\\min}$.\nThe eigenvalues of $X^T X = n \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ are found by solving the characteristic equation $\\det(X^T X - \\lambda I) = 0$:\n$$ (n-\\lambda)^2 - (n\\rho)^2 = 0 \\implies n-\\lambda = \\pm n\\rho \\implies \\lambda = n(1 \\mp \\rho) $$\nAssuming $\\rho \\ge 0$, as would be typical for overlapping HRFs, we have $\\lambda_{\\max} = n(1+\\rho)$ and $\\lambda_{\\min} = n(1-\\rho)$. If $\\rho < 0$, the roles of $\\mp$ are swapped, but the ratio still involves $|\\rho|$. The condition number $\\kappa_X > 1$ implies $\\rho \\neq 0$. Let's proceed with $\\rho \\ge 0$.\n$$ \\kappa_X^2 = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{n(1+\\rho)}{n(1-\\rho)} = \\frac{1+\\rho}{1-\\rho} $$\nWe solve for $\\rho$ in terms of $\\kappa_X^2$:\n$$ \\kappa_X^2(1-\\rho) = 1+\\rho \\implies \\kappa_X^2 - 1 = \\rho(1+\\kappa_X^2) \\implies \\rho = \\frac{\\kappa_X^2 - 1}{\\kappa_X^2 + 1} $$\nNow we express the VIF term, $(1-\\rho^2)^{-1}$, using $\\kappa_X$:\n$$ 1 - \\rho^2 = 1 - \\left(\\frac{\\kappa_X^2 - 1}{\\kappa_X^2 + 1}\\right)^2 = \\frac{(\\kappa_X^2 + 1)^2 - (\\kappa_X^2 - 1)^2}{(\\kappa_X^2 + 1)^2} = \\frac{(\\kappa_X^4 + 2\\kappa_X^2 + 1) - (\\kappa_X^4 - 2\\kappa_X^2 + 1)}{(\\kappa_X^2 + 1)^2} = \\frac{4\\kappa_X^2}{(\\kappa_X^2 + 1)^2} $$\nThe VIF is the reciprocal: $\\frac{1}{1-\\rho^2} = \\frac{(\\kappa_X^2 + 1)^2}{4\\kappa_X^2}$.\nSubstituting this back into the expression for variance, we obtain the desired analytical form:\n$$ \\operatorname{Var}(\\hat{\\beta}_{1}) = \\frac{\\sigma^2}{n} \\frac{(\\kappa_X^2 + 1)^2}{4\\kappa_X^2} $$\nThis expression quantifies how the variance of the estimator $\\hat{\\beta}_1$ is inflated by collinearity, as captured by the condition number $\\kappa_X$.\n\n**Part 2: Calculation of Statistical Power**\n\nWe are testing the null hypothesis $H_0: \\beta_1=0$ against the two-sided alternative $H_1: \\beta_1 \\neq 0$ at a significance level $\\alpha=0.05$. The problem states to use a large-sample normal approximation. Under this approximation, the test statistic $Z = \\hat{\\beta}_1 / \\text{SE}(\\hat{\\beta}_1)$ follows a standard normal distribution $\\mathcal{N}(0,1)$ under $H_0$. The standard error is $\\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\operatorname{Var}(\\hat{\\beta}_1)}$.\nThe rejection region for a two-sided test is $|Z| > z_{1-\\alpha/2}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. For $\\alpha=0.05$, we need $z_{0.975} \\approx 1.95996$.\n\nPower is the probability of rejecting $H_0$ given that $H_1$ is true. Here, the true effect is $\\beta_1 = 0.30$. Under $H_1$, the estimator $\\hat{\\beta}_1$ is approximately normally distributed as $\\hat{\\beta}_1 \\sim \\mathcal{N}(\\beta_1, \\operatorname{Var}(\\hat{\\beta}_1))$.\nPower is calculated as:\n$$ \\text{Power} = P(|\\hat{\\beta}_1| > z_{1-\\alpha/2} \\cdot \\text{SE}(\\hat{\\beta}_1) \\mid \\beta_1) $$\n$$ \\text{Power} = P(\\hat{\\beta}_1 > z_{1-\\alpha/2} \\cdot \\text{SE}(\\hat{\\beta}_1)) + P(\\hat{\\beta}_1 < -z_{1-\\alpha/2} \\cdot \\text{SE}(\\hat{\\beta}_1)) $$\nWe standardize the random variable $\\hat{\\beta}_1$ by subtracting its mean $\\beta_1$ and dividing by its standard deviation $\\text{SE}(\\hat{\\beta}_1)$. Let $Z' = (\\hat{\\beta}_1 - \\beta_1) / \\text{SE}(\\hat{\\beta}_1)$, where $Z' \\sim \\mathcal{N}(0,1)$.\n$$ \\text{Power} = P\\left(Z' > z_{1-\\alpha/2} - \\frac{\\beta_1}{\\text{SE}(\\hat{\\beta}_1)}\\right) + P\\left(Z' < -z_{1-\\alpha/2} - \\frac{\\beta_1}{\\text{SE}(\\hat{\\beta}_1)}\\right) $$\nLet $\\Phi$ denote the cumulative distribution function (CDF) of the standard normal distribution. The power is:\n$$ \\text{Power} = \\left(1 - \\Phi\\left(z_{1-\\alpha/2} - \\frac{\\beta_1}{\\text{SE}(\\hat{\\beta}_1)}\\right)\\right) + \\Phi\\left(-z_{1-\\alpha/2} - \\frac{\\beta_1}{\\text{SE}(\\hat{\\beta}_1)}\\right) $$\nNow we substitute the given values: $n=300$, $\\sigma^2=1$, $\\kappa_X=5$, $\\beta_1=0.30$, and $\\alpha=0.05$.\nFirst, calculate $\\operatorname{Var}(\\hat{\\beta}_1)$:\n$$ \\operatorname{Var}(\\hat{\\beta}_{1}) = \\frac{1}{300} \\frac{(5^2 + 1)^2}{4(5^2)} = \\frac{1}{300} \\frac{26^2}{4(25)} = \\frac{1}{300} \\frac{676}{100} = \\frac{676}{30000} = \\frac{169}{7500} $$\nThe standard error is:\n$$ \\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\frac{169}{7500}} = \\frac{13}{\\sqrt{7500}} = \\frac{13}{50\\sqrt{3}} $$\nNext, calculate the non-centrality term $\\frac{\\beta_1}{\\text{SE}(\\hat{\\beta}_1)}$:\n$$ \\frac{\\beta_1}{\\text{SE}(\\hat{\\beta}_1)} = \\frac{0.30}{13 / (50\\sqrt{3})} = \\frac{0.30 \\times 50\\sqrt{3}}{13} = \\frac{15\\sqrt{3}}{13} $$\nNumerically, this is $\\frac{15\\sqrt{3}}{13} \\approx 1.9985195$.\nThe critical value is $z_{0.975} = \\Phi^{-1}(0.975) \\approx 1.9599635$.\nNow we compute the arguments of the CDF $\\Phi$:\n$$ z_{1-\\alpha/2} - \\frac{\\beta_1}{\\text{SE}(\\hat{\\beta}_1)} \\approx 1.9599635 - 1.9985195 = -0.038556 $$\n$$ -z_{1-\\alpha/2} - \\frac{\\beta_1}{\\text{SE}(\\hat{\\beta}_1)} \\approx -1.9599635 - 1.9985195 = -3.958483 $$\nSubstitute these values into the power expression:\n$$ \\text{Power} = (1 - \\Phi(-0.038556)) + \\Phi(-3.958483) $$\nUsing the symmetry property $1-\\Phi(-z) = \\Phi(z)$:\n$$ \\text{Power} = \\Phi(0.038556) + \\Phi(-3.958483) $$\nUsing a standard normal CDF calculator:\n$$ \\Phi(0.038556) \\approx 0.515374 $$\n$$ \\Phi(-3.958483) \\approx 0.0000375 $$\nThe total power is the sum of these probabilities:\n$$ \\text{Power} \\approx 0.515374 + 0.0000375 = 0.5154115 $$\nRounding to four significant figures as requested, we get $0.5154$.",
            "answer": "$$\n\\boxed{0.5154}\n$$"
        },
        {
            "introduction": "While analytical formulas for power are illuminating, they are often difficult or impossible to derive for the complex, multi-level models common in neuroscience. Monte Carlo simulation provides a powerful and flexible alternative, allowing researchers to estimate power for virtually any experimental design and analysis plan. This hands-on coding exercise guides you through building a complete simulation-based power analysis workflow, from generating data under a specific alternative hypothesis to implementing a principled stopping rule to ensure the precision of your power estimate. ",
            "id": "4196312",
            "problem": "You are tasked with designing and implementing a Monte Carlo power analysis workflow for a common group-level comparison in neuroscience data analysis. The workflow must simulate data under a specified alternative hypothesis, apply the planned statistical test to each simulated dataset, and estimate statistical power as the proportion of rejections. In addition, it must include a convergence diagnostic for the Monte Carlo estimator and employ a principled stopping rule.\n\nContext and model. Consider a within-subject experiment with two conditions (for example, baseline and stimulation). For each subject $i \\in \\{1,\\dots,N\\}$, let the unobserved subject-specific true condition effect be $\\Delta_i$. Assume a group-level mean effect $\\mu$ and between-subject variability $\\tau^2$, so that $\\Delta_i$ is modeled as Gaussian with $\\Delta_i \\sim \\mathcal{N}(\\mu,\\tau^2)$. Each subject contributes $K$ trials per condition, and trial-level measurement noise is Gaussian with variance $\\sigma^2$, independent across trials and conditions. The observed subject-level difference score $d_i$ is the difference between the subject’s sample mean responses across the two conditions. Under the stated assumptions, $d_i$ can be simulated by first drawing $\\Delta_i \\sim \\mathcal{N}(\\mu,\\tau^2)$ and then adding condition-averaging noise $e_i \\sim \\mathcal{N}\\!\\left(0, \\frac{2\\sigma^2}{K}\\right)$ to obtain $d_i = \\Delta_i + e_i$. Equivalently, marginally across subjects, $d_i \\sim \\mathcal{N}\\!\\left(\\mu, \\tau^2 + \\frac{2\\sigma^2}{K}\\right)$.\n\nPlanned test. The planned group-level test is a two-sided one-sample Student $t$-test of the null hypothesis $H_0\\!:\\, \\mathbb{E}[d_i] = 0$ against the alternative $H_1\\!:\\, \\mathbb{E}[d_i] \\neq 0$, computed on the $N$ subject-level difference scores $\\{d_i\\}_{i=1}^N$, with significance level $\\alpha$. The test statistic uses the sample mean and unbiased sample standard deviation of $\\{d_i\\}$ and is compared to the Student $t$ distribution with $N-1$ degrees of freedom to obtain a two-sided $p$-value.\n\nMonte Carlo power estimation. Power is defined as the probability, under the specified alternative, that the planned test rejects $H_0$. Your program must:\n- Simulate independent replicate datasets under the specified alternative model;\n- Apply the planned test to each replicate to compute rejection indicators;\n- Estimate power as the sample mean of the rejection indicators, expressed as a decimal (not a percentage);\n- Provide a convergence diagnostic using a normal-approximate confidence interval for the Monte Carlo estimator at nominal coverage $0.95$, and implement a stopping rule based on a user-specified half-width tolerance $\\varepsilon$;\n- Continue simulating in batches until the half-width criterion is met or a maximum number of simulations $R_{\\max}$ is reached, with a minimum initial number of simulations $R_{\\min}$ before checking the stopping rule.\n\nRandomness and reproducibility. Use a fixed random seed of $12345$ to ensure reproducible results.\n\nRequired inputs and test suite. Implement your workflow for the following test suite, where each case specifies $(N, K, \\mu, \\sigma, \\tau, \\alpha, B, \\varepsilon, R_{\\max}, R_{\\min})$:\n- Case A: $N=30$, $K=80$, $\\mu=0.3$, $\\sigma=1.0$, $\\tau=0.6$, $\\alpha=0.05$, $B=1000$, $\\varepsilon=0.01$, $R_{\\max}=100000$, $R_{\\min}=1000$.\n- Case B: $N=25$, $K=80$, $\\mu=0.0$, $\\sigma=1.0$, $\\tau=0.6$, $\\alpha=0.05$, $B=1000$, $\\varepsilon=0.01$, $R_{\\max}=100000$, $R_{\\min}=1000$.\n- Case C: $N=8$, $K=20$, $\\mu=0.2$, $\\sigma=1.5$, $\\tau=0.8$, $\\alpha=0.05$, $B=1000$, $\\varepsilon=0.01$, $R_{\\max}=100000$, $R_{\\min}=1000$.\n- Case D: $N=60$, $K=100$, $\\mu=0.5$, $\\sigma=1.0$, $\\tau=0.3$, $\\alpha=0.05$, $B=1000$, $\\varepsilon=0.01$, $R_{\\max}=100000$, $R_{\\min}=1000$.\n\nConvergence diagnostic and stopping rule. After each batch, compute a normal-approximate confidence interval for the Monte Carlo estimator of power at nominal coverage $0.95$, and check whether its half-width is less than or equal to $\\varepsilon$. Only begin checking after at least $R_{\\min}$ simulations. If the half-width criterion is met, stop; otherwise continue until $R_{\\max}$ is reached. Report the final Monte Carlo standard error and the total number of simulations used. The half-width computation must be entirely internal to your program; no user input is allowed.\n\nOutput specification. For each case, report a list of three values: the estimated power (as a decimal), the Monte Carlo standard error, and the total number of simulated datasets used. Round the two floating-point quantities to $4$ decimal places, and leave the simulation count as an integer. Aggregate the four case results into a single list in the order A, B, C, D. Your program should produce a single line of output containing this aggregate list, printed as a comma-separated list enclosed in square brackets, for example, $[[p_A, s_A, r_A],[p_B, s_B, r_B],[p_C, s_C, r_C],[p_D, s_D, r_D]]$, where each $p_{\\cdot}$ and $s_{\\cdot}$ is a float and each $r_{\\cdot}$ is an integer.",
            "solution": "The solution is designed by first formalizing the data-generating process and the statistical test, and then constructing an iterative algorithm to estimate power with a specified precision.\n\n1.  **Data-Generating Process**\n    For each simulated experiment, we need to generate a dataset of $N$ subject-level difference scores $\\{d_i\\}_{i=1}^N$. According to the problem statement, the marginal distribution for a single subject's difference score $d_i$ is a Gaussian distribution. The mean of this distribution is the true group-level effect $\\mu$, and its variance is the sum of the between-subject variance $\\tau^2$ and the within-subject measurement-and-averaging variance $\\frac{2\\sigma^2}{K}$.\n    Therefore, for each simulation, we generate $N$ independent and identically distributed samples $\\{d_i\\}_{i=1}^N$ from the distribution:\n    $$\n    d_i \\sim \\mathcal{N}\\left(\\mu, \\sigma_d^2\\right) \\quad \\text{where} \\quad \\sigma_d^2 = \\tau^2 + \\frac{2\\sigma^2}{K}\n    $$\n    The parameters for this process, $(N, K, \\mu, \\sigma, \\tau)$, are specified for each test case.\n\n2.  **Statistical Test and Rejection Event**\n    For each generated dataset $\\{d_i\\}_{i=1}^N$, a two-sided one-sample Student's $t$-test is performed to test the null hypothesis $H_0: \\mathbb{E}[d_i] = 0$. The test statistic is calculated as:\n    $$\n    t = \\frac{\\bar{d}}{\\hat{s}_d / \\sqrt{N}}\n    $$\n    where $\\bar{d}$ is the sample mean of the $d_i$ values and $\\hat{s}_d$ is their unbiased sample standard deviation. This observed $t$-statistic is compared against the Student's $t$-distribution with $N-1$ degrees of freedom to compute a $p$-value. The null hypothesis is rejected if the obtained $p$-value is less than the specified significance level $\\alpha$. This rejection event is a Bernoulli trial, which we can code as an indicator variable $I_r$ for each replication $r$ ($I_r=1$ for rejection, $I_r=0$ otherwise).\n\n3.  **Monte Carlo Power Estimation and Convergence**\n    Statistical power is the probability of correctly rejecting the null hypothesis when a specific alternative hypothesis is true. In our Monte Carlo framework, we estimate this probability by the proportion of rejections across a large number of simulated experiments. If $R$ is the total number of simulations, the power estimate $\\hat{p}$ is:\n    $$\n    \\hat{p} = \\frac{1}{R} \\sum_{r=1}^R I_r\n    $$\n    The precision of this estimator increases with $R$. To quantify this precision, we use the standard error of the proportion:\n    $$\n    SE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{R}}\n    $$\n    This standard error allows us to construct a confidence interval for the true power. A normal-approximate $95\\%$ confidence interval for the true power $p$ is given by $\\hat{p} \\pm z_{0.975} \\cdot SE(\\hat{p})$, where $z_{0.975} \\approx 1.96$ is the $97.5$-th percentile of the standard normal distribution. The half-width of this interval is $HW = z_{0.975} \\cdot SE(\\hat{p})$.\n\n4.  **Algorithmic Procedure with Stopping Rule**\n    A robust simulation requires a principled stopping rule to ensure the estimate is stable without running an excessive number of simulations. The algorithm proceeds as follows:\n    - Initialize a random number generator with a fixed seed of $12345$ for reproducibility.\n    - Initialize the total number of simulations $R = 0$ and the number of rejections to $0$.\n    - Enter a loop that continues as long as $R < R_{\\max}$.\n    - Inside the loop, simulate a batch of $B$ datasets. For each dataset, perform the $t$-test and record the outcome (rejection or not).\n    - Update the total number of simulations $R$ and the total count of rejections.\n    - After the total number of simulations $R$ has reached at least $R_{\\min}$, check the convergence criterion at the end of each batch.\n    - Calculate the current power estimate $\\hat{p}$ and the half-width $HW$ of its $95\\%$ confidence interval.\n    - If $HW \\le \\varepsilon$, the desired precision has been reached. The loop terminates.\n    - If the loop completes because $R$ reaches $R_{\\max}$ without the precision criterion being met, the simulation stops, and the final results are reported based on all $R_{\\max}$ simulations.\n    - Upon termination, the final values for the estimated power $\\hat{p}$, the Monte Carlo standard error $SE(\\hat{p})$, and the total number of simulations $R$ are reported. For Case B, where $\\mu=0$, the data are simulated under the null hypothesis, so the \"power\" being estimated is, in fact, the Type I error rate, which should converge to $\\alpha=0.05$.\n\nThis entire procedure is implemented for each test case provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef run_power_simulation(\n    N, K, mu, sigma, tau, alpha, B, epsilon, R_max, R_min, rng\n):\n    \"\"\"\n    Performs a Monte Carlo power analysis for a one-sample t-test.\n\n    Args:\n        N (int): Number of subjects.\n        K (int): Number of trials per condition.\n        mu (float): Group-level mean effect.\n        sigma (float): Trial-level measurement noise standard deviation.\n        tau (float): Between-subject standard deviation of the true effect.\n        alpha (float): Significance level for the t-test.\n        B (int): Batch size for simulations.\n        epsilon (float): Desired half-width for the 95% CI of the power estimate.\n        R_max (int): Maximum number of simulations.\n        R_min (int): Minimum number of simulations before checking convergence.\n        rng (np.random.Generator): NumPy random number generator instance.\n\n    Returns:\n        tuple: A tuple containing:\n            - float: The estimated power.\n            - float: The Monte Carlo standard error of the power estimate.\n            - int: The total number of simulations run.\n    \"\"\"\n    # Calculate the variance of the observed difference scores d_i\n    # Var(d_i) = Var(Delta_i) + Var(e_i) where Delta_i ~ N(mu, tau^2)\n    # and e_i ~ N(0, 2*sigma^2/K).\n    # Since Delta_i and e_i are independent, Var(d_i) = tau^2 + 2*sigma^2/K.\n    var_d = tau**2 + 2 * sigma**2 / K\n    std_d = np.sqrt(var_d)\n\n    z_critical = stats.norm.ppf(1 - 0.05 / 2)  # For 95% CI, z_{0.975}\n\n    total_simulations = 0\n    total_rejections = 0\n\n    while total_simulations < R_max:\n        # Simulate a batch of B experiments\n        # Each experiment has N subjects.\n        # Shape: (B, N)\n        d_samples = rng.normal(loc=mu, scale=std_d, size=(B, N))\n\n        # Perform one-sample t-test for each of the B experiments.\n        # popmean is 0 for H0. axis=1 to perform tests across subjects.\n        # The alternative is 'two-sided' by default.\n        t_stat, p_values = stats.ttest_1samp(d_samples, popmean=0.0, axis=1)\n\n        # Count rejections in this batch\n        batch_rejections = np.sum(p_values < alpha)\n\n        total_rejections += batch_rejections\n        total_simulations += B\n\n        # Check for convergence if minimum simulations are met\n        if total_simulations >= R_min:\n            # Estimate power\n            p_hat = total_rejections / total_simulations\n            \n            # Handle edge cases where p_hat is 0 or 1 for SE calculation\n            if p_hat == 0.0 or p_hat == 1.0:\n                se_p_hat = 0.0\n            else:\n                se_p_hat = np.sqrt(p_hat * (1 - p_hat) / total_simulations)\n\n            # Calculate CI half-width\n            half_width = z_critical * se_p_hat\n\n            # Stop if desired precision is reached\n            if half_width <= epsilon:\n                break\n    \n    # Final calculations after loop termination\n    final_p_hat = total_rejections / total_simulations\n    if final_p_hat == 0.0 or final_p_hat == 1.0:\n        final_se_p_hat = 0.0\n    else:\n        final_se_p_hat = np.sqrt(final_p_hat * (1 - final_p_hat) / total_simulations)\n\n    return final_p_hat, final_se_p_hat, total_simulations\n\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Create a single random number generator for reproducibility.\n    # The state of this generator will be passed through the simulations.\n    rng = np.random.default_rng(12345)\n\n    # Test suite: (N, K, mu, sigma, tau, alpha, B, epsilon, R_max, R_min)\n    test_cases = [\n        # Case A\n        (30, 80, 0.3, 1.0, 0.6, 0.05, 1000, 0.01, 100000, 1000),\n        # Case B\n        (25, 80, 0.0, 1.0, 0.6, 0.05, 1000, 0.01, 100000, 1000),\n        # Case C\n        (8, 20, 0.2, 1.5, 0.8, 0.05, 1000, 0.01, 100000, 1000),\n        # Case D\n        (60, 100, 0.5, 1.0, 0.3, 0.05, 1000, 0.01, 100000, 1000),\n    ]\n\n    results = []\n    for params in test_cases:\n        p_hat, se_p, n_sims = run_power_simulation(*params, rng=rng)\n        \n        # Format the results for the current case\n        results.append(\n            f\"[{round(p_hat, 4)},{round(se_p, 4)},{n_sims}]\"\n        )\n    \n    # Print the aggregated results in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}