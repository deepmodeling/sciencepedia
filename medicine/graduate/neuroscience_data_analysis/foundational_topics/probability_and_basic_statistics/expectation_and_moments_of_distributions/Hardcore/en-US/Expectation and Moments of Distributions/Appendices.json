{
    "hands_on_practices": [
        {
            "introduction": "Many phenomena in neuroscience, from the distribution of synaptic strengths to neuronal firing rates, do not follow a simple Gaussian distribution. Instead, they often exhibit \"heavy tails,\" where extreme events are more common than expected. This practice  provides a foundational exercise in working with such distributions, using the Pareto distribution as an analytically tractable model for synaptic strengths. By deriving the probability density function from its survival function, you will directly confront the mathematical reality that for heavy-tailed systems, higher-order moments like the variance may not be finite, a critical insight for building valid statistical models of neural data.",
            "id": "4160998",
            "problem": "In a dataset of excitatory synaptic strengths recorded from layer $2/3$ pyramidal neurons, investigators report scale-invariant heavy tails: for strengths $x$ above a physiological lower bound $x_{0}  0$, the survival function obeys $\\Pr(X \\ge x) = \\left(\\frac{x_{0}}{x}\\right)^{\\alpha}$ for $x \\ge x_{0}$, with a shape parameter $\\alpha  0$. Assume that $X$ is a continuous random variable supported on $[x_{0}, \\infty)$ and that the survival function completely characterizes the distribution.\n\nUsing only core definitions from probability theory in data analysis—namely, the relationship between the probability density function $f_{X}(x)$ and the survival function $S_{X}(x)$ via $f_{X}(x) = -\\frac{d}{dx}S_{X}(x)$, the normalization condition $\\int_{x_{0}}^{\\infty} f_{X}(x)\\,dx = 1$, and the definition of the $k$th raw moment $E[X^{k}] = \\int_{x_{0}}^{\\infty} x^{k} f_{X}(x)\\,dx$—construct the probability density function consistent with the reported survival function and derive the closed-form expression for the $k$th raw moment $E[X^{k}]$. Then, determine the values of $k \\in \\mathbb{R}$ for which the $k$th moment exists (is finite).\n\nExpress your final answer as a single closed-form analytic expression in terms of $\\alpha$, $k$, and $x_{0}$. No numerical approximation or rounding is required. Return your final expression without units.",
            "solution": "The problem statement is critically validated as well-posed, scientifically grounded, and objective. The provided survival function, $S_{X}(x) = \\left(\\frac{x_{0}}{x}\\right)^{\\alpha}$ for $x \\ge x_{0}$, describes a Pareto Type I distribution, a standard model for scale-invariant phenomena observed in various scientific domains, including neuroscience. The givens are self-contained and mathematically consistent. The task is to derive the probability density function (PDF), the $k$th raw moment, and the condition for the moment's existence, using fundamental definitions from probability theory.\n\nFirst, we construct the probability density function, $f_{X}(x)$, from the given survival function, $S_{X}(x)$. The problem specifies the relationship $f_{X}(x) = -\\frac{d}{dx}S_{X}(x)$.\nThe survival function is given as:\n$$S_{X}(x) = \\left(\\frac{x_{0}}{x}\\right)^{\\alpha} = x_{0}^{\\alpha} x^{-\\alpha} \\quad \\text{for } x \\ge x_{0}$$\nWe compute the derivative of $S_{X}(x)$ with respect to $x$:\n$$\\frac{d}{dx}S_{X}(x) = \\frac{d}{dx} \\left( x_{0}^{\\alpha} x^{-\\alpha} \\right) = x_{0}^{\\alpha} (-\\alpha) x^{-\\alpha-1} = -\\alpha x_{0}^{\\alpha} x^{-(\\alpha+1)}$$\nApplying the given relationship, the PDF is:\n$$f_{X}(x) = - \\left( -\\alpha x_{0}^{\\alpha} x^{-(\\alpha+1)} \\right) = \\alpha x_{0}^{\\alpha} x^{-(\\alpha+1)}$$\nThis expression is valid for the support of the random variable, $x \\in [x_{0}, \\infty)$. For $x  x_{0}$, $f_{X}(x) = 0$.\n\nAs a necessary check for consistency, we verify that this PDF satisfies the normalization condition, $\\int_{x_{0}}^{\\infty} f_{X}(x)\\,dx = 1$.\n$$\\int_{x_{0}}^{\\infty} \\alpha x_{0}^{\\alpha} x^{-(\\alpha+1)} \\,dx = \\alpha x_{0}^{\\alpha} \\int_{x_{0}}^{\\infty} x^{-(\\alpha+1)} \\,dx$$\nThe integral is evaluated as:\n$$\\int x^{-(\\alpha+1)} \\,dx = \\frac{x^{-(\\alpha+1)+1}}{-(\\alpha+1)+1} = \\frac{x^{-\\alpha}}{-\\alpha}$$\nEvaluating the definite integral:\n$$\\alpha x_{0}^{\\alpha} \\left[ \\frac{x^{-\\alpha}}{-\\alpha} \\right]_{x_{0}}^{\\infty} = \\alpha x_{0}^{\\alpha} \\left( \\lim_{b \\to \\infty} \\frac{b^{-\\alpha}}{-\\alpha} - \\frac{x_{0}^{-\\alpha}}{-\\alpha} \\right)$$\nSince the problem states $\\alpha  0$, the limit $\\lim_{b \\to \\infty} b^{-\\alpha} = 0$. The expression becomes:\n$$\\alpha x_{0}^{\\alpha} \\left( 0 - \\frac{x_{0}^{-\\alpha}}{-\\alpha} \\right) = \\alpha x_{0}^{\\alpha} \\left( \\frac{x_{0}^{-\\alpha}}{\\alpha} \\right) = x_{0}^{\\alpha} x_{0}^{-\\alpha} = 1$$\nThe PDF is correctly normalized, confirming the consistency of the provided definitions.\n\nNext, we derive the closed-form expression for the $k$th raw moment, $E[X^{k}]$, using the definition $E[X^{k}] = \\int_{x_{0}}^{\\infty} x^{k} f_{X}(x)\\,dx$.\n$$E[X^{k}] = \\int_{x_{0}}^{\\infty} x^{k} \\left( \\alpha x_{0}^{\\alpha} x^{-(\\alpha+1)} \\right) \\,dx$$\n$$E[X^{k}] = \\alpha x_{0}^{\\alpha} \\int_{x_{0}}^{\\infty} x^{k} x^{-(\\alpha+1)} \\,dx = \\alpha x_{0}^{\\alpha} \\int_{x_{0}}^{\\infty} x^{k-\\alpha-1} \\,dx$$\nThis is an integral of a power-law function. We find the antiderivative of the integrand, assuming $k-\\alpha-1 \\neq -1$ (i.e., $k \\neq \\alpha$):\n$$\\int x^{k-\\alpha-1} \\,dx = \\frac{x^{k-\\alpha-1+1}}{k-\\alpha-1+1} = \\frac{x^{k-\\alpha}}{k-\\alpha}$$\nWe can now evaluate the definite integral:\n$$\\int_{x_{0}}^{\\infty} x^{k-\\alpha-1} \\,dx = \\left[ \\frac{x^{k-\\alpha}}{k-\\alpha} \\right]_{x_{0}}^{\\infty} = \\lim_{b \\to \\infty} \\frac{b^{k-\\alpha}}{k-\\alpha} - \\frac{x_{0}^{k-\\alpha}}{k-\\alpha}$$\nFor the moment to exist, this integral must converge to a finite value. The convergence is determined by the behavior of the term $b^{k-\\alpha}$ as $b \\to \\infty$. This limit is finite (specifically, zero) if and only if the exponent is negative:\n$$k-\\alpha  0 \\implies k  \\alpha$$\nIf this condition holds, the integral evaluates to:\n$$0 - \\frac{x_{0}^{k-\\alpha}}{k-\\alpha} = \\frac{x_{0}^{k-\\alpha}}{\\alpha-k}$$\nIf $k \\ge \\alpha$, the integral diverges, and the moment does not exist. Specifically, if $k = \\alpha$, the integrand becomes $x^{-1}$, whose integral is $\\ln(x)$, which diverges as $x \\to \\infty$. If $k  \\alpha$, the exponent $k-\\alpha$ is positive, and $x^{k-\\alpha}$ diverges as $x \\to \\infty$.\n\nThus, for the $k$th moment to be finite, we must have $k  \\alpha$. Under this condition, we substitute the result of the integral back into the expression for $E[X^{k}]$:\n$$E[X^{k}] = \\alpha x_{0}^{\\alpha} \\left( \\frac{x_{0}^{k-\\alpha}}{\\alpha-k} \\right)$$\nSimplifying the terms involving $x_0$:\n$$E[X^{k}] = \\frac{\\alpha}{\\alpha-k} x_{0}^{\\alpha} x_{0}^{k-\\alpha} = \\frac{\\alpha}{\\alpha-k} x_{0}^{\\alpha+k-\\alpha} = \\frac{\\alpha}{\\alpha-k} x_{0}^{k}$$\nThis expression represents the $k$th raw moment of the distribution, which is defined for all real numbers $k$ such that $k  \\alpha$.\n\nIn summary, the probability density function is $f_{X}(x) = \\alpha x_{0}^{\\alpha} x^{-(\\alpha+1)}$ on $[x_0, \\infty)$, and the $k$th raw moment, $E[X^k]$, exists if and only if $k  \\alpha$. The closed-form expression for the moment, under this condition, is derived above.",
            "answer": "$$\\boxed{\\frac{\\alpha}{\\alpha-k} x_{0}^{k}}$$"
        },
        {
            "introduction": "Building on the concept of heavy tails, we now turn to a more general and powerful framework: stable distributions. These distributions naturally arise when modeling systems driven by the superposition of many independent inputs, a common scenario in neural circuits. This exercise  uses the characteristic function—a tool more general than the moment-generating function—to analyze a system where the mean fluctuation is well-defined but the variance is infinite. Understanding the implications is crucial, as it renders many standard analysis techniques that rely on second-order statistics, such as sample variance or Principal Component Analysis, fundamentally unreliable.",
            "id": "4160959",
            "problem": "A neural circuit receives a superposition of many independent synaptic inputs within a short time window, resulting in aggregate postsynaptic current fluctuations that are empirically heavy-tailed and approximately stable under aggregation. Consider modeling a single-window fluctuation by a symmetric strictly stable law with tail index $\\alpha \\in (1,2)$. In order to construct a concrete and analytically tractable example, take $\\alpha = \\tfrac{3}{2}$ and define a random variable $X$ by its characteristic function\n$$\n\\varphi_{X}(t) \\equiv \\mathbb{E}[\\exp(i t X)] = \\exp\\!\\big(i \\mu t - \\sigma^{3/2} |t|^{3/2}\\big),\n$$\nwhere $\\sigma  0$ is a scale parameter and $\\mu \\in \\mathbb{R}$ is a location parameter. This is a symmetric $\\alpha$-stable model frequently used to capture heavy-tailed synaptic fluctuations.\n\nStarting only from the definitions of expectation and characteristic function, and using well-tested facts about the asymptotic tail behavior of stable laws, do the following:\n- Derive $\\mathbb{E}[X]$ and state whether $\\mathbb{E}[X^{2}]$ exists for this model. Justify your conclusions by explicit reasoning from the characteristic function and from asymptotic tail behavior.\n- Briefly explain the implication of your conclusion for variance-based analysis of synaptic fluctuations (for example, methods that rely on finite second moments).\n\nExpress your final answer as a single closed-form analytic expression for $\\mathbb{E}[X]$ in terms of $\\mu$. No rounding is required, and no physical units are to be reported in the final answer.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- **Model:** Aggregate postsynaptic current fluctuations in a neural circuit.\n- **Distribution:** Symmetric strictly stable law.\n- **Tail Index:** $\\alpha = \\frac{3}{2}$, where the general case is $\\alpha \\in (1,2)$.\n- **Random Variable:** $X$.\n- **Characteristic Function:** $\\varphi_{X}(t) \\equiv \\mathbb{E}[\\exp(i t X)] = \\exp(i \\mu t - \\sigma^{3/2} |t|^{3/2})$.\n- **Parameters:** Scale parameter $\\sigma  0$, location parameter $\\mu \\in \\mathbb{R}$.\n- **Tasks:**\n    1. Derive $\\mathbb{E}[X]$ starting from the definitions of expectation and characteristic function.\n    2. Determine if $\\mathbb{E}[X^{2}]$ exists.\n    3. Justify conclusions using both the characteristic function and the asymptotic tail behavior of stable laws.\n    4. Explain the implication for variance-based analysis.\n- **Final Answer Requirement:** A single closed-form analytic expression for $\\mathbb{E}[X]$ in terms of $\\mu$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is scientifically and mathematically sound. Stable distributions are a cornerstone of probability theory for modeling heavy-tailed phenomena. The given characteristic function is the standard form for a symmetric $\\alpha$-stable distribution with a location shift. The use of such a model for synaptic current fluctuations is a recognized approach in theoretical neuroscience. The value $\\alpha = \\frac{3}{2}$ is a valid tail index within the specified range $(1,2)$.\n- **Well-Posed:** The problem is well-posed. The distribution is unambiguously defined by its characteristic function. The tasks are specific and ask for the derivation of standard statistical properties (moments), which can be uniquely determined from the provided information.\n- **Objective:** The language is formal, precise, and devoid of subjective or opinion-based statements.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is a standard exercise in a graduate-level probability or statistical physics course, properly contextualized within neuroscience. The solution process will now proceed.\n\n### Derivation of $\\mathbb{E}[X]$ and Analysis of $\\mathbb{E}[X^{2}]$\n\nA fundamental property linking the characteristic function $\\varphi_X(t)$ of a random variable $X$ to its moments is that the $n$-th moment $\\mathbb{E}[X^n]$ can be calculated from the $n$-th derivative of $\\varphi_X(t)$ at $t=0$, provided this derivative exists:\n$$\n\\mathbb{E}[X^n] = i^{-n} \\frac{d^n \\varphi_X(t)}{dt^n} \\bigg|_{t=0}\n$$\nThe existence of the $n$-th derivative of $\\varphi_X(t)$ at $t=0$ is a necessary and sufficient condition for the existence of the $n$-th moment, $\\mathbb{E}[X^n]$.\n\n**1. Calculation of the First Moment, $\\mathbb{E}[X]$**\n\nWe compute the first derivative of the given characteristic function, $\\varphi_{X}(t) = \\exp(i \\mu t - \\sigma^{3/2} |t|^{3/2})$.\nUsing the chain rule, for $t \\neq 0$:\n$$\n\\frac{d\\varphi_X(t)}{dt} = \\varphi_X(t) \\cdot \\frac{d}{dt} \\left( i \\mu t - \\sigma^{3/2} |t|^{3/2} \\right)\n$$\nThe derivative of the term $|t|^{3/2}$ must be handled carefully. For $t \\neq 0$, its derivative is $\\frac{3}{2}|t|^{1/2} \\text{sgn}(t)$, where $\\text{sgn}(t)$ is the sign function.\nThus, for $t \\neq 0$:\n$$\n\\frac{d\\varphi_X(t)}{dt} = \\exp(i \\mu t - \\sigma^{3/2} |t|^{3/2}) \\cdot \\left( i \\mu - \\sigma^{3/2} \\frac{3}{2} |t|^{1/2} \\text{sgn}(t) \\right)\n$$\nTo find the derivative at $t=0$, we must examine the limit of this expression as $t \\to 0$.\n$$\n\\lim_{t \\to 0} \\frac{d\\varphi_X(t)}{dt} = \\lim_{t \\to 0} \\left[ \\exp(i \\mu t - \\sigma^{3/2} |t|^{3/2}) \\cdot \\left( i \\mu - \\sigma^{3/2} \\frac{3}{2} |t|^{1/2} \\text{sgn}(t) \\right) \\right]\n$$\nAs $t \\to 0$, the first term $\\exp(i \\mu t - \\sigma^{3/2} |t|^{3/2}) \\to \\exp(0) = 1$. The second term's behavior is determined by the term $|t|^{1/2}$, which approaches $0$.\nSo, the limit is:\n$$\n\\lim_{t \\to 0} \\frac{d\\varphi_X(t)}{dt} = 1 \\cdot (i \\mu - 0) = i \\mu\n$$\nSince the limit exists, the first derivative of $\\varphi_X(t)$ at $t=0$ is well-defined and equals $i\\mu$. Therefore, the first moment exists.\n$$\n\\mathbb{E}[X] = i^{-1} \\frac{d\\varphi_X(t)}{dt} \\bigg|_{t=0} = i^{-1} (i\\mu) = \\mu\n$$\nThe expectation of the random variable $X$ is equal to the location parameter $\\mu$.\n\n**2. Existence of the Second Moment, $\\mathbb{E}[X^2]$**\n\nWe must now assess the existence of the second derivative of $\\varphi_X(t)$ at $t=0$. We differentiate the expression for $\\frac{d\\varphi_X(t)}{dt}$ using the product rule:\n$$\n\\frac{d^2\\varphi_X(t)}{dt^2} = \\frac{d}{dt} \\left[ \\varphi_X(t) \\cdot \\left( i \\mu - \\frac{3\\sigma^{3/2}}{2} |t|^{1/2} \\text{sgn}(t) \\right) \\right]\n$$\n$$\n\\frac{d^2\\varphi_X(t)}{dt^2} = \\frac{d\\varphi_X(t)}{dt} \\left( i \\mu - \\frac{3\\sigma^{3/2}}{2} |t|^{1/2} \\text{sgn}(t) \\right) + \\varphi_X(t) \\frac{d}{dt} \\left( i \\mu - \\frac{3\\sigma^{3/2}}{2} |t|^{1/2} \\text{sgn}(t) \\right)\n$$\nLet's analyze the second term as $t \\to 0$. We need the derivative of $|t|^{1/2} \\text{sgn}(t)$. This function is equivalent to $t^{1/2}$ for $t  0$ and $-(-t)^{1/2}$ for $t  0$.\nFor $t  0$, the derivative is $\\frac{1}{2} t^{-1/2}$.\nFor $t  0$, the derivative is $-\\frac{1}{2}(-t)^{-1/2}(-1) = \\frac{1}{2}(-t)^{-1/2}$.\nIn both cases, for $t \\neq 0$, the derivative is $\\frac{1}{2}|t|^{-1/2}$.\nThe derivative of the second part of the product is therefore:\n$$\n\\frac{d}{dt} \\left( i \\mu - \\frac{3\\sigma^{3/2}}{2} |t|^{1/2} \\text{sgn}(t) \\right) = - \\frac{3\\sigma^{3/2}}{2} \\frac{1}{2}|t|^{-1/2} = - \\frac{3\\sigma^{3/2}}{4|t|^{1/2}}\n$$\nAs $t \\to 0$, this term diverges to $-\\infty$. Because one of the terms in the expression for the second derivative diverges at $t=0$, the second derivative $\\frac{d^2\\varphi_X(t)}{dt^2}$ does not exist at $t=0$. Consequently, the second moment $\\mathbb{E}[X^2]$ does not exist as a finite value.\n\n**3. Justification from Asymptotic Tail Behavior**\n\nThe problem states that $X$ follows a symmetric stable law with tail index $\\alpha = \\frac{3}{2}$. A well-tested fact for such distributions is that the probability density function $p(x)$ has an asymptotic power-law tail:\n$$\np(x) \\sim c|x|^{-(\\alpha+1)} \\quad \\text{as } |x| \\to \\infty,\n$$\nfor some positive constant $c$. Here, with $\\alpha = \\frac{3}{2}$, we have $p(x) \\sim c|x|^{-5/2}$.\n\nThe $k$-th absolute moment, $\\mathbb{E}[|X|^k]$, is finite if and only if the integral $\\int_{-\\infty}^\\infty |x|^k p(x) dx$ converges. Due to symmetry, this is equivalent to the convergence of $2\\int_0^\\infty x^k p(x) dx$.\nFor large $x$, the integrand behaves as $x^k \\cdot c x^{-(\\alpha+1)} = c x^{k-\\alpha-1}$.\nThe integral converges if and only if the exponent is less than $-1$, i.e., $k-\\alpha-1  -1$, which simplifies to $k  \\alpha$.\n\nLet's apply this condition to our specific case where $\\alpha = \\frac{3}{2}$:\n- For the first moment, $k=1$. The condition is $1  \\frac{3}{2}$, which is true. Therefore, $\\mathbb{E}[|X|]$ is finite, which implies that $\\mathbb{E}[X]$ is also finite and well-defined. This confirms our prior result.\n- For the second moment, $k=2$. The condition is $2  \\frac{3}{2}$, which is false. Since the condition is not met, the integral for $\\mathbb{E}[|X|^2]$ diverges. This means $\\mathbb{E}[|X|^2] = \\infty$, and therefore the second moment $\\mathbb{E}[X^2]$ does not exist (is infinite). This independently confirms the conclusion from the characteristic function analysis.\n\n**4. Implication for Variance-Based Analysis**\n\nThe variance of a random variable is defined as $\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. We have established that $\\mathbb{E}[X] = \\mu$ is finite, but $\\mathbb{E}[X^2]$ is infinite. Therefore, the variance of the synaptic current fluctuations, under this model, is infinite.\n\nThis has profound implications for data analysis. Many standard statistical methods are fundamentally premised on the existence of a finite second moment (and thus, finite variance). Examples include:\n- **Sample variance and standard deviation:** These statistics will not converge to a stable value as more data is collected. Instead, they will tend to grow with the sample size, and their values will be highly sensitive to extreme events (the \"heavy tails\").\n- **Least-squares optimization:** Methods like linear regression that minimize squared error are optimal under assumptions of finite variance (e.g., the Gauss-Markov theorem). For heavy-tailed data, their estimates can be severely biased by outliers.\n- **Principal Component Analysis (PCA):** This method is based on the covariance matrix, which contains variances on its diagonal. If the variance is infinite, the covariance matrix is undefined, and PCA is not applicable.\n- **Signal-to-Noise Ratio (SNR):** When defined in terms of signal and noise power (variance), this metric becomes ill-defined.\n\nIn conclusion, for synaptic fluctuations accurately described by this heavy-tailed stable model, any analysis pipeline that relies on finite variance is mathematically unsound and will produce unstable, unreliable, and potentially misleading results. Analysis must instead use tools appropriate for heavy-tailed distributions. Dispersion should be characterized by an alternative measure, such as the scale parameter $\\sigma$ of the stable distribution or robust statistics like the interquartile range.",
            "answer": "$$\n\\boxed{\\mu}\n$$"
        },
        {
            "introduction": "While the previous exercises focused on the existence of moments, this practice shifts our attention to their practical computation, especially when analytical solutions are unavailable. In neuroscience, we frequently model a neuron's output as a nonlinear transformation of its membrane potential, $V$. This exercise  tackles the common problem of computing the expected output, $E[f(V)]$, for a nonlinear function $f$ and a Gaussian-distributed input $V$. You will implement a powerful numerical technique, Gaussian-Hermite quadrature, to approximate the expectation integral and analyze its sensitivity to model parameters, developing an essential skill for the analysis and simulation of realistic neural models.",
            "id": "4160986",
            "problem": "Consider a model of neuronal input-output transformation in which the membrane potential $V$ is a random variable with a Gaussian distribution $V \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and the instantaneous output is a bounded nonlinearity $f(\\beta V) = \\tanh(\\beta V)$, where $\\beta$ is a gain parameter controlling sensitivity. The goal is to compute the expectation $E[\\tanh(\\beta V)]$ and to analyze its sensitivity with respect to the gain parameter $\\beta$.\n\nStarting from the foundational definition of expectation for a continuous random variable and the Probability Density Function (PDF) of the normal distribution, the expectation of a measurable function $g(V)$ is defined as\n$$\nE[g(V)] = \\int_{-\\infty}^{\\infty} g(v)\\, \\phi(v; \\mu, \\sigma^2)\\, dv,\n$$\nwhere\n$$\n\\phi(v; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(v - \\mu)^2}{2\\sigma^2}\\right)\n$$\nis the normal PDF. In this problem, $g(v) = \\tanh(\\beta v)$. You must compute $E[\\tanh(\\beta V)]$ using numerical quadrature based on a change of variables that reduces the integral to one with the weight $e^{-x^2}$, allowing approximation by Gaussian-Hermite quadrature. Do not use any closed-form shortcuts; derive and implement the quadrature method from the above core definitions.\n\nAdditionally, define the local sensitivity of $E[\\tanh(\\beta V)]$ to $\\beta$ as the derivative\n$$\nS(\\beta) = \\frac{d}{d\\beta} E[\\tanh(\\beta V)],\n$$\nand compute it using the same quadrature framework by differentiating under the integral sign. Validate this derivative by an independent finite-difference approximation\n$$\nS_{\\text{fd}}(\\beta) = \\frac{E[\\tanh((\\beta + \\Delta) V)] - E[\\tanh((\\beta - \\Delta) V)]}{2\\Delta},\n$$\nwith a small step $\\Delta$ that you must choose to balance truncation and rounding error. Report also the absolute discrepancy\n$$\n\\varepsilon = \\left| S(\\beta) - S_{\\text{fd}}(\\beta) \\right|.\n$$\n\nAll quantities here are dimensionless. Your implementation must be numerically stable for a broad range of parameters. Use a fixed quadrature order for all cases, justified by the properties of the integrand and the normal weight.\n\nTest Suite:\nCompute the tuple of results $\\left[E[\\tanh(\\beta V)],\\, S(\\beta),\\, S_{\\text{fd}}(\\beta),\\, \\varepsilon\\right]$ for each of the following parameter sets $(\\mu, \\sigma, \\beta)$:\n1. $(\\mu, \\sigma, \\beta) = (0.5, 1.0, 1.2)$\n2. $(\\mu, \\sigma, \\beta) = (0.0, 1.0, 10.0)$\n3. $(\\mu, \\sigma, \\beta) = (3.0, 0.5, 2.0)$\n4. $(\\mu, \\sigma, \\beta) = (-1.0, 0.2, 5.0)$\n5. $(\\mu, \\sigma, \\beta) = (1.234, 10^{-6}, 0.75)$\n6. $(\\mu, \\sigma, \\beta) = (2.0, 1.5, 0.0)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a comma-separated list of the four floating-point values for a test case, in the order specified above. For example, the output format must be\n$$\n[\\,[E_1, S_1, S_{\\text{fd},1}, \\varepsilon_1],\\, [E_2, S_2, S_{\\text{fd},2}, \\varepsilon_2],\\, \\dots\\, ].\n$$\nNo additional text or whitespace beyond what is needed to form a valid Python list literal should be printed.",
            "solution": "The problem requires the numerical computation of the expectation $E[\\tanh(\\beta V)]$ and its sensitivity $S(\\beta) = \\frac{d}{d\\beta}E[\\tanh(\\beta V)]$, where the membrane potential $V$ is a Gaussian-distributed random variable, $V \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The core of the task is to derive and implement a solution based on Gaussian-Hermite quadrature.\n\nThe expectation of a function $g(V)$ of a continuous random variable $V$ with probability density function (PDF) $\\phi(v)$ is given by:\n$$ E[g(V)] = \\int_{-\\infty}^{\\infty} g(v) \\phi(v; \\mu, \\sigma^2) dv $$\nFor this problem, $g(v) = \\tanh(\\beta v)$ and $\\phi(v; \\mu, \\sigma^2)$ is the Gaussian PDF:\n$$ \\phi(v; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(v - \\mu)^2}{2\\sigma^2}\\right) $$\nThe expectation integral is therefore:\n$$ E[\\tanh(\\beta V)] = \\int_{-\\infty}^{\\infty} \\tanh(\\beta v) \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(v - \\mu)^2}{2\\sigma^2}\\right) dv $$\n\nTo utilize Gaussian-Hermite quadrature, which is designed for integrals of the form $\\int_{-\\infty}^{\\infty} h(x)e^{-x^2}dx$, we must perform a change of variables. We set the argument of the exponential to $-x^2$:\n$$ x^2 = \\frac{(v - \\mu)^2}{2\\sigma^2} \\implies x = \\frac{v - \\mu}{\\sqrt{2}\\sigma} $$\nThis gives the following transformation for $v$ and its differential $dv$:\n$$ v = \\mu + \\sqrt{2}\\sigma x $$\n$$ dv = \\sqrt{2}\\sigma dx $$\nSubstituting these into the expectation integral yields:\n$$ E[\\tanh(\\beta V)] = \\int_{-\\infty}^{\\infty} \\tanh(\\beta (\\mu + \\sqrt{2}\\sigma x)) \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-x^2} (\\sqrt{2}\\sigma dx) $$\nSimplifying the expression, we find:\n$$ E[\\tanh(\\beta V)] = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\infty} \\tanh(\\beta(\\mu + \\sqrt{2}\\sigma x)) e^{-x^2} dx $$\nThis integral is now in a form suitable for Gaussian-Hermite quadrature. The quadrature rule approximates the integral as a weighted sum:\n$$ \\int_{-\\infty}^{\\infty} f(x) e^{-x^2} dx \\approx \\sum_{i=1}^n w_i f(x_i) $$\nwhere $x_i$ are the roots (nodes) of the physicist's Hermite polynomial of degree $n$, and $w_i$ are the corresponding weights. Applying this rule to our expectation, with $f(x) = \\frac{1}{\\sqrt{\\pi}} \\tanh(\\beta(\\mu + \\sqrt{2}\\sigma x))$, we get the numerical approximation:\n$$ E[\\tanh(\\beta V)] \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{n} w_i \\tanh(\\beta(\\mu + \\sqrt{2}\\sigma x_i)) $$\n\nNext, we compute the sensitivity $S(\\beta) = \\frac{d}{d\\beta} E[\\tanh(\\beta V)]$. We can apply the Leibniz integral rule (differentiation under the integral sign) because the integrand and its partial derivative with respect to $\\beta$ are continuous and the integral converges uniformly.\n$$ S(\\beta) = \\frac{d}{d\\beta} \\int_{-\\infty}^{\\infty} \\tanh(\\beta v) \\phi(v) dv = \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial\\beta} \\left[ \\tanh(\\beta v) \\right] \\phi(v) dv $$\nThe partial derivative is $\\frac{\\partial}{\\partial\\beta} \\tanh(\\beta v) = v \\cdot \\text{sech}^2(\\beta v)$, where $\\text{sech}(z) = 1/\\cosh(z)$. This leads to:\n$$ S(\\beta) = \\int_{-\\infty}^{\\infty} v \\, \\text{sech}^2(\\beta v) \\, \\phi(v) dv = E[V \\, \\text{sech}^2(\\beta V)] $$\nWe can compute this new expectation using the same quadrature framework. The function to be integrated is now $g_S(v) = v \\, \\text{sech}^2(\\beta v)$. Applying the same change of variables $v = \\mu + \\sqrt{2}\\sigma x$:\n$$ S(\\beta) \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{n} w_i \\left[ (\\mu + \\sqrt{2}\\sigma x_i) \\, \\text{sech}^2(\\beta(\\mu + \\sqrt{2}\\sigma x_i)) \\right] $$\nFor the special case $\\beta = 0$, the expressions simplify. $E[\\tanh(0 \\cdot V)] = E[0] = 0$. The sensitivity becomes $S(0) = E[V \\cdot \\text{sech}^2(0)] = E[V \\cdot 1] = E[V] = \\mu$. These analytical results provide a valuable check for the numerical implementation.\n\nThe numerical implementation uses the following design choices:\n1.  **Quadrature Order**: A fixed order of $n=64$ is chosen for the Gaussian-Hermite quadrature. The integrands are infinitely differentiable ($C^\\infty$) functions. For such smooth functions, Gaussian quadrature exhibits exponential convergence. A high order of $n=64$ ensures high precision across all provided test cases, including those with parameters (like large $\\beta$) that result in steeper integrands, without incurring a prohibitive computational cost. The quadrature nodes $x_i$ and weights $w_i$ are pre-computed using `scipy.special.roots_hermite`.\n2.  **Finite Difference Validation**: The analytical derivative $S(\\beta)$ is validated against a central finite-difference approximation:\n    $$ S_{\\text{fd}}(\\beta) = \\frac{E[\\tanh((\\beta + \\Delta) V)] - E[\\tanh((\\beta - \\Delta) V)]}{2\\Delta} $$\n    The step size is chosen as $\\Delta = 10^{-6}$. For double-precision floating-point arithmetic, this value is small enough to minimize the truncation error, which is of order $\\mathcal{O}(\\Delta^2)$, while being large enough to avoid significant round-off error from catastrophic cancellation, which scales as $\\mathcal{O}(\\epsilon_{machine}/\\Delta)$.\n3.  **Numerical Stability**: The term $\\text{sech}^2(z) = 1/\\cosh^2(z)$ is implemented directly. For large arguments $z$, $\\cosh(z)$ can overflow. However, `numpy.cosh` correctly evaluates to infinity (`inf`) for large inputs, and $1/\\text{inf}^2$ correctly evaluates to $0.0$, ensuring the computation is numerically stable without requiring special-cased logic for the tails.\n\nThe final tuple of results to be computed for each parameter set $(\\mu, \\sigma, \\beta)$ is $[E[\\tanh(\\beta V)], S(\\beta), S_{\\text{fd}}(\\beta), \\varepsilon]$, where $\\varepsilon = |S(\\beta) - S_{\\text{fd}}(\\beta)|$ is the absolute discrepancy between the analytical derivative and its finite-difference approximation.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_hermite\n\ndef solve():\n    \"\"\"\n    Computes expectation and sensitivity of a transformed Gaussian random variable.\n    \"\"\"\n    # Define the test suite of parameters (mu, sigma, beta).\n    test_cases = [\n        (0.5, 1.0, 1.2),\n        (0.0, 1.0, 10.0),\n        (3.0, 0.5, 2.0),\n        (-1.0, 0.2, 5.0),\n        (1.234, 1e-6, 0.75),\n        (2.0, 1.5, 0.0),\n    ]\n\n    # Set numerical parameters for the computation.\n    # A high quadrature order is chosen for accuracy with smooth integrands.\n    QUADRATURE_ORDER = 64\n    # Delta for finite difference is chosen to balance truncation and round-off error.\n    DELTA = 1e-6\n\n    # Pre-compute Gaussian-Hermite quadrature nodes and weights for efficiency.\n    x_nodes, w_nodes = roots_hermite(QUADRATURE_ORDER)\n    SQRT_PI = np.sqrt(np.pi)\n    SQRT_2 = np.sqrt(2.0)\n\n    def calculate_expectation(func_of_v, mu, sigma):\n        \"\"\"\n        Computes E[func_of_v(V)] for V ~ N(mu, sigma^2) using Gauss-Hermite quadrature.\n        \"\"\"\n        # Change of variables: v = mu + sqrt(2)*sigma*x\n        v_vals = mu + SQRT_2 * sigma * x_nodes\n        integrand_vals = func_of_v(v_vals)\n        \n        # Apply quadrature rule: sum(w_i * f(x_i))\n        integral_approx = np.sum(w_nodes * integrand_vals)\n        \n        # Final expectation includes the 1/sqrt(pi) prefactor from the derivation.\n        return integral_approx / SQRT_PI\n\n    results = []\n    for mu, sigma, beta in test_cases:\n        # 1. Compute E[tanh(beta*V)]\n        if beta == 0.0:\n            E_val = 0.0\n        else:\n            f_tanh = lambda v: np.tanh(beta * v)\n            E_val = calculate_expectation(f_tanh, mu, sigma)\n\n        # 2. Compute S(beta) = E[V * sech^2(beta*V)]\n        if beta == 0.0:\n            # Analytical result: S(0) = E[V] = mu\n            S_val = mu\n        else:\n            f_sensitivity = lambda v: v / np.cosh(beta * v)**2\n            S_val = calculate_expectation(f_sensitivity, mu, sigma)\n\n        # 3. Compute S_fd(beta) using central finite difference\n        beta_plus = beta + DELTA\n        f_tanh_plus = lambda v: np.tanh(beta_plus * v)\n        E_plus = calculate_expectation(f_tanh_plus, mu, sigma)\n        \n        beta_minus = beta - DELTA\n        f_tanh_minus = lambda v: np.tanh(beta_minus * v)\n        E_minus = calculate_expectation(f_tanh_minus, mu, sigma)\n        \n        S_fd_val = (E_plus - E_minus) / (2 * DELTA)\n        \n        # 4. Compute absolute discrepancy epsilon\n        eps_val = np.abs(S_val - S_fd_val)\n\n        results.append([E_val, S_val, S_fd_val, eps_val])\n\n    # Format the output as a string representing a list of lists with no spaces.\n    outer_parts = []\n    for res in results:\n        inner_parts = [repr(val) for val in res]\n        outer_parts.append(f\"[{','.join(inner_parts)}]\")\n    \n    print(f\"[{','.join(outer_parts)}]\")\n\nsolve()\n```"
        }
    ]
}