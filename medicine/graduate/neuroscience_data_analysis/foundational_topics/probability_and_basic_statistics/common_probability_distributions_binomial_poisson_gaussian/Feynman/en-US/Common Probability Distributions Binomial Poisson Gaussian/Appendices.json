{
    "hands_on_practices": [
        {
            "introduction": "While the Poisson distribution provides a foundational model for neuronal spike counts, real data often exhibits 'overdispersion'—variance that exceeds the mean. This practice explores a powerful solution: a hierarchical Gamma-Poisson mixture model. By treating the underlying firing rate $\\lambda$ not as a fixed constant but as a random variable, we can account for latent biological fluctuations and derive the Negative Binomial distribution, a more flexible model for count data. ",
            "id": "4146731",
            "problem": "A neuronal spike count in a short analysis bin can be modeled as a draw from a Poisson distribution with an instantaneous rate. In realistic scenarios, the rate fluctuates across repeated trials due to latent modulators. One principled way to model this fluctuation is to assume that the latent rate itself is random. Consider a hierarchical model in which the latent rate $\\lambda$ is random across trials and the observed count $N$ conditional on $\\lambda$ is Poisson.\n\nYou are asked to demonstrate, from first principles, how this hierarchical model leads to a negative binomial distribution for $N$, and then to implement a simulator that uses this mixture construction to generate synthetic spike counts. The simulator should then be validated against the theoretical mean and variance implied by the model for several test cases that probe different regimes of dispersion, including a regime approaching the Poisson limit and a regime with strong overdispersion, which are scientifically relevant for neuronal variability analysis.\n\nUse the following foundational base only: the probability mass function of a Poisson random variable, the probability density function of a Gamma random variable, properties of the Gamma function, the law of total probability, and the law of total expectation and variance. Do not use any target formulas for the negative binomial distribution in your derivations; instead, derive the required expressions from these foundational elements.\n\nTasks to be completed:\n\n- Start from the conditional model $N \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$ and the prior $\\lambda \\sim \\mathrm{Gamma}(k,\\beta)$ where $k>0$ is the shape and $\\beta>0$ is the rate. Using only the law of total probability and the definitions of the Poisson probability mass function and Gamma probability density function, derive the marginal distribution of $N$ and show that it is a negative binomial distribution parameterized by $k$ and a success probability $p$ expressed in terms of $\\beta$. Then, using only the law of total expectation and the law of total variance, derive expressions for $\\mathbb{E}[N]$ and $\\mathrm{Var}(N)$ in terms of $k$ and $\\beta$.\n\n- Interpret the mixture sampling procedure as sampling latent rate fluctuations: explain how drawing $\\lambda$ from a Gamma distribution models across-trial rate variability and how the subsequent Poisson sampling models within-trial counting noise.\n\n- Design and implement an algorithm to simulate $T$ independent draws of $N$ under this hierarchical model by first sampling $\\lambda$ and then sampling $N \\mid \\lambda$. The implementation must accept the tuple $(k,\\beta,T,\\mathrm{seed})$ and return the empirical mean and variance of the simulated counts along with their theoretical counterparts and a pass/fail boolean based on relative error tolerances.\n\n- Validation and output specification. For each test case, simulate counts with a fixed pseudo-random seed and compute:\n  - the empirical mean $\\hat{m}$,\n  - the empirical variance $\\hat{v}$,\n  - the theoretical mean $m_\\mathrm{th}$,\n  - the theoretical variance $v_\\mathrm{th}$,\n  - the empirical variance-to-mean ratio $r_\\mathrm{emp}=\\hat{v}/\\hat{m}$,\n  - the theoretical variance-to-mean ratio $r_\\mathrm{th}=v_\\mathrm{th}/m_\\mathrm{th}$.\n  Use relative error tolerances of $\\tau_m=0.02$ for the mean and $\\tau_v=0.05$ for the variance. A test case passes if both $\\lvert \\hat{m}-m_\\mathrm{th}\\rvert/m_\\mathrm{th}\\le \\tau_m$ and $\\lvert \\hat{v}-v_\\mathrm{th}\\rvert/v_\\mathrm{th}\\le \\tau_v$.\n\nTest suite to be implemented and evaluated:\n\n- Case A (general overdispersion): $k=10.0$, $\\beta=2.0$, $T=120000$, $\\mathrm{seed}=12345$.\n- Case B (near-Poisson limit via small latent variance): $k=1000.0$, $\\beta=200.0$, $T=200000$, $\\mathrm{seed}=24680$.\n- Case C (strong overdispersion, heavy-tailed rates): $k=0.5$, $\\beta=0.5$, $T=150000$, $\\mathrm{seed}=13579$.\n\nFinal output format requirement:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponding to a test case must itself be a list of the form $[\\hat{m},\\hat{v},m_\\mathrm{th},v_\\mathrm{th},r_\\mathrm{emp},r_\\mathrm{th},\\mathrm{pass}]$ where the entries are in that exact order and types are floats for the numerical quantities and a boolean for the pass indicator. The final output should therefore look like a single list of three inner lists, one per test case, for example $[[\\cdots],[\\cdots],[\\cdots]]$ with no additional text. No physical units or angles are involved in this task, and all quantities are dimensionless.",
            "solution": "The problem requires a demonstration from first principles that a Gamma-Poisson mixture model for neuronal spike counts results in a Negative Binomial distribution. This involves deriving the marginal probability mass function (PMF) of the spike count $N$, as well as its theoretical mean and variance. Further, a simulation must be implemented to validate these theoretical results. The entire process is grounded in fundamental principles of probability theory.\n\nFirst, we provide a conceptual interpretation of the hierarchical model. The model is specified in two stages:\n$1$. The latent firing rate $\\lambda$ is drawn from a Gamma distribution: $\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$, where $k>0$ is the shape parameter and $\\beta>0$ is the rate parameter. This stage models the slow, trial-to-trial fluctuations in neuronal excitability. In experimental settings, factors like attention, motivation, or slow neuromodulatory changes can cause the average firing rate of a neuron to vary across repeated trials. The Gamma distribution is a flexible and mathematically convenient choice for modeling this non-negative, continuous rate variable. The variance of this distribution, $\\mathrm{Var}(\\lambda) = k/\\beta^2$, quantifies the magnitude of this across-trial rate variability.\n$2$. Conditional on a specific rate $\\lambda$ for a given trial, the observed spike count $N$ is drawn from a Poisson distribution: $N \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$. This stage models the inherent stochasticity of spike generation. For a fixed underlying neuronal state (i.e., a fixed $\\lambda$), the process of emitting action potentials is a random, point-like process in time, which is well-approximated by a Poisson process. The variance of this process is equal to its mean, $\\mathrm{Var}(N \\mid \\lambda) = \\lambda$. This is often termed \"counting noise\" or \"within-trial\" variability.\n\nBy combining these two sources of randomness, the model captures a key feature of real neuronal data: overdispersion, where the total variance of spike counts across trials is greater than the mean count.\n\nNext, we derive the marginal distribution of $N$. The problem states the conditional distribution of the count $N$ given the rate $\\lambda$ is Poisson, and the prior distribution of the rate $\\lambda$ is Gamma. The respective probability functions are:\n- Poisson PMF: $P(N=n \\mid \\lambda) = \\frac{\\lambda^n e^{-\\lambda}}{n!}$ for $n \\in \\{0, 1, 2, \\dots\\}$.\n- Gamma PDF: $p(\\lambda \\mid k, \\beta) = \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} e^{-\\beta\\lambda}$ for $\\lambda > 0$.\n\nTo find the marginal PMF of $N$, we apply the law of total probability, integrating over all possible values of the latent variable $\\lambda$:\n$$P(N=n) = \\int_{0}^{\\infty} P(N=n \\mid \\lambda) \\, p(\\lambda \\mid k, \\beta) \\, d\\lambda$$\nSubstituting the specific forms of the Poisson PMF and Gamma PDF:\n$$P(N=n) = \\int_{0}^{\\infty} \\left( \\frac{\\lambda^n e^{-\\lambda}}{n!} \\right) \\left( \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} e^{-\\beta\\lambda} \\right) d\\lambda$$\nWe can group the terms that do not depend on $\\lambda$ outside the integral:\n$$P(N=n) = \\frac{\\beta^k}{n! \\Gamma(k)} \\int_{0}^{\\infty} \\lambda^{n+k-1} e^{-(\\beta+1)\\lambda} d\\lambda$$\nThe integral has the form of the kernel of a Gamma distribution. Recall that for a Gamma distribution with shape $\\alpha$ and rate $\\delta$, the integral of its PDF is 1, which implies:\n$$\\int_{0}^{\\infty} x^{\\alpha-1} e^{-\\delta x} dx = \\frac{\\Gamma(\\alpha)}{\\delta^\\alpha}$$\nIn our integral, we can identify the shape as $\\alpha = n+k$ and the rate as $\\delta = \\beta+1$. Applying this formula, the integral evaluates to:\n$$\\int_{0}^{\\infty} \\lambda^{(n+k)-1} e^{-(\\beta+1)\\lambda} d\\lambda = \\frac{\\Gamma(n+k)}{(\\beta+1)^{n+k}}$$\nSubstituting this result back into the expression for $P(N=n)$:\n$$P(N=n) = \\frac{\\beta^k}{n! \\Gamma(k)} \\frac{\\Gamma(n+k)}{(\\beta+1)^{n+k}}$$\nTo show this is a Negative Binomial distribution, we rearrange the terms using the identity $\\binom{n+r-1}{n} = \\frac{\\Gamma(n+r)}{n! \\Gamma(r)}$ with $r=k$:\n$$P(N=n) = \\frac{\\Gamma(n+k)}{n! \\Gamma(k)} \\left(\\frac{\\beta}{\\beta+1}\\right)^k \\left(\\frac{1}{\\beta+1}\\right)^n = \\binom{n+k-1}{n} \\left(\\frac{\\beta}{\\beta+1}\\right)^k \\left(\\frac{1}{\\beta+1}\\right)^n$$\nThis is the PMF of a Negative Binomial distribution with parameters $r=k$ and a success probability $p = \\frac{\\beta}{\\beta+1}$, where $n$ represents the number of failures before $r$ successes. Thus, we have shown that $N \\sim \\mathrm{NB}(k, p = \\frac{\\beta}{\\beta+1})$.\n\nNext, we derive the theoretical mean ($m_\\mathrm{th}$) and variance ($v_\\mathrm{th}$) of $N$ using the laws of total expectation and variance.\nThe law of total expectation states: $\\mathbb{E}[N] = \\mathbb{E}[\\mathbb{E}[N \\mid \\lambda]]$.\n- The inner expectation is the mean of a Poisson distribution with parameter $\\lambda$: $\\mathbb{E}[N \\mid \\lambda] = \\lambda$.\n- The outer expectation is the mean of $\\lambda$, which follows a $\\mathrm{Gamma}(k, \\beta)$ distribution. So, $\\mathbb{E}[\\lambda] = k/\\beta$.\n- Therefore, the theoretical mean of $N$ is:\n$$m_\\mathrm{th} = \\mathbb{E}[N] = \\mathbb{E}[\\lambda] = \\frac{k}{\\beta}$$\n\nThe law of total variance states: $\\mathrm{Var}(N) = \\mathbb{E}[\\mathrm{Var}(N \\mid \\lambda)] + \\mathrm{Var}(\\mathbb{E}[N \\mid \\lambda])$.\n- For the first term, $\\mathbb{E}[\\mathrm{Var}(N \\mid \\lambda)]$: The variance of a Poisson distribution is equal to its mean, so $\\mathrm{Var}(N \\mid \\lambda) = \\lambda$. The expectation is therefore $\\mathbb{E}[\\lambda] = k/\\beta$.\n- For the second term, $\\mathrm{Var}(\\mathbb{E}[N \\mid \\lambda])$: We established that $\\mathbb{E}[N \\mid \\lambda] = \\lambda$. This term is thus $\\mathrm{Var}(\\lambda)$. The variance of a $\\mathrm{Gamma}(k, \\beta)$ distribution is shape/rate$^2$, so $\\mathrm{Var}(\\lambda) = k/\\beta^2$.\n- Summing the two terms gives the theoretical variance of $N$:\n$$v_\\mathrm{th} = \\mathrm{Var}(N) = \\frac{k}{\\beta} + \\frac{k}{\\beta^2}$$\nThis confirms that the variance is greater than the mean, as $k/\\beta^2 > 0$.\n\nFinally, the theoretical variance-to-mean ratio is:\n$$r_\\mathrm{th} = \\frac{v_\\mathrm{th}}{m_\\mathrm{th}} = \\frac{k/\\beta + k/\\beta^2}{k/\\beta} = 1 + \\frac{1}{\\beta}$$\nThis ratio, also known as the Fano factor, is a measure of dispersion. As $\\beta \\to \\infty$, $r_\\mathrm{th} \\to 1$, and the model approaches the Poisson limit.\n\nThe implementation will simulate this two-stage process, calculate the empirical statistics ($\\hat{m}$, $\\hat{v}$), compare them against the theoretical values ($m_\\mathrm{th}$, $v_\\mathrm{th}$), and report a pass/fail status for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation_and_validation(k, beta, T, seed):\n    \"\"\"\n    Simulates spike counts from a Gamma-Poisson mixture model and validates\n    empirical statistics against theoretical predictions.\n\n    Args:\n        k (float): Shape parameter of the Gamma distribution for the rate λ.\n        beta (float): Rate parameter of the Gamma distribution for the rate λ.\n        T (int): Number of independent trials to simulate.\n        seed (int): Seed for the pseudo-random number generator.\n\n    Returns:\n        list: A list containing the following values in order:\n              [empirical mean, empirical variance, theoretical mean,\n               theoretical variance, empirical variance-to-mean ratio,\n               theoretical variance-to-mean ratio, pass/fail boolean].\n    \"\"\"\n    # Set up the random number generator with a specific seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # --- Simulation Step ---\n    # Step 1: Sample T latent rates from the Gamma distribution.\n    # numpy.random.Generator.gamma uses a scale parameter, which is 1/rate.\n    scale = 1.0 / beta\n    lambda_samples = rng.gamma(shape=k, scale=scale, size=T)\n\n    # Step 2: For each sampled rate, sample a spike count from the Poisson distribution.\n    n_samples = rng.poisson(lam=lambda_samples)\n\n    # --- Analysis and Validation Step ---\n    # Calculate empirical statistics from the simulated counts\n    m_hat = np.mean(n_samples)\n    # Use ddof=1 for the unbiased sample variance estimate\n    v_hat = np.var(n_samples, ddof=1)\n    \n    # Avoid division by zero if the empirical mean is zero\n    if m_hat == 0:\n        r_emp = np.nan\n    else:\n        r_emp = v_hat / m_hat\n\n    # Calculate theoretical statistics based on the derived formulas\n    m_th = k / beta\n    v_th = (k / beta) + (k / (beta**2))\n    \n    if m_th == 0:\n        r_th = np.nan\n    else:\n        r_th = v_th / m_th\n\n    # Define validation tolerances\n    tau_m = 0.02  # Relative error tolerance for the mean\n    tau_v = 0.05  # Relative error tolerance for the variance\n\n    # Check if the simulation passes the validation criteria\n    # Handle cases where theoretical mean/variance could be zero to avoid division by zero\n    passed = True\n    if m_th > 0:\n        err_m = abs(m_hat - m_th) / m_th\n        if err_m > tau_m:\n            passed = False\n    elif m_hat != 0: # If m_th is 0, m_hat should also be 0\n        passed = False\n        \n    if v_th > 0:\n        err_v = abs(v_hat - v_th) / v_th\n        if err_v > tau_v:\n            passed = False\n    elif v_hat != 0: # If v_th is 0, v_hat should also be 0\n        passed = False\n\n    return [m_hat, v_hat, m_th, v_th, r_emp, r_th, passed]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (k, beta, T, seed)\n    test_cases = [\n        (10.0, 2.0, 120000, 12345),   # Case A: General overdispersion\n        (1000.0, 200.0, 200000, 24680), # Case B: Near-Poisson limit\n        (0.5, 0.5, 150000, 13579),    # Case C: Strong overdispersion\n    ]\n\n    results = []\n    for case in test_cases:\n        k, beta, T, seed = case\n        result_case = run_simulation_and_validation(k, beta, T, seed)\n        results.append(result_case)\n\n    outer_list_str = []\n    for res in results:\n        # Convert boolean to lowercase 'true'/'false' if needed, but Python's default str() is acceptable.\n        inner_list_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]},{res[5]},{str(res[6]).lower()}]\"\n        outer_list_str.append(inner_list_str)\n\n    print(f\"[{','.join(outer_list_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A central goal in analyzing neural data is to estimate the underlying firing rate $\\lambda$ of a neuron. This exercise moves from modeling to inference, asking: how much information does a spike count actually contain about its generating rate? You will derive the Fisher information from first principles, a core concept in statistics that establishes the theoretical best-case precision for any unbiased estimator of $\\lambda$, revealing how our certainty improves as we observe for longer. ",
            "id": "4146747",
            "problem": "In a spike-counting experiment in systems neuroscience, a single neuron is modeled as emitting spikes according to a homogeneous Poisson process with unknown constant rate parameter $\\lambda$ (spikes per second). You observe the total spike count $K$ over a single continuous observation window of duration $T$ (seconds). Assume that, conditional on $\\lambda$, the count $K$ is a Poisson random variable with mean $\\lambda T$. Use only fundamental definitions and well-tested facts to derive the Fisher information about $\\lambda$ contained in $K$.\n\nStart from the following foundational elements: the probability mass function of a Poisson random variable, the definition of the likelihood and log-likelihood for observed data, the definition of Fisher information as the expectation of the negative second derivative of the log-likelihood with respect to the parameter, and standard moment identities for the Poisson distribution that can be justified from its definition. Do not assume or invoke any specialized result about Fisher information for the Poisson distribution without derivation.\n\nAdditionally, suppose the observation window is partitioned into $N$ independent equal-duration bins, each of length $\\Delta$ such that $T = N \\Delta$, and let $K_{1},\\dots,K_{N}$ denote the spike counts in the bins. Under the homogeneous Poisson process model, justify how information additivity across independent observations recovers the same expression you derived for the single-window case, and interpret how Fisher information scales with the total observation time $T$ for spike rate estimation.\n\nProvide your final answer as a single closed-form analytic expression in terms of $\\lambda$ and $T$ only. Do not include units in your final answer. No numerical rounding is required for this problem.",
            "solution": "The problem requires the derivation of the Fisher information about the rate parameter $\\lambda$ of a homogeneous Poisson process, given a spike count $K$ over an observation duration $T$. The derivation must proceed from first principles.\n\nFirst, we establish the statistical model. The spike count $K$ in a time interval of duration $T$ is a random variable following a Poisson distribution with mean $\\mu = \\lambda T$. The probability mass function (PMF) for observing $k$ spikes is given by:\n$$P(K=k | \\lambda) = \\frac{(\\lambda T)^k \\exp(-\\lambda T)}{k!}$$\nfor $k \\in \\{0, 1, 2, \\dots\\}$.\n\nFor an observed count $K$, the likelihood function $L(\\lambda|K)$ is defined as the PMF viewed as a function of the parameter $\\lambda$:\n$$L(\\lambda|K) = P(K=K | \\lambda) = \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!}$$\nThe log-likelihood function, $\\ell(\\lambda|K) = \\ln L(\\lambda|K)$, is often more convenient for differentiation:\n$$\\ell(\\lambda|K) = \\ln\\left( \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!} \\right)$$\nUsing the properties of the logarithm, this simplifies to:\n$$\\ell(\\lambda|K) = K \\ln(\\lambda T) - \\lambda T - \\ln(K!)$$\n$$\\ell(\\lambda|K) = K(\\ln \\lambda + \\ln T) - \\lambda T - \\ln(K!)$$\n\nThe Fisher information, $I(\\lambda)$, is defined as the expectation of the negative second derivative of the log-likelihood function with respect to the parameter $\\lambda$. We first compute the derivatives. The first derivative, known as the score function, is:\n$$\\frac{\\partial \\ell}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left[ K\\ln \\lambda + K\\ln T - \\lambda T - \\ln(K!) \\right]$$\nTreating $K$ and $T$ as constants with respect to the differentiation by $\\lambda$:\n$$\\frac{\\partial \\ell}{\\partial \\lambda} = \\frac{K}{\\lambda} - T$$\n\nNext, we compute the second derivative:\n$$\\frac{\\partial^2 \\ell}{\\partial \\lambda^2} = \\frac{\\partial}{\\partial \\lambda} \\left( \\frac{K}{\\lambda} - T \\right) = -\\frac{K}{\\lambda^2}$$\n\nThe Fisher information $I(\\lambda)$ is then calculated by taking the expectation of the negative of this second derivative. The expectation $E[\\cdot]$ is taken with respect to the distribution of the data, which in this case is the random variable $K \\sim \\text{Poisson}(\\lambda T)$.\n$$I(\\lambda) = E\\left[ - \\frac{\\partial^2 \\ell}{\\partial \\lambda^2} \\right] = E\\left[ - \\left( -\\frac{K}{\\lambda^2} \\right) \\right] = E\\left[ \\frac{K}{\\lambda^2} \\right]$$\nSince $\\lambda$ is a constant with respect to the expectation over $K$, we can write:\n$$I(\\lambda) = \\frac{1}{\\lambda^2} E[K]$$\nThe expectation of a Poisson random variable with mean $\\lambda T$ is precisely its mean. Thus, $E[K] = \\lambda T$. Substituting this into the equation for $I(\\lambda)$:\n$$I(\\lambda) = \\frac{1}{\\lambda^2} (\\lambda T) = \\frac{T}{\\lambda}$$\nThis is the Fisher information about $\\lambda$ contained in the single observation $K$.\n\nNow, we address the second part of the problem. The observation window of total duration $T$ is partitioned into $N$ independent, equal-duration bins, each of length $\\Delta = T/N$. Let $K_1, \\dots, K_N$ be the spike counts in these respective bins. For a homogeneous Poisson process, the spike counts in disjoint time intervals are independent random variables. The count $K_i$ in each bin $i$ follows a Poisson distribution with mean $\\lambda \\Delta$.\n\nThe total likelihood for the set of observations $\\{K_1, \\dots, K_N\\}$ is the product of the individual likelihoods due to their independence:\n$$L_{total}(\\lambda | K_1, \\dots, K_N) = \\prod_{i=1}^N L_i(\\lambda | K_i)$$\nConsequently, the total log-likelihood is the sum of the individual log-likelihoods:\n$$\\ell_{total}(\\lambda | K_1, \\dots, K_N) = \\ln\\left( \\prod_{i=1}^N L_i(\\lambda | K_i) \\right) = \\sum_{i=1}^N \\ln(L_i(\\lambda | K_i)) = \\sum_{i=1}^N \\ell_i(\\lambda | K_i)$$\nThe Fisher information for this set of independent observations is derived from the total log-likelihood. Due to the linearity of differentiation and expectation, the total Fisher information is the sum of the Fisher information from each independent observation:\n$$I_{total}(\\lambda) = E\\left[ -\\frac{\\partial^2 \\ell_{total}}{\\partial \\lambda^2} \\right] = E\\left[ -\\sum_{i=1}^N \\frac{\\partial^2 \\ell_i}{\\partial \\lambda^2} \\right] = \\sum_{i=1}^N E\\left[ -\\frac{\\partial^2 \\ell_i}{\\partial \\lambda^2} \\right] = \\sum_{i=1}^N I_i(\\lambda)$$\nEach $I_i(\\lambda)$ is the Fisher information from a single observation $K_i$ over a duration $\\Delta$. We can use our previous result, replacing $T$ with $\\Delta$:\n$$I_i(\\lambda) = \\frac{\\Delta}{\\lambda}$$\nThe total information is the sum over all $N$ bins:\n$$I_{total}(\\lambda) = \\sum_{i=1}^N \\frac{\\Delta}{\\lambda} = N \\frac{\\Delta}{\\lambda}$$\nSince the total duration is $T = N \\Delta$, we can substitute this back:\n$$I_{total}(\\lambda) = \\frac{N\\Delta}{\\lambda} = \\frac{T}{\\lambda}$$\nThis result confirms the principle of information additivity for independent observations and recovers the same expression derived for the single, unpartitioned observation window.\n\nThe interpretation of the result $I(\\lambda) = T/\\lambda$ is that the amount of information the data provides about the rate parameter $\\lambda$ scales linearly with the total observation time $T$. This is intuitive: a longer observation period provides more data, which in turn allows for a more precise estimation of the underlying rate. The Cramér-Rao Lower Bound, which states that the variance of any unbiased estimator $\\hat{\\lambda}$ is bounded by $\\text{Var}(\\hat{\\lambda}) \\ge 1/I(\\lambda)$, implies $\\text{Var}(\\hat{\\lambda}) \\ge \\lambda/T$. This shows that the minimum achievable estimation variance decreases inversely with observation time, meaning the precision of the rate estimate improves as the square root of the observation time.",
            "answer": "$$\\boxed{\\frac{T}{\\lambda}}$$"
        },
        {
            "introduction": "We now shift from analyzing the timing of spikes to identifying their origin in recordings containing multiple neurons—a task known as spike sorting. This practice introduces the Gaussian Mixture Model (GMM), where spike features from each neuron are modeled as a distinct Gaussian cluster in a high-dimensional space. By deriving the update rules for the Expectation-Maximization (EM) algorithm, you will unpack the mechanism of one of the most fundamental unsupervised learning algorithms used in neuroscience. ",
            "id": "4146702",
            "problem": "Extracellular recordings from a cortical microelectrode array yield multiunit activity (MUA), comprising spike events from multiple neurons simultaneously. For each detected spike event, a feature vector is extracted by standard preprocessing, for example, projecting the waveform onto the first several principal components obtained by Principal Component Analysis (PCA). Let there be $N$ events, with feature vectors $\\mathbf{x}_{n} \\in \\mathbb{R}^{d}$ for $n \\in \\{1,\\dots,N\\}$. Assume each event arises from one of $K$ latent neuronal sources, and that, conditional on source $k \\in \\{1,\\dots,K\\}$, the feature distribution is multivariate Gaussian with isotropic covariance. Specifically, posit the Gaussian mixture model\n$$\np(\\mathbf{x}_{n} \\mid \\boldsymbol{\\Theta}) \\;=\\; \\sum_{k=1}^{K} \\pi_{k} \\,\\mathcal{N}\\!\\big(\\mathbf{x}_{n} \\,\\big|\\, \\boldsymbol{\\mu}_{k}, \\sigma_{k}^{2}\\mathbf{I}\\big),\n$$\nwhere $\\boldsymbol{\\Theta} = \\big\\{ \\{\\pi_{k}\\}_{k=1}^{K}, \\{\\boldsymbol{\\mu}_{k}\\}_{k=1}^{K}, \\{\\sigma_{k}^{2}\\}_{k=1}^{K} \\big\\}$, $\\pi_{k} \\in (0,1)$ are mixture weights satisfying $\\sum_{k=1}^{K} \\pi_{k} = 1$, $\\boldsymbol{\\mu}_{k} \\in \\mathbb{R}^{d}$ are component mean vectors, and $\\sigma_{k}^{2} > 0$ are component variances. The multivariate Gaussian density with isotropic covariance is defined by\n$$\n\\mathcal{N}\\!\\big(\\mathbf{x} \\,\\big|\\, \\boldsymbol{\\mu}, \\sigma^{2}\\mathbf{I}\\big) \\;=\\; (2\\pi \\sigma^{2})^{-\\tfrac{d}{2}} \\exp\\!\\Big(-\\tfrac{1}{2\\sigma^{2}} \\big\\|\\mathbf{x}-\\boldsymbol{\\mu}\\big\\|^{2}\\Big).\n$$\nUsing the maximum likelihood principle and the definition of the multivariate Gaussian density, and introducing latent indicator variables $z_{nk} \\in \\{0,1\\}$ with $\\sum_{k=1}^{K} z_{nk} = 1$ to represent the unknown source assignments, formulate the Expectation-Maximization (EM) algorithm for this model. Derive from first principles the explicit update equations for:\n- the responsibilities $r_{nk} \\equiv p(z_{nk}=1 \\mid \\mathbf{x}_{n}, \\boldsymbol{\\Theta})$ in the expectation step,\n- the component means $\\boldsymbol{\\mu}_{k}$ in the maximization step,\n- the component variances $\\sigma_{k}^{2}$ in the maximization step,\nassuming isotropic covariance $\\sigma_{k}^{2}\\mathbf{I}$ for each component $k$.\n\nYour final answer must present the three closed-form analytic expressions for $r_{nk}$, $\\boldsymbol{\\mu}_{k}$, and $\\sigma_{k}^{2}$ as a single composite expression. No rounding is required. Express the final answer without units. Ensure all symbols are consistent with the definitions above.",
            "solution": "The Expectation-Maximization (EM) algorithm is an iterative method used to find maximum likelihood estimates for the parameters of a statistical model with latent variables. The objective is to maximize the log-likelihood of the observed data, $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$, which for the given Gaussian Mixture Model (GMM) is:\n$$\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\Theta}) = \\sum_{n=1}^{N} \\ln \\left( \\sum_{k=1}^{K} \\pi_{k} \\,\\mathcal{N}\\!\\big(\\mathbf{x}_{n} \\,\\big|\\, \\boldsymbol{\\mu}_{k}, \\sigma_{k}^{2}\\mathbf{I}\\big) \\right)\n$$\nDirectly maximizing this expression is difficult due to the sum inside the logarithm. The EM algorithm simplifies this by introducing latent variables $z_{nk}$, where $z_{nk}=1$ if data point $\\mathbf{x}_n$ was generated by component $k$, and $z_{nk}=0$ otherwise. This allows us to work with the complete-data log-likelihood, $\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta})$:\n$$\n\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} \\ln \\left[ \\pi_k \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\sigma_k^2 \\mathbf{I}) \\right]\n$$\nUsing the definition of the isotropic Gaussian, this becomes:\n$$\n\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} \\left[ \\ln \\pi_k - \\frac{d}{2}\\ln(2\\pi\\sigma_k^2) - \\frac{1}{2\\sigma_k^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k\\|^2 \\right]\n$$\nThe EM algorithm alternates between two steps: the E-step and the M-step.\n\n**1. Expectation (E) Step**\n\nIn the E-step, we compute the expectation of the complete-data log-likelihood with respect to the posterior distribution of the latent variables, given the data and the current parameter estimates $\\boldsymbol{\\Theta}^{\\text{old}}$. This defines the function $\\mathcal{Q}(\\boldsymbol{\\Theta}, \\boldsymbol{\\Theta}^{\\text{old}})$. This requires finding the expectation of each $z_{nk}$, which is the posterior probability that point $\\mathbf{x}_n$ belongs to component $k$. This quantity is called the responsibility, $r_{nk}$.\n$$\nr_{nk} \\equiv \\mathbb{E}[z_{nk}] = p(z_{nk}=1 \\mid \\mathbf{x}_n, \\boldsymbol{\\Theta}^{\\text{old}})\n$$\nUsing Bayes' theorem, we derive the expression for the responsibilities:\n$$\nr_{nk} = \\frac{p(z_{nk}=1, \\mathbf{x}_n \\mid \\boldsymbol{\\Theta}^{\\text{old}})}{p(\\mathbf{x}_n \\mid \\boldsymbol{\\Theta}^{\\text{old}})} = \\frac{\\pi_k^{\\text{old}} \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k^{\\text{old}}, (\\sigma_k^2)^{\\text{old}} \\mathbf{I})}{\\sum_{j=1}^{K} \\pi_j^{\\text{old}} \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_j^{\\text{old}}, (\\sigma_j^2)^{\\text{old}} \\mathbf{I})}\n$$\nThis is the first required update equation.\n\n**2. Maximization (M) Step**\n\nIn the M-step, we update the parameters $\\boldsymbol{\\Theta}^{\\text{new}}$ by maximizing the $\\mathcal{Q}$ function with respect to $\\boldsymbol{\\Theta}$, keeping the responsibilities $r_{nk}$ fixed. For clarity, the 'new' and 'old' superscripts are omitted from the parameters being optimized.\n$$\n\\mathcal{Q}(\\boldsymbol{\\Theta}, \\boldsymbol{\\Theta}^{\\text{old}}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} \\left[ \\ln \\pi_k - \\frac{d}{2}\\ln(\\sigma_k^2) - \\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2\\sigma_k^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k\\|^2 \\right]\n$$\n\n**Update for Component Means $\\boldsymbol{\\mu}_k$**:\nWe maximize $\\mathcal{Q}$ with respect to $\\boldsymbol{\\mu}_k$ by setting the gradient to zero.\n$$\n\\frac{\\partial \\mathcal{Q}}{\\partial \\boldsymbol{\\mu}_k} = \\frac{\\partial}{\\partial \\boldsymbol{\\mu}_k} \\sum_{n=1}^{N} r_{nk} \\left( - \\frac{1}{2\\sigma_k^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k\\|^2 \\right) = \\sum_{n=1}^{N} r_{nk} \\frac{1}{\\sigma_k^2} (\\mathbf{x}_n - \\boldsymbol{\\mu}_k) = \\mathbf{0}\n$$\nSolving for $\\boldsymbol{\\mu}_k$ yields the update equation:\n$$\n\\boldsymbol{\\mu}_k = \\frac{\\sum_{n=1}^{N} r_{nk} \\mathbf{x}_{n}}{\\sum_{n=1}^{N} r_{nk}}\n$$\n\n**Update for Component Variances $\\sigma_k^2$**:\nSimilarly, we differentiate $\\mathcal{Q}$ with respect to $\\sigma_k^2$ and set the result to zero, using the newly computed mean $\\boldsymbol{\\mu}_k$.\n$$\n\\frac{\\partial \\mathcal{Q}}{\\partial (\\sigma_k^2)} = \\sum_{n=1}^{N} r_{nk} \\left[ -\\frac{d}{2\\sigma_k^2} + \\frac{1}{2(\\sigma_k^2)^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k\\|^2 \\right] = 0\n$$\nSolving for $\\sigma_k^2$ gives the update equation:\n$$\nd \\sigma_k^2 \\sum_{n=1}^{N} r_{nk} = \\sum_{n=1}^{N} r_{nk} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k\\|^2 \\quad \\implies \\quad \\sigma_k^2 = \\frac{\\sum_{n=1}^{N} r_{nk} \\|\\mathbf{x}_{n} - \\boldsymbol{\\mu}_{k}\\|^{2}}{d \\sum_{n=1}^{N} r_{nk}}\n$$\nThese expressions for $r_{nk}$, $\\boldsymbol{\\mu}_k$, and $\\sigma_k^2$ provide the update rules for the EM algorithm.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nr_{nk} \\\\\n\\boldsymbol{\\mu}_{k} \\\\\n\\sigma_{k}^{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{\\pi_{k} (\\sigma_{k}^{2})^{-\\frac{d}{2}} \\exp(-\\frac{1}{2\\sigma_{k}^{2}} \\|\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{k}\\|^{2})}{\\sum_{j=1}^{K} \\pi_{j} (\\sigma_{j}^{2})^{-\\frac{d}{2}} \\exp(-\\frac{1}{2\\sigma_{j}^{2}} \\|\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{j}\\|^{2})} \\\\\n\\frac{\\sum_{n=1}^{N} r_{nk} \\mathbf{x}_{n}}{\\sum_{n=1}^{N} r_{nk}} \\\\\n\\frac{\\sum_{n=1}^{N} r_{nk} \\|\\mathbf{x}_{n} - \\boldsymbol{\\mu}_{k}\\|^{2}}{d \\sum_{n=1}^{N} r_{nk}}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}