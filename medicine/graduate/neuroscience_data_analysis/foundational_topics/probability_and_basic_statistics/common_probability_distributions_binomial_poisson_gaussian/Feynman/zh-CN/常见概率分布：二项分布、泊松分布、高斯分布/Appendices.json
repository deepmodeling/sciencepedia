{
    "hands_on_practices": [
        {
            "introduction": "泊松过程是描述神经元尖峰发放的一个基准模型。一个基本问题是，我们能从观测到的尖峰计数中以多高的精度估计其发放率 $\\lambda$？本练习将引导你运用第一性原理，推导一个关键的理论工具——费雪信息（Fisher Information），它量化了数据中包含的关于参数的信息量，并为任何无偏估计量的方差设定了理论下界。通过这个练习 ，你将深入理解尖峰率估计的理论极限，以及观测时间等实验参数如何影响估计精度。",
            "id": "4146747",
            "problem": "在系统神经科学的脉冲计数实验中，单个神经元被建模为根据一个具有未知恒定速率参数 $\\lambda$（脉冲/秒）的齐次泊松过程发放脉冲。您在一个持续时间为 $T$（秒）的单个连续观测窗口内观测到总脉冲数 $K$。假设在给定 $\\lambda$ 的条件下，计数 $K$ 是一个均值为 $\\lambda T$ 的泊松随机变量。仅使用基本定义和经过充分检验的事实，推导包含在 $K$ 中的关于 $\\lambda$ 的费雪信息。\n\n从以下基本要素出发：泊松随机变量的概率质量函数，观测数据的似然和对数似然的定义，费雪信息定义为对数似然函数对参数的负二阶导数的期望值，以及可以从泊松分布的定义中证明的标准矩恒等式。在没有推导的情况下，不要假设或引用任何关于泊松分布费雪信息的专门结果。\n\n此外，假设观测窗口被划分为 $N$ 个独立的等长时间段，每个时间段的长度为 $\\Delta$，使得 $T = N \\Delta$，并令 $K_{1},\\dots,K_{N}$ 表示这些时间段中的脉冲计数。在齐次泊松过程模型下，论证独立观测的信息可加性如何恢复您为单窗口情况推导出的相同表达式，并解释费雪信息如何随总观测时间 $T$ 变化以进行脉冲率估计。\n\n以仅包含 $\\lambda$ 和 $T$ 的单一闭式解析表达式的形式提供您的最终答案。最终答案中不要包含单位。此问题不需要进行数值四舍五入。",
            "solution": "该问题要求在给定观测持续时间 $T$ 内的脉冲计数 $K$ 的情况下，推导齐次泊松过程的速率参数 $\\lambda$ 的费雪信息。推导必须从第一性原理出发。\n\n首先，我们建立统计模型。在持续时间为 $T$ 的时间间隔内的脉冲计数 $K$ 是一个服从均值为 $\\mu = \\lambda T$ 的泊松分布的随机变量。观测到 $k$ 个脉冲的概率质量函数 (PMF) 由下式给出：\n$$P(K=k | \\lambda) = \\frac{(\\lambda T)^k \\exp(-\\lambda T)}{k!}$$\n对于 $k \\in \\{0, 1, 2, \\dots\\}$。\n\n对于一个观测到的计数 $K$，似然函数 $L(\\lambda|K)$ 被定义为视为参数 $\\lambda$ 的函数的 PMF：\n$$L(\\lambda|K) = P(K=K | \\lambda) = \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!}$$\n对数似然函数 $\\ell(\\lambda|K) = \\ln L(\\lambda|K)$ 通常更便于求导：\n$$\\ell(\\lambda|K) = \\ln\\left( \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!} \\right)$$\n利用对数的性质，这可以简化为：\n$$\\ell(\\lambda|K) = K \\ln(\\lambda T) - \\lambda T - \\ln(K!)$$\n$$\\ell(\\lambda|K) = K(\\ln \\lambda + \\ln T) - \\lambda T - \\ln(K!)$$\n\n费雪信息 $I(\\lambda)$ 定义为对数似然函数关于参数 $\\lambda$ 的负二阶导数的期望值。我们首先计算导数。一阶导数，即得分函数，为：\n$$\\frac{\\partial \\ell}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left[ K\\ln \\lambda + K\\ln T - \\lambda T - \\ln(K!) \\right]$$\n在对 $\\lambda$ 求导时，将 $K$ 和 $T$ 视为常数：\n$$\\frac{\\partial \\ell}{\\partial \\lambda} = \\frac{K}{\\lambda} - T$$\n\n接下来，我们计算二阶导数：\n$$\\frac{\\partial^2 \\ell}{\\partial \\lambda^2} = \\frac{\\partial}{\\partial \\lambda} \\left( \\frac{K}{\\lambda} - T \\right) = -\\frac{K}{\\lambda^2}$$\n\n然后通过取该二阶导数的负值的期望来计算费雪信息 $I(\\lambda)$。期望 $E[\\cdot]$ 是关于数据分布的，在本例中即随机变量 $K \\sim \\text{Poisson}(\\lambda T)$。\n$$I(\\lambda) = E\\left[ - \\frac{\\partial^2 \\ell}{\\partial \\lambda^2} \\right] = E\\left[ - \\left( -\\frac{K}{\\lambda^2} \\right) \\right] = E\\left[ \\frac{K}{\\lambda^2} \\right]$$\n由于 $\\lambda$ 相对于关于 $K$ 的期望是常数，我们可以写出：\n$$I(\\lambda) = \\frac{1}{\\lambda^2} E[K]$$\n均值为 $\\lambda T$ 的泊松随机变量的期望恰好是其均值。因此，$E[K] = \\lambda T$。将此代入 $I(\\lambda)$ 的方程：\n$$I(\\lambda) = \\frac{1}{\\lambda^2} (\\lambda T) = \\frac{T}{\\lambda}$$\n这就是单个观测 $K$ 中包含的关于 $\\lambda$ 的费雪信息。\n\n现在，我们处理问题的第二部分。总持续时间为 $T$ 的观测窗口被划分为 $N$ 个独立的、等长的时段，每个时段的长度为 $\\Delta = T/N$。令 $K_1, \\dots, K_N$ 为这些相应时段中的脉冲计数。对于齐次泊松过程，不相交时间间隔内的脉冲计数是独立的随机变量。每个时段 $i$ 中的计数 $K_i$ 服从均值为 $\\lambda \\Delta$ 的泊松分布。\n\n由于独立性，观测集 $\\{K_1, \\dots, K_N\\}$ 的总似然是各个似然的乘积：\n$$L_{total}(\\lambda | K_1, \\dots, K_N) = \\prod_{i=1}^N L_i(\\lambda | K_i)$$\n因此，总对数似然是各个对数似然的和：\n$$\\ell_{total}(\\lambda | K_1, \\dots, K_N) = \\ln\\left( \\prod_{i=1}^N L_i(\\lambda | K_i) \\right) = \\sum_{i=1}^N \\ln(L_i(\\lambda | K_i)) = \\sum_{i=1}^N \\ell_i(\\lambda | K_i)$$\n这组独立观测的费雪信息是从总对数似然中推导出来的。由于求导和求期望的线性性质，总费雪信息是来自每个独立观测的费雪信息的总和：\n$$I_{total}(\\lambda) = E\\left[ -\\frac{\\partial^2 \\ell_{total}}{\\partial \\lambda^2} \\right] = E\\left[ -\\sum_{i=1}^N \\frac{\\partial^2 \\ell_i}{\\partial \\lambda^2} \\right] = \\sum_{i=1}^N E\\left[ -\\frac{\\partial^2 \\ell_i}{\\partial \\lambda^2} \\right] = \\sum_{i=1}^N I_i(\\lambda)$$\n每个 $I_i(\\lambda)$ 是在持续时间 $\\Delta$ 内从单个观测 $K_i$ 中获得的费雪信息。我们可以使用之前的结果，将 $T$ 替换为 $\\Delta$：\n$$I_i(\\lambda) = \\frac{\\Delta}{\\lambda}$$\n总信息是所有 $N$ 个时段的总和：\n$$I_{total}(\\lambda) = \\sum_{i=1}^N \\frac{\\Delta}{\\lambda} = N \\frac{\\Delta}{\\lambda}$$\n因为总持续时间为 $T = N \\Delta$，我们可以将其代回：\n$$I_{total}(\\lambda) = \\frac{N\\Delta}{\\lambda} = \\frac{T}{\\lambda}$$\n这个结果证实了独立观测的信息可加性原理，并恢复了为单个未分割的观测窗口推导出的相同表达式。\n\n对结果 $I(\\lambda) = T/\\lambda$ 的解释是，数据提供的关于速率参数 $\\lambda$ 的信息量与总观测时间 $T$ 呈线性关系。这是符合直觉的：更长的观测周期提供更多数据，从而可以更精确地估计基础速率。Cramér-Rao 下界指出，任何无偏估计量 $\\hat{\\lambda}$ 的方差都有界，即 $\\text{Var}(\\hat{\\lambda}) \\ge 1/I(\\lambda)$，这意味着 $\\text{Var}(\\hat{\\lambda}) \\ge \\lambda/T$。这表明，可达到的最小估计方差与观测时间成反比减小，意味着速率估计的精度随着观测时间的平方根而提高。",
            "answer": "$$\\boxed{\\frac{T}{\\lambda}}$$"
        },
        {
            "introduction": "真实的神经数据往往表现出比简单泊松模型预测的更大变异性，这种现象被称为“超离散”（overdispersion）。为了处理这一普遍问题，我们可以构建一个分层模型，如此处的伽马-泊松混合模型，它通过对试验间发放率的波动进行建模来解释额外的变异性。这个实践练习  将理论与计算相结合，要求你不仅要从理论上推导出该混合模型产生的负二项分布，还要通过编写模拟程序来验证其统计特性，从而将抽象的统计理论与具体的计算实践联系起来。",
            "id": "4146731",
            "problem": "在短分析窗口内的神经元尖峰计数可以被建模为从具有瞬时速率的泊松分布中抽取的一个样本。在实际情景中，由于潜在调制因子的影响，该速率在重复试验中会发生波动。对这种波动进行建模的一种原则性方法是假设潜在速率本身是随机的。考虑一个层级模型，其中潜在速率 $\\lambda$ 在各次试验中是随机的，而给定 $\\lambda$ 的观测计数 $N$ 服从泊松分布。\n\n您需要从第一性原理出发，证明这个层级模型如何导出一个负二项分布，然后实现一个模拟器，使用这种混合构造来生成合成的尖峰计数。随后，应针对该模型在几种探究不同离散程度区制下的理论均值和方差，对模拟器进行验证。这些区制包括接近泊松极限的区制和具有强过度离散的区制，这二者对于神经元变异性分析具有科学上的重要性。\n\n仅使用以下基础知识：泊松随机变量的概率质量函数、伽马随机变量的概率密度函数、伽马函数的性质、全概率定律以及全期望定律和全方差定律。在您的推导中，不要使用任何关于负二项分布的目标公式；相反，应从这些基本要素中推导出所需的表达式。\n\n待完成的任务：\n\n- 从条件模型 $N \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$ 和先验分布 $\\lambda \\sim \\mathrm{Gamma}(k,\\beta)$（其中 $k>0$ 是形状参数，$\\beta>0$ 是速率参数）出发。仅使用全概率定律以及泊松概率质量函数和伽马概率密度函数的定义，推导 $N$ 的边际分布，并证明它是一个由 $k$ 和一个用 $\\beta$ 表示的成功概率 $p$ 参数化的负二项分布。然后，仅使用全期望定律和全方差定律，推导 $\\mathbb{E}[N]$ 和 $\\mathrm{Var}(N)$ 关于 $k$ 和 $\\beta$ 的表达式。\n\n- 将混合抽样过程解释为对潜在速率波动的抽样：解释从伽马分布中抽取 $\\lambda$ 如何为跨试验的速率变异性建模，以及随后的泊松抽样如何为试验内的计数噪声建模。\n\n- 设计并实现一个算法，通过首先抽样 $\\lambda$，然后抽样 $N \\mid \\lambda$，来模拟此层级模型下 $N$ 的 $T$ 次独立抽取。该实现必须接受元组 $(k,\\beta,T,\\mathrm{seed})$，并返回模拟计数的经验均值和方差，以及它们的理论对应值，和一个基于相对误差容限的通过/失败布尔值。\n\n- 验证和输出规范。对于每个测试用例，使用固定的伪随机种子模拟计数并计算：\n  - 经验均值 $\\hat{m}$，\n  - 经验方差 $\\hat{v}$，\n  - 理论均值 $m_\\mathrm{th}$，\n  - 理论方差 $v_\\mathrm{th}$，\n  - 经验方差均值比 $r_\\mathrm{emp}=\\hat{v}/\\hat{m}$，\n  - 理论方差均值比 $r_\\mathrm{th}=v_\\mathrm{th}/m_\\mathrm{th}$。\n  对均值使用 $\\tau_m=0.02$ 的相对误差容限，对方差使用 $\\tau_v=0.05$ 的相对误差容限。如果 $\\lvert \\hat{m}-m_\\mathrm{th}\\rvert/m_\\mathrm{th}\\le \\tau_m$ 和 $\\lvert \\hat{v}-v_\\mathrm{th}\\rvert/v_\\mathrm{th}\\le \\tau_v$ 都成立，则测试用例通过。\n\n要实现和评估的测试套件：\n\n- 案例 A（一般过度离散）：$k=10.0$, $\\beta=2.0$, $T=120000$, $\\mathrm{seed}=12345$。\n- 案例 B（通过小潜在方差接近泊松极限）：$k=1000.0$, $\\beta=200.0$, $T=200000$, $\\mathrm{seed}=24680$。\n- 案例 C（强过度离散，重尾速率）：$k=0.5$, $\\beta=0.5$, $T=150000$, $\\mathrm{seed}=13579$。\n\n最终输出格式要求：\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表。每个对应于一个测试用例的元素本身必须是一个形式为 $[\\hat{m},\\hat{v},m_\\mathrm{th},v_\\mathrm{th},r_\\mathrm{emp},r_\\mathrm{th},\\mathrm{pass}]$ 的列表，其中的条目顺序完全一致，数值量的类型为浮点数，通过指示符的类型为布尔值。因此，最终输出应看起来像一个包含三个内部列表的单一列表，每个测试用例一个，例如 $[[\\cdots],[\\cdots],[\\cdots]]$，不含任何额外文本。此任务不涉及任何物理单位或角度，所有量均为无量纲。",
            "solution": "该问题要求从第一性原理出发，证明用于神经元尖峰计数的伽马-泊松混合模型会产生负二项分布。这涉及推导尖峰计数 $N$ 的边际概率质量函数 (PMF) 及其理论均值和方差。此外，必须实现一个模拟来验证这些理论结果。整个过程都基于概率论的基本原理。\n\n首先，我们提供该层级模型的概念性解释。该模型分两个阶段指定：\n1. 潜在发放率 $\\lambda$ 从伽马分布中抽取：$\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$，其中 $k>0$ 是形状参数，$\\beta>0$ 是速率参数。此阶段为神经元兴奋性中缓慢的、逐次试验的波动建模。在实验环境中，诸如注意力、动机或缓慢的神经调质变化等因素可能导致神经元的平均发放率在重复试验中发生变化。伽马分布是为这种非负连续速率变量建模的灵活且数学上方便的选择。该分布的方差 $\\mathrm{Var}(\\lambda) = k/\\beta^2$ 量化了这种跨试验速率变异性的大小。\n2. 在给定试验的特定速率 $\\lambda$ 条件下，观测到的尖峰计数 $N$ 从泊松分布中抽取：$N \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$。此阶段为尖峰生成过程固有的随机性建模。对于固定的潜在神经元状态（即固定的 $\\lambda$），发放动作电位的过程是时间上的一个随机点过程，可以很好地用泊松过程来近似。该过程的方差等于其均值，即 $\\mathrm{Var}(N \\mid \\lambda) = \\lambda$。这通常被称为“计数噪声”或“试验内”变异性。\n\n通过结合这两种随机性来源，该模型捕捉了真实神经元数据的一个关键特征：过度离散，即跨试验的尖峰计数总方差大于平均计数。\n\n接下来，我们推导 $N$ 的边际分布。问题陈述了在给定速率 $\\lambda$ 时计数 $N$ 的条件分布是泊松分布，速率 $\\lambda$ 的先验分布是伽马分布。各自的概率函数为：\n- 泊松 PMF: $P(N=n \\mid \\lambda) = \\frac{\\lambda^n e^{-\\lambda}}{n!}$ for $n \\in \\{0, 1, 2, \\dots\\}$。\n- 伽马 PDF: $p(\\lambda \\mid k, \\beta) = \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} e^{-\\beta\\lambda}$ for $\\lambda > 0$。\n\n为了找到 $N$ 的边际 PMF，我们应用全概率定律，对潜在变量 $\\lambda$ 的所有可能值进行积分：\n$$P(N=n) = \\int_{0}^{\\infty} P(N=n \\mid \\lambda) \\, p(\\lambda \\mid k, \\beta) \\, d\\lambda$$\n代入泊松 PMF 和伽马 PDF 的具体形式：\n$$P(N=n) = \\int_{0}^{\\infty} \\left( \\frac{\\lambda^n e^{-\\lambda}}{n!} \\right) \\left( \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} e^{-\\beta\\lambda} \\right) d\\lambda$$\n我们可以将不依赖于 $\\lambda$ 的项移到积分符号外：\n$$P(N=n) = \\frac{\\beta^k}{n! \\Gamma(k)} \\int_{0}^{\\infty} \\lambda^{n+k-1} e^{-(\\beta+1)\\lambda} d\\lambda$$\n该积分具有伽马分布核的形式。回想一下，形状为 $\\alpha$、速率为 $\\delta$ 的伽马 PDF 积分为 1：\n$$\\int_{0}^{\\infty} \\frac{\\delta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\delta x} dx = 1 \\implies \\int_{0}^{\\infty} x^{\\alpha-1} e^{-\\delta x} dx = \\frac{\\Gamma(\\alpha)}{\\delta^\\alpha}$$\n在我们的积分中，我们可以确定形状为 $\\alpha = n+k$，速率为 $\\delta = \\beta+1$。应用此公式，积分的计算结果为：\n$$\\int_{0}^{\\infty} \\lambda^{(n+k)-1} e^{-(\\beta+1)\\lambda} d\\lambda = \\frac{\\Gamma(n+k)}{(\\beta+1)^{n+k}}$$\n将此结果代回 $P(N=n)$ 的表达式中：\n$$P(N=n) = \\frac{\\beta^k}{n! \\Gamma(k)} \\frac{\\Gamma(n+k)}{(\\beta+1)^{n+k}}$$\n为了证明这是一个负二项分布，我们重新整理这些项。二项式系数 $\\binom{a}{b}$ 可以用伽马函数表示为 $\\frac{\\Gamma(a+1)}{b! \\Gamma(a-b+1)}$。一个更通用的形式是 $\\binom{n+r-1}{n} = \\frac{\\Gamma(n+r)}{n! \\Gamma(r)}$。使用这个恒等式，并令 $r=k$：\n$$P(N=n) = \\frac{\\Gamma(n+k)}{n! \\Gamma(k)} \\frac{\\beta^k}{(\\beta+1)^k (\\beta+1)^n} = \\binom{n+k-1}{n} \\left(\\frac{\\beta}{\\beta+1}\\right)^k \\left(\\frac{1}{\\beta+1}\\right)^n$$\n这是负二项分布的 PMF，其参数为 $r=k$，成功概率为 $p = \\frac{\\beta}{\\beta+1}$。在 $r$ 次成功之前，失败的次数为 $n$。因此，我们已经证明了 $N \\sim \\mathrm{NB}(k, p = \\frac{\\beta}{\\beta+1})$。\n\n接下来，我们使用全期望定律和全方差定律推导 $N$ 的理论均值 ($m_\\mathrm{th}$) 和方差 ($v_\\mathrm{th}$)，而不依赖于负二项分布的已知公式。\n全期望定律表明：$\\mathbb{E}[N] = \\mathbb{E}[\\mathbb{E}[N \\mid \\lambda]]$。\n- 内部期望是参数为 $\\lambda$ 的泊松分布的均值：$\\mathbb{E}[N \\mid \\lambda] = \\lambda$。\n- 外部期望是 $\\lambda$ 的期望，它服从 $\\mathrm{Gamma}(k, \\beta)$ 分布。伽马分布的均值是形状/速率。所以，$\\mathbb{E}[\\lambda] = k/\\beta$。\n- 因此，$N$ 的理论均值为：\n$$m_\\mathrm{th} = \\mathbb{E}[N] = \\mathbb{E}[\\lambda] = \\frac{k}{\\beta}$$\n\n全方差定律表明：$\\mathrm{Var}(N) = \\mathbb{E}[\\mathrm{Var}(N \\mid \\lambda)] + \\mathrm{Var}(\\mathbb{E}[N \\mid \\lambda])$。\n- 对于第一项，$\\mathbb{E}[\\mathrm{Var}(N \\mid \\lambda)]$：泊松分布的方差等于其均值，所以 $\\mathrm{Var}(N \\mid \\lambda) = \\lambda$。因此，该期望为 $\\mathbb{E}[\\lambda] = k/\\beta$。\n- 对于第二项，$\\mathrm{Var}(\\mathbb{E}[N \\mid \\lambda])$：我们已经确定 $\\mathbb{E}[N \\mid \\lambda] = \\lambda$。因此，这一项是 $\\mathrm{Var}(\\lambda)$。$\\mathrm{Gamma}(k, \\beta)$ 分布的方差是形状/速率$^2$，所以 $\\mathrm{Var}(\\lambda) = k/\\beta^2$。\n- 将这两项相加，得到 $N$ 的理论方差：\n$$v_\\mathrm{th} = \\mathrm{Var}(N) = \\frac{k}{\\beta} + \\frac{k}{\\beta^2}$$\n这证实了方差大于均值，因为对于给定的约束条件 $k>0$ 和 $\\beta>0$，有 $k/\\beta^2 > 0$。\n\n最后，理论方差均值比为：\n$$r_\\mathrm{th} = \\frac{v_\\mathrm{th}}{m_\\mathrm{th}} = \\frac{k/\\beta + k/\\beta^2}{k/\\beta} = 1 + \\frac{1}{\\beta}$$\n这个比率，也称为范诺因子 (Fano factor)，是离散程度的一种度量。值为 $1$ 表示泊松过程，而值大于 $1$ 表示过度离散。该模型中的过度离散程度仅由伽马速率参数 $\\beta$ 控制。当 $\\beta \\to \\infty$ 时，$r_\\mathrm{th} \\to 1$，模型接近泊松极限。这是因为 $\\mathrm{Var}(\\lambda) = k/\\beta^2 \\to 0$，意味着潜在速率 $\\lambda$ 变得非随机。\n\n该实现将模拟这个两阶段过程，计算经验统计量（$\\hat{m}$、$\\hat{v}$），使用 $\\tau_m=0.02$ 和 $\\tau_v=0.05$ 的相对误差容限将它们与理论值（$m_\\mathrm{th}$、$v_\\mathrm{th}$）进行比较，并报告每个测试用例的通过/失败状态。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation_and_validation(k, beta, T, seed):\n    \"\"\"\n    Simulates spike counts from a Gamma-Poisson mixture model and validates\n    empirical statistics against theoretical predictions.\n\n    Args:\n        k (float): Shape parameter of the Gamma distribution for the rate λ.\n        beta (float): Rate parameter of the Gamma distribution for the rate λ.\n        T (int): Number of independent trials to simulate.\n        seed (int): Seed for the pseudo-random number generator.\n\n    Returns:\n        list: A list containing the following values in order:\n              [empirical mean, empirical variance, theoretical mean,\n               theoretical variance, empirical variance-to-mean ratio,\n               theoretical variance-to-mean ratio, pass/fail boolean].\n    \"\"\"\n    # Set up the random number generator with a specific seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # --- Simulation Step ---\n    # Step 1: Sample T latent rates from the Gamma distribution.\n    # numpy.random.Generator.gamma uses a scale parameter, which is 1/rate.\n    scale = 1.0 / beta\n    lambda_samples = rng.gamma(shape=k, scale=scale, size=T)\n\n    # Step 2: For each sampled rate, sample a spike count from the Poisson distribution.\n    n_samples = rng.poisson(lam=lambda_samples)\n\n    # --- Analysis and Validation Step ---\n    # Calculate empirical statistics from the simulated counts\n    m_hat = np.mean(n_samples)\n    # Use ddof=1 for the unbiased sample variance estimate\n    v_hat = np.var(n_samples, ddof=1)\n    \n    # Avoid division by zero if the empirical mean is zero\n    if m_hat == 0:\n        r_emp = np.nan\n    else:\n        r_emp = v_hat / m_hat\n\n    # Calculate theoretical statistics based on the derived formulas\n    m_th = k / beta\n    v_th = (k / beta) + (k / (beta**2))\n    \n    if m_th == 0:\n        r_th = np.nan\n    else:\n        r_th = v_th / m_th\n\n    # Define validation tolerances\n    tau_m = 0.02  # Relative error tolerance for the mean\n    tau_v = 0.05  # Relative error tolerance for the variance\n\n    # Check if the simulation passes the validation criteria\n    # Handle cases where theoretical mean/variance could be zero to avoid division by zero\n    passed = True\n    if m_th > 0:\n        err_m = abs(m_hat - m_th) / m_th\n        if err_m > tau_m:\n            passed = False\n    elif m_hat != 0: # If m_th is 0, m_hat should also be 0\n        passed = False\n        \n    if v_th > 0:\n        err_v = abs(v_hat - v_th) / v_th\n        if err_v > tau_v:\n            passed = False\n    elif v_hat != 0: # If v_th is 0, v_hat should also be 0\n        passed = False\n\n    return [m_hat, v_hat, m_th, v_th, r_emp, r_th, passed]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (k, beta, T, seed)\n    test_cases = [\n        (10.0, 2.0, 120000, 12345),   # Case A: General overdispersion\n        (1000.0, 200.0, 200000, 24680), # Case B: Near-Poisson limit\n        (0.5, 0.5, 150000, 13579),    # Case C: Strong overdispersion\n    ]\n\n    results = []\n    for case in test_cases:\n        k, beta, T, seed = case\n        result_case = run_simulation_and_validation(k, beta, T, seed)\n        results.append(result_case)\n\n    # Final print statement in the exact required format.\n    # The default string representation for a Python list is used.\n    # map(str, results) would stringify each inner list, which is not the desired format.\n    # Instead, we directly stringify the list of lists.\n    # To match the required output format of [val,val,...] instead of Python's [val, val, ...],\n    # we build the string manually.\n    outer_list_str = []\n    for res in results:\n        # Convert boolean to lowercase 'true'/'false' if needed, but 'True'/'False' is standard\n        # The prompt just says \"boolean\", so Python's default str() is acceptable. Let's use it.\n        inner_list_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]},{res[5]},{res[6]}]\"\n        outer_list_str.append(inner_list_str)\n\n    print(f\"[{','.join(outer_list_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在神经科学数据分析中，我们不仅关心尖峰发放的时间（计数），还常常需要识别尖峰的来源，这一过程称为“尖峰排序”（spike sorting）。尖峰排序通常被构建为一个聚类问题，其中尖峰的波形特征被用于区分不同的神经元。高斯混合模型（Gaussian Mixture Model, GMM）是完成此任务的强大工具。此练习  将带你深入该模型的核心，从第一性原理推导用于拟合GMM的期望最大化（Expectation-Maximization, EM）算法的更新方程，从而揭示这一经典的无监督学习方法如何运作。",
            "id": "4146702",
            "problem": "来自皮层微电极阵列的胞外记录产生多单元活动 (MUA)，其包含来自多个神经元的同步尖峰事件。对于每个检测到的尖峰事件，通过标准预处理提取一个特征向量，例如，通过主成分分析 (PCA) 将波形投影到前几个主成分上。设有 $N$ 个事件，其特征向量为 $\\mathbf{x}_{n} \\in \\mathbb{R}^{d}$，其中 $n \\in \\{1,\\dots,N\\}$。假设每个事件源于 $K$ 个潜在神经元来源中的一个，并且在以来源 $k \\in \\{1,\\dots,K\\}$ 为条件下，特征分布是具有各向同性协方差的多元高斯分布。具体来说，我们设定高斯混合模型为\n$$\np(\\mathbf{x}_{n} \\mid \\boldsymbol{\\Theta}) \\;=\\; \\sum_{k=1}^{K} \\pi_{k} \\,\\mathcal{N}\\!\\big(\\mathbf{x}_{n} \\,\\big|\\, \\boldsymbol{\\mu}_{k}, \\sigma_{k}^{2}\\mathbf{I}\\big),\n$$\n其中 $\\boldsymbol{\\Theta} = \\big\\{ \\{\\pi_{k}\\}_{k=1}^{K}, \\{\\boldsymbol{\\mu}_{k}\\}_{k=1}^{K}, \\{\\sigma_{k}^{2}\\}_{k=1}^{K} \\big\\}$，$\\pi_{k} \\in (0,1)$ 是满足 $\\sum_{k=1}^{K} \\pi_{k} = 1$ 的混合权重，$\\boldsymbol{\\mu}_{k} \\in \\mathbb{R}^{d}$ 是分量均值向量，$\\sigma_{k}^{2} > 0$ 是分量方差。具有各向同性协方差的多元高斯密度定义为\n$$\n\\mathcal{N}\\!\\big(\\mathbf{x} \\,\\big|\\, \\boldsymbol{\\mu}, \\sigma^{2}\\mathbf{I}\\big) \\;=\\; (2\\pi \\sigma^{2})^{-\\tfrac{d}{2}} \\exp\\!\\Big(-\\tfrac{1}{2\\sigma^{2}} \\big\\|\\mathbf{x}-\\boldsymbol{\\mu}\\big\\|^{2}\\Big).\n$$\n使用最大似然原理和多元高斯密度的定义，并引入潜在指示变量 $z_{nk} \\in \\{0,1\\}$（其中 $\\sum_{k=1}^{K} z_{nk} = 1$）来表示未知的来源分配，为该模型构建期望最大化 (EM) 算法。从第一性原理推导以下各项的显式更新方程：\n- 期望步骤中的责任度 $r_{nk} \\equiv p(z_{nk}=1 \\mid \\mathbf{x}_{n}, \\boldsymbol{\\Theta})$，\n- 最大化步骤中的分量均值 $\\boldsymbol{\\mu}_{k}$，\n- 最大化步骤中的分量方差 $\\sigma_{k}^{2}$，假设每个分量 $k$ 具有各向同性协方差 $\\sigma_{k}^{2}\\mathbf{I}$。\n\n您的最终答案必须将 $r_{nk}$、$\\boldsymbol{\\mu}_{k}$ 和 $\\sigma_{k}^{2}$ 的三个闭式解析表达式呈现为单一的复合表达式。无需四舍五入。最终答案应不带单位。确保所有符号与上述定义一致。",
            "solution": "用户希望通过期望最大化 (EM) 算法找到具有各向同性协方差的高斯混合模型 (GMM) 的更新方程。\n\n### 第 1 步：问题验证\n\n**1.1. 提取已知条件**\n\n- **模型：** 针对 $N$ 个特征向量 $\\mathbf{x}_{n} \\in \\mathbb{R}^{d}$ 的高斯混合模型 (GMM)。\n- **混合模型密度：** $p(\\mathbf{x}_{n} \\mid \\boldsymbol{\\Theta}) = \\sum_{k=1}^{K} \\pi_{k} \\,\\mathcal{N}\\!\\big(\\mathbf{x}_{n} \\,\\big|\\, \\boldsymbol{\\mu}_{k}, \\sigma_{k}^{2}\\mathbf{I}\\big)$。\n- **参数：** $\\boldsymbol{\\Theta} = \\big\\{ \\{\\pi_{k}\\}_{k=1}^{K}, \\{\\boldsymbol{\\mu}_{k}\\}_{k=1}^{K}, \\{\\sigma_{k}^{2}\\}_{k=1}^{K} \\big\\}$。\n- **混合权重：** $\\pi_{k} \\in (0,1)$ 且 $\\sum_{k=1}^{K} \\pi_{k} = 1$。\n- **分量均值：** $\\boldsymbol{\\mu}_{k} \\in \\mathbb{R}^{d}$。\n- **分量方差：** $\\sigma_{k}^{2} > 0$。\n- **各向同性高斯密度：** $\\mathcal{N}\\!\\big(\\mathbf{x} \\,\\big|\\, \\boldsymbol{\\mu}, \\sigma^{2}\\mathbf{I}\\big) = (2\\pi \\sigma^{2})^{-\\tfrac{d}{2}} \\exp\\!\\Big(-\\tfrac{1}{2\\sigma^{2}} \\big\\|\\mathbf{x}-\\boldsymbol{\\mu}\\big\\|^{2}\\Big)$。\n- **潜在变量：** $z_{nk} \\in \\{0,1\\}$ 且 $\\sum_{k=1}^{K} z_{nk} = 1$。\n- **任务：** 推导责任度 $r_{nk} \\equiv p(z_{nk}=1 \\mid \\mathbf{x}_{n}, \\boldsymbol{\\Theta})$、分量均值 $\\boldsymbol{\\mu}_{k}$ 和分量方差 $\\sigma_{k}^{2}$ 的 EM 更新方程。\n\n**1.2. 使用提取的已知条件进行验证**\n\n- **科学依据：** 该问题描述了使用高斯混合模型进行尖峰排序，这是计算神经科学中一种标准且成熟的技术。EM 算法是拟合此类模型的经典方法。所有概念都是统计学和机器学习的基础。\n- **定义明确：** 该问题是统计学中一个标准的推导任务。给定模型和目标（通过 EM 进行最大似然估计），推导会得出一组唯一的更新方程。\n- **客观性：** 问题使用精确的数学符号和术语陈述。没有主观或模糊的语言。\n- **完整性：** 问题提供了执行推导所需的所有定义和约束。\n- **未发现其他缺陷。**\n\n**1.3. 结论与行动**\n\n问题有效。将提供完整的推导过程。\n\n### 第 2 步：EM 算法的推导\n\n期望最大化 (EM) 算法是一种迭代方法，用于为具有潜在变量的模型寻找最大似然估计。其目标是最大化观测数据 $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$ 的对数似然。\n\n对数似然由下式给出：\n$$\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\Theta}) = \\ln \\prod_{n=1}^{N} p(\\mathbf{x}_n \\mid \\boldsymbol{\\Theta}) = \\sum_{n=1}^{N} \\ln \\left( \\sum_{k=1}^{K} \\pi_{k} \\,\\mathcal{N}\\!\\big(\\mathbf{x}_{n} \\,\\big|\\, \\boldsymbol{\\mu}_{k}, \\sigma_{k}^{2}\\mathbf{I}\\big) \\right)\n$$\n由于对数函数内部存在求和，直接最大化此表达式很困难。EM 算法通过引入潜在变量 $z_{nk}$ 来规避这个问题，其中如果数据点 $\\mathbf{x}_n$ 由分量 $k$ 生成，则 $z_{nk}=1$，否则 $z_{nk}=0$。\n\n使用观测数据 $\\mathbf{X}$ 和潜在数据 $\\mathbf{Z}=\\{z_{nk}\\}$ 的完全数据对数似然为：\n$$\n\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} \\ln \\left[ \\pi_k \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\sigma_k^2 \\mathbf{I}) \\right]\n$$\n$$\n\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} \\left[ \\ln \\pi_k - \\frac{d}{2}\\ln(2\\pi) - \\frac{d}{2}\\ln(\\sigma_k^2) - \\frac{1}{2\\sigma_k^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k\\|^2 \\right]\n$$\nEM 算法在两个步骤之间迭代：期望 (E) 步骤和最大化 (M) 步骤。\n\n**2.1. 期望步骤 (E-step)**\n\n在 E 步骤中，我们计算完全数据对数似然关于给定数据和当前参数估计 $\\boldsymbol{\\Theta}^{\\text{old}}$ 下潜在变量后验分布的期望。这定义了函数 $\\mathcal{Q}(\\boldsymbol{\\Theta}, \\boldsymbol{\\Theta}^{\\text{old}})$。\n$$\n\\mathcal{Q}(\\boldsymbol{\\Theta}, \\boldsymbol{\\Theta}^{\\text{old}}) = \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\Theta}^{\\text{old}}}[\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta})]\n$$\n由于期望的线性性质，我们只需要潜在变量的期望 $\\mathbb{E}[z_{nk}]$。这个期望是点 $\\mathbf{x}_n$ 属于分量 $k$ 的后验概率，定义为责任度 $r_{nk}$。\n$$\nr_{nk} = \\mathbb{E}[z_{nk}] = p(z_{nk}=1 \\mid \\mathbf{x}_n, \\boldsymbol{\\Theta}^{\\text{old}})\n$$\n使用贝叶斯定理：\n$$\nr_{nk} = \\frac{p(z_{nk}=1 \\mid \\boldsymbol{\\Theta}^{\\text{old}}) \\, p(\\mathbf{x}_n \\mid z_{nk}=1, \\boldsymbol{\\Theta}^{\\text{old}})}{p(\\mathbf{x}_n \\mid \\boldsymbol{\\Theta}^{\\text{old}})} = \\frac{\\pi_k^{\\text{old}} \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k^{\\text{old}}, (\\sigma_k^2)^{\\text{old}} \\mathbf{I})}{\\sum_{j=1}^{K} \\pi_j^{\\text{old}} \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_j^{\\text{old}}, (\\sigma_j^2)^{\\text{old}} \\mathbf{I})}\n$$\n这是第一个所需的表达式。E 步骤涉及使用当前参数计算所有 $n \\in \\{1,\\dots,N\\}$ 和 $k \\in \\{1,\\dots,K\\}$ 的这些 $r_{nk}$ 值。\n\n**2.2. 最大化步骤 (M-step)**\n\n在 M 步骤中，我们关于参数 $\\boldsymbol{\\Theta} = \\{\\pi_k, \\boldsymbol{\\mu}_k, \\sigma_k^2\\}$ 最大化 $\\mathcal{Q}$ 函数，以获得更新后的参数 $\\boldsymbol{\\Theta}^{\\text{new}}$。责任度 $r_{nk}$ 被视为固定常数。\n$$\n\\mathcal{Q}(\\boldsymbol{\\Theta}, \\boldsymbol{\\Theta}^{\\text{old}}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} \\left[ \\ln \\pi_k - \\frac{d}{2}\\ln(2\\pi\\sigma_k^2) - \\frac{1}{2\\sigma_k^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k\\|^2 \\right]\n$$\n\n**分量均值 $\\boldsymbol{\\mu}_k$ 的更新**\n\n我们通过将 $\\mathcal{Q}$ 对 $\\boldsymbol{\\mu}_k$ 的梯度设为零来找到最大值，只考虑依赖于 $\\boldsymbol{\\mu}_k$ 的项。\n$$\n\\frac{\\partial \\mathcal{Q}}{\\partial \\boldsymbol{\\mu}_k} = \\frac{\\partial}{\\partial \\boldsymbol{\\mu}_k} \\sum_{n=1}^{N} \\sum_{j=1}^{K} r_{nj} \\left[ - \\frac{1}{2\\sigma_j^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_j\\|^2 \\right] = \\sum_{n=1}^{N} r_{nk} \\left[ - \\frac{1}{2\\sigma_k^2} \\frac{\\partial}{\\partial \\boldsymbol{\\mu}_k} (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^T(\\mathbf{x}_n - \\boldsymbol{\\mu}_k) \\right]\n$$\n$$\n\\frac{\\partial \\mathcal{Q}}{\\partial \\boldsymbol{\\mu}_k} = \\sum_{n=1}^{N} r_{nk} \\left[ - \\frac{1}{2\\sigma_k^2} (-2(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)) \\right] = \\frac{1}{\\sigma_k^2} \\sum_{n=1}^{N} r_{nk} (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)\n$$\n将梯度设为零：\n$$\n\\sum_{n=1}^{N} r_{nk} (\\mathbf{x}_n - \\boldsymbol{\\mu}_k) = \\mathbf{0} \\quad\\implies\\quad \\sum_{n=1}^{N} r_{nk} \\mathbf{x}_n = \\boldsymbol{\\mu}_k \\sum_{n=1}^{N} r_{nk}\n$$\n求解新的均值 $\\boldsymbol{\\mu}_k^{\\text{new}}$：\n$$\n\\boldsymbol{\\mu}_k^{\\text{new}} = \\frac{\\sum_{n=1}^{N} r_{nk} \\mathbf{x}_{n}}{\\sum_{n=1}^{N} r_{nk}}\n$$\n这是第二个所需的表达式。我们将聚类 $k$ 中的有效点数定义为 $N_k = \\sum_{n=1}^{N} r_{nk}$。那么 $\\boldsymbol{\\mu}_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{n=1}^{N} r_{nk} \\mathbf{x}_{n}$。\n\n**分量方差 $\\sigma_k^2$ 的更新**\n\n我们通过将 $\\mathcal{Q}$ 对 $\\sigma_k^2$ 的导数设为零来找到最大值。注意，在此步骤中我们必须使用新计算的均值 $\\boldsymbol{\\mu}_k^{\\text{new}}$。\n$$\n\\frac{\\partial \\mathcal{Q}}{\\partial (\\sigma_k^2)} = \\frac{\\partial}{\\partial (\\sigma_k^2)} \\sum_{n=1}^{N} r_{nk} \\left[ -\\frac{d}{2}\\ln(\\sigma_k^2) - \\frac{1}{2\\sigma_k^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2 \\right]\n$$\n$$\n\\frac{\\partial \\mathcal{Q}}{\\partial (\\sigma_k^2)} = \\sum_{n=1}^{N} r_{nk} \\left[ -\\frac{d}{2\\sigma_k^2} + \\frac{1}{2(\\sigma_k^2)^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2 \\right]\n$$\n将导数设为零：\n$$\n\\sum_{n=1}^{N} r_{nk} \\left( -\\frac{d}{2\\sigma_k^2} + \\frac{\\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2}{2(\\sigma_k^2)^2} \\right) = 0\n$$\n乘以 $2(\\sigma_k^2)^2$ 得：\n$$\n\\sum_{n=1}^{N} r_{nk} \\left( -d \\sigma_k^2 + \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2 \\right) = 0\n$$\n$$\nd \\sigma_k^2 \\sum_{n=1}^{N} r_{nk} = \\sum_{n=1}^{N} r_{nk} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2\n$$\n求解新的方差 $(\\sigma_k^2)^{\\text{new}}$：\n$$\n(\\sigma_k^2)^{\\text{new}} = \\frac{\\sum_{n=1}^{N} r_{nk} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2}{d \\sum_{n=1}^{N} r_{nk}}\n$$\n这是第三个所需的表达式。因子 $d$ 源于各向同性高斯的维度。\n\n### 更新方程总结\n在 EM 算法的每次迭代中，我们首先使用当前参数计算责任度 $r_{nk}$ (E-step)，然后使用这些责任度重新估计模型参数 (M-step)。为方便表示，上标 \"old\" 和 \"new\" 被省略，但需理解计算是按顺序进行的。\n\n**E 步骤：**\n$$ r_{nk} = \\frac{\\pi_{k} \\,\\mathcal{N}(\\mathbf{x}_{n} \\mid \\boldsymbol{\\mu}_{k}, \\sigma_{k}^{2}\\mathbf{I})}{\\sum_{j=1}^{K} \\pi_{j} \\,\\mathcal{N}(\\mathbf{x}_{n} \\mid \\boldsymbol{\\mu}_{j}, \\sigma_{j}^{2}\\mathbf{I})} $$\n**M 步骤：**\n$$ \\boldsymbol{\\mu}_{k} = \\frac{\\sum_{n=1}^{N} r_{nk} \\mathbf{x}_{n}}{\\sum_{n=1}^{N} r_{nk}} $$\n$$ \\sigma_{k}^{2} = \\frac{\\sum_{n=1}^{N} r_{nk} \\|\\mathbf{x}_{n} - \\boldsymbol{\\mu}_{k}\\|^{2}}{d \\sum_{n=1}^{N} r_{nk}} $$\n(注意：$\\pi_k$ 的更新公式为 $\\pi_k = \\frac{1}{N} \\sum_{n=1}^N r_{nk}$，但这并非题目要求。)\n这三个表达式构成了该模型 EM 算法的核心。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nr_{nk} \\\\\n\\boldsymbol{\\mu}_{k} \\\\\n\\sigma_{k}^{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{\\pi_{k} (\\sigma_{k}^{2})^{-\\frac{d}{2}} \\exp(-\\frac{1}{2\\sigma_{k}^{2}} \\|\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{k}\\|^{2})}{\\sum_{j=1}^{K} \\pi_{j} (\\sigma_{j}^{2})^{-\\frac{d}{2}} \\exp(-\\frac{1}{2\\sigma_{j}^{2}} \\|\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{j}\\|^{2})} \\\\\n\\frac{\\sum_{n=1}^{N} r_{nk} \\mathbf{x}_{n}}{\\sum_{n=1}^{N} r_{nk}} \\\\\n\\frac{\\sum_{n=1}^{N} r_{nk} \\|\\mathbf{x}_{n} - \\boldsymbol{\\mu}_{k}\\|^{2}}{d \\sum_{n=1}^{N} r_{nk}}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}