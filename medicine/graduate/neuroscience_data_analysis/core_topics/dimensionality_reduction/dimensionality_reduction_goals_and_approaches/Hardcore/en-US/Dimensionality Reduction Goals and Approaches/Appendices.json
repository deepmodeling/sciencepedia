{
    "hands_on_practices": [
        {
            "introduction": "Applying a method like Principal Component Analysis (PCA) is often just the first step in analyzing high-dimensional neural data. To derive scientific insights, we must interpret the resulting low-dimensional components by relating them back to the individual neurons. This exercise  provides a principled way to quantify each neuron's contribution to the dominant patterns of population activity, transforming abstract principal components into interpretable, biologically-grounded features.",
            "id": "4156634",
            "problem": "A neuroscience data matrix can be represented as $X \\in \\mathbb{R}^{n \\times t}$, where $n$ is the number of neurons and $t$ is the number of samples (such as time points or trials). Each row of $X$ corresponds to a single neuronâ€™s activity across samples. As a fundamental preprocessing step, each neuron's activity must be centered by subtracting its sample mean so that the analysis focuses on variance structure around zero. Dimensionality reduction seeks to find a low-dimensional representation that captures the dominant variance structure of $X$. A principled and widely used approach is based on orthonormal decompositions whose properties are well-tested in scientific practice.\n\nFundamental base:\n- Singular Value Decomposition (SVD) of the centered data: given a centered matrix $X$, there exists a factorization $X = U \\Sigma V^\\top$ where $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{t \\times r}$ have orthonormal columns, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is diagonal with nonnegative singular values in nonincreasing order, and $r = \\mathrm{rank}(X)$. The sample covariance in neuron space is $X X^\\top$, and a well-tested identity is $X X^\\top = U \\Sigma^2 U^\\top$, where $\\Sigma^2$ denotes the diagonal matrix of squared singular values.\n- Principal Component Analysis (PCA) is the method that chooses orthonormal axes in neuron space to maximize projected variance. For the centered matrix $X$, the PCA axes are the columns of $U$, and the variance captured by the $j$-th axis equals the square of the corresponding singular value.\n\nTask:\nUsing these fundamental facts, define a principled notion of neuron-specific importance by mapping low-dimensional components back to neurons via loading contributions. The goal is to quantify how much of the low-dimensional variance is attributable to each neuron. Construct the following quantities:\n1. The contribution of a neuron $i$ to a low-dimensional component $j$ should be defined as a nonnegative quantity that sums over neurons to the variance captured by component $j$, and that depends only on the structure of $X$ through the orthonormal basis of components and their captured variances.\n2. The neuron-specific importance should be defined by aggregating the contributions of neuron $i$ over the top $k$ components and normalizing by the total variance captured by these $k$ components, resulting in a decimal fraction in $[0,1]$ for each neuron. If the total captured variance by the top $k$ components is zero, return the zero vector of length $n$.\n\nYour program must:\n- Center $X$ by subtracting the mean of each row.\n- Compute a truncated orthonormal basis of the top $k$ low-dimensional components using the SVD identity $X = U \\Sigma V^\\top$ and the associated captured variances.\n- Map low-dimensional components back to neurons by constructing loading contributions that satisfy the properties above.\n- Aggregate contributions to produce a neuron importance vector of length $n$, with entries expressed as decimals.\n\nTest suite:\nImplement your program to process the following parameter sets. For each case, the result must be the neuron importance vector as a list of floats.\n\n- Case A (happy path, correlated neurons and an orthogonal pattern):\n  $$\n  X = \\begin{bmatrix}\n  0 & 1 & 2 & 3 & 4 \\\\\n  0 & 2 & 4 & 6 & 8 \\\\\n  1 & -1 & 1 & -1 & 1\n  \\end{bmatrix},\\quad k = 2.\n  $$\n- Case B (boundary with a zero-variance neuron):\n  $$\n  X = \\begin{bmatrix}\n  1 & 1 & 1 & 1 \\\\\n  0 & 1 & 0 & 1 \\\\\n  2 & 3 & 4 & 5\n  \\end{bmatrix},\\quad k = 2.\n  $$\n- Case C (duplicate neurons):\n  $$\n  X = \\begin{bmatrix}\n  1 & 2 & 3 & 4 & 5 \\\\\n  1 & 2 & 3 & 4 & 5 \\\\\n  5 & 4 & 3 & 2 & 1\n  \\end{bmatrix},\\quad k = 1.\n  $$\n- Case D (anisotropic variance dominated by one neuron):\n  $$\n  X = \\begin{bmatrix}\n  10 & 0 & -10 & 0 & 10 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  0 & 1 & 0 & 1 & 0\n  \\end{bmatrix},\\quad k = 1.\n  $$\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this top-level list is itself a list of decimal fractions corresponding to the neuron importance values for a test case, in the same row order as $X$. For example: \"[[a,b,c],[d,e,f],...]\".\n- No physical units or angle units are required.\n- Percentages must be expressed as decimals, not with a percentage sign.\n\nCoverage expectations:\n- Case A tests the general scenario with correlated neurons and an orthogonal pattern.\n- Case B tests handling of a neuron with zero variance after centering.\n- Case C tests how duplicated neurons share importance under a single dominant component.\n- Case D tests dominance by a single highly variable neuron.\n\nYour solution must be implementable in any modern programming language. The final answer must be a single, complete, standalone Python program that meets the runtime environment constraints listed later and produces exactly the specified output format.",
            "solution": "The user-provided problem has been analyzed according to the specified validation protocol.\n\n### Step 1: Extract Givens\n- **Data Matrix**: $X \\in \\mathbb{R}^{n \\times t}$, where $n$ is the number of neurons and $t$ is the number of samples.\n- **Preprocessing**: Each row of $X$ must be centered by subtracting its sample mean.\n- **Dimensionality Reduction Goal**: Find a low-dimensional representation capturing dominant variance.\n- **Fundamental Base (SVD)**: For a centered matrix $X$, the Singular Value Decomposition is $X = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{t \\times r}$ have orthonormal columns, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is a diagonal matrix of non-increasing singular values, and $r = \\mathrm{rank}(X)$.\n- **Fundamental Base (Covariance Identity)**: The sample covariance in neuron space is $X X^\\top = U \\Sigma^2 U^\\top$, where $\\Sigma^2$ contains squared singular values.\n- **Fundamental Base (PCA)**: Principal Component Analysis (PCA) axes for centered $X$ are the columns of $U$. The variance captured by the $j$-th axis is the square of the corresponding singular value, $\\sigma_j^2$.\n- **Task 1 (Contribution Definition)**: Define the contribution of a neuron $i$ to a component $j$. This quantity must be:\n    1. Nonnegative.\n    2. Sum over all neurons $i$ to the variance of component $j$.\n    3. Depend only on the orthonormal basis of components ($U$) and their captured variances ($\\sigma_j^2$).\n- **Task 2 (Neuron Importance Definition)**: Define neuron-specific importance by aggregating the contributions of neuron $i$ over the top $k$ components and normalizing by the total variance of these $k$ components. The result for each neuron is a decimal fraction in $[0,1]$. If the total captured variance is zero, the importance vector is a zero vector of length $n$.\n- **Test Cases**:\n    - Case A: $X = \\begin{bmatrix} 0 & 1 & 2 & 3 & 4 \\\\ 0 & 2 & 4 & 6 & 8 \\\\ 1 & -1 & 1 & -1 & 1 \\end{bmatrix}, k = 2$.\n    - Case B: $X = \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 0 & 1 & 0 & 1 \\\\ 2 & 3 & 4 & 5 \\end{bmatrix}, k = 2$.\n    - Case C: $X = \\begin{bmatrix} 1 & 2 & 3 & 4 & 5 \\\\ 1 & 2 & 3 & 4 & 5 \\\\ 5 & 4 & 3 & 2 & 1 \\end{bmatrix}, k = 1$.\n    - Case D: $X = \\begin{bmatrix} 10 & 0 & -10 & 0 & 10 \\\\ 1 & 1 & 1 & 1 & 1 \\\\ 0 & 1 & 0 & 1 & 0 \\end{bmatrix}, k = 1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The problem is based on Principal Component Analysis (PCA) via Singular Value Decomposition (SVD), which are fundamental and universally accepted methods in linear algebra and data analysis. The relationship between SVD and the covariance matrix eigendecomposition ($X X^\\top = U \\Sigma^2 U^\\top$) is a standard mathematical identity. The task of quantifying variable contributions to principal components is a standard and meaningful part of interpreting PCA results.\n- **Well-Posed**: The problem is well-posed. It asks for the construction of a quantity (\"neuron-specific importance\") based on a set of clear, constructive constraints. These constraints lead to a unique and stable mathematical formula, as will be demonstrated in the solution. The inputs ($X$, $k$) and the desired output format are specified, and an edge case (zero variance) is explicitly handled.\n- **Objective**: The problem statement is written in precise, objective, and mathematical language. It is free from ambiguity, subjectivity, or opinion-based claims.\n- **No other flaws are detected**: The problem is not unsound, incomplete, unrealistic, or trivial. It represents a standard, formalizable task in scientific data analysis.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A principled solution will now be developed.\n\n### Principle-Based Design\nThe solution is constructed by following the principles and definitions provided in the problem statement.\n\n**1. Data Centering**\nThe first step is to center the data matrix $X$. For each neuron $i$ (row $i$), we compute its mean activity $\\mu_i$ across all $t$ samples.\n$$\n\\mu_i = \\frac{1}{t} \\sum_{j=1}^{t} X_{ij}\n$$\nWe then form the centered data matrix, which we denote as $X_c$, by subtracting the corresponding mean from each element.\n$$\n(X_c)_{ij} = X_{ij} - \\mu_i\n$$\nThis procedure ensures that the subsequent analysis focuses on the variance structure of the neural activity, rather than being influenced by baseline offsets in firing rates.\n\n**2. Orthonormal Decomposition via SVD**\nThe core of the analysis is the Singular Value Decomposition (SVD) of the centered matrix $X_c$. As stated in the problem, this decomposition is given by:\n$$\nX_c = U \\Sigma V^\\top\n$$\nHere, the columns of $U \\in \\mathbb{R}^{n \\times r}$ are the principal components (or principal axes) in the $n$-dimensional neuron space. These are orthonormal vectors, forming a basis. The diagonal matrix $\\Sigma \\in \\mathbb{R}^{r \\times r}$ contains the singular values $\\sigma_j$ in non-increasing order. The variance captured by the $j$-th principal component is $\\lambda_j = \\sigma_j^2$.\n\n**3. Neuron Contribution to a Component**\nThe problem requires constructing a quantity, let's call it $C_{ij}$, representing the contribution of neuron $i$ to the variance of component $j$. This quantity must satisfy three specific properties. We propose a definition and verify it against these properties.\n\nLet $u_j$ be the $j$-th column of $U$, with elements $u_{ij}$. The vector $u_j$ represents the $j$-th principal component axis. The value $u_{ij}$ is the loading of neuron $i$ on this component. We define the contribution as:\n$$\nC_{ij} = u_{ij}^2 \\sigma_j^2\n$$\nLet's verify this definition:\n1.  **Nonnegativity**: Since $u_{ij}^2 \\ge 0$ and $\\sigma_j^2 \\ge 0$, their product $C_{ij}$ is always nonnegative. This property is satisfied.\n2.  **Sum to Component Variance**: We must check if $\\sum_{i=1}^{n} C_{ij} = \\sigma_j^2$.\n    $$\n    \\sum_{i=1}^{n} C_{ij} = \\sum_{i=1}^{n} u_{ij}^2 \\sigma_j^2 = \\sigma_j^2 \\left( \\sum_{i=1}^{n} u_{ij}^2 \\right)\n    $$\n    Since the columns of $U$ are orthonormal vectors, the sum of the squares of the elements in any column is $1$, i.e., $\\|u_j\\|^2 = \\sum_{i=1}^{n} u_{ij}^2 = 1$. Therefore:\n    $$\n    \\sum_{i=1}^{n} C_{ij} = \\sigma_j^2 \\cdot 1 = \\sigma_j^2\n    $$\n    This property is satisfied. The aformentioned definition correctly partitions the variance of a component among all neurons.\n3.  **Dependency**: The quantity $C_{ij}$ depends only on $u_{ij}$ (from the orthonormal basis $U$) and $\\sigma_j^2$ (the captured variance), as required.\n\nThis definition of contribution is therefore principled and consistent with all requirements.\n\n**4. Neuron-Specific Importance**\nThe final step is to calculate the neuron-specific importance, $I_i$, for each neuron $i$. The problem defines this as the aggregation of contributions over the top $k$ components, normalized by the total variance captured by those same components.\n\nThe total contribution of neuron $i$ to the top $k$ components is the sum of its individual contributions:\n$$\n\\text{Total Contribution}_i = \\sum_{j=1}^{k} C_{ij} = \\sum_{j=1}^{k} u_{ij}^2 \\sigma_j^2\n$$\nThe total variance captured by the top $k$ components is the sum of their individual variances:\n$$\n\\text{Total Variance}_k = \\sum_{j=1}^{k} \\sigma_j^2\n$$\nThe neuron-specific importance $I_i$ is the ratio of these two quantities:\n$$\nI_i = \\frac{\\sum_{j=1}^{k} u_{ij}^2 \\sigma_j^2}{\\sum_{j=1}^{k} \\sigma_j^2}\n$$\nThis value represents the fraction of the total variance in the top-$k$ dimensional subspace that is attributable to neuron $i$. If the denominator is zero (i.e., the centered matrix is a zero matrix), the problem specifies to return a zero vector, which this formula handles correctly by the provided explicit condition. Summing $I_i$ over all neurons gives $\\sum_{i=1}^n I_i = 1$, confirming it's a valid fractional distribution.\n\n**Algorithmic Implementation**\nThe procedure is as follows:\n1.  Given $X$ and $k$, compute the row-wise mean of $X$ and subtract it to get $X_c$.\n2.  Compute the SVD of $X_c$ to obtain $U$, singular values $S$, and $V^\\top$.\n3.  Select the top $k$ columns of $U$ (let's call this submatrix $U_k$) and the top $k$ singular values (vector $S_k$).\n4.  Calculate the variances of the top $k$ components: `variances_k = S_k**2`.\n5.  Calculate the total variance: `total_variance_k = sum(variances_k)`.\n6.  If `total_variance_k` is $0$, return a vector of zeros of length $n$.\n7.  Calculate the numerator for each neuron: `neuron_total_contributions`, an $n$-element vector where the $i$-th element is $\\sum_{j=1}^k (U_k)_{ij}^2 \\cdot (\\text{variances\\_k})_j$.\n8.  Divide `neuron_total_contributions` by `total_variance_k` to get the final importance vector.\n\nThis sequence of operations directly implements the derived formula and satisfies all problem constraints.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_importance(X: np.ndarray, k: int) -> np.ndarray:\n    \"\"\"\n    Calculates neuron-specific importance based on PCA/SVD.\n\n    Args:\n        X (np.ndarray): The data matrix of shape (n, t), where n is the number\n                        of neurons and t is the number of samples.\n        k (int): The number of top principal components to consider.\n\n    Returns:\n        np.ndarray: A 1D array of length n containing the importance score\n                    for each neuron, expressed as a decimal fraction.\n    \"\"\"\n    n, t = X.shape\n\n    # Step 1: Center the data by subtracting the mean of each row.\n    # Using keepdims=True ensures that the mean array has shape (n, 1)\n    # for correct broadcasting during subtraction.\n    row_means = X.mean(axis=1, keepdims=True)\n    X_centered = X - row_means\n\n    # Step 2: Compute the Singular Value Decomposition (SVD).\n    # full_matrices=False is more efficient as we don't need the full U or V.\n    # U will have shape (n, min(n,t)), S will be a 1D array of singular values.\n    try:\n        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n    except np.linalg.LinAlgError:\n        # This can happen for ill-conditioned matrices, though unlikely with test data.\n        # A matrix of all zeros would result in zero singular values.\n        return np.zeros(n)\n\n    # Effective number of components cannot exceed the rank of the matrix.\n    num_components = len(S)\n    k_eff = min(k, num_components)\n\n    if k_eff == 0:\n        return np.zeros(n)\n        \n    # Step 3: Select top k components and their variances.\n    U_k = U[:, :k_eff]\n    S_k = S[:k_eff]\n    variances_k = S_k**2\n\n    # Step 4: Calculate total variance captured by the top k components.\n    total_variance_k = np.sum(variances_k)\n\n    # Handle the edge case where total variance is zero.\n    if total_variance_k == 0:\n        return np.zeros(n)\n\n    # Step 5: Calculate neuron contributions and aggregate them.\n    # The term u_ij^2 * sigma_j^2 is the contribution of neuron i to component j.\n    # We sum these contributions over the top k components for each neuron.\n    # In NumPy, (U_k**2) has shape (n, k_eff). `variances_k` is broadcasted\n    # for element-wise multiplication across columns.\n    neuron_contributions = (U_k**2) * variances_k\n    neuron_total_contributions = np.sum(neuron_contributions, axis=1)\n\n    # Step 6: Normalize to get the final importance scores.\n    neuron_importance = neuron_total_contributions / total_variance_k\n\n    return neuron_importance\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [0, 1, 2, 3, 4],\n            [0, 2, 4, 6, 8],\n            [1, -1, 1, -1, 1]\n        ], dtype=float), 2),\n        (np.array([\n            [1, 1, 1, 1],\n            [0, 1, 0, 1],\n            [2, 3, 4, 5]\n        ], dtype=float), 2),\n        (np.array([\n            [1, 2, 3, 4, 5],\n            [1, 2, 3, 4, 5],\n            [5, 4, 3, 2, 1]\n        ], dtype=float), 1),\n        (np.array([\n            [10, 0, -10, 0, 10],\n            [1, 1, 1, 1, 1],\n            [0, 1, 0, 1, 0]\n        ], dtype=float), 1)\n    ]\n\n    results = []\n    for X, k in test_cases:\n        importance_vector = calculate_importance(X, k)\n        results.append(importance_vector.tolist())\n\n    # Format the output string to exactly match the specification:\n    # \"[[a,b,c],[d,e,f],...]\" without spaces after commas inside inner lists.\n    list_of_strings = []\n    for res_list in results:\n        inner_str = '[' + ','.join(map(str, res_list)) + ']'\n        list_of_strings.append(inner_str)\n    \n    final_output_str = '[' + ','.join(list_of_strings) + ']'\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While linear methods like PCA excel at capturing flat, hyperplane-like structures, neural population activity is often organized along curved manifolds. Kernel Principal Component Analysis (kPCA) offers a powerful solution by extending PCA to uncover these nonlinear relationships using the 'kernel trick'. In this hands-on practice , you will derive kPCA from first principles and apply it to a simulated dataset, building the foundational skills needed to analyze complex, curved geometries in neural data.",
            "id": "4156661",
            "problem": "You are given a collection of simulated neural recordings represented as high-dimensional vectors. Each vector corresponds to a single trial consisting of spike counts from $n_{\\text{neurons}}$ neurons across $n_{\\text{bins}}$ time bins, flattened into a single vector. The task is to formally derive Kernel Principal Component Analysis (Kernel PCA) from first principles and implement an algorithm to compute embeddings using a Gaussian kernel, including proper centering in feature space. Then, apply this algorithm to the specified neural dataset and produce quantitative outputs for a test suite.\n\nFundamental base and core definitions:\n- Principal Component Analysis (PCA) in feature space starts from the covariance operator defined on centered feature vectors. Consider a mapping $\\phi:\\mathbb{R}^d\\to\\mathcal{H}$ into a Reproducing Kernel Hilbert Space (RKHS). For $n$ training samples $\\{x_i\\}_{i=1}^n$, let $\\Phi_i=\\phi(x_i)-\\bar{\\phi}$ be the centered feature vectors with $\\bar{\\phi}=\\frac{1}{n}\\sum_{i=1}^n\\phi(x_i)$. The feature-space covariance operator is $C=\\frac{1}{n}\\sum_{i=1}^n \\Phi_i\\Phi_i^{\\top}$. PCA seeks eigenpairs $(\\lambda,w)$ satisfying $Cw=\\lambda w$ with unit norm $\\|w\\|_{\\mathcal{H}}=1$.\n- The kernel trick uses the Gram matrix $K$ whose entries are $K_{ij}=k(x_i,x_j)=\\langle \\phi(x_i),\\phi(x_j)\\rangle_{\\mathcal{H}}$. Feature-space centering corresponds to centering $K$ via a centering matrix.\n- For a Gaussian kernel, $k(x,y)=\\exp\\left(-\\frac{\\|x-y\\|^2}{2\\sigma^2}\\right)$ for a bandwidth parameter $\\sigma>0$.\n\nDerivation tasks:\n1. Starting from the PCA definition in feature space, show that any principal direction satisfies $w=\\sum_{i=1}^n\\alpha_i\\Phi_i$ for some coefficients $\\alpha\\in\\mathbb{R}^n$ and derive an eigenvalue problem in terms of the centered Gram matrix $K_c$. Explicitly derive the relationship between the eigenvalues of the covariance operator $C$ and those of the centered Gram matrix $K_c$.\n2. Derive the centering of the Gram matrix in feature space using the centering matrix $H=I-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$, and prove that $K_c=HKH$ corresponds to inner products between centered feature vectors, i.e., $(K_c)_{ij}=\\langle \\Phi_i,\\Phi_j\\rangle_{\\mathcal{H}}$.\n3. For out-of-sample embedding of a new point $x$, derive the formula for the centered kernel vector $k_c(x)\\in\\mathbb{R}^n$ with entries $[k_c(x)]_i=\\langle \\Phi_i,\\phi(x)-\\bar{\\phi}\\rangle_{\\mathcal{H}}$, expressed solely in terms of $K$, the training mean, and $k(x_i,x)$. Derive the projection onto principal directions in terms of the eigenvectors and eigenvalues of $K_c$.\n\nAlgorithmic implementation requirements:\n- Construct a simulated neural dataset with the following specifications to ensure scientific realism and reproducibility:\n  - Set a fixed random seed $s=12345$.\n  - Use $n_{\\text{neurons}}=40$, $n_{\\text{bins}}=20$, and $n_{\\text{trials}}=60$ for training, and generate one additional held-out test trial.\n  - Draw baseline firing rates per neuron independently from a uniform distribution over $[5,15]$ (in arbitrary units per bin).\n  - Define a temporal modulation $g(t)=\\exp\\left(-\\frac{(t-t_0)^2}{2\\sigma_t^2}\\right)$ with $t_0=10$ and $\\sigma_t=3$ for $t=0,1,\\dots,19$.\n  - Define condition-specific amplitudes: for condition $A$ (half the trials), draw amplitudes $a_i^{(A)}$ independently from a normal distribution with mean $3$ and standard deviation $1$; for condition $B$ (other half), draw $a_i^{(B)}$ independently from a normal distribution with mean $-3$ and standard deviation $1$. Add trial-wise amplitude noise by adding independent $\\mathcal{N}(0,0.5)$ to each neuron's amplitude. Clip instantaneous rates at a minimum of $0.1$ to avoid negative rates.\n  - Generate spike counts per bin using independent Poisson draws with the specified rates and flatten each trial into a vector in $\\mathbb{R}^{800}$.\n  - Standardize the training data across trials: for each feature dimension, subtract the across-trial mean and divide by the across-trial standard deviation; replace any zero standard deviation by $1$ to avoid division by zero. Apply the same centering and scaling to the held-out test trial using the training mean and standard deviation.\n\n- Kernel PCA implementation details:\n  - Compute the Gaussian kernel matrix $K\\in\\mathbb{R}^{n\\times n}$ for the training data using the specified $\\sigma$ by $K_{ij}=\\exp\\left(-\\frac{\\|x_i-x_j\\|^2}{2\\sigma^2}\\right)$.\n  - Center the Gram matrix in feature space using $H=I-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ and $K_c=HKH$.\n  - Solve the symmetric eigenvalue problem for $K_c$ to obtain eigenvalues $\\mu_k\\ge 0$ and orthonormal eigenvectors $v_k\\in\\mathbb{R}^n$ satisfying $K_c v_k=\\mu_k v_k$. Ensure numerical stability by sorting eigenvalues in descending order and clipping any tiny negative eigenvalues caused by numerical error to $0$.\n  - Normalize so that the feature-space principal directions have unit norm by setting $w_k=\\frac{1}{\\sqrt{\\mu_k}}\\sum_{i=1}^n (v_k)_i \\Phi_i$ for $\\mu_k>0$.\n  - Compute training embeddings by $y_{k}(x_j)=\\langle w_k,\\Phi_j\\rangle_{\\mathcal{H}}=\\sqrt{\\mu_k}\\,(v_k)_j$.\n  - For a new point $x$, compute the centered kernel vector $k_c(x)$ via\n    $$k_c(x)=k(x)-\\frac{1}{n}K\\mathbf{1}-\\frac{1}{n}\\left(\\sum_{i=1}^n k(x_i,x)\\right)\\mathbf{1}+\\frac{1}{n^2}\\left(\\mathbf{1}^{\\top}K\\mathbf{1}\\right)\\mathbf{1},$$\n    where $k(x)$ is the vector with entries $k(x_i,x)$, and $\\mathbf{1}$ is the all-ones vector. Then compute out-of-sample embeddings by\n    $$y_k(x)=\\frac{v_k^{\\top}k_c(x)}{\\sqrt{\\mu_k}},\\quad \\text{with the convention } y_k(x)=0 \\text{ if }\\mu_k=0.$$\n\nTest suite and outputs:\n- Use the single training dataset and held-out test trial defined above for all test cases. For each case below, compute the required outputs.\n  1. Case $A$ (happy path): $\\sigma=5.0$, retain $m=3$ leading components. Output the list of the first $m$ explained variance fractions, computed as $\\left[\\frac{\\mu_1}{\\sum_{j}\\mu_j},\\frac{\\mu_2}{\\sum_{j}\\mu_j},\\frac{\\mu_3}{\\sum_{j}\\mu_j}\\right]$ as decimal floats.\n  2. Case $B$ (small bandwidth edge case): $\\sigma=0.1$, retain $m=2$ leading components. Output the list of the first two out-of-sample embedding coordinates for the held-out test trial $[y_1(x_{\\text{test}}),y_2(x_{\\text{test}})]$ as floats.\n  3. Case $C$ (large bandwidth boundary case): $\\sigma=1000.0$. Output the Frobenius norm of the centered Gram matrix $\\|K_c\\|_F$ as a single float, which should be near zero for this case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The three elements correspond to the outputs for Cases $A$, $B$, and $C$, in order. Since Cases $A$ and $B$ require lists, the final format will be of the form\n  $$[ [f_1,f_2,f_3], [y_1,y_2], \\nu ]$$\n  where $f_i$ are explained fraction floats, $y_i$ are embedding floats, and $\\nu$ is the Frobenius norm float.",
            "solution": "The problem statement is well-posed, scientifically grounded, and internally consistent. It provides a complete set of definitions, parameters, and tasks for a formal derivation and implementation of Kernel Principal Component Analysis (Kernel PCA) applied to a simulated neuroscience dataset. All necessary components are specified, allowing for a unique and verifiable solution.\n\nWe will proceed by first providing the requested theoretical derivations and then implementing the algorithm as specified.\n\n### 1. Derivation of the Kernel PCA Eigenvalue Problem\n\nThe objective of PCA in the feature space $\\mathcal{H}$ is to find the eigenvectors $w \\in \\mathcal{H}$ of the covariance operator $C = \\frac{1}{n}\\sum_{i=1}^n \\Phi_i\\Phi_i^{\\top}$, where $\\Phi_i = \\phi(x_i) - \\bar{\\phi}$ are the centered feature vectors. The eigenvalue problem is given by:\n$$Cw = \\lambda w, \\quad \\text{with } \\|w\\|_{\\mathcal{H}}=1$$\nSubstituting the definition of $C$:\n$$ \\frac{1}{n}\\sum_{i=1}^n \\Phi_i\\Phi_i^{\\top} w = \\lambda w $$\nThe term $\\Phi_i^{\\top} w$ is the inner product $\\langle \\Phi_i, w \\rangle_{\\mathcal{H}}$, which is a scalar. This equation demonstrates that any eigenvector $w$ must lie in the span of the centered data points in the feature space, $\\{\\Phi_i\\}_{i=1}^n$. Therefore, $w$ can be expressed as a linear combination of these vectors:\n$$ w = \\sum_{i=1}^n \\alpha_i \\Phi_i $$\nfor some coefficients $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^{\\top} \\in \\mathbb{R}^n$.\n\nSubstituting this expansion of $w$ back into the eigenvalue equation:\n$$ \\frac{1}{n}\\sum_{j=1}^n \\Phi_j \\langle \\Phi_j, \\sum_{i=1}^n \\alpha_i \\Phi_i \\rangle = \\lambda \\sum_{i=1}^n \\alpha_i \\Phi_i $$\nTo solve for the coefficients $\\alpha_i$, we project this entire equation onto a basis vector $\\Phi_k$ for an arbitrary $k \\in \\{1, \\dots, n\\}$:\n$$ \\langle \\Phi_k, \\frac{1}{n}\\sum_{j=1}^n \\Phi_j \\sum_{i=1}^n \\alpha_i \\langle \\Phi_j, \\Phi_i \\rangle \\rangle = \\langle \\Phi_k, \\lambda \\sum_{i=1}^n \\alpha_i \\Phi_i \\rangle $$\nUsing the linearity of the inner product:\n$$ \\frac{1}{n}\\sum_{j=1}^n \\langle \\Phi_k, \\Phi_j \\rangle \\sum_{i=1}^n \\alpha_i \\langle \\Phi_j, \\Phi_i \\rangle = \\lambda \\sum_{i=1}^n \\alpha_i \\langle \\Phi_k, \\Phi_i \\rangle $$\nLet us define the centered Gram matrix $K_c \\in \\mathbb{R}^{n \\times n}$ with entries $(K_c)_{ij} = \\langle \\Phi_i, \\Phi_j \\rangle_{\\mathcal{H}}$. The equation becomes:\n$$ \\frac{1}{n}\\sum_{j=1}^n (K_c)_{kj} \\sum_{i=1}^n (K_c)_{ji} \\alpha_i = \\lambda \\sum_{i=1}^n (K_c)_{ki} \\alpha_i $$\nIn matrix notation, this is:\n$$ \\frac{1}{n} K_c (K_c \\alpha) = \\lambda (K_c \\alpha) $$\nThis implies that $K_c \\alpha$ is an eigenvector of $K_c$ with eigenvalue $n\\lambda$. If $K_c \\alpha \\neq 0$, then $\\alpha$ is also an eigenvector of $K_c$ with the same eigenvalue. Let $\\mu = n\\lambda$. The eigenvalue problem for the coefficients $\\alpha$ is:\n$$ K_c \\alpha = \\mu \\alpha $$\nThis demonstrates that solving the eigenvalue problem for the $n \\times n$ centered Gram matrix $K_c$ is equivalent to solving the PCA problem in the (potentially infinite-dimensional) feature space $\\mathcal{H}$. The eigenvalues $\\mu$ of $K_c$ are related to the eigenvalues $\\lambda$ of the covariance operator $C$ by $\\lambda = \\mu/n$.\n\nThe normalization condition $\\|w\\|_{\\mathcal{H}}=1$ imposes a constraint on the eigenvectors $\\alpha$:\n$$ 1 = \\|w\\|^2 = \\langle \\sum_{i=1}^n \\alpha_i \\Phi_i, \\sum_{j=1}^n \\alpha_j \\Phi_j \\rangle = \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j \\langle \\Phi_i, \\Phi_j \\rangle = \\alpha^{\\top} K_c \\alpha $$\nSubstituting $K_c \\alpha = \\mu \\alpha$:\n$$ 1 = \\alpha^{\\top} (\\mu \\alpha) = \\mu (\\alpha^{\\top} \\alpha) = \\mu \\|\\alpha\\|^2 $$\nThus, $\\|\\alpha\\|^2 = 1/\\mu$. If we find orthonormal eigenvectors $v_k$ of $K_c$ such that $K_c v_k = \\mu_k v_k$ and $v_k^{\\top}v_k=1$, the corresponding coefficient vector $\\alpha_k$ is a scaled version of $v_k$, specifically $\\alpha_k = v_k / \\sqrt{\\mu_k}$. The feature-space principal direction is then $w_k = \\frac{1}{\\sqrt{\\mu_k}}\\sum_{i=1}^n (v_k)_i \\Phi_i$.\n\n### 2. Derivation of the Centered Gram Matrix\n\nThe entries of the centered Gram matrix $K_c$ are $(K_c)_{ij} = \\langle \\Phi_i, \\Phi_j \\rangle_{\\mathcal{H}}$, where $\\Phi_i = \\phi(x_i) - \\bar{\\phi}$ and $\\bar{\\phi} = \\frac{1}{n}\\sum_{l=1}^n \\phi(x_l)$. We expand the inner product:\n$$ (K_c)_{ij} = \\langle \\phi(x_i) - \\bar{\\phi}, \\phi(x_j) - \\bar{\\phi} \\rangle = \\langle \\phi(x_i), \\phi(x_j) \\rangle - \\langle \\phi(x_i), \\bar{\\phi} \\rangle - \\langle \\bar{\\phi}, \\phi(x_j) \\rangle + \\langle \\bar{\\phi}, \\bar{\\phi} \\rangle $$\nWe express each term using the kernel function $k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle$ and the uncentered Gram matrix $K$ with $K_{ij}=k(x_i,x_j)$:\n\\begin{itemize}\n    \\item $\\langle \\phi(x_i), \\phi(x_j) \\rangle = K_{ij}$\n    \\item $\\langle \\phi(x_i), \\bar{\\phi} \\rangle = \\langle \\phi(x_i), \\frac{1}{n}\\sum_{l=1}^n \\phi(x_l) \\rangle = \\frac{1}{n}\\sum_{l=1}^n \\langle \\phi(x_i), \\phi(x_l) \\rangle = \\frac{1}{n}\\sum_{l=1}^n K_{il}$\n    \\item $\\langle \\bar{\\phi}, \\phi(x_j) \\rangle = \\frac{1}{n}\\sum_{l=1}^n K_{lj}$\n    \\item $\\langle \\bar{\\phi}, \\bar{\\phi} \\rangle = \\langle \\frac{1}{n}\\sum_{l=1}^n \\phi(x_l), \\frac{1}{n}\\sum_{m=1}^n \\phi(x_m) \\rangle = \\frac{1}{n^2}\\sum_{l=1}^n \\sum_{m=1}^n \\langle \\phi(x_l), \\phi(x_m) \\rangle = \\frac{1}{n^2}\\sum_{l,m} K_{lm}$\n\\end{itemize}\nCombining these gives:\n$$ (K_c)_{ij} = K_{ij} - \\frac{1}{n}\\sum_{l=1}^n K_{il} - \\frac{1}{n}\\sum_{l=1}^n K_{lj} + \\frac{1}{n^2}\\sum_{l,m} K_{lm} $$\nNow we show this is equivalent to $HKH$, where $H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ is the centering matrix ($I$ is the identity matrix, $\\mathbf{1}$ is the all-ones vector).\n$$ K_c = HKH = (I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}) K (I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}) $$\nExpanding this product:\n$$ K_c = K - \\frac{1}{n}K\\mathbf{1}\\mathbf{1}^{\\top} - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}K + \\frac{1}{n^2}\\mathbf{1}\\mathbf{1}^{\\top}K\\mathbf{1}\\mathbf{1}^{\\top} $$\nLet's analyze the $(i,j)$-th element of each matrix term:\n\\begin{itemize}\n    \\item $(K)_{ij} = K_{ij}$\n    \\item $(K\\mathbf{1}\\mathbf{1}^{\\top})_{ij} = (K\\mathbf{1})_i (\\mathbf{1}^{\\top})_j = (\\sum_{l=1}^n K_{il}) \\cdot 1 = \\sum_{l=1}^n K_{il}$\n    \\item $(\\mathbf{1}\\mathbf{1}^{\\top}K)_{ij} = (\\mathbf{1})_i (\\mathbf{1}^{\\top}K)_j = 1 \\cdot (\\sum_{l=1}^n K_{lj}) = \\sum_{l=1}^n K_{lj}$\n    \\item $(\\mathbf{1}\\mathbf{1}^{\\top}K\\mathbf{1}\\mathbf{1}^{\\top})_{ij} = (\\mathbf{1})_i (\\mathbf{1}^{\\top}K\\mathbf{1}) (\\mathbf{1}^{\\top})_j$. The middle term $\\mathbf{1}^{\\top}K\\mathbf{1} = \\sum_{l,m} K_{lm}$ is a scalar. Thus, the $(i,j)$-th element is $\\sum_{l,m} K_{lm}$.\n\\end{itemize}\nAssembling the terms for $(HKH)_{ij}$:\n$$ (HKH)_{ij} = K_{ij} - \\frac{1}{n}\\sum_{l=1}^n K_{il} - \\frac{1}{n}\\sum_{l=1}^n K_{lj} + \\frac{1}{n^2}\\sum_{l,m} K_{lm} $$\nThis matches the expression for $(K_c)_{ij}$ derived from first principles. Thus, centering the Gram matrix via $K_c=HKH$ is equivalent to computing the inner products of the centered feature vectors.\n\n### 3. Derivation of Out-of-Sample Embedding\n\nFor a new data point $x$, its embedding onto the $k$-th principal component is the projection of its centered feature vector $\\Phi(x) = \\phi(x) - \\bar{\\phi}$ onto the principal direction $w_k$:\n$$ y_k(x) = \\langle w_k, \\Phi(x) \\rangle_{\\mathcal{H}} $$\nUsing the expansion $w_k = \\sum_{i=1}^n (\\alpha_k)_i \\Phi_i$ where $\\alpha_k = v_k/\\sqrt{\\mu_k}$ (for $\\mu_k > 0$):\n$$ y_k(x) = \\langle \\sum_{i=1}^n (\\alpha_k)_i \\Phi_i, \\Phi(x) \\rangle = \\sum_{i=1}^n (\\alpha_k)_i \\langle \\Phi_i, \\Phi(x) \\rangle $$\nLet's define a vector $k_c(x) \\in \\mathbb{R}^n$ whose elements are $[k_c(x)]_i = \\langle \\Phi_i, \\Phi(x) \\rangle$. The projection is then $y_k(x) = \\alpha_k^{\\top} k_c(x)$. Substituting $\\alpha_k = v_k/\\sqrt{\\mu_k}$:\n$$ y_k(x) = \\frac{v_k^{\\top} k_c(x)}{\\sqrt{\\mu_k}} $$\nIf $\\mu_k=0$, the corresponding principal component does not exist (zero variance), and the projection is conventionally set to $0$.\n\nNow we derive the formula for the entries of $k_c(x)$:\n$$ [k_c(x)]_i = \\langle \\Phi_i, \\Phi(x) \\rangle = \\langle \\phi(x_i) - \\bar{\\phi}, \\phi(x) - \\bar{\\phi} \\rangle $$\n$$ = \\langle \\phi(x_i), \\phi(x) \\rangle - \\langle \\phi(x_i), \\bar{\\phi} \\rangle - \\langle \\bar{\\phi}, \\phi(x) \\rangle + \\langle \\bar{\\phi}, \\bar{\\phi} \\rangle $$\nThe terms are evaluated as before:\n\\begin{itemize}\n    \\item $\\langle \\phi(x_i), \\phi(x) \\rangle = k(x_i, x)$\n    \\item $\\langle \\phi(x_i), \\bar{\\phi} \\rangle = \\frac{1}{n}\\sum_{j=1}^n K_{ij}$\n    \\item $\\langle \\bar{\\phi}, \\phi(x) \\rangle = \\frac{1}{n}\\sum_{j=1}^n k(x_j, x)$\n    \\item $\\langle \\bar{\\phi}, \\bar{\\phi} \\rangle = \\frac{1}{n^2}\\sum_{j,l} K_{jl}$\n\\end{itemize}\nSo, for each $i \\in \\{1, \\dots, n\\}$:\n$$ [k_c(x)]_i = k(x_i, x) - \\frac{1}{n}\\sum_{j=1}^n K_{ij} - \\frac{1}{n}\\sum_{j=1}^n k(x_j, x) + \\frac{1}{n^2}\\sum_{j,l} K_{jl} $$\nIn vector form, let $k(x)$ be the vector with entries $k(x_i, x)$ and $\\mathbf{1}$ be the all-ones vector. The equation for the vector $k_c(x)$ is:\n$$ k_c(x) = k(x) - \\frac{1}{n}K\\mathbf{1} - \\frac{1}{n}(\\mathbf{1}^{\\top}k(x))\\mathbf{1} + \\frac{1}{n^2}(\\mathbf{1}^{\\top}K\\mathbf{1})\\mathbf{1} $$\nThis matches the formula provided in the problem statement and allows the out-of-sample embedding to be computed using only kernel evaluations with the training data and the new point.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate data generation, Kernel PCA computation,\n    and evaluation for the specified test cases.\n    \"\"\"\n\n    def generate_data(seed=12345):\n        \"\"\"\n        Generates simulated neural data according to the problem specification.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Parameters\n        n_neurons = 40\n        n_bins = 20\n        n_trials_train = 60\n        n_trials_total = n_trials_train + 1  # +1 for test trial\n        d = n_neurons * n_bins\n        \n        # Baseline firing rates\n        baseline_rates = rng.uniform(5, 15, size=n_neurons)\n        \n        # Temporal modulation\n        t = np.arange(n_bins)\n        t0, sigma_t = 10, 3\n        g = np.exp(-(t - t0)**2 / (2 * sigma_t**2))\n        \n        # Condition-specific amplitudes\n        a_A = rng.normal(3, 1, size=n_neurons)\n        a_B = rng.normal(-3, 1, size=n_neurons)\n        \n        all_trials_data = np.zeros((n_trials_total, d))\n        \n        for trial_idx in range(n_trials_total):\n            # The test trial (last one) is chosen to be from Condition A\n            if trial_idx  n_trials_train / 2 or trial_idx == n_trials_train:\n                base_amplitudes = a_A\n            else: # Condition B\n                base_amplitudes = a_B\n            \n            trial_rates = np.zeros((n_neurons, n_bins))\n            for neuron_idx in range(n_neurons):\n                # Add trial-wise amplitude noise\n                amp_noise = rng.normal(0, 0.5)\n                total_amplitude = base_amplitudes[neuron_idx] + amp_noise\n                \n                # Calculate rate using the additive model\n                rate_profile = baseline_rates[neuron_idx] + total_amplitude * g\n                trial_rates[neuron_idx, :] = rate_profile\n            \n            # Clip rates\n            clipped_rates = np.maximum(trial_rates, 0.1)\n            \n            # Generate spike counts from Poisson distribution\n            spike_counts = rng.poisson(clipped_rates)\n            \n            # Flatten and store\n            all_trials_data[trial_idx, :] = spike_counts.flatten()\n            \n        X_train = all_trials_data[:n_trials_train, :]\n        x_test = all_trials_data[n_trials_train, :]\n        \n        # Standardize data\n        mean_train = np.mean(X_train, axis=0)\n        std_train = np.std(X_train, axis=0)\n        # Avoid division by zero\n        std_train[std_train == 0] = 1.0\n        \n        X_train_std = (X_train - mean_train) / std_train\n        x_test_std = (x_test - mean_train) / std_train\n        \n        return X_train_std, x_test_std\n\n    def run_kernel_pca(X_train, x_test, sigma, case_params):\n        \"\"\"\n        Implements the Kernel PCA algorithm and computes results for a test case.\n        \"\"\"\n        n = X_train.shape[0]\n        \n        # Compute Gaussian kernel matrix\n        sq_dists = squareform(pdist(X_train, 'sqeuclidean'))\n        K = np.exp(-sq_dists / (2 * sigma**2))\n        \n        # Center the Gram matrix\n        H = np.eye(n) - np.ones((n, n)) / n\n        K_c = H @ K @ H\n        \n        # Solve the eigenvalue problem for the centered Gram matrix\n        evals, evecs = np.linalg.eigh(K_c)\n        \n        # Sort eigenvalues and eigenvectors in descending order\n        idx = np.argsort(evals)[::-1]\n        evals = evals[idx]\n        evecs = evecs[:, idx]\n        \n        # Clip numerical noise resulting in small negative eigenvalues\n        evals[evals  0] = 0\n        \n        # --- Compute case-specific outputs ---\n        case_id = case_params['id']\n        if case_id == 'A':\n            m = case_params['m']\n            total_variance = np.sum(evals)\n            if total_variance > 0:\n                explained_variances = evals[:m] / total_variance\n            else:\n                explained_variances = np.zeros(m)\n            return explained_variances.tolist()\n            \n        elif case_id == 'B':\n            m = case_params['m']\n            # Compute kernel vector for the test point\n            k_test = np.exp(-np.sum((X_train - x_test)**2, axis=1) / (2 * sigma**2))\n            \n            # Compute the centered kernel vector k_c(x_test)\n            one_n = np.ones(n)\n            k_c_test = (k_test - (K @ one_n) / n - \n                        (np.sum(k_test) / n) * one_n + \n                        (np.sum(K) / n**2) * one_n)\n            \n            # Compute out-of-sample embeddings\n            embeddings = []\n            for k in range(m):\n                mu_k = evals[k]\n                v_k = evecs[:, k]\n                if mu_k > 1e-15: # Use a small threshold for stability\n                    y_k = (v_k.T @ k_c_test) / np.sqrt(mu_k)\n                else:\n                    y_k = 0.0\n                embeddings.append(y_k)\n            return embeddings\n\n        elif case_id == 'C':\n            # Compute Frobenius norm of the centered Gram matrix\n            norm_kc = np.linalg.norm(K_c, 'fro')\n            return norm_kc\n\n    # Generate data once\n    X_train_std, x_test_std = generate_data()\n\n    # Define test cases\n    test_cases = [\n        {'id': 'A', 'sigma': 5.0, 'm': 3},\n        {'id': 'B', 'sigma': 0.1, 'm': 2},\n        {'id': 'C', 'sigma': 1000.0, 'm': None},\n    ]\n    \n    results = []\n    for case_params in test_cases:\n        result = run_kernel_pca(X_train_std, x_test_std, case_params['sigma'], case_params)\n        results.append(result)\n        \n    # Format final output string\n    res_a_str = f\"[{','.join(f'{x:.7f}' for x in results[0])}]\"\n    res_b_str = f\"[{','.join(f'{x:.7f}' for x in results[1])}]\"\n    res_c_str = f\"{results[2]:.7f}\"\n\n    print(f\"[{res_a_str},{res_b_str},{res_c_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a toolbox containing both linear and nonlinear dimensionality reduction methods, a critical question arises: which algorithm is right for my data? The answer depends on the data's intrinsic structure, which is often unknown. This capstone practice  guides you through the process of 'testing the tools' by applying PCA, Isomap, and kPCA to synthetic datasets with known ground-truth geometries. By systematically identifying the failure modes of each method, you will develop the critical judgment needed to select and validate the appropriate DR approach for real-world exploratory data analysis.",
            "id": "4156684",
            "problem": "You are given the task of designing and evaluating dimensionality reduction approaches in the context of neuroscience data analysis under controlled noise and nonlinearity. The overall aim is to identify method-specific failure cases to guide practical choice of dimensionality reduction. The setting is as follows. Let there be a population of neurons with responses recorded across repeated trials, forming a data matrix $X \\in \\mathbb{R}^{T \\times N}$, where $T$ denotes the number of samples and $N$ denotes the number of neurons. The latent structure that drives neural responses is assumed to be low-dimensional, with ground-truth coordinates $Z \\in \\mathbb{R}^{T \\times d}$ known in synthetic tasks.\n\nFundamental base:\n- Distances: For any vectors $a, b \\in \\mathbb{R}^{m}$, the Euclidean distance is $||a - b||_2 = \\sqrt{\\sum_{i=1}^{m} (a_i - b_i)^2}$, and the squared Euclidean distance is $\\sum_{i=1}^{m} (a_i - b_i)^2$.\n- Singular Value Decomposition (SVD): Any centered data matrix $X$ admits $X = U \\Sigma V^\\top$ with orthonormal $U, V$ and nonnegative singular values in $\\Sigma$, which supplies principal directions via Principal Component Analysis (PCA).\n- Kernel methods: Kernel Principal Component Analysis (kPCA) uses a positive semi-definite kernel matrix $K$ to map implicitly into a feature space and finds principal directions by eigen-decomposition of a centered kernel.\n- Graph geodesics and Isometric Mapping (Isomap): A $k$-nearest neighbor graph can be built from $X$ using Euclidean distances; geodesic distances are shortest-path distances in this graph. Classical Multidimensional Scaling (MDS) then finds coordinates matching these distances by double-centering the squared distance matrix.\n- Orthogonal Procrustes alignment: Given embeddings $Y, Z \\in \\mathbb{R}^{T \\times d}$, an optimal rotation and isotropic scaling are found by solving $\\min_{R, s} ||s Y R - Z||_F$ subject to $R^\\top R = I$, which aligns $Y$ to $Z$ up to rigid transform and scale.\n- Stress measure: Given two distance matrices $D_Y, D_Z \\in \\mathbb{R}^{T \\times T}$, define\n$$\\text{stress}(Y,Z) = \\sqrt{\\frac{\\sum_{ij} \\left(D_Y[i,j] - D_Z[i,j]\\right)^2}{\\sum_{ij} D_Z[i,j]^2}}.$$\n\nYour program must:\n1) Generate $X$ for each test case using a specified latent manifold and a specified mapping to high-dimensional neural responses. You must incorporate controlled Gaussian noise with standard deviation $\\sigma$ for each test case. Angles must be treated in radians.\n2) Apply three methods to produce $d = 2$ dimensional embeddings: Principal Component Analysis (PCA), Isometric Mapping (Isomap), and Kernel Principal Component Analysis (kPCA) using a radial basis function (RBF) kernel $K_{ij} = \\exp\\left(-\\gamma ||x_i - x_j||_2^2\\right)$ with a given bandwidth parameter $\\gamma$.\n3) Align each recovered embedding $Y$ to the ground-truth $Z$ using orthogonal Procrustes (rotation and isotropic scaling only). Compute the stress using pairwise Euclidean distances between aligned embeddings and ground-truth coordinates.\n4) For each method in each test case, determine failure as a boolean: failure is $1$ if stress exceeds a specified threshold $\\tau$ or if the method yields a degenerate embedding (e.g., not finite due to a disconnected graph for Isomap); otherwise failure is $0$.\n\nThe test suite consists of the following four parameterized tasks. All numerical values are to be interpreted as real numbers:\n- Test case $1$ (Linear plane, low noise):\n    - Latent manifold: $Z$ sampled uniformly from $[-1,1]^2$.\n    - Mapping: linear mixing into neural responses, $X = Z W^\\top + \\varepsilon$ with $W \\in \\mathbb{R}^{N \\times 2}$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n    - Parameters: $T = 60$, $N = 40$, $\\sigma = 0.05$, $k = 8$ (for Isomap), $\\gamma = 2.0$ (for kPCA), $\\tau = 0.15$.\n\n- Test case $2$ (Swiss roll, moderate noise):\n    - Latent manifold: parameters $t$ sampled uniformly from $[1.5\\pi,4.5\\pi]$, height $h$ from $[0,1]$; ground truth $Z = [t, h]$.\n    - Observed coordinates: $(x, y, z) = (t \\cos t, h, t \\sin t)$ forming a swiss roll.\n    - Mapping: nonlinear basis of observed coordinates into neural responses, including polynomial and trigonometric terms; additive Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n    - Parameters: $T = 80$, $N = 50$, $\\sigma = 0.1$, $k = 10$, $\\gamma = 5.0$, $\\tau = 0.20$.\n\n- Test case $3$ (Ring with high nonlinearity and noise):\n    - Latent manifold: angle $\\theta$ sampled uniformly from $[0,2\\pi]$, in radians. Ground truth $Z = [\\cos \\theta, \\sin \\theta]$.\n    - Distortion: radial factor $r = 1 + 0.3 \\sin(3\\theta)$ produces observed ring coordinates $(u,v) = (r \\cos \\theta, r \\sin \\theta)$.\n    - Mapping: neural responses via nonlinear, orientation-like tuning resembling Von Mises with modulation by $r$; additive Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n    - Parameters: $T = 64$, $N = 60$, $\\sigma = 0.2$, $k = 12$, $\\gamma = 3.0$, $\\tau = 0.22$.\n\n- Test case $4$ (Swiss roll with graph disconnection):\n    - Same latent manifold and mapping design as test case $2$, but choose $k$ small to induce disconnected $k$-nearest neighbor graph.\n    - Parameters: $T = 80$, $N = 50$, $\\sigma = 0.05$, $k = 3$, $\\gamma = 5.0$, $\\tau = 0.20$.\n\nYour program must set a fixed random seed to ensure reproducible results. For each test case, compute the failure booleans for the three methods in the order [PCA, Isomap, kPCA], yielding a list of three integers in $\\{0,1\\}$. Aggregate the results for all four test cases into a single list of lists in the required final output format.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the comma-separated list of the three failure booleans per test case, also enclosed in square brackets. For example, the structure should be of the form $[[a_1,a_2,a_3],[b_1,b_2,b_3],[c_1,c_2,c_3],[d_1,d_2,d_3]]$, where each $a_i, b_i, c_i, d_i \\in \\{0,1\\}$.",
            "solution": "The evaluation pipeline involves three main stages: generating synthetic data with known latent structure, applying each dimensionality reduction algorithm to the data, and quantitatively evaluating the quality of the recovered embedding against the ground truth.\n\n**1. Data Generation Models**\n\nA fixed random seed is used for all random processes to ensure reproducibility. The target embedding dimension is $d=2$ for all cases.\n\n- **Test Case 1: Linear Plane**\n  The latent coordinates $Z \\in \\mathbb{R}^{T \\times d}$ are sampled from a uniform distribution over the square $[-1,1]^2$. The high-dimensional data $X \\in \\mathbb{R}^{T \\times N}$ is a linear projection of $Z$ plus Gaussian noise. The mapping is $X = Z W^\\top + \\varepsilon$, where $W \\in \\mathbb{R}^{N \\times d}$ is a mixing matrix and $\\varepsilon \\in \\mathbb{R}^{T \\times N}$ is a noise matrix. The entries of $W$ are drawn independently from a standard normal distribution $\\mathcal{N}(0,1)$, and the entries of $\\varepsilon$ are from $\\mathcal{N}(0, \\sigma^2)$.\n  The parameters are $T=60$, $N=40$, $\\sigma=0.05$.\n\n- **Test Case 2: Swiss Roll**\n  The latent manifold is parameterized by $t$ and $h$. We sample $t \\in [1.5\\pi, 4.5\\pi]$ and $h \\in [0, 1]$ uniformly. The ground-truth coordinates are $Z = [t, h] \\in \\mathbb{R}^{T \\times 2}$. These are mapped to a 3D Swiss roll structure $(x_i, y_i, z_i) = (t_i \\cos t_i, h_i, t_i \\sin t_i)$. Neural responses are a nonlinear function of these 3D coordinates. We define a basis expansion $\\Phi(x,y,z) \\in \\mathbb{R}^6$ as $\\Phi(x,y,z) = [x, y, z, x^2, z^2, xz]$. The high-dimensional data is then $X = \\Phi(x,y,z) W^\\top + \\varepsilon$, where $W \\in \\mathbb{R}^{N \\times 6}$ is a random mixing matrix with entries from $\\mathcal{N}(0,1)$, and $\\varepsilon$ is Gaussian noise with standard deviation $\\sigma$.\n  The parameters are $T=80$, $N=50$, $\\sigma=0.1$.\n\n- **Test Case 3: Distorted Ring**\n  The latent manifold is a circle, parameterized by an angle $\\theta \\in [0, 2\\pi]$ sampled uniformly. The ground-truth coordinates are $Z=[\\cos \\theta, \\sin \\theta]$. This circle is radially distorted in a 2D observation space by a factor $r = 1 + 0.3 \\sin(3\\theta)$. The neural responses are modeled as Von Mises-like tuning curves. For each neuron $j \\in \\{1, \\dots, N\\}$, we define a preferred angle $\\phi_j$, spaced uniformly in $[0, 2\\pi]$. The response of neuron $j$ for sample $i$ with angle $\\theta_i$ and radial factor $r_i$ is $X_{ij}^{\\text{clean}} = r_i \\exp(\\kappa \\cos(\\theta_i - \\phi_j))$ with concentration $\\kappa=3$. The final data is $X = X^{\\text{clean}} + \\varepsilon$, with noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n  The parameters are $T=64$, $N=60$, $\\sigma=0.2$.\n\n- **Test Case 4: Disconnected Swiss Roll**\n  The data generation follows the exact same procedure as in Test Case 2. The parameters are $T=80$, $N=50$, $\\sigma=0.05$. The key difference for this test case is the Isomap parameter $k=3$, which is chosen to be small enough to likely cause graph disconnection.\n\n**2. Dimensionality Reduction Algorithms**\n\n- **Principal Component Analysis (PCA)**: PCA finds a low-dimensional linear projection of the data that preserves maximal variance.\n  1. Center the data matrix: $X_c = X - \\bar{X}$, where $\\bar{X}$ is the mean of each column (neuron).\n  2. Compute the singular value decomposition (SVD) of the centered data: $X_c = U \\Sigma V^\\top$.\n  3. The principal components (the embedding) are the projections of the data onto the first $d$ principal directions (the first $d$ columns of $V$): $Y_{PCA} = X_c V_{:,:d}$.\n\n- **Isometric Mapping (Isomap)**: Isomap is a manifold learning technique that estimates the intrinsic geometry of the data by approximating geodesic distances.\n  1. For each point, find its $k$ nearest neighbors based on Euclidean distance in the high-dimensional space $\\mathbb{R}^N$.\n  2. Construct a neighborhood graph where nodes are data points and an edge exists between a point and its neighbors. The edge weight is their Euclidean distance.\n  3. Compute the shortest path distances between all pairs of nodes in the graph (e.g., using Dijkstra's or Floyd-Warshall algorithm). This yields the geodesic distance matrix $D_G$. If the graph is disconnected, some distances will be infinite, which constitutes a failure case for the algorithm.\n  4. Apply classical Multidimensional Scaling (MDS) to $D_G$. First, compute the Gram matrix $B = -\\frac{1}{2} J D_G^2 J$, where $D_G^2$ is the element-wise square of $D_G$ and $J=I - \\frac{1}{T}\\mathbf{1}\\mathbf{1}^\\top$ is the centering matrix.\n  5. Compute the top $d$ eigenvalues $\\lambda_1, \\dots, \\lambda_d$ and corresponding eigenvectors $v_1, \\dots, v_d$ of $B$.\n  6. The embedding is $Y_{Isomap} = [ \\sqrt{\\lambda_1}v_1, \\dots, \\sqrt{\\lambda_d}v_d ]$.\n\n- **Kernel Principal Component Analysis (kPCA)**: kPCA generalizes PCA to nonlinear settings by using the \"kernel trick\".\n  1. Compute the kernel matrix $K \\in \\mathbb{R}^{T \\times T}$ using the radial basis function (RBF) kernel: $K_{ij} = \\exp(-\\gamma ||x_i - x_j||_2^2)$, where $x_i, x_j$ are rows of $X$.\n  2. Center the kernel matrix: $K_c = J K J$.\n  3. Compute the top $d$ eigenvalues $\\lambda_1, \\dots, \\lambda_d$ and corresponding eigenvectors $\\alpha_1, \\dots, \\alpha_d$ of $K_c$. The eigenvectors are normalized such that $\\alpha_k^\\top\\alpha_k=1$.\n  4. The embedding is $Y_{kPCA} = [ \\sqrt{\\lambda_1}\\alpha_1, \\dots, \\sqrt{\\lambda_d}\\alpha_d ]$.\n\n**3. Evaluation Procedure**\n\n- **Orthogonal Procrustes Alignment**: To compare the recovered embedding $Y$ with the ground-truth $Z$, we must align them. We solve $\\min_{R, s} ||s Y R - Z||_F$ for a rotation matrix $R$ and a scalar $s$.\n  1. Center both matrices: $Y_c = Y - \\bar{Y}$, $Z_c = Z - \\bar{Z}$.\n  2. Compute the SVD of the cross-covariance matrix: $M = Y_c^\\top Z_c = U \\Sigma V^\\top$.\n  3. The optimal rotation is $R = U V^\\top$.\n  4. The optimal scale is $s = \\text{trace}(\\Sigma) / \\text{trace}(Y_c^\\top Y_c)$.\n  5. The aligned embedding is $Y_{aligned} = s Y_c R + \\bar{Z}$.\n\n- **Stress Calculation**: The quality of the embedding is quantified by the stress metric, which measures the discrepancy between pairwise distances in the aligned embedding $Y_{aligned}$ and the ground-truth $Z$.\n  1. Compute the pairwise Euclidean distance matrices $D_{Y_{aligned}}$ and $D_Z$.\n  2. The stress is calculated as:\n  $$ \\text{stress}(Y_{aligned}, Z) = \\sqrt{\\frac{\\sum_{ij} \\left(D_{Y_{aligned}}[i,j] - D_Z[i,j]\\right)^2}{\\sum_{ij} D_Z[i,j]^2}} $$\n\n- **Failure Determination**: For each method on each test case, a failure is recorded (boolean value $1$) if the stress exceeds a given threshold $\\tau$, or if the algorithm itself fails (e.g., Isomap on a disconnected graph). Otherwise, it is a success (boolean value $0$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd, eigh\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.csgraph import shortest_path\n\ndef generate_data(case_params):\n    \"\"\"Generates synthetic data for a given test case.\"\"\"\n    case_num = case_params['case']\n    T = case_params['T']\n    N = case_params['N']\n    sigma = case_params['sigma']\n    np.random.seed(42) # Ensure reproducibility within data generation\n    \n    if case_num == 1:\n        # Linear plane\n        d = 2\n        Z = np.random.uniform(-1, 1, size=(T, d))\n        W = np.random.randn(N, d)\n        noise = np.random.randn(T, N) * sigma\n        X = Z @ W.T + noise\n        return X, Z\n\n    elif case_num == 2 or case_num == 4:\n        # Swiss roll\n        t = np.random.uniform(1.5 * np.pi, 4.5 * np.pi, size=T)\n        h = np.random.uniform(0, 1, size=T)\n        Z = np.vstack((t, h)).T\n\n        obs_x = t * np.cos(t)\n        obs_y = h\n        obs_z = t * np.sin(t)\n        \n        # Nonlinear basis mapping\n        phi_dim = 6\n        phi_data = np.vstack((obs_x, obs_y, obs_z, obs_x**2, obs_z**2, obs_x*obs_z)).T\n        W = np.random.randn(N, phi_dim)\n        noise = np.random.randn(T, N) * sigma\n        X = phi_data @ W.T + noise\n        return X, Z\n\n    elif case_num == 3:\n        # Distorted ring\n        theta = np.random.uniform(0, 2 * np.pi, size=T)\n        Z = np.vstack((np.cos(theta), np.sin(theta))).T\n        \n        r = 1 + 0.3 * np.sin(3 * theta)\n        \n        # Von Mises-like tuning curves\n        kappa = 3.0\n        pref_angles = np.linspace(0, 2 * np.pi, N, endpoint=False)\n        \n        X_clean = np.zeros((T, N))\n        for i in range(T):\n            for j in range(N):\n                X_clean[i, j] = r[i] * np.exp(kappa * np.cos(theta[i] - pref_angles[j]))\n        \n        noise = np.random.randn(T, N) * sigma\n        X = X_clean + noise\n        return X, Z\n    \n    return None, None\n\ndef do_pca(X, d=2):\n    \"\"\"Performs Principal Component Analysis.\"\"\"\n    X_c = X - X.mean(axis=0)\n    _, _, Vt = svd(X_c, full_matrices=False)\n    V = Vt.T\n    Y = X_c @ V[:, :d]\n    return Y, False\n\ndef do_isomap(X, d=2, k=8):\n    \"\"\"Performs Isometric Mapping.\"\"\"\n    T = X.shape[0]\n    dist_mat = squareform(pdist(X, 'euclidean'))\n    \n    adj = np.zeros((T, T))\n    for i in range(T):\n        neighbors = np.argsort(dist_mat[i, :])[1:k+1]\n        adj[i, neighbors] = dist_mat[i, neighbors]\n        adj[neighbors, i] = dist_mat[neighbors, i]\n\n    graph = csr_matrix(adj)\n    D_geo = shortest_path(csgraph=graph, directed=False, method='auto', unweighted=False)\n\n    if np.any(np.isinf(D_geo)):\n        return None, True # Failure due to disconnected graph\n\n    D2 = D_geo**2\n    J = np.eye(T) - np.ones((T, T)) / T\n    B = -0.5 * J @ D2 @ J\n\n    try:\n        eigvals, eigvecs = eigh(B, subset_by_index=[T - d, T - 1])\n    except np.linalg.LinAlgError:\n        return None, True # Eigendecomposition failed\n\n    eigvals, eigvecs = np.flip(eigvals), np.flip(eigvecs, axis=1)\n    \n    # Handle potential negative eigenvalues from numerical instability\n    valid_eigvals = np.maximum(eigvals[:d], 0)\n    \n    Y = eigvecs[:, :d] @ np.diag(np.sqrt(valid_eigvals))\n    return Y, False\n\ndef do_kpca(X, d=2, gamma=1.0):\n    \"\"\"Performs Kernel Principal Component Analysis.\"\"\"\n    T = X.shape[0]\n    D_euc_sq = squareform(pdist(X, 'sqeuclidean'))\n    K = np.exp(-gamma * D_euc_sq)\n    \n    J = np.eye(T) - np.ones((T, T)) / T\n    K_c = J @ K @ J\n    \n    try:\n        eigvals, eigvecs = eigh(K_c, subset_by_index=[T - d, T - 1])\n    except np.linalg.LinAlgError:\n        return None, True # Eigendecomposition failed\n\n    eigvals, eigvecs = np.flip(eigvals), np.flip(eigvecs, axis=1)\n    \n    valid_eigvals = np.maximum(eigvals[:d], 0)\n    Y = eigvecs[:, :d] * np.sqrt(valid_eigvals)\n    return Y, False\n\ndef procrustes_align(Y, Z):\n    \"\"\"Aligns embedding Y to ground-truth Z using rotation and isotropic scaling.\"\"\"\n    Y_mean = Y.mean(axis=0)\n    Z_mean = Z.mean(axis=0)\n    Yc = Y - Y_mean\n    Zc = Z - Z_mean\n\n    M = Yc.T @ Zc\n    U, s, Vt = svd(M)\n    R = U @ Vt\n    \n    # Avoid division by zero for degenerate embeddings\n    Yc_ss = np.sum(Yc**2)\n    if Yc_ss  1e-12:\n      return Y # Cannot align a zero-variance embedding\n\n    scale = np.sum(s) / Yc_ss\n    Y_aligned = scale * Yc @ R + Z_mean\n    return Y_aligned\n\ndef calculate_stress(Y_aligned, Z):\n    \"\"\"Calculates the stress between aligned embedding and ground-truth.\"\"\"\n    Dy = pdist(Y_aligned)\n    Dz = pdist(Z)\n    \n    # Avoid division by zero if Z has no variance\n    Dz_ss = np.sum(Dz**2)\n    if Dz_ss  1e-12:\n        return 0.0 if np.sum((Dy - Dz)**2)  1e-12 else np.inf\n        \n    stress = np.sqrt(np.sum((Dy - Dz)**2) / Dz_ss)\n    return stress\n\ndef solve():\n    \"\"\"Main function to run the test suite.\"\"\"\n    np.random.seed(42) # Set global seed for reproducibility\n    \n    test_cases = [\n        {'case': 1, 'T': 60, 'N': 40, 'sigma': 0.05, 'k': 8, 'gamma': 2.0, 'tau': 0.15},\n        {'case': 2, 'T': 80, 'N': 50, 'sigma': 0.1, 'k': 10, 'gamma': 5.0, 'tau': 0.20},\n        {'case': 3, 'T': 64, 'N': 60, 'sigma': 0.2, 'k': 12, 'gamma': 3.0, 'tau': 0.22},\n        {'case': 4, 'T': 80, 'N': 50, 'sigma': 0.05, 'k': 3, 'gamma': 5.0, 'tau': 0.20},\n    ]\n\n    all_results = []\n    \n    methods = [\n        ('pca', do_pca),\n        ('isomap', do_isomap),\n        ('kpca', do_kpca)\n    ]\n\n    for params in test_cases:\n        X, Z = generate_data(params)\n        tau = params['tau']\n        case_failures = []\n        \n        for name, func in methods:\n            failure = 0\n            \n            # Select correct parameters for the method\n            method_params = {'d': 2}\n            if name == 'isomap':\n                method_params['k'] = params['k']\n            elif name == 'kpca':\n                method_params['gamma'] = params['gamma']\n\n            Y, algo_failed = func(X, **method_params)\n            \n            if algo_failed or Y is None:\n                failure = 1\n            else:\n                Y_aligned = procrustes_align(Y, Z)\n                stress = calculate_stress(Y_aligned, Z)\n                if stress > tau:\n                    failure = 1\n            \n            case_failures.append(failure)\n        all_results.append(case_failures)\n        \n    # Format the final output\n    output_str = '[' + ','.join(['[' + ','.join(map(str, res)) + ']' for res in all_results]) + ']'\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}