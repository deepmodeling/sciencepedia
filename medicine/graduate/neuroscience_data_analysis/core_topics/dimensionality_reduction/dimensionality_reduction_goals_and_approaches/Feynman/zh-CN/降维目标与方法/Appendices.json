{
    "hands_on_practices": [
        {
            "introduction": "在应用主成分分析（PCA）后，一个至关重要的步骤是解释所产生的主成分的生物学意义。本练习将指导您建立一种量化方法，用以确定每个神经元对 PCA 所识别出的主要活动模式的贡献程度。通过计算神经元特异性重要性，我们可以将抽象的数学成分转化为具体的、可解释的神经科学见解。",
            "id": "4156634",
            "problem": "一个神经科学数据矩阵可以表示为 $X \\in \\mathbb{R}^{n \\times t}$，其中 $n$ 是神经元的数量， $t$ 是样本的数量（例如时间点或试验）。$X$ 的每一行对应单个神经元在所有样本中的活动。作为一个基本的预处理步骤，每个神经元的活动都必须通过减去其样本均值来进行中心化，以便分析能集中在零点周围的方差结构上。降维旨在找到一个能够捕获 $X$ 的主要方差结构的低维表示。一种有原则且被广泛使用的方法是基于正交分解，其属性在科学实践中已经过充分检验。\n\n基本基础：\n- 中心化数据的奇异值分解 (SVD)：给定一个中心化矩阵 $X$，存在一个分解 $X = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times r}$ 和 $V \\in \\mathbb{R}^{t \\times r}$ 具有正交列，$\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是一个对角矩阵，其对角线上的元素为非负的奇异值，并按非递增顺序排列，且 $r = \\mathrm{rank}(X)$。神经元空间中的样本协方差为 $X X^\\top$，一个经过充分检验的恒等式是 $X X^\\top = U \\Sigma^2 U^\\top$，其中 $\\Sigma^2$ 表示由奇异值平方构成的对角矩阵。\n- 主成分分析 (PCA) 是一种在神经元空间中选择正交轴以最大化投影方差的方法。对于中心化矩阵 $X$，PCA 轴是 $U$ 的列，第 $j$ 个轴捕获的方差等于相应奇异值的平方。\n\n任务：\n利用这些基本事实，通过载荷贡献将低维分量映射回神经元，从而定义一个有原则的神经元特异性重要性概念。目标是量化每个神经元对低维方差的贡献程度。构建以下量：\n1. 神经元 $i$ 对低维分量 $j$ 的贡献应定义为一个非负量，该量对所有神经元求和后等于分量 $j$ 捕获的方差，并且其值仅通过分量的正交基及其捕获的方差来依赖于 $X$ 的结构。\n2. 神经元特异性重要性应通过聚合神经元 $i$ 对前 $k$ 个分量的贡献，并用这 $k$ 个分量捕获的总方差进行归一化来定义，从而为每个神经元得到一个在 $[0,1]$ 区间内的小数。如果前 $k$ 个分量捕获的总方差为零，则返回长度为 $n$ 的零向量。\n\n你的程序必须：\n- 通过减去每行的均值来对 $X$ 进行中心化。\n- 使用 SVD 恒等式 $X = U \\Sigma V^\\top$ 计算前 $k$ 个低维分量的截断正交基及相关的捕获方差。\n- 通过构建满足上述属性的载荷贡献，将低维分量映射回神经元。\n- 聚合贡献，生成一个长度为 $n$ 的神经元重要性向量，其条目以小数表示。\n\n测试套件：\n实现你的程序来处理以下参数集。对于每种情况，结果必须是神经元重要性向量，形式为浮点数列表。\n\n- 情况 A（理想路径，相关神经元和正交模式）：\n  $$\n  X = \\begin{bmatrix}\n  0  1  2  3  4 \\\\\n  0  2  4  6  8 \\\\\n  1  -1  1  -1  1\n  \\end{bmatrix},\\quad k = 2.\n  $$\n- 情况 B（边界情况，含零方差神经元）：\n  $$\n  X = \\begin{bmatrix}\n  1  1  1  1 \\\\\n  0  1  0  1 \\\\\n  2  3  4  5\n  \\end{bmatrix},\\quad k = 2.\n  $$\n- 情况 C（重复的神经元）：\n  $$\n  X = \\begin{bmatrix}\n  1  2  3  4  5 \\\\\n  1  2  3  4  5 \\\\\n  5  4  3  2  1\n  \\end{bmatrix},\\quad k = 1.\n  $$\n- 情况 D（由单个神经元主导的各向异性方差）：\n  $$\n  X = \\begin{bmatrix}\n  10  0  -10  0  10 \\\\\n  1  1  1  1  1 \\\\\n  0  1  0  1  0\n  \\end{bmatrix},\\quad k = 1.\n  $$\n\n输出规格：\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。此顶层列表的每个元素本身是一个小数列表，对应于一个测试用例的神经元重要性值，其顺序与 $X$ 中的行顺序相同。例如：“[[a,b,c],[d,e,f],...]”。\n- 不需要物理单位或角度单位。\n- 百分比必须以小数表示，不得使用百分号。\n\n覆盖预期：\n- 情况 A 测试具有相关神经元和正交模式的一般场景。\n- 情况 B 测试处理中心化后方差为零的神经元。\n- 情况 C 测试在单个主导分量下，重复的神经元如何共享重要性。\n- 情况 D 测试由单个高变异性神经元主导的情况。\n\n你的解决方案必须能用任何现代编程语言实现。最终答案必须是一个单一、完整、独立的 Python 程序，该程序满足后续列出的运行时环境约束，并能准确生成指定的输出格式。",
            "solution": "已根据指定的验证协议对用户提供的问题进行了分析。\n\n### 第 1 步：提取给定信息\n- **数据矩阵**：$X \\in \\mathbb{R}^{n \\times t}$，其中 $n$ 是神经元数量， $t$ 是样本数量。\n- **预处理**：$X$ 的每一行都必须通过减去其样本均值来进行中心化。\n- **降维目标**：找到一个能够捕获主要方差的低维表示。\n- **基本基础 (SVD)**：对于一个中心化矩阵 $X$，其奇异值分解为 $X = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times r}$ 和 $V \\in \\mathbb{R}^{t \\times r}$ 具有正交列，$\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是一个对角矩阵，其对角线元素为非递增的奇异值，且 $r = \\mathrm{rank}(X)$。\n- **基本基础（协方差恒等式）**：神经元空间中的样本协方差为 $X X^\\top = U \\Sigma^2 U^\\top$，其中 $\\Sigma^2$ 包含奇异值的平方。\n- **基本基础 (PCA)**：对于中心化的 $X$，主成分分析 (PCA) 轴是 $U$ 的列。第 $j$ 个轴捕获的方差是相应奇异值 $\\sigma_j^2$ 的平方。\n- **任务 1（贡献定义）**：定义神经元 $i$ 对分量 $j$ 的贡献。这个量必须是：\n    1. 非负。\n    2. 对所有神经元 $i$ 求和后等于分量 $j$ 的方差。\n    3. 仅依赖于分量的正交基 ($U$) 及其捕获的方差 ($\\sigma_j^2$)。\n- **任务 2（神经元重要性定义）**：通过聚合神经元 $i$ 对前 $k$ 个分量的贡献，并用这 $k$ 个分量捕获的总方差进行归一化，来定义神经元特异性重要性。每个神经元的结果是一个在 $[0,1]$ 区间内的小数。如果捕获的总方差为零，则重要性向量是一个长度为 $n$ 的零向量。\n- **测试用例**：\n    - 情况 A: $X = \\begin{bmatrix} 0  1  2  3  4 \\\\ 0  2  4  6  8 \\\\ 1  -1  1  -1  1 \\end{bmatrix}, k = 2$。\n    - 情况 B: $X = \\begin{bmatrix} 1  1  1  1 \\\\ 0  1  0  1 \\\\ 2  3  4  5 \\end{bmatrix}, k = 2$。\n    - 情况 C: $X = \\begin{bmatrix} 1  2  3  4  5 \\\\ 1  2  3  4  5 \\\\ 5  4  3  2  1 \\end{bmatrix}, k = 1$。\n    - 情况 D: $X = \\begin{bmatrix} 10  0  -10  0  10 \\\\ 1  1  1  1  1 \\\\ 0  1  0  1  0 \\end{bmatrix}, k = 1$。\n\n### 第 2 步：使用提取的给定信息进行验证\n根据验证标准对问题进行评估。\n\n- **科学性**：该问题基于主成分分析 (PCA) 和奇异值分解 (SVD)，它们是线性代数和数据分析中的基本且被普遍接受的方法。SVD 与协方差矩阵特征分解 ($X X^\\top = U \\Sigma^2 U^\\top$) 之间的关系是一个标准的数学恒等式。量化变量对主成分的贡献是解释 PCA 结果的一个标准且有意义的部分。\n- **良构性**：该问题是良构的。它要求基于一组清晰的、构造性的约束来构建一个量（“神经元特异性重要性”）。这些约束导出了一个唯一且稳定的数学公式，正如将在解决方案中展示的那样。输入 ($X$, $k$) 和期望的输出格式都已指定，并且明确处理了一个边界情况（零方差）。\n- **客观性**：问题陈述是用精确、客观和数学的语言编写的。它没有歧义、主观性或基于观点的断言。\n- **未检测到其他缺陷**：该问题不是不健全、不完整、不切实际或琐碎的。它代表了科学数据分析中一个标准的、可形式化的任务。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。现在将开发一个有原则的解决方案。\n\n### 基于原则的设计\n解决方案是根据问题陈述中提供的原则和定义构建的。\n\n**1. 数据中心化**\n第一步是中心化数据矩阵 $X$。对于每个神经元 $i$（第 $i$ 行），我们计算它在所有 $t$ 个样本中的平均活动 $\\mu_i$。\n$$\n\\mu_i = \\frac{1}{t} \\sum_{j=1}^{t} X_{ij}\n$$\n然后，我们通过从每个元素中减去相应的均值来形成中心化数据矩阵，我们将其表示为 $X_c$。\n$$\n(X_c)_{ij} = X_{ij} - \\mu_i\n$$\n此过程确保后续分析侧重于神经活动的方差结构，而不是受放电率基线偏移的影响。\n\n**2. 通过 SVD 进行正交分解**\n分析的核心是中心化矩阵 $X_c$ 的奇异值分解 (SVD)。如问题所述，此分解由下式给出：\n$$\nX_c = U \\Sigma V^\\top\n$$\n在这里，$U \\in \\mathbb{R}^{n \\times r}$ 的列是 $n$ 维神经元空间中的主成分（或主轴）。这些是正交向量，构成一个基。对角矩阵 $\\Sigma \\in \\mathbb{R}^{r \\times r}$ 包含按非递增顺序排列的奇异值 $\\sigma_j$。第 $j$ 个主成分捕获的方差为 $\\lambda_j = \\sigma_j^2$。\n\n**3. 神经元对分量的贡献**\n问题要求构建一个量，我们称之为 $C_{ij}$，表示神经元 $i$ 对分量 $j$ 方差的贡献。这个量必须满足三个特定属性。我们提出了一个定义，并根据这些属性对其进行验证。\n\n设 $u_j$ 是 $U$ 的第 $j$ 列，其元素为 $u_{ij}$。向量 $u_j$ 代表第 $j$ 个主成分轴。值 $u_{ij}$ 是神经元 $i$ 在此分量上的载荷。我们将贡献定义为：\n$$\nC_{ij} = u_{ij}^2 \\sigma_j^2\n$$\n让我们验证这个定义：\n1.  **非负性**：由于 $u_{ij}^2 \\ge 0$ 且 $\\sigma_j^2 \\ge 0$，它们的乘积 $C_{ij}$ 总是非负的。此属性得到满足。\n2.  **对分量方差求和**：我们必须检查是否 $\\sum_{i=1}^{n} C_{ij} = \\sigma_j^2$。\n    $$\n    \\sum_{i=1}^{n} C_{ij} = \\sum_{i=1}^{n} u_{ij}^2 \\sigma_j^2 = \\sigma_j^2 \\left( \\sum_{i=1}^{n} u_{ij}^2 \\right)\n    $$\n    由于 $U$ 的列是正交向量，任何列中元素的平方和为 1，即 $\\|u_j\\|^2 = \\sum_{i=1}^{n} u_{ij}^2 = 1$。因此：\n    $$\n    \\sum_{i=1}^{n} C_{ij} = \\sigma_j^2 \\cdot 1 = \\sigma_j^2\n    $$\n    此属性得到满足。上述定义正确地将一个分量的方差分配给所有神经元。\n3.  **依赖性**：量 $C_{ij}$ 仅依赖于 $u_{ij}$（来自正交基 $U$）和 $\\sigma_j^2$（捕获的方差），符合要求。\n\n因此，这个贡献的定义是有原则的，并且与所有要求一致。\n\n**4. 神经元特异性重要性**\n最后一步是计算每个神经元 $i$ 的神经元特异性重要性 $I_i$。问题将此定义为对前 $k$ 个分量的贡献进行聚合，并由这些分量捕获的总方差进行归一化。\n\n神经元 $i$ 对前 $k$ 个分量的总贡献是其各个贡献的总和：\n$$\n\\text{Total Contribution}_i = \\sum_{j=1}^{k} C_{ij} = \\sum_{j=1}^{k} u_{ij}^2 \\sigma_j^2\n$$\n前 $k$ 个分量捕获的总方差是其各自方差的总和：\n$$\n\\text{Total Variance}_k = \\sum_{j=1}^{k} \\sigma_j^2\n$$\n神经元特异性重要性 $I_i$ 是这两个量的比率：\n$$\nI_i = \\frac{\\sum_{j=1}^{k} u_{ij}^2 \\sigma_j^2}{\\sum_{j=1}^{k} \\sigma_j^2}\n$$\n这个值表示在前 $k$ 维子空间中，可归因于神经元 $i$ 的总方差的分数。如果分母为零（即中心化矩阵是零矩阵），问题指定返回一个零向量，该公式通过提供的明确条件正确处理了这种情况。将所有神经元的 $I_i$ 相加得到 $\\sum_{i=1}^n I_i = 1$，这证实了它是一个有效的分数分布。\n\n**算法实现**\n步骤如下：\n1.  给定 $X$ 和 $k$，计算 $X$ 的行均值并减去它以得到 $X_c$。\n2.  计算 $X_c$ 的 SVD 以获得 $U$、奇异值 $S$ 和 $V^\\top$。\n3.  选择 $U$ 的前 $k$ 列（我们将此子矩阵称为 $U_k$）和前 $k$ 个奇异值（向量 $S_k$）。\n4.  计算前 $k$ 个分量的方差：`variances_k = S_k**2`。\n5.  计算总方差：`total_variance_k = sum(variances_k)`。\n6.  如果 `total_variance_k` 为 $0$，则返回一个长度为 $n$ 的零向量。\n7.  计算每个神经元的分子：`neuron_total_contributions`，一个 $n$ 元素向量，其中第 $i$ 个元素是 $\\sum_{j=1}^k (U_k)_{ij}^2 \\cdot (\\text{variances\\_k})_j$。\n8.  将 `neuron_total_contributions` 除以 `total_variance_k` 以获得最终的重要性向量。\n\n这个操作序列直接实现了推导出的公式，并满足所有问题约束。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_importance(X: np.ndarray, k: int) -> np.ndarray:\n    \"\"\"\n    Calculates neuron-specific importance based on PCA/SVD.\n\n    Args:\n        X (np.ndarray): The data matrix of shape (n, t), where n is the number\n                        of neurons and t is the number of samples.\n        k (int): The number of top principal components to consider.\n\n    Returns:\n        np.ndarray: A 1D array of length n containing the importance score\n                    for each neuron, expressed as a decimal fraction.\n    \"\"\"\n    n, t = X.shape\n\n    # Step 1: Center the data by subtracting the mean of each row.\n    # Using keepdims=True ensures that the mean array has shape (n, 1)\n    # for correct broadcasting during subtraction.\n    row_means = X.mean(axis=1, keepdims=True)\n    X_centered = X - row_means\n\n    # Step 2: Compute the Singular Value Decomposition (SVD).\n    # full_matrices=False is more efficient as we don't need the full U or V.\n    # U will have shape (n, min(n,t)), S will be a 1D array of singular values.\n    try:\n        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n    except np.linalg.LinAlgError:\n        # This can happen for ill-conditioned matrices, though unlikely with test data.\n        # A matrix of all zeros would result in zero singular values.\n        return np.zeros(n)\n\n    # Effective number of components cannot exceed the rank of the matrix.\n    num_components = len(S)\n    k_eff = min(k, num_components)\n\n    if k_eff == 0:\n        return np.zeros(n)\n        \n    # Step 3: Select top k components and their variances.\n    U_k = U[:, :k_eff]\n    S_k = S[:k_eff]\n    variances_k = S_k**2\n\n    # Step 4: Calculate total variance captured by the top k components.\n    total_variance_k = np.sum(variances_k)\n\n    # Handle the edge case where total variance is zero.\n    if total_variance_k == 0:\n        return np.zeros(n)\n\n    # Step 5: Calculate neuron contributions and aggregate them.\n    # The term u_ij^2 * sigma_j^2 is the contribution of neuron i to component j.\n    # We sum these contributions over the top k components for each neuron.\n    # In NumPy, (U_k**2) has shape (n, k_eff). `variances_k` is broadcasted\n    # for element-wise multiplication across columns.\n    neuron_contributions = (U_k**2) * variances_k\n    neuron_total_contributions = np.sum(neuron_contributions, axis=1)\n\n    # Step 6: Normalize to get the final importance scores.\n    neuron_importance = neuron_total_contributions / total_variance_k\n\n    return neuron_importance\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [0, 1, 2, 3, 4],\n            [0, 2, 4, 6, 8],\n            [1, -1, 1, -1, 1]\n        ], dtype=float), 2),\n        (np.array([\n            [1, 1, 1, 1],\n            [0, 1, 0, 1],\n            [2, 3, 4, 5]\n        ], dtype=float), 2),\n        (np.array([\n            [1, 2, 3, 4, 5],\n            [1, 2, 3, 4, 5],\n            [5, 4, 3, 2, 1]\n        ], dtype=float), 1),\n        (np.array([\n            [10, 0, -10, 0, 10],\n            [1, 1, 1, 1, 1],\n            [0, 1, 0, 1, 0]\n        ], dtype=float), 1)\n    ]\n\n    results = []\n    for X, k in test_cases:\n        importance_vector = calculate_importance(X, k)\n        results.append(importance_vector.tolist())\n\n    # Format the output string to exactly match the specification:\n    # \"[[a,b,c],[d,e,f],...]\" without spaces after commas inside inner lists.\n    list_of_strings = []\n    for res_list in results:\n        inner_str = '[' + ','.join(map(str, res_list)) + ']'\n        list_of_strings.append(inner_str)\n    \n    final_output_str = '[' + ','.join(list_of_strings) + ']'\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在主成分分析的应用中，一个核心的实际问题是如何确定应保留的主成分数量 $k$。本练习将引导您超越传统的碎石图“肘部法则”等主观标准，采用一种源于随机矩阵理论（RMT）的强大技术来客观地厘定数据的内在维度。通过利用 Marchenko-Pastur 定律区分真实的神经信号与随机噪声，您将掌握一种有原则的、数据驱动的模型选择方法。",
            "id": "4156671",
            "problem": "给定一个针对包含 $p$ 个神经元的神经元群体的样本协方差矩阵 $\\hat{\\Sigma} \\in \\mathbb{R}^{p \\times p}$，该矩阵由 $n$ 个零均值活动的样本计算得出。目标是使用主成分分析 (PCA) 通过计算作为 $k$ 的函数的方差解释比例来量化降维效果，并基于随机矩阵理论 (RMT) 设计一个客观的碎石图准则。从以下基础定义开始：样本协方差为 $\\hat{\\Sigma} = \\frac{1}{n} X^\\top X$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 是零均值数据矩阵；$\\hat{\\Sigma}$ 的特征分解为 $\\hat{\\Sigma} = V \\Lambda V^\\top$，其特征值为 $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0$。总方差为 $\\mathrm{tr}(\\hat{\\Sigma}) = \\sum_{i=1}^{p} \\lambda_i$，前 $k$ 个主成分解释的方差比例为 $\\mathrm{FVE}(k) = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{p} \\lambda_i}$。在方差为 $\\sigma^2$、比率为 $q = \\frac{p}{n}$ 的各向同性噪声模型下，随机矩阵理论 (RMT) 中的 Marchenko–Pastur (MP) 定律指出，纯噪声样本协方差的特征值集中在区间 $[\\lambda_{-}, \\lambda_{+}]$ 内，其中 $\\lambda_{\\pm} = \\sigma^2 (1 \\pm \\sqrt{q})^2$。使用此定律来定义一个碎石图选择准则：选择 $k$ 作为严格大于 $\\lambda_{+}$ 的特征值的数量。\n\n通过指定 $p$、$n$、$\\sigma^2$ 以及一个满足 $r \\leq p$ 的非负尖峰强度列表 $(s_1, \\dots, s_r)$，在每个测试中确定性地构造 $\\hat{\\Sigma}$。假设一个与坐标轴对齐的尖峰协方差模型：$\\hat{\\Sigma} = \\sigma^2 I_p + \\sum_{j=1}^{r} s_j e_j e_j^\\top$，其中 $I_p$ 是 $p \\times p$ 单位矩阵，$e_j$ 是第 $j$ 个标准基向量，因此特征值为 $(\\sigma^2 + s_1, \\dots, \\sigma^2 + s_r, \\sigma^2, \\dots, \\sigma^2)$。\n\n从第一性原理出发，实现以下任务：\n- 按降序计算 $\\hat{\\Sigma}$ 的排序后特征值 $(\\lambda_1, \\dots, \\lambda_p)$。\n- 对所有从 $1$ 到 $p$ 的整数 $k$，计算 $\\mathrm{FVE}(k)$。\n- 计算 MP 上界 $\\lambda_{+} = \\sigma^2 (1 + \\sqrt{p/n})^2$，并选择 $k$ 作为大于 $\\lambda_{+}$ 的特征值数量。\n\n不涉及物理单位。所有报告的分数必须是小数。将每个 $\\mathrm{FVE}(k)$ 四舍五入到 $6$ 位小数。\n\n测试套件：\n- 测试 $1$：$p = 10$，$n = 400$，$\\sigma^2 = 0.5$，尖峰 $(3.0, 2.0, 1.0)$。\n- 测试 $2$：$p = 10$，$n = 200$，$\\sigma^2 = 1.0$，尖峰 $()$ (无尖峰)。\n- 测试 $3$：$p = 10$，$n = 50$，$\\sigma^2 = 1.0$，尖峰 $(0.7)$。\n- 测试 $4$：$p = 10$，$n = 100$，$\\sigma^2 = 0.8$，尖峰 $(1.5, 0.9)$。\n\n你的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。对于每个测试，输出一个内部列表，该列表包含选定的整数 $k$，后跟完整的序列 $[\\mathrm{FVE}(1), \\mathrm{FVE}(2), \\dots, \\mathrm{FVE}(p)]$，其中序列值为四舍五入到 $6$ 位小数的浮点数。例如，总体格式为 $[[k_1, f_{1,1}, \\dots, f_{1,p}], [k_2, f_{2,1}, \\dots, f_{2,p}], [k_3, \\dots], [k_4, \\dots]]$。",
            "solution": "问题要求使用主成分分析 (PCA) 对指定的协方差矩阵进行分析，以量化维度并利用随机矩阵理论 (RMT) 的准则确定有效维度。该过程涉及计算由主成分解释的方差比例 (FVE) 并应用 Marchenko-Pastur (MP) 定律。\n\n### 基于原理的设计\n\n该解决方案基于线性代数、统计学和 RMT 的几个关键原理。\n\n1.  **主成分分析 (PCA)**：PCA 是一种降维技术，它将一组可能相关的变量的观测值转换为一组称为主成分的线性不相关变量。这些成分经过排序，使得前几个成分保留了原始数据中存在的大部分方差。在数学上，这是通过对样本协方差矩阵 $\\hat{\\Sigma}$ 进行特征分解来实现的。$\\hat{\\Sigma}$ 的特征向量是主成分（最大方差方向），相应的特征值 $(\\lambda_i)$ 则量化了沿这些方向的方差。\n\n2.  **方差解释比例 (FVE)**：评估降维效果的一个主要指标是 FVE。数据中的总方差由协方差矩阵的迹 $\\mathrm{tr}(\\hat{\\Sigma})$ 捕获，它等于其特征值之和 $\\sum_{i=1}^{p} \\lambda_i$。前 $k$ 个主成分捕获的方差是前 $k$ 个特征值之和 $\\sum_{i=1}^{k} \\lambda_i$。因此，$k$ 个成分的 FVE 定义为以下比率：\n    $$\n    \\mathrm{FVE}(k) = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{p} \\lambda_i}\n    $$\n    这提供了一个累积度量，衡量当数据投影到 $k$ 维子空间时保留了多少信息（方差）。\n\n3.  **尖峰协方差模型**：问题指定了使用“尖峰”模型来确定性地构造协方差矩阵。该模型假设真实的协方差结构由一个简单的噪声分量（各向同性噪声）加上几个代表强低维信号的“尖峰”组成。指定的矩阵是 $\\hat{\\Sigma} = \\sigma^2 I_p + \\sum_{j=1}^{r} s_j e_j e_j^\\top$。在这里，$\\sigma^2 I_p$ 代表各向同性噪声协方差，其中 $\\sigma^2$ 是噪声方差，$I_p$ 是 $p \\times p$ 单位矩阵。项 $\\sum_{j=1}^{r} s_j e_j e_j^\\top$ 代表信号分量，其中 $s_j > 0$ 是“尖峰”强度，$e_j$ 是标准基向量。由于这个和的对角结构，$\\hat{\\Sigma}$ 的特征值可以直接给出。有 $r$ 个形式为 $\\sigma^2 + s_j$ 的“尖峰”特征值和 $p-r$ 个等于 $\\sigma^2$ 的“块”特征值。\n\n4.  **随机矩阵理论 (RMT) 与 Marchenko-Pastur (MP) 定律**：RMT 提供了关于大型随机矩阵特征值分布的强大结果。对于一个从只包含各向同性噪声（即无信号）的 $p$ 维数据的 $n$ 个样本计算出的样本协方差矩阵，当 $n, p \\to \\infty$ 且其比率 $q = p/n$ 保持不变时，MP 定律描述了其特征值的极限分布。关键是，预测特征值会位于一个由 $\\lambda_{\\pm} = \\sigma^2 (1 \\pm \\sqrt{q})^2$ 界定的连续块分布内。在尖峰模型的背景下，“弹出”并严格位于理论上界 $\\lambda_{+}$ 之上的特征值被认为对应于真实信号，而那些在块分布 $[0, \\lambda_{+}]$ 内的特征值则归因于噪声。这为选择显著成分的数量 $k$ 提供了一个客观的、数据驱动的准则，从而避免了传统碎石图检查的主观性。所选维度 $k$ 是特征值 $\\lambda_i > \\lambda_{+}$ 的数量。\n\n### 算法步骤\n\n基于这些原理，每个测试用例的求解过程如下：\n\n1.  **确定特征值**：给定参数 $p$、$\\sigma^2$ 以及 $r$ 个尖峰强度的列表 $(s_1, \\dots, s_r)$，构造 $p$ 个特征值的集合。这个集合是 $\\{\\sigma^2 + s_1, \\dots, \\sigma^2 + s_r\\} \\cup \\{\\sigma^2, \\dots, \\sigma^2\\}$，其中噪声特征值 $\\sigma^2$ 重复了 $p-r$ 次。\n\n2.  **排序特征值**：按降序对特征值进行排序，以获得有序谱 $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p$。\n\n3.  **计算总方差**：通过对所有特征值求和来计算总方差：$V_{\\text{total}} = \\sum_{i=1}^{p} \\lambda_i$。\n\n4.  **计算 FVE 数组**：计算排序后特征值的累积和，$C_k = \\sum_{i=1}^{k} \\lambda_i$，其中 $k=1, \\dots, p$。然后计算 FVE 数组为 $\\mathrm{FVE}(k) = C_k / V_{\\text{total}}$。每个元素四舍五入到 6 位小数。\n\n5.  **计算 MP 阈值**：计算纵横比 $q = p/n$。然后计算 Marchenko-Pastur 上界阈值 $\\lambda_{+} = \\sigma^2 (1 + \\sqrt{q})^2$。\n\n6.  **选择维度 $k$**：计算严格大于阈值 $\\lambda_{+}$ 的排序后特征值 $\\lambda_i$ 的数量。这个数量就是所选的内在维度 $k$。\n\n7.  **格式化输出**：对于该测试用例，将所选的整数 $k$ 和包含 $p$ 个 FVE 值的列表组合成一个单独的列表。对所有测试用例重复此操作，并按指定格式化最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the dimensionality reduction problem for a suite of test cases.\n    For each case, it calculates the eigenvalues of a spiked covariance matrix,\n    computes the fraction of variance explained (FVE) array, and determines\n    the number of significant components (k) using a Marchenko-Pastur criterion.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (p, n, sigma_sq, spikes_tuple)\n        (10, 400, 0.5, (3.0, 2.0, 1.0)),\n        (10, 200, 1.0, ()),\n        (10, 50, 1.0, (0.7,)),\n        (10, 100, 0.8, (1.5, 0.9)),\n    ]\n\n    results = []\n    for p, n, sigma_sq, spikes in test_cases:\n        # Main logic to calculate the result for one case goes here.\n\n        # 1. Construct the list of eigenvalues from the spiked covariance model.\n        # The matrix is Sigma = sigma^2 * I + diag(s_1, ..., s_r, 0, ...).\n        # Its eigenvalues are (sigma^2 + s_j) for j=1..r and sigma^2 for the rest.\n        num_spikes = len(spikes)\n        eigenvalues = [sigma_sq + s for s in spikes] + [sigma_sq] * (p - num_spikes)\n        \n        # 2. Sort the eigenvalues in descending order.\n        eigenvalues = np.array(eigenvalues)\n        eigenvalues.sort() # Sorts in-place, ascending\n        sorted_eigs = eigenvalues[::-1] # Reverse to get descending order\n\n        # 3. Compute the total variance.\n        total_variance = np.sum(sorted_eigs)\n\n        # 4. Compute the Fraction of Variance Explained (FVE) array.\n        # Handle the edge case of zero total variance to avoid division by zero.\n        if total_variance > 0:\n            cumulative_variance = np.cumsum(sorted_eigs)\n            fve_array = cumulative_variance / total_variance\n        else:\n            # If total variance is zero, all eigenvalues are zero.\n            # FVE is ill-defined, but we can treat it as an array of zeros.\n            fve_array = np.zeros(p)\n            \n        fve_rounded = np.round(fve_array, 6)\n\n        # 5. Compute the Marchenko-Pastur upper edge.\n        q = p / n\n        lambda_plus = sigma_sq * (1 + np.sqrt(q))**2\n\n        # 6. Select k as the count of eigenvalues strictly greater than lambda_plus.\n        selected_k = np.sum(sorted_eigs > lambda_plus)\n\n        # 7. Format the result for this test case.\n        case_result = [int(selected_k)] + fve_rounded.tolist()\n        results.append(str(case_result))\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, represented as a string.\n    # Example: [[k1, ...], [k2, ...]]\n    # The template `f\"[{','.join(map(str, results))}]` with `results` as a list of lists\n    # will produce `[[k1,...],[k2,...]]`. This matches structural requirements.\n    # Since `results` here is a list of strings, it will also work.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "尽管主成分分析非常强大，但其本质上是一种线性方法，可能无法捕捉到神经数据中固有的复杂非线性结构。本练习将向您介绍核主成分分析（Kernel PCA），这是一种能够揭示非线性数据流形的先进扩展。您将从第一性原理出发进行推导并亲手实现该算法，从而理解“核技巧”（kernel trick）如何让我们在隐式的高维特征空间中执行 PCA，以揭示线性方法无法探查的模式。",
            "id": "4156661",
            "problem": "给定一个由高维向量表示的模拟神经记录集合。每个向量对应一个单一试验，由 $n_{\\text{neurons}}$ 个神经元在 $n_{\\text{bins}}$ 个时间窗内的脉冲计数组成，并被扁平化为一个单一向量。任务是从第一性原理出发，形式化地推导核主成分分析（Kernel PCA），并实现一个算法，使用高斯核计算嵌入，包括在特征空间中进行适当的中心化。然后，将此算法应用于指定的神经数据集，并为测试套件生成量化输出。\n\n基本和核心定义：\n- 特征空间中的主成分分析（PCA）从定义在中心化特征向量上的协方差算子开始。考虑一个映射 $\\phi:\\mathbb{R}^d\\to\\mathcal{H}$ 到再生核希尔伯特空间（RKHS）。对于 $n$ 个训练样本 $\\{x_i\\}_{i=1}^n$，令 $\\Phi_i=\\phi(x_i)-\\bar{\\phi}$ 为中心化的特征向量，其中 $\\bar{\\phi}=\\frac{1}{n}\\sum_{i=1}^n\\phi(x_i)$。特征空间协方差算子为 $C=\\frac{1}{n}\\sum_{i=1}^n \\Phi_i\\Phi_i^{\\top}$。PCA 寻求满足 $Cw=\\lambda w$ 且单位范数 $\\|w\\|_{\\mathcal{H}}=1$ 的特征对 $(\\lambda,w)$。\n- 核技巧使用格拉姆矩阵 $K$，其元素为 $K_{ij}=k(x_i,x_j)=\\langle \\phi(x_i),\\phi(x_j)\\rangle_{\\mathcal{H}}$。特征空间中心化对应于通过一个中心化矩阵对 $K$ 进行中心化。\n- 对于高斯核，$k(x,y)=\\exp\\left(-\\frac{\\|x-y\\|^2}{2\\sigma^2}\\right)$，其中带宽参数 $\\sigma>0$。\n\n推导任务：\n1. 从特征空间中的PCA定义出发，证明任何主方向都满足 $w=\\sum_{i=1}^n\\alpha_i\\Phi_i$（对于某些系数 $\\alpha\\in\\mathbb{R}^n$），并根据中心化格拉姆矩阵 $K_c$ 推导出特征值问题。明确推导协方差算子 $C$ 的特征值与中心化格拉姆矩阵 $K_c$ 的特征值之间的关系。\n2. 使用中心化矩阵 $H=I-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$，推导在特征空间中对格拉姆矩阵进行中心化的过程，并证明 $K_c=HKH$ 对应于中心化特征向量之间的内积，即 $(K_c)_{ij}=\\langle \\Phi_i,\\Phi_j\\rangle_{\\mathcal{H}}$。\n3. 对于新数据点 $x$ 的样本外嵌入，推导中心化核向量 $k_c(x)\\in\\mathbb{R}^n$ 的公式，其元素为 $[k_c(x)]_i=\\langle \\Phi_i,\\phi(x)-\\bar{\\phi}\\rangle_{\\mathcal{H}}$，且该公式仅用 $K$、训练均值和 $k(x_i,x)$ 表示。根据 $K_c$ 的特征向量和特征值，推导到主方向上的投影公式。\n\n算法实现要求：\n- 构建一个模拟神经数据集，具有以下规范，以确保科学真实性和可复现性：\n  - 设置固定随机种子 $s=12345$。\n  - 训练时使用 $n_{\\text{neurons}}=40$，$n_{\\text{bins}}=20$，$n_{\\text{trials}}=60$，并额外生成一个留出的测试试验。\n  - 每个神经元的基线发放率独立地从 $[5,15]$（单位为任意单位/时间窗）上的均匀分布中抽取。\n  - 定义时间调制 $g(t)=\\exp\\left(-\\frac{(t-t_0)^2}{2\\sigma_t^2}\\right)$，其中 $t_0=10$，$\\sigma_t=3$，对于 $t=0,1,\\dots,19$。\n  - 定义特定条件的振幅：对于条件 $A$（一半的试验），振幅 $a_i^{(A)}$ 独立地从均值为 $3$、标准差为 $1$ 的正态分布中抽取；对于条件 $B$（另一半试验），$a_i^{(B)}$ 独立地从均值为 $-3$、标准差为 $1$ 的正态分布中抽取。通过向每个神经元的振幅添加独立的 $\\mathcal{N}(0,0.5)$ 来增加逐试验的振幅噪声。将瞬时发放率的最小值限制在 $0.1$，以避免负发放率。\n  - 每个时间窗的脉冲计数使用具有指定发放率的独立泊松抽样生成，并将每个试验扁平化为 $\\mathbb{R}^{800}$ 中的一个向量。\n  - 对训练数据进行跨试验标准化：对于每个特征维度，减去跨试验均值并除以跨试验标准差；将任何为零的标准差替换为 $1$，以避免除以零。使用训练均值和标准差对留出的测试试验应用相同的中心化和缩放。\n\n- 核PCA实现细节：\n  - 使用指定的 $\\sigma$ 计算训练数据的高斯核矩阵 $K\\in\\mathbb{R}^{n\\times n}$，公式为 $K_{ij}=\\exp\\left(-\\frac{\\|x_i-x_j\\|^2}{2\\sigma^2}\\right)$。\n  - 使用 $H=I-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ 和 $K_c=HKH$ 在特征空间中对格拉姆矩阵进行中心化。\n  - 求解 $K_c$ 的对称特征值问题，以获得特征值 $\\mu_k\\ge 0$ 和满足 $K_c v_k=\\mu_k v_k$ 的标准正交特征向量 $v_k\\in\\mathbb{R}^n$。通过按降序对特征值进行排序，并将由数值误差引起的任何微小负特征值裁剪为 $0$ 来确保数值稳定性。\n  - 对于 $\\mu_k>0$，通过设置 $w_k=\\frac{1}{\\sqrt{\\mu_k}}\\sum_{i=1}^n (v_k)_i \\Phi_i$ 来进行归一化，使得特征空间主方向具有单位范数。\n  - 通过 $y_{k}(x_j)=\\langle w_k,\\Phi_j\\rangle_{\\mathcal{H}}=\\sqrt{\\mu_k}\\,(v_k)_j$ 计算训练嵌入。\n  - 对于一个新点 $x$，通过以下公式计算中心化核向量 $k_c(x)$\n    $$k_c(x)=k(x)-\\frac{1}{n}K\\mathbf{1}-\\frac{1}{n}\\left(\\sum_{i=1}^n k(x_i,x)\\right)\\mathbf{1}+\\frac{1}{n^2}\\left(\\mathbf{1}^{\\top}K\\mathbf{1}\\right)\\mathbf{1},$$\n    其中 $k(x)$ 是元素为 $k(x_i,x)$ 的向量，$\\mathbf{1}$ 是全一向量。然后通过以下公式计算样本外嵌入\n    $$y_k(x)=\\frac{v_k^{\\top}k_c(x)}{\\sqrt{\\mu_k}},\\quad \\text{约定当 } \\mu_k=0 \\text{ 时 } y_k(x)=0。$$\n\n测试套件和输出：\n- 对所有测试用例使用上面定义的单个训练数据集和留出的测试试验。对于下面的每个用例，计算所需的输出。\n  1. 用例 A（正常路径）：$\\sigma=5.0$，保留 $m=3$ 个主成分。输出前 $m$ 个方差解释比例的列表，计算为 $\\left[\\frac{\\mu_1}{\\sum_{j}\\mu_j},\\frac{\\mu_2}{\\sum_{j}\\mu_j},\\frac{\\mu_3}{\\sum_{j}\\mu_j}\\right]$，格式为十进制浮点数。\n  2. 用例 B（小带宽边缘情况）：$\\sigma=0.1$，保留 $m=2$ 个主成分。输出留出测试试验的前两个样本外嵌入坐标 $[y_1(x_{\\text{test}}),y_2(x_{\\text{test}})]$ 的列表，格式为浮点数。\n  3. 用例 C（大带宽边界情况）：$\\sigma=1000.0$。输出中心化格拉姆矩阵的弗罗贝尼乌斯范数 $\\|K_c\\|_F$ 作为单个浮点数，在这种情况下该值应接近于零。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。三个元素按顺序对应于用例 A、B 和 C 的输出。由于用例 A 和 B 需要列表，最终格式将为\n  [[f_1, f_2, f_3], [y_1, y_2], ν],\n  其中 $f_i$ 是方差解释比例浮点数，$y_i$ 是嵌入浮点数，ν 是弗罗贝尼乌斯范数浮点数。",
            "solution": "该问题陈述结构良好、科学基础扎实且内部一致。它为应用于模拟神经科学数据集的核主成分分析（Kernel PCA）的形式化推导和实现，提供了一套完整的定义、参数和任务。所有必要的组件都已指定，从而能够得到一个唯一且可验证的解。\n\n我们将首先提供所要求的理论推导，然后按规定实现算法。\n\n### 1. 核PCA特征值问题的推导\n\n特征空间 $\\mathcal{H}$ 中PCA的目标是找到协方差算子 $C = \\frac{1}{n}\\sum_{i=1}^n \\Phi_i\\Phi_i^{\\top}$ 的特征向量 $w \\in \\mathcal{H}$，其中 $\\Phi_i = \\phi(x_i) - \\bar{\\phi}$ 是中心化的特征向量。特征值问题由下式给出：\n$$Cw = \\lambda w, \\quad \\text{其中 } \\|w\\|_{\\mathcal{H}}=1$$\n代入 $C$ 的定义：\n$$ \\frac{1}{n}\\sum_{i=1}^n \\Phi_i\\Phi_i^{\\top} w = \\lambda w $$\n项 $\\Phi_i^{\\top} w$ 是内积 $\\langle \\Phi_i, w \\rangle_{\\mathcal{H}}$，它是一个标量。此方程表明，任何特征向量 $w$ 都必须位于特征空间中中心化数据点 $\\{\\Phi_i\\}_{i=1}^n$ 的张成空间内。因此，$w$ 可以表示为这些向量的线性组合：\n$$ w = \\sum_{i=1}^n \\alpha_i \\Phi_i $$\n对于某些系数 $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^{\\top} \\in \\mathbb{R}^n$。\n\n将 $w$ 的这个展开式代回特征值方程：\n$$ \\frac{1}{n}\\sum_{j=1}^n \\Phi_j \\langle \\Phi_j, \\sum_{i=1}^n \\alpha_i \\Phi_i \\rangle = \\lambda \\sum_{i=1}^n \\alpha_i \\Phi_i $$\n为了求解系数 $\\alpha_i$，我们将整个方程投影到一个基向量 $\\Phi_k$ 上，对于任意 $k \\in \\{1, \\dots, n\\}$：\n$$ \\langle \\Phi_k, \\frac{1}{n}\\sum_{j=1}^n \\Phi_j \\sum_{i=1}^n \\alpha_i \\langle \\Phi_j, \\Phi_i \\rangle \\rangle = \\langle \\Phi_k, \\lambda \\sum_{i=1}^n \\alpha_i \\Phi_i \\rangle $$\n利用内积的线性性质：\n$$ \\frac{1}{n}\\sum_{j=1}^n \\langle \\Phi_k, \\Phi_j \\rangle \\sum_{i=1}^n \\alpha_i \\langle \\Phi_j, \\Phi_i \\rangle = \\lambda \\sum_{i=1}^n \\alpha_i \\langle \\Phi_k, \\Phi_i \\rangle $$\n我们定义中心化格拉姆矩阵 $K_c \\in \\mathbb{R}^{n \\times n}$，其元素为 $(K_c)_{ij} = \\langle \\Phi_i, \\Phi_j \\rangle_{\\mathcal{H}}$。方程变为：\n$$ \\frac{1}{n}\\sum_{j=1}^n (K_c)_{kj} \\sum_{i=1}^n (K_c)_{ji} \\alpha_i = \\lambda \\sum_{i=1}^n (K_c)_{ki} \\alpha_i $$\n用矩阵表示法，即：\n$$ \\frac{1}{n} K_c (K_c \\alpha) = \\lambda (K_c \\alpha) $$\n这表明 $K_c \\alpha$ 是 $K_c$ 的一个特征向量，特征值为 $n\\lambda$。如果 $K_c \\alpha \\neq 0$，那么 $\\alpha$ 也是 $K_c$ 的一个特征向量，具有相同的特征值。令 $\\mu = n\\lambda$。系数 $\\alpha$ 的特征值问题是：\n$$ K_c \\alpha = \\mu \\alpha $$\n这表明，求解 $n \\times n$ 中心化格拉姆矩阵 $K_c$ 的特征值问题等价于在（可能无限维的）特征空间 $\\mathcal{H}$ 中求解PCA问题。$K_c$ 的特征值 $\\mu$ 与协方差算子 $C$ 的特征值 $\\lambda$ 之间的关系是 $\\lambda = \\mu/n$。\n\n归一化条件 $\\|w\\|_{\\mathcal{H}}=1$ 对特征向量 $\\alpha$ 施加了一个约束：\n$$ 1 = \\|w\\|^2 = \\langle \\sum_{i=1}^n \\alpha_i \\Phi_i, \\sum_{j=1}^n \\alpha_j \\Phi_j \\rangle = \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j \\langle \\Phi_i, \\Phi_j \\rangle = \\alpha^{\\top} K_c \\alpha $$\n代入 $K_c \\alpha = \\mu \\alpha$：\n$$ 1 = \\alpha^{\\top} (\\mu \\alpha) = \\mu (\\alpha^{\\top} \\alpha) = \\mu \\|\\alpha\\|^2 $$\n因此，$\\|\\alpha\\|^2 = 1/\\mu$。如果我们找到 $K_c$ 的标准正交特征向量 $v_k$，使得 $K_c v_k = \\mu_k v_k$ 且 $v_k^{\\top}v_k=1$，则相应的系数向量 $\\alpha_k$ 是 $v_k$ 的一个缩放版本，具体为 $\\alpha_k = v_k / \\sqrt{\\mu_k}$。特征空间主方向则为 $w_k = \\frac{1}{\\sqrt{\\mu_k}}\\sum_{i=1}^n (v_k)_i \\Phi_i$。\n\n### 2. 中心化格拉姆矩阵的推导\n\n中心化格拉姆矩阵 $K_c$ 的元素是 $(K_c)_{ij} = \\langle \\Phi_i, \\Phi_j \\rangle_{\\mathcal{H}}$，其中 $\\Phi_i = \\phi(x_i) - \\bar{\\phi}$ 且 $\\bar{\\phi} = \\frac{1}{n}\\sum_{l=1}^n \\phi(x_l)$。我们展开内积：\n$$ (K_c)_{ij} = \\langle \\phi(x_i) - \\bar{\\phi}, \\phi(x_j) - \\bar{\\phi} \\rangle = \\langle \\phi(x_i), \\phi(x_j) \\rangle - \\langle \\phi(x_i), \\bar{\\phi} \\rangle - \\langle \\bar{\\phi}, \\phi(x_j) \\rangle + \\langle \\bar{\\phi}, \\bar{\\phi} \\rangle $$\n我们使用核函数 $k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle$ 和未中心化的格拉姆矩阵 $K$（其元素为 $K_{ij}=k(x_i,x_j)$）来表示每一项：\n\\begin{itemize}\n    \\item $\\langle \\phi(x_i), \\phi(x_j) \\rangle = K_{ij}$\n    \\item $\\langle \\phi(x_i), \\bar{\\phi} \\rangle = \\langle \\phi(x_i), \\frac{1}{n}\\sum_{l=1}^n \\phi(x_l) \\rangle = \\frac{1}{n}\\sum_{l=1}^n \\langle \\phi(x_i), \\phi(x_l) \\rangle = \\frac{1}{n}\\sum_{l=1}^n K_{il}$\n    \\item $\\langle \\bar{\\phi}, \\phi(x_j) \\rangle = \\frac{1}{n}\\sum_{l=1}^n K_{lj}$\n    \\item $\\langle \\bar{\\phi}, \\bar{\\phi} \\rangle = \\langle \\frac{1}{n}\\sum_{l=1}^n \\phi(x_l), \\frac{1}{n}\\sum_{m=1}^n \\phi(x_m) \\rangle = \\frac{1}{n^2}\\sum_{l=1}^n \\sum_{m=1}^n \\langle \\phi(x_l), \\phi(x_m) \\rangle = \\frac{1}{n^2}\\sum_{l,m} K_{lm}$\n\\end{itemize}\n将这些项组合起来得到：\n$$ (K_c)_{ij} = K_{ij} - \\frac{1}{n}\\sum_{l=1}^n K_{il} - \\frac{1}{n}\\sum_{l=1}^n K_{lj} + \\frac{1}{n^2}\\sum_{l,m} K_{lm} $$\n现在我们证明这等价于 $HKH$，其中 $H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ 是中心化矩阵（$I$ 是单位矩阵，$\\mathbf{1}$ 是全一向量）。\n$$ K_c = HKH = (I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}) K (I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}) $$\n展开这个乘积：\n$$ K_c = K - \\frac{1}{n}K\\mathbf{1}\\mathbf{1}^{\\top} - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}K + \\frac{1}{n^2}\\mathbf{1}\\mathbf{1}^{\\top}K\\mathbf{1}\\mathbf{1}^{\\top} $$\n我们分析每个矩阵项的第 $(i,j)$ 个元素：\n\\begin{itemize}\n    \\item $(K)_{ij} = K_{ij}$\n    \\item $(K\\mathbf{1}\\mathbf{1}^{\\top})_{ij} = (K\\mathbf{1})_i (\\mathbf{1}^{\\top})_j = (\\sum_{l=1}^n K_{il}) \\cdot 1 = \\sum_{l=1}^n K_{il}$\n    \\item $(\\mathbf{1}\\mathbf{1}^{\\top}K)_{ij} = (\\mathbf{1})_i (\\mathbf{1}^{\\top}K)_j = 1 \\cdot (\\sum_{l=1}^n K_{lj}) = \\sum_{l=1}^n K_{lj}$\n    \\item $(\\mathbf{1}\\mathbf{1}^{\\top}K\\mathbf{1}\\mathbf{1}^{\\top})_{ij} = (\\mathbf{1})_i (\\mathbf{1}^{\\top}K\\mathbf{1}) (\\mathbf{1}^{\\top})_j$。中间项 $\\mathbf{1}^{\\top}K\\mathbf{1} = \\sum_{l,m} K_{lm}$ 是一个标量。因此，第 $(i,j)$ 个元素是 $\\sum_{l,m} K_{lm}$。\n\\end{itemize}\n组合 $(HKH)_{ij}$ 的各项：\n$$ (HKH)_{ij} = K_{ij} - \\frac{1}{n}\\sum_{l=1}^n K_{il} - \\frac{1}{n}\\sum_{l=1}^n K_{lj} + \\frac{1}{n^2}\\sum_{l,m} K_{lm} $$\n这与从第一性原理推导出的 $(K_c)_{ij}$ 的表达式相匹配。因此，通过 $K_c=HKH$ 对格拉姆矩阵进行中心化等价于计算中心化特征向量的内积。\n\n### 3. 样本外嵌入的推导\n\n对于一个新的数据点 $x$，其在第 $k$ 个主成分上的嵌入是其中心化特征向量 $\\Phi(x) = \\phi(x) - \\bar{\\phi}$ 在主方向 $w_k$ 上的投影：\n$$ y_k(x) = \\langle w_k, \\Phi(x) \\rangle_{\\mathcal{H}} $$\n使用展开式 $w_k = \\sum_{i=1}^n (\\alpha_k)_i \\Phi_i$，其中 $\\alpha_k = v_k/\\sqrt{\\mu_k}$（对于 $\\mu_k > 0$）：\n$$ y_k(x) = \\langle \\sum_{i=1}^n (\\alpha_k)_i \\Phi_i, \\Phi(x) \\rangle = \\sum_{i=1}^n (\\alpha_k)_i \\langle \\Phi_i, \\Phi(x) \\rangle $$\n我们定义一个向量 $k_c(x) \\in \\mathbb{R}^n$，其元素为 $[k_c(x)]_i = \\langle \\Phi_i, \\Phi(x) \\rangle$。投影则为 $y_k(x) = \\alpha_k^{\\top} k_c(x)$。代入 $\\alpha_k = v_k/\\sqrt{\\mu_k}$：\n$$ y_k(x) = \\frac{v_k^{\\top} k_c(x)}{\\sqrt{\\mu_k}} $$\n如果 $\\mu_k=0$，则相应的主成分不存在（方差为零），投影通常设置为 $0$。\n\n现在我们推导 $k_c(x)$ 中元素的公式：\n$$ [k_c(x)]_i = \\langle \\Phi_i, \\Phi(x) \\rangle = \\langle \\phi(x_i) - \\bar{\\phi}, \\phi(x) - \\bar{\\phi} \\rangle $$\n$$ = \\langle \\phi(x_i), \\phi(x) \\rangle - \\langle \\phi(x_i), \\bar{\\phi} \\rangle - \\langle \\bar{\\phi}, \\phi(x) \\rangle + \\langle \\bar{\\phi}, \\bar{\\phi} \\rangle $$\n各项的计算与之前类似：\n\\begin{itemize}\n    \\item $\\langle \\phi(x_i), \\phi(x) \\rangle = k(x_i, x)$\n    \\item $\\langle \\phi(x_i), \\bar{\\phi} \\rangle = \\frac{1}{n}\\sum_{j=1}^n K_{ij}$\n    \\item $\\langle \\bar{\\phi}, \\phi(x) \\rangle = \\frac{1}{n}\\sum_{j=1}^n k(x_j, x)$\n    \\item $\\langle \\bar{\\phi}, \\bar{\\phi} \\rangle = \\frac{1}{n^2}\\sum_{j,l} K_{jl}$\n\\end{itemize}\n因此，对于每个 $i \\in \\{1, \\dots, n\\}$：\n$$ [k_c(x)]_i = k(x_i, x) - \\frac{1}{n}\\sum_{j=1}^n K_{ij} - \\frac{1}{n}\\sum_{j=1}^n k(x_j, x) + \\frac{1}{n^2}\\sum_{j,l} K_{jl} $$\n以向量形式，令 $k(x)$ 是元素为 $k(x_i, x)$ 的向量，$\\mathbf{1}$ 是全一向量。向量 $k_c(x)$ 的方程是：\n$$ k_c(x) = k(x) - \\frac{1}{n}K\\mathbf{1} - \\frac{1}{n}(\\mathbf{1}^{\\top}k(x))\\mathbf{1} + \\frac{1}{n^2}(\\mathbf{1}^{\\top}K\\mathbf{1})\\mathbf{1} $$\n这与问题陈述中提供的公式相符，并允许仅使用与训练数据和新数据点的核函数求值来计算样本外嵌入。",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate data generation, Kernel PCA computation,\n    and evaluation for the specified test cases.\n    \"\"\"\n\n    def generate_data(seed=12345):\n        \"\"\"\n        Generates simulated neural data according to the problem specification.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Parameters\n        n_neurons = 40\n        n_bins = 20\n        n_trials_train = 60\n        n_trials_total = n_trials_train + 1  # +1 for test trial\n        d = n_neurons * n_bins\n        \n        # Baseline firing rates\n        baseline_rates = rng.uniform(5, 15, size=n_neurons)\n        \n        # Temporal modulation\n        t = np.arange(n_bins)\n        t0, sigma_t = 10, 3\n        g = np.exp(-(t - t0)**2 / (2 * sigma_t**2))\n        \n        # Condition-specific amplitudes\n        a_A = rng.normal(3, 1, size=n_neurons)\n        a_B = rng.normal(-3, 1, size=n_neurons)\n        \n        all_trials_data = np.zeros((n_trials_total, d))\n        \n        for trial_idx in range(n_trials_total):\n            # The test trial (last one) is chosen to be from Condition A\n            if trial_idx  n_trials_train / 2 or trial_idx == n_trials_train:\n                base_amplitudes = a_A\n            else: # Condition B\n                base_amplitudes = a_B\n            \n            trial_rates = np.zeros((n_neurons, n_bins))\n            for neuron_idx in range(n_neurons):\n                # Add trial-wise amplitude noise\n                amp_noise = rng.normal(0, 0.5)\n                total_amplitude = base_amplitudes[neuron_idx] + amp_noise\n                \n                # Calculate rate using the additive model\n                rate_profile = baseline_rates[neuron_idx] + total_amplitude * g\n                trial_rates[neuron_idx, :] = rate_profile\n            \n            # Clip rates\n            clipped_rates = np.maximum(trial_rates, 0.1)\n            \n            # Generate spike counts from Poisson distribution\n            spike_counts = rng.poisson(clipped_rates)\n            \n            # Flatten and store\n            all_trials_data[trial_idx, :] = spike_counts.flatten()\n            \n        X_train = all_trials_data[:n_trials_train, :]\n        x_test = all_trials_data[n_trials_train, :]\n        \n        # Standardize data\n        mean_train = np.mean(X_train, axis=0)\n        std_train = np.std(X_train, axis=0)\n        # Avoid division by zero\n        std_train[std_train == 0] = 1.0\n        \n        X_train_std = (X_train - mean_train) / std_train\n        x_test_std = (x_test - mean_train) / std_train\n        \n        return X_train_std, x_test_std\n\n    def run_kernel_pca(X_train, x_test, sigma, case_params):\n        \"\"\"\n        Implements the Kernel PCA algorithm and computes results for a test case.\n        \"\"\"\n        n = X_train.shape[0]\n        \n        # Compute Gaussian kernel matrix\n        sq_dists = squareform(pdist(X_train, 'sqeuclidean'))\n        K = np.exp(-sq_dists / (2 * sigma**2))\n        \n        # Center the Gram matrix\n        H = np.eye(n) - np.ones((n, n)) / n\n        K_c = H @ K @ H\n        \n        # Solve the eigenvalue problem for the centered Gram matrix\n        evals, evecs = np.linalg.eigh(K_c)\n        \n        # Sort eigenvalues and eigenvectors in descending order\n        idx = np.argsort(evals)[::-1]\n        evals = evals[idx]\n        evecs = evecs[:, idx]\n        \n        # Clip numerical noise resulting in small negative eigenvalues\n        evals[evals  0] = 0\n        \n        # --- Compute case-specific outputs ---\n        case_id = case_params['id']\n        if case_id == 'A':\n            m = case_params['m']\n            total_variance = np.sum(evals)\n            if total_variance > 0:\n                explained_variances = evals[:m] / total_variance\n            else:\n                explained_variances = np.zeros(m)\n            return explained_variances.tolist()\n            \n        elif case_id == 'B':\n            m = case_params['m']\n            # Compute kernel vector for the test point\n            k_test = np.exp(-np.sum((X_train - x_test)**2, axis=1) / (2 * sigma**2))\n            \n            # Compute the centered kernel vector k_c(x_test)\n            one_n = np.ones(n)\n            k_c_test = (k_test - (K @ one_n) / n - \n                        (np.sum(k_test) / n) * one_n + \n                        (np.sum(K) / n**2) * one_n)\n            \n            # Compute out-of-sample embeddings\n            embeddings = []\n            for k in range(m):\n                mu_k = evals[k]\n                v_k = evecs[:, k]\n                if mu_k > 1e-15: # Use a small threshold for stability\n                    y_k = (v_k.T @ k_c_test) / np.sqrt(mu_k)\n                else:\n                    y_k = 0.0\n                embeddings.append(y_k)\n            return embeddings\n\n        elif case_id == 'C':\n            # Compute Frobenius norm of the centered Gram matrix\n            norm_kc = np.linalg.norm(K_c, 'fro')\n            return norm_kc\n\n    # Generate data once\n    X_train_std, x_test_std = generate_data()\n\n    # Define test cases\n    test_cases = [\n        {'id': 'A', 'sigma': 5.0, 'm': 3},\n        {'id': 'B', 'sigma': 0.1, 'm': 2},\n        {'id': 'C', 'sigma': 1000.0, 'm': None},\n    ]\n    \n    results = []\n    for case_params in test_cases:\n        result = run_kernel_pca(X_train_std, x_test_std, case_params['sigma'], case_params)\n        results.append(result)\n        \n    # Format final output string\n    res_a_str = f\"[{','.join(f'{x:.7f}' for x in results[0])}]\"\n    res_b_str = f\"[{','.join(f'{x:.7f}' for x in results[1])}]\"\n    res_c_str = f\"{results[2]:.7f}\"\n\n    print(f\"[{res_a_str},{res_b_str},{res_c_str}]\")\n\nsolve()\n```"
        }
    ]
}