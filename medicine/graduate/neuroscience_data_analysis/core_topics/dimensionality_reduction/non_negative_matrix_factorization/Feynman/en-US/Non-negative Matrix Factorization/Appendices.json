{
    "hands_on_practices": [
        {
            "introduction": "To effectively use any algorithm, it is crucial to first understand its core mechanics. This first practice provides a hands-on calculation of the multiplicative update rules that form the heart of NMF . By performing one full iteration on a carefully chosen set of matrices, you will gain a concrete feel for how the algorithm updates its factors and confirm its behavior when it starts at a minimum of the cost function.",
            "id": "4182176",
            "problem": "A laboratory records spike counts across $N=3$ neurons and $T=3$ time bins, summarized in the non-negative data matrix $X \\in \\mathbb{R}_{\\ge 0}^{3 \\times 3}$. The goal is to perform Non-negative Matrix Factorization (NMF) to approximate $X \\approx W H$, where $W \\in \\mathbb{R}_{\\ge 0}^{3 \\times 2}$ encodes neuron loadings on $K=2$ latent assemblies and $H \\in \\mathbb{R}_{\\ge 0}^{2 \\times 3}$ encodes assembly activations across time. Assume the squared-error objective with Frobenius norm and non-negativity constraints, and perform one full multiplicative-update iteration defined as updating $W$ given $H$ and then updating $H$ given the updated $W$, derived from the stated objective without external heuristics.\n\nThe specific matrices are:\n$$\nX = \\begin{bmatrix}\n2 & 4 & 6 \\\\\n2 & 4 & 6 \\\\\n2 & 4 & 6\n\\end{bmatrix},\\quad\nW^{(0)} = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1\n\\end{bmatrix},\\quad\nH^{(0)} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n1 & 2 & 3\n\\end{bmatrix}.\n$$\n\nStarting from the squared-error cost $J(W,H) = \\tfrac{1}{2}\\|X - W H\\|_{F}^{2}$ subject to $W \\ge 0$ and $H \\ge 0$, derive the multiplicative updates for $W$ and $H$ and then implement exactly one iteration on the given $X$, $W^{(0)}$, and $H^{(0)}$ to obtain $W^{(1)}$ and $H^{(1)}$. Express your final answer as a single row matrix listing the six updated entries of $W^{(1)}$ in row-major order followed by the six updated entries of $H^{(1)}$ in row-major order. If numerical values require rounding, round your answer to four significant figures.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of non-negative matrix factorization (NMF), a standard method in data analysis. The problem is well-posed, providing all necessary data matrices ($X$, $W^{(0)}$, $H^{(0)}$), parameters ($N=3$, $T=3$, $K=2$), a clear objective function ($J(W,H) = \\frac{1}{2}\\|X - W H\\|_{F}^{2}$), and a precisely defined task (one full multiplicative-update iteration). The data and constraints are self-consistent, and the language is objective and unambiguous. The problem can be solved to produce a unique, meaningful answer.\n\nThe core of the problem is to minimize the cost function $J(W,H)$ subject to the constraints that the elements of $W$ and $H$ are non-negative.\n$$\nJ(W,H) = \\frac{1}{2} \\|X - WH\\|_{F}^{2} = \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{T} \\left(X_{ij} - \\sum_{k=1}^{K} W_{ik} H_{kj}\\right)^2\n$$\nThe multiplicative update rules are derived by considering the gradient of $J$ with respect to the elements of $W$ and $H$. The rules provide a fixed-point iteration that respects the non-negativity constraint.\n\nThe gradient with respect to an element $W_{ab}$ of $W$ is:\n$$\n\\frac{\\partial J}{\\partial W_{ab}} = \\sum_{j=1}^{T} \\left(X_{aj} - \\sum_{k=1}^{K} W_{ak} H_{kj}\\right) (-H_{bj}) = - \\sum_{j=1}^{T} (X - WH)_{aj} (H^T)_{jb} = -((X-WH)H^T)_{ab}\n$$\nIn matrix form, the gradient is $\\nabla_W J = -(X-WH)H^T = WHH^T - XH^T$. The multiplicative update rule for $W$ is:\n$$\nW_{ab} \\leftarrow W_{ab} \\frac{(XH^T)_{ab}}{(WHH^T)_{ab}}\n$$\nThis can be written compactly using element-wise (Hadamard) product $\\odot$ and division as:\n$$\nW \\leftarrow W \\odot \\frac{XH^T}{WHH^T}\n$$\nSimilarly, the gradient with respect to an element $H_{cd}$ of $H$ is:\n$$\n\\frac{\\partial J}{\\partial H_{cd}} = \\sum_{i=1}^{N} \\left(X_{id} - \\sum_{k=1}^{K} W_{ik} H_{kd}\\right) (-W_{ic}) = - \\sum_{i=1}^{N} (W^T)_{ci} (X - WH)_{id} = -(W^T(X-WH))_{cd}\n$$\nIn matrix form, $\\nabla_H J = -W^T(X-WH) = W^TWH - W^TX$. The multiplicative update rule for $H$ is:\n$$\nH_{cd} \\leftarrow H_{cd} \\frac{(W^TX)_{cd}}{(W^TWH)_{cd}}\n$$\nThis can be written compactly as:\n$$\nH \\leftarrow H \\odot \\frac{W^TX}{W^TWH}\n$$\nThe problem specifies one full iteration: updating $W$ first to obtain $W^{(1)}$, then updating $H$ using $W^{(1)}$ to obtain $H^{(1)}$.\n\nThe given initial matrices are:\n$$\nX = \\begin{bmatrix} 2 & 4 & 6 \\\\ 2 & 4 & 6 \\\\ 2 & 4 & 6 \\end{bmatrix},\\quad W^{(0)} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{bmatrix},\\quad H^{(0)} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix}\n$$\nFirst, let us observe that the initial matrices provide a perfect reconstruction of $X$:\n$$\nW^{(0)} H^{(0)} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix} = \\begin{bmatrix} 1+1 & 2+2 & 3+3 \\\\ 1+1 & 2+2 & 3+3 \\\\ 1+1 & 2+2 & 3+3 \\end{bmatrix} = \\begin{bmatrix} 2 & 4 & 6 \\\\ 2 & 4 & 6 \\\\ 2 & 4 & 6 \\end{bmatrix} = X\n$$\nThis implies that the initial cost $J(W^{(0)}, H^{(0)}) = 0$. Since $J \\ge 0$, this is a global minimum, and we expect the update rules to yield no change, i.e., the initial point is a fixed point of the iterative algorithm. We will now verify this through explicit calculation.\n\nStep 1: Update $W$ to find $W^{(1)}$.\nThe update rule is $W^{(1)} = W^{(0)} \\odot \\frac{X (H^{(0)})^T}{W^{(0)} H^{(0)} (H^{(0)})^T}$.\nWe compute the numerator of the update factor:\n$$\n(H^{(0)})^T = \\begin{bmatrix} 1 & 1 \\\\ 2 & 2 \\\\ 3 & 3 \\end{bmatrix}\n$$\n$$\nX (H^{(0)})^T = \\begin{bmatrix} 2 & 4 & 6 \\\\ 2 & 4 & 6 \\\\ 2 & 4 & 6 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 2 & 2 \\\\ 3 & 3 \\end{bmatrix} = \\begin{bmatrix} 2+8+18 & 2+8+18 \\\\ 2+8+18 & 2+8+18 \\\\ 2+8+18 & 2+8+18 \\end{bmatrix} = \\begin{bmatrix} 28 & 28 \\\\ 28 & 28 \\\\ 28 & 28 \\end{bmatrix}\n$$\nFor the denominator, we use the fact that $W^{(0)}H^{(0)} = X$:\n$$\nW^{(0)} H^{(0)} (H^{(0)})^T = X (H^{(0)})^T = \\begin{bmatrix} 28 & 28 \\\\ 28 & 28 \\\\ 28 & 28 \\end{bmatrix}\n$$\nThe update factor is the element-wise division of the numerator by the denominator:\n$$\n\\frac{X (H^{(0)})^T}{W^{(0)} H^{(0)} (H^{(0)})^T} = \\frac{\\begin{bmatrix} 28 & 28 \\\\ 28 & 28 \\\\ 28 & 28 \\end{bmatrix}}{\\begin{bmatrix} 28 & 28 \\\\ 28 & 28 \\\\ 28 & 28 \\end{bmatrix}} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n$$\nNow, we apply the update to $W^{(0)}$:\n$$\nW^{(1)} = W^{(0)} \\odot \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\odot \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n$$\nThus, $W^{(1)} = W^{(0)}$.\n\nStep 2: Update $H$ to find $H^{(1)}$, using the newly computed $W^{(1)}$.\nThe update rule is $H^{(1)} = H^{(0)} \\odot \\frac{(W^{(1)})^T X}{(W^{(1)})^T W^{(1)} H^{(0)}}$.\nSince $W^{(1)} = W^{(0)}$, we have $(W^{(1)})^T = (W^{(0)})^T = \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$.\nWe compute the numerator of the update factor:\n$$\n(W^{(1)})^T X = \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 4 & 6 \\\\ 2 & 4 & 6 \\\\ 2 & 4 & 6 \\end{bmatrix} = \\begin{bmatrix} 6 & 12 & 18 \\\\ 6 & 12 & 18 \\end{bmatrix}\n$$\nNow, we compute the denominator:\n$$\n(W^{(1)})^T W^{(1)} = \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\end{bmatrix}\n$$\n$$\n(W^{(1)})^T W^{(1)} H^{(0)} = \\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\end{bmatrix} \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix} = \\begin{bmatrix} 3+3 & 6+6 & 9+9 \\\\ 3+3 & 6+6 & 9+9 \\end{bmatrix} = \\begin{bmatrix} 6 & 12 & 18 \\\\ 6 & 12 & 18 \\end{bmatrix}\n$$\nThe update factor is:\n$$\n\\frac{(W^{(1)})^T X}{(W^{(1)})^T W^{(1)} H^{(0)}} = \\frac{\\begin{bmatrix} 6 & 12 & 18 \\\\ 6 & 12 & 18 \\end{bmatrix}}{\\begin{bmatrix} 6 & 12 & 18 \\\\ 6 & 12 & 18 \\end{bmatrix}} = \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}\n$$\nFinally, we apply the update to $H^{(0)}$:\n$$\nH^{(1)} = H^{(0)} \\odot \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix} \\odot \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix}\n$$\nThus, $H^{(1)} = H^{(0)}$.\n\nAfter one full iteration, the matrices are unchanged:\n$$\nW^{(1)} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{bmatrix}, \\quad H^{(1)} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix}\n$$\nThe final answer requires the six entries of $W^{(1)}$ in row-major order, followed by the six entries of $H^{(1)}$ in row-major order.\nThe entries of $W^{(1)}$ are $W_{11}=1$, $W_{12}=1$, $W_{21}=1$, $W_{22}=1$, $W_{31}=1$, $W_{32}=1$.\nThe entries of $H^{(1)}$ are $H_{11}=1$, $H_{12}=2$, $H_{13}=3$, $H_{21}=1$, $H_{22}=2$, $H_{23}=3$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 2 & 3 & 1 & 2 & 3\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The choice of rank, $r$, is perhaps the most critical parameter in NMF, as it determines the number of latent components discovered. This exercise guides you through the design of a robust cross-validation procedure to select $r$ in a principled way, a skill essential for producing reproducible and scientifically valid results . It highlights how to properly separate training and validation data to estimate how well the model will perform on unseen data.",
            "id": "4182160",
            "problem": "A neuroscience laboratory has recorded a non-negative data matrix $X \\in \\mathbb{R}_{+}^{n \\times t}$, where $n$ denotes the number of neurons and $t$ denotes the number of time bins, with $X_{ij}$ representing the activity level of neuron $i$ during time bin $j$. To identify latent neural assemblies and their activation patterns, the lab intends to factor $X$ into two non-negative matrices $W \\in \\mathbb{R}_{+}^{n \\times r}$ and $H \\in \\mathbb{R}_{+}^{r \\times t}$ such that $X \\approx WH$, where $r$ is the factorization rank. The factorization should be chosen to balance approximation quality and generalization, avoiding overfitting.\n\nStarting from the core definition of non-negative matrix factorization (NMF), the lab wants to derive a scientifically sound cross-validation (CV) protocol using an entry-wise mask to hold out a subset of entries of $X$ and to select the rank $r$ by optimizing reconstruction error on held-out data. The design must respect non-negativity constraints, avoid leakage of held-out information into training, and provide a principled rule for choosing $r$ based on held-out performance. Consider that for squared-error reconstruction, the training objective restricted to observed entries can be expressed using a binary mask $M \\in \\{0,1\\}^{n \\times t}$, and held-out error is computed on the complement $\\bar{M} = \\mathbf{1} - M$. Alternatively, for count-like data, a divergence appropriate for Poisson observations may be used.\n\nWhich option most correctly specifies a mask-based CV procedure that is consistent with first principles and produces a principled rule for selecting $r$?\n\nA. Draw a random binary mask $M \\in \\{0,1\\}^{n \\times t}$ that retains a fixed proportion $p \\in (0,1)$ of entries as training ($M_{ij} = 1$) and holds out the remainder for validation ($\\bar{M}_{ij} = 1 - M_{ij}$). For a candidate rank $r$, fit NMF by minimizing the masked squared reconstruction loss subject to $W \\ge 0$ and $H \\ge 0$, for example,\n$$\n\\min_{W \\ge 0,\\, H \\ge 0}\\;\\; \\sum_{i=1}^{n} \\sum_{j=1}^{t} M_{ij} \\bigl(X_{ij} - [WH]_{ij}\\bigr)^{2},\n$$\nand compute the validation error\n$$\nE_{\\mathrm{val}}(r) = \\frac{1}{\\|\\bar{M}\\|_{1}} \\sum_{i=1}^{n} \\sum_{j=1}^{t} \\bar{M}_{ij} \\bigl(X_{ij} - [W H]_{ij}\\bigr)^{2}.\n$$\nRepeat this over $K$ independent masks and $S$ random initializations, average $E_{\\mathrm{val}}(r)$ across masks and seeds, and select the smallest rank $r$ that minimizes the mean validation error. Finally, refit NMF on the full matrix $X$ at the selected rank $r$ to obtain the reported $W$ and $H$.\n\nB. For each candidate rank $r$, fit NMF on the full matrix $X$ without masking by minimizing the squared reconstruction loss over all entries under $W \\ge 0$ and $H \\ge 0$, and select the rank $r$ that yields the smallest training reconstruction error. Report the corresponding $W$ and $H$.\n\nC. Partition the columns of $X$ into training and validation sets. For each candidate rank $r$, fit NMF on the training columns only to obtain $W_{\\mathrm{train}}$ and $H_{\\mathrm{train}}$, then compute validation error on the held-out columns by multiplying $W_{\\mathrm{train}} H_{\\mathrm{train}}$ and comparing to the held-out columns. Select the rank $r$ with the smallest validation error and report $W_{\\mathrm{train}}$ and $H_{\\mathrm{train}}$.\n\nD. Use a binary mask $M$ to designate held-out entries, but impute all held-out entries as zeros during training. Fit NMF by minimizing the full-data squared reconstruction loss with imputed zeros and select the rank $r$ that minimizes this imputation-augmented training loss. Report the corresponding $W$ and $H$.\n\nE. Draw a random entry-wise mask $M$ as in option A, but for count-like neural data use the generalized Kullback–Leibler divergence (KL) appropriate for Poisson observations. For a candidate rank $r$, fit NMF by minimizing\n$$\n\\min_{W \\ge 0,\\, H \\ge 0}\\;\\; \\sum_{i=1}^{n} \\sum_{j=1}^{t} M_{ij} \\left( X_{ij} \\log\\frac{X_{ij}}{[WH]_{ij}} - X_{ij} + [WH]_{ij} \\right),\n$$\nand compute the held-out divergence\n$$\nD_{\\mathrm{val}}(r) = \\frac{1}{\\|\\bar{M}\\|_{1}} \\sum_{i=1}^{n} \\sum_{j=1}^{t} \\bar{M}_{ij} \\left( X_{ij} \\log\\frac{X_{ij}}{[WH]_{ij}} - X_{ij} + [WH]_{ij} \\right).\n$$\nRepeat over $K$ masks and $S$ seeds, and choose the smallest rank $r$ whose mean $D_{\\mathrm{val}}(r)$ is within $1$ standard error of the minimum across ranks (the $1$-standard-error rule). Finally, refit NMF on the full matrix $X$ at the selected rank $r$.\n\nSelect all options that are correct.",
            "solution": "The problem statement poses a valid and standard question in computational neuroscience and machine learning: how to select the rank $r$ for non-negative matrix factorization (NMF) of a data matrix $X \\in \\mathbb{R}_{+}^{n \\times t}$ in a way that avoids overfitting. The proposed method is a mask-based cross-validation (CV) procedure. The problem is scientifically grounded, well-posed, and contains sufficient information to evaluate the proposed methodologies.\n\nThe core principles for a valid cross-validation procedure for model selection (here, selecting the rank $r$) are:\n1.  **Data Partitioning**: The available data must be partitioned into a training set, used for learning the model parameters ($W$ and $H$), and a disjoint validation set, used for evaluating the model's generalization performance. Information from the validation set must not be used during training.\n2.  **Model Fitting**: For each candidate model complexity (each rank $r$), the model parameters are estimated by minimizing a loss function on the training data only. For NMF, this involves finding $W$ and $H$ that minimize the reconstruction error of the training data entries, subject to non-negativity constraints.\n3.  **Performance Evaluation**: The fitted model is then used to predict the values for the held-out validation data, and the prediction error on this set is calculated. This validation error serves as an estimate of the model's generalization error.\n4.  **Model Selection**: The validation errors are compared across the different ranks $r$. The rank that yields the best validation performance (e.g., minimum error) is chosen. Heuristics like the \"one-standard-error rule\" can be used to favor simpler models.\n5.  **Final Model Training**: Once the optimal rank $r^*$ is selected, it is standard practice to re-train the model on the entire dataset $X$ using rank $r^*$. This final step leverages all available data to produce the best possible parameter estimates for the chosen model complexity.\n\nThe problem describes a mask-based approach where a binary mask $M \\in \\{0,1\\}^{n \\times t}$ defines the training set (entries where $M_{ij}=1$) and the validation set (entries where $M_{ij}=0$). We will now evaluate each option against these principles.\n\n### Evaluation of Option A\n\nThis option proposes a mask-based CV procedure with a squared error (Frobenius norm) loss function.\n\n1.  **Data Partitioning**: It correctly uses a binary mask $M$ to separate training entries ($M_{ij}=1$) from validation entries ($\\bar{M}_{ij}=1$).\n2.  **Model Fitting**: It correctly states that for a given rank $r$, the model parameters $W$ and $H$ are found by minimizing the loss function on the training entries only:\n    $$ \\min_{W \\ge 0,\\, H \\ge 0}\\;\\; \\sum_{i=1}^{n} \\sum_{j=1}^{t} M_{ij} \\bigl(X_{ij} - [WH]_{ij}\\bigr)^{2} $$\n    This properly prevents information from the held-out data from influencing the training process.\n3.  **Performance Evaluation**: It correctly calculates the validation error as the mean squared error on the held-out entries:\n    $$ E_{\\mathrm{val}}(r) = \\frac{1}{\\|\\bar{M}\\|_{1}} \\sum_{i=1}^{n} \\sum_{j=1}^{t} \\bar{M}_{ij} \\bigl(X_{ij} - [W H]_{ij}\\bigr)^{2} $$\n    The normalization by $\\|\\bar{M}\\|_1$ (the number of held-out entries) is appropriate.\n4.  **Model Selection**: It proposes selecting the smallest rank $r$ that minimizes the mean validation error, averaged over multiple masks ($K$) and random initializations ($S$). This is a principled rule for selecting $r$ based on generalization performance, with the preference for the smallest rank embodying the principle of parsimony.\n5.  **Final Model Training**: It correctly concludes by refitting the model on the full matrix $X$ at the selected rank.\n\nThis procedure adheres to all the principles of a sound cross-validation protocol. The use of squared error is a standard choice for NMF.\n\n**Verdict on A**: **Correct**.\n\n### Evaluation of Option B\n\nThis option proposes fitting NMF on the full matrix and selecting the rank based on training error.\n\nThe training reconstruction error for NMF is a non-increasing function of the rank $r$. A model with a higher rank has more degrees of freedom and will almost always achieve a lower (or equal) reconstruction error on the same data it was trained on. Therefore, a rule that selects the rank $r$ yielding the smallest training error will invariably choose the largest candidate rank $r$. This leads to selecting the most complex model, which is highly likely to be overfitted to the specific noise in the dataset. This approach completely fails to estimate or control for generalization error, which is the primary purpose of cross-validation.\n\n**Verdict on B**: **Incorrect**.\n\n### Evaluation of Option C\n\nThis option proposes partitioning the columns of $X$ (time bins). This is a valid form of block cross-validation. However, the description of the procedure is flawed.\n\nAfter fitting NMF on the training columns to get $W_{\\mathrm{train}}$ and $H_{\\mathrm{train}}$, the option suggests computing validation error by comparing held-out columns to $W_{\\mathrm{train}} H_{\\mathrm{train}}$. This is incorrect. The matrix $H_{\\mathrm{train}}$ contains the activation patterns for the *training* columns, not the *validation* columns. The correct procedure would be to fix $W_{\\mathrm{train}}$ and then solve for the activations of the validation columns, $H_{\\mathrm{val}}$, by minimizing $\\|X_{\\mathrm{val}} - W_{\\mathrm{train}} H_{\\mathrm{val}}\\|^2$. The validation error is the result of this new optimization. The option omits this crucial step.\n\nFurthermore, it suggests reporting $W_{\\mathrm{train}}$ and $H_{\\mathrm{train}}$ as the final result, which were derived only from a subset of the data. The standard and better practice is to refit the model on the full dataset using the selected rank, as done in options A and E.\n\n**Verdict on C**: **Incorrect**.\n\n### Evaluation of Option D\n\nThis option proposes imputing held-out entries with zeros and then minimizing the reconstruction error on this modified matrix.\n\nThis procedure has two major flaws. First, it corrupts the dataset. By replacing unknown values with $0$, it forces the NMF algorithm to learn factors that explain these artificial zeros, which biases the resulting $W$ and $H$. This is not a principled way to handle missing data. Second, similar to option B, it selects the rank $r$ based on minimizing a *training loss* (the loss on the zero-imputed matrix). As established, minimizing training loss leads to overfitting and is not a valid method for model selection. It fails to perform a true validation on held-out data.\n\n**Verdict on D**: **Incorrect**.\n\n### Evaluation of Option E\n\nThis option proposes a procedure analogous to Option A, but uses the generalized Kullback-Leibler (KL) divergence as the loss function and a more refined model selection rule.\n\n1.  **Data Partitioning, Model Fitting, Performance Evaluation**: The structure is identical to that of option A and is correct. It uses a mask to separate training and validation, minimizes loss on the training set, and evaluates on the validation set.\n2.  **Loss Function**: It uses the generalized KL divergence:\n    $$ D(X || WH) = \\sum_{i,j} \\left( X_{ij} \\log\\frac{X_{ij}}{[WH]_{ij}} - X_{ij} + [WH]_{ij} \\right) $$\n    As noted in the problem statement, this loss function is appropriate for count-like data and corresponds to assuming a Poisson distribution for the data, which is a common and suitable model for neural spike counts. This choice is therefore scientifically well-motivated for the application domain.\n3.  **Model Selection**: It suggests the \"one-standard-error rule\": select the smallest (most parsimonious) rank $r$ whose validation performance is not statistically significantly worse than the best-performing rank. This is a standard, statistically-grounded heuristic in model selection that provides a robust way to balance model performance and complexity, explicitly addressing the \"principled rule\" requirement.\n4.  **Final Model Training**: It correctly specifies refitting the model on the full matrix $X$ at the selected rank.\n\nThis option describes a complete, correct, and statistically sophisticated cross-validation protocol that is particularly well-suited for the problem's context.\n\n**Verdict on E**: **Correct**.\n\n### Conclusion\n\nBoth options A and E describe correct and principled mask-based cross-validation procedures for selecting the rank in NMF. Option A uses the standard squared error loss, while Option E uses the KL divergence, which is often more appropriate for the type of neuroscience data described. Option E also includes the \"one-standard-error rule,\" which is a well-regarded refinement for model selection. Since the problem asks to select all correct options, and both A and E detail valid, self-consistent, and scientifically sound procedures, both are correct.",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "NMF is not a one-size-fits-all tool; the choice of objective function should reflect the statistical properties of the data. This coding exercise allows you to empirically investigate how using a Poisson-based objective (KL-divergence) versus a Gaussian-based one (Frobenius norm) impacts the sparsity of the recovered neural activations . This practice will solidify your understanding of why tailoring the model to the data is critical for extracting meaningful features like sparse neural activity.",
            "id": "4182157",
            "problem": "You are given a data analysis task grounded in Non-negative Matrix Factorization (NMF) within the neuroscience setting, where neuronal spike train recordings are naturally modeled as count data. Your goal is to implement and empirically compare the sparsity of the activation matrix under two different likelihood models with an identical sparsity regularizer. The comparison must be done in a self-contained program that produces a single-line output aggregating the results for a specified test suite.\n\nFundamental base to use:\n- Non-negative Matrix Factorization (NMF) decomposes a non-negative data matrix $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$ into the product $W H$ where $W \\in \\mathbb{R}_{\\ge 0}^{n \\times r}$ and $H \\in \\mathbb{R}_{\\ge 0}^{r \\times m}$, with $r$ denoting the number of components.\n- Under a Gaussian noise model with independent, identically distributed noise of fixed variance, the negative log-likelihood reduces (up to constants) to the squared Frobenius reconstruction loss $||X - W H||_F^2$. Under a Poisson count model appropriate for spike counts, the negative log-likelihood is proportional to the generalized Kullback–Leibler divergence (KL), often written as $\\sum_{i,j}\\left[X_{ij} \\log\\left(\\frac{X_{ij}}{(W H)_{ij}}\\right) - X_{ij} + (W H)_{ij}\\right]$.\n- A sparsity-promoting penalty on the activation matrix $H$ can be implemented by adding the term $\\lambda \\, ||H||_1$ to the objective, with $\\lambda \\ge 0$ and $||H||_1 = \\sum_{i,j} |H_{ij}|$.\n\nTask specification:\n1. For each test case in the provided test suite, generate a synthetic non-negative dataset $X$ with dimensions $n \\times m$ and rank parameter $r$. For Poisson-like data, construct $X$ by sampling from independent Poisson distributions with mean $(W_{\\text{true}} H_{\\text{true}})$, where $W_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{n \\times r}$ and $H_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{r \\times m}$ are non-negative ground-truth factors with sparse temporal activations in $H_{\\text{true}}$. For Gaussian-like data, construct $X$ as $X = \\max\\{0, W_{\\text{true}} H_{\\text{true}} + \\epsilon\\}$ with additive mean-zero Gaussian noise, clipped to remain non-negative. Ensure a deterministic setup by using fixed random seeds in code.\n2. Implement two NMF estimators with identical non-negativity constraints and identical $\\lambda$ applied to the same $||H||_1$ sparsity regularizer:\n   - Estimator $\\mathcal{G}$: Minimize the Gaussian-based objective $J_{\\mathcal{G}}(W,H) = \\frac{1}{2} ||X - W H||_F^2 + \\lambda ||H||_1$.\n   - Estimator $\\mathcal{P}$: Minimize the Poisson-based objective $J_{\\mathcal{P}}(W,H) = D_{\\mathrm{KL}}(X \\| W H) + \\lambda ||H||_1$, where $D_{\\mathrm{KL}}(X \\| Y)$ denotes the generalized Kullback–Leibler divergence.\n   Both estimators must enforce $W \\ge 0$ and $H \\ge 0$ throughout optimization, and must use multiplicative-update style iterations derived from the stationarity conditions and non-negativity constraints without introducing negative entries. You must not rely on any external libraries beyond the specified runtime environment.\n3. Use identical initialization strategies for both estimators within each test case, with small positive random entries for $W$ and $H$. Use a fixed number of iterations. To avoid scale ambiguity, renormalize the columns of $W$ in each iteration to have unit $\\ell_1$ sum and rescale the corresponding rows of $H$ accordingly to keep $W H$ unchanged. This prevents artificial changes in sparsity purely due to scaling.\n4. Quantify sparsity of the estimated activation matrix $H$ using Hoyer's sparsity index applied to the vectorized $H$. For a non-negative vector $v \\in \\mathbb{R}_{\\ge 0}^N$, define the index as\n   $$ s(v) = \\frac{\\sqrt{N} - \\frac{||v||_1}{||v||_2}}{\\sqrt{N} - 1}, $$\n   with the conventions that $s(v) = 1$ if $v$ is the zero vector and that very small denominators must be handled safely in code. For a matrix $H$, apply this formula to its vectorization. Report $s(H_{\\mathcal{P}})$ for the Poisson-based estimator and $s(H_{\\mathcal{G}})$ for the Gaussian-based estimator.\n5. For each test case, compute the boolean result $b = \\left[s(H_{\\mathcal{P}}) \\ge s(H_{\\mathcal{G}})\\right]$, which indicates whether the Poisson-based estimator produces an activation matrix $H$ with greater or equal sparsity compared to the Gaussian-based estimator when using the same $\\lambda$.\n6. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3]$).\n\nTest suite:\n- Case $1$ (happy path, Poisson-like data): $n = 30$, $m = 200$, $r = 4$, iterations $= 200$, $\\lambda = 0.1$, Poisson-like data.\n- Case $2$ (boundary condition on regularization): $n = 30$, $m = 200$, $r = 4$, iterations $= 200$, $\\lambda = 0$, Poisson-like data.\n- Case $3$ (strong regularization, potential shrinkage): $n = 30$, $m = 200$, $r = 4$, iterations $= 200$, $\\lambda = 1.0$, Poisson-like data.\n- Case $4$ (different noise family): $n = 30$, $m = 200$, $r = 4$, iterations $= 200$, $\\lambda = 0.1$, Gaussian-like data with non-negativity enforced by clipping.\n\nScientific realism and explanation target:\n- Explain in your solution, starting from the maximum-likelihood principles for Gaussian and Poisson models and the non-negativity constraints, why the Poisson-based objective often encourages sparse activations in $H$ in spike-count data, especially when combined with identical $\\lambda$ in the $\\ell_1$ penalty. Avoid shortcut formulas in the problem statement; the solution must derive the multiplicative updates from first principles and the Karush–Kuhn–Tucker (KKT) conditions and connect them to observed sparsity differences.\n\nFinal output format:\n- Your program must print a single line in the exact format $[b_1,b_2,b_3,b_4]$, where each $b_i$ is a boolean corresponding to the above test cases in order.",
            "solution": "The problem requires the implementation and comparison of two Non-negative Matrix Factorization (NMF) estimators, one based on a Gaussian noise model and the other on a Poisson count model, to analyze their effect on the sparsity of the resulting activation matrix $H$. The comparison is performed on synthetic neuroscience-style data.\n\nThe core of NMF is to approximate a non-negative data matrix $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$ as a product of two lower-rank non-negative matrices, $W \\in \\mathbb{R}_{\\ge 0}^{n \\times r}$ and $H \\in \\mathbb{R}_{\\ge 0}^{r \\times m}$. This is formulated as a constrained optimization problem:\n$$ \\min_{W \\ge 0, H \\ge 0} J(W, H) $$\nwhere $J(W, H)$ is the objective function. The problem specifies two objectives, both including an $\\ell_1$ sparsity regularizer on the activation matrix $H$:\n1.  Gaussian-based objective: $J_{\\mathcal{G}}(W,H) = \\frac{1}{2} ||X - W H||_F^2 + \\lambda ||H||_1$\n2.  Poisson-based objective: $J_{\\mathcal{P}}(W,H) = D_{\\mathrm{KL}}(X \\| W H) + \\lambda ||H||_1$, where $D_{\\mathrm{KL}}(X \\| Y) = \\sum_{i,j}\\left[X_{ij} \\log\\left(\\frac{X_{ij}}{Y_{ij}}\\right) - X_{ij} + Y_{ij}\\right]$.\n\nTo solve this, we employ multiplicative update rules. These rules are derived to iteratively seek a stationary point that satisfies the Karush-Kuhn-Tucker (KKT) conditions for this non-negativity constrained problem. For a generic parameter matrix $\\Theta$, the KKT conditions include the complementary slackness condition $\\Theta \\odot \\nabla_{\\Theta} J = 0$, where $\\odot$ denotes element-wise multiplication. An update rule of the form $\\Theta \\leftarrow \\Theta \\odot \\frac{(\\nabla_\\Theta J)^-}{(\\nabla_\\Theta J)^+}$ is designed to converge towards such a point, where $\\nabla_\\Theta J = (\\nabla_\\Theta J)^+ - (\\nabla_\\Theta J)^-$ is the decomposition of the gradient into its positive and negative parts.\n\n### Derivation of Multiplicative Update Rules\n\n**1. Gaussian Model Objective ($J_{\\mathcal{G}}$)**\n\nThe objective is $J_{\\mathcal{G}}(W,H) = \\frac{1}{2} ||X - WH||_F^2 + \\lambda \\sum_{k,j} H_{kj}$.\n\n**Update for $H$**:\nThe gradient with respect to $H$ is:\n$$ \\nabla_H J_{\\mathcal{G}} = -W^T(X - WH) + \\lambda \\mathbf{1} = W^TWH - W^TX + \\lambda \\mathbf{1} $$\nwhere $\\mathbf{1}$ is a matrix of ones of the same size as $H$. We identify the positive and negative parts of the gradient:\n$$ (\\nabla_H J_{\\mathcal{G}})^+ = W^TWH + \\lambda \\mathbf{1} $$\n$$ (\\nabla_H J_{\\mathcal{G}})^- = W^TX $$\nThe multiplicative update for $H$ is thus:\n$$ H \\leftarrow H \\odot \\frac{(W^TX)}{(W^TWH + \\lambda \\mathbf{1})} $$\n\n**Update for $W$**:\nThe gradient with respect to $W$ (note that the regularizer does not depend on $W$) is:\n$$ \\nabla_W J_{\\mathcal{G}} = -(X - WH)H^T = WHH^T - XH^T $$\nThe positive and negative parts are:\n$$ (\\nabla_W J_{\\mathcal{G}})^+ = WHH^T $$\n$$ (\\nabla_W J_{\\mathcal{G}})^- = XH^T $$\nThe multiplicative update for $W$ is:\n$$ W \\leftarrow W \\odot \\frac{(XH^T)}{(WHH^T)} $$\n\n**2. Poisson Model Objective ($J_{\\mathcal{P}}$)**\n\nThe relevant part of the objective for differentiation is $J_{\\mathcal{P}}(W,H) \\propto \\sum_{i,j} \\left((WH)_{ij} - X_{ij}\\log(WH)_{ij}\\right) + \\lambda \\sum_{k,j} H_{kj}$.\n\n**Update for $H$**:\nThe gradient of the KL-divergence term wrt $H_{kj}$ is $\\sum_i W_{ik} \\left(1 - \\frac{X_{ij}}{(WH)_{ij}}\\right)$. The full gradient is:\n$$ \\frac{\\partial J_{\\mathcal{P}}}{\\partial H_{kj}} = \\sum_i W_{ik} - \\sum_i W_{ik} \\frac{X_{ij}}{(WH)_{ij}} + \\lambda $$\nIn matrix form, letting $\\mathbf{1}_{n \\times m}$ be a matrix of ones:\n$$ \\nabla_H J_{\\mathcal{P}} = W^T\\mathbf{1}_{n \\times m} - W^T(X \\oslash WH) + \\lambda \\mathbf{1} $$\nwhere $\\oslash$ denotes element-wise division. The positive and negative parts are:\n$$ (\\nabla_H J_{\\mathcal{P}})^+ = W^T\\mathbf{1}_{n \\times m} + \\lambda \\mathbf{1} $$\n$$ (\\nabla_H J_{\\mathcal{P}})^- = W^T(X \\oslash WH) $$\nThe multiplicative update for $H$ is:\n$$ H \\leftarrow H \\odot \\frac{W^T(X \\oslash WH)}{W^T\\mathbf{1}_{n \\times m} + \\lambda \\mathbf{1}} $$\n\n**Update for $W$**:\nSimilarly, the gradient with respect to $W$ is:\n$$ \\nabla_W J_{\\mathcal{P}} = \\mathbf{1}_{n \\times m}H^T - (X \\oslash WH)H^T $$\nThe positive and negative parts are:\n$$ (\\nabla_W J_{\\mathcal{P}})^+ = \\mathbf{1}_{n \\times m}H^T $$\n$$ (\\nabla_W J_{\\mathcal{P}})^- = (X \\oslash WH)H^T $$\nThe multiplicative update for $W$ is:\n$$ W \\leftarrow W \\odot \\frac{((X \\oslash WH)H^T)}{(\\mathbf{1}_{n \\times m}H^T)} $$\nTo avoid numerical instability, a small epsilon, $\\epsilon > 0$, is added to all denominators during implementation.\n\n### Normalization\nThe factorization $X \\approx WH$ has an inherent scaling ambiguity: for any scalar $s > 0$, $X \\approx (Ws)(H/s)$. This can interfere with the sparsity penalty, as shrinking $W$ and expanding $H$ would increase the penalty $\\lambda||H||_1$ without changing the reconstruction $WH$. To resolve this, after each update of $W$ and $H$, we normalize the columns of $W$ to have a unit $\\ell_1$-norm and apply the inverse scaling to the rows of $H$. For each component $k=1, \\dots, r$:\n1. Compute the column sum $s_k = \\sum_i W_{ik}$.\n2. Update the $k$-th column of $W$: $W_{:,k} \\leftarrow W_{:,k} / s_k$.\n3. Update the $k$-th row of $H$: $H_{k,:} \\leftarrow H_{k,:} \\cdot s_k$.\nThis ensures the product $WH$ remains invariant while fixing the scale of the basis vectors in $W$.\n\n### Scientific Basis for Sparsity Differences\n\nThe Poisson-based NMF is expected to yield sparser activation matrices $H$ than the Gaussian-based NMF, especially for count-like data, due to the fundamental properties of the KL-divergence objective.\n\n1.  **Handling of Zeroes**: Neuronal spike data is often sparse, meaning many entries in the data matrix $X$ are zero. Consider the update rule for $H$ in the Poisson model. The numerator contains the term $W^T(X \\oslash WH)$. If a column $j$ of the data matrix is all zero ($X_{:,j} = 0$), then the corresponding column in the numerator term also becomes zero. This forces the multiplicative factor for the $j$-th column of $H$ to be zero, driving $H_{:,j}$ to zero. The algorithm naturally learns that if there is no activity in a time bin, the activations of all components should be zero. The Gaussian update rule lacks this direct and powerful mechanism; its numerator $W^TX$ is much less likely to be exactly zero, so it does not enforce sparsity as strongly in response to zero-count data.\n\n2.  **Asymmetric Cost Function**: The Gaussian objective penalizes errors symmetrically: the cost for overestimating a data point ($ (WH)_{ij} > X_{ij} $) is the same as for underestimating it by the same amount. The KL-divergence is asymmetric. For a given $X_{ij} > 0$, the cost $(WH)_{ij} - X_{ij}\\log(WH)_{ij}$ grows linearly as $(WH)_{ij} \\to \\infty$ but goes to $+\\infty$ very steeply as $(WH)_{ij} \\to 0$. This strong penalty on underestimation forces the model to capture the non-zero counts, while the weaker penalty on overestimation is more permissive. Critically, to minimize the overall cost, the model is incentivized to set $(WH)_{ij}$ to very small values (approaching zero) wherever $X_{ij}$ contains small counts or noise, rather than fitting that noise. This behavior effectively \"prunes\" small, unnecessary activation values in $H$, promoting a sparser representation.\n\nIn summary, the Poisson model's likelihood function is intrinsically matched to the statistical properties of count data. This leads to update rules that naturally promote sparsity, a property that is merely supplemented, not created, by the explicit $\\ell_1$ regularizer. The Gaussian model, being based on an assumption of additive, symmetric noise, lacks this inherent sparsity-inducing structure.",
            "answer": "```python\nimport numpy as np\n\ndef hoyer_sparsity(v):\n    \"\"\"\n    Calculates the Hoyer sparsity index for a non-negative vector.\n    s(v) = (sqrt(N) - ||v||_1 / ||v||_2) / (sqrt(N) - 1)\n    \"\"\"\n    v_flat = v.flatten()\n    N = v_flat.size\n    if N <= 1:\n        return 1.0\n\n    norm1 = np.linalg.norm(v_flat, 1)\n    norm2 = np.linalg.norm(v_flat, 2)\n    \n    # Handle the case where the vector is all zeros\n    if norm2 < 1e-9:\n        return 1.0\n    \n    # Handle the case where sqrt(N) - 1 is zero\n    sqrt_N = np.sqrt(N)\n    if abs(sqrt_N - 1.0) < 1e-9:\n        return 1.0\n\n    sparsity = (sqrt_N - norm1 / norm2) / (sqrt_N - 1.0)\n    return sparsity\n\ndef generate_data(n, m, r, data_type, seed):\n    \"\"\"\n    Generates synthetic data matrix X.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Ground truth factors\n    W_true = rng.random((n, r))\n    \n    # Make H_true sparse (e.g., 80% sparsity)\n    H_true_dense = rng.random((r, m))\n    sparsity_mask = rng.random((r, m)) > 0.8\n    H_true = H_true_dense * sparsity_mask\n    \n    # Normalize W_true\n    W_true /= W_true.sum(axis=0, keepdims=True)\n\n    Lambda = W_true @ H_true\n    \n    if data_type == 'poisson':\n        X = rng.poisson(Lambda)\n    elif data_type == 'gaussian':\n        noise_std = 0.1 * np.std(Lambda)\n        noise = rng.normal(0, noise_std, size=(n, m))\n        X = np.maximum(0, Lambda + noise)\n    else:\n        raise ValueError(\"Invalid data_type specified.\")\n        \n    return X\n\ndef nmf(X, r, iterations, lambda_reg, model, init_seed):\n    \"\"\"\n    Performs Non-negative Matrix Factorization.\n    \"\"\"\n    n, m = X.shape\n    rng = np.random.default_rng(init_seed)\n    \n    # Initialize W and H with small positive random values\n    W = rng.random((n, r)) * 0.1 + 1e-4\n    H = rng.random((r, m)) * 0.1 + 1e-4\n    \n    eps = 1e-9\n\n    ones_nm = np.ones((n, m))\n\n    for _ in range(iterations):\n        if model == 'gaussian':\n            # Update H\n            H_num = W.T @ X\n            H_den = W.T @ W @ H + lambda_reg + eps\n            H *= H_num / H_den\n            \n            # Update W\n            W_num = X @ H.T\n            W_den = W @ H @ H.T + eps\n            W *= W_num / W_den\n            \n        elif model == 'poisson':\n            WH = W @ H + eps\n            # Update H\n            H_num = W.T @ (X / WH)\n            H_den = W.T @ ones_nm + lambda_reg + eps\n            H *= H_num / H_den\n            \n            # Update W\n            WH = W @ H + eps\n            W_num = (X / WH) @ H.T\n            W_den = ones_nm @ H.T + eps\n            W *= W_num / W_den\n        \n        # Normalize W columns and rescale H rows\n        w_col_sums = W.sum(axis=0)\n        W /= (w_col_sums + eps)\n        H *= w_col_sums.reshape(r, 1)\n\n    return W, H\n\ndef solve():\n    test_cases = [\n        # n, m, r, iterations, lambda, data_type\n        (30, 200, 4, 200, 0.1, 'poisson'),\n        (30, 200, 4, 200, 0.0, 'poisson'),\n        (30, 200, 4, 200, 1.0, 'poisson'),\n        (30, 200, 4, 200, 0.1, 'gaussian'),\n    ]\n\n    results = []\n    main_rng = np.random.default_rng(42)  # For reproducible sub-seeds\n\n    for i, (n, m, r, iterations, lambda_reg, data_type) in enumerate(test_cases):\n        data_seed = main_rng.integers(10000)\n        init_seed = main_rng.integers(10000)\n        \n        # Generate data\n        X = generate_data(n, m, r, data_type, seed=data_seed)\n        \n        # Run Gaussian-based NMF\n        _, H_g = nmf(X, r, iterations, lambda_reg, 'gaussian', init_seed=init_seed)\n        \n        # Run Poisson-based NMF\n        _, H_p = nmf(X, r, iterations, lambda_reg, 'poisson', init_seed=init_seed)\n        \n        # Compute sparsities\n        s_g = hoyer_sparsity(H_g)\n        s_p = hoyer_sparsity(H_p)\n        \n        # Compare and store result\n        results.append(s_p >= s_g)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}