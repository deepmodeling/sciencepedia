{
    "hands_on_practices": [
        {
            "introduction": "要掌握非负矩阵分解（NMF）的原理，首先需要理解其核心的优化算法。这个练习将带你手动执行一次乘性更新迭代，这是NMF中最常用的算法之一 。通过这个具体的计算过程，你将亲身体验因子矩阵 $W$ 和 $H$ 是如何在保持非负性的同时，逐步迭代以逼近原始数据矩阵 $X$ 的。",
            "id": "4182176",
            "problem": "一个实验室记录了 $N=3$ 个神经元在 $T=3$ 个时间区间内的尖峰计数，数据汇总在非负数据矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{3 \\times 3}$ 中。目标是执行非负矩阵分解 (NMF) 来近似 $X \\approx W H$，其中 $W \\in \\mathbb{R}_{\\ge 0}^{3 \\times 2}$ 编码了神经元在 $K=2$ 个潜在组合上的负载，而 $H \\in \\mathbb{R}_{\\ge 0}^{2 \\times 3}$ 编码了组合随时间的激活情况。假设目标函数为带有弗罗贝尼乌斯范数 (Frobenius norm) 和非负性约束的平方误差，并执行一次完整的乘法更新迭代，该迭代定义为：在给定 $H$ 的情况下更新 $W$，然后使用更新后的 $W$ 更新 $H$。此更新规则由所述目标函数导出，不含外部启发式方法。\n\n具体的矩阵如下：\n$$\nX = \\begin{bmatrix}\n2  4  6 \\\\\n2  4  6 \\\\\n2  4  6\n\\end{bmatrix},\\quad\nW^{(0)} = \\begin{bmatrix}\n1  1 \\\\\n1  1 \\\\\n1  1\n\\end{bmatrix},\\quad\nH^{(0)} = \\begin{bmatrix}\n1  2  3 \\\\\n1  2  3\n\\end{bmatrix}.\n$$\n\n从平方误差成本函数 $J(W,H) = \\tfrac{1}{2}\\|X - W H\\|_{F}^{2}$（约束条件为 $W \\ge 0$ 和 $H \\ge 0$）出发，推导出 $W$ 和 $H$ 的乘法更新规则，然后在给定的 $X$、$W^{(0)}$ 和 $H^{(0)}$ 上执行恰好一次迭代，以获得 $W^{(1)}$ 和 $H^{(1)}$。将最终答案表示为一个单行矩阵，其中按行主序（row-major order）列出 $W^{(1)}$ 的六个更新后条目，其后是按行主序排列的 $H^{(1)}$ 的六个更新后条目。如果数值需要四舍五入，请将答案保留到四位有效数字。",
            "solution": "所述问题是有效的。它在科学上基于非负矩阵分解（NMF）的原理，这是数据分析中的一种标准方法。该问题是适定的 (well-posed)，提供了所有必需的数据矩阵（$X$、$W^{(0)}$、$H^{(0)}$）、参数（$N=3$、$T=3$、$K=2$）、一个明确的目标函数（$J(W,H) = \\frac{1}{2}\\|X - W H\\|_{F}^{2}$），以及一个精确定义的任务（一次完整的乘法更新迭代）。数据和约束条件是自洽的，语言客观且无歧义。该问题可以被解决，并产生一个唯一的、有意义的答案。\n\n问题的核心是在 $W$ 和 $H$ 的元素均为非负的约束条件下，最小化成本函数 $J(W,H)$。\n$$\nJ(W,H) = \\frac{1}{2} \\|X - WH\\|_{F}^{2} = \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{T} \\left(X_{ij} - \\sum_{k=1}^{K} W_{ik} H_{kj}\\right)^2\n$$\n乘法更新规则是通过考虑 $J$ 相对于 $W$ 和 $H$ 元素的梯度来推导的。这些规则提供了一个遵循非负性约束的定点迭代。\n\n相对于 $W$ 的一个元素 $W_{ab}$ 的梯度是：\n$$\n\\frac{\\partial J}{\\partial W_{ab}} = \\sum_{j=1}^{T} \\left(X_{aj} - \\sum_{k=1}^{K} W_{ak} H_{kj}\\right) (-H_{bj}) = - \\sum_{j=1}^{T} (X - WH)_{aj} (H^T)_{jb} = -((X-WH)H^T)_{ab}\n$$\n以矩阵形式表示，梯度为 $\\nabla_W J = -(X-WH)H^T = WHH^T - XH^T$。$W$ 的乘法更新规则是：\n$$\nW_{ab} \\leftarrow W_{ab} \\frac{(XH^T)_{ab}}{(WHH^T)_{ab}}\n$$\n这可以使用逐元素（哈达玛）乘积 $\\odot$ 和除法紧凑地写为：\n$$\nW \\leftarrow W \\odot \\frac{XH^T}{WHH^T}\n$$\n类似地，相对于 $H$ 的一个元素 $H_{cd}$ 的梯度是：\n$$\n\\frac{\\partial J}{\\partial H_{cd}} = \\sum_{i=1}^{N} \\left(X_{id} - \\sum_{k=1}^{K} W_{ik} H_{kd}\\right) (-W_{ic}) = - \\sum_{i=1}^{N} (W^T)_{ci} (X - WH)_{id} = -(W^T(X-WH))_{cd}\n$$\n以矩阵形式表示，$\\nabla_H J = -W^T(X-WH) = W^TWH - W^TX$。$H$ 的乘法更新规则是：\n$$\nH_{cd} \\leftarrow H_{cd} \\frac{(W^TX)_{cd}}{(W^TWH)_{cd}}\n$$\n这可以紧凑地写为：\n$$\nH \\leftarrow H \\odot \\frac{W^TX}{W^TWH}\n$$\n问题指定了一次完整的迭代：首先更新 $W$ 以获得 $W^{(1)}$，然后使用 $W^{(1)}$ 更新 $H$ 以获得 $H^{(1)}$。\n\n给定的初始矩阵是：\n$$\nX = \\begin{bmatrix} 2  4  6 \\\\ 2  4  6 \\\\ 2  4  6 \\end{bmatrix},\\quad W^{(0)} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix},\\quad H^{(0)} = \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix}\n$$\n首先，我们观察到初始矩阵完美地重构了 $X$：\n$$\nW^{(0)} H^{(0)} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix} = \\begin{bmatrix} 1+1  2+2  3+3 \\\\ 1+1  2+2  3+3 \\\\ 1+1  2+2  3+3 \\end{bmatrix} = \\begin{bmatrix} 2  4  6 \\\\ 2  4  6 \\\\ 2  4  6 \\end{bmatrix} = X\n$$\n这意味着初始成本 $J(W^{(0)}, H^{(0)}) = 0$。由于 $J \\ge 0$，这是一个全局最小值，我们预期更新规则不会产生任何变化，即初始点是迭代算法的一个不动点。我们现在将通过显式计算来验证这一点。\n\n第1步：更新 $W$ 以求得 $W^{(1)}$。\n更新规则是 $W^{(1)} = W^{(0)} \\odot \\frac{X (H^{(0)})^T}{W^{(0)} H^{(0)} (H^{(0)})^T}$。\n我们计算更新因子的分子：\n$$\n(H^{(0)})^T = \\begin{bmatrix} 1  1 \\\\ 2  2 \\\\ 3  3 \\end{bmatrix}\n$$\n$$\nX (H^{(0)})^T = \\begin{bmatrix} 2  4  6 \\\\ 2  4  6 \\\\ 2  4  6 \\end{bmatrix} \\begin{bmatrix} 1  1 \\\\ 2  2 \\\\ 3  3 \\end{bmatrix} = \\begin{bmatrix} 2+8+18  2+8+18 \\\\ 2+8+18  2+8+18 \\\\ 2+8+18  2+8+18 \\end{bmatrix} = \\begin{bmatrix} 28  28 \\\\ 28  28 \\\\ 28  28 \\end{bmatrix}\n$$\n对于分母，我们利用 $W^{(0)}H^{(0)} = X$ 这一事实：\n$$\nW^{(0)} H^{(0)} (H^{(0)})^T = X (H^{(0)})^T = \\begin{bmatrix} 28  28 \\\\ 28  28 \\\\ 28  28 \\end{bmatrix}\n$$\n更新因子是分子与分母的逐元素相除：\n$$\n\\frac{X (H^{(0)})^T}{W^{(0)} H^{(0)} (H^{(0)})^T} = \\frac{\\begin{bmatrix} 28  28 \\\\ 28  28 \\\\ 28  28 \\end{bmatrix}}{\\begin{bmatrix} 28  28 \\\\ 28  28 \\\\ 28  28 \\end{bmatrix}} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix}\n$$\n现在，我们将更新应用于 $W^{(0)}$：\n$$\nW^{(1)} = W^{(0)} \\odot \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix} \\odot \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix}\n$$\n因此，$W^{(1)} = W^{(0)}$。\n\n第2步：使用新计算出的 $W^{(1)}$ 更新 $H$ 以求得 $H^{(1)}$。\n更新规则是 $H^{(1)} = H^{(0)} \\odot \\frac{(W^{(1)})^T X}{(W^{(1)})^T W^{(1)} H^{(0)}}$。\n因为 $W^{(1)} = W^{(0)}$，我们有 $(W^{(1)})^T = (W^{(0)})^T = \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix}$。\n我们计算更新因子的分子：\n$$\n(W^{(1)})^T X = \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix} \\begin{bmatrix} 2  4  6 \\\\ 2  4  6 \\\\ 2  4  6 \\end{bmatrix} = \\begin{bmatrix} 6  12  18 \\\\ 6  12  18 \\end{bmatrix}\n$$\n现在，我们计算分母：\n$$\n(W^{(1)})^T W^{(1)} = \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix} \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 3  3 \\\\ 3  3 \\end{bmatrix}\n$$\n$$\n(W^{(1)})^T W^{(1)} H^{(0)} = \\begin{bmatrix} 3  3 \\\\ 3  3 \\end{bmatrix} \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix} = \\begin{bmatrix} 3+3  6+6  9+9 \\\\ 3+3  6+6  9+9 \\end{bmatrix} = \\begin{bmatrix} 6  12  18 \\\\ 6  12  18 \\end{bmatrix}\n$$\n更新因子是：\n$$\n\\frac{(W^{(1)})^T X}{(W^{(1)})^T W^{(1)} H^{(0)}} = \\frac{\\begin{bmatrix} 6  12  18 \\\\ 6  12  18 \\end{bmatrix}}{\\begin{bmatrix} 6  12  18 \\\\ 6  12  18 \\end{bmatrix}} = \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix}\n$$\n最后，我们将更新应用于 $H^{(0)}$：\n$$\nH^{(1)} = H^{(0)} \\odot \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix} = \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix} \\odot \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix} = \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix}\n$$\n因此，$H^{(1)} = H^{(0)}$。\n\n经过一次完整的迭代后，矩阵保持不变：\n$$\nW^{(1)} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix}, \\quad H^{(1)} = \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix}\n$$\n最终答案要求按行主序排列 $W^{(1)}$ 的六个条目，其后是按行主序排列的 $H^{(1)}$ 的六个条目。\n$W^{(1)}$ 的条目是 $W_{11}=1$, $W_{12}=1$, $W_{21}=1$, $W_{22}=1$, $W_{31}=1$, $W_{32}=1$。\n$H^{(1)}$ 的条目是 $H_{11}=1$, $H_{12}=2$, $H_{13}=3$, $H_{21}=1$, $H_{22}=2$, $H_{23}=3$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1  1  1  1  1  1  2  3  1  2  3\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在应用NMF时，一个关键且具挑战性的决策是如何选择成分的数量（即因子分解的秩 $r$）。这个练习旨在引导你设计一个科学严谨的交叉验证方案，以解决这一模型选择问题 。通过在留出数据上评估模型的泛化能力，你将学会一种有原则的方法来确定最佳的秩 $r$，从而在模型的复杂度和拟合优度之间找到平衡。",
            "id": "4182160",
            "problem": "一个神经科学实验室记录了一个非负数据矩阵 $X \\in \\mathbb{R}_{+}^{n \\times t}$，其中 $n$ 表示神经元的数量，$t$ 表示时间窗的数量，$X_{ij}$ 代表神经元 $i$ 在时间窗 $j$ 内的活动水平。为了识别潜在的神经元集合及其激活模式，该实验室打算将 $X$ 分解为两个非负矩阵 $W \\in \\mathbb{R}_{+}^{n \\times r}$ 和 $H \\in \\mathbb{R}_{+}^{r \\times t}$，使得 $X \\approx WH$，其中 $r$ 是分解秩。分解的选择应在近似质量和泛化能力之间取得平衡，避免过拟合。\n\n从非负矩阵分解 (NMF) 的核心定义出发，该实验室希望推导出一个科学上合理的交叉验证 (CV) 方案，该方案使用逐元素掩码来留出 $X$ 的一部分元素，并通过优化留出数据上的重构误差来选择秩 $r$。该设计必须遵守非负性约束，避免将留出数据的信息泄露到训练过程中，并提供一个基于留出数据性能来选择 $r$ 的原则性规则。考虑到对于平方误差重构，限制在观测元素上的训练目标可以使用一个二元掩码 $M \\in \\{0,1\\}^{n \\times t}$ 来表示，而留出误差则在其补集 $\\bar{M} = \\mathbf{1} - M$ 上计算。或者，对于计数类数据，可以使用适用于泊松观测的散度。\n\n哪个选项最正确地描述了一个基于掩码的CV过程，该过程与基本原则一致，并能产生一个选择 $r$ 的原则性规则？\n\nA. 随机抽取一个二元掩码 $M \\in \\{0,1\\}^{n \\times t}$，保留固定比例 $p \\in (0,1)$ 的元素作为训练数据 ($M_{ij} = 1$)，并将其余元素留作验证数据 ($\\bar{M}_{ij} = 1 - M_{ij}$)。对于一个候选秩 $r$，通过在 $W \\ge 0$ 和 $H \\ge 0$ 的约束下最小化带掩码的平方重构损失来拟合NMF，例如，\n$$\n\\min_{W \\ge 0,\\, H \\ge 0}\\;\\; \\sum_{i=1}^{n} \\sum_{j=1}^{t} M_{ij} \\bigl(X_{ij} - [WH]_{ij}\\bigr)^{2},\n$$\n并计算验证误差\n$$\nE_{\\mathrm{val}}(r) = \\frac{1}{\\|\\bar{M}\\|_{1}} \\sum_{i=1}^{n} \\sum_{j=1}^{t} \\bar{M}_{ij} \\bigl(X_{ij} - [W H]_{ij}\\bigr)^{2}.\n$$\n在 $K$ 个独立的掩码和 $S$ 次随机初始化上重复此过程，对所有掩码和种子取 $E_{\\mathrm{val}}(r)$ 的平均值，并选择使平均验证误差最小的最小秩 $r$。最后，在选定的秩 $r$ 下，对完整矩阵 $X$ 重新拟合NMF，以获得最终报告的 $W$ 和 $H$。\n\nB. 对于每个候选秩 $r$，在完整矩阵 $X$ 上拟合NMF，不使用掩码，通过在 $W \\ge 0$ 和 $H \\ge 0$ 约束下最小化所有元素的平方重构损失，并选择产生最小训练重构误差的秩 $r$。报告相应的 $W$ 和 $H$。\n\nC. 将 $X$ 的列划分为训练集和验证集。对于每个候选秩 $r$，仅在训练列上拟合NMF以获得 $W_{\\mathrm{train}}$ 和 $H_{\\mathrm{train}}$，然后通过将 $W_{\\mathrm{train}} H_{\\mathrm{train}}$ 与留出列进行比较来计算留出列上的验证误差。选择验证误差最小的秩 $r$，并报告 $W_{\\mathrm{train}}$ 和 $H_{\\mathrm{train}}$。\n\nD. 使用一个二元掩码 $M$ 来指定留出的元素，但在训练期间将所有留出的元素插补为零。通过最小化插补零后的全数据平方重构损失来拟合NMF，并选择最小化此插补增强训练损失的秩 $r$。报告相应的 $W$ 和 $H$。\n\nE. 如选项A中一样，随机抽取一个逐元素的掩码 $M$，但对于计数类神经数据，使用适用于泊松观测的广义Kullback–Leibler (KL) 散度。对于一个候选秩 $r$，通过最小化以下式子来拟合NMF：\n$$\n\\min_{W \\ge 0,\\, H \\ge 0}\\;\\; \\sum_{i=1}^{n} \\sum_{j=1}^{t} M_{ij} \\left( X_{ij} \\log\\frac{X_{ij}}{[WH]_{ij}} - X_{ij} + [WH]_{ij} \\right),\n$$\n并计算留出散度\n$$\nD_{\\mathrm{val}}(r) = \\frac{1}{\\|\\bar{M}\\|_{1}} \\sum_{i=1}^{n} \\sum_{j=1}^{t} \\bar{M}_{ij} \\left( X_{ij} \\log\\frac{X_{ij}}{[WH]_{ij}} - X_{ij} + [WH]_{ij} \\right).\n$$\n在 $K$ 个掩码和 $S$ 个种子上重复此过程，并选择其平均 $D_{\\mathrm{val}}(r)$ 在所有秩的最小值的一个标准误范围内的最小秩 $r$（单标准误规则）。最后，在选定的秩 $r$ 下，对完整矩阵 $X$ 重新拟合NMF。\n\n选择所有正确的选项。",
            "solution": "问题陈述提出了一个在计算神经科学和机器学习中有效且标准的问题：如何为数据矩阵 $X \\in \\mathbb{R}_{+}^{n \\times t}$ 的非负矩阵分解 (NMF) 选择秩 $r$ 以避免过拟合。所提出的方法是一种基于掩码的交叉验证 (CV) 程序。该问题具有科学依据，提法明确，并包含足够的信息来评估所提出的方法。\n\n一个有效的模型选择（这里是选择秩 $r$）交叉验证程序的核心原则是：\n1.  **数据划分**：必须将可用数据划分为训练集（用于学习模型参数 $W$ 和 $H$）和不相交的验证集（用于评估模型的泛化性能）。验证集的信息绝不能在训练期间使用。\n2.  **模型拟合**：对于每个候选的模型复杂度（每个秩 $r$），通过仅在训练数据上最小化损失函数来估计模型参数。对于NMF，这涉及找到 $W$ 和 $H$ 以最小化训练数据元素的重构误差，同时满足非负性约束。\n3.  **性能评估**：然后使用拟合好的模型来预测留出验证数据的值，并计算该集合上的预测误差。该验证误差可作为模型泛化误差的估计。\n4.  **模型选择**：在不同的秩 $r$ 之间比较验证误差。选择产生最佳验证性能（例如，最小误差）的秩。可以使用“单标准误规则”之类的启发式方法来偏好更简单的模型。\n5.  **最终模型训练**：一旦选定最优秩 $r^*$，标准做法是使用秩 $r^*$ 在整个数据集 $X$ 上重新训练模型。这最后一步利用所有可用数据，为所选模型复杂度生成最佳的参数估计。\n\n问题描述了一种基于掩码的方法，其中二元掩码 $M \\in \\{0,1\\}^{n \\times t}$ 定义了训练集（$M_{ij}=1$ 的元素）和验证集（$M_{ij}=0$ 的元素）。我们现在将根据这些原则评估每个选项。\n\n### 选项A的评估\n\n该选项提出了一种使用平方误差（弗罗贝尼乌斯范数）损失函数的基于掩码的CV程序。\n\n1.  **数据划分**：它正确地使用一个二元掩码 $M$ 将训练元素 ($M_{ij}=1$) 与验证元素 ($\\bar{M}_{ij}=1$) 分开。\n2.  **模型拟合**：它正确地指出，对于给定的秩 $r$，通过仅在训练元素上最小化损失函数来找到模型参数 $W$ 和 $H$：\n    $$ \\min_{W \\ge 0,\\, H \\ge 0}\\;\\; \\sum_{i=1}^{n} \\sum_{j=1}^{t} M_{ij} \\bigl(X_{ij} - [WH]_{ij}\\bigr)^{2} $$\n    这恰当地防止了留出数据的信息影响训练过程。\n3.  **性能评估**：它正确地将验证误差计算为留出元素上的均方误差：\n    $$ E_{\\mathrm{val}}(r) = \\frac{1}{\\|\\bar{M}\\|_{1}} \\sum_{i=1}^{n} \\sum_{j=1}^{t} \\bar{M}_{ij} \\bigl(X_{ij} - [W H]_{ij}\\bigr)^{2} $$\n    用 $\\|\\bar{M}\\|_1$（留出元素的数量）进行归一化是合适的。\n4.  **模型选择**：它建议选择使平均验证误差最小的最小秩 $r$，该平均值是在多个掩码 ($K$) 和随机初始化 ($S$) 上计算的。这是一个基于泛化性能选择 $r$ 的原则性规则，偏好最小的秩体现了简约性原则。\n5.  **最终模型训练**：它正确地以在选定秩下对完整矩阵 $X$ 重新拟合模型作为结尾。\n\n此程序遵循了健全的交叉验证方案的所有原则。使用平方误差是NMF的标准选择。\n\n**对A的裁定**：**正确**。\n\n### 选项B的评估\n\n该选项建议在完整矩阵上拟合NMF，并根据训练误差选择秩。\n\n对于NMF，训练重构误差是秩 $r$ 的一个非增函数。具有更高秩的模型拥有更多自由度，几乎总能在其训练数据上实现更低（或相等）的重构误差。因此，选择产生最小训练误差的秩 $r$ 的规则将总是选择最大的候选秩 $r$。这会导致选择最复杂的模型，而该模型极有可能对数据集中的特定噪声过拟合。这种方法完全未能估计或控制泛化误差，而这正是交叉验证的主要目的。\n\n**对B的裁定**：**不正确**。\n\n### 选项C的评估\n\n该选项建议划分 $X$ 的列（时间窗）。这是一种有效的块交叉验证形式。然而，该过程的描述存在缺陷。\n\n在训练列上拟合NMF得到 $W_{\\mathrm{train}}$ 和 $H_{\\mathrm{train}}$ 后，该选项建议通过比较留出列和 $W_{\\mathrm{train}} H_{\\mathrm{train}}$ 来计算验证误差。这是不正确的。矩阵 $H_{\\mathrm{train}}$ 包含的是*训练*列的激活模式，而不是*验证*列的。正确的程序应该是固定 $W_{\\mathrm{train}}$，然后通过最小化 $\\|X_{\\mathrm{val}} - W_{\\mathrm{train}} H_{\\mathrm{val}}\\|^2$ 来求解验证列的激活值 $H_{\\mathrm{val}}$。验证误差是这个新优化过程的结果。该选项忽略了这一关键步骤。\n\n此外，它建议报告 $W_{\\mathrm{train}}$ 和 $H_{\\mathrm{train}}$ 作为最终结果，而它们仅从数据的子集中得出。标准且更好的做法是使用选定的秩在完整数据集上重新拟合模型，如选项A和E所示。\n\n**对C的裁定**：**不正确**。\n\n### 选项D的评估\n\n该选项建议用零插补留出的元素，然后最小化这个修改后矩阵的重构误差。\n\n这个程序有两个主要缺陷。首先，它破坏了数据集。通过用 $0$ 替换未知值，它迫使NMF算法学习解释这些人造零的因子，这会使得到的 $W$ 和 $H$ 产生偏差。这不是处理缺失数据的原则性方法。其次，与选项B类似，它根据最小化一个*训练损失*（在零插补矩阵上的损失）来选择秩 $r$。如前所述，最小化训练损失会导致过拟合，不是一种有效的模型选择方法。它未能对留出数据进行真正的验证。\n\n**对D的裁定**：**不正确**。\n\n### 选项E的评估\n\n该选项提出了一个与选项A类似的程序，但使用了广义Kullback-Leibler (KL) 散度作为损失函数和一个更精细的模型选择规则。\n\n1.  **数据划分、模型拟合、性能评估**：其结构与选项A相同且正确。它使用掩码分离训练和验证数据，在训练集上最小化损失，并在验证集上进行评估。\n2.  **损失函数**：它使用广义KL散度：\n    $$ D(X || WH) = \\sum_{i,j} \\left( X_{ij} \\log\\frac{X_{ij}}{[WH]_{ij}} - X_{ij} + [WH]_{ij} \\right) $$\n    正如问题陈述中所指出的，这个损失函数适用于计数类数据，并对应于假设数据服从泊松分布，这对于神经脉冲计数来说是一个常见且合适的模型。因此，这一选择在科学上对于该应用领域是很有根据的。\n3.  **模型选择**：它建议使用“单标准误规则”：选择其验证性能与表现最佳的秩相比没有统计学显著差异的最小（最简约）的秩 $r$。这是模型选择中一个标准的、有统计学依据的启发式方法，它提供了一种稳健的方式来平衡模型性能和复杂性，明确满足了“原则性规则”的要求。\n4.  **最终模型训练**：它正确地指明在选定的秩下对完整矩阵 $X$ 重新拟合模型。\n\n该选项描述了一个完整、正确且在统计上更成熟的交叉验证方案，特别适合于问题所处的背景。\n\n**对E的裁定**：**正确**。\n\n### 结论\n\n选项A和E都描述了正确且有原则的基于掩码的交叉验证程序，用于在NMF中选择秩。选项A使用标准的平方误差损失，而选项E使用KL散度，后者通常更适用于所描述的神经科学数据类型。选项E还包含了“单标准误规则”，这是模型选择中一个备受推崇的改进方法。由于问题要求选择所有正确的选项，并且A和E都详细说明了有效、自洽且科学上合理的程序，因此它们都是正确的。",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "NMF的强大之处在于其灵活性，我们可以根据数据的统计特性选择不同的目标函数。本练习将通过一个编程实践，让你直观地比较两种常用目标函数——弗罗贝尼乌斯范数（适用于高斯噪声）和KL散度（适用于泊松计数的神经脉冲数据）——对结果的影响 。你将通过实现和比较，深刻理解为什么基于泊松模型的NMF在分析神经科学数据时，能够更自然地得到稀疏的激活模式。",
            "id": "4182157",
            "problem": "您将执行一项基于非负矩阵分解 (NMF) 的数据分析任务，该任务在神经科学背景下展开，其中神经元脉冲序列记录被自然地建模为计数数据。您的目标是实现并凭经验比较在两个不同似然模型下，使用相同稀疏性正则化器时激活矩阵的稀疏性。此比较必须在一个独立的程序中完成，该程序为指定的测试套件生成单行输出，汇总所有结果。\n\n使用的基本原理：\n- 非负矩阵分解 (NMF) 将一个非负数据矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$ 分解为乘积 $W H$，其中 $W \\in \\mathbb{R}_{\\ge 0}^{n \\times r}$ 且 $H \\in \\mathbb{R}_{\\ge 0}^{r \\times m}$，$r$ 表示组分的数量。\n- 在高斯噪声模型下，假设噪声是独立同分布且方差固定，负对数似然（在不计常数的情况下）简化为弗罗贝尼乌斯范数平方的重构损失 $||X - W H||_F^2$。在适用于脉冲计数的泊松计数模型下，负对数似然与广义库尔贝克-莱布勒散度 (KL) 成正比，通常写作 $\\sum_{i,j}\\left[X_{ij} \\log\\left(\\frac{X_{ij}}{(W H)_{ij}}\\right) - X_{ij} + (W H)_{ij}\\right]$。\n- 可以通过向目标函数添加项 $\\lambda \\, ||H||_1$ 来对激活矩阵 $H$ 施加促进稀疏性的惩罚，其中 $\\lambda \\ge 0$ 且 $||H||_1 = \\sum_{i,j} |H_{ij}|$。\n\n任务规范：\n1. 对于所提供测试套件中的每个测试用例，生成一个维度为 $n \\times m$、秩参数为 $r$ 的合成非负数据集 $X$。对于类泊松数据，通过从独立的泊松分布中采样来构建 $X$，其均值为 $(W_{\\text{true}} H_{\\text{true}})$，其中 $W_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{n \\times r}$ 和 $H_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{r \\times m}$ 是非负的真实因子，且 $H_{\\text{true}}$ 中具有稀疏的时间激活。对于类高斯数据，将 $X$ 构建为 $X = \\max\\{0, W_{\\text{true}} H_{\\text{true}} + \\epsilon\\}$，其中包含附加的零均值高斯噪声，并进行裁剪以保持非负性。通过在代码中使用固定的随机种子来确保确定性的设置。\n2. 实现两个 NMF 估计器，它们具有相同的非负性约束和应用于相同 $||H||_1$ 稀疏性正则化器的相同 $\\lambda$：\n   - 估计器 $\\mathcal{G}$：最小化基于高斯模型的目标函数 $J_{\\mathcal{G}}(W,H) = \\frac{1}{2} ||X - W H||_F^2 + \\lambda ||H||_1$。\n   - 估计器 $\\mathcal{P}$：最小化基于泊松模型的目标函数 $J_{\\mathcal{P}}(W,H) = D_{\\mathrm{KL}}(X \\| W H) + \\lambda ||H||_1$，其中 $D_{\\mathrm{KL}}(X \\| Y)$ 表示广义库尔贝克-莱布勒散度。\n   两个估计器在整个优化过程中都必须强制 $W \\ge 0$ 和 $H \\ge 0$，并且必须使用从平稳性条件和非负性约束推导出的乘性更新风格的迭代，而不引入负值项。除了指定的运行时环境外，您不得依赖任何外部库。\n3. 在每个测试用例中，对两个估计器使用相同的初始化策略，为 $W$ 和 $H$ 使用小的正随机项。使用固定的迭代次数。为避免尺度模糊性，在每次迭代中重新归一化 $W$ 的列，使其 $\\ell_1$ 和为单位1，并相应地重新缩放 $H$ 的对应行以保持 $W H$ 不变。这可以防止纯粹由于缩放而引起稀疏性的人为变化。\n4. 使用应用于向量化 $H$ 的 Hoyer 稀疏性指数来量化估计激活矩阵 $H$ 的稀疏性。对于一个非负向量 $v \\in \\mathbb{R}_{\\ge 0}^N$，该指数定义为\n   $$ s(v) = \\frac{\\sqrt{N} - \\frac{||v||_1}{||v||_2}}{\\sqrt{N} - 1}, $$\n   并约定，如果 $v$ 是零向量，则 $s(v) = 1$，且在代码中必须安全地处理非常小的分母。对于矩阵 $H$，将其向量化后应用此公式。报告基于泊松模型的估计器得到的 $s(H_{\\mathcal{P}})$ 和基于高斯模型的估计器得到的 $s(H_{\\mathcal{G}})$。\n5. 对于每个测试用例，计算布尔结果 $b = \\left[s(H_{\\mathcal{P}}) \\ge s(H_{\\mathcal{G}})\\right]$，该结果表示在使用相同的 $\\lambda$ 时，基于泊松模型的估计器产生的激活矩阵 $H$ 是否比基于高斯模型的估计器具有更高或相等的稀疏性。\n6. 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，$[result_1,result_2,result_3]$）。\n\n测试套件：\n- 用例 1（理想情况，类泊松数据）：$n = 30$, $m = 200$, $r = 4$, 迭代次数 $= 200$, $\\lambda = 0.1$, 类泊松数据。\n- 用例 2（正则化的边界条件）：$n = 30$, $m = 200$, $r = 4$, 迭代次数 $= 200$, $\\lambda = 0$, 类泊松数据。\n- 用例 3（强正则化，可能导致收缩）：$n = 30$, $m = 200$, $r = 4$, 迭代次数 $= 200$, $\\lambda = 1.0$, 类泊松数据。\n- 用例 4（不同的噪声族）：$n = 30$, $m = 200$, $r = 4$, 迭代次数 $= 200$, $\\lambda = 0.1$, 类高斯数据，通过裁剪强制非负性。\n\n科学现实主义与解释目标：\n- 在您的解决方案中，从高斯模型和泊松模型的最大似然原理以及非负性约束出发，解释为什么基于泊松模型的目标函数通常能在脉冲计数数据中鼓励 $H$ 的稀疏激活，特别是当与 $\\ell_1$ 惩罚中相同的 $\\lambda$ 结合使用时。避免在问题陈述中使用简化的公式；解决方案必须从第一性原理和卡罗需-库恩-塔克 (KKT) 条件推导出乘性更新法则，并将其与观察到的稀疏性差异联系起来。\n\n最终输出格式：\n- 您的程序必须以 $[b_1,b_2,b_3,b_4]$ 的确切格式打印单行，其中每个 $b_i$ 是一个布尔值，按顺序对应上述测试用例。",
            "solution": "该问题要求实现并比较两种非负矩阵分解 (NMF) 估计器，一种基于高斯噪声模型，另一种基于泊松计数模型，以分析它们对所得激活矩阵 $H$ 稀疏性的影响。该比较在合成的神经科学风格数据上进行。\n\nNMF 的核心是将一个非负数据矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$ 近似为两个低秩非负矩阵的乘积，$W \\in \\mathbb{R}_{\\ge 0}^{n \\times r}$ 和 $H \\in \\mathbb{R}_{\\ge 0}^{r \\times m}$。这被形式化为一个约束优化问题：\n$$ \\min_{W \\ge 0, H \\ge 0} J(W, H) $$\n其中 $J(W, H)$ 是目标函数。问题指定了两个目标函数，两者都包含对激活矩阵 $H$ 的 $\\ell_1$ 稀疏性正则化器：\n1.  基于高斯模型的目标函数：$J_{\\mathcal{G}}(W,H) = \\frac{1}{2} ||X - W H||_F^2 + \\lambda ||H||_1$\n2.  基于泊松模型的目标函数：$J_{\\mathcal{P}}(W,H) = D_{\\mathrm{KL}}(X \\| W H) + \\lambda ||H||_1$，其中 $D_{\\mathrm{KL}}(X \\| Y) = \\sum_{i,j}\\left[X_{ij} \\log\\left(\\frac{X_{ij}}{Y_{ij}}\\right) - X_{ij} + Y_{ij}\\right]$。\n\n为了解决这个问题，我们采用乘性更新法则。这些法则是为了迭代地寻找满足此非负约束问题的卡罗需-库恩-塔克 (KKT) 条件的平稳点而推导出来的。对于一个通用参数矩阵 $\\Theta$，KKT 条件包括互补松弛条件 $\\Theta \\odot \\nabla_{\\Theta} J = 0$，其中 $\\odot$ 表示逐元素相乘。形如 $\\Theta \\leftarrow \\Theta \\odot \\frac{(\\nabla_\\Theta J)^-}{(\\nabla_\\Theta J)^+}$ 的更新法则被设计用来收敛到这样一个点，其中 $\\nabla_\\Theta J = (\\nabla_\\Theta J)^+ - (\\nabla_\\Theta J)^-$ 是梯度分解为其正部和负部的形式。\n\n### 乘性更新法则的推导\n\n**1. 高斯模型目标 ($J_{\\mathcal{G}}$)**\n\n目标函数为 $J_{\\mathcal{G}}(W,H) = \\frac{1}{2} ||X - WH||_F^2 + \\lambda \\sum_{k,j} H_{kj}$。\n\n**$H$ 的更新**：\n关于 $H$ 的梯度为：\n$$ \\nabla_H J_{\\mathcal{G}} = -W^T(X - WH) + \\lambda \\mathbf{1} = W^TWH - W^TX + \\lambda \\mathbf{1} $$\n其中 $\\mathbf{1}$ 是一个与 $H$ 大小相同的全一矩阵。我们确定梯度的正部和负部：\n$$ (\\nabla_H J_{\\mathcal{G}})^+ = W^TWH + \\lambda \\mathbf{1} $$\n$$ (\\nabla_H J_{\\mathcal{G}})^- = W^TX $$\n因此，$H$ 的乘性更新法则为：\n$$ H \\leftarrow H \\odot \\frac{(W^TX)}{(W^TWH + \\lambda \\mathbf{1})} $$\n\n**$W$ 的更新**：\n关于 $W$ 的梯度（请注意正则化器不依赖于 $W$）为：\n$$ \\nabla_W J_{\\mathcal{G}} = -(X - WH)H^T = WHH^T - XH^T $$\n正部和负部分别为：\n$$ (\\nabla_W J_{\\mathcal{G}})^+ = WHH^T $$\n$$ (\\nabla_W J_{\\mathcal{G}})^- = XH^T $$\n$W$ 的乘性更新法则为：\n$$ W \\leftarrow W \\odot \\frac{(XH^T)}{(WHH^T)} $$\n\n**2. 泊松模型目标 ($J_{\\mathcal{P}}$)**\n\n目标函数中与微分相关的部分是 $J_{\\mathcal{P}}(W,H) \\propto \\sum_{i,j} \\left((WH)_{ij} - X_{ij}\\log(WH)_{ij}\\right) + \\lambda \\sum_{k,j} H_{kj}$。\n\n**$H$ 的更新**：\nKL 散度项关于 $H_{kj}$ 的梯度是 $\\sum_i W_{ik} \\left(1 - \\frac{X_{ij}}{(WH)_{ij}}\\right)$。完整的梯度为：\n$$ \\frac{\\partial J_{\\mathcal{P}}}{\\partial H_{kj}} = \\sum_i W_{ik} - \\sum_i W_{ik} \\frac{X_{ij}}{(WH)_{ij}} + \\lambda $$\n在矩阵形式中，令 $\\mathbf{1}_{n \\times m}$ 为全一矩阵：\n$$ \\nabla_H J_{\\mathcal{P}} = W^T\\mathbf{1}_{n \\times m} - W^T(X \\oslash WH) + \\lambda \\mathbf{1} $$\n其中 $\\oslash$ 表示逐元素相除。正部和负部分别为：\n$$ (\\nabla_H J_{\\mathcal{P}})^+ = W^T\\mathbf{1}_{n \\times m} + \\lambda \\mathbf{1} $$\n$$ (\\nabla_H J_{\\mathcal{P}})^- = W^T(X \\oslash WH) $$\n$H$ 的乘性更新法则为：\n$$ H \\leftarrow H \\odot \\frac{W^T(X \\oslash WH)}{W^T\\mathbf{1}_{n \\times m} + \\lambda \\mathbf{1}} $$\n\n**$W$ 的更新**：\n类似地，关于 $W$ 的梯度为：\n$$ \\nabla_W J_{\\mathcal{P}} = \\mathbf{1}_{n \\times m}H^T - (X \\oslash WH)H^T $$\n正部和负部分别为：\n$$ (\\nabla_W J_{\\mathcal{P}})^+ = \\mathbf{1}_{n \\times m}H^T $$\n$$ (\\nabla_W J_{\\mathcal{P}})^- = (X \\oslash WH)H^T $$\n$W$ 的乘性更新法则为：\n$$ W \\leftarrow W \\odot \\frac{((X \\oslash WH)H^T)}{(\\mathbf{1}_{n \\times m}H^T)} $$\n为了避免数值不稳定，在实现过程中会向所有分母添加一个小的 epsilon 值，$\\epsilon  0$。\n\n### 归一化\n分解 $X \\approx WH$ 具有固有的尺度模糊性：对于任何标量 $s  0$，都有 $X \\approx (Ws)(H/s)$。这会干扰稀疏性惩罚，因为缩小 $W$ 并扩大 $H$ 会增加惩罚项 $\\lambda||H||_1$，而重构结果 $WH$ 保持不变。为了解决这个问题，在每次更新 $W$ 和 $H$ 之后，我们将 $W$ 的列归一化为单位 $\\ell_1$-范数，并对 $H$ 的行应用逆向缩放。对每个组分 $k=1, \\dots, r$：\n1. 计算列和 $s_k = \\sum_i W_{ik}$。\n2. 更新 $W$ 的第 $k$ 列：$W_{:,k} \\leftarrow W_{:,k} / s_k$。\n3. 更新 $H$ 的第 $k$ 行：$H_{k,:} \\leftarrow H_{k,:} \\cdot s_k$。\n这确保了乘积 $WH$ 保持不变，同时固定了 $W$ 中基向量的尺度。\n\n### 稀疏性差异的科学依据\n\n由于 KL 散度目标函数的基本性质，基于泊松模型的 NMF 预计会比基于高斯模型的 NMF 产生更稀疏的激活矩阵 $H$，尤其对于计数类型的数据。\n\n1.  **对零值的处理**：神经元脉冲数据通常是稀疏的，意味着数据矩阵 $X$ 中的许多项为零。考虑泊松模型中 $H$ 的更新法则。分子包含项 $W^T(X \\oslash WH)$。如果数据矩阵的第 $j$ 列全为零（$X_{:,j} = 0$），那么分子项中对应的列也会变为零。这会迫使 $H$ 第 $j$ 列的乘性因子为零，从而将 $H_{:,j}$ 推向零。算法自然地学习到，如果一个时间窗内没有活动，那么所有组分的激活都应为零。高斯更新法则缺乏这种直接而强大的机制；其分子 $W^TX$ 几乎不可能恰好为零，因此它在响应零计数数据时不会如此强力地强制稀疏性。\n\n2.  **非对称成本函数**：高斯目标函数对误差的惩罚是对称的：高估一个数据点（$(WH)_{ij}  X_{ij}$）的成本与低估相同量的成本相同。而 KL 散度是非对称的。对于给定的 $X_{ij}  0$，成本 $(WH)_{ij} - X_{ij}\\log(WH)_{ij}$ 随着 $(WH)_{ij} \\to \\infty$ 线性增长，但随着 $(WH)_{ij} \\to 0$ 急剧趋向于 $+\\infty$。这种对低估的强力惩罚迫使模型捕捉非零计数，而对高估的较弱惩罚则更具宽容性。关键是，为了最小化总成本，模型被激励在 $X_{ij}$ 包含小计数或噪声的地方将 $(WH)_{ij}$ 设置为非常小的值（接近零），而不是去拟合这些噪声。这种行为有效地“修剪”了 $H$ 中微小、不必要的激活值，从而促进了更稀疏的表示。\n\n总而言之，泊松模型的似然函数本质上与计数数据的统计特性相匹配。这导致其更新法则能够自然地促进稀疏性，而显式的 $\\ell_1$ 正则化器只是补充而非创造了这一特性。高斯模型基于附加的、对称的噪声假设，因此缺乏这种内在的稀疏性诱导结构。",
            "answer": "```python\nimport numpy as np\n\ndef hoyer_sparsity(v):\n    \"\"\"\n    Calculates the Hoyer sparsity index for a non-negative vector.\n    s(v) = (sqrt(N) - ||v||_1 / ||v||_2) / (sqrt(N) - 1)\n    \"\"\"\n    v_flat = v.flatten()\n    N = v_flat.size\n    if N == 1:\n        return 1.0\n\n    norm1 = np.linalg.norm(v_flat, 1)\n    norm2 = np.linalg.norm(v_flat, 2)\n    \n    # Handle the case where the vector is all zeros\n    if norm2  1e-9:\n        return 1.0\n    \n    # Handle the case where sqrt(N) - 1 is zero\n    sqrt_N = np.sqrt(N)\n    if abs(sqrt_N - 1.0)  1e-9:\n        return 1.0\n\n    sparsity = (sqrt_N - norm1 / norm2) / (sqrt_N - 1.0)\n    return sparsity\n\ndef generate_data(n, m, r, data_type, seed):\n    \"\"\"\n    Generates synthetic data matrix X.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Ground truth factors\n    W_true = rng.random((n, r))\n    \n    # Make H_true sparse (e.g., 80% sparsity)\n    H_true_dense = rng.random((r, m))\n    sparsity_mask = rng.random((r, m)) > 0.8\n    H_true = H_true_dense * sparsity_mask\n    \n    # Normalize W_true\n    W_true /= W_true.sum(axis=0, keepdims=True)\n\n    Lambda = W_true @ H_true\n    \n    if data_type == 'poisson':\n        X = rng.poisson(Lambda)\n    elif data_type == 'gaussian':\n        noise_std = 0.1 * np.std(Lambda)\n        noise = rng.normal(0, noise_std, size=(n, m))\n        X = np.maximum(0, Lambda + noise)\n    else:\n        raise ValueError(\"Invalid data_type specified.\")\n        \n    return X\n\ndef nmf(X, r, iterations, lambda_reg, model, init_seed):\n    \"\"\"\n    Performs Non-negative Matrix Factorization.\n    \"\"\"\n    n, m = X.shape\n    rng = np.random.default_rng(init_seed)\n    \n    # Initialize W and H with small positive random values\n    W = rng.random((n, r)) * 0.1 + 1e-4\n    H = rng.random((r, m)) * 0.1 + 1e-4\n    \n    eps = 1e-9\n\n    ones_nm = np.ones((n, m))\n\n    for _ in range(iterations):\n        if model == 'gaussian':\n            # Update H\n            H_num = W.T @ X\n            H_den = W.T @ W @ H + lambda_reg + eps\n            H *= H_num / H_den\n            \n            # Update W\n            W_num = X @ H.T\n            W_den = W @ H @ H.T + eps\n            W *= W_num / W_den\n            \n        elif model == 'poisson':\n            WH = W @ H + eps\n            # Update H\n            H_num = W.T @ (X / WH)\n            H_den = W.T @ ones_nm + lambda_reg + eps\n            H *= H_num / H_den\n            \n            # Update W\n            WH = W @ H + eps\n            W_num = (X / WH) @ H.T\n            W_den = ones_nm @ H.T + eps\n            W *= W_num / W_den\n        \n        # Normalize W columns and rescale H rows\n        w_col_sums = W.sum(axis=0)\n        W /= (w_col_sums + eps)\n        H *= w_col_sums.reshape(r, 1)\n\n    return W, H\n\ndef solve():\n    test_cases = [\n        # n, m, r, iterations, lambda, data_type\n        (30, 200, 4, 200, 0.1, 'poisson'),\n        (30, 200, 4, 200, 0.0, 'poisson'),\n        (30, 200, 4, 200, 1.0, 'poisson'),\n        (30, 200, 4, 200, 0.1, 'gaussian'),\n    ]\n\n    results = []\n    main_rng = np.random.default_rng(42)  # For reproducible sub-seeds\n\n    for i, (n, m, r, iterations, lambda_reg, data_type) in enumerate(test_cases):\n        data_seed = main_rng.integers(10000)\n        init_seed = main_rng.integers(10000)\n        \n        # Generate data\n        X = generate_data(n, m, r, data_type, seed=data_seed)\n        \n        # Run Gaussian-based NMF\n        _, H_g = nmf(X, r, iterations, lambda_reg, 'gaussian', init_seed=init_seed)\n        \n        # Run Poisson-based NMF\n        _, H_p = nmf(X, r, iterations, lambda_reg, 'poisson', init_seed=init_seed)\n        \n        # Compute sparsities\n        s_g = hoyer_sparsity(H_g)\n        s_p = hoyer_sparsity(H_p)\n        \n        # Compare and store result\n        results.append(s_p >= s_g)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}