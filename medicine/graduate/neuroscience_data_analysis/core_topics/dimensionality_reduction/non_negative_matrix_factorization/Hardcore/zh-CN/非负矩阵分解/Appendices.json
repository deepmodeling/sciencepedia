{
    "hands_on_practices": [
        {
            "introduction": "本练习提供了一个具体的、分步计算 NMF 乘法更新法则的过程。通过手动执行一次迭代，您将亲身体验因子矩阵 $W$ 和 $H$ 是如何被逐步优化以更好地逼近数据矩阵 $X$ 的。在深入研究更复杂的实现和理论问题之前，这项基础练习对于揭开 NMF 算法核心机制的神秘面纱至关重要 。",
            "id": "4182176",
            "problem": "一个实验室记录了 $N=3$ 个神经元在 $T=3$ 个时间窗内的脉冲计数，这些数据汇总在非负数据矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{3 \\times 3}$ 中。目标是执行非负矩阵分解 (NMF) 来近似 $X \\approx W H$，其中 $W \\in \\mathbb{R}_{\\ge 0}^{3 \\times 2}$ 编码了神经元在 $K=2$ 个潜在集群上的载荷，而 $H \\in \\mathbb{R}_{\\ge 0}^{2 \\times 3}$ 编码了这些集群随时间的激活情况。假设目标函数为带有弗罗贝尼乌斯范数的平方误差和非负性约束，并执行一次完整的乘法更新迭代。该迭代定义为：在给定 $H$ 的情况下更新 $W$，然后使用更新后的 $W$ 更新 $H$。此更新规则由所述目标函数推导得出，不含任何外部启发式方法。\n\n具体的矩阵如下：\n$$\nX = \\begin{bmatrix}\n2  4  6 \\\\\n2  4  6 \\\\\n2  4  6\n\\end{bmatrix},\\quad\nW^{(0)} = \\begin{bmatrix}\n1  1 \\\\\n1  1 \\\\\n1  1\n\\end{bmatrix},\\quad\nH^{(0)} = \\begin{bmatrix}\n1  2  3 \\\\\n1  2  3\n\\end{bmatrix}.\n$$\n\n从平方误差成本函数 $J(W,H) = \\tfrac{1}{2}\\|X - W H\\|_{F}^{2}$（约束条件为 $W \\ge 0$ 和 $H \\ge 0$）出发，推导出 $W$ 和 $H$ 的乘法更新规则，然后对给定的 $X$、$W^{(0)}$ 和 $H^{(0)}$ 执行恰好一次迭代，以获得 $W^{(1)}$ 和 $H^{(1)}$。将你的最终答案表示为一个单行矩阵，其中首先按行主序（row-major order）列出 $W^{(1)}$ 的六个更新后元素，然后是按行主序排列的 $H^{(1)}$ 的六个更新后元素。如果数值需要取整，请将答案四舍五入到四位有效数字。",
            "solution": "所述问题是有效的。它在科学上基于非负矩阵分解 (NMF) 的原理，这是数据分析中的一种标准方法。该问题是适定的 (well-posed)，提供了所有必要的数据矩阵（$X$、$W^{(0)}$、$H^{(0)}$）、参数（$N=3$、$T=3$、$K=2$）、一个清晰的目标函数（$J(W,H) = \\frac{1}{2}\\|X - W H\\|_{F}^{2}$）以及一个精确定义的任务（一次完整的乘法更新迭代）。数据和约束是自洽的，语言客观且无歧义。该问题可以被解决并得出一个唯一且有意义的答案。\n\n问题的核心是在 $W$ 和 $H$ 的元素为非负的约束条件下，最小化成本函数 $J(W,H)$。\n$$\nJ(W,H) = \\frac{1}{2} \\|X - WH\\|_{F}^{2} = \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{T} \\left(X_{ij} - \\sum_{k=1}^{K} W_{ik} H_{kj}\\right)^2\n$$\n乘法更新规则是通过考虑 $J$ 相对于 $W$ 和 $H$ 元素的梯度来推导的。这些规则提供了一种满足非负性约束的不动点迭代方法。\n\n关于 $W$ 的一个元素 $W_{ab}$ 的梯度为：\n$$\n\\frac{\\partial J}{\\partial W_{ab}} = \\sum_{j=1}^{T} \\left(X_{aj} - \\sum_{k=1}^{K} W_{ak} H_{kj}\\right) (-H_{bj}) = - \\sum_{j=1}^{T} (X - WH)_{aj} (H^T)_{jb} = -((X-WH)H^T)_{ab}\n$$\n以矩阵形式表示，梯度为 $\\nabla_W J = -(X-WH)H^T = WHH^T - XH^T$。$W$ 的乘法更新规则是：\n$$\nW_{ab} \\leftarrow W_{ab} \\frac{(XH^T)_{ab}}{(WHH^T)_{ab}}\n$$\n使用逐元素（哈达玛）乘积 $\\odot$ 和除法，这可以紧凑地写作：\n$$\nW \\leftarrow W \\odot \\frac{XH^T}{WHH^T}\n$$\n类似地，关于 $H$ 的一个元素 $H_{cd}$ 的梯度为：\n$$\n\\frac{\\partial J}{\\partial H_{cd}} = \\sum_{i=1}^{N} \\left(X_{id} - \\sum_{k=1}^{K} W_{ik} H_{kd}\\right) (-W_{ic}) = - \\sum_{i=1}^{N} (W^T)_{ci} (X - WH)_{id} = -(W^T(X-WH))_{cd}\n$$\n以矩阵形式表示，$\\nabla_H J = -W^T(X-WH) = W^TWH - W^TX$。$H$ 的乘法更新规则是：\n$$\nH_{cd} \\leftarrow H_{cd} \\frac{(W^TX)_{cd}}{(W^TWH)_{cd}}\n$$\n这可以紧凑地写作：\n$$\nH \\leftarrow H \\odot \\frac{W^TX}{W^TWH}\n$$\n问题指定执行一次完整的迭代：首先更新 $W$ 得到 $W^{(1)}$，然后使用 $W^{(1)}$ 更新 $H$ 得到 $H^{(1)}$。\n\n给定的初始矩阵是：\n$$\nX = \\begin{bmatrix} 2  4  6 \\\\ 2  4  6 \\\\ 2  4  6 \\end{bmatrix},\\quad W^{(0)} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix},\\quad H^{(0)} = \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix}\n$$\n首先，我们观察到初始矩阵可以完美地重构 $X$：\n$$\nW^{(0)} H^{(0)} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix} = \\begin{bmatrix} 1+1  2+2  3+3 \\\\ 1+1  2+2  3+3 \\\\ 1+1  2+2  3+3 \\end{bmatrix} = \\begin{bmatrix} 2  4  6 \\\\ 2  4  6 \\\\ 2  4  6 \\end{bmatrix} = X\n$$\n这意味着初始成本 $J(W^{(0)}, H^{(0)}) = 0$。由于 $J \\ge 0$，这是一个全局最小值，我们预期更新规则不会产生任何变化，即初始点是该迭代算法的一个不动点。我们现在通过显式计算来验证这一点。\n\n步骤1：更新 $W$ 以求得 $W^{(1)}$。\n更新规则是 $W^{(1)} = W^{(0)} \\odot \\frac{X (H^{(0)})^T}{W^{(0)} H^{(0)} (H^{(0)})^T}$。\n我们计算更新因子的分子：\n$$\n(H^{(0)})^T = \\begin{bmatrix} 1  1 \\\\ 2  2 \\\\ 3  3 \\end{bmatrix}\n$$\n$$\nX (H^{(0)})^T = \\begin{bmatrix} 2  4  6 \\\\ 2  4  6 \\\\ 2  4  6 \\end{bmatrix} \\begin{bmatrix} 1  1 \\\\ 2  2 \\\\ 3  3 \\end{bmatrix} = \\begin{bmatrix} 2+8+18  2+8+18 \\\\ 2+8+18  2+8+18 \\\\ 2+8+18  2+8+18 \\end{bmatrix} = \\begin{bmatrix} 28  28 \\\\ 28  28 \\\\ 28  28 \\end{bmatrix}\n$$\n对于分母，我们利用 $W^{(0)}H^{(0)} = X$ 这个事实：\n$$\nW^{(0)} H^{(0)} (H^{(0)})^T = X (H^{(0)})^T = \\begin{bmatrix} 28  28 \\\\ 28  28 \\\\ 28  28 \\end{bmatrix}\n$$\n更新因子是分子与分母进行逐元素相除的结果：\n$$\n\\frac{X (H^{(0)})^T}{W^{(0)} H^{(0)} (H^{(0)})^T} = \\frac{\\begin{bmatrix} 28  28 \\\\ 28  28 \\\\ 28  28 \\end{bmatrix}}{\\begin{bmatrix} 28  28 \\\\ 28  28 \\\\ 28  28 \\end{bmatrix}} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix}\n$$\n现在，我们将更新应用于 $W^{(0)}$：\n$$\nW^{(1)} = W^{(0)} \\odot \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix} \\odot \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix}\n$$\n因此，$W^{(1)} = W^{(0)}$。\n\n步骤2：使用新计算出的 $W^{(1)}$ 更新 $H$ 以求得 $H^{(1)}$。\n更新规则是 $H^{(1)} = H^{(0)} \\odot \\frac{(W^{(1)})^T X}{(W^{(1)})^T W^{(1)} H^{(0)}}$。\n由于 $W^{(1)} = W^{(0)}$，我们有 $(W^{(1)})^T = (W^{(0)})^T = \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix}$。\n我们计算更新因子的分子：\n$$\n(W^{(1)})^T X = \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix} \\begin{bmatrix} 2  4  6 \\\\ 2  4  6 \\\\ 2  4  6 \\end{bmatrix} = \\begin{bmatrix} 6  12  18 \\\\ 6  12  18 \\end{bmatrix}\n$$\n现在，我们计算分母：\n$$\n(W^{(1)})^T W^{(1)} = \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix} \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 3  3 \\\\ 3  3 \\end{bmatrix}\n$$\n$$\n(W^{(1)})^T W^{(1)} H^{(0)} = \\begin{bmatrix} 3  3 \\\\ 3  3 \\end{bmatrix} \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix} = \\begin{bmatrix} 3+3  6+6  9+9 \\\\ 3+3  6+6  9+9 \\end{bmatrix} = \\begin{bmatrix} 6  12  18 \\\\ 6  12  18 \\end{bmatrix}\n$$\n更新因子是：\n$$\n\\frac{(W^{(1)})^T X}{(W^{(1)})^T W^{(1)} H^{(0)}} = \\frac{\\begin{bmatrix} 6  12  18 \\\\ 6  12  18 \\end{bmatrix}}{\\begin{bmatrix} 6  12  18 \\\\ 6  12  18 \\end{bmatrix}} = \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix}\n$$\n最后，我们将更新应用于 $H^{(0)}$：\n$$\nH^{(1)} = H^{(0)} \\odot \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix} = \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix} \\odot \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\end{bmatrix} = \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix}\n$$\n因此，$H^{(1)} = H^{(0)}$。\n\n经过一次完整的迭代后，矩阵保持不变：\n$$\nW^{(1)} = \\begin{bmatrix} 1  1 \\\\ 1  1 \\\\ 1  1 \\end{bmatrix}, \\quad H^{(1)} = \\begin{bmatrix} 1  2  3 \\\\ 1  2  3 \\end{bmatrix}\n$$\n最终答案要求先按行主序（row-major order）给出 $W^{(1)}$ 的六个元素，然后按行主序给出 $H^{(1)}$ 的六个元素。\n$W^{(1)}$ 的元素是 $W_{11}=1$, $W_{12}=1$, $W_{21}=1$, $W_{22}=1$, $W_{31}=1$, $W_{32}=1$。\n$H^{(1)}$ 的元素是 $H_{11}=1$, $H_{12}=2$, $H_{13}=3$, $H_{21}=1$, $H_{22}=2$, $H_{23}=3$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1  1  1  1  1  1  2  3  1  2  3\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "应用 NMF 的一个关键步骤是选择适当的成分数量，即秩 $r$。这个选择代表了模型复杂性与泛化能力之间的权衡。本练习挑战您像数据科学家一样思考，设计一个稳健的交叉验证方案来选择最佳的秩，以确保您的发现不仅仅是过拟合的产物 。理解这一过程是获得科学上有效且可复现结果的关键。",
            "id": "4182160",
            "problem": "一个神经科学实验室记录了一个非负数据矩阵 $X \\in \\mathbb{R}_{+}^{n \\times t}$，其中 $n$ 表示神经元的数量，$t$ 表示时间窗的数量，$X_{ij}$ 代表神经元 $i$ 在时间窗 $j$ 内的活动水平。为了识别潜在的神经元集合及其激活模式，该实验室打算将 $X$ 分解为两个非负矩阵 $W \\in \\mathbb{R}_{+}^{n \\times r}$ 和 $H \\in \\mathbb{R}_{+}^{r \\times t}$，使得 $X \\approx WH$，其中 $r$ 是分解秩。分解的选择应平衡近似质量和泛化能力，避免过拟合。\n\n从非负矩阵分解（NMF）的核心定义出发，该实验室希望推导出一个科学上合理的交叉验证（CV）方案，该方案使用逐元素掩码来留出 $X$ 的一部分元素，并通过优化留出数据上的重构误差来选择秩 $r$。设计必须尊重非负性约束，避免留出信息泄漏到训练中，并为基于留出集上的性能选择 $r$ 提供一个有原则的规则。考虑对于平方误差重构，限制在观测元素上的训练目标可以用一个二元掩码 $M \\in \\{0,1\\}^{n \\times t}$ 来表示，而留出误差则在其补集 $\\bar{M} = \\mathbf{1} - M$ 上计算。另外，对于计数类数据，可以使用适用于泊松观测的散度。\n\n哪个选项最正确地指定了一种基于掩码的交叉验证程序，该程序符合基本原则，并为选择 $r$ 提供了有原则的规则？\n\nA. 抽取一个随机二元掩码 $M \\in \\{0,1\\}^{n \\times t}$，它保留固定比例 $p \\in (0,1)$ 的元素作为训练数据（$M_{ij} = 1$），并将剩余部分留作验证数据（$\\bar{M}_{ij} = 1 - M_{ij}$）。对于一个候选秩 $r$，通过在 $W \\ge 0$ 和 $H \\ge 0$ 的约束下最小化带掩码的平方重构损失来拟合 NMF，例如，\n$$\n\\min_{W \\ge 0,\\, H \\ge 0}\\;\\; \\sum_{i=1}^{n} \\sum_{j=1}^{t} M_{ij} \\bigl(X_{ij} - [WH]_{ij}\\bigr)^{2},\n$$\n并计算验证误差\n$$\nE_{\\mathrm{val}}(r) = \\frac{1}{\\|\\bar{M}\\|_{1}} \\sum_{i=1}^{n} \\sum_{j=1}^{t} \\bar{M}_{ij} \\bigl(X_{ij} - [W H]_{ij}\\bigr)^{2}.\n$$\n对 $K$ 个独立的掩码和 $S$ 次随机初始化重复此过程，在所有掩码和种子上平均 $E_{\\mathrm{val}}(r)$，并选择最小化平均验证误差的最小秩 $r$。最后，在选定的秩 $r$ 上对完整矩阵 $X$ 重新拟合 NMF，以获得报告的 $W$ 和 $H$。\n\nB. 对于每个候选秩 $r$，在不使用掩码的情况下对完整矩阵 $X$ 进行 NMF 拟合，方法是在 $W \\ge 0$ 和 $H \\ge 0$ 的约束下最小化所有元素上的平方重构损失，并选择产生最小训练重构误差的秩 $r$。报告相应的 $W$ 和 $H$。\n\nC. 将 $X$ 的列划分为训练集和验证集。对于每个候选秩 $r$，仅在训练列上拟合 NMF 以获得 $W_{\\mathrm{train}}$ 和 $H_{\\mathrm{train}}$，然后通过将 $W_{\\mathrm{train}} H_{\\mathrm{train}}$ 相乘并与留出列进行比较来计算留出列上的验证误差。选择具有最小验证误差的秩 $r$，并报告 $W_{\\mathrm{train}}$ 和 $H_{\\mathrm{train}}$。\n\nD. 使用二元掩码 $M$ 来指定留出的元素，但在训练期间将所有留出的元素插补为零。通过最小化带有插补零的完整数据平方重构损失来拟合 NMF，并选择最小化此经插补增强的训练损失的秩 $r$。报告相应的 $W$ 和 $H$。\n\nE. 如选项 A 中那样抽取一个随机的逐元素掩码 $M$，但对于计数类神经数据，使用适用于泊松观测的广义 Kullback–Leibler (KL) 散度。对于一个候选秩 $r$，通过最小化以下表达式来拟合 NMF\n$$\n\\min_{W \\ge 0,\\, H \\ge 0}\\;\\; \\sum_{i=1}^{n} \\sum_{j=1}^{t} M_{ij} \\left( X_{ij} \\log\\frac{X_{ij}}{[WH]_{ij}} - X_{ij} + [WH]_{ij} \\right),\n$$\n并计算留出散度\n$$\nD_{\\mathrm{val}}(r) = \\frac{1}{\\|\\bar{M}\\|_{1}} \\sum_{i=1}^{n} \\sum_{j=1}^{t} \\bar{M}_{ij} \\left( X_{ij} \\log\\frac{X_{ij}}{[WH]_{ij}} - X_{ij} + [WH]_{ij} \\right).\n$$\n对 $K$ 个掩码和 $S$ 个种子重复此过程，并选择其平均 $D_{\\mathrm{val}}(r)$ 在所有秩的最小值的一个标准误范围内的最小秩 $r$（1-标准误规则）。最后，在选定的秩 $r$ 上对完整矩阵 $X$ 重新拟合 NMF。\n\n选择所有正确的选项。",
            "solution": "题目陈述提出了一个在计算神经科学和机器学习领域中有效且标准的问题：如何为数据矩阵 $X \\in \\mathbb{R}_{+}^{n \\times t}$ 的非负矩阵分解（NMF）选择秩 $r$，以避免过拟合。所提出的方法是一种基于掩码的交叉验证（CV）程序。这个问题在科学上是有根据的，提法是妥当的，并且包含足够的信息来评估所提出的方法。\n\n用于模型选择（这里是选择秩 $r$）的有效交叉验证程序的核心原则是：\n1.  **数据划分**：必须将可用数据划分为训练集（用于学习模型参数 $W$ 和 $H$）和不相交的验证集（用于评估模型的泛化性能）。验证集的信息绝不能在训练期间使用。\n2.  **模型拟合**：对于每个候选的模型复杂度（每个秩 $r$），通过仅在训练数据上最小化损失函数来估计模型参数。对于 NMF，这涉及找到 $W$ 和 $H$ 来最小化训练数据元素的重构误差，同时满足非负性约束。\n3.  **性能评估**：然后使用拟合好的模型来预测留出的验证数据的值，并计算在该集合上的预测误差。这个验证误差可作为模型泛化误差的估计。\n4.  **模型选择**：比较不同秩 $r$ 的验证误差。选择产生最佳验证性能（例如，最小误差）的秩。可以使用诸如“一倍标准误规则”之类的启发式方法来偏好更简单的模型。\n5.  **最终模型训练**：一旦选定了最优秩 $r^*$，标准做法是使用秩 $r^*$ 在整个数据集 $X$ 上重新训练模型。这最后一步利用所有可用数据，为所选模型复杂度生成尽可能好的参数估计。\n\n问题描述了一种基于掩码的方法，其中二元掩码 $M \\in \\{0,1\\}^{n \\times t}$ 定义了训练集（$M_{ij}=1$ 的元素）和验证集（$M_{ij}=0$ 的元素）。我们现在将根据这些原则评估每个选项。\n\n### 选项 A 的评估\n\n该选项提出了一种使用平方误差（Frobenius 范数）损失函数的基于掩码的交叉验证程序。\n\n1.  **数据划分**：它正确地使用二元掩码 $M$ 将训练元素（$M_{ij}=1$）与验证元素（$\\bar{M}_{ij}=1$）分开。\n2.  **模型拟合**：它正确地陈述了对于给定的秩 $r$，模型参数 $W$ 和 $H$ 是通过仅在训练元素上最小化损失函数来找到的：\n    $$ \\min_{W \\ge 0,\\, H \\ge 0}\\;\\; \\sum_{i=1}^{n} \\sum_{j=1}^{t} M_{ij} \\bigl(X_{ij} - [WH]_{ij}\\bigr)^{2} $$\n    这恰当地防止了留出数据的信息影响训练过程。\n3.  **性能评估**：它正确地将验证误差计算为留出元素上的均方误差：\n    $$ E_{\\mathrm{val}}(r) = \\frac{1}{\\|\\bar{M}\\|_{1}} \\sum_{i=1}^{n} \\sum_{j=1}^{t} \\bar{M}_{ij} \\bigl(X_{ij} - [W H]_{ij}\\bigr)^{2} $$\n    用 $\\|\\bar{M}\\|_1$（留出元素的数量）进行归一化是合适的。\n4.  **模型选择**：它建议选择最小化平均验证误差的最小秩 $r$，该平均验证误差是在多个掩码（$K$）和随机初始化（$S$）上平均得到的。这是基于泛化性能选择 $r$ 的一个有原则的规则，其中偏好最小秩体现了简约性原则。\n5.  **最终模型训练**：它正确地以在选定的秩上对完整矩阵 $X$ 重新拟合模型作为结束。\n\n此程序遵循了可靠交叉验证方案的所有原则。使用平方误差是 NMF 的标准选择。\n\n**对 A 的判断**：**正确**。\n\n### 选项 B 的评估\n\n该选项建议在完整矩阵上拟合 NMF，并根据训练误差选择秩。\n\nNMF 的训练重构误差是秩 $r$ 的一个非增函数。具有更高秩的模型具有更多自由度，在训练所用的相同数据上几乎总能实现更低（或相等）的重构误差。因此，一个选择产生最小训练误差的秩 $r$ 的规则将总是选择最大的候选秩 $r$。这导致选择最复杂的模型，而这个模型极有可能对数据集中的特定噪声过拟合。这种方法完全没有估计或控制泛化误差，而这正是交叉验证的主要目的。\n\n**对 B 的判断**：**不正确**。\n\n### 选项 C 的评估\n\n该选项建议划分 $X$ 的列（时间窗）。这是一种有效的块交叉验证形式。然而，该程序的描述是有缺陷的。\n\n在训练列上拟合 NMF 得到 $W_{\\mathrm{train}}$ 和 $H_{\\mathrm{train}}$ 后，该选项建议通过比较留出列与 $W_{\\mathrm{train}} H_{\\mathrm{train}}$ 来计算验证误差。这是不正确的。矩阵 $H_{\\mathrm{train}}$ 包含的是*训练*列的激活模式，而不是*验证*列的。正确的程序应该是固定 $W_{\\mathrm{train}}$，然后通过最小化 $\\|X_{\\mathrm{val}} - W_{\\mathrm{train}} H_{\\mathrm{val}}\\|^2$ 来求解验证列的激活 $H_{\\mathrm{val}}$。验证误差是这个新优化的结果。该选项忽略了这一关键步骤。\n\n此外，它建议将仅从数据子集导出的 $W_{\\mathrm{train}}$ 和 $H_{\\mathrm{train}}$ 作为最终结果报告。标准且更好的做法是使用选定的秩在完整数据集上重新拟合模型，如选项 A 和 E 所做的那样。\n\n**对 C 的判断**：**不正确**。\n\n### 选项 D 的评估\n\n该选项建议将留出的元素用零插补，然后在此修改后的矩阵上最小化重构误差。\n\n这个程序有两个主要缺陷。首先，它破坏了数据集。通过用 $0$ 替换未知值，它迫使 NMF 算法学习解释这些人为零的因子，这会使得到的 $W$ 和 $H$ 产生偏差。这不是处理缺失数据的有原则的方法。其次，与选项 B 类似，它基于最小化一个*训练损失*（在零插补矩阵上的损失）来选择秩 $r$。如前所述，最小化训练损失会导致过拟合，并且不是有效的模型选择方法。它未能对留出数据进行真正的验证。\n\n**对 D 的判断**：**不正确**。\n\n### 选项 E 的评估\n\n该选项提出了一个与选项 A 类似的程序，但使用了广义 Kullback-Leibler (KL) 散度作为损失函数，并采用了一种更精细的模型选择规则。\n\n1.  **数据划分、模型拟合、性能评估**：结构与选项 A 相同并且是正确的。它使用掩码分离训练和验证数据，在训练集上最小化损失，并在验证集上进行评估。\n2.  **损失函数**：它使用广义 KL 散度：\n    $$ D(X || WH) = \\sum_{i,j} \\left( X_{ij} \\log\\frac{X_{ij}}{[WH]_{ij}} - X_{ij} + [WH]_{ij} \\right) $$\n    正如问题陈述中所指出的，这种损失函数适用于计数类数据，并对应于假设数据服从泊松分布，这是神经脉冲计数的一个常见且合适的模型。因此，这个选择在应用领域具有充分的科学动机。\n3.  **模型选择**：它建议采用“一倍标准误规则”：选择其验证性能与表现最佳的秩相比没有统计学显著差异的最小（最简约）的秩 $r$。这是模型选择中一个标准的、有统计学依据的启发式方法，它提供了一种在模型性能和复杂性之间进行权衡的稳健方式，明确满足了“有原则的规则”的要求。\n4.  **最终模型训练**：它正确地指明了在选定的秩上对完整矩阵 $X$ 重新拟合模型。\n\n这个选项描述了一个完整、正确且在统计上更成熟的交叉验证方案，特别适合问题的背景。\n\n**对 E 的判断**：**正确**。\n\n### 结论\n\n选项 A 和 E 都描述了用于在 NMF 中选择秩的正确且有原则的基于掩码的交叉验证程序。选项 A 使用标准的平方误差损失，而选项 E 使用 KL 散度，后者通常更适合所描述的神经科学数据类型。选项 E 还包含了“一倍标准误规则”，这是模型选择中一个广受认可的改进方法。由于问题要求选择所有正确的选项，并且 A 和 E 都详细说明了有效、自洽且科学上合理的程序，因此两者都是正确的。",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "目标函数的选择至关重要，因为它编码了我们对数据底层统计特性的假设。这项高级练习将指导您实现并比较两种 NMF 变体：一种基于标准平方误差（Frobenius 范数），另一种基于更适合神经元脉冲等计数数据的 Kullback-Leibler 散度。通过检验最终得到的激活矩阵 $H$ 的稀疏性，您将直接观察到一个有原则的统计模型选择如何能够带来更具可解释性和更有意义的科学发现 。",
            "id": "4182157",
            "problem": "给定一项基于非负矩阵分解 (NMF) 的数据分析任务，其背景为神经科学领域，其中神经元脉冲序列记录被自然地建模为计数数据。您的目标是实现并根据经验比较在两种不同似然模型下，使用相同稀疏性正则化器时激活矩阵的稀疏性。此比较必须在一个独立的程序中完成，该程序为指定的测试套件生成单行输出，汇总结果。\n\n使用的基本原理：\n- 非负矩阵分解 (NMF) 将一个非负数据矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$ 分解为乘积 $W H$，其中 $W \\in \\mathbb{R}_{\\ge 0}^{n \\times r}$ 且 $H \\in \\mathbb{R}_{\\ge 0}^{r \\times m}$，$r$ 表示成分的数量。\n- 在具有独立同分布且方差固定的高斯噪声模型下，负对数似然（在忽略常数的情况下）可简化为平方弗罗贝尼乌斯重构损失 $\\|X - W H\\|_F^2$。在适用于脉冲计数的泊松计数模型下，负对数似然与广义库尔贝克-莱布勒散度 (KL) 成正比，通常写作 $\\sum_{i,j}\\left[X_{ij} \\log\\left(\\frac{X_{ij}}{(W H)_{ij}}\\right) - X_{ij} + (W H)_{ij}\\right]$。\n- 通过向目标函数添加 $\\lambda \\|H\\|_1$ 项，可以对激活矩阵 $H$ 施加一个促进稀疏性的惩罚，其中 $\\lambda \\ge 0$ 且 $\\|H\\|_1 = \\sum_{i,j} |H_{ij}|$。\n\n任务规格：\n1. 对于所提供测试套件中的每个测试用例，生成一个维度为 $n \\times m$、秩参数为 $r$ 的合成非负数据集 $X$。对于类泊松数据，通过从均值为 $(W_{\\text{true}} H_{\\text{true}})$ 的独立泊松分布中采样来构建 $X$，其中 $W_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{n \\times r}$ 和 $H_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{r \\times m}$ 是非负的真实因子，且 $H_{\\text{true}}$ 中具有稀疏的时间激活。对于类高斯数据，通过 $X = \\max\\{0, W_{\\text{true}} H_{\\text{true}} + \\epsilon\\}$ 构建 $X$，其中加入均值为零的加性高斯噪声，并进行裁剪以保持非负性。通过在代码中使用固定的随机种子来确保确定性设置。\n2. 实现两个 NMF 估计器，它们具有相同的非负性约束和应用于相同 $\\|H\\|_1$ 稀疏性正则化器的相同 $\\lambda$：\n   - 估计器 $\\mathcal{G}$：最小化基于高斯模型的目标函数 $J_{\\mathcal{G}}(W,H) = \\frac{1}{2} \\|X - W H\\|_F^2 + \\lambda \\|H\\|_1$。\n   - 估计器 $\\mathcal{P}$：最小化基于泊松模型的目标函数 $J_{\\mathcal{P}}(W,H) = D_{\\mathrm{KL}}(X \\| W H) + \\lambda \\|H\\|_1$，其中 $D_{\\mathrm{KL}}(X \\| Y)$ 表示广义库尔贝克-莱布勒散度。\n   两个估计器在整个优化过程中都必须强制 $W \\ge 0$ 和 $H \\ge 0$，并且必须使用从平稳性条件和非负性约束推导出的乘性更新式迭代，且不引入负值。除了指定的运行时环境外，您不得依赖任何外部库。\n3. 在每个测试用例中，对两个估计器使用相同的初始化策略，即为 $W$ 和 $H$ 设置小的正随机项。使用固定的迭代次数。为避免尺度模糊性，在每次迭代中重新归一化 $W$ 的列，使其 $\\ell_1$ 范数和为 1，并相应地重新缩放 $H$ 的行，以保持 $W H$ 不变。这可以防止纯粹由缩放引起的稀疏性人为变化。\n4. 使用 Hoyer 稀疏性指数（应用于向量化的 $H$）来量化估计出的激活矩阵 $H$ 的稀疏性。对于一个非负向量 $v \\in \\mathbb{R}_{\\ge 0}^N$，该指数定义为\n   $$ s(v) = \\frac{\\sqrt{N} - \\frac{\\|v\\|_1}{\\|v\\|_2}}{\\sqrt{N} - 1}, $$\n   并约定如果 $v$ 是零向量，则 $s(v) = 1$，且在代码中必须安全地处理非常小的分母。对于矩阵 $H$，将此公式应用于其向量化形式。报告基于泊松模型的估计器得到的 $s(H_{\\mathcal{P}})$ 和基于高斯模型的估计器得到的 $s(H_{\\mathcal{G}})$。\n5. 对于每个测试用例，计算布尔结果 $b = \\left[s(H_{\\mathcal{P}}) \\ge s(H_{\\mathcal{G}})\\right]$，该结果表示在使用相同 $\\lambda$ 的情况下，基于泊松模型的估计器产生的激活矩阵 $H$ 是否比基于高斯模型的估计器具有更高或相等的稀疏性。\n6. 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，`[result_1,result_2,result_3]`）。\n\n测试套件：\n- 案例 1（理想路径，类泊松数据）：$n = 30$, $m = 200$, $r = 4$, 迭代次数 $= 200$, $\\lambda = 0.1$, 类泊松数据。\n- 案例 2（正则化的边界条件）：$n = 30$, $m = 200$, $r = 4$, 迭代次数 $= 200$, $\\lambda = 0$, 类泊松数据。\n- 案例 3（强正则化，可能导致收缩）：$n = 30$, $m = 200$, $r = 4$, 迭代次数 $= 200$, $\\lambda = 1.0$, 类泊松数据。\n- 案例 4（不同噪声族）：$n = 30$, $m = 200$, $r = 4$, 迭代次数 $= 200$, $\\lambda = 0.1$, 类高斯数据，通过裁剪强制非负性。\n\n科学真实性与解释目标：\n- 在您的解决方案中，从高斯模型和泊松模型的最大似然原理以及非负性约束出发，解释为什么基于泊松模型的目标函数通常能在脉冲计数数据中鼓励 $H$ 的稀疏激活，特别是当与 $\\ell_1$ 惩罚中相同的 $\\lambda$ 结合使用时。避免使用问题陈述中的快捷公式；解决方案必须从第一性原理和卡罗需-库恩-塔克 (KKT) 条件推导出乘性更新，并将其与观察到的稀疏性差异联系起来。\n\n最终输出格式：\n- 您的程序必须以 `[b_1,b_2,b_3,b_4]` 的确切格式打印单行输出，其中每个 $b_i$ 是一个布尔值，按顺序对应上述测试用例。",
            "solution": "该问题要求实现并比较两种非负矩阵分解 (NMF) 估计器，一种基于高斯噪声模型，另一种基于泊松计数模型，以分析它们对所得激活矩阵 $H$ 稀疏性的影响。比较在合成的神经科学风格数据上进行。\n\nNMF 的核心是将一个非负数据矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$ 近似为两个低秩非负矩阵的乘积，$W \\in \\mathbb{R}_{\\ge 0}^{n \\times r}$ 和 $H \\in \\mathbb{R}_{\\ge 0}^{r \\times m}$。这被表述为一个约束优化问题：\n$$ \\min_{W \\ge 0, H \\ge 0} J(W, H) $$\n其中 $J(W, H)$ 是目标函数。问题指定了两个目标函数，两者都包含对激活矩阵 $H$ 的 $\\ell_1$ 稀疏性正则化器：\n1.  基于高斯模型的目标函数：$J_{\\mathcal{G}}(W,H) = \\frac{1}{2} \\|X - W H\\|_F^2 + \\lambda \\|H\\|_1$\n2.  基于泊松模型的目标函数：$J_{\\mathcal{P}}(W,H) = D_{\\mathrm{KL}}(X \\| W H) + \\lambda \\|H\\|_1$，其中 $D_{\\mathrm{KL}}(X \\| Y) = \\sum_{i,j}\\left[X_{ij} \\log\\left(\\frac{X_{ij}}{Y_{ij}}\\right) - X_{ij} + Y_{ij}\\right]$。\n\n为解决此问题，我们采用乘性更新法则。推导这些法则是为了迭代地寻找满足此非负约束问题的卡罗需-库恩-塔克 (KKT) 条件的稳定点。对于一个通用参数矩阵 $\\Theta$，KKT 条件包括互补松弛条件 $\\Theta \\odot \\nabla_{\\Theta} J = 0$，其中 $\\odot$ 表示逐元素乘法。形如 $\\Theta \\leftarrow \\Theta \\odot \\frac{(\\nabla_\\Theta J)^-}{(\\nabla_\\Theta J)^+}$ 的更新法则被设计用于收敛到这样的点，其中 $\\nabla_\\Theta J = (\\nabla_\\Theta J)^+ - (\\nabla_\\Theta J)^-$ 是梯度分解为其正部和负部的形式。\n\n### 乘性更新法则的推导\n\n**1. 高斯模型目标函数 ($J_{\\mathcal{G}}$)**\n\n目标函数为 $J_{\\mathcal{G}}(W,H) = \\frac{1}{2} \\|X - WH\\|_F^2 + \\lambda \\sum_{k,j} H_{kj}$。\n\n**$H$ 的更新**：\n关于 $H$ 的梯度是：\n$$ \\nabla_H J_{\\mathcal{G}} = -W^T(X - WH) + \\lambda \\mathbf{1} = W^TWH - W^TX + \\lambda \\mathbf{1} $$\n其中 $\\mathbf{1}$ 是一个与 $H$ 大小相同的全一矩阵。我们识别梯度的正部和负部：\n$$ (\\nabla_H J_{\\mathcal{G}})^+ = W^TWH + \\lambda \\mathbf{1} $$\n$$ (\\nabla_H J_{\\mathcal{G}})^- = W^TX $$\n因此，$H$ 的乘性更新法则为：\n$$ H \\leftarrow H \\odot \\frac{(W^TX)}{(W^TWH + \\lambda \\mathbf{1})} $$\n\n**$W$ 的更新**：\n关于 $W$ 的梯度（注意正则化器不依赖于 $W$）是：\n$$ \\nabla_W J_{\\mathcal{G}} = -(X - WH)H^T = WHH^T - XH^T $$\n正部和负部是：\n$$ (\\nabla_W J_{\\mathcal{G}})^+ = WHH^T $$\n$$ (\\nabla_W J_{\\mathcal{G}})^- = XH^T $$\n$W$ 的乘性更新法则为：\n$$ W \\leftarrow W \\odot \\frac{(XH^T)}{(WHH^T)} $$\n\n**2. 泊松模型目标函数 ($J_{\\mathcal{P}}$)**\n\n目标函数中与微分相关的部分是 $J_{\\mathcal{P}}(W,H) \\propto \\sum_{i,j} \\left((WH)_{ij} - X_{ij}\\log(WH)_{ij}\\right) + \\lambda \\sum_{k,j} H_{kj}$。\n\n**$H$ 的更新**：\nKL 散度项关于 $H_{kj}$ 的梯度是 $\\sum_i W_{ik} \\left(1 - \\frac{X_{ij}}{(WH)_{ij}}\\right)$。完整的梯度是：\n$$ \\frac{\\partial J_{\\mathcal{P}}}{\\partial H_{kj}} = \\sum_i W_{ik} - \\sum_i W_{ik} \\frac{X_{ij}}{(WH)_{ij}} + \\lambda $$\n以矩阵形式表示，令 $\\mathbf{1}_{n \\times m}$ 为一个全一矩阵：\n$$ \\nabla_H J_{\\mathcal{P}} = W^T\\mathbf{1}_{n \\times m} - W^T(X \\oslash WH) + \\lambda \\mathbf{1} $$\n其中 $\\oslash$ 表示逐元素除法。正部和负部是：\n$$ (\\nabla_H J_{\\mathcal{P}})^+ = W^T\\mathbf{1}_{n \\times m} + \\lambda \\mathbf{1} $$\n$$ (\\nabla_H J_{\\mathcal{P}})^- = W^T(X \\oslash WH) $$\n$H$ 的乘性更新法则为：\n$$ H \\leftarrow H \\odot \\frac{W^T(X \\oslash WH)}{W^T\\mathbf{1}_{n \\times m} + \\lambda \\mathbf{1}} $$\n\n**$W$ 的更新**：\n类似地，关于 $W$ 的梯度是：\n$$ \\nabla_W J_{\\mathcal{P}} = \\mathbf{1}_{n \\times m}H^T - (X \\oslash WH)H^T $$\n正部和负部是：\n$$ (\\nabla_W J_{\\mathcal{P}})^+ = \\mathbf{1}_{n \\times m}H^T $$\n$$ (\\nabla_W J_{\\mathcal{P}})^- = (X \\oslash WH)H^T $$\n$W$ 的乘性更新法则为：\n$$ W \\leftarrow W \\odot \\frac{((X \\oslash WH)H^T)}{(\\mathbf{1}_{n \\times m}H^T)} $$\n为避免数值不稳定，在实现过程中会向所有分母添加一个小的 epsilon 值 $\\epsilon > 0$。\n\n### 归一化\n分解 $X \\approx WH$ 存在固有的尺度模糊性：对于任何标量 $s > 0$，有 $X \\approx (Ws)(H/s)$。这会干扰稀疏性惩罚，因为缩小 $W$ 并扩大 $H$ 会增加惩罚项 $\\lambda||H||_1$，而重构结果 $WH$ 保持不变。为了解决这个问题，在每次更新 $W$ 和 $H$ 之后，我们将 $W$ 的列归一化，使其 $\\ell_1$-范数为 1，并对 $H$ 的行应用逆向缩放。对每个成分 $k=1, \\dots, r$：\n1. 计算列和 $s_k = \\sum_i W_{ik}$。\n2. 更新 $W$ 的第 $k$ 列：$W_{:,k} \\leftarrow W_{:,k} / s_k$。\n3. 更新 $H$ 的第 $k$ 行：$H_{k,:} \\leftarrow H_{k,:} \\cdot s_k$。\n这确保了乘积 $WH$ 保持不变，同时固定了 $W$ 中基向量的尺度。\n\n### 稀疏性差异的科学依据\n\n基于泊松模型的 NMF 预计会比基于高斯模型的 NMF 产生更稀疏的激活矩阵 $H$，尤其对于计数类数据，这源于 KL 散度目标函数的基本性质。\n\n1.  **对零的处理**：神经元脉冲数据通常是稀疏的，意味着数据矩阵 $X$ 中的许多项为零。考虑泊松模型中 $H$ 的更新法则。分子包含项 $W^T(X \\oslash WH)$。如果数据矩阵的第 $j$ 列全为零（$X_{:,j} = 0$），那么分子项中对应的列也会变为零。这会强制 $H$ 的第 $j$ 列的乘性因子为零，从而驱使 $H_{:,j}$ 趋于零。算法自然地学习到，如果一个时间窗内没有活动，则所有成分的激活都应为零。高斯更新法则缺乏这种直接而强大的机制；其分子 $W^TX$ 极不可能恰好为零，因此它在响应零计数数据时不会如此强力地强制稀疏性。\n\n2.  **非对称代价函数**：高斯目标函数对称地惩罚误差：高估一个数据点（$(WH)_{ij} > X_{ij}$）与低估相同量的代价是相同的。KL 散度则是不对称的。对于给定的 $X_{ij} > 0$，代价 $(WH)_{ij} - X_{ij}\\log(WH)_{ij}$ 随 $(WH)_{ij} \\to \\infty$ 线性增长，但随 $(WH)_{ij} \\to 0$ 急剧趋向于 $+\\infty$。这种对低估的强烈惩罚迫使模型捕获非零计数，而对高估的较弱惩罚则更具容许性。关键是，为了最小化总代价，模型被激励在 $X_{ij}$ 包含小计数或噪声的地方将 $(WH)_{ij}$ 设置为非常小的值（接近零），而不是去拟合这些噪声。这种行为有效地“修剪”了 $H$ 中微小、不必要的激活值，从而促进了更稀疏的表示。\n\n总之，泊松模型的似然函数在本质上与计数数据的统计特性相匹配。这导致了能够自然促进稀疏性的更新法则，这一特性仅仅是被显式的 $\\ell_1$ 正则化器所补充，而非创造。基于加性、对称噪声假设的高斯模型则缺乏这种内在的稀疏性诱导结构。",
            "answer": "```python\nimport numpy as np\n\ndef hoyer_sparsity(v):\n    \"\"\"\n    Calculates the Hoyer sparsity index for a non-negative vector.\n    s(v) = (sqrt(N) - ||v||_1 / ||v||_2) / (sqrt(N) - 1)\n    \"\"\"\n    v_flat = v.flatten()\n    N = v_flat.size\n    if N == 1:\n        return 1.0\n\n    norm1 = np.linalg.norm(v_flat, 1)\n    norm2 = np.linalg.norm(v_flat, 2)\n    \n    # Handle the case where the vector is all zeros\n    if norm2  1e-9:\n        return 1.0\n    \n    # Handle the case where sqrt(N) - 1 is zero\n    sqrt_N = np.sqrt(N)\n    if abs(sqrt_N - 1.0)  1e-9:\n        return 1.0\n\n    sparsity = (sqrt_N - norm1 / norm2) / (sqrt_N - 1.0)\n    return sparsity\n\ndef generate_data(n, m, r, data_type, seed):\n    \"\"\"\n    Generates synthetic data matrix X.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Ground truth factors\n    W_true = rng.random((n, r))\n    \n    # Make H_true sparse (e.g., 80% sparsity)\n    H_true_dense = rng.random((r, m))\n    sparsity_mask = rng.random((r, m)) > 0.8\n    H_true = H_true_dense * sparsity_mask\n    \n    # Normalize W_true\n    W_true /= W_true.sum(axis=0, keepdims=True)\n\n    Lambda = W_true @ H_true\n    \n    if data_type == 'poisson':\n        X = rng.poisson(Lambda)\n    elif data_type == 'gaussian':\n        noise_std = 0.1 * np.std(Lambda)\n        noise = rng.normal(0, noise_std, size=(n, m))\n        X = np.maximum(0, Lambda + noise)\n    else:\n        raise ValueError(\"Invalid data_type specified.\")\n        \n    return X\n\ndef nmf(X, r, iterations, lambda_reg, model, init_seed):\n    \"\"\"\n    Performs Non-negative Matrix Factorization.\n    \"\"\"\n    n, m = X.shape\n    rng = np.random.default_rng(init_seed)\n    \n    # Initialize W and H with small positive random values\n    W = rng.random((n, r)) * 0.1 + 1e-4\n    H = rng.random((r, m)) * 0.1 + 1e-4\n    \n    eps = 1e-9\n\n    ones_nm = np.ones((n, m))\n\n    for _ in range(iterations):\n        if model == 'gaussian':\n            # Update H\n            H_num = W.T @ X\n            H_den = W.T @ W @ H + lambda_reg + eps\n            H *= H_num / H_den\n            \n            # Update W\n            W_num = X @ H.T\n            W_den = W @ H @ H.T + eps\n            W *= W_num / W_den\n            \n        elif model == 'poisson':\n            WH = W @ H + eps\n            # Update H\n            H_num = W.T @ (X / WH)\n            H_den = W.T @ ones_nm + lambda_reg + eps\n            H *= H_num / H_den\n            \n            # Update W\n            WH = W @ H + eps\n            W_num = (X / WH) @ H.T\n            W_den = ones_nm @ H.T + eps\n            W *= W_num / W_den\n        \n        # Normalize W columns and rescale H rows\n        w_col_sums = W.sum(axis=0)\n        W /= (w_col_sums + eps)\n        H *= w_col_sums.reshape(r, 1)\n\n    return W, H\n\ndef solve():\n    test_cases = [\n        # n, m, r, iterations, lambda, data_type\n        (30, 200, 4, 200, 0.1, 'poisson'),\n        (30, 200, 4, 200, 0.0, 'poisson'),\n        (30, 200, 4, 200, 1.0, 'poisson'),\n        (30, 200, 4, 200, 0.1, 'gaussian'),\n    ]\n\n    results = []\n    main_rng = np.random.default_rng(42)  # For reproducible sub-seeds\n\n    for i, (n, m, r, iterations, lambda_reg, data_type) in enumerate(test_cases):\n        data_seed = main_rng.integers(10000)\n        init_seed = main_rng.integers(10000)\n        \n        # Generate data\n        X = generate_data(n, m, r, data_type, seed=data_seed)\n        \n        # Run Gaussian-based NMF\n        _, H_g = nmf(X, r, iterations, lambda_reg, 'gaussian', init_seed=init_seed)\n        \n        # Run Poisson-based NMF\n        _, H_p = nmf(X, r, iterations, lambda_reg, 'poisson', init_seed=init_seed)\n        \n        # Compute sparsities\n        s_g = hoyer_sparsity(H_g)\n        s_p = hoyer_sparsity(H_p)\n        \n        # Compare and store result\n        results.append(s_p >= s_g)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}