{
    "hands_on_practices": [
        {
            "introduction": "To truly interpret a principal component, we must understand its relationship with the original variables. This exercise goes to the heart of this relationship by examining loadings, which quantify the correlation between variables and components. You will derive and apply formulas to see how a component's variance is a sum of contributions from each original variable, and conversely, how a variable's variance is partitioned across the principal components, a concept known as communality .",
            "id": "5220656",
            "problem": "A clinical feature extraction pipeline in Artificial Intelligence (AI) for precision medicine considers three standardized laboratory variables: fasting plasma glucose $x_{1}$, triglycerides $x_{2}$, and high-density lipoprotein cholesterol $x_{3}$. Principal component analysis (PCA) is performed on the sample correlation matrix, yielding an orthogonal transformation to uncorrelated components whose variances equal their eigenvalues. Two components are retained with eigenvalues $\\lambda_{1} = 1.8$ and $\\lambda_{2} = 0.9$. The loading matrix for the retained components is \n$$\nL = \\begin{pmatrix}\n\\sqrt{0.9} & 0 \\\\\n\\frac{\\sqrt{1.8}}{2} & \\sqrt{0.45} \\\\\n\\frac{\\sqrt{1.8}}{2} & -\\sqrt{0.45}\n\\end{pmatrix},\n$$\nwhere the entry $L_{ij}$ is the loading of variable $x_{i}$ on component $j$.\n\nStarting from the core definition of PCA as an orthogonal spectral decomposition of the correlation matrix with orthonormal eigenvectors and the property that the variance of each component equals its eigenvalue, derive how to quantify:\n- the fraction of the variance of the first retained component that is attributable to $x_{1}$ using squared loadings, and \n- the cumulative communality of $x_{1}$ across the two retained components as a sum of squared loadings.\n\nThen compute the cumulative communality of $x_{1}$ across the retained components. Express the final answer as a decimal number with no units. No rounding is required.",
            "solution": "The problem requires a derivation of two quantities related to principal component analysis (PCA) and a subsequent calculation. The foundation for these derivations lies in the definitions of principal components, their variances (eigenvalues), and the loadings of the original variables on these components. We begin by formalizing these core concepts.\n\nLet the standardized original variables be denoted by the vector $X = (x_1, x_2, x_3)^T$, where for each variable $x_i$, the mean is $0$ and the variance is $1$. The analysis is performed on the $3 \\times 3$ sample correlation matrix $R$. The principal components, $y_j$, are linear combinations of the original variables. The $j$-th principal component is given by $y_j = e_{1j}x_1 + e_{2j}x_2 + e_{3j}x_3 = e_j^T X$, where $e_j$ is the $j$-th orthonormal eigenvector of $R$. The variance of the $j$-th component is equal to the corresponding eigenvalue $\\lambda_j$, so $\\text{Var}(y_j) = \\lambda_j$.\n\nThe loading of variable $x_i$ on component $y_j$, denoted $L_{ij}$, is defined as their correlation coefficient: $L_{ij} = \\text{Corr}(x_i, y_j)$. This can be expressed in terms of the eigenvectors and eigenvalues.\n$$\nL_{ij} = \\text{Corr}(x_i, y_j) = \\frac{\\text{Cov}(x_i, y_j)}{\\sqrt{\\text{Var}(x_i) \\text{Var}(y_j)}}\n$$\nSince the variables are standardized, $\\text{Var}(x_i) = 1$. The component variance is $\\text{Var}(y_j) = \\lambda_j$. The covariance is $\\text{Cov}(x_i, y_j) = \\text{Cov}(x_i, \\sum_{k=1}^3 e_{kj}x_k) = \\sum_{k=1}^3 e_{kj} \\text{Cov}(x_i, x_k)$. Since $\\text{Cov}(x_i, x_k) = R_{ik}$, this becomes $\\sum_{k=1}^3 R_{ik} e_{kj}$, which is the $i$-th element of the vector $R e_j$. As $e_j$ is an eigenvector of $R$, we have $R e_j = \\lambda_j e_j$. Thus, $\\text{Cov}(x_i, y_j) = \\lambda_j e_{ij}$.\nSubstituting these back into the correlation formula gives the fundamental relationship for loadings:\n$$\nL_{ij} = \\frac{\\lambda_j e_{ij}}{\\sqrt{1 \\cdot \\lambda_j}} = e_{ij}\\sqrt{\\lambda_j}\n$$\n\n**Derivation of the fraction of component variance**\n\nThe first part of the task is to derive how to quantify the fraction of the variance of the first retained component ($y_1$) that is attributable to the variable $x_1$. The total variance of the component $y_1$ is $\\lambda_1$. We need to show how this variance is partitioned among the original variables.\n\nLet's sum the squared loadings for a given component $j$ across all variables $i = 1, 2, 3$:\n$$\n\\sum_{i=1}^3 L_{ij}^2 = \\sum_{i=1}^3 (e_{ij}\\sqrt{\\lambda_j})^2 = \\sum_{i=1}^3 e_{ij}^2 \\lambda_j = \\lambda_j \\sum_{i=1}^3 e_{ij}^2\n$$\nThe vector $e_j = (e_{1j}, e_{2j}, e_{3j})^T$ is an eigenvector of unit length, which means it is normalized: $\\|e_j\\|^2 = e_{1j}^2 + e_{2j}^2 + e_{3j}^2 = 1$.\nTherefore, we arrive at the identity:\n$$\n\\sum_{i=1}^3 L_{ij}^2 = \\lambda_j \\cdot 1 = \\lambda_j\n$$\nThis identity states that the sum of the squared loadings for a principal component $j$ is equal to its variance, $\\lambda_j$. This provides a natural decomposition of the component's variance. The term $L_{ij}^2$ is interpreted as the contribution of variable $x_i$ to the total variance of component $y_j$.\n\nConsequently, the fraction of the variance of component $y_j$ that is attributable to variable $x_i$ is the ratio of this contribution to the total variance:\n$$\n\\text{Fraction for } x_i \\text{ in } y_j = \\frac{L_{ij}^2}{\\lambda_j}\n$$\nFor the specific case requested—the fraction for $x_1$ in the first component $y_1$—we set $i=1$ and $j=1$:\n$$\n\\text{Fraction for } x_1 \\text{ in } y_1 = \\frac{L_{11}^2}{\\lambda_1}\n$$\nThis completes the first requested derivation.\n\n**Derivation of cumulative communality**\n\nThe second part of the task is to derive the quantification for the cumulative communality of $x_1$ across the two retained components. The communality of a variable is the proportion of its variance that is explained by a set of retained components. Since the variables are standardized, $\\text{Var}(x_i) = 1$.\n\nThe set of eigenvectors $\\{e_1, e_2, e_3\\}$ forms an orthonormal basis. We can express the original variable $x_i$ as a linear combination of the principal components $y_j$. The transformation is $Y = E^T X$, where $E$ is the matrix whose columns are the eigenvectors $e_j$. Since $E$ is orthogonal ($E^T = E^{-1}$), the inverse transformation is $X = E Y$. For a specific variable $x_i$, this is:\n$$\nx_i = \\sum_{j=1}^3 E_{ij} y_j = \\sum_{j=1}^3 e_{ij} y_j\n$$\nThe variance of $x_i$ can be calculated from this expression. Since the principal components $y_j$ are uncorrelated, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(x_i) = \\text{Var}\\left(\\sum_{j=1}^3 e_{ij} y_j\\right) = \\sum_{j=1}^3 \\text{Var}(e_{ij} y_j) = \\sum_{j=1}^3 e_{ij}^2 \\text{Var}(y_j) = \\sum_{j=1}^3 e_{ij}^2 \\lambda_j\n$$\nUsing the relationship $L_{ij}^2 = e_{ij}^2 \\lambda_j$, we can rewrite this as:\n$$\n\\text{Var}(x_i) = \\sum_{j=1}^3 L_{ij}^2\n$$\nSince $\\text{Var}(x_i) = 1$, we have the identity $1 = \\sum_{j=1}^3 L_{ij}^2$. This shows that the total variance of a standardized variable is the sum of its squared loadings across all principal components.\n\nThe cumulative communality of a variable $x_i$ across a set of $k$ retained components is the portion of its variance accounted for by those components. This is simply the sum of the corresponding squared loadings. For $k=2$ retained components, the cumulative communality of $x_i$ is:\n$$\nh_i^2 = \\sum_{j=1}^2 L_{ij}^2 = L_{i1}^2 + L_{i2}^2\n$$\nFor the specific case of variable $x_1$, the cumulative communality across the two retained components is:\n$$\nh_1^2 = L_{11}^2 + L_{12}^2\n$$\nThis completes the second requested derivation.\n\n**Calculation of the cumulative communality of $x_1$**\n\nWe are asked to compute the cumulative communality of $x_1$ across the two retained components. Based on the derivation above, this quantity is $h_1^2 = L_{11}^2 + L_{12}^2$.\nThe problem provides the loading matrix for the two retained components:\n$$\nL = \\begin{pmatrix}\n\\sqrt{0.9} & 0 \\\\\n\\frac{\\sqrt{1.8}}{2} & \\sqrt{0.45} \\\\\n\\frac{\\sqrt{1.8}}{2} & -\\sqrt{0.45}\n\\end{pmatrix}\n$$\nThe entries in the first row correspond to the variable $x_1$. Thus, we have:\n-   $L_{11}$, the loading of $x_1$ on the first component, is $\\sqrt{0.9}$.\n-   $L_{12}$, the loading of $x_1$ on the second component, is $0$.\n\nSubstituting these values into the formula for communality:\n$$\nh_1^2 = (\\sqrt{0.9})^2 + (0)^2 = 0.9 + 0 = 0.9\n$$\nThe cumulative communality of $x_1$ across the two retained components is $0.9$. This means that $90\\%$ of the variance in fasting plasma glucose ($x_1$) is explained by the first two principal components.",
            "answer": "$$\\boxed{0.9}$$"
        },
        {
            "introduction": "A crucial step in any PCA workflow is deciding how many components to retain and interpret, as selecting too few can miss important structure, while selecting too many incorporates noise. This practice moves beyond simple rules of thumb by implementing Horn's parallel analysis, a robust statistical method for this very purpose . You will simulate eigenvalues from null data to create a statistical baseline, allowing you to distinguish components that represent true signal from those that likely arise from random sampling.",
            "id": "5220669",
            "problem": "You are given the task of implementing Horn’s parallel analysis for Principal Component Analysis (PCA) in a way that is reproducible and testable. Start from the following fundamental base: given a data matrix $X \\in \\mathbb{R}^{n \\times p}$, define the column-centered matrix $X_c$ by subtracting the sample mean of each column, define the sample covariance matrix $S$ as $S = \\frac{1}{n-1} X_c^\\top X_c$, and define the sample correlation matrix $R$ by standardizing each column of $X_c$ to unit variance prior to forming $S$. By the spectral theorem, $R$ is symmetric and thus has real eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p \\ge 0$. The eigenvalues quantify the variance explained by each principal component in the correlation metric. Under the null hypothesis that the $p$ variables are mutually independent with finite variance (so that the population correlation matrix is the identity matrix), finite-sample effects induce fluctuations in the eigenvalues of the sample correlation matrix away from $1$. Horn’s idea is to estimate the sampling distribution of the eigenvalues under the null hypothesis by generating null data and comparing the observed eigenvalues to a null criterion.\n\nImplement the following two null-data generation mechanisms, each of which encodes the null hypothesis of no cross-variable structure while controlling finite-sample effects:\n\n- Permutation-based null: independently permute the entries within each column of $X$ to destroy cross-variable dependence while preserving each variable’s univariate empirical distribution and marginal variance.\n- Gaussian-based null: generate a matrix $Z \\in \\mathbb{R}^{n \\times p}$ whose entries are independent draws from a standard normal distribution, and analyze its sample correlation eigenvalues.\n\nUse the following decision rule for retaining components. Let $\\lambda^{\\mathrm{obs}}_j$ denote the $j$-th largest eigenvalue of the observed sample correlation matrix, and let $\\{\\lambda^{\\mathrm{null},(s)}_j\\}_{s=1}^m$ be the $j$-th largest eigenvalues computed from $m$ independent null replications. Define a null criterion $c_j$ either as the empirical mean $c_j = \\frac{1}{m} \\sum_{s=1}^m \\lambda^{\\mathrm{null},(s)}_j$ (mean-threshold rule) or as the empirical $q$-quantile $c_j$ such that a proportion $q$ of $\\{\\lambda^{\\mathrm{null},(s)}_j\\}$ are less than or equal to $c_j$ (quantile-threshold rule). Retain exactly those components whose observed eigenvalues exceed the corresponding null criteria, that is, retain the number $k$ given by $k = \\left| \\{ j \\in \\{1,\\dots,p\\} : \\lambda^{\\mathrm{obs}}_j > c_j \\} \\right|$.\n\nYour program must do the following, without any user input, using only the specified runtime environment. For each test case below, generate the data $X$ as described, compute the observed correlation eigenvalues, simulate null eigenvalues using the specified null mode with the specified number of replications $m$, compute the corresponding null criteria, apply the decision rule, and output the number of retained components $k$ as an integer. All random number generation must be reproducible using the stated seeds. Use the correlation matrix for PCA in all cases.\n\nTest suite. For each case, $n$ is the number of observations, $p$ is the number of variables, $m$ is the number of null replications, $q$ is the quantile used when the quantile rule is requested, and an explicit random seed is specified. In cases with latent structure, generate data from a factor model $X = F L^\\top + \\sigma E$ where $F \\in \\mathbb{R}^{n \\times r}$ has independent standard normal entries, $L \\in \\mathbb{R}^{p \\times r}$ is a fixed loading matrix described below, $E \\in \\mathbb{R}^{n \\times p}$ has independent standard normal entries, and $\\sigma > 0$ is a noise scale. After generating $X$, do not perform any additional rescaling beyond the correlation-standardization internal to PCA. Use a fresh pseudorandom generator initialized at the given seed for generating $F$ and $E$ in each case, and for null generation initialize a separate pseudorandom generator with the seed offset by adding $911$ to the given seed.\n\n- Test case A (two strong factors, Gaussian-null, quantile rule): $n = 400$, $p = 6$, $r = 2$, seed $= 7$, noise scale $\\sigma = 0.3$, loading matrix $L \\in \\mathbb{R}^{6 \\times 2}$ defined by $L_{j1} = 0.9$, $L_{j2} = 0.1$ for $j \\in \\{1,2,3\\}$ and $L_{j1} = 0.1$, $L_{j2} = 0.9$ for $j \\in \\{4,5,6\\}$, null mode $=$ Gaussian, $m = 1000$, quantile $q = 0.95$.\n- Test case B (same data-generating mechanism as A, Gaussian-null, mean rule): identical to A except use the mean-threshold rule instead of the quantile rule and set $m = 1000$.\n- Test case C (one dominant factor, permutation-null, quantile rule): $n = 250$, $p = 5$, $r = 1$, seed $= 11$, noise scale $\\sigma = 0.2$, loading matrix $L \\in \\mathbb{R}^{5 \\times 1}$ with all entries equal to $0.9$, null mode $=$ permutation, $m = 1000$, quantile $q = 0.95$.\n- Test case D (pure noise, Gaussian-null, stricter quantile rule): $n = 500$, $p = 10$, seed $= 29$, generate $X$ as independent standard normal entries (that is, set $r = 0$ and ignore $F L^\\top$), null mode $=$ Gaussian, $m = 1500$, quantile $q = 0.99$.\n- Test case E (two strong factors, small sample, permutation-null, quantile rule): $n = 60$, $p = 8$, $r = 2$, seed $= 31$, noise scale $\\sigma = 0.25$, loading matrix $L \\in \\mathbb{R}^{8 \\times 2}$ defined by $L_{j1} = 0.85$, $L_{j2} = 0.1$ for $j \\in \\{1,2,3,4\\}$ and $L_{j1} = 0.1$, $L_{j2} = 0.85$ for $j \\in \\{5,6,7,8\\}$, null mode $=$ permutation, $m = 1200$, quantile $q = 0.95$.\n\nDecision rule specification. For the quantile rule, for each component index $j \\in \\{1,\\dots,p\\}$, compute $c_j$ as the empirical $q$-quantile of the simulated null eigenvalues at position $j$ across $m$ replications, then retain component $j$ if and only if $\\lambda^{\\mathrm{obs}}_j > c_j$. For the mean rule, replace $c_j$ by the empirical mean of the null eigenvalues at position $j$.\n\nFinal output format. Your program should produce a single line of output containing a comma-separated list of the $k$ values for the five test cases, in order A through E, enclosed in square brackets; for example, an output of the form $[k_A,k_B,k_C,k_D,k_E]$. The output entries must be integers. No additional text or lines should be printed.",
            "solution": "The problem requires the implementation of Horn's parallel analysis, a statistical procedure for determining the number of components to retain in a Principal Component Analysis (PCA). The method is based on a comparison between the eigenvalues derived from the observed data and those derived from null-hypothesis data, where no underlying correlational structure is assumed to exist. This principled approach helps to distinguish between components representing substantive variance (signal) and those representing random sampling fluctuation (noise). The implementation will be validated against several test cases with specified data generation mechanisms and decision rules.\n\nThe core algorithm of parallel analysis proceeds in four steps:\n1.  Compute the eigenvalues from the sample correlation matrix of the observed data.\n2.  Generate multiple datasets under a null hypothesis of variable independence and compute the eigenvalues for each.\n3.  Establish a threshold criterion for each component based on the distribution of simulated null eigenvalues.\n4.  Retain the number of observed components whose eigenvalues exceed their corresponding criterion.\n\nLet the observed data be given by a matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of observations and $p$ is the number of variables.\n\n**Step 1: Calculation of Observed Eigenvalues**\n\nThe analysis is performed on the sample correlation matrix $R$, which is invariant to linear transformations of the individual variables. The calculation of $R$ involves centering and scaling the data. First, the column-centered matrix $X_c$ is computed by subtracting the mean of each column from its entries:\n$$\n(X_c)_{ij} = X_{ij} - \\bar{X}_j, \\quad \\text{where} \\quad \\bar{X}_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij}\n$$\nNext, each column of $X_c$ is standardized to have a sample variance of $1$. Let $\\hat{\\sigma}_j$ be the sample standard deviation of the $j$-th column, calculated as $\\hat{\\sigma}_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_{ij} - \\bar{X}_j)^2}$. The standardized data matrix $X_{\\text{std}}$ is then:\n$$\n(X_{\\text{std}})_{ij} = \\frac{(X_c)_{ij}}{\\hat{\\sigma}_j}\n$$\nThe sample correlation matrix $R$ is computed from $X_{\\text{std}}$:\n$$\nR = \\frac{1}{n-1} X_{\\text{std}}^\\top X_{\\text{std}}\n$$\nBy the spectral theorem for real symmetric matrices, $R$ possesses $p$ real eigenvalues, which we order from largest to smallest: $\\lambda^{\\mathrm{obs}}_1 \\ge \\lambda^{\\mathrm{obs}}_2 \\ge \\cdots \\ge \\lambda^{\\mathrm{obs}}_p \\ge 0$. These are the \"observed\" eigenvalues.\n\n**Step 2: Simulation of Null Eigenvalue Distributions**\n\nThe null hypothesis $H_0$ posits that the $p$ variables are mutually uncorrelated in the population. To estimate the distribution of sample eigenvalues under $H_0$, we generate $m$ null datasets, each of size $n \\times p$, and compute their respective correlation matrix eigenvalues. The problem specifies two mechanisms for generating null data, each preserving certain properties of the original data while destroying the correlation structure.\n\n*   **Gaussian-based Null:** A matrix $Z \\in \\mathbb{R}^{n \\times p}$ is generated with entries being independent and identically distributed draws from a standard normal distribution, $Z_{ij} \\sim \\mathcal{N}(0, 1)$. This model assumes that under the null hypothesis, the data originates from a multivariate normal distribution with a population correlation matrix equal to the identity matrix $I$.\n*   **Permutation-based Null:** For each column $j$ of the original data matrix $X$, the $n$ entries are randomly permuted. This process is performed independently for each of the $p$ columns. This method destroys the inter-variable correlations while precisely preserving the marginal empirical distribution of each variable. It is a non-parametric approach that does not make distributional assumptions like normality.\n\nFor each of the $m$ null replications, indexed by $s \\in \\{1, \\dots, m\\}$, a null data matrix $X_{\\text{null}}^{(s)}$ is generated. The corresponding sample correlation matrix $R_{\\text{null}}^{(s)}$ is computed, and its eigenvalues $\\{\\lambda^{\\mathrm{null},(s)}_j\\}_{j=1}^p$ are calculated and sorted in descending order. This process yields an $m \\times p$ matrix of null eigenvalues, where each column represents the empirical sampling distribution for the $j$-th ordered eigenvalue under the null hypothesis.\n\n**Step 3: Establishment of the Null Criterion**\n\nFrom the simulated null eigenvalue distributions, a decision criterion $c_j$ is established for each component $j \\in \\{1, \\dots, p\\}$. The problem defines two rules for this:\n\n*   **Mean-threshold rule:** The criterion $c_j$ is the average of the $j$-th eigenvalues across all $m$ null simulations:\n    $$\n    c_j = \\frac{1}{m} \\sum_{s=1}^m \\lambda^{\\mathrm{null},(s)}_j\n    $$\n*   **Quantile-threshold rule:** The criterion $c_j$ is the empirical $q$-quantile of the distribution of the $j$-th null eigenvalues. For a given quantile $q \\in (0, 1)$, $c_j$ is a value such that approximately a fraction $q$ of the simulated eigenvalues $\\{\\lambda^{\\mathrm{null},(s)}_j\\}_{s=1}^m$ are less than or equal to it.\n\n**Step 4: Application of the Decision Rule**\n\nThe final step is to compare the observed eigenvalues to the established criteria. A component $j$ is considered to represent a statistically significant source of variance if its observed eigenvalue is greater than the corresponding null-hypothesis criterion. The total number of components to retain, $k$, is the count of such components:\n$$\nk = \\left| \\{ j \\in \\{1, \\dots, p\\} : \\lambda^{\\mathrm{obs}}_j > c_j \\} \\right|\n$$\nThis procedure is applied to each test case, using the specified parameters, random seeds, data generation models ($X = F L^\\top + \\sigma E$ or pure noise), and decision rules to determine the value of $k$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Horn's parallel analysis for PCA and runs a suite of test cases.\n    \"\"\"\n\n    def _get_correlation_eigenvalues(X):\n        \"\"\"\n        Computes the eigenvalues of the sample correlation matrix of X.\n        \n        Args:\n            X (np.ndarray): Data matrix of shape (n, p).\n        \n        Returns:\n            np.ndarray: A 1D array of p eigenvalues, sorted in descending order.\n        \"\"\"\n        n, _ = X.shape\n        \n        # Center the data\n        X_c = X - X.mean(axis=0)\n        \n        # Standardize the data to have unit sample variance\n        # ddof=1 for sample standard deviation\n        col_stds = X_c.std(axis=0, ddof=1)\n        \n        # Avoid division by zero for columns with zero variance\n        col_stds[col_stds == 0] = 1.0\n        \n        X_std = X_c / col_stds\n        \n        # Compute the correlation matrix\n        R = (X_std.T @ X_std) / (n - 1)\n        \n        # Compute eigenvalues of the symmetric matrix R\n        # eigvalsh is efficient for Hermitian (real-symmetric) matrices\n        eigenvalues = np.linalg.eigvalsh(R)\n        \n        # Sort eigenvalues in descending order\n        return np.sort(eigenvalues)[::-1]\n\n    def run_parallel_analysis(\n        n, p, seed, m, null_mode, rule,\n        r=0, L=None, sigma=None, q=None\n    ):\n        \"\"\"\n        Runs a single instance of Horn's parallel analysis.\n        \"\"\"\n        # 1. Generate observed data and its eigenvalues\n        data_rng = np.random.default_rng(seed)\n        \n        if r > 0 and L is not None and sigma is not None:\n            F = data_rng.standard_normal((n, r))\n            E = data_rng.standard_normal((n, p))\n            X_obs = F @ L.T + sigma * E\n        else: # Pure noise case\n            X_obs = data_rng.standard_normal((n, p))\n            \n        obs_eigenvalues = _get_correlation_eigenvalues(X_obs)\n\n        # 2. Simulate null eigenvalues\n        null_rng = np.random.default_rng(seed + 911)\n        null_eigenvalues_matrix = np.zeros((m, p))\n\n        for s in range(m):\n            if null_mode == 'Gaussian':\n                X_null = null_rng.standard_normal((n, p))\n            elif null_mode == 'permutation':\n                X_null = np.zeros((n, p))\n                for j in range(p):\n                    X_null[:, j] = null_rng.permutation(X_obs[:, j])\n            else:\n                raise ValueError(\"Invalid null_mode specified.\")\n            \n            null_eigenvalues_matrix[s, :] = _get_correlation_eigenvalues(X_null)\n            \n        # 3. Establish the null criterion\n        if rule == 'mean':\n            criteria = np.mean(null_eigenvalues_matrix, axis=0)\n        elif rule == 'quantile' and q is not None:\n            criteria = np.quantile(null_eigenvalues_matrix, q, axis=0)\n        else:\n            raise ValueError(\"Invalid rule or missing q for quantile rule.\")\n\n        # 4. Apply the decision rule\n        k = np.sum(obs_eigenvalues > criteria)\n        return k\n\n    # --- Test Suite ---\n    \n    test_cases = [\n        # Case A\n        {'n': 400, 'p': 6, 'r': 2, 'seed': 7, 'sigma': 0.3,\n         'L': np.array([[0.9, 0.1]]*3 + [[0.1, 0.9]]*3),\n         'null_mode': 'Gaussian', 'm': 1000, 'rule': 'quantile', 'q': 0.95},\n        # Case B\n        {'n': 400, 'p': 6, 'r': 2, 'seed': 7, 'sigma': 0.3,\n         'L': np.array([[0.9, 0.1]]*3 + [[0.1, 0.9]]*3),\n         'null_mode': 'Gaussian', 'm': 1000, 'rule': 'mean', 'q': None},\n        # Case C\n        {'n': 250, 'p': 5, 'r': 1, 'seed': 11, 'sigma': 0.2,\n         'L': np.full((5, 1), 0.9),\n         'null_mode': 'permutation', 'm': 1000, 'rule': 'quantile', 'q': 0.95},\n        # Case D\n        {'n': 500, 'p': 10, 'r': 0, 'seed': 29, 'sigma': None, 'L': None,\n         'null_mode': 'Gaussian', 'm': 1500, 'rule': 'quantile', 'q': 0.99},\n        # Case E\n        {'n': 60, 'p': 8, 'r': 2, 'seed': 31, 'sigma': 0.25,\n         'L': np.array([[0.85, 0.1]]*4 + [[0.1, 0.85]]*4),\n         'null_mode': 'permutation', 'm': 1200, 'rule': 'quantile', 'q': 0.95}\n    ]\n\n    results = []\n    for case_params in test_cases:\n        k = run_parallel_analysis(**case_params)\n        results.append(k)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "When applying PCA to multiple datasets, such as recordings from different sessions or subjects, a notorious technical challenge arises: the sign of principal component loadings is arbitrary. This sign indeterminacy can make direct comparisons misleading, but it can be solved by aligning components to a consistent orientation . In this hands-on coding practice, you will implement a standard technique to enforce sign consistency, ensuring that your interpretations of neural population dynamics are stable and comparable across experiments.",
            "id": "4172070",
            "problem": "You are analyzing multi-session neuronal population recordings and applying Principal Component Analysis (PCA) to the same set of features across sessions. In Neuroscience data analysis, Principal Component Analysis (PCA) produces principal component loading vectors whose direction is defined only up to a sign, meaning that if $v$ is a principal component loading vector, then $-v$ represents the same subspace and explains the same variance. This sign indeterminacy impedes interpretation across sessions unless a consistent orientation convention is enforced. A common and scientifically grounded convention is to orient each principal component by requiring that its inner product with an interpretable anchor vector is nonnegative.\n\nStarting from the fundamental base:\n- The empirical covariance matrix $C \\in \\mathbb{R}^{F \\times F}$ of features is defined by the sample average of centered outer products, and its eigenvectors are the principal component loading directions.\n- If $u$ is an eigenvector of $C$ associated with eigenvalue $\\lambda$, then $-u$ is also an eigenvector associated with the same eigenvalue $\\lambda$. Therefore, PCA loading vectors are directionally ambiguous with respect to sign.\n- Inner products (dot products) encode alignment in $\\mathbb{R}^{F}$, and the sign of an inner product $w^{\\top} l$ indicates whether $l$ points to the same half-space as $w$ or the opposite.\n\nYou are given, for multiple test cases, a set of session loading matrices and a set of anchor vectors that define the desired orientation of each component. You must enforce sign consistency across sessions by flipping the sign of any session’s component loading vector whose inner product with the corresponding anchor vector is negative. If the inner product equals zero, do not flip. After alignment, you must report two quantities per test case: the total number of sign flips performed across all sessions and components, and the minimum anchor inner product value across all sessions and components.\n\nDefinitions and notation:\n- Let there be $S$ sessions, $F$ features, and $K$ components. For session $s \\in \\{0, \\ldots, S-1\\}$, let $L^{(s)} \\in \\mathbb{R}^{F \\times K}$ denote the loading matrix, with column $k$ denoted $l^{(s)}_{k} \\in \\mathbb{R}^{F}$.\n- Let the anchors be $W \\in \\mathbb{R}^{F \\times K}$, with column $k$ denoted $w_{k} \\in \\mathbb{R}^{F}$. The orientation rule is: if $w_{k}^{\\top} l^{(s)}_{k} < 0$, replace $l^{(s)}_{k}$ by $-l^{(s)}_{k}$; if $w_{k}^{\\top} l^{(s)}_{k} \\ge 0$, leave it unchanged.\n- The alignment summary metrics are:\n  1. The integer total flip count $N_{\\text{flip}} = \\sum_{s=0}^{S-1} \\sum_{k=1}^{K} \\mathbf{1}\\{w_{k}^{\\top} l^{(s)}_{k} < 0\\}$.\n  2. The float minimum aligned anchor inner product $m_{\\min} = \\min_{s,k} w_{k}^{\\top} \\tilde{l}^{(s)}_{k}$, where $\\tilde{l}^{(s)}_{k}$ denotes the potentially flipped loading vector after enforcing the rule.\n\nTask:\n- Implement a program that applies the above orientation rule using the provided test suite. Compute $N_{\\text{flip}}$ and $m_{\\min}$ for each test case. The float $m_{\\min}$ must be rounded to six decimal places. There are no physical units involved.\n\nAssumptions:\n- Component identities across sessions are already matched by index $k$, and only sign consistency must be enforced.\n- All vectors and matrices are real-valued, and feature dimensionalities are consistent within each test case.\n\nTest suite:\n- Test case $1$ ($S=2$, $F=4$, $K=2$):\n  - Session $0$: columns $l^{(0)}_{1}$ and $l^{(0)}_{2}$ in\n    $$\n    L^{(0)} = \\begin{bmatrix}\n    0.8 & -0.2 \\\\\n    0.1 & 0.9 \\\\\n    0.0 & 0.05 \\\\\n    -0.1 & 0.0\n    \\end{bmatrix}.\n    $$\n  - Session $1$:\n    $$\n    L^{(1)} = -L^{(0)} = \\begin{bmatrix}\n    -0.8 & 0.2 \\\\\n    -0.1 & -0.9 \\\\\n    -0.0 & -0.05 \\\\\n    0.1 & -0.0\n    \\end{bmatrix}.\n    $$\n  - Anchors (columns $w_{1}$, $w_{2}$):\n    $$\n    W = \\begin{bmatrix}\n    1 & 0 \\\\\n    0 & 1 \\\\\n    0 & 0 \\\\\n    0 & 0\n    \\end{bmatrix}.\n    $$\n- Test case $2$ ($S=3$, $F=5$, $K=3$):\n  - Session $0$:\n    $$\n    L^{(0)} = \\begin{bmatrix}\n    0.5 & 0.0 & 0.0 \\\\\n    0.2 & -0.1 & 0.1 \\\\\n    0.1 & 0.7 & 0.0 \\\\\n    0.0 & 0.2 & 0.6 \\\\\n    0.1 & 0.0 & 0.3\n    \\end{bmatrix}.\n    $$\n  - Session $1$ (component $2$ flipped relative to anchors):\n    $$\n    L^{(1)} = \\begin{bmatrix}\n    0.5 & -0.0 & 0.0 \\\\\n    0.2 & 0.1 & 0.1 \\\\\n    0.1 & -0.7 & 0.0 \\\\\n    0.0 & -0.2 & 0.6 \\\\\n    0.1 & -0.0 & 0.3\n    \\end{bmatrix}.\n    $$\n  - Session $2$ (components $1$ and $3$ flipped relative to anchors):\n    $$\n    L^{(2)} = \\begin{bmatrix}\n    -0.5 & 0.0 & -0.0 \\\\\n    -0.2 & -0.1 & -0.1 \\\\\n    -0.1 & 0.7 & -0.0 \\\\\n    -0.0 & 0.2 & -0.6 \\\\\n    -0.1 & 0.0 & -0.3\n    \\end{bmatrix}.\n    $$\n  - Anchors:\n    $$\n    W = \\begin{bmatrix}\n    1 & 0 & 0 \\\\\n    0 & 0 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 0 & 1 \\\\\n    0 & 0 & 0\n    \\end{bmatrix}.\n    $$\n- Test case $3$ ($S=2$, $F=3$, $K=2$):\n  - Session $0$:\n    $$\n    L^{(0)} = \\begin{bmatrix}\n    1.0 & 0.0 \\\\\n    0.0 & 0.0 \\\\\n    0.0 & 0.9\n    \\end{bmatrix}.\n    $$\n  - Session $1$:\n    $$\n    L^{(1)} = \\begin{bmatrix}\n    -1.0 & 0.0 \\\\\n    -0.0 & 0.0 \\\\\n    -0.0 & -0.9\n    \\end{bmatrix}.\n    $$\n  - Anchors:\n    $$\n    W = \\begin{bmatrix}\n    0 & 0 \\\\\n    1 & 0 \\\\\n    0 & 1\n    \\end{bmatrix}.\n    $$\n  In this case, note that for component $1$, the anchor inner product is $0$ both before and after any flip, so no flip should be applied for zero inner product.\n\nRequired output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case $i \\in \\{1,2,3\\}$, output two values in order: the integer $N_{\\text{flip}}$ followed by the float $m_{\\min}$ rounded to six decimal places. Thus the final line contains $6$ values in total in the order $[N_{\\text{flip}}^{(1)}, m_{\\min}^{(1)}, N_{\\text{flip}}^{(2)}, m_{\\min}^{(2)}, N_{\\text{flip}}^{(3)}, m_{\\min}^{(3)}]$.",
            "solution": "The problem is scientifically and mathematically valid. It addresses a fundamental and practical issue in the application of Principal Component Analysis (PCA) to multi-session or multi-subject data, specifically the sign ambiguity of eigenvectors. The method proposed for resolving this ambiguity—orienting each principal component (PC) loading vector relative to a predefined anchor vector—is a standard, interpretable, and computationally straightforward technique. The problem is well-posed, with all necessary data and definitions provided to compute a unique solution.\n\nThe core principle is that for any symmetric matrix, such as the empirical covariance matrix $C \\in \\mathbb{R}^{F \\times F}$ of $F$ features, if $u \\in \\mathbb{R}^F$ is an eigenvector with eigenvalue $\\lambda$, then $-u$ is also an eigenvector with the same eigenvalue $\\lambda$, since $C(-u) = -Cu = -(\\lambda u) = \\lambda(-u)$. The PC loading vectors are the eigenvectors of $C$, and thus their direction is arbitrary up to a sign. This ambiguity hinders direct comparison of loading vectors across different datasets (e.g., recording sessions).\n\nTo enforce a consistent orientation, we introduce a set of anchor vectors, $W \\in \\mathbb{R}^{F \\times K}$, where $K$ is the number of principal components. Each column $w_k$ of $W$ serves as a reference for the $k$-th PC. The orientation of the loading vector $l_k^{(s)}$ for component $k$ from session $s$ is determined by the sign of its inner product with the corresponding anchor $w_k$. The inner product, $w_k^\\top l_k^{(s)}$, measures the projection of $l_k^{(s)}$ onto $w_k$. A positive sign indicates alignment in the same general direction, while a negative sign indicates alignment in the opposite direction.\n\nThe prescribed orientation rule is to ensure this inner product is always non-negative: for every session $s \\in \\{0, \\ldots, S-1\\}$ and component $k \\in \\{1, \\ldots, K\\}$, the loading vector must satisfy $w_k^\\top \\tilde{l}_k^{(s)} \\ge 0$, where $\\tilde{l}_k^{(s)}$ is the final, aligned loading vector.\n\nThe algorithm to achieve this is as follows:\n1.  For each session $s$ and each component $k$, calculate the inner product $p_{s,k} = w_k^\\top l_k^{(s)}$.\n2.  Critically examine the sign of $p_{s,k}$. If $p_{s,k} < 0$, the loading vector $l_k^{(s)}$ is misaligned with its anchor. To correct this, we flip its sign. The aligned vector becomes $\\tilde{l}_k^{(s)} = -l_k^{(s)}$. This action is counted as a \"sign flip\". The new, aligned inner product is $w_k^\\top \\tilde{l}_k^{(s)} = w_k^\\top (-l_k^{(s)}) = -p_{s,k}$, which is guaranteed to be positive.\n3.  If $p_{s,k} \\ge 0$, the loading vector $l_k^{(s)}$ is already correctly oriented (or is orthogonal to the anchor, in which case its orientation is not changed). The aligned vector is simply $\\tilde{l}_k^{(s)} = l_k^{(s)}$, and no flip is counted. The aligned inner product remains $p_{s,k}$.\n\nAfter applying this rule to all loading vectors, we compute two summary metrics for each test case:\n1.  The total number of sign flips, $N_{\\text{flip}}$, is the sum of all instances where the condition $w_k^\\top l\n_k^{(s)} < 0$ was met:\n    $$\n    N_{\\text{flip}} = \\sum_{s=0}^{S-1} \\sum_{k=1}^{K} \\mathbf{1}\\{w_{k}^{\\top} l^{(s)}_{k} < 0\\}\n    $$\n    where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, equal to $1$ if its argument is true and $0$ otherwise.\n\n2.  The minimum aligned anchor inner product, $m_{\\min}$, is the minimum value among all inner products after a consistent orientation has been enforced. By construction, all aligned inner products are non-negative.\n    $$\n    m_{\\min} = \\min_{s,k} \\{w_{k}^{\\top} \\tilde{l}^{(s)}_{k}\\}\n    $$\n    This value quantifies the \"worst-case\" alignment between any component and its anchor across all sessions after correction.\n\nThis entire procedure can be implemented efficiently using matrix operations. For each session $s$, the set of $K$ inner products $\\{w_k^\\top l_k^{(s)}\\}_{k=1}^K$ can be computed by taking the element-wise product of the matrices $W$ and $L^{(s)}$ and summing along the feature dimension (axis $0$). Subsequently, a boolean mask can identify which components require flipping, and the aligned inner products can be computed by taking the absolute value of the original inner products. This process is repeated for each session, and the results are aggregated to find $N_{\\text{flip}}$ and $m_{\\min}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA loading vector alignment problem for the given test suite.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (S=2, F=4, K=2)\n        {\n            \"session_loadings\": [\n                np.array([\n                    [0.8, -0.2],\n                    [0.1, 0.9],\n                    [0.0, 0.05],\n                    [-0.1, 0.0]\n                ]),\n                np.array([\n                    [-0.8, 0.2],\n                    [-0.1, -0.9],\n                    [-0.0, -0.05],\n                    [0.1, -0.0]\n                ])\n            ],\n            \"anchors\": np.array([\n                [1, 0],\n                [0, 1],\n                [0, 0],\n                [0, 0]\n            ])\n        },\n        # Test case 2 (S=3, F=5, K=3)\n        {\n            \"session_loadings\": [\n                np.array([\n                    [0.5, 0.0, 0.0],\n                    [0.2, -0.1, 0.1],\n                    [0.1, 0.7, 0.0],\n                    [0.0, 0.2, 0.6],\n                    [0.1, 0.0, 0.3]\n                ]),\n                np.array([\n                    [0.5, -0.0, 0.0],\n                    [0.2, 0.1, 0.1],\n                    [0.1, -0.7, 0.0],\n                    [0.0, -0.2, 0.6],\n                    [0.1, -0.0, 0.3]\n                ]),\n                np.array([\n                    [-0.5, 0.0, -0.0],\n                    [-0.2, -0.1, -0.1],\n                    [-0.1, 0.7, -0.0],\n                    [-0.0, 0.2, -0.6],\n                    [-0.1, 0.0, -0.3]\n                ])\n            ],\n            \"anchors\": np.array([\n                [1, 0, 0],\n                [0, 0, 0],\n                [0, 1, 0],\n                [0, 0, 1],\n                [0, 0, 0]\n            ])\n        },\n        # Test case 3 (S=2, F=3, K=2)\n        {\n            \"session_loadings\": [\n                np.array([\n                    [1.0, 0.0],\n                    [0.0, 0.0],\n                    [0.0, 0.9]\n                ]),\n                np.array([\n                    [-1.0, 0.0],\n                    [-0.0, 0.0],\n                    [-0.0, -0.9]\n                ])\n            ],\n            \"anchors\": np.array([\n                [0, 0],\n                [1, 0],\n                [0, 1]\n            ])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        session_loadings = case[\"session_loadings\"]\n        W = case[\"anchors\"]\n        \n        total_flip_count = 0\n        all_aligned_products = []\n\n        for L_s in session_loadings:\n            # For each component k, compute the inner product w_k^T * l_k^(s)\n            # This can be done for all components at once via element-wise\n            # multiplication and summing over the feature dimension (axis=0).\n            inner_products = np.sum(W * L_s, axis=0)\n            \n            # Count the number of flips needed for this session.\n            # A flip is needed if the inner product is negative.\n            num_flips_session = np.sum(inner_products  0)\n            total_flip_count += num_flips_session\n            \n            # After alignment, the inner product w_k^T * l_tilde_k^(s)\n            # will be positive if a flip occurred, and its original non-negative\n            # value otherwise. This is equivalent to taking the absolute value\n            # of the original inner product, since if it was >= 0 it is unchanged,\n            # and if it was  0 it becomes -p > 0.\n            aligned_products = np.abs(inner_products)\n            \n            all_aligned_products.extend(aligned_products)\n\n        # Find the minimum of all aligned inner products.\n        # If there are no components/sessions, handle that edge case,\n        # though problem constraints imply this list is non-empty.\n        if all_aligned_products:\n            min_aligned_product = np.min(all_aligned_products)\n        else:\n            # Fallback for empty case, though not expected\n            min_aligned_product = 0.0\n\n        results.append(str(total_flip_count))\n        results.append(f\"{min_aligned_product:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}