{
    "hands_on_practices": [
        {
            "introduction": "诠释主成分的第一步是理解其与原始变量的关系。本练习将重点关注从载荷（loadings）中派生的两个关键指标：单个变量对某个成分方差的贡献，以及共同度（communality），即模型所能解释的单个变量的方差比例。这项练习将主成分的抽象概念与具体测量联系起来，为后续的科学解释奠定定量基础。",
            "id": "5220656",
            "problem": "在用于精准医疗的人工智能 (AI) 临床特征提取流程中，考虑了三个标准化的实验室变量：空腹血糖 $x_{1}$、甘油三酯 $x_{2}$ 和高密度脂蛋白胆固醇 $x_{3}$。对样本相关性矩阵进行主成分分析 (PCA; Principal Component Analysis)，得到一个正交变换，将变量转换为不相关的成分，这些成分的方差等于其特征值。保留了两个主成分，其特征值为 $\\lambda_{1} = 1.8$ 和 $\\lambda_{2} = 0.9$。保留成分的载荷矩阵为 \n$$\nL = \\begin{pmatrix}\n\\sqrt{0.9}  0 \\\\\n\\frac{\\sqrt{1.8}}{2}  \\sqrt{0.45} \\\\\n\\frac{\\sqrt{1.8}}{2}  -\\sqrt{0.45}\n\\end{pmatrix},\n$$\n其中元素 $L_{ij}$ 是变量 $x_{i}$ 在成分 $j$ 上的载荷。\n\n从 PCA 的核心定义出发，即相关性矩阵使用标准正交特征向量进行的正交谱分解，以及每个成分的方差等于其特征值的性质，推导如何量化：\n- 使用平方载荷，第一个保留成分的方差中可归因于 $x_{1}$ 的比例，以及 \n- $x_{1}$ 在两个保留成分上的累积共同度，表示为平方载荷之和。\n\n然后计算 $x_{1}$ 在保留成分上的累积共同度。将最终答案表示为一个不带单位的小数。无需四舍五入。",
            "solution": "该问题要求推导与主成分分析 (PCA) 相关的两个量，并进行后续计算。这些推导的基础在于主成分、其方差（特征值）以及原始变量在这些成分上的载荷的定义。我们首先将这些核心概念形式化。\n\n设标准化后的原始变量由向量 $X = (x_1, x_2, x_3)^T$ 表示，其中对于每个变量 $x_i$，其均值为 $0$，方差为 $1$。分析是在 $3 \\times 3$ 的样本相关性矩阵 $R$ 上进行的。主成分 $y_j$ 是原始变量的线性组合。第 $j$ 个主成分由 $y_j = e_{1j}x_1 + e_{2j}x_2 + e_{3j}x_3 = e_j^T X$ 给出，其中 $e_j$ 是 $R$ 的第 $j$ 个标准正交特征向量。第 $j$ 个成分的方差等于相应的特征值 $\\lambda_j$，因此 $\\text{Var}(y_j) = \\lambda_j$。\n\n变量 $x_i$ 在成分 $y_j$ 上的载荷，记为 $L_{ij}$，定义为它们的相关系数：$L_{ij} = \\text{Corr}(x_i, y_j)$。这可以用特征向量和特征值来表示。\n$$\nL_{ij} = \\text{Corr}(x_i, y_j) = \\frac{\\text{Cov}(x_i, y_j)}{\\sqrt{\\text{Var}(x_i) \\text{Var}(y_j)}}\n$$\n由于变量是标准化的，$\\text{Var}(x_i) = 1$。成分方差为 $\\text{Var}(y_j) = \\lambda_j$。协方差为 $\\text{Cov}(x_i, y_j) = \\text{Cov}(x_i, \\sum_{k=1}^3 e_{kj}x_k) = \\sum_{k=1}^3 e_{kj} \\text{Cov}(x_i, x_k)$。由于 $\\text{Cov}(x_i, x_k) = R_{ik}$，这变为 $\\sum_{k=1}^3 R_{ik} e_{kj}$，即向量 $R e_j$ 的第 $i$ 个元素。因为 $e_j$ 是 $R$ 的一个特征向量，我们有 $R e_j = \\lambda_j e_j$。因此，$\\text{Cov}(x_i, y_j) = \\lambda_j e_{ij}$。\n将这些代入相关性公式，得到载荷的基本关系：\n$$\nL_{ij} = \\frac{\\lambda_j e_{ij}}{\\sqrt{1 \\cdot \\lambda_j}} = e_{ij}\\sqrt{\\lambda_j}\n$$\n\n**成分方差分数的推导**\n\n任务的第一部分是推导如何量化第一个保留成分 ($y_1$) 的方差中可归因于变量 $x_1$ 的比例。成分 $y_1$ 的总方差是 $\\lambda_1$。我们需要展示这个方差是如何在原始变量之间划分的。\n\n让我们对给定成分 $j$ 在所有变量 $i = 1, 2, 3$ 上的平方载荷求和：\n$$\n\\sum_{i=1}^3 L_{ij}^2 = \\sum_{i=1}^3 (e_{ij}\\sqrt{\\lambda_j})^2 = \\sum_{i=1}^3 e_{ij}^2 \\lambda_j = \\lambda_j \\sum_{i=1}^3 e_{ij}^2\n$$\n向量 $e_j = (e_{1j}, e_{2j}, e_{3j})^T$ 是一个单位长度的特征向量，这意味着它是标准化的：$\\|e_j\\|^2 = e_{1j}^2 + e_{2j}^2 + e_{3j}^2 = 1$。\n因此，我们得到以下恒等式：\n$$\n\\sum_{i=1}^3 L_{ij}^2 = \\lambda_j \\cdot 1 = \\lambda_j\n$$\n该恒等式表明，主成分 $j$ 的平方载荷之和等于其方差 $\\lambda_j$。这提供了对成分方差的自然分解。项 $L_{ij}^2$ 被解释为变量 $x_i$ 对成分 $y_j$ 总方差的贡献。\n\n因此，成分 $y_j$ 的方差中可归因于变量 $x_i$ 的比例是该贡献与总方差的比值：\n$$\n\\text{Fraction for } x_i \\text{ in } y_j = \\frac{L_{ij}^2}{\\lambda_j}\n$$\n对于所要求的特定情况——$x_1$ 在第一个成分 $y_1$ 中的比例——我们设 $i=1$ 和 $j=1$：\n$$\n\\text{Fraction for } x_1 \\text{ in } y_1 = \\frac{L_{11}^2}{\\lambda_1}\n$$\n这就完成了第一个要求的推导。\n\n**累积共同度的推导**\n\n任务的第二部分是推导 $x_1$ 在两个保留成分上的累积共同度的量化方法。一个变量的共同度是指其方差中能被一组保留成分解释的比例。由于变量是标准化的，$\\text{Var}(x_i) = 1$。\n\n特征向量集合 $\\{e_1, e_2, e_3\\}$ 构成一个标准正交基。我们可以将原始变量 $x_i$ 表示为主成分 $y_j$ 的线性组合。该变换为 $Y = E^T X$，其中 $E$ 是以特征向量 $e_j$ 为列的矩阵。由于 $E$ 是正交的 ($E^T = E^{-1}$)，逆变换为 $X = E Y$。对于特定变量 $x_i$，这表示为：\n$$\nx_i = \\sum_{j=1}^3 E_{ij} y_j = \\sum_{j=1}^3 e_{ij} y_j\n$$\n变量 $x_i$ 的方差可以从此表达式计算得出。由于主成分 $y_j$ 是不相关的，和的方差等于方差的和：\n$$\n\\text{Var}(x_i) = \\text{Var}\\left(\\sum_{j=1}^3 e_{ij} y_j\\right) = \\sum_{j=1}^3 \\text{Var}(e_{ij} y_j) = \\sum_{j=1}^3 e_{ij}^2 \\text{Var}(y_j) = \\sum_{j=1}^3 e_{ij}^2 \\lambda_j\n$$\n使用关系式 $L_{ij}^2 = e_{ij}^2 \\lambda_j$，我们可以将其改写为：\n$$\n\\text{Var}(x_i) = \\sum_{j=1}^3 L_{ij}^2\n$$\n由于 $\\text{Var}(x_i) = 1$，我们得到恒等式 $1 = \\sum_{j=1}^3 L_{ij}^2$。这表明一个标准化变量的总方差是其在所有主成分上的平方载荷之和。\n\n变量 $x_i$ 在一组 $k$ 个保留成分上的累积共同度是其方差中由这些成分解释的部分。这即是相应平方载荷的和。对于 $k=2$ 个保留成分，变量 $x_i$ 的累积共同度为：\n$$\nh_i^2 = \\sum_{j=1}^2 L_{ij}^2 = L_{i1}^2 + L_{i2}^2\n$$\n对于变量 $x_1$ 的特定情况，其在两个保留成分上的累积共同度为：\n$$\nh_1^2 = L_{11}^2 + L_{12}^2\n$$\n这就完成了第二个要求的推导。\n\n**$x_1$ 累积共同度的计算**\n\n我们被要求计算 $x_1$ 在两个保留成分上的累积共同度。根据上述推导，这个量是 $h_1^2 = L_{11}^2 + L_{12}^2$。\n问题给出了两个保留成分的载荷矩阵：\n$$\nL = \\begin{pmatrix}\n\\sqrt{0.9}  0 \\\\\n\\frac{\\sqrt{1.8}}{2}  \\sqrt{0.45} \\\\\n\\frac{\\sqrt{1.8}}{2}  -\\sqrt{0.45}\n\\end{pmatrix}\n$$\n第一行中的元素对应于变量 $x_1$。因此，我们有：\n-   $L_{11}$，$x_1$ 在第一个成分上的载荷，为 $\\sqrt{0.9}$。\n-   $L_{12}$，$x_1$ 在第二个成分上的载荷，为 $0$。\n\n将这些值代入共同度公式：\n$$\nh_1^2 = (\\sqrt{0.9})^2 + (0)^2 = 0.9 + 0 = 0.9\n$$\n$x_1$ 在两个保留成分上的累积共同度是 $0.9$。这意味着空腹血糖 ($x_1$) 的方差中有 $90\\%$ 被前两个主成分解释。",
            "answer": "$$\\boxed{0.9}$$"
        },
        {
            "introduction": "在解释主成分之前，一个关键问题是如何确定保留多少个成分进行分析。简单的启发式规则往往会产生误导。本练习将引导你实现一种更为稳健的、基于模拟的方法——霍恩并行分析（Horn's Parallel Analysis），通过构建一个统计基准来区分有意义的成分和由随机抽样噪声产生的成分。掌握这种方法将为严谨的模型选择提供一个强大的工具。",
            "id": "5220669",
            "problem": "您的任务是以一种可复现和可测试的方式，为主成分分析（PCA）实现 Horn 并行分析。从以下基本点出发：给定一个数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$，通过减去每列的样本均值来定义列中心化矩阵 $X_c$；定义样本协方差矩阵 $S$ 为 $S = \\frac{1}{n-1} X_c^\\top X_c$；在形成 $S$ 之前，通过将 $X_c$ 的每一列标准化为单位方差来定义样本相关系数矩阵 $R$。根据谱定理，$R$ 是对称的，因此具有实特征值 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p \\ge 0$。这些特征值量化了在相关性度量下每个主成分所解释的方差。在零假设（即 $p$ 个变量相互独立且具有有限方差，因此总体相关系数矩阵是单位矩阵）下，有限样本效应会导致样本相关系数矩阵的特征值在 1 附近波动。Horn 的思想是通过生成虚无数据来估计零假设下特征值的抽样分布，并将观测到的特征值与一个虚无标准进行比较。\n\n实现以下两种虚无数据生成机制，每种机制都在控制有限样本效应的同时，编码了无跨变量结构的零假设：\n\n- 基于置换的虚无模型：独立地置换 $X$ 每列内的条目，以破坏跨变量依赖性，同时保留每个变量的单变量经验分布和边缘方差。\n- 基于高斯的虚无模型：生成一个矩阵 $Z \\in \\mathbb{R}^{n \\times p}$，其条目是来自标准正态分布的独立抽样，并分析其样本相关系数特征值。\n\n使用以下决策规则来保留成分。设 $\\lambda^{\\mathrm{obs}}_j$ 表示观测样本相关系数矩阵的第 $j$ 大特征值，设 $\\{\\lambda^{\\mathrm{null},(s)}_j\\}_{s=1}^m$ 是从 $m$ 次独立虚无复制中计算出的第 $j$ 大特征值。将虚无标准 $c_j$ 定义为经验均值 $c_j = \\frac{1}{m} \\sum_{s=1}^m \\lambda^{\\mathrm{null},(s)}_j$（均值阈值规则），或定义为经验 $q$-分位数 $c_j$，使得 $\\{\\lambda^{\\mathrm{null},(s)}_j\\}$ 中有比例为 $q$ 的值小于或等于 $c_j$（分位数阈值规则）。精确保留那些观测特征值超过相应虚无标准的成分，即保留由 $k = \\left| \\{ j \\in \\{1,\\dots,p\\} : \\lambda^{\\mathrm{obs}}_j > c_j \\} \\right|$ 给出的成分数量 $k$。\n\n您的程序必须在没有任何用户输入的情况下，仅使用指定的运行时环境完成以下操作。对于下方的每个测试用例，按描述生成数据 $X$，计算观测相关系数特征值，使用指定的虚无模式和指定的复制次数 $m$ 模拟虚无特征值，计算相应的虚无标准，应用决策规则，并以整数形式输出保留的成分数量 $k$。所有随机数生成必须使用指定的种子以保证可复现性。在所有情况下，都应使用相关系数矩阵进行 PCA。\n\n测试套件。对于每个用例，$n$ 是观测次数，$p$ 是变量数，$m$ 是虚无复制次数，当请求分位数规则时 $q$ 是所用的分位数，并且指定了一个明确的随机种子。在具有潜在结构的用例中，从因子模型 $X = F L^\\top + \\sigma E$ 生成数据，其中 $F \\in \\mathbb{R}^{n \\times r}$ 具有独立的标准正态分布条目，$L \\in \\mathbb{R}^{p \\times r}$ 是下述的固定载荷矩阵，$E \\in \\mathbb{R}^{n \\times p}$ 具有独立的标准正态分布条目，$\\sigma > 0$ 是一个噪声尺度。生成 $X$ 后，除了 PCA 内部的相关性标准化外，不要执行任何额外的缩放。在每个用例中，使用以给定种子初始化的新的伪随机数生成器来生成 $F$ 和 $E$，对于虚无数据生成，则使用给定种子加上 $911$ 作为偏移量的种子来初始化一个单独的伪随机数生成器。\n\n- 测试用例 A（两个强因子，高斯虚无模型，分位数规则）：$n = 400$，$p = 6$，$r = 2$，种子 $= 7$，噪声尺度 $\\sigma = 0.3$，载荷矩阵 $L \\in \\mathbb{R}^{6 \\times 2}$ 定义为当 $j \\in \\{1,2,3\\}$ 时 $L_{j1} = 0.9, L_{j2} = 0.1$，当 $j \\in \\{4,5,6\\}$ 时 $L_{j1} = 0.1, L_{j2} = 0.9$；虚无模式 $=$ 高斯，$m = 1000$，分位数 $q = 0.95$。\n- 测试用例 B（与 A 相同的数据生成机制，高斯虚无模型，均值规则）：与 A 相同，但使用均值阈值规则代替分位数规则，并设置 $m = 1000$。\n- 测试用例 C（一个主导因子，置换虚无模型，分位数规则）：$n = 250$，$p = 5$，$r = 1$，种子 $= 11$，噪声尺度 $\\sigma = 0.2$，载荷矩阵 $L \\in \\mathbb{R}^{5 \\times 1}$ 的所有条目均为 $0.9$；虚无模式 $=$ 置换，$m = 1000$，分位数 $q = 0.95$。\n- 测试用例 D（纯噪声，高斯虚无模型，更严格的分位数规则）：$n = 500$，$p = 10$，种子 $= 29$，将 $X$ 生成为独立的标准正态条目（即设置 $r = 0$ 并忽略 $F L^\\top$）；虚无模式 $=$ 高斯，$m = 1500$，分位数 $q = 0.99$。\n- 测试用例 E（两个强因子，小样本，置换虚无模型，分位数规则）：$n = 60$，$p = 8$，$r = 2$，种子 $= 31$，噪声尺度 $\\sigma = 0.25$，载荷矩阵 $L \\in \\mathbb{R}^{8 \\times 2}$ 定义为当 $j \\in \\{1,2,3,4\\}$ 时 $L_{j1} = 0.85, L_{j2} = 0.1$，当 $j \\in \\{5,6,7,8\\}$ 时 $L_{j1} = 0.1, L_{j2} = 0.85$；虚无模式 $=$ 置换，$m = 1200$，分位数 $q = 0.95$。\n\n决策规则说明。对于分位数规则，对于每个成分索引 $j \\in \\{1,\\dots,p\\}$，将 $c_j$ 计算为 $m$ 次复制中位置 $j$ 上的模拟虚无特征值的经验 $q$-分位数，当且仅当 $\\lambda^{\\mathrm{obs}}_j > c_j$ 时保留成分 $j$。对于均值规则，用位置 $j$ 上的虚无特征值的经验均值替换 $c_j$。\n\n最终输出格式。您的程序应生成单行输出，其中包含五个测试用例（按 A 到 E 的顺序）的 $k$ 值，以逗号分隔并用方括号括起来；例如，形如 $[k_A,k_B,k_C,k_D,k_E]$ 的输出。输出条目必须是整数。不应打印任何额外的文本或行。",
            "solution": "该问题要求实现 Horn 并行分析，这是一种用于确定在主成分分析（PCA）中保留多少成分的统计程序。该方法基于对观测数据得出的特征值与在无潜在相关结构假设下从零假设数据得出的特征值进行比较。这种有原则的方法有助于区分代表实质性方差（信号）的成分和代表随机抽样波动（噪声）的成分。该实现将通过几个具有指定数据生成机制和决策规则的测试用例进行验证。\n\n并行分析的核心算法分四步进行：\n1.  从观测数据的样本相关系数矩阵计算特征值。\n2.  在变量独立的零假设下生成多个数据集，并为每个数据集计算特征值。\n3.  基于模拟的虚无特征值分布，为每个成分建立一个阈值标准。\n4.  保留其特征值超过相应标准的观测成分的数量。\n\n设观测数据由矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 给出，其中 $n$ 是观测次数，$p$ 是变量数。\n\n**第 1 步：计算观测特征值**\n\n分析在样本相关系数矩阵 $R$ 上进行，该矩阵对单个变量的线性变换具有不变性。$R$ 的计算涉及对数据进行中心化和缩放。首先，通过从每列的条目中减去该列的均值来计算列中心化矩阵 $X_c$：\n$$\n(X_c)_{ij} = X_{ij} - \\bar{X}_j, \\quad \\text{where} \\quad \\bar{X}_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij}\n$$\n接下来，将 $X_c$ 的每一列标准化，使其样本方差为 1。设 $\\hat{\\sigma}_j$ 为第 $j$ 列的样本标准差，计算公式为 $\\hat{\\sigma}_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_{ij} - \\bar{X}_j)^2}$。则标准化数据矩阵 $X_{\\text{std}}$ 为：\n$$\n(X_{\\text{std}})_{ij} = \\frac{(X_c)_{ij}}{\\hat{\\sigma}_j}\n$$\n样本相关系数矩阵 $R$ 由 $X_{\\text{std}}$ 计算得出：\n$$\nR = \\frac{1}{n-1} X_{\\text{std}}^\\top X_{\\text{std}}\n$$\n根据实对称矩阵的谱定理，$R$ 具有 $p$ 个实特征值，我们将其从大到小排序：$\\lambda^{\\mathrm{obs}}_1 \\ge \\lambda^{\\mathrm{obs}}_2 \\ge \\cdots \\ge \\lambda^{\\mathrm{obs}}_p \\ge 0$。这些就是“观测”特征值。\n\n**第 2 步：模拟虚无特征值分布**\n\n零假设 $H_0$ 假定 $p$ 个变量在总体中是相互不相关的。为了估计 $H_0$ 下样本特征值的分布，我们生成 $m$ 个大小均为 $n \\times p$ 的虚无数据集，并计算它们各自的相关系数矩阵特征值。问题指定了两种生成虚无数据的机制，每种机制在破坏相关结构的同时保留了原始数据的某些属性。\n\n*   **基于高斯的虚无模型：** 生成一个矩阵 $Z \\in \\mathbb{R}^{n \\times p}$，其条目是来自标准正态分布 $Z_{ij} \\sim \\mathcal{N}(0, 1)$ 的独立同分布抽样。该模型假设在零假设下，数据源于一个总体相关系数矩阵等于单位矩阵 $I$ 的多元正态分布。\n*   **基于置换的虚无模型：** 对于原始数据矩阵 $X$ 的每一列 $j$，其 $n$ 个条目被随机置换。这个过程对 $p$ 列中的每一列都独立进行。该方法在精确保留每个变量的边缘经验分布的同时，破坏了变量间的相关性。这是一种非参数方法，不作正态性等分布假设。\n\n对于 $m$ 次虚无复制中的每一次（由 $s \\in \\{1, \\dots, m\\}$ 索引），生成一个虚无数据矩阵 $X_{\\text{null}}^{(s)}$。计算相应的样本相关系数矩阵 $R_{\\text{null}}^{(s)}$，并计算其特征值 $\\{\\lambda^{\\mathrm{null},(s)}_j\\}_{j=1}^p$ 并按降序排序。这个过程产生一个 $m \\times p$ 的虚无特征值矩阵，其中每一列代表了在零假设下第 $j$ 个有序特征值的经验抽样分布。\n\n**第 3 步：建立虚无标准**\n\n从模拟的虚无特征值分布中，为每个成分 $j \\in \\{1, \\dots, p\\}$ 建立一个决策标准 $c_j$。问题为此定义了两种规则：\n\n*   **均值阈值规则：** 标准 $c_j$ 是所有 $m$ 次虚无模拟中第 $j$ 个特征值的平均值：\n    $$\n    c_j = \\frac{1}{m} \\sum_{s=1}^m \\lambda^{\\mathrm{null},(s)}_j\n    $$\n*   **分位数阈值规则：** 标准 $c_j$ 是第 $j$ 个虚无特征值分布的经验 $q$-分位数。对于给定的分位数 $q \\in (0, 1)$，$c_j$ 是一个值，使得大约有比例为 $q$ 的模拟特征值 $\\{\\lambda^{\\mathrm{null},(s)}_j\\}_{s=1}^m$ 小于或等于它。\n\n**第 4 步：应用决策规则**\n\n最后一步是将观测特征值与已建立的标准进行比较。如果一个成分 $j$ 的观测特征值大于其对应的零假设标准，则认为该成分代表了一个统计上显著的方差来源。要保留的总成分数 $k$ 是此类成分的计数：\n$$\nk = \\left| \\{ j \\in \\{1, \\dots, p\\} : \\lambda^{\\mathrm{obs}}_j > c_j \\} \\right|\n$$\n此过程应用于每个测试用例，使用指定的参数、随机种子、数据生成模型（$X = F L^\\top + \\sigma E$ 或纯噪声）和决策规则来确定 $k$ 的值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Horn's parallel analysis for PCA and runs a suite of test cases.\n    \"\"\"\n\n    def _get_correlation_eigenvalues(X):\n        \"\"\"\n        Computes the eigenvalues of the sample correlation matrix of X.\n        \n        Args:\n            X (np.ndarray): Data matrix of shape (n, p).\n        \n        Returns:\n            np.ndarray: A 1D array of p eigenvalues, sorted in descending order.\n        \"\"\"\n        n, _ = X.shape\n        \n        # Center the data\n        X_c = X - X.mean(axis=0)\n        \n        # Standardize the data to have unit sample variance\n        # ddof=1 for sample standard deviation\n        col_stds = X_c.std(axis=0, ddof=1)\n        \n        # Avoid division by zero for columns with zero variance\n        col_stds[col_stds == 0] = 1.0\n        \n        X_std = X_c / col_stds\n        \n        # Compute the correlation matrix\n        R = (X_std.T @ X_std) / (n - 1)\n        \n        # Compute eigenvalues of the symmetric matrix R\n        # eigvalsh is efficient for Hermitian (real-symmetric) matrices\n        eigenvalues = np.linalg.eigvalsh(R)\n        \n        # Sort eigenvalues in descending order\n        return np.sort(eigenvalues)[::-1]\n\n    def run_parallel_analysis(\n        n, p, seed, m, null_mode, rule,\n        r=0, L=None, sigma=None, q=None\n    ):\n        \"\"\"\n        Runs a single instance of Horn's parallel analysis.\n        \"\"\"\n        # 1. Generate observed data and its eigenvalues\n        data_rng = np.random.default_rng(seed)\n        \n        if r > 0 and L is not None and sigma is not None:\n            F = data_rng.standard_normal((n, r))\n            E = data_rng.standard_normal((n, p))\n            X_obs = F @ L.T + sigma * E\n        else: # Pure noise case\n            X_obs = data_rng.standard_normal((n, p))\n            \n        obs_eigenvalues = _get_correlation_eigenvalues(X_obs)\n\n        # 2. Simulate null eigenvalues\n        null_rng = np.random.default_rng(seed + 911)\n        null_eigenvalues_matrix = np.zeros((m, p))\n\n        for s in range(m):\n            if null_mode == 'Gaussian':\n                X_null = null_rng.standard_normal((n, p))\n            elif null_mode == 'permutation':\n                X_null = np.zeros((n, p))\n                for j in range(p):\n                    X_null[:, j] = null_rng.permutation(X_obs[:, j])\n            else:\n                raise ValueError(\"Invalid null_mode specified.\")\n            \n            null_eigenvalues_matrix[s, :] = _get_correlation_eigenvalues(X_null)\n            \n        # 3. Establish the null criterion\n        if rule == 'mean':\n            criteria = np.mean(null_eigenvalues_matrix, axis=0)\n        elif rule == 'quantile' and q is not None:\n            criteria = np.quantile(null_eigenvalues_matrix, q, axis=0)\n        else:\n            raise ValueError(\"Invalid rule or missing q for quantile rule.\")\n\n        # 4. Apply the decision rule\n        k = np.sum(obs_eigenvalues > criteria)\n        return k\n\n    # --- Test Suite ---\n    \n    test_cases = [\n        # Case A\n        {'n': 400, 'p': 6, 'r': 2, 'seed': 7, 'sigma': 0.3,\n         'L': np.array([[0.9, 0.1]]*3 + [[0.1, 0.9]]*3),\n         'null_mode': 'Gaussian', 'm': 1000, 'rule': 'quantile', 'q': 0.95},\n        # Case B\n        {'n': 400, 'p': 6, 'r': 2, 'seed': 7, 'sigma': 0.3,\n         'L': np.array([[0.9, 0.1]]*3 + [[0.1, 0.9]]*3),\n         'null_mode': 'Gaussian', 'm': 1000, 'rule': 'mean', 'q': None},\n        # Case C\n        {'n': 250, 'p': 5, 'r': 1, 'seed': 11, 'sigma': 0.2,\n         'L': np.full((5, 1), 0.9),\n         'null_mode': 'permutation', 'm': 1000, 'rule': 'quantile', 'q': 0.95},\n        # Case D\n        {'n': 500, 'p': 10, 'r': 0, 'seed': 29, 'sigma': None, 'L': None,\n         'null_mode': 'Gaussian', 'm': 1500, 'rule': 'quantile', 'q': 0.99},\n        # Case E\n        {'n': 60, 'p': 8, 'r': 2, 'seed': 31, 'sigma': 0.25,\n         'L': np.array([[0.85, 0.1]]*4 + [[0.1, 0.85]]*4),\n         'null_mode': 'permutation', 'm': 1200, 'rule': 'quantile', 'q': 0.95}\n    ]\n\n    results = []\n    for case_params in test_cases:\n        k = run_parallel_analysis(**case_params)\n        results.append(k)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在神经科学研究中，当比较不同实验、被试或条件下的主成分分析（PCA）结果时，一个常见的陷阱是主成分（特征向量）的符号不确定性。由于主成分载荷向量的方向是任意的，直接比较可能会导致错误的结论。本练习提供了一种动手实践的方法，通过与一个“锚向量”对齐，来强制主成分具有一致的方向，从而确保跨数据集比较的稳定性和解释的一致性。",
            "id": "4172070",
            "problem": "您正在分析多会话神经元群体记录，并对跨会话的同一组特征应用主成分分析（PCA）。在神经科学数据分析中，主成分分析（PCA）产生的主成分加载向量的方向仅能确定到符号级别，这意味着如果 $v$ 是一个主成分加载向量，那么 $-v$ 代表相同的子空间并解释相同的方差。这种符号不确定性阻碍了跨会话的解释，除非强制执行一致的方向约定。一种常见且有科学依据的约定是通过要求每个主成分与一个可解释的锚向量的内积为非负来确定其方向。\n\n从基本原理开始：\n- 特征的经验协方差矩阵 $C \\in \\mathbb{R}^{F \\times F}$ 由中心化外积的样本平均值定义，其特征向量是主成分加载方向。\n- 如果 $u$ 是 $C$ 的一个与特征值 $\\lambda$ 相关联的特征向量，那么 $-u$ 也是与同一特征值 $\\lambda$ 相关联的特征向量。因此，PCA 加载向量在符号上具有方向模糊性。\n- 内积（点积）编码了 $\\mathbb{R}^{F}$ 中的对齐情况，内积 $w^{\\top} l$ 的符号表示 $l$ 是指向与 $w$ 相同的半空间还是相反的半空间。\n\n对于多个测试用例，您将获得一组会话加载矩阵和一组定义每个主成分期望方向的锚向量。您必须通过翻转任何会话中与相应锚向量的内积为负的主成分加载向量的符号，来强制实现跨会话的符号一致性。如果内积等于零，则不进行翻转。对齐后，您必须为每个测试用例报告两个量：在所有会话和主成分上执行的符号翻转总数，以及在所有会话和主成分上的最小锚内积值。\n\n定义和符号：\n- 设有 $S$ 个会话，$F$ 个特征和 $K$ 个主成分。对于会话 $s \\in \\{0, \\ldots, S-1\\}$，令 $L^{(s)} \\in \\mathbb{R}^{F \\times K}$ 表示加载矩阵，其第 $k$ 列表示为 $l^{(s)}_{k} \\in \\mathbb{R}^{F}$。\n- 令锚向量为 $W \\in \\mathbb{R}^{F \\times K}$，其第 $k$ 列表示为 $w_{k} \\in \\mathbb{R}^{F}$。方向规则是：如果 $w_{k}^{\\top} l^{(s)}_{k}  0$，则将 $l^{(s)}_{k}$ 替换为 $-l^{(s)}_{k}$；如果 $w_{k}^{\\top} l^{(s)}_{k} \\ge 0$，则保持不变。\n- 对齐摘要指标是：\n  1. 整数总翻转计数 $N_{\\text{flip}} = \\sum_{s=0}^{S-1} \\sum_{k=1}^{K} \\mathbf{1}\\{w_{k}^{\\top} l^{(s)}_{k}  0\\}$。\n  2. 浮点数最小对齐锚内积 $m_{\\min} = \\min_{s,k} w_{k}^{\\top} \\tilde{l}^{(s)}_{k}$，其中 $\\tilde{l}^{(s)}_{k}$ 表示在强制执行规则后可能被翻转的加载向量。\n\n任务：\n- 实现一个程序，使用提供的测试套件应用上述方向规则。为每个测试用例计算 $N_{\\text{flip}}$ 和 $m_{\\min}$。浮点数 $m_{\\min}$ 必须四舍五入到六位小数。不涉及物理单位。\n\n假设：\n- 跨会话的主成分身份已通过索引 $k$ 匹配，只需强制执行符号一致性。\n- 所有向量和矩阵都是实值的，并且在每个测试用例中特征维度是一致的。\n\n测试套件：\n- 测试用例 1 ($S=2$, $F=4$, $K=2$):\n  - 会话 0：列 $l^{(0)}_{1}$ 和 $l^{(0)}_{2}$ 位于\n    $$\n    L^{(0)} = \\begin{bmatrix}\n    0.8  -0.2 \\\\\n    0.1  0.9 \\\\\n    0.0  0.05 \\\\\n    -0.1  0.0\n    \\end{bmatrix}.\n    $$\n  - 会话 1：\n    $$\n    L^{(1)} = -L^{(0)} = \\begin{bmatrix}\n    -0.8  0.2 \\\\\n    -0.1  -0.9 \\\\\n    -0.0  -0.05 \\\\\n    0.1  -0.0\n    \\end{bmatrix}.\n    $$\n  - 锚向量（列 $w_{1}$，$w_{2}$）：\n    $$\n    W = \\begin{bmatrix}\n    1  0 \\\\\n    0  1 \\\\\n    0  0 \\\\\n    0  0\n    \\end{bmatrix}.\n    $$\n- 测试用例 2 ($S=3$, $F=5$, $K=3$):\n  - 会话 0：\n    $$\n    L^{(0)} = \\begin{bmatrix}\n    0.5  0.0  0.0 \\\\\n    0.2  -0.1  0.1 \\\\\n    0.1  0.7  0.0 \\\\\n    0.0  0.2  0.6 \\\\\n    0.1  0.0  0.3\n    \\end{bmatrix}.\n    $$\n  - 会话 1（主成分 2 相对于锚向量翻转）：\n    $$\n    L^{(1)} = \\begin{bmatrix}\n    0.5  -0.0  0.0 \\\\\n    0.2  0.1  0.1 \\\\\n    0.1  -0.7  0.0 \\\\\n    0.0  -0.2  0.6 \\\\\n    0.1  -0.0  0.3\n    \\end{bmatrix}.\n    $$\n  - 会话 2（主成分 1 和 3 相对于锚向量翻转）：\n    $$\n    L^{(2)} = \\begin{bmatrix}\n    -0.5  0.0  -0.0 \\\\\n    -0.2  -0.1  -0.1 \\\\\n    -0.1  0.7  -0.0 \\\\\n    -0.0  0.2  -0.6 \\\\\n    -0.1  0.0  -0.3\n    \\end{bmatrix}.\n    $$\n  - 锚向量：\n    $$\n    W = \\begin{bmatrix}\n    1  0  0 \\\\\n    0  0  0 \\\\\n    0  1  0 \\\\\n    0  0  1 \\\\\n    0  0  0\n    \\end{bmatrix}.\n    $$\n- 测试用例 3 ($S=2$, $F=3$, $K=2$):\n  - 会话 0：\n    $$\n    L^{(0)} = \\begin{bmatrix}\n    1.0  0.0 \\\\\n    0.0  0.0 \\\\\n    0.0  0.9\n    \\end{bmatrix}.\n    $$\n  - 会话 1：\n    $$\n    L^{(1)} = \\begin{bmatrix}\n    -1.0  0.0 \\\\\n    -0.0  0.0 \\\\\n    -0.0  -0.9\n    \\end{bmatrix}.\n    $$\n  - 锚向量：\n    $$\n    W = \\begin{bmatrix}\n    0  0 \\\\\n    1  0 \\\\\n    0  1\n    \\end{bmatrix}.\n    $$\n  在这种情况下，请注意对于主成分 1，锚内积在任何翻转前后均为 0，因此对于零内积不应应用翻转。\n\n所需输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例 $i \\in \\{1,2,3\\}$，按顺序输出两个值：整数 $N_{\\text{flip}}$，后跟四舍五入到六位小数的浮点数 $m_{\\min}$。因此，最后一行总共包含 6 个值，顺序为 $[N_{\\text{flip}}^{(1)}, m_{\\min}^{(1)}, N_{\\text{flip}}^{(2)}, m_{\\min}^{(2)}, N_{\\text{flip}}^{(3)}, m_{\\min}^{(3)}]$。",
            "solution": "该问题在科学上和数学上都是有效的。它解决了在主成分分析（PCA）应用于多会话或多主体数据中的一个基本且实际的问题，即特征向量的符号模糊性。为解决这种模糊性而提出的方法——将每个主成分（PC）加载向量相对于预定义的锚向量进行定向——是一种标准、可解释且计算上直接的技术。该问题是适定的，提供了计算唯一解所需的所有必要数据和定义。\n\n核心原理是，对于任何对称矩阵，例如 $F$ 个特征的经验协方差矩阵 $C \\in \\mathbb{R}^{F \\times F}$，如果 $u \\in \\mathbb{R}^F$ 是一个特征值为 $\\lambda$ 的特征向量，那么 $-u$ 也是一个特征值为 $\\lambda$ 的特征向量，因为 $C(-u) = -Cu = -(\\lambda u) = \\lambda(-u)$。PC 加载向量是 $C$ 的特征向量，因此它们的方向在符号上是任意的。这种模糊性阻碍了在不同数据集（例如，记录会话）之间直接比较加载向量。\n\n为强制实现一致的方向，我们引入一组锚向量 $W \\in \\mathbb{R}^{F \\times K}$，其中 $K$ 是主成分的数量。$W$ 的每一列 $w_k$ 作为第 $k$ 个 PC 的参考。来自会话 $s$ 的主成分 $k$ 的加载向量 $l_k^{(s)}$ 的方向由其与相应锚向量 $w_k$ 的内积的符号确定。内积 $w_k^\\top l_k^{(s)}$ 衡量了 $l_k^{(s)}$ 在 $w_k$ 上的投影。正号表示在相同大致方向上对齐，而负号表示在相反方向上对齐。\n\n规定的方向规则是确保该内积始终为非负：对于每个会话 $s \\in \\{0, \\ldots, S-1\\}$ 和主成分 $k \\in \\{1, \\ldots, K\\}$，加载向量必须满足 $w_k^\\top \\tilde{l}_k^{(s)} \\ge 0$，其中 $\\tilde{l}_k^{(s)}$ 是最终对齐的加载向量。\n\n实现这一点的算法如下：\n1.  对于每个会话 $s$ 和每个主成分 $k$，计算内积 $p_{s,k} = w_k^\\top l_k^{(s)}$。\n2.  仔细检查 $p_{s,k}$ 的符号。如果 $p_{s,k}  0$，则加载向量 $l_k^{(s)}$ 与其锚向量未对齐。为纠正此问题，我们翻转其符号。对齐后的向量变为 $\\tilde{l}_k^{(s)} = -l_k^{(s)}$。此操作计为一次“符号翻转”。新的、对齐后的内积为 $w_k^\\top \\tilde{l}_k^{(s)} = w_k^\\top (-l_k^{(s)}) = -p_{s,k}$，保证为正。\n3.  如果 $p_{s,k} \\ge 0$，则加载向量 $l_k^{(s)}$ 已正确定向（或与锚向量正交，此情况下其方向不变）。对齐后的向量即为 $\\tilde{l}_k^{(s)} = l_k^{(s)}$，不计入翻转次数。对齐后的内积仍为 $p_{s,k}$。\n\n将此规则应用于所有加载向量后，我们为每个测试用例计算两个摘要指标：\n1.  符号翻转总数 $N_{\\text{flip}}$ 是满足条件 $w_k^\\top l_k^{(s)}  0$ 的所有实例的总和：\n    $$\n    N_{\\text{flip}} = \\sum_{s=0}^{S-1} \\sum_{k=1}^{K} \\mathbf{1}\\{w_{k}^{\\top} l^{(s)}_{k}  0\\}\n    $$\n    其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数，如果其参数为真，则等于 1，否则等于 0。\n\n2.  最小对齐锚内积 $m_{\\min}$ 是在强制执行一致方向后所有内积中的最小值。根据构造，所有对齐后的内积都是非负的。\n    $$\n    m_{\\min} = \\min_{s,k} \\{w_{k}^{\\top} \\tilde{l}^{(s)}_{k}\\}\n    $$\n    该值量化了校正后所有会话中任何主成分与其锚向量之间的“最差情况”对齐程度。\n\n整个过程可以使用矩阵运算高效实现。对于每个会话 $s$，可以通过对矩阵 $W$ 和 $L^{(s)}$ 进行元素级乘积并沿特征维度（轴 0）求和，来计算 $K$ 个内积的集合 $\\{w_k^\\top l_k^{(s)}\\}_{k=1}^K$。随后，一个布尔掩码可以识别哪些主成分需要翻转，而对齐后的内积可以通过取原始内积的绝对值来计算。对每个会话重复此过程，并汇总结果以找到 $N_{\\text{flip}}$ 和 $m_{\\min}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA loading vector alignment problem for the given test suite.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (S=2, F=4, K=2)\n        {\n            \"session_loadings\": [\n                np.array([\n                    [0.8, -0.2],\n                    [0.1, 0.9],\n                    [0.0, 0.05],\n                    [-0.1, 0.0]\n                ]),\n                np.array([\n                    [-0.8, 0.2],\n                    [-0.1, -0.9],\n                    [-0.0, -0.05],\n                    [0.1, -0.0]\n                ])\n            ],\n            \"anchors\": np.array([\n                [1, 0],\n                [0, 1],\n                [0, 0],\n                [0, 0]\n            ])\n        },\n        # Test case 2 (S=3, F=5, K=3)\n        {\n            \"session_loadings\": [\n                np.array([\n                    [0.5, 0.0, 0.0],\n                    [0.2, -0.1, 0.1],\n                    [0.1, 0.7, 0.0],\n                    [0.0, 0.2, 0.6],\n                    [0.1, 0.0, 0.3]\n                ]),\n                np.array([\n                    [0.5, -0.0, 0.0],\n                    [0.2, 0.1, 0.1],\n                    [0.1, -0.7, 0.0],\n                    [0.0, -0.2, 0.6],\n                    [0.1, -0.0, 0.3]\n                ]),\n                np.array([\n                    [-0.5, 0.0, -0.0],\n                    [-0.2, -0.1, -0.1],\n                    [-0.1, 0.7, -0.0],\n                    [-0.0, 0.2, -0.6],\n                    [-0.1, 0.0, -0.3]\n                ])\n            ],\n            \"anchors\": np.array([\n                [1, 0, 0],\n                [0, 0, 0],\n                [0, 1, 0],\n                [0, 0, 1],\n                [0, 0, 0]\n            ])\n        },\n        # Test case 3 (S=2, F=3, K=2)\n        {\n            \"session_loadings\": [\n                np.array([\n                    [1.0, 0.0],\n                    [0.0, 0.0],\n                    [0.0, 0.9]\n                ]),\n                np.array([\n                    [-1.0, 0.0],\n                    [-0.0, 0.0],\n                    [-0.0, -0.9]\n                ])\n            ],\n            \"anchors\": np.array([\n                [0, 0],\n                [1, 0],\n                [0, 1]\n            ])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        session_loadings = case[\"session_loadings\"]\n        W = case[\"anchors\"]\n        \n        total_flip_count = 0\n        all_aligned_products = []\n\n        for L_s in session_loadings:\n            # For each component k, compute the inner product w_k^T * l_k^(s)\n            # This can be done for all components at once via element-wise\n            # multiplication and summing over the feature dimension (axis=0).\n            inner_products = np.sum(W * L_s, axis=0)\n            \n            # Count the number of flips needed for this session.\n            # A flip is needed if the inner product is negative.\n            num_flips_session = np.sum(inner_products  0)\n            total_flip_count += num_flips_session\n            \n            # After alignment, the inner product w_k^T * l_tilde_k^(s)\n            # will be positive if a flip occurred, and its original non-negative\n            # value otherwise. This is equivalent to taking the absolute value\n            # of the original inner product, since if it was >= 0 it is unchanged,\n            # and if it was  0 it becomes -p > 0.\n            aligned_products = np.abs(inner_products)\n            \n            all_aligned_products.extend(aligned_products)\n\n        # Find the minimum of all aligned inner products.\n        # If there are no components/sessions, handle that edge case,\n        # though problem constraints imply this list is non-empty.\n        if all_aligned_products:\n            min_aligned_product = np.min(all_aligned_products)\n        else:\n            # Fallback for empty case, though not expected\n            min_aligned_product = 0.0\n\n        results.append(str(total_flip_count))\n        results.append(f\"{min_aligned_product:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}