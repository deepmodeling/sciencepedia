{
    "hands_on_practices": [
        {
            "introduction": "At its core, Demixed Principal Component Analysis (dPCA) partitions a dataset's total variance into components attributable to different task parameters, much like an Analysis of Variance (ANOVA). This exercise provides a concrete, hands-on calculation to demonstrate this fundamental decomposition on a simple $2 \\times 2$ data matrix. By computing the marginalizations and verifying their orthogonality, you will gain a foundational understanding of the algebraic structure that underpins dPCA in balanced experimental designs .",
            "id": "4154721",
            "problem": "Consider Demixed Principal Component Analysis (dPCA), which demixes neural population activity into components that depend on distinct task variables by using condition-averaged marginalizations. In a balanced, fully crossed two-factor design with stimuli and decisions, the basic averaging operators are defined by uniform weights across condition levels. The Frobenius inner product between two matrices is the sum of elementwise products across all entries. Let there be $S=2$ stimuli and $D=2$ decisions, and consider a single-feature, trial-averaged neural response matrix $Y \\in \\mathbb{R}^{2 \\times 2}$ given by\n$$\nY \\;=\\; \\begin{pmatrix}\n3 & 1 \\\\\n5 & 9\n\\end{pmatrix}.\n$$\nUsing only the foundational definitions of factor-wise averaging under uniform weights and the Frobenius inner product, perform the following:\n- Compute the grand mean component, the stimulus-only marginalization, the decision-only marginalization, and the stimulus-decision interaction marginalization for $Y$.\n- Using the Frobenius inner product, verify the orthogonality of the mean-free marginalizations (stimulus-only, decision-only, and interaction) under uniform averaging weights by calculating the sum of the three pairwise Frobenius inner products of these mean-free components.\n\nReport the scalar value of the sum of the three pairwise Frobenius inner products. No rounding is required. Express the final answer as a pure number without any units.",
            "solution": "The problem requires performing a component decomposition of a given data matrix, analogous to an analysis of variance (ANOVA), and then verifying the orthogonality of the resulting mean-free components using the Frobenius inner product. This is a foundational aspect of Demixed Principal Component Analysis (dPCA) for balanced experimental designs.\n\nThe given trial-averaged neural response matrix is:\n$$ Y = \\begin{pmatrix} 3 & 1 \\\\ 5 & 9 \\end{pmatrix} $$\nThis represents data from a fully crossed design with $S=2$ stimulus conditions and $D=2$ decision conditions.\n\nThe total data matrix $Y$ can be decomposed into a sum of four orthogonal components: the grand mean component, the stimulus-only component, the decision-only component, and the stimulus-decision interaction component.\n$$ Y = Y_{gm} + Y_{stim} + Y_{dec} + Y_{int} $$\nHere, $Y_{stim}$, $Y_{dec}$, and $Y_{int}$ are the \"mean-free marginalizations\" mentioned in the problem. We will compute each of these four components.\n\nFirst, we compute the necessary averages using uniform weights as specified. The grand mean, $\\bar{y}$, is the average of all entries in $Y$:\n$$ \\bar{y} = \\frac{1}{S \\times D} \\sum_{s=1}^{S} \\sum_{d=1}^{D} y_{sd} = \\frac{1}{2 \\times 2} (3 + 1 + 5 + 9) = \\frac{18}{4} = 4.5 $$\nThe averages across decisions for each stimulus are:\n$$ \\bar{y}_{s=1, \\cdot} = \\frac{1}{D} (y_{11} + y_{12}) = \\frac{1}{2} (3 + 1) = 2 $$\n$$ \\bar{y}_{s=2, \\cdot} = \\frac{1}{D} (y_{21} + y_{22}) = \\frac{1}{2} (5 + 9) = 7 $$\nThe averages across stimuli for each decision are:\n$$ \\bar{y}_{\\cdot, d=1} = \\frac{1}{S} (y_{11} + y_{21}) = \\frac{1}{2} (3 + 5) = 4 $$\n$$ \\bar{y}_{\\cdot, d=2} = \\frac{1}{S} (y_{12} + y_{22}) = \\frac{1}{2} (1 + 9) = 5 $$\n\nNow, we define and compute the four components of the decomposition.\n\n1.  **Grand Mean Component ($Y_{gm}$)**: This is a matrix where every entry is the grand mean $\\bar{y}$.\n    $$ Y_{gm} = \\begin{pmatrix} \\bar{y} & \\bar{y} \\\\ \\bar{y} & \\bar{y} \\end{pmatrix} = \\begin{pmatrix} 4.5 & 4.5 \\\\ 4.5 & 4.5 \\end{pmatrix} $$\n\n2.  **Stimulus-Only Marginalization ($Y_{stim}$)**: This is the first mean-free component, capturing variation due to stimuli alone. It is computed by taking the matrix of stimulus-wise averages and subtracting the grand mean component.\n    $$ Y_{stim} = \\begin{pmatrix} \\bar{y}_{s=1, \\cdot} & \\bar{y}_{s=1, \\cdot} \\\\ \\bar{y}_{s=2, \\cdot} & \\bar{y}_{s=2, \\cdot} \\end{pmatrix} - Y_{gm} = \\begin{pmatrix} 2 & 2 \\\\ 7 & 7 \\end{pmatrix} - \\begin{pmatrix} 4.5 & 4.5 \\\\ 4.5 & 4.5 \\end{pmatrix} = \\begin{pmatrix} -2.5 & -2.5 \\\\ 2.5 & 2.5 \\end{pmatrix} $$\n\n3.  **Decision-Only Marginalization ($Y_{dec}$)**: This is the second mean-free component, capturing variation due to decisions alone. It is computed by taking the matrix of decision-wise averages and subtracting the grand mean component.\n    $$ Y_{dec} = \\begin{pmatrix} \\bar{y}_{\\cdot, d=1} & \\bar{y}_{\\cdot, d=2} \\\\ \\bar{y}_{\\cdot, d=1} & \\bar{y}_{\\cdot, d=2} \\end{pmatrix} - Y_{gm} = \\begin{pmatrix} 4 & 5 \\\\ 4 & 5 \\end{pmatrix} - \\begin{pmatrix} 4.5 & 4.5 \\\\ 4.5 & 4.5 \\end{pmatrix} = \\begin{pmatrix} -0.5 & 0.5 \\\\ -0.5 & 0.5 \\end{pmatrix} $$\n\n4.  **Stimulus-Decision Interaction Marginalization ($Y_{int}$)**: This is the third mean-free component, capturing the remaining variation not explained by the main effects or the grand mean. It is the residual left after subtracting the other components from the original data matrix.\n    $$ Y_{int} = Y - Y_{gm} - Y_{stim} - Y_{dec} $$\n    Substituting the definitions of the components:\n    $$ Y_{int} = Y - Y_{gm} - \\left( \\begin{pmatrix} 2 & 2 \\\\ 7 & 7 \\end{pmatrix} - Y_{gm} \\right) - \\left( \\begin{pmatrix} 4 & 5 \\\\ 4 & 5 \\end{pmatrix} - Y_{gm} \\right) $$\n    $$ Y_{int} = Y - \\begin{pmatrix} 2 & 2 \\\\ 7 & 7 \\end{pmatrix} - \\begin{pmatrix} 4 & 5 \\\\ 4 & 5 \\end{pmatrix} + Y_{gm} $$\n    $$ Y_{int} = \\begin{pmatrix} 3 & 1 \\\\ 5 & 9 \\end{pmatrix} - \\begin{pmatrix} 2 & 2 \\\\ 7 & 7 \\end{pmatrix} - \\begin{pmatrix} 4 & 5 \\\\ 4 & 5 \\end{pmatrix} + \\begin{pmatrix} 4.5 & 4.5 \\\\ 4.5 & 4.5 \\end{pmatrix} $$\n    $$ Y_{int} = \\begin{pmatrix} 3-2-4+4.5 & 1-2-5+4.5 \\\\ 5-7-4+4.5 & 9-7-5+4.5 \\end{pmatrix} = \\begin{pmatrix} 1.5 & -1.5 \\\\ -1.5 & 1.5 \\end{pmatrix} $$\n\nThe problem asks to verify the orthogonality of the three mean-free marginalizations ($Y_{stim}$, $Y_{dec}$, $Y_{int}$) by calculating the sum of their pairwise Frobenius inner products. The Frobenius inner product of two matrices $A, B \\in \\mathbb{R}^{m \\times n}$ is defined as $\\langle A, B \\rangle_F = \\sum_{i=1}^{m} \\sum_{j=1}^{n} A_{ij} B_{ij}$.\n\nWe compute the three pairwise inner products:\n\n1.  $\\langle Y_{stim}, Y_{dec} \\rangle_F$:\n    $$ \\langle Y_{stim}, Y_{dec} \\rangle_F = (-2.5)(-0.5) + (-2.5)(0.5) + (2.5)(-0.5) + (2.5)(0.5) $$\n    $$ = 1.25 - 1.25 - 1.25 + 1.25 = 0 $$\n\n2.  $\\langle Y_{stim}, Y_{int} \\rangle_F$:\n    $$ \\langle Y_{stim}, Y_{int} \\rangle_F = (-2.5)(1.5) + (-2.5)(-1.5) + (2.5)(-1.5) + (2.5)(1.5) $$\n    $$ = -3.75 + 3.75 - 3.75 + 3.75 = 0 $$\n\n3.  $\\langle Y_{dec}, Y_{int} \\rangle_F$:\n    $$ \\langle Y_{dec}, Y_{int} \\rangle_F = (-0.5)(1.5) + (0.5)(-1.5) + (-0.5)(-1.5) + (0.5)(1.5) $$\n    $$ = -0.75 - 0.75 + 0.75 + 0.75 = 0 $$\n\nEach of the three pairwise Frobenius inner products is $0$, confirming their mutual orthogonality. This is a general property for balanced designs.\n\nThe final step is to calculate the sum of these three pairwise inner products:\n$$ \\text{Sum} = \\langle Y_{stim}, Y_{dec} \\rangle_F + \\langle Y_{stim}, Y_{int} \\rangle_F + \\langle Y_{dec}, Y_{int} \\rangle_F = 0 + 0 + 0 = 0 $$\nThe total sum is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The power of dPCA to \"demix\" neural activity lies in its intelligent use of data centering. This practice explores how different centering strategies allow us to isolate specific sources of variance before applying Principal Component Analysis. By comparing the covariance matrices resulting from global versus per-condition mean subtraction, you will see precisely how dPCA can disentangle mixed signals corresponding to different task variables, such as stimulus and time .",
            "id": "4154819",
            "problem": "You are analyzing a simple neuronal population dataset to illustrate how centering choices influence demixing in demixed principal component analysis (dPCA). Consider a population of $2$ neurons recorded under $2$ stimuli ($s \\in \\{1,2\\}$) and $2$ time points ($t \\in \\{1,2\\}$). The trial-averaged population activity vectors (in spike-rate units, but units are not required in the final answer) for each condition are:\n- $x_{:,1,1} = \\begin{pmatrix}8\\\\3\\end{pmatrix}$,\n- $x_{:,1,2} = \\begin{pmatrix}8\\\\7\\end{pmatrix}$,\n- $x_{:,2,1} = \\begin{pmatrix}2\\\\3\\end{pmatrix}$,\n- $x_{:,2,2} = \\begin{pmatrix}2\\\\7\\end{pmatrix}$.\n\nAssume that demixed principal component analysis (dPCA) seeks to separate stimulus-related and time-related components by appropriate centering schemes before computing covariance matrices across neurons. You will compare two centering schemes:\n- Global mean subtraction: subtract the grand mean across all $(s,t)$ conditions from each observation, then compute the covariance across neurons using the population normalization by the number of observations.\n- Per-condition mean subtraction (within-stimulus centering): for each stimulus $s$, subtract the mean across time $t$ for that stimulus from the corresponding observations, then compute the covariance across neurons using the same population normalization.\n\nStarting from foundational definitions of centering and covariance for multivariate data, compute the two covariance matrices implied by these centering schemes, identify the largest eigenvalue (i.e., the first principal component variance) for each, and report the ratio of the largest eigenvalue under global mean subtraction to the largest eigenvalue under per-condition mean subtraction. Provide your final answer as an exact value. No rounding is required. Do not include units in your final answer.",
            "solution": "We begin by formalizing the data and the two centering schemes. Let $N=2$ denote the number of neurons and let $M=4$ denote the number of observed condition-time combinations. The observations are $x_{m} \\in \\mathbb{R}^{2}$ for $m \\in \\{1,2,3,4\\}$ corresponding to $(s,t) \\in \\{(1,1),(1,2),(2,1),(2,2)\\}$, respectively:\n$$\nx_{1}=\\begin{pmatrix}8\\\\3\\end{pmatrix},\\quad\nx_{2}=\\begin{pmatrix}8\\\\7\\end{pmatrix},\\quad\nx_{3}=\\begin{pmatrix}2\\\\3\\end{pmatrix},\\quad\nx_{4}=\\begin{pmatrix}2\\\\7\\end{pmatrix}.\n$$\nBy definition, the population covariance across neurons for centered data $\\tilde{x}_{m}$ is\n$$\nC \\;=\\; \\frac{1}{M}\\sum_{m=1}^{M} \\tilde{x}_{m}\\,\\tilde{x}_{m}^{\\top}.\n$$\nWe will compute this under two different centering schemes.\n\nGlobal mean subtraction: Let the grand mean across all conditions be\n$$\n\\bar{x} \\;=\\; \\frac{1}{M}\\sum_{m=1}^{M} x_{m}.\n$$\nCompute $\\bar{x}$ componentwise. For neuron $1$,\n$$\n\\bar{x}_{1} \\;=\\; \\frac{8+8+2+2}{4} \\;=\\; \\frac{20}{4} \\;=\\; 5.\n$$\nFor neuron $2$,\n$$\n\\bar{x}_{2} \\;=\\; \\frac{3+7+3+7}{4} \\;=\\; \\frac{20}{4} \\;=\\; 5.\n$$\nHence $\\bar{x}=\\begin{pmatrix}5\\\\5\\end{pmatrix}$. The centered observations are\n$$\n\\tilde{x}_{1}=\\begin{pmatrix}3\\\\-2\\end{pmatrix},\\quad\n\\tilde{x}_{2}=\\begin{pmatrix}3\\\\2\\end{pmatrix},\\quad\n\\tilde{x}_{3}=\\begin{pmatrix}-3\\\\-2\\end{pmatrix},\\quad\n\\tilde{x}_{4}=\\begin{pmatrix}-3\\\\2\\end{pmatrix}.\n$$\nCompute the covariance:\n$$\nC_{\\text{global}} \\;=\\; \\frac{1}{4}\\sum_{m=1}^{4}\\tilde{x}_{m}\\tilde{x}_{m}^{\\top}.\n$$\nNote that for each $\\tilde{x}_{m}=\\begin{pmatrix}\\tilde{x}_{m,1}\\\\\\tilde{x}_{m,2}\\end{pmatrix}$, $\\tilde{x}_{m,1}\\in\\{3,-3\\}$ and $\\tilde{x}_{m,2}\\in\\{-2,2\\}$ occur equally often with independent signs across the $4$ observations, and the cross-term averages to zero. Therefore,\n$$\n\\frac{1}{4}\\sum_{m=1}^{4}\\tilde{x}_{m,1}^{2} \\;=\\; 9,\\qquad\n\\frac{1}{4}\\sum_{m=1}^{4}\\tilde{x}_{m,2}^{2} \\;=\\; 4,\\qquad\n\\frac{1}{4}\\sum_{m=1}^{4}\\tilde{x}_{m,1}\\tilde{x}_{m,2} \\;=\\; 0.\n$$\nThus\n$$\nC_{\\text{global}} \\;=\\; \\begin{pmatrix}9 & 0\\\\ 0 & 4\\end{pmatrix}.\n$$\nThe eigenvalues of a diagonal matrix are the diagonal entries, so the largest eigenvalue of $C_{\\text{global}}$ is\n$$\n\\lambda_{\\max}(C_{\\text{global}}) \\;=\\; 9.\n$$\n\nPer-condition mean subtraction (within-stimulus centering): For each stimulus $s\\in\\{1,2\\}$, compute the mean across the two time points $t\\in\\{1,2\\}$ and subtract it from the corresponding observations with that $s$. For $s=1$,\n$$\n\\bar{x}^{(s=1)} \\;=\\; \\frac{1}{2}\\left(\\begin{pmatrix}8\\\\3\\end{pmatrix}+\\begin{pmatrix}8\\\\7\\end{pmatrix}\\right) \\;=\\; \\begin{pmatrix}8\\\\5\\end{pmatrix}.\n$$\nFor $s=2$,\n$$\n\\bar{x}^{(s=2)} \\;=\\; \\frac{1}{2}\\left(\\begin{pmatrix}2\\\\3\\end{pmatrix}+\\begin{pmatrix}2\\\\7\\end{pmatrix}\\right) \\;=\\; \\begin{pmatrix}2\\\\5\\end{pmatrix}.\n$$\nThe within-stimulus centered observations become\n$$\n\\tilde{x}_{1}^{(\\text{pc})}=\\begin{pmatrix}0\\\\-2\\end{pmatrix},\\quad\n\\tilde{x}_{2}^{(\\text{pc})}=\\begin{pmatrix}0\\\\2\\end{pmatrix},\\quad\n\\tilde{x}_{3}^{(\\text{pc})}=\\begin{pmatrix}0\\\\-2\\end{pmatrix},\\quad\n\\tilde{x}_{4}^{(\\text{pc})}=\\begin{pmatrix}0\\\\2\\end{pmatrix}.\n$$\nCompute the covariance with population normalization:\n$$\nC_{\\text{pc}} \\;=\\; \\frac{1}{4}\\sum_{m=1}^{4}\\tilde{x}_{m}^{(\\text{pc})}\\left(\\tilde{x}_{m}^{(\\text{pc})}\\right)^{\\top}.\n$$\nClearly, the first coordinate is identically $0$ after within-stimulus centering, and the second coordinate takes values $\\pm 2$ equally often. Therefore,\n$$\nC_{\\text{pc}} \\;=\\; \\begin{pmatrix}0 & 0\\\\ 0 & 4\\end{pmatrix},\n$$\nwhose largest eigenvalue is\n$$\n\\lambda_{\\max}(C_{\\text{pc}}) \\;=\\; 4.\n$$\n\nImpact on demixing: Under global mean subtraction, both stimulus offsets (along neuron $1$) and temporal modulations (along neuron $2$) contribute to variance, yielding $C_{\\text{global}}=\\begin{pmatrix}9&0\\\\0&4\\end{pmatrix}$ and a dominant component aligned with neuron $1$. Under per-condition centering, the stimulus mean offsets are removed within each stimulus, isolating time-related variance so that $C_{\\text{pc}}=\\begin{pmatrix}0&0\\\\0&4\\end{pmatrix}$, concentrating variance along neuron $2$. This illustrates how within-condition centering facilitates demixing by eliminating stimulus main effects from the covariance.\n\nThe requested ratio of the largest eigenvalue under global mean subtraction to that under per-condition mean subtraction is\n$$\n\\frac{\\lambda_{\\max}(C_{\\text{global}})}{\\lambda_{\\max}(C_{\\text{pc}})} \\;=\\; \\frac{9}{4}.\n$$\nNo rounding is required.",
            "answer": "$$\\boxed{\\frac{9}{4}}$$"
        },
        {
            "introduction": "After performing dPCA, a critical question is how successfully the variance has been demixed. In an ideal case, the subspaces corresponding to different task parameters would be perfectly orthogonal, but in practice, there is often some \"leakage\" or overlap. This exercise introduces a rigorous method to quantify this overlap by computing the principal angles between the demixed subspaces, providing a crucial metric for interpreting the quality and limitations of the dPCA model .",
            "id": "4154790",
            "problem": "You are analyzing demixed principal component analysis (dPCA) subspaces derived from neuronal population data. For each task variable index $m$, the demixed component directions are assembled as a column-space basis matrix $U^{(m)} \\in \\mathbb{R}^{p \\times k_m}$, whose columns span the demixed subspace $\\mathcal{S}_m \\subset \\mathbb{R}^p$. In practice, the columns of $U^{(m)}$ may not be orthonormal. To scientifically quantify cross-variable leakage, you are asked to define the principal angles $\\theta_i$ between two subspaces $\\mathcal{S}_m$ and $\\mathcal{S}_{m'}$ and compute them using Singular Value Decomposition (SVD), a well-established linear algebra factorization. Use these angles to construct a leakage index that is interpretable in the context of dPCA.\n\nStarting from the fundamental definitions of subspaces, orthonormal bases, and the optimization-based definition of principal angles, design and implement a program that, for each pair $(U^{(m)}, U^{(m')})$, performs the following steps:\n\n- Orthonormalize the columns of $U^{(m)}$ and $U^{(m')}$ to obtain two orthonormal basis matrices $Q^{(m)}$ and $Q^{(m')}$ spanning the same subspaces. Use a numerically stable method based on $QR$ factorization.\n\n- Compute the principal angles $\\theta_i$ between $\\mathcal{S}_m$ and $\\mathcal{S}_{m'}$ via Singular Value Decomposition (SVD), applied to an appropriate matrix derived from $Q^{(m)}$ and $Q^{(m')}$. Derive the connection between the SVD outputs and principal angles from first principles; do not use or state any shortcut formulas directly in the problem statement.\n\n- Define a symmetric leakage index $L_{m,m'}$ as the average of squared cosines of the principal angles, i.e., $$L_{m,m'} = \\frac{1}{r} \\sum_{i=1}^{r} \\cos^2(\\theta_i),$$ where $r = \\min(k_m, k_{m'})$. This index is dimensionless and lies in the closed interval $[0, 1]$, with values closer to $1$ indicating stronger overlap and potential leakage between demixed subspaces.\n\n- Return the principal angles in degrees and the leakage index $L_{m,m'}$. All angles must be expressed in degrees. Round all floating-point outputs to six decimal places.\n\nTest Suite. For scientific coverage of typical and edge cases in dPCA subspace comparisons, use the following parameter sets. In each case, $U^{(m)}$ and $U^{(m')}$ are given explicitly as column-stacked matrices, and all entries are real numbers.\n\n- Case $1$ (happy path, identical subspaces, maximal overlap):\n  Ambient dimension $p = 4$. Let\n  $$U^{(m)} = \\begin{bmatrix}\n  1 & 0 \\\\\n  0 & 1 \\\\\n  0 & 0 \\\\\n  0 & 0\n  \\end{bmatrix}, \\quad\n  U^{(m')} = \\begin{bmatrix}\n  1 & 0 \\\\\n  0 & 1 \\\\\n  0 & 0 \\\\\n  0 & 0\n  \\end{bmatrix}.$$\n\n- Case $2$ (orthogonal subspaces, no overlap):\n  Ambient dimension $p = 4$. Let\n  $$U^{(m)} = \\begin{bmatrix}\n  1 & 0 \\\\\n  0 & 1 \\\\\n  0 & 0 \\\\\n  0 & 0\n  \\end{bmatrix}, \\quad\n  U^{(m')} = \\begin{bmatrix}\n  0 & 0 \\\\\n  0 & 0 \\\\\n  1 & 0 \\\\\n  0 & 1\n  \\end{bmatrix}.$$\n\n- Case $3$ (partial overlap, unequal subspace dimensions):\n  Ambient dimension $p = 5$. Let the angle parameter be $\\theta = 30^\\circ$ with $\\cos(\\theta) = \\frac{\\sqrt{3}}{2}$ and $\\sin(\\theta) = \\frac{1}{2}$. Define\n  $$U^{(m)} = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 1 \\\\\n  0 & 0 & 0 \\\\\n  0 & 0 & 0\n  \\end{bmatrix}, \\quad\n  U^{(m')} = \\begin{bmatrix}\n  \\cos(\\theta) & 0 \\\\\n  \\sin(\\theta) & 0 \\\\\n  0 & 0 \\\\\n  0 & 1 \\\\\n  0 & 0\n  \\end{bmatrix}.$$\n\n- Case $4$ (numerical edge case, nearly collinear vectors and non-orthonormal columns):\n  Ambient dimension $p = 3$. Let $\\varepsilon = 10^{-8}$. Define\n  $$U^{(m)} = \\begin{bmatrix}\n  1 & 1 \\\\\n  0 & \\varepsilon \\\\\n  0 & 0\n  \\end{bmatrix}, \\quad\n  U^{(m')} = \\begin{bmatrix}\n  1 \\\\\n  -\\varepsilon \\\\\n  0\n  \\end{bmatrix}.$$\n\nImplementation and Output Requirements.\n\n- For each case, compute the list of principal angles $\\theta_i$ (in degrees) sorted in ascending order, then append the leakage index $L_{m,m'}$ as the last element of the list. Round all numerical outputs to six decimal places.\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to one test case and is itself a list of floating-point numbers (the principal angles followed by the leakage index). For example, the output format must be $$[\\text{case}_1,\\text{case}_2,\\text{case}_3,\\text{case}_4],$$ where each $\\text{case}_i$ is a list.\n\n- Angle unit: degrees. No physical units are otherwise required. Express all floating-point outputs rounded to six decimal places.\n\n- All computations must be fully self-contained and based strictly on the provided matrices; do not require any external input.",
            "solution": "The problem requires the formulation and implementation of a procedure to quantify the alignment, or \"leakage,\" between two subspaces, $\\mathcal{S}_m$ and $\\mathcal{S}_{m'}$, derived from a demixed Principal Component Analysis (dPCA). This is accomplished by computing the principal angles between the subspaces and then using them to calculate a defined leakage index, $L_{m,m'}$. The subspaces are provided via their basis matrices, $U^{(m)}$ and $U^{(m')}$, whose columns may not be orthonormal.\n\nThe solution proceeds in three main steps: first, constructing orthonormal bases for the subspaces; second, deriving the principal angles from first principles using the Singular Value Decomposition (SVD); and third, calculating the leakage index.\n\n### 1. Orthonormal Basis Construction\nA subspace is uniquely defined by a set of basis vectors, but this representation is not unique. For geometric calculations such as angles, it is computationally and conceptually necessary to work with orthonormal bases. Given a basis matrix $U \\in \\mathbb{R}^{p \\times k}$ whose columns span a $k$-dimensional subspace $\\mathcal{S} \\subset \\mathbb{R}^p$, we must find a matrix $Q \\in \\mathbb{R}^{p \\times k}$ whose columns are orthonormal (i.e., $Q^T Q = I_k$, where $I_k$ is the $k \\times k$ identity matrix) and span the same subspace, $\\text{span}(Q) = \\text{span}(U) = \\mathcal{S}$.\n\nA numerically stable method for this transformation is the QR factorization. For a matrix $A \\in \\mathbb{R}^{p \\times k}$ with $p \\ge k$ and full column rank, the 'thin' or 'reduced' QR factorization yields a unique decomposition $A = QR$, where $Q \\in \\mathbb{R}^{p \\times k}$ is a matrix with orthonormal columns and $R \\in \\mathbb{R}^{k \\times k}$ is an upper triangular, invertible matrix. The columns of $Q$ constitute the desired orthonormal basis.\n\nApplying this to the given matrices $U^{(m)}$ and $U^{(m')}$, we perform QR factorization to obtain the orthonormal basis matrices $Q^{(m)}$ and $Q^{(m')}$:\n$$ U^{(m)} = Q^{(m)} R^{(m)} $$\n$$ U^{(m')} = Q^{(m')} R^{(m')} $$\nThese matrices $Q^{(m)} \\in \\mathbb{R}^{p \\times k_m}$ and $Q^{(m')} \\in \\mathbb{R}^{p \\times k_{m'}}$ will be used for all subsequent calculations.\n\n### 2. Principal Angles from First Principles via SVD\nThe principal angles, $\\theta_i$, between two subspaces, $\\mathcal{S}_m$ and $\\mathcal{S}_{m'}$, provide a hierarchical and comprehensive measure of their relative orientation. They are defined recursively based on an optimization principle.\n\nThe first and smallest principal angle, $\\theta_1$, is the minimum possible angle between a vector $\\mathbf{u} \\in \\mathcal{S}_m$ and a vector $\\mathbf{v} \\in \\mathcal{S}_{m'}$. Maximizing the cosine of the angle is equivalent to minimizing the angle itself. Thus, $\\theta_1$ is defined by:\n$$ \\cos(\\theta_1) = \\max_{\\substack{\\mathbf{u} \\in \\mathcal{S}_m, \\|\\mathbf{u}\\|_2=1 \\\\ \\mathbf{v} \\in \\mathcal{S}_{m'}, \\|\\mathbf{v}\\|_2=1}} \\mathbf{u}^T \\mathbf{v} $$\nThe vectors $\\mathbf{u}_1$ and $\\mathbf{v}_1$ achieving this maximum are the first pair of principal vectors. Subsequent principal angles $\\theta_i$ for $i > 1$ are found by repeating this maximization process in the orthogonal complements of the previously found principal vectors.\n\nTo establish a computational method using SVD, we express the vectors $\\mathbf{u}$ and $\\mathbf{v}$ in terms of the orthonormal bases $Q^{(m)}$ and $Q^{(m')}$. Any unit vector $\\mathbf{u} \\in \\mathcal{S}_m$ can be written as $\\mathbf{u} = Q^{(m)} \\mathbf{x}$ for some coordinate vector $\\mathbf{x} \\in \\mathbb{R}^{k_m}$ satisfying $\\|\\mathbf{x}\\|_2 = 1$. Similarly, $\\mathbf{v} = Q^{(m')} \\mathbf{y}$ for some $\\mathbf{y} \\in \\mathbb{R}^{k_{m'}}$ with $\\|\\mathbf{y}\\|_2 = 1$. The dot product $\\mathbf{u}^T \\mathbf{v}$ can then be rewritten as:\n$$ \\mathbf{u}^T \\mathbf{v} = (Q^{(m)} \\mathbf{x})^T (Q^{(m')} \\mathbf{y}) = \\mathbf{x}^T ( (Q^{(m)})^T Q^{(m')} ) \\mathbf{y} $$\nLet $M = (Q^{(m)})^T Q^{(m')}$. The matrix $M \\in \\mathbb{R}^{k_m \\times k_{m'}}$ encapsulates the inner products between the basis vectors of the two subspaces. The optimization problem for $\\cos(\\theta_1)$ is now:\n$$ \\cos(\\theta_1) = \\max_{\\substack{\\|\\mathbf{x}\\|_2=1 \\\\ \\|\\mathbf{y}\\|_2=1}} \\mathbf{x}^T M \\mathbf{y} $$\nThis expression is the definition of the spectral norm (or largest singular value, $\\sigma_1$) of the matrix $M$. The Courant-Fischer theorem establishes that the singular values of $M$, sorted in descending order $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r \\ge 0$ where $r = \\min(k_m, k_{m'})$, are the solutions to the successive maximization problems that define the principal angles.\n\nTherefore, the cosines of the principal angles are precisely the singular values of the matrix $M = (Q^{(m)})^T Q^{(m')}$:\n$$ \\sigma_i = \\cos(\\theta_i) \\quad \\text{for } i = 1, \\dots, r $$\nSince the columns of $Q^{(m)}$ and $Q^{(m')}$ are unit vectors, the entries of $M$ are cosines, and the singular values $\\sigma_i$ are guaranteed to be in the interval $[0, 1]$. To compute the angles, we first find the singular values $\\sigma_i$ of $M$, and then calculate $\\theta_i = \\arccos(\\sigma_i)$. For numerical robustness, it is prudent to clip the computed singular values to the range $[0.0, 1.0]$ before applying the arccosine function to prevent errors from floating-point inaccuracies. The angles obtained will be in radians and must be converted to degrees.\n\n### 3. Leakage Index Calculation\nThe problem defines a symmetric leakage index $L_{m,m'}$ as the average of the squared cosines of the principal angles:\n$$ L_{m,m'} = \\frac{1}{r} \\sum_{i=1}^{r} \\cos^2(\\theta_i), \\quad \\text{where } r = \\min(k_m, k_{m'}) $$\nThis index provides a single scalar metric for subspace overlap. A value of $L_{m,m'} = 1$ indicates that the smaller subspace is entirely contained within the larger one (maximal overlap), while $L_{m,m'} = 0$ indicates that the subspaces are completely orthogonal (no overlap).\nSubstituting $\\cos(\\theta_i) = \\sigma_i$, the formula is directly computable from the singular values:\n$$ L_{m,m'} = \\frac{1}{r} \\sum_{i=1}^{r} \\sigma_i^2 $$\nThe final step is to sort the computed principal angles in ascending order and present them along with the leakage index, with all values rounded to $6$ decimal places as required.",
            "answer": "$$\\boxed{\\text{[[0.000000,0.000000,1.000000],[90.000000,90.000000,0.000000],[0.000000,90.000000,0.500000],[0.000000,1.000000]]}}$$"
        }
    ]
}