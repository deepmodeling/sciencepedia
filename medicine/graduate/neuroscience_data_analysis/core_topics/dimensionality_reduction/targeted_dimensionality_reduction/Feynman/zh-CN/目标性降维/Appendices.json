{
    "hands_on_practices": [
        {
            "introduction": "与捕捉总体方差的无监督方法（如主成分分析PCA）相比，靶向降维（TDR）擅长识别与特定任务相关的神经活动模式。本练习将指导您计算来自TDR和PCA的轴，并量化它们的对齐程度，从而提供一个动手实践的比较。通过这个练习，您将对TDR如何明确地分离出能够预测任务变量的维度有一个具体的理解。",
            "id": "4197341",
            "problem": "给定一个神经群体活动矩阵 $X \\in \\mathbb{R}^{N \\times T}$，表示在 $T$ 个时间点上观测到的 $N$ 个神经元，以及一个任务回归量矩阵 $R \\in \\mathbb{R}^{K \\times T}$，表示在相同的 $T$ 个时间点上的 $K$ 个与任务相关的变量。您的目标是使用靶向降维来量化神经数据中的任务对齐，方法是比较主成分分析（PCA）得到的主轴与通过线性预测任务变量得到的靶向轴。核心步骤必须从基本定义推导如下：沿时间维度对 $X$ 进行均值中心化，通过数学上严谨的分解获得主轴，通过解决从神经活动中预测每个回归量的最小二乘线性预测问题来构建靶向轴，并计算每个主轴和每个靶向轴之间在神经元权重上的 Pearson 相关性。高的绝对相关性应被解释为主导神经群体模式与任务相关方向之间的对齐。\n\n您必须使用的起点假设：\n- 均值中心化是沿时间维度执行的，以便每个神经元的时间序列均值为零。\n- 主成分分析（PCA）被定义为在神经元空间中使均值中心化数据方差最大化的正交方向。\n- 靶向降维寻求在神经元空间中能够从神经活动中线性预测每个任务回归量的方向。\n- Pearson 相关性是量化轴之间神经元权重的线性相似性的合适度量。\n\n对于下面的每个测试用例，实施以下程序：\n- 沿时间维度对 $X$ 和每个回归量 $r_k$ 进行均值中心化。\n- 计算神经元空间中的前 $M$ 个主轴。\n- 通过解决从神经活动中对每个回归量进行线性最小二乘预测的问题，计算神经元空间中的 $K$ 个靶向轴。\n- 计算每个主轴和每个靶向轴之间的 Pearson 相关系数，作为神经元索引的函数，然后取所有配对中的最大绝对相关性。\n- 将任务对齐判断为布尔值，当且仅当最大绝对相关性大于或等于指定的阈值 $\\tau$ 时，对齐为真。\n\n不涉及角度，也不涉及物理单位。所需输出中没有百分比。您的实现必须能从指定的数值输入中产生确定性和可复现的结果。\n\n测试套件规范。对于每个案例，$X$ 的形状为 $N \\times T$，$R$ 的形状为 $K \\times T$，主轴的数量为 $M$，对齐阈值为 $\\tau$。\n\n- 案例 1（理想情况，强对齐）：\n  - $N = 3$, $T = 4$, $K = 1$, $M = 1$, $\\tau = 0.9$。\n  - $X$ 的行：第 1 行为 $1,2,3,4$；第 2 行为 $0,0,0,0$；第 3 行为 $0,0,0,0$。\n  - $R$ 的行：第 1 行为 $1,2,3,4$。\n\n- 案例 2（正交性/弱对齐）：\n  - $N = 3$, $T = 4$, $K = 1$, $M = 1$, $\\tau = 0.8$。\n  - $X$ 的行：第 1 行为 $1,2,-1,0$；第 2 行为 $2,-1,2,-1$；第 3 行为 $0,0,0,0$。\n  - $R$ 的行：第 1 行为 $1,2,-1,0$。\n\n- 案例 3（符号反转但在绝对值上对齐）：\n  - $N = 3$, $T = 4$, $K = 1$, $M = 1$, $\\tau = 0.9$。\n  - $X$ 的行：第 1 行为 $-1,-2,-3,-4$；第 2 行为 $0,0,0,0$；第 3 行为 $0,0,0,0$。\n  - $R$ 的行：第 1 行为 $1,2,3,4$。\n\n- 案例 4（方差为零的退化回归量）：\n  - $N = 3$, $T = 4$, $K = 1$, $M = 1$, $\\tau = 0.5$。\n  - $X$ 的行：第 1 行为 $1,0,1,0$；第 2 行为 $0,1,0,1$；第 3 行为 $1,1,1,1$。\n  - $R$ 的行：第 1 行为 $0,0,0,0$。\n\n- 案例 5（多轴对齐，两个任务维度）：\n  - $N = 4$, $T = 5$, $K = 2$, $M = 2$, $\\tau = 0.9$。\n  - 定义 $r_1$ 为 $1,0,1,0,1$，定义 $r_2$ 为 $0,1,0,1,0$。\n  - 定义 $b_1$ 为 $1,0,1,0$，定义 $b_2$ 为 $0,1,0,1$。\n  - 将 $X$ 构建为外积 $b_1 r_1^\\top$ 和 $b_2 r_2^\\top$ 的和，即神经元 $i$ 和时间 $t$ 处的元素等于 $b_{1,i} r_{1,t} + b_{2,i} r_{2,t}$。\n  - $R$ 的行：第 1 行为 $r_1$；第 2 行为 $r_2$。\n\n每个案例的实现要求：\n- 对 $X$ 和 $R$ 中的每个回归量沿时间进行均值中心化。\n- 使用一种能产生神经元空间中标准正交主轴的分解方法。\n- 通过求解神经元空间中的最小二乘法正规方程来构建靶向轴。\n- 计算轴之间神经元索引上的 Pearson 相关性。如果任一轴在神经元条目上的标准差为零，则该配对的相关性定义为 $0.0$。\n- 相对于 $\\tau$ 将高的绝对相关性解释为对齐。\n\n最终输出格式：\n- 您的程序应产生一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，$[result_1,result_2,\\dots]$），其中每个 $result_i$ 是一个布尔值，表示使用指定阈值 $\\tau$ 时案例 $i$ 的任务对齐情况。",
            "solution": "该问题是有效的。它提出了一个定义明确、有科学依据的流程，用于使用线性代数和统计学中的标准方法来量化神经数据中的任务对齐。我现在将提供一个完整的解决方案。\n\n目标是确定神经群体活动的主导模式是否与神经元空间中能够预测特定任务变量的方向对齐。这是通过比较主成分分析（PCA）得到的主轴与线性回归得到的靶向轴来实现的。\n\n设神经活动矩阵为 $X \\in \\mathbb{R}^{N \\times T}$，任务回归量矩阵为 $R \\in \\mathbb{R}^{K \\times T}$。该过程包括以下步骤：\n\n1.  **数据均值中心化：**\n    第一步是移除每个神经元的平均活动水平以及每个任务回归量随时间的平均值。这确保了后续分析侧重于围绕均值的波动。\n    对于神经数据矩阵 $X$，神经元 $i$ 在 $T$ 个时间点上的均值为 $\\mu_{X_i} = \\frac{1}{T} \\sum_{t=1}^{T} X_{it}$。中心化后的矩阵记为 $X_c$，其条目为 $(X_c)_{it} = X_{it} - \\mu_{X_i}$。\n    同样，对于每个任务回归量 $r_k$（$R$ 的第 $k$ 行），其随时间的均值为 $\\mu_{r_k} = \\frac{1}{T} \\sum_{t=1}^{T} R_{kt}$。中心化后的回归量 $(r_k)_c$ 的条目为 $((r_k)_c)_t = R_{kt} - \\mu_{r_k}$。设 $R_c$ 是由这些中心化的行向量组成的矩阵。\n\n2.  **主成分分析（PCA）获取主轴：**\n    PCA 用于在 $N$ 维神经元空间中找到一组正交轴，这些轴捕获了神经数据 $X_c$ 中最大方差的方向。这些轴被称为主轴。\n    数据投影到单位向量 $w \\in \\mathbb{R}^N$ 上的方差由空间协方差矩阵 $C_X = \\frac{1}{T-1} X_c X_c^\\top$ 的特征向量最大化。\n    一种计算上稳健的找到这些轴的方法是通过对中心化数据矩阵进行奇异值分解（SVD）：\n    $$X_c = U \\Sigma V^\\top$$\n    这里，$U \\in \\mathbb{R}^{N \\times N}$ 是一个其列为左奇异向量的矩阵，$\\Sigma \\in \\mathbb{R}^{N \\times T}$ 是一个奇异值的矩形对角矩阵，$V \\in \\mathbb{R}^{T \\times T}$ 是一个其列为右奇异向量的矩阵。$U$ 的列是 $X_c X_c^\\top$ 的特征向量，代表主轴。$\\Sigma$ 中的奇异值与沿这些轴的标准差有关。\n    前 $M$ 个主轴 $\\{u_1, u_2, \\dots, u_M\\}$ 是矩阵 $U$ 的前 $M$ 列，对应于 $M$ 个最大的奇异值。\n\n3.  **线性回归获取靶向轴：**\n    靶向轴是神经元空间中对任务回归量最具预测性的方向。对于每个中心化的任务回归量 $(r_k)_c \\in \\mathbb{R}^{1 \\times T}$，我们寻求一个权重向量 $w_k \\in \\mathbb{R}^N$（即靶向轴），使得神经活动的线性组合 $w_k^\\top X_c$ 能最好地预测 $(r_k)_c$。\n    这被表述为一个线性最小二乘问题。我们的目标是找到使实际回归量值与预测值之间的平方误差和最小化的 $w_k$：\n    $$ \\min_{w_k} \\| (r_k)_c^\\top - X_c^\\top w_k \\|^2_2 $$\n    这里，我们将回归量时间序列 $(r_k)_c^\\top \\in \\mathbb{R}^{T \\times 1}$ 建模为 $X_c^\\top \\in \\mathbb{R}^{T \\times N}$ 各列的线性组合，其系数是 $w_k \\in \\mathbb{R}^{N \\times 1}$ 中的神经元权重。\n    解可以通过求解正规方程得到。一种数值上稳定的方法是使用伪逆，这等同于使用标准的最小二乘求解器。每个靶向轴 $w_k$ 的解为：\n    $$ w_k = (X_c^\\top)^\\dagger (r_k)_c^\\top $$\n    其中 $(X_c^\\top)^\\dagger$ 是 $X_c^\\top$ 的 Moore-Penrose 伪逆。这将产生 $K$ 个靶向轴 $\\{w_1, w_2, \\dots, w_K\\}$。\n\n4.  **使用 Pearson 相关性量化对齐：**\n    主轴 $u_m$ 和靶向轴 $w_k$ 之间的对齐程度由在其 $N$ 个神经元权重条目上计算的 Pearson 相关系数来量化。对于两个向量 $a, b \\in \\mathbb{R}^N$，相关性为：\n    $$ \\rho(a, b) = \\frac{\\sum_{i=1}^N (a_i - \\bar{a})(b_i - \\bar{b})}{\\sqrt{\\sum_{i=1}^N (a_i - \\bar{a})^2} \\sqrt{\\sum_{i=1}^N (b_i - \\bar{b})^2}} $$\n    其中 $\\bar{a}$ 和 $\\bar{b}$ 分别是向量 $a$ 和 $b$ 中元素的均值。高的绝对相关性 $|\\rho|$ 表明两个轴在神经元空间中代表相似的方向（最多相差一个符号翻转）。根据问题陈述，如果任一向量的标准差为零，则相关性定义为 $0.0$。\n\n5.  **最终对齐决策：**\n    我们计算所有 $M \\times K$ 对主轴和靶向轴的绝对 Pearson 相关性。这些值的最大值 $\\rho_{\\text{max}}$ 代表了在任何主导神经模式和任何任务预测方向之间找到的最强对齐。\n    $$ \\rho_{\\text{max}} = \\max_{m \\in \\{1,\\dots,M\\}, k \\in \\{1,\\dots,K\\}} |\\rho(u_m, w_k)| $$\n    任务对齐的最终决策是一个布尔值，通过将 $\\rho_{\\text{max}}$ 与给定的阈值 $\\tau$进行比较来确定：\n    $$ \\text{对齐} = (\\rho_{\\text{max}} \\ge \\tau) $$\n\n该过程提供了一种严谨且定量的方法，用以评估内在神经动力学与外部任务要求之间的关系。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the task alignment problem for a suite of test cases.\n    For each case, it computes principal axes from neural data (X) and\n    targeted axes from task regressors (R), then determines if there is\n    significant alignment based on a correlation threshold.\n    \"\"\"\n    # Test suite specification.\n    test_cases = [\n        # Case 1 (happy path, strong alignment)\n        {\n            \"X\": np.array([[1., 2., 3., 4.], [0., 0., 0., 0.], [0., 0., 0., 0.]]),\n            \"R\": np.array([[1., 2., 3., 4.]]),\n            \"M\": 1,\n            \"tau\": 0.9,\n        },\n        # Case 2 (orthogonality/low alignment)\n        {\n            \"X\": np.array([[1., 2., -1., 0.], [2., -1., 2., -1.], [0., 0., 0., 0.]]),\n            \"R\": np.array([[1., 2., -1., 0.]]),\n            \"M\": 1,\n            \"tau\": 0.8,\n        },\n        # Case 3 (sign inversion but aligned in absolute value)\n        {\n            \"X\": np.array([[-1., -2., -3., -4.], [0., 0., 0., 0.], [0., 0., 0., 0.]]),\n            \"R\": np.array([[1., 2., 3., 4.]]),\n            \"M\": 1,\n            \"tau\": 0.9,\n        },\n        # Case 4 (degenerate regressor with zero variance)\n        {\n            \"X\": np.array([[1., 0., 1., 0.], [0., 1., 0., 1.], [1., 1., 1., 1.]]),\n            \"R\": np.array([[0., 0., 0., 0.]]),\n            \"M\": 1,\n            \"tau\": 0.5,\n        },\n        # Case 5 (multi-axis alignment, two task dimensions)\n        {\n            \"X\": np.array([\n                [1., 0., 1., 0., 1.],\n                [0., 1., 0., 1., 0.],\n                [1., 0., 1., 0., 1.],\n                [0., 1., 0., 1., 0.]\n            ]),\n            \"R\": np.array([\n                [1., 0., 1., 0., 1.],\n                [0., 1., 0., 1., 0.]\n            ]),\n            \"M\": 2,\n            \"tau\": 0.9,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X, R, M, tau = case[\"X\"], case[\"R\"], case[\"M\"], case[\"tau\"]\n        N, T = X.shape\n        K = R.shape[0]\n\n        # Step 1: Mean-center X and R along the time dimension (axis=1)\n        X_c = X - X.mean(axis=1, keepdims=True)\n        R_c = R - R.mean(axis=1, keepdims=True)\n\n        # Step 2: Compute Principal Axes using SVD\n        # U contains the principal axes as columns\n        # We use full_matrices=False for efficiency, as we only need the N x N U matrix\n        if X_c.shape[1] > X_c.shape[0]: # T > N case\n             U, s, Vt = np.linalg.svd(X_c, full_matrices=True)\n        else:\n             U, s, Vt = np.linalg.svd(X_c, full_matrices=False)\n        principal_axes = U[:, :M]\n\n        # Step 3: Compute Targeted Axes via Linear Least-Squares\n        targeted_axes = []\n        for k in range(K):\n            r_k_c = R_c[k, :]\n            # We solve X_c.T @ w_k = r_k_c.T for w_k\n            # np.linalg.lstsq solves a @ x = b. Here a=X_c.T, b=r_k_c.T, x=w_k\n            w_k, residuals, rank, s_vals = np.linalg.lstsq(X_c.T, r_k_c.T, rcond=None)\n            targeted_axes.append(w_k)\n\n        # Step 4: Compute max absolute Pearson correlation\n        max_abs_corr = 0.0\n        for m in range(M):\n            pa = principal_axes[:, m]\n            for k in range(K):\n                ta = targeted_axes[k]\n\n                # Handle the case where an axis has zero standard deviation\n                pa_std = np.std(pa)\n                ta_std = np.std(ta)\n                \n                if pa_std == 0.0 or ta_std == 0.0:\n                    corr = 0.0\n                else:\n                    # np.corrcoef returns a 2x2 matrix\n                    corr_matrix = np.corrcoef(pa, ta)\n                    corr = corr_matrix[0, 1]\n                \n                max_abs_corr = max(max_abs_corr, abs(corr))\n\n        # Step 5: Decide alignment\n        alignment = max_abs_corr >= tau\n        results.append(str(alignment))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一旦确定了与任务相关的维度，关键的下一步是解释其生物学意义：哪些神经元是该维度的主要驱动因素？本练习介绍了一种将沿靶向轴解释的总方差分解为单个神经元贡献的方法。掌握这项技术对于从抽象的维度转向关于特定细胞在神经回路中作用的具体假设至关重要。",
            "id": "4197409",
            "problem": "考虑一个神经群体数据集，由一个试验×神经元活动矩阵 $X \\in \\mathbb{R}^{T \\times N}$、一个任务回归量 $y \\in \\mathbb{R}^{T}$，以及一个可选的代表目标维度的固定轴 $a \\in \\mathbb{R}^{N}$ 表示。你的目标是计算神经元级别对神经元空间中单个轴上的任务相关信号的可解释方差的贡献，并根据其贡献对神经元进行排序。必须使用最小二乘回归、方差和协方差的定义，从基本原理出发解决该任务，除了线性代数和基本运算外，不依赖任何预封装的统计函数。\n\n从以下核心定义和事实出发：\n- 用于从 $X$ 预测 $y$ 的普通最小二乘解 $a^{\\star}$ 在 $a \\in \\mathbb{R}^{N}$ 上最小化平方误差 $\\|X a - y\\|_2^2$。当 $X$ 的列线性相关时，Moore–Penrose 伪逆可得出最小范数解。\n- 对于中心化数据（已移除列均值），样本协方差矩阵为 $\\Sigma = \\frac{1}{T-1} X^{\\top} X$，一个中心化向量的样本方差为 $s_y^2 = \\frac{1}{T-1} y^{\\top} y$。\n- 对于神经元空间中的任意轴 $a \\in \\mathbb{R}^{N}$，投影 $X a$ 的方差为 $\\mathrm{Var}(X a) = a^{\\top} \\Sigma a$。\n- 投影方差的神经元级别加性分解可由 $c_i = a_i \\left(\\Sigma a\\right)_i$（对于 $i \\in \\{0,\\dots,N-1\\}$）定义，其满足 $\\sum_{i=0}^{N-1} c_i = a^{\\top}\\Sigma a$。\n\n你必须为每个测试用例实现以下计算过程：\n- 对 $X$ 进行列向中心化，并通过减去其在所有试验中的均值来对 $y$ 进行中心化。对 $X$ 和 $y$ 使用相同的 $T$。\n- 如果没有提供轴 $a$，则使用 Moore–Penrose 伪逆求解 $y$ 对 $X$ 的最小二乘回归来计算 $a^{\\star}$。将此 $a^{\\star}$ 用作轴。\n- 计算样本协方差矩阵 $\\Sigma = \\frac{1}{T-1} X^{\\top} X$ 和 $s_y^2 = \\frac{1}{T-1} y^{\\top} y$。\n- 计算每个神经元的贡献 $c_i = a_i \\left(\\Sigma a\\right)_i$（对于 $i \\in \\{0,\\dots,N-1\\}$），使得 $\\sum_i c_i = a^{\\top} \\Sigma a = \\mathrm{Var}(X a)$。当 $s_y^2 > 0$ 时，将神经元 $i$ 贡献的可解释方差分数定义为 $f_i = \\frac{c_i}{s_y^2}$。如果 $s_y^2 = 0$，则对所有 $i$ 定义 $f_i = 0$。\n- 当 $s_y^2 > 0$ 时，将沿该轴的总可解释方差分数定义为 $R^2 = \\frac{\\sum_i c_i}{s_y^2}$；当 $s_y^2 = 0$ 时，定义 $R^2 = 0$。\n- 按 $f_i$ 降序对神经元进行排序。按神经元索引升序（即，最小索引优先）打破平局关系。\n\n你的程序必须处理以下测试套件。对于所有测试用例，索引都是从0开始的。所有矩阵和向量都已明确指定。\n\n测试用例 1（通用，良态）：\n- $T = 6$, $N = 4$。\n- $X = \\begin{bmatrix}\n1  0  2  1 \\\\\n0  1  1  2 \\\\\n2  1  3  0 \\\\\n3  2  5  1 \\\\\n1  1  2  2 \\\\\n0  0  1  0\n\\end{bmatrix}$。\n- $y = \\begin{bmatrix} 1  0  1  2  1  0 \\end{bmatrix}^{\\top}$。\n- 提供的轴：无（通过最小二乘法计算）。\n\n测试用例 2（包含一个零方差神经元）：\n- $T = 4$, $N = 3$。\n- $X = \\begin{bmatrix}\n1  2  5 \\\\\n1  0  5 \\\\\n1  -1  5 \\\\\n1  1  5\n\\end{bmatrix}$。\n- $y = \\begin{bmatrix} 0  1  -1  0 \\end{bmatrix}^{\\top}$。\n- 提供的轴：无（通过最小二乘法计算）。\n\n测试用例 3（外部提供的轴）：\n- $T = 5$, $N = 3$。\n- $X = \\begin{bmatrix}\n0  1  2 \\\\\n1  2  3 \\\\\n2  3  4 \\\\\n3  4  5 \\\\\n4  5  6\n\\end{bmatrix}$。\n- $y = \\begin{bmatrix} -1  0  1  2  3 \\end{bmatrix}^{\\top}$。\n- 提供的轴：$a = \\begin{bmatrix} 1.0  -0.5  0.0 \\end{bmatrix}^{\\top}$。\n\n测试用例 4（任务回归量方差为零）：\n- $T = 4$, $N = 2$。\n- $X = \\begin{bmatrix}\n0  1 \\\\\n1  1 \\\\\n2  1 \\\\\n3  1\n\\end{bmatrix}$。\n- $y = \\begin{bmatrix} 5  5  5  5 \\end{bmatrix}^{\\top}$。\n- 提供的轴：无（通过最小二乘法计算，但 $s_y^2 = 0$ 将强制 $R^2 = 0$ 且所有 $f_i = 0$）。\n\n测试用例 5（共线性神经元，奇异设计）：\n- $T = 5$, $N = 3$。\n- $X = \\begin{bmatrix}\n0  0  1 \\\\\n1  2  2 \\\\\n2  4  3 \\\\\n3  6  4 \\\\\n4  8  5\n\\end{bmatrix}$。\n- $y = \\begin{bmatrix} 0  1  2  3  4 \\end{bmatrix}^{\\top}$。\n- 提供的轴：无（使用伪逆通过最小二乘法计算）。\n\n最终输出规范：\n- 对于每个测试用例，生成一个三元组，包含：1) 四舍五入到6位小数的标量 $R^2$，2) 按 $f_i$ 降序排列（平局时按索引升序）的神经元索引列表，3) 按索引顺序排列的 $f_i$（对于 $i = 0, \\dots, N-1$）列表，每个值四舍五入到6位小数。\n- 你的程序应生成单行输出，其中包含这些三元组的逗号分隔列表，并用方括号括起来。例如： $[\\,[R^2_1,[i_{1,1},\\dots],[f_{1,0},\\dots]],\\,[R^2_2,[i_{2,1},\\dots],[f_{2,0},\\dots]],\\,\\dots\\,]$。\n- 此问题中没有物理单位或角度单位。所有报告的分数必须是小数，而不是百分比。",
            "solution": "该问题在计算神经科学和统计学领域提出了一个定义明确且有科学依据的任务。它要求实现一种将任务变量的可解释方差归因于单个神经元的特定方法。所有提供的定义在数学上都是合理的，计算过程也得到了明确规定，包括处理潜在问题，如共线性（通过 Moore-Penrose 伪逆）和因变量中的零方差。该问题是自洽的、客观的且内部一致的。因此，该问题是有效的，可以推导出解决方案。\n\n该方法将通过遵循规定的计算步骤来实现。该程序的核心是量化任务相关信号 $y$ 中的方差有多少可以由高维神经元空间中投影到特定一维轴 $a$ 上的神经活动 $X$ 来解释。\n\n首先，必须准备原始数据。令 $X \\in \\mathbb{R}^{T \\times N}$ 为 $N$ 个神经元在 $T$ 次试验中的神经活动矩阵，令 $y \\in \\mathbb{R}^{T}$ 为相应的任务回归量。神经活动和任务回归量都必须通过减去各自的均值来进行中心化。这一步确保了后续的方差和协方差计算不受基线偏移的影响。中心化后的数据表示为 $X_c$ 和 $y_c$。\n$X$ 的列均值 $\\mu_X \\in \\mathbb{R}^{N}$ 和 $y$ 的均值 $\\mu_y \\in \\mathbb{R}$ 计算如下：\n$$ \\mu_{X,j} = \\frac{1}{T} \\sum_{i=0}^{T-1} X_{ij} \\quad \\text{for } j=0, \\dots, N-1 $$\n$$ \\mu_y = \\frac{1}{T} \\sum_{i=0}^{T-1} y_i $$\n然后，中心化矩阵为 $X_c = X - \\mathbf{1}\\mu_X^{\\top}$ 和 $y_c = y - \\mu_y\\mathbf{1}$，其中 $\\mathbf{1}$ 是一个包含 $T$ 个 1 的列向量。\n\n其次，必须确定投影轴 $a \\in \\mathbb{R}^{N}$。问题规定了两种情况：\n1. 轴 $a$ 是外部提供的。这对应于测试一个关于神经状态空间中特定方向的预定义假设。\n2. 没有提供轴。在这种情况下，它被计算为从 $X_c$ 预测 $y_c$ 的最优线性解码器。这就是普通最小二乘（OLS）解 $a^{\\star}$，它最小化了平方误差 $\\|X_c a - y_c\\|_2^2$。为确保即使在 $X_c$ 的列线性相关（即设计是奇异的）时也存在唯一解，我们使用 Moore-Penrose 伪逆 $X_c^{+}$ 计算 $a^{\\star}$：\n$$ a = a^{\\star} = X_c^{+} y_c $$\n这在所有可能的解中给出了最小范数权重向量，是一种标准且稳健的方法。\n\n第三，我们计算必要的二阶统计量。对于中心化数据，神经活动的样本协方差矩阵 $\\Sigma \\in \\mathbb{R}^{N \\times N}$ 为：\n$$ \\Sigma = \\frac{1}{T-1} X_c^{\\top} X_c $$\n任务回归量的样本方差 $s_y^2$ 为：\n$$ s_y^2 = \\frac{1}{T-1} y_c^{\\top} y_c $$\n分母 $T-1$ 用于样本方差/协方差的无偏估计。如果任务回归量没有方差，即 $s_y^2=0$，则会出现一个特殊情况。在这种情况下，没有方差可以被解释，因此问题正确地规定所有得到的分数都应为零。\n\n第四，我们计算每个神经元对可解释方差的贡献。投影到轴 $a$ 上的神经数据的总方差由 $\\mathrm{Var}(X_c a) = a^{\\top}\\Sigma a$ 给出。问题将该方差定义为一个特定的、分解到每个神经元分量 $c_i$ 的加性分解：\n$$ c_i = a_i (\\Sigma a)_i \\quad \\text{for } i=0, \\dots, N-1 $$\n其中 $a_i$ 是轴向量 $a$ 的第 $i$ 个元素，而 $(\\Sigma a)_i$ 是协方差矩阵与轴向量相乘所得向量的第 $i$ 个元素。这些分量的总和恢复了总投影方差：$\\sum_{i=0}^{N-1} c_i = \\sum_{i=0}^{N-1} a_i (\\Sigma a)_i = a^{\\top}(\\Sigma a) = \\mathrm{Var}(X_c a)$。\n\n第五，我们对这些贡献进行归一化，以将其表示为任务回归量总方差 $s_y^2$ 的分数。由神经元 $i$ 贡献的可解释方差分数表示为 $f_i$：\n$$ f_i = \\frac{c_i}{s_y^2} \\quad (\\text{if } s_y^2 > 0) $$\n沿轴 $a$ 的总可解释方差分数，表示为 $R^2$，是这些单独分数的总和：\n$$ R^2 = \\sum_{i=0}^{N-1} f_i = \\frac{\\sum_{i=0}^{N-1} c_i}{s_y^2} = \\frac{a^{\\top}\\Sigma a}{s_y^2} \\quad (\\text{if } s_y^2 > 0) $$\n如果 $s_y^2=0$，则 $R^2=0$ 且所有 $f_i=0$。当 $a$ 是 OLS 解 $a^{\\star}$ 时，这个 $R^2$ 就是标准的决定系数。\n\n最后，为了识别最具影响力的神经元，我们根据它们的贡献 $f_i$ 按降序进行排序。为确保排序唯一，任何 $f_i$ 值的平局关系都通过按升序对相应的神经元索引进行排序来打破。\n\n这个完整的、分步的过程提供了一种严谨且可复现的方法，用于分析神经元对任务编码的贡献，其基础是线性代数和统计学的基本原理。对于每个提供的测试用例，实现将忠实地遵循此设计。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes neuron-wise contributions to explained variance for a series of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"id\": 1,\n            \"T\": 6, \"N\": 4,\n            \"X\": np.array([\n                [1, 0, 2, 1],\n                [0, 1, 1, 2],\n                [2, 1, 3, 0],\n                [3, 2, 5, 1],\n                [1, 1, 2, 2],\n                [0, 0, 1, 0]\n            ], dtype=float),\n            \"y\": np.array([1, 0, 1, 2, 1, 0], dtype=float),\n            \"a\": None\n        },\n        {\n            \"id\": 2,\n            \"T\": 4, \"N\": 3,\n            \"X\": np.array([\n                [1, 2, 5],\n                [1, 0, 5],\n                [1, -1, 5],\n                [1, 1, 5]\n            ], dtype=float),\n            \"y\": np.array([0, 1, -1, 0], dtype=float),\n            \"a\": None\n        },\n        {\n            \"id\": 3,\n            \"T\": 5, \"N\": 3,\n            \"X\": np.array([\n                [0, 1, 2],\n                [1, 2, 3],\n                [2, 3, 4],\n                [3, 4, 5],\n                [4, 5, 6]\n            ], dtype=float),\n            \"y\": np.array([-1, 0, 1, 2, 3], dtype=float),\n            \"a\": np.array([1.0, -0.5, 0.0], dtype=float)\n        },\n        {\n            \"id\": 4,\n            \"T\": 4, \"N\": 2,\n            \"X\": np.array([\n                [0, 1],\n                [1, 1],\n                [2, 1],\n                [3, 1]\n            ], dtype=float),\n            \"y\": np.array([5, 5, 5, 5], dtype=float),\n            \"a\": None\n        },\n        {\n            \"id\": 5,\n            \"T\": 5, \"N\": 3,\n            \"X\": np.array([\n                [0, 0, 1],\n                [1, 2, 2],\n                [2, 4, 3],\n                [3, 6, 4],\n                [4, 8, 5]\n            ], dtype=float),\n            \"y\": np.array([0, 1, 2, 3, 4], dtype=float),\n            \"a\": None\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        a_provided = case[\"a\"]\n        T, N = X.shape\n\n        # Step 1: Center data\n        X_c = X - np.mean(X, axis=0)\n        y_c = y - np.mean(y)\n\n        # Step 2: Determine axis a\n        if a_provided is not None:\n            a = a_provided\n        else:\n            X_c_plus = np.linalg.pinv(X_c)\n            a = X_c_plus @ y_c\n        \n        # Step 3: Compute sample covariance and variance\n        # Handle cases where T = 1, which leads to division by zero for sample variance\n        if T = 1:\n            s_y_sq = 0.0\n            Sigma = np.zeros((N, N))\n        else:\n            s_y_sq = (y_c.T @ y_c) / (T - 1)\n            Sigma = (X_c.T @ X_c) / (T - 1)\n\n        # Use a small tolerance for floating point comparison to zero\n        if s_y_sq  1e-12:\n            s_y_sq = 0.0\n        \n        # Step 4, 5: Compute R^2 and f_i, handling s_y^2 = 0 case\n        if s_y_sq == 0.0:\n            R2 = 0.0\n            f = np.zeros(N)\n        else:\n            c = a * (Sigma @ a)  # Element-wise product\n            sum_c = np.sum(c)\n            R2 = sum_c / s_y_sq\n            f = c / s_y_sq\n\n        # Step 6: Rank neurons by descending f_i, breaking ties with ascending index\n        neuron_scores = list(zip(f, range(N)))\n        neuron_scores.sort(key=lambda x: (-x[0], x[1]))\n        ranked_indices = [idx for score, idx in neuron_scores]\n\n        # Step 7: Format results for the current test case\n        R2_rounded = round(R2, 6)\n        f_rounded = [round(val, 6) for val in f]\n        \n        all_results.append([R2_rounded, ranked_indices, f_rounded])\n\n    # Final print statement in the exact required format\n    case_strs = []\n    for r2, indices, f_vals in all_results:\n        indices_str = f\"[{','.join(map(str, indices))}]\"\n        f_vals_str = f\"[{','.join(map(str, f_vals))}]\"\n        case_strs.append(f\"[{r2},{indices_str},{f_vals_str}]\")\n\n    print(f\"[{','.join(case_strs)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "检验神经编码鲁棒性的一个关键标准是其随时间的稳定性。在比较不同记录会话中发现的神经维度时，我们面临着旋转模糊性的挑战——底层的神经子空间可能相同，但描述它的具体轴可能是任意旋转的。这个高级练习介绍了普氏分析（Procrustes analysis），这是一种解决这种模糊性的强大技术，从而能够对跨会话的神经表征的稳定性进行原则性的比较和量化。",
            "id": "4197417",
            "problem": "给定两个会话的多元神经负荷矩阵，它们表示由靶向降维产生的轴。每个会话被建模为一个实值矩阵，其列张成一个低维轴子空间。设第一个会话的负荷矩阵为 $L_1 \\in \\mathbb{R}^{n \\times k}$，第二个会话为 $L_2 \\in \\mathbb{R}^{n \\times k}$，其中 $n$ 是神经元数量，$k$ 是轴的数量。目标是分析跨会话的轴的稳定性，并通过正交普罗克勒斯忒斯对齐消除旋转模糊性，然后量化直接矩阵差异的改善程度，并根据相关性准则确定对齐后的轴是否稳定。\n\n从线性代数中的以下基本定义和事实开始：\n- 一个矩阵 $R \\in \\mathbb{R}^{k \\times k}$ 是正交的，当且仅当 $R^\\top R = I_k$，其中 $I_k$ 是 $k \\times k$ 的单位矩阵。\n- 矩阵 $A \\in \\mathbb{R}^{p \\times q}$ 的弗罗贝尼乌斯范数定义为 $\\|A\\|_F = \\sqrt{\\sum_{i=1}^{p} \\sum_{j=1}^{q} a_{ij}^2}$。\n- 奇异值分解 (SVD) 将任意矩阵 $M \\in \\mathbb{R}^{p \\times q}$ 分解为 $M = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{p \\times p}$ 和 $V \\in \\mathbb{R}^{q \\times q}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{p \\times q}$ 是对角矩阵，其对角线上的元素为非负数。\n- 按神经元进行列中心化意味着对每一列 $j$，减去其 $n$ 个条目的均值。列单位归一化意味着将每个中心化后的列除以其欧几里得范数，使得每列的范数为单位1。\n\n为每对 $(L_1, L_2)$ 实现以下分析：\n1. 对 $L_1$ 和 $L_2$ 的各列按神经元进行中心化和单位归一化，得到 $\\tilde{L}_1$ 和 $\\tilde{L}_2$。\n2. 计算原始归一化弗罗贝尼乌斯差异 $d_{\\text{raw}} = \\|\\tilde{L}_1 - \\tilde{L}_2\\|_F / \\|\\tilde{L}_2\\|_F$。\n3. 解决正交普罗克勒斯忒斯对齐问题：找到一个正交矩阵 $R \\in \\mathbb{R}^{k \\times k}$，使得 $\\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F$ 最小。使用此对齐计算 $d_{\\text{aligned}} = \\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F / \\|\\tilde{L}_2\\|_F$。\n4. 定义改善量为 $\\Delta = d_{\\text{raw}} - d_{\\text{aligned}}$。\n5. 计算对齐后的轴与目标轴之间的平均绝对列相关性，定义为 $\\tilde{L}_1 R$ 的第 $j$ 列与 $\\tilde{L}_2$ 的第 $j$ 列之间余弦相似度的绝对值在 $j=1,\\dots,k$ 上的算术平均值，即 $\\frac{1}{k} \\sum_{j=1}^{k} \\left| \\frac{(\\tilde{L}_1 R)_{\\cdot j}^\\top \\tilde{L}_2_{\\cdot j}}{\\|(\\tilde{L}_1 R)_{\\cdot j}\\|_2 \\, \\|\\tilde{L}_2_{\\cdot j}\\|_2} \\right|$。通过将此平均值与阈值 $\\tau$ 进行比较来确定稳定性，当且仅当平均绝对相关性大于或等于 $\\tau$ 时，声明该对是稳定的。\n\n凡是指定角度的地方，均使用弧度制。本问题中不出现物理单位。将所有布尔决策表示为精确的布尔值，所有数值结果表示为实值浮点数。\n\n测试套件规范：\n- 设 $n = 6$ 和 $k = 2$。\n- 定义\n$$\nL_1 = \\begin{bmatrix}\n0.5  -0.2 \\\\\n0.3  0.1 \\\\\n0.0  0.4 \\\\\n-0.1  0.6 \\\\\n0.2  -0.3 \\\\\n0.7  0.0\n\\end{bmatrix}.\n$$\n构造四对 $(L_1, L_2)$:\n1. 情况 A (恒等)：$L_2 = L_1$。\n2. 情况 B (轴空间中的旋转)：$L_2 = L_1 Q$，其中\n$$\nQ = \\begin{bmatrix}\n\\cos(\\pi/6)  -\\sin(\\pi/6) \\\\\n\\sin(\\pi/6)  \\cos(\\pi/6)\n\\end{bmatrix},\n$$\n角度以弧度为单位。\n3. 情况 C (带符号翻转的置换)：$L_2 = L_1 P$，其中\n$$\nP = \\begin{bmatrix}\n0  -1 \\\\\n1  \\phantom{-}0\n\\end{bmatrix}.\n$$\n4. 情况 D (正交补轴)：设 $N$ 是 $\\mathbb{R}^n$ 中 $L_1$ 列空间的正交补空间的一个标准正交基，等价于 $L_1^\\top$ 的零空间。设 $N = [n_1 \\, n_2 \\, n_3 \\, n_4] \\in \\mathbb{R}^{n \\times (n-k)}$，其列以任意但确定的顺序排列。设置 $L_2 = [n_1 \\, n_2] \\in \\mathbb{R}^{n \\times 2}$。\n\n稳定性阈值：\n- 使用 $\\tau = 0.98$。\n\n你的程序应为每种情况计算有序对 $\\left[\\Delta, \\text{stable}\\right]$，其中 $\\Delta$ 是作为浮点数的改善量，$\\text{stable}$ 是一个布尔值，指示对齐后的平均绝对相关性是否超过或等于 $\\tau$。\n\n最终输出格式：\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素对应一个测试用例，并且本身是一个形式为 $\\left[\\Delta,\\text{stable}\\right]$ 的双元素列表。例如，打印的行应类似于 $[[\\Delta_1,\\text{stable}_1],[\\Delta_2,\\text{stable}_2],[\\Delta_3,\\text{stable}_3],[\\Delta_4,\\text{stable}_4]]$。",
            "solution": "该问题要求对两个神经负荷矩阵 $L_1, L_2 \\in \\mathbb{R}^{n \\times k}$ 进行比较分析，这两个矩阵代表了两个实验会话中通过靶向降维分析得到的轴。核心任务是使用正交普罗克勒斯忒斯分析来消除旋转模糊性，量化由此带来的矩阵相似性改善，并基于相关性准则评估对齐后轴的稳定性。\n\n该分析分为五个不同的步骤，这些步骤基于线性代数和多元统计的基本原理。\n\n**1. 负荷矩阵的归一化**\n\n原始负荷矩阵 $L_1$ 和 $L_2$ 可能在一些科学上无意义的方面存在差异，例如轴的全局平移或缩放。为了将比较集中在轴的几何方向上，我们首先对每个矩阵进行归一化。此过程独立应用于每个列向量。\n\n对于负荷矩阵 $L \\in \\mathbb{R}^{n \\times k}$ 的每一列 $L_{\\cdot j}$：\n首先，我们通过减去其元素的均值来进行列中心化。中心化后的列 $L_{c, \\cdot j}$ 由下式给出：\n$$\nL_{c, \\cdot j} = L_{\\cdot j} - \\mu_j \\mathbf{1}\n$$\n其中 $\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} L_{ij}$ 是第 $j$ 列的均值，$\\mathbf{1} \\in \\mathbb{R}^n$ 是一个全为1的向量。此步骤确保分析对于沿每个轴的神经活动基线漂移是不变的。\n\n其次，我们进行单位归一化。将中心化后的列除以其欧几里得范数（$L^2$-范数），以产生一个单位向量。归一化后的列 $\\tilde{L}_{\\cdot j}$ 是：\n$$\n\\tilde{L}_{\\cdot j} = \\frac{L_{c, \\cdot j}}{\\|L_{c, \\cdot j}\\|_2}\n$$\n其中 $\\| \\cdot \\|_2$ 表示欧几里得范数。此步骤使比较不受轴的任意缩放的影响。\n\n将此过程应用于 $L_1$ 和 $L_2$ 的所有列，得到归一化矩阵 $\\tilde{L}_1$ 和 $\\tilde{L}_2$。这些矩阵的每一列都是一个单位向量，并且与向量 $\\mathbf{1}$ 正交。$\\tilde{L}$ 的列空间位于一个维度至多为 $n-1$ 的子空间中。\n\n**2. 原始差异**\n\n在对齐之前，我们使用归一化矩阵之差的弗罗贝尼乌斯范数来量化两组轴之间的初始差异。为了使该度量相对于轴数 $k$ 具有尺度不变性，我们用目标矩阵 $\\tilde{L}_2$ 的弗罗贝尼乌斯范数对其进行归一化。原始差异 $d_{\\text{raw}}$ 定义为：\n$$\nd_{\\text{raw}} = \\frac{\\|\\tilde{L}_1 - \\tilde{L}_2\\|_F}{\\|\\tilde{L}_2\\|_F}\n$$\n由于 $\\tilde{L}_2$ 的 $k$ 个列向量均为单位范数，其弗罗贝尼乌斯范数的平方为 $\\|\\tilde{L}_2\\|_F^2 = \\sum_{j=1}^{k} \\|(\\tilde{L}_2)_{\\cdot j}\\|_2^2 = \\sum_{j=1}^{k} 1^2 = k$。因此，分母就是 $\\sqrt{k}$。\n\n**3. 正交普罗克勒斯忒斯对齐**\n\n由 $L_1$ 和 $L_2$ 的列所张成的子空间可能非常相似，但具体的基向量（即列向量）可能相互之间存在旋转。这种旋转模糊性是降维方法中的一个常见问题。我们通过找到一个最优旋转来解决它，该旋转能将 $\\tilde{L}_1$ 的轴最好地对齐到 $\\tilde{L}_2$ 的轴。这就是正交普罗克勒斯忒斯问题。我们寻求一个正交矩阵 $R \\in \\mathbb{R}^{k \\times k}$，它能最小化旋转后的源矩阵 $\\tilde{L}_1 R$ 与目标矩阵 $\\tilde{L}_2$ 之间差异的弗罗贝尼乌斯范数：\n$$\n\\min_{R: R^\\top R=I_k} \\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F\n$$\n这个问题的解可以通过奇异值分解 (SVD) 找到。设 $M = \\tilde{L}_2^\\top \\tilde{L}_1 \\in \\mathbb{R}^{k \\times k}$。我们计算 $M$ 的 SVD 为 $M = U \\Sigma V^\\top$，其中 $U$ 和 $V$ 是 $k \\times k$ 的正交矩阵。最优旋转矩阵 $R$ 由下式给出：\n$$\nR = V U^\\top\n$$\n有了这个最优旋转 $R$，我们将对齐后的源矩阵定义为 $\\tilde{L}_{1,a} = \\tilde{L}_1 R$。然后计算对齐后的差异 $d_{\\text{aligned}}$：\n$$\nd_{\\text{aligned}} = \\frac{\\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F}{\\|\\tilde{L}_2\\|_F} = \\frac{\\|\\tilde{L}_{1,a} - \\tilde{L}_2\\|_F}{\\sqrt{k}}\n$$\n\n**4. 对齐带来的改善**\n\n由普罗克勒斯忒斯对齐导致的差异减少量告诉我们，初始差异中有多少可归因于轴的简单旋转。改善量 $\\Delta$ 是原始差异与对齐后差异之差：\n$$\n\\Delta = d_{\\text{raw}} - d_{\\text{aligned}}\n$$\n较大的 $\\Delta$ 值表明轴之间确实存在旋转，并且对齐是有效的。接近于零的 $\\Delta$ 值表明轴要么已经很好地对齐，要么它们的几何结构存在根本性差异，无法通过旋转来校正。\n\n**5. 通过相关性进行稳定性分析**\n\n在全局对齐轴系统之后，我们通过测量对齐后的源矩阵 $\\tilde{L}_{1,a}$ 与目标矩阵 $\\tilde{L}_2$ 相应列之间的相似性来评估单个轴的稳定性。相似性由每对列之间的余弦相似度（或对于中心化数据是皮尔逊相关性）的绝对值来量化。对于第 $j$ 个轴，相关性为：\n$$\n\\rho_j = \\left| \\frac{(\\tilde{L}_{1,a})_{\\cdot j}^\\top (\\tilde{L}_2)_{\\cdot j}}{\\|(\\tilde{L}_{1,a})_{\\cdot j}\\|_2 \\|(\\tilde{L}_2)_{\\cdot j}\\|_2} \\right|\n$$\n请注意，虽然 $\\tilde{L}_2$ 的列根据构造具有单位范数，但对齐后的矩阵 $\\tilde{L}_{1,a} = \\tilde{L}_1 R$ 的列不保证具有单位范数，因此必须计算它们的范数。\n\n整体稳定性由所有 $k$ 个轴上这些绝对相关性的算术平均值来判断：\n$$\n\\bar{\\rho} = \\frac{1}{k}\\sum_{j=1}^k \\rho_j\n$$\n如果此平均相关性达到或超过预定义的阈值 $\\tau$，则该对负荷矩阵被声明为“稳定的”。对于此问题，$\\tau=0.98$。\n$$\n\\text{stable} = (\\bar{\\rho} \\ge \\tau)\n$$\n\n**应用于测试用例**\n该分析应用于以 $n=6$、$k=2$ 和给定的 $L_1$ 定义的四种情况。\n\n情况 A (恒等)：$L_2 = L_1$。矩阵完全相同。归一化后得到 $\\tilde{L}_1 = \\tilde{L}_2$。原始差异为 $0$。普罗克勒斯忒斯对齐产生 $R=I_k$，对齐后差异也为 $0$。因此 $\\Delta = 0$。列间相关性全为 $1$，所以平均相关性为 $1$，即 $\\ge 0.98$。轴是稳定的。\n\n情况 B (旋转)：$L_2 = L_1 Q$，其中 $Q$ 是一个旋转矩阵。$L_1$ 和 $L_2$ 的列空间是相同的。列式归一化过程不是线性的，所以 $\\tilde{L}_2$ 不是 $\\tilde{L}_1$ 的简单旋转。这将导致一个非零的 $d_{\\text{raw}}$。然而，底层的几何结构被保留了下来。普罗克勒斯忒斯分析预计会找到一个旋转 $R$，能将 $\\tilde{L}_1$ 与 $\\tilde{L}_2$ 紧密对齐，从而得到一个小的 $d_{\\text{aligned}}$ 和一个大的 $\\Delta$。最终的平均相关性应该很高，表明是稳定的。\n\n情况 C (置换与符号翻转)：$L_2 = L_1 P$，其中 $P$ 是一个表示置换和符号翻转的正交矩阵。这与情况 B 类似，因为 $P$ 也是一个正交变换。我们预计初始差异很大，但通过普罗克勒斯忒斯对齐会显著减小，从而产生一个大的 $\\Delta$ 和一个稳定的结论。\n\n情况 D (正交补)：$L_2$ 的列选自 $L_1$ 列空间的正交补空间的一个标准正交基。根据构造，$L_1$ 和 $L_2$ 的列空间是正交的。这种正交性在归一化步骤中很大程度上得以保留。因此，$\\tilde{L}_1$ 和 $\\tilde{L}_2$ 的列几乎是正交的。任何旋转都无法对齐这两个根本不同的子空间。普罗克勒斯忒斯对齐不会带来显著改善，所以 $\\Delta$ 将接近 $0$。对齐后轴之间的相关性将接近 $0$，远低于 $\\tau$，表明轴是不稳定的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the neural loading matrix stability analysis.\n    \"\"\"\n\n    def analyze_stability(L1, L2, tau):\n        \"\"\"\n        Performs the 5-step analysis for a given pair of loading matrices (L1, L2).\n        \n        Args:\n            L1 (np.ndarray): The first loading matrix (n x k).\n            L2 (np.ndarray): The second loading matrix (n x k).\n            tau (float): The stability threshold for mean absolute correlation.\n            \n        Returns:\n            list: A list containing [delta, stable], where delta is the improvement\n                  and stable is a boolean.\n        \"\"\"\n        n, k = L1.shape\n\n        # Step 1: Column-center and unit-normalize L1 and L2\n        def normalize_matrix(L):\n            mean = L.mean(axis=0)\n            centered = L - mean\n            norm = np.linalg.norm(centered, axis=0)\n            # Handle columns that might have zero norm after centering\n            tilde_L = np.divide(centered, norm, out=np.zeros_like(centered, dtype=float), where=norm!=0)\n            return tilde_L\n\n        L1_tilde = normalize_matrix(L1)\n        L2_tilde = normalize_matrix(L2)\n        \n        # Denominator for discrepancy calculations\n        # Since L2_tilde has unit-norm columns, its Frobenius norm is sqrt(k)\n        norm_L2_tilde_F = np.sqrt(k)\n\n        # Step 2: Compute the raw normalized Frobenius discrepancy\n        d_raw = np.linalg.norm(L1_tilde - L2_tilde, 'fro') / norm_L2_tilde_F\n        \n        # Step 3: Solve the orthogonal Procrustes alignment problem\n        # Find R that minimizes ||L1_tilde @ R - L2_tilde||_F\n        M = L2_tilde.T @ L1_tilde\n        U, s, Vh = linalg.svd(M)\n        R = Vh.T @ U.T\n        \n        L1_aligned = L1_tilde @ R\n        \n        # Compute the aligned discrepancy\n        d_aligned = np.linalg.norm(L1_aligned - L2_tilde, 'fro') / norm_L2_tilde_F\n\n        # Step 4: Define the improvement\n        delta = d_raw - d_aligned\n\n        # Step 5: Compute mean absolute columnwise correlation and determine stability\n        correlations = []\n        for j in range(k):\n            col1 = L1_aligned[:, j]\n            col2 = L2_tilde[:, j]\n            \n            norm1 = np.linalg.norm(col1)\n            norm2 = np.linalg.norm(col2) # This will be 1.0\n            \n            if norm1 == 0 or norm2 == 0:\n                corr = 0.0\n            else:\n                corr = np.dot(col1, col2) / (norm1 * norm2)\n            correlations.append(np.abs(corr))\n        \n        mean_abs_corr = np.mean(correlations)\n        stable = mean_abs_corr >= tau\n        \n        return [delta, stable]\n\n    # Test suite specification\n    n = 6\n    k = 2\n    tau = 0.98\n\n    L1 = np.array([\n        [0.5, -0.2],\n        [0.3, 0.1],\n        [0.0, 0.4],\n        [-0.1, 0.6],\n        [0.2, -0.3],\n        [0.7, 0.0]\n    ])\n\n    results = []\n\n    # Case A: Identity\n    L2_A = L1\n    results.append(analyze_stability(L1, L2_A, tau))\n\n    # Case B: Rotation in axis space\n    angle = np.pi / 6\n    Q = np.array([\n        [np.cos(angle), -np.sin(angle)],\n        [np.sin(angle), np.cos(angle)]\n    ])\n    L2_B = L1 @ Q\n    results.append(analyze_stability(L1, L2_B, tau))\n\n    # Case C: Permutation with sign flip\n    P = np.array([\n        [0, -1],\n        [1, 0]\n    ])\n    L2_C = L1 @ P\n    results.append(analyze_stability(L1, L2_C, tau))\n\n    # Case D: Orthogonal-complement axes\n    # Basis for the null space of L1.T, which is the orthogonal complement of colspan(L1)\n    null_space_basis = linalg.null_space(L1.T)\n    L2_D = null_space_basis[:, :k]\n    results.append(analyze_stability(L1, L2_D, tau))\n    \n    # Format the final output string\n    # Convert each sublist [delta, stable] to its string representation\n    # e.g., '[0.0, True]'\n    # Then join them with commas and wrap in brackets\n    # to get '[[0.0, True],[...]]'\n    final_output_str = f\"[{','.join(map(str, results))}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}