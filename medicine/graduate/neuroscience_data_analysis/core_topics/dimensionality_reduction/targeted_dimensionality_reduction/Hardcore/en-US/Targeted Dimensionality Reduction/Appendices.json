{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the power of Targeted Dimensionality Reduction (TDR), it is essential to contrast it with its unsupervised counterpart, Principal Component Analysis (PCA). While PCA identifies axes that capture the most variance in neural activity, TDR seeks axes that are most predictive of a specific task variable. This practice will guide you through implementing both methods to quantify the alignment between high-variance neural modes and task-relevant dimensions, building a concrete understanding of how TDR isolates signals that PCA might overlook .",
            "id": "4197341",
            "problem": "You are given a neural population activity matrix $X \\in \\mathbb{R}^{N \\times T}$ representing $N$ neurons observed over $T$ time points, and a task regressor matrix $R \\in \\mathbb{R}^{K \\times T}$ representing $K$ task-related variables over the same $T$ time points. Your objective is to quantify task alignment in the neural data using targeted dimensionality reduction by comparing principal axes from Principal Component Analysis (PCA) with targeted axes derived from linear prediction of task variables. The core steps must be derived from fundamental definitions as follows: mean-center $X$ along the time dimension, obtain principal axes via a mathematically rigorous decomposition, construct targeted axes by solving the least-squares linear prediction problem of each regressor from the neural activity, and compute the Pearson correlation between each principal axis and each targeted axis across neuron weights. High absolute correlation should be interpreted as alignment between dominant neural population modes and task-relevant directions.\n\nStarting point assumptions that you must use:\n- Mean-centering is performed along the time dimension so that each neuron's time series has zero mean.\n- Principal Component Analysis (PCA) is defined as the orthogonal directions in neuron space that maximize variance of the mean-centered data.\n- Targeted dimensionality reduction seeks neuron-space directions that linearly predict each task regressor from the neural activity.\n- Pearson correlation is the appropriate metric to quantify linear similarity across neuron weights between axes.\n\nFor each test case below, implement the procedure to:\n- Mean-center $X$ and each regressor $r_k$ across time.\n- Compute the leading $M$ principal axes in neuron space.\n- Compute $K$ targeted axes in neuron space by solving the linear least-squares prediction of each regressor from the neural activity.\n- Compute the Pearson correlation coefficient between each principal axis and each targeted axis as a function over neuron indices, and then take the maximum absolute correlation across all pairs.\n- Decide task alignment as a boolean where alignment is true if and only if the maximum absolute correlation is greater than or equal to a specified threshold $\\tau$.\n\nAngles are not involved, and no physical units are involved. There are no percentages in the required output. Your implementation must produce results that are deterministic and reproducible from the specified numerical inputs.\n\nTest suite specification. For each case, the shape of $X$ is $N \\times T$, the shape of $R$ is $K \\times T$, the number of principal axes is $M$, and the alignment threshold is $\\tau$.\n\n- Case $1$ (happy path, strong alignment):\n  - $N = 3$, $T = 4$, $K = 1$, $M = 1$, $\\tau = 0.9$.\n  - $X$ rows: row $1$ is $1,2,3,4$; row $2$ is $0,0,0,0$; row $3$ is $0,0,0,0$.\n  - $R$ rows: row $1$ is $1,2,3,4$.\n\n- Case $2$ (orthogonality/low alignment):\n  - $N = 3$, $T = 4$, $K = 1$, $M = 1$, $\\tau = 0.8$.\n  - $X$ rows: row $1$ is $1,2,-1,0$; row $2$ is $2,-1,2,-1$; row $3$ is $0,0,0,0$.\n  - $R$ rows: row $1$ is $1,2,-1,0$.\n\n- Case $3$ (sign inversion but aligned in absolute value):\n  - $N = 3$, $T = 4$, $K = 1$, $M = 1$, $\\tau = 0.9$.\n  - $X$ rows: row $1$ is $-1,-2,-3,-4$; row $2$ is $0,0,0,0$; row $3$ is $0,0,0,0$.\n  - $R$ rows: row $1$ is $1,2,3,4$.\n\n- Case $4$ (degenerate regressor with zero variance):\n  - $N = 3$, $T = 4$, $K = 1$, $M = 1$, $\\tau = 0.5$.\n  - $X$ rows: row $1$ is $1,0,1,0$; row $2$ is $0,1,0,1$; row $3$ is $1,1,1,1$.\n  - $R$ rows: row $1$ is $0,0,0,0$.\n\n- Case $5$ (multi-axis alignment, two task dimensions):\n  - $N = 4$, $T = 5$, $K = 2$, $M = 2$, $\\tau = 0.9$.\n  - Define $r_1$ as $1,0,1,0,1$, and $r_2$ as $0,1,0,1,0$.\n  - Define $b_1$ as $1,0,1,0$, and $b_2$ as $0,1,0,1$.\n  - Construct $X$ as the sum of the outer products $b_1 r_1^\\top$ and $b_2 r_2^\\top$, i.e., the element at neuron $i$ and time $t$ equals $b_{1,i} r_{1,t} + b_{2,i} r_{2,t}$.\n  - $R$ rows: row $1$ is $r_1$; row $2$ is $r_2$.\n\nImplementation requirements for each case:\n- Use mean-centering along time for $X$ and for each regressor in $R$.\n- Use a decomposition that yields orthonormal principal axes in neuron space.\n- Construct targeted axes by solving the least-squares normal equations in neuron space.\n- Compute Pearson correlation across neuron indices between axes. If either axis has zero standard deviation across neuron entries, define the correlation as $0.0$ for that pair.\n- Interpret high absolute correlation as alignment relative to $\\tau$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$), where each $result_i$ is a boolean indicating task alignment for case $i$ using the specified threshold $\\tau$.",
            "solution": "The problem is valid. It presents a well-defined, scientifically grounded procedure for quantifying task alignment in neural data using standard methods from linear algebra and statistics. I will now provide a complete solution.\n\nThe objective is to determine if the dominant modes of neural population activity are aligned with directions in neuron space that are predictive of specific task variables. This is achieved by comparing principal axes from Principal Component Analysis (PCA) with targeted axes derived from linear regression.\n\nLet the neural activity matrix be $X \\in \\mathbb{R}^{N \\times T}$ and the task regressor matrix be $R \\in \\mathbb{R}^{K \\times T}$. The procedure involves the following steps:\n\n1.  **Data Mean-Centering:**\n    The first step is to remove the mean activity level of each neuron and the mean value of each task regressor over time. This ensures that the subsequent analysis focuses on fluctuations around the mean.\n    For the neural data matrix $X$, the mean of neuron $i$ across $T$ time points is $\\mu_{X_i} = \\frac{1}{T} \\sum_{t=1}^{T} X_{it}$. The centered matrix, denoted as $X_c$, has entries $(X_c)_{it} = X_{it} - \\mu_{X_i}$.\n    Similarly, for each task regressor $r_k$ (the $k$-th row of $R$), its mean across time is $\\mu_{r_k} = \\frac{1}{T} \\sum_{t=1}^{T} R_{kt}$. The centered regressor, $(r_k)_c$, has entries $((r_k)_c)_t = R_{kt} - \\mu_{r_k}$. Let $R_c$ be the matrix composed of these centered row vectors.\n\n2.  **Principal Component Analysis (PCA) for Principal Axes:**\n    PCA is used to find a set of orthogonal axes in the $N$-dimensional neuron space that capture the directions of maximum variance in the neural data $X_c$. These axes are called principal axes.\n    The variance of the data projected onto a unit vector $w \\in \\mathbb{R}^N$ is maximized by the eigenvectors of the spatial covariance matrix $C_X = \\frac{1}{T-1} X_c X_c^\\top$.\n    A computationally robust method to find these axes is via the Singular Value Decomposition (SVD) of the centered data matrix:\n    $$X_c = U \\Sigma V^\\top$$\n    Here, $U \\in \\mathbb{R}^{N \\times N}$ is a matrix whose columns are the left singular vectors, $\\Sigma \\in \\mathbb{R}^{N \\times T}$ is a rectangular diagonal matrix of singular values, and $V \\in \\mathbb{R}^{T \\times T}$ is a matrix whose columns are the right singular vectors. The columns of $U$ are the eigenvectors of $X_c X_c^\\top$ and represent the principal axes. The singular values in $\\Sigma$ are related to the standard deviation along these axes.\n    The leading $M$ principal axes, $\\{u_1, u_2, \\dots, u_M\\}$, are the first $M$ columns of the matrix $U$, corresponding to the $M$ largest singular values.\n\n3.  **Linear Regression for Targeted Axes:**\n    Targeted axes are directions in neuron space that are most predictive of the task regressors. For each centered task regressor $(r_k)_c \\in \\mathbb{R}^{1 \\times T}$, we seek a weight vector $w_k \\in \\mathbb{R}^N$ (the targeted axis) such that the linear combination of neural activities, $w_k^\\top X_c$, provides the best possible prediction of $(r_k)_c$.\n    This is formulated as a linear least-squares problem. We aim to find the $w_k$ that minimizes the sum of squared errors between the actual regressor values and the predicted values:\n    $$ \\min_{w_k} \\| (r_k)_c^\\top - X_c^\\top w_k \\|^2_2 $$\n    Here, we model the regressor time series, $(r_k)_c^\\top \\in \\mathbb{R}^{T \\times 1}$, as a linear combination of the columns of $X_c^\\top \\in \\mathbb{R}^{T \\times N}$, with the coefficients being the neuron weights in $w_k \\in \\mathbb{R}^{N \\times 1}$.\n    The solution is given by solving the normal equations. A numerically stable approach is to use the pseudoinverse, which is equivalent to using a standard least-squares solver. The solution for each targeted axis $w_k$ is:\n    $$ w_k = (X_c^\\top)^\\dagger (r_k)_c^\\top $$\n    where $(X_c^\\top)^\\dagger$ is the Moore-Penrose pseudoinverse of $X_c^\\top$. This yields $K$ targeted axes, $\\{w_1, w_2, \\dots, w_K\\}$.\n\n4.  **Quantifying Alignment with Pearson Correlation:**\n    The alignment between a principal axis $u_m$ and a targeted axis $w_k$ is quantified by the Pearson correlation coefficient calculated over their $N$ neuron weight entries. For two vectors $a, b \\in \\mathbb{R}^N$, the correlation is:\n    $$ \\rho(a, b) = \\frac{\\sum_{i=1}^N (a_i - \\bar{a})(b_i - \\bar{b})}{\\sqrt{\\sum_{i=1}^N (a_i - \\bar{a})^2} \\sqrt{\\sum_{i=1}^N (b_i - \\bar{b})^2}} $$\n    where $\\bar{a}$ and $\\bar{b}$ are the means of the elements in vectors $a$ and $b$, respectively. A high absolute correlation, $|\\rho|$, indicates that the two axes represent similar directions in neuron space (up to a sign flip). Per the problem statement, if either vector has a standard deviation of zero, the correlation is defined as $0.0$.\n\n5.  **Final Alignment Decision:**\n    We compute the absolute Pearson correlation for all $M \\times K$ pairs of principal and targeted axes. The maximum of these values, $\\rho_{\\text{max}}$, represents the strongest alignment found between any dominant neural mode and any task-predictive direction.\n    $$ \\rho_{\\text{max}} = \\max_{m \\in \\{1,\\dots,M\\}, k \\in \\{1,\\dots,K\\}} |\\rho(u_m, w_k)| $$\n    The final decision of task alignment is a boolean value, determined by comparing $\\rho_{\\text{max}}$ to a given threshold $\\tau$:\n    $$ \\text{Alignment} = (\\rho_{\\text{max}} \\ge \\tau) $$\n\nThis procedure provides a rigorous and quantitative method for assessing the relationship between intrinsic neural dynamics and external task demands.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the task alignment problem for a suite of test cases.\n    For each case, it computes principal axes from neural data (X) and\n    targeted axes from task regressors (R), then determines if there is\n    significant alignment based on a correlation threshold.\n    \"\"\"\n    # Test suite specification.\n    test_cases = [\n        # Case 1 (happy path, strong alignment)\n        {\n            \"X\": np.array([[1., 2., 3., 4.], [0., 0., 0., 0.], [0., 0., 0., 0.]]),\n            \"R\": np.array([[1., 2., 3., 4.]]),\n            \"M\": 1,\n            \"tau\": 0.9,\n        },\n        # Case 2 (orthogonality/low alignment)\n        {\n            \"X\": np.array([[1., 2., -1., 0.], [2., -1., 2., -1.], [0., 0., 0., 0.]]),\n            \"R\": np.array([[1., 2., -1., 0.]]),\n            \"M\": 1,\n            \"tau\": 0.8,\n        },\n        # Case 3 (sign inversion but aligned in absolute value)\n        {\n            \"X\": np.array([[-1., -2., -3., -4.], [0., 0., 0., 0.], [0., 0., 0., 0.]]),\n            \"R\": np.array([[1., 2., 3., 4.]]),\n            \"M\": 1,\n            \"tau\": 0.9,\n        },\n        # Case 4 (degenerate regressor with zero variance)\n        {\n            \"X\": np.array([[1., 0., 1., 0.], [0., 1., 0., 1.], [1., 1., 1., 1.]]),\n            \"R\": np.array([[0., 0., 0., 0.]]),\n            \"M\": 1,\n            \"tau\": 0.5,\n        },\n        # Case 5 (multi-axis alignment, two task dimensions)\n        {\n            \"X\": np.array([\n                [1., 0., 1., 0., 1.],\n                [0., 1., 0., 1., 0.],\n                [1., 0., 1., 0., 1.],\n                [0., 1., 0., 1., 0.]\n            ]),\n            \"R\": np.array([\n                [1., 0., 1., 0., 1.],\n                [0., 1., 0., 1., 0.]\n            ]),\n            \"M\": 2,\n            \"tau\": 0.9,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X, R, M, tau = case[\"X\"], case[\"R\"], case[\"M\"], case[\"tau\"]\n        N, T = X.shape\n        K = R.shape[0]\n\n        # Step 1: Mean-center X and R along the time dimension (axis=1)\n        X_c = X - X.mean(axis=1, keepdims=True)\n        R_c = R - R.mean(axis=1, keepdims=True)\n\n        # Step 2: Compute Principal Axes using SVD\n        # U contains the principal axes as columns\n        # We use full_matrices=False for efficiency, as we only need the N x N U matrix\n        if X_c.shape[1]  X_c.shape[0]: # T  N case\n             U, s, Vt = np.linalg.svd(X_c, full_matrices=True)\n        else:\n             U, s, Vt = np.linalg.svd(X_c, full_matrices=False)\n        principal_axes = U[:, :M]\n\n        # Step 3: Compute Targeted Axes via Linear Least-Squares\n        targeted_axes = []\n        for k in range(K):\n            r_k_c = R_c[k, :]\n            # We solve X_c.T @ w_k = r_k_c.T for w_k\n            # np.linalg.lstsq solves a @ x = b. Here a=X_c.T, b=r_k_c.T, x=w_k\n            w_k, residuals, rank, s_vals = np.linalg.lstsq(X_c.T, r_k_c.T, rcond=None)\n            targeted_axes.append(w_k)\n\n        # Step 4: Compute max absolute Pearson correlation\n        max_abs_corr = 0.0\n        for m in range(M):\n            pa = principal_axes[:, m]\n            for k in range(K):\n                ta = targeted_axes[k]\n\n                # Handle the case where an axis has zero standard deviation\n                pa_std = np.std(pa)\n                ta_std = np.std(ta)\n                \n                if pa_std == 0.0 or ta_std == 0.0:\n                    corr = 0.0\n                else:\n                    # np.corrcoef returns a 2x2 matrix\n                    corr_matrix = np.corrcoef(pa, ta)\n                    corr = corr_matrix[0, 1]\n                \n                max_abs_corr = max(max_abs_corr, abs(corr))\n\n        # Step 5: Decide alignment\n        alignment = max_abs_corr = tau\n        results.append(str(alignment))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from implementation to theoretical insight, we explore how analytical choices shape the results of TDR. The way we model neural noise—whether we assume it is independent across neurons or correlated—fundamentally alters the 'optimal' axis for separating experimental conditions. In this exercise, you will derive the discriminant axes from first principles under different noise models, providing a rigorous understanding of the assumptions that underpin common preprocessing pipelines in TDR .",
            "id": "4197375",
            "problem": "You are analyzing trial-averaged firing rates from a population of $n=3$ neurons recorded during a baseline epoch and a stimulus epoch. The data are modeled as follows: baseline activity is multivariate Gaussian with mean $\\boldsymbol{\\mu}_{b}$ and covariance $\\boldsymbol{\\Sigma}_{b}$; stimulus-epoch activity is multivariate Gaussian with mean $\\boldsymbol{\\mu}_{s}$ and the same within-class covariance as baseline. You will estimate a one-dimensional task axis using Targeted Dimensionality Reduction (TDR) by constructing a linear projection that maximizes separation of stimulus from baseline under the shared Gaussian-noise assumption. Consider the following two preprocessing pipelines:\n\n- Pipeline A (baseline subtraction and full noise whitening): You subtract baseline to form a condition difference $\\Delta \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_{s} - \\boldsymbol{\\mu}_{b}$ and estimate the linear discriminant axis that maximizes the ratio of between-class variance to within-class variance using the full baseline covariance $\\boldsymbol{\\Sigma}_{b}$.\n\n- Pipeline B (baseline subtraction and per-neuron $z$-scoring): You subtract baseline as above and then $z$-score each neuron using baseline variability only, i.e., scale by $\\mathbf{D}^{-1}$ where $\\mathbf{D} = \\operatorname{diag}(\\sigma_{b,1}, \\sigma_{b,2}, \\sigma_{b,3})$ with $\\sigma_{b,i} = \\sqrt{(\\boldsymbol{\\Sigma}_{b})_{ii}}$. You then estimate the linear discriminant axis in the $z$-scored coordinates under the same shared-noise assumption but using the diagonal-noise model induced by $\\mathbf{D}$.\n\nStarting from first principles of linear discriminant analysis (derive the optimal axis by maximizing separation defined by the between-class to within-class variance ratio under the shared covariance model), derive the analytical expressions for the axis directions produced by Pipeline A and Pipeline B in the original neural coordinate system. Then, compute the principal angle between these two axis directions,\n$$\n\\theta = \\arccos\\left(\\frac{\\boldsymbol{w}_{A}^{\\top}\\boldsymbol{w}_{B}}{\\|\\boldsymbol{w}_{A}\\|\\,\\|\\boldsymbol{w}_{B}\\|}\\right),\n$$\nexpressed in radians, for the following parameters:\n$$\n\\boldsymbol{\\mu}_{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad\n\\boldsymbol{\\mu}_{s} = \\begin{pmatrix} 2 \\\\ 5 \\\\ 0 \\end{pmatrix}, \\quad\n\\boldsymbol{\\Sigma}_{b} = \\begin{pmatrix} 2  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix}.\n$$\nUse the baseline standard deviations for $z$-scoring, i.e., $\\sigma_{b,1} = \\sqrt{2}$, $\\sigma_{b,2} = \\sqrt{3}$, and $\\sigma_{b,3} = \\sqrt{2}$, so that $\\mathbf{D} = \\operatorname{diag}(\\sqrt{2}, \\sqrt{3}, \\sqrt{2})$. Provide the final answer as a single closed-form analytic expression for $\\theta$ in radians. No rounding is required. In your derivation, explicitly articulate how baseline subtraction and $z$-scoring affect axis estimation and justify these preprocessing choices based on the shared-covariance, Gaussian-noise model.",
            "solution": "The problem requires the derivation and comparison of two linear discriminant axes, $\\boldsymbol{w}_{A}$ and $\\boldsymbol{w}_{B}$, obtained through two different preprocessing pipelines for analyzing neural data. The core method is Linear Discriminant Analysis (LDA), which seeks a projection axis $\\boldsymbol{w}$ that maximizes the ratio of between-class variance to within-class variance.\n\nLet the two classes (baseline and stimulus) be modeled by multivariate Gaussian distributions with means $\\boldsymbol{\\mu}_{b}$ and $\\boldsymbol{\\mu}_{s}$, respectively, and a shared within-class covariance matrix $\\boldsymbol{\\Sigma}$. The difference between the class means is $\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_{s} - \\boldsymbol{\\mu}_{b}$.\n\nWhen data points $\\boldsymbol{x}$ are projected onto an axis defined by the vector $\\boldsymbol{w}$, the projected coordinate is $y = \\boldsymbol{w}^{\\top}\\boldsymbol{x}$. The means of the projected distributions are $\\boldsymbol{w}^{\\top}\\boldsymbol{\\mu}_{b}$ and $\\boldsymbol{w}^{\\top}\\boldsymbol{\\mu}_{s}$. The variance of the projected between-class means is:\n$$\nV_{\\text{between}} = (\\boldsymbol{w}^{\\top}\\boldsymbol{\\mu}_{s} - \\boldsymbol{w}^{\\top}\\boldsymbol{\\mu}_{b})^{2} = (\\boldsymbol{w}^{\\top}\\Delta\\boldsymbol{\\mu})^{2} = \\boldsymbol{w}^{\\top}(\\Delta\\boldsymbol{\\mu}\\Delta\\boldsymbol{\\mu}^{\\top})\\boldsymbol{w}\n$$\nThe projected within-class variance is given by:\n$$\nV_{\\text{within}} = \\boldsymbol{w}^{\\top}\\boldsymbol{\\Sigma}\\boldsymbol{w}\n$$\nThe objective is to maximize the Rayleigh quotient $J(\\boldsymbol{w})$:\n$$\nJ(\\boldsymbol{w}) = \\frac{V_{\\text{between}}}{V_{\\text{within}}} = \\frac{\\boldsymbol{w}^{\\top}(\\Delta\\boldsymbol{\\mu}\\Delta\\boldsymbol{\\mu}^{\\top})\\boldsymbol{w}}{\\boldsymbol{w}^{\\top}\\boldsymbol{\\Sigma}\\boldsymbol{w}}\n$$\nThe solution to this maximization problem is given by the generalized eigenvalue problem $(\\Delta\\boldsymbol{\\mu}\\Delta\\boldsymbol{\\mu}^{\\top})\\boldsymbol{w} = \\lambda\\boldsymbol{\\Sigma}\\boldsymbol{w}$. Since $\\Delta\\boldsymbol{\\mu}\\Delta\\boldsymbol{\\mu}^{\\top}$ is a rank-one matrix, the vector $(\\Delta\\boldsymbol{\\mu}\\Delta\\boldsymbol{\\mu}^{\\top})\\boldsymbol{w}$ is always proportional to $\\Delta\\boldsymbol{\\mu}$. This leads to the well-known solution where the optimal axis direction $\\boldsymbol{w}_{\\text{opt}}$ is proportional to the difference of the means transformed by the inverse of the covariance matrix:\n$$\n\\boldsymbol{w}_{\\text{opt}} \\propto \\boldsymbol{\\Sigma}^{-1}\\Delta\\boldsymbol{\\mu}\n$$\nThis fundamental result forms the basis for deriving the axes for both pipelines.\n\nFirst, we compute the condition difference vector $\\Delta\\boldsymbol{\\mu}$ from the given parameters:\n$$\n\\boldsymbol{\\mu}_{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad \\boldsymbol{\\mu}_{s} = \\begin{pmatrix} 2 \\\\ 5 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_{s} - \\boldsymbol{\\mu}_{b} = \\begin{pmatrix} 2 - 1 \\\\ 5 - 2 \\\\ 0 - (-1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 1 \\end{pmatrix}\n$$\n\n**Pipeline A: Baseline subtraction and full noise whitening**\n\nThis pipeline uses the full baseline covariance matrix $\\boldsymbol{\\Sigma}_{b}$ as the shared covariance $\\boldsymbol{\\Sigma}$. The discriminant axis $\\boldsymbol{w}_{A}$ is therefore:\n$$\n\\boldsymbol{w}_{A} \\propto \\boldsymbol{\\Sigma}_{b}^{-1}\\Delta\\boldsymbol{\\mu}\n$$\nWe are given the covariance matrix:\n$$\n\\boldsymbol{\\Sigma}_{b} = \\begin{pmatrix} 2  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix}\n$$\nTo find its inverse, we first compute the determinant: $\\det(\\boldsymbol{\\Sigma}_{b}) = 2(3 \\cdot 2 - 1 \\cdot 1) - 1(1 \\cdot 2 - 1 \\cdot 0) + 0 = 2(5) - 2 = 8$.\nThe inverse matrix is $\\boldsymbol{\\Sigma}_{b}^{-1} = \\frac{1}{\\det(\\boldsymbol{\\Sigma}_{b})} \\text{adj}(\\boldsymbol{\\Sigma}_{b})$:\n$$\n\\boldsymbol{\\Sigma}_{b}^{-1} = \\frac{1}{8}\\begin{pmatrix} 3 \\cdot 2 - 1 \\cdot 1  -(1 \\cdot 2 - 1 \\cdot 0)  1 \\cdot 1 - 3 \\cdot 0 \\\\ -(1 \\cdot 2 - 0 \\cdot 1)  2 \\cdot 2 - 0 \\cdot 0  -(2 \\cdot 1 - 0 \\cdot 1) \\\\ 1 \\cdot 1 - 3 \\cdot 0  -(2 \\cdot 1 - 1 \\cdot 0)  2 \\cdot 3 - 1 \\cdot 1 \\end{pmatrix}^{\\top} = \\frac{1}{8}\\begin{pmatrix} 5  -2  1 \\\\ -2  4  -2 \\\\ 1  -2  5 \\end{pmatrix}\n$$\nNow, we compute the direction of $\\boldsymbol{w}_{A}$:\n$$\n\\boldsymbol{w}_{A} \\propto \\frac{1}{8}\\begin{pmatrix} 5  -2  1 \\\\ -2  4  -2 \\\\ 1  -2  5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{8}\\begin{pmatrix} 5 - 6 + 1 \\\\ -2 + 12 - 2 \\\\ 1 - 6 + 5 \\end{pmatrix} = \\frac{1}{8}\\begin{pmatrix} 0 \\\\ 8 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThus, we can choose the representative vector for the axis from Pipeline A to be $\\boldsymbol{w}_{A} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n\n**Pipeline B: Baseline subtraction and per-neuron $z$-scoring**\n\nThis pipeline involves transforming the data before applying LDA. The data is $z$-scored using the baseline standard deviations, which corresponds to a linear transformation by the matrix $\\mathbf{D}^{-1}$, where $\\mathbf{D} = \\operatorname{diag}(\\sigma_{b,1}, \\sigma_{b,2}, \\sigma_{b,3})$.\n$$\n\\mathbf{D} = \\operatorname{diag}(\\sqrt{(\\boldsymbol{\\Sigma}_{b})_{11}}, \\sqrt{(\\boldsymbol{\\Sigma}_{b})_{22}}, \\sqrt{(\\boldsymbol{\\Sigma}_{b})_{33}}) = \\operatorname{diag}(\\sqrt{2}, \\sqrt{3}, \\sqrt{2})\n$$\nIn this z-scored space, the mean difference vector is $\\Delta\\boldsymbol{\\mu}_{z} = \\mathbf{D}^{-1}\\Delta\\boldsymbol{\\mu}$. The pipeline specifies using a \"diagonal-noise model\", which in this normalized space corresponds to assuming the covariance matrix is the identity matrix, $\\boldsymbol{\\Sigma}_{\\text{assumed}} = \\mathbf{I}$. The discriminant axis in the z-scored space, $\\boldsymbol{w}_{B,z}$, is then:\n$$\n\\boldsymbol{w}_{B,z} \\propto \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\Delta\\boldsymbol{\\mu}_{z} = \\mathbf{I}^{-1}(\\mathbf{D}^{-1}\\Delta\\boldsymbol{\\mu}) = \\mathbf{D}^{-1}\\Delta\\boldsymbol{\\mu}\n$$\nTo find the axis $\\boldsymbol{w}_{B}$ in the original coordinate system, we must map $\\boldsymbol{w}_{B,z}$ back. A projection in the z-scored space is $y = \\boldsymbol{w}_{B,z}^{\\top}\\boldsymbol{z}$. Substituting $\\boldsymbol{z} = \\mathbf{D}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_{b})$ gives $y = \\boldsymbol{w}_{B,z}^{\\top}\\mathbf{D}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_{b})$. The projection vector in the original space is therefore $\\boldsymbol{w}_{B} = (\\mathbf{D}^{-1})^{\\top}\\boldsymbol{w}_{B,z} = \\mathbf{D}^{-1}\\boldsymbol{w}_{B,z}$. Substituting the expression for $\\boldsymbol{w}_{B,z}$:\n$$\n\\boldsymbol{w}_{B} \\propto \\mathbf{D}^{-1}(\\mathbf{D}^{-1}\\Delta\\boldsymbol{\\mu}) = \\mathbf{D}^{-2}\\Delta\\boldsymbol{\\mu}\n$$\nThis is equivalent to performing LDA in the original space but assuming a diagonal covariance matrix $\\boldsymbol{\\Sigma} = \\mathbf{D}^{2} = \\operatorname{diag}((\\boldsymbol{\\Sigma}_{b})_{11}, (\\boldsymbol{\\Sigma}_{b})_{22}, (\\boldsymbol{\\Sigma}_{b})_{33})$.\nWe compute $\\mathbf{D}^{-2} = \\operatorname{diag}(1/2, 1/3, 1/2)$.\n$$\n\\boldsymbol{w}_{B} \\propto \\begin{pmatrix} 1/2  0  0 \\\\ 0  1/3  0 \\\\ 0  0  1/2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1 \\\\ 1/2 \\end{pmatrix}\n$$\nFor simplicity, we can choose the representative vector $\\boldsymbol{w}_{B} = 2 \\times \\begin{pmatrix} 1/2 \\\\ 1 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$.\n\nThe choice of preprocessing pipeline reflects different assumptions about the noise structure. Baseline subtraction is a standard step to center the problem on separating the signal component $\\Delta\\boldsymbol{\\mu}$ from the noise. Pipeline A is optimal under the assumption that the full covariance matrix $\\boldsymbol{\\Sigma}_{b}$ is known and accurately reflects the noise structure. It performs an optimal \"whitening\" of the noise. Pipeline B simplifies this by assuming noise is independent across neurons (after scaling by their individual variances). This is a common practical choice when the full covariance is difficult to estimate reliably or when one wants a more robust (though potentially suboptimal) decoder that relies only on per-neuron statistics.\n\n**Angle Calculation**\n\nFinally, we compute the principal angle $\\theta$ between the two axes.\n$$\n\\theta = \\arccos\\left(\\frac{\\boldsymbol{w}_{A}^{\\top}\\boldsymbol{w}_{B}}{\\|\\boldsymbol{w}_{A}\\|\\,\\|\\boldsymbol{w}_{B}\\|}\\right)\n$$\nUsing our derived vectors $\\boldsymbol{w}_{A} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $\\boldsymbol{w}_{B} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$:\nThe dot product is:\n$$\n\\boldsymbol{w}_{A}^{\\top}\\boldsymbol{w}_{B} = (0)(1) + (1)(2) + (0)(1) = 2\n$$\nThe norms of the vectors are:\n$$\n\\|\\boldsymbol{w}_{A}\\| = \\sqrt{0^{2} + 1^{2} + 0^{2}} = 1\n$$\n$$\n\\|\\boldsymbol{w}_{B}\\| = \\sqrt{1^{2} + 2^{2} + 1^{2}} = \\sqrt{1 + 4 + 1} = \\sqrt{6}\n$$\nSubstituting these values into the formula for $\\theta$:\n$$\n\\theta = \\arccos\\left(\\frac{2}{1 \\cdot \\sqrt{6}}\\right) = \\arccos\\left(\\frac{2}{\\sqrt{6}}\\right)\n$$\nThe argument can be simplified:\n$$\n\\frac{2}{\\sqrt{6}} = \\frac{\\sqrt{4}}{\\sqrt{6}} = \\sqrt{\\frac{4}{6}} = \\sqrt{\\frac{2}{3}}\n$$\nTherefore, the angle is:\n$$\n\\theta = \\arccos\\left(\\sqrt{\\frac{2}{3}}\\right)\n$$",
            "answer": "$$\\boxed{\\arccos\\left(\\sqrt{\\frac{2}{3}}\\right)}$$"
        },
        {
            "introduction": "A crucial test for any scientific finding is its reproducibility, and for neural representations, this means assessing their stability across different recording sessions. However, comparing axes learned from separate datasets is non-trivial due to inherent rotational ambiguities. This advanced practice introduces the Orthogonal Procrustes problem as a principled method to align neural subspaces, allowing you to quantitatively measure their stability and address one of the most significant practical challenges in longitudinal neural data analysis .",
            "id": "4197417",
            "problem": "You are given two sessions of multivariate neural loading matrices representing axes produced by targeted dimensionality reduction. Each session is modeled as a real-valued matrix with columns that span a low-dimensional axis subspace. Let the first session be a loading matrix $L_1 \\in \\mathbb{R}^{n \\times k}$ and the second session be $L_2 \\in \\mathbb{R}^{n \\times k}$, where $n$ is the number of neurons and $k$ is the number of axes. The goal is to analyze the stability of axes across sessions and to remove rotational ambiguity by orthogonal Procrustes alignment, then quantify the improvement in direct matrix discrepancy and determine whether the aligned axes are stable according to a correlation criterion.\n\nStart from the following base definitions and facts in linear algebra:\n- A matrix $R \\in \\mathbb{R}^{k \\times k}$ is orthogonal if and only if $R^\\top R = I_k$, where $I_k$ is the $k \\times k$ identity matrix.\n- The Frobenius norm of a matrix $A \\in \\mathbb{R}^{p \\times q}$ is defined by $\\|A\\|_F = \\sqrt{\\sum_{i=1}^{p} \\sum_{j=1}^{q} a_{ij}^2}$.\n- The Singular Value Decomposition (SVD) factorizes any matrix $M \\in \\mathbb{R}^{p \\times q}$ as $M = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{p \\times p}$ and $V \\in \\mathbb{R}^{q \\times q}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{p \\times q}$ is diagonal with nonnegative entries.\n- Column centering by neurons means subtracting, for each column $j$, the mean across its $n$ entries. Column unit normalization means dividing each centered column by its Euclidean norm so that each column has unit norm.\n\nImplement the following analysis for each pair $(L_1, L_2)$:\n1. Column-center $L_1$ and $L_2$ across neurons and unit-normalize each column, obtaining $\\tilde{L}_1$ and $\\tilde{L}_2$.\n2. Compute the raw normalized Frobenius discrepancy $d_{\\text{raw}} = \\|\\tilde{L}_1 - \\tilde{L}_2\\|_F / \\|\\tilde{L}_2\\|_F$.\n3. Solve the orthogonal Procrustes alignment problem: find an orthogonal $R \\in \\mathbb{R}^{k \\times k}$ that minimizes $\\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F$. Use this alignment to compute $d_{\\text{aligned}} = \\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F / \\|\\tilde{L}_2\\|_F$.\n4. Define the improvement as $\\Delta = d_{\\text{raw}} - d_{\\text{aligned}}$.\n5. Compute the mean absolute columnwise correlation between the aligned axes and the target axes, defined as the arithmetic mean across $j=1,\\dots,k$ of the absolute value of the cosine similarity between the $j$-th column of $\\tilde{L}_1 R$ and the $j$-th column of $\\tilde{L}_2$, i.e., $\\frac{1}{k} \\sum_{j=1}^{k} \\left| \\frac{(\\tilde{L}_1 R)_{\\cdot j}^\\top \\tilde{L}_2_{\\cdot j}}{\\|(\\tilde{L}_1 R)_{\\cdot j}\\|_2 \\, \\|\\tilde{L}_2_{\\cdot j}\\|_2} \\right|$. Determine stability by thresholding this mean at $\\tau$, declaring the pair stable if and only if the mean absolute correlation is greater than or equal to $\\tau$.\n\nUse angle values in radians wherever angles are specified. No physical units appear in this problem. Express all boolean decisions as exact boolean values and all numerical results as real-valued floats.\n\nTest suite specification:\n- Let $n = 6$ and $k = 2$.\n- Define\n$$\nL_1 = \\begin{bmatrix}\n0.5  -0.2 \\\\\n0.3  0.1 \\\\\n0.0  0.4 \\\\\n-0.1  0.6 \\\\\n0.2  -0.3 \\\\\n0.7  0.0\n\\end{bmatrix}.\n$$\nConstruct four pairs $(L_1, L_2)$:\n1. Case A (identity): $L_2 = L_1$.\n2. Case B (rotation in axis space): $L_2 = L_1 Q$, where\n$$\nQ = \\begin{bmatrix}\n\\cos(\\pi/6)  -\\sin(\\pi/6) \\\\\n\\sin(\\pi/6)  \\cos(\\pi/6)\n\\end{bmatrix},\n$$\nwith angle specified in radians.\n3. Case C (permutation with sign flip): $L_2 = L_1 P$, where\n$$\nP = \\begin{bmatrix}\n0  -1 \\\\\n1  \\phantom{-}0\n\\end{bmatrix}.\n$$\n4. Case D (orthogonal-complement axes): Let $N$ be an orthonormal basis for the orthogonal complement of the column space of $L_1$ in $\\mathbb{R}^n$, equivalently the null space of $L_1^\\top$. Let $N = [n_1 \\, n_2 \\, n_3 \\, n_4] \\in \\mathbb{R}^{n \\times (n-k)}$ with columns ordered arbitrarily but deterministically. Set $L_2 = [n_1 \\, n_2] \\in \\mathbb{R}^{n \\times 2}$.\n\nStability threshold:\n- Use $\\tau = 0.98$.\n\nYour program should compute, for each case, the ordered pair $\\left[\\Delta, \\text{stable}\\right]$, where $\\Delta$ is the improvement as a float and $\\text{stable}$ is a boolean indicating whether the mean absolute correlation after alignment exceeds or equals $\\tau$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list of the form $\\left[\\Delta,\\text{stable}\\right]$. For example, the printed line should look like $[[\\Delta_1,\\text{stable}_1],[\\Delta_2,\\text{stable}_2],[\\Delta_3,\\text{stable}_3],[\\Delta_4,\\text{stable}_4]]$.",
            "solution": "The problem requires a comparative analysis of two neural loading matrices, $L_1, L_2 \\in \\mathbb{R}^{n \\times k}$, which represent axes from a targeted dimensionality reduction analysis across two experimental sessions. The core tasks are to remove rotational ambiguity using Orthogonal Procrustes analysis, quantify the resulting improvement in matrix similarity, and assess the stability of the aligned axes based on a correlation criterion.\n\nThe analysis proceeds in five distinct steps, grounded in fundamental principles of linear algebra and multivariate statistics.\n\n**1. Normalization of Loading Matrices**\n\nThe raw loading matrices $L_1$ and $L_2$ may differ in ways that are scientifically uninteresting, such as global shifts or scaling of the axes. To focus the comparison on the geometric orientation of the axes, we first normalize each matrix. This process is applied to each column vector independently.\n\nFor each column $L_{\\cdot j}$ of a loading matrix $L \\in \\mathbb{R}^{n \\times k}$:\nFirst, we perform column-centering by subtracting the mean of its elements. The centered column $L_{c, \\cdot j}$ is given by:\n$$\nL_{c, \\cdot j} = L_{\\cdot j} - \\mu_j \\mathbf{1}\n$$\nwhere $\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} L_{ij}$ is the mean of the $j$-th column and $\\mathbf{1} \\in \\mathbb{R}^n$ is a vector of ones. This step ensures that the analysis is invariant to baseline shifts in neural activity along each axis.\n\nSecond, we perform unit-normalization. The centered column is divided by its Euclidean norm ($L^2$-norm) to produce a unit vector. The normalized column $\\tilde{L}_{\\cdot j}$ is:\n$$\n\\tilde{L}_{\\cdot j} = \\frac{L_{c, \\cdot j}}{\\|L_{c, \\cdot j}\\|_2}\n$$\nwhere $\\| \\cdot \\|_2$ denotes the Euclidean norm. This step makes the comparison invariant to the arbitrary scaling of the axes.\n\nApplying this procedure to all columns of $L_1$ and $L_2$ yields the normalized matrices $\\tilde{L}_1$ and $\\tilde{L}_2$. Each column of these matrices is a unit vector and is orthogonal to the vector $\\mathbf{1}$. The column space of $\\tilde{L}$ lies in a subspace of dimension at most $n-1$.\n\n**2. Raw Discrepancy**\n\nBefore alignment, we quantify the initial difference between the two sets of axes using the Frobenius norm of the difference between the normalized matrices. To make this measure scale-invariant with respect to the number of axes $k$, we normalize it by the Frobenius norm of the target matrix, $\\tilde{L}_2$. The raw discrepancy $d_{\\text{raw}}$ is defined as:\n$$\nd_{\\text{raw}} = \\frac{\\|\\tilde{L}_1 - \\tilde{L}_2\\|_F}{\\|\\tilde{L}_2\\|_F}\n$$\nSince each of the $k$ columns of $\\tilde{L}_2$ has a unit norm, its squared Frobenius norm is $\\|\\tilde{L}_2\\|_F^2 = \\sum_{j=1}^{k} \\|(\\tilde{L}_2)_{\\cdot j}\\|_2^2 = \\sum_{j=1}^{k} 1^2 = k$. Thus, the denominator is simply $\\sqrt{k}$.\n\n**3. Orthogonal Procrustes Alignment**\n\nThe subspaces spanned by the columns of $L_1$ and $L_2$ might be very similar, but the specific basis vectors (the columns) could be rotated with respect to each other. This rotational ambiguity is a common issue in dimensionality reduction methods. We resolve it by finding an optimal rotation that best aligns the axes of $\\tilde{L}_1$ to those of $\\tilde{L}_2$. This is the Orthogonal Procrustes problem. We seek an orthogonal matrix $R \\in \\mathbb{R}^{k \\times k}$ that minimizes the Frobenius norm of the difference between the rotated source matrix $\\tilde{L}_1 R$ and the target matrix $\\tilde{L}_2$:\n$$\n\\min_{R: R^\\top R=I_k} \\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F\n$$\nThe solution to this problem is found using the Singular Value Decomposition (SVD). Let $M = \\tilde{L}_2^\\top \\tilde{L}_1 \\in \\mathbb{R}^{k \\times k}$. We compute the SVD of $M$ as $M = U \\Sigma V^\\top$, where $U$ and $V$ are $k \\times k$ orthogonal matrices. The optimal rotation matrix $R$ is given by:\n$$\nR = V U^\\top\n$$\nWith this optimal rotation $R$, we define the aligned source matrix as $\\tilde{L}_{1,a} = \\tilde{L}_1 R$. The discrepancy after alignment, $d_{\\text{aligned}}$, is then calculated:\n$$\nd_{\\text{aligned}} = \\frac{\\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F}{\\|\\tilde{L}_2\\|_F} = \\frac{\\|\\tilde{L}_{1,a} - \\tilde{L}_2\\|_F}{\\sqrt{k}}\n$$\n\n**4. Improvement due to Alignment**\n\nThe reduction in discrepancy due to the Procrustes alignment tells us how much of the initial difference was attributable to a simple rotation of the axes. The improvement, $\\Delta$, is the difference between the raw and aligned discrepancies:\n$$\n\\Delta = d_{\\text{raw}} - d_{\\text{aligned}}\n$$\nA large value of $\\Delta$ indicates that the axes were indeed rotated relative to each other, and that alignment was effective. A value of $\\Delta$ near zero suggests that the axes were either already well-aligned or that their geometric structures are fundamentally different in a way that cannot be corrected by rotation.\n\n**5. Stability Analysis via Correlation**\n\nAfter globally aligning the axis systems, we assess the stability of individual axes by measuring the similarity between corresponding columns of the aligned source matrix $\\tilde{L}_{1,a}$ and the target matrix $\\tilde{L}_2$. The similarity is quantified by the absolute value of the cosine similarity (or Pearson correlation for centered data) between each pair of columns. For the $j$-th axis, the correlation is:\n$$\n\\rho_j = \\left| \\frac{(\\tilde{L}_{1,a})_{\\cdot j}^\\top (\\tilde{L}_2)_{\\cdot j}}{\\|(\\tilde{L}_{1,a})_{\\cdot j}\\|_2 \\|(\\tilde{L}_2)_{\\cdot j}\\|_2} \\right|\n$$\nNote that while the columns of $\\tilde{L}_2$ have unit norm by construction, the columns of the aligned matrix $\\tilde{L}_{1,a} = \\tilde{L}_1 R$ are not guaranteed to have unit norm, so their norms must be computed.\n\nThe overall stability is judged by the arithmetic mean of these absolute correlations across all $k$ axes:\n$$\n\\bar{\\rho} = \\frac{1}{k}\\sum_{j=1}^k \\rho_j\n$$\nThe pair of loading matrices is declared \"stable\" if this mean correlation meets or exceeds a predefined threshold $\\tau$. For this problem, $\\tau=0.98$.\n$$\n\\text{stable} = (\\bar{\\rho} \\ge \\tau)\n$$\n\n**Application to Test Cases**\nThe analysis is applied to four cases defined with $n=6$, $k=2$, and a given $L_1$.\n\nCase A (Identity): $L_2 = L_1$. The matrices are identical. Normalization yields $\\tilde{L}_1 = \\tilde{L}_2$. The raw discrepancy is $0$. The Procrustes alignment yields $R=I_k$ and the aligned discrepancy is also $0$. Thus $\\Delta = 0$. The columnwise correlations are all $1$, so the mean correlation is $1$, which is $\\ge 0.98$. The axes are stable.\n\nCase B (Rotation): $L_2 = L_1 Q$, where $Q$ is a rotation matrix. The column spaces of $L_1$ and $L_2$ are identical. The column-wise normalization procedure is not linear, so $\\tilde{L}_2$ is not a simple rotation of $\\tilde{L}_1$. This will result in a non-zero $d_{\\text{raw}}$. However, the underlying geometric structure is preserved. Procrustes analysis is expected to find a rotation $R$ that closely aligns $\\tilde{L}_1$ with $\\tilde{L}_2$, leading to a small $d_{\\text{aligned}}$ and a large $\\Delta$. The resulting mean correlation should be high, indicating stability.\n\nCase C (Permutation and Sign Flip): $L_2 = L_1 P$, where $P$ is an orthogonal matrix representing a permutation and a sign flip. This is similar to case B, as $P$ is also an orthogonal transformation. We expect a large initial discrepancy that is significantly reduced by the Procrustes alignment, yielding a large $\\Delta$ and a finding of stability.\n\nCase D (Orthogonal Complement): The columns of $L_2$ are chosen from an orthonormal basis for the orthogonal complement of the column space of $L_1$. The column spaces of $L_1$ and $L_2$ are, by construction, orthogonal. This orthogonality is largely preserved through the normalization step. Consequently, the columns of $\\tilde{L}_1$ and $\\tilde{L}_2$ are nearly orthogonal. No rotation can align these two fundamentally different subspaces. The Procrustes alignment will provide no significant improvement, so $\\Delta$ will be close to $0$. The correlations between aligned axes will be near $0$, far below $\\tau$, indicating the axes are not stable.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the neural loading matrix stability analysis.\n    \"\"\"\n\n    def analyze_stability(L1, L2, tau):\n        \"\"\"\n        Performs the 5-step analysis for a given pair of loading matrices (L1, L2).\n        \n        Args:\n            L1 (np.ndarray): The first loading matrix (n x k).\n            L2 (np.ndarray): The second loading matrix (n x k).\n            tau (float): The stability threshold for mean absolute correlation.\n            \n        Returns:\n            list: A list containing [delta, stable], where delta is the improvement\n                  and stable is a boolean.\n        \"\"\"\n        n, k = L1.shape\n\n        # Step 1: Column-center and unit-normalize L1 and L2\n        def normalize_matrix(L):\n            mean = L.mean(axis=0)\n            centered = L - mean\n            norm = np.linalg.norm(centered, axis=0)\n            # Handle columns that might have zero norm after centering\n            tilde_L = np.divide(centered, norm, out=np.zeros_like(centered, dtype=float), where=norm!=0)\n            return tilde_L\n\n        L1_tilde = normalize_matrix(L1)\n        L2_tilde = normalize_matrix(L2)\n        \n        # Denominator for discrepancy calculations\n        # Since L2_tilde has unit-norm columns, its Frobenius norm is sqrt(k)\n        norm_L2_tilde_F = np.sqrt(k)\n\n        # Step 2: Compute the raw normalized Frobenius discrepancy\n        d_raw = np.linalg.norm(L1_tilde - L2_tilde, 'fro') / norm_L2_tilde_F\n        \n        # Step 3: Solve the orthogonal Procrustes alignment problem\n        # Find R that minimizes ||L1_tilde @ R - L2_tilde||_F\n        M = L2_tilde.T @ L1_tilde\n        U, s, Vh = linalg.svd(M)\n        R = Vh.T @ U.T\n        \n        L1_aligned = L1_tilde @ R\n        \n        # Compute the aligned discrepancy\n        d_aligned = np.linalg.norm(L1_aligned - L2_tilde, 'fro') / norm_L2_tilde_F\n\n        # Step 4: Define the improvement\n        delta = d_raw - d_aligned\n\n        # Step 5: Compute mean absolute columnwise correlation and determine stability\n        correlations = []\n        for j in range(k):\n            col1 = L1_aligned[:, j]\n            col2 = L2_tilde[:, j]\n            \n            norm1 = np.linalg.norm(col1)\n            norm2 = np.linalg.norm(col2) # This will be 1.0\n            \n            if norm1 == 0 or norm2 == 0:\n                corr = 0.0\n            else:\n                corr = np.dot(col1, col2) / (norm1 * norm2)\n            correlations.append(np.abs(corr))\n        \n        mean_abs_corr = np.mean(correlations)\n        stable = mean_abs_corr = tau\n        \n        return [delta, stable]\n\n    # Test suite specification\n    n = 6\n    k = 2\n    tau = 0.98\n\n    L1 = np.array([\n        [0.5, -0.2],\n        [0.3, 0.1],\n        [0.0, 0.4],\n        [-0.1, 0.6],\n        [0.2, -0.3],\n        [0.7, 0.0]\n    ])\n\n    results = []\n\n    # Case A: Identity\n    L2_A = L1\n    results.append(analyze_stability(L1, L2_A, tau))\n\n    # Case B: Rotation in axis space\n    angle = np.pi / 6\n    Q = np.array([\n        [np.cos(angle), -np.sin(angle)],\n        [np.sin(angle), np.cos(angle)]\n    ])\n    L2_B = L1 @ Q\n    results.append(analyze_stability(L1, L2_B, tau))\n\n    # Case C: Permutation with sign flip\n    P = np.array([\n        [0, -1],\n        [1, 0]\n    ])\n    L2_C = L1 @ P\n    results.append(analyze_stability(L1, L2_C, tau))\n\n    # Case D: Orthogonal-complement axes\n    # Basis for the null space of L1.T, which is the orthogonal complement of colspan(L1)\n    null_space_basis = linalg.null_space(L1.T)\n    L2_D = null_space_basis[:, :k]\n    results.append(analyze_stability(L1, L2_D, tau))\n    \n    # Format the final output string\n    # Convert each sublist [delta, stable] to its string representation\n    # e.g., '[0.0, True]'\n    # Then join them with commas and wrap in brackets\n    # to get '[[0.0, True],[...]]'\n    final_output_str = f\"[{','.join(map(str, results))}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}