{
    "hands_on_practices": [
        {
            "introduction": "在应用目标性降维之前，我们必须对数据预处理做出选择，而这些选择并非随意的，它们反映了我们对神经数据噪声结构的潜在假设。本练习将通过线性判别分析（LDA）的视角，引导您从第一性原理出发，推导两种不同预处理流程所产生的任务轴。通过这个过程，您将深入理解诸如z-score标准化等常见步骤背后的统计学意义，以及它们如何影响我们对神经编码的最终解读。",
            "id": "4197375",
            "problem": "您正在分析在一个基线期和一个刺激期内记录的由 $n=3$ 个神经元组成的群体的试次平均发放率。数据模型如下：基线活动服从均值为 $\\boldsymbol{\\mu}_{b}$、协方差为 $\\boldsymbol{\\Sigma}_{b}$ 的多元高斯分布；刺激期活动服从均值为 $\\boldsymbol{\\mu}_{s}$ 的多元高斯分布，且与基线具有相同的类内协方差。您将使用目标性降维（TDR）方法，通过构建一个线性投影来估计一个一维的任务轴，该投影在共享高斯噪声的假设下，最大化刺激与基线之间的分离度。请考虑以下两种预处理流程：\n\n- 流程 A（基线减除和完全噪声白化）：您通过减去基线形成条件差异 $\\Delta \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_{s} - \\boldsymbol{\\mu}_{b}$，并使用完整的基线协方差 $\\boldsymbol{\\Sigma}_{b}$ 来估计线性判别轴，该轴最大化了类间方差与类内方差之比。\n\n- 流程 B（基线减除和逐神经元 $z$-score 标准化）：您如上所述减去基线，然后仅使用基线变异性对每个神经元进行 $z$-score 标准化，即通过 $\\mathbf{D}^{-1}$ 进行缩放，其中 $\\mathbf{D} = \\operatorname{diag}(\\sigma_{b,1}, \\sigma_{b,2}, \\sigma_{b,3})$ 且 $\\sigma_{b,i} = \\sqrt{(\\boldsymbol{\\Sigma}_{b})_{ii}}$。然后，您在 $z$-score 标准化后的坐标系中，在相同的共享噪声假设下，但使用由 $\\mathbf{D}$ 导出的对角噪声模型来估计线性判别轴。\n\n从线性判别分析的基本原理出发（即，在共享协方差模型下，通过最大化由类间方差与类内方差之比定义的分离度来推导最优轴），推导流程 A 和流程 B 在原始神经坐标系中产生的轴方向的解析表达式。然后，对于以下参数，计算这两个轴方向之间的主夹角，\n$$\n\\theta = \\arccos\\left(\\frac{\\boldsymbol{w}_{A}^{\\top}\\boldsymbol{w}_{B}}{\\|\\boldsymbol{w}_{A}\\|\\,\\|\\boldsymbol{w}_{B}\\|}\\right),\n$$\n以弧度表示：\n$$\n\\boldsymbol{\\mu}_{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad\n\\boldsymbol{\\mu}_{s} = \\begin{pmatrix} 2 \\\\ 5 \\\\ 0 \\end{pmatrix}, \\quad\n\\boldsymbol{\\Sigma}_{b} = \\begin{pmatrix} 2  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix}.\n$$\n使用基线标准差进行 $z$-score 标准化，即 $\\sigma_{b,1} = \\sqrt{2}$，$\\sigma_{b,2} = \\sqrt{3}$ 和 $\\sigma_{b,3} = \\sqrt{2}$，因此 $\\mathbf{D} = \\operatorname{diag}(\\sqrt{2}, \\sqrt{3}, \\sqrt{2})$。将最终答案以 $\\theta$ 的单个闭式解析表达式形式给出，单位为弧度。无需四舍五入。在您的推导中，明确阐述基线减除和 $z$-score 标准化如何影响轴的估计，并基于共享协方差、高斯噪声模型来论证这些预处理选择的合理性。",
            "solution": "该问题要求推导和比较通过两种不同的神经数据分析预处理流程获得的两个线性判别轴 $\\boldsymbol{w}_{A}$ 和 $\\boldsymbol{w}_{B}$。核心方法是线性判别分析（LDA），该方法旨在寻找一个投影轴 $\\boldsymbol{w}$，以最大化类间方差与类内方差之比。\n\n设两个类别（基线和刺激）由多元高斯分布建模，其均值分别为 $\\boldsymbol{\\mu}_{b}$ 和 $\\boldsymbol{\\mu}_{s}$，并共享一个类内协方差矩阵 $\\boldsymbol{\\Sigma}$。类均值之差为 $\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_{s} - \\boldsymbol{\\mu}_{b}$。\n\n当数据点 $\\boldsymbol{x}$ 投影到由向量 $\\boldsymbol{w}$ 定义的轴上时，投影坐标为 $y = \\boldsymbol{w}^{\\top}\\boldsymbol{x}$。投影后分布的均值为 $\\boldsymbol{w}^{\\top}\\boldsymbol{\\mu}_{b}$ 和 $\\boldsymbol{w}^{\\top}\\boldsymbol{\\mu}_{s}$。投影后的类间均值方差为：\n$$\nV_{\\text{between}} = (\\boldsymbol{w}^{\\top}\\boldsymbol{\\mu}_{s} - \\boldsymbol{w}^{\\top}\\boldsymbol{\\mu}_{b})^{2} = (\\boldsymbol{w}^{\\top}\\Delta\\boldsymbol{\\mu})^{2} = \\boldsymbol{w}^{\\top}(\\Delta\\boldsymbol{\\mu}\\Delta\\boldsymbol{\\mu}^{\\top})\\boldsymbol{w}\n$$\n投影后的类内方差由下式给出：\n$$\nV_{\\text{within}} = \\boldsymbol{w}^{\\top}\\boldsymbol{\\Sigma}\\boldsymbol{w}\n$$\n目标是最大化瑞利商 $J(\\boldsymbol{w})$：\n$$\nJ(\\boldsymbol{w}) = \\frac{V_{\\text{between}}}{V_{\\text{within}}} = \\frac{\\boldsymbol{w}^{\\top}(\\Delta\\boldsymbol{\\mu}\\Delta\\boldsymbol{\\mu}^{\\top})\\boldsymbol{w}}{\\boldsymbol{w}^{\\top}\\boldsymbol{\\Sigma}\\boldsymbol{w}}\n$$\n这个最大化问题的解由广义特征值问题 $(\\Delta\\boldsymbol{\\mu}\\Delta\\boldsymbol{\\mu}^{\\top})\\boldsymbol{w} = \\lambda\\boldsymbol{\\Sigma}\\boldsymbol{w}$ 给出。由于 $\\Delta\\boldsymbol{\\mu}\\Delta\\boldsymbol{\\mu}^{\\top}$ 是一个秩为 1 的矩阵，向量 $(\\Delta\\boldsymbol{\\mu}\\Delta\\boldsymbol{\\mu}^{\\top})\\boldsymbol{w}$ 总是与 $\\Delta\\boldsymbol{\\mu}$ 成比例。这导出了一个众所周知的解，其中最优轴方向 $\\boldsymbol{w}_{\\text{opt}}$ 与经协方差矩阵的逆变换后的均值差成比例：\n$$\n\\boldsymbol{w}_{\\text{opt}} \\propto \\boldsymbol{\\Sigma}^{-1}\\Delta\\boldsymbol{\\mu}\n$$\n这一基本结果构成了推导两个流程的轴的基础。\n\n首先，我们根据给定的参数计算条件差异向量 $\\Delta\\boldsymbol{\\mu}$：\n$$\n\\boldsymbol{\\mu}_{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad \\boldsymbol{\\mu}_{s} = \\begin{pmatrix} 2 \\\\ 5 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_{s} - \\boldsymbol{\\mu}_{b} = \\begin{pmatrix} 2 - 1 \\\\ 5 - 2 \\\\ 0 - (-1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 1 \\end{pmatrix}\n$$\n\n**流程 A：基线减除和完全噪声白化**\n\n该流程使用完整的基线协方差矩阵 $\\boldsymbol{\\Sigma}_{b}$ 作为共享协方差 $\\boldsymbol{\\Sigma}$。因此，判别轴 $\\boldsymbol{w}_{A}$ 为：\n$$\n\\boldsymbol{w}_{A} \\propto \\boldsymbol{\\Sigma}_{b}^{-1}\\Delta\\boldsymbol{\\mu}\n$$\n给定的协方差矩阵为：\n$$\n\\boldsymbol{\\Sigma}_{b} = \\begin{pmatrix} 2  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix}\n$$\n为了求其逆矩阵，我们首先计算行列式：$\\det(\\boldsymbol{\\Sigma}_{b}) = 2(3 \\cdot 2 - 1 \\cdot 1) - 1(1 \\cdot 2 - 1 \\cdot 0) + 0 = 2(5) - 2 = 8$。\n逆矩阵为 $\\boldsymbol{\\Sigma}_{b}^{-1} = \\frac{1}{\\det(\\boldsymbol{\\Sigma}_{b})} \\text{adj}(\\boldsymbol{\\Sigma}_{b})$：\n$$\n\\boldsymbol{\\Sigma}_{b}^{-1} = \\frac{1}{8}\\begin{pmatrix} 3 \\cdot 2 - 1 \\cdot 1  -(1 \\cdot 2 - 1 \\cdot 0)  1 \\cdot 1 - 3 \\cdot 0 \\\\ -(1 \\cdot 2 - 0 \\cdot 1)  2 \\cdot 2 - 0 \\cdot 0  -(2 \\cdot 1 - 0 \\cdot 1) \\\\ 1 \\cdot 1 - 3 \\cdot 0  -(2 \\cdot 1 - 1 \\cdot 0)  2 \\cdot 3 - 1 \\cdot 1 \\end{pmatrix}^{\\top} = \\frac{1}{8}\\begin{pmatrix} 5  -2  1 \\\\ -2  4  -2 \\\\ 1  -2  5 \\end{pmatrix}\n$$\n现在，我们计算 $\\boldsymbol{w}_{A}$ 的方向：\n$$\n\\boldsymbol{w}_{A} \\propto \\frac{1}{8}\\begin{pmatrix} 5  -2  1 \\\\ -2  4  -2 \\\\ 1  -2  5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{8}\\begin{pmatrix} 5 - 6 + 1 \\\\ -2 + 12 - 2 \\\\ 1 - 6 + 5 \\end{pmatrix} = \\frac{1}{8}\\begin{pmatrix} 0 \\\\ 8 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n因此，我们可以选择流程 A 的轴的代表向量为 $\\boldsymbol{w}_{A} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$。\n\n**流程 B：基线减除和逐神经元 $z$-score 标准化**\n\n该流程在应用 LDA 之前涉及对数据进行变换。数据使用基线标准差进行 $z$-score 标准化，这对应于通过矩阵 $\\mathbf{D}^{-1}$ 的线性变换，其中 $\\mathbf{D} = \\operatorname{diag}(\\sigma_{b,1}, \\sigma_{b,2}, \\sigma_{b,3})$。\n$$\n\\mathbf{D} = \\operatorname{diag}(\\sqrt{(\\boldsymbol{\\Sigma}_{b})_{11}}, \\sqrt{(\\boldsymbol{\\Sigma}_{b})_{22}}, \\sqrt{(\\boldsymbol{\\Sigma}_{b})_{33}}) = \\operatorname{diag}(\\sqrt{2}, \\sqrt{3}, \\sqrt{2})\n$$\n在这个 $z$-score 标准化后的空间中，均值差向量为 $\\Delta\\boldsymbol{\\mu}_{z} = \\mathbf{D}^{-1}\\Delta\\boldsymbol{\\mu}$。该流程指定使用“对角噪声模型”，在这个归一化空间中，这对应于假设协方差矩阵是单位矩阵，即 $\\boldsymbol{\\Sigma}_{\\text{assumed}} = \\mathbf{I}$。在 $z$-score 标准化空间中的判别轴 $\\boldsymbol{w}_{B,z}$ 则为：\n$$\n\\boldsymbol{w}_{B,z} \\propto \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\Delta\\boldsymbol{\\mu}_{z} = \\mathbf{I}^{-1}(\\mathbf{D}^{-1}\\Delta\\boldsymbol{\\mu}) = \\mathbf{D}^{-1}\\Delta\\boldsymbol{\\mu}\n$$\n为了在原始坐标系中找到轴 $\\boldsymbol{w}_{B}$，我们必须将 $\\boldsymbol{w}_{B,z}$ 映射回去。在 $z$-score 标准化空间中的一个投影是 $y = \\boldsymbol{w}_{B,z}^{\\top}\\boldsymbol{z}$。代入 $\\boldsymbol{z} = \\mathbf{D}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_{b})$ 得到 $y = \\boldsymbol{w}_{B,z}^{\\top}\\mathbf{D}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_{b})$。因此，在原始空间中的投影向量为 $\\boldsymbol{w}_{B} = (\\mathbf{D}^{-1})^{\\top}\\boldsymbol{w}_{B,z} = \\mathbf{D}^{-1}\\boldsymbol{w}_{B,z}$。代入 $\\boldsymbol{w}_{B,z}$ 的表达式：\n$$\n\\boldsymbol{w}_{B} \\propto \\mathbf{D}^{-1}(\\mathbf{D}^{-1}\\Delta\\boldsymbol{\\mu}) = \\mathbf{D}^{-2}\\Delta\\boldsymbol{\\mu}\n$$\n这等价于在原始空间中执行 LDA，但假设协方差矩阵是对角矩阵 $\\boldsymbol{\\Sigma} = \\mathbf{D}^{2} = \\operatorname{diag}((\\boldsymbol{\\Sigma}_{b})_{11}, (\\boldsymbol{\\Sigma}_{b})_{22}, (\\boldsymbol{\\Sigma}_{b})_{33})$。\n我们计算 $\\mathbf{D}^{-2} = \\operatorname{diag}(1/2, 1/3, 1/2)$。\n$$\n\\boldsymbol{w}_{B} \\propto \\begin{pmatrix} 1/2  0  0 \\\\ 0  1/3  0 \\\\ 0  0  1/2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1 \\\\ 1/2 \\end{pmatrix}\n$$\n为简化起见，我们可以选择代表向量 $\\boldsymbol{w}_{B} = 2 \\times \\begin{pmatrix} 1/2 \\\\ 1 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$。\n\n预处理流程的选择反映了对噪声结构的不同假设。基线减除是一个标准步骤，其目的是将问题集中于从噪声中分离出信号分量 $\\Delta\\boldsymbol{\\mu}$。在假设完整的协方差矩阵 $\\boldsymbol{\\Sigma}_{b}$ 已知且能准确反映噪声结构的情况下，流程 A 是最优的。它对噪声执行了最优的“白化”处理。流程 B 通过假设神经元之间的噪声是独立的（在按其各自方差进行缩放后）来简化了这个问题。当完整的协方差难以可靠估计，或者当人们想要一个更稳健（尽管可能次优）且仅依赖于单个神经元统计数据的解码器时，这是一个常见的实际选择。\n\n**角度计算**\n\n最后，我们计算两个轴之间的主夹角 $\\theta$。\n$$\n\\theta = \\arccos\\left(\\frac{\\boldsymbol{w}_{A}^{\\top}\\boldsymbol{w}_{B}}{\\|\\boldsymbol{w}_{A}\\|\\,\\|\\boldsymbol{w}_{B}\\|}\\right)\n$$\n使用我们推导出的向量 $\\boldsymbol{w}_{A} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$ 和 $\\boldsymbol{w}_{B} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$：\n点积为：\n$$\n\\boldsymbol{w}_{A}^{\\top}\\boldsymbol{w}_{B} = (0)(1) + (1)(2) + (0)(1) = 2\n$$\n向量的范数为：\n$$\n\\|\\boldsymbol{w}_{A}\\| = \\sqrt{0^{2} + 1^{2} + 0^{2}} = 1\n$$\n$$\n\\|\\boldsymbol{w}_{B}\\| = \\sqrt{1^{2} + 2^{2} + 1^{2}} = \\sqrt{1 + 4 + 1} = \\sqrt{6}\n$$\n将这些值代入 $\\theta$ 的公式中：\n$$\n\\theta = \\arccos\\left(\\frac{2}{1 \\cdot \\sqrt{6}}\\right) = \\arccos\\left(\\frac{2}{\\sqrt{6}}\\right)\n$$\n参数可以简化为：\n$$\n\\frac{2}{\\sqrt{6}} = \\frac{\\sqrt{4}}{\\sqrt{6}} = \\sqrt{\\frac{4}{6}} = \\sqrt{\\frac{2}{3}}\n$$\n因此，角度为：\n$$\n\\theta = \\arccos\\left(\\sqrt{\\frac{2}{3}}\\right)\n$$",
            "answer": "$$\\boxed{\\arccos\\left(\\sqrt{\\frac{2}{3}}\\right)}$$"
        },
        {
            "introduction": "理解了理论假设之后，下一步是亲手实现一个核心的目标性降维分析。目标性降维不仅是为了找到与任务相关的神经维度，更关键的是要理解是哪些神经元构成了这个维度。本练习将指导您从零开始，利用最小二乘回归找到预测任务变量的最佳轴，并实现一种将解释方差分解回单个神经元的方法。",
            "id": "4197409",
            "problem": "考虑一个神经群体数据集，由一个试验-神经元活动矩阵 $X \\in \\mathbb{R}^{T \\times N}$、一个任务回归量 $y \\in \\mathbb{R}^{T}$，以及可选的一个代表目标维度的固定轴 $a \\in \\mathbb{R}^{N}$ 表示。您的目标是计算在神经元空间中沿单个轴的任务相关信号的解释方差的逐神经元贡献，并根据贡献对神经元进行排序。此任务必须从第一性原理出发，使用最小二乘回归、方差和协方差的定义来解决，除了线性代数和基本运算外，不依赖任何预打包的统计函数。\n\n从以下核心定义和事实开始：\n- 预测 $y$ 从 $X$ 的普通最小二乘解 $a^{\\star}$ 最小化了在 $a \\in \\mathbb{R}^{N}$ 上的平方误差 $\\|X a - y\\|_2^2$。当 $X$ 具有线性相关的列时，Moore–Penrose 伪逆产生最小范数解。\n- 对于中心化数据（已移除列均值），样本协方差矩阵为 $\\Sigma = \\frac{1}{T-1} X^{\\top} X$，中心化向量的样本方差为 $s_y^2 = \\frac{1}{T-1} y^{\\top} y$。\n- 对于神经元空间中的任意轴 $a \\in \\mathbb{R}^{N}$，投影 $X a$ 的方差为 $\\mathrm{Var}(X a) = a^{\\top} \\Sigma a$。\n- 投影方差的逐神经元加性分解可以定义为 $c_i = a_i \\left(\\Sigma a\\right)_i$ for $i \\in \\{0,\\dots,N-1\\}$，它满足 $\\sum_{i=0}^{N-1} c_i = a^{\\top}\\Sigma a$。\n\n您必须为每个测试用例实现以下计算流程：\n- 对 $X$ 进行列中心化，并通过减去其在所有试验中的均值来对 $y$ 进行中心化。在 $X$ 和 $y$ 中使用相同的 $T$。\n- 如果未提供轴 $a$，则通过使用 Moore–Penrose 伪逆求解 $y$ 对 $X$ 的最小二乘回归来计算 $a^{\\star}$。使用这个 $a^{\\star}$ 作为轴。\n- 计算样本协方差矩阵 $\\Sigma = \\frac{1}{T-1} X^{\\top} X$ 和 $s_y^2 = \\frac{1}{T-1} y^{\\top} y$。\n- 计算每个神经元的贡献 $c_i = a_i \\left(\\Sigma a\\right)_i$ for $i \\in \\{0,\\dots,N-1\\}$，使得 $\\sum_i c_i = a^{\\top} \\Sigma a = \\mathrm{Var}(X a)$。当 $s_y^2  0$ 时，将神经元 $i$ 贡献的解释方差分数定义为 $f_i = \\frac{c_i}{s_y^2}$。如果 $s_y^2 = 0$，则对所有 $i$ 定义 $f_i = 0$。\n- 将沿轴的总解释方差分数定义为 $R^2 = \\frac{\\sum_i c_i}{s_y^2}$（当 $s_y^2  0$ 时），以及 $R^2 = 0$（当 $s_y^2 = 0$ 时）。\n- 按 $f_i$ 降序对神经元进行排序。若出现平局，则按神经元索引升序排列（即，最小的索引在前）。\n\n您的程序必须处理以下测试套件。对于所有测试用例，索引均为 $0$ 基。所有矩阵和向量都明确指定。\n\n测试用例 $1$（通用，良态）：\n- $T = 6$, $N = 4$。\n- $X = \\begin{bmatrix}\n1  0  2  1 \\\\\n0  1  1  2 \\\\\n2  1  3  0 \\\\\n3  2  5  1 \\\\\n1  1  2  2 \\\\\n0  0  1  0\n\\end{bmatrix}$。\n- $y = \\begin{bmatrix} 1  0  1  2  1  0 \\end{bmatrix}^{\\top}$。\n- 提供的轴：无（通过最小二乘法计算）。\n\n测试用例 $2$（包含一个零方差神经元）：\n- $T = 4$, $N = 3$。\n- $X = \\begin{bmatrix}\n1  2  5 \\\\\n1  0  5 \\\\\n1  -1  5 \\\\\n1  1  5\n\\end{bmatrix}$。\n- $y = \\begin{bmatrix} 0  1  -1  0 \\end{bmatrix}^{\\top}$。\n- 提供的轴：无（通过最小二乘法计算）。\n\n测试用例 $3$（外部提供轴）：\n- $T = 5$, $N = 3$。\n- $X = \\begin{bmatrix}\n0  1  2 \\\\\n1  2  3 \\\\\n2  3  4 \\\\\n3  4  5 \\\\\n4  5  6\n\\end{bmatrix}$。\n- $y = \\begin{bmatrix} -1  0  1  2  3 \\end{bmatrix}^{\\top}$。\n- 提供的轴：$a = \\begin{bmatrix} 1.0  -0.5  0.0 \\end{bmatrix}^{\\top}$。\n\n测试用例 $4$（零方差的任务回归量）：\n- $T = 4$, $N = 2$。\n- $X = \\begin{bmatrix}\n0  1 \\\\\n1  1 \\\\\n2  1 \\\\\n3  1\n\\end{bmatrix}$。\n- $y = \\begin{bmatrix} 5  5  5  5 \\end{bmatrix}^{\\top}$。\n- 提供的轴：无（通过最小二乘法计算，但 $s_y^2 = 0$ 将强制 $R^2 = 0$ 且所有 $f_i = 0$）。\n\n测试用例 $5$（共线神经元，奇异设计）：\n- $T = 5$, $N = 3$。\n- $X = \\begin{bmatrix}\n0  0  1 \\\\\n1  2  2 \\\\\n2  4  3 \\\\\n3  6  4 \\\\\n4  8  5\n\\end{bmatrix}$。\n- $y = \\begin{bmatrix} 0  1  2  3  4 \\end{bmatrix}^{\\top}$。\n- 提供的轴：无（通过带伪逆的最小二乘法计算）。\n\n最终输出规范：\n- 对于每个测试用例，生成一个三元组，包含：\n  $1)$ 标量 $R^2$，四舍五入到 $6$ 位小数，\n  $2)$ 按 $f_i$ 降序排列的神经元索引列表，平局时按索引升序处理，\n  $3)$ $f_i$ 列表（$i = 0, \\dots, N-1$），按索引顺序排列，每个值四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含一个由这些三元组组成的、用方括号括起来的逗号分隔列表。例如：$[\\,[R^2_1,[i_{1,1},\\dots],[f_{1,0},\\dots]],\\,[R^2_2,[i_{2,1},\\dots],[f_{2,0},\\dots]],\\,\\dots\\,]$。\n- 此问题中没有物理单位或角度单位。所有报告的分数必须是小数，而不是百分比。",
            "solution": "该问题在计算神经科学和统计学领域提出了一个定义明确且有科学依据的任务。它要求实现一种特定方法，将任务变量的解释方差归因于单个神经元。所有提供的定义在数学上都是合理的，计算流程也得到了明确无误的规定，包括处理潜在问题的方法，如共线性（通过 Moore-Penrose 伪逆）和因变量的零方差。该问题是自成一体、客观且内部一致的。因此，该问题是有效的，并且可以推导出解决方案。\n\n该方法将通过遵循规定的计算步骤来实现。该程序的核心是量化一个任务相关信号 $y$ 中的方差，在多大程度上可以由投影到高维神经元空间中特定一维轴 $a$ 上的神经活动 $X$ 来解释。\n\n首先，必须准备原始数据。设 $X \\in \\mathbb{R}^{T \\times N}$ 为 $N$ 个神经元在 $T$ 次试验中的神经活动矩阵，设 $y \\in \\mathbb{R}^{T}$ 为相应的任务回归量。神经活动和任务回归量都必须通过减去各自的均值来进行中心化。此步骤确保后续的方差和协方差计算不受基线偏移的影响。中心化后的数据表示为 $X_c$ 和 $y_c$。\n$X$ 的列均值 $\\mu_X \\in \\mathbb{R}^{N}$ 和 $y$ 的均值 $\\mu_y \\in \\mathbb{R}$ 计算如下：\n$$ \\mu_{X,j} = \\frac{1}{T} \\sum_{i=0}^{T-1} X_{ij} \\quad \\text{for } j=0, \\dots, N-1 $$\n$$ \\mu_y = \\frac{1}{T} \\sum_{i=0}^{T-1} y_i $$\n然后，中心化矩阵为 $X_c = X - \\mathbf{1}\\mu_X^{\\top}$ 和 $y_c = y - \\mu_y\\mathbf{1}$，其中 $\\mathbf{1}$ 是一个包含 $T$ 个 1 的列向量。\n\n其次，必须确定投影轴 $a \\in \\mathbb{R}^{N}$。问题指定了两种情况：\n$1$. 轴 $a$ 是外部提供的。这对应于测试一个关于神经状态空间中特定方向的预定义假设。\n$2$. 未提供轴。在这种情况下，它被计算为从 $X_c$ 预测 $y_c$ 的最优线性解码器。这是普通最小二乘（OLS）解 $a^{\\star}$，它最小化了平方误差 $\\|X_c a - y_c\\|_2^2$。为确保即使在 $X_c$ 的列线性相关（即设计是奇异的）时也存在唯一解，我们使用 Moore-Penrose 伪逆 $X_c^{+}$ 来计算 $a^{\\star}$：\n$$ a = a^{\\star} = X_c^{+} y_c $$\n这在所有可能的解中给出了最小范数权重向量，这是一种标准且稳健的方法。\n\n第三，我们计算必要的二阶统计量。对于中心化数据，神经活动的样本协方差矩阵 $\\Sigma \\in \\mathbb{R}^{N \\times N}$ 为：\n$$ \\Sigma = \\frac{1}{T-1} X_c^{\\top} X_c $$\n任务回归量的样本方差 $s_y^2$ 为：\n$$ s_y^2 = \\frac{1}{T-1} y_c^{\\top} y_c $$\n分母 $T-1$ 用于样本方差/协方差的无偏估计。如果任务回归量没有方差，即 $s_y^2=0$，则出现一种特殊情况。在这种情况下，没有方差可以被解释，因此问题正确地规定所有结果分数应为零。\n\n第四，我们计算对解释方差的逐神经元贡献。投影到轴 $a$ 上的神经数据的总方差由 $\\mathrm{Var}(X_c a) = a^{\\top}\\Sigma a$ 给出。问题定义了将此方差加性分解为逐神经元分量 $c_i$ 的特定方法：\n$$ c_i = a_i (\\Sigma a)_i \\quad \\text{for } i=0, \\dots, N-1 $$\n其中 $a_i$ 是轴向量 $a$ 的第 $i$ 个元素，$(\\Sigma a)_i$ 是协方差矩阵与轴向量乘积所得向量的第 $i$ 个元素。这些分量的总和恢复了总投影方差：$\\sum_{i=0}^{N-1} c_i = \\sum_{i=0}^{N-1} a_i (\\Sigma a)_i = a^{\\top}(\\Sigma a) = \\mathrm{Var}(X_c a)$。\n\n第五，我们将这些贡献归一化，以表示为任务回归量总方差 $s_y^2$ 的分数。由神经元 $i$ 贡献的解释方差分数表示为 $f_i$：\n$$ f_i = \\frac{c_i}{s_y^2} \\quad (\\text{if } s_y^2  0) $$\n沿轴 $a$ 的总解释方差分数，表示为 $R^2$，是这些单独分数的总和：\n$$ R^2 = \\sum_{i=0}^{N-1} f_i = \\frac{\\sum_{i=0}^{N-1} c_i}{s_y^2} = \\frac{a^{\\top}\\Sigma a}{s_y^2} \\quad (\\text{if } s_y^2  0) $$\n如果 $s_y^2=0$，那么 $R^2=0$ 并且所有 $f_i=0$。当 $a$ 是 OLS 解 $a^{\\star}$ 时，这个 $R^2$ 就是标准的决定系数。\n\n最后，为识别最具影响力的神经元，我们根据它们的贡献 $f_i$ 按降序对它们进行排序。为确保唯一的排序，$f_i$ 值的任何平局都通过按升序对相应的神经元索引进行排序来解决。\n\n这个完整、分步的程序提供了一种严谨且可复现的方法，用于分析神经元对任务编码的贡献，其基础是线性代数和统计学的基本原理。实现将忠实地遵循此设计来处理每个提供的测试用例。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes neuron-wise contributions to explained variance for a series of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"id\": 1,\n            \"T\": 6, \"N\": 4,\n            \"X\": np.array([\n                [1, 0, 2, 1],\n                [0, 1, 1, 2],\n                [2, 1, 3, 0],\n                [3, 2, 5, 1],\n                [1, 1, 2, 2],\n                [0, 0, 1, 0]\n            ], dtype=float),\n            \"y\": np.array([1, 0, 1, 2, 1, 0], dtype=float),\n            \"a\": None\n        },\n        {\n            \"id\": 2,\n            \"T\": 4, \"N\": 3,\n            \"X\": np.array([\n                [1, 2, 5],\n                [1, 0, 5],\n                [1, -1, 5],\n                [1, 1, 5]\n            ], dtype=float),\n            \"y\": np.array([0, 1, -1, 0], dtype=float),\n            \"a\": None\n        },\n        {\n            \"id\": 3,\n            \"T\": 5, \"N\": 3,\n            \"X\": np.array([\n                [0, 1, 2],\n                [1, 2, 3],\n                [2, 3, 4],\n                [3, 4, 5],\n                [4, 5, 6]\n            ], dtype=float),\n            \"y\": np.array([-1, 0, 1, 2, 3], dtype=float),\n            \"a\": np.array([1.0, -0.5, 0.0], dtype=float)\n        },\n        {\n            \"id\": 4,\n            \"T\": 4, \"N\": 2,\n            \"X\": np.array([\n                [0, 1],\n                [1, 1],\n                [2, 1],\n                [3, 1]\n            ], dtype=float),\n            \"y\": np.array([5, 5, 5, 5], dtype=float),\n            \"a\": None\n        },\n        {\n            \"id\": 5,\n            \"T\": 5, \"N\": 3,\n            \"X\": np.array([\n                [0, 0, 1],\n                [1, 2, 2],\n                [2, 4, 3],\n                [3, 6, 4],\n                [4, 8, 5]\n            ], dtype=float),\n            \"y\": np.array([0, 1, 2, 3, 4], dtype=float),\n            \"a\": None\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        a_provided = case[\"a\"]\n        T, N = X.shape\n\n        # Step 1: Center data\n        X_c = X - np.mean(X, axis=0)\n        y_c = y - np.mean(y)\n\n        # Step 2: Determine axis a\n        if a_provided is not None:\n            a = a_provided\n        else:\n            X_c_plus = np.linalg.pinv(X_c)\n            a = X_c_plus @ y_c\n        \n        # Step 3: Compute sample covariance and variance\n        # Handle cases where T = 1, which leads to division by zero for sample variance\n        if T == 1:\n            s_y_sq = 0.0\n            Sigma = np.zeros((N, N))\n        else:\n            s_y_sq = (y_c.T @ y_c) / (T - 1)\n            Sigma = (X_c.T @ X_c) / (T - 1)\n\n        # Use a small tolerance for floating point comparison to zero\n        if s_y_sq  1e-12:\n            s_y_sq = 0.0\n        \n        # Step 4, 5: Compute R^2 and f_i, handling s_y^2 = 0 case\n        if s_y_sq == 0.0:\n            R2 = 0.0\n            f = np.zeros(N)\n        else:\n            c = a * (Sigma @ a)  # Element-wise product\n            sum_c = np.sum(c)\n            R2 = sum_c / s_y_sq\n            f = c / s_y_sq\n\n        # Step 6: Rank neurons by descending f_i, breaking ties with ascending index\n        neuron_scores = list(zip(f, range(N)))\n        neuron_scores.sort(key=lambda x: (-x[0], x[1]))\n        ranked_indices = [idx for score, idx in neuron_scores]\n\n        # Step 7: Format results for the current test case\n        R2_rounded = round(R2, 6)\n        f_rounded = [round(val, 6) for val in f]\n        \n        all_results.append([R2_rounded, ranked_indices, f_rounded])\n\n    # Final print statement in the exact required format\n    case_strs = []\n    for r2, indices, f_vals in all_results:\n        indices_str = f\"[{','.join(map(str, indices))}]\"\n        f_vals_str = f\"[{','.join(map(str, f_vals))}]\"\n        case_strs.append(f\"[{r2},{indices_str},{f_vals_str}]\")\n\n    print(f\"[{','.join(case_strs)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "验证结果的稳健性是科学研究的关键环节。本练习将处理一个高级但实际的问题：如何比较在不同实验中发现的神经表征，并解决降维方法中固有的旋转模糊性。您将学习并实现Procrustes分析，这是一种用于对齐向量空间的标准技术，从而能够可靠地评估神经维度的稳定性。",
            "id": "4197417",
            "problem": "给定两个会话 (session) 的多元神经加载矩阵，这些矩阵代表了由靶向降维产生的轴。每个会话被建模为一个实值矩阵，其列向量张成一个低维轴子空间。设第一个会话的加载矩阵为 $L_1 \\in \\mathbb{R}^{n \\times k}$，第二个会话的加载矩阵为 $L_2 \\in \\mathbb{R}^{n \\times k}$，其中 $n$ 是神经元的数量，$k$ 是轴的数量。目标是分析跨会话的轴的稳定性，并通过正交普氏对齐 (orthogonal Procrustes alignment) 来消除旋转不确定性，然后量化直接矩阵差异的改善程度，并根据相关性准则确定对齐后的轴是否稳定。\n\n从线性代数中的以下基本定义和事实开始：\n- 一个矩阵 $R \\in \\mathbb{R}^{k \\times k}$ 是正交的，当且仅当 $R^\\top R = I_k$，其中 $I_k$ 是 $k \\times k$ 的单位矩阵。\n- 矩阵 $A \\in \\mathbb{R}^{p \\times q}$ 的弗罗贝尼乌斯范数 (Frobenius norm) 定义为 $\\|A\\|_F = \\sqrt{\\sum_{i=1}^{p} \\sum_{j=1}^{q} a_{ij}^2}$。\n- 奇异值分解 (Singular Value Decomposition, SVD) 将任意矩阵 $M \\in \\mathbb{R}^{p \\times q}$ 分解为 $M = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{p \\times p}$ 和 $V \\in \\mathbb{R}^{q \\times q}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{p \\times q}$ 是具有非负元素的对角矩阵。\n- 按神经元进行列中心化 (Column centering) 是指对每一列 $j$，减去其 $n$ 个元素的均值。列单位归一化 (Column unit normalization) 是指将每个中心化后的列除以其欧几里得范数，使得每列的范数为单位1。\n\n为每对 $(L_1, L_2)$ 实现以下分析：\n1. 对 $L_1$ 和 $L_2$ 按神经元维度进行列中心化，并对每列进行单位归一化，得到 $\\tilde{L}_1$ 和 $\\tilde{L}_2$。\n2. 计算原始归一化弗罗贝尼乌斯差异 $d_{\\text{raw}} = \\|\\tilde{L}_1 - \\tilde{L}_2\\|_F / \\|\\tilde{L}_2\\|_F$。\n3. 解决正交普氏对齐问题：找到一个正交矩阵 $R \\in \\mathbb{R}^{k \\times k}$，使得 $\\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F$ 最小化。使用此对齐计算 $d_{\\text{aligned}} = \\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F / \\|\\tilde{L}_2\\|_F$。\n4. 将改善程度定义为 $\\Delta = d_{\\text{raw}} - d_{\\text{aligned}}$。\n5. 计算对齐轴与目标轴之间的平均绝对列相关性，定义为对 $j=1,\\dots,k$，$\\tilde{L}_1 R$ 的第 $j$ 列与 $\\tilde{L}_2$ 的第 $j$ 列之间余弦相似度绝对值的算术平均值，即 $\\frac{1}{k} \\sum_{j=1}^{k} \\left| \\frac{(\\tilde{L}_1 R)_{\\cdot j}^\\top \\tilde{L}_2_{\\cdot j}}{\\|(\\tilde{L}_1 R)_{\\cdot j}\\|_2 \\, \\|\\tilde{L}_2_{\\cdot j}\\|_2} \\right|$。通过将此平均值与阈值 $\\tau$ 比较来确定稳定性，当且仅当平均绝对相关性大于或等于 $\\tau$ 时，判定该对矩阵是稳定的。\n\n凡是指定角度之处，均使用弧度值。本问题不涉及物理单位。将所有布尔决策表示为精确的布尔值，所有数值结果表示为实值浮点数。\n\n测试套件规范：\n- 设 $n = 6$ 且 $k = 2$。\n- 定义\n$$\nL_1 = \\begin{bmatrix}\n0.5  -0.2 \\\\\n0.3  0.1 \\\\\n0.0  0.4 \\\\\n-0.1  0.6 \\\\\n0.2  -0.3 \\\\\n0.7  0.0\n\\end{bmatrix}.\n$$\n构建四对 $(L_1, L_2)$：\n1. 情况A (恒等)：$L_2 = L_1$。\n2. 情况B (轴空间中的旋转)：$L_2 = L_1 Q$，其中\n$$\nQ = \\begin{bmatrix}\n\\cos(\\pi/6)  -\\sin(\\pi/6) \\\\\n\\sin(\\pi/6)  \\cos(\\pi/6)\n\\end{bmatrix},\n$$\n角度以弧度指定。\n3. 情况C (带符号翻转的置换)：$L_2 = L_1 P$，其中\n$$\nP = \\begin{bmatrix}\n0  -1 \\\\\n1  \\phantom{-}0\n\\end{bmatrix}.\n$$\n4. 情况D (正交补轴)：设 $N$ 是 $L_1$ 在 $\\mathbb{R}^n$ 中列空间的正交补空间（等价于 $L_1^\\top$ 的零空间）的一组标准正交基。设 $N = [n_1 \\, n_2 \\, n_3 \\, n_4] \\in \\mathbb{R}^{n \\times (n-k)}$，其列向量以任意但确定的顺序排列。设置 $L_2 = [n_1 \\, n_2] \\in \\mathbb{R}^{n \\times 2}$。\n\n稳定性阈值：\n- 使用 $\\tau = 0.98$。\n\n你的程序应为每种情况计算有序对 $\\left[\\Delta, \\text{stable}\\right]$，其中 $\\Delta$ 是作为浮点数的改善程度，$\\text{stable}$ 是一个布尔值，指示对齐后的平均绝对相关性是否超过或等于 $\\tau$。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试用例，本身是一个形如 $\\left[\\Delta,\\text{stable}\\right]$ 的双元素列表。例如，打印的行应类似于 $[[\\Delta_1,\\text{stable}_1],[\\Delta_2,\\text{stable}_2],[\\Delta_3,\\text{stable}_3],[\\Delta_4,\\text{stable}_4]]$。",
            "solution": "该问题要求对两个神经加载矩阵 $L_1, L_2 \\in \\mathbb{R}^{n \\times k}$ 进行比较分析，这两个矩阵代表了在两个实验会话中通过靶向降维分析得到的轴。核心任务是使用正交普氏分析消除旋转不确定性，量化由此带来的矩阵相似性改善程度，并基于相关性准则评估对齐后轴的稳定性。\n\n该分析分五个不同步骤进行，这些步骤基于线性代数和多元统计的基本原理。\n\n**1. 加载矩阵的归一化**\n\n原始加载矩阵 $L_1$ 和 $L_2$ 可能在一些科学上不重要的方式上有所不同，例如轴的全局平移或缩放。为了将比较集中在轴的几何方向上，我们首先对每个矩阵进行归一化。此过程独立应用于每个列向量。\n\n对于加载矩阵 $L \\in \\mathbb{R}^{n \\times k}$ 的每一列 $L_{\\cdot j}$：\n首先，我们通过减去其元素的均值来进行列中心化。中心化后的列 $L_{c, \\cdot j}$ 由下式给出：\n$$\nL_{c, \\cdot j} = L_{\\cdot j} - \\mu_j \\mathbf{1}\n$$\n其中 $\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} L_{ij}$ 是第 $j$ 列的均值，$\\mathbf{1} \\in \\mathbb{R}^n$ 是一个全为1的向量。此步骤确保分析对每个轴上神经活动的基线漂移不敏感。\n\n其次，我们进行单位归一化。将中心化后的列除以其欧几里得范数（$L^2$-范数），以产生一个单位向量。归一化后的列 $\\tilde{L}_{\\cdot j}$ 为：\n$$\n\\tilde{L}_{\\cdot j} = \\frac{L_{c, \\cdot j}}{\\|L_{c, \\cdot j}\\|_2}\n$$\n其中 $\\| \\cdot \\|_2$ 表示欧几里得范数。此步骤使比较对轴的任意缩放不敏感。\n\n将此过程应用于 $L_1$ 和 $L_2$ 的所有列，得到归一化矩阵 $\\tilde{L}_1$ 和 $\\tilde{L}_2$。这些矩阵的每一列都是单位向量，并且与向量 $\\mathbf{1}$ 正交。$\\tilde{L}$ 的列空间位于一个维度至多为 $n-1$ 的子空间中。\n\n**2. 原始差异**\n\n在对齐之前，我们使用归一化矩阵之差的弗罗贝尼乌斯范数来量化两组轴之间的初始差异。为了使该度量相对于轴数 $k$ 具有尺度不变性，我们用目标矩阵 $\\tilde{L}_2$ 的弗罗贝尼乌斯范数对其进行归一化。原始差异 $d_{\\text{raw}}$ 定义为：\n$$\nd_{\\text{raw}} = \\frac{\\|\\tilde{L}_1 - \\tilde{L}_2\\|_F}{\\|\\tilde{L}_2\\|_F}\n$$\n由于 $\\tilde{L}_2$ 的 $k$ 个列向量均为单位范数，其弗罗贝尼乌斯范数的平方为 $\\|\\tilde{L}_2\\|_F^2 = \\sum_{j=1}^{k} \\|(\\tilde{L}_2)_{\\cdot j}\\|_2^2 = \\sum_{j=1}^{k} 1^2 = k$。因此，分母就是 $\\sqrt{k}$。\n\n**3. 正交普氏对齐**\n\n由 $L_1$ 和 $L_2$ 的列张成的子空间可能非常相似，但具体的基向量（即列向量）可能相互之间存在旋转。这种旋转不确定性是降维方法中的一个常见问题。我们通过寻找一个最佳旋转，将 $\\tilde{L}_1$ 的轴最好地对齐到 $\\tilde{L}_2$ 的轴，来解决这个问题。这就是正交普氏问题。我们寻求一个正交矩阵 $R \\in \\mathbb{R}^{k \\times k}$，它最小化旋转后的源矩阵 $\\tilde{L}_1 R$ 与目标矩阵 $\\tilde{L}_2$ 之间的弗罗贝尼乌斯范数：\n$$\n\\min_{R: R^\\top R=I_k} \\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F\n$$\n这个问题的解可以通过奇异值分解 (SVD) 找到。设 $M = \\tilde{L}_2^\\top \\tilde{L}_1 \\in \\mathbb{R}^{k \\times k}$。我们计算 $M$ 的SVD为 $M = U \\Sigma V^\\top$，其中 $U$ 和 $V$ 是 $k \\times k$ 的正交矩阵。最佳旋转矩阵 $R$ 由下式给出：\n$$\nR = V U^\\top\n$$\n利用这个最佳旋转 $R$，我们将对齐后的源矩阵定义为 $\\tilde{L}_{1,a} = \\tilde{L}_1 R$。然后计算对齐后的差异 $d_{\\text{aligned}}$：\n$$\nd_{\\text{aligned}} = \\frac{\\|\\tilde{L}_1 R - \\tilde{L}_2\\|_F}{\\|\\tilde{L}_2\\|_F} = \\frac{\\|\\tilde{L}_{1,a} - \\tilde{L}_2\\|_F}{\\sqrt{k}}\n$$\n\n**4. 对齐带来的改善**\n\n普氏对齐导致的差异减少告诉我们，初始差异中有多少可归因于轴的简单旋转。改善程度 $\\Delta$ 是原始差异与对齐后差异之差：\n$$\n\\Delta = d_{\\text{raw}} - d_{\\text{aligned}}\n$$\n较大的 $\\Delta$ 值表明轴之间确实存在旋转，并且对齐是有效的。接近于零的 $\\Delta$ 值表明轴要么已经很好地对齐，要么它们的几何结构存在根本不同，无法通过旋转来校正。\n\n**5. 通过相关性进行稳定性分析**\n\n在全局对齐轴系统之后，我们通过测量对齐后的源矩阵 $\\tilde{L}_{1,a}$ 和目标矩阵 $\\tilde{L}_2$ 的相应列之间的相似性来评估单个轴的稳定性。相似性由每对列之间的余弦相似度（对于中心化数据即皮尔逊相关系数）的绝对值来量化。对于第 $j$ 个轴，相关性为：\n$$\n\\rho_j = \\left| \\frac{(\\tilde{L}_{1,a})_{\\cdot j}^\\top (\\tilde{L}_2)_{\\cdot j}}{\\|(\\tilde{L}_{1,a})_{\\cdot j}\\|_2 \\|(\\tilde{L}_2)_{\\cdot j}\\|_2} \\right|\n$$\n注意，虽然 $\\tilde{L}_2$ 的列根据构造具有单位范数，但对齐矩阵 $\\tilde{L}_{1,a} = \\tilde{L}_1 R$ 的列不保证具有单位范数，因此必须计算它们的范数。\n\n整体稳定性由所有 $k$ 个轴的这些绝对相关性的算术平均值来判断：\n$$\n\\bar{\\rho} = \\frac{1}{k}\\sum_{j=1}^k \\rho_j\n$$\n如果该平均相关性达到或超过预定义的阈值 $\\tau$，则该对加载矩阵被声明为“稳定”。对于本问题，$\\tau=0.98$。\n$$\n\\text{stable} = (\\bar{\\rho} \\ge \\tau)\n$$\n\n**应用于测试用例**\n该分析应用于以 $n=6$，$k=2$ 和给定的 $L_1$ 定义的四种情况。\n\n情况A (恒等)：$L_2 = L_1$。矩阵完全相同。归一化后得到 $\\tilde{L}_1 = \\tilde{L}_2$。原始差异为 $0$。普氏对齐得到 $R=I_k$，对齐后的差异也为 $0$。因此 $\\Delta = 0$。列相关性均为 $1$，因此平均相关性为 $1$，大于等于 $0.98$。轴是稳定的。\n\n情况B (旋转)：$L_2 = L_1 Q$，其中 $Q$ 是一个旋转矩阵。$L_1$ 和 $L_2$ 的列空间相同。列归一化过程不是线性的，因此 $\\tilde{L}_2$ 不是 $\\tilde{L}_1$ 的简单旋转。这将导致非零的 $d_{\\text{raw}}$。然而，底层的几何结构得以保留。普氏分析预期会找到一个能将 $\\tilde{L}_1$ 与 $\\tilde{L}_2$ 紧密对齐的旋转 $R$，从而得到一个小的 $d_{\\text{aligned}}$ 和一个大的 $\\Delta$。最终的平均相关性应该很高，表明是稳定的。\n\n情况C (置换和符号翻转)：$L_2 = L_1 P$，其中 $P$ 是一个代表置换和符号翻转的正交矩阵。这与情况B类似，因为 $P$ 也是一个正交变换。我们预期会有一个大的初始差异，该差异通过普氏对齐得到显著减小，从而产生一个大的 $\\Delta$ 和稳定的结论。\n\n情况D (正交补)：$L_2$ 的列选自 $L_1$ 列空间的正交补空间的一组标准正交基。根据构造，$L_1$ 和 $L_2$ 的列空间是正交的。这种正交性在很大程度上通过归一化步骤得以保持。因此，$\\tilde{L}_1$ 和 $\\tilde{L}_2$ 的列几乎是正交的。任何旋转都无法对齐这两个根本不同的子空间。普氏对齐不会带来显著改善，因此 $\\Delta$ 将接近 $0$。对齐后轴之间的相关性将接近 $0$，远低于 $\\tau$，表明轴是不稳定的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the neural loading matrix stability analysis.\n    \"\"\"\n\n    def analyze_stability(L1, L2, tau):\n        \"\"\"\n        Performs the 5-step analysis for a given pair of loading matrices (L1, L2).\n        \n        Args:\n            L1 (np.ndarray): The first loading matrix (n x k).\n            L2 (np.ndarray): The second loading matrix (n x k).\n            tau (float): The stability threshold for mean absolute correlation.\n            \n        Returns:\n            list: A list containing [delta, stable], where delta is the improvement\n                  and stable is a boolean.\n        \"\"\"\n        n, k = L1.shape\n\n        # Step 1: Column-center and unit-normalize L1 and L2\n        def normalize_matrix(L):\n            mean = L.mean(axis=0)\n            centered = L - mean\n            norm = np.linalg.norm(centered, axis=0)\n            # Handle columns that might have zero norm after centering\n            tilde_L = np.divide(centered, norm, out=np.zeros_like(centered, dtype=float), where=norm!=0)\n            return tilde_L\n\n        L1_tilde = normalize_matrix(L1)\n        L2_tilde = normalize_matrix(L2)\n        \n        # Denominator for discrepancy calculations\n        # Since L2_tilde has unit-norm columns, its Frobenius norm is sqrt(k)\n        norm_L2_tilde_F = np.sqrt(k)\n\n        # Step 2: Compute the raw normalized Frobenius discrepancy\n        d_raw = np.linalg.norm(L1_tilde - L2_tilde, 'fro') / norm_L2_tilde_F\n        \n        # Step 3: Solve the orthogonal Procrustes alignment problem\n        # Find R that minimizes ||L1_tilde @ R - L2_tilde||_F\n        M = L2_tilde.T @ L1_tilde\n        U, s, Vh = linalg.svd(M)\n        R = Vh.T @ U.T\n        \n        L1_aligned = L1_tilde @ R\n        \n        # Compute the aligned discrepancy\n        d_aligned = np.linalg.norm(L1_aligned - L2_tilde, 'fro') / norm_L2_tilde_F\n\n        # Step 4: Define the improvement\n        delta = d_raw - d_aligned\n\n        # Step 5: Compute mean absolute columnwise correlation and determine stability\n        correlations = []\n        for j in range(k):\n            col1 = L1_aligned[:, j]\n            col2 = L2_tilde[:, j]\n            \n            norm1 = np.linalg.norm(col1)\n            norm2 = np.linalg.norm(col2) # This will be 1.0\n            \n            if norm1 == 0 or norm2 == 0:\n                corr = 0.0\n            else:\n                corr = np.dot(col1, col2) / (norm1 * norm2)\n            correlations.append(np.abs(corr))\n        \n        mean_abs_corr = np.mean(correlations)\n        stable = mean_abs_corr >= tau\n        \n        return [delta, stable]\n\n    # Test suite specification\n    n = 6\n    k = 2\n    tau = 0.98\n\n    L1 = np.array([\n        [0.5, -0.2],\n        [0.3, 0.1],\n        [0.0, 0.4],\n        [-0.1, 0.6],\n        [0.2, -0.3],\n        [0.7, 0.0]\n    ])\n\n    results = []\n\n    # Case A: Identity\n    L2_A = L1\n    results.append(analyze_stability(L1, L2_A, tau))\n\n    # Case B: Rotation in axis space\n    angle = np.pi / 6\n    Q = np.array([\n        [np.cos(angle), -np.sin(angle)],\n        [np.sin(angle), np.cos(angle)]\n    ])\n    L2_B = L1 @ Q\n    results.append(analyze_stability(L1, L2_B, tau))\n\n    # Case C: Permutation with sign flip\n    P = np.array([\n        [0, -1],\n        [1, 0]\n    ])\n    L2_C = L1 @ P\n    results.append(analyze_stability(L1, L2_C, tau))\n\n    # Case D: Orthogonal-complement axes\n    # Basis for the null space of L1.T, which is the orthogonal complement of colspan(L1)\n    null_space_basis = linalg.null_space(L1.T)\n    L2_D = null_space_basis[:, :k]\n    results.append(analyze_stability(L1, L2_D, tau))\n    \n    # Format the final output string\n    # Convert each sublist [delta, stable] to its string representation\n    # e.g., '[0.0, True]'\n    # Then join them with commas and wrap in brackets\n    # to get '[[0.0, True],[...]]'\n    final_output_str = f\"[{','.join(map(str, results))}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}