{
    "hands_on_practices": [
        {
            "introduction": "This practice focuses on a crucial aspect of applying GPFA to neural spike counts: the observation model. While the standard GPFA model assumes Gaussian observations, neural data often consist of discrete counts, better described by a Poisson distribution. This exercise  will guide you through deriving the gradient of the log-likelihood for a Poisson-GPFA model, a fundamental step required for fitting the model parameters using gradient-based optimization algorithms.",
            "id": "4166232",
            "problem": "Consider a standard Gaussian Process Factor Analysis (GPFA) observation model for multi-neuron spike counts. There are $p$ neurons, $q$ latent dimensions, and $T$ discrete time bins. The latent state at time $t$ is $x_t \\in \\mathbb{R}^q$, and the observed spike counts are $y_t \\in \\mathbb{N}^p$, with entries $y_{it} \\in \\{0,1,2,\\dots\\}$ for neuron $i \\in \\{1,\\dots,p\\}$ and time $t \\in \\{1,\\dots,T\\}$. Conditional on the latent states $\\{x_t\\}_{t=1}^T$, the observations are modeled as conditionally independent across neurons and time with a Poisson distribution and a log link:\n$$y_{it} \\mid x_t \\sim \\text{Poisson}\\left(\\lambda_{it}\\right), \\quad \\lambda_{it} = \\exp\\left(c_i^{\\top} x_t + d_i\\right),$$\nwhere $c_i^{\\top}$ is the $i$th row of the loading matrix $C \\in \\mathbb{R}^{p \\times q}$, and $d \\in \\mathbb{R}^p$ is a bias vector. Let $\\lambda_t = \\exp\\left(C x_t + d\\right) \\in \\mathbb{R}^p$ denote the intensity vector at time $t$, where the exponential is applied elementwise. Assume conditional independence of $y_{it}$ given $\\{x_t\\}$ across all $i$ and $t$.\n\nYou may use only fundamental definitions: the Poisson probability mass function for $y \\in \\{0,1,2,\\dots\\}$ with rate $\\lambda > 0$ is\n$$P(y \\mid \\lambda) = \\frac{\\exp(-\\lambda)\\,\\lambda^{y}}{y!}.$$\n\nTreat $\\{x_t\\}_{t=1}^T$ as fixed and known. Derive from first principles the gradient of the conditional log-likelihood $\\ell(C,d) = \\ln P\\left(\\{y_t\\}_{t=1}^T \\mid \\{x_t\\}_{t=1}^T, C, d\\right)$ with respect to $C$ and with respect to $d$, and express your result explicitly in terms of $x_t$, $y_t$, $C$, and $d$. Your final expressions must not contain any expectations or unspecified constants. Express elementwise exponentials using $\\exp(\\cdot)$ and write your answer as compact matrix expressions that sum over $t \\in \\{1,\\dots,T\\}$.\n\nNo numerical approximation is required. The final answer must be a single closed-form analytic expression. If you provide multiple components, present them together in a single row matrix.",
            "solution": "The objective is to derive the gradient of the conditional log-likelihood, $\\ell(C,d) = \\ln P\\left(\\{y_t\\}_{t=1}^T \\mid \\{x_t\\}_{t=1}^T, C, d\\right)$, with respect to the loading matrix $C \\in \\mathbb{R}^{p \\times q}$ and the bias vector $d \\in \\mathbb{R}^p$.\n\nThe observation model states that, conditional on the latent states $\\{x_t\\}_{t=1}^T$, spike counts $y_{it}$ are independent Poisson random variables with rate $\\lambda_{it} = \\exp(c_i^{\\top} x_t + d_i)$. The probability mass function for a single observation $y_{it}$ is given by\n$$P(y_{it} \\mid \\lambda_{it}) = \\frac{\\exp(-\\lambda_{it})\\,\\lambda_{it}^{y_{it}}}{y_{it}!}$$\nDue to the conditional independence assumption, the total likelihood is the product of the individual probabilities over all neurons $i \\in \\{1, \\dots, p\\}$ and time bins $t \\in \\{1, \\dots, T\\}$:\n$$P\\left(\\{y_t\\}_{t=1}^T \\mid \\{x_t\\}_{t=1}^T, C, d\\right) = \\prod_{t=1}^T \\prod_{i=1}^p P(y_{it} \\mid \\lambda_{it})$$\nThe conditional log-likelihood $\\ell(C,d)$ is the natural logarithm of this likelihood:\n$$\\ell(C,d) = \\ln \\left( \\prod_{t=1}^T \\prod_{i=1}^p \\frac{\\exp(-\\lambda_{it})\\,\\lambda_{it}^{y_{it}}}{y_{it}!} \\right) = \\sum_{t=1}^T \\sum_{i=1}^p \\ln \\left( \\frac{\\exp(-\\lambda_{it})\\,\\lambda_{it}^{y_{it}}}{y_{it}!} \\right)$$\nUsing the properties of the logarithm, this simplifies to:\n$$\\ell(C,d) = \\sum_{t=1}^T \\sum_{i=1}^p \\left( \\ln(\\exp(-\\lambda_{it})) + \\ln(\\lambda_{it}^{y_{it}}) - \\ln(y_{it}!) \\right) = \\sum_{t=1}^T \\sum_{i=1}^p \\left( -\\lambda_{it} + y_{it} \\ln(\\lambda_{it}) - \\ln(y_{it}!) \\right)$$\nWe are given the log-link function $\\lambda_{it} = \\exp(c_i^{\\top} x_t + d_i)$, which implies $\\ln(\\lambda_{it}) = c_i^{\\top} x_t + d_i$. Substituting these into the log-likelihood expression gives:\n$$\\ell(C,d) = \\sum_{t=1}^T \\sum_{i=1}^p \\left( -\\exp(c_i^{\\top} x_t + d_i) + y_{it} (c_i^{\\top} x_t + d_i) - \\ln(y_{it}!) \\right)$$\nTo find the gradients, we differentiate this expression with respect to the parameters.\n\nFirst, we compute the gradient with respect to the bias vector $d$. The gradient $\\nabla_d \\ell$ is a vector of size $p \\times 1$ whose $j$-th component is $\\frac{\\partial \\ell}{\\partial d_j}$.\n$$\\frac{\\partial \\ell}{\\partial d_j} = \\frac{\\partial}{\\partial d_j} \\sum_{t=1}^T \\sum_{i=1}^p \\left( -\\exp(c_i^{\\top} x_t + d_i) + y_{it} (c_i^{\\top} x_t + d_i) - \\ln(y_{it}!) \\right)$$\nThe derivative is non-zero only for terms where $i=j$. For all other terms where $i \\neq j$, the derivative with respect to $d_j$ is zero.\n$$\\frac{\\partial \\ell}{\\partial d_j} = \\sum_{t=1}^T \\frac{\\partial}{\\partial d_j} \\left( -\\exp(c_j^{\\top} x_t + d_j) + y_{jt} (c_j^{\\top} x_t + d_j) \\right)$$\nUsing the chain rule, $\\frac{\\partial}{\\partial u} \\exp(u) = \\exp(u)$, we get:\n$$\\frac{\\partial \\ell}{\\partial d_j} = \\sum_{t=1}^T \\left( -\\exp(c_j^{\\top} x_t + d_j) \\cdot 1 + y_{jt} \\cdot 1 \\right) = \\sum_{t=1}^T (y_{jt} - \\exp(c_j^{\\top} x_t + d_j))$$\nRecalling that $\\lambda_{jt} = \\exp(c_j^{\\top} x_t + d_j)$, we have $\\frac{\\partial \\ell}{\\partial d_j} = \\sum_{t=1}^T (y_{jt} - \\lambda_{jt})$.\nAssembling these partial derivatives into the gradient vector $\\nabla_d \\ell \\in \\mathbb{R}^p$ and using the vector notations $y_t \\in \\mathbb{R}^p$ and $\\lambda_t = \\exp(Cx_t+d) \\in \\mathbb{R}^p$, we obtain the compact matrix expression:\n$$\\nabla_d \\ell(C,d) = \\sum_{t=1}^T (y_t - \\lambda_t) = \\sum_{t=1}^T (y_t - \\exp(Cx_t + d))$$\nNext, we compute the gradient with respect to the loading matrix $C$. The gradient $\\nabla_C \\ell$ is a matrix of size $p \\times q$ whose $(j,k)$-th element is $\\frac{\\partial \\ell}{\\partial C_{jk}}$. The term $c_i^{\\top}x_t$ can be written as $\\sum_{m=1}^q C_{im}x_{mt}$.\n$$\\frac{\\partial \\ell}{\\partial C_{jk}} = \\frac{\\partial}{\\partial C_{jk}} \\sum_{t=1}^T \\sum_{i=1}^p \\left( -\\exp(c_i^{\\top} x_t + d_i) + y_{it} (c_i^{\\top} x_t + d_i) - \\ln(y_{it}!) \\right)$$\nThe derivative is non-zero only for terms where the row index $i=j$.\n$$\\frac{\\partial \\ell}{\\partial C_{jk}} = \\sum_{t=1}^T \\frac{\\partial}{\\partial C_{jk}} \\left( -\\exp(c_j^{\\top} x_t + d_j) + y_{jt} (c_j^{\\top} x_t + d_j) \\right)$$\nThe derivative of the argument $c_j^{\\top} x_t + d_j = \\sum_{m=1}^q C_{jm}x_{mt} + d_j$ with respect to $C_{jk}$ is $x_{kt}$, which is the $k$-th component of the vector $x_t$.\nApplying the chain rule:\n$$\\frac{\\partial \\ell}{\\partial C_{jk}} = \\sum_{t=1}^T \\left( (-\\exp(c_j^{\\top} x_t + d_j) + y_{jt}) \\cdot \\frac{\\partial (c_j^{\\top} x_t + d_j)}{\\partial C_{jk}} \\right) = \\sum_{t=1}^T (y_{jt} - \\exp(c_j^{\\top} x_t + d_j)) \\cdot x_{kt}$$\nSo, $(\\nabla_C \\ell)_{jk} = \\sum_{t=1}^T (y_{jt} - \\lambda_{jt}) x_{kt}$.\nThis expression for the $(j,k)$-th element of the gradient matrix corresponds to the $(j,k)$-th element of the sum of outer products $(y_t - \\lambda_t)x_t^{\\top}$ over $t$. The term $(y_t - \\lambda_t)$ is a $p \\times 1$ vector and $x_t^{\\top}$ is a $1 \\times q$ vector, so their outer product is a $p \\times q$ matrix.\nThus, the gradient matrix $\\nabla_C \\ell$ can be written compactly as:\n$$\\nabla_C \\ell(C,d) = \\sum_{t=1}^T (y_t - \\lambda_t) x_t^{\\top} = \\sum_{t=1}^T (y_t - \\exp(Cx_t + d)) x_t^{\\top}$$\nThe two gradients, $\\nabla_C \\ell$ and $\\nabla_d \\ell$, are the required results.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sum_{t=1}^{T} \\left(y_t - \\exp(Cx_t + d)\\right)x_t^{\\top} & \\sum_{t=1}^{T} \\left(y_t - \\exp(Cx_t + d)\\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A powerful model is only useful if it is computationally tractable. This exercise  shifts our focus from the statistical formulation of GPFA to its algorithmic complexity. By analyzing the computational cost of exact inference, you will discover why GPFA can be slow for long time series and derive the significant speedup offered by sparse approximations using inducing points, a critical technique for applying these models to large-scale neuroscience data.",
            "id": "4173640",
            "problem": "Consider Gaussian Process Factor Analysis (GPFA) applied to a neural population of size $N$ measured over $T$ uniformly sampled time points. The latent state at time $t$ is $x_{t} \\in \\mathbb{R}^{k}$, and the observation model is linear-Gaussian: $y_{t} = C x_{t} + d + \\varepsilon_{t}$ with $\\varepsilon_{t} \\sim \\mathcal{N}(0, R)$, where $R \\in \\mathbb{R}^{N \\times N}$ is diagonal and $C \\in \\mathbb{R}^{N \\times k}$ is a loading matrix. Each latent dimension $x^{(j)} = \\{x^{(j)}_{t}\\}_{t=1}^{T}$ is an independent sample from a zero-mean Gaussian process (GP) with covariance function $\\kappa_{j}(\\cdot, \\cdot)$, so that $x^{(j)} \\sim \\mathcal{N}(0, K_{j})$ with $K_{j} \\in \\mathbb{R}^{T \\times T}$, and the $k$ latent processes are a priori independent.\n\nAssume that observations have been noise-whitened and loadings have been orthonormalized in the whitened space so that $R = I_{N}$ and $C^{\\top} C = I_{k}$, and that all kernels $\\kappa_{j}$ are stationary and admit dense Gram matrices $K_{j}$ of full rank. You are interested in computing the posterior distribution over the latent trajectories conditioned on the data $\\{y_{t}\\}_{t=1}^{T}$ under exact GPFA inference, and also under a sparse GPFA approximation that uses $M$ inducing points per latent dimension with locations $\\{u_{m}\\}_{m=1}^{M}$ and employs a variational inducing-point construction that yields computations involving $K_{UU} \\in \\mathbb{R}^{M \\times M}$ and $K_{TU} \\in \\mathbb{R}^{T \\times M}$ for each latent.\n\nStarting only from the following base facts and definitions:\n- The posterior for a linear-Gaussian model with a Gaussian prior is Gaussian and can be obtained by solving linear systems or factoring positive definite matrices.\n- Given a positive definite matrix of size $n \\times n$, a Cholesky factorization of a positive definite $n \\times n$ matrix costs on the order of $n^{3}$ arithmetic operations.\n- For dense matrices of sizes compatible with multiplication, a matrix-matrix multiply costs on the order of the product of the dimensions.\n- For $k$ independent Gaussian processes with time covariance matrices $\\{K_{j}\\}_{j=1}^{k}$, the prior precision is block-diagonal across latent dimensions with blocks $K_{j}^{-1}$.\n\nDerive the asymptotic leading-order arithmetic cost, in terms of $k$ and $T$, of computing the exact GPFA posterior over the latent trajectories under the stated assumptions. Then, derive the asymptotic leading-order arithmetic cost, in terms of $k$, $T$, and $M$, for computing the corresponding sparse GPFA posterior using $M$ inducing points per latent dimension with dense $K_{UU}$ and $K_{TU}$, where computations are performed using standard variational sparse GP conditioning with dense linear algebra and without exploiting additional structure beyond what is stated.\n\nFinally, define the asymptotic speedup factor $S(T, M)$ as the ratio of the exact cost to the sparse cost, canceling any common multiplicative factors, and ignoring constant factors and lower-order terms. Provide $S(T, M)$ as a single closed-form analytic expression in terms of $T$ and $M$ only. Do not use Big-O notation in your final expression for $S(T, M)$, and do not include units. Your final answer must be a single analytic expression.",
            "solution": "### Derivation of Computational Costs\n\nThe goal is to determine the leading-order computational cost for inferring the posterior distribution over the latent trajectories, $p(\\mathbf{x}|\\mathbf{y})$, where $\\mathbf{x} \\in \\mathbb{R}^{kT}$ represents all latent variables across all $k$ dimensions and $T$ time points, and $\\mathbf{y} \\in \\mathbb{R}^{NT}$ represents all observations. Since the prior on $\\mathbf{x}$ is Gaussian and the likelihood is linear-Gaussian, the posterior is also Gaussian. Computing the posterior is equivalent to computing its mean and covariance. This is dominated by solving a large linear system involving the posterior precision matrix.\n\nThe log-posterior is proportional to the sum of the log-prior and the log-likelihood:\n$$ \\ln p(\\mathbf{x}|\\mathbf{y}) \\propto \\ln p(\\mathbf{x}) + \\ln p(\\mathbf{y}|\\mathbf{x}) $$\nThe log-prior, given $k$ independent latent processes $x^{(j)} \\sim \\mathcal{N}(0, K_j)$, is:\n$$ \\ln p(\\mathbf{x}) = -\\frac{1}{2} \\sum_{j=1}^{k} (x^{(j)})^{\\top} K_{j}^{-1} x^{(j)} + \\text{const.} $$\nThe log-likelihood, given the observation model $y_{t} = C x_{t} + d + \\varepsilon_{t}$ with $\\varepsilon_t \\sim \\mathcal{N}(0, I_N)$, is:\n$$ \\ln p(\\mathbf{y}|\\mathbf{x}) = \\sum_{t=1}^{T} \\ln \\mathcal{N}(y_t | C x_t + d, I_N) = -\\frac{1}{2} \\sum_{t=1}^{T} \\|y_t - d - C x_t\\|^2_2 + \\text{const.} $$\nThe quadratic term in $x_t$ within the log-likelihood is $-\\frac{1}{2} \\sum_{t=1}^{T} x_t^{\\top} C^{\\top} C x_t$. Using the given assumption $C^{\\top} C = I_k$, this simplifies to:\n$$ -\\frac{1}{2} \\sum_{t=1}^{T} x_t^{\\top} I_k x_t = -\\frac{1}{2} \\sum_{t=1}^{T} x_t^{\\top} x_t $$\nThis sum can be rewritten in terms of the trajectories $x^{(j)}$:\n$$ \\sum_{t=1}^{T} x_t^{\\top} x_t = \\sum_{t=1}^{T} \\sum_{j=1}^{k} (x_t^{(j)})^2 = \\sum_{j=1}^{k} \\sum_{t=1}^{T} (x_t^{(j)})^2 = \\sum_{j=1}^{k} (x^{(j)})^{\\top} x^{(j)} $$\nThe Hessian of the negative log-posterior with respect to $\\mathbf{x}$ gives the posterior precision matrix, $\\Sigma_{\\text{post}}^{-1}$. The quadratic part of the negative log-posterior is:\n$$ \\frac{1}{2} \\sum_{j=1}^{k} (x^{(j)})^{\\top} K_{j}^{-1} x^{(j)} + \\frac{1}{2} \\sum_{j=1}^{k} (x^{(j)})^{\\top} I_T x^{(j)} = \\frac{1}{2} \\sum_{j=1}^{k} (x^{(j)})^{\\top} (K_{j}^{-1} + I_T) x^{(j)} $$\nThis reveals that the posterior precision matrix is block-diagonal, with $k$ blocks of size $T \\times T$. The problem decouples into $k$ independent posterior inference problems, one for each latent dimension $j \\in \\{1, \\dots, k\\}$. The posterior precision for each $x^{(j)}$ is $K_{j}^{-1} + I_T$.\n\n**1. Cost of Exact GPFA**\n\nFor each latent dimension $j$, the main computational task is to handle the $T \\times T$ posterior precision matrix. The posterior covariance is $(K_{j}^{-1} + I_T)^{-1}$. Using the Woodbury matrix identity, this is equivalent to $K_j(K_j+I_T)^{-1}$. The dominant computational step is the inversion or factorization of the dense $T \\times T$ matrix $G_j = K_j + I_T$.\nAs per the problem statement, we use Cholesky factorization for this, which costs $\\mathcal{O}(T^3)$ for a single $T \\times T$ matrix. Since this must be done for each of the $k$ independent latent dimensions, the total cost is:\n$$ C_{\\text{exact}} = k \\times \\mathcal{O}(T^3) = \\mathcal{O}(kT^3) $$\nThe leading-order term for the exact computation cost is therefore proportional to $kT^3$.\n\n**2. Cost of Sparse GPFA**\n\nIn the sparse GPFA approximation with $M$ inducing points per latent, the computations also decouple across the $k$ latent dimensions due to the $C^{\\top}C=I_k$ assumption. We analyze the cost for a single latent dimension. The variational sparse GP method avoids direct operations on $T \\times T$ matrices by introducing $M$ inducing variables. The core of the computation involves finding the variational posterior over these $M$ variables.\n\nThe cost is dominated by operations involving the $M \\times M$ covariance matrix of inducing points, $K_{UU}$, and the $T \\times M$ cross-covariance matrix, $K_{TU}$. In the standard variational formulation (e.g., Titsias, 2009; Hensman et al., 2013), the posterior updates require computing a matrix of the form $B = K_{UU} + K_{UT} K_{TU}$ (assuming unit observation noise variance, which corresponds to our $I_T$ term in the exact case). Let's analyze the cost of forming and inverting or factorizing $B$.\n- The matrix $K_{UU}$ is $M \\times M$.\n- The matrix $K_{TU}$ is $T \\times M$, and $K_{UT}$ is $M \\times T$.\n- The product $P = K_{UT} K_{TU}$ is an $M \\times M$ matrix. The cost of this matrix-matrix multiplication is $\\mathcal{O}(M \\times T \\times M) = \\mathcal{O}(M^2 T)$.\n- Forming $B = K_{UU} + P$ is an addition of two $M \\times M$ matrices, costing $\\mathcal{O}(M^2)$.\n- The Cholesky factorization of the $M \\times M$ matrix $B$ costs $\\mathcal{O}(M^3)$.\n\nThe total cost for a single latent dimension is dominated by the most expensive of these steps. Therefore, the cost per latent is $\\mathcal{O}(M^2 T + M^3)$.\nSince this computation is performed independently for each of the $k$ latent dimensions, the total cost for the sparse approximation is:\n$$ C_{\\text{sparse}} = k \\times \\mathcal{O}(M^2 T + M^3) = \\mathcal{O}(k M^2 T + k M^3) $$\nThe leading-order term for the sparse computation cost is proportional to $k(M^2 T + M^3)$.\n\n**3. Asymptotic Speedup Factor $S(T, M)$**\n\nThe speedup factor $S(T, M)$ is defined as the ratio of the exact cost to the sparse cost, ignoring constants and canceling common factors.\n$$ S(T, M) = \\frac{\\text{Leading-Order Term of } C_{\\text{exact}}}{\\text{Leading-Order Term of } C_{\\text{sparse}}} $$\nSubstituting the derived cost terms:\n$$ S(T, M) = \\frac{k T^3}{k (M^2 T + M^3)} $$\nWe cancel the common multiplicative factor $k$:\n$$ S(T, M) = \\frac{T^3}{M^2 T + M^3} $$\nFactoring out $M^2$ from the denominator gives the final expression:\n$$ S(T, M) = \\frac{T^3}{M^2(T + M)} $$\nThis is the required single closed-form analytic expression for the speedup factor in terms of $T$ and $M$.",
            "answer": "$$\\boxed{\\frac{T^3}{M^2(T + M)}}$$"
        },
        {
            "introduction": "The final step in mastering a model is often to implement it from first principles. This practice  provides a hands-on coding challenge to build and verify the core inference engine of GPFA. You will implement two mathematically equivalent routes for computing the posterior distribution over latent variables, reinforcing your understanding of Gaussian conditioning and providing a robust, unit-tested foundation for a full GPFA implementation.",
            "id": "4166080",
            "problem": "Consider Gaussian Process Factor Analysis (GPFA) under a linear Gaussian observation model. Let latent trajectories be modeled as independent Gaussian processes and observations be linear mixtures of the latent states with additive Gaussian noise. Specifically, for time points $t_1,\\dots,t_T$, let the latent state at time $t_i$ be $x(t_i) \\in \\mathbb{R}^k$, and observations be $y(t_i) \\in \\mathbb{R}^p$ such that $y(t_i) = C x(t_i) + d + \\varepsilon(t_i)$, where $C \\in \\mathbb{R}^{p \\times k}$ is a loading matrix, $d \\in \\mathbb{R}^p$ is an offset vector, and $\\varepsilon(t_i) \\sim \\mathcal{N}(0, \\sigma_n^2 I_p)$ with $\\sigma_n^2 > 0$.\n\nStarting from the definition of a Gaussian process, the construction of the joint multivariate normal distribution over the stacked latent variables and observations, and the conditioning rule for a jointly Gaussian vector, derive two mathematically equivalent posterior conditioning routes for $p(x \\mid y)$: one that conditions in latent space using precision operators and one that marginalizes into observation space before conditioning. Then, design unit tests that numerically verify the equivalence of these two routes by comparing posterior means and covariances computed by each route on several scientifically plausible parameter settings.\n\nYou must implement a complete, runnable program that:\n- Constructs the Gaussian process prior covariance over the stacked latent variables using the squared exponential (radial basis function) kernel $k(t, s) = \\sigma_f^2 \\exp\\!\\big(-\\tfrac{1}{2} (t - s)^2 / \\ell^2\\big)$ for each latent dimension, with possibly distinct hyperparameters $(\\sigma_f^2, \\ell)$ per latent.\n- Builds the linear observation operator that maps the stacked latent vector to the stacked observation vector across all times.\n- Simulates latent trajectories and observations from the specified model to ensure internal consistency of the tests.\n- Computes the posterior mean and covariance via both routes implied strictly by Gaussian conditioning.\n- Compares the two posterior results and returns a boolean per test indicating whether both the posterior mean and covariance agree to within a specified absolute tolerance.\n\nUse the following test suite, where each test specifies $(k, p, T)$, the time points, per-latent kernel hyperparameters $(\\sigma_f^2, \\ell)$, the loading matrix $C$, the observation noise variance $\\sigma_n^2$, and a random seed for simulation reproducibility. All offsets $d$ are set to $0$.\n\nTest $1$ (general, well-conditioned case):\n- $k = 2$, $p = 3$, $T = 5$, $t = [0, 1, 2, 3, 4]$.\n- Latent $1$: $\\sigma_f^2 = 1.0$, $\\ell = 1.2$; Latent $2$: $\\sigma_f^2 = 0.5$, $\\ell = 0.7$.\n- $C = \\begin{bmatrix} 1.0 & -0.5 \\\\ 0.2 & 0.8 \\\\ -0.3 & 0.1 \\end{bmatrix}$.\n- $\\sigma_n^2 = 0.1$, seed $= 123$.\n\nTest $2$ (near noise-free boundary):\n- $k = 3$, $p = 2$, $T = 4$, $t = [0.0, 0.5, 1.5, 2.2]$.\n- Latent $1$: $\\sigma_f^2 = 1.0$, $\\ell = 0.9$; Latent $2$: $\\sigma_f^2 = 1.0$, $\\ell = 1.1$; Latent $3$: $\\sigma_f^2 = 1.0$, $\\ell = 0.5$.\n- $C$ is fixed by the random seed and drawn from a standard normal distribution, then scaled by $0.5$ to improve conditioning, with seed $= 7$.\n- $\\sigma_n^2 = 10^{-6}$, seed $= 7$ for data generation.\n\nTest $3$ (high noise boundary):\n- $k = 1$, $p = 5$, $T = 6$, $t = [0, 1, 2, 3, 4, 5]$.\n- Latent $1$: $\\sigma_f^2 = 2.0$, $\\ell = 1.0$.\n- $C$ is fixed by the random seed and drawn from a standard normal distribution, with seed $= 42$.\n- $\\sigma_n^2 = 100.0$, seed $= 42$ for data generation.\n\nTest $4$ (scalar latent and observation):\n- $k = 1$, $p = 1$, $T = 3$, $t = [0.0, 0.3, 0.9]$.\n- Latent $1$: $\\sigma_f^2 = 1.5$, $\\ell = 0.4$.\n- $C = [ [1.0] ]$.\n- $\\sigma_n^2 = 0.05$, seed $= 202$.\n\nTest $5$ (multi-latent, irregular times, well-conditioned square loading):\n- $k = 4$, $p = 4$, $T = 3$, $t = [0.0, 0.2, 0.7]$.\n- Latent $1$: $\\sigma_f^2 = 0.7$, $\\ell = 0.3$; Latent $2$: $\\sigma_f^2 = 1.1$, $\\ell = 0.6$; Latent $3$: $\\sigma_f^2 = 0.9$, $\\ell = 0.9$; Latent $4$: $\\sigma_f^2 = 0.5$, $\\ell = 0.4$.\n- $C$ is constructed by taking a random $4 \\times 4$ matrix drawn from a standard normal distribution (seed $= 99$), computing a $\\mathrm{QR}$ factorization to obtain an orthonormal matrix $Q$, and setting $C = Q \\operatorname{diag}(1.0, 0.5, 0.7, 1.3)$.\n- $\\sigma_n^2 = 0.3$, seed $= 99$ for data generation.\n\nTolerance for agreement:\n- The absolute tolerance for both mean and covariance comparisons is $\\tau = 5 \\times 10^{-8}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is a boolean indicating whether test $i$ passes (both posterior mean and covariance agree within the tolerance). No physical units are involved, and all parameters are dimensionless.",
            "solution": "### Mathematical Derivation of Posterior Conditioning Routes\n\n**1. Model Specification**\n\nLet the latent trajectories for $j \\in \\{1, \\dots, k\\}$ be independent Gaussian Processes (GPs), denoted as $x_j(t) \\sim \\mathcal{GP}(0, k_j(t, s))$. The prior mean is zero, and the covariance is given by the squared exponential kernel:\n$$ k_j(t, s; \\sigma_{f,j}^2, \\ell_j) = \\sigma_{f,j}^2 \\exp\\left(-\\frac{1}{2} \\frac{(t-s)^2}{\\ell_j^2}\\right) $$\nAt a set of $T$ discrete time points $\\{t_1, \\dots, t_T\\}$, the latent state is a vector $x(t_i) = [x_1(t_i), \\dots, x_k(t_i)]^T \\in \\mathbb{R}^k$. The corresponding observation $y(t_i) \\in \\mathbb{R}^p$ is generated by the linear-Gaussian model:\n$$ y(t_i) = C x(t_i) + d + \\varepsilon(t_i) $$\nwhere $C \\in \\mathbb{R}^{p \\times k}$ is the loading matrix, $d \\in \\mathbb{R}^p$ is an offset vector (given as $d=0$), and $\\varepsilon(t_i) \\sim \\mathcal{N}(0, \\sigma_n^2 I_p)$ is independent and identically distributed Gaussian noise.\n\n**2. Assembling the Joint Distribution**\n\nTo perform inference, we stack all variables across all $T$ time points:\n-   Stacked latent vector: $X = [x(t_1)^T, \\dots, x(t_T)^T]^T \\in \\mathbb{R}^{kT}$.\n-   Stacked observation vector: $Y = [y(t_1)^T, \\dots, y(t_T)^T]^T \\in \\mathbb{R}^{pT}$.\n\nThe prior distribution over the stacked latent vector $X$ is a multivariate Gaussian, $p(X) = \\mathcal{N}(X \\mid 0, K_x)$, as GPs are defined by the property that any finite collection of samples has a multivariate Gaussian distribution. The mean is a zero vector of size $kT \\times 1$. The prior covariance matrix $K_x \\in \\mathbb{R}^{kT \\times kT}$ is a block matrix composed of $T \\times T$ blocks, where each block is of size $k \\times k$. The $(i, j)$-th block is given by $\\operatorname{Cov}(x(t_i), x(t_j)^T)$. Due to the specified independence of the $k$ latent processes, this block is a diagonal matrix:\n$$ [K_x]_{i,j} = \\operatorname{Cov}(x(t_i), x(t_j)^T) = \\operatorname{diag}\\left(k_1(t_i, t_j), \\dots, k_k(t_i, t_j)\\right) $$\n\nThe observation model for the stacked vectors can be written as $p(Y \\mid X) = \\mathcal{N}(Y \\mid C_{full}X, R_y)$, where:\n-   $C_{full} = \\operatorname{blkdiag}(C, \\dots, C)$ is a $(pT) \\times (kT)$ block-diagonal matrix with $T$ instances of $C$ on its main diagonal.\n-   $R_y = \\sigma_n^2 I_{pT}$ is the $(pT) \\times (pT)$ observation noise covariance matrix, where $I_{pT}$ is the identity matrix of size $pT$.\n\nSince both the prior $p(X)$ and the likelihood $p(Y \\mid X)$ are Gaussian, the joint distribution $p(X, Y) = p(Y \\mid X) p(X)$ is also Gaussian. Given $X \\sim \\mathcal{N}(0, K_x)$ and $Y = C_{full}X + \\varepsilon_{full}$ with $\\varepsilon_{full} \\sim \\mathcal{N}(0, R_y)$, the parameters of the joint distribution are:\n$$\n\\begin{pmatrix} X \\\\ Y \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} K_x & K_x C_{full}^T \\\\ C_{full} K_x & C_{full} K_x C_{full}^T + R_y \\end{pmatrix} \\right)\n$$\n\n**3. Derivation of the Posterior $p(X \\mid Y)$**\n\nThe posterior distribution $p(X \\mid Y)$ is obtained by conditioning the joint Gaussian. We derive its mean $\\mu_{post}$ and covariance $\\Sigma_{post}$ via two equivalent routes.\n\n**Route 1: Conditioning in Latent Space via Precision Operators**\n\nThis approach uses Bayes' rule on the probability densities, focusing on the quadratic forms in the exponent (related to the precision or information matrix). The posterior log-density is the sum of the log-prior and log-likelihood, up to a normalization constant:\n$$ \\ln p(X \\mid Y) = \\ln p(Y \\mid X) + \\ln p(X) + \\text{const.} $$\nThe log-likelihood, ignoring constants, is:\n$$ \\ln p(Y \\mid X) \\propto -\\frac{1}{2}(Y - C_{full}X)^T R_y^{-1} (Y-C_{full}X) = -\\frac{1}{2}X^T C_{full}^T R_y^{-1} C_{full}X + X^T C_{full}^T R_y^{-1} Y - \\frac{1}{2}Y^T R_y^{-1} Y $$\nThe log-prior is:\n$$ \\ln p(X) \\propto -\\frac{1}{2} X^T K_x^{-1} X $$\nCombining terms that depend on $X$:\n$$ \\ln p(X \\mid Y) \\propto -\\frac{1}{2} X^T (K_x^{-1} + C_{full}^T R_y^{-1} C_{full}) X + X^T (C_{full}^T R_y^{-1} Y) $$\nThis expression is a quadratic form in $X$, which is characteristic of a Gaussian log-density. By completing the square, we identify the posterior precision matrix $\\Lambda_{post}$ (the inverse of the covariance matrix) as the coefficient of the quadratic term, and the posterior mean $\\mu_{post}$ from the linear term.\nThe posterior precision matrix is the sum of the prior precision and the precision contributed by the likelihood:\n$$ \\Lambda_{post,1} = K_x^{-1} + C_{full}^T R_y^{-1} C_{full} $$\nThe posterior covariance matrix is the inverse of the a posteriori precision:\n$$ \\Sigma_{post,1} = (K_x^{-1} + C_{full}^T R_y^{-1} C_{full})^{-1} $$\nThe posterior mean is given by $\\mu_{post} = \\Sigma_{post} \\times (\\text{coefficient of the linear term})$:\n$$ \\mu_{post,1} = \\Sigma_{post,1} (C_{full}^T R_y^{-1} Y) $$\nSubstituting $R_y = \\sigma_n^2 I_{pT}$, the equations become:\n$$ \\Sigma_{post,1} = \\left(K_x^{-1} + \\frac{1}{\\sigma_n^2} C_{full}^T C_{full}\\right)^{-1} $$\n$$ \\mu_{post,1} = \\Sigma_{post,1} \\left(\\frac{1}{\\sigma_n^2} C_{full}^T Y\\right) $$\nThis route requires inverting two matrices of size $(kT) \\times (kT)$: $K_x$ and $\\Lambda_{post,1}$.\n\n**Route 2: Marginalizing into Observation Space**\n\nThis approach uses the standard formulas for conditioning a partitioned Gaussian random vector, as derived for the joint distribution of $(X,Y)$. Let the partitioned covariance matrix be:\n$$ \\Sigma = \\begin{pmatrix} \\Sigma_{XX} & \\Sigma_{XY} \\\\ \\Sigma_{YX} & \\Sigma_{YY} \\end{pmatrix} = \\begin{pmatrix} K_x & K_x C_{full}^T \\\\ C_{full} K_x & C_{full} K_x C_{full}^T + R_y \\end{pmatrix} $$\nThe conditional distribution $p(X \\mid Y=y)$ is Gaussian with mean and covariance:\n$$ \\mu_{X \\mid Y=y} = \\mu_X + \\Sigma_{XY} \\Sigma_{YY}^{-1} (y - \\mu_Y) $$\n$$ \\Sigma_{X \\mid Y=y} = \\Sigma_{XX} - \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YX} $$\nWith prior means $\\mu_X = 0$ and $\\mu_Y = 0$, we substitute the corresponding blocks:\n$$ \\mu_{post,2} = (K_x C_{full}^T) (C_{full} K_x C_{full}^T + R_y)^{-1} Y $$\n$$ \\Sigma_{post,2} = K_x - (K_x C_{full}^T) (C_{full} K_x C_{full}^T + R_y)^{-1} (C_{full} K_x) $$\nSubstituting $R_y = \\sigma_n^2 I_{pT}$:\n$$ \\mu_{post,2} = K_x C_{full}^T (C_{full} K_x C_{full}^T + \\sigma_n^2 I_{pT})^{-1} Y $$\n$$ \\Sigma_{post,2} = K_x - K_x C_{full}^T (C_{full} K_x C_{full}^T + \\sigma_n^2 I_{pT})^{-1} C_{full} K_x $$\nThis route requires a single matrix inversion of size $(pT) \\times (pT)$, corresponding to the marginal observation covariance $\\Sigma_{YY}$.\n\nThe mathematical equivalence between Route $1$ and Route $2$ is a direct consequence of the Woodbury matrix identity, which provides a formula for the inverse of a sum of matrices. The numerical unit tests specified in the problem are designed to confirm this equivalence in a practical implementation.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import block_diag\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define test cases as per the problem description.\n    test_cases = [\n        {\n            \"name\": \"Test 1\",\n            \"k\": 2, \"p\": 3, \"T\": 5, \"times\": np.array([0., 1., 2., 3., 4.]),\n            \"kernel_params\": [(1.0, 1.2), (0.5, 0.7)],\n            \"C\": np.array([[1.0, -0.5], [0.2, 0.8], [-0.3, 0.1]]),\n            \"sigma_n2\": 0.1,\n            \"seed\": 123\n        },\n        {\n            \"name\": \"Test 2\",\n            \"k\": 3, \"p\": 2, \"T\": 4, \"times\": np.array([0.0, 0.5, 1.5, 2.2]),\n            \"kernel_params\": [(1.0, 0.9), (1.0, 1.1), (1.0, 0.5)],\n            \"C\": \"random\", \"C_seed\": 7,\n            \"sigma_n2\": 1e-6,\n            \"seed\": 7\n        },\n        {\n            \"name\": \"Test 3\",\n            \"k\": 1, \"p\": 5, \"T\": 6, \"times\": np.array([0., 1., 2., 3., 4., 5.]),\n            \"kernel_params\": [(2.0, 1.0)],\n            \"C\": \"random\", \"C_seed\": 42,\n            \"sigma_n2\": 100.0,\n            \"seed\": 42\n        },\n        {\n            \"name\": \"Test 4\",\n            \"k\": 1, \"p\": 1, \"T\": 3, \"times\": np.array([0.0, 0.3, 0.9]),\n            \"kernel_params\": [(1.5, 0.4)],\n            \"C\": np.array([[1.0]]),\n            \"sigma_n2\": 0.05,\n            \"seed\": 202\n        },\n        {\n            \"name\": \"Test 5\",\n            \"k\": 4, \"p\": 4, \"T\": 3, \"times\": np.array([0.0, 0.2, 0.7]),\n            \"kernel_params\": [(0.7, 0.3), (1.1, 0.6), (0.9, 0.9), (0.5, 0.4)],\n            \"C\": \"qr_random\", \"C_seed\": 99,\n            \"sigma_n2\": 0.3,\n            \"seed\": 99\n        }\n    ]\n\n    tolerance = 5e-8\n    results = []\n\n    for case in test_cases:\n        test_result = run_test(case, tolerance)\n        results.append(test_result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef run_test(case_params, tolerance):\n    \"\"\"\n    Executes a single GPFA test case.\n    \"\"\"\n    k, p, T = case_params[\"k\"], case_params[\"p\"], case_params[\"T\"]\n    times = case_params[\"times\"]\n    kernel_params = case_params[\"kernel_params\"]\n    sigma_n2 = case_params[\"sigma_n2\"]\n    seed = case_params[\"seed\"]\n\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct or generate loading matrix C\n    if isinstance(case_params[\"C\"], str):\n        c_mode = case_params[\"C\"]\n        c_seed = case_params[\"C_seed\"]\n        # Use a single reproducible stream for the whole test.\n        if c_mode == \"random\":\n            if k == 3 and p == 2: # Test 2\n                C = rng.standard_normal(size=(p, k)) * 0.5\n            else: # Test 3\n                C = rng.standard_normal(size=(p, k))\n        elif c_mode == \"qr_random\":\n            A = rng.standard_normal(size=(p, k))\n            Q, _ = np.linalg.qr(A)\n            D_vals = np.array([1.0, 0.5, 0.7, 1.3])\n            C = Q @ np.diag(D_vals)\n    else:\n        C = case_params[\"C\"]\n\n    # 2. Build prior covariance Kx and full loading matrix C_full\n    Kx = build_Kx(k, T, times, kernel_params)\n    C_full = block_diag(*([C] * T))\n\n    # 3. Simulate latent and observed data\n    X_true = rng.multivariate_normal(np.zeros(k * T), Kx)\n    noise = rng.normal(0, np.sqrt(sigma_n2), size=p * T)\n    Y_observed = C_full @ X_true + noise\n\n    # 4. Compute posterior via Route 1 (Latent Space / Precision)\n    Kx_inv = np.linalg.inv(Kx)\n    Lambda_post1 = Kx_inv + (1 / sigma_n2) * C_full.T @ C_full\n    Sigma_post1 = np.linalg.inv(Lambda_post1)\n    mu_post1 = Sigma_post1 @ ((1 / sigma_n2) * C_full.T @ Y_observed)\n\n    # 5. Compute posterior via Route 2 (Observation Space / Covariance)\n    Ky = C_full @ Kx @ C_full.T + sigma_n2 * np.eye(p * T)\n    Ky_inv = np.linalg.inv(Ky)\n    \n    # Pre-calculate to avoid recomputing Kx @ C_full.T\n    Kx_CT = Kx @ C_full.T\n\n    mu_post2 = Kx_CT @ Ky_inv @ Y_observed\n    Sigma_post2 = Kx - Kx_CT @ Ky_inv @ C_full @ Kx\n\n    # 6. Compare results\n    mean_agree = np.allclose(mu_post1, mu_post2, atol=tolerance, rtol=0)\n    cov_agree = np.allclose(Sigma_post1, Sigma_post2, atol=tolerance, rtol=0)\n\n    return mean_agree and cov_agree\n\n\ndef squared_exponential_kernel(t1, t2, var_f, length_scale):\n    \"\"\"\n    Squared exponential (RBF) kernel function.\n    \"\"\"\n    return var_f * np.exp(-0.5 * ((t1 - t2) / length_scale)**2)\n\ndef build_Kx(k, T, times, kernel_params):\n    \"\"\"\n    Builds the prior covariance matrix Kx for the stacked latent vector X.\n    X is stacked time-first: X = [x(t1)', ..., x(tT)']'.\n    \"\"\"\n    Kx = np.zeros((k * T, k * T))\n    for i in range(T):\n        for j in range(T):\n            ti, tj = times[i], times[j]\n            # Create the (i,j)-th block, which is k x k and diagonal\n            cov_diag_block = np.zeros(k)\n            for latent_idx in range(k):\n                var_f, length_scale = kernel_params[latent_idx]\n                cov_diag_block[latent_idx] = squared_exponential_kernel(ti, tj, var_f, length_scale)\n            \n            Kx[i*k:(i+1)*k, j*k:(j+1)*k] = np.diag(cov_diag_block)\n    return Kx\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}