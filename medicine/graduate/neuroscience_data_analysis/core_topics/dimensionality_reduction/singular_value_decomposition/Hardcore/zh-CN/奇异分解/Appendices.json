{
    "hands_on_practices": [
        {
            "introduction": "奇异值分解 (SVD) 的核心是奇异值的计算。这个基础练习  将引导你通过SVD最基本的定义来手动计算一个简单 $2 \\times 2$ 矩阵的奇异值，即通过构建并求解 $A^T A$ 的特征值问题。掌握这一过程是理解SVD背后代数机制的第一步。",
            "id": "1071366",
            "problem": "计算 $2 \\times 2$ 矩阵\n\n$$\nA = \\begin{bmatrix} 1  1 \\\\ 1  0 \\end{bmatrix}.\n$$\n\n的奇异值。\n\n将奇异值表示为最简根式形式，并按降序排列。",
            "solution": "1. $A$ 的奇异值 $\\sigma_i$ 是 $A^T A$ 的特征值的平方根。\n\n2. 计算\n$$A^T A = \\begin{bmatrix}1 & 1\\\\1 & 0\\end{bmatrix}\\begin{bmatrix}1 & 1\\\\1 & 0\\end{bmatrix}\n=\\begin{bmatrix}2 & 1\\\\1 & 1\\end{bmatrix}.$$\n\n3. $A^T A$ 的特征多项式是\n$$\\det\\bigl(\\begin{bmatrix}2 & 1\\\\1 & 1\\end{bmatrix}-\\lambda I\\bigr)\n=(2-\\lambda)(1-\\lambda)-1\n=\\lambda^2-3\\lambda+1.$$\n\n4. 解方程 $\\lambda^2-3\\lambda+1=0$：\n$$\\lambda=\\frac{3\\pm\\sqrt{9-4}}{2}=\\frac{3\\pm\\sqrt5}{2}.$$\n\n5. 因此，奇异值为\n$$\\sigma_1=\\sqrt{\\frac{3+\\sqrt5}{2}},\\quad\n\\sigma_2=\\sqrt{\\frac{3-\\sqrt5}{2}},$$\n按降序排列。",
            "answer": "$$\\boxed{\\sqrt{\\frac{3+\\sqrt{5}}{2}},\\ \\sqrt{\\frac{3-\\sqrt{5}}{2}}}$$"
        },
        {
            "introduction": "理解SVD与特征值分解的关键区别对于其在数据分析中的正确应用至关重要。本练习  挑战你跳出纯粹的代数计算，从几何角度思考变换的本质。通过构建一个其奇异向量和特征向量显著不同的矩阵，你将深化对SVD如何揭示变换的主要拉伸方向（即使不存在不变方向）的理解。",
            "id": "3275104",
            "problem": "特征向量和奇异向量之间的一个核心几何区别是：特征向量捕捉的是在线性映射下在缩放意义下保持不变的方向，而奇异向量捕捉的是映射拉伸或收缩最剧烈的正交方向。仅利用这些概念基础以及特征向量和奇异值分解（SVD）的定义，判断下列哪个选项正确地将一个具体的 $2 \\times 2$ 实矩阵 $A$ 与一个有效的几何原因配对，该原因解释了为什么其奇异向量与特征向量显著不同。选择所有适用的选项。\n\nA. $A = \\begin{bmatrix} 0  -1 \\\\ 2  0 \\end{bmatrix}$。原因：$A$ 是一个旋转 $\\pi/2$ 和各向异性缩放的复合，所以 $A$ 没有实数不变方向（没有实特征向量），但其奇异向量是实数标准正交轴，与 $A$ 将单位圆映射成的椭圆的主轴对齐；因此，奇异向量与特征向量显著不同。\n\nB. $A = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}$。原因：因为 $A$ 有非零的非对角线元素，其奇异向量必定与特征向量无关；几何上，$A$ 从单位圆产生的椭圆的轴与特征方向无关，所以这两组方向不同。\n\nC. $A = \\begin{bmatrix} 3  1 \\\\ 0  1 \\end{bmatrix}$。原因：对于任何上三角矩阵，奇异向量都与特征向量重合；几何上，剪切不改变由特征方向定义的主轴，所以没有差异。\n\nD. $A = \\begin{bmatrix} 0  -1 \\\\ 1  0 \\end{bmatrix}$。原因：$A$ 是一个纯旋转 $\\pi/2$，所以它没有实特征向量，而其奇异向量构成一个实数标准正交基，因为单位圆的像是其自身；因此，奇异向量和特征向量显著不同。\n\n你的答案应指出所有且仅有那些提供了正确示例和正确几何原因的选项。",
            "solution": "该问题陈述经评估有效。它在科学上基于线性代数原理，是适定且客观的。它为进行严谨分析提供了充分信息。\n\n问题的核心在于一个 $2 \\times 2$ 实矩阵 $A$ 的特征向量和奇异向量之间的几何区别。\n矩阵 $A$ 的一个特征向量 $v$ 是一个非零向量，对于某个标量特征值 $\\lambda$，它满足方程 $Av = \\lambda v$。从几何上看，$A$ 对特征向量 $v$ 的作用是将其按因子 $\\lambda$ 拉伸或收缩，而不改变其方向。$v$ 的方向是变换 $A$ 下的一个不变子空间。对于一个实矩阵 $A$，对应于实特征值的特征向量是代表这些不变方向的实向量。\n\n矩阵 $A$ 的奇异值分解（SVD）由 $A = U\\Sigma V^T$ 给出，其中 $U$ 和 $V$ 是正交矩阵，$\\Sigma$ 是一个对角矩阵，其非负元素称为奇异值。$V$ 的列是右奇异向量，$U$ 的列是左奇异向量。从几何上看，右奇异向量 $\\{v_i\\}$ 在定义域中形成一个标准正交基，A 将其映射到值域中的一个正交向量集 $\\{u_i \\sigma_i\\}$。左奇异向量 $\\{u_i\\}$ 是这些结果向量的方向。因此，奇异向量 $v_i$ 标识了单位球面被 $A$ 映射到的椭球体的主轴。奇异值 $\\sigma_i$ 代表这些半轴的长度。右奇异向量是对称矩阵 $A^T A$ 的特征向量，左奇异向量是对称矩阵 $A A^T$ 的特征向量。作为实对称矩阵的特征向量，奇异向量总是实数，并且可以选择为标准正交的。\n\n当 $A$ 不是正规矩阵（$A^T A \\neq A A^T$）时，特征向量和奇异向量之间会出现显著差异。对于正规矩阵，特征向量构成一个标准正交集，并与奇异向量密切相关。具体来说，对于一个实对称矩阵（$A=A^T$），特征向量就是奇异向量。对于一般的非正规矩阵，其特征向量不一定是正交的，并且它们可以与奇异向量不同。此外，一个实矩阵可以有复数特征值和特征向量，这意味着它没有实数不变方向。然而，它的奇异向量总是实数。\n\n我们现在来评估每个选项。\n\n**选项 A：** $A = \\begin{bmatrix} 0  -1 \\\\ 2  0 \\end{bmatrix}$。原因：$A$ 是一个旋转 $\\pi/2$ 和各向异性缩放的复合，所以 $A$ 没有实数不变方向（没有实特征向量），但其奇异向量是实数标准正交轴，与 $A$ 将单位圆映射成的椭圆的主轴对齐；因此，奇异向量与特征向量显著不同。\n\n首先，我们分析矩阵 $A$。我们通过求解特征方程 $\\det(A - \\lambda I) = 0$ 来找到其特征值：\n$$ \\det \\begin{pmatrix} -\\lambda  -1 \\\\ 2  -\\lambda \\end{pmatrix} = (-\\lambda)(-\\lambda) - (-1)(2) = \\lambda^2 + 2 = 0 $$\n特征值为 $\\lambda = \\pm i\\sqrt{2}$。由于特征值是纯虚数，矩阵 $A$ 没有实特征向量。陈述“$A$ 没有实数不变方向”是正确的。\n\n接下来，我们寻找奇异向量。右奇异向量是 $A^T A$ 的特征向量：\n$$ A^T A = \\begin{bmatrix} 0  2 \\\\ -1  0 \\end{bmatrix} \\begin{bmatrix} 0  -1 \\\\ 2  0 \\end{bmatrix} = \\begin{bmatrix} 4  0 \\\\ 0  1 \\end{bmatrix} $$\n这个对角矩阵的特征值是 $\\sigma_1^2 = 4$ 和 $\\sigma_2^2 = 1$。对应的特征向量是标准基向量 $v_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ 和 $v_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。这些是 $A$ 的右奇异向量。它们是实数并构成一个标准正交基。\n左奇异向量是 $A A^T$ 的特征向量：\n$$ A A^T = \\begin{bmatrix} 0  -1 \\\\ 2  0 \\end{bmatrix} \\begin{bmatrix} 0  2 \\\\ -1  0 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix} $$\n特征值同样是 $1$ 和 $4$。对应的特征向量是 $u_2 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ (对于 $\\sigma_2^2=1$) 和 $u_1 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ (对于 $\\sigma_1^2=4$)。这些是左奇异向量。\n将 $A$ 描述为旋转和各向异性缩放的复合是准确的；例如，$A = \\begin{bmatrix} 1  0 \\\\ 0  2 \\end{bmatrix} \\begin{bmatrix} 0  -1 \\\\ 1  0 \\end{bmatrix}$。单位圆 $x^2+y^2=1$ 在 $A$ 作用下的像是点集 $(x', y')=(-y, 2x)$，它满足 $(x')^2 + (y'/2)^2 = 1$，这是一个半轴沿坐标轴的椭圆，与左奇异向量匹配。\n该推理完全正确。该矩阵没有实特征向量，但它有一组唯一的、实数的、标准正交的奇异向量。这是一个显著的差异。\n\n结论：**正确**。\n\n**选项 B：** $A = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}$。原因：因为 $A$ 有非零的非对角线元素，其奇异向量必定与特征向量无关；几何上，$A$ 从单位圆产生的椭圆的轴与特征方向无关，所以这两组方向不同。\n\n矩阵 $A$ 是一个实对称矩阵，即 $A=A^T$。对于任何实对称矩阵，谱定理保证其特征向量构成一个标准正交基。奇异向量也源自对称矩阵的特征向量，即 $A^TA$ 和 $AA^T$。由于 $A=A^T$，我们有 $A^TA = AA^T = A^2$。$A^2$ 的特征向量与 $A$ 的特征向量相同。因此，对于一个实对称矩阵，特征向量方向的集合与左、右奇异向量方向的集合是相同的。奇异值是特征值的绝对值。\n所给出的原因声称，因为 $A$ 有非零的非对角线元素，其特征向量和奇异向量必定无关。这在根本上是错误的。决定性因素是其对称性，而不是非对角线元素是否为零。在这种情况下，因为 $A$ 是对称的，其特征向量和奇异向量重合。没有差异。\n\n结论：**错误**。\n\n**选项 C：** $A = \\begin{bmatrix} 3  1 \\\\ 0  1 \\end{bmatrix}$。原因：对于任何上三角矩阵，奇异向量都与特征向量重合；几何上，剪切不改变由特征方向定义的主轴，所以没有差异。\n\n该原因声称“对于任何上三角矩阵，奇异向量都与特征向量重合”。这是错误的。这个性质只对正规矩阵成立。一个上三角矩阵是正规的当且仅当它是对角的。给定的矩阵 $A$ 不是对角的，也不是正规的：\n$A^T A = \\begin{bmatrix} 9  3 \\\\ 3  2 \\end{bmatrix}$\n$A A^T = \\begin{bmatrix} 10  1 \\\\ 1  1 \\end{bmatrix}$\n由于 $A^T A \\neq A A^T$，该矩阵不是正规的，其特征向量预计不会是其奇异向量。\n我们来找 $A$ 的特征向量。由于 $A$ 是三角矩阵，特征值是对角线元素：$\\lambda_1 = 3, \\lambda_2 = 1$。\n对于 $\\lambda_1 = 3$: $(A-3I)v=0 \\implies \\begin{bmatrix} 0  1 \\\\ 0  -2 \\end{bmatrix}v=0 \\implies v_1=k\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n对于 $\\lambda_2 = 1$: $(A-1I)v=0 \\implies \\begin{bmatrix} 2  1 \\\\ 0  0 \\end{bmatrix}v=0 \\implies v_2=k\\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}$。\n特征向量不是正交的。奇异向量作为对称矩阵 $A^TA$ 的特征向量，必须是正交的。因此，特征向量和奇异向量不可能是相同的。原因中的断言是错误的，并且“没有差异”的结论也是错误的。\n\n结论：**错误**。\n\n**选项 D：** $A = \\begin{bmatrix} 0  -1 \\\\ 1  0 \\end{bmatrix}$。原因：$A$ 是一个纯旋转 $\\pi/2$，所以它没有实特征向量，而其奇异向量构成一个实数标准正交基，因为单位圆的像是其自身；因此，奇异向量和特征向量显著不同。\n\n矩阵 $A$ 代表一个逆时针旋转 $\\theta = \\pi/2$。我们求其特征值：\n$$ \\det(A - \\lambda I) = \\det \\begin{pmatrix} -\\lambda  -1 \\\\ 1  -\\lambda \\end{pmatrix} = \\lambda^2 + 1 = 0 $$\n特征值为 $\\lambda = \\pm i$。由于这些是复数，$A$ 没有实特征向量。陈述“$A$ 是一个纯旋转 $\\pi/2$，所以它没有实特征向量”是正确的。\n接下来，我们考察奇异向量。矩阵 $A$ 是正交的，意味着 $A^T A = A A^T = I$。\n$$ A^T A = \\begin{bmatrix} 0  1 \\\\ -1  0 \\end{bmatrix} \\begin{bmatrix} 0  -1 \\\\ 1  0 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} = I $$\n单位矩阵 $I$ 的特征值都是 $1$。因此，$A$ 的奇异值为 $\\sigma_1 = \\sigma_2 = 1$。$I$ 的特征向量（即 $A$ 的右奇异向量）可以是 $\\mathbb{R}^2$ 中的任意一组标准正交向量。左奇异向量同理。\n提供的几何原因是“单位圆的像是其自身”。对于旋转来说，这是正确的，也正是所有奇异值都为 $1$ 的原因。因为奇异值相等，所以没有唯一的最大拉伸方向，这导致了奇异向量的非唯一性。然而，奇异向量的任何有效选择都必须是一个实数标准正交基。例如，我们可以选择标准基 $v_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, v_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。\n特征向量是复数，而奇异向量是实数。这构成了显著的差异。所提供的推理是完全有效的。\n\n结论：**正确**。",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "在神经科学数据分析中，SVD不仅是一种分解工具，更是一种揭示高维数据内在结构的强大方法。这个高级练习  将你置于一个真实的研究场景中：如何从充满噪声的神经活动数据中确定其有效维度或秩。通过对比启发式的scree plot方法和基于信息论的MDL准则，你将学会如何使用SVD进行严谨的模型选择，这是降维和特征提取的关键一步。",
            "id": "4193353",
            "problem": "一个实验室记录了一个神经活动矩阵 $X \\in \\mathbb{R}^{n \\times t}$，其中 $n$ 表示神经元的数量，$t$ 表示时间点的数量。在对每一行进行跨时间均值中心化后，对于每个 $i \\in \\{1,\\dots,n\\}$ 都有 $\\sum_{j=1}^{t} X_{ij} = 0$。设 $X$ 的奇异值分解（SVD）为 $X = U \\Sigma V^\\top$，奇异值为 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r \\ge 0$，其中 $r = \\min\\{n,t\\}$。该团队考虑通过一个秩为 $k$ 的信号加噪声模型 $X = S + E$ 来对 $X$ 进行低秩建模，其中 $S$ 的秩 $\\mathrm{rank}(S) \\le k$，$E$ 代表噪声。\n\n他们希望 (i) 指定一个碎石图（scree plot）准则来选择 $k$，并阐明其在神经数据上的局限性，以及 (ii) 在高斯噪声模型下，基于最小描述长度（MDL）原则，提出了一个有原则的信息论选择规则。假设以下基本依据：\n\n- 弗罗贝尼乌斯范数（Frobenius norm）下的最佳秩-$k$ 近似是通过截断SVD给出的，并且残差平方和等于被舍弃的奇异值的平方和。\n- 在独立同分布的高斯噪声模型 $E_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$ 下，残差的负对数似然与由 $\\sigma^2$ 缩放的残差平方和成正比，并且在最大似然估计下，方差是经验残差方差。\n- 一个两部分的MDL编码长度平衡了拟合优度（最大似然估计下的负对数似然）与一个随自由参数数量和样本大小增加而增加的模型复杂度项。\n\n哪个选项正确地指定了一个正式的碎石图准则及其在神经数据背景下的实质性局限性，并且还给出了一个用SVD和秩-$k$矩阵的基本参数计数表示的、用于选择 $k$ 的一致的MDL目标函数？\n\nA. 碎石图准则：选择 $\\hat{k}$ 作为对数奇异值序列的最大离散曲率的索引，$\\hat{k}_{\\mathrm{elbow}} = \\arg\\max_{i \\in \\{2,\\dots,r-1\\}} \\left(\\log \\sigma_{i-1} - 2 \\log \\sigma_i + \\log \\sigma_{i+1}\\right)$。局限性：当奇异值逐渐衰减时，“肘部”可能不明确；对神经记录中常见的尺度变化、异方差或时间相关噪声敏感；并且在 $n$ 或 $t$ 较小时不稳定。MDL：在 $E_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$ 条件下，通过最小化 $L(k) = \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + \\frac{1}{2}\\,k\\,(n+t-k)\\,\\log(nt) + C$ 来选择 $k$，其中 $\\mathrm{RSS}_k = \\sum_{i=k+1}^{r} \\sigma_i^2$ 且 $C$ 不依赖于 $k$。\n\nB. 碎石图准则：选择最小的 $k$ 使得 $\\sigma_k = \\bar{\\sigma}$，其中 $\\bar{\\sigma} = \\frac{1}{r}\\sum_{i=1}^{r}\\sigma_i$。局限性：如果数据已标准化则没有局限性。MDL：通过最小化 $L(k) = \\frac{nt}{2}\\log(\\mathrm{RSS}_k) + k(n+t)\\log(\\sigma_1)$ 来选择 $k$。\n\nC. 碎石图准则：选择最小的 $k$ 使得 $\\sum_{i=1}^{k}\\sigma_i^2 / \\sum_{i=1}^{r}\\sigma_i^2 \\ge 0.9$。局限性：阈值 $0.9$ 是常规性的，可能无法反映神经噪声的特性。MDL：使用一个赤池型（Akaike-type）分数 $A(k) = nt \\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + 2k$，将参数数量视为 $k$。\n\nD. 碎石图准则：选择 $k$ 作为满足 $\\sigma_{k+1} \\le \\hat{\\sigma}\\,(\\sqrt{n}+\\sqrt{t})$ 的最大索引，其中 $\\hat{\\sigma}^2 = \\mathrm{RSS}_m/(nt)$（对于某个固定的 $m$）。局限性：在各向同性噪声下没有局限性。MDL：$L(k) = \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + \\frac{k}{2}\\log(nt)$。\n\n选择唯一的最佳选项。",
            "solution": "题干陈述已经过验证，被认为是一个在统计数据分析领域中一致、适定且有科学依据的问题。问题要求为数据矩阵 $X \\in \\mathbb{R}^{n \\times t}$ 的低秩近似选择秩 $k$ 时，正确地指定一个碎石图启发式方法和一个最小描述长度（MDL）准则。我们将根据所提供的原则依次解决每个部分。\n\n第一部分：碎石图准则及其局限性\n\n碎石图按降序显示奇异值 $\\sigma_i$（或它们的平方，即协方差矩阵的特征值）。其启发式方法是在图中识别一个“肘部”，它代表了从谱的陡峭“信号”部分到平坦“噪声”部分的过渡点。秩 $k$ 被选为恰好在这个噪声基底开始之前的索引。\n\n一个识别肘部的正式方法是找到最大曲率点。对于一个离散点序列 $(i, y_i)$，其中 $y_i = \\log \\sigma_i$，曲率可以用二阶差分的幅度来近似。$\\log \\sigma_i$ 序列是递减的。一个肘部对应于曲线最凸的点。离散二阶导数由 $(\\log \\sigma_{i+1} - \\log \\sigma_i) - (\\log \\sigma_i - \\log \\sigma_{i-1}) = \\log \\sigma_{i-1} - 2\\log \\sigma_i + \\log \\sigma_{i+1}$ 给出。我们寻求最大化这个量来找到最“尖锐”的肘部。因此，一个正式的准则是 $\\hat{k}_{\\mathrm{elbow}} = \\arg\\max_{i \\in \\{2,\\dots,r-1\\}} \\left(\\log \\sigma_{i-1} - 2 \\log \\sigma_i + \\log \\sigma_{i+1}\\right)$。\n\n这个启发式方法的局限性对于神经数据尤为突出：\n1.  模糊性：神经群体活动在其奇异值谱中通常表现出类似幂律的衰减。这意味着没有尖锐的肘部，而是一种平滑、逐渐的衰减，使得 $k$ 的选择高度主观且不稳定。\n2.  噪声结构：碎石图启发式方法隐含地假设噪声 $E$ 是独立同分布的高斯噪声，从而在碎石图中产生一个平坦的“噪声基底”。神经噪声很少这么简单；它通常在时间上是相关的（例如，由于钙指示剂动力学或局部场电位 LFP 的渗透），并且在神经元之间是异方差的（一些神经元本质上比其他神经元更嘈杂）。这种结构化噪声不会有平坦的奇异值谱，从而掩盖或产生伪肘部。\n3.  采样问题：对于有限的 $n$ 和 $t$，样本奇异值只是真实潜在总体值的估计。这种估计误差可以移动肘部的位置，使得 $k$ 的选择不稳定，尤其对于小数据集。\n\n第二部分：最小描述长度（MDL）准则\n\nMDL 原则在模型拟合度和模型复杂度之间提供了一个正式的权衡。要最小化的目标函数是 $L(k) = -\\log P(X| M_k, \\hat{\\theta}_k) + \\text{Penalty}(k)$，其中第一项是在给定复杂度为 $k$ 的最佳拟合模型的情况下数据的负对数似然，而惩罚项则惩罚更复杂的模型。\n\n1.  拟合优度项：我们给定一个独立同分布的高斯噪声模型 $E_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$。给定秩为 $k$ 的信号 $S$ 和噪声方差 $\\sigma^2$ 时，数据 $X$ 的对数似然是：\n    $$ \\log P(X|S, \\sigma^2) = -\\frac{nt}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|X-S\\|_F^2 $$\n    根据 Eckart-Young-Mirsky 定理，最小化弗罗贝尼乌斯范数 $\\|X-S\\|_F^2$ 的最佳秩-$k$ 近似 $S_k$ 是通过截断 $X$ 的 SVD 得到的。残差平方和是 $\\mathrm{RSS}_k = \\|X-S_k\\|_F^2 = \\sum_{i=k+1}^r \\sigma_i^2$。对于给定的 $k$，我们需要 $\\sigma^2$ 的最大似然估计（MLE）。对对数似然函数关于 $\\sigma^2$ 求导并令其为零，得到 MLE：\n    $$ \\hat{\\sigma}_k^2 = \\frac{\\|X-S_k\\|_F^2}{nt} = \\frac{\\mathrm{RSS}_k}{nt} $$\n    将此代回对数似然函数，得到最大化的对数似然：\n    $$ \\log P(X | S_k, \\hat{\\sigma}_k^2) = -\\frac{nt}{2}\\log(2\\pi) - \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) - \\frac{nt}{2} $$\n    其负值，在舍去对 $k$ 而言是常数的项之后，给出了 MDL 准则的拟合优度部分：$\\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right)$。\n\n2.  模型复杂度项：惩罚项通常具有 $\\frac{D}{2}\\log N_s$ 的形式，其中 $D$ 是模型中的自由参数数量，$N_s$ 是数据样本的数量。\n    *   参数数量 ($D$)：$\\mathbb{R}^{n \\times t}$ 中的一个秩-$k$ 矩阵可以由 $k(n+t)$ 个参数指定（对于大小为 $n \\times k$ 和 $t \\times k$ 的因子），但这有一个 $k^2$ 维的冗余。真实的自由度数量是 $D = k(n+t) - k^2 = k(n+t-k)$。\n    *   样本数量 ($N_s$)：我们有一个 $n \\times t$ 的观测矩阵，所以 $N_s = nt$。\n    因此惩罚项是 $\\frac{1}{2} k(n+t-k) \\log(nt)$。\n\n结合这些，需要对 $k$ 最小化的 MDL 目标函数是：\n$$ L(k) = \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + \\frac{1}{2} k(n+t-k) \\log(nt) $$\n其中 $\\mathrm{RSS}_k = \\sum_{i=k+1}^{r} \\sigma_i^2$。任何不依赖于 $k$ 的项都可以被吸收到一个加性常数 $C$ 中。\n\n选项评估：\n\nA. 碎石图准则：$\\hat{k}_{\\mathrm{elbow}} = \\arg\\max_{i \\in \\{2,\\dots,r-1\\}} \\left(\\log \\sigma_{i-1} - 2 \\log \\sigma_i + \\log \\sigma_{i+1}\\right)$。这正确地将肘部形式化为对数奇异值的最大离散曲率点。局限性：正确地指出了因逐渐衰减导致的模糊性、对现实神经噪声结构（异方差性、时间相关性）的敏感性以及对小样本的不稳定性。MDL：$L(k) = \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + \\frac{1}{2}\\,k\\,(n+t-k)\\,\\log(nt) + C$。这与我们的推导完全匹配。**正确**。\n\nB. 碎石图准则：使用一个特设的规则，将 $\\sigma_k$ 与均值 $\\bar{\\sigma}$ 进行比较，这不是标准的肘部准则。局限性：错误地声称如果数据被标准化则没有局限性。标准化不能消除时间相关性或其他噪声结构。MDL：拟合项 $\\frac{nt}{2}\\log(\\mathrm{RSS}_k)$ 是不正确的，因为它省略了归一化因子 $1/(nt)$。惩罚项 $k(n+t)\\log(\\sigma_1)$ 使用了不正确的参数计数和一个不正当的缩放因子 $\\log(\\sigma_1)$。**不正确**。\n\nC. 碎石图准则：这描述了一个“解释方差百分比”（PVE）准则，而不是碎石图/肘部准则。虽然PVE是选择 $k$ 的一种方法，但它解决的是一个不同的问题（保留多少方差），而不是识别信号/噪声的过渡。MDL：提出了一个赤池型分数，而不是MDL分数。MDL中的惩罚项依赖于 $\\log(N_s)$，而在AIC中它是一个常数 $2$。它还将参数数量错误地指定为 $k$ 而不是 $k(n+t-k)$。**不正确**。\n\nD. 碎石图准则：准则 $\\sigma_{k+1} \\le \\hat{\\sigma}\\,(\\sqrt{n}+\\sqrt{t})$ 是从随机矩阵理论中推导出的随机矩阵最大奇异值的阈值，而不是碎石图肘部准则。局限性：“在各向同性噪声下没有局限性”的说法是错误的；随机矩阵理论的结果是渐近的，并且有其自身的假设和局限性。MDL：拟合项是正确的，但惩罚项 $\\frac{k}{2}\\log(nt)$ 使用了不正确的参数计数 $k$。**不正确**。\n\n基于从第一性原理出发的详尽推导，选项 A 为碎石图准则及其局限性，以及 MDL 目标函数两者都提供了正确的形式化表述。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}