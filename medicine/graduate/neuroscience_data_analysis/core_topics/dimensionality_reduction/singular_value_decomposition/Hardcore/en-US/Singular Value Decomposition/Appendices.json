{
    "hands_on_practices": [
        {
            "introduction": "Before applying Singular Value Decomposition (SVD) to complex, high-dimensional data, it is essential to master its foundational concepts. This first exercise provides a hands-on opportunity to compute the singular values of a simple $2 \\times 2$ matrix directly from their definition. By working through this calculation , you will solidify your understanding of the relationship between a matrix $A$, its transpose $A^T$, and the singular values that capture its geometric 'stretching' properties.",
            "id": "1071366",
            "problem": "Compute the singular values of the $2 \\times 2$ matrix \n\n$$\nA = \\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\end{bmatrix}.\n$$\n\nExpress the singular values in simplified radical form and list them in decreasing order.",
            "solution": "1. The singular values $\\sigma_i$ of $A$ are the square roots of the eigenvalues of $A^T A$.\n\n2. Compute \n$$A^T A = \\begin{bmatrix}1&1\\\\1&0\\end{bmatrix}\\begin{bmatrix}1&1\\\\1&0\\end{bmatrix}\n=\\begin{bmatrix}2&1\\\\1&1\\end{bmatrix}.$$\n\n3. The characteristic polynomial of $A^T A$ is\n$$\\det\\bigl(\\begin{bmatrix}2&1\\\\1&1\\end{bmatrix}-\\lambda I\\bigr)\n=(2-\\lambda)(1-\\lambda)-1\n=\\lambda^2-3\\lambda+1.$$\n\n4. Solve $\\lambda^2-3\\lambda+1=0$:\n$$\\lambda=\\frac{3\\pm\\sqrt{9-4}}{2}=\\frac{3\\pm\\sqrt5}{2}.$$\n\n5. Thus the singular values are\n$$\\sigma_1=\\sqrt{\\frac{3+\\sqrt5}{2}},\\quad\n\\sigma_2=\\sqrt{\\frac{3-\\sqrt5}{2}},$$\nlisted in decreasing order.",
            "answer": "$$\\boxed{\\sqrt{\\frac{3+\\sqrt{5}}{2}},\\ \\sqrt{\\frac{3-\\sqrt{5}}{2}}}$$"
        },
        {
            "introduction": "A frequent point of confusion is the distinction between a matrix's eigenvectors and its singular vectors. While eigenvectors describe directions that are left invariant (up to scaling) by a transformation, singular vectors reveal the principal axes of that transformation. This conceptual practice  challenges you to explore this crucial geometric difference, which is fundamental to correctly interpreting the components derived from SVD in data analysis, such as the principal components in Principal Component Analysis (PCA).",
            "id": "3275104",
            "problem": "A central difference between eigenvectors and singular vectors is geometric: eigenvectors capture directions that are invariant up to scaling under a linear map, whereas singular vectors capture orthogonal directions along which the map stretches or contracts the most. Using only these conceptual bases and the definitions of eigenvectors and singular value decomposition (SVD), decide which of the following options correctly pair a concrete real $2 \\times 2$ matrix $A$ with a valid geometric reason why its singular vectors are dramatically different from its eigenvectors. Select all that apply.\n\nA. $A = \\begin{bmatrix} 0 & -1 \\\\ 2 & 0 \\end{bmatrix}$. Reason: $A$ is a composition of a rotation by $ \\pi/2 $ and anisotropic scaling, so $A$ has no real invariant directions (no real eigenvectors), yet its singular vectors are real orthonormal axes aligned with the principal axes of the ellipse $A$ maps the unit circle onto; therefore, the singular vectors are dramatically different from the eigenvectors.\n\nB. $A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$. Reason: Because $A$ has nonzero off-diagonal entries, its singular vectors must be unrelated to its eigenvectors; geometrically, the ellipse $A$ produces from the unit circle has axes unrelated to the eigen-directions, so the two sets of directions diverge.\n\nC. $A = \\begin{bmatrix} 3 & 1 \\\\ 0 & 1 \\end{bmatrix}$. Reason: For any upper-triangular matrix, the singular vectors coincide with the eigenvectors; geometrically, the shear does not change the principal axes defined by the eigen-directions, so there is no divergence.\n\nD. $A = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$. Reason: $A$ is a pure rotation by $ \\pi/2 $, so it has no real eigenvectors, whereas its singular vectors form a real orthonormal basis because the image of the unit circle is itself; thus, the singular vectors and eigenvectors are dramatically different.\n\nYour answer should identify all and only the option(s) that provide both a correct example and a correct geometric reason.",
            "solution": "The core of this problem is the geometric difference between eigenvectors (invariant directions) and singular vectors (principal axes of transformation). An eigenvector $v$ of a real matrix $A$ satisfies $Av = \\lambda v$, meaning its direction is unchanged. A real matrix may have no real eigenvectors (if its eigenvalues are complex). The right singular vectors $\\{v_i\\}$ are always a real orthonormal basis for the input space that gets mapped to the orthogonal axes of the output ellipsoid. They are the eigenvectors of the symmetric matrix $A^T A$, and are thus always real and orthogonal. A significant difference arises when $A$ is not normal ($A^T A \\neq A A^T$).\n\nLet's analyze each option:\n\n**A. $A = \\begin{bmatrix} 0 & -1 \\\\ 2 & 0 \\end{bmatrix}$**:\nThe characteristic equation is $\\det(A - \\lambda I) = \\lambda^2 + 2 = 0$, giving complex eigenvalues $\\lambda = \\pm i\\sqrt{2}$. Thus, $A$ has no real eigenvectors (no real invariant directions).\nHowever, $A^T A = \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix}$, which is diagonal. Its eigenvectors (the right singular vectors of $A$) are the standard basis vectors $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\nSince the eigenvectors are complex but the singular vectors are a real orthonormal basis, they are dramatically different. The reasoning is correct.\n\n**B. $A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$**:\nThis matrix is real and symmetric ($A = A^T$). For any real symmetric matrix, the eigenvectors form an orthonormal basis and are identical to the singular vectors (left and right). The reasoning given in the option is therefore false.\n\n**C. $A = \\begin{bmatrix} 3 & 1 \\\\ 0 & 1 \\end{bmatrix}$**:\nThis matrix is upper-triangular but not normal ($A^T A \\neq A A^T$). Its eigenvectors, $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $\\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}$, are not orthogonal. Its singular vectors, being eigenvectors of $A^T A$, must be orthogonal. Therefore, the eigenvectors and singular vectors are different. The reason given (\"singular vectors coincide with the eigenvectors\") is false.\n\n**D. $A = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$**:\nThis matrix represents a pure rotation. Its eigenvalues are $\\lambda = \\pm i$, so it has no real eigenvectors.\nThe matrix is orthogonal, so $A^T A = I$. The eigenvalues of $I$ are both 1, making the singular values $\\sigma_1 = \\sigma_2 = 1$. The singular vectors are not uniquely defined, but any choice (e.g., the standard basis) forms a real orthonormal basis.\nThe eigenvectors are complex, while the singular vectors are real. This is a dramatic difference. The reasoning is correct.\n\nBoth options A and D provide correct examples with valid geometric reasoning.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "Applying SVD for dimensionality reduction in neuroscience raises a critical practical question: how many dimensions, or singular values, should we retain? This advanced practice  immerses you in a realistic neural data analysis scenario, moving beyond simple heuristics like the 'elbow' of a scree plot. You will evaluate a principled, information-theoretic method for selecting the optimal rank $k$, balancing model fit against complexity to identify the true underlying structure in noisy neural signals.",
            "id": "4193353",
            "problem": "A laboratory records a matrix of neural activity $X \\in \\mathbb{R}^{n \\times t}$, where $n$ denotes the number of neurons and $t$ denotes the number of time points, after mean-centering each row across time so that $\\sum_{j=1}^{t} X_{ij} = 0$ for each $i \\in \\{1,\\dots,n\\}$. Let the singular value decomposition (SVD) of $X$ be $X = U \\Sigma V^\\top$, with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r \\ge 0$, where $r = \\min\\{n,t\\}$. The team considers low-rank modeling of $X$ via a rank-$k$ signal plus noise model $X = S + E$, where $S$ has $\\mathrm{rank}(S) \\le k$ and $E$ represents noise.\n\nThey wish to (i) specify a scree plot criterion to select $k$ and articulate its limitations for neural data, and (ii) propose a principled information-theoretic selection rule, grounded in the Minimum Description Length (MDL) principle, under a Gaussian noise model. Assume the following fundamental bases:\n\n- The best rank-$k$ approximation in Frobenius norm is given by truncating the SVD, and the residual sum of squares equals the sum of squared discarded singular values.\n- Under an independent and identically distributed Gaussian noise model, $E_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$, the negative log-likelihood of the residuals is proportional to the residual sum of squares scaled by $\\sigma^2$, and at the maximum likelihood estimate the variance is the empirical residual variance.\n- A two-part MDL code length balances goodness of fit (negative log-likelihood at the maximum likelihood estimate) with a model complexity term that grows with the number of free parameters and the sample size.\n\nWhich option correctly specifies a formal scree plot criterion and its substantive limitations in neural data settings, and also gives a consistent MDL objective for selecting $k$ in terms of the SVD and basic parameter counting for rank-$k$ matrices?\n\nA. Scree criterion: choose $\\hat{k}$ as the index of the maximum discrete curvature of the log-singular-value sequence, $\\hat{k}_{\\mathrm{elbow}} = \\arg\\max_{i \\in \\{2,\\dots,r-1\\}} \\left(\\log \\sigma_{i-1} - 2 \\log \\sigma_i + \\log \\sigma_{i+1}\\right)$. Limitations: the elbow can be ambiguous when singular values decay gradually, sensitive to scaling and heteroscedastic or temporally correlated noise common in neural recordings, and unstable in small $n$ or $t$. MDL: under $E_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$, select $k$ by minimizing $L(k) = \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + \\frac{1}{2}\\,k\\,(n+t-k)\\,\\log(nt) + C$, where $\\mathrm{RSS}_k = \\sum_{i=k+1}^{r} \\sigma_i^2$ and $C$ does not depend on $k$.\n\nB. Scree criterion: choose the smallest $k$ such that $\\sigma_k = \\bar{\\sigma}$, where $\\bar{\\sigma} = \\frac{1}{r}\\sum_{i=1}^{r}\\sigma_i$. Limitations: none provided the data are standardized. MDL: select $k$ by minimizing $L(k) = \\frac{nt}{2}\\log(\\mathrm{RSS}_k) + k(n+t)\\log(\\sigma_1)$.\n\nC. Scree criterion: choose the smallest $k$ such that $\\sum_{i=1}^{k}\\sigma_i^2 / \\sum_{i=1}^{r}\\sigma_i^2 \\ge 0.9$. Limitations: the threshold $0.9$ is conventional and may not reflect neural noise properties. MDL: use an Akaike-type score $A(k) = nt \\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + 2k$, treating the number of parameters as $k$.\n\nD. Scree criterion: choose $k$ as the largest index for which $\\sigma_{k+1} \\le \\hat{\\sigma}\\,(\\sqrt{n}+\\sqrt{t})$, where $\\hat{\\sigma}^2 = \\mathrm{RSS}_m/(nt)$ for some fixed $m$. Limitations: none under isotropic noise. MDL: $L(k) = \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + \\frac{k}{2}\\log(nt)$.\n\nSelect the single best option.",
            "solution": "The problem asks for the correct specification of a scree plot heuristic and a Minimum Description Length (MDL) criterion for selecting the rank $k$ of a low-rank approximation to a data matrix $X \\in \\mathbb{R}^{n \\times t}$. We will analyze the components of each option based on established statistical principles.\n\n**Part 1: Scree Plot Criterion and Limitations**\n\nA scree plot displays singular values ($\\sigma_i$) to identify an \"elbow\" where the spectrum transitions from signal to a noise floor.\n*   **Formalization:** The elbow corresponds to the point of maximum curvature in the plot of $\\log(\\sigma_i)$ versus $i$. For a discrete sequence, this can be approximated by maximizing the second difference: $\\hat{k}_{\\mathrm{elbow}} = \\arg\\max_{i} (\\log \\sigma_{i-1} - 2\\log \\sigma_i + \\log \\sigma_{i+1})$. Option A correctly identifies this formalization.\n*   **Limitations:** In neuroscience, the singular value spectrum often follows a power-law decay, leading to an ambiguous elbow. Furthermore, the assumption of a flat noise floor is violated by the structured (temporally correlated, heteroscedastic) noise common in neural recordings. The method is also unstable for small sample sizes. Option A correctly lists these substantive limitations.\n\n**Part 2: Minimum Description Length (MDL) Criterion**\n\nThe MDL principle seeks to minimize a code length, which balances model fit (negative log-likelihood) and model complexity (a penalty term).\n$L(k) = -\\log P(X| M_k, \\hat{\\theta}_k) + \\text{Penalty}(k)$.\n\n*   **Goodness-of-Fit Term:** For an i.i.d. Gaussian noise model $E_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$, the maximized log-likelihood term (after estimating $\\sigma^2$ from the residuals) simplifies to $\\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right)$, plus constants that do not depend on $k$. Here, $\\mathrm{RSS}_k = \\sum_{i=k+1}^r \\sigma_i^2$ is the residual sum of squares for the optimal rank-$k$ approximation.\n*   **Model Complexity Term:** The penalty term is typically $\\frac{D}{2}\\log N_s$, where $D$ is the number of free parameters and $N_s$ is the number of data samples.\n    *   The number of degrees of freedom ($D$) for a rank-$k$ matrix in $\\mathbb{R}^{n \\times t}$ is $k(n+t-k)$.\n    *   The number of data samples ($N_s$) is $nt$.\n    *   Therefore, the penalty term is $\\frac{1}{2} k(n+t-k) \\log(nt)$.\n\nCombining these gives the MDL objective function: $L(k) = \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + \\frac{1}{2} k(n+t-k) \\log(nt)$.\n\n**Evaluation of Options**\n\n*   **A:** The scree criterion, its limitations, and the MDL formula all match our derivation. This option is correct.\n*   **B:** The scree criterion is ad-hoc, the limitations are dismissed incorrectly, and the MDL formula is incorrect in both its fit and penalty terms.\n*   **C:** This uses a \"percent variance explained\" criterion (not a scree plot) and an AIC-like penalty with an incorrect parameter count ($k$ instead of $k(n+t-k)$).\n*   **D:** This uses a criterion from random matrix theory (not a scree plot) and an MDL penalty with an incorrect parameter count ($k$).\n\nTherefore, Option A is the only one that correctly specifies all components as requested.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}