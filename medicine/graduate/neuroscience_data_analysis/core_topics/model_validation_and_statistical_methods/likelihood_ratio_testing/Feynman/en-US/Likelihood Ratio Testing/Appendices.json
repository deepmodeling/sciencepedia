{
    "hands_on_practices": [
        {
            "introduction": "To build a strong intuition for the Likelihood Ratio (LR) test, it is invaluable to see how this general principle connects to familiar statistical methods. This first exercise guides you through the process of deriving the well-known two-sample $t$-test from the fundamental principles of likelihood theory. By demonstrating that the LR statistic is a monotonic function of the $t$-statistic, you will see how the powerful and abstract LR framework unifies and provides a theoretical foundation for classical hypothesis tests .",
            "id": "4174050",
            "problem": "A cognitive electrophysiology lab is analyzing trial-averaged spike counts recorded from a single neuron under two task conditions, denoted condition A and condition B. On each trial, spike counts are accumulated over a fixed $100$ ms analysis window. For condition A, there are $n_{1}$ trials with counts $\\{x_{i}\\}_{i=1}^{n_{1}}$. For condition B, there are $n_{2}$ trials with counts $\\{y_{j}\\}_{j=1}^{n_{2}}$. By standard arguments from the Central Limit Theorem (CLT), the within-condition trial-to-trial variability in spike counts is well-approximated by a normal distribution. Assume the two conditions have independent samples, condition-specific means, and a common unknown variance. Formally, assume $x_{i} \\sim \\mathcal{N}(\\mu_{1},\\sigma^{2})$ independently for $i=1,\\dots,n_{1}$ and $y_{j} \\sim \\mathcal{N}(\\mu_{2},\\sigma^{2})$ independently for $j=1,\\dots,n_{2}$, where $\\sigma^{2}$ is the same across conditions but unknown.\n\nYou will construct the Likelihood Ratio (LR) test for the null hypothesis $H_{0}:\\mu_{1}=\\mu_{2}$ against the two-sided alternative $H_{1}:\\mu_{1}\\neq\\mu_{2}$ under the equal-variance normal model. Begin from first principles: write down the joint likelihood, define the LR statistic as the ratio of the maximized likelihood under $H_{0}$ to that under $H_{1}$, and obtain the closed-form LR in terms of observable sample quantities. Then, connect the LR test to the classical pooled two-sample $t$-statistic by algebraically expressing the LR statistic as an explicit function of the $t$-statistic and the sample sizes, showing the monotone relationship that makes the LR test reject for large $|t|$.\n\nLet $\\bar{x}=\\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}}x_{i}$ and $\\bar{y}=\\frac{1}{n_{2}}\\sum_{j=1}^{n_{2}}y_{j}$ denote the sample means, and let $s_{1}^{2}=\\frac{1}{n_{1}-1}\\sum_{i=1}^{n_{1}}(x_{i}-\\bar{x})^{2}$ and $s_{2}^{2}=\\frac{1}{n_{2}-1}\\sum_{j=1}^{n_{2}}(y_{j}-\\bar{y})^{2}$ denote the unbiased sample variances. Define the pooled variance $s_{p}^{2}=\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}$ and the two-sample pooled $t$-statistic $t=\\frac{\\bar{x}-\\bar{y}}{s_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$.\n\nYour final answer must be a single, closed-form analytic expression for $-2\\log\\Lambda$ purely in terms of $t$, $n_{1}$, and $n_{2}$. No numerical rounding is required, and no units are involved. Express the final answer exactly as a function of $t$, $n_{1}$, and $n_{2}$.",
            "solution": "The user wants to construct the Likelihood Ratio (LR) test for the equality of two means from independent normal populations with a common unknown variance, and then express the LR test statistic in terms of the classical two-sample pooled $t$-statistic.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- Data: Two independent sets of samples, $\\{x_{i}\\}_{i=1}^{n_{1}}$ from condition A and $\\{y_{j}\\}_{j=1}^{n_{2}}$ from condition B.\n- Model Assumptions: $x_{i} \\sim \\mathcal{N}(\\mu_{1},\\sigma^{2})$ for $i=1,\\dots,n_{1}$ and $y_{j} \\sim \\mathcal{N}(\\mu_{2},\\sigma^{2})$ for $j=1,\\dots,n_{2}$. The samples are independent, and the variance $\\sigma^{2}$ is common but unknown.\n- Hypotheses: Null hypothesis $H_{0}:\\mu_{1}=\\mu_{2}$ versus the alternative hypothesis $H_{1}:\\mu_{1}\\neq\\mu_{2}$.\n- Definitions:\n    - Sample means: $\\bar{x}=\\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}}x_{i}$ and $\\bar{y}=\\frac{1}{n_{2}}\\sum_{j=1}^{n_{2}}y_{j}$.\n    - Unbiased sample variances: $s_{1}^{2}=\\frac{1}{n_{1}-1}\\sum_{i=1}^{n_{1}}(x_{i}-\\bar{x})^{2}$ and $s_{2}^{2}=\\frac{1}{n_{2}-1}\\sum_{j=1}^{n_{2}}(y_{j}-\\bar{y})^{2}$.\n    - Pooled variance: $s_{p}^{2}=\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}$.\n    - Two-sample pooled $t$-statistic: $t=\\frac{\\bar{x}-\\bar{y}}{s_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$.\n\n**1.2. Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a canonical example of hypothesis testing in statistics, firmly based on the principles of likelihood theory and normal distribution models. The scenario of comparing spike counts is a realistic application in neuroscience data analysis.\n- **Well-Posed:** The problem is clearly stated, providing all necessary definitions and a specific goal: deriving a closed-form expression. A unique solution exists.\n- **Objective:** The problem is formulated in precise mathematical language, free from subjectivity.\n\n**1.3. Verdict and Action**\nThe problem is valid. The solution process will proceed.\n\n### Step 2: Derivation of the Likelihood Ratio Test\n\n**2.1. The Likelihood Function**\nLet the total number of observations be $N = n_{1}+n_{2}$. The joint likelihood function for the parameters $\\mu_{1}, \\mu_{2}, \\sigma^{2}$ given the data is the product of the individual probability density functions:\n$$L(\\mu_{1}, \\mu_{2}, \\sigma^{2}) = \\prod_{i=1}^{n_{1}} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(x_{i} - \\mu_{1})^{2}}{2\\sigma^{2}}\\right) \\prod_{j=1}^{n_{2}} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(y_{j} - \\mu_{2})^{2}}{2\\sigma^{2}}\\right)$$\nThis simplifies to:\n$$L(\\mu_{1}, \\mu_{2}, \\sigma^{2}) = (2\\pi\\sigma^{2})^{-N/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\left[ \\sum_{i=1}^{n_{1}} (x_{i} - \\mu_{1})^{2} + \\sum_{j=1}^{n_{2}} (y_{j} - \\mu_{2})^{2} \\right]\\right)$$\nThe corresponding log-likelihood is:\n$$\\log L = -\\frac{N}{2}\\log(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}} \\left[ \\sum_{i=1}^{n_{1}} (x_{i} - \\mu_{1})^{2} + \\sum_{j=1}^{n_{2}} (y_{j} - \\mu_{2})^{2} \\right]$$\n\n**2.2. Maximization under the Alternative Hypothesis ($H_{1}$)**\nUnder $H_{1}: \\mu_{1} \\neq \\mu_{2}$, the parameters $\\mu_{1}, \\mu_{2}, \\sigma^{2}$ are unconstrained. We find the Maximum Likelihood Estimators (MLEs) by setting the partial derivatives of $\\log L$ to zero.\n- For $\\mu_{1}$: $\\frac{\\partial \\log L}{\\partial \\mu_{1}} = \\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n_{1}}(x_{i} - \\mu_{1}) = 0 \\implies \\hat{\\mu}_{1} = \\bar{x}$.\n- For $\\mu_{2}$: $\\frac{\\partial \\log L}{\\partial \\mu_{2}} = \\frac{1}{\\sigma^{2}}\\sum_{j=1}^{n_{2}}(y_{j} - \\mu_{2}) = 0 \\implies \\hat{\\mu}_{2} = \\bar{y}$.\n- For $\\sigma^{2}$: $\\frac{\\partial \\log L}{\\partial \\sigma^{2}} = -\\frac{N}{2\\sigma^{2}} + \\frac{1}{2(\\sigma^{2})^{2}} \\left[ \\sum(x_{i} - \\hat{\\mu}_{1})^{2} + \\sum(y_{j} - \\hat{\\mu}_{2})^{2} \\right] = 0$.\nThis gives the MLE for $\\sigma^{2}$ under $H_{1}$:\n$$\\hat{\\sigma}^{2}_{H_{1}} = \\frac{1}{N} \\left[ \\sum_{i=1}^{n_{1}}(x_{i} - \\bar{x})^{2} + \\sum_{j=1}^{n_{2}}(y_{j} - \\bar{y})^{2} \\right]$$\nThe maximized likelihood under $H_1$, denoted $L_{H_{1}}^{\\max}$, is obtained by substituting these MLEs back into the likelihood function:\n$$L_{H_{1}}^{\\max} = (2\\pi\\hat{\\sigma}^{2}_{H_{1}})^{-N/2} \\exp\\left(-\\frac{N\\hat{\\sigma}^{2}_{H_{1}}}{2\\hat{\\sigma}^{2}_{H_{1}}}\\right) = (2\\pi\\hat{\\sigma}^{2}_{H_{1}})^{-N/2} \\exp\\left(-\\frac{N}{2}\\right)$$\n\n**2.3. Maximization under the Null Hypothesis ($H_{0}$)**\nUnder $H_{0}: \\mu_{1} = \\mu_{2} = \\mu$, the model is constrained. We find the MLEs for $\\mu$ and $\\sigma^{2}$.\n- For $\\mu$: We maximize $\\log L$ with the constraint. The derivative with respect to $\\mu$ is:\n$\\frac{\\partial \\log L}{\\partial \\mu} = \\frac{1}{\\sigma^{2}}\\left[\\sum(x_{i} - \\mu) + \\sum(y_{j} - \\mu)\\right] = 0 \\implies n_{1}\\bar{x} - n_{1}\\mu + n_{2}\\bar{y} - n_{2}\\mu = 0$.\nThis gives the MLE for the common mean $\\mu$:\n$$\\hat{\\mu}_{H_{0}} = \\frac{n_{1}\\bar{x} + n_{2}\\bar{y}}{n_{1} + n_{2}}$$\nThis is the overall mean of all data points.\n- For $\\sigma^{2}$: The form of the derivative is the same, yielding:\n$$\\hat{\\sigma}^{2}_{H_{0}} = \\frac{1}{N} \\left[ \\sum_{i=1}^{n_{1}}(x_{i} - \\hat{\\mu}_{H_{0}})^{2} + \\sum_{j=1}^{n_{2}}(y_{j} - \\hat{\\mu}_{H_{0}})^{2} \\right]$$\nThe maximized likelihood under $H_0$, denoted $L_{H_{0}}^{\\max}$, is:\n$$L_{H_{0}}^{\\max} = (2\\pi\\hat{\\sigma}^{2}_{H_{0}})^{-N/2} \\exp\\left(-\\frac{N\\hat{\\sigma}^{2}_{H_{0}}}{2\\hat{\\sigma}^{2}_{H_{0}}}\\right) = (2\\pi\\hat{\\sigma}^{2}_{H_{0}})^{-N/2} \\exp\\left(-\\frac{N}{2}\\right)$$\n\n**2.4. The Likelihood Ratio Statistic**\nThe LR statistic $\\Lambda$ is the ratio of the maximized likelihoods:\n$$\\Lambda = \\frac{L_{H_{0}}^{\\max}}{L_{H_{1}}^{\\max}} = \\frac{(2\\pi\\hat{\\sigma}^{2}_{H_{0}})^{-N/2} \\exp(-N/2)}{(2\\pi\\hat{\\sigma}^{2}_{H_{1}})^{-N/2} \\exp(-N/2)} = \\left(\\frac{\\hat{\\sigma}^{2}_{H_{1}}}{\\hat{\\sigma}^{2}_{H_{0}}}\\right)^{N/2}$$\nThe LRT rejects $H_{0}$ for small values of $\\Lambda$. This is equivalent to rejecting for large values of $-2\\log\\Lambda$:\n$$-2\\log\\Lambda = -2 \\log\\left( \\left(\\frac{\\hat{\\sigma}^{2}_{H_{1}}}{\\hat{\\sigma}^{2}_{H_{0}}}\\right)^{N/2} \\right) = -N \\log\\left(\\frac{\\hat{\\sigma}^{2}_{H_{1}}}{\\hat{\\sigma}^{2}_{H_{0}}}\\right) = N \\log\\left(\\frac{\\hat{\\sigma}^{2}_{H_{0}}}{\\hat{\\sigma}^{2}_{H_{1}}}\\right)$$\n\n**2.5. Connecting to the $t$-statistic**\nWe introduce the sum of squares decomposition familiar from ANOVA.\nThe numerator of $N\\hat{\\sigma}^{2}_{H_{1}}$ is the Residual Sum of Squares ($SS_{R}$), which is the sum of squared deviations from group means:\n$$SS_{R} = \\sum_{i=1}^{n_{1}}(x_{i} - \\bar{x})^{2} + \\sum_{j=1}^{n_{2}}(y_{j} - \\bar{y})^{2} = (n_{1}-1)s_{1}^{2} + (n_{2}-1)s_{2}^{2}$$\nFrom the problem definition, $s_{p}^{2} = \\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2} = \\frac{SS_{R}}{n_{1}+n_{2}-2}$. Thus, $SS_{R} = (n_{1}+n_{2}-2)s_{p}^{2}$.\nSo, $\\hat{\\sigma}^{2}_{H_{1}} = \\frac{SS_{R}}{N}$.\n\nThe numerator of $N\\hat{\\sigma}^{2}_{H_{0}}$ is the Total Sum of Squares ($SS_{T}$), the sum of squared deviations from the overall mean $\\hat{\\mu}_{H_0}$:\n$$SS_{T} = \\sum_{i=1}^{n_{1}}(x_{i} - \\hat{\\mu}_{H_{0}})^{2} + \\sum_{j=1}^{n_2}(y_{j} - \\hat{\\mu}_{H_{0}})^{2}$$\nA standard identity states $SS_{T} = SS_{R} + SS_{B}$, where $SS_{B}$ is the Between-group Sum of Squares:\n$$SS_{B} = n_{1}(\\bar{x}-\\hat{\\mu}_{H_{0}})^{2} + n_{2}(\\bar{y}-\\hat{\\mu}_{H_{0}})^{2} = \\frac{n_{1}n_{2}}{n_{1}+n_{2}}(\\bar{x}-\\bar{y})^{2}$$\nNow we can write the ratio of variances:\n$$\\frac{\\hat{\\sigma}^{2}_{H_{0}}}{\\hat{\\sigma}^{2}_{H_{1}}} = \\frac{SS_{T}/N}{SS_{R}/N} = \\frac{SS_{T}}{SS_{R}} = \\frac{SS_{R}+SS_{B}}{SS_{R}} = 1 + \\frac{SS_{B}}{SS_{R}}$$\nThe LR statistic becomes:\n$$-2\\log\\Lambda = N \\log\\left(1 + \\frac{SS_{B}}{SS_{R}}\\right) = (n_{1}+n_{2}) \\log\\left(1 + \\frac{SS_{B}}{SS_{R}}\\right)$$\nTo finalize, we express the ratio $\\frac{SS_{B}}{SS_{R}}$ in terms of the $t$-statistic. By definition, $t = \\frac{\\bar{x}-\\bar{y}}{s_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$. Squaring this gives:\n$$t^{2} = \\frac{(\\bar{x}-\\bar{y})^{2}}{s_{p}^{2}\\left(\\frac{1}{n_{1}}+\\frac{1}{n_{2}}\\right)} = \\frac{(\\bar{x}-\\bar{y})^{2}}{s_{p}^{2}\\left(\\frac{n_{1}+n_{2}}{n_{1}n_{2}}\\right)} = \\frac{n_{1}n_{2}(\\bar{x}-\\bar{y})^{2}}{s_{p}^{2}(n_{1}+n_{2})}$$\nFrom the expression for $SS_{B}$, we have $(\\bar{x}-\\bar{y})^{2} = \\frac{n_{1}+n_{2}}{n_{1}n_{2}}SS_{B}$. Substituting this into the equation for $t^{2}$:\n$$t^{2} = \\frac{n_{1}n_{2}}{s_{p}^{2}(n_{1}+n_{2})} \\left( \\frac{n_{1}+n_{2}}{n_{1}n_{2}} SS_{B} \\right) = \\frac{SS_{B}}{s_{p}^{2}}$$\nNow we form the desired ratio:\n$$\\frac{SS_{B}}{SS_{R}} = \\frac{t^{2}s_{p}^{2}}{SS_{R}} = \\frac{t^{2}s_{p}^{2}}{(n_{1}+n_{2}-2)s_{p}^{2}} = \\frac{t^{2}}{n_{1}+n_{2}-2}$$\nSubstituting this back into the expression for $-2\\log\\Lambda$:\n$$-2\\log\\Lambda = (n_{1}+n_{2}) \\log\\left(1 + \\frac{t^{2}}{n_{1}+n_{2}-2}\\right)$$\nThis final expression shows $-2\\log\\Lambda$ as a function of $t$, $n_1$, and $n_2$. Since the natural logarithm is a strictly increasing function, $-2\\log\\Lambda$ is a monotonically increasing function of $t^{2}$, and thus of $|t|$. This demonstrates that rejecting $H_{0}$ for large values of $-2\\log\\Lambda$ is equivalent to rejecting $H_{0}$ for large values of $|t|$, establishing the equivalence of the LR test and the two-sided pooled $t$-test.",
            "answer": "$$\\boxed{(n_1+n_2)\\log\\left(1 + \\frac{t^2}{n_1+n_2-2}\\right)}$$"
        },
        {
            "introduction": "Moving from foundational theory to practical application, this exercise addresses a core task in modern neuroscience data analysis: testing hypotheses within the Generalized Linear Model (GLM) framework. GLMs are indispensable for modeling neural data such as spike counts or binary behavioral outcomes. This hands-on coding problem will require you to implement the LR test to compare nested GLMs, allowing you to determine the statistical significance of specific covariates, a crucial skill for interpreting the relationship between neural activity and experimental variables .",
            "id": "4174116",
            "problem": "Consider independent neural observations modeled by generalized linear models (GLMs) in neuroscientific data analysis. The GLM framework assumes that, for each observation index $i$, the response $y_i$ has a distribution in the exponential family with mean $E[y_i] = \\mu_i$ linked to predictors $x_i$ via a linear predictor $\\eta_i = x_i^\\top \\beta$, where $\\beta$ is the parameter vector and the link function relates $\\mu_i$ to $\\eta_i$. Assume the canonical link for each distribution. You are asked to implement the Likelihood Ratio (LR) test to compare nested models: a reduced model and a full model, where the full model contains all parameters of the reduced model plus additional parameters. The LR test compares the maximized log-likelihoods under the two models according to the likelihood principle. Under standard regularity conditions, the LR test statistic has an asymptotic chi-square approximation with degrees of freedom given by the difference in parameter counts between the full and reduced models.\n\nYour program must:\n- Fit GLMs by maximum likelihood for the Poisson distribution with the canonical log link and the Bernoulli distribution with the canonical logit link.\n- For each test case, construct a reduced design matrix and a full design matrix that differ by one additional covariate column in the full model, both including an intercept term; estimate the maximum likelihood parameters for both models; compute the LR statistic; compute the chi-square approximation $p$-value using degrees of freedom equal to the difference in parameter counts between the full and reduced models; and decide whether to reject the reduced model at significance level $\\alpha = 0.05$.\n\nUse the following test suite, where each case specifies the model type, sample size, random seed, and the true data-generating parameters for simulation. For all cases, the reduced design matrix consists of an intercept and one covariate $x_1$, and the full design matrix consists of an intercept, $x_1$, and an additional covariate $x_2$. In all cases, $x_1$ is drawn as $x_1 \\sim \\mathcal{N}(0,1)$ and $x_2$ is drawn independently as $x_2 \\sim \\mathcal{N}(0,1)$ unless otherwise stated. The intercept is always included as a constant column of ones.\n\n- Case A (happy path, Poisson counts with a real effect):\n    - Model type: Poisson (canonical log link).\n    - Sample size: $n = 200$.\n    - Random seed: $7$.\n    - True reduced parameters: $\\beta_{\\text{red}} = (\\beta_0,\\beta_1) = (\\log(8), 0.5)$.\n    - True full parameters: $\\beta_{\\text{full}} = (\\beta_0,\\beta_1,\\beta_2) = (\\log(8), 0.5, 0.7)$.\n    - Data generation: simulate $y_i \\sim \\text{Poisson}(\\mu_i)$ with $\\mu_i = \\exp(\\eta_i)$ and $\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}$.\n\n- Case B (edge case, Poisson counts with no added effect):\n    - Model type: Poisson (canonical log link).\n    - Sample size: $n = 200$.\n    - Random seed: $11$.\n    - True reduced parameters: $\\beta_{\\text{red}} = (\\beta_0,\\beta_1) = (\\log(8), 0.5)$.\n    - True full parameters: $\\beta_{\\text{full}} = (\\beta_0,\\beta_1,\\beta_2) = (\\log(8), 0.5, 0.0)$.\n    - Data generation: same as Case A.\n\n- Case C (happy path, Bernoulli spikes with a real effect):\n    - Model type: Bernoulli (canonical logit link).\n    - Sample size: $n = 300$.\n    - Random seed: $13$.\n    - True reduced parameters: $\\beta_{\\text{red}} = (\\beta_0,\\beta_1) = (-1.0, 0.8)$.\n    - True full parameters: $\\beta_{\\text{full}} = (\\beta_0,\\beta_1,\\beta_2) = (-1.0, 0.8, -1.0)$.\n    - Data generation: simulate $y_i \\sim \\text{Bernoulli}(p_i)$ with $p_i = \\mu_i = \\sigma(\\eta_i)$, $\\sigma(z) = 1/(1 + e^{-z})$, and $\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}$.\n\n- Case D (edge case, Bernoulli with small sample and near collinearity, no added effect):\n    - Model type: Bernoulli (canonical logit link).\n    - Sample size: $n = 50$.\n    - Random seed: $21$.\n    - True reduced parameters: $\\beta_{\\text{red}} = (\\beta_0,\\beta_1) = (-0.5, 0.3)$.\n    - True full parameters: $\\beta_{\\text{full}} = (\\beta_0,\\beta_1,\\beta_2) = (-0.5, 0.3, 0.0)$.\n    - Covariate structure: draw $x_1 \\sim \\mathcal{N}(0,1)$; set $x_2 = x_1 + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, 0.05^2)$ to induce near collinearity.\n    - Data generation: same as Case C.\n\nAlgorithmic requirements:\n- Implement maximum likelihood estimation using Newton's method specialized to GLMs via iteratively reweighted least squares (IRLS) for canonical links. For Poisson, use $\\mu_i = \\exp(\\eta_i)$. For Bernoulli, use $\\mu_i = \\sigma(\\eta_i)$.\n- Compute the maximized log-likelihood for each fitted model. For the Poisson model, the log-likelihood may include the term $-\\sum_i \\log(y_i!)$; for the LR statistic the constant terms cancel across models with the same data.\n- Compute the LR test statistic as twice the difference between the maximized log-likelihoods of the full and reduced models. Compute the chi-square approximation $p$-value using degrees of freedom $\\nu = k_{\\text{full}} - k_{\\text{red}}$, where $k_{\\text{full}}$ and $k_{\\text{red}}$ are the number of free parameters in the full and reduced models, respectively (counting the intercept).\n- Decide whether to reject the reduced model at $\\alpha = 0.05$ if the $p$-value is strictly less than $0.05$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list of lists, where each inner list is of the form $[\\text{LR statistic}, \\text{$p$-value}, \\text{degrees of freedom}, \\text{decision}]$ with the first two entries as floats, the third as an integer, and the fourth as a boolean. For example: $[[\\ell_1, p_1, \\nu_1, d_1],[\\ell_2, p_2, \\nu_2, d_2],[\\ell_3, p_3, \\nu_3, d_3],[\\ell_4, p_4, \\nu_4, d_4]]$.",
            "solution": "The problem requires the implementation of the Likelihood Ratio (LR) test for comparing nested Generalized Linear Models (GLMs), specifically for Poisson and Bernoulli distributions, and applying this test to four distinct simulated datasets. The entire process, from model fitting to hypothesis testing, must be developed from first principles.\n\nA Generalized Linear Model is defined by three components:\n1.  A **random component**, which specifies the probability distribution of the response variable $y_i$ from the exponential family. In this problem, we consider the Poisson distribution for counts and the Bernoulli distribution for binary outcomes.\n2.  A **systematic component**, which is a linear predictor $\\eta_i$ formed from a set of covariates $x_i$: $\\eta_i = x_i^\\top \\beta$.\n3.  A **link function** $g$, which connects the expected value of the response, $\\mu_i = E[y_i]$, to the linear predictor: $g(\\mu_i) = \\eta_i$. We will use the canonical link function for each distribution, as it imparts desirable statistical properties to the model, such as the concavity of the log-likelihood function.\n\nFor the **Poisson model**, used for count data $y_i \\in \\{0, 1, 2, ...\\}$, the canonical link is the natural logarithm, $g(\\mu_i) = \\log(\\mu_i) = \\eta_i$. The inverse link is therefore $\\mu_i = \\exp(\\eta_i)$. The variance of the response is equal to its mean, $\\text{Var}(y_i) = \\mu_i$.\n\nFor the **Bernoulli model**, used for binary data $y_i \\in \\{0, 1\\}$, the canonical link is the logit function, $g(\\mu_i) = \\text{logit}(\\mu_i) = \\log\\left(\\frac{\\mu_i}{1-\\mu_i}\\right) = \\eta_i$. The inverse link is the sigmoid (or logistic) function, $\\mu_i = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}$. The variance is given by $\\text{Var}(y_i) = \\mu_i(1-\\mu_i)$.\n\nThe model parameters $\\beta$ are estimated by maximizing the log-likelihood function, $l(\\beta; y) = \\sum_i l_i(\\beta; y_i)$. This maximization is typically achieved using a numerical optimization algorithm. For GLMs with canonical links, the Newton-Raphson method simplifies to an elegant and efficient algorithm known as **Iteratively Reweighted Least Squares (IRLS)**. The IRLS algorithm provides an update rule for the parameter vector $\\beta$ at each iteration $t+1$:\n$$ \\beta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)} $$\nThis equation represents a weighted least squares regression of an \"adjusted response\" $z^{(t)}$ on the design matrix $X$, with weights given by $W^{(t)}$.\nThe components are:\n- $X$ is the $n \\times p$ design matrix, where $n$ is the number of observations and $p$ is the number of parameters.\n- $W^{(t)}$ is an $n \\times n$ diagonal matrix of weights. For a canonical link, the diagonal elements are the variances of the responses, calculated using the parameters from the previous iteration: $W_{ii}^{(t)} = \\text{Var}(y_i | \\beta^{(t)}) = V(\\mu_i^{(t)})$.\n  - For the Poisson model: $W_{ii}^{(t)} = \\mu_i^{(t)} = \\exp(x_i^\\top \\beta^{(t)})$.\n  - For the Bernoulli model: $W_{ii}^{(t)} = \\mu_i^{(t)}(1-\\mu_i^{(t)})$.\n- $z^{(t)}$ is the $n \\times 1$ adjusted response vector. Its elements are defined as $z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)})(\\frac{\\partial \\eta_i}{\\partial \\mu_i})^{(t)}$. For a canonical link, $\\frac{\\partial \\eta_i}{\\partial \\mu_i} = \\frac{1}{V(\\mu_i)}$, so the expression simplifies to $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{V(\\mu_i^{(t)})}$.\n\nThe algorithm is initialized with a starting guess for $\\beta$ (e.g., a zero vector) and iterates until the change in $\\beta$ between successive iterations falls below a predefined tolerance.\n\nOnce the Maximum Likelihood Estimate (MLE) $\\hat{\\beta}$ is found, the maximized value of the log-likelihood is computed. The specific form of the log-likelihood function, ignoring any terms that do not depend on $\\beta$, is:\n- For the Poisson model: $l(\\hat{\\beta}) = \\sum_{i=1}^n \\left( y_i (x_i^\\top \\hat{\\beta}) - \\exp(x_i^\\top \\hat{\\beta}) \\right)$.\n- For the Bernoulli model: $l(\\hat{\\beta}) = \\sum_{i=1}^n \\left( y_i (x_i^\\top \\hat{\\beta}) - \\log(1 + \\exp(x_i^\\top \\hat{\\beta})) \\right)$.\n\nThe **Likelihood Ratio (LR) test** is used to compare two nested models: a reduced model ($H_0$) and a full model ($H_1$), where the reduced model is a special case of the full model (i.e., its parameter space $\\Theta_0$ is a subset of the full model's parameter space $\\Theta_1$). The LR test statistic is given by:\n$$ \\Lambda = 2 \\left( l(\\hat{\\beta}_{\\text{full}}) - l(\\hat{\\beta}_{\\text{red}}) \\right) $$\nwhere $l(\\hat{\\beta}_{\\text{full}})$ and $l(\\hat{\\beta}_{\\text{red}})$ are the maximized log-likelihoods for the full and reduced models, respectively.\n\nAccording to Wilks' theorem, under the null hypothesis that the reduced model is the true model, the statistic $\\Lambda$ asymptotically follows a chi-square ($\\chi^2$) distribution. The degrees of freedom ($\\nu$) of this distribution are equal to the difference in the number of parameters between the two models: $\\nu = k_{\\text{full}} - k_{\\text{red}}$.\n\nIn this problem, the reduced model includes an intercept and one covariate $x_1$, so $k_{\\text{red}} = 2$. The full model adds a second covariate $x_2$, so $k_{\\text{full}} = 3$. The null hypothesis under test is $H_0: \\beta_2 = 0$, and the degrees of freedom for the LR test is $\\nu = 3 - 2 = 1$. The $p$-value is calculated as the probability $P(\\chi^2_1 \\ge \\Lambda)$. We reject the null hypothesis (i.e., we conclude the full model provides a significantly better fit) if the $p$-value is less than the significance level $\\alpha = 0.05$.\n\nThe implementation will proceed by first simulating data for each test case according to the specified parameters. Then, for each case, we will fit both the reduced and full GLMs using our IRLS implementation. Finally, we will compute the LR statistic $\\Lambda$, its associated $p$-value from the $\\chi^2_1$ distribution, and make a decision based on the $\\alpha=0.05$ threshold.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Performs Likelihood Ratio (LR) tests for nested Generalized Linear Models (GLMs)\n    on four specified test cases.\n    \"\"\"\n\n    def fit_glm(X, y, model_type, tol=1e-8, max_iter=50):\n        \"\"\"\n        Fits a GLM using Iteratively Reweighted Least Squares (IRLS).\n\n        Args:\n            X (np.ndarray): Design matrix (n_samples, n_features).\n            y (np.ndarray): Response vector (n_samples,).\n            model_type (str): 'poisson' or 'bernoulli'.\n            tol (float): Convergence tolerance.\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            tuple: (beta_hat, log_likelihood)\n                   - beta_hat (np.ndarray): Estimated parameters.\n                   - log_likelihood (float): Maximized log-likelihood.\n        \"\"\"\n        if model_type == 'poisson':\n            link_fn = np.exp\n            variance_fn = lambda mu: mu\n            log_likelihood_fn = lambda beta, X, y: np.sum(y * (X @ beta) - np.exp(X @ beta))\n        elif model_type == 'bernoulli':\n            # Use a stable sigmoid implementation\n            def sigmoid(eta):\n                return np.piecewise(eta, [eta > 0], [\n                    lambda x: 1 / (1 + np.exp(-x)),\n                    lambda x: np.exp(x) / (1 + np.exp(x))\n                ])\n            link_fn = sigmoid\n            variance_fn = lambda mu: mu * (1 - mu)\n            log_likelihood_fn = lambda beta, X, y: np.sum(y * (X @ beta) - np.log(1 + np.exp(X @ beta)))\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n\n        beta = np.zeros(X.shape[1])\n        \n        for _ in range(max_iter):\n            eta = X @ beta\n            mu = link_fn(eta)\n            \n            # Numerical stability for Bernoulli\n            if model_type == 'bernoulli':\n                mu = np.clip(mu, 1e-10, 1 - 1e-10)\n\n            var = variance_fn(mu)\n            W = np.diag(var)\n\n            # For canonical link, d(mu)/d(eta) = Var(mu)\n            z = eta + (y - mu) / var\n            \n            # Solve the weighted least squares problem\n            # (X.T @ W @ X) @ beta_new = X.T @ W @ z\n            try:\n                # Add a small regularization term for stability, e.g., in case of collinearity\n                reg = 1e-8 * np.eye(X.shape[1])\n                beta_new = np.linalg.solve(X.T @ W @ X + reg, X.T @ W @ z)\n            except np.linalg.LinAlgError:\n                # Fallback to pseudo-inverse if solve fails\n                beta_new = np.linalg.pinv(X.T @ W @ X) @ (X.T @ W @ z)\n            \n            if np.linalg.norm(beta_new - beta) < tol:\n                beta = beta_new\n                break\n            \n            beta = beta_new\n        \n        log_likelihood = log_likelihood_fn(beta, X, y)\n        return beta, log_likelihood\n\n    test_cases = [\n        {\n            'name': 'A', 'model_type': 'poisson', 'n': 200, 'seed': 7,\n            'beta_true': np.array([np.log(8), 0.5, 0.7]), 'collinear': False\n        },\n        {\n            'name': 'B', 'model_type': 'poisson', 'n': 200, 'seed': 11,\n            'beta_true': np.array([np.log(8), 0.5, 0.0]), 'collinear': False\n        },\n        {\n            'name': 'C', 'model_type': 'bernoulli', 'n': 300, 'seed': 13,\n            'beta_true': np.array([-1.0, 0.8, -1.0]), 'collinear': False\n        },\n        {\n            'name': 'D', 'model_type': 'bernoulli', 'n': 50, 'seed': 21,\n            'beta_true': np.array([-0.5, 0.3, 0.0]), 'collinear': True\n        }\n    ]\n\n    results = []\n    \n    alpha = 0.05\n\n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        n = case['n']\n        \n        # Generate covariates\n        x1 = rng.normal(0, 1, n)\n        if case['collinear']:\n            epsilon = rng.normal(0, 0.05, n)\n            x2 = x1 + epsilon\n        else:\n            x2 = rng.normal(0, 1, n)\n            \n        X_full = np.c_[np.ones(n), x1, x2]\n        \n        # Generate response data y from the full model\n        eta_true = X_full @ case['beta_true']\n        if case['model_type'] == 'poisson':\n            mu_true = np.exp(eta_true)\n            y = rng.poisson(mu_true)\n        else: # bernoulli\n            p_true = 1 / (1 + np.exp(-eta_true))\n            y = rng.binomial(1, p_true)\n            \n        # Define reduced and full design matrices\n        X_red = X_full[:, :2]\n\n        # Fit reduced model (H0)\n        _, ll_red = fit_glm(X_red, y, case['model_type'])\n\n        # Fit full model (H1)\n        _, ll_full = fit_glm(X_full, y, case['model_type'])\n        \n        # Perform Likelihood Ratio Test\n        lr_statistic = 2 * (ll_full - ll_red)\n        if lr_statistic < 0: # Due to numerical precision, can be slightly negative\n            lr_statistic = 0\n\n        dof = X_full.shape[1] - X_red.shape[1]\n        \n        p_value = chi2.sf(lr_statistic, dof)\n        \n        decision = p_value < alpha\n        \n        results.append([lr_statistic, p_value, dof, decision])\n\n    # Format the final output string\n    output_str = '['\n    for i, res in enumerate(results):\n        # Format: [LR_statistic, p-value, degrees of freedom, decision]\n        # Floats, int, bool\n        output_str += f\"[{res[0]:.8f},{res[1]:.8f},{res[2]},{str(res[3]).lower()}]\"\n        if i < len(results) - 1:\n            output_str += ','\n    output_str += ']'\n    \n    # Due to float representation, direct string formatting is safer than map/join\n    # Example format: [[18.84777553,0.00001423,1,true],[0.00001096,0.99735496,1,false], ...\n    # This format is not standard Python list repr. Using this custom builder.\n    \n    # Re-evaluating the output format:\n    # \"a comma-separated list of lists...[[\\ell_1, p_1, \\nu_1, d_1],...]\"\n    # This implies standard list representation printing.\n    print(results)\n\n\nsolve()\n```"
        },
        {
            "introduction": "The standard asymptotic theory for the Likelihood Ratio Test, known as Wilks' theorem, relies on certain regularity conditions. This advanced problem explores a critical exception: when a parameter under the null hypothesis lies on the boundary of its permissible space. This scenario is not just a theoretical curiosity but arises in practical neuroscience models, such as when testing for zero-inflation in spike counts. By deriving the non-standard null distribution for this case, you will gain a deeper understanding of the LRT's theoretical underpinnings and be better equipped to handle a wider range of statistical models .",
            "id": "4174137",
            "problem": "Consider spike count data recorded from a single neuron across $n$ non-overlapping time bins, producing observations $\\{Y_{i}\\}_{i=1}^{n}$ where each $Y_{i} \\in \\{0,1,2,\\dots\\}$. To capture excess zeros due to unobserved network states or refractory periods, analysts sometimes posit a zero-inflated count model, in which, conditional on parameters, a fixed fraction of trials are structural zeros. Specifically, suppose the data are modeled by a zero-inflated Poisson distribution with parameters $(\\lambda,\\psi)$, where $\\lambda>0$ is the Poisson rate and $\\psi \\in [0,1)$ is the zero-inflation parameter representing the probability of a structural zero. The model is\n$$\n\\mathbb{P}(Y=y \\mid \\lambda,\\psi)=\n\\begin{cases}\n\\psi+(1-\\psi)\\exp(-\\lambda), & y=0, \\\\\n(1-\\psi)\\exp(-\\lambda)\\frac{\\lambda^{y}}{y!}, & y\\in\\{1,2,\\dots\\}.\n\\end{cases}\n$$\nWe are interested in testing for the presence of zero inflation using a Likelihood Ratio Test (LRT) of the null hypothesis $H_{0}\\!:\\ \\psi=0$ versus the alternative $H_{1}\\!:\\ \\psi>0$. Under $H_{0}$ the parameter $\\psi$ is on the boundary of its parameter space $[0,1)$.\n\nLet $\\ell_{n}(\\lambda,\\psi)$ denote the total log-likelihood for the sample $\\{Y_{i}\\}_{i=1}^{n}$ under the zero-inflated Poisson model, and define the LRT statistic\n$$\n\\Lambda_{n}=2\\left\\{\\sup_{\\lambda>0,\\ \\psi\\in[0,1)}\\ \\ell_{n}(\\lambda,\\psi)\\ -\\ \\sup_{\\lambda>0,\\ \\psi=0}\\ \\ell_{n}(\\lambda,0)\\right\\}.\n$$\nAssume standard regularity conditions for maximum likelihood estimation, including differentiability of $\\ell_{n}$ in a neighborhood of $(\\lambda,\\psi)=(\\lambda_{0},0)$, existence of Fisher information, and asymptotic normality of the score under $H_{0}$ after suitable reparameterization to handle nuisance effects of $\\lambda$.\n\nStarting from the core definitions of the score and Fisher information, and using a second-order Taylor expansion of the log-likelihood around $(\\lambda_{0},\\psi=0)$, derive the large-sample ($n\\to\\infty$) null distribution of the LRT statistic $\\Lambda_{n}$ for the boundary case $\\psi\\ge 0$. Express your final result as a single closed-form analytic expression for the cumulative distribution function $F_{\\Lambda}(t)$ of the limiting distribution for $t\\ge 0$, in terms of the standard normal cumulative distribution function $\\Phi(\\cdot)$. No numerical evaluation is required, and no units are involved. The final answer must be a single analytic expression.",
            "solution": "The user-provided problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   **Data**: $\\{Y_{i}\\}_{i=1}^{n}$ are $n$ independent and identically distributed observations from a zero-inflated Poisson distribution, where $Y_{i} \\in \\{0,1,2,\\dots\\}$.\n-   **Model**: The probability mass function is given by\n    $$\n    \\mathbb{P}(Y=y \\mid \\lambda,\\psi)=\n    \\begin{cases}\n    \\psi+(1-\\psi)\\exp(-\\lambda), & y=0, \\\\\n    (1-\\psi)\\exp(-\\lambda)\\frac{\\lambda^{y}}{y!}, & y\\in\\{1,2,\\dots\\}.\n    \\end{cases}\n    $$\n-   **Parameters**: The Poisson rate $\\lambda > 0$ and the zero-inflation probability $\\psi \\in [0,1)$.\n-   **Hypothesis Test**: The null hypothesis is $H_{0}: \\psi=0$ versus the alternative hypothesis $H_1: \\psi>0$.\n-   **Test Statistic**: The Likelihood Ratio Test (LRT) statistic is\n    $$\n    \\Lambda_{n}=2\\left\\{\\sup_{\\lambda>0,\\ \\psi\\in[0,1)}\\ \\ell_{n}(\\lambda,\\psi)\\ -\\ \\sup_{\\lambda>0,\\ \\psi=0}\\ \\ell_{n}(\\lambda,0)\\right\\},\n    $$\n    where $\\ell_n(\\lambda, \\psi)$ is the total log-likelihood.\n-   **Assumptions**: Standard regularity conditions for maximum likelihood estimation are assumed to hold, and we are interested in the large-sample ($n\\to\\infty$) limit.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. The zero-inflated Poisson model is a standard statistical tool used in many fields, including neuroscience, to model count data with an excess of zeros. The likelihood ratio test is a fundamental method of hypothesis testing. The problem is well-posed, asking for the derivation of a limiting distribution under a specific, well-defined setup. The core of the problem lies in the fact that the null hypothesis $H_0: \\psi=0$ places the parameter $\\psi$ on the boundary of its parameter space $[0,1)$, which is a known complication in asymptotic theory. The problem is objective and free of ambiguities. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full derivation of the solution will now be provided.\n\n### Derivation of the Limiting Distribution\nThe problem requires the derivation of the asymptotic distribution of the LRT statistic $\\Lambda_n$ for testing $H_0: \\psi=0$ against $H_1: \\psi>0$. The crucial feature is that the null value $\\psi=0$ lies on the boundary of the parameter space for $\\psi$, which is $[0,1)$. This configuration violates a key regularity condition for the standard Wilks' theorem, which would otherwise predict a $\\chi_1^2$ limiting distribution. Therefore, a different asymptotic theory, as developed by Chernoff (1954) and Self & Liang (1987) for parameters on a boundary, must be invoked.\n\nLet $\\hat{\\theta}_0 = (\\hat{\\lambda}_0, 0)$ be the maximum likelihood estimator (MLE) of the parameters $(\\lambda, \\psi)$ under the null hypothesis $H_0$. Here, $\\hat{\\lambda}_0$ is the MLE for a standard Poisson model, which is the sample mean $\\bar{Y}$. Let $\\hat{\\theta}_1 = (\\hat{\\lambda}_1, \\hat{\\psi}_1)$ be the MLE under the alternative hypothesis, where the maximization is over $\\lambda>0$ and $\\psi \\in [0,1)$.\n\nThe asymptotic behavior of $\\Lambda_n$ is determined by the behavior of the log-likelihood function in the neighborhood of the true parameter value under $H_0$. The MLE under the alternative, $\\hat{\\psi}_1$, is constrained to be non-negative. Its value depends on the gradient of the log-likelihood function with respect to $\\psi$ at the null-restricted MLE, $(\\hat{\\lambda}_0, 0)$. Let $S_{\\psi, a} = \\frac{\\partial \\ell_n}{\\partial \\psi}\\big|_{(\\hat{\\lambda}_0, 0)}$ be the score for $\\psi$ evaluated at the null MLE.\n\nUnder $H_0$, and by the assumption of asymptotic normality of the score, $S_{\\psi, a}$ (after proper scaling by $\\frac{1}{\\sqrt{n}}$) is asymptotically normal with a mean of $0$. Consequently, for large $n$, the probability that $S_{\\psi, a}$ is positive is approximately $1/2$, and the probability that it is non-positive is approximately $1/2$.\n\nWe analyze two cases:\n\n1.  **Case 1: $S_{\\psi, a} \\le 0$.**\n    If the score for $\\psi$ is non-positive, the log-likelihood function does not increase as $\\psi$ moves from $0$ into the parameter space $(0,1)$. Given the constraint $\\psi \\ge 0$, the maximum of the likelihood function under the alternative is achieved at the boundary, i.e., $\\hat{\\psi}_1 = 0$. In this scenario, the MLE under the alternative is identical to the MLE under the null: $(\\hat{\\lambda}_1, \\hat{\\psi}_1) = (\\hat{\\lambda}_0, 0)$. Consequently, the log-likelihoods are equal, and the LRT statistic is $\\Lambda_n=0$. This event occurs with an asymptotic probability of $1/2$.\n\n2.  **Case 2: $S_{\\psi, a} > 0$.**\n    If the score for $\\psi$ is positive, the log-likelihood function is increasing in $\\psi$ at $\\psi=0$. This implies that the MLE under the alternative will be in the interior of the parameter space, i.e., $\\hat{\\psi}_1 > 0$. In this situation, the boundary constraint is not active. The problem locally behaves like an unconstrained test of a single parameter. For this subset of the sample space, the LRT statistic $\\Lambda_n$ converges in distribution to a chi-squared random variable with $1$ degree of freedom, $\\chi_1^2$. This event occurs with an asymptotic probability of $1/2$.\n\nCombining these two outcomes, the limiting distribution of $\\Lambda_n$ is a mixture of a point mass at $0$ and a $\\chi_1^2$ distribution, with equal weights of $1/2$. Formally, if $\\Lambda$ is a random variable following this limiting distribution, we can write:\n$$\n\\Lambda \\sim \\frac{1}{2}\\chi_0^2 + \\frac{1}{2}\\chi_1^2\n$$\nwhere $\\chi_0^2$ represents a degenerate distribution with a point mass at $0$.\n\nWe now derive the cumulative distribution function (CDF), $F_{\\Lambda}(t) = \\mathbb{P}(\\Lambda \\le t)$, for $t \\ge 0$.\nThe CDF of the mixture is the weighted average of the individual CDFs:\n$$\nF_{\\Lambda}(t) = \\frac{1}{2} F_{\\chi_0^2}(t) + \\frac{1}{2} F_{\\chi_1^2}(t)\n$$\nFor $t \\ge 0$, the CDF of a point mass at $0$ is $F_{\\chi_0^2}(t) = \\mathbb{P}(0 \\le t) = 1$.\nThe CDF of a $\\chi_1^2$ distribution, $F_{\\chi_1^2}(t)$, can be expressed in terms of the standard normal CDF, $\\Phi(\\cdot)$. A $\\chi_1^2$ random variable is the square of a standard normal random variable, $Z \\sim N(0,1)$. Thus, for $t \\ge 0$:\n$$\nF_{\\chi_1^2}(t) = \\mathbb{P}(Z^2 \\le t) = \\mathbb{P}(-\\sqrt{t} \\le Z \\le \\sqrt{t})\n$$\n$$\nF_{\\chi_1^2}(t) = \\Phi(\\sqrt{t}) - \\Phi(-\\sqrt{t})\n$$\nUsing the symmetry property of the standard normal distribution, $\\Phi(-z) = 1 - \\Phi(z)$, we get:\n$$\nF_{\\chi_1^2}(t) = \\Phi(\\sqrt{t}) - (1 - \\Phi(\\sqrt{t})) = 2\\Phi(\\sqrt{t}) - 1\n$$\nSubstituting these results back into the expression for $F_{\\Lambda}(t)$:\n$$\nF_{\\Lambda}(t) = \\frac{1}{2}(1) + \\frac{1}{2}(2\\Phi(\\sqrt{t}) - 1)\n$$\n$$\nF_{\\Lambda}(t) = \\frac{1}{2} + \\Phi(\\sqrt{t}) - \\frac{1}{2}\n$$\n$$\nF_{\\Lambda}(t) = \\Phi(\\sqrt{t})\n$$\nThis is the closed-form analytic expression for the CDF of the limiting distribution of $\\Lambda_n$ for $t \\ge 0$.",
            "answer": "$$\n\\boxed{\\Phi(\\sqrt{t})}\n$$"
        }
    ]
}