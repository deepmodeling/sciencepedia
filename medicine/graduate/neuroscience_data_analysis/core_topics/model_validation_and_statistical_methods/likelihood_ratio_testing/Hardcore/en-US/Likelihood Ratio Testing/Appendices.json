{
    "hands_on_practices": [
        {
            "introduction": "Before applying the Likelihood Ratio Test (LRT) to complex, multi-parameter models, it is instructive to see how this general principle recovers familiar results from classical statistics. This practice guides you through the process of deriving the ubiquitous two-sample $t$-test from first principles, starting with the likelihood function for two normally distributed groups. By showing that the LRT statistic is a monotonic function of the squared $t$-statistic, you will solidify your understanding of how the LRT provides a unifying framework for hypothesis testing .",
            "id": "4174050",
            "problem": "A cognitive electrophysiology lab is analyzing trial-averaged spike counts recorded from a single neuron under two task conditions, denoted condition A and condition B. On each trial, spike counts are accumulated over a fixed $100$ ms analysis window. For condition A, there are $n_{1}$ trials with counts $\\{x_{i}\\}_{i=1}^{n_{1}}$. For condition B, there are $n_{2}$ trials with counts $\\{y_{j}\\}_{j=1}^{n_{2}}$. By standard arguments from the Central Limit Theorem (CLT), the within-condition trial-to-trial variability in spike counts is well-approximated by a normal distribution. Assume the two conditions have independent samples, condition-specific means, and a common unknown variance. Formally, assume $x_{i} \\sim \\mathcal{N}(\\mu_{1},\\sigma^{2})$ independently for $i=1,\\dots,n_{1}$ and $y_{j} \\sim \\mathcal{N}(\\mu_{2},\\sigma^{2})$ independently for $j=1,\\dots,n_{2}$, where $\\sigma^{2}$ is the same across conditions but unknown.\n\nYou will construct the Likelihood Ratio (LR) test for the null hypothesis $H_{0}:\\mu_{1}=\\mu_{2}$ against the two-sided alternative $H_{1}:\\mu_{1}\\neq\\mu_{2}$ under the equal-variance normal model. Begin from first principles: write down the joint likelihood, define the LR statistic as the ratio of the maximized likelihood under $H_{0}$ to that under $H_{1}$, and obtain the closed-form LR in terms of observable sample quantities. Then, connect the LR test to the classical pooled two-sample $t$-statistic by algebraically expressing the LR statistic as an explicit function of the $t$-statistic and the sample sizes, showing the monotone relationship that makes the LR test reject for large $|t|$.\n\nLet $\\bar{x}=\\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}}x_{i}$ and $\\bar{y}=\\frac{1}{n_{2}}\\sum_{j=1}^{n_{2}}y_{j}$ denote the sample means, and let $s_{1}^{2}=\\frac{1}{n_{1}-1}\\sum_{i=1}^{n_{1}}(x_{i}-\\bar{x})^{2}$ and $s_{2}^{2}=\\frac{1}{n_{2}-1}\\sum_{j=1}^{n_{2}}(y_{j}-\\bar{y})^{2}$ denote the unbiased sample variances. Define the pooled variance $s_{p}^{2}=\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}$ and the two-sample pooled $t$-statistic $t=\\frac{\\bar{x}-\\bar{y}}{s_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$.\n\nYour final answer must be a single, closed-form analytic expression for $-2\\ln\\Lambda$ purely in terms of $t$, $n_{1}$, and $n_{2}$. No numerical rounding is required, and no units are involved. Express the final answer exactly as a function of $t$, $n_{1}$, and $n_{2}$.",
            "solution": "The user wants to construct the Likelihood Ratio (LR) test for the equality of two means from independent normal populations with a common unknown variance, and then express the LR test statistic in terms of the classical two-sample pooled $t$-statistic.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- Data: Two independent sets of samples, $\\{x_{i}\\}_{i=1}^{n_{1}}$ from condition A and $\\{y_{j}\\}_{j=1}^{n_{2}}$ from condition B.\n- Model Assumptions: $x_{i} \\sim \\mathcal{N}(\\mu_{1},\\sigma^{2})$ for $i=1,\\dots,n_{1}$ and $y_{j} \\sim \\mathcal{N}(\\mu_{2},\\sigma^{2})$ for $j=1,\\dots,n_{2}$. The samples are independent, and the variance $\\sigma^{2}$ is common but unknown.\n- Hypotheses: Null hypothesis $H_{0}:\\mu_{1}=\\mu_{2}$ versus the alternative hypothesis $H_{1}:\\mu_{1}\\neq\\mu_{2}$.\n- Definitions:\n    - Sample means: $\\bar{x}=\\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}}x_{i}$ and $\\bar{y}=\\frac{1}{n_{2}}\\sum_{j=1}^{n_{2}}y_{j}$.\n    - Unbiased sample variances: $s_{1}^{2}=\\frac{1}{n_{1}-1}\\sum_{i=1}^{n_{1}}(x_{i}-\\bar{x})^{2}$ and $s_{2}^{2}=\\frac{1}{n_{2}-1}\\sum_{j=1}^{n_{2}}(y_{j}-\\bar{y})^{2}$.\n    - Pooled variance: $s_{p}^{2}=\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}$.\n    - Two-sample pooled $t$-statistic: $t=\\frac{\\bar{x}-\\bar{y}}{s_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$.\n\n**1.2. Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a canonical example of hypothesis testing in statistics, firmly based on the principles of likelihood theory and normal distribution models. The scenario of comparing spike counts is a realistic application in neuroscience data analysis.\n- **Well-Posed:** The problem is clearly stated, providing all necessary definitions and a specific goal: deriving a closed-form expression. A unique solution exists.\n- **Objective:** The problem is formulated in precise mathematical language, free from subjectivity.\n\n**1.3. Verdict and Action**\nThe problem is valid. The solution process will proceed.\n\n### Step 2: Derivation of the Likelihood Ratio Test\n\n**2.1. The Likelihood Function**\nLet the total number of observations be $N = n_{1}+n_{2}$. The joint likelihood function for the parameters $\\mu_{1}, \\mu_{2}, \\sigma^{2}$ given the data is the product of the individual probability density functions:\n$$L(\\mu_{1}, \\mu_{2}, \\sigma^{2}) = \\prod_{i=1}^{n_{1}} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(x_{i} - \\mu_{1})^{2}}{2\\sigma^{2}}\\right) \\prod_{j=1}^{n_{2}} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(y_{j} - \\mu_{2})^{2}}{2\\sigma^{2}}\\right)$$\nThis simplifies to:\n$$L(\\mu_{1}, \\mu_{2}, \\sigma^{2}) = (2\\pi\\sigma^{2})^{-N/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\left[ \\sum_{i=1}^{n_{1}} (x_{i} - \\mu_{1})^{2} + \\sum_{j=1}^{n_{2}} (y_{j} - \\mu_{2})^{2} \\right]\\right)$$\nThe corresponding log-likelihood is:\n$$\\ln L = -\\frac{N}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}} \\left[ \\sum_{i=1}^{n_{1}} (x_{i} - \\mu_{1})^{2} + \\sum_{j=1}^{n_{2}} (y_{j} - \\mu_{2})^{2} \\right]$$\n\n**2.2. Maximization under the Alternative Hypothesis ($H_{1}$)**\nUnder $H_{1}: \\mu_{1} \\neq \\mu_{2}$, the parameters $\\mu_{1}, \\mu_{2}, \\sigma^{2}$ are unconstrained. We find the Maximum Likelihood Estimators (MLEs) by setting the partial derivatives of $\\ln L$ to zero.\n- For $\\mu_{1}$: $\\frac{\\partial \\ln L}{\\partial \\mu_{1}} = \\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n_{1}}(x_{i} - \\mu_{1}) = 0 \\implies \\hat{\\mu}_{1} = \\bar{x}$.\n- For $\\mu_{2}$: $\\frac{\\partial \\ln L}{\\partial \\mu_{2}} = \\frac{1}{\\sigma^{2}}\\sum_{j=1}^{n_{2}}(y_{j} - \\mu_{2}) = 0 \\implies \\hat{\\mu}_{2} = \\bar{y}$.\n- For $\\sigma^{2}$: $\\frac{\\partial \\ln L}{\\partial \\sigma^{2}} = -\\frac{N}{2\\sigma^{2}} + \\frac{1}{2(\\sigma^{2})^{2}} \\left[ \\sum(x_{i} - \\hat{\\mu}_{1})^{2} + \\sum(y_{j} - \\hat{\\mu}_{2})^{2} \\right] = 0$.\nThis gives the MLE for $\\sigma^{2}$ under $H_{1}$:\n$$\\hat{\\sigma}^{2}_{H_{1}} = \\frac{1}{N} \\left[ \\sum_{i=1}^{n_{1}}(x_{i} - \\bar{x})^{2} + \\sum_{j=1}^{n_{2}}(y_{j} - \\bar{y})^{2} \\right]$$\nThe maximized likelihood under $H_1$, denoted $L_{H_{1}}^{\\max}$, is obtained by substituting these MLEs back into the likelihood function:\n$$L_{H_{1}}^{\\max} = (2\\pi\\hat{\\sigma}^{2}_{H_{1}})^{-N/2} \\exp\\left(-\\frac{N\\hat{\\sigma}^{2}_{H_{1}}}{2\\hat{\\sigma}^{2}_{H_{1}}}\\right) = (2\\pi\\hat{\\sigma}^{2}_{H_{1}})^{-N/2} \\exp\\left(-\\frac{N}{2}\\right)$$\n\n**2.3. Maximization under the Null Hypothesis ($H_{0}$)**\nUnder $H_{0}: \\mu_{1} = \\mu_{2} = \\mu$, the model is constrained. We find the MLEs for $\\mu$ and $\\sigma^{2}$.\n- For $\\mu$: We maximize $\\ln L$ with the constraint. The derivative with respect to $\\mu$ is:\n$\\frac{\\partial \\ln L}{\\partial \\mu} = \\frac{1}{\\sigma^{2}}\\left[\\sum(x_{i} - \\mu) + \\sum(y_{j} - \\mu)\\right] = 0 \\implies n_{1}\\bar{x} - n_{1}\\mu + n_{2}\\bar{y} - n_{2}\\mu = 0$.\nThis gives the MLE for the common mean $\\mu$:\n$$\\hat{\\mu}_{H_{0}} = \\frac{n_{1}\\bar{x} + n_{2}\\bar{y}}{n_{1} + n_{2}}$$\nThis is the overall mean of all data points.\n- For $\\sigma^{2}$: The form of the derivative is the same, yielding:\n$$\\hat{\\sigma}^{2}_{H_{0}} = \\frac{1}{N} \\left[ \\sum_{i=1}^{n_{1}}(x_{i} - \\hat{\\mu}_{H_{0}})^{2} + \\sum_{j=1}^{n_{2}}(y_{j} - \\hat{\\mu}_{H_{0}})^{2} \\right]$$\nThe maximized likelihood under $H_0$, denoted $L_{H_{0}}^{\\max}$, is:\n$$L_{H_{0}}^{\\max} = (2\\pi\\hat{\\sigma}^{2}_{H_{0}})^{-N/2} \\exp\\left(-\\frac{N\\hat{\\sigma}^{2}_{H_{0}}}{2\\hat{\\sigma}^{2}_{H_{0}}}\\right) = (2\\pi\\hat{\\sigma}^{2}_{H_{0}})^{-N/2} \\exp\\left(-\\frac{N}{2}\\right)$$\n\n**2.4. The Likelihood Ratio Statistic**\nThe LR statistic $\\Lambda$ is the ratio of the maximized likelihoods:\n$$\\Lambda = \\frac{L_{H_{0}}^{\\max}}{L_{H_{1}}^{\\max}} = \\frac{(2\\pi\\hat{\\sigma}^{2}_{H_{0}})^{-N/2} \\exp(-N/2)}{(2\\pi\\hat{\\sigma}^{2}_{H_{1}})^{-N/2} \\exp(-N/2)} = \\left(\\frac{\\hat{\\sigma}^{2}_{H_{1}}}{\\hat{\\sigma}^{2}_{H_{0}}}\\right)^{N/2}$$\nThe LRT rejects $H_{0}$ for small values of $\\Lambda$. This is equivalent to rejecting for large values of $-2\\ln\\Lambda$:\n$$-2\\ln\\Lambda = -2 \\ln\\left( \\left(\\frac{\\hat{\\sigma}^{2}_{H_{1}}}{\\hat{\\sigma}^{2}_{H_{0}}}\\right)^{N/2} \\right) = -N \\ln\\left(\\frac{\\hat{\\sigma}^{2}_{H_{1}}}{\\hat{\\sigma}^{2}_{H_{0}}}\\right) = N \\ln\\left(\\frac{\\hat{\\sigma}^{2}_{H_{0}}}{\\hat{\\sigma}^{2}_{H_{1}}}\\right)$$\n\n**2.5. Connecting to the $t$-statistic**\nWe introduce the sum of squares decomposition familiar from ANOVA.\nThe numerator of $N\\hat{\\sigma}^{2}_{H_{1}}$ is the Residual Sum of Squares ($SS_{R}$), which is the sum of squared deviations from group means:\n$$SS_{R} = \\sum_{i=1}^{n_{1}}(x_{i} - \\bar{x})^{2} + \\sum_{j=1}^{n_{2}}(y_{j} - \\bar{y})^{2} = (n_{1}-1)s_{1}^{2} + (n_{2}-1)s_{2}^{2}$$\nFrom the problem definition, $s_{p}^{2} = \\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2} = \\frac{SS_{R}}{n_{1}+n_{2}-2}$. Thus, $SS_{R} = (n_{1}+n_{2}-2)s_{p}^{2}$.\nSo, $\\hat{\\sigma}^{2}_{H_{1}} = \\frac{SS_{R}}{N}$.\n\nThe numerator of $N\\hat{\\sigma}^{2}_{H_{0}}$ is the Total Sum of Squares ($SS_{T}$), the sum of squared deviations from the overall mean $\\hat{\\mu}_{H_0}$:\n$$SS_{T} = \\sum_{i=1}^{n_{1}}(x_{i} - \\hat{\\mu}_{H_{0}})^{2} + \\sum_{j=1}^{n_2}(y_{j} - \\hat{\\mu}_{H_{0}})^{2}$$\nA standard identity states $SS_{T} = SS_{R} + SS_{B}$, where $SS_{B}$ is the Between-group Sum of Squares:\n$$SS_{B} = n_{1}(\\bar{x}-\\hat{\\mu}_{H_{0}})^{2} + n_{2}(\\bar{y}-\\hat{\\mu}_{H_{0}})^{2} = \\frac{n_{1}n_{2}}{n_{1}+n_{2}}(\\bar{x}-\\bar{y})^{2}$$\nNow we can write the ratio of variances:\n$$\\frac{\\hat{\\sigma}^{2}_{H_{0}}}{\\hat{\\sigma}^{2}_{H_{1}}} = \\frac{SS_{T}/N}{SS_{R}/N} = \\frac{SS_{T}}{SS_{R}} = \\frac{SS_{R}+SS_{B}}{SS_{R}} = 1 + \\frac{SS_{B}}{SS_{R}}$$\nThe LR statistic becomes:\n$$-2\\ln\\Lambda = N \\ln\\left(1 + \\frac{SS_{B}}{SS_{R}}\\right) = (n_{1}+n_{2}) \\ln\\left(1 + \\frac{SS_{B}}{SS_{R}}\\right)$$\nTo finalize, we express the ratio $\\frac{SS_{B}}{SS_{R}}$ in terms of the $t$-statistic. By definition, $t = \\frac{\\bar{x}-\\bar{y}}{s_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$. Squaring this gives:\n$$t^{2} = \\frac{(\\bar{x}-\\bar{y})^{2}}{s_{p}^{2}\\left(\\frac{1}{n_{1}}+\\frac{1}{n_{2}}\\right)} = \\frac{(\\bar{x}-\\bar{y})^{2}}{s_{p}^{2}\\left(\\frac{n_{1}+n_{2}}{n_{1}n_{2}}\\right)} = \\frac{n_{1}n_{2}(\\bar{x}-\\bar{y})^{2}}{s_{p}^{2}(n_{1}+n_{2})}$$\nFrom the expression for $SS_{B}$, we have $(\\bar{x}-\\bar{y})^{2} = \\frac{n_{1}+n_{2}}{n_{1}n_{2}}SS_{B}$. Substituting this into the equation for $t^{2}$:\n$$t^{2} = \\frac{n_{1}n_{2}}{s_{p}^{2}(n_{1}+n_{2})} \\left( \\frac{n_{1}+n_{2}}{n_{1}n_{2}} SS_{B} \\right) = \\frac{SS_{B}}{s_{p}^{2}}$$\nNow we form the desired ratio:\n$$\\frac{SS_{B}}{SS_{R}} = \\frac{t^{2}s_{p}^{2}}{SS_{R}} = \\frac{t^{2}s_{p}^{2}}{(n_{1}+n_{2}-2)s_{p}^{2}} = \\frac{t^{2}}{n_{1}+n_{2}-2}$$\nSubstituting this back into the expression for $-2\\ln\\Lambda$:\n$$-2\\ln\\Lambda = (n_{1}+n_{2}) \\ln\\left(1 + \\frac{t^{2}}{n_{1}+n_{2}-2}\\right)$$\nThis final expression shows $-2\\ln\\Lambda$ as a function of $t$, $n_1$, and $n_2$. Since the natural logarithm is a strictly increasing function, $-2\\ln\\Lambda$ is a monotonically increasing function of $t^{2}$, and thus of $|t|$. This demonstrates that rejecting $H_{0}$ for large values of $-2\\ln\\Lambda$ is equivalent to rejecting $H_{0}$ for large values of $|t|$, establishing the equivalence of the LR test and the two-sided pooled $t$-test.",
            "answer": "$$\\boxed{(n_1+n_2)\\ln\\left(1 + \\frac{t^2}{n_1+n_2-2}\\right)}$$"
        },
        {
            "introduction": "In contemporary neuroscience, we often test hypotheses within the rich framework of Generalized Linear Models (GLMs), which can capture complex relationships between neural responses and experimental variables. This hands-on coding challenge simulates a common scenario: testing for a stimulus effect in a Poisson regression model while accounting for nuisance factors like adaptation. Completing this exercise  will equip you with the practical skills to implement the Generalized Likelihood Ratio Test (GLRT) by numerically optimizing the likelihood under both null and alternative hypotheses, a process that involves the essential technique of profiling over nuisance parameters.",
            "id": "4174084",
            "problem": "You are analyzing binned spike counts from a single neuron under a fixed stimulus protocol. Assume the following generative model: for each bin index $t \\in \\{1,2,\\dots,T\\}$, the observed spike count $y_t$ is an independent realization from a Poisson distribution with rate parameter $\\lambda_t$, and the rate obeys a log-linear relation with exogenous covariates. That is, $y_t \\sim \\text{Poisson}(\\lambda_t)$ and $\\log \\lambda_t = \\eta_t$, where $\\eta_t$ is an affine function of covariates specified below. The covariates include two stimulus features and a known nuisance regressor that captures adaptation effects. The model parameters are collected in a vector $\\theta$.\n\nFoundational facts to use:\n- The Poisson distribution with rate parameter $\\lambda_t$ has probability mass function $p(y_t \\mid \\lambda_t) = \\frac{\\lambda_t^{y_t} e^{-\\lambda_t}}{y_t!}$ for integer $y_t \\ge 0$.\n- Under independence across bins, the likelihood for $\\theta$ given observations $\\{y_t\\}_{t=1}^T$ is the product of marginal probabilities, and the log-likelihood is the sum of marginal log-probabilities.\n\nYou will conduct the Generalized Likelihood Ratio Test (GLRT) for the null hypothesis \"no stimulus effect\" against the general alternative in a multi-parameter setting by numerically optimizing the log-likelihood and profiling over nuisance parameters. The null hypothesis fixes the stimulus-effect parameters to zero while leaving the nuisance parameters free; the alternative hypothesis allows all parameters to vary freely.\n\nModel specification:\n- For each bin $t$, define $s_{1,t}$ and $s_{2,t}$ as stimulus features and $a_t$ as a nuisance regressor reflecting adaptation. The linear predictor is $\\eta_t = \\beta_0 + \\beta_1 s_{1,t} + \\beta_2 s_{2,t} + \\gamma a_t$, where $\\beta_0$ is the intercept (baseline log-rate), $(\\beta_1,\\beta_2)$ are stimulus-effect parameters, and $\\gamma$ is the nuisance adaptation coefficient. The full parameter vector is $\\theta = (\\beta_0, \\beta_1, \\beta_2, \\gamma)$.\n- The null hypothesis is $H_0: \\beta_1 = 0, \\beta_2 = 0$, with nuisance $(\\beta_0,\\gamma)$ free.\n- The alternative hypothesis is $H_1: (\\beta_0,\\beta_1,\\beta_2,\\gamma)$ all free.\n\nYou must compute the GLRT statistic for three deterministic test cases by numerically maximizing the log-likelihood under $H_0$ and $H_1$ and profiling over nuisance parameters as appropriate. Angles used in trigonometric functions must be interpreted in radians.\n\nTest suite:\n- Case A (moderate sample, clear stimulus effect):\n  - $T = 30$.\n  - For $t = 1,2,\\dots,30$: define $s_{1,t} = \\sin\\left(\\frac{2\\pi t}{10}\\right)$, $s_{2,t} = \\cos\\left(\\frac{2\\pi t}{7}\\right)$, and $a_t = \\exp\\left(-\\frac{t}{15}\\right)$.\n  - Construct counts deterministically from a fixed parameter set: $\\beta_0^\\star = \\log(6)$, $\\beta_1^\\star = 0.6$, $\\beta_2^\\star = -0.4$, $\\gamma^\\star = 0.3$. Define $\\eta_t^\\star = \\beta_0^\\star + \\beta_1^\\star s_{1,t} + \\beta_2^\\star s_{2,t} + \\gamma^\\star a_t$, $\\lambda_t^\\star = \\exp(\\eta_t^\\star)$, and $y_t = \\left\\lfloor \\lambda_t^\\star + 0.5 \\right\\rfloor$.\n- Case B (moderate sample, null stimulus effect):\n  - $T = 30$.\n  - For $t = 1,2,\\dots,30$: define $s_{1,t} = \\sin\\left(\\frac{2\\pi t}{10}\\right)$, $s_{2,t} = \\cos\\left(\\frac{2\\pi t}{7}\\right)$, and $a_t = \\exp\\left(-\\frac{t}{20}\\right)$.\n  - Construct counts with no stimulus effect: $\\beta_0^\\star = \\log(6)$, $\\beta_1^\\star = 0$, $\\beta_2^\\star = 0$, $\\gamma^\\star = 0.3$. Define $\\eta_t^\\star = \\beta_0^\\star + \\gamma^\\star a_t$, $\\lambda_t^\\star = \\exp(\\eta_t^\\star)$, and $y_t = \\left\\lfloor \\lambda_t^\\star + 0.5 \\right\\rfloor$.\n- Case C (small sample, weak stimulus effect):\n  - $T = 8$.\n  - For $t = 1,2,\\dots,8$: define $s_{1,t} = \\sin\\left(\\frac{2\\pi t}{6}\\right)$, $s_{2,t} = \\cos\\left(\\frac{2\\pi t}{5}\\right)$, and $a_t = \\exp\\left(-\\frac{t}{10}\\right)$.\n  - Construct counts deterministically: $\\beta_0^\\star = \\log(3)$, $\\beta_1^\\star = 0.2$, $\\beta_2^\\star = 0.1$, $\\gamma^\\star = 0.5$. Define $\\eta_t^\\star = \\beta_0^\\star + \\beta_1^\\star s_{1,t} + \\beta_2^\\star s_{2,t} + \\gamma^\\star a_t$, $\\lambda_t^\\star = \\exp(\\eta_t^\\star)$, and $y_t = \\left\\lfloor \\lambda_t^\\star + 0.5 \\right\\rfloor$.\n\nComputational requirements:\n- For each case, treat $\\{y_t\\}_{t=1}^T$ and the covariates as given data. Implement numerical optimization to obtain the maximum of the log-likelihood under $H_0$ and under $H_1$, with $H_0$ enforcing $\\beta_1 = 0$ and $\\beta_2 = 0$ and $H_1$ leaving all parameters free. This yields profiling over nuisance parameters $(\\beta_0,\\gamma)$ under $H_0$.\n- Use stable numerical methods to evaluate the log-likelihood for the Poisson model and to perform unconstrained optimization over the parameter vectors of appropriate dimension.\n- After obtaining the two maximized values, compute the GLRT statistic for each case and use the standard large-sample approximation to the null distribution, which is a chi-square distribution with degrees of freedom equal to the number of constrained parameters difference between $H_1$ and $H_0$ (here, $2$), to obtain a decision at significance level $\\alpha = 0.05$ and a p-value expressed as a decimal.\n\nFinal output specification:\n- Your program should produce a single line of output containing the GLRT statistics for the three cases, in order A, B, C, as a comma-separated list enclosed in square brackets, for example, $\\left[\\text{stat}_A,\\text{stat}_B,\\text{stat}_C\\right]$. Each element must be a floating-point number. No other text should be printed.",
            "solution": "` tag)\n\n1.  **Principle of GLRT**: I'll start by defining the Generalized Likelihood Ratio Test statistic.\n    *   Likelihood function $L(\\theta \\mid y) = \\prod_{t=1}^T p(y_t \\mid \\theta)$.\n    *   Log-likelihood $\\ell(\\theta \\mid y) = \\sum_{t=1}^T \\log p(y_t \\mid \\theta)$.\n    *   For the Poisson model:\n        $\\ell(\\theta \\mid y) = \\sum_{t=1}^T \\log\\left(\\frac{\\lambda_t^{y_t} e^{-\\lambda_t}}{y_t!}\\right) = \\sum_{t=1}^T (y_t \\log \\lambda_t - \\lambda_t - \\log(y_t!))$.\n        Since $\\log \\lambda_t = \\eta_t = \\beta_0 + \\beta_1 s_{1,t} + \\beta_2 s_{2,t} + \\gamma a_t$, we can substitute this in.\n        $\\ell(\\theta \\mid y) = \\sum_{t=1}^T (y_t (\\beta_0 + \\beta_1 s_{1,t} + \\beta_2 s_{2,t} + \\gamma a_t) - e^{\\beta_0 + \\beta_1 s_{1,t} + \\beta_2 s_{2,t} + \\gamma a_t} - \\log(y_t!))$.\n    *   The term $\\sum \\log(y_t!)$ is constant with respect to $\\theta$, so it can be dropped for optimization. The function to maximize is:\n        $\\ell^*(\\theta) = \\sum_{t=1}^T (y_t \\eta_t - e^{\\eta_t})$.\n    *   The GLRT statistic is $\\Lambda = -2 \\left( \\sup_{\\theta \\in \\Theta_0} \\ell(\\theta) - \\sup_{\\theta \\in \\Theta_1} \\ell(\\theta) \\right)$.\n        Where $\\Theta_0$ is the parameter space under $H_0$ and $\\Theta_1$ is the parameter space under $H_1$.\n        Let $\\hat{\\theta}_0$ be the MLE under $H_0$ and $\\hat{\\theta}_1$ be the MLE under $H_1$.\n        $\\Lambda = -2 (\\ell(\\hat{\\theta}_0) - \\ell(\\hat{\\theta}_1))$.\n\n2.  **Model Specifics**:\n    *   **Under $H_1$ (Alternative hypothesis)**: The model is \"full\".\n        $\\eta_t = \\beta_0 + \\beta_1 s_{1,t} + \\beta_2 s_{2,t} + \\gamma a_t$.\n        The parameter vector to optimize is $\\theta_1 = (\\beta_0, \\beta_1, \\beta_2, \\gamma)$. We need to find $\\hat{\\theta}_1 = \\text{argmax}_{\\theta_1} \\ell(\\theta_1)$. This is a 4-dimensional unconstrained optimization problem.\n    *   **Under $H_0$ (Null hypothesis)**: The model is \"reduced\".\n        $\\beta_1 = 0$, $\\beta_2 = 0$.\n        $\\eta_t = \\beta_0 + \\gamma a_t$.\n        The parameter vector to optimize is $\\theta_0 = (\\beta_0, \\gamma)$. We need to find $\\hat{\\theta}_0 = \\text{argmax}_{\\theta_0} \\ell(\\theta_0)$. This is a 2-dimensional unconstrained optimization problem.\n\n3.  **Numerical Optimization**:\n    *   I need to implement a function that calculates the negative log-likelihood (since optimizers typically minimize).\n    *   The negative of the relevant part of the log-likelihood is:\n        $f(\\theta) = -\\ell^*(\\theta) = \\sum_{t=1}^T (e^{\\eta_t} - y_t \\eta_t)$.\n    *   I will use `scipy.optimize.minimize` to find the parameters that minimize this function. A good starting point for the optimization would be a vector of zeros, or perhaps the true parameters for the case where they are known, but starting from zeros is more general.\n    *   I will define two objective functions: one for the full model (4 parameters) and one for the reduced model (2 parameters).\n\n4.  **Step-by-step calculation for each case**:\n    *   **For each Case (A, B, C)**:\n        a. Generate the covariates $s_{1,t}, s_{2,t}, a_t$ and the \"observed\" data $y_t$ according to the rules.\n        b. **Fit the full model ($H_1$)**:\n           - Define the objective function for the 4-parameter model.\n           - Use `scipy.optimize.minimize` to find $\\hat{\\theta}_1 = (\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\gamma})$.\n           - Calculate the maximized log-likelihood value, $\\ell(\\hat{\\theta}_1)$. Note that the optimizer returns the minimum of the *negative* log-likelihood, so $\\ell(\\hat{\\theta}_1) = -\\text{min_val}$.\n        c. **Fit the reduced model ($H_0$)**:\n           - Define the objective function for the 2-parameter model.\n           - Use `scipy.optimize.minimize` to find the MLEs for the nuisance parameters, let's call them $\\tilde{\\theta}_0 = (\\tilde{\\beta}_0, \\tilde{\\gamma})$. The full parameter vector under $H_0$ is $\\hat{\\theta}_0 = (\\tilde{\\beta}_0, 0, 0, \\tilde{\\gamma})$.\n           - Calculate the maximized log-likelihood value, $\\ell(\\hat{\\theta}_0)$.\n        d. **Calculate the GLRT statistic**:\n           - $\\Lambda = 2 (\\ell(\\hat{\\theta}_1) - \\ell(\\hat{\\theta}_0))$.\n\n5.  **Final Polish**: Ensure all mathematical entities are in LaTeX and the explanation flows logically. I will mention that the log-likelihood function for the Poisson GLM is convex, which guarantees a unique maximum, making numerical optimization well-behaved.\n\n---\n\n#### Part 3: Code Implementation (`<final_answer>` tag)\n\n1.  **Structure**:\n    *   Create a `solve()` function.\n    *   Inside `solve()`, define the test cases as a list of tuples or dictionaries. This will make the code clean. Each element in the list will contain all the necessary parameters for a case ($T$, $\\beta^\\star$ vector, and rules for covariates).\n    *   Loop through the test cases.\n    *   Inside the loop, for each case:\n        a. **Generate Data**: Write code to generate $s_1, s_2, a, y$ vectors using `numpy`. This is deterministic.\n        b. **Define Log-Likelihood Functions**:\n           - Create a function `neg_log_likelihood_h1(params, s1, s2, a, y)` that takes a 4-element `params` vector and the data.\n           - Create a function `neg_log_likelihood_h0(params, a, y)` that takes a 2-element `params` vector and the relevant data.\n        c. **Optimization**:\n           - Use `scipy.optimize.minimize` to find the minimum of `neg_log_likelihood_h1`. I'll use `method='BFGS'` or `'L-BFGS-B'` which are good quasi-Newton methods. A starting guess `x0` of all zeros is a reasonable choice.\n           - `res_h1 = minimize(neg_log_likelihood_h1, x0=[0,0,0,0], args=(...))`\n           - `max_ll_h1 = -res_h1.fun`\n           - Use `scipy.optimize.minimize` to find the minimum of `neg_log_likelihood_h0`.\n           - `res_h0 = minimize(neg_log_likelihood_h0, x0=[0,0], args=(...))`\n           - `max_ll_h0 = -res_h0.fun`\n        d. **Compute Statistic**:\n           - `glrt_stat = 2 * (max_ll_h1 - max_ll_h0)`\n        e. **Store Result**: Append `glrt_stat` to a `results` list.\n    *   **Final Print**: After the loop, format the `results` list into the required string `\"[stat_A,stat_B,stat_C]\"` and print it.\n\n2.  **Imports**: `import numpy as np`, `from scipy.optimize import minimize`. This matches the requirements.\n\n3.  **Code Details**:\n    *   The `neg_log_likelihood` functions will compute $\\eta_t$ for all $t$ using vector operations, then compute $\\sum (e^{\\eta_t} - y_t \\eta_t)$. This will be efficient.\n    *   Example for `neg_log_likelihood_h1`:\n        ```python\n        def neg_log_likelihood_h1(params, s1, s2, a, y):\n            beta0, beta1, beta2, gamma = params\n            eta = beta0 + beta1 * s1 + beta2 * s2 + gamma * a\n            # A small guard against overflow in exp(eta)\n            # if eta is very large, the term is dominated by exp(eta).\n            # If eta is very negative, it's dominated by -y*eta.\n            # SciPy's optimizers are generally robust to this.\n            log_lambda = eta\n            lambda_ = np.exp(log_lambda)\n            # The log-likelihood to be maximized is sum(y * log_lambda - lambda_)\n            # We minimize its negative\n            return np.sum(lambda_ - y * log_lambda)\n        ```\n    *   Example for `neg_log_likelihood_h0`:\n        ```python\n        def neg_log_likelihood_h0(params, a, y):\n            beta0, gamma = params\n            eta = beta0 + gamma * a\n            log_lambda = eta\n            lambda_ = np.exp(log_lambda)\n            return np.sum(lambda_ - y * log_lambda)\n        ```\n    *   The problem statement says to treat the data $\\{y_t\\}$ as given. The deterministic generation rule $y_t = \\left\\lfloor \\lambda_t^\\star + 0.5 \\right\\rfloor$ is simply how the test case data is constructed. It's equivalent to standard rounding to the nearest integer. `np.round()` does this. (`floor(x+0.5)` is equivalent to standard rounding for non-negative x, except for half-integers where it rounds up, while `np.round` rounds to the nearest even integer. The results might be slightly different. `np.floor(x + 0.5).astype(int)` should be used to be precise).\n\n4.  **Testing**: I will mentally run through Case C (small sample) to check the logic.\n    *   $T=8$.\n    *   $\\beta^\\star = (\\log(3), 0.2, 0.1, 0.5)$.\n    *   $s_{1,t} = \\sin(2\\pi t/6)$, $s_{2,t} = \\cos(2\\pi t/5)$, $a_t = \\exp(-t/10)$.\n    *   I'll need to generate these arrays for $t=1, \\dots, 8$.\n    *   Then calculate $\\eta_t^\\star$ and $\\lambda_t^\\star = \\exp(\\eta_t^\\star)$.\n    *   Then $y_t = \\text{round}(\\lambda_t^\\star)$. `np.round()` does this. (`floor(x+0.5)` is equivalent to standard rounding for non-negative x, except for half-integers where it rounds up, while `np.round` rounds to the nearest even integer. The results might be slightly different. `np.floor(x + 0.5).astype(int)` should be used to be precise).\n    *   Then, with $y_t, s_{1,t}, s_{2,t}, a_t$, I'll perform the two optimizations and compute the statistic. The logic seems sound.\n\nLet's double-check the GLRT formula.\n$\\Lambda = -2 \\log \\frac{\\sup_{\\theta \\in \\Theta_0} L(\\theta)}{\\sup_{\\theta \\in \\Theta_1} L(\\theta)} = -2 (\\log(\\sup L_0) - \\log(\\sup L_1)) = -2(\\ell(\\hat{\\theta}_0) - \\ell(\\hat{\\theta}_1)) = 2(\\ell(\\hat{\\theta}_1) - \\ell(\\hat{\\theta}_0))$.\nThe formula is correct.\n\nThe value of the log-likelihood is $\\sum (y_t \\log \\lambda_t - \\lambda_t - \\log(y_t!))$.\nThe optimizer minimizes $\\sum (\\lambda_t - y_t \\log \\lambda_t)$.\nLet the minimized value be $V$. So $V = \\min \\sum (\\lambda_t - y_t \\log \\lambda_t)$.\nThe maximized log-likelihood part is $\\max \\sum (y_t \\log \\lambda_t - \\lambda_t) = -V$.\nSo, $\\ell(\\hat{\\theta}) = -V - \\sum \\log(y_t!)$.\nThen $\\ell(\\hat{\\theta}_1) = -V_1 - \\sum \\log(y_t!)$ and $\\ell(\\hat{\\theta}_0) = -V_0 - \\sum \\log(y_t!)$.\nThe statistic is $\\Lambda = 2(\\ell(\\hat{\\theta}_1) - \\ell(\\hat{\\theta}_0)) = 2( (-V_1 - \\sum \\log(y_t!)) - (-V_0 - \\sum \\log(y_t!)) ) = 2(-V_1 + V_0) = 2(V_0 - V_1)$.\n$V_0$ is the minimum of the negative log-likelihood under $H_0$.\n$V_1$ is the minimum of the negative log-likelihood under $H_1$.\nMy previously derived formula `2 * (max_ll_h1 - max_ll_h0)` is correct if `max_ll_h1 = -V1` and `max_ll_h0 = -V0`. So `2 * (-V1 - (-V0)) = 2 * (V0 - V1)`. This is correct.\n\nThe problem is well-defined and I have a clear path to the solution and the code. I will now proceed with writing the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nimport math\n\ndef solve():\n    \"\"\"\n    Computes the Generalized Likelihood Ratio Test (GLRT) statistic for three\n    test cases of a Poisson log-linear model.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"T\": 30,\n            \"s1_func\": lambda t: np.sin(2 * np.pi * t / 10),\n            \"s2_func\": lambda t: np.cos(2 * np.pi * t / 7),\n            \"a_func\": lambda t: np.exp(-t / 15),\n            \"params_star\": [np.log(6), 0.6, -0.4, 0.3], # beta0, beta1, beta2, gamma\n        },\n        {\n            \"name\": \"Case B\",\n            \"T\": 30,\n            \"s1_func\": lambda t: np.sin(2 * np.pi * t / 10),\n            \"s2_func\": lambda t: np.cos(2 * np.pi * t / 7),\n            \"a_func\": lambda t: np.exp(-t / 20),\n            \"params_star\": [np.log(6), 0.0, 0.0, 0.3],\n        },\n        {\n            \"name\": \"Case C\",\n            \"T\": 8,\n            \"s1_func\": lambda t: np.sin(2 * np.pi * t / 6),\n            \"s2_func\": lambda t: np.cos(2 * np.pi * t / 5),\n            \"a_func\": lambda t: np.exp(-t / 10),\n            \"params_star\": [np.log(3), 0.2, 0.1, 0.5],\n        },\n    ]\n\n    results = []\n\n    # Objective function for the alternative hypothesis H1 (full model)\n    def neg_log_likelihood_h1(params, s1, s2, a, y):\n        # params: [beta0, beta1, beta2, gamma]\n        beta0, beta1, beta2, gamma = params\n        eta = beta0 + beta1 * s1 + beta2 * s2 + gamma * a\n        lambda_t = np.exp(eta)\n        # We minimize the negative of the log-likelihood, ignoring constant terms\n        # -sum(y*log(lambda) - lambda) = sum(lambda - y*eta)\n        return np.sum(lambda_t - y * eta)\n\n    # Objective function for the null hypothesis H0 (reduced model)\n    def neg_log_likelihood_h0(params, a, y):\n        # params: [beta0, gamma]\n        beta0, gamma = params\n        eta = beta0 + gamma * a\n        lambda_t = np.exp(eta)\n        return np.sum(lambda_t - y * eta)\n\n    for case in test_cases:\n        T = case[\"T\"]\n        t_range = np.arange(1, T + 1)\n        \n        # Generate covariates\n        s1 = case[\"s1_func\"](t_range)\n        s2 = case[\"s2_func\"](t_range)\n        a = case[\"a_func\"](t_range)\n        \n        # Generate deterministic spike counts\n        b0_star, b1_star, b2_star, g_star = case[\"params_star\"]\n        eta_star = b0_star + b1_star * s1 + b2_star * s2 + g_star * a\n        lambda_star = np.exp(eta_star)\n        y = np.floor(lambda_star + 0.5).astype(int)\n\n        # Optimization under H1 (full model)\n        # Initial guess for the 4 parameters\n        x0_h1 = np.zeros(4)\n        res_h1 = minimize(neg_log_likelihood_h1, x0_h1, args=(s1, s2, a, y), method='BFGS')\n        min_neg_ll_h1 = res_h1.fun\n\n        # Optimization under H0 (reduced model)\n        # Initial guess for the 2 parameters (beta0, gamma)\n        x0_h0 = np.zeros(2)\n        res_h0 = minimize(neg_log_likelihood_h0, x0_h0, args=(a, y), method='BFGS')\n        min_neg_ll_h0 = res_h0.fun\n        \n        # The maximized log-likelihood (up to a constant) is the negative\n        # of the minimized value of the objective function.\n        max_ll_h1 = -min_neg_ll_h1\n        max_ll_h0 = -min_neg_ll_h0\n\n        # Calculate the GLRT statistic\n        # Lambda = 2 * (max_log_likelihood(H1) - max_log_likelihood(H0))\n        glrt_stat = 2 * (max_ll_h1 - max_ll_h0)\n        \n        # The problem statement mentions checking the result, but not including it\n        # in the output. For reference, the critical value for chi-square with 2\n        # degrees of freedom at alpha=0.05 is ~5.99.\n        # import scipy.stats\n        # p_value = 1 - scipy.stats.chi2.cdf(glrt_stat, df=2)\n        \n        results.append(glrt_stat)\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The large-sample approximation of the LRT statistic to a $\\chi^2$ distribution, a result of Wilks' theorem, is powerful but depends on crucial regularity conditions. This advanced problem explores a common scenario where these conditions are violated: testing a parameter that lies on the boundary of its parameter space under the null hypothesis, such as testing for the presence of zero-inflation ($\\psi=0$) in a spike count model. By deriving the correct asymptotic null distribution—a mixture of a point mass at zero and a $\\chi^2$ distribution—you will learn to handle these non-standard but important cases correctly, avoiding erroneous p-values and ensuring the statistical rigor of your conclusions .",
            "id": "4174137",
            "problem": "Consider spike count data recorded from a single neuron across $n$ non-overlapping time bins, producing observations $\\{Y_{i}\\}_{i=1}^{n}$ where each $Y_{i} \\in \\{0,1,2,\\dots\\}$. To capture excess zeros due to unobserved network states or refractory periods, analysts sometimes posit a zero-inflated count model, in which, conditional on parameters, a fixed fraction of trials are structural zeros. Specifically, suppose the data are modeled by a zero-inflated Poisson distribution with parameters $(\\lambda,\\psi)$, where $\\lambda>0$ is the Poisson rate and $\\psi \\in [0,1)$ is the zero-inflation parameter representing the probability of a structural zero. The model is\n$$\n\\mathbb{P}(Y=y \\mid \\lambda,\\psi)=\n\\begin{cases}\n\\psi+(1-\\psi)\\exp(-\\lambda), & y=0, \\\\\n(1-\\psi)\\exp(-\\lambda)\\frac{\\lambda^{y}}{y!}, & y\\in\\{1,2,\\dots\\}.\n\\end{cases}\n$$\nWe are interested in testing for the presence of zero inflation using a Likelihood Ratio Test (LRT) of the null hypothesis $H_{0}\\!:\\ \\psi=0$ versus the alternative $H_{1}\\!:\\ \\psi>0$. Under $H_{0}$ the parameter $\\psi$ is on the boundary of its parameter space $[0,1)$.\n\nLet $\\ell_{n}(\\lambda,\\psi)$ denote the total log-likelihood for the sample $\\{Y_{i}\\}_{i=1}^{n}$ under the zero-inflated Poisson model, and define the LRT statistic\n$$\n\\Lambda_{n}=2\\left\\{\\sup_{\\lambda>0,\\ \\psi\\in[0,1)}\\ \\ell_{n}(\\lambda,\\psi)\\ -\\ \\sup_{\\lambda>0,\\ \\psi=0}\\ \\ell_{n}(\\lambda,0)\\right\\}.\n$$\nAssume standard regularity conditions for maximum likelihood estimation, including differentiability of $\\ell_{n}$ in a neighborhood of $(\\lambda,\\psi)=(\\lambda_{0},0)$, existence of Fisher information, and asymptotic normality of the score under $H_{0}$ after suitable reparameterization to handle nuisance effects of $\\lambda$.\n\nStarting from the core definitions of the score and Fisher information, and using a second-order Taylor expansion of the log-likelihood around $(\\lambda_{0},\\psi=0)$, derive the large-sample ($n\\to\\infty$) null distribution of the LRT statistic $\\Lambda_{n}$ for the boundary case $\\psi\\ge 0$. Express your final result as a single closed-form analytic expression for the cumulative distribution function $F_{\\Lambda}(t)$ of the limiting distribution for $t\\ge 0$, in terms of the standard normal cumulative distribution function $\\Phi(\\cdot)$. No numerical evaluation is required, and no units are involved. The final answer must be a single analytic expression.",
            "solution": "The user-provided problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   **Data**: $\\{Y_{i}\\}_{i=1}^{n}$ are $n$ independent and identically distributed observations from a zero-inflated Poisson distribution, where $Y_{i} \\in \\{0,1,2,\\dots\\}$.\n-   **Model**: The probability mass function is given by\n    $$\n    \\mathbb{P}(Y=y \\mid \\lambda,\\psi)=\n    \\begin{cases}\n    \\psi+(1-\\psi)\\exp(-\\lambda), & y=0, \\\\\n    (1-\\psi)\\exp(-\\lambda)\\frac{\\lambda^{y}}{y!}, & y\\in\\{1,2,\\dots\\}.\n    \\end{cases}\n    $$\n-   **Parameters**: The Poisson rate $\\lambda > 0$ and the zero-inflation probability $\\psi \\in [0,1)$.\n-   **Hypothesis Test**: The null hypothesis is $H_{0}: \\psi=0$ versus the alternative hypothesis $H_1: \\psi>0$.\n-   **Test Statistic**: The Likelihood Ratio Test (LRT) statistic is\n    $$\n    \\Lambda_{n}=2\\left\\{\\sup_{\\lambda>0,\\ \\psi\\in[0,1)}\\ \\ell_{n}(\\lambda,\\psi)\\ -\\ \\sup_{\\lambda>0,\\ \\psi=0}\\ \\ell_{n}(\\lambda,0)\\right\\},\n    $$\n    where $\\ell_n(\\lambda, \\psi)$ is the total log-likelihood.\n-   **Assumptions**: Standard regularity conditions for maximum likelihood estimation are assumed to hold, and we are interested in the large-sample ($n\\to\\infty$) limit.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. The zero-inflated Poisson model is a standard statistical tool used in many fields, including neuroscience, to model count data with an excess of zeros. The likelihood ratio test is a fundamental method of hypothesis testing. The problem is well-posed, asking for the derivation of a limiting distribution under a specific, well-defined setup. The core of the problem lies in the fact that the null hypothesis $H_0: \\psi=0$ places the parameter $\\psi$ on the boundary of its parameter space $[0,1)$, which is a known complication in asymptotic theory. The problem is objective and free of ambiguities. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full derivation of the solution will now be provided.\n\n### Derivation of the Limiting Distribution\nThe problem requires the derivation of the asymptotic distribution of the LRT statistic $\\Lambda_n$ for testing $H_0: \\psi=0$ against $H_1: \\psi>0$. The crucial feature is that the null value $\\psi=0$ lies on the boundary of the parameter space for $\\psi$, which is $[0,1)$. This configuration violates a key regularity condition for the standard Wilks' theorem, which would otherwise predict a $\\chi_1^2$ limiting distribution. Therefore, a different asymptotic theory, as developed by Chernoff (1954) and Self & Liang (1987) for parameters on a boundary, must be invoked.\n\nLet $\\hat{\\theta}_0 = (\\hat{\\lambda}_0, 0)$ be the maximum likelihood estimator (MLE) of the parameters $(\\lambda, \\psi)$ under the null hypothesis $H_0$. Here, $\\hat{\\lambda}_0$ is the MLE for a standard Poisson model, which is the sample mean $\\bar{Y}$. Let $\\hat{\\theta}_1 = (\\hat{\\lambda}_1, \\hat{\\psi}_1)$ be the MLE under the alternative hypothesis, where the maximization is over $\\lambda>0$ and $\\psi \\in [0,1)$.\n\nThe asymptotic behavior of $\\Lambda_n$ is determined by the behavior of the log-likelihood function in the neighborhood of the true parameter value under $H_0$. The MLE under the alternative, $\\hat{\\psi}_1$, is constrained to be non-negative. Its value depends on the gradient of the log-likelihood function with respect to $\\psi$ at the null-restricted MLE, $(\\hat{\\lambda}_0, 0)$. Let $S_{\\psi, a} = \\frac{\\partial \\ell_n}{\\partial \\psi}\\big|_{(\\hat{\\lambda}_0, 0)}$ be the score for $\\psi$ evaluated at the null MLE.\n\nUnder $H_0$, and by the assumption of asymptotic normality of the score, $S_{\\psi, a}$ (after proper scaling by $\\frac{1}{\\sqrt{n}}$) is asymptotically normal with a mean of $0$. Consequently, for large $n$, the probability that $S_{\\psi, a}$ is positive is approximately $1/2$, and the probability that it is non-positive is approximately $1/2$.\n\nWe analyze two cases:\n\n1.  **Case 1: $S_{\\psi, a} \\le 0$.**\n    If the score for $\\psi$ is non-positive, the log-likelihood function does not increase as $\\psi$ moves from $0$ into the parameter space $(0,1)$. Given the constraint $\\psi \\ge 0$, the maximum of the likelihood function under the alternative is achieved at the boundary, i.e., $\\hat{\\psi}_1 = 0$. In this scenario, the MLE under the alternative is identical to the MLE under the null: $(\\hat{\\lambda}_1, \\hat{\\psi}_1) = (\\hat{\\lambda}_0, 0)$. Consequently, the log-likelihoods are equal, and the LRT statistic is $\\Lambda_n=0$. This event occurs with an asymptotic probability of $1/2$.\n\n2.  **Case 2: $S_{\\psi, a} > 0$.**\n    If the score for $\\psi$ is positive, the log-likelihood function is increasing in $\\psi$ at $\\psi=0$. This implies that the MLE under the alternative will be in the interior of the parameter space, i.e., $\\hat{\\psi}_1 > 0$. In this situation, the boundary constraint is not active. The problem locally behaves like an unconstrained test of a single parameter. For this subset of the sample space, the LRT statistic $\\Lambda_n$ converges in distribution to a chi-squared random variable with $1$ degree of freedom, $\\chi_1^2$. This event occurs with an asymptotic probability of $1/2$.\n\nCombining these two outcomes, the limiting distribution of $\\Lambda_n$ is a mixture of a point mass at $0$ and a $\\chi_1^2$ distribution, with equal weights of $1/2$. Formally, if $\\Lambda$ is a random variable following this limiting distribution, we can write:\n$$\n\\Lambda \\sim \\frac{1}{2}\\chi_0^2 + \\frac{1}{2}\\chi_1^2\n$$\nwhere $\\chi_0^2$ represents a degenerate distribution with a point mass at $0$.\n\nWe now derive the cumulative distribution function (CDF), $F_{\\Lambda}(t) = \\mathbb{P}(\\Lambda \\le t)$, for $t \\ge 0$.\nThe CDF of the mixture is the weighted average of the individual CDFs:\n$$\nF_{\\Lambda}(t) = \\frac{1}{2} F_{\\chi_0^2}(t) + \\frac{1}{2} F_{\\chi_1^2}(t)\n$$\nFor $t \\ge 0$, the CDF of a point mass at $0$ is $F_{\\chi_0^2}(t) = \\mathbb{P}(0 \\le t) = 1$.\nThe CDF of a $\\chi_1^2$ distribution, $F_{\\chi_1^2}(t)$, can be expressed in terms of the standard normal CDF, $\\Phi(\\cdot)$. A $\\chi_1^2$ random variable is the square of a standard normal random variable, $Z \\sim N(0,1)$. Thus, for $t \\ge 0$:\n$$\nF_{\\chi_1^2}(t) = \\mathbb{P}(Z^2 \\le t) = \\mathbb{P}(-\\sqrt{t} \\le Z \\le \\sqrt{t})\n$$\n$$\nF_{\\chi_1^2}(t) = \\Phi(\\sqrt{t}) - \\Phi(-\\sqrt{t})\n$$\nUsing the symmetry property of the standard normal distribution, $\\Phi(-z) = 1 - \\Phi(z)$, we get:\n$$\nF_{\\chi_1^2}(t) = \\Phi(\\sqrt{t}) - (1 - \\Phi(\\sqrt{t})) = 2\\Phi(\\sqrt{t}) - 1\n$$\nSubstituting these results back into the expression for $F_{\\Lambda}(t)$:\n$$\nF_{\\Lambda}(t) = \\frac{1}{2}(1) + \\frac{1}{2}(2\\Phi(\\sqrt{t}) - 1)\n$$\n$$\nF_{\\Lambda}(t) = \\frac{1}{2} + \\Phi(\\sqrt{t}) - \\frac{1}{2}\n$$\n$$\nF_{\\Lambda}(t) = \\Phi(\\sqrt{t})\n$$\nThis is the closed-form analytic expression for the CDF of the limiting distribution of $\\Lambda_n$ for $t \\ge 0$.",
            "answer": "$$\n\\boxed{\\Phi(\\sqrt{t})}\n$$"
        }
    ]
}