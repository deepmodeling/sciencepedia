{
    "hands_on_practices": [
        {
            "introduction": "A fundamental step in designing any neuroscience experiment is determining the necessary sample size. Collecting too little data can render an experiment inconclusive, while collecting too much wastes time and resources. This exercise  provides hands-on practice in one of the most common forms of sample size planning: calculating the number of trials needed to estimate a binomial probability—such as the probability of a neuron spiking in response to a stimulus—with a predetermined level of precision for a confidence interval.",
            "id": "4159958",
            "problem": "In a single-neuron stimulus–response experiment, each trial is coded as a Bernoulli random variable $X_i \\in \\{0,1\\}$ indicating whether at least one action potential (spike) occurs within a fixed post-stimulus analysis window ($X_i=1$ for spike, $X_i=0$ otherwise). Assume trials are independent and identically distributed with $\\Pr(X_i=1)=p$. A pilot session produced $n_0=60$ trials with $\\sum_{i=1}^{n_0} X_i=12$, yielding the pilot estimator $\\hat{p}_0=12/60$. You plan a new session with $n$ trials to estimate $p$ by the sample proportion $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n X_i$.\n\nStarting from the Central Limit Theorem for $\\hat{p}$ and the definition of a two-sided $(1-\\alpha)$ confidence interval (CI) for $p$ based on the standard normal quantile $z_{1-\\alpha/2}$ (defined by $\\Pr(Z \\le z_{1-\\alpha/2})=1-\\alpha/2$ for $Z\\sim \\mathcal{N}(0,1)$), derive the leading-order approximation to the CI width as a function of $p$ and $n$. Then, using the pilot estimate as the anticipated value of $p$ (i.e., plug in $p=\\hat{p}_0$), determine the smallest integer $n$ such that the approximate CI width is at most the target width $w=0.10$ for confidence level $1-\\alpha=0.95$.\n\nReport the minimal integer $n$ that meets the width requirement. No units are needed.",
            "solution": "The user has provided a well-posed and scientifically grounded problem statement that requires the calculation of the necessary sample size for a binomial proportion confidence interval. The problem is valid, and I will proceed with a full solution.\n\nThe problem requires determining the sample size $n$ for a new experiment. The data from each trial, $X_i$, is a Bernoulli random variable with parameter $p$, where $p$ is the probability of observing a spike. The trials are independent and identically distributed. The estimator for $p$ is the sample proportion, $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n X_i$.\n\nThe mean and variance of the estimator $\\hat{p}$ are given by:\n$$ E[\\hat{p}] = p $$\n$$ \\text{Var}(\\hat{p}) = \\frac{p(1-p)}{n} $$\n\nAccording to the Central Limit Theorem (CLT), for a sufficiently large sample size $n$, the sampling distribution of $\\hat{p}$ is approximately normal. We can standardize $\\hat{p}$ to obtain a variable that is approximately distributed as a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$:\n$$ Z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\approx \\mathcal{N}(0, 1) $$\n\nA two-sided $(1-\\alpha)$ confidence interval (CI) for $p$ is constructed based on this approximation. Let $z_{1-\\alpha/2}$ be the quantile of the standard normal distribution such that $\\Pr(Z \\le z_{1-\\alpha/2}) = 1-\\alpha/2$. The interval is defined by the probability statement:\n$$ \\Pr\\left(-z_{1-\\alpha/2} \\le \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\le z_{1-\\alpha/2}\\right) \\approx 1-\\alpha $$\n\nTo obtain a solvable interval for $p$, the standard (Wald-type) approximation replaces the unknown parameter $p$ in the denominator (the standard error term) with its estimate $\\hat{p}$. This gives the CI:\n$$ \\hat{p} \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} $$\n\nThe width of this confidence interval, which we denote as $W$, is the difference between the upper and lower bounds:\n$$ W = \\left(\\hat{p} + z_{1-\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right) - \\left(\\hat{p} - z_{1-\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right) = 2 z_{1-\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} $$\n\nFor the purpose of sample size calculation, we are determining $n$ before the experiment is run, so we do not have a value for $\\hat{p}$. The problem asks for the approximate CI width as a function of the true proportion $p$ and the sample size $n$. This formula becomes the basis for our planning:\n$$ W(p, n) = 2 z_{1-\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}} $$\n\nWe are tasked with finding the smallest integer $n$ such that this anticipated width is at most a target width $w = 0.10$. The problem specifies that we should use the pilot estimate $\\hat{p}_0$ as the anticipated value for $p$. The pilot study provides:\n$$ n_0 = 60 $$\n$$ \\sum_{i=1}^{n_0} X_i = 12 $$\n$$ \\hat{p}_0 = \\frac{12}{60} = \\frac{1}{5} = 0.2 $$\n\nThe confidence level is specified as $1-\\alpha = 0.95$. This implies $\\alpha = 0.05$, and $\\alpha/2 = 0.025$. The required normal quantile is $z_{1-0.025} = z_{0.975}$. From standard statistical tables, the value for $z_{0.975}$ is approximately $1.96$.\n\nWe set up the inequality for the sample size $n$:\n$$ W(\\hat{p}_0, n) \\le w $$\n$$ 2 z_{0.975} \\sqrt{\\frac{\\hat{p}_0(1-\\hat{p}_0)}{n}} \\le w $$\n\nTo solve for $n$, we rearrange the inequality:\n$$ \\sqrt{\\frac{\\hat{p}_0(1-\\hat{p}_0)}{n}} \\le \\frac{w}{2 z_{0.975}} $$\nSquaring both sides, which are positive, we get:\n$$ \\frac{\\hat{p}_0(1-\\hat{p}_0)}{n} \\le \\left(\\frac{w}{2 z_{0.975}}\\right)^2 $$\nFinally, we isolate $n$:\n$$ n \\ge \\hat{p}_0(1-\\hat{p}_0) \\left(\\frac{2 z_{0.975}}{w}\\right)^2 $$\n\nNow, we substitute the specified values: $\\hat{p}_0 = 0.2$, $w = 0.10$, and $z_{0.975} \\approx 1.96$.\n$$ n \\ge 0.2(1-0.2) \\left(\\frac{2 \\times 1.96}{0.10}\\right)^2 $$\n$$ n \\ge 0.2(0.8) \\left(\\frac{3.92}{0.10}\\right)^2 $$\n$$ n \\ge 0.16 \\times (39.2)^2 $$\n$$ n \\ge 0.16 \\times 1536.64 $$\n$$ n \\ge 245.8624 $$\n\nSince the number of trials $n$ must be an integer, we must take the smallest integer value that satisfies this condition. Therefore, we round up to the next integer.\n$$ n = 246 $$\nThis is the minimal sample size required to achieve a confidence interval width of at most $0.10$ with $95\\%$ confidence, based on the pilot data.",
            "answer": "$$\n\\boxed{246}\n$$"
        },
        {
            "introduction": "Comparing the average neural response between two experimental conditions is a cornerstone of systems neuroscience. However, the standard t-test relies on the often-unrealistic assumption that the trial-to-trial variability is identical across conditions. This practice  tackles this real-world challenge by guiding you through the application of Welch's t-test, a more robust alternative for comparing means when variances are unequal. You will derive and apply the Satterthwaite approximation to find the correct degrees of freedom, ensuring your statistical inferences are valid.",
            "id": "4159938",
            "problem": "A single cortical neuron was recorded under two sensory stimulus conditions, labeled condition $\\mathrm{A}$ and condition $\\mathrm{B}$. For each condition, independent trials were collected, and the spike rate (in spikes per second) was computed per trial. Empirically, trial-to-trial variability differs across conditions, so an equal-variance assumption is not justified. Let the within-condition sample sizes be $n_{\\mathrm{A}} = 40$ and $n_{\\mathrm{B}} = 30$, the within-condition sample means be $\\bar{x}_{\\mathrm{A}} = 22.1$ and $\\bar{x}_{\\mathrm{B}} = 18.9$, and the unbiased within-condition sample variances be $s_{\\mathrm{A}}^{2} = 64.0$ and $s_{\\mathrm{B}}^{2} = 36.0$, all expressed in units of spikes per second for means and squared spikes per second for variances. Assume independent sampling across trials and conditions, finite second moments, and that for the estimators involved, normal approximation applies by the Central Limit Theorem (CLT).\n\nStarting from the definitions of the sample mean, unbiased sample variance, and properties of expectations and variances for independent samples, and without imposing an equal-variance assumption, derive the point estimator for the mean difference $\\Delta = \\mu_{\\mathrm{A}} - \\mu_{\\mathrm{B}}$, its estimated standard error, and the two-sided confidence interval with confidence level $0.95$ for $\\Delta$ under the heteroscedastic two-sample setting (Welch’s framework). In order to ensure correct coverage under heteroscedasticity, derive the Satterthwaite approximation for the degrees of freedom used in the corresponding Student’s $t$ quantile. Then, using the provided data, compute the Satterthwaite degrees of freedom numerically.\n\nExpress any confidence interval endpoints in spikes per second. Report only the Satterthwaite degrees of freedom as your final answer, and round it to $4$ significant figures. The degrees of freedom is a dimensionless quantity.",
            "solution": "The problem is valid as it represents a standard and well-posed statistical estimation task (the Behrens-Fisher problem) grounded in realistic biophysical data analysis. It provides all necessary information, is free of contradictions, and asks for a specific derivation and calculation based on established statistical principles.\n\nThe goal is to derive the point estimator, its standard error, and the associated confidence interval for the difference in mean spike rates between two conditions, $\\Delta = \\mu_{\\mathrm{A}} - \\mu_{\\mathrm{B}}$, without assuming equal variances. This requires the Welch-Satterthwaite approximation for the degrees of freedom.\n\nLet the random variables representing the spike rates for the two conditions be $X_{\\mathrm{A}}$ and $X_{\\mathrm{B}}$, with population means $E[X_{\\mathrm{A}}] = \\mu_{\\mathrm{A}}$ and $E[X_{\\mathrm{B}}] = \\mu_{\\mathrm{B}}$, and population variances $\\text{Var}(X_{\\mathrm{A}}) = \\sigma_{\\mathrm{A}}^2$ and $\\text{Var}(X_{\\mathrm{B}}) = \\sigma_{\\mathrm{B}}^2$. The data consist of two independent samples of sizes $n_{\\mathrm{A}}$ and $n_{\\mathrm{B}}$.\n\n**1. Point Estimator and its Properties**\n\nThe parameter of interest is the difference between the population means, $\\Delta = \\mu_{\\mathrm{A}} - \\mu_{\\mathrm{B}}$. A natural point estimator for $\\Delta$ is the difference between the sample means, $\\hat{\\Delta} = \\bar{X}_{\\mathrm{A}} - \\bar{X}_{\\mathrm{B}}$.\n\nThe estimator $\\hat{\\Delta}$ is unbiased, as its expected value is equal to the true parameter $\\Delta$:\n$$ E[\\hat{\\Delta}] = E[\\bar{X}_{\\mathrm{A}} - \\bar{X}_{\\mathrm{B}}] = E[\\bar{X}_{\\mathrm{A}}] - E[\\bar{X}_{\\mathrm{B}}] = \\mu_{\\mathrm{A}} - \\mu_{\\mathrm{B}} = \\Delta $$\n\nThe variance of this estimator, assuming independent samples, is the sum of the variances of the individual sample means:\n$$ \\text{Var}(\\hat{\\Delta}) = \\text{Var}(\\bar{X}_{\\mathrm{A}} - \\bar{X}_{\\mathrm{B}}) = \\text{Var}(\\bar{X}_{\\mathrm{A}}) + \\text{Var}(\\bar{X}_{\\mathrm{B}}) $$\nSince $\\text{Var}(\\bar{X}) = \\frac{\\sigma^2}{n}$, we have:\n$$ \\text{Var}(\\hat{\\Delta}) = \\frac{\\sigma_{\\mathrm{A}}^2}{n_{\\mathrm{A}}} + \\frac{\\sigma_{\\mathrm{B}}^2}{n_{\\mathrm{B}}} $$\n\n**2. Estimated Standard Error**\n\nThe true population variances $\\sigma_{\\mathrm{A}}^2$ and $\\sigma_{\\mathrm{B}}^2$ are unknown. We estimate them using the unbiased sample variances, $s_{\\mathrm{A}}^2$ and $s_{\\mathrm{B}}^2$. The estimated variance of $\\hat{\\Delta}$ is:\n$$ \\widehat{\\text{Var}}(\\hat{\\Delta}) = \\frac{s_{\\mathrm{A}}^2}{n_{\\mathrm{A}}} + \\frac{s_{\\mathrm{B}}^2}{n_{\\mathrm{B}}} $$\nThe estimated standard error of the difference is the square root of this estimated variance:\n$$ \\widehat{SE}(\\hat{\\Delta}) = \\sqrt{\\frac{s_{\\mathrm{A}}^2}{n_{\\mathrm{A}}} + \\frac{s_{\\mathrm{B}}^2}{n_{\\mathrm{B}}} } $$\n\n**3. Confidence Interval and Welch's t-statistic**\n\nFor constructing a confidence interval, we use the pivotal quantity (a test statistic whose distribution is known):\n$$ T = \\frac{\\hat{\\Delta} - \\Delta}{\\widehat{SE}(\\hat{\\Delta})} = \\frac{(\\bar{X}_{\\mathrm{A}} - \\bar{X}_{\\mathrm{B}}) - (\\mu_{\\mathrm{A}} - \\mu_{\\mathrm{B}})}{\\sqrt{\\frac{s_{\\mathrm{A}}^2}{n_{\\mathrm{A}}} + \\frac{s_{\\mathrm{B}}^2}{n_{\\mathrm{B}}}}} $$\nUnder the assumption of heteroscedasticity (unequal variances), the exact distribution of $T$ is not a Student's $t$-distribution. The Welch-Satterthwaite approximation states that $T$ can be approximated by a Student's $t$-distribution with an effective degrees of freedom, $\\nu$.\nA two-sided $100(1-\\alpha)\\%$ confidence interval for $\\Delta$ is then given by:\n$$ \\hat{\\Delta} \\pm t_{\\alpha/2, \\nu} \\cdot \\widehat{SE}(\\hat{\\Delta}) $$\nwhere $t_{\\alpha/2, \\nu}$ is the critical value from the Student's $t$-distribution with $\\nu$ degrees of freedom that leaves a tail probability of $\\alpha/2$.\n\n**4. Satterthwaite Approximation for Degrees of Freedom ($\\nu$)**\n\nThe Satterthwaite approximation for $\\nu$ is derived by matching the first two moments of the denominator's square, $\\widehat{\\text{Var}}(\\hat{\\Delta}) = \\frac{s_{\\mathrm{A}}^2}{n_{\\mathrm{A}}} + \\frac{s_{\\mathrm{B}}^2}{n_{\\mathrm{B}}}$, to those of a scaled chi-squared distribution. The distribution of the sample variance $s^2$ is related to the chi-squared distribution: $\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}$. This implies that $\\text{Var}(s^2) = \\frac{2(\\sigma^2)^2}{n-1}$.\nThe variance of the sum of the estimated variance components is:\n$$ \\text{Var}\\left(\\frac{s_{\\mathrm{A}}^2}{n_{\\mathrm{A}}} + \\frac{s_{\\mathrm{B}}^2}{n_{\\mathrm{B}}}\\right) = \\frac{\\text{Var}(s_{\\mathrm{A}}^2)}{n_{\\mathrm{A}}^2} + \\frac{\\text{Var}(s_{\\mathrm{B}}^2)}{n_{\\mathrm{B}}^2} = \\frac{2(\\sigma_{\\mathrm{A}}^2)^2}{n_{\\mathrm{A}}^2(n_{\\mathrm{A}}-1)} + \\frac{2(\\sigma_{\\mathrm{B}}^2)^2}{n_{\\mathrm{B}}^2(n_{\\mathrm{B}}-1)} $$\nBy matching moments, the effective degrees of freedom $\\nu$ is found to be:\n$$ \\nu = \\frac{\\left( \\frac{\\sigma_{\\mathrm{A}}^2}{n_{\\mathrm{A}}} + \\frac{\\sigma_{\\mathrm{B}}^2}{n_{\\mathrm{B}}} \\right)^2}{\\frac{\\left(\\frac{\\sigma_{\\mathrm{A}}^2}{n_{\\mathrm{A}}}\\right)^2}{n_{\\mathrm{A}}-1} + \\frac{\\left(\\frac{\\sigma_{\\mathrm{B}}^2}{n_{\\mathrm{B}}}\\right)^2}{n_{\\mathrm{B}}-1}} $$\nIn practice, the unknown population variances $\\sigma^2$ are replaced by their sample estimates $s^2$:\n$$ \\nu \\approx \\frac{\\left( \\frac{s_{\\mathrm{A}}^2}{n_{\\mathrm{A}}} + \\frac{s_{\\mathrm{B}}^2}{n_{\\mathrm{B}}} \\right)^2}{\\frac{\\left(\\frac{s_{\\mathrm{A}}^2}{n_{\\mathrm{A}}}\\right)^2}{n_{\\mathrm{A}}-1} + \\frac{\\left(\\frac{s_{\\mathrm{B}}^2}{n_{\\mathrm{B}}}\\right)^2}{n_{\\mathrm{B}}-1}} $$\n\n**5. Numerical Calculation of Degrees of Freedom**\n\nWe are given the following data:\n- Sample sizes: $n_{\\mathrm{A}} = 40$, $n_{\\mathrm{B}} = 30$\n- Sample variances: $s_{\\mathrm{A}}^2 = 64.0$, $s_{\\mathrm{B}}^2 = 36.0$\n\nFirst, we calculate the estimated variance components:\n$$ \\frac{s_{\\mathrm{A}}^2}{n_{\\mathrm{A}}} = \\frac{64.0}{40} = 1.6 $$\n$$ \\frac{s_{\\mathrm{B}}^2}{n_{\\mathrm{B}}} = \\frac{36.0}{30} = 1.2 $$\n\nNext, we substitute these into the Satterthwaite formula.\nThe numerator is:\n$$ \\left( \\frac{s_{\\mathrm{A}}^2}{n_{\\mathrm{A}}} + \\frac{s_{\\mathrm{B}}^2}{n_{\\mathrm{B}}} \\right)^2 = (1.6 + 1.2)^2 = (2.8)^2 = 7.84 $$\nThe denominator is:\n$$ \\frac{\\left(\\frac{s_{\\mathrm{A}}^2}{n_{\\mathrm{A}}}\\right)^2}{n_{\\mathrm{A}}-1} + \\frac{\\left(\\frac{s_{\\mathrm{B}}^2}{n_{\\mathrm{B}}}\\right)^2}{n_{\\mathrm{B}}-1} = \\frac{(1.6)^2}{40-1} + \\frac{(1.2)^2}{30-1} = \\frac{2.56}{39} + \\frac{1.44}{29} $$\nNow, we compute the denominator's value:\n$$ \\frac{2.56}{39} + \\frac{1.44}{29} = \\frac{2.56 \\times 29 + 1.44 \\times 39}{39 \\times 29} = \\frac{74.24 + 56.16}{1131} = \\frac{130.4}{1131} $$\nFinally, we compute $\\nu$:\n$$ \\nu = \\frac{7.84}{\\frac{130.4}{1131}} = \\frac{7.84 \\times 1131}{130.4} = \\frac{8867.04}{130.4} \\approx 67.998773... $$\nRounding this result to $4$ significant figures gives $68.00$. The degrees of freedom is a dimensionless quantity.",
            "answer": "$$ \\boxed{68.00} $$"
        },
        {
            "introduction": "Beyond frequentist methods, the Bayesian paradigm provides a flexible framework for modeling neural data and quantifying uncertainty. This practice  transitions you into this approach through a hands-on coding challenge: estimating a neuron's firing rate. You will implement a full Bayesian analysis for a Poisson process model, leveraging the elegance of conjugate priors to derive the posterior distribution. Crucially, you will also perform a prior sensitivity analysis, a vital skill for assessing how your initial assumptions influence the final conclusions.",
            "id": "4159910",
            "problem": "A spike train from a single cortical neuron is recorded across non-overlapping time windows in which spike counts are aggregated. For each window indexed by $k$, the observed spike count is denoted by $y_k \\in \\{0,1,2,\\dots\\}$ and the exposure duration is denoted by $t_k > 0$ measured in seconds. Assume conditional independence of counts across windows and model the counts using a Poisson process with constant rate parameter $\\lambda$ over the windows such that the likelihood factor for window $k$ is given by the Poisson probability mass function $p(y_k \\mid \\lambda) = \\exp(-\\lambda t_k) (\\lambda t_k)^{y_k} / y_k!$. Assume a Gamma prior with shape-rate parameterization on $\\lambda$, with density $p(\\lambda \\mid a,b) = b^{a} \\lambda^{a-1} \\exp(-b \\lambda) / \\Gamma(a)$ for $\\lambda > 0$, shape parameter $a > 0$, rate parameter $b > 0$, and where $\\Gamma(\\cdot)$ denotes the gamma function.\n\nStarting from Bayes’ rule and the above definitions of the Poisson likelihood and the Gamma prior, derive the posterior distribution for $\\lambda$ given a dataset $\\{(y_k, t_k)\\}_{k=1}^{K}$ and the hyperparameters $(a,b)$. Use this posterior to construct an equal-tailed Bayesian credible interval (BCI) of nominal level $1-\\alpha$, where $\\alpha \\in (0,1)$ is specified per test case. The interval endpoints must be defined using the cumulative distribution function (CDF) inverse (quantile) of the posterior distribution. Define the credible interval width as the upper endpoint minus the lower endpoint. The width must be expressed in spikes per second and reported as a real number in units of spikes per second. All credible levels must be expressed as decimals (for example, $0.95$ for a $95$ percent credible interval).\n\nPerform a prior sensitivity analysis by computing the credible interval width for several choices of $(a,b)$ across the following test suite. For each test case, you must use the specified data $\\{y_k\\}$, exposures $\\{t_k\\}$ (in seconds), credible level $1-\\alpha$, and the ordered list of hyperparameter pairs $(a,b)$. Your program must compute the width for each $(a,b)$ in the given order, for each test case in the given order, and aggregate all widths into a single flat list in that exact order. The final outputs must be rounded to $6$ decimal places.\n\nTest Suite:\n- Test case $1$ (moderate counts, equal exposures): counts $\\{4,3,5,2,6\\}$, exposures in seconds $\\{1,1,1,1,1\\}$, credible level $1-\\alpha$ with $\\alpha=0.05$, hyperparameters $(a,b)$ in order: $(1,1)$, $(10,10)$, $(0.1,0.1)$.\n- Test case $2$ (all-zero counts, varying exposures): counts $\\{0,0,0,0\\}$, exposures in seconds $\\{2,1,3,4\\}$, credible level $1-\\alpha$ with $\\alpha=0.05$, hyperparameters $(a,b)$ in order: $(0.5,0.5)$, $(5,5)$, $(0.1,0.1)$.\n- Test case $3$ (high counts, long exposures): counts $\\{50,55,47,60\\}$, exposures in seconds $\\{2,2,2,2\\}$, credible level $1-\\alpha$ with $\\alpha=0.01$, hyperparameters $(a,b)$ in order: $(1,1)$, $(20,20)$, $(100,1)$.\n- Test case $4$ (sparse counts, equal exposures, prior-data conflict exploration): counts $\\{1,0,2,1,1\\}$, exposures in seconds $\\{1,1,1,1,1\\}$, credible level $1-\\alpha$ with $\\alpha=0.10$, hyperparameters $(a,b)$ in order: $(20,1)$, $(1,20)$, $(1,1)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[w_1,w_2,\\dots]$, where each $w_i$ is the credible interval width (in spikes per second) for one $(a,b)$ setting, in the order specified above across all test cases. Each $w_i$ must be a floating-point number rounded to exactly $6$ decimal places.",
            "solution": "The problem statement is evaluated to be valid as it is scientifically grounded, well-posed, objective, and provides all necessary information for a unique solution. The model, based on a Poisson process for spike counts and a conjugate Gamma prior for the rate parameter, is a standard and appropriate framework in computational neuroscience for analyzing such data. The task is to perform a Bayesian analysis to compute credible interval widths, which is a well-defined statistical procedure.\n\nThe derivation of the solution proceeds as follows, adhering to the principles of Bayesian inference.\n\nFirst, we define the components of Bayes' rule, which states that the posterior distribution is proportional to the product of the likelihood and the prior distribution:\n$$\np(\\lambda \\mid \\text{data}, a, b) \\propto p(\\text{data} \\mid \\lambda) \\times p(\\lambda \\mid a, b)\n$$\n\nThe data consists of a set of spike counts and exposure durations, $\\{(y_k, t_k)\\}_{k=1}^{K}$.\n\nThe likelihood function, $p(\\text{data} \\mid \\lambda)$, is the probability of observing the entire dataset given a specific value of the rate parameter $\\lambda$. Due to the assumption of conditional independence of counts across windows, the total likelihood is the product of the individual Poisson probabilities for each window:\n$$\nL(\\lambda) = p(\\{(y_k, t_k)\\}_{k=1}^{K} \\mid \\lambda) = \\prod_{k=1}^{K} p(y_k \\mid \\lambda) = \\prod_{k=1}^{K} \\frac{\\exp(-\\lambda t_k) (\\lambda t_k)^{y_k}}{y_k!}\n$$\nFor the purpose of finding the posterior distribution of $\\lambda$, we can ignore terms that do not depend on $\\lambda$. This leads to the likelihood function being proportional to:\n$$\nL(\\lambda) \\propto \\prod_{k=1}^{K} \\exp(-\\lambda t_k) \\lambda^{y_k}\n$$\nBy combining the exponents, we can simplify this expression:\n$$\nL(\\lambda) \\propto \\exp\\left(-\\lambda \\sum_{k=1}^{K} t_k\\right) \\lambda^{\\sum_{k=1}^{K} y_k}\n$$\nLet's define the total spike count as $Y = \\sum_{k=1}^{K} y_k$ and the total exposure time as $T = \\sum_{k=1}^{K} t_k$. The likelihood simplifies to:\n$$\nL(\\lambda) \\propto \\lambda^{Y} \\exp(-\\lambda T)\n$$\n\nThe prior distribution for $\\lambda$ is given as a Gamma distribution with shape parameter $a > 0$ and rate parameter $b > 0$:\n$$\np(\\lambda \\mid a, b) = \\frac{b^{a}}{\\Gamma(a)} \\lambda^{a-1} \\exp(-b \\lambda)\n$$\nAs a function of $\\lambda$, the prior is proportional to:\n$$\np(\\lambda \\mid a, b) \\propto \\lambda^{a-1} \\exp(-b \\lambda)\n$$\n\nNow, we apply Bayes' rule by multiplying the (proportional) likelihood and prior:\n$$\np(\\lambda \\mid \\{(y_k, t_k)\\}, a, b) \\propto \\left( \\lambda^{Y} \\exp(-\\lambda T) \\right) \\times \\left( \\lambda^{a-1} \\exp(-b \\lambda) \\right)\n$$\nCombining the terms involving $\\lambda$:\n$$\np(\\lambda \\mid \\{(y_k, t_k)\\}, a, b) \\propto \\lambda^{Y + a - 1} \\exp(-\\lambda (T + b))\n$$\nThis resulting expression is the kernel of a Gamma distribution. This demonstrates that the Gamma distribution is a conjugate prior for the Poisson likelihood. The posterior distribution of $\\lambda$ is therefore also a Gamma distribution with updated parameters. Let the posterior shape parameter be $a'$ and the posterior rate parameter be $b'$:\n$$\na' = a + Y = a + \\sum_{k=1}^{K} y_k\n$$\n$$\nb' = b + T = b + \\sum_{k=1}^{K} t_k\n$$\nThus, the posterior distribution is $\\lambda \\mid \\text{data}, a, b \\sim \\text{Gamma}(a', b')$.\n\nThe problem requires the construction of an equal-tailed Bayesian credible interval (BCI) of nominal level $1-\\alpha$. This interval is defined by the lower and upper quantiles of the posterior distribution. Let $F_{\\text{post}}^{-1}(q)$ be the quantile function (the inverse of the cumulative distribution function, CDF) of the posterior Gamma distribution, $\\text{Gamma}(a', b')$, for a given probability $q \\in (0,1)$.\n\nThe lower endpoint of the credible interval, $\\lambda_L$, is the value below which lies $\\alpha/2$ of the posterior probability mass:\n$$\n\\lambda_L = F_{\\text{post}}^{-1}(\\alpha/2)\n$$\nThe upper endpoint, $\\lambda_U$, is the value below which lies $1 - \\alpha/2$ of the posterior probability mass:\n$$\n\\lambda_U = F_{\\text{post}}^{-1}(1 - \\alpha/2)\n$$\nThe credible interval for $\\lambda$ is then $[\\lambda_L, \\lambda_U]$.\n\nThe width of this credible interval, $W$, is the difference between the upper and lower endpoints:\n$$\nW = \\lambda_U - \\lambda_L = F_{\\text{post}}^{-1}(1 - \\alpha/2) - F_{\\text{post}}^{-1}(\\alpha/2)\n$$\nThe units of $\\lambda$, and therefore of the width $W$, are spikes per second.\n\nThe computational procedure for each test case is:\n1.  Given the data $\\{y_k\\}$ and $\\{t_k\\}$, compute the sufficient statistics $Y = \\sum y_k$ and $T = \\sum t_k$.\n2.  For each pair of prior hyperparameters $(a, b)$, compute the posterior parameters $a' = a + Y$ and $b' = b + T$.\n3.  Using the quantile function for a Gamma distribution with shape $a'$ and rate $b'$, find the $\\alpha/2$ and $1-\\alpha/2$ quantiles. Note that many software libraries parameterize the Gamma distribution using a shape parameter $a'$ and a scale parameter $\\theta = 1/b'$.\n4.  Calculate the width $W$ as the difference between these quantiles.\n5.  Store the result and repeat for all hyperparameter pairs and test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import gamma\n\ndef solve():\n    \"\"\"\n    Computes the width of Bayesian credible intervals for a Poisson rate parameter\n    based on spike count data, using a conjugate Gamma prior.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (counts, exposures, alpha, list_of_hyperparameters).\n    test_cases = [\n        # Test case 1\n        (\n            np.array([4, 3, 5, 2, 6]),\n            np.array([1, 1, 1, 1, 1]),\n            0.05,\n            [(1, 1), (10, 10), (0.1, 0.1)]\n        ),\n        # Test case 2\n        (\n            np.array([0, 0, 0, 0]),\n            np.array([2, 1, 3, 4]),\n            0.05,\n            [(0.5, 0.5), (5, 5), (0.1, 0.1)]\n        ),\n        # Test case 3\n        (\n            np.array([50, 55, 47, 60]),\n            np.array([2, 2, 2, 2]),\n            0.01,\n            [(1, 1), (20, 20), (100, 1)]\n        ),\n        # Test case 4\n        (\n            np.array([1, 0, 2, 1, 1]),\n            np.array([1, 1, 1, 1, 1]),\n            0.10,\n            [(20, 1), (1, 20), (1, 1)]\n        ),\n    ]\n\n    results = []\n    \n    # Iterate through each test case\n    for counts, exposures, alpha, hyper_params_list in test_cases:\n        \n        # Calculate sufficient statistics from the data\n        total_counts = np.sum(counts)\n        total_exposure = np.sum(exposures)\n        \n        # Iterate through each set of hyperparameters for the current test case\n        for a_prior, b_prior in hyper_params_list:\n            \n            # Calculate posterior parameters for the Gamma distribution\n            # Posterior shape a' = a_prior + sum(y_k)\n            # Posterior rate b' = b_prior + sum(t_k)\n            a_posterior = a_prior + total_counts\n            b_posterior = b_prior + total_exposure\n            \n            # The scipy.stats.gamma distribution is parameterized by\n            # shape `a` and scale `scale`, where scale = 1 / rate.\n            scale_posterior = 1 / b_posterior\n            \n            # Define the lower and upper quantile levels for the equal-tailed interval\n            lower_quantile_level = alpha / 2.0\n            upper_quantile_level = 1.0 - (alpha / 2.0)\n            \n            # Use the percent point function (ppf), which is the inverse of the CDF,\n            # to find the interval endpoints.\n            lambda_lower = gamma.ppf(\n                lower_quantile_level, a=a_posterior, scale=scale_posterior\n            )\n            lambda_upper = gamma.ppf(\n                upper_quantile_level, a=a_posterior, scale=scale_posterior\n            )\n            \n            # Calculate the width of the credible interval\n            width = lambda_upper - lambda_lower\n            \n            # Round the result to 6 decimal places and append to the list\n            results.append(round(width, 6))\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}