{
    "hands_on_practices": [
        {
            "introduction": "To build intuition for how regularization works, we begin with a foundational exercise. This problem  demonstrates the core mechanism of ridge regression in a scenario with highly correlated predictors—a common challenge in neuroscience data where different features may encode redundant information. By working through the calculations, you will directly quantify how the ridge penalty shrinks coefficient estimates and, in doing so, reduces their sampling variance compared to Ordinary Least Squares (OLS).",
            "id": "4190298",
            "problem": "A sensory neuroscience experiment records a neuron's firing rate across three stimulus presentations. Two regressors in the design matrix, representing two stimulus features, are nearly collinear. Let the linear model be $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^{3}$ is the firing-rate vector, $X \\in \\mathbb{R}^{3 \\times 2}$ is the design matrix, $\\beta \\in \\mathbb{R}^{2}$ are the regression coefficients, and $\\varepsilon \\in \\mathbb{R}^{3}$ are independent and identically distributed (i.i.d.) Gaussian noise terms with zero mean and variance $\\sigma^{2}$.\n\nYou are given\n$$\nX = \\begin{pmatrix}\n1 & 2 \\\\\n2 & 4 \\\\\n3 & 7\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n5 \\\\\n10 \\\\\n17\n\\end{pmatrix}.\n$$\n\nConsider ridge regression with penalty parameter $\\lambda = 1$. Starting from the linear model assumptions and the defining optimization principle for ridge regression, derive the ridge coefficient vector and the sampling variance of the first coefficient under both Ordinary Least Squares (OLS) and ridge. Then quantify shrinkage by computing the ratio $r$ of the variance of the first ridge coefficient estimate to the variance of the first OLS coefficient estimate. Express your final answer as a row vector containing the two ridge coefficients followed by the ratio, in exact form with no rounding.",
            "solution": "The linear model is given by $y = X \\beta + \\varepsilon$, where the noise term $\\varepsilon$ is assumed to be a vector of i.i.d. random variables with $E[\\varepsilon] = 0$ and $\\text{Cov}(\\varepsilon) = E[\\varepsilon \\varepsilon^T] = \\sigma^2 I$, where $I$ is the identity matrix.\n\nThe Ordinary Least Squares (OLS) estimator, $\\hat{\\beta}_{OLS}$, is found by minimizing the residual sum of squares (RSS):\n$$\n\\text{RSS}(\\beta) = (y - X\\beta)^T(y - X\\beta) = ||y - X\\beta||_2^2\n$$\nThe solution is given by the normal equations, leading to:\n$$\n\\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T y\n$$\nThe sampling covariance matrix of the OLS estimator is:\n$$\n\\text{Cov}(\\hat{\\beta}_{OLS}) = \\text{Cov}((X^T X)^{-1} X^T y) = (X^T X)^{-1} X^T \\text{Cov}(y) ((X^T X)^{-1} X^T)^T\n$$\nSince $\\text{Cov}(y) = \\text{Cov}(X\\beta + \\varepsilon) = \\text{Cov}(\\varepsilon) = \\sigma^2 I$, this simplifies to:\n$$\n\\text{Cov}(\\hat{\\beta}_{OLS}) = \\sigma^2 (X^T X)^{-1} X^T I X (X^T X)^{-1} = \\sigma^2 (X^T X)^{-1}\n$$\n\nRidge regression adds an $L_2$ penalty term to the RSS to regularize the coefficients, which is particularly useful in cases of multicollinearity. The ridge estimator, $\\hat{\\beta}_{Ridge}$, is found by minimizing:\n$$\n\\text{RSS}_{Ridge}(\\beta) = ||y - X\\beta||_2^2 + \\lambda ||\\beta||_2^2\n$$\nwhere $\\lambda > 0$ is the penalty parameter. The solution is:\n$$\n\\hat{\\beta}_{Ridge} = (X^T X + \\lambda I)^{-1} X^T y\n$$\nThe sampling covariance matrix of the ridge estimator is derived similarly:\n$$\n\\text{Cov}(\\hat{\\beta}_{Ridge}) = \\text{Cov}((X^T X + \\lambda I)^{-1} X^T y) = (X^T X + \\lambda I)^{-1} X^T (\\sigma^2 I) X ((X^T X + \\lambda I)^{-1})^T\n$$\nBecause $X^T X + \\lambda I$ is symmetric, its inverse is also symmetric. Thus:\n$$\n\\text{Cov}(\\hat{\\beta}_{Ridge}) = \\sigma^2 (X^T X + \\lambda I)^{-1} X^T X (X^T X + \\lambda I)^{-1}\n$$\n\nWe are given:\n$$\nX = \\begin{pmatrix}\n1 & 2 \\\\\n2 & 4 \\\\\n3 & 7\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n5 \\\\\n10 \\\\\n17\n\\end{pmatrix}, \\quad\n\\lambda = 1\n$$\nFirst, we compute $X^T X$ and $X^T y$:\n$$\nX^T X = \\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 7 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\\\ 3 & 7 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 2 \\cdot 2 + 3 \\cdot 3 & 1 \\cdot 2 + 2 \\cdot 4 + 3 \\cdot 7 \\\\ 2 \\cdot 1 + 4 \\cdot 2 + 7 \\cdot 3 & 2 \\cdot 2 + 4 \\cdot 4 + 7 \\cdot 7 \\end{pmatrix} = \\begin{pmatrix} 1+4+9 & 2+8+21 \\\\ 2+8+21 & 4+16+49 \\end{pmatrix} = \\begin{pmatrix} 14 & 31 \\\\ 31 & 69 \\end{pmatrix}\n$$\n$$\nX^T y = \\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 7 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 10 \\\\ 17 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 5 + 2 \\cdot 10 + 3 \\cdot 17 \\\\ 2 \\cdot 5 + 4 \\cdot 10 + 7 \\cdot 17 \\end{pmatrix} = \\begin{pmatrix} 5+20+51 \\\\ 10+40+119 \\end{pmatrix} = \\begin{pmatrix} 76 \\\\ 169 \\end{pmatrix}\n$$\n\nNow, we calculate the ridge coefficient vector $\\hat{\\beta}_{Ridge}$. We need the matrix $(X^T X + \\lambda I)^{-1}$ with $\\lambda = 1$. Let $S = X^T X$.\n$$\nS + \\lambda I = \\begin{pmatrix} 14 & 31 \\\\ 31 & 69 \\end{pmatrix} + 1 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 15 & 31 \\\\ 31 & 70 \\end{pmatrix}\n$$\nThe determinant is $\\det(S + I) = 15 \\cdot 70 - 31 \\cdot 31 = 1050 - 961 = 89$.\nThe inverse is:\n$$\n(S + I)^{-1} = \\frac{1}{89} \\begin{pmatrix} 70 & -31 \\\\ -31 & 15 \\end{pmatrix}\n$$\nNow we compute the ridge coefficients:\n$$\n\\hat{\\beta}_{Ridge} = (S+I)^{-1} X^T y = \\frac{1}{89} \\begin{pmatrix} 70 & -31 \\\\ -31 & 15 \\end{pmatrix} \\begin{pmatrix} 76 \\\\ 169 \\end{pmatrix} = \\frac{1}{89} \\begin{pmatrix} 70 \\cdot 76 - 31 \\cdot 169 \\\\ -31 \\cdot 76 + 15 \\cdot 169 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}_{Ridge} = \\frac{1}{89} \\begin{pmatrix} 5320 - 5239 \\\\ -2356 + 2535 \\end{pmatrix} = \\frac{1}{89} \\begin{pmatrix} 81 \\\\ 179 \\end{pmatrix} = \\begin{pmatrix} 81/89 \\\\ 179/89 \\end{pmatrix}\n$$\nThe two ridge coefficients are $\\hat{\\beta}_{Ridge,1} = \\frac{81}{89}$ and $\\hat{\\beta}_{Ridge,2} = \\frac{179}{89}$.\n\nNext, we calculate the variance of the first coefficient for OLS, $\\text{Var}(\\hat{\\beta}_{OLS,1})$. This is the first diagonal element of $\\sigma^2 S^{-1}$.\n$$\n\\det(S) = 14 \\cdot 69 - 31 \\cdot 31 = 966 - 961 = 5\n$$\nThe inverse of $S = X^TX$ is:\n$$\nS^{-1} = \\frac{1}{5} \\begin{pmatrix} 69 & -31 \\\\ -31 & 14 \\end{pmatrix}\n$$\nThe variance of the first OLS coefficient is:\n$$\n\\text{Var}(\\hat{\\beta}_{OLS,1}) = \\sigma^2 (S^{-1})_{11} = \\frac{69}{5} \\sigma^2\n$$\n\nNow, we calculate the variance of the first ridge coefficient, $\\text{Var}(\\hat{\\beta}_{Ridge,1})$. This is the first diagonal element of $\\sigma^2 (S + I)^{-1} S (S + I)^{-1}$.\nLet $Z = (S + I)^{-1} = \\frac{1}{89} \\begin{pmatrix} 70 & -31 \\\\ -31 & 15 \\end{pmatrix}$. The covariance matrix is $\\sigma^2 Z S Z$.\nFirst, calculate $SZ$:\n$$\nSZ = \\begin{pmatrix} 14 & 31 \\\\ 31 & 69 \\end{pmatrix} \\frac{1}{89} \\begin{pmatrix} 70 & -31 \\\\ -31 & 15 \\end{pmatrix} = \\frac{1}{89} \\begin{pmatrix} 14(70) + 31(-31) & 14(-31) + 31(15) \\\\ 31(70) + 69(-31) & 31(-31) + 69(15) \\end{pmatrix} = \\frac{1}{89} \\begin{pmatrix} 980-961 & -434+465 \\\\ 2170-2139 & -961+1035 \\end{pmatrix} = \\frac{1}{89} \\begin{pmatrix} 19 & 31 \\\\ 31 & 74 \\end{pmatrix}\n$$\nNow, calculate $Z(SZ)$:\n$$\nZ(SZ) = \\left( \\frac{1}{89} \\begin{pmatrix} 70 & -31 \\\\ -31 & 15 \\end{pmatrix} \\right) \\left( \\frac{1}{89} \\begin{pmatrix} 19 & 31 \\\\ 31 & 74 \\end{pmatrix} \\right) = \\frac{1}{89^2} \\begin{pmatrix} 70(19) - 31(31) & 70(31) - 31(74) \\\\ -31(19) + 15(31) & -31(31) + 15(74) \\end{pmatrix}\n$$\nThe $(1,1)$ entry is what we need for the variance of the first coefficient:\n$$\n(ZSZ)_{11} = \\frac{1}{89^2} (70 \\cdot 19 - 31 \\cdot 31) = \\frac{1}{7921} (1330 - 961) = \\frac{369}{7921}\n$$\nThe variance of the first ridge coefficient is:\n$$\n\\text{Var}(\\hat{\\beta}_{Ridge,1}) = \\sigma^2 (ZSZ)_{11} = \\frac{369}{7921} \\sigma^2\n$$\n\nFinally, we compute the ratio $r$ of the variance of the first ridge coefficient estimate to the variance of the first OLS coefficient estimate.\n$$\nr = \\frac{\\text{Var}(\\hat{\\beta}_{Ridge,1})}{\\text{Var}(\\hat{\\beta}_{OLS,1})} = \\frac{\\frac{369}{7921} \\sigma^2}{\\frac{69}{5} \\sigma^2} = \\frac{369}{7921} \\cdot \\frac{5}{69}\n$$\nWe can simplify this fraction. Notice that $369 = 9 \\times 41$ and $69 = 3 \\times 23$.\n$$\nr = \\frac{369}{69} \\cdot \\frac{5}{7921} = \\frac{123}{23} \\cdot \\frac{5}{7921} = \\frac{123 \\times 5}{23 \\times 7921} = \\frac{615}{182183}\n$$\nThis fraction cannot be simplified further, as $615 = 3 \\times 5 \\times 41$, $182183 = 23 \\times 89^2$, and these have no common prime factors.\n\nThe final answer is a row vector containing the two ridge coefficients followed by the ratio $r$.\n$$\n\\begin{pmatrix} \\hat{\\beta}_{Ridge,1} & \\hat{\\beta}_{Ridge,2} & r \\end{pmatrix} = \\begin{pmatrix} \\frac{81}{89} & \\frac{179}{89} & \\frac{615}{182183} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{81}{89} & \\frac{179}{89} & \\frac{615}{182183}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Real-world neural data, such as spike counts, often require models beyond simple linear regression. This exercise  extends the concept of regularization to the Poisson Generalized Linear Model (GLM), a standard tool for analyzing count data. You will explore the crucial role of offset terms in modeling firing rates over variable time windows and derive the estimate for the model's intercept, which, unlike the slope coefficients, is not subject to penalization.",
            "id": "4190264",
            "problem": "You are modeling extracellular single-unit spike counts from a cortical neuron recorded across $n=5$ non-overlapping analysis windows of varying duration in a stimulus experiment. Let $Y_i$ denote the spike count in window $i$ and $t_i$ its duration in seconds. You fit a penalized Poisson Generalized Linear Model (GLM) with a canonical log link to the conditional mean $ \\mu_i = \\mathbb{E}[Y_i \\mid X_i]$, where the linear predictor for window $i$ is\n$$\n\\eta_i \\;=\\; \\beta_0 \\;+\\; X_i^{\\top} \\beta \\;+\\; o_i,\n$$\nwith offset $o_i = \\ln(t_i)$ to account for varying exposure times. The covariate vector $X_i \\in \\mathbb{R}^K$ contains $K$ trial-level predictors derived from the stimulus and spike-history features that have been standardized column-wise: each column has been centered to mean $0$ and scaled to unit variance prior to fitting. You use an elastic net penalty with tuning parameters chosen such that at the optimizer all slope coefficients are exactly zero, that is, $\\hat{\\beta} = 0$. In penalized GLMs, intercepts and offsets are not penalized.\n\nData: the durations are $t_1=1.0$, $t_2=1.2$, $t_3=0.8$, $t_4=1.5$, $t_5=0.5$, and the observed spike counts are $y_1=5$, $y_2=7$, $y_3=4$, $y_4=8$, $y_5=1$.\n\nFrom first principles of the Poisson likelihood with canonical link, derive the estimating equation for the intercept under an offset and explain, in one or two sentences, how standardization of $X$ and the presence of an offset $o_i$ are handled in penalized GLMs in this neural setting. Then, using the given data and the fact that $\\hat{\\beta}=0$ while the predictors $X$ are centered and scaled, compute the exact closed-form value of the intercept $\\hat{\\beta}_0$ that maximizes the penalized objective. Express your final answer as a simplified analytic expression; do not approximate or round.",
            "solution": "### Solution Derivation\n\nThe objective is to find the value of the intercept, $\\hat{\\beta}_0$, that maximizes the penalized log-likelihood function. The log-likelihood for a Poisson-distributed random variable $Y_i$ with mean $\\mu_i$ is given by:\n$$\n\\ell_i(\\mu_i; y_i) = y_i \\ln(\\mu_i) - \\mu_i - \\ln(y_i!)\n$$\nThe total log-likelihood for $n$ independent observations is the sum over all windows:\n$$\nL = \\sum_{i=1}^{n} \\ell_i(\\mu_i; y_i) = \\sum_{i=1}^{n} \\left( y_i \\ln(\\mu_i) - \\mu_i - \\ln(y_i!) \\right)\n$$\nThe GLM connects the mean $\\mu_i$ to the linear predictor $\\eta_i$ via the link function. For the canonical log link, we have $\\ln(\\mu_i) = \\eta_i$, which implies $\\mu_i = \\exp(\\eta_i)$. The linear predictor is $\\eta_i = \\beta_0 + X_i^{\\top} \\beta + o_i$. Substituting these into the log-likelihood function gives:\n$$\nL(\\beta_0, \\beta) = \\sum_{i=1}^{n} \\left( y_i (\\beta_0 + X_i^{\\top} \\beta + o_i) - \\exp(\\beta_0 + X_i^{\\top} \\beta + o_i) \\right) - \\sum_{i=1}^{n} \\ln(y_i!)\n$$\nThe penalized objective function to be maximized is $L_p(\\beta_0, \\beta) = L(\\beta_0, \\beta) - P(\\beta)$, where $P(\\beta)$ is the elastic net penalty function that depends only on the slope coefficients $\\beta$. The estimates $(\\hat{\\beta}_0, \\hat{\\beta})$ are found by setting the gradients of $L_p$ to zero.\n\nTo find the estimating equation for the intercept $\\beta_0$, we compute the partial derivative of $L_p$ with respect to $\\beta_0$. Since the penalty term $P(\\beta)$ does not depend on $\\beta_0$, its derivative is zero.\n$$\n\\frac{\\partial L_p}{\\partial \\beta_0} = \\frac{\\partial L}{\\partial \\beta_0} = \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^{n} \\left( y_i (\\beta_0 + X_i^{\\top} \\beta + o_i) - \\exp(\\beta_0 + X_i^{\\top} \\beta + o_i) \\right)\n$$\nDifferentiating term by term:\n$$\n\\frac{\\partial L}{\\partial \\beta_0} = \\sum_{i=1}^{n} \\left( \\frac{\\partial}{\\partial \\beta_0} (y_i \\beta_0) - \\frac{\\partial}{\\partial \\beta_0} \\exp(\\beta_0 + X_i^{\\top} \\beta + o_i) \\right)\n$$\n$$\n\\frac{\\partial L}{\\partial \\beta_0} = \\sum_{i=1}^{n} \\left( y_i - \\exp(\\beta_0 + X_i^{\\top} \\beta + o_i) \\cdot 1 \\right) = \\sum_{i=1}^{n} (y_i - \\exp(\\eta_i)) = \\sum_{i=1}^{n} (y_i - \\mu_i)\n$$\nSetting this derivative to zero gives the estimating equation for $\\beta_0$:\n$$\n\\sum_{i=1}^{n} \\left( y_i - \\exp(\\hat{\\beta}_0 + X_i^{\\top} \\hat{\\beta} + o_i) \\right) = 0\n$$\nThis is the general estimating equation for the intercept derived from first principles.\n\nStandardization of covariates $X$ ensures the penalty is applied equitably to all coefficients by placing them on a common scale. The offset $o_i$ is a known structural predictor, not an estimated parameter, that accounts for varying observation times $t_i$ and is therefore correctly excluded from penalization.\n\nNow, we use the given data and conditions to compute the specific value of $\\hat{\\beta}_0$. The problem states that at the solution, the estimated slope coefficients are zero, i.e., $\\hat{\\beta} = 0$. Substituting this into the estimating equation:\n$$\n\\sum_{i=1}^{n} \\left( y_i - \\exp(\\hat{\\beta}_0 + X_i^{\\top} 0 + o_i) \\right) = 0\n$$\n$$\n\\sum_{i=1}^{n} \\left( y_i - \\exp(\\hat{\\beta}_0 + o_i) \\right) = 0\n$$\nThis can be rewritten as:\n$$\n\\sum_{i=1}^{n} y_i = \\sum_{i=1}^{n} \\exp(\\hat{\\beta}_0 + o_i)\n$$\nUsing the property of exponents, $\\exp(a+b) = \\exp(a)\\exp(b)$:\n$$\n\\sum_{i=1}^{n} y_i = \\sum_{i=1}^{n} \\exp(\\hat{\\beta}_0) \\exp(o_i)\n$$\nSince $\\exp(\\hat{\\beta}_0)$ is a constant with respect to the summation index $i$, we can factor it out:\n$$\n\\sum_{i=1}^{n} y_i = \\exp(\\hat{\\beta}_0) \\sum_{i=1}^{n} \\exp(o_i)\n$$\nWe are given that the offset is $o_i = \\ln(t_i)$. Therefore, $\\exp(o_i) = \\exp(\\ln(t_i)) = t_i$. Substituting this into the equation:\n$$\n\\sum_{i=1}^{n} y_i = \\exp(\\hat{\\beta}_0) \\sum_{i=1}^{n} t_i\n$$\nSolving for $\\exp(\\hat{\\beta}_0)$:\n$$\n\\exp(\\hat{\\beta}_0) = \\frac{\\sum_{i=1}^{n} y_i}{\\sum_{i=1}^{n} t_i}\n$$\nFinally, taking the natural logarithm of both sides gives the closed-form expression for $\\hat{\\beta}_0$:\n$$\n\\hat{\\beta}_0 = \\ln\\left(\\frac{\\sum_{i=1}^{n} y_i}{\\sum_{i=1}^{n} t_i}\\right)\n$$\nThis result is intuitive: when all covariate effects are zero, the model's baseline log-rate $\\hat{\\beta}_0$ is the logarithm of the overall average firing rate, which is the total number of spikes divided by the total observation time.\n\nNow, we substitute the given data into this expression.\nThe observed spike counts are $y = (5, 7, 4, 8, 1)$. The sum is:\n$$\n\\sum_{i=1}^{5} y_i = 5 + 7 + 4 + 8 + 1 = 25\n$$\nThe window durations are $t = (1.0, 1.2, 0.8, 1.5, 0.5)$. The sum is:\n$$\n\\sum_{i=1}^{5} t_i = 1.0 + 1.2 + 0.8 + 1.5 + 0.5 = 5.0\n$$\nSubstituting these sums into the expression for $\\hat{\\beta}_0$:\n$$\n\\hat{\\beta}_0 = \\ln\\left(\\frac{25}{5.0}\\right) = \\ln(5)\n$$\nThe value of the intercept is $\\ln(5)$.",
            "answer": "$$\\boxed{\\ln(5)}$$"
        },
        {
            "introduction": "Perhaps the most critical skill in applying machine learning to high-dimensional data is knowing how to obtain an honest estimate of a model's performance. This problem  confronts the pervasive issue of \"double dipping,\" where using the same data to both select features and evaluate performance leads to overly optimistic and invalid conclusions. By analyzing valid and invalid validation protocols, you will learn to design rigorous analysis pipelines using techniques like nested cross-validation, ensuring your findings are robust and generalizable.",
            "id": "4190259",
            "problem": "A laboratory is analyzing a functional Magnetic Resonance Imaging (fMRI) dataset to predict a behavioral score $Y$ from voxel-level predictors $X \\in \\mathbb{R}^{p}$, where there are $n$ participants and $p$ voxels with $p \\gg n$. The team plans to use the Least Absolute Shrinkage and Selection Operator (lasso) to select a sparse set of voxels and estimate their coefficients, and then report generalization error. A colleague proposes to first select features and estimate coefficients using the entire dataset, and then run $K$-fold cross validation (CV) to estimate error.\n\nFrom a first-principles perspective, consider the risk $R(f) = \\mathbb{E}\\left[(Y - f(X))^{2}\\right]$ for a predictor $f$, and assume independent and identically distributed (i.i.d.) samples $(X_{i}, Y_{i})$ drawn from a fixed distribution. In a high-dimensional neuroscience setting (for example, voxel activity with $p \\gg n$), feature screening based on empirical correlations is common. Under a global null model where the true coefficients are zero, empirical feature screening implicitly chooses the largest empirical correlations $r_{j} = \\frac{1}{n}\\sum_{i=1}^{n} X_{ij} Y_{i}$, which, by classical tail bounds for sub-Gaussian random variables, satisfy $\\mathbb{E}\\left[\\max_{1 \\le j \\le p} |r_{j}|\\right] \\asymp \\sqrt{\\frac{\\log p}{n}}$. Conditioning estimation on such a selection event induces a dependence between model choice and the data used to estimate its risk.\n\nSelect all statements that correctly describe the consequences of using the same data to select features and estimate coefficients and that propose a valid nested or split-sample procedure for lasso in this neuroscience data analysis context.\n\nA. Using the same data to screen features and estimate coefficients creates selection-induced bias (also called “double dipping”): conditional on the selection event, the chosen features have upwardly biased empirical associations, leading to optimistic estimates of generalization error. A valid nested CV procedure for lasso is: partition the data into $F$ outer folds; for each outer fold, treat its held-out portion as an assessment set, and within the remaining data perform an inner CV to select the regularization parameter $\\lambda$ (and hence the feature set) using only the inner-training splits; refit lasso on the entire outer-training set with the chosen $\\lambda$; evaluate predictions on the outer assessment set; aggregate outer-fold errors to estimate $R(f)$; finally, refit lasso on the full dataset with the $\\lambda$ chosen by inner CV if a single deployed model is desired. This breaks the dependence between selection/estimation and assessment.\n\nB. Because lasso shrinks coefficients, performing feature selection and coefficient estimation on the full dataset and then computing ordinary least squares confidence intervals on those selected features using the same data yields unbiased inference and generalization error; additional nesting or splitting is unnecessary.\n\nC. A valid split-sample protocol in high-dimensional neuroscience is to divide the dataset into three disjoint sets: a screening set, a tuning-training set, and a final test set. Use the screening set to preselect a candidate pool of features (for example, based on empirical correlations), use inner CV within the tuning-training set to choose $\\lambda$ and fit lasso, optionally refit lasso on the union of screening and tuning-training sets with the tuned $\\lambda$, and report generalization error only on the untouched test set. This procedure ensures that the reported test error is not biased by the selection process.\n\nD. In the regime $p \\gg n$, regularization guarantees that using the same data for feature selection and coefficient estimation will not bias the estimated test error; bootstrap resampling of the residuals from the full-data lasso fit is sufficient to obtain unbiased generalization error without nesting or splitting.\n\nChoose all that apply.",
            "solution": "The fundamental principle at stake is the requirement for statistical independence between the data used to train a model (including all selection and tuning steps) and the data used to assess its generalization performance. The colleague's proposal violates this principle. By first fitting a lasso model to the *entire* dataset, information from all samples is used to select the features (i.e., the voxels with non-zero coefficients). Subsequently performing $K$-fold CV to estimate the error of this procedure is biased. In each fold, the model has been trained on a set of features that were selected, in part, using information from the hold-out test set for that fold. This \"information leak\" leads to a systematically optimistic (underestimated) generalization error. The problem is exacerbated in the $p \\gg n$ regime, where the probability of finding large spurious correlations is high, as indicated by the provided tail bound.\n\nA valid procedure must ensure that the data used for the final performance assessment has not been used in any way to select features, tune hyperparameters, or train the model being assessed.\n\n#### Option A\n\nThis statement makes two principal claims: one about the nature of the problem and one about a valid solution.\n\n1.  **Description of the Consequence:** \"Using the same data to screen features and estimate coefficients creates selection-induced bias (also called “double dipping”): conditional on the selection event, the chosen features have upwardly biased empirical associations, leading to optimistic estimates of generalization error.\" This is a precise and correct description of the statistical issue. When we select features based on high empirical correlation with the outcome $Y$ from a large pool of $p$ candidates, we are likely to select some features that are high due to random chance (sampling variability), not a true underlying association. Evaluating the model's performance on the same data that produced these spuriously high correlations will naturally yield an overly optimistic result.\n\n2.  **Proposed Solution (Nested CV):** The statement outlines a nested cross-validation procedure. Let's analyze its steps:\n    *   The data is split into $F$ outer folds. For each fold $f \\in \\{1, ..., F\\}$, the fold is held out as the outer-assessment set.\n    *   The model selection process (choosing the lasso regularization parameter $\\lambda$) is performed *only* on the remaining $F-1$ folds (the outer-training set) using an *inner* cross-validation loop.\n    *   Crucially, the outer-assessment set is not touched during this inner loop.\n    *   Once the optimal $\\lambda$ is found, a model is trained on the entire outer-training set using this $\\lambda$.\n    *   This model's performance is then evaluated on the pristine outer-assessment set.\n    *   The errors from all $F$ outer folds are aggregated to produce a single, unbiased estimate of the generalization error $R(f)$.\n    *   The final step of refitting on all data to produce a deployable model is also standard practice.\n\nThis procedure correctly isolates the assessment data in each outer fold from the model selection and training process, thereby \"breaking the dependence\" and yielding an unbiased estimate of generalization error.\n\n**Verdict for A: Correct**\n\n#### Option B\n\nThis statement claims: \"Because lasso shrinks coefficients, performing feature selection and coefficient estimation on the full dataset and then computing ordinary least squares confidence intervals on those selected features using the same data yields unbiased inference and generalization error\".\n\nThis is incorrect on multiple grounds.\n1.  **Shrinkage and Bias:** While lasso's shrinkage helps to reduce variance and prevent overfitting compared to ordinary least squares (OLS) in a $p \\gg n$ setting, it does not eliminate the selection bias in the *validation procedure* if done improperly.\n2.  **Post-Selection Inference:** Computing standard OLS confidence intervals for coefficients that were selected by a data-driven procedure (like lasso) on the same data is a statistically invalid practice. This fails to account for the uncertainty of the selection step itself, leading to confidence intervals that are too narrow and p-values that are too small. This field of research is known as post-selection inference, and it requires specialized methods to produce valid results. Standard OLS is not one of them.\n3.  **Generalization Error:** As established, using the same data for selection and evaluation leads to an optimistically biased estimate of generalization error. The statement that this procedure is \"unbiased\" is false.\n\n**Verdict for B: Incorrect**\n\n#### Option C\n\nThis statement proposes a split-sample protocol.\n\n1.  **Procedure:** The data is partitioned into three disjoint sets: screening, tuning-training, and final test.\n    *   **Screening Set:** Used to perform an initial, coarse feature selection. This isolates the initial screening from subsequent steps.\n    *   **Tuning-Training Set:** Used for all model development, including hyperparameter ($\\lambda$) tuning via CV and fitting the lasso model.\n    *   **Final Test Set:** This set is held out and used only once, at the very end, to report the final generalization error.\n2.  **Validity:** This procedure rigorously enforces the separation of data used for different stages of model building and evaluation. The final test set remains \"untouched\" by any part of the feature selection, hyperparameter tuning, or coefficient estimation process. Therefore, the performance measured on this test set provides an unbiased estimate of the model's generalization error on new data. The optional step of refitting on the combined screening and tuning-training sets before final evaluation is also valid, as it still does not involve the test set.\n\nThis three-way split is a standard and valid approach to avoid selection bias, especially when the initial number of features is extremely large and a pre-screening step is computationally or methodologically desirable.\n\n**Verdict for C: Correct**\n\n#### Option D\n\nThis statement claims: \"In the regime $p \\gg n$, regularization guarantees that using the same data for feature selection and coefficient estimation will not bias the estimated test error; bootstrap resampling of the residuals from the full-data lasso fit is sufficient to obtain unbiased generalization error without nesting or splitting.\"\n\nThis is incorrect.\n1.  **Regularization's Role:** Regularization is a part of the *model fitting* process, not the *model validation* protocol. It cannot correct a flawed validation protocol. The bias problem stems from re-using data for selection and testing, a flaw that regularization does not address.\n2.  **Bootstrap Validity:** Applying a standard bootstrap procedure to estimate the prediction error of a model that was itself selected using the entire dataset will inherit the same optimism and selection bias. The bootstrap samples are drawn from the original dataset. If the original dataset produced a model that looks spuriously good, the bootstrap samples drawn from it will also tend to produce models that look spuriously good, leading to a biased error estimate. While more advanced bootstrap methods (e.g., the `.632+` estimator) are designed to partially correct for this, the simple residual bootstrap as described is not \"sufficient\" and does not replace the need for proper data splitting or nesting.\n\n**Verdict for D: Incorrect**\n\n### Final Conclusion\n\nStatements A and C correctly identify the problem of selection bias and propose statistically valid procedures (nested cross-validation and a three-way data split, respectively) to obtain an unbiased estimate of generalization error. Statements B and D are based on common but profound misconceptions about the roles of regularization, post-selection inference, and bootstrapping in a high-dimensional setting.",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}