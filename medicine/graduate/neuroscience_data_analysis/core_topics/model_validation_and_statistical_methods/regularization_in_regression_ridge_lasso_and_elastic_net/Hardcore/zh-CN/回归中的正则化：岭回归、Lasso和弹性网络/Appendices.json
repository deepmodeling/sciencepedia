{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握岭回归，动手计算是必不可少的。本练习将引导你处理一个具有高度相关预测变量的典型神经科学场景，这也是岭回归最能发挥作用的场景。通过从头开始推导岭回归系数并将其估计的方差与普通最小二乘法（OLS）进行比较，你将直观地理解 $\\ell_2$ 惩罚项是如何通过稳定系数来降低模型方差的。",
            "id": "4190298",
            "problem": "一个感觉神经科学实验记录了一个神经元在三次刺激呈现中的放电率。设计矩阵中的两个回归量，代表了两个刺激特征，它们是近似共线的。设线性模型为 $y = X \\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^{3}$ 是放电率向量，$X \\in \\mathbb{R}^{3 \\times 2}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^{2}$ 是回归系数，$\\varepsilon \\in \\mathbb{R}^{3}$ 是独立同分布（i.i.d.）的高斯噪声项，其均值为零，方差为 $\\sigma^{2}$。\n\n给定\n$$\nX = \\begin{pmatrix}\n1  2 \\\\\n2  4 \\\\\n3  7\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n5 \\\\\n10 \\\\\n17\n\\end{pmatrix}.\n$$\n\n考虑岭回归，惩罚参数为 $\\lambda = 1$。从线性模型的假设和岭回归的定义性优化原理出发，推导岭系数向量，以及在普通最小二乘法（OLS）和岭回归下第一个系数的抽样方差。然后，通过计算第一个岭系数估计的方差与第一个OLS系数估计的方差之比 $r$ 来量化收缩。将您的最终答案表示为一个行向量，其中包含两个岭系数和该比率，以精确形式表示，无需四舍五入。",
            "solution": "用户提供了一个问题，要求将岭回归应用于具有近似共线性回归量的线性模型。任务是计算岭回归系数，并比较在普通最小二乘法（OLS）和岭回归下第一个系数估计的方差。\n\n该问题在科学上和数学上是明确定义的。所有必要的数据和参数都已提供，问题没有歧义。这是一个标准的统计问题，设置在感觉神经科学的合理背景下。因此，该问题是有效的，我们可以继续进行求解。\n\n线性模型由 $y = X \\beta + \\varepsilon$ 给出，其中噪声项 $\\varepsilon$ 假定为独立同分布（i.i.d.）随机变量的向量，满足 $E[\\varepsilon] = 0$ 和 $\\text{Cov}(\\varepsilon) = E[\\varepsilon \\varepsilon^T] = \\sigma^2 I$，其中 $I$ 是单位矩阵。\n\n普通最小二乘法（OLS）估计量 $\\hat{\\beta}_{OLS}$ 是通过最小化残差平方和（RSS）得到的：\n$$\n\\text{RSS}(\\beta) = (y - X\\beta)^T(y - X\\beta) = ||y - X\\beta||_2^2\n$$\n解由正规方程给出，得到：\n$$\n\\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T y\n$$\nOLS估计量的抽样协方差矩阵为：\n$$\n\\text{Cov}(\\hat{\\beta}_{OLS}) = \\text{Cov}((X^T X)^{-1} X^T y) = (X^T X)^{-1} X^T \\text{Cov}(y) ((X^T X)^{-1} X^T)^T\n$$\n因为 $\\text{Cov}(y) = \\text{Cov}(X\\beta + \\varepsilon) = \\text{Cov}(\\varepsilon) = \\sigma^2 I$，这可以简化为：\n$$\n\\text{Cov}(\\hat{\\beta}_{OLS}) = \\sigma^2 (X^T X)^{-1}\n$$\n\n岭回归在残差平方和（RSS）上增加一个 $L_2$ 惩罚项来正则化系数，这在多重共线性的情况下特别有用。岭回归估计量 $\\hat{\\beta}_{Ridge}$ 是通过最小化以下表达式得到的：\n$$\n\\text{RSS}_{Ridge}(\\beta) = ||y - X\\beta||_2^2 + \\lambda ||\\beta||_2^2\n$$\n其中 $\\lambda > 0$ 是惩罚参数。解为：\n$$\n\\hat{\\beta}_{Ridge} = (X^T X + \\lambda I)^{-1} X^T y\n$$\n岭回归估计量的抽样协方差矩阵可以类似地推导：\n$$\n\\text{Cov}(\\hat{\\beta}_{Ridge}) = \\text{Cov}((X^T X + \\lambda I)^{-1} X^T y) = (X^T X + \\lambda I)^{-1} X^T (\\sigma^2 I) X ((X^T X + \\lambda I)^{-1})^T\n$$\n因为 $X^T X + \\lambda I$ 是对称的，所以它的逆矩阵也是对称的。因此：\n$$\n\\text{Cov}(\\hat{\\beta}_{Ridge}) = \\sigma^2 (X^T X + \\lambda I)^{-1} X^T X (X^T X + \\lambda I)^{-1}\n$$\n\n我们已知：\n$$\nX = \\begin{pmatrix}\n1  2 \\\\\n2  4 \\\\\n3  7\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n5 \\\\\n10 \\\\\n17\n\\end{pmatrix}, \\quad\n\\lambda = 1\n$$\n首先，我们计算 $X^T X$ 和 $X^T y$：\n$$\nX^T X = \\begin{pmatrix} 1  2  3 \\\\ 2  4  7 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 2  4 \\\\ 3  7 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 2 \\cdot 2 + 3 \\cdot 3  1 \\cdot 2 + 2 \\cdot 4 + 3 \\cdot 7 \\\\ 2 \\cdot 1 + 4 \\cdot 2 + 7 \\cdot 3  2 \\cdot 2 + 4 \\cdot 4 + 7 \\cdot 7 \\end{pmatrix} = \\begin{pmatrix} 1+4+9  2+8+21 \\\\ 2+8+21  4+16+49 \\end{pmatrix} = \\begin{pmatrix} 14  31 \\\\ 31  69 \\end{pmatrix}\n$$\n$$\nX^T y = \\begin{pmatrix} 1  2  3 \\\\ 2  4  7 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 10 \\\\ 17 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 5 + 2 \\cdot 10 + 3 \\cdot 17 \\\\ 2 \\cdot 5 + 4 \\cdot 10 + 7 \\cdot 17 \\end{pmatrix} = \\begin{pmatrix} 5+20+51 \\\\ 10+40+119 \\end{pmatrix} = \\begin{pmatrix} 76 \\\\ 169 \\end{pmatrix}\n$$\n\n现在，我们计算岭系数向量 $\\hat{\\beta}_{Ridge}$。我们需要矩阵 $(X^T X + \\lambda I)^{-1}$，其中 $\\lambda = 1$。令 $S = X^T X$。\n$$\nS + \\lambda I = \\begin{pmatrix} 14  31 \\\\ 31  69 \\end{pmatrix} + 1 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 15  31 \\\\ 31  70 \\end{pmatrix}\n$$\n行列式为 $\\det(S + I) = 15 \\cdot 70 - 31 \\cdot 31 = 1050 - 961 = 89$。\n逆矩阵为：\n$$\n(S + I)^{-1} = \\frac{1}{89} \\begin{pmatrix} 70  -31 \\\\ -31  15 \\end{pmatrix}\n$$\n现在我们计算岭系数：\n$$\n\\hat{\\beta}_{Ridge} = (S+I)^{-1} X^T y = \\frac{1}{89} \\begin{pmatrix} 70  -31 \\\\ -31  15 \\end{pmatrix} \\begin{pmatrix} 76 \\\\ 169 \\end{pmatrix} = \\frac{1}{89} \\begin{pmatrix} 70 \\cdot 76 - 31 \\cdot 169 \\\\ -31 \\cdot 76 + 15 \\cdot 169 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}_{Ridge} = \\frac{1}{89} \\begin{pmatrix} 5320 - 5239 \\\\ -2356 + 2535 \\end{pmatrix} = \\frac{1}{89} \\begin{pmatrix} 81 \\\\ 179 \\end{pmatrix} = \\begin{pmatrix} 81/89 \\\\ 179/89 \\end{pmatrix}\n$$\n两个岭系数为 $\\hat{\\beta}_{Ridge,1} = \\frac{81}{89}$ 和 $\\hat{\\beta}_{Ridge,2} = \\frac{179}{89}$。\n\n接下来，我们计算第一个OLS系数的方差 $\\text{Var}(\\hat{\\beta}_{OLS,1})$。这是 $\\sigma^2 S^{-1}$ 的第一个对角元素。\n$$\n\\det(S) = 14 \\cdot 69 - 31 \\cdot 31 = 966 - 961 = 5\n$$\n$S = X^TX$ 的逆矩阵为：\n$$\nS^{-1} = \\frac{1}{5} \\begin{pmatrix} 69  -31 \\\\ -31  14 \\end{pmatrix}\n$$\n第一个OLS系数的方差为：\n$$\n\\text{Var}(\\hat{\\beta}_{OLS,1}) = \\sigma^2 (S^{-1})_{11} = \\frac{69}{5} \\sigma^2\n$$\n\n现在，我们计算第一个岭系数的方差 $\\text{Var}(\\hat{\\beta}_{Ridge,1})$。这是 $\\sigma^2 (S + I)^{-1} S (S + I)^{-1}$ 的第一个对角元素。\n令 $Z = (S + I)^{-1} = \\frac{1}{89} \\begin{pmatrix} 70  -31 \\\\ -31  15 \\end{pmatrix}$。协方差矩阵是 $\\sigma^2 Z S Z$。\n首先，计算 $SZ$：\n$$\nSZ = \\begin{pmatrix} 14  31 \\\\ 31  69 \\end{pmatrix} \\frac{1}{89} \\begin{pmatrix} 70  -31 \\\\ -31  15 \\end{pmatrix} = \\frac{1}{89} \\begin{pmatrix} 14(70) + 31(-31)  14(-31) + 31(15) \\\\ 31(70) + 69(-31)  31(-31) + 69(15) \\end{pmatrix} = \\frac{1}{89} \\begin{pmatrix} 980-961  -434+465 \\\\ 2170-2139  -961+1035 \\end{pmatrix} = \\frac{1}{89} \\begin{pmatrix} 19  31 \\\\ 31  74 \\end{pmatrix}\n$$\n现在，计算 $Z(SZ)$：\n$$\nZ(SZ) = \\left( \\frac{1}{89} \\begin{pmatrix} 70  -31 \\\\ -31  15 \\end{pmatrix} \\right) \\left( \\frac{1}{89} \\begin{pmatrix} 19  31 \\\\ 31  74 \\end{pmatrix} \\right) = \\frac{1}{89^2} \\begin{pmatrix} 70(19) - 31(31)  70(31) - 31(74) \\\\ -31(19) + 15(31)  -31(31) + 15(74) \\end{pmatrix}\n$$\n我们需要的第一个系数的方差是 $(1,1)$ 项：\n$$\n(ZSZ)_{11} = \\frac{1}{89^2} (70 \\cdot 19 - 31 \\cdot 31) = \\frac{1}{7921} (1330 - 961) = \\frac{369}{7921}\n$$\n第一个岭系数的方差是：\n$$\n\\text{Var}(\\hat{\\beta}_{Ridge,1}) = \\sigma^2 (ZSZ)_{11} = \\frac{369}{7921} \\sigma^2\n$$\n\n最后，我们计算第一个岭系数估计的方差与第一个OLS系数估计的方差之比 $r$。\n$$\nr = \\frac{\\text{Var}(\\hat{\\beta}_{Ridge,1})}{\\text{Var}(\\hat{\\beta}_{OLS,1})} = \\frac{\\frac{369}{7921} \\sigma^2}{\\frac{69}{5} \\sigma^2} = \\frac{369}{7921} \\cdot \\frac{5}{69}\n$$\n我们可以简化这个分数。注意 $369 = 9 \\times 41$ 和 $69 = 3 \\times 23$。\n$$\nr = \\frac{369}{69} \\cdot \\frac{5}{7921} = \\frac{123}{23} \\cdot \\frac{5}{7921} = \\frac{615}{182183}\n$$\n这个分数无法进一步简化，因为 $615 = 3 \\times 5 \\times 41$， $182183 = 23 \\times 89^2$，它们没有共同的质因数。\n\n最终答案是一个行向量，包含两个岭系数和比率 $r$。\n$$\n\\begin{pmatrix} \\hat{\\beta}_{Ridge,1}  \\hat{\\beta}_{Ridge,2}  r \\end{pmatrix} = \\begin{pmatrix} \\frac{81}{89}  \\frac{179}{89}  \\frac{615}{182183} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{81}{89}  \\frac{179}{89}  \\frac{615}{182183}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Lasso 不仅是一种正则化工具，也是一种强大的特征选择方法，但它的选择机制也带来了一个微妙的后果：对真实且重要的预测变量的系数产生系统性的向零偏差。本练习将挑战你思考标准 Lasso 拟合的局限性，并探索一种被称为“去偏”（debiasing）的强大两阶段修正程序。掌握这一程序及其正确的验证方法，是成为一名成熟的数据分析师的关键一步。",
            "id": "4190269",
            "problem": "一个系统神经科学实验室正在构建一个线性编码模型，用以根据一组时滞刺激特征和行为协变量来预测经试次平均和z-score标准化的神经元发放率。令响应向量为 $y \\in \\mathbb{R}^{n}$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其各列已标准化至零均值和单位方差。假设数据遵循线性高斯模型 $y = X \\beta^{\\star} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ 且 $\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是稀疏的。该实验室通过求解以下问题来拟合最小绝对收缩和选择算子 (Lasso)：\n$$\n\\hat{\\beta}^{\\text{lasso}} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2n} \\lVert y - X \\beta \\rVert_{2}^{2} + \\lambda \\lVert \\beta \\rVert_{1} \\right\\}\n$$\n其中调节参数 $\\lambda  0$ 通过 K 折交叉验证 (CV) 选择。他们观察到，$\\beta^{\\star}$ 中几个真实值较大的系数被保留在所选的支持集中，但它们的幅度似乎向零衰减。该团队考虑采用一个两阶段程序来减轻这种衰减：首先拟合 Lasso 以选择一个支持集 $\\hat{S} = \\{ j : \\hat{\\beta}^{\\text{lasso}}_{j} \\neq 0 \\}$，然后在 $\\hat{S}$ 上重新拟合一个无惩罚项的最小二乘模型。\n\n在这一神经科学背景下，以下哪些陈述正确地解释了衰减的来源以及所提出的重新拟合方案的统计特性和实用性？\n\nA. 对于任何 $\\lambda > 0$，所选支持集 $\\hat{S}$ 内的 Lasso 解满足一阶最优性条件，这些条件意味着，相对于在 $\\hat{S}$ 上的最小二乘拟合，拟合的系数会系统性地向零收缩；在 $\\hat{S}$ 上重新拟合普通最小二乘法 (OLS) 可以消除在给定支持集下的这种收缩偏差，并且通常以增加方差为代价来减少系数偏差。仅当 $|\\hat{S}|  n$ 且 $X_{\\hat{S}}$ 具有满列秩时，这种重新拟合才是良定义的。\n\nB. 因为 Lasso 执行系数的硬阈值处理，任何幸存下来的系数（即，任何 $j \\in \\hat{S}$）都是无偏的，并且等于其支持集上的 OLS 估计，所以没有衰减需要纠正。\n\nC. 对于任何设计矩阵 $X$，无论多重共线性或样本大小如何，在 Lasso 选择的支持集上重新拟合 OLS 总是能够严格地改善估计误差和预测风险，优于原始的 Lasso。\n\nD. 在 $p > n$ 的高维情况下，即使 $|\\hat{S}| \\ge n$，在 $\\hat{S}$ 上的无惩罚项重新拟合仍然是良态的，因为 Lasso 选择的特征保证是相互正交的。\n\nE. 为了获得该两阶段程序样本外性能的无偏估计，整个流程——通过 Lasso 选择 $\\hat{S}$ 并在 $\\hat{S}$ 上重新拟合——必须被包含在重采样循环中（例如，在交叉验证 (CV) 的每一折内或在嵌套 CV 的外层循环中），而不是在完整数据上进行选择，然后在折内进行重新拟合。\n\nF. 当 $X_{\\hat{S}}$ 由于所选神经协变量之间的共线性而呈病态时，将无惩罚项的重新拟合替换为带一个小的 $\\ell_2$ 惩罚项的重新拟合（在 $\\hat{S}$ 上进行岭回归），可以产生有利的偏差-方差权衡，通常能改善相对于原始 Lasso 和无惩罚项重新拟合的预测性能，同时仍然在很大程度上减轻 $\\hat{S}$ 内的 Lasso 收缩。",
            "solution": "本题要求在神经科学数据的线性编码模型背景下，评估关于 LASSO 估计器性质和一个两阶段重新拟合程序的几个陈述。我们将首先为 LASSO 的行为建立理论基础，然后分析每个陈述。\n\n线性模型由 $y = X \\beta^{\\star} + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^{n}$，$X \\in \\mathbb{R}^{n \\times p}$，$\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是稀疏的，且 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。LASSO 估计器是以下凸优化问题的解：\n$$\n\\hat{\\beta}^{\\text{lasso}} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2n} \\lVert y - X \\beta \\rVert_{2}^{2} + \\lambda \\lVert \\beta \\rVert_{1} \\right\\}\n$$\n对于某个 $\\lambda  0$。\n\n观察到的衰减（收缩偏差）的来源可以从一阶最优性条件，也称为 Karush–Kuhn–Tucker (KKT) 条件来理解。令 $L(\\beta) = \\frac{1}{2n} \\lVert y - X \\beta \\rVert_{2}^{2} + \\lambda \\lVert \\beta \\rVert_{1}$。最小二乘项的梯度是 $\\nabla_{\\beta} \\left( \\frac{1}{2n} \\lVert y - X \\beta \\rVert_{2}^{2} \\right) = -\\frac{1}{n} X^T (y - X\\beta)$。$\\ell_1$ 范数项的次梯度是一个向量 $g \\in \\mathbb{R}^p$，其中：\n$$\ng_j = \\begin{cases} \\text{sign}(\\beta_j)  \\text{if } \\beta_j \\neq 0 \\\\ v_j, \\text{ where } v_j \\in [-1, 1]  \\text{if } \\beta_j = 0 \\end{cases}\n$$\nKKT 条件指出，在最小值 $\\hat{\\beta}^{\\text{lasso}}$ 处，我们必须有 $0 \\in -\\frac{1}{n} X^T (y - X\\hat{\\beta}^{\\text{lasso}}) + \\lambda g$，其中 $g \\in \\partial \\lVert \\hat{\\beta}^{\\text{lasso}} \\rVert_1$。这给出了第 $j$ 个分量的两种情况：\n1.  如果 $\\hat{\\beta}^{\\text{lasso}}_j \\neq 0$（即，$j \\in \\hat{S}$），则 $g_j = \\text{sign}(\\hat{\\beta}^{\\text{lasso}}_j)$，条件变为：\n    $$\n    -\\frac{1}{n} X_j^T (y - X\\hat{\\beta}^{\\text{lasso}}) + \\lambda \\, \\text{sign}(\\hat{\\beta}^{\\text{lasso}}_j) = 0 \\quad \\implies \\quad \\frac{1}{n} X_j^T (y - X\\hat{\\beta}^{\\text{lasso}}) = \\lambda \\, \\text{sign}(\\hat{\\beta}^{\\text{lasso}}_j)\n    $$\n2.  如果 $\\hat{\\beta}^{\\text{lasso}}_j = 0$（即，$j \\notin \\hat{S}$），则 $g_j \\in [-1, 1]$，条件变为：\n    $$\n    -\\frac{1}{n} X_j^T (y - X\\hat{\\beta}^{\\text{lasso}}) + \\lambda g_j = 0 \\quad \\implies \\quad \\left| \\frac{1}{n} X_j^T (y - X\\hat{\\beta}^{\\text{lasso}}) \\right| \\le \\lambda\n    $$\n对于任何活动系数（$j \\in \\hat{S}$），相应特征向量 $X_j$ 与残差向量的内积固定为 $\\pm n\\lambda$。相反，在同一支持集 $\\hat{S}$ 上的无惩罚普通最小二乘（OLS）拟合将产生满足正规方程 $X_{\\hat{S}}^T (y - X_{\\hat{S}} \\hat{\\beta}^{\\text{ols}}_{\\hat{S}}) = 0$ 的系数 $\\hat{\\beta}^{\\text{ols}}_{\\hat{S}}$。LASSO 的 KKT 条件对活动预测变量所要求的非零残差相关性是系数收缩的来源；为了满足这个条件，幅度 $|\\hat{\\beta}^{\\text{lasso}}_j|$ 必须小于相应的无惩罚 OLS 估计值。这种现象被称为软阈值处理。\n\n提出的两阶段程序包括首先选择支持集 $\\hat{S} = \\{ j : \\hat{\\beta}^{\\text{lasso}}_{j} \\neq 0 \\}$，然后在这个子集上计算一个 OLS 估计：$\\hat{\\beta}^{\\text{refit}} = (X_{\\hat{S}}^T X_{\\hat{S}})^{-1} X_{\\hat{S}}^T y$。这个程序有时被称为“松弛 LASSO”或“去偏 LASSO”。\n\n现在我们评估每个陈述。\n\n**A. 对于任何 $\\lambda > 0$，所选支持集 $\\hat{S}$ 内的 LASSO 解满足一阶最优性条件，这些条件意味着，相对于在 $\\hat{S}$ 上的最小二乘拟合，拟合的系数会系统性地向零收缩；在 $\\hat{S}$ 上重新拟合普通最小二乘法 (OLS) 可以消除在给定支持集下的这种收缩偏差，并且通常以增加方差为代价来减少系数偏差。仅当 $|\\hat{S}|  n$ 且 $X_{\\hat{S}}$ 具有满列秩时，这种重新拟合才是良定义的。**\n\n-   第一部分，通过最优性条件解释收缩，如上所述是正确的。LASSO 的 KKT 条件强制活动预测变量和残差之间存在非零相关性，导致相对于在相同支持集上的 OLS 拟合（其相关性为零）的收缩。\n-   第二部分正确地指出了重新拟合的效果：它消除了所选系数依赖于 $\\lambda$ 的收缩偏差。这是因为在给定正确模型的条件下，OLS 是无偏的。然而，移除正则化（惩罚项）通常会增加估计的方差。对均方误差的净效应取决于具体的偏差-方差权衡。这个描述是准确的。\n-   第三部分陈述了 OLS 重新拟合是良定义的条件。OLS 解需要求 $X_{\\hat{S}}^T X_{\\hat{S}}$ 的逆。这个逆存在当且仅当 $X_{\\hat{S}}$ 具有满列秩，这意味着列是线性无关的，并且所选特征的数量小于或等于样本数量 ($|\\hat{S}| \\le n$)。通常需要严格不等式 $|\\hat{S}|  n$ 以获得稳定的解。这是一个正确且必要的条件。\n-   结论：**正确**。\n\n**B. 因为 LASSO 执行系数的硬阈值处理，任何幸存下来的系数（即，任何 $j \\in \\hat{S}$）都是无偏的，并且等于其支持集上的 OLS 估计，所以没有衰减需要纠正。**\n\n-   LASSO 执行的是软阈值处理，而不是硬阈值处理。硬阈值处理会将低于某个幅度的系数设为零，而保持其他系数为其无惩罚值（例如 OLS 值）。软阈值处理则连续地将所有系数向零收缩，并在它们越过一个阈值时将其设为零。\n-   声称幸存的系数是无偏的且等于其支持集上的 OLS 估计是错误的，这一点已由 KKT 条件所证实。$\\ell_1$ 惩罚项明确地为所有非零系数引入了朝向零的偏差（衰减）。\n-   结论：**不正确**。\n\n**C. 对于任何设计矩阵 $X$，无论多重共线性或样本大小如何，在 LASSO 选择的支持集上重新拟合 OLS 总是能够严格地改善估计误差和预测风险，优于原始的 LASSO。**\n\n-   “严格地改善”这个术语是一个绝对的断言，这是错误的。虽然重新拟合通常通过减少偏差来改善性能，但这并非保证。通过移除惩罚项的正则化效应，估计的方差会增加。如果方差的增加大于偏差平方的减少，则总的均方误差（估计误差）和预测风险将会恶化。\n-   “对于任何设计矩阵 $X$，无论多重共线性如何”这一条款也是错误的。如果 $X_{\\hat{S}}$ 中所选的特征是高度共线的，OLS 重新拟合的方差可能会非常大，导致解比稳定的 LASSO 估计差得多。\n-   结论：**不正确**。\n\n**D. 在 $p > n$ 的高维情况下，即使 $|\\hat{S}| \\ge n$，在 $\\hat{S}$ 上的无惩罚项重新拟合仍然是良态的，因为 LASSO 选择的特征保证是相互正交的。**\n\n-   “LASSO 选择的特征保证是相互正交的”这个前提是错误的。LASSO 并不提供这样的保证。在存在相关特征的情况下，LASSO 可能会从一组中选择一个，但最终选择的特征集仍然可能包含显著的相关性。\n-   对于 $|\\hat{S}| \\ge n$ 的情况，重新拟合是良态的这一结论也是错误的。如果预测变量的数量 ($|\\hat{S}|$) 大于样本数量 ($n$)，无惩罚的 OLS 回归是欠定的；如果 $|\\hat{S}| = n$，则可能是病态的。没有进一步的正则化，唯一解是不存在的。\n-   结论：**不正确**。\n\n**E. 为了获得该两阶段程序样本外性能的无偏估计，整个流程——通过 LASSO 选择 $\\hat{S}$ 并在 $\\hat{S}$ 上重新拟合——必须被包含在重采样循环中（例如，在交叉验证 (CV) 的每一折内或在嵌套 CV 的外层循环中），而不是在完整数据上进行选择，然后在折内进行重新拟合。**\n\n-   这个陈述描述了正确模型验证的一个基本原则。任何使用数据来构建模型结构的步骤，包括特征选择，都必须是所评估程序的一部分。\n-   如果在交叉验证之前对整个数据集执行特征选择（第一阶段），那么选择过程就已经被那些稍后将在每一折中用作测试集的数据所“告知”。这种从测试集到模型训练过程的“信息泄露”会使性能估计失效，通常导致对真实样本外误差的评估过于乐观（向下偏倚）。\n-   因此，为了获得对泛化误差的恰当估计，整个两阶段流程必须被视为一个单一的模型拟合程序，并在交叉验证循环的每一折内独立执行，仅使用该折的训练数据。\n-   结论：**正确**。\n\n**F. 当 $X_{\\hat{S}}$ 由于所选神经协变量之间的共线性而呈病态时，将无惩罚项的重新拟合替换为带一个小的 $\\ell_2$ 惩罚项的重新拟合（在 $\\hat{S}$ 上进行岭回归），可以产生有利的偏差-方差权衡，通常能改善相对于原始 LASSO 和无惩罚项重新拟合的预测性能，同时仍然在很大程度上减轻 $\\hat{S}$ 内的 LASSO 收缩。**\n\n-   如对选项 C 的讨论，$X_{\\hat{S}}$ 中的病态（高度共线性）是 OLS 重新拟合的一个主要问题，因为它会夸大系数估计的方差。\n-   在所选子集 $X_{\\hat{S}}$ 上应用岭回归（$\\ell_2$ 惩罚）是解决此问题的标准技术。$\\ell_2$ 惩罚通过在 $X_{\\hat{S}}^T X_{\\hat{S}}$ 的对角线上增加一个小的正值来稳定矩阵求逆，这以重新引入少量偏差为代价来减小估计的方差。\n-   这个过程确实可以产生比原始 LASSO（可能有太多偏差）或 OLS 重新拟合（可能有太多方差）更好的偏差-方差权衡。它仍然减轻了原始的、通常是严重的 LASSO 收缩，因为岭惩罚通常选择得较小，允许系数比其 LASSO 对应值变得更大。这种方法让人联想到弹性网络正则化器。\n-   结论：**正确**。",
            "answer": "$$\\boxed{AEF}$$"
        },
        {
            "introduction": "在学习了如何拟合和优化正则化模型之后，最后关键的一步是公正地评估它们的预测性能，这在高维神经科学数据中尤其困难，因为其中充满了伪相关。本练习聚焦于一个关键且常被误解的问题：“双重蘸取”（double dipping）或称选择性偏误。它将巩固你的理解，即为何用于模型选择和训练的数据必须与用于最终性能评估的数据严格分开，并介绍嵌套交叉验证等最佳实践。",
            "id": "4190259",
            "problem": "一个实验室正在分析一个功能性磁共振成像 (fMRI) 数据集，以根据体素级预测变量 $X \\in \\mathbb{R}^{p}$ 来预测行为得分 $Y$。其中有 $n$ 名参与者和 $p$ 个体素，且 $p \\gg n$。该团队计划使用最小绝对收缩和选择算子 (Lasso) 来选择一个稀疏的体素集并估计其系数，然后报告泛化误差。一位同事建议先使用整个数据集选择特征并估计系数，然后运行 $K$ 折交叉验证 (CV) 来估计误差。\n\n从基本原理的角度来看，对于一个预测器 $f$，考虑其风险 $R(f) = \\mathbb{E}\\left[(Y - f(X))^{2}\\right]$，并假设样本 $(X_{i}, Y_{i})$ 是从一个固定分布中抽取的独立同分布 (i.i.d.) 样本。在高维神经科学背景下（例如，体素活动中 $p \\gg n$），基于经验相关性的特征筛选是常见的。在真实系数为零的全局零假设模型下，经验性特征筛选会隐式地选择最大的经验相关性 $r_{j} = \\frac{1}{n}\\sum_{i=1}^{n} X_{ij} Y_{i}$，根据亚高斯随机变量的经典尾部界限，这些相关性满足 $\\mathbb{E}\\left[\\max_{1 \\le j \\le p} |r_{j}|\\right] \\asymp \\sqrt{\\frac{\\log p}{n}}$。以这样的选择事件为条件进行估计，会在模型选择和用于估计其风险的数据之间引入依赖关系。\n\n选择所有正确描述“使用相同数据选择特征和估计系数的后果”并“在此神经科学数据分析背景下为 Lasso 提出有效的嵌套或样本分割程序”的陈述。\n\nA. 使用相同的数据进行特征筛选和系数估计会产生选择性偏误（也称为“二次蘸取”，double dipping）：以选择事件为条件，被选中的特征会具有向上偏倚的经验关联，从而导致对泛化误差的估计过于乐观。一个针对 Lasso 的有效嵌套交叉验证程序是：将数据划分为 $F$ 个外层折；对于每个外层折，将其留出部分视为评估集，在剩余的数据内部执行内层交叉验证，仅使用内层训练分割来选择正则化参数 $\\lambda$（并因此选择特征集）；使用选择的 $\\lambda$ 在整个外层训练集上重新拟合 Lasso；在外层评估集上评估预测结果；汇总所有外层折的误差以估计 $R(f)$；最后，如果需要一个单一的部署模型，则使用内层交叉验证选择的 $\\lambda$ 在完整数据集上重新拟合 Lasso。这打破了选择/估计与评估之间的依赖关系。\n\nB. 因为 Lasso 会收缩系数，所以在完整数据集上执行特征选择和系数估计，然后使用相同数据对所选特征计算普通最小二乘置信区间，可以得到无偏的推断和泛化误差；额外的嵌套或分割是不必要的。\n\nC. 在高维神经科学中，一个有效的样本分割方案是将数据集划分为三个不相交的集合：一个筛选集、一个调优-训练集和一个最终测试集。使用筛选集预选一个候选特征池（例如，基于经验相关性），在调优-训练集内部使用内层交叉验证来选择 $\\lambda$ 并拟合 Lasso，可以选择性地使用调优后的 $\\lambda$ 在筛选集和调优-训练集的并集上重新拟合 Lasso，并且只在未接触过的测试集上报告泛化误差。这个程序确保了所报告的测试误差不会因选择过程而产生偏误。\n\nD. 在 $p \\gg n$ 的情况下，正则化保证了使用相同数据进行特征选择和系数估计不会使估计的测试误差产生偏误；对全数据 Lasso 拟合的残差进行自助法重采样足以在不进行嵌套或分割的情况下获得无偏的泛化误差。\n\n选择所有适用的选项。",
            "solution": "用户提供了一个关于在高维统计背景（`$p \\gg n$`）下进行模型验证的问题陈述，这种情况在神经科学数据分析中很常见。核心任务是识别出关于有缺陷的验证程序的陷阱以及像嵌套交叉验证或数据分割等有效替代方案的正确实施的正确陈述。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n*   **领域：** 功能性磁共振成像 (fMRI) 数据分析。\n*   **目标：** 从体素级预测变量 `$X \\in \\mathbb{R}^{p}$` 预测行为得分 `$Y$`。\n*   **数据维度：** 高维，`$p$`（体素/预测变量数量）远大于 `$n$`（参与者/样本数量），即 `$p \\gg n$`。\n*   **提议的有缺陷的程序：** 一位同事建议先使用整个数据集选择特征并估计系数，然后对同一数据集运行 `$K$` 折交叉验证 (CV) 来估计预测误差。\n*   **关注的方法：** 最小绝对收缩和选择算子 (Lasso)。\n*   **理论背景：**\n    *   目标是为预测器 `$f$` 估计真实风险 `$R(f) = \\mathbb{E}\\left[(Y - f(X))^{2}\\right]$`。\n    *   数据 `$(X_{i}, Y_{i})$` 被假定为独立同分布 (i.i.d.)。\n    *   在全局零假设模型（无真实效应）下，最大经验相关性 `$r_{j} = \\frac{1}{n}\\sum_{i=1}^{n} X_{ij} Y_{i}$` 预计为非零，具体来说 `$\\mathbb{E}\\left[\\max_{1 \\le j \\le p} |r_{j}|\\right] \\asymp \\sqrt{\\frac{\\log p}{n}}$`。\n    *   核心问题是，如果使用相同的数据进行模型估计和特征选择，那么将模型估计以特征选择事件为条件会引入依赖性，从而使风险估计产生偏误。\n\n**步骤2：使用提取的已知条件进行验证**\n\n问题陈述是**有效的**。\n\n1.  **科学上合理：** 该问题牢固地植根于已建立的统计学习理论原则。选择性偏误问题，也称为“二次蘸取”或“赢家诅咒”，是高维数据分析中一个关键且有充分记载的现象。所提供的关于最大伪相关的理论界限是高维概率论中的一个标准结果，并正确地形式化了问题的根源。fMRI 分析的背景是这个问题普遍存在且严重的典型现实世界例子。\n2.  **提问清晰：** 问题表述清晰。它要求用户识别那些既描述了已知统计谬误又提出了标准的、有效的纠正程序的正确陈述。这要求对模型验证有概念性的理解，而不仅仅是机械计算。\n3.  **客观性：** 语言技术性强、精确，没有主观或模糊的术语。\n\n**步骤3：结论与行动**\n\n问题是有效的。我将继续提供解答。\n\n### 推导与选项分析\n\n关键的基本原则是，用于训练模型（包括所有选择和调优步骤）的数据与用于评估其泛化性能的数据之间必须保持统计独立性。同事的提议违反了这一原则。通过首先在*整个*数据集上拟合一个 Lasso 模型，所有样本的信息都被用来选择特征（即非零系数的体素）。随后执行 `$K$` 折交叉验证来估计此程序的误差是有偏的。在每一折中，模型都是在一组特征上进行训练的，而这些特征的选择部分地利用了该折的留出测试集的信息。这种“信息泄露”会导致系统性地过于乐观（被低估）的泛化误差。在 `$p \\gg n$` 的情况下，这个问题会加剧，因为发现大的伪相关的概率很高，正如所提供的尾部界限所示。\n\n一个有效的程序必须确保用于最终性能评估的数据在任何方面都没有被用于选择特征、调整超参数或训练被评估的模型。\n\n#### 选项 A\n\n该陈述提出了两个主要主张：一个关于问题的性质，另一个关于一个有效的解决方案。\n\n1.  **对后果的描述：** “使用相同的数据进行特征筛选和系数估计会产生选择性偏误（也称为“二次蘸取”）：以选择事件为条件，被选中的特征会具有向上偏倚的经验关联，从而导致对泛化误差的估计过于乐观。” 这是对该统计问题的精确且正确的描述。当我们从 `$p$` 个候选特征的大池中根据与结果 `$Y$` 的高经验相关性来选择特征时，我们很可能会选择一些因随机机会（抽样变异性）而非真实潜在关联而表现出高相关性的特征。在产生这些伪高相关性的同一数据上评估模型的性能，自然会得出过于乐观的结果。\n\n2.  **提议的解决方案（嵌套交叉验证）：** 该陈述概述了一个嵌套交叉验证程序。让我们分析其步骤：\n    *   数据被分成 `$F$` 个外层折。对于每个折 `$f \\in \\{1, ..., F\\}$`，该折被留出作为外层评估集。\n    *   模型选择过程（选择 Lasso 正则化参数 `$\\lambda$`）*仅*在剩余的 `$F-1$` 个折（外层训练集）上使用*内层*交叉验证循环进行。\n    *   关键是，外层评估集在此内层循环期间未被触及。\n    *   一旦找到最优的 `$\\lambda$`，就使用这个 `$\\lambda$` 在整个外层训练集上训练一个模型。\n    *   然后在这个原始的外层评估集上评估该模型的性能。\n    *   来自所有 `$F$` 个外层折的误差被汇总，以产生一个单一的、无偏的泛化误差 `$R(f)$` 估计。\n    *   最后一步，即在所有数据上重新拟合以产生一个可部署的模型，也是标准做法。\n\n这个程序正确地将每个外层折中的评估数据与模型选择和训练过程隔离开来，从而“打破了依赖关系”，并得出了一个无偏的泛化误差估计。\n\n**A选项的结论：正确**\n\n#### 选项 B\n\n该陈述声称：“因为 Lasso 会收缩系数，所以在完整数据集上执行特征选择和系数估计，然后使用相同数据对所选特征计算普通最小二乘置信区间，可以得到无偏的推断和泛化误差”。\n\n这在多个方面都是不正确的。\n1.  **收缩与偏误：** 虽然与普通最小二乘法 (OLS) 相比，Lasso 的收缩有助于在 `$p \\gg n$` 的情况下减少方差和防止过拟合，但如果验证程序不当，它并不能消除*验证程序*中的选择性偏误。\n2.  **选择后推断：** 对于通过数据驱动过程（如 Lasso）在同一数据上选择的系数，计算标准的 OLS 置信区间是一种统计上无效的做法。这没有考虑到选择步骤本身的不确定性，导致置信区间过窄，p值过小。这个研究领域被称为选择后推断，它需要专门的方法来产生有效的结果。标准 OLS 并非其中之一。\n3.  **泛化误差：** 如前所述，使用相同的数据进行选择和评估会导致对泛化误差的估计过于乐观。该陈述称此过程是“无偏的”是错误的。\n\n**B选项的结论：不正确**\n\n#### 选项 C\n\n该陈述提出了一个样本分割方案。\n\n1.  **程序：** 数据被划分为三个不相交的集合：筛选集、调优-训练集和最终测试集。\n    *   **筛选集：** 用于执行初步的、粗略的特征选择。这将初始筛选与后续步骤隔离开来。\n    *   **调优-训练集：** 用于所有模型开发，包括通过 CV 进行超参数（`$\\lambda$`）调优和拟合 Lasso 模型。\n    *   **最终测试集：** 这个集合被留出，只在最后使用一次，以报告最终的泛化误差。\n2.  **有效性：** 该程序严格执行了用于模型构建和评估不同阶段的数据分离。最终测试集在特征选择、超参数调优或系数估计的任何部分都保持“未被触及”。因此，在该测试集上测量的性能提供了模型在新数据上泛化误差的无偏估计。在最终评估前，在筛选集和调优-训练集的组合上重新拟合的可选步骤也是有效的，因为它仍然不涉及测试集。\n\n这种三向分割是避免选择性偏误的标准且有效的方法，特别是在初始特征数量极大且预筛选步骤在计算上或方法上是可取的情况下。\n\n**C选项的结论：正确**\n\n#### 选项 D\n\n该陈述声称：“在 `$p \\gg n$` 的情况下，正则化保证了使用相同数据进行特征选择和系数估计不会使估计的测试误差产生偏误；对全数据 Lasso 拟合的残差进行自助法重采样足以在不进行嵌套或分割的情况下获得无偏的泛化误差。”\n\n这是不正确的。\n1.  **正则化的作用：** 正则化是*模型拟合*过程的一部分，而不是*模型验证*协议的一部分。它不能纠正一个有缺陷的验证协议。偏误问题源于重复使用数据进行选择和测试，正则化并不能解决这个缺陷。\n2.  **自助法的有效性：** 对一个本身使用整个数据集选择的模型应用标准自助法程序来估计预测误差，将会继承同样的乐观性和选择性偏误。自助法样本是从原始数据集中抽取的。如果原始数据集产生了一个看起来虚假地好的模型，那么从中抽取的自助法样本也倾向于产生看起来虚假地好的模型，从而导致有偏的误差估计。虽然更高级的自助法（例如 `.632+` 估计量）旨在部分纠正这个问题，但如此处所述的简单残差自助法并非“足够”，也不能替代正确的数据分割或嵌套。\n\n**D选项的结论：不正确**\n\n### 最终结论\n\n陈述 A 和 C 正确地指出了选择性偏误的问题，并提出了统计上有效的程序（分别是嵌套交叉验证和三向数据分割），以获得对泛化误差的无偏估计。陈述 B 和 D 基于在高维背景下对正则化、选择后推断和自助法作用的常见但深刻的误解。",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}