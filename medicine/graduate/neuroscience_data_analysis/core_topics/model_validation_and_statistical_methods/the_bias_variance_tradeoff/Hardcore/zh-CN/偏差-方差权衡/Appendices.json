{
    "hands_on_practices": [
        {
            "introduction": "我们从神经科学中的一个基本技术——信号平均法开始。在记录局部场电位（LFP）或事件相关电位（ERP）时，研究人员通常通过平均多次试验的响应来提高信噪比。这个练习将展示对抗方差最直接的方法，即通过平均多个独立的试验来降低估计的方差，而不引入任何偏差。通过这个练习 ，你将巩固对均方误差（MSE）中方差成分的理解，为后续探索更复杂的有偏估计器场景打下坚实的基础。",
            "id": "4198188",
            "problem": "在重复呈现感觉刺激的同时，记录皮层局部场电位 (LFP) 的诱发振幅。在第 $i$ 次试次中，测量的振幅被建模为 $Y_{i} = \\theta + \\epsilon_{i}$，其中 $\\theta$ 是一个固定但未知的响应振幅，$\\epsilon_{i}$ 是加性噪声。假设噪声变量 $\\epsilon_{1}, \\epsilon_{2}, \\ldots, \\epsilon_{m}$ 是独立同分布的，其期望为 $\\mathbb{E}[\\epsilon_{i}] = 0$，方差为 $\\operatorname{Var}(\\epsilon_{i}) = \\sigma^{2}$。通过对 $m$ 次独立试次进行平均来构建 $\\theta$ 的一个估计量：$\\hat{\\theta}_{m} = \\frac{1}{m}\\sum_{i=1}^{m} Y_{i}$。仅从期望、方差和均方误差 (MSE) 的核心定义以及独立性的性质出发，推导在这个神经科学背景下，估计量 $\\hat{\\theta}_{m}$ 相对于单次试次测量的方差缩减因子。然后，当估计量是无偏时，使用推导出的表达式并结合均方误差 (MSE) 分解为偏差和方差的方法，确定使 $\\hat{\\theta}_{m}$ 的 MSE 小于或等于目标阈值 $T$ 所需的最小试次次数 $m$。计算时，使用 $\\sigma^{2} = 9$（单位为 $(\\mu\\mathrm{V})^{2}$）和 $T = 0.75$（单位为 $(\\mu\\mathrm{V})^{2}$）。将最终的试次次数表示为整数。",
            "solution": "问题指定了一个测量模型 $Y_{i} = \\theta + \\epsilon_{i}$，其独立噪声满足 $\\mathbb{E}[\\epsilon_{i}] = 0$ 且 $\\operatorname{Var}(\\epsilon_{i}) = \\sigma^{2}$。通过平均 $m$ 次试次形成的估计量为\n$$\n\\hat{\\theta}_{m} = \\frac{1}{m}\\sum_{i=1}^{m} Y_{i}.\n$$\n我们从核心定义开始。期望算子 $\\mathbb{E}[\\cdot]$ 满足线性性，方差算子 $\\operatorname{Var}(\\cdot)$ 对于任意标量 $a$ 和随机变量 $X$ 满足 $\\operatorname{Var}(aX) = a^{2}\\operatorname{Var}(X)$，且对于独立随机变量 $X$ 和 $Z$ 满足 $\\operatorname{Var}(X+Z) = \\operatorname{Var}(X) + \\operatorname{Var}(Z)$。参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的均方误差 (MSE) 定义为\n$$\n\\operatorname{MSE}(\\hat{\\theta}) = \\mathbb{E}\\!\\left[(\\hat{\\theta} - \\theta)^{2}\\right],\n$$\n并可通过以下公式分解为偏差和方差：\n$$\n\\operatorname{MSE}(\\hat{\\theta}) = \\left(\\mathbb{E}[\\hat{\\theta}] - \\theta\\right)^{2} + \\operatorname{Var}(\\hat{\\theta}).\n$$\n\n我们首先考察 $\\hat{\\theta}_{m}$ 的偏差。根据期望的线性性，\n$$\n\\mathbb{E}[\\hat{\\theta}_{m}] = \\mathbb{E}\\!\\left[\\frac{1}{m}\\sum_{i=1}^{m} Y_{i}\\right] = \\frac{1}{m}\\sum_{i=1}^{m} \\mathbb{E}[Y_{i}].\n$$\n因为 $Y_{i} = \\theta + \\epsilon_{i}$ 且 $\\mathbb{E}[\\epsilon_{i}] = 0$，所以我们有 $\\mathbb{E}[Y_{i}] = \\theta$。因此，\n$$\n\\mathbb{E}[\\hat{\\theta}_{m}] = \\frac{1}{m}\\sum_{i=1}^{m} \\theta = \\theta,\n$$\n所以偏差，定义为 $\\mathbb{E}[\\hat{\\theta}_{m}] - \\theta$，为 $0$。因此，$\\hat{\\theta}_{m}$ 是无偏的，均方误差简化为方差：\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{m}) = \\operatorname{Var}(\\hat{\\theta}_{m}).\n$$\n\n接下来，我们推导 $\\operatorname{Var}(\\hat{\\theta}_{m})$。使用 $\\operatorname{Var}(aX) = a^{2}\\operatorname{Var}(X)$ 和独立性，\n$$\n\\operatorname{Var}\\!\\left(\\hat{\\theta}_{m}\\right) = \\operatorname{Var}\\!\\left(\\frac{1}{m}\\sum_{i=1}^{m} Y_{i}\\right) = \\frac{1}{m^{2}} \\operatorname{Var}\\!\\left(\\sum_{i=1}^{m} Y_{i}\\right) = \\frac{1}{m^{2}} \\sum_{i=1}^{m} \\operatorname{Var}(Y_{i}),\n$$\n其中独立性意味着和的方差等于方差的和。因为 $Y_{i} = \\theta + \\epsilon_{i}$ 且 $\\theta$ 是一个常数，所以 $\\operatorname{Var}(Y_{i}) = \\operatorname{Var}(\\epsilon_{i}) = \\sigma^{2}$。因此，\n$$\n\\operatorname{Var}\\!\\left(\\hat{\\theta}_{m}\\right) = \\frac{1}{m^{2}} \\sum_{i=1}^{m} \\sigma^{2} = \\frac{1}{m^{2}} \\cdot m \\cdot \\sigma^{2} = \\frac{\\sigma^{2}}{m}.\n$$\n\n我们现在将方差缩减因子定义为平均后估计量的方差与单次试次测量的方差之比。单次试次测量的方差为\n$$\n\\operatorname{Var}(Y_{1}) = \\sigma^{2}.\n$$\n因此，方差缩减因子 $\\rho(m)$ 为\n$$\n\\rho(m) = \\frac{\\operatorname{Var}\\!\\left(\\hat{\\theta}_{m}\\right)}{\\operatorname{Var}(Y_{1})} = \\frac{\\sigma^{2}/m}{\\sigma^{2}} = \\frac{1}{m}.\n$$\n这表明，与单次试次相比，对 $m$ 次独立试次求平均会将方差减小为 $1/m$。\n\n因为估计量是无偏的，均方误差等于其方差，所以 MSE 小于或等于目标阈值 $T$ 的条件是\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{m}) = \\frac{\\sigma^{2}}{m} \\leq T.\n$$\n解出 $m$，\n$$\nm \\geq \\frac{\\sigma^{2}}{T}.\n$$\n代入给定值 $\\sigma^{2} = 9$ 和 $T = 0.75$（单位均为 $(\\mu\\mathrm{V})^{2}$，以确保单位一致性），\n$$\nm \\geq \\frac{9}{0.75} = 12.\n$$\n由于 $m$ 必须是整数试次数，且边界恰好为 $12$，因此所需的最小试次次数为 $12$。",
            "answer": "$$\\boxed{12}$$"
        },
        {
            "introduction": "在掌握了简单的信号平均后，我们转向构建预测模型，例如用于预测神经元发放率的线性编码模型。在高维刺激空间中，普通的最小二乘模型常常因为方差过高而导致过拟合。本练习  介绍了岭回归，这是一种通过主动引入少量偏差来换取方差大幅降低的经典正则化技术。你将通过解析推导均方误差，并观察正则化参数 $\\lambda$ 如何调控偏差与方差之间的平衡，从而深刻理解正则化的工作原理。",
            "id": "4198237",
            "problem": "单个神经元的发放率通过一个线性编码模型来建模，以响应高维刺激。在每次试验 $t \\in \\{1,\\dots,n\\}$ 中，刺激向量 $x_t \\in \\mathbb{R}^{p}$ 是从一个均值为零、协方差为 $\\Sigma_{X} \\succ 0$ 的多元正态分布中独立同分布 (i.i.d.) 抽取的，观测到的响应为 $y_t = x_t^{\\top} \\beta^{\\ast} + \\varepsilon_t$，其中 $\\beta^{\\ast} \\in \\mathbb{R}^{p}$ 是真实信号向量，$\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^{2})$ 且已知的 $\\sigma^{2} > 0$。令 $X \\in \\mathbb{R}^{n \\times p}$ 是由行向量 $x_t^{\\top}$ 堆叠而成的设计矩阵。\n\n考虑带有惩罚参数 $\\lambda > 0$ 的岭回归估计量，其定义为 $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_p)^{-1} X^{\\top} y$，其中 $I_p$ 是 $p \\times p$ 的单位矩阵，$y \\in \\mathbb{R}^{n}$ 是响应向量。在试验次数 $n$ 很大的情况下，将格拉姆矩阵近似为 $X^{\\top} X \\approx n \\Sigma_{X}$。\n\n在 $\\Sigma_{X}$ 的特征基中进行分析：令 $\\Sigma_{X} = U \\operatorname{diag}(s_1,\\dots,s_p) U^{\\top}$，其中 $U$ 是正交矩阵且 $s_i > 0$。假设在此基下，真实信号的所有分量都等于一个已知的常数振幅 $B > 0$，即 $U^{\\top} \\beta^{\\ast} = (B,\\dots,B)^{\\top}$。仅使用均方误差 (MSE) 和偏差-方差分解的定义，以及多元正态分布的性质，在 $X^{\\top} X = n \\Sigma_{X}$ 的近似下，推导出均方误差 $\\operatorname{MSE}(\\lambda) = \\mathbb{E}\\big[\\|\\hat{\\beta}_{\\lambda} - \\beta^{\\ast}\\|_{2}^{2}\\big]$ 的精确表达式，并确定使该 MSE 最小的 $\\lambda$ 值。\n\n在最终答案中，仅以闭式解析表达式的形式报告使 MSE 最小的 $\\lambda$。无需四舍五入。",
            "solution": "目标是推导岭回归估计量 $\\hat{\\beta}_{\\lambda}$ 的均方误差 (MSE)，并找到使该误差最小化的正则化参数 $\\lambda$ 的值。MSE 定义为 $\\operatorname{MSE}(\\lambda) = \\mathbb{E}\\big[\\|\\hat{\\beta}_{\\lambda} - \\beta^{\\ast}\\|_{2}^{2}\\big]$。\n\n我们从 MSE 的偏差-方差分解开始：\n$$\n\\operatorname{MSE}(\\hat{\\beta}_{\\lambda}) = \\underbrace{\\|\\mathbb{E}[\\hat{\\beta}_{\\lambda}] - \\beta^{\\ast}\\|_{2}^{2}}_{\\text{偏差平方}} + \\underbrace{\\mathbb{E}\\big[\\|\\hat{\\beta}_{\\lambda} - \\mathbb{E}[\\hat{\\beta}_{\\lambda}]\\|_{2}^{2}\\big]}_{\\text{方差}}\n$$\n期望 $\\mathbb{E}$ 是对噪声 $\\varepsilon$ 和刺激 $X$ 的分布计算的。问题指定使用近似 $X^{\\top} X = n \\Sigma_{X}$，我们在分析中将其视为精确等式。\n\n首先，我们通过代入线性模型 $y = X \\beta^{\\ast} + \\varepsilon$ 来表示估计量 $\\hat{\\beta}_{\\lambda}$：\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_p)^{-1} X^{\\top} y = (X^{\\top} X + \\lambda I_p)^{-1} X^{\\top} (X \\beta^{\\ast} + \\varepsilon)\n$$\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_p)^{-1} X^{\\top} X \\beta^{\\ast} + (X^{\\top} X + \\lambda I_p)^{-1} X^{\\top} \\varepsilon\n$$\n使用近似 $X^{\\top} X = n \\Sigma_{X}$，我们得到：\n$$\n\\hat{\\beta}_{\\lambda} = (n \\Sigma_{X} + \\lambda I_p)^{-1} n \\Sigma_{X} \\beta^{\\ast} + (n \\Sigma_{X} + \\lambda I_p)^{-1} X^{\\top} \\varepsilon\n$$\n\n接下来，我们计算偏差项。$\\hat{\\beta}_{\\lambda}$ 的期望是关于噪声 $\\varepsilon$ 的。鉴于 $\\mathbb{E}[\\varepsilon] = 0$，第二项的期望为零。\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\lambda}] = (n \\Sigma_{X} + \\lambda I_p)^{-1} n \\Sigma_{X} \\beta^{\\ast}\n$$\n因此，偏差向量为：\n$$\n\\operatorname{Bias}(\\hat{\\beta}_{\\lambda}) = \\mathbb{E}[\\hat{\\beta}_{\\lambda}] - \\beta^{\\ast} = (n \\Sigma_{X} + \\lambda I_p)^{-1} n \\Sigma_{X} \\beta^{\\ast} - \\beta^{\\ast}\n$$\n$$\n= \\left[ (n \\Sigma_{X} + \\lambda I_p)^{-1} n \\Sigma_{X} - I_p \\right] \\beta^{\\ast}\n$$\n$$\n= (n \\Sigma_{X} + \\lambda I_p)^{-1} \\left[ n \\Sigma_{X} - (n \\Sigma_{X} + \\lambda I_p) \\right] \\beta^{\\ast}\n$$\n$$\n= -\\lambda (n \\Sigma_{X} + \\lambda I_p)^{-1} \\beta^{\\ast}\n$$\n偏差平方是该向量的欧几里得范数的平方：\n$$\n\\|\\operatorname{Bias}(\\hat{\\beta}_{\\lambda})\\|_{2}^{2} = \\lambda^2 \\| (n \\Sigma_{X} + \\lambda I_p)^{-1} \\beta^{\\ast} \\|_{2}^{2}\n$$\n为简化此表达式，我们在 $\\Sigma_{X} = U \\operatorname{diag}(s_1,\\dots,s_p) U^{\\top}$ 的特征基中进行计算。\n令 $A = n \\Sigma_{X} + \\lambda I_p = U (n \\operatorname{diag}(s_i) + \\lambda I_p) U^{\\top} = U \\operatorname{diag}(ns_i + \\lambda) U^{\\top}$。\n其逆矩阵为 $A^{-1} = U \\operatorname{diag}((ns_i + \\lambda)^{-1}) U^{\\top}$。\nL2范数的平方在正交变换 ($U$) 下是不变的，所以 $\\|v\\|_{2}^{2} = \\|U^{\\top}v\\|_{2}^{2}$。\n$$\n\\|\\operatorname{Bias}(\\hat{\\beta}_{\\lambda})\\|_{2}^{2} = \\lambda^2 \\| U^{\\top} (n \\Sigma_{X} + \\lambda I_p)^{-1} \\beta^{\\ast} \\|_{2}^{2}\n$$\n$$\n= \\lambda^2 \\| \\operatorname{diag}((ns_i + \\lambda)^{-1}) U^{\\top} \\beta^{\\ast} \\|_{2}^{2}\n$$\n给定 $U^{\\top} \\beta^{\\ast} = (B, \\dots, B)^{\\top}$，范数内向量的第 $j$ 个分量是 $\\frac{B}{ns_j + \\lambda}$。将这些分量的平方相加，得到：\n$$\n\\|\\operatorname{Bias}(\\hat{\\beta}_{\\lambda})\\|_{2}^{2} = \\lambda^2 \\sum_{j=1}^{p} \\left(\\frac{B}{ns_j + \\lambda}\\right)^2 = \\lambda^2 B^2 \\sum_{j=1}^{p} \\frac{1}{(ns_j + \\lambda)^2}\n$$\n现在，我们计算方差项。方差是 $\\hat{\\beta}_{\\lambda}$ 的协方差矩阵的迹。\n$$\n\\hat{\\beta}_{\\lambda} - \\mathbb{E}[\\hat{\\beta}_{\\lambda}] = (n \\Sigma_{X} + \\lambda I_p)^{-1} X^{\\top} \\varepsilon\n$$\n以 $X$ 为条件的协方差矩阵是：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\lambda} | X) = \\mathbb{E}_{\\varepsilon} [(\\hat{\\beta}_{\\lambda} - \\mathbb{E}[\\hat{\\beta}_{\\lambda}]) (\\hat{\\beta}_{\\lambda} - \\mathbb{E}[\\hat{\\beta}_{\\lambda}])^{\\top} | X]\n$$\n$$\n= (n \\Sigma_{X} + \\lambda I_p)^{-1} X^{\\top} \\mathbb{E}[\\varepsilon \\varepsilon^{\\top}] X (n \\Sigma_{X} + \\lambda I_p)^{-1}\n$$\n由于 $\\varepsilon_t$ 是独立同分布的，方差为 $\\sigma^2$，所以 $\\mathbb{E}[\\varepsilon \\varepsilon^{\\top}] = \\sigma^2 I_n$。\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\lambda} | X) = \\sigma^2 (n \\Sigma_{X} + \\lambda I_p)^{-1} X^{\\top}X (n \\Sigma_{X} + \\lambda I_p)^{-1}\n$$\n使用近似 $X^{\\top}X = n \\Sigma_X$：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\lambda}) = \\sigma^2 (n \\Sigma_{X} + \\lambda I_p)^{-1} n\\Sigma_X (n \\Sigma_{X} + \\lambda I_p)^{-1}\n$$\n方差是该矩阵的迹。利用迹的循环性质和特征基变换：\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\lambda}) = \\operatorname{Tr}(\\operatorname{Cov}(\\hat{\\beta}_{\\lambda})) = \\sigma^2 \\operatorname{Tr} \\left[ n\\Sigma_X (n \\Sigma_{X} + \\lambda I_p)^{-2} \\right]\n$$\n$$\n= \\sigma^2 \\operatorname{Tr} \\left[ n U \\operatorname{diag}(s_i) U^{\\top} U \\operatorname{diag}((ns_i+\\lambda)^{-2}) U^{\\top} \\right]\n$$\n$$\n= \\sigma^2 \\operatorname{Tr} \\left[ U \\operatorname{diag}\\left(\\frac{ns_i}{(ns_i+\\lambda)^2}\\right) U^{\\top} \\right] = \\sigma^2 \\operatorname{Tr} \\left[ \\operatorname{diag}\\left(\\frac{ns_i}{(ns_i+\\lambda)^2}\\right) \\right]\n$$\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\lambda}) = \\sigma^2 \\sum_{j=1}^{p} \\frac{ns_j}{(ns_j + \\lambda)^2}\n$$\n结合偏差平方和方差，MSE 为：\n$$\n\\operatorname{MSE}(\\lambda) = \\lambda^2 B^2 \\sum_{j=1}^{p} \\frac{1}{(ns_j + \\lambda)^2} + \\sigma^2 \\sum_{j=1}^{p} \\frac{ns_j}{(ns_j + \\lambda)^2} = \\sum_{j=1}^{p} \\frac{\\lambda^2 B^2 + \\sigma^2 ns_j}{(ns_j + \\lambda)^2}\n$$\n为了找到最小值，我们将 $\\operatorname{MSE}(\\lambda)$ 对 $\\lambda$ 求导，并令结果为 $0$。我们可以在求和号内逐项求导。对于每一项 $j$，使用求导的商法则：\n$$\n\\frac{d}{d\\lambda} \\left[ \\frac{\\lambda^2 B^2 + \\sigma^2 ns_j}{(ns_j + \\lambda)^2} \\right] = \\frac{ (2\\lambda B^2)(ns_j+\\lambda)^2 - (\\lambda^2 B^2 + \\sigma^2 ns_j) \\cdot 2(ns_j+\\lambda) }{(ns_j + \\lambda)^4}\n$$\n$$\n= \\frac{ 2(ns_j+\\lambda) \\left[ \\lambda B^2(ns_j+\\lambda) - (\\lambda^2 B^2 + \\sigma^2 ns_j) \\right] }{(ns_j + \\lambda)^4}\n$$\n$$\n= \\frac{ 2 \\left[ \\lambda B^2 ns_j + \\lambda^2 B^2 - \\lambda^2 B^2 - \\sigma^2 ns_j \\right] }{(ns_j + \\lambda)^3}\n$$\n$$\n= \\frac{ 2ns_j (\\lambda B^2 - \\sigma^2) }{(ns_j + \\lambda)^3}\n$$\n总 MSE 的导数是所有项对 $j$ 的求和：\n$$\n\\frac{d}{d\\lambda} \\operatorname{MSE}(\\lambda) = \\sum_{j=1}^{p} \\frac{ 2ns_j (\\lambda B^2 - \\sigma^2) }{(ns_j + \\lambda)^3}\n$$\n我们可以将求和中所有项的公因子提取出来：\n$$\n\\frac{d}{d\\lambda} \\operatorname{MSE}(\\lambda) = (\\lambda B^2 - \\sigma^2) \\sum_{j=1}^{p} \\frac{2ns_j}{(ns_j + \\lambda)^3}\n$$\n令导数为零：\n$$\n(\\lambda B^2 - \\sigma^2) \\sum_{j=1}^{p} \\frac{2ns_j}{(ns_j + \\lambda)^3} = 0\n$$\n由于 $n>0$, $s_j>0$ 且 $\\lambda>0$，求和项是严格为正的。因此，要使乘积为零，第一个因子必须为零：\n$$\n\\lambda B^2 - \\sigma^2 = 0\n$$\n解出 $\\lambda$ 可得：\n$$\n\\lambda = \\frac{\\sigma^2}{B^2}\n$$\n为确认这是一个最小值，我们可以检验 $\\operatorname{MSE}(\\lambda)$ 在此点的二阶导数。二阶导数是正的，因为在临界点，主导项将是 $ B^2 \\sum_{j=1}^{p} \\frac{2ns_j}{(ns_j + \\lambda)^3} > 0 $。因此，该 $\\lambda$ 值使 MSE 最小化。",
            "answer": "$$\n\\boxed{\\frac{\\sigma^2}{B^2}}\n$$"
        },
        {
            "introduction": "前面的练习侧重于解析解，但在实际研究中，我们通常不知道计算最优权衡所需的真实参数，因此需要一种数据驱动的方法来选择模型复杂度。这个练习将理论付诸实践，你将使用Nadaraya-Watson核回归来估计神经元的调谐曲线，其中核带宽 $h$ 控制着模型的平滑度，进而影响其偏差和方差。通过实现留一法交叉验证（LOOCV）来寻找最优带宽 ，你将获得模型选择的核心技能，并直观地观察到偏差-方差权衡如何体现为一条U形的误差曲线。",
            "id": "4198196",
            "problem": "您的任务是实现一个有原则的估计器选择程序，以量化和说明模拟神经科学调谐数据中的偏差-方差权衡。考虑一个标量刺激变量 $s \\in [0,1]$、一个平滑的确定性调谐函数 $f(s)$，以及通过加性高斯噪声在采样刺激 $s_i$ 处生成的带噪观测值 $y_i$。您将使用带有高斯核的 Nadaraya-Watson 核回归，并执行留一法交叉验证 (LOOCV) 来为一组带宽值 $h$ 估计预测风险，然后选择使 LOOCV 风险最小化的 $h$。\n\n基本原理和定义：\n- 观测数据包含 $n$ 个独立样本 $\\{(s_i, y_i)\\}_{i=1}^n$，其中 $s_i \\sim \\text{Uniform}(0,1)$ 且 $y_i = f(s_i) + \\varepsilon_i$，$\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ 是独立噪声。\n- 对于任何查询点 $s$，使用高斯核和带宽 $h > 0$ 的 Nadaraya-Watson 核回归估计器为\n$$\n\\widehat{f}_h(s) = \\frac{\\sum_{j=1}^n K_h(s - s_j)\\, y_j}{\\sum_{j=1}^n K_h(s - s_j)},\n\\quad\nK_h(u) = \\exp\\!\\left(-\\frac{u^2}{2h^2}\\right).\n$$\n- 对于带宽 $h$，留一法交叉验证 (LOOCV) 预测风险定义为\n$$\nR_{\\text{LOO}}(h) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\widehat{f}^{(-i)}_h(s_i)\\right)^2,\n$$\n其中 $\\widehat{f}^{(-i)}_h(s)$ 是使用除第 $i$ 个样本外的所有样本计算得到的 Nadaraya-Watson 估计器（即从求和中排除 $\\{(s_i,y_i)\\}$）。此定义形式化了当每个点都由其余点预测时的期望平方预测误差。如果在预测 $y_i$ 时，分母 $\\sum_{j\\neq i} K_h(s_i - s_j)$ 在数值上为零（对于极小的 $h$ 可能发生），则将预测定义为留一法均值 $\\widehat{f}^{(-i)}_h(s_i) = \\frac{1}{n-1}\\sum_{j\\neq i} y_j$，以确保风险是良定义的。\n\n您的任务：\n1. 对于下面指定的每个测试用例，使用给定的调谐函数 $f(s)$、样本大小 $n$、噪声标准差 $\\sigma$ 和用于可复现性的伪随机种子，模拟数据集 $\\{(s_i, y_i)\\}_{i=1}^n$。\n2. 对于固定的带宽网格 $h \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20,\\,0.40,\\,0.80\\}$，计算如上定义的 $R_{\\text{LOO}}(h)$。\n3. 选择使 $R_{\\text{LOO}}(h)$ 最小化的带宽 $\\widehat{h}$。如果出现平局（即多个带宽在数值容差内达到相同的最小风险），则选择最小化器中最小的 $h$。\n4. 生成一行输出，其中包含所有测试用例选定的带宽 $\\widehat{h}$，按顺序排列，格式为逗号分隔的列表并用方括号括起来，例如 $[h_1,h_2,h_3]$。\n\n测试套件规范（所有刺激都是无单位的，不使用角度；输出中没有物理单位）：\n- 用例 A：$n = 50$，$\\sigma = 0.15$，种子 $= 101$。调谐函数\n$$\nf(s) = 2 \\;+\\; 4\\,\\exp\\!\\left( -\\frac{(s-0.30)^2}{2\\cdot 0.05^2} \\right) \\;+\\; 3\\,\\exp\\!\\left( -\\frac{(s-0.70)^2}{2\\cdot 0.08^2} \\right).\n$$\n- 用例 B：$n = 80$，$\\sigma = 0.50$，种子 $= 202$。$f(s)$ 与用例 A 相同。\n- 用例 C：$n = 15$，$\\sigma = 0.20$，种子 $= 303$。$f(s)$ 与用例 A 相同。\n- 用例 D：$n = 40$，$\\sigma = 0.00$，种子 $= 404$。常数调谐函数\n$$\nf(s) = 1.5.\n$$\n- 用例 E：$n = 60$，$\\sigma = 0.10$，种子 $= 505$。振荡调谐函数\n$$\nf(s) = \\sin(8\\pi s) \\;+\\; 0.5\\,\\cos(3\\pi s).\n$$\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含结果，格式为逗号分隔的列表并用方括号括起来，具体为按顺序排列的用例 A 到 E 的选定带宽 $\\widehat{h}$，即 $[\\widehat{h}_A,\\widehat{h}_B,\\widehat{h}_C,\\widehat{h}_D,\\widehat{h}_E]$。",
            "solution": "所呈现的问题是一个定义明确且科学严谨的计算统计学练习，具体涉及非参数回归的模型选择。它基于偏差-方差权衡、核密度估计和交叉验证等基本原则，这些是在包括神经科学在内的众多科学领域中进行数据分析的标准和必要工具。该问题提供了所有必要的定义、参数以及清晰、客观的成功标准。它是完整、一致且计算上可行的。因此，该问题被认为是有效的，并提供如下解决方案。\n\n这个问题的核心是为一个 Nadaraya-Watson 回归估计器选择一个最优的模型复杂度参数——高斯核的带宽 $h$。选择必须是有原则的，需要平衡两个相互竞争的目标：良好地拟合训练数据（低偏差）和泛化到新的、未见过的数据（低方差）。这就是经典的偏差-方差权衡。\n\n让我们首先形式化各个组成部分。数据由模型 $y_i = f(s_i) + \\varepsilon_i$ 生成，其中 $f(s)$ 是一个确定性但未知的调谐函数，$s_i$ 是从 $\\text{Uniform}(0,1)$ 分布中采样的刺激，$\\varepsilon_i$ 是来自高斯分布 $\\mathcal{N}(0, \\sigma^2)$ 的独立同分布噪声项。我们的目标是构建一个能够很好地逼近 $f$ 的估计器 $\\widehat{f}$。\n\n在查询点 $s$ 处的 Nadaraya-Watson 估计器是观测响应 $y_j$ 的加权平均值：\n$$\n\\widehat{f}_h(s) = \\frac{\\sum_{j=1}^n K_h(s - s_j)\\, y_j}{\\sum_{j=1}^n K_h(s - s_j)}\n$$\n函数 $K_h(u) = \\exp(-u^2 / (2h^2))$ 是一个带宽为 $h > 0$ 的高斯核。带宽 $h$ 控制核的宽度，从而控制所得估计 $\\widehat{f}_h(s)$ 的平滑度。\n\n$h$ 的选择通过调节偏差-方差权衡，至关重要地决定了估计器的性能：\n- 一个小的 $h$ 值意味着一个窄核。估计值 $\\widehat{f}_h(s)$ 仅受与 $s$ 非常接近的数据点 $(s_j, y_j)$ 的影响。这使得模型能够捕捉 underlying function $f(s)$ 中的细微变化，从而导致低偏差。然而，估计值对这些少数局部点中的噪声 $\\varepsilon_j$ 变得高度敏感，导致高方差。这构成了过拟合。\n- 一个大的 $h$ 值意味着一个宽核。估计值 $\\widehat{f}_h(s)$ 是对广大区域数据点的平均。这平滑了局部噪声，导致低方差。然而，这种过度的平滑可能会抹去真实函数 $f(s)$ 的重要特征，如峰值或振荡，从而导致高偏差。这构成了欠拟合。\n\n为了找到最优平衡，我们需要一个对模型泛化误差（即其在新数据上的期望预测误差）的可靠估计。对训练数据本身天真地计算均方误差是不合适的，因为它总是偏爱偏差最低（$h$ 最小）的模型，从而助长过拟合。留一法交叉验证 (LOOCV) 提供了对该泛化误差的近乎无偏的估计。\n\nLOOCV 风险 $R_{\\text{LOO}}(h)$ 定义为：\n$$\nR_{\\text{LOO}}(h) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\widehat{f}^{(-i)}_h(s_i)\\right)^2\n$$\n此处，$\\widehat{f}^{(-i)}_h(s_i)$ 是对第 $i$ 个观测值 $y_i$ 的预测，它是使用排除了第 $i$ 个点 $(s_i, y_i)$ 的数据集训练的估计器计算得出的。这个过程确保了对于每个点，其预测都是由一个在构建过程中没有见过该特定点的模型做出的，从而模拟了预测新数据的过程。最小化 $R_{\\text{LOO}}(h)$ 的带宽 $h$ 被认为是能提供最佳泛化性能的带宽。\n\n解决该问题的算法如下：\n1.  对于五个测试用例中的每一个，我们首先使用指定的参数生成数据集 $\\{(s_i, y_i)\\}_{i=1}^n$：样本大小 $n$、噪声标准差 $\\sigma$、真实调谐函数 $f(s)$ 以及用于可复现性的伪随机种子。刺激 $s_i$ 从 $\\text{Uniform}(0,1)$ 中抽取，响应计算为 $y_i = f(s_i) + \\mathcal{N}(0, \\sigma^2)$。\n2.  对于每个测试用例的数据集，我们遍历所提供的带宽网格 $h \\in \\{0.02, 0.05, 0.10, 0.20, 0.40, 0.80\\}$。\n3.  对于每个 $h$ 值，我们计算 LOOCV 风险 $R_{\\text{LOO}}(h)$。这涉及一个遍历所有数据点 $i = 1, \\dots, n$ 的内循环。在这个内循环的每次迭代中，我们计算预测值 $\\widehat{f}^{(-i)}_h(s_i)$。\n4.  留一法预测计算如下：\n    $$\n    \\widehat{f}^{(-i)}_h(s_i) = \\frac{\\sum_{j \\neq i} K_h(s_i - s_j)\\, y_j}{\\sum_{j \\neq i} K_h(s_i - s_j)}\n    $$\n    一个关键的边界情况，如问题中所述，是当分母 $\\sum_{j \\neq i} K_h(s_i - s_j)$ 在数值上为零时（如果所有其他点 $s_j$ 都远离 $s_i$，对于非常小的 $h$ 可能会发生这种情况）。在这种情况下，预测默认为被排除的响应的均值：$\\widehat{f}^{(-i)}_h(s_i) = \\frac{1}{n-1}\\sum_{j\\neq i} y_j$。\n5.  在计算完所有候选带宽的 LOOCV 风险后，我们选择最优带宽 $\\widehat{h}$ 作为使 $R_{\\text{LOO}}(h)$ 最小化的那个。如果多个带宽产生相同的最小风险，则根据平局规则选择其中最小的一个。\n6.  收集每个测试用例选定的带宽 $\\widehat{h}$，并按指定格式格式化最终列表。\n\n此过程构成了在非参数设置中进行数据驱动模型选择的严谨且标准的方法论。实现将利用 `numpy` 中的向量化操作来提高效率，特别是在计算核权重和 LOOCV 预测时。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a principled estimator selection procedure to quantify and illustrate\n    the bias-variance tradeoff in simulated neuroscience tuning data.\n    \"\"\"\n\n    # Define the set of candidate bandwidths for the Gaussian kernel.\n    h_grid = [0.02, 0.05, 0.10, 0.20, 0.40, 0.80]\n\n    # Define the tuning functions for the different test cases.\n    def f_case_A(s):\n        \"\"\"Tuning function for Cases A, B, and C.\"\"\"\n        term1 = 4.0 * np.exp(-(s - 0.30)**2 / (2 * 0.05**2))\n        term2 = 3.0 * np.exp(-(s - 0.70)**2 / (2 * 0.08**2))\n        return 2.0 + term1 + term2\n\n    def f_case_D(s):\n        \"\"\"Tuning function for Case D.\"\"\"\n        return np.full_like(s, 1.5)\n\n    def f_case_E(s):\n        \"\"\"Tuning function for Case E.\"\"\"\n        return np.sin(8 * np.pi * s) + 0.5 * np.cos(3 * np.pi * s)\n\n    func_map = {\n        'A': f_case_A,\n        'B': f_case_A,\n        'C': f_case_A,\n        'D': f_case_D,\n        'E': f_case_E,\n    }\n\n    # Specification for the five test cases.\n    test_cases = [\n        {'case_id': 'A', 'n': 50, 'sigma': 0.15, 'seed': 101},\n        {'case_id': 'B', 'n': 80, 'sigma': 0.50, 'seed': 202},\n        {'case_id': 'C', 'n': 15, 'sigma': 0.20, 'seed': 303},\n        {'case_id': 'D', 'n': 40, 'sigma': 0.00, 'seed': 404},\n        {'case_id': 'E', 'n': 60, 'sigma': 0.10, 'seed': 505},\n    ]\n\n    selected_bandwidths = []\n\n    for case in test_cases:\n        n, sigma, seed, case_id = case['n'], case['sigma'], case['seed'], case['case_id']\n        \n        # 1. Simulate the dataset for the current test case.\n        rng = np.random.default_rng(seed)\n        s_samples = rng.uniform(0, 1, n)\n        true_f = func_map[case_id](s_samples)\n        y_samples = true_f + rng.normal(0, sigma, n)\n        \n        loo_risks = []\n\n        # 2. For each bandwidth, compute the LOOCV risk.\n        for h in h_grid:\n            squared_errors = np.zeros(n)\n            \n            # This loop implements the Leave-One-Out procedure.\n            for i in range(n):\n                # Isolate the i-th point and the leave-one-out set.\n                s_i, y_i = s_samples[i], y_samples[i]\n                s_loo = np.delete(s_samples, i)\n                y_loo = np.delete(y_samples, i)\n                \n                # Calculate kernel weights for the prediction at s_i.\n                u = s_i - s_loo\n                weights = np.exp(-u**2 / (2 * h**2))\n                \n                # Calculate the denominator of the Nadaraya-Watson estimator.\n                denominator = np.sum(weights)\n                \n                # Check for the special case of a numerically zero denominator.\n                if np.isclose(denominator, 0):\n                    # Fallback to the mean of the left-out y values.\n                    if n > 1:\n                        y_hat_i = np.mean(y_loo)\n                    else: # A case with n=1 would have no other points.\n                        y_hat_i = 0.0\n                else:\n                    # Standard Nadaraya-Watson prediction.\n                    numerator = np.sum(weights * y_loo)\n                    y_hat_i = numerator / denominator\n                \n                squared_errors[i] = (y_i - y_hat_i)**2\n            \n            # The LOOCV risk is the mean of the squared errors.\n            R_loo = np.mean(squared_errors)\n            loo_risks.append(R_loo)\n        \n        # 3. Select the bandwidth that minimizes the LOOCV risk.\n        # np.argmin() inherently handles the tie-breaking rule by returning\n        # the index of the first minimum, which corresponds to the smallest h.\n        min_risk_idx = np.argmin(loo_risks)\n        best_h = h_grid[min_risk_idx]\n        selected_bandwidths.append(best_h)\n\n    # 4. Produce the final output in the specified format.\n    print(f\"[{','.join(map(str, selected_bandwidths))}]\")\n\n\nsolve()\n```"
        }
    ]
}