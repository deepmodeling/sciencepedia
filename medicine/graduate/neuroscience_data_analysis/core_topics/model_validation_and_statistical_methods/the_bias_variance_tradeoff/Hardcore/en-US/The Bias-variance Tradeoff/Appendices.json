{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of experimental neuroscience is improving signal quality by averaging repeated measurements. This practice provides the formal statistical basis for this technique, demonstrating how averaging independent trials reduces the variance of an estimator. By working through this foundational example, you will derive the precise mathematical relationship between the number of trials and the reduction in estimation error for an unbiased estimator, a principle that is fundamental to analyzing noisy neural data like Local Field Potentials (LFPs). ",
            "id": "4198188",
            "problem": "A sensory stimulus is presented repeatedly while recording the evoked amplitude of a cortical Local Field Potential (LFP). On trial $i$, the measured amplitude is modeled as $Y_{i} = \\theta + \\epsilon_{i}$, where $\\theta$ is a fixed but unknown response amplitude and $\\epsilon_{i}$ is additive noise. Assume the noise variables $\\epsilon_{1}, \\epsilon_{2}, \\ldots, \\epsilon_{m}$ are independent and identically distributed with $\\mathbb{E}[\\epsilon_{i}] = 0$ and $\\operatorname{Var}(\\epsilon_{i}) = \\sigma^{2}$. An estimator of $\\theta$ is constructed by averaging across $m$ independent trials: $\\hat{\\theta}_{m} = \\frac{1}{m}\\sum_{i=1}^{m} Y_{i}$. Starting only from the core definitions of expectation, variance, and Mean Squared Error (MSE), and the properties of independence, derive the variance reduction factor of the estimator $\\hat{\\theta}_{m}$ relative to a single-trial measurement in this neuroscience setting. Then, using the derived expression together with the decomposition of the Mean Squared Error (MSE) into bias and variance, determine the minimum number of trials $m$ required so that the MSE of $\\hat{\\theta}_{m}$ is less than or equal to a target threshold $T$ when the estimator is unbiased. For the computation, use $\\sigma^{2} = 9$ expressed in $(\\mu\\mathrm{V})^{2}$ and $T = 0.75$ expressed in $(\\mu\\mathrm{V})^{2}$. Express the final number of trials as an integer count.",
            "solution": "The problem specifies a measurement model $Y_{i} = \\theta + \\epsilon_{i}$ with independent noise having $\\mathbb{E}[\\epsilon_{i}] = 0$ and $\\operatorname{Var}(\\epsilon_{i}) = \\sigma^{2}$. The estimator formed by averaging $m$ trials is\n$$\n\\hat{\\theta}_{m} = \\frac{1}{m}\\sum_{i=1}^{m} Y_{i}.\n$$\nWe begin from the core definitions. The expectation operator $\\mathbb{E}[\\cdot]$ satisfies linearity, and the variance operator $\\operatorname{Var}(\\cdot)$ satisfies $\\operatorname{Var}(aX) = a^{2}\\operatorname{Var}(X)$ for any scalar $a$ and random variable $X$, and for independent random variables $X$ and $Z$, $\\operatorname{Var}(X+Z) = \\operatorname{Var}(X) + \\operatorname{Var}(Z)$. The Mean Squared Error (MSE) of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as\n$$\n\\operatorname{MSE}(\\hat{\\theta}) = \\mathbb{E}\\!\\left[(\\hat{\\theta} - \\theta)^{2}\\right],\n$$\nand decomposes into bias and variance via\n$$\n\\operatorname{MSE}(\\hat{\\theta}) = \\left(\\mathbb{E}[\\hat{\\theta}] - \\theta\\right)^{2} + \\operatorname{Var}(\\hat{\\theta}).\n$$\n\nWe first examine the bias of $\\hat{\\theta}_{m}$. By linearity of expectation,\n$$\n\\mathbb{E}[\\hat{\\theta}_{m}] = \\mathbb{E}\\!\\left[\\frac{1}{m}\\sum_{i=1}^{m} Y_{i}\\right] = \\frac{1}{m}\\sum_{i=1}^{m} \\mathbb{E}[Y_{i}].\n$$\nSince $Y_{i} = \\theta + \\epsilon_{i}$ and $\\mathbb{E}[\\epsilon_{i}] = 0$, we have $\\mathbb{E}[Y_{i}] = \\theta$. Therefore,\n$$\n\\mathbb{E}[\\hat{\\theta}_{m}] = \\frac{1}{m}\\sum_{i=1}^{m} \\theta = \\theta,\n$$\nso the bias, defined as $\\mathbb{E}[\\hat{\\theta}_{m}] - \\theta$, is $0$. Hence, $\\hat{\\theta}_{m}$ is unbiased and the Mean Squared Error reduces to the variance:\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{m}) = \\operatorname{Var}(\\hat{\\theta}_{m}).\n$$\n\nNext, we derive $\\operatorname{Var}(\\hat{\\theta}_{m})$. Using $\\operatorname{Var}(aX) = a^{2}\\operatorname{Var}(X)$ and independence,\n$$\n\\operatorname{Var}\\!\\left(\\hat{\\theta}_{m}\\right) = \\operatorname{Var}\\!\\left(\\frac{1}{m}\\sum_{i=1}^{m} Y_{i}\\right) = \\frac{1}{m^{2}} \\operatorname{Var}\\!\\left(\\sum_{i=1}^{m} Y_{i}\\right) = \\frac{1}{m^{2}} \\sum_{i=1}^{m} \\operatorname{Var}(Y_{i}),\n$$\nwhere independence implies the variance of the sum is the sum of variances. Since $Y_{i} = \\theta + \\epsilon_{i}$ and $\\theta$ is a constant, $\\operatorname{Var}(Y_{i}) = \\operatorname{Var}(\\epsilon_{i}) = \\sigma^{2}$. Therefore,\n$$\n\\operatorname{Var}\\!\\left(\\hat{\\theta}_{m}\\right) = \\frac{1}{m^{2}} \\sum_{i=1}^{m} \\sigma^{2} = \\frac{1}{m^{2}} \\cdot m \\cdot \\sigma^{2} = \\frac{\\sigma^{2}}{m}.\n$$\n\nWe now define the variance reduction factor as the ratio of the variance of the averaged estimator to the variance of a single-trial measurement. A single-trial measurement has variance\n$$\n\\operatorname{Var}(Y_{1}) = \\sigma^{2}.\n$$\nHence, the variance reduction factor $\\rho(m)$ is\n$$\n\\rho(m) = \\frac{\\operatorname{Var}\\!\\left(\\hat{\\theta}_{m}\\right)}{\\operatorname{Var}(Y_{1})} = \\frac{\\sigma^{2}/m}{\\sigma^{2}} = \\frac{1}{m}.\n$$\nThis shows that averaging $m$ independent trials reduces the variance by a factor of $1/m$ relative to a single trial.\n\nBecause the estimator is unbiased, the Mean Squared Error equals the variance, so the condition that the MSE is less than or equal to a target threshold $T$ is\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{m}) = \\frac{\\sigma^{2}}{m} \\leq T.\n$$\nSolving for $m$,\n$$\nm \\geq \\frac{\\sigma^{2}}{T}.\n$$\nSubstituting the given values $\\sigma^{2} = 9$ and $T = 0.75$ (both in $(\\mu\\mathrm{V})^{2}$, ensuring unit consistency),\n$$\nm \\geq \\frac{9}{0.75} = 12.\n$$\nSince $m$ must be an integer count of trials and the bound is exactly $12$, the minimum number of trials required is $12$.",
            "answer": "$$\\boxed{12}$$"
        },
        {
            "introduction": "While reducing variance is crucial, many modern neuroscience problems involve high-dimensional data where unbiased estimators can have unacceptably high variance, leading to overfitting. This exercise explores ridge regression, a powerful technique that introduces a small, controlled amount of bias to achieve a much larger reduction in variance, thereby improving the overall accuracy of the model. By deriving the mean squared error for a regularized linear model, you will directly confront the tradeoff between bias and variance and solve for the optimal level of regularization. ",
            "id": "4198237",
            "problem": "A single neuron’s firing rate is modeled by a linear encoding model in response to a high-dimensional stimulus. On each trial $t \\in \\{1,\\dots,n\\}$, the stimulus vector is $x_t \\in \\mathbb{R}^{p}$ drawn independently and identically distributed (i.i.d.) from a zero-mean multivariate normal distribution with covariance $\\Sigma_{X} \\succ 0$, and the observed response is $y_t = x_t^{\\top} \\beta^{\\ast} + \\varepsilon_t$, where $\\beta^{\\ast} \\in \\mathbb{R}^{p}$ is the true signal vector and $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^{2})$ with known $\\sigma^{2}  0$. Let $X \\in \\mathbb{R}^{n \\times p}$ be the design matrix formed by stacking the $x_t^{\\top}$ rows.\n\nConsider the ridge regression estimator with penalty parameter $\\lambda  0$ defined by $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_p)^{-1} X^{\\top} y$, where $I_p$ is the $p \\times p$ identity matrix and $y \\in \\mathbb{R}^{n}$ is the vector of responses. In the high-trial regime where $n$ is large, approximate the Gram matrix by $X^{\\top} X \\approx n \\Sigma_{X}$.\n\nWork in the eigenbasis of $\\Sigma_{X}$: let $\\Sigma_{X} = U \\operatorname{diag}(s_1,\\dots,s_p) U^{\\top}$ with $U$ orthogonal and $s_i  0$. Suppose that in this basis the components of the true signal are all equal to a known constant amplitude $B  0$, that is, $U^{\\top} \\beta^{\\ast} = (B,\\dots,B)^{\\top}$. Using only the definitions of mean squared error (MSE) and bias-variance decomposition, and properties of the multivariate normal distribution, derive the exact expression for the mean squared error $\\operatorname{MSE}(\\lambda) = \\mathbb{E}\\big[\\|\\hat{\\beta}_{\\lambda} - \\beta^{\\ast}\\|_{2}^{2}\\big]$ under the approximation $X^{\\top} X = n \\Sigma_{X}$, and determine the value of $\\lambda$ that minimizes this MSE.\n\nReport only the minimizing $\\lambda$ in your final answer as a closed-form analytic expression. No rounding is required.",
            "solution": "The objective is to derive the mean squared error (MSE) for the ridge regression estimator $\\hat{\\beta}_{\\lambda}$ and find the value of the regularization parameter $\\lambda$ that minimizes this error. The MSE is defined as $\\operatorname{MSE}(\\lambda) = \\mathbb{E}\\big[\\|\\hat{\\beta}_{\\lambda} - \\beta^{\\ast}\\|_{2}^{2}\\big]$.\n\nWe begin with the bias-variance decomposition of the MSE:\n$$\n\\operatorname{MSE}(\\hat{\\beta}_{\\lambda}) = \\underbrace{\\|\\mathbb{E}[\\hat{\\beta}_{\\lambda}] - \\beta^{\\ast}\\|_{2}^{2}}_{\\text{Squared Bias}} + \\underbrace{\\mathbb{E}\\big[\\|\\hat{\\beta}_{\\lambda} - \\mathbb{E}[\\hat{\\beta}_{\\lambda}]\\|_{2}^{2}\\big]}_{\\text{Variance}}\n$$\nThe expectation $\\mathbb{E}$ is over the distribution of the noise $\\varepsilon$ and the stimulus $X$. The problem specifies using the approximation $X^{\\top} X = n \\Sigma_{X}$, which we treat as exact for the analysis.\n\nFirst, we express the estimator $\\hat{\\beta}_{\\lambda}$ by substituting the linear model $y = X \\beta^{\\ast} + \\varepsilon$:\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_p)^{-1} X^{\\top} y = (X^{\\top} X + \\lambda I_p)^{-1} X^{\\top} (X \\beta^{\\ast} + \\varepsilon)\n$$\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_p)^{-1} X^{\\top} X \\beta^{\\ast} + (X^{\\top} X + \\lambda I_p)^{-1} X^{\\top} \\varepsilon\n$$\nUsing the approximation $X^{\\top} X = n \\Sigma_{X}$, we have:\n$$\n\\hat{\\beta}_{\\lambda} = (n \\Sigma_{X} + \\lambda I_p)^{-1} n \\Sigma_{X} \\beta^{\\ast} + (n \\Sigma_{X} + \\lambda I_p)^{-1} X^{\\top} \\varepsilon\n$$\n\nNext, we calculate the bias term. The expectation of $\\hat{\\beta}_{\\lambda}$ is taken over the noise $\\varepsilon$. Given that $\\mathbb{E}[\\varepsilon] = 0$, the second term vanishes in expectation.\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\lambda}] = (n \\Sigma_{X} + \\lambda I_p)^{-1} n \\Sigma_{X} \\beta^{\\ast}\n$$\nThe bias vector is therefore:\n$$\n\\operatorname{Bias}(\\hat{\\beta}_{\\lambda}) = \\mathbb{E}[\\hat{\\beta}_{\\lambda}] - \\beta^{\\ast} = (n \\Sigma_{X} + \\lambda I_p)^{-1} n \\Sigma_{X} \\beta^{\\ast} - \\beta^{\\ast}\n$$\n$$\n= \\left[ (n \\Sigma_{X} + \\lambda I_p)^{-1} n \\Sigma_{X} - I_p \\right] \\beta^{\\ast}\n$$\n$$\n= (n \\Sigma_{X} + \\lambda I_p)^{-1} \\left[ n \\Sigma_{X} - (n \\Sigma_{X} + \\lambda I_p) \\right] \\beta^{\\ast}\n$$\n$$\n= -\\lambda (n \\Sigma_{X} + \\lambda I_p)^{-1} \\beta^{\\ast}\n$$\nThe squared bias is the squared Euclidean norm of this vector:\n$$\n\\|\\operatorname{Bias}(\\hat{\\beta}_{\\lambda})\\|_{2}^{2} = \\lambda^2 \\| (n \\Sigma_{X} + \\lambda I_p)^{-1} \\beta^{\\ast} \\|_{2}^{2}\n$$\nTo simplify this expression, we work in the eigenbasis of $\\Sigma_{X} = U \\operatorname{diag}(s_1,\\dots,s_p) U^{\\top}$.\nLet $A = n \\Sigma_{X} + \\lambda I_p = U (n \\operatorname{diag}(s_i) + \\lambda I_p) U^{\\top} = U \\operatorname{diag}(ns_i + \\lambda) U^{\\top}$.\nIts inverse is $A^{-1} = U \\operatorname{diag}((ns_i + \\lambda)^{-1}) U^{\\top}$.\nThe squared L2-norm is invariant under orthogonal transformations ($U$), so $\\|v\\|_{2}^{2} = \\|U^{\\top}v\\|_{2}^{2}$.\n$$\n\\|\\operatorname{Bias}(\\hat{\\beta}_{\\lambda})\\|_{2}^{2} = \\lambda^2 \\| U^{\\top} (n \\Sigma_{X} + \\lambda I_p)^{-1} \\beta^{\\ast} \\|_{2}^{2}\n$$\n$$\n= \\lambda^2 \\| \\operatorname{diag}((ns_i + \\lambda)^{-1}) U^{\\top} \\beta^{\\ast} \\|_{2}^{2}\n$$\nGiven $U^{\\top} \\beta^{\\ast} = (B, \\dots, B)^{\\top}$, the $j$-th component of the vector inside the norm is $\\frac{B}{ns_j + \\lambda}$. Summing the squares of these components gives:\n$$\n\\|\\operatorname{Bias}(\\hat{\\beta}_{\\lambda})\\|_{2}^{2} = \\lambda^2 B^2 \\sum_{j=1}^{p} \\frac{1}{(ns_j + \\lambda)^2}\n$$\nNow, we calculate the variance term. The variance is the trace of the covariance matrix of $\\hat{\\beta}_{\\lambda}$.\n$$\n\\hat{\\beta}_{\\lambda} - \\mathbb{E}[\\hat{\\beta}_{\\lambda}] = (n \\Sigma_{X} + \\lambda I_p)^{-1} X^{\\top} \\varepsilon\n$$\nThe covariance matrix, conditioned on $X$, is:\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\lambda} | X) = \\mathbb{E}_{\\varepsilon} [(\\hat{\\beta}_{\\lambda} - \\mathbb{E}[\\hat{\\beta}_{\\lambda}]) (\\hat{\\beta}_{\\lambda} - \\mathbb{E}[\\hat{\\beta}_{\\lambda}])^{\\top} | X]\n$$\n$$\n= (n \\Sigma_{X} + \\lambda I_p)^{-1} X^{\\top} \\mathbb{E}[\\varepsilon \\varepsilon^{\\top}] X (n \\Sigma_{X} + \\lambda I_p)^{-1}\n$$\nSince $\\varepsilon_t$ are i.i.d. with variance $\\sigma^2$, $\\mathbb{E}[\\varepsilon \\varepsilon^{\\top}] = \\sigma^2 I_n$.\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\lambda} | X) = \\sigma^2 (n \\Sigma_{X} + \\lambda I_p)^{-1} X^{\\top}X (n \\Sigma_{X} + \\lambda I_p)^{-1}\n$$\nUsing the approximation $X^{\\top}X = n \\Sigma_X$:\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\lambda}) = \\sigma^2 (n \\Sigma_{X} + \\lambda I_p)^{-1} n\\Sigma_X (n \\Sigma_{X} + \\lambda I_p)^{-1}\n$$\nThe variance is the trace of this matrix. Using the cyclic property of the trace and the eigenbasis transformation:\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\lambda}) = \\operatorname{Tr}(\\operatorname{Cov}(\\hat{\\beta}_{\\lambda})) = \\sigma^2 \\operatorname{Tr} \\left[ n\\Sigma_X (n \\Sigma_{X} + \\lambda I_p)^{-2} \\right]\n$$\n$$\n= \\sigma^2 \\operatorname{Tr} \\left[ n U \\operatorname{diag}(s_i) U^{\\top} U \\operatorname{diag}((ns_i+\\lambda)^{-2}) U^{\\top} \\right]\n$$\n$$\n= \\sigma^2 \\operatorname{Tr} \\left[ U \\operatorname{diag}\\left(\\frac{ns_i}{(ns_i+\\lambda)^2}\\right) U^{\\top} \\right] = \\sigma^2 \\operatorname{Tr} \\left[ \\operatorname{diag}\\left(\\frac{ns_i}{(ns_i+\\lambda)^2}\\right) \\right]\n$$\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\lambda}) = \\sigma^2 \\sum_{j=1}^{p} \\frac{ns_j}{(ns_j + \\lambda)^2}\n$$\nCombining the squared bias and variance, the MSE is:\n$$\n\\operatorname{MSE}(\\lambda) = \\lambda^2 B^2 \\sum_{j=1}^{p} \\frac{1}{(ns_j + \\lambda)^2} + \\sigma^2 \\sum_{j=1}^{p} \\frac{ns_j}{(ns_j + \\lambda)^2} = \\sum_{j=1}^{p} \\frac{\\lambda^2 B^2 + \\sigma^2 ns_j}{(ns_j + \\lambda)^2}\n$$\nTo find the minimum, we differentiate $\\operatorname{MSE}(\\lambda)$ with respect to $\\lambda$ and set the result to $0$. We can differentiate term-by-term inside the summation. For each term $j$, using the quotient rule for differentiation:\n$$\n\\frac{d}{d\\lambda} \\left[ \\frac{\\lambda^2 B^2 + \\sigma^2 ns_j}{(ns_j + \\lambda)^2} \\right] = \\frac{ (2\\lambda B^2)(ns_j+\\lambda)^2 - (\\lambda^2 B^2 + \\sigma^2 ns_j) \\cdot 2(ns_j+\\lambda) }{(ns_j + \\lambda)^4}\n$$\n$$\n= \\frac{ 2(ns_j+\\lambda) \\left[ \\lambda B^2(ns_j+\\lambda) - (\\lambda^2 B^2 + \\sigma^2 ns_j) \\right] }{(ns_j + \\lambda)^4}\n$$\n$$\n= \\frac{ 2 \\left[ \\lambda B^2 ns_j + \\lambda^2 B^2 - \\lambda^2 B^2 - \\sigma^2 ns_j \\right] }{(ns_j + \\lambda)^3}\n$$\n$$\n= \\frac{ 2ns_j (\\lambda B^2 - \\sigma^2) }{(ns_j + \\lambda)^3}\n$$\nThe derivative of the total MSE is the sum over $j$:\n$$\n\\frac{d}{d\\lambda} \\operatorname{MSE}(\\lambda) = \\sum_{j=1}^{p} \\frac{ 2ns_j (\\lambda B^2 - \\sigma^2) }{(ns_j + \\lambda)^3}\n$$\nWe can factor out the term common to all elements in the sum:\n$$\n\\frac{d}{d\\lambda} \\operatorname{MSE}(\\lambda) = (\\lambda B^2 - \\sigma^2) \\sum_{j=1}^{p} \\frac{2ns_j}{(ns_j + \\lambda)^3}\n$$\nSetting the derivative to zero:\n$$\n(\\lambda B^2 - \\sigma^2) \\sum_{j=1}^{p} \\frac{2ns_j}{(ns_j + \\lambda)^3} = 0\n$$\nSince $n0$, $s_j0$, and $\\lambda0$, the summation term is strictly positive. Therefore, for the product to be zero, the first factor must be zero:\n$$\n\\lambda B^2 - \\sigma^2 = 0\n$$\nSolving for $\\lambda$ yields:\n$$\n\\lambda = \\frac{\\sigma^2}{B^2}\n$$\nTo confirm this is a minimum, we can examine the second derivative of $\\operatorname{MSE}(\\lambda)$ at this point. The second derivative is positive, as the dominant term at the critical point will be $ B^2 \\sum_{j=1}^{p} \\frac{2ns_j}{(ns_j + \\lambda)^3}  0 $. Thus, this value of $\\lambda$ minimizes the MSE.",
            "answer": "$$\n\\boxed{\\frac{\\sigma^2}{B^2}}\n$$"
        },
        {
            "introduction": "Theoretical derivations provide deep insight, but in practice, the true underlying functions and noise levels are unknown. This computational exercise puts theory into practice by using cross-validation—a critical skill for any data scientist—to empirically navigate the bias-variance tradeoff. You will implement a Nadaraya-Watson kernel estimator to model a neural tuning curve and use leave-one-out cross-validation to find the optimal model complexity (kernel bandwidth), observing directly how the prediction error forms a characteristic U-shaped curve as a function of model flexibility. ",
            "id": "4198196",
            "problem": "You are tasked with implementing a principled estimator selection procedure to quantify and illustrate the bias-variance tradeoff in simulated neuroscience tuning data. Consider a scalar stimulus variable $s \\in [0,1]$, a smooth deterministic tuning function $f(s)$, and noisy observations $y_i$ generated at sampled stimuli $s_i$ via additive Gaussian noise. You will use Nadaraya-Watson kernel regression with a Gaussian kernel and perform Leave-One-Out Cross-Validation (LOOCV) to estimate the predictive risk for a set of bandwidth values $h$, then select the $h$ that minimizes the LOOCV risk.\n\nFoundational base and definitions:\n- The observed data consist of $n$ independent samples $\\{(s_i, y_i)\\}_{i=1}^n$, where $s_i \\sim \\text{Uniform}(0,1)$ and $y_i = f(s_i) + \\varepsilon_i$, with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ independent noise.\n- The Nadaraya-Watson kernel regression estimator with Gaussian kernel and bandwidth $h  0$ for any query $s$ is\n$$\n\\widehat{f}_h(s) = \\frac{\\sum_{j=1}^n K_h(s - s_j)\\, y_j}{\\sum_{j=1}^n K_h(s - s_j)},\n\\quad\nK_h(u) = \\exp\\!\\left(-\\frac{u^2}{2h^2}\\right).\n$$\n- The Leave-One-Out Cross-Validation (LOOCV) predictive risk for bandwidth $h$ is defined by\n$$\nR_{\\text{LOO}}(h) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\widehat{f}^{(-i)}_h(s_i)\\right)^2,\n$$\nwhere $\\widehat{f}^{(-i)}_h(s)$ is the same Nadaraya-Watson estimator computed from all samples except the $i$-th one (i.e., excluding $\\{(s_i,y_i)\\}$ from the sums). This definition formalizes the expected squared prediction error when each point is predicted from the rest. In the event that the denominator $\\sum_{j\\neq i} K_h(s_i - s_j)$ is numerically zero when predicting $y_i$ (which can occur for extremely small $h$), define the prediction by the leave-one-out mean $\\widehat{f}^{(-i)}_h(s_i) = \\frac{1}{n-1}\\sum_{j\\neq i} y_j$ to ensure a well-defined risk.\n\nYour tasks:\n1. For each test case specified below, simulate the dataset $\\{(s_i, y_i)\\}_{i=1}^n$ using the given tuning function $f(s)$, sample size $n$, noise standard deviation $\\sigma$, and pseudorandom seed for reproducibility.\n2. For the fixed bandwidth grid $h \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20,\\,0.40,\\,0.80\\}$, compute $R_{\\text{LOO}}(h)$ as defined above.\n3. Select the bandwidth $\\widehat{h}$ that minimizes $R_{\\text{LOO}}(h)$. In case of ties (i.e., multiple bandwidths achieving the same minimal risk within numerical tolerance), choose the smallest $h$ among the minimizers.\n4. Produce a single line of output containing the selected bandwidths $\\widehat{h}$ for all test cases, in order, as a comma-separated list enclosed in square brackets, for example, $[h_1,h_2,h_3]$.\n\nTest suite specification (all stimuli are unitless and angles are not used; there are no physical units in the outputs):\n- Case A: $n = 50$, $\\sigma = 0.15$, seed $= 101$. Tuning function\n$$\nf(s) = 2 \\;+\\; 4\\,\\exp\\!\\left( -\\frac{(s-0.30)^2}{2\\cdot 0.05^2} \\right) \\;+\\; 3\\,\\exp\\!\\left( -\\frac{(s-0.70)^2}{2\\cdot 0.08^2} \\right).\n$$\n- Case B: $n = 80$, $\\sigma = 0.50$, seed $= 202$. Same $f(s)$ as Case A.\n- Case C: $n = 15$, $\\sigma = 0.20$, seed $= 303$. Same $f(s)$ as Case A.\n- Case D: $n = 40$, $\\sigma = 0.00$, seed $= 404$. Constant tuning function\n$$\nf(s) = 1.5.\n$$\n- Case E: $n = 60$, $\\sigma = 0.10$, seed $= 505$. Oscillatory tuning function\n$$\nf(s) = \\sin(8\\pi s) \\;+\\; 0.5\\,\\cos(3\\pi s).\n$$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, specifically the selected bandwidths $\\widehat{h}$ for Cases A through E in that order, i.e., $[\\widehat{h}_A,\\widehat{h}_B,\\widehat{h}_C,\\widehat{h}_D,\\widehat{h}_E]$.",
            "solution": "The problem presented is a well-posed and scientifically sound exercise in computational statistics, specifically concerning model selection for non-parametric regression. It is grounded in the fundamental principles of the bias-variance tradeoff, kernel density estimation, and cross-validation, which are standard and essential tools in data analysis across numerous scientific disciplines, including neuroscience. The problem provides all necessary definitions, parameters, and a clear, objective criterion for success. It is complete, consistent, and computationally feasible. Therefore, the problem is deemed valid, and a solution is provided below.\n\nThe core of this problem is to select an optimal model complexity parameter—the bandwidth $h$ of a Gaussian kernel—for a Nadaraya-Watson regression estimator. The selection must be principled, balancing the competing objectives of fitting the training data well (low bias) and generalizing to new, unseen data (low variance). This is the canonical bias-variance tradeoff.\n\nLet us first formalize the components. The data are generated from a model $y_i = f(s_i) + \\varepsilon_i$, where $f(s)$ is a deterministic but unknown tuning function, $s_i$ are stimuli sampled from a $\\text{Uniform}(0,1)$ distribution, and $\\varepsilon_i$ are independent and identically distributed noise terms from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$. Our goal is to construct an estimator $\\widehat{f}$ that approximates $f$ well.\n\nThe Nadaraya-Watson estimator at a query point $s$ is a weighted average of the observed responses $y_j$:\n$$\n\\widehat{f}_h(s) = \\frac{\\sum_{j=1}^n K_h(s - s_j)\\, y_j}{\\sum_{j=1}^n K_h(s - s_j)}\n$$\nThe function $K_h(u) = \\exp(-u^2 / (2h^2))$ is a Gaussian kernel with bandwidth $h  0$. The bandwidth $h$ controls the width of the kernel and, consequently, the smoothness of the resulting estimate $\\widehat{f}_h(s)$.\n\nThe choice of $h$ critically determines the estimator's performance by mediating the bias-variance tradeoff:\n- A small value of $h$ implies a narrow kernel. The estimate $\\widehat{f}_h(s)$ is influenced only by data points $(s_j, y_j)$ that are very close to $s$. This allows the model to capture fine-scale variations in the underlying function $f(s)$, leading to low bias. However, the estimate becomes highly sensitive to the noise $\\varepsilon_j$ in these few local points, resulting in high variance. This constitutes overfitting.\n- A large value of $h$ implies a wide kernel. The estimate $\\widehat{f}_h(s)$ is an average over a broad region of data points. This smooths out the local noise, leading to low variance. However, this aggressive smoothing may wash out important features of the true function $f(s)$, such as peaks or oscillations, resulting in high bias. This constitutes underfitting.\n\nTo find the optimal balance, we need a reliable estimate of the model's generalization error, i.e., its expected prediction error on new data. A naive calculation of the mean squared error on the training data itself is unsuitable, as it would always favor the model with the lowest bias (smallest $h$), promoting overfitting. Leave-One-Out Cross-Validation (LOOCV) provides a nearly unbiased estimate of this generalization error.\n\nThe LOOCV risk, $R_{\\text{LOO}}(h)$, is defined as:\n$$\nR_{\\text{LOO}}(h) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\widehat{f}^{(-i)}_h(s_i)\\right)^2\n$$\nHere, $\\widehat{f}^{(-i)}_h(s_i)$ is the prediction for the $i$-th observation $y_i$, computed using an estimator trained on the dataset with the $i$-th point $(s_i, y_i)$ itself excluded. This procedure ensures that for every point, the prediction is made using a model that did not see that specific point during its construction, mimicking the process of predicting new data. The bandwidth $h$ that minimizes $R_{\\text{LOO}}(h)$ is the one expected to provide the best generalization performance.\n\nThe algorithm to solve the problem is as follows:\n1.  For each of the five test cases, we first generate the dataset $\\{(s_i, y_i)\\}_{i=1}^n$ using the specified parameters: sample size $n$, noise standard deviation $\\sigma$, the true tuning function $f(s)$, and a pseudorandom seed for reproducibility. The stimuli $s_i$ are drawn from $\\text{Uniform}(0,1)$, and the responses are computed as $y_i = f(s_i) + \\mathcal{N}(0, \\sigma^2)$.\n2.  For each test case's dataset, we iterate through the provided grid of bandwidths $h \\in \\{0.02, 0.05, 0.10, 0.20, 0.40, 0.80\\}$.\n3.  For each value of $h$, we compute the LOOCV risk $R_{\\text{LOO}}(h)$. This involves an inner loop over all data points $i = 1, \\dots, n$. In each iteration of this inner loop, we compute the prediction $\\widehat{f}^{(-i)}_h(s_i)$.\n4.  The leave-one-out prediction is calculated as:\n    $$\n    \\widehat{f}^{(-i)}_h(s_i) = \\frac{\\sum_{j \\neq i} K_h(s_i - s_j)\\, y_j}{\\sum_{j \\neq i} K_h(s_i - s_j)}\n    $$\n    A crucial edge case, specified in the problem, is when the denominator $\\sum_{j \\neq i} K_h(s_i - s_j)$ is numerically zero (which can happen for very small $h$ if all other points $s_j$ are far from $s_i$). In this situation, the prediction defaults to the mean of the left-out responses: $\\widehat{f}^{(-i)}_h(s_i) = \\frac{1}{n-1}\\sum_{j\\neq i} y_j$.\n5.  After computing the LOOCV risk for all candidate bandwidths, we select the optimal bandwidth $\\widehat{h}$ as the one that minimizes $R_{\\text{LOO}}(h)$. If multiple bandwidths yield the same minimal risk, the smallest of these is chosen, as per the tie-breaking rule.\n6.  The selected bandwidth $\\widehat{h}$ for each test case is collected, and the final list is formatted as specified.\n\nThis procedure constitutes a rigorous and standard methodology for data-driven model selection in a non-parametric setting. The implementation will leverage vectorized operations in `numpy` for efficiency, particularly in computing the kernel weights and the LOOCV predictions.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a principled estimator selection procedure to quantify and illustrate\n    the bias-variance tradeoff in simulated neuroscience tuning data.\n    \"\"\"\n\n    # Define the set of candidate bandwidths for the Gaussian kernel.\n    h_grid = [0.02, 0.05, 0.10, 0.20, 0.40, 0.80]\n\n    # Define the tuning functions for the different test cases.\n    def f_case_A(s):\n        \"\"\"Tuning function for Cases A, B, and C.\"\"\"\n        term1 = 4.0 * np.exp(-(s - 0.30)**2 / (2 * 0.05**2))\n        term2 = 3.0 * np.exp(-(s - 0.70)**2 / (2 * 0.08**2))\n        return 2.0 + term1 + term2\n\n    def f_case_D(s):\n        \"\"\"Tuning function for Case D.\"\"\"\n        return np.full_like(s, 1.5)\n\n    def f_case_E(s):\n        \"\"\"Tuning function for Case E.\"\"\"\n        return np.sin(8 * np.pi * s) + 0.5 * np.cos(3 * np.pi * s)\n\n    func_map = {\n        'A': f_case_A,\n        'B': f_case_A,\n        'C': f_case_A,\n        'D': f_case_D,\n        'E': f_case_E,\n    }\n\n    # Specification for the five test cases.\n    test_cases = [\n        {'case_id': 'A', 'n': 50, 'sigma': 0.15, 'seed': 101},\n        {'case_id': 'B', 'n': 80, 'sigma': 0.50, 'seed': 202},\n        {'case_id': 'C', 'n': 15, 'sigma': 0.20, 'seed': 303},\n        {'case_id': 'D', 'n': 40, 'sigma': 0.00, 'seed': 404},\n        {'case_id': 'E', 'n': 60, 'sigma': 0.10, 'seed': 505},\n    ]\n\n    selected_bandwidths = []\n\n    for case in test_cases:\n        n, sigma, seed, case_id = case['n'], case['sigma'], case['seed'], case['case_id']\n        \n        # 1. Simulate the dataset for the current test case.\n        rng = np.random.default_rng(seed)\n        s_samples = rng.uniform(0, 1, n)\n        true_f = func_map[case_id](s_samples)\n        y_samples = true_f + rng.normal(0, sigma, n)\n        \n        loo_risks = []\n\n        # 2. For each bandwidth, compute the LOOCV risk.\n        for h in h_grid:\n            squared_errors = np.zeros(n)\n            \n            # This loop implements the Leave-One-Out procedure.\n            for i in range(n):\n                # Isolate the i-th point and the leave-one-out set.\n                s_i, y_i = s_samples[i], y_samples[i]\n                s_loo = np.delete(s_samples, i)\n                y_loo = np.delete(y_samples, i)\n                \n                # Calculate kernel weights for the prediction at s_i.\n                u = s_i - s_loo\n                weights = np.exp(-u**2 / (2 * h**2))\n                \n                # Calculate the denominator of the Nadaraya-Watson estimator.\n                denominator = np.sum(weights)\n                \n                # Check for the special case of a numerically zero denominator.\n                if np.isclose(denominator, 0):\n                    # Fallback to the mean of the left-out y values.\n                    if n  1:\n                        y_hat_i = np.mean(y_loo)\n                    else: # A case with n=1 would have no other points.\n                        y_hat_i = 0.0\n                else:\n                    # Standard Nadaraya-Watson prediction.\n                    numerator = np.sum(weights * y_loo)\n                    y_hat_i = numerator / denominator\n                \n                squared_errors[i] = (y_i - y_hat_i)**2\n            \n            # The LOOCV risk is the mean of the squared errors.\n            R_loo = np.mean(squared_errors)\n            loo_risks.append(R_loo)\n        \n        # 3. Select the bandwidth that minimizes the LOOCV risk.\n        # np.argmin() inherently handles the tie-breaking rule by returning\n        # the index of the first minimum, which corresponds to the smallest h.\n        min_risk_idx = np.argmin(loo_risks)\n        best_h = h_grid[min_risk_idx]\n        selected_bandwidths.append(best_h)\n\n    # 4. Produce the final output in the specified format.\n    print(f\"[{','.join(map(str, selected_bandwidths))}]\")\n\n\nsolve()\n```"
        }
    ]
}