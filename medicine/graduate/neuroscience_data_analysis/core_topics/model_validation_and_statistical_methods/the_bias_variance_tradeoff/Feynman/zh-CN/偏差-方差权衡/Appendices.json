{
    "hands_on_practices": [
        {
            "introduction": "在神经科学数据分析中，一个基本技术是通过对多次试验进行平均来增强信噪比，例如在事件相关电位（ERP）或局部场电位（LFP）的分析中。这个练习  将这一实践形式化，要求你从期望和方差的基本定义出发，推导平均如何降低估计量的方差。通过这个实践，你将从根本上理解控制模型方差的第一个也是最直接的策略。",
            "id": "4198188",
            "problem": "在重复呈现感觉刺激的同时，记录诱发的皮层局部场电位 (LFP) 的振幅。在第 $i$ 次试验中，测得的振幅被建模为 $Y_{i} = \\theta + \\epsilon_{i}$，其中 $\\theta$ 是一个固定但未知的响应振幅，而 $\\epsilon_{i}$ 是加性噪声。假设噪声变量 $\\epsilon_{1}, \\epsilon_{2}, \\ldots, \\epsilon_{m}$ 是独立同分布的，其期望 $\\mathbb{E}[\\epsilon_{i}] = 0$ 且方差 $\\operatorname{Var}(\\epsilon_{i}) = \\sigma^{2}$。通过对 $m$ 次独立试验进行平均来构造 $\\theta$ 的一个估计量：$\\hat{\\theta}_{m} = \\frac{1}{m}\\sum_{i=1}^{m} Y_{i}$。仅从期望、方差和均方误差 (MSE) 的核心定义以及独立性的性质出发，推导在该神经科学背景下，估计量 $\\hat{\\theta}_{m}$ 相对于单次试验测量的方差缩减因子。然后，利用推导出的表达式以及均方误差 (MSE) 到偏差和方差的分解，确定当估计量是无偏的时，为使 $\\hat{\\theta}_{m}$ 的 MSE 小于或等于目标阈值 $T$ 所需的最小试验次数 $m$。计算时，使用以 $(\\mu\\mathrm{V})^2$ 为单位的 $\\sigma^{2} = 9$ 和以 $(\\mu\\mathrm{V})^2$ 为单位的 $T = 0.75$。将最终的试验次数表示为一个整数。",
            "solution": "问题指定了一个测量模型 $Y_{i} = \\theta + \\epsilon_{i}$，其中噪声是独立的，且满足 $\\mathbb{E}[\\epsilon_{i}] = 0$ 和 $\\operatorname{Var}(\\epsilon_{i}) = \\sigma^{2}$。通过对 $m$ 次试验求平均得到的估计量是\n$$\n\\hat{\\theta}_{m} = \\frac{1}{m}\\sum_{i=1}^{m} Y_{i}.\n$$\n我们从核心定义开始。期望算子 $\\mathbb{E}[\\cdot]$ 满足线性性质，方差算子 $\\operatorname{Var}(\\cdot)$ 对于任意标量 $a$ 和随机变量 $X$ 满足 $\\operatorname{Var}(aX) = a^{2}\\operatorname{Var}(X)$，对于独立的随机变量 $X$ 和 $Z$，满足 $\\operatorname{Var}(X+Z) = \\operatorname{Var}(X) + \\operatorname{Var}(Z)$。参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的均方误差 (MSE) 定义为\n$$\n\\operatorname{MSE}(\\hat{\\theta}) = \\mathbb{E}\\!\\left[(\\hat{\\theta} - \\theta)^{2}\\right],\n$$\n并且可以通过以下方式分解为偏差和方差\n$$\n\\operatorname{MSE}(\\hat{\\theta}) = \\left(\\mathbb{E}[\\hat{\\theta}] - \\theta\\right)^{2} + \\operatorname{Var}(\\hat{\\theta}).\n$$\n\n我们首先检验 $\\hat{\\theta}_{m}$ 的偏差。根据期望的线性性质，\n$$\n\\mathbb{E}[\\hat{\\theta}_{m}] = \\mathbb{E}\\!\\left[\\frac{1}{m}\\sum_{i=1}^{m} Y_{i}\\right] = \\frac{1}{m}\\sum_{i=1}^{m} \\mathbb{E}[Y_{i}].\n$$\n因为 $Y_{i} = \\theta + \\epsilon_{i}$ 且 $\\mathbb{E}[\\epsilon_{i}] = 0$，我们有 $\\mathbb{E}[Y_{i}] = \\theta$。因此，\n$$\n\\mathbb{E}[\\hat{\\theta}_{m}] = \\frac{1}{m}\\sum_{i=1}^{m} \\theta = \\theta,\n$$\n所以偏差，定义为 $\\mathbb{E}[\\hat{\\theta}_{m}] - \\theta$，为 $0$。因此，$\\hat{\\theta}_{m}$ 是无偏的，均方误差简化为方差：\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{m}) = \\operatorname{Var}(\\hat{\\theta}_{m}).\n$$\n\n接下来，我们推导 $\\operatorname{Var}(\\hat{\\theta}_{m})$。利用 $\\operatorname{Var}(aX) = a^{2}\\operatorname{Var}(X)$ 和独立性，\n$$\n\\operatorname{Var}\\!\\left(\\hat{\\theta}_{m}\\right) = \\operatorname{Var}\\!\\left(\\frac{1}{m}\\sum_{i=1}^{m} Y_{i}\\right) = \\frac{1}{m^{2}} \\operatorname{Var}\\!\\left(\\sum_{i=1}^{m} Y_{i}\\right) = \\frac{1}{m^{2}} \\sum_{i=1}^{m} \\operatorname{Var}(Y_{i}),\n$$\n其中独立性意味着和的方差等于方差的和。因为 $Y_{i} = \\theta + \\epsilon_{i}$ 且 $\\theta$ 是一个常数，所以 $\\operatorname{Var}(Y_{i}) = \\operatorname{Var}(\\epsilon_{i}) = \\sigma^{2}$。因此，\n$$\n\\operatorname{Var}\\!\\left(\\hat{\\theta}_{m}\\right) = \\frac{1}{m^{2}} \\sum_{i=1}^{m} \\sigma^{2} = \\frac{1}{m^{2}} \\cdot m \\cdot \\sigma^{2} = \\frac{\\sigma^{2}}{m}.\n$$\n\n我们现在将方差缩减因子定义为平均后估计量的方差与单次试验测量方差的比值。单次试验测量的方差为\n$$\n\\operatorname{Var}(Y_{1}) = \\sigma^{2}.\n$$\n因此，方差缩减因子 $\\rho(m)$ 是\n$$\n\\rho(m) = \\frac{\\operatorname{Var}\\!\\left(\\hat{\\theta}_{m}\\right)}{\\operatorname{Var}(Y_{1})} = \\frac{\\sigma^{2}/m}{\\sigma^{2}} = \\frac{1}{m}.\n$$\n这表明，对 $m$ 次独立试验求平均，会将方差减小到单次试验方差的 $1/m$。\n\n因为估计量是无偏的，均方误差等于方差，所以 MSE 小于或等于目标阈值 $T$ 的条件是\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{m}) = \\frac{\\sigma^{2}}{m} \\leq T.\n$$\n解出 $m$，\n$$\nm \\geq \\frac{\\sigma^{2}}{T}.\n$$\n代入给定值 $\\sigma^{2} = 9$ 和 $T = 0.75$ (两者单位均为 $(\\mu\\mathrm{V})^2$，确保单位一致性)，\n$$\nm \\geq \\frac{9}{0.75} = 12.\n$$\n因为 $m$ 必须是整数的试验次数，且界限恰好为 $12$，所以所需的最小试验次数是 $12$。",
            "answer": "$$\\boxed{12}$$"
        },
        {
            "introduction": "在理论上理解权衡之后，我们转向一个更实际的问题：当真实信号和噪声水平未知时，我们如何在实践中选择模型复杂度？这个动手编程练习  将指导你使用一种强大的非参数方法——Nadaraya-Watson核回归，并通过留一法交叉验证（LOOCV）来经验性地确定最佳核带宽 $h$。通过解决这个问题，你将掌握一种通用的、数据驱动的策略，用于在偏差和方差之间找到最佳平衡点，这是现代神经科学数据分析中的一项核心技能。",
            "id": "4198196",
            "problem": "您的任务是实现一个有原则的估计器选择程序，以量化和阐释模拟神经科学调谐数据中的偏差-方差权衡。考虑一个标量刺激变量 $s \\in [0,1]$，一个平滑的确定性调谐函数 $f(s)$，以及在采样刺激点 $s_i$ 处通过加性高斯噪声生成的带噪观测值 $y_i$。您将使用带有高斯核的 Nadaraya-Watson 核回归，并执行留一法交叉验证 (LOOCV) 来估计一组带宽值 $h$ 的预测风险，然后选择使 LOOCV 风险最小化的 $h$。\n\n基本原理和定义：\n- 观测数据由 $n$ 个独立样本 $\\{(s_i, y_i)\\}_{i=1}^n$ 组成，其中 $s_i \\sim \\text{Uniform}(0,1)$ 且 $y_i = f(s_i) + \\varepsilon_i$，$\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ 为独立噪声。\n- 对于任意查询点 $s$，带有高斯核和带宽 $h > 0$ 的 Nadaraya-Watson 核回归估计器为\n$$\n\\widehat{f}_h(s) = \\frac{\\sum_{j=1}^n K_h(s - s_j)\\, y_j}{\\sum_{j=1}^n K_h(s - s_j)},\n\\quad\nK_h(u) = \\exp\\!\\left(-\\frac{u^2}{2h^2}\\right).\n$$\n- 带宽 $h$ 的留一法交叉验证 (LOOCV) 预测风险定义为\n$$\nR_{\\text{LOO}}(h) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\widehat{f}^{(-i)}_h(s_i)\\right)^2,\n$$\n其中 $\\widehat{f}^{(-i)}_h(s)$ 是使用除第 $i$ 个样本外的所有样本（即在求和中排除 $\\{(s_i,y_i)\\}$）计算得出的相同 Nadaraya-Watson 估计器。该定义将“用其余数据点预测每个数据点”时的期望平方预测误差进行了形式化。如果在预测 $y_i$ 时分母 $\\sum_{j\\neq i} K_h(s_i - s_j)$ 在数值上为零（这在 $h$ 极小时可能发生），则通过留一法均值 $\\widehat{f}^{(-i)}_h(s_i) = \\frac{1}{n-1}\\sum_{j\\neq i} y_j$ 来定义预测值，以确保风险有明确定义。\n\n您的任务：\n1. 对于下述每个测试用例，使用给定的调谐函数 $f(s)$、样本量 $n$、噪声标准差 $\\sigma$ 和用于可复现性的伪随机种子，模拟数据集 $\\{(s_i, y_i)\\}_{i=1}^n$。\n2. 对于固定的带宽网格 $h \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20,\\,0.40,\\,0.80\\}$，计算如上定义的 $R_{\\text{LOO}}(h)$。\n3. 选择使 $R_{\\text{LOO}}(h)$ 最小化的带宽 $\\widehat{h}$。如果出现平局（即在数值容差内有多个带宽达到相同的最小风险），则在这些最小化器中选择最小的 $h$。\n4. 生成单行输出，其中包含所有测试用例的所选带宽 $\\widehat{h}$，按顺序排列，形式为用方括号括起来的逗号分隔列表，例如 $[h_1,h_2,h_3]$。\n\n测试套件规范（所有刺激都是无量纲的，不使用角度；输出中没有物理单位）：\n- 用例 A：$n = 50$，$\\sigma = 0.15$，种子 $= 101$。调谐函数\n$$\nf(s) = 2 \\;+\\; 4\\,\\exp\\!\\left( -\\frac{(s-0.30)^2}{2\\cdot 0.05^2} \\right) \\;+\\; 3\\,\\exp\\!\\left( -\\frac{(s-0.70)^2}{2\\cdot 0.08^2} \\right).\n$$\n- 用例 B：$n = 80$，$\\sigma = 0.50$，种子 $= 202$。$f(s)$ 与用例 A 相同。\n- 用例 C：$n = 15$，$\\sigma = 0.20$，种子 $= 303$。$f(s)$ 与用例 A 相同。\n- 用例 D：$n = 40$，$\\sigma = 0.00$，种子 $= 404$。常数调谐函数\n$$\nf(s) = 1.5.\n$$\n- 用例 E：$n = 60$，$\\sigma = 0.10$，种子 $= 505$。振荡调谐函数\n$$\nf(s) = \\sin(8\\pi s) \\;+\\; 0.5\\,\\cos(3\\pi s).\n$$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，形式为用方括号括起来的逗号分隔列表，具体为按顺序排列的用例 A 到 E 的所选带宽 $\\widehat{h}$，即 $[\\widehat{h}_A,\\widehat{h}_B,\\widehat{h}_C,\\widehat{h}_D,\\widehat{h}_E]$。",
            "solution": "所提出的问题是计算统计学中一个定义明确且科学严谨的练习，具体涉及非参数回归的模型选择。它基于偏差-方差权衡、核密度估计和交叉验证等基本原则，这些是包括神经科学在内的众多科学领域中数据分析的标准和基本工具。该问题提供了所有必要的定义、参数以及一个清晰、客观的成功标准。它具备完整性、一致性和计算可行性。因此，该问题被认定为有效，并于下文提供解答。\n\n此问题的核心是为 Nadaraya-Watson 回归估计器选择一个最优的模型复杂度参数——即高斯核的带宽 $h$。选择过程必须有原则，需平衡两个相互竞争的目标：良好地拟合训练数据（低偏差）和泛化到新的、未见过的数据（低方差）。这就是典型的偏差-方差权衡。\n\n首先让我们对各组成部分进行形式化。数据由模型 $y_i = f(s_i) + \\varepsilon_i$ 生成，其中 $f(s)$ 是一个确定性但未知的调谐函数，$s_i$ 是从 $\\text{Uniform}(0,1)$ 分布中采样的刺激点，而 $\\varepsilon_i$ 是来自高斯分布 $\\mathcal{N}(0, \\sigma^2)$ 的独立同分布噪声项。我们的目标是构建一个能很好地近似 $f$ 的估计器 $\\widehat{f}$。\n\n在查询点 $s$ 处的 Nadaraya-Watson 估计器是观测响应 $y_j$ 的加权平均值：\n$$\n\\widehat{f}_h(s) = \\frac{\\sum_{j=1}^n K_h(s - s_j)\\, y_j}{\\sum_{j=1}^n K_h(s - s_j)}\n$$\n函数 $K_h(u) = \\exp(-u^2 / (2h^2))$ 是带宽为 $h > 0$ 的高斯核。带宽 $h$ 控制核的宽度，从而控制最终估计 $\\widehat{f}_h(s)$ 的平滑度。\n\n$h$ 的选择通过调节偏差-方差权衡，对估计器的性能起着决定性作用：\n- 较小的 $h$ 值意味着核较窄。估计值 $\\widehat{f}_h(s)$ 仅受与 $s$ 非常接近的数据点 $(s_j, y_j)$ 的影响。这使得模型能够捕捉真实函数 $f(s)$ 中的精细变化，从而导致低偏差。然而，估计值对这些少数局部点中的噪声 $\\varepsilon_j$ 变得高度敏感，导致高方差。这构成了过拟合。\n- 较大的 $h$ 值意味着核较宽。估计值 $\\widehat{f}_h(s)$ 是对广大区域数据点的平均。这平滑了局部噪声，导致低方差。然而，这种激进的平滑可能会抹去真实函数 $f(s)$ 的重要特征（如峰值或振荡），导致高偏差。这构成了欠拟合。\n\n为了找到最佳平衡点，我们需要对模型的泛化误差（即其在新数据上的期望预测误差）进行可靠的估计。在训练数据本身上直接计算均方误差的简单方法是不合适的，因为它总是会偏好偏差最低（$h$ 最小）的模型，从而助长过拟合。留一法交叉验证 (LOOCV) 为这种泛化误差提供了一个近似无偏的估计。\n\nLOOCV 风险 $R_{\\text{LOO}}(h)$ 定义为：\n$$\nR_{\\text{LOO}}(h) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\widehat{f}^{(-i)}_h(s_i)\\right)^2\n$$\n此处，$\\widehat{f}^{(-i)}_h(s_i)$ 是对第 $i$ 个观测值 $y_i$ 的预测，它是使用排除了第 $i$ 个数据点 $(s_i, y_i)$ 本身的数据集训练出的估计器计算得出的。此过程确保了对每个点的预测都是由一个在构建过程中没有见过该特定点的模型做出的，从而模拟了预测新数据的过程。最小化 $R_{\\text{LOO}}(h)$ 的带宽 $h$ 就是预期能提供最佳泛化性能的带宽。\n\n解决该问题的算法如下：\n1. 对五个测试用例中的每一个，我们首先使用指定的参数（样本量 $n$、噪声标准差 $\\sigma$、真实调谐函数 $f(s)$ 以及用于可复现性的伪随机种子）生成数据集 $\\{(s_i, y_i)\\}_{i=1}^n$。刺激点 $s_i$ 从 $\\text{Uniform}(0,1)$ 中抽取，响应值计算为 $y_i = f(s_i) + \\mathcal{N}(0, \\sigma^2)$。\n2. 对每个测试用例的数据集，我们遍历所提供的带宽网格 $h \\in \\{0.02, 0.05, 0.10, 0.20, 0.40, 0.80\\}$。\n3. 对于每个 $h$ 值，我们计算 LOOCV 风险 $R_{\\text{LOO}}(h)$。这涉及一个遍历所有数据点 $i = 1, \\dots, n$ 的内循环。在该内循环的每次迭代中，我们计算预测值 $\\widehat{f}^{(-i)}_h(s_i)$。\n4. 留一法预测的计算方式如下：\n    $$\n    \\widehat{f}^{(-i)}_h(s_i) = \\frac{\\sum_{j \\neq i} K_h(s_i - s_j)\\, y_j}{\\sum_{j \\neq i} K_h(s_i - s_j)}\n    $$\n    问题中指定的一个关键边界情况是，当分母 $\\sum_{j \\neq i} K_h(s_i - s_j)$ 在数值上为零时（如果所有其他点 $s_j$ 都远离 $s_i$，这在 $h$ 非常小时可能发生）。在这种情况下，预测值默认为被留出的响应值的均值：$\\widehat{f}^{(-i)}_h(s_i) = \\frac{1}{n-1}\\sum_{j\\neq i} y_j$。\n5. 在为所有候选带宽计算完 LOOCV 风险后，我们选择最优带宽 $\\widehat{h}$，即最小化 $R_{\\text{LOO}}(h)$ 的那个。如果多个带宽产生相同的最小风险，则根据平局规则选择其中最小的那个。\n6. 收集每个测试用例选定的带宽 $\\widehat{h}$，并按规定格式化最终列表。\n\n该过程构成了在非参数设置下进行数据驱动模型选择的严谨且标准的方法。实现将利用 `numpy` 中的向量化操作来提高效率，特别是在计算核权重和 LOOCV 预测时。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a principled estimator selection procedure to quantify and illustrate\n    the bias-variance tradeoff in simulated neuroscience tuning data.\n    \"\"\"\n\n    # Define the set of candidate bandwidths for the Gaussian kernel.\n    h_grid = [0.02, 0.05, 0.10, 0.20, 0.40, 0.80]\n\n    # Define the tuning functions for the different test cases.\n    def f_case_A(s):\n        \"\"\"Tuning function for Cases A, B, and C.\"\"\"\n        term1 = 4.0 * np.exp(-(s - 0.30)**2 / (2 * 0.05**2))\n        term2 = 3.0 * np.exp(-(s - 0.70)**2 / (2 * 0.08**2))\n        return 2.0 + term1 + term2\n\n    def f_case_D(s):\n        \"\"\"Tuning function for Case D.\"\"\"\n        return np.full_like(s, 1.5)\n\n    def f_case_E(s):\n        \"\"\"Tuning function for Case E.\"\"\"\n        return np.sin(8 * np.pi * s) + 0.5 * np.cos(3 * np.pi * s)\n\n    func_map = {\n        'A': f_case_A,\n        'B': f_case_A,\n        'C': f_case_A,\n        'D': f_case_D,\n        'E': f_case_E,\n    }\n\n    # Specification for the five test cases.\n    test_cases = [\n        {'case_id': 'A', 'n': 50, 'sigma': 0.15, 'seed': 101},\n        {'case_id': 'B', 'n': 80, 'sigma': 0.50, 'seed': 202},\n        {'case_id': 'C', 'n': 15, 'sigma': 0.20, 'seed': 303},\n        {'case_id': 'D', 'n': 40, 'sigma': 0.00, 'seed': 404},\n        {'case_id': 'E', 'n': 60, 'sigma': 0.10, 'seed': 505},\n    ]\n\n    selected_bandwidths = []\n\n    for case in test_cases:\n        n, sigma, seed, case_id = case['n'], case['sigma'], case['seed'], case['case_id']\n        \n        # 1. Simulate the dataset for the current test case.\n        rng = np.random.default_rng(seed)\n        s_samples = rng.uniform(0, 1, n)\n        true_f = func_map[case_id](s_samples)\n        y_samples = true_f + rng.normal(0, sigma, n)\n        \n        loo_risks = []\n\n        # 2. For each bandwidth, compute the LOOCV risk.\n        for h in h_grid:\n            squared_errors = np.zeros(n)\n            \n            # This loop implements the Leave-One-Out procedure.\n            for i in range(n):\n                # Isolate the i-th point and the leave-one-out set.\n                s_i, y_i = s_samples[i], y_samples[i]\n                s_loo = np.delete(s_samples, i)\n                y_loo = np.delete(y_samples, i)\n                \n                # Calculate kernel weights for the prediction at s_i.\n                u = s_i - s_loo\n                weights = np.exp(-u**2 / (2 * h**2))\n                \n                # Calculate the denominator of the Nadaraya-Watson estimator.\n                denominator = np.sum(weights)\n                \n                # Check for the special case of a numerically zero denominator.\n                if np.isclose(denominator, 0):\n                    # Fallback to the mean of the left-out y values.\n                    if n > 1:\n                        y_hat_i = np.mean(y_loo)\n                    else: # A case with n=1 would have no other points.\n                        y_hat_i = 0.0\n                else:\n                    # Standard Nadaraya-Watson prediction.\n                    numerator = np.sum(weights * y_loo)\n                    y_hat_i = numerator / denominator\n                \n                squared_errors[i] = (y_i - y_hat_i)**2\n            \n            # The LOOCV risk is the mean of the squared errors.\n            R_loo = np.mean(squared_errors)\n            loo_risks.append(R_loo)\n        \n        # 3. Select the bandwidth that minimizes the LOOCV risk.\n        # np.argmin() inherently handles the tie-breaking rule by returning\n        # the index of the first minimum, which corresponds to the smallest h.\n        min_risk_idx = np.argmin(loo_risks)\n        best_h = h_grid[min_risk_idx]\n        selected_bandwidths.append(best_h)\n\n    # 4. Produce the final output in the specified format.\n    print(f\"[{','.join(map(str, selected_bandwidths))}]\")\n\n\nsolve()\n```"
        }
    ]
}