{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds our understanding of permutation tests in their most fundamental form. Before relying on computational shortcuts, it is essential to grasp the exact, combinatorial nature of the null distribution. This exercise requires you to compute the exact $p$-value by considering all $2^n$ possible sign configurations for a small dataset, providing a concrete foundation for the logic that underpins all permutation-based inference. ",
            "id": "4185210",
            "problem": "A visual evoked potential study compares condition A versus condition B at a single occipital electroencephalography electrode. For each of $n=8$ subjects, the paired within-subject difference $d_i$ is the average amplitude (in microvolts) in a fixed post-stimulus window for condition A minus condition B. After standard preprocessing and artifact rejection, the observed paired differences are\n$$\nd = \\{+3,\\,+3,\\,+2,\\,-2,\\,+1,\\,+1,\\,-1,\\,+1\\}.\n$$\nAssume the standard sign-flipping permutation model for a paired design under the null hypothesis of no condition effect: conditional on the set of magnitudes $\\{|d_i|\\}$, all $2^{n}$ assignments of signs to the $n$ magnitudes are equally likely. Consider the two-sided permutation test whose test statistic is the mean difference,\n$$\nT = \\frac{1}{n}\\sum_{i=1}^{n} d_i,\n$$\nand whose $p$-value is defined as the fraction of the $2^{n}$ sign configurations for which the absolute value of the permuted statistic is greater than or equal to the absolute value of the observed statistic. Compute the exact two-sided permutation $p$-value using all $2^{n}$ sign configurations. Express your final answer as a single simplified exact fraction. Do not round.",
            "solution": "The user wants to compute the exact two-sided permutation p-value for a paired t-test scenario.\n\n### Step 1: Extract Givens\n- Number of subjects: $n=8$.\n- Observed paired differences: $d = \\{+3,\\,+3,\\,+2,\\,-2,\\,+1,\\,+1,\\,-1,\\,+1\\}$.\n- Null hypothesis ($H_0$): No condition effect.\n- Permutation model: Under $H_0$, conditional on the set of magnitudes $\\{|d_i|\\}$, all $2^n$ sign assignments are equally likely.\n- Test statistic: $T = \\frac{1}{n}\\sum_{i=1}^{n} d_i$.\n- P-value definition: The proportion of the $2^n$ permutations for which $|T_{perm}| \\ge |T_{obs}|$, where $T_{obs}$ is the observed statistic and $T_{perm}$ is the statistic for a given permutation.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Groundedness**: The problem describes a sign-flipping permutation test, a standard and rigorous non-parametric method for analyzing paired data, commonly used in fields like neuroscience for analyzing EEG/MEG data. The methodology is scientifically sound.\n- **Well-Posedness**: The problem is well-posed. The dataset, sample size, test statistic, null distribution, and definition of the p-value are all explicitly provided. This allows for the computation of a unique, exact answer.\n- **Objectivity**: The problem is stated in clear, objective language devoid of any subjective or ambiguous terms.\n- **Completeness**: All necessary information is present. The data set is complete, and the number of subjects $n=8$ is specified.\n- **Consistency**: The givens are internally consistent. The number of data points matches $n=8$.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution\nThe problem asks for the exact two-sided p-value from a sign-flipping permutation test on a set of $n=8$ paired differences.\n\nFirst, we calculate the observed value of the test statistic, $T_{obs}$. The test statistic is the mean difference, $T = \\frac{1}{n}\\sum_{i=1}^{n} d_i$.\nThe observed data are $d = \\{+3,\\,+3,\\,+2,\\,-2,\\,+1,\\,+1,\\,-1,\\,+1\\}$.\nThe sum of these observed differences is:\n$$ \\sum_{i=1}^{8} d_i = 3 + 3 + 2 + (-2) + 1 + 1 + (-1) + 1 = 8 $$\nThe observed test statistic is:\n$$ T_{obs} = \\frac{1}{8} \\sum_{i=1}^{8} d_i = \\frac{8}{8} = 1 $$\nFor a two-sided test, we are interested in the absolute value of the statistic, which is $|T_{obs}| = |1| = 1$.\n\nThe p-value is the proportion of all possible permutations of the signs for which the absolute value of the resulting test statistic, $|T_{perm}|$, is greater than or equal to $|T_{obs}|$.\n$$ p = \\frac{\\text{count of permutations where }|T_{perm}| \\ge |T_{obs}|}{\\text{total number of permutations}} $$\nUnder the null hypothesis, we consider the magnitudes of the differences as fixed and permute their signs. The set of magnitudes is:\n$$ \\{|d_i|\\} = \\{|+3|,|+3|,|+2|,|-2|,|+1|,|+1|,|-1|,|+1|\\} = \\{3, 3, 2, 2, 1, 1, 1, 1\\} $$\nThe total number of possible sign assignments is $2^n = 2^8 = 256$. Each assignment is equally likely under the null hypothesis.\n\nA permuted statistic is of the form $T_{perm} = \\frac{1}{n} \\sum_{i=1}^{n} s_i |d_i|$, where each $s_i$ can be either $+1$ or $-1$.\nThe condition for the p-value calculation is $|T_{perm}| \\ge 1$.\n$$ \\left| \\frac{1}{8} \\sum_{i=1}^{8} s_i |d_i| \\right| \\ge 1 $$\nMultiplying by $8$, this is equivalent to finding the number of sign arrangements $\\{s_i\\}$ such that:\n$$ \\left| \\sum_{i=1}^{8} s_i |d_i| \\right| \\ge 8 $$\nLet $S_{perm} = \\sum_{i=1}^{8} s_i |d_i|$. We need to count the number of permutations where $|S_{perm}| \\ge 8$.\n\nTo facilitate counting, let $P$ be the sum of the magnitudes that are assigned a positive sign ($s_i = +1$) and $N$ be the sum of the magnitudes assigned a negative sign ($s_i = -1$).\nThe total sum of all magnitudes is constant:\n$$ \\sum_{i=1}^{8} |d_i| = 3+3+2+2+1+1+1+1 = 14 $$\nSo, $P+N = 14$.\nThe permuted sum is $S_{perm} = P - N$. Substituting $N = 14 - P$, we get:\n$$ S_{perm} = P - (14 - P) = 2P - 14 $$\nThe condition $|S_{perm}| \\ge 8$ becomes $|2P - 14| \\ge 8$. This inequality splits into two cases:\n1. $2P - 14 \\ge 8 \\implies 2P \\ge 22 \\implies P \\ge 11$\n2. $2P - 14 \\le -8 \\implies 2P \\le 6 \\implies P \\le 3$\n\nWe now need to count the number of ways to choose a subset of the magnitudes $\\{3, 3, 2, 2, 1, 1, 1, 1\\}$ whose sum $P$ is either greater than or equal to $11$ or less than or equal to $3$. Note that the magnitudes are not unique; there are two $3$s, two $2$s, and four $1$s.\n\nCase 1: Count configurations where $P \\ge 11$.\n- To get $P=14$: We must select all magnitudes to have a positive sign. There is only $1$ way to do this.\n- To get $P=13$: The sum of omitted magnitudes must be $14-13=1$. We must omit one of the four $1$s. There are $\\binom{4}{1} = 4$ ways.\n- To get $P=12$: The sum of omitted magnitudes must be $14-12=2$. This can be achieved by omitting one $2$ (there are two $2$s, so $2$ ways) or by omitting two $1$s (there are four $1$s, so $\\binom{4}{2}=6$ ways). Total ways: $2+6=8$.\n- To get $P=11$: The sum of omitted magnitudes must be $14-11=3$. This can be achieved by omitting one $3$ (there are two $3$s, so $2$ ways), or by omitting one $2$ and one $1$ (two $2$s, four $1$s, so $\\binom{2}{1}\\binom{4}{1}=8$ ways), or by omitting three $1$s (four $1$s, so $\\binom{4}{3}=4$ ways). Total ways: $2+8+4=14$.\n\nThe total number of configurations for $P \\ge 11$ is $1 (P=14) + 4 (P=13) + 8 (P=12) + 14 (P=11) = 27$.\n\nCase 2: Count configurations where $P \\le 3$.\nThis case is symmetric to Case 1. For every configuration with a sum of positive parts $P$, the configuration with all signs flipped will have a sum of positive parts equal to $N = 14-P$. So, if $P \\ge 11$, then $14-P \\le 3$. The number of ways to get a sum $P' \\le 3$ is the same as the number of ways to get a sum $P \\ge 11$. Thus, there must be $27$ such configurations. Let's verify by direct counting.\n- To get $P=0$: We must select no magnitudes to have a positive sign (i.e., choose the empty set). There is only $1$ way.\n- To get $P=1$: We must select a subset of magnitudes summing to $1$. This means selecting one of the four $1$s. There are $\\binom{4}{1} = 4$ ways.\n- To get $P=2$: We can select one $2$ (2 ways) or two $1$s ($\\binom{4}{2}=6$ ways). Total ways: $2+6=8$.\n- To get $P=3$: We can select one $3$ (2 ways), or one $2$ and one $1$ ($\\binom{2}{1}\\binom{4}{1}=8$ ways), or three $1$s ($\\binom{4}{3}=4$ ways). Total ways: $2+8+4=14$.\n\nThe total number of configurations for $P \\le 3$ is $1 (P=0) + 4 (P=1) + 8 (P=2) + 14 (P=3) = 27$.\n\nThe total number of permutations satisfying $|S_{perm}| \\ge 8$ is the sum of the counts from both cases: $27 + 27 = 54$.\n\nThe p-value is the ratio of this count to the total number of permutations:\n$$ p = \\frac{54}{256} $$\nThis fraction can be simplified by dividing the numerator and denominator by their greatest common divisor, which is $2$:\n$$ p = \\frac{27}{128} $$\nThe numerator is $3^3$ and the denominator is $2^7$, so the fraction is in its simplest form.",
            "answer": "$$\n\\boxed{\\frac{27}{128}}\n$$"
        },
        {
            "introduction": "While exact tests provide a solid theoretical basis, they become computationally intractable for even moderately sized datasets. This practice transitions us to the workhorse of modern permutation testing: the Monte Carlo simulation. You will implement a sign-flipping test using a large but finite number of random permutations to approximate the null distribution, a technique essential for analyzing real-world neuroscience data. This exercise also highlights the importance of the finite-sample adjustment to the $p$-value, a crucial detail for robust statistical reporting. ",
            "id": "4185203",
            "problem": "You are given paired subject-level differences in normalized electroencephalography (EEG) alpha power between two experimental conditions. Each difference is unitless because alpha power has been pre-normalized per subject to z-scores prior to differencing. Under the null hypothesis that there is no condition effect, the sign of each subject-level difference is exchangeable, meaning that flipping each difference by multiplication with either $+1$ or $-1$ is equally likely. This implies that, conditional on the observed magnitudes of the paired differences, the null distribution of the mean difference can be generated by random sign-flips across subjects. Starting from this exchangeability principle and the definition of a two-sided hypothesis test as the probability that the absolute test statistic drawn under the null is at least as extreme as the observed absolute test statistic, write a program to compute the two-sided permutation $p$-value for the observed mean difference by Monte Carlo simulation using $m$ random sign-flip permutations, with a finite-sample adjustment that avoids zero estimates. For each test case, compute the observed mean difference $d_{\\text{obs}}$ over $n$ subjects, generate $m$ random sign-flip permutations, form the permutation mean differences $d^{(j)}$ for $j=1,\\dots,m$, count the number $b$ of permutations for which $\\lvert d^{(j)} \\rvert \\ge \\lvert d_{\\text{obs}} \\rvert$, and output the adjusted two-sided Monte Carlo permutation $p$-value based on $b$ and $m$. Your implementation must use a fixed pseudorandom number generator seed for each test case to ensure reproducible results.\n\nYour program should implement the above for the following test suite. In each case, treat the listed numbers as the vector of paired differences $\\Delta = [\\Delta_1,\\dots,\\Delta_n]$, use two-sided testing, and the specified number of permutations and seed:\n\nCase $1$ (happy path, moderate $n$, many permutations): $\\Delta = [0.38,0.52,0.41,0.65,0.29,0.76,0.44,0.58,0.31,0.49,0.55,0.60,0.33,0.47,0.69,0.36]$, $m=10000$, seed $=7$.\n\nCase $2$ (near-null differences, moderate permutations): $\\Delta = [-0.12,0.08,-0.05,0.03,-0.02,0.01,-0.09,0.07,-0.04,0.05]$, $m=5000$, seed $=17$.\n\nCase $3$ (small $n$, large observed effect): $\\Delta = [0.90,0.80,1.10,0.70]$, $m=2000$, seed $=3$.\n\nCase $4$ (boundary, zero permutations): $\\Delta = [0.20,-0.10,0.30]$, $m=0$, seed $=11$.\n\nCase $5$ (boundary, one permutation): $\\Delta = [0.30,-0.20,0.10,-0.10,0.05,-0.05]$, $m=1$, seed $=123$.\n\nFor each case, output the two-sided permutation $p$-value as a floating-point number rounded to $6$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\"[p_1,p_2,p_3,p_4,p_5]\"$. The final outputs are unitless real numbers in $[0,1]$ and must be presented with $6$ digits after the decimal point.",
            "solution": "The problem statement is scientifically sound, well-posed, and objective. It outlines a standard and widely accepted statistical procedure—the Monte Carlo permutation test for paired samples—which is frequently applied in neuroscience for analyzing data from sources like electroencephalography (EEG). All necessary parameters, including data vectors, number of permutations, and random seeds, are provided for each test case, ensuring a unique and reproducible solution. The problem is formalizable and relevant to its stated field. Therefore, the problem is valid and I will proceed with a full solution.\n\nThe core task is to compute a two-sided permutation $p$-value for the mean of paired differences, based on the principle of exchangeability under the null hypothesis. The solution involves a Monte Carlo simulation to approximate the null distribution of the mean difference.\n\n**Underlying Principles and Algorithmic Formulation**\n\n1.  **Null Hypothesis and Exchangeability**: The null hypothesis, $H_0$, posits that there is no systematic effect of the experimental condition on the measured EEG alpha power. A direct consequence of $H_0$ is the principle of exchangeability: for each subject's paired difference, $\\Delta_i$, its sign is arbitrary. That is, the observed difference $\\Delta_i$ and its sign-flipped counterpart $-\\Delta_i$ are equally likely. This implies that under $H_0$, any combination of sign-flips across the $n$ subjects is equally probable. The total number of such combinations is $2^n$.\n\n2.  **Test Statistic**: The test statistic used to quantify the overall effect is the mean of the paired differences. The observed value of this statistic is calculated from the provided data vector $\\Delta = [\\Delta_1, \\dots, \\Delta_n]$ as:\n    $$d_{\\text{obs}} = \\frac{1}{n} \\sum_{i=1}^{n} \\Delta_i$$\n\n3.  **Null Distribution by Permutation**: To construct the null distribution of the test statistic, we simulate the consequences of $H_0$. A single permutation, indexed by $j$, involves creating a new data set $\\Delta^{(j)}$ by randomly flipping the signs of the original differences. This is achieved by generating a random sign vector $\\mathbf{s}^{(j)} = [s_1^{(j)}, s_2^{(j)}, \\dots, s_n^{(j)}]$, where each $s_i^{(j)}$ is drawn independently from a discrete uniform distribution over $\\{-1, +1\\}$. The permuted data vector is then:\n    $$\\Delta^{(j)} = [\\Delta_1 \\cdot s_1^{(j)}, \\Delta_2 \\cdot s_2^{(j)}, \\dots, \\Delta_n \\cdot s_n^{(j)}]$$\n    From this, we compute the corresponding permuted mean difference:\n    $$d^{(j)} = \\frac{1}{n} \\sum_{i=1}^{n} \\Delta_i^{(j)}$$\n\n4.  **Monte Carlo Approximation and $p$-value Calculation**: Generating all $2^n$ possible sign combinations is computationally prohibitive for even moderate $n$. Instead, we use a Monte Carlo approach, generating a large number, $m$, of random permutations. This yields a sample of $m$ test statistics $\\{d^{(1)}, d^{(2)}, \\dots, d^{(m)}\\}$ that approximates the true null distribution.\n\n    The problem specifies a two-sided test. The $p$-value is the probability of observing a test statistic under the null hypothesis that is at least as extreme as the observed one. For a two-sided test, \"extremity\" is measured by the absolute value. The $p$-value is thus $P(\\lvert d_{\\text{null}} \\rvert \\ge \\lvert d_{\\text{obs}} \\rvert)$.\n\n    We estimate this probability from our Monte Carlo sample by counting the number of permutations, $b$, for which the permuted mean difference is at least as extreme as the observed one:\n    $$b = \\sum_{j=1}^{m} \\mathbb{I}(\\lvert d^{(j)} \\rvert \\ge \\lvert d_{\\text{obs}} \\rvert)$$\n    where $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise.\n\n5.  **Finite-Sample Adjustment**: A naive $p$-value estimate would be $b/m$. However, this can yield a $p$-value of $0$, which is an overstatement of statistical significance, as the true $p$-value for an exact test can never be less than $1/2^n$. To provide a more conservative and stable estimate that avoids $p=0$, we use a standard adjustment. This involves including the observed statistic itself as part of the permutation distribution. The adjusted Monte Carlo $p$-value is:\n    $$p = \\frac{b+1}{m+1}$$\n    This formula effectively counts the observed outcome in both the numerator (as it is always as extreme as itself) and the denominator, ensuring that $p \\in [1/(m+1), 1]$. This is particularly important for boundary cases, such as when $m=0$. If $m=0$, then $b=0$, and the formula correctly yields $p = (0+1)/(0+1) = 1$, reflecting a complete lack of evidence against $H_0$ from the permutation procedure.\n\n**Algorithm Implementation**\n\nFor each test case defined by a data vector $\\Delta$, number of permutations $m$, and a random seed:\n\n1.  Initialize a pseudorandom number generator with the specified seed to ensure reproducibility.\n2.  Compute the observed mean difference, $d_{\\text{obs}}$, from the input vector $\\Delta$. Let $n$ be the number of subjects (the length of $\\Delta$).\n3.  If $m=0$, the $p$-value is $1.0$ by the adjusted formula.\n4.  If $m > 0$:\n    a. Generate an $m \\times n$ matrix of random signs, where each entry is either $-1$ or $+1$ with equal probability.\n    b. Create $m$ permuted data sets by performing an element-wise product between each row of the sign matrix and the original data vector $\\Delta$. This can be efficiently achieved using array broadcasting.\n    c. Compute the mean for each of the $m$ permuted data sets to obtain the vector of permuted means, $[d^{(1)}, \\dots, d^{(m)}]$.\n    d. Take the absolute value of both the permuted means and the observed mean.\n    e. Count the number of permuted means, $b$, whose absolute value is greater than or equal to the absolute value of the observed mean.\n5.  Calculate the final adjusted $p$-value as $p = (b+1)/(m+1)$.\n6.  The result is formatted to $6$ decimal places.\n\nThis procedure will be applied to each of the five test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes two-sided permutation p-values for a suite of test cases.\n\n    This function implements a Monte Carlo permutation test for the mean of\n    paired differences. It follows the principles of exchangeability under the\n    null hypothesis and uses a finite-sample adjustment for the p-value.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'delta': [0.38, 0.52, 0.41, 0.65, 0.29, 0.76, 0.44, 0.58, 0.31, 0.49, 0.55, 0.60, 0.33, 0.47, 0.69, 0.36], 'm': 10000, 'seed': 7},\n        {'delta': [-0.12, 0.08, -0.05, 0.03, -0.02, 0.01, -0.09, 0.07, -0.04, 0.05], 'm': 5000, 'seed': 17},\n        {'delta': [0.90, 0.80, 1.10, 0.70], 'm': 2000, 'seed': 3},\n        {'delta': [0.20, -0.10, 0.30], 'm': 0, 'seed': 11},\n        {'delta': [0.30, -0.20, 0.10, -0.10, 0.05, -0.05], 'm': 1, 'seed': 123},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        delta_data = np.array(case['delta'])\n        m = case['m']\n        seed = case['seed']\n        n = len(delta_data)\n\n        # Step 1: Compute the observed mean difference\n        d_obs = np.mean(delta_data)\n        abs_d_obs = np.abs(d_obs)\n        \n        # Handle the boundary case where m=0 permutations are requested\n        if m == 0:\n            # According to the (b+1)/(m+1) formula, with m=0, b must be 0.\n            # p-value = (0+1)/(0+1) = 1.0\n            p_value = 1.0\n            results.append(p_value)\n            continue\n\n        # Step 2: Generate m random sign-flip permutations\n        # Initialize a random number generator with the specified seed for reproducibility\n        rng = np.random.default_rng(seed=seed)\n        \n        # Generate an (m, n) matrix of random signs {-1, 1}\n        # This is more direct than generating {0, 1} and mapping\n        random_signs = rng.choice([-1, 1], size=(m, n))\n        \n        # Create permuted datasets by element-wise multiplication.\n        # Broadcasting delta_data (1, n) against random_signs (m, n).\n        permuted_deltas = random_signs * delta_data\n\n        # Step 3: Form the permutation mean differences\n        permuted_means = np.mean(permuted_deltas, axis=1)\n\n        # Step 4: Count the number of permutations where |d_perm| >= |d_obs|\n        b = np.sum(np.abs(permuted_means) >= abs_d_obs)\n        \n        # Step 5: Compute the adjusted two-sided Monte Carlo permutation p-value\n        p_value = (b + 1) / (m + 1)\n        \n        results.append(p_value)\n\n    # Final print statement in the exact required format.\n    # Each p-value is formatted to 6 decimal places.\n    formatted_results = [f\"{p:.6f}\" for p in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "We now scale up our methods to address one of the most significant challenges in neuroimaging analysis: the massive multiple comparisons problem. When testing hypotheses across thousands of voxels or sensors, controlling the family-wise error rate (FWER) is critical. This advanced practice introduces the elegant Westfall–Young step-down procedure, which leverages the joint distribution of test statistics captured by permutations to provide strong FWER control while accounting for the complex spatial correlation in the data. By deriving and applying this method, you will gain a powerful tool for rigorous, large-scale hypothesis testing. ",
            "id": "4185191",
            "problem": "A multi-voxel functional Magnetic Resonance Imaging (fMRI) group comparison uses a General Linear Model (GLM) to test for a condition effect across $m$ voxels. Let the per-voxel test statistic be the one-sided $t$-statistic, and assume under the global null hypothesis that the labels are exchangeable so that permutation resampling approximates the joint null distribution of the test statistics across voxels. The goal is to control the Family-Wise Error Rate (FWER) in the strong sense using the Westfall–Young (W-Y) step-down procedure, and to compute adjusted $p$-values for ordered hypotheses.\n\nStarting from the definition of FWER as the probability of at least one false rejection among the family of $m$ hypotheses, and the exchangeability principle under the global null with permutation resampling, derive from first principles how the step-down adjusted $p$-values arise from the empirical joint distribution of subset maxima of the test statistics across permutations. Explicitly outline the algorithm that, given ordered hypotheses by decreasing observed evidence, generates the W-Y step-down adjusted $p$-values in a multi-voxel analysis.\n\nThen, apply your derivation to the following concrete setting. There are $m = 4$ voxels, with observed test statistics ordered by decreasing value as\n$$\nT_{\\text{obs}} = \\begin{pmatrix} 4.0 \\\\ 3.5 \\\\ 3.1 \\\\ 2.4 \\end{pmatrix},\n$$\ncorresponding to voxels $v_1, v_2, v_3, v_4$ in this observed order. You are given $B = 20$ label permutations. For each permutation $b \\in \\{1,\\dots,20\\}$, the permuted test statistics for $(v_1, v_2, v_3, v_4)$ are provided by the matrix\n$$\n\\begin{pmatrix}\n4.2 & 2.1 & 1.0 & 0.5 \\\\\n4.1 & 3.0 & 2.0 & 1.8 \\\\\n2.8 & 3.6 & 3.0 & 2.2 \\\\\n2.5 & 1.2 & 3.7 & 1.5 \\\\\n2.9 & 2.0 & 1.5 & 3.8 \\\\\n3.0 & 2.6 & 3.2 & 2.4 \\\\\n2.3 & 0.9 & 3.3 & 0.4 \\\\\n2.1 & 2.2 & 1.1 & 3.1 \\\\\n3.6 & 2.5 & 2.0 & 0.7 \\\\\n3.2 & 1.9 & 2.6 & 2.4 \\\\\n2.7 & 3.4 & 2.7 & 1.9 \\\\\n3.1 & 2.8 & 2.9 & 2.0 \\\\\n2.4 & 1.5 & 2.7 & 2.8 \\\\\n3.3 & 2.7 & 2.5 & 2.3 \\\\\n2.0 & 3.2 & 2.1 & 2.0 \\\\\n3.5 & 3.3 & 2.8 & 2.9 \\\\\n2.2 & 2.4 & 2.2 & 2.6 \\\\\n3.7 & 2.1 & 2.9 & 2.7 \\\\\n2.6 & 1.8 & 0.7 & 2.5 \\\\\n3.0 & 2.2 & 2.3 & 1.9\n\\end{pmatrix}.\n$$\nAssume a one-sided test in the positive direction. Use the empirical proportion without any continuity correction (that is, use $\\frac{1}{B}\\sum_{b=1}^{B}(\\cdot)$, not the plus-one correction) for computing empirical exceedance probabilities from permutations. Compute the Westfall–Young step-down adjusted $p$-value for the third ordered hypothesis (that is, the hypothesis associated with $v_3$ and observed statistic $3.1$). Express your final numerical answer as a decimal, and round your answer to four significant figures.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It provides a formal description of a standard statistical procedure in neuroimaging data analysis, the Westfall–Young (W-Y) permutation-based step-down procedure for Family-Wise Error Rate (FWER) control, and supplies all necessary data for a specific calculation. The problem is valid and can be solved as follows.\n\n### Part 1: Derivation from First Principles\n\nThe Family-Wise Error Rate (FWER) is the probability of making at least one Type I error (a false rejection of a null hypothesis) among a family of $m$ simultaneous hypothesis tests. Let $H_1, \\dots, H_m$ be the null hypotheses for the $m$ voxels. Let $I_0 \\subseteq \\{1, \\dots, m\\}$ be the index set of the a priori unknown true null hypotheses. The FWER is defined as:\n$$\n\\text{FWER} = P \\left( \\bigcup_{j \\in I_0} \\{\\text{reject } H_j\\} \\right)\n$$\nStrong control of the FWER at level $\\alpha$ requires that $\\text{FWER} \\le \\alpha$ for any configuration of true and false nulls (i.e., for any $I_0$).\n\nThe Westfall–Young procedure achieves strong FWER control by leveraging the joint distribution of the test statistics, which is approximated using permutation resampling under the assumption of exchangeability. The procedure is a step-down method, meaning it tests hypotheses sequentially, starting from the one with the most evidence against the null.\n\nLet $t_{\\text{obs},1}, \\dots, t_{\\text{obs},m}$ be the observed test statistics for the $m$ voxels. The first step is to order the hypotheses based on the observed evidence. For a one-sided test in the positive direction, we order the statistics from largest to smallest:\n$$\nt_{\\text{obs},(1)} \\ge t_{\\text{obs},(2)} \\ge \\dots \\ge t_{\\text{obs},(m)}\n$$\nLet $H_{(1)}, H_{(2)}, \\dots, H_{(m)}$ be the corresponding ordered hypotheses.\n\nThe core idea is to compute an adjusted $p$-value, $\\tilde{p}_{(j)}$, for each ordered hypothesis $H_{(j)}$. An adjusted $p$-value represents the smallest FWER level $\\alpha$ at which the hypothesis would be rejected.\n\nThe unadjusted step-down $p$-value, $p'_{(j)}$, for hypothesis $H_{(j)}$ is defined as the probability of observing a test statistic at least as large as $t_{\\text{obs},(j)}$ from any of the remaining hypotheses $H_{(j)}, H_{(j+1)}, \\dots, H_{(m)}$, under the global null hypothesis. Formally:\n$$\np'_{(j)} = P \\left( \\max_{k=j, \\dots, m} T_{(k)} \\ge t_{\\text{obs},(j)} \\mid H_C \\right)\n$$\nwhere $T_{(k)}$ are the random variables representing the test statistics and $H_C = \\bigcap_{j=1}^m H_j$ is the complete null hypothesis.\n\nThe W-Y procedure uses permutations to estimate these probabilities. We generate $B$ permutations of the group labels. For each permutation $b \\in \\{1, \\dots, B\\}$, we compute a full set of $m$ test statistics, $\\{t_{b,1}, t_{b,2}, \\dots, t_{b,m}\\}$. These $B$ sets of statistics form an empirical approximation of the joint null distribution.\n\nTo estimate $p'_{(j)}$, we construct, for each permutation $b$, the maximum test statistic over the relevant subset of voxels. Let the original indices of the ordered hypotheses be $\\pi(1), \\pi(2), \\dots, \\pi(m)$, such that $t_{\\text{obs},\\pi(1)} = t_{\\text{obs},(1)}$, etc. For each permutation $b$, we compute the successive maxima:\n$$\nM_{b,j} = \\max_{k=j, \\dots, m} \\{t_{b,\\pi(k)}\\}\n$$\nThe empirical estimate of the unadjusted step-down $p$-value is then the proportion of permutations where this maximum exceeds the observed statistic:\n$$\n\\hat{p}'_{(j)} = \\frac{1}{B} \\sum_{b=1}^{B} I\\left(M_{b,j} \\ge t_{\\text{obs},(j)}\\right)\n$$\nwhere $I(\\cdot)$ is the indicator function.\n\nFinally, to ensure that rejecting $H_{(j)}$ implies rejecting all preceding hypotheses $H_{(k)}$ with $k < j$ (a logical requirement of the step-down procedure), the adjusted $p$-values, $\\tilde{p}_{(j)}$, are enforced to be monotonically non-decreasing. This is achieved through the following recursive adjustment:\n$$\n\\tilde{p}_{(1)} = \\hat{p}'_{(1)}\n$$\n$$\n\\tilde{p}_{(j)} = \\max\\left(\\tilde{p}_{(j-1)}, \\hat{p}'_{(j)}\\right) \\text{ for } j = 2, \\dots, m\n$$\nThis ensures that $\\tilde{p}_{(1)} \\le \\tilde{p}_{(2)} \\le \\dots \\le \\tilde{p}_{(m)}$. This final set of $\\tilde{p}_{(j)}$ values are the W-Y step-down adjusted $p$-values.\n\n### Part 2: Algorithm Outline\n\nGiven the observed test statistics and a matrix of permuted test statistics:\n1.  **Order Hypotheses**: Let the observed test statistics for $m$ voxels be $t_{\\text{obs},1}, \\dots, t_{\\text{obs},m}$. Order them such that $t_{\\text{obs},(1)} \\ge t_{\\text{obs},(2)} \\ge \\dots \\ge t_{\\text{obs},(m)}$. Let the voxels corresponding to this order be $v_{(1)}, \\dots, v_{(m)}$.\n2.  **Generate Successive Maxima Distributions**: From the $B \\times m$ matrix of permuted statistics, where columns correspond to the original voxel indices, create a new $B \\times m$ matrix of successive maxima, $M$. For each permutation $b \\in \\{1, \\dots, B\\}$ and each ordered hypothesis $j \\in \\{1, \\dots, m\\}$, calculate:\n    $$\n    M_{b,j} = \\max_{k=j, \\dots, m} \\{t_{b, v_{(k)}}\\}\n    $$\n    The $j$-th column of $M$ is the empirical null distribution for the maximum of the test statistics from the set $\\{v_{(j)}, \\dots, v_{(m)}\\}$.\n3.  **Compute Raw Step-down p-values**: For each ordered hypothesis $j=1, \\dots, m$, calculate the raw step-down $p$-value, $\\hat{p}'_{(j)}$, by comparing the observed statistic $t_{\\text{obs},(j)}$ to the corresponding distribution of maxima $M_{*,j}$:\n    $$\n    \\hat{p}'_{(j)} = \\frac{1}{B} \\sum_{b=1}^{B} I\\left(M_{b,j} \\ge t_{\\text{obs},(j)}\\right)\n    $$\n4.  **Enforce Monotonicity**: Compute the final adjusted $p$-values, $\\tilde{p}_{(j)}$, by taking the cumulative maximum of the raw $p$-values:\n    $$\n    \\tilde{p}_{(j)} = \\max_{k=1, \\dots, j} \\{\\hat{p}'_{(k)}\\}\n    $$\n\n### Part 3: Application to the Concrete Setting\n\nWe are given $m=4$ voxels, $B=20$ permutations, and the observed test statistics are already ordered:\n$$\nT_{\\text{obs}} = \\begin{pmatrix} t_{\\text{obs},1} \\\\ t_{\\text{obs},2} \\\\ t_{\\text{obs},3} \\\\ t_{\\text{obs},4} \\end{pmatrix} = \\begin{pmatrix} 4.0 \\\\ 3.5 \\\\ 3.1 \\\\ 2.4 \\end{pmatrix}\n$$\nThe corresponding voxels are $v_1, v_2, v_3, v_4$. The goal is to compute $\\tilde{p}_3$, the adjusted $p$-value for the third ordered hypothesis.\n\n1.  **Ordering**: The hypotheses are already ordered by the observed statistics, so we proceed with indices $1, 2, 3, 4$.\n\n2.  **Generate Successive Maxima**: We need to compute the distributions for $M_{*,1}, M_{*,2}, M_{*,3}$. Let the provided $20 \\times 4$ permutation matrix be $T_{\\text{perm}}$.\n    -   $M_{b,1} = \\max(t_{b,1}, t_{b,2}, t_{b,3}, t_{b,4})$\n    -   $M_{b,2} = \\max(t_{b,2}, t_{b,3}, t_{b,4})$\n    -   $M_{b,3} = \\max(t_{b,3}, t_{b,4})$\n\n    The columns of the maxima matrix $M$ are:\n    $M_{*,1} = \\begin{pmatrix} 4.2, 4.1, 3.6, 3.7, 3.8, 3.2, 3.3, 3.1, 3.6, 3.2, 3.4, 3.1, 2.8, 3.3, 3.2, 3.5, 2.6, 3.7, 2.6, 3.0 \\end{pmatrix}^T$\n    $M_{*,2} = \\begin{pmatrix} 2.1, 3.0, 3.6, 3.7, 3.8, 3.2, 3.3, 3.1, 2.5, 2.6, 3.4, 2.9, 2.8, 2.7, 3.2, 3.3, 2.6, 2.9, 2.5, 2.3 \\end{pmatrix}^T$\n    $M_{*,3} = \\begin{pmatrix} 1.0, 2.0, 3.0, 3.7, 3.8, 3.2, 3.3, 3.1, 2.0, 2.6, 2.7, 2.9, 2.8, 2.5, 2.1, 2.9, 2.6, 2.9, 2.5, 2.3 \\end{pmatrix}^T$\n\n3.  **Compute Raw Step-down p-values**:\n    -   To find $\\tilde{p}_3$, we first need $\\hat{p}'_1, \\hat{p}'_2$, and $\\hat{p}'_3$.\n    -   For $\\hat{p}'_1$, we compare $M_{*,1}$ with $t_{\\text{obs},1} = 4.0$. The values in $M_{*,1}$ that are $\\ge 4.0$ are $\\{4.2, 4.1\\}$. The count is $2$.\n    $$\n    \\hat{p}'_1 = \\frac{2}{20} = 0.1\n    $$\n    -   For $\\hat{p}'_2$, we compare $M_{*,2}$ with $t_{\\text{obs},2} = 3.5$. The values in $M_{*,2}$ that are $\\ge 3.5$ are $\\{3.6, 3.7, 3.8\\}$. The count is $3$.\n    $$\n    \\hat{p}'_2 = \\frac{3}{20} = 0.15\n    $$\n    -   For $\\hat{p}'_3$, we compare $M_{*,3}$ with $t_{\\text{obs},3} = 3.1$. The values in $M_{*,3}$ that are $\\ge 3.1$ are $\\{3.7, 3.8, 3.2, 3.3, 3.1\\}$. The count is $5$.\n    $$\n    \\hat{p}'_3 = \\frac{5}{20} = 0.25\n    $$\n\n4.  **Enforce Monotonicity**:\n    -   The first adjusted $p$-value is $\\tilde{p}_1 = \\hat{p}'_1 = 0.1$.\n    -   The second adjusted $p$-value is $\\tilde{p}_2 = \\max(\\tilde{p}_1, \\hat{p}'_2) = \\max(0.1, 0.15) = 0.15$.\n    -   The third adjusted $p$-value, which is the value we seek, is:\n    $$\n    \\tilde{p}_3 = \\max(\\tilde{p}_2, \\hat{p}'_3) = \\max(0.15, 0.25) = 0.25\n    $$\nThe W-Y step-down adjusted $p$-value for the third ordered hypothesis is $0.25$. The problem requires the answer to be a decimal rounded to four significant figures. This gives $0.2500$.",
            "answer": "$$\n\\boxed{0.2500}\n$$"
        }
    ]
}