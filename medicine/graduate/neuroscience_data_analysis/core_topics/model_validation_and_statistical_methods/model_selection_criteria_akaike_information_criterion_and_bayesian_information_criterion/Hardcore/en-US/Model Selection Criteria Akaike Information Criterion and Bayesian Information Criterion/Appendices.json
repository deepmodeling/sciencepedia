{
    "hands_on_practices": [
        {
            "introduction": "Understanding information criteria begins with mastering their calculation. This first exercise provides a clear, hypothetical scenario from epidemiology where you can apply the formulas for both the Akaike Information Criterion ($AIC$) and the Bayesian Information Criterion ($BIC$) to two competing models. By working through this problem , you will gain direct insight into how the different penalty terms for model complexity can lead these two powerful criteria to favor different models, a common and important outcome in practice.",
            "id": "4595206",
            "problem": "A hospital-based cross-sectional study investigates risk factors for current influenza infection among adult patients presenting with acute respiratory symptoms. The binary outcome is infection status, coded as $Y_{i} \\in \\{0,1\\}$ for patient $i$, with a logistic regression generalized linear model using the logit link. Two nested models are fit by maximum likelihood on the same dataset of $n = 1000$ independent patients.\n\nModel $\\mathcal{M}_{1}$ (full) includes an intercept and $5$ covariate terms (total parameter count $k_{1} = 6$). Model $\\mathcal{M}_{0}$ (reduced) includes an intercept and $3$ covariate terms (total parameter count $k_{0} = 4$). The maximized log-likelihoods are $\\ell_{1} = -120$ for $\\mathcal{M}_{1}$ and $\\ell_{0} = -125$ for $\\mathcal{M}_{0}$. Using the definitions of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) for model selection in generalized linear models, compute the AIC and BIC for each model and determine, based on each criterion separately, which model is preferred.\n\nReport your numerical results for $(\\mathrm{AIC}_{1}, \\mathrm{BIC}_{1}, \\mathrm{AIC}_{0}, \\mathrm{BIC}_{0})$ as a single row matrix, rounded to four significant figures. No units are required. The preference decision does not need to be included in the reported matrix.",
            "solution": "The problem requires the calculation of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) for two nested logistic regression models, $\\mathcal{M}_{1}$ (full) and $\\mathcal{M}_{0}$ (reduced), and a determination of which model is preferred by each criterion.\n\nThe problem provides the following information:\nThe sample size is $n = 1000$ patients.\nFor the full model $\\mathcal{M}_{1}$:\nThe number of parameters is $k_{1} = 6$.\nThe maximized log-likelihood is $\\ell_{1} = -120$.\n\nFor the reduced model $\\mathcal{M}_{0}$:\nThe number of parameters is $k_{0} = 4$.\nThe maximized log-likelihood is $\\ell_{0} = -125$.\n\nThe Akaike Information Criterion (AIC) is defined as:\n$$ \\mathrm{AIC} = 2k - 2\\ell $$\nwhere $k$ is the number of estimated parameters in the model and $\\ell$ is the maximized value of the log-likelihood function for the model.\n\nThe Bayesian Information Criterion (BIC) is defined as:\n$$ \\mathrm{BIC} = k \\log(n) - 2\\ell $$\nwhere $n$ is the number of observations, or sample size.\n\nFor both criteria, a model with a lower value is generally preferred, as it indicates a better balance between model fit (higher $\\ell$) and model parsimony (lower $k$).\n\nFirst, we compute the AIC and BIC for the full model, $\\mathcal{M}_{1}$.\nUsing the given values $k_{1} = 6$, $\\ell_{1} = -120$, and $n = 1000$:\n$$ \\mathrm{AIC}_{1} = 2k_{1} - 2\\ell_{1} = 2(6) - 2(-120) = 12 + 240 = 252 $$\n$$ \\mathrm{BIC}_{1} = k_{1} \\log(n) - 2\\ell_{1} = 6 \\log(1000) - 2(-120) = 6 \\log(1000) + 240 $$\nTo find the numerical value of $\\mathrm{BIC}_{1}$, we use $\\log(1000) \\approx 6.907755$:\n$$ \\mathrm{BIC}_{1} \\approx 6(6.907755) + 240 \\approx 41.44653 + 240 = 281.44653 $$\n\nNext, we compute the AIC and BIC for the reduced model, $\\mathcal{M}_{0}$.\nUsing the given values $k_{0} = 4$, $\\ell_{0} = -125$, and $n = 1000$:\n$$ \\mathrm{AIC}_{0} = 2k_{0} - 2\\ell_{0} = 2(4) - 2(-125) = 8 + 250 = 258 $$\n$$ \\mathrm{BIC}_{0} = k_{0} \\log(n) - 2\\ell_{0} = 4 \\log(1000) - 2(-125) = 4 \\log(1000) + 250 $$\nUsing $\\log(1000) \\approx 6.907755$:\n$$ \\mathrm{BIC}_{0} \\approx 4(6.907755) + 250 \\approx 27.63102 + 250 = 277.63102 $$\n\nNow, we determine the preferred model based on each criterion by comparing the calculated values.\nFor the AIC:\n$\\mathrm{AIC}_{1} = 252$ and $\\mathrm{AIC}_{0} = 258$.\nSince $\\mathrm{AIC}_{1}  \\mathrm{AIC}_{0}$ ($252  258$), the Akaike Information Criterion prefers the full model, $\\mathcal{M}_{1}$.\n\nFor the BIC:\n$\\mathrm{BIC}_{1} \\approx 281.4465$ and $\\mathrm{BIC}_{0} \\approx 277.6310$.\nSince $\\mathrm{BIC}_{0}  \\mathrm{BIC}_{1}$ ($277.6310  281.4465$), the Bayesian Information Criterion prefers the reduced model, $\\mathcal{M}_{0}$.\n\nThe divergence in preference is a known characteristic when comparing these two criteria. The penalty for model complexity in BIC, $k \\log(n)$, is larger than the penalty in AIC, $2k$, whenever $\\log(n) > 2$, which is true for sample sizes $n > e^2 \\approx 7.4$. In this case, with $n = 1000$, the BIC imposes a much stronger penalty for the two additional parameters in $\\mathcal{M}_{1}$, leading to the selection of the more parsimonious model $\\mathcal{M}_{0}$.\n\nThe problem requires the numerical results for $(\\mathrm{AIC}_{1}, \\mathrm{BIC}_{1}, \\mathrm{AIC}_{0}, \\mathrm{BIC}_{0})$ rounded to four significant figures.\n$\\mathrm{AIC}_{1} = 252$, which is $252.0$ to four significant figures.\n$\\mathrm{BIC}_{1} \\approx 281.44653$, which rounds to $281.4$.\n$\\mathrm{AIC}_{0} = 258$, which is $258.0$ to four significant figures.\n$\\mathrm{BIC}_{0} \\approx 277.63102$, which rounds to $277.6$.\nTherefore, the final vector of values is $(252.0, 281.4, 258.0, 277.6)$.",
            "answer": "$$ \\boxed{\\begin{pmatrix} 252.0  281.4  258.0  277.6 \\end{pmatrix}} $$"
        },
        {
            "introduction": "Theory is best understood through application, and this next practice moves from abstract calculation to a concrete data analysis task. In this exercise , you will fit polynomial models to data and use code to apply the Akaike Information Criterion ($AIC$) and Bayesian Information Criterion ($BIC$) to select the best one. This problem is designed to build your practical skills in using information criteria to navigate the fundamental trade-off between goodness-of-fit and model complexity, thereby avoiding the common pitfall of overfitting.",
            "id": "2408012",
            "problem": "You are given several independent datasets consisting of paired values $(x_i,y_i)$. Assume that the observation errors in $y_i$ are independent and identically distributed Gaussian random variables with zero mean and unknown variance. For each dataset, consider two competing models for $y$ as a function of $x$: a quadratic polynomial of degree $2$ and a cubic polynomial of degree $3$. For each model, fit the coefficients by ordinary least squares under the stated Gaussian error assumption, then perform model selection using the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). The number of free parameters $k$ equals the polynomial degree plus one. Decide which model is preferred by AIC and which is preferred by BIC for each dataset, and report these decisions.\n\nDatasets (each presented as an ordered list of $x$ values and a corresponding ordered list of $y$ values of the same length):\n\n- Test case $1$:\n  - $x$: $[-3,-2,-1,0,1,2,3]$\n  - $y$: $[9.45,4.90,2.30,0.95,1.13,3.09,6.17]$\n\n- Test case $2$:\n  - $x$: $[-2,-1,0,1,2,3,4]$\n  - $y$: $[3.35,1.73,0.52,-0.03,0.94,3.75,9.31]$\n\n- Test case $3$:\n  - $x$: $[-1,-0.5,0,1,2]$\n  - $y$: $[1.53,1.605,2.01,3.49,5.96]$\n\n- Test case $4$:\n  - $x$: $[-3,-2,-1,0,1,2,3,4]$\n  - $y$: $[2.035,1.48,1.295,0.98,0.915,0.77,0.985,1.35]$\n\nYour program must, for each dataset, fit both candidate models and compute both the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) based on the Gaussian maximum-likelihood under the fitted residuals. For each criterion separately, select the model that attains the smaller value. Represent the quadratic model by the integer $2$ and the cubic model by the integer $3$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each item corresponds to one test case in the same order as above and must itself be a two-element list $[a,b]$ where $a$ is the preferred model by AIC and $b$ is the preferred model by BIC. For example, a valid output for four test cases is $[[2,2],[3,3],[2,2],[2,2]]$.\n\nNo physical units are required in your answer. Angles are not involved. Express all final decisions as integers.",
            "solution": "The problem is one of model selection. We are to decide which of two polynomial models, a quadratic or a cubic, provides a better description of the given datasets. The decision is to be made using the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). Both criteria implement the principle of parsimony, balancing the goodness of fit with model complexity.\n\nA polynomial model of degree $d$ is a function of the form:\n$$ f(x; \\mathbf{\\beta}) = \\sum_{j=0}^{d} \\beta_j x^j $$\nwhere $\\mathbf{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_d)$ is the vector of coefficients.\n\nFor a given dataset consisting of $n$ points $(x_i, y_i)$, the first step is to determine the optimal coefficients $\\hat{\\mathbf{\\beta}}$ for each model. This is achieved through Ordinary Least Squares (OLS), which minimizes the Residual Sum of Squares (RSS):\n$$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - f(x_i; \\hat{\\mathbf{\\beta}}))^2 $$\nA lower RSS indicates a better fit to the data.\n\nHowever, a more complex model (higher degree $d$) will almost always achieve a lower RSS. To prevent overfitting, we use information criteria that penalize model complexity. Under the assumption of i.i.d. Gaussian errors, the AIC and BIC are given by:\n$$ \\text{AIC} = n \\log\\left(\\frac{\\text{RSS}}{n}\\right) + 2k $$\n$$ \\text{BIC} = n \\log\\left(\\frac{\\text{RSS}}{n}\\right) + k \\log(n) $$\nHere, $n$ is the number of data points, and $k$ is the number of free parameters in the model. The problem explicitly states that $k$ is the polynomial degree plus one ($k=d+1$).\n- For the quadratic model ($d=2$), we have $k=2+1=3$.\n- For the cubic model ($d=3$), we have $k=3+1=4$.\n\nThe term $n \\log(\\text{RSS}/n)$ is related to the maximum likelihood of the model and represents the goodness of fit. The terms $2k$ (for AIC) and $k \\log(n)$ (for BIC) are penalty terms for model complexity. Since $\\log(n) > 2$ for $n \\ge 8$, BIC imposes a stronger penalty on complexity than AIC for datasets of this size. For each criterion, the model with the smaller value is preferred.\n\nThe procedure for each dataset is as follows:\n1.  For the quadratic model ($d=2, k=3$), compute the best-fit coefficients and the corresponding $\\text{RSS}_2$. Calculate $\\text{AIC}_2$ and $\\text{BIC}_2$.\n2.  For the cubic model ($d=3, k=4$), compute the best-fit coefficients and the corresponding $\\text{RSS}_3$. Calculate $\\text{AIC}_3$ and $\\text{BIC}_3$.\n3.  Compare $\\text{AIC}_2$ and $\\text{AIC}_3$. The model corresponding to the smaller value is the AIC preference.\n4.  Compare $\\text{BIC}_2$ and $\\text{BIC}_3$. The model corresponding to the smaller value is the BIC preference.\n\nThis procedure will be systematically applied to all provided test cases to derive the final solution. The computations will be performed using numerical libraries capable of robust polynomial fitting.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for the given datasets.\n\n    For each dataset, it fits a quadratic (degree 2) and a cubic (degree 3)\n    polynomial model. It then calculates the Akaike Information Criterion (AIC)\n    and Bayesian Information Criterion (BIC) for both models and determines\n    which model is preferred by each criterion.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"x\": [-3, -2, -1, 0, 1, 2, 3],\n            \"y\": [9.45, 4.90, 2.30, 0.95, 1.13, 3.09, 6.17]\n        },\n        {\n            \"x\": [-2, -1, 0, 1, 2, 3, 4],\n            \"y\": [3.35, 1.73, 0.52, -0.03, 0.94, 3.75, 9.31]\n        },\n        {\n            \"x\": [-1, -0.5, 0, 1, 2],\n            \"y\": [1.53, 1.605, 2.01, 3.49, 5.96]\n        },\n        {\n            \"x\": [-3, -2, -1, 0, 1, 2, 3, 4],\n            \"y\": [2.035, 1.48, 1.295, 0.98, 0.915, 0.77, 0.985, 1.35]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x_data = np.array(case[\"x\"])\n        y_data = np.array(case[\"y\"])\n        n = len(x_data)\n\n        # --- Model 1: Quadratic Polynomial ---\n        deg2 = 2\n        k2 = deg2 + 1  # Number of free parameters as per problem statement\n        \n        # polyfit with full=True returns extra information, including RSS.\n        # The second returned element is an array containing the RSS.\n        _, residuals2_info, _, _, _ = np.polyfit(x_data, y_data, deg2, full=True)\n        # In case of a perfect fit for n = deg, residuals_info can be empty.\n        rss2 = residuals2_info[0] if residuals2_info.size  0 else 1e-16\n\n        # Calculate AIC and BIC\n        # Using the formulation for least squares with Gaussian errors.\n        aic2 = n * np.log(rss2 / n) + 2 * k2\n        bic2 = n * np.log(rss2 / n) + k2 * np.log(n)\n\n        # --- Model 2: Cubic Polynomial ---\n        deg3 = 3\n        k3 = deg3 + 1  # Number of free parameters\n        \n        _, residuals3_info, _, _, _ = np.polyfit(x_data, y_data, deg3, full=True)\n        rss3 = residuals3_info[0] if residuals3_info.size  0 else 1e-16\n        \n        # Calculate AIC and BIC\n        aic3 = n * np.log(rss3 / n) + 2 * k3\n        bic3 = n * np.log(rss3 / n) + k3 * np.log(n)\n\n        # --- Model Selection ---\n        # The model with the lower criterion value is preferred.\n        # Model 2 = Quadratic, Model 3 = Cubic\n        aic_choice = 3 if aic3  aic2 else 2\n        bic_choice = 3 if bic3  bic2 else 2\n\n        results.append([aic_choice, bic_choice])\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, e.g., [[2,2],[3,3],...].\n    # map(str, results) will convert each inner list to its string representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Standard model selection criteria rely on large-sample approximations, a condition not always met in neuroscience research where data can be costly to acquire. This advanced practice  introduces the small-sample corrected Akaike Information Criterion ($AICc$), a vital tool for robust model selection in such scenarios. You will go beyond simple application by deriving the $AICc$ from foundational principles, providing a deeper understanding of why this correction is necessary and how it works.",
            "id": "4178100",
            "problem": "A cognitive neuroscience experiment measures trial-averaged membrane potential responses from a single neuron to $n$ presentations of a complex visual stimulus. An encoding model posits that the mean response is a linear function of $p$ known stimulus features, and the residual measurement noise is independent and identically distributed Gaussian. Let the observed responses be $\\{y_{i}\\}_{i=1}^{n}$, the design matrix be $X \\in \\mathbb{R}^{n \\times p}$ (with an explicit column for the intercept included among the $p$ features), and the model parameters be the linear coefficients $\\beta \\in \\mathbb{R}^{p}$ and the noise variance $\\sigma^{2}  0$. The residual sum of squares at the maximum likelihood fit is $S = \\sum_{i=1}^{n} (y_{i} - x_{i}^{\\top}\\hat{\\beta})^{2}$.\n\nStarting only from the following fundamental bases:\n- The Gaussian likelihood for independent observations, and its logarithm.\n- The definition of Kullbackâ€“Leibler divergence (KLD) between the true data-generating distribution and a candidate model family, and the principle of minimizing the expected KLD as a model selection goal.\n\nDerive the small-sample corrected Akaike Information Criterion (AICc) for this linear-Gaussian encoding model, carefully identifying the parameter count $k$ for this model. Then evaluate the derived AICc for the dataset with $n = 36$, $p = 6$, and $S = 18.0$.\n\nExpress the final answer as a single real number, rounded to four significant figures. The criterion value is dimensionless; report it without units.",
            "solution": "The task is to derive the small-sample corrected Akaike Information Criterion (AICc) for a linear-Gaussian model from first principles and then to evaluate it for a given dataset.\n\n**Part 1: Derivation of AICc**\n\nThe derivation proceeds in three stages: first, defining the maximized log-likelihood for the model; second, introducing the Akaike Information Criterion (AIC) from its foundation in Kullback-Leibler (KL) divergence; and third, applying the small-sample correction to obtain AICc.\n\n**Stage 1: Maximized Log-Likelihood**\n\nThe model assumes that the observed responses $\\{y_i\\}_{i=1}^n$ are independent and drawn from a Gaussian distribution, $y_i \\sim \\mathcal{N}(x_i^{\\top}\\beta, \\sigma^2)$. The probability density function (PDF) for a single observation $y_i$ is:\n$$ P(y_i|\\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^{\\top}\\beta)^2}{2\\sigma^2}\\right) $$\nDue to the independence of observations, the likelihood function for the entire dataset $y = \\{y_1, \\dots, y_n\\}$ is the product of the individual PDFs:\n$$ L(\\beta, \\sigma^2|y) = \\prod_{i=1}^n P(y_i|\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i^{\\top}\\beta)^2\\right) $$\nThe log-likelihood function, $\\ell(\\beta, \\sigma^2|y) = \\log(L)$, is:\n$$ \\ell(\\beta, \\sigma^2|y) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i^{\\top}\\beta)^2 $$\nTo find the maximum likelihood estimates (MLEs), we maximize $\\ell$ with respect to the parameters $\\beta$ and $\\sigma^2$. Maximizing $\\ell$ with respect to $\\beta$ is equivalent to minimizing the residual sum of squares, $\\sum_{i=1}^n (y_i - x_i^{\\top}\\beta)^2$. The problem provides that the value of this sum at the MLE for $\\beta$, denoted $\\hat{\\beta}$, is $S = \\sum_{i=1}^n (y_i - x_i^{\\top}\\hat{\\beta})^2$.\n\nSubstituting $\\hat{\\beta}$ and $S$ into the log-likelihood function gives:\n$$ \\ell(\\hat{\\beta}, \\sigma^2|y) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{S}{2\\sigma^2} $$\nNext, we maximize this expression with respect to $\\sigma^2$ by setting its derivative to zero:\n$$ \\frac{\\partial\\ell}{\\partial(\\sigma^2)} = -\\frac{n}{2\\sigma^2} + \\frac{S}{2(\\sigma^2)^2} = 0 \\implies \\hat{\\sigma}^2_{ML} = \\frac{S}{n} $$\nSubstituting this MLE for the variance back into the log-likelihood function yields the maximized log-likelihood, $\\ell_{max}$:\n$$ \\ell_{max} = \\ell(\\hat{\\beta}, \\hat{\\sigma}^2|y) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left(\\frac{S}{n}\\right) - \\frac{S}{2(S/n)} $$\n$$ \\ell_{max} = -\\frac{n}{2} \\left[ \\log(2\\pi) + \\log\\left(\\frac{S}{n}\\right) + 1 \\right] $$\n\n**Stage 2: Akaike Information Criterion (AIC)**\n\nModel selection aims to choose a model that best approximates the true, unknown data-generating process. A principled way to measure the discrepancy between the true distribution $f$ and a model distribution $g$ is the Kullback-Leibler divergence, $D_{KL}(f||g)$. The goal is to select the model that minimizes the expected KLD.\n\nAkaike demonstrated that $-2\\ell_{max} + 2k$ is an approximately unbiased estimator for a quantity proportional to this expected KLD, where $k$ is the number of estimated parameters in the model. This gives rise to the Akaike Information Criterion:\n$$ \\text{AIC} = -2\\ell_{max} + 2k $$\nFor the linear-Gaussian model, the parameters are the $p$ regression coefficients (elements of $\\beta$) and the noise variance $\\sigma^2$. Therefore, the total number of estimated parameters is $k = p+1$.\n\n**Stage 3: Small-Sample Correction (AICc)**\n\nThe derivation of AIC relies on an asymptotic approximation ($n \\to \\infty$). When the sample size $n$ is not large relative to the number of parameters $k$, AIC can be a biased estimator and tends to favor overly complex models. For linear regression models with normally distributed errors, a less biased criterion, the AICc, was developed. It adjusts the penalty term for the number of parameters. The AICc is given by:\n$$ \\text{AICc} = \\text{AIC} + \\frac{2k(k+1)}{n-k-1} $$\nSubstituting the expression for AIC, we get:\n$$ \\text{AICc} = -2\\ell_{max} + 2k + \\frac{2k(k+1)}{n-k-1} $$\nThis is the final expression for AICc for the linear-Gaussian model, derived from the specified fundamental principles.\n\n**Part 2: Evaluation of AICc**\n\nWe are given the following values for the dataset:\n-   Number of observations, $n = 36$.\n-   Number of stimulus features (including intercept), $p = 6$.\n-   Residual sum of squares, $S = 18.0$.\n\nFirst, we determine the number of parameters, $k$:\n$$ k = p+1 = 6+1 = 7 $$\nNext, we calculate the maximized log-likelihood, $\\ell_{max}$, using the derived formula:\n$$ \\ell_{max} = -\\frac{36}{2} \\left[ \\log\\left(2\\pi\\right) + \\log\\left(\\frac{18.0}{36}\\right) + 1 \\right] $$\n$$ \\ell_{max} = -18 \\left[ \\log(2\\pi) + \\log(0.5) + 1 \\right] $$\n$$ \\ell_{max} = -18 \\left[ \\log(2\\pi) - \\log(2) + 1 \\right] = -18 \\left[ \\log(\\pi) + 1 \\right] $$\nUsing the values $\\pi \\approx 3.14159$ and $\\log(\\pi) \\approx 1.14473$, we have:\n$$ \\ell_{max} \\approx -18 (1.14473 + 1) = -18(2.14473) \\approx -38.60514 $$\nNow we calculate the standard AIC:\n$$ \\text{AIC} = -2\\ell_{max} + 2k \\approx -2(-38.60514) + 2(7) = 77.21028 + 14 = 91.21028 $$\nFinally, we calculate the correction term and add it to find AICc:\n$$ \\frac{2k(k+1)}{n-k-1} = \\frac{2(7)(7+1)}{36-7-1} = \\frac{2 \\cdot 7 \\cdot 8}{28} = \\frac{112}{28} = 4 $$\n$$ \\text{AICc} = \\text{AIC} + 4 \\approx 91.21028 + 4 = 95.21028 $$\nThe problem requires the answer to be rounded to four significant figures.\n$$ \\text{AICc} \\approx 95.21 $$",
            "answer": "$$\n\\boxed{95.21}\n$$"
        }
    ]
}