{
    "hands_on_practices": [
        {
            "introduction": "我们从一个模型选择的基本应用开始。这个练习在一个真实的流行病学模型场景中，对赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 进行了直接比较。通过为两个嵌套模型计算这些准则 ，你将亲手实践它们的公式，并观察它们对复杂度的不同惩罚如何导致不同的模型选择结论。",
            "id": "4595206",
            "problem": "一项在医院进行的横断面研究调查了出现急性呼吸道症状的成年患者当前流感感染的风险因素。二元结果是感染状态，对于患者 $i$ 编码为 $Y_{i} \\in \\{0,1\\}$，使用 logit 连接函数的逻辑斯蒂回归广义线性模型进行分析。在包含 $n = 1000$ 名独立患者的同一数据集上，通过最大似然法拟合了两个嵌套模型。\n\n模型 $\\mathcal{M}_{1}$（完整模型）包含一个截距项和 5 个协变量项（总参数数量 $k_{1} = 6$）。模型 $\\mathcal{M}_{0}$（简化模型）包含一个截距项和 3 个协变量项（总参数数量 $k_{0} = 4$）。对于模型 $\\mathcal{M}_{1}$，最大化对数似然为 $\\ell_{1} = -120$；对于模型 $\\mathcal{M}_{0}$，最大化对数似然为 $\\ell_{0} = -125$。使用广义线性模型中用于模型选择的赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 的定义，计算每个模型的 AIC 和 BIC，并分别根据每个准则确定哪个模型更优。\n\n将您的数值结果 $(\\mathrm{AIC}_{1}, \\mathrm{BIC}_{1}, \\mathrm{AIC}_{0}, \\mathrm{BIC}_{0})$ 以单行矩阵的形式报告，四舍五入到四位有效数字。不需要单位。偏好决策无需包含在报告的矩阵中。",
            "solution": "该问题要求计算两个嵌套逻辑斯蒂回归模型 $\\mathcal{M}_{1}$（完整模型）和 $\\mathcal{M}_{0}$（简化模型）的赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC)，并确定每个准则偏好哪个模型。\n\n问题提供了以下信息：\n样本量为 $n = 1000$ 名患者。\n对于完整模型 $\\mathcal{M}_{1}$：\n参数数量为 $k_{1} = 6$。\n最大化对数似然为 $\\ell_{1} = -120$。\n\n对于简化模型 $\\mathcal{M}_{0}$：\n参数数量为 $k_{0} = 4$。\n最大化对数似然为 $\\ell_{0} = -125$。\n\n赤池信息准则 (AIC) 定义为：\n$$ \\mathrm{AIC} = 2k - 2\\ell $$\n其中 $k$ 是模型中估计参数的数量，$\\ell$ 是模型对数似然函数的最大化值。\n\n贝叶斯信息准则 (BIC) 定义为：\n$$ \\mathrm{BIC} = k \\ln(n) - 2\\ell $$\n其中 $n$ 是观测数量或样本量。\n\n对于这两个准则，通常偏好值较低的模型，因为它表示在模型拟合度（较高的 $\\ell$）和模型简约性（较低的 $k$）之间取得了更好的平衡。\n\n首先，我们计算完整模型 $\\mathcal{M}_{1}$ 的 AIC 和 BIC。\n使用给定值 $k_{1} = 6$，$\\ell_{1} = -120$ 和 $n = 1000$：\n$$ \\mathrm{AIC}_{1} = 2k_{1} - 2\\ell_{1} = 2(6) - 2(-120) = 12 + 240 = 252 $$\n$$ \\mathrm{BIC}_{1} = k_{1} \\ln(n) - 2\\ell_{1} = 6 \\ln(1000) - 2(-120) = 6 \\ln(1000) + 240 $$\n为了计算 $\\mathrm{BIC}_{1}$ 的数值，我们使用 $\\ln(1000) \\approx 6.907755$：\n$$ \\mathrm{BIC}_{1} \\approx 6(6.907755) + 240 \\approx 41.44653 + 240 = 281.44653 $$\n\n接下来，我们计算简化模型 $\\mathcal{M}_{0}$ 的 AIC 和 BIC。\n使用给定值 $k_{0} = 4$，$\\ell_{0} = -125$ 和 $n = 1000$：\n$$ \\mathrm{AIC}_{0} = 2k_{0} - 2\\ell_{0} = 2(4) - 2(-125) = 8 + 250 = 258 $$\n$$ \\mathrm{BIC}_{0} = k_{0} \\ln(n) - 2\\ell_{0} = 4 \\ln(1000) - 2(-125) = 4 \\ln(1000) + 250 $$\n使用 $\\ln(1000) \\approx 6.907755$：\n$$ \\mathrm{BIC}_{0} \\approx 4(6.907755) + 250 \\approx 27.63102 + 250 = 277.63102 $$\n\n现在，我们通过比较计算出的值，根据每个准则确定偏好的模型。\n对于 AIC：\n$\\mathrm{AIC}_{1} = 252$ 和 $\\mathrm{AIC}_{0} = 258$。\n由于 $\\mathrm{AIC}_{1}  \\mathrm{AIC}_{0}$ ($252  258$)，赤池信息准则偏好完整模型 $\\mathcal{M}_{1}$。\n\n对于 BIC：\n$\\mathrm{BIC}_{1} \\approx 281.4465$ 和 $\\mathrm{BIC}_{0} \\approx 277.6310$。\n由于 $\\mathrm{BIC}_{0}  \\mathrm{BIC}_{1}$ ($277.6310  281.4465$)，贝叶斯信息准则偏好简化模型 $\\mathcal{M}_{0}$。\n\n在比较这两个准则时，偏好上的分歧是一个已知的特征。当 $\\ln(n) > 2$ 时，即样本量 $n > e^2 \\approx 7.4$ 时，BIC 中对模型复杂度的惩罚项 $k \\ln(n)$ 大于 AIC 中的惩罚项 $2k$。在本例中，$n = 1000$，因此 BIC 对 $\\mathcal{M}_{1}$ 中额外的两个参数施加了更强的惩罚，从而导致选择了更简约的模型 $\\mathcal{M}_{0}$。\n\n问题要求将 $(\\mathrm{AIC}_{1}, \\mathrm{BIC}_{1}, \\mathrm{AIC}_{0}, \\mathrm{BIC}_{0})$ 的数值结果四舍五入到四位有效数字。\n$\\mathrm{AIC}_{1} = 252$，保留四位有效数字为 $252.0$。\n$\\mathrm{BIC}_{1} \\approx 281.44653$，四舍五入为 $281.4$。\n$\\mathrm{AIC}_{0} = 258$，保留四位有效数字为 $258.0$。\n$\\mathrm{BIC}_{0} \\approx 277.63102$，四舍五入为 $277.6$。\n因此，最终的数值向量是 $(252.0, 281.4, 258.0, 277.6)$。",
            "answer": "$$ \\boxed{\\begin{pmatrix} 252.0  281.4  258.0  277.6 \\end{pmatrix}} $$"
        },
        {
            "introduction": "在简单计算的基础上，这个练习将深入探讨 AIC 和 BIC 的核心逻辑。你将推导出，根据每个准则，增加一个额外参数需要模型拟合优度（对数似然）达到多大的最小提升量。这个练习  清晰地揭示了惩罚项的实际意义，并展示了 BIC 的惩罚项为何会随样本量 $n$ 变化，而 AIC 则不会。",
            "id": "3919103",
            "problem": "一个合成生物学团队正在对诱导条件下启动子的单细胞信使核糖核酸（$\\mathrm{mRNA}$）计数进行建模。他们为 $n$ 个独立细胞比较了两个转录爆发的嵌套随机模型：一个参数维度为 $k_{S}$ 的较简单模型 $\\mathcal{M}_{S}$，以及一个参数维度为 $k_{C} = k_{S} + 1$ 的较复杂模型 $\\mathcal{M}_{C}$，后者增加了一个捕捉诱导依赖性爆发大小的、具有生物学解释意义的参数。设 $\\ln \\hat{L}_{S}$ 和 $\\ln \\hat{L}_{C}$ 分别表示这两个模型在其各自的最大似然估计（MLE）下计算出的最大化对数似然。将复杂模型相对于简单模型的对数似然改进定义为 $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$，对于通过最大似然拟合的嵌套模型，有 $\\Delta \\ln \\hat{L} \\geq 0$。\n\n从赤池信息准则（AIC）和贝叶斯信息准则（BIC）作为结合了拟合优度项和复杂度惩罚项的惩罚准则的标准定义出发，并仅使用上述假设，推导在给定样本量 $n$ 的情况下，根据每个准则，复杂模型 $\\mathcal{M}_{C}$ 优于简单模型 $\\mathcal{M}_{S}$ 所需的 $\\Delta \\ln \\hat{L}$ 的最小阈值。将您的最终答案表示为两个关于 $n$ 的精确解析表达式，以单行矩阵 $[\\Delta_{\\mathrm{AIC}} \\quad \\Delta_{\\mathrm{BIC}}]$ 的形式呈现，其中 $\\Delta_{\\mathrm{AIC}}$ 和 $\\Delta_{\\mathrm{BIC}}$ 分别是使复杂模型受到赤池信息准则和贝叶斯信息准则青睐的 $\\Delta \\ln \\hat{L}$ 的最小值。在您的最终表达式中，无需进行四舍五入，也无需使用单位。",
            "solution": "目标是根据赤池信息准则（AIC）和贝叶斯信息准则（BIC），确定为了使更复杂的模型 $\\mathcal{M}_{C}$ 优于更简单的模型 $\\mathcal{M}_{S}$ 所需的对数似然改进 $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$ 的最小阈值。\n\n对于像AIC和BIC这样的模型选择准则，准则值较低的模型更优。因此，如果 $\\text{AIC}(\\mathcal{M}_{C})  \\text{AIC}(\\mathcal{M}_{S})$ 和 $\\text{BIC}(\\mathcal{M}_{C})  \\text{BIC}(\\mathcal{M}_{S})$，则分别表示 $\\mathcal{M}_{C}$ 优于 $\\mathcal{M}_{S}$。阈值对应于等号成立的点。\n\n设 $k$ 是模型中估计参数的数量，$\\hat{L}$ 是模型似然函数的最大化值，而 $n$ 是数据点的数量（在此情况下，是独立细胞的数量）。\n\n**1. 赤池信息准则（AIC）的推导**\n\nAIC的标准定义是：\n$$ \\text{AIC} = 2k - 2\\ln \\hat{L} $$\n\n对于我们的两个模型 $\\mathcal{M}_{S}$ 和 $\\mathcal{M}_{C}$，AIC值分别为：\n$$ \\text{AIC}_{S} = 2k_{S} - 2\\ln \\hat{L}_{S} $$\n$$ \\text{AIC}_{C} = 2k_{C} - 2\\ln \\hat{L}_{C} $$\n\n当 $\\text{AIC}_{C}  \\text{AIC}_{S}$ 时，复杂模型 $\\mathcal{M}_{C}$ 优于简单模型 $\\mathcal{M}_{S}$。我们可以将此不等式写作：\n$$ 2k_{C} - 2\\ln \\hat{L}_{C}  2k_{S} - 2\\ln \\hat{L}_{S} $$\n\n为了找到关于对数似然改进的条件，我们重排不等式，将似然项和参数计数项分组：\n$$ 2\\ln \\hat{L}_{C} - 2\\ln \\hat{L}_{S} > 2k_{C} - 2k_{S} $$\n$$ 2(\\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}) > 2(k_{C} - k_{S}) $$\n\n根据问题的定义 $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$，我们有：\n$$ 2\\Delta \\ln \\hat{L} > 2(k_{C} - k_{S}) $$\n$$ \\Delta \\ln \\hat{L} > k_{C} - k_{S} $$\n\n问题陈述，复杂模型的参数维度比简单模型多一，即 $k_{C} = k_{S} + 1$，这意味着 $k_{C} - k_{S} = 1$。将此代入不等式，得到：\n$$ \\Delta \\ln \\hat{L} > 1 $$\n\n为了使 $\\mathcal{M}_{C}$ 更优所需要的 $\\Delta \\ln \\hat{L}$ 的最小值是该不等式的临界值。因此，AIC的最小阈值为：\n$$ \\Delta_{\\text{AIC}} = 1 $$\n\n**2. 贝叶斯信息准则（BIC）的推导**\n\nBIC的标准定义是：\n$$ \\text{BIC} = k\\ln(n) - 2\\ln \\hat{L} $$\n其中 $n$ 是样本量。\n\n对于我们的两个模型，BIC值分别为：\n$$ \\text{BIC}_{S} = k_{S}\\ln(n) - 2\\ln \\hat{L}_{S} $$\n$$ \\text{BIC}_{C} = k_{C}\\ln(n) - 2\\ln \\hat{L}_{C} $$\n\n当 $\\text{BIC}_{C}  \\text{BIC}_{S}$ 时，复杂模型 $\\mathcal{M}_{C}$ 优于简单模型 $\\mathcal{M}_{S}$：\n$$ k_{C}\\ln(n) - 2\\ln \\hat{L}_{C}  k_{S}\\ln(n) - 2\\ln \\hat{L}_{S} $$\n\n我们再次重排以分离出对数似然改进项：\n$$ 2\\ln \\hat{L}_{C} - 2\\ln \\hat{L}_{S} > k_{C}\\ln(n) - k_{S}\\ln(n) $$\n$$ 2(\\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}) > (k_{C} - k_{S})\\ln(n) $$\n\n代入 $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$：\n$$ 2\\Delta \\ln \\hat{L} > (k_{C} - k_{S})\\ln(n) $$\n\n再次，我们使用 $k_{C} - k_{S} = 1$ 这个事实：\n$$ 2\\Delta \\ln \\hat{L} > \\ln(n) $$\n$$ \\Delta \\ln \\hat{L} > \\frac{1}{2}\\ln(n) $$\n\n在BIC准则下，为了使 $\\mathcal{M}_{C}$ 更优所需要的 $\\Delta \\ln \\hat{L}$ 的最小值是该条件的边界值。因此，BIC的最小阈值为：\n$$ \\Delta_{\\text{BIC}} = \\frac{1}{2}\\ln(n) $$\n\n最终答案由推导出的两个阈值 $\\Delta_{\\text{AIC}}$ 和 $\\Delta_{\\text{BIC}}$ 组成，以行矩阵的形式呈现。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  \\frac{1}{2}\\ln(n) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在许多现实场景中，尤其是在神经科学领域，我们常常处理有限的数据。这个高级练习通过引入校正的赤池信息准则 (AICc) 来应对这一挑战，这对于小样本量的情况至关重要。你将从信息论基础（Kullback-Leibler 散度）出发，为线性高斯模型推导 AICc 公式并加以应用 ，从而体会当模型复杂度相对于数据量较高时，这种校正的重要性。",
            "id": "4178100",
            "problem": "一个认知神经科学实验测量了单个神经元对一个复杂视觉刺激进行 $n$ 次呈现后的试验平均膜电位响应。一个编码模型假设平均响应是 $p$ 个已知刺激特征的线性函数，并且残余测量噪声是独立同分布的高斯噪声。设观测到的响应为 $\\{y_{i}\\}_{i=1}^{n}$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$（在 $p$ 个特征中包含一个显式的截距列），模型参数为线性系数 $\\beta \\in \\mathbb{R}^{p}$ 和噪声方差 $\\sigma^{2}  0$。在最大似然拟合下的残差平方和为 $S = \\sum_{i=1}^{n} (y_{i} - x_{i}^{\\top}\\hat{\\beta})^{2}$。\n\n仅从以下基本原理出发：\n- 独立观测值的高斯似然函数及其对数。\n- 真实数据生成分布与候选模型族之间的 Kullback–Leibler 散度 (KLD) 的定义，以及将最小化期望 KLD 作为模型选择目标的原则。\n\n推导此线性高斯编码模型的小样本校正 Akaike 信息准则 (AICc)，并仔细确定该模型的参数数量 $k$。然后，对 $n = 36$，$p = 6$，$S = 18.0$ 的数据集，计算推导出的 AICc 值。\n\n将最终答案表示为一个实数，四舍五入到四位有效数字。该准则值为无量纲量；报告时无需单位。",
            "solution": "任务是从第一性原理出发，推导线性高斯模型的小样本校正 Akaike 信息准则 (AICc)，然后对给定的数据集进行评估。\n\n**第一部分：AICc 的推导**\n\n推导过程分三个阶段进行：首先，定义模型的最大化对数似然；其次，从 Kullback-Leibler (KL) 散度的基础上引入 Akaike 信息准则 (AIC)；第三，应用小样本校正得到 AICc。\n\n**阶段 1：最大化对数似然**\n\n模型假设观测到的响应 $\\{y_i\\}_{i=1}^n$ 是独立的，并且服从高斯分布 $y_i \\sim \\mathcal{N}(x_i^{\\top}\\beta, \\sigma^2)$。单个观测值 $y_i$ 的概率密度函数 (PDF) 为：\n$$ P(y_i|\\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^{\\top}\\beta)^2}{2\\sigma^2}\\right) $$\n由于观测值的独立性，整个数据集 $y = \\{y_1, \\dots, y_n\\}$ 的似然函数是各个 PDF 的乘积：\n$$ L(\\beta, \\sigma^2|y) = \\prod_{i=1}^n P(y_i|\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i^{\\top}\\beta)^2\\right) $$\n对数似然函数 $\\ell(\\beta, \\sigma^2|y) = \\ln(L)$ 为：\n$$ \\ell(\\beta, \\sigma^2|y) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i^{\\top}\\beta)^2 $$\n为了找到最大似然估计 (MLEs)，我们相对于参数 $\\beta$ 和 $\\sigma^2$ 最大化 $\\ell$。相对于 $\\beta$ 最大化 $\\ell$ 等价于最小化残差平方和 $\\sum_{i=1}^n (y_i - x_i^{\\top}\\beta)^2$。问题给出，在 $\\beta$ 的 MLE（记为 $\\hat{\\beta}$）处，该和的值为 $S = \\sum_{i=1}^n (y_i - x_i^{\\top}\\hat{\\beta})^2$。\n\n将 $\\hat{\\beta}$ 和 $S$ 代入对数似然函数，得到：\n$$ \\ell(\\hat{\\beta}, \\sigma^2|y) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{S}{2\\sigma^2} $$\n接下来，我们通过将其关于 $\\sigma^2$ 的导数设为零来最大化该表达式：\n$$ \\frac{\\partial\\ell}{\\partial(\\sigma^2)} = -\\frac{n}{2\\sigma^2} + \\frac{S}{2(\\sigma^2)^2} = 0 \\implies \\hat{\\sigma}^2_{ML} = \\frac{S}{n} $$\n将这个方差的 MLE 代回对数似然函数，得到最大化对数似然 $\\ell_{max}$：\n$$ \\ell_{max} = \\ell(\\hat{\\beta}, \\hat{\\sigma}^2|y) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{S}{n}\\right) - \\frac{S}{2(S/n)} $$\n$$ \\ell_{max} = -\\frac{n}{2} \\left[ \\ln(2\\pi) + \\ln\\left(\\frac{S}{n}\\right) + 1 \\right] $$\n\n**阶段 2：Akaike 信息准则 (AIC)**\n\n模型选择旨在选择一个能最好地逼近真实、未知的数据生成过程的模型。衡量真实分布 $f$ 和模型分布 $g$ 之间差异的一种原则性方法是 Kullback-Leibler 散度 $D_{KL}(f||g)$。目标是选择使期望 KLD 最小化的模型。\n\nAkaike 证明了 $-2\\ell_{max} + 2k$ 是与此期望 KLD 成正比的量的一个近似无偏估计量，其中 $k$ 是模型中估计参数的数量。这引出了 Akaike 信息准则：\n$$ \\text{AIC} = -2\\ell_{max} + 2k $$\n对于线性高斯模型，参数是 $p$ 个回归系数（$\\beta$ 的元素）和噪声方差 $\\sigma^2$。因此，估计参数的总数为 $k = p+1$。\n\n**阶段 3：小样本校正 (AICc)**\n\nAIC 的推导依赖于渐近逼近 ($n \\to \\infty$)。当样本量 $n$相对于参数数量 $k$ 不够大时，AIC 可能是一个有偏的估计量，并倾向于选择过于复杂的模型。对于具有正态分布误差的线性回归模型，一个偏差较小的准则 AICc 被提了出来。它调整了对参数数量的惩罚项。AICc 由下式给出：\n$$ \\text{AICc} = \\text{AIC} + \\frac{2k(k+1)}{n-k-1} $$\n代入 AIC 的表达式，我们得到：\n$$ \\text{AICc} = -2\\ell_{max} + 2k + \\frac{2k(k+1)}{n-k-1} $$\n这就是从指定的基本原理推导出的线性高斯模型的 AICc 的最终表达式。\n\n**第二部分：AICc 的评估**\n\n我们已知数据集的以下值：\n-   观测数量，$n = 36$。\n-   刺激特征数量（包括截距），$p = 6$。\n-   残差平方和，$S = 18.0$。\n\n首先，我们确定参数数量 $k$：\n$$ k = p+1 = 6+1 = 7 $$\n接下来，我们使用推导出的公式计算最大化对数似然 $\\ell_{max}$：\n$$ \\ell_{max} = -\\frac{36}{2} \\left[ \\ln\\left(2\\pi\\right) + \\ln\\left(\\frac{18.0}{36}\\right) + 1 \\right] $$\n$$ \\ell_{max} = -18 \\left[ \\ln(2\\pi) + \\ln(0.5) + 1 \\right] $$\n$$ \\ell_{max} = -18 \\left[ \\ln(2\\pi) - \\ln(2) + 1 \\right] = -18 \\left[ \\ln(\\pi) + 1 \\right] $$\n使用值 $\\pi \\approx 3.14159$ 和 $\\ln(\\pi) \\approx 1.14473$，我们有：\n$$ \\ell_{max} \\approx -18 (1.14473 + 1) = -18(2.14473) \\approx -38.60514 $$\n现在我们计算标准的 AIC：\n$$ \\text{AIC} = -2\\ell_{max} + 2k \\approx -2(-38.60514) + 2(7) = 77.21028 + 14 = 91.21028 $$\n最后，我们计算校正项并将其相加以求得 AICc：\n$$ \\frac{2k(k+1)}{n-k-1} = \\frac{2(7)(7+1)}{36-7-1} = \\frac{2 \\cdot 7 \\cdot 8}{28} = \\frac{112}{28} = 4 $$\n$$ \\text{AICc} = \\text{AIC} + 4 \\approx 91.21028 + 4 = 95.21028 $$\n问题要求将答案四舍五入到四位有效数字。\n$$ \\text{AICc} \\approx 95.21 $$",
            "answer": "$$\n\\boxed{95.21}\n$$"
        }
    ]
}