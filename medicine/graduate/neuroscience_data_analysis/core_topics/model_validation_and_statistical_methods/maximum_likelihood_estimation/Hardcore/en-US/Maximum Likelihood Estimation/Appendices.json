{
    "hands_on_practices": [
        {
            "introduction": "Let's begin with a fundamental application of Maximum Likelihood Estimation. This first exercise  walks through the core mechanics of deriving an estimator for a single-parameter model, using the exponential distribution, which is often used to model phenomena like the time between neural spikes. By maximizing the log-likelihood function, you will see how the most intuitive estimate for the mean, the sample mean, emerges directly from this powerful first principle.",
            "id": "1933604",
            "problem": "A network engineer is analyzing traffic on a data network. The time intervals between the arrivals of consecutive data packets at a specific router are modeled as independent and identically distributed random variables. Let a single such inter-arrival time be represented by the random variable $X$. Based on extensive prior studies, this time is assumed to follow an exponential distribution with the Probability Density Function (PDF) given by:\n$$f(x; \\theta) = \\frac{1}{\\theta} \\exp\\left(-\\frac{x}{\\theta}\\right), \\quad \\text{for } x > 0$$\nwhere $\\theta > 0$ is the unknown true mean inter-arrival time. To estimate this parameter for the current network conditions, the engineer collects a random sample of $n$ inter-arrival times: $x_1, x_2, \\dots, x_n$.\n\nYour task is to determine the Maximum Likelihood Estimator (MLE) for the mean inter-arrival time, $\\theta$. Express your answer as a function of the sample observations $x_1, x_2, \\dots, x_n$.",
            "solution": "We model $X_{1},\\dots,X_{n}$ as independent and identically distributed with PDF $f(x;\\theta)=\\theta^{-1}\\exp(-x/\\theta)$ for $x>0$ and $\\theta>0$. The likelihood function for observations $x_{1},\\dots,x_{n}$ is\n$$\nL(\\theta)=\\prod_{i=1}^{n}f(x_{i};\\theta)=\\prod_{i=1}^{n}\\left(\\theta^{-1}\\exp\\left(-\\frac{x_{i}}{\\theta}\\right)\\right)=\\theta^{-n}\\exp\\left(-\\frac{\\sum_{i=1}^{n}x_{i}}{\\theta}\\right).\n$$\nThe log-likelihood is\n$$\n\\ell(\\theta)=\\ln L(\\theta)=-n\\ln\\theta-\\frac{1}{\\theta}\\sum_{i=1}^{n}x_{i}.\n$$\nDifferentiate with respect to $\\theta$ and set to zero to find the critical point:\n$$\n\\frac{d\\ell}{d\\theta}=-\\frac{n}{\\theta}+\\frac{\\sum_{i=1}^{n}x_{i}}{\\theta^{2}}=0.\n$$\nMultiplying both sides by $\\theta^{2}$ yields\n$$\n-\\!n\\theta+\\sum_{i=1}^{n}x_{i}=0 \\quad\\Rightarrow\\quad \\hat{\\theta}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}.\n$$\nTo verify this is a maximum, compute the second derivative:\n$$\n\\frac{d^{2}\\ell}{d\\theta^{2}}=\\frac{n}{\\theta^{2}}-\\frac{2\\sum_{i=1}^{n}x_{i}}{\\theta^{3}}.\n$$\nEvaluating at $\\hat{\\theta}$ and using $\\sum_{i=1}^{n}x_{i}=n\\hat{\\theta}$ gives\n$$\n\\left.\\frac{d^{2}\\ell}{d\\theta^{2}}\\right|_{\\theta=\\hat{\\theta}}=\\frac{n}{\\hat{\\theta}^{2}}-\\frac{2n\\hat{\\theta}}{\\hat{\\theta}^{3}}=-\\frac{n}{\\hat{\\theta}^{2}}<0,\n$$\nso $\\hat{\\theta}$ maximizes the log-likelihood. Since each $x_{i}>0$, the estimator is positive and lies in the parameter space. Therefore, the MLE for $\\theta$ is the sample mean.",
            "answer": "$$\\boxed{\\frac{1}{n}\\sum_{i=1}^{n}x_{i}}$$"
        },
        {
            "introduction": "Building on the basics, we now tackle a multi-parameter problem that is central to analyzing continuous neural data like Local Field Potentials (LFPs). In this practice , you will derive the MLEs for both the mean $\\mu$ and variance $\\sigma^2$ of a Gaussian distribution. This exercise reveals a crucial and sometimes counter-intuitive property of MLE: the resulting estimator for variance is systematically biased, a key insight that highlights the distinction between an optimal estimator and an unbiased one.",
            "id": "4177490",
            "problem": "A laboratory records extracellular Local Field Potential (LFP) signals from a hippocampal region during a sustained behavioral task. After band-pass filtering to isolate a specific oscillatory band, the analyst extracts $n$ short, non-overlapping time windows and summarizes each window by the instantaneous amplitude $y_i$ at a fixed phase of the oscillation, for $i=1,\\dots,n$. Under a standard generative model for aggregated synaptic inputs in this band, assume the amplitudes are independent and identically distributed as $y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$, where $\\mu$ and $\\sigma^2$ denote the unknown mean and variance of the band-limited amplitude.\n\nStarting only from the probability density function of the Gaussian distribution and the independence assumption, use the maximum likelihood principle to derive the estimators of $\\mu$ and $\\sigma^2$. Then, treating the estimators as random variables under the true model, analyze the expectation of the variance estimator to quantify its bias. Finally, contrast this bias with the unbiased variance estimator that uses Bessel’s correction.\n\nYour tasks:\n- Derive the maximum likelihood estimators $\\hat{\\mu}$ and $\\hat{\\sigma}^2$ for $\\mu$ and $\\sigma^2$ based on the sample $\\{y_i\\}_{i=1}^{n}$.\n- Compute $\\mathbb{E}[\\hat{\\sigma}^2]$ under the true model and obtain the bias $\\mathrm{Bias}(\\hat{\\sigma}^2)=\\mathbb{E}[\\hat{\\sigma}^2]-\\sigma^2$ as a closed-form analytic expression in terms of $n$ and $\\sigma^2$.\n- Briefly explain how the unbiased estimator that replaces the denominator $n$ with $n-1$ removes this bias.\n\nProvide as your final answer only the bias $\\mathrm{Bias}(\\hat{\\sigma}^2)$ in closed form. No rounding is required, and no units are required.",
            "solution": "The problem requires the derivation of the maximum likelihood estimators for the parameters of a Gaussian distribution, followed by an analysis of the bias of the variance estimator.\n\nThe data consists of a set of $n$ independent and identically distributed (i.i.d.) observations $\\{y_i\\}_{i=1}^{n}$, where each $y_i$ is drawn from a Gaussian (Normal) distribution with unknown mean $\\mu$ and unknown variance $\\sigma^2$. The probability density function (PDF) for a single observation $y_i$ is given by:\n$$f(y_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)$$\n\n**Step 1: Derivation of the Maximum Likelihood Estimators (MLEs)**\n\nDue to the i.i.d. assumption, the likelihood function $L(\\mu, \\sigma^2)$ for the entire sample is the product of the individual PDFs:\n$$L(\\mu, \\sigma^2 | \\{y_i\\}) = \\prod_{i=1}^{n} f(y_i | \\mu, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right)$$\nTo find the parameters that maximize this function, it is computationally simpler to maximize the log-likelihood function, $\\ell(\\mu, \\sigma^2) = \\ln(L(\\mu, \\sigma^2))$:\n$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\nThe MLEs $\\hat{\\mu}$ and $\\hat{\\sigma}^2$ are the values of $\\mu$ and $\\sigma^2$ that maximize $\\ell$. We find them by setting the partial derivatives of $\\ell$ with respect to $\\mu$ and $\\sigma^2$ to zero.\n\nFirst, we differentiate with respect to $\\mu$:\n$$\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} 2(y_i - \\mu)(-1) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)$$\nSetting the derivative to zero and solving for the estimator $\\hat{\\mu}$:\n$$\\frac{1}{\\hat{\\sigma}^2}\\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0 \\implies \\sum_{i=1}^{n} y_i - n\\hat{\\mu} = 0 \\implies \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$$\nThe MLE for the mean, $\\hat{\\mu}$, is the sample mean.\n\nNext, we differentiate with respect to $\\sigma^2$. Let $\\theta = \\sigma^2$ for convenience.\n$$\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( -\\frac{n}{2}\\ln(\\theta) - \\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) = -\\frac{n}{2\\theta} + \\frac{1}{2\\theta^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\nSetting the derivative to zero and substituting the estimators $\\hat{\\theta} = \\hat{\\sigma}^2$ and $\\hat{\\mu}$:\n$$-\\frac{n}{2\\hat{\\sigma}^2} + \\frac{1}{2(\\hat{\\sigma}^2)^2}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0$$\nMultiplying by $2(\\hat{\\sigma}^2)^2$ yields:\n$$-n\\hat{\\sigma}^2 + \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$$\nThe MLE for the variance is $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$.\n\n**Step 2: Bias of the Variance Estimator $\\hat{\\sigma}^2$**\n\nThe bias of an estimator is the difference between its expected value and the true value of the parameter being estimated. We need to compute $\\mathrm{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2$. First, we compute the expectation $\\mathbb{E}[\\hat{\\sigma}^2]$.\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right]$$\nWe rewrite the term inside the summation by adding and subtracting the true mean $\\mu$:\n$$y_i - \\hat{\\mu} = (y_i - \\mu) - (\\hat{\\mu} - \\mu)$$\nThe sum of squares becomes:\n$$\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = \\sum_{i=1}^{n} [(y_i - \\mu) - (\\hat{\\mu} - \\mu)]^2 = \\sum_{i=1}^{n} \\left[ (y_i - \\mu)^2 - 2(y_i - \\mu)(\\hat{\\mu} - \\mu) + (\\hat{\\mu} - \\mu)^2 \\right]$$\n$$= \\sum_{i=1}^{n}(y_i - \\mu)^2 - 2(\\hat{\\mu} - \\mu)\\sum_{i=1}^{n}(y_i - \\mu) + n(\\hat{\\mu} - \\mu)^2$$\nRecognizing that $\\sum_{i=1}^{n}(y_i - \\mu) = n(\\frac{1}{n}\\sum y_i - \\mu) = n(\\hat{\\mu} - \\mu)$, we substitute this into the expression:\n$$\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = \\sum_{i=1}^{n}(y_i - \\mu)^2 - 2(\\hat{\\mu} - \\mu)n(\\hat{\\mu} - \\mu) + n(\\hat{\\mu} - \\mu)^2 = \\sum_{i=1}^{n}(y_i - \\mu)^2 - n(\\hat{\\mu} - \\mu)^2$$\nNow we take the expectation of this expression using the linearity of expectation:\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n}(y_i - \\mu)^2\\right] - n\\mathbb{E}\\left[(\\hat{\\mu} - \\mu)^2\\right]$$\nThe first term is $\\mathbb{E}\\left[\\sum_{i=1}^{n}(y_i - \\mu)^2\\right] = \\sum_{i=1}^{n}\\mathbb{E}[(y_i - \\mu)^2] = \\sum_{i=1}^{n}\\sigma^2 = n\\sigma^2$, since $\\mathbb{E}[(y_i - \\mu)^2]$ is the definition of the variance $\\sigma^2$.\nThe second term involves the variance of the sample mean, $\\mathrm{Var}(\\hat{\\mu})$. The estimator $\\hat{\\mu}$ is unbiased, so $\\mathbb{E}[\\hat{\\mu}] = \\mu$. Thus, $\\mathbb{E}[(\\hat{\\mu} - \\mu)^2] = \\mathrm{Var}(\\hat{\\mu})$.\n$$\\mathrm{Var}(\\hat{\\mu}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\mathrm{Var}(y_i) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\sigma^2 = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$$\nSubstituting these results back:\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = n\\sigma^2 - n\\left(\\frac{\\sigma^2}{n}\\right) = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2$$\nFinally, we compute $\\mathbb{E}[\\hat{\\sigma}^2]$:\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\frac{1}{n}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n}(n-1)\\sigma^2 = \\frac{n-1}{n}\\sigma^2$$\nThe bias is then:\n$$\\mathrm{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\nThe MLE for the variance, $\\hat{\\sigma}^2$, is a biased estimator, as it systematically underestimates the true variance $\\sigma^2$ by a factor of $\\frac{n-1}{n}$.\n\n**Step 3: Bessel's Correction**\n\nThe bias arises because the sum of squared deviations is calculated around the sample mean $\\hat{\\mu}$, which is itself derived from the data, rather than the true mean $\\mu$. Since the sample mean minimizes the sum of squared deviations for a given sample, $\\sum (y_i - \\hat{\\mu})^2$ is always less than or equal to $\\sum (y_i - c)^2$ for any other constant $c$, including $\\mu$. This leads to the underestimation.\n\nThe unbiased sample variance, commonly denoted as $S^2$, incorporates Bessel's correction by replacing the denominator $n$ with $n-1$:\n$$S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$$\nThe expectation of this corrected estimator is:\n$$\\mathbb{E}[S^2] = \\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right]$$\nUsing our previous result that $\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = (n-1)\\sigma^2$:\n$$\\mathbb{E}[S^2] = \\frac{1}{n-1}(n-1)\\sigma^2 = \\sigma^2$$\nThus, $\\mathrm{Bias}(S^2) = \\mathbb{E}[S^2] - \\sigma^2 = 0$. The correction factor $\\frac{n}{n-1}$ applied to the biased MLE $\\hat{\\sigma}^2$ perfectly cancels the bias, resulting in an unbiased estimator. The denominator $n-1$ is interpreted as the degrees of freedom of the sum of squares, accounting for the one degree of freedom lost to the estimation of the sample mean.",
            "answer": "$$\\boxed{-\\frac{\\sigma^2}{n}}$$"
        },
        {
            "introduction": "Many processes in neuroscience involve hidden states that cannot be directly observed, from discrete cognitive states to cell-type identities within a population. This challenge introduces the Expectation-Maximization (EM) algorithm, an iterative method for finding maximum likelihood estimates in latent variable models . By working through the E- and M-steps for a Gaussian Mixture Model, you will learn a powerful and widely applicable technique for uncovering hidden structure in complex biological data.",
            "id": "3899853",
            "problem": "A clinical laboratory is developing a probabilistic classifier to distinguish between two physiological states based on a continuous biomarker measured in serum, such as a cardiac troponin concentration. To account for heterogeneity in the patient population, the biomarker value for patient $i$, denoted $x_i$, is modeled as being drawn independently from a $2$-component Gaussian mixture: with probability $\\pi$ the patient belongs to state $1$ (e.g., acute injury) and $x_i$ is generated from a Gaussian distribution with mean $\\mu_1$ and known variance $\\sigma_1^2$, and with probability $1-\\pi$ the patient belongs to state $2$ (e.g., baseline) and $x_i$ is generated from a Gaussian distribution with mean $\\mu_2$ and known variance $\\sigma_2^2$. Formally, for each $i \\in \\{1,\\dots,n\\}$,\n$$\nx_i \\sim \\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2),\n$$\nwhere $\\mathcal{N}(x \\mid \\mu, \\sigma^2)$ denotes the Gaussian probability density function with mean $\\mu$ and variance $\\sigma^2$, and $\\sigma_1^2$ and $\\sigma_2^2$ are known, positive constants determined by prior assay validation studies. The unknown parameters are the mixing proportion $\\pi \\in (0,1)$ and the component means $\\mu_1$ and $\\mu_2$.\n\nUsing only the independence of samples, the definition of the mixture model, Bayes' rule, and the principle of Maximum Likelihood Estimation (MLE), derive the Expectation-Maximization (EM) algorithm’s responsibilities for component $1$ and the maximization-step updates for $\\pi$, $\\mu_1$, and $\\mu_2$. Specifically, let $\\gamma_{i1}$ denote the responsibility that component $1$ takes for datum $x_i$ in the expectation-step, and let $\\pi^{\\text{new}}$, $\\mu_1^{\\text{new}}$, and $\\mu_2^{\\text{new}}$ denote the updated parameter estimates in the maximization-step. Express your final answer as closed-form analytic expressions in terms of $\\{x_i\\}_{i=1}^n$, $\\pi$, $\\mu_1$, $\\mu_2$, $\\sigma_1^2$, and $\\sigma_2^2$.\n\nYour final answer must be a single row matrix containing, in order, the expressions for $\\gamma_{i1}$, $\\pi^{\\text{new}}$, $\\mu_1^{\\text{new}}$, and $\\mu_2^{\\text{new}}$. No numerical evaluation or rounding is required.",
            "solution": "The Expectation-Maximization (EM) algorithm is an iterative method for finding maximum likelihood estimates of parameters in statistical models that depend on unobserved latent variables. The problem at hand fits this framework perfectly. The observed data are the biomarker values $\\{x_i\\}_{i=1}^n$. The latent variables are the unknown component memberships for each observation.\n\nLet us define a latent variable $z_i$ for each observation $x_i$. We can represent $z_i$ as a binary indicator, where $z_{i1} = 1$ if $x_i$ is drawn from component $1$ and $z_{i1} = 0$ otherwise. Correspondingly, $z_{i2} = 1 - z_{i1}$, indicating membership in component $2$. The parameters to be estimated are $\\theta = \\{\\pi, \\mu_1, \\mu_2\\}$. The complete data set is $(X, Z) = (\\{x_i\\}_{i=1}^n, \\{z_i\\}_{i=1}^n)$.\n\nThe joint probability of the complete data $(x_i, z_i)$ for a single observation is given by:\n$$\np(x_i, z_i \\mid \\theta) = p(x_i \\mid z_i, \\theta) p(z_i \\mid \\theta)\n$$\nGiven the binary representation of $z_i$, we can write this as:\n$$\np(x_i, z_i \\mid \\theta) = \\left[ \\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right]^{z_{i1}} \\left[ (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right]^{z_{i2}}\n$$\nAssuming the observations are independent and identically distributed (i.i.d.), the complete-data likelihood is the product of the individual probabilities:\n$$\nL_c(\\theta \\mid X, Z) = p(X, Z \\mid \\theta) = \\prod_{i=1}^n p(x_i, z_i \\mid \\theta) = \\prod_{i=1}^n \\left[ \\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right]^{z_{i1}} \\left[ (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right]^{z_{i2}}\n$$\nThe complete-data log-likelihood, $\\ell_c(\\theta) = \\ln L_c(\\theta \\mid X, Z)$, is:\n$$\n\\ell_c(\\theta) = \\sum_{i=1}^n \\left[ z_{i1} \\left( \\ln\\pi + \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right) + z_{i2} \\left( \\ln(1-\\pi) + \\ln\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right) \\right]\n$$\nThe EM algorithm consists of two steps: the Expectation (E) step and the Maximization (M) step.\n\n**E-Step: Compute the responsibilities**\n\nIn the E-step, we compute the expectation of the complete-data log-likelihood with respect to the posterior distribution of the latent variables $Z$, given the observed data $X$ and the current parameter estimates $\\theta^{\\text{old}} = \\{\\pi^{\\text{old}}, \\mu_1^{\\text{old}}, \\mu_2^{\\text{old}}\\}$. This forms the Q-function:\n$$\nQ(\\theta \\mid \\theta^{\\text{old}}) = E_{Z \\mid X, \\theta^{\\text{old}}}[\\ell_c(\\theta)]\n$$\nDue to the linearity of expectation, we only need to compute the expectation of the latent variables $z_{ik}$. This expectation is the posterior probability that observation $x_i$ belongs to component $k$, given $x_i$ and the current parameters. This posterior probability is called the \"responsibility\" of component $k$ for observation $i$. Let $\\gamma_{ik} = E[z_{ik} \\mid x_i, \\theta^{\\text{old}}]$.\n\nThe problem asks for $\\gamma_{i1}$, the responsibility of component $1$ for datum $x_i$. Using Bayes' rule, and dropping the 'old' superscript for notational clarity as per the problem's request for the final expression format:\n$$\n\\gamma_{i1} = p(z_{i1}=1 \\mid x_i, \\theta) = \\frac{p(x_i \\mid z_{i1}=1, \\theta) p(z_{i1}=1 \\mid \\theta)}{p(x_i \\mid \\theta)}\n$$\nThe denominator is the marginal probability of $x_i$, which is the sum over both components:\n$$\np(x_i \\mid \\theta) = \\sum_{k=1}^2 p(x_i \\mid z_{ik}=1, \\theta) p(z_{ik}=1 \\mid \\theta) = \\pi \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)\n$$\nThe numerator is the term for component $1$: $\\pi \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)$.\nThus, the responsibility $\\gamma_{i1}$ is:\n$$\n\\gamma_{i1} = \\frac{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)}{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)}\n$$\nBy symmetry, the responsibility of component $2$ is $\\gamma_{i2} = 1 - \\gamma_{i1}$.\nSubstituting the expectations of $z_{ik}$ with the responsibilities $\\gamma_{ik}$, the Q-function becomes:\n$$\nQ(\\theta \\mid \\theta^{\\text{old}}) = \\sum_{i=1}^n \\left[ \\gamma_{i1} \\left( \\ln\\pi + \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right) + \\gamma_{i2} \\left( \\ln(1-\\pi) + \\ln\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right) \\right]\n$$\n\n**M-Step: Maximize the Q-function**\n\nIn the M-step, we find the new parameter estimates $\\theta^{\\text{new}}$ that maximize the Q-function. We do this by taking the partial derivatives of $Q(\\theta \\mid \\theta^{\\text{old}})$ with respect to each parameter in $\\theta = \\{\\pi, \\mu_1, \\mu_2\\}$ and setting them to zero.\n\n1.  **Update for $\\pi$:**\n    We maximize the terms in $Q$ that depend on $\\pi$: $\\sum_{i=1}^n \\left[ \\gamma_{i1}\\ln\\pi + \\gamma_{i2}\\ln(1-\\pi) \\right]$. Taking the derivative with respect to $\\pi$ and setting it to $0$:\n    $$\n    \\frac{\\partial Q}{\\partial \\pi} = \\sum_{i=1}^n \\left( \\frac{\\gamma_{i1}}{\\pi} - \\frac{\\gamma_{i2}}{1-\\pi} \\right) = \\frac{1}{\\pi}\\sum_{i=1}^n \\gamma_{i1} - \\frac{1}{1-\\pi}\\sum_{i=1}^n \\gamma_{i2} = 0\n    $$\n    Let $N_1 = \\sum_{i=1}^n \\gamma_{i1}$ and $N_2 = \\sum_{i=1}^n \\gamma_{i2}$. Note that $N_1 + N_2 = \\sum_i(\\gamma_{i1}+\\gamma_{i2}) = \\sum_i 1 = n$.\n    $$\n    \\frac{N_1}{\\pi} = \\frac{N_2}{1-\\pi} \\implies N_1(1-\\pi) = N_2\\pi \\implies N_1 = (N_1+N_2)\\pi = n\\pi\n    $$\n    Solving for $\\pi$ gives the updated estimate $\\pi^{\\text{new}}$:\n    $$\n    \\pi^{\\text{new}} = \\frac{N_1}{n} = \\frac{1}{n}\\sum_{i=1}^n \\gamma_{i1}\n    $$\n\n2.  **Update for $\\mu_1$:**\n    We maximize the terms in $Q$ that depend on $\\mu_1$: $\\sum_{i=1}^n \\gamma_{i1} \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)$.\n    $$\n    \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) = -\\frac{1}{2}\\ln(2\\pi\\sigma_1^2) - \\frac{(x_i - \\mu_1)^2}{2\\sigma_1^2}\n    $$\n    Taking the derivative with respect to $\\mu_1$ and setting it to $0$:\n    $$\n    \\frac{\\partial Q}{\\partial \\mu_1} = \\frac{\\partial}{\\partial \\mu_1} \\sum_{i=1}^n \\gamma_{i1} \\left( -\\frac{(x_i - \\mu_1)^2}{2\\sigma_1^2} \\right) = \\sum_{i=1}^n \\gamma_{i1} \\left( \\frac{x_i - \\mu_1}{\\sigma_1^2} \\right) = 0\n    $$\n    Since $\\sigma_1^2 > 0$, we have $\\sum_{i=1}^n \\gamma_{i1} (x_i - \\mu_1) = 0$, which implies:\n    $$\n    \\sum_{i=1}^n \\gamma_{i1}x_i = \\mu_1 \\sum_{i=1}^n \\gamma_{i1}\n    $$\n    Solving for $\\mu_1$ gives the updated estimate $\\mu_1^{\\text{new}}$:\n    $$\n    \\mu_1^{\\text{new}} = \\frac{\\sum_{i=1}^n \\gamma_{i1} x_i}{\\sum_{i=1}^n \\gamma_{i1}}\n    $$\n\n3.  **Update for $\\mu_2$:**\n    The derivation is symmetric to that for $\\mu_1$. We maximize the terms in $Q$ dependent on $\\mu_2$: $\\sum_{i=1}^n \\gamma_{i2} \\ln\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)$.\n    $$\n    \\frac{\\partial Q}{\\partial \\mu_2} = \\frac{\\partial}{\\partial \\mu_2} \\sum_{i=1}^n \\gamma_{i2} \\left( -\\frac{(x_i - \\mu_2)^2}{2\\sigma_2^2} \\right) = \\sum_{i=1}^n \\gamma_{i2} \\left( \\frac{x_i - \\mu_2}{\\sigma_2^2} \\right) = 0\n    $$\n    This implies $\\sum_{i=1}^n \\gamma_{i2} (x_i - \\mu_2) = 0$, leading to:\n    $$\n    \\mu_2^{\\text{new}} = \\frac{\\sum_{i=1}^n \\gamma_{i2} x_i}{\\sum_{i=1}^n \\gamma_{i2}}\n    $$\n    Substituting $\\gamma_{i2} = 1 - \\gamma_{i1}$, we obtain the expression in terms of $\\gamma_{i1}$:\n    $$\n    \\mu_2^{\\text{new}} = \\frac{\\sum_{i=1}^n (1 - \\gamma_{i1}) x_i}{\\sum_{i=1}^n (1 - \\gamma_{i1})}\n    $$\n\nThese expressions for $\\gamma_{i1}$, $\\pi^{\\text{new}}$, $\\mu_1^{\\text{new}}$, and $\\mu_2^{\\text{new}}$ constitute one full iteration of the EM algorithm. The algorithm proceeds by calculating the responsibilities in the E-step using the current parameters, and then using these responsibilities to compute updated parameters in the M-step. This process is repeated until convergence.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)}{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)} & \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{i1} & \\frac{\\sum_{i=1}^{n} \\gamma_{i1} x_i}{\\sum_{i=1}^{n} \\gamma_{i1}} & \\frac{\\sum_{i=1}^{n} (1-\\gamma_{i1}) x_i}{\\sum_{i=1}^{n} (1-\\gamma_{i1})} \\end{pmatrix}}\n$$"
        }
    ]
}