{
    "hands_on_practices": [
        {
            "introduction": "我们的第一个练习将通过神经科学中的一个基本模型——高斯分布，来巩固我们对最大似然估计（MLE）的理解。通过推导局部场电位（LFP）振幅的均值和方差的估计量，我们不仅将应用最大似然估计的核心机制，还将揭示一个关键的细微之处——方差的最大似然估计量中固有的偏差及其修正方法。这项练习能够提升我们的分析能力，并强调批判性地评估我们所用估计量属性的重要性。",
            "id": "4177490",
            "problem": "一个实验室在持续性行为任务中记录了海马区的细胞外局部场电位 (LFP) 信号。在进行带通滤波以分离特定的振荡频带后，分析人员提取了 $n$ 个短的、不重叠的时间窗口，并用振荡固定相位下的瞬时振幅 $y_i$ 来概括每个窗口，其中 $i=1,\\dots,n$。在该频带中聚合突触输入的标准生成模型下，假设振幅是独立同分布的，即 $y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$，其中 $\\mu$ 和 $\\sigma^2$ 分别表示带限振幅的未知均值和方差。\n\n仅从高斯分布的概率密度函数和独立性假设出发，使用最大似然原理推导 $\\mu$ 和 $\\sigma^2$ 的估计量。然后，在真实模型下将估计量视为随机变量，分析方差估计量的期望以量化其偏差。最后，将此偏差与使用 Bessel 校正的无偏方差估计量进行对比。\n\n你的任务：\n- 基于样本 $\\{y_i\\}_{i=1}^{n}$ 推导 $\\mu$ 和 $\\sigma^2$ 的最大似然估计量 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^2$。\n- 在真实模型下计算 $\\mathbb{E}[\\hat{\\sigma}^2]$，并得到偏差 $\\mathrm{Bias}(\\hat{\\sigma}^2)=\\mathbb{E}[\\hat{\\sigma}^2]-\\sigma^2$ 的一个以 $n$ 和 $\\sigma^2$ 表示的封闭形式解析表达式。\n- 简要解释将分母 $n$ 替换为 $n-1$ 的无偏估计量是如何消除此偏差的。\n\n最终答案只需提供 $\\mathrm{Bias}(\\hat{\\sigma}^2)$ 的封闭形式。无需四舍五入，无需单位。",
            "solution": "该问题要求推导高斯分布参数的最大似然估计量，然后分析方差估计量的偏差。\n\n数据由一组 $n$ 个独立同分布 (i.i.d.) 的观测值 $\\{y_i\\}_{i=1}^{n}$ 组成，其中每个 $y_i$ 都从均值 $\\mu$ 和方差 $\\sigma^2$ 未知的高斯（正态）分布中抽取。单个观测值 $y_i$ 的概率密度函数 (PDF) 由下式给出：\n$$f(y_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)$$\n\n**第1步：推导最大似然估计量 (MLEs)**\n\n由于 i.i.d. 假设，整个样本的似然函数 $L(\\mu, \\sigma^2)$ 是各个 PDF 的乘积：\n$$L(\\mu, \\sigma^2 | \\{y_i\\}) = \\prod_{i=1}^{n} f(y_i | \\mu, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right)$$\n为了找到使该函数最大化的参数，计算上更简单的是最大化对数似然函数 $\\ell(\\mu, \\sigma^2) = \\ln(L(\\mu, \\sigma^2))$：\n$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\nMLEs $\\hat{\\mu}$ 和 $\\hat{\\sigma}^2$ 是使 $\\ell$ 最大化的 $\\mu$ 和 $\\sigma^2$ 的值。我们通过将 $\\ell$ 对 $\\mu$ 和 $\\sigma^2$ 的偏导数设为零来找到它们。\n\n首先，我们对 $\\mu$ 求导：\n$$\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} 2(y_i - \\mu)(-1) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)$$\n将导数设为零，求解估计量 $\\hat{\\mu}$：\n$$\\frac{1}{\\hat{\\sigma}^2}\\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0 \\implies \\sum_{i=1}^{n} y_i - n\\hat{\\mu} = 0 \\implies \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$$\n均值的 MLE，$\\hat{\\mu}$，是样本均值。\n\n接下来，我们对 $\\sigma^2$ 求导。为方便起见，令 $\\theta = \\sigma^2$。\n$$\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( -\\frac{n}{2}\\ln(\\theta) - \\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) = -\\frac{n}{2\\theta} + \\frac{1}{2\\theta^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\n将导数设为零，并代入估计量 $\\hat{\\theta} = \\hat{\\sigma}^2$ 和 $\\hat{\\mu}$：\n$$-\\frac{n}{2\\hat{\\sigma}^2} + \\frac{1}{2(\\hat{\\sigma}^2)^2}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0$$\n乘以 $2(\\hat{\\sigma}^2)^2$ 得：\n$$-n\\hat{\\sigma}^2 + \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$$\n方差的 MLE 是 $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$。\n\n**第2步：方差估计量 $\\hat{\\sigma}^2$ 的偏差**\n\n估计量的偏差是其期望值与被估计参数的真实值之间的差。我们需要计算 $\\mathrm{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2$。首先，我们计算期望 $\\mathbb{E}[\\hat{\\sigma}^2]$。\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right]$$\n我们通过加上和减去真实均值 $\\mu$ 来重写求和内的项：\n$$y_i - \\hat{\\mu} = (y_i - \\mu) - (\\hat{\\mu} - \\mu)$$\n平方和变为：\n$$\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = \\sum_{i=1}^{n} [(y_i - \\mu) - (\\hat{\\mu} - \\mu)]^2 = \\sum_{i=1}^{n} \\left[ (y_i - \\mu)^2 - 2(y_i - \\mu)(\\hat{\\mu} - \\mu) + (\\hat{\\mu} - \\mu)^2 \\right]$$\n$$= \\sum_{i=1}^{n}(y_i - \\mu)^2 - 2(\\hat{\\mu} - \\mu)\\sum_{i=1}^{n}(y_i - \\mu) + n(\\hat{\\mu} - \\mu)^2$$\n认识到 $\\sum_{i=1}^{n}(y_i - \\mu) = n(\\frac{1}{n}\\sum y_i - \\mu) = n(\\hat{\\mu} - \\mu)$，我们将其代入表达式中：\n$$\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = \\sum_{i=1}^{n}(y_i - \\mu)^2 - 2(\\hat{\\mu} - \\mu)n(\\hat{\\mu} - \\mu) + n(\\hat{\\mu} - \\mu)^2 = \\sum_{i=1}^{n}(y_i - \\mu)^2 - n(\\hat{\\mu} - \\mu)^2$$\n现在我们利用期望的线性性质来求该表达式的期望：\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n}(y_i - \\mu)^2\\right] - n\\mathbb{E}\\left[(\\hat{\\mu} - \\mu)^2\\right]$$\n第一项是 $\\mathbb{E}\\left[\\sum_{i=1}^{n}(y_i - \\mu)^2\\right] = \\sum_{i=1}^{n}\\mathbb{E}[(y_i - \\mu)^2] = \\sum_{i=1}^{n}\\sigma^2 = n\\sigma^2$，因为 $\\mathbb{E}[(y_i - \\mu)^2]$ 是方差 $\\sigma^2$ 的定义。\n第二项涉及样本均值的方差 $\\mathrm{Var}(\\hat{\\mu})$。估计量 $\\hat{\\mu}$ 是无偏的，所以 $\\mathbb{E}[\\hat{\\mu}] = \\mu$。因此，$\\mathbb{E}[(\\hat{\\mu} - \\mu)^2] = \\mathrm{Var}(\\hat{\\mu})$。\n$$\\mathrm{Var}(\\hat{\\mu}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\mathrm{Var}(y_i) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\sigma^2 = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$$\n将这些结果代回：\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = n\\sigma^2 - n\\left(\\frac{\\sigma^2}{n}\\right) = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2$$\n最后，我们计算 $\\mathbb{E}[\\hat{\\sigma}^2]$：\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\frac{1}{n}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n}(n-1)\\sigma^2 = \\frac{n-1}{n}\\sigma^2$$\n那么偏差是：\n$$\\mathrm{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\n方差的 MLE，$\\hat{\\sigma}^2$，是一个有偏估计量，因为它系统地以 $\\frac{n-1}{n}$ 的因子低估了真实方差 $\\sigma^2$。\n\n**第3步：Bessel 校正**\n\n偏差的产生是因为平方偏差和是围绕样本均值 $\\hat{\\mu}$ 计算的，而样本均值本身是从数据中导出的，而不是真实均值 $\\mu$。由于样本均值使给定样本的平方偏差和最小化，对于任何其他常数 $c$（包括 $\\mu$），$\\sum (y_i - \\hat{\\mu})^2$ 总是小于或等于 $\\sum (y_i - c)^2$。这导致了低估。\n\n无偏样本方差，通常表示为 $S^2$，通过将分母 $n$ 替换为 $n-1$ 来包含 Bessel 校正：\n$$S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$$\n这个校正后估计量的期望是：\n$$\\mathbb{E}[S^2] = \\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right]$$\n使用我们之前的结果 $\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = (n-1)\\sigma^2$：\n$$\\mathbb{E}[S^2] = \\frac{1}{n-1}(n-1)\\sigma^2 = \\sigma^2$$\n因此，$\\mathrm{Bias}(S^2) = \\mathbb{E}[S^2] - \\sigma^2 = 0$。应用于有偏 MLE $\\hat{\\sigma}^2$ 的校正因子 $\\frac{n}{n-1}$ 完美地抵消了偏差，从而得到一个无偏估计量。分母 $n-1$ 被解释为平方和的自由度，说明了因估计样本均值而损失的一个自由度。",
            "answer": "$$\\boxed{-\\frac{\\sigma^2}{n}}$$"
        },
        {
            "introduction": "在直接参数估计的基础上，本问题在一个更复杂的框架——广义线性模型（GLM）中探讨最大似然估计的行为，该模型常用于神经脉冲序列分析。我们将研究一个被称为“完全分离”的关键病态案例，即预测变量完美地解释了神经元的响应。这种情况揭示了无惩罚最大似然估计的局限性，并引入了正则化作为一种有原则的解决方案，这是现代神经科学家工具箱中的一个重要工具。",
            "id": "4177431",
            "problem": "在脉冲序列分析中，您将时间分箱成小的时间窗口，并记录一个二元响应 $y_i \\in \\{0,1\\}$，指示在时间窗口 $i$ 中是否发生了脉冲。您使用带有逻辑斯谛链接的伯努利广义线性模型 (GLM) 来建模条件脉冲概率，即 $y_i \\mid \\mathbf{x}_i \\sim \\text{Bernoulli}(p_i)$，其中 $p_i = \\sigma(\\eta_i)$ 且 $\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$，$\\sigma(z) = \\dfrac{1}{1+e^{-z}}$。假设设计包含一个截距项和一个二元协变量 $x_i \\in \\{0,1\\}$，指示是否施加了刺激。根据经验，您观察到每当 $x_i = 1$ 时，总会有一个脉冲 ($y_i=1$)；而每当 $x_i = 0$ 时，从不产生脉冲 ($y_i=0$)。假设 $x_i=1$ 和 $x_i=0$ 的行都至少出现一次。考虑该模型下 $\\boldsymbol{\\beta} = (\\beta_0,\\beta_1)^{\\top}$ 的无惩罚最大似然估计 (MLE)，以及在负对数似然上增加一个强度为 $\\lambda  0$ 的 $\\ell_2$ 惩罚项的惩罚似然（等价于对 $\\boldsymbol{\\beta}$ 施加一个方差与 $\\lambda^{-1}$ 成正比的零均值高斯先验）。\n\n从伯努利对数似然 $\\ell(\\boldsymbol{\\beta}) = \\sum_i \\big( y_i \\log p_i + (1-y_i)\\log(1-p_i) \\big)$ 和逻辑斯谛链接的定义出发，论述在所述数据配置下 $\\beta_0$ 和 $\\beta_1$ 的可辨识性、无惩罚 MLE 的存在性与行为、Fisher 信息的行为，以及 $\\ell_2$ 正则化对估计量存在性和唯一性的影响。\n\n选择所有正确的陈述。\n\nA. 在这些数据下，无惩罚的 MLE 不作为有限向量存在；对数似然仅在 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ 的极限情况下，沿着使每当 $y_i=1$ 时 $p_i \\to 1$ 且每当 $y_i=0$ 时 $p_i \\to 0$ 的方向上达到最大化，其对数似然的上确界等于 $0$。\n\nB. 无惩罚的 MLE 存在且唯一，但具有非常大的标准误；得分函数的发散是一种数值假象，会随着更多次迭代而消失。\n\nC. 无惩罚的对数似然有多个不同的有限最大化子，对应于不同的分离超平面，并且在任何最大化子处的 Hessian 矩阵（观测信息）都是负定的，因此标准的渐近正态推断适用。\n\nD. 对协变量 $x_i$ 进行仿射重缩放（中心化和缩放）可以恢复一个有限的无惩罚 MLE，而不会改变拟合概率，因为似然函数在此类变换下是不变的。\n\nE. 在负对数似然上增加一个 $\\ell_2$ 惩罚项会产生一个严格凸的惩罚目标函数，该函数具有唯一的有限最小化子；由此产生的估计量将原本发散的系数向 $0$ 收缩，并解决了由分离引起的最大似然估计不存在的问题。",
            "solution": "用户需要对问题陈述进行批判性验证，然后是完整的推导和逐个选项的分析。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n-   **模型**：带有逻辑斯谛链接函数的伯努利广义线性模型 (GLM)。\n-   **响应变量**：$y_i \\in \\{0,1\\}$，指示在时间窗口 $i$ 中是否发生脉冲。\n-   **条件概率**：$y_i \\mid \\mathbf{x}_i \\sim \\text{Bernoulli}(p_i)$。\n-   **链接函数**：$p_i = \\sigma(\\eta_i)$，其中 $\\sigma(z) = \\dfrac{1}{1+e^{-z}}$。\n-   **线性预测器**：$\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$。\n-   **设计**：模型包含一个截距项和一个二元协变量 $x_i \\in \\{0,1\\}$。设计向量为 $\\mathbf{x}_i = (1, x_i)^{\\top}$，参数向量为 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)^{\\top}$。因此，$\\eta_i = \\beta_0 + x_i\\beta_1$。\n-   **数据配置**：\n    -   如果 $x_i = 1$，则 $y_i = 1$（总是如此）。\n    -   如果 $x_i = 0$，则 $y_i = 0$（总是如此）。\n-   **数据假设**：两种情况（$x_i=1$ 和 $x_i=0$）在数据集中都至少出现一次。\n-   **估计方法**：\n    1.  无惩罚的最大似然估计 (MLE)。\n    2.  在负对数似然上增加一个强度为 $\\lambda  0$ 的 $\\ell_2$ 惩罚项的惩罚似然。\n-   **对数似然函数**：$\\ell(\\boldsymbol{\\beta}) = \\sum_i \\big( y_i \\log p_i + (1-y_i)\\log(1-p_i) \\big)$。\n\n**步骤2：使用提取的已知条件进行验证**\n\n-   **科学依据**：该问题描述了逻辑斯谛回归，这是一种标准且基础的统计方法。它在计算神经科学中常用于分箱后的脉冲序列数据。所描述的数据配置是一种众所周知统计现象，称为“完全分离”或“完美预测”。所要求的分析涉及在这种条件下估计量的数学性质。该问题在统计学及其在神经科学中的应用方面有坚实的基础。\n-   **适定性**：该问题是适定的。它提出了清晰的数据结构和特定的模型，并询问相应估计量的存在性、唯一性和行为。这些问题是精确的，并有明确的数学答案。\n-   **客观性**：该问题使用客观和精确的数学语言陈述。没有主观或基于观点的断言。\n\n**步骤3：结论和行动**\n\n问题陈述是有效的。它科学合理、适定且客观。它描述了逻辑斯谛回归中的一个经典场景，并就统计估计量的性质提出了相关问题。我现在将继续进行解题推导和选项分析。\n\n### 推导与求解\n\n该问题描述了一个**完全分离**的场景，其中预测变量的线性组合完美地分开了两个结果类别。在这里，预测变量 $x_i$ 本身就完美地分开了结果 $y_i=0$ 和 $y_i=1$。\n\n**1. 无惩罚 MLE 的分析**\n\n对数似然函数由下式给出：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_i \\left[ y_i \\log p_i + (1-y_i)\\log(1-p_i) \\right] $$\n设 $I_0 = \\{i \\mid x_i=0\\}$ 和 $I_1 = \\{i \\mid x_i=1\\}$。设 $n_0 = |I_0|$ 和 $n_1 = |I_1|$。根据假设，$n_0 \\ge 1$ 且 $n_1 \\ge 1$。\n\n根据数据配置：\n-   对于 $i \\in I_0$，我们有 $x_i=0$ 和 $y_i=0$。线性预测器为 $\\eta_i = \\beta_0$，所以 $p_i = \\sigma(\\beta_0)$。\n-   对于 $i \\in I_1$，我们有 $x_i=1$ 和 $y_i=1$。线性预测器为 $\\eta_i = \\beta_0 + \\beta_1$，所以 $p_i = \\sigma(\\beta_0 + \\beta_1)$。\n\n将此代入对数似然表达式：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i \\in I_0} \\left[ 0 \\cdot \\log p_i + (1-0)\\log(1-p_i) \\right] + \\sum_{i \\in I_1} \\left[ 1 \\cdot \\log p_i + (1-1)\\log(1-p_i) \\right] $$\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i \\in I_0} \\log(1 - \\sigma(\\beta_0)) + \\sum_{i \\in I_1} \\log(\\sigma(\\beta_0 + \\beta_1)) $$\n$$ \\ell(\\boldsymbol{\\beta}) = n_0 \\log(1 - \\sigma(\\beta_0)) + n_1 \\log(\\sigma(\\beta_0 + \\beta_1)) $$\n\n为了最大化 $\\ell(\\boldsymbol{\\beta})$，我们必须同时最大化两项。对于 $u \\in (0,1)$，函数 $\\log(u)$ 在 $u \\to 1$ 时最大化，此时 $\\log(u) \\to 0$。\n-   为了最大化第一项，我们需要 $1 - \\sigma(\\beta_0) \\to 1$，这意味着 $\\sigma(\\beta_0) \\to 0$。这在 $\\beta_0 \\to -\\infty$ 时发生。\n-   为了最大化第二项，我们需要 $\\sigma(\\beta_0 + \\beta_1) \\to 1$。这在 $\\beta_0 + \\beta_1 \\to +\\infty$ 时发生。\n\n这两个条件可以同时满足。例如，考虑路径 $\\beta_0 = -c$ 和 $\\beta_1 = 2c$，对于某个常数 $c  0$。那么 $\\beta_0 + \\beta_1 = c$。当 $c \\to \\infty$ 时，我们有 $\\beta_0 \\to -\\infty$ 和 $\\beta_0 + \\beta_1 \\to +\\infty$。沿着这条路径，$\\|\\boldsymbol{\\beta}\\| = \\sqrt{(-c)^2 + (2c)^2} = c\\sqrt{5} \\to \\infty$。\n\n对数似然的值趋近于：\n$$ \\lim_{\\substack{\\beta_0 \\to -\\infty \\\\ \\beta_0+\\beta_1 \\to +\\infty}} \\ell(\\boldsymbol{\\beta}) = n_0 \\log(1) + n_1 \\log(1) = 0 $$\n对数似然的上确界是 $0$。然而，对于任何有限向量 $\\boldsymbol{\\beta}$，这个值都永远无法达到，因为对于任何有限的 $z$，$\\sigma(z)$ 都严格介于 $0$ 和 $1$ 之间。因此，无惩罚的最大似然估计 (MLE) 不作为有限向量存在。似然函数仅在参数向量 $\\boldsymbol{\\beta}$ 的范数趋于无穷大的极限情况下才被最大化。\n\nFisher 信息矩阵是 $\\mathcal{I}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$，其中 $\\mathbf{W}$ 是一个对角矩阵，其元素为 $W_{ii} = p_i(1-p_i)$。随着参数发散，概率 $p_i$ 趋近于 $0$ 或 $1$。在任何一种情况下，$p_i(1-p_i) \\to 0$。因此，$\\mathbf{W} \\to \\mathbf{0}$，Fisher 信息矩阵变为奇异矩阵，这与估计量的无限方差是一致的。\n\n**2. 惩罚 MLE 的分析**\n\n问题要求考虑在负对数似然上增加一个 $\\ell_2$ 惩罚项。要最小化的目标函数是：\n$$ J(\\boldsymbol{\\beta}) = - \\ell(\\boldsymbol{\\beta}) + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2 \\quad \\text{for } \\lambda  0 $$\n等价地，我们可以最大化惩罚对数似然：\n$$ \\ell_p(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) - \\lambda \\|\\boldsymbol{\\beta}\\|_2^2 $$\n我们来分析 $\\ell_p(\\boldsymbol{\\beta})$ 的 Hessian 矩阵。无惩罚对数似然的 Hessian 矩阵是 $\\mathbf{H}(\\boldsymbol{\\beta}) = -\\sum_i p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^\\top$。该矩阵是负半定的。惩罚项 $-\\lambda \\|\\boldsymbol{\\beta}\\|_2^2 = -\\lambda(\\beta_0^2 + \\beta_1^2)$ 的 Hessian 矩阵是 $-2\\lambda \\mathbf{I}$，其中 $\\mathbf{I}$ 是 $2 \\times 2$ 的单位矩阵。对于 $\\lambda  0$，这是一个负定矩阵。\n惩罚对数似然的 Hessian 矩阵是：\n$$ \\mathbf{H}_p(\\boldsymbol{\\beta}) = \\mathbf{H}(\\boldsymbol{\\beta}) - 2\\lambda \\mathbf{I} = -\\left( \\sum_i p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^\\top + 2\\lambda \\mathbf{I} \\right) $$\n矩阵 $\\sum_i p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^\\top$ 是正半定的，而 $2\\lambda \\mathbf{I}$ 是正定的。它们的和是正定的。因此，对于所有的 $\\boldsymbol{\\beta}$，$\\mathbf{H}_p(\\boldsymbol{\\beta})$ 都是负定的。这证明了 $\\ell_p(\\boldsymbol{\\beta})$ 是一个严格凹函数。\n\n一个严格凹函数最多只有一个最大化子。为了证明存在一个有限的最大化子，我们观察到当 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ 时，惩罚项 $-\\lambda \\|\\boldsymbol{\\beta}\\|_2^2 \\to -\\infty$。由于无惩罚的对数似然 $\\ell(\\boldsymbol{\\beta})$ 的上界为 $0$，当 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ 时，惩罚对数似然 $\\ell_p(\\boldsymbol{\\beta}) \\to -\\infty$。这种矫顽性保证了全局最大值必须在 $\\boldsymbol{\\beta}$ 的一个有限值处存在。\n因为该函数是严格凹的，所以这个有限的最大化子是唯一的。惩罚项将系数向 $0$ 收缩，防止了在无惩罚情况下出现的发散现象，从而提供了一个唯一的、稳定的解。\n\n### 选项评估\n\n**A. 在这些数据下，无惩罚的 MLE 不作为有限向量存在；对数似然仅在 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ 的极限情况下，沿着使每当 $y_i=1$ 时 $p_i \\to 1$ 且每当 $y_i=0$ 时 $p_i \\to 0$ 的方向上达到最大化，其对数似然的上确界等于 $0$。**\n该陈述精确地描述了完全分离对逻辑斯谛回归的后果。我们的推导证实了 MLE 不作为有限向量存在，对数似然通过将概率推向其观测到的二元结果（$0$ 或 $1$）来最大化，这需要 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$，并且对数似然的上确界确实是 $0$。\n**结论：正确。**\n\n**B. 无惩罚的 MLE 存在且唯一，但具有非常大的标准误；得分函数的发散是一种数值假象，会随着更多次迭代而消失。**\n这是不正确的。无惩罚的 MLE 不作为有限向量存在。参数估计发散的现象是完全分离条件下似然函数的一个基本数学性质，而不是一个数值假象。具有有限 MLE 但标准误非常大的情况可能发生在*准完全分离*中，但本问题描述的是完全分离。\n**结论：不正确。**\n\n**C. 无惩罚的对数似然有多个不同的有限最大化子，对应于不同的分离超平面，并且在任何最大化子处的 Hessian 矩阵（观测信息）都是负定的，因此标准的渐近正态推断适用。**\n这个陈述在多个方面都是不正确的。不存在*有限的*最大化子。上确界是渐近达到的。因此，没有 Hessian 矩阵可以在“任何最大化子处”进行评估。依赖于有限 MLE 和非奇异 Fisher 信息矩阵的标准渐近推断在此不适用。\n**结论：不正确。**\n\n**D. 对协变量 $x_i$ 进行仿射重缩放（中心化和缩放）可以恢复一个有限的无惩罚 MLE，而不会改变拟合概率，因为似然函数在此类变换下是不变的。**\n这是不正确的。完全分离问题是几何问题；两个类别在预测变量空间中可以被一个超平面完美分离。预测变量的仿射变换（$x'_i = ax_i+b$）是特征空间的一个可逆线性变换（如果我们包括截距项）。它不改变类别的可分性。新的参数将与旧的参数相关，但它们仍然需要发散以最大化似然。重缩放协变量不能解决 MLE 不存在的问题。\n**结论：不正确。**\n\n**E. 在负对数似然上增加一个 $\\ell_2$ 惩罚项会产生一个严格凸的惩罚目标函数，该函数具有唯一的有限最小化子；由此产生的估计量将原本发散的系数向 $0$ 收缩，并解决了由分离引起的最大似然估计不存在的问题。**\n这个陈述是正确的。如上所述，添加 $\\ell_2$ 惩罚（在贝叶斯框架中等同于零均值高斯先验）使得要最小化的目标函数（负对数似然加惩罚项）成为严格凸函数。这种严格凸性，结合函数在 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ 时的行为，保证了唯一的、有限的最小化子的存在。这种正则化技术，对于 GLM 而言被称为岭回归，通过惩罚大的参数值，有效地解决了由完全分离引起的估计量不存在问题。\n**结论：正确。**",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "我们的最后一个练习将最大似然估计的原理扩展到包含隐藏或潜变量的模型，这在生物数据中是一种常见情况。利用高斯混合模型对生物标志物数据进行分类，我们将推导期望最大化（EM）算法，这是一种在数据不完整时寻找最大似然估计的迭代方法。这项练习展示了如何巧妙地运用最大似然估计来揭示数据的内在结构，这是解决诸如细胞类型聚类或脉冲分类等问题的核心任务。",
            "id": "3899853",
            "problem": "一个临床实验室正在开发一种概率分类器，用于根据血清中测量的连续生物标志物（例如心肌肌钙蛋白浓度）来区分两种生理状态。为了解释患者群体的异质性，患者 $i$ 的生物标志物值 $x_i$ 被建模为独立地从一个双组分高斯混合模型中抽取：患者以概率 $\\pi$ 属于状态 1（例如，急性损伤），其 $x_i$ 从一个均值为 $\\mu_1$、已知方差为 $\\sigma_1^2$ 的高斯分布中生成；患者以概率 $1-\\pi$ 属于状态 2（例如，基线），其 $x_i$ 从一个均值为 $\\mu_2$、已知方差为 $\\sigma_2^2$ 的高斯分布中生成。形式上，对于每个 $i \\in \\{1,\\dots,n\\}$，\n$$\nx_i \\sim \\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2),\n$$\n其中 $\\mathcal{N}(x \\mid \\mu, \\sigma^2)$ 表示均值为 $\\mu$、方差为 $\\sigma^2$ 的高斯概率密度函数，$\\sigma_1^2$ 和 $\\sigma_2^2$ 是由先前的分析验证研究确定的已知正常数。未知参数是混合比例 $\\pi \\in (0,1)$ 和组分均值 $\\mu_1$ 和 $\\mu_2$。\n\n仅使用样本的独立性、混合模型的定义、贝叶斯法则和最大似然估计（MLE）原理，推导期望最大化（EM）算法对组分 1 的责任（responsibilities）以及最大化步骤中对 $\\pi$、$\\mu_1$ 和 $\\mu_2$ 的更新。具体来说，令 $\\gamma_{i1}$ 表示在期望步骤中组分 1 对数据点 $x_i$ 所负的责任，并令 $\\pi^{\\text{new}}$、$\\mu_1^{\\text{new}}$ 和 $\\mu_2^{\\text{new}}$ 表示在最大化步骤中更新后的参数估计值。将最终答案表示为关于 $\\{x_i\\}_{i=1}^n$、$\\pi$、$\\mu_1$、$\\mu_2$、$\\sigma_1^2$ 和 $\\sigma_2^2$ 的闭式解析表达式。\n\n你的最终答案必须是一个单行矩阵，按顺序包含 $\\gamma_{i1}$、$\\pi^{\\text{new}}$、$\\mu_1^{\\text{new}}$ 和 $\\mu_2^{\\text{new}}$ 的表达式。无需进行数值计算或四舍五入。",
            "solution": "该问题是有效的。它提出了一个标准的、适定的统计估计任务：为一个具有已知方差的双组分高斯混合模型（GMM）推导期望最大化（EM）算法的更新方程。该问题具有科学依据、是客观的，并包含了得出唯一且有意义解所需的所有信息。\n\n期望最大化（EM）算法是一种迭代方法，用于在依赖于未观测到的潜在变量的统计模型中寻找参数的最大似然估计。当前的问题完全符合这个框架。观测数据是生物标志物值 $\\{x_i\\}_{i=1}^n$。潜在变量是每个观测值的未知组分隶属关系。\n\n我们为每个观测值 $x_i$ 定义一个潜在变量 $z_i$。我们可以将 $z_i$ 表示为一个二元指示变量，如果 $x_i$ 从组分 1 中抽取，则 $z_{i1} = 1$，否则 $z_{i1} = 0$。相应地，$z_{i2} = 1 - z_{i1}$，表示隶属于组分 2。待估计的参数是 $\\theta = \\{\\pi, \\mu_1, \\mu_2\\}$。完整数据集是 $(X, Z) = (\\{x_i\\}_{i=1}^n, \\{z_i\\}_{i=1}^n)$。\n\n单个观测的完整数据 $(x_i, z_i)$ 的联合概率由下式给出：\n$$\np(x_i, z_i \\mid \\theta) = p(x_i \\mid z_i, \\theta) p(z_i \\mid \\theta)\n$$\n给定 $z_i$ 的二元表示，我们可以将其写为：\n$$\np(x_i, z_i \\mid \\theta) = \\left[ \\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right]^{z_{i1}} \\left[ (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right]^{z_{i2}}\n$$\n假设观测值是独立同分布的（i.i.d.），完整数据似然是各个概率的乘积：\n$$\nL_c(\\theta \\mid X, Z) = p(X, Z \\mid \\theta) = \\prod_{i=1}^n p(x_i, z_i \\mid \\theta) = \\prod_{i=1}^n \\left[ \\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right]^{z_{i1}} \\left[ (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right]^{z_{i2}}\n$$\n完整数据对数似然 $\\mathcal{L}_c(\\theta) = \\ln L_c(\\theta \\mid X, Z)$ 是：\n$$\n\\mathcal{L}_c(\\theta) = \\sum_{i=1}^n \\left[ z_{i1} \\left( \\ln\\pi + \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right) + z_{i2} \\left( \\ln(1-\\pi) + \\ln\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right) \\right]\n$$\nEM 算法包含两个步骤：期望（E）步骤和最大化（M）步骤。\n\n**E 步骤：计算责任**\n\n在 E 步骤中，我们在给定观测数据 $X$ 和当前参数估计 $\\theta^{\\text{old}} = \\{\\pi^{\\text{old}}, \\mu_1^{\\text{old}}, \\mu_2^{\\text{old}}\\}$ 的条件下，计算完整数据对数似然关于潜在变量 $Z$ 的后验分布的期望。这构成了 Q 函数：\n$$\nQ(\\theta \\mid \\theta^{\\text{old}}) = E_{Z \\mid X, \\theta^{\\text{old}}}[\\mathcal{L}_c(\\theta)]\n$$\n由于期望的线性性质，我们只需要计算潜在变量 $z_{ik}$ 的期望。这个期望是在给定 $x_i$ 和当前参数的条件下，观测值 $x_i$ 属于组分 $k$ 的后验概率。这个后验概率被称为组分 $k$ 对观测值 $i$ 的“责任”。令 $\\gamma_{ik} = E[z_{ik} \\mid x_i, \\theta^{\\text{old}}]$。\n\n问题要求解 $\\gamma_{i1}$，即组分 1 对数据点 $x_i$ 的责任。使用贝叶斯法则，并且根据问题对最终表达式格式的要求，为了符号简洁，省略 'old' 上标：\n$$\n\\gamma_{i1} = p(z_{i1}=1 \\mid x_i, \\theta) = \\frac{p(x_i \\mid z_{i1}=1, \\theta) p(z_{i1}=1 \\mid \\theta)}{p(x_i \\mid \\theta)}\n$$\n分母是 $x_i$ 的边际概率，即对两个组分的求和：\n$$\np(x_i \\mid \\theta) = \\sum_{k=1}^2 p(x_i \\mid z_{ik}=1, \\theta) p(z_{ik}=1 \\mid \\theta) = \\pi \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)\n$$\n分子是组分 1 的项：$\\pi \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)$。因此，责任 $\\gamma_{i1}$ 是：\n$$\n\\gamma_{i1} = \\frac{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)}{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)}\n$$\n根据对称性，组分 2 的责任是 $\\gamma_{i2} = 1 - \\gamma_{i1}$。\n用责任 $\\gamma_{ik}$ 替换 $z_{ik}$ 的期望，Q 函数变为：\n$$\nQ(\\theta \\mid \\theta^{\\text{old}}) = \\sum_{i=1}^n \\left[ \\gamma_{i1} \\left( \\ln\\pi + \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right) + \\gamma_{i2} \\left( \\ln(1-\\pi) + \\ln\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right) \\right]\n$$\n\n**M 步骤：最大化 Q 函数**\n\n在 M 步骤中，我们寻找新的参数估计 $\\theta^{\\text{new}}$ 来最大化 Q 函数。我们通过对 $Q(\\theta \\mid \\theta^{\\text{old}})$ 关于 $\\theta = \\{\\pi, \\mu_1, \\mu_2\\}$ 中的每个参数求偏导数并令其为零来实现。\n\n1.  **$\\pi$ 的更新：**\n    我们最大化 Q 中依赖于 $\\pi$ 的项：$\\sum_{i=1}^n \\left[ \\gamma_{i1}\\ln\\pi + \\gamma_{i2}\\ln(1-\\pi) \\right]$。对其关于 $\\pi$ 求导并令其为 0：\n    $$\n    \\frac{\\partial Q}{\\partial \\pi} = \\sum_{i=1}^n \\left( \\frac{\\gamma_{i1}}{\\pi} - \\frac{\\gamma_{i2}}{1-\\pi} \\right) = \\frac{1}{\\pi}\\sum_{i=1}^n \\gamma_{i1} - \\frac{1}{1-\\pi}\\sum_{i=1}^n \\gamma_{i2} = 0\n    $$\n    令 $N_1 = \\sum_{i=1}^n \\gamma_{i1}$ 且 $N_2 = \\sum_{i=1}^n \\gamma_{i2}$。注意 $N_1 + N_2 = \\sum_i(\\gamma_{i1}+\\gamma_{i2}) = \\sum_i 1 = n$。\n    $$\n    \\frac{N_1}{\\pi} = \\frac{N_2}{1-\\pi} \\implies N_1(1-\\pi) = N_2\\pi \\implies N_1 = (N_1+N_2)\\pi = n\\pi\n    $$\n    求解 $\\pi$ 得到更新后的估计值 $\\pi^{\\text{new}}$：\n    $$\n    \\pi^{\\text{new}} = \\frac{N_1}{n} = \\frac{1}{n}\\sum_{i=1}^n \\gamma_{i1}\n    $$\n\n2.  **$\\mu_1$ 的更新：**\n    我们最大化 Q 中依赖于 $\\mu_1$ 的项：$\\sum_{i=1}^n \\gamma_{i1} \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)$。\n    $$\n    \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) = -\\frac{1}{2}\\ln(2\\pi\\sigma_1^2) - \\frac{(x_i - \\mu_1)^2}{2\\sigma_1^2}\n    $$\n    对其关于 $\\mu_1$ 求导并令其为 0：\n    $$\n    \\frac{\\partial Q}{\\partial \\mu_1} = \\frac{\\partial}{\\partial \\mu_1} \\sum_{i=1}^n \\gamma_{i1} \\left( -\\frac{(x_i - \\mu_1)^2}{2\\sigma_1^2} \\right) = \\sum_{i=1}^n \\gamma_{i1} \\left( \\frac{x_i - \\mu_1}{\\sigma_1^2} \\right) = 0\n    $$\n    由于 $\\sigma_1^2  0$，我们有 $\\sum_{i=1}^n \\gamma_{i1} (x_i - \\mu_1) = 0$，这意味着：\n    $$\n    \\sum_{i=1}^n \\gamma_{i1}x_i = \\mu_1 \\sum_{i=1}^n \\gamma_{i1}\n    $$\n    求解 $\\mu_1$ 得到更新后的估计值 $\\mu_1^{\\text{new}}$：\n    $$\n    \\mu_1^{\\text{new}} = \\frac{\\sum_{i=1}^n \\gamma_{i1} x_i}{\\sum_{i=1}^n \\gamma_{i1}}\n    $$\n\n3.  **$\\mu_2$ 的更新：**\n    其推导与 $\\mu_1$ 的推导对称。我们最大化 Q 中依赖于 $\\mu_2$ 的项：$\\sum_{i=1}^n \\gamma_{i2} \\ln\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)$。\n    $$\n    \\frac{\\partial Q}{\\partial \\mu_2} = \\frac{\\partial}{\\partial \\mu_2} \\sum_{i=1}^n \\gamma_{i2} \\left( -\\frac{(x_i - \\mu_2)^2}{2\\sigma_2^2} \\right) = \\sum_{i=1}^n \\gamma_{i2} \\left( \\frac{x_i - \\mu_2}{\\sigma_2^2} \\right) = 0\n    $$\n    这意味着 $\\sum_{i=1}^n \\gamma_{i2} (x_i - \\mu_2) = 0$，从而得到：\n    $$\n    \\mu_2^{\\text{new}} = \\frac{\\sum_{i=1}^n \\gamma_{i2} x_i}{\\sum_{i=1}^n \\gamma_{i2}}\n    $$\n    代入 $\\gamma_{i2} = 1 - \\gamma_{i1}$，我们得到用 $\\gamma_{i1}$ 表示的表达式：\n    $$\n    \\mu_2^{\\text{new}} = \\frac{\\sum_{i=1}^n (1 - \\gamma_{i1}) x_i}{\\sum_{i=1}^n (1 - \\gamma_{i1})}\n    $$\n\n这些关于 $\\gamma_{i1}$、$\\pi^{\\text{new}}$、$\\mu_1^{\\text{new}}$ 和 $\\mu_2^{\\text{new}}$ 的表达式构成了一次完整的 EM 算法迭代。该算法的流程是：在 E 步骤中使用当前参数计算责任，然后在 M 步骤中使用这些责任来计算更新后的参数。重复此过程直至收敛。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)}{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)}  \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{i1}  \\frac{\\sum_{i=1}^{n} \\gamma_{i1} x_i}{\\sum_{i=1}^{n} \\gamma_{i1}}  \\frac{\\sum_{i=1}^{n} (1-\\gamma_{i1}) x_i}{\\sum_{i=1}^{n} (1-\\gamma_{i1})} \\end{pmatrix}}\n$$"
        }
    ]
}