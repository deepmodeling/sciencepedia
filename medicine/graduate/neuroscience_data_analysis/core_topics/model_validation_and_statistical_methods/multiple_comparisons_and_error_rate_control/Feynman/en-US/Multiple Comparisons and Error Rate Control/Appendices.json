{
    "hands_on_practices": [
        {
            "introduction": "Controlling the family-wise error rate (FWER) is crucial when you want to avoid making even a single false discovery across a set of hypotheses. The Hochberg procedure provides a powerful \"step-up\" approach that offers greater statistical power than the classic Bonferroni correction while still strictly controlling the FWER under common dependence assumptions. This exercise  will walk you through the mechanics of applying this important method, honing your ability to make rigorous statistical decisions across multiple tests.",
            "id": "4179693",
            "problem": "A laboratory is analyzing spike-train derived features across four cortical regions to assess condition-related changes in neural activity. Each region yields a separate hypothesis test comparing a condition to baseline. Let there be $m=4$ null hypotheses with observed $p$-values $p=(0.001, 0.02, 0.03, 0.2)$. The analysis goal is to control the Family-Wise Error Rate (FWER), defined as the probability $P(V \\ge 1)$ of making at least one false rejection among the $m$ null hypotheses. In large-scale neurophysiology, it is standard to rely on $p$-values that are valid in the sense that, under a true null, each $p$-value is stochastically dominated by a Uniform$(0,1)$ random variable, and joint distributional conditions such as independence or certain forms of positive dependence can be assessed from the data-generating process (for example, through study design or residual diagnostics after prewhitening).\n\nStarting from these foundational definitions and facts, and without assuming any shortcut formulas, do the following:\n\n1. State the Hochberg step-up multiple-testing procedure for FWER control, including the ordering of $p$-values, the decision rule, and the family of critical values. Your statement must explicitly include the requirement on the joint dependence structure under which FWER control is guaranteed (independence or a specified positive dependence condition).\n2. Apply the stated procedure to the given $p$-values $p=(0.001, 0.02, 0.03, 0.2)$ at level $\\alpha=0.05$ to determine the total number of null hypotheses to reject.\n\nReport your final answer as a single integer equal to the number of rejections. No rounding is needed. No units should be included in the final answer.",
            "solution": "The problem as stated is formally sound and well-posed. It presents a standard statistical task based on established principles of multiple hypothesis testing. All necessary data and conditions are provided, and there are no scientific or logical inconsistencies.\n\nThe problem requires a two-part response: first, a formal statement of the Hochberg step-up procedure, and second, its application to the provided data.\n\n### Part 1: The Hochberg Step-Up Procedure\n\nLet there be $m$ null hypotheses, $H_1, H_2, \\dots, H_m$, with corresponding observed $p$-values, $p_1, p_2, \\dots, p_m$. The goal is to control the Family-Wise Error Rate (FWER) at a pre-specified significance level $\\alpha$. The FWER is the probability of making one or more Type I errors (false rejections) among the family of hypotheses. The Hochberg procedure is a step-up method that provides this control under certain conditions.\n\nThe procedure is executed as follows:\n\n1.  **Ordering of $p$-values**: The $m$ observed $p$-values are ordered from smallest to largest:\n    $$p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$$\n    Let $H_{(1)}, H_{(2)}, \\dots, H_{(m)}$ be the null hypotheses corresponding to these ordered $p$-values.\n\n2.  **Family of Critical Values**: For each ordered $p$-value $p_{(i)}$, where $i \\in \\{1, 2, \\dots, m\\}$, a critical value is defined. The Hochberg procedure uses the family of critical values $c_i$ given by:\n    $$c_i = \\frac{\\alpha}{m - i + 1}$$\n    These are the same critical values as in the Holm-Bonferroni step-down procedure, but they are applied in a different manner.\n\n3.  **Decision Rule**: The decision rule for the Hochberg step-up procedure is as follows:\n    Find the largest index $k \\in \\{1, 2, \\dots, m\\}$ such that the inequality\n    $$p_{(k)} \\le \\frac{\\alpha}{m - k + 1}$$\n    is satisfied.\n    - If such an index $k$ exists, then reject all null hypotheses $H_{(j)}$ for $j = 1, 2, \\dots, k$.\n    - If no such index $k$ exists (i.e., the inequality is not met for any $i \\in \\{1, \\dots, m\\}$), then do not reject any of the null hypotheses.\n\n4.  **Dependence Condition**: The Hochberg procedure is proven to control the FWER at level $\\alpha$ (i.e., $FWER \\le \\alpha$) under the assumption that the $p$-values corresponding to the true null hypotheses are independent. This guarantee was later extended by Sarkar (1998) to hold under positive regression dependence on subsets (PRDS), a form of positive dependence that is relevant in many biological and physical systems.\n\n### Part 2: Application to the Given Data\n\nWe are given the following information:\n- Number of hypotheses: $m=4$\n- Observed $p$-values: $p=(0.001, 0.02, 0.03, 0.2)$\n- Desired FWER control level: $\\alpha=0.05$\n\nWe now apply the Hochberg procedure step-by-step.\n\n1.  **Order the $p$-values**: The given $p$-values are already almost sorted. The ordered set is:\n    $$p_{(1)} = 0.001$$\n    $$p_{(2)} = 0.02$$\n    $$p_{(3)} = 0.03$$\n    $$p_{(4)} = 0.2$$\n\n2.  **Apply the Decision Rule**: We need to find the largest index $k \\in \\{1, 2, 3, 4\\}$ that satisfies the condition $p_{(k)} \\le \\frac{\\alpha}{m - k + 1}$. We will test this inequality for each index $i$ from $i=1$ to $i=4$.\n\n    - For $i=1$:\n      The critical value is $\\frac{\\alpha}{m-1+1} = \\frac{0.05}{4} = 0.0125$.\n      We check if $p_{(1)} \\le 0.0125$.\n      $0.001 \\le 0.0125$. The inequality holds for $i=1$.\n\n    - For $i=2$:\n      The critical value is $\\frac{\\alpha}{m-2+1} = \\frac{0.05}{3} \\approx 0.01667$.\n      We check if $p_{(2)} \\le 0.01667$.\n      $0.02 \\not\\le 0.01667$. The inequality does not hold for $i=2$.\n\n    - For $i=3$:\n      The critical value is $\\frac{\\alpha}{m-3+1} = \\frac{0.05}{2} = 0.025$.\n      We check if $p_{(3)} \\le 0.025$.\n      $0.03 \\not\\le 0.025$. The inequality does not hold for $i=3$.\n\n    - For $i=4$:\n      The critical value is $\\frac{\\alpha}{m-4+1} = \\frac{0.05}{1} = 0.05$.\n      We check if $p_{(4)} \\le 0.05$.\n      $0.2 \\not\\le 0.05$. The inequality does not hold for $i=4$.\n\n3.  **Determine Rejections**: The set of indices for which the inequality $p_{(i)} \\le \\frac{\\alpha}{m - i + 1}$ is satisfied is $\\{1\\}$. The largest index in this set is $k=1$.\n\nAccording to the Hochberg decision rule, we reject all hypotheses $H_{(j)}$ for $j=1, \\dots, k$. Since $k=1$, we reject the single hypothesis $H_{(1)}$, which corresponds to the $p$-value of $0.001$.\n\nTherefore, the total number of null hypotheses to reject is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "For situations demanding the strongest guarantees against false positives, the closed testing principle provides a flexible and powerful framework for FWER control. It ensures that any rejection is internally coherent across all related hypotheses, a concept known as strong control, by testing all logical intersections of the elementary hypotheses. In this practice , you will construct a full testing closure and apply Simes' local tests, revealing the logical elegance and rigor of this advanced method.",
            "id": "4179727",
            "problem": "In a cortical microcircuit experiment, a researcher fits a General Linear Model (GLM) to multi-trial local field potential data from a single region of interest, testing three pre-specified, orthogonal contrasts corresponding to distinct task manipulations: contrast $\\mathrm{C}_1$ (early sensory gain), contrast $\\mathrm{C}_2$ (late decision bias), and contrast $\\mathrm{C}_3$ (top-down expectation). The researcher wishes to strongly control the family-wise error rate (FWER) across these $3$ contrasts at level $\\alpha=0.05$ using the closed testing principle with Simes local tests applied to every intersection null. Assume the marginal $p$-values for the three contrasts are continuous, super-uniform under their nulls, and satisfy positive regression dependency on subsets (PRDS), so that Simes local tests are valid for all intersections in this family. The observed marginal $p$-values are $p=(0.001, 0.06, 0.04)$ in the order $(\\mathrm{C}_1,\\mathrm{C}_2,\\mathrm{C}_3)$.\n\nStarting from the foundational definitions of family-wise error rate, intersection nulls, and the closed testing principle, construct the full closure (all intersection hypotheses) for these $3$ contrasts and apply Simes local tests at level $\\alpha$ to determine which of the elementary hypotheses $\\{\\mathrm{C}_1,\\mathrm{C}_2,\\mathrm{C}_3\\}$ are rejected by closed testing. Report your final decisions for $(\\mathrm{C}_1,\\mathrm{C}_2,\\mathrm{C}_3)$ as a $1 \\times 3$ indicator row vector, with entry $1$ denoting “reject” and $0$ denoting “retain.” No rounding is required and no physical units apply.",
            "solution": "The problem requires the application of the closed testing principle to a family of three hypotheses to control the family-wise error rate (FWER) at a specified level $\\alpha$. The solution involves three main steps: (1) defining the set of all intersection hypotheses (the closure), (2) applying a local test (Simes' test) to each intersection hypothesis, and (3) applying the closed testing rule to determine which elementary hypotheses can be rejected.\n\nFirst, we define the foundational concepts. The Family-Wise Error Rate (FWER) is the probability of making one or more Type I errors (falsely rejecting a true null hypothesis) across a family of hypothesis tests. Strong control of the FWER at level $\\alpha$ means that this probability is no greater than $\\alpha$, regardless of which subset of the null hypotheses are true.\n\nLet the three elementary null hypotheses corresponding to contrasts $\\mathrm{C}_1$, $\\mathrm{C}_2$, and $\\mathrm{C}_3$ be $H_1$, $H_2$, and $H_3$, respectively. The given marginal $p$-values are $p_1 = 0.001$, $p_2 = 0.06$, and $p_3 = 0.04$. The desired FWER control level is $\\alpha = 0.05$.\n\nStep 1: Construct the full closure of the hypothesis family.\nThe closed testing principle requires us to consider all possible intersection null hypotheses. For a family of $k = 3$ elementary hypotheses $\\{H_1, H_2, H_3\\}$, the closure is the set of all intersection hypotheses $H_I = \\bigcap_{i \\in I} H_i$ for all non-empty subsets $I \\subseteq \\{1, 2, 3\\}$. The full set of intersection hypotheses to be tested is:\n- Size 3: $H_{\\{1,2,3\\}} = H_1 \\cap H_2 \\cap H_3$\n- Size 2: $H_{\\{1,2\\}} = H_1 \\cap H_2$, $H_{\\{1,3\\}} = H_1 \\cap H_3$, $H_{\\{2,3\\}} = H_2 \\cap H_3$\n- Size 1: $H_{\\{1\\}} = H_1$, $H_{\\{2\\}} = H_2$, $H_{\\{3\\}} = H_3$\n\nStep 2: Apply Simes' local tests.\nFor each intersection hypothesis $H_I$, we apply Simes' test at level $\\alpha = 0.05$. Let the set of $p$-values corresponding to the hypotheses in $I$ be $\\{p_i\\}_{i \\in I}$, and let $p_{(1:I)} \\le p_{(2:I)} \\le \\dots \\le p_{(|I|:I)}$ be these $p$-values sorted in non-decreasing order. Simes' test rejects $H_I$ if there exists at least one $j \\in \\{1, \\dots, |I|\\}$ such that $p_{(j:I)} \\le \\frac{j \\alpha}{|I|}$. The problem states that the PRDS condition holds, which validates the use of Simes' test.\n\nTest of $H_{\\{1,2,3\\}}$:\n- The set of $p$-values is $\\{0.001, 0.06, 0.04\\}$.\n- The ordered $p$-values are $p_{(1)} = 0.001$, $p_{(2)} = 0.04$, $p_{(3)} = 0.06$.\n- The size of the intersection is $|I|=3$.\n- Simes' conditions:\n  - For $j=1$: Is $p_{(1)} \\le \\frac{1 \\cdot 0.05}{3}$? $0.001 \\le 0.016\\overline{6}$. Yes.\n- Since the condition is met for $j=1$, we **reject** $H_{\\{1,2,3\\}}$.\n\nTest of $H_{\\{1,2\\}}$:\n- The set of $p$-values is $\\{0.001, 0.06\\}$.\n- The ordered $p$-values are $p_{(1)} = 0.001$, $p_{(2)} = 0.06$.\n- The size of the intersection is $|I|=2$.\n- Simes' conditions:\n  - For $j=1$: Is $p_{(1)} \\le \\frac{1 \\cdot 0.05}{2}$? $0.001 \\le 0.025$. Yes.\n- We **reject** $H_{\\{1,2\\}}$.\n\nTest of $H_{\\{1,3\\}}$:\n- The set of $p$-values is $\\{0.001, 0.04\\}$.\n- The ordered $p$-values are $p_{(1)} = 0.001$, $p_{(2)} = 0.04$.\n- The size of the intersection is $|I|=2$.\n- Simes' conditions:\n  - For $j=1$: Is $p_{(1)} \\le \\frac{1 \\cdot 0.05}{2}$? $0.001 \\le 0.025$. Yes.\n- We **reject** $H_{\\{1,3\\}}$.\n\nTest of $H_{\\{2,3\\}}$:\n- The set of $p$-values is $\\{0.06, 0.04\\}$.\n- The ordered $p$-values are $p_{(1)} = 0.04$, $p_{(2)} = 0.06$.\n- The size of the intersection is $|I|=2$.\n- Simes' conditions:\n  - For $j=1$: Is $p_{(1)} \\le \\frac{1 \\cdot 0.05}{2}$? $0.04 \\le 0.025$. No.\n  - For $j=2$: Is $p_{(2)} \\le \\frac{2 \\cdot 0.05}{2}$? $0.06 \\le 0.05$. No.\n- Since none of the conditions are met, we **do not reject (retain)** $H_{\\{2,3\\}}$.\n\nTest of $H_{\\{1\\}}$, $H_{\\{2\\}}$, $H_{\\{3\\}}$ (Size 1 intersections):\nFor a single hypothesis, Simes' test reduces to a simple comparison of the $p$-value with $\\alpha$.\n- For $H_{\\{1\\}}$: Is $p_1 \\le \\alpha$? $0.001 \\le 0.05$. Yes. We **reject** $H_{\\{1\\}}$.\n- For $H_{\\{2\\}}$: Is $p_2 \\le \\alpha$? $0.06 \\le 0.05$. No. We **retain** $H_{\\{2\\}}$.\n- For $H_{\\{3\\}}$: Is $p_3 \\le \\alpha$? $0.04 \\le 0.05$. Yes. We **reject** $H_{\\{3\\}}$.\n\nSummary of local test results:\n- $H_{\\{1,2,3\\}}$: Rejected\n- $H_{\\{1,2\\}}$: Rejected\n- $H_{\\{1,3\\}}$: Rejected\n- $H_{\\{2,3\\}}$: Retained\n- $H_{\\{1\\}}$: Rejected\n- $H_{\\{2\\}}$: Retained\n- $H_{\\{3\\}}$: Rejected\n\nStep 3: Apply the closed testing rule.\nAn elementary hypothesis $H_k$ is rejected if and only if **all** intersection hypotheses $H_I$ that contain $H_k$ (i.e., $k \\in I$) are rejected by their respective local tests.\n\nDecision for $H_1$ (Contrast $\\mathrm{C}_1$):\n- The set of intersections containing $H_1$ is $\\{H_{\\{1\\}}, H_{\\{1,2\\}}, H_{\\{1,3\\}}, H_{\\{1,2,3\\}}\\}$.\n- The local tests for these hypotheses resulted in: Reject, Reject, Reject, Reject.\n- Since all relevant intersections were rejected, we **reject** $H_1$.\n\nDecision for $H_2$ (Contrast $\\mathrm{C}_2$):\n- The set of intersections containing $H_2$ is $\\{H_{\\{2\\}}, H_{\\{1,2\\}}, H_{\\{2,3\\}}, H_{\\{1,2,3\\}}\\}$.\n- The local tests for these hypotheses resulted in: Retain, Reject, Retain, Reject.\n- Since not all relevant intersections were rejected (specifically, $H_{\\{2\\}}$ and $H_{\\{2,3\\}}$ were retained), we must **retain** $H_2$.\n\nDecision for $H_3$ (Contrast $\\mathrm{C}_3$):\n- The set of intersections containing $H_3$ is $\\{H_{\\{3\\}}, H_{\\{1,3\\}}, H_{\\{2,3\\}}, H_{\\{1,2,3\\}}\\}$.\n- The local tests for these hypotheses resulted in: Reject, Reject, Retain, Reject.\n- Since not all relevant intersections were rejected (specifically, $H_{\\{2,3\\}}$ was retained), we must **retain** $H_3$.\n\nThe final decisions for the elementary hypotheses $(H_1, H_2, H_3)$ are (Reject, Retain, Retain). These decisions are encoded as an indicator vector $(1, 0, 0)$. It is noteworthy that although the marginal p-value for $H_3$ is $p_3 = 0.04  \\alpha$, it is not rejected by the closed testing procedure because the intersection hypothesis $H_{\\{2,3\\}}$ could not be rejected. This demonstrates how the procedure protects against family-wise error by imposing a logical coherence on the set of rejections.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  0  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In large-scale neuroscience research, controlling the False Discovery Rate (FDR) is often a more powerful and practical goal than controlling the FWER. The Benjamini-Hochberg (BH) procedure is the standard for this, but what if you have prior knowledge about certain hypotheses? This exercise  explores the weighted BH procedure, a modification that allows you to strategically allocate statistical power based on pre-existing evidence, a common scenario in targeted experiments.",
            "id": "4179764",
            "problem": "In a neurophysiology experiment, investigators tested $m=4$ hypotheses about stimulus-modulated oscillatory power across distinct cortical regions, obtaining $p$-values $p=(0.001, 0.02, 0.03, 0.2)$. Prior domain knowledge supplied nonnegative emphasis weights $w=(3, 1, 1, 0.5)$ reflecting relative confidence in each region’s involvement. Assume the tests are independent. The False Discovery Rate (FDR) is defined as $\\mathrm{FDR}=\\mathbb{E}\\!\\left[\\frac{V}{R}\\right]$, where $V$ is the number of false rejections and $R$ is the total number of rejections, with the convention that $\\frac{0}{0}=0$. \n\nUsing the weighted Benjamini–Hochberg (BH) procedure, treat weights as fixed and normalize them so that their sum equals the number of hypotheses. Then apply the step-up decision rule at FDR target level $\\alpha=0.1$ using the normalized weights. Separately, apply the unweighted BH procedure at the same level $\\alpha=0.1$. \n\nCompute the difference between the number of rejections produced by the weighted BH procedure and the number produced by the unweighted BH procedure (weighted minus unweighted). Express your final answer as a single real number. No rounding is required.",
            "solution": "The problem requires the application of two multiple comparison correction procedures, the weighted and unweighted Benjamini-Hochberg (BH) methods, to a set of $p$-values from a neurophysiology experiment. We must then compute the difference in the number of rejected null hypotheses between the two methods.\n\nFirst, we establish the given parameters. The number of hypotheses is $m=4$. The observed p-values are $p = (p_1, p_2, p_3, p_4) = (0.001, 0.02, 0.03, 0.2)$. The emphasis weights are $w = (w_1, w_2, w_3, w_4) = (3, 1, 1, 0.5)$. The target False Discovery Rate (FDR) level is $\\alpha=0.1$.\n\nWe begin with the weighted Benjamini-Hochberg procedure. The problem specifies that the weights must be normalized such that their sum equals the number of hypotheses, $m$.\nThe sum of the given weights is:\n$$ \\sum_{i=1}^{m} w_i = 3 + 1 + 1 + 0.5 = 5.5 $$\nThe normalization constant, $C$, is the ratio of the desired sum ($m$) to the actual sum:\n$$ C = \\frac{m}{\\sum_{i=1}^{m} w_i} = \\frac{4}{5.5} = \\frac{4}{11/2} = \\frac{8}{11} $$\nThe normalized weights, $w'_i = C \\cdot w_i$, are:\n$$ w'_1 = 3 \\times \\frac{8}{11} = \\frac{24}{11} $$\n$$ w'_2 = 1 \\times \\frac{8}{11} = \\frac{8}{11} $$\n$$ w'_3 = 1 \\times \\frac{8}{11} = \\frac{8}{11} $$\n$$ w'_4 = 0.5 \\times \\frac{8}{11} = \\frac{1}{2} \\times \\frac{8}{11} = \\frac{4}{11} $$\nWe verify that the sum of the normalized weights is indeed $m$: $\\frac{24}{11} + \\frac{8}{11} + \\frac{8}{11} + \\frac{4}{11} = \\frac{44}{11} = 4$.\n\nThe weighted BH procedure, as specified, involves applying the standard step-up rule to p-values adjusted by the normalized weights. Let's define the adjusted p-values as $q_i = p_i / w'_i$:\n$$ q_1 = \\frac{0.001}{24/11} = \\frac{0.011}{24} \\approx 0.0004583 $$\n$$ q_2 = \\frac{0.02}{8/11} = \\frac{0.22}{8} = 0.0275 $$\n$$ q_3 = \\frac{0.03}{8/11} = \\frac{0.33}{8} = 0.04125 $$\n$$ q_4 = \\frac{0.2}{4/11} = \\frac{2.2}{4} = 0.55 $$\n\nNext, we sort these adjusted p-values in ascending order, denoted by $q_{(i)}$. In this case, the original ordering is preserved:\n$$ q_{(1)} = q_1 \\approx 0.0004583 $$\n$$ q_{(2)} = q_2 = 0.0275 $$\n$$ q_{(3)} = q_3 = 0.04125 $$\n$$ q_{(4)} = q_4 = 0.55 $$\n\nThe BH step-up rule is to find the largest integer $k$ such that $q_{(k)} \\le \\frac{k}{m} \\alpha$. We then reject the null hypotheses corresponding to $q_{(1)}, \\dots, q_{(k)}$. With $m=4$ and $\\alpha=0.1$, we test this condition for $k=1, \\dots, 4$:\nFor $k=1$: $q_{(1)} \\approx 0.0004583 \\le \\frac{1}{4} \\times 0.1 = 0.025$. The condition is met.\nFor $k=2$: $q_{(2)} = 0.0275 \\le \\frac{2}{4} \\times 0.1 = 0.05$. The condition is met.\nFor $k=3$: $q_{(3)} = 0.04125 \\le \\frac{3}{4} \\times 0.1 = 0.075$. The condition is met.\nFor $k=4$: $q_{(4)} = 0.55 \\le \\frac{4}{4} \\times 0.1 = 0.1$. The condition is not met.\n\nThe largest $k$ that satisfies the condition is $k=3$. Therefore, the weighted BH procedure rejects the first three hypotheses ($H_1, H_2, H_3$). The number of rejections for the weighted procedure is $R_w = 3$.\n\nNext, we perform the unweighted Benjamini-Hochberg procedure. This procedure is applied directly to the original p-values. First, we sort the p-values in ascending order, denoted by $p_{(i)}$:\n$$ p_{(1)} = 0.001 $$\n$$ p_{(2)} = 0.02 $$\n$$ p_{(3)} = 0.03 $$\n$$ p_{(4)} = 0.2 $$\n\nThe unweighted BH step-up rule is to find the largest integer $k$ such that $p_{(k)} \\le \\frac{k}{m} \\alpha$. We reject the null hypotheses corresponding to $p_{(1)}, \\dots, p_{(k)}$.\nWe test this condition for $k=1, \\dots, 4$:\nFor $k=1$: $p_{(1)} = 0.001 \\le \\frac{1}{4} \\times 0.1 = 0.025$. The condition is met.\nFor $k=2$: $p_{(2)} = 0.02 \\le \\frac{2}{4} \\times 0.1 = 0.05$. The condition is met.\nFor $k=3$: $p_{(3)} = 0.03 \\le \\frac{3}{4} \\times 0.1 = 0.075$. The condition is met.\nFor $k=4$: $p_{(4)} = 0.2 \\le \\frac{4}{4} \\times 0.1 = 0.1$. The condition is not met.\n\nThe largest $k$ that satisfies the condition is $k=3$. Thus, the unweighted BH procedure also rejects the first three hypotheses ($H_1, H_2, H_3$). The number of rejections for the unweighted procedure is $R_u = 3$.\n\nFinally, we compute the difference between the number of rejections from the weighted procedure and the unweighted procedure:\n$$ \\text{Difference} = R_w - R_u = 3 - 3 = 0 $$\n\nThe difference between the number of rejections is $0$.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}