{
    "hands_on_practices": [
        {
            "introduction": "Leave-One-Out Cross-Validation (LOOCV) can seem computationally daunting, as its definition implies refitting a model $n$ times for a dataset of size $n$. However, for the fundamental case of ordinary least squares (OLS) regression, a remarkable analytical shortcut exists. This practice guides you through the mathematical derivation of this shortcut, revealing how the LOOCV error can be calculated from a single model fit using residuals and leverage values .",
            "id": "4152064",
            "problem": "Consider a linear ordinary least squares (OLS) regression used in neuroscience data analysis to model a z-scored firing rate response $y \\in \\mathbb{R}^{n}$ from a design matrix $X \\in \\mathbb{R}^{n \\times p}$ encoding experimental covariates (including an intercept). Let the OLS estimator be defined by the normal equations, the hat matrix by $H = X (X^{\\top} X)^{-1} X^{\\top}$, the fitted values by $\\hat{y} = H y$, and the residuals by $e = y - \\hat{y}$. Leave-one-out cross-validation (LOOCV) evaluates prediction quality by, for each index $i$, refitting OLS after removing observation $i$ and then computing the prediction at the held-out covariate $x_{i}^{\\top}$.\n\nStarting only from the fundamental OLS definitions and standard linear algebra identities (including the Sherman–Morrison–Woodbury identity), derive the LOOCV prediction residual at observation $i$ in terms of the full-sample residual $e_{i}$ and the leverage $h_{ii}$, and then express the LOOCV mean-squared error (MSE) in a single closed form that depends only on $\\{e_{i}\\}$ and $\\{h_{ii}\\}$. Explain why this expression allows one to compute LOOCV MSE without refitting $n$ separate OLS models in neural regression pipelines.\n\nThen, apply your derivation to the following experiment with $n = 6$ trials and $p = 3$ parameters (an intercept and two stimulus features). Suppose the full-sample OLS fit has produced the residuals\n$$\ne_{1} = 0.8,\\quad e_{2} = -0.2,\\quad e_{3} = 1.1,\\quad e_{4} = -0.5,\\quad e_{5} = 0.3,\\quad e_{6} = -1.5,\n$$\nand the hat matrix diagonal entries (leverages)\n$$\nh_{11} = 0.7,\\quad h_{22} = 0.4,\\quad h_{33} = 0.6,\\quad h_{44} = 0.3,\\quad h_{55} = 0.5,\\quad h_{66} = 0.5,\n$$\nwith $\\sum_{i=1}^{6} h_{ii} = p$ consistent with OLS geometry. Compute the LOOCV MSE as a single real number. Round your final numeric answer to four significant figures. The response variable is z-scored, so report a unitless value.",
            "solution": "The problem asks for a derivation of the leave-one-out cross-validation (LOOCV) mean-squared error (MSE) for an ordinary least squares (OLS) regression model, an explanation of its computational efficiency, and an application of the derived formula to a specific dataset.\n\nLet the full dataset consist of $n$ observations. The OLS model is $y = X\\beta + \\epsilon$, where $y \\in \\mathbb{R}^{n}$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, and $\\beta \\in \\mathbb{R}^{p}$ is the vector of coefficients. The OLS estimator for $\\beta$ based on the full sample is $\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y$. The fitted values are $\\hat{y} = X\\hat{\\beta} = X(X^{\\top} X)^{-1} X^{\\top} y = Hy$, where $H = X(X^{\\top} X)^{-1} X^{\\top}$ is the hat matrix. The residuals are $e = y - \\hat{y}$, with the $i$-th residual being $e_i = y_i - \\hat{y}_i$. The diagonal elements of the hat matrix, $h_{ii} = x_i^{\\top}(X^{\\top} X)^{-1}x_i$, are the leverages, where $x_i^{\\top}$ is the $i$-th row of $X$.\n\nOur first goal is to derive the LOOCV prediction residual for the $i$-th observation. Let $X_{(-i)}$ and $y_{(-i)}$ denote the design matrix and response vector with the $i$-th observation removed. The OLS estimator computed without the $i$-th observation is $\\hat{\\beta}_{(-i)} = (X_{(-i)}^{\\top} X_{(-i)})^{-1} X_{(-i)}^{\\top} y_{(-i)}$. The LOOCV prediction for $y_i$ is $\\hat{y}_{i,(-i)} = x_i^{\\top} \\hat{\\beta}_{(-i)}$, and the corresponding prediction residual is $e_{i, \\text{LOO}} = y_i - \\hat{y}_{i,(-i)}$.\n\nWe can express the matrix $X^{\\top} X$ as a sum of outer products: $X^{\\top} X = \\sum_{j=1}^{n} x_j x_j^{\\top}$. It follows that $X_{(-i)}^{\\top} X_{(-i)} = \\sum_{j \\neq i} x_j x_j^{\\top} = X^{\\top} X - x_i x_i^{\\top}$. To find the inverse of this matrix, we use the Sherman-Morrison formula, a special case of the Sherman–Morrison–Woodbury identity, which states that $(A - uv^{\\top})^{-1} = A^{-1} + \\frac{A^{-1}uv^{\\top}A^{-1}}{1 - v^{\\top}A^{-1}u}$.\nLetting $A = X^{\\top} X$ and $u = v = x_i$, we have:\n$$\n(X_{(-i)}^{\\top} X_{(-i)})^{-1} = (X^{\\top} X - x_i x_i^{\\top})^{-1} = (X^{\\top} X)^{-1} + \\frac{(X^{\\top} X)^{-1} x_i x_i^{\\top} (X^{\\top} X)^{-1}}{1 - x_i^{\\top} (X^{\\top} X)^{-1} x_i}\n$$\nRecognizing the leverage $h_{ii} = x_i^{\\top}(X^{\\top} X)^{-1}x_i$ in the denominator, this simplifies to:\n$$\n(X_{(-i)}^{\\top} X_{(-i)})^{-1} = (X^{\\top} X)^{-1} + \\frac{(X^{\\top} X)^{-1} x_i x_i^{\\top} (X^{\\top} X)^{-1}}{1 - h_{ii}}\n$$\nNext, we relate $X_{(-i)}^{\\top} y_{(-i)}$ to the full-sample quantities: $X^{\\top} y = \\sum_{j=1}^{n} x_j y_j = X_{(-i)}^{\\top} y_{(-i)} + x_i y_i$, so $X_{(-i)}^{\\top} y_{(-i)} = X^{\\top} y - x_i y_i$.\n\nNow we can write an expression for $\\hat{\\beta}_{(-i)}$:\n$$\n\\hat{\\beta}_{(-i)} = \\left((X^{\\top} X)^{-1} + \\frac{(X^{\\top} X)^{-1} x_i x_i^{\\top} (X^{\\top} X)^{-1}}{1 - h_{ii}}\\right) (X^{\\top} y - x_i y_i)\n$$\nExpanding this expression:\n$$\n\\hat{\\beta}_{(-i)} = (X^{\\top} X)^{-1}(X^{\\top} y - x_i y_i) + \\frac{(X^{\\top} X)^{-1} x_i [x_i^{\\top} (X^{\\top} X)^{-1} (X^{\\top} y - x_i y_i)]}{1 - h_{ii}}\n$$\nRecall that $\\hat{\\beta} = (X^{\\top} X)^{-1}X^{\\top}y$, $\\hat{y}_i = x_i^{\\top}\\hat{\\beta}$, and $h_{ii} = x_i^{\\top}(X^{\\top} X)^{-1}x_i$. The term in the square brackets becomes $x_i^{\\top}\\hat{\\beta} - y_i(x_i^{\\top}(X^{\\top} X)^{-1}x_i) = \\hat{y}_i - y_i h_{ii}$.\n$$\n\\hat{\\beta}_{(-i)} = \\hat{\\beta} - (X^{\\top} X)^{-1}x_i y_i + \\frac{(X^{\\top} X)^{-1} x_i (\\hat{y}_i - y_i h_{ii})}{1 - h_{ii}}\n$$\nTo simplify, we combine the terms involving $(X^{\\top} X)^{-1}x_i$:\n$$\n\\hat{\\beta}_{(-i)} = \\hat{\\beta} - (X^{\\top} X)^{-1}x_i \\left( y_i - \\frac{\\hat{y}_i - y_i h_{ii}}{1 - h_{ii}} \\right)\n$$\nThe term in the parentheses simplifies:\n$$\ny_i - \\frac{\\hat{y}_i - y_i h_{ii}}{1 - h_{ii}} = \\frac{y_i(1 - h_{ii}) - (\\hat{y}_i - y_i h_{ii})}{1 - h_{ii}} = \\frac{y_i - y_i h_{ii} - \\hat{y}_i + y_i h_{ii}}{1 - h_{ii}} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} = \\frac{e_i}{1 - h_{ii}}\n$$\nThus, the relationship between the leave-one-out and full-sample coefficient estimators is:\n$$\n\\hat{\\beta}_{(-i)} = \\hat{\\beta} - \\frac{(X^{\\top} X)^{-1} x_i e_i}{1 - h_{ii}}\n$$\nNow we can find the LOOCV prediction residual, $e_{i, \\text{LOO}} = y_i - x_i^{\\top} \\hat{\\beta}_{(-i)}$:\n$$\ne_{i, \\text{LOO}} = y_i - x_i^{\\top} \\left( \\hat{\\beta} - \\frac{(X^{\\top} X)^{-1} x_i e_i}{1 - h_{ii}} \\right) = (y_i - x_i^{\\top}\\hat{\\beta}) + \\frac{x_i^{\\top}(X^{\\top} X)^{-1} x_i e_i}{1 - h_{ii}}\n$$\nRecognizing $e_i = y_i - x_i^{\\top}\\hat{\\beta}$ and $h_{ii} = x_i^{\\top}(X^{\\top} X)^{-1}x_i$, we obtain:\n$$\ne_{i, \\text{LOO}} = e_i + \\frac{h_{ii} e_i}{1 - h_{ii}} = e_i \\left(1 + \\frac{h_{ii}}{1 - h_{ii}}\\right) = e_i \\left(\\frac{1 - h_{ii} + h_{ii}}{1 - h_{ii}}\\right) = \\frac{e_i}{1 - h_{ii}}\n$$\nThis is the desired relationship between the LOOCV residual and the ordinary residual. These LOOCV residuals are also known as the PRESS (Predicted Residual Sum of Squares) residuals.\n\nThe LOOCV mean-squared error is the average of the squared LOOCV residuals:\n$$\n\\text{MSE}_{\\text{LOO}} = \\frac{1}{n} \\sum_{i=1}^{n} (e_{i, \\text{LOO}})^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{e_i}{1 - h_{ii}}\\right)^2\n$$\nThis is the required closed-form expression.\n\nThis formula provides a profound computational advantage. A naive, brute-force calculation of LOOCV MSE would require fitting the OLS model $n$ times, one for each omitted observation. For a dataset with $n$ observations and $p$ features, each OLS fit has a computational complexity of approximately $O(np^2 + p^3)$. The total cost would be $O(n(np^2+p^3))$. In contrast, the derived formula allows us to calculate the exact LOOCV MSE by fitting the model only once on the full dataset to obtain the residuals $\\{e_i\\}$ and the leverages $\\{h_{ii}\\}$. The cost of this single fit is $O(np^2 + p^3)$, followed by an $O(n)$ calculation to compute the sum. For large $n$, common in neuroscience data (e.g., from spike trains over many trials or time points), this reduces the computational burden by a factor of approximately $n$, making LOOCV a computationally feasible and efficient method for model evaluation.\n\nNow we apply this result to the given experimental data.\nWe are given:\n- Number of trials, $n = 6$.\n- Number of parameters, $p = 3$.\n- Full-sample residuals, $\\{ e_1, e_2, e_3, e_4, e_5, e_6 \\} = \\{ 0.8, -0.2, 1.1, -0.5, 0.3, -1.5 \\}$.\n- Full-sample leverages, $\\{ h_{11}, h_{22}, h_{33}, h_{44}, h_{55}, h_{66} \\} = \\{ 0.7, 0.4, 0.6, 0.3, 0.5, 0.5 \\}$.\n\nUsing the formula for LOOCV MSE:\n$$\n\\text{MSE}_{\\text{LOO}} = \\frac{1}{6} \\sum_{i=1}^{6} \\left(\\frac{e_i}{1 - h_{ii}}\\right)^2\n$$\nWe compute the squared LOOCV residual for each observation:\n- For $i=1$: $\\left( \\frac{0.8}{1 - 0.7} \\right)^2 = \\left( \\frac{0.8}{0.3} \\right)^2 = \\left( \\frac{8}{3} \\right)^2 = \\frac{64}{9}$\n- For $i=2$: $\\left( \\frac{-0.2}{1 - 0.4} \\right)^2 = \\left( \\frac{-0.2}{0.6} \\right)^2 = \\left( \\frac{-1}{3} \\right)^2 = \\frac{1}{9}$\n- For $i=3$: $\\left( \\frac{1.1}{1 - 0.6} \\right)^2 = \\left( \\frac{1.1}{0.4} \\right)^2 = \\left( \\frac{11}{4} \\right)^2 = \\frac{121}{16}$\n- For $i=4$: $\\left( \\frac{-0.5}{1 - 0.3} \\right)^2 = \\left( \\frac{-0.5}{0.7} \\right)^2 = \\left( \\frac{-5}{7} \\right)^2 = \\frac{25}{49}$\n- For $i=5$: $\\left( \\frac{0.3}{1 - 0.5} \\right)^2 = \\left( \\frac{0.3}{0.5} \\right)^2 = \\left( \\frac{3}{5} \\right)^2 = \\frac{9}{25}$\n- For $i=6$: $\\left( \\frac{-1.5}{1 - 0.5} \\right)^2 = \\left( \\frac{-1.5}{0.5} \\right)^2 = (-3)^2 = 9$\n\nNow, we sum these values:\n$$\n\\sum_{i=1}^{6} (e_{i, \\text{LOO}})^2 = \\frac{64}{9} + \\frac{1}{9} + \\frac{121}{16} + \\frac{25}{49} + \\frac{9}{25} + 9\n$$\n$$\n= \\frac{65}{9} + \\frac{121}{16} + \\frac{25}{49} + \\frac{9}{25} + 9\n$$\n$$\n\\approx 7.222222 + 7.5625 + 0.510204 + 0.36 + 9 = 24.654926\n$$\nFinally, we compute the mean by dividing by $n=6$:\n$$\n\\text{MSE}_{\\text{LOO}} = \\frac{24.654926}{6} \\approx 4.109154\n$$\nRounding to four significant figures, the LOOCV MSE is $4.109$.",
            "answer": "$$\\boxed{4.109}$$"
        },
        {
            "introduction": "The elegant shortcut for LOOCV in linear models does not directly translate to more complex, iteratively-fit models like the Generalized Linear Models (GLMs) used to analyze neural spike counts. This exercise challenges you to implement and compare two approaches for a Poisson GLM: the computationally intensive 'brute-force' method of exact refitting and a sophisticated one-step approximation derived from influence functions. Through this coding practice, you will gain a deep, practical understanding of the trade-offs between computational efficiency and predictive accuracy in advanced modeling scenarios .",
            "id": "4152074",
            "problem": "You are tasked with implementing a complete, runnable program that computes the Leave-One-Out Cross-Validation (LOOCV) deviance for a Poisson Generalized Linear Model (GLM) with a logarithmic link function from first principles, and then assessing the error introduced by a one-step approximation based on influence functions relative to exact refitting. The program must be self-contained and produce the specified output without requiring any user input.\n\nBegin from the following fundamental base:\n\n- A Poisson Generalized Linear Model (GLM) with a logarithmic link function models spike counts as follows. For each observation index $i$ with feature vector $\\mathbf{x}_i \\in \\mathbb{R}^p$, the conditional mean is $\\mu_i = \\mathbb{E}[Y_i \\mid \\mathbf{x}_i] = \\exp(\\eta_i)$ where the linear predictor is $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$. The observed spike count $y_i$ is modeled as $Y_i \\sim \\mathrm{Poisson}(\\mu_i)$ independently over $i$.\n- The score function for the Poisson GLM with canonical log link is $\\mathbf{U}(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\mathbf{x}_i (y_i - \\mu_i)$, and the expected Fisher information matrix is $\\mathbf{I}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$ where $\\mathbf{W} = \\mathrm{diag}(\\mu_1, \\ldots, \\mu_n)$ and $\\mathbf{X}$ is the $n \\times p$ design matrix with rows $\\mathbf{x}_i^\\top$.\n- The Poisson deviance for a fitted mean vector $\\boldsymbol{\\mu}$ against observed counts $\\mathbf{y}$ is defined by\n$$\nD(\\mathbf{y}, \\boldsymbol{\\mu}) = 2 \\sum_{i=1}^n \\left( y_i \\log\\left(\\frac{y_i}{\\mu_i}\\right) - (y_i - \\mu_i) \\right),\n$$\nwith the convention that the term $y_i \\log(y_i / \\mu_i)$ is taken to be $0$ when $y_i = 0$.\n\nDefine Leave-One-Out Cross-Validation (LOOCV) deviance for a fitted model as the sum of per-observation deviance contributions computed with the mean for the left-out observation predicted by refitting on the remaining $n-1$ observations.\n\nYour program must implement the following:\n\n1. Fit a Poisson GLM with a logarithmic link function using Iteratively Reweighted Least Squares (IRLS) to obtain the maximum likelihood estimate $\\hat{\\boldsymbol{\\beta}}$ on the full dataset. Use a numerically stable implementation that solves the weighted normal equations $\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\\,\\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{z}$ at each iteration, where the working response is $\\mathbf{z} = \\boldsymbol{\\eta} + (\\mathbf{y} - \\boldsymbol{\\mu}) \\oslash \\boldsymbol{\\mu}$, $\\boldsymbol{\\eta} = \\mathbf{X}\\boldsymbol{\\beta}$, and $\\oslash$ denotes elementwise division. Initialize with an intercept equal to $\\log(\\overline{y} + \\epsilon)$ where $\\overline{y}$ is the sample mean of the counts and $\\epsilon$ is a small positive constant, and all non-intercept coefficients set to $0$. Convergence should be defined by a small threshold on $\\lVert \\boldsymbol{\\beta}_{\\mathrm{new}} - \\boldsymbol{\\beta}_{\\mathrm{old}} \\rVert_2$ or a maximum iteration cap.\n2. Compute the exact LOOCV deviance by, for each observation index $i$, refitting the GLM on the dataset with observation $i$ removed to obtain $\\hat{\\boldsymbol{\\beta}}_{(-i)}$, predicting $\\mu_{(-i), i} = \\exp(\\mathbf{x}_i^\\top \\hat{\\boldsymbol{\\beta}}_{(-i)})$, and summing the deviance contributions\n$$\nd_i^{\\mathrm{exact}} = 2 \\left( y_i \\log\\left(\\frac{y_i}{\\mu_{(-i), i}}\\right) - (y_i - \\mu_{(-i), i}) \\right).\n$$\n3. Compute a one-step approximate LOOCV deviance using a statistically principled first-order influence function approach based on the weighted hat matrix and a single update from the full-data fit. Do not derive or assume any shortcut formulas beyond the fundamental base stated above in your implementation logic; the approximation must be derived from the score and information structure.\n4. Assess the error introduced by the approximation relative to exact refitting by computing:\n   - The absolute difference in total LOOCV deviance, $E_{\\mathrm{total}} = \\left| \\sum_{i=1}^n d_i^{\\mathrm{exact}} - \\sum_{i=1}^n d_i^{\\mathrm{approx}} \\right|$.\n   - The maximum absolute difference over per-observation contributions, $E_{\\mathrm{max}} = \\max_i \\left| d_i^{\\mathrm{exact}} - d_i^{\\mathrm{approx}} \\right|$.\n   - The mean absolute difference over per-observation contributions, $E_{\\mathrm{mean}} = \\frac{1}{n} \\sum_{i=1}^n \\left| d_i^{\\mathrm{exact}} - d_i^{\\mathrm{approx}} \\right|$.\n\nUse the following test suite. For each test case, generate a design matrix with an intercept and $p$ standardized features, simulate spike counts according to the specified true coefficients, and then apply the above procedures.\n\nLet the design matrix be constructed by\n- Setting the first column of $\\mathbf{X}$ to ones (intercept).\n- Drawing the remaining $p - 1$ feature columns independently from a standard normal distribution $\\mathcal{N}(0, 1)$ using the given random seed.\n- In the specified high-leverage case, multiply a single randomly chosen row’s non-intercept features by a scaling factor.\n\nFor each case, compute the true mean vector by $\\boldsymbol{\\mu}^{\\mathrm{true}} = \\exp(\\mathbf{X}\\boldsymbol{\\beta}^{\\mathrm{true}})$ and simulate the spike counts with $y_i \\sim \\mathrm{Poisson}(\\mu^{\\mathrm{true}}_i)$ independently.\n\nTest suite specifications:\n\n- Case $1$ (happy path, typical firing rates): $n = 64$, $p = 4$ (including intercept), random seed $123$, true coefficients $\\boldsymbol{\\beta}^{\\mathrm{true}} = [1.5, 0.25, -0.10, 0.15]^\\top$ (intercept first).\n- Case $2$ (low rates with many zeros): $n = 64$, $p = 4$, random seed $456$, true coefficients $\\boldsymbol{\\beta}^{\\mathrm{true}} = [-0.30, 0.20, -0.05, 0.10]^\\top$.\n- Case $3$ (high rates): $n = 64$, $p = 4$, random seed $789$, true coefficients $\\boldsymbol{\\beta}^{\\mathrm{true}} = [3.00, 0.10, 0.05, -0.02]^\\top$.\n- Case $4$ (edge case with one high-leverage point): $n = 16$, $p = 4$, random seed $321$, true coefficients $\\boldsymbol{\\beta}^{\\mathrm{true}} = [1.00, 0.50, 0.00, -0.40]^\\top$, and multiply the non-intercept features of a single row (choose the row index uniformly at random) by a scaling factor of $8$.\n\nYour program should produce a single line of output containing the results for all four cases as a comma-separated list enclosed in square brackets, where each case’s result is a list of three floats ordered as $[E_{\\mathrm{total}}, E_{\\mathrm{max}}, E_{\\mathrm{mean}}]$. For example, the final output format must be exactly:\n$$\n\\texttt{[[E1\\_total,E1\\_max,E1\\_mean],[E2\\_total,E2\\_max,E2\\_mean],[E3\\_total,E3\\_max,E3\\_mean],[E4\\_total,E4\\_max,E4\\_mean]]}\n$$\nNo physical units are required since spike counts are dimensionless counts per bin. Angles are not involved. All ratios and differences must be expressed as floats without a percentage sign.",
            "solution": "We present a principle-based solution that follows from the stated foundational definitions of the Poisson Generalized Linear Model (GLM) with a logarithmic link, the Iteratively Reweighted Least Squares (IRLS) fitting procedure, and influence-function-based case-deletion approximations.\n\n1. Model and estimation foundation. For a Poisson GLM with a logarithmic link, the conditional mean is $\\mu_i = \\exp(\\eta_i)$ with $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ and $Y_i \\sim \\mathrm{Poisson}(\\mu_i)$ independently. The log-likelihood is\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i \\eta_i - \\mu_i - \\log(y_i!) \\right),\n$$\nwhich yields the score function\n$$\n\\mathbf{U}(\\boldsymbol{\\beta}) = \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}} = \\sum_{i=1}^n \\mathbf{x}_i (y_i - \\mu_i) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}),\n$$\nand the expected Fisher information matrix\n$$\n\\mathbf{I}(\\boldsymbol{\\beta}) = - \\mathbb{E}\\left[ \\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^\\top} \\right] = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}, \\quad \\mathbf{W} = \\mathrm{diag}(\\mu_1, \\ldots, \\mu_n).\n$$\nThe maximum likelihood estimate $\\hat{\\boldsymbol{\\beta}}$ satisfies $\\mathbf{U}(\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$.\n\n2. Iteratively Reweighted Least Squares (IRLS). IRLS is derived from a second-order Taylor expansion around the current parameter estimate, leading to the weighted normal equations\n$$\n\\left( \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X} \\right) \\boldsymbol{\\beta}_{\\mathrm{new}} = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{z},\n$$\nwith the working response vector\n$$\n\\mathbf{z} = \\boldsymbol{\\eta} + \\frac{\\mathbf{y} - \\boldsymbol{\\mu}}{\\boldsymbol{\\mu}},\n$$\nwhere division is elementwise, $\\boldsymbol{\\eta} = \\mathbf{X}\\boldsymbol{\\beta}$, and $\\boldsymbol{\\mu} = \\exp(\\boldsymbol{\\eta})$. At each iteration, we form $\\mathbf{W}$ from the current $\\boldsymbol{\\mu}$, compute $\\mathbf{z}$, and solve for $\\boldsymbol{\\beta}_{\\mathrm{new}}$. Convergence is checked via the change norm $\\lVert \\boldsymbol{\\beta}_{\\mathrm{new}} - \\boldsymbol{\\beta}_{\\mathrm{old}} \\rVert_2$ below a tolerance or hitting a maximum number of iterations. A small ridge term can be added to $\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$ for numerical stability; this does not change the conceptual foundation but improves conditioning.\n\n3. Deviance. The Poisson deviance compares a fitted mean vector $\\boldsymbol{\\mu}$ to observed counts $\\mathbf{y}$ with\n$$\nD(\\mathbf{y}, \\boldsymbol{\\mu}) = 2 \\sum_{i=1}^n \\left( y_i \\log\\left(\\frac{y_i}{\\mu_i}\\right) - (y_i - \\mu_i) \\right),\n$$\nwhere the convention is that the logarithmic term is zero when $y_i = 0$. The Leave-One-Out Cross-Validation (LOOCV) deviance is obtained by summing the per-observation deviance contributions using leave-one-out predictions.\n\n4. Exact LOOCV deviance. For exact LOOCV, each observation index $i$ is removed, the GLM is refit to obtain $\\hat{\\boldsymbol{\\beta}}_{(-i)}$, and the left-out mean is predicted by\n$$\n\\mu_{(-i), i} = \\exp\\left( \\mathbf{x}_i^\\top \\hat{\\boldsymbol{\\beta}}_{(-i)} \\right).\n$$\nThe per-observation deviance contribution is then\n$$\nd_i^{\\mathrm{exact}} = 2 \\left( y_i \\log\\left(\\frac{y_i}{\\mu_{(-i), i}}\\right) - (y_i - \\mu_{(-i), i}) \\right),\n$$\nand the total exact LOOCV deviance is $\\sum_{i=1}^n d_i^{\\mathrm{exact}}$.\n\n5. One-step approximate LOOCV via influence functions. The influence-function approach starts from the score equation satisfied at the full-data maximum likelihood estimate $\\hat{\\boldsymbol{\\beta}}$:\n$$\n\\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}) = \\mathbf{0}.\n$$\nDeleting observation $i$ alters the score to\n$$\n\\mathbf{U}_{(-i)}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}) - \\mathbf{x}_i (y_i - \\mu_i) + \\left( \\frac{\\partial \\mathbf{U}}{\\partial \\boldsymbol{\\beta}} \\right) \\Delta \\boldsymbol{\\beta} + \\cdots,\n$$\nwhere the linear approximation retains the first-order term in the parameter change $\\Delta \\boldsymbol{\\beta} = \\boldsymbol{\\beta}_{(-i)} - \\hat{\\boldsymbol{\\beta}}$ and uses the expected Fisher information matrix evaluated at the fitted model, $\\mathbf{I}(\\hat{\\boldsymbol{\\beta}}) = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X} = \\mathbf{A}$, with $\\mathbf{W} = \\mathrm{diag}(\\mu_1, \\ldots, \\mu_n)$ and $\\mu_j = \\exp(\\mathbf{x}_j^\\top \\hat{\\boldsymbol{\\beta}})$. Careful case-deletion linearization produces\n$$\n\\mathbf{A}_{(-i)} \\Delta \\boldsymbol{\\beta} \\approx \\mathbf{x}_i (y_i - \\mu_i),\n$$\nwhere $\\mathbf{A}_{(-i)} = \\mathbf{A} - w_i \\mathbf{x}_i \\mathbf{x}_i^\\top$ is the case-deleted information matrix with $w_i = \\mu_i$. Applying the Sherman–Morrison identity for rank-one updates,\n$$\n(\\mathbf{A} - \\mathbf{u}\\mathbf{u}^\\top)^{-1} = \\mathbf{A}^{-1} + \\frac{\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{u}^\\top \\mathbf{A}^{-1}}{1 - \\mathbf{u}^\\top \\mathbf{A}^{-1} \\mathbf{u}},\n$$\nwith $\\mathbf{u} = \\sqrt{w_i}\\,\\mathbf{x}_i$, yields\n$$\n\\Delta \\boldsymbol{\\beta} \\approx \\mathbf{A}_{(-i)}^{-1} \\mathbf{x}_i (y_i - \\mu_i)\n= \\frac{\\mathbf{A}^{-1} \\mathbf{x}_i}{1 - h_{ii}} (y_i - \\mu_i),\n$$\nwhere the weighted leverage (hat) value is\n$$\nh_{ii} = w_i \\,\\mathbf{x}_i^\\top \\mathbf{A}^{-1} \\mathbf{x}_i.\n$$\nTherefore, the change in the linear predictor at the $i$-th observation is\n$$\n\\Delta \\eta_i = \\mathbf{x}_i^\\top \\Delta \\boldsymbol{\\beta} \\approx \\frac{\\mathbf{x}_i^\\top \\mathbf{A}^{-1} \\mathbf{x}_i}{1 - h_{ii}} (y_i - \\mu_i) = \\frac{h_{ii}}{w_i (1 - h_{ii})} (y_i - \\mu_i).\n$$\nThe one-step approximate leave-one-out linear predictor for observation $i$ is thus\n$$\n\\eta_{(-i), i}^{\\mathrm{approx}} \\approx \\eta_i + \\frac{h_{ii}}{w_i (1 - h_{ii})} (y_i - \\mu_i),\n$$\nand the corresponding mean is\n$$\n\\mu_{(-i), i}^{\\mathrm{approx}} \\approx \\exp\\left( \\eta_{(-i), i}^{\\mathrm{approx}} \\right).\n$$\nThis mapping preserves positivity of the mean, consistent with the log link. From these approximate means, compute the per-observation deviance contributions\n$$\nd_i^{\\mathrm{approx}} = 2 \\left( y_i \\log\\left(\\frac{y_i}{\\mu_{(-i), i}^{\\mathrm{approx}}}\\right) - (y_i - \\mu_{(-i), i}^{\\mathrm{approx}}) \\right),\n$$\nand sum to obtain the approximate LOOCV deviance.\n\n6. Error metrics. Define the error measures comparing exact and approximate LOOCV deviance by\n$$\nE_{\\mathrm{total}} = \\left| \\sum_{i=1}^n d_i^{\\mathrm{exact}} - \\sum_{i=1}^n d_i^{\\mathrm{approx}} \\right|,\n\\quad\nE_{\\mathrm{max}} = \\max_i \\left| d_i^{\\mathrm{exact}} - d_i^{\\mathrm{approx}} \\right|,\n\\quad\nE_{\\mathrm{mean}} = \\frac{1}{n} \\sum_{i=1}^n \\left| d_i^{\\mathrm{exact}} - d_i^{\\mathrm{approx}} \\right|.\n$$\n\n7. Test suite generation. For each case, construct $\\mathbf{X}$ by setting the first column to ones (intercept) and sampling the remaining columns i.i.d. from $\\mathcal{N}(0, 1)$ using the given seed. For the high-leverage case, select one row index uniformly at random and multiply its non-intercept features by $8$. With the specified $\\boldsymbol{\\beta}^{\\mathrm{true}}$, compute $\\boldsymbol{\\mu}^{\\mathrm{true}} = \\exp(\\mathbf{X}\\boldsymbol{\\beta}^{\\mathrm{true}})$ and draw $y_i \\sim \\mathrm{Poisson}(\\mu_i^{\\mathrm{true}})$ independently.\n\n8. Program structure. Implement:\n- An IRLS solver that returns $\\hat{\\boldsymbol{\\beta}}$, the fitted means $\\boldsymbol{\\mu}$, and the final information matrix $\\mathbf{A} = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$.\n- Exact LOOCV by refitting with each observation removed and computing $d_i^{\\mathrm{exact}}$ and their sum.\n- One-step approximate LOOCV using the influence-function-based update for $\\eta_{(-i), i}^{\\mathrm{approx}}$ and computing $d_i^{\\mathrm{approx}}$.\n- The error metrics $(E_{\\mathrm{total}}, E_{\\mathrm{max}}, E_{\\mathrm{mean}})$ for each case.\n\n9. Output. Aggregate the results for the four cases into a single line exactly formatted as\n$$\n\\texttt{[[E1\\_total,E1\\_max,E1\\_mean],[E2\\_total,E2\\_max,E2\\_mean],[E3\\_total,E3\\_max,E3\\_mean],[E4\\_total,E4\\_max,E4\\_mean]]}.\n$$\n\nThis approach tests first-principles reasoning about LOOCV in Poisson GLMs, demonstrates the derivation of one-step case-deletion approximations from the score and information, and provides a reproducible computational assessment for typical neuron firing rate regimes including low-rate, moderate-rate, high-rate, and high-leverage scenarios.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef irls_poisson_log(X, y, max_iter=200, tol=1e-10, ridge=1e-8):\n    \"\"\"\n    Fit a Poisson GLM with log link using IRLS.\n    Returns beta_hat, mu_hat, A (X^T W X at convergence).\n    \"\"\"\n    n, p = X.shape\n    # Initialize: intercept as log(mean(y)+epsilon), others zero\n    eps = 1e-8\n    beta = np.zeros(p)\n    beta[0] = np.log(np.maximum(y.mean(), eps))\n    for _ in range(max_iter):\n        eta = X @ beta\n        # Ensure numerical stability in exp\n        eta = np.clip(eta, -50, 50)\n        mu = np.exp(eta)\n        # Working response z = eta + (y - mu)/mu\n        # Avoid division by zero\n        mu_safe = np.maximum(mu, eps)\n        z = eta + (y - mu_safe) / mu_safe\n        # Weights W = diag(mu)\n        # Form A = X^T W X and b = X^T W z\n        WX = X * mu_safe[:, None]\n        A = X.T @ WX\n        # Ridge stabilization\n        A += ridge * np.eye(p)\n        b = X.T @ (mu_safe * z)\n        # Solve for new beta\n        try:\n            beta_new = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Fall back to least squares using pseudo-inverse\n            beta_new = np.linalg.pinv(A) @ b\n        # Check convergence\n        if np.linalg.norm(beta_new - beta)  tol:\n            beta = beta_new\n            break\n        beta = beta_new\n    # Final compute\n    eta = X @ beta\n    eta = np.clip(eta, -50, 50)\n    mu = np.exp(eta)\n    WX = X * mu[:, None]\n    A = X.T @ WX\n    A += ridge * np.eye(p)\n    return beta, mu, A\n\ndef poisson_deviance(y, mu):\n    \"\"\"\n    Compute Poisson deviance contributions: 2 * [y * log(y/mu) - (y - mu)]\n    with convention that y * log(y/mu) = 0 when y == 0.\n    \"\"\"\n    eps = 1e-12\n    mu_safe = np.maximum(mu, eps)\n    term = np.zeros_like(y, dtype=float)\n    mask = y > 0\n    term[mask] = y[mask] * np.log(y[mask] / mu_safe[mask])\n    dev = 2.0 * (term - (y - mu_safe))\n    return dev\n\ndef exact_loocv_deviance(X, y, ridge=1e-8):\n    \"\"\"\n    Compute exact LOOCV deviance by refitting model with each observation left out.\n    Returns per-observation deviance contributions and their sum.\n    \"\"\"\n    n = X.shape[0]\n    dev_exact = np.zeros(n, dtype=float)\n    for i in range(n):\n        mask = np.ones(n, dtype=bool)\n        mask[i] = False\n        X_loo = X[mask, :]\n        y_loo = y[mask]\n        beta_loo, _, _ = irls_poisson_log(X_loo, y_loo, ridge=ridge)\n        # Predict mean for the left-out observation\n        eta_i = X[i, :] @ beta_loo\n        eta_i = np.clip(eta_i, -50, 50)\n        mu_i = np.exp(eta_i)\n        # Deviance contribution for observation i\n        dev_exact[i] = poisson_deviance(np.array([y[i]], dtype=float), np.array([mu_i], dtype=float))[0]\n    return dev_exact, float(dev_exact.sum())\n\ndef approx_loocv_deviance(X, y, ridge=1e-8):\n    \"\"\"\n    Compute one-step approximate LOOCV deviance using influence-function-based update.\n    Returns per-observation deviance contributions and their sum.\n    \"\"\"\n    # Fit full model\n    beta_hat, mu_hat, A = irls_poisson_log(X, y, ridge=ridge)\n    # Precompute inverse-related solve for hat values h_ii\n    n = X.shape[0]\n    dev_approx = np.zeros(n, dtype=float)\n    # For stability, ensure mu_hat positive\n    eps = 1e-12\n    mu_hat = np.maximum(mu_hat, eps)\n    for i in range(n):\n        x_i = X[i, :]\n        # Solve A s = x_i to compute x_i^T A^{-1} x_i without explicit inversion\n        try:\n            s = np.linalg.solve(A, x_i)\n        except np.linalg.LinAlgError:\n            s = np.linalg.pinv(A) @ x_i\n        xAx = float(x_i @ s)\n        w_i = mu_hat[i]\n        h_ii = w_i * xAx\n        # Compute eta_i and one-step approx eta_{(-i),i}\n        eta_i = float(x_i @ beta_hat)\n        # One-step update for linear predictor:\n        # eta_{(-i),i} ≈ eta_i + (h_ii / (w_i * (1 - h_ii))) * (y_i - mu_i)\n        denom = max(1.0 - h_ii, 1e-12)  # avoid division by zero\n        delta_eta = (h_ii / (w_i * denom)) * (y[i] - mu_hat[i])\n        eta_loo_approx = eta_i + delta_eta\n        eta_loo_approx = float(np.clip(eta_loo_approx, -50, 50))\n        mu_loo_approx = float(np.exp(eta_loo_approx))\n        dev_approx[i] = poisson_deviance(np.array([y[i]], dtype=float), np.array([mu_loo_approx], dtype=float))[0]\n    return dev_approx, float(dev_approx.sum())\n\ndef simulate_case(n, p, seed, beta_true, leverage=False, scale=1.0):\n    \"\"\"\n    Generate design matrix X with intercept and p-1 standard normal features,\n    simulate y ~ Poisson(exp(X beta_true)).\n    If leverage=True, pick a random row and scale its non-intercept features by scale.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Intercept + p-1 features\n    X = np.ones((n, p))\n    if p > 1:\n        X[:, 1:] = rng.standard_normal((n, p - 1))\n    if leverage and p > 1:\n        idx = rng.integers(0, n)\n        X[idx, 1:] *= scale\n    # True mean and Poisson sampling\n    eta_true = X @ beta_true\n    eta_true = np.clip(eta_true, -50, 50)\n    mu_true = np.exp(eta_true)\n    y = rng.poisson(mu_true)\n    return X, y\n\ndef evaluate_case(n, p, seed, beta_true, leverage=False, scale=1.0):\n    \"\"\"\n    Fit, compute exact and approximate LOOCV deviance, and error metrics.\n    Returns [E_total, E_max, E_mean].\n    \"\"\"\n    X, y = simulate_case(n, p, seed, beta_true, leverage, scale)\n    dev_exact, dev_sum_exact = exact_loocv_deviance(X, y)\n    dev_approx, dev_sum_approx = approx_loocv_deviance(X, y)\n    diffs = np.abs(dev_exact - dev_approx)\n    E_total = abs(dev_sum_exact - dev_sum_approx)\n    E_max = float(diffs.max())\n    E_mean = float(diffs.mean())\n    return [E_total, E_max, E_mean]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: happy path, typical firing rates\n        {\"n\": 64, \"p\": 4, \"seed\": 123, \"beta_true\": np.array([1.5, 0.25, -0.10, 0.15]), \"leverage\": False, \"scale\": 1.0},\n        # Case 2: low rates with many zeros\n        {\"n\": 64, \"p\": 4, \"seed\": 456, \"beta_true\": np.array([-0.30, 0.20, -0.05, 0.10]), \"leverage\": False, \"scale\": 1.0},\n        # Case 3: high rates\n        {\"n\": 64, \"p\": 4, \"seed\": 789, \"beta_true\": np.array([3.00, 0.10, 0.05, -0.02]), \"leverage\": False, \"scale\": 1.0},\n        # Case 4: edge case with one high-leverage point\n        {\"n\": 16, \"p\": 4, \"seed\": 321, \"beta_true\": np.array([1.00, 0.50, 0.00, -0.40]), \"leverage\": True, \"scale\": 8.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        res = evaluate_case(case[\"n\"], case[\"p\"], case[\"seed\"], case[\"beta_true\"], case[\"leverage\"], case[\"scale\"])\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    # Produce a single line with a Python-style list of lists.\n    def fmt_list(lst):\n        return \"[\" + \",\".join(f\"{x}\" for x in lst) + \"]\"\n    print(\"[\" + \",\".join(fmt_list(r) for r in results) + \"]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A robust cross-validation estimate depends not only on the modeling step but on the integrity of the entire analysis pipeline. A common but critical error is 'data leakage,' where information from the validation set inadvertently influences the training process, often during preprocessing. This practice uses the example of EEG baseline correction to demonstrate how this leakage occurs and, more importantly, how to design a 'fold-aware' preprocessing step that respects the separation of training and validation data, ensuring an unbiased performance estimate .",
            "id": "4152103",
            "problem": "Consider an Electroencephalography (EEG) experiment with $N$ independent trials from a single subject, each recorded on $C$ channels, with a pre-stimulus baseline window $t \\in [-\\tau, 0)$ and a post-stimulus analysis window $t \\in [0, T)$. For each trial $i \\in \\{1,\\dots,N\\}$ and channel $c \\in \\{1,\\dots,C\\}$, let $B_{i,c}(t)$ denote the baseline signal and $S_{i,c}(t)$ denote the post-stimulus signal in the analysis window. Define the per-trial baseline average $\\bar{B}_{i,c} = \\frac{1}{L} \\sum_{t \\in \\mathcal{W}} B_{i,c}(t)$ over a fixed baseline subwindow $\\mathcal{W} \\subset [-\\tau, 0)$ of $L$ time samples.\n\nSuppose features $X_i \\in \\mathbb{R}^d$ are extracted from the baseline-corrected post-stimulus signals. Two baseline correction strategies are considered:\n\n1. Cross-trial baseline subtraction: Estimate a global per-channel baseline $\\hat{\\mu}_c = \\frac{1}{N} \\sum_{i=1}^{N} \\bar{B}_{i,c}$, and subtract $\\hat{\\mu}_c$ from all channels in all trials before feature extraction, i.e., use $S'_{i,c}(t) = S_{i,c}(t) - \\hat{\\mu}_c$.\n\n2. Per-trial baseline subtraction: Subtract each trial’s own baseline average from its post-stimulus signal, i.e., $S'_{i,c}(t) = S_{i,c}(t) - \\bar{B}_{i,c}$.\n\nA classifier $f$ is trained to predict a trial label $y_i \\in \\{0,1\\}$ from $X_i$, and performance is estimated using $K$-fold cross-validation, with folds $(\\mathcal{T}_k, \\mathcal{V}_k)$ for $k \\in \\{1,\\dots,K\\}$, where $\\mathcal{T}_k$ and $\\mathcal{V}_k$ are training and validation index sets, respectively, and $\\mathcal{T}_k \\cap \\mathcal{V}_k = \\emptyset$, $\\mathcal{T}_k \\cup \\mathcal{V}_k = \\{1,\\dots,N\\}$. The cross-validation estimator of risk is\n$$\n\\hat{R}_{CV} = \\frac{1}{K} \\sum_{k=1}^{K} \\frac{1}{|\\mathcal{V}_k|} \\sum_{i \\in \\mathcal{V}_k} \\ell\\!\\left(f_k\\!\\left(\\Phi_k(X_i)\\right),\\, y_i\\right),\n$$\nwhere $\\ell$ is a bounded loss, $f_k$ is the classifier trained on features transformed by a preprocessing mapping $\\Phi_k$ that may depend on data used to fit it, and $X_i$ denotes the features extracted from baseline-corrected signals according to the chosen strategy.\n\nAssume trials are independent and identically distributed (i.i.d.), the baseline and post-stimulus windows are free of label-dependent artifacts prior to any processing, and $f_k$ is trained only on $\\mathcal{T}_k$ after preprocessing.\n\nWhich of the following statements are correct regarding leakage induced by cross-trial baseline subtraction and a fold-aware baseline estimation procedure under $K$-fold cross-validation and Leave-One-Out Cross-Validation (LOOCV)?\n\nA. Estimating a global cross-trial baseline $\\hat{\\mu}_c$ once using all $N$ trials prior to fold assignment does not introduce leakage because the estimator does not use labels and only accesses pre-stimulus data.\n\nB. A fold-aware cross-trial baseline estimator defined by $\\hat{\\mu}_{c}^{(k)} = \\frac{1}{|\\mathcal{T}_k|} \\sum_{i \\in \\mathcal{T}_k} \\bar{B}_{i,c}$ and applied to both training and validation trials within fold $k$ prevents leakage and yields an unbiased estimator of generalization risk under the i.i.d. assumption.\n\nC. In Leave-One-Out Cross-Validation (LOOCV), using the global $\\hat{\\mu}_c$ estimated on all $N$ trials, including the held-out trial, does not bias performance because the held-out trial’s contribution is $1/N$, which vanishes as $N$ grows.\n\nD. A fold-aware baseline estimator computed on training trials only reduces leakage but generally increases the variance of the baseline estimate relative to using all $N$ trials; nonetheless, cross-validation remains an unbiased estimator of generalization risk under i.i.d. trials.\n\nE. Per-trial baseline subtraction $S'_{i,c}(t) = S_{i,c}(t) - \\bar{B}_{i,c}$, applied identically to training and validation trials, is fold-agnostic and does not induce leakage because it does not use information from other trials when transforming a given trial.",
            "solution": "We start from fundamental definitions. Cross-validation constructs estimates of out-of-sample risk by holding out validation sets $\\mathcal{V}_k$, fitting the entire preprocessing-and-learning pipeline on $\\mathcal{T}_k$, and evaluating on $\\mathcal{V}_k$. Under independent and identically distributed (i.i.d.) trials and proper isolation of validation data from any step whose parameters are fit using training data, $\\hat{R}_{CV}$ is an unbiased estimator of the generalization risk associated with the pipeline mapping training data to a predictor $f_k$ and preprocessing $\\Phi_k$.\n\nData leakage occurs when information from $\\mathcal{V}_k$ affects the fitted object used at training time, thereby breaking the independence between the validation evaluation and the training pipeline. Crucially, this includes unsupervised preprocessing whose parameters are estimated using all data (i.e., including $\\mathcal{V}_k$), because it induces statistical dependence between the transform applied to validation trials and those same validation trials.\n\nTo analyze cross-trial baseline subtraction, consider the global per-channel baseline estimator\n$$\n\\hat{\\mu}_c = \\frac{1}{N} \\sum_{i=1}^{N} \\bar{B}_{i,c}.\n$$\nLet $j \\in \\mathcal{V}_k$ be a validation trial. If we subtract $\\hat{\\mu}_c$ from all trials before fold assignment, the transformed validation signal is\n$$\nS'_{j,c}(t) = S_{j,c}(t) - \\hat{\\mu}_c = S_{j,c}(t) - \\frac{1}{N}\\bar{B}_{j,c} - \\frac{1}{N}\\sum_{i \\neq j} \\bar{B}_{i,c}.\n$$\nThe preprocessing transform $\\Phi$ here is parameterized by $\\hat{\\mu}_c$, which is a function of all trials, including the trial $j$ under evaluation. Consequently, the transformed validation features $X_j$ depend on $j$ not only through its own raw signals but also through the fitted parameter $\\hat{\\mu}_c$ that includes $\\bar{B}_{j,c}$. This induces nonzero covariance between the transform parameter and the validation data:\n$$\n\\operatorname{Cov}\\!\\left(\\hat{\\mu}_c,\\, \\bar{B}_{j,c}\\right) = \\operatorname{Var}\\!\\left(\\bar{B}_{j,c}\\right)\\cdot \\frac{1}{N} \\neq 0,\n$$\nassuming $\\operatorname{Var}\\!\\left(\\bar{B}_{j,c}\\right)  0$. Therefore, the validation loss $\\ell\\!\\left(f_k\\!\\left(\\Phi(X_j)\\right), y_j\\right)$ is not independent of the training pipeline fitted prior to fold-splitting, and $\\hat{R}_{CV}$ is biased downward (optimistic), even though labels were not used to fit $\\hat{\\mu}_c$. The effect size scales with $1/N$ per trial, but it is not zero for finite $N$, so leakage is present.\n\nA fold-aware cross-trial baseline estimator addresses this by estimating per-fold baselines using only training trials:\n$$\n\\hat{\\mu}_{c}^{(k)} = \\frac{1}{|\\mathcal{T}_k|} \\sum_{i \\in \\mathcal{T}_k} \\bar{B}_{i,c}.\n$$\nWithin fold $k$, use $S'_{i,c}(t) = S_{i,c}(t) - \\hat{\\mu}_{c}^{(k)}$ for all $i \\in \\mathcal{T}_k \\cup \\mathcal{V}_k$ before feature extraction. Now, for $j \\in \\mathcal{V}_k$, the transform parameter $\\hat{\\mu}_{c}^{(k)}$ is independent of $\\bar{B}_{j,c}$ and $S_{j,c}(t)$ under i.i.d. trials, since it is a function only of $\\{\\bar{B}_{i,c} : i \\in \\mathcal{T}_k\\}$. This restores the required independence between validation data and the fitted training pipeline, ensuring that $\\hat{R}_{CV}$ remains an unbiased estimator of the risk of the pipeline that fits $\\hat{\\mu}_{c}^{(k)}$ on training data. The price paid is increased variance of $\\hat{\\mu}_{c}^{(k)}$ relative to $\\hat{\\mu}_c$:\n$$\n\\operatorname{Var}\\!\\left(\\hat{\\mu}_{c}^{(k)}\\right) = \\frac{1}{|\\mathcal{T}_k|}\\operatorname{Var}\\!\\left(\\bar{B}_{i,c}\\right), \\quad\n\\operatorname{Var}\\!\\left(\\hat{\\mu}_c\\right) = \\frac{1}{N}\\operatorname{Var}\\!\\left(\\bar{B}_{i,c}\\right),\n$$\nso $\\operatorname{Var}\\!\\left(\\hat{\\mu}_{c}^{(k)}\\right) \\ge \\operatorname{Var}\\!\\left(\\hat{\\mu}_c\\right)$ because $|\\mathcal{T}_k| \\le N$. This increased variance can make per-fold transformations noisier, but it does not introduce bias into the cross-validation risk estimate provided the i.i.d. assumption holds.\n\nFor Leave-One-Out Cross-Validation (LOOCV), $K = N$ and $|\\mathcal{T}_k| = N - 1$. Using a global estimator $\\hat{\\mu}_c$ including the held-out trial $j$ still contaminates the validation transformation with information from $j$, with contribution weight $1/N$. Although this influence is small for large $N$, it is nonzero for finite $N$, so leakage remains and the estimator is not strictly unbiased. The correct LOOCV implementation uses\n$$\n\\hat{\\mu}_{c}^{(-j)} = \\frac{1}{N - 1} \\sum_{i \\neq j} \\bar{B}_{i,c},\n$$\nfor the held-out trial $j$, again restoring independence.\n\nFinally, consider per-trial baseline subtraction $S'_{i,c}(t) = S_{i,c}(t) - \\bar{B}_{i,c}$. This transformation is computed entirely from the trial $i$ itself and does not depend on other trials. When applied identically to all trials in both training and validation sets, it does not create cross-fold dependence or leakage. It is a sample-wise preprocessing step, not a fitted parameter requiring training data, and so it is fold-agnostic.\n\nWe now evaluate each option:\n\nA. Incorrect. Even though labels are not used, fitting $\\hat{\\mu}_c$ on all $N$ trials before cross-validation uses validation data to set a preprocessing parameter. This creates dependency between $\\Phi$ and the validation trials, inducing leakage and bias.\n\nB. Correct. Estimating $\\hat{\\mu}_{c}^{(k)}$ using only $\\mathcal{T}_k$ and applying it within fold $k$ prevents validation data from affecting the fitted preprocessing. Under i.i.d. trials, this yields an unbiased cross-validation risk estimate.\n\nC. Incorrect. The contribution $1/N$ of the held-out trial to $\\hat{\\mu}_c$ is small but nonzero. For finite $N$, this still causes leakage and bias. Proper LOOCV requires excluding the held-out trial from the baseline estimate.\n\nD. Correct. Using only training trials to estimate the baseline increases the variance of the baseline estimator relative to using all trials, but it eliminates leakage. Under i.i.d. trials, the cross-validation estimator remains unbiased for the generalization risk of the fold-aware pipeline.\n\nE. Correct. Per-trial baseline subtraction uses only information from the trial itself and does not fit parameters on other data. Applied consistently, it does not cause cross-fold leakage.",
            "answer": "$$\\boxed{BDE}$$"
        }
    ]
}