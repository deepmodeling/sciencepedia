{
    "hands_on_practices": [
        {
            "introduction": "Before conducting any quantitative analysis, an essential first practice is to develop an intuition for the different signals present in raw EEG and MEG recordings. This exercise challenges you to apply fundamental biophysical and signal processing principles to differentiate the brain signals of interest from common non-neural contaminants. By learning to recognize the distinct spatial and spectral \"fingerprints\" of neurogenic ERPs/ERFs versus ocular and muscle artifacts, you build a foundational skill for assessing data quality and designing effective cleaning strategies .",
            "id": "4160438",
            "problem": "A researcher records electroencephalography (EEG) and magnetoencephalography (MEG) during a visual oddball task. They observe three recurring sensor-space phenomena across runs. The goal is to classify ocular and muscle artifacts, and to explain how their spatial and spectral signatures differ from neurogenic event-related potentials (ERPs) and event-related fields (ERFs). You must reason from the following foundational bases and definitions:\n\n1. Under the quasi-static approximation of Maxwell’s equations, scalp potentials and magnetic fields arise from primary cortical current sources and the associated secondary volume currents. In a simplified sensor-space forward model, the measurement vector obeys a linear mixing relation $x(t) = L s(t) + \\epsilon(t)$, where $x(t)$ is the sensor vector, $L$ is the leadfield matrix determined by geometry and conductivity, $s(t)$ are latent source time courses, and $\\epsilon(t)$ is noise.\n\n2. The power spectral density $P_x(f)$ of a time series $x(t)$ describes the distribution of power across frequency $f$; for a time-limited signal with Fourier transform $X(f)$ over duration $T$, $P_x(f) = \\frac{1}{T}\\lvert X(f)\\rvert^{2}$.\n\n3. Biophysical time constants shape spectral content: postsynaptic currents underlying cortical ERPs/ERFs have characteristic time scales on the order of $10$–$100\\,\\mathrm{ms}$, while ocular artifacts arise from corneo-retinal dipole rotations during blinks ($\\sim 100$–$300\\,\\mathrm{ms}$) or saccades ($\\sim 30$–$80\\,\\mathrm{ms}$), and electromyography (EMG) from facial and scalp muscles reflects muscle fiber action potentials lasting $\\sim 1$–$5\\,\\mathrm{ms}$.\n\n4. Volume conduction through the skull acts as a spatial smoother and imposes frequency-dependent attenuation that can be modeled in first approximation as a low-pass filter with magnitude response $H(f) = \\frac{1}{\\sqrt{1+(2\\pi f \\tau)^{2}}}$ for an effective time constant $\\tau$, attenuating higher-frequency cortical potentials more than lower-frequency ones in EEG. Magnetoencephalography is less affected by skull conductivity, but cortical synaptic dynamics still constrain ERF spectral content.\n\nUsing these bases, evaluate the following statements about the origins, spatial patterns, and spectral characteristics of ocular and muscle artifacts versus neurogenic ERPs/ERFs. Select all statements that are correct.\n\nA. Ocular artifacts originate from rotations and deformations of the corneo-retinal dipole and eyelid movement, contributing predominantly low-frequency power ($f \\lesssim 4\\,\\mathrm{Hz}$) to $P_x(f)$. In EEG, their spatial pattern exhibits large anterior loadings (frontal electrodes) and in MEG they often lack a canonical cortical dipole pattern; their amplitude co-varies with the vertical electrooculogram (EOG).\n\nB. Muscle artifacts (EMG) produce broadband high-frequency power ($\\sim 20$–$300\\,\\mathrm{Hz}$) with spatially focal sensor-space patterns concentrated at lateral and temporal edge electrodes or sensors near facial and scalp muscles; they typically show poor trial-to-trial phase consistency relative to stimulus onset and can inflate apparent gamma-band power.\n\nC. Neurogenic ERPs in EEG and ERFs in MEG reflect time-locked synaptic currents in cortex that, after the forward model $x(t) = L s(t) + \\epsilon(t)$ and the effective low-pass properties of skull and synaptic dynamics, express phase-locked averages with spatially extended dipolar topographies consistent with cortical current dipoles and with most energy below $\\sim 30\\,\\mathrm{Hz}$.\n\nD. Magnetoencephalography is effectively immune to ocular and muscle artifacts because it is insensitive to radial sources; therefore, MEG rarely shows contamination from blinks, saccades, or facial muscle activity.\n\nE. Because blinks are brief events, ocular artifacts concentrate power in the $30$–$80\\,\\mathrm{Hz}$ gamma band. A high-pass filter with cutoff $f_{c}=20\\,\\mathrm{Hz}$ will remove ocular artifacts while preserving ERPs and removing muscle artifacts.\n\nChoose all correct options.",
            "solution": "The user wants me to act as a STEM professor and critically validate and solve a problem regarding the classification of artifacts and neurogenic signals in EEG/MEG data.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the specified procedure.\n\n#### Step 1: Extract Givens\n\n1.  The forward model is a linear mixing relation: $x(t) = L s(t) + \\epsilon(t)$, where $x(t)$ is the sensor measurement vector, $L$ is the leadfield matrix, $s(t)$ are latent source time courses, and $\\epsilon(t)$ is noise. This model is based on the quasi-static approximation of Maxwell’s equations.\n2.  The power spectral density (PSD) of a time series $x(t)$ is given by $P_x(f) = \\frac{1}{T}\\lvert X(f)\\rvert^{2}$, where $X(f)$ is the Fourier transform of the signal over a duration $T$.\n3.  Biophysical time constants for different signal sources are provided:\n    *   Postsynaptic currents (ERPs/ERFs): $10$–$100\\,\\mathrm{ms}$\n    *   Ocular artifacts (blinks): $\\sim 100$–$300\\,\\mathrm{ms}$\n    *   Ocular artifacts (saccades): $\\sim 30$–$80\\,\\mathrm{ms}$\n    *   Muscle artifacts (EMG): $\\sim 1$–$5\\,\\mathrm{ms}$\n4.  Volume conduction effects:\n    *   The skull acts as a spatial smoother and a low-pass filter for EEG, with a magnitude response approximated by $H(f) = \\frac{1}{\\sqrt{1+(2\\pi f \\tau)^{2}}}$.\n    *   MEG is less affected by skull conductivity.\n    *   Cortical synaptic dynamics also constrain the spectral content of ERFs.\n\n#### Step 2: Validate Using Extracted Givens\n\nI will now assess the problem's validity based on the extracted givens.\n\n*   **Scientifically Grounded**: The problem is firmly rooted in the standard biophysical and mathematical models used in EEG and MEG data analysis. The quasi-static approximation of Maxwell's equations (Given 1), the definition of the power spectral density (Given 2), the typical time constants of neurogenic and artifactual sources (Given 3), and the filtering effects of head tissues (Given 4) are all cornerstones of this field. There is no pseudoscience or controversial information.\n*   **Well-Posed**: The problem asks for an evaluation of several statements based on the provided principles. This is a classification task, not a problem requiring a unique numerical solution, but it is well-structured. For each statement, the provided bases are sufficient to determine its correctness.\n*   **Objective**: The language is precise and technical. The statements to be evaluated are objective claims about the properties of signals and artifacts, which can be verified against the provided principles and established scientific facts.\n\nThe problem does not exhibit any of the invalidity flaws:\n1.  **Scientific/Factual Unsoundness**: The premises are sound.\n2.  **Non-Formalizable/Irrelevant**: The problem is directly relevant and formalizable within neuroscience data analysis.\n3.  **Incomplete/Contradictory**: The setup is self-consistent and provides sufficient information.\n4.  **Unrealistic/Infeasible**: The data (time constants) and models are realistic.\n5.  **Ill-Posed**: The task is clear and leads to a definitive evaluation of each option.\n6.  **Pseudo-Profound/Trivial**: The problem requires a solid understanding of the interplay between biophysics, signal processing, and the physics of EEG/MEG, making it a substantive reasoning task.\n7.  **Outside Scientific Verifiability**: All claims are verifiable.\n\n#### Step 3: Verdict and Action\n\nThe problem statement is **valid**. I will proceed with the detailed solution and evaluation of each option.\n\n### Solution Derivation and Option Analysis\n\nBased on the validated problem statement, I will now derive the correct answer by analyzing each option.\n\n#### Option A Evaluation\n\n*   **Statement**: Ocular artifacts originate from rotations and deformations of the corneo-retinal dipole and eyelid movement, contributing predominantly low-frequency power ($f \\lesssim 4\\,\\mathrm{Hz}$) to $P_x(f)$. In EEG, their spatial pattern exhibits large anterior loadings (frontal electrodes) and in MEG they often lack a canonical cortical dipole pattern; their amplitude co-varies with the vertical electrooculogram (EOG).\n\n*   **Analysis**:\n    1.  **Origin**: The description of the source as the corneo-retinal dipole is correct. The eye globe has a standing potential, positive at the cornea and negative at the retina. Eye movements (rotations, blinks) reorient this dipole, generating recordable electric and magnetic fields. This is consistent with the general understanding of biosignals.\n    2.  **Spectral Content**: Given 3 states that blinks have a duration of $\\sim 100$–$300\\,\\mathrm{ms}$. A fundamental principle of Fourier analysis (related to Given 2) is that the characteristic frequency of a signal is inversely related to its duration. A duration of $300\\,\\mathrm{ms}$ corresponds to a frequency of $1/(0.3\\,\\mathrm{s}) \\approx 3.3\\,\\mathrm{Hz}$. The smooth, slow nature of a blink concentrates its power at very low frequencies. Therefore, the claim that ocular artifacts contribute predominantly low-frequency power, specifically $f \\lesssim 4\\,\\mathrm{Hz}$, is consistent with the provided time constants.\n    3.  **Spatial Pattern**: The eyes are located at the front of the head. According to the forward model $x(t) = L s(t) + \\epsilon(t)$ (Given 1), sources near a set of sensors (the frontal electrodes) will produce the largest signals at those sensors. This explains the large anterior loadings in EEG. Since the source is non-cortical, its magnetic field pattern in MEG would not match the pattern expected from a current dipole within the cortical sheet, which is what is meant by a \"canonical cortical dipole pattern\".\n    4.  **Correlation with EOG**: The electrooculogram (EOG) is a technique specifically designed to measure eye movements using electrodes placed around the eyes. A high correlation between frontal EEG/MEG sensor activity and EOG channels is the standard method for identifying and subsequently removing ocular artifacts.\n\n*   **Verdict**: The statement is a complete and accurate description of ocular artifacts in EEG/MEG. **Correct**.\n\n#### Option B Evaluation\n\n*   **Statement**: Muscle artifacts (EMG) produce broadband high-frequency power ($\\sim 20$–$300\\,\\mathrm{Hz}$) with spatially focal sensor-space patterns concentrated at lateral and temporal edge electrodes or sensors near facial and scalp muscles; they typically show poor trial-to-trial phase consistency relative to stimulus onset and can inflate apparent gamma-band power.\n\n*   **Analysis**:\n    1.  **Spectral Content**: Given 3 states that muscle fiber action potentials are very brief, lasting $\\sim 1$–$5\\,\\mathrm{ms}$. This short duration implies high-frequency content. A duration of $5\\,\\mathrm{ms}$ corresponds to a fundamental frequency of $1/(0.005\\,\\mathrm{s}) = 200\\,\\mathrm{Hz}$, and $1\\,\\mathrm{ms}$ corresponds to $1000\\,\\mathrm{Hz}$. The summation of many such brief, asynchronous potentials from muscle fibers results in a signal with broadband power, accurately described as extending from $\\sim 20\\,\\mathrm{Hz}$ to well over $100\\,\\mathrm{Hz}$ (here cited as $\\sim 20$–$300\\,\\mathrm{Hz}$).\n    2.  **Spatial Pattern**: Muscles of the scalp, face, and neck (e.g., temporalis, frontalis, masseter) are superficial and located at the periphery of the standard EEG/MEG sensor array. The forward model (Given 1) dictates that superficial sources produce high-amplitude, spatially focal signals on the nearest sensors. Thus, EMG contamination is typically concentrated at temporal and other edge sensors.\n    3.  **Phase Consistency**: Unlike neurogenic ERPs, which are, by definition, phase-locked to a stimulus, muscle activity is generally not. A subject may tense their jaw or swallow at random times relative to stimulus onset. This lack of phase consistency is a key feature used to separate EMG from event-related neural responses.\n    4.  **Gamma Band Inflation**: The gamma band is commonly defined as approximately $30$–$100\\,\\mathrm{Hz}$. Since the EMG spectrum fully overlaps and extends beyond this band, its presence can be mistaken for or add to the neural gamma-band activity, a well-known and serious confound in cognitive neuroscience research.\n\n*   **Verdict**: The statement is a complete and accurate description of muscle artifacts. **Correct**.\n\n#### Option C Evaluation\n\n*   **Statement**: Neurogenic ERPs in EEG and ERFs in MEG reflect time-locked synaptic currents in cortex that, after the forward model $x(t) = L s(t) + \\epsilon(t)$ and the effective low-pass properties of skull and synaptic dynamics, express phase-locked averages with spatially extended dipolar topographies consistent with cortical current dipoles and with most energy below $\\sim 30\\,\\mathrm{Hz}$.\n\n*   **Analysis**:\n    1.  **Origin and Nature**: The statement correctly identifies ERPs/ERFs as the averaged, time-locked responses to stimuli, reflecting cortical synaptic currents. This is the standard definition.\n    2.  **Signal Propagation and Filtering**: It correctly incorporates the forward model $x(t) = L s(t) + \\epsilon(t)$ (Given 1) and the filtering effects. Given 3 provides the time scale for postsynaptic currents ($10$–$100\\,\\mathrm{ms}$), which intrinsically limits high-frequency content. Given 4 explicitly notes the low-pass filtering effect of the skull in EEG. The combination of source dynamics and volume conduction shapes the final measured signal.\n    3.  **Spatial and Spectral Features**: The process of averaging across trials isolates the phase-locked portion of the signal. The leadfield $L$ projects the activity of a localized cortical source (modeled as a current dipole) onto the sensor array, resulting in a spatially distributed, or \"extended,\" topography. For EEG, this pattern is often dipolar. The combination of synaptic time constants and low-pass filtering by the head (for EEG) ensures that most of the signal power in the resulting ERP is in the lower frequencies, typically below $30\\,\\mathrm{Hz}$ or $40\\,\\mathrm{Hz}$ (e.g., the P300 component in an oddball task has a dominant frequency around $1$-$3\\,\\mathrm{Hz}$).\n\n*   **Verdict**: The statement provides a concise and accurate synthesis of the principles governing neurogenic signals. **Correct**.\n\n#### Option D Evaluation\n\n*   **Statement**: Magnetoencephalography is effectively immune to ocular and muscle artifacts because it is insensitive to radial sources; therefore, MEG rarely shows contamination from blinks, saccades, or facial muscle activity.\n\n*   **Analysis**:\n    1.  **Premise**: The statement claims MEG is \"effectively immune\" to artifacts because of its insensitivity to radial sources. While it is true that MEG is maximally sensitive to tangential currents and minimally sensitive to purely radial currents, the sources of artifacts are not purely radial.\n    2.  **Ocular Artifacts**: The rotation of the corneo-retinal dipole during a saccade or blink has significant tangential components relative to the spherical geometry of the head and sensor array. These tangential currents produce strong magnetic fields that are readily detected by MEG, often with larger amplitudes than the neural signals of interest.\n    3.  **Muscle Artifacts**: Currents from muscle fibers in the face, scalp, and neck are oriented in various complex ways, very few of which are purely radial with respect to the MEG sensors. These currents generate strong magnetic fields that are a major source of high-frequency contamination in MEG data.\n    4.  **Conclusion**: The conclusion that MEG \"rarely shows contamination\" is factually incorrect. Both ocular and muscle artifacts are prominent and problematic in MEG recordings. The reasoning is flawed because it misapplies the principle of radial insensitivity.\n\n*   **Verdict**: The statement is based on a flawed premise and leads to a false conclusion. **Incorrect**.\n\n#### Option E Evaluation\n\n*   **Statement**: Because blinks are brief events, ocular artifacts concentrate power in the $30$–$80\\,\\mathrm{Hz}$ gamma band. A high-pass filter with cutoff $f_{c}=20\\,\\mathrm{Hz}$ will remove ocular artifacts while preserving ERPs and removing muscle artifacts.\n\n*   **Analysis**:\n    1.  **Blink Spectrum**: The premise that blinks are \"brief events\" in a way that would produce high-frequency power is incorrect. As per Given 3, blinks are neurologically slow events with durations of $\\sim 100$–$300\\,\\mathrm{ms}$. As analyzed for Option A, this concentrates their power at very low frequencies ($< 10\\,\\mathrm{Hz}$), not in the gamma band ($30$–$80\\,\\mathrm{Hz}$).\n    2.  **Filtering Proposal**: The statement proposes a single high-pass filter at $f_c = 20\\,\\mathrm{Hz}$ for three distinct purposes:\n        *   *Remove ocular artifacts*: This would be effective, as most blink power is below $20\\,\\mathrm{Hz}$.\n        *   *Preserve ERPs*: This is incorrect. As established in Option C, a large portion of ERP energy is below $30\\,\\mathrm{Hz}$. A high-pass filter at $20\\,\\mathrm{Hz}$ would severely distort and attenuate, if not entirely remove, major ERP components like the P300, which is central to the oddball task mentioned.\n        *   *Remove muscle artifacts*: This is incorrect. As established in Option B, muscle artifacts produce broadband power starting around $20\\,\\mathrm{Hz}$ and extending to several hundred Hz. A high-pass filter at $20\\,\\mathrm{Hz}$ would *pass* the entire muscle artifact band, doing nothing to remove it. To remove muscle artifacts, a *low-pass* filter would be required.\n    \n*   **Verdict**: The statement begins with a factually incorrect premise about the spectral content of blinks and then proposes a filtering strategy that would fail at two of its three stated goals, critically destroying the signal of interest (ERPs). **Incorrect**.",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "Baseline correction is a ubiquitous and deceptively simple step in virtually all standard ERP/ERF processing pipelines. This practice moves beyond the procedural application of this technique, tasking you with analytically deriving the bias it can introduce. By working through the mathematics, you will gain a deeper, quantitative understanding of how slow drifts or lingering activity in the pre-stimulus interval systematically distort post-stimulus amplitude measurements, a critical insight for robust data analysis .",
            "id": "4160321",
            "problem": "Consider a univariate electroencephalography time series recorded around a stimulus onset at time $0$ for an event-related potential (ERP) or event-related field (ERF). Let the observed signal be modeled as the linear superposition $x(t) = s(t) + o(t) + n(t)$, where $s(t)$ is the stimulus-locked ERP/ERF component that satisfies $s(t) = 0$ for $t < 0$, $o(t)$ is an overlapping process that is not strictly stimulus-locked (for example, slow drift or lingering activity from previous events), and $n(t)$ is zero-mean stochastic noise. Baseline correction is defined as subtracting the mean of the observed signal over a pre-stimulus interval $[-T_{b}, 0)$, with $T_{b} > 0$, so that the baseline-corrected signal is $y(t) = x(t) - \\frac{1}{T_{b}} \\int_{-T_{b}}^{0} x(\\tau)\\, d\\tau$. Let the amplitude estimate at a specific post-stimulus time $t_{0} > 0$ be the value $y(t_{0})$.\n\nStarting from the superposition principle and the definitions above, derive an expression for the expectation $\\mathbb{E}[y(t_{0})]$ in terms of $s(t_{0})$ and $o(t)$, and define the bias $b(t_{0})$ as the difference between $\\mathbb{E}[y(t_{0})]$ and the true stimulus-locked amplitude $s(t_{0})$. Then, under the specific overlapping-process model $o(t) = \\alpha + \\beta t$ for all $t \\in \\mathbb{R}$, with constants $\\alpha \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$, compute $b(t_{0})$ explicitly. Express your final answer as a single closed-form analytic expression for $b(t_{0})$. No numerical approximation or rounding is required.",
            "solution": "The user-provided problem statement has been analyzed and validated as scientifically sound, well-posed, and complete. We may therefore proceed with the derivation of a solution.\n\nThe observed signal is modeled as a linear superposition:\n$$x(t) = s(t) + o(t) + n(t)$$\nwhere $s(t)$ is the stimulus-locked signal, $o(t)$ is an overlapping process, and $n(t)$ is zero-mean stochastic noise, implying $\\mathbb{E}[n(t)] = 0$ for all $t$. We are given that $s(t) = 0$ for $t < 0$. The stimulus onset is at $t=0$.\n\nThe baseline-corrected signal, $y(t)$, is defined by subtracting the mean of the signal over the pre-stimulus interval $[-T_{b}, 0)$, where $T_{b} > 0$:\n$$y(t) = x(t) - \\frac{1}{T_{b}} \\int_{-T_{b}}^{0} x(\\tau)\\, d\\tau$$\n\nThe goal is to determine the expectation of the amplitude estimate $y(t_0)$ at a post-stimulus time $t_{0} > 0$. We begin by taking the expectation of the expression for $y(t_0)$:\n$$\\mathbb{E}[y(t_{0})] = \\mathbb{E}\\left[x(t_{0}) - \\frac{1}{T_{b}} \\int_{-T_{b}}^{0} x(\\tau)\\, d\\tau\\right]$$\n\nBy the linearity of the expectation operator, we can distribute it across the terms:\n$$\\mathbb{E}[y(t_{0})] = \\mathbb{E}[x(t_{0})] - \\mathbb{E}\\left[\\frac{1}{T_{b}} \\int_{-T_{b}}^{0} x(\\tau)\\, d\\tau\\right]$$\n\nSince the integral is a linear operator, and assuming we can exchange the order of expectation and integration (a condition typically met, as per Fubini's theorem for well-behaved stochastic processes), this becomes:\n$$\\mathbb{E}[y(t_{0})] = \\mathbb{E}[x(t_{0})] - \\frac{1}{T_{b}} \\int_{-T_{b}}^{0} \\mathbb{E}[x(\\tau)]\\, d\\tau$$\n\nNext, we substitute the model for $x(t)$ into the expectation $\\mathbb{E}[x(t)]$:\n$$\\mathbb{E}[x(t)] = \\mathbb{E}[s(t) + o(t) + n(t)]$$\nApplying linearity of expectation again:\n$$\\mathbb{E}[x(t)] = \\mathbb{E}[s(t)] + \\mathbb{E}[o(t)] + \\mathbb{E}[n(t)]$$\n\nIn this model, $s(t)$ and $o(t)$ are treated as deterministic signals, thus they are unaffected by the expectation operator (i.e., $\\mathbb{E}[s(t)] = s(t)$ and $\\mathbb{E}[o(t)] = o(t)$). The noise term $n(t)$ is defined as having zero mean, so $\\mathbb{E}[n(t)] = 0$. This simplifies the expectation of the observed signal to:\n$$\\mathbb{E}[x(t)] = s(t) + o(t)$$\n\nSubstituting this result back into the expression for $\\mathbb{E}[y(t_{0})]$:\n$$\\mathbb{E}[y(t_{0})] = (s(t_0) + o(t_0)) - \\frac{1}{T_{b}} \\int_{-T_{b}}^{0} (s(\\tau) + o(\\tau))\\, d\\tau$$\n$$\\mathbb{E}[y(t_{0})] = s(t_0) + o(t_0) - \\frac{1}{T_{b}} \\int_{-T_{b}}^{0} s(\\tau)\\, d\\tau - \\frac{1}{T_{b}} \\int_{-T_{b}}^{0} o(\\tau)\\, d\\tau$$\n\nAccording to the problem statement, the stimulus-locked component $s(t)$ is zero for all negative time, i.e., $s(t) = 0$ for $t < 0$. The integration interval for the baseline is $[-T_{b}, 0)$, which lies entirely within this region. Therefore, the integral of $s(\\tau)$ over this interval is zero:\n$$\\int_{-T_{b}}^{0} s(\\tau)\\, d\\tau = \\int_{-T_{b}}^{0} 0\\, d\\tau = 0$$\n\nThe expression for the expected value of the baseline-corrected signal simplifies to:\n$$\\mathbb{E}[y(t_{0})] = s(t_0) + o(t_0) - \\frac{1}{T_{b}} \\int_{-T_{b}}^{0} o(\\tau)\\, d\\tau$$\n\nThe bias, $b(t_0)$, is defined as the difference between this expectation and the true stimulus-locked amplitude $s(t_0)$:\n$$b(t_{0}) = \\mathbb{E}[y(t_{0})] - s(t_{0})$$\nSubstituting our derived expression for $\\mathbb{E}[y(t_{0})]$:\n$$b(t_{0}) = \\left( s(t_0) + o(t_0) - \\frac{1}{T_{b}} \\int_{-T_{b}}^{0} o(\\tau)\\, d\\tau \\right) - s(t_0)$$\n$$b(t_{0}) = o(t_0) - \\frac{1}{T_{b}} \\int_{-T_{b}}^{0} o(\\tau)\\, d\\tau$$\nThis general expression shows that the bias is the value of the overlapping process at time $t_0$ minus its average value over the pre-stimulus baseline interval.\n\nWe are now tasked with computing this bias for the specific case where the overlapping process is a linear function of time:\n$$o(t) = \\alpha + \\beta t$$\nwith constants $\\alpha \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$.\n\nFirst, we evaluate $o(t_0)$:\n$$o(t_0) = \\alpha + \\beta t_0$$\n\nSecond, we compute the integral term:\n$$\\frac{1}{T_{b}} \\int_{-T_{b}}^{0} o(\\tau)\\, d\\tau = \\frac{1}{T_{b}} \\int_{-T_{b}}^{0} (\\alpha + \\beta \\tau)\\, d\\tau$$\nEvaluating the definite integral:\n$$\\int_{-T_{b}}^{0} (\\alpha + \\beta \\tau)\\, d\\tau = \\left[ \\alpha\\tau + \\frac{\\beta}{2}\\tau^{2} \\right]_{-T_{b}}^{0}$$\n$$= \\left( \\alpha(0) + \\frac{\\beta}{2}(0)^{2} \\right) - \\left( \\alpha(-T_{b}) + \\frac{\\beta}{2}(-T_{b})^{2} \\right)$$\n$$= 0 - \\left( -\\alpha T_{b} + \\frac{\\beta}{2}T_{b}^{2} \\right) = \\alpha T_{b} - \\frac{\\beta}{2}T_{b}^{2}$$\nNow, dividing by $T_b$:\n$$\\frac{1}{T_{b}} \\left( \\alpha T_{b} - \\frac{\\beta}{2}T_{b}^{2} \\right) = \\alpha - \\frac{\\beta T_{b}}{2}$$\n\nFinally, we substitute these results back into the expression for the bias $b(t_0)$:\n$$b(t_{0}) = (\\alpha + \\beta t_0) - \\left(\\alpha - \\frac{\\beta T_{b}}{2}\\right)$$\n$$b(t_{0}) = \\alpha + \\beta t_0 - \\alpha + \\frac{\\beta T_{b}}{2}$$\n$$b(t_{0}) = \\beta t_0 + \\frac{\\beta T_{b}}{2}$$\nThis can be written in factored form as:\n$$b(t_{0}) = \\beta \\left( t_0 + \\frac{T_{b}}{2} \\right)$$\nThis is the closed-form analytic expression for the bias introduced by applying pre-stimulus baseline correction in the presence of a linear drift. The bias is directly proportional to the slope $\\beta$ of the drift and depends on both the time of measurement $t_0$ and the duration of the baseline period $T_b$. The constant offset $\\alpha$ is completely removed by the baseline correction procedure.",
            "answer": "$$\\boxed{\\beta \\left(t_{0} + \\frac{T_{b}}{2}\\right)}$$"
        },
        {
            "introduction": "A significant challenge in ERP/ERF research is that neural components evoked by an event often overlap in time, creating composite waveforms that are difficult to disentangle with simple averaging. This advanced computational practice introduces a powerful modeling solution using a general linear model framework, where each underlying neural response is represented by a set of flexible temporal basis functions. By implementing this model to fit synthetic data, you will gain hands-on experience with a principled method for decomposing and quantifying overlapping components, a technique at the forefront of modern ERP/ERF analysis .",
            "id": "4160457",
            "problem": "You are given a scenario grounded in the analysis of Event-Related Potentials (ERPs) and Event-Related Fields (ERFs), where a measured sensor waveform is modeled as a linear superposition of overlapping neural components in the presence of additive noise. The aim is to separate the contributions of multiple overlapping components by representing each component’s time course as a linear combination of temporal basis functions and fitting this model to the sensor data. The solution must start from the core principles that the measured signal is a sum of component responses and that the noise is additive and Gaussian, and proceed to a principled estimation method that leverages these assumptions.\n\nA single sensor measurement over time is modeled as follows. Let the time series be sampled uniformly at sampling frequency $f_s$ (in Hz) over an interval $[0, T]$ seconds, producing $N$ samples at times $t_i = i/f_s$ for $i = 0, 1, \\dots, N-1$. The measured signal $y(t)$ is assumed to be the sum of $K$ components and additive noise:\n$$\ny(t) = \\sum_{k=1}^{K} s_k(t) + \\varepsilon(t),\n$$\nwhere for each component $k$, the time course $s_k(t)$ is represented as a linear combination of $M$ temporal basis functions that are anchored to a component-specific latency $\\tau_k$:\n$$\ns_k(t) = \\sum_{m=1}^{M} \\beta_{k,m}\\,\\phi_m\\!\\big((t - \\tau_k)\\big).\n$$\nThe unknown coefficients $\\beta_{k,m}$ are the amplitudes (in microvolts) of basis functions that describe the component shape, and $\\varepsilon(t)$ is zero-mean additive Gaussian noise with variance $\\sigma^2$. The temporal basis functions $\\phi_m(\\cdot)$ are fixed and known; in this task, use Gaussian basis functions parameterized by centers $c_m$ and a common width $w$:\n$$\n\\phi_m(u) = \\exp\\!\\left(-\\tfrac{1}{2}\\,\\frac{(u - c_m)^2}{w^2}\\right).\n$$\nDefine the design matrix $X \\in \\mathbb{R}^{N \\times (K M)}$ whose columns are the basis functions evaluated at the sampled times, with column ordering by component index first and then basis index: for $k \\in \\{1,\\dots,K\\}$ and $m \\in \\{1,\\dots,M\\}$, the column with index $j = (k-1)M + m$ is given entrywise by\n$$\nX_{i, j} = \\phi_m\\!\\big((t_i - \\tau_k)\\big).\n$$\nWith $\\beta \\in \\mathbb{R}^{K M}$ denoting the vector of all coefficients $\\beta_{k,m}$ stacked in the same order, the model is\n$$\ny = X \\beta + \\varepsilon.\n$$\n\nYour task is to:\n- Construct the design matrix $X$ from the given parameters.\n- Generate synthetic data $y$ from a known ground-truth coefficient vector $\\beta^{\\star}$ by $y = X \\beta^{\\star} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n- Estimate $\\hat{\\beta}$ by minimizing a squared-error objective with an $\\ell_2$-penalty (ridge), that is, solve\n$$\n\\hat{\\beta} = \\underset{\\beta}{\\arg\\min}\\ \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2,\n$$\nwhere $\\lambda \\ge 0$ is a given penalty parameter. When $\\lambda = 0$, this reduces to ordinary least squares. The estimator must be implemented for general $\\lambda \\ge 0$.\n- For each test case, compute the maximum absolute coefficient error\n$$\nE_{\\max} = \\max_{j} \\left| \\hat{\\beta}_j - \\beta^{\\star}_j \\right|\n$$\nexpressed in microvolts, as a floating-point value.\n\nUse the following test suite. All numeric values are given explicitly and are scientifically plausible for electrophysiological signals. Use time in seconds, sampling frequency in Hz, and amplitudes in microvolts. Angles are not involved. For random noise generation, use the provided integer random seed for reproducibility. For each case, construct $t_i$ for $i = 0, 1, \\dots, N-1$ with $N = \\lfloor f_s T \\rfloor + 1$ (inclusive of both endpoints).\n\nTest Case A (happy path, moderate overlap, high signal-to-noise ratio):\n- Sampling frequency $f_s = 1000$ Hz, duration $T = 0.300$ s.\n- Number of components $K = 2$, number of basis functions per component $M = 3$.\n- Component latencies $\\tau = [0.080,\\ 0.140]$ s.\n- Basis centers $c = [-0.012,\\ 0.000,\\ 0.012]$ s, common width $w = 0.008$ s.\n- Ground-truth coefficients (in microvolts), ordered by component then basis:\n  $\\beta^{\\star} = [4.0,\\ -2.0,\\ 1.0,\\ -3.0,\\ 2.0,\\ -1.5]$.\n- Noise standard deviation $\\sigma = 0.200$ microvolts.\n- Ridge penalty $\\lambda = 0.000$.\n- Random seed $s = 42$.\n\nTest Case B (stronger overlap, lower signal-to-noise ratio, mild regularization):\n- Sampling frequency $f_s = 1000$ Hz, duration $T = 0.300$ s.\n- Number of components $K = 2$, number of basis functions per component $M = 3$.\n- Component latencies $\\tau = [0.100,\\ 0.120]$ s.\n- Basis centers $c = [-0.012,\\ 0.000,\\ 0.012]$ s, common width $w = 0.008$ s.\n- Ground-truth coefficients (in microvolts), ordered by component then basis:\n  $\\beta^{\\star} = [3.5,\\ -1.0,\\ 0.5,\\ 2.0,\\ -2.5,\\ 1.0]$.\n- Noise standard deviation $\\sigma = 0.500$ microvolts.\n- Ridge penalty $\\lambda = 0.100$.\n- Random seed $s = 43$.\n\nTest Case C (three components, severe overlap, stronger regularization):\n- Sampling frequency $f_s = 1000$ Hz, duration $T = 0.300$ s.\n- Number of components $K = 3$, number of basis functions per component $M = 4$.\n- Component latencies $\\tau = [0.090,\\ 0.110,\\ 0.130]$ s.\n- Basis centers $c = [-0.018,\\ -0.006,\\ 0.006,\\ 0.018]$ s, common width $w = 0.010$ s.\n- Ground-truth coefficients (in microvolts), ordered by component then basis:\n  $\\beta^{\\star} = [2.2,\\ -1.2,\\ 0.6,\\ -0.1,\\ -1.5,\\ 2.8,\\ -2.0,\\ 0.7,\\ 0.8,\\ -1.0,\\ 1.2,\\ -0.5]$.\n- Noise standard deviation $\\sigma = 0.700$ microvolts.\n- Ridge penalty $\\lambda = 0.500$.\n- Random seed $s = 44$.\n\nProgram requirements:\n- Implement the model construction, data synthesis, coefficient estimation, and error computation as described above.\n- For each of the three test cases, output the maximum absolute coefficient error $E_{\\max}$ as a floating-point number (in microvolts).\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets (for example, $[r_A,r_B,r_C]$), with $r_A$ corresponding to Test Case A, $r_B$ to Test Case B, and $r_C$ to Test Case C. No additional text should be printed.",
            "solution": "We begin from the principle of linear superposition of time-locked neural responses: the measured signal $y(t)$ at a sensor is modeled as the sum of component waveforms plus additive noise. Specifically, for Event-Related Potentials (ERPs) or Event-Related Fields (ERFs), when component latencies are known or hypothesized, a component’s shape can be flexibly represented by a linear combination of a small set of temporal basis functions. This yields a linear generative model,\n$$\ny(t) = \\sum_{k=1}^{K} \\sum_{m=1}^{M} \\beta_{k,m}\\,\\phi_m\\!\\big((t - \\tau_k)\\big) + \\varepsilon(t), \\quad \\varepsilon(t) \\sim \\mathcal{N}(0, \\sigma^2),\n$$\nwhich, when discretized at sampling instants $t_i = i / f_s$ for $i = 0, 1, \\dots, N-1$, can be written in matrix form as\n$$\ny = X \\beta + \\varepsilon,\n$$\nwhere $y \\in \\mathbb{R}^{N}$ is the stacked sample vector, $X \\in \\mathbb{R}^{N \\times (K M)}$ is the design matrix whose columns are the basis functions evaluated at the sample times and appropriately shifted by the component latencies, and $\\beta \\in \\mathbb{R}^{K M}$ stacks the unknown basis coefficients by component and basis index.\n\nConstruction of the design matrix is a direct application of evaluating the chosen basis functions. With Gaussian basis functions\n$$\n\\phi_m(u) = \\exp\\!\\left(-\\tfrac{1}{2}\\,\\frac{(u - c_m)^2}{w^2}\\right),\n$$\nand component latencies $\\tau_k$, the $(i,j)$-th entry of $X$ for column $j = (k-1)M + m$ is\n$$\nX_{i,j} = \\exp\\!\\left(-\\tfrac{1}{2}\\,\\frac{\\big((t_i - \\tau_k) - c_m\\big)^2}{w^2}\\right).\n$$\nThis ensures each component has its own set of time-local basis functions anchored at its latency, enabling overlapping components to be represented distinctly if the basis is sufficiently expressive.\n\nEstimation derives from a probabilistic principle. Under the assumption that $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$, the likelihood of parameters $\\beta$ is\n$$\np(y \\mid \\beta) \\propto \\exp\\!\\left(-\\tfrac{1}{2 \\sigma^2} \\| y - X \\beta \\|_2^2 \\right).\n$$\nMaximizing this likelihood (equivalently minimizing the negative log-likelihood) leads to minimizing the squared error $\\| y - X \\beta \\|_2^2$, yielding the ordinary least squares estimator when unconstrained. To mitigate instability due to multicollinearity of overlapping basis functions and to control variance under low signal-to-noise ratio, one may incorporate an $\\ell_2$ penalty with weight $\\lambda \\ge 0$, corresponding to a Gaussian prior on $\\beta$ with variance proportional to $1/\\lambda$. The resulting estimator is the ridge estimator:\n$$\n\\hat{\\beta} = \\underset{\\beta}{\\arg\\min}\\ \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2.\n$$\nSetting the gradient of the objective with respect to $\\beta$ to zero yields the normal equations with regularization:\n$$\n(X^\\top X + \\lambda I)\\, \\hat{\\beta} = X^\\top y.\n$$\nWhen $\\lambda = 0$, this reduces to the ordinary least squares normal equations. For $\\lambda > 0$, $(X^\\top X + \\lambda I)$ is strictly positive definite, improving numerical conditioning and handling severe overlap between component bases.\n\nAlgorithmic steps:\n1. For each test case, construct the time vector $t_i = i/f_s$ for $i = 0, \\dots, \\lfloor f_s T \\rfloor$ to cover $[0, T]$ with uniform sampling. This yields $N = \\lfloor f_s T \\rfloor + 1$ samples.\n2. Build the design matrix $X$ by evaluating the Gaussian basis functions with the specified centers $c_m$ and width $w$ at each $t_i$ for each component latency $\\tau_k$, forming $K M$ columns ordered by component then basis.\n3. Form the ground truth coefficient vector $\\beta^{\\star}$ by stacking coefficients for each component in the prescribed order.\n4. Synthesize the observation vector by $y = X \\beta^{\\star} + \\varepsilon$, where $\\varepsilon$ is drawn from a zero-mean Gaussian with standard deviation $\\sigma$. Use the provided integer random seed to initialize a pseudo-random number generator to ensure reproducibility.\n5. Estimate $\\hat{\\beta}$ by solving $(X^\\top X + \\lambda I)\\, \\hat{\\beta} = X^\\top y$ for the specified $\\lambda$. This can be performed using a numerically stable linear solver on the symmetric positive semidefinite (or definite, if $\\lambda > 0$) matrix.\n6. Compute the maximum absolute error across coefficients, $E_{\\max} = \\max_j | \\hat{\\beta}_j - \\beta^{\\star}_j |$. Because the basis functions have unit peak ($\\phi_m(0) = 1$ at their centers) and coefficients are in microvolts, $E_{\\max}$ is naturally expressed in microvolts.\n7. Repeat for all test cases.\n\nDesign considerations:\n- Severe component overlap and closely spaced basis centers can induce multicollinearity, elevating the condition number of $X^\\top X$. The ridge penalty $\\lambda$ regularizes the solution by shrinking coefficients, improving stability and interpretability at the expense of bias.\n- The sampling scheme includes both endpoints to ensure the temporal window $[0, T]$ is fully represented.\n- The use of Gaussian basis functions with specified centers $c_m$ and a common width $w$ provides a compact, smooth, and physiologically plausible parametric family for modeling component shapes.\n\nFinally, the program must output a single line: a list $[r_A, r_B, r_C]$ where each $r$ is the $E_{\\max}$ for the corresponding test case, as floating-point numbers in microvolts and in that order. No additional text should be printed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef gaussian_basis(u, centers, width):\n    \"\"\"\n    Evaluate a bank of Gaussian basis functions at offsets u.\n    centers: array of shape (M,)\n    width: scalar > 0\n    Returns: array of shape (len(u), M)\n    \"\"\"\n    u = u[:, None]  # (N, 1)\n    centers = centers[None, :]  # (1, M)\n    return np.exp(-0.5 * ((u - centers) ** 2) / (width ** 2))\n\ndef build_design_matrix(t, taus, centers, width):\n    \"\"\"\n    Construct design matrix X with columns ordered by component then basis.\n    t: array of shape (N,), absolute time samples\n    taus: array of shape (K,), component latencies\n    centers: array of shape (M,), basis centers (relative to latency)\n    width: scalar > 0, Gaussian width\n    Returns: X of shape (N, K*M)\n    \"\"\"\n    N = t.size\n    K = len(taus)\n    M = len(centers)\n    X = np.zeros((N, K * M), dtype=float)\n    for k, tau in enumerate(taus):\n        # offsets relative to component latency\n        u = t - tau  # (N,)\n        Phi = gaussian_basis(u, centers, width)  # (N, M)\n        # place into design matrix\n        start = k * M\n        X[:, start:start + M] = Phi\n    return X\n\ndef synthesize_data(X, beta_true, sigma, seed):\n    \"\"\"\n    Generate y = X beta_true + epsilon, epsilon ~ N(0, sigma^2 I)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    noise = rng.normal(loc=0.0, scale=sigma, size=X.shape[0])\n    return X @ beta_true + noise\n\ndef ridge_estimate(X, y, lam):\n    \"\"\"\n    Compute ridge estimate: (X^T X + lam I)^{-1} X^T y\n    \"\"\"\n    XT = X.T\n    A = XT @ X\n    if lam > 0:\n        A = A + lam * np.eye(A.shape[0])\n    b = XT @ y\n    # Solve A beta = b\n    beta_hat = np.linalg.solve(A, b)\n    return beta_hat\n\ndef max_abs_error(beta_hat, beta_true):\n    return float(np.max(np.abs(beta_hat - beta_true)))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"fs\": 1000.0,\n            \"T\": 0.300,\n            \"taus\": [0.080, 0.140],\n            \"centers\": [-0.012, 0.000, 0.012],\n            \"width\": 0.008,\n            \"beta_true\": [4.0, -2.0, 1.0, -3.0, 2.0, -1.5],\n            \"sigma\": 0.200,\n            \"lam\": 0.0,\n            \"seed\": 42\n        },\n        {\n            \"fs\": 1000.0,\n            \"T\": 0.300,\n            \"taus\": [0.100, 0.120],\n            \"centers\": [-0.012, 0.000, 0.012],\n            \"width\": 0.008,\n            \"beta_true\": [3.5, -1.0, 0.5, 2.0, -2.5, 1.0],\n            \"sigma\": 0.500,\n            \"lam\": 0.1,\n            \"seed\": 43\n        },\n        {\n            \"fs\": 1000.0,\n            \"T\": 0.300,\n            \"taus\": [0.090, 0.110, 0.130],\n            \"centers\": [-0.018, -0.006, 0.006, 0.018],\n            \"width\": 0.010,\n            \"beta_true\": [2.2, -1.2, 0.6, -0.1, -1.5, 2.8, -2.0, 0.7, 0.8, -1.0, 1.2, -0.5],\n            \"sigma\": 0.700,\n            \"lam\": 0.5,\n            \"seed\": 44\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        fs = case[\"fs\"]\n        T = case[\"T\"]\n        # Time vector inclusive of both endpoints\n        N = int(np.floor(fs * T)) + 1\n        t = np.linspace(0.0, T, N)\n        taus = np.array(case[\"taus\"], dtype=float)\n        centers = np.array(case[\"centers\"], dtype=float)\n        width = float(case[\"width\"])\n        beta_true = np.array(case[\"beta_true\"], dtype=float)\n        sigma = float(case[\"sigma\"])\n        lam = float(case[\"lam\"])\n        seed = int(case[\"seed\"])\n\n        # Build design matrix\n        X = build_design_matrix(t, taus, centers, width)\n\n        # Synthesize data\n        y = synthesize_data(X, beta_true, sigma, seed)\n\n        # Estimate coefficients\n        beta_hat = ridge_estimate(X, y, lam)\n\n        # Compute metric\n        err = max_abs_error(beta_hat, beta_true)\n        results.append(err)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}