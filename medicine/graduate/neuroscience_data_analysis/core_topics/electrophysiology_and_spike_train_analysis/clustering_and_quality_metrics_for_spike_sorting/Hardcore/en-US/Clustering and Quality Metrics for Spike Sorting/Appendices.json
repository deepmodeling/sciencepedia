{
    "hands_on_practices": [
        {
            "introduction": "The ultimate test of a spike sorting algorithm is to compare its output against a known ground truth, a scenario often realized in recordings that combine extracellular probes with simultaneous intracellular or juxtacellular recordings from a single neuron. This practice provides the foundational tools for such a comparison. By constructing a confusion matrix, you will learn to calculate per-unit precision and recall, allowing you to quantify an algorithm's accuracy and diagnose common failure modes like cluster merges (false positives) and splits (false negatives) .",
            "id": "4146355",
            "problem": "A spike sorting algorithm produced three sorted units, denoted $U_1$, $U_2$, and $U_3$. A simultaneous intracellular recording provides ground truth labels for three neurons, denoted $GT_1$, $GT_2$, and $GT_3$. Each detected spike has been matched to a ground truth neuron or deemed unmatched (noise/background), yielding the following counts:\n\n- For $U_1$: $100$ spikes match $GT_1$, $45$ spikes match $GT_2$, $0$ spikes match $GT_3$, and $5$ spikes are unmatched.\n- For $U_2$: $5$ spikes match $GT_1$, $0$ spikes match $GT_2$, $65$ spikes match $GT_3$, and $0$ spikes are unmatched.\n- For $U_3$: $0$ spikes match $GT_1$, $55$ spikes match $GT_2$, $5$ spikes match $GT_3$, and $30$ spikes are unmatched.\n\nThe total number of ground truth spikes for each neuron is: $GT_1$ has $120$ spikes, $GT_2$ has $100$ spikes, and $GT_3$ has $80$ spikes.\n\nUsing the standard evaluation framework for spike sorting quality:\n- Construct the confusion matrix $M$ with rows indexed by sorted units $\\{U_1,U_2,U_3\\}$ and columns indexed by ground truth labels $\\{GT_1,GT_2,GT_3,\\text{None}\\}$, where $\\text{None}$ indicates unmatched spikes.\n- For each sorted unit $U_i$, identify its dominant ground truth label as the column with maximal count in its row, and compute the per-unit precision and per-unit recall with respect to that dominant label. Interpret how merges (one sorted unit containing spikes from multiple ground truth neurons) and splits (one ground truth neuron’s spikes distributed across multiple sorted units) affect these metrics.\n\nSelect all statements that are correct:\n\nA. The row of the confusion matrix $M$ corresponding to $U_1$ is $(100,\\,45,\\,0,\\,5)$, and the per-unit precision and recall for $U_1$ with respect to $GT_1$ are $100/150$ and $100/120$, respectively.\n\nB. $GT_3$ is split across $U_2$ and $U_3$, which lowers the per-unit recall of $U_2$ relative to the total $GT_3$ spikes; for $U_2$, the per-unit precision and recall with respect to $GT_3$ are $65/70$ and $65/80$, respectively.\n\nC. $U_1$ represents a merge of $GT_1$ and $GT_2$, so its per-unit precision with respect to $GT_1$ is high; specifically, it equals $100/120$.\n\nD. The per-unit recall for $U_3$ with respect to $GT_2$ is $55/90$.\n\nE. Considering all sorted units together, the aggregate recall for $GT_1$ is $105/120$.",
            "solution": "The problem statement provides data regarding the performance of a spike sorting algorithm and asks for an evaluation of several statements about quality metrics. The problem is scientifically grounded, well-posed, and internally consistent.\n\nFirst, we formalize the problem by validating the setup and defining the necessary terms.\n\nThe givens are:\nSorted units: $U_1$, $U_2$, $U_3$.\nGround truth neurons: $GT_1$, $GT_2$, $GT_3$.\n\nThe counts of matched and unmatched spikes for each sorted unit are:\n- For $U_1$: $100$ spikes match $GT_1$, $45$ spikes match $GT_2$, $0$ spikes match $GT_3$, and $5$ spikes are unmatched.\n- For $U_2$: $5$ spikes match $GT_1$, $0$ spikes match $GT_2$, $65$ spikes match $GT_3$, and $0$ spikes are unmatched.\n- For $U_3$: $0$ spikes match $GT_1$, $55$ spikes match $GT_2$, $5$ spikes match $GT_3$, and $30$ spikes are unmatched.\n\nThe total number of ground truth spikes for each neuron is:\n- $N_{GT_1} = 120$\n- $N_{GT_2} = 100$\n- $N_{GT_3} = 80$\n\nThe task is to construct the confusion matrix $M$ and evaluate statements about per-unit precision and recall.\n\nThe confusion matrix $M$ has rows indexed by sorted units $\\{U_1, U_2, U_3\\}$ and columns by ground truth labels $\\{GT_1, GT_2, GT_3, \\text{None}\\}$. The entry $M_{ij}$ is the number of spikes in unit $U_i$ that match ground truth neuron $GT_j$.\n\nBased on the provided data, the confusion matrix $M$ is:\n$$ M = \\begin{pmatrix} 100  45  0  5 \\\\ 5  0  65  0 \\\\ 0  55  5  30 \\end{pmatrix} $$\n\nThe total number of spikes in each sorted unit is the sum of its corresponding row in $M$:\n- Total spikes in $U_1$: $N_{U_1} = 100 + 45 + 0 + 5 = 150$\n- Total spikes in $U_2$: $N_{U_2} = 5 + 0 + 65 + 0 = 70$\n- Total spikes in $U_3$: $N_{U_3} = 0 + 55 + 5 + 30 = 90$\n\nFor a given sorted unit $U_i$, we identify its dominant ground truth label $GT_j$ as the one with the highest count in the $i$-th row of $M$ (ignoring the 'None' column).\n- For $U_1$, the dominant label is $GT_1$ ($100  45$).\n- For $U_2$, the dominant label is $GT_3$ ($65  5$).\n- For $U_3$, the dominant label is $GT_2$ ($55  5$).\n\nWe now define per-unit precision and recall with respect to the dominant ground truth label $GT_j$ for a unit $U_i$:\n- **Precision**: The fraction of spikes in $U_i$ that are correctly from $GT_j$.\n  $$ \\text{Precision}(U_i, GT_j) = \\frac{\\text{Number of spikes in } U_i \\text{ from } GT_j}{\\text{Total number of spikes in } U_i} = \\frac{M_{ij}}{N_{U_i}} $$\n- **Recall**: The fraction of all spikes from $GT_j$ that are correctly captured in $U_i$.\n  $$ \\text{Recall}(U_i, GT_j) = \\frac{\\text{Number of spikes in } U_i \\text{ from } GT_j}{\\text{Total number of spikes from } GT_j} = \\frac{M_{ij}}{N_{GT_j}} $$\n\nNow we evaluate each option.\n\n**A. The row of the confusion matrix $M$ corresponding to $U_1$ is $(100,\\,45,\\,0,\\,5)$, and the per-unit precision and recall for $U_1$ with respect to $GT_1$ are $100/150$ and $100/120$, respectively.**\nThe row for $U_1$ is indeed $(100, 45, 0, 5)$ from the problem description. The dominant ground truth label for $U_1$ is $GT_1$.\n- Total spikes in $U_1$ is $N_{U_1} = 150$.\n- Total spikes from $GT_1$ is $N_{GT_1} = 120$.\n- The number of spikes in $U_1$ from $GT_1$ is $M_{11} = 100$.\n- Precision for $U_1$ w.r.t. $GT_1$ is $\\frac{M_{11}}{N_{U_1}} = \\frac{100}{150}$.\n- Recall for $U_1$ w.r.t. $GT_1$ is $\\frac{M_{11}}{N_{GT_1}} = \\frac{100}{120}$.\nBoth parts of the statement are arithmetically and conceptually correct. The presence of $45$ spikes from $GT_2$ and $5$ unmatched spikes reduces precision, demonstrating a \"merge\" error. The failure to capture all $120$ spikes of $GT_1$ reduces recall.\n**Verdict: Correct.**\n\n**B. $GT_3$ is split across $U_2$ and $U_3$, which lowers the per-unit recall of $U_2$ relative to the total $GT_3$ spikes; for $U_2$, the per-unit precision and recall with respect to $GT_3$ are $65/70$ and $65/80$, respectively.**\nFirst, we check if $GT_3$ is split.\n- $U_2$ contains $65$ spikes from $GT_3$.\n- $U_3$ contains $5$ spikes from $GT_3$.\nSince spikes from the single ground truth neuron $GT_3$ are found in multiple sorted units, it is by definition \"split\". This split, along with any missed spikes (total found for $GT_3$ is $65+5=70$, out of $80$), means that no single sorted unit captures all of $GT_3$'s spikes, thus lowering the per-unit recall for any associated unit. The reasoning is sound.\nNext, we check the calculations for $U_2$. The dominant label for $U_2$ is $GT_3$.\n- Total spikes in $U_2$ is $N_{U_2} = 70$.\n- Total spikes from $GT_3$ is $N_{GT_3} = 80$.\n- The number of spikes in $U_2$ from $GT_3$ is $M_{23} = 65$.\n- Precision for $U_2$ w.r.t. $GT_3$ is $\\frac{M_{23}}{N_{U_2}} = \\frac{65}{70}$.\n- Recall for $U_2$ w.r.t. $GT_3$ is $\\frac{M_{23}}{N_{GT_3}} = \\frac{65}{80}$.\nThe reasoning and the calculated values are correct.\n**Verdict: Correct.**\n\n**C. $U_1$ represents a merge of $GT_1$ and $GT_2$, so its per-unit precision with respect to $GT_1$ is high; specifically, it equals $100/120$.**\n$U_1$ contains $100$ spikes from $GT_1$ and $45$ from $GT_2$. This is a \"merge\". A merge error inherently *lowers* precision because the denominator of the precision formula (total spikes in the unit) is inflated by false positives (spikes from other neurons). The reasoning \"so its per-unit precision... is high\" is flawed. Furthermore, the statement claims the precision is $\\frac{100}{120}$.\n- As calculated for option A, the precision for $U_1$ w.r.t. $GT_1$ is $\\frac{100}{150}$.\n- The value $\\frac{100}{120}$ is the recall, not the precision.\nThe statement contains flawed reasoning and an incorrect value for precision.\n**Verdict: Incorrect.**\n\n**D. The per-unit recall for $U_3$ with respect to $GT_2$ is $55/90$.**\nThe dominant ground truth label for $U_3$ is $GT_2$. We calculate the recall for $U_3$ w.r.t. $GT_2$.\n- Total spikes from $GT_2$ is $N_{GT_2} = 100$.\n- The number of spikes in $U_3$ from $GT_2$ is $M_{32} = 55$.\n- Recall for $U_3$ w.r.t. $GT_2$ is $\\frac{M_{32}}{N_{GT_2}} = \\frac{55}{100}$.\nThe statement claims the recall is $\\frac{55}{90}$. Let's analyze this value. The total number of spikes in $U_3$ is $N_{U_3} = 90$. Therefore, $\\frac{55}{90}$ is the *precision* of $U_3$ w.r.t. $GT_2$, not the recall. The statement misidentifies precision as recall.\n**Verdict: Incorrect.**\n\n**E. Considering all sorted units together, the aggregate recall for $GT_1$ is $105/120$.**\nAggregate recall for a ground truth neuron $GT_j$ is the total number of its spikes detected across all sorted units, divided by the total number of spikes for that neuron.\n- Spikes from $GT_1$ found in $U_1$: $100$.\n- Spikes from $GT_1$ found in $U_2$: $5$.\n- Spikes from $GT_1$ found in $U_3$: $0$.\n- Total spikes from $GT_1$ detected by the algorithm = $100 + 5 + 0 = 105$.\n- The total number of actual spikes from $GT_1$ is $N_{GT_1} = 120$.\n- Aggregate recall for $GT_1$ = $\\frac{\\text{Total } GT_1 \\text{ spikes detected}}{\\text{Total } GT_1 \\text{ spikes}} = \\frac{105}{120}$.\nThe statement is correct.\n**Verdict: Correct.**\n\nIn summary, statements A, B, and E are correct.",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "In most neuroscience experiments, ground truth is unavailable, forcing us to rely on quality metrics derived from the data itself. A fundamental metric is the Signal-to-Noise Ratio (SNR), which quantifies how clearly a unit's spikes stand out from the background electrical noise. This exercise explores a critical challenge in calculating SNR: robustly estimating the noise level in the presence of artifacts and outliers, demonstrating how the choice of statistical estimator can dramatically impact the assessment of unit quality .",
            "id": "4146368",
            "problem": "A single-unit cluster extracted from extracellular recordings is evaluated by a Signal-to-Noise Ratio (SNR) metric to assess unit quality. Let the unit’s template peak amplitude be $A = 5\\,\\mu\\mathrm{V}$. The SNR is defined as $\\,\\mathrm{SNR} = A / \\sigma_{n}\\,$, where $\\sigma_{n}$ is a scalar estimate of the baseline noise scale on the same channel.\n\nYou have a long stretch of baseline (spike-free) noise from which to estimate $\\sigma_{n}$. Due to occasional motion artifacts, the noise is contaminated and can be modeled as a symmetric two-component mixture: with probability $(1-\\varepsilon)$, samples are drawn from a zero-mean Gaussian with variance $\\sigma^{2}$, and with probability $\\varepsilon$, samples are drawn from a zero-mean Gaussian with larger variance $\\sigma_{o}^{2}$. Assume $\\varepsilon = 0.05$, $\\sigma = 1\\,\\mu\\mathrm{V}$, and $\\sigma_{o} = 10\\,\\mu\\mathrm{V}$.\n\nConsider two population-level noise estimators:\n- The standard deviation estimator $\\sigma_{\\mathrm{sd}}$, defined by $\\sigma_{\\mathrm{sd}} = \\sqrt{\\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2}}$ for the mixture.\n- The scaled Median Absolute Deviation (MAD) estimator $\\sigma_{\\mathrm{mad}}$, defined as $\\sigma_{\\mathrm{mad}} = c \\cdot \\mathrm{median}(|X - \\mathrm{median}(X)|)$ with $c = 1/\\Phi^{-1}(0.75) \\approx 1.4826$ to be Fisher-consistent for a zero-mean Gaussian with standard deviation $\\sigma$, where $\\Phi$ is the standard normal cumulative distribution function. For a symmetric zero-mean distribution, $\\mathrm{median}(X) = 0$, so $\\mathrm{MAD} = \\mathrm{median}(|X|)$.\n\nUsing only the core definitions above, and reasoning about population quantities for the specified mixture, determine which statement is most accurate about the relative robustness of $\\sigma_{\\mathrm{mad}}$ versus $\\sigma_{\\mathrm{sd}}$ under the given contamination and the resulting SNR values for the unit. Choose the single best option.\n\nA. Under the $5\\%$ contamination by a broad component, the population standard deviation is inflated to approximately $\\sigma_{\\mathrm{sd}} \\approx 2.44\\,\\mu\\mathrm{V}$, yielding $\\mathrm{SNR}_{\\mathrm{sd}} \\approx 2.05$. In contrast, the scaled MAD changes only slightly to approximately $\\sigma_{\\mathrm{mad}} \\approx 1.06\\,\\mu\\mathrm{V}$, yielding $\\mathrm{SNR}_{\\mathrm{mad}} \\approx 4.7$, close to the uncontaminated value $A/\\sigma = 5$. This illustrates the robustness of MAD to outliers relative to the standard deviation.\n\nB. Because the contamination is symmetric and zero-mean, both $\\sigma_{\\mathrm{sd}}$ and $\\sigma_{\\mathrm{mad}}$ are inflated by essentially the same factor, so both SNR estimates are approximately $\\mathrm{SNR} \\approx 2.05$.\n\nC. The Median Absolute Deviation (MAD) has a breakdown point of $50\\%$, which implies that even $5\\%$ contamination causes it to fail catastrophically, underestimating $\\sigma_{n}$ and overestimating SNR.\n\nD. The standard deviation is unbiased for any zero-mean distribution, so $\\sigma_{\\mathrm{sd}} \\approx 1\\,\\mu\\mathrm{V}$ despite the outliers, and both SNR estimates are approximately $\\mathrm{SNR} \\approx 5$.",
            "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded in the principles of signal processing and robust statistics as applied to neuroscience, is well-posed, objective, and contains all necessary information to derive a unique solution.\n\nThe problem requires the calculation of two different population-level estimators of noise scale, $\\sigma_{\\mathrm{sd}}$ and $\\sigma_{\\mathrm{mad}}$, for a specified mixture distribution, and then to compare the resulting Signal-to-Noise Ratios (SNRs).\n\nThe noise is modeled as a random variable $X$ drawn from a mixture of two zero-mean Gaussian distributions:\n- $X \\sim \\mathcal{N}(0, \\sigma^2)$ with probability $(1-\\varepsilon)$, where $\\sigma = 1\\,\\mu\\mathrm{V}$.\n- $X \\sim \\mathcal{N}(0, \\sigma_o^2)$ with probability $\\varepsilon$, where $\\sigma_o = 10\\,\\mu\\mathrm{V}$.\nThe contamination fraction is $\\varepsilon = 0.05$. The signal amplitude is $A = 5\\,\\mu\\mathrm{V}$.\n\n**1. Calculation of the Standard Deviation Estimator ($\\sigma_{\\mathrm{sd}}$)**\n\nThe estimator $\\sigma_{\\mathrm{sd}}$ is the population standard deviation of the mixture distribution. It is defined as $\\sigma_{\\mathrm{sd}} = \\sqrt{\\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2}}$.\n\nFirst, we calculate the mean of the mixture, $\\mathbb{E}[X]$. By the law of total expectation:\n$$ \\mathbb{E}[X] = (1-\\varepsilon) \\cdot \\mathbb{E}[\\mathcal{N}(0, \\sigma^2)] + \\varepsilon \\cdot \\mathbb{E}[\\mathcal{N}(0, \\sigma_o^2)] = (1-\\varepsilon) \\cdot 0 + \\varepsilon \\cdot 0 = 0 $$\nThe mixture distribution is symmetric and has a mean of $0$.\n\nNext, we calculate the second moment, $\\mathbb{E}[X^2]$.\n$$ \\mathbb{E}[X^2] = (1-\\varepsilon) \\cdot \\mathbb{E}[(\\mathcal{N}(0, \\sigma^2))^2] + \\varepsilon \\cdot \\mathbb{E}[(\\mathcal{N}(0, \\sigma_o^2))^2] $$\nFor a zero-mean random variable, the second moment is equal to its variance. Thus, $\\mathbb{E}[(\\mathcal{N}(0, \\sigma^2))^2] = \\sigma^2$ and $\\mathbb{E}[(\\mathcal{N}(0, \\sigma_o^2))^2] = \\sigma_o^2$.\nPlugging in the given values:\n$$ \\mathbb{E}[X^2] = (1 - 0.05) \\cdot (1\\,\\mu\\mathrm{V})^2 + 0.05 \\cdot (10\\,\\mu\\mathrm{V})^2 $$\n$$ \\mathbb{E}[X^2] = 0.95 \\cdot 1\\,\\mu\\mathrm{V}^2 + 0.05 \\cdot 100\\,\\mu\\mathrm{V}^2 = 0.95\\,\\mu\\mathrm{V}^2 + 5\\,\\mu\\mathrm{V}^2 = 5.95\\,\\mu\\mathrm{V}^2 $$\nThe variance of the mixture is $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbbE[X])^2 = 5.95 - 0^2 = 5.95\\,\\mu\\mathrm{V}^2$.\nThe standard deviation estimator is therefore:\n$$ \\sigma_{\\mathrm{sd}} = \\sqrt{5.95\\,\\mu\\mathrm{V}^2} \\approx 2.4393\\,\\mu\\mathrm{V} $$\nRounding to two decimal places, we get $\\sigma_{\\mathrm{sd}} \\approx 2.44\\,\\mu\\mathrm{V}$.\n\nThe SNR calculated using this estimator is:\n$$ \\mathrm{SNR}_{\\mathrm{sd}} = \\frac{A}{\\sigma_{\\mathrm{sd}}} = \\frac{5\\,\\mu\\mathrm{V}}{2.4393\\,\\mu\\mathrm{V}} \\approx 2.0498 $$\nRounding to two decimal places, we get $\\mathrm{SNR}_{\\mathrm{sd}} \\approx 2.05$.\n\n**2. Calculation of the Scaled MAD Estimator ($\\sigma_{\\mathrm{mad}}$)**\n\nThe estimator $\\sigma_{\\mathrm{mad}}$ is defined as $\\sigma_{\\mathrm{mad}} = c \\cdot \\mathrm{median}(|X - \\mathrm{median}(X)|)$, where $c \\approx 1.4826$.\nSince the mixture distribution is symmetric and zero-mean, $\\mathrm{median}(X) = 0$. The definition simplifies to $\\sigma_{\\mathrm{mad}} = c \\cdot \\mathrm{median}(|X|)$.\nLet $m = \\mathrm{median}(|X|)$. By definition, $m$ is the value such that $P(|X| \\le m) = 0.5$.\nThe cumulative distribution function (CDF) of $|X|$ is $F_{|X|}(y) = P(|X| \\le y)$.\n$$ P(|X| \\le y) = (1-\\varepsilon)P(|X_1| \\le y) + \\varepsilon P(|X_2| \\le y) $$\nwhere $X_1 \\sim \\mathcal{N}(0, \\sigma^2)$ and $X_2 \\sim \\mathcal{N}(0, \\sigma_o^2)$.\nThe probability $P(|Z| \\le y)$ for a $Z \\sim \\mathcal{N}(0,s^2)$ variable is $2\\Phi(y/s) - 1$, where $\\Phi$ is the standard normal CDF.\nWe need to solve for $m$:\n$$ (1-\\varepsilon) \\left(2\\Phi\\left(\\frac{m}{\\sigma}\\right) - 1\\right) + \\varepsilon \\left(2\\Phi\\left(\\frac{m}{\\sigma_o}\\right) - 1\\right) = 0.5 $$\nPlugging in the values $\\varepsilon=0.05$, $\\sigma=1$, $\\sigma_o=10$:\n$$ 0.95 \\left(2\\Phi(m) - 1\\right) + 0.05 \\left(2\\Phi\\left(\\frac{m}{10}\\right) - 1\\right) = 0.5 $$\nThis equation must be solved for $m$ numerically. Let's test the value implied by option A.\nOption A suggests $\\sigma_{\\mathrm{mad}} \\approx 1.06\\,\\mu\\mathrm{V}$. This would imply $m = \\sigma_{\\mathrm{mad}}/c \\approx 1.06 / 1.4826 \\approx 0.715\\,\\mu\\mathrm{V}$.\nLet's check if $m=0.715$ satisfies the equation:\n$$ 0.95 \\left(2\\Phi(0.715) - 1\\right) + 0.05 \\left(2\\Phi(0.0715) - 1\\right) $$\nUsing standard normal tables or calculator, $\\Phi(0.715) \\approx 0.7627$ and $\\Phi(0.0715) \\approx 0.5285$.\n$$ 0.95(2 \\cdot 0.7627 - 1) + 0.05(2 \\cdot 0.5285 - 1) \\approx 0.95(0.5254) + 0.05(0.057) $$\n$$ \\approx 0.49913 + 0.00285 = 0.50198 $$\nThis value is extremely close to $0.5$, confirming that $m \\approx 0.715\\,\\mu\\mathrm{V}$ is an excellent approximation for the median of $|X|$.\nUsing this median, the estimator is:\n$$ \\sigma_{\\mathrm{mad}} = c \\cdot m \\approx 1.4826 \\cdot 0.715\\,\\mu\\mathrm{V} \\approx 1.060\\,\\mu\\mathrm{V} $$\nSo, $\\sigma_{\\mathrm{mad}} \\approx 1.06\\,\\mu\\mathrm{V}$ is correct.\n\nThe SNR calculated using this estimator is:\n$$ \\mathrm{SNR}_{\\mathrm{mad}} = \\frac{A}{\\sigma_{\\mathrm{mad}}} = \\frac{5\\,\\mu\\mathrm{V}}{1.06\\,\\mu\\mathrm{V}} \\approx 4.717 $$\nRounding to one decimal place, we get $\\mathrm{SNR}_{\\mathrm{mad}} \\approx 4.7$.\n\nFor comparison, in the uncontaminated case ($\\varepsilon=0$), the noise is purely $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma=1\\,\\mu\\mathrm{V}$. The true noise scale is $\\sigma_n = 1\\,\\mu\\mathrm{V}$, and the SNR would be $A/\\sigma = 5/1 = 5$. The MAD estimator, being Fisher-consistent for a Gaussian, would also give $\\sigma_{\\mathrm{mad}} = 1\\,\\mu\\mathrm{V}$.\nThe contamination inflates $\\sigma_{\\mathrm{sd}}$ from $1\\,\\mu\\mathrm{V}$ to $2.44\\,\\mu\\mathrm{V}$ (a $144\\%$ increase), while it only inflates $\\sigma_{\\mathrmmad}$ from $1\\,\\mu\\mathrm{V}$ to $1.06\\,\\mu\\mathrm{V}$ (a $6\\%$ increase).\n\n**Option-by-Option Analysis**\n\n**A. Under the $5\\%$ contamination by a broad component, the population standard deviation is inflated to approximately $\\sigma_{\\mathrm{sd}} \\approx 2.44\\,\\mu\\mathrm{V}$, yielding $\\mathrm{SNR}_{\\mathrm{sd}} \\approx 2.05$. In contrast, the scaled MAD changes only slightly to approximately $\\sigma_{\\mathrm{mad}} \\approx 1.06\\,\\mu\\mathrm{V}$, yielding $\\mathrm{SNR}_{\\mathrm{mad}} \\approx 4.7$, close to the uncontaminated value $A/\\sigma = 5$. This illustrates the robustness of MAD to outliers relative to the standard deviation.**\n- Our calculation yielded $\\sigma_{\\mathrm{sd}} \\approx 2.44\\,\\mu\\mathrm{V}$ and $\\mathrm{SNR}_{\\mathrm{sd}} \\approx 2.05$. This matches.\n- Our calculation yielded $\\sigma_{\\mathrm{mad}} \\approx 1.06\\,\\mu\\mathrm{V}$ and $\\mathrm{SNR}_{\\mathrm{mad}} \\approx 4.7$. This matches.\n- The value $\\mathrm{SNR}_{\\mathrm{mad}} \\approx 4.7$ is indeed close to the uncontaminated value of $5$, while $\\mathrm{SNR}_{\\mathrm{sd}} \\approx 2.05$ is not.\n- The conclusion that this demonstrates the robustness of MAD is the correct interpretation of the results. The MAD estimator is far less sensitive to the $5\\%$ of high-variance outliers.\n- **Verdict: Correct.**\n\n**B. Because the contamination is symmetric and zero-mean, both $\\sigma_{\\mathrm{sd}}$ and $\\sigma_{\\mathrm{mad}}$ are inflated by essentially the same factor, so both SNR estimates are approximately $\\mathrm{SNR} \\approx 2.05$.**\n- This is factually incorrect. Our calculations show that $\\sigma_{\\mathrm{sd}}$ is inflated by a factor of $\\approx 2.44$, while $\\sigma_{\\mathrm{mad}}$ is inflated by a factor of $\\approx 1.06$. These are not \"essentially the same\". The resulting SNR values are $\\approx 2.05$ and $\\approx 4.7$, which are also very different. Symmetry of the contamination does not imply equal impact on different scale estimators.\n- **Verdict: Incorrect.**\n\n**C. The Median Absolute Deviation (MAD) has a breakdown point of $50\\%$, which implies that even $5\\%$ contamination causes it to fail catastrophically, underestimating $\\sigma_{n}$ and overestimating SNR.**\n- This statement misinterprets the meaning of a breakdown point. A breakdown point of $50\\%$ is the highest possible and indicates extreme robustness. It means the estimator can tolerate up to $50\\%$ contamination. A $5\\%$ contamination level is well within the regime where MAD is expected to perform robustly, not \"fail catastrophically\". Our calculations confirm this: the MAD estimate remains close to the ideal value.\n- **Verdict: Incorrect.**\n\n**D. The standard deviation is unbiased for any zero-mean distribution, so $\\sigma_{\\mathrm{sd}} \\approx 1\\,\\mu\\mathrm{V}$ despite the outliers, and both SNR estimates are approximately $\\mathrm{SNR} \\approx 5$.**\n- The claim that the population standard deviation of the mixture is $\\approx 1\\,\\mu\\mathrm{V}$ is false. Our calculation shows it is $\\approx 2.44\\,\\mu\\mathrm{V}$. The standard deviation is, by its nature (based on squared deviations), highly sensitive to outliers, so it is expected to increase significantly. The premise is false, and therefore the conclusion that the SNR is $\\approx 5$ is also false.\n- **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A high-quality neural unit must not only be distinct from background noise but also well-isolated from the activity of other neurons. This practice moves beyond simple SNR to a more sophisticated metric of cluster purity, the L-ratio, which quantifies the degree of contamination from spikes outside the cluster. By assuming a statistical model for the cluster's shape in feature space, you will calculate the probability that nearby, unassigned spikes are actually 'contaminants' that should have been excluded .",
            "id": "4146418",
            "problem": "You are given a set of spike waveform feature distances and cluster statistics used in spike sorting quality assessment. Assume feature vectors of a single candidate cluster are generated by a Multivariate Normal distribution with mean $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}_{C}$. Under this model, the squared Mahalanobis distance $d^{2}$ of a sample from the cluster follows a Chi-Square distribution with $p$ degrees of freedom, where $p$ is the dimensionality of the feature space and equals the size of $\\boldsymbol{\\Sigma}_{C}$.\n\nYour task is to compute, for each test case, a numerical contamination likelihood proxy known as the L-ratio. For a list of Mahalanobis distances $\\{d_{i}\\}$ computed for spikes not assigned to the cluster, the contamination likelihood proxy requires, for each $d_{i}$, evaluating the probability that a true cluster member would have a squared Mahalanobis distance greater than or equal to $d_{i}^{2}$ under the Chi-Square distribution with $p$ degrees of freedom. Summing these probabilities over all non-cluster spikes produces a quantity that, when normalized by the number of spikes assigned to the cluster, $N_{\\mathrm{cluster}}$, provides a contamination likelihood proxy. You must implement this computation using the Chi-Square survival function (the complement of the Cumulative Distribution Function (CDF)) to ensure numerical stability.\n\nScientific assumptions:\n- The cluster features follow a Multivariate Normal distribution.\n- The squared Mahalanobis distance of true cluster members follows a Chi-Square distribution with $p$ degrees of freedom, where $p$ equals the dimensionality of $\\boldsymbol{\\Sigma}_{C}$.\n- The non-cluster Mahalanobis distances $\\{d_{i}\\}$ are computed relative to the cluster mean and covariance $\\boldsymbol{\\Sigma}_{C}$.\n\nFor each test case, compute the L-ratio as a single real number and round it to $6$ decimal places. No physical units are involved. The final output must aggregate results from all test cases in a single line as a comma-separated list enclosed in square brackets, for example, $[r_{1},r_{2},r_{3}]$, where each $r_{i}$ is the rounded L-ratio for test case $i$.\n\nTest suite:\n- Test case $1$:\n  - Cluster covariance $\\boldsymbol{\\Sigma}_{C} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$.\n  - Non-cluster Mahalanobis distances $[2.0, 3.0, 5.0]$.\n  - Number of spikes assigned to the cluster $N_{\\mathrm{cluster}} = 150$.\n- Test case $2$:\n  - Cluster covariance $\\boldsymbol{\\Sigma}_{C} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  0.8 \\end{bmatrix}$.\n  - Non-cluster Mahalanobis distances $[0.1, 0.2, 0.05, 0.3]$.\n  - Number of spikes assigned to the cluster $N_{\\mathrm{cluster}} = 50$.\n- Test case $3$:\n  - Cluster covariance $\\boldsymbol{\\Sigma}_{C} = \\begin{bmatrix} 2.0  0.0 \\\\ 0.0  2.0 \\end{bmatrix}$.\n  - Non-cluster Mahalanobis distances $[10.0, 12.0]$.\n  - Number of spikes assigned to the cluster $N_{\\mathrm{cluster}} = 200$.\n- Test case $4$:\n  - Cluster covariance $\\boldsymbol{\\Sigma}_{C} = \\begin{bmatrix} 1.0 \\end{bmatrix}$.\n  - Non-cluster Mahalanobis distances $[0.0, 10^{-8}]$.\n  - Number of spikes assigned to the cluster $N_{\\mathrm{cluster}} = 1$.\n\nYour program should produce a single line of output containing the L-ratio results for the four test cases as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places, in the exact format $[r_{1},r_{2},r_{3},r_{4}]$.",
            "solution": "The problem statement has been rigorously validated and found to be scientifically grounded, well-posed, and objective. It provides a complete and consistent set of definitions and data required to compute the L-ratio, a recognized quality metric in the field of spike sorting for neuroscience data analysis. The underlying statistical assumptions are standard and appropriate for the context. Therefore, a unique, stable, and meaningful solution can be derived.\n\nThe primary task is to calculate the L-ratio for several test cases. The L-ratio serves as a proxy for the contamination of a candidate spike cluster. It is defined based on the statistical properties of the cluster's feature distribution. The problem assumes that feature vectors of spikes belonging to a given cluster are samples from a Multivariate Normal distribution with mean $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}_{C}$. A fundamental property of this distribution is that the squared Mahalanobis distance, $d^2$, of a true member from the cluster center follows a Chi-Square ($\\chi^2$) distribution with $p$ degrees of freedom, where $p$ is the dimensionality of the feature space.\n\nThe L-ratio is formally defined as:\n$$\nL_{\\text{ratio}} = \\frac{1}{N_{\\text{cluster}}} \\sum_{i} P(\\chi^2_p \\ge d_i^2)\n$$\nIn this equation:\n- $N_{\\text{cluster}}$ is the total number of spikes assigned to the candidate cluster.\n- The summation is over all spikes that are *not* assigned to the cluster.\n- $\\{d_i\\}$ is the set of Mahalanobis distances of these non-cluster spikes, calculated relative to the candidate cluster's mean and covariance.\n- $p$ is the number of degrees of freedom for the Chi-Square distribution, which is equal to the dimensionality of the feature space, given by the size of the covariance matrix $\\boldsymbol{\\Sigma}_{C}$. For an $n \\times n$ matrix $\\boldsymbol{\\Sigma}_{C}$, we have $p=n$.\n- $P(\\chi^2_p \\ge d_i^2)$ is the probability that a value randomly drawn from a $\\chi^2$ distribution with $p$ degrees of freedom is greater than or equal to the squared Mahalanobis distance $d_i^2$. This probability corresponds to the survival function (SF) of the $\\chi^2$ distribution, which is the complement of the Cumulative Distribution Function (CDF), i.e., $S(x; p) = 1 - \\text{CDF}(x; p)$. Using the survival function directly is specified for better numerical precision, especially for large values of the argument where the CDF would be close to $1$.\n\nThe computational procedure for each test case is as follows:\n\n1.  **Determine Degrees of Freedom ($p$)**: Extract the dimensionality $p$ from the given cluster covariance matrix $\\boldsymbol{\\Sigma}_{C}$. If $\\boldsymbol{\\Sigma}_{C}$ is an $n \\times n$ matrix, then $p = n$.\n\n2.  **Calculate Probabilities**: For each non-cluster Mahalanobis distance $d_i$ in the provided list, first compute its square, $d_i^2$. Then, evaluate the Chi-Square survival function for this value with $p$ degrees of freedom, $S(d_i^2; p) = P(\\chi^2_p \\ge d_i^2)$.\n\n3.  **Sum and Normalize**: Sum all the probabilities calculated in the previous step: $\\text{Sum} = \\sum_i S(d_i^2; p)$.\n\n4.  **Compute L-ratio**: Divide the sum by the number of spikes in the cluster, $N_{\\text{cluster}}$, to obtain the final L-ratio: $L_{\\text{ratio}} = \\frac{\\text{Sum}}{N_{\\text{cluster}}}$.\n\n5.  **Round**: Round the computed L-ratio to $6$ decimal places as required.\n\nThis procedure will be applied to each of the four test cases.\n\n**Test Case 1:**\n- $\\boldsymbol{\\Sigma}_{C}$ is a $2 \\times 2$ matrix, so $p=2$.\n- Non-cluster Mahalanobis distances are $[2.0, 3.0, 5.0]$. The squared distances are $[4.0, 9.0, 25.0]$.\n- $N_{\\text{cluster}} = 150$.\n- $L_{\\text{ratio}} = \\frac{1}{150} \\left( P(\\chi^2_2 \\ge 4.0) + P(\\chi^2_2 \\ge 9.0) + P(\\chi^2_2 \\ge 25.0) \\right)$.\n\n**Test Case 2:**\n- $\\boldsymbol{\\Sigma}_{C}$ is a $3 \\times 3$ matrix, so $p=3$.\n- Non-cluster Mahalanobis distances are $[0.1, 0.2, 0.05, 0.3]$. The squared distances are $[0.01, 0.04, 0.0025, 0.09]$.\n- $N_{\\text{cluster}} = 50$.\n- $L_{\\text{ratio}} = \\frac{1}{50} \\left( P(\\chi^2_3 \\ge 0.01) + P(\\chi^2_3 \\ge 0.04) + P(\\chi^2_3 \\ge 0.0025) + P(\\chi^2_3 \\ge 0.09) \\right)$.\n\n**Test Case 3:**\n- $\\boldsymbol{\\Sigma}_{C}$ is a $2 \\times 2$ matrix, so $p=2$.\n- Non-cluster Mahalanobis distances are $[10.0, 12.0]$. The squared distances are $[100.0, 144.0]$.\n- $N_{\\text{cluster}} = 200$.\n- $L_{\\text{ratio}} = \\frac{1}{200} \\left( P(\\chi^2_2 \\ge 100.0) + P(\\chi^2_2 \\ge 144.0) \\right)$.\n\n**Test Case 4:**\n- $\\boldsymbol{\\Sigma}_{C}$ is a $1 \\times 1$ matrix, so $p=1$.\n- Non-cluster Mahalanobis distances are $[0.0, 10^{-8}]$. The squared distances are $[0.0, 10^{-16}]$.\n- $N_{\\text{cluster}} = 1$.\n- $L_{\\text{ratio}} = \\frac{1}{1} \\left( P(\\chi^2_1 \\ge 0.0) + P(\\chi^2_1 \\ge 10^{-16}) \\right)$. Note that $P(\\chi^2_p \\ge 0.0) = 1$ for any $p > 0$.\n\nThe implementation will use numerical libraries to evaluate the Chi-Square survival function accurately.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Computes the L-ratio for a set of test cases based on spike sorting statistics.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"Sigma_C\": np.array([[1.0, 0.0], [0.0, 0.5]]),\n            \"non_cluster_distances\": np.array([2.0, 3.0, 5.0]),\n            \"N_cluster\": 150\n        },\n        # Test case 2\n        {\n            \"Sigma_C\": np.array([[1.0, 0.2, 0.0], [0.2, 1.5, 0.1], [0.0, 0.1, 0.8]]),\n            \"non_cluster_distances\": np.array([0.1, 0.2, 0.05, 0.3]),\n            \"N_cluster\": 50\n        },\n        # Test case 3\n        {\n            \"Sigma_C\": np.array([[2.0, 0.0], [0.0, 2.0]]),\n            \"non_cluster_distances\": np.array([10.0, 12.0]),\n            \"N_cluster\": 200\n        },\n        # Test case 4\n        {\n            \"Sigma_C\": np.array([[1.0]]),\n            \"non_cluster_distances\": np.array([0.0, 1e-8]),\n            \"N_cluster\": 1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        Sigma_C = case[\"Sigma_C\"]\n        non_cluster_distances = case[\"non_cluster_distances\"]\n        N_cluster = case[\"N_cluster\"]\n\n        # The number of degrees of freedom p is the dimensionality of Sigma_C.\n        p = Sigma_C.shape[0]\n\n        # Square the Mahalanobis distances for non-cluster spikes.\n        squared_distances = non_cluster_distances**2\n\n        # For each squared distance, compute the probability P(X = d^2)\n        # using the Chi-Square survival function (1 - CDF).\n        # This is more numerically stable than 1 - cdf(x) for large x.\n        probabilities = chi2.sf(squared_distances, df=p)\n\n        # Sum these probabilities.\n        sum_of_probs = np.sum(probabilities)\n\n        # Normalize by the number of spikes in the cluster to get the L-ratio.\n        l_ratio = sum_of_probs / N_cluster\n\n        # Round the result to 6 decimal places.\n        rounded_l_ratio = round(l_ratio, 6)\n        results.append(rounded_l_ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}