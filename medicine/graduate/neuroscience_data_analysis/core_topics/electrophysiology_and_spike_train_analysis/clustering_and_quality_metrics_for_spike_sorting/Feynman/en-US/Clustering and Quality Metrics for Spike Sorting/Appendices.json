{
    "hands_on_practices": [
        {
            "introduction": "The ultimate test of a spike sorting algorithm is its performance against a known \"ground truth,\" often obtained from simultaneous intracellular or juxtacellular recordings. This exercise provides a practical scenario to quantify sorting accuracy using a confusion matrix. By calculating precision and recall, you will learn to diagnose common sorting errors such as \"merges\" (a single sorted unit containing spikes from multiple real neurons) and \"splits\" (a single real neuron's spikes being divided among multiple sorted units) .",
            "id": "4146355",
            "problem": "A spike sorting algorithm produced three sorted units, denoted $U_1$, $U_2$, and $U_3$. A simultaneous intracellular recording provides ground truth labels for three neurons, denoted $GT_1$, $GT_2$, and $GT_3$. Each detected spike has been matched to a ground truth neuron or deemed unmatched (noise/background), yielding the following counts:\n\n- For $U_1$: $100$ spikes match $GT_1$, $45$ spikes match $GT_2$, $0$ spikes match $GT_3$, and $5$ spikes are unmatched.\n- For $U_2$: $5$ spikes match $GT_1$, $0$ spikes match $GT_2$, $65$ spikes match $GT_3$, and $0$ spikes are unmatched.\n- For $U_3$: $0$ spikes match $GT_1$, $55$ spikes match $GT_2$, $5$ spikes match $GT_3$, and $30$ spikes are unmatched.\n\nThe total number of ground truth spikes for each neuron is: $GT_1$ has $120$ spikes, $GT_2$ has $100$ spikes, and $GT_3$ has $80$ spikes.\n\nUsing the standard evaluation framework for spike sorting quality:\n- Construct the confusion matrix $M$ with rows indexed by sorted units $\\{U_1,U_2,U_3\\}$ and columns indexed by ground truth labels $\\{GT_1,GT_2,GT_3,\\text{None}\\}$, where $\\text{None}$ indicates unmatched spikes.\n- For each sorted unit $U_i$, identify its dominant ground truth label as the column with maximal count in its row, and compute the per-unit precision and per-unit recall with respect to that dominant label. Interpret how merges (one sorted unit containing spikes from multiple ground truth neurons) and splits (one ground truth neuron’s spikes distributed across multiple sorted units) affect these metrics.\n\nSelect all statements that are correct:\n\nA. The row of the confusion matrix $M$ corresponding to $U_1$ is $(100,\\,45,\\,0,\\,5)$, and the per-unit precision and recall for $U_1$ with respect to $GT_1$ are $100/150$ and $100/120$, respectively.\n\nB. $GT_3$ is split across $U_2$ and $U_3$, which lowers the per-unit recall of $U_2$ relative to the total $GT_3$ spikes; for $U_2$, the per-unit precision and recall with respect to $GT_3$ are $65/70$ and $65/80$, respectively.\n\nC. $U_1$ represents a merge of $GT_1$ and $GT_2$, so its per-unit precision with respect to $GT_1$ is high; specifically, it equals $100/120$.\n\nD. The per-unit recall for $U_3$ with respect to $GT_2$ is $55/90$.\n\nE. Considering all sorted units together, the aggregate recall for $GT_1$ is $105/120$.",
            "solution": "The problem statement provides data regarding the performance of a spike sorting algorithm and asks for an evaluation of several statements about quality metrics. The problem is scientifically grounded, well-posed, and internally consistent.\n\nFirst, we formalize the problem by validating the setup and defining the necessary terms.\n\nThe givens are:\nSorted units: $U_1$, $U_2$, $U_3$.\nGround truth neurons: $GT_1$, $GT_2$, $GT_3$.\n\nThe counts of matched and unmatched spikes for each sorted unit are:\n- For $U_1$: $100$ spikes match $GT_1$, $45$ spikes match $GT_2$, $0$ spikes match $GT_3$, and $5$ spikes are unmatched.\n- For $U_2$: $5$ spikes match $GT_1$, $0$ spikes match $GT_2$, $65$ spikes match $GT_3$, and $0$ spikes are unmatched.\n- For $U_3$: $0$ spikes match $GT_1$, $55$ spikes match $GT_2$, $5$ spikes match $GT_3$, and $30$ spikes are unmatched.\n\nThe total number of ground truth spikes for each neuron is:\n- $N_{GT_1} = 120$\n- $N_{GT_2} = 100$\n- $N_{GT_3} = 80$\n\nThe task is to construct the confusion matrix $M$ and evaluate statements about per-unit precision and recall.\n\nThe confusion matrix $M$ has rows indexed by sorted units $\\{U_1, U_2, U_3\\}$ and columns by ground truth labels $\\{GT_1, GT_2, GT_3, \\text{None}\\}$. The entry $M_{ij}$ is the number of spikes in unit $U_i$ that match ground truth neuron $GT_j$.\n\nBased on the provided data, the confusion matrix $M$ is:\n$$ M = \\begin{pmatrix} 100 & 45 & 0 & 5 \\\\ 5 & 0 & 65 & 0 \\\\ 0 & 55 & 5 & 30 \\end{pmatrix} $$\n\nThe total number of spikes in each sorted unit is the sum of its corresponding row in $M$:\n- Total spikes in $U_1$: $N_{U_1} = 100 + 45 + 0 + 5 = 150$\n- Total spikes in $U_2$: $N_{U_2} = 5 + 0 + 65 + 0 = 70$\n- Total spikes in $U_3$: $N_{U_3} = 0 + 55 + 5 + 30 = 90$\n\nFor a given sorted unit $U_i$, we identify its dominant ground truth label $GT_j$ as the one with the highest count in the $i$-th row of $M$ (ignoring the 'None' column).\n- For $U_1$, the dominant label is $GT_1$ ($100 > 45$).\n- For $U_2$, the dominant label is $GT_3$ ($65 > 5$).\n- For $U_3$, the dominant label is $GT_2$ ($55 > 5$).\n\nWe now define per-unit precision and recall with respect to the dominant ground truth label $GT_j$ for a unit $U_i$:\n- **Precision**: The fraction of spikes in $U_i$ that are correctly from $GT_j$.\n  $$ \\text{Precision}(U_i, GT_j) = \\frac{\\text{Number of spikes in } U_i \\text{ from } GT_j}{\\text{Total number of spikes in } U_i} = \\frac{M_{ij}}{N_{U_i}} $$\n- **Recall**: The fraction of all spikes from $GT_j$ that are correctly captured in $U_i$.\n  $$ \\text{Recall}(U_i, GT_j) = \\frac{\\text{Number of spikes in } U_i \\text{ from } GT_j}{\\text{Total number of spikes from } GT_j} = \\frac{M_{ij}}{N_{GT_j}} $$\n\nNow we evaluate each option.\n\n**A. The row of the confusion matrix $M$ corresponding to $U_1$ is $(100,\\,45,\\,0,\\,5)$, and the per-unit precision and recall for $U_1$ with respect to $GT_1$ are $100/150$ and $100/120$, respectively.**\nThe row for $U_1$ is indeed $(100, 45, 0, 5)$ from the problem description. The dominant ground truth label for $U_1$ is $GT_1$.\n- Total spikes in $U_1$ is $N_{U_1} = 150$.\n- Total spikes from $GT_1$ is $N_{GT_1} = 120$.\n- The number of spikes in $U_1$ from $GT_1$ is $M_{11} = 100$.\n- Precision for $U_1$ w.r.t. $GT_1$ is $\\frac{M_{11}}{N_{U_1}} = \\frac{100}{150}$.\n- Recall for $U_1$ w.r.t. $GT_1$ is $\\frac{M_{11}}{N_{GT_1}} = \\frac{100}{120}$.\nBoth parts of the statement are arithmetically and conceptually correct. The presence of $45$ spikes from $GT_2$ and $5$ unmatched spikes reduces precision, demonstrating a \"merge\" error. The failure to capture all $120$ spikes of $GT_1$ reduces recall.\n**Verdict: Correct.**\n\n**B. $GT_3$ is split across $U_2$ and $U_3$, which lowers the per-unit recall of $U_2$ relative to the total $GT_3$ spikes; for $U_2$, the per-unit precision and recall with respect to $GT_3$ are $65/70$ and $65/80$, respectively.**\nFirst, we check if $GT_3$ is split.\n- $U_2$ contains $65$ spikes from $GT_3$.\n- $U_3$ contains $5$ spikes from $GT_3$.\nSince spikes from the single ground truth neuron $GT_3$ are found in multiple sorted units, it is by definition \"split\". This split, along with any missed spikes (total found for $GT_3$ is $65+5=70$, out of $80$), means that no single sorted unit captures all of $GT_3$'s spikes, thus lowering the per-unit recall for any associated unit. The reasoning is sound.\nNext, we check the calculations for $U_2$. The dominant label for $U_2$ is $GT_3$.\n- Total spikes in $U_2$ is $N_{U_2} = 70$.\n- Total spikes from $GT_3$ is $N_{GT_3} = 80$.\n- The number of spikes in $U_2$ from $GT_3$ is $M_{23} = 65$.\n- Precision for $U_2$ w.r.t. $GT_3$ is $\\frac{M_{23}}{N_{U_2}} = \\frac{65}{70}$.\n- Recall for $U_2$ w.r.t. $GT_3$ is $\\frac{M_{23}}{N_{GT_3}} = \\frac{65}{80}$.\nThe reasoning and the calculated values are correct.\n**Verdict: Correct.**\n\n**C. $U_1$ represents a merge of $GT_1$ and $GT_2$, so its per-unit precision with respect to $GT_1$ is high; specifically, it equals $100/120$.**\n$U_1$ contains $100$ spikes from $GT_1$ and $45$ from $GT_2$. This is a \"merge\". A merge error inherently *lowers* precision because the denominator of the precision formula (total spikes in the unit) is inflated by false positives (spikes from other neurons). The reasoning \"so its per-unit precision... is high\" is flawed. Furthermore, the statement claims the precision is $\\frac{100}{120}$.\n- As calculated for option A, the precision for $U_1$ w.r.t. $GT_1$ is $\\frac{100}{150}$.\n- The value $\\frac{100}{120}$ is the recall, not the precision.\nThe statement contains flawed reasoning and an incorrect value for precision.\n**Verdict: Incorrect.**\n\n**D. The per-unit recall for $U_3$ with respect to $GT_2$ is $55/90$.**\nThe dominant ground truth label for $U_3$ is $GT_2$. We calculate the recall for $U_3$ w.r.t. $GT_2$.\n- Total spikes from $GT_2$ is $N_{GT_2} = 100$.\n- The number of spikes in $U_3$ from $GT_2$ is $M_{32} = 55$.\n- Recall for $U_3$ w.r.t. $GT_2$ is $\\frac{M_{32}}{N_{GT_2}} = \\frac{55}{100}$.\nThe statement claims the recall is $\\frac{55}{90}$. Let's analyze this value. The total number of spikes in $U_3$ is $N_{U_3} = 90$. Therefore, $\\frac{55}{90}$ is the *precision* of $U_3$ w.r.t. $GT_2$, not the recall. The statement misidentifies precision as recall.\n**Verdict: Incorrect.**\n\n**E. Considering all sorted units together, the aggregate recall for $GT_1$ is $105/120$.**\nAggregate recall for a ground truth neuron $GT_j$ is the total number of its spikes detected across all sorted units, divided by the total number of spikes for that neuron.\n- Spikes from $GT_1$ found in $U_1$: $100$.\n- Spikes from $GT_1$ found in $U_2$: $5$.\n- Spikes from $GT_1$ found in $U_3$: $0$.\n- Total spikes from $GT_1$ detected by the algorithm = $100 + 5 + 0 = 105$.\n- The total number of actual spikes from $GT_1$ is $N_{GT_1} = 120$.\n- Aggregate recall for $GT_1$ = $\\frac{\\text{Total } GT_1 \\text{ spikes detected}}{\\text{Total } GT_1 \\text{ spikes}} = \\frac{105}{120}$.\nThe statement is correct.\n**Verdict: Correct.**\n\nIn summary, statements A, B, and E are correct.",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "In most experiments, ground truth data is unavailable, forcing us to rely on unsupervised quality metrics to assess cluster purity. This practice introduces the L-ratio, a powerful metric for estimating the contamination of a cluster based on the statistical properties of its feature distribution. By assuming a cluster's features follow a multivariate Gaussian distribution, you will calculate the probability that spikes *not* in the cluster might actually belong to it, providing a quantitative handle on potential \"false positive\" errors .",
            "id": "4146418",
            "problem": "You are given a set of spike waveform feature distances and cluster statistics used in spike sorting quality assessment. Assume feature vectors of a single candidate cluster are generated by a Multivariate Normal distribution with mean $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}_{C}$. Under this model, the squared Mahalanobis distance $d^{2}$ of a sample from the cluster follows a Chi-Square distribution with $p$ degrees of freedom, where $p$ is the dimensionality of the feature space and equals the size of $\\boldsymbol{\\Sigma}_{C}$.\n\nYour task is to compute, for each test case, a numerical contamination likelihood proxy known as the L-ratio. For a list of Mahalanobis distances $\\{d_{i}\\}$ computed for spikes not assigned to the cluster, the contamination likelihood proxy requires, for each $d_{i}$, evaluating the probability that a true cluster member would have a squared Mahalanobis distance greater than or equal to $d_{i}^{2}$ under the Chi-Square distribution with $p$ degrees of freedom. Summing these probabilities over all non-cluster spikes produces a quantity that, when normalized by the number of spikes assigned to the cluster, $N_{\\mathrm{cluster}}$, provides a contamination likelihood proxy. You must implement this computation using the Chi-Square survival function (the complement of the Cumulative Distribution Function (CDF)) to ensure numerical stability.\n\nScientific assumptions:\n- The cluster features follow a Multivariate Normal distribution.\n- The squared Mahalanobis distance of true cluster members follows a Chi-Square distribution with $p$ degrees of freedom, where $p$ equals the dimensionality of $\\boldsymbol{\\Sigma}_{C}$.\n- The non-cluster Mahalanobis distances $\\{d_{i}\\}$ are computed relative to the cluster mean and covariance $\\boldsymbol{\\Sigma}_{C}$.\n\nFor each test case, compute the L-ratio as a single real number and round it to $6$ decimal places. No physical units are involved. The final output must aggregate results from all test cases in a single line as a comma-separated list enclosed in square brackets, for example, [$r_1,r_2,r_3$], where each $r_i$ is the rounded L-ratio for test case $i$.\n\nTest suite:\n- Test case $1$:\n  - Cluster covariance $\\boldsymbol{\\Sigma}_{C} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 0.5 \\end{bmatrix}$.\n  - Non-cluster Mahalanobis distances $[2.0, 3.0, 5.0]$.\n  - Number of spikes assigned to the cluster $N_{\\mathrm{cluster}} = 150$.\n- Test case $2$:\n  - Cluster covariance $\\boldsymbol{\\Sigma}_{C} = \\begin{bmatrix} 1.0 & 0.2 & 0.0 \\\\ 0.2 & 1.5 & 0.1 \\\\ 0.0 & 0.1 & 0.8 \\end{bmatrix}$.\n  - Non-cluster Mahalanobis distances $[0.1, 0.2, 0.05, 0.3]$.\n  - Number of spikes assigned to the cluster $N_{\\mathrm{cluster}} = 50$.\n- Test case $3$:\n  - Cluster covariance $\\boldsymbol{\\Sigma}_{C} = \\begin{bmatrix} 2.0 & 0.0 \\\\ 0.0 & 2.0 \\end{bmatrix}$.\n  - Non-cluster Mahalanobis distances $[10.0, 12.0]$.\n  - Number of spikes assigned to the cluster $N_{\\mathrm{cluster}} = 200$.\n- Test case $4$:\n  - Cluster covariance $\\boldsymbol{\\Sigma}_{C} = \\begin{bmatrix} 1.0 \\end{bmatrix}$.\n  - Non-cluster Mahalanobis distances $[0.0, 10^{-8}]$.\n  - Number of spikes assigned to the cluster $N_{\\mathrm{cluster}} = 1$.\n\nYour program should produce a single line of output containing the L-ratio results for the four test cases as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places, in the exact format [$r_1,r_2,r_3,r_4$].",
            "solution": "The problem statement has been rigorously validated and found to be scientifically grounded, well-posed, and objective. It provides a complete and consistent set of definitions and data required to compute the L-ratio, a recognized quality metric in the field of spike sorting for neuroscience data analysis. The underlying statistical assumptions are standard and appropriate for the context. Therefore, a unique, stable, and meaningful solution can be derived.\n\nThe primary task is to calculate the L-ratio for several test cases. The L-ratio serves as a proxy for the contamination of a candidate spike cluster. It is defined based on the statistical properties of the cluster's feature distribution. The problem assumes that feature vectors of spikes belonging to a given cluster are samples from a Multivariate Normal distribution with mean $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}_{C}$. A fundamental property of this distribution is that the squared Mahalanobis distance, $d^2$, of a true member from the cluster center follows a Chi-Square ($\\chi^2$) distribution with $p$ degrees of freedom, where $p$ is the dimensionality of the feature space.\n\nThe L-ratio is formally defined as:\n$$\nL_{\\text{ratio}} = \\frac{1}{N_{\\text{cluster}}} \\sum_{i} P(\\chi^2_p \\ge d_i^2)\n$$\nIn this equation:\n- $N_{\\text{cluster}}$ is the total number of spikes assigned to the candidate cluster.\n- The summation is over all spikes that are *not* assigned to the cluster.\n- $\\{d_i\\}$ is the set of Mahalanobis distances of these non-cluster spikes, calculated relative to the candidate cluster's mean and covariance.\n- $p$ is the number of degrees of freedom for the Chi-Square distribution, which is equal to the dimensionality of the feature space, given by the size of the covariance matrix $\\boldsymbol{\\Sigma}_{C}$. For an $n \\times n$ matrix $\\boldsymbol{\\Sigma}_{C}$, we have $p=n$.\n- $P(\\chi^2_p \\ge d_i^2)$ is the probability that a value randomly drawn from a $\\chi^2$ distribution with $p$ degrees of freedom is greater than or equal to the squared Mahalanobis distance $d_i^2$. This probability corresponds to the survival function (SF) of the $\\chi^2$ distribution, which is the complement of the Cumulative Distribution Function (CDF), i.e., $S(x; p) = 1 - \\text{CDF}(x; p)$. Using the survival function directly is specified for better numerical precision, especially for large values of the argument where the CDF would be close to $1$.\n\nThe computational procedure for each test case is as follows:\n\n1.  **Determine Degrees of Freedom ($p$)**: Extract the dimensionality $p$ from the given cluster covariance matrix $\\boldsymbol{\\Sigma}_{C}$. If $\\boldsymbol{\\Sigma}_{C}$ is an $n \\times n$ matrix, then $p = n$.\n\n2.  **Calculate Probabilities**: For each non-cluster Mahalanobis distance $d_i$ in the provided list, first compute its square, $d_i^2$. Then, evaluate the Chi-Square survival function for this value with $p$ degrees of freedom, $S(d_i^2; p) = P(\\chi^2_p \\ge d_i^2)$.\n\n3.  **Sum and Normalize**: Sum all the probabilities calculated in the previous step: $\\text{Sum} = \\sum_i S(d_i^2; p)$.\n\n4.  **Compute L-ratio**: Divide the sum by the number of spikes in the cluster, $N_{\\text{cluster}}$, to obtain the final L-ratio: $L_{\\text{ratio}} = \\frac{\\text{Sum}}{N_{\\text{cluster}}}$.\n\n5.  **Round**: Round the computed L-ratio to $6$ decimal places as required.\n\nThis procedure will be applied to each of the four test cases.\n\n**Test Case 1:**\n- $\\boldsymbol{\\Sigma}_{C}$ is a $2 \\times 2$ matrix, so $p=2$.\n- Non-cluster Mahalanobis distances are $[2.0, 3.0, 5.0]$. The squared distances are $[4.0, 9.0, 25.0]$.\n- $N_{\\text{cluster}} = 150$.\n- $L_{\\text{ratio}} = \\frac{1}{150} \\left( P(\\chi^2_2 \\ge 4.0) + P(\\chi^2_2 \\ge 9.0) + P(\\chi^2_2 \\ge 25.0) \\right)$.\n\n**Test Case 2:**\n- $\\boldsymbol{\\Sigma}_{C}$ is a $3 \\times 3$ matrix, so $p=3$.\n- Non-cluster Mahalanobis distances are $[0.1, 0.2, 0.05, 0.3]$. The squared distances are $[0.01, 0.04, 0.0025, 0.09]$.\n- $N_{\\text{cluster}} = 50$.\n- $L_{\\text{ratio}} = \\frac{1}{50} \\left( P(\\chi^2_3 \\ge 0.01) + P(\\chi^2_3 \\ge 0.04) + P(\\chi^2_3 \\ge 0.0025) + P(\\chi^2_3 \\ge 0.09) \\right)$.\n\n**Test Case 3:**\n- $\\boldsymbol{\\Sigma}_{C}$ is a $2 \\times 2$ matrix, so $p=2$.\n- Non-cluster Mahalanobis distances are $[10.0, 12.0]$. The squared distances are $[100.0, 144.0]$.\n- $N_{\\text{cluster}} = 200$.\n- $L_{\\text{ratio}} = \\frac{1}{200} \\left( P(\\chi^2_2 \\ge 100.0) + P(\\chi^2_2 \\ge 144.0) \\right)$.\n\n**Test Case 4:**\n- $\\boldsymbol{\\Sigma}_{C}$ is a $1 \\times 1$ matrix, so $p=1$.\n- Non-cluster Mahalanobis distances are $[0.0, 10^{-8}]$. The squared distances are $[0.0, 10^{-16}]$.\n- $N_{\\text{cluster}} = 1$.\n- $L_{\\text{ratio}} = \\frac{1}{1} \\left( P(\\chi^2_1 \\ge 0.0) + P(\\chi^2_1 \\ge 10^{-16}) \\right)$. Note that $P(\\chi^2_p \\ge 0.0) = 1$ for any $p > 0$.\n\nThe implementation will use numerical libraries to evaluate the Chi-Square survival function accurately.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Computes the L-ratio for a set of test cases based on spike sorting statistics.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"Sigma_C\": np.array([[1.0, 0.0], [0.0, 0.5]]),\n            \"non_cluster_distances\": np.array([2.0, 3.0, 5.0]),\n            \"N_cluster\": 150\n        },\n        # Test case 2\n        {\n            \"Sigma_C\": np.array([[1.0, 0.2, 0.0], [0.2, 1.5, 0.1], [0.0, 0.1, 0.8]]),\n            \"non_cluster_distances\": np.array([0.1, 0.2, 0.05, 0.3]),\n            \"N_cluster\": 50\n        },\n        # Test case 3\n        {\n            \"Sigma_C\": np.array([[2.0, 0.0], [0.0, 2.0]]),\n            \"non_cluster_distances\": np.array([10.0, 12.0]),\n            \"N_cluster\": 200\n        },\n        # Test case 4\n        {\n            \"Sigma_C\": np.array([[1.0]]),\n            \"non_cluster_distances\": np.array([0.0, 1e-8]),\n            \"N_cluster\": 1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        Sigma_C = case[\"Sigma_C\"]\n        non_cluster_distances = case[\"non_cluster_distances\"]\n        N_cluster = case[\"N_cluster\"]\n\n        # The number of degrees of freedom p is the dimensionality of Sigma_C.\n        p = Sigma_C.shape[0]\n\n        # Square the Mahalanobis distances for non-cluster spikes.\n        squared_distances = non_cluster_distances**2\n\n        # For each squared distance, compute the probability P(X >= d^2)\n        # using the Chi-Square survival function (1 - CDF).\n        # This is more numerically stable than 1 - cdf(x) for large x.\n        probabilities = chi2.sf(squared_distances, df=p)\n\n        # Sum these probabilities.\n        sum_of_probs = np.sum(probabilities)\n\n        # Normalize by the number of spikes in the cluster to get the L-ratio.\n        l_ratio = sum_of_probs / N_cluster\n\n        # Round the result to 6 decimal places.\n        rounded_l_ratio = round(l_ratio, 6)\n        results.append(rounded_l_ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The foundation of most modern spike sorters is the concept of a \"template\"—an idealized average waveform for a given neuron. This exercise delves into the core optimization problem of how these templates are learned from the data. You will derive the solution to a regularized least-squares objective function, gaining insight into how an algorithm can robustly estimate a neuron's electrical fingerprint while avoiding overfitting to background noise .",
            "id": "4146401",
            "problem": "A multichannel spike sorting system operates on whitened extracellular waveforms recorded on $C$ channels over $T$ time samples per snippet. Each snippet is vectorized into $\\mathbb{R}^{d}$ with $d = C T$. For a single cluster, suppose we have $N$ candidate spikes with waveforms $\\{\\mathbf{x}_{i} \\in \\mathbb{R}^{d}\\}_{i=1}^{N}$, soft responsibilities $\\{r_{i} \\in [0,1]\\}_{i=1}^{N}$ for this cluster, and scalar amplitudes $\\{a_{i} \\in \\mathbb{R}\\}_{i=1}^{N}$ obtained from a preceding amplitude estimation step. Let the cluster template be $\\mathbf{t} \\in \\mathbb{R}^{d}$. Define the objective that penalizes expected residual energy across assigned spikes and includes regularization to control overfitting:\n$$\nJ(\\mathbf{t}) \\;=\\; \\frac{1}{2} \\sum_{i=1}^{N} r_{i} \\left\\| \\mathbf{x}_{i} - a_{i} \\mathbf{t} \\right\\|_{2}^{2} \\;+\\; \\frac{\\lambda}{2} \\left\\| \\mathbf{t} \\right\\|_{2}^{2} \\;+\\; \\frac{\\gamma}{2} \\left\\| \\mathbf{L} \\mathbf{t} \\right\\|_{2}^{2},\n$$\nwhere $\\lambda > 0$ controls overall template magnitude, $\\gamma \\ge 0$ controls roughness, and $\\mathbf{L} \\in \\mathbb{R}^{m \\times d}$ is a fixed linear operator that applies a second-order finite difference along the time axis independently on each channel (block-diagonal across channels). Assume that noise has been whitened so that residuals are isotropically weighted.\n\nTasks:\n- Starting from first principles of least-squares and linear operators, derive the gradient $\\nabla_{\\mathbf{t}} J(\\mathbf{t})$.\n- From this, derive a gradient descent update $\\mathbf{t}^{(+)} = \\mathbf{t}^{(-)} - \\eta \\nabla_{\\mathbf{t}} J(\\mathbf{t}^{(-)})$ for a step size $\\eta > 0$.\n- By setting the gradient to zero, derive the unique closed-form minimizer $\\hat{\\mathbf{t}}$ in terms of $\\{r_{i}\\}$, $\\{a_{i}\\}$, $\\{\\mathbf{x}_{i}\\}$, $\\lambda$, $\\gamma$, and $\\mathbf{L}$. Clearly indicate the identity matrix $\\mathbf{I} \\in \\mathbb{R}^{d \\times d}$ that appears in your expression.\n- Briefly explain, in one or two sentences, how the regularizers controlled by $\\lambda$ and $\\gamma$ prevent overfitting noise in the template update.\n\nProvide your final answer as the single closed-form analytic expression for $\\hat{\\mathbf{t}}$. No numerical evaluation is required, and no units are needed. Do not report any intermediate results as your final answer.",
            "solution": "The problem as stated is scientifically grounded, self-contained, and well-posed. The objective function $J(\\mathbf{t})$ is a strictly convex function of the template vector $\\mathbf{t}$, which guarantees the existence of a unique minimizer. We may therefore proceed with the derivation.\n\nThe objective function to be minimized with respect to the template $\\mathbf{t} \\in \\mathbb{R}^{d}$ is:\n$$\nJ(\\mathbf{t}) = \\frac{1}{2} \\sum_{i=1}^{N} r_{i} \\left\\| \\mathbf{x}_{i} - a_{i} \\mathbf{t} \\right\\|_{2}^{2} + \\frac{\\lambda}{2} \\left\\| \\mathbf{t} \\right\\|_{2}^{2} + \\frac{\\gamma}{2} \\left\\| \\mathbf{L} \\mathbf{t} \\right\\|_{2}^{2}\n$$\nTo find the minimizer, we must first compute the gradient of $J(\\mathbf{t})$ with respect to $\\mathbf{t}$, denoted as $\\nabla_{\\mathbf{t}} J(\\mathbf{t})$. To facilitate differentiation, we express the squared $L_2$-norms using vector transposes:\n$$\nJ(\\mathbf{t}) = \\frac{1}{2} \\sum_{i=1}^{N} r_{i} (\\mathbf{x}_{i} - a_{i} \\mathbf{t})^{\\top}(\\mathbf{x}_{i} - a_{i} \\mathbf{t}) + \\frac{\\lambda}{2} \\mathbf{t}^{\\top}\\mathbf{t} + \\frac{\\gamma}{2} (\\mathbf{L}\\mathbf{t})^{\\top}(\\mathbf{L}\\mathbf{t})\n$$\nWe expand the first term:\n$$\n(\\mathbf{x}_{i} - a_{i} \\mathbf{t})^{\\top}(\\mathbf{x}_{i} - a_{i} \\mathbf{t}) = \\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i} - 2a_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{t} + a_{i}^{2}\\mathbf{t}^{\\top}\\mathbf{t}\n$$\nAnd rewrite the third term:\n$$\n(\\mathbf{L}\\mathbf{t})^{\\top}(\\mathbf{L}\\mathbf{t}) = \\mathbf{t}^{\\top}\\mathbf{L}^{\\top}\\mathbf{L}\\mathbf{t}\n$$\nThe objective function becomes:\n$$\nJ(\\mathbf{t}) = \\frac{1}{2} \\sum_{i=1}^{N} r_{i} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i} - 2a_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{t} + a_{i}^{2}\\mathbf{t}^{\\top}\\mathbf{t}) + \\frac{\\lambda}{2} \\mathbf{t}^{\\top}\\mathbf{t} + \\frac{\\gamma}{2} \\mathbf{t}^{\\top}\\mathbf{L}^{\\top}\\mathbf{L}\\mathbf{t}\n$$\nWe now differentiate $J(\\mathbf{t})$ with respect to $\\mathbf{t}$ term by term, using the standard matrix calculus identities $\\nabla_{\\mathbf{v}} (\\mathbf{b}^{\\top}\\mathbf{v}) = \\mathbf{b}$ and $\\nabla_{\\mathbf{v}} (\\mathbf{v}^{\\top}\\mathbf{A}\\mathbf{v}) = (\\mathbf{A} + \\mathbf{A}^{\\top})\\mathbf{v}$.\n\nThe gradient of the first term is:\n$$\n\\nabla_{\\mathbf{t}} \\left( \\frac{1}{2} \\sum_{i=1}^{N} r_{i} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i} - 2a_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{t} + a_{i}^{2}\\mathbf{t}^{\\top}\\mathbf{t}) \\right) = \\frac{1}{2} \\sum_{i=1}^{N} r_{i} (-2a_{i}\\mathbf{x}_{i} + 2a_{i}^{2}\\mathbf{t}) = \\sum_{i=1}^{N} r_{i} (a_{i}^{2}\\mathbf{t} - a_{i}\\mathbf{x}_{i})\n$$\n$$\n= \\left(\\sum_{i=1}^{N} r_{i} a_{i}^{2}\\right)\\mathbf{t} - \\sum_{i=1}^{N} r_{i} a_{i}\\mathbf{x}_{i}\n$$\nThe gradient of the second (ridge) regularization term is:\n$$\n\\nabla_{\\mathbf{t}} \\left( \\frac{\\lambda}{2} \\mathbf{t}^{\\top}\\mathbf{t} \\right) = \\frac{\\lambda}{2} (2\\mathbf{t}) = \\lambda \\mathbf{t}\n$$\nThe gradient of the third (smoothing) regularization term is:\n$$\n\\nabla_{\\mathbf{t}} \\left( \\frac{\\gamma}{2} \\mathbf{t}^{\\top}\\mathbf{L}^{\\top}\\mathbf{L}\\mathbf{t} \\right) = \\frac{\\gamma}{2} (2\\mathbf{L}^{\\top}\\mathbf{L}\\mathbf{t}) = \\gamma \\mathbf{L}^{\\top}\\mathbf{L}\\mathbf{t}\n$$\nNote that $\\mathbf{L}^{\\top}\\mathbf{L}$ is a symmetric matrix.\n\nCombining these results, the full gradient $\\nabla_{\\mathbf{t}} J(\\mathbf{t})$ is:\n$$\n\\nabla_{\\mathbf{t}} J(\\mathbf{t}) = \\left(\\sum_{i=1}^{N} r_{i} a_{i}^{2}\\right)\\mathbf{t} - \\sum_{i=1}^{N} r_{i} a_{i}\\mathbf{x}_{i} + \\lambda \\mathbf{t} + \\gamma \\mathbf{L}^{\\top}\\mathbf{L}\\mathbf{t}\n$$\nGrouping the terms that multiply $\\mathbf{t}$ and introducing the identity matrix $\\mathbf{I} \\in \\mathbb{R}^{d \\times d}$ for correct dimensionality yields the final expression for the gradient:\n$$\n\\nabla_{\\mathbf{t}} J(\\mathbf{t}) = \\left( \\left(\\sum_{i=1}^{N} r_{i} a_{i}^{2} + \\lambda \\right)\\mathbf{I} + \\gamma \\mathbf{L}^{\\top}\\mathbf{L} \\right) \\mathbf{t} - \\sum_{i=1}^{N} r_{i} a_{i}\\mathbf{x}_{i}\n$$\nFrom this gradient, the gradient descent update rule for the template, with current estimate $\\mathbf{t}^{(-)}$ and step size $\\eta > 0$, is:\n$$\n\\mathbf{t}^{(+)} = \\mathbf{t}^{(-)} - \\eta \\nabla_{\\mathbf{t}} J(\\mathbf{t}^{(-)})\n$$\n$$\n\\mathbf{t}^{(+)} = \\mathbf{t}^{(-)} - \\eta \\left[ \\left( \\left(\\sum_{i=1}^{N} r_{i} a_{i}^{2} + \\lambda \\right)\\mathbf{I} + \\gamma \\mathbf{L}^{\\top}\\mathbf{L} \\right) \\mathbf{t}^{(-)} - \\sum_{i=1}^{N} r_{i} a_{i}\\mathbf{x}_{i} \\right]\n$$\nTo find the unique closed-form minimizer $\\hat{\\mathbf{t}}$, we set the gradient to zero, $\\nabla_{\\mathbf{t}} J(\\hat{\\mathbf{t}}) = \\mathbf{0}$:\n$$\n\\left( \\left(\\sum_{i=1}^{N} r_{i} a_{i}^{2} + \\lambda \\right)\\mathbf{I} + \\gamma \\mathbf{L}^{\\top}\\mathbf{L} \\right) \\hat{\\mathbf{t}} - \\sum_{i=1}^{N} r_{i} a_{i}\\mathbf{x}_{i} = \\mathbf{0}\n$$\nThis is a linear system of equations of the form $\\mathbf{A}\\hat{\\mathbf{t}} = \\mathbf{b}$, where:\n$$\n\\mathbf{A} = \\left(\\sum_{i=1}^{N} r_{i} a_{i}^{2} + \\lambda \\right)\\mathbf{I} + \\gamma \\mathbf{L}^{\\top}\\mathbf{L}\n$$\n$$\n\\mathbf{b} = \\sum_{i=1}^{N} r_{i} a_{i}\\mathbf{x}_{i}\n$$\nThe matrix $\\mathbf{A}$ is the Hessian of the objective function $J(\\mathbf{t})$. Because $\\lambda > 0$, the term $(\\sum r_i a_i^2 + \\lambda)\\mathbf{I}$ is positive definite. Since $\\gamma \\ge 0$ and $\\mathbf{L}^{\\top}\\mathbf{L}$ is positive semi-definite, their product is also positive semi-definite. The sum of a positive definite and a positive semi-definite matrix is positive definite, so $\\mathbf{A}$ is invertible. We can therefore solve for $\\hat{\\mathbf{t}}$ by left-multiplying by the inverse of $\\mathbf{A}$:\n$$\n\\hat{\\mathbf{t}} = \\mathbf{A}^{-1}\\mathbf{b} = \\left( \\left(\\sum_{i=1}^{N} r_{i} a_{i}^{2} + \\lambda \\right)\\mathbf{I} + \\gamma \\mathbf{L}^{\\top}\\mathbf{L} \\right)^{-1} \\left( \\sum_{i=1}^{N} r_{i} a_{i}\\mathbf{x}_{i} \\right)\n$$\nThis expression represents the optimal template that minimizes the regularized residual energy.\n\nFinally, the regularizers prevent overfitting. The $\\lambda$-regularizer penalizes the template's overall magnitude (squared $L_2$-norm), preventing it from fitting noise by shrinking its coefficients. The $\\gamma$-regularizer penalizes roughness, as measured by the finite difference operator $\\mathbf{L}$, thereby promoting smoother, more physically plausible template waveforms that are less susceptible to high-frequency noise.",
            "answer": "$$\n\\boxed{\\left( \\left(\\sum_{i=1}^{N} r_{i} a_{i}^{2} + \\lambda \\right)\\mathbf{I} + \\gamma \\mathbf{L}^{\\top}\\mathbf{L} \\right)^{-1} \\left( \\sum_{i=1}^{N} r_{i} a_{i}\\mathbf{x}_{i} \\right)}\n$$"
        }
    ]
}