{
    "hands_on_practices": [
        {
            "introduction": "Before we can interpret peaks in a cross-correlogram as evidence of neural synchrony, we must first establish a theoretical baseline. This involves answering a critical question: what should the correlogram look like if two neurons are firing completely independently? This exercise () guides you through a foundational derivation using the homogeneous Poisson process, a standard model for random spiking. By calculating the expected shape of the correlogram under the null hypothesis of independence, you will establish the \"floor\" of chance coincidences, a crucial first step for any rigorous synchrony analysis.",
            "id": "4151144",
            "problem": "Two simultaneously recorded neurons produce spike trains modeled as independent homogeneous Poisson point processes on the observation interval $[0,T]$, denoted $X$ and $Y$, with constant intensities (rates) $\\lambda_{x}$ and $\\lambda_{y}$, respectively. By homogeneity, for any interval of length $L$, the spike count has expectation $\\lambda L$, and by independence, spike counts in disjoint intervals and across processes are independent. Consider the following binned cross-correlogram estimator of lagged coincidences per unit time at lag $\\tau \\in \\mathbb{R}$ with bin width $\\Delta > 0$:\n$$\n\\hat{C}_{xy}(\\tau) \\equiv \\frac{1}{T} \\sum_{t_{i} \\in X \\cap [0,T]} N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big),\n$$\nwhere $N_{Y}(I)$ denotes the spike count of process $Y$ in interval $I$. Assume $T$ is large and $|\\tau| + \\Delta \\ll T$ so that boundary effects can be neglected for the purpose of computing expectations. Starting only from the definitions of a homogeneous Poisson process and the independence of $X$ and $Y$, derive the expected value $\\mathbb{E}[\\hat{C}_{xy}(\\tau)]$ as an explicit expression in $\\lambda_{x}$, $\\lambda_{y}$, and $\\Delta$, and show that it does not depend on $\\tau$. Provide your final answer as a single closed-form analytic expression. No numerical approximation is required.",
            "solution": "The problem asks for the expected value of the binned cross-correlogram estimator, $\\mathbb{E}[\\hat{C}_{xy}(\\tau)]$, for two independent homogeneous Poisson spike trains, $X$ and $Y$, with constant intensities $\\lambda_x$ and $\\lambda_y$ respectively.\n\nThe estimator is defined as:\n$$\n\\hat{C}_{xy}(\\tau) \\equiv \\frac{1}{T} \\sum_{t_{i} \\in X \\cap [0,T]} N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big)\n$$\nwhere $T$ is the observation interval duration, $\\Delta$ is the bin width, $\\tau$ is the time lag, $t_i$ are the spike times of process $X$, and $N_Y(I)$ is the number of spikes from process $Y$ in a given interval $I$.\n\nWe begin by applying the expectation operator to the definition of $\\hat{C}_{xy}(\\tau)$:\n$$\n\\mathbb{E}[\\hat{C}_{xy}(\\tau)] = \\mathbb{E}\\left[ \\frac{1}{T} \\sum_{t_{i} \\in X \\cap [0,T]} N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big) \\right]\n$$\nBy the linearity of expectation, the constant factor $\\frac{1}{T}$ can be moved outside the expectation:\n$$\n\\mathbb{E}[\\hat{C}_{xy}(\\tau)] = \\frac{1}{T} \\mathbb{E}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big) \\right]\n$$\nThe summation is over the spike times $\\{t_i\\}$ of the point process $X$, which are themselves random variables. To evaluate the expectation of this sum, we apply the law of total expectation (also known as the tower property), by conditioning on the realization of the spike train $X$. Let the specific realization of the process $X$ be the set of spike times $\\{t_i\\}$.\n$$\n\\mathbb{E}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} \\dots \\right] = \\mathbb{E}_{X}\\left[ \\mathbb{E}_{Y}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big) \\bigg| X = \\{t_i\\} \\right] \\right]\n$$\nThe problem states that the processes $X$ and $Y$ are independent. Therefore, conditioning on $X$ does not alter the statistics of $Y$. The inner expectation over $Y$ can be calculated by treating the spike times $\\{t_i\\}$ of $X$ as fixed values.\n$$\n= \\mathbb{E}_{X}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} \\mathbb{E}_{Y}\\left[ N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big) \\right] \\right]\n$$\nNext, we evaluate the inner expectation, $\\mathbb{E}_{Y}[\\dots]$. The process $Y$ is a homogeneous Poisson process with constant intensity $\\lambda_y$. A fundamental property of such a process is that the expected number of events in an interval of length $L$ is $\\lambda_y L$. The interval of interest is $[t_i + \\tau, t_i + \\tau + \\Delta)$, which has a length of $\\Delta$. Thus, the expected count is:\n$$\n\\mathbb{E}_{Y}\\left[ N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big) \\right] = \\lambda_y \\Delta\n$$\nThis result is a constant, independent of the specific spike time $t_i$ and the lag $\\tau$. This is a direct consequence of the homogeneity of process $Y$. Substituting this constant value back into our main expression:\n$$\n\\mathbb{E}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} \\dots \\right] = \\mathbb{E}_{X}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} (\\lambda_y \\Delta) \\right]\n$$\nThe constant term $\\lambda_y \\Delta$ can be factored out of the summation:\n$$\n= \\mathbb{E}_{X}\\left[ (\\lambda_y \\Delta) \\sum_{t_{i} \\in X \\cap [0,T]} 1 \\right]\n$$\nThe sum $\\sum_{t_{i} \\in X \\cap [0,T]} 1$ is, by definition, the total number of spikes of process $X$ within the observation interval $[0, T]$. This is denoted as $N_X([0, T])$.\n$$\n= \\mathbb{E}_{X}\\left[ (\\lambda_y \\Delta) N_X([0, T]) \\right]\n$$\nFactoring the constant term $\\lambda_y \\Delta$ out of the expectation gives:\n$$\n= (\\lambda_y \\Delta) \\mathbb{E}_{X}\\left[ N_X([0, T]) \\right]\n$$\nNow, we must evaluate the expected number of spikes for process $X$. Since $X$ is a homogeneous Poisson process with constant intensity $\\lambda_x$, the expected number of spikes in an interval of length $T$ is:\n$$\n\\mathbb{E}_{X}\\left[ N_X([0, T]) \\right] = \\lambda_x T\n$$\nThe problem states that we can neglect boundary effects, which is justified by the assumption $|\\tau| + \\Delta \\ll T$. This ensures our calculation holds for the entire interval.\n\nCombining the pieces, the expectation of the summation term is:\n$$\n\\mathbb{E}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} \\dots \\right] = (\\lambda_y \\Delta) (\\lambda_x T) = \\lambda_x \\lambda_y T \\Delta\n$$\nFinally, we substitute this result back into the expression for $\\mathbb{E}[\\hat{C}_{xy}(\\tau)]$:\n$$\n\\mathbb{E}[\\hat{C}_{xy}(\\tau)] = \\frac{1}{T} (\\lambda_x \\lambda_y T \\Delta) = \\lambda_x \\lambda_y \\Delta\n$$\nThis expression is the expected value of the cross-correlogram. It depends on the firing rates of the two neurons, $\\lambda_x$ and $\\lambda_y$, and the bin width, $\\Delta$. As required, the result does not depend on the lag $\\tau$, a characteristic feature for independent homogeneous processes, representing the expected rate of chance coincidences.",
            "answer": "$$\n\\boxed{\\lambda_{x} \\lambda_{y} \\Delta}\n$$"
        },
        {
            "introduction": "Theoretical models often assume infinite data, but real-world neural recordings are always finite. This practical limitation introduces subtle but significant artifacts into our statistical estimates. This practice problem () delves into the issue of \"edge effects\" in cross-correlation, where the decreasing overlap of signals at large time lags introduces a systematic bias, making the correlogram appear to decay even if the underlying correlation is constant. By deriving the correct normalization factor, you will learn how to construct an unbiased estimator, a critical skill for accurately interpreting the structure of correlograms from finite experimental data.",
            "id": "4151174",
            "problem": "Two simultaneously recorded neurons in the primate visual cortex produce spike trains that are converted to continuous rate signals by convolving each spike train with a Gaussian kernel and then subtracting the sample mean over the recording epoch to yield zero-mean signals. Denote these zero-mean, continuous-time processes by $x(t)$ and $y(t)$, and assume the underlying joint process is wide-sense stationary (WSS), meaning the second-order statistics do not depend on absolute time, and ergodic, meaning ensemble averages equal time averages in the limit of long recordings. The goal is to estimate the continuous-time cross-correlation function $R_{xy}(\\tau) = \\mathbb{E}[x(t)\\,y(t+\\tau)]$ from a single finite recording of duration $T>0$ seconds. The recorded signals are truncated to the observation window: $x_T(t) = x(t)\\,\\mathbb{1}_{[0,T]}(t)$ and $y_T(t) = y(t)\\,\\mathbb{1}_{[0,T]}(t)$, where $\\mathbb{1}_{A}(t)$ is the indicator function that equals $1$ if $t\\in A$ and $0$ otherwise. One natural estimator is the time average $\\hat{R}_{0}(\\tau) = \\dfrac{1}{T}\\int_{0}^{T} x_T(t)\\,y_T(t+\\tau)\\,dt$, which suffers from edge effects at large $|\\tau|$ because only part of the window overlaps for $t$ and $t+\\tau$.\n\nStarting from the definitions above and the assumptions of wide-sense stationarity and ergodicity, reason about how the overlap of the finite window affects the expected value of $\\hat{R}_{0}(\\tau)$ and identify a normalization that removes the lag-dependent bias for $|\\tau|<T$. Select the option that provides both an unbiased estimator and a correct justification of its denominator in terms of the measure of the overlap interval.\n\nA. $\\hat{R}_{A}(\\tau) = \\dfrac{1}{T}\\int_{0}^{T} x_T(t)\\,y_T(t+\\tau)\\,dt$ for all $\\tau$, because ergodicity ensures that dividing by $T$ yields an unbiased time average regardless of $\\tau$.\n\nB. $\\hat{R}_{B}(\\tau) = \\dfrac{1}{T-|\\tau|}\\int_{0}^{T} x_T(t)\\,y_T(t+\\tau)\\,dt$ for $|\\tau|<T$, and $\\hat{R}_{B}(\\tau)=0$ for $|\\tau|\\ge T$, because the denominator equals the Lebesgue measure of the overlap interval $[0,T]\\cap[-\\tau,\\,T-\\tau]$, yielding equal weighting per valid pair $(t,\\,t+\\tau)$ and removing the lag-dependent bias.\n\nC. $\\hat{R}_{C}(\\tau) = \\dfrac{1}{T+|\\tau|}\\int_{0}^{T} x_T(t)\\,y_T(t+\\tau)\\,dt$ for all $\\tau$, because a larger denominator at larger $|\\tau|$ compensates for the diminishing overlap at the edges.\n\nD. $\\hat{R}_{D}(\\tau) = \\dfrac{1}{T-|\\tau|}\\int_{0}^{T} x_T(t)\\,y_T(t+\\tau)\\,dt$ for $|\\tau|<T$, justified by equalizing the variance across lags; unbiasedness follows automatically from variance normalization.",
            "solution": "The problem statement will first be validated for scientific and logical integrity.\n\n### Step 1: Extract Givens\n- Two continuous-time, zero-mean signals, $x(t)$ and $y(t)$, are derived from neuronal spike trains.\n- The underlying joint process is assumed to be wide-sense stationary (WSS) and ergodic.\n- The quantity to be estimated is the cross-correlation function, defined as $R_{xy}(\\tau) = \\mathbb{E}[x(t)\\,y(t+\\tau)]$.\n- The signals are observed over a finite duration $T>0$, resulting in truncated signals: $x_T(t) = x(t)\\,\\mathbb{1}_{[0,T]}(t)$ and $y_T(t) = y(t)\\,\\mathbb{1}_{[0,T]}(t)$.\n- A proposed estimator is defined as $\\hat{R}_{0}(\\tau) = \\dfrac{1}{T}\\int_{0}^{T} x_T(t)\\,y_T(t+\\tau)\\,dt$.\n- The problem states that $\\hat{R}_{0}(\\tau)$ suffers from edge effects and asks for an estimator that removes the lag-dependent bias for $|\\tau|<T$, along with a correct justification.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes a standard approach in computational neuroscience and signal processing for estimating correlation from finite-time data. The assumptions (WSS, ergodicity) and the issue of bias in finite-sample estimators are fundamental concepts in these fields. The setup is scientifically sound.\n- **Well-Posed**: The problem is clearly defined. It provides all necessary mathematical definitions and assumptions to derive the expected value of an estimator and thereby analyze its bias. The goal is unambiguous.\n- **Objective**: The problem is stated in precise, objective mathematical language.\n- **Completeness and Consistency**: The problem is self-contained. It defines the signals, the target function, the available data, and a starting-point estimator. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It presents a standard, well-posed question in statistical signal processing. I will now proceed to derive the solution.\n\n### Derivation of the Unbiased Estimator\n\nThe general form of the estimator for the cross-correlation function from the truncated signals $x_T(t)$ and $y_T(t)$ is based on a time average. Let's consider an estimator of the form:\n$$ \\hat{R}(\\tau) = N(\\tau) \\int_{-\\infty}^{\\infty} x_T(t)\\,y_T(t+\\tau)\\,dt $$\nwhere $N(\\tau)$ is a normalization factor that depends on the lag $\\tau$.\n\nSubstituting the definitions of $x_T(t)$ and $y_T(t)$:\n$$ \\hat{R}(\\tau) = N(\\tau) \\int_{-\\infty}^{\\infty} [x(t)\\,\\mathbb{1}_{[0,T]}(t)] \\cdot [y(t+\\tau)\\,\\mathbb{1}_{[0,T]}(t+\\tau)]\\,dt $$\nThe product of the indicator functions, $\\mathbb{1}_{[0,T]}(t) \\cdot \\mathbb{1}_{[0,T]}(t+\\tau)$, is non-zero only when both $t \\in [0,T]$ and $t+\\tau \\in [0,T]$. The second condition implies $t \\in [-\\tau, T-\\tau]$. Therefore, the integrand is non-zero only for $t$ in the intersection of these two intervals: $t \\in [0,T] \\cap [-\\tau, T-\\tau]$. The limits of the integral can be replaced by this intersection interval.\n\nLet's determine the length of this overlap interval for $|\\tau| < T$:\n- If $\\tau \\ge 0$, the intersection is $[0, T-\\tau]$. The length (Lebesgue measure) is $(T-\\tau) - 0 = T-\\tau$.\n- If $\\tau < 0$, the intersection is $[-\\tau, T]$. The length is $T - (-\\tau) = T+\\tau$.\nFor any $\\tau$, we can express $\\tau$ as $\\text{sgn}(\\tau)|\\tau|$. For $\\tau<0$, $T+\\tau = T-|\\tau|$. For $\\tau\\ge 0$, $T-\\tau = T-|\\tau|$. Thus, for any $\\tau$ such that $|\\tau| < T$, the length of the overlap interval is $T-|\\tau|$. For $|\\tau| \\ge T$, the intersection is either a single point or empty, so its measure is $0$.\n\nAn estimator is unbiased if its expected value equals the true value. Let's compute the expected value of the integral part of the estimator, denoted as $I(\\tau) = \\int_{-\\infty}^{\\infty} x_T(t)\\,y_T(t+\\tau)\\,dt$.\n$$ \\mathbb{E}[I(\\tau)] = \\mathbb{E}\\left[ \\int_{-\\infty}^{\\infty} x(t)y(t+\\tau)\\,\\mathbb{1}_{[0,T]}(t)\\,\\mathbb{1}_{[0,T]}(t+\\tau)\\,dt \\right] $$\nAssuming we can interchange expectation and integration (Fubini's theorem), which is standard in this context:\n$$ \\mathbb{E}[I(\\tau)] = \\int_{-\\infty}^{\\infty} \\mathbb{E}[x(t)y(t+\\tau)]\\,\\mathbb{1}_{[0,T]}(t)\\,\\mathbb{1}_{[0,T]}(t+\\tau)\\,dt $$\nDue to the WSS assumption, $\\mathbb{E}[x(t)y(t+\\tau)] = R_{xy}(\\tau)$, which does not depend on $t$. We can pull it out of the integral:\n$$ \\mathbb{E}[I(\\tau)] = R_{xy}(\\tau) \\int_{-\\infty}^{\\infty} \\mathbb{1}_{[0,T]}(t)\\,\\mathbb{1}_{[0,T]}(t+\\tau)\\,dt $$\nThe remaining integral is simply the measure of the overlap interval, which we found to be $T-|\\tau|$ for $|\\tau|<T$ and $0$ for $|\\tau|\\ge T$.\nSo, for $|\\tau|<T$:\n$$ \\mathbb{E}[I(\\tau)] = R_{xy}(\\tau) \\cdot (T-|\\tau|) $$\n\nThe full estimator is $\\hat{R}(\\tau) = N(\\tau) I(\\tau)$. Its expectation is:\n$$ \\mathbb{E}[\\hat{R}(\\tau)] = N(\\tau) \\mathbb{E}[I(\\tau)] = N(\\tau) \\cdot R_{xy}(\\tau) \\cdot (T-|\\tau|) $$\nFor $\\hat{R}(\\tau)$ to be an unbiased estimator of $R_{xy}(\\tau)$, we must have $\\mathbb{E}[\\hat{R}(\\tau)] = R_{xy}(\\tau)$. This requires:\n$$ N(\\tau) \\cdot (T-|\\tau|) = 1 \\implies N(\\tau) = \\frac{1}{T-|\\tau|} $$\nThis normalization is valid for $|\\tau|<T$. For $|\\tau|\\ge T$, the integral is $0$, so the estimator should be defined as $0$.\n\nThe unbiased estimator is therefore:\n$$ \\hat{R}_{\\text{unbiased}}(\\tau) = \\begin{cases} \\dfrac{1}{T-|\\tau|}\\int_{0}^{T} x_T(t)\\,y_T(t+\\tau)\\,dt & \\text{if } |\\tau|<T \\\\ 0 & \\text{if } |\\tau|\\ge T \\end{cases} $$\nThe denominator, $T-|\\tau|$, is the measure of the interval over which non-zero products $x(t)y(t+\\tau)$ can be computed. Dividing by this quantity ensures that the estimate for each lag $\\tau$ is an average over the number of available data pairs, thereby correcting the bias introduced by the finite window.\n\nThe ergodicity assumption is what justifies using a time average from a single, long realization to estimate the ensemble average $R_{xy}(\\tau)$ in the first place (i.e., $\\lim_{T\\to\\infty} \\hat{R}_{\\text{unbiased}}(\\tau) = R_{xy}(\\tau)$). However, it does not correct the bias for a finite $T$.\n\n### Option-by-Option Analysis\n\n**A. $\\hat{R}_{A}(\\tau) = \\dfrac{1}{T}\\int_{0}^{T} x_T(t)\\,y_T(t+\\tau)\\,dt$ for all $\\tau$, because ergodicity ensures that dividing by $T$ yields an unbiased time average regardless of $\\tau$.**\nThis is the estimator $\\hat{R}_0(\\tau)$ from the problem description. Its normalization factor is $N(\\tau)=1/T$. Its expected value, for $|\\tau|<T$, is:\n$$ \\mathbb{E}[\\hat{R}_A(\\tau)] = \\frac{1}{T} \\mathbb{E}[I(\\tau)] = \\frac{1}{T} \\cdot R_{xy}(\\tau) \\cdot (T-|\\tau|) = R_{xy}(\\tau) \\left(1 - \\frac{|\\tau|}{T}\\right) $$\nSince $\\mathbb{E}[\\hat{R}_A(\\tau)] \\neq R_{xy}(\\tau)$ for $\\tau \\neq 0$, the estimator is biased. The justification about ergodicity is a misapplication of the concept; ergodicity relates to the convergence in the limit $T \\to \\infty$, it does not remove finite-sample bias.\n**Verdict: Incorrect.**\n\n**B. $\\hat{R}_{B}(\\tau) = \\dfrac{1}{T-|\\tau|}\\int_{0}^{T} x_T(t)\\,y_T(t+\\tau)\\,dt$ for $|\\tau|<T$, and $\\hat{R}_{B}(\\tau)=0$ for $|\\tau|\\ge T$, because the denominator equals the Lebesgue measure of the overlap interval $[0,T]\\cap[-\\tau,\\,T-\\tau]$, yielding equal weighting per valid pair $(t,\\,t+\\tau)$ and removing the lag-dependent bias.**\nThe formula for the estimator matches our derived unbiased estimator. The justification is also perfectly correct. The denominator $T-|\\tau|$ is precisely the Lebesgue measure of the overlap interval, and dividing by this measure is what removes the geometrically induced, lag-dependent bias. This corresponds to calculating the mean over the set of available product terms for a given lag $\\tau$.\n**Verdict: Correct.**\n\n**C. $\\hat{R}_{C}(\\tau) = \\dfrac{1}{T+|\\tau|}\\int_{0}^{T} x_T(t)\\,y_T(t+\\tau)\\,dt$ for all $\\tau$, because a larger denominator at larger $|\\tau|$ compensates for the diminishing overlap at the edges.**\nThe normalization factor is $N(\\tau) = 1/(T+|\\tau|)$. The expected value for $|\\tau|<T$ would be:\n$$ \\mathbb{E}[\\hat{R}_C(\\tau)] = \\frac{1}{T+|\\tau|} \\cdot R_{xy}(\\tau) \\cdot (T-|\\tau|) = R_{xy}(\\tau) \\frac{T-|\\tau|}{T+|\\tau|} $$\nThis is not an unbiased estimator. The denominator $T+|\\tau|$ is incorrect and does not properly compensate for the diminishing overlap.\n**Verdict: Incorrect.**\n\n**D. $\\hat{R}_{D}(\\tau) = \\dfrac{1}{T-|\\tau|}\\int_{0}^{T} x_T(t)\\,y_T(t+\\tau)\\,dt$ for $|\\tau|<T$, justified by equalizing the variance across lags; unbiasedness follows automatically from variance normalization.**\nThe formula for the estimator is correct. However, the justification is flawed. The normalization by $1/(T-|\\tau|)$ does not equalize the variance. The variance of this unbiased estimator is approximately inversely proportional to $T-|\\tau|$, meaning the variance *increases* as $|\\tau| \\to T$. This is a well-known bias-variance trade-off; the unbiased estimator has high variance at large lags. The biased estimator from option A has a lower (and more stable) variance but is biased. Furthermore, the claim that unbiasedness follows from variance normalization is incorrect. Bias and variance are separate properties of an estimator, and correcting one does not automatically correct or determine the other. The normalization by $1/(T-|\\tau|)$ is a bias correction, not a variance normalization.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Estimating a cross-correlogram is only half the battle; the ultimate goal is to make a statistical inference about which features, such as peaks, are significant. Testing for significance at each time lag individually inflates the probability of finding false positivesâ€”the classic \"multiple comparisons problem.\" This capstone coding exercise () walks you through a complete, modern workflow to address this challenge, from simulating realistic spike train data to implementing a powerful max-statistic permutation test. By building a null distribution from surrogate data, you will learn to control the family-wise error rate and confidently identify genuine neural synchrony, a technique at the forefront of robust data analysis.",
            "id": "4151171",
            "problem": "Consider two discrete-time spike-count sequences $x_{t}$ and $y_{t}$ sampled at a fixed bin width $d t$ over a duration $T$, where $t \\in \\{0,1,2,\\dots,N-1\\}$ and $N = \\lfloor T / d t \\rfloor$. The goal is to assess neural synchrony via the cross-correlogram while controlling for the increased false positive risk arising from multiple comparisons across lags. Begin from the following base:\n\n- The Pearson correlation coefficient between two finite sequences $a_{t}$ and $b_{t}$ of equal length $m$ is defined by\n$$\nr(a,b) = \\frac{\\sum_{t=0}^{m-1} (a_{t} - \\bar{a})(b_{t} - \\bar{b})}{\\sqrt{\\sum_{t=0}^{m-1}(a_{t}-\\bar{a})^{2}} \\sqrt{\\sum_{t=0}^{m-1}(b_{t}-\\bar{b})^{2}}},\n$$\nwhere $\\bar{a}$ and $\\bar{b}$ are the sample means of $a_{t}$ and $b_{t}$, respectively.\n\n- The cross-correlogram between $x_{t}$ and $y_{t}$ is the function of integer lag $\\ell$ defined by computing the Pearson correlation coefficient between the overlapped segments of $x_{t}$ and $y_{t}$ at that lag, i.e., for $\\ell \\geq 0$\n$$\nr(\\ell) = r\\big(x_{0:(N-1-\\ell)}, y_{\\ell:(N-1)}\\big),\n$$\nand for $\\ell < 0$\n$$\nr(\\ell) = r\\big(x_{(-\\ell):(N-1)}, y_{0:(N-1+\\ell)}\\big).\n$$\n\n- The Family-Wise Error Rate (FWER) is the probability of making at least one false rejection among a family of simultaneous hypothesis tests. In this setting, each lag $\\ell \\in \\{-L,\\dots,+L\\}$ defines one test for excess correlation at that lag. To control FWER at level $\\alpha$, you must use a max-statistic approach: under the null hypothesis of no true synchrony, generate null surrogates by circularly shifting $y_{t}$ by a random offset (uniformly distributed over allowed shifts that preserve stationarity) to destroy alignment while preserving marginal properties, compute the cross-correlogram for each surrogate, take the maximum absolute correlation across lags for each surrogate, and use the empirical $(1-\\alpha)$ quantile of these maxima as a threshold $t_{\\alpha}$ that controls FWER over the entire family of lag tests. Then declare significant those observed lags whose absolute observed correlation exceeds $t_{\\alpha}$.\n\nTask. Implement a program that:\n\n1. Simulates binned spike-count sequences $x_{t}$ and $y_{t}$ with realistic neural spiking structure by combining independent Poisson spike-generation with an additive synchronous component:\n   - Independent background: draw $x_{t}$ and $y_{t}$ counts as independent Poisson random variables per bin with means $\\lambda_{x} d t$ and $\\lambda_{y} d t$, respectively.\n   - Synchronous component: draw a Poisson number of synchronous events with rate $\\lambda_{\\mathrm{sync}}$ over $[0,T)$, place each event time $u$ in $x_{t}$ at $u + \\varepsilon_{x}$ and in $y_{t}$ at $u + \\tau + \\varepsilon_{y}$, where $\\tau$ is a fixed synchrony lag and $\\varepsilon_{x}, \\varepsilon_{y}$ are independent Gaussian jitters with standard deviation $\\sigma$ (values are clamped to remain in $[0,T)$). Bin these events into $x_{t}$ and $y_{t}$ by incrementing the appropriate bin count.\n\n2. Computes the observed cross-correlogram $r(\\ell)$ for integer lags $\\ell \\in \\{-L,\\dots,+L\\}$, where $L$ is specified in seconds and converted to bins by $L_{\\mathrm{bins}} = \\lfloor L / d t \\rfloor$.\n\n3. Constructs a null distribution for the max-statistic by generating $K$ circular-shift surrogates of $y_{t}$ (each surrogate $y^{(k)}_{t}$ is $y_{(t+s_{k}) \\bmod N}$ for a randomly chosen shift $s_{k}$), computing the cross-correlogram $r^{(k)}(\\ell)$ for each surrogate, and recording $M_{k} = \\max_{\\ell \\in \\{-L,\\dots,+L\\}} \\left| r^{(k)}(\\ell) \\right|$.\n\n4. Determines the threshold $t_{\\alpha}$ as the empirical $(1-\\alpha)$ quantile of $\\{M_{1},\\dots,M_{K}\\}$ and returns the set of observed lags $\\ell$ such that $\\left| r(\\ell) \\right| \\geq t_{\\alpha}$, expressed as lag values in seconds $\\ell \\cdot d t$ rounded to three decimals.\n\nAssumptions and constraints:\n- You must treat $\\ell$ as an integer number of bins and convert lag values to seconds for the final output.\n- Use circular shifts for surrogates to preserve stationarity of $y_{t}$ while breaking alignment with $x_{t}$ under the null hypothesis of no synchrony.\n- If any overlapped segment has zero variance (all counts identical), define its correlation as $0$ for that lag to avoid undefined values.\n- All time quantities must be expressed in seconds; lag outputs must be rounded to three decimals in seconds.\n- The output per test case must be a list of floats (lag values in seconds), and the aggregated program output must be a single list containing one such list per test case.\n\nTest suite:\nImplement the program for the following parameter sets. Each parameter set is a tuple specifying $(T, d t, \\lambda_{x}, \\lambda_{y}, \\lambda_{\\mathrm{sync}}, \\tau, \\sigma, L, K, \\alpha, \\text{seed})$. Use the seeds as provided to ensure deterministic outputs.\n\n- Case $1$: $(T = \\, 20, d t = \\, 0.005, \\lambda_{x} = \\, 15, \\lambda_{y} = \\, 15, \\lambda_{\\mathrm{sync}} = \\, 5, \\tau = \\, 0, \\sigma = \\, 0.003, L = \\, 0.025, K = \\, 200, \\alpha = \\, 0.05, \\text{seed} = \\, 101)$.\n\n- Case $2$: $(T = \\, 20, d t = \\, 0.005, \\lambda_{x} = \\, 15, \\lambda_{y} = \\, 15, \\lambda_{\\mathrm{sync}} = \\, 5, \\tau = \\, 0.015, \\sigma = \\, 0.003, L = \\, 0.025, K = \\, 200, \\alpha = \\, 0.05, \\text{seed} = \\, 102)$.\n\n- Case $3$: $(T = \\, 20, d t = \\, 0.005, \\lambda_{x} = \\, 15, \\lambda_{y} = \\, 15, \\lambda_{\\mathrm{sync}} = \\, 0, \\tau = \\, 0, \\sigma = \\, 0, L = \\, 0.025, K = \\, 200, \\alpha = \\, 0.05, \\text{seed} = \\, 103)$.\n\n- Case $4$ (boundary condition with zero-lag only): $(T = \\, 10, d t = \\, 0.005, \\lambda_{x} = \\, 20, \\lambda_{y} = \\, 20, \\lambda_{\\mathrm{sync}} = \\, 5, \\tau = \\, 0, \\sigma = \\, 0.002, L = \\, 0, K = \\, 200, \\alpha = \\, 0.05, \\text{seed} = \\, 104)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list of significant lag values (in seconds, rounded to three decimals) for one test case. For example, an output with four test cases must look like $[\\,[0.000,0.015],\\,[0.000],\\,[\\,],\\,[0.000]\\,]$, where an empty list $[\\,]$ indicates no significant lags detected.",
            "solution": "The approach is grounded in statistical testing principles and the definition of the cross-correlogram. We proceed step by step.\n\n1. Fundamental definitions. For discrete-time spike-count sequences $x_{t}$ and $y_{t}$ sampled at bin width $d t$ over duration $T$ with $N = \\lfloor T / d t \\rfloor$ bins, the Pearson correlation coefficient is defined for two equal-length segments by\n$$\nr(a,b) = \\frac{\\sum_{t=0}^{m-1} (a_{t} - \\bar{a})(b_{t} - \\bar{b})}{\\sqrt{\\sum_{t=0}^{m-1}(a_{t}-\\bar{a})^{2}} \\sqrt{\\sum_{t=0}^{m-1}(b_{t}-\\bar{b})^{2}}}.\n$$\nThe cross-correlogram $r(\\ell)$ across integer lags $\\ell$ uses this correlation on overlapped segments. For $\\ell \\geq 0$,\n$$\nr(\\ell) = r\\big(x_{0:(N-1-\\ell)}, y_{\\ell:(N-1)}\\big),\n$$\nand for $\\ell < 0$,\n$$\nr(\\ell) = r\\big(x_{(-\\ell):(N-1)}, y_{0:(N-1+\\ell)}\\big).\n$$\nIf either overlapped segment has zero variance, the correlation is undefined; we set $r(\\ell) = 0$ for robustness.\n\n2. Multiple comparisons across lags and Family-Wise Error Rate (FWER). Testing $H_{0}:\\text{no synchrony}$ at each lag $\\ell \\in \\{-L,\\dots,+L\\}$ introduces a family of tests. The Family-Wise Error Rate (FWER) is $P(\\text{at least one false positive among the tests})$. If each lag test is run at level $\\alpha$ independently, the probability of at least one false positive increases with the number of lags. To control FWER at level $\\alpha$ without overly conservative corrections, we use a max-statistic approach tailored to the dependency structure across lags.\n\n3. Max-statistic approach. Under $H_{0}$ of no true synchrony, a valid null procedure is to circularly shift $y_{t}$ by a random offset $s$ (uniform on $\\{0,1,\\dots,N-1\\}$) to produce $y^{(k)}_{t} = y_{(t+s_{k}) \\bmod N}$. This preserves stationarity and marginal distribution of $y_{t}$ while destroying alignment with $x_{t}$. For each surrogate $k \\in \\{1,\\dots,K\\}$, compute the cross-correlogram $r^{(k)}(\\ell)$ over $\\ell \\in \\{-L_{\\mathrm{bins}},\\dots,+L_{\\mathrm{bins}}\\}$ and record the max-statistic\n$$\nM_{k} = \\max_{\\ell \\in \\{-L_{\\mathrm{bins}},\\dots,+L_{\\mathrm{bins}}\\}} \\left| r^{(k)}(\\ell) \\right|.\n$$\nThe null distribution of $M_{k}$ approximates the sampling distribution of the maximum absolute correlation under $H_{0}$. Let $t_{\\alpha}$ be the empirical $(1-\\alpha)$ quantile of $\\{M_{1},\\dots,M_{K}\\}$. Then\n$$\nP\\left(\\max_{\\ell} \\left| r(\\ell) \\right| \\geq t_{\\alpha} \\,\\big|\\, H_{0}\\right) \\approx \\alpha,\n$$\nwhich implies FWER control at level approximately $\\alpha$ for the family of lag tests. Consequently, declaring those lags $\\ell$ significant where $\\left| r(\\ell) \\right| \\geq t_{\\alpha}$ yields a set of discoveries with controlled FWER.\n\n4. Simulation of spike trains. To generate realistic sequences, we combine independent Poisson background with synchronous events. For each bin, sample independent counts:\n$$\nx_{t}^{\\text{bg}} \\sim \\text{Poisson}(\\lambda_{x} d t), \\quad y_{t}^{\\text{bg}} \\sim \\text{Poisson}(\\lambda_{y} d t).\n$$\nFor the synchronous component, sample a Poisson number of events with mean $\\lambda_{\\mathrm{sync}} T$. For each event with base time $u$, generate perturbed times\n$$\nu_{x} = \\mathrm{clip}(u + \\varepsilon_{x}, 0, T), \\quad u_{y} = \\mathrm{clip}(u + \\tau + \\varepsilon_{y}, 0, T),\n$$\nwith $\\varepsilon_{x}, \\varepsilon_{y} \\sim \\mathcal{N}(0,\\sigma^{2})$. Bin these times into $x_{t}$ and $y_{t}$ by incrementing the corresponding bin counts:\n$$\nt_{x} = \\left\\lfloor \\frac{u_{x}}{d t} \\right\\rfloor, \\quad t_{y} = \\left\\lfloor \\frac{u_{y}}{d t} \\right\\rfloor.\n$$\nThe resulting sequences are $x_{t} = x_{t}^{\\text{bg}} + x_{t}^{\\text{sync}}$ and $y_{t} = y_{t}^{\\text{bg}} + y_{t}^{\\text{sync}}$.\n\n5. Algorithmic steps.\n- Convert $L$ seconds to bins: $L_{\\mathrm{bins}} = \\lfloor L / d t \\rfloor$.\n- Compute observed $r(\\ell)$ for all integer $\\ell \\in \\{-L_{\\mathrm{bins}},\\dots,+L_{\\mathrm{bins}}\\}$.\n- For $k = 1$ to $K$: draw a random shift $s_{k}$ uniformly from $\\{0,1,\\dots,N-1\\}$, set $y^{(k)}_{t} = y_{(t+s_{k}) \\bmod N}$, compute $r^{(k)}(\\ell)$ over lags, record $M_{k} = \\max_{\\ell} |r^{(k)}(\\ell)|$.\n- Compute $t_{\\alpha}$ as the empirical $(1-\\alpha)$ quantile of $\\{M_{k}\\}$.\n- Output the set $\\{\\ell \\cdot d t : |r(\\ell)| \\geq t_{\\alpha}\\}$ in seconds, rounded to three decimals.\n\n6. Edge cases and boundary conditions. If $L = 0$, only the zero-lag test is performed; the max-statistic reduces to the absolute correlation at zero lag. If any overlapped segment has zero variance, set $r(\\ell) = 0$ to avoid divisions by zero. The circular shift surrogates preserve auto-correlation in $y_{t}$ but render alignment with $x_{t}$ effectively random under $H_{0}$, justifying their use for the null max-statistic.\n\n7. Output specification. For each test case, a list of significant lag values in seconds (rounded to three decimals) is returned. The final program output is a single line containing a comma-separated list of these lists, enclosed in square brackets. Units are seconds for all lag values in the output, and $\\alpha$ is specified as a decimal (e.g., $0.05$).\n\nWith these steps, the implementation controls the Family-Wise Error Rate (FWER) using the max-statistic across lags, providing statistically principled detection of synchrony peaks in the cross-correlogram.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate_spike_counts(T, dt, rate_x, rate_y, sync_rate, tau, sigma, rng):\n    \"\"\"\n    Simulate binned spike-count sequences x_t and y_t:\n    - Independent Poisson background per bin.\n    - Add synchronous events with jitter and lag tau.\n    \"\"\"\n    N = int(np.floor(T / dt))\n    # Background Poisson counts\n    mean_x = rate_x * dt\n    mean_y = rate_y * dt\n    x_bg = rng.poisson(mean_x, size=N)\n    y_bg = rng.poisson(mean_y, size=N)\n\n    x = x_bg.copy()\n    y = y_bg.copy()\n\n    # Synchronous component\n    if sync_rate > 0:\n        n_sync = rng.poisson(sync_rate * T)\n        if n_sync > 0:\n            # Draw base event times uniformly over [0, T)\n            base_times = rng.random(n_sync) * T\n            # Jitters\n            jitter_x = rng.normal(0.0, sigma, size=n_sync) if sigma > 0 else np.zeros(n_sync)\n            jitter_y = rng.normal(0.0, sigma, size=n_sync) if sigma > 0 else np.zeros(n_sync)\n            # Compute event times for x and y, apply lag tau to y\n            times_x = base_times + jitter_x\n            times_y = base_times + tau + jitter_y\n            # Clamp to [0, T)\n            times_x = np.clip(times_x, 0.0, T - 1e-12)\n            times_y = np.clip(times_y, 0.0, T - 1e-12)\n            # Bin indices\n            bins_x = np.floor(times_x / dt).astype(int)\n            bins_y = np.floor(times_y / dt).astype(int)\n            # Increment counts\n            # Multiple events can land in the same bin; use add.at for correctness\n            np.add.at(x, bins_x, 1)\n            np.add.at(y, bins_y, 1)\n\n    return x, y\n\ndef pearson_corr(x_seg, y_seg):\n    \"\"\"\n    Compute Pearson correlation for two equal-length 1D arrays.\n    If std dev of either segment is zero, return 0.\n    Uses sample correlation with denominator (n-1)*std_x*std_y.\n    \"\"\"\n    n = x_seg.size\n    if n  2:\n        return 0.0\n    # Use ddof=0 for std to match problem definition of Pearson correlation\n    # which implies population-like sums, but sample-like mean.\n    # The final ratio is invariant to ddof if n is consistent,\n    # but using ddof=1 is more standard for sample correlation.\n    # The problem description's formula uses sums, which corresponds to ddof=0 in numpy's std.\n    # Let's stick to the problem formula.\n    x_mean = x_seg.mean()\n    y_mean = y_seg.mean()\n    x_std = x_seg.std()\n    y_std = y_seg.std()\n    \n    if x_std == 0 or y_std == 0:\n        return 0.0\n\n    numerator = np.sum((x_seg - x_mean) * (y_seg - y_mean))\n    denominator = np.sqrt(np.sum((x_seg - x_mean)**2)) * np.sqrt(np.sum((y_seg - y_mean)**2))\n    \n    return numerator / denominator\n\n\ndef correlogram(x, y, L_bins):\n    \"\"\"\n    Compute Pearson cross-correlogram r(lag) for lags in [-L_bins, ..., +L_bins].\n    \"\"\"\n    N = x.size\n    lags = np.arange(-L_bins, L_bins + 1, dtype=int)\n    r_vals = np.zeros(lags.shape[0], dtype=float)\n    for i, lag in enumerate(lags):\n        if lag >= 0:\n            x_seg = x[:N - lag] if lag != 0 else x\n            y_seg = y[lag:] if lag != 0 else y\n        else:\n            k = -lag\n            x_seg = x[k:]\n            y_seg = y[:N - k]\n        r_vals[i] = pearson_corr(x_seg, y_seg)\n    return lags, r_vals\n\ndef max_stat_threshold(x, y, L_bins, K, alpha, rng):\n    \"\"\"\n    Build null distribution via circular shifts of y and return the (1-alpha) quantile threshold.\n    \"\"\"\n    N = x.size\n    max_vals = np.empty(K, dtype=float)\n    # Precompute shifts\n    shifts = rng.integers(1, N, size=K) # Avoid shift by 0\n    for k in range(K):\n        y_shift = np.roll(y, shifts[k])\n        _, r_vals = correlogram(x, y_shift, L_bins)\n        max_vals[k] = np.max(np.abs(r_vals))\n    # Empirical (1 - alpha) quantile; use 'higher' to be conservative in FWER control\n    threshold = np.quantile(max_vals, 1.0 - alpha, method='higher')\n    return threshold\n\ndef run_test_case(params):\n    \"\"\"\n    Execute one test case: simulate data, compute correlogram, compute max-stat threshold,\n    and return list of significant lags (in seconds, rounded to 3 decimals).\n    params: (T, dt, rate_x, rate_y, sync_rate, tau, sigma, L, K, alpha, seed)\n    \"\"\"\n    T, dt, rate_x, rate_y, sync_rate, tau, sigma, L, K, alpha, seed = params\n    rng = np.random.default_rng(seed)\n    x, y = simulate_spike_counts(T, dt, rate_x, rate_y, sync_rate, tau, sigma, rng)\n    L_bins = int(np.floor(L / dt))\n    lags_bins, r_obs = correlogram(x, y, L_bins)\n    thr = max_stat_threshold(x, y, L_bins, K, alpha, rng)\n    sig_mask = np.abs(r_obs) >= thr\n    sig_lags_bins = lags_bins[sig_mask]\n    sig_lags_sec = (sig_lags_bins.astype(float) * dt).tolist()\n    # Round to 3 decimals for output\n    sig_lags_sec_rounded = [round(val, 3) for val in sig_lags_sec]\n    return sorted(sig_lags_sec_rounded)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (T, dt, lambda_x, lambda_y, lambda_sync, tau, sigma, L, K, alpha, seed)\n    test_cases = [\n        (20.0, 0.005, 15.0, 15.0, 5.0, 0.0,    0.003, 0.025, 200, 0.05, 101),\n        (20.0, 0.005, 15.0, 15.0, 5.0, 0.015,  0.003, 0.025, 200, 0.05, 102),\n        (20.0, 0.005, 15.0, 15.0, 0.0, 0.0,    0.0,   0.025, 200, 0.05, 103),\n        (10.0, 0.005, 20.0, 20.0, 5.0, 0.0,    0.002, 0.000, 200, 0.05, 104),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Build the string manually to ensure formatting.\n    def format_list(lst):\n      if not lst:\n        return '[]'\n      return '[' + ','.join([f'{x:.3f}' for x in lst]) + ']'\n\n    # Correcting for potential negative zeros from rounding\n    formatted_results = []\n    for res_list in results:\n      clean_list = []\n      for val in res_list:\n        if val == -0.0:\n          clean_list.append(0.0)\n        else:\n          clean_list.append(val)\n      formatted_results.append(clean_list)\n      \n    print('[' + ','.join([format_list(r) for r in formatted_results]) + ']')\n\nsolve()\n```"
        }
    ]
}