{
    "hands_on_practices": [
        {
            "introduction": "The first step in many spike detection algorithms is to set an amplitude threshold based on the background noise level. However, the presence of large-amplitude spikes or artifacts can bias standard estimators like the sample standard deviation. This exercise introduces a robust statistical method, the median absolute deviation (MAD), to estimate the noise scale in a way that is insensitive to such outliers, allowing for more reliable threshold setting .",
            "id": "4194221",
            "problem": "In extracellular spike detection, robust thresholding requires an estimate of the background noise scale that is insensitive to occasional large-amplitude artifacts. Consider a voltage trace $x(t)$ recorded at sampling rate $f_{s} = 30\\,\\text{kHz}$ from an extracellular electrode. The background noise in the trace is modeled as a stationary, zero-mean Gaussian process with unknown standard deviation $\\sigma$, and artifacts are sparse and nonstationary. An artifact-free interval $I$ of duration $0.5\\,\\text{ms}$ has been identified, yielding $15$ samples. The samples in $I$ (expressed in microvolts) are\n$$\n[-8,\\; 5,\\; -3,\\; 0,\\; 2,\\; -1,\\; 4,\\; -6,\\; 7,\\; -2,\\; 3,\\; -4,\\; 1,\\; -5,\\; 6]\\,\\mu\\text{V}.\n$$\nStarting from the definition of the median, the symmetry of the zero-mean Gaussian distribution, and the cumulative distribution function (CDF) of the normal distribution, derive the exact constant $k$ such that $\\mathrm{median}(|X|) = k\\,\\sigma$ when $X \\sim \\mathcal{N}(0,\\sigma^{2})$. Using this $k$, define a robust noise scale estimator based on the median absolute deviation (MAD) computed on the artifact-free samples as\n$$\n\\hat{\\sigma} \\equiv \\frac{\\mathrm{median}(|x_{i}|)}{k}.\n$$\nCompute $\\hat{\\sigma}$ numerically from the provided samples. Round your answer to four significant figures and express the final value in microvolts. Your final answer must be a single real number.",
            "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   Voltage trace: $x(t)$\n-   Sampling rate: $f_{s} = 30\\,\\text{kHz}$\n-   Background noise model: Stationary, zero-mean Gaussian process, $X \\sim \\mathcal{N}(0, \\sigma^2)$, with unknown standard deviation $\\sigma$.\n-   Artifact-free interval $I$ of duration $0.5\\,\\text{ms}$.\n-   Number of samples in $I$: $15$.\n-   Samples in $I$: $[-8,\\; 5,\\; -3,\\; 0,\\; 2,\\; -1,\\; 4,\\; -6,\\; 7,\\; -2,\\; 3,\\; -4,\\; 1,\\; -5,\\; 6]\\,\\mu\\text{V}$.\n-   Definition of the constant $k$: $\\mathrm{median}(|X|) = k\\,\\sigma$ for $X \\sim \\mathcal{N}(0,\\sigma^{2})$.\n-   Definition of the estimator: $\\hat{\\sigma} \\equiv \\frac{\\mathrm{median}(|x_{i}|)}{k}$.\n-   Required output: Numerical value of $\\hat{\\sigma}$, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n1.  **Scientific Soundness**: The problem describes a robust statistical estimation method (using the median absolute deviation, MAD) to find the scale of Gaussian noise, a standard technique in signal processing and neuroscience. The model of background noise as a zero-mean Gaussian process is a common and valid approximation.\n2.  **Completeness and Consistency**: All necessary information is provided. The number of samples is consistent with the sampling rate and interval duration: $(0.5 \\times 10^{-3}\\,\\text{s}) \\times (30 \\times 10^{3}\\,\\text{s}^{-1}) = 15$ samples, which matches the count of provided data points. The givens are self-consistent.\n3.  **Well-Posedness**: The problem asks for a derivation of a standard statistical constant and its application to a dataset. The tasks are clearly defined and lead to a unique, stable solution.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Derivation of the constant $k$\nLet $X$ be a random variable following a zero-mean Gaussian distribution, $X \\sim \\mathcal{N}(0, \\sigma^2)$. We are tasked with finding the constant $k$ such that the median of the absolute value of $X$ is $k\\sigma$. Let $Y = |X|$. The median of $Y$, denoted $m_Y$, is defined by the property $P(Y \\le m_Y) = \\frac{1}{2}$.\nFrom the problem statement, we have $m_Y = \\mathrm{median}(|X|) = k\\sigma$. Substituting this into the definition of the median:\n$$\nP(|X| \\le k\\sigma) = \\frac{1}{2}\n$$\nThis inequality can be expanded as:\n$$\nP(-k\\sigma \\le X \\le k\\sigma) = \\frac{1}{2}\n$$\nTo evaluate this probability, we standardize the random variable $X$ by defining $Z = \\frac{X}{\\sigma}$. The variable $Z$ follows the standard normal distribution, $Z \\sim \\mathcal{N}(0, 1)$. The inequality becomes:\n$$\nP(-k \\le Z \\le k) = \\frac{1}{2}\n$$\nLet $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution, defined as $\\Phi(z) = P(Z \\le z)$. The probability can be expressed in terms of the CDF:\n$$\nP(-k \\le Z \\le k) = \\Phi(k) - \\Phi(-k)\n$$\nDue to the symmetry of the standard normal distribution about $0$, we have the identity $\\Phi(-k) = 1 - \\Phi(k)$. Substituting this into the equation gives:\n$$\n\\Phi(k) - (1 - \\Phi(k)) = \\frac{1}{2}\n$$\n$$\n2\\Phi(k) - 1 = \\frac{1}{2}\n$$\nSolving for $\\Phi(k)$:\n$$\n2\\Phi(k) = 1 + \\frac{1}{2} = \\frac{3}{2}\n$$\n$$\n\\Phi(k) = \\frac{3}{4} = 0.75\n$$\nThus, $k$ is the value for which the cumulative probability of the standard normal distribution is $0.75$. This is the $75$th percentile, or third quartile, of the standard normal distribution. The exact analytical expression for $k$ is the inverse of the standard normal CDF evaluated at $0.75$:\n$$\nk = \\Phi^{-1}(0.75)\n$$\nThis completes the derivation of the constant $k$.\n\n### Computation of the noise scale estimator $\\hat{\\sigma}$\nThe estimator for the standard deviation $\\sigma$ is given by:\n$$\n\\hat{\\sigma} = \\frac{\\mathrm{median}(|x_{i}|)}{k}\n$$\nwhere $\\{x_i\\}$ are the provided voltage samples. The procedure is as follows:\n\n1.  **List the samples:**\n    The given samples are $x_i = [-8, 5, -3, 0, 2, -1, 4, -6, 7, -2, 3, -4, 1, -5, 6]$. The units are $\\mu\\text{V}$.\n\n2.  **Compute the absolute values of the samples:**\n    The set of absolute values, $|x_i|$, is $[8, 5, 3, 0, 2, 1, 4, 6, 7, 2, 3, 4, 1, 5, 6]$.\n\n3.  **Find the median of the absolute values:**\n    First, we sort the absolute values in non-decreasing order. The sorted list is:\n    $$\n    [0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 8]\n    $$\n    There are $N=15$ samples, which is an odd number. The median is the middle value, which is located at position $\\frac{N+1}{2} = \\frac{15+1}{2} = 8$.\n    The $8$th value in the sorted list is $4$. Therefore:\n    $$\n    \\mathrm{median}(|x_i|) = 4\\,\\mu\\text{V}\n    $$\n\n4.  **Calculate $\\hat{\\sigma}$:**\n    We now substitute the median and the expression for $k$ into the estimator formula:\n    $$\n    \\hat{\\sigma} = \\frac{4\\,\\mu\\text{V}}{\\Phi^{-1}(0.75)}\n    $$\n    The numerical value of $k = \\Phi^{-1}(0.75)$ is approximately $0.67448975$.\n    $$\n    \\hat{\\sigma} \\approx \\frac{4}{0.67448975}\\,\\mu\\text{V} \\approx 5.930436\\,\\mu\\text{V}\n    $$\n\n5.  **Round the result:**\n    The problem asks for the answer to be rounded to four significant figures. The first four significant figures of $5.930436$ are $5.930$. The fifth digit is $4$, so we round down.\n    $$\n    \\hat{\\sigma} \\approx 5.930\\,\\mu\\text{V}\n    $$\nThis value represents the robustly estimated standard deviation of the background noise.",
            "answer": "$$\n\\boxed{5.930}\n$$"
        },
        {
            "introduction": "After implementing a spike detector, we must rigorously quantify its performance. This practice guides you through the process of constructing Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves from scratch, which are essential tools for visualizing the trade-off between sensitivity and specificity. By implementing the calculations and comparing the resulting Area Under the Curve (AUC) metrics, you will gain a deep understanding of how to evaluate and compare detectors, particularly in the common scenario of severe class imbalance where PR curves are often more informative .",
            "id": "4194179",
            "problem": "You are given detection outputs from extracellular spike detection experiments in the form of real-valued scores and binary ground-truth labels indicating whether each timestamped observation corresponds to a true spike event. The goal is to derive Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves from first principles by sweeping a decision threshold across all unique score levels, compute the areas under these curves, and evaluate which curve is more informative under class imbalance. You must construct a complete, runnable program that executes these steps on a fixed test suite and outputs the requested metrics in the specified format.\n\nFundamental base and definitions to be used:\n- Given a set of observations indexed by $i \\in \\{1,\\dots,M\\}$, let the score be $s_i \\in \\mathbb{R}$ and the ground-truth label be $y_i \\in \\{0,1\\}$, where $y_i = 1$ indicates a true spike and $y_i = 0$ indicates a non-spike.\n- For any decision threshold $\\tau \\in \\mathbb{R}$, the predicted class is $\\hat{y}_i(\\tau) = 1$ if $s_i \\ge \\tau$ and $\\hat{y}_i(\\tau) = 0$ otherwise.\n- Let $P = \\sum_{i=1}^{M} \\mathbb{1}\\{y_i = 1\\}$ be the total number of positives and $N = \\sum_{i=1}^{M} \\mathbb{1}\\{y_i = 0\\}$ be the total number of negatives. Assume $P \\ge 1$ and $N \\ge 1$.\n- At threshold $\\tau$, define the confusion-matrix counts: true positives $TP(\\tau)$, false positives $FP(\\tau)$, false negatives $FN(\\tau)$, and true negatives $TN(\\tau)$, following their standard definitions. From these, define:\n  - True Positive Rate (also called sensitivity or recall) $TPR(\\tau) = \\dfrac{TP(\\tau)}{P}$.\n  - False Positive Rate $FPR(\\tau) = \\dfrac{FP(\\tau)}{N}$.\n  - Precision $Prec(\\tau) = \\dfrac{TP(\\tau)}{TP(\\tau) + FP(\\tau)}$ when $TP(\\tau) + FP(\\tau) \\ge 1$.\n\nCurve construction mandate:\n- Construct the ROC curve as the set of points $\\{(FPR(\\tau), TPR(\\tau))\\}$ obtained by sweeping $\\tau$ over the set of unique score values $U = \\{u_1, u_2, \\dots, u_K\\}$ sorted in strictly decreasing order, and include the endpoints $(0,0)$ and $(1,1)$ implied by $\\tau = +\\infty$ and $\\tau = -\\infty$, respectively. At each unique level, all items with equal score must be included simultaneously in the prediction set.\n- Construct the PR curve as the set of points $\\{(Rec(\\tau), Prec(\\tau))\\}$ with $Rec(\\tau) = TPR(\\tau)$ computed at the same sequence of thresholds over $U$ in decreasing order, interpreted as a right-continuous, step-wise function of recall.\n\nArea-under-curve definitions:\n- Define the area under the ROC curve $AUC_{\\mathrm{ROC}}$ as the Riemann integral of $TPR$ with respect to $FPR$ over $[0,1]$, computed numerically by the trapezoidal rule over the piecewise-linear path formed by the ROC points in ascending $FPR$.\n- Define the area under the PR curve $AUC_{\\mathrm{PR}}$ as the Riemann–Stieltjes integral of $Prec$ with respect to $Rec$ over $[0,1]$, computed numerically as a step-wise sum using right-continuous precision: if $(Rec_k, Prec_k)$ are the PR points ordered by non-decreasing recall and $Rec_0 = 0$, then\n  $$AUC_{\\mathrm{PR}} = \\sum_{k=1}^{K} \\left(Rec_k - Rec_{k-1}\\right) \\cdot Prec_k.$$\n\nInformativeness under class imbalance:\n- Let the prevalence be $\\pi = \\dfrac{P}{P+N}$. Consider normalized deviations from chance baselines:\n  - For ROC, the random baseline is $0.5$, so define $I_{\\mathrm{ROC}} = \\dfrac{AUC_{\\mathrm{ROC}} - 0.5}{0.5}$.\n  - For PR, the random baseline is $\\pi$, so define $I_{\\mathrm{PR}} = \\dfrac{AUC_{\\mathrm{PR}} - \\pi}{1 - \\pi}$ for $\\pi \\in [0,1)$.\n- Define the boolean indicator $B$ that PR is more informative than ROC by $B = \\mathbb{1}\\{I_{\\mathrm{PR}} > I_{\\mathrm{ROC}}\\}$.\n\nTask:\n- Implement a program that, for each test case, builds the ROC and PR curves as above by sweeping thresholds over unique score values in strictly decreasing order, computes $AUC_{\\mathrm{ROC}}$ and $AUC_{\\mathrm{PR}}$, computes $\\pi$, $I_{\\mathrm{ROC}}$, $I_{\\mathrm{PR}}$, and then $B$.\n- For numerical reporting, round $AUC_{\\mathrm{ROC}}$ and $AUC_{\\mathrm{PR}}$ to $6$ decimal places. The boolean $B$ must be reported as either the literal $True$ or $False$.\n\nTest suite:\nProvide the following three test cases, each as a pair of lists $(\\mathbf{s}, \\mathbf{y})$, where $\\mathbf{s}$ are scores and $\\mathbf{y}$ are labels:\n- Case $1$ (moderately separable, roughly balanced):\n  - $\\mathbf{s}^{(1)} = [\\,0.95,\\,0.90,\\,0.85,\\,0.80,\\,0.75,\\,0.70,\\,0.65,\\,0.60,\\,0.55,\\,0.50,\\,0.45,\\,0.40\\,]$\n  - $\\mathbf{y}^{(1)} = [\\,1,\\,1,\\,1,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0\\,]$\n- Case $2$ (severely imbalanced with a hard positive near low score):\n  - $\\mathbf{s}^{(2)} = [\\,0.99,\\,0.80,\\,0.75,\\,0.70,\\,0.65,\\,0.60,\\,0.58,\\,0.57,\\,0.56,\\,0.55,\\,0.54,\\,0.53,\\,0.52,\\,0.50,\\,0.49,\\,0.48,\\,0.30,\\,0.25,\\,0.20,\\,0.10\\,]$\n  - $\\mathbf{y}^{(2)} = [\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,0,\\,0,\\,0\\,]$\n- Case $3$ (all scores tied; boundary behavior):\n  - $\\mathbf{s}^{(3)} = [\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50\\,]$\n  - $\\mathbf{y}^{(3)} = [\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,0\\,]$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a list of lists, one inner list per test case, in the same order as above. Each inner list must be of the form $[AUC_{\\mathrm{ROC}}, AUC_{\\mathrm{PR}}, B]$, where $AUC_{\\mathrm{ROC}}$ and $AUC_{\\mathrm{PR}}$ are floats rounded to $6$ decimal places, and $B$ is a boolean. For example, an output with three test cases must look exactly like\n  \"[[0.812345,0.701234,True],[0.923456,0.812345,True],[0.500000,0.300000,False]]\"\nwith no extra spaces and no additional text.",
            "solution": "The problem requires the implementation of an algorithm to compute Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves, along with their respective areas under the curve ($AUC_{\\mathrm{ROC}}$ and $AUC_{\\mathrm{PR}}$), from a given set of detection scores and ground-truth labels. Furthermore, it asks for a comparison of their informativeness under class imbalance. The entire process must be derived from first principles as specified.\n\nThe core of the methodology is to evaluate the classification performance at various decision thresholds. A sample is classified as positive if its score $s_i$ is greater than or equal to a threshold $\\tau$, i.e., $\\hat{y}_i(\\tau) = 1$ if $s_i \\ge \\tau$. By varying $\\tau$ from $+\\infty$ down to $-\\infty$, we can trace the performance of the classifier, which forms the basis for the ROC and PR curves.\n\nFirst, let's establish the necessary quantities. We are given $M$ observations, each with a score $s_i$ and a binary label $y_i \\in \\{0, 1\\}$. The total number of positive instances (true spikes) is $P = \\sum_{i=1}^{M} \\mathbb{1}\\{y_i = 1\\}$, and the total number of negative instances (non-spikes) is $N = \\sum_{i=1}^{M} \\mathbb{1}\\{y_i = 0\\}$. The problem assumes $P \\ge 1$ and $N \\ge 1$.\n\nAt a given threshold $\\tau$, the four cardinal outcomes of binary classification are:\n- True Positives ($TP(\\tau)$): Number of instances where $s_i \\ge \\tau$ and $y_i = 1$.\n- False Positives ($FP(\\tau)$): Number of instances where $s_i \\ge \\tau$ and $y_i = 0$.\n- True Negatives ($TN(\\tau)$): Number of instances where $s_i < \\tau$ and $y_i = 0$.\n- False Negatives ($FN(\\tau)$): Number of instances where $s_i < \\tau$ and $y_i = 1$.\n\nFrom these, we define the key rates:\n- True Positive Rate (TPR), or Recall: $TPR(\\tau) = Rec(\\tau) = \\frac{TP(\\tau)}{P}$. This is the fraction of actual positives that are correctly identified.\n- False Positive Rate (FPR): $FPR(\\tau) = \\frac{FP(\\tau)}{N}$. This is the fraction of actual negatives that are incorrectly identified as positive.\n- Precision: $Prec(\\tau) = \\frac{TP(\\tau)}{TP(\\tau) + FP(\\tau)}$, defined for $TP(\\tau) + FP(\\tau) \\ge 1$. This is the fraction of predicted positives that are actually correct.\n\nThe algorithm proceeds as follows:\n\n**Step 1: Data Preparation and Sorting**\nThe fundamental insight for constructing these curves efficiently is that the values of $TP(\\tau)$, $FP(\\tau)$, and consequently all derived metrics, only change when the threshold $\\tau$ crosses a score value present in the data. Therefore, instead of sweeping $\\tau$ continuously, we only need to consider the unique score values as thresholds.\n\nThe problem mandates that we sort the unique scores in strictly decreasing order. A more robust and standard algorithm, which correctly handles tied scores as specified, is to sort all individual observations. We combine the scores $s_i$ and labels $y_i$ into pairs $(s_i, y_i)$ and sort these pairs in descending order based on the score $s_i$. In case of tied scores, the relative order of labels does not affect the calculation of cumulative $TP$ and $FP$ counts for that threshold level.\n\n**Step 2: Curve Point Generation**\nWe iterate through the sorted list of observations, effectively lowering the decision threshold. The problem specifies that all items with an equal score must be included simultaneously. This is achieved by processing observations in groups with tied scores.\n\nLet's initialize the cumulative counts $TP_{cum} = 0$ and $FP_{cum} = 0$. These correspond to a threshold $\\tau > \\max(s_i)$, where nothing is classified as positive. This gives us the starting point for the ROC curve, $(FPR, TPR) = (0, 0)$.\n\nWe then iterate through the sorted observations, grouping them by score. For each group of observations with a unique score value $u$:\n1. Count the number of positives ($\\Delta TP$) and negatives ($\\Delta FP$) within this group.\n2. Update the cumulative counts: $TP_{cum} \\leftarrow TP_{cum} + \\Delta TP$ and $FP_{cum} \\leftarrow FP_{cum} + \\Delta FP$. These updated counts represent the total true and false positives for a threshold $\\tau=u$.\n3. Calculate the new point on the curves:\n   - $FPR_{new} = \\frac{FP_{cum}}{N}$\n   - $TPR_{new} = \\frac{TP_{cum}}{P}$\n   - $Prec_{new} = \\frac{TP_{cum}}{TP_{cum} + FP_{cum}}$\n4. A new ROC point $(FPR_{new}, TPR_{new})$ and a new PR point $(TPR_{new}, Prec_{new})$ are generated. This process is repeated for all unique score values. The final state after processing all observations will have $TP_{cum} = P$ and $FP_{cum} = N$, yielding the ROC endpoint $(1, 1)$.\n\n**Step 3: Area Under Curve (AUC) Calculation**\n- **$AUC_{\\mathrm{ROC}}$**: The ROC curve is the set of generated $(FPR, TPR)$ points, starting from $(0,0)$ and ending at $(1,1)$. The area is computed using the trapezoidal rule on the piecewise-linear path connecting these points in order of increasing $FPR$. For a sequence of ROC points $(x_k, y_k)$ where $x_k = FPR_k$ and $y_k = TPR_k$, the area is:\n  $$AUC_{\\mathrm{ROC}} = \\sum_{k=1}^{L} \\frac{(y_k + y_{k-1})}{2} (x_k - x_{k-1})$$\n  where $L+1$ is the number of points in the ROC path.\n- **$AUC_{\\mathrm{PR}}$**: The PR curve is given by the points $(Rec_k, Prec_k)$, where $Rec_k = TPR_k$. The area is defined as an integral of a right-continuous step function. The provided formula is a numerical computation of this integral, equivalent to what is often called Average Precision:\n  $$AUC_{\\mathrm{PR}} = \\sum_{k=1}^{K} (Rec_k - Rec_{k-1}) \\cdot Prec_k$$\n  Here, $(Rec_k, Prec_k)$ are the PR points generated in order of non-decreasing recall, and we define $Rec_0 = 0$. Each term in the sum represents the area of a rectangle with height $Prec_k$ and width $(Rec_k - Rec_{k-1})$, which is the increase in recall at step $k$.\n\n**Step 4: Informativeness Comparison**\nThe curves' informativeness is assessed by normalizing their AUC values with respect to the performance of a random classifier.\n- For ROC, a random classifier yields an $AUC_{\\mathrm{ROC}}$ of $0.5$. The normalized information is $I_{\\mathrm{ROC}} = \\frac{AUC_{\\mathrm{ROC}} - 0.5}{0.5}$.\n- For PR, a random classifier yields an $AUC_{\\mathrm{PR}}$ equal to the prevalence of the positive class, $\\pi = \\frac{P}{P+N}$. The normalized information is $I_{\\mathrm{PR}} = \\frac{AUC_{\\mathrm{PR}} - \\pi}{1 - \\pi}$.\nThe boolean indicator $B = \\mathbb{1}\\{I_{\\mathrm{PR}} > I_{\\mathrm{ROC}}\\}$ determines if the PR curve is more informative than the ROC curve under this specific normalization. This comparison is particularly relevant in cases of class imbalance (i.e., when $\\pi$ is far from $0.5$), where the PR curve is often considered a more sensitive measure of performance than the ROC curve. The ROC baseline is constant, while the PR baseline changes with imbalance, making the latter a more challenging benchmark to beat.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the full analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (moderately separable, roughly balanced)\n        (\n            [0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40],\n            [1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n        ),\n        # Case 2 (severely imbalanced with a hard positive near low score)\n        (\n            [0.99, 0.80, 0.75, 0.70, 0.65, 0.60, 0.58, 0.57, 0.56, 0.55, 0.54, 0.53, 0.52, 0.50, 0.49, 0.48, 0.30, 0.25, 0.20, 0.10],\n            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n        ),\n        # Case 3 (all scores tied; boundary behavior)\n        (\n            [0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50],\n            [1, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n        )\n    ]\n\n    all_results = []\n    for scores, labels in test_cases:\n        result = _calculate_metrics(np.array(scores), np.array(labels))\n        all_results.append(result)\n\n    # Format the final output string\n    formatted_results = []\n    for auc_roc, auc_pr, b_flag in all_results:\n        formatted_results.append(f\"[{auc_roc:.6f},{auc_pr:.6f},{str(b_flag)}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _calculate_metrics(scores, labels):\n    \"\"\"\n    Calculates ROC/PR AUC and informativeness for a single test case.\n    \"\"\"\n    # Total number of positive and negative samples\n    pos_total = np.sum(labels == 1)\n    neg_total = len(labels) - pos_total\n\n    # The problem assumes P>=1 and N>=1, so no need to check for zero division here.\n    \n    # Combine scores and labels, then sort by score descending.\n    # A secondary sort key is not strictly needed for the logic but is good practice.\n    indices = np.argsort(scores, kind='mergesort')[::-1]\n    sorted_scores = scores[indices]\n    sorted_labels = labels[indices]\n    \n    roc_fpr = [0.0]\n    roc_tpr = [0.0]\n    pr_points = []\n\n    tp_cum = 0\n    fp_cum = 0\n    i = 0\n    while i < len(sorted_scores):\n        current_score = sorted_scores[i]\n        \n        # Group all items with the same score\n        j = i\n        while j < len(sorted_scores) and sorted_scores[j] == current_score:\n            j += 1\n        \n        # Count positives and negatives in this tied-score group\n        labels_in_group = sorted_labels[i:j]\n        delta_tp = np.sum(labels_in_group == 1)\n        \n        # Update cumulative counts. This happens after processing the group.\n        tp_cum += delta_tp\n        fp_cum += (j - i) - delta_tp\n\n        # Calculate metrics for this new threshold level\n        tpr = tp_cum / pos_total\n        fpr = fp_cum / neg_total\n        \n        roc_tpr.append(tpr)\n        roc_fpr.append(fpr)\n        \n        # Precision is only defined when TP+FP > 0\n        if (tp_cum + fp_cum) > 0:\n            precision = tp_cum / (tp_cum + fp_cum)\n            pr_points.append((tpr, precision))\n\n        i = j\n\n    # Compute Area Under the ROC Curve using the trapezoidal rule\n    # np.trapz integrates y (tpr) with respect to x (fpr)\n    auc_roc = np.trapz(roc_tpr, roc_fpr)\n\n    # Compute Area Under the PR Curve (Average Precision)\n    auc_pr = 0.0\n    last_recall = 0.0\n    # pr_points are naturally sorted by non-decreasing recall\n    for recall, precision in pr_points:\n        auc_pr += (recall - last_recall) * precision\n        last_recall = recall\n\n    # Compute informativeness metrics\n    prevalence = pos_total / (pos_total + neg_total)\n    \n    # ROC informativeness\n    i_roc = (auc_roc - 0.5) / 0.5\n    \n    # PR informativeness\n    # Prevent division by zero if prevalence is 1 (all positive samples)\n    if prevalence < 1.0:\n        i_pr = (auc_pr - prevalence) / (1 - prevalence)\n    else: # If pi=1, the denominator is 0. By convention, if auc_pr=1, info=1, else can be undefined/0.\n          # For this problem, N>=1, so pi < 1 is always true.\n        i_pr = 0.0\n\n    b_flag = i_pr > i_roc\n    \n    return auc_roc, auc_pr, b_flag\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        },
        {
            "introduction": "Once spikes are detected, their waveforms are converted into feature vectors for clustering. However, features derived from the same waveform (e.g., peak amplitude, valley depth, spike width) are often highly correlated, a condition known as multicollinearity, which can degrade the performance of downstream sorting algorithms. This hands-on exercise demonstrates how to use the Variance Inflation Factor (VIF) as a diagnostic tool to iteratively identify and remove redundant features, leading to a more parsimonious and robust feature set for analysis .",
            "id": "4194200",
            "problem": "You are given a mathematical abstraction of feature engineering for extracellular neural spike sorting. For each spike event, a feature vector is constructed from its waveform. The goal is to standardize features, analyze their covariance to understand redundancy, and avoid multicollinearity using the Variance Inflation Factor (VIF). You must implement a fully deterministic program that generates synthetic feature datasets under specified conditions, standardizes the features, computes the sample covariance matrix, and then iteratively removes redundant features using VIF until a specified redundancy threshold is met. The program must produce a single-line output aggregating the selected feature indices for each test case.\n\nUse only definitions and facts that are standard in statistical signal processing and multivariate analysis, interpreted in the context of neuroscience data analysis:\n- Feature standardization (z-scoring) across features: for each feature dimension (column), subtract the empirical mean and divide by the empirical standard deviation computed from the data.\n- The unbiased sample covariance matrix is computed with denominator $n-1$, where $n$ is the number of samples (spikes).\n- The Variance Inflation Factor (VIF) for a feature quantifies the inflation of its variance due to linear dependence on other features. For a feature $j$, it is defined by $VIF_j = \\dfrac{1}{1 - R_j^2}$, where $R_j^2$ is the coefficient of determination when regressing feature $j$ on all other features via ordinary least squares.\n- To ensure numerical robustness in ordinary least squares when computing $R_j^2$, use the Moore–Penrose pseudoinverse for linear solves when necessary.\n\nYour program must implement the following steps for each test case:\n1. Generate a synthetic feature matrix $X \\in \\mathbb{R}^{n \\times p}$ according to the specified generative model for that test case. All random sampling must use the specified seed and independent standard normal variates as indicated. All features are real-valued and dimensionless.\n2. Perform z-scoring across features (i.e., standardize each column of $X$ by subtracting the column mean and dividing by the column standard deviation computed with denominator $n-1$).\n3. Compute the unbiased sample covariance matrix $S$ of the standardized data using denominator $n-1$.\n4. Compute the Variance Inflation Factor (VIF) for each feature by regressing that feature on all the remaining features using ordinary least squares and computing $R^2$; then compute $VIF = 1/(1 - R^2)$. If numerical singularity arises, use the Moore–Penrose pseudoinverse to obtain a least-squares solution. All computations must be deterministic given the seed.\n5. Iteratively eliminate features to enforce a VIF threshold $\\tau$: while any feature’s VIF exceeds $\\tau$, remove the single feature with the largest VIF. If there are ties up to a tolerance of $10^{-12}$, remove the feature having the largest original column index. After each removal, recompute z-scoring, the sample covariance matrix, and VIF on the remaining features. Stop when all remaining features have VIF less than or equal to $\\tau$, or only a single feature remains.\n6. Return the retained feature indices as a list of original $0$-indexed column indices, sorted in ascending order.\n\nTest suite. Implement exactly the following three test cases, generating $X$ column-wise as specified. In each case, $g_0, g_1, \\dots$ denote independent standard normal vectors in $\\mathbb{R}^n$, sampled with the given seed. All square roots are the principal roots. Use the same seed for the whole test case to generate all required $g_k$ and any shared latent variables in the order they are needed. For clarity, denote by $n$ the number of spikes (rows) and by $p$ the number of features (columns).\n\n- Test case A (strong pairwise correlations and moderate coupling):\n  - Seed: $271828$.\n  - Dimensions: $n = 1500$, $p = 4$.\n  - Threshold: $\\tau = 5$.\n  - Construction:\n    - $x_0 = g_0$.\n    - $x_1 = 0.95\\,x_0 + \\sqrt{1 - 0.95^2}\\,g_1$.\n    - $x_2 = g_2$.\n    - $x_3 = 0.6\\,x_2 + \\sqrt{1 - 0.6^2}\\,g_3$.\n    - Form $X = [x_0, x_1, x_2, x_3]$ by column concatenation.\n\n- Test case B (nearly orthogonal features with a weak shared latent driver):\n  - Seed: $161803$.\n  - Dimensions: $n = 1200$, $p = 5$.\n  - Threshold: $\\tau = 5$.\n  - Construction:\n    - Let $s$ be a standard normal vector of length $n$.\n    - For $j \\in \\{0,1,2,3,4\\}$, set $x_j = g_j + 0.05\\,s$.\n    - Form $X = [x_0, x_1, x_2, x_3, x_4]$.\n\n- Test case C (compound multicollinearity including a near-linear combination and a highly correlated pair):\n  - Seed: $314159$.\n  - Dimensions: $n = 1000$, $p = 6$.\n  - Threshold: $\\tau = 4$.\n  - Construction:\n    - $x_0 = g_0$.\n    - $x_1 = g_1$.\n    - $x_2 = g_2$.\n    - $x_3 = x_0 + x_1 + 0.05\\,g_3$.\n    - $x_4 = 0.9\\,x_2 + \\sqrt{1 - 0.9^2}\\,g_4$.\n    - $x_5 = g_5$.\n    - Form $X = [x_0, x_1, x_2, x_3, x_4, x_5]$.\n\nFinal output format. Your program should produce a single line of output containing a comma-separated list of the retained index lists for the three test cases, enclosed in a single pair of square brackets, with each inner list also enclosed in square brackets and containing the retained original feature indices in ascending order. For example, a valid output format is of the form $[[i_{A,1}, i_{A,2}, \\dots], [i_{B,1}, \\dots], [i_{C,1}, \\dots]]$ with no additional text. The entries must be integers. No physical units are involved, and no angles are present.",
            "solution": "The problem requires the implementation of a deterministic algorithm for feature selection based on the Variance Inflation Factor (VIF). The process involves generating synthetic datasets with specified correlation structures, followed by an iterative procedure to remove features exhibiting high multicollinearity until a given threshold is met. The solution adheres strictly to the definitions and steps provided.\n\n### Principle-Based Design\n\nThe solution is designed around core principles of multivariate statistical analysis, specifically addressing the issue of multicollinearity in feature sets.\n\n1.  **Synthetic Data Generation**: The foundation of this problem is the creation of synthetic feature matrices $X \\in \\mathbb{R}^{n \\times p}$. The generative models provided for each test case are linear constructions based on independent standard normal variates, denoted as $g_k \\in \\mathbb{R}^n$. This method is a standard practice for creating datasets with precisely controlled, a priori known correlation structures. Such datasets are invaluable for validating the correctness and performance of statistical algorithms, as the expected outcome can often be reasoned from the data's construction. For instance, a feature defined as $x_1 = \\rho x_0 + \\sqrt{1-\\rho^2} g_1$ will have a theoretical correlation of $\\rho$ with feature $x_0$, allowing for a direct test of the algorithm's ability to detect this relationship.\n\n2.  **Feature Standardization (Z-scoring)**: Before assessing multicollinearity, all features are standardized. For each feature vector (column) $x_j$ in the data matrix $X$, its standardized counterpart $z_j$ is calculated as:\n    $$ z_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j} $$\n    where $\\mu_j$ is the empirical mean of feature $j$, and $\\sigma_j$ is its empirical standard deviation computed with a denominator of $n-1$ (unbiased estimator of variance). Standardization transforms all features to a common scale (zero mean and unit variance), which is crucial for two reasons: it prevents features with larger scales from disproportionately influencing the analysis, and it makes the resulting sample covariance matrix of the standardized data equivalent to the sample correlation matrix.\n\n3.  **Variance Inflation Factor (VIF)**: VIF is the central metric for quantifying multicollinearity. For a given feature $j$, its VIF is defined as:\n    $$ VIF_j = \\frac{1}{1 - R_j^2} $$\n    Here, $R_j^2$ is the coefficient of determination from an ordinary least squares (OLS) regression of feature $j$ (as the response variable) against all other features (as predictors). A high $R_j^2$ value (close to $1$) indicates that feature $j$ can be accurately predicted by a linear combination of the other features, implying redundancy. Consequently, a high VIF value signals severe multicollinearity. A common rule of thumb considers $VIF > 5$ or $VIF > 10$ as problematic.\n\n4.  **$R^2$ Calculation via Ordinary Least Squares (OLS)**: As per the problem's explicit instruction, $R_j^2$ is computed for each feature. Given the standardized feature matrix $Z$, we select one column $z_j$ as the response vector $y$ and the remaining columns as the predictor matrix $Z_{\\text{others}}$. The OLS problem is to find the coefficient vector $\\beta$ that minimizes the residual sum of squares in the model $y \\approx Z_{\\text{others}} \\beta$. For numerical stability, particularly in cases of near-singularity, the solution is found using the Moore-Penrose pseudoinverse:\n    $$ \\beta = (Z_{\\text{others}}^\\dagger) y $$\n    The predicted response is $\\hat{y} = Z_{\\text{others}} \\beta$. Since the data is centered (mean zero), the total sum of squares (TSS) and explained sum of squares (ESS) are simplified to $TSS = \\sum y_i^2$ and $ESS = \\sum \\hat{y}_i^2$. The coefficient of determination is then robustly computed as:\n    $$ R_j^2 = \\frac{ESS}{TSS} = \\frac{\\sum \\hat{y}_i^2}{\\sum y_i^2} $$\n\n5.  **Iterative Feature Elimination**: A greedy backward elimination strategy is employed to reduce multicollinearity. The algorithm proceeds as follows:\n    a.  Start with the full set of features.\n    b.  In each iteration, calculate the VIF for every feature currently in the set. This requires re-standardizing the current feature subset and re-computing all VIFs.\n    c.  Identify the maximum VIF value, $VIF_{\\text{max}}$.\n    d.  If $VIF_{\\text{max}}$ is less than or equal to the specified threshold $\\tau$, the process terminates as the remaining features meet the desired redundancy criterion.\n    e.  If $VIF_{\\text{max}} > \\tau$, the single feature with the highest VIF is removed. A specific tie-breaking rule is applied: if multiple features share the highest VIF (within a tolerance of $10^{-12}$), the one with the largest original column index is eliminated.\n    f.  The process repeats until the condition in (d) is met or only one feature remains.\n\nThis iterative, deterministic procedure ensures that the most redundant features are progressively removed, resulting in a final feature set with controlled multicollinearity. The final output is the sorted list of original, 0-indexed indices of the features that survive this elimination process.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n\n    def generator_A(n, rng):\n        \"\"\"Generates data for Test Case A.\"\"\"\n        g0 = rng.standard_normal(n)\n        g1 = rng.standard_normal(n)\n        g2 = rng.standard_normal(n)\n        g3 = rng.standard_normal(n)\n        x0 = g0\n        x1 = 0.95 * x0 + np.sqrt(1 - 0.95**2) * g1\n        x2 = g2\n        x3 = 0.6 * x2 + np.sqrt(1 - 0.6**2) * g3\n        return np.column_stack([x0, x1, x2, x3])\n\n    def generator_B(n, rng):\n        \"\"\"Generates data for Test Case B.\"\"\"\n        s = rng.standard_normal(n)\n        columns = []\n        for _ in range(5):\n            g = rng.standard_normal(n)\n            x = g + 0.05 * s\n            columns.append(x)\n        return np.column_stack(columns)\n\n    def generator_C(n, rng):\n        \"\"\"Generates data for Test Case C.\"\"\"\n        g0 = rng.standard_normal(n)\n        g1 = rng.standard_normal(n)\n        g2 = rng.standard_normal(n)\n        g3 = rng.standard_normal(n)\n        g4 = rng.standard_normal(n)\n        g5 = rng.standard_normal(n)\n        x0 = g0\n        x1 = g1\n        x2 = g2\n        x3 = x0 + x1 + 0.05 * g3\n        x4 = 0.9 * x2 + np.sqrt(1 - 0.9**2) * g4\n        x5 = g5\n        return np.column_stack([x0, x1, x2, x3, x4, x5])\n\n    test_cases = [\n        {'n': 1500, 'p': 4, 'tau': 5, 'seed': 271828, 'generator': generator_A},\n        {'n': 1200, 'p': 5, 'tau': 5, 'seed': 161803, 'generator': generator_B},\n        {'n': 1000, 'p': 6, 'tau': 4, 'seed': 314159, 'generator': generator_C},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case['n'], case['p'], case['tau'], case['seed'], case['generator'])\n        all_results.append(result)\n\n    # Format the final output string exactly as specified.\n    formatted_results = ','.join(map(str, all_results))\n    print(f\"[{formatted_results}]\")\n\ndef calculate_vifs_ols(standardized_features):\n    \"\"\"\n    Calculates VIF for each feature by regressing it on the others.\n    \"\"\"\n    n_samples, num_features = standardized_features.shape\n    vifs = np.zeros(num_features)\n\n    for i in range(num_features):\n        y = standardized_features[:, i]\n        \n        # Create predictor matrix excluding the current feature\n        X_reg = np.delete(standardized_features, i, axis=1)\n\n        # Handle edge case where there's only one predictor\n        if X_reg.shape[1] == 0:\n            vifs[i] = 1.0\n            continue\n        \n        # Solve for regression coefficients using Moore-Penrose pseudoinverse\n        beta = np.linalg.pinv(X_reg) @ y\n        \n        # Calculate predicted values\n        y_hat = X_reg @ beta\n        \n        # Calculate R^2. For standardized data, TSS = sum(y**2)\n        tss = np.sum(y**2)\n        ess = np.sum(y_hat**2)\n        \n        if tss < 1e-12:\n            # If TSS is zero, the feature column was constant (zero variance),\n            # which is perfectly predictable. R^2 is 1.\n            r_squared = 1.0\n        else:\n            r_squared = ess / tss\n            # Clamp to 1.0 to handle potential floating point inaccuracies\n            r_squared = min(r_squared, 1.0)\n            \n        # Calculate VIF from R^2\n        if 1.0 - r_squared < 1e-12:\n            vifs[i] = np.inf\n        else:\n            vifs[i] = 1.0 / (1.0 - r_squared)\n            \n    return vifs\n\ndef process_case(n, p, tau, seed, generator_func):\n    \"\"\"\n    Processes a single test case: generates data and iteratively removes features based on VIF.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    X = generator_func(n, rng)\n    \n    original_indices = list(range(p))\n    \n    while True:\n        num_features = len(original_indices)\n        if num_features <= 1:\n            break\n            \n        current_features = X[:, original_indices]\n        \n        # Step 2: Perform z-scoring on the current set of features\n        mean = np.mean(current_features, axis=0)\n        std = np.std(current_features, axis=0, ddof=1)\n        \n        # To avoid division by zero for constant features\n        std[std == 0] = 1.0\n        \n        standardized_features = (current_features - mean) / std\n\n        # Step 4: Compute VIF for each feature using OLS\n        vifs = calculate_vifs_ols(standardized_features)\n        \n        # Step 5: Iteratively eliminate features\n        max_vif = np.max(vifs)\n        \n        if max_vif <= tau:\n            break\n            \n        # Find all features with VIF close to the max (tie-breaking)\n        candidates = np.where(vifs >= max_vif - 1e-12)[0]\n        \n        # Map local indices of candidates to their original indices\n        candidate_original_indices = [original_indices[i] for i in candidates]\n        \n        # Remove the one with the largest original index\n        idx_to_remove = max(candidate_original_indices)\n        original_indices.remove(idx_to_remove)\n        \n    original_indices.sort()\n    return original_indices\n\nsolve()\n```"
        }
    ]
}