{
    "hands_on_practices": [
        {
            "introduction": "线性判别分析（LDA）的核心在于一个权重向量，它以最佳方式组合神经特征以进行分类。这个练习将引导您从第一性原理（贝叶斯规则和高斯假设）推导出这个基本的权重向量，并将其应用于一个具体的数值例子。通过这个实践，您将巩固对统计理论如何转化为解码器关键部分的理解，这是掌握LDA的基础。",
            "id": "4174380",
            "problem": "在一个两类神经解码任务中，一个群体响应向量 $\\mathbf{x} \\in \\mathbb{R}^{2}$ 在线性判别分析（LDA）的经典生成假设下被建模：每个类别 $k \\in \\{0,1\\}$ 根据一个多元正态分布生成 $\\mathbf{x}$，该分布具有类相关的均值 $\\boldsymbol{\\mu}_{k}$ 和一个类无关的协方差 $\\boldsymbol{\\Sigma}$。决策规则是根据贝叶斯决策理论，在错分代价相等的情况下，由对数似然比和先验概率构建的。从多元正态对数密度和贝叶斯决策规则出发，推导用于分离类别的线性判别函数中与 $\\mathbf{x}$ 相乘的经验LDA解码权重向量 $\\hat{\\mathbf{w}}$。然后，使用来自一个具有两个条件（类别）的神经科学实验的以下经验估计量，\n- 类别样本数量 $n_{0} = 50$ 和 $n_{1} = 60$，\n- 样本均值 $\\hat{\\boldsymbol{\\mu}}_{0} = (0, 1)$ 和 $\\hat{\\boldsymbol{\\mu}}_{1} = (1, 2)$，\n- 合并协方差\n$$\n\\hat{\\boldsymbol{\\Sigma}} = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix},\n$$\n计算这个二维特征空间的经验LDA解码权重向量 $\\hat{\\mathbf{w}}$ 的数值。将 $\\hat{\\mathbf{w}}$ 的最终数值分量精确地表示为最简分数。不需要四舍五入。",
            "solution": "问题要求推导经验线性判别分析（LDA）解码权重向量 $\\hat{\\mathbf{w}}$，并根据一组给定的经验数据进行数值计算。推导和计算过程如下。\n\n**1. LDA权重向量的推导**\n\n分类是基于给定观测向量 $\\mathbf{x} \\in \\mathbb{R}^{d}$ 时，两个类别 $k \\in \\{0,1\\}$ 的后验概率。根据具有相等错分代价的贝叶斯决策理论，如果类别 $k=1$ 的后验概率 $P(k=1|\\mathbf{x})$ 大于类别 $k=0$ 的后验概率，即 $P(k=1|\\mathbf{x}) > P(k=0|\\mathbf{x})$，我们就将 $\\mathbf{x}$ 分配给类别 $k=1$。\n\n使用贝叶斯定理，即 $P(k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|k)P(k)}{p(\\mathbf{x})}$，其中 $p(\\mathbf{x}|k)$ 是类条件密度，$P(k)$ 是类先验概率，决策规则可以重写为：\n$$\n\\frac{p(\\mathbf{x}|k=1)P(k=1)}{p(\\mathbf{x})} > \\frac{p(\\mathbf{x}|k=0)P(k=0)}{p(\\mathbf{x})}\n$$\n这可以简化为 $p(\\mathbf{x}|k=1)P(k=1) > p(\\mathbf{x}|k=0)P(k=0)$。为了获得线性决策函数，我们对两边取自然对数：\n$$\n\\ln(p(\\mathbf{x}|k=1)) + \\ln(P(k=1)) > \\ln(p(\\mathbf{x}|k=0)) + \\ln(P(k=0))\n$$\n这个不等式定义了一个判别函数 $\\delta(\\mathbf{x})$，我们将其与阈值 $0$ 进行比较：\n$$\n\\delta(\\mathbf{x}) = \\ln\\left(\\frac{p(\\mathbf{x}|k=1)}{p(\\mathbf{x}|k=0)}\\right) + \\ln\\left(\\frac{P(k=1)}{P(k=0)}\\right) > 0\n$$\n问题指出，类条件密度是多元正态分布，$p(\\mathbf{x}|k) = \\mathcal{N}(\\mathbf{x}; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma})$，具有一个共同的协方差矩阵 $\\boldsymbol{\\Sigma}$。对数密度为：\n$$\n\\ln(p(\\mathbf{x}|k)) = -\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) - \\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(\\boldsymbol{\\Sigma}))\n$$\n判别函数的第一项是对数似然比，$\\ln(p(\\mathbf{x}|k=1)) - \\ln(p(\\mathbf{x}|k=0))$。项 $-\\frac{d}{2}\\ln(2\\pi)$ 和 $-\\frac{1}{2}\\ln(\\det(\\boldsymbol{\\Sigma}))$ 对两个类别是共同的，在差值中被抵消：\n$$\n\\ln\\left(\\frac{p(\\mathbf{x}|k=1)}{p(\\mathbf{x}|k=0)}\\right) = -\\frac{1}{2}\\left[ (\\mathbf{x} - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1) - (\\mathbf{x} - \\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_0) \\right]\n$$\n展开二次型 $(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) = \\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - 2\\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} + \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_k$，我们发现关于 $\\mathbf{x}$ 的二次项 $\\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x}$ 被抵消了。\n$$\n\\ln\\left(\\frac{p(\\mathbf{x}|k=1)}{p(\\mathbf{x}|k=0)}\\right) = -\\frac{1}{2}\\left[ (-2\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} + \\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1) - (-2\\boldsymbol{\\mu}_0^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} + \\boldsymbol{\\mu}_0^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0) \\right]\n$$\n$$\n= \\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - \\boldsymbol{\\mu}_0^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}(\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0)\n$$\n$$\n= (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}(\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0)\n$$\n判别函数 $\\delta(\\mathbf{x})$ 写成 $\\mathbf{w}^T \\mathbf{x} + w_0$ 的形式。权重向量 $\\mathbf{w}$ 是表达式中与 $\\mathbf{x}$ 相乘的部分。通过观察，$\\mathbf{w}^T = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}^{-1}$。对两边取转置，并注意到 $\\boldsymbol{\\Sigma}^{-1}$ 是对称的，我们得到权重向量：\n$$\n\\mathbf{w} = \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)\n$$\n对于经验情况，我们将真实参数替换为从数据中得到的样本估计值 $(\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}})$，以获得经验权重向量 $\\hat{\\mathbf{w}}$：\n$$\n\\hat{\\mathbf{w}} = \\hat{\\boldsymbol{\\Sigma}}^{-1}(\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0)\n$$\n\n**2. 数值计算**\n\n提供的经验估计量是：\n- 样本均值：$\\hat{\\boldsymbol{\\mu}}_{0} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 和 $\\hat{\\boldsymbol{\\mu}}_{1} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\n- 合并协方差矩阵：$\\hat{\\boldsymbol{\\Sigma}} = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix}$。\n\n首先，我们计算样本均值之差：\n$$\n\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n其次，我们求合并协方差矩阵的逆矩阵 $\\hat{\\boldsymbol{\\Sigma}}^{-1}$。对于一个通用的 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，其逆矩阵由 $\\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$ 给出。$\\hat{\\boldsymbol{\\Sigma}}$ 的行列式是 $\\det(\\hat{\\boldsymbol{\\Sigma}}) = (3)(2) - (1)(1) = 5$。\n因此，逆矩阵是：\n$$\n\\hat{\\boldsymbol{\\Sigma}}^{-1} = \\frac{1}{5} \\begin{pmatrix} 2  -1 \\\\ -1  3 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5}  -\\frac{1}{5} \\\\ -\\frac{1}{5}  \\frac{3}{5} \\end{pmatrix}\n$$\n最后，我们将这些代入 $\\hat{\\mathbf{w}}$ 的公式中：\n$$\n\\hat{\\mathbf{w}} = \\hat{\\boldsymbol{\\Sigma}}^{-1}(\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0) = \\begin{pmatrix} \\frac{2}{5}  -\\frac{1}{5} \\\\ -\\frac{1}{5}  \\frac{3}{5} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n执行矩阵-向量乘法：\n$$\n\\hat{\\mathbf{w}} = \\begin{pmatrix} \\frac{2}{5}(1) - \\frac{1}{5}(1) \\\\ -\\frac{1}{5}(1) + \\frac{3}{5}(1) \\end{pmatrix} = \\begin{pmatrix} \\frac{2-1}{5} \\\\ \\frac{-1+3}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ \\frac{2}{5} \\end{pmatrix}\n$$\n经验LDA解码权重向量的分量是 $\\frac{1}{5}$ 和 $\\frac{2}{5}$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{5} \\\\\n\\frac{2}{5}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在二分类问题之上，本练习将LDA框架扩展到多类别场景，这在涉及多种刺激或条件的神经科学实验中非常普遍。您将推导出一组用于在多个类别之间做出决策的判别函数，并应用它们来对一个新的神经响应进行分类。这项实践展示了LDA方法的可扩展性和通用性，使其能够应对更复杂的解码挑战。",
            "id": "4174496",
            "problem": "一个神经科学实验室正在从多电极记录中获得的试验平均、二维神经响应特征中解码离散的刺激类别。该解码器使用线性判别分析（LDA）构建，该方法假设类条件神经响应是多变量正态的，并且各类别之间具有共同的协方差，并使用贝叶斯法则来选择后验概率最大的类别。\n\n存在三个刺激类别，索引为 $k \\in \\{1,2,3\\}$。共享协方差是单位矩阵 $\\boldsymbol{\\Sigma}=\\mathbf{I}$，类别先验概率相等，类别均值为 $\\boldsymbol{\\mu}_1=(1,0)$、$\\boldsymbol{\\mu}_2=(0,1)$ 和 $\\boldsymbol{\\mu}_3=(-1,0)$。一次新的试验产生一个特征向量 $\\mathbf{x}=(0.2,0.3)$。\n\n仅从贝叶斯法则和多变量正态对数似然出发，在各类别间协方差相同且先验概率相等的假设下，推导出LDA判别函数 $g_k(\\mathbf{x})$。然后，对 $k=1,2,3$ 计算 $g_k(\\mathbf{x})$，并解码使 $g_k(\\mathbf{x})$ 最大化的类别索引 $k^{\\ast}$。将解码的类别索引 $k^{\\ast}$ 作为最终答案报告。最终答案必须是一个没有单位的整数。",
            "solution": "这个问题是有效的，因为它提出了一个在神经科学中常用的统计数据分析领域中自洽、有科学依据且定义明确的问题。所有必需的数据和假设都已提供。\n\n目标是通过找到最大化后验概率 $P(C_k|\\mathbf{x})$ 的类别，将新数据点 $\\mathbf{x}$ 分类到三个类别之一 $C_k$（其中 $k \\in \\{1, 2, 3\\}$）。根据贝叶斯法则，后验概率由下式给出：\n$$\nP(C_k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|C_k) P(C_k)}{p(\\mathbf{x})}\n$$\n其中 $p(\\mathbf{x}|C_k)$ 是在给定类别 $C_k$ 的条件下观测到 $\\mathbf{x}$ 的类条件概率密度，$P(C_k)$ 是类别 $C_k$ 的先验概率，而 $p(\\mathbf{x}) = \\sum_{j=1}^{3} p(\\mathbf{x}|C_j) P(C_j)$ 是证据。由于 $p(\\mathbf{x})$ 对所有类别都是相同的，因此最大化后验概率 $P(C_k|\\mathbf{x})$ 等同于最大化似然与先验的乘积 $p(\\mathbf{x}|C_k) P(C_k)$。\n\n在计算上，处理这个乘积的对数更为方便，因为对数是一个严格递增的函数，不会改变最大值的位置。我们将判别函数 $g_k(\\mathbf{x})$ 定义为该乘积的自然对数：\n$$\ng_k(\\mathbf{x}) = \\ln\\left( p(\\mathbf{x}|C_k) P(C_k) \\right) = \\ln(p(\\mathbf{x}|C_k)) + \\ln(P(C_k))\n$$\n问题陈述，类条件密度 $p(\\mathbf{x}|C_k)$ 是均值为 $\\boldsymbol{\\mu}_k$、共享协方差矩阵为 $\\boldsymbol{\\Sigma}$ 的多变量正态分布。对于一个 $d$ 维特征向量 $\\mathbf{x}$，其概率密度函数为：\n$$\np(\\mathbf{x} | C_k) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\\right)\n$$\n对似然函数取自然对数，得到：\n$$\n\\ln(p(\\mathbf{x} | C_k)) = \\ln\\left(\\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}}\\right) - \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\n$$\n$$\n\\ln(p(\\mathbf{x} | C_k)) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\n$$\n将此代入 $g_k(\\mathbf{x})$ 的表达式中，我们得到：\n$$\ng_k(\\mathbf{x}) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) + \\ln(P(C_k))\n$$\n为了找到使 $g_k(\\mathbf{x})$ 最大化的类别 $k$，我们可以舍去任何相对于 $k$ 是常数的项。$-\\frac{d}{2}\\ln(2\\pi)$ 项是常数。由于协方差矩阵 $\\boldsymbol{\\Sigma}$ 对所有类别是共同的，所以 $-\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}|$ 项也是常数。问题陈述先验概率 $P(C_k)$ 相等，因此 $\\ln(P(C_k))$ 也是一个可以舍去的常数。这给我们留下一个简化的决策规则，即最大化：\n$$\ng'_k(\\mathbf{x}) = -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\n$$\n让我们展开二次项：\n$$\n(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) = \\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} - 2 \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} + \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_k\n$$\n将其代回 $g'_k(\\mathbf{x})$ 中：\n$$\ng'_k(\\mathbf{x}) = -\\frac{1}{2} \\left( \\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} - 2 \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} + \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_k \\right)\n$$\n项 $-\\frac{1}{2} \\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x}$ 也与 $k$ 无关，可以舍去。我们剩下要最大化的是：\n$$\ng_k(\\mathbf{x}) = -\\frac{1}{2} \\left( - 2 \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} + \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_k \\right)\n$$\n这可以简化为具有共同协方差和相等先验概率的标准线性判别函数：\n$$\ng_k(\\mathbf{x}) = \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} - \\frac{1}{2} \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_k\n$$\n这就是推导出的判别函数。现在，我们将其应用于问题的具体参数。\n维度为 $d=2$。共享协方差矩阵是单位矩阵 $\\boldsymbol{\\Sigma} = \\mathbf{I} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$，所以它的逆矩阵也是单位矩阵 $\\boldsymbol{\\Sigma}^{-1} = \\mathbf{I}$。\n新的特征向量是 $\\mathbf{x} = \\begin{pmatrix} 0.2 \\\\ 0.3 \\end{pmatrix}$。\n类别均值为 $\\boldsymbol{\\mu}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$、$\\boldsymbol{\\mu}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 和 $\\boldsymbol{\\mu}_3 = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$。\n\n当 $\\boldsymbol{\\Sigma}^{-1} = \\mathbf{I}$ 时，判别函数简化为：\n$$\ng_k(\\mathbf{x}) = \\boldsymbol{\\mu}_k^T \\mathbf{x} - \\frac{1}{2} \\boldsymbol{\\mu}_k^T \\boldsymbol{\\mu}_k\n$$\n我们现在为每个类别 $k \\in \\{1, 2, 3\\}$ 计算此函数的值。\n\n对于类别 $k=1$：\n$$\ng_1(\\mathbf{x}) = \\boldsymbol{\\mu}_1^T \\mathbf{x} - \\frac{1}{2} \\boldsymbol{\\mu}_1^T \\boldsymbol{\\mu}_1 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ 0.3 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\ng_1(\\mathbf{x}) = (1)(0.2) + (0)(0.3) - \\frac{1}{2} ((1)^2 + (0)^2) = 0.2 - \\frac{1}{2}(1) = -0.3\n$$\n\n对于类别 $k=2$：\n$$\ng_2(\\mathbf{x}) = \\boldsymbol{\\mu}_2^T \\mathbf{x} - \\frac{1}{2} \\boldsymbol{\\mu}_2^T \\boldsymbol{\\mu}_2 = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ 0.3 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\n$$\ng_2(\\mathbf{x}) = (0)(0.2) + (1)(0.3) - \\frac{1}{2} ((0)^2 + (1)^2) = 0.3 - \\frac{1}{2}(1) = -0.2\n$$\n\n对于类别 $k=3$：\n$$\ng_3(\\mathbf{x}) = \\boldsymbol{\\mu}_3^T \\mathbf{x} - \\frac{1}{2} \\boldsymbol{\\mu}_3^T \\boldsymbol{\\mu}_3 = \\begin{pmatrix} -1  0 \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ 0.3 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} -1  0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n$$\n$$\ng_3(\\mathbf{x}) = (-1)(0.2) + (0)(0.3) - \\frac{1}{2} ((-1)^2 + (0)^2) = -0.2 - \\frac{1}{2}(1) = -0.7\n$$\n\n比较判别函数的值：\n$g_1(\\mathbf{x}) = -0.3$\n$g_2(\\mathbf{x}) = -0.2$\n$g_3(\\mathbf{x}) = -0.7$\n\nLDA 规则是选择使 $g_k(\\mathbf{x})$ 最大化的类别 $k^*$：\n$$\nk^* = \\arg\\max_{k \\in \\{1,2,3\\}} g_k(\\mathbf{x})\n$$\n最大值是 $-0.2$，对应于类别 $k=2$。\n因此，解码的类别索引是 $k^* = 2$。",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "最后的这项实践练习在抽象理论与计算实践之间架起了一座桥梁。您将首先为一个已知的生成模型分析推导出贝叶斯最优准确率，这为解码性能设定了理论上限。接着，您将编写一个模拟程序，用采样数据训练一个LDA分类器，并验证其实证性能收敛于此理论最优值，从而加深对“当LDA的基本假设被满足时，它就是理想的线性分类器”这一核心概念的理解。",
            "id": "4174390",
            "problem": "要求您设计并分析一个仿真，用于在统计上明确定义的生成模型下，评估线性判别分析（LDA）在神经解码中的性能。考虑一个两类神经群体编码，其中观测到的特征向量 $\\mathbf{x} \\in \\mathbb{R}^2$ 服从以下类别条件模型：对于类别标签 $y \\in \\{0,1\\}$，$\\mathbf{x} \\mid y=k \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma})$，其中 $\\mathcal{N}$ 表示多元正态分布。假设类别先验概率相等，$P(y=0)=P(y=1)=\\tfrac{1}{2}$。协方差矩阵由相关系数 $ \\rho \\in (-1,1)$ 参数化，且方差为单位1，\n$$\n\\boldsymbol{\\Sigma}(\\rho) = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix},\n$$\n均值向量为\n$$\n\\boldsymbol{\\mu}_0 = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\qquad \\boldsymbol{\\mu}_1 = \\begin{pmatrix}\\delta \\\\ \\delta\\end{pmatrix},\n$$\n其中 $\\delta \\in \\mathbb{R}$ 控制均值间的分离程度。您将通过推导和仿真来验证，在这些假设下，LDA 能够达到贝叶斯最优分类准确率。\n\n从第一性原理出发，使用以下适合本情景的基础知识：\n- 贝叶斯分类器的定义，即最大化后验概率 $P(y=k \\mid \\mathbf{x})$ 的决策规则，这等价于对从类别条件密度和类别先验推导出的对数似然比进行阈值处理。\n- 多元正态密度函数及其在均值 $\\boldsymbol{\\mu}$ 和协方差 $\\boldsymbol{\\Sigma}$ 下的对数似然的显式形式。\n- 一个统计学事实：相等的类别先验和相等的类别协方差会产生一个关于 $\\mathbf{x}$ 的线性决策边界。\n\n您的任务是：\n1. 从多元正态对数似然比和相等的先验概率出发，推导出该模型的最优贝叶斯决策规则，明确地将线性判别方向和决策阈值表示为 $\\boldsymbol{\\mu}_0$、$\\boldsymbol{\\mu}_1$ 和 $\\boldsymbol{\\Sigma}$ 的函数。未经推导，不得引入任何快捷公式。\n2. 使用导出的一维判别变量，推导贝叶斯最优准确率的解析表达式，将其表示为 $\\boldsymbol{\\mu}_0$ 和 $\\boldsymbol{\\mu}_1$ 在 $\\boldsymbol{\\Sigma}$ 下的马氏距离平方的函数。将准确率表示为一个标量 $A(\\delta,\\rho) \\in [0,1]$。\n3. 实现一个仿真，以估计从数据中训练的LDA的经验准确率。对于每组参数设置，根据指定的高斯模型为每个类别生成 $n$ 个独立样本，从样本中估计类别均值和合并协方差，构建LDA判别器，并在从相同生成模型中抽取的留出集上计算经验分类准确率。为训练和测试使用单个大样本以最小化方差，并确保估计的协方差在数值上是稳定的。本问题不涉及物理单位。\n\n测试套件和要求的输出：\n- 使用以下参数值，每个参数值产生一个独立的测试用例：\n    1. $(\\rho,\\delta,n) = (0,1,80000)$。\n    2. $(\\rho,\\delta,n) = (0.8,1,80000)$。\n    3. $(\\rho,\\delta,n) = (-0.8,1,80000)$。\n    4. $(\\rho,\\delta,n) = (0.99,0.5,80000)$。\n    5. $(\\rho,\\delta,n) = (0.2,0,80000)$。\n- 对于每个测试用例，计算经验LDA准确率与任务2中推导的解析贝叶斯最优准确率之间的绝对差。令 $\\varepsilon = 0.02$，并返回一个布尔值，指示此绝对差是否小于或等于 $\\varepsilon$。\n- 最终输出格式：您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，“[$\\texttt{result1}$,$\\texttt{result2}$,$\\texttt{result3}$,$\\texttt{result4}$,$\\texttt{result5}$]”），其中每个 $\\texttt{resulti}$ 是对应于第 $i$ 个测试用例的布尔值。\n\n按照最终答案部分的要求，使用单个、自包含的程序实现解决方案。不允许外部输入或文件，并且应为随机数生成器设定种子以确保可复现性。您不能在本问题陈述中提供任何目标公式；请在您的解决方案中进行推导。",
            "solution": "该问题要求在一个指定的高斯生成模型下，对一个两类分类问题，通过推导和基于仿真的验证来评估线性判别分析（LDA）的性能。任务的核心是证明对于该模型，当类别条件密度是具有相同协方差矩阵的多元正态分布时，LDA分类器等价于贝叶斯最优分类器，并且其在大型数据集上的经验准确率收敛于解析推导出的贝叶斯最优准确率。\n\n解决方案按要求分为三个部分进行：\n1.  最优贝叶斯决策规则的推导。\n2.  解析贝叶斯最优准确率的推导。\n3.  设计并实现一个仿真，以计算训练后的LDA分类器的经验准确率，并将其与解析结果进行比较。\n\n### 1. 贝叶斯决策规则的推导\n\n目标是找到最小化分类错误概率的决策规则。对于给定的特征向量 $\\mathbf{x}$，贝叶斯分类器将其分配给最大化后验概率 $P(y=k \\mid \\mathbf{x})$ 的类别 $y=k$。决策规则是，如果 $P(y=1 \\mid \\mathbf{x}) > P(y=0 \\mid \\mathbf{x})$，则选择类别 $y=1$，否则选择类别 $y=0$。\n\n使用贝叶斯定理，$P(y=k \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid y=k)P(y=k)}{P(\\mathbf{x})}$。决策规则变为：\n$$\n\\frac{P(\\mathbf{x} \\mid y=1)P(y=1)}{P(\\mathbf{x})} > \\frac{P(\\mathbf{x} \\mid y=0)P(y=0)}{P(\\mathbf{x})}\n$$\n由于给定的类别先验概率相等，$P(y=0) = P(y=1) = \\frac{1}{2}$，这可以简化为对类别条件似然的比较：\n$$\nP(\\mathbf{x} \\mid y=1) > P(\\mathbf{x} \\mid y=0)\n$$\n对两边取自然对数（这是一个单调变换），我们得到等价的对数似然比检验：\n$$\n\\ln P(\\mathbf{x} \\mid y=1) - \\ln P(\\mathbf{x} \\mid y=0) > 0\n$$\n问题陈述，类别条件密度 $P(\\mathbf{x} \\mid y=k)$ 是一个二维多元正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma})$。其概率密度函数为：\n$$\nP(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_k)^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_k)\\right)\n$$\n其中 $d=2$ 是维度。对数似然为：\n$$\n\\ln P(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_k)^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_k)\n$$\n将此代入对数似然比检验，常数项 $-\\frac{d}{2}\\ln(2\\pi)$ 和 $-\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}|$（因为 $\\boldsymbol{\\Sigma}$ 对两个类别是共有的）会消掉：\n$$\n-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_1)^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_1) - \\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_0)^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_0)\\right) > 0\n$$\n$$\n(\\mathbf{x}-\\boldsymbol{\\mu}_0)^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_0) - (\\mathbf{x}-\\boldsymbol{\\mu}_1)^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_1) > 0\n$$\n展开二次型：\n$$\n(\\mathbf{x}^T\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - 2\\mathbf{x}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0 + \\boldsymbol{\\mu}_0^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0) - (\\mathbf{x}^T\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - 2\\mathbf{x}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_1^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1) > 0\n$$\n关于 $\\mathbf{x}$ 的二次项 $\\mathbf{x}^T\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}$ 消掉了。重新排列剩余项，得到一个线性决策规则：\n$$\n2\\mathbf{x}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 - 2\\mathbf{x}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0 > \\boldsymbol{\\mu}_1^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0\n$$\n$$\n(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)^T\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} > \\frac{1}{2}(\\boldsymbol{\\mu}_1^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0)\n$$\n这是 $\\mathbf{w}^T \\mathbf{x} > c$ 的形式，它定义了一个线性分类器。**线性判别方向**是：\n$$\n\\mathbf{w} = \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)\n$$\n而**决策阈值**是：\n$$\nc = \\frac{1}{2}(\\boldsymbol{\\mu}_1^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0) = \\frac{1}{2}(\\mathbf{w}^T\\boldsymbol{\\mu}_1 + \\mathbf{w}^T\\boldsymbol{\\mu}_0) = \\mathbf{w}^T\\left(\\frac{\\boldsymbol{\\mu}_0 + \\boldsymbol{\\mu}_1}{2}\\right)\n$$\n这就是最优贝叶斯分类器的决策规则，其形式与LDA分类器完全相同。\n\n### 2. 贝叶斯最优准确率的推导\n\nLDA/贝叶斯分类器将数据 $\\mathbf{x}$ 投影到方向 $\\mathbf{w}$ 上，并基于一个阈值进行分类。投影后的标量变量是 $z = \\mathbf{w}^T \\mathbf{x}$。由于 $\\mathbf{x}$ 是一个高斯随机向量，因此 $z$ 是一个高斯随机变量。其在类别 $y=k$ 条件下的分布是 $z \\mid y=k \\sim \\mathcal{N}(E[z|y=k], \\text{Var}(z|y=k))$。\n\n对于类别 $k$，投影变量的均值是：\n$$\n\\mu_{z,k} = E[\\mathbf{w}^T \\mathbf{x} \\mid y=k] = \\mathbf{w}^T E[\\mathbf{x} \\mid y=k] = \\mathbf{w}^T \\boldsymbol{\\mu}_k\n$$\n方差对两个类别是相同的，因为 $\\boldsymbol{\\Sigma}$ 是共享的：\n$$\n\\sigma_z^2 = \\text{Var}(\\mathbf{w}^T \\mathbf{x}) = \\mathbf{w}^T \\boldsymbol{\\Sigma} \\mathbf{w} = (\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0))^T \\boldsymbol{\\Sigma} (\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)) = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}^{-T} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)\n$$\n由于 $\\boldsymbol{\\Sigma}$ 是协方差矩阵，它是对称的（$\\boldsymbol{\\Sigma}^T=\\boldsymbol{\\Sigma}$），其逆矩阵也是对称的。\n$$\n\\sigma_z^2 = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0) = \\Delta^2\n$$\n这个量 $\\Delta^2$ 是均值 $\\boldsymbol{\\mu}_0$ 和 $\\boldsymbol{\\mu}_1$ 之间的马氏距离的平方。投影数据的标准差是 $\\sigma_z = \\Delta$。\n\n现在分类任务是一维的：给定 $z_0 \\sim \\mathcal{N}(\\mu_{z,0}, \\Delta^2)$ 和 $z_1 \\sim \\mathcal{N}(\\mu_{z,1}, \\Delta^2)$，求最优阈值和由此产生的准确率。最优阈值是均值的中点：\n$$\nc_z = \\frac{\\mu_{z,0} + \\mu_{z,1}}{2} = \\frac{\\mathbf{w}^T\\boldsymbol{\\mu}_0 + \\mathbf{w}^T\\boldsymbol{\\mu}_1}{2} = \\mathbf{w}^T\\left(\\frac{\\boldsymbol{\\mu}_0 + \\boldsymbol{\\mu}_1}{2}\\right) = c\n$$\n总准确率 $A$ 是类别条件准确率的平均值，因为先验概率相等：\n$A = \\frac{1}{2} P(\\text{正确} \\mid y=0) + \\frac{1}{2} P(\\text{正确} \\mid y=1)$。假设 $\\mathbf{w}^T\\boldsymbol{\\mu}_1 > \\mathbf{w}^T\\boldsymbol{\\mu}_0$，决策为 $y=1$ 如果 $z>c_z$。\n$$\nP(\\text{正确} \\mid y=1) = P(z > c_z \\mid y=1) = P\\left(\\frac{z-\\mu_{z,1}}{\\Delta} > \\frac{c_z-\\mu_{z,1}}{\\Delta}\\right) = P\\left(Z > \\frac{(\\mu_{z,0}+\\mu_{z,1})/2 - \\mu_{z,1}}{\\Delta}\\right)\n$$\n其中 $Z \\sim \\mathcal{N}(0,1)$。这等于 $P(Z > \\frac{\\mu_{z,0}-\\mu_{z,1}}{2\\Delta}) = P(Z  \\frac{\\mu_{z,1}-\\mu_{z,0}}{2\\Delta})$。类似地，$P(\\text{正确} \\mid y=0) = P(Z  \\frac{\\mu_{z,1}-\\mu_{z,0}}{2\\Delta})$。\n投影均值的分离度是 $\\mu_{z,1}-\\mu_{z,0} = \\mathbf{w}^T(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_0) = (\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_0) = \\Delta^2$。\n所以每个类别的准确率是 $P(Z  \\frac{\\Delta^2}{2\\Delta}) = P(Z  \\frac{\\Delta}{2}) = \\Phi(\\frac{\\Delta}{2})$，其中 $\\Phi$ 是标准正态分布的累积分布函数（CDF）。\n总准确率为 $A = \\Phi(\\frac{\\Delta}{2})$。\n\n对于特定模型： $\\boldsymbol{\\mu}_0 = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$, $\\boldsymbol{\\mu}_1 = \\begin{pmatrix}\\delta \\\\ \\delta\\end{pmatrix}$, $\\boldsymbol{\\Sigma}(\\rho) = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$。\n逆协方差矩阵是 $\\boldsymbol{\\Sigma}^{-1} = \\frac{1}{1-\\rho^2}\\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}$。\n马氏距离的平方是：\n$$\n\\Delta^2 = (\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_0) = \\begin{pmatrix}\\delta  \\delta\\end{pmatrix} \\frac{1}{1-\\rho^2}\\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} \\begin{pmatrix}\\delta \\\\ \\delta\\end{pmatrix}\n$$\n$$\n\\Delta^2 = \\frac{\\delta^2}{1-\\rho^2} \\begin{pmatrix}1-\\rho  1-\\rho\\end{pmatrix} \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\frac{\\delta^2(2-2\\rho)}{1-\\rho^2} = \\frac{2\\delta^2(1-\\rho)}{(1-\\rho)(1+\\rho)} = \\frac{2\\delta^2}{1+\\rho}\n$$\n因此，贝叶斯最优准确率为：\n$$\nA(\\delta, \\rho) = \\Phi\\left(\\frac{1}{2}\\sqrt{\\frac{2\\delta^2}{1+\\rho}}\\right) = \\Phi\\left(\\frac{|\\delta|}{\\sqrt{2(1+\\rho)}}\\right)\n$$\n\n### 3. 仿真设计\n\n该仿真验证了在有限的大型数据集上训练的LDA分类器所达到的经验准确率，能够紧密逼近理论上的贝叶斯最优准确率 $A(\\delta, \\rho)$。\n\n对于每组参数 $(\\rho, \\delta, n)$：\n1.  **解析准确率计算**：使用推导出的公式计算理论准确率 $A_{Bayes}$。对于数值实现，标准正态CDF $\\Phi(x)$ 使用误差函数 $\\text{erf}(x)$ 表示：$\\Phi(x) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right)$。\n    $$\n    A_{Bayes} = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{|\\delta|}{\\sqrt{2(1+\\rho)}\\sqrt{2}}\\right)\\right) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{|\\delta|}{2\\sqrt{1+\\rho}}\\right)\\right)\n    $$\n2.  **数据生成**：生成两个数据集：一个训练集和一个留出测试集。使用一个独立的测试集（“留出集”）对于获得分类器泛化性能的无偏估计至关重要。\n    *   **训练集**：类别0的 $n$ 个样本，$X_{train,0} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma})$，以及类别1的 $n$ 个样本，$X_{train,1} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma})$。\n    *   **测试集**：为确保准确率估计的低方差，独立生成一个同样大小的大型测试集：类别0的 $n$ 个样本，$X_{test,0} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma})$，以及类别1的 $n$ 个样本，$X_{test,1} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma})$。固定的随机种子确保可复现性。\n3.  **LDA参数估计**：从训练数据中估计LDA参数。\n    *   样本均值：$\\hat{\\boldsymbol{\\mu}}_0 = \\frac{1}{n}\\sum_{\\mathbf{x} \\in X_{train,0}} \\mathbf{x}$，$\\hat{\\boldsymbol{\\mu}}_1 = \\frac{1}{n}\\sum_{\\mathbf{x} \\in X_{train,1}} \\mathbf{x}$。\n    *   样本协方差：$\\hat{\\boldsymbol{\\Sigma}}_0$ 和 $\\hat{\\boldsymbol{\\Sigma}}_1$ 是每个类别的无偏样本协方差矩阵。\n    *   合并协方差：$\\hat{\\boldsymbol{\\Sigma}}_{pooled} = \\frac{(n-1)\\hat{\\boldsymbol{\\Sigma}}_0 + (n-1)\\hat{\\boldsymbol{\\Sigma}}_1}{(n-1)+(n-1)} = \\frac{\\hat{\\boldsymbol{\\Sigma}}_0 + \\hat{\\boldsymbol{\\Sigma}}_1}{2}$。\n4.  **经验准确率计算**：将训练好的分类器应用于测试集。\n    *   计算估计的权重向量 $\\hat{\\mathbf{w}} = \\hat{\\boldsymbol{\\Sigma}}_{pooled}^{-1}(\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0)$ 和阈值 $\\hat{c} = \\hat{\\mathbf{w}}^T(\\hat{\\boldsymbol{\\mu}}_0 + \\hat{\\boldsymbol{\\mu}}_1)/2$。\n    *   通过将得分 $\\hat{\\mathbf{w}}^T \\mathbf{x}_{test}$ 与阈值 $\\hat{c}$ 进行比较来对测试样本 $\\mathbf{x}_{test}$ 进行分类。\n    *   经验准确率 $A_{emp}$ 是测试集中被正确分类的样本的比例。\n5.  **比较**：计算绝对差 $|A_{emp} - A_{Bayes}|$，并与容差 $\\varepsilon=0.02$ 进行核对。结果是一个布尔值。\n\n对所有五个测试用例重复此过程。大的样本量 $n=80000$ 确保估计的参数 $(\\hat{\\boldsymbol{\\mu}}_k, \\hat{\\boldsymbol{\\Sigma}}_{pooled})$ 非常接近真实参数，因此训练后的LDA的经验准确率应非常接近理论上的贝叶斯最优准确率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Solves the LDA simulation problem by deriving, simulating, and comparing\n    analytic and empirical classification accuracies.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (rho, delta, n)\n        (0.0, 1.0, 80000),\n        (0.8, 1.0, 80000),\n        (-0.8, 1.0, 80000),\n        (0.99, 0.5, 80000),\n        (0.2, 0.0, 80000),\n    ]\n\n    # Global parameters\n    random_seed = 42\n    epsilon = 0.02\n    \n    # Use a single RNG for reproducibility across all cases\n    rng = np.random.default_rng(random_seed)\n\n    # -------- Helper Functions --------\n\n    def calculate_analytic_accuracy(rho, delta):\n        \"\"\"\n        Calculates the theoretical Bayes-optimal accuracy A(delta, rho).\n        A = Phi(|delta| / sqrt(2 * (1 + rho))), where Phi is the standard normal CDF.\n        \"\"\"\n        if 1.0 + rho == 0:\n            # Avoid division by zero or sqrt of negative. Given rho in (-1,1), this is only for rho -> -1.\n            # If rho = -1 and delta != 0, accuracy is 1. If delta = 0, it's 0.5.\n            # For this problem's constraints, 1+rho is always > 0.\n            return 1.0 if delta != 0 else 0.5\n\n        if delta == 0:\n            return 0.5\n        \n        # A = Phi(x) = 0.5 * (1 + erf(x / sqrt(2)))\n        # x = |delta| / sqrt(2 * (1 + rho))\n        arg_for_erf = abs(delta) / (2.0 * np.sqrt(1.0 + rho))\n        accuracy = 0.5 * (1.0 + erf(arg_for_erf))\n        \n        return accuracy\n\n    def run_simulation(rho, delta, n_samples_per_class, rng_instance):\n        \"\"\"\n        Runs the LDA simulation for a given set of parameters.\n        \"\"\"\n        # 1. Define the true generative model parameters\n        mu0_true = np.array([0.0, 0.0])\n        mu1_true = np.array([delta, delta])\n        cov_true = np.array([[1.0, rho], [rho, 1.0]])\n\n        # 2. Generate training data\n        train_data_0 = rng_instance.multivariate_normal(mu0_true, cov_true, size=n_samples_per_class)\n        train_data_1 = rng_instance.multivariate_normal(mu1_true, cov_true, size=n_samples_per_class)\n\n        # 3. Estimate LDA parameters from training data\n        mu0_hat = np.mean(train_data_0, axis=0)\n        mu1_hat = np.mean(train_data_1, axis=0)\n        \n        # Unbiased sample covariance (ddof=1)\n        cov0_hat = np.cov(train_data_0, rowvar=False, ddof=1)\n        cov1_hat = np.cov(train_data_1, rowvar=False, ddof=1)\n        \n        # Pooled covariance\n        cov_pooled_hat = (cov0_hat + cov1_hat) / 2.0\n        \n        # Ensure numerical stability for inversion, although with n=80000 it's unlikely to be an issue\n        try:\n            inv_cov_pooled_hat = np.linalg.inv(cov_pooled_hat)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if singular, or add regularization (not specified, so inv is preferred)\n            inv_cov_pooled_hat = np.linalg.pinv(cov_pooled_hat)\n\n        # 4. Construct the trained LDA classifier\n        w_hat = inv_cov_pooled_hat @ (mu1_hat - mu0_hat)\n        c_hat = 0.5 * w_hat.T @ (mu0_hat + mu1_hat)\n\n        # 5. Generate a separate held-out test set\n        test_data_0 = rng_instance.multivariate_normal(mu0_true, cov_true, size=n_samples_per_class)\n        test_data_1 = rng_instance.multivariate_normal(mu1_true, cov_true, size=n_samples_per_class)\n\n        # 6. Compute empirical accuracy on the test set\n        # For class 0, prediction is correct if w^T.x = c\n        scores_0 = test_data_0 @ w_hat\n        correct_0 = np.sum(scores_0 = c_hat)\n        \n        # For class 1, prediction is correct if w^T.x > c\n        scores_1 = test_data_1 @ w_hat\n        correct_1 = np.sum(scores_1 > c_hat)\n\n        total_correct = correct_0 + correct_1\n        total_samples = 2 * n_samples_per_class\n        empirical_accuracy = total_correct / total_samples\n        \n        return empirical_accuracy\n\n    # -------- Main Logic --------\n\n    results = []\n    for rho_val, delta_val, n_val in test_cases:\n        # Calculate analytic Bayes-optimal accuracy\n        analytic_acc = calculate_analytic_accuracy(rho_val, delta_val)\n        \n        # Estimate empirical LDA accuracy via simulation\n        empirical_acc = run_simulation(rho_val, delta_val, n_val, rng)\n        \n        # Compare and store the boolean result\n        abs_diff = abs(empirical_acc - analytic_acc)\n        results.append(abs_diff = epsilon)\n\n    # Final print statement in the exact required format.\n    # The Python 'bool' type stringifies to 'True' and 'False' with a capital letter.\n    # The problem example shows lowercase 'true', 'false'. Let's match that.\n    print(f\"[{','.join(map(str, results)).lower()}]\")\n\nsolve()\n\n```"
        }
    ]
}