{
    "hands_on_practices": [
        {
            "introduction": "Before we can interpret or diagnose a linear model, we must first understand how its core parameters are estimated. This first practice focuses on the fundamental mechanics of Ordinary Least Squares (OLS), demonstrating how to calculate the slope and intercept of the regression line from basic summary statistics . Mastering this foundational calculation is the essential first step in applying linear regression to analyze relationships in data.",
            "id": "4840047",
            "problem": "A clinical pharmacology team is conducting an early-phase dose-finding investigation of a novel antihypertensive agent. For each participant, the team records the administered dose in milligrams and the change in Systolic Blood Pressure (SBP) in millimeters of mercury at four weeks. Assume the relationship between dose and SBP change for participant $i$ follows the simple linear regression model\n$$\nY_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 X_i \\;+\\; \\varepsilon_i,\n$$\nwhere $Y_i$ is the SBP change, $X_i$ is the dose, $\\beta_0$ and $\\beta_1$ are unknown regression parameters, and the errors $\\varepsilon_i$ are independent with mean $0$ and constant variance, reflecting the standard conditions under which ordinary least squares estimation is justified in medical evidence generation.\n\nA dataset of $n=40$ participants yields the following summary statistics: sample means $\\bar{X}=5$ and $\\bar{Y}=12$, and centered sums $S_{xx}=160$ and $S_{xy}=320$. Starting from the least squares principle, derive the estimators $(\\hat{\\beta}_0,\\hat{\\beta}_1)$ and compute their numerical values using the provided summaries. Then, provide a clinically meaningful interpretation of the slope $\\hat{\\beta}_1$ in the context of dose-response for antihypertensive efficacy, explicitly identifying the change in SBP per unit change in dose. Report $(\\hat{\\beta}_0,\\hat{\\beta}_1)$ as exact values with no rounding. The interpretation should use milligrams for dose and millimeters of mercury for SBP change, but the numerical estimators themselves should be reported without units.",
            "solution": "The problem is evaluated as valid. It presents a well-posed, scientifically grounded, and objective question in statistical inference applied to a standard clinical research context. The provided summary statistics are complete and consistent, allowing for a unique solution via the ordinary least squares method.\n\nThe simple linear regression model is given by\n$$\nY_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 X_i \\;+\\; \\varepsilon_i,\n$$\nwhere $Y_i$ is the change in Systolic Blood Pressure (SBP), $X_i$ is the administered dose, $\\beta_0$ and $\\beta_1$ are the unknown regression parameters, and $\\varepsilon_i$ are independent, identically distributed random errors with mean $E[\\varepsilon_i] = 0$ and constant variance $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2$.\n\nThe principle of ordinary least squares (OLS) seeks to find the estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that minimize the sum of squared residuals, $SSR$. The sum of squared residuals is defined as the function of $\\beta_0$ and $\\beta_1$:\n$$\nSSR(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^{n} (Y_i - (\\beta_0 + \\beta_1 X_i))^2.\n$$\nTo find the values of $\\beta_0$ and $\\beta_1$ that minimize this function, we take the partial derivatives of $SSR(\\beta_0, \\beta_1)$ with respect to each parameter and set them equal to zero. These are the normal equations.\n\nThe partial derivative with respect to $\\beta_0$ is:\n$$\n\\frac{\\partial SSR}{\\partial \\beta_0} = \\sum_{i=1}^{n} 2(Y_i - \\beta_0 - \\beta_1 X_i)(-1) = -2 \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i).\n$$\nSetting this to zero yields the first normal equation for the estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$:\n$$\n\\sum_{i=1}^{n} (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = 0 \\\\\n\\sum Y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1 \\sum X_i = 0.\n$$\nDividing by the sample size $n$ gives:\n$$\n\\bar{Y} - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\bar{X} = 0,\n$$\nwhich can be rearranged to express the intercept estimator $\\hat{\\beta}_0$ in terms of the slope estimator $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}.\n$$\nThis derivation shows that the least squares regression line must pass through the point of sample means, $(\\bar{X}, \\bar{Y})$.\n\nThe partial derivative with respect to $\\beta_1$ is:\n$$\n\\frac{\\partial SSR}{\\partial \\beta_1} = \\sum_{i=1}^{n} 2(Y_i - \\beta_0 - \\beta_1 X_i)(-X_i) = -2 \\sum_{i=1}^{n} X_i(Y_i - \\beta_0 - \\beta_1 X_i).\n$$\nSetting this to zero yields the second normal equation:\n$$\n\\sum_{i=1}^{n} X_i(Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = 0 \\\\\n\\sum X_i Y_i - \\hat{\\beta}_0 \\sum X_i - \\hat{\\beta}_1 \\sum X_i^2 = 0.\n$$\nNow, substitute the expression for $\\hat{\\beta}_0$ from the first normal equation into the second:\n$$\n\\sum X_i Y_i - (\\bar{Y} - \\hat{\\beta}_1 \\bar{X}) \\sum X_i - \\hat{\\beta}_1 \\sum X_i^2 = 0 \\\\\n\\sum X_i Y_i - \\bar{Y}\\sum X_i + \\hat{\\beta}_1 \\bar{X}\\sum X_i - \\hat{\\beta}_1 \\sum X_i^2 = 0.\n$$\nRearranging to solve for $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 \\left( \\bar{X}\\sum X_i - \\sum X_i^2 \\right) = \\bar{Y}\\sum X_i - \\sum X_i Y_i.\n$$\nMultiplying by $-1$ and substituting $\\sum X_i = n\\bar{X}$:\n$$\n\\hat{\\beta}_1 \\left( \\sum X_i^2 - n\\bar{X}^2 \\right) = \\sum X_i Y_i - n\\bar{X}\\bar{Y}.\n$$\nThe terms in parentheses are the definitions of the centered sums of squares and products:\n$S_{xx} = \\sum_{i=1}^{n} (X_i - \\bar{X})^2 = \\sum X_i^2 - n\\bar{X}^2$.\n$S_{xy} = \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) = \\sum X_i Y_i - n\\bar{X}\\bar{Y}$.\n\nThus, the derived estimator for the slope parameter $\\beta_1$ is:\n$$\n\\hat{\\beta}_1 = \\frac{\\sum X_i Y_i - n\\bar{X}\\bar{Y}}{\\sum X_i^2 - n\\bar{X}^2} = \\frac{S_{xy}}{S_{xx}}.\n$$\nThe derived estimators are therefore $\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}$ and $\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}$.\n\nNow, we compute the numerical values using the provided summary statistics for the dataset of $n=40$ participants: $\\bar{X}=5$, $\\bar{Y}=12$, $S_{xx}=160$, and $S_{xy}=320$.\n\nFirst, we compute the slope estimate $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{320}{160} = 2.\n$$\nNext, we compute the intercept estimate $\\hat{\\beta}_0$:\n$$\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X} = 12 - (2)(5) = 12 - 10 = 2.\n$$\nThe estimated regression equation is $\\hat{Y}_i = 2 + 2 X_i$. The estimated parameters are $(\\hat{\\beta}_0, \\hat{\\beta}_1) = (2, 2)$.\n\nFinally, we provide a clinically meaningful interpretation of the slope estimator, $\\hat{\\beta}_1$. The slope of a regression line represents the estimated change in the response variable for a one-unit increase in the predictor variable. Here, the response $Y$ is the change in SBP (in millimeters of mercury, mmHg) and the predictor $X$ is the dose (in milligrams, mg).\n\nThe value $\\hat{\\beta}_1 = 2$ indicates that for every $1$ milligram increase in the dose of the novel agent, the change in Systolic Blood Pressure is estimated to increase by $2$ millimeters of mercury. A positive value for the SBP change indicates an increase in blood pressure. Therefore, this result suggests that, within the observed dose range, the drug has a hypertensive effect, which is contrary to the intended purpose of an antihypertensive agent. This is a critical finding in an early-phase trial, suggesting the agent may not be effective and could be harmful.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2 & 2 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Fitting a line to data is only half the story; we must also rigorously check if that line is an appropriate model. This practice introduces the critical skill of model diagnostics through the interpretation of a residual-versus-fitted plot . Learning to spot tell-tale patterns like curvature or 'funnels' in the residuals is essential for validating your model's assumptions and ensuring your conclusions are trustworthy.",
            "id": "4193057",
            "problem": "A laboratory quantifies a sensory neuron's firing rate response to a graded stimulus magnitude. For each trial index $i=1,\\dots,n$, the stimulus magnitude is $x_i$ and the measured firing rate (averaged over a fixed window to obtain a continuous rate in $\\mathrm{Hz}$) is $y_i$. Consider the simple linear regression model of firing rate on stimulus magnitude with ordinary least squares (OLS): the conditional mean obeys $E[Y\\mid X=x]=\\beta_0+\\beta_1 x$, and the error satisfies $Y=\\beta_0+\\beta_1 x+\\varepsilon$ with $E[\\varepsilon\\mid X]=0$ and $\\operatorname{Var}(\\varepsilon\\mid X)=\\sigma^2$. Let $\\hat{y}_i$ denote the fitted values and $e_i=y_i-\\hat{y}_i$ denote the residuals.\n\nIn this neural context, one wishes to use a residual-versus-fitted plot to diagnose possible nonlinearity in $E[Y\\mid X]$ or heteroscedasticity in $\\operatorname{Var}(Y\\mid X)$. From first principles, the diagnostic relies on the facts that under a correctly specified linear mean function and constant variance, $E[e_i\\mid X]=0$ and $\\operatorname{Var}(e_i\\mid X)=\\sigma^2$ do not depend on $x_i$, and that $\\hat{y}_i$ is a linear function of $x_i$. Which of the following options best outlines how to construct the residual-versus-fitted diagnostic and how to interpret its features to assess nonlinearity or heteroscedasticity in the fit of firing rate on stimulus magnitude?\n\nA. Plot $e_i$ on the vertical axis against $\\hat{y}_i$ on the horizontal axis, add a horizontal reference line at $e=0$, and overlay a nonparametric smoother such as locally estimated scatterplot smoothing (LOESS) of $e_i$ against $\\hat{y}_i$; interpret a systematic curvature in the smooth as evidence of nonlinearity in $E[Y\\mid X]$, and an increasing or decreasing vertical spread of $e_i$ with $\\hat{y}_i$ (a “funnel” shape) as evidence of heteroscedasticity in $\\operatorname{Var}(Y\\mid X)$.\n\nB. Plot $y_i$ on the vertical axis against $x_i$ on the horizontal axis together with the fitted regression line; interpret any curvature of the fitted line as heteroscedasticity and a larger deviation of points above the line at higher $x_i$ as nonlinearity.\n\nC. Plot $e_i$ on the vertical axis against the trial index $i$ on the horizontal axis; interpret any apparent trend in the residuals over $i$ as nonlinearity in $E[Y\\mid X]$ and any increase in spread of $e_i$ over $i$ as heteroscedasticity in $\\operatorname{Var}(Y\\mid X)$.\n\nD. Plot $\\sqrt{\\lvert e_i\\rvert}$ on the vertical axis against $\\hat{y}_i$ on the horizontal axis and overlay a LOESS curve; interpret curvature in the LOESS curve as nonlinearity in $E[Y\\mid X]$, and disregard changes in vertical spread as they are expected under a correct linear model.",
            "solution": "The fundamental goal of residual analysis is to check the assumptions of the regression model. The two key assumptions mentioned are the linearity of the conditional mean, $E[Y\\mid X=x]$, and the constancy of the error variance (homoscedasticity), $\\operatorname{Var}(Y\\mid X=x) = \\sigma^2$. The observable residuals, $e_i = y_i - \\hat{y}_i$, serve as estimates of the unobservable theoretical errors, $\\varepsilon_i$. If the model is correctly specified, a plot of the residuals should reveal no systematic patterns.\n\nA residual-versus-fitted plot, which graphs the residuals $e_i$ against the fitted values $\\hat{y}_i$, is a primary diagnostic tool for assessing these two assumptions.\n\n1.  **Construction**: The plot is constructed with the fitted values $\\hat{y}_i$ on the horizontal axis and the corresponding residuals $e_i$ on the vertical axis. A horizontal reference line at $e=0$ is crucial, as the residuals are expected to be centered around $0$. To aid in detecting subtle trends, a nonparametric smoother (e.g., LOESS) is often overlaid.\n\n2.  **Interpretation for Linearity**: The linearity assumption, $E[Y\\mid X] = \\beta_0 + \\beta_1 X$, implies that the residuals should have a mean of $0$ across the range of fitted values. If the LOESS curve shows a distinct, systematic curvature (e.g., a 'U' shape), it indicates the linear model fails to capture the true relationship, violating the linearity assumption.\n\n3.  **Interpretation for Homoscedasticity**: The homoscedasticity assumption, $\\operatorname{Var}(Y\\mid X) = \\sigma^2$ (a constant), implies that the vertical spread of the residuals should be roughly constant. If the plot shows a \"funnel\" or \"megaphone\" shape, where the spread increases or decreases as $\\hat{y}_i$ changes, it indicates heteroscedasticity (non-constant variance).\n\nBased on this, we evaluate the options:\n\n*   **A.** This option perfectly describes the standard construction and interpretation of a residual-versus-fitted plot. It correctly identifies the axes, the reference line, the use of a smoother to diagnose the mean structure (nonlinearity), and the use of the overall spread to diagnose the variance structure (heteroscedasticity). This is the correct procedure.\n\n*   **B.** This is incorrect. A simple linear regression line is by definition a straight line and cannot have curvature. This option confuses a plot of raw data with a diagnostic plot and misinterprets the concepts.\n\n*   **C.** A plot of residuals versus trial index or time is primarily used to check for autocorrelation (i.e., lack of independence in the errors), a common issue in time-series data. It is not the standard or most direct tool for diagnosing nonlinearity or heteroscedasticity with respect to the predictor.\n\n*   **D.** This describes a scale-location plot, which is used specifically to diagnose heteroscedasticity. A non-flat LOESS curve on this plot suggests non-constant variance. This option incorrectly states that curvature in this plot's smoother indicates nonlinearity in the mean. The instruction to disregard changes in vertical spread is also nonsensical.\n\nTherefore, Option A is the only one that correctly and comprehensively describes the construction and interpretation of a residual-versus-fitted plot for diagnosing nonlinearity and heteroscedasticity.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "When diagnostic plots reveal that our simple model is inadequate, the next step is not to abandon the analysis but to refine the model. This exercise demonstrates how to address systematic patterns in residuals by incorporating additional explanatory variables, such as a categorical covariate and its interaction with our primary predictor . This practice transitions from simple to multiple linear regression, a crucial step toward building models that more accurately reflect the complexity of biological data.",
            "id": "2429434",
            "problem": "You analyze a clinical cohort of $n$ patients receiving a new therapeutic compound. For each patient $i \\in \\{1,\\dots,n\\}$, let $Y_i$ denote recovery time in days, $X_i$ denote administered drug dosage in milligrams per kilogram, and $G_i$ denote a binary gender indicator with $G_i=1$ for male and $G_i=0$ for female. You fit a simple linear regression of $Y_i$ on $X_i$ under the usual assumptions that the errors are independent, have mean zero conditional on the predictors, and have constant variance. Upon inspecting the residuals versus $X_i$, you observe a systematic gender-based pattern: males and females show visibly different residual trends with $X_i$, and the residuals within each gender appear centered away from zero.\n\nWhich of the following is the most appropriate next analytical step to address this observed pattern within the linear modeling framework?\n\nA. Extend the model to include a gender main effect and a dosage-by-gender interaction, then formally test whether these terms are needed (for example, by an $F$-test comparing nested models), and re-examine the residuals.\n\nB. Apply a logarithmic transformation to $X_i$ and refit the simple linear regression without adding any covariates, because such a transformation will remove group-specific residual patterns.\n\nC. Delete enough observations from the gender group exhibiting larger residuals so that the residuals by gender appear balanced, and then refit the original simple linear regression.\n\nD. Replace the linear regression with an unsupervised clustering algorithm to partition patients by residual patterns, since clustering will account for the observed structure better than a regression model.",
            "solution": "The initial simple linear regression model is: $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$. This model assumes a common intercept ($\\beta_0$) and a common slope ($\\beta_1$) for all patients, regardless of gender.\n\nThe analysis of residuals reveals two distinct problems, both related to the gender covariate $G_i$:\n1.  **\"Residuals within each gender appear centered away from zero.\"** This indicates that the model systematically mis-predicts the average recovery time for each gender. For example, the model might consistently underpredict recovery time for males and overpredict for females. This implies that the intercept of the regression line should be different for males and females. This is addressed by adding a main effect for gender, $G_i$, to the model.\n2.  **\"Males and females show visibly different residual trends with $X_i$.\"** A \"trend\" in residuals versus a predictor means the effect of the predictor is misspecified. Since this trend differs by gender, it means the effect of drug dosage ($X_i$) on recovery time ($Y_i$) is not the same for males and females. The slope of the regression line should depend on gender. This is known as an interaction effect and is modeled by adding an interaction term, $X_i \\cdot G_i$.\n\nCombining these solutions leads to a multiple linear regression model that allows for both a different intercept and a different slope for each gender:\n$$Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 G_i + \\beta_3 (X_i \\cdot G_i) + \\epsilon''_i$$\nThis model is equivalent to fitting separate regression lines for each gender:\n*   For females ($G_i=0$): $E[Y_i | X_i, G_i=0] = \\beta_0 + \\beta_1 X_i$\n*   For males ($G_i=1$): $E[Y_i | X_i, G_i=1] = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) X_i$\n\nNow, we evaluate the provided options:\n\n*   **A.** This option proposes extending the model to include a gender main effect ($\\beta_2 G_i$) and a dosage-by-gender interaction ($\\beta_3 (X_i \\cdot G_i)$). This directly addresses the two issues observed in the residuals. It also includes the correct subsequent steps of statistical best practice: formally testing the significance of the new terms (e.g., via a partial $F$-test) and re-examining the residuals of the new model to confirm that the problem has been resolved. This is the correct and most comprehensive approach.\n\n*   **B.** A logarithmic transformation of $X_i$ is used to model a non-linear relationship between $Y_i$ and $X_i$. It does not address group-specific differences in intercepts or slopes. This approach would fail to correct the observed gender-based patterns.\n\n*   **C.** Deleting data to make the residuals look better is a form of data manipulation and is scientifically unethical. It invalidates the model and leads to biased results.\n\n*   **D.** Replacing regression with unsupervised clustering changes the analytical goal. Clustering identifies groups in data but does not model the relationship between predictors and an outcome variable. It is not a solution for improving the current predictive model.\n\nTherefore, option A describes the most appropriate and statistically sound next step.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}