{
    "hands_on_practices": [
        {
            "introduction": "A foundational tool for neural decoding is the ordinary least squares (OLS) linear model. However, when the activities of different neurons are correlated—a common occurrence in neural populations—the OLS estimator can become highly unstable. This exercise guides you through a first-principles derivation to reveal precisely how this collinearity inflates the variance of the decoder's predictions, connecting a core concept from linear algebra (the eigenvalues of the data covariance matrix) to a critical challenge in practical decoding . Understanding this connection is the first step toward building robust decoders.",
            "id": "4189997",
            "problem": "A neural decoding laboratory seeks to reconstruct a one-dimensional kinematic variable from a population of $p$ simultaneously recorded neurons using linear regression-based decoding. In each trial $t \\in \\{1,\\dots,n\\}$, the observed kinematic scalar is modeled by the linear model $y_{t} = x_{t}^{\\top}\\beta^{\\star} + \\varepsilon_{t}$, where $x_{t} \\in \\mathbb{R}^{p}$ is the vector of preprocessed neural covariates, $\\beta^{\\star} \\in \\mathbb{R}^{p}$ is the true decoding weight vector, and $\\varepsilon_{t}$ are independent noise terms satisfying $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$. The design matrix is $X \\in \\mathbb{R}^{n \\times p}$ with rows $x_{t}^{\\top}$, and the ordinary least squares (OLS) estimator of the weights is used to construct the decoder. Consider the predictive output of this decoder at a new unit-norm covariate $x_{\\mathrm{new}} \\in \\mathbb{R}^{p}$ with $\\|x_{\\mathrm{new}}\\|_2=1$.\n\nStarting from fundamental properties of the linear model, and using spectral decomposition of $X^{\\top}X$, derive, from first principles, the worst-case predictive variance of the linear decoder output $x_{\\mathrm{new}}^{\\top}\\hat{\\beta}$ over all unit-norm $x_{\\mathrm{new}}$. Then evaluate this worst-case predictive variance when the eigenvalues of $X^{\\top}X$ are given by $(\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4})=(500,2,1,0.05)$ and the noise variance is $\\sigma^{2}=0.2$. Round your final numerical answer to four significant figures. Provide the final answer as a single real number without units.",
            "solution": "The problem statement has been validated and is deemed sound, self-contained, and scientifically grounded. We will proceed with a full derivation.\n\nThe problem asks for the worst-case predictive variance of a linear decoder. Let us begin by establishing the statistical properties of the ordinary least squares (OLS) estimator $\\hat{\\beta}$. The linear model is given in vector form as $y = X\\beta^{\\star} + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the vector of observations, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta^{\\star} \\in \\mathbb{R}^{p}$ is the true weight vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ is the vector of noise terms. The noise terms are independent and identically distributed as $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$, which implies that the noise vector $\\varepsilon$ has a multivariate normal distribution $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix.\n\nThe OLS estimator $\\hat{\\beta}$ is given by the formula:\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y $$\nSubstituting the model for $y$, we can express $\\hat{\\beta}$ in terms of the true parameter $\\beta^{\\star}$ and the noise $\\varepsilon$:\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}(X\\beta^{\\star} + \\varepsilon) = (X^{\\top}X)^{-1}X^{\\top}X\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon = \\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon $$\nThe design matrix $X$ is considered fixed (non-random). The only random component in this expression for $\\hat{\\beta}$ is the noise vector $\\varepsilon$.\n\nFirst, we determine the expected value of $\\hat{\\beta}$:\n$$ \\mathrm{E}[\\hat{\\beta}] = \\mathrm{E}[\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon] = \\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\mathrm{E}[\\varepsilon] $$\nSince $\\mathrm{E}[\\varepsilon] = 0$, we have $\\mathrm{E}[\\hat{\\beta}] = \\beta^{\\star}$. This confirms that the OLS estimator is unbiased.\n\nNext, we derive the covariance matrix of $\\hat{\\beta}$. The covariance matrix is defined as $\\mathrm{Cov}(\\hat{\\beta}) = \\mathrm{E}[(\\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}])(\\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}])^{\\top}]$.\n$$ \\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}] = (X^{\\top}X)^{-1}X^{\\top}\\varepsilon $$\n$$ \\mathrm{Cov}(\\hat{\\beta}) = \\mathrm{E}[((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)^{\\top}] = \\mathrm{E}[(X^{\\top}X)^{-1}X^{\\top}\\varepsilon\\varepsilon^{\\top}X((X^{\\top}X)^{-1})^{\\top}] $$\nSince $X$ is non-random, we can move it outside the expectation:\n$$ \\mathrm{Cov}(\\hat{\\beta}) = (X^{\\top}X)^{-1}X^{\\top}\\mathrm{E}[\\varepsilon\\varepsilon^{\\top}]X((X^{\\top}X)^{-1})^{\\top} $$\nThe term $\\mathrm{E}[\\varepsilon\\varepsilon^{\\top}]$ is the covariance matrix of the noise, which is $\\mathrm{Cov}(\\varepsilon) = \\sigma^2 I_n$. The matrix $X^{\\top}X$ is symmetric, so its inverse is also symmetric.\n$$ \\mathrm{Cov}(\\hat{\\beta}) = (X^{\\top}X)^{-1}X^{\\top}(\\sigma^2 I_n)X(X^{\\top}X)^{-1} = \\sigma^2 (X^{\\top}X)^{-1}X^{\\top}X(X^{\\top}X)^{-1} = \\sigma^2 (X^{\\top}X)^{-1} $$\nThis is a fundamental result for the OLS estimator.\n\nThe problem requires us to analyze the predictive output of the decoder for a new covariate vector $x_{\\mathrm{new}} \\in \\mathbb{R}^p$. The predicted value (decoder output) is $\\hat{y}_{\\mathrm{new}} = x_{\\mathrm{new}}^{\\top}\\hat{\\beta}$. We need to find the variance of this prediction. Using the general property for the variance of a linear transformation of a random vector, $\\mathrm{Var}(A z) = A \\mathrm{Cov}(z) A^{\\top}$, with $A = x_{\\mathrm{new}}^{\\top}$ and $z = \\hat{\\beta}$:\n$$ \\mathrm{Var}(x_{\\mathrm{new}}^{\\top}\\hat{\\beta}) = x_{\\mathrm{new}}^{\\top} \\mathrm{Cov}(\\hat{\\beta}) x_{\\mathrm{new}} $$\nSubstituting the expression for $\\mathrm{Cov}(\\hat{\\beta})$:\n$$ \\mathrm{Var}(x_{\\mathrm{new}}^{\\top}\\hat{\\beta}) = x_{\\mathrm{new}}^{\\top} (\\sigma^2 (X^{\\top}X)^{-1}) x_{\\mathrm{new}} = \\sigma^2 x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} $$\nThe task is to find the worst-case (maximum) predictive variance over all possible unit-norm covariate vectors $x_{\\mathrm{new}}$. The condition is given as $\\|x_{\\mathrm{new}}\\|_2 = 1$. The problem is thus to solve the following optimization:\n$$ \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( \\sigma^2 x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) = \\sigma^2 \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) $$\nThe expression $x^{\\top} M x$ for a symmetric matrix $M$ and a unit vector $x$ is known as a Rayleigh quotient. The maximum value of the Rayleigh quotient is the largest eigenvalue of the matrix $M$. In our case, $M = (X^{\\top}X)^{-1}$.\nSo, we have:\n$$ \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) = \\lambda_{\\max}((X^{\\top}X)^{-1}) $$\nLet the eigenvalues of the matrix $X^{\\top}X$ be denoted by $\\lambda_i$. Since $X^{\\top}X$ is a symmetric positive definite matrix, its eigenvalues are real and positive. The eigenvalues of the inverse matrix, $(X^{\\top}X)^{-1}$, are the reciprocals of the eigenvalues of $X^{\\top}X$, i.e., $1/\\lambda_i$.\nTherefore, the largest eigenvalue of $(X^{\\top}X)^{-1}$ is the reciprocal of the smallest eigenvalue of $X^{\\top}X$:\n$$ \\lambda_{\\max}((X^{\\top}X)^{-1}) = \\max_{i} \\left( \\frac{1}{\\lambda_i} \\right) = \\frac{1}{\\min_{i}(\\lambda_i)} = \\frac{1}{\\lambda_{\\min}(X^{\\top}X)} $$\nSubstituting this back into the expression for the worst-case variance:\n$$ \\text{Worst-case variance} = \\sigma^2 \\cdot \\frac{1}{\\lambda_{\\min}(X^{\\top}X)} $$\nThe worst-case direction for $x_{\\mathrm{new}}$ corresponds to the eigenvector of $X^{\\top}X$ associated with its smallest eigenvalue, $\\lambda_{\\min}$.\n\nNow we can evaluate this expression using the provided numerical values.\nThe eigenvalues of $X^{\\top}X$ are given as $(\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4) = (500, 2, 1, 0.05)$.\nThe smallest of these eigenvalues is:\n$$ \\lambda_{\\min}(X^{\\top}X) = \\min\\{500, 2, 1, 0.05\\} = 0.05 $$\nThe noise variance is given as $\\sigma^2 = 0.2$.\n\nPlugging these values into our derived formula for the worst-case predictive variance:\n$$ \\text{Worst-case variance} = \\frac{\\sigma^2}{\\lambda_{\\min}(X^{\\top}X)} = \\frac{0.2}{0.05} = 4 $$\nThe problem requires the answer to be rounded to four significant figures. The exact answer is $4$. Expressed with four significant figures, this is $4.000$.",
            "answer": "$$\n\\boxed{4.000}\n$$"
        },
        {
            "introduction": "Having established how collinearity can destabilize OLS decoders, we now turn to a powerful solution: regularization. Ridge regression combats this instability by adding a penalty term to the objective function, which discourages overly complex models with large weights. This practice provides a rigorous mathematical workout, guiding you to derive the ridge regression solution and its expected out-of-sample error. By uncovering the famous bias-variance decomposition, you will gain a deep, quantitative understanding of how regularization trades a small amount of model bias for a large reduction in estimator variance, ultimately improving predictive accuracy .",
            "id": "5002219",
            "problem": "A laboratory is developing a Brain-Computer Interface (BCI) decoder that maps neuronal firing rates to the hand’s scalar tangential velocity during a center-out reaching task. In each time bin indexed by $t \\in \\{1,\\ldots,N\\}$ of width $\\Delta t$, the preprocessed firing rate vector is $\\mathbf{r}_{t} \\in \\mathbb{R}^{p}$ (already z-scored and whitened across neurons so that the sample covariance is the identity), and the simultaneously measured hand velocity is $v_{t} \\in \\mathbb{R}$. Assume a linear-Gaussian encoding model $v_{t} = \\mathbf{r}_{t}^{\\top} \\boldsymbol{\\beta} + \\varepsilon_{t}$ where $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$ are independent across $t$ and independent of $\\mathbf{r}_{t}$. Stack the data into the design matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$ with rows $\\mathbf{r}_{t}^{\\top}$ and target vector $\\mathbf{y} \\in \\mathbb{R}^{N}$ with entries $v_{t}$. Because the neuronal features were whitened using the training data, you may assume the empirical second moment satisfies $\\mathbf{X}^{\\top}\\mathbf{X} = N \\mathbf{I}_{p}$.\n\nYou train a ridge regression decoder by minimizing the penalized least-squares objective\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2},\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter and $\\|\\cdot\\|$ denotes the Euclidean norm.\n\nTasks:\n1) Starting from the model $ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{N})$, derive the closed-form solution $\\widehat{\\mathbf{w}}$ that minimizes $J(\\mathbf{w};\\lambda)$.\n\n2) For a new, independent test sample $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$ drawn from the same distribution, with $\\mathbf{r}_{\\mathrm{new}}$ independent of training data and satisfying $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}] = \\mathbf{0}$ and $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$, derive the expected out-of-sample mean squared prediction error\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]\n$$\nas an explicit function of $\\lambda$, $N$, $p$, $\\sigma^{2}$, and $\\boldsymbol{\\beta}$. Your derivation must start from the definitions above and the assumptions on $\\mathbf{X}$ and $\\mathbf{r}_{\\mathrm{new}}$, and it must expose the bias-variance decomposition that depends on $\\lambda$.\n\n3) To make the trade-off explicit and independent of a particular unknown $\\boldsymbol{\\beta}$, assume a hierarchical prior consistent with neural population codes: $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$ with $\\tau^{2} > 0$. Average your expression for $\\mathcal{E}(\\lambda)$ over this prior and simplify to a scalar function of $\\lambda$, $N$, $p$, $\\sigma^{2}$, and $\\tau^{2}$.\n\n4) Using your averaged expression, determine the value $\\lambda^{\\star}$ that minimizes the expected out-of-sample mean squared prediction error. Then, evaluate this optimum numerically for\n- $p = 100$,\n- $N = 10000$,\n- $\\sigma^{2} = 0.04$,\n- $\\tau^{2} = 0.01$.\nExpress the final value of $\\lambda^{\\star}$ as a pure number without units. If rounding is necessary, round to four significant figures. If not, provide the exact value.",
            "solution": "The problem asks for a multi-step analysis of a ridge regression decoder in the context of a Brain-Computer Interface (BCI). The analysis involves deriving the decoder, its out-of-sample error, and the optimal regularization parameter under a specific data model and prior.\n\n### Task 1: Derivation of the Ridge Regression Estimator $\\widehat{\\mathbf{w}}$\nThe ridge regression estimator $\\widehat{\\mathbf{w}}$ is found by minimizing the objective function\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2}\n$$\nwhere $\\|\\cdot\\|$ is the Euclidean norm. We can write the squared norms in terms of vector transposes:\n$$\nJ(\\mathbf{w};\\lambda) = (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^{\\top}(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\nExpanding the first term gives:\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - \\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w} - \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{w} + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\nSince $\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y}$ is a scalar, it equals its transpose $\\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w}$. Thus, we can combine the cross-terms:\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\nTo find the minimum, we take the gradient of $J(\\mathbf{w};\\lambda)$ with respect to $\\mathbf{w}$ and set it to zero. Using standard matrix calculus results ($\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{a} = \\mathbf{a}$ and $\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{M}\\mathbf{w} = 2\\mathbf{M}\\mathbf{w}$ for symmetric $\\mathbf{M}$):\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w};\\lambda) = -2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\nSetting the gradient to the zero vector gives the solution $\\widehat{\\mathbf{w}}$:\n$$\n-2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{0}\n$$\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{X}^{\\top}\\mathbf{y}\n$$\nThe formal solution is $\\widehat{\\mathbf{w}} = (\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$. The problem states the assumption that the empirical second moment is $\\mathbf{X}^{\\top}\\mathbf{X} = N\\mathbf{I}_{p}$. Substituting this into the expression for $\\widehat{\\mathbf{w}}$:\n$$\n\\widehat{\\mathbf{w}} = (N\\mathbf{I}_{p} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = ((N+\\lambda)\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{I}_{p}^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\nThis is the closed-form solution for $\\widehat{\\mathbf{w}}$ under the given assumption.\n\n### Task 2: Expected Out-of-Sample Mean Squared Prediction Error\nWe are asked to derive $\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]$. The expectation is taken over the randomness in the training data (which makes $\\widehat{\\mathbf{w}}$ random) and the new test sample $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$.\nThe true model for the new sample is $v_{\\mathrm{new}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}$, where $\\varepsilon_{\\mathrm{new}} \\sim \\mathcal{N}(0,\\sigma^{2})$. Substituting this into the error term:\n$$\nv_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}) - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}\n$$\nSquaring this expression:\n$$\n(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2} + 2\\varepsilon_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}^{2}\n$$\nNow we take the expectation. The new noise term $\\varepsilon_{\\mathrm{new}}$ is independent of the training data (and thus $\\widehat{\\mathbf{w}}$) and the new features $\\mathbf{r}_{\\mathrm{new}}$. Since $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}] = 0$, the cross-term vanishes. We have $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}^{2}] = \\sigma^{2}$.\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] + \\sigma^{2}\n$$\nThe remaining expectation is over $\\mathbf{r}_{\\mathrm{new}}$ and $\\widehat{\\mathbf{w}}$. We can rewrite the term inside the expectation using the trace trick: $(\\mathbf{a}^{\\top}\\mathbf{b})^2 = \\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b} = \\mathrm{tr}(\\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}) = \\mathrm{tr}(\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}\\mathbf{b}^{\\top})$. It is simpler to use $\\mathbb{E}[x^2] = \\mathbb{E}[\\mathrm{tr}(x^2)]$ where $x$ is a scalar.\n$$\n\\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] = \\mathbb{E}\\left[\\mathrm{tr}\\left((\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top} (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right)\\right]\n$$\nBy linearity of trace and expectation, and since $\\widehat{\\mathbf{w}}$ (from training data) is independent of $\\mathbf{r}_{\\mathrm{new}}$:\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right)\n$$\nUsing the assumption $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$:\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top}\\mathbf{I}_{p}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right) = \\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right]\n$$\nSo, $\\mathcal{E}(\\lambda) = \\mathbb{E}[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}] + \\sigma^2$. The expectation $\\mathbb{E}[\\cdot]$ is now only over the training data randomness. We now perform a bias-variance decomposition:\n$$\n\\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right] = \\left\\|\\boldsymbol{\\beta} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\right\\|^{2} + \\mathbb{E}\\left[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^{2}\\right] = \\text{Bias}(\\widehat{\\mathbf{w}})^{2} + \\text{Var}(\\widehat{\\mathbf{w}})\n$$\nWe derive the bias and variance terms. First, substitute $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ into the expression for $\\widehat{\\mathbf{w}}$:\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\n$$\nThe expectation of $\\widehat{\\mathbf{w}}$ (over $\\boldsymbol{\\varepsilon}$) is:\n$$\n\\mathbb{E}[\\widehat{\\mathbf{w}}] = \\mathbb{E}\\left[\\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\\right] = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}]) = \\frac{N}{N+\\lambda}\\boldsymbol{\\beta}\n$$\nThe squared bias is:\n$$\n\\text{Bias}(\\widehat{\\mathbf{w}})^{2} = \\left\\|\\mathbb{E}[\\widehat{\\mathbf{w}}] - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|\\frac{N}{N+\\lambda}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|-\\frac{\\lambda}{N+\\lambda}\\boldsymbol{\\beta}\\right\\|^{2} = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2}\n$$\nThe variance term is $\\mathbb{E}[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^2] = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}}))$.\n$$\n\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}] = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\n$$\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\mathbb{E}\\left[(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])^{\\top}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbb{E}\\left[\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]\\mathbf{X}\n$$\nUsing $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]=\\sigma^2\\mathbf{I}_N$ and $\\mathbf{X}^{\\top}\\mathbf{X}=N\\mathbf{I}_p$:\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{I}_{N}\\mathbf{X} = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{X} = \\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\n$$\nThe variance is the trace of this covariance matrix:\n$$\n\\text{Var}(\\widehat{\\mathbf{w}}) = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}})) = \\mathrm{tr}\\left(\\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\\right) = \\frac{pN\\sigma^2}{(N+\\lambda)^2}\n$$\nCombining all terms, the expected out-of-sample error is:\n$$\n\\mathcal{E}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### Task 3: Averaging Over the Prior for $\\boldsymbol{\\beta}$\nWe are given a prior distribution over the true weights, $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$. We need to compute $\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\mathbb{E}_{\\boldsymbol{\\beta}}[\\mathcal{E}(\\lambda)]$. The only term in $\\mathcal{E}(\\lambda)$ that depends on $\\boldsymbol{\\beta}$ is $\\|\\boldsymbol{\\beta}\\|^{2}$. We compute its expectation under the prior:\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\sum_{i=1}^{p}\\beta_{i}^{2}\\right] = \\sum_{i=1}^{p}\\mathbb{E}_{\\boldsymbol{\\beta}}[\\beta_{i}^{2}]\n$$\nFor each component $\\beta_i \\sim \\mathcal{N}(0, \\tau^{2})$, the second moment is $\\mathbb{E}[\\beta_{i}^{2}] = \\mathrm{Var}(\\beta_i) + (\\mathbb{E}[\\beta_i])^{2} = \\tau^{2} + 0^{2} = \\tau^{2}$.\nTherefore, the expected squared norm is:\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\sum_{i=1}^{p}\\tau^{2} = p\\tau^{2}\n$$\nSubstituting this into the expression for $\\mathcal{E}(\\lambda)$:\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}p\\tau^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{p\\lambda^{2}\\tau^{2} + pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### Task 4: Optimal Regularization Parameter $\\lambda^{\\star}$\nTo find the $\\lambda$ that minimizes $\\mathcal{E}_{\\mathrm{avg}}(\\lambda)$, we differentiate with respect to $\\lambda$ and set the derivative to zero. The constant term $\\sigma^{2}$ can be ignored during minimization.\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{d}{d\\lambda}\\left( \\frac{p(\\lambda^{2}\\tau^{2} + N\\sigma^2)}{(N+\\lambda)^2} \\right)\n$$\nUsing the quotient rule $\\frac{d}{dx}(\\frac{u}{v}) = \\frac{u'v - uv'}{v^2}$:\nLet $u(\\lambda) = p(\\lambda^{2}\\tau^{2} + N\\sigma^2)$ and $v(\\lambda) = (N+\\lambda)^2$.\nThen $u'(\\lambda) = 2p\\lambda\\tau^{2}$ and $v'(\\lambda) = 2(N+\\lambda)$.\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{(2p\\lambda\\tau^{2})(N+\\lambda)^2 - p(\\lambda^{2}\\tau^{2} + N\\sigma^2) \\cdot 2(N+\\lambda)}{(N+\\lambda)^4}\n$$\nSetting the derivative to zero and assuming $\\lambda \\ge 0, N \\ge 1$, we can simplify by dividing by the non-zero factor $2p(N+\\lambda)$:\n$$\n(\\lambda\\tau^{2})(N+\\lambda) - (\\lambda^{2}\\tau^{2} + N\\sigma^2) = 0\n$$\n$$\nN\\lambda\\tau^{2} + \\lambda^{2}\\tau^{2} - \\lambda^{2}\\tau^{2} - N\\sigma^2 = 0\n$$\n$$\nN\\lambda\\tau^{2} = N\\sigma^2\n$$\nSince $N \\ge 1$ and $\\tau^{2} > 0$ (given), we can divide by $N\\tau^{2}$ to find the optimal $\\lambda^{\\star}$:\n$$\n\\lambda^{\\star} = \\frac{\\sigma^2}{\\tau^2}\n$$\nThis result is elegantly simple and represents the ratio of the noise variance in the measurements to the prior variance of the model parameters. The second derivative can be checked to confirm this is a minimum. The numerator of the derivative simplified to $2pN(\\lambda\\tau^2 - \\sigma^2)(N+\\lambda)$, which is negative for $\\lambda < \\sigma^2/\\tau^2$ and positive for $\\lambda > \\sigma^2/\\tau^2$, confirming a minimum.\n\nFinally, we evaluate this expression numerically with the provided values:\n- $\\sigma^{2} = 0.04$\n- $\\tau^{2} = 0.01$\nThe values for $p$ and $N$ are not needed to find $\\lambda^{\\star}$ in this idealized setting.\n$$\n\\lambda^{\\star} = \\frac{0.04}{0.01} = 4\n$$\nThe optimal value is exactly $4$.",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "We now bridge the gap between the mathematical theory of regularization and its practical application. Real-world neural recordings often contain redundant or highly correlated features, leading to rank-deficient data matrices where standard models fail. This hands-on exercise challenges you to derive and implement two distinct but related solutions for this problem: the minimum-norm solution via the Moore-Penrose pseudoinverse and the familiar ridge regression solution . By comparing their behavior on carefully constructed test cases, you will develop a practical intuition for how to handle the ubiquitous challenge of multicollinearity in neural data.",
            "id": "4190072",
            "problem": "You are given a linear decoding setting for neuronal population activity where the design matrix $X \\in \\mathbb{R}^{n \\times p}$ has rank deficiency due to redundant neurons. The decoding target is a continuous scalar $y \\in \\mathbb{R}^{n}$. The model is $y \\approx X w$, where $w \\in \\mathbb{R}^{p}$ are regression weights mapping neuronal activity to the target. Redundancy in neuronal features can make $X$ non-invertible in the sense that $X^{\\top} X$ is singular. Starting from the fundamental base of least-squares decoding under the linear model $y \\approx X w$ and the optimality conditions for minimizing squared error, derive a pseudoinverse-based minimal-norm solution for $w$ and a ridge-regularized solution for $w$. The derivation must justify each step from first principles: the least-squares optimality condition, how rank deficiency obstructs direct inversion, why the Singular Value Decomposition (SVD) provides a constructive path to the Moore–Penrose pseudoinverse, and how adding an $\\ell_{2}$ penalty resolves ill-posedness in ridge regression.\n\nAfter deriving both solutions, implement them and compare their predictions on the following test suite. For the pseudoinverse, implement an SVD-based pseudoinverse with a numerically meaningful singular-value threshold that treats singular values below that threshold as zero. For ridge regression, implement the regularized solution parameterized by $\\lambda > 0$.\n\nDefine the following test suite matrices and targets:\n\n- Test case $1$ (redundant neurons with duplicated and linearly dependent columns):\n$$\nX_{1} = \\begin{bmatrix}\n1 & 0 & 0 & 2 \\\\\n2 & 1 & 1 & 4 \\\\\n3 & 0 & 0 & 6 \\\\\n4 & 1 & 1 & 8 \\\\\n5 & 0 & 0 & 10\n\\end{bmatrix}, \\quad\ny_{1} = \\begin{bmatrix}\n0.6 \\\\\n-0.2 \\\\\n1.55 \\\\\n1.0 \\\\\n2.4\n\\end{bmatrix}, \\quad\n\\lambda_{1} = 0.1.\n$$\n\n- Test case $2$ (extreme redundancy: all columns identical):\n$$\nX_{2} = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{bmatrix}, \\quad\ny_{2} = \\begin{bmatrix}\n2.0 \\\\\n2.1 \\\\\n1.9 \\\\\n2.0\n\\end{bmatrix}, \\quad\n\\lambda_{2} = 10.0.\n$$\n\n- Test case $3$ (near collinearity: one column nearly a linear combination of another with small perturbations):\n$$\nX_{3} = \\begin{bmatrix}\n0 & 1 & 0.0 \\\\\n1 & 0 & 1.01 \\\\\n2 & 1 & 1.99 \\\\\n3 & 0 & 3.02 \\\\\n4 & 1 & 3.98 \\\\\n5 & 0 & 5.0\n\\end{bmatrix}, \\quad\ny_{3} = \\begin{bmatrix}\n0.7 \\\\\n0.32 \\\\\n1.28 \\\\\n0.91 \\\\\n1.89 \\\\\n1.5\n\\end{bmatrix}, \\quad\n\\lambda_{3} = 10^{-6}.\n$$\n\nFor each test case, compute:\n- The pseudoinverse-based decoder $w_{\\mathrm{pinv}}$ and its predictions $\\hat{y}_{\\mathrm{pinv}} = X w_{\\mathrm{pinv}}$.\n- The ridge-regularized decoder $w_{\\mathrm{ridge}}$ and its predictions $\\hat{y}_{\\mathrm{ridge}} = X w_{\\mathrm{ridge}}$.\n\nThen report the following two float quantities for each case:\n- The mean squared difference between predictions,\n$$\nd = \\frac{1}{n} \\left\\| \\hat{y}_{\\mathrm{pinv}} - \\hat{y}_{\\mathrm{ridge}} \\right\\|_{2}^{2}.\n$$\n- The difference in mean squared decoding error on the training data,\n$$\n\\Delta = \\frac{1}{n} \\left\\| y - \\hat{y}_{\\mathrm{pinv}} \\right\\|_{2}^{2} - \\frac{1}{n} \\left\\| y - \\hat{y}_{\\mathrm{ridge}} \\right\\|_{2}^{2}.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a two-element list $[d,\\Delta]$. For example, the format must be like $[[d_{1},\\Delta_{1}],[d_{2},\\Delta_{2}],[d_{3},\\Delta_{3}]]$. All values are dimensionless real numbers. Ensure numerical stability by using a singular-value threshold of the form $\\tau = \\epsilon \\cdot \\max(n,p) \\cdot \\sigma_{\\max}$, where $\\epsilon$ is machine precision and $\\sigma_{\\max}$ is the largest singular value of $X$.",
            "solution": "The task is to derive and implement two solutions for a linear regression problem, $y \\approx Xw$, where the design matrix $X$ may be rank-deficient. The two methods are the minimal-norm least-squares solution via the pseudoinverse and the ridge regression solution.\n\n### Part 1: Least-Squares Formulation and Optimality\n\nThe standard approach to fitting the linear model $y \\approx Xw$ is to minimize the sum of squared differences between the observed target values $y$ and the predictions $Xw$. This defines the least-squares objective function, $L(w)$:\n$$\nL(w) = \\| y - Xw \\|_2^2\n$$\nwhere $y \\in \\mathbb{R}^n$ is the vector of target values, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix of neuronal features, and $w \\in \\mathbb{R}^p$ is the vector of decoding weights. The norm $\\| \\cdot \\|_2$ denotes the Euclidean norm.\n\nTo find the optimal weights $w$ that minimize $L(w)$, we compute the gradient of $L(w)$ with respect to $w$ and set it to zero. First, we expand the objective function:\n$$\nL(w) = (y - Xw)^\\top (y - Xw) = y^\\top y - y^\\top Xw - (Xw)^\\top y + (Xw)^\\top(Xw) = y^\\top y - 2y^\\top Xw + w^\\top X^\\top Xw\n$$\nThe gradient with respect to the vector $w$ is:\n$$\n\\nabla_w L(w) = \\frac{\\partial}{\\partial w} (y^\\top y - 2w^\\top X^\\top y + w^\\top X^\\top Xw) = -2X^\\top y + 2X^\\top Xw\n$$\nSetting the gradient to zero, $\\nabla_w L(w) = 0$, gives the optimality condition:\n$$\n-2X^\\top y + 2X^\\top Xw = 0 \\implies X^\\top Xw = X^\\top y\n$$\nThis set of linear equations is known as the **normal equations**.\n\n### Part 2: The Problem of Rank Deficiency\n\nIf the matrix $X$ has full column rank (i.e., $\\text{rank}(X) = p$, meaning all its columns are linearly independent), then the matrix $X^\\top X$ is a $p \\times p$ positive definite matrix and is therefore invertible. In this case, there is a unique solution for $w$:\n$$\nw = (X^\\top X)^{-1} X^\\top y\n$$\nHowever, the problem states that we have redundant neurons, which implies that columns of $X$ are linearly dependent. This means $X$ is rank-deficient, i.e., $\\text{rank}(X) < p$. Consequently, the matrix $X^\\top X$ is singular (non-invertible). The normal equations $X^\\top Xw = X^\\top y$ no longer have a unique solution; instead, there is an entire subspace of solutions that all minimize the squared error $\\|y - Xw\\|_2^2$ equally. This ill-posed nature requires a principled way to choose a single solution or to modify the problem itself.\n\n### Part 3: Solution 1 - The Minimal-Norm Pseudoinverse Solution\n\nWhen infinite solutions exist, a common and principled approach is to select the solution $w$ that has the minimum Euclidean norm $\\|w\\|_2$. This prevents the weights from taking arbitrarily large values, which is often a desirable property. This minimum-norm least-squares solution can be constructed using the Singular Value Decomposition (SVD).\n\nThe SVD of the matrix $X$ is given by:\n$$\nX = U \\Sigma V^\\top\n$$\nwhere:\n- $U \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix ($UU^\\top = U^\\top U = I_n$) whose columns are the left singular vectors.\n- $V \\in \\mathbb{R}^{p \\times p}$ is an orthogonal matrix ($VV^\\top = V^\\top V = I_p$) whose columns are the right singular vectors.\n- $\\Sigma \\in \\mathbb{R}^{n \\times p}$ is a diagonal matrix containing the singular values $\\sigma_i \\ge 0$. The number of non-zero singular values, $r$, is the rank of $X$.\n\nThe Moore-Penrose pseudoinverse of $X$, denoted $X^+$, is defined using the SVD as:\n$$\nX^+ = V \\Sigma^+ U^\\top\n$$\nwhere $\\Sigma^+ \\in \\mathbb{R}^{p \\times n}$ is constructed by taking the reciprocal of the non-zero singular values on the diagonal of $\\Sigma^\\top$ and keeping the zeros. Specifically, if $(\\Sigma)_{ii} = \\sigma_i > 0$, then $(\\Sigma^+)_{ii} = 1/\\sigma_i$. Due to numerical precision, singular values are typically thresholded: values below a small threshold $\\tau > 0$ are treated as zero.\n\nThe minimal-norm least-squares solution for $w$ is given by:\n$$\nw_{\\mathrm{pinv}} = X^+ y\n$$\nThis solution $w_{\\mathrm{pinv}}$ satisfies the normal equations (i.e., it is a true least-squares solution) and, among all vectors $w$ that satisfy them, $w_{\\mathrm{pinv}}$ has the smallest norm $\\|w\\|_2$.\n\n### Part 4: Solution 2 - Ridge Regression\n\nAn alternative approach to handle the singularity of $X^\\top X$ is to regularize the objective function. Ridge regression adds an $\\ell_2$ penalty on the magnitude of the weights, which biases the solution towards smaller weights. The modified objective function is:\n$$\nL_{\\mathrm{ridge}}(w) = \\| y - Xw \\|_2^2 + \\lambda \\|w\\|_2^2\n$$\nwhere $\\lambda > 0$ is a regularization parameter that controls the trade-off between fitting the data and keeping the weights small.\n\nTo find the optimal $w_{\\mathrm{ridge}}$, we again take the gradient and set it to zero:\n$$\nL_{\\mathrm{ridge}}(w) = (y - Xw)^\\top(y - Xw) + \\lambda w^\\top w = y^\\top y - 2w^\\top X^\\top y + w^\\top X^\\top Xw + \\lambda w^\\top w\n$$\nThe gradient is:\n$$\n\\nabla_w L_{\\mathrm{ridge}}(w) = -2X^\\top y + 2X^\\top Xw + 2\\lambda w\n$$\nSetting the gradient to zero yields the regularized normal equations:\n$$\n-2X^\\top y + 2(X^\\top X + \\lambda I)w = 0 \\implies (X^\\top X + \\lambda I)w = X^\\top y\n$$\nThe key insight here is that the matrix $(X^\\top X + \\lambda I)$ is invertible for any $\\lambda > 0$. The matrix $X^\\top X$ is positive semi-definite; its eigenvalues are all non-negative ($\\mu_i \\ge 0$). Adding $\\lambda I$ shifts all eigenvalues by $\\lambda$, so the eigenvalues of $(X^\\top X + \\lambda I)$ are $\\mu_i + \\lambda$. Since $\\lambda > 0$, all these eigenvalues are strictly positive. A symmetric matrix with strictly positive eigenvalues is positive definite and thus invertible.\n\nThis resolves the singularity issue, leading to a unique solution for $w$:\n$$\nw_{\\mathrm{ridge}} = (X^\\top X + \\lambda I)^{-1} X^\\top y\n$$\nThis solution is not a least-squares solution in the original sense (it does not minimize $\\|y - Xw\\|_2^2$), but it minimizes the combined objective of fit and penalty.\n\n### Summary of Predictions and Comparison Metrics\n\nThe predictions from each model are:\n- $\\hat{y}_{\\mathrm{pinv}} = X w_{\\mathrm{pinv}} = X X^+ y$\n- $\\hat{y}_{\\mathrm{ridge}} = X w_{\\mathrm{ridge}} = X (X^\\top X + \\lambda I)^{-1} X^\\top y$\n\nThe comparison metrics are:\n- The mean squared difference between predictions: $d = \\frac{1}{n} \\left\\| \\hat{y}_{\\mathrm{pinv}} - \\hat{y}_{\\mathrm{ridge}} \\right\\|_{2}^{2}$\n- The difference in mean squared decoding error: $\\Delta = \\text{MSE}_{\\mathrm{pinv}} - \\text{MSE}_{\\mathrm{ridge}} = \\frac{1}{n} \\left\\| y - \\hat{y}_{\\mathrm{pinv}} \\right\\|_{2}^{2} - \\frac{1}{n} \\left\\| y - \\hat{y}_{\\mathrm{ridge}} \\right\\|_{2}^{2}$\n\nSince $w_{\\mathrm{pinv}}$ is a minimizer of the unregularized least-squares cost function, we expect $\\text{MSE}_{\\mathrm{pinv}} \\le \\text{MSE}_{\\mathrm{ridge}}$, which implies that $\\Delta \\le 0$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and compares pseudoinverse and ridge regression decoders for\n    neuroscience data with rank-deficient feature matrices.\n    \"\"\"\n\n    # Define the test suite matrices and targets as specified in the problem.\n    X1 = np.array([\n        [1, 0, 0, 2],\n        [2, 1, 1, 4],\n        [3, 0, 0, 6],\n        [4, 1, 1, 8],\n        [5, 0, 0, 10]\n    ], dtype=float)\n    y1 = np.array([[0.6], [-0.2], [1.55], [1.0], [2.4]], dtype=float)\n    lambda1 = 0.1\n\n    X2 = np.array([\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]\n    ], dtype=float)\n    y2 = np.array([[2.0], [2.1], [1.9], [2.0]], dtype=float)\n    lambda2 = 10.0\n\n    X3 = np.array([\n        [0, 1, 0.0],\n        [1, 0, 1.01],\n        [2, 1, 1.99],\n        [3, 0, 3.02],\n        [4, 1, 3.98],\n        [5, 0, 5.0]\n    ], dtype=float)\n    y3 = np.array([[0.7], [0.32], [1.28], [0.91], [1.89], [1.5]], dtype=float)\n    lambda3 = 1e-6\n\n    test_cases = [\n        (X1, y1, lambda1),\n        (X2, y2, lambda2),\n        (X3, y3, lambda3)\n    ]\n\n    results = []\n    for X, y, lam in test_cases:\n        n, p = X.shape\n\n        # 1. Pseudoinverse-based decoder (w_pinv)\n        # Compute SVD: X = U * diag(s) * Vh\n        U, s, Vh = np.linalg.svd(X, full_matrices=False)\n        \n        # Define the singular value threshold as specified\n        eps = np.finfo(X.dtype).eps\n        sigma_max = np.max(s) if s.size > 0 else 0.0\n        tau = np.max(X.shape) * sigma_max * eps\n\n        # Invert non-zero singular values to form Sigma+\n        s_inv = np.zeros_like(s)\n        significant_s_indices = s > tau\n        s_inv[significant_s_indices] = 1.0 / s[significant_s_indices]\n        \n        # Compute w_pinv = V * Sigma+ * U.T * y\n        # We use Vh.T for V and right-multiply by U.T @ y\n        w_pinv = Vh.T @ (np.diag(s_inv) @ (U.T @ y))\n        y_hat_pinv = X @ w_pinv\n\n        # 2. Ridge-regularized decoder (w_ridge)\n        # Compute w_ridge = (X.T * X + lambda * I)^-1 * X.T * y\n        I = np.identity(p)\n        term1 = np.linalg.inv(X.T @ X + lam * I)\n        term2 = X.T @ y\n        w_ridge = term1 @ term2\n        y_hat_ridge = X @ w_ridge\n\n        # 3. Compute comparison metrics\n        # Mean squared difference between predictions\n        d = (1/n) * np.sum((y_hat_pinv - y_hat_ridge)**2)\n\n        # Mean squared error for each prediction\n        mse_pinv = (1/n) * np.sum((y - y_hat_pinv)**2)\n        mse_ridge = (1/n) * np.sum((y - y_hat_ridge)**2)\n\n        # Difference in mean squared decoding error\n        Delta = mse_pinv - mse_ridge\n        \n        results.append([d, Delta])\n\n    # Format the final output as a string representing a list of lists.\n    formatted_results = [f\"[{d_val},{Delta_val}]\" for d_val, Delta_val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}