{
    "hands_on_practices": [
        {
            "introduction": "有了特征之后，下一步便是拟合模型。尽管普通最小二乘法（$OLS$）是最简单的回归方法，但在从具有相关活动的神经元群体（即共线性）解码时，其结果可能极不稳定。本练习将运用线性代数精确地揭示共线性如何急剧放大解码器预测的方差，从而使其变得不可靠。理解这种失效模式对于认识到使用更稳健技术（如正则化）的必要性至关重要。",
            "id": "4189997",
            "problem": "一个神经解码实验室试图使用基于线性回归的解码方法，从一个包含$p$个同时记录的神经元的群体中重建一个一维运动学变量。在每次试验 $t \\in \\{1,\\dots,n\\}$ 中，观测到的运动学标量由线性模型 $y_{t} = x_{t}^{\\top}\\beta^{\\star} + \\varepsilon_{t}$ 建模，其中 $x_{t} \\in \\mathbb{R}^{p}$ 是预处理后的神经协变量向量，$\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是真实的解码权重向量，$\\varepsilon_{t}$ 是满足 $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$ 的独立噪声项。设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其行向量为 $x_{t}^{\\top}$，并使用权重的普通最小二乘（OLS）估计量来构建解码器。考虑该解码器在一个新的单位范数协变量 $x_{\\mathrm{new}} \\in \\mathbb{R}^{p}$（满足 $|x_{\\mathrm{new}}|=1$）上的预测输出。\n\n从线性模型的基本性质出发，利用 $X^{\\top}X$ 的谱分解，从第一性原理推导线性解码器输出 $x_{\\mathrm{new}}^{\\top}\\hat{\\beta}$ 在所有单位范数 $x_{\\mathrm{new}}$ 上的最坏情况下的预测方差。然后，当 $X^{\\top}X$ 的特征值为 $(\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4})=(500,2,1,0.05)$ 且噪声方差为 $\\sigma^{2}=0.2$ 时，计算这个最坏情况下的预测方差。将您的最终数值答案四舍五入到四位有效数字。将最终答案以一个不带单位的实数形式给出。",
            "solution": "问题陈述已经过验证，被认为是合理的、自洽的且有科学依据的。我们将进行完整的推导。\n\n问题要求解线性解码器的最坏情况预测方差。让我们首先确定普通最小二乘（OLS）估计量 $\\hat{\\beta}$ 的统计性质。线性模型以向量形式给出：$y = X\\beta^{\\star} + \\varepsilon$，其中 $y \\in \\mathbb{R}^{n}$ 是观测向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是真实权重向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是噪声项向量。噪声项是独立同分布的，服从 $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$，这意味着噪声向量 $\\varepsilon$ 服从多元正态分布 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\nOLS 估计量 $\\hat{\\beta}$ 由以下公式给出：\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y $$\n将 $y$ 的模型代入，我们可以用真实参数 $\\beta^{\\star}$ 和噪声 $\\varepsilon$ 来表示 $\\hat{\\beta}$：\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}(X\\beta^{\\star} + \\varepsilon) = (X^{\\top}X)^{-1}X^{\\top}X\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon = \\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon $$\n设计矩阵 $X$ 被认为是固定的（非随机的）。$\\hat{\\beta}$ 的这个表达式中唯一的随机成分是噪声向量 $\\varepsilon$。\n\n首先，我们确定 $\\hat{\\beta}$ 的期望值：\n$$ \\mathrm{E}[\\hat{\\beta}] = \\mathrm{E}[\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon] = \\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\mathrm{E}[\\varepsilon] $$\n由于 $\\mathrm{E}[\\varepsilon] = 0$，我们有 $\\mathrm{E}[\\hat{\\beta}] = \\beta^{\\star}$。这证实了 OLS 估计量是无偏的。\n\n接下来，我们推导 $\\hat{\\beta}$ 的协方差矩阵。协方差矩阵定义为 $\\mathrm{Cov}(\\hat{\\beta}) = \\mathrm{E}[(\\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}])(\\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}])^{\\top}]$。\n$$ \\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}] = (X^{\\top}X)^{-1}X^{\\top}\\varepsilon $$\n$$ \\mathrm{Cov}(\\hat{\\beta}) = \\mathrm{E}[((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)^{\\top}] = \\mathrm{E}[(X^{\\top}X)^{-1}X^{\\top}\\varepsilon\\varepsilon^{\\top}X((X^{\\top}X)^{-1})^{\\top}] $$\n由于 $X$ 是非随机的，我们可以将其移到期望符号外面：\n$$ \\mathrm{Cov}(\\hat{\\beta}) = (X^{\\top}X)^{-1}X^{\\top}\\mathrm{E}[\\varepsilon\\varepsilon^{\\top}]X((X^{\\top}X)^{-1})^{\\top} $$\n项 $\\mathrm{E}[\\varepsilon\\varepsilon^{\\top}]$ 是噪声的协方差矩阵，即 $\\mathrm{Cov}(\\varepsilon) = \\sigma^2 I_n$。矩阵 $X^{\\top}X$ 是对称的，所以它的逆矩阵也是对称的。\n$$ \\mathrm{Cov}(\\hat{\\beta}) = (X^{\\top}X)^{-1}X^{\\top}(\\sigma^2 I_n)X(X^{\\top}X)^{-1} = \\sigma^2 (X^{\\top}X)^{-1}X^{\\top}X(X^{\\top}X)^{-1} = \\sigma^2 (X^{\\top}X)^{-1} $$\n这是 OLS 估计量的一个基本结果。\n\n问题要求我们分析解码器对于一个新协变量向量 $x_{\\mathrm{new}} \\in \\mathbb{R}^p$ 的预测输出。预测值（解码器输出）为 $\\hat{y}_{\\mathrm{new}} = x_{\\mathrm{new}}^{\\top}\\hat{\\beta}$。我们需要找到这个预测的方差。使用随机向量线性变换的方差的一般性质 $\\mathrm{Var}(A z) = A \\mathrm{Cov}(z) A^{\\top}$，其中 $A = x_{\\mathrm{new}}^{\\top}$ 且 $z = \\hat{\\beta}$：\n$$ \\mathrm{Var}(x_{\\mathrm{new}}^{\\top}\\hat{\\beta}) = x_{\\mathrm{new}}^{\\top} \\mathrm{Cov}(\\hat{\\beta}) x_{\\mathrm{new}} $$\n代入 $\\mathrm{Cov}(\\hat{\\beta})$ 的表达式：\n$$ \\mathrm{Var}(x_{\\mathrm{new}}^{\\top}\\hat{\\beta}) = x_{\\mathrm{new}}^{\\top} (\\sigma^2 (X^{\\top}X)^{-1}) x_{\\mathrm{new}} = \\sigma^2 x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} $$\n任务是找到在所有可能的单位范数协变量向量 $x_{\\mathrm{new}}$ 上的最坏情况（最大）预测方差。给定的条件是 $|x_{\\mathrm{new}}|=1$，在此上下文中意味着欧几里得范数为 $1$，即 $\\|x_{\\mathrm{new}}\\|_2 = 1$。因此，问题是求解以下优化问题：\n$$ \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( \\sigma^2 x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) = \\sigma^2 \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) $$\n对于对称矩阵 $M$ 和单位向量 $x$，表达式 $x^{\\top} M x$ 被称为瑞利商。瑞利商的最大值是矩阵 $M$ 的最大特征值。在我们的例子中，$M = (X^{\\top}X)^{-1}$。\n所以，我们有：\n$$ \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) = \\lambda_{\\max}((X^{\\top}X)^{-1}) $$\n设矩阵 $X^{\\top}X$ 的特征值为 $\\lambda_i$。由于 $X^{\\top}X$ 是一个对称正定矩阵，其特征值为实数且为正。其逆矩阵 $(X^{\\top}X)^{-1}$ 的特征值是 $X^{\\top}X$ 特征值的倒数，即 $1/\\lambda_i$。\n因此，$(X^{\\top}X)^{-1}$ 的最大特征值是 $X^{\\top}X$ 最小特征值的倒数：\n$$ \\lambda_{\\max}((X^{\\top}X)^{-1}) = \\max_{i} \\left( \\frac{1}{\\lambda_i} \\right) = \\frac{1}{\\min_{i}(\\lambda_i)} = \\frac{1}{\\lambda_{\\min}(X^{\\top}X)} $$\n将此代回最坏情况方差的表达式中：\n$$ \\text{最坏情况方差} = \\sigma^2 \\cdot \\frac{1}{\\lambda_{\\min}(X^{\\top}X)} $$\n$x_{\\mathrm{new}}$ 的最坏情况方向对应于与 $X^{\\top}X$ 的最小特征值 $\\lambda_{\\min}$ 相关联的特征向量。\n\n现在我们可以使用给定的数值来计算这个表达式。\n$X^{\\top}X$ 的特征值给定为 $(\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4) = (500, 2, 1, 0.05)$。\n这些特征值中最小的是：\n$$ \\lambda_{\\min}(X^{\\top}X) = \\min\\{500, 2, 1, 0.05\\} = 0.05 $$\n噪声方差给定为 $\\sigma^2 = 0.2$。\n\n将这些值代入我们推导出的最坏情况预测方差公式中：\n$$ \\text{最坏情况方差} = \\frac{\\sigma^2}{\\lambda_{\\min}(X^{\\top}X)} = \\frac{0.2}{0.05} = 4 $$\n问题要求答案四舍五入到四位有效数字。精确答案是 $4$。用四位有效数字表示，即为 $4.000$。",
            "answer": "$$\n\\boxed{4.000}\n$$"
        },
        {
            "introduction": "岭回归是一种强大的技术，可以有效解决前一练习中强调的共线性和过拟合问题。通过在目标函数中加入一个惩罚项，岭回归引入了少量偏差，以换取方差的大幅降低。本练习将深入探讨岭回归的内在机制，引导你完整推导其估计量、样本外预测误差，并最终求解最优正则化参数，从而深刻揭示机器学习中核心的偏差-方差权衡。",
            "id": "5002219",
            "problem": "一个实验室正在开发一种脑机接口（BCI）解码器，该解码器在中心向外伸展任务中将神经元发放率映射到手的标量切向速度。在每个宽度为 $\\Delta t$、索引为 $t \\in \\{1,\\ldots,N\\}$ 的时间窗内，预处理后的发放率向量为 $\\mathbf{r}_{t} \\in \\mathbb{R}^{p}$（已在神经元间进行 z-score 标准化和白化，因此样本协方差是单位矩阵），同时测得的手部速度为 $v_{t} \\in \\mathbb{R}$。假设一个线性高斯编码模型 $v_{t} = \\mathbf{r}_{t}^{\\top} \\boldsymbol{\\beta} + \\varepsilon_{t}$，其中 $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$ 在不同的 $t$ 之间是独立的，并且与 $\\mathbf{r}_{t}$ 独立。将数据堆叠成设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$（其行为 $\\mathbf{r}_{t}^{\\top}$）和目标向量 $\\mathbf{y} \\in \\mathbb{R}^{N}$（其条目为 $v_{t}$）。由于神经元特征是使用训练数据进行白化的，你可以假设经验二阶矩满足 $\\mathbf{X}^{\\top}\\mathbf{X} = N \\mathbf{I}_{p}$。\n\n你通过最小化惩罚最小二乘目标函数来训练一个岭回归解码器：\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2},\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数，$\\|\\cdot\\|$ 表示欧几里得范数。\n\n任务：\n1) 从模型 $ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$（其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{N})$）出发，推导最小化 $J(\\mathbf{w};\\lambda)$ 的闭式解 $\\widehat{\\mathbf{w}}$。\n\n2) 对于一个新的、独立的、来自相同分布的测试样本 $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$，其中 $\\mathbf{r}_{\\mathrm{new}}$ 与训练数据独立，并满足 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}] = \\mathbf{0}$ 和 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$，推导期望样本外均方预测误差\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]\n$$\n作为 $\\lambda$、$N$、$p$、$\\sigma^{2}$ 和 $\\boldsymbol{\\beta}$ 的显式函数。你的推导必须从上述定义以及对 $\\mathbf{X}$ 和 $\\mathbf{r}_{\\mathrm{new}}$ 的假设出发，并且必须揭示依赖于 $\\lambda$ 的偏差-方差分解。\n\n3) 为了使权衡显式化且独立于某个特定的未知 $\\boldsymbol{\\beta}$，假设一个与神经群体编码一致的层次先验：$\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$，其中 $\\tau^{2} > 0$。将你对 $\\mathcal{E}(\\lambda)$ 的表达式在此先验上取平均，并简化为一个关于 $\\lambda$、$N$、$p$、$\\sigma^{2}$ 和 $\\tau^{2}$ 的标量函数。\n\n4) 使用你平均后的表达式，确定使期望样本外均方预测误差最小化的值 $\\lambda^{\\star}$。然后，对下列数值进行数值计算：\n- $p = 100$,\n- $N = 10000$,\n- $\\sigma^{2} = 0.04$,\n- $\\tau^{2} = 0.01$。\n将 $\\lambda^{\\star}$ 的最终值表示为一个无单位的纯数字。如果需要四舍五入，则保留四位有效数字。如果不需要，则提供精确值。",
            "solution": "该问题要求在脑机接口（BCI）的背景下，对岭回归解码器进行多步分析。分析内容包括在特定的数据模型和先验条件下，推导解码器、其样本外误差以及最优正则化参数。\n\n### 任务1：岭回归估计量 $\\widehat{\\mathbf{w}}$ 的推导\n岭回归估计量 $\\widehat{\\mathbf{w}}$ 是通过最小化目标函数找到的\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2}\n$$\n其中 $\\|\\cdot\\|$ 是欧几里得范数。我们可以用向量转置来表示平方范数：\n$$\nJ(\\mathbf{w};\\lambda) = (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^{\\top}(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n展开第一项得到：\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - \\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w} - \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{w} + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n由于 $\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y}$ 是一个标量，它等于其转置 $\\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w}$。因此，我们可以合并交叉项：\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\n为了找到最小值，我们对 $J(\\mathbf{w};\\lambda)$ 关于 $\\mathbf{w}$ 求梯度，并令其为零。使用标准的矩阵微积分结果（对于对称矩阵 $\\mathbf{M}$，有 $\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{a} = \\mathbf{a}$ 和 $\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{M}\\mathbf{w} = 2\\mathbf{M}\\mathbf{w}$）：\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w};\\lambda) = -2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\n将梯度设为零向量，得到解 $\\widehat{\\mathbf{w}}$：\n$$\n-2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{0}\n$$\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{X}^{\\top}\\mathbf{y}\n$$\n形式解是 $\\widehat{\\mathbf{w}} = (\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$。问题陈述了经验二阶矩的假设是 $\\mathbf{X}^{\\top}\\mathbf{X} = N\\mathbf{I}_{p}$。将此代入 $\\widehat{\\mathbf{w}}$ 的表达式中：\n$$\n\\widehat{\\mathbf{w}} = (N\\mathbf{I}_{p} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = ((N+\\lambda)\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{I}_{p}^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n这是在给定假设下 $\\widehat{\\mathbf{w}}$ 的闭式解。\n\n### 任务2：期望样本外均方预测误差\n我们被要求推导 $\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]$。期望是针对训练数据的随机性（这使得 $\\widehat{\\mathbf{w}}$ 是随机的）和新的测试样本 $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$ 来计算的。\n新样本的真实模型是 $v_{\\mathrm{new}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}$，其中 $\\varepsilon_{\\mathrm{new}} \\sim \\mathcal{N}(0,\\sigma^{2})$。将此代入误差项：\n$$\nv_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}) - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}\n$$\n对该表达式求平方：\n$$\n(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2} + 2\\varepsilon_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}^{2}\n$$\n现在我们取期望。新的噪声项 $\\varepsilon_{\\mathrm{new}}$ 与训练数据（因此也与 $\\widehat{\\mathbf{w}}$）和新特征 $\\mathbf{r}_{\\mathrm{new}}$ 独立。由于 $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}] = 0$，交叉项消失。我们有 $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}^{2}] = \\sigma^{2}$。\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] + \\sigma^{2}\n$$\n剩下的期望是关于 $\\mathbf{r}_{\\mathrm{new}}$ 和 $\\widehat{\\mathbf{w}}$ 的。我们可以使用迹技巧（trace trick）重写期望内的项：$(\\mathbf{a}^{\\top}\\mathbf{b})^2 = \\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b} = \\mathrm{tr}(\\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}) = \\mathrm{tr}(\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}\\mathbf{b}^{\\top})$。更简单的方法是使用 $\\mathbb{E}[x^2] = \\mathbb{E}[\\mathrm{tr}(x^2)]$，其中 $x$ 是一个标量。\n$$\n\\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] = \\mathbb{E}\\left[\\mathrm{tr}\\left((\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top} (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right)\\right]\n$$\n根据迹和期望的线性性质，并且由于 $\\widehat{\\mathbf{w}}$（来自训练数据）与 $\\mathbf{r}_{\\mathrm{new}}$ 独立：\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right)\n$$\n使用假设 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$：\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top}\\mathbf{I}_{p}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right) = \\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right]\n$$\n所以，$\\mathcal{E}(\\lambda) = \\mathbb{E}[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}] + \\sigma^2$。现在的期望 $\\mathbb{E}[\\cdot]$ 仅针对训练数据的随机性。我们现在进行偏差-方差分解：\n$$\n\\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right] = \\left\\|\\boldsymbol{\\beta} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\right\\|^{2} + \\mathbb{E}\\left[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^{2}\\right] = \\text{Bias}(\\widehat{\\mathbf{w}})^{2} + \\text{Var}(\\widehat{\\mathbf{w}})\n$$\n我们推导偏差和方差项。首先，将 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ 代入 $\\widehat{\\mathbf{w}}$ 的表达式：\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\n$$\n$\\widehat{\\mathbf{w}}$ 的期望（关于 $\\boldsymbol{\\varepsilon}$）是：\n$$\n\\mathbb{E}[\\widehat{\\mathbf{w}}] = \\mathbb{E}\\left[\\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\\right] = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}]) = \\frac{N}{N+\\lambda}\\boldsymbol{\\beta}\n$$\n平方偏差是：\n$$\n\\text{Bias}(\\widehat{\\mathbf{w}})^{2} = \\left\\|\\mathbb{E}[\\widehat{\\mathbf{w}}] - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|\\frac{N}{N+\\lambda}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|-\\frac{\\lambda}{N+\\lambda}\\boldsymbol{\\beta}\\right\\|^{2} = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2}\n$$\n方差项是 $\\mathbb{E}[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^2] = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}}))$。\n$$\n\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}] = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\n$$\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\mathbb{E}\\left[(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])^{\\top}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbb{E}\\left[\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]\\mathbf{X}\n$$\n使用 $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]=\\sigma^2\\mathbf{I}_N$ 和 $\\mathbf{X}^{\\top}\\mathbf{X}=N\\mathbf{I}_p$：\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{I}_{N}\\mathbf{X} = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{X} = \\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\n$$\n方差是该协方差矩阵的迹：\n$$\n\\text{Var}(\\widehat{\\mathbf{w}}) = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}})) = \\mathrm{tr}\\left(\\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\\right) = \\frac{pN\\sigma^2}{(N+\\lambda)^2}\n$$\n结合所有项，期望样本外误差为：\n$$\n\\mathcal{E}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### 任务3：在 $\\boldsymbol{\\beta}$ 的先验上取平均\n给定真实权重的先验分布为 $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$。我们需要计算 $\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\mathbb{E}_{\\boldsymbol{\\beta}}[\\mathcal{E}(\\lambda)]$。$\\mathcal{E}(\\lambda)$ 中唯一依赖于 $\\boldsymbol{\\beta}$ 的项是 $\\|\\boldsymbol{\\beta}\\|^{2}$。我们计算其在先验下的期望：\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\sum_{i=1}^{p}\\beta_{i}^{2}\\right] = \\sum_{i=1}^{p}\\mathbb{E}_{\\boldsymbol{\\beta}}[\\beta_{i}^{2}]\n$$\n对于每个分量 $\\beta_i \\sim \\mathcal{N}(0, \\tau^{2})$，其二阶矩为 $\\mathbb{E}[\\beta_{i}^{2}] = \\mathrm{Var}(\\beta_i) + (\\mathbb{E}[\\beta_i])^{2} = \\tau^{2} + 0^{2} = \\tau^{2}$。\n因此，期望平方范数为：\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\sum_{i=1}^{p}\\tau^{2} = p\\tau^{2}\n$$\n将此代入 $\\mathcal{E}(\\lambda)$ 的表达式：\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}p\\tau^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{p\\lambda^{2}\\tau^{2} + pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### 任务4：最优正则化参数 $\\lambda^{\\star}$\n为了找到最小化 $\\mathcal{E}_{\\mathrm{avg}}(\\lambda)$ 的 $\\lambda$，我们对其关于 $\\lambda$ 求导，并令导数为零。在最小化过程中可以忽略常数项 $\\sigma^{2}$。\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{d}{d\\lambda}\\left( \\frac{p(\\lambda^{2}\\tau^{2} + N\\sigma^2)}{(N+\\lambda)^2} \\right)\n$$\n使用商法则 $\\frac{d}{dx}(\\frac{u}{v}) = \\frac{u'v - uv'}{v^2}$：\n令 $u(\\lambda) = p(\\lambda^{2}\\tau^{2} + N\\sigma^2)$ 和 $v(\\lambda) = (N+\\lambda)^2$。\n则 $u'(\\lambda) = 2p\\lambda\\tau^{2}$ 和 $v'(\\lambda) = 2(N+\\lambda)$。\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{(2p\\lambda\\tau^{2})(N+\\lambda)^2 - p(\\lambda^{2}\\tau^{2} + N\\sigma^2) \\cdot 2(N+\\lambda)}{(N+\\lambda)^4}\n$$\n令导数为零，并假设 $\\lambda \\ge 0, N \\ge 1$，我们可以除以非零因子 $2p(N+\\lambda)$ 来进行简化：\n$$\n(\\lambda\\tau^{2})(N+\\lambda) - (\\lambda^{2}\\tau^{2} + N\\sigma^2) = 0\n$$\n$$\nN\\lambda\\tau^{2} + \\lambda^{2}\\tau^{2} - \\lambda^{2}\\tau^{2} - N\\sigma^2 = 0\n$$\n$$\nN\\lambda\\tau^{2} = N\\sigma^2\n$$\n由于 $N \\ge 1$ 且 $\\tau^{2} > 0$（给定），我们可以除以 $N\\tau^{2}$ 找到最优的 $\\lambda^{\\star}$：\n$$\n\\lambda^{\\star} = \\frac{\\sigma^2}{\\tau^2}\n$$\n这个结果非常简洁，表示测量中的噪声方差与模型参数的先验方差之比。可以检查二阶导数以确认这是一个最小值。导数的分子简化为 $2pN(\\lambda\\tau^2 - \\sigma^2)(N+\\lambda)$，当 $\\lambda  \\sigma^2/\\tau^2$ 时为负，当 $\\lambda > \\sigma^2/\\tau^2$ 时为正，这确认了它是一个最小值。\n\n最后，我们使用给定的值进行数值计算：\n- $\\sigma^{2} = 0.04$\n- $\\tau^{2} = 0.01$\n在这种理想化的设置下，找到 $\\lambda^{\\star}$ 并不需要 $p$ 和 $N$ 的值。\n$$\n\\lambda^{\\star} = \\frac{0.04}{0.01} = 4\n$$\n最优值恰好为 $4$。",
            "answer": "$$\n\\boxed{4}\n$$"
        }
    ]
}