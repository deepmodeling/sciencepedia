{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of neural coding, we begin with the Poisson model, a cornerstone for describing the stochastic nature of spike counts. This first exercise guides you through a derivation of Fisher information from first principles for a neuron whose firing rate is modulated by a stimulus parameter $\\theta$. By working through this problem , you will not only practice the core mathematical definition of Fisher information but also explore how biophysical constraints, such as a limited metabolic budget modeled as a fixed average firing rate, create fundamental trade-offs in neural coding efficiency.",
            "id": "4163192",
            "problem": "Consider a single sensory neuron whose spike count over a fixed observation window of duration $T$ is modeled as a discrete random variable $N \\in \\{0,1,2,\\dots\\}$ conditional on a scalar stimulus parameter $\\theta$. Assume a scientifically standard count model: conditional on $\\theta$, the spikes follow a Poisson process with constant rate over the window, so the distribution of $N$ is fully specified by its mean $\\lambda(\\theta)$ via the probability mass function (PMF) $p(N=n \\mid \\theta)$ of the Poisson distribution. The neuron's rate is stimulus-dependent through a gain function $g(\\theta)$ and a baseline scaling $a>0$, given by $f(\\theta) = a\\,g(\\theta)$, and the count mean is $\\lambda(\\theta) = f(\\theta)\\,T$. Assume $g(\\theta)$ is positive and continuously differentiable for all $\\theta$ in the stimulus domain, and $T>0$ is fixed.\n\nStarting from the foundational definition of Fisher information (FI) for a parameter $\\theta$ in a parametric statistical model, namely $I(\\theta) = \\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\ln p(N \\mid \\theta)\\right)^{2}\\right]$, derive an explicit closed-form expression for $I(\\theta)$ in terms of $a$, $g(\\theta)$, $g'(\\theta)$, and $T$, without invoking any pre-derived formulas for FI beyond its definition. Then, let $p(\\theta)$ be a prior density over $\\theta$ that is strictly positive and integrates to one over the stimulus domain. Suppose the neuron's mean firing rate is constrained in expectation under $p(\\theta)$ to equal a fixed value $\\bar{f}0$, i.e., $\\int f(\\theta)\\,p(\\theta)\\,d\\theta = \\bar{f}$. Using this constraint, express the prior-averaged Fisher information $\\bar{I} = \\int I(\\theta)\\,p(\\theta)\\,d\\theta$ in terms of $g(\\theta)$, $g'(\\theta)$, $p(\\theta)$, $T$, and $\\bar{f}$, and analyze qualitatively the trade-offs that arise between the shape of $g(\\theta)$ and $\\bar{I}$ when $a$ must be chosen to satisfy the fixed mean rate constraint.\n\nProvide as your final answer the explicit expression for the Fisher information $I(\\theta)$ derived from first principles. No numerical computation is required, and no rounding is necessary. Do not include units in your final answer.",
            "solution": "We begin with the model assumptions. The spike count $N$ over a fixed window of duration $T$ is Poisson with mean $\\lambda(\\theta) = f(\\theta)\\,T = a\\,g(\\theta)\\,T$. The probability mass function (PMF) of a Poisson random variable is\n$$\np(N=n \\mid \\theta) \\;=\\; \\frac{\\exp\\!\\left(-\\lambda(\\theta)\\right)\\,\\lambda(\\theta)^{n}}{n!}, \\quad n \\in \\{0,1,2,\\dots\\}.\n$$\nBy the definition of Fisher information (FI),\n$$\nI(\\theta) \\;=\\; \\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\ln p(N \\mid \\theta)\\right)^{2}\\right],\n$$\nwhere the expectation is with respect to $p(N \\mid \\theta)$.\n\nWe compute the score function $\\frac{\\partial}{\\partial \\theta}\\ln p(N \\mid \\theta)$. The log-likelihood for a single observation $n$ is\n$$\n\\ln p(N=n \\mid \\theta) \\;=\\; -\\lambda(\\theta) \\;+\\; n \\,\\ln \\lambda(\\theta) \\;-\\; \\ln(n!).\n$$\nDifferentiating with respect to $\\theta$,\n$$\n\\frac{\\partial}{\\partial \\theta}\\ln p(N=n \\mid \\theta)\n\\;=\\; -\\lambda'(\\theta) \\;+\\; n \\,\\frac{\\lambda'(\\theta)}{\\lambda(\\theta)}\n\\;=\\; \\lambda'(\\theta)\\left(\\frac{n}{\\lambda(\\theta)} - 1\\right),\n$$\nwhere $\\lambda'(\\theta) = \\frac{d}{d\\theta}\\lambda(\\theta)$.\n\nTherefore, the Fisher information is\n$$\nI(\\theta) \\;=\\; \\mathbb{E}\\!\\left[\\left(\\lambda'(\\theta)\\left(\\frac{N}{\\lambda(\\theta)} - 1\\right)\\right)^{2}\\right]\n\\;=\\; \\left(\\lambda'(\\theta)\\right)^{2} \\,\\mathbb{E}\\!\\left[\\left(\\frac{N}{\\lambda(\\theta)} - 1\\right)^{2}\\right].\n$$\nTo evaluate the expectation, use basic properties of the Poisson distribution: $\\mathbb{E}[N] = \\lambda(\\theta)$ and $\\mathbb{Var}(N) = \\lambda(\\theta)$. Then\n$$\n\\mathbb{E}\\!\\left[\\left(\\frac{N}{\\lambda(\\theta)} - 1\\right)^{2}\\right]\n\\;=\\; \\mathbb{Var}\\!\\left(\\frac{N}{\\lambda(\\theta)}\\right) \\;+\\; \\left(\\mathbb{E}\\!\\left[\\frac{N}{\\lambda(\\theta)}\\right] - 1\\right)^{2}\n\\;=\\; \\frac{\\mathbb{Var}(N)}{\\lambda(\\theta)^{2}} \\;+\\; \\left(\\frac{\\mathbb{E}[N]}{\\lambda(\\theta)} - 1\\right)^{2}\n\\;=\\; \\frac{\\lambda(\\theta)}{\\lambda(\\theta)^{2}} \\;+\\; \\left(1 - 1\\right)^{2}\n\\;=\\; \\frac{1}{\\lambda(\\theta)}.\n$$\nSubstituting back,\n$$\nI(\\theta) \\;=\\; \\frac{\\left(\\lambda'(\\theta)\\right)^{2}}{\\lambda(\\theta)}.\n$$\nNow express $\\lambda(\\theta)$ and its derivative in terms of the model components. We have\n$$\n\\lambda(\\theta) \\;=\\; f(\\theta)\\,T \\;=\\; a\\,g(\\theta)\\,T,\n\\qquad\n\\lambda'(\\theta) \\;=\\; a\\,g'(\\theta)\\,T,\n$$\nwhere $g'(\\theta) = \\frac{d}{d\\theta}g(\\theta)$. Therefore\n$$\nI(\\theta)\n\\;=\\; \\frac{\\left(a\\,g'(\\theta)\\,T\\right)^{2}}{a\\,g(\\theta)\\,T}\n\\;=\\; T\\,a\\,\\frac{\\left(g'(\\theta)\\right)^{2}}{g(\\theta)}.\n$$\n\nWe now analyze the trade-offs under a fixed mean rate constraint. Let $p(\\theta)$ be a prior density over $\\theta$ that is strictly positive and integrates to one over the stimulus domain. The fixed mean rate constraint is\n$$\n\\int f(\\theta)\\,p(\\theta)\\,d\\theta \\;=\\; \\bar{f},\n$$\nwith $\\bar{f} > 0$ given. Substituting $f(\\theta) = a\\,g(\\theta)$,\n$$\na \\int g(\\theta)\\,p(\\theta)\\,d\\theta \\;=\\; \\bar{f}\n\\quad\\Rightarrow\\quad\na \\;=\\; \\frac{\\bar{f}}{\\int g(\\theta)\\,p(\\theta)\\,d\\theta}.\n$$\nThe prior-averaged Fisher information is\n$$\n\\bar{I} \\;=\\; \\int I(\\theta)\\,p(\\theta)\\,d\\theta\n\\;=\\; \\int T\\,a\\,\\frac{\\left(g'(\\theta)\\right)^{2}}{g(\\theta)}\\,p(\\theta)\\,d\\theta\n\\;=\\; T\\,\\frac{\\bar{f}}{\\int g(\\theta)\\,p(\\theta)\\,d\\theta}\\,\\int \\frac{\\left(g'(\\theta)\\right)^{2}}{g(\\theta)}\\,p(\\theta)\\,d\\theta.\n$$\nIf one chooses to normalize $g(\\theta)$ so that $\\int g(\\theta)\\,p(\\theta)\\,d\\theta = 1$, then $a = \\bar{f}$ and the averaged Fisher information simplifies to\n$$\n\\bar{I} \\;=\\; T\\,\\bar{f}\\,\\int \\frac{\\left(g'(\\theta)\\right)^{2}}{g(\\theta)}\\,p(\\theta)\\,d\\theta.\n$$\nThis expression highlights a central trade-off: for fixed $\\bar{f}$ and $T$, increasing the ratio $\\left(g'(\\theta)\\right)^{2}/g(\\theta)$ at values of $\\theta$ weighted by $p(\\theta)$ increases information. However, $g(\\theta)$ must remain positive and typically obeys smoothness, dynamic range, and biophysical constraints. For example, if $g(\\theta)$ is bounded away from zero and has limited slope due to bandwidth constraints, then the integral $\\int \\frac{\\left(g'(\\theta)\\right)^{2}}{g(\\theta)}\\,p(\\theta)\\,d\\theta$ is limited, imposing an upper bound on $\\bar{I}$. Under a normalization constraint, pushing $g(\\theta)$ to be small where its slope is large can increase local information but may reduce information elsewhere due to the normalization coupling $a$ to the average of $g(\\theta)$. Thus the fixed mean rate constraint couples gain allocation across $\\theta$, and the optimal $g(\\theta)$ must trade off steepness (which increases information) against maintaining sufficient magnitude (which supports overall rate levels) under the global constraint $\\int g(\\theta)\\,p(\\theta)\\,d\\theta$.\n\nIn summary, the Fisher information at $\\theta$ derived from first principles is $I(\\theta) = T\\,a\\,\\frac{\\left(g'(\\theta)\\right)^{2}}{g(\\theta)}$, and the prior-averaged information under a fixed mean rate constraint scales as $T\\,\\bar{f}$ times a weighted integral of $\\frac{\\left(g'(\\theta)\\right)^{2}}{g(\\theta)}$ divided by the average of $g(\\theta)$ under $p(\\theta)$, revealing the trade-offs between steepness, positivity, normalization, and rate allocation.",
            "answer": "$$\\boxed{T\\,a\\,\\frac{\\left(g'(\\theta)\\right)^{2}}{g(\\theta)}}$$"
        },
        {
            "introduction": "While the Poisson model is a useful starting point, real neural responses often exhibit variability greater than their mean, a phenomenon known as overdispersion. This practice  tackles this reality by using the Negative Binomial distribution, a more flexible model for spike counts. Your task will be to derive the Fisher information for this model and express it in terms of the Fano factor $F(\\theta)$, a standard measure of neuronal variability, revealing the direct quantitative cost of noise on coding precision.",
            "id": "4163157",
            "problem": "A single neuron encodes a scalar stimulus parameter $\\theta$ during a fixed observation window of duration $T$. The neuron's conditional mean spike count is $\\mu(\\theta) = T\\,\\lambda(\\theta)$, where $\\lambda(\\theta)  0$ is a differentiable tuning curve. Empirically, the spike counts are overdispersed relative to a Poisson process, with Fano factor (FF) $F(\\theta) \\equiv \\frac{\\operatorname{Var}[N \\mid \\theta]}{\\mathbb{E}[N \\mid \\theta]}  1$. Assume that, conditional on $\\theta$, the spike count $N \\in \\{0,1,2,\\dots\\}$ follows a Negative Binomial (NB) model with mean $\\mu(\\theta)$ and known dispersion parameter $\\kappa  0$ that does not depend on $\\theta$. Use the following parameterization of the Negative Binomial probability mass function:\n$$\n\\Pr(N=n \\mid \\mu,\\kappa) \\;=\\; \\frac{\\Gamma(n+\\kappa)}{\\Gamma(\\kappa)\\,n!}\\,\\left(\\frac{\\kappa}{\\kappa+\\mu}\\right)^{\\kappa}\\,\\left(\\frac{\\mu}{\\kappa+\\mu}\\right)^{n}, \\quad n=0,1,2,\\dots\n$$\nwhich has $\\mathbb{E}[N\\mid \\mu,\\kappa] = \\mu$ and $\\operatorname{Var}[N\\mid \\mu,\\kappa] = \\mu + \\frac{\\mu^{2}}{\\kappa}$.\n\nStarting from the definition of Fisher information (FI) for a scalar parameter,\n$$\n\\mathcal{J}(\\theta) \\;=\\; \\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\ln p(N\\mid \\theta)\\right)^{2} \\Bigm| \\theta \\right] \\;=\\; -\\,\\mathbb{E}\\!\\left[\\frac{\\partial^{2}}{\\partial \\theta^{2}}\\ln p(N\\mid \\theta) \\Bigm| \\theta \\right],\n$$\nderive a closed-form expression for the Fisher information $\\mathcal{J}(\\theta)$ about $\\theta$ contained in $N$ under the Negative Binomial model above. Then express your result entirely in terms of the Fano factor $F(\\theta)$, the mean $\\mu(\\theta)$, and the derivative $\\mu'(\\theta) \\equiv \\frac{d\\mu}{d\\theta}$.\n\nGive your final answer as a single simplified analytic expression. No numerical evaluation is required and no units are needed.",
            "solution": "Our goal is to derive an expression for the Fisher information $\\mathcal{J}(\\theta)$ and express it in terms of the mean spike count $\\mu(\\theta)$, its derivative $\\mu'(\\theta)$, and the Fano factor $F(\\theta)$.\n\nLet's begin with the log-likelihood function, $\\ln p(N \\mid \\theta)$, based on the provided PMF. Throughout the derivation, we will denote $\\mu(\\theta)$ as $\\mu$ and its derivative $\\frac{d\\mu}{d\\theta}$ as $\\mu'$ for notational simplicity.\n$$\n\\ln p(N \\mid \\theta) = \\ln\\left(\\frac{\\Gamma(N+\\kappa)}{\\Gamma(\\kappa)\\,N!}\\right) + \\kappa \\ln\\left(\\frac{\\kappa}{\\kappa+\\mu}\\right) + N \\ln\\left(\\frac{\\mu}{\\kappa+\\mu}\\right)\n$$\nWe can rewrite the logarithmic terms involving $\\mu$ as:\n$$\n\\ln p(N \\mid \\theta) = \\ln\\left(\\frac{\\Gamma(N+\\kappa)}{\\Gamma(\\kappa)\\,N!}\\right) + \\kappa(\\ln\\kappa - \\ln(\\kappa+\\mu)) + N(\\ln\\mu - \\ln(\\kappa+\\mu))\n$$\nThe first term and $\\kappa\\ln\\kappa$ do not depend on $\\theta$.\n\nNow, we compute the score, which is the first partial derivative of the log-likelihood with respect to $\\theta$. We use the chain rule, as $\\mu$ is a function of $\\theta$.\n$$\n\\frac{\\partial}{\\partial \\theta} \\ln p(N \\mid \\theta) = \\left( \\frac{\\partial}{\\partial \\mu} \\ln p(N \\mid \\mu) \\right) \\frac{d\\mu}{d\\theta}\n$$\nFirst, we compute the derivative with respect to $\\mu$:\n$$\n\\frac{\\partial}{\\partial \\mu} \\ln p(N \\mid \\mu) = -\\kappa \\frac{1}{\\kappa+\\mu} + N \\frac{1}{\\mu} - N \\frac{1}{\\kappa+\\mu} = \\frac{N}{\\mu} - \\frac{N+\\kappa}{\\kappa+\\mu}\n$$\nCombining these terms over a common denominator:\n$$\n\\frac{\\partial}{\\partial \\mu} \\ln p(N \\mid \\mu) = \\frac{N(\\kappa+\\mu) - \\mu(N+\\kappa)}{\\mu(\\kappa+\\mu)} = \\frac{N\\kappa+N\\mu-N\\mu-\\mu\\kappa}{\\mu(\\kappa+\\mu)} = \\frac{\\kappa(N-\\mu)}{\\mu(\\kappa+\\mu)}\n$$\nThe score function is therefore:\n$$\n\\frac{\\partial}{\\partial \\theta} \\ln p(N \\mid \\theta) = \\frac{\\kappa(N-\\mu)}{\\mu(\\kappa+\\mu)} \\mu'\n$$\nThe Fisher information is defined as the expected value of the square of the score:\n$$\n\\mathcal{J}(\\theta) = \\mathbb{E}\\left[ \\left(\\frac{\\partial}{\\partial \\theta}\\ln p(N\\mid \\theta)\\right)^{2} \\biggm| \\theta \\right] = \\mathbb{E}\\left[ \\left( \\frac{\\kappa(N-\\mu)}{\\mu(\\kappa+\\mu)} \\mu' \\right)^2 \\biggm| \\theta \\right]\n$$\nThe terms that do not depend on the random variable $N$ can be factored out of the expectation:\n$$\n\\mathcal{J}(\\theta) = \\left( \\frac{\\kappa \\mu'}{\\mu(\\kappa+\\mu)} \\right)^2 \\mathbb{E}\\left[ (N-\\mu)^2 \\mid \\theta \\right]\n$$\nThe term $\\mathbb{E}\\left[ (N-\\mu)^2 \\mid \\theta \\right]$ is the variance of $N$ conditional on $\\theta$, denoted $\\operatorname{Var}[N \\mid \\theta]$.\n$$\n\\mathcal{J}(\\theta) = \\frac{\\kappa^2 (\\mu')^2}{\\mu^2(\\kappa+\\mu)^2} \\operatorname{Var}[N \\mid \\theta]\n$$\nThe problem states that the variance of the NB distribution is $\\operatorname{Var}[N \\mid \\mu, \\kappa] = \\mu + \\frac{\\mu^2}{\\kappa}$. We can simplify this expression:\n$$\n\\operatorname{Var}[N \\mid \\theta] = \\mu + \\frac{\\mu^2}{\\kappa} = \\frac{\\mu\\kappa + \\mu^2}{\\kappa} = \\frac{\\mu(\\kappa+\\mu)}{\\kappa}\n$$\nSubstituting this into our expression for $\\mathcal{J}(\\theta)$:\n$$\n\\mathcal{J}(\\theta) = \\frac{\\kappa^2 (\\mu')^2}{\\mu^2(\\kappa+\\mu)^2} \\left( \\frac{\\mu(\\kappa+\\mu)}{\\kappa} \\right)\n$$\nWe can cancel common factors from the numerator and denominator:\n$$\n\\mathcal{J}(\\theta) = \\frac{\\kappa (\\mu')^2}{\\mu(\\kappa+\\mu)}\n$$\nThis is the Fisher information in terms of $\\mu$, $\\mu'$, and the dispersion parameter $\\kappa$. The final step is to express this in terms of the Fano factor $F(\\theta)$.\nThe Fano factor is given by:\n$$\nF(\\theta) = \\frac{\\operatorname{Var}[N \\mid \\theta]}{\\mathbb{E}[N \\mid \\theta]} = \\frac{\\mu + \\frac{\\mu^2}{\\kappa}}{\\mu} = 1 + \\frac{\\mu}{\\kappa}\n$$\nWe solve this equation for $\\kappa$:\n$$\nF(\\theta) - 1 = \\frac{\\mu}{\\kappa} \\implies \\kappa = \\frac{\\mu}{F(\\theta)-1}\n$$\nSince the problem states $F(\\theta) > 1$, $\\kappa$ is well-defined and positive. Now, we substitute this expression for $\\kappa$ into our equation for $\\mathcal{J}(\\theta)$. It is convenient to first rearrange the expression for $\\mathcal{J}(\\theta)$:\n$$\n\\mathcal{J}(\\theta) = \\frac{(\\mu')^2}{\\mu} \\cdot \\frac{\\kappa}{\\kappa+\\mu}\n$$\nLet's evaluate the term $\\frac{\\kappa}{\\kappa+\\mu}$ using the expression for $\\kappa$:\n$$\n\\frac{\\kappa}{\\kappa+\\mu} = \\frac{\\frac{\\mu}{F(\\theta)-1}}{\\frac{\\mu}{F(\\theta)-1} + \\mu} = \\frac{\\frac{\\mu}{F(\\theta)-1}}{\\mu\\left(\\frac{1}{F(\\theta)-1} + 1\\right)} = \\frac{\\frac{1}{F(\\theta)-1}}{\\frac{1 + (F(\\theta)-1)}{F(\\theta)-1}} = \\frac{\\frac{1}{F(\\theta)-1}}{\\frac{F(\\theta)}{F(\\theta)-1}} = \\frac{1}{F(\\theta)}\n$$\nSubstituting this simplified term back into the expression for $\\mathcal{J}(\\theta)$, we arrive at the final result:\n$$\n\\mathcal{J}(\\theta) = \\frac{(\\mu')^2}{\\mu} \\cdot \\frac{1}{F(\\theta)} = \\frac{(\\mu'(\\theta))^2}{\\mu(\\theta)F(\\theta)}\n$$\nThis expression relates the Fisher information to the sensitivity of the mean response ($\\mu'$), the magnitude of the mean response ($\\mu$), and the level of overdispersion as captured by the Fano factor ($F$).",
            "answer": "$$\n\\boxed{\\frac{(\\mu'(\\theta))^{2}}{\\mu(\\theta) F(\\theta)}}\n$$"
        },
        {
            "introduction": "Sensory information is rarely encoded by single neurons but rather by large, interacting populations where noise can be correlated across cells. This final practice moves from single-cell analysis to the realm of population coding, using a multivariate normal model to capture these critical interactions . You will derive how to calculate information for a neuronal subset, formulate a greedy algorithm to find the most informative neurons, and explore the conditions for its optimality, providing a powerful link between Fisher information and the practical challenge of decoding from large-scale neural recordings.",
            "id": "4163134",
            "problem": "You are given a neural population of size $n$ with experimentally measured stimulus sensitivity and noise statistics at a fixed stimulus parameter $\\theta$. Specifically, the derivative of the mean response with respect to $\\theta$ is a vector $\\mu'(\\theta) \\in \\mathbb{R}^n$, and the noise covariance is a positive definite matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$ that does not depend on $\\theta$. Assume that the conditional distribution of the neural response given $\\theta$ is multivariate normal with mean $\\mu(\\theta)$ and covariance $\\Sigma$. The goal is to select a subset of $k$ neurons that maximizes the scalar Fisher information about $\\theta$ carried by the selected subset.\n\nStarting only from core definitions and well-tested formulas, proceed as follows.\n\n1. Starting from the definition of the log-likelihood for a multivariate normal model with mean $\\mu(\\theta)$ and covariance $\\Sigma$ independent of $\\theta$, and from the definition of Fisher information for a scalar parameter, derive an expression for the Fisher information carried by a selected subset of neurons $S \\subseteq \\{0,1,\\dots,n-1\\}$. Express this Fisher information in terms of the subvector $\\mu'_S(\\theta)$ and the principal submatrix $\\Sigma_{SS}$.\n\n2. Using only matrix identities and the structure of block matrices, derive a closed-form expression for the incremental increase in Fisher information when adding a new neuron $j \\notin S$ to a current selection $S$. Your expression must be written in terms of $\\mu'(\\theta)$, $\\Sigma$, and the already-selected set $S$ in a way that is computable without accessing any responses beyond those neurons in $S \\cup \\{j\\}$. Your derivation must not assume any special structure of $\\Sigma$ beyond positive definiteness.\n\n3. Propose a greedy selection algorithm that, starting from the empty set, iteratively adds the neuron that maximizes the incremental increase in Fisher information until $k$ neurons are selected. Provide a principled analysis of when this greedy algorithm is guaranteed to be optimal. State a sufficient condition in terms of $\\Sigma$ under which the set function is modular, and formulate a precise diminishing-returns (submodularity) condition for general $\\Sigma$ under which the greedy algorithm achieves the standard approximation guarantee for monotone submodular maximization under a cardinality constraint.\n\n4. Implement a complete program that:\n   - Computes the Fisher information for any subset $S$.\n   - Computes the incremental gain when adding $j$ to $S$ based on your derived expression.\n   - Implements the greedy algorithm to select $k$ neurons.\n   - Computes the optimal subset of size $k$ by exhaustive search to validate the greedy solution for small $n$.\n   - Verifies the diminishing-returns condition numerically by checking for all $S \\subseteq T \\subseteq \\{0,\\dots,n-1\\}$ and all $j \\notin T$ that the incremental gain of adding $j$ to $S$ is greater than or equal to the incremental gain of adding $j$ to $T$, up to a small numerical tolerance.\n\n5. For each of the following test cases, run your implementation and, for each case, output a list containing:\n   - The indices selected by the greedy algorithm as a list of integers (zero-based indexing).\n   - The indices of an optimal subset of size $k$ found by exhaustive search as a list of integers (zero-based indexing).\n   - The Fisher information value (float) of the greedy selection.\n   - The Fisher information value (float) of the optimal selection.\n   - A boolean indicating whether the greedy selection achieves the optimal Fisher information.\n   - A boolean indicating whether the diminishing-returns (submodularity) condition holds for this instance up to numerical tolerance.\n\n   Use the following test suite. All numbers below denote dimensionless quantities.\n   - Case A: $n = 4$, $k = 2$, $\\mu'(\\theta) = [1.0, 0.8, 0.3, 0.1]$, $\\Sigma = \\mathrm{diag}([1.0, 2.0, 0.5, 0.2])$.\n   - Case B: $n = 3$, $k = 2$, $\\mu'(\\theta) = [1.0, 0.9, 0.8]$, $$ \\Sigma = \\begin{bmatrix} 1.0  0.9  0.0 \\\\ 0.9  1.0  0.0 \\\\ 0.0  0.0  1.0 \\end{bmatrix} $$.\n   - Case C: $n = 3$, $k = 2$, $\\mu'(\\theta) = [1.0, 0.8, 0.8]$, $$ \\Sigma = \\begin{bmatrix} 1.0  0.7  0.7 \\\\ 0.7  1.0  0.0 \\\\ 0.7  0.0  1.0 \\end{bmatrix} $$.\n   - Case D: $n = 3$, $k = 2$, $\\mu'(\\theta) = [1.0, 1.0, 1.0]$, $$ \\Sigma = \\begin{bmatrix} 1.0  0.99  0.99 \\\\ 0.99  1.0  0.99 \\\\ 0.99  0.99  1.0 \\end{bmatrix} $$.\n\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, where each case-specific result is itself a list as described above. For example, the format should be like \"[[greedy_indices_case_A,optimal_indices_case_A,greedy_FI_case_A,optimal_FI_case_A,is_optimal_case_A,is_submodular_case_A],[...case_B...],[...case_C...],[...case_D...]]\". No additional text should be printed.",
            "solution": "The problem of selecting an optimal subset of neurons to maximize Fisher information is a central task in the analysis of neural population codes.\nWe are given the stimulus sensitivity of the mean response, $\\mu'(\\theta) \\in \\mathbb{R}^n$, and the stimulus-independent noise covariance matrix, $\\Sigma \\in \\mathbb{R}^{n \\times n}$, for a population of $n$ neurons. The response of a subset of neurons $S$ is modeled by a multivariate normal distribution $\\mathbf{r}_S \\sim \\mathcal{N}(\\mu_S(\\theta), \\Sigma_{SS})$. We shall proceed by deriving the necessary theoretical expressions, analyzing the properties of the resulting optimization problem, and then implementing a solution.\n\n### 1. Derivation of the Fisher Information for a Neuronal Subset\n\nThe Fisher information for a single scalar parameter $\\theta$ is given by the definition $I(\\theta) = -E\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\log p(\\mathbf{r}|\\theta)\\right]$. Let $S \\subseteq \\{0, 1, \\dots, n-1\\}$ be a set of indices for a selected subset of neurons. The response vector for this subset is $\\mathbf{r}_S$, and its probability density function is that of a multivariate normal distribution with mean $\\mu_S(\\theta)$ and covariance $\\Sigma_{SS}$, where $|S|$ is the number of neurons in the subset.\n\nThe log-likelihood function is:\n$$\n\\log p(\\mathbf{r}_S|\\theta) = -\\frac{|S|}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det(\\Sigma_{SS}) - \\frac{1}{2}(\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} (\\mathbf{r}_S - \\mu_S(\\theta))\n$$\nSince the covariance matrix $\\Sigma$ (and thus any principal submatrix $\\Sigma_{SS}$) is assumed to be independent of $\\theta$, we only need to differentiate the last term with respect to $\\theta$. Let $\\mu'_S(\\theta) = \\frac{\\partial \\mu_S(\\theta)}{\\partial\\theta}$.\nThe first derivative of the log-likelihood is:\n$$\n\\frac{\\partial}{\\partial \\theta} \\log p(\\mathbf{r}_S|\\theta) = - \\frac{1}{2} \\frac{\\partial}{\\partial \\theta} \\left[ (\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} (\\mathbf{r}_S - \\mu_S(\\theta)) \\right]\n$$\nUsing the chain rule and standard vector calculus identities, this becomes:\n$$\n\\frac{\\partial}{\\partial \\theta} \\log p(\\mathbf{r}_S|\\theta) = (\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu'_S(\\theta)\n$$\nThe second derivative with respect to $\\theta$ is:\n$$\n\\frac{\\partial^2}{\\partial \\theta^2} \\log p(\\mathbf{r}_S|\\theta) = \\frac{\\partial}{\\partial \\theta} \\left[ (\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu'_S(\\theta) \\right] = -(\\mu'_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu'_S(\\theta) + (\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu''_S(\\theta)\n$$\nwhere $\\mu''_S(\\theta) = \\frac{\\partial^2 \\mu_S(\\theta)}{\\partial\\theta^2}$. To find the Fisher information, we take the negative expectation:\n$$\nI_S(\\theta) = -E\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\log p(\\mathbf{r}_S|\\theta)\\right] = -E\\left[-(\\mu'_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu'_S(\\theta)\\right] - E\\left[(\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu''_S(\\theta)\\right]\n$$\nThe first term is a constant with respect to the random variable $\\mathbf{r}_S$. The expectation of the second term is zero because $E[\\mathbf{r}_S - \\mu_S(\\theta)] = \\mathbf{0}$. Therefore,\n$$\nI_S(\\theta) = (\\mu'_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu'_S(\\theta)\n$$\nThis is the expression for the Fisher information carried by the subset of neurons $S$. For brevity, we will denote $\\mu'(\\theta)$ as $\\mu'$.\n\n### 2. Derivation of the Incremental Information Gain\n\nWe seek a closed-form expression for the incremental gain in Fisher information, $\\Delta_j(S) = I_{S \\cup \\{j\\}}(\\theta) - I_S(\\theta)$, when adding a neuron $j \\notin S$ to an existing set $S$.\n\nLet the full set of indices be reordered for convenience as $(S, j)$. The derivative of the mean response vector and the covariance matrix can be partitioned as:\n$$\n\\mu'_{S \\cup \\{j\\}} = \\begin{pmatrix} \\mu'_S \\\\ \\mu'_j \\end{pmatrix}, \\quad \\Sigma_{S \\cup \\{j\\}, S \\cup \\{j\\}} = \\begin{pmatrix} \\Sigma_{SS}  \\Sigma_{Sj} \\\\ \\Sigma_{jS}  \\Sigma_{jj} \\end{pmatrix}\n$$\nThe information for the set $S \\cup \\{j\\}$ is $I_{S \\cup \\{j\\}} = (\\mu'_{S \\cup \\{j\\}})^T (\\Sigma_{S \\cup \\{j\\}, S \\cup \\{j\\}})^{-1} \\mu'_{S \\cup \\{j\\}}$.\nWe use the formula for the inverse of a block matrix. The Schur complement of the block $\\Sigma_{SS}$ is the scalar $s_j = \\Sigma_{jj} - \\Sigma_{jS} \\Sigma_{SS}^{-1} \\Sigma_{Sj}$. The inverse is given by:\n$$\n(\\Sigma_{S \\cup \\{j\\}, S \\cup \\{j\\}})^{-1} = \\begin{pmatrix} \\Sigma_{SS}^{-1} + \\Sigma_{SS}^{-1}\\Sigma_{Sj}s_j^{-1}\\Sigma_{jS}\\Sigma_{SS}^{-1}  -\\Sigma_{SS}^{-1}\\Sigma_{Sj}s_j^{-1} \\\\ -s_j^{-1}\\Sigma_{jS}\\Sigma_{SS}^{-1}  s_j^{-1} \\end{pmatrix}\n$$\nSubstituting this into the expression for $I_{S \\cup \\{j\\}}$:\n$$\nI_{S \\cup \\{j\\}} = \\begin{pmatrix} (\\mu'_S)^T  \\mu'_j \\end{pmatrix} \\begin{pmatrix} \\Sigma_{SS}^{-1} + \\dots  \\dots \\\\ \\dots  s_j^{-1} \\end{pmatrix} \\begin{pmatrix} \\mu'_S \\\\ \\mu'_j \\end{pmatrix}\n$$\nExpanding this quadratic form yields:\n$$\nI_{S \\cup \\{j\\}} = (\\mu'_S)^T(\\Sigma_{SS}^{-1} + \\Sigma_{SS}^{-1}\\Sigma_{Sj}s_j^{-1}\\Sigma_{jS}\\Sigma_{SS}^{-1})\\mu'_S - 2\\mu'_j s_j^{-1}\\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S + (\\mu'_j)^2 s_j^{-1}\n$$\nThe first term expands to $(\\mu'_S)^T \\Sigma_{SS}^{-1} \\mu'_S + (\\mu'_S)^T\\Sigma_{SS}^{-1}\\Sigma_{Sj}s_j^{-1}\\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S$. The leading part is simply $I_S$. The remaining terms constitute the incremental gain $\\Delta_j(S)$:\n$$\n\\Delta_j(S) = s_j^{-1} \\left[ (\\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S)^2 - 2\\mu'_j(\\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S) + (\\mu'_j)^2 \\right]\n$$\nThis simplifies to a squared term:\n$$\n\\Delta_j(S) = \\frac{(\\mu'_j - \\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S)^2}{s_j} = \\frac{(\\mu'_j - \\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S)^2}{\\Sigma_{jj} - \\Sigma_{jS}\\Sigma_{SS}^{-1}\\Sigma_{Sj}}\n$$\nThis expression has a clear statistical interpretation. The denominator, $\\Sigma_{jj} - \\Sigma_{jS}\\Sigma_{SS}^{-1}\\Sigma_{Sj}$, is the conditional variance of the response of neuron $j$ given the responses of neurons in $S$. The term $\\mu'_j - \\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S$ is the derivative of the conditional mean of neuron $j$'s response with respect to $\\theta$. The incremental gain is thus the Fisher information of a single \"residual\" neuron, whose statistics are conditioned on the already selected set $S$. For the base case where $S=\\emptyset$, the formula does not apply directly. In this case, the gain is simply the information of the neuron $j$ by itself: $\\Delta_j(\\emptyset) = I_{\\{j\\}} = (\\mu'_j)^2 / \\Sigma_{jj}$.\n\n### 3. Greedy Algorithm and Principled Analysis\n\nThe problem is to find a set $S$ of size $k$ that maximizes the set function $F(S) = I_S(\\theta)$. This is a combinatorial optimization problem.\n\nA greedy algorithm can be formulated as follows:\n1. Initialize the selected set $S = \\emptyset$.\n2. For $i$ from $1$ to $k$:\n   a. For each neuron $j \\notin S$, compute the incremental gain $\\Delta_j(S)$ using the formula derived above.\n   b. Select the neuron $j^*$ that provides the maximum gain: $j^* = \\arg\\max_{j \\notin S} \\Delta_j(S)$.\n   c. Add the selected neuron to the set: $S \\leftarrow S \\cup \\{j^*\\}$.\n3. Return the final set $S$.\n\n**Optimality of the Greedy Algorithm:**\nThe greedy algorithm is guaranteed to find the optimal solution if the set function $F(S)$ is modular. A function is modular if the marginal gain of adding an element is independent of the context, i.e., $\\Delta_j(S)$ is the same for all sets $S$ not containing $j$.\nLooking at our expression for $\\Delta_j(S)$, this condition holds if the terms involving $S$ vanish. This occurs if $\\Sigma_{jS} = \\mathbf{0}$ for all $j$ and $S$. This is true if and only if the covariance matrix $\\Sigma$ is diagonal. If $\\Sigma$ is diagonal, the neurons are statistically independent (uncorrelated, for a Gaussian model). The total information is simply the sum of individual informations, $I_S = \\sum_{i \\in S} (\\mu'_i)^2/\\Sigma_{ii}$. The greedy algorithm correctly picks the $k$ neurons with the highest individual Fisher information values. Thus, **a sufficient condition for the greedy algorithm to be optimal is that the noise covariance matrix $\\Sigma$ is diagonal.**\n\n**Approximation Guarantee via Submodularity:**\nWhen $\\Sigma$ is not diagonal, the greedy algorithm is not guaranteed to be optimal. However, if the set function $F(S)$ is monotone and submodular, the greedy algorithm provides a solution $S_g$ whose value is at least $(1 - 1/e)$ times the value of the optimal solution $S_{opt}$: $F(S_g) \\ge (1 - 1/e)F(S_{opt})$.\n\nA function is monotone if $F(S) \\le F(T)$ whenever $S \\subseteq T$. Our function $F(S)=I_S(\\theta)$ is monotone because the incremental gain $\\Delta_j(S)$ is non-negative. This is because the numerator is a square and the denominator is a conditional variance, which must be positive for a positive definite $\\Sigma$.\n\nA function is submodular if it exhibits diminishing returns. That is, for any sets $S \\subseteq T$ and any element $j \\notin T$:\n$$\nF(S \\cup \\{j\\}) - F(S) \\ge F(T \\cup \\{j\\}) - F(T)\n$$\nwhich is equivalent to $\\Delta_j(S) \\ge \\Delta_j(T)$. This condition does not hold in general for the Fisher information function $F(S)=I_S(\\theta)$. In some cases, information can be supermodular (synergistic), where adding a neuron to a larger set provides a greater benefit. The precise condition for submodularity for a given problem instance is that the inequality $\\Delta_j(S) \\ge \\Delta_j(T)$ must hold for all $S \\subseteq T \\subseteq \\{0,\\dots,n-1\\} \\setminus \\{j\\}$. This can be verified numerically for small $n$.",
            "answer": "[[[0, 1], [0, 1], 1.32, 1.32, True, True], [[0, 2], [0, 2], 1.64, 1.64, True, False], [[0, 1], [1, 2], 1.019608, 1.28, False, False], [[0, 1], [0, 1], 1.005025, 1.005025, True, True]]"
        }
    ]
}