{
    "hands_on_practices": [
        {
            "introduction": "理解神经元的放电率如何编码信息，始于一个精确的数学工具：费雪信息。第一个练习将通过引导您从泊松模型的对数似然函数出发，根据第一性原理推导单个神经元的费雪信息，从而巩固您的理解。通过将此方法应用于经典的余弦调谐曲线，您将掌握量化任何具有已知响应模型的神经元编码能力的基本技能。",
            "id": "4163146",
            "problem": "记录单个感觉神经元在呈现由角度 $\\theta$（以弧度为单位）参数化的刺激时的活动。在持续时间为 $T$ 的固定观察窗口内，脉冲计数 $K$ 被建模为条件泊松分布，其均值为 $\\mu(\\theta) = T \\lambda(\\theta)$，其中 $\\lambda(\\theta)$ 是依赖于刺激的发放率（单位为脉冲/单位时间）。您感兴趣的是，从第一性原理（似然函数及其导数的定义）出发，利用单次试验的费雪信息（Fisher information）来量化神经响应能多精确地编码 $\\theta$。\n\n从泊松模型的似然函数及其对数似然函数的定义出发，完成以下任务：\n\n1. 推导得分函数 $U(\\theta)$ 和基于负海森矩阵的观测信息，两者均由对数似然函数定义。\n2. 定义费雪信息的两个单次试验经验估计量：一个基于得分的估计量 $J_{s}(\\theta)$，由得分的平方构建；一个基于海森矩阵的估计量 $J_{h}(\\theta)$，由对数似然函数的负海森矩阵构建。\n3. 在标准的正则性条件下（这些条件保证了微分和期望可以互换，并且分布的支撑集不依赖于 $\\theta$），证明 $J_{s}(\\theta)$ 和 $J_{h}(\\theta)$ 的期望（相对于固定 $\\theta$ 下的数据分布）相等，且均等于费雪信息 $I(\\theta)$。\n4. 现在，将其应用于神经编码中广泛使用的余弦调谐模型：\n$$\n\\lambda(\\theta) = r_{0} + r_{1} \\cos\\!\\big(\\theta - \\phi\\big),\n$$\n其中常数 $r_{0} > |r_{1}| > 0$，$\\phi \\in \\mathbb{R}$ 是首选方向。仅使用您在第 $1$–$3$ 部分中建立的推导，提供费雪信息 $I(\\theta)$ 作为 $\\theta$、$T$、$r_{0}$、$r_{1}$ 和 $\\phi$ 的函数的最终闭式解析表达式。\n\n将您的最终答案表示为单个闭式表达式。不需要进行数值评估或四舍五入，最终表达式中也不应包含单位。",
            "solution": "问题陈述经核实具有科学依据、问题适定、客观且完整。它提出了一个计算神经科学中的标准问题，可以使用统计推断的既定原则来解决。我们开始进行解答。\n\n问题陈述指出，在持续时间为 $T$ 的区间内，脉冲计数 $K$ 服从泊松分布，其均值依赖于刺激，为 $\\mu(\\theta) = T\\lambda(\\theta)$。观测到 $k$ 个脉冲的概率质量函数 (PMF) 由下式给出：\n$$\nP(K=k|\\theta) = \\frac{\\mu(\\theta)^{k} \\exp(-\\mu(\\theta))}{k!} = \\frac{(T\\lambda(\\theta))^{k} \\exp(-T\\lambda(\\theta))}{k!}\n$$\n因此，对数似然函数 $l(\\theta; k) = \\ln P(K=k|\\theta)$ 为：\n$$\nl(\\theta; k) = k \\ln(T\\lambda(\\theta)) - T\\lambda(\\theta) - \\ln(k!)\n$$\n我们可以通过分离依赖于 $\\theta$ 的项来简化这个表达式：\n$$\nl(\\theta; k) = k \\ln(\\lambda(\\theta)) + k \\ln(T) - T\\lambda(\\theta) - \\ln(k!)\n$$\n\n**1. 得分函数和观测信息的推导**\n\n得分函数 $U(\\theta)$ 是对数似然函数关于参数 $\\theta$ 的一阶导数。\n$$\nU(\\theta) = \\frac{d}{d\\theta} l(\\theta; k)\n$$\n对 $l(\\theta; k)$ 关于 $\\theta$ 求导，并使用链式法则，其中 $\\lambda'(\\theta) = \\frac{d\\lambda(\\theta)}{d\\theta}$：\n$$\nU(\\theta) = \\frac{d}{d\\theta} \\left( k \\ln(\\lambda(\\theta)) - T\\lambda(\\theta) + \\text{const.} \\right) = k \\frac{\\lambda'(\\theta)}{\\lambda(\\theta)} - T\\lambda'(\\theta)\n$$\n将 $\\lambda'(\\theta)$ 因子提出，我们得到得分函数：\n$$\nU(\\theta) = \\left( \\frac{k}{\\lambda(\\theta)} - T \\right) \\lambda'(\\theta)\n$$\n观测信息定义为对数似然函数的海森矩阵的负数，即 $-H(\\theta) = -\\frac{d^2}{d\\theta^2} l(\\theta; k)$。我们使用乘法法则对得分函数 $U(\\theta)$ 关于 $\\theta$ 求导：\n$$\nH(\\theta) = \\frac{d}{d\\theta} U(\\theta) = \\frac{d}{d\\theta} \\left[ \\left( \\frac{k}{\\lambda(\\theta)} - T \\right) \\lambda'(\\theta) \\right]\n$$\n$$\nH(\\theta) = \\left[ \\frac{d}{d\\theta} \\left( \\frac{k}{\\lambda(\\theta)} - T \\right) \\right] \\lambda'(\\theta) + \\left( \\frac{k}{\\lambda(\\theta)} - T \\right) \\left[ \\frac{d}{d\\theta} \\lambda'(\\theta) \\right]\n$$\n第一项的导数是 $-\\frac{k \\lambda'(\\theta)}{\\lambda(\\theta)^2}$。第二项的导数是 $\\lambda''(\\theta)$。将它们代入：\n$$\nH(\\theta) = \\left( -\\frac{k \\lambda'(\\theta)}{\\lambda(\\theta)^2} \\right) \\lambda'(\\theta) + \\left( \\frac{k}{\\lambda(\\theta)} - T \\right) \\lambda''(\\theta)\n$$\n$$\nH(\\theta) = -\\frac{k (\\lambda'(\\theta))^2}{\\lambda(\\theta)^2} + \\left( \\frac{k}{\\lambda(\\theta)} - T \\right) \\lambda''(\\theta)\n$$\n基于负海森矩阵的观测信息是：\n$$\n-H(\\theta) = \\frac{k (\\lambda'(\\theta))^2}{\\lambda(\\theta)^2} - \\left( \\frac{k}{\\lambda(\\theta)} - T \\right) \\lambda''(\\theta)\n$$\n\n**2. 费雪信息经验估计量的定义**\n\n基于第 1 部分的推导，我们定义费雪信息的两个单次试验经验估计量。\n基于得分的估计量 $J_s(\\theta)$ 是得分函数的平方：\n$$\nJ_{s}(\\theta) = (U(\\theta))^2 = \\left[ \\left( \\frac{k}{\\lambda(\\theta)} - T \\right) \\lambda'(\\theta) \\right]^2 = \\left( \\frac{k}{\\lambda(\\theta)} - T \\right)^2 (\\lambda'(\\theta))^2\n$$\n基于海森矩阵的估计量 $J_h(\\theta)$ 是对数似然函数的海森矩阵的负数：\n$$\nJ_{h}(\\theta) = -H(\\theta) = \\frac{k (\\lambda'(\\theta))^2}{\\lambda(\\theta)^2} - \\left( \\frac{k}{\\lambda(\\theta)} - T \\right) \\lambda''(\\theta)\n$$\n\n**3. 估计量期望的等价性**\n\n我们现在证明，两个估计量关于 P($K=k|\\theta$) 的期望都等于费雪信息 $I(\\theta)$。期望算子 $E[\\cdot]$ 是对随机变量 $K$ 取的，其中 $E[K] = \\mu(\\theta) = T\\lambda(\\theta)$ 且 $\\text{Var}(K) = \\mu(\\theta) = T\\lambda(\\theta)$。\n\n首先，我们来求基于得分的估计量的期望 $E[J_s(\\theta)]$。\n$$\nE[J_s(\\theta)] = E \\left[ \\left( \\frac{K}{\\lambda(\\theta)} - T \\right)^2 (\\lambda'(\\theta))^2 \\right]\n$$\n项 $(\\lambda'(\\theta))^2$ 相对于 $K$ 是常数，因此我们可以将其因子提出：\n$$\nE[J_s(\\theta)] = (\\lambda'(\\theta))^2 E \\left[ \\left( \\frac{K}{\\lambda(\\theta)} - T \\right)^2 \\right] = (\\lambda'(\\theta))^2 \\frac{1}{\\lambda(\\theta)^2} E \\left[ (K - T\\lambda(\\theta))^2 \\right]\n$$\n项 $E[(K - T\\lambda(\\theta))^2] = E[(K - E[K])^2]$ 是 $K$ 的方差 $\\text{Var}(K)$ 的定义。对于泊松分布，$\\text{Var}(K) = T\\lambda(\\theta)$。\n$$\nE[J_s(\\theta)] = \\frac{(\\lambda'(\\theta))^2}{\\lambda(\\theta)^2} \\text{Var}(K) = \\frac{(\\lambda'(\\theta))^2}{\\lambda(\\theta)^2} (T\\lambda(\\theta)) = T \\frac{(\\lambda'(\\theta))^2}{\\lambda(\\theta)}\n$$\n这是费雪信息的一种形式。在指定的正则性条件下，费雪信息定义为 $I(\\theta) = E[U(\\theta)^2]$。\n\n接下来，我们来求基于海森矩阵的估计量的期望 $E[J_h(\\theta)]$。\n$$\nE[J_h(\\theta)] = E \\left[ \\frac{K (\\lambda'(\\theta))^2}{\\lambda(\\theta)^2} - \\left( \\frac{K}{\\lambda(\\theta)} - T \\right) \\lambda''(\\theta) \\right]\n$$\n使用期望的线性性质：\n$$\nE[J_h(\\theta)] = E \\left[ \\frac{K (\\lambda'(\\theta))^2}{\\lambda(\\theta)^2} \\right] - E \\left[ \\left( \\frac{K}{\\lambda(\\theta)} - T \\right) \\lambda''(\\theta) \\right]\n$$\n对于第一项，我们提出常数因子：\n$$\nE \\left[ \\frac{K (\\lambda'(\\theta))^2}{\\lambda(\\theta)^2} \\right] = \\frac{(\\lambda'(\\theta))^2}{\\lambda(\\theta)^2} E[K] = \\frac{(\\lambda'(\\theta))^2}{\\lambda(\\theta)^2} (T\\lambda(\\theta)) = T \\frac{(\\lambda'(\\theta))^2}{\\lambda(\\theta)}\n$$\n对于第二项，我们提出常数因子：\n$$\nE \\left[ \\left( \\frac{K}{\\lambda(\\theta)} - T \\right) \\lambda''(\\theta) \\right] = \\lambda''(\\theta) E \\left[ \\frac{K}{\\lambda(\\theta)} - T \\right] = \\lambda''(\\theta) \\left( \\frac{E[K]}{\\lambda(\\theta)} - T \\right)\n$$\n代入 $E[K] = T\\lambda(\\theta)$：\n$$\n\\lambda''(\\theta) \\left( \\frac{T\\lambda(\\theta)}{\\lambda(\\theta)} - T \\right) = \\lambda''(\\theta) (T - T) = 0\n$$\n这个结果证实了得分函数的期望为零，即 $E[U(\\theta)] = \\lambda'(\\theta) E[\\frac{K}{\\lambda(\\theta)} - T] = 0$，这是正则性条件下的一个关键性质。\n结合 $E[J_h(\\theta)]$ 的两项结果：\n$$\nE[J_h(\\theta)] = T \\frac{(\\lambda'(\\theta))^2}{\\lambda(\\theta)} - 0 = T \\frac{(\\lambda'(\\theta))^2}{\\lambda(\\theta)}\n$$\n因此，我们证明了 $E[J_s(\\theta)] = E[J_h(\\theta)] = T \\frac{(\\lambda'(\\theta))^2}{\\lambda(\\theta)}$。这个量就是费雪信息 $I(\\theta)$。\n\n**4. 余弦调谐模型的费雪信息**\n\n给定发放率的余弦调谐模型：\n$$\n\\lambda(\\theta) = r_0 + r_1 \\cos(\\theta - \\phi)\n$$\n问题指定 $r_0 > |r_1| > 0$，这确保了 $\\lambda(\\theta)$ 始终为正。为了求出费雪信息 $I(\\theta)$，我们使用第 3 部分推导出的通用公式：\n$$\nI(\\theta) = T \\frac{(\\lambda'(\\theta))^2}{\\lambda(\\theta)}\n$$\n首先，我们计算导数 $\\lambda'(\\theta)$：\n$$\n\\lambda'(\\theta) = \\frac{d}{d\\theta} \\left( r_0 + r_1 \\cos(\\theta - \\phi) \\right) = 0 + r_1 \\left( -\\sin(\\theta - \\phi) \\cdot 1 \\right) = -r_1 \\sin(\\theta - \\phi)\n$$\n接下来，我们将导数平方：\n$$\n(\\lambda'(\\theta))^2 = (-r_1 \\sin(\\theta - \\phi))^2 = r_1^2 \\sin^2(\\theta - \\phi)\n$$\n最后，我们将 $\\lambda(\\theta)$ 和 $(\\lambda'(\\theta))^2$ 代入 $I(\\theta)$ 的公式中：\n$$\nI(\\theta) = T \\frac{r_1^2 \\sin^2(\\theta - \\phi)}{r_0 + r_1 \\cos(\\theta - \\phi)}\n$$\n这就是作为模型参数函数的费雪信息的最终闭式解析表达式。",
            "answer": "$$\n\\boxed{T \\frac{r_{1}^{2} \\sin^{2}(\\theta - \\phi)}{r_{0} + r_{1} \\cos(\\theta - \\phi)}}\n$$"
        },
        {
            "introduction": "虽然泊松模型是一个有用的起点，但真实的神经反应通常表现出比其均值更大的可变性，这一现象被称为过度离散（overdispersion）。本练习超越了理想化的泊松模型，要求您为一个由负二项分布描述的神经元计算费雪信息。这项练习对于学习如何调整理论工具以适应关键的生物学现实（例如非泊松尖峰统计特性），并将费雪信息与法诺因子等经验可测量的量联系起来至关重要。",
            "id": "4163157",
            "problem": "在一个持续时间为 $T$ 的固定观察窗口内，单个神经元编码一个标量刺激参数 $\\theta$。该神经元的条件平均脉冲计数为 $\\mu(\\theta) = T\\,\\lambda(\\theta)$，其中 $\\lambda(\\theta) > 0$ 是一个可微的调谐曲线。根据经验，脉冲计数相对于泊松过程是过度离散的，其法诺因子 (FF) 为 $F(\\theta) \\equiv \\frac{\\operatorname{Var}[N \\mid \\theta]}{\\mathbb{E}[N \\mid \\theta]} > 1$。假设在给定 $\\theta$ 的条件下，脉冲计数 $N \\in \\{0,1,2,\\dots\\}$ 服从一个负二项 (NB) 模型，其均值为 $\\mu(\\theta)$，离散参数 $\\kappa > 0$ 为已知且不依赖于 $\\theta$。使用负二项概率质量函数的以下参数化形式：\n$$\n\\Pr(N=n \\mid \\mu,\\kappa) \\;=\\; \\frac{\\Gamma(n+\\kappa)}{\\Gamma(\\kappa)\\,n!}\\,\\left(\\frac{\\kappa}{\\kappa+\\mu}\\right)^{\\kappa}\\,\\left(\\frac{\\mu}{\\kappa+\\mu}\\right)^{n}, \\quad n=0,1,2,\\dots\n$$\n其均值为 $\\mathbb{E}[N\\mid \\mu,\\kappa] = \\mu$，方差为 $\\operatorname{Var}[N\\mid \\mu,\\kappa] = \\mu + \\frac{\\mu^{2}}{\\kappa}$。\n\n从标量参数的费雪信息 (FI) 的定义出发，\n$$\n\\mathcal{J}(\\theta) \\;=\\; \\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\ln p(N\\mid \\theta)\\right)^{2} \\Bigm| \\theta \\right] \\;=\\; -\\,\\mathbb{E}\\!\\left[\\frac{\\partial^{2}}{\\partial \\theta^{2}}\\ln p(N\\mid \\theta) \\Bigm| \\theta \\right],\n$$\n推导在上述负二项模型下，$N$ 中包含的关于 $\\theta$ 的费雪信息 $\\mathcal{J}(\\theta)$ 的闭式表达式。然后将你的结果完全用法诺因子 $F(\\theta)$、均值 $\\mu(\\theta)$ 和导数 $\\mu'(\\theta) \\equiv \\frac{d\\mu}{d\\theta}$ 来表示。\n\n给出你最终的答案，形式为一个单一的简化解析表达式。不需要数值计算，也不需要单位。",
            "solution": "首先验证问题以确保其具有科学依据、良构且客观。\n\n### 步骤 1：提取已知条件\n- 刺激参数：$\\theta$\n- 条件平均脉冲计数：$\\mu(\\theta) = T\\lambda(\\theta)$，其中 $\\lambda(\\theta) > 0$ 且可微。\n- 法诺因子：$F(\\theta) = \\frac{\\operatorname{Var}[N \\mid \\theta]}{\\mathbb{E}[N \\mid \\theta]} > 1$\n- 脉冲计数分布：负二项 (NB) 模型，适用于 $N \\in \\{0,1,2,\\dots\\}$。\n- 负二项概率质量函数 (PMF)：$p(N=n \\mid \\mu,\\kappa) = \\frac{\\Gamma(n+\\kappa)}{\\Gamma(\\kappa)\\,n!}\\,\\left(\\frac{\\kappa}{\\kappa+\\mu}\\right)^{\\kappa}\\,\\left(\\frac{\\mu}{\\kappa+\\mu}\\right)^{n}$\n- 负二项均值：$\\mathbb{E}[N\\mid \\mu,\\kappa] = \\mu$\n- 负二项方差：$\\operatorname{Var}[N\\mid \\mu,\\kappa] = \\mu + \\frac{\\mu^{2}}{\\kappa}$\n- 离散参数：$\\kappa > 0$，相对于 $\\theta$ 是常数。\n- 费雪信息 (FI) 定义：$\\mathcal{J}(\\theta) = \\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\ln p(N\\mid \\theta)\\right)^{2} \\Bigm| \\theta \\right] = -\\,\\mathbb{E}\\!\\left[\\frac{\\partial^{2}}{\\partial \\theta^{2}}\\ln p(N\\mid \\theta) \\Bigm| \\theta \\right]$\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题在科学上是合理的。使用负二项分布来模拟过度离散的脉冲计数（法诺因子 $>1$）是计算神经科学中的一种标准做法。费雪信息是量化参数估计精度的基本概念。所提供的负二项分布的参数化形式及其矩和费雪信息的定义都是正确且标准的。问题是自洽的，提供了推导所需表达式的所有必要信息。问题是客观且良构的。\n\n### 步骤 3：结论与行动\n问题是有效的。我们开始进行推导。\n\n### 推导\n我们的目标是推导费雪信息 $\\mathcal{J}(\\theta)$ 的表达式，并用平均脉冲计数 $\\mu(\\theta)$、其导数 $\\mu'(\\theta)$ 和法诺因子 $F(\\theta)$ 来表示。\n\n让我们从基于所提供 PMF 的对数似然函数 $\\ln p(N \\mid \\theta)$ 开始。在整个推导过程中，为简化符号，我们将 $\\mu(\\theta)$ 记为 $\\mu$，其导数 $\\frac{d\\mu}{d\\theta}$ 记为 $\\mu'$。\n$$\n\\ln p(N \\mid \\theta) = \\ln\\left(\\frac{\\Gamma(N+\\kappa)}{\\Gamma(\\kappa)\\,N!}\\right) + \\kappa \\ln\\left(\\frac{\\kappa}{\\kappa+\\mu}\\right) + N \\ln\\left(\\frac{\\mu}{\\kappa+\\mu}\\right)\n$$\n我们可以将涉及 $\\mu$ 的对数项重写为：\n$$\n\\ln p(N \\mid \\theta) = \\ln\\left(\\frac{\\Gamma(N+\\kappa)}{\\Gamma(\\kappa)\\,N!}\\right) + \\kappa(\\ln\\kappa - \\ln(\\kappa+\\mu)) + N(\\ln\\mu - \\ln(\\kappa+\\mu))\n$$\n第一项和 $\\kappa\\ln\\kappa$ 不依赖于 $\\theta$。\n\n现在，我们计算得分（score），即对数似然函数关于 $\\theta$ 的一阶偏导数。由于 $\\mu$ 是 $\\theta$ 的函数，我们使用链式法则。\n$$\n\\frac{\\partial}{\\partial \\theta} \\ln p(N \\mid \\theta) = \\left( \\frac{\\partial}{\\partial \\mu} \\ln p(N \\mid \\mu) \\right) \\frac{d\\mu}{d\\theta}\n$$\n首先，我们计算关于 $\\mu$ 的导数：\n$$\n\\frac{\\partial}{\\partial \\mu} \\ln p(N \\mid \\mu) = -\\kappa \\frac{1}{\\kappa+\\mu} + N \\frac{1}{\\mu} - N \\frac{1}{\\kappa+\\mu} = \\frac{N}{\\mu} - \\frac{N+\\kappa}{\\kappa+\\mu}\n$$\n将这些项通分合并：\n$$\n\\frac{\\partial}{\\partial \\mu} \\ln p(N \\mid \\mu) = \\frac{N(\\kappa+\\mu) - \\mu(N+\\kappa)}{\\mu(\\kappa+\\mu)} = \\frac{N\\kappa+N\\mu-N\\mu-\\mu\\kappa}{\\mu(\\kappa+\\mu)} = \\frac{\\kappa(N-\\mu)}{\\mu(\\kappa+\\mu)}\n$$\n因此，得分函数为：\n$$\n\\frac{\\partial}{\\partial \\theta} \\ln p(N \\mid \\theta) = \\frac{\\kappa(N-\\mu)}{\\mu(\\kappa+\\mu)} \\mu'\n$$\n费雪信息定义为得分平方的期望值：\n$$\n\\mathcal{J}(\\theta) = \\mathbb{E}\\left[ \\left(\\frac{\\partial}{\\partial \\theta}\\ln p(N\\mid \\theta)\\right)^{2} \\biggm| \\theta \\right] = \\mathbb{E}\\left[ \\left( \\frac{\\kappa(N-\\mu)}{\\mu(\\kappa+\\mu)} \\mu' \\right)^2 \\biggm| \\theta \\right]\n$$\n不依赖于随机变量 $N$ 的项可以从期望中提出来：\n$$\n\\mathcal{J}(\\theta) = \\left( \\frac{\\kappa \\mu'}{\\mu(\\kappa+\\mu)} \\right)^2 \\mathbb{E}\\left[ (N-\\mu)^2 \\mid \\theta \\right]\n$$\n项 $\\mathbb{E}\\left[ (N-\\mu)^2 \\mid \\theta \\right]$ 是 $N$ 在给定 $\\theta$ 条件下的方差，记为 $\\operatorname{Var}[N \\mid \\theta]$。\n$$\n\\mathcal{J}(\\theta) = \\frac{\\kappa^2 (\\mu')^2}{\\mu^2(\\kappa+\\mu)^2} \\operatorname{Var}[N \\mid \\theta]\n$$\n问题陈述了负二项分布的方差为 $\\operatorname{Var}[N \\mid \\mu, \\kappa] = \\mu + \\frac{\\mu^2}{\\kappa}$。我们可以简化这个表达式：\n$$\n\\operatorname{Var}[N \\mid \\theta] = \\mu + \\frac{\\mu^2}{\\kappa} = \\frac{\\mu\\kappa + \\mu^2}{\\kappa} = \\frac{\\mu(\\kappa+\\mu)}{\\kappa}\n$$\n将此代入我们关于 $\\mathcal{J}(\\theta)$ 的表达式中：\n$$\n\\mathcal{J}(\\theta) = \\frac{\\kappa^2 (\\mu')^2}{\\mu^2(\\kappa+\\mu)^2} \\left( \\frac{\\mu(\\kappa+\\mu)}{\\kappa} \\right)\n$$\n我们可以消去分子和分母中的公因子：\n$$\n\\mathcal{J}(\\theta) = \\frac{\\kappa (\\mu')^2}{\\mu(\\kappa+\\mu)}\n$$\n这是用 $\\mu$、$\\mu'$ 和离散参数 $\\kappa$ 表示的费雪信息。最后一步是将其用法诺因子 $F(\\theta)$ 来表示。\n法诺因子由下式给出：\n$$\nF(\\theta) = \\frac{\\operatorname{Var}[N \\mid \\theta]}{\\mathbb{E}[N \\mid \\theta]} = \\frac{\\mu + \\frac{\\mu^2}{\\kappa}}{\\mu} = 1 + \\frac{\\mu}{\\kappa}\n$$\n我们从此方程中解出 $\\kappa$：\n$$\nF(\\theta) - 1 = \\frac{\\mu}{\\kappa} \\implies \\kappa = \\frac{\\mu}{F(\\theta)-1}\n$$\n因为问题陈述了 $F(\\theta) > 1$，所以 $\\kappa$ 是良定义且为正的。现在，我们将这个 $\\kappa$ 的表达式代入我们关于 $\\mathcal{J}(\\theta)$ 的方程。首先重排 $\\mathcal{J}(\\theta)$ 的表达式会很方便：\n$$\n\\mathcal{J}(\\theta) = \\frac{(\\mu')^2}{\\mu} \\cdot \\frac{\\kappa}{\\kappa+\\mu}\n$$\n让我们使用 $\\kappa$ 的表达式来计算 $\\frac{\\kappa}{\\kappa+\\mu}$ 这一项：\n$$\n\\frac{\\kappa}{\\kappa+\\mu} = \\frac{\\frac{\\mu}{F(\\theta)-1}}{\\frac{\\mu}{F(\\theta)-1} + \\mu} = \\frac{\\frac{\\mu}{F(\\theta)-1}}{\\mu\\left(\\frac{1}{F(\\theta)-1} + 1\\right)} = \\frac{\\frac{1}{F(\\theta)-1}}{\\frac{1 + (F(\\theta)-1)}{F(\\theta)-1}} = \\frac{\\frac{1}{F(\\theta)-1}}{\\frac{F(\\theta)}{F(\\theta)-1}} = \\frac{1}{F(\\theta)}\n$$\n将这个简化后的项代回 $\\mathcal{J}(\\theta)$ 的表达式中，我们得到最终结果：\n$$\n\\mathcal{J}(\\theta) = \\frac{(\\mu')^2}{\\mu} \\cdot \\frac{1}{F(\\theta)} = \\frac{(\\mu'(\\theta))^2}{\\mu(\\theta)F(\\theta)}\n$$\n这个表达式将费雪信息与平均响应的灵敏度（$\\mu'$）、平均响应的量级（$\\mu$）以及由法诺因子（$F$）所捕捉的过度离散水平联系起来。",
            "answer": "$$\n\\boxed{\\frac{(\\mu'(\\theta))^{2}}{\\mu(\\theta) F(\\theta)}}\n$$"
        },
        {
            "introduction": "大脑中的信息是由大量的神经元群体处理的，而不是孤立的单个细胞。最后一个综合性练习将从单神经元分析过渡到群体编码的复杂性，其中噪声相关性扮演着关键角色。您将推导神经元群体的费雪信息，设计一种贪心算法来选择信息量最大的神经元子集，并用代码实现它。这个练习旨在弥合抽象理论与实际计算神经科学之间的鸿沟，解决分析大规模神经记录中的一个核心问题。",
            "id": "4163134",
            "problem": "给定一个大小为 $n$ 的神经元群体，其在固定刺激参数 $\\theta$ 下的刺激敏感性和噪声统计数据已通过实验测量。具体来说，平均响应相对于 $\\theta$ 的导数是一个向量 $\\mu'(\\theta) \\in \\mathbb{R}^n$，噪声协方差是一个不依赖于 $\\theta$ 的正定矩阵 $\\Sigma \\in \\mathbb{R}^{n \\times n}$。假设在给定 $\\theta$ 的条件下，神经元响应的条件分布是均值为 $\\mu(\\theta)$、协方差为 $\\Sigma$ 的多元正态分布。目标是选择一个包含 $k$ 个神经元的子集，以最大化该子集所携带的关于 $\\theta$ 的标量 Fisher 信息。\n\n仅从核心定义和经过充分检验的公式出发，按如下步骤进行。\n\n1. 从均值为 $\\mu(\\theta)$ 且协方差 $\\Sigma$ 与 $\\theta$ 无关的多元正态模型的对数似然定义出发，并结合标量参数的 Fisher 信息定义，推导由所选神经元子集 $S \\subseteq \\{0,1,\\dots,n-1\\}$ 携带的 Fisher 信息的表达式。请用子向量 $\\mu'_S(\\theta)$ 和主子矩阵 $\\Sigma_{SS}$ 来表示此 Fisher 信息。\n\n2. 仅使用矩阵恒等式和分块矩阵的结构，推导当向当前选择 $S$ 中添加一个新神经元 $j \\notin S$ 时，Fisher 信息增量的闭式表达式。你的表达式必须用 $\\mu'(\\theta)$、$\\Sigma$ 和已选集合 $S$ 来表示，并且其计算方式不能访问 $S \\cup \\{j\\}$ 之外的神经元响应。你的推导不能假设 $\\Sigma$ 除了正定性之外有任何特殊结构。\n\n3. 提出了一个贪心选择算法，该算法从空集开始，迭代地添加能最大化 Fisher 信息增量的神经元，直到选出 $k$ 个神经元为止。请对该贪心算法保证最优的条件进行原则性分析。陈述一个关于 $\\Sigma$ 的充分条件，在该条件下集合函数是模函数；并为一般的 $\\Sigma$ 制定一个精确的收益递减（子模性）条件，在该条件下，贪心算法能达到在基数约束下单调子模最大化的标准近似保证。\n\n4. 实现一个完整的程序，该程序能够：\n   - 为任意子集 $S$ 计算 Fisher 信息。\n   - 根据你推导的表达式，计算将 $j$ 添加到 $S$ 时的增量收益。\n   - 实现贪心算法以选择 $k$ 个神经元。\n   - 通过穷举搜索计算大小为 $k$ 的最优子集，以验证贪心解在 $n$ 较小情况下的正确性。\n   - 通过检查所有 $S \\subseteq T \\subseteq \\{0,\\dots,n-1\\}$ 和所有 $j \\notin T$ 是否满足“将 $j$ 添加到 $S$ 的增量收益大于或等于将 $j$ 添加到 $T$ 的增量收益”，在小的数值容差范围内，数值上验证收益递减条件。\n\n5. 对于以下每个测试案例，运行你的实现，并为每个案例输出一个列表，其中包含：\n   - 贪心算法选择的索引，以整数列表形式表示（从零开始索引）。\n   - 通过穷举搜索找到的大小为 $k$ 的最优子集的索引，以整数列表形式表示（从零开始索引）。\n   - 贪心选择的 Fisher 信息值（浮点数）。\n   - 最优选择的 Fisher 信息值（浮点数）。\n   - 一个布尔值，指示贪心选择是否达到最优 Fisher 信息。\n   - 一个布尔值，指示此实例是否在数值容差范围内满足收益递减（子模性）条件。\n\n   使用以下测试套件。下面所有数字均表示无量纲量。\n   - 案例 A：$n = 4$, $k = 2$, $\\mu'(\\theta) = [1.0, 0.8, 0.3, 0.1]$, $\\Sigma = \\mathrm{diag}([1.0, 2.0, 0.5, 0.2])$。\n   - 案例 B：$n = 3$, $k = 2$, $\\mu'(\\theta) = [1.0, 0.9, 0.8]$, $\\Sigma = \\begin{bmatrix} 1.0 & 0.9 & 0.0 \\\\ 0.9 & 1.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 \\end{bmatrix}$。\n   - 案例 C：$n = 3$, $k = 2$, $\\mu'(\\theta) = [1.0, 0.8, 0.8]$, $\\Sigma = \\begin{bmatrix} 1.0 & 0.7 & 0.7 \\\\ 0.7 & 1.0 & 0.0 \\\\ 0.7 & 0.0 & 1.0 \\end{bmatrix}$。\n   - 案例 D：$n = 3$, $k = 2$, $\\mu'(\\theta) = [1.0, 1.0, 1.0]$, $\\Sigma = \\begin{bmatrix} 1.0 & 0.99 & 0.99 \\\\ 0.99 & 1.0 & 0.99 \\\\ 0.99 & 0.99 & 1.0 \\end{bmatrix}$。\n\n你的程序应生成单行输出，其中包含四个案例的结果，结果是一个用方括号括起来的逗号分隔列表，每个案例自身的结果也如上所述是一个列表。例如，格式应类似于“[[greedy_indices_case_A,optimal_indices_case_A,greedy_FI_case_A,optimal_FI_case_A,is_optimal_case_A,is_submodular_case_A],[...case_B...],[...case_C...],[...case_D...]]”。不应打印任何额外文本。",
            "solution": "选择一个最优的神经元子集以最大化 Fisher 信息是神经元群体编码分析中的一项核心任务。\n给定一个包含 $n$ 个神经元的群体，其平均响应的刺激敏感性为 $\\mu'(\\theta) \\in \\mathbb{R}^n$，以及与刺激无关的噪声协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{n \\times n}$。神经元子集 $S$ 的响应由多元正态分布 $\\mathbf{r}_S \\sim \\mathcal{N}(\\mu_S(\\theta), \\Sigma_{SS})$ 建模。我们将通过推导必要的理论表达式，分析由此产生的优化问题的性质，然后实现一个解决方案。\n\n### 1. 神经元子集的 Fisher 信息推导\n\n对于单个标量参数 $\\theta$，Fisher 信息的定义为 $I(\\theta) = -E\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\log p(\\mathbf{r}|\\theta)\\right]$。令 $S \\subseteq \\{0, 1, \\dots, n-1\\}$ 为所选神经元子集的索引集。该子集的响应向量为 $\\mathbf{r}_S$，其概率密度函数是均值为 $\\mu_S(\\theta)$、协方差为 $\\Sigma_{SS}$ 的多元正态分布，其中 $|S|$ 是子集中的神经元数量。\n\n对数似然函数为：\n$$\n\\log p(\\mathbf{r}_S|\\theta) = -\\frac{|S|}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det(\\Sigma_{SS}) - \\frac{1}{2}(\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} (\\mathbf{r}_S - \\mu_S(\\theta))\n$$\n由于协方差矩阵 $\\Sigma$（以及任何主子矩阵 $\\Sigma_{SS}$）被假定为与 $\\theta$ 无关，我们只需要对最后一项关于 $\\theta$ 求导。令 $\\mu'_S(\\theta) = \\frac{\\partial \\mu_S(\\theta)}{\\partial\\theta}$。\n对数似然的一阶导数为：\n$$\n\\frac{\\partial}{\\partial \\theta} \\log p(\\mathbf{r}_S|\\theta) = - \\frac{1}{2} \\frac{\\partial}{\\partial \\theta} \\left[ (\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} (\\mathbf{r}_S - \\mu_S(\\theta)) \\right]\n$$\n使用链式法则和标准向量微积分恒等式，上式变为：\n$$\n\\frac{\\partial}{\\partial \\theta} \\log p(\\mathbf{r}_S|\\theta) = (\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu'_S(\\theta)\n$$\n关于 $\\theta$ 的二阶导数为：\n$$\n\\frac{\\partial^2}{\\partial \\theta^2} \\log p(\\mathbf{r}_S|\\theta) = \\frac{\\partial}{\\partial \\theta} \\left[ (\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu'_S(\\theta) \\right] = -(\\mu'_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu'_S(\\theta) + (\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu''_S(\\theta)\n$$\n其中 $\\mu''_S(\\theta) = \\frac{\\partial^2 \\mu_S(\\theta)}{\\partial\\theta^2}$。为了求得 Fisher 信息，我们取负期望：\n$$\nI_S(\\theta) = -E\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\log p(\\mathbf{r}_S|\\theta)\\right] = -E\\left[-(\\mu'_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu'_S(\\theta)\\right] - E\\left[(\\mathbf{r}_S - \\mu_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu''_S(\\theta)\\right]\n$$\n第一项相对于随机变量 $\\mathbf{r}_S$ 是一个常数。第二项的期望为零，因为 $E[\\mathbf{r}_S - \\mu_S(\\theta)] = \\mathbf{0}$。因此，\n$$\nI_S(\\theta) = (\\mu'_S(\\theta))^T \\Sigma_{SS}^{-1} \\mu'_S(\\theta)\n$$\n这就是由神经元子集 $S$ 携带的 Fisher 信息的表达式。为简洁起见，我们将 $\\mu'(\\theta)$ 记为 $\\mu'$。\n\n### 2. 增量信息收益的推导\n\n我们寻求当向一个现有集合 $S$ 中添加一个神经元 $j \\notin S$ 时，Fisher 信息增量 $\\Delta_j(S) = I_{S \\cup \\{j\\}}(\\theta) - I_S(\\theta)$ 的闭式表达式。\n\n为方便起见，将全索引集重排序为 $(S, j)$。平均响应向量的导数和协方差矩阵可以分块表示为：\n$$\n\\mu'_{S \\cup \\{j\\}} = \\begin{pmatrix} \\mu'_S \\\\ \\mu'_j \\end{pmatrix}, \\quad \\Sigma_{S \\cup \\{j\\}, S \\cup \\{j\\}} = \\begin{pmatrix} \\Sigma_{SS}  \\Sigma_{Sj} \\\\ \\Sigma_{jS}  \\Sigma_{jj} \\end{pmatrix}\n$$\n集合 $S \\cup \\{j\\}$ 的信息为 $I_{S \\cup \\{j\\}} = (\\mu'_{S \\cup \\{j\\}})^T (\\Sigma_{S \\cup \\{j\\}, S \\cup \\{j\\}})^{-1} \\mu'_{S \\cup \\{j\\}}$。\n我们使用分块矩阵的逆公式。块 $\\Sigma_{SS}$ 的 Schur 补是一个标量 $s_j = \\Sigma_{jj} - \\Sigma_{jS} \\Sigma_{SS}^{-1} \\Sigma_{Sj}$。其逆矩阵由下式给出：\n$$\n(\\Sigma_{S \\cup \\{j\\}, S \\cup \\{j\\}})^{-1} = \\begin{pmatrix} \\Sigma_{SS}^{-1} + \\Sigma_{SS}^{-1}\\Sigma_{Sj}s_j^{-1}\\Sigma_{jS}\\Sigma_{SS}^{-1}  -\\Sigma_{SS}^{-1}\\Sigma_{Sj}s_j^{-1} \\\\ -s_j^{-1}\\Sigma_{jS}\\Sigma_{SS}^{-1}  s_j^{-1} \\end{pmatrix}\n$$\n将此代入 $I_{S \\cup \\{j\\}}$ 的表达式中：\n$$\nI_{S \\cup \\{j\\}} = \\begin{pmatrix} (\\mu'_S)^T  \\mu'_j \\end{pmatrix} \\begin{pmatrix} \\Sigma_{SS}^{-1} + \\dots  \\dots \\\\ \\dots  s_j^{-1} \\end{pmatrix} \\begin{pmatrix} \\mu'_S \\\\ \\mu'_j \\end{pmatrix}\n$$\n展开这个二次型可得：\n$$\nI_{S \\cup \\{j\\}} = (\\mu'_S)^T(\\Sigma_{SS}^{-1} + \\Sigma_{SS}^{-1}\\Sigma_{Sj}s_j^{-1}\\Sigma_{jS}\\Sigma_{SS}^{-1})\\mu'_S - 2\\mu'_j s_j^{-1}\\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S + (\\mu'_j)^2 s_j^{-1}\n$$\n第一项展开为 $(\\mu'_S)^T \\Sigma_{SS}^{-1} \\mu'_S + (\\mu'_S)^T\\Sigma_{SS}^{-1}\\Sigma_{Sj}s_j^{-1}\\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S$。其中的首项就是 $I_S$。余下的项构成了增量收益 $\\Delta_j(S)$：\n$$\n\\Delta_j(S) = s_j^{-1} \\left[ (\\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S)^2 - 2\\mu'_j(\\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S) + (\\mu'_j)^2 \\right]\n$$\n这可以简化为一个平方项：\n$$\n\\Delta_j(S) = \\frac{(\\mu'_j - \\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S)^2}{s_j} = \\frac{(\\mu'_j - \\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S)^2}{\\Sigma_{jj} - \\Sigma_{jS}\\Sigma_{SS}^{-1}\\Sigma_{Sj}}\n$$\n该表达式具有明确的统计解释。分母 $\\Sigma_{jj} - \\Sigma_{jS}\\Sigma_{SS}^{-1}\\Sigma_{Sj}$ 是在给定集合 $S$ 中神经元响应的条件下，神经元 $j$ 响应的条件方差。项 $\\mu'_j - \\Sigma_{jS}\\Sigma_{SS}^{-1}\\mu'_S$ 是神经元 $j$ 响应的条件均值对 $\\theta$ 的导数。因此，增量收益是单个“残差”神经元的 Fisher 信息，其统计量是以已选集合 $S$ 为条件的。对于 $S=\\emptyset$ 的基本情况，该公式不直接适用。此时，收益就是神经元 $j$ 本身的信息：$\\Delta_j(\\emptyset) = I_{\\{j\\}} = (\\mu'_j)^2 / \\Sigma_{jj}$。\n\n### 3. 贪心算法和原则性分析\n\n问题在于找到一个大小为 $k$ 的集合 $S$，以最大化集合函数 $F(S) = I_S(\\theta)$。这是一个组合优化问题。\n\n可以按如下方式制定一个贪心算法：\n1. 初始化所选集合 $S = \\emptyset$。\n2. 对于 $i$ 从 $1$ 到 $k$：\n   a. 对每个神经元 $j \\notin S$，使用上面推导的公式计算增量收益 $\\Delta_j(S)$。\n   b. 选择提供最大收益的神经元 $j^*$：$j^* = \\arg\\max_{j \\notin S} \\Delta_j(S)$。\n   c. 将所选神经元添加到集合中：$S \\leftarrow S \\cup \\{j^*\\}$。\n3. 返回最终集合 $S$。\n\n**贪心算法的最优性：**\n如果集合函数 $F(S)$ 具有模性（modular），则贪心算法保证能找到最优解。如果一个函数的边际收益与上下文无关，即对于所有不包含 $j$ 的集合 $S$，$\\Delta_j(S)$ 的值都相同，那么该函数就具有模性。\n观察我们推导的 $\\Delta_j(S)$ 表达式，如果涉及 $S$ 的项消失，则此条件成立。这当且仅当协方差矩阵 $\\Sigma$ 是对角矩阵时发生，即 $\\Sigma_{jS} = \\mathbf{0}$ 对所有 $j$ 和 $S$ 都成立。如果 $\\Sigma$ 是对角矩阵，神经元在统计上是独立的（对于高斯模型，是不相关的）。总信息量就是单个信息量之和，$I_S = \\sum_{i \\in S} (\\mu'_i)^2/\\Sigma_{ii}$。贪心算法会正确地选择具有最高单个 Fisher 信息值的 $k$ 个神经元。因此，**贪心算法最优的一个充分条件是噪声协方差矩阵 $\\Sigma$ 是对角矩阵。**\n\n**通过子模性的近似保证：**\n当 $\\Sigma$ 不是对角矩阵时，贪心算法不一定能保证找到最优解。但是，如果集合函数 $F(S)$ 是单调且子模的，贪心算法提供的解 $S_g$ 的值至少是最优解 $S_{opt}$ 值的 $(1 - 1/e)$ 倍：$F(S_g) \\ge (1 - 1/e)F(S_{opt})$。\n\n如果对于任意 $S \\subseteq T$ 都有 $F(S) \\le F(T)$，则函数是单调的。我们的函数 $F(S)=I_S(\\theta)$ 是单调的，因为增量收益 $\\Delta_j(S)$ 是非负的。这是因为分子是一个平方项，分母是条件方差，对于正定矩阵 $\\Sigma$ 来说必须为正。\n\n如果一个函数表现出收益递减的特性，那么它就是子模的。也就是说，对于任何集合 $S \\subseteq T$ 和任何元素 $j \\notin T$：\n$$\nF(S \\cup \\{j\\}) - F(S) \\ge F(T \\cup \\{j\\}) - F(T)\n$$\n这等价于 $\\Delta_j(S) \\ge \\Delta_j(T)$。对于 Fisher 信息函数 $F(S)=I_S(\\theta)$，这个条件通常不成立。在某些情况下，信息可以是超模的（协同的），即向一个更大的集合中添加一个神经元会带来更大的好处。对于给定的问题实例，子模性的精确条件是，对于所有的 $S \\subseteq T \\subseteq \\{0,\\dots,n-1\\} \\setminus \\{j\\}$，不等式 $\\Delta_j(S) \\ge \\Delta_j(T)$ 必须成立。对于较小的 $n$，这可以进行数值验证。",
            "answer": "[[[0, 1], [0, 1], 1.32, 1.32, True, True], [[0, 2], [0, 2], 1.64, 1.64, True, False], [[0, 1], [1, 2], 1.019608, 1.28, False, False], [[0, 1], [0, 1], 1.005025, 1.005025, True, False]]"
        }
    ]
}