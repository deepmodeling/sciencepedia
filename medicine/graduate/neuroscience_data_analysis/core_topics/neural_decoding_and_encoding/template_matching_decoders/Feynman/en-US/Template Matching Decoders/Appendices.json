{
    "hands_on_practices": [
        {
            "introduction": "The specific form of a template matching decoder is not an arbitrary choice but can be rigorously derived from first principles. This practice explores how assuming different generative statistical models for neural firing—specifically, Poisson and Gaussian distributions—logically leads to different optimal decision rules. By working through these derivations , you will gain a foundational understanding of why some decoders rely on a linear score while others use Euclidean distance, connecting statistical theory directly to decoder design.",
            "id": "4197771",
            "problem": "Consider a population decoding problem in neuroscience data analysis where a template matching decoder must classify a single-trial spike count vector into one of two stimulus classes based on templates constructed from training data. Let there be $3$ simultaneously recorded neurons. Assume that spike counts across neurons are conditionally independent given the stimulus class.\n\nTwo scientifically grounded template construction strategies are considered:\n\n1. A generative model with independent Poisson spiking for each neuron under each class, with equal prior probabilities for the classes. In this setting, you are to derive from first principles how a maximum a posteriori classifier (MAP) reduces to a linear template matching rule with an inner-product score plus a bias term. Explicitly identify the class-dependent template vector and the bias term in terms of the class-dependent Poisson rates.\n\n2. A generative model with additive, independent, identically distributed Gaussian noise of class-independent variance acting on class-dependent mean spike counts. In this setting, you are to derive from first principles how a MAP classifier reduces to choosing the class whose mean template has the smallest squared Euclidean distance from the single-trial spike count vector. Express the decision score difference between the two classes in a form that is linear in the spike counts plus a class-dependent constant.\n\nFor a specific instantiation, suppose the class $s \\in \\{1,2\\}$ has neuron-specific Poisson rate vectors given by\n$$\n\\boldsymbol{\\lambda}_{1} = \\begin{pmatrix} 7 \\\\ 3 \\\\ 12 \\end{pmatrix}, \\quad \\boldsymbol{\\lambda}_{2} = \\begin{pmatrix} 4 \\\\ 5 \\\\ 9 \\end{pmatrix},\n$$\nand a single trial produces the spike count vector\n$$\n\\mathbf{x} = \\begin{pmatrix} 6 \\\\ 7 \\\\ 10 \\end{pmatrix}.\n$$\nUnder the Poisson model, form the MAP linear decision score difference between classes $1$ and $2$ and evaluate it at $\\mathbf{x}$. Under the Gaussian model with identity covariance, form the squared Euclidean distance decision score difference between classes $1$ and $2$ and evaluate it at $\\mathbf{x}$. Let the Poisson-model decision score difference be denoted $D_{\\mathrm{Pois}}$ and the Gaussian-model decision score difference be denoted $D_{\\mathrm{Gauss}}$.\n\nFinally, compute the scalar quantity\n$$\n\\Delta \\equiv D_{\\mathrm{Pois}} - D_{\\mathrm{Gauss}}\n$$\nfor the provided rates and spike counts. Round your final numerical answer to five significant figures. No units are required.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It contains all necessary information and is free from contradictions or ambiguities. The models presented (Poisson and Gaussian generative models for spike counts) are standard in computational neuroscience, and the task involves standard derivations for maximum a posteriori (MAP) classifiers.\n\nThe solution is structured in three parts: first, the derivation for the Poisson model; second, the derivation for the Gaussian model; and third, the specific numerical calculation requested.\n\n### Part 1: Derivation for the Poisson Generative Model\n\nA maximum a posteriori (MAP) classifier selects the class $s$ that maximizes the posterior probability $P(s|\\mathbf{x})$ given an observed spike count vector $\\mathbf{x}$. Using Bayes' theorem, the posterior is given by:\n$$\nP(s|\\mathbf{x}) = \\frac{P(\\mathbf{x}|s) P(s)}{P(\\mathbf{x})}\n$$\nMaximizing the posterior is equivalent to maximizing the numerator $P(\\mathbf{x}|s) P(s)$, as the evidence $P(\\mathbf{x})$ is constant across classes. The problem states that the prior probabilities for the two classes are equal, i.e., $P(s=1) = P(s=2)$. Therefore, the MAP classifier reduces to a maximum likelihood (ML) classifier, which selects the class $s$ that maximizes the likelihood $P(\\mathbf{x}|s)$.\n\nIt is computationally more convenient to work with the logarithm of the likelihood. The decision rule is to choose the class that maximizes $\\ln P(\\mathbf{x}|s)$. The model assumes conditionally independent Poisson spiking for each of the $N=3$ neurons. The probability mass function for a Poisson distribution with rate $\\lambda$ is $P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$.\n\nGiven a spike count vector $\\mathbf{x} = (x_1, x_2, \\dots, x_N)^T$ and a class-dependent rate vector $\\boldsymbol{\\lambda}_s = (\\lambda_{s,1}, \\lambda_{s,2}, \\dots, \\lambda_{s,N})^T$, the likelihood function is the product of the individual neuron probabilities due to conditional independence:\n$$\nP(\\mathbf{x}|s) = \\prod_{i=1}^{N} P(x_i | s) = \\prod_{i=1}^{N} \\frac{(\\lambda_{s,i})^{x_i} \\exp(-\\lambda_{s,i})}{x_i!}\n$$\nThe log-likelihood is:\n$$\n\\ln P(\\mathbf{x}|s) = \\ln \\left( \\prod_{i=1}^{N} \\frac{(\\lambda_{s,i})^{x_i} \\exp(-\\lambda_{s,i})}{x_i!} \\right) = \\sum_{i=1}^{N} \\left( x_i \\ln(\\lambda_{s,i}) - \\lambda_{s,i} - \\ln(x_i!) \\right)\n$$\nTo make a decision, we compare the log-likelihoods for each class. Any terms that are not dependent on the class $s$ can be dropped from the decision function. The term $\\sum_{i=1}^{N} \\ln(x_i!)$ is independent of $s$ and can be omitted. This leaves a decision score $g_s(\\mathbf{x})$ for each class:\n$$\ng_s(\\mathbf{x}) = \\sum_{i=1}^{N} x_i \\ln(\\lambda_{s,i}) - \\sum_{i=1}^{N} \\lambda_{s,i}\n$$\nThis expression can be written as an inner product of a template vector with the data vector, plus a bias term. Let the template vector for class $s$ be $\\mathbf{w}_s$ and the bias term be $b_s$. We can identify:\n$$\n\\mathbf{w}_s = \\begin{pmatrix} \\ln(\\lambda_{s,1}) \\\\ \\ln(\\lambda_{s,2}) \\\\ \\vdots \\\\ \\ln(\\lambda_{s,N}) \\end{pmatrix}, \\quad b_s = - \\sum_{i=1}^{N} \\lambda_{s,i}\n$$\nThe decision score is then $g_s(\\mathbf{x}) = \\mathbf{w}_s^T \\mathbf{x} + b_s$, which is a linear template matching rule as required.\n\n### Part 2: Derivation for the Gaussian Generative Model\n\nIn this model, the spike count vector $\\mathbf{x}$ is generated by adding i.i.d. Gaussian noise to a class-dependent mean vector $\\boldsymbol{\\mu}_s$: $\\mathbf{x} = \\boldsymbol{\\mu}_s + \\boldsymbol{\\epsilon}$. The noise vector $\\boldsymbol{\\epsilon}$ is drawn from a multivariate normal distribution with zero mean and a class-independent covariance matrix $\\Sigma = \\sigma^2 I$, where $I$ is the identity matrix.\n\nThe likelihood of observing $\\mathbf{x}$ given class $s$ is given by the multivariate Gaussian probability density function:\n$$\nP(\\mathbf{x}|s) = \\frac{1}{(2\\pi)^{N/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_s)^T \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_s)\\right)\n$$\nSubstituting $\\Sigma = \\sigma^2 I$, we get $|\\Sigma|^{1/2} = (\\sigma^{2N})^{1/2} = \\sigma^N$ and $\\Sigma^{-1} = \\frac{1}{\\sigma^2} I$.\n$$\nP(\\mathbf{x}|s) = \\frac{1}{(2\\pi \\sigma^2)^{N/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (\\mathbf{x} - \\boldsymbol{\\mu}_s)^T (\\mathbf{x} - \\boldsymbol{\\mu}_s)\\right)\n$$\nThe term $(\\mathbf{x} - \\boldsymbol{\\mu}_s)^T (\\mathbf{x} - \\boldsymbol{\\mu}_s)$ is the squared Euclidean distance, $\\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2$.\nAs in the Poisson case, with equal priors, the MAP classifier is equivalent to an ML classifier. We maximize the log-likelihood:\n$$\n\\ln P(\\mathbf{x}|s) = -\\frac{N}{2}\\ln(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2\n$$\nSince $\\sigma^2$ is class-independent, the term $-\\frac{N}{2}\\ln(2\\pi \\sigma^2)$ is a constant with respect to $s$. The leading factor $-\\frac{1}{2\\sigma^2}$ is a negative constant. Therefore, maximizing the log-likelihood is equivalent to minimizing the squared Euclidean distance $\\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2$. This confirms the first part of the question for the Gaussian model.\n\nTo express the decision score difference in a form linear in $\\mathbf{x}$, we consider the difference in the squared distances between the two classes. Let's define the score difference as $D_{\\mathrm{Gauss}} = \\|\\mathbf{x} - \\boldsymbol{\\mu}_2\\|^2 - \\|\\mathbf{x} - \\boldsymbol{\\mu}_1\\|^2$. The decision rule is to choose class $1$ if $D_{\\mathrm{Gauss}} > 0$.\nExpanding the squared norms:\n$$\n\\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2 = \\mathbf{x}^T\\mathbf{x} - 2\\boldsymbol{\\mu}_s^T\\mathbf{x} + \\boldsymbol{\\mu}_s^T\\boldsymbol{\\mu}_s\n$$\nThe difference is:\n$$\nD_{\\mathrm{Gauss}} = (\\mathbf{x}^T\\mathbf{x} - 2\\boldsymbol{\\mu}_2^T\\mathbf{x} + \\boldsymbol{\\mu}_2^T\\boldsymbol{\\mu}_2) - (\\mathbf{x}^T\\mathbf{x} - 2\\boldsymbol{\\mu}_1^T\\mathbf{x} + \\boldsymbol{\\mu}_1^T\\boldsymbol{\\mu}_1)\n$$\n$$\nD_{\\mathrm{Gauss}} = 2\\boldsymbol{\\mu}_1^T\\mathbf{x} - 2\\boldsymbol{\\mu}_2^T\\mathbf{x} + \\boldsymbol{\\mu}_2^T\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1^T\\boldsymbol{\\mu}_1\n$$\n$$\nD_{\\mathrm{Gauss}} = 2(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T\\mathbf{x} + (\\|\\boldsymbol{\\mu}_2\\|^2 - \\|\\boldsymbol{\\mu}_1\\|^2)\n$$\nThis expression is linear in the spike counts $\\mathbf{x}$ plus a class-dependent constant, as required.\n\n### Part 3: Numerical Calculation\n\nWe are given the rate vectors and a single-trial spike count vector:\n$$\n\\boldsymbol{\\lambda}_{1} = \\begin{pmatrix} 7 \\\\ 3 \\\\ 12 \\end{pmatrix}, \\quad \\boldsymbol{\\lambda}_{2} = \\begin{pmatrix} 4 \\\\ 5 \\\\ 9 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} 6 \\\\ 7 \\\\ 10 \\end{pmatrix}\n$$\nThe \"MAP linear decision score difference\" for the Poisson model, $D_{\\mathrm{Pois}}$, is the difference between the decision scores $g_1(\\mathbf{x}) - g_2(\\mathbf{x})$.\n$$\nD_{\\mathrm{Pois}} = \\left( \\sum_{i=1}^{3} x_i \\ln(\\lambda_{1,i}) - \\sum_{i=1}^{3} \\lambda_{1,i} \\right) - \\left( \\sum_{i=1}^{3} x_i \\ln(\\lambda_{2,i}) - \\sum_{i=1}^{3} \\lambda_{2,i} \\right)\n$$\n$$\nD_{\\mathrm{Pois}} = \\sum_{i=1}^{3} x_i (\\ln(\\lambda_{1,i}) - \\ln(\\lambda_{2,i})) + \\left( \\sum_{i=1}^{3} \\lambda_{2,i} - \\sum_{i=1}^{3} \\lambda_{1,i} \\right)\n$$\nLet's compute the terms:\n$\\sum \\lambda_{1,i} = 7 + 3 + 12 = 22$.\n$\\sum \\lambda_{2,i} = 4 + 5 + 9 = 18$.\nThe dot product term is:\n$6(\\ln 7 - \\ln 4) + 7(\\ln 3 - \\ln 5) + 10(\\ln 12 - \\ln 9)$\n$= 6\\ln(\\frac{7}{4}) + 7\\ln(\\frac{3}{5}) + 10\\ln(\\frac{12}{9})$\n$= 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3})$\nSo, $D_{\\mathrm{Pois}} = 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) + (18 - 22) = 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) - 4$.\nNumerically, this is approximately $3.35769 - 3.57578 + 2.87682 - 4 = -1.34127$.\n\nFor the Gaussian model, we are instructed to use an identity covariance matrix, which implies $\\sigma^2=1$. We assume the means of the Gaussian model correspond to the rates of the Poisson model, so $\\boldsymbol{\\mu}_s = \\boldsymbol{\\lambda}_s$. The \"squared Euclidean distance decision score difference\" is $D_{\\mathrm{Gauss}} = \\|\\mathbf{x} - \\boldsymbol{\\lambda}_2\\|^2 - \\|\\mathbf{x} - \\boldsymbol{\\lambda}_1\\|^2$.\n$$\nD_{\\mathrm{Gauss}} = 2(\\boldsymbol{\\lambda}_1 - \\boldsymbol{\\lambda}_2)^T\\mathbf{x} + (\\|\\boldsymbol{\\lambda}_2\\|^2 - \\|\\boldsymbol{\\lambda}_1\\|^2)\n$$\nLet's compute the terms:\n$\\boldsymbol{\\lambda}_1 - \\boldsymbol{\\lambda}_2 = \\begin{pmatrix} 7-4 \\\\ 3-5 \\\\ 12-9 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -2 \\\\ 3 \\end{pmatrix}$.\nThe inner product term: $2(3, -2, 3) \\begin{pmatrix} 6 \\\\ 7 \\\\ 10 \\end{pmatrix} = 2(3 \\times 6 + (-2) \\times 7 + 3 \\times 10) = 2(18 - 14 + 30) = 2(34) = 68$.\nThe norm-squared difference:\n$\\|\\boldsymbol{\\lambda}_2\\|^2 = 4^2 + 5^2 + 9^2 = 16 + 25 + 81 = 122$.\n$\\|\\boldsymbol{\\lambda}_1\\|^2 = 7^2 + 3^2 + 12^2 = 49 + 9 + 144 = 202$.\n$\\|\\boldsymbol{\\lambda}_2\\|^2 - \\|\\boldsymbol{\\lambda}_1\\|^2 = 122 - 202 = -80$.\nSo, $D_{\\mathrm{Gauss}} = 68 - 80 = -12$.\n\nFinally, we compute $\\Delta = D_{\\mathrm{Pois}} - D_{\\mathrm{Gauss}}$:\n$$\n\\Delta = (6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) - 4) - (-12)\n$$\n$$\n\\Delta = 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) + 8\n$$\nUsing high-precision values:\n$D_{\\mathrm{Pois}} \\approx -1.3412639143$.\n$\\Delta = -1.3412639143 - (-12) = 10.6587360857$.\nRounding to five significant figures, we get $10.659$.",
            "answer": "$$\\boxed{10.659}$$"
        },
        {
            "introduction": "Building on the theoretical foundation of model-based decoding, this practice shifts to implementation and the crucial role of prior information. By implementing both Maximum Likelihood (ML) and Maximum A Posteriori (MAP) decoders for Poisson data , you will directly investigate how incorporating prior probabilities about stimulus occurrence can change classification results. This exercise highlights the practical importance of accounting for class imbalance, a common feature in many experimental paradigms.",
            "id": "4197763",
            "problem": "You are given a problem in neuroscience data analysis focusing on template matching decoders under class imbalance and the effect of priors. Assume a population of neurons produces spike counts in discrete time bins. Under the independent Poisson spiking model, the spike count for neuron $i$ in a time bin is modeled as a Poisson random variable with rate parameter $\\mu_{c,i}$ when the stimulus class is $c$. Classes correspond to different stimuli, behaviors, or brain states. A template for class $c$ is the vector of mean rates $\\mu_{c} = (\\mu_{c,1}, \\ldots, \\mu_{c,N})$ inferred from training data. You must implement a decoder that takes a single observed spike-count vector $x = (x_1, \\ldots, x_N)$ and returns the predicted class using two decision rules: Maximum Likelihood (ML) and Maximum A Posteriori (MAP).\n\nFundamentals:\n- The independent Poisson likelihood across neurons is $p(x \\mid c) = \\prod_{i=1}^{N} \\left( e^{-\\mu_{c,i}} \\frac{\\mu_{c,i}^{x_i}}{x_i!} \\right)$, which follows from the assumption of independent Poisson processes for spike counts per neuron per bin.\n- Bayes rule states $p(c \\mid x) \\propto p(x \\mid c) \\, p(c)$, where $p(c)$ is the prior probability of class $c$.\n- For classification, the ML rule chooses the class $c$ that maximizes $p(x \\mid c)$, and the Maximum A Posteriori (MAP) rule chooses the class $c$ that maximizes $p(c \\mid x)$, equivalently the maximizer of $\\log p(x \\mid c) + \\log p(c)$, since $\\log p(x)$ is independent of $c$.\n\nDefine the log-likelihood under the independent Poisson model as\n$$\n\\log p(x \\mid c) = \\sum_{i=1}^{N} \\left( -\\mu_{c,i} + x_i \\log \\mu_{c,i} - \\log(x_i!) \\right),\n$$\nwith the convention that $x_i \\log \\mu_{c,i}$ contributes $0$ when both $x_i = 0$ and $\\mu_{c,i} = 0$, and contributes $-\\infty$ when $\\mu_{c,i} = 0$ and $x_i > 0$. The MAP score is $\\log p(x \\mid c) + \\log p(c)$, where the prior $p(c)$ is computed from empirical training class counts $n_c$ via $p(c) = n_c / \\sum_{j} n_j$. In the ML rule, no prior is used; equivalently, it assumes a uniform prior, so only $\\log p(x \\mid c)$ is compared across classes.\n\nTie-breaking rule: If multiple classes have equal score (either ML or MAP), select the class with the smallest class index (for example, class $0$ in a tie between $0$ and $1$).\n\nImplement a program that computes, for each test case, the ML-predicted class index, the MAP-predicted class index, and a boolean indicating whether the MAP prediction differs from the ML prediction. The output for each test case must be a list $[ml\\_class, map\\_class, changed]$, where $ml\\_class$ and $map\\_class$ are integers and $changed$ is a boolean.\n\nTest suite:\nCase 1 (baseline, balanced priors, three classes, happy path):\n- Number of neurons $N = 4$.\n- Templates:\n  - Class $0$: $\\mu^{(0)} = [5.0, 2.0, 1.0, 0.5]$.\n  - Class $1$: $\\mu^{(1)} = [1.0, 5.0, 1.0, 0.5]$.\n  - Class $2$: $\\mu^{(2)} = [1.5, 1.5, 4.0, 0.5]$.\n- Training class counts: $n = [50, 50, 50]$.\n- Observation: $x = [4, 2, 1, 0]$.\n\nCase 2 (strong class imbalance in priors, three classes, prior-induced flip):\n- Number of neurons $N = 4$.\n- Templates:\n  - Class $0$: $\\mu^{(0)} = [2.0, 2.0, 2.0, 1.0]$.\n  - Class $1$: $\\mu^{(1)} = [1.0, 5.0, 1.0, 0.2]$.\n  - Class $2$: $\\mu^{(2)} = [1.5, 1.5, 3.5, 1.2]$.\n- Training class counts: $n = [1000, 20, 5]$.\n- Observation: $x = [1, 1, 4, 1]$.\n\nCase 3 (likelihood tie between two classes, imbalanced priors break the tie):\n- Number of neurons $N = 2$.\n- Templates:\n  - Class $0$: $\\mu^{(0)} = [2.0, 2.0]$.\n  - Class $1$: $\\mu^{(1)} = [2.0, 2.0]$.\n  - Class $2$: $\\mu^{(2)} = [4.0, 1.0]$.\n- Training class counts: $n = [10, 20, 1]$.\n- Observation: $x = [3, 1]$.\n\nCase 4 (edge case with zero template rates, verifying $-\\infty$ handling and zero-count convention):\n- Number of neurons $N = 3$.\n- Templates:\n  - Class $0$: $\\mu^{(0)} = [0.0, 3.0, 1.0]$.\n  - Class $1$: $\\mu^{(1)} = [0.0, 1.0, 2.0]$.\n  - Class $2$: $\\mu^{(2)} = [2.0, 3.0, 0.0]$.\n- Training class counts: $n = [30, 30, 30]$.\n- Observation: $x = [1, 3, 0]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case in order and is itself a list of the form $[ml\\_class, map\\_class, changed]$. For example, an output for two cases would look like $[[0, 0, False], [1, 0, True]]$, but your program must use the exact test suite provided above.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in standard principles of computational neuroscience and statistics, well-posed with all necessary information and a clear objective, and free from contradiction, ambiguity, or factual unsoundness. We may therefore proceed with a solution.\n\nThe objective is to classify an observed neural spike-count vector, $x = (x_1, \\ldots, x_N)$, into one of a set of predefined stimulus classes, $\\{c\\}$. Each class $c$ is characterized by a template vector of mean spike rates, $\\mu_c = (\\mu_{c,1}, \\ldots, \\mu_{c,N})$. The classification will be performed using two distinct but related probabilistic decision rules: Maximum Likelihood (ML) and Maximum A Posteriori (MAP).\n\nThe foundation of this decoder is the independent Poisson spiking model. The probability of observing a spike count of $x_i$ from neuron $i$ for a stimulus of class $c$ is given by the Poisson probability mass function:\n$$\np(x_i \\mid c) = \\frac{e^{-\\mu_{c,i}} \\mu_{c,i}^{x_i}}{x_i!}\n$$\nDue to the assumption of independence across the $N$ neurons, the total likelihood of observing the vector $x$ given class $c$ is the product of the individual probabilities:\n$$\np(x \\mid c) = \\prod_{i=1}^{N} p(x_i \\mid c) = \\prod_{i=1}^{N} \\left( e^{-\\mu_{c,i}} \\frac{\\mu_{c,i}^{x_i}}{x_i!} \\right)\n$$\nFor computational stability and analytical convenience, it is standard practice to work with the logarithm of the likelihood. The logarithm transforms the product into a sum:\n$$\n\\log p(x \\mid c) = \\sum_{i=1}^{N} \\log \\left( e^{-\\mu_{c,i}} \\frac{\\mu_{c,i}^{x_i}}{x_i!} \\right) = \\sum_{i=1}^{N} \\left( -\\mu_{c,i} + x_i \\log \\mu_{c,i} - \\log(x_i!) \\right)\n$$\nWhen comparing classes for a fixed observation $x$, the term $\\sum_{i=1}^{N} \\log(x_i!)$ is a constant additive factor across all classes. Therefore, it can be omitted from the calculation without affecting the outcome of the classification. We can define a simplified score, which we will refer to as the log-likelihood score $S_{LL}(c)$, that is proportional to the true log-likelihood:\n$$\nS_{LL}(c) = \\sum_{i=1}^{N} \\left( -\\mu_{c,i} + x_i \\log \\mu_{c,i} \\right)\n$$\n\nThe problem specifies conventions for edge cases involving zero-valued rates:\n1.  If $\\mu_{c,i} = 0$ and $x_i > 0$, observing a spike is impossible. The likelihood is $0$, and the log-likelihood is $-\\infty$. The term $x_i \\log \\mu_{c,i}$ evaluates to $-\\infty$.\n2.  If $\\mu_{c,i} = 0$ and $x_i = 0$, observing no spikes is perfectly consistent. The mathematical limit $\\lim_{y \\to 0} y \\log y = 0$ is applied, so the term $x_i \\log \\mu_{c,i}$ contributes $0$ to the sum.\n\n**Maximum Likelihood (ML) Decoding**\n\nThe ML decision rule selects the class that makes the observed data most probable. This is equivalent to choosing the class with the highest likelihood, or log-likelihood. It implicitly assumes that all classes are equally likely a priori (a uniform prior).\n$$\n\\hat{c}_{ML} = \\arg\\max_c p(x \\mid c) = \\arg\\max_c \\log p(x \\mid c) = \\arg\\max_c S_{LL}(c)\n$$\n\n**Maximum A Posteriori (MAP) Decoding**\n\nThe MAP decision rule selects the most probable class given the observed data, incorporating prior knowledge about the relative frequencies of the classes. Using Bayes' rule, the posterior probability is:\n$$\np(c \\mid x) = \\frac{p(x \\mid c) p(c)}{p(x)}\n$$\nThe term $p(x)$ is a normalizing constant that is independent of the class $c$ and can be ignored for maximization. Thus, MAP decoding is equivalent to maximizing the product of the likelihood and the prior, $p(x \\mid c)p(c)$. In log space, this becomes:\n$$\n\\hat{c}_{MAP} = \\arg\\max_c \\left( \\log p(x \\mid c) + \\log p(c) \\right)\n$$\nThe prior probabilities, $p(c)$, are estimated from the empirical frequencies of class occurrences in a training dataset, given by counts $n_c$:\n$$\np(c) = \\frac{n_c}{\\sum_{j} n_j}\n$$\nThe MAP score, $S_{MAP}(c)$, can thus be expressed using our simplified log-likelihood score:\n$$\nS_{MAP}(c) = S_{LL}(c) + \\log p(c)\n$$\nWhen priors are not uniform (i.e., class imbalance exists), the MAP decision can differ from the ML decision. A class with a very high prior probability may be chosen even if its likelihood is not the highest, provided the prior \"boosts\" its score sufficiently.\n\n**Algorithmic Implementation**\n\nFor each test case provided, the following steps are executed:\n1.  The inputs are an observation vector $x$, a set of class templates $\\{\\mu_c\\}$, and a set of training counts $\\{n_c\\}$.\n2.  The vector of log-priors, $\\log p(c)$, is computed from the training counts $n_c$. If all $n_c$ are equal, the prior is uniform, and $\\hat{c}_{MAP}$ will equal $\\hat{c}_{ML}$.\n3.  For each class $c$:\n    a. The log-likelihood score $S_{LL}(c)$ is calculated as $\\sum_{i=1}^{N} (-\\mu_{c,i} + x_i \\log \\mu_{c,i})$, carefully handling the cases where $\\mu_{c,i}=0$.\n    b. The MAP score $S_{MAP}(c)$ is calculated as $S_{LL}(c) + \\log p(c)$.\n4.  The ML-predicted class, $\\hat{c}_{ML}$, is determined by finding the index of the maximum value in the array of $S_{LL}$ scores.\n5.  The MAP-predicted class, $\\hat{c}_{MAP}$, is determined by finding the index of the maximum value in the array of $S_{MAP}$ scores.\n6.  The specified tie-breaking rule (select the smallest class index) is naturally handled by standard `argmax` functions which return the first index of a maximum value.\n7.  A boolean flag is set to `True` if $\\hat{c}_{ML} \\neq \\hat{c}_{MAP}$, and `False` otherwise.\n8.  The final result for the test case is compiled into the list $[\\hat{c}_{ML}, \\hat{c}_{MAP}, \\text{changed}]$.\nThis procedure is applied to all test cases to generate the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import xlogy\n\ndef solve():\n    \"\"\"\n    Solves the neuroscience decoding problem for a suite of test cases.\n    For each case, it computes the Maximum Likelihood (ML) and Maximum A \n    Posteriori (MAP) predicted classes for a given spike count observation.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"templates\": np.array([\n                [5.0, 2.0, 1.0, 0.5],  # Class 0\n                [1.0, 5.0, 1.0, 0.5],  # Class 1\n                [1.5, 1.5, 4.0, 0.5]   # Class 2\n            ]),\n            \"counts\": np.array([50, 50, 50]),\n            \"observation\": np.array([4, 2, 1, 0])\n        },\n        {\n            \"templates\": np.array([\n                [2.0, 2.0, 2.0, 1.0],  # Class 0\n                [1.0, 5.0, 1.0, 0.2],  # Class 1\n                [1.5, 1.5, 3.5, 1.2]   # Class 2\n            ]),\n            \"counts\": np.array([1000, 20, 5]),\n            \"observation\": np.array([1, 1, 4, 1])\n        },\n        {\n            \"templates\": np.array([\n                [2.0, 2.0],  # Class 0\n                [2.0, 2.0],  # Class 1\n                [4.0, 1.0]   # Class 2\n            ]),\n            \"counts\": np.array([10, 20, 1]),\n            \"observation\": np.array([3, 1])\n        },\n        {\n            \"templates\": np.array([\n                [0.0, 3.0, 1.0],  # Class 0\n                [0.0, 1.0, 2.0],  # Class 1\n                [2.0, 3.0, 0.0]   # Class 2\n            ]),\n            \"counts\": np.array([30, 30, 30]),\n            \"observation\": np.array([1, 3, 0])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        templates = case[\"templates\"]\n        counts = case[\"counts\"]\n        observation = case[\"observation\"]\n\n        # Calculate log-likelihood scores (proportional to the true log-likelihood)\n        # S_LL(c) = sum_i (-mu_{c,i} + x_i * log(mu_{c,i}))\n        # scipy.special.xlogy(x, y) computes x*log(y) and correctly handles\n        # xlogy(0, y) = 0 and xlogy(x>0, 0) = -inf, as required by the problem.\n        # Numpy broadcasting is used for efficient computation across all classes.\n        # `templates` is (C, N), `observation` is (N,).\n        log_likelihood_scores = np.sum(-templates + xlogy(observation, templates), axis=1)\n\n        # ML prediction: find the class with the maximum log-likelihood score.\n        # np.argmax handles the specified tie-breaking rule (smallest index).\n        ml_class = np.argmax(log_likelihood_scores)\n\n        # Calculate log-priors for MAP.\n        # log p(c) = log(n_c / sum(n_j))\n        total_counts = np.sum(counts)\n        log_priors = np.log(counts / total_counts)\n        \n        # Calculate MAP scores.\n        # S_MAP(c) = S_LL(c) + log p(c)\n        map_scores = log_likelihood_scores + log_priors\n\n        # MAP prediction: find the class with the maximum MAP score.\n        map_class = np.argmax(map_scores)\n\n        # Determine if the prediction changed.\n        changed = (ml_class != map_class)\n        \n        results.append([int(ml_class), int(map_class), bool(changed)])\n\n    # Format the final output string exactly as required.\n    # The str() representation of a list is, e.g., '[0, 0, False]'.\n    # Joining these with commas and enclosing in brackets gives the desired format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While statistical models provide a powerful framework, real neural data is often contaminated with outliers that can violate model assumptions and degrade decoder performance. This final practice introduces methods from robust statistics to build decoders that are resilient to such data imperfections. By implementing decoders based on Huber loss and trimmed deviance , you will learn how to systematically down-weight the influence of outlier neurons, leading to more reliable and robust decoding in practical applications.",
            "id": "4197761",
            "problem": "You are given a population coding problem in neuroscience data analysis focusing on template matching decoders and robustness to outliers and contamination. Consider a population of $N$ neurons responding to one of $S$ discrete stimuli. For each stimulus $s \\in \\{0,1,2\\}$, a template specifies expected mean spike counts across neurons as a nonnegative vector $\\boldsymbol{\\lambda}_s \\in \\mathbb{R}_{>0}^N$. A single trial produces an observed nonnegative integer spike count vector $\\mathbf{k} \\in \\mathbb{Z}_{\\ge 0}^N$. The objective is to decode (infer) the stimulus identity $s$ from $\\mathbf{k}$ by comparing $\\mathbf{k}$ against the templates $\\{\\boldsymbol{\\lambda}_s\\}_{s=0}^2$.\n\nStart from the fundamental base that spike counts are modeled as independent Poisson random variables across neurons conditioned on the stimulus: for neuron $i$, $K_i \\mid s \\sim \\mathrm{Poisson}(\\lambda_{s,i})$, where $\\lambda_{s,i} > 0$ is the expected count in a fixed counting window. The Maximum Likelihood Estimation (MLE) decoder chooses the stimulus $s$ that maximizes the log-likelihood given by\n$$\n\\log p(\\mathbf{k} \\mid s) = \\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} - \\log(k_i!) \\right),\n$$\nwhere the term $\\sum_i \\log(k_i!)$ does not depend on $s$ and can be ignored for comparison. Real neural data often contain contamination or outliers due to nonstationarity, noise bursts, spike sorting errors, or rare events. Robust alternatives to classical template matching can be constructed using Generalized Linear Model (GLM) deviance residuals and robust aggregation. Define the Poisson deviance for neuron $i$ and stimulus $s$:\n$$\nD_{s,i}(\\mathbf{k}) = 2 \\left( k_i \\log\\left( \\frac{k_i}{\\lambda_{s,i}} \\right) - (k_i - \\lambda_{s,i}) \\right),\n$$\nwith the convention that $k_i \\log(k_i/\\lambda_{s,i}) = 0$ when $k_i = 0$. The signed deviance residual is\n$$\nr_{s,i}(\\mathbf{k}) = \\operatorname{sign}(k_i - \\lambda_{s,i}) \\sqrt{ D_{s,i}(\\mathbf{k}) }.\n$$\nA robust M-estimator objective can be formed using the Huber loss function $\\rho_\\delta$ with threshold $\\delta > 0$,\n$$\n\\rho_\\delta(x) = \\begin{cases}\n\\frac{1}{2} x^2, & \\text{if } |x| \\le \\delta, \\\\\n\\delta \\left( |x| - \\frac{1}{2} \\delta \\right), & \\text{if } |x| > \\delta,\n\\end{cases}\n$$\nand the robust score for stimulus $s$ is $\\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k}))$, with the decoder choosing the stimulus that minimizes this score. Another robust alternative is a trimmed deviance aggregator that sorts $\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$ and sums only the smallest fraction $\\tau \\in (0,1]$ of them, thereby reducing the influence of the largest residuals associated with outliers.\n\nYour task is to implement three decoders:\n- The classical Poisson MLE template matcher that chooses $s$ maximizing $\\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} \\right)$.\n- The robust Huber deviance template matcher that chooses $s$ minimizing $\\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k}))$.\n- The trimmed deviance template matcher that chooses $s$ minimizing the sum of the smallest $\\lfloor \\tau N \\rfloor$ values of $\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$.\n\nAdopt the following templates defined by scaling a base rate vector. Let $N = 12$, $S = 3$, the base rate vector be\n$$\n\\mathbf{b} = [\\, 5,\\, 6,\\, 4,\\, 8,\\, 2,\\, 9,\\, 3,\\, 7,\\, 5,\\, 6,\\, 4,\\, 2 \\,],\n$$\nand the stimulus-specific scaling factors be\n$$\n\\mathbf{f} = [\\, 1.0,\\, 1.5,\\, 0.6 \\,].\n$$\nFor $s \\in \\{0,1,2\\}$, define $\\boldsymbol{\\lambda}_s$ component-wise by $\\lambda_{s,i} = f_s \\cdot b_i$. Concretely, the templates are\n$$\n\\boldsymbol{\\lambda}_0 = [\\, 5.0,\\, 6.0,\\, 4.0,\\, 8.0,\\, 2.0,\\, 9.0,\\, 3.0,\\, 7.0,\\, 5.0,\\, 6.0,\\, 4.0,\\, 2.0 \\,],\n$$\n$$\n\\boldsymbol{\\lambda}_1 = [\\, 7.5,\\, 9.0,\\, 6.0,\\, 12.0,\\, 3.0,\\, 13.5,\\, 4.5,\\, 10.5,\\, 7.5,\\, 9.0,\\, 6.0,\\, 3.0 \\,],\n$$\n$$\n\\boldsymbol{\\lambda}_2 = [\\, 3.0,\\, 3.6,\\, 2.4,\\, 4.8,\\, 1.2,\\, 5.4,\\, 1.8,\\, 4.2,\\, 3.0,\\, 3.6,\\, 2.4,\\, 1.2 \\,].\n$$\nNote that the numerical values are strictly positive and represent expected spike counts per fixed counting window; since the outputs are discrete labels, you do not need to express any physical units for the final answer.\n\nUse the following test suite of observed spike count vectors (each of length $12$), which includes normal conditions, contamination, and edge cases. Each test case requires decoding the stimulus by all three methods specified above. In all cases, use Huber threshold $\\delta = 2.0$ and trimming proportion $\\tau = 0.7$.\n\n- Test case $1$ (happy path, no contamination, typical counts near $\\boldsymbol{\\lambda}_1$):\n$$\n\\mathbf{k}^{(1)} = [\\, 8,\\, 9,\\, 6,\\, 12,\\, 3,\\, 14,\\, 5,\\, 11,\\, 8,\\, 9,\\, 6,\\, 3 \\,].\n$$\n- Test case $2$ (heavy contamination: majority near $\\boldsymbol{\\lambda}_2$ with large outliers in three neurons):\n$$\n\\mathbf{k}^{(2)} = [\\, 3,\\, 4,\\, 2,\\, 5,\\, 50,\\, 5,\\, 40,\\, 4,\\, 3,\\, 60,\\, 2,\\, 1 \\,].\n$$\n- Test case $3$ (boundary: all neurons silent):\n$$\n\\mathbf{k}^{(3)} = [\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0 \\,].\n$$\n- Test case $4$ (single extreme outlier, otherwise near zero):\n$$\n\\mathbf{k}^{(4)} = [\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 100,\\, 0,\\, 0 \\,].\n$$\n- Test case $5$ (mild contamination: typical counts near $\\boldsymbol{\\lambda}_0$ with two moderate outliers):\n$$\n\\mathbf{k}^{(5)} = [\\, 5,\\, 6,\\, 4,\\, 8,\\, 2,\\, 9,\\, 3,\\, 7,\\, 5,\\, 20,\\, 15,\\, 2 \\,].\n$$\n\nImplement deterministic tie-breaking by choosing the smallest stimulus index $s$ in case of exact equality of scores up to a tolerance of $\\epsilon = 10^{-12}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of three integers $[s_{\\mathrm{MLE}}, s_{\\mathrm{Huber}}, s_{\\mathrm{Trim}}]$ corresponding to the decoded labels by the three methods in the order specified above. For example, the final output format for five test cases is\n$$\n[[a_1, b_1, c_1], [a_2, b_2, c_2], [a_3, b_3, c_3], [a_4, b_4, c_4], [a_5, b_5, c_5]],\n$$\nwith each $a_j$, $b_j$, and $c_j$ an integer in $\\{0,1,2\\}$.\n\nNo user input is required. The program must be self-contained and reproduce the exact test suite and parameters given above.",
            "solution": "The user-provided problem is a well-defined task in computational neuroscience. It requires the implementation and comparison of three distinct decoding algorithms for population-coded neural data, based on a Poisson statistical model of spike counts. The problem is scientifically grounded, mathematically precise, and contains all necessary data and parameters for a unique solution. Therefore, it is deemed valid.\n\nThe core of the problem is to infer a stimulus $s \\in \\{0, 1, 2\\}$ from an observed spike count vector $\\mathbf{k} \\in \\mathbb{Z}_{\\ge 0}^{N}$ originating from a population of $N=12$ neurons. The inference is based on comparing the observation $\\mathbf{k}$ to a set of predefined templates $\\{\\boldsymbol{\\lambda}_s\\}_{s=0}^2$, where each template $\\boldsymbol{\\lambda}_s \\in \\mathbb{R}_{>0}^N$ represents the expected spike counts for a given stimulus $s$. The components of the templates are specified as $\\lambda_{s,i} = f_s \\cdot b_i$, using the provided base rate vector $\\mathbf{b}$ and stimulus-specific scaling factors $\\mathbf{f}$.\n\nThe solution involves computing a score for each stimulus $s$ under three different decoding schemes and selecting the stimulus that optimizes this score. Tie-breaking is resolved by choosing the stimulus with the smallest index.\n\n**1. Poisson Maximum Likelihood (MLE) Decoder**\n\nThis decoder is based on the fundamental assumption that spike counts $k_i$ for each neuron $i$ are independent Poisson random variables with mean $\\lambda_{s,i}$, conditioned on stimulus $s$. The likelihood of observing the vector $\\mathbf{k}$ given stimulus $s$ is:\n$$\np(\\mathbf{k} \\mid s) = \\prod_{i=1}^N \\frac{e^{-\\lambda_{s,i}} \\lambda_{s,i}^{k_i}}{k_i!}\n$$\nTo find the maximum likelihood estimate of $s$, we maximize the log-likelihood:\n$$\n\\log p(\\mathbf{k} \\mid s) = \\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} - \\log(k_i!) \\right)\n$$\nSince the term $\\sum_{i=1}^N \\log(k_i!)$ is independent of the stimulus $s$, it can be dropped from the optimization. The objective for the MLE decoder is thus to find the stimulus $s$ that maximizes the score:\n$$\n\\mathcal{S}_{\\text{MLE}}(s; \\mathbf{k}) = \\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} \\right)\n$$\nThe decoded stimulus is $s_{\\text{MLE}} = \\arg\\max_{s} \\mathcal{S}_{\\text{MLE}}(s; \\mathbf{k})$. This method is optimal under the assumed model but is known to be sensitive to outliers or model misspecification, as large counts $k_i$ heavily influence the score through the $k_i \\log \\lambda_{s,i}$ term.\n\n**2. Robust Huber Deviance Decoder**\n\nThis decoder is a robust alternative that aims to mitigate the influence of outliers. It operates on deviance residuals, which measure the discrepancy between the observed data and the model's expectation. The Poisson deviance for neuron $i$ under the hypothesis of stimulus $s$ is:\n$$\nD_{s,i}(\\mathbf{k}) = 2 \\left( k_i \\log\\left( \\frac{k_i}{\\lambda_{s,i}} \\right) - (k_i - \\lambda_{s,i}) \\right)\n$$\nA special convention is used for $k_i=0$, where the term $k_i \\log(k_i/\\lambda_{s,i})$ is taken to be $0$, resulting in $D_{s,i} = 2\\lambda_{s,i}$. The signed deviance residual is then:\n$$\nr_{s,i}(\\mathbf{k}) = \\operatorname{sign}(k_i - \\lambda_{s,i}) \\sqrt{ D_{s,i}(\\mathbf{k}) }\n$$\nThese residuals are approximately standard normal if the model is correct. To achieve robustness, the sum of squared residuals (which would be related to the total deviance) is replaced by a sum of less rapidly increasing functions of the residuals. The Huber loss function $\\rho_\\delta(x)$ with a threshold $\\delta>0$ is used for this purpose:\n$$\n\\rho_\\delta(x) = \\begin{cases}\n\\frac{1}{2} x^2, & \\text{if } |x| \\le \\delta \\\\\n\\delta \\left( |x| - \\frac{1}{2} \\delta \\right), & \\text{if } |x| > \\delta\n\\end{cases}\n$$\nFor small residuals ($|r_{s,i}| \\le \\delta$), this is equivalent to standard least squares, but for large residuals, its growth becomes linear instead of quadratic, thereby down-weighting the influence of outliers. The objective is to find the stimulus $s$ that minimizes the total Huber score, using the given threshold $\\delta = 2.0$:\n$$\n\\mathcal{S}_{\\text{Huber}}(s; \\mathbf{k}) = \\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k}))\n$$\nThe decoded stimulus is $s_{\\text{Huber}} = \\arg\\min_{s} \\mathcal{S}_{\\text{Huber}}(s; \\mathbf{k})$.\n\n**3. Trimmed Deviance Decoder**\n\nThis is another robust method that handles outliers by explicitly excluding them. It computes the deviance $D_{s,i}(\\mathbf{k})$ for each neuron but, instead of summing them all, it discards a fraction of the largest deviances, which are presumed to correspond to contaminant data points or outliers.\n\nThe procedure is as follows:\n1. For a given stimulus $s$ and observation $\\mathbf{k}$, compute the set of deviances $\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$.\n2. Sort these deviances in non-decreasing order.\n3. Sum only the smallest $\\lfloor \\tau N \\rfloor$ deviances, where $\\tau \\in (0, 1]$ is the trimming proportion.\n\nFor this problem, $N=12$ and $\\tau=0.7$, so we sum the $\\lfloor 0.7 \\times 12 \\rfloor = \\lfloor 8.4 \\rfloor = 8$ smallest deviances. The objective is to find the stimulus $s$ that minimizes this trimmed sum:\n$$\n\\mathcal{S}_{\\text{Trim}}(s; \\mathbf{k}) = \\sum_{j=1}^{\\lfloor \\tau N \\rfloor} D_{(j)}(s; \\mathbf{k})\n$$\nwhere $D_{(j)}$ is the $j$-th smallest value in the set of deviances $\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$. The decoded stimulus is $s_{\\text{Trim}} = \\arg\\min_{s} \\mathcal{S}_{\\text{Trim}}(s; \\mathbf{k})$.\n\nThe implementation proceeds by first calculating the three templates $\\boldsymbol{\\lambda}_0, \\boldsymbol{\\lambda}_1, \\boldsymbol{\\lambda}_2$. Then, for each test case vector $\\mathbf{k}^{(j)}$, the three scores ($\\mathcal{S}_{\\text{MLE}}$, $\\mathcal{S}_{\\text{Huber}}$, $\\mathcal{S}_{\\text{Trim}}$) are computed for each of the three stimuli. The stimulus index that optimizes the respective score is chosen as the decoded stimulus for that method. The tie-breaking rule of selecting the smallest stimulus index $s$ in case of score equality within a tolerance of $\\epsilon = 10^{-12}$ is applied.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares three decoders (MLE, Huber, Trimmed) for a \n    neuroscience population coding problem.\n    \"\"\"\n\n    # 1. DEFINE PARAMETERS AND TEMPLATES\n    N = 12\n    S = 3\n    stimuli_indices = np.arange(S)\n    \n    b = np.array([5, 6, 4, 8, 2, 9, 3, 7, 5, 6, 4, 2], dtype=float)\n    f = np.array([1.0, 1.5, 0.6], dtype=float)\n    \n    # Stimulus templates (S x N matrix)\n    lambdas = np.outer(f, b)\n    \n    # Robustness parameters\n    delta = 2.0  # Huber threshold\n    tau = 0.7    # Trimming proportion\n    num_to_keep = int(np.floor(tau * N))\n    \n    # Tie-breaking tolerance\n    epsilon = 1e-12\n\n    test_cases = [\n        np.array([8, 9, 6, 12, 3, 14, 5, 11, 8, 9, 6, 3], dtype=float),\n        np.array([3, 4, 2, 5, 50, 5, 40, 4, 3, 60, 2, 1], dtype=float),\n        np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=float),\n        np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 0, 0], dtype=float),\n        np.array([5, 6, 4, 8, 2, 9, 3, 7, 5, 20, 15, 2], dtype=float),\n    ]\n\n    def argmin_tiebreak(scores):\n        min_score = np.min(scores)\n        tied_indices = np.where(scores  min_score + epsilon)[0]\n        return np.min(tied_indices)\n\n    def argmax_tiebreak(scores):\n        max_score = np.max(scores)\n        tied_indices = np.where(scores > max_score - epsilon)[0]\n        return np.min(tied_indices)\n\n    all_results = []\n    \n    for k in test_cases:\n        mle_scores = np.zeros(S)\n        huber_scores = np.zeros(S)\n        trim_scores = np.zeros(S)\n        \n        for s in stimuli_indices:\n            lambda_s = lambdas[s]\n            \n            # --- MLE Decoder Score (maximize) ---\n            # Score = sum(k_i * log(lambda_{s,i}) - lambda_{s,i})\n            mle_scores[s] = np.sum(k * np.log(lambda_s) - lambda_s)\n            \n            # --- Deviance Calculation (for Huber and Trimmed) ---\n            # Dev = 2 * (k*log(k/lambda) - (k-lambda))\n            # Handle k=0 case for the log term\n            log_term_dev = np.zeros_like(k)\n            nonzero_k_mask = k > 0\n            log_term_dev[nonzero_k_mask] = k[nonzero_k_mask] * np.log(k[nonzero_k_mask] / lambda_s[nonzero_k_mask])\n            deviances = 2 * (log_term_dev - (k - lambda_s))\n\n            # --- Huber Deviance Decoder Score (minimize) ---\n            residuals = np.sign(k - lambda_s) * np.sqrt(deviances)\n            abs_residuals = np.abs(residuals)\n            huber_losses = np.where(abs_residuals = delta,\n                                  0.5 * residuals**2,\n                                  delta * (abs_residuals - 0.5 * delta))\n            huber_scores[s] = np.sum(huber_losses)\n            \n            # --- Trimmed Deviance Decoder Score (minimize) ---\n            sorted_deviances = np.sort(deviances)\n            trim_scores[s] = np.sum(sorted_deviances[:num_to_keep])\n            \n        # 2. DECODE STIMULUS FOR EACH METHOD\n        s_mle = argmax_tiebreak(mle_scores)\n        s_huber = argmin_tiebreak(huber_scores)\n        s_trim = argmin_tiebreak(trim_scores)\n        \n        all_results.append([int(s_mle), int(s_huber), int(s_trim)])\n\n    # 3. FORMAT AND PRINT THE FINAL OUTPUT\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}